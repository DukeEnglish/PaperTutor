MotionLCM: Real-time Controllable Motion
Generation via Latent Consistency Model
WenxunDai1,Ling-HaoChen1,JingboWang2∗,JinpengLiu1,BoDai2∗,YansongTang1
1TsinghuaUniversity,2ShanghaiAILaboratory
{wxdai2001, thu.lhchen, wangjingbo1219, liu.jinpeng.55}@gmail.com
{doubledaibo, tangyansong15}@gmail.com
Projectpage: https://dai-wenxun.github.io/MotionLCM-page
Text-to-Motion Motion Control
“a man walks forward at a slow pace “a person jauntily
and trips but catches himself.” skipsforward.”
31ms 34ms
SparseControl
“a man walks forward “the person was pushed
in a snake like pattern.” but did not fall.”
30ms 36ms
DenseControl
Figure1: WeproposeMotionLCM,areal-timecontrollablemotionlatentconsistencymodel,which
iscapableofachievinghigh-qualitytext-to-motionandprecisemotioncontrolresults(bothsparse
anddenseconditions)in∼30ms.
Abstract
ThisworkintroducesMotionLCM,extendingcontrollablemotiongenerationto
areal-timelevel. Existingmethodsforspatialcontrolintext-conditionedmotion
generation suffer from significant runtime inefficiency. To address this issue,
wefirstproposethemotionlatentconsistencymodel(MotionLCM)formotion
generation,buildinguponthelatentdiffusionmodel[7](MLD).Byemployingone-
step(orfew-step)inference,wefurtherimprovetheruntimeefficiencyofthemotion
latentdiffusionmodelformotiongeneration. Toensureeffectivecontrollability,
weincorporateamotionControlNetwithinthelatentspaceofMotionLCMand
enableexplicitcontrolsignals(e.g.,pelvistrajectory)inthevanillamotionspace
tocontrolthegenerationprocessdirectly,similartocontrollingotherlatent-free
diffusion models for motion generation [18]. By employing these techniques,
ourapproachcangeneratehumanmotionswithtextandcontrolsignalsinreal-
time. Experimentalresultsdemonstratetheremarkablegenerationandcontrolling
capabilitiesofMotionLCMwhilemaintainingreal-timeruntimeefficiency.
∗Correspondence:JingboWangandBoDai.
Preprint,technicalreport(version1.0).
4202
rpA
03
]VC.sc[
1v95791.4042:viXra1 Introduction
Text-to-motion generation (T2M) has at-
AITS FID Methods
tracted increasing attention [1, 36, 47, 31, 4 0.017s 3.734 TEMOS
12] due to its important roles in many 0.038s 1.067 T2M
applications [23, 51, 53]. Previous at- 3 14.74s 0.630 MotionDiffuse
tempts mainly focus on GANs [1, 27], D IF 02 .4 2. 17 74 ss 0 0..5 44 74
3
M MD LDM
VAEs [15, 35, 36, 3] and diffusion mod- 2
0.030s 0.467 MotionLCM
els[7,47,60,6,8,41,58,61,28]viapair-
wisetext-motiondata[33,13,45,37]and 1 Ours Diffusion models
achieveimpressivegenerationresults. Ex-
isting approaches [47, 7, 60] mainly take 0.02 0.04 0.08 0.1 1 10 20 30
diffusionmodelsasabasegenerativemodel, Average Inference Time per Sentence (AITS) in seconds
Figure2: ComparisonoftheinferencetimecostsonHu-
owing to their powerful ability to model
manML3D[13]. WecomparetheAITSandFIDmetrics
motiondistribution. However, thesediffu-
withsixSOTAmethods.Thecloserthemodelistotheorigin
sionfashionsinevitablyrequireconsiderable
thebetter.Diffusion-basedmodelsareindicatedbytheblue
samplingstepsformotionsynthesisduring
dashedbox.OurMotionLCMachievesreal-timeinference
inference,evenwithsomesamplingacceler- speedwhileensuringhigh-qualitymotiongeneration.
ationmethods[43].Specifically,MDM[47]
andMLD[7]require∼12sand∼0.2stogenerateahigh-qualitymotionsequence.Suchlowefficiency
blockstheapplicationsofgeneratinghigh-qualitymotionsinvariousreal-timescenarios.
Inadditiontothelanguagedescriptionitselfservingasacoarsecontrolsignal,anotherlineofresearch
focusesoncontrollingthemotiongenerationwithspatialconstraints[41,18,54]. Althoughthese
attempts enjoy impressive controlling ability in the T2M task, there still exists a significant gap
towards real-time applications. For example, based on MDM [47], OmniControl [54] exhibits a
relativelylongmotiongenerationtime,∼72spersequence.Therefore,trading-offbetweengeneration
qualityandefficiencyisachallengingproblem. Asaresult,inthispaper,wetargetthereal-time
controllablemotiongenerationresearchproblem.
Recently, the concept of consistency models [44, 32] has been introduced in image generation,
resultinginsignificantprogressbyenablingefficientandhigh-fidelityimagesynthesiswithaminimal
number of steps (e.g., 4 steps vs. 50 steps). These properties perfectly align with our goal of
acceleratingmotiongenerationwithoutcompromisinggenerationquality. Asaresult,wepropose
MotionLCM (Motion Latent Consistency Model) distilled by a motion latent diffusion model,
MLD[7],totacklethelow-efficiencyproblemindiffusionsampling. Tothebestofourknowledge,
weintroduceconsistencydistillationintothemotiongenerationareaforthefirsttimeandaccelerate
motiongenerationtoareal-timelevelvialatentconsistencydistillation.
Here,inMotionLCM,wearefacinganotherchallengeonhowtocontrolmotionswithspatialsignals
(e.g.,pelvistrajectory)inthelatentspace. Previousmethods[54,6,41,18]modelhumanmotionsin
thevanillamotionspaceandcanmanipulatethemotiondirectlyinthedenoisingprocess. However,
forourlatent-diffusion-basedMotionLCM,itisnon-trivialtofeedthecontrolsignalsintothelatent
space. Thisisbecausecodesinthelatentspacehavenoexplicitmotionsemantics,whichcannotbe
manipulateddirectlybycontrollingsignals. Inspiredbythenotablesuccessof[59]incontrollable
imagegeneration[40],weintroduceamotionControlNettocontrolmotiongenerationinthelatent
space. However, the naïve motion ControlNet is not totally sufficient to provide supervision for
controllingsignals. Themainreasonisthelackofexplicitsupervisioninthemotionspace. Therefore,
duringthetrainingphase,wedecodethepredictedlatentcodesthroughthefrozenVAE[20]decoder
intovanillamotionspacetoprovideexplicitcontrolsupervisiononthegeneratedmotion. Thanksto
thepowerfulone-stepinferencecapabilityofMotionLCM,thelatentcodesgeneratedbyMotionLCM
cansignificantlyfacilitatecontrolsupervisionbothinthelatentandmotionspacefortrainingthe
motionControlNetcomparedtoMLD[7].
With our key designs, our proposed MotionLCM successfully enjoys the balance between the
generationqualityandefficiencyincontrollablemotiongeneration. Beforedeliveringintodetail,we
wouldliketosumupourcorecontributionsasfollows.
• We propose a Motion Latent Consistency Model (MotionLCM) via latent consistency
distillationonthemotionlatentdiffusionmodelextendingcontrollablemotiongenerationto
areal-timelevel.
2• Building upon our achievement of real-time motion generation, we introduce a motion
ControlNet,enablinghigh-qualitycontrollablemotiongeneration.
• ExtensiveexperimentalresultsshowthatMotionLCMenjoysagoodbalanceofgeneration
quality,controllingcapability,andreal-timeefficiency.
2 RelatedWork
2.1 HumanMotionGeneration
Generating human motions can be divided into two main fashions according to inputs: motion
synthesis1)withoutanycondition[57,63,62,47,39],and2)withsomegivenmulti-modalconditions,
suchasactionlabels[10,21,35,15,55],textualdescription[2,13,31,3,36,47,60,46,1,27,7,51],
audioormusic[42,26,8,22,48,24]. Togeneratediverse,natural,andhigh-qualityhumanmotions,
many generative models have been explored by [2, 27, 36]. Recently, diffusion-based models
significantlyimprovedthemotiongenerationperformanceanddiversity[47,7,8,60,25,56]with
stabletraining.Specifically,MotionDiffuse[60]representsthefirsttext-basedmotiondiffusionmodel
thatprovidesfine-grainedinstructionsonbodypartsandachievesarbitrary-lengthmotionsynthesis
withtime-variedtextprompts. MDM[47]introducesamotiondiffusionmodelthatoperatesonraw
motiondata,enablingbothhigh-qualitygenerationandgenericconditioningthattogethercomprise
a good baseline for new motion generation tasks. The work most relevant to ours is MLD [7],
whichintroducesamotionlatent-baseddiffusionmodeltoenhancegenerativequalityandreduce
computationalresourcerequirements. TheprimaryconceptinvolvesinitiallytrainingaVAE[20]
for motion embedding, followed by implementing latent diffusion [40] within the learned latent
space. However,thesediffusionfashionsinevitablyrequireconsiderablesamplingstepsformotion
synthesisduringinference,evenwithsomesamplingaccelerationmethods[43]. Thus,wepropose
MotionLCM,whichnotonlyguaranteeshigh-qualitymotiongenerationbutalsoenhancesefficiency.
2.2 MotionControl
Formotioncontrol,MDM[47]andHumanMAC[6]demonstratezero-shotcontrollingusingdiffusion
models. Following this, Shafir et al. [41] propose PriorMDM to generate long-sequence human
motion,enablingjointandtrajectory-levelcontrolandediting. Additionally,GMD[18]incorporates
spatialconstraintsthroughatwo-stagediffusionapproach. However,GMD’scontrolislimitedto
2Dpelvispositions,restrictingitsadaptabilityacrossvariouspracticalscenarios. OmniControl[54]
integratesflexiblespatialcontrolsignalsacrossdifferentjointsbycombininganalyticspatialguidance
andrealismguidanceintothediffusionmodel,ensuringthatthegeneratedmotioncloselyconforms
to the input control signals. TLControl [50] leverages a disentangled latent space for diverse
humanmotion,enablinghigh-fidelitymotiongenerationalignedwithbothlanguagedescriptionsand
specifiedtrajectories. However,TLControlrequiresadditionaltest-timeoptimization. Whilethese
approachesachievegoodcontrolqualityundergivenconditions,thereremainsasignificantgapin
acceleratingthemodeltoreal-timeperformance. Weintroduceourtechnicalsolutionsasfollows.
3 Method
Inthissection,wefirstbrieflyintroducepreliminariesaboutlatentconsistencymodelsinSec.3.1.
Then,wedescribehowtoperformlatentconsistencydistillationformotiongenerationinSec.3.2,
followedbyourimplementationofmotioncontrolinlatentspaceinSec.3.3.
3.1 Preliminaries
The Consistency Model (CM) [44] introduces a kind of efficient generative model designed for
efficient one-step or few-step generation. Given a Probability Flow ODE (a.k.a. PF-ODE) that
smoothly converts data to noise, the CM is to learn the function f(·,·) (i.e., the solution of the
PF-ODE) that maps any points on the ODE trajectory to its origin distribution. The consistency
functionisformallydefinedasf :(x ,t)(cid:55)−→x ,wheret∈[0,T],T >0isafixedconstant,ϵisa
t ϵ
smallpositivenumbertoavoidnumericalinstability,andthexˆ canbetreatedasanapproximate
ϵ
samplefromthedatadistribution(xˆ ∼p (x)). Accordingto[44],theconsistencyfunctionshould
ϵ data
satisfytheself-consistencyproperty(Definition1).
3Definition1 Self-consistencyProperty.Theself-consistencypropertyofconsistencyfunctionf(·,·)
canbedefinedas,
f(x ,t)=f(x ,t′),∀t,t′ ∈[ϵ,T]. (1)
t t′
AsshowninDefinition1,theself-consistencypropertyindicatesthatforarbitrarypairsof(x ,t)
t
on the same PF-ODE trajectory, the outputs of the model should be consistent. The goal of a
parameterizedconsistencymodelf (·,·)istolearnaconsistencyfunctionfromdatabyenforcing
Θ
theself-consistencypropertyinEq.(1). Toensurethef (x,ϵ)=xproperty,theconsistencymodel
Θ
isparameterizedasfollowsviaskipconnections,
f (x,t)=c (t)x+c (t)F (x,t), (2)
Θ skip out Θ
wherec (t)andc (t)aredifferentiablefunctionswithc (ϵ)=1andc (ϵ)=0,andF (·,·)
skip out skip out Θ
isadeepneuralnetworktolearntheself-consistency. TheCMtrainedfromdistillingtheknowledge
ofpre-traineddiffusionmodelsiscalledConsistencyDistillation. Theconsistencylossisdefinedas,
L(Θ,Θ−;Φ)=E(cid:2) d(cid:0) f (x ,t ),f (xˆΦ,t )(cid:1)(cid:3) , (3)
Θ tn+1 n+1 Θ− tn n
whered(·,·)isachosenmetricfunctionformeasuringthedistancebetweentwosamples. f (·,·)
Θ
andf (·,·)arereferredtoas“onlinenetwork”and“targetnetwork”accordingto[44]. Besides,
Θ−
Θ−isupdatedwiththeexponentialmovingaverage(EMA)oftheparameterΘ2. InEq.(3),xˆΦ is
tn
theone-stepestimationfromx . Here,thexˆΦ canbeformulatedas,
tn+1 tn
xˆΦ ←x +(t −t )Φ(x ,t ), (4)
tn tn+1 n n+1 tn+1 n+1
whereΦ(·,·)isaone-stepODEsolverappliedtoPF-ODE.
Latent Consistency Models (LCMs) [32] conduct the consistency distillation in the latent space
D ={(z,c)|z=E(x),(x,c)∈D},whereDdenotesthedataset,cisthegivencondition,andE
z
isthepre-trainedencoder. Insteadofensuringconsistencybetweenadjacenttimestepst →t ,
n+1 n
LCMs[32]aredesignedtoensureconsistencybetweenthecurrenttimestepandk-stepaway,i.e.,
t → t , thereby significantly reducing convergence time costs. As classifier-free guidance
n+k n
(CFG) [16] plays a crucial role in synthesizing high-quality text-aligned visual contents, LCMs
integrateCFGintothedistillationasfollows,
zˆΦ,w ←z +(1+w)Φ(z ,t ,t ,c)−wΦ(z ,t ,t ,∅). (5)
tn tn+k tn+k n+k n tn+k n+k n
wherewdenotestheCFGscalewhichisuniformlysampledfrom[w ,w ]andkistheskipping
min max
interval. Besides,theinputofΦisexpandedduetothek-stepconsistencyandthegivencondition.
3.2 MotionLCM:MotionLatentConsistencyModel
Motioncompressionintothelatentspace. Motivatedby[44,32],weproposeMotionLCM(Motion
LatentConsistencyModel)totacklethelow-efficiencyprobleminmotiondiffusionmodels[47,60],
unleashingthepotentialofLCMinthemotiongenerationtask. SimilartoMLD[7],ourMotionLCM
adopts a consistency model in the motion latent space. We choose the powerful MLD [7] as the
underlyingdiffusionmodeltodistillfrom. Weaimtoachieveafew-step(2∼4)andevenone-step
inferencewithoutcompromisingmotionquality. InMLD,theautoencoder(E,D)isfirsttrainedto
compressahighdimensionalmotionintoalowdimensionallatentvectorz=E(x),whichisthen
decodedtoreconstructthemotionasxˆ =D(z). Trainingdiffusionmodelsinthemotionlatentspace
greatlyreducesthecomputationalrequirementscomparedtothevanilladiffusionmodelstrainedon
rawmotionsequences(i.e.,motionspace)andspeedsuptheinferenceprocess. Accordingly,wetake
goodadvantageofthemotionlatentspaceforconsistencydistillation.
Motionlatentconsistencydistillation. Anoverviewofourmotionlatentconsistencydistillation
isdescribedinFig.3(a). Arawmotionsequencex1:N = {xi}N isasequenceofhumanposes
0 i=1
representedbyxi ∈RK,whereK isthedimensionoftheposerepresentationandN isthenumber
offrames. Wefollow[13]tousetheredundantmotionrepresentationforourexperiments,whichis
2EMAoperation:Θ− ←sg(µΘ−+(1−µ)Θ),wheresg(·)denotesthestopgradoperationandµsatisfies
0≤µ<1.
4x1:𝑁
xො1 0:𝑁
0 re Latent Space Extract Traj.
ℰd o
c n
z0 Diffuse z𝑛+𝑘
𝒟
E Decoder Motion Space ControlLoss
zො𝑛
Recon.
xො1:𝑁
zො0
Loss
z0
L
0 𝒟re d
o c e D
(MNO
oe
tn
it
owl ni Ln
o
Ce
r Mk )
𝚯EMA
𝚯−
NT ea twrg oe rt
k
O 𝚽DE
𝑘
S -so tl ev per NTe ea twch oe r𝚯 r k∗
MotionLCM CoM nto rt oio
lNn𝚯 eta
apS
tneta
Traj.
Motion Space zො0 Con Lsi os ste sncy zො0− zො0∗ z𝑛
ec Encod 𝚯er
b
(a) Motion Latent Consistency Distillation (b)MotionControlin Latent Space
Figure 3: The overview of MotionLCM. (a) Motion Latent Consistency Distillation (Sec. 3.2).
Givenarawmotionsequencex1:N, apre-trainedVAEencoderfirstcompressesitintothelatent
0
space,thenperformsaforwarddiffusiontoaddn+kstepnoise. Then,thenoisyz isfedinto
n+k
the online network and teacher network to predict the clean latent. The target network takes the
k-stepestimationresultsoftheteacheroutputtopredictthecleanlatent. Tolearnself-consistency,a
lossisappliedtoenforcetheoutputoftheonlinenetworkandtargetnetworktobeconsistent. (b)
MotionControlinLatentSpace(Sec.3.3). WiththepowerfulMotionLCMtrainedinthefirststage,
weincorporateamotionControlNetintotheMotionLCMtoachievecontrollablemotiongeneration.
Furthermore,weleveragethedecodedmotiontosupervisethespatialcontrolsignals.
widelyusedinpreviouswork[47,60,7]. AsshownintheFig.3(a),givenarawmotionsequence
x1:N, a pre-trained VAE encoder first compresses it into the latent space, z = E(x ). Then, a
0 0 0
forwarddiffusionoperationwithn+kstepsisconductedtoaddnoiseonz ,wherekistheskipping
0
intervalillustratedinSec.3.1. Thenoisyz isfedtothefrozenteachernetwork,thetrainable
n+k
onlinenetworktopredictthecleanzˆ∗,andzˆ . Thetargetnetworkusesthecleanerzˆ obtainedbya
0 0 n
k-stepODESolverΦ,suchasDDIM[43]topredictthezˆ−. Notethatzˆ andzˆ− areobtainedby
0 0 0
theconsistencyfunctionf(·,·)inEq.(2). Sincetheclassifier-freeguidance(CFG)isessentialtothe
conditioncontrollingfordiffusionmodels,weintegrateCFGintothelatentconsistencydistillation,
zˆ ←z +(1+w)Φ(z ,t ,t ,c)−wΦ(z ,t ,t ,∅), (6)
n n+k n+k n+k n n+k n+k n
where c is the text condition and w denotes the guidance scale. To ensure the self-consistency
propertydefinedinEq.(1),theconsistencydistillationlossisdesignedas,
L (Θ,Θ−)=E[d(f (z ,t ),f (zˆ ,t ))], (7)
LCD Θ n+k n+k Θ− n n
where d(·,·) is a distance measuring function, such as L2 loss or Huber loss [17]. As discussed
inSec.3.1,theparametersofthetargetnetworkareupdatedwiththeexponentialmovingaverage
(EMA)oftheparametersoftheonlinenetwork. Herewedefinethe“teachernetwork”asthepre-
trainedmotionlatentdiffusionmodel,e.g.,MLD[7]. Accordingly,theonlineandtargetnetworksare
initializedwiththeparametersoftheteachernetwork.
Duringtheinferencephase,ourMotionLCMcansamplehigh-qualitymotionsinonlyonestepto
produceandachievethefastestruntime(∼30mspermotionsequence)comparedtoothermotion
diffusionmodels,whichareshowninFig.4.
3.3 ControllableMotionGenerationinLatentSpace
Afteraddressingthelow-efficiencyissueinthemotionlatentdiffusionmodel,wedelveintoanother
exploration of real-time motion controlling. Inspired by the great success of ControlNet [59]
in controllable image generation, we introduce a motion ControlNet on MotionLCM to utilize
the trajectory of joint(s) given by users to control the motion generation in MotionLCM. In our
MotionLCM,weinitializethemotionControlNetwithatrainablecopyofMotionLCM.Specifically,
eachlayerinthemotionControlNetisappendedwithazero-initializedlinearlayerforeliminating
randomnoiseintheinitialtrainingsteps.
AsshowninFig.3(b),thetrajectoryisdefinedastheglobalabsolutepositionsofthecontrolling
joint(s)following[54]. Inourcontrollingpipeline,wedesignaTrajectoryEncoderconsistingof
5stackedtransformer[49]layerstoencodetrajectorysignals. Weappendaglobaltoken(i.e.,[CLS])
beforethestartofthetrajectorysequenceastheoutputvectoroftheencoder,whichisthenaddedto
thenoisyz andfedintothemotionControlNet. Forsimplicity,weomittheinputsoftextcondition
n
candtimestept(botharefedintothefrozenMotionLCMandthetrainablemotionControlNet).
UndertheguidanceofmotionControlNet,MotionLCMpredictsthedenoisedzˆ . Thereconstructed
0
latentcanbeoptimizedbythereconstructionloss,
L (Θa,Θb)=E[d(zˆ ,z )], (8)
recon 0 0
whereΘaandΘbaretheparametersofthemotionControlNetandTrajectoryEncoder. However,
duringtraining,thesolereconstructionsupervisioninthelatentspaceisinsufficient,whichisalso
verifiedinourmotioncontrolexperimentsinTab.5. Wearguethisisbecausethecontrollablemotion
generationrequiresmoredetailedconstraints,whichcannotbeeffectivelyprovidedsolelybythe
reconstructionlossinthelatentspace.UnlikepreviousmethodslikeOmniControl[54],whichdirectly
diffuse in motion space, allowing explicit supervision of control signals, effectively supervising
controlsignalsinthelatentspaceisnon-trivial. Therefore,weutilizethefrozenVAE[20]decoder
D(·)todecodezˆ intothemotionspace,obtainingthepredictedmotionxˆ ,therebyintroducingthe
0 0
controllinglossasfollows,
(cid:34)(cid:80) (cid:80)
m ||R(xˆ ) −R(x )
||2(cid:35)
L (Θa,Θb)=E i j ij 0 ij 0 ij 2 , (9)
control (cid:80) (cid:80) m
i j ij
whereR(·)convertsthejoint’slocalpositionstoglobalabsolutelocations,m ∈{0,1}isthebinary
ij
jointmaskatframeiforthejointj. ThenweoptimizetheparametersinmotionControlNetΘaand
TrajectoryEncoderΘbwithanoverallobjective,
Θa,Θb =argmin(L +λL ), (10)
recon control
Θa,Θb
whereλistheweighttobalancethetwolosses. Thisdesignenablesexplicitcontrolsignalstodirectly
influencethegenerationprocess,similartocontrollingotherlatent-freediffusionmodelsformotion
generation[18]. Comprehensiveexperimentsdemonstratethatintroducedsupervisionisveryhelpful
inimprovingthequalityofcontrol,whichwillbeintroducedinthefollowingsection.
4 Experiments
Inthissection,wefirstpresenttheexperimentalsetupdetailsinSec.4.1. Subsequently,weprovide
quantitativeandqualitativecomparisonstoevaluatetheeffectivenessofourproposedMotionLCM
framework in Sec. 4.2 and Sec. 4.3. Finally, we conduct comprehensive ablation studies on Mo-
tionLCMinSec.4.4. Theseexperimentsdemonstratetheeffectivenessoftheproposedmethod.
4.1 Experimentalsetup
Datasets. OurexperimentsareconductedonthepopularHumanML3D[13]datasetwhichoffers
anextensivecollectionofhumanmotions,featuring14,616uniquehumanmotionsequencesfrom
AMASS[33]andHumanAct12[15],pairedwith44,970textualdescriptions. Forafaircomparison
withpreviousmethods[13],wetaketheredundantmotionrepresentation,includingrootvelocity,
rootheight,localjointpositions,velocities,rotationsinrootspace,andthefootcontactbinarylabels.
Evaluationmetrics. Weextendtheevaluationmetricsofpreviousworks[13,54,7]. (1)Timecosts:
wereporttheAverageInferenceTimeperSentence(AITS)[7]toevaluatetheinferenceefficiency
ofmodels. (2)Motionquality: FrechetInceptionDistance(FID)isadoptedasaprincipalmetric
toevaluatethefeaturedistributionsbetweenthegeneratedandrealmotions. Thefeatureextractor
employedisfrom[13]. (3)Motiondiversity: MultiModality(MModality)measuresthegeneration
diversityconditionedonthesametextandDiversitycalculatesvariancethroughfeatures[13]. (4)
Conditionmatching: Following[13],wecalculatethemotion-retrievalprecision(R-Precision)to
reportthetext-motionTop1/2/3matchingaccuracyandMultimodalDistance(MMDist)calculates
thedistancebetweenmotionsandtexts. (5)Controlerror: Trajectoryerror(Traj. err.) quantifies
theproportionsofunsuccessfultrajectories,characterizedbyanyjointlocationerrorsurpassinga
predeterminedthreshold. Locationerror(Loc. err.) representstheproportionofjointlocationsthat
6R-Precision↑
Methods AITS↓ FID↓ MMDist↓ Diversity→ MModality↑
Top1 Top2 Top3
Real - 0.511±.003 0.703±.003 0.797±.002 0.002±.000 2.794±.008 9.503±.065 -
Seq2Seq[38] - 0.180±.002 0.300±.002 0.396±.002 11.75±.035 5.529±.007 6.223±.061 -
LJ2P[2] - 0.246±.001 0.387±.002 0.486±.002 11.02±.046 5.296±.008 7.676±.058 -
T2G[4] - 0.165±.001 0.267±.002 0.345±.002 7.664±.030 6.030±.008 6.409±.071 -
Hier[11] - 0.301±.002 0.425±.002 0.552±.004 6.532±.024 5.012±.018 8.332±.042 -
TEMOS[36] 0.017 0.424±.002 0.612±.002 0.722±.002 3.734±.028 3.703±.008 8.973±.071 0.368±.018
T2M[13] 0.038 0.457±.002 0.639±.003 0.740±.003 1.067±.002 3.340±.008 9.188±.002 2.090±.083
MDM[47] 24.74 0.320±.005 0.498±.004 0.611±.007 0.544±.044 5.566±.027 9.559±.086 2.799±.072
MotionDiffuse[60] 14.74 0.491±.001 0.681±0.001 0.782±.001 0.630±.001 3.113±.001 9.410±.049 1.553±.042
MLD[7] 0.217 0.481±.003 0.673±.003 0.772±.002 0.473±.013 3.196±.010 9.724±.082 2.413±.079
MLD∗[7] 0.225 0.504±.002 0.698±.003 0.796±.002 0.450±.011 3.052±.009 9.634±.064 2.267±.082
MotionLCM(1-Step) 0.030 0.502±.003 0.701±.002 0.803±.002 0.467±.012 3.022±.009 9.631±.066 2.172±.082
MotionLCM(2-Step) 0.035 0.505±.003 0.705±.002 0.805±.002 0.368±.011 2.986±.008 9.640±.052 2.187±.094
MotionLCM(4-Step) 0.043 0.502±.003 0.698±.002 0.798±.002 0.304±.012 3.012±.007 9.607±.066 2.259±.092
Table1:Comparisonoftext-conditionalmotionsynthesisonHumanML3D[13]dataset. Wecompute
suggestedmetricsfollowing[13]. Werepeattheevaluation20timesforeachmetricandreportthe
averagewitha95%confidenceinterval. “→”indicatesthattheclosertotherealdata, thebetter.
Boldandunderlineindicatethebestandthesecondbestresult. “∗”denotesthereproducedversion
ofMLD[7]. TheperformanceofourMotionLCMinone-stepinference(30ms)surpassesallstate-
of-the-artmodels,providingampleevidencefortheeffectivenessoflatentconsistencydistillation.
arenotreachedwithinaspecifiedthresholddistance. Averageerror(Avg. err.) denotesthemean
distancebetweenthejointpositionsinthegeneratedmotionandthegivencontroltrajectories.
Implementationdetails. OurbaselinemotiondiffusionmodelisbasedonMLD[7]. Wereproduce
MLDwithhigherperformance. Unlessotherwisespecified,allourexperimentsareconductedonthis
model. ForMotionLCM,weemployanAdamW[19]optimizerfor96Kiterationsusingacosine
decaylearningrateschedulerand1Kiterationsoflinearwarm-up. Abatchsizeof256andalearning
rateof2e-4areused. Wesetthetrainingguidancescalerange[w ,w ]=[5,15]withthetesting
min max
guidance scale as 7.5, and adopt the EMA rate µ = 0.95 by default. We use DDIM-Solver [43]
withskippingstepk =20andchoosetheHuber[17]lossasthedistancemeasuringfunctiond. For
motionControlNet,weemployanAdamW[19]foroptimizerforlongertrainingof192Kiterations
with1Kiterationsoflinearwarm-up. Thebatchsizeandlearningratearesetas128and1e-4. The
learning rate scheduler is the same as the first stage. We follow [54] to use the global absolute
locationsofthecontroljointtotrainmotionControlNet. Forthetrainingobjective,weemployd
astheL2lossandsetthecontrollossweightλto1.0bydefault. Weimplementourmodelusing
PyTorch[34]withtrainingononeNVIDIARTX4090GPU.WeuseoneTeslaV100GPUtoalign
thetext-to-motioninferenceexperimentsettingsinMLD[7]. Forcontrollablemotiongeneration,all
modelsareevaluatedfortheirinferencespeedononeNVIDIARTX4090GPU.
4.2 ComparisonsonText-to-motion
Inthefollowingpart,wefirstevaluateourMotionLCMonthetext-to-motion(T2M)task.Wecompare
ourmethodwithsomeT2MbaselinesonHumanML3D[13]withsuggestedmetrics[13]underthe
95%confidenceintervalfrom20timesrunning. AsMotionLCMisbasedonMLD,wemainlyfocus
ontheperformancecomparedwithMLD.Forevaluatingtimeefficiency,wecomparetheAverage
InferenceTimeperSentence(AITS)withTEMOS[36],T2M[13],MDM[47],MotionDiffuse[60]
andMLD[7]. TheresultsareborrowedfromMLD[7]. Thedeterministicmethods[38,2,11,4],
areunabletoproducediverseresultsfromasingleinputandthusweleavetheirMModalitymetrics
empty.Forthequantitativeresults,asshowninTab.1,ourMotionLCMboastsanimpressivereal-time
runtimeefficiency,averagingaround30mspermotionsequenceduringinference. Thisperformance
exceeds that of previous diffusion-based methods [47, 60, 7] and even surpasses MLD [7] by an
orderofmagnitude. Furthermore,despiteemployingonlyone-stepinference,ourMotionLCMcan
approximateorevensurpasstheperformanceofMLD[7](DDIM[43]50steps). Withtwo-step
inference,weachievethebestR-PrecisionandMMDistmetrics,whileincreasingthesamplingsteps
tofouryieldsthebestFID.Theaboveresultsdemonstratetheeffectivenessoflatentconsistency
7Real MotionLCM (Ours) MLD MDM
“a person walks clockwisein a large curve while swinging their arms.”
0.033s 0.201s 22.58s
“a person walking like a bird and then sniffing the air.”
0.031s 0.192s 21.16s
“a person slightly bent over with right hand pressing against the air walks forward slowly”
0.031s 0.197s 23.44s
Figure4: Qualitativecomparisonofthestate-of-the-artmethodsintext-to-motiontask. Weprovide
thevisualizedmotionresultsandrealreferencesfromthreetextprompts.Withonlyone-stepinference,
MotionLCMachievesthefastestmotiongenerationwhileproducinghigh-qualitymovementsthat
closelymatchthetextualdescriptions.
AITS↓ FID↓ R-Precision↑ Diversity→ Traj.err.↓ Loc.err.↓ Avg.err.↓
Methods
Top3 (50cm) (50cm)
Real - 0.002 0.797 9.503 0.000 0.000 0.000
MDM[47] 18.84 0.698 0.602 9.197 0.4022 0.3076 0.5959
PriorMDM[47] 18.84 0.475 0.583 9.156 0.3457 0.2132 0.4417
GMD∗[18] 126.10 0.576 0.665 9.206 0.0931 0.0321 0.1439
OmniControl∗[54] 72.88 0.218 0.687 9.422 0.0387 0.0096 0.0338
OmniControl[54] 37.80 0.212 0.678 9.773 0.3041 0.1873 0.3226
MoitonLCM 0.034(↑550×) 0.531 0.752 9.253 0.1887 0.0769 0.1897
Table2: Quantitativeresultsofcomparisonwithstate-of-the-artmethodsonHumanML3D[13]test
set. “→”meansclosertorealdataisbetter. “∗”meansthemodelusingguideddiffusion[9]. All
modelsaretrainedonpelviscontrol.
distillation. Forthequalitativeresults,asshowninFig.4,MotionLCMnotonlyacceleratesmotion
generationtoreal-timespeedsbutalsodelivershigh-qualityoutputs,aligningwithtextualdescriptions.
4.3 ComparisonsonControllableMotionGeneration
AsshowninTab.2,wecompareourMotionLCMwith[41,18,54]andreportmetricsfrom[54,7].
DuetoGMD[18]andOmniControl[18]utilizinghigh-costguided-diffusion[9],wereimplement
OmniControlwithoutusingguideddiffusionforafaircomparison. TheresultsindicatethatMo-
tionLCMperformswellinmotioncontrolandachievesstate-of-the-artperformanceintext-motion
alignment,asdemonstratedbyR-Precision. Evenwithoutguideddiffusion,inferenceforasingle
motion in OmniControl [54] still takes around 37 seconds, which is unacceptable for real-time
applications. Incontrast,MotionLCMonlyrequires∼0.03seconds,achievingaspeedupofnearly
↑1100×overOmniControland↑550×overthePriorMDM(thefastestmethod).
8R-Precision↑
Methods FID↓ MMDist↓ Diversity→ MModality↑
Top1
Real 0.511±.003 0.002±.000 2.794±.008 9.503±.065 -
MotionLCM(w∈[5,15]) 0.502±.003 0.467±.012 3.022±.009 9.631±.066 2.172±.082
MotionLCM(w∈[2,18]) 0.497±.003 0.481±.009 3.030±.010 9.644±.073 2.226±.091
MotionLCM(w=7.5) 0.486±.002 0.479±.009 3.094±.009 9.610±.072 2.320±.097
MotionLCM(µ=0.95) 0.502±.003 0.467±.012 3.022±.009 9.631±.066 2.172±.082
MotionLCM(µ=0.50) 0.498±.003 0.478±.009 3.022±.010 9.655±.071 2.188±.087
MotionLCM(µ=0) 0.499±.003 0.505±.008 3.018±.009 9.706±.070 2.123±.085
MotionLCM(k=50) 0.488±.003 0.547±.011 3.096±.010 9.511±.074 2.324±.091
MotionLCM(k=20) 0.502±.003 0.467±.012 3.022±.009 9.631±.066 2.172±.082
MotionLCM(k=10) 0.497±.003 0.449±.009 3.017±.010 9.693±.075 2.133±.086
MotionLCM(k=5) 0.488±.003 0.438±.009 3.044±.009 9.647±.074 2.147±.083
MotionLCM(k=1) 0.442±.002 0.635±.011 3.255±.008 9.384±.080 2.146±.075
MotionLCM(w/Huber) 0.502±.003 0.467±.012 3.022±.009 9.631±.066 2.172±.082
MotionLCM(w/L2) 0.486±.002 0.622±.010 3.114±.009 9.573±.069 2.218±.086
Table3:Ablationstudyondifferenttrainingguidancescalerange[w ,w ],EMArateµ,skipping
min max
intervalk andlosstype. WeusemetricsinTab.1andadoptaone-stepinferencesettingwiththe
testingCFGscaleof7.5forfaircomparison.
R-Precision(Top3)↑ FID↓
Methods
1-Step 2-Step 4-Step 1-Step 2-Step 4-Step
DDIM[43] 0.337±.002 0.375±.002 0.460±.003 4.022±.043 2.802±.038 0.966±.018
DPM[29] 0.337±.002 0.374±.002 0.477±.002 4.022±.043 2.798±.038 0.727±.015
DPM++[30] 0.337±.002 0.375±.002 0.478±.003 4.022±.043 2.798±.038 0.684±.015
MotionLCM 0.803±.002 0.805±.002 0.798±.002 0.467±.012 0.368±.011 0.304±.012
Table4: QuantitativeresultswiththetestingCFGscalew =7.5. MotionLCMnotablyoutperforms
baselinemethodsonHumanML3D[13]dataset,demonstratingtheeffectivenessoflatentconsistency
distillation. Boldindicatesthebestresult.
4.4 AblationStudies
ImpactofthehyperparametersoftrainingMotionLCM.Weconductacomprehensiveanalysisof
thetraininghyperparametersofMotionLCM,includingthetrainingguidancescalerange[w ,w ],
min max
EMArateµ,skippingintervalk,andthelosstype. Wesummarizetheevaluationresultsbasedon
one-stepinferenceinTab.3. Wefindoutthatusingadynamictrainingguidancescaleduringtraining
leadstoanimprovementinmodelperformancecomparedtousingastatictrainingguidancescale,
suchasw =7.5. Additionally,anexcessivelylargerangeforthetrainingguidancescalecanalso
negativelyimpacttheperformanceofthemodel(e.g.,w ∈[2,18]). RegardingtheEMArateµ,we
observethatthelargerthevalueofµ,thebettertheperformanceofthemodel,whichindicatesthat
maintainingaslowerupdaterateforthetargetnetworkΘ−helpsimprovetheperformanceoflatent
consistencydistillation. Whentheskippingintervalkcontinuestoincrease,theperformanceofthe
distillationmodelimprovescontinuously,butlargervaluesofk(e.g.,k =50)mayresultininferior
results. Asforthelosstype,theHuberloss[17]significantlyoutperformstheL2loss,demonstrating
thesuperiorrobustnessofthemethod.
ComparisontootherODESolvers. Tovalidatetheeffectivenessoflatentconsistencydistillation,
we compare three ODE solvers (DDIM [43], DPM [29], DPM++ [30]). The quantitative results
inTab.4demonstratethatourMotionLCMnotablyoutperformsbaselinemethods. Moreover,unlike
DDIM[43], DPM[29], andDPM++[30], requiringmorepeakmemorypersamplingstepwhen
usingCFG,MotionLCMonlyrequiresoneforwardpass,savingbothtimeandmemorycosts.
9AITS FID↓ R-Precision↑ Diversity→ Traj.err.↓ Loc.err.↓ Avg.err.↓
Methods
Top3 (50cm) (50cm)
Real - 0.002 0.797 9.503 0.000 0.000 0.000
MLD[7](λ=0) 0.447 0.784 0.722 9.371 0.3666 0.2150 0.3937
MLD[7](λ=1) 0.447 0.829 0.744 9.221 0.3139 0.1710 0.2893
MotionLCM(λ=0) 0.034 0.325 0.763 9.164 0.2693 0.1354 0.2805
MoitonLCM(λ=1) 0.034 0.531 0.752 9.253 0.1887 0.0769 0.1897
Table5:QuantitativeexperimentalresultsofmotioncontrolforMLD[7]andMotionLCM.Thelatent
generatedbyMotionLCMismorebeneficialfortrainingmotionControlNetcomparedtoMLD[7].
Thetwomodelsaretrainedonpelviscontrol.
TheeffectivenessofMotioLCMlatentformotioncontrol. Toverifytheeffectivenessofthelatent
generatedbyourMotionLCMcomparedtoMLDfortrainingmotionControlNet,weconductedthe
followingtwosetsofexperiments: onewithλ = 0,meaningsupervisiononlyinthelatentspace,
andtheotherwithλ=1,indicatingadditionalsupervisioninthemotionspacewithacontrolloss
weightof1. WepresenttheexperimentalresultsinTab.5. Itcanbeobservedthatunderthesame
experimentalsettings,MotionLCMmaintainshigherfidelityandsignificantlyoutperformsMLDin
motioncontrolperformance.Furthermore,intermsofinferencespeed,MLDutilizesDDIM[43]with
50steps,whileMotionLCMonlyrequiresone-stepinference,resultingina∼↑13×speedup. This
demonstratestheeffectivenessofthelatentgeneratedbyMotionLCMfortrainingmotionControlNet.
5 ConclusionandLimitation
Conclusion.Thisworkproposesanefficientcontrollablemotiongenerationframework,MotionLCM.
Followingthecorepipelineofthepreviouscontrollablemotiongenerationframework,MotionLCM
isbasedonamotionlatentdiffusionmodel. Byintroducinglatentconsistencydistillation,MotionLM
enjoys the trade-off between runtime efficiency and generation quality. Moreover, thanks to the
motionControlNetmanipulationinthelatentspace,ourmethodenjoysgoodcontrollingabilitywith
givenconditions. Extensiveexperimentsshowtheeffectivenessofourmethodandkeydesigns.
Limitationandfuturework. AlthoughMotionLCMenjoysagoodtrade-offbetweengeneration
qualityandefficiencyinthetext-to-motiontask. However,forthemotioncontroltask,OmniCon-
trol[54]andGMD[18],whichutilizeguideddiffusion[9],stilloutperformMotionLCMsignificantly
intermsofperformance. Besides,wedonotresolvethephysicalimplausibleproblemofgenerated
motionandlearnmotiondistributionfromnoisyoranomalousdata[5,52]. Weleavetheseissuesas
ourfuturework. Exceptforthese,wewillalsofocusonautomaticallyannotatinghigh-qualitytexts
formotions,targetingbuildingaclosed-loopofbi-directionaltext-motionsynthesis,whichisquite
essentialtoalargerdatascale.
Acknowledgement
TheauthorteamwouldliketoacknowledgeShunlinLu(TheChineseUniversityofHongKong,
Shenzhen),YimingXie(NortheasternUniversity),ZhiyangDou(TheUniversityofHongKong),and
JiahaoCui(SunYat-senUniversity)fortheirhelpfultechnicaldiscussionandsuggestions.
10References
[1] Ahn,H.,Ha,T.,Choi,Y.,Yoo,H.,Oh,S.: Text2action: Generativeadversarialsynthesisfrom
languagetoaction.In: ICRA.pp.5915–5920(2018)
[2] Ahuja,C.,Morency,L.P.: Language2pose: Naturallanguagegroundedposeforecasting.In:
3DV.pp.719–728(2019)
[3] Athanasiou,N.,Petrovich,M.,Black,M.J.,Varol,G.: Teach: Temporalactioncompositionfor
3dhumans.In: 3DV.pp.414–423(2022)
[4] Bhattacharya, U., Rewkowski, N., Banerjee, A., Guhan, P., Bera, A., Manocha, D.:
Text2gestures: A transformer-based network for generating emotive body gestures for vir-
tualagents.In: VR.pp.1–10(2021)
[5] Chen,L.H.,Li,H.,Zhang,W.,Huang,J.,Ma,X.,Cui,J.,Li,N.,Yoo,J.: Anomman: Detect
anomaliesonmulti-viewattributednetworks.InformationSciences628,1–21(2023)
[6] Chen,L.H.,Zhang,J.,Li,Y.,Pang,Y.,Xia,X.,Liu,T.:Humanmac:Maskedmotioncompletion
forhumanmotionprediction.In: ICCV.pp.9544–9555(2023)
[7] Chen,X.,Jiang,B.,Liu,W.,Huang,Z.,Fu,B.,Chen,T.,Yu,G.: Executingyourcommandsvia
motiondiffusioninlatentspace.In: CVPR.pp.18000–18010(2023)
[8] Dabral,R.,Mughal,M.H.,Golyanik,V.,Theobalt,C.: Mofusion: Aframeworkfordenoising-
diffusion-basedmotionsynthesis.In: CVPR.pp.9760–9770(2023)
[9] Dhariwal,P.,Nichol,A.: Diffusionmodelsbeatgansonimagesynthesis.NeurIPSpp.8780–
8794(2021)
[10] Dou,Z.,Chen,X.,Fan,Q.,Komura,T.,Wang,W.: C·ase: Learningconditionaladversarial
skillembeddingsforphysics-basedcharacters.In: SIGGRAPHAsia.pp.1–11(2023)
[11] Ghosh, A., Cheema, N., Oguz, C., Theobalt, C., Slusallek, P.: Synthesis of compositional
animationsfromtextualdescriptions.In: ICCV.pp.1396–1406(2021)
[12] Guo,C.,Mu,Y.,Javed,M.G.,Wang,S.,Cheng,L.: Momask: Generativemaskedmodelingof
3dhumanmotions.In: CVPR(2024)
[13] Guo,C.,Zou,S.,Zuo,X.,Wang,S.,Ji,W.,Li,X.,Cheng,L.: Generatingdiverseandnatural
3dhumanmotionsfromtext.In: CVPR.pp.5152–5161(2022)
[14] Guo, C., Zuo, X., Wang, S., Cheng, L.: Tm2t: Stochastic and tokenized modeling for the
reciprocalgenerationof3dhumanmotionsandtexts.In: ECCV.pp.580–597(2022)
[15] Guo,C.,Zuo,X.,Wang,S.,Zou,S.,Sun,Q.,Deng,A.,Gong,M.,Cheng,L.: Action2motion:
Conditionedgenerationof3dhumanmotions.In: ACMMM.pp.2021–2029(2020)
[16] Ho,J.,Salimans,T.: Classifier-freediffusionguidance.arXivpreprintarXiv:2207.12598(2022)
[17] Huber,P.J.: Robustestimationofalocationparameter.TheAnnalsofMathematicalStatistics
35(1),73–101(1964)
[18] Karunratanakul,K.,Preechakul,K.,Suwajanakorn,S.,Tang,S.: Guidedmotiondiffusionfor
controllablehumanmotionsynthesis.In: ICCV.pp.2151–2162(2023)
[19] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980(2014)
[20] Kingma,D.P.,Welling,M.: Auto-encodingvariationalbayes.arXivpreprintarXiv:1312.6114
(2013)
[21] Lee,T.,Moon,G.,Lee,K.M.: Multiact: Long-term3dhumanmotiongenerationfrommultiple
actionlabels.In: AAAI.pp.1231–1239(2023)
[22] Li,B.,Zhao,Y.,Zhelun,S.,Sheng,L.: Danceformer: Musicconditioned3ddancegeneration
withparametricmotiontransformer.In: AAAI.pp.1272–1279(2022)
[23] Li,P.,Aberman,K.,Zhang,Z.,Hanocka,R.,Sorkine-Hornung,O.: Ganimator: Neuralmotion
synthesisfromasinglesequence.TOG41(4),1–12(2022)
[24] Li,R.,Zhang,Y.,Zhang,Y.,Zhang,H.,Guo,J.,Zhang,Y.,Liu,Y.,Li,X.: Lodge: Acoarseto
finediffusionnetworkforlongdancegenerationguidedbythecharacteristicdanceprimitives.
In: CVPR(2024)
11[25] Li,R.,Zhao,J.,Zhang,Y.,Su,M.,Ren,Z.,Zhang,H.,Tang,Y.,Li,X.: Finedance: Afine-
grainedchoreographydatasetfor3dfullbodydancegeneration.In: ICCV.pp.10234–10243
(2023)
[26] Li,R.,Yang,S.,Ross,D.A.,Kanazawa,A.: Aichoreographer: Musicconditioned3ddance
generationwithaist++.In: ICCV.pp.13401–13412(2021)
[27] Lin,X.,Amer,M.R.: Humanmotionmodelingusingdvgans.arXivpreprintarXiv:1804.10652
(2018)
[28] Liu, J., Dai, W., Wang, C., Cheng, Y., Tang, Y., Tong, X.: Plan, posture and go: Towards
open-worldtext-to-motiongeneration.arXivpreprintarXiv:2312.14828(2023)
[29] Lu,C.,Zhou,Y.,Bao,F.,Chen,J.,Li,C.,Zhu,J.: Dpm-solver: Afastodesolverfordiffusion
probabilisticmodelsamplinginaround10steps.NeurIPSpp.5775–5787(2022)
[30] Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., Zhu, J.: Dpm-solver++: Fast solver for guided
samplingofdiffusionprobabilisticmodels.arXivpreprintarXiv:2211.01095(2022)
[31] Lu, S., Chen, L.H., Zeng, A., Lin, J., Zhang, R., Zhang, L., Shum, H.Y.: Humantomato:
Text-alignedwhole-bodymotiongeneration.arXivpreprintarXiv:2310.12978(2023)
[32] Luo,S.,Tan,Y.,Huang,L.,Li,J.,Zhao,H.: Latentconsistencymodels: Synthesizinghigh-
resolutionimageswithfew-stepinference.arXivpreprintarXiv:2310.04378(2023)
[33] Mahmood, N., Ghorbani, N., Troje, N.F., Pons-Moll, G., Black, M.J.: Amass: Archive of
motioncaptureassurfaceshapes.In: ICCV.pp.5442–5451(2019)
[34] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,
Gimelshein,N.,Antiga,L.,etal.:Pytorch:Animperativestyle,high-performancedeeplearning
library.NeurIPS32(2019)
[35] Petrovich, M., Black, M.J., Varol, G.: Action-conditioned3dhumanmotionsynthesiswith
transformervae.In: ICCV.pp.10985–10995(2021)
[36] Petrovich,M.,Black,M.J.,Varol,G.: Temos: Generatingdiversehumanmotionsfromtextual
descriptions.In: ECCV.pp.480–497(2022)
[37] Plappert,M.,Mandery,C.,Asfour,T.: Thekitmotion-languagedataset.Bigdata4(4),236–252
(2016)
[38] Plappert, M., Mandery, C., Asfour, T.: Learning a bidirectional mapping between human
whole-bodymotionandnaturallanguageusingdeeprecurrentneuralnetworks.Roboticsand
AutonomousSystems109,13–26(2018)
[39] Raab, S., Leibovitch, I., Li, P., Aberman, K., Sorkine-Hornung, O., Cohen-Or, D.: Modi:
Unconditionalmotionsynthesisfromdiversedata.In: CVPR.pp.13873–13883(2023)
[40] Rombach,R.,Blattmann,A.,Lorenz,D.,Esser,P.,Ommer,B.:High-resolutionimagesynthesis
withlatentdiffusionmodels.In: CVPR.pp.10684–10695(2022)
[41] Shafir,Y.,Tevet,G.,Kapon,R.,Bermano,A.H.: Humanmotiondiffusionasagenerativeprior.
In: ICLR(2023)
[42] Siyao,L.,Yu,W.,Gu,T.,Lin,C.,Wang,Q.,Qian,C.,Loy,C.C.,Liu,Z.: Bailando: 3ddance
generationbyactor-criticgptwithchoreographicmemory.In: ProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition.pp.11050–11059(2022)
[43] Song,J.,Meng,C.,Ermon,S.: Denoisingdiffusionimplicitmodels.In: ICLR(2021)
[44] Song,Y.,Dhariwal,P.,Chen,M.,Sutskever,I.: Consistencymodels.In: ICML(2023)
[45] Tang,Y.,Liu,J.,Liu,A.,Yang,B.,Dai,W.,Rao,Y.,Lu,J.,Zhou,J.,Li,X.: Flag3d: A3d
fitnessactivitydatasetwithlanguageinstruction.In: CVPR.pp.22106–22117(2023)
[46] Tevet,G.,Gordon,B.,Hertz,A.,Bermano,A.H.,Cohen-Or,D.: Motionclip: Exposinghuman
motiongenerationtoclipspace.In: ECCV.pp.358–374(2022)
[47] Tevet, G., Raab, S., Gordon, B., Shafir, Y., Cohen-Or, D., Bermano, A.H.: Human motion
diffusionmodel.In: ICLR(2022)
[48] Tseng,J.,Castellon,R.,Liu,K.: Edge: Editabledancegenerationfrommusic.In: CVPR.pp.
448–458(2023)
12[49] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł.,
Polosukhin,I.: Attentionisallyouneed.NeurIPS(2017)
[50] Wan,W.,Dou,Z.,Komura,T.,Wang,W.,Jayaraman,D.,Liu,L.: Tlcontrol: Trajectoryand
languagecontrolforhumanmotionsynthesis.arXivpreprintarXiv:2311.17135(2023)
[51] Wang,Z.,Chen,Y.,Liu,T.,Zhu,Y.,Liang,W.,Huang,S.: Humanise: Language-conditioned
humanmotiongenerationin3dscenes.NeurIPSpp.14959–14971(2022)
[52] Xia,X.,Liu,T.,Wang,N.,Han,B.,Gong,C.,Niu,G.,Sugiyama,M.: Areanchorpointsreally
indispensableinlabel-noiselearning? NeurIPS32(2019)
[53] Xiao,Z.,Wang,T.,Wang,J.,Cao,J.,Zhang,W.,Dai,B.,Lin,D.,Pang,J.:Unifiedhuman-scene
interactionviapromptedchain-of-contacts.In: ICLR(2024)
[54] Xie,Y.,Jampani,V.,Zhong,L.,Sun,D.,Jiang,H.: Omnicontrol: Controlanyjointatanytime
forhumanmotiongeneration.In: ICLR(2023)
[55] Xu,L.,Song,Z.,Wang,D.,Su,J.,Fang,Z.,Ding,C.,Gan,W.,Yan,Y.,Jin,X.,Yang,X.,etal.:
Actformer: A gan-based transformer towards general action-conditioned 3d human motion
generation.In: ICCV.pp.2228–2238(2023)
[56] Xu,S.,Li,Z.,Wang,Y.X.,Gui,L.Y.: Interdiff: Generating3dhuman-objectinteractionswith
physics-informeddiffusion.In: ICCV.pp.14928–14940(2023)
[57] Yan,S.,Li,Z.,Xiong,Y.,Yan,H.,Lin,D.: Convolutionalsequencegenerationforskeleton-
basedactionsynthesis.In: ICCV.pp.4394–4402(2019)
[58] Yuan,Y.,Song,J.,Iqbal,U.,Vahdat,A.,Kautz,J.: Physdiff: Physics-guidedhumanmotion
diffusionmodel.In: ICCV(2023)
[59] Zhang,L.,Rao,A.,Agrawala,M.:Addingconditionalcontroltotext-to-imagediffusionmodels.
In: ICCV.pp.3836–3847(2023)
[60] Zhang,M.,Cai,Z.,Pan,L.,Hong,F.,Guo,X.,Yang,L.,Liu,Z.: Motiondiffuse: Text-driven
humanmotiongenerationwithdiffusionmodel.arXivpreprintarXiv:2208.15001(2022)
[61] Zhang, M., Guo, X., Pan, L., Cai, Z., Hong, F., Li, H., Yang, L., Liu, Z.: Remodiffuse:
Retrieval-augmentedmotiondiffusionmodel.In: ICCV(2023)
[62] Zhang, Y., Black, M.J., Tang, S.: Perpetual motion: Generating unbounded human motion.
arXivpreprintarXiv:2007.13886(2020)
[63] Zhao,R.,Su,H.,Ji,Q.: Bayesianadversarialhumanmotionsynthesis.In: CVPR.pp.6225–
6234(2020)
13AppendixforMotionLCM
Intheappendix,weprovideadditionaldetailsandexperimentsnotincludedinthemaintext.
• AppendixA:Additionalexperiments.
• AppendixB:Detailsoftheevaluationmetrics.
• AppendixC:Supplementaryquantitativeresults.
A AdditionalExperiments
A.1 Impactofthecontrollossweightλ.
As shown in Tab. 6, we conduct ablation studies on the control loss weight λ. We found that as
theweightincreases,thequalityofmotiongeneratedcontinuouslydecreases(asindicatedbyFID),
despite improving motion control performance. To balance motion quality and motion control
performance,wechooseλ=1toreportourfinalresults.
Methods FID↓ R-Precision↑ Diversity→ Traj.err.↓ Loc.err.↓ Avg.err.↓
Top3 (50cm) (50cm)
Real 0.002 0.797 9.503 0.000 0.000 0.000
MotionLCM(λ=0.0) 0.325 0.763 9.164 0.2693 0.1354 0.2805
MotionLCM(λ=0.1) 0.370 0.771 9.096 0.2318 0.1113 0.2367
MotionLCM(λ=1.0) 0.531 0.752 9.253 0.1887 0.0769 0.1897
MotionLCM(λ=2.0) 0.807 0.748 8.926 0.1482 0.0579 0.1741
MotionLCM(λ=5.0) 1.724 0.725 8.902 0.1541 0.0468 0.1594
Table 6: Ablation studies on the control loss weight λ. We report the results on pelvis control.
Increasingthecontrollossweightcanenhancemotioncontrolperformancebutwoulddecreasethe
motionquality. Thus,weuseλ=1tobalancethetwoaspects.
B MetricDefinitions
Time costs: To assess the inference efficiency of models, we follow [7] to report the Average
Inference Time per Sentence (AITS) measured in seconds. We calculate AITS on the test set of
HumanML3D[13],setthebatchsizeto1,andexcludethetimecostformodelanddatasetloading.
Motionquality: FrechetInceptionDistance(FID)measuresthedistributionaldifferencebetweenthe
generatedandrealmotions,calculatedusingthefeatureextractorassociatedwithaspecificdataset.
Motion diversity: Following [15, 14], we report Diversity and MultiModality to evaluate the
generated motion diversity. Diversity measures the variance of the generated motions across the
wholeset. Specifically,twosubsetsofthesamesizeS arerandomlysampledfromallgenerated
d
motionswiththeirextractedmotionfeaturevectors{v ,...,v }and{v′,...,v′ }. Thediversity
1 Sd 1 Sd
metricisdefinedasfollows,
1
(cid:88)Sd
′
Diversity= ||v −v || . (11)
S i i 2
d
i=1
DifferentfromDiversity,MultiModality(MModality)measureshowmuchthegeneratedmotions
diversifywithineachtextdescription. Specifically,asetoftextdescriptionswithsizeC israndomly
sampledfromalldescriptions. ThenwerandomlysampletwosubsetswiththesamesizeI fromall
generatedmotionsconditionedbyc-thtextdescription,withextractedfeaturevectors{v ,...,v }
c,1 c,I
and{v′ ,...,v′ }. MModalityisformalizedasfollows,
c,1 c,I
C I
1 (cid:88)(cid:88) ′
MModality= ||v −v || . (12)
C×I c,i c,i 2
c=1i=1
14Conditionmatching: [13]providesmotion/textfeatureextractorstogenerategeometricallyclosed
featuresformatchedtext-motionpairsandviceversa. Underthisfeaturespace,evaluatingmotion-
retrievalprecision(R-Precision)involvesmixingthegeneratedmotionwith31mismatchedmotions
andthencalculatingthetext-motionTop-1/2/3matchingaccuracy. MultimodalDistance(MMDist)
calculatesthedistancebetweenthegeneratedmotionandtext.
Controlerror: Following[54], wereportTrajectory, Location, andAverageerrorstoassessthe
motioncontrolperformance. Trajectoryerror(Traj. err.) isdefinedastheproportionsofunsuccessful
trajectories,i.e.,ifthereisajointinthegeneratedmotionthatexceedsacertaindistancethreshold
fromthecorrespondingjointinthegivencontroltrajectory,itisviewedasafailedtrajectory. Similar
totheTrajectoryerror,Locationerror(Loc. err.) isdefinedastheproportionofunsuccessfuljoints.
Inourexperiments,weadopt50cmasthedistancethresholdtocalculatetheTrajectoryerrorand
Locationerror. Averageerror(Avg. err.) denotesthemeandistancebetweenthejointpositionsinthe
generatedmotionandthegivencontroltrajectories.
C MoreQualitativeResults
In this section, we provide more qualitative results of our model. Fig. 5 illustrates the model’s
generationresultsonthetaskoftext-to-motion. Allvideoscanbefoundontheprojectpage.
“a person “a person is doing “the man is throwing “a person runs forward
does a jump” jumping jacks” his right hand” and stops short.”
“the person is doing “a person waves “this person bends “the person is
a dance move.” both arms in the air.” forwardas if to bow.” jogging around.”
“with arms out to the sides “a hunched individual slowly “a man walks forward in a
a person walks forward”wobblesforward in a drunken manner.” snake like pattern.”
Figure5: MorequalitativeresultsofMotionLCMonthetaskoftext-to-motion.
15