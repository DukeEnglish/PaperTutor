Fairness Without Demographics in Human-Centered Federated Learning
ShailyRoy∗, HarshitSharma∗, AsifSalekin
SyracuseUniversity
{sroy15,hsharm04,asalekin}@syr.edu
Abstract FL facilitates model training across multiple edge devices
while maintaining data localization, thereby respecting user
Federated learning (FL) enables collaborative privacy [7] and mitigating risks associated with centralized
modeltrainingwhilepreservingdataprivacy,mak-
datarepositories.Thisalignswithregulatoryconstraintssuch
ing it suitable for decentralized human-centered
astheEuropeanGeneralDataProtectionRegulation(GDPR),
AI applications. However, a significant research
California Consumer Privacy Act (CCPA), or the Health In-
gap remains in ensuring fairness in these systems. surancePortabilityandAccountabilityAct(HIPAA)[8].FL’s
Current fairness strategies in FL require knowl- suitability for practical human-centered AI applications has
edge of bias-creating/sensitive attributes, clashing
ledtoitsadoptioninvariousdomains,includinghealthmoni-
withFL’sprivacyprinciples. Moreover,inhuman- toring[9],productivitytracking[10],andactivityrecognition
centered datasets, sensitive attributes may remain [11]viawearabledevicesandsmartphones.
latent. To tackle these challenges, we present a
Recent literature has drawn attention to the inadequate
novel bias mitigation approach inspired by “Fair-
consideration of fairness in human-centered AI and sensing
ness without Demographics" in machine learning.
methodologies [12; 13; 14]. Notably, FL presents unique
The presented approach achieves fairness without
challenges in achieving fairness. The decentralized nature
needing knowledge of sensitive attributes by min-
of data can result in varying data distributions across de-
imizing the top eigenvalue of the Hessian matrix
vices, potentially amplifying biases [15]. State-of-the-art
duringtraining,ensuringequitablelosslandscapes
FLfairnessapproachesencounterasignificantchallengedue
across FL participants. Notably, we introduce a
to the inherent trade-off between fairness and privacy [16].
novelFLaggregationschemethatpromotespartic-
Theseapproachesoftennecessitateaccesstoindividuals’sen-
ipating models based on error rates and loss land-
sitive information, whether at the client or server level, thus
scapecurvatureattributes,fosteringfairnessacross
contradicting FL’s privacy-preserving nature [17; 18; 15; 7;
the FL system. This work represents the first ap-
19]. Consequently, there arises a pressing need to enhance
proach to attaining “Fairness without Demograph-
fairnessinFLwithoutexplicitlyrequiringknowledgeofsen-
ics" in human-centered FL. Through comprehen-
sitiveorbias-creatingattributes.
sive evaluation, our approach demonstrates effec-
Moreover,human-centeredFLmayinadvertentlyperpetu-
tiveness in balancing fairness and efficacy across
ate biased treatment of factors that are not explicitly recog-
variousreal-worldapplications,FLsetups,andsce-
nizedasrequiringfairnessprotectionduringmodeltraining.
narios involving single and multiple bias-inducing
For example, indoor sensing using Wi-Fi channel state in-
factors, representing a significant advancement in
formation (CSI) is affected by environmental variables like
human-centeredFL.
building materials. In FL-based Wi-Fi CSI activity detec-
tion training, bias may arise if most users reside in concrete
1 Introduction buildings. Sincebuildingmaterialinformationisabsentfrom
mostdatasets,addressingsuchbiaseswithoutexplicitknowl-
The integration of advanced machine learning (ML) algo-
edge of the influencing attribute, such as building materials,
rithms with ubiquitous sensing technologies has revolution-
isessential,aconcepttermed“FairnesswithoutDemograph-
izedhuman-centeredcomputing[1;2],enablingeverydayde-
ics" [20]. Moreover, conventional human-centered sensing
vices,likesmartphonesandwearablesincollectingrichsen-
datasetstypicallylackindividuals’sensitiveinformation,fur-
sorydata,fuelingmyriadAI-drivenapplicationsfromgesture
ther underscoring the necessity of such fairness in human-
recognition[3]tostressdetection[4].
centeredFL[12;21;14].
Edge and distributed computing have emerged as pivotal
paradigmsinthislandscape[5],withfederatedlearning(FL) While recent literature has explored “Fairness without
gainingprominenceasadecentralizedtrainingapproach[6]. Demographics" in centralized AI training [20; 22; 23], its
compatibility with the decentralized FL paradigm has been
∗Theseauthorscontributedequallytothiswork. deemed challenging due to the absence of knowledge re-
4202
rpA
03
]GL.sc[
1v52791.4042:viXragardingglobaldatadistributionsduringthelocaledgemodel lth client. This study focuses on the challenging scenario
training as discussed in Section 3. To our knowledge, this where no central server can access client data or where no
paperisthefirsttotacklethe“FairnesswithoutDemograph- pre-existingdatasetsareavailabletoserveasaproxyforthe
ics"challengewithinthecontextofhuman-centeredFL.The overallclientdatadistribution.
contributionsofthepaperare: Assumekbias-creatingfactors,i.e.,sensitiveattributesex-
ist, and each has k discrete value, k,k ∈ N. E.g., for
• Since centralized-based Fairness without Demographics b b
the sensitive attribute ‘sex,’ there can be two discrete values
methodologies are incompatible with FL due to non-
k ∈ {0,1} corresponding to male (0) or female (1). Each
shareabletraininginstances,disjointmodels,andlossland- b
data instance x conveys traits of its membership to one of
scapes,thepaperaddressesthechallengebyintroducinga t
thecategoriesofeachsensitiveattribute. Forexample,ifsex,
novel approach, named Hessian-Aware Federated Learn-
age, and sensor locations are the k = 3 sensitive attributes,
ing (HA-FL). HA-FL leverages recent literature on loss-
an individual’s sensor data sample conveys the traits of the
landscape curvature attributes, specifically sharpness, to
promotefairness[24;25]. Unlikepriorworks,HA-FLnot respective user’s sensitive attributes’ categories, i.e., the in-
dividual’s respective age, sex, and location, concerning the
onlyutilizesloss-landscapesharpnesstoensurefairnessin
sensor. Thispaper’sfairness-attaininggoalis,foreachofthe
locally trained models but also introduces a local models’
sensitiveattributesk,themodel’sperformanceacrossthedis-
sharpnessawareaggregationphase, attaininganimproved
tributions, i.e., groups Dk := {(x ,y ) : x ∈ jth-category
fairness and efficacy balance. HA-FL offers a robust al- j i i i
ternativetotraditionalbiasmitigationschemesinFL,pre- of k}kb are as close as possible. E.g., for the sensitive at-
j=1
servesprivacy, andaddresseschallengesposedbyhuman- tribute ‘sex,’ there are two distributions, i.e., groups: Dsex
male
centereddata,requiringnoknowledgeofthebias-creating andDsex ,andthemodel’sperformanceacrossthemneeds
female
attributethatrequiresfairnessprotection. tobeequitable. Thismeansthatoverall, theMLmodelper-
formance will be equitable among all individuals participat-
• In human-centered FL, it’s typical for a single individ-
ingintheFLsystem,irrespectiveoftheirmembershipinthe
ual to use an edge platform like a smartphone. However,
categories of the sensitive attributes. Most importantly, this
state-of-the-art(SOTA)FLapproachesthatachievefairness
study’s challenge is that the k sensitive attributes and their
byleveragingknownsensitiveinformationencounterchal-
k categoriesareunknownduringtraining, i.e., the‘fairness
lengesinsuchscenarios. Nonetheless,HA-FLexhibitseq- b
withoutdemographics’setting.
uitableperformanceacrossFLsystemswithsingleindivid-
uals, multiple-personsetups, oracombinationofbothon-
edgeplatforms(referredtoasclientsinFLterms),thereby 3 BackgroundandRelatedWork
enhancingitspracticalityinreal-worldhuman-centeredAI
Fairness in ML has gained significant attention [26; 27;
applications(Section6.2,RQ3).
28], initially focusing on removing or obfuscating sensitive
• Human-centered data may have multiple sensitive at- attributesinrawdatabeforetraining[28],butresultinginsub-
tributes at the same time, creating bias. Where SOTA FL optimal solutions [7]. Recent studies explore manipulating
approachesthatachievefairnessbyleveragingknownsen- thelossfunctiontoenforcefairnessconstraintsduringmodel
sitiveinformationfacechallengesinimprovingfairnessfor training[26]. However,thesestrategiesfacechallengesinFL
multiplesensitiveattributessimultaneously,HA-FLexcels duetoprivacyconstraints[29;15].
in attaining fairness for all, making it highly impactful. Recentresearchhasattemptedtoaddressbiasmitigationin
(Section6.2,RQ4) FLby)byincorporatingfairnessconstraintsattheclientside
• We demonstrate the effectiveness of HA-FL using three
[17;18].However,achievingfairnesssolelyontheclientside
real-world human sensing datasets, each involving dis- doesnotensureoverallfairness[19;15]. Newerapproaches
tinct sensing technologies, applications, and sensitive at- propose bias mitigation during aggregation [15; 19; 30; 7]
tributeinformation.Ourcomprehensiveassessmentsestab- orahybridapproachcombiningclient-sideandaggregation-
lish HA-FL’s superior performance in enhancing fairness side bias mitigation [8; 31; 32; 33]. Further details on them
and efficacy balance in human-centered AI, even without are in Appendix B.1. Notably, HA-FL, follows the hybrid
knowingthebias-creatingfactors. framework.
RecentliteratureoutsideofFLexploresfairnessstrategies
A detailed background on FL and Fairness is provided in
withoutexplicitlyconsideringsensitiveattributeinformation,
AppendixA,andcommonBiasesinHuman-centereddataare
knownas‘FairnesswithoutDemographics’[20;23;34].One
discussedinAppendixB.4.
approach [20] identifies and boosts instances that are rela-
tivelylessaddressedwithoutconsideringsensitiveattributes.
2 ProblemStatement
However, in FL, each client operates with its own training
IntheFLcontext,let’sconsiderascenariowithq edgeplat- instances, making it challenging to identify such instances
forms participating in the decentralized training. Such edge and apply this approach directly. Another approach [23;
platforms are termed ‘clients’, where each may possess one 34]involvesknowledgedistillationtoenhancefairnessstart-
or more individuals’ data. The overall FL system com- ingfrompre-trainedbiasedmodels. However,inFL,models
prises n individuals, with the t’th individual’s data denoted arenotcentrallyshared, hinderingtheapplicationofknowl-
as P = {x ,y }n where x and y are data instances and edge distillation approaches. Additionally, another recent
t t t t=1
correspondinglabels, andP ∈ Q , whereQ isthedatafor study [35] introduced an approach requiring users’ private
t l linformation as model input, conflicting with FL’s privacy-
preservingnature. FurtherdetailsareinAppendixB.2.
Hence, none of these approaches serve as true baselines
forFL,andachieving‘FairnesswithoutDemographics’inFL
remainsanunaddressedresearchquestion.
4 MotivationbehindtheApproach
Figure2:Disparityinthetopeigen-valueλ(H )betweengroup
client
fordifferentFLapproaches.
Figure1:sex-wisedisparityfordifferentFLapproaches
First, this section presents a preliminary study investigat-
ingbiasamongFLclientsbasedonvariousfactors/attributes.
WeutilizedrecentFLapproacheswithoutfairness-awareness
orconstraints,suchasFedAvg,FedSAM,andFedSWA,em-
ploying an off-the-shelf gesture recognition model on the
WIDARdataset[3]. OuranalysisrevealedbiasinF1scores
basedonsexacrossallapproaches,asdepictedinFigure1.
As outlined in Section 3, current Fairness without De-
Figure3:Approachoverview
mographics methodologies are incompatible with FL due
to the distinct characteristics of FL systems, including
non-shareable training instances, disjoint models, and loss 5 Approach
landscapes. To address this challenge, this paper draws Figure 3 provides an overview of HA-FL. At each round of
upon recent literature examining the impact of loss curva- theFLprocess,everyclientutilizesitsownindividuals’data
ture attributes, specifically its sharpness during model train- to train local models, aiming to optimize accuracy and en-
ing/optimizationonfairness[24;25].
sure fairness. Subsequent to local training in each round,
The maximum eigenvalue of the Hessian matrix H, asso- the server aggregates the individual client models, promot-
ciatedwiththeclassificationlossduringmodeltraining,indi- ingincreasedfairnessandaccuracy,guidedbyourSharpness-
catesthesteepestdirectiononthelosssurface[36]. Ahigher awareaggregationstrategy. Theaggregatedglobalmodelis
top eigenvalue implies a sharper minimum in the loss land- then redistributed to the clients for the next round of the FL
scape, while a lower top eigenvalue suggests a flatter mini- process. The following sections discuss the design choices
mum,oftenpreferredforthestabilityandgeneralizabilityof anddetailsoftheHA-FLapproach.
neural networks [32]. Recent literature suggests that mini-
mizing disparity in the top eigenvalue of H across distribu- 5.1 DesignChoices
tions/groups of sensitive attributes Dk mitigates unfairness Ourdesignchoicesforclient-sidelocaltrainingandaggrega-
j
duetothesefactors. tionstepstoensurefairnesswithoutdemographicsarebelow.
ThissectioninvestigateswhetherFLtrainingconformsto
FairLocalTrainingObjective
the literature discussed above. Figure 2 displays the top
In the FL context, each client learns a model, denoted as
eigenvalues for the sensitive attribute ‘sex’ across two dis-
tributions, Dsex andDsex , forthreeabove-discussednon- f client. Wehavetwoobjectivesonclientside: (1)maximizing
male female accuracyand(2)attainingfairnessindecisionmaking.
fairness-awareFLapproachesusingtheWIDARdataset.The
Weintroducearegularizationtermintheclient’slossfunc-
visualization exposes a disparity in the top eigenvalue be-
tion to promote fairness at the client level. This term is de-
tweenthetwodistributionsforallFLapproaches,indicating
rivedfromtheHessianmatrixH oftheclient’sclassifica-
that the relationship between fairness and loss landscape at- lclient
tion loss function l . The learning objective at the client
tributesobservedintheliteratureextendstoFLsystems. client
levelcanbeexpressedasfollows:
This observation inspired the development of our loss-
landscape sharpness aware HA-FL strategy, which utilizes
the top eigenvalue of the Hessian to attain flatter and equi- argmin Loss client =α×l client(f client(x i))
table loss-landscapes for all local client models, leading to 1 (1)
+(1−α)× λ(H )
achievingFairnesswithoutDemographics. N lclient correct
correctHere,themaximizationofaccuracyisattainedbyminimiz- noted as G0, the number of clients participating in FL pro-
ingtheloss: l (f (x )). cess represented by N, the total number of rounds for the
client client i
Theregularizationtermpenalizesthetopeigenvalueofthe FLprocessindicatedbyR,thenumberofepochsEforcon-
Hessianmatrix: λ(H ) ,associatedwiththelossfor ducting local training on each client’s side, the cycle length
lclient correct
correctly classified samples. This term is informed by prior c for implementing the stochastic weight averaging (SWA)
research[24],indicatingadirectrelationshipbetweenthetop protocol[32],theweightingfactorαusedtobalancetheclient
eigenvalue of the Hessian matrix and the maximum uncer- loss,andthebatchsizeButilizedbytheclientsduringtrain-
taintyinpredictionorproximitytothedecisionboundaryfor ing. DifferentmodulesofHA-FLaredetailedbelow:
various input samples x . By decreasing the top eigenvalue
i
foraccuratelyclassifiedsamples,weenhancecertaintyacross Algorithm1Hessian-awareFederatedLearning(HA-FL)
allcorrectlyclassifiedsamples,indirectlymitigatingpotential
Input: ⋄ G0: Initial random model, ⋄ N: Number of
disparitiesamongthem. Therelationisexplainedindetailin
Clients, ⋄ R:Totalrounds, ⋄ E:localtrainingepochs, ⋄ c:
AppendixC.
cyclelength, ⋄ η: SWAstartingthreshold, ⋄ α: clientloss
Weintroduceα,ahyper-parameterthatactsasabalancing
weightingfactor, ⋄ B: Clientbatchsize
factorbetweenthetwoobjectives. αisidentifiedusingagrid
Parameter: ⋄ L[]: Array to store classification loss of
search. We will use λ(H ) instead of λ(H ) for
client lclient correct clients, ⋄ T[]: ArraytostoretopeigenvaluesofclientsHes-
simplicityfortherestofthesection.
sian, ⋄ S: Softmaxoperator
Recent literature shows that only attaining fairness at the
Returns:G :Globalmodel
client’slevelresultsinsub-optimalperformanceglobally[19; SWA
37]. The next section focuses on the measures taken at the 1: foreachroundr=0toR-1do
aggregationstagetoaddressthechallenge. 2: ifr =η×R then
3: G ←Gr
SWA
Sharpness-awareAggregationtoPromoteFairness 4: endif
Aftercompletinglocaltraining,eachclient’smodelpossesses 5: ifn≥η×Rthen
its own accuracy and maximum uncertainty in predictions, 6: γ =γ(r)
as depicted in Figure 3. Consequently, not all client models 7: endif
should equally contribute to deriving the aggregated global 8: foreachClient n=0toN-1do
n
modeltoachievetheabove-discusseddualobjective. 9: params←Gr,Client n,E,B,α,γ
To tackle this, the HA-FL approach’s Sharpness-aware 10: fr,lossr,λr ←TrainClient(params)
n n n
Aggregation prioritizes clients with (1) low error rates and 11: L[n]←ϵ+ 1
lossr
(2)lowermaximaluncertaintytopromotefairness. n
12: T[k]←ϵ+ 1
Followingtheweightedaggregationscheme[6]forderiv- λr
n
ingtheFLglobalmodel,theHA-FLapproachcomputesthe 13: endfor
aggregated global model G for a specific round r with total 14: Wr =S(S(L)·S(T))
N clientsbyfollowing: 15: Gr+1 =(cid:80)N βr ·fr ∀βr ∈Wr
n=1 n n n
16: ifr ≥η×Randmod(r,c)=0then
(cid:88)N 17: n models ←n/c
Gr+1 = n=1β nr ·f nr, whereβ nr ∈Wr (2) 11 98 :: endG
S ifWA
← nmo nd mel o× dG elS sW +A 1+Gr
Here, f nr represents the model of the client n, Wr = 20: endfor
{β nr |∀n∈{1,...,N}}denotesthesetofweightingfactors 21: return G
SWA
for clients. To calculate the weights, at each round r, the
server collects each nth client’s classification error losseval
n
and the top eigenvalue λe nval calculated from its respective LocalClientTraining:
evaluationsetasaproxyofmaximumuncertainty. Combin- FollowingthestandardFLprotocol[6],theprocessbeginsat
ingthisinformation,theweightingfactorsarecalculatedfol-
the server level by distributing a randomly initialized global
lowingtheequation: modelG0andothertrainingparameterstotheclientsforlocal
trainingthroughtheTrainClientmodule(line9-10).
Wr =S(S(L)·S(T)) (3)
TrainClient Module, as shown in Algorithm 2, receives
Here, we use the Softmax operator denoted as S for theglobalmodel, alongwithtrainingconfigurationssuchas
(cid:110) (cid:111)N thenumberoflocaltrainingepochsE,batchsizeB,learning
simplicity. Notably, L = ϵ+ 1 and T =
losseval rateγ,andtheclientlossweightingfactorαfromtheserver.
n n=1
(cid:110) (cid:111)N On the client side, data is divided into a training set D
ϵ+ 1 andϵisasmallconstantaddedtopreventdi- train
λeval andanevaluationsetD . ThemodelistrainedonD
n n=1 eval train
visionbyzeroduringweightcalculation. (line-7), where we utilized the Sharpness-Aware Minimiza-
tion(SAM)optimizerformodeltraining[38]. Thechoiceof
5.2 Hessian-awareFederatedLearningApproach
themodeloptimizerisinfluencedbypriorworksinFL,which
Algorithm 1 outlines HA-FL approach, which begins with claimthatusingSAMoptimizerattheclientlevelhelpswith
thefollowinginputs: arandomlyinitializedglobalmodelde- convergenceandpromoteslocalflatness[32]toachievebettergeneralizabilityfortheclients. Algorithm2TrainClient(Gt,Client ,E,B,α,γ)
k
The client-local training follows the dual objective dis-
Parameter: ⋄ Gt:InitialmodelsentfromServerside, ⋄ E:
cussed in Section 5.1, as per equation 1. The model perfor-
local training epochs at Client side, ⋄ Client : Client ID
mance is assessed on the disjoint D (line-9) to identify k
eval ⋄ B: Client Batch size, ⋄ α: client loss weighting factor,
the best-performing model. For this model, the evaluation
⋄ γ: clientlearningrate
losseval_loss n andthetopeigenvalueoftheHessianmatrix Returns:fr,Lossr ,λr
eval_λ arecalculated. Theoptimalmodel’sevaluationloss n eval,n eval,n
andtopn eigenvaluearestoredinLossr andλr ,respec- 1: Initialize: f nr ←Gr {Server-sideglobalmodel}
tively (Lines 10,11). These metricsev aa nl, dn the we ev ia gl, hn ts of the 2: Initialize: l n{ClientClassificationloss}
selectedmodelarethensentbacktotheserver(line-13). 3: Initialize: SAMoptimizer
4: Split: Client ndataD nintoD trainandD eval
Sharpness-awareaggregation: 5: foreachepoche=0toE-1do
Uponreceiptoftheclients’localtrainingreturns,asperAl- 6: foreachbatchi=0toBinD train do
gorithm1,theserverstartsthemodelaggregationphase. Our 7: argmin Loss
n
Sharpness-awareaggregationmethodcalculatesweightsfor 8: endfor
eachclientbasedontheirclassificationlossandthetopeigen- 9: Evaluatef nonD eval get: eval_loss n,eval_λ n
valueoftheHessianmatrixrelatedtotheirloss(lines13,14). 10: Lossr ←eval_loss
eval,n n
Clientswithlowerclassificationlossandlowertopeigenval- 11: λr eval,n ←eval_λ n
uesareassignedhigherweights(lines10,11). 12: endfor
StochasticWeightAveraging(SWA): Ontheserverside, 13: return fr,Lossr ,λr
n eval,n eval,n
we also use SWA [39], a technique shown to enhance gen-
eralization in FL systems [32]. The significance of gen-
eralization in fair models is underscored by research [40;
41]focusingonimprovingtheirrobustnessandabilitytoper- balanced representation in our assessments. Each dataset is
paired with its dedicated neural network model (detailed in
formfairlyacrossunseendata. Thisaddressesthechallenge
of fairness constraint overfitting [41], where models fair on AppendixD.3), adaptedfromoff-the-shelfbenchmarkmod-
elsintheliterature[44;45;46],tailoredforFLclassification
training data may still exhibit unfairness on unseen data. In
tasks. Specifically, we conducted gesture recognition using
human-centered FL, ensuring fairness and robustness across
WiFi signals on WIDAR, human activity recognition using
unseen instances is critical; hence, the adoption of SWA is
accelerometerandgyroscopereadingsforHugadb,andstress
crucial in HA-FL. SWA averages the global models at reg-
detectionbasedonphysiologicalparametersandaccelerom-
ular intervals, i.e., cycles c (line 17), and adjusts the learn-
eterreadingsfortheEDAdataset. Wehavemeticulouslyse-
ing rate throughout the FL process (line 6) to achieve flat-
lectedandtunedmultiplehyperparametersforeachoftheem-
terminima,therebypreventingoverfittingastheprocessad-
ployedmodels,asdetailedinAppendixD.4.
vances. The SWA process begins after a threshold (η) num-
beroftheroundsarecomplete(lines1-4);forourevaluations, EvaluationMetrics: TomeasuretheeffectivenessofFL,
thisthresholdissettoη =20%.Theearlystartthresholdwas weutilizedF1ScoresandAccuracy,whileforassessingfair-
chosenduetothelimiteddatasize,whichallowsforquicker ness,weemployedEqualOpportunityGap(EOgap)metrics,
convergenceinoursetting. inlinewithexistingliterature[19;15]. Furtherdetailscanbe
foundinAppendixA.
6 ExperimentalEvaluation
6.2 EvaluationonHA-FL’sEffectiveness
In this section, we analyze the performance of HA-FL ap-
proachunderdifferentconfigurationsanddatasets.
To establish the effectiveness of HA-FL, we have investi-
6.1 Datasets,Models,andMetrics gatedspecificresearchquestions(RQ)discussedbelow.
Onlyafewhuman-sensingdatasetsencompassdatafromdi-
RQ1: CouldHA-FLimprovethefairnessandefficacy
verse demographic groups. This section performs compre-
balancethanthecontemporarybenign(i.e.,non-fairness
hensive evaluations on three such human-centered datasets:
aware)FLapproaches?
EDA [42] with 48 participants, WIDAR [3] with 16 partici-
pants,andHAR(HugaDB)[43]with18participants(details To investigate, we compare HA-FL with benign FL models
in Appendix D.1). Each dataset comprises known specific (Fedavg,Fedsam,andFedswa)acrossthreedatasets,examin-
bias contributing factors: ‘orientation’ relative to the sensor ingfairnessbyevaluatingtheEOgapbetween‘sex.’ Results
and‘sex’forWIDAR;the‘hand’inwhichthesmartwatchis inTable 1show anotable decreasein theEOgap with HA-
worn and ‘sex’ of the participant for EDA; and ‘sex’ alone FL, indicating enhanced fairness compared to conventional
for HugaDB. Although HA-FL does not require knowledge FLmethods. Whileaperformance-fairnesstradeoffwasob-
ofsensitiveattributesduringtraining,weleveragedtheafore- servedintheWIDARdataset,whichisconsistentwithprior
mentioned factors from each dataset as proxies for all un- literature[37],ourapproachmaintainedaccuracyintheother
knownfactorstoevaluatetheeffectivenessofHA-FL. two datasets, showcasing its improved efficacy and fairness
The data partitioning for training and testing maintains a balance over benign approaches without using sensitive at-
consistent80:20ratioacrosseachclientparticipant,ensuring tributeinformationduringtraining.Dataset Model F1Score Accuracy EOGap Model F1Score Accuracy EOGap
WIDAR Fedavg 0.8648 87.08% 0.4135 Fedavg 0.8616 86.33% 0.4199
Fedsam 0.8896 89.72% 0.4212 Fedsam 0.8799 88.31% 0.4327
Fedswa 0.8703 88.35% 0.3930 Fedswa 0.8694 87.29% 0.4225
HA-FL 0.8012 82.92% 0.3457 HA-FL 0.8542 86.14% 0.3879
EDA Fedavg 0.8064 74.82% 0.3421
Fedsam 0.8280 82.82% 0.2980 Table3: ComparisonofunfairFLandHA-FLModelPerformance
Fedswa 0.8161 82.90% 0.2945 Metrics on WIDAR Datasets for single and multi-person client
HA-FL 0.7946 81.38% 0.2592 distribution.
HAR Fedavg 0.8424 83.78% 0.4630
Fedsam 0.8894 88.60% 0.4178
Fedswa 0.8431 83.66% 0.4142
achievingfairnessevenwithsingle-personclientsbyregulat-
HA-FL 0.8347 82.75% 0.4015
ingthelosslandscapecharacteristicsinsteadofdirectlyopti-
mizing for fairness-loss during training. Table 3 showcases
Table1: PerformanceMetricsonDifferentDatasetsforbenignap-
HA-FL’s performance across the WIDAR dataset, encom-
proachesFedavg,Fedsam,FedswavsProposed(HA-FL)Approach.
passingbothsingleandmulti-persondatawithoutguaranteed
inclusion of all group data instances for sensitive attributes.
Dataset Model F1Score Accuracy EOGap
ClientdistributiondetailsareavailableinAppendixD.2. De-
EO-Fedavg 0.8358 85.21% 0.3725
WIDAR FB-Fedavg 0.8134 82.39% 0.3892 spite a modest 1% accuracy sacrifice compared to Table 1,
HA-FL 0.8012 82.92% 0.3457 HA-FLnotablyreducestheEOGap,demonstratingitseffec-
EO-Fedavg 0.7705 74.57% 0.2574 tiveness in achieving fairness without imposing strict inclu-
EDA FB-Fedavg 0.7611 72.11% 0.2627 sionrequirements.
HA-FL 0.7946 81.38% 0.2592
EO-Fedavg 0.8357 82.89% 0.4178
RQ4: CouldHA-FLimprovefairnessformultiple
HAR FB-Fedavg 0.8315 83.01% 0.3761
sensitiveattributessimultaneously?
HA-FL 0.8347 82.75% 0.4015
InFLapplications, human-centereddatafrequentlyinvolves
Table 2: Comparison of HA-FL with the ’Equalized Odds (EO)
multiple sensitive attributes such as sex, race, age, region,
LosswithFedavg’and’Fairbatch(FB)withFedavg’models
and socioeconomic status. Despite attempts to mitigate bias
in FL, most methods concentrate on addressing one sensi-
RQ2:CouldHA-FLattainfairnesscomparabletothe tiveattributeatatime[19;31],whichpresentschallengesin
existingSOTAknown-fairness-awarestrategiesinFL? comprehensively addressing bias across multiple attributes.
To investigate, we compared HA-FL with SOTA fairness- While some researchers considered the presence of multiple
aware FL methods like ‘Fairbatch with Fedavg’ [47] and sensitive attributes, they struggle to effectively reduce bias
‘Equalized odds loss with Fedavg’ [48]. Details of these for each sensitive attribute [7]. HA-FL does not utilize any
methods are in Appendix B.3. These methods ensure fair- sensitive attribute information during training, thereby im-
ness by considering specific known factors/attributes during proving fairness across multiple sensitive attributes simulta-
FLtraining. However,theformermethod[47]compromises neouslyandaddressesthisresearchgap.
FL’sprivacy-preservingnatureasclientssharesuchinforma- Toassessitseffectiveness,wecomputedfairnessregarding
tion with servers. The evaluation results are shown in Ta- othersensitiveattributesbesides‘sex’: ‘hands’and‘orienta-
ble2,wheretheknownfairnessattainingfactor/attributedur- tion’concerningthesensor,forthesamemodelsevaluatedin
ingtrainingforthetwofairness-awaremethodsis‘sex,’ and Table1fortheEDAandWIDARdatasets,respectively.HAR
theEOgapinrespectto‘sex’iscomputedforfairnesscom- hasonlyonesensitiveattribute,‘sex,’hence,thissectionex-
parisons. HA-FL, without prior knowledge during training, cludes HAR evaluation. The results, summarized in Table
achievescomparablefairness. Notably,observingthelowest 4,demonstratenotableimprovementsinfairnessacrossother
EOgapsforWIDAR,EDA,andHAR(0.34,0.24,and0.37, sensitiveattributesaswell.
respectively),ourapproachmatchesthebestgapsinWIDAR
Notably, the EO gap reduction in the EDA dataset was
andEDAandiscomparabletothebest-performingapproach
achieved with a modest sacrifice of 1% in F1 score than
in the HAR dataset. This outcome represents a significant
the Table 1 results when considering the sensitive attribute
advancement and introduces a more privacy-preserving and
left and right “hand." A similar result is observed for the
adaptablesolutionforfairnessinFL.
WIDAR dataset, showcasing enhanced fairness across mul-
RQ3: CanfairnessbeachievedinFLwithsingleand tipleattributessimultaneously
multipleindividualscomprisingclients? Additionally, there is a discrepancy in F1 scores between
Conventional fairness-enhancing FL methods often require theWIDARdatasetresultsinTables1and4. Thisdifference
clientswithdiversegroupsofindividuals, necessitatingdata stems from deliberate resampling techniques used to induce
instancesfromalltargetedsensitiveattributegroupstoattain datasetimbalanceduringtheevaluationoftheorientationat-
fairness.Forexample,toattainsex-wisefairness,thesemeth- tribute. Theintentionalintroductionofimbalanceservedasa
ods require that each client includes both male and female testtoassessthemodel’scapabilitytohandlescenarioswith
individuals. Notably, HA-FL eliminates this requirement, unevenlydistributedattributes.Dataset Sensitive F1Score Accuracy EOGap Model
Attribute
0.8064 74.82% 0.5714 Fedavg
EDA Hand
0.8016 74.49% 0.4850 Fedsam
0.8089 82.30% 0.4903 Fedswa
0.7946 81.38% 0.4744 HA-FL
0.8641 86.53% 0.4788 Fedavg
WIDAR Orientation
0.9736 97.56% 0.5194 Fedsam
0.8759 88.49% 0.4448 Fedswa
0.8241 84.66% 0.3863 HA-FL
(a) F1ScoreSex-wiseDifference (b) λ(H )Sex-wiseDiffer-
client
Table4:PerformanceMetricscomparisonundersensitiveattributes ence
otherthan‘sex.’
Figure5: ComparisonofSex-wise(a)F1Scoreand(b)λ(H )
client
valuesfortheselectedroundoftheFedavgandtheHA-FLmodel
intheWIDARDataset.
suchknowledge.
6.4 Ablationstudy
We conducted a comprehensive ablation study to meticu-
lously guide the choices in our proposed approach. While
the study is detailed in Appendix E, a brief summary of the
(a) Multi-PersonClientsFLsys-(b) Both Single and Multi-
outcomesisoutlinedbelow:
tem PersonClientsFLsystem
• Achieving fairness solely through client-side fair train-
Figure4:Variationofλ(H )fortheWIDARdatasetonFedavg ing (Algorithm 2), without incorporating the Sharpness-
client
andHA-FLmodelsacrossdifferentclientdistributions. awareAggregation(Algorithm1lines14-15),orviceversa,
yieldssub-optimaloutcomes,underscoringthesignificance
oftheintroducedHA-FL’shybridapproach.
6.3 DiscussiononHA-FL’sEffectiveness
• Thoroughassessmentoftheaggregationstrategyconfirms
WhileSection6.2confirmstheeffectivenessofHA-FL,this that integrating both classification and top eigenvalue loss
section delves into and discusses the underlying reasons be- andfavoringclientswithlowerloss,asdescribedinSection
hinditseffectiveness. AsoutlinedinSections4and5.1,HA- 5.1,enhancesthebalancebetweenefficacyandfairness.
FLfostersfairnessbyreducingthetopeigenvalueoftheHes-
sian matrix (λ(H client)), resulting in flatter loss landscapes 7 BroaderImpactandLimitation
amonglocalclientmodelsandtherebyensuringmoresimilar
This first of its kind paper addresses an important research
λ(H )acrossthem,enablingequitableefficacyoverall.
client gap lies at the intersection of fairness and federated learn-
ToverifywhetherHA-FLindeedupholdsthisassumption,
inginthecontextofhuman-centeredapplicationsandsetsthe
we visualize the λ(H ) among single-person and multi-
client stageforamoreequitableandresponsiblefutureindecentral-
personclientFLsystems,asshowninFigure4.Regardlessof
izedhuman-centeredcomputing,ensuringthatthebenefitsof
the sensitive attributes, HA-FL consistently reduces the top
technologicaladvancementsareaccessibleandfairtoall.
eigenvalue of the Hessian matrix and improves the discrep-
Limitation: Achieving fairness in FL systems requires at-
ancyofλ(H )acrossclients. Thisvisualizationconfirms
client taining a balance between fairness and model performance
thatHA-FLleadstoamoresimilarandflatterlosslandscape
[37]. The quest to ensure fairness for individual clients of-
acrossclients.
tenresultsinreducedperformanceacrosstheboard,anissue
Furthermore, we investigate whether similar and flatter
highlighted in literature [19; 7; 49; 50]. Although HA-FL
loss landscapes, indicated by similar and lower λ(H ),
client demonstrates significant effectiveness in achieving this bal-
indeedpromoteequitableefficacyacrossgroupsordistribu-
ance,asdiscussedinSection6.2,RQ1,itexhibitsminorper-
tions of sensitive attributes. For the aforementioned mod-
formance degradation in the WIDAR dataset. This could be
els,wevisualizetheF1scoresanddifferencesinλ(H )
client attributedtoouraggregationscheme,outlinedinSection5.1
among male and female groups regarding the sensitive at-
Equation 3, which equally weighs low error rates and lower
tribute ‘sex,’ as depicted in Figure 5. In the HA-FL model,
maximaluncertainty,thuslimitingtheabilitytobalanceaccu-
theλ(H )valuesforbothmaleandfemalegroupssignif-
client racyandfairness. Theissuecouldberesolvedbyintroducing
icantly decrease from 0.22 to 0.01 and 0.19 to 0.01, respec-
ahyper-parameterastheweightingfactor.
tively. This reduction and alignment result in identical F1
scoresamongthesegroupsintheHA-FLmodel. Thesevisu-
8 Conclusion
alizations affirm that HA-FL, consistent with contemporary
fairnesstheories,indeedalignswithourassumption,thereby This paper introduces HA-FL, a novel bias mitigation strat-
promoting equitable efficacy across all participants, regard- egyandaninnovativeSharpness-awareaggregationscheme
less of their sensitive attributes, and even in the absence of which addresses the challenge of incorporating fairness intohuman-centered FL without needing knowledge of sensitive
informationorbias-inducingattributestherebyupholdingthe
privacy-preservingtraitofFL.Throughthoroughevaluation,
HA-FLprovesitseffectivenessinstrikingabalancebetween
fairness and performance across a variety of real-world ap-
plications and FL setups, addressing both single and multi-
ple bias factors without the need for knowledge of sensitive
attributes. HA-FL paves the way for equitable and privacy-
preservingdecentralizedhuman-centeredAIlearning,show-
casingitspotentialforsubstantialimpact.References [13] X. Xu, X. Liu, H. Zhang, W. Wang, S. Nepal, Y. Se-
fidgar,W.Seo,K.S.Kuehn,J.F.Huckins,M.E.Morris,
[1] C.Li,Z.Cao,andY.Liu,“Deepaienabledubiquitous
et al., “Globem: Cross-dataset generalization of longi-
wireless sensing: A survey,” ACM Computing Surveys
tudinal human behavior modeling,” Proceedings of the
(CSUR),vol.54,no.2,pp.1–35,2021.
ACMonInteractive, Mobile, WearableandUbiquitous
[2] D. Puccinelli and M. Haenggi, “Wireless sensor net- Technologies,vol.6,no.4,pp.1–34,2023.
works: applicationsandchallengesofubiquitoussens-
[14] H. Zhang, L. Wang, Y. Sheng, X. Xu, J. Mankoff, and
ing,”IEEECircuitsandsystemsmagazine,vol.5,no.3,
A. K. Dey, “A framework for designing fair ubiqui-
pp.19–31,2005.
touscomputingsystems,”inAdjunctProceedingsofthe
[3] Y.Zhang,Y.Zheng,K.Qian,G.Zhang,Y.Liu,C.Wu, 2023ACMInternationalJointConferenceonPervasive
andZ.Yang,“Widar3.0:Zero-effortcross-domainges- and Ubiquitous Computing & the 2023 ACM Interna-
turerecognitionwithwi-fi,”IEEETransactionsonPat- tional Symposium on Wearable Computing, pp. 366–
ternAnalysisandMachineIntelligence,vol.44,no.11, 373,2023.
pp.8671–8688,2021.
[15] Y. Djebrouni, N. Benarba, O. Touat, P. De Rosa,
[4] G.Vos,K.Trinh,Z.Sarnyai,andM.R.Azghadi,“Gen- S. Bouchenak, A. Bonifati, P. Felber, V. Marangozova,
eralizable machine learning for stress monitoring from andV.Schiavoni,“Biasmitigationinfederatedlearning
wearable devices: A systematic literature review,” In- foredgecomputing,”ProceedingsoftheACMonInter-
ternationalJournalofMedicalInformatics, p.105026, active,Mobile,WearableandUbiquitousTechnologies,
2023. vol.7,no.4,pp.1–35,2024.
[5] Z.Zhou,X.Chen,E.Li,L.Zeng,K.Luo,andJ.Zhang, [16] H.Chen,T.Zhu,T.Zhang,W.Zhou,andP.S.Yu,“Pri-
“Edge intelligence: Paving the last mile of artificial vacyandfairnessinfederatedlearning: ontheperspec-
intelligence with edge computing,” Proceedings of the tiveoftrade-off,”ACMComputingSurveys,2023.
IEEE,vol.107,no.8,pp.1738–1762,2019.
[17] Y.Zeng,H.Chen,andK.Lee,“Improvingfairnessvia
[6] B.McMahan,E.Moore,D.Ramage,S.Hampson,and federated learning,” arXiv preprint arXiv:2110.15545,
B. A. y Arcas, “Communication-efficient learning of 2021.
deepnetworksfromdecentralizeddata,”inArtificialin- [18] A. Papadaki, N. Martinez, M. Bertran, G. Sapiro, and
telligenceandstatistics,pp.1273–1282,PMLR,2017.
M. Rodrigues, “Minimax demographic group fairness
[7] D. Y. Zhang, Z. Kou, and D. Wang, “Fairfl: A fair infederatedlearning,”inProceedingsofthe2022ACM
federated learning approach to reducing demographic Conference on Fairness, Accountability, and Trans-
biasinprivacy-sensitiveclassificationmodels,”in2020 parency,pp.142–159,2022.
IEEEInternationalConferenceonBigData(BigData), [19] Y. H. Ezzeldin, S. Yan, C. He, E. Ferrara, and A. S.
pp.1051–1060,IEEE,2020. Avestimehr,“Fairfed: Enablinggroupfairnessinfeder-
[8] A. Abay, Y. Zhou, N. Baracaldo, S. Rajamoni, ated learning,” in Proceedings of the AAAI Conference
E.Chuba,andH.Ludwig,“Mitigatingbiasinfederated onArtificialIntelligence,vol.37,pp.7494–7502,2023.
learning,”arXivpreprintarXiv:2012.02447,2020. [20] P.Lahoti,A.Beutel,J.Chen,K.Lee,F.Prost,N.Thain,
[9] D. A. Epstein, C. Caldeira, M. C. Figueiredo, X. Lu, X. Wang, and E. Chi, “Fairness without demographics
L. M. Silva, L. Williams, J. H. Lee, Q. Li, S. Ahuja, through adversarially reweighted learning,” Advances
Q. Chen, et al., “Mapping and taking stock of the per- in neural information processing systems, vol. 33,
sonal informatics literature,” Proceedings of the ACM pp.728–740,2020.
onInteractive,Mobile,WearableandUbiquitousTech- [21] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and
nologies,vol.4,no.4,pp.1–38,2020. A.Galstyan,“Asurveyonbiasandfairnessinmachine
[10] Y.-H. Kim, J. H. Jeon, E. K. Choe, B. Lee, K. Kim, learning,” ACM computing surveys (CSUR), vol. 54,
no.6,pp.1–35,2021.
andJ.Seo,“Timeaware: Leveragingframingeffectsto
enhance personal productivity,” in Proceedings of the [22] T. Hashimoto, M. Srivastava, H. Namkoong, and
2016CHIConferenceonHumanFactorsinComputing P. Liang, “Fairness without demographics in repeated
Systems,pp.272–283,2016. lossminimization,”inInternationalConferenceonMa-
[11] T. Shaik, X. Tao, N. Higgins, R. Gururajan, Y. Li,
chineLearning,pp.1929–1938,PMLR,2018.
X. Zhou, and U. R. Acharya, “Fedstack: Personalized [23] J. Chai, T. Jang, and X. Wang, “Fairness without de-
activity monitoring using stacked federated learning,” mographics through knowledge distillation,” Advances
Knowledge-BasedSystems,vol.257,p.109929,2022. in Neural Information Processing Systems, vol. 35,
pp.19152–19164,2022.
[12] S.Yfantidou,M.Constantinides,D.Spathis,A.Vakali,
D. Quercia, and F. Kawsar, “Beyond accuracy: A [24] C.Tran,F.Fioretto,J.-E.Kim,andR.Naidu,“Pruning
critical review of fairness in machine learning for has a disparate impact on model accuracy,” Advances
mobile and wearable computing,” arXiv preprint in Neural Information Processing Systems, vol. 35,
arXiv:2303.15585,2023. pp.17652–17664,2022.[25] H.Wang,J.Hong,J.Zhou,andZ.Wang,“Howrobust [39] P. Izmailov, D. Podoprikhin, T. Garipov, D. Vetrov,
isyourfairness? evaluatingandsustainingfairnessun- and A. G. Wilson, “Averaging weights leads to
derunseendistributionshifts,”Transactionsonmachine wideroptimaandbettergeneralization,”arXivpreprint
learningresearch,vol.2023,2023. arXiv:1803.05407,2018.
[26] W. Zhang and E. Ntoutsi, “Faht: an adaptive [40] J. Ferry, U. Aivodji, S. Gambs, M.-J. Huguet, and
fairness-aware decision tree classifier,” arXiv preprint M. Siala, “Improving fairness generalization through a
arXiv:1907.07237,2019. sample-robust optimization method,” Machine Learn-
[27] D. Y. Zhang, Y. Huang, Y. Zhang, and D. Wang, ing,vol.112,no.6,pp.2131–2192,2023.
“Crowd-assisteddisastersceneassessmentwithhuman-
[41] A.Cotter,M.Gupta,H.Jiang,N.Srebro,K.Sridharan,
ai interactive attention,” in Proceedings of the AAAI
S.Wang,B.Woodworth,andS.You,“Trainingfairness-
ConferenceonArtificialIntelligence,vol.34,pp.2717–
constrained classifiers to generalize,” in ICML 2018
2724,2020.
workshop:“fairness, accountability, and transparency
[28] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and inmachinelearning(FAT/ML),2018.
R.Zemel,“Fairnessthroughawareness,”inProceedings
of the 3rd innovations in theoretical computer science [42] Y.Xiao,H.Sharma,Z.Zhang,D.Bergen-Cico,T.Rah-
conference,pp.214–226,2012. man, and A. Salekin, “Reading between the heat:
Co-teaching body thermal signatures for non-intrusive
[29] D. Pessach and E. Shmueli, “A review on fairness in
stress detection,” Proceedings of the ACM on Interac-
machine learning,” ACM Computing Surveys (CSUR),
tive, Mobile, Wearable and Ubiquitous Technologies,
vol.55,no.3,pp.1–44,2022.
vol.7,no.4,pp.1–30,2024.
[30] T. Li, M. Sanjabi, A. Beirami, and V. Smith, “Fair re-
[43] R. Chereshnev and A. Kertész-Farkas, “Hugadb: Hu-
sourceallocationinfederatedlearning,” arXivpreprint
man gait database for activity recognition from wear-
arXiv:1905.10497,2019.
able inertial sensor networks,” in Analysis of Images,
[31] S. Cui, W. Pan, J. Liang, C. Zhang, and F. Wang,
Social Networks and Texts: 6th International Confer-
“Addressing algorithmic disparity and performance in-
ence, AIST 2017, Moscow, Russia, July 27–29, 2017,
consistencyinfederatedlearning,” AdvancesinNeural
RevisedSelectedPapers6,pp.131–141,Springer,2018.
Information Processing Systems, vol. 34, pp. 26091–
26102,2021. [44] J. Yang, X. Chen, D. Wang, H. Zou, C. X. Lu, S. Sun,
and L. Xie, “Sensefi: A library and benchmark on
[32] D. Caldarola, B. Caputo, and M. Ciccone, “Improv-
deep-learning-empowered wifi human sensing,” Pat-
ing generalization in federated learning by seeking flat
terns,vol.4,no.3,2023.
minima,”inEuropeanConferenceonComputerVision,
pp.654–672,Springer,2022. [45] H. Sharma, Y. Xiao, V. Tumanova, and A. Salekin,
[33] S.I.A.Meerza,L.Liu,J.Zhang,andJ.Liu,“Glocalfair: “Psychophysiological arousal in young children who
Jointly improving global and local group fairness in stutter: An interpretable ai approach,” Proceedings of
federated learning,” arXiv preprint arXiv:2401.03562, the ACM on interactive, mobile, wearable and ubiqui-
2024. toustechnologies,vol.6,no.3,pp.1–32,2022.
[34] J. Chai and X. Wang, “Self-supervised fair representa- [46] S.Pirttikangas,K.Fujinami,andT.Nakajima,“Feature
tionlearningwithoutdemographics,”AdvancesinNeu- selection and activity recognition from wearable sen-
ralInformationProcessingSystems,vol.35,pp.27100– sors,” in Ubiquitous Computing Systems: Third Inter-
27113,2022. national Symposium, UCS 2006, Seoul, Korea, Octo-
[35] Q.Liu, H.Jiang, F.Wang, Y.Zhuang, L.Wu, W.Gao, ber11-13,2006.Proceedings3,pp.516–527,Springer,
E.Chen, etal., “Fairlisa: Fairusermodelingwithlim- 2006.
itedsensitiveattributesinformation,”AdvancesinNeu- [47] Y. Roh, K. Lee, S. E. Whang, and C. Suh, “Fairbatch:
ralInformationProcessingSystems,vol.36,2024.
Batch selection for model fairness,” arXiv preprint
[36] S.Jastrze˛bski,Z.Kenton,N.Ballas,A.Fischer,Y.Ben- arXiv:2012.01696,2020.
gio, and A. Storkey, “On the relation between the
[48] A.Fukuchi, Y.Yabe, andM.Sode, “fairtorch: Pytorch
sharpestdirectionsofdnnlossandthesgdsteplength,”
implementationofparitylossasconstraintsfunctionfor
arXivpreprintarXiv:1807.05031,2018.
fairnessinmachinelearning,”2020.
[37] X. Gu, Z. Tianqing, J. Li, T. Zhang, W. Ren, and K.-
K. R. Choo, “Privacy, accuracy, and model fairness [49] M. T. Al Amin, T. Abdelzaher, D. Wang, and B. Szy-
trade-offsinfederatedlearning,”Computers&Security, manski, “Crowd-sensing with polarized sources,” in
vol.122,p.102907,2022. 2014 IEEE International Conference on Distributed
ComputinginSensorSystems,pp.67–74,IEEE,2014.
[38] P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur,
“Sharpness-aware minimization for efficiently improv- [50] D. Wang, B. K. Szymanski, T. Abdelzaher, H. Ji, and
ing generalization,” in International Conference on L. Kaplan, “The age of social sensing,” Computer,
LearningRepresentations,2021. vol.52,no.1,pp.36–45,2019.[51] M.Hall,L.vanderMaaten,L.Gustafson,M.Jones,and of fairness: Exploring the institutional logics of mul-
A.Adcock, “Asystematicstudyofbiasamplification,” tistakeholder microlending recommendation,” in Pro-
arXivpreprintarXiv:2201.11706,2022. ceedingsofthe2023ACMConferenceonFairness,Ac-
countability,andTransparency,pp.1652–1663,2023.
[52] A. Castelnovo, R. Crupi, G. Greco, D. Regoli, I. G.
Penco, and A. C. Cosentini, “A clarification of the nu- [67] G. Hinton, O. Vinyals, and J. Dean, “Distilling
ances in the fairness metrics landscape,” Scientific Re- the knowledge in a neural network,” arXiv preprint
ports,vol.12,no.1,p.4209,2022. arXiv:1503.02531,2015.
[53] T. Räz, “Group fairness: Independence revisited,” in [68] N.Papernot,P.McDaniel,X.Wu,S.Jha,andA.Swami,
Proceedings of the 2021 ACM conference on fairness, “Distillation as a defense to adversarial perturbations
accountability,andtransparency,pp.129–137,2021. againstdeepneuralnetworks,”in2016IEEEsymposium
onsecurityandprivacy(SP),pp.582–597,IEEE,2016.
[54] Y. Djebrouni, “Towards bias mitigation in federated
learning,”in16thEuroSysDoctoralWorkshop,2022. [69] F. Ding, M. Hardt, J. Miller, and L. Schmidt, “Re-
tiring adult: New datasets for fair machine learning,”
[55] L. Chu, L. Wang, Y. Dong, J. Pei, Z. Zhou, and
Advances in neural information processing systems,
Y. Zhang, “Fedfair: Training fair models in cross-
vol.34,pp.6478–6490,2021.
silo federated learning. arxiv 2021,” arXiv preprint
arXiv:2109.05662. [70] S. Yang, P. Luo, C.-C. Loy, and X. Tang, “From facial
parts responses to face detection: A deep learning ap-
[56] M.Mohri,G.Sivek,andA.T.Suresh,“Agnosticfeder-
proach,”inProceedingsoftheIEEEinternationalcon-
atedlearning,”inInternationalConferenceonMachine
ferenceoncomputervision,pp.3676–3684,2015.
Learning,pp.4615–4625,PMLR,2019.
[71] A. Agarwal, A. Beygelzimer, M. Dudík, J. Langford,
[57] T. Huang, W. Lin, L. Shen, K. Li, and A. Y. Zomaya,
andH.Wallach,“Areductionsapproachtofairclassifi-
“Stochastic client selection for federated learning with
cation,” in International conference on machine learn-
volatileclients,”IEEEInternetofThingsJournal,vol.9,
ing,pp.60–69,PMLR,2018.
no.20,pp.20055–20070,2022.
[72] S.Yfantidou,M.Constantinides,D.Spathis,A.Vakali,
[58] T.Song,Y.Tong,andS.Wei,“Profitallocationforfed-
D. Quercia, and F. Kawsar, “The state of algorithmic
erated learning,” in 2019 IEEE International Confer-
fairnessinmobilehuman-computerinteraction,”inPro-
ence on Big Data (Big Data), pp. 2577–2586, IEEE,
ceedings of the 25th International Conference on Mo-
2019.
bileHuman-ComputerInteraction,pp.1–7,2023.
[59] T. Li, S. Hu, A. Beirami, and V. Smith, “Ditto:
[73] B.Bent,B.A.Goldstein,W.A.Kibbe,andJ.P.Dunn,
Fairandrobustfederatedlearningthroughpersonaliza-
“Investigatingsourcesofinaccuracyinwearableoptical
tion,” in International Conference on Machine Learn-
heartratesensors,” NPJdigitalmedicine, vol.3, no.1,
ing,pp.6357–6368,PMLR,2021.
p.18,2020.
[60] W. Du, D. Xu, X. Wu, and H. Tong, “Fairness-aware
[74] K. Bayoumy, M. Gaber, A. Elshafeey, O. Mhaimeed,
agnosticfederatedlearning,”inProceedingsofthe2021
E. H. Dineen, F. A. Marvel, S. S. Martin, E. D. Muse,
SIAMInternationalConferenceonDataMining(SDM),
M. P. Turakhia, K. G. Tarakji, et al., “Smart wearable
pp.181–189,SIAM,2021.
devices in cardiovascular care: where we are and how
[61] J. Angwin, J. Larson, S. Mattu, and L. Kirchner, “Ma- tomoveforward,”NatureReviewsCardiology,vol.18,
chine bias,” in Ethics of data and analytics, pp. 254– no.8,pp.581–599,2021.
264,AuerbachPublications,2022.
[75] P.J.Colvonen,P.N.DeYoung,N.-O.A.Bosompra,and
[62] B. Becker and R. Kohavi, “Adult.” UCI Ma- R. L. Owens, “Limiting racial disparities and bias for
chine Learning Repository, 1996. DOI: wearabledevicesinhealthscienceresearch,”2020.
https://doi.org/10.24432/C5XW20.
[76] F.Nikseresht,R.Yan,R.Lew,Y.Liu,R.M.Sebastian,
[63] L.F.Wightman,“Lsacnationallongitudinalbarpassage and A. Doryab, “Detection of racial bias from physio-
study.lsacresearchreportseries.,”1998. logical responses,” in Advances in Usability, User Ex-
perience,WearableandAssistiveTechnology: Proceed-
[64] S. L. Blodgett, L. Green, and B. O’Connor, “De-
ingsoftheAHFE2021VirtualConferencesonUsabil-
mographic dialectal variation in social media: A
ity and User Experience, Human Factors and Wear-
casestudyofafrican-americanenglish,”arXivpreprint
able Technologies, Human Factors in Virtual Environ-
arXiv:1608.08868,2016.
ments and Game Design, and Human Factors and As-
[65] F.Drago,R.Galbiati,andP.Vertova,“Prisonconditions sistive Technology, July 25-29, 2021, USA, pp. 59–66,
and recidivism,” American law and economics review, Springer,2021.
vol.13,no.1,pp.103–130,2011.
[77] U. Schimmack, “The implicit association test: A
[66] J. J. Smith, A. Buhayh, A. Kathait, P. Ragothaman, method in search of a construct,” Perspectives on Psy-
N. Mattei, R. Burke, and A. Voida, “The many faces chologicalScience,vol.16,no.2,pp.396–414,2021.[78] H. Y. Aldosky, “Impact of obesity and gender differ-
encesonelectrodermalactivities.,”GeneralPhysiology
&Biophysics,vol.38,no.6,2019.
[79] M. A. U. Alam, “Ai-fairness towards activity recogni-
tionofolderadults,”inMobiQuitous2020-17thEAIIn-
ternationalConferenceonMobileandUbiquitousSys-
tems: Computing, Networking and Services, pp. 108–
117,2020.
[80] J. A. Nanzer and R. L. Rogers, “Human presence de-
tectionusingmillimeter-waveradiometry,”IEEETrans-
actionsonMicrowaveTheoryandTechniques, vol.55,
no.12,pp.2727–2733,2007.
[81] A. Sengupta, F. Jin, R. Zhang, and S. Cao, “mm-
pose: Real-time human skeletal posture estimation us-
ing mmwave radars and cnns,” IEEE Sensors Journal,
vol.20,no.17,pp.10032–10044,2020.
[82] Y. S. Park, Y.-S. Shin, J. Kim, and A. Kim, “3d
ego-motion estimation using low-cost mmwave radars
via radar velocity factor for pose-graph slam,” IEEE
Robotics and Automation Letters, vol. 6, no. 4,
pp.7691–7698,2021.
[83] M. Kaur, “Wireless sensor networks: Issues & chal-
lenges,”
[84] D. Sathya, V. Chaithra, S. Adiga, G. Srujana, and
M.Priyanka,“Systematicreviewonon-airhanddoodle
systemforthepurposeofauthentication,”in2023Third
International Conference on Artificial Intelligence and
SmartEnergy(ICAIS),pp.1460–1467,IEEE,2023.
[85] F.Wang,J.Han,S.Zhang,X.He,andD.Huang,“Csi-
net: Unified human body characterization and pose
recognition,”arXivpreprintarXiv:1810.03064,2018.
[86] R. Shahbazian and I. Trubitsyna, “Human sensing by
using radio frequency signals: A survey on occupancy
andactivitydetection,”IEEEAccess,2023.
[87] C. McCarthy, N. Pradhan, C. Redpath, and A. Adler,
“Validationoftheempaticae4wristband,”in2016IEEE
EMBSinternationalstudentconference(ISC),pp.1–4,
IEEE,2016.A Appendix
ThecodeanddataforthisworkareaccessibleviatheGitHubrepository(anonymousgithubrepo). AlsoaPowerPointpresen-
tationsummarizingthepaperisincludedintheGitHubrepository.
A.1 APrimeronFairness
Thisprimerprovidesanoverviewofthealgorithmicfairness[28]inregardtomachinelearningapplicationsandthecommonly
usedmetricforevaluatingfairness. Inmachinelearning(ML),wheremodelslearnfromlargedatasets,anyexistingbiasesin
thedatacanbeamplifiedandperpetuatedbythemodel[51],causingunfairdecision-making.Therearemultipleviewsofmodel
fairnessasperrecentliterature,likeindividual[52]andgroupfairness[53]. IncentralizedML,priorworksonfairnesswithout
demographics[22;20;23]ensuregroupfairness. Groupfairnessisinspiredbytheintuitiveideaofequality[52],meaningthat
groupfairnessisfocusedontheequitabletreatmentofdifferentgroups,oftendefinedbysensitiveattributeslikerace,sex,age,
etc. The scope of this work is achieving group fairness for FL systems, as outlined in Section 2, ‘Problem Statement’ of the
mainpaper. Thenextsectiondiscussesacommonfairnessmetricwehaveemployedtomeasurethefairnessofoursystem.
A.2 CommonFairnessmetric
Therearevariousevaluationmetrics[28;15]thathavebeenusedtoquantifythefairnessofmachinelearningapproaches. This
sectionintroducesonesuchmetriccalledtheEqualopportunitygap,whichisusedforthiswork:
EqualOpportunityGap: Equalopportunitygapisameasureof
∆ =P(Yˆ =1|S =1,Y =1)−P(Yˆ =1|S =0,Y =1)
EO
The equation calculates the difference in the probability of a positive prediction (Yˆ = 1) given a positive true outcome
(Y =1)acrossthedifferentgroupsdefinedbysensitiveattributeS.
PriorworksonfairnessinFL(whichrequireaccesstosensitiveattributes)[19;54;55]haveusedEqualopportunitygapasa
fairnessmetric,whichmotivatedourchoiceforthefairnessevaluationmetricforthiswork.
A.3 APrimeronFederatedLearning
Federatedlearning(FL)[6]representsaparadigmshiftindistributedmachinelearning,focusingontrainingalgorithmsacross
decentralizeddeviceswhilekeepingallthetrainingdatalocal[15;17]. Thismethodisfundamentallydesignedtoaddresspri-
vacyconcernsinherentintraditionalcentralizedmachinelearningapproaches[7]. InFL,aglobalmodelisinitiallydistributed
to all participants (in FL term clients), each of whom possesses a local dataset. These clients independently train the (local)
model on their data and then send only the model parameters (or parameter updates), not the actual data, to a central server.
The central server aggregates these updates to refine the global model and sends it back to the clients. This iterative process
continuesuntilthemodelachievessatisfactoryperformance. Thisapproachensuresthatsensitivedataremainsontheclient’s
localdevice[6],therebysignificantlymitigatingprivacyandsecurityrisks.
Mathematically,FLaimstominimizeaglobalobjectivefunction,whichistypicallyanaggregationoflocalobjectivefunc-
tionscorrespondingtoeachclient’s(participatingentity’s)dataset. Theformulationcanbeexpressedasfollows:
K
1 (cid:88)
F(w)= F (w)
K k
k=1
Here,F(w)denotestheglobalobjectivefunction,wrepresentsthemodelparameters,Kisthenumberofclientsparticipating
inthelearningprocess,andF (w)isthelocalobjectivefunctionforthekth client. Themodelparametersareupdatedbased
k
ontheaggregatedupdatesfromallclients,reflectingthecollaborativenatureoflearninginfederatedsettings.
Thisprivacy-preservingtechnique, coupledwithitsdistributednature, positionsFLasapivotalmethodinthefieldofdis-
tributed(i.e.,de-centralized)machinelearning,particularlyforapplicationsinvolvingsensitiveorpersonaldata.
B Priorworks
B.1 BiasMitigationinFL
BiasinFLsystemshasbeenextensivelystudiedbyresearchers[31;17;15;8]inrecentyears. Broadly,thebiasmitigationfor
FLsystemscanbefollowingtypes:
1. Performance-BasedFairness: Thisapproachfocusesonequitabledecisionsacrossallclientmodels. Itinvolvesmech-
anisms at either the client or server level or both to enhance fairness across all participating entities (in human-centered
AI,acrossallindividualsirrespectiveofwhichclienttheybelongto),particularlybenefitingunder-performinggroupsor
clients[8;17;31;56]. Thisdefinitionoffairnessisinlinewiththefocusofthispaper.2. Contribution Fairness: In traditional FL, due to bandwidth constraints and budget limitations, only a subset of clients
isselectedforeachtraininground, leavingoutsometrainingdata. Onesignificantconsequenceisthepotentialforbias
in the global model. When certain clients are consistently left out of the training process or have limited opportunities
tocontributetheirdata,theirspecificpatternsorcharacteristicsmaybeunderrepresentedorentirelyabsentintheglobal
model. As a result, the global model may not adequately generalize to the data distributions of these excluded clients,
leadingtosubparefficacyforthem. Severalaggregation-timeclientselectionstrategiesaredevelopedtoaddresssuchbias
[57;58],whichareoutofscopeofthispaper.
Our workfocuses on achieving Performance-based Fairness for FLsystems. Thefollowing paragraphs providea detailed
discussionofthis.
Client-sideFairnessApproachesinFL:
Client-sideapproachesputconstraintsonthelocalclient-sidemodeltrainingtoattainfairness. Inclient-basedapproaches,Li
etal. [59]introducedaframeworkfordevelopingdistinctmodelsforeachdeviceaimedatenhancingfairnessandrobustness.
This method addresses data heterogeneity at the client level by incorporating a regularization term in the training process.
Thistermalignstheclientmodelwiththeoptimalglobalmodelateachtraininground,guidingeachclienttowardstheglobal
model iteratively. Zeng et al. [17] introduced FedFB, a method that integrates FL with the Fair Batch technique [47]. This
approach modifies the weight of samples during the client training phase to mitigate imbalances in data distribution among
variousgroups.TheobjectiveofFedFBistopreventthemodelfromdisproportionatelybenefitingonegroupoverothers.Duet
al. [60]ensuredfairnessattheclientlevelbyincorporatingafairnessconstraintasahyperparameterdirectlyintheclientloss
function. Theconstraintensuresthattheclientnotonlyfocusesonachievinghigheraccuracybutalsoonmaintainingfairness
acrossvariousdatadistributions. Thefairnessconstraintactsasaregularizerintheoptimizationprocessandguidesthemodels
toavoidsolutionsthatworkwellonaveragebutperformpoorlyforspecificgroupsordatadistributionsitencountersduring
thedeployment. AgnosticFL[56]alsoworksbyregularizationattheclientlevel. Itusesmin-maxoptimizationtominimize
theworst-caseclientloss. Anotherwork[18]usedthemin-maxoptimizationstrategywhereeachclientcalculatesanempirical
risk factor based on the sensitive attributes at the client level, which is then sent to the server where the server calculates an
importancescorebasedonclientriskinordertoreducetheworst-caserisk.TheFairness-awareAgnosticFL[60]alsoworkson
themin-maxprinciple,wherefairnessisachievedbyincorporatingafairnessconstraintintheclientlossfunction. Thefairness
constraint acts as a regularizer in the optimization process, guiding the model to avoid solutions that would perform well on
averagebutpoorlyforspecificgroupsordistributions.
Notably,allthecurrentstate-of-the-artclient-sidefairnessapproachesinFLneedknowledgeofsensitiveattributes[59;17;
60; 18; 60] or works under the assumption of clients having access to data from all sensitive attributes [15; 56]. Moreover,
accesstosensitiveattributesmightnotbeavailableduetoprivacyandethicalobligations[8]inFL.
Server-sideFairnessApproachesinFL:
Server-sideapproachesfocusontheaggregationstepintheFLprocesstopromotesystem-widefairness. Ezzeldinetal. [19]
proposeanaggregationschemeforFLsystems. Theirapproach,FairFed,whichisagnostictothelocaldebiasingmechanism,
worksbyeachclientperforminglocaldebiasingonitsownlocaldataset,thusmaintainingdatadecentralizationandavoiding
the exchange of any explicit information about its local data composition. To amplify the local debiasing performance, the
clientsevaluatethefairnessoftheglobalmodelontheirlocaldatasetsineachFLroundandcollectivelycollaboratewiththe
servertoadjustitsmodelaggregationweights.However,duringtheirlocaldebiasingmechanismandcalculationofaggregation
weights,FairFedrequiresaccesstoinformationaboutthedatadistribution,suchassensitivegroupmembership. FairFl[7]is
anotherserver-sidefairnessapproachforFLsystemsthatusesmulti-agentreinforcementlearningtooptimizebothfairnessand
accuracy under privacy constraints. They propose a client selection scheme based on team Markov game theory [7]. FairFL
requires the calculation of an estimated Shapley value for different demographic groups, which represents the sensitivity of
themodelaccuracytodifferentdemographicgroupsintheclientdatadistributionsandrequiresaccesstosensitiveinformation
aboutthedata. Morerecently,Djebrouni. etal. [15]proposeAstral,whichisanaggregationapproachthatselectsclientsfor
aggregationweightsusingmutationandcrossoveroperatorsfromtheDifferentialEvolutionalgorithm. Thisprocessgenerates
a diverse range of candidate solutions for aggregation weights, which are then used to compute global aggregated models.
However,duringtheselectionandaggregationofclients,Astralrequiresaccesstoinformationaboutsensitiveattributesofthe
data. Henceconflictingwiththeprivacy-preservingnatureofFL.
Hybrid(Client+Server)FairnessApproachesinFL:
ThereareseveralworksthatprovideahybridbiasmitigationstrategyforFLsystemsthatworksatboththeclientandserver
side.Abayetal.[8]proposedthatthelocalfairtrainingattheclientlevelandlocalmodelre-weightingbasedontheknowledge
oflocalsensitiveattributesimprovesfairnessattheclientlevel. Theyalsoproposeaglobalmodelre-weightingschemeifthe
client shares the data with the server. Cui et al. [31] proposed that each client maintains a fairness budget and local model
weights are updated (by multiplying by factor alpha) such that the worst-case loss is minimized during aggregation. In the
aggregation,theyprioritizetheworst-performingclientsinordertominimizetheworst-caseperformance.
We have compared the efficacy of HA-FL with FedSAM and FedSWA models, which were presented by a recent study
[32] that aims to improve the generalizability of FL systems by promoting optimal clients model by using Sharpness-awareminimization (SAM) at the client level and Stochastic-weight averaging (SWA) at the server level. They analyze the Hes-
sian eigenvalues and show that using this training protocol in FL systems helps improve convergence and generalizability,
specificallyincasesofheterogeneousdatadistributions. Notably,wecomparedHA-FLtothisapproachasitimprovesmodel
performance on different distributions in overall client data by enhancing model generalizability without accessing any sen-
sitive information at the client or server level. During our evaluations, we observed that FedSAM and FedSWA did provide
improvements to the model fairness as well. However, HA-FL outperforms both of these models consistently for all three
datasets
B.2 UnknownDemographicFairness
Thissectiondiscussespriorworksinmachinelearningaimedatenhancingfairnessandreducingbias,particularlyinscenarios
where protected or unknown subgroups are involved. This means that the bias-creating factors or sensitive attributes are
unknown. It highlights various methodologies and their implications in achieving equitable outcomes in diverse data-driven
contexts.
Lahoti et al. [20] explored Rawlsian fairness by leveraging correlations between observed and protected attributes
to safeguard fairness for unidentified protected subgroups. They employed adversarially reweighted learning (ARL) to
minimize the disparity of hypothesized subgroups, enhancing classifier performance in three fairness datasets [61; 62;
63]. ARL uniquely assigns individual weights to each training sample, influencing their impact on the learner’s loss based
oninferredsubgroupmembership.
Hashimotoetal.[22]investigatedtheshortandlong-termeffectsofempiricalriskmanagement(ERM)onfairnessregarding
unknownsubgroups.Theyshowedthatuserattritionfromdisadvantagedgroupsleadstounfairmodels.Tocounteractthis,they
useddistributionallyrobustoptimization(DRO),whichsloweddisadvantageduserattritionandimprovedmodelperformance
post-attrition[64]. However,thisapproachassumesthatuserscanopt-out,limitingitsapplicabilityincertainfairnesscontexts
[65;66].
Chai et al. [23] addressed the fairness issue for unknown protected subgroups differently, using distillation. This method
trains an efficient model from a more complex one, using soft labels to encode class information [67; 68]. Applied to three
fairnessdatasets[69;61;70],thisapproachshowedenhancedaccuracyandfairnesscomparedtoothermethods.
B.3 SOTAknown-fairnessawarestrategies-Approacheswecomparedinthemainpaper
To compare the efficacy of HA-FL with SOTA known-fairness aware strategies (under RQ-2), we adapted an evaluations
framework similar to the prior work by Zheng et al. [17]. Their work modified the standard Fedavg protocol to incorporate
fairness directly into the FL process by introducing fair training mechanisms at the client level, like fairbatch [47]. Along
withfairbatch[47], weincorporateanotherfairtrainingmechanismlikeEqualized-odds(EO)loss[48;71], whichisanother
SOTA framework for centralized fair training. The integration of the SOTA known-fairness mechanisms at the client level
didimprovefairnesscomparedtotheconventionalFedavgbutcameatthecostofprivacyinlinewiththepriorfindings[17].
HA-FL, however, has the advantage of being privacy-preserving and providing better or equivalent fairness benefits without
significantperformancetrade-offs,asillustratedinTable2inthemainpaper.
B.4 CommonBiasesinHuman-centereddata
Theissueofdatabiasandlackofbenchmarkdatasetswithsensitiveattributesisasignificantcauseofconcernforthehuman-
centeredAIandcomputingcommunity[21;14]. ArecentstudybyYfantidouetal. [72]showsthatthereisaresearchgapin
thestudyofbiasinhuman-centeredAIapplications,withmerely5%oftheworksinthedomainmeetingcurrentstandardsfor
fairnessreporting,suchasprovidingdetaileddemographicanalyses. Additionally,recentworkshavefoundthatmeasurement
inaccuraciesandtheconceptdriftphenomenainsensordataleadtoperformancedisparitiesacrossdifferentsensitiveattributes
[12]. Thecommonbiasesassociatedwiththemodalitiesweevaluatedinthisworkinclude:
BiasesinPhysiologicalSensorData: Wearabledeviceslikesmartwatchesusesensorslikephotoplethysmography(PPG)
to measure heart rate and other vital signs, but their accuracy can be influenced by device type, placement, skin tone, and
individualdifferences [73;74;75;45]. Studieshave shownthatphysiologicalsignals, notablyelectrodermalactivity(EDA),
canindicateracialbias[76;77]andareaffectedbyobesityandsex,withsignificantdifferencesinEDAresponsesamongobese
individualsandbetweengenders[78]. Thisunderscoresthecomplexityandpotentialbiasesinphysiologicalstatemonitoring
technologies.
BiasesinInertial/Motionsensordata: Arecentstudy[79]exploredbiasesininertialmeasurementunit(IMU)dataused
foractivityrecognitioninolderadults. Itidentifiedthatbiasesmayariseduetothediverseabilitiesandfunctionaldisabilities
amongolderadults. Thestudyacknowledgesthatthesameactivitycanpresentdifferentlyamongindividualsbasedonageand
functionalability,evenforthesamepersonovertime. Thesevariationscouldleadtobiasedactivityrecognition,highlighting
theimportanceofaddressingAIfairnessinthisdomain.
Biasesinradarsensordata: BiaseshavealsobeeninDopplerradarslikemillimeter-wave(mmWave)sensors,whichare
popularfortaskslikepresencedetection[80]andposeestimation[81]. ThestudybyParketal. [82]investigatedtheuseof
mmWaveradarsfor3Dmotionestimationinrobotics. Theyhighlightedthechallengesandbiasesassociatedwithusingthese
sensors,particularlyinharshorfeature-poorenvironmentslikefoggyconditionsoruneventerrain,anddevelopedamethodtocompensateforthelimitationsandbiasesoftheradarsensors,suchastheirsensitivitytoenvironmentalfactorsandthecasing’s
impactonmeasurementquality.
C RelationshipbetweenFairnessandtheHessian
Tranetal. [24]presentsaformulathatlinksthetopeigenvalueofasensitiveattributesa,denotedasλ(H ),tothebehavior
l,a
ofaclassifierf:
1 (cid:88)
λ(H )≤ (f(x))(1−f(x)) ×∥∇f(x)∥2
l,a |D |
a (cid:124) (cid:123)(cid:122) (cid:125)
(x,y)∈Daclosenesstothedecisionboundary
(4)
+|f(x)−y|×λ(∇2f(x))
(cid:124) (cid:123)(cid:122) (cid:125)
Error
The equation illustrates the upper-bound of top eigenvalue of a group’s Hessian matrix, λ(H ), and its relationship with
l,a
two key factors: (1) the group’s proximity to the decision boundary, and (2) the prediction accuracy for the group. It shows
that λ(H ) increases with the classifier’s prediction uncertainty (f(x) ≈ 0.5) and decreases with high prediction certainty
l,a
(f(x) ≈ 0or1). Toensurefairnessindecision-making,thedisparityinλ(H )acrosssensitivegroupsinthedatasetshould
l,a
beminimized[24]. Intuitively, aclassifierthatexhibitsminimaldisparityinthetopeigenvaluesfordifferentgroups, suchas
malesandfemales,maintainsconsistentpredictioncertaintyregardlessofthegrouptowhichsamplexbelongs.
Intuition behind using correctly classified samples: By only considering the correctly classified samples, the first term
in equation 4 becomes the deciding factor and represents a proxy for closeness to the decision boundary or the maximal
uncertaintyinprediction. Inourapproach,weaimtoreducethemaximaluncertaintyinpredictionsforallcorrectlyclassified
samples,makingthesepredictionsmoreconsistentfordifferentsensitiveattributes. Intuitively,thismeansinstancesbelonging
todifferentsensitiveattributesorgroupswouldbeclosertogetherinthedecisionplaneaswereducethemaximalprediction
uncertaintyofallinstanceswithoutanyknowledgeabouttheirgroupaffiliations.
D DatasetandModelDescription
Thissectionprovidesadetailedoverviewofthedatasetandthemodelsweutilizedinourevaluations.
D.1 DatasetDescription
Widar Dataset: The widar3.0 dataset [3] contains WIFI channel state information (CSI) [83] which represents the variation
in the Wifi channel portrayed by commodity wifi devices [83; 3]. The WIFI CSI contains fine-grained information about
variations in the Wifi channel based on the Doppler phenomenon [3]. These signals have been used by prior researchers for
gesturedetection[3],presencedetection[84],poseestimation[85],humanidentificationandactivitydetection[86].Thedataset
containsdatafrom16participants,4Females,and12Males,withfivedifferentorientations(labeledas1to5). Weremoved
afemaleparticipantfromthedatasetduetotheabsenceofgroundtruth. Theselectedtwogestures,‘Slide’and‘Sweep,’had
datainallorientationsformaleandfemaleparticipantsinthepresenceofthewifisensor. Sinceallparticipantshadanequal
numberofsamplesindifferentorientations,i.e.,orientations1to5,wecreatedadisparityinthedatasetbasedonorientations
by randomly sub-sampling 50% of the data belonging to orientations 4 and 5. This was done to create a training imbalance
representativeofreal-lifescenarioswheresomegroupshavemorerepresentationcomparedtoothers.Weconsideredlabels1to
3asmajororientationandtherestasminororientation. Forourwork,wehavetrainedamodeltoperformbinaryclassification
todetectSweepvs.SlidegesturesusingWIFI-CSIdataandfairnessonthebasisof‘sex’and‘orientation’assensitiveattributes.
HARDataset: WehaveutilizedtheHuGaDB(HumanGaitDatabase)dataset[43]forHAR(HumanActivityRecognition)
purposes,whichcomprisesadiversearrayofhumanactivityrecordingsfrom18participants. Theparticipantgroupincludes4
femalesand14males.Inourwork,wetrainedabinaryclassificationmodeltodistinguishbetweenStandingandotheractivities
fromthisdataset,withfairnessconsiderationsbasedon‘sex’asthesensitiveattribute.
EDADataset: Xiaoetal. [42]collectedthisdataset. Thisdatasetwasobtainedfrom48individuals,including40females
and8males,whoparticipatedinfourdifferentstress-inducingtasks(Arithmetic,BadMemory,StressVideos,Pre-condition),
eachassociatedwithuniquestressors. Thedatasetisnotpubliclyavailableyet;Accesstothedatasethasbeenobtainedthrough
proper IRB approvals/extensions. Due to the continuation of data collection by the Xiao et al. [42]’s research group, this
paper’sreporteddatasethasmoreindividualsthanreportedbytheXiaoetal. [42]paper. Thedatasethasindividualswearing
EmpaticaE4[87](collectingEDAsensingdata)intheirleftand/orrighthandsandindividuals’sexinformation. Hence,this
paperutilized‘hand’informationand‘sex’assensitiveattributesforevaluation. Utilizingthisdataset,weperformedabinary
stressdetectionclassificationtask.
D.2 ClientDistribution:
Distinctclientdistributionsareestablishedforeachdatasetinourstudy.• To benchmark our model against state-of-the-art methodologies that require knowledge of sensitive attributes for model
training, wedeliberatelystructuredtheclientdistributiontoincludeatleastoneparticipantfromeachsensitiveattribute
category. Table 5 provides a detailed overview of participant distribution across clients, categorizing them based on the
sensitiveattributesofmalesandfemales.
• Giventhatourmodelsdonotdependonsensitiveattributeinformationduringtraining,wealsodemonstratetheireffective-
nessbyemulatingtheapproachinclientswitheithersingleand/ormultipleparticipants. Thisintentionaldiversification,
asdepictedinTable6,doesnotguaranteethepresenceofbothsensitiveattributeswithineachclient.
Dataset Clients Male Female
Client1 4 1
WIDAR Client2 4 1
Client3 4 1
Client1 4 1
HAR Client2 4 1
Client3 4 1
Client4 2 1
Client1 2 4
Client2 4 2
Client3 1 6
EDA Client4 1 6
Client5 2 3
Client6 1 7
Client7 3 6
Table5:Randomdistributionofmulti-personclientsofWIDAR,HAR,andEDAdatasets.
Clients Male Female
Client1 1 1
Client2 2 1
Client3 1 0
Client4 2 0
Client5 2 1
Client6 1 0
Client7 1 0
Client8 2 0
Table6:Singleandmulti-personclientsdistributionofWIDARdataset
D.3 ModelDescription
Widar Model: The Widar Model architecture (adapted from [44]) is comprised of two fully connected layers within a se-
quentialmodule,featuringaninputsizeof22*20*20. Adropoutlayerwitha40%dropoutrateandaRectifiedLinearUnit
(ReLU)activationfunctionfollowthefirstlinearlayer,whilethesecondlinearlayeroutputstheclassificationresult.Themodel
reshapestheinputtensorduringtheforwardpassandincorporatesdropoutlayerstomitigateover-fitting.
HARModel: TheHARmodelarchitecture(adaptedfrom[46])ischaracterizedbyitscomplexity,featuringtwosequential
fullyconnectedlayers. Thefirstlayerconsistsoflineartransformationswithinputandoutputsizesof792to1024,followedby
dropoutwitha30%probability,RectifiedLinearUnit(ReLU)activation,andInstanceNormalization. Subsequently,another
linear layer reduces the dimensionality to 512, followed by similar dropout, ReLU, and Instance Normalization. The second
layerfurtherreducesthedimensionalityfrom512to128,followedbyalinearlayerwithanoutputsizeequaltothespecified
number of classes. The final output is obtained by applying the sigmoid function to the result. During the forward pass, the
inputtensorisprocessedthroughtheselayers,reshapedwhennecessary,andtheoutputisreturnedafterapplyingthesigmoid
function.
EDAModel: TheEDAmodelarchitecture(adaptedfrom[45])comprisesthreesequentialfullyconnectedlayers. Thefirst
layer takes input of size 34 and produces an output of size 64, incorporating batch normalization and Rectified Linear Unit
(ReLU)activation. Thesecondlayerfurtherprocessestheoutput,reducingittosize128withReLUactivation. Thefinallayer
transformstheoutputtothespecifiedembeddingsize.D.4 Hyperparameters
Wehaveproducedresultsfromthreedifferentseedsetsandpresentedoneofthemhere.Thecompletesetofresultsforallthree
seedscanbefoundinTable7.
Dataset F1Score Accuracy EOGap Model Seed
0.842 0.838 0.463 fedavg 16
0.835 0.828 0.404 HA-FL 16
HAR 0.849 0.846 0.451 fedavg 17
0.821 0.812 0.413 HA-FL 17
0.845 0.841 0.454 fedavg 15
0.814 0.805 0.412 HA-FL 15
0.806 0.748 0.342 fedavg 2023
0.795 0.814 0.259 HA-FL 2023
EDA 0.794 0.755 0.323 fedavg 2024
0.748 0.782 0.242 HA-FL 2024
0.796 0.703 0.390 fedavg 2022
0.763 0.797 0.242 HA-FL 2022
0.874 0.871 0.414 fedavg 2023
0.830 0.829 0.346 HA-FL 2023
0.876 0.874 0.420 fedavg 2024
Widar 0.814 0.818 0.334 HA-FL 2024
0.873 0.870 0.417 fedavg 2022
0.806 0.813 0.323 HA-FL 2022
Table7:PerformancemetricsforFLmodelswithdifferentseedvalues
TheapproachusesseveralhyperparametersforBenignandHA-FLmodeltraining. Table8and9showsthehyperparameter
valuesforbenignandHA-FLmodeltraining, respectively. Thesevaluescorrespondtothebestresultsobtainedusingagrid
search.WithinthecontextoftheHA-FLmodel,theSWALearningRatespecificallydenotesthelearningrateatwhichtheSWA
modelisintroducedintothetrainingprocess. TheSWAStartRoundisindicativeoftheroundatwhichtheSWAmechanismis
initiated. Additionally,thecycleparametersignifiesthatposttheinitiationofSWA,theglobalmodelandlearningrateundergo
updatesusingSWAinacyclicmanner,witheachcyclerepresentingadistinctiterationofthisprocess. Thetermϵpertainsto
theweightingvalue,whichensuresthemaintenanceoftherelativeranksofeachclientduringtheaggregationprocess. Opting
forasmallervalueofϵprovedbeneficialforbetterrankingprecision.
Hyperparameter WIDAR EDA HAR
LearningRate 0.1 0.1 0.001
Epochs 3 3 3
TotalRound 80 80 100
Table8:HyperparametersforbenignFedavgmodel
Hyperparameter WIDAR EDA HAR
LearningRate 0.01 0.01 0.001
SWALearningRate 0.1 0.001 0.001
SWAstartround 16 16 16
cycle 5 5 3
Epochs 3 3 3
alpha 0.92 0.92 0.92
ϵ 0.005 0.005 0.005
TotalRounds 80 80 80
Table9:HyperparametersforproposedHA-FLmodelE ExperimentDetails
E.1 Detailsofablationstudy
Weconductedacomprehensiveablationstudytometiculouslyguidethechoicesinourproposedapproach. Thestudyconsis-
tentlydemonstratestheeffectivenessofourmodelacrossalldatasets.
TheIndependentEffectofSharpness-awareAggregationanditsAblationAnalysis
ThissectionevaluatestheeffectofSharpness-awareaggregationtoenhance‘fairnesswithoutdemographics’independently.
Previousstudieshaveexploredbenignclient-sidetraining,usingvariousweightedaggregationschemestopromotefairness
[19]. HA-FL,forinstance,alignswithPessimisticWeightedAggregation(P-W),amethodthatexcludeshighlybiased(based
on known bias-creating factors) models during aggregation. However, HA-FL takes a different approach, assigning lower
weights to highly biased (based on loss-landscape sharpness; without the knowledge of the bias-creating factors) and low-
efficacylocalclient-sidemodels,therebypromotingfairnesswithoutcompletelyexcludingthem.
TounderstandthebenefitsofHA-FL’sSharpness-awareaggregationstrategyindependently(withoutclient-sidelocaltrain-
ingfairnessconstraints)inpromotingclientswithboth(1)lowerrorratesand(2)lowermaximaluncertaintyweevaluatedthree
followingvariations,whileallofthemmaintainingbenignlocalclient-sidetrainingwithoutanyfairnessobjectives.
• HA-FL’s aggregation strategy as outlined in Equation 3 of the main paper while maintaining benign local client-side
trainingwithoutanyfairnessobjectives.
• HA-FL’s aggregation strategy modification, where just selecting clients with lower maximal uncertainty in prediction
(BasedonT inEquation3),whilemaintainingbenignlocalclient-sidetrainingwithoutanyfairnessobjectives.
• HA-FL’saggregationstrategymodification,wherejustselectingclientswithlowererrorrate(BasedonLinEquation3),
whilemaintainingbenignlocalclient-sidetrainingwithoutanyfairnessobjectives.
Theevaluationresultsarepresentedinrowshighlightedin‘blue’inTable10. First, theseresultsconsistentlydemonstrate
that across all datasets, HA-FL’s aggregation strategy, as outlined in Equations 2 and 3, outperforms others. Meaning this
evaluationconfirmsthatintegratingbothclassificationandtopeigenvaluelossandfavoringclientswithlowerloss(asoutlined
inEquations2and3)enhancesthebalancebetweenefficacyandfairness.
Furthermore,thefactthatfairermodelsareachievedevenwithoutlocalclient-sidefairnessconstraintsintheseevaluations,
comparedtothebenignbaselinespresentedinrowshighlightedin‘red’inTable10,establishesthatHA-FL’sSharpness-aware
aggregationindependentlypromotes‘fairnesswithoutdemographics.’
TheIndependentEffectofSharpness-awareClient-sideLocalTraining
ThissectionevaluatestheindependenteffectofSharpness-awareclient-sidelocaltrainingasoutlinedinEquation1toenhance
‘fairnesswithoutdemographics.’ Toconductthisevaluation,weimplementedavariantwhereclient-sidelocaltrainingincor-
poratesfairnessconstraintsbasedonEquation1,whileemployingabenignaggregationschemewhereallweightsinEquation
2aresetto‘1.’ Toensureafaircomparison,theSWAaggregationstrategy,outlinedinAlgorithm1,remainsemployedinthis
approach. The evaluation results are showcased in rows highlighted in ‘yellow’ in Table 10, indicating its superior enhance-
mentoffairnesscomparedtothebenignapproachesdepictedin‘red’highlightedrowsacrossalldatasets. Thisunderscoresthe
independentinfluenceofclient-sidesharpness-awarelocaltraininginenhancing‘fairnesswithoutdemographics.’
ImportanceofIntegratingSharpness-AwareClient-sideTrainingandAggregationStrategyforFairnesswithout
Demographics
TheevaluationsdiscussedabovehavedemonstratedthatboththeSharpness-AwareAggregationStrategy,outlinedinEquations
2and3,andtheSharpness-Awareclient-sidelocaltraining,depictedbyEquation1inthemainpaper,independentlycontribute
topromotingfairness. However,theseindividualstrategiesexhibitlesseffectiveness(fairnessandefficacybalance)compared
tothepresentedHA-FLapproach, whichincorporatesbothstrategies, asindicatedbythe‘green’highlightedrowsacrossall
datasetsinTable10. ThisunderscorestheeffectivenessofAlgorithm1inattaining‘fairnesswithoutdemographics.’Dataset Client-Side Aggregation F1Score Accuracy EOGap ∆Gap
Training Strategy
Benign Fedavg 0.8648 87.08% 0.4135 -
SAM Fedavg 0.8896 89.72% 0.4212 ↑0.0077
WIDAR SAM Fedswa 0.8703 88.35% 0.3930 ↓0.0205
SAM+λ(H )(Eq1) Fedswa 0.7932 82.61% 0.3546 ↓0.0589
client
SAM Fedswa+S(L)(Eq3) 0.6253 72.54% 0.2701 ↓0.1434
SAM Fedswa+S(T)(Eq3) 0.8413 85.60% 0.3597 ↓0.0538
SAM SharpnessAwareaggregation 0.7977 82.62% 0.3597 ↓0.0538
SAM+λ(H )(Eq1) SharpnessAwareaggregation 0.8012 82.92% 0.3457 ↓0.0678
client
Benign Fedavg 0.8064 74.82% 0.3421 -
SAM Fedavg 0.8280 82.82% 0.2980 ↓0.0441
EDA SAM Fedswa 0.8161 82.90% 0.2945 ↓0.0476
SAM+λ(H )(Eq1) Fedswa 0.7992 81.82% 0.2962 ↓0.0459
client
SAM Fedswa+S(L)(Eq3) 0.5874 64.93% 0.2292 ↓0.1129
SAM Fedswa+S(T)(Eq3) 0.7793 77.83% 0.2557 ↓0.0864
SAM SharpnessAwareaggregation 0.7841 81.24% 0.2645 ↓0.0776
SAM+λ(H )(Eq1) SharpnessAwareaggregation 0.7946 81.38% 0.2592 ↓0.0829
client
Benign Fedavg 0.8424 83.78% 0.4630 -
SAM Fedavg 0.8894 88.60% 0.4178 ↓0.0452
HAR SAM Fedswa 0.8431 83.66% 0.4142 ↓0.0488
SAM+λ(H )(Eq1) Fedswa 0.8431 83.66% 0.4107 ↓0.0523
client
SAM Fedswa+S(L)(Eq3) 0.7585 82.96% 0.4103 ↓0.0527
SAM Fedswa+S(T)(Eq3) 0.8321 82.48% 0.4023 ↓0.0607
SAM SharpnessAwareaggregation 0.8287 82.44% 0.4190 ↓0.0440
SAM+λ(H )(Eq1) SharpnessAwareaggregation 0.8347 82.75% 0.4015 ↓0.0615
client
Table10:PerformancemetricsonthedifferentdatasetsfordifferentFLstrategiesandmodelconfigurations.