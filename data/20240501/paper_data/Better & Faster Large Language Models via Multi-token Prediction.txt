Better & Faster Large Language Models via Multi-token Prediction
FabianGloeckle*12 BadrYoubiIdrissi*13 BaptisteRozière1 DavidLopez-Paz+1 GabrielSynnaeve+1
Abstract diction remains an inefficient way of acquiring language,
worldknowledgeandreasoningcapabilities.Moreprecisely,
LargelanguagemodelssuchasGPTandLlama
teacherforcingwithnext-tokenpredictionlatchesonlocal
aretrainedwithanext-tokenpredictionloss. In
patternsandoverlooks“hard”decisions. Consequently,it
thiswork,wesuggestthattraininglanguagemod-
remainsafactthatstate-of-the-artnext-tokenpredictorscall
elstopredictmultiplefuturetokensatonceresults
forordersofmagnitudemoredatathanhumanchildrento
inhighersampleefficiency. Morespecifically,at
arriveatthesameleveloffluency(Frank,2023).
eachpositioninthetrainingcorpus, weaskthe
modeltopredictthefollowingntokensusingn
independentoutputheads,operatingontopofa
sharedmodeltrunk. Consideringmulti-tokenpre-
dictionasanauxiliarytrainingtask,wemeasure
improveddownstreamcapabilitieswithnoover-
head in training time for both code and natural
languagemodels. Themethodisincreasinglyuse-
ful for larger model sizes, and keeps its appeal
whentrainingformultipleepochs. Gainsarees-
pecially pronounced on generative benchmarks
likecoding, whereourmodelsconsistentlyout-
perform strong baselines by several percentage
points. Our13Bparametermodelssolves12%
moreproblemsonHumanEvaland17%moreon
MBPPthancomparablenext-tokenmodels. Ex-
perimentsonsmallalgorithmictasksdemonstrate
that multi-token prediction is favorable for the
developmentofinductionheadsandalgorithmic
reasoningcapabilities. Asanadditionalbenefit,
modelstrainedwith4-tokenpredictionareupto
3×fasteratinference,evenwithlargebatchsizes.
1.Introduction
Humanityhascondenseditsmostingeniousundertakings,
surprising findings and beautiful productions into text.
Large Language Models (LLMs) trained on all of these
corpora are able to extract impressive amounts of world
Figure1: Overviewofmulti-tokenprediction. (Top)Dur-
knowledge, aswellasbasicreasoningcapabilitiesbyim-
ingtraining,themodelpredicts4futuretokensatonce,by
plementingasimple—yetpowerful—unsupervisedlearning
meansofasharedtrunkand4dedicatedoutputheads. Dur-
task: next-token prediction. Despite the recent wave of
inginference,weemployonlythenext-tokenoutputhead.
impressiveachievements(OpenAI,2023),next-tokenpre-
Optionally,theotherthreeheadsmaybeusedtospeed-up
*Equalcontribution+Lastauthors1FAIRatMeta2CERMICS inferencetime. (Bottom)Multi-tokenpredictionimproves
EcoledesPontsParisTech3LISNUniversitéParis-Saclay. Cor- pass@1ontheMBPPcodetask,significantlysoasmodel
respondenceto:FabianGloeckle<fgloeckle@meta.com>,Badr
sizeincreases. Errorbarsareconfidenceintervalsof90%
YoubiIdrissi<byoubi@meta.com>.
computedwithbootstrappingoverdatasetsamples.
1
4202
rpA
03
]LC.sc[
1v73791.4042:viXraBetter&FasterLargeLanguageModelsviaMulti-tokenPrediction
Inthisstudy,wearguethattrainingLLMstopredictmultiple n future tokens (see Figure 1). This leads to the follow-
tokensatoncewilldrivethesemodelstowardbettersample ingfactorizationofthemulti-tokenpredictioncross-entropy
efficiency.AsanticipatedinFigure1,multi-tokenprediction loss:
instructstheLLMtopredictthenfuturetokensfromeach (cid:88)
L =− logP (x |z )·P (z |x )
positioninthetrainingcorpora,allatonceandinparallel(Qi n θ t+n:t+1 t:1 θ t:1 t:1
t
etal.,2020).
n
(cid:88)(cid:88)
=− logP (x |z )·P (z |x ).
θ t+i t:1 θ t:1 t:1
Contributions While multi-token prediction has been
t i=1
studiedinpreviousliterature(Qietal.,2020),thepresent
workoffersthefollowingcontributions: Inpractice,ourarchitectureconsistsofasharedtransformer
trunkf producingthehiddenrepresentationz fromthe
s t:1
1. Weproposeasimplemulti-tokenpredictionarchitec- observedcontextx t:1,nindependentoutputheadsimple-
turewithnotraintimeormemoryoverhead(Section2). mented in terms of transformer layers f hi, and a shared
unembedding matrix f . Therefore, to predict n future
u
2. We provide experimental evidence that this training tokens,wecompute:
paradigmisbeneficialatscale,withmodelsupto13B
parameterssolvingaround15%morecodeproblems P θ(x t+i |x t:1)=softmax(f u(f hi(f s(x t:1)))),
onaverage(Section3).
for i = 1,...n, where, in particular, P (x | x ) is
θ t+1 t:1
ournext-tokenpredictionhead. SeeAppendixBforother
3. Multi-tokenpredictionenablesself-speculativedecod-
variationsofmulti-tokenpredictionarchitectures.
ing,makingmodelsupto3timesfasteratinference
timeacrossawiderangeofbatch-sizes(Section3.2).
Memory-efficientimplementation Onebigchallengein
trainingmulti-tokenpredictorsisreducingtheirGPUmem-
Whilecost-freeandsimple,multi-tokenpredictionisanef-
ory utilization. To see why this is the case, recall that in
fectivemodificationtotrainstrongerandfastertransformer
currentLLMsthevocabularysizeV ismuchlargerthanthe
models. Wehopethatourworkspursinterestinnovelaux-
dimensiondofthelatentrepresentation—therefore,logit
iliarylossesforLLMswellbeyondnext-tokenprediction,
vectorsbecometheGPUmemoryusagebottleneck. Naive
astoimprovetheperformance,coherence,andreasoning
implementationsofmulti-tokenpredictorsthatmaterialize
abilitiesofthesefascinatingmodels.
alllogitsandtheirgradients,bothofshape(n,V),severely
limit the allowable batch-size and average GPU memory
2.Method
utilization. Because of these reasons, in our architecture
weproposetocarefullyadaptthesequenceofforwardand
Standardlanguagemodelinglearnsaboutalargetextcorpus
backwardoperations,asillustratedinFigure2. Inparticular,
x ,...x by implementing a next-token prediction task.
1 T aftertheforwardpassthroughthesharedtrunkf ,wese-
Formally, the learning objective is to minimize the cross- s
quentiallycomputetheforwardandbackwardpassofeach
entropyloss
independentoutputheadf ,accumulatinggradientsatthe
i
(cid:88) trunk. Whilethiscreateslogits(andtheirgradients)forthe
L =− logP (x |x ), (1)
1 θ t+1 t:1 outputheadf ,thesearefreedbeforecontinuingtothenext
i
t
outputheadf ,requiringthelong-termstorageonlyofthe
i+1
whereP isourlargelanguagemodelundertraining,asto d-dimensionaltrunkgradient∂L /∂f . Insum, wehave
θ n s
maximizetheprobabilityofx asthenextfuturetoken, reducedthepeakGPUmemoryutilizationfromO(nV +d)
t+1
giventhehistoryofpasttokensx =x ,...,x . toO(V +d),atnoexpenseinruntime(TableS5).
t:1 t 1
Inthiswork, wegeneralizetheabovebyimplementinga
Inference Duringinferencetime,themostbasicuseofthe
multi-tokenpredictiontask,whereateachpositionofthe
proposedarchitectureisvanillanext-tokenautoregressive
trainingcorpus,themodelisinstructedtopredictnfuture
predictionusingthenext-tokenpredictionheadP (x |
tokensatonce. Thistranslatesintothecross-entropyloss θ t+1
x ),whilediscardingallothers. However,theadditional
t:1
(cid:88) outputheadscanbeleveragedtospeedupdecodingfromthe
L =− logP (x |x ). (2)
n θ t+n:t+1 t:1 next-tokenpredictionheadwithself-speculativedecoding
t
methodssuchasblockwiseparalleldecoding(Sternetal.,
To make matters tractable, we assume that our large lan- 2018)—avariantofspeculativedecoding(Leviathanetal.,
guagemodelP employsasharedtrunktoproducealatent 2023)withouttheneedforanadditionaldraftmodel—and
θ
representation z of the observed context x , then fed speculativedecodingwithMedusa-liketreeattention(Cai
t:1 t:1
intonindependentheadstopredictinparalleleachofthe etal.,2024).
2Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
MBPP Human Eval
+4.5
+1.7
2 5 2 3 7
7 11 24 26 -0.6 5 13 14
-1.7
+3.9
+5.0
10 21
27 36 54 57
Figure2: Orderoftheforward/backwardinann-token 5 9 13
predictionmodelwithn = 2heads. Byperformingthe -5.4 -1.0 17 29 34
forward/backwardontheheadsinsequentialorder,weavoid
materializingallunembeddinglayergradientsinmemory +2.2 30 45 51
simultaneouslyandreducepeakGPUmemoryusage. +7.5
60 75 77
11 17 24
3.Experimentsonrealdata -9.8 -2.3 30 52 56
Wedemonstratetheefficacyofmulti-tokenpredictionlosses
bysevenlarge-scaleexperiments. Section3.1showshow
multi-token prediction is increasingly useful when grow-
ingthemodelsize. Section3.2showshowtheadditional Figure3: Resultsofn-tokenpredictionmodelsonMBPP
predictionheadscanspeedupinferencebyafactorof3× bymodelsize. Wetrainmodelsofsixsizesintherange
usingspeculativedecoding. Section3.3demonstrateshow or 300M to 13B total parameters on code, and evaluate
multi-tokenpredictionpromoteslearninglonger-termpat- pass@1,10,100ontheMBPP(Austinetal.,2021)andHu-
terns,afactmostapparentintheextremecaseofbyte-level manEval(Chenetal.,2021)benchmarkwith1000samples.
tokenization. Section3.4showsthat4-tokenpredictorleads Multi-tokenpredictionmodelsareworsethanthebaseline
to strong gains with a tokenizer of size 32k. Section 3.5 forsmallmodelsizes,butoutperformthebaselineatscale.
illustratesthatthebenefitsofmulti-tokenpredictionremain Errorbarsareconfidenceintervalsof90%computedwith
fortrainingrunswithmultipleepochs. Section3.6show- bootstrappingoverdatasetsamples.
casestherichrepresentationspromotedbypretrainingwith
multi-token prediction losses by finetuning on the Code-
Contests dataset (Li et al., 2022). Section 3.7 shows that ure3forMBPP(Austinetal.,2021)andHumanEval(Chen
thebenefitsofmulti-tokenpredictioncarrytonaturallan- etal., 2021)showthat itis possible, withthe exactsame
guage models, improving generative evaluations such as computationalbudget,tosqueezemuchmoreperformance
summarization,whilenotregressingsignificantlyonstan- out of large language models given a fixed dataset using
dardbenchmarksbasedonmultiplechoicequestionsand multi-tokenprediction.
negativelog-likelihoods.
Webelievethisusefulnessonlyatscaletobealikelyreason
To allow fair comparisons between next-token predictors why multi-token prediction has so far been largely over-
andn-tokenpredictors,theexperimentsthatfollowalways lookedasapromisingtraininglossforlargelanguagemodel
comparemodelswithanequalamountofparameters. That training.
is,whenweaddn−1layersinfuturepredictionheads,we
removen−1layersfromthesharedmodeltrunk. Please 3.2.Fasterinference
refer to Table S14 for the model architectures and to Ta-
We implement greedy self-speculative decoding (Stern
bleS13foranoverviewofthehyperparametersweusein
etal.,2018)withheterogeneousbatchsizesusingxForm-
ourexperiments.
ers(Lefaudeuxetal.,2022)andmeasuredecodingspeeds
ofourbest4-tokenpredictionmodelwith7Bparameters
3.1.Benefitsscalewithmodelsize
on completing prompts taken from a test dataset of code
To study this phenomenon, we train models of six sizes and natural language (Table S2) not seen during training.
in the range 300M to 13B parameters from scratch on at Weobserveaspeedupof3.0×oncodewithanaverageof
least 91B tokens of code. The evaluation results in Fig- 2.5 accepted tokens out of 3 suggestions on code, and of
3
B3.0 B6.0 B3.1 B3 B7.6 B31 B3.0 B6.0 B3.1 B3 B7.6 B31
Pass@1
Pass@10
Pass@100Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
Table1: Multi-tokenpredictionimprovesperformanceandunlocksefficientbyteleveltraining. Wecomparemodels
with7Bparameterstrainedfromscratchon200Bandon314BbytesofcodeontheMBPP(Austinetal.,2021),HumanEval
(Chenetal.,2021)andAPPS(Hendrycksetal.,2021)benchmarks. Multi-tokenpredictionlargelyoutperformsnexttoken
predictiononthesesettings. AllnumberswerecalculatedusingtheestimatorfromChenetal.(2021)basedon200samples
perproblem. Thetemperatureswerechosenoptimally(basedontestscores;i.e. theseareoracletemperatures)foreach
model,datasetandpass@kandarereportedinTableS12.
MBPP HumanEval APPS/Intro
Trainingdata Vocabulary n
@1 @10 @100 @1 @10 @100 @1 @10 @100
1 19.3 42.4 64.7 18.1 28.2 47.8 0.1 0.5 2.4
313Bbytes 8 32.3 50.0 69.6 21.8 34.1 57.9 1.2 5.7 14.0
bytes
(0.5epochs) 16 28.6 47.1 68.0 20.4 32.7 54.3 1.0 5.0 12.9
32 23.0 40.7 60.3 17.2 30.2 49.7 0.6 2.8 8.8
1 30.0 53.8 73.7 22.8 36.4 62.0 2.8 7.8 17.4
2 30.3 55.1 76.2 22.2 38.5 62.6 2.1 9.0 21.7
200Btokens
32ktokens 4 33.8 55.9 76.9 24.0 40.1 66.1 1.6 7.1 19.9
(0.8epochs)
6 31.9 53.9 73.1 20.6 38.4 63.9 3.5 10.8 22.7
8 30.7 52.2 73.4 20.0 36.6 59.6 3.5 10.4 22.1
1Ttokens 1 40.7 65.4 83.4 31.7 57.6 83.0 5.4 17.8 34.1
32ktokens
(4epochs) 4 43.1 65.9 83.7 31.6 57.3 86.2 4.3 15.6 33.7
2.7×ontext. Onan8-bytepredictionmodel,theinference 3.4.Searchingfortheoptimaln
speedupis6.4×(TableS3). Pretrainingwithmulti-token
Tobetterunderstandtheeffectofthenumberofpredicted
predictionallowstheadditionalheadstobemuchmoreac-
tokens,wedidcomprehensiveablationsonmodelsofscale
curatethanasimplefinetuningofanext-tokenprediction
7Btrainedon200Btokensofcode. Wetryn = 1,2,4,6
model,thusallowingourmodelstounlockself-speculative
and8inthissetting. Resultsintable1showthattraining
decoding’sfullpotential.
with4-futuretokensoutperformsalltheothermodelscon-
sistentlythroughoutHumanEvalandMBPPforpassat1,
3.3.Learningglobalpatternswithmulti-byteprediction
10and100metrics: +3.8%,+2.1%and+3.2%forMBPP
Toshowthatthenext-tokenpredictiontasklatchestolocal and+1.2%,+3.7%and+4.1%forHumanEval.Interestingly,
patterns,wewenttotheextremecaseofbyte-leveltokeniza- forAPPS/Intro,n = 6takestheleadwith+0.7%,+3.0%
tionbytraininga7Bparameterbyte-leveltransformeron and+5.3%. Itisverylikelythattheoptimalwindowsize
314B bytes, which is equivalent to around 116B tokens. depends on input data distribution. As for the byte level
The8-bytepredictionmodelachievesastoundingimprove- modelstheoptimalwindowsizeismoreconsistent(8bytes)
mentscomparedtonext-byteprediction,solving67%more acrossthesebenchmarks.
problems on MBPP pass@1 and 20% more problems on
HumanEvalpass@1. 3.5.Trainingformultipleepochs
Multi-bytepredictionisthereforeaverypromisingavenue Multi-tokentrainingstillmaintainsanedgeonnext-token
to unlock efficient training of byte-level models. Self- prediction when trained on multiple epochs of the same
speculativedecodingcanachievespeedupsof6timesfor data. The improvements diminish but we still have a
the 8-byte prediction model, which would allow to fully +2.4%increaseonpass@1onMBPPand+3.2%increase
compensatethecostoflongerbyte-levelsequencesatinfer- onpass@100onHumanEval,whilehavingsimilarperfor-
encetimeandevenbefasterthananext-tokenprediction mancefortherest. AsforAPPS/Intro,awindowsizeof4
modelbynearlytwotimes. The8-bytepredictionmodel wasalreadynotoptimalwith200Btokensoftraining.
isastrongbyte-basedmodel,approachingtheperformance
oftoken-basedmodelsdespitehavingbeentrainedon1.7× 3.6.Finetuningmulti-tokenpredictors
lessdata.
Pretrainedmodelswithmulti-tokenpredictionlossalsoout-
performnext-tokenmodelsforuseinfinetunings. Weevalu-
atethisbyfinetuning7BparametermodelsfromSection3.3
4Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
ontheCodeContestsdataset(Lietal.,2022). Wecompare 52.5
n
the4-tokenpredictionmodelwiththenext-tokenprediction
50.0 1
baseline,andincludeasettingwherethe4-tokenprediction
2
47.5
model is stripped off its additional prediction heads and 4
finetuned using the classical next-token prediction target. 45.0
AccordingtotheresultsinFigure4,bothwaysoffinetuning
42.5
the4-tokenpredictionmodeloutperformthenext-tokenpre-
40.0
dictionmodelonpass@kacrossk. Thismeansthemodels
are both better at understanding and solving the task and 37.5
atgeneratingdiverseanswers. NotethatCodeContestsis
35.0
themostchallengingcodingbenchmarkweevaluateinthis
5000 10000 15000 20000 25000
study. Next-tokenpredictionfinetuningontopof4-token Training step
predictionpretrainingappearstobethebestmethodoverall,
Figure5: Multi-tokentrainingwith7Bmodelsdoesn’t
inlinewiththeclassicalparadigmofpretrainingwithauxil-
improveperformanceonchoicetasks. Thisfigureshows
iarytasksfollowedbytask-specificfinetuning. Pleaserefer
theevolutionofaverageaccuracyof6standardNLPbench-
toAppendixFfordetails.
marks. Detailed results in Appendix G for 7B models
trained on 200B tokens of language data. The 2 future
tokenmodelhasthesameperformanceasthebaselineand
10.0
the4futuretokenmodelregressesabit. Largermodelsizes
5.0 mightbenecessarytoseeimprovementsonthesetasks.
2.0
throughouttraining. The4-futuretokenpredictionmodel
1.0
suffersaperformancedegradation. Detailednumbersare
n=1, n'=1
0.5 n=4, n'=1 reportedinAppendixG.
n=4, n'=4
However, we do not believe that multiple-choice and
0.2
likelihood-basedbenchmarksaresuitedtoeffectivelydis-
1 10 100 1000
cerngenerativecapabilitiesoflanguagemodels. Inorder
k
toavoidtheneedforhumanannotationsofgenerationqual-
Figure 4: Comparison of finetuning performance on ityorlanguagemodeljudges—whichcomeswithitsown
CodeContests. We finetune a 4-token prediction model pitfalls,aspointedoutbyKooetal.(2023)—weconduct
on CodeContests (Li et al., 2022) (train split) using n′- evaluationsonsummarizationandnaturallanguagemath-
token prediction as training loss with n′ = 4 or n′ = 1, ematicsbenchmarksandcomparepretrainedmodelswith
and compare to a finetuning of the next-token prediction trainingsetssizesof200Band500Btokensandwithnext-
baseline model (n = n′ = 1). For evaluation, we gen- tokenandmulti-tokenpredictionlosses,respectively.
erate1000samplespertestproblemforeachtemperature
For summarization, we use eight benchmarks where
T ∈{0.5,0.6,0.7,0.8,0.9},andcomputepass@kforeach
ROUGEmetrics(Lin,2004)withrespecttoaground-truth
valueofkandT. Shownisk (cid:55)→max pass_at(k,T),i.e.
T
summaryallowautomaticevaluationofgeneratedtexts. We
wegrantaccesstoatemperatureoracle. Weobservethat
finetuneeachpretrainedmodeloneachbenchmark’strain-
bothwaysoffinetuningthe4-tokenpredictionmodelout-
ingdatasetforthreeepochsandselectthecheckpointwith
perform the next-token prediction baseline. Intriguingly,
thehighestROUGE-LF scoreonthevalidationdataset.
usingnext-tokenpredictionfinetuningontopofthe4-token 1
Figure6showsthatmulti-tokenpredictionmodelswithboth
predictionmodelappearstobethebestmethodoverall.
n = 2andn = 4improveoverthenext-tokenbaselinein
ROUGE-LF scoresforbothtrainingdatasetsizes, with
1
3.7.Multi-tokenpredictiononnaturallanguage theperformancegapshrinkingwithlargerdatasetsize. All
metricscanbefoundinAppendixH.
Toevaluatemulti-tokenpredictiontrainingonnaturallan-
guage, we train models of size 7B parameters on 200B For natural language mathematics, we evaluate the pre-
tokensofnaturallanguagewitha4-token,2-tokenandnext- trainedmodelsin8-shotmodeontheGSM8Kbenchmark
tokenpredictionloss,respectively. InFigure 5,weevaluate (Cobbeetal.,2021)andmeasureaccuracyofthefinalan-
theresultingcheckpointson6standardNLPbenchmarks. swerproducedafterachain-of-thoughtelicitedbythefew-
Onthesebenchmarks,the2-futuretokenpredictionmodel shot examples. We evaluate pass@k metrics to quantify
performs on par with the next-token prediction baseline diversityandcorrectnessofanswerslikeincodeevaluations
5
)%(
k@ssap
ycarucca
egarevABetter&FasterLargeLanguageModelsviaMulti-tokenPrediction
27.5 n=1 0.5
n=2
27.0 n=4 0.4
26.5 0.3
26.0 0.2
25.5 0.1 n=1 (baseline)
n=2 (ours)
25.0 0.0
200 500 1 3 10 30 100 300 1000
Training tokens (B) Parameters (M)
Figure6: Performanceonabstractivetextsummariza- Figure7:Inductioncapabilityofn-tokenpredictionmod-
tion. Average ROUGE-L (longest common subsequence els. Shownisaccuracyonthesecondtokenoftwotoken
overlap)F scorefor7Bmodelstrainedon200Band500B namesthathavealreadybeenmentionedpreviously. Shown
1
tokensofnaturallanguageoneightsummarizationbench- are numbers for models trained with a next-token and a
marks. We finetune the respective models on each task’s 2-tokenpredictionloss,respectively,withtwoindependent
trainingdataseparatelyforthreeepochsandselectthecheck- runs each. The lines denote per-loss averages. For small
pointswithhighestROUGE-LF validationscore. Both modelsizes,next-tokenpredictionmodelslearnpractically
1
n = 2andn = 4multi-tokenpredictionmodelshavean noorsignificantlyworseinductioncapabilitythan2-token
advantage over next-token prediction models. Individual predictionmodels,withtheirdisadvantagedisappearingat
scores per dataset and more details can be found in Ap- thesizeof100Mnonembeddingparameters.
pendixH.
capability in a controlled way. Training small models of
sizes1Mto1Bnonembeddingparametersonadatasetof
andusesamplingtemperaturesbetween0.2and1.4. The childrenstories,wemeasureinductioncapabilitybymeans
resultsaredepictedinFigureS13inAppendixI.For200B ofanadaptedtestset: in100storiesfromtheoriginaltest
trainingtokens,then = 2modelclearlyoutperformsthe split,wereplacethecharacternamesbyrandomlygenerated
next-tokenpredictionbaseline, whilethepatternreverses namesthatconsistoftwotokenswiththetokenizerweem-
after500Btokensandn=4isworsethroughout. ploy. Predictingthefirstofthesetwotokensislinkedtothe
semanticsoftheprecedingtext,whilepredictingthesecond
4.Ablationsonsyntheticdata tokenofeachname’soccurrenceafterithasbeenmentioned
atleastoncecanbeseenasapureinductiontask. Inour
Whatdrivestheimprovementsindownstreamperformance experiments,wetrainforupto90epochsandperformearly
ofmulti-tokenpredictionmodelsonallofthetaskswehave stopping with respect to the test metric (i.e. we allow an
considered? Byconductingtoyexperimentsoncontrolled epochoracle). Figure7reportsinductioncapabilityasmea-
trainingdatasetsandevaluationtasks,wedemonstratethat suredbyaccuracyonthenames’secondtokensinrelation
multi-tokenpredictionleadstoqualitativechangesinmodel tomodelsizefortworunswithdifferentseeds.
capabilities and generalization behaviors. In particular,
We find that 2-token prediction loss leads to a vastly im-
Section 4.1 shows that for small model sizes, induction
provedformationofinductioncapabilityformodelsofsize
capability—as discussed by Olsson et al. (2022)—either
30Mnonembeddingparametersandbelow,withtheiradvan-
onlyformswhenusingmulti-tokenpredictionastraining
tagedisappearingforsizesof100Mnonembeddingparame-
loss,oritisvastlyimprovedbyit. Moreover,Section4.2
tersandabove.1 Weinterpretthisfindingasfollows: multi-
showsthatmulti-tokenpredictionimprovesgeneralization
token prediction losses help models to learn transferring
onanarithmetictask,evenmoresothantriplingmodelsize.
informationacrosssequencepositions, whichlendsitself
to the formation of induction heads and other in-context
4.1.Inductioncapability
learningmechanisms. However,onceinductioncapability
Inductiondescribesasimplepatternofreasoningthatcom- hasbeenformed,theselearnedfeaturestransforminduction
pletespartialpatternsbytheirmostrecentcontinuation(Ols-
1Notethataperfectscoreisnotreachableinthisbenchmark
sonetal.,2022). Inotherwords,ifasentencecontains“AB”
assomeofthetokensinthenamesintheevaluationdatasetnever
andlatermentions“A”,inductionisthepredictionthatthe appearinthetrainingdata,andinourarchitecture,embeddingand
continuationis“B”.Wedesignasetuptomeasureinduction unembeddingparametersarenotlinked.
6
1F
L-EGUOR
.gvA
sseccus
noitcudnIBetter&FasterLargeLanguageModelsviaMulti-tokenPrediction
100 andcanbeusedtoadjustthedifficultyofbothin-domain
n=1
(m≤5)andout-of-domain(m>5)generalizationevalua-
n=2
80 tions. Theevaluationsareconductedwithgreedysampling
n=4
onafixedtestsetof2000samplespernumberofoperations.
60 We train models of two small sizes with 30M and 100M
nonembeddingparameters,respectively. Thissimulatesthe
40 conditionsoflargelanguagemodelstrainedonmassivetext
corporawhicharelikewiseunder-parameterizedandunable
20 tomemorizetheirentiretrainingdatasets.
Multi-tokenpredictionimprovesalgorithmicreasoningca-
0
pabilitiesasmeasuredbythistaskacrosstaskdifficulties
11 22 33 44 55 66 77 88 99 1100
(Figure 8). In particular, it leads to impressive gains in
# operations
out-of-distributiongeneralization,despitethelowabsolute
numbers. Increasing the model size from 30M to 100M
in-domain out-of-domain parameters, on the other hand, does not improve evalua-
tionaccuracyasmuchasreplacingnext-tokenpredictionby
Figure8: Accuracyonapolynomialarithmetictaskwith
multi-tokenpredictiondoes(FigureS16). InAppendixK,
varyingnumberofoperationsperexpression. Training
we furthermore show that multi-token prediction models
withmulti-tokenpredictionlossesincreasesaccuracyacross
retain their advantage over next-token prediction models
taskdifficulties. Inparticular,italsosignificantlyimproves
onthistaskwhentrainedandevaluatedwithpausetokens
out-of-domaingeneralizationperformance,albeitatalow
(Goyaletal.,2023).
absolutelevel. Triplingthemodelsize,ontheotherhand,
hasaconsiderablysmallereffectthanreplacingnext-token
prediction with multi-token prediction loss (Figure S16). 5.Whydoesitwork? Somespeculation
Shown are two independent runs per configuration with
Why does multi-token prediction afford superior perfor-
100Mparametermodels.
manceoncodingevaluationbenchmarks,andonsmallal-
gorithmicreasoningtasks? Ourintuition,developedinthis
intoataskthatcanbesolvedlocallyatthecurrenttokenand section,isthatmulti-tokenpredictionmitigatesthedistri-
learnedwithnext-tokenpredictionalone. Fromthispoint butional discrepancy between training-time teacher forc-
on,multi-tokenpredictionactuallyhurtsonthisrestricted ingandinference-timeautoregressivegeneration. Wesup-
benchmark—but we surmise that there are higher forms portthisviewwithanillustrativeargumentontheimplicit
ofin-contextreasoningtowhichitfurthercontributes,as weightsmulti-tokenpredictionassignstotokensdepending
evidencedbytheresultsinSection3.1. InFigureS14,we ontheirrelevanceforthecontinuationofthetext,aswellas
provide evidence for this explanation: replacing the chil- withaninformation-theoreticdecompositionofmulti-token
drenstoriesdatasetbyahigher-quality9:1mixofabooks predictionloss.
datasetwiththechildrenstories,weenforcetheformation
of induction capability early in training by means of the 5.1.Lookaheadreinforceschoicepoints
datasetalone. Byconsequence,exceptforthetwosmallest
Not all token decisions are equally important for gener-
modelsizes,theadvantageofmulti-tokenpredictiononthe
ating useful texts from language models (Bachmann and
taskdisappears: featurelearningofinductionfeatureshas
Nagarajan,2024;Linetal.,2024).Whilesometokensallow
convertedthetaskintoapurenext-tokenpredictiontask.
stylistic variations that do not constrain the remainder of
thetext,othersrepresentchoicepointsthatarelinkedwith
4.2.Algorithmicreasoning
higher-levelsemanticpropertiesofthetextandmaydecide
Algorithmicreasoningtasksallowtomeasuremoreinvolved whetherananswerisperceivedasusefulorderailing.
formsofin-contextreasoningthaninductionalone.Wetrain
Multi-tokenpredictionimplicitlyassignsweightstotraining
andevaluatemodelsonataskonpolynomialarithmeticin
tokensdependingonhowcloselytheyarecorrelatedwith
the ring F [X]/(X5) with unary negation, addition, mul-
7 theirsuccessors. Asanillustrativeexample, considerthe
tiplicationandcompositionofpolynomialsasoperations.
sequence depicted in Figure 9 where one transition is a
Thecoefficientsoftheoperandsandtheoperatorsaresam-
hard-to-predictchoicepointwhiletheothertransitionsare
pleduniformly. Thetaskistoreturnthecoefficientsofthe
considered“inconsequential”. Inconsequentialtransitions
polynomialscorrespondingtotheresultingexpressions.The
following a choice point are likewise hard to predict in
numbermofoperationscontainedintheexpressionsisse-
advance. Bymarkingandcountinglossterms,wefindthat
lecteduniformlyfromtherangefrom1to5attrainingtime,
7
)%(
ycaruccABetter&FasterLargeLanguageModelsviaMulti-tokenPrediction
ofthetexttocome. InAppendixL.2,wegivearelativever-
sionoftheaboveequationsthatshowstheincreasedweight
ofrelativemutualinformationinalossdecompositionof
2-tokenpredictionloss.
6.Relatedwork
Languagemodelinglosses Dongetal.(2019)andTay
etal.(2022)trainonamixtureofdenoisingtaskswithdif-
Figure9: Multi-tokenpredictionlossassignshigherim- ferentattentionmasks(full,causalandprefixattention)to
plicit weights to consequential tokens. Shown is a se- bridgetheperformancegapwithnexttokenpretrainingon
quenceinwhichalltransitionsexcept“5→A”areeasyto generative tasks. Tay et al. (2022) uses the span corrup-
predict, alongsidethecorrespondingpredictiontargetsin tionobjective,whichreplacesspansoftokenswithspecial
3-tokenprediction. Sincetheconsequencesofthedifficult tokensfortheencoderandthedecoderthenpredictsthecon-
transition“5→A”arelikewisehardtopredict,thistransi- tentsofthosespans. UnlikeUniLM,thisallowsfullcausal
tionreceivesahigherimplicitweightintheoveralllossvia trainingwithteacherforcing. Similarly,Yangetal.(2019)
itscorrelates“3→A”,...,“5→C”. trainonpermutedsequences,whileconservingtheoriginal
positionalembeddings,effectivelytrainingthemodeltopre-
dictvariouspartsofthesequencegivenamixofpastand
n-tokenpredictionassociatesaweightof n(n+1) tochoice future information. This permuted language modeling is
2
points via their correlates, and a smaller weight of n to theclosesttasktoourssinceitallowspredictingbeyondthe
inconsequential points. Please refer to Appendix L.3 for nexttoken. Howeveralloftheselanguagemodelingtasks
moredetails. Generally,webelievethatthequalityoftext train on a small percentage of the input text: on average
generationsdependsonpickingtherightdecisionsatchoice only15%ofthetokensarebackwardedthrough. ForDong
points,andthatn-tokenpredictionlossespromotethose. etal.(2019),wherethemaskingisdoneinBERTstyle,it
ishardtomaskmorethan15%sinceitdestroystoomuch
5.2.Information-theoreticargument information. ForTayetal.(2022),itistechnicallypossible
tohavealargerproportionbutinpractice,thesettingsused
Languagemodelsaretypicallytrainedbyteacher-forcing,
havebetween15%and25%ofmaskedtokens. (Yangetal.,
wherethemodelreceivesthegroundtruthforeachfuture
2019)alsomakesitpossibletotrainonthewholesequence
tokenduringtraining. However,duringtesttimegeneration
sinceitisonlypermuted,andnoinformationislost. Yet,
isunguidedandautoregressive,wherebyerrorsaccumulate.
in practice, since the completely random permutation is
Teacher-forcing,weargue,encouragesmodelstofocuson
veryhardtoreconstruct,only15%arepredictedfortraining
predictingwellintheveryshortterm,atthepotentialex-
stabilityreasons.
penseofignoringlonger-termdependenciesintheoverall
structureofthegeneratedsequence.
Toillustratetheimpactofmulti-tokenprediction,consider Multi-tokenpredictioninlanguagemodelling Qietal.
the following information-theoretic argument. Here, X (2020)arguethatmulti-tokenpredictionencouragesplan-
denotesthenextfuturetoken,andY thesecond-nextfuture ning,improvesrepresentationsandpreventstheoverfitting
token.Theproductionofbothofthesetokensisconditioned onlocalpatternsthatcanresultfromteacher-forcedtraining.
onsomeobserved,inputcontextC,thatweomitfromour However, their technical approach replicates the residual
equations for simplicity. When placed before token X, streamn-foldwhileoursallowsforcompute-matchedcom-
vanillanext-tokenpredictionconcernsthequantityH(X), parisonsandmakestheresidualrepresentationsparticipate
whilemulti-tokenpredictionwithn = 2aimsatH(X)+ moredirectlyintheauxiliarylossterms. Sternetal.(2018)
H(Y). Wedecomposethesetwoquantitiesas: andCaietal.(2024)proposemodelfinetuningswithmulti-
token prediction for faster inference but do not study the
H(X)=H(X |Y)+I(X;Y), effectsofsuchalossduringpretraining.Paletal.(2023)use
H(X)+H(Y)=H(X |Y)+2I(X;Y)+H(Y |X). probingmethodstoshowthatnext-tokenpredictionmodels
areabletopredictadditionalconsecutivetokenstoacertain
BydiscardingthetermH(Y | X)—whichappearsagain extent,butlesssothanourmodelswhicharespecifically
whenpredictingatthefollowingposition—weobservethat trainedforthistask. JianyuZhang(2024)observeimprove-
2-tokenpredictionincreasestheimportanceofI(X;Y)by mentsinlanguagemodellingtaskswithmulti-labelbinary
afactorof2.So,multi-tokenpredictorsaremoreaccurateat classificationovertheoccurrenceofvocabularywordsin
predictingtokensX thatareofrelevancefortheremainder thefutureasanauxiliarylearningtask.
8Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
Self-speculativedecoding Sternetal.(2018)are,tothe Impactstatement
best of our knowledge, the first to suggest a speculative
The goal of this paper is to make language models more
decodingschemeforfasterinference. Ourarchitecturere-
compute and data efficient. While this may in principle
placestheirlinearpredictionheadsbytransformerlayers,
reducetheecologicalimpactoftrainingLLMs,weshallbe
butisotherwisesimilar.Byreorganizingtheorderofthefor-
carefulaboutreboundeffects. Allsocietaladvantages,as
ward/backward,wecanusealllosstermsinsteadofstochas-
well as risks, of LLMs should be considered while using
tically picking one head for loss computation. Cai et al.
thiswork.
(2024)presentamoreelaborateself-speculativedecoding
schemethatusesthetop-kpredictionsofeachheadinstead
of the best one only. It can be used with the multi-token Environmentalimpact
predictionmodelswetrain.
Inaggregate,trainingallmodelsreportedinthepaperre-
quiredaround500KGPUhoursofcomputationonhardware
Multi-target prediction Multi-task learning is the oftypeA100-80GBandH100. Estimatedtotalemissions
paradigmoftrainingneuralnetworksjointlyonseveraltasks were around 50 tCO2eq, 100% of which were offset by
toimproveperformanceonthetasksofinterest(Caruana, Meta’ssustainabilityprogram.
1997). Learningwithsuchauxiliarytasksallowsmodelsto
exploitdependenciesbetweentargetvariablesandcaneven Acknowledgements
bepreferableinthecaseofindependenttargets(Waegeman
etal.,2019). Whilemorespecificallytailoredarchitectures WethankJianyuZhang,LéonBottou,EmmanuelDupoux,
for multi-target prediction are conceivable (Spyromitros- Pierre-EmmanuelMazaré,YannLeCun,QuentinGarrido,
Xioufisetal.,2016;Readetal.,2021),moderndeeplearn- MegiDervishi,MathurinVideauandTimothéeDarcetand
ingapproachesusuallyrelyonlargesharedmodeltrunks otherFAIRPhDstudentsandCodeGenteammembersfor
withseparatepredictionheadsfortherespectivetasks(Caru- helpfuldiscussions. WethankJonasGehringforhistech-
ana,1997;Silveretal.,2016;Lampleetal.,2022)likewe nicalexpertiseandtheoriginalLlamateamandxFormers
do. Multi-target prediction has been shown to be a suc- teamforenablingthiskindofresearch.
cessfulstrategyinvariousdomains,e.g. forlearningtime
seriespredictionwithmoredistanttimestepsinthefuture
asauxiliarytargets(VapnikandVashist,2009)orforlearn-
ingfromvideoswithseveralfutureframes(Mathieuetal.,
2016;Srivastavaetal.,2016)orrepresentationsoffuture
frames(Vondricketal.,2016)asauxiliarytargets.
7.Conclusion
Wehaveproposedmulti-tokenpredictionasanimprovement
overnext-tokenpredictionintraininglanguagemodelsfor
generativeorreasoningtasks.Ourexperiments(upto7Bpa-
rametersand1Ttokens)showthatthisisincreasinglyuseful
for larger models and in particular show strong improve-
ments for code tasks. We posit that our method reduces
distributionmismatchbetweenteacher-forcedtrainingand
autoregressivegeneration. Whenusedwithspeculativede-
coding,exactinferencegets3timesfaster.
Infutureworkwewouldliketobetterunderstandhowtoau-
tomaticallychooseninmulti-tokenpredictionlosses. One
possibility to do so is to use loss scales and loss balanc-
ing(Défossezetal.,2022). Also,optimalvocabularysizes
formulti-tokenpredictionarelikelydifferentfromthosefor
next-tokenprediction,andtuningthemcouldleadtobetter
results,aswellasimprovedtrade-offsbetweencompressed
sequencelengthandcompute-per-byteexpenses. Finally,
we would like to develop improved auxiliary prediction
lossesthatoperateinembeddingspaces(LeCun,2022).
9Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
References AlexanderR.Fabbri,IreneLi,TianweiShe,SuyiLi,and
Dragomir R. Radev. Multi-news: a large-scale multi-
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten
documentsummarizationdatasetandabstractivehierar-
Bosma,HenrykMichalewski,DavidDohan,EllenJiang,
chicalmodel,2019.
Carrie Cai, Michael Terry, Quoc Le, et al. Program
synthesis with large language models. arXiv preprint MehrdadFarahani.Summarizationusingbert2bertmodelon
arXiv:2108.07732,2021. wikisummarydataset. https://github.com/m3hrdadfi/wiki-
summary,2020.
GregorBachmannandVaishnavhNagarajan. Thepitfalls
ofnext-tokenprediction,2024. MehrdadFarahani,MohammadGharachorloo,andMoham-
madManthouri. Leveragingparsbertandpretrainedmt5
Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam
forpersianabstractivetextsummarization. In202126th
Shazeer. Scheduled sampling for sequence prediction
InternationalComputerConference,ComputerSociety
withrecurrentneuralnetworks,2015.
of Iran (CSICC). IEEE, March 2021. doi: 10.1109/
csicc52343.2021.9420563. URL http://dx.doi.
Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng
org/10.1109/CSICC52343.2021.9420563.
Gao, and Yejin Choi. Piqa: Reasoning about physical
commonsenseinnaturallanguage,2019.
MichaelCFrank. Bridgingthedatagapbetweenchildren
andlargelanguagemodels. TrendsinCognitiveSciences,
Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng,
2023.
JasonD.Lee,DemingChen,andTriDao. Medusa: Sim-
plellminferenceaccelerationframeworkwithmultiple
BogdanGliwa, IwonaMochol, MaciejBiesek, andAlek-
decodingheads,2024.
sander Wawer. Samsum corpus: A human-annotated
dialoguedatasetforabstractivesummarization. InPro-
RichCaruana. Multitasklearning. Machinelearning,28:
ceedings of the 2nd Workshop on New Frontiers in
41–75,1997.
Summarization.AssociationforComputationalLinguis-
MarkChen,JerryTworek,HeewooJun,QimingYuan,Hen- tics, 2019. doi: 10.18653/v1/d19-5409. URL http:
riquePonde,JaredKaplan,HarriEdwards,YuraBurda, //dx.doi.org/10.18653/v1/D19-5409.
Nicholas Joseph, Greg Brockman, et al. Evaluating
SachinGoyal,ZiweiJi,AnkitSinghRawat,AdityaKrishna
largelanguagemodelstrainedoncode. arXivpreprint
Menon,SanjivKumar,andVaishnavhNagarajan. Think
arXiv:2107.03374,2021.
beforeyouspeak: Traininglanguagemodelswithpause
NakhunChumpolsathien.Usingknowledgedistillationfrom tokens,2023.
keywordextractiontoimprovetheinformativenessofneu-
DanHendrycks,StevenBasart,SauravKadavath,Mantas
ralcross-lingualsummarization. Master’sthesis,Beijing
Mazeika,AkulArora,EthanGuo,CollinBurns,Samir
InstituteofTechnology,2020.
Puranik,HoraceHe,DawnSong,etal. Measuringcod-
KarlCobbe,VineetKosaraju,MohammadBavarian,Mark ing challenge competence with apps. arXiv preprint
Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, arXiv:2105.09938,2021.
Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al.
AriHoltzman,JanBuys,LiDu,MaxwellForbes,andYejin
Training verifiers to solve math word problems. arXiv
Choi. Thecuriouscaseofneuraltextdegeneration,2020.
preprintarXiv:2110.14168,2021.
JianyuZhangLeonBottou. Multi-labelclassificationasan
LiDong,NanYang,WenhuiWang,FuruWei,XiaodongLiu,
auxiliarylossforlanguagemodelling. personalcommu-
Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen
nication,2024.
Hon. Unifiedlanguagemodelpre-trainingfornaturallan-
guageunderstandingandgeneration. InProceedingsof MandarJoshi,EunsolChoi,DanielS.Weld,andLukeZettle-
the33rdInternationalConferenceonNeuralInformation moyer. Triviaqa: Alargescaledistantlysupervisedchal-
ProcessingSystems,pages13063–13075,2019. lengedatasetforreadingcomprehension,2017.
Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Diederik Kingma and Jimmy Ba. Adam: A method for
YossiAdi. Highfidelityneuralaudiocompression. arXiv stochasticoptimization. ICLR,2015.
preprintarXiv:2210.13438,2022.
Ryan Koo, Minhwa Lee, Vipul Raheja, Jong Inn Park,
MoussaKamalEddine,AntoineJ.P.Tixier,andMichalis ZaeMyungKim,andDongyeopKang. Benchmarking
Vazirgiannis. Barthez: a skilled pretrained french cognitivebiasesinlargelanguagemodelsasevaluators.
sequence-to-sequencemodel,2021. arXivpreprintarXiv:2309.17012,2023.
10Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
TomKwiatkowski,JennimariaPalomaki,OliviaRedfield, summarizationusingsequence-to-sequencernnsandbe-
MichaelCollins,AnkurParikh,ChrisAlberti,Danielle yond,2016.
Epstein,IlliaPolosukhin,MatthewKelcey,JacobDevlin,
ShashiNarayan,ShayB.Cohen,andMirellaLapata. Don’t
KentonLee,KristinaN.Toutanova,LlionJones,Ming-
givemethedetails,justthesummary! topic-awarecon-
WeiChang,AndrewDai,JakobUszkoreit,QuocLe,and
volutionalneuralnetworksforextremesummarization,
SlavPetrov. Naturalquestions: abenchmarkforquestion
2018.
answeringresearch. TransactionsoftheAssociationof
ComputationalLinguistics,2019. Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas
Joseph, Nova DasSarma, Tom Henighan, Ben Mann,
GuillaumeLample,Marie-AnneLachaux,ThibautLavril,
Amanda Askell, Yuntao Bai, Anna Chen, Tom Con-
Xavier Martinet, Amaury Hayat, Gabriel Ebner, Au-
erly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds,
rélienRodriguez,andTimothéeLacroix. Hypertreeproof
Danny Hernandez, Scott Johnston, Andy Jones, Jack-
searchforneuraltheoremproving,2022.
son Kernion, Liane Lovitt, Kamal Ndousse, Dario
Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam
YannLeCun. Apathtowardsautonomousmachineintelli-
genceversion0.9.2,2022-06-27. OpenReview,62(1), McCandlish, and Chris Olah. In-context learning
and induction heads. Transformer Circuits Thread,
2022.
2022. https://transformer-circuits.pub/2022/in-context-
BenjaminLefaudeux,FranciscoMassa,DianaLiskovich, learning-and-induction-heads/index.html.
WenhanXiong,VittorioCaggiano,SeanNaren,MinXu,
OpenAI. Gpt-4technicalreport,2023.
JieruHu,MartaTintore,SusanZhang,PatrickLabatut,
and Daniel Haziza. xformers: A modular and hack- LongOuyang,JeffWu,XuJiang,DiogoAlmeida,CarrollL.
abletransformermodellinglibrary.https://github. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
com/facebookresearch/xformers,2022. Agarwal,KatarinaSlama,AlexRay,JohnSchulman,Ja-
cobHilton,FraserKelton,LukeMiller,MaddieSimens,
YanivLeviathan, MatanKalman, andYossiMatias. Fast
Amanda Askell, Peter Welinder, Paul Christiano, Jan
inference from transformers via speculative decoding,
Leike, and Ryan Lowe. Training language models to
2023.
followinstructionswithhumanfeedback,2022.
Yujia Li, David Choi, Junyoung Chung, Nate Kush-
KoyenaPal,JiudingSun,AndrewYuan,ByronC.Wallace,
man, Julian Schrittwieser, Rémi Leblond, Tom Eccles,
and David Bau. Future lens: Anticipating subsequent
JamesKeeling,FelixGimeno,AgustinDalLago,etal.
tokensfromasinglehiddenstate,2023.
Competition-levelcodegenerationwithalphacode. Sci-
ence,378(6624):1092–1097,2022. Weizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan
Duan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou.
Chin-YewLin. ROUGE:Apackageforautomaticevalu- Prophetnet: Predicting future n-gram for sequence-to-
ation of summaries. In Text Summarization Branches sequencepre-training,2020.
Out, pages 74–81, Barcelona, Spain, July 2004. Asso-
ciation for Computational Linguistics. URL https: Jesse Read, Bernhard Pfahringer, Geoffrey Holmes, and
//aclanthology.org/W04-1013. EibeFrank. Classifierchains: Areviewandperspectives.
JournalofArtificialIntelligenceResearch,70:683–718,
ZhenghaoLin,ZhibinGou,YeyunGong,XiaoLiu,Yelong 2021.
Shen,RuochenXu,ChenLin,YujiuYang,JianJiao,Nan
MelissaRoemmele,CosminAdrianBejan,andAndrewS
Duan,andWeizhuChen. Rho-1: Notalltokensarewhat
Gordon. Choiceofplausiblealternatives: Anevaluation
youneed,2024.
ofcommonsensecausalreasoning. In2011AAAISpring
IlyaLoshchilovandFrankHutter. Sgdr: Stochasticgradient SymposiumSeries,2011.
descentwithwarmrestarts,2017.
MaartenSap,HannahRashkin,DerekChen,RonanLeBras,
IlyaLoshchilovandFrankHutter. Decoupledweightdecay and Yejin Choi. Socialiqa: Commonsense reasoning
regularization,2019. aboutsocialinteractions,2019.
DavidSilver,AjaHuang,ChrisJMaddison,ArthurGuez,
MichaelMathieu,CamilleCouprie,andYannLeCun. Deep
LaurentSifre,GeorgeVanDenDriessche,JulianSchrit-
multi-scalevideopredictionbeyondmeansquareerror,
twieser, Ioannis Antonoglou, Veda Panneershelvam,
2016.
Marc Lanctot, et al. Mastering the game of go with
RameshNallapati,BowenZhou,CiceroNogueiradossan- deepneuralnetworksandtreesearch. nature,529(7587):
tos,CaglarGulcehre,andBingXiang. Abstractivetext 484–489,2016.
11Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
AadityaKSingh,StephanieCYChan,TedMoskovitz,Erin
Grant, Andrew M Saxe, and Felix Hill. The transient
nature of emergent in-context learning in transformers.
arXivpreprintarXiv:2311.08360,2023.
Eleftherios Spyromitros-Xioufis, Grigorios Tsoumakas,
WilliamGroves,andIoannisVlahavas. Multi-targetre-
gression via input space expansion: treating targets as
inputs. MachineLearning,104:55–98,2016.
NitishSrivastava,ElmanMansimov,andRuslanSalakhut-
dinov. Unsupervised learning of video representations
usinglstms,2016.
MitchellStern,NoamShazeer,andJakobUszkoreit. Block-
wise parallel decoding for deep autoregressive models,
2018.
Yi Tay, Mostafa Dehghani, Vinh Q Tran, Xavier Gar-
cia,JasonWei,XuezhiWang,HyungWonChung,Sia-
mak Shakeri, Dara Bahri, Tal Schuster, et al. Ul2:
Unifyinglanguagelearningparadigms. arXivpreprint
arXiv:2205.05131,2022.
Vladimir Vapnik and Akshay Vashist. A new learning
paradigm: Learningusingprivilegedinformation. Neural
networks,22(5-6):544–557,2009.
Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.
Anticipatingvisualrepresentationsfromunlabeledvideo,
2016.
Willem Waegeman, Krzysztof Dembczyn´ski, and Eyke
Hüllermeier. Multi-target prediction: a unifying view
onproblemsandmethods. DataMiningandKnowledge
Discovery,33:293–324,2019.
VikasYadav,StevenBethard,andMihaiSurdeanu. Quick
and (not so) dirty: Unsupervised selection of justifica-
tionsentencesformulti-hopquestionanswering. arXiv
preprintarXiv:1911.07176,2019.
ZhilinYang,ZihangDai,YimingYang,JaimeCarbonell,
Russ R Salakhutdinov, and Quoc V Le. Xlnet: Gen-
eralized autoregressive pretraining for language under-
standing. InAdvancesinneuralinformationprocessing
systems,pages5753–5763,2019.
RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,
andYejinChoi. Hellaswag: Canamachinereallyfinish
yoursentence?,2019.
12Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
A.Additionalresultsonself-speculativedecoding
1.0
3.0
2.5 0.8
2.0
0.6
1.5
0.4
k=1 k=1
1.0
k=2 k=2
0.2
0.5 k=3 k=3
k=4 k=4
0.0 0.0
1 8 16 24 32 40 1 8 16 24 32 40
Batch size Batch size
FigureS10: Decodingspeedsandlatencieswithself-speculativedecodingrelativetostandardautoregressivedecoding.
Weusekheadsofa4-tokenpredictionmodelandevaluatedecodingspeedsofacodemodelasexplainedinTableS2. All
numbersarerelativetotheautoregressive(k =1)baselinewiththesamebatchsize.
TableS2: Relativespeedupswithself-speculativedecoding. Forwikipediaandbooksweprompta7Bparametermodel
trainedon500Btokens,andforcodeweprompta7Bparametermodeltrainedon1Ttokensofcodeon4200sequencesof
512tokensfromatestdatasetnotseenduringtraining,andgeneratecompletionsconsistingof512tokensusinggreedy
self-speculativedecoding(Sternetal.,2018)usingtheindicatednumberofheadsfroma4-tokenpredictionmodel. Note
thatthemaximalspeedupthatcanbeobtainedwithself-speculativedecodingusingkheadsisk. Thelastcolumnshowsthe
averagenumberoftokensretrievedfromaforwardcontainingthissequence(bothverificationandprediction). Thespeedup
wasevaluatedatthemaximalbatchsizeof42,butisconstantacrossbatchsizes(FigureS10).
Wikipedia Books Code
#Headsused Rel.speedup Tokens/forward Rel.speedup Tokens/forward Rel.speedup Tokens/forward
1 1.00 1.00 1.00 1.00 1.00 1.00
2 1.79 1.88 1.77 1.87 1.85 1.94
3 2.35 2.57 2.32 2.56 2.54 2.78
4 2.74 3.12 2.67 3.09 3.05 3.50
TableS3:Relativespeedupswithself-speculativedecodingwithbyte-levelmodelsoncode. Wepromptthe7Bparameter
modelsfromSection3.3on4096sequencesof1024bytesofcodenotseenduringtraining, andgeneratecompletions
consisting of 1024 bytes using greedy self-speculative decoding (Stern et al., 2018) as in Table S2. The speedup was
evaluatedatabatchsizeof16.
n=8 n=16 n=32
#Headsused Rel.speedup Tokens/forward Rel.speedup Tokens/forward Rel.speedup tokens/forward
1 1.00 1.00 1.00 1.00 1.00 1.00
2 1.94 1.98 1.94 1.98 1.93 1.97
4 3.67 3.84 3.63 3.81 3.62 3.80
8 6.39 7.04 6.25 6.92 6.22 6.89
12 − − 8.07 9.36 8.01 9.30
16 − − 9.24 11.20 9.15 11.15
20 − − − − 9.83 12.61
24 − − − − 10.34 13.67
28 − − − − 10.55 14.58
32 − − − − 10.84 15.35
13
)evitaler(
tuphguorhT
)evitaler(
ycnetaLBetter&FasterLargeLanguageModelsviaMulti-tokenPrediction
B.Alternativearchitectures
TableS4: Alternativearchitecturesimproveonbaselinebutnotasconsistently. Alternativearchitecturesformulti-token
predictionareworthexploringtoimproveefficiency. HerewetriedAnticausal,causalandlinearandshowednosignificant
improvementwithrespecttoParallelarchitecture.
MBPP HumanEval APPS/Intro
n Headtype Architecture +Layers @1 @10 @100 @1 @10 @100 @1 @10 @100
1 transformer parallel 0 30.0 53.8 73.7 22.8 36.4 62.0 2.8 7.8 17.4
linear parallel 0 33.6 55.0 76.2 21.9 38.5 63.7 3.1 10.1 23.0
anticausal 0 30.8 54.8 75.3 20.9 38.4 64.5 2.0 8.7 21.6
4
transformer causal 0 31.9 54.9 74.9 20.9 38.1 67.3 4.0 11.6 22.8
0 33.8 55.9 76.9 24.0 40.1 66.1 1.6 7.1 19.9
parallel
3 33.3 55.7 77.3 22.4 39.4 66.7 2.6 9.5 22.1
ThearchitecturedescribedinSection2isnottheonlysensibleoption,butprovedtechnicallyviableandwell-performingin
ourexperiments. Wedescribeandcomparealternativearchitecturesinthissection.
Replicatedunembeddings Replicatingtheunembeddingmatrixntimesisasimplemethodforimplementingmulti-token
predictionarchitectures. However,itrequiresmatriceswithshapes(d,nV)inthenotationofSection2,whichisprohibitive
forlarge-scaletrainings.
Linearheads Apartfrom usinga singletransformerlayer forthe headsH , otherarchitectures areconceivable. We
i
experimented with a single linear layer without any nonlinearity as heads, amounting to linear probing of the model’s
residualrepresentationz. Architectureswithmorethanonelayerperheadarealsopossible,butwedidnotpursuethis
directionfurther.
Causalandanticausalvariant InsteadofmakingthepredictionheadsP (x |z )architecturallyindependentofeach
i t+i t:1
other,wecanalsoallowthemtorelyonotherheads’(pre-unembedding)outputs. Inacausalvariant,laterpredictionheads
areappliedontopofthepreviousones,i.e. thei-thpredictionheadP isgivenby
i
P (x |·)=softmax◦f ◦f ◦f ···◦f ◦f .
θ t+i u hi hi−1 h1 s
Inanotheranticausalvariant,thenetworkstartsbypredictingthemostdistanttokensbeforegraduallyrefininguptothe
followingtoken:
P (x |·)=softmax◦f ◦f ◦f ···◦f ◦f .
θ t+i u hi hi+1 hn s
Thesearchitectureslikewiseallowasequentialforward/backwardorderastheparallelarchitecturefromSection2. Thisis
describedinFigureS11.
14Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
4
Head 2 Loss 2
5
3 6
7
Head 1 Loss 1
8
2 9
Trunk
1 10
Input
FigureS11: Orderoftheforward/backwardinacausaln-tokenpredictionmodelwithn = 2heads. Likeinthe
forward/backwarddepictedforparallelpredictionheadsinFigure2,weavoidmaterializingallunembeddinglayergradients
inmemorysimultaneouslyandreducepeakGPUmemoryusagesignificantly. Theiterationovertheheadsstartswiththe
onefurthesttothetrunk. Ateachhead,agradientfromthesucceedingpredictionheadsandfromthehead’sownlossare
accumulatedforboththehead’soutputanditsweights.
C.Trainingspeeds
TableS5:Trainingtimerelativetonext-tokenpredictiontraining. Theslightoverheadwhenusingmulti-tokenprediction
hereisexplainedbyasuboptimaluseofFullyShardedDataParallel. Inourimplementation,whendoingseparatebackward
passesforeachhead,welosetheoverlapoflayerweightcommunicationandcomputation,thereforeitincursaveryslight
overheadthatcanberemovedifreimplementedcorrectly.
Model n=1 n=2 n=4
0.3B 1.00 1.07 1.22
0.6B 1.00 1.05 1.13
1.3B 1.00 1.04 1.12
3B 1.00 1.02 1.07
6.7B 1.00 1.02 1.07
13B 1.00 1.04 1.09
D.Finetuning
TableS6: FinetuningLLama2withmulti-tokenpredictiondoesnotsignificantlyimproveperformance. Wetriedto
finetuneLLama2with4-tokenpredictionbutthisdidnotyieldsignificantimprovementscomparedtothebaseline. We
supposethatthisnewlosschangestheinitializationtoobrutallyandneverreallyrecovers. Westillsomeimprovementsfor
exampleonMBPPPass@1. Allrunsuse200Btokensofcode.
MBPP HumanEval APPS/Intro
n Headtype +Layers @1 @10 @100 @1 @10 @100 @1 @10 @100
1 transformer 0 39.6 65.1 82.4 31.4 57.7 84.7 10.0 21.6 36.7
linear 0 39.3 63.7 81.3 29.0 53.4 82.2 6.9 20.0 34.0
4
0 38.3 62.2 80.1 27.9 53.6 82.4 5.8 18.2 34.3
transformer
3 42.5 64.4 81.3 28.7 56.9 82.4 7.8 21.2 37.3
15Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
E.Additionalresultsonmodelscalingbehavior
TableS7: ScalingmodelsizeFullresultsofscalingmodelsizewithn=1,2and4.
MBPP HumanEval
ModelSize Fut @1 @10 @100 @1 @10 @100
1 1.8 10.4 29.9 1.9 5.0 10.9
0.3B 2 1.7 10.1 27.2 1.5 4.4 10.3
4 1.0 6.3 20.1 1.2 4.0 8.6
1 4.7 21.0 45.2 2.9 8.5 16.7
0.6B 2 4.6 21.0 44.7 3.2 8.9 16.2
4 3.0 15.6 38.0 2.7 7.7 15.5
1 6.8 27.0 51.0 4.6 13.1 24.3
1.3B 2 7.3 27.5 51.7 5.4 13.6 23.3
4 7.4 27.6 50.1 4.8 12.3 22.5
1 11.1 36.4 60.4 7.2 17.2 29.8
3B 2 11.8 37.2 60.5 8.0 18.2 31.2
4 12.7 37.6 61.1 7.2 18.5 33.3
1 23.9 54.2 74.7 12.8 29.3 51.7
6.7B 2 24.7 54.8 76.4 13.2 32.2 53.9
4 26.0 55.8 76.0 13.8 33.2 58.5
1 26.0 57.1 77.0 14.1 33.6 56.0
13B 2 30.5 60.5 79.4 15.2 36.9 60.0
4 30.5 61.0 79.2 15.8 38.6 63.5
F.DetailsonCodeContestsfinetuning
WeusethePythonsubsetoftheCodeContests(Lietal.,2022)trainsplitwithrewardannotations(“correct”/“incorrect”)
andconditiononcorrectsolutionsatevaluationtime. Forevaluation,wegenerate1000samplesperproblemfromthetest
splitforeachtemperatureT ∈{0.5,0.6,0.7,0.8,0.9},andcomputetheunbiasedestimatorforpass@kfromChenetal.
(2021)foreachvalueofkandT.Itispossiblethatmodelsthatwerepretrainedwithdifferentlosseshavedifferentrespective
optimaltemperaturesforpass@k,sowecomputeandshowk (cid:55)→max pass_at(k,T)inFigure4. Inotherwords,wegrant
T
pass@kaccesstoatemperatureoracle. Forsmallvaluesofk,pass@kmeasuresthecapabilityofunderstandingandsolving
taskswhileforlargek,itadditionallyfavorsdiversityinoutputs. AccordingtotheresultsinFigure4,multi-tokenprediction
pretrainingleadstofinetunedmodelsthatarebetteronbothaxes.
16Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
G.Additionalresultsonnaturallanguagebenchmarks
WeevaluatethemodelsfromSection3.7onstandardnaturallanguageprocessingbenchmarks: ARCChallenge(Yadav
etal.,2019),COPA(Roemmeleetal.,2011),Hellaswag(Zellersetal.,2019),NaturalQuestions(Kwiatkowskietal.,2019),
PIQA(Bisketal.,2019),SIQA(Sapetal.,2019)andTriviaQA(Joshietal.,2017).
arc_challenge copa hellaswag
80
35 60
30 70 50
40
25
nq piqa siqa
15
75 46
n
10
1
70 44
2
5
4
65 42
10000 20000 10000 20000
global_step global_step
tqa
40
30
20
10
10000 20000
global_step
FigureS12: Multipletokentrainingwith7Bmodelsdoesn’timproveperformanceonchoicetasks. Thisfigureshows
theevolutionofaverageaccuracyofsomestandardNLPbenchmarks(ARCChallengeCOPAHellaswagMMLUNatural
Questions PIQA SIQA and TriviaQA. For the 7B models trained on 200B tokens of language data, the 2 future token
modelhasthesameperformanceasthebaselineandthe4futuretokenmodelregressesabit. Largermodelsizesmightbe
necessarytoseeimprovementsonthesetasks.
17
eulav
eulav
eulavBetter&FasterLargeLanguageModelsviaMulti-tokenPrediction
H.Additionalresultsonabstractivetextsummarization
Inthissection,wereportcomprehensiveevaluationresultsonsummarizationtasksforthe7Bparametermodelstrainedon
200Band500BtokensofnaturallanguagefromSection3.7.
TableS8: Comprehensiveevaluationonabstractivetextsummarization. ROUGE-n(n-gramoverlap)andROUGE-L
(longestcommonsubsequenceoverlap)F scoresfor7Bmodelstrainedon200Band500Btokensofnaturallanguage,
1
respectively. Thelastthreecolumnscorrespondtomodelstrainedon500Btokens,thepreviousthreetomodelstrainedon
200Btokens. Shownarenumbersofthen = 1baselineandtheabsolutedifferenceofn = 2andn = 4modelstrained
onthesamenumberoftokens. Summary-levelROUGE-L(“ROUGE-L ”)isreportedwhereitdiffersfromROUGE-L.
sum
ModelcheckpointswithmaximalvalidationROUGE-LF areselectedseparatelyforeachmodeldatasetandmodeltype
1
andreportedinthefirstrowcorrespondingtoeachdataset. Boldfacefornumberswithin0.05differencetothebestonefor
eachdatasetsizeseparately.
Task Metric Baseline200B ∆ ∆ Baseline500B ∆ ∆
n=2 n=4 n=2 n=4
evaluationepoch 2 2 2 2 2 2
ROUGE-1 42.88 +0.74 +0.74 43.77 +0.55 +0.50
ROUGE-2 19.56 +0.52 +0.53 20.34 +0.52 +0.34
CNN/Dailymail(Nallapatietal.,2016)
ROUGE-3 11.11 +0.39 +0.35 11.69 +0.36 +0.19
ROUGE-L 29.72 +0.66 +0.49 30.51 +0.48 +0.37
ROUGE-Lsum 40.18 +0.72 +0.68 41.02 +0.56 +0.52
evaluationepoch 1 3 3 2 3 2
ROUGE-1 44.48 +1.70 +1.72 45.87 +1.05 +0.69
Multi-News(Fabbrietal.,2019) ROUGE-2 16.88 +0.44 +0.70 17.56 +0.42 +0.40
ROUGE-3 9.63 -0.06 +0.17 9.91 +0.22 +0.18
ROUGE-L 23.82 +0.17 +0.40 24.22 +0.20 +0.26
evaluationepoch 2 2 3 2 1 3
ROUGE-1 32.95 +0.41 +0.35 33.37 +0.32 +0.78
OrangeSum(Eddineetal.,2021) ROUGE-2 13.90 +0.31 +0.36 14.22 +0.25 +0.53
ROUGE-3 8.01 +0.19 +0.21 8.12 +0.22 +0.48
ROUGE-L 23.62 +0.36 +0.51 23.91 +0.23 +0.66
evaluationepoch 1 1 1 1 2 3
ROUGE-1 1.03 +0.02 0.00 0.92 +0.09 +0.05
pn-summary(Farahanietal.,2021) ROUGE-2 0.13 +0.02 +0.03 0.15 0.00 0.00
ROUGE-3 0.02 0.00 +0.02 0.02 0.00 +0.02
ROUGE-L 1.02 +0.03 +0.01 0.91 +0.09 +0.05
evaluationepoch 3 3 3 3 3 3
ROUGE-1 51.39 +0.70 +0.63 52.54 -0.24 +0.69
SAMSum(Gliwaetal.,2019) ROUGE-2 26.46 +0.76 +0.30 27.74 -0.20 +0.82
ROUGE-3 16.40 +0.91 +0.28 17.56 -0.30 +0.71
ROUGE-L 42.59 +0.90 +0.51 43.92 -0.10 +0.63
evaluationepoch 2 3 3 3 3 3
ROUGE-1 45.08 +0.63 +1.12 45.48 +0.77 +0.91
ThaiSum(Chumpolsathien,2020) ROUGE-2 27.85 +0.30 +0.73 28.07 +0.74 +0.64
ROUGE-3 15.73 +0.04 +0.43 15.82 +0.50 +0.30
ROUGE-L 44.92 +0.64 +1.12 45.31 +0.76 +0.89
evaluationepoch 3 3 3 3 3 3
ROUGE-1 10.16 +0.67 -0.23 12.80 -0.17 -0.99
WikiSummary(Farahani,2020) ROUGE-2 4.46 -0.03 -0.09 6.17 -0.11 -0.69
ROUGE-3 1.31 +0.21 +0.13 1.98 -0.08 -0.33
ROUGE-L 10.11 +0.65 -0.28 12.69 -0.17 -0.99
evaluationepoch 2 2 3 2 2 3
ROUGE-1 42.16 +0.71 +1.07 43.42 +0.78 +0.67
XSum(Narayanetal.,2018) ROUGE-2 19.19 +0.54 +0.55 20.32 +0.68 +0.34
ROUGE-3 10.43 +0.38 +0.28 11.23 +0.48 +0.20
ROUGE-L 34.03 +0.67 +0.92 35.18 +0.79 +0.63
18Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
TableS9: Performanceonabstractivetextsummarization. ROUGE-L(longestcommonsubsequenceoverlap)F score
1
for7Bmodelstrainedon200Band500Btokensofnaturallanguage. Wefinetunetherespectivemodelsoneachtask’s
training data separately for a given number of epochs and select the checkpoints with maximal ROUGE-L F on the
1
validationdataset. Thesecondandfifthcolumnreportthenumbersforanext-tokenpredictionmodel,whilethethird,fourth,
sixthandseventhonereporttheabsoluteimprovementsfor2-tokenand4-tokenpredictionmodelstrainedonthesame
amountofdata,respectively. Boldfacefornumberswithin0.05differencetothebestoneforeachdatasetsizeseparately.
Dataset Baseline200B ∆ ∆ Baseline500B ∆ ∆
n=2 n=4 n=2 n=4
CNN/Dailymail 29.72 +0.66 +0.49 30.51 +0.48 +0.37
Multi-News 23.82 +0.17 +0.40 24.22 +0.20 +0.26
OrangeSum 23.62 +0.36 +0.51 23.91 +0.23 +0.66
pn-summary 1.02 +0.03 +0.01 0.91 +0.09 +0.05
SAMSum 42.59 +0.90 +0.51 43.92 -0.10 +0.63
ThaiSum 44.92 +0.64 +1.12 45.31 +0.76 +0.89
WikiSummary 10.11 +0.65 -0.28 12.69 -0.17 -0.99
XSum 34.03 +0.67 +0.92 35.18 +0.79 +0.63
Average 26.23 +0.51 +0.46 27.08 +0.28 +0.31
TableS10: Summarystatisticsforabstractivetextsummarizationevaluations. ReportedareaveragesforROUGE-nand
ROUGE-LmetricsacrossalldatasetsfromTableS8,separatelyforprecision,recallandF score. Both2-tokenand4-token
1
predictionmodelsoutperformthenext-tokenpredictionbaseline. Trainedon500Btokens,4-tokenpredictionmodelsappear
betteratrecallmetricswhile2-tokenpredictionmodelsappearbetteratprecisionmetrics. Modelcheckpointsareselected
asdescribedinTableS8. Boldfacefornumberswithin0.05differencetothebestoneforeachdatasetsizeseparately.
Metric Aspect Baseline200B ∆ ∆ Baseline500B ∆ ∆
n=2 n=4 n=2 n=4
F 33.77 +0.70 +0.68 34.77 +0.39 +0.41
1
ROUGE-1 precision 35.76 +0.88 +0.83 37.03 +0.42 -0.04
recall 34.37 +0.45 +0.45 35.14 +0.35 +0.68
F 16.06 +0.36 +0.39 16.82 +0.29 +0.30
1
ROUGE-2 precision 16.97 +0.40 +0.43 17.91 +0.29 +0.03
recall 16.34 +0.28 +0.35 16.99 +0.32 +0.48
F 9.08 +0.26 +0.23 9.54 +0.18 +0.22
1
ROUGE-3 precision 9.59 +0.29 +0.28 10.17 +0.18 +0.05
recall 9.26 +0.21 +0.20 9.65 +0.21 +0.35
F 26.23 +0.51 +0.46 27.08 +0.28 +0.31
1
ROUGE-L precision 27.79 +0.62 +0.55 28.85 +0.28 -0.09
recall 26.71 +0.37 +0.32 27.40 +0.28 +0.57
F 27.53 +0.52 +0.48 28.40 +0.29 +0.33
1
ROUGE-L precision 29.07 +0.64 +0.58 30.15 +0.29 -0.08
sum
recall 28.13 +0.35 +0.33 28.81 +0.29 +0.60
19Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
I.Additionalresultsonmathematicalreasoninginnaturallanguage
200B tokens 500B tokens
8
n = 1
n = 2
3 6
n = 4
4
2
2
20 30
20
15
60
60
50
40
40
0.2 0.4 0.6 0.8 1.0 1.2 1.4 0.2 0.4 0.6 0.8 1.0 1.2 1.4
Temperature Temperature
Figure S13: Performance on the mathematical reasoning benchmark GSM8K (Cobbe et al., 2021). We evaluate
pretrainednext-tokenandmulti-tokenpredictionmodelstrainedon200Band500Btokensofnaturallanguagein8-shot
mode using nucleus sampling (Holtzman et al., 2020) with probability mass 0.95 and various sampling temperatures.
Reportedarethefrequenciesofthecorrectfinalanswertoappearamongk samples,fork = 1,10,100,estimatedfrom
200sampleslikeincodegenerationbenchmarks(Chenetal.,2021). After200Btokens,the2-tokenpredictionmodel
hasaclearadvantageoverthenext-tokenbaselinebuttheorderreversesafter500Btokens. The4-tokenpredictionmodel
isworsethroughout. WeinterpretthissimilarlytothefindingsinSection4.1: thefollow-your-nosechains-of-thought
requiredforGSM8Kmaybedifficulttolearnfromalimitedamountofdata,attestingtothedataefficiencyofmulti-token
prediction training. Once the correct circuits for correct autoregressive chains-of-thought in this domain have formed,
however,multi-tokenpredictioncomesatacost.
20
)%(
01@ssap
)%(
001@ssap
)%(
1@ssapBetter&FasterLargeLanguageModelsviaMulti-tokenPrediction
J.Additionalresultsoninductionlearning
1.000
0.975
0.950
0.925
0.900
0.875 n=1 (baseline)
n=2 (ours)
0.850
1 3 10 30 100 300 1000
Parameters (M)
FigureS14:Inductioncapabilityofn-tokenpredictionmodelstrainedonhigher-qualitydata. Shownisaccuracyonthe
secondtokenoftwotokennamesthathavealreadybeenmentionedpreviously. Trainingona9:1mixofabooksdatasetand
thechildrenstoriydataset,weobservethatinductioncapabilityformssignificantlyearlierintraining(notshownhere)andto
ahigherdegree. Webelievethatthisisexplainedbothbecauseourevaluationdatasetnolongercontainsout-of-distribution
tokens(Section4.1)andbecausethehigher-qualitydatacontainedinthebooksdatasetmakesinductionnecessaryearlieron
(especiallyforsmallmodels,cf. Singhetal.(2023)). Inparticular,byenforcingtheformationofinductioncapabilityinthe
modelbymeansofthedataset–insteadoftheloss–theadvantageof2-tokenpredictionmodelsonthistaskdisappears
exceptforthesmallestmodels: featurelearningconvertsthetaskintoapurenext-tokenpredictiontask.
21
sseccus
noitcudnIBetter&FasterLargeLanguageModelsviaMulti-tokenPrediction
K.Additionalresultsonalgorithmicreasoning
Weinvestigatethefollowingcomputation-sharinghypothesisforexplainingtheefficacyofmulti-tokenpredictionastraining
loss.
Thepredictiondifficultyofdifferenttokensinnaturaltextvariesgreatly. Sometokensmaybethecontinuations
ofpartialwordsthatareuniquelydeterminedfromtheirprecedingcontextwithoutanyeffort,whileothersmay
require to predict theorem names in difficult mathematical proofs or the correct answer to an exam question.
Languagemodelswithresidualconnectionshavebeenshowntorefinetheiroutputtokendistributionwitheach
successive layer, and can be trained with early exit strategies that spend variable amounts of computational
resourcespertokenposition. Multi-tokenpredictionlossesexplicitlyencourageinformation-sharingbetween
adjacent token positions and can thus be viewed as a method to learn allocating computational resources in
languagemodelsmoreefficientlytothetokensthatbenefitmostofit.
Tocheckthetruthofthishypothesis,weaugmentthepolynomialarithmetictaskfromSection4.2withavaryingnumberof
pausetokens(Goyaletal.,2023)insertedbetweenthequestionandatokenthatdenotesthebeginningoftheanswer. Pause
tokensintroduceadditionalcomputationalresourcesthatcanbeexpendedforcomputationsthatareexpectedtobeuseful
lateroninthesequence,inotherwords:tostartthinkingabouttheanswer. Accordingtothecomputation-sharinghypothesis,
multi-tokenpredictionmodelslearninformation-sharingandthuscomputation-sharingbetweentokenpositionsmoreeasily,
andmaybebetteratmakinguseoftheseadditionalcomputationalresourcesthannext-tokenpredictionmodelsare. In
FigureS15,weshowtheevaluationresultsonthepolynomialarithmetictaskwithafixednumberofpausetokensinserted
bothattrainingandevaluationtime. Multi-tokenpredictionmodelslikewiseoutperformnext-tokenpredictionmodels
onthesetaskvariantsacrosstaskdifficultiesandmodelsizes. However,wedonotseestrongevidenceofawideningor
shrinkingofthisgapi.e. wecannotconcludefromtheseexperimentsontheveracityofthecomputation-sharinghypothesis.
InTableS11,wereportresultsfromanotherexperimentinthesamespirit: byaddingspacesandnewlinestoHumanEval
andMBPPprompts,weadd“pausetokens”inasomewhatnaturalway. Accordingtotheseresults,multi-tokenprediction
modelshaveaslightadvantageatusingthisadditionallyprovidedcompute,buttheeffectismarginal.
100 100
n=1 n=1
n=2 n=2
80 80
n=4 n=4
60 60
40 40
20 20
0 0
11 22 33 44 55 66 77 88 99 1100 11 22 33 44 55 66 77 88 99 1100
# operations # operations
in-domain out-of-domain in-domain out-of-domain
(a)5pausetokens (b)10pausetokens
FigureS15: Accuracyonapolynomialarithmetictaskwithvaryingnumberofoperationsperexpressionandpause
tokens. WetrainandevaluatemodelsonthepolynomialarithmetictaskdescribedinSection4.2,modifiedbytheaddition
ofpausetokens(Goyaletal.,2023): betweenthequestionandtheequalitysignthatindicatesthebeginningoftheanswer,
weaddaconstantnumberofpausetokensbothintrainingandevaluation. Forbothavariantwithfiveandwithtenpause
tokens,respectively,weobservecomparableimprovementsfromusingmulti-tokenpredictiontotheonesobtainedinthe
casewithoutpausetokens(Figure8).
22
)%(
ycaruccA
)%(
ycaruccABetter&FasterLargeLanguageModelsviaMulti-tokenPrediction
TableS11: Utilizationofadditionalwhitespacetokensincodebenchmarks.
Task Whitespace n=1 n=4
APPS/Intro spaces+newline +0.21 +0.34
APPS/Intro newline +0.79 +0.69
HumanEval spaces+newline -0.72 -0.16
HumanEval newline -0.26 +0.10
MBPP spaces+newline -0.10 -0.06
MBPP newline +0.03 -0.08
Average -0.01 +0.14
30M, n=1
100
30M, n=2
30M, n=4
100M, n=1
100M, n=2
100M, n=4
80
60
40
20
0
11 22 33 44 55 66 77 88 99 1100
# operations
in-domain out-of-domain
FigureS16: Accuracyonapolynomialarithmetictaskfortwomodelsizes. Wetrainandevaluatemodelswith30Mand
100MparametersonthepolynomialarithmetictaskdescribedinSection4.2. Triplingthemodelsizehasasmallereffect
onperformancethanreplacingnext-tokenpredictionlossbymulti-tokenprediction. Shownaretwoindependentrunsper
configurationandtheirmeans,the100MparametermodelsbeingidenticaltotheonesinFigure8.
23
)%(
ycaruccABetter&FasterLargeLanguageModelsviaMulti-tokenPrediction
TableS12: Optimaltemperaturesforallnumbersintable 1
MBPP HumanEval APPS/Intro
Trainingdata Vocabulary n
@1 @10 @100 @1 @10 @100 @1 @10 @100
1 0.2 0.8 0.8 0.1 0.8 0.8 0.8 0.8 0.8
313Bbytes 8 0.1 0.8 0.8 0.1 0.8 0.8 0.4 0.4 0.4
bytes
(0.5epochs) 16 0.1 0.8 0.8 0.1 0.8 0.8 0.4 0.4 0.4
32 0.1 0.4 0.8 0.1 0.4 0.8 0.1 0.4 0.4
1 0.1 0.8 0.8 0.1 0.8 0.8 0.1 0.4 0.8
2 0.1 0.8 0.8 0.2 0.8 0.8 0.4 0.4 0.8
200Btokens
32ktokens 4 0.1 0.8 0.8 0.1 0.8 0.8 0.2 0.8 0.8
(0.8epochs)
6 0.1 0.8 0.8 0.2 0.8 0.8 0.4 0.4 0.8
8 0.1 0.8 0.8 0.1 0.8 0.8 0.2 0.4 0.8
1Ttokens 1 0.1 0.8 0.8 0.1 0.8 0.8 0.1 0.4 0.8
32ktokens
(4epochs) 4 0.1 0.8 0.8 0.2 0.8 0.8 0.4 0.8 0.8
L.Additionalintuitionsonmulti-tokenprediction
L.1.Comparisontoscheduledsampling
InSection5.2,wearguedthatmulti-tokenpredictionreducesthedistributionmismatchbetweenteacher-forcedtrainingand
autoregressiveevaluationoflanguagemodels. Scheduledsampling(Bengioetal.,2015)isacurriculumlearningmethod
thatlikewiseaimstobridgethisgapinsequencepredictiontasksbygraduallyreplacingmoreandmoreinputtokenswith
model-generatedones.
Whileeffectiveinareassuchastimeseriesforecasting,scheduledsamplingis,inouropinion,inapplicabletolanguage
modellingduetothediscretenatureoftext. Replacinggroundtruthinputsequencesbyinterleavingsofgroundtruthand
model-generatedtokensfrequentlyresultsinungrammatical,factuallywrongorotherwiseincoherenttext,whichshould
beavoidedatallcost. Moreover,unlikemulti-tokenprediction,thetechniqueoriginallydevelopedforrecurrentneural
networkscannoteasilybeadaptedforparalleltrainingsetupsliketheonesoftransformermodels.
L.2.Information-theoreticargument
We give details on the information-theoretic terms appearing in the decomposition in Section 5.2 and derive a relative
versionthatsimilarlyallowstodecomposemulti-tokenpredictionlosses. AsinSection5.2,denotebyX thenexttoken
andbyY thesecond-nextone,andomitconditioningontheprecedingcontextC foreaseofnotation. InSection5.2,we
decomposedH(X)+H(Y)—thequantityofinterestfor2-tokenpredictionmodels—asfollows:
H(X)+H(Y)=H(X |Y)+2I(X;Y)+H(Y |X). (3)
Letusexplaineachoftheterms. Theentropytermsdenotetheuncertaintycontainedintheground-truthrandomvariables
X andY. 2 ThetermH(Y |X)isaclassicalnext-tokenentropyfortheprefix(C,X). TheconditionalentropyH(X |Y)
isamoretheoreticalentitynotmodelledbycausalmodels. ItdescribestheuncertaintyaboutX giventheprefixC andsuffix
Y,andthereforecapturesthelocalvariationsofX thatdonotaffectthecontinuationofthetextY. Themutualinformation
I(X;Y)ontheotherhanddescribestheinformationaboutY containedinX (andviceversa)andthereforecapturesthe
variationsofX whichconstrainthecontinuationofthetext.
However, the argument given in Section 5.2 relies on the assumption that multi-token prediction losses obey a similar
decomposition as the sum of the ground-truth entropies themselves. Let us make this rigorous. Denote by p(x,y) the
jointdistributionofX andY,byp(x)(shortforp (x))themarginaldistributionofX andbyp(y)theoneofY. Denote
X
thedensitiesofthemodel’spredictionsbyq(x,y),q(x)andq(y),respectively,conditionaldistributionsbyp(x|y)and
Kullback-LeiblerdivergencefromqtopbyD(p∥q)andcross-entropyfromqtopbyH(p,q).
Definition L.1. The conditional cross-entropy H(p ,q ) of X conditioned on Y from q to p is defined as the
X|Y X|Y
2Inparticular,theydonotrefertomodelpredictions.
24Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
expectationunderyofthecross-entropybetweenthedistributionsp andq conditionedony,informulas:
X X
H(p ,q )= E H(p ,q )= E H(p(·|y),q(·|y)).
X|Y X|Y X|Y=y X|Y=y
y∼pY y∼pY
DefinitionL.2. TherelativemutualinformationI (X;Y)ofX andY fromqrelativetopisdefinedby
p∥q
I (X;Y)=D(p∥q ⊗q )−D(p∥q).
p∥q X Y
WehaveI (X;Y)=H(p ,q )+H(p ,q )−H(p,q),I (X;Y)=I (X;Y)reducestostandardmutualinforma-
p∥q X X Y Y p∥p p
tionunderthedistributionpandI (X;Y)issymmetricinX andY butcanbenegative.
p∥q
WehavethefollowingrelativeversionofthedecompositionH(X)=H(X |Y)+I(X;Y).
LemmaL.3. H(p ,q )=H(p ,q )+I (X;Y).
X X X|Y X|Y p∥q
Proof. Wecalculate
(cid:88)
H(p ,q )=− p(x)logq(x)
X X
x
(cid:88)
=− p(x,y)logq(x)
x,y
(cid:88) q(x)q(y)p(x,y)q(x,y)
=− p(x,y)log
p(x,y) q(x,y) q(y)
x,y
(cid:88)
=D(p∥q ⊗q )−D(p∥q)− p(y)p(x|y)logq(x|y)
X Y
x,y
(cid:88)
=I (X;Y)+ p(y)H(p ,q )
p∥q X|y Y|y
y
=I (X;Y)+H(p ,q ).
p∥q X|Y X|Y
Symmetrizing,wegetthedesiredrelativeversionofH(X)+H(Y)=H(X |Y)+2I(X;Y)+H(Y |X):
H(p ,q )+H(p ,q )=H(p ,q )+2I (X;Y)+H(p ,q ).
X X Y Y X|Y X|Y p∥q Y|X Y|X
Settingptobetheempiricaldistributionofthetrainingdata,theleft-handsidedescribesthecross-entropylossusedto
train2-tokenpredictionmodels. Theright-handsidegivesthedecompositionintoalocalcross-entropyterm,amutual
informationtermwithweighttwoandashiftednext-tokencross-entropyterm. Weinterpretthisasfollows: byaddingthe
termH(p ,q )totheloss,2-tokenpredictionincentivizesmodelstoprecomputefeatureswhichwillbecomeusefulfor
Y Y
predictingY inthenextstepandincreasestheweightoftherelativemutualinformationtermintheloss. Whatdoesrelative
mutualinformationactuallymean? ByinterpretingKullback-LeiblerdivergenceD(p∥q)astheaveragenumberofbits
neededinadditiontosenddatafrompwithacodeoptimizedforqinsteadofp,weseethatminimizing
I (X;Y)=D(p∥q ⊗q )−D(p∥q)
p∥q X Y
meansminimizingtheaveragenumberofadditionalbitsneededtosenddatafrompwithacodeoptimizedforqthattreats
X andY asindependentcomparedtoonethatdoesnot. Ifthisnumberissmall,qmanagedtoexploitthemutualinformation
ofX andY underp.
L.3.Lookaheadreinforceschoicepoints
Trainingwithmulti-headpredictionincreasestheimportanceofchoicepointsinthelossincomparisontoinconsequential
decisions. Tomakethisargument,wepresentasimplifiedmodeloflanguagemodelling. Considerasequentialdecisiontask
andamodelM thatistrainedinateacher-forcedwayonoptimaltrajectories. Wedistinguishchoicepoints–transitionsthat
leadtodifferentoutcomes–andinconsequentialdecisionswhichdonot(FigureS17(a)and(b)).
25Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
(a)
(c)
(b)
FigureS17: Exampleofasequentialpredictiontaskwithderailing. Thegoalistogofromthearrowtothetrophy.
Turningaroundisnotallowed. Mosttransitionsareunique,buttherearetwoturnstobetakencorrectly,theconsequential
decisions(a)and(c). Turn(b)isaninconsequentialdecision: thepathsjoinrightafterit. Nexttotransitions(a)and(b),
wesketchhowa4-steppredictionlosscanplacemoreemphasisonconsequentialtransitionsthaninconsequentialones
duringteacher-forcedtraining. Nexttotransition(c),wesketchhowa4-steplookaheadcanpreventmodelsfromtaking
irreversiblesuboptimaldecisionsduringautoregressivedecoding.
Moreformally,assumethatthelanguagemodelisdeployedinareinforcementlearningsettinglikeinreinforcementlearning
fromhumanfeedback(Ouyangetal.,2022)(statesarepromptsfollowedbythepartialsequenceoftokensx generatedso
t:1
far,actionsaresingletokensx togenerate,rewardsareexternalR(x )). Thequantity
t+1 t:1
 
(cid:88)
V π(x t:1)=E xt+i∼π(xt+i−1:1),i≥1 R(x t+i:1)
i≥0
isthevalueofthestatex followingthepolicyπ,while
t:1
(cid:114)
σ (x )= Var [V (x )]
π t:1 π t+1:1
xt+1∼π(xt:1)
quantifiestheimportanceofthedecisionx onthevaluethereafter. Choicepointscanformallybeviewedasstepstfor
t+1
whichσ (x )islarge,whileinconsequentialpointsarestepswhereitislow. Notethatforcompletionmodels,thereisno
π t:1
explicitreward,andourargumentismerelymeanttoillustratewhatwemeanbychoicepoints.
Derailing denotes a situation where autoregressive generation of trajectories from M at inference time results in bad
outcomesafterM madeamistakeonachoicepoint. Evenifsubsequently,M actsoptimallygiventhischoice,thefinal
outcomecanbesignificantlyworsethantheoutcomeoftheoptimaltrajectory.
Stayingintheteacher-forcedsetting, weask: WhatistheimpactoftrainingM withn-steppredictioninsteadofnext-
step prediction on this task? Say x → x is a choice point in an optimal trajectory with the suboptimal choice
t t+1
beingx → x˜ (FigureS17(a)). Assumethatthetrajectoriesprecedingx andsucceedingx andx˜ consistof
t t+1 t t+1 t+1
inconsequentialtransitions,thelatterdenotedbyx˜ →x˜ . Wewillcomparethelossesofateacher-forcednext-step
t+j t+j+1
predictionmodelandateacher-forcedn-steppredictionmodelonthepartialtrajectory(x ,...x ). Forthenext-step
t−n+1 t
predictionmodel,thepredictionsare(x ,...,x ,x˜ )withasinglewrongprediction. Thepredictionsofann-step
t−n+2 t t+1
predictionmodelattimet−n+i,i=1,...,nare(x ,...,x ,x˜ ,...,x˜ )withiwrongpredictions. Inother
t−n+i+1 t t+1 t+i
words, an n-step prediction model receives 1+...+n = n(n+1) loss terms pertaining to such a choice point and its
2
consequences,whileeachinconsequentialtransition(FigureS17(b))isonlyreinforcedntimesasoftenasinanext-step
prediction model. In other words, choice points receive on average n+1 times more importance in the loss of n-step
2
predictionmodelsthaninnext-steppredictionmodels.
26Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
AsarguedinSection5.1,webelievethatthismodelcapturesimportantfeaturesoftrainingandinferencewithlanguage
models: choicepointsaresemanticallyimportantturningpointsinthegeneratedtexts,suchasthefinalanswertoaquestion
oraspecificlineofcode,whileinconsequentialdecisionscanbeachoiceamongsynonymsorofvariablenamesincode.
Apartfromthistrainingdynamicspointofview,wehypothesizethatn-steppredictionalsoallowstheformationofcircuits
thatspecificallyspotinconsistenciesbetweenpredictionsforearlierandlatersteps. Forinstance,ifinanearlylayerof
themodel,itcanbepredictedthatadecisionx →x˜ leadstosuboptimaloutcomesx˜ (FigureS17(c)),subsequent
t t+1 t+n
layerscanreducetheprobabilityofx →x˜ inthemodel’snext-stepprediction. Suchbehaviorsalsohappeninnext-step
t t+1
predictionmodelsgivenenoughcapacity,butourexperimentsinSection4.2pointtothefactthatcircuitsofthiskindare
formedmoreeasilyinmulti-steparchitecturesthatenforcetherequiredinformationx˜ tobeavailabletothemodelwhen
t+n
predictingx˜ . Webelievethatthissituationappearsfrequentlyinnaturallanguageandcodemodelling,forinstancewhere
t+1
aninitialanswertoaquestioncontradictstheresultsofthechainofthoughtbroughtforwardwiththeintentiontojustifyit.
Inmoregeneralterms,thissituationariseswheneverpredictingfirstx˜ forsome1<i≤nandthenx˜ basedonx˜
n+i n+1 n+i
iseasierthanpredictingx˜ directly. Wediscussthisphenomenonoffactorizationordersinthenextsectionandpresenta
n+1
specificinstanceofitthatfrequentlyappearsinmodellingnaturallanguage.
L.4.Factorizationorders
Causallanguagemodellingfactorizesprobabilitiesovertextsequencesx ···x classicallyas
t 1
t
(cid:89)
P(x ···x )= P(x |x ···x ).
t 1 i i−1 1
i=1
While moving forward in time is certainly the most natural choice of factorization order, there exist cases where it is
suboptimal. Ininflectionallanguages,forinstance,agreementbetweenrelatedsentencepartsisafrequentpatternwithone
worddirectingthegrammaticalformsofothers. ConsidertheGermansentence
WiekonntenauchWortemeinerdurstendenSeelegenügen?3
FriedrichHölderlin,FragmentvonHyperion(1793)
where"genügen"requiresadativecaseobjectandthen"Seele"requiresthepossessivepronoun"mein"tobeinfemale
singular dative form "meiner" and the participle "durstend" to be in female singular dative form in weak declination
"durstenden"becauseitfollows"meiner". Inotherwords,thefactorizationorder
WiekonntenauchWorte→genügen→Seele→meiner→durstenden?
isarguablyaneasieroneforconstructingtheabovesentence. Humansaswellaslanguagemodelsthereforehavetoperform
thisfactorization(whichdeviatesfromthecausalorderinwhichpredictionstakeplace!) withintheirlatentactivations,and
a4-tokenpredictionlossmakesthiseasierasitexplicitlyencouragesmodelstohaveallinformationaboutthesuccessive4
tokensinitslatentrepresentations.
3roughly:Howcouldwordsbeenoughformythirstysoul?
27Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
M.Traininghyperparameters
TableS13: Overviewofalltraininghyperparametersused. Weschedulealllearningrateswithalinearwarmupand
cosinedecay(LoshchilovandHutter,2017)toafractionofthepeaklearningratewhichisdepictedinthelastcolumn
(“decayratio”). AllexperimentsusetheAdam(KingmaandBa,2015)optimizerwithβ =0.9,β =0.95anddecoupled
1 2
L weightdecay(LoshchilovandHutter,2019)coefficient0.1. WeclipgradientstoamaximalEuclideannormof1.0inall
2
experimentsexceptCodeContestsfinetunings,whereweuse0.1instead. Summarizationfinetuningscorrespondtothree
epochsonalldatasetsexceptBigPatent(1epoch). Byte-levelmodelsusethearchitecturewithreplicatedunembeddings
fromAppendixB.
Model Batchsize(220) Steps Tokens(B) Warmupsteps PeakLR Contextlength Decayratio
Modelscaling(Section3.1)
0.3B 8 10,850 91.0 1000 3×10−4 4096 0.03
0.6B 8 10,850 91.0 1000 3×10−4 4096 0.03
1.3B 8 10,850 91.0 1000 3×10−4 4096 0.03
3B 8 10,850 91.0 1000 3×10−4 4096 0.03
7B 8 25,000 209.7 2000 3×10−4 4096 0.03
13B 8 25,000 209.7 1000 3×10−4 4096 0.03
Codemodels(Section3)
7B200B 8 25,000 209.7 2000 3×10−4 4096 0.03
7B500B 7 68,570 503.3 2000 3×10−4 4096 0.03
7B1T 7 136,240 1000.0 2000 3×10−4 4096 0.03
Byte-levelmodels(Section3.3)
7B314GB 12 25,000 314.6 2000 3×10−4 8192 0.03
Languagemodels(Section3.7)
7B200B 8 25,000 209.7 2000 3×10−4 4096 0.10
7B500B 8 60,000 503.3 2000 3×10−4 4096 0.10
Inductiontask(Section4.1)
1M–1B 0.25 100,000 26.2 2000 10−4 2048 0.03
1M–1B(AppendixJ) 0.5 50000 26.2 2000 10−4 2048 0.03
Arithmetictask(Section4.2)
30M 0.25 100,000 26.2 2000 10−4 1024 0.03
100M 0.25 100,000 26.2 2000 10−4 2048 0.03
Summarization(Section3.7)
BigPatent 0.125 76,680 10.1 100 3×10−5 4096 0.03
CNN/Dailymail 0.125 7,140 0.9 100 3×10−5 4096 0.03
Multi-News 0.125 3,330 0.4 100 3×10−5 4096 0.03
OrangeSum 0.125 360 0.0 100 3×10−5 4096 0.03
pn-summary 0.125 3,450 0.5 100 3×10−5 4096 0.03
SAMSum 0.125 60 0.0 100 3×10−5 4096 0.03
ThaiSum 0.125 23,640 3.1 100 3×10−5 4096 0.03
WikiSummary 0.125 2,550 0.3 100 3×10−5 4096 0.03
XSum 0.125 2,760 0.4 100 3×10−5 4096 0.03
CodeContests(Section3.6)
7B 0.25 13,000 3.6 400 5×10−5 4096 0.004
28Better&FasterLargeLanguageModelsviaMulti-tokenPrediction
TableS14: Overviewofmodelarchitecturesusedforscalinganalyses.
Name Dimension Layers Heads
1M 128 5 4
3M 256 4 8
10M 384 6 8
30M 512 10 8
100M 768 14 12
300M 1024 25 16
1B 1536 36 24
0.3B 1024 18 16
0.6B 1280 27 20
1.3B 2048 24 16
3B 2560 36 20
6.7B(“7B”) 4096 32 32
13B 5120 40 40
29