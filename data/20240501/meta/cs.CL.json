[
    {
        "title": "DOCCI: Descriptions of Connected and Contrasting Images",
        "authors": "Yasumasa OnoeSunayana RaneZachary BergerYonatan BittonJaemin ChoRoopal GargAlexander KuZarana ParekhJordi Pont-TusetGarrett TanzerSu WangJason Baldridge",
        "links": "http://arxiv.org/abs/2404.19753v1",
        "entry_id": "http://arxiv.org/abs/2404.19753v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19753v1",
        "summary": "Vision-language datasets are vital for both text-to-image (T2I) and\nimage-to-text (I2T) research. However, current datasets lack descriptions with\nfine-grained detail that would allow for richer associations to be learned by\nmodels. To fill the gap, we introduce Descriptions of Connected and Contrasting\nImages (DOCCI), a dataset with long, human-annotated English descriptions for\n15k images that were taken, curated and donated by a single researcher intent\non capturing key challenges such as spatial relations, counting, text\nrendering, world knowledge, and more. We instruct human annotators to create\ncomprehensive descriptions for each image; these average 136 words in length\nand are crafted to clearly distinguish each image from those that are related\nor similar. Each description is highly compositional and typically encompasses\nmultiple challenges. Through both quantitative and qualitative analyses, we\ndemonstrate that DOCCI serves as an effective training resource for\nimage-to-text generation -- a PaLI 5B model finetuned on DOCCI shows equal or\nsuperior results compared to highly-performant larger models like LLaVA-1.5 7B\nand InstructBLIP 7B. Furthermore, we show that DOCCI is a useful testbed for\ntext-to-image generation, highlighting the limitations of current text-to-image\nmodels in capturing long descriptions and fine details.",
        "updated": "2024-04-30 17:56:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19753v1"
    },
    {
        "title": "Better & Faster Large Language Models via Multi-token Prediction",
        "authors": "Fabian GloeckleBadr Youbi IdrissiBaptiste RozièreDavid Lopez-PazGabriel Synnaeve",
        "links": "http://arxiv.org/abs/2404.19737v1",
        "entry_id": "http://arxiv.org/abs/2404.19737v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19737v1",
        "summary": "Large language models such as GPT and Llama are trained with a next-token\nprediction loss. In this work, we suggest that training language models to\npredict multiple future tokens at once results in higher sample efficiency.\nMore specifically, at each position in the training corpus, we ask the model to\npredict the following n tokens using n independent output heads, operating on\ntop of a shared model trunk. Considering multi-token prediction as an auxiliary\ntraining task, we measure improved downstream capabilities with no overhead in\ntraining time for both code and natural language models. The method is\nincreasingly useful for larger model sizes, and keeps its appeal when training\nfor multiple epochs. Gains are especially pronounced on generative benchmarks\nlike coding, where our models consistently outperform strong baselines by\nseveral percentage points. Our 13B parameter models solves 12 % more problems\non HumanEval and 17 % more on MBPP than comparable next-token models.\nExperiments on small algorithmic tasks demonstrate that multi-token prediction\nis favorable for the development of induction heads and algorithmic reasoning\ncapabilities. As an additional benefit, models trained with 4-token prediction\nare up to 3 times faster at inference, even with large batch sizes.",
        "updated": "2024-04-30 17:33:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19737v1"
    },
    {
        "title": "Iterative Reasoning Preference Optimization",
        "authors": "Richard Yuanzhe PangWeizhe YuanKyunghyun ChoHe HeSainbayar SukhbaatarJason Weston",
        "links": "http://arxiv.org/abs/2404.19733v1",
        "entry_id": "http://arxiv.org/abs/2404.19733v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19733v1",
        "summary": "Iterative preference optimization methods have recently been shown to perform\nwell for general instruction tuning tasks, but typically make little\nimprovement on reasoning tasks (Yuan et al., 2024, Chen et al., 2024). In this\nwork we develop an iterative approach that optimizes the preference between\ncompeting generated Chain-of-Thought (CoT) candidates by optimizing for winning\nvs. losing reasoning steps that lead to the correct answer. We train using a\nmodified DPO loss (Rafailov et al., 2023) with an additional negative\nlog-likelihood term, which we find to be crucial. We show reasoning improves\nacross repeated iterations of this scheme. While only relying on examples in\nthe training set, our approach results in increasing accuracy for\nLlama-2-70B-Chat from 55.6% to 81.6% on GSM8K (and 88.7% with majority voting\nout of 32 samples), from 12.5% to 20.8% on MATH, and from 77.8% to 86.7% on\nARC-Challenge, which outperforms other Llama-2-based models not relying on\nadditionally sourced datasets.",
        "updated": "2024-04-30 17:28:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19733v1"
    },
    {
        "title": "PANGeA: Procedural Artificial Narrative using Generative AI for Turn-Based Video Games",
        "authors": "Steph BuongiornoLawrence Jake KlinkertTanishq ChawlaZixin ZhuangCorey Clark",
        "links": "http://arxiv.org/abs/2404.19721v1",
        "entry_id": "http://arxiv.org/abs/2404.19721v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19721v1",
        "summary": "This research introduces Procedural Artificial Narrative using Generative AI\n(PANGeA), a structured approach for leveraging large language models (LLMs),\nguided by a game designer's high-level criteria, to generate narrative content\nfor turn-based role-playing video games (RPGs). Distinct from prior\napplications of LLMs used for video game design, PANGeA innovates by not only\ngenerating game level data (which includes, but is not limited to, setting, key\nitems, and non-playable characters (NPCs)), but by also fostering dynamic,\nfree-form interactions between the player and the environment that align with\nthe procedural game narrative. The NPCs generated by PANGeA are\npersonality-biased and express traits from the Big 5 Personality Model in their\ngenerated responses. PANGeA addresses challenges behind ingesting free-form\ntext input, which can prompt LLM responses beyond the scope of the game\nnarrative. A novel validation system that uses the LLM's intelligence evaluates\ntext input and aligns generated responses with the unfolding narrative. Making\nthese interactions possible, PANGeA is supported by a server that hosts a\ncustom memory system that supplies context for augmenting generated responses\nthus aligning them with the procedural narrative. For its broad application,\nthe server has a REST interface enabling any game engine to integrate directly\nwith PANGeA, as well as an LLM interface adaptable with local or private LLMs.\nPANGeA's ability to foster dynamic narrative generation by aligning responses\nwith the procedural narrative is demonstrated through an empirical study and\nablation test of two versions of a demo game. These are, a custom,\nbrowser-based GPT and a Unity demo. As the results show, PANGeA holds potential\nto assist game designers in using LLMs to generate narrative-consistent content\neven when provided varied and unpredictable, free-form text input.",
        "updated": "2024-04-30 17:11:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19721v1"
    },
    {
        "title": "ThangDLU at #SMM4H 2024: Encoder-decoder models for classifying text data on social disorders in children and adolescents",
        "authors": "Hoang-Thang TaAbu Bakar Siddiqur RahmanLotfollah NajjarAlexander Gelbukh",
        "links": "http://arxiv.org/abs/2404.19714v1",
        "entry_id": "http://arxiv.org/abs/2404.19714v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19714v1",
        "summary": "This paper describes our participation in Task 3 and Task 5 of the #SMM4H\n(Social Media Mining for Health) 2024 Workshop, explicitly targeting the\nclassification challenges within tweet data. Task 3 is a multi-class\nclassification task centered on tweets discussing the impact of outdoor\nenvironments on symptoms of social anxiety. Task 5 involves a binary\nclassification task focusing on tweets reporting medical disorders in children.\nWe applied transfer learning from pre-trained encoder-decoder models such as\nBART-base and T5-small to identify the labels of a set of given tweets. We also\npresented some data augmentation methods to see their impact on the model\nperformance. Finally, the systems obtained the best F1 score of 0.627 in Task 3\nand the best F1 score of 0.841 in Task 5.",
        "updated": "2024-04-30 17:06:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19714v1"
    }
]