[
    {
        "title": "KAN: Kolmogorov-Arnold Networks",
        "authors": "Ziming LiuYixuan WangSachin VaidyaFabian RuehleJames HalversonMarin SoljačićThomas Y. HouMax Tegmark",
        "links": "http://arxiv.org/abs/2404.19756v1",
        "entry_id": "http://arxiv.org/abs/2404.19756v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19756v1",
        "summary": "Inspired by the Kolmogorov-Arnold representation theorem, we propose\nKolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer\nPerceptrons (MLPs). While MLPs have fixed activation functions on nodes\n(\"neurons\"), KANs have learnable activation functions on edges (\"weights\").\nKANs have no linear weights at all -- every weight parameter is replaced by a\nunivariate function parametrized as a spline. We show that this seemingly\nsimple change makes KANs outperform MLPs in terms of accuracy and\ninterpretability. For accuracy, much smaller KANs can achieve comparable or\nbetter accuracy than much larger MLPs in data fitting and PDE solving.\nTheoretically and empirically, KANs possess faster neural scaling laws than\nMLPs. For interpretability, KANs can be intuitively visualized and can easily\ninteract with human users. Through two examples in mathematics and physics,\nKANs are shown to be useful collaborators helping scientists (re)discover\nmathematical and physical laws. In summary, KANs are promising alternatives for\nMLPs, opening opportunities for further improving today's deep learning models\nwhich rely heavily on MLPs.",
        "updated": "2024-04-30 17:58:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19756v1"
    },
    {
        "title": "The lazy (NTK) and rich ($μ$P) regimes: a gentle tutorial",
        "authors": "Dhruva Karkada",
        "links": "http://arxiv.org/abs/2404.19719v1",
        "entry_id": "http://arxiv.org/abs/2404.19719v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19719v1",
        "summary": "A central theme of the modern machine learning paradigm is that larger neural\nnetworks achieve better performance on a variety of metrics. Theoretical\nanalyses of these overparameterized models have recently centered around\nstudying very wide neural networks. In this tutorial, we provide a nonrigorous\nbut illustrative derivation of the following fact: in order to train wide\nnetworks effectively, there is only one degree of freedom in choosing\nhyperparameters such as the learning rate and the size of the initial weights.\nThis degree of freedom controls the richness of training behavior: at minimum,\nthe wide network trains lazily like a kernel machine, and at maximum, it\nexhibits feature learning in the so-called $\\mu$P regime. In this paper, we\nexplain this richness scale, synthesize recent research results into a coherent\nwhole, offer new perspectives and intuitions, and provide empirical evidence\nsupporting our claims. In doing so, we hope to encourage further study of the\nrichness scale, as it may be key to developing a scientific theory of feature\nlearning in practical deep neural networks.",
        "updated": "2024-04-30 17:11:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19719v1"
    },
    {
        "title": "PCA for Point Processes",
        "authors": "Franck PicardVincent RivoirardAngelina RocheVictor Panaretos",
        "links": "http://arxiv.org/abs/2404.19661v1",
        "entry_id": "http://arxiv.org/abs/2404.19661v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19661v1",
        "summary": "We introduce a novel statistical framework for the analysis of replicated\npoint processes that allows for the study of point pattern variability at a\npopulation level. By treating point process realizations as random measures, we\nadopt a functional analysis perspective and propose a form of functional\nPrincipal Component Analysis (fPCA) for point processes. The originality of our\nmethod is to base our analysis on the cumulative mass functions of the random\nmeasures which gives us a direct and interpretable analysis. Key theoretical\ncontributions include establishing a Karhunen-Lo\\`{e}ve expansion for the\nrandom measures and a Mercer Theorem for covariance measures. We establish\nconvergence in a strong sense, and introduce the concept of principal measures,\nwhich can be seen as latent processes governing the dynamics of the observed\npoint patterns. We propose an easy-to-implement estimation strategy of\neigenelements for which parametric rates are achieved. We fully characterize\nthe solutions of our approach to Poisson and Hawkes processes and validate our\nmethodology via simulations and diverse applications in seismology, single-cell\nbiology and neurosiences, demonstrating its versatility and effectiveness. Our\nmethod is implemented in the pppca R-package.",
        "updated": "2024-04-30 15:57:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19661v1"
    },
    {
        "title": "Be Aware of the Neighborhood Effect: Modeling Selection Bias under Interference",
        "authors": "Haoxuan LiChunyuan ZhengSihao DingPeng WuZhi GengFuli FengXiangnan He",
        "links": "http://arxiv.org/abs/2404.19620v1",
        "entry_id": "http://arxiv.org/abs/2404.19620v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19620v1",
        "summary": "Selection bias in recommender system arises from the recommendation process\nof system filtering and the interactive process of user selection. Many\nprevious studies have focused on addressing selection bias to achieve unbiased\nlearning of the prediction model, but ignore the fact that potential outcomes\nfor a given user-item pair may vary with the treatments assigned to other\nuser-item pairs, named neighborhood effect. To fill the gap, this paper\nformally formulates the neighborhood effect as an interference problem from the\nperspective of causal inference and introduces a treatment representation to\ncapture the neighborhood effect. On this basis, we propose a novel ideal loss\nthat can be used to deal with selection bias in the presence of neighborhood\neffect. We further develop two new estimators for estimating the proposed ideal\nloss. We theoretically establish the connection between the proposed and\nprevious debiasing methods ignoring the neighborhood effect, showing that the\nproposed methods can achieve unbiased learning when both selection bias and\nneighborhood effect are present, while the existing methods are biased.\nExtensive semi-synthetic and real-world experiments are conducted to\ndemonstrate the effectiveness of the proposed methods.",
        "updated": "2024-04-30 15:20:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19620v1"
    },
    {
        "title": "Neural Dynamic Data Valuation",
        "authors": "Zhangyong LiangHuanhuan GaoJi Zhang",
        "links": "http://arxiv.org/abs/2404.19557v1",
        "entry_id": "http://arxiv.org/abs/2404.19557v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19557v1",
        "summary": "Data constitute the foundational component of the data economy and its\nmarketplaces. Efficient and fair data valuation has emerged as a topic of\nsignificant interest.\\ Many approaches based on marginal contribution have\nshown promising results in various downstream tasks. However, they are well\nknown to be computationally expensive as they require training a large number\nof utility functions, which are used to evaluate the usefulness or value of a\ngiven dataset for a specific purpose. As a result, it has been recognized as\ninfeasible to apply these methods to a data marketplace involving large-scale\ndatasets. Consequently, a critical issue arises: how can the re-training of the\nutility function be avoided? To address this issue, we propose a novel data\nvaluation method from the perspective of optimal control, named the neural\ndynamic data valuation (NDDV). Our method has solid theoretical interpretations\nto accurately identify the data valuation via the sensitivity of the data\noptimal control state. In addition, we implement a data re-weighting strategy\nto capture the unique features of data points, ensuring fairness through the\ninteraction between data points and the mean-field states. Notably, our method\nrequires only training once to estimate the value of all data points,\nsignificantly improving the computational efficiency. We conduct comprehensive\nexperiments using different datasets and tasks. The results demonstrate that\nthe proposed NDDV method outperforms the existing state-of-the-art data\nvaluation methods in accurately identifying data points with either high or low\nvalues and is more computationally efficient.",
        "updated": "2024-04-30 13:39:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19557v1"
    }
]