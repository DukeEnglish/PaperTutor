[
    {
        "title": "KAN: Kolmogorov-Arnold Networks",
        "authors": "Ziming LiuYixuan WangSachin VaidyaFabian RuehleJames HalversonMarin SoljačićThomas Y. HouMax Tegmark",
        "links": "http://arxiv.org/abs/2404.19756v1",
        "entry_id": "http://arxiv.org/abs/2404.19756v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19756v1",
        "summary": "Inspired by the Kolmogorov-Arnold representation theorem, we propose\nKolmogorov-Arnold Networks (KANs) as promising alternatives to Multi-Layer\nPerceptrons (MLPs). While MLPs have fixed activation functions on nodes\n(\"neurons\"), KANs have learnable activation functions on edges (\"weights\").\nKANs have no linear weights at all -- every weight parameter is replaced by a\nunivariate function parametrized as a spline. We show that this seemingly\nsimple change makes KANs outperform MLPs in terms of accuracy and\ninterpretability. For accuracy, much smaller KANs can achieve comparable or\nbetter accuracy than much larger MLPs in data fitting and PDE solving.\nTheoretically and empirically, KANs possess faster neural scaling laws than\nMLPs. For interpretability, KANs can be intuitively visualized and can easily\ninteract with human users. Through two examples in mathematics and physics,\nKANs are shown to be useful collaborators helping scientists (re)discover\nmathematical and physical laws. In summary, KANs are promising alternatives for\nMLPs, opening opportunities for further improving today's deep learning models\nwhich rely heavily on MLPs.",
        "updated": "2024-04-30 17:58:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19756v1"
    },
    {
        "title": "DOCCI: Descriptions of Connected and Contrasting Images",
        "authors": "Yasumasa OnoeSunayana RaneZachary BergerYonatan BittonJaemin ChoRoopal GargAlexander KuZarana ParekhJordi Pont-TusetGarrett TanzerSu WangJason Baldridge",
        "links": "http://arxiv.org/abs/2404.19753v1",
        "entry_id": "http://arxiv.org/abs/2404.19753v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19753v1",
        "summary": "Vision-language datasets are vital for both text-to-image (T2I) and\nimage-to-text (I2T) research. However, current datasets lack descriptions with\nfine-grained detail that would allow for richer associations to be learned by\nmodels. To fill the gap, we introduce Descriptions of Connected and Contrasting\nImages (DOCCI), a dataset with long, human-annotated English descriptions for\n15k images that were taken, curated and donated by a single researcher intent\non capturing key challenges such as spatial relations, counting, text\nrendering, world knowledge, and more. We instruct human annotators to create\ncomprehensive descriptions for each image; these average 136 words in length\nand are crafted to clearly distinguish each image from those that are related\nor similar. Each description is highly compositional and typically encompasses\nmultiple challenges. Through both quantitative and qualitative analyses, we\ndemonstrate that DOCCI serves as an effective training resource for\nimage-to-text generation -- a PaLI 5B model finetuned on DOCCI shows equal or\nsuperior results compared to highly-performant larger models like LLaVA-1.5 7B\nand InstructBLIP 7B. Furthermore, we show that DOCCI is a useful testbed for\ntext-to-image generation, highlighting the limitations of current text-to-image\nmodels in capturing long descriptions and fine details.",
        "updated": "2024-04-30 17:56:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19753v1"
    },
    {
        "title": "Scale-Robust Timely Asynchronous Decentralized Learning",
        "authors": "Purbesh MitraSennur Ulukus",
        "links": "http://arxiv.org/abs/2404.19749v1",
        "entry_id": "http://arxiv.org/abs/2404.19749v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19749v1",
        "summary": "We consider an asynchronous decentralized learning system, which consists of\na network of connected devices trying to learn a machine learning model without\nany centralized parameter server. The users in the network have their own local\ntraining data, which is used for learning across all the nodes in the network.\nThe learning method consists of two processes, evolving simultaneously without\nany necessary synchronization. The first process is the model update, where the\nusers update their local model via a fixed number of stochastic gradient\ndescent steps. The second process is model mixing, where the users communicate\nwith each other via randomized gossiping to exchange their models and average\nthem to reach consensus. In this work, we investigate the staleness criteria\nfor such a system, which is a sufficient condition for convergence of\nindividual user models. We show that for network scaling, i.e., when the number\nof user devices $n$ is very large, if the gossip capacity of individual users\nscales as $\\Omega(\\log n)$, we can guarantee the convergence of user models in\nfinite time. Furthermore, we show that the bounded staleness can only be\nguaranteed by any distributed opportunistic scheme by $\\Omega(n)$ scaling.",
        "updated": "2024-04-30 17:54:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19749v1"
    },
    {
        "title": "Mixed Continuous and Categorical Flow Matching for 3D De Novo Molecule Generation",
        "authors": "Ian DunnDavid Ryan Koes",
        "links": "http://arxiv.org/abs/2404.19739v1",
        "entry_id": "http://arxiv.org/abs/2404.19739v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19739v1",
        "summary": "Deep generative models that produce novel molecular structures have the\npotential to facilitate chemical discovery. Diffusion models currently achieve\nstate of the art performance for 3D molecule generation. In this work, we\nexplore the use of flow matching, a recently proposed generative modeling\nframework that generalizes diffusion models, for the task of de novo molecule\ngeneration. Flow matching provides flexibility in model design; however, the\nframework is predicated on the assumption of continuously-valued data. 3D de\nnovo molecule generation requires jointly sampling continuous and categorical\nvariables such as atom position and atom type. We extend the flow matching\nframework to categorical data by constructing flows that are constrained to\nexist on a continuous representation of categorical data known as the\nprobability simplex. We call this extension SimplexFlow. We explore the use of\nSimplexFlow for de novo molecule generation. However, we find that, in\npractice, a simpler approach that makes no accommodations for the categorical\nnature of the data yields equivalent or superior performance. As a result of\nthese experiments, we present FlowMol, a flow matching model for 3D de novo\ngenerative model that achieves improved performance over prior flow matching\nmethods, and we raise important questions about the design of prior\ndistributions for achieving strong performance in flow matching models. Code\nand trained models for reproducing this work are available at\nhttps://github.com/dunni3/FlowMol",
        "updated": "2024-04-30 17:37:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19739v1"
    },
    {
        "title": "Fairness Without Demographics in Human-Centered Federated Learning",
        "authors": "Roy ShailySharma HarshitSalekin Asif",
        "links": "http://arxiv.org/abs/2404.19725v1",
        "entry_id": "http://arxiv.org/abs/2404.19725v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19725v1",
        "summary": "Federated learning (FL) enables collaborative model training while preserving\ndata privacy, making it suitable for decentralized human-centered AI\napplications. However, a significant research gap remains in ensuring fairness\nin these systems. Current fairness strategies in FL require knowledge of\nbias-creating/sensitive attributes, clashing with FL's privacy principles.\nMoreover, in human-centered datasets, sensitive attributes may remain latent.\nTo tackle these challenges, we present a novel bias mitigation approach\ninspired by \"Fairness without Demographics\" in machine learning. The presented\napproach achieves fairness without needing knowledge of sensitive attributes by\nminimizing the top eigenvalue of the Hessian matrix during training, ensuring\nequitable loss landscapes across FL participants. Notably, we introduce a novel\nFL aggregation scheme that promotes participating models based on error rates\nand loss landscape curvature attributes, fostering fairness across the FL\nsystem. This work represents the first approach to attaining \"Fairness without\nDemographics\" in human-centered FL. Through comprehensive evaluation, our\napproach demonstrates effectiveness in balancing fairness and efficacy across\nvarious real-world applications, FL setups, and scenarios involving single and\nmultiple bias-inducing factors, representing a significant advancement in\nhuman-centered FL.",
        "updated": "2024-04-30 17:19:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19725v1"
    }
]