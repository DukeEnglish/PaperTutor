[
    {
        "title": "Lightplane: Highly-Scalable Components for Neural 3D Fields",
        "authors": "Ang CaoJustin JohnsonAndrea VedaldiDavid Novotny",
        "links": "http://arxiv.org/abs/2404.19760v1",
        "entry_id": "http://arxiv.org/abs/2404.19760v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19760v1",
        "summary": "Contemporary 3D research, particularly in reconstruction and generation,\nheavily relies on 2D images for inputs or supervision. However, current designs\nfor these 2D-3D mapping are memory-intensive, posing a significant bottleneck\nfor existing methods and hindering new applications. In response, we propose a\npair of highly scalable components for 3D neural fields: Lightplane Render and\nSplatter, which significantly reduce memory usage in 2D-3D mapping. These\ninnovations enable the processing of vastly more and higher resolution images\nwith small memory and computational costs. We demonstrate their utility in\nvarious applications, from benefiting single-scene optimization with\nimage-level losses to realizing a versatile pipeline for dramatically scaling\n3D reconstruction and generation. Code:\n\\url{https://github.com/facebookresearch/lightplane}.",
        "updated": "2024-04-30 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19760v1"
    },
    {
        "title": "MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model",
        "authors": "Wenxun DaiLing-Hao ChenJingbo WangJinpeng LiuBo DaiYansong Tang",
        "links": "http://arxiv.org/abs/2404.19759v1",
        "entry_id": "http://arxiv.org/abs/2404.19759v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19759v1",
        "summary": "This work introduces MotionLCM, extending controllable motion generation to a\nreal-time level. Existing methods for spatial control in text-conditioned\nmotion generation suffer from significant runtime inefficiency. To address this\nissue, we first propose the motion latent consistency model (MotionLCM) for\nmotion generation, building upon the latent diffusion model (MLD). By employing\none-step (or few-step) inference, we further improve the runtime efficiency of\nthe motion latent diffusion model for motion generation. To ensure effective\ncontrollability, we incorporate a motion ControlNet within the latent space of\nMotionLCM and enable explicit control signals (e.g., pelvis trajectory) in the\nvanilla motion space to control the generation process directly, similar to\ncontrolling other latent-free diffusion models for motion generation. By\nemploying these techniques, our approach can generate human motions with text\nand control signals in real-time. Experimental results demonstrate the\nremarkable generation and controlling capabilities of MotionLCM while\nmaintaining real-time runtime efficiency.",
        "updated": "2024-04-30 17:59:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19759v1"
    },
    {
        "title": "Invisible Stitch: Generating Smooth 3D Scenes with Depth Inpainting",
        "authors": "Paul EngstlerAndrea VedaldiIro LainaChristian Rupprecht",
        "links": "http://arxiv.org/abs/2404.19758v1",
        "entry_id": "http://arxiv.org/abs/2404.19758v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19758v1",
        "summary": "3D scene generation has quickly become a challenging new research direction,\nfueled by consistent improvements of 2D generative diffusion models. Most prior\nwork in this area generates scenes by iteratively stitching newly generated\nframes with existing geometry. These works often depend on pre-trained\nmonocular depth estimators to lift the generated images into 3D, fusing them\nwith the existing scene representation. These approaches are then often\nevaluated via a text metric, measuring the similarity between the generated\nimages and a given text prompt. In this work, we make two fundamental\ncontributions to the field of 3D scene generation. First, we note that lifting\nimages to 3D with a monocular depth estimation model is suboptimal as it\nignores the geometry of the existing scene. We thus introduce a novel depth\ncompletion model, trained via teacher distillation and self-training to learn\nthe 3D fusion process, resulting in improved geometric coherence of the scene.\nSecond, we introduce a new benchmarking scheme for scene generation methods\nthat is based on ground truth geometry, and thus measures the quality of the\nstructure of the scene.",
        "updated": "2024-04-30 17:59:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19758v1"
    },
    {
        "title": "DOCCI: Descriptions of Connected and Contrasting Images",
        "authors": "Yasumasa OnoeSunayana RaneZachary BergerYonatan BittonJaemin ChoRoopal GargAlexander KuZarana ParekhJordi Pont-TusetGarrett TanzerSu WangJason Baldridge",
        "links": "http://arxiv.org/abs/2404.19753v1",
        "entry_id": "http://arxiv.org/abs/2404.19753v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19753v1",
        "summary": "Vision-language datasets are vital for both text-to-image (T2I) and\nimage-to-text (I2T) research. However, current datasets lack descriptions with\nfine-grained detail that would allow for richer associations to be learned by\nmodels. To fill the gap, we introduce Descriptions of Connected and Contrasting\nImages (DOCCI), a dataset with long, human-annotated English descriptions for\n15k images that were taken, curated and donated by a single researcher intent\non capturing key challenges such as spatial relations, counting, text\nrendering, world knowledge, and more. We instruct human annotators to create\ncomprehensive descriptions for each image; these average 136 words in length\nand are crafted to clearly distinguish each image from those that are related\nor similar. Each description is highly compositional and typically encompasses\nmultiple challenges. Through both quantitative and qualitative analyses, we\ndemonstrate that DOCCI serves as an effective training resource for\nimage-to-text generation -- a PaLI 5B model finetuned on DOCCI shows equal or\nsuperior results compared to highly-performant larger models like LLaVA-1.5 7B\nand InstructBLIP 7B. Furthermore, we show that DOCCI is a useful testbed for\ntext-to-image generation, highlighting the limitations of current text-to-image\nmodels in capturing long descriptions and fine details.",
        "updated": "2024-04-30 17:56:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19753v1"
    },
    {
        "title": "Visual Fact Checker: Enabling High-Fidelity Detailed Caption Generation",
        "authors": "Yunhao GeXiaohui ZengJacob Samuel HuffmanTsung-Yi LinMing-Yu LiuYin Cui",
        "links": "http://arxiv.org/abs/2404.19752v1",
        "entry_id": "http://arxiv.org/abs/2404.19752v1",
        "pdf_url": "http://arxiv.org/pdf/2404.19752v1",
        "summary": "Existing automatic captioning methods for visual content face challenges such\nas lack of detail, content hallucination, and poor instruction following. In\nthis work, we propose VisualFactChecker (VFC), a flexible training-free\npipeline that generates high-fidelity and detailed captions for both 2D images\nand 3D objects. VFC consists of three steps: 1) proposal, where image-to-text\ncaptioning models propose multiple initial captions; 2) verification, where a\nlarge language model (LLM) utilizes tools such as object detection and VQA\nmodels to fact-check proposed captions; 3) captioning, where an LLM generates\nthe final caption by summarizing caption proposals and the fact check\nverification results. In this step, VFC can flexibly generate captions in\nvarious styles following complex instructions. We conduct comprehensive\ncaptioning evaluations using four metrics: 1) CLIP-Score for image-text\nsimilarity; 2) CLIP-Image-Score for measuring the image-image similarity\nbetween the original and the reconstructed image generated by a text-to-image\nmodel using the caption. 3) human study on Amazon Mechanical Turk; 4) GPT-4V\nfor fine-grained evaluation. Evaluation results show that VFC outperforms\nstate-of-the-art open-sourced captioning methods for 2D images on the COCO\ndataset and 3D assets on the Objaverse dataset. Our study demonstrates that by\ncombining open-source models into a pipeline, we can attain captioning\ncapability comparable to proprietary models such as GPT-4V, despite being over\n10x smaller in model size.",
        "updated": "2024-04-30 17:55:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.19752v1"
    }
]