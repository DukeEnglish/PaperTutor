4D Panoptic Scene Graph Generation
JingkangYang1,JunCen2,WenxuanPeng1,ShuaiLiu3,FangzhouHong1,
XiangtaiLi1,KaiyangZhou4,QifengChen2,ZiweiLiu1(cid:0)
1S-Lab,NanyangTechnologicalUniversity
2TheHongKongUniversityofScienceandTechnology
3BeijingUniversityofPostsandTelecommunications 4HongKongBaptistUniversity
https://github.com/Jingkang50/PSG4D
(a) Visual Input from the 4D Dynamic World (c) Reasoning & Planning
Perceiving Executing [keep sending scene graph…]
PSG4D
#12 #30 Oh wait, seems a man fell from his
bike, we should go help him pick up LLM
his bike. Where is the bike?
#0 The bike is 2.4m ahead,
PSG4D lying on the ground.
(b) PSG-4D: 4D Panoptic Scene Graph Parsing
Good, go forward 2.4m to the
bike, straighten the bike,and LLM
#0 boothbehind fence beside check whether the man injured.
#12 #30
bike adult r- #i2 d 0i -n #g 1# 2l 2oo 4k -i #n 3g 0atadult-1adult-3 Controller Executing!
#12-#18
#1ly 8in -g # 3o 0n gfa rll oin ug f nro dm standin #g 2 3o -n #30 Reasoning
Figure1: ConceptualillustrationofPSG-4D.PSG-4Disessentiallyaspatiotemporalrepresentationcapturing
not only fine-grained semantics in image pixels (i.e., panoptic segmentation masks) but also the temporal
relationalinformation(i.e.,scenegraphs).In(a)and(b),themodelabstractsinformationstreaminginRGB-D
videosinto(i)nodesthatrepresententitieswithaccuratelocationandstatusinformationand(ii)edgesthat
encapsulatethetemporalrelations.Sucharich4DrepresentationservesasabridgebetweenthePSG-4Dsystem
andalargelanguagemodel,whichgreatlyfacilitatesthedecision-makingprocess,asillustratedin(c).
Abstract
Wearelivinginathree-dimensionalspacewhilemovingforwardthroughafourth
dimension: time. To allow artificial intelligence to develop a comprehensive
understandingofsucha4Denvironment,weintroduce4DPanopticSceneGraph
(PSG-4D), a new representation that bridges the raw visual data perceived in a
dynamic 4D world and high-level visual understanding. Specifically, PSG-4D
abstractsrich4Dsensorydataintonodes, whichrepresententitieswithprecise
locationandstatusinformation,andedges,whichcapturethetemporalrelations.
Tofacilitateresearchinthisnewarea,webuildarichlyannotatedPSG-4Ddataset
consistingof3KRGB-Dvideoswithatotalof1Mframes,eachofwhichislabeled
with 4D panoptic segmentation masks as well as fine-grained, dynamic scene
graphs. TosolvePSG-4D,weproposePSG4DFormer,aTransformer-basedmodel
that can predict panoptic segmentation masks, track masks along the time axis,
andgeneratethecorrespondingscenegraphsviaarelationcomponent. Extensive
experimentsonthenewdatasetshowthatourmethodcanserveasastrongbaseline
forfutureresearchonPSG-4D.Intheend,weprovideareal-worldapplication
example to demonstrate how we can achieve dynamic scene understanding by
integratingalargelanguagemodelintoourPSG-4Dsystem.
(cid:0)
Correspondingauthor.Contact:{jingkang001, ziwei.liu}@ntu.edu.sg
37thConferenceonNeuralInformationProcessingSystems(NeurIPS2023).
4202
yaM
61
]VC.sc[
1v50301.5042:viXra1 Introduction
Theemergenceofintelligentagents,autonomoussystems,androbotsdemandsaprofoundunder-
standingofreal-worldenvironments[1–6]. Thisunderstandinginvolvesmorethanjustrecognizing
individualobjects–itrequiresanintricateunderstandingoftherelationshipsbetweentheseobjects.
Inthiscontext,researchonSceneGraphGeneration(SGG)[7],hassoughttoprovideamoredetailed,
relationalperspectiveonsceneunderstanding. Inthisapproach,scenegraphsrepresentobjectsas
nodesandtheirrelationshipsasedges,offeringamorecomprehensiveandstructuredunderstanding
ofthescene[8,7,9–11]. PanopticSceneGraphGeneration(PSG)[12]expandsthescopeofSGGto
encompasspixel-levelpreciseobjectlocalizationandcomprehensivesceneunderstanding,including
backgroundelements. ThenPSGhasbeenfurtherextendedtothedomainofvideos[13]withthe
inspirationfromVideoSceneGraphGeneration(VidSGG)[14,15].
Theutilityofscenegraphsalsoextendsintotherealmof3Dperception,introducingtheconcept
of3DSceneGraphs(3DSG)[16,17]. 3DSGsofferapreciserepresentationofobjectlocationsand
inter-objectrelationshipswithinthree-dimensionalscenes[18,19]. Despitethesedevelopments,the
existingapproacheshavenotfullyintegrateddynamic,spatio-temporalrelationships,particularly
thoseinvolvinghuman-objectandhuman-humaninteractions. ConsiderFigure1asanillustrative
example.Traditional3Dscenegraphmethodsmayrecognizethestaticelementsofthisscene,suchas
identifyingaboothsituatedontheground. However,amoreideal,advanced,anddynamicperception
isrequiredforreal-worldscenarios.Forinstance,asystemshouldbecapableofidentifyingadynamic
eventlikeapersonwhohasfallenofftheirbike,sothatitcouldthencomprehendthenecessityto
offerassistance,likehelpingthepersonstandupandstabilizetheirbike.
Therefore, our work takes a significant step towards a more comprehensive approach to sensing
andunderstandingtheworld. Weintroduceanewtask,the4DPanopticSceneGraph(PSG-4D),
aimingtobridgethegapbetweenrawvisualinputsinadynamic4Dworldandhigh-levelvisual
understanding. PSG-4Dcomprisestwomainelements: nodes,representingentitieswithaccurate
locationandstatusinformation,andedges,denotingtemporalrelations. Thistaskencapsulatesboth
spatialandtemporaldimensions,bringingusclosertoatrueunderstandingofthedynamicworld.
Tofacilitateresearchonthisnewtask,wecontributeanextensivelyannotatedPSG-4Ddatasetthat
iscomposedof2sub-sets,PSG4D-GTAandPSG4D-HOI.ThePSG4D-GTAsubsetconsistsof67
RGB-Dvideoswithatotalof28Kframes,selectedfromtheSAIL-VOS3Ddataset[20]collected
fromthevideogameGrandTheftAutoV(GTA-V)[21]. ThePSG4D-HOIsubsetisacollection
of3Kegocentricreal-worldvideossampledfromtheHOI4Ddataset[22]. Allframesineitherof
thesubsetarelabeledwith4Dpanopticsegmentationmasksaswellasfine-grained,dynamicscene
graphs. Webelievethisdatasetwillserveasavaluableresourceforresearchersinthefield.
Totacklethisnoveltask,weproposeaunifiedframeworkcalledPSG4DFormer.Thisunifiedstructure
encapsulatestwoprimarycomponents: a4DPanopticSegmentationmodelandaRelationmodel.
The4DPanopticSegmentationmodelisdesignedtoaccommodatebothRGB-Dandpointcloud
datainputs,yieldinga4Dpanopticsegmentation. Thisoutputcomprises3Dobjectmasks,whichare
continuouslytrackedacrosstemporaldimensions. Then,theRelationmodelacceptsthese3Dmask
tubesandutilizesaspatial-temporaltransformerarchitecturetodelineatelong-termdependenciesand
intricateinter-entityrelationships,subsequentlyyieldingarelationalscenegraph. Throughextensive
experiments,wedemonstratetheeffectivenessoftheproposedPSG-4DtaskandthePSG4DFormer
model. Our work constitutes a pivotal step towards a comprehensive understanding of dynamic
environments,settingthestageforfutureresearchinthisexcitingandcrucialareaofstudy.
Insummary,wemakethefollowingcontributionstothecommunity:
• ANewTask: Weproposeanovelscenegraphgenerationtaskfocusingonthepredictionof
4DpanopticscenegraphsfromRGB-Dorpointcloudvideosequences.
• ANewDataset: WeprovideaPSG-4Ddataset,whichcoversdiverseviewpoints: (i)athird-
viewsyntheticsubset(PSG4D-GTA)and(ii)anegocentricreal-worldsubset(PSG4D-HOI).
• A Unified Framework: We propose a unified two-stage model composed of a feature
extractorandarelationlearner. Inaddition,weofferdemosupportforbothsyntheticand
real-worldscenariostofacilitatefutureresearchandreal-worldapplications.
• Open-SourceCodebase:Weopen-sourceourcodebasetofacilitatefuturePSG-4Dresearch.
22 RelatedWork
SceneGraphGeneration(SGG) SGGtransformsanimageintoagraph,wherenodesrepresent
objects and edges represent relationships [7]. Several datasets [23] and methods, including two-
stage[8,7,9–11]andone-stagemodels[24,12,25],havebeendevelopedforSGG.Videoscene
graph generation (VidSGG) extends SGG to videos with notable datasets [14, 15, 26]. Despite
progress, limitationsremaininSGGandVidSGGduetonoisygroundingannotationscausedby
coarseboundingboxannotationsandtrivialrelationdefinitions.Recentworkonpanopticscenegraph
generation(PSG)[12,27–31]hasattemptedtoovercometheseissues,andPVSG[13,32]further
extendsitintothevideodomain. ThispaperpresentsanextensionofPSGintoa4Ddynamicworld,
meetingtheneedsofactiveagentsforpreciselocationandcomprehensivesceneunderstanding.
3D Scene Graph Generation 3D Scene Graphs (3DSGs) offer a precise 3D representation
of object locations and inter-object relationships, making them a vital tool for intelligent agents
operatinginreal-worldenvironments[16,17]. 3DSGscanbecategorizedintoflatandhierarchical
structures[33]. Theformerrepresentsobjectsandrelationshipsasasimplegraph[18,19],whilethe
latterlayersthestructuresof3Dscenes[34,35]. Recent3DSGtechniques[19]employPointNet[36]
with3DobjectdetectorsonpointcloudsorRGBDscans,generating3Dgraphsviagraphneural
networks [18]. Some settings, such as Kimera [37], emphasize pairwise spatiotemporal status
to facilitate task planning, while incremental 3DSG necessitates agents to progressively explore
environments[38]. However,thesegraphslargelyrepresentpositionalrelations,lackingdynamic
spatiotemporalrelationslikehuman-objectinteractionsandhuman-humanrelations.
4DPerception Researchon4Dperceptioncanbedividedbythespecificdataformattheyuse.
ThefirstoneisRGB-Dvideo,whichcanbeeasilyobtainedusingcheapsensors,e.g.Kinect,and
iPhone. With the additional depth data, more geometric and spatial information can be used for
reliableandrobustdetection[39–41]andsegmentation[42–45]. ForRGB-Dvideo,thedepthinputis
usuallytreatedlikeimages. Butforpointcloudsvideo,3Dorhigherdimensionconvolutions[46–49]
aremorecommonlyused,especiallyonLiDARpointcloudvideosforautonomousdrivingperception
system. Inthiswork, beyondthe4Dpanopticsegmentation, wefocusonmoredailyscenesand
pursueamorehigh-levelandstructuredunderstandingof4Dscenesbybuilding4Dscenegraphs.
3 ThePSG-4DProblem
The PSG-4D task is aimed at generating a dynamic scene graph, which describes a given 4D
environment. In this context, each node corresponds to an object, while each edge represents a
spatial-temporalrelation. ThePSG-4DmodelingestseitheranRGB-Dvideosequenceorapoint
cloudvideosequence,subsequentlyoutputtingaPSG-4DscenegraphG. Thisgraphiscomposedof
4DobjectbinarymasktubesM,objectlabelsO,andrelationsR.
Theobjectbinarymasktubes, m ∈ {0,1}T×H×W×4, expressthe3Dlocationandextentofthe
i
trackedobjectiovertime(T)inthecaseofanRGB-Dsequenceinput,whilem ∈{0,1}T×M×6is
i
usedforpointcloudvideoinputs. Here,4denotesRGB-Dvalues,and6representsXYZplusRGB
values. Mstandsforthenumberofpointcloudsofinterest. Theobjectlabel,o ∈CO,designatesthe
i
categoryoftheobject. Therelationr ∈CR representsasubjectandanobjectlinkedbyapredicate
i
class and a time period. CO and CR refer to the object and predicate classes, respectively. The
PSG-4Dtaskcanbemathematicallyformulatedas:
Pr(G|I)=Pr(M,O,R|I), (1)
whereIrepresentstheinputRGB-Dvideosequenceorpointcloudrepresentation.
EvaluationMetrics ForevaluatingtheperformanceofthePSG-4Dmodel,weemploytheR@K
and mR@K metrics, traditionally used in the scene graph generation tasks. R@K calculates the
tripletrecall,whilemR@Kcomputesthemeanrecall,bothconsideringthetopKtripletsfromthe
PSG-4Dmodel. Asuccessfulrecallofaground-truthtripletmustmeetthefollowingcriteria: 1)
correctcategorylabelsforthesubject,object,andpredicate;2)avolumeIntersectionoverUnion
(vIOU)greaterthan0.5betweenthepredictedmasktubesandtheground-truthtubes. Whenthese
criteriaaresatisfied,asoftrecallscoreisrecorded,representingthetimevIOUbetweenthepredicted
andtheground-truthtimeperiods.
3Table1: IllustrationofthePSG-4Ddatasetandrelateddatasets. Unlikethestatic3Dindoor
scenesusuallyfoundin3DSGdatasets,thePSG-4Ddatasetintroducesdynamic3Dvideos,each
annotatedwithpanopticsegmentation. Various3Dvideodatasetswereevaluatedaspotentialsources
forPSG-4D,resultinginthecreationoftwosubsets: PSG4D-GTAandPSG4D-HOI.Regarding
annotations, PS represents Panoptic Segmentation, BB represents Bounding Box, SS represents
SemanticSegmentation,KPrepresentskeypoints,andPCrepresentspointclouds. TPVrepresents
third-person-view.
Dataset Type Scale View #ObjCls #RelCls Annotation Year
3DSSG[18] 3DSG 363KRGB-Dimages,1482scans,478scenes, TPV 534 40 3Dmodel,3Dgraph 2020
Rel3D[50] 3DSG 27KRGB-Dimages,99903DScenes TPV 67 30 3Dmodel 2020
ScanNet[51] 3DImages 2.5MRGB-Dimages,1513indoorscenes TPV 20 - SS,3Dmodel 2017
Matterport3D[52] 3DImages 194,400RGB-Dimages,90building-scalescenes TPV 40 - SS,3Dmodel 2017
Nuscenes[53] 2DVideo+PC 1Kvideos(avg.20s),1.3Mpointclouds Vehicle 23 - 3DBB 2020
WAYMO[54] 2DVideo+PC 1.2Kvideos(avg.20s),177Kpointclouds Vehicle 20 - 2DBB,3DBB 2020
Sail-VOS3D[55] 3DVideo 484videos,238KRGB-Dimage,6807clips egocentric 178 - SS,3Dmodel 2021
HOI4D[56] 3DVideo 4Kvideos,2.4MRGB-Dimage,610indoorscenes egocentric 16 11 PS,KP 2022
EgoBody[57] 3DVideo 125videos,199KRGB-Dimages,15indoorscenes egocentric,TPV 36 13 3Dmodel,KP 2022
PSG4D-GTA PSG4D 67videos(avg.84s),28KRGB-Dimages,28.3Bpointclouds TPV 35 43 PS,4DSG 2023
PSG4D-HOI PSG4D 2973videos(avg.20s),891KRGB-Dimages,282indoorscenes egocentric 46 15 PS,4DSG 2023
4 ThePSG-4DDataset
ThissectionoutlinesthedevelopmentofthePSG-4Ddataset. Webeginbyexploringexistingdatasets
thatinspiredthecreationofPSG-4D,followedbyapresentationofitsstatistics,andfinallyabrief
overviewofthestepsinvolvedinitsconstruction.
4.1 LeveragingExistingDatasetsforPSG-4D
RatherthanconstructingthePSG-4Ddatasetfromthegroundup,wesoughttoevaluatewhether
currently available datasets could either directly support or be adapted for the PSG-4D task. As
showninTable1,ourinitialexplorationfocusedon3Ddatasets,including3Dscenegraphdatasets
like3DSGG[18]andRel3D[50],alongwithmoreconventional3DdatasetssuchasScanNet[51]
andMatterport3D[52]. However,whilethesedatasetscanbeusedtoreconstructentirescenesand
cangenerate3Dvideosaccordingly,theresultingscenesremainstaticandlackdynamicelements.
Wethenshiftedourfocustovideodatasetscontaining3Dinformation. Autonomousdrivingdatasets
suchasNuscenes[53]andWAYMO[54]incorporatepointcloudvideos,particularlybird’s-eyeview
footage. Nevertheless,thevehicleswithinthesescenesareonlycapturedin2Dvideo. Whilethis
technicallyconstitutesadynamic4Dscene,itdoesnotalignwellwiththeobjectivesofthisstudy.
Thedynamicrelationsintrafficscenariosarerelativelylimited,andourgoalistodevelopavisual
understandingmodelforembodiedAI[58–61]thatcaptures3Dscenesfromtheagent’sperspective,
notabird’s-eyeview.
Anothercategoryof3DvideosusesRGB-Dsequencesasinput,whichcanbeeasilyconvertedinto
pointclouds. Thisdataformatalignsperfectlywiththeoperationofintelligentagents,mimicking
humanperception,whichcapturescontinuousRGBimageswithdepth. Thankfully,recentdatasets
likeSAIL-VOS3D[55],HOI4D[56],andEgoBody[57]haveadoptedthisapproach. WhileSAIL-
VOS3DusessyntheticdatafromtheGTAgame[21],theHOI4DdatasetcapturesegocentricRGB-D
videosofsimpletasks,suchastoolpicking. Ontheotherhand,theEgoBodydataset[57]records
office activities like conversations, but lacks segmentation annotation and is primarily intended
forhumanposereconstruction. Despiteitswealthofvideos,theobjectinteractioninEgoBodyis
limited. In the medical domain, 4D-OR [60] excels in providing detailed depictions of surgical
scenes,showcasingitsspecializedutility. Tocatertoabroaderspectrumofresearchapplications,
weformulatedthePSG-4Ddataset,integratingtheversatilestrengthsoftheSAIL-VOS3D[55]and
HOI4D[56]datasets.
4.2 DatasetStatistics
Figure2presentsaselectionoffourvideoframes,drawnfromboththePSG4D-GTAandPSG4D-
HOIdatasets. EachframeisanRGB-Dvideowithcorrespondingpanopticsegmentationannotations.
Underneatheachscene,wedepicttheassociatedscenegraphandstatisticalwordclouds. Annotators
constructedthesescenegraphsastriplets,completewithframeduration. ThePSG4D-GTAdatasetis
4Figure2: TheExamplesandWordCloudsofPSG-4Ddataset. ThePSG-4Ddatasetcontains2
subsets,including(a)PSG4D-GTAselectedfromtheSAIL-VOS3D[20]dataset,and(b)PSG4D-
HOIfromHOI4D[22]dataset. Weselected4framesofanexamplevideofromeachsubset. Each
frame has aligned RGB and depth with panoptic segmentation annotation. The scene graph is
annotatedintheformoftriplets. Thewordcloudforobjectandrelationcategoriesineachdatasetis
alsorepresented.
particularlynoteworthyforitscomposition:itcontains67videoswithanaveragelengthof84seconds,
amountingto27,700RGB-Dimages,28.3billionpointclouds,andcomprises35objectcategories,
and43relationshipcategories. Thissyntheticdatasetwascapturedfromathird-personperspective.
Incontrast,thePSG4D-HOIdatasetiscompiledfromanegocentricperspective,providingadifferent
contextforanalysis. Itincludes2,973videoswithanaveragedurationof20seconds,equatingto
891,000RGB-Dimagesacross282indoorscenes. Thisdatasetincludes46objectcategoriesand15
object-objectrelationshipcategories,offeringadiverserangeofreal-worlddataforthestudy. The
combinationofthesetwodatasetsoffersacomprehensiveunderstandingof4Denvironmentsdue
totheircomplementarynature. Astatisticaloverviewofbothdatasetscanbefoundinthefinaltwo
rowsofTable1.
4.3 DatasetConstructionPipeline
As outlined in Section 4.1, the PSG4D-GTA is built upon the SAIL-VOS 3D dataset, while the
PSG4D-HOI is derived from the HOI4D dataset. To adapt the SAIL-VOS 3D dataset for our
purpose, wecommencedwithacomprehensivereviewofall178GTAvideoswithinthedataset.
ThisstageinvolvedameticulouseliminationprocesstoexcludevideoscontainingNSFWcontent,
resulting ina refinedpool of67 videos. The SAIL-VOS3D dataset, whichis equipped with3D
instancesegmentation,requiredadditionalannotationforbackgroundelementstointegratepanoptic
segmentation.LeveragingthePVSGannotationpipeline,weemployedaneventdetectionmethod[62]
to isolate the key frames. The background elements within these key frames were subsequently
annotatedusingthepre-annotationprovidedbytheSAMmodel[63]. Uponcompletionofkeyframe
annotations,theAOTmethod[64]wasutilizedtopropagatethesegmentationacrosstheentirevideo
sequence. Thefinalstepinvolvedoverlayingtheinstancesegmentationonthestuffsegmentation,
therebycompletingtheprocess. TheHOI-4Ddataset,devoidofNSFWcontent,alreadyprovidesa
4Dpanopticsegmentation. Consequently,weincludedallvideosfromtheHOI-4Ddatasetinthe
PSG4D-HOIdatasetwithoutfurthermodifications.
Uponcompletionof4Dpanopticsegmentationannotation,weproceedtoannotatethedynamicscene
graphaccordingtothemasks. AlthoughHOI4Dincludesactionannotationconcerningtheperson,it
doesn’taccountforinteractionsbetweenobjects. Nevertheless,certainactionssuchas“pickup”are
appropriatelyconsideredpredicates,andweautomaticallypositionthekeyobjectinthevideotoform
asubject-verb-objecttriplet. Oncetheautomatic-annotateddatasetisprepared,weaskannotators
toreviewandrevisethepre-annotationstoensureaccuracy. AsSAIL-VOS3Dlacksallkindsof
relationalannotation, we commencescene graphannotation fromscratch. The entireannotation
processisdiligentlyexecutedbytheauthorsaroundtheclock.
5 Methodology
ThissectiondetailsaunifiedpipelinePSG4DFormerforaddressingthePSG-4Dproblem. Asshown
inFigure3,ourapproachcomprisestwostages. Theinitial4Dpanopticsegmentationstageaimsto
segmentall4Dentities,includingobjectsandbackgroundelementsinFigure3(a),withtheaccurate
5wall
4D Panoptic bottle-2bottle-3bottle-4
bottle-5 Relation
4D
#3
5S 4c -e #5n 83e Graph
Segmentation bottle-1 table-1 hand-1 reach out for bottle-1
Model #584 -#892
Model table-2 hand-1 h…and-1 grasp bottle-1
ground
Option 1: RGB-D Input … mask tube … … … Relation in interest
RGB mask feature … … E Dn ec
p m
cac lasla
k
ss ss
:
:
b
fh oea tan
tt
lud
ere bottle-1 hand-1 … …
FCFCFCFCFCFCFC
hand-1 g [r 5a 8s 4p , b 8o 9t 2tl ]e-1
Enc
m ca lask ss: tf ae ba lt eur…e
…table-1
bottle-1 hand-1
Option 2: Point Cloud Input predicted mask tubes with feature Train GT confidence score of “Grasp”
hand Feature
Point Cloud bottle Association
Segmentor feature
ta…ble tube ground truth mask tubes ground truth relations (e.g., hand grasp bottle)
(a) Frame-Level Panoptic Segmentation (b) Tracking (c) Inference(↑)and Training (↓)of Relation Model
Figure3: IllustrationofthePSG4DFormerpipeline. ThisunifiedpipelinesupportsbothRGB-D
andpointcloudvideoinputsandiscomposedoftwomaincomponents: 4Dpanopticsegmentation
modeling and relation modeling. The first stage seeks to obtain the 4D panoptic segmentation
maskforeachobject,alongwithitscorrespondingfeaturetubespanningthevideolength. Thisis
accomplishedwiththeaidof(a)frame-levelpanopticsegmentationand(b)atrackingmodel. The
subsequentstage(c)employsaspatial-temporaltransformertopredictpairwiserelationsbasedonall
featuretubesderivedfromthefirststage.
temporalassociationinFigure3(b). Weextractfeaturesforeachobjectandobtainfeaturetubes
accordingtotrackingresultsforsubsequentrelationmodelinginFigure3(c).
5.1 4DPanopticSegmentationModeling
AsspecifiedinSection3,givena3Dvideoclipinput,suchasanRGB-DsequenceofI∈RT×H×W×4
orapointcloudsequenceofI∈RT×M×6,theinitialstage’sgoalistosegmentandtrackeachpixel
non-overlappingly. Themodelpredictsasetofvideoclipswiththeoutputof(m ,q ,p )N ,where
i i i i=1
m denotesthetrackedobjectmasktube,q denotesthetrackedfeaturetube,andp representsthe
i i i
probability of the object belonging to each category. N is the number of entities, encompassing
thingsandstuffclasses.
Frame-Level Panoptic Segmentation with RGB-D Sequence Given the dual input of RGB
anddepthimages,weadoptaseparation-and-aggregationgate(SA-Gate)[65]toefficientlyblend
informationfrombothmodalities. Thiscombinedfeatureset,enrichedwithdatafrombothinputs,is
thenfedintoarobustMask2Former[4]forframe-levelpanopticsegmentation. Intheinferencestage,
attheframet,givenanRGB-DimageI,theMask2FormerwithSA-Gatedirectlyoutputsasetof
objectqueryfeaturesqt ∈Rd,i=1,...,N,eachqtrepresentingoneentityattheframet.
i i
Frame-LevelPanopticSegmentationwithPointCloudSequence Apartfromperceivingpoint
cloudsequencesdirectly,3DpointcloudcoordinatescanbecalculatedandconvertedfromRGB-D
data. ThisconversioninvolvescomputingtheNormalizedDeviceCoordinates(NDC)usingthedepth
mapandprojectingtheNDCtoworldcoordinatesusingthetransformationmatrixprovided. We
retainonlypointswithadepthbelowadefinedthresholdλ,discardingdistant,lessrelevantelements
like far-off mountains. To leverage texture information from the image, point cloud coordinates
canbeaugmentedwithcorrespondingRGBvalues,creatingacolorfulpointcloudrepresentation
P∈RM×6,whereM isthetotalnumberofpointsinaframe.
WeemployDKNet[66],astate-of-the-artindoorsegmentationmethod,asourpointcloudsegmenta-
tionnetwork. Itprocessesinputpointcloudswitha3DUNet-like[67]backboneandusessparse
convolutions[68]forfeatureextraction. DKNetlocalizesinstancecentroidsbasedonacandidate
mining branch and encodes each instance’s information into an instance kernel k ∈ Rd. These
i
instance kernels {k }N are used as the weights of a few convolution layers to obtain the final
i i=1
instancemasks.
6
erutaef
erawa-htped
citponaP rotnemgeS
laropmeT-laitapS
redocnE
remrofsnarT
ecnerefnITracking Afterframe-levelpanopticsegmentation,welinkeachframeviausingUniTrack[69]
fortrackingtoobtainthefinaltrackedvideocubesforeachclipforeithermodalityinput. Specifically,
instead of incorporating an additional appearance model for tracking embedding extraction, we
directlyutilizetheinstancekernels{k }N fromthesegmentationstepofDKNet,orobjectquery
i i=1
features{q }N fromMask2Formerasthetrackingembeddingsfortheassociation. Wefindthat
i i=1
theinstancekernelsexhibitsufficientdistinctivenessfortrackingpurposes,evenwhendealingwith
differentobjectsbelongingtothesamesemanticclass. Thisisprimarilybecauseeachinstancekernel
isdesignedtomaximizetheresponseforaspecificinstancewhilesuppressingtheresponsesofall
otherinstances,includingthosewiththesamesemanticclass. Foravideosequencewiththelength
T,theobtained4DfeaturetubesarenotedasQ ={qt}T .
i i t=1
5.2 RelationModeling: 4DSceneGraphGeneration
TheobjectquerytubesQ andmasktubesm formabridgebetweenthefirstandsecondstages.
i i
Thesefeaturetubesfirstpassthroughaspatial-temporaltransformerencoder,whichaugmentsthem
withbothglobalcontextinformationfromtheoverallimageandglobaltemporalspace.
Spatial-TemporalTransformerEncoder Toinfusethefeaturetubeswithadditionaltemporal
dimensioninformationandcharacteristicsfromotherobjectsinthescene,wedrawinspirationfrom
the Spatial-Temporal Transformer [70]. A spatial encoder is initially employed. For all objects
co-occurringatthesametimet,atwo-layertransformerencoderisappliedtotheinput,comprising
allobjectfeaturesspecifictotimeframet. Thespatially-encodedfeaturetubeupdatestheobject
featuretubeinto{q˜t}N . Subsequently,atemporaltransformerencoderupdateseachobjectfeature
i i=1
tubealongthetemporaldimensionT. Byleveragingboththespatialandtemporalencoders, we
obtainthefinalfeaturetube{qˆt}N ,readyforrelationtraining.
i i=1
RelationClassificationTraining Totraintherelationmodelbasedontheupdatedquerytube,a
trainingsetforrelationtrainingmustbeconstructed. Itisworthnotingthattherelationannotation
inthetrainingsetisintheformof“object-1relationobject-2”,withthemasktubeofbothobjects
annotated. Tostart,weassociatetheupdatedquerytubewithgroundtruthobjects. Foreachground
truthtube,wefindthemostsuitableupdatedquerytubebycalculatingthevideoIntersectionover
Union(vIOU)betweengroundtruthmasktubes,andassignthequeryfeaturetubetotherespective
objects. A frame-level predicate classification is conducted with the assistance of a lightweight
fully-connectedlayer. Theinferenceoftherelationclassificationcomponentsimplycomputesthe
relationprobabilitybetweenpairsofqˆtandqˆt.
i j
6 Experiments
Table2presentstheresultsofexperimentsconductedonthePSG-4Ddataset. ForRGB-Dsequences,
anImageNet-pretrainedResNet-101servesasboththeRGBanddepthencoder. Wesetthetraining
durationto12epochs. TheDKNet,trainedfromscratch,requiresalongertrainingperiodof200
epochs. Inthesecondstage,bothspatialandtemporaltransformerencodersspantwolayers,and
trainingcontinuesforanadditional100epochs.BesidesthestandardPSG4DFormer,wealsoexamine
variantswiththetemporalencoderremoved(denotedas“/t”)andthedepthbranchremoved(denoted
as“/d”). Asabaseline,weusethe3DSGGmodel[18],whichemploysaGNNmodeltoencode
frame-levelobjectandrelationinformation,withoutconsideringtemporaldata.
RGB-Dvs.PointCloudInput Table2isdividedintotwosections.Theupperpart(#1-#3)reports
resultsfrompointcloudinput,whilethelatterpart(#4-#7)detailsresultsfromtheRGB-Dsequence.
ItappearsthattheRGB-Dsequencegenerallyyieldsbetterresultsthanthepointcloudsequence,
particularly for the PSG4D-GTA dataset. This could potentially be attributed to the ResNet-101
backboneusedfortheRGB-Ddata,whichbeingpretrainedonImageNet,exhibitsrobustperformance
oncomplexdatasetslikePSG4D-GTA.Meanwhile,thePSG4D-HOIdatasetseemstoofferamore
consistentscenariowithabundanttrainingdata,thusnarrowingtheperformancegapbetweenthe
pointcloudandRGB-Dmethods.
SignificanceofDepth TheresultsinTable2alsoallowustoevaluatetheimportanceofdepth
intheRGB-Dmethod. Specifically,wedesignedavariantofPSG4DFormer(markedas“/d”)that
7Table 2: Main Results on PSG4D. Experimental results are reported on both the PSG4D-GTA
andPSG4D-HOIdatasets. Inadditiontocomparingwithtraditional3DSGGmethods,weconduct
experimentstocomparethePSG4DFormeranditsvariants. Thisincludesaversionwiththetemporal
encoderremoved(denotedas“/t”)andonewiththedepthbranchremoved(denotedas“/d”).
Input PSG4D-GTA PSG4D-HOI
Method
Type
R/mR@20 R/mR@50 R/mR@100 R/mR@20 R/mR@50 R/mR@100
Point #13DSGG[18] 1.48/0.73 2.16/0.79 2.92/0.85 3.46/2.19 3.15/2.47 4.96/2.84
Cloud #2PSG4DFormer/t 2.25/1.03 2.67/1.72 3.14/2.05 3.26/2.04 3.16/2.35 4.18/2.64
Sequence #3PSG4DFormer 4.33/2.10 4.83/2.93 5.22/3.13 5.36/3.10 5.61/3.95 6.76/4.17
#43DSGG[18] 2.29/0.92 2.46/1.01 3.81/1.45 4.23/2.19 4.47/2.31 4.86/2.41
RGB-D #5PSG4DFormer/t 4.43/1.34 4.89/2.42 5.26/2.83 4.44/2.37 4.83/2.43 5.21/2.84
Sequence #6PSG4DFormer/d 4.40/1.42 4.91/1.93 5.49/2.27 5.49/3.42 5.97/3.92 6.43/4.21
#7PSG4DFormer 6.68/3.31 7.17/3.85 7.22/4.02 5.62/3.65 6.16/4.16 6.28/4.97
doesn’tutilizethedepthbranch. Inotherwords,boththeRGBencoderandtheSA-Gateareremoved,
turningthepipelineintoavideoscenegraphgenerationpipeline. Theperformanceofthisvariantis
inferiorcomparedtotheoriginal,whichhighlightsthesignificanceofdepthinformationinthescene
graphgenerationtask.
Necessity of Temporal Attention Table 2 includes two methods that do not utilize temporal
attention. Specifically,the3DSGGbaselinelearnsinteractionsbetweenstaticobjectfeaturesusinga
graphconvolutionalnetwork,whilePSG4DFormer/t removesthetemporaltransformerencoders.
Theresultsdemonstratethatignoringthetemporalcomponentcouldleadtosub-optimaloutcomes,
emphasizingtheimportanceoftemporalattentionin4Dscenegraphgeneration.
7 Real-WorldApplication
ThissectionillustratesthedeploymentofthePSG-4Dmodelinareal-worldapplication,specifically
withinaservicerobot. Itextendsbeyondtheoreticalconceptsandcomputationalmodels,delving
intothepracticalintegrationandexecutionofthiscutting-edgetechnology. AsshowninFigure4,the
focushereistodemonstratehowtherobotleveragesthePSG-4Dmodel(pretrainedfromPSG4D-HOI,
RGB-Dinput)tointerpretandrespondtoitssurroundingseffectively.
InteractionwithLargeLanguageModels Therecentadvancementsinlargelanguagemodels
(LLMs)havedisplayedtheirexceptionalcapabilitiesinreasoningandplanning[71]. LLMshave
beenutilizedasplannersinnumerousrecentstudiestobridgedifferentmodalities,pavingthewayfor
moreintuitiveandefficienthuman-machineinteraction[72]. Inthiswork,weemployGPT-4[71],as
theprimaryplanner. Designedtoalignwithhumaninstruction,GPT-4communicateswiththerobot
bytranslatingtherawscenegraphrepresentationsintocomprehensiblehumanlanguage. Therefore,
theinteractionbeginswiththeprompt,“Iamaservicerobot. Forevery30seconds,Iwillgiveyou
whatIhaveseeninthelast30seconds. PleasesuggestmewhatIcouldserve.”Subsequently,every
30seconds,therobotengageswithGPT-4,providinganupdate: “Inthepast30s,whatIcaptured
is: <fromstart_timetoend_time,object-1relationobject-2>,<...>,<...>.”ThisenablesGPT-4to
analyzethesituationandprovideappropriatefeedback.
Post-ProcessingforExecution TheeffectivedeploymentofthePSG-4Dmodelnecessitatesa
robustsetofpredefinedactionsthattherobotcanexecute. Currently,theactionlistincludestasks
suchaspickinguplitterandengaginginconversationwithindividuals. AfterGPT-4providesits
suggestions,itisfurtherpromptedtoselectasuitableactionfromthispredefinedlistfortherobotto
execute. However,theflexibilityofthissystemallowsfortheexpansionofthisactionlist,pavingthe
wayformorecomplexandvariedtasksinthefuture. Toencouragecommunityinvolvementandthe
developmentoffascinatingapplications,wealsoreleasetherobotdeploymentmodulealongsidethe
PSG4Dcodebase. Thedemorobotispricedatapproximately$1.2Kandcomesequippedwithan
RGB-Dsensor,microphone,speakers,andaroboticarm.
8(a) The RGB-D sequence that is captured by the robot. (d) Robot Reaction
(b) PSG-4D Parsing (c) Reasoning & Planning
14.2s –17.6s I am a service robot, In the past
person-1drinking frombottle-1
PSG4D 30s, what I captured is: [ ]. Is
18.8s –20.0s there anything I could serve?
person-1throwing bottle-1
20.0s –30.0s 1. Cleanup: …; Don‘t litter!
bottle-1on ground 2. Reminder: …
Figure 4: Demonstration of a Robot Deployed with the PSG-4D Model. The service robot
interpretstheRGB-Dsequenceshownin(a),whereamanisseendrinkingcoffeeandsubsequently
droppingtheemptybottleontheground. Therobotprocessesthissequence,translatingitintoa4D
scenegraphdepictedin(b). Thisgraphcomprisesasetoftemporallystampedtriplets,witheach
objectassociatedwithapanopticmask,accuratelygroundingitin3Dspace. Therobotregularly
updatesitsPSG4DtoGPT-4,awaitingfeedbackandinstructions. Inthisscenario,GPT-4advisesthe
robottocleanupthediscardedbottleandremindthemanabouthisaction. Thisdirectiveistranslated
intorobotaction,asvisualizedin(d).
8 Conclusion,Challenges,andOutlook
Thispaperpresentsanovelanddemandingextensiontothetraditionalscenegraphgeneration,the4D
PanopticSceneGraphGeneration,whichincorporatesthespatio-temporaldomainintotheframework.
Weintroduceacomprehensiveframework,thePSG4DFormer,capableofprocessingbothRGB-D
andpointcloudsequences. Thesuccessfuldeploymentofthispipelineinapracticalservicerobot
scenario underscores its potential in real-world applications. However, these achievements also
highlightthenascentstateofthisfield,emphasizingthenecessityforcontinuedadvancementsto
fullyexploitthepotentialof4DPanopticSceneGraphGeneration.
Challenges Despiteencouragingresults,wehavealsorevealedseveralpersistentchallengesinthe
realmof4DPanopticSceneGraphGeneration. Throughourdemonstration,wefoundthatcurrent
models,whetherderivedfromPSG4D-GTAorPSG4D-HOI,canhandleonlysimplescenesandfalter
whenfacedwithmorecomplexreal-worldenvironments. Notably,thereexistrobustmodelstrained
inthe2Dworld. Findingeffectiveandefficientstrategiestoadaptthesemodelstothe4Ddomain
presentsacompellingdirectionforfutureexploration.
Outlook Futureworkinthisfieldpresentsseveralintriguingtrajectories. Thereisapressingneed
for more efficient algorithms for 4D Panoptic Scene Graph Generation, which can handle larger
and more diverse environments. Equally important is the creation of comprehensive and diverse
datasetsthatwouldallowmorerigorousevaluationandfosteradvancementsinmodeldevelopment.
ParticularlynoteworthyisarecentDigitalTwindataset[73],whichpromisesahighlevelofaccuracy
andphotorealism,aligningseamlesslywiththeobjectivesofPSG4D.Thisdatasetwillbeincorporated
asthethirdsubsetofthePSG4Ddataset,readilyaccessiblefromourcodebase.Inadditiontorobotics,
asdemonstratedbythepracticalapplicationofPSG4DFormer,wearealsoexploringitspotentialasan
autonomousplayerintheGTAgame. Actually,ourrecentendeavorOctopus[58]strivestocomplete
GTAmissionsbyemployingavisual-languageprogrammertogenerateexecutableactioncode. In
contrasttothepreviouslypassivetaskcompletion,theapplicationinthispaperactivelyperceives
andunderstandstheenvironment,showcasingashifttowardsautonomyinrobotics. Furthermore,
Octopus[58]utilizesa4Dscenegraphstructuretocaptureenvironmentalinformationduringthe
visual-languageprogrammertraining,exemplifyingapracticalapplicationofthePSG4Dmodality.
9Weeagerlyanticipatethefutureprogressinthefieldof4DPanopticSceneGraphGenerationandits
potentialtorevolutionizeourunderstandingofreal-worlddynamics.
PotentialNegativeSocietalImpacts Thisworkreleasesadatasetcontaininghumanbehaviors,
posingpossiblegenderandsocialbiasesinherentlyfromdata. Potentialusersareencouragedto
considertherisksofoversightingethicalissuesinimbalanceddata,especiallyinunderrepresented
minorityclasses. Nevertheless,alltheNSFWcontentisremovedfromthedataset.
Acknowledgement
This study is supported by the Ministry of Education, Singapore, under its MOE AcRF Tier 2
(MOE-T2EP20221-0012),NTUNAP,andundertheRIE2020IndustryAlignmentFund–Industry
CollaborationProjects(IAF-ICP)FundingInitiative,theNationalKeyR&DProgramofChinaunder
grantnumber2022ZD0161501,aswellascashandin-kindcontributionfromtheindustrypartner(s).
References
[1] XiaojianMa,SilongYong,ZilongZheng,QingLi,YitaoLiang,Song-ChunZhu,andSiyuanHuang.
Sqa3d:Situatedquestionansweringin3dscenes. arXivpreprintarXiv:2210.07474,2022. 2
[2] DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,Ayzaan
Wahid,JonathanTompson,QuanVuong,TianheYu,etal. Palm-e:Anembodiedmultimodallanguage
model. arXivpreprintarXiv:2303.03378,2023.
[3] SoniaRaychaudhuri,TommasoCampari,UnnatJain,ManolisSavva,andAngelXChang. Reduce,reuse,
recycle:Modularmulti-objectnavigation. arXivpreprintarXiv:2304.03696,2023.
[4] BowenCheng,IshanMisra,AlexanderG.Schwing,AlexanderKirillov,andRohitGirdhar. Masked-
attentionmasktransformerforuniversalimagesegmentation. 2022. 6
[5] XiangtaiLi,HaoboYuan,WenweiZhang,GuangliangCheng,JiangmiaoPang,andChenChangeLoy.
Tube-link:Aflexiblecrosstubebaselineforuniversalvideosegmentation. InICCV,2023.
[6] XiangtaiLi,HenghuiDing,WenweiZhang,HaoboYuan,GuangliangCheng,PangJiangmiao,KaiChen,
ZiweiLiu,andChenChangeLoy. Transformer-basedvisualsegmentation: Asurvey. arXivpre-print,
2023. 2
[7] DanfeiXu,YukeZhu,ChristopherBChoy,andLiFei-Fei. Scenegraphgenerationbyiterativemessage
passing. InCVPR,2017. 2,3
[8] KaihuaTang,HanwangZhang,BaoyuanWu,WenhanLuo,andWeiLiu. Learningtocomposedynamic
treestructuresforvisualcontexts. InCVPR,2019. 2,3
[9] RowanZellers,MarkYatskar,SamThomson,andYejinChoi. Neuralmotifs:Scenegraphparsingwith
globalcontext. InCVPR,2018. 2,3
[10] MohammedSuhail,AbhayMittal,BehjatSiddiquie,ChrisBroaddus,JayanEledath,GerardMedioni,and
LeonidSigal. Energy-basedlearningforscenegraphgeneration. InCVPR,2021.
[11] YiwuZhong,JingShi,JianweiYang,ChenliangXu,andYinLi. Learningtogeneratescenegraphfrom
naturallanguagesupervision. InICCV,2021. 2,3
[12] JingkangYang,YiZheAng,ZujinGuo,KaiyangZhou,WayneZhang,andZiweiLiu. Panopticscene
graphgeneration. InEuropeanConferenceonComputerVision,pages178–196.Springer,2022. 2,3
[13] JingkangYang,WenxuanPeng,XiangtaiLi,ZujinGuo,LiangyuChen,BoLi,ZhengMa,KaiyangZhou,
WayneZhang,ChenChangeLoy,andZiweiLiu. Panopticvideoscenegraphgeneration. InCVPR,2023.
2,3
[14] XindiShang,TongweiRen,JingfanGuo,HanwangZhang,andTat-SengChua. Videovisualrelation
detection. InACMMM,2017. 2,3
[15] XindiShang,DonglinDi,JunbinXiao,YuCao,XunYang,andTat-SengChua. Annotatingobjectsand
relationsinuser-generatedvideos. InICMR,2019. 2,3
[16] MatthewFisher,ManolisSavva,andPatHanrahan. Characterizingstructuralrelationshipsinscenesusing
graphkernels. InACMSIGGRAPH2011papers,pages1–12.2011. 2,3
10[17] RobertFTobler. Separatingsemanticsfromrendering: ascenegraphbasedarchitectureforgraphics
applications. TheVisualComputer,27(6-8):687–695,2011. 2,3
[18] JohannaWald,HelisaDhamo,NassirNavab,andFedericoTombari. Learning3dsemanticscenegraphs
from3dindoorreconstructions. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages3961–3970,2020. 2,3,4,7,8
[19] ShoulongZhang,AiminHao,HongQin,etal. Knowledge-inspired3dscenegraphpredictioninpoint
cloud. AdvancesinNeuralInformationProcessingSystems,34:18620–18632,2021. 2,3
[20] Y.-T.Hu,J.Wang,R.A.Yeh,andA.G.Schwing. SAIL-VOS3D:ASyntheticDatasetandBaselinesfor
ObjectDetectionand3DMeshReconstructionfromVideoData. InProc.CVPR,2021. 2,5
[21] Grandtheftautov,2014. 2,4
[22] YunzeLiu, YunLiu, CheJiang, KangboLyu, WeikangWan, HaoShen, BoqiangLiang, ZhoujieFu,
HeWang,andLiYi. Hoi4d: A4degocentricdatasetforcategory-levelhuman-objectinteraction. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),pages
21013–21022,June2022. 2,5
[23] RanjayKrishna,YukeZhu,OliverGroth,JustinJohnson,KenjiHata,JoshuaKravitz,StephanieChen,
Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei. Visual genome:
Connectinglanguageandvisionusingcrowdsourceddenseimageannotations. IJCV,2017. 3
[24] RongjieLi,SongyangZhang,andXumingHe. Sgtr:End-to-endscenegraphgenerationwithtransformer.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages19486–
19496,2022. 3
[25] YurenCong, MichaelYingYang, andBodoRosenhahn. Reltr: Relationtransformerforscenegraph
generation. arXivpreprintarXiv:2201.11460,2022. 3
[26] JingweiJi,RanjayKrishna,LiFei-Fei,andJuanCarlosNiebles. Actiongenome:Actionsascompositions
ofspatio-temporalscenegraphs. InCVPR,2020. 3
[27] JinghaoWang,ZhengyuWen,XiangtaiLi,ZujinGuo,JingkangYang,andZiweiLiu. Pairthenrelation:
Pair-netforpanopticscenegraphgeneration. arXivpreprintarXiv:2307.08699,2023. 3
[28] ChengyangZhao,YikangShen,ZhenfangChen,MingyuDing,andChuangGan. Textpsg:Panopticscene
graphgenerationfromtextualdescriptions. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages2839–2850,2023.
[29] ZijianZhou,MiaojingShi,andHolgerCaesar. Hilo:Exploitinghighlowfrequencyrelationsforunbiased
panopticscenegraphgeneration. arXivpreprintarXiv:2303.15994,2023.
[30] JulianLorenz,FlorianBarthel,DanielKienzle,andRainerLienhart. Haystack:Apanopticscenegraph
datasettoevaluaterarepredicateclasses. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages62–70,2023.
[31] JingkangYang,ZhengMa,QixunWang,XiaofengGuo,HaofanWang,ZiweiLiu,WayneZhang,Xing
Xu,andHaiZhang. Thepsgchallenge:towardscomprehensivesceneunderstanding. NationalScience
Review,10(6):nwad126,2023. 3
[32] Xiangtai Li, Wenwei Zhang, Jiangmiao Pang, Kai Chen, Guangliang Cheng, Yunhai Tong, and
Chen Change Loy. Video k-net: A simple, strong, and unified baseline for video segmentation. In
CVPR,2022. 3
[33] JaewonBae,DongminShin,KangbeenKo,JuchanLee,andUe-HwanKim. Asurveyon3dscenegraphs:
Definition,generationandapplication. InRobotIntelligenceTechnologyandApplications7:Resultsfrom
the10thInternationalConferenceonRobotIntelligenceTechnologyandApplications,pages136–147.
Springer,2023. 3
[34] Ue-Hwan Kim, Jin-Man Park, Taek-Jin Song, and Jong-Hwan Kim. 3-d scene graph: A sparse and
semanticrepresentationofphysicalenvironmentsforintelligentagents. IEEEtransactionsoncybernetics,
50(12):4921–4933,2019. 3
[35] IroArmeni,Zhi-YangHe,JunYoungGwak,AmirRZamir,MartinFischer,JitendraMalik,andSilvio
Savarese. 3dscenegraph:Astructureforunifiedsemantics,3dspace,andcamera. InICCV,2019. 3
11[36] CharlesRQi,HaoSu,KaichunMo,andLeonidasJGuibas. Pointnet:Deeplearningonpointsetsfor3d
classificationandsegmentation. InCVPR,2017. 3
[37] AntoniRosinol,AndrewViolette,MarcusAbate,NathanHughes,YunChang,JingnanShi,ArjunGupta,
and Luca Carlone. Kimera: From slam to spatial perception with 3d dynamic scene graphs. The
InternationalJournalofRoboticsResearch,40(12-14):1510–1546,2021. 3
[38] Shun-ChengWu,JohannaWald,KeisukeTateno,NassirNavab,andFedericoTombari. Scenegraphfusion:
Incremental3dscenegraphpredictionfromrgb-dsequences. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages7515–7525,2021. 3
[39] HemaKoppulaandAshutoshSaxena. Learningspatio-temporalstructurefromrgb-dvideosforhuman
activitydetectionandanticipation. InInternationalconferenceonmachinelearning, pages792–800.
PMLR,2013. 3
[40] QianXie,OussamaRemil,YanwenGuo,MengWang,MingqiangWei,andJunWang. Objectdetection
andtrackingunderocclusionforobject-levelrgb-dvideosegmentation. IEEETransactionsonMultimedia,
20(3):580–592,2017.
[41] GuyueZhang,JunLiu,HengduoLi,YanQiuChen,andLarrySDavis.Jointhumandetectionandheadpose
estimationviamultistreamnetworksforrgb-dvideos. IEEESignalProcessingLetters,24(11):1666–1670,
2017. 3
[42] DavidWeikersdorfer,AlexanderSchick,andDanielCremers. Depth-adaptivesupervoxelsforrgb-dvideo
segmentation. In2013IEEEInternationalConferenceonImageProcessing,pages2708–2712.IEEE,
2013. 3
[43] HuazhuFu,DongXu,andStephenLin. Object-basedmultipleforegroundsegmentationinrgbdvideo.
IEEETransactionsonImageProcessing,26(3):1418–1427,2017.
[44] StevenHickson,StanBirchfield,IrfanEssa,andHenrikChristensen. Efficienthierarchicalgraph-based
segmentationofrgbdvideos. InProceedingsoftheIEEEConferenceonComputerVisionandPattern
Recognition,pages344–351,2014.
[45] NumairKhan,QianZhang,LucasKasser,HenryStone,MinHKim,andJamesTompkin. View-consistent
4dlightfieldsuperpixelsegmentation. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages7811–7819,2019. 3
[46] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski
convolutionalneuralnetworks. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pages3075–3084,2019. 3
[47] WenweiZhang,HuiZhou,ShuyangSun,ZheWang,JianpingShi,andChenChangeLoy. Robustmulti-
modalitymulti-objecttracking. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pages2365–2374,2019.
[48] XinshuoWeng,YongxinWang,YunzeMan,andKrisMKitani. Gnn3dmot:Graphneuralnetworkfor3d
multi-objecttrackingwith2d-3dmulti-featurelearning. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages6499–6508,2020.
[49] XinshuoWeng,JianrenWang,DavidHeld,andKrisKitani. 3dmulti-objecttracking:Abaselineandnew
evaluationmetrics. In2020IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS),
pages10359–10366.IEEE,2020. 3
[50] AnkitGoyal,KaiyuYang,DaweiYang,andJiaDeng. Rel3d: Aminimallycontrastivebenchmarkfor
groundingspatialrelationsin3d. AdvancesinNeuralInformationProcessingSystems,33:10514–10525,
2020. 4
[51] AngelaDai,AngelXChang,ManolisSavva,MaciejHalber,ThomasFunkhouser,andMatthiasNießner.
Scannet:Richly-annotated3dreconstructionsofindoorscenes. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pages5828–5839,2017. 4
[52] AngelChang,AngelaDai,ThomasFunkhouser,MaciejHalber,MatthiasNiessner,ManolisSavva,Shuran
Song,AndyZeng,andYindaZhang. Matterport3d: Learningfromrgb-ddatainindoorenvironments.
arXivpreprintarXiv:1709.06158,2017. 4
[53] HolgerCaesar,VarunBankiti,AlexHLang,SourabhVora,VeniceErinLiong,QiangXu,AnushKrishnan,
YuPan,GiancarloBaldan,andOscarBeijbom.nuscenes:Amultimodaldatasetforautonomousdriving.In
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages11621–11631,
2020. 4
12[54] PeiSun,HenrikKretzschmar,XerxesDotiwalla,AurelienChouard,VijaysaiPatnaik,PaulTsui,JamesGuo,
YinZhou,YuningChai,BenjaminCaine,etal. Scalabilityinperceptionforautonomousdriving:Waymo
opendataset. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
pages2446–2454,2020. 4
[55] Yuan-TingHu,JiahongWang,RaymondAYeh,andAlexanderGSchwing. Sail-vos3d: Asynthetic
datasetandbaselinesforobjectdetectionand3dmeshreconstructionfromvideodata. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages1418–1428,2021. 4
[56] YunzeLiu, YunLiu, CheJiang, KangboLyu, WeikangWan, HaoShen, BoqiangLiang, ZhoujieFu,
HeWang,andLiYi. Hoi4d: A4degocentricdatasetforcategory-levelhuman-objectinteraction. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages21013–
21022,2022. 4
[57] SiweiZhang,QianliMa,YanZhang,ZhiyinQian,TaeinKwon,MarcPollefeys,FedericaBogo,and
SiyuTang. Egobody:Humanbodyshapeandmotionofinteractingpeoplefromhead-mounteddevices.
In ComputerVision–ECCV2022: 17th EuropeanConference, TelAviv, Israel, October23–27, 2022,
Proceedings,PartVI,pages180–200.Springer,2022. 4
[58] Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Chencheng Jiang, Haoran Tan, Jiamu
Kang, YuanhanZhang, KaiyangZhou, etal. Octopus: Embodiedvision-languageprogrammerfrom
environmentalfeedback. arXivpreprintarXiv:2310.08588,2023. 4,9
[59] KrishanRana,JesseHaviland,SouravGarg,JadAbou-Chakra,IanReid,andNikoSuenderhauf. Sayplan:
Grounding large language models using 3d scene graphs for scalable task planning. arXiv preprint
arXiv:2307.06135,2023.
[60] EgeÖzsoy,EvinPınarÖrnek,UlrichEck,TobiasCzempiel,FedericoTombari,andNassirNavab. 4d-or:
Semanticscenegraphsforordomainmodeling. InInternationalConferenceonMedicalImageComputing
andComputer-AssistedIntervention,pages475–485.Springer,2022. 4
[61] SaeidAmiri,KishanChandan,andShiqiZhang. Reasoningwithscenegraphsforrobotplanningunder
partialobservability. IEEERoboticsandAutomationLetters,7(2):5560–5567,2022. 4
[62] KiyotakaOtsujiandYoshinobuTonomura.Projectiondetectingfilterforvideocutdetection.InProceedings
ofthefirstACMinternationalconferenceonMultimedia,pages251–257,1993. 5
[63] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete
Xiao, SpencerWhitehead, AlexanderCBerg, Wan-YenLo, etal. Segmentanything. arXivpreprint
arXiv:2304.02643,2023. 5
[64] Zongxin Yang, Yunchao Wei, and Yi Yang. Associating objects with transformers for video object
segmentation. InNeurIPS,2021. 5
[65] XiaokangChen,Kwan-YeeLin,JingboWang,WayneWu,ChenQian,HongshengLi,andGangZeng.
Bi-directionalcross-modalityfeaturepropagationwithseparation-and-aggregationgateforrgb-dsemantic
segmentation. InEuropeanConferenceonComputerVision(ECCV),2020. 6
[66] YizhengWu,MinShi,ShuaiyuanDu,HaoLu,ZhiguoCao,andWeicaiZhong. 3dinstancesas1dkernels.
In ComputerVision–ECCV2022: 17th EuropeanConference, TelAviv, Israel, October23–27, 2022,
Proceedings,PartXXIX,pages235–252.Springer,2022. 6
[67] OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomedical
imagesegmentation. InMedicalImageComputingandComputer-AssistedIntervention–MICCAI2015:
18thInternationalConference,Munich,Germany,October5-9,2015,Proceedings,PartIII18,pages
234–241.Springer,2015. 6
[68] Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3d semantic segmentation with
submanifoldsparseconvolutionalnetworks. InProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition,pages9224–9232,2018. 6
[69] ZhongdaoWang,HengshuangZhao,Ya-LiLi,ShengjinWang,PhilipHSTorr,andLucaBertinetto. Do
differenttrackingtasksrequiredifferentappearancemodels? NeurIPS,2021. 7
[70] YurenCong, WentongLiao, HannoAckermann, BodoRosenhahn, andMichaelYingYang. Spatial-
temporaltransformerfordynamicscenegraphgeneration. InProceedingsoftheIEEE/CVFinternational
conferenceoncomputervision,pages16372–16382,2021. 7
[71] OpenAI. Gpt-4technicalreport. ArXiv,abs/2303.08774,2023. 8
13[72] WenlongHuang,P.Abbeel,DeepakPathak,andIgorMordatch. Languagemodelsaszero-shotplanners:
Extractingactionableknowledgeforembodiedagents. ArXiv,abs/2201.07207,2022. 8
[73] XiaqingPan,NicholasCharron,YongqianYang,ScottPeters,ThomasWhelan,ChenKong,OmkarParkhi,
RichardNewcombe,andYuhengCarlRen. Ariadigitaltwin:Anewbenchmarkdatasetforegocentric
3dmachineperception. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pages20133–20143,2023. 9
14