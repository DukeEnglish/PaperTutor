A DESIGN TRAJECTORY MAP OF HUMAN-AI COLLABORATIVE
REINFORCEMENT LEARNING SYSTEMS: SURVEY AND
TAXONOMY ∗
ZhaoxingLi
SchoolofElectronicsandComputerScience
UniversityofSouthampton
Southampton
{zhaoxing.li}@soton.ac.uk
ABSTRACT
Drivenbythealgorithmicadvancementsinreinforcementlearningandtheincreasingnumberof
implementationsofhuman-AIcollaboration,CollaborativeReinforcementLearning(CRL)hasbeen
receivinggrowingattention. Despitethisrecentupsurge,thisareaisstillrarelysystematicallystudied.
Inthispaper,weprovideanextensivesurvey,investigatingCRLmethodsbasedonbothinteractive
reinforcementlearningalgorithmsandhuman-AIcollaborativeframeworksthatwereproposedin
thepastdecade. Weelucidateanddiscussviasynergisticanalysismethodsboththegrowthofthe
field and the state-of-the-art; we conceptualise the existing frameworks from the perspectives of
designpatterns, collaborativelevels, partiesandcapabilities, andreviewinteractivemethodsand
algorithmic models. Specifically, we create a new Human-AI CRL Design Trajectory Map, as a
systematic modelling tool for the selection of existing CRL frameworks, as well as a method of
designingnewCRLsystems,andfinallyofimprovingfutureCRLdesigns.Furthermore,weelaborate
genericHuman-AICRLchallenges,providingtheresearchcommunitywithaguidetowardsnovel
researchdirections. Theaimofthispaperistoempowerresearcherswithasystematicframeworkfor
thedesignofefficientand’natural’human-AIcollaborativemethods,makingitpossibletoworkon
maximisedrealisationofhumans’andAI’spotentials.
Keywords CollaborativeRL·DesignPatterns·InteractiveMethods·CollaborativeLevels·Parties·Capabilities
1 Introduction
WiththerapiddevelopmentofArtificialIntelligence(AI)inrecentyears,themainstreammediaholdstwoopposing
views: AIwill’savetheworld’[1]or’destroy’it[2]. AIisdescribedasthe’saviour’,tofreehumansfromlabour,while
itisalsodescribedasthe’devil’whotakesawayworkers’jobs[3]. Regardlessofone’spointofview,AIisplayingan
increasinglysignificantpartinthefutureworld. WeakAI,strongAI,andsuperAI arethreestagesofAIdevelopment,
asproposedbyJohnSearle[4]. Duetothelimitationsofcurrenttechnology,Searlebelievesthatwearesupposedto
havebeenandstillbeinthe’weakAI’stageforalongtime. Thatis,atthiscurrentstage,AIoftenperformsmuch
worsethanhumansinhighlycomplexdecision-makingtasksthatrequireconsiderationsofmoralityandrisk,butmuch
betterintaskswithwell-specifiedfeedbackandlargescaledata. Therefore,thetwoextremesituationsdescribedby
themediaarestillfarfromthecurrentstagethatwe’veachieved[1,2]. ExploringthusthewayforhumansandAIto
bettercooperate,withthegoalofcomplementingeachother’sshortcomings,mayprovidethebestwayforwardforthe
immediatefuture.
A common classification of Artificial Intelligence algorithms is supervised learning, unsupervised learning, and
reinforcementlearning[5]. Problemsinvolvingdecision-makinggenerallylieinthefieldofreinforcementlearning[6],
andhowhumansandAIagentscancooperateandcomplementeachother’sshortcomingsisparticularlyimportant.
∗Citation:
4202
yaM
61
]CH.sc[
1v41201.5042:viXraRunningTitleforHeader
WhiletheinteractionbetweenhumansandAIagentsisanemergingresearchdirection,theresearchontheinteraction
betweenhumansandcomputershasalonghistory. Thecommunityhasproposedseveralpatternsofhuman-computer
interaction. Forexample,in1983,HollnagelandWoodsproposedtheCognitiveSystemsEngineering(CES)model
[7]. In1991,Schmidtetal. createdaconceptualparadigmclassifyinghuman-computercollaborationintothreelevels:
augmentative,integrative,anddebative[8]. In2009,Johnsonetal. proposedaco-activedesignpatterninhuman-AI
jointactivity[9].
Overthelastfewyears,collaborativeorinteractivereinforcementlearninghasbecomeanewfieldwithinthemachine
learningregime. SomeexcellentrecentsurveystudiesonCollaborativeReinforcementLearning(CollaborativeRL,
or CRL) have emerged, demonstrating this new field’s importance. They cover a wide range of issues including
CRL in general such as [10], and CRL applied in specific domains, such as safe RL [11], inverse RL [12], and
explainableRL[13]. Otherstudiesconcentrateonspecificdesignmethodologiessuchasuserfeedbackandtestbeds
of the environment [14]. When performing a brief search on Google Scholar with the keywords ’interactive AI’,
’collaboration’,’reinforcementlearning’,and’HCI’(Human-ComputerInteraction)fortheperiodfrom2011to2022,
wefoundthattherearemanysurveysorliteraturereviewspublished. Forexample,onlybetweenJanuary2020and
October2022,5surveysorliteraturereviewswerepublished. NajarandAnisreviewedreinforcementlearningbased
onhumanadvice[15]. GomezandRandy’sworkfocusedonhuman-centeredreinforcementlearning[16]. Puiutta
andErickpresentedareviewofexplainablereinforcementlearning[13]. ArzateandChristianpresentedasurveyon
thedesignprinciplesandopenchallengesofinteractivereinforcementlearning[17]. SuranandShwetaconsecrated
oncollectiveintelligence[18]. Itcanbeobservedthatthisresearchdirectionisgainingincreasingattentionfromthe
community. However,surveyingcollaborationbetweenhumansandAIagentsisbeingoverlooked,letaloneidentifying
theprobablefuturedirectionofthegrowthinthisfield. Tobridgethisgap,wethusaimtoaddressthefollowingresearch
question:
Howmaydesignersapproachtheconstructionprocessofhuman-AIcollaborativereinforcementlearningsystemsin
anstructuredmanner?
Toanswerthisresearchquestion,wesummariseexistingcollaborationapproachesandofferourownperspectivesand
proposals. Welookatclassichuman-machineinteractionstrategiesthathavehadasignificanteffectontheevolutionof
theHuman-ComputerInteraction(HCI)field. Weintendtogivescholarsandindustrypractitionerswithadesigntoolkit
thatcombinesarchetypesandspecifictoolsinamicro-view[19](seeFigure2). Furthermore,ourstudyintroduces
Human-AICollaborativeReinforcementLearningDesignTrajectoryMap,(seeFigure5),anewcategorisation
approachandsystematicmodellingtoolthatseekstosuggestresearchobjectivesforthenextgenerationofHuman-AI
CollaborativeDesign. Similartohowbuildersrequiretheblueprintdesignaswellasinstructionsonhowtoplan
differentfunctionalpartsandchoosevarioustypesofmaterialsforthehouse,theDesignTrajectoryMapprovides
researcherswithacomprehensivereviewregardingthedesignpatternsforHuman-AICollaborativeReinforcement
Learningsystems(seeSection4)andguidanceonhowtocustomisethecharacteristicsofdifferentcomponentstomeet
theirspecificrequirements(seeSections5and6),aswellashowtocustomisethealgorithmicmodels(seeSections8)
andtheinteractivemethods(seeSection7).
Thisstudybuildsonourpreviouswork[20]publishedatDIS’21: DesigningInteractiveSystemsConference2021. In
thatwork,weproposeaHuman-AIDesignModelthatdesignsaCRLmodelfromthreedifferentperspectives: Human,
AIagent,andDesignpattern,whichisastraightforwardandeffectivemethod(SeeFigure1). Insubsequentresearch,
wefoundthatinordertobuildaCRLsystemfrommacrotomicro,welackedtheconsiderationsofcollaborativelevels,
partiesandcapabilities,whicharecrucialfordesigningthefunctionsanddetailsofeachfunctionalparty. Therefore,in
thiswork,weaddanewSection6. Inaddition,basedontheoriginalstudy,Weimprovedindepthandtopicscovered.
wecreateamoreinnovativecomprehensiveframeworkandamorecompletetaxonomy,coveringfromdesignpatterns
toalgorithms,aswellasincreasingthereviewwith55publications,accountingfor41percentnewliterature. Asa
result,theprimarycontributionsofthissurveyareasfollows:
1. First, we summarise the most significant Human-AI Collaborative Design Patterns, which might help
academicsandpractitionersintheHCIfield.
2. Second,wepresenttheCollaborativeReinforcementLearning(CRL)DesignTrajectoryMap,anovelCRL
ClassificationandTaxonomyasasystematicmodellingtool,toassistresearchersinselectingandimproving
newCRLdesigns.
3. Third, we take stock and summarise the most recent Collaborative Reinforcement Learning algorithms,
analysingthestate-of-the-artatthestartofthisnewdecade.
4. Fourth,asaroadmaptogoodHuman-AICollaboration,weidentifyseveralgeneralCRLproblemsforfuture
studyinthisfield.
2RunningTitleforHeader
Algorithmic Interactive
Models Methods
DesignPatterns
Figure1: Human-AIcollaborationDesignModel: Fromahumanperspective,wefocusonhowhumansinteractwith
AIagents;fromanAIagent’sperspective,wefocusonhowAIagentsaccepthumaninstructionsorsuggestionsin
algorithmimplementation;andfromacollaborationpatternperspective,wefocusonwhatkindofwaythathumansand
AIcollaborate.
2 Background
ReinforcementLearningisderivedfromtheoriesofanimallearningandparameterdisturbanceadaptivecontrol[17].
The intuition is: if an agent’s actions result in positive rewards (reinforcing signals), this type of behavior will be
reinforced,increasingtheagent’sinclinationtorepeatthebehaviorinfutureacts. ThegoalofReinforcementLearning
istotraintheagenttofindtheoptimalstrategyforeachdiscretestatewhilemaximizingtheexpecteddiscountedrewards
[6]. Mathematically,areinforcementlearningprocesscanbedescribedasaMarkovDecisionProcess(MDP),defined
bythetupleM=⟨S,A,T,R,γ⟩,whichisacyclicalprocess,whereanagenttakesactionAtochangeitsstateS to
obtainarewardRfromtheprocessofinteractingwiththeenvironment;γisthediscountfactor;T :S×A(cid:55)→Pr[S]is
thetransitionfunction;theexpectedlong-termrewardfollowspolicyπ,representedastheQ-functionQπ(s,a),which
iscomputedas:
(cid:34) ∞ (cid:35)
(cid:88)
Qπ(s,a)=E γtR |s =s,a =a (1)
π t 0 0
t=0
Q∗(s,a)=max Qπ(s,a)isanoptimalvaluefunction. Anyoptimalpolicyπ∗thatmaximisestheexpectedreward
π
foreachstateisthesolutiontotheMDP.
ReinforcementLearningdiffersfromtheothertwotypesofMachineLearning: SupervisedLearningandUnsupervised
Learning. InSupervisedLearning,amodellearnsthemappingrelationshipbetweeninputX andlabely through
a set of paired and labelled data, to solve Regression and Classification problems. In Unsupervised Learning, a
modellearnsunlabelleddatawithoutanyguidance,tosolveAssociationandClusteringproblems,fordiscovering
underlyingpatternsofthedata. InReinforcementLearning,amodellearnsthemappingrelationshipbetweenstates
andactions(non-predefineddata),tosolveExploitationorExplorationproblems. Themappingdirectsthemodel,or
theagent,tomakeoptimaldecisionsbasedonthesestatestowardsmaximisingcumulativerewards[6]. Thelearning
process emphasises the interactions between the agent and the environment that gives ‘reward signals’ during the
agent’s continual or exhaustive attempts of all the possible strategies to be adopted in a certain ’state’, rather than
directingtheagenthowtocreatethe’correct’action[21]. A’rewardsignal’isusuallyascalarsignalandanassessment
of the quality of the generated action by the agent. This way, the agent learns knowledge from the environment
throughdiscretefeedback ofactions, whichitusestooptimisetheparametersthatmightleadtoanoptimalresult.
Withminimalinformationgivenbytheexternalenvironment,theagentmustlearnbyitsowninteractionswiththe
environment,frequentlyfromthegroundup.Ifthe’rewardsignal’randthe’action’Awereknown,thesecorresponding
representation-labeldatamightbeutilisedtotrainamodelusingsupervisedlearning,however,itisoftenimpracticable
toexhaustallconceivableactionsinanenvironment andcreatethecorresponding’rewardsignals’. Thisiswhere
ReinforcementLearningmayhelp. ReinforcementlearningoftenbeatsSupervisedLearninginscenarioswherethe
discreteactionspaceissmall,suchasthegameofGoorAtari[22].
Bellman proposed the mathematical theory of dynamic programming in 1955 [23]. The Bellman condition was
consideredtobethecrucialtheoreticalfoundationforreinforcementlearning. Then,in1957,BellmanproposedMarkov
DecisionProcesses(MDPs)[24],whicharenowusedinmostreinforcementlearningalgorithms. Afterthe1960s,the
conceptsofreinforcementandreinforcementlearninggraduallyappearedintheliterature. In1963,asystemcalled
3RunningTitleforHeader
STeLLAwasdeveloped,whichallowstrialanderrorlearningthroughinteractionswiththeenvironment[25]. Michie
proposedanearlyreinforcementlearningsystemcalledMENACE.In1975,Hollandproposedanadaptivesystem
basedontheselectionprincipleinhisbook’AdaptationinNaturalandArtificialSystems’[26]. Thisisregardedasone
ofthemostsignificanteventsintheevolutionofreinforcementlearning. Thebookalsoincludedgeneticalgorithms,
whichaidedinthedevelopmentofoptimizationalgorithms.
RummeryandNiranjansuggestedSARSA,orstate-action-reward-state-action,in1994onthebasisofQ-learning. In
termsofdecision-making,SARSAissimilartoQ-learning,howeveritdiffersintermsoftheupdatingmethod. SARSA
employsanon-policymethod,whereasQ-learningemploysanoff-policyone. [27]. Thrunetal. introducedtheMonte
Carlopositioningmethodin1999,whichusesprobabilitytosolvetherobotpositioningproblem[28]. Comparedto
traditionalgridmethods,itismoreefficientandsavesmemory[27].
Withtheadvancementofcomputerpowerandtheadvancementofdeeplearning,numerousapproachescombining
deeplearningwithreinforcementlearninghavelatelybeenpresented. In2013,Mnihetal.,fromtheDeepMindteam,
proposedDeepQ-Learning(DQL)[29]. ThisapproachemploysQ-learningtodiscovertheappropriatecontrolrules
aftertransferringdatafromhigh-dimensionalsensoryinputtoaconvolutionalneuralnetworktoextractfeatures. This
team’sAlphaGodefeatedtheworldGochampionwithascoreof3:0in2017. InDecemberofsameyear,themore
advancedAlphaZeroachievedAlphaGomasterbyself-learningwithouttheassistanceofhumanknowledgeinonly21
days,andexceededallversionsafter40days[30]. Sincethen,ReinforcementLearninghasmadegreatprogress[5].
In2021,Chenetal. proposedamethodthattransformsreinforcementlearningintoasequencemodellingproblem
calledthedecisiontransformermodel[31]. Thishasgraduallybeenappliedinfieldssuchasgames[32],robotics[33],
computervision[34],naturallanguageprocessing(NLP)[35],andrecommendersystems[36]. Forexample,theOpen
AI2teamcreatedaninteractivereinforcementlearningmethodthatusedhumanfeedback,tolearnsummarisation[37].
Anotherrecentprojectproposedbythisteam,GPT-3,hasalsomaderevolutionaryachievementsinthefieldofNLP
[35].
Duetoitsstrongpotentialandfirmtheoreticalfoundation,ReinforcementLearninghasrecentlybeenoneoftheresearch
areasinAItechnologiesthatattractthemostattention[6]. However,itfacesmanychallenges. Currently,ontheone
hand,ReinforcementLearningonlyworkswellwhentheenvironmentisdefinite,i.e.,thestateoftheenvironmentis
fullyobservable. Inparticular,therearedefinedrulesingameslikeGo,andtheactionspaceisdiscreteandconstrained.
Inotherwords,theagentneedsagreatdegreeofpriorknowledge,tounderstanditsstateinacomplexenvironment[38].
Ontheotherhand,eveniftheagenthasbeengivenwell-specifiedfeedback,theinexplicabilityandincomprehensibility
causedbytheagent’sunconsciousisstillinadequatefortheagenttodecideontheprecisenextaction[17]. Furthermore,
mostapplicationsofReinforcementLearningtodatehaveonlybeenforplayinggames,suchaschessandAtari.
3 MethodologyandScope
3.1 LiteratureCollection
ThisreviewfocusesonhithertoundiscoveredareasofresearchoncollaborationsbetweenhumansandAIagents. We
furtherrefineourdatapool,specificallynarrowingtheselectiontothefollowingselectedtargetareas: HCI,Human-AI
Collaboration,ReinforcementLearning,andExplainableAI,publishedintherecentdecade-thetimeperiodbetween
2011and2022fromGoogleScholar. Intotal,oursearchyield237articlesusingkeywordsincludingcollaborative
reinforcementlearning,interactivereinforcementlearning,human-computerinteraction,anddesignpatterns. They
werepublishedinjournalsandconferences,includingtopvenuessuchasTOCHI3,IJHCS4,AAAI5,CHI6,UbiComp7,
UIST8,andIEEE9. Followingamanualreviewofthetitleandabstractofeacharticle,weeliminated103articlesas
irrelevant,leaving134articlesasthesourceofthissurvey.
3.2 Human-AICollaborativeReinforcementLearningClassification
We use an inductive method to organise the literature we collected, and proposed our new classification method,
inspiredbythetraditionalhuman-machineinteractionresearch. PreviousworkbyNajarandAnismainlytargeted
2OpenAIwebsite:www.openai.com
3ThewebsiteofTOCHI:https://dl.acm.org/journal/tochi
4ThewebsiteofIJHCS:http://dblp.uni-trier.de/db/journals/ijmms/
5ThewebsiteofAAAI:www.aaai.org/
6ThewebsiteofCHI:chi2021.acm.org/
7ThewebsiteofUbiComp:www.ubicomp.org/
8ThewebsiteofUIST:uist.acm.org/
9ThewebsiteofIEEE:www.ieee.org/
4RunningTitleforHeader
Table1: CRLClassificationappliedtothePoolofPapersaboutCollaborativeRL,publishedbetween2011-2022
CollaborativeRL References
CognitiveSystemsEngineeringPatterns [7]
Design BoschandBronkhorst’sPatterns [40]
Patterns CoactiveDesignPatterns [41]
Schmidt’sPatterns [42]
Collaborative AugmentativeLevelcollaboration [43],[22],[44],[45],[46],[47]
Levels IntegrativeLevelcollaboration [41],[42],[48],[49],[50],[51],[8]
and DebativeLevelcollaboration [8],[52],[53],[11],[54],[17],[55]
Parties Collaborativetypes [56],[57]
Understanding [58],[59],[60],[61],[62],[63],[64],[65]
Collaborative Communication [57],[66],[67],[68],[69],[38]
Capabilities Commitments [70],[71][72],[70],[73],[74],[75],[76],[77]
Institutions [78],[79],[80],[81],[82],[83],[84],[85],[86]
Interactive ExplicitMethods [87],[88],[89],[90],[91],[87],[92],[39]
Methods ImplicitMethods [93],[43],[94],[95],[96],[97],[98],[99],[100],[101]
Multi-modelMethods [102],[103],[104],[105],[106]
Reward-basedmethods [107],[87],[108],[96],[97],[98],[99]
Algorithmic Policy-basedMethods [109],[89],[55],[97],[98]
Models ValueFunctionbasedmethods [110],[111],[29],[30],[43]
Exploration-processmethods [91],[112],[99],[100],[101],[41],[8]
physicalinteractionbetweenhumansandmachinesfromahumanperspective[15]. Intheearlystageofcomputerand
engineering,therewasnoinvolvedconceptofAI.So,thereisonlyresearchonhuman-machineinteractions. Cruzetal.
proposedthatthehuman-AIinteractionisakindofhuman-machineinteractioningeneral[39]. Inthispaper,wehave
collectedparadigmsofhuman-machineinteractionintheearlystage,whichalsocouldapplytohuman-AIinteraction.
Therefore,inthefollowingsection,wewillusetheconceptofhuman-AIinteractioninaunifiedmanner. Inthework
ofArzateandChirstian,moreattentionwaspaidtothealgorithmicmodeloftheAIagent[17]. Afterreviewingthe
literatureofhuman-machineinteractionintraditionalengineering,wefoundthattheinteractionbetweenhumansand
AIcorrespondstothesedesignpatterns. Inparticular,Schmidt’smodel[8]notonlycombinestheinteractivemethods
andalgorithmicmodels,butalsoprovidesdifferentdesignideas,accordingtodifferenthuman-AIcollaborativelevels.
Basedonthecommoncharacteristicsoftheclassificationoftheseliterature,wederivethenewHuman-AICollaborative
ReinforcementLearning(CRL)Classification(seeTable1).
3.3 Human-AICollaborativeReinforcementLearningTaxonomy
We incorporate past work in a novel way to create a new taxonomy. We draw on Schmidt’s Machine Interaction
Pattern[8],Dafoe’sCollaborationPartiesClassificationModel[57],andArzate’sAlgorithmicClassificationModel
Collaboration Method [17], to generate a novel taxonomy method from coarse to fine granularity. Based on this
approachandpopulatethemwithrepresentativeworksfromtheliterature,forastructuredapproach(seeTable1),we
definefiveaxes: includeDesignPatterns,CollaborativeLevelsandParties,CollaborativeCapabilities,Interactive
Methods,andAlgorithmicModels. Thesefiveaxesarethenusedtocreateataxonomy,asshowninFigure2,which
mightbeusedasasystematicmodellingtoolforHCIresearchersandpractitionerstoselectandimprovetheirnewCRL
designs.
4 Human-AICollaborativeDesignPatterns
Human-AI collaborative design patterns may be used to provide an efficient and repeatable approach for building
human-AIcollaborativesystems[113]. Reliabledesignpatternsmightincreasethesesystems’quality, reusability,
andmaintainability. Inthisarticle,wecollectthemostrecogniseddesignpatternsintheliteratureofhuman-machine
collaborationforacademicsandpractitionerstopopulatetheCRLTaxonomyasshowninFigure2.
In comparison to human-AI collaboration, human-machine collaboration has long been a source of concern for
researchers. In the early stages of the development of human-machine interactions, domain experts believed that
thecollaborationbetweenhumansandmachineswasaphysical,lower-leveltypeofcollaboration[8]. Specifically,
5RunningTitleforHeader
Cognitive Systems Engineering (CSE)
Bosch‘s Framework
Design Patterns
Coactive Design Paradigm
Schmidt's framework
Augmentative Level
Integrative Level
Collaborative Levels and Parties
Debative Level
Collaborative Parties
Understanding
CRL System Communication
Collaborative Capabilities
Design Map
Commitments
Institutions
Explicit Interaction
Interactive Methods
Implicit Interaction
Multi-module Interaction
Rewards Based Methods
Value Based Methods
Algorithmic Models
Policy Based Methods
Exploration-process Based Methods
Figure2: AnewCRLtaxonomyforinteractivemethodsanddesignpatterns
machines were used by humans exclusively through physical contact, in the lack of feedback between machines
andhumans,whichmightbeviewedasakindofunidirectionalinteraction. Afterdecadesofdevelopment,several
recognisedhuman-machinecollaborationdesignpatternshavebeencreated,whichwesummarisebelow.
4.1 CSEPattern
CognitiveSystemsEngineering(CSE),coinedbyHollnagelandWoods,actsatthelevelofcognitivefunctions[7].
CSE is the first framework proposed to analyse the human-machine information exchange interaction. CSE is a
frameworkforhuman-machinecollaboration,wheremachines’planandexplore’usingtheknowledgeorinformation
providedbyhumans. Thisengineeringmethodsuggeststhathuman-machinecollaborationoccursataconsciouslevel
6RunningTitleforHeader
ofcommunication. Itisaperceptualmodeinwhichthemachineisemployedasasensoryextensiontoassistwith
humanactivities.
Atthislevel,themajorchallengeistoidentifyappropriateinteractivemethodstooptimisehumaninformationprocessing.
However,CSEhasbeenconstrainedinthatithasonlyexploredbasicandlow-levelcommunications,leavingoutmore
complicatedproblemsandenvironments.
4.2 BoschandBronkhorst’sPattern
BoschandBronkhorstdefinedthreelevelsofHuman-AIcollaboration: 1)unidirectionalinteraction,inwhichhumans
assistmachinesormachinesexplainthemselvestohumans;2)bi-directionalinteraction,and3)collaborationbetween
humansandmachines[40]. Thevastmajorityofthecurrentlyexistingmethodshaveonlyaddressedthefirstlevel.
Thisframework’scontributionistoprovideaviewpointonthedirectionsofcollaborationbetweenhumansandmachines.
Furthermore,itconstructsthedirectionbasedontherolesthathumansandmachinesplayinatask,withonebeing
thesubjectofataskandtheotheraidingtheopposingside. Itisbelievedtohelpinthedevelopmentofmoreefficient
communicationmethods. Forexample,ifitisahuman-centredframework,moreconsiderationsshouldbegivento
howtotransforma’machinelanguage’intointerpretableinformationsuchthathumanscanbetterunderstand;whereas
ifitisamachine-centredframework,moreconsiderationsshouldbegiventowhathumanknowledgecouldimprove
machineefficiency.
4.3 CoactiveDesignPattern
Johnsonetal. proposedaCoactivedesignpatterninhuman-AIjointactivities. Theyexperimentedwithacollaboration
systemfromtheperspectiveofobservability,predictability,anddirectability[41]. Observabilityconcernstheabilityof
bothrobots(orAIagents)andhumanstoobserveeachother’spertinentaspectsofstatus,aswellastheknowledgeof
theteam,tasks,andtheenvironment. Predictabilityreferstothestatethattheactionsofbothrobots(orAIagents)and
humanscanbepredictedsuchthattheymayrelyoneachother’sactionstoperformtheirownactions. Directability
refers tothe abilityof bothrobots (orAI agents)and humans todirect eachother’s behaviour ina complimentary
manner.
ThisframeworkissimilartotheBoschandBronkhorst’sframeworkinthatitconsidersthedirectionofinteractionsand
dividesitintodifferentlevels. However,itislimitedduetoalackofrobustnessandsecurityconsiderations.
4.4 Schmidt’sPattern
Schmidtbelievesthatcollaborationshouldbetailoredtodiverseneeds,fulfildifferentfunctions,andbecarriedoutina
varietyofwaysdependingonthecircumstances. collaborationmaybesummarisedasfollows: 1)theaugmentative
level,inwhichoneroleinthepartnership(humanorAIagent)assiststheotherinperformingtasks;2)theintegrative
level,inwhichbothsidesoftheteamshareinformationandassisteachotherincompletingtaskstogether;and3)the
debativelevel,inwhichtasksarecompletedthroughdebateandnegotiationbetweenhumansandAIagents,especially
whendealingwithcomplexissues[42].
Thisframeworkconsidersnotonlytheinformationexchangedirectioninthecollaboration,butalsodifferentlevelsof
collaboration,aswellastherobustness,security,andpotentialethicalconsiderations.
5 CollaborativeLevelsandParties
Thepatternsdescribedaboveframethemodesofhuman-AIcollaborationfromdifferentperspectives. Theyarealso
verycomparableintermsofcompartmentalisingcollaborationmodesormethods,i.e.,intosingle-directionassistance,
bi-directionalcollaboration,andhigher-stagefusedcollaboration. Inthissurvey,weutiliseafusionviewpointthat
combines interactive methods and design patterns based on Schmidt’s collaboration pattern to classify the current
collaborativereinforcementlearningtechniques. Schmidt’smodelisdividedintothreelevels: augmentative,integrative,
anddebative. WedrewapyramidmodelbasedonSchmidt’smodel(seeFigure3). Wehighlightsignificantresearch
thathaveemergedateachlevelandtheissuesthatshouldbeexaminedinthefirstthreesub-sectionsthatfollow. We
alsodiscussthecharacteristics,advantages,anddisadvantagesofthesedifferentmethods,aswellashowtodevelop
newmethodsinthefuture.
Apart from the classification of collaborative levels where humans and AI agents are both viewed as a whole, we
alsodiscusscollaborationfromamicroperspectivebasedontheframeworkproposedbyDafoeetal.,wherediverse
constellationsofhumansandAIagentsarediscussed,whichwerefertoas“collaborativeparties”inthefinalsub-section.
7RunningTitleforHeader
Debative
Level
Integrative Level
Augmentative Level
Figure3: Triangleofdifferentcollaborativelevels: thefirstlevelisAugmentativeLevelcollaboration;thesecondlevel
isIntegrativeLevelcollaboration;andthethirdlevelisDebativeLevelcollaboration.
5.1 AugmentativeLevelcollaboration
collaborationattheAugmentativeLevelentailsonepartnercompensatingfortheshortcomingsoftheother[8]. AIhas
showedconsiderablepromiseinlarge-scaledataprocessingwithwell-definedrules,aswellasinnatural-perception
fields such as digital image recognition [34], natural language processing [114], among others. Nevertheless, in a
complexambiguousenvironment,AIperformancelagsconsiderablybelowthatofhumans. TheAugmentativeLevel
approachesofferedbythecommunityaremostlydividedintotwotypes. First,AItakestheleadindecision-making,
whilehumansassistAIinenhancingprocessingefficiency. Inthiscase,humansusepriorknowledgetohelptheagents
specifythestatespaceandefficientlyobtainrewardsfromthecomplexenvironment. Second,humansplaytheprimary
roleindecision-making,withAIassistingintheprocess. Inthiscase,theAIagentsexplainthetacticsusedtohelp
humansmakefasterdecisionsinasimpleenvironment. Atthesub-levelofhumanshelpingAIagentsimproveefficiency,
wewillcategorisehowtheycommunicatebasedonwhichpartsofthealgorithmhumans’helpcanbeinjected. Atthe
sub-levelofAIagentshelpinghumans,wemainlyfocusonhowAIagentsmayinformhumansaboutwhytheymake
particulardecisions.
5.1.1 Human->AI
ThemostessentialaspectintheroleofhumanssupportingAIagentsindecision-makinghasbeenhowtoefficiently
deliverinformationtoAIagentswhilereducinghumanwearinesstoaminimal. Uptothispoint,manyhuman-AI
collaborativereinforcementlearningalgorithmshavebeenproposed,whichmaybecategorisedintoexplicitinteraction
modal,implicitinteractionmodal,andmulti-modalmethodsbasedondifferentformsofinteractions(detailedinSection
4). FindingabetterwayforhumanstodirectlyinteractwithAIagentsisstillanessentialresearchpriority.
5.1.2 AI->Human
InthetaskofAIagentsassistinghumansindecision-making,themostchallengingproblemliesininterpretability.
Interpretability(orexplainability)referstothedegreetowhichhumanscanunderstandtherationaleunderpinning
machines’ decision-making [115, 20]. The interpretability of AI models refers to the clarification of the internal
mechanismandtheunderstandingoftheresults. Themoreinterpretablethemodel,theeasieritisforpeopletotrustit
[13,116]. Itssignificancemaybeseeninthefollowingaspects: inthemodellingphase,interpretabilitymayassist
developersinunderstandingthelearningprocess,comparingalternativealgorithms,optimisingtheprocedure,and
8RunningTitleforHeader
fine-tuningthemodels;intheoperationphase,AIagentscanexplaintheinternalmechanismandinterpretthemodel
outcomestothedecision-maker(i.e.,humans). Consideradecision-makingrecommendationmodel,beforethemodel
runs,multipleinterpretablealgorithmswiththeirrespectiveadvantagescanbeprovidedtohumanstochoosefrom;and
afterthemodelistrained,themodelmustexplaintohumanswhyitrecommendedaspecificsolutiongivenaspecific
context.
PatternsunderlyingtheaboveproblemslieundertheumbrellaofeXplainableAI(XAI),whichiscommonlyregarded
ascriticalforthepracticaldeploymentofAImodels. DARPAlaunchedtheXAIin2016[22,117,118]. Thebasic
objectiveofXAIistocreatemachinelearningmodelsthat,whencombinedwithproperexplanationtechniques,will
allowhumanstobettercomprehendandeventuallyacceptandtrustthemodel’spredictions. Theliteraturegenerally
proposes two types of explainability: 1) transparent models, which are embedded inside the operation of the AI
algorithms, leadingtoexplainabilitybydesign, appliedtosimplerAIalgorithmswithlessaccurateresults; and2)
post-hocmodels,whichareperformedafterinitialmodelshavebeentrained. Thistypeofmethodsisusuallymore
efficient,butitislessreliablethantransparentmodels[44,119,120].
At present, there are a few intrinsic interpretability Reinforcement Learning methods. Verma et al. introduced a
Programmatically Interpretable Reinforcement Learning method (PIRL) [45, 121]. This method is an upgrade of
traditional Deep Reinforcement Learning (DRL). In DRL, due to the ’black-box’ nature of neural networks, it is
difficult to represent policies. To tackle the ’black-box’ challenge, PIRL introduces an advanced human-readable
programminglanguagetodefineneuralnetworkpolicies. Shuetal. introducedahierarchicalandinterpretablemulti-
taskreinforcementlearningframework,whereacomplextaskisbrokenintoseveralsub-tasksandthenahierarchical
strategyisusedtocompletethelearningwith’weaksupervision’fromhumans. Bybreakingataskintosub-tasksand
thusmakingalearnedstrategytraceabletothem,andbyexplainingtherelationshipbetweendifferenthierarchiesofthe
sub-tasks,thismethodbuildsintrinsicinterpretability.
ComparedwithintrinsicallyintepretableReinforcementLearningmethods,post-hocmethodsaresimplerinalgorithm
structureandmoreefficientinthecomputingprocess. Atpresent,manypost-hocmethodshavebeenproposed. For
example,Liuetal. proposedanexplainableDRLmethodbasedonlinearmodelU-trees[46]. Thisisastochastic
gradientdescentframeworkforexplainingcomplexmodelsbyusinglinearmodelU-treestofitQ-functions. Thereis
alsoaSoftDecisionTree(SDT)method,whichprovidespost-hocexplanationsbyextractingpolicies. Madumaletal.
introducedanexplainablemethodthroughacausallens. Inthisframework,anAIagentlearnstoplayStarCraftII,a
largedynamicspacestrategygame[47,122]. Togenerateanexplanation,theysimplifytheentiregamestatestofour
basicactionsandninebasicstates,andthenusethesebasiccausalfactorstoconstructanexplanationforwhytheAI
agentchoosesactionAoveractionB.
5.2 IntegrativeLevelcollaboration
Integrativecollaborationentailsusingthevariousadvantagesofbothpartiestocompleteatask. Atthislevel,humans
andAIagentsareregardedasbeinginterdependent. Themaintaskisbrokenintoseveralsub-tasks,andhumansandAI
agentscanperformjustthosethattheyareskilledat[8]. Attheintegrativelevel,humansandAIagentsplayequalroles
inthesystem. Informationexchangeatthislevelisgenerallyreferredtoas’communication’intheliterature[8,123].
Inthefollowingsub-sections,first,wesummarisethecommunicationmethodsinthiscooperativepattern. Then,we
discusshowtomakethecommunicatingpartiestrusteachother. Onthisbasis,thesystemneedsresiliencetoenhance
itsrobustnessinordertobetterdealwiththecomplexconditionsintherealworld.
5.2.1 Communication
AgrandchallengeofcollaborativereinforcementlearningishowhumansandAIagentscommunicatewitheachother.
Onlywhencommunicationisseamlesscantheymakedecisionsonthenextactionsfollowingeachother’sfeedback.
Liangetal. proposedanimplicithuman-AIcollaborationframeworkbasedonGriceanconversationaltheory[124]to
playthegameHanabi. TheAIagentmustcooperatewiththehumantowinthegame. Inthisframework,theAIagent
triestounderstandtheimpliedmeaningofhuman’snaturallanguagesuggestionsinadialoguebox[51].
Cordona-RiveraandYoungproposedanAIPlanning-basedGameplayDiscourseGenerationframeworktoachieve
communicationbetweenhumanplayersandthegame[50]. PabloandMarkusproposedanapproachofHuman-AI
collaborationbyplanningandrecognitionoftheplan[49]. Johnsonetal. proposedatestbedforjointactivities. The
uniquefeatureofthistestbedisthatitcanbeappliednotonlyininteractiveexperimentsformultipleagentsbutalsoin
interactiveexperimentsbetweenhumansandagents[9]. Aseriesofworkswerecarriedoutonthistestbedtostudythe
collaborationofhumansandagentsinateam. Forexample,Matthewetal. introducedtherelationshipbetweenthe
interdependenceandautonomyinahuman-AIcollaborationsystem[48].
9RunningTitleforHeader
5.2.2 Trust
Basedontheestablishedcommunications,howtomakethepartnerstrusteachothertocompletethetaskisalsocrucial.
AlthoughthecommunityhasnotyetproposedacleardefinitionoftrustbetweenhumansandAIagents,itisgenerally
regarded as a psychological state [8]. Johnson et al. proposed a Coactive Design framework for human-AI joint
activities. Intheirframework,theauthorsproposedacollaborationsystemfollowingtheperspectiveofobservability,
predictability, and directability [41]. These components are critical for humans and AI agents to collaborate in a
trustworthymanner.
5.2.3 Resilience
Resilienceisanotheressentialfeatureinhuman-AIcollaboration. Onthepremiseofcommunicationandmutualtrust,
incomplexproblems,withpossibledelaysandinformationnoise,howtoestablisharesilientmechanismtomakethe
systemmorerobustiscrucial. Aneffectivehuman-machinecollaborationmechanismshouldbeabletodiagnosea
problemquicklyandprovideremedialexplanationsaftertheproblemoccurssothatthesystemcangetbackoncourse
[41]. Ziebaetal. proposedamechanismtomeasuretheresilienceofhuman-machinesystems,thatis,theabilityto
anticipate,avoid,andrecoverfromaccidentstoanormalstate[42]. Thisisinstructivetodesignacooperativesystem,
asitisnecessarytoconsiderhowthesystemrespondstoemergenciesandthusrecoversquickly.
5.3 DebativeLevelcollaboration
DebativemodelscomeintoplaywhenhumansandAIagentsholddifferentopinionsondecision-makinginatask,and
theydebatetofindtheoptimumsolutionbasedontheirdifferingknowledgeandunderstandings. Modelsareoften
requiredtomeetthefollowingrequirements. First,humansandAIagentsshareaunifiedgoal,andachievingthatgoal
istheprimarytask. Forbothparties,adebatewithoutaunifiedgoalismeaningless. Second,bothpartieshavestrong
justificationsfortheirdecisionsandhaveinsightsintoaproblembasedontheirrespectivecognitivemodels. Third,both
partiescaneffectivelycommunicateandexplaintheirdecisionstoeachother. Communicationandinterpretabilityare
thepremisesofthedebate. Fourth,thereareclearevaluationcriteriatomeasuretheoutcomefromadebatetoensurean
optimalresult. Fifth,bothpartiescanlearnandadjusttheirownknowledgeafteradebatetoachievebetterresultsinthe
future[8,125].
Asknowledge-baseddecisionsarefragileandcontroversial,itisnecessarytodebatetheresults[52]. Inacomplexand
uncertainenvironment,afulldebatewillbetterdemonstratetheadvantagesanddisadvantagesofdifferentdecisions.
collaborationatthelevelofdebaterequiresthatbothhumansandAIagentshavesufficientlyhighprofessionalismina
specificcomplexdomain. Reinforcementlearningalgorithmsbasedonthislevelarescarcelystudiedintheliterature,
butweexpectthatasthefieldprogresses,thisformofcollaborationwillattractmoreattention.
Geoffreyetal. introducedaframeworkthatenablestwoagentstodebatewitheachother,withahumanjudgedeciding
who to trust in the end [53]. Although it has not yet been applied to the debate between humans and agents, this
frameworkmeetstherequirementsoutlinedabove. Intheirexperiment,thetwoagentsattemptedtopersuadehuman
judgestobelievetheirjudgmentsontheMNISTdata[126]. First,thegoalofthetwoagentswereunified. Second,
thetwoagentshavedifferentjudgmentsbasedontheirownalgorithmicperceptions. Third,bothagentsareableto
generate simple explanations to persuade human judges. Fourth, human judges have intuitive knowledge to make
accuratejudgments. Thisexperimentisenlighteningforfutureresearch,especiallyinhuman-agentandmulti-agent
debatecollaboration.
5.4 CollaborativeParties
Thedifferentlevelsofhuman-AIcollaborationtakeamacroviewofhumanandAI,lookingatthembothasawhole.
ThecollaborationofhumanswithAI,ontheotherhand,canbesplitdownintodifferentcombinationsofpartiesfroma
microperspectiveorinconsiderationsofpracticalscenarios. Forexample,futurescenarioscouldincludeinteractions
betweenahumanandseveralAI-agents,interactionsbetweenhumangroupsandAI-agentgroups,ormorediversified
fusionofthetwo. Therefore,inthissection,wewilldiscussthetypesofinteractionsbetweenhumansandAIagents
fromthemicroperspective(AIagents-agents,human-AIagents,human-human,andmorecomplexconstellations).
Dafoeetal. categorisescooperativerolesintothreecategories: AI-agent,HumanandOrganisations. Collaborative
typesbasedonthenumberofrolesinvolvedintothecollaborationintosixtypes[57](seeFigure4):
1. Human-Humancollaboration: theclassichuman-to-humancollaborativemodel;
2. CooperativeTools: theAIagentisusedtoenhancecollaboration,suchaslanguagetranslation.
10RunningTitleforHeader
3. Alignment and Safety: the AI agent acts like an assistant to help humans solve problems, such as the
relationshipbetweenvehiclesandhumansinautonomousdriving.
4. Human-AI-Human-AIcollaboration: Withthedevelopmentof5GtechnologyandAItechnology,large-scale
humangroupsandAIgroupsmaycooperateintheforeseeablefuture,suchasthelevel5autonomousdriving
[56].
5. The Planner Perspective: This approach is to strengthen the collaboration and infrastructure of the entire
societyfromtheplannerperspectiveofsocialconstruction,ratherthanthecollaborationbetweenanindividual
andasingleAI,e.g.,socialmediaandnetworkcommunications.
6. OrganisationsandSociety: Collaborationcouldhaveamorecomplicatedstructure,withmultipletypesof
hierarchicalcollaborationoracomplexinternalstructure.
(1). Human-Human
(2). Cooperative Tools
(3). Alignment and Safety (4). {Human-AI}-{Human-AI}
(5). The Planner Perspective (6). Organizations and Society
Figure4: Differentpartiesintheprocessofcollaboration.
6 CollaborativeCapabilities
Intheprevioussection,wediscussedthedifferentlevelsofhuman-AIcollaborationfromamacroperspective,where
bothhumanandAIagentareviewedasintegralparts. However,intherealscenariosorfromanmicroperspective,
theremaybeinteractionsbetweenahumanandseveralAI-agents,orinteractionsbetweenhumangroupsandAIgroups.
Therefore, in this section, we will discuss the types of human-AI interactions from the micro perspective (i.e., AI
agents-AIagents,human-AIagents,human-human,andmorecomplexconstellations),aswellasdiscusswhatkindsof
collaborationcapabilitiestheagentrequireforgroupsinteractions.
Dafoeetal. dividescollaborationcapabilitiesofagentsinto4types: Understanding,Communication,Commitments,
andInstitutions[57].
6.1 Understanding
Inhuman-AICollaboration,anAIagent’sabilitytounderstandtheenvironmentandpredicttheconsequencesofits
actions is crucial for reaching mutually beneficial results. In game theory, there are many discussions about how
importantitistounderstandmulti-rolecollaboration. Forexample,inNashequilibrium,eachstrategyisrequiredtobe
thebestresponseafterfullyunderstandingother’sstrategies[58]. Moreover,undertheconstraintofpartialinformation,
Bayesian Nash Equilibrium and Perfect Bayesian Equilibrium provide a solution for how multi-role collaboration
shouldenableotherparticipantstobetterunderstandeachother’sstrategies[127].
11RunningTitleforHeader
Incollaborativereinforcementlearning,themostimportanttypeofunderstandingislearningthepreferenceofthe
otherAIagents,namelythevalues,goals,rewardfunctionsoftheotherparties. Humansunderstandhowtobetter
providefeedbackorrewardstotheAIagentinordertohelpitconvergefasterandfunctionmoreefficiency. SomeAI
researchersattemptedtodirectlylearntheAIagent’sbehaviour. Forexample,Albrecht’sstudy[59]summarisedhow
humansobservetheAIagent’sbehaviourinordertounderstandtheagent. InverseReinforcementLearning. orIRL,is
atypeofresearchinwhichAIagentsareobliviousorindifferenttohumans[60]. Thistypeofmethodrequireshumans
toinjecttheirpriorknowledge[61]orcontroltheAIagenttomakeafewfirststeps[62,63].
Besides, in a more complex decision-making environment, humans may consciously or unconsciously hide their
opinionsorideascausingsignificantchallengesforAIagents. Therehavebeensomestudiesontheapplicationof
humanrecursivemind-readingmethodsinnegotiationstoovercomethischallenge[64],andsomestudieshaveapplied
thismethodtothegameHanabitoimprovethecollaborationbetweenhumansandAIagents[65].
6.2 Communication
Understandingandcollaborationcouldbedifficulttoachievewithouteffectivecommunication. AIagentsmayoften
getabetterunderstandingofothers’behaviour,intentions,andpreferencesbycommunicatingdirectlywiththemrather
thanjustobservingandinteractingwiththemonaregularbasis. ThefindingofPareto-optimalequilibriummaybe
madeeasierasaresultofinformationexchange[57].
CommonGround Acommongroundisnecessaryforcollaboration. Themessagesenderandreceivershouldusethe
samecommunicationprotocolsothateachmayunderstandthemeaningoftheother’smessage.
Manystudieshavebeenconductedonmachine-to-machinecommunicationproblems,whichareusuallyreferredtoas
emergencycommunication[66,67,68,69,128]. However,therearefewstudiesonhowtoestablishcommonground
andeffectivecommunicationbetweenhumansandAIagents. Theestablishmentofacommongroundisarguablythe
mostdifficultchallenge[57].
BandwidthandLatency Thebandwidthofcommunicationreferstothevolumeofdatathatmaybetransferredina
givendurationoftime[57]. Latencyreferstothetimeittakesforamessagetobetransmittedandreceived[57]. How
toenhancebandwidthandminimiselatencyinhuman-AIcollaborationhaslongbeenstudied,andsomepromising
techniqueshavebeenproposed,includingbrain-computerinterfacetechnologies,whicharedesignedtoconnecthuman
braindirectlyviahardwaretomaximumbandwidthwiththeshortestpossiblelatency[38].
6.3 Commitments
TheaforementionedcapabilitiesofUnderstandingandCommunicationstrivetoovercomethedifficultiesincollaboration
causedbyinaccurateorinadequateinformation. Collaboration,evenwithabundantinformation,maystillfail. Social
scientists have identified "commitment issues", or the inability to make credible threats or promises, as a primary
causeofcollaborationfailure. Prominentresearchevenclaimsthattheproblemofcommitmentisthemostsignificant
impedimenttorationalAIagentcollaboration[70]. Asubstantialvolumeofresearchhasexploredthecommitment
issuesthataffectcollaboration[71,72,70,73,74].
ManydifferentstudiestriedtobuildcommitmentsbetweenHumanandAI.Somestudieshaveattemptedtodevelopa
commitmentcontractmethodbetweenhumansandAIagentsbasedonsemantics[75,76,77,128].
6.4 Institutions
Obtainingtherequisiteunderstanding,communication,andcommitmentforcollaborationoftennecessitatestheuseofa
socialframework. Ineconomicsandpolitics,thissocialframeworkistypicallyreferredtoabstractlyasinstitutions[57].
DecentralisedInstitutions Indecentralisedinstitutions,thereisnosingleinstitutionscentre,andeachindividualis
connectedtoeachother,andcontinuouslyencouragestheconstructionofthestructurethroughtheinteractionwitheach
other. Additionally,manymulti-agentsystemconstructionmethodshavebeenproposedtoaidinmulti-agentsystems
communication,planning,anddecision-making[78,79,80,81,82,129].
CentralisedInstitutions Centralisedinstitutionsinvolveacentralisedauthoritythatcandefinetherulesandlimit
otherparticipants[57]. Themulti-agentsystemsresearchcommunityattemptstobuildacollaborativemechanism
amongst agents using approaches based on centralised institutions [130]. Several studies investigated the use of
12RunningTitleforHeader
centralised multi-agent systems in automatic auction systems [83, 84, 85]. The method of centralised multi-agent
path-findingtechniquecouldbeutilisedinautonomousvehicleobstacledetectioninthefuture[86].
7 InteractiveMethods
Traditionalreinforcementlearningmethodsrequireexcessivetraining-timeincomplexenvironments,andtheirapplica-
tionsareoftenconfinedtoscenarioswithclearrules. Aneffectivewaytomitigatetheselimitsistheuseofdifferent
strengthsofhumanandAIandcomplementingoneother’sinadequacies. ThisapproachisknownasCollaborative
ReinforcementLearning(CRL).CRLemployshuman-in-the-looptrainingtoimprovetheperformanceofalgorithms
ortohelphumansimprovedecision-makingefficiency[17]. RecentCRLresearchhasfocusedondevelopingAIthat
cancommunicatewithhumansinamorenaturalway[17]. Therearetwotypesofinteractivemethods: explicitand
implicit. Inanexplicitmethod,humansprovidetheAIagentswithclearnumericalfeedback,explicitly. Thismethod
ispreferableforAIagentssinceitallowthemtoprocessthefeedbackmoreeasily, butitislikelytocausehuman
fatigueduetotheambiguityofnumericalrepresentations,resultingininefficiencyinalong-termtrainingprocess. Inan
implicitmethod,humansgivefeedbacktoAIagentsthroughnaturalinteractionssuchaspostureandgaze,asopposed
toexplicitmethods,whichprovideclearnumericalfeedback. ThismethodplacesmoredemandsontheAIagent,butit
mayimprovethefatigueresistanceofhumantrainers,allowingforlong-termandstablecollaboration[104]. Basedon
theseunsolvedproblems,inthissection,wepresenthuman-AIinteractionsfromtheperspectiveofinteractivemethods.
7.1 ExplicitInteractiveMethods
Currently, mostAIagentslearnfromhumanfeedbackviaexplicitinteractivemethods. Humansprovidefeedback
directlytotheAIagentviakeyboard,sliderbar,ormousetoprovideclearalpha-numericalfeedback[87,88,89,90].
Forexample,ThomazandBreazealproposedamethodofsendingfeedbacktotheAIagentbyusingthemousetoclick
ontheslidingbar[91]. KnoxandStoneproposedtheTAMERframework,whichallowsanAIagenttolearnfrom
MDPandhumanadvicebyhavingahumantrainerclickthemousetoindicatethedesiredactions[87]. Thesemethods
aremoreefficientthantraditionalreinforcementlearning,andcanachievespecificgoalsincomplexenvironmentswith
theassistanceofhumans.
However,thereactiontimeofhumantrainersmaycausedelayedfeedback,leavingtheAIagentunsureofwhichactions
thehumanfeedbackwasaimedat,especiallyforAIagentswithfrequentactions. Acommonsolutionistosetadelay
parametertoexpressthepasttime-steps. Forexample,Warnelletal. proposedamethodtoobtainthedelaydistributions
ofthehumantrainerstoimprovealgorithmefficiency[92]. KnoxandStoneprovidedanotherwayforestimatingthe
delay: usingaprobabilitydensityfunction[131]. Moreover,thesemethodsmaybeunfavourabletonon-professional
humantrainers,whoneedtospendasignificantamountoftimelearningtheuserinterfaceandthemeaningoffeedback
representedbyeachoperation. Simultaneously,thiskindofinteractionscaneasilymakehumantrainersimpatient.
HumantrainerscanalsoprovideexplicitfeedbacktoAIagentsusinghardwaredeliverymethods[39],wherefeedback
isbegenerallyconvertedintoanumericvaluedirectlyviathehardwaredevices,suchaskeyboards. However,amore
user-friendliermethodisfortheAIagenttolearnimplicitfeedbackfromnaturalinteractionswithtrainers.
7.2 ImplicitInteractiveMethods
Asidefromreceivingfeedbackdirectlyfromhumantrainersviaexplicitinteractivemethods,theAIagentscanalso
learnviaimplicitinteractivemethods.
Implicitinteractionmethodsreducethelearningcostofhumantrainers,astheycandirectlyparticipateintrainingthe
AIagentswithoutspecificlearning. Atthesametime,amorenaturalwayofinteractionmayreducethefatigueof
humantrainers. Manyimplicitinteractivemethodshavelatelybeenproposed. Forexample,feedbackcanbebasedon
naturallanguage,facialexpressions,emotions,gestures,andactions,aswellastheincorporationofmultiplenatural
interactivemethods. Inanidealscenario, humanscouldtraintheAIagentinthesamewaythattheyinteractwith
humansintherealworld. Below,wesummarisesomeofthemostprominentimplicitinteractivemethods.
GesturalFeedback. Gesturesaresometimesconsideredtobeaformofunconscioushumancommunication. Itis
alsoconsideredtobeaneffectivewaytocomplementothercommunicationforms,anditisevenmoreusefulthanother
communicationmethodsforuserswhoarespeech-orhearing-impaired. Forexample,VoylesandKhoslaproposeda
frameworkwhichcantrainrobotsbyimitatinghumangesture[93]. Moonetal. introducedamethodofusinggestures
tocommandtheAIagenttolearntocontrolawheelchair[43]. Thesemethodsareveryfriendlytohumantrainersand
donotrequireanyparticulartrainingontheirpart.
13RunningTitleforHeader
FacialFeedback. Lietal. trainedamappingmodeltomapimplicitemotionstovarioustypesofexplicitfeedback
data. Facialexpressionsweremarkedwithdifferenttypesoffeedbackinadvance,suchas1for“happy”and0or-1for
“sadness”[94]. Basedonthiswork,Gadanhointroducedafacialfeedbackreinforcementlearningmethodbasedonan
emotionrecognitionsystem. ThesystemcanlearntodecidewhentochangeorreinforceitsbehaviourwithQ-learning
byidentifyinghumanemotions[95]. Arakawaetal. introducedtheDQN-TAMERmodel,whereanAIagentmay
obtainfacialexpressionsviaacamera,andthenusethefacialexpressiondatatomapdifferentemotionsasimplicit
rewardstoimprovelearningefficiency[96]. Veeriahetal. proposedamethodwheretheagentmayanalysehuman
facialfeaturesfromcameraimagestogainadditionalrewards. Asaresult,theAIagentcanquicklyadapttotheuser’s
facialchangesinordertocompletethetask[97]. Oneofthelimitationsofthiskindofmethodsisthathumanemotions
cannotbeidentifiedmerelybasedonfacialexpressions,andtheremaybeadelayinconvertingmachinerecognition
expressionsintofeedback.
NaturalLanguageFeedback. Whencomparedtofacialexpressionandgesturetrackingfeedbackmethods,natural
language feedback makes it easier to convert the token vector of the sentence into quantitative feedback. Natural
languagefeedbackcanbetransformedandappliedtoseveralaspectsofreinforcementlearning,suchasrewards,values,
andpolicies. Goyaletal. introducedtheLEARN(LanguagE-ActionRewardNetwork)method,whichisareward
shapingmethod[98]. Inthestate-actionspaceofthetask,ifmostoftherewardsignalsare0s,wecallitthesparsityof
rewards. Sparserewardsmaycausethealgorithmtoconvergeslowly. AIagentsneedtointeractwiththeenvironment
severaltimesandlearnfromalargenumberofsamplestoreachanoptimalsolution. Onesolutiontothisproblemis
toprovidetheAIagentwithabonusrewardinadditiontotherewardfunctionwhenevertheAIagenttakesaright
steptowardthegoal. Thisprocessiscalledrewardshaping. MaclinandShavlikproposedRATLE(Reinforcementand
Advice,ConsultingLearningEnvironment)[99],wheretheAIagentcantranslatehumannaturallanguagesuggestions
intofeedbackfortheQ-valuefunctiontoacceleratethelearningprocess. Kuhlmannproposedamethodbasedon
transformingnaturallanguagesuggestionsintoanalgorithm-understandableformallanguagetooptimisethelearning
policy[100]. Inadditiontothemethodsdescribedabovefortransformingintodifferentpartsofthealgorithm,natural
languagecanalsobeusedtodirectlyguidetheAIagent’slearningpolicy. Forexample,Williamsetal. proposedan
object-orientedMarkovDecisionProcess(MDP)frameworkwhichcanmapthenaturallanguagetorewardsfeedback
[101].
7.3 Multi-modalFeedback
Theresearchaboveisfocusedonasingleinputinteractionmethod. Multi-modalinteractions,ontheotherhand,are
moreprevalentandefficientinday-to-dayhuman-humaninteractions. Multi-modecommunicationhasthefollowing
benefits. First,whenasingle-modepieceofinformationisdisruptedbynoiseorocclusion,othermodescanbeused
asinformationsupplements. Second,whenmulti-modalinteractionisavailable,ithasthepotentialtoimprovethe
robustnessandreliabilityofcommunication. Queketal. introducedaframeworkforanalysinglanguage’smutual
support and accompanying gestures [102]. Cruz et al. proposed a dynamic multi-modal audiovisual interaction
frameworkthatwouldallowhumanstoprovidefeedbackusingtheirvoicesandgestures[103]. Griffithetal. [109]
introduced a multi-modal interaction method based on hand gestures and speech recognition system, which was
restrictedtooperatinggeometricobjectsonmaps. Weberetal. [103]developedadynamicaudiovisualintegration
methodthatallowshumanstoinputinformationvianaturallanguageandgestures. Intheaboveexperiments,multi-
modeinteractionsgenerallyoutperformedsingle-modeinteractions. Mostofthecurrentmulti-modeinteractionsare
merelyacombinationoftwomodes,suchasanytwoofvoice,gesture,sound,andvision. Oneoftheproblemsofthe
abovemulti-modalmethodsistheirinabilitytocombinevariousformsofhumanfeedback. Theabilityofhumans
todirectlyinteractwithAIagentsusingmultiplemethodsatthesametimeremainsunexplored. Inthefuture,these
multi-modeinteractivemethodscanbecombinedinmoreformstodevelopeffectivehuman-AIcollaborationfora
widerrangeofscenarios.
Somestudiestakeintoaccounttheeffectofhumanfatiguecausedbyincreasingtrainingtimeonthequantityand
qualityoffeedback. Astrainingdurationgrows,humantrainersbecomeexhausted,reducingtheamountoffeedback
whilesimultaneouslyloweringthequalityofthefeedback[104,106]. Methodsforencouraginghumantrainerstoraise
interactionexcitementthroughgamificationwereproposed;suchmethodshavebeenfoundtodecreasewearinessand
effectivelyimprovehumantrainers’efficiency[105].
8 AlgorithmicModels
In the previous section, we analysed how humans provide feedback to AI agents. In this section, we categorise
algorithmicmodelsbasedonhowagentsreceiveandprocesshumanfeedback.
14RunningTitleforHeader
8.1 Reward-basedMethods
Reward-basedmethodsacceleratethelearningprocessbyadjustingtherewardthattheAIagentreceivesfromthe
environment. Concretely,aftertheAIagentreceivesfeedbackfromtheenvironment,humanscanscaleupordownthe
rewardsbasedontheirknowledge,potentiallyacceleratingthelearningprocess[108]. Computationally,thereward
from human H(s,a,s′), is added to the reward from the environmental reward R(s,a,s′) to get the new reward
R¯(s,a,s′).
R¯(s,a,s′)=R(s,a,s′)+H(s,a,s′), (2)
ThomazandBreazealproposedamethodfornon-experthumantrainerstoinfluencetheAIagent’snextactionby
providingapositiveoranegativenumericalreward. Iftheagentreceivedanegativefeedback,itwouldattemptto
reversethepreviousactioninordertogetahigherscore[107].
KnoxandStonefirstintroducedtheTAMERalgorithm,whichuseshumandemonstrationasinputtoguidetheAIto
performbetter[87]. BasedontheTAMERmethod,Rikuetal. introducedaframeworkwhichcombinesdeeplearning
methodandTAMER,namedDQN-TAMER,whererewardswereshapedbythehuman’snumericalbinaryfeedback
andenvironments[108]. Additionally,Arakawaetal. investigatedafacialexpressionfunctionbasedonreward-shaped
method,whichisappliedinamaze-likeenvironmentgame[132]. Thehumantrainers’facialexpressionscouldprovide
feedbacktotheAIagents. Themajorshortcomingisthattherecognitionofthehumanfacialexpressionisimprecise
andintermittent.
Rosenfeld et al. develops a heuristic function method, where the AI agent receive feedback generated by hand-
engineereddatafromthehumantrainer[133]. Theexperiment’sfindings[134]indicatethatheuristicfunctionsmay
beanaturalmethodforAIagentstolearnfromhumantrainers. Theprimarydisadvantageofthisapproachisthatit
requireshumantrainerswithextensiveprofessionalbackgroundsandprogrammingskills. Itwillbeextremelyhostile
tonon-professionalusers.
Reward-basedmethodscanefficientlyexpeditethelearningprocessinanenvironmentwithsparserewards,butthere
arecertaindrawbacks,listedbelow. Thefirstproblemis"creditallocation",whichisespeciallyproblematicinarapidly
changingenvironmentwherehumansmaybetooslowtoprovidetimelyfeedback. Therefore,themethod’slimitation
remainshowtomaphumanrewardstocorrespondingactions. Thesecondproblemis"rewardhacking",wheretheAI
agentmayachievethegreatestrewardsbyusingwaysthathumanswouldnotexpect[17].
8.2 InverseRewardDesignMethods
Theagentisconstantlyattemptingtooptimisethehuman-designedrewardfunction. WhendesigningtheAIagent,
humandevelopersalwayssettherewardfunctionbasedontheexperimentalenvironment,buttheAIagentalways
encountersanewenvironment. Usingtheoriginalrewardfunctiondesignedbyhumansinanewenvironmentmay
leadtopoorconvergence. Mindermannetal. presentedaninverserewardfunctioninresponsetothisissue[135]. To
obtainthetruetarget,thismethodisbasedonthedesignedrewardfunctionandthetrainedMDP.Thisallowsagents
toeffectivelyadapttothenewenvironment,eliminatingtheissueofrewardhacking. Morespecifically,thismethod
takesthedesignedrewardfunction,thetestenvironmentmodel,andtheMDPinthenewenvironmentasinput. Thena
Bayesianfunctionmapstheproxyrewardstotherealrewards. Theexperimentin[136]demonstratesthattheinverse
rewardmethodcouldsuccessfullyboosttheAIagent’slearningefficiency.
8.3 Policy-basedMethods
Policy-basedmethodsmodifythelearningpolicyoftheAIagentactionprocesstoencouragetheactiontofitwhat
thehumantrainersexpect[17]. HumantrainersmaybeawareofalargenumberofpotentialoptimalactionsAina
givenstateS,theprobabilityofhumanprovidingfeedbacktotheAIagentcanbedenotedasC,where0 < C < 1.
Thedifferencebetweenpositiveandnegativehumanfeedbackcanbeexpressedas∆ . Theprobabilitythathumans
s,a
givepolicyfeedbackPr (a)inagivenstateS canbeexpressedas
c
C∆s,a
Pr (a)= (3)
c C∆s,a +(1−C)∆s,a
At present, the method that uses human critique for state and action pairs as input to shape agent policy is widely
accepted. Griffithetal. proposedanoptimalpolicymethodbasedonhumanfeedback,aBayesianmethodthattakes
15RunningTitleforHeader
asinputcritiquesforeachstateandactionpair[109]. Theexperimentsin[88]suggestthatthispolicy-basedmethod
outperformsotherreward-basedmethods.
KreningandFeighconductedanexperimentinwhichtheycomparedtwodifferentpolicy-basedmethodsthatcould
bringabetteruserexperience[137]. ThefirstoneiscritiquefeedbackmethodproposedbyGriffith[109],andthe
secondistheirNewtonianactionadvicemethod[137]. Theresultisthatthemethodofactionadviceisbetterandthe
timerequiredisreduced.
MacGlashan et al. proposed a Convergent Actor-Critic method, COACH (Corrective Advice Communicated by
Humans). Thisframeworkallowsnon-expertstousenumericalbinaryfeedbacktoformulatepoliciesthroughcorrective
suggestions[89]. Dilipetal. proposedadeepCOACHmethodbasedontheoriginalCOACH,whichusesrawpixels
asinputtotraintheAIagent’spolicy. Theauthorsarguedthattheuseofhighlyrepresentativeinputsfacilitatesthe
applicationofthealgorithminmorecomplexenvironments[55].
Whencomparedtoreward-basedmethods,theadvantageofpolicy-basedmethodsisthattheydonotrequirespecific
feedbackfromhumanstoAIagents. Nevertheless,humansmustdeterminewhichstrategymaybethemosteffective
assistingtheAIagent. Thismayhavehigherrequirementsforthepriorknowledgeofhumantrainers.
8.4 ValueFunctionbasedMethods
Valuefunctionbasedmethodsestimatefuturerewardstoobtainthehighestpotentialrewardattheendofthetask,by
usinghumanknowledge[17]. TheycombinethevaluerepresentinghumanpreferencewiththevalueobtainedbytheAI
agentfromtheenvironmenttopromotethelearningprocess. Matthewetal. proposedamethodthatcombineshuman
preferenceandagentvaluecalledHuman-AgentTransfer(HAT)[110]. Thealgorithmgeneratesastrategybasedon
recordedhumantrainerpreferences,whichitthenappliestoshapetheQ-valuefunction. Thisshapingprocessprovides
astablerewardforthestate-actionpair,intheQ-learningprocess. Brysetal. proposedamethodthatuseshuman
demonstrationsasinputforavaluenamedRLfD.ThismethodgeneratesaGaussianfunctionbyhumandemonstration
toguidetheexplorationprocessoftheQ(λ)algorithm[111].
Despitethefactthatvaluefunctionbasedmethodsarelikelytobeaneffectivewayforminimisinghumanfeedback,
therearenowjustafewstudiesbasedonit.
8.5 ExplorationProcessbasedMethods
ReinforcementlearningisamethodinwhichanAIagentneedstocontinuouslyinteractwiththeenvironmentand
completetasksbasedonrewards. ThismeansthattheAIagentneedstoperformactionsthatithasnevertriedbefore.
Thisprocessisreferredtoastheexplorationprocess. Inexplorationprocessbasedmethods,humanscanincreasethe
efficiencybyreducingAIagenterrorsandunnecessaryattempts[108]. Explorationprocessbasedmethodsaimto
minimisetheactionspacebyinjectingpriorhumanknowledgetoguidetheAIagent’sexplorationinordertoincrease
learningefficiency.
ThomazandBreazealconductedanexperimentinthegameSophie’sKitchentoevaluatehumanguidancethathelpsthe
AIagentminimiseitsactionspaceinordertoenhancelearningefficiency[91]. Theresultssuggestthatemploying
humanpriorknowledgetolimitlowutilityeffortsismoreefficientthanusingscalarrewardfunctions[112]. Suay
et al. developed an upgrading approach in which the user may help exploration by highlighting goal states in the
environment[138].Yuetal.proposedanapproach,termedasactionbiasing,whichleveragesuserfeedbacktostimulate
theAIagent’sexplorationprocess. Thesumoftheagentanduservaluefunctionsisemployedasavaluefunction,to
incorporatehumanfeedbackintotheAIagent’slearningprocess[139]. Thesemethodsareconsideredeffective,but
theygenerallyneedtobetrainedbyhumans,andthistrainingprocessrequiresalotofprofessionalknowledgeand
participation.
In general, collaborative reinforcement learninghas shown greatpotential inimprovingthe efficiency ofdecision-
makingtasks. However,furtherresearchisneededtodeterminehowtobuildtheenvironmentmodelsinwhichhumans
interactwithAIagents. Thesemodelsshouldconsidernotonlytheeffectivenessandefficiencyofinteractivemethods,
butalsointerpretability,accountability,andpossibleethicalissuesindecision-making. Therefore,inthefollowing
sections,werefertotheliteratureonthepatternofhuman-machinerelationsintheengineeringfield,andpropose
guidanceforfuturedevelopmentofcollaborativereinforcementlearningmethods.
16RunningTitleforHeader
9 DesignTrajectoryMap
BasedonthepreviousCRLtaxonomy,weproposeanovelCRLTrajectoryDesignMaptoguideresearchersdesignCRL
systems. Whenresearchersstartdesigningahuman-AIcollaborativereinforcementlearningsystem,theycouldfollow
ourCRLTrajectoryDesignMap(Figure5)stepbystep. First,theystartwithselectingacollaborativepatternfroma
macroperspectiveintheDesignPatterns(Section4)category. Next,theychoosedifferentcollaborativelevelsanda
numberoftheparticipantsintheCollaborativeLevelsandParties(Section5). Afterthat,theychoosethecooperative
capabilitiesthateverypartyshouldhaveintheCollaborationCapabilities(Section6). Finally,theyselectsuitable
interactivemethodsandalgorithmicmodelsforthespecifictaskrequirementcategoriesofInteractiveMethods(Section
7)andAlgorithmicModels(Section8).
Figure2presentsournewlyproposedCRLtaxonomy,whichcontainsthemostcommonlyusedandhighlycitedmethods
anddesignpatternsintheCRLresearcharea,andwhichcanalsobeusedasaTrajectoryMap(seeFigure5)ofdesigning
collaborativereinforcementlearningsystems,asfollows. ResearchersmayutiliseourTrajectoryMaptodeveloptheir
architectureastheygofromthetopDesignPatternstothenext,untilthemostdetailedAlgorithmicModelsisselected.
IntheMap,thefirstpartsuggestsDesignPatterns,whicharethemostpopularstructureofhuman-AIcollaborative
frameworksintheCRLdomain. Theseincludecognitivesystemsengineering(CSE)[7],Bosch’sframework[40],the
Coactivedesignpattern[41]andSchmidt’sframework[42]. ThesecondpartaretheCollaborativeLevelsandParties,
whiletheThirdpartcoversCollaborativeCapabilities,whichincludeunderstanding,communication,commitments,
andinstitutions. TheforthpartisInteractiveMethods,includingexplicitandimplicitinteractionmethods,aswellas
multi-moduleinteractionmodes[17]. ThelastpartreflectsAlgorithmicModels,whichcontainsreward-basedmethods
[108],value-basedmethods[110],policy-basedmethods[109],andexploration-process-basedmethods[91]. This
taxonomycouldbeusedasasystematicmodellingtoolforresearchersandpractitionerstoselectandimprovetheir
newCRLdesigns. TheycouldchooseanarchetypeinDesignPatternsfortheoverallarchitectureatstart. Then,they
couldselectaCollaborativeLevelandthenumbersofthePartiesinthecollaboration. Afterthat,theycouldselectthe
CollaborativeCapabilitiesthattheAIagentsshouldhave,andselectsuitableInteractiveMethodsandAlgorithmic
Modelsthatcanmeettherequirementsofspecifictasks. Ifresearcherswishtolearnaboutthemostadvancedtechnology
developedinthelastdecade,theycouldchecktheclassificationweprovideinTable1.
Human-AI
Collaborative
Design Pattern
Collaborative
Levels & Parties
Collaborative
Capabilities
Interactive
Methods
Algorithmic
Models
Figure5: ADesignTrajectoryMapofCollaborativeReinforcementLearningSystems
17RunningTitleforHeader
10 FutureWorkRecommendations
Reinforcementlearningseemstohavereachedaplateauafterexperiencingarapiddevelopment.Itisdifficulttoimprove
theefficiencyofAIagentsinacomplexenvironmentwithoutclearfeedback. Theresearchcommunityhasproposed
somecollaborativemethodstoovercometheseobstacles. Forexample,humansdeliverfeedbacktoAIagentsthrough
hardwareorsensorstoimprovealgorithmsefficiency;AIagentsprovidehumanswithexplanationsofdecisionsto
improvethecredibilityofalgorithms. However,researchinthisareaisonlyatthebeginningstage,andtherearemany
openchallengestobetackled. Inthefollowingsections,werecommendseveralpromisingfutureresearchdirectionsin
thefieldofCollaborativeReinforcementLearning(CRL).
CombiningDifferentInteractiveMethods Developmorenaturalmulti-feedbackinteractivemethodsbystudying
the advantages and disadvantages of different interactive methods. Single interactive methods would have higher
requirementsfromhumansandcouldbeinefficient,whereasmulti-modalinteractivemethodswouldlowerinteraction
barriersandimproveefficiency,providinguserswithabetterinteractiveexperience[55].
Inthedesignpatternsmentionedabove,CombiningDifferentInteractiveMethodsbelongsto’AugmentativeLevel
collaboration’. Itisthebasisfortheapplicationofcollaborationtechnologyinreal-lifescenarios;itisalsoanimportant
factortoimproveuserexperience. Therefore,researcherscouldworkonmoreadvancedinteractionmodesbasedon
thisdesignconceptsandappliedtodifferentscenarios.
UserModelling Itisimportanttobuildgenericusermodelstoenablethesystemtoacceptuserfeedbackrobustly.
Suchmodelscouldbeusedtobuildhuman-AIcollaborationapplicationsthatreducehumanfatiguebydetectingand
predictinghumanbehaviour[17],duetotheirabilityofadaptingtointeractionchannelsandfeedbacktypesaccording
to the user’s preferences. This would require empirical studies to find a way to map between user types and their
preferredinteractionchannelsandfeedbacktypes.
Inthepatternsmentionedabove,usermodellingisoneofthemostimportantissuesof’IntegrativeLevelcollaboration’.
OnlywithaccuratemodelscouldhumansandAIagentscommunicatewithoutbarriers. Theabilityofpredictingeach
other’sbehaviourcouldgeneratetrust. Understandingtheunexpectedsituationsthatmayoccurforeachparticipant
couldestablishamoreflexiblerelationshipandimprovetheentiresystem’srobustness. Duetotherapiddevelopment
ofAIinrecentyears,thereisstillalotofusermodellingworkthathasnotbeencarriedout. ResearcherscouldbuildAI
andusermodelsbasedontheperspectiveof’IntegrativeLevelcollaboration’: communication,trust,andresilience.
LackofHumanCollaboratingDataandEvaluationMethods HumansubjectiveisessentialforimprovingCRL
technologyduetotheinclusionofhumaninvolvement. However,thereislimitedresearchonhumandatacollectingand
evaluation[132]. Thiscircumstanceiscausedbyavarietyoffactors. Forexample,theexpenseofcollectinghumandata
istoohigh. Variedtypesofsubjectivehumandataneeddifferenttechniquesofcollection,whichisverychallenging
technically.
Thetypicalapproachtothisproblemistoconductmoreexperimentsinordertocollectsufficientdata. Moreover,create
evaluationtechniquesbymergingseveraldisciplines(e.g. psychology). However,therearecertaininnovativewaysthat
needourattention. Strouseetal. proposedacollaboratingmethodwithouthumandata[140],whichiscalledFictitious
Co-Play(FCP).Inthiswork,TheytraintheAIagenttobethebestreactionbyapopulationofself-playagentsandtheir
previouscheckpointstakenduringtraining. ItmightsparkinnovativethinkingfortrainingAIagents.
SafeInteractiveRL Despitetheempiricalsuccessofreinforcementlearningalgorithms, wehaveverylittleun-
derstandingofthewaysuch’black-box’modelswork. Thismeansthatthesystemcannotberesponsiblefortheir
owndecisions[11]. Therefore,howtoestablishamechanismtoprotecthumansafetyandavoidunknowndiscrim-
inationbecomesveryimportant. Solvingthisproblemiscrucialfortheuseofinteractivereinforcementlearningin
high-dimensionalenvironmentsintherealworld.
Inthepatternsmentionedabove,safeinteractiveRLisanimportantissueforthe’IntegrativeLevelcollaboration’and
the’DebativeLevelcollaboration’. IntermsofIntegrativeLevelcollaboration,howtoensurethesafetyofhumansis
oneofthekeyfactorsforhumanstotrustAI.IntermsofDebativelevelcollaboration,whenthedecisionsofhumans
andAIagentsareinconsistent,howtoprotecttheinterestsisnotonlyanengineeringissue,butalsoanethicalissue.
Thisissuerequiresthejointeffortsofmultipledisciplinessuchaslaw,sociology,andethics.
ExplainableCollaborativeReinforcementLearning Sincereinforcementlearningneedstobetrainedviaenviron-
mentalfeedback,differentsensorsorhumanfeedbackwillleadtosignificantlydifferentoutcomes. Besides,explaining
whytheAImakesthespecificdecisioniscriticalforhumanstotrustAIandallocateessentialtaskstotheAIagent.
Therefore,developingtheexplainablecollaborativereinforcementlearningalgorithmisrequired.
18RunningTitleforHeader
ExplainablecollaborativeRLapproachaimstohelppeopleunderstandhowtheagentperceivestheenvironmentand
howitmakesdecisionsbyimprovingthetransparencyofthemodel. Improvingthetransparencyofthemodelnotonly
allowshumanstofindbetterwaystotraintheagent,butalsoallowsthemtotrusttheagenttocollaborate[141,142,22].
High-dimensionalScenarios Atpresent,bothreinforcementlearningandcollaborativereinforcementlearninghave
arelativelylimitedapplicationscenario. Themajorityofexperimentswereconductedinvirtualgamingenvironments.
The agent is confronted with a modest dimensional environment, and there is a small number of variables. This
complicatestheimplementationofCRLinhigh-latitudereal-worldscenarios. Thereisanalgorithmthatefficiently
transfershumanbehaviourtotheagentthroughanautoencoder[55]. Anotherpossibilityistoutilisecrowd-sourcingto
sendhumaninput[104]. However,thiskindofissuerequiresmoreinvestigation.
FastEvaluationofHuman-Acts Intherealscenario,manytasksneedtobeexecutedwithrapidresponse,suchas
stocktradingandautonomousdriving. Howtoquicklyevaluatethereactionoftheotherpartytomakethenextdecision
iscrucial. Somestudiesarecurrentlyfocusedonthisarea, and, inparticular, exploringthemethodsofevaluating
humanbehaviorthroughvisualisationmodels[143,136,144,145],butmoreformsofevaluationofhumanbehaviour
areneeded.
DynamicMentalModels ThecooperativemodelrequiresadynamicmentalmodelfrombothhumansandAIagents,
astheyconstantlyobserveandlearnabouteachother. Italsoneedstoupdatestrategiestimelyduringthelearning
process[54]. Withtheincreaseofcollaborationexperience,itwouldbeusefultoinjectexperienceintothenewlearning
process. Atthisstage,theimplicithumanpriorknowledgeneedstobegraduallytransformedintoexplicitexperience
guidelinesforfuturetasks. Therefore,establishingadynamicmentalmodelmaygreatlypromotethedevelopmentof
human-AIcollaboration.
Inthepatternsmentionedabove,dynamicmentalmodelsarecriticaltothethreelevels,makingdynamicadjustments
accordingtodifferentcollaborationlevels,whichhasagreateffectonreducingpowerconsumptionandimproving
efficiency[21]. Thisisahugechallengeforresearchers. Apotentialdirectionistodesigngeneraldynamicmental
modelsaccordingtothetaxonomyweprovideinthispaper.
11 Conclusions
Inthispaper,wehavepresentedasurveyofCollaborativeReinforcementLearning(CollaborativeRL,orCRL)to
empowertheresearchintohuman-AIinteractionsandcooperativedesigns. Thisanalysisresultedinusproposinganew
CRLclassificationmethod(seeTable1),calledCRLDesignTrajectoryMap(seeFigure5)andanewCRLtaxonomy
(seeFigure2)asasystematicmodellingtoolforselectingandimprovingnewCRLdesigns. Researcherscoulduseour
TrajectoryMaptodesignaCRLsystemfromscratchorusepartsofitaccordingtotheirneedstorefinetheirsystem.
Forexample,researcherscouldselecttheirdesiredsystemstructureinHuman-AICollaborativeDesignPattern,identify
andsatisfytherequirementsofdifferentcomponentsinCollaborativeLevels&PartiesandCollaborativeCapabilities,
andselectdifferentdesigncomponentsinAlgorithmicModelsandInteractiveMethods. Thisisacomprehensivedesign
approachfromtoptobottomandfrommacrotomicro. Summarising,throughthissurvey,weprovideresearchersand
practitionerswiththetoolstostartimprovingandcreatingnewdesignsforCRLmethods.
References
[1] Keith Ronald Skene. Artificial Intelligence and the Environmental Crisis: Can Technology Really Save the
World? Routledge,2019.
[2] EliezerYudkowskyetal.Artificialintelligenceasapositiveandnegativefactoringlobalrisk.Globalcatastrophic
risks,1(303):184,2008.
[3] GeorgeZarkadakis. Inourownimage: willartificialintelligencesaveordestroyus? RandomHouse,2015.
[4] AaronSloman. Didsearleattackstrongstrongorweakstrongai. ArtificialIntelligenceandItsApplications,
JohnWileyandSons,1986.
[5] ElizabethGibney. Googleaialgorithmmastersancientgameofgo. NatureNews,529(7587):445,2016.
[6] RichardSSuttonandAndrewGBarto. Reinforcementlearning: Anintroduction. MITpress,2018.
[7] ErikHollnagelandDavidDWoods. Cognitivesystemsengineering: Newwineinnewbottles. International
journalofman-machinestudies,18(6):583–600,1983.
19RunningTitleforHeader
[8] KjeldSchmidt,JRasmussen,BBrehmer,andJLeplat. Cooperativework: Aconceptualframework. Distributed
decisionmaking: Cognitivemodelsforcooperativework,pages75–110,1991.
[9] MatthewJohnson, CatholijnJonker, BirnaVanRiemsdijk, PaulJFeltovich, andJeffreyMBradshaw. Joint
activitytestbed: Blocksworldforteams(bw4t). InInternationalWorkshoponEngineeringSocietiesinthe
AgentsWorld,pages254–256.Springer,2009.
[10] SaleemaAmershi,MayaCakmak,WilliamBradleyKnox,andToddKulesza. Powertothepeople: Theroleof
humansininteractivemachinelearning. AiMagazine,35(4):105–120,2014.
[11] JavierGarcıaandFernandoFernández. Acomprehensivesurveyonsafereinforcementlearning. Journalof
MachineLearningResearch,16(1):1437–1480,2015.
[12] YangGao,JanPeters,AntoniosTsourdos,ShaoZhifei,andErMengJoo. Asurveyofinversereinforcement
learningtechniques. InternationalJournalofIntelligentComputingandCybernetics,2012.
[13] ErikaPuiuttaandEricVeith. Explainablereinforcementlearning: Asurvey. arXivpreprintarXiv:2005.06247,
2020.
[14] JanLeike,DavidKrueger,TomEveritt,MiljanMartic,VishalMaini,andShaneLegg. Scalableagentalignment
viarewardmodeling: aresearchdirection. arXivpreprintarXiv:1811.07871,2018.
[15] AnisNajarandMohamedChetouani. Reinforcementlearningwithhumanadvice.asurvey. arXivpreprint
arXiv:2005.11016,2020.
[16] Guangliang Li, Randy Gomez, Keisuke Nakamura, and Bo He. Human-centered reinforcement learning: a
survey. IEEETransactionsonHuman-MachineSystems,49(4):337–349,2019.
[17] ChristianArzateCruzandTakeoIgarashi. Asurveyoninteractivereinforcementlearning: Designprinciplesand
openchallenges. InProceedingsofthe2020ACMDesigningInteractiveSystemsConference,pages1195–1209,
2020.
[18] ShwetaSuran,VishwajeetPattanaik,andDirkDraheim. Frameworksforcollectiveintelligence: Asystematic
literaturereview. ACMComputingSurveys(CSUR),53(1):1–36,2020.
[19] AndreasClassen,PatrickHeymans,andPierre-YvesSchobbens. What’sinafeature:Arequirementsengineering
perspective. InInternationalConferenceonFundamentalApproachestoSoftwareEngineering,pages16–30.
Springer,2008.
[20] ZhaoxingLi,LeiShi,AlexandraICristea,andYunzhanZhou. Asurveyofcollaborativereinforcementlearning:
Interactivemethodsanddesignpatterns. InDesigningInteractiveSystemsConference2021,pages1579–1590,
2021.
[21] GesinaSchwalbeandMartinSchels. Asurveyonmethodsforthesafetyassuranceofmachinelearningbased
systems. In10thEuropeanCongressonEmbeddedRealTimeSoftwareandSystems(ERTS2020),2020.
[22] Amina Adadi and Mohammed Berrada. Peeking inside the black-box: A survey on explainable artificial
intelligence(xai). IEEEAccess,6:52138–52160,2018.
[23] RichardBellman. Dynamicprogrammingandanewformalisminthetheoryofintegralequations. Proceedings
oftheNationalAcademyofSciencesoftheUnitedStatesofAmerica,41(1):31,1955.
[24] RichardBellman. Amarkoviandecisionprocess. Journalofmathematicsandmechanics,pages679–684,1957.
[25] JohnHAndreae. Stella: Aschemeforalearningmachine. IFACProceedingsVolumes,1(2):497–502,1963.
[26] JohnHenryHollandetal.Adaptationinnaturalandartificialsystems:anintroductoryanalysiswithapplications
tobiology,control,andartificialintelligence. MITpress,1992.
[27] Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems, volume 37.
UniversityofCambridge,DepartmentofEngineeringCambridge,UK,1994.
[28] FrankDellaert,DieterFox,WolframBurgard,andSebastianThrun. Montecarlolocalizationformobilerobots.
InProceedings1999IEEEInternationalConferenceonRoboticsandAutomation(Cat.No.99CH36288C),
volume2,pages1322–1328.IEEE,1999.
[29] VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AlexGraves,IoannisAntonoglou,DaanWierstra,and
MartinRiedmiller. Playingatariwithdeepreinforcementlearning. arXivpreprintarXiv:1312.5602,2013.
[30] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc
Lanctot,LaurentSifre,DharshanKumaran,ThoreGraepel,etal. Masteringchessandshogibyself-playwitha
generalreinforcementlearningalgorithm. arXivpreprintarXiv:1712.01815,2017.
20RunningTitleforHeader
[31] LiliChen,KevinLu,AravindRajeswaran,KiminLee,AdityaGrover,MichaelLaskin,PieterAbbeel,Aravind
Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. arXiv
preprintarXiv:2106.01345,2021.
[32] Guillaume Lample and Devendra Singh Chaplot. Playing fps games with deep reinforcement learning. In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume31,2017.
[33] JensKober,JAndrewBagnell,andJanPeters. Reinforcementlearninginrobotics: Asurvey. TheInternational
JournalofRoboticsResearch,32(11):1238–1274,2013.
[34] AVBernsteinandEVBurnaev. Reinforcementlearningincomputervision. InTenthInternationalConference
onMachineVision(ICMV2017),volume10696,page106961S.InternationalSocietyforOpticsandPhotonics,
2018.
[35] GwernBranwen. Gpt-3creativefiction. 2020.
[36] David Rohde, Stephen Bonner, Travis Dunlop, Flavian Vasile, and Alexandros Karatzoglou. Recogym: A
reinforcementlearningenvironmentfortheproblemofproductrecommendationinonlineadvertising. arXiv
preprintarXiv:1808.00720,2018.
[37] NisanStiennon, LongOuyang, JeffWu, DanielMZiegler, RyanLowe, ChelseaVoss, AlecRadford, Dario
Amodei,andPaulChristiano. Learningtosummarizefromhumanfeedback. arXivpreprintarXiv:2009.01325,
2020.
[38] XiangZhang,LinaYao,XianzhiWang,JessicaMonaghan,DavidMcalpine,andYuZhang. Asurveyondeep
learning-basednon-invasivebrainsignals: recentadvancesandnewfrontiers. JournalofNeuralEngineering,
18(3):031002,2021.
[39] FranciscoCruz,GermanIParisi,JohannesTwiefel,andStefanWermter. Multi-modalintegrationofdynamicau-
diovisualpatternsforaninteractivereinforcementlearningscenario. In2016IEEE/RSJInternationalConference
onIntelligentRobotsandSystems(IROS),pages759–766.IEEE,2016.
[40] Karel van den Bosch and Adelbert Bronkhorst. Human-ai cooperation to benefit military decision making.
NATO,2018.
[41] MatthewJohnson, JeffreyMBradshaw, PaulJFeltovich, CatholijnMJonker, MBirnaVanRiemsdijk, and
MaartenSierhuis. Coactivedesign: Designingsupportforinterdependenceinjointactivity. JournalofHuman-
RobotInteraction,3(1):43–69,2014.
[42] StéphaneZieba,PhilippePolet,FrédéricVanderhaegen,andSergeDebernard. Principlesofadjustableautonomy:
aframeworkforresilienthuman–machinecooperation. Cognition,Technology&Work,12(3):193–203,2010.
[43] InhyukMoon,MyungjoonLee,JeicheongRyu,andMuseongMun. Intelligentroboticwheelchairwithemg-,
gesture-,andvoice-basedinterfaces. InProceedings2003IEEE/RSJInternationalConferenceonIntelligent
RobotsandSystems(IROS2003)(Cat.No.03CH37453),volume4,pages3453–3458.IEEE,2003.
[44] MikeWu,SonaliParbhoo,MichaelCHughes,RyanKindle,LeoACeli,MaurizioZazzi,VolkerRoth,and
FinaleDoshi-Velez. Regionaltreeregularizationforinterpretabilityindeepneuralnetworks. InAAAI,pages
6413–6421,2020.
[45] AbhinavVerma,VijayaraghavanMurali,RishabhSingh,PushmeetKohli,andSwaratChaudhuri. Programmati-
callyinterpretablereinforcementlearning. arXivpreprintarXiv:1804.02477,2018.
[46] GuiliangLiu,OliverSchulte,WangZhu,andQingcanLi. Towardinterpretabledeepreinforcementlearning
withlinearmodelu-trees. InJointEuropeanConferenceonMachineLearningandKnowledgeDiscoveryin
Databases,pages414–429.Springer,2018.
[47] PrashanMadumal,TimMiller,LizSonenberg,andFrankVetere. Explainablereinforcementlearningthrougha
causallens. arXivpreprintarXiv:1905.10958,2019.
[48] KarelvandenBosch, TjeerdSchoonderwoerd, RomyBlankendaal, andMarkNeerincx. Sixchallengesfor
human-aico-learning. InInternationalConferenceonHuman-ComputerInteraction,pages572–589.Springer,
2019.
[49] PabloSaumaChacónandMarkusEger. Pandemicasachallengeforhuman-aicooperation. InProceedingsof
theAIIDEworkshoponExperimentalAIinGames,2019.
[50] Rogelio Enrique Cardona-Rivera and Robert Michael Young. Games as conversation. In Tenth Artificial
IntelligenceandInteractiveDigitalEntertainmentConference,2014.
[51] ClaireLiang,JuliaProft,ErikAndersen,andRossAKnepper. Implicitcommunicationofactionableinformation
inhuman-aiteams. InProceedingsofthe2019CHIConferenceonHumanFactorsinComputingSystems,pages
1–13,2019.
21RunningTitleforHeader
[52] RKLING. Routinedecision-making-thefutureofbureaucracy-inbar,m,1981.
[53] GeoffreyIrving,PaulChristiano,andDarioAmodei. Aisafetyviadebate. arXivpreprintarXiv:1805.00899,
2018.
[54] JohnRFrederiksen,BarbaraYWhite,andJoshuaGutwill. Dynamicmentalmodelsinlearningscience: The
importanceofconstructingderivationallinkagesamongmodels. JournalofResearchinScienceTeaching: The
OfficialJournaloftheNationalAssociationforResearchinScienceTeaching,36(7):806–836,1999.
[55] Dilip Arumugam, Jun Ki Lee, Sophie Saskin, and Michael L Littman. Deep reinforcement learning from
policy-dependenthumanfeedback. arXivpreprintarXiv:1902.04257,2019.
[56] MariaHaglandDongoRémiKouabenan. Safeontheroad–doesadvanceddriver-assistancesystemsuseaffect
roadriskperception? TransportationresearchpartF:trafficpsychologyandbehaviour,73:488–498,2020.
[57] AllanDafoe,EdwardHughes,YoramBachrach,TantumCollins,KevinRMcKee,JoelZLeibo,KateLarson,
andThoreGraepel. Openproblemsincooperativeai. arXivpreprintarXiv:2012.08630,2020.
[58] JonHovi. Games,threats,andtreaties: understandingcommitmentsininternationalrelations. Burns&Oates,
1998.
[59] StefanoVAlbrechtandPeterStone. Autonomousagentsmodellingotheragents: Acomprehensivesurveyand
openproblems. ArtificialIntelligence,258:66–95,2018.
[60] AndrewYNg,StuartJRussell,etal. Algorithmsforinversereinforcementlearning. InIcml,volume1,page2,
2000.
[61] StuartArmstrongandSörenMindermann. Occam’srazorisinsufficienttoinferthepreferencesofirrational
agents. arXivpreprintarXiv:1712.05812,2017.
[62] Kareem Amin, Nan Jiang, and Satinder Singh. Repeated inverse reinforcement learning. arXiv preprint
arXiv:1705.05427,2017.
[63] Shi Bai, Jinkun Wang, Fanfei Chen, and Brendan Englot. Information-theoretic exploration with bayesian
optimization. In2016IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS),pages
1816–1822.IEEE,2016.
[64] HarmendeWeerd,RinekeVerbrugge,andBartVerheij. Negotiatingwithotherminds: theroleofrecursive
theory of mind in negotiation with incomplete information. Autonomous Agents and Multi-Agent Systems,
31(2):250–287,2017.
[65] NolanBard,JakobNFoerster,SarathChandar,NeilBurch,MarcLanctot,HFrancisSong,EmilioParisotto,
Vincent Dumoulin, Subhodeep Moitra, Edward Hughes, et al. The hanabi challenge: A new frontier for ai
research. ArtificialIntelligence,280:103216,2020.
[66] KyleWagner,JamesAReggia,JuanUriagereka,andGeraldSWilkinson. Progressinthesimulationofemergent
communicationandlanguage. AdaptiveBehavior,11(1):37–69,2003.
[67] SainbayarSukhbaatar,RobFergus,etal. Learningmultiagentcommunicationwithbackpropagation. Advances
inneuralinformationprocessingsystems,29:2244–2252,2016.
[68] JakobNFoerster,YannisMAssael,NandoDeFreitas,andShimonWhiteson. Learningtocommunicatewith
deepmulti-agentreinforcementlearning. arXivpreprintarXiv:1605.06676,2016.
[69] KrisCao,AngelikiLazaridou,MarcLanctot,JoelZLeibo,KarlTuyls,andStephenClark. Emergentcommuni-
cationthroughnegotiation. arXivpreprintarXiv:1804.03980,2018.
[70] JamesDFearon. Rationalistexplanationsforwar. Internationalorganization,49(3):379–414,1995.
[71] AmartyaSen. Goals,commitment,andidentity. JLEcon.&Org.,1:341,1985.
[72] DouglassCNorth. Institutionsandcrediblecommitment. JournalofInstitutionalandTheoreticalEconomics
(JITE)/ZeitschriftfürdiegesamteStaatswissenschaft,pages11–23,1993.
[73] KyleBagwell. Commitmentandobservabilityingames. GamesandEconomicBehavior,8(2):271–280,1995.
[74] MatthewOJacksonandMassimoMorelli. Thereasonsforwars: anupdatedsurvey. InThehandbookonthe
politicaleconomyofwar.EdwardElgarPublishing,2011.
[75] JohnCriswell,NathanDautenhahn,andVikramAdve. Kcofi: Completecontrol-flowintegrityforcommodity
operatingsystemkernels. In2014IEEESymposiumonSecurityandPrivacy,pages292–307.IEEE,2014.
[76] LoiLuu,Duc-HiepChu,HrishiOlickel,PrateekSaxena,andAquinasHobor. Makingsmartcontractssmarter.
InProceedingsofthe2016ACMSIGSACconferenceoncomputerandcommunicationssecurity,pages254–269,
2016.
22RunningTitleforHeader
[77] ChristopherKFrantzandMariuszNowostawski. Frominstitutionstocode: Towardsautomatedgenerationof
smartcontracts. In2016IEEE1stInternationalWorkshopsonFoundationsandApplicationsofSelf*Systems
(FAS*W),pages210–215.IEEE,2016.
[78] ReidGSmith. Thecontractnetprotocol: High-levelcommunicationandcontrolinadistributedproblemsolver.
IEEETransactionsoncomputers,29(12):1104–1113,1980.
[79] BryanHorlingandVictorLesser. Asurveyofmulti-agentorganizationalparadigms. TheKnowledgeengineering
review,19(4):281–316,2004.
[80] JacquesFerberandGerhardWeiss. Multi-agentsystems: anintroductiontodistributedartificialintelligence,
volume1. Addison-WesleyReading,1999.
[81] AlanHBondandLesGasser. Readingsindistributedartificialintelligence. MorganKaufmann,2014.
[82] JavierVázquez-Salceda,VirginiaDignum,andFrankDignum. Organizingmultiagentsystems. Autonomous
AgentsandMulti-AgentSystems,11(3):307–360,2005.
[83] PaulDütting,FelixFischer,PichayutJirapinyo,JohnKLai,BenjaminLubin,andDavidCParkes. Payment
rulesthroughdiscriminant-basedclassifiers,2015.
[84] PaulDütting,ZheFeng,HarikrishnaNarasimhan,DavidParkes,andSaiSrivatsaRavindranath. Optimalauctions
throughdeeplearning. InInternationalConferenceonMachineLearning,pages1706–1715.PMLR,2019.
[85] AndreaTacchetti,DJStrouse,MartaGarnelo,ThoreGraepel,andYoramBachrach. Aneuralarchitecturefor
designingtruthfulandefficientauctions. arXivpreprintarXiv:1907.05181,2019.
[86] OfraAmir,GuniSharon,andRoniStern. Multi-agentpathfindingasacombinatorialauction. InTwenty-Ninth
AAAIConferenceonArtificialIntelligence,2015.
[87] WBradleyKnoxandPeterStone. Interactivelyshapingagentsviahumanreinforcement: Thetamerframework.
InProceedingsofthefifthinternationalconferenceonKnowledgecapture,pages9–16,2009.
[88] RobertLoftin,BeiPeng,JamesMacGlashan,MichaelLLittman,MatthewETaylor,JeffHuang,andDavidL
Roberts. Learningbehaviorsviahuman-delivereddiscretefeedback: modelingimplicitfeedbackstrategiesto
speeduplearning. Autonomousagentsandmulti-agentsystems,30(1):30–59,2016.
[89] JamesMacGlashan,MarkKHo,RobertLoftin,BeiPeng,DavidRoberts,MatthewETaylor,andMichaelL
Littman. Interactivelearningfrompolicy-dependenthumanfeedback. arXivpreprintarXiv:1701.06049,2017.
[90] AndreaLockerdThomaz,GuyHoffman,andCynthiaBreazeal. Real-timeinteractivereinforcementlearningfor
robots. InAAAI2005workshoponhumancomprehensiblemachinelearning,2005.
[91] AndreaLThomazandCynthiaBreazeal. Teachablerobots: Understandinghumanteachingbehaviortobuild
moreeffectiverobotlearners. ArtificialIntelligence,172(6-7):716–737,2008.
[92] GarrettWarnell,NicholasWaytowich,VernonLawhern,andPeterStone. Deeptamer: Interactiveagentshaping
inhigh-dimensionalstatespaces. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume32,
2018.
[93] RichardMVoylesandPradeepKKhosla. Gesture-basedprogramming: Apreliminarydemonstration. InPro-
ceedings1999IEEEInternationalConferenceonRoboticsandAutomation(Cat.No.99CH36288C),volume1,
pages708–713.IEEE,1999.
[94] GuangliangLi,HamdiDibekliog˘lu,ShimonWhiteson,andHayleyHung. Facialfeedbackforreinforcement
learning: acasestudyandofflineanalysisusingthetamerframework. AutonomousAgentsandMulti-Agent
Systems,34(1):1–29,2020.
[95] Sandra Clara Gadanho. Learning behavior-selection by emotions and cognition in a multi-goal robot task.
JournalofMachineLearningResearch,4(Jul):385–412,2003.
[96] RikuArakawa,SosukeKobayashi,YuyaUnno,YutaTsuboi,andShin-ichiMaeda. Dqn-tamer: Human-in-the-
loopreinforcementlearningwithintractablefeedback. arXivpreprintarXiv:1810.11748,2018.
[97] GorenGordon,SamuelSpaulding,JacquelineKoryWestlund,JinJooLee,LukePlummer,MaraynaMartinez,
MadhurimaDas,andCynthiaBreazeal. Affectivepersonalizationofasocialrobottutorforchildren’ssecond
languageskills. InProceedingsoftheThirtiethAAAIConferenceonArtificialIntelligence,pages3951–3957,
2016.
[98] PrasoonGoyal,ScottNiekum,andRaymondJMooney. Usingnaturallanguageforrewardshapinginreinforce-
mentlearning. arXivpreprintarXiv:1903.02020,2019.
[99] Richard Maclin and Jude W Shavlik. Creating advice-taking reinforcement learners. Machine Learning,
22(1-3):251–281,1996.
23RunningTitleforHeader
[100] GregoryKuhlmann,PeterStone,RaymondMooney,andJudeShavlik. Guidingareinforcementlearnerwith
naturallanguageadvice: Initialresultsinrobocupsoccer. InTheAAAI-2004workshoponsupervisorycontrolof
learningandadaptivesystems.SanJose,CA,2004.
[101] EdwardCWilliams,NakulGopalan,MineRhee,andStefanieTellex. Learningtoparsenaturallanguageto
groundedrewardfunctionswithweaksupervision. In2018IEEEInternationalConferenceonRoboticsand
Automation(ICRA),pages1–7.IEEE,2018.
[102] FrancisQuek,DavidMcNeill,RobertBryll,SusanDuncan,Xin-FengMa,CemilKirbas,KarlEMcCullough,
andRashidAnsari. Multimodalhumandiscourse: gestureandspeech. ACMTransactionsonComputer-Human
Interaction(TOCHI),9(3):171–193,2002.
[103] KlausWeber,HannesRitschel,FlorianLingenfelser,andElisabethAndré. Real-timeadaptationofaroboticjoke
tellerbasedonhumansocialsignals. 2018.
[104] CharlesIsbell,ChristianRShelton,MichaelKearns,SatinderSingh,andPeterStone. Asocialreinforcement
learningagent. InProceedingsofthefifthinternationalconferenceonAutonomousagents,pages377–384,2001.
[105] PascalLessel,MaximilianAltmeyer,LeaVerenaSchmeer,andAntonioKrüger."enableordisablegamification?"
analyzingtheimpactofchoiceinagamifiedimagetaggingtask. InProceedingsofthe2019CHIConferenceon
HumanFactorsinComputingSystems,pages1–12,2019.
[106] Mark K Ho, Michael L Littman, Fiery Cushman, and Joseph L Austerweil. Teaching with rewards and
punishments: Reinforcementorcommunication? InCogSci,2015.
[107] AndreaLThomaz,GuyHoffman,andCynthiaBreazeal. Reinforcementlearningwithhumanteachers: Under-
standinghowpeoplewanttoteachrobots. InROMAN2006-The15thIEEEInternationalSymposiumonRobot
andHumanInteractiveCommunication,pages352–357.IEEE,2006.
[108] Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from
demonstration. Roboticsandautonomoussystems,57(5):469–483,2009.
[109] ShaneGriffith,KaushikSubramanian,JonathanScholz,CharlesLIsbell,andAndreaLThomaz. Policyshaping:
Integratinghumanfeedbackwithreinforcementlearning. InAdvancesinneuralinformationprocessingsystems,
pages2625–2633,2013.
[110] Matthew E Taylor, Halit Bener Suay, and Sonia Chernova. Integrating reinforcement learning with human
demonstrationsofvaryingability. InThe10thInternationalConferenceonAutonomousAgentsandMultiagent
Systems-Volume2,pages617–624,2011.
[111] MaoLi,TimBrys,andDanielKudenko. Introspectivereinforcementlearningandlearningfromdemonstration.
InAAMAS,pages1992–1994,2018.
[112] AndreaLThomazandCynthiaBreazeal. Addingguidancetointeractivereinforcementlearning. InProceedings
oftheTwentiethConferenceonArtificialIntelligence(AAAI),2006.
[113] RudolfFerenc,ArpadBeszedes,LajosFulop,andJanosLele. Designpatternminingenhancedbymachine
learning. In21stIEEEInternationalConferenceonSoftwareMaintenance(ICSM’05),pages295–304.IEEE,
2005.
[114] WilliamYangWang,JiweiLi,andXiaodongHe. Deepreinforcementlearningfornlp. InProceedingsofthe
56thAnnualMeetingoftheAssociationforComputationalLinguistics: TutorialAbstracts,pages19–21,2018.
[115] AndreasHolzinger. Frommachinelearningtoexplainableai. In2018worldsymposiumondigitalintelligence
forsystemsandmachines(DISA),pages55–66.IEEE,2018.
[116] ZhaoxingLi,LeiShi,AlexandraCristea,YunzhanZhou,ChenghaoXiao,andZiqiPan. Simstu-transformer: A
transformer-basedapproachtosimulatingstudentbehaviour.InInternationalConferenceonArtificialIntelligence
inEducation,pages348–351.Springer,2022.
[117] ZhaoxingLi,MarkJacobsen,LeiShi,YunzhanZhou,andJindiWang. Broaderanddeeper: Amulti-features
withlatentrelationsbertknowledgetracingmodel. InEuropeanConferenceonTechnologyEnhancedLearning,
pages183–197.Springer,2023.
[118] ZhaoxingLi.DeepReinforcementLearningApproachesforTechnologyEnhancedLearning.PhDthesis,Durham
University,2023.
[119] ZhaoxingLi,LeiShi,JindiWang,AlexandraICristea,andYunzhanZhou. Sim-gail: Agenerativeadversarial
imitation learning approach of student modelling for intelligent tutoring systems. Neural Computing and
Applications,35(34):24369–24388,2023.
24RunningTitleforHeader
[120] Zhaoxing Li, Lei Shi, Yunzhan Zhou, and Jindi Wang. Towards student behaviour simulation: a decision
transformer based approach. In International Conference on Intelligent Tutoring Systems, pages 553–562.
Springer,2023.
[121] ZhaoxingLi, JujieYang, JindiWang, JiayiFeng, SebastianStein, etal. Lbkt: alstmbert-basedknowledge
tracingmodelforlong-sequencedata. 2024.
[122] GeWang. Humansintheloop: Thedesignofinteractiveaisystems,2020.
[123] JindiWang,IoannisIvrissimtzis,ZhaoxingLi,YunzhanZhou,andLeiShi. Exploringthepotentialofimmersive
virtualenvironmentsforlearningamericansignlanguage. InEuropeanConferenceonTechnologyEnhanced
Learning,pages459–474.Springer,2023.
[124] DeirdreWilsonandDanSperber. Ongrice’stheoryofconversation. Conversationanddiscourse,pages155–78,
1981.
[125] JindiWang,IoannisIvrissimtzis,ZhaoxingLi,YunzhanZhou,andLeiShi. User-definedhandgestureinterface
to improve user experience of learning american sign language. In International Conference on Intelligent
TutoringSystems,pages479–490.Springer,2023.
[126] LiDeng. Themnistdatabaseofhandwrittendigitimagesformachinelearningresearch[bestoftheweb]. IEEE
SignalProcessingMagazine,29(6):141–142,2012.
[127] H Clark Barrett. Deciding what to observe: Thoughts for a post-weird generation. Evolution and Human
Behavior,41(5):445–453,2020.
[128] JindiWang,IoannisIvrissimtzis,ZhaoxingLi,andLeiShi. Comparativeefficacyof2dand3dvirtualreality
games in american sign language learning. In The 31st IEEE Conference on Virtual Reality and 3D User
Interfaces.NewcastleUniversity,2024.
[129] JindiWang,IoannisIvrissimtzis,ZhaoxingLi,andLeiShi. Impactofpersonalisedaichatassistantonmediated
human-humantextualconversations: Exploringfemale-maledifferences. InCompanionProceedingsofthe29th
InternationalConferenceonIntelligentUserInterfaces,pages78–83,2024.
[130] JeffreySRosenscheinandGiladZlotkin. Rulesofencounter: designingconventionsforautomatednegotiation
amongcomputers. MITpress,1994.
[131] WilliamBradleyKnox. Learningfromhuman-generatedreward. 2012.
[132] ChristianArzateCruzandTakeoIgarashi. Interactivereinforcementlearningforautonomousbehaviordesign.
InArtificialIntelligenceforHumanComputerInteraction: AModernApproach,pages345–375.Springer,2021.
[133] ArielRosenfeld,MosheCohen,MatthewETaylor,andSaritKraus. Leveraginghumanknowledgeintabular
reinforcementlearning: Astudyofhumansubjects. TheKnowledgeEngineeringReview,33,2018.
[134] ReinaldoACBianchi,MuriloFMartins,CarlosHCRibeiro,andAnnaHRCosta. Heuristically-accelerated
multiagentreinforcementlearning. IEEEtransactionsoncybernetics,44(2):252–265,2013.
[135] SörenMindermann,RohinShah,AdamGleave,andDylanHadfield-Menell. Activeinverserewarddesign. arXiv
preprintarXiv:1809.03060,2018.
[136] DylanHadfield-Menell,SmithaMilli,PieterAbbeel,StuartRussell,andAncaDragan. Inverserewarddesign.
arXivpreprintarXiv:1711.02827,2017.
[137] SamanthaKreningandKarenMFeigh. Interactionalgorithmeffectonhumanexperiencewithreinforcement
learning. ACMTransactionsonHuman-RobotInteraction(THRI),7(2):1–22,2018.
[138] HalitBenerSuayandSoniaChernova.Effectofhumanguidanceandstatespacesizeoninteractivereinforcement
learning. In2011Ro-Man,pages1–6.IEEE,2011.
[139] ChaoYu,TianpeiYang,WenxuanZhu,GuangliangLi,etal. Learningshapingstrategiesinhuman-in-the-loop
interactivereinforcementlearning. arXivpreprintarXiv:1811.04272,2018.
[140] DJStrouse,KevinRMcKee,MattBotvinick,EdwardHughes,andRichardEverett. Collaboratingwithhumans
withouthumandata. arXivpreprintarXiv:2110.08176,2021.
[141] YangLiu,ShaonanWang,JiajunZhang,andChengqingZong.Experience-basedcausalitylearningforintelligent
agents. ACMTransactionsonAsianandLow-ResourceLanguageInformationProcessing(TALLIP),18(4):1–22,
2019.
[142] SamanthaKreningandKarenMFeigh. Effectofinteractiondesignonthehumanexperiencewithinteractive
reinforcement learning. In Proceedings of the 2019 on Designing Interactive Systems Conference, pages
1089–1100,2019.
25RunningTitleforHeader
[143] Dan Amir and Ofra Amir. Highlights: Summarizing agent behavior to people. In Proceedings of the 17th
InternationalConferenceonAutonomousAgentsandMultiAgentSystems,pages1168–1176,2018.
[144] PatrikkDSørensen,JeppehMOlsen,andSebastianRisi. Breedingadiversityofsupermariobehaviorsthrough
interactiveevolution. In2016IEEEConferenceonComputationalIntelligenceandGames(CIG),pages1–7.
IEEE,2016.
[145] Aaron Wilson, Alan Fern, and Prasad Tadepalli. A bayesian approach for policy learning from trajectory
preferencequeries. Advancesinneuralinformationprocessingsystems,25:1133–1141,2012.
26