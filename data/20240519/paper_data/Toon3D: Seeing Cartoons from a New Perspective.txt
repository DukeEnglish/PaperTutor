Toon3D: Seeing Cartoons from a New Perspective
Ethan Weber∗2, Riley Peterlinz∗2, Rohan Mathur2,
Frederik Warburg1, Alexei A. Efros2, and Angjoo Kanazawa2
1Teton.ai 2UC Berkeley
Hand-drawn images Toon3D Labeler Densely aligned point cloud and cameras Results after alignment
Correspondences Gaussian Splatting
Transient masks
Warped perspective images
Predicted depth
Warped images
Fig.1: Reconstructing geometrically inconsistent scenes. Toon3D takes a set
of geometrically inconsistent images with correspondences and predicted depth maps
and creates a consistent 3D model. This is achieved by warping the images in 2D and
solvingforcameraparametersviaaligningbackprojectedpointclouds.Theimagesare
warped to fit a perspective projection. With Toon3D, we can visualize cartoons from
novel views and analyze geometrical inconsistencies in the drawings.
Abstract. Inthiswork,werecovertheunderlying3Dstructureofnon-
geometrically consistent scenes. We focus our analysis on hand-drawn
images from cartoons and anime. Many cartoons are created by artists
without a 3D rendering engine, which means that any new image of a
sceneis hand-drawn.The hand-drawn imagesare usuallyfaithful repre-
sentations of the world, but only in a qualitative sense, since it is diffi-
cult for humans to draw multiple perspectives of an object or scene 3D
consistently. Nevertheless, people can easily perceive 3D scenes from in-
consistentinputs!Inthiswork,wecorrectfor2Ddrawinginconsistencies
torecoveraplausible3Dstructuresuchthatthenewlywarpeddrawings
are consistent with each other. Our pipeline consists of a user-friendly
annotation tool, camera pose estimation, and image deformation to re-
coveradensestructure.Ourmethodwarpsimagestoobeyaperspective
cameramodel,enablingouralignedresultstobepluggedintonovel-view
synthesisreconstructionmethodstoexperiencecartoonsfromviewpoints
never drawn before. Our project page is https://toon3d.studio/.
Keywords: 3D Reconstruction · Non-geometric · Deformation
1 Introduction
Humans typically have little trouble inferring the relative camera poses and 3D
structure from hand-drawn cartoons. However, current structure-from-motion
* Equal contribution
4202
yaM
61
]VC.sc[
1v02301.5042:viXra2 E. Weber et al.
Rick and Morty House Bob’s Burgers Restaurant
Fig.2: Geometrical inconsistencies in cartoons. Are these white arrows consis-
tent?Itisincrediblydifficulttotellasahuman,butCOLMAPandSfMpipelinesfail
on these images, even with our hand-labeled correspondences.
(SfM) pipelines fail to reconstruct these scenes because (1) the images are not
geometricallyconsistent,(2)theimagesdonotobeyphysicallyplausiblecamera
models, and (3) the scenes are typically only drawn from a sparse set of views
as shown in Fig. 2. In this work, we overcome these challenges by proposing a
non-rigid piecewise-rigid deformable optimization framework (see Fig. 1) that
recovers camera poses and dense structure from non-geometrically consistent
images.Wereconstructaplausible3Dstructureforhand-drawnscenes,andthe
deformed input images obey a perspective camera model.
Our pipeline consists of three main steps as follows: sparse alignment, dense
alignment,and3DGaussianrefinement.1)Sparsealignment.Ourpipelinetakes
asetofcorrespondencesasinput,whichwebackprojectinto3Dusingthedepth
from a monodepth network [19]. We align these sparse correspondences in 3D
to coarsely estimate the camera intrinsic and extrinsic parameters. 2) Dense
alignment. Starting from this camera configuration, we initialize a 3D mesh at
the sparse correspondences and deform its vertices until achieving near-perfect
alignment at the 3D correspondences. We regularize our warps with 2D and 3D
rigidity losses to prevent degenerate solutions. Finally, we can use our aligned
meshwithbarycentricinterpolationtodenselywarptheRGBimagesin2Dand
thedepthmapin3D.3) Gaussian refinement.Weuseourdenselyalignedpoint
cloud to initialize 3D Gaussian Splatting [20] to enable a more immersive visual
experience than point clouds or meshes.
WealsoproposetheToon3DLabelerwhichisauser-friendlyannotationtool,
where a user can label point correspondences between images while segmenting
transient objects. The Toon3D Labeler is a hosted website with no installa-
tion,soanyonecangetupandrunningwithiteasily.Weintentionallyhighlight
Toon3DLabelerasacontributionofourpaperbecauseartistsworkwithcartoon
drawings regularly, and this tool fits nicely into a human-in-the-loop framework
forrecovering3Dfromthesedrawings.Ourrecovered3Dmodelmayhelpartists
draw novel viewpoints. We use our labeler to label 12 scenes from popular car-
toonsandanime,suchasSpongeBob(Fig.1)andSpiritedAway,andwerelease
these as the Toon3D Dataset.Toon3D: Seeing Cartoons from a New Perspective 3
To the best of our knowledge, we are the first to present a pipeline for re-
constructing cartoon or hand-drawn scenes. Our pipeline yields reliable camera
poses,whereas COLMAP[28]fails toestimate cameraposes(even withhuman-
annotatedcorrespondences)duetogeometricalinconsistencies.Furthermore,the
2Dimagewarpingsoftheoriginalimagesenableustobetterreconstructthefull
3D geometry, while also revealing geometrical inconsistencies in the drawings.
We evaluate our pipeline on 12 popular scenes (10 cartoon TV shows, 1
movie) to highlight the effectiveness of our pipeline in obtaining good camera
poses and reconstructions. We show reconstructions of our recovered 3D point
clouds and renders of our estimated 3D Gaussian Splatting [20] representation.
We evaluate our proposed alignment steps and losses qualitatively and quanti-
tatively.Wedemonstratethatourwarpscanhighlightgeometricinconsistencies
inhand-drawnimages.Tofurthervalidatethequalityofourestimatedposes,we
show that we can obtain the 3D geometry of Airbnb rooms with sparse views.
Finally, we show that Toon3D is also useful for reconstructing the 3D geometry
from paintings depicting the same landmark from different views.
Toon3D is a step toward achieving a qualitative 3D understanding of car-
toons. Humans can perceive scenes that are not necessarily 3D consistent but
current algorithms cannot process such imagery due to non-perspective or non-
consistentdrawings.Wevalidateourpipelineandwillreleasealldata,code,and
tools for easily processing any cartoon. We hope our contribution serves as a
useful framework to build upon for 3D cartoon understanding. Please see our
project page https://toon3d.studio/ for additional video results.
2 Related work
Multi-viewgeometryestimation.Structure-from-Motion(SfM)[12,30]takes
in images, detects and matches correspondences, and solves for camera param-
eters. COLMAP [28] is a popular SfM pipeline, but it fails for wide baseline
images(fewcorrespondences),imageswithalotofmovingobjects,orgeometric
inconsistencies typically present in cartoons. Improvements in keypoint detec-
tion[9,10],matching[27,32]andoptimizations[35]havebeenproposedtobetter
handlewidebaselines[37]andberobusttotransientobjects[3],however,weare
thefirsttoadaptanSfMmethodtonon-geometricalscenes.Ourpipelineallows
the user to annotate reliable correspondences and transient regions, while our
novelperimagewarpinghandlesthenon-geometricaldeformationsoftenseenin
cartoons.
Afterrecoveringcameraparameters,multi-viewstereo(MVS)methods,such
as NeRF [22] or Gaussian Splatting [20], can be used to create dense 3D recon-
structions.MostMVSmethodsdegradewhenthesceneisonlyseenfromasparse
setofviews.Manyworks[23,39–41]incorporatepriorstoreduceblurryartifacts.
Inourpipeline,weusemonodepthprior[19]toobtainapointcloudthatweuse
to initialize our Gaussian Splatting representation.
Reconstructingimagecollections.Facade[7],aseminalearlyworkinimage-
based modeling and rendering, used a set of photographs of an architectural4 E. Weber et al.
Points Masks and depth
Fig.3: Toon3D Labeler. Here are some screenshots from our Toon3D Labeler. We
can label points and masks, and we can visualize any set of these and the depth map.
This is a general labeling tool but especially useful for obtaining the inputs required
for Toon3D.
scene to recover a textured 3D model using structure-from-motion with human-
specified volumetric constraints. Phototourism [30] and Building Rome in a
Day[1]pioneeredtheuseoflargeonlinephotocollectionsfor3Dreconstruction.
Object-centric methods like CMR [17,18] recover 3D models of animals which
can explain each image with a deformation. For non-rigid dynamic scenes, there
exist methods like Nerfies [24] which explain small variations in a video via a
3D model with a time-conditioned warp field. With methods that require defor-
mation, techniques such as ARAP [31] are useful. Cartoons, in contrast, have
multiple images depicting the same instance but are not geometrically consis-
tent [34]. This is a different and under-explored problem setting.
Paintings to 3D.Mostattemptsatrecovering3Dfromdrawingsandpaintings
have focused on the single view setting, with missing 3D information provided
either manually by the user or via learning. Important early user-assisted ap-
proaches for generating 3D scenes from a single painting include Tour into the
Picture[14],whichassumedsingle-pointperspective,andthemoregeneralSingle
View Metrology [5]. Automatic Photo Popup [13] replaced the manual parts of
thereconstructionprocesswithearlymachinelearningtechniques,andwasable
to generalize to paintings. Aubry et al. [2] is a rare attempt to connect different
paintings of the same scene by using a 3D model. There has also been a few
attempts to recover a 3D model from a set of sketches of the same object [8,11].
Computer vision in TV and Film. SfM and NeRF has been used to recon-
structandanalyzesitcomTVshows:Pavlakoset al.[25]recovercamerashotlo-
cations,3Dposesofhumans,andprovidegazeunderstanding,enablingapplica-
tionssuchaspost-productionre-renderingwithnovelcamerapaths.MovieNet[15]
proposes a large dataset of popular films annotated with bounding boxes, ac-
tions, and cinematic style for a holistic understanding of movies. Zhu et al. [43]
proposes to align movies and books to obtain fine-grained descriptions of ap-
pearancesofobjectsandcharactersaswellashigh-levelsemanticunderstanding
intohowcharactersthinkandreason.Additionally,someworkshavelookedinto
character reconstruction for cartoon characters [4,16,29] but none have looked
at recovering camera poses and reconstructing full 3D environments. Our work
is most similar to [25], but we use hand-drawn cartoons with inconsistencies
instead of real sitcom frames.Toon3D: Seeing Cartoons from a New Perspective 5
3 Toon3D Labeler and Dataset
We present the Toon3D Labeler and Toon3D Dataset, which consists of 12 car-
toonscenes(10TVshows,1movie)and78labeledimages.Tab.1(leftcolumn)
lists our cartoon scenes. Associated with the dataset, we highlight the Toon3D
Labeler(Fig.3),whichwedeveloptocreatethedataset.TheToon3DLabelerisa
hostedwebsitewithnoinstallation,soanyonecanaccessanduseit.Itfitsnicely
into a human-in-the-loop framework for recovering 3D from these drawings.
3.1 Data processing and labeling
Preprocessing. We collect a set of N images denoted by I = {I |I ,...,I }.
i 1 N
EachscenetypicallyhasN ≤10imageswithwidebaselines.Wepreprocessthese
images by running a monocular depth network to obtain predicted depths D =
{D |I ,...,I }. We normalize our depth maps by dividing it by the maximum
i 1 N
value across all depth maps. We use Marigold [19] for the depth prediction. We
also run Segment Anything (SAM) [21] to get a set of masks per image.
Labeling. We create an annotation tool, Toon3D Labeler, designed to take the
preprocessed data and display it in our custom web viewer for labeling points
andselectingSAMmasksastransientregionstodiscard.Wealsohavetheability
to visualize the depth map in the viewer to make sure to avoid clicking points
directly on depth discontinuities. To select SAM mask, the user simply hovers
over a region, the mask will be highlighted, and it can be toggled on and off
to discard those transient pixels. Also, our annotation tool is useful for scenes
whereCOLMAPfailsandonewantshand-labeledcorrespondencese.g.Sec.5.4.
At the end of labeling, we obtain a set of pixel correspondences X = {x }
i,c
where i is the image index and c is the correspondence index. We also have a
valid correspondences mask m = {0,1}. When m = 0, the correspondence
i,c i,c
is not visible from that viewpoint. We denote the predicted depth of the corre-
spondences with d =D (x ).
i,c i i,c
4 Method
Our method, Toon3D, consumes point correspondences and mask annotations
and then performs the steps of sparse alignment followed by dense alignment to
outputcameraposes,a3Dpointcloud,andwarpedimagesthatobeyaperspec-
tive camera model. We load this point cloud into Gaussian Splatting to create
a more immersive novel-view experience and visualize the warps to highlight
geometrical inconsistencies in the input images. Fig. 4 shows an overview of our
framework.
4.1 Sparse alignment
Thefirststepofourpipelineobtainsaninitialsetofcameraposes.Simplyusing
our human annotated correspondences with COLMAP fails due to the geomet-
ric inconsistencies in cartoon drawings. Instead, we propose to backproject the
correspondences into 3D with a predicted depth to create a point cloud P and
i6 E. Weber et al.
Toon3D Dataset Toon3D Method
Depth estimation & segmentation Sparse alignment Gaussian refinement
(Sec 3.1) (Sec 4.1) (Sec 4.3)
{depths, masks} {f,R,t} {point cloud, warped images}
Label correspondences & transients Dense alignment
(Sec 3.1) (Sec 4.2)
Gaussian Splatting
Warped images
Dense point
Perspective cloud
cameras
Fig.4:Pipelineoverview.Ourframeworkconsistsoflabelingimageswithourinter-
activeToon3DLabelertool,recoveringcameraposesandaligningadensepointcloud,
andvisualizingthedensereconstructionwithGaussianstocreateanimmersivevisual
experience.
solve for camera rotations R, translations t, focal length f, depth scale s, and
shift h that maximize alignment. More precisely, points are backprojected as
follows:
p(x )=R ·K−1·(s ·d +h ). (1)
i,c i i i i,c i
Our main loss pulls together correspondences in 3D
N N M
1 (cid:88)(cid:88)(cid:88)
L = m ·||p(x )−p(x )||2. (2)
3D |X| i,c i,c j,c 2
i=1 j<ic=1
In addition, we apply the following losses for regularization
N N N
L =||1− 1 (cid:88) s ||2, L =(cid:88) ||1− f i,x||2, L =(cid:88) f +f , (3)
scale N i aspect f focal i,x i,y
i,y
i=1 i=1 i=1
whereL encouragesascaleclosesto1suchthatthescenedoesnotshrink,
scale
L balances f and f to maintain aspect ratio of the camera with the
aspect i,x i,y
original image, and L penalizes large focal length to prefer wide-angle cam-
focal
eras over far away and zoomed in shots. We also have losses that penalize scales
s andshiftsh iftheybecomenegativewithL (x)=||1 (cid:80)N max(0,−x )||2.
i i neg N i=1 i
Our sparse alignment objective is as follows:
argminJ = L +λ L +λ L +λ L
sparse 3D scale scale aspect aspect focal focal
R,t,f,s,h (4)
+λ (L (s)+L (h))
neg neg neg
4.2 Dense alignment
With a coarse estimate of the camera poses recovered, we refine the sparse key-
points and densely align the point clouds with 2D image warping and 3D depthToon3D: Seeing Cartoons from a New Perspective 7
Initial cameras Initial Sparse alignment Dense alignment Recovered point cloud
and point clouds correspondences
Fig.5: Sparse & Dense Alignment Steps. The sparse alignment stage coarsely
aligns the point clouds while optimizing for camera intrinsics and extrinsics. Dense
alignment refines these estimates while also warping the images to obey a perspective
cameramodel.Aftertherefinementstages,weobtainapointcloudandposedimages.
warping. This entails warping the images in 2D to better align the sparse corre-
spondences and adjusting the dense depth to better align to the sparse points.
Todothis,wefirstturneachtrainingimageandpredicteddepthintoa3Dmesh
with vertices V ∈ RM×3 and faces F ∈ RK×3. We then apply a piecewise-rigid
deformationtothismesh,whereV istheinitial2DpointforimageiandV
i,xy i,z
is the initial depth. We optimize the V of each image with various 2D and 3D
regularizerstoconstrainthewarps.Wedothiswhilerepeatingtheoptimization
from Sec. 4.1 but now more freedom is allowed to further minimize L and
3D
find a solution, shown in Fig. 5 where the colored star points converge in dense
alignment.
Image vertices and topology. We turn each image into vertices and faces,
which represent the 2D warp and depth offset. We do this by first marking all
labeledcorrespondencesx asvertices.WeuseDelaunaytriangulationtocreate
i,c
the mesh. Selecting vertices near depth boundaries results in meshes with edges
thatlieonnaturaledges.Thishelpsmakethewarpslookmorenaturalsinceour
warpingisappliedtoupdatetheimagecoloranddepthwithdenseinterpolation.
See Fig. 6 for a depiction of our topology and how it looks in 3D.
Rigidity regularizers. We regularize the warps to prevent degenerate solu-
tions.Ourlossesseektomaintainagoodstructurein2Dwhilepullingthedense
3Dstructureintoplace.Weproposethreemainlosses.L [17,31]encour-
ARAP2D
ages the triangles to maintain a similar orientation and spacing to their starting
configuration. L penalizes if the triangle face gets too small or flips. L en-
flip z
courages the warped depth to be similar to the original predicted depth. More
specifically,
N
L = 1 (cid:88) (cid:88) ||V [f]−A V′ [f]||2, (5)
ARAP2D N ×|F| i i→j i
i=1f∈Fi
N
1 (cid:88) (cid:88)
L = ||min(0,t −det(V [f]))||2, (6)
flip N ×|F| area i
i=1f∈Fi
N M
1 (cid:88)(cid:88)
L = m ·||d′ −d ||, (7)
z N ×|X| i,c i,c i,c
i=1c=18 E. Weber et al.
Image and depth map Mesh connected at labeled points Mesh in 3D Backprojected
point cloud
Fig.6: Image mesh topology. We start with an image and predicted depth map
(left). Then, we create a mesh with the 2D correspondences to define the topology
(middle left). This mesh lives in 3D, where larger diamonds are closer to the camera
(middle right). We can query the RGB and depth map to update the dense 3D point
cloud using barycentric interpolation on this mesh.
where A ∈ R2×3 is the best fit 2D rigid transformation in the image plane
a→b
thattransformsverticesfromfaceatofaceb,V [f]∈R3×3istheverticesindexed
i
at face f, t is the minimum area a face can be, and det gives the signed face
area
area.Wesett to10%oftheoriginalfacearea.Ourdensealignmentobjective
area
is as follows:
argmin J = J +λ L +λ L +λ L
dense sparse ARAP2D ARAP2D flip flip z z (8)
R,t,f,s,h,V
Dense interpolation. Finally, we use barycentric interpolation to warp the
RGBanddepthmapsaccordingtoourdeformedverticesV′.WewarptheRGB
image with barycentric interpolation according to the original vertices V and
the deformed mesh V′. Similarly, we compute a depth offset and apply it to the
originaldepthimagesd toobtaind′.Theresultingoutputisanimageanddepth
i i
map that obey the perspective camera model as well as the global 3D geometry.
4.3 Gaussian refinement
At this point, we have aligned depth maps which are backprojected into a com-
bined 3D point cloud. We could visualize the point cloud as-is, but we find that
Gaussian Splatting can create a more immersive experience. Gaussian Splat-
ting [20] is typically initialized by a sparse point cloud from COLMAP, but
instead, we initialize it with our dense point cloud. We add a few sparse-view
regularizersincludingtherankinglossfrom[38](toreconstructscenestobecon-
sistent with the predicted depth) and a total variation [42] loss in novel views
interpolated between pairs of training views.
5 Experiments
5.1 Cartoon reconstruction
In Fig. 7 we show the results from our pipeline on multiple popular cartoon
scenes. In the left column, we show the point cloud and some of the posedToon3D: Seeing Cartoons from a New Perspective 9
Krusty Krab
Bob’s Burgers
Spirited Away
Mystery Machine
Planet Express
Rick and Morty
Fig.7: Our 3D reconstructions of cartoons. We recover camera intrinsics and
poses,reconstructadenselyalignedpointcloud,andoptimizetheresultwithGaussian
Splattingtocreateamoreimmersiveview.FortheKrustyKrabSpongeBobscene(top),
we label point correspondences between walls to reconstruct two rooms together.10 E. Weber et al.
Original Difference Warped
Fig.8: Visualizing inconsistencies.Weshowthemostinconsistentregionsinafew
imagesfromdifferentscenesbyoverlayingtheoriginalimage(left)ontopofthewarped
image (right) to construct a difference image (middle). More blurry regions indicate
where the images warped more to achieve 3D perspective consistency.
images found after the dense alignment stage. We have removed the labeled
transient regions from the final point clouds and do not train on these regions
during Gaussian refinement. The remaining columns show rendered novel views
after the Gaussian refinement stage. We encourage the reader to see our project
page for video results. From start to completion, our method takes on the order
of minutes. Finding a few images of a cartoon scene and labeling points is quick
due to the web-based viewer (Fig. 3), and running our camera alignment and
warping takes approximately 1 minute on an NVIDIA RTX A5000. Running
Gaussian Splatting with the Nerfstudio [33] implementation and our additional
losses takes ∼3 minutes, where we train for just 2K iterations to obtain sharp
results.
5.2 Visualizing inconsistencies
One unique aspect of Toon3D is that we keep the original images around rather
than discarding them. They are warped in 2D to obey the global 3D geometry
andaperspectivecameramodel.InFig.8weshowwheretheimagesdeformthe
most to create a unified consistent 3D structure. This is fundamentally different
thanalternativesparse-viewgenerativemethods,e.g.Dreambooth3D[26]which
suB
loohcS
cigaM
sregruB
s’boBToon3D: Seeing Cartoons from a New Perspective 11
No scale and shift + no aspect reg. Scale and shift + no aspect reg. Scale and shift
+ aspect reg.
Improves aspect ratio
Warp no reg. Warp w/ ARAP Warp w/ z reg. Warp
w/ ARAP and z reg.
Scale and shift Scale and shift Scale and shift
+ aspect reg. + focal reg. + aspect + focal reg.
Cameras outside room Cameras within room
Fig.9: 3D alignment ablations. Row 1 (Rick and Morty House) shows regular-
ization’s impact on scene shaping. Optimized shift and scale parameters can adjust
point clouds to better align at correspondences. This is evident as the starred points
converge. The aspect regularization keeps the optimized image close to its original
aspect ratio. Row 2 (BoJack Horseman House) explores the effects of different warp
regularizers(L andL )onscenewarping.Withoutanyregularization,warping
ARAP2D z
distortsscenegeometry.ARAPaloneresultsinpoor3Dwarpsduetoinaccuratedepth.
z regularizationalonelimitsscenemovement,maintainingrigidstructuresclosetothe
original depth map. Using both strikes a good balance between correctly positioning
geometryandpreservingstructuralintegrity.Finally,Row3(AvatarHouse)highlights
how focal regularization keeps cameras inside the building room rather than outside
and zoomed in.
fine-tunesonacollectionofimagesandthenhallucinatesascene.Havingaccess
to such warp field is useful in artists workflows and might help us understand
howhumansperceive3Dfrominconsistent2Ddrawings.Additionally,itprovides
insights into the artistic techniques used to convey 3D or to emphasize regions
in drawings without strictly adhering to physical laws.
5.3 3D alignment ablations
Inthissection,weexperimentwithdifferentdegreesoffreedomduringoptimiza-
tion and with different regularizations turned on during optimization. We show
qualitative results in Fig. 9 and quantitative results in Tab. 1.
esuoH
ytroM
dna
kciR
esuoH
kcaJoB
esuoH
eS
gniS
aB
ratavA12 E. Weber et al.
Table 1: Quantitative ablations. Here we compare various regularizers for our
method. We report L on 5 correspondence points that we randomly remove from
3D
every labeled image. See Sec. 5.3 for details on the ablation names. W(A/Z) is our
method with all camera parameters freely optimized and using all proposed losses. In
this table, we multiply L by 100 for better readability.
3D
Scene C C(S)C(S/A)C(S/F)C(S/A/F) W W(A)W(Z)W(A/Z)
Avatar House 4.39 6.22 5.47 6.40 5.50 8.79 7.00 4.50 3.38
Bob’s Burgers 4.14 3.77 3.80 3.78 3.81 3.56 4.58 4.00 3.86
BoJack Room 13.4 21.2 33.8 20.6 34.7 16.1 15.3 12.4 11.6
Fam. Guy Dining 2.72 2.52 2.80 2.54 2.81 4.84 4.90 2.45 2.50
Fam. Guy House 3.54 3.92 3.18 3.99 3.21 3.51 3.53 2.93 2.86
Krusty Krab 20.1 10.3 8.47 10.6 8.69 14.9 13.9 9.38 7.73
Magic School Bus3.21 2.46 2.54 2.45 2.55 3.48 3.15 2.60 2.25
Mystery Machine 9.99 18.1 17.0 17.7 16.3 12.4 12.7 10.6 10.8
Planet Express 4.42 3.43 3.15 3.45 3.17 3.72 3.53 4.14 3.54
Simpsons House 1.04 1.03 1.02 1.04 1.03 0.91 1.17 0.70 0.42
Rick and Morty 0.90 0.78 0.77 0.79 0.78 2.31 2.51 0.64 0.53
Spirited Away 8.01 9.37 6.31 9.41 6.37 5.13 4.67 5.51 5.25
Average 6.32 6.93 7.36 6.90 7.40 6.64 6.41 4.98 4.56
Qualitative results. For our default method, we have all parameters free (in-
cluding scale and shift) with all regularization losses turned on. We show the
qualitativetrade-offsforourvariouslossesinFig.9.Wefindthatourlosseshelp
align structure while maintain an accurate aspect ratio, preventing degenerate
warps, and favoring cameras inside walls rather than far away and zoomed in.
Please see the caption for more details.
Quantitative results. Our task is most naturally evaluated qualitatively, but
tobethorough,wedesignasimplemetrictoevaluate3Dconsistency,withresults
reported in Tab. 1. We randomly select and remove 5 labeled correspondence
points from each image of our 12 scenes in our Toon3D Dataset. We then run
ourmethodswithvariousparametersandregularizationsturnedonandoff,indi-
catedbylettersinthetopofTab.1.WereportL onthesecorrespondencesto
3D
measure their resulting distances after withholding them from optimization. C
means only camera optimization is used while W means both camera optimiza-
tion and image warping is applied. S means we are optimizing scale and shift.
ForcamerasC,AmeansweregularizewithL ,F meansweregularizewith
aspect
L . For warps W, A means we regularize with L and Z means L is
focal ARAP2D z
used. Our proposed method uses all regularizers W(A/Z) and achieves the best
results averaged over all the scenes.
5.4 Sparse-view reconstruction
In this section, we reconstruct sparse photo collections from two Airbnb rooms
fromalisting(8photosofabedroom,shownintheprojectpage,and5photosofToon3D: Seeing Cartoons from a New Perspective 13
Sparse-view image collection (from Airbnb) Default COLMAP Our reconstruction with Toon3D
COLMAP w/ Toon3D labels
Fig.10: Sparse-view reconstruction.Ourpipelinecanreconstructsparse-viewim-
age collections (left). COLMAP by default only registers 2 out of 5 images and fails
to recover structure (middle top). Using Toon3D Labeler correspondences, we get
COLMAPtowork(middlebottom)butitisinitializedwithaverysparsepointcloud
andcannotrecoverdensedetailsproperly.UsingToon3D,wecanfullyreconstructthe
room.
alivingroom,showninFig.10).ThistaskisverydifficultbecauseSfMpipelines
likeCOLMAPfailtofindenoughcorrespondencestoaccuratelyrecoverallposes.
Furthermore, even with accurate camera poses, the sparse-view reconstruction
settingisespeciallyhardwithoutpriorsorspecializedmethodslikeRegNeRF[23]
orReconFusion[40].Weattackthissparse-viewAirbnbsettingwithourmethod
for two reasons: (1) to show that we can get COLMAP to work with labeled
correspondences from the Toon3D Labeler and (2) to show that our end-to-end
method works for real sparse photo collections, indicating applications beyond
cartoons.
Camera parameter estimation. When running COLMAP on our Airbnb
collections, default COLMAP only registers 46% of the images. This could be
possibly improved with better correspondences, e.g. [9,10,36], but there is no
guarantee of finding enough inlier correspondences if automated methods are
used. With our Toon3D Labeler, however, we can manually label the images
quickly and get COLMAP to succeed for all images. We compare the recovered
COLMAP cameras with our correspondences with the cameras recovered from
Toon3D.Themeanrelativerotationdistancebetweencorrespondingpairsinour
reconstructionsvs.COLMAP’sisquitelowatonly8.29◦,indicatingourcameras
aresimilartoonesrecoveredbyCOLMAPwithhuman-labeledcorrespondences.
We do not compare translations or focal lengths due to ambiguity between the
two,butwenotethatourcamerarelativerotationsmatchCOLMAPquitewell,
suggesting that our camera pose estimation is accurate.
Qualitativeresults.Weshowqualitativeresultsforsparse-viewreconstruction
on real images in Fig. 10 and videos on the project page.
5.5 Reconstructing paintings
Ourpipelinecanalsoreconstructpaintings.InFig.11,weshowthatToon3Dcan
reconstructTheTreviFountain.WefindpaintingsfromTheOxfordDataset[6].
Our method has an advantage over automated methods like COLMAP which14 E. Weber et al.
Paintings of The Trevi Fountain Aligned point clouds Reconstruction with Gaussians
and cameras
Fig.11: Reconstructing paintings with Toon3D. Our method enables recon-
structing paintings. On the left, we show a few paintings of The Trevi Fountain. In
themiddle,weshowtherecoveredpointcloudandcameras(withwarpedandcropped
images). On the right, we densify the point cloud with Gaussian Splatting.
rely on perspective geometry and photometric consistency. Our method, alter-
natively, uses provided correspondences and predicted depth and then warps
images into perspective images and obtains a consolidated 3D model. This en-
ables Toon3D to work on paintings as well.
6 Conclusion
Ourpipelinetakeshuman-labeledcorrespondencesandpredicteddepthmapsas
input. If either the labels are inaccurate or the predicted depth maps fail, our
method will produce less accurate results. We cannot use automatic correspon-
dence methods due to sensitivity towards outliers, so we design a user-friendly
Toon3D Labeler which will be hosted online with no user installation required.
Thiswillhelpanyonequicklygetstartedwithlabelingcartoonimagesandobtain
a 3D model in minutes. Incorporating diffusion priors or data-driven methods
to reconstruct cartoons in an end-to-end fashion could be an interesting avenue
forfuturework.Weinsteadpresentamethodthatreliesonpredicteddepthand
sparse correspondences to optimize cameras and non-rigid deformations such
that our posed images obey perspective projection models. Finally, we encour-
age that our method should be used ethically and responsibly when creating
content for visual media.
Acknowledgements
This project is supported in part by IARPA DOI/IBC 140D0423C0035. The
views and conclusions contained herein are those of the authors and do not
represent the official policies or endorsements of IARPA, DOI/IBC, of the U.S.
Government. We would like to thank Qianqian Wang, Justin Kerr, Brent Yi,
DavidMcAllister,MatthewTancik,EvonneNg,AnjaliThakrar,ChristianFoley,
Abhishek Kar, Georgios Pavlakos, the Nerfstudio team, and the KAIR lab for
discussions, feedback, and technical support. We also thank Ian Mitchell and
Roland Jose for helping to label points.Toon3D: Seeing Cartoons from a New Perspective 15
Appendix
6.1 Video
A narrated video is available on our project page https://toon3d.studio/.
Thisvideocontainsanoverviewofthepaperandvideoresults.Itiscomplemen-
tary to our paper, which is composed of screen-captured and rendered frames.
Our video results are more immersive than what 2D figures can convey.
6.2 Toon3D Dataset
We choose to use cartoon scenes that are hand-drawn rather than using ani-
mated scenes that are rendered or based on an underlying 3D model. We select
a variety of cartoons based on popularity. Table 2 shows our datasets and rele-
vantannotationinfo,includinghowmanyimagesweusetocreateeachsceneand
how many point labels are used. We use a varying number of point labels, rang-
ing from only 46 points (Magic School Bus) to as many as 191 points (BoJack
Room) in a particular scene. This range is meant to convey the robustness of
ourmethodtohandleafewormanyuser-definedcorrespondences.OurToon3D
Labeler will be released so others can label scenes as they desire.
6.3 Sparse-view reconstruction data
We obtain sparse-view images from Airbnb from this listing: https://www.
airbnb.com/rooms/833261990707199349.Ourprojectpageshowsthetworooms
and their images. The “Living room”, shown in the paper as well, has 5 images.
“Bedroom 2" has 8 images. Renders of our Toon3D reconstructions are shown
for both rooms on the project page.
Table 2: Toon3D Dataset. Here are some statistics for the Toon3D Dataset. We
have ∼7 images per scene, for a total of 79 images across the 12 scenes. Each image
has on average 18.3 points per image, but it varies per scene.
Num images Num points Average num points per image
Avatar House 8 156 19.5
Bob’s Burgers 7 147 21.0
BoJack Room 12 191 15.9
Family Guy Dining 7 184 26.3
Family Guy House 6 133 22.2
Krusty Krab 9 82 9.11
Magic School Bus 5 46 9.20
Mystery Machine 6 55 9.17
Planet Express 5 137 27.4
Simpsons House 5 137 27.4
Rick and Morty 4 99 24.8
Spirited Away 5 75 15.0
Total 79 1442 18.316 E. Weber et al.
References
1. Agarwal, S., Snavely, N., Simon, I., Seitz, S.M., Szeliski, R.: Building rome in a
day.In: 2009 IEEE12thInternationalConferenceon ComputerVision. pp.72–79
(2009). https://doi.org/10.1109/ICCV.2009.5459148 4
2. Aubry,M.,Russell,B.,Sivic,J.:Painting-to-3Dmodelalignmentviadiscriminative
visual elements. ACM Transactions on Graphics (2013) 4
3. Bescos, B., Fácil, J.M., Civera, J., Neira, J.: Dynaslam: Tracking, mapping, and
inpaintingindynamicscenes.IEEERoboticsandAutomationLetters3(4),4076–
4083 (2018) 3
4. Chen, S., Zhang, K., Shi, Y., Wang, H., Zhu, Y., Song, G., An, S., Kristjansson,
J., Yang, X., Zwicker, M.: Panic-3d: Stylized single-view 3d reconstruction from
portraits of anime characters. In: CVPR (2023) 4
5. Criminisi, A., Reid, I., Zisserman, A.: Single view metrology. IJCV (2000) 4
6. Crowley, E.J., Zisserman, A.: In search of art. In: Computer Vision-ECCV 2014
Workshops: Zurich, Switzerland, September 6-7 and 12, 2014, Proceedings, Part I
13. pp. 54–70. Springer (2015) 13
7. Debevec, P.E., Taylor, C.J., Malik, J.: Modeling and rendering architecture from
photographs: A hybrid geometry-and image-based approach. In: SIGGRAPH’96
(1996) 3
8. Delanoy, J., Aubry, M., Isola, P., Efros, A.A., Bousseau, A.: 3d sketching using
multi-view deep volumetric prediction. Proceedings of the ACM on Computer
Graphics and Interactive Techniques 1(1), 1–22 (2018) 4
9. DeTone, D., Malisiewicz, T., Rabinovich, A.: Superpoint: Self-supervised interest
point detection and description. In: Proceedings of the IEEE conference on com-
puter vision and pattern recognition workshops. pp. 224–236 (2018) 3, 13
10. Dusmanu, M., Rocco, I., Pajdla, T., Pollefeys, M., Sivic, J., Torii, A., Sattler, T.:
D2-net:Atrainablecnnforjointdetectionanddescriptionoflocalfeatures.arXiv
preprint arXiv:1905.03561 (2019) 3, 13
11. Guillard, B., Remelli, E., Yvernay, P., Fua, P.: Sketch2mesh: Reconstructing and
editing 3d shapes from sketches. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision. pp. 13023–13032 (2021) 4
12. Hartley,R.,Zisserman,A.:Multipleviewgeometryincomputervision.Cambridge
university press (2003) 3
13. Hoiem, D., Efros, A.A., Hebert, M.: Automatic photo pop-up. In: ACM SIG-
GRAPH 2005 Papers, pp. 577–584 (2005) 4
14. Horry, Y., Anjyo, K.I., Arai, K.: Tour into the picture: using a spidery mesh in-
terfacetomakeanimationfromasingleimage.In:Proceedingsofthe24thannual
conference on Computer graphics and interactive techniques. pp. 225–232 (1997)
4
15. Huang, Q., Xiong, Y., Rao, A., Wang, J., Lin, D.: Movienet: A holistic dataset
for movie understanding. In: Computer Vision–ECCV 2020: 16th European Con-
ference,Glasgow,UK,August23–28,2020,Proceedings,PartIV16.pp.709–727.
Springer (2020) 4
16. Jain,E.,Sheikh,Y.,Mahler,M.,Hodgins,J.:Three-dimensionalproxiesforhand-
drawn characters. ACM Transactions on Graphics (ToG) 31(1), 1–16 (2012) 4
17. Kanazawa, A., Kovalsky, S., Basri, R., Jacobs, D.: Learning 3d deformation of
animals from 2d images. In: Computer Graphics Forum (2016) 4, 7
18. Kanazawa,A.,Tulsiani,S.,Efros,A.A.,Malik,J.:Learningcategory-specificmesh
reconstructionfromimagecollections.In:ProceedingsoftheEuropeanConference
on Computer Vision (ECCV). pp. 371–386 (2018) 4Toon3D: Seeing Cartoons from a New Perspective 17
19. Ke, B., Obukhov, A., Huang, S., Metzger, N., Daudt, R.C., Schindler, K.: Repur-
posingdiffusion-basedimagegeneratorsformonoculardepthestimation(2023) 2,
3, 5
20. Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for
real-time radiance field rendering. ACM Transactions on Graphics 42(4) (July
2023), https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/ 2, 3, 8
21. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint
arXiv:2304.02643 (2023) 5
22. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.:Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis.Commu-
nications of the ACM 65(1), 99–106 (2021) 3
23. Niemeyer,M.,Barron,J.T.,Mildenhall,B.,Sajjadi,M.S.,Geiger,A.,Radwan,N.:
Regnerf:Regularizingneuralradiancefieldsforviewsynthesisfromsparseinputs.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 5480–5490 (2022) 3, 13
24. Park,K.,Sinha,U.,Barron,J.T.,Bouaziz,S.,Goldman,D.B.,Seitz,S.M.,Martin-
Brualla, R.: Nerfies: Deformable neural radiance fields. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 5865–5874 (2021)
4
25. Pavlakos*, G., Weber*, E., , Tancik, M., Kanazawa, A.: The one where they re-
constructed 3d humans and environments in tv shows. In: ECCV (2022) 4
26. Raj, A., Kaza, S., Poole, B., Niemeyer, M., Ruiz, N., Mildenhall, B., Zada, S.,
Aberman, K., Rubinstein, M., Barron, J., et al.: Dreambooth3d: Subject-driven
text-to-3d generation. arXiv preprint arXiv:2303.13508 (2023) 10
27. Sarlin, P.E., DeTone, D., Malisiewicz, T., Rabinovich, A.: Superglue: Learning
feature matching with graph neural networks. In: Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition. pp. 4938–4947 (2020) 3
28. Schönberger, J.L., Frahm, J.M.: Structure-from-Motion Revisited. In: Conference
on Computer Vision and Pattern Recognition (CVPR) (2016) 3
29. Smith, H.J., Zheng, Q., Li, Y., Jain, S., Hodgins, J.K.: A method for animating
children’s drawings of the human figure. ACM Trans. Graph. 42(3) (jun 2023).
https://doi.org/10.1145/3592788, https://doi.org/10.1145/3592788 4
30. Snavely,N.,Seitz,S.M.,Szeliski,R.:Phototourism:exploringphotocollectionsin
3d. In: ACM siggraph 2006 papers, pp. 835–846 (2006) 3, 4
31. Sorkine, O., Alexa, M.: As-rigid-as-possible surface modeling. In: Symposium on
Geometry processing. vol. 4, pp. 109–116. Citeseer (2007) 4, 7
32. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local fea-
turematchingwithtransformers.In:ProceedingsoftheIEEE/CVFconferenceon
computer vision and pattern recognition. pp. 8922–8931 (2021) 3
33. Tancik,M.,Weber,E.,Ng,E.,Li,R.,Yi,B.,Wang,T.,Kristoffersen,A.,Austin,J.,
Salahi,K.,Ahuja,A.,etal.:Nerfstudio:Amodularframeworkforneuralradiance
field development. In: ACM SIGGRAPH 2023 Conference Proceedings. pp. 1–12
(2023) 10
34. Thomas,F.,Johnston,O.:Theillusionoflife:Disneyanimation.(NoTitle)(1995)
4
35. Tirado-Garín, J., Warburg, F., Civera, J.: Dac: Detector-agnostic spatial covari-
ances for deep local features. arXiv preprint arXiv:2305.12250 (2023) 3
36. Tyszkiewicz,M.,Fua,P.,Trulls,E.:Disk:Learninglocalfeatureswithpolicygra-
dient.AdvancesinNeuralInformationProcessingSystems33,14254–14265(2020)
1318 E. Weber et al.
37. Vallone, A., Warburg, F., Hansen, H., Hauberg, S., Civera, J.: Danish airs and
grounds: A dataset for aerial-to-street-level place recognition and localization.
IEEE Robotics and Automation Letters 7(4), 9207–9214 (2022) 3
38. Wang, G., Chen, Z., Loy, C.C., Liu, Z.: Sparsenerf: Distilling depth ranking for
few-shot novel view synthesis. arXiv preprint arXiv:2303.16196 (2023) 8
39. Warburg, F., Weber, E., Tancik, M., Holynski, A., Kanazawa, A.: Nerf-
busters: Removing ghostly artifacts from casually captured nerfs. arXiv preprint
arXiv:2304.10532 (2023) 3
40. Wu, R., Mildenhall, B., Henzler, P., Park, K., Gao, R., Watson, D., Srinivasan,
P.P., Verbin, D., Barron, J.T., Poole, B., et al.: Reconfusion: 3d reconstruction
with diffusion priors. arXiv preprint arXiv:2312.02981 (2023) 3, 13
41. Yu, A., Ye, V., Tancik, M., Kanazawa, A.: pixelnerf: Neural radiance fields from
one or few images. in 2021 ieee. In: CVF Conference on Computer Vision and
Pattern Recognition (CVPR). pp. 4576–4585 (2020) 3
42. Zhou, C., Zhang, H., Shen, X., Jia, J.: Unsupervised learning of stereo matching.
In: Proceedings of the IEEE International Conference on Computer Vision. pp.
1567–1575 (2017) 8
43. Zhu,Y.,Kiros,R.,Zemel,R.,Salakhutdinov,R.,Urtasun,R.,Torralba,A.,Fidler,
S.:Aligningbooksandmovies:Towardsstory-likevisualexplanationsbywatching
movies and reading books. In: Proceedings of the IEEE international conference
on computer vision. pp. 19–27 (2015) 4