Subgradient Convergence Implies Subdifferential Convergence on
Weakly Convex Functions: With Uniform Rates Guarantees
Feng Ruan∗
May 17, 2024
Abstract
Innonsmooth,nonconvexstochasticoptimization,understandingtheuniformconvergenceof
subdifferentialmappingsiscrucialforanalyzingstationarypointsofsampleaverageapproxima-
tions ofrisk as they approachthe population risk. Yet, characterizingthis convergenceremains
a fundamental challenge.
Thisworkintroducesanovelperspectivebyconnectingtheuniformconvergenceofsubdiffer-
ential mappings to that of subgradient mappings as empirical risk converges to the population
risk. Weprovethat,forstochasticweakly-convexobjectives,andwithinanyopenset,auniform
boundontheconvergenceofsubgradients—chosenarbitrarilyfromthecorrespondingsubdiffer-
ential sets—translates to a uniform bound on the convergence of the subdifferential sets itself,
measured by the Hausdorff metric.
Usingthistechnique,wederiveuniformconvergenceratesforsubdifferentialsetsofstochastic
convex-composite objectives. Our results do not rely on key distributional assumptions in the
literature, which require the population and finite sample subdifferentials to be continuous in
the Hausdorff metric, yet still provide tight convergence rates. These guarantees lead to new
insights into the nonsmooth landscapes of such objectives within finite samples.
1 Introduction
At the core of nonsmooth, nonconvex stochastic optimization lies a fundamental yet underex-
plored challenge—characterizing the statistical uniform convergence behavior of set-valued subdif-
ferential mappings [SX07]. This convergence is essential for understanding how the empirical risk,
derived from sample average approximation, approximates the true population risk. While results
on the uniform convergence of objective values provide important insights, they do not directly
characterize the convergence behaviors of stationary points formulated via subdifferentials. Ana-
lyzing the uniform convergence of these set-valued subdifferential mappings of the empirical risk is
crucial for a comprehensive understanding of statistical estimation and the behaviors of numerical
algorithms in stochastic programming [SDR21, Section 7.2]. Despite its importance, this charac-
terization of subdifferential set convergence remains elusive, warranting further investigation (see
Section 1.1 for related works).
Thisworkaddressesthisgapbyintroducinganewperspectivetounderstandtheuniformconver-
gence behavior of subdifferentialsets, specifically for stochastic weakly convex objectives—a promi-
nent class of nonsmooth, nonconvex objectives with widespread applications [PR92, PR96, DD20].
Our approach hinges on a fundamental property of weakly convex functions. Specifically, for any
pair of weakly convex functions, weestablish thatover any arbitrary openset withintheir domains,
the uniform bound on the distance between their respective set-valued subdifferential mappings,
∗
Department of Statistics and DataScience, Northwestern University
1
4202
yaM
61
]CO.htam[
1v98201.5042:viXrameasuredviatheHausdorffmetric,isupperboundedbytheuniformboundonthedistancebetween
any corresponding vector-valued subgradient mappings of these functions (Theorem 3.1). This de-
terministic result immediately implies a crucial connection between the uniform convergence of
subdifferentials and the uniform convergence of subgradient mappings in sample average approxi-
mations, enabling the analysis of the former through the latter (Theorem 3.4).
We further apply our technique to an important subclass of stochastic weakly-convex objec-
tives called stochastic convex-composite objectives [Bur85, DR18], which include robust phase re-
trieval [DR19], blind deconvolution [CDDD21], matrix sensing [LZMCSV20, CCD+21], conditional
Value-at-Risk [RU00], and nonlinear quantile regressions [KP96], commonly encountered in the
field of machine learning, statistics, imaging, and risk management (Theorem 3.9). We establish
uniform convergence of subdifferential sets of stochastic weakly-convex objectives by analyzing the
convergence behavior of stochastic subgradient mappings (Theorem 4.2). Our result, which does
not rely on a common distributional assumption in the literature requiring the subdiffrential map-
pings to becontinuous underHausdorff metric [SX07, Theorem 3], provides tight convergence rates
in the context of many existing studies (see Section 1.1 for a detailed explanation). For instance,
in robust phase retrieval [DR19, DDP20], our results (Theorem 5.1) achieve a tight d/m rate
(modulologarithmic factors ond,m)foruniformconvergence of subdifferentialmappingsunderthe
p
Hausdorffdistancewheredisthedimensionandmisthesamplesize. Thisiscomplementary tothe
state-of-the-art 4 d/m rate, which is established under the graphical distance of subdifferentials,
through a different technique based on Attouch’s epigraphical convergence theorem [Att77, DD22].
p
Thisleadstonewunderstandingofthenonsmoothlandscapeoftheseobjectives, e.g., thelandscape
of the finite sample robust phase retrieval initially studied in [DDP20], including the locations and
convergence properties of stationary points.
1.1 Related Work and Contributions
This work contributes to the extensive literature on characterizing the behavior of uniform conver-
gence of subdifferentials in sample average approximation settings, for which we review. Notably,
we saturate our contributions to existing work focused on settings where the objective functions
defining the risks are simultaneously nonconvex and nonsmooth.
A significant body of literature has utilized set-valued variational analysis [RW98] and random
set theory [Mol05] to study the uniform convergence of subdifferentials from empirical to popula-
tion risks [SX07]. This technique is popular because subdifferential mappings of many nonsmooth
and nonconvex functions—including weakly convex functions—often exhibit structural properties
such as outer semicontinuity, and the subdifferentials themselves form convex sets, unlike discon-
tinuous subgradient mappings [RW98]. This technique discretizes the parameter space, achieving
initial subdifferential convergence at discrete points via Artstein and Vitale’s strong law of large
numbers for random convex sets [AV75]. The core assumption is then the subdifferential mapping
of both population and the finite sample risks are continuous under the Hausdorff metric. This
continuity—not merely outer semicontinuity—is crucial, see [SX07, Remark 2], as it enables the
extension of the initial convergence from discrete points to uniform convergence of subdifferential
sets over the entire parameter space. Without continuity, using outer-semicontinuity this approach
can still ensure uniform convergence of enlarged empirical subdifferentials to enlarged population
subdifferentials; however, this does not apply to the original subdifferentials [SX07].
Therefore, acommon andcore assumptionof thisapproach toattain uniformconvergence is the
continuity needed for both the population and empirical subdifferentials; while it facilitates many
applications, it is sometimes restrictive and does not capture the full picture of subdifferential con-
vergence [SX07, Remark 2]. For achieving uniform convergence with a rate guarantee, stronger
2assumptions, such as H¨older continuity of subdifferentials or related H-calmness conditions, are
often necessary [Xu10]. Some research aims to relax the requirement to piecewise continuity of
subdifferential sets over disjoint regions, but this still necessitates the underlying probability mea-
sure being nonatomic to ensure that samples almost surely avoid the discontinuity boundaries of
the subdifferentials [RX11].
This work presents a novel perspective to establish uniform convergence of subdifferential map-
pings that distinguishes it apart from the existing literature. Specifically, our work targets the
domain of stochastic weakly-convex classes, pivotal to numerous data-driven applications [DD20].
The fundamental principle of our approach is that, for stochastic weakly convex objectives, achiev-
inguniformconvergence—and theassociated rates—ofsubdifferentialsetsusingsampleaverage ap-
proximation requires only the uniform convergence and rates of any chosen pair of the subgradient
mappings of the corresponding empirical and population risk functions. As a result, we can derive
uniform convergence (and also rates) for a specific subclass termed stochastic convex-composite
objectives, without the need for continuity assumptions on the subdifferential mappings of either
population or finite sample objectives often required in the existing approach. Importantly, obtain-
ing uniform convergence for discontinuous subgradient mappings in stochastic convex-composite
objectives still demands sophisticated tools from statistical learning theory, such as chaining, the
concept of shattering, and the notion of Vapnik–Chervonenkis dimension [Vap13].
Notably, there is a recent advance in the literature that also targets at establishment of uniform
subdifferential convergence within the domain of stochastic weakly convex classes [DDP20, DD22].
However, there are significant differences between our results and their results. First, there is a
subtledifference in terms of the criteria for convergence. Our notion of uniformconvergence on any
open domain implies their notion of graphical convergence of subdifferential mappings on the same
domain, but the reverse is not generally true (see Remark of Theorem 3.1). Therefore, our results
are established under a topologically weaker notion of convergence. Second, there is an important
difference in the approaches and the established rates. Their approach, grounded in the utilization
of Moreau envelope smoothing [Mor65], and Attouch’s epigraphical convergence theorem [Att77],
achieves a convergence rate whose dependence on the sample size m is often at m−1/4. The reason
is that the core principle behind their approach, which aims to derive graphical subdifferential
distance bounds through the closeness of objective values, often inherently results in suboptimal
guarantees in terms of the sample size [DD22, Section 5]. Our results are notably precise regarding
the sample size m−1/2, although they apply mainly to stochastic convex-composite objectives and
introduce additional logarithmic factors. This indicates that further research is essential to fully
delineate the advantages and limitations of both approaches.
Finally, there are many interesting alternative ideas in the literature based on directly smooth-
ing the nonsmooth objective to attain uniform convergence of gradients of the smoothed objec-
tives [XZ09], also used extensively in the statistics literature, e.g., in the context of quantile regres-
sions [Hor98]. These results, however, do not directly characterize the convergence behavior of the
original subdifferential sets.
1.2 Roadmap and Paper Organizations
Section 2 provides the basic notation and definitions. Section 3 describes the general principle for
analyzing the uniform convergence behavior for the subdifferential sets for weakly convex func-
tions. Section 4 characterizes the explicit uniform convergence rates of the subdifferential sets for
stochastic convex-composite minimizations. Section 5 gives concrete applications illustrating our
techniques on the problem of robust phase retrieval. The remainders are proofs and discussions.
32 Notation and Basic Definitions
For x,y R, we let x y = min x,y and x y = max x,y . In Rd, we use and , to denote
∈ ∧ { } ∨ { } k·k h· ·i
the standard Euclidean norm and inner product respectively. We let B(x ;r)= x : x x < r
0 0
{ k − k }
denote the open ℓ ball with center at x Rd and radius r. For matrices in Rd×d, we identify each
2 0
suchmatrixwithavector inRd2 anddefin∈ e A,B astr(ABT), wheretrdenotesthetrace. Foraset
h i
X, we use cl,int to denote the closure and interior respectively for the set X. For a closed convex
set X, we use ι to denote the + -valued indicator for the set X, that is ι (x) =0 if x X and
X X
∞ ∈
+ otherwise. The normal cone to X at x is (x) := v Rd : v,y x 0 for all y X .
X
∞ N { ∈ h − i≤ ∈ }
For a function f : Rd R + , its epigraph is the set (x,α) Rd R α f(x) . It is
→ ∪{ ∞} { ∈ × | ≥ }
called closed if its epigraph is closed in Rd R. It is called subdifferentially regular at x if f(x) is
×
finite and its epigraph is Clarke regular at (x,f(x)) as a subset of Rd R [RW98, Definition 7.25].
×
A function f :Rd R is said to be 1 smooth on Rd if it has continuous gradients f :Rd Rd.
→ C ∇ →
2.1 Weakly Convex Functions and Subdifferentials
We say f : Rd R + is λ locally weakly-convex near x (also known as lower- 2 [RW98] or
→ ∪{ ∞} C
semiconvex [BDLM10]) if there exists ǫ > 0 such that
λ
y f(y)+ y 2, y B(x;ǫ)
7→ 2 k k ∈
is convex [RW98, Chapter 10.G]. We say a function f : Rd R + is a locally weakly convex
→ ∪{ ∞}
function on Rd if at every x , there is λ < such that f is λ locally weakly-convex near
O ⊆ ∈ O ∞
x. For a function f : Rd R + and a point x with f(x) finite, we let ∂f(x) denote the
→ ∪{ ∞}
Fr´echet subdifferential (or regular [RW98, Chapter 8.B]) of f at the point x,
∂f(x)= g Rd :f(y) f(x)+ g,y x +o( y x ) as y x .
∈ ≥ h − i k − k →
n o
Here, any g ∂f(x) is referred to as a subgradient of f at the point x. We say v is a horizon
∈
subgradient of f at x with f(x) finite, written as v ∂∞f(x)if there exist sequences x ,v ∂f(x )
i i i
∈ ∈
and τ 0 satisfying (x ,f(x ),τ v ) (x,f(x),v) [RW98, Chapter 8.B].
i i i i i
→ →
Notably, when f is smooth, the subdifferential ∂f(x) consists only of the gradient f(x) ,
{∇ }
while for convex function, it coincides with the subdifferential in convex analysis [Roc70]. For a
weakly convex function f, the subdifferential ∂f(x) is non-empty, compact, and convex for every x
in the interior of domf. Notably, these properties follow from the corresponding results for convex
function in convex analysis [RW98, Chapter 8].
2.2 Set-valued Analysis
OurdefinitionsfollowcloselythereferencesofRockafellarandWet[RW98],AubinandFrankowska[AF09].
For a set A Rd and y Rd, we denote by dist(y,A) = inf y z the distance from y to A
z∈A
⊆ ∈ k − k
with respect to the Euclidean norm . For two sets A ,A Rd, we denote by
1 2
k·k ⊆
D(A ,A )= sup dist(x,A )
1 2 2
x∈A1
the deviation of the set A from the set A , by
1 2
H(A ,A ) = max D(A ,A ),D(A ,A )
1 2 1 2 2 1
{ }
4the Hausdorff distance between A and A . Given a sequence of sets A Rd, the limit supremum
1 2 n
⊆
of the sets consists of limit points of subsequences y A , that is,
nk
∈
nk
limsupA = y : n , y A s.t. y y as k .
n
{ ∃
k
∃
nk
∈
nk nk
→ → ∞}
n
The limit infimum of the sets consists of limit points of sequences y A , that is:
n n
∈
liminfA = y : y A s.t. y y .
n n n n
n { ∃ ∈ → }
We let G :X ⇒ Rd denote a set-valued mapping from X to Rd, and dom(G) := x :G(x) = . A
{ 6 ∅}
function g : X Rd is said to bea selection of G: X ⇒ Rd if g(x) G(x) for every x X. We say
→ ∈ ∈
G is outersemicontinuous if for any sequence x x dom(G), we have limsup G(x ) G(x).
n → ∈ n n ⊆
We say G is innersemicontinuous if for any x x dom(G), we have liminf G(x ) G(x). We
n n n
→ ∈ ⊇
say G :X ⇒ Rd is continuous if it is both outersemicontinuous and innersemicontinuous.
Foreveryweaklyconvexfunctionf : Rd R + ,thesubgradientmapping∂f : intdom(f) ⇒
→ ∪{ ∞}
Rd is outersemicontinuous [RW98, Chapter 8].
2.3 Probability Space and Random Set-Valued Map
Let (Ξ,G,P) denote a probability space. We frequently consider random set-valued mapping of the
form : X Ξ ⇒ Rd. Let B denote the space of nonempty, compact subsets of Rd. By [RW98,
A ×
Theorem 14.4], we say the mapping ξ (x,ξ) is measurable if and only if for every B,
7→ A S ∈
(x, )−1 = ω :A(x,ω) is a measurable set under (Ξ,G,P).
A · S { ∈ S}
By aselection of the randomset (x,ξ), werefer to arandom vector a(x,ξ) (x,ξ), meaning
A ∈ A
that ξ a(x,ξ) is measurable. Note that such selection exists if ξ (x,ξ) is closed-valued and
7→ 7→ A
measurable [Aum65], see also [AF09, Theorem 8.4]. For a map : X Ξ ⇒ Rd and a probability
A ×
measure P, we define its expectation, following Aumann [Aum65]. At every x X:
∈
(x,ξ)P(dξ) := a(x,ξ)P(dξ) a(x,ξ) (x,ξ) for ξ Ξ, a(x, ) integrable .
A | ∈ A ∈ ·
ZΞ (cid:26)ZΞ (cid:27)
We say f : Rd Ξ R + is a random function when ξ f(x,ξ) is measurable at every
× → ∪{ ∞} 7→
x X [SDR21,Chapter7]. Supposearandomfunctionf alsosatisfiesx f(x,ξ)islocally weakly
∈ 7→
convex, and real-valued near a point x Rd, then the subdifferential ∂f(x,ξ) satisfies ξ ∂f(x,ξ)
∈ 7→
is measurable at that x Rd, following [SDR21, Section 7.2.6] and [XZ09, Proposition 2.1].
∈
2.4 Measurability Issues
Thissubsectioncanbemostlyskippedonafirstread,butitisessentialforaddressingmeasurability
issues that arise when considering the supremum of random functions over an uncountable index
set [Bil86]. This is important because we are interested in the supremum of the Hausdorff distance
between stochastic subdifferential mappings over a given domain.
Toaddressthesemeasurabilityissues,weusetheconceptofoutermeasureP∗ andinnermeasure
P frequently adopted in empirical process theory [vdVW96, Section 1.2-5]. Given a probability
∗
space (Ξ,G,P), every subset B Ξ is assigned with an outer measure:
⊆
P∗(B) = inf P(A) :B A,A G .
{ ⊆ ∈ }
Notably, P∗ issubadditive: P∗(B B ) P∗(B )+P∗(B ),followingtheunionbound. Correspond-
1 2 1 2
∪ ≤
ingly, the inner measure is P (B)= sup P(A) :B A,A G . Clearly, P (B) = 1 P∗(Ξ B). Fi-
∗ ∗
{ ⊇ ∈ } − \
nally, when B G is measurable, then the inner and outer measure agree: P∗(B) = P (B)= P(B).
∗
∈
53 General Theory
3.1 Uniform Subdifferential Bounds via Selections
In this section, we introduceTheorem 3.1, a general technique for obtaining uniform boundson the
subdifferentials between two real-valued, locally weakly convex functions. The theorem states that
the supremum of the Hausdorff distance between their subdifferentials over any open set is upper
bounded by the supremum of the norm differences between any selected subgradient mappings.
Theorem3.1mightinitiallyseemsurprisingbecauseatanysinglepointx,theHausdorffdistance
between the subdifferentials at x can exceed the norm difference between the selected subgradients
at x. To further appreciate this result, we will provide counterexamples in the remark to highlight
why the assumption of being an open set is crucial.
O
To the best of our knowledge, we are unaware of such a result documented in the literature.
We note that in the (weakly) convex settings, graphical distance between subdifferentials underthe
Hausdorff metric is typically established through controlling the epi-distance of functions due to
Attouch’s celebrated theorem [Att77, AB93, DD22]. See, e.g., [RW98, Theorem 12.35]. We shall
make comparisons to clarify two distance metrics of subdifferential mappings in the second remark:
the supremum of the Hausdorff distance between subdifferential mappings over x in an open set ,
O
and the graphical distance between subdifferential mappings over the same open set .
O
Theorem 3.1. Let f and f be locally weakly convex functions from to R, where is an open
1 2
O O
set in Rd. Let g and g be selections of the subdifferentials ∂f and ∂f on , respectively, i.e.,
1 2 1 2
O
obeying g (x) ∂f (x) and g (x) ∂f (x) for all x . Then the following inequality holds:
1 1 2 2
∈ ∈ ∈ O
supH(∂f (x),∂f (x)) sup g (x) g (x) . (3.1)
1 2 1 2
≤ k − k
x∈O x∈O
Proof. Our proof is structured into two parts. In the first part, we demonstrate that Theorem 3.1
holds under the assumption that is open and convex, and that both f and f are convex on .
1 2
O O
In the second part, we address the general case where f and f are locally weakly convex on
1 2
an open set . We show that this scenario can be reduced to the convex case, thus concluding our
O
proof of Theorem 3.1.
Part I. In this part, we address the case where f and f are both real-valued convex functions
1 2
on an open and convex set . Recall the definition of Hausdorff distance:
O
H(∂f (x),∂f (x)) = max D(∂f (x),∂f (x)),D(∂f (x),∂f (x)) .
1 2 1 2 2 1
{ }
By symmetry, it suffices to prove sup D(∂f (x),∂f (x)) sup g (x) g (x) , or equiva-
x∈O 1 2 ≤ x∈Ok 1 − 2 k
lently,
sup sup inf y y sup g (x) g (x) . (3.2)
1 2 1 2
x∈Oy1∈∂f1(x)y2∈∂f2(x)k − k ≤ x∈Ok − k
By applying convex duality, we obtain:
inf y y = inf sup y y ,v = sup inf y y ,v ,
1 2 1 2 1 2 (3.3)
y2∈∂f2(x)k − k y2∈∂f2(x)v:kvk≤1h − i v:kvk≤1y2∈∂f2(x)h − i
where the interchangeability between inf and sup in the second identity is due to Sion’s minimax
theorem [Sio58]. By substituting this identity back into equation (3.2), our goal reduces to prove:
sup sup sup inf y y ,v sup g (x) g (x) .
1 2 1 2
x∈Oy1∈∂f1(x)v:kvk≤1y2∈∂f2(x)h − i ≤ x∈Ok − k
6This ultimately reduces to proving for any x and y ∂f (x), and any v with v 1:
1 1
∈ O ∈ k k ≤
inf y y ,v sup g (x) g (x) . (3.4)
1 2 1 2
y2∈∂f2(x)h − i ≤ x∈Ok − k
To prove this inequality, we invoke a perturbation argument standard in convex analysis. We
startbytakingasequencet 0asn witht > 0andx+t v pern N. Suchasequence
n n n
→ → ∞ ∈ O ∈
t
n
exists because is open. Next, we pick any cluster point y˜
2
of the sequence g 2(x+t nv) n∈N.
O { }
Given that g (x+t v) ∂f (x+t v), and the subdifferential map z ∂f (z) is locally bounded
2 n 2 n 2
∈ 7→
[HUL96, Proposition 6.2.2], a cluster point y˜ necessarily exists in Rd. Furthermore, because the
2
subdifferential map z ∂f (z) is outer semicontinuous [HUL96, Theorem 6.2.4], y˜ must belong
2 2
7→
to the set ∂f (x). Finally, by selecting a subsequence of t if necessary, we can assume W.L.O.G.
2 n
that the following limit holds:
lim g (x+t v) = y˜ .
2 n 2
n→∞
Given convexity of f on the convex set , y ∂f (x) and g (x+t v) ∂f (x+t v), we obtain
1 1 1 1 n 1 n
O ∈ ∈
for every n N:
∈ 1
y ,v (f (x+t v) f (x)) g (x+t v),v .
1 1 n 1 1 n
h i ≤ t − ≤ h i
n
Thus, we have for every v with v 1:
k k ≤
inf y y ,v y y˜ ,v liminf g (x+t v) g (x+t v),v ,
1 2 1 2 1 n 2 n
y2∈∂f2(x)h − i ≤ h − i ≤ n→∞ h − i
which by an application of Cauchy-Schwartz inequality to the RHS, yields:
inf y y ,v liminf g (x+t v) g (x+t v) sup g (x) g (x) .
1 2 1 n 2 n 1 2
y2∈∂f2(x)h − i ≤ n→∞ k − k ≤ x∈Ok − k
This shows that the desired inequality (3.4) holds for any x and y ∂f (x), and any v
1 1
∈ O ∈
with v 1. Thus, we successfully establish the assertion of Theorem 3.1 in the convex settings.
k k ≤
Part II. In the second part, we study the general case where f and f are both locally weakly
1 2
convex on an open set , extending the established results for convex cases in Part I.
O
Let z be arbitrary. Then there is a convex, open neighborhood of U such that f and
z 1
∈ O ⊆O
f are weakly convex on U . To utilize the established results for convex functions in Part I, we
2 z
transform f and f into convex functions on U by adding a quadratic regularization. Specifically,
1 2 z
we consider:
L L
f˜ = f + 2 and f˜ = f + 2.
1 1 2 2
2 k·k 2 k·k
where L is a large constant. This addition renders both f˜ and f˜ convex on U .
1 2 z
Asaresultofthistransformation,wemodifytheselectionsfromthesubdifferentialsaccordingly:
g˜ (x) = g (x)+Lx and g˜ (x) = g (x)+Lx.
1 1 2 2
Importantly, this modification does not affect the Hausdorff distance between the subdifferentials
nor the norm of the difference between selections from the subdifferentials, i.e., for every x U :
z
∈
H(∂f (x),∂f (x)) = H(∂f˜(x),∂f˜(x)) and g (x) g (x) = g˜ (x) g˜(x) .
1 2 1 2 1 2 1
k − k k − k
7Since f˜ and f˜ are convex on the open, convex set U , and both g˜ and g˜ are selections from ∂f˜
1 2 z 1 2 1
and ∂f˜, we can apply the result as proved in Part I to these transformed functions to conclude:
2
sup H(∂f (x),∂f (x)) = sup H(∂f˜(x),∂f˜(x))
1 2 1 2
x∈Uz x∈Uz
sup g˜ (x) g˜ (x) = sup g (x) g (x) .
1 2 1 2
≤ k − k k − k
x∈Uz x∈Uz
Since z U , and U , it follows that for every z :
z z
∈ ⊆ O ∈O
H(∂f (z),∂f (z)) sup g (x) g (x) .
1 2 1 2
≤ k − k
x∈O
Taking supremum over z , this observation directly leads to the conclusion of Theorem 3.1:
∈ O
supH(∂f (x),∂f (x)) sup g (x) g (x) .
1 2 1 2
≤ k − k
x∈O x∈O
Remark 3.2 (Necessity of being an open set). Theorem 3.1 would not hold if were not an
O O
open set. Consider the convex functions f (x) = x and f (x) = 2x . Notably, the subdifferential
1 2
| | | |
sets are
1 x 0 2 x 0
≥ ≥
∂f (x) = [ 1,1] x = 0, ∂f (x) = [ 2,2] x = 0 .
1  2 
− −
  1 x 0   2 x 0
− ≤ − ≤
Let the subgradient selectionsg and g of ∂f and ∂f respectively be as follows:
1 2 1 2 
1 x > 0 2 x > 0
g (x) = 0 x = 0, g (x) = 0 x = 0 .
1  2 
  1 x < 0   2 x < 0
− −
 
Consider the case where = 0, which is a singleton, andthus a closed set. Then
O { }
supH(∂f (x),∂f (x)) = H(∂f (0),∂f (0)) = 1> 0 = g (0) g (0) = sup g (x) g (x) .
1 2 1 2 1 2 1 2
| − | k − k
x∈O x∈O
This illustrates the necessity for to be an open set for Theorem 3.1 to hold.
O
Remark 3.3 (Comparison of Metrics on Subdifferential Mappings). Let Rd be open. For a
O ⊆
pair of locally weakly convex functions f : R for i= 1,2, we define:
i
O →
d (∂f ,∂f ):= supH(∂f (x),∂f (x)), d (∂f ,∂f ) := H(gph ∂f ,gph ∂f ).
1 1 2 1 2 2 1 2 O 1 O 2 (3.5)
x∈O
In defining d , for a locally weakly convex f : R, we recall the graph of its subdifferential:
2
O →
gph ∂f = (x,y) Rd :y ∂f(x) ,
O { ∈O× ∈ }
where the Cartesian product Rd Rd Rd = R2d is equipped with the Euclidean distance. By
∼
O× ⊆ ×
definition,d ,d aremetricsonS = g : ⇒ Rd :g = ∂f for some locally weakly convex f : R .
1 2 O
{ O O → }
The metric d is known as the graphical distance, aligning well with applications of Attouch’s epi-
2
graphical convergence theorem [Att77], see also [DD22, Theorem 5.1].
8We document relations between d and d . For every open set , and every pair of locally
1 2
O
weakly convex f : R for i = 1,2, the following bound holds from the definition:
i
O →
d (∂f ,∂f ) d (∂f ,∂f ). (3.6)
2 1 2 1 1 2
≤
In other words, the metric d is topologically stronger than the metric d on the space S . On the
1 2 O
other hand, when = R, we can construct locally weakly convex functions f and f such that
1,n 2,n
O
lim d (∂f ,∂f )= 0 while lim d (∂f ,∂f ) = 1, (3.7)
2 1,n 2,n 1 1,n 2,n
n→∞ n→∞
meaning the metrics d and d are not equivalent in general. To construct such a sequence, we can
1 2
take convex functions f (x) = x , and f (x) = 1 x 1 + 1 x+ 1 , which yields:
1,n | | 2,n 2 − n 2 n
(cid:12) (cid:12) 1 (cid:12) x(cid:12) > 1/n
(cid:12) (cid:12) (cid:12) (cid:12)
1 x 0 [0,1] x = 1/n

≥
∂f (x)= [ 1,1] x = 0, and ∂f (x) =  0 x ( 1/n,1/n) .
1,n  2,n 
−   ∈ −
 
 1 x 0 [ 1,0] x = 1/n
− ≤ − −
1 x < 1/n
 
  − −

A direct verifcation gives d 1(∂f 1,n,∂f 2,n) = 1 and d 2(∂f 1,n,  ∂f 2,n) = 1/n for every n N.
∈
To conclude, the metric d is topologically stronger than the metric d on S for every open
1 2 O
set . An upper bound on d (∂f ,∂f ) induces an upper bound on d (∂f ,∂f ). Nevertheless, the
1 1 2 2 1 2
O
two metrics are not equivalent in general.
3.2 Implications to Stochastic Weakly Convex Minimizations
Theorem3.1provideskeyinsightsintotheuniformconvergenceofsubdifferentialsinstochasticmin-
imizations. Theorem 3.4, which builds on top of Theorem 3.1, shows that for stochastic minimizing
objectives with certain weak convexity requirements, the uniform convergence of subdifferential
sets can be effectively understood by analyzing the uniform convergence of any pair of selections
from the subdifferentials of both population and empirical objectives.
To formalize our results, we begin by introducing our setup. Let f(, ) : Rd Ξ R denote a
· · × →
real-valued random function. We are sampling ξ P where P is supported on Ξ, assumed to be a
∼
Euclidean space. We assume P is a complete measure to avoid some measurability issues.
Under this setup, the population risk is given by:
minφ(x) = f(x)+R(x)+ι X(x) where f(x) = E ξ∼P[f(x,ξ)] = f(x,ξ)P(dξ),
x∈Rd
ZΞ
whereas its associated empirical risk is given by:
m
1
xm ∈i Rn dφ S(x) = f S(x)+R(x)+ι X(x) where f S(x) = E ξ∼P m[f(x,ξ)] =
m
f(x,ξ i).
i=1
X
In the above, ξ ,ξ ,...,ξ are i.i.d. samples drawn from the distribution P, where P denotes the
1 2 m m
empirical distribution of ξ ,ξ ,...,ξ . The function R : Rd R is a closed convex function. The
1 2 m
→
constraint set X is a nonempty, closed, and convex set. We are interested in establishing an upper
boundfor therate of convergence of subdifferentials of theempirical objective φ to thepopulation
S
objective φover some subsetof X, frequently modeled as X where is an open set in Rd. This
∩O O
openness requirement for originates from Theorem 3.1 and, despite this condition, our result is
O
relevant to a broad range of interesting applications.
To ease our discussions, we assume:
9Assumption A. f(x)= f(x,ξ)P(dξ) < for all x where is an open set.
Ξ ∞ ∈ O O
Assumption A implies,Rwith probability one, f and f
S
are real-valued functions on . In many
O
practical scenarios, the objective f has its domain Rd, for which Assumption A naturally holds.
Our results also require the following locally weak convexity assumption on f(,ξ).
·
Assumption B. For all x in , there exists ǫ(x) > 0 and λ(x,ξ) 0 such that
O ≥
λ(x,ξ)
y f(y,ξ)+ y 2
7→ 2 k k
is closed and convex on the ball cl(B(x;ǫ(x))) and E[λ(x,ξ)] < .
∞
Assumption B immediately implies the function f (x) is locally weakly convex on (with
S
O
probability one). To see this, for each x ,
∈O
y f (y)+ E ξ∼P m[λ(x,ξ)] y 2 = 1 m f(y,ξ )+ λ(x,ξ i) y 2
S i
7→ 2 k k m 2 k k
i=1(cid:18) (cid:19)
X
is convex on the open neighborhood B(x;ǫ(x)). Similarly, the function f(x) = E[f (x)] is locally
S
weakly convex within .
O
Building on Theorem 3.1, Theorem 3.4 naturally follows.
Theorem 3.4. Let Assumptions A and B hold. Let G and G be selections of the subdifferentials
S
∂f and ∂f over x respectively, i.e., obeying the inclusions G(x) ∂f(x) and G (x) ∂f (x)
S S S
∈ O ∈ ∈
for every x . Then, the following inequality holds with probability one:
∈ O
sup H(φ(x),φ (x)) sup G(x) G (x) .
S S
≤ k − k
x∈O∩X x∈O
Proof. Given that X is closed and convex, ι is subdifferentially regular at any x X [RW98,
X
∈
Exercise 8.14]. The functions f(x) and f (x) are real-valued, closed and weakly convex on by
S
O
Assumptions A and B. Thus they are subdifferentially regular on [RW98, Corollary 8.11], with
O
∂∞f(x) = ∂∞f (x) = 0 for all x . Similarly, the real-valued, closed, and convex function r,
S
{ } ∈ O
is subdifferentially regular at x with ∂∞R(x) = 0 .
∈ O { }
These regularity properties of the functions enable the application of basic subdifferential cal-
culus [RW98, Corollary 10.9]. Accordingly, the subdifferentials at x X are given by:
∈
∂φ(x) = ∂f(x)+∂R(x)+ (x), ∂φ (x)= ∂f (x)+∂R(x)+ (x).
X S S X
N N
Here, the addition is the Minkowski sum. A fundamental property of Hausdorff distance is that
H(A +A ,A +A ) H(A ,A )
1 3 2 3 1 2
≤
holds for any sets A ,A ,A . Consequently, this implies for every x X:
1 2 3
∈
H(∂φ(x),∂φ (x)) H(∂f(x),∂f (x)).
S S
≤
Since both functions f(x) and f (x) are real-valued and locally weakly convex functions on due
S
O
to Assumptions A and B, the following chain of inequalities then follows:
sup H(∂φ(x),∂φ (x)) sup H(∂f(x),∂f (x))
S S
≤
x∈O∩X x∈O∩X
supH(∂f(x),∂f (x)) sup G(x) G (x) .
S S
≤ ≤ k − k
x∈O x∈O
The last inequality is due to Theorem 3.1, acknowledging that, with probability one, f and f are
S
real-valued and weakly-convex on the open set .
O
103.3 A Class of Stochastic Weakly Convex Minimizations
Oneimportantexampleofstochasticweakly convex minimizationscorrespondstothesettingwhere
f(x,ξ) = h(c(x;ξ)). (3.8)
Here, h : Rk R is a closed, convex and real-valued function, and c(,ξ) : Rd Rk is 1 smooth
→ · → C
on Rd for every ξ. Under this scenario, the population and empirical objectives become
minφ(x) = f(x)+R(x)+ι X(x) where f(x)= E ξ∼P[h(c(x;ξ))] = h(c(x;ξ))P(dξ)
x∈Rd
ZΞ
m . (3.9)
1
xm ∈i Rn dφ S(x) = f S(x)+R(x)+ι X(x) where f S(x) = E ξ∼P m[h(c(x;ξ))] =
m
h(c(x;ξ i))
i=1
X
In the literature, it is known that under mild conditions, the convex composition f(,ξ) = h(c(;ξ))
· ·
satisfiesweak-convexity requirementsinAssumptionB,resultinginlocallyweaklyconvexobjectives
f(x) and f (x) that fit into our diagrams in Section 3.2, see, e.g., [DR18, Claim 1] and [DD19].
S
We will revisit these conditions subsequently before we present our main results in Theorem 3.9.
We give examples of stochastic convex-composite objectives that appear in statistics, machine
learning, imaging, and risk management. More examples can be found in [DD19, Section 2.1].
Example 3.5 (Robust Phase Retrieval). Phase retrieval is a computational problem with appli-
cations across various fields, including imaging, X-ray crystallography, and speech processing. The
(real-valued) phase retrieval seeks to detect a point x satisfying a ,x 2 = b where a Rd, and
i i i
|h i| ∈
b R for i = 1,2,...,m. We can choose ξ = (a,b) Rd R, h(z) = z and c(x;ξ) = (aTx) b,
i
∈ ∈ × | | −
and X = Rd, in which case the form (3.9) gives an exact penalty formulation for solving the
collection of quadratic equations, which yields strong statistical recovery and robustness guaran-
tees [EM14, DR19, DDP20], among other nonconvex formulations [CLS15, WGE17, SQW18].
Example 3.6 (RobustMatrixSensing). Thisproblemcanbeviewedasavariantofphaseretrieval.
LetA ,A ,...,A RD×D bemeasurementmatrices. Giventhemeasurementb = A ,M +η
1 2 m i i ♯ i
∈ h i ∈
R, where M RD×D is the true matrix, and η R is the noise corruption, the goal is to recover a
♯ i
∈ ∈
low-rank approximation of M , which is modeled through XXT where X RD×r0 where 1 r
♯ 0
∈ ≤ ≤
D. Recently, there has been a series of efforts showing that the following potential function has
strongstability guaranteesundercertain assumptionsonAandη [LZMCSV20,CCD+21,DJC+21]:
m
1
min A ,XXT b .
i i
X∈Rd×r m |h i− |
i=1
X
This falls into the form (3.9) by simply setting x = X RD×r0, ξ = (A,b) where A RD×D,b R,
∈ ∈ ∈
h(z) = z and c(x;ξ) = c(x;(A,b)) = A,XXT b.
| | h i−
Example 3.7 (Conditional Value-at-Risk). Let ℓ(w,ξ) represent a decision rule parameterized by
w Rd for a data point ξ, where ξ P. Instead of minimizing the expected value E ξ∼P[ℓ(w,ξ)], it
∈ ∼
is often preferableto minimize theconditional expectation of the randomvariable ℓ(w, ) over its α-
·
tail,whereα (0,1)isgiven. ThisquantityistermedtheConditionalValue-at-Risk(cVaR)[SDR21,
∈
Section 6]. Remarkably, a seminal work shows that minimizing cVaR can be formulated as [RU00]:
min E [(1 α)γ +(ℓ(w,ξ) γ) ].
ξ∼P +
γ∈R,w∈Rd − −
Now we suppose ℓ(,ξ) is 1 smooth. This falls into the form (3.9) by setting x =(γ,w) R Rd,
· C ∈ ×
h(z) = (z) , c(x;ξ) = c((γ,w);ξ) = ℓ(w,ξ) γ, and R(x) = R((γ,w)) = (1 α)γ.
+
− −
11Wenowrevisittheconditionsontheconvex functionhandsmoothfunctionc(x,ξ)thatguaran-
tees the weak-convexity assumptions for the composition f(x,ξ)= h(c(x,ξ)), ensuring the regular-
ity required for subdifferential calculus rules. We take the following conditions—local Lipschitzian
and integrability conditions on h,c—from [DR18, Section 2.2].
Assumption A’. f(x) = h(c(x;ξ))P(dξ) < for every x where is an open set.
Ξ ∞ ∈ O O
Assumption B’. For everRy x , there is ǫ(x) > 0 such that
∈ O
• sup c(y;ξ)∂h(c(y;ξ)) is integrable with respect to ξ P.
y∈B(x;ǫ(x))k∇
k ∼
• c(y;ξ) c(y′;ξ) β (x,ξ) y y′ for y,y′ B(x;ǫ(x)), where E[β (x,ξ)] < .
ǫ ǫ
k∇ −∇ k ≤ k − k ∈ ∞
Given Assumptions A’ and B’, the following basic property on the stochastic convex-composite
objectives follows from [DR18, Claim 1+Lemma 3.6]. We use c(x;ξ) to denote the gradient of a
∇
smooth function c with respect to x for a given ξ.
Lemma 3.8. Let Assumptions A’ and B’ hold. Then f(x;ξ)= h(c(x;ξ)) satisfies Assumption B.
Moreover,
∂f(x;ξ) = ∂h(c(x;ξ)) c(x;ξ),
∇
with
∂f(x) = E ξ∼P[∂f(x;ξ)], and ∂f S(x) = E ξ∼P m[∂f(x;ξ)].
For ease of reference in future discussions, we establish a result that stems directly from Theo-
rem 3.4 and Lemma 3.8 for stochastic convex-composite minimizations.
Theorem 3.9. Let Assumptions A’ and B’ hold for f(x,ξ) = h(c(x;ξ)). Suppose at every x ,
∈ O
the functions G(x) and G (x) obey:
S
G(x)
∈
E ξ∼P[∂f(x;ξ)] and G S(x)
∈
E ξ∼P m[∂f(x;ξ)].
Then, the following inequality holds with probability one:
sup H(φ(x),φ (x)) sup G(x) G (x) .
S S
≤ k − k
x∈X∩O x∈O
In the above, φ(x) and φ (x) refer to the convex-composite objectives in equation (3.9).
S
4 Explicit Uniform Convergence Rates
In this section, we will delve deep into the convergence rate of the subdifferentials
sup H(φ(x),φ (x)) (4.1)
S
x∈X∩O
for a class of stochastic convex-composite objectives where
φ(x) = E ξ∼P[h(c(x;ξ))]+R(x)+ι X(x), φ S(x) = E ξ∼P m[h(c(x;ξ))]+R(x)+ι X(x). (4.2)
Here, the crucial assumption is that h : R R is a one-dimensional convex function. The function
→
c(;ξ) : Rd R is 1 smooth on Rd for every ξ, aligning with the framework in Section 3.3. As
· → C
before, R : Rd R is closed and convex, and X is nonempty, closed and convex. This setting is
→
12particularly relevant as it covers a broad spectrum of practical objectives, including all examples
previously discussed in Section 3.3. In our considerations, the set is usually an open ball B(x ;r)
0
O
in the Euclidean space where x Rd is the center and r > 0 is the radius.
0
∈
Despitetheirwideapplications, tightboundsonthesubdifferentialdifferenceoftheseobjectives
remain largely open in the literature [DD22, Section 5], which motivates our investigations. We
will show in Section 5 how our main result in this section, Theorem 4.2, yield tight bounds on
the subdifferential differences, and leads to sharp quantitative characterizations on the nonsmooth
landscape of stochastic convex-composite formulations in finite samples.
4.1 Subgradient Selections
Theorem 3.9 shows that, under regularity Assumptions A’ and B’, there is the bound:
sup H(φ(x),φ (x)) sup G(x) G (x) (4.3)
S S
≤ k − k
x∈X∩O x∈O
where isanopenset. Remarkably,GandG canbeanypairofselectionsfromthesubdifferentials
S
O
(cf. Lemma 3.8 and Theorem 3.9):
G(x) E [∂h(c(x;ξ)) c(x;ξ)], G (x) E [∂h(c(x;ξ)) c(x;ξ)]. (4.4)
∈
ξ∼P
∇
S
∈
ξ∼Pm
∇
The main goal of this subsection is to select a pair of subgradients G(x) and G (x) that are simple
S
to analyze—from a probabilistic sense—regarding their uniform differences over an open set .
O
This selection process begins with identifying a subgradient for the nonsmooth convex function
h, which we decompose into a sum h = hsm +hns of a smooth component hsm and a nonsmooth
component hns. To do so, we use h′ and h′ to denote the right-hand and left-hand derivative of h,
+ −
respectively. We enumerate the set of nondifferentiable points t R : h′ (t) = h′ (t) as t ∞ ,
{ ∈ + 6 − } { j }j=1
given the fact that these nondifferentiable points are always countable [BV10, Theorem 2.1.2].
For themajority of Section 4, we will work underthe assumption that ∞ (h′ (t ) h′ (t )) <
j=1 + j − − j
so that hns and hsm are well-defined, although this assumption will be unnecessary in our final
∞ P
bound in Theorem 4.2.
Under this assumption, we can decompose this nonsmooth h into two distinct parts:
h(x) = hsm(z)+hns(z). (4.5)
Here, hns : R R (where “ns” stands for nonsmooth) denotes the nonsmooth component of h,
7→
which is defined by:
∞
hns(x) = a (z t ) , (4.6)
j j +
· −
j=1
X
where the coefficients are defined by a = h′ (t ) h′ (t ) > 0. Notably, hns is well-defined because
∞ a = ∞ (h′ (t ) h′ (t )) < j . Th+ e sj mo− oth− coj mponent hsm :R R (where “sm” stands
j=1 j j=1 + j − − j ∞ 7→
for smooth) is defined as hsm = h hns. By construction, hsm is a 1 smooth and convex function,
P P − C
as one can easily verify its derivative (hsm)′ is continuous and monotonically increasing on R.
To select a subgradient of the convex function h, we first note that the smooth component hsm
possesses a unique subgradient, which corresponds to its derivative:
gsm(z) = (hsm)′(z). (4.7)
13For the nonsmooth component hns, a subgradient at every z R is chosen as follows:
∈
∞
gns(z) = a 1(z t ) ∂hns(z), (4.8)
j j
≥ ∈
j=1
X
where z 1(z t ) is an indicator function that is a subgradient for the mapping z (z t ) ,
j j +
7→ ≥ 7→ −
taking the value 1 when z t and 0 otherwise. Combining these, we define a subgradient g for h
j
≥
at every z:
g(z) := (gsm+gns)(z) ∂h(z).
∈
With g(z) defined, we can then pick the subgradients G and G that satisfy equation (4.4):
S
G(x) := E [g(c(x,ξ)) c(x,ξ)]
ξ∼P
·∇ . (4.9)
G (x) := E [g(c(x,ξ)) c(x,ξ)]
S ξ∼Pm
·∇
To summarize, G and G are well-defined whenever ∞ (h′ (t ) h′ (t )) < .
S j=1 + j − − j ∞
In the next subsection, we will provide a uniform convergence result that establishes a high
P
probability upper bound on G (x) G(x) over x in a Euclidean ball in Rd. We will address the
S
k − k
following question.
Question: Given a center x Rd, a radius r, a threshold δ (0,1), what values of u can we
0
∈ ∈
choose such that
P∗ sup G (x) G(x) u δ. (4.10)
S
x:x∈B(x0;r)k − k ≥
!
≤
Answerstothisquestionwillautomatically yieldahighprobabilityupperboundonthesubdifferen-
tialdifferencesup x∈X∩B(x0;r)H(φ(x),φ S(x))byapplyingequation(4.3)totheopenset
O
= B(x 0;r).
Recall P∗ denotes the outer probability (Section 2.4), which is required because the supremum in
equation (4.10) may not be measurable under the probability space.
4.2 A Uniform Bound on Subgradient Selections
This section examines the uniform rate of convergence of G to G over any Euclidean ball of radius
S
r, a question that is arised in equation (4.10). Recall our definitions:
∞
G (x) = E a 1 c(x,ξ) t c(x,ξ)+(hsm)′(c(x,ξ)) c(x,ξ)
S ξ∼Pm j
{ ≥
j
}·∇ ·∇
(cid:20)j=1 (cid:21)
X . (4.11)
∞
G(x) = E a 1 c(x,ξ) t c(x,ξ)+(hsm)′(c(x,ξ)) c(x,ξ)
ξ∼P j j
{ ≥ }·∇ ·∇
(cid:20)j=1 (cid:21)
X
A key challenge in examining the rate of uniform convergence from G to G is due to the
S
nonsmooth nature of G and G, whose definition involves indicator functions, as outlined in equa-
S
tion (4.11). To resolve this challenge, our approach leverages the notion of Vapnik–Chervonenkis
dimension (VC dimension) [VC71], a concept familiar to experts in statistical learning theory. In
short words, VC dimension measures the complexity of sets used in defining these indicator func-
tions (termed as classifiers in statistical learning theory), and this complexity measure governs the
uniform generalizability from empirical averages to expectations for these indicator functions.
We state the definition of VC dimension for completeness [Vap13, Section 3.6].
14Definition 4.1 (VC Dimension). Let denote a set family, and E a set. Let E = H E
H H∩ { ∩ |
H . We say a set E is shattered if E contains all the subsets of E, i.e., E = 2|E|.
∈ H} H∩ |H∩ |
The VC dimension vc( ) is the cardinality of the largest set that is shattered by . If arbitrarily
H H
large sets can be shattered, the VC dimension of is .
H ∞
The collection of sets that appear in the indicator functions will be denoted as:
= ξ Ξ :c(x,ξ) t x Rd,t R . (4.12)
F {{ ∈ ≥ }| ∈ ∈ }
More transparently, the VC dimension of , denoted by vc( ), is the largest integer N such that
F F
there exist ξ ,ξ ,...,ξ Ξ so that for any binary labeling sequence b ,b ,...,b 0,1 , it is
1 2 N 1 2 N
∈ ∈ { }
possible to find parameters x Rd,t R that satisfy:
∈ ∈
b = 1 c(x;ξ ) t for every 1 i N.
i i
{ ≥ } ≤ ≤
The complexity measure vc( ) characterizes generalization capabilities of and bounds the uni-
F F
form rate of convergence from G to G. For common techniques of upper boundingVC dimensions
S
of set families, see [vdVW96, Chapter 2]. When themappingx c(x;ξ) are polynomials in x Rd
7→ ∈
for any given ξ—a condition met in our applications—we establish a result giving a tight upper
bound of the VC dimension, based on results in real algebraic geometry. See Section 6 for details.
Our main results rely on Lipschitz continuity and regularity assumptions concerning the tails
of the random vectors c(x,ξ) and (hsm)′(c(x,ξ)) c(x,ξ). These random vectors appear in our
∇ ·∇
subgradient selection (4.11), and our assumptions about them align with standard treatments in
the literature. Tostate the assumptions, wefirstrecall the concept of asubexponentialtail random
vector. We define this in terms of Orlicz norms, following [Ver18, Section 3.4.4].
Definition 4.2. A random vector Z is said to be σ-subexponential if for every v with v = 1:
k k
Z,v
E exp |h i| 2.
σ ≤
(cid:20) (cid:18) (cid:19)(cid:21)
Examplesofσ-sub-exponentialrandomvectorsincludeanormaldistributionZ N(0,Σ)where
∼
the largest eigenvalue of Σ is bounded above by cσ, or distributions with bounded support, such
as Z cσ almost surely, with the numerical constant c = 1/2 [Ver18, Section 2].
k k≤
By Markov’s inequality, P( v,Z σ u) 2e−u foranyunitvector v andu 0. Furthermore,
|h i| ≥ · ≤ ≥
byBernstein’s inequality [Ver18, Theorem2.8.1] andthecentering propertyofsub-exponentialran-
dom variables [Ver18, Exercise 2.7.10], for i.i.d. σ-subexponential random variables Z ,Z ,...,Z ,
1 2 m
there is the exponential tail bound for the mean 1 m Z that holds for every v with v = 1,
m i=1 i k k
and u 0:
≥ m P
1
P Z E[Z],v σ u 2exp cmmin u2,u . (4.13)
i
m (cid:12) h − i(cid:12) ≥ · ! ≤ − { }
(cid:12) (cid:12)Xi=1 (cid:12)
(cid:12) (cid:0) (cid:1)
In the above, c > 0 is an(cid:12)absolute constant(cid:12).
(cid:12) (cid:12)
Wefirstassumetherandomvectors c(x ,ξ)and(hsm)′(c(x ,ξ)) c(x ,ξ)aresubexponential,
0 0 0
∇ ·∇
where we recall that x is the center of the ball of interest (see equation (4.10)). This facilitates an
0
initial exponential tail probability bound on the difference between G (x ) and G(x ).
S 0 0
Assumption C.1. Each of the two random vectors
c(x ,ξ) and (hsm)′(c(x ,ξ)) c(x ,ξ)
0 0 0
∇ ·∇
is σ-subexponential.
15To furtherensurea uniformexponential tail probability boundon the difference between G (x)
S
and G(x) over all possiblex B(x ;r), we willassumethat theincrements of the randomprocesses
0
∈
{∇c(x,ξ) }x∈B(x0;r) and {(hsm)′(c(x,ξ)) ∇c(x,ξ) }x∈B(x0;r) are subexponential in the following sense.
Assumption C.2. For every x ,x B(x ;r), each of the two random vectors
1 2 0
∈
c(x ,ξ) c(x ,ξ) and e(x ,ξ) e(x ,ξ)
1 2 1 2
∇ −∇ −
is σ x x subexponential. In the above, e(x,ξ) = (hsm)′(c(x,ξ)) c(x,ξ) for every x B(x ;r).
1 2 0
k − k ∇ ∈
Thisassumptioniscommonlyusedinempiricalprocesstheorytoextendhighprobabilitybounds
from individual points to the uniform control of the supremum of stochastic processes, utilizing the
chaining technique. Notably, this assumptionnaturally holds ina variety of statistical applications.
For a reference, see [vdVW96, Chapter 2].
We are now ready to present our main result. The proof of Theorem 4.1, which relies on tools
from statistical learning theory, is relatively standard, and thus deferred to Section A.
Theorem 4.1. Assume Assumptions C.1 and C.2. Assume ∞ (h′ (t ) h′ (t )) < .
j=1 + j − − j ∞
There exists a universal constant C > 0 such that for every δ (0,1):
P ∈
P∗ sup G(x) G (x) Crσζ max ∆,∆2 δ. (4.14)
S
x:x∈B(x0;r)k − k ≥ · { }
!
≤
In the above,
∞
ζ = 1+ (h′ (t ) h′ (t )) (4.15)
+ j − − j
j=1
X
where t enumerate points where h is non-differentiable, and
j
1 1
∆ = d+vc( )logm+log( ) , (4.16)
m · F δ
r
(cid:16) (cid:17)
where vc( ) is the VC dimension of the set family defined in equation (4.12).
F F
Recall P∗ denotes the outer probability (Section 2.4), also used to address measurability issues
with supremums in empirical process theory [vdVW96]. This is required because the supremum in
equation (4.14) may not be measurable under the probability space.
Theorem 4.1 establishes a uniform closeness between the selected subgradient G and G over
S
the Euclidean ball B(x ;r). In the final bound (4.14), ζ captures how the nondifferentiability of h
0
could potentially deteriorate the convergence rate. Crucially, the error term ∆ in equation (4.16)
demonstrates how the dimensionality d and the complexity measure vc( ) of the set family
F F
influence the uniform convergence rate.
Thepresenceofmax ∆,∆2 inthefinalboundinsteadofjust∆stemsfromthesub-exponential
{ }
assumption (cf. min u,u2 in the Bernstein inequality (4.13)). When ∆ 1, which is typical for
{ } ≤
large sample size m, this term simplifies to ∆, which is (d+vc( ))/m up to logarithmic factors.
F
Our convergence rates align well with the standard uniform convergence rate d/m for averages of
p
smooth functions and vc( )/m for indicator functions in the statistics literature [Wai19]. This
F p
suggests our bounds are in general tight up to logarithmic factors in m.
p
Finally, the logm term arises from the subexponential tail assumption and can be removed if a
normboundednessassumptionreplacesitinAssumptionC.2,orbyusingmoreadvancedarguments
involving additional probabilistic control of the envelope function of c(x,ξ) and (hsm)′(c(x,ξ))
∇ ·
c(x,ξ)[vdVW96,Chapter2]. Forconcisenessandclarity,thispaperdoesnotpursuesuchremoval.
∇
164.3 Implication: A Uniform Bound on Subdifferentials
We integrate Theorem 3.9 and Theorem 4.1 to document our final result, Theorem 4.2, on uniform
convergence of subdifferentials. The only work is to streamline the assumptions, recognizing that
Assumptions C.1 and C.2 essentially imply that Assumption B’ holds for =B(x ;r).
0
O
Theorem 4.2. Assume Assumptions C.1–C.2, and f(x)= E [h(c(x;ξ))] < for x B(x ;r).
ξ∼P 0
∞ ∈
Then there exists a universal constant C > 0 such that for every δ (0,1):
∈
P∗ sup H(∂φ(x),∂φ (x)) Crσζ max ∆,∆2 δ. (4.17)
S
x:x∈X∩B(x0;r) ≥ · { } ! ≤
In the above, ζ,∆ follows the same definition in the statement of Theorem 4.1.
Proof. We can assume without loss of generality that ζ = 1+ ∞ (h′ (t ) h′ (t )) < . If this
j=1 + j − − j ∞
sum were infinite, then equation (4.17) would be trivially satisfied.
P
Given Theorem 3.9 and Theorem 4.1, and with Assumption A’ assumed for = B(x ;r) in the
0
O
statementofTheorem4.2,theremainingtaskistoshowthatAssumptionB’ismetfor = B(x ;r).
0
O
This can be achieved by first recognizing that for y B(x ;r):
0
∈
c(y;ξ)∂h(c(y;ξ)) c(y;ξ) ∂hns(c(y;ξ)) + c(y;ξ) (hsm)′(c(y;ξ))
k∇ k ≤ k∇ · k ∇ ·
∞
(cid:13) (cid:13)
c(y;ξ) (h′ (t ) h′(cid:13)(t ))+ c(y;ξ) (hsm)′(cid:13)(c(y;ξ)) .
≤ k∇ k· + j − − j ∇ ·
j=1
X (cid:13) (cid:13)
(cid:13) (cid:13)
Assumptions C.1–C.2 imply that sup c(y;ξ) and sup (hsm)′(c(y;ξ)) c(y;ξ)
y∈B(x0;r)k∇
k
y∈B(x0;r)k
∇ k
are integrable over ξ P due to the standard chaining argument [Ver18, Section 8]. This implies
that sup y∈B(x0;r) k∇c(∼ y;ξ)∂h(c(y;ξ))
k
is also integrable over ξ since ∞ j=1(h′ +(t j) −h′ −(t j)) < ∞,
thus confirming the first condition of Assumption B’ holds for = B(x ;r). The second condition
0
O P
of Assumption B’ follows trivially from Assumption C.2.
Remark 4.3. Theorem 4.2 establishes a rate for uniform convergence of subdifferential mappings
forstochasticconvex-compositeobjectives at max d,vc( ) /m,modulologarithmicfactors. Our
{ F }
resultsaredifferentfromapproachesthatdependonthecontinuity ofbothempiricalandpopulation
p
subdifferentials under the Hausdorff metric, or require a nonatomic probability distribution P to
achieve a 1/√m rate [SX07, RX11] (see Section 1.1 for a more detailed literature review). Indeed,
Theorem 4.2 achieves this tight rate without requiring continuity in the subdifferential mappings
of the objective or specific conditions on the data distribution P.
Intheliteratureonuniformconvergenceratesofsubdifferentialmappingsforstochasticweakly-
convex objectives, methods using Moreau envelope smoothing [Mor65] and Attouch’s epigraphical
convergencetheorem[Att77]achieveaconvergencerateof 4 1/msuboptimalintermsofthesample
size m [DDP20, DD22]. This 4 1/m convergence is measured using graphical distance, a notion
p
which is topologically stronger than the uniform convergence criterion we consider in Theorem 4.2
p
(seeRemarkofTheorem3.1). Ourworkcomplements thesestudiesbyshowingaconvergence rate’s
dependenceonsamplesizemtobe 1/m(modulologarithmicfactors)underatopologicallyweaker
notion of convergence.
p
In summary, for the class of nonsmooth, nonconvex, stochastic convex-composite objectives we
study, our results often provide more precise convergence rates or require fewer assumptions about
the distribution P. Examples in Section 5 make this concrete.
175 Applications
Inthissection,wereturntosomeoftheexamplespreviouslydiscussedinSection3.3todemonstrate
the application of our main Theorem 4.2. We will concretely evaluate the bounds in Theorem 4.2
for some of these objectives, compare these to existing results in the literature, and use them to
derive new findings.
5.1 Illustration I: Phase Retrieval
We start with the (real-valued) robust phase retrieval objective (Example 3.5):
minΦ(x) where Φ(x) = E (a,b)∼P a,x 2 b .
x |h i − |
1 m (5.1)
m xinΦ S(x) where Φ S(x) = E (a,b)∼P m|ha,x i2 −b
|
=
m
|ha i,x i2 −b i |.
i=1
X
This objective has been studied in the literature [EM14, DR19, DDP20], yet tight bounds on the
gap between the empirical and population subdifferential maps have yet to be established.
Intheabove,x Rddenotestheunknownsignalvector,a Rddenotesthemeasurementvector,
∈ ∈
b Rdenotestheresponse,andP referstotheempiricaldistributionof(a ,b ),(a ,b ),...,(a ,b )
m 1 1 2 2 m m
∈
which are i.i.d. sampled from the distribution P. This objective falls into the class of stochastic
convex-composite objective in Section 4, as we can pick ξ = (a,b) Rd R, h(z) = z and
∈ × | |
c(x;ξ) = (aTx)2 b. Given a radius r > 0, we are interested in a high probability bound onto the
−
subdifferential gap for the empirical and population phase retrieval objectives over x cl(B(0;r)):
∈
sup H(∂Φ(x),∂Φ (x)).
S
x:kxk≤r
We need to specify conditions on the distribution of measurements (a,b) P that allow us to
∼
apply Theorem 4.2. We recall the concept of a subgaussian tail random vector [Ver18, Section 2.5].
Definition 5.1. A random vector Z is said to be σ-subgaussian if for every vector v with v = 1:
k k
Z,v 2
E exp h i 2.
σ2 ≤
(cid:20) (cid:18) (cid:19)(cid:21)
Examples of a σ-subgaussian distribution include normal distributions Z N(0,(cσ)2I), and
∼
uniform distribution on the hypercubeZ unif (cσ) d where c > 0 can be any constant obeying
∼ {± }
c 1/4. Applying Theorem 4.2, we derive an upper bound on the subdifferential gap for the
≤
robust phase retrieval. Remarkably, our result, which achieves an almost optimal m−1/2 rate (up
to logarithmic factors), holds regardless of whether the random variables a and b are discrete or
continuous. This contrasts with existing literature, which either requires a continuity assumption
or attains a slower m−1/4 rate (see Remark of Theorem 4.2).
Theorem 5.1. Assume the measurement vector a is σ-subgaussian, and E[b 2]< .
| | ∞
Then there exists a universal constant C > 0 such that for every δ (0,1) and r > 0:
∈
P∗ sup H(∂Φ(x),∂Φ (x)) Cσr max ∆ ,∆2 δ. (5.2)
S ≥ · { Φ Φ} ≤
x:kxk≤r !
In the above,
1 1
∆ = dlogdlogm+log( ) .
Φ
sm · δ
(cid:18) (cid:19)
18Proof. We apply Theorem 4.2. We first check the assumptions. For the nonsmooth h(z) = z , it
| |
is decomposed as h = hns+hsm where hsm(z) = z and hns(z) = 2(z) .
+
−
• (Assumption C.1). For the phase retrieval problem,
c(x,ξ) = 2 a,x a, (hsm)′(c(x;ξ)) c(x;ξ) = 2 a,x a.
∇ h i ∇ − h i
Thus, at thecenter x = 0 Rd, both gradients vanish: c(0,ξ) = (hsm)′(c(0;ξ)) c(0;ξ) = 0.
∈ ∇ ∇
• (Assumption C.2). It is easy to verify that
c(x ,ξ) c(x ,ξ)= 2 a,x x a
1 2 1 2
∇ −∇ h − i ,
e(x ,ξ) e(x ,ξ)= 2 a,x x a
1 2 2 1
− h − i
where e(x,ξ) =(hsm)′(c(x;ξ)) c(x;ξ). Notably, for every vector v, a,v is v -subgaussian.
∇ h i k k
Given the property that the product of two subgaussian random variables is subexponen-
tial [Ver18, Lemma 2.7.7], we derive that for some universal constant C > 0, each of
c(x ,ξ) c(x ,ξ),v = 2 a,x x a,v
1 2 1 2
h∇ −∇ i h − ih i
e(x ,ξ) e(x ,ξ) = 2 a,x x a,v
1 2 2 1
− h − ih i
must be C x x subexponential for every unit vector v with v = 1.
1 2
k − k k k
• (Integrability). Notably E[c(x;ξ)2] = E[b a,x 2]< for every x Rd.
| | | −h i| ∞ ∈
We then compute the bound in Theorem 4.2. Namely, we need to compute ζ and vc( ).
F
For the nonsmooth function h(z) = z , the corresponding value of ζ is given by
| |
ζ =1+h′ (0) h′ (0) = 3,
+ − −
as 0 is the only nondifferentiable point of h. The corresponding is given by
F
= a Rd,b R : a,x 2 b t x Rd,t R .
F { ∈ ∈ h i − ≥ } | ∈ ∈
n o
We now bound its VC dimension using Theorem 6.1, whose proof is based on a beautiful theorem
in real algebraic geometry [Mil64, Tho65, Mat13]. Note x a,x 2 b is a degree 2 polynomial in
7→ h i −
x Rd for every a,b. Using Theorem 6.1, we obtain that for some universal constant C > 0:
∈
vc( ) Cdlog(d).
F ≤
Given r > 0, we apply Theorem 6.1 to the open ball B(0;2r), and weobtain that for some universal
constant C > 0:
P∗ sup H(∂Φ(x),∂Φ (x)) Cσr max ∆ ,∆2 δ. (5.3)
x:x∈B(0;2r) S ≥ · { Φ Φ} ! ≤
Theorem 5.1 then follows by recognizing the set inclusion: cl(B(0;r)) B(0;2r).
⊆
In the literature on optimization, statistics and machine learning, establishing a uniform upper
bound on the convergence of subdifferentials is important because it facilitates our understanding
of the landscape of the nonsmooth, nonconvex finite sample objective φ , e.g., the location of
S
stationary points, growth conditions, etc., through the lens of the population objective φ. We shall
give further applications in Section 5.1.1 how Theorem 5.1 contributes to existing results to refine
ourunderstandingof thelandscapeof thenonsmoothphaseretrieval objective Φ infinitesamples.
S
Notably, characterization of thenonsmoothlandscapeof empirical risk function is an important
andprofoundtopic,andSection5.1.1presentsonlyaverybasicexampleofhowtoapplyourgeneral
theorem. Further results on this topic will be detailed in an upcoming manuscript.
195.1.1 Noiseless Phase Retrieval
We consider the noiseless phase retrieval, where the measurement b obeys:
b = a,x¯ 2
h i
for some fixed vector x¯ Rd. In this case, the objectives Φ and Φ become:
S
∈
m
1
Φ S(x)= a i,x 2 a i,x¯ 2 , Φ(x) = E a∼P[ a,x 2 a,x¯ 2 ].
m |h i −h i | |h i −h i |
i=1
X
Let us denote the set of stationary points of Φ and Φ to be:
S
= x Rd : 0 ∂Φ (x) and = x Rd :0 ∂Φ(x) .
S S
Z { ∈ ∈ } Z { ∈ ∈ }
For standardnormalmeasurementvectors a N(0,I), arecentimportantandinteresting study
∼
first characterizes , the locations of stationary points of the population objective Φ(x) [DDP20].
Z
Remarkably, the authors further develop a quantitative version of Attouch’s epi-convergence the-
orem to show that the stationary points of the finite samples objective Φ converge to those of
S
the population objective Φ at a rate of 4 d/m [DDP20]. More precisely, [DDP20, Theorem 5.2,
Corollary 6.3] prove that, there exist numerical constants c,C > 0 such that when m Cd, with
p ≥
an inner probability at least 1 Cexp( cd) (see Section 2.4 for a definition of inner probability):
− −
d
D( , ) C 4 x¯ .
S
Z Z ≤ mk k
r
In words, with high probability, for any stationary point of Φ , there is a stationary pointof Φ that
S
is at most a distance of C 4 d/m x¯ in the ℓ norm away from it. It is important to note that the
2
k k
slow rate in m, namely 1/√4m, appears intrinsic to the approach based on the quantitative version
p
of Attouch’s epi-convergence theorem [DD22, Section 5].
In this paper, we show a stronger result for large sample size m: the stationary points of the
finite samples objective Φ converge to those of the population objective Φ at a rate of d/m
S
up to logarithmic factors in d,m. This rate improvement in m is due to our bound on uniform
p
convergence of subdifferentials of Φ and Φ in Theorem 5.1, and our proof of Theorem 5.2 benefits
S
from the characterization of approximate stationary points of the population objective Φ [DDP20,
Corollary5.3]. Nonetheless,ourresult,Theorem5.2,comeswithatradeoffofadditionallogarithmic
factors in d and m.
Theorem 5.2. Assume a N(0,I). Then there exist numerical constants c,C > 0 such that when
∼
m Cd, with an inner probability at least 1 Cexp( cd):
≥ − −
d d
D( , ) C log(d)log(m)+ log(d)log(m) x¯ .
S
Z Z ≤ m m k k
r !
The formal proof of Theorem 5.2 is deferred to Appendix, Section B.
5.2 Illustration II: Matrix Sensing
Our second illustration demonstrates the flexibility of our framework. Consider the robust matrix
sensing objective [LZMCSV20, CCD+21, DJC+21]:
minΦ¯(X) where Φ¯(X) = E (a,b)∼P A,XXT b .
X |h i− |
m (5.4)
1
m XinΦ¯ S(X) where Φ¯ S(X) = E (a,b)∼P m|hA,XXT i−b
|
=
m
|hA i,XXT i−b i |.
i=1
X
20Here, A RD×D represents the measurement matrices, b R the measurements, and X RD×r0 a
∈ ∈ ∈
low-rank matrix, typically with r significantly smaller than D in applications. This objective falls
0
into the stochastic convex-composite objectives in Section 4, as we can simply set x = X RD×r0,
∈
ξ = (A,b) RD×D R, h(z) = z and c(x;ξ) = c(x;(A,b)) = A,XXT b.
∈ × | | h i−
Another application of Theorem 4.2 yields an upper bound on the subdifferential gap for the
robustmatrixsensingobjective. RecallamatrixA RD×D isaσ-subgaussianrandommatrixifthe
∈
vectorized matrix vec(A) RD×D is a σ-subgaussian vector in the sense of Definition 5.1 [Ver18].
∈
Corollary 5.3. Assume the measurement matrix A RD×D is σ-subgaussian, and E[b 2]< .
∈ | | ∞
Then there exists a universal constant C > 0 such that for every δ (0,1) and r > 0:
∈
P∗ sup H(∂Φ¯(X),∂Φ¯ (X)) Cσr max ∆ ,∆2 δ. (5.5)
S
≥ · {
Φ¯ Φ¯
} ≤
X:kXk ≤r !
F
In the above,
1 1
∆ = Dr log(Dr )logm+log( ) .
Φ¯
sm ·
0 0
δ
(cid:18) (cid:19)
The proof is provided in Section C and is analogous to the phase retrieval case. The primary
challenge, which comes from evaluating the VC dimension, is addressed using Theorem 6.1.
6 VC Dimension Bounds
We establish a result on upper bounding the VC dimension of the set defined in equation (4.12)
F
whenallthefunctionsx c(x;ξ) arepolynomials. Notably, thisresultdoesnotrequireξ c(x;ξ)
7→ 7→
to be polynomials in the data ξ.
Theorem 6.1. Consider x Rd and ξ Ξ, with each mapping x c(x;ξ) being a polynomial of
∈ ∈ 7→
degree at most K for every ξ Ξ, and K 1. The VC dimension of the set (cf. equation (4.12))
∈ ≥ F
= ξ Ξ :c(x,ξ) t x Rd,t R .
F {{ ∈ ≥ }| ∈ ∈ }
is bounded by Cdlog(Kd), where C > 0 is a universal constant.
Definition 6.1. Let p ,p ,...,p be a sequence of functions where each function p : Rd R. A
1 2 N i
→
vector σ 1,0,+1 N is called a sign pattern of p ,p ,...,p if there exists an x Rd such that
1 2 N
∈ {− } ∈
the sign of p (x) is σ for all i = 1,2,...,n, where σ is the ith coordinate of σ.
i i i
The following theorem, rooted in the field of real algebraic geometry, is referenced from Ma-
tousek’s textbook on discrete geometry [Mat13]. See [Mil64, Tho65] and [War68, Theorem 3].
Theorem 6.2 ([Mat13, Theorem 6.2.1]). Let p ,p ,...,p be d-variate real polynomials of degree
1 2 N
at most K. The number of sign patterns of p ,p ,...,p is bounded by
1 2 N
d
50KN
.
d
(cid:18) (cid:19)
Proof of Theorem 6.1. Define z = (x,t) Rd R = Rd+1. Given a dataset ξ ,ξ ,...,ξ , let us
∈ × ∼ 1 2 N
define the functions q (z) = q (x,t) = c(x;ξ ) t for 1 i N. Notably, each q is a d+1-variate
i i i i
− ≤ ≤
polynomial with degree at most K.
21Consider the set of binary patterns b defined by:
= (b ,b ,...,b ) 0,1 N :
1 2 N
B { ∈ { }
there exists z Rd+1 such that b = 1 q (z) 0 for all 1 i N .
i i
∈ { ≥ } ≤ ≤ }
Bydefinition,thenumberofpossiblebinarypatterns, ,cannotexceedthenumberofsignpatterns
|B|
for the same polynomials q ,q ,...,q . According to Theorem 6.2, the number of sign patterns is
1 2 N
bounded by (50KN/(d+1))d+1. Therefore, we conclude:
d+1
50KN
.
|B|≤ d+1
(cid:18) (cid:19)
Remarkably, this bound on is valid for any choice of the data points ξ ,ξ ,...,ξ . To establish
1 2 N
|B|
an upper limit on the VC dimension of , it remains to analyze the set of possible integers N that
F
satisfies the following inequality:
d+1
50KN
< 2N.
d+1
(cid:18) (cid:19)
A straightforward algebraic manipulation reveals that there exists a universal constant C > 0
such that setting N > Cdlog(Kd) ensures the inequality holds. The conclusion is that for any
dataset ξ ,ξ ,...,ξ with size N > Cdlog(Kd), the total number of binary patterns generated by
1 2 N
q (z) = c(x;ξ ) t with x ranging over Rd and t over R is strictly fewer than 2N.
i i
−
This proves the bound on VC dimension: vc( ) Cdlog(Kd) as desired.
F ≤
7 Discussions
This paper presents a general technique for proving subdifferential convergence in weakly convex
functions through the establishment of subgradient convergence (Theorem 3.1). We demonstrate
this approach to stochastic convex-composite minimizations—an important class of weakly convex
objectives—and derive concrete convergence rates for subdifferentials of empirical objectives using
tools from statistical learning theories (Theorem 4.2). Our results achieve optimal rates with tight
dependenceonthesamplesizeanddimensionality(uptologarithmicfactors)anddonotrelyonkey
distributional assumptions in the literature requiringthe subdifferentialmappings to becontinuous
under the Hausdorff metric. In Section 5, we demonstrate how our results lead to complementary
understanding of subdifferential convergence and its uses for landscape analysis, in more specific
applications such as robust phase retrieval and matrix sensing problems.
There are many interesting future directions to extend the scope of the current work. Notable
examples include:
1. Can Theorem 4.2 be extended to other weakly-convex models, such as max-of-smooth func-
tions [Roc81, HL23]? Extending this theorem could provide tools for solving other weakly-
convex optimization problems.
2. Can Theorem 3.1 be extended to other set-valued mappings, such as outer semicontinuous
monotone operators? Extending this theorem could enhance our understanding of solutions
to certain stochastic variational inequalities [FP03].
3. Theorem 4.2 could facilitate our understanding of the landscape of a certain class of nons-
mooth empirical risk minimizations. Exploring its statistical applications, such as nonlinear
quantile regressions [KP96, Koe05], may lead to new insights into data-driven models.
228 Acknowledgement
Feng Ruan extends his gratitude to his friends and advisor—John Duchi, Qiyang Han, X.Y. Han,
Kate Yao and Yichen Zhang—for their support and patience as he tackled this once seemingly
challengingproblem. AshiftinperspectiveduringhistimeatNorthwesternledhimtothissolution.
References
[AB93] H´edy Attouch and Gerald Beer, On the convergence of subdifferentials of convex
functions, Archiv der Mathematik 60 (1993), no. 4, 389–400.
[AF09] Jean-Pierre Aubin and H´el`ene Frankowska, Set-valued analysis, Springer, 2009.
[Att77] H´edy Attouch, Convergence de fonctions convexes, des sous-diff´erentiels et semi-
groupes associ´es, CR Acad. Sci. Paris 284 (1977), no. 539-542, 13.
[Aum65] RobertJAumann,Integrals of set-valued functions,Journalofmathematicalanalysis
and applications 12 (1965), no. 1, 1–12.
[AV75] ZviArtsteinandRichardAVitale, Astrong law oflarge numbersfor random compact
sets, The Annals of Probability (1975), 879–882.
[BDLM10] J´eroˆme Bolte, Aris Daniilidis, Olivier Ley, and Laurent Mazet, Characterizations
of lojasiewicz inequalities: subgradient flows, talweg, convexity, Transactions of the
American Mathematical Society 362 (2010), no. 6, 3319–3363.
[Bil86] P. Billingsley, Probability and measure, Second ed., Wiley, 1986.
[Bur85] James V Burke, Descent methods for composite nondifferentiable optimization prob-
lems, Mathematical Programming 33 (1985), 260–279.
[BV10] JonathanMBorweinandJonDVanderwerff,Convex functions: constructions, char-
acterizations and counterexamples, vol. 109, CambridgeUniversity Press Cambridge,
2010.
[CCD+21] Vasileios Charisopoulos, Yudong Chen, Damek Davis, Mateo D´ıaz, Lijun Ding, and
Dmitriy Drusvyatskiy, Low-rank matrix recovery with composite optimization: good
conditioning and rapid convergence, Foundations of Computational Mathematics 21
(2021), no. 6, 1505–1593.
[CDDD21] Vasileios Charisopoulos,DamekDavis,MateoD´ıaz,andDmitriyDrusvyatskiy, Com-
posite optimization for robust rank one bilinear sensing, Information and Inference:
A Journal of the IMA 10 (2021), no. 2, 333–396.
[CLS15] Emmanuel J Candes, Xiaodong Li, and Mahdi Soltanolkotabi, Phase retrieval via
wirtinger flow: Theory and algorithms, IEEE Transactions on Information Theory
61 (2015), no. 4, 1985–2007.
[DD19] Damek Davis and Dmitriy Drusvyatskiy, Stochastic model-based minimization of
weakly convex functions, SIAM Journal on Optimization 29 (2019), no. 1, 207–239.
[DD20] , Subgradient methods under weak convexity and tame geometry, SIAG/OPT
Views and News 28 (2020), 1–10.
23[DD22] , Graphical convergence of subgradients in nonconvex optimization and learn-
ing, Mathematics of Operations Research 47 (2022), no. 1, 209–231.
[DDP20] Damek Davis, Dmitriy Drusvyatskiy, and Courtney Paquette, The nonsmooth land-
scape of phase retrieval, IMA Journal of Numerical Analysis 40 (2020), no. 4, 2652–
2695.
[DJC+21] Lijun Ding, Liwei Jiang, Yudong Chen, Qing Qu, and Zhihui Zhu, Rank overspeci-
fied robust matrix recovery: Subgradient method and exact recovery, arXiv preprint
arXiv:2109.11154 (2021).
[DR18] John C Duchi and Feng Ruan, Stochastic methods for composite and weakly convex
optimization problems, SIAM Journal on Optimization 28 (2018), no. 4, 3229–3259.
[DR19] , Solving (most) of a set of quadratic equalities: Composite optimization for
robust phase retrieval, Information and Inference: A Journal of the IMA 8 (2019),
no. 3, 471–529.
[Dud67] Richard M Dudley, The sizes of compact subsets of hilbert space and continuity of
gaussian processes, Journal of Functional Analysis 1 (1967), no. 3, 290–330.
[EM14] Yonina C Eldar and Shahar Mendelson, Phase retrieval: Stability and recovery guar-
antees, Applied and Computational Harmonic Analysis 36 (2014), no. 3, 473–494.
[FP03] Francisco Facchinei and Jong-Shi Pang, Finite-dimensional variational inequalities
and complementarity problems, Springer, 2003.
[HL23] XYHanandAdrianSLewis, Survey descent: A multipoint generalization of gradient
descent for nonsmooth optimization, SIAMJournalonOptimization33(2023), no.1,
36–62.
[Hor98] Joel L Horowitz, Bootstrap methods for median regression models, Econometrica
(1998), 1327–1351.
[HUL96] Jean-Baptiste Hiriart-Urruty and Claude Lemar´echal, Convex analysis and mini-
mization algorithms I: Fundamentals, vol. 305, Springer science & business media,
1996.
[Koe05] Roger Koenker, Quantile regression, vol. 38, Cambridge university press, 2005.
[KP96] Roger Koenker and Beum J Park, An interior point algorithm for nonlinear quantile
regression, Journal of Econometrics 71 (1996), no. 1-2, 265–283.
[LZMCSV20] Xiao Li, Zhihui Zhu, Anthony Man-Cho So, and Rene Vidal, Nonconvex robust low-
rank matrix recovery, SIAM Journal on Optimization 30 (2020), no. 1, 660–686.
[Mat13] Jiri Matousek, Lectures on discrete geometry, vol. 212, Springer New York, 2013.
[Mil64] John Milnor, On the betti numbers of real varieties, Proceedings of the American
Mathematical Society 15 (1964), no. 2, 275–280.
[Mol05] Ilya Molchanov, Theory of random sets, vol. 19, Springer, 2005.
24[Mor65] Jean-Jacques Moreau, Proximit´e et dualit´e dans un espace hilbertien, Bulletin de la
Soci´et´e math´ematique de France 93 (1965), 273–299.
[PR92] Ren´e Poliquin and R Tyrrell Rockafellar, Amenable functions in optimization, Non-
smooth optimization: methods and applications (Erice, 1991) (1992), 338–353.
[PR96] , Prox-regular functions in variational analysis, Transactions of theAmerican
Mathematical Society 348 (1996), no. 5, 1805–1838.
[Roc70] R Tyrrell Rockafellar, Convex analysis, Princeton University Press, 1970.
[Roc81] R Tyrrell Rockafellar, Favorable classes of lipschitz continuous functions in subgra-
dient optimization.
[RU00] R Tyrrell Rockafellar and Stanislav Uryasev, Optimization of conditional value-at-
risk, Journal of risk 2 (2000), 21–42.
[RW98] R Tyrrell Rockafellar and Roger JB Wets, Variational analysis, Springer, New York,
1998.
[RX11] Daniel Ralph and Huifu Xu, Convergence of stationary points of sample average
two-stage stochastic programs: A generalized equation approach, Mathematics of Op-
erations Research 36 (2011), no. 3, 568–592.
[Sau72] Norbert Sauer, On the density of families of sets, Journal of Combinatorial Theory,
Series A 13 (1972), no. 1, 145–147.
[SDR21] Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski, Lectures on
stochastic programming: modeling and theory, SIAM, 2021.
[She72] SaharonShelah,A combinatorial problem; stability and order for models and theories
in infinitary languages, Pacific Journal of Mathematics 41 (1972), no. 1, 247–261.
[Sio58] Maurice Sion, On general minimax theorems., Pacific J. Math. 8 (1958), no. 4, 171–
176.
[SQW18] Ju Sun, Qing Qu, and John Wright, A geometric analysis of phase retrieval, Foun-
dations of Computational Mathematics 18 (2018), 1131–1198.
[SX07] Alexander Shapiroand Huifu Xu, Uniform laws of large numbers for set-valued map-
pings and subdifferentials of random functions, Journal of Mathematical Analysis
and Applications 325 (2007), no. 2, 1390–1399.
[Tho65] R Thom, On the homology of real algebraic varieties, Differential and Combinatorial
Topology (1965).
[Vap13] Vladimir N Vapnik, The nature of statistical learning theory, Springer science &
business media, 2013.
[VC71] Vladimir N Vapnik and A Ya Chervonenkis, On the uniform convergence of relative
frequencies of events to their probabilities, vol. 16, 1971, pp. 264–280.
[vdVW96] A. W. van der Vaart and J. A. Wellner, Weak convergence and empirical processes:
With applications to statistics, Springer, New York, 1996.
25[Ver18] Roman Vershynin, High-dimensional probability: An introduction with applications
in data science, vol. 47, Cambridge university press, 2018.
[Wai19] Martin J Wainwright, High-dimensional statistics: A non-asymptotic viewpoint,
vol. 48, Cambridge university press, 2019.
[War68] Hugh E Warren, Lower bounds for approximation by nonlinear manifolds, Transac-
tions of the American Mathematical Society 133 (1968), no. 1, 167–178.
[WGE17] Gang Wang, Georgios B Giannakis, and Yonina C Eldar, Solving systems of random
quadratic equations via truncated amplitude flow, IEEE Transactions on Information
Theory 64 (2017), no. 2, 773–794.
[Xu10] Huifu Xu, Uniform exponential convergence of sample average random functions un-
der general sampling with applications in stochastic programming, Journal of Math-
ematical Analysis and Applications 368 (2010), no. 2, 692–710.
[XZ09] HuifuXuandDaliZhang,Smooth sample average approximation of stationary points
in nonsmooth stochastic optimization and applications, Mathematical programming
119 (2009), 371–401.
26A Proof of Theorem 4.1
In this section, we prove Theorem 4.1. The key to our proof is to establish high probability upper
bounds of the following two quantities:
Γ 1 = x∈B(xsu 0;p r),t∈RkE ξ∼P m[1 {c(x;ξ)
≥
t }·∇c(x;ξ)] −E ξ∼P[1 {c(x;ξ)
≥
t }·∇c(x;ξ)]
k
Γ 2 = x∈Bsu (xp
0;r)
E ξ∼P m[(hsm)′(c(x;ξ)) ·∇c(x;ξ)] −E ξ∼P[(hsm)′(c(x;ξ)) ·∇c(x;ξ)]
(cid:13) (cid:13)
(cid:13) (cid:13)
Our interests on these two quantities Γ ,Γ are motivated by the following inequality:
1 2
∞
sup G (x) G(x) a Γ +Γ . (A.1)
S j 1 2
x∈B(x0;r)k − k ≤
(cid:16)Xj=1
(cid:17)·
Below, we will establish the existence of a universal constant C > 0 such that for every δ (0,1):
∈
P∗(Γ Cσr max ∆,∆2 ) δ, P∗(Γ Cσr max ∆,∆2 ) δ (A.2)
1 2
≥ · { } ≤ ≥ · { } ≤
where ∆ has the same definition as in equation (4.16):
1 1
∆ = d+vc( )logm+log( ) .
m · F δ
r
(cid:16) (cid:17)
Given these high probability bounds on Γ ,Γ in equation (A.2), Theorem 4.1 follows by plugging
1 2
these high probability bounds into equation (A.1).
It remains to prove the high probability bounds on Γ and Γ as specified in equation (A.2).
1 2
To unify the proofs for these two bounds, we utilize Theorem A.1, which details the uniform
convergence rate involving indicator functions and smooth functions. The proof of Theorem A.1 is
in Section A.1.
Theorem A.1. Consider two subsets of Rd, and , where is a Euclidean ball centered at
1 2 2
Z Z Z
z with radius r. Let o : Ξ R and w : Ξ Rd. Suppose ξ ,ξ ,...,ξ are i.i.d. samples
2,0 1 2 1 2 m
Z × → Z × →
from a distribution P with support Ξ. For any z and z , we define:
1 1 2 2
∈ Z ∈ Z
m
1
K (z ,z ,t) = 1 o(z ,ξ ) t w(z ,ξ ),
m 1 2 1 i 2 i
m { ≥ }
i=1 .
X
K(z 1,z 2,t) =E ξ∼P 1 o(z 1,ξ) t w(z 2,ξ)
{ ≥ }
(cid:20) (cid:21)
Assume the following:
(a) The random variable w(z ,ξ) is sub-exponential with parameter σ2.
2,0
(b) For every z ,z′ , the random vector w(z ,ξ) w(z′,ξ) is σ z z′ subexponential.
2 2 ∈Z2 2 − 2 k 2 − 2k
(c) The random function z w(z ,ξ) and z o(z ,ξ) are continuous.
2 2 1 1
7→ 7→
Under these conditions, there exists a universal constant C > 0 such that for every δ (0,1):
∈
P∗ sup K (z ,z ,t) K(z ,z ,t) Cσr max ∆ ,∆2 δ (A.3)
z1∈Z1,z2∈Z2,t∈Rk m 1 2 − 1 2 k≥ · { W W}
!
≤
27where
1 1
∆ = d+vc( )log(m)+log( ) .
W
m · W δ
r
(cid:16) (cid:17)
Here, := ξ Ξ :o(z ,ξ) t z ,t R , and vc( ) denotes its VC dimension.
1 1 1
W {{ ∈ ≥ } | ∈ Z ∈ } W
WewillapplyTheoremA.1fortwo differentsetsoffunctionso,w todeducethehighprobability
bounds on Γ ,Γ in equation (A.2). Below we set = Rd and = B(x ;r).
1 2 1 2 0
Z Z
First, we set o(x,ξ) = c(x,ξ) and w(x′,ξ) = c(x′,ξ). Given Assumptions C.1 and C.2, the
∇
conditions of Theorem A.1 are satisfied. Applying this theorem yields the following guarantee. Let
Γ′ 1 = x∈Rd,t∈Rsu ,xp ′∈B(x0;r) E ξ∼P m[1 {c(x;ξ) ≥ t }·∇c(x′;ξ)] −E ξ∼P[1 {c(x;ξ) ≥ t }·∇c(x′;ξ)]
(cid:13) (cid:13)
(cid:13) (cid:13)
Then there exists a universal constant C > 0 such that for every δ (0,1):
∈
P∗(Γ′ Cσr max ∆,∆2 ) δ,
1 ≥ · { } ≤
where ∆ is defined as in equation (4.16). Since Γ Γ′ by definition, this establishes the first
1 ≤ 1
inequality of equation (A.2).
Next, we set o(x,ξ) 0 and w(x′,ξ) = (hsm)′(c(x′;ξ)) c(x′;ξ). Given Assumptions C.1
≡ · ∇
and C.2, the conditions of Theorem A.1 are satisfied. Applying this theorem provides the following
guarantee. Let
Γ′ 2 = t∈R,xs ′∈u Bp
(x0;r)
E ξ∼P m[1 {0 ≥ t }·(hsm)′(c(x′;ξ)) ∇c(x′;ξ)] −E ξ∼P[1 {0 ≥ t }·(hsm)′(c(x′;ξ)) ∇c(x′;ξ)]
(cid:13) (cid:13)
(cid:13) (cid:13)
There exists a universal constant C > 0 such that for every δ (0,1):
∈
P∗(Γ′ Cσr max ∆,∆2 ) δ,
2 ≥ · { } ≤
where ∆ is defined as in equation (4.16). Since Γ Γ′ by definition, this establishes the second
2 ≤ 2
inequality of equation (A.2).
At this point, the proof of Theorem 4.1 is complete.
A.1 Proof of Theorem A.1
Ourproof of Theorem A.1 combines two well-established techniques for proving uniform boundson
the supremum of a random process in the statistical learning theory: chaining, which constructs a
sequenceofprogressively finerapproximationstothecontinuousprocess,andSauer-ShelahLemma,
a combinatorial principle using the VC dimension to control the complexity of the function class
underconsideration[vdVW96,Chapter2]. Thechainingargumentspecificallyaddressesthecontin-
uous parameter z , and the complexity of its increments is then controlled using the VC dimension
2
technique applied to the function class specified by the parameter z . To deal with the measurabil-
1
ity issues, we use the notion of outer integral [vdVW96, Chapter 1.2], and prove that our process
is measurable after symmetrization, following the idea in [vdVW96, Chapter 2.3].
Given the subtlety in dealing with measurability, and differences from existing literature, which
typically focuses either on chaining or on VC dimension arguments applied to indicator functions,
we provide detailed explanations of our combined approach to ensure clarity and completeness.
28A.1.1 Chaining Argument
For notational simplicity, below we denote:
R (z ,z ,t) = K (z ,z ,t) K(z ,z ,t).
m 1 2 m 1 2 1 2
−
The main idea in the chaining argument involves discretizing the parameter z into increasingly
2
finer subsets, constructing chains to approximate the supremumof R (z ,z ,t), which allows more
m 1 2
tractable probabilistic analysis.
Letǫ = r2−k foreveryk 0. Anǫ -coverof ,denotedby ,enablestheapproximationof
k
≥
k Z2 Z2,ǫk
every pointz byapointπ (z ) suchthat z π (z ) ǫ . Specifically, wetake the
2
∈
Z2 k 2
∈
Z2,ǫk
k
2
−
k 2
k≤
k
initialcover = z toconsistsolelyofthecenterz oftheball . Byemployingastandard
Z2,ǫ0
{
2,0
}
2,0 Z2
volume-type argument, we can further select the cover such that log plog(2+r/ǫ )
Z2,ǫk |Z2,ǫk|
≤
k
for every k 1 [Ver18, Corollary 4.2.13].
≥
Using these coverings, we can decompose R (z ,z ) through a chaining argument [Dud67]:
m 1 2
∞
R (z ,z ,t) = R (z ,z ,t)+ R (z ,π (z ),t) R (z ,π (z ),t) .
m 1 2 m 1 2,0 m 1 k 2 m 1 k−1 2
−
Xk=1h i
This decomposition is valid because the limit R (z ,z ,t) = lim R (z ,π (z ),t) holds with
m 1 2 k→∞ m 1 k 2
probability one, as z R (z ,z ,t) is continuous by assumption. Applying the supremum over
2 m 1 2
7→
z , z , t after taking the norm , and using the triangle inequality we obtain:
1 2
k·k
sup R (z ,z ,t) sup R (z ,z ,t)
m 1 2 m 1 2,0
z1∈Z1,z2∈Z2,t∈Rk k ≤ z1∈Z1,t∈Rk k
∞
+ sup R (z ,π (z ),t) R (z ,π (z ),t) .
m 1 k 2 m 1 k−1 2
k=1z1∈Z1,z2∈Z2,t∈Rk − k
X
(A.4)
We will bound the tail behavior of every single term in this sum. To this end, the following lemma
is useful. For every δ (0,1), define:
∈
1 1
∆(δ) := d+vc( )log(m)+log( ) . (A.5)
sm · W δ
(cid:18) (cid:19)
Lemma A.2. There exists a universal constant C > 0 such that for any given pair z ,z′ , the
2 2 ∈Z2
following occurs with an inner probability at least 1 δ:
−
sup R (z ,z ,t) R (z ,z′,t) Cσ z z′ max ∆(δ),∆(δ)2 .
z1∈Z1,t∈R m 1 2 − m 1 2 ≤ · 2 − 2 · { }
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
Similarly, for the center z of , the following occurs with an inner probability at least 1 δ:
2,0 2
Z −
sup R (z ,z ,t) Cσ max ∆(δ),∆(δ)2 .
m 1 2,0
z1∈Z1,t∈Rk k ≤ · { }
The proof of Lemma A.2 is deferred to Section A.1.2.
Back to our bound of the tail behavior of every term on the RHS of equation (A.4). Fix
δ (0,1). Let us denote δ = δ/2 and
0
∈
δ = δ 2−(k+1) ( )−1
k
· ·
|Z2,ǫk||Z2,ǫk−1|
29for each k 1. Below we use C to denote the universal constant that appears in Lemma A.2.
≥
First, by Lemma A.2, with an inner probability at least 1 δ/2:
−
sup R (z ,z ,t) Cσ max ∆(δ ),∆(δ )2 .
m 1 2,0 0 0
z1∈Z1,t∈Rk k ≤ · { }
This controls the first term on the RHS of equation (A.4).
Next, the adjustment of δ into δ allows the use of a union bound to extend the probability
k
guaranteesfromLemmaA.2fromindividualpairstoallpairswithinthecoversets and .
Z2,ǫk Z2,ǫk−1
Specifically, for each k 1, with an inner probability at least 1 δ 2−(k+1), there is the bound:
≥ − ·
sup R (z ,π (z ),t) R (z ,π (z ),t) 3Cσǫ max ∆(δ ),∆(δ )2 .
m 1 k 2 m 1 k−1 2 k k k
z1∈Z1,z2∈Z2,t∈Rk − k ≤ · { }
This result holds because the cardinality of pairs of (π (z ),π (z )) over z is capped by
k 2 k−1 2 2 2
{ } ∈Z
, and there is the bound π (z ) π (z ) 3ǫ that holds for every z .
|Z2,ǫk||Z2,ǫk−1|
k
k 2
−
k−1 2
k ≤
k 2
∈
Z2
Bysubstitutingalltheabovehighprobability boundsintotheRHSofequation (A.4),andusing
the union bounds, noting ∞ δ 2−(k+1) = δ, we obtain with an inner probability at least 1 δ:
k=0 · −
P ∞
sup R (z ,z ,t) Cσ max ∆(δ ),∆(δ )2 +3Cσ ǫ max ∆(δ ),∆(δ )2 .
m 1 2 0 0 k k k
z1∈Z1,z2∈Z2,t∈Rk k ≤ · { } ·
k=1
· { }
X
ToaccumulatetermsontheRHS,wededuceabasicboundwhoseproofisdeferredtoSectionA.1.3.
Lemma A.3. For some universal constant c > 0:
∞
ǫ max ∆(δ ),∆(δ )2 crmax ∆(δ),∆(δ)2 .
k k k
· { } ≤ { }
k=1
X
Ultimately, this proves the existence of a universal constant C¯ > 0 such that
sup R (z ,z ,t) C¯σr max ∆(δ),∆(δ)2
m 1 2
z1∈Z1,z2∈Z2,t∈Rk k ≤ · { }
holds with probability at least 1 δ. The proof of Theorem A.1 is then complete.
−
A.1.2 Bounding Individual Term using VC-Dimension (Proof of Lemma A.2)
We give aunifyingprooffor thetwo statements in LemmaA.2. Tosimplify thenotation, wedefine:
ψ (ξ) = 1 o(z ,ξ) t .
z1,t
{
1
≥ }
We will show that it suffices to prove the following more general statement.
Lemma A.4. There exists a universal constant C > 0 such that the following holds. Define:
m
1
W (z ,t) = ψ (ξ )u(ξ ) E[ψ (ξ)u(ξ)]. (A.6)
m 1
m
z1,t i i
−
z1,t
i=1
X
where u(ξ) in Rd is σ¯ subexponential. Then it happens with an inner probability at least 1 δ:
−
sup W (z ,t) Cσ¯ max ∆(δ),∆(δ)2 .
m 1
z1∈Z1,t∈Rk k ≤ · { }
where ∆(δ) is defined in equation (A.5).
30WewillfirstdemonstratehowLemmaA.4impliesLemmaA.2anddefertheproofofLemmaA.4
to the end. Introduce two notation:
u
z2,z
2′(ξ) = w(z 2,ξ) −w(z 2′,ξ), u z2,0(ξ) = w(z 2,0,ξ).
These notation lead us to the following critical representation:
m
1
R m(z 1,z 2,t) −R m(z 1,z 2′,t) =
m
ψ z1,t(ξ i)u
z2,z
2′(ξ i) −E ψ z1,t(ξ)u
z2,z
2′(ξ)
Xi=1 h i. (A.7)
m
1
R (z ,z ,t) = ψ (ξ )u (ξ ) E ψ (ξ)u (ξ)
m 1 2,0
m
z1,t i z2,0 i
−
z1,t z2,0
i=1
X (cid:2) (cid:3)
Note the similarities between the expressions in equation (A.6) and in equation (A.7). Because
u
z2,z
2′(ξ) and u z2,0(ξ) are subexponential random vectors, the conditions of Lemma A.2 are met.
More precisely, u
z2,z
2′(ξ) is σ kz
2
−z 2′
k
subexponential for every pair of z 2,z 2′
∈
Z2, and u z2,0(ξ) is
σ subexponential. This allows us to leverage Lemma A.4 to conclude that Lemma A.2 follows.
In the remainder, we prove Lemma A.4. Our proof exploits the fact that the VC dimension of
the set family controls the Rademacher complexity associated with the indicators ψ (ξ).
W
z1,t
To deal with outer probability, we need the notion of outer integral [vdVW96, Chapter 1.2].
Definition A.1. Given a probability space (Ξ,G,P), and an arbitrary map T : Ξ R + ,
7→ ∪{ ∞}
we define the outer integral of T by:
E∗[T] = inf E[U] :U T, U : Ξ R + is measurable, and E[U] exists .
{ ≥ 7→ ∪{ ∞} }
Outer integrals exhibit monotone property: if T (ξ) T (ξ) for every ξ, then E∗[T ] E∗[T ].
1 2 1 2
≤ ≤
This is essentially all we need, as the remaining properties of outer integral will be reference from
the textbook [vdVW96]. To ensure clarity when discussing the calculus rules of the outer integral,
we will explicitly reference the relevant chapters in the textbook [vdVW96].
Proof of Lemma A.4. Wedefinethemoment-generating function,usingthenotionof outerintegral
(see Definition A.1):
U(τ) := E∗ exp τ sup W (z ,t) . (A.8)
m 1
"
· z1∈Z1,t∈Rk k
!#
We aim to find a tight upper bound for U(τ) for every τ > 0, which will allow us to infer a high
probability upper bound under the outer measure onto the random variable:
sup W (z ,t) .
m 1
z1∈Z1,t∈Rk k
Let V denote a 1/2 cover of unit ball in Rd with log V dlog6. Following a standard
1/2 1/2
| | ≤
argument (see, e.g., [Ver18, Exercise 4.2.2]), we know that for every v¯ Rd:
∈
v¯ 2 max v¯,v .
k k ≤ v∈V h i
1/2
Applying this inequality to W (z ,t) for every z and t R, we obtain:
m 1 1 1
∈ Z ∈
U(τ) E∗ exp 2τ sup max W (z ,t),v (A.9)
m 1
≤ " · z1∈Z1,t∈R v∈V 1/2h i !#
31We employ symmetrization techniques from empirical process theory to upper bound the RHS.
To simplify the notation, we introduce a function u (ξ) = u(ξ),v , which allows us to express:
v
h i
m
1
W (z ,t),v = ψ (ξ )u (ξ ) E[ψ (ξ)u (ξ)].
h
m 1
i m
z1,t i v i
−
z1,t v
i=1
X
Furthermore, we introduce i.i.d. Rademacher random variables ǫ m with P(ǫ = 1) = 1/2,
{ i }i=1 i ±
independentof thedata samples ξ m . To simplify ournotation, we useǫ to denote thevector
{ i }i=1 1:m
(ǫ ,ǫ ,...,ǫ ) and ξ to denote the vector (ξ ,ξ ,...,ξ ).
1 2 m 1:m 1 2 m
Applyingsymmetrizationtechniquesfromempiricalprocesstheoryfortheouterintegral[vdVW96,
Lemma 2.3.1], we deduce an upper bound for U(τ):
m
1
U(τ) E∗ exp 4τ sup max ǫ ψ (ξ )u (ξ ) (A.10)
≤ " · z1∈Z1,t∈Rv∈V 1/2 m i=1
i z1,t i v i
!#
X
where E∗ is understood as the outer integral on the product probability space involving the data
samplesξ andtheindependentRademacherrandomvariablesǫ . Tobemoreformal,wefollow
1:m 1:m
[vdVW96, Chapter2.3]. Wecanthinkofourdatasamplesξ beingdefinedontheproductproba-
1:m
bilityspace(Ξ⊗m,σ(G⊗m),P⊗m),whereΞ⊗mrepresentsthem-foldCartesianproductofΞ,σ(G⊗m)
represents the σ-field generated by the m-fold product of G, and P⊗m is the m-fold product of P,
where ξ ,ξ ,...,ξ corresponds to the coordinate projections. Then, we introduce Rademacher
1 2 m
randomvariablesǫ = (ǫ ,ǫ ,...,ǫ )definedonanotherprobabilityspace(Ξ′,H ,P′). Wefinally
1:m 1 2 m
define the product probability space of interest:
(Ξ⊗m Ξ′,σ(G H ),P⊗m P′). (A.11)
× ⊗ ⊗
Inthisproductprobabilityspace, thesampleξ correspondstothefirstmcoordinatesprojections
1:m
and the additional Rademacher random variables ǫ dependingonly on the (m+1)st coordinate.
1:m
Lemma A.5 is the critical component that resolves the measurability issue. From the empirical
process theory, we know that measurability of certain forms is needed at this point, as Fubini’s
theorem is not valid for outer integrals [vdVW96, Chapter 2.3]. Lemma A.5 follows the established
ideas to resolve the measurability issues, see, e.g., discussions in the textbook on the measurable
class [vdVW96, Definition 2.3.3]. The proof of Lemma A.5 is deferred to Section A.1.4.
Lemma A.5. The mapping:
m
1
(ξ ,ǫ ) sup max ǫ ψ (ξ )u (ξ )
1:m 1:m
7→ z1∈Z1,t∈Rv∈V
1/2
m
i=1
i z1,t i v i
X
is measurable on the product probability space defined in equation (A.11).
Lemma A.5 provides the justification for replacing the outer integral E∗ on the RHS of equa-
tion (A.10) with the standard integral E. Indeed, equation (A.10) now leads to
m
1
U(τ) E exp 4τ sup max ǫ ψ (ξ )u (ξ ) (A.12)
≤ " · z1∈Z1,t∈Rv∈V 1/2 m i=1
i z1,t i v i
!#
X
where the expectation on the RHS now is the standard expectation, taken jointly over the samples
ξ and the independent Rademacher random variables ξ .
1:m 1:m
32To further upper bound the expectation on the RHS of inequality (A.10), we first consider
conditioning on the samples ξ . This means that we condition on the values of ξ ,ξ ,...,ξ , and
1:m 1 2 m
study the quantity:
m
1
V(τ,ξ )= E exp τ sup max ǫ ψ (ξ )u (ξ )
1:m ǫ1:m
" · z1∈Z1,t∈Rv∈V 1/2 m i=1
i z1,t i v i
!#
X
wherethesymbolE meansthestandardexpectation taken solely over theRademacher variables
ǫ1:m
ǫ . AstheFubini’stheoremholdsforthestandardexpectation, underthisnotation,theinequality
1:m
(A.12) then becomes:
U(τ) E [V(4τ,ξ )] (A.13)
≤
ξ1:m 1:m
where the expectation on the RHS is taken over the samples ξ . That is to say, to upper bound
1:m
U(τ), we first focus on bounding V(τ,ξ ) for every possible value of ξ .
1:m 1:m
Fix ξ . Consider the set:
1:m
H(ξ ):= (ψ (ξ ),ψ (ξ ),...,ψ (ξ )) :z ,t R 0,1 m
1:m
{
z1,t 1 z1,t 2 z1,t m 1
∈
Z1
∈ }⊆ { }
which represents all binary outcomes of applying ψ to the data points ξ across all z .
z1 1:m 1
∈
Z1
Given that the VC-dimension of the set collection
:= ξ Ξ :o(z ,ξ) t z ,t R
1 1 1
W {{ ∈ ≥ } | ∈ Z ∈ }
is upper bounded by vc( ), Sauer-Shelah Lemma then constrains the size of H(ξ ) to obey
1:m
W
H(ξ ) (2m)|vc(W)|. (A.14)
1:m
| | ≤
For a reference of Sauer-Shelah Lemma, see, e.g., [Sau72, She72] or [Ver18, Theorem 8.3.16].
Notably, this definition also allows replacing the supremum over continuous z values with the
1
supremum over the finite binary labeling set H(ξ ). Specifically, there is the upper bound:
1:m
m m
1 1
sup max ǫ ψ (ξ )u (ξ ) max max ǫ b u (ξ ).
z1∈Z1,t∈Rv∈V
1/2
m
i=1
i z1,t i v i
≤ b∈H(ξ1:m)v∈V
1/2
m
i=1
i i v i
X X
On the RHS, b H(ξ ) 0,1 m denotes a vector whose components are b = (b ,b ,...,b ).
1:m 1 2 m
∈ ⊆{ }
This establishes an upper bound on V(τ,ξ ) that applies for every τ > 0:
1:m
m
1
V(τ,ξ ) E exp τ max max ǫ b u (ξ ) .
1:m
≤
ǫ1:m
"
·b∈H(ξ1:m)v∈V
1/2
m
i=1
i i v i
!#
X
By boundingthe exponential of a supremumwith the sumof exponentials across all configurations,
we can further upper bound V(τ,ξ ) and derive:
1:m
m
τ
V(τ,ξ ) E exp ǫ b u (ξ )
1:m
≤
ǫ1:m
m ·
i i v i

!
b∈H X(ξ1:m)v∈ XV 1/2 Xi=1 (A.15)
 m 
τ
= E exp ǫ b u (ξ )
ǫi
m ·
i i v i
b∈H X(ξ1:m)v∈ XV 1/2 Yi=1 (cid:20) (cid:18) (cid:19)(cid:21)
where the symbol E means the expectation is taken solely over the Rademacher variables ǫ .
ǫi i
33Given that for any b 0,1 and q R, the expectation over ǫ obeys:
i i i
∈ { } ∈
E [exp(ǫ b q )] = cosh(b q ) cosh(q ) = E [exp(ǫ q )]
ǫi i i i i i
≤
i ǫi i i
this allows us to upper bound the RHS in equation (A.15), removing the dependence on b :
1:m
m
τ
V(τ,ξ ) E exp ǫ u (ξ )
1:m
≤
ǫi
m ·
i v i
v∈ XV 1/2b∈H X(ξ1:m) Yi=1 h (cid:16) (cid:17)i
m
τ
= H(ξ ) E exp ǫ u (ξ ) .
|
1:m
|
ǫi
m ·
i v i
v∈ XV 1/2Yi=1 h (cid:16) (cid:17)i
Given the bound we have established for H(ξ ) (2m)|vc(W)| in equation (A.14), we deduce:
1:m
| | ≤
m
τ
V(τ,ξ ) (2m)vc(W) E exp ǫ u (ξ ) . (A.16)
1:m
≤
ǫi
m ·
i v i
v∈ XV 1/2Yi=1 h (cid:16) (cid:17)i
Notably, this bound applies to every possible ξ value.
1:m
Finally, we substitute this bound into inequality (A.13), resulting in:
m
4τ
U(τ)
≤
E ξ1:m[V(4τ,ξ 1:m)]
≤
(2m)vc(W) E ǫi,ξi exp
m
·ǫ iu v(ξ i) (A.17)
v∈V i=1 (cid:20) (cid:18) (cid:19)(cid:21)
X1/2Y
where E denotes the joint expectation taken over ǫ and ξ .
ǫi,ξi i i
Sinceu (ξ)isσ¯-sub-exponential,thenǫ u (ξ )isalsoσ¯-sub-exponentialforeveryv bydefinition.
v i v i
Using [Ver18, Proposition 2.7.1(e)], this implies that, for every θ , v, τ (0,cm/σ¯):
2
∈
4τ σ¯2τ2
E exp ǫ u (ξ ) exp C
ǫi,ξi m · i v i ≤ m2
(cid:20) (cid:18) (cid:19)(cid:21) (cid:18) (cid:19)
where c,C > 0 are absolute constants. Because V 6d, we further obtain for τ (0,cm/σ¯):
1/2
| |≤ ∈
σ¯2τ2
U(τ) (2m)vc(W)6dexp C . (A.18)
≤ m
(cid:18) (cid:19)
Recall the definition of U(τ), which yields:
σ¯2τ2
E∗ exp τ sup W (z ,t) (2m)vc(W)6dexp C .
m 1
"
· z1∈Z1,t∈Rk k
!#
≤
(cid:18)
m
(cid:19)
By applying Markov’s inequality underthe outer integral [vdVW96] (which follows from the mono-
tone property of outer integral discussed below Definition A.1), we establish that there exists a
universal constant c > 0 such that for every u 0:
≥
mu2 mu
P∗ sup W (z ,t) u (2m)vc(W)6d exp cmin , .
z1∈Z1,t∈Rk m 1 k ≥
!
≤ · (cid:18)−
(cid:26)
σ¯2 σ¯
(cid:27)(cid:19)
Equivalently, there exists a universal constant C > 0 such that for every δ (0,1), the following
∈
happens with an inner probability at least 1 δ:
−
sup W (z ,t) Cσ¯ max ∆(δ),∆(δ)2 ,
m 1
z1∈Z1,t∈Rk k ≤ · { }
where ∆(δ) is defined in equation (A.5).
34A.1.3 A Basic Bound
This section proves Lemma A.3. By definition:
1 1
∆(δ ) = d+vc( )logm+log( )+log(2k+1)+log +log .
k
sm · W δ
|Z2,ǫk| |Z2,ǫk−1|
(cid:18) (cid:19)
Recall that log dlog(2+r/ǫ ) dlog(2+2k). This yields that
|Z2,ǫk|
≤
k
≤
1 1 12kd
∆(δ ) d+vc( )logm+log( ) +
k
≤ sm ·
(cid:18)
W δ
(cid:19) r
m
As a result, there exists a universal constant c > 0 such that
∞ ∞ ∞
1 1 12kd
ǫ ∆(δ ) ǫ d+vc( )logm+log( ) + ǫ
k k k k
k=1
≤
k=1
!·sm ·
(cid:18)
W δ
(cid:19) k=1 r
m
X X X
1 1
cr d+vc( )logm+log( ) = cr∆(δ).
≤ ·sm · W δ
(cid:18) (cid:19)
Similarly, one can prove the existence of a universal constant c> 0 such that
∞
ǫ ∆(δ )2 cr∆(δ)2.
k k
≤
k=1
X
This then concludes the existence of a universal constant c > 0 such that
∞ ∞ ∞
ǫ max ∆(δ ),∆(δ )2 ǫ ∆(δ )+ ǫ ∆(δ )2 2crmax ∆(δ),∆(δ)2 .
k k k k k k k
{ } ≤ ≤ { }
k=1 k=1 k=1
X X X
A.1.4 A Result on Measurability
WeproveLemmaA.5. Let o andRo beanycountabledensesubsetsof andR,respectively. Our
Z1 Z1
main observation is that the supremum over the uncountable sets ,R is equal to the supremum
1
Z
over the countable sets o,Ro, holding for every data instances ξ ,ǫ :
Z1 1:m 1:m
m m
1 1
sup max ǫ ψ (ξ )u (ξ ) = sup max ǫ ψ (ξ )u (ξ ) (A.19)
z1∈Z1,t∈Rv∈V 1/2 m i=1
i z1,t i v i
z1∈Z 1o,t∈Rov∈V 1/2 m i=1
i z1,t i v i
X X
Since the RHS is the pointwise supremum over countable measurable functions of ξ ,ǫ , it is
1:m 1:m
measurable. Therefore, the LHS must also be measurable under the product space. To complete
the proof of Lemma A.5, it suffices to prove equation (A.19).
Fix the values of ξ and ǫ . It suffices to show that for every z and t R, there exist
1:m 1:m 1 1
∈ Z ∈
zo o and to Ro such that for every i= 1,2,...,m:
1 ∈ Z1 ∈
ψ z1,t(ξ i)= ψ
z
1o,to(ξ i). (A.20)
Recall that ψ (ξ) = 1 o(z ,ξ) t . First, we note an important observation. For every z , there
z1,t
{
1
≥ }
1
exists to Ro with to < t such that for every i = 1,2,...,m:
∈
ψ (ξ ) = ψ (ξ ) and o(z ,ξ ) to = 0.
z1,t i z1,to i 1 i
− 6
35Fix this to. Next, since z o(z ,ξ) is continuous for every ξ, this implies the existence of zo o
1 7→ 1 1 ∈ Z1
close to z such that for every i = 1,2,...,m:
1
ψ z1,to(ξ i) = ψ
z
1o,to(ξ i).
This proves that for every z and t R, there is the existence of the pair zo o and to Ro
1 ∈ Z1 ∈ 1 ∈Z1 ∈
such that equation (A.20) holds for all i= 1,2,...,m. This completes the proof of Lemma A.5.
B Proof of Theorem 5.2
Our approach comprises two main steps. First, we demonstrate that every stationary point of the
empiricalobjective Φ mustbeapproximately stationary forthepopulationobjective Φ, leveraging
S
the uniform convergence of subdifferentials in Theorem 5.1. Second, we utilize established results
from the literature [DDP20, Section 5] on these (approximate) stationary points of Φ to pinpoint
the location of empirical stationary points.
To set up the stage, we recall the location of the stationary point and approximate stationary
point of the population objective Φ formally derived in [DDP20].
Theorem B.1 ([DDP20, Theorem 5.2]). The stationary points of the population objective Φ are
0 x¯ x : x,x¯ = 0, x = c x¯
{ }∪{± }∪{ h i k k ·k k}
where c> 0 is the unique solution of the equation π = c +arctan(c).
4 1+c2
The following Theorem B.2 can be easily deduced from [DDP20, Theorem 5.2+Theorem 5.3].
Recall that = x :0 ∂Φ(x) denotes the set of stationary points of Φ.
Z { ∈ }
Theorem B.2 ([DDP20, Theorem 5.3]). There exist numerical constants γ,C > 0 such that the
following holds. For any point x Rd with
∈
ǫ = dist(0;∂Φ(x)) γ x ,
≤ k k
it must be the case that x C x¯ and x satisfies dist(x, ) Cdist(0,∂Φ(x)).
k k≤ k k Z ≤
The following result is immediate given [DDP20, Corollary 6.3].
Proposition B.3 ([DDP20, Corollary 6.3]). There exist a numerical constant C > 0 so that for
m Cd, with an inner probability at least 1 2exp( d)1, the set of stationary points of Φ
S S
≥ − − Z
satisfies:
m
( , ) C 4 x¯ .
S
D Z Z ≤ d k k
r
We shall build on top of these existing results to establish Theorem 5.2. First, Proposition B.3
implies there exists a numerical constant C > 0 such that:
1
P ( x C x¯ x ) 1 2e−d. (B.1)
∗ 1 S
k k ≤ k k ∀ ∈ Z ≥ −
Corollary B.4 refines Theorem 5.1 using a standard peeling argument to substitute the norm x
k k
for the constant radius r in the original probability expression in equation (5.2). This adjustment
allows varying distances x from the origin, ranging from r to r, enhancing the applicability of the
probability bounds over different scales. For completeness, we give its proof in Section B.1.
1The original expression in thepaper[DDP20], 1−2exp(−min{c m,d2}), should becorrected to 1−2exp(−d).
2
36Corollary B.4. Assume the measurement vector a is σ-subgaussian, and E[b 2]< .
| | ∞
Then there exists a universal constant C > 0 such that for every δ (0,1) and r r > 0:
∈ ≥
P H(∂Φ(x),∂Φ (x)) Cσ x max ∆˜ ,∆˜2 x satisfying r x r 1 δ. (B.2)
∗ S ≤ k k· { Φ Φ} ∀ ≤ k k ≤ ≥ −
(cid:16) (cid:17)
In the above,
1 log(er/r)
∆˜ = dlogdlogm+log( ) . (B.3)
Φ
sm · δ
(cid:18) (cid:19)
We are now ready to prove Theorem 5.2. We pick
d
r = C x¯ , r = C x¯ , δ = e−d, σ =4
1 1
m k k k k
r
in Corollary B.4. We then derive for some absolute constant C :
2
P H(∂Φ(x),∂Φ (x)) C x max ∆ ,∆2 ,
∗ S ≤ 2 k k· { 0 0}
(cid:18) (B.4)
x satisfying C d/m x¯ x C x¯ 1 e−d.
1 1
∀ k k≤ k k ≤ k k ≥ −
(cid:19)
p
where
d
∆ = logdlogm.
0
m
r
We without loss of generality assume that C max ∆ ,∆2 γ where γ is the numerical constant
2 { 0 0} ≤
specified in Theorem B.2, since otherwise Theorem 5.2 trivially follows from equation (B.1).
With equations (B.1) and (B.4), we know that with an inner probability at least 1 3e−d, the
−
following simultaneously happen:
1. x C x¯ for every x .
1 S
k k≤ k k ∈Z
2. H(∂Φ(x),∂Φ (x)) C x max ∆ ,∆2 for every x with C d/m x¯ x C x¯ .
S ≤ 2 k k· { 0 0} 1 k k≤ k k ≤ 1 k k
Consequentially, on this event, any stationary point x∗ of Φ
S
must eipther obey:
x∗ C d/m x¯
1
k k ≤ k k
or satisfies p
H(∂Φ(x∗),∂Φ (x∗)) C x∗ max ∆ ,∆2 .
S ≤ 2 k k· { 0 0}
In the first case, since 0 by Theorem B.1, we immediately obtain:
∈ Z
dist(x∗, ) x∗ C d/m x¯ .
1
Z ≤ k k ≤ k k
In the second case, since 0 ∂Φ (x∗), then dist(0,∂Φ(xp∗)) H(∂Φ(x∗),∂Φ (x∗)) by definition of
S S
∈ ≤
Hausdorff distance, and thus:
dist(0,∂Φ(x∗)) C x∗ max ∆ ,∆2 .
≤ 2 k k· { 0 0}
Since we have assumed C max ∆ ,∆2 γ, by Theorem B.2 we obtain that x∗ must obey
2 { 0 0} ≤
dist(x∗, ) C x∗ max ∆ ,∆2 C C x¯ max ∆ ,∆2 .
Z ≤ 2 k k· { 0 0}≤ 1 2 k k· { 0 0}
where we recall that x C x¯ for every x on this event.
1 S
k k≤ k k ∈ Z
Summarizing, we have established with an inner probability at least 1 3e−d, any stationary
−
point x∗ of Φ must obey:
S
dist(x∗, ) C x¯ max ∆ ,∆2
Z ≤ k k· { 0 0}
where C > 0 is an absolute constant. This completes the proof of Theorem 5.2.
37B.1 Proof of Corollary B.4
We can define a sequence r(i) N so that r(0) = r, r(N) = r, r(i+1)/r(i) e and N log(er/r).
{ }i=1 ≤ ≤
Under the condition of Corollary B.4, we can apply Theorem 5.1 and obtain for every 1 i N
≤ ≤
P∗ sup H(∂Φ(x),∂Φ (x)) Cσr(i) max ∆˜ ,∆˜2 δ/N
S ≥ · { Φ Φ} ≤
x:kxk≤r(i) !
In the above, C is a universal constant. A union bound shows that:
P sup H(∂Φ(x),∂Φ (x)) Cσr(i) max ∆˜ ,∆˜2 for every 1 i N 1 δ.
∗ S ≤ · { Φ Φ} ≤ ≤ ≥ −
x:kxk≤r(i) !
On this event, for every x with r x r, we find 1 i(x) N such that r(i(x)−1) x r(i(x)),
≤ k k ≤ ≤ ≤ ≤ ≤
and then we obtain that
H(∂Φ(x),∂Φ (x)) Cσr(i(x)) max ∆˜ ,∆˜2 Ceσ x max ∆˜ ,∆˜2
S ≤ · { Φ Φ} ≤ k k { Φ Φ}
where the last inequality is due to r(i(x))/x r(i(x))/r(i(x)−1) e by construction. Hence, we have
≤ ≤
proven the existence of a numerical constant C′ > 0 such that:
P H(∂Φ(x),∂Φ (x)) C′σ x max ∆˜ ,∆˜2 for all x such that r x r 1 δ.
∗ S ≤ k k { Φ Φ} ≤ k k≤ ≥ −
(cid:16) (cid:17)
This completes the proof of Corollary B.4.
C Proof of Corollary 5.3
We apply Theorem 4.2. We first check the assumptions. For the nonsmooth h(z) = z , it is
| |
decomposed as h= hns+hsm where hsm(z) = z and hns(z) = 2(z) .
+
−
• (Assumption C.1). For the matrix sensing problem,
c(X,ξ) = (A+AT)X, (hsm)′(c(X;ξ)) c(X;ξ) = (A+AT)X.
∇ ∇ −
Thus, at X = 0 Rd×r, both gradients vanish: c(0,ξ) = (hsm)′(c(0;ξ)) c(0;ξ) = 0.
∈ ∇ ∇
• (Assumption C.2). It is easy to verify that
c(X ,ξ) c(X ,ξ) = (A+AT)(X X )
1 2 1 2
∇ −∇ − ,
e(X ,ξ) e(X ,ξ) = (A+AT)(X X )
1 2 2 1
− −
where e(X,ξ) = (hsm)′(c(X;ξ)) c(X;ξ). For every matrix V RD×D, A,V is V -
∇ ∈ h i k kF
subgaussian. Given the property that the sum of two subgaussian random variables is still
subgaussian [Ver18, Exercise 2.5.7], we deduce that A+AT,V is 2 V -subgaussian for
h i k kF
every matrix V. As a consequence, we derive that, each of
V, c(X ,ξ) c(X ,ξ) = (A+AT),(X X )TV
1 2 1 2
h ∇ −∇ i h − i,
V,e(X ,ξ) e(X ,ξ) = (A+AT),(X X )TV
1 2 2 1
h − i h − i
must be2 X X subgaussian, and thus C X X subexponentialfor every matrix
k 1 − 2 kF k 1 − 2 kF
V RD×D with V = 1 for some universal constant C > 0. Here we use the fact that
∈ k kF
every sub-gaussian distribution is subexponential [Ver18, Section 2.7],
38• (Integrability). Notably E[c(X;ξ)2]= E[b a,X 2]< for every x Rd.
| | | −h i| ∞ ∈
We then compute the bound in Theorem 4.2. Namely, we need to compute ζ and vc( ).
F
For the nonsmooth function h(z) = z , the corresponding value of ζ is given by ζ = 3 as 0 is
| |
the only nondifferentiable point of h. The corresponding is given by
F
= A RD×D,b R : A,XXT b t X RD×r0,t R .
F { ∈ ∈ h i− ≥ } | ∈ ∈
Note that X A,X(cid:8) XT b is a degree 2 polynomial in X for every A,b. We t(cid:9) hen bound its VC
7→ h i−
dimension using Theorem 6.1, which yields that for some universal constant C > 0:
vc( ) CDr log(Dr ).
0 0
F ≤
Corollary 5.3 then follows by applying Theorem 6.1.
39