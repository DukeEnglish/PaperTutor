Beyond Theorems: A Counterexample to Potential Markov
Game Criteria
FATEMEHFARDNO,UniversityofWaterloo,Canada
SEYEDMAJIDZAHEDI,UniversityofWaterloo,Canada
Thereareonlylimitedclassesofmulti-playerstochasticgamesinwhichindependentlearningisguaranteed
toconvergetoaNashequilibrium.Markovpotentialgamesareakeyexampleofsuchclasses.Priorworkhas
outlinedsetsofsufficientconditionsforastochasticgametoqualifyasaMarkovpotentialgame.However,
theseconditionsoftenimposestrictlimitationsonthegame’sstructureandtendtobechallengingtoverify.
Toaddress these limitations, Mguniet al. [12] introducea relaxed notionofMarkovpotentialgames and
offeranalternativesetofnecessaryconditionsforcategorizingstochasticgamesaspotentialgames.Under
these conditions, the authors claim that a deterministic Nash equilibriumcan be computed efficiently by
solvingadualMarkovdecisionprocess.Inthispaper,weofferevidencerefutingthisclaimbypresentinga
counterexample.
1 INTRODUCTION
Inrecentyears,therehasbeenagrowinginterestinapplyingmulti-agentreinforcementlearning
to find strategies thatconverge to a Nash equilibrium in multi-player stochastic games [2, 5, 8–
11,14].Insingle-agentenvironments,manylearningalgorithmsareguaranteedtoconvergetoop-
timalpoliciesundersomemildconditions[15–17].However,deployingindependentsingle-agent
learning algorithmsin a multi-agent environment does not guaranteefinding Nash equilibrium
policies[6].Themainreasonforthisisthat,fromtheperspectiveofalearningagent,theenviron-
ment undergoes constant changes as other agents concurrently learn, and these environmental
changespartlydependontheactionsofthelearningagent.
Anotherchallengeindeployingmulti-agentlearningisefficiency.Thecomputationofastation-
aryNashequilibriumforgeneralstochasticgamesisknowntobecomputationallyintractable[3].
Consequently,therearenoefficientmulti-agentlearningalgorithmsforlearningstationaryNash
equilibriumstrategiesingeneralstochasticgames.However,therearespecificclassesofstochas-
tic gameswhereNash equilibrium strategies couldbecomputedefficiently.One notableclassis
Markovpotentialgames(MPGs),withinwhichmulti-agentlearning,andspecificallyindependent
learning,exhibitspromisingconvergenceproperties[7,10,11].
AnMPGischaracterizedbytheexistenceofaglobalfunction,knownasthepotentialfunction,
whereachangeinanagent’slong-termpayoffduetoaunilateralchangeintheagent’sstrategy
equatestothechangeinthepotentialfunction.Thisuniquepropertyfacilitatestheuseofefficient
multi-agentlearningmethods,suchastheindependentpolicygradient,ensuringconvergenceto
a stationary Nash equilibrium strategy [10]. However, determining whether a stochastic game
qualifiesasanMPGbysearchingforsuchapotentialfunctionisnotalwaysstraightforward.Con-
sequently,giventhehighlydesirablepropertiesofMPGs,itbecomesessentialtoidentifysufficient
conditionsforcategorizinggamesasMPGs.
Existingresearchhasestablishedsetsofsufficientconditionsforidentifyingastochasticgame
asanMPG[10,11].However,theseconditionsoftenimposestringentrestrictionsonthegame’s
structure andmaynot beeasy to verify. To address these limitations, ina recentwork[12],the
authorsintroducea relaxednotionofMPGsandoffer analternative setofnecessaryconditions
forcategorizingstochasticgamesaspotentialgames.Theauthorsclaimthatmeetingthesecon-
ditions ensures theexistence of a deterministic stationary Nash equilibrium in thegame,which
Authors’addresses:FatemehFardno,UniversityofWaterloo,Waterloo,Canada,ffardno@uwaterloo.ca;SeyedMajidZa-
hedi,UniversityofWaterloo,Waterloo,Canada,smzahedi@uwaterloo.ca.
4202
yaM
31
]TG.sc[
1v60280.5042:viXracorrespondstotheoptimalsolutionofasingle-agentMarkovdecisionprocess(MDP)constructed
basedontheoriginalstochasticgame.Essentially,thedeterministicstationaryNashequilibrium
of the original stochastic game can be efficiently computed by solving its (dual) MDP. This, in
turn,guaranteestheconvergenceofmulti-agentlearningmethodstoNashequilibriumstrategies.
In this paper, we scrutinize this key claim and present a counterexample to Theorem 1 in [12],
establishingacasewherethetheoremdoesnothold.
2 BACKGROUNDANDRELATEDWORK
In this section, we first briefly introduce stochastic games. We then provide some background
on Markovpotential games. We then discuss sufficient conditions for a stochastic gameto be a
Markovpotentialgame.
2.1 StochasticGames
WestartbydefiningMarkovdecisionprocesses(MDP)asatooltostudydecisionmakinginsingle-
agentenvironments:
Definition1 (MDP). AMarkov decision processis atuple (𝑆,𝐴,𝑟,𝑝).𝑆 is the state space.𝐴 is
theactionspace.𝑟 : 𝑆 ×𝐴 ↦→ Risthepayofffunction.And𝑝 : 𝑆 ×𝐴×𝑆 ↦→ [0,1] isthetransition
probabilityfunction.
Please note that in this definition, the action space is assumed to be the same across all states.
Relaxingthisassumptionintroducesadditionalnotation,butapartfromthat,itdoesnotposeany
significantdifficultyorinsight.
ForanyMDP,astationarystrategy𝜋 :𝑆×𝐴↦→ [0,1]mapseachstate-actionpairtoaprobability.
Wewrite𝜋(𝑎|𝑠)todenotetheprobabilityoftakingaction𝑎instate𝑠understrategy𝜋.Astrategy
𝜋 iscalleddeterministicifforallstates𝑠 ∈𝑆,thereexistsanaction𝑎 ∈𝐴forwhich𝜋(𝑎 |𝑠) =1.
Stochasticgames(a.k.a.Markovgames)extendMDPstomulti-agentenvironments:
Definition2(Stochasticgame). An𝑛-agentstochastic gameisatuple (𝑁,𝑆,𝑨,𝒓,𝑝)1.𝑁 is
thesetofagents.𝑆 isthestatespace.𝑨=𝐴 ×···×𝐴 ,where𝐴 istheactionspaceofagent𝑖 ∈ 𝑁.
1 𝑛 𝑖
𝒓 =𝑟 ×···×𝑟 ,where𝑟 :𝑆×𝑨↦→Risthepayofffunctionforagent𝑖 ∈𝑁.And𝑝 :𝑆×𝑨×𝑆 ↦→ [0,1]
1 𝑛 𝑖
isthetransitionprobabilityfunction.
Noteagainthattheactionspaceofeachagentisassumedtobethesameacrossallstates.Similar
toDefinition1,thisassumptioncanbeeasilyremoved.
Weuse𝜋 to denotethestrategy of agent𝑖.Thejointstrategy profile ofall agentsis denoted
𝑖
by 𝝅 = 𝜋 × ··· ×𝜋 . And the joint strategy profile of all agents except agent𝑖 is denoted by
1 𝑛
𝝅 =𝜋 ×···×𝜋 ×𝜋 ×···×𝜋 2.
−𝑖 1 𝑖−1 𝑖+1 𝑛
Ininfinite-horizonstochasticgames,thelong-termvalueofstate𝑠 toagent𝑖 forstrategy𝝅 is
theexpectedsumofagent𝑖’sdiscountedpayoffs:
∞
𝑉 𝑖𝝅 (𝑠) ,E 𝝅 𝛾𝑡𝑟 𝑖(𝑠 𝑡,𝒂 𝒕) 𝑠 0 =𝑠 , (1)
" Õ𝑡=0
(cid:12)
#
whereE 𝝅[·]denotestheexpectedvalueofarandomvariable(cid:12) (cid:12) giventhatagentsfollowjointstrategy
profile𝝅,and𝛾 isthediscountfactor3.Agentsareconsideredtobeself-interested. Eachagent’s
objectiveistomaximizeitsownlong-termvalue.Abest-responsestrategyisastrategythatachieves
1Weuseboldfonttorepresentvectors.
2Weusethenotation−𝑖toindicateallagentsexceptagent𝑖.
3𝛾determineshowmuchagentsdiscountfuturepayoffs.thehighestvalueforanagentgivenotheragents’strategies[1].Nashequilibriumisajointstrategy
whereeachagent’sstrategyisabestresponsetoothers’:
Definition3(𝝐-Nasheqilibrium). Let𝜖 ≥ 0.Theninan𝑛-agentstochasticgame,an𝜖-Nash
equilibriumisastrategyprofile𝝅∗ =𝜋∗,...𝜋∗ suchthat:
1 𝑛
𝑉𝝅∗
(𝑠)
≥𝑉(𝜋𝑖,𝝅 −∗ 𝑖)
(𝑠)−𝜖
𝑖 𝑖
forallstates𝑠 ∈𝑆,allagents𝑖 ∈𝑁,andallstrategies𝜋 ∈ Π ,whereΠ isthesetofallstrategiesfor
𝑖 𝑖 𝑖
agent𝑖.
When𝜖
=0,wesimplycallthisaNashequilibrium.ANashequilibriumstrategy𝝅∗
isstationary
if 𝜋∗ is stationary for all agents𝑖 ∈ 𝑁. All stochastic games have at least one stationary Nash
𝑖
equilibrium[4].
Computing a stationary 𝜖-Nash equilibrium for general stochastic games is computationally
intractable[3]4.Thisimpliesthattherearenoefficientmulti-agentreinforcementlearningalgo-
rithmsforlearningstationaryNashequilibriumstrategiesingeneralstochasticgames.However,
therearespecificclassesofstochasticgamesforwhichNashequilibriumstrategiescouldbecom-
putedefficiently.AkeyexampleisMarkovpotentialgames(MPG).
2.2 MarkovPotentialGames
MondererandShapleyintroducetheconceptofpotentialgamesinthenormalform[13].Potential
gamesrepresent multi-agentcoordination,asall agents’ payoffsareperfectlyalignedwith each
other via a potential function. MPGs extend the concept of potential games from normal-form
gamestostochasticgames.AstochasticgamequalifiesasanMPGifthereexistsaglobalfunction,
calledthepotentialfunction,suchthatifanyagentunilaterallychangestheirstrategy,thechange
in their long-term value for each state mirrors precisely the change observed in the potential
functionatthatparticularstate:
Definition4(MPG). AstochasticgameisanMPGifthereexistsastrategy-dependentfunction
𝜙𝝅 :𝑆 ↦→Rforstrategies𝝅 ∈ 𝚷suchthat:
𝑉𝝅 (𝑠)−𝑉(𝜋𝑖′,𝝅 −𝑖) (𝑠) =𝜙𝝅 (𝑠)−𝜙(𝜋𝑖′,𝝅 −𝑖)(𝑠)
𝑖 𝑖
forallagents𝑖 ∈𝑁,allstates𝑠 ∈𝑆,allstrategies𝝅 = (𝜋 ,𝝅 ) ∈ 𝚷,andallstrategies𝜋′ ∈ Π .
𝑖 −𝑖 𝑖 𝑖
AnyMPGhasatleastonestationary Nashequilibriumstrategyprofilethatisdeterministic [10].
Furthermore,independentlearningconvergestoan𝜖-NashequilibriumstrategyinMPGs:
Theorem1([10,Theorem1.1]). Inan𝑛-agentMPG,ifallagentsrunindependentpolicygradi-
ent,thenforany𝜖 > 0,thelearningdynamicsreachesan𝜖-Nashequilibriumstrategyafter𝑂(1/𝜖2)
iterations.
Themain idea behind the aforementioned theorem is that in MPGs, applying projected gradient
ascent (PGA) on thepotential function𝜙 leads to the emergenceof an𝜖-Nash equilibrium. And
the key element in the proof of this theorem is the equality of the derivatives between value
functions and the potential function in MPGs. More recently, in [7], the authors show that in
anMPG,independentnaturalpolicygradientalsoconvergestoanequilibrium.
4Infact,theauthors showthat computing astationarycoarse-correlated equilibrium,whichisamore relaxednotion
comparedtoNashequilibrium,ingeneralstochasticgamesiscomputationallyintractable.2.3 FromStochasticGamestoMPG
InDefinition4,theconditionisfairlystronganddifficulttoverifyinpracticeforgeneralstochas-
tic games. Given the MPGs’ desiderata, it becomes imperative to delineate the specific types of
stochasticgamesthatalignwiththecriteriaoutlinedinDefinition4.Tothisend,priorworkhas
providedsetsofsufficientconditions[10,11].Todiscusstheseconditions,wefirstneedtodefine
theclassofone-shotpotentialstochasticgames(OPSGs).WedefineastochasticgameasanOPSG
ifimmediatepayoffsatanystatearecapturedbyapotentialgameatthatstate:
Definition5(OPSG). An𝑛-aagentstochastic gameisOPSGifthereexists aone-shotpotential
functionΦ:𝑆×𝑨↦→Rsuchthat:
𝑟 (𝑠,𝒂)−𝑟 (𝑠,𝑎′,𝒂 ) =Φ(𝑠,𝒂)−Φ(𝑠,𝑎′,𝒂 )
𝑖 𝑖 𝑖 −𝑖 𝑖 −𝑖
forall𝑖 ∈𝑁,allstates𝑠 ∈𝑆,allactionprofiles𝒂 = (𝑎 ,𝒂 ) ∈𝑨,andallactions𝑎′ ∈𝐴 .
𝑖 −𝑖 𝑖 𝑖
In[10],theauthorsshowthatanOPSGisMPGifeitherofthetwofollowingconditionshold:
(i)agent-independenttransitionsand(ii)equalityofindividualdummyterms.(i)holdsifthegame’s
transitionprobabilityfunctiondoesnotdependonagents’jointaction:
Condition1(Agent-independenttransitions). AnOPSGhasagent-independenttransi-
tionsifforallstates𝑠,𝑠′ ∈𝑆 andactionprofiles𝒂 ∈𝑨:
𝑝(𝑠′ |𝑠,𝒂) =𝑝(𝑠′ |𝑠).
And(ii)holdsifthedummytermsofeachagent’simmediatepayoffsareequalacrossallstates:
Condition2(Eqalityof individual dummy terms). An OPSGwith one-shot potential
function Φ satisfies the equality of individualdummy terms iffor each agent𝑖 ∈ 𝑁, there exists a
function𝑣 :𝑆×𝑨 ↦→Rsuchthat:
𝑖 −𝑖
𝑟 (𝑠,𝑎 ,𝒂 ) =Φ(𝑠,𝑎 ,𝒂 )+𝑣𝑖(𝑠,𝒂 ),
𝑖 𝑖 −𝑖 𝑖 −𝑖 −𝑖
and
∞
∇ E 𝛾𝑡𝑣 (𝑠 ,𝒂𝑡 ) 𝑠 =𝑠 =𝑐 1
𝜋𝑖(𝑠) 𝑖 𝑡 −𝑖 0 𝑠
" Õ𝑡=0
(cid:12)
#
forallstates𝑠,𝑠′ ∈𝑆,𝑐
𝑠
∈R,and1 ∈R|𝐴𝑖|,where𝜋 𝑖(𝑠)(cid:12) (cid:12)isthestrategyofagent𝑖 atstate𝑠.
In[12],the authorsarguethat Condition 1 and Condition 2 impose strong limitations onthe
structureofone-shotpotentialstochasticgames.Toavoidtheselimitations,theauthorspropose
analternativecondition:
Condition3(Statetransitivity). AnOPSGwithone-shotpotentialfunctionΦsatisfiesstate
transitivityifwehave:
𝑟 (𝑠,𝒂)−𝑟 (𝑠′,𝒂) =Φ(𝑠,𝒂)−Φ(𝑠′,𝒂)
𝑖 𝑖
forallagents𝑖 ∈𝑁,allstates𝑠,𝑠′ ∈𝑆,andallactionprofiles𝒂 ∈𝐴.
Statetransitivityensurethatthedifferenceinimmediatepayoffsforchangingstateisthesamefor
eachagent.Theauthorsthenpresentatheoremthatclaimsthefollowing.
Claim1([12,Theorem1]). Let𝐺 ≔ (𝑁,𝑆,𝑨,𝒓,𝑝)beanOPSGwithone-shotpotentialfunction
Φ. Suppose that 𝐺 satisfies Condition 3. Then 𝐺 has a deterministic stationary Nash equilibrium
that correspondsto the optimal solution of the dual MDP definedas𝐺′ ≔ (𝑆,𝑨,Φ,𝑝).That is, the
deterministicstationaryNashequilibriumof𝐺 canbeefficientlycomputedbysolving𝐺′.3 ANALYSIS
Inthissection,weprovideacounterexampleforwhichClaim1failstohold.
3.1 Counterexample
Since action space and state space are assumed to be continuous in [12], our counterexample
includescontinuousactionandstatespaces.Similarly,since[12]focusesoninfinite-horizongames,
ourcounterexampleconsidersaninfinite-horizongame.
Considergame𝐺,aninfinite-horizon,two-agentstochasticgamedefinedas:
• Thesetofagentsis𝑁 ={1,2}.
• Thecontinuousstatespaceis𝑆 = [0,1].
• Theactionspacesare𝐴 =𝐴 = [0,1].
1 2
• Thepayofffunctionsforanystate𝑠 ∈𝑆 andanyactionprofile𝑎 = (𝑎 ,𝑎 ) are:
1 2
𝑟 (𝑠,𝑎 ,𝑎 ) =𝑠−(𝑠−𝑎 )2−4/(2−𝑎 ),
1 1 2 2 2
and
𝑟 (𝑠,𝑎 ,𝑎 ) =𝑠−(𝑠−𝑎 )2.
2 1 2 2
• Thestatetransitionfunctiononlydependsontheactionofagent1andcanbewrittenas:
1 if𝑠′ =𝑎 , and
𝑝(𝑠′ |𝑠,𝑎 ,𝑎 ) = 1
1 2
(0 otherwise.
It can be easily verified the aforementioned game𝐺 satisfies all the assumptions in [12]. In
particular,thepayofffunctionsarebounded,measurablefunctionsintheactions,Lipschitz, and
continuouslydifferentiableinthestateandactions.
Next,weshowthat𝐺 isanOPSG.Wedothisbyshowingthatimmediatepayoffsatanystate
arecapturedbyapotentialgameatthatstate.Considerthefollowingpotentialfunction:
Φ(𝑠,𝑎 ,𝑎 ) =𝑠−(𝑠−𝑎 )2.
1 2 2
Itiseasytoseethat𝐺 isapotentialgameateachstate𝑠 ∈𝑆 withthepotentialfunctionΦ(𝑠,·):
𝑟 (𝑠,𝑎 ,𝑎 )−𝑟 (𝑠,𝑎′,𝑎 ) =Φ(𝑠,𝑎 ,𝑎 )−Φ(𝑠,𝑎′,𝑎 ) =0,
1 1 2 1 1 2 1 2 1 2
and
𝑟 (𝑠,𝑎 ,𝑎 )−𝑟 (𝑠,𝑎 ,𝑎′) =Φ(𝑠,𝑎 ,𝑎 )−Φ(𝑠,𝑎 ,𝑎′) = (𝑠−𝑎′)2−(𝑠−𝑎 )2.
2 1 2 2 1 2 1 2 1 2 2 2
Itisalsoeasytoseethat𝐺satisfiesCondition3forallstates𝑠,𝑠′ ∈𝑆andactionprofiles𝑎 = (𝑎 ,𝑎 ):
1 2
𝑟 (𝑠,𝑎 ,𝑎 )−𝑟 (𝑠′,𝑎 ,𝑎 ) =Φ(𝑠,𝑎 ,𝑎 )−Φ(𝑠′,𝑎 ,𝑎 ) = (𝑠−𝑠′)−(𝑠−𝑎 )2+(𝑠′−𝑎 )2,
1 1 2 1 1 2 1 2 1 2 2 2
and
𝑟 (𝑠,𝑎 ,𝑎 )−𝑟 (𝑠′,𝑎 ,𝑎 ) =Φ(𝑠,𝑎 ,𝑎 )−Φ(𝑠′,𝑎 ,𝑎 ) = (𝑠−𝑠′)−(𝑠−𝑎 )2+(𝑠′−𝑎 )2.
2 1 2 2 1 2 1 2 1 2 2 2
Forcontradiction,letusassumethatClaim1holdsfor𝐺.Thenwecanconstruct𝐺’sdualMDP,
𝐺′,asfollows.Theactionspaceis𝑨 =𝐴 ×𝐴 .Theactionateachstateis𝒂 = (𝑎 ,𝑎 ) ∈ 𝑨.And
1 2 1 2
thepayofffunctionis:
𝑟(𝑠,(𝑎 ,𝑎 )) =Φ(𝑠,𝑎 ,𝑎 ) =𝑠−(𝑠−𝑎 )2.
1 2 1 2 2
Finally,𝐺′hasthesametransitionprobabilityfunctionas𝐺.ItcanbeeasilyshownthatthisMDP
hasthefollowingunique(deterministic)optimalstrategy:
1 if (𝑎 ,𝑎 ) = (1,𝑠), and
𝝅∗((𝑎 ,𝑎 ) |𝑠) = 1 2 (2)
1 2
(0 otherwise.Thisoptimaljointstrategyprescribestaking𝑎 =1and𝑎 =𝑠 inanystate𝑠 ∈𝑆.
1 2
Next,weshowthatthisjointstrategyprofileisnotaNashequilibriumof𝐺.Toseethis,suppose
thatagent2’sstrategyistotake𝑎 =𝑠 ineverystate𝑠.Byfixingagent2’sstationarystrategy,we
2
canfindagent1’sbestresponsebyconstructinganMDPwiththeimmediatepayofffunctionof:
𝑟 (𝑠,𝑎 ) =𝑠−4/(2−𝑠). (3)
1 1
InthisMDP,agent1’sactiondoesnotdirectlyaffecttheimmediatepayoffateachstate.However,
agent1’sactionsaffectthelong-termpayoffbydeterminingthenextstatesthroughthetransition
probability function.Given(3),agent1’slong-termpayoffis maximizedwhen𝑠 = 0.Hence,the
uniqueoptimalstrategyofagent1istotake𝑎 =0ateverystate𝑠.Thismeansthat𝝅∗in(2)does
1
notcorrespondto𝐺’sstationaryNashequilibrium,acontradiction!
WenotethatastationaryNashequilibriumof𝐺 isforagent1and2torespectivelytake𝑎 =0
1
and𝑎 = 𝑠 inallstates𝑠 ∈ 𝑆.Starting from𝑠 = 0,theaveragepayoffofagent1underthisNash
2
equilibriumis-2,andtheaveragepayoffofagent2is0.
4 CONCLUSION
Inthispaper,wefirstintroducedstochasticgamesbriefly.Wethenprovidedbackgroundinforma-
tiononMarkovpotential gamesanddiscussedthesufficientconditionsforastochastic gameto
beclassifiedasaMarkovpotentialgame.Furthermore,weexaminedthemainclaimof[12]and
presented a counterexample to its Theorem 1, demonstrating that the theorem does not always
hold.
REFERENCES
[1] LawrenceEBlume.1995.Thestatisticalmechanicsofbest-responsestrategyrevision.GamesandEconomicBehavior
11,2(1995),111–145.
[2] VivekSBorkar.2002.ReinforcementlearninginMarkovianevolutionarygames.AdvancesinComplexSystems5,01
(2002),55–72.
[3] ConstantinosDaskalakis,NoahGolowich,andKaiqingZhang.2023. ThecomplexityofMarkovequilibriuminsto-
chasticgames.InThe36thAnnualConferenceonLearningTheory.4180–4234.
[4] ArlingtonMFink.1964. Equilibriuminastochasticn-persongame. JournalofScienceoftheHiroshimaUniversity,
seriesai(mathematics)28,1(1964),89–93.
[5] Jakob Foerster,Richard YChen,MaruanAl-Shedivat,ShimonWhiteson,PieterAbbeel,andIgor Mordatch.2018.
LearningwithOpponent-LearningAwareness.(2018),122–130.
[6] JakobFoerster,NantasNardelli,GregoryFarquhar,TriantafyllosAfouras,PhilipHSTorr,PushmeetKohli,andShimon
Whiteson.2017.Stabilisingexperiencereplayfordeepmulti-agentreinforcementlearning.InProceedingsofthe34th
InternationalConferenceonMachineLearning(ICML).1146–1155.
[7] RoyFox,StephenMMcaleer,WillOverman,andIoannisPanageas.2022. Independentnaturalpolicygradiental-
waysconvergesinMarkovpotentialgames.InProceedingsoftheInternationalConferenceonArtificialIntelligenceand
Statistics.4414–4425.
[8] Hongyi Guo, Zuyue Fu,Zhuoran Yang, and ZhaoranWang. 2021. Decentralizedsingle-timescale actor-critic on
zero-sumtwo-playerstochasticgames.InProceedingsoftheInternationalConferenceonMachine Learning(ICML).
3899–3909.
[9] JunlingHuandMichaelPWellman.2003. NashQ-learningforgeneral-sumstochasticgames. JournalofMachine
LearningResearch4(2003),1039–1069.
[10] StefanosLeonardos,WillOverman,IoannisPanageas,andGeorgiosPiliouras.2021. Globalconvergenceofmulti-
agentpolicygradientinMarkovpotentialgames.arXivpreprintarXiv:2106.01969(2021).
[11] SergioValcarcelMacua,JavierZazo,andSantiagoZazo.2018. Learningparametricclosed-looppoliciesforMarkov
potentialgames.arXivpreprintarXiv:1802.00899(2018).
[12] DavidHMguni,YutongWu,YaliDu,YaodongYang,ZiyiWang,MinneLi,YingWen,JoelJennings,andJunWang.
2021. Learninginnonzero-sumstochasticgameswithpotentials.InProceedingsoftheInternationalConferenceon
MachineLearning(ICML).7688–7699.
[13] DovMondererandLloydSShapley.1996.Potentialgames.GamesandEconomicBehavior14,1(1996),124–143.[14] JulienPérolat,FlorianStrub,BilalPiot,andOlivierPietquin.2017.LearningNashequilibriumforgeneral-sumMarkov
gamesfrombatchdata.InProceedingsofthe20thInternationalConferenceonArtificialIntelligenceandStatistics.232–
241.
[15] RichardSSuttonandAndrewGBarto.2018.Reinforcementlearning:Anintroduction. MITpress.
[16] ChristopherJCHWatkinsandPeterDayan.1992.Q-learning.MachineLearning8(1992),279–292.
[17] ChristopherJohnCornishHellabyWatkins.1989.Learningfromdelayedrewards.(1989).