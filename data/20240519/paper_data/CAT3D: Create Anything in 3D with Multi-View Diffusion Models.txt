CAT3D: Create Anything in 3D
with Multi-View Diffusion Models
RuiqiGao1* AleksanderHołyn´ski1* PhilippHenzler2 ArthurBrussee1
RicardoMartin-Brualla2 PratulSrinivasan1 JonathanT.Barron1 BenPoole1*
1GoogleDeepMind 2GoogleResearch *equalcontribution
Text-to-image-to-3D Realimageto3D Sparsemulti-viewto3D
Figure1: CAT3Denables3Dscenecreationfromanynumberofgeneratedorrealimages.
Abstract
Advancesin3Dreconstructionhaveenabledhigh-quality3Dcapture,butrequirea
usertocollecthundredstothousandsofimagestocreatea3Dscene. Wepresent
CAT3D,amethodforcreatinganythingin3Dbysimulatingthisreal-worldcapture
processwithamulti-viewdiffusionmodel. Givenanynumberofinputimagesand
asetoftargetnovelviewpoints,ourmodelgenerateshighlyconsistentnovelviews
ofascene. Thesegeneratedviewscanbeusedasinputtorobust3Dreconstruction
techniquestoproduce3Drepresentationsthatcanberenderedfromanyviewpoint
inreal-time. CAT3Dcancreateentire3Dscenesinaslittleasoneminute,and
outperformsexistingmethodsforsingleimageandfew-view3Dscenecreation.
Seeourprojectpageforresultsandinteractivedemos: cat3d.github.io.
4202
yaM
61
]VC.sc[
1v41301.5042:viXra1 Introduction
Thedemandfor3Dcontentishigherthanever,sinceitisessentialforenablingreal-timeinteractivity
forgames,visualeffects,andwearablemixedrealitydevices. Despitethehighdemand,high-quality
3Dcontentremainsrelativelyscarce. Unlike2Dimagesandvideoswhichcanbeeasilycaptured
withconsumerphotographydevices,creating3Dcontentrequirescomplexspecializedtoolsanda
substantialinvestmentoftimeandeffort.
Fortunately,recentadvancementsinphotogrammetrytechniqueshavegreatlyimprovedtheaccessibil-
ityof3Dassetcreationfrom2Dimages. MethodssuchasNeRF[1],Instant-NGP[2],andGaussian
Splatting[3]allowanyonetocreate3Dcontentbytakingphotosofarealsceneandoptimizingarep-
resentationofthatscene’sunderlying3Dgeometryandappearance. Theresulting3Drepresentation
canthenberenderedfromanyviewpoint,similartotraditional3Dassets. Unfortunately,creating
detailedscenesstillrequiresalabor-intensiveprocessofcapturinghundredstothousandsofphotos.
Captureswithinsufficientcoverageofthescenecanleadtoanill-posedoptimizationproblem,which
oftenresultsinincorrectgeometryandappearanceand,consequently,implausibleimagerywhen
renderingtherecovered3Dmodelfromnovelviewpoints.
Reducing this requirement from dense multi-view captures to less exhaustive inputs, such as a
singleimageortext,wouldenablemoreaccessible3Dcontentcreation. Priorworkhasdeveloped
specializedsolutionsfordifferentinputsettings,suchasgeometryregularizationtechniquestargeted
forsparse-viewreconstruction[4,5],feed-forwardmodelstrainedtocreate3Dobjectsfromsingle
images [6], or the use of image-conditioned [7] or text-conditioned [8] generative priors in the
optimizationprocess—buteachofthesespecializedmethodscomeswithassociatedlimitationsin
quality,efficiency,andgenerality.
Inthispaper, weinsteadfocusonthefundamentalproblemthatlimitstheuseofestablished3D
reconstructionmethodsintheobservation-limitedsetting:aninsufficientnumberofsupervisingviews.
Ratherthandevisingspecializedsolutionsfordifferentinputregimes[8,9,7],asharedsolutionis
toinsteadsimplycreatemoreobservations—collapsingtheless-constrained,under-determined3D
creationproblemstothefully-constrained,fully-observed3Dreconstructionsetting. Thisway,we
reformulateadifficultill-posedreconstructionproblemasagenerationproblem: givenanynumber
ofinputimages,generateacollectionofconsistentnovelobservationsofthe3Dscene. Recentvideo
generativemodelsshowpromiseinaddressingthischallenge, astheydemonstratethecapability
tosynthesizevideoclipsfeaturingplausible3Dstructure[10,11,12,13,14,15]. However,these
modelsareoftenexpensivetosamplefrom,challengingtocontrol,andlimitedtosmoothandshort
cameratrajectories.
Oursystem,CAT3D,insteadaccomplishesthisthroughamulti-viewdiffusionmodeltrainedspecifi-
callyfornovel-viewsynthesis. Givenanynumberofinputviewsandanyspecifiednovelviewpoints,
ourmodelgeneratesmultiple3D-consistentimagesthroughanefficientparallelsamplingstrategy.
Thesegeneratedimagesaresubsequentlyfedthrougharobust3Dreconstructionpipelinetoproduce
a3Drepresentationthatcanberenderedatinteractiveratesfromanyviewpoint. Weshowthatour
modeliscapableofproducingphotorealisticresultsofarbitraryobjectsorscenesfromanynumber
ofcapturedorsynthesizedinputviewsinaslittleasoneminute. Weevaluateourworkacrossvarious
inputsettings,rangingfromsparsemulti-viewcapturestoasinglecapturedimage,andevenjusta
textprompt(byusingatext-to-imagemodeltogenerateaninputimagefromthatprompt). CAT3D
outperforms prior works for measurable tasks (such as the multi-view capture case) on multiple
benchmarks, and is an order of magnitude faster than previous state-of-the-art. For tasks where
empiricalperformanceisdifficulttomeasure(suchastext-to-3Dandsingleimageto3D),CAT3D
comparesfavorablywithpriorworkinallsettings.
2 RelatedWork
Creatingentire3Dscenesfromlimitedobservationsrequires3Dgeneration,e.g.,creatingcontentin
unseenregions,andourworkbuildsontheever-growingresearchareaof3Dgenerativemodels[16].
Duetotherelativescarcityof3Ddatasets,muchresearchin3Dgenerationiscenteredontransferring
knowledgelearnedby2Dimage-spacepriors,as2Ddataisabundant. Ourdiffusionmodelisbuilt
ontherecentdevelopmentofvideoandmulti-viewdiffusionmodelsthatproducehighlyconsistent
novelviews. Weshowthatpairingthesemodelswith3Dreconstruction,similarto [17,58],enables
2efficientandhighquality3Dcreation. Belowwediscusshowourworkisrelatedtoseveralareasof
priorwork.
2Dpriors. Givenlimitedinformationsuchastext,pretrainedtext-to-imagemodelscanprovidea
stronggenerativepriorfortext-to-3Dgeneration. However,distillingtheknowledgepresentinthese
image-basedpriorsintoacoherent3Dmodelcurrentlyrequiresaniterativedistillationapproach.
DreamFusion[8]introducedScoreDistillationSampling(SDS)tosynthesize3Dobjects(asNeRFs)
fromtextprompts. Researchinthisspacehasaimedtoimprovedistillationstrategies[18,19,20,21,
22],swapinother3Drepresentations[23,24,25,26,27],andamortizetheoptimizationprocess[28].
Usingtext-basedpriorsforsingle-image-to-3Dhasalsoshownpromise[29,30,31],butrequiresa
complexbalancingoftheimageobservationwithadditionalconstraints. Incorporatingpriorssuchas
monoculardepthmodelsorinpaintingmodelshasbeenusefulforcreating3Dscenes,buttendsto
resultinpoorglobalgeometry[32,33,34,35].
2Dpriorswithcameraconditioning. Whiletext-to-imagemodelsexcelatgeneratingvisually
appealingimages,theylackprecisecontrolovertheposeofimages,andthusrequireatime-consuming
3Ddistillationprocesstoencouragethe3Dmodeltoconformtothe2Dprior. Toovercomethis
limitation, several approaches train or fine-tune generative models with explicit image and pose
conditioning[7,36,37,38,39,40,41]. Thesemodelsprovidestrongerpriorsforwhatanobject
or scene should look like given text and/or input image(s), but they also model all output views
independently. In cases where there is little uncertainty in what should appear at novel views,
reasoning about generated views independently is sufficient for efficient 3D reconstruction [42].
Butwhenthereissomeuncertaintyexists,thesetop-performingmethodsstillrequireexpensive3D
distillationtoresolvetheinconsistenciesbetweendifferentnovelviews.
Multi-viewpriors. Modelingthecorrelationsbetweenmultipleviewsprovidesamuchstronger
prior for what 3D content is consistent with partial observations. Methods like MVDream [43],
ImageDream[9],Zero123++[44],ConsistNet[45],SyncDreamer[46]andViewDiff [47]fine-tune
text-to-imagemodelstogeneratemultipleviewssimultaneously. CAT3Dissimilarinarchitecture
toImageDream,wherethemulti-viewdependencyiscapturedbyanarchitectureresemblingvideo
diffusionmodelswith3Dself-attention. Giventhisstrongerprior,thesepapersalsodemonstrate
higherqualityandmoreefficient3Dextraction.
Videopriors. Videodiffusionmodelshavedemonstratedanastonishingcapabilityofgenerating
realisticvideos[48,49,10,12,15,13,50],andarethoughttoimplicitlyreasonabout3D.However,
itremainschallengingtouseoff-the-shelfvideodiffusionmodelsfor3Dgenerationforanumberof
reasons. Currentmodelslackexactcameracontrols,limitinggenerationtoclipswithonlysmooth
and short camera trajectories, and struggle to generate videos with only camera motion but no
scene dynamics. Several works have proposed to resolve these challenges by fine-tuning video
diffusionmodelsforcamera-controledormulti-viewgeneration. Forexample,AnimateDiff[51]
LoRAfine-tunedavideodiffusionmodelwithfixedtypesofcameramotions,andMotionCtrl[52]
conditionedthemodelonarbitraryspecifiedcameratrajectories. ViVid-1-to-3[53]combinesanovel
viewsynthesismodelandavideodiffusionmodelforgeneratingsmoothtrajectories. SVD-MV[54],
IM-3D [17] and SV3D [54] further explored leveraging camera-controlled or multi-view video
diffusionmodelsfor3Dgeneration. However,theircameratrajectoriesarelimitedtoorbitalones
surroundingthecentercontent. Theseapproachesmainlyfocuson3Dobjectgeneration,anddonot
workfor3Dscenes,few-view3Dreconstruction,orobjectsincontext(objectsthathavenotbeen
maskedorotherwiseseparatedfromtheimage’sbackground).
Feed-forward methods. Another line of research is to learn feed-forward models that take a
few views as input, and output 3D representations directly, without an optimization process per
instance[6,55,56,57,58,59,60]. Thesemethodscanproduce3Drepresentationsefficiently(within
afewseconds),butthequalityisoftenworsethanapproachesbuiltonimage-spacepriors.
3 Method
CAT3Disatwo-stepapproachfor3Dcreation: first,wegeneratealargenumberofnovelviews
consistentwithoneormoreinputviewsusingamulti-viewdiffusionmodel,andsecond,werun
34
Figure2: Qualitativeresults: CAT3Dcancreatehigh-quality3Dobjectsorscenesfromanumber
ofinputmodalities: aninputimagegeneratedbyatext-to-imagemodel(rows1-2),asinglecaptured
realimage(rows3-4),andmultiplecapturedrealimages(row5).Observed Views and Target Poses An Observed View A Target View
Encoder
Ray Coordinates Input Latents Ray Coordinates Random Latents
}
Observed & Generated Views <latexit sha1_base64="4JGGia3Pnt2sLLuK1gpER8+3IBE=">AAAD/XicdVNLbxMxEHYbHmV5tXDksmIbiUuj7CY8jiu4cCyItJXWq8pxvI0VP1a2lySyLP4AV/gH3BBXfgs/gP+BvQlS0sdIlma+8TeeGc+Ma0a16ff/7Ox2bt2+c3fvXnT/wcNHj/cPnpxo2ShMRlgyqc7GSBNGBRkZahg5qxVBfMzI6Xj2LvhPPxOlqRSfzLImJUcXglYUI+Ohj9Cd7yf9Xr+V+KqSrpUErOX4/GD3L5xI3HAiDGZI6yLt16a0SBmKGXERbDSpEZ6hC1J4VSBOdGnbVF3c9cgkrqTyR5i4RTcZFnGtl3zsb3JkpvqyL4DX+jASmLCt1+0c6aUPtg0GqpGS6W14USmELyVvqjelpaJuDBF4lXvVsNjIOLQynlBFsGFLryCsqC8/xlPkwxjf8CiCHM0I8j9ivB1BQeZYco7ExELd1ERRXktNXDEoCxiSkmpS2iS1rVGjlrd5M7c2yZxNBs7LjfFyV2TlNpL7qM5Tb+bkIQvbjeIYSokYvRCt4c1pRdkh5LmZJmmSHbY2xOoa52DT6bqR+98Aaaa+/u3H/Uguta/YIKXk3BVpWQzCELW1K8JWz8PpWC4sVA0jBZxQTha1gmFugi6yDBqyMMHOjnpZbaAnokXoobO9YW2cW0XhM6LE0ZA363jhGz3Jjpz187t0ljsrHAxjJ5kdpm1/I78Y6eU1uKqcZL30Ve/lh2GSv12vyB54Bp6DFyAFr0EO3oNjMAIYVOAr+Aa+d750fnR+dn6tru7urDlPwZZ0fv8DvmFUXQ==</latexit> Set of observed and
unobserved views
}
>tixetal/<==QXUFmvD8vf0ZZwPlDru7urt6nd+Rnf057d+aA+rAOVYIAMjNo3OE0rFAyFD6pB45Byv21vSG2hl/eV03LZcqKu1Ue6Y87I/1mpdk5JjxAHrsjl0t781zpjJ3zGhj363AZ0EL6MhX0Wc2WY9ObooXkonAabZpnjOHMMyqBDy6iguFmg1ahTQxZBj0AVs4CWpP8zWJ8K1WLECzQWpVB3kXKIY/atugU/H3u/+aaaA89+Rqb6TD25aoOx2YbHSmmJZmL5hkdRp1c4tCRvYkoSYIejbvQIkb+bT5Mq7LpNlT2VyFfjL7sBNxZy2c7M5drljGVr1Si2SpmkSixCoEDXNtkXRRE1dLExE7ocYZeQB1Bvi9j8I0MHCiC8fjxwkPlx/8CqsCyrLFGsFBlnyQLOIjNsVvXl4FBDuJaplejqvVyLEmSU41W6SGpqG0gtPUa6c0+1tCLmSAj+XD4LyqvpkJ3bsz3ltGnFZcTR4i5RyTqrkgc9c3FVbnGdOBSV4J1Ch6ZEpSDbREXGKmBS0a61tLy6IZGDiAH3Ix5L3DG/4XOrEUprSqK+V+rX9fy7dC9jhO+IUYlgXcUJmILzfSRqlOxPPhvL2jX6IzMfBVxq5ghaZkRBGNBSG7cqyglRMhS2oxpnPc/jHNcw/TnXvf3c+2tb2xO7/ff61a0aM+cGeeT8+amlIds0SlQvB+Pg/sgfXBB3Hg/VA4PLySyl2a1PV0Ivxp8qWXJtIyCc4uij8YC7juUibImskDXt5VmHbYHExMxbLNVdciX/DAAA>"=EBI3+8REpg1KuLLs2tnP3aiGGJ4"=46esab_1ahs tixetal<
Multi-View Latent Diffusion Model
NeRF Set of generated views Generated Latents Generated Image
Figure3: Illustrationofthemethod. Givenonetomanyviews,CAT3Dcreatesa3Drepresentation
of the entire scene in as little as one minute. CAT3D has two stages: (1) generate a large set of
syntheticviewsfromamulti-viewlatentdiffusionmodelconditionedontheinputviewsalongside
thecameraposesoftargetviews;(2)runarobust3Dreconstructionpipelineontheobservedand
generatedviewstolearnaNeRFrepresentation. Thisdecouplingofthegenerativepriorfromthe
3Dreconstructionprocessresultsinbothcomputationalefficiencyimprovementsandareduction
inmethodologicalcomplexityrelativetopriorwork[7,8,41],whilealsoyieldingimprovedimage
quality.
arobust3Dreconstructionpipelineonthegeneratedviews(seeFigure3). Belowwedescribeour
multi-viewdiffusionmodel(Section3.1),ourmethodforgeneratingalargesetofnearlyconsistent
novelviewsfromit(Section3.2),andhowthesegeneratedviewsareusedina3Dreconstruction
pipeline(Section3.3).
3.1 Multi-ViewDiffusionModel
Wetrainamulti-viewdiffusionmodelthattakesasingleormultipleviewsofa3Dsceneasinput
andgeneratesmultipleoutputimagesgiventheircameraposes(where“aview”isapairedimage
anditscamerapose). Specifically,givenM conditionalviewscontainingM imagesIcondandtheir
correspondingcameraparameterspcond,themodellearnstocapturethejointdistributionofN target
imagesItgtassumingtheirN targetcameraparametersptgtarealsogiven:
p(cid:0) Itgt Icond,pcond,ptgt(cid:1) . (1)
|
Modelarchitecture. Ourmodelarchitectureissimilartovideolatentdiffusionmodels(LDMs)[48,
11],butwithcameraposeembeddingsforeachimageinsteadoftimeembeddings. Givenasetof
conditionalandtargetimages,themodelencodeseveryindividualimageintoalatentrepresentation
throughanimagevariationalauto-encoder[61]. Then,adiffusionmodelistrainedtoestimatethe
jointdistributionofthelatentrepresentationsgivenconditioningsignals. Weinitializethemodel
fromanLDMtrainedfortext-to-imagegenerationsimilarto[62]trainedonweb-scaleimagedata,
withaninputimageresolutionof512 512 3andlatentswithshape64 64 8. Asisoften
× × × ×
doneinvideodiffusionmodels[49,10,11],themainbackboneofourmodelremainsthepretrained
2Ddiffusionmodelbutwithadditionallayersconnectingthelatentsofmultipleinputimages. Asin
[43],weuse3Dself-attention(2Dinspaceand1Dacrossimages)insteadofsimple1Dself-attention
acrossimages. Wedirectlyinflatetheexisting2Dself-attentionlayersafterevery2Dresidualblock
oftheoriginalLDMtoconnectlatentswith3Dself-attentionlayerswhileinheritingtheparameters
fromthepre-trainedmodel,introducingminimalamountofextramodelparameters. Wefoundthat
conditioningoninputviewsthrough3Dself-attentionlayersremovedtheneedforPixelNeRF[63]
5
redoceDandCLIPimageembeddings[64]usedbythepriorstate-of-the-artmodelonfew-viewreconstruction,
ReconFusion[7]. WeuseFlashAttention[65,66]forfasttrainingandsampling,andfine-tuneallthe
weightsofthelatentdiffusionmodel. Similartopriorwork[10,67],wefounditimportanttoshift
thenoisescheduletowardshighnoiselevelsaswemovefromthepre-trainedimagediffusionmodel
toourmulti-viewdiffusionmodelthatcapturesdataofhigherdimensionality. Concretely,following
logicsimilarto[67], weshiftthelogsignal-to-noiseratiobylog(N), whereN isthenumberof
targetimages. Fortraining,latentsoftargetimagesarenoiseperturbedwhilelatentsofconditional
imagesarekeptasclean,andthediffusionlossisdefinedonlyontargetimages. Abinarymaskis
concatenatedtothelatentsalongthechanneldimension,todenoteconditioningvs. targetimages. To
dealwithmultiple3Dgenerationsettings,wetrainasingleversatilemodelthatcanmodelatotalof
8conditioningandtargetviews(N +M =8),andrandomlyselectthenumberofconditionalviews
N tobe1or3duringtraining,correspondingto7and5targetviewsrespectively. SeeAppendixB
formoremodeldetails.
Camera conditioning. To condition on the camera pose, we use a camera ray representation
(“raymap”)thatisthesameheightandwidthasthelatentrepresentations[37,68]andencodesthe
rayoriginanddirectionateachspatiallocation. Theraysarecomputedrelativetothecamerapose
ofthefirstconditionalimage,soourposerepresentationisinvarianttorigidtransformationsof3D
worldcoordinates. Raymapsforeachimageareconcatenatedchannel-wiseontothelatentsforthe
correspondingimage.
3.2 GeneratingNovelViews
Givenasetofinputviews,ourgoalistogeneratealargesetofconsistentviewstofullycoverthe
sceneandenableaccurate3Dreconstruction. Todothis,weneedtodecideonthesetofcamera
posestosample,andweneedtodesignasamplingstrategythatcanuseamulti-viewdiffusionmodel
trainedonasmallnumberofviewstogenerateamuchlargersetofconsistentviews.
Camera trajectories. Compared to 3D object reconstruction where orbital camera trajectories
can be effective, a challenge of 3D scene reconstruction is that the views required to fully cover
a scene can be complex and depend on the scene content. We empirically found that designing
reasonablecameratrajectoriesfordifferenttypesofscenesiscrucialtoachievecompellingfew-view
3Dreconstruction. Thecamerapathsmustbesufficientlythoroughanddensetofully-constrainthe
reconstructionproblem,butalsomustnotpassthroughobjectsinthesceneorviewscenecontent
fromunusualangles. Insummary,weexplorefourtypesofcamerapathsbasedonthecharacteristic
of a scene: (1) orbital paths of different scales and heights around the center scene, (2) forward
facingcirclepathsofdifferentscalesandoffsets,(3)splinepathsofdifferentoffsets,and(4)spiral
trajectoriesalongacylindricalpath,movingintoandoutofthescene. SeeAppendixCformore
details.
Generatingalargesetofsyntheticviews. Achallengeinapplyingourmulti-viewdiffusionmodel
tonovelviewsynthesisisthatitwastrainedwithasmallandfinitesetofinputandoutputviews—
just8intotal. Toincreasethetotalnumberofoutputviews,weclusterthetargetviewpointsinto
smallergroupsandgenerateeachgroupindependentlygiventheconditioningviews. Wegrouptarget
viewswithclosecamerapositions,astheseviewsaretypicallythemostdependent. Forsingle-image
conditioning,weadoptanautoregressivesamplingstrategy,wherewefirstgenerateasetof7anchor
viewsthatcoverthescene(similarto[41],andchosenusingthegreedyinitializationfrom[69]),and
thengeneratetheremaininggroupsofviewsinparallelgiventheobservedandanchorviews. This
allowsustoefficientlygeneratealargesetofsyntheticviewswhilestillpreservingbothlong-range
consistencybetweenanchorviewsandlocalsimilaritybetweennearbyviews. Forthesingle-image
setting,wegenerate80views,whileforthefew-viewsettingweuse480-960views. SeeAppendixC
fordetails.
Conditioninglargersetsofinputviewsandnon-squareimages. Toexpandthenumberofviews
wecanconditionon,wechoosethenearestM viewsastheconditioningset,asin[7]. Weexperi-
mentedwithsimplyincreasingthesequencelengthofthemulti-viewdiffusionarchitectureduring
sampling,butfoundthatthenearestviewconditioningandgroupedsamplingstrategyperformed
better. Tohandlewideaspectratioimages,wecombinesquaresamplesfromsquare-croppedinput
viewswithwidesamplescroppedfrominputviewspaddedtobesquare.
6Input ReconFusion[7] CAT3D(ours) GroundTruth
Figure 4: Qualitative comparison of few-view reconstruction on scenes from the mip-NeRF
360[73]andCO3D[74]datasets. Samplesshownherearerenderedimages,with3inputcaptured
views. ComparedtobaselineapproacheslikeReconFusion[7],CAT3Dalignswithgroundtruthwell
inseenregions,whilehallucinatingplausiblecontentinunseenregions.
3.3 Robust3Dreconstruction
Ourmulti-viewdiffusionmodelgeneratesasetofhigh-qualitysyntheticviewsthatarereasonably
consistentwitheachother. However,thegeneratedviewsaregenerallynotperfectly3Dconsistent.
Indeed, generating perfectly 3D consistent images remains a very challenging problem even for
currentstate-of-the-artvideodiffusionmodels[70]. Since3Dreconstructionmethodshavebeen
designedtotakephotographs(whicharebydefinitionperfectlyconsistent)asinput,wemodifythe
standardNeRFtrainingproceduretoimproveitsrobustnesstoinconsistentinputviews.
We build upon Zip-NeRF [71], whose training procedure minimizes the sum of a photometric
reconstruction loss, a distortion loss, an interlevel loss, and a normalized L2 weight regularizer.
Weadditionallyincludeaperceptualloss(LPIPS[72])betweentherenderedimageandtheinput
image. Comparedtothephotometricreconstructionloss,LPIPSemphasizeshigh-levelsemantic
similaritybetweentherenderedandobservedimages,whileignoringpotentialinconsistenciesin
low-levelhigh-frequencydetails. Sincegeneratedviewsclosertotheobservedviewstendtohave
lessuncertaintyandarethereforemoreconsistent,weweightthelossesforgeneratedviewsbasedon
thedistancetothenearestobservedview. Thisweightingisuniformatthebeginningofthetraining,
andisgraduallyannealedtoaweightingfunctionthatmorestronglypenalizesreconstructionlosses
forviewsclosertooneoftheobservedviews. SeeAppendixDforadditionaldetails.
4 Experiments
Wetrainedthemulti-viewdiffusionmodelatthecoreofCAT3Donfourdatasetswithcamerapose
annotations:Objaverse[75],CO3D[74],RealEstate10k[76]andMVImgNet[77].Wethenevaluated
CAT3Donthefew-viewreconstructiontask(Section4.1)andthesingleimageto3Dtask(Section
4.2),demonstratingqualitativeandquantitativeimprovementsoverpriorwork. Thedesignchoices
thatledtoCAT3DareablatedanddiscussedfurtherinSection4.3.
7Dataset 3-view 6-view 9-view
Method PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS
↑ ↑ ↓ ↑ ↑ ↓ ↑ ↑ ↓
RealEstate10K
Zip-NeRF∗[71] 20.77 0.774 0.332 27.34 0.906 0.180 31.56 0.947 0.118
ZeroNVS∗[41] 19.11 0.675 0.422 22.54 0.744 0.374 23.73 0.766 0.358
ReconFusion[7] 25.84 0.910 0.144 29.99 0.951 0.103 31.82 0.961 0.092
CAT3D(ours) 26.78 0.917 0.132 31.07 0.954 0.092 32.20 0.963 0.082
LLFF
Zip-NeRF∗[71] 17.23 0.574 0.373 20.71 0.764 0.221 23.63 0.830 0.166
ZeroNVS∗[41] 15.91 0.359 0.512 18.39 0.449 0.438 18.79 0.470 0.416
ReconFusion[7] 21.34 0.724 0.203 24.25 0.815 0.152 25.21 0.848 0.134
CAT3D(ours) 21.58 0.731 0.181 24.71 0.833 0.121 25.63 0.860 0.107
DTU
Zip-NeRF∗[71] 9.18 0.601 0.383 8.84 0.589 0.370 9.23 0.592 0.364
ZeroNVS∗[41] 16.71 0.716 0.223 17.70 0.737 0.205 17.92 0.745 0.200
ReconFusion[7] 20.74 0.875 0.124 23.62 0.904 0.105 24.62 0.921 0.094
CAT3D(ours) 22.02 0.844 0.121 24.28 0.899 0.095 25.92 0.928 0.073
CO3D
Zip-NeRF∗[71] 14.34 0.496 0.652 14.48 0.497 0.617 14.97 0.514 0.590
ZeroNVS∗[41] 17.13 0.581 0.566 19.72 0.627 0.515 20.50 0.640 0.500
ReconFusion[7] 19.59 0.662 0.398 21.84 0.714 0.342 22.95 0.736 0.318
CAT3D(ours) 20.57 0.666 0.351 22.79 0.726 0.292 23.58 0.752 0.273
Mip-NeRF360
Zip-NeRF∗[71] 12.77 0.271 0.705 13.61 0.284 0.663 14.30 0.312 0.633
ZeroNVS∗[41] 14.44 0.316 0.680 15.51 0.337 0.663 15.99 0.350 0.655
ReconFusion[7] 15.50 0.358 0.585 16.93 0.401 0.544 18.19 0.432 0.511
CAT3D(ours) 16.62 0.377 0.515 17.72 0.425 0.482 18.67 0.460 0.460
Table1: Quantitativecomparisonoffew-view3Dreconstruction. CAT3Doutperformsbaseline
approachesacrossnearlyallsettingsandmetrics(modifiedbaselinesdenotedwith∗takenfrom[7]).
4.1 Few-View3DReconstruction
We first evaluate CAT3D on five real-world benchmark datasets for few-view 3D reconstruction.
Amongthose,CO3D[74]andRealEstate10K[76]arein-distributiondatasetswhosetrainingsplits
werepartofourtrainingset(weusetheirtestsplitsforevaluation),whereasDTU[78],LLFF[79]and
themip-NeRF360dataset[73]areout-of-distributiondatasetsthatwerenotpartofthetrainingdataset.
WetestedCAT3Donthe3,6and9viewreconstructiontasks,withthesametrainandevalsplitsas[7].
InTable1,wecomparetothestate-of-the-artfordense-viewNeRFreconstructionwithnolearned
priors(Zip-NeRF[71])andmethodsthatheavilyleveragegenerativepriorssuchasZeroNVS[41]
andReconFusion[7]. WefindthatCAT3Dachievesstate-of-the-artperformanceacrossnearlyall
settings,whilealsoreducinggenerationtimefrom1hour(forZeroNVSandReconFusion)downtoa
fewminutes. CAT3DoutperformsbaselineapproachesonmorechallengingdatasetslikeCO3Dand
mip-NeRF360byalargermargin,therebydemonstratingitsvalueinreconstructinglargeandhighly
detailedscenes. Figure4showsthequalitativecomparison. Inunobservedregions,CAT3Disableto
hallucinateplausibletexturedcontentwhilestillpreservinggeometryandappearancefromtheinput
views,whereaspriorworksoftenproduceblurrydetailsandoversmoothedbackgrounds.
4.2 Singleimageto3D
CAT3Dsupportstheefficientgenerationofdiverse3Dcontentfromjustasingleinputview. Evalua-
tioninthisunder-constrainedregimeischallengingastherearemany3Dscenesconsistentwiththe
singleview,forexamplescenesofdifferentscales. Wethusfocusoursingleimageevaluationon
qualitativecomparisons(Figure5),andquantitativesemanticevaluationswithCLIP[64](Table2).
Onscenes,CAT3DproduceshigherresolutionresultsthanZeroNVS[41]andRealmDreamer[80],
andforbothscenesandobjectswebetterpreservedetailsfromtheinputimage. Onimageswith
segmentedobjects,ourgeometryisoftenworsethanexistingapproacheslikeImageDream[9]and
DreamCraft3D[81],butmaintainscompetitiveCLIPscores. Comparedtothesepriorapproachesthat
iterativelyleverageagenerativepriorin3Ddistillation,CAT3Dismorethananorderofmagnitude
faster. Fastergenerationmethodshavebeenproposedforobjects[6,82,83],butproducesignificantly
lowerresolutionresultsthantheiriterativecounterparts,sotheyarenotincludedinthiscomparison.
8
reisaE
ytlucfifiDtesataD
redraH
→−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−←RealmDreamer[80] ZeroNVS[41] ImageDream[9] DreamCraft3D[81]
Figure5: 3Dcreationfromsingleinputimages. Renderingsof3DmodelsfromCAT3D(middle
row)arehigherqualitythanbaselines(bottomrow)forscenes,andcompetitiveforobjects. Notethat
scaleambiguityamplifiesthedifferencesinrenderingsbetweenmethods.
Table2: Evaluatingimage-to-3Dqualitywith
Model Time(min) CLIP(Image)
CLIPimagescoresonexamplesfrom[9](num-
ImageDream[9] 120 83.77 5.2 bersreproducedfrom[17]). CAT3Dproduces
±
One2345++[84] 0.75 83.78 6.4 competitiveresultstoobject-centricbaselines
±
IM-3D(NeRF)[17] 40 87.37 5.4 whilealsoworkingonwholescenes.
±
IM-3D[17] 3 91.40 5.5
±
CAT3D(ours) 1 88.54 8.6
±
IM-3D[17]achievesbetterperformanceonsegmentedobjectswithsimilarruntime,butdoesnot
workonscenes,oronobjectsincontext.
4.3 Ablations
AtthecoreofCAT3Disamulti-viewdiffusionmodelthathasbeentrainedtogenerateconsistent
novelviews. Weconsideredseveraldifferentmodelvariants,andevaluatedboththeirsamplequality
(onin-domainandout-of-domaindatasets)andfew-view3Dreconstructionperformance. Wealso
compareimportantdesignchoicesfor3Dreconstruction. Resultsfromourablationstudyarereported
in Table 3 and Figure 6 in Appendix A and summarized below. Overall, we found that video
diffusionarchitectures,with3Dself-attention(spatiotemporal)andraymapembeddingsofcamera
pose,produceconsistentenoughviewstorecover3Drepresentationswhencombinedwithrobust
reconstructionlosses.
Image and pose. Previous work [7] used PixelNerf [63] feature-map conditioning for multiple
inputviews. WefoundthatreplacingPixelNeRFwithattention-basedconditioninginaconditional
videodiffusionarchitectureusingaper-imageembeddingofthecameraposeresultsinimproved
samplesand3Dreconstructions,whilealsoreducingmodelcomplexityandthenumberofparameters.
9
tupnI
)sruo(D3TAC
senilesaBWefoundthatembeddingthecameraposeasalow-dimensionalvector(asin[36])workswellfor
in-domainsamples,butgeneralizespoorlycomparedtoraymapconditioning(seeSection3.1).
Increasingthenumberofviews. Wefoundthatjointlymodelingmultipleoutputviews(i.e.,5or7
viewsinsteadof1)improvessamplemetrics—evenmetricsthatevaluatethequalityofeachoutput
imageindependently. Jointlymodelingmultipleoutputscreatesmoreconsistentviewsthatresultin
animproved3Dreconstructionaswell.
Attentionlayers. Wefoundthat3Dself-attention(spatiotemporal)iscrucial,asityieldsimproved
performancerelativetofactorized2Dself-attention(spatial-only)and1Dself-attention(temporal-
only). Whilemodelswith3Dself-attentioninthefinestfeaturemaps(64 64)resultinthehighest
×
fidelityimages,theyincurasignificantcomputationaloverheadfortrainingandsamplingforrelative
smallgaininfidelity. Wethereforedecidedtouse3Dself-attentiononlyinfeaturemapsofsize
32 32andsmaller.
×
Multi-viewdiffusionmodeltraining. Initializingfromapre-trainedtext-to-imagelatentdiffusion
modelimprovedperformanceonout-of-domainexamples. Weexperimentedwithfine-tuningthe
multi-viewdiffusionmodeltomultiplevariantsspecializedforspecificnumbersofinputsandoutputs
views,butfoundthatasinglemodeljointlytrainedon8frameswitheither1or3conditioningviews
wassufficienttoenableaccuratesingleimageandfew-view3Dreconstruction.
3Dreconstruction. LPIPSlossiscrucialforachievinghigh-qualitytextureandgeometry,aligning
withfindingsin [17,7].OnMip-NeRF360,increasingthenumberofgeneratedviewsfrom80(single
elliptical orbit) to 720 (nine orbits) improved central object geometry but sometimes introduced
backgroundblur,probablyduetoinconsistenciesingeneratedcontent.
5 Discussion
WepresentCAT3D,aunifiedapproachfor3Dcontentcreationfromanynumberofinputimages.
CAT3Dleveragesamulti-viewdiffusionmodelforgeneratinghighlyconsistentnovelviewsofa
3Dscene,whicharetheninputintoa3Dmulti-viewreconstructionpipeline. CAT3Ddecouplesthe
generativepriorfrom3Dextraction,leadingtoefficient,simple,andhigh-quality3Dgeneration.
AlthoughCAT3Dproducescompellingresultsandoutperformspriorworksonmultipletasks, it
haslimitations. Becauseourtrainingdatasetshaveroughlyconstantcameraintrinsicsforviewsof
thesamescene,thetrainedmodelcannothandletestcaseswellwhereinputviewsarecapturedby
multiplecameraswithdifferentintrinsics. ThegenerationqualityofCAT3Dreliesontheexpressivity
ofthebasetext-to-imagemodel,anditperformsworseinthecaseswherescenecontentisoutof
distributionforthebasemodel. Thenumberofoutputviewssupportedbyourmulti-viewdiffusion
modelisstillrelativelysmall,sowhenwegeneratealargesetofsamplesfromourmodel,notall
viewsmaybe3Dconsistentwitheachother. Finally,CAT3Dusesmanually-constructedcamera
trajectoriesthatcoverthescenethoroughly(seeAppendixC),whichmaybedifficulttodesignfor
large-scaleopen-ended3Denvironments.
There are a few directions worth exploring in future work to improve CAT3D. The multi-view
diffusion model may benefit from being initialized from a pre-trained video diffusion model, as
observedby[10,17]. Theconsistencyofsamplescouldbefurtherimprovedbyextendingthenumber
of conditioning and target views handled by the model. Automatically determining the camera
trajectoriesrequiredfordifferentscenescouldincreasetheflexibilityofthesystem.
Acknowledgements We would like to thank Daniel Watson, Rundi Wu, Richard Tucker, Jason
Baldridge,MichaelNiemeyer,RickSzeliski,JordiPont-Tuset,AndeepTorr,IrinaBlok,DougEck,
andHennaNandwanifortheirvaluablecontributions.
References
[1] BenMildenhall,PratulP.Srinivasan,MatthewTancik,JonathanT.Barron,RaviRamamoorthi,
andRenNg. NeRF:RepresentingScenesasNeuralRadianceFieldsforViewSynthesis. ECCV,
2020.
[2] ThomasMüller,AlexEvans,ChristophSchied,andAlexanderKeller. Instantneuralgraphics
primitiveswithamultiresolutionhashencoding. SIGGRAPH,2022.
10[3] BernhardKerbl,GeorgiosKopanas,ThomasLeimkühler,andGeorgeDrettakis. 3DGaussian
SplattingforReal-TimeRadianceFieldRendering. SIGGRAPH,2023.
[4] JiaweiYang,MarcoPavone,andYueWang. FreeNeRF:ImprovingFew-shotNeuralRendering
withFreeFrequencyRegularization. CVPR,2023.
[5] NagabhushanSomraj,AdithyanKaranayil,andRajivSoundararajan. SimpleNeRF:Regulariz-
ingSparseInputNeuralRadianceFieldswithSimplerSolutions. SIGGRAPHAsia,2023.
[6] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan
Sunkavalli,TrungBui,andHaoTan. LRM:LargeReconstructionModelforSingleImageto
3D. arXiv:2311.04400,2023.
[7] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson,
Pratul P. Srinivasan, Dor Verbin, Jonathan T. Barron, Ben Poole, and Aleksander Holynski.
Reconfusion: 3dreconstructionwithdiffusionpriors,2023.
[8] BenPoole,AjayJain,JonathanT.Barron,andBenMildenhall. DreamFusion: Text-to-3Dusing
2DDiffusion. ICLR,2022.
[9] PengWangandYichunShi. Imagedream:Image-promptmulti-viewdiffusionfor3dgeneration.
arXiv:2312.02201,2023.
[10] AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,MaciejKilian,Do-
minikLorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal. Stablevideodiffusion:
Scalinglatentvideodiffusionmodelstolargedatasets. arXiv:2311.15127,2023.
[11] AndreasBlattmann, RobinRombach, HuanLing, TimDockhorn, SeungWookKim, Sanja
Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent
diffusionmodels. CVPR,2023.
[12] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh
Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu Video: Factorizing
Text-to-VideoGenerationbyExplicitImageConditioning. arXiv:2311.10709,2023.
[13] OmerBar-Tal,HilaChefer,OmerTov,CharlesHerrmann,RoniPaiss,ShiranZada,ArielEphrat,
JunhwaHur,YuanzhenLi,TomerMichaeli,etal. Lumiere: Aspace-timediffusionmodelfor
videogeneration. arXiv,2024.
[14] AgrimGupta,LijunYu,KihyukSohn,XiuyeGu,MeeraHahn,LiFei-Fei,IrfanEssa,LuJiang,
andJoséLezama. Photorealisticvideogenerationwithdiffusionmodels,2023.
[15] TimBrooks,BillPeebles,ConnorHolmes,WillDePue,YufeiGuo,LiJing,DavidSchnurr,Joe
Taylor,TroyLuhman,EricLuhman,ClarenceNg,RickyWang,andAdityaRamesh. Video
generationmodelsasworldsimulators. 2024.
[16] RyanPo,WangYifan,VladislavGolyanik,KfirAberman,JonathanTBarron,AmitHBermano,
EricRyanChan,TaliDekel,AleksanderHolynski,AngjooKanazawa,etal. Stateofthearton
diffusionmodelsforvisualcomputing. arXiv:2310.07204,2023.
[17] LukeMelas-Kyriazi,IroLaina,ChristianRupprecht,NataliaNeverova,AndreaVedaldi,Oran
Gafni,andFilipposKokkinos. IM-3D:IterativeMultiviewDiffusionandReconstructionfor
High-Quality3DGeneration,2024.
[18] YukunHuang,JiananWang,YukaiShi,XianbiaoQi,Zheng-JunZha,andLeiZhang. Dream-
Time: AnImprovedOptimizationStrategyforText-to-3DContentCreation. arXiv,2023.
[19] Peihao Wang, Zhiwen Fan, Dejia Xu, Dilin Wang, Sreyas Mohan, Forrest Iandola, Rakesh
Ranjan,YileiLi,QiangLiu,ZhangyangWang,etal. SteinDreamer: VarianceReductionfor
Text-to-3DScoreDistillationviaSteinIdentity. arXiv,2023.
[20] SubinKim,KyungminLee,JuneSukChoi,JongheonJeong,KihyukSohn,andJinwooShin.
Collaborativescoredistillationforconsistentvisualediting. NeurIPS,36,2024.
11[21] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu.
ProlificDreamer: High-Fidelity and Diverse Text-to-3D Generation with Variational Score
Distillation. NeurIPS,2023.
[22] AyaanHaque,MatthewTancik,AlexeiAEfros,AleksanderHolynski,andAngjooKanazawa.
Instruct-nerf2nerf: Editing 3d scenes with instructions. In Proceedings of the IEEE/CVF
InternationalConferenceonComputerVision,pages19740–19750,2023.
[23] RuiChen,YongweiChen,NingxinJiao,andKuiJia. Fantasia3d: Disentanglinggeometryand
appearanceforhigh-qualitytext-to-3dcontentcreation. ICCV,2023.
[24] Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,XiaohuiZeng,XunHuang,Karsten
Kreis,SanjaFidler,Ming-YuLiu,andTsung-YiLin. Magic3D:High-ResolutionText-to-3D
ContentCreation. CVPR,2023.
[25] JiaxiangTang,JiaweiRen,HangZhou,ZiweiLiu,andGangZeng. Dreamgaussian: Generative
gaussiansplattingforefficient3dcontentcreation. arXiv:2309.16653,2023.
[26] TaoranYi,JieminFang,GuanjunWu,LingxiXie,XiaopengZhang,WenyuLiu,QiTian,and
XinggangWang. Gaussiandreamer: Fastgenerationfromtextto3dgaussiansplattingwith
pointcloudpriors. arXiv:2310.08529,2023.
[27] DaveEpstein,BenPoole,BenMildenhall,AlexeiAEfros,andAleksanderHolynski. Disentan-
gled3dscenegenerationwithlayoutlearning. arXivpreprintarXiv:2402.16936,2024.
[28] JonathanLorraine,KevinXie,XiaohuiZeng,Chen-HsuanLin,TowakiTakikawa,Nicholas
Sharp, Tsung-Yi Lin, Ming-Yu Liu, Sanja Fidler, and James Lucas. ATT3D: Amortized
Text-to-3DObjectSynthesis. ICCV,2023.
[29] LukeMelas-Kyriazi,IroLaina,ChristianRupprecht,andAndreaVedaldi. Realfusion: 360deg
reconstructionofanyobjectfromasingleimage. CVPR,2023.
[30] GuochengQian,JinjieMai,AbdullahHamdi,JianRen,AliaksandrSiarohin,BingLi,Hsin-
YingLee,IvanSkorokhodov,PeterWonka,SergeyTulyakov,etal. Magic123: OneImageto
High-Quality3DObjectGenerationUsingBoth2Dand3DDiffusionPriors. arXiv:2306.17843,
2023.
[31] JunshuTang,TengfeiWang,BoZhang,TingZhang,RanYi,LizhuangMa,andDongChen.
Make-It-3D:High-Fidelity3DCreationfromASingleImagewithDiffusionPrior. ICCV,2023.
[32] LukasHöllein,AngCao,AndrewOwens,JustinJohnson,andMatthiasNießner. Text2Room:
ExtractingTextured3DMeshesfrom2DText-to-ImageModels. ICCV,2023.
[33] SaurabhSaxena, AbhishekKar, MohammadNorouzi, andDavidJFleet. Monoculardepth
estimationusingdiffusionmodels. arXiv:2302.14816,2023.
[34] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William T
Freeman,ForresterCole,DeqingSun,NoahSnavely,JiajunWu,etal. WonderJourney: Going
fromAnywheretoEverywhere. arXiv:2312.03884,2023.
[35] EthanWeber,AleksanderHołyn´ski,VarunJampani,SaurabhSaxena,NoahSnavely,Abhishek
Kar,andAngjooKanazawa. Nerfiller: Completingscenesviagenerative3dinpainting. arXiv
preprintarXiv:2312.04560,2023.
[36] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl
Vondrick. Zero-1-to-3: Zero-ShotOneImageto3DObject. arXiv,2023.
[37] DanielWatson,WilliamChan,RicardoMartin-Brualla,JonathanHo,AndreaTagliasacchi,and
MohammadNorouzi. Novelviewsynthesiswithdiffusionmodels. arXiv:2210.04628,2022.
[38] AmitRaj,SrinivasKaza,BenPoole,MichaelNiemeyer,BenMildenhall,NatanielRuiz,Shiran
Zada,KfirAberman,MichaelRubenstein,JonathanBarron,YuanzhenLi,andVarunJampani.
DreamBooth3D:Subject-DrivenText-to-3DGeneration. ICCV,2023.
12[39] JiataoGu,AlexTrevithick,Kai-EnLin,JoshuaMSusskind,ChristianTheobalt,LingjieLiu,
andRaviRamamoorthi. NerfDiff: Single-imageViewSynthesiswithNeRF-guidedDistillation
from3D-awareDiffusion. ICML,2023.
[40] EricRChan,KokiNagano,MatthewAChan,AlexanderWBergman,JeongJoonPark,Axel
Levy,MiikaAittala,ShaliniDeMello,TeroKarras,andGordonWetzstein. GeNVS:Generative
novelviewsynthesiswith3D-awarediffusionmodels. arXiv,2023.
[41] KyleSargent,ZizhangLi,TanmayShah,CharlesHerrmann,Hong-XingYu,YunzhiZhang,
EricRyanChan,DmitryLagun,LiFei-Fei,DeqingSun,etal. ZeroNVS:Zero-Shot360-Degree
ViewSynthesisfromaSingleImage. CVPR,2024.
[42] MinghuaLiu,ChaoXu,HaianJin,LinghaoChen,MukundVarmaT,ZexiangXu,andHaoSu.
One-2-3-45: AnySingleImageto3DMeshin45SecondswithoutPer-ShapeOptimization.
arXiv,2023.
[43] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li, and Xiao Yang. MVDream:
Multi-viewDiffusionfor3DGeneration. arXiv,2023.
[44] RuoxiShi,HanshengChen,ZhuoyangZhang,MinghuaLiu,ChaoXu,XinyueWei,Linghao
Chen,ChongZeng,andHaoSu. Zero123++: asingleimagetoconsistentmulti-viewdiffusion
basemodel,2023.
[45] JiayuYang,ZiangCheng,YunfeiDuan,PanJi,andHongdongLi. ConsistNet: Enforcing3D
ConsistencyforMulti-viewImagesDiffusion. arXiv:2310.10343,2023.
[46] YuanLiu,ChengLin,ZijiaoZeng,XiaoxiaoLong,LingjieLiu,TakuKomura,andWenping
Wang. SyncDreamer: Generating Multiview-consistent Images from a Single-view Image.
arXiv,2023.
[47] Lukas Höllein, Aljaž Božicˇ, Norman Müller, David Novotny, Hung-Yu Tseng, Christian
Richardt,MichaelZollhöfer,andMatthiasNießner. Viewdiff: 3d-consistentimagegeneration
withtext-to-imagemodels,2024.
[48] JonathanHo,TimSalimans,AlexeyGritsenko,WilliamChan,MohammadNorouzi,andDavidJ
Fleet. Videodiffusionmodels. arXiv:2204.03458,2022.
[49] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,
DiederikPKingma,BenPoole,MohammadNorouzi,DavidJFleet,etal. Imagenvideo: High
definitionvideogenerationwithdiffusionmodels. arXiv:2210.02303,2022.
[50] Siddhant Jain, Daniel Watson, Eric Tabellion, Aleksander Hołyn´ski, Ben Poole, and Janne
Kontkanen. Videointerpolationwithdiffusionmodels. arXivpreprintarXiv:2404.01203,2024.
[51] YuweiGuo,CeyuanYang,AnyiRao,YaohuiWang,YuQiao,DahuaLin,andBoDai. Ani-
matediff: Animateyourpersonalizedtext-to-imagediffusionmodelswithoutspecifictuning.
arXivpreprintarXiv:2307.04725,2023.
[52] ZhouxiaWang,ZiyangYuan,XintaoWang,TianshuiChen,MenghanXia,PingLuo,andYing
Shan. Motionctrl: Aunifiedandflexiblemotioncontrollerforvideogeneration. arXivpreprint
arXiv:2312.03641,2023.
[53] Jeong-giKwak,ErqunDong,YuheJin,HanseokKo,ShwetaMahajan,andKwangMooYi.
ViVid-1-to-3: NovelViewSynthesiswithVideoDiffusionModels. arXiv:2312.01305,2023.
[54] VikramVoleti, Chun-HanYao, MarkBoss, AdamLetts, DavidPankratz, DmitryTochilkin,
ChristianLaforte,RobinRombach,andVarunJampani. SV3D:NovelMulti-viewSynthesis
and3DGenerationfromaSingleImageusingLatentVideoDiffusion,2024.
[55] AnchitGupta,WenhanXiong,YixinNie,IanJones,andBarlasOg˘uz. 3DGen: TriplaneLatent
DiffusionforTexturedMeshGeneration. arXiv:2303.05371,2023.
[56] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Viewset Diffusion: (0-
)Image-Conditioned3DGenerativeModelsfrom2DData. ICCV,2023.
13[57] YinghaoXu,HaoTan,FujunLuan,SaiBi,PengWang,JiahaoLi,ZifanShi,KalyanSunkavalli,
GordonWetzstein,ZexiangXu,andKaiZhang. DMV3D:DenoisingMulti-ViewDiffusion
using3DLargeReconstructionModel,2023.
[58] JiahaoLi,HaoTan,KaiZhang,ZexiangXu,FujunLuan,YinghaoXu,YicongHong,Kalyan
Sunkavalli, GregShakhnarovich, andSaiBi. Instant3D:FastText-to-3DwithSparse-View
GenerationandLargeReconstructionModel. arXiv:2311.06214,2023.
[59] StanislawSzymanowicz,ChristianRupprecht,andAndreaVedaldi. Splatterimage: Ultra-fast
single-view3dreconstruction. arXiv:2312.13150,2023.
[60] KaiZhang,SaiBi,HaoTan,YuanboXiangli,NanxuanZhao,KalyanSunkavalli,andZexiang
Xu. GS-LRM:LargeReconstructionModelfor3DGaussianSplatting. arXiv:2404.19702,
2024.
[61] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv:1312.6114,
2013.
[62] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
High-ResolutionImageSynthesiswithLatentDiffusionModels. CVPR,2022.
[63] AlexYu,VickieYe,MatthewTancik,andAngjooKanazawa. pixelNeRF:NeuralRadiance
FieldsfromOneorFewImages. CVPR,2021.
[64] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. ICML,2021.
[65] TriDao,DanFu,StefanoErmon,AtriRudra,andChristopherRé. Flashattention: Fastand
memory-efficientexactattentionwithio-awareness. NeurIPS,35,2022.
[66] Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning.
arXiv:2307.08691,2023.
[67] EmielHoogeboom,JonathanHeek,andTimSalimans. Simplediffusion: End-to-enddiffusion
forhighresolutionimages. ICML,2023.
[68] MehdiSMSajjadi,HenningMeyer,EtiennePot,UrsBergmann,KlausGreff,NohaRadwan,
SuhaniVora,MarioLucˇic´,DanielDuckworth,AlexeyDosovitskiy,etal. Scenerepresentation
transformer: Geometry-free novel view synthesis through set-latent scene representations.
CVPR,2022.
[69] David Arthur and Sergei Vassilvitskii. k-means++: the advantages of careful seeding. In
ACM-SIAMSymposiumonDiscreteAlgorithms,2007.
[70] Ayush Sarkar, Hanlin Mai, Amitabh Mahapatra, Svetlana Lazebnik, David A Forsyth, and
AnandBhattad. ShadowsDon’tLieandLinesCan’tBend! GenerativeModelsdon’tknow
ProjectiveGeometry...fornow. arXiv:2311.17138,2023.
[71] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman.
Zip-NeRF:Anti-AliasedGrid-BasedNeuralRadianceFields. ICCV,2023.
[72] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunreason-
ableeffectivenessofdeepfeaturesasaperceptualmetric. CVPR,2018.
[73] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman.
Mip-NeRF360: UnboundedAnti-AliasedNeuralRadianceFields. CVPR,2022.
[74] JeremyReizenstein,RomanShapovalov,PhilippHenzler,LucaSbordone,PatrickLabatut,and
DavidNovotny. CommonObjectsin3D:Large-ScaleLearningandEvaluationofReal-life3D
CategoryReconstruction. ICCV,2021.
[75] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt,
LudwigSchmidt,KianaEhsani,AniruddhaKembhavi,andAliFarhadi. Objaverse: Auniverse
ofannotated3dobjects. CVPR,2023.
14[76] TinghuiZhou,RichardTucker,JohnFlynn,GrahamFyffe,andNoahSnavely. Stereomagnifi-
cation: Learningviewsynthesisusingmultiplaneimages. SIGGRAPH,2018.
[77] XianggangYu,MutianXu,YidanZhang,HaolinLiu,ChongjieYe,YushuangWu,ZizhengYan,
ChenmingZhu,ZhangyangXiong,TianyouLiang,etal. MVImgNet: ALarge-scaleDatasetof
Multi-viewImages. CVPR,2023.
[78] RasmusJensen,AndersDahl,GeorgeVogiatzis,EnginTola,andHenrikAanæs. Largescale
multi-viewstereopsisevaluation. CVPR,2014.
[79] BenMildenhall, PratulPSrinivasan, RodrigoOrtiz-Cayon, NimaKhademiKalantari, Ravi
Ramamoorthi,RenNg,andAbhishekKar. LocalLightFieldFusion: PracticalViewSynthesis
withPrescriptiveSamplingGuidelines. SIGGRAPH,2019.
[80] JaidevShriram,AlexTrevithick,LingjieLiu,andRaviRamamoorthi. RealmDreamer: Text-
Driven3DSceneGenerationwithInpaintingandDepthDiffusion,2024.
[81] JingxiangSun,BoZhang,RuizhiShao,LizhenWang,WenLiu,ZhendaXie,andYebinLiu.
DreamCraft3D:Hierarchical3DGenerationwithBootstrappedDiffusionPrior. arXiv,2023.
[82] Zi-Xin Zou, Zhipeng Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Yan-Pei Cao, and
Song-HaiZhang. TriplaneMeetsGaussianSplatting: FastandGeneralizableSingle-View3D
ReconstructionwithTransformers,2023.
[83] DmitryTochilkin,DavidPankratz,ZexiangLiu,ZixuanHuang,AdamLetts,YangguangLi,
Ding Liang, Christian Laforte, Varun Jampani, and Yan-Pei Cao. Triposr: Fast 3d object
reconstructionfromasingleimage,2024.
[84] MinghuaLiu,RuoxiShi,LinghaoChen,ZhuoyangZhang,ChaoXu,XinyueWei,Hansheng
Chen,ChongZeng,JiayuanGu,andHaoSu. One-2-3-45++: Fastsingleimageto3dobjects
withconsistentmulti-viewgenerationand3ddiffusion. arXiv:2311.07885,2023.
[85] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598,2022.
[86] JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. arXiv
preprintarXiv:2010.02502,2020.
A Ablations
HereweconductablationstudiesoverseveralimportantdecisionsthatledtoCAT3D.Weconsider
severalmulti-viewdiffusionmodelvariants,andevaluatedthemonnovel-viewsynthesiswithheld-out
validationsets(4ksamples)fromthetrainingdatasets(in-domainsamples)andfromthemip-NeRF
360dataset(out-of-domainsamples),aswellas3-viewreconstructiononthemip-NeRF360dataset
(out-of-domainrenders). Unlessotherwisespecified,modelsinthissectionaretrainedwith3input
views and 5 output views, evaluated after 120k optimization iterations. Quantitative results are
summarizedinTable3. Thenwediscussimportant3Dreconstructiondesignchoices,withqualitative
comparisoninFigure6.
Numberoftargetviews. Westartfromthesettingwherethemodeltakes3conditionalviewsas
inputandgeneratesasingletargetview,identicaltotheReconFusionbaseline[7]. Resultsshow
thatourmulti-viewarchitectureisfavoredwhendealingwithmultipleinputviews,comparedtothe
PixelNeRFusedby[7]. Wealsoshowthatgoingfromasingletargetviewto5targetviewsresultsin
asignificantimprovementonbothnovel-viewsynthesisandfew-viewreconstruction.
Cameraconditioning. AsmentionedinSection3.1,wecomparedtwocameraparameterizations,
one is an 8-dimensional encoding vector fed through cross-attention layers that contains relative
position,relativerotationquaternion,andabsolutefocallength. Theotheriscameraraysfedthrough
channel-wiseconcatenationwiththeinputlatents. Weobservethatthelatterperformsbetteracross
allmetrics.
15In-domain Out-of-domain
diffusionsamples diffusionsamples NeRFrenderings
Setting PSNR SSIM LPIPS PSNR SSIM LPIPS PSNR SSIM LPIPS
↑ ↑ ↓ ↑ ↑ ↓ ↑ ↑ ↓
Baseline
ReconFusion[7] — — — 14.01 0.265 0.568 15.49 0.358 0.585
#targetviews
3cond1tgt 18.85 0.638 0.359 14.12 0.262 0.553 16.17 0.360 0.546
3cond5tgt 21.66 0.733 0.277 14.63 0.278 0.515 16.29 0.368 0.530
Cameraconditioning
Low-dimvector 21.17 0.710 0.304 14.19 0.266 0.530 15.97 0.359 0.544
Raymap 21.66 0.733 0.277 14.63 0.278 0.515 16.29 0.368 0.530
Attentionlayers
Temporalattention 18.62 0.653 0.362 13.41 0.250 0.582 15.03 0.330 0.595
3Dattentionuntil16 16 21.41 0.730 0.281 14.23 0.274 0.530 16.21 0.364 0.541
3Dattentionuntil32×32
21.66 0.733 0.277 14.63 0.278 0.515 16.29 0.368 0.530
3Dattentionuntil64×64(full)
22.83 0.783 0.235 14.64 0.274 0.509 16.35 0.367 0.528
×
Modeltraining
Fromscratch,3cond5tgt 21.16 0.722 0.282 13.88 0.255 0.546 15.68 0.348 0.557
Frompretrained,3cond5tgt 21.66 0.733 0.277 14.63 0.278 0.515 16.29 0.368 0.530
Frompretrained,3cond5tgt,1Miters 22.49 0.757 0.256 15.19 0.303 0.482 16.58 0.384 0.509
Frompretrained,jointly,1.4Miters 22.96 0.777 0.235 15.15 0.294 0.488 16.62 0.377 0.515
Table 3: Ablation study of multi-view diffusion models. A comparison of model variants and
parameterssettingsacrossin-domainsequences(amixtureofObjaverse,Co3D,RealEstate10kand
MVImgNet—allsequencesinthecorrespondingevalsetsofourtrainingdata)andout-of-domain
sequences(examplesfromthemip-NeRF360dataset). Weevaluatethequalityofsamplesfromthe
diffusionmodel,aswellasrenderingsfromthesubsequentlyoptimizedNeRF.
Attentionlayers. Animportantdesignchoiceisthetypeandnumberofself-attentionlayersused
whenconnectingmultipleviews. AsshowninTable3,itiscriticaltouse3Dself-attentioninsteadof
temporal1Dself-attention. However,3Dattentionisexpensive;3Dattentionatthelargestfeature
maps(64 64)isof32ksequencelengthandthisincursasignificantcomputationoverheadduring
×
trainingandsamplingwithamarginalperformancegain,especiallyforout-of-domainsamplesand
renderings. Wethereforechosetouse3Dattentiononlyforfeaturemapsofsize32 32andsmaller.
×
Multi-viewdiffusionmodeltraining. Wecomparedthesettingsoftrainingthemulti-viewdiffusion
modelsfromscratchwithinitializingfromapre-trainedtext-to-imagelatentdiffusionmodel. The
latterperformsbetter,especiallyforout-of-domaincases. Wefurthertrainedthemodelformore
iterationsandobservedaconsistentperformancegainupuntil1Miterations. Thenwefine-tuned
the model for handling both cases of 1 conditional + 7 target views and 3 conditional + 5 target
views(jointly),foranother0.4Miterations. Wefoundthisjointfinetuningleadstobetterin-domain
novel-viewsynthesisresultswith3conditionalviews,andout-of-domainresultsthatareon-parwith
thepreviousmodel.
3Dreconstruction. Wefoundperceptualdistance(LPIPS)lossiscrucialinrecoveringhigh-quality
textureandgeometry,similarto[17,7]. Wealsocomparedtheuseof80viewsalongoneorbitalpath
with720viewsalongninevariablyscaledorbitalpaths. IntheMip-NeRF360setting,increasing
thenumberofviewshelpsbetterregularizethegeometryofcentralobjects,butsometimesleadsto
blurriertexturesinthebackground,duetoinconsistenciesingeneratedcontent.
B DetailsofMulti-ViewDiffusionModel
Weinitializethemulti-viewdiffusionmodelfromalatentdiffusionmodel(LDM)trainedfortext-to-
imagegenerationsimilarto[62]trainedonwebscaleimagedatasets. SeeFigure7foravisualization
ofthemodelarchitecture. WemodifytheLDMtotakemulti-viewimagesasinputbyinflatingthe
2Dself-attentionafterevery2Dresidualblocksto3Dself-attention[43]. Ourmodeladdsminimal
additionalparameterstothebackbonemodel: justafewadditionalconvolutionchannelsattheinput
layertohandleconditioninginformation. Wedropthetextembeddingfromtheoriginalmodel. Our
latentdiffusionmodelhas850Mparameters,smallerthanexistingapproachesbuiltonvideodiffusion
modelssuchasIM-3D[17](4.3B)andSV3D[54](1.5B).
16(a)720generatedviews
(b)80generatedviews
(c)WithoutLPIPSloss
Figure6: Qualitativecomparisonof3Dreconstructiondesignchoices. Renderedimages(left)and
depthmaps(right)ofaMip-NeRF360sceneunderdifferentsettings: (a)720generatedviewsalong
multipleorbitalpaths,(b)80generatedviewsonasingleorbitalpath,and(c)720views,withoutthe
perceptual(LPIPS)loss.
Wefine-tunethefulllatentdiffusionmodelfor1.4Miterationswithabatchsizeof128andalearning
rateof5 10−5. Thefirst1Miterationsaretrainedwiththesettingof1conditionalviewand7target
×
views,whiletherest0.4Miterationsaretrainedwithanequalmixtureof1condition+7targetviews
and3conditional+5targetviews. Following[7]wedrawtrainingsampleswithequalprobability
fromthefourtrainingdatasets. Weenableclassifier-freeguidance(CFG)[85]byrandomlydropping
theconditionalimagesandcameraposeswithaprobabilityof0.1duringtraining.
C DetailsofGeneratingNovelViews
WeuseDDIM[86]with50samplingstepsandCFGguidanceweight3forgeneratingnovelviews.
Ittakes5secondstogenerate80viewson16A100GPUs. AsmentionedinSection3.2,selecting
cameratrajectoriesthatfullycoverthe3Dsceneisimportantforhigh-quality3Dgenerationresults.
SeeFigure8foranillustrationofthecameratrajectoriesweuse. Forthesingleimage-to-3Dsetting
weusetwodifferenttypesofcameratrajectories,eachcontaining80views:
• Aspiralaroundacylinder-liketrajectorythatmovesintoandoutofthescene.
• Anorbittrajectoryforimageswithacentralobject.
Forfew-viewreconstruction,wecreatedifferenttrajectoriesbasedonthecharacteristicsofdifferent
datasets:
• RealEstate10K:wecreateasplinepathfittedfromtheinputviews,andshiftthetrajectories
alongthexz-planebycertainoffsets,resultingin800views.
178×512×512 Input Images Output Images
Encoder Decoder
Relative camera raymap
Observed mask 2x
8×64×64 Downsampling Skip Upsampling Spatial Conv2D
Block Block Stack
1/2× 2× Spatiotemporal
Joint Attention
8×32×32 Downsampling Upsampling
Block Skip Block*
1/2× 2×
Joint Attention
8×16×16 Downsampling Skip Upsampling B T H W C -> B (T H W) C
Block Block*
Flash Attention
1/2× 2×
B (T H W) C -> B T H W C
8×8×8 Middle Conv2D
Stack
Figure7: Illustrationofthenetwork. CAT3Dbuildsonalatenttext-to-imagediffusionmodel. The
inputimagesofsize512are8 downsampledtolatentsofsize64 64,whichareconcatenated
× ×
withtherelativecameraraymapandabinarymaskthatindicateswhetherornottheimagehasbeen
observed. A2DU-Netwithtemporalconnectionsisutilizedforbuildingthelatentdiffusionmodel.
Aftereachresidualblockwithresolution 32 32,weinflatetheoriginal2Dself-attention(spatial)
≤ ×
ofthetext-to-imagemodeltobe3Dself-attention(spatiotemporal). Weremovethetextembedding
conditioningoftheoriginalmodel.
• LLFFandDTU:wecreateaforward-facingcirclepathfittedfromallviewsinthetraining
set,scaleit,andshiftalongthez-axisbycertainoffsets,resultingin960and480views
respectively.
• CO3D: we create a spline path fitted from the input views, and scale the trajectories by
multiplefactors,resultingin640views.
• Mip-NeRF360: wecreateaellipticalpathfittedfromallviewsinthetrainingset,scaleit,
andshiftalongthez-axisbycertainoffsets,resultingin720views.
Forgeneratinganchorviewsinthesingle-imagesetting,weusedthemodelwith1conditionalinput
and7targetoutputs. Forgeneratingthefullsetofviews(bothinthesingle-imageanchoredsetting,
aswellasinthemulti-viewsetting),weusedthemodelwith3conditionalinputsandgroupsof5
targetoutputs,selectedbytheirindicesinthecameratrajectories.
Anchor selection. For single-image conditioning, we first select a set of target viewpoints as
anchorstogroundthescenecontent. Toselectasetofanchorviewsthatarespreadthroughoutthe
sceneandprovidegoodcoverage,weusetheinitializationstrategyfrom[69]. Thismethodgreedily
selectsthecamerawhosepositionisfurthestawayfromthealreadyselectedviews. Wefoundthisto
workwellonavarietyoftrajectory-likeviewsetsaswellasrandomviewsthathavebeenspread
throughoutascene.
D Detailsof3DReconstruction
FortheZip-NeRF[71]baseline,wefollow[7]tomakeafewmodificationsofthehyperparameters
(SeeAppendixDin [7])thatbettersuitthefew-viewreconstructionsetting. Weuseasmallerview
dependencenetworkwithwidth32anddepth1,andasmallernumberoftrainingiterationsof1000,
whichhelpsavoidoverfittingandsubstantiallyspeedsupthetrainingandrendering. Oursynthetic
viewsamplingand3Dreconstructionprocessisrunon16A100GPUs. Forfew-viewreconstruction,
wesample128 128patchesofraysandtrainwithaglobalbatchsizeof1Mthattakes4minutes
×
1819
(a)Singleimageto3D,forward (b)Singleimageto3D,orbit
(c)RealEstate10K (d)LLFF
(e)DTU (f)CO3D
(g)Mip-NeRF360
Figure8: Cameratrajectoriesforgeneratingnovelviews. Withineachpanel,leftshowstheside
view and right shows the top view of the trajectories, colored by indices of views. (a)-(b): two
typesoftrajectoriesusedbysingleimageto3D.Observedviewishighlightedinred,whileanchor
viewsarehighlightedinorange. (c)-(g): trajectoriesusedby3Dreconstruction. 3inputviewsare
highlightedinred.totrain. Forsingleimageto3D,weuse32 32patchesofraysandaglobalbatchsizeof65kthat
takes55secondstotrain. Learningrateislo× garithmicallydecayedfrom0.04to10−3. Theweight
ofthe perceptualloss(LPIPS) issetto 0.25 forsingle imageto3D andfew-viewreconstruction
on RealState10K, LLFF and DTU datasets, and to 1.0 for few-view reconstruction on CO3D an
MipNeRF-360dataets.
Distancebasedweighting. Wedesignaweightingschedulethatupweightsviewsclosertocaptured
viewsinthelaterstageoftrainingtoimprovedetails.Specifically,theweightingisgivenbyaGaussian
kernel: w
exp(cid:0) bs2(cid:1)
,wheresisthedistancetotheclosestcapturedviewandbisascalingfactor.
∝ −
Forfew-viewreconstruction,bislinearlyannealedfrom0to15. Wealsoannealtheweightingof
generatedviewsgloballytofurtheremphasizetheimportanceofcapturedviews.
20