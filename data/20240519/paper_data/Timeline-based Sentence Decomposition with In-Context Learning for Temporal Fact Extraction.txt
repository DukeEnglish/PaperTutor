Timeline-based Sentence Decomposition with In-Context Learning for
Temporal Fact Extraction
JianhaoChen1,HaoyuanOuyang1,JunyangRen1,WentaoDing2,WeiHu1,YuzhongQu1
1StateKeyLaboratoryforNovelSoftwareTechnology,NanjingUniversity,China
2StateKeyLaboratoryofGeneralArtificialIntelligence,BeijingInstituteforGeneralArtificialIntelligence(BIGAI)
{jh_chen, hyouyang, jyren}@smail.nju.edu.cn, dingwentao@bigai.ai, {whu, yzqu}@nju.edu.cn
Abstract
Facts extraction is pivotal for constructing
knowledge graphs. Recently, the increasing
demandfortemporalfactsindownstreamtasks
hasledtotheemergenceofthetaskoftemporal
fact extraction. In this paper, we specifically
addresstheextractionoftemporalfactsfrom
naturallanguagetext. Previousstudiesfailto
handle the challenge of establishing time-to-
factcorrespondencesincomplexsentences. To
overcomethishurdle, weproposeatimeline-
based sentence decomposition strategy using
largelanguagemodels(LLMs)within-context Head Entity Relation Label Tail Entity Time QualifierTime Value
NBA MVP Winner Shaquille O'Neal Point in Time 2000
learning,ensuringafine-grainedunderstanding All-Star game MVP Winner Shaquille O'Neal Point in Time 2000
Finals MVP Winner Shaquille O'Neal Point in Time 2000
of the timeline associated with various facts.
NBA MVP Winner Willis Reed Point in Time 1970
In addition, we evaluate the performance of All-Star game MVP Winner Willis Reed Point in Time 1970
Finals MVP Winner Willis Reed Point in Time 1970
LLMsfordirecttemporalfactextractionand NBA MVP Winner Michael Jordan Point in Time 1996
All-Star game MVP Winner Michael Jordan Point in Time 1996
getunsatisfactoryresults. Tothisend,weintro-
Finals MVP Winner Michael Jordan Point in Time 1996
duceTSDRE,amethodthatincorporatesthe NBA MVP Winner Michael Jordan Point in Time 1998
All-Star game MVP Winner Michael Jordan Point in Time 1998
decomposition capabilities of LLMs into the Finals MVP Winner Michael Jordan Point in Time 1998
traditional fine-tuning of smaller pre-trained
Figure1: Adifficulttemporalfactextractionexample,
languagemodels(PLMs). Tosupporttheevalu-
whichcontains12temporalfactsinonesentence.
ation,weconstructComplexTRED,acomplex
temporal fact extraction dataset. Our experi-
mentsshowthatTSDREachievesstate-of-the-
tothetriple. Atemporalfactcanbeformalizedasa
art results on both HyperRED-Temporal and
ComplexTREDdatasets. quintuple(e head,r,e tail,q,t),wherethetimequal-
ifierq andtimevaluetindicatethetimedimension
1 Introduction
informationofthetriple.
Acquiringknowledgehaslongbeenafundamental Although many works (Wang et al., 2011; Liu
challengeinthefieldofartificialintelligence. Typ- et al., 2021; Chia et al., 2022) have explored the
ically,theacquiredknowledgeisstoredinknowl- taskoftemporalfactextraction,thechallengeofes-
edge graphs (KGs) as triples, comprising a head tablishingcorrespondencebetweentimeandtriples
entity,arelation,andatailentity. Recently,there remainsunresolved. Figure1showsadifficultex-
hasbeenasignificantsurgeintheneed(Chenetal., ample of temporal fact extraction. It can be seen
2023;Liuetal.,2023;Dingetal.,2022;Xuetal., that the difficulty of this example mainly comes
2023;Lietal.,2022;Mezni,2022)foracquiring fromtheuseof“theother"toimplythattheother
temporalfacts. Forinstance,usersmayseekinfor- twoplayersalsowonthreeawardsinoneyear. Be-
mationfromtheKGs,suchas“WhendidMichael sides, natural language expressions are typically
Jordan win the NBA Finals MVP?”. However, succinct,with"MichaelJordanin1996and1998"
traditionaltripleslike(NBAFinalsMVP,winner, aloneexpressingsixtemporalfacts. Existingmeth-
Michael Jordan) cannot meet such needs due to odscannothandlethissituationwithoutexplicitly
thelackoftimedimension. Thiscanbesolvedby stating what event occurred at a certain point in
addingtimeinformationlike(PointInTime,1996) time,leadingtosuboptimalperformanceinaddress-
4202
yaM
61
]LC.sc[
1v88201.5042:viXra
E
W in n er
S h a q u ille O 'N e a l is o n e
N B A M V P A ll-S ta r g a m e M V,
P oint in T im es
a m e y e a r (2 0 0 0 ); th e o th e r
a n d M ic h a e l J o rd a n in 1 9 9 6 a
P oint in T im e
W in n er
H e a d E n tity; R e la tio n L a b e l; T
x tra c te d F a c ts :
o f o n ly th re e p la y e rs to w in
P F in a ls M V Pa n d a w a rd s in th e
p la y e rs a re W illis R e e d in 1 9 7 0P
oint in T im e
W in n ern
d 1 9 9 8 .
P oint in T im e
a il E n tity ; T im e Q u a lifie r; T im e V a lu eingcomplexnarrativeswithinterwoventimelines. sentences with multiple time expressions or
To address this gap, we propose a timeline- temporalfacts.
basedsentencedecompositionstrategy. Ourstrat-
egy involves breaking down sentences according 2 RelatedWork
to their timelines to capture the temporal dimen-
sionsoffacts,ensuringafine-grainedunderstand- 2.1 TemporalFactExtraction
ingofthetimelineassociatedwithvariousfacts. In
Temporal fact extraction methods are mainly di-
thepast,sentencedecompositionoftenrequireda
videdintotwocategories: pattern-basedmethods
large amount of training corpus. But now, the in-
anddeeplearning-basedmethods.
contextlearningcapabilityoflargelanguagemod-
els(LLMs)empowersustoperformtimeline-based
Pattern-basedmethods Existingworkattempts
sentencedecompositionwithouttrainingcorpus.
to treat relationships and qualifiers as a whole to
Tothebestofourknowledge,wearethefirstto
mine corresponding patterns in text, or use se-
investigate the application of LLMs for temporal
quence annotation and classification methods. T-
fact extraction tasks. We conduct an evaluation
Yago(Wangetal.,2010)andasimilarstudy(Kuzey
toassesstheperformanceofemployingLLMsdi-
and Weikum, 2012) extract temporal instances
rectlyfortemporalfactextraction. However,itis
from semi-structured data like Wikipedia’s In-
unsatisfactorythattheperformanceofLLMsdoes
foboxes,Categories,andLists,limitingcoverage
notsurpassthetraditionalapproachoffine-tuning
tofreelyavailabletext. Pravda(Wangetal.,2011)
smallerPLMs. Toenhancetheperformanceoftem-
uses textual patterns to represent candidate facts
poralfactextractionmethods,wecomeupwiththe
andlabelsthemthroughagraph-basedlabelpropa-
ideaofcombiningthesurprisingsentencedecom-
gationalgorithm. Liuetal.(2021)appliestheidea
position capability of LLMs with the traditional
ofdistantsupervision,leveragingexistingtemporal
wayoffine-tuningsmallpre-trainedlanguagemod-
factstolearncorrespondingpatternsfromwebtext
els. Experiments show that our method based on
andsubsequentlyapplyingthemtotheextraction
thecombiningideaachievesSOTAresults.
process.
In addition to research on extraction methods,
there are currently few benchmarks (Chia et al.,
Deep learning-based methods CubeRE (Chia
2022;Wangetal.,2012)fortemporalfactextrac-
etal.,2022)firstemploysasequencelabelingap-
tion, and they do not pay much attention to sen-
proach to identify entities and time, followed by
tences involving complex time related narratives
classificationoftherelationsandqualifiersbetween
(wecallthemcomplexsentencesforshort). Tothis
them.
end,weconstructatemporalfactextractiondataset
Previous Studies have not fully addressed the
composedofcomplexsentencesforevaluation.
challengesposedbycomplexsentencesintempo-
Insummary,ourmaincontributionsinthispaper
ral fact extraction. Moreover, we are the first to
areoutlinedasfollows:
explore the application of LLMs for the task of
1. We summarize the unique challenge of the temporalfactextraction.
temporal fact extraction task and propose
a timeline-based sentence decomposition 2.2 SentenceDecomposition
methodonnaturallanguageusingin-context
Sentencedecompositionisacommonmethodused
learningenhancedbyhumanfeedback.
for tasks in the NLP field. From a technical per-
spective, sentence decomposition can be roughly
2. Weproposetimeline-basedsentencedecom-
divided into two categories: supervised learning
position (TSD) for the temporal fact extrac-
methods(Huangetal.,2023)andrule-basedmeth-
tiontask. Evaluationresultsdemonstratethat
ods(Huetal.,2021). Supervisedlearningrequires
TSDindeedhelpsmodelsunderstandthecor-
sufficienttrainingcorpus, whilerule-basedmeth-
respondencebetweeneventsandtime.
odsarelabor-intensiveandoftensufferfrompoor
3. We conduct evaluations of the methods that coverage issues. In the era of large models, we
utilize LLMs in the task of temporal extrac- exploreusingin-contextlearningforsentencede-
tion. Moreover,webuildanoveldatasetCom- composition,whichdoesnotrequiretask-specific
plexTRED,whichconsistsof19,148complex trainingdataandhassufficientcoverage.2.3 In-ContextLearning Example1. PeterWhittlewaselectedafellowof
theRoyalSocietyin1981.
Large language models, exemplified by the GPT
ExtractedTemporalFacts:
series (Brown et al., 2020; OpenAI, 2023) and
(PeterWhittle,memberof,RoyalSociety,start
the LlaMA (Touvron et al., 2023a,b) family, are
time,1981)
knowntohaveimpressivein-contextlearningabili-
ties. TheseLLMshavebeenproventobeableto Ontheotherhand,acomplexsentenceinvolves
solveacompletelynewproblemwithasmallnum- twoormoretimeelementsortwoormorefacts,of-
berofexampleswithouttheneedfortask-specific tenintroducingchallengesintimeselectionorrela-
trainingleadingtoasurgeinexplorationwithinthe tionextraction. Understandingthecorrespondence
domainofin-contextlearning. betweentimeandfactsbecomesmorechallenging
Generally,researchonin-contextlearningcanbe in the presence of multiple time references com-
categorizedintotwomainareas. Thefirstfocuses pared to a situation with a single time reference.
on the strategy for selecting examples (Li et al., Figure1hasshownacomplexsentencewithmore
2023) to enhance performance, while the second thantwotimeelements.
delvesintotheinterpretabilityaspects(Hanetal., Example2providesanotherconcreteinstanceof
2023) of in-context learning. Our study focuses acomplexsentenceinthecontextoftemporalfact
on example selection. We iteratively constructed extraction.
demonstrativeexamplesfortaskscenarioswithout
Example2. 20November1883,JulesFerrysuc-
trainingcorpus. Moreover,weincorporatehuman
ceeds Challemel-Lacour as Minister of Foreign
feedbacktoguidethemodelandpreventcommon
Affairs.
errors.
ExtractedTemporalFacts:
3 Preliminary (JulesFerry,positionheld,MinisterofForeign
Affairs,starttime,20November1883)
3.1 ProblemFormulation
(Challemel-Lacour, position held, Minister of
Givenaninputsentenceofnwordss={x ,x ,..., ForeignAffairs,endtime,20November1883)
1 2
x }, theobjectiveofthetemporalfactextraction (JulesFerry,replaces,Challemel-Lacour,point
n
task is to extract all existing temporal facts in s. intime,20November1883)
Formally,atemporalfactisrepresentedas(e ,
head AlthoughthereisonlyonepointintimeinExam-
r,e ,q,t). Anentityeisaconsecutivespanof
tail ple 2, this sentence contains three temporal facts.
words where e = {x , x , ..., x }, i,j ∈ {1, ...,
i i+1 j This is mainly because the word "succeeds" ex-
n}. r represents the relation between head entity
presses the relationship of succession at this mo-
e and tail entity e . r ∈ R, where R is the
head tail ment. Implicitexpressionssuchastheseimplythe
predefined set of relation labels. The qualifier q
connectionofeventsinthetimedimension,which
andthetimevaluetindicatesthetimedimension
bringschallengestothecompleteextractionoftem-
oftherelationtriplet(e ,r,e ). q ∈ Q,where
head tail poralfacts.
Qisthepredefinedsetofqualifierlabels.
4 Method
3.2 ComplexSentenceinTemporalFact
Extraction Inthissection,webeginbypresentingtwodirect
Inthispaper,wedelineatethedistinctionsbetween approaches to leverage LLMs for temporal fact
simple and complex sentences in the context of extraction. Followingthat,weintroduceourextrac-
temporalfactextraction. Priorresearchhasshown tionmethods,encompassingatimeline-basedsen-
a lack of emphasis on complex sentences. How- tence decomposition strategy and the fine-tuning
ever,thispapershiftsitsfocustothetemporalfact ofgenerativemodelsusingdecompositionresults
extractionofcomplexsentences. fortraining.
Asimplesentencecomprisesonlyonetimeele-
4.1 In-ContextLearningwithChatGPT3.5
mentandonetemporalfact,presentingrelatively
andFine-tuningOpen-SourceLLM
lowerdifficultyinrelationextractionandtimese-
lection. Example1providesaconcreteinstanceof WetrytoemployLLMsdirectlyfortemporalfact
a simple sentence in the context of temporal fact extraction through in-context learning and fine-
extraction. tuning.Text: Shaquille O'Neal is one of only three players to win NBA MVP, All-Star game MVP and Finals MVP
awards in the same year (2000); the other players are Willis Reed in 1970 and Michael Jordan in 1996 and 1998 .
InputPrompt: time
Extractthequintuples[‘headentity’,‘relation’, recognition Decomposition Input Prompt:
‘tail entity’, ‘qualifier’, ‘time point’] from the Extract the quintuples [‘head entity’,
input Text. The output format is a list of 2000 ‘relation’, ‘tail entity’, ‘qualifier’, ‘time
quintuples. 1970 point’] from the input Text. The output
Text: 1996 formatisalistofquintuples.TIMELINE
ShaquilleO'Nealisoneofonlythreeplayers… 1998 givestheresultsoftheeventsintheTEXT
sortedaccordingtothetimelinetoassistin
Flan-T5 yourextraction.
Text:
Flan-T5 Shaquille O'Neal is one of only three…
Output: Timeline:
[‘NBA MVP’/‘All-Star game 22000000:: SShhaaqquuiillllee OO''NNeeaall……
Output: M ‘ShV aP q’ u/‘ ilF li ena Ols 'NeaM l’,V ‘P p’ o, int‘ iw nin tin me er ’’ ,, 1 11 19 99 97 97 90 60 6: ::
:
WW MMii iill ccll hhiiss
aa
eeRR
ll
ee JJee oodd
rr
dd……
aann……
TSD
[‘NBA MVP’/‘All-Star game MVP’/‘Finals ‘2000’], 11999988:: MMiicchhaaeell JJoorrddaann……
MVP’, ‘winner’, ‘Shaquille O'Neal’, ‘point in […,‘WillisReed’,…,‘1970’],
time’,‘2000’] […,‘MichaelJordan’,…,‘1996’],
[…,‘MichaelJordan’,…,‘1998’],
Low Recall High Recall
Figure2:FrameworkcomparisonbetweenFlan-T5andTSDRE.LeveragingTimeline-basedsentencedecomposition
fortrainingcansignificantlyimprovetherecalloftheFlan-T5.
Specifically, we apply in-context learning to Decomposition: 2000: Shaquille O’Neal is
ChatGPT3.5. Toconstructtheprompt,wefirstgive one of only three players to win NBA MVP,
ataskdescription: Extract all the quintuples All-Star game MVP, and Finals MVP awards
[subject, relation, object, qualifier, in the same year. 1970: Willis Reed won
time point] from the input text,followedby NBA MVP, All-Star game MVP, and Finals MVP
thespecifiedrelationshiplistandqualifierlist. We awards in the same year. 1996: Michael
thenselect48examplesatrandomfromthetrain Jordan won NBA MVP, All-Star game MVP, and
settoensurethatall relationsandqualifiershave Finals MVP awards in the same year. 1998:
atleastone-shotexamples. Thecompleteprompt Michael Jordan won NBA MVP, All-Star game
is more than 2,000 tokens and will be shown in MVP, and Finals MVP awards in the same
AppendixE. year.
WealsotrytoLoRAfine-tuneLlama2(7B).We
stillgiveLlama2thetaskdescriptionastheinstruc-
tiontomakeLlama2betterunderstandthetask.
4.2 Timeline-basedSentenceDecomposition
1970 1996 1998 2000
Timeline-based sentence decomposition can be
Figure3: Playerawardsarepresentedinatimeline.
used to understand and process texts containing
temporalinformation. Ithelpsorganizeandpresent
Due to the absence of annotated data, we con-
thetimelineinsentences,makingiteasierformod-
siderleveragingthein-contextlearningcapability
els to understand and follow the development of
of LLMs to accomplish the decomposition task.
events.
Before starting in-context learning, we use SU-
Anexampleofdecompositionisasfollows: Time(ChangandManning,2012)toidentifytime
Text: Shaquille O’Neal is one of only expressionsinsentences,asspecializedtoolsexcel
three players to win NBA MVP, All-Star intimerecognitioncomparedtolargeLLMs. With-
game MVP, and Finals MVP awards in the outevenadecompositionexample,weiteratively
same year (2000); the other players are constructthepromptforin-contextlearning. First,
Willis Reed in 1970 and Michael Jordan in weonlygiveChatGPT3.5thetaskdescriptionand
1996 and 1998. onetestsentence:
Time: [‘2000’, ‘1970’, ‘1996’, ‘1998’] Instruction: First, I will give you aText, and secondly, I will give you the the Judge of Gallura and married the
time contained in the Text. You need heiress Elena.
to sort out the events that occurred at Figure2illustratesthedistinctionbetweentrain-
each time I provided based on the Text. ing directly and training with the utilization of
The Decomposition format is required to temporal-basedsentencedecomposition(TSD)in-
contain a time point and corresponding formationintheframework. Wenamethemethod
events in each sentence. Each sentence offine-tuningFlan-T5usingTSDasTSDRE.
contains the complete elements of the
5 ComplexTRED:AComplexTemporal
event, such as subject, predicate, and
object. FactExtractionDataset
Weundertakethisexplorationtounderstandthe
Thereareveryfewtemporalfactextractiondatasets
output format preferences of ChatGPT3.5. This
availabletodate. Pravda(Wangetal.,2012)lacks
knowledgewillguideusinuncoveringthepoten-
annotationlabelsandthereforecannotbeusedfor
tialofthemodeltoachieveoptimalperformanceby
supervisedlearning. TheWiki-PeopleDataset(Liu
aligningwiththesepreferences. Subsequently,we
etal.,2021)isnotopensource. HyperRED(Chia
makeslightmodificationstotheoutputtogenerate
et al., 2022) is a hyper-relational fact extraction
ademonstrativeexamplethatisbothaccurateand
dataset,48%ofwhicharetemporalfacts. However,
in line with the model’s preferred output format.
manysamplesinHyperREDareoverlysimplistic
Afterseveraliterationslikethis,wegetthedemon-
andinsufficienttodepictthepotentialdifficulties
strativeexamplesneededforin-contextlearning.
in complex practical scenarios. Specifically, the
majorityofsentencesinHyperREDconsistsolely
HumanFeedbackEnhancedIn-ContextLearn-
ofasingletemporalexpression,andtheirrespective
ing We find that even though we have shown
extractionresultscompriseasingulartemporalfact.
ChatGPT3.5high-qualitydemonstrativeexamples,
We need a complex temporal fact extraction
it still makes mistakes when outputting. We add
dataset to evaluate our method and train existing
negativeexamplesintheprompttohelpthemodel
models on their ability to extract temporal facts
avoid common mistakes. We carefully select the
fromcomplextemporalsentences. Belowwewill
examplesthatwebelievearerepresentativeofthe
introducedatacollectionanddatasetstatistics.
mistakesmadebyChatGPT3.5. Subsequently,we
addhumanfeedbacktopointoutthemistakesand
5.1 DataCollection
providecorrectedanswers. Thecompleteprompt
It is very challenging to construct a large-scale,
willbeshowninAppendixE.
diverse complex temporal fact extraction dataset.
4.3 Fine-tuningModelswithTimeline-based Thetwomajordifficultiesarethenumberofcom-
SentenceDecomposition plextemporalsentencesandhigh-qualityannotated
labels. Inordertocollectenoughdata,weusetwo
We combine the natural language understanding
methodstoobtaindata: usingdistantsupervision
and reasoning capabilities emerging from very
toobtainthealignmentofwebtextandtemporal
largeLMswithsmallerLMsfine-tunedforspecific
factinKG,andmanuallycorrectingthesamplesin
tasks. Specifically, we fine-tune generative lan-
HyperRED. We will introduce our meansof con-
guagemodels(Llama2andFlan-T5)withtimeline-
trollingthequalityofthedatasetintheintroduction
basedsentencedecomposition. Wesplicethetext
ofeachmethod.
inthedatasetwiththedecompositionresultsgen-
eratedbyChatGPT3.5asanewinputfortraining. DistantSupervision Wecollecttheintroduction
The following is an example of the input we pro- sectionsof343,603DBpediaarticles,thefulltext
videformodels: of401,796Wikipediaarticles,and3,002,373Wiki-
Input: Text: Lamberto Visconti di Eldizio datatemporalfactsforalignment. Specifically,we
( died 1225 ) was the Judge of Gallura from use DBpedia Spotlight (Mendes et al., 2011) for
1206 , when he married the heiress Elena entitylinkingandSUTime(ChangandManning,
, to his own death . Decomposition: 1225: 2012) to extract time entities from DBpedia and
Lamberto Visconti di Eldizio passed away Wikidataarticles. Indistantsupervision, ifasen-
and ended serving as the Judge of Gallura. tencecontainstheheadentity,tailentityandtem-
1206: Lamberto Visconti di Eldizio became poral entity of a temporal fact, then we align thefacttothesentence. HyperRED-Temporalintermsofsentencelength,
Distant supervision encounters two significant temporalexpressions,andtemporalfactscontained
challenges: noise and incomplete facts (The sen- per sentence, which reflects the difficulty of the
tencemayexpressfactsthatarenotintheknowl- datasettoacertainextent.
edgegraph). However,byaligningthetwoentities
plus time, the noise problem is greatly mitigated. Metrics HyperR. ComplexT.
Totackletheincompletefactsproblem,welever- Sentencelength 31.47 41.27
ageChatGPTtocompletethefactscontainedinthe
Timeexpressions 1.95 3.46
sentence. Subsequently,weorganize50computer
Temporalfacts 1.38 2.10
scienceundergraduatestudentstomanuallylabel
thecorrectnessoftheaddedfactsbyChatGPT3.5.
Table2: Sample-levelcomparisonbetweenHyperRED-
Finally,weobtainabout17,000complexsentences
TemporalandComplexTRED.Eachmetriciscalculated
throughdistantsupervision.
onanaveragepersamplebasis.
ManualCorrectionfromHyperRED Samples
in HyperRED are alignment with Wikipedia text
andWikidatadata,whicharefromthesamesource 6 Experiments
as our samples. Therefore, we correct the labels
6.1 ExperimentalSetup
ofsomecomplexsentencesfromHyperREDand
absorbthemintoourdataset. Wefirstselectabout Datasets Weassesstemporalfactextractionus-
7,000 sentences in HyperRED that contain more ingHyperRED(Chiaetal.,2022),apubliclyavail-
thanorequaltotwotimeexpressionsascandidates. able benchmark, and ComplexTRED, a dataset
Thenweorganize70computerscienceundergrad- specifically crafted by us for complex temporal
uates to correct the labels of each sentence, and fact extraction. We select samples in HyperRED
thecorrectedsampleswillbeaddedtoourdataset. whose labels are temporal facts, delete the rela-
Finally,weget2,589correctedsamples. tions that have little correlation with time, and fi-
nallyformHyperRED-Temporalastheevaluation
5.2 DatasetStatistics dataset. Thestatisticsofthetwodatasetsareshown
Toensurethatthetrainset,devset,andtestsetdata inTable1.
adheretoanindependentandidenticaldistribution,
ComparedMethods Wecompareourapproach
weemploystratifiedsamplingontheoriginaldata
with the SOTA extraction method reported on
basedontherelationtypes. Ourgoalistomaintain
CubeRE (Chia et al., 2022) and several baseline
an8:1:1ratioforeachrelationtypeinthetrain,dev,
methods. We evaluated the performance of di-
andtestset,respectively. Theresultingsplittingof
rectly employing LLMs for temporal fact extrac-
the dataset is presented in Table 1. We manually
tion, including LoRA fine-tuning the 7B version
checkallsamplesinthedevandtestsettoensure
of Llama2 (Touvron et al., 2023b) and naive in-
thedataset’squality.
contextlearningwithChatGPT3.5. Wealsotried
toleverageourTimeline-basedSentenceDecompo-
Datasets #Sent. #Facts #Rel.
sition(TSD)toenhanceLoRAfine-tunedLlama2.
HyperR. Train 17,004 23,507 45
In addition to directly employing LLMs, we
Dev 432 582 37 also evaluated the performance of Flan-T5-
Large (Chung et al., 2022), a relatively smaller
Test 1,712 2,391 41
pre-trainedlanguagemodel(PLM).Wetransplant
ComplexT. Train 16,573 33,632 40
the method of Wadhwa et al. (2023) on relation
Dev 1,679 4,025 40 extraction (triple extraction) to temporal fact ex-
traction,whichtrainsFlan-T5withLLM-generated
Test 1,584 3,964 39
explanations. WeuseChatGPT3.5togeneratethe
Table1: Statisticsaboutthenumberofsentences,tem- explanationsrequiredfortraininginsteadofGPT-3
poralfacts,andrelationsofthetwodatasets. intheWadhwaetal.(2023),becauseChatGPT3.5
hasnotyetcomeoutwhenWadhwaetal.(2023)
The analysis of the sample is shown in the Ta- was published and we believe ChatGPT3.5 per-
ble 2. ComplexTRED significantly outperforms formsbetterthanGPT-3.HyperRED-Temporal ComplexTRED
Methods
Pr Re F Pr Re F
1 1
ChatGPT3.5† 11.26 20.08 14.43 19.01 23.57 21.05
Llama2wLoRA† (Touvronetal.,2023b) 56.95 31.87 40.87 35.72 17.74 23.71
Llama2wLoRA† +TSD† (OURS) 56.60 48.06 51.98 40.64 27.18 32.58
CubeRE(Chiaetal.,2022) 55.31 49.64 52.33 41.69 27.92 33.44
Flan-T5(Chungetal.,2022) 66.64 61.06 63.73 47.64 34.86 40.26
Flan-T5+Explanation† (Wadhwaetal.,2023) 64.99 58.85 61.76 47.69 34.07 39.75
TSDRE† wFlan-T5(OURS) 68.61 64.91 66.71 48.99 37.61 42.55
Table3: MainresultsonHyperRED-TemporalandComplexTRED.†denotesthatthecorrespondingfoundation
modelisaLLM.
Evaluationmetrics Wereporttheprecision(de- improvement on HyperRED-Temporal, and a 9-
notedasP),recall(denotedasR),andF scoreof point F score improvement on ComplexTRED.
1 1
the evaluation results. When calculating metrics, Thisresultindicatesthatintegratingourdecompo-
weemployanexactmatch(atstringlevel)approach sition method into training enables the model to
withoutfollowingWadhwaetal.(2023)’smethod betterlearntherelationshipsbetweensentencefea-
of utilizing manual assessment for the outputs of turesandtemporalfacts. Anotherpossiblereason
large-scalemodels. we speculate is that high-quality decomposition
results are suitable as training data and make the
6.2 Mainresults trainingofLlama2moresufficient.
Wefirstreporttheresultsofmethodsdirectlybased
ResultsofcombiningLLMsandsmallerPLMs.
onLLMs,andsubsequently,wereporttheresults
Flan-T5(Large)achievedsurprisingresultsonboth
of approaches that involve combining LLMs and
datasets. We believe that compared to LLMs,
smallerPLMs.
smallerPLMsareeasiertofine-tuneandfitwhen
thetrainingdataisinsufficient. However,enhanc-
LLMs results. Based on Table 3, it is evident
ingFlan-T5withChatGPT3.5-generatedexplana-
that both Fine-tuned open-source LLMs and in-
tionresultedinatinydropinF onbothdatasets.
context learning LLMs do not perform satisfac- 1
Thismethodmaybedisadvantageousunderexact
torily on both datasets. Among them, in-context
match,whichwehavementionedintheevaluation
ChatGPT3.5consistentlygetsthelowestF score.
1
metricsinSection6.1. Anotherreasonwebelieve
Itisreasonabletoachievesuchperformancewith-
isthatthetemporalfactextractiontaskisinherently
out training. HyperRED-Temporal includes 45
morechallengingtointerpretcomparedtothefact
relation types and 4 qualifier types, while Com-
(triple) extraction task. Finally, our method TS-
plexTREDincludes40relationtypesand4quali-
DRE,whichenhancesFlan-T5withtimeline-based
fiertypes. Despiteprovidingdetailedtasksettings
sentencedecomposition(TSD),achievesstate-of-
for ChatGPT3.5, it is expected to be exceedingly
the-art results on both datasets. We achieve re-
challengingtoprovideaccurateanswersacrossall
markable results by combining the strengths of
180(45*4)/160(40*4)categoriesbasedsolelyon
bothlargeandsmallLMs: LargeLMsexcelatef-
afew-shotexamples. Fine-tunedLlama2(7B)per-
fectivelyorganizingtimelinesinnaturallanguage,
formsbetterthanin-contextChatGPT3.5,butitsF
1
whilesmallLMsprovemoreadeptatprecisefine-
scorestillfallsconsiderablyshortofCubeRE.This
tuningforspecifictasks.
couldbeattributedtotheinsufficienttrainingsam-
ples to effectively support such a large language
6.3 DecompositionQuality
model,leadingtosuboptimalfitting. Additionally,
we conducted experiments using our decomposi- Werandomlyselect100sentencesandinvitethree
tionmethodtoenhanceLlama2training. Thisre- experts to evaluate our decomposition results of
sulted in Llama2 achieving an 11-point F score thesesentences. Westilluseprecisionandrecallas
1evaluationmetrics. Thegoalofourdecomposition nition(NER)error,RelationExtractionerror,Time
istodividedifferenteventsinthetextintodifferent Selectionerror,QualifierClassificationerror,False
pointsintime. Inthisscenario,TruePositiverefers Negative and Missing Facts. It is worth noting
tothecountofeventscorrectlyclassifiedattheirre- thattheerrorratesofTimeselectionandQualifier
spectivetimepointsinthepredictionresults. False Classificationareverylow,showingthatTSDRE
Positiveindicatesthecountofeventserroneously perform well when faced with time-to-fact corre-
classifiedatthewrongtimepoint. FalseNegative spondences. Inaddition,underrelaxedstandards,
represents the count of events that were not pre- thepredictionresultsofentitiesoverlappingwith
dicted. It must be pointed out that the relatively the answer entities and the prediction results of
ambiguousaspectisthatopinionsdifferamongin- falsenegativescanbothbecountedascorrect. This
dividualsregardingwhatshouldbeconsideredan meansthattheactualperformanceofthemodelis
eventinnaturallanguagetext. muchbetterthanthescoresundertheexactmatch
measurement. Finally, completely wrong NER,
Precision Recall wrongRelationExtraction,andMissingFactsare
stillthelegacyproblemsoffact(triple)extraction.
Prompt 93.80 93.53
Prompt+feedback 94.65 95.57
6.5 CaseStudy
As is shown in Figure 2, Flan-T5 fails to capture
Table4: HumanevaluationofTimeline-basedsentence
decompositionresults. informationregardingtheawardsoftheothertwo
players, aside from O’Neal. However, after we
Table4showsthescoresofmanualevaluation. incorporatedecompositionintotraining,Flan-T5
Overall, our decomposition results surpass 90 in successfullyoutputsalltemporalfacts. Smallerlan-
bothprecisionandrecallmetrics,indicatingthatwe guagemodelsdohavelimitedlearningcapabilities
have acquired high-quality auxiliary information forimplicitexpressions,sointroducingLLMswith
fortraining. Thisestablishesasolidfoundationfor powerfulunderstandingcapabilitiesfornaturallan-
enhancingmodelperformance. Moreover,prompt guagecaneffectivelymakeupforthisshortcoming.
enhancedbyhumanfeedbackhasfurtherimproved
precisionandrecall. Thisshowsthathumanfeed- 7 Conclusion
backondemonstrativeexamplescanenableLLMs
In this paper, we explore the application of large
tounderstandthetaskrequirementbetterandavoid
languagemodels(LLMs)intheextractionoftem-
somecommonerrors.
poralfacts. Ourattemptsindicatethatdirectlyem-
6.4 ErrorAnalysis ploying LLMs for temporal fact extraction falls
short of achieving satisfactory results. To tackle
We select 50 sentences from each of the two
thisissue,weintroduceatimeline-basedsentence
datasetswhoseF scoresarelessthan1toanalyze
1
decomposition(TSD)method. Buildinguponthis,
TSDRE’sperformance. Theresultsareillustrated
we propose TSDRE, which employs a relatively
inTable5. Weclassifyerrortypesaccordingtothe
smaller PLM as its foundation, combined with
LLM-drivenTSDtoachievetheextraction. Exper-
MainErrors HyperR. ComplexT.
iments demonstrate that TSDRE achieves SOTA
NER 14% 24% resultsontwodatasetsandincorporatingTSDinto
-totallywrong 8% 12% thetrainingprocesscanenhancetheperformance
-overlapped 6% 12% ofLLMsontemporalfactextractiontasks. Inthe
RelationExtraction 20% 22% future,aninterestingtopicwouldbetoexplorethe
QualifierClassification 0% 0% extractionoftemporalfactsfromtextthatnecessi-
TimeSelection 2% 6% tate inferring the occurrence time based on exist-
FalseNegative 30% 22% ingtemporalreferences,suchas“threedayslater”,
MissingFacts 34% 26% whichhasnotyetreceivedwidespreadattention.
Table5: Thestatisticsofmainerrorsofsampledsen- Limitations
tences.
Ourcontributiondoeshaveimportantlimitations.
elementsofthequintupleasNamedEntityRecog- First,ourdecompositionresultsrelyonChatGPTforcompletion,anddecompositionwithouttraining References
theopen-sourceLLMscannotachievethedesired
TomB.Brown,BenjaminMann,NickRyder,Melanie
results.
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Second,weonlytestedtheeffectofChatGPTon Neelakantan,PranavShyam,GirishSastry,Amanda
theGPT3.5-turbomodel,butnotonthelatestGPT4 Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
orGPT4-turbo,duetothesignificantlyhighercost
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
involved.
ClemensWinter,ChristopherHesse,MarkChen,Eric
Third,fordocument-leveltemporalfactextrac- Sigler,MateuszLitwin,ScottGray,BenjaminChess,
tion,whencombinedwithtime-basedsentencede- Jack Clark, Christopher Berner, Sam McCandlish,
Alec Radford, Ilya Sutskever, and Dario Amodei.
compositionresults,theinputmayexceedthemax-
2020. Languagemodelsarefew-shotlearners. InAd-
imumlengthallowedbythegenerativemodel.
vancesinNeuralInformationProcessingSystems33:
Finally, our dataset construction inevitably in- AnnualConferenceonNeuralInformationProcess-
ing Systems 2020, NeurIPS 2020, December 6-12,
troducesnoiseproblemsduetotheuseofdistant
2020,virtual.
supervision. Additionally,duetolimitedresources,
weonlycheckedthevalidationsetandtestsetof
Angel X. Chang and Christopher D. Manning. 2012.
ComplexTRED, which may result in some noise Sutime: Alibraryforrecognizingandnormalizing
issuesinthetrainingset. timeexpressions. InProceedingsoftheEighthIn-
ternationalConferenceonLanguageResourcesand
Evaluation, LREC2012, Istanbul, Turkey, May23-
EthicsStatement
25,2012,pages3735–3740.EuropeanLanguageRe-
sourcesAssociation(ELRA).
Firstandforemost,ourproposedTSDREmethod
strivestoenhancetheoverallperformanceofRE ZiyangChen,JinzhiLiao,andXiangZhao.2023. Multi-
models in extracting temporal facts. However, granularitytemporalquestionansweringoverknowl-
giventheinherentblack-boxnatureofthegenera- edgegraphs. InProceedingsofthe61stAnnualMeet-
ing of the Association for Computational Linguis-
tivemodel,itisinevitablethattheextractedfacts
tics (Volume 1: Long Papers), ACL 2023, Toronto,
may possess certain quality issues. Hence, when Canada,July9-14,2023,pages11378–11392.Asso-
employing our method to extract temporal facts ciationforComputationalLinguistics.
andutilizethemfordownstreamtasks,usersmust
exercise caution in discerning the authenticity of Yew Ken Chia, Lidong Bing, Sharifah Mahani Alju-
nied,LuoSi,andSoujanyaPoria.2022. Adataset
thesefactsinordertomitigatepotentialreal-world
forhyper-relationalextractionandacube-fillingap-
consequencesarisingfromerroneousinformation.
proach. InProceedingsofthe2022Conferenceon
Secondly,fordatasetconstruction,wehavegath- EmpiricalMethodsinNaturalLanguageProcessing,
EMNLP2022,AbuDhabi,UnitedArabEmirates,De-
eredtextfromWikipediaandDBpedia,aswellas
cember7-11,2022,pages10114–10133.Association
factsfromWikidata. Thesearepubliclyavailable
forComputationalLinguistics.
datasets commonly utilized for dataset construc-
tion. Wikidata facts are under the Creative Com- HyungWonChung,LeHou,ShayneLongpre,Barret
monsCC0License1,whilethetextsobtainedfrom Zoph,YiTay,WilliamFedus,EricLi,XuezhiWang,
bothWikipediaandDBpediaarelicensedunderthe MostafaDehghani,SiddharthaBrahma,AlbertWeb-
son, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-
CreativeCommonsAttribution-ShareAlike3.0Un-
gun,XinyunChen,AakankshaChowdhery,Sharan
portedLicense23. Thus,weareabletofreelyutilize
Narang,GauravMishra,AdamsYu,VincentY.Zhao,
thisdatatoconstructourdataset, andourdataset YanpingHuang,AndrewM.Dai,HongkunYu,Slav
will be released under the same license. Further- Petrov, EdH.Chi, JeffDean, JacobDevlin, Adam
Roberts, DennyZhou, QuocV.Le, andJasonWei.
more,wehaveorganizedsomehumanannotators
2022. Scalinginstruction-finetunedlanguagemodels.
throughout the dataset construction process, and
CoRR,abs/2210.11416.
eachannotatorhasbeendulycompensatedbased
ontheirrespectiveworkinghours. WentaoDing,HaoChen,HuayuLi,andYuzhongQu.
2022. Semanticframeworkbasedquerygeneration
for temporal question answering over knowledge
1https://www.wikidata.org/wiki/Wikidata: graphs. InProceedingsofthe2022Conferenceon
Licensing#Uses EmpiricalMethodsinNaturalLanguageProcessing,
2https://www.dbpedia-spotlight.org/licenses EMNLP2022,AbuDhabi,UnitedArabEmirates,De-
3https://en.wikipedia.org/wiki/Wikipedia: cember7-11,2022,pages1867–1877.Association
Copyrights forComputationalLinguistics.VolkerGast,LennartBierkandt,StephanDruskat,and YonghaoLiu,DiLiang,MengyuLi,FaustoGiunchiglia,
ChristophRzymski.2016. Enrichingtimebank: To- Ximing Li, Sirui Wang, Wei Wu, Lan Huang, Xi-
wardsamorepreciseannotationoftemporalrelations aoyue Feng, and Renchu Guan. 2023. Local and
inatext. InProceedingsoftheTenthInternational global:Temporalquestionansweringviainformation
ConferenceonLanguageResourcesandEvaluation fusion. In Proceedings of the Thirty-Second Inter-
(LREC’16),pages3844–3850. nationalJointConferenceonArtificialIntelligence,
IJCAI 2023, 19th-25th August 2023, Macao, SAR,
XiaochuangHan,DanielSimig,TodorMihaylov,Yulia China,pages5141–5149.ijcai.org.
Tsvetkov,AsliCelikyilmaz,andTianluWang.2023.
Understandingin-contextlearningviasupportivepre- YuLiu,WenHua,andXiaofangZhou.2021. Temporal
training data. In Proceedings of the 61st Annual knowledge extraction from large-scale text corpus.
Meeting of the Association for Computational Lin- WorldWideWeb,24(1):135–156.
guistics(Volume1:LongPapers),ACL2023,Toronto,
Canada,July9-14,2023,pages12660–12673.Asso- PabloN.Mendes,MaxJakob,AndrésGarcía-Silva,and
ChristianBizer.2011. Dbpediaspotlight: shedding
ciationforComputationalLinguistics.
lightonthewebofdocuments. InProceedingsthe
7thInternationalConferenceonSemanticSystems,
XixinHu,YihengShu,XiangHuang,andYuzhongQu.
I-SEMANTICS 2011, Graz, Austria, September 7-
2021. Edg-basedquestiondecompositionforcom-
9,2011,ACMInternationalConferenceProceeding
plexquestionansweringoverknowledgebases. In
TheSemanticWeb-ISWC2021-20thInternational Series,pages1–8.ACM.
SemanticWebConference,ISWC2021,VirtualEvent,
HaithemMezni.2022. Temporalknowledgegraphem-
October24-28,2021,Proceedings,volume12922of
beddingforeffectiveservicerecommendation. IEEE
LectureNotesinComputerScience,pages128–145.
Trans.Serv.Comput.,15(5):3077–3088.
Springer.
OpenAI. 2023. GPT-4 technical report. CoRR,
XiangHuang,SitaoCheng,YihengShu,YuhengBao,
abs/2303.08774.
andYuzhongQu.2023. Questiondecompositiontree
for answering complex questions over knowledge
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
bases. InThirty-SeventhAAAIConferenceonArtifi-
Martinet,Marie-AnneLachaux,TimothéeLacroix,
cialIntelligence,AAAI2023,Thirty-FifthConference
BaptisteRozière,NamanGoyal,EricHambro,Faisal
onInnovativeApplicationsofArtificialIntelligence,
Azhar,AurélienRodriguez,ArmandJoulin,Edouard
IAAI 2023, Thirteenth Symposium on Educational
Grave,andGuillaumeLample.2023a. Llama: Open
AdvancesinArtificialIntelligence,EAAI2023,Wash-
and efficient foundation language models. CoRR,
ington,DC,USA,February7-14,2023,pages12924–
abs/2302.13971.
12932.AAAIPress.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
ErdalKuzeyandGerhardWeikum.2012. Extraction bert, Amjad Almahairi, Yasmine Babaei, Nikolay
oftemporalfactsandeventsfromwikipedia. In2nd Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Temporal Web Analytics Workshop, TempWeb ’12, Bhosale,DanBikel,LukasBlecher,CristianCanton-
Lyon,France,April16-17,2012,pages25–32.ACM. Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan CynthiaGao,VedanujGoswami,NamanGoyal,An-
Ghazvininejad,AbdelrahmanMohamed,OmerLevy, thonyHartshorn,SagharHosseini,RuiHou,Hakan
VesStoyanov,andLukeZettlemoyer.2019. Bart:De- Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
noisingsequence-to-sequencepre-trainingfornatural IsabelKloumann,ArtemKorenev,PunitSinghKoura,
languagegeneration,translation,andcomprehension. Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
arXivpreprintarXiv:1910.13461. anaLiskovich,YinghaiLu,YuningMao,XavierMar-
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
XiaonanLi,KaiLv,HangYan,TianyangLin,WeiZhu, bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
YuanNi,GuotongXie,XiaolingWang,andXipeng stein,RashiRungta,KalyanSaladi,AlanSchelten,
Qiu. 2023. Unified demonstration retriever for in- Ruan Silva, Eric Michael Smith, Ranjan Subrama-
contextlearning. InProceedingsofthe61stAnnual nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
Meeting of the Association for Computational Lin- lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
guistics(Volume1:LongPapers),ACL2023,Toronto, ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
Canada,July9-14,2023,pages4644–4668.Associa- Melanie Kambadur, Sharan Narang, Aurélien Ro-
tionforComputationalLinguistics. driguez,RobertStojnic,SergeyEdunov,andThomas
Scialom. 2023b. Llama 2: Open foundation and
Yujia Li, Shiliang Sun, and Jing Zhao. 2022. Tirgn: fine-tunedchatmodels. CoRR,abs/2307.09288.
Time-guided recurrent graph network with local-
global historical patterns for temporal knowledge NaushadUzZaman,HectorLlorens,JamesAllen,Leon
graphreasoning. InProceedingsoftheThirty-First Derczynski,MarcVerhagen,andJamesPustejovsky.
International Joint Conference on Artificial Intelli- 2012. Tempeval-3: Evaluating events, time ex-
gence,IJCAI2022,Vienna,Austria,23-29July2022, pressions, and temporal relations. arXiv preprint
pages2152–2158.ijcai.org. arXiv:1206.5333.Somin Wadhwa, Silvio Amir, and Byron C. Wallace.
2023. Revisitingrelationextractionintheeraoflarge
languagemodels. InProceedingsofthe61stAnnual
Meeting of the Association for Computational Lin-
guistics(Volume1:LongPapers),ACL2023,Toronto,
Canada,July9-14,2023,pages15566–15589.Asso-
ciationforComputationalLinguistics.
Yafang Wang, Maximilian Dylla, Marc Spaniol, and
GerhardWeikum.2012. Couplinglabelpropagation
andconstraintsfortemporalfactextraction. InThe
50thAnnualMeetingoftheAssociationforCompu-
tationalLinguistics,ProceedingsoftheConference,
July8-14,2012,JejuIsland,Korea-Volume2: Short
Papers, pages 233–237. The Association for Com-
puterLinguistics.
YafangWang,BinYang,LizhenQu,MarcSpaniol,and
GerhardWeikum.2011. Harvestingfactsfromtex-
tualwebsourcesbyconstrainedlabelpropagation. In
Proceedingsofthe20thACMConferenceonInforma-
tionandKnowledgeManagement,CIKM2011,Glas-
gow,UnitedKingdom,October24-28,2011,pages
837–846.ACM.
YafangWang,MingjieZhu,LizhenQu,MarcSpaniol,
andGerhardWeikum.2010. TimelyYAGO:harvest-
ing,querying,andvisualizingtemporalknowledge
fromwikipedia. InEDBT2010,13thInternational
ConferenceonExtendingDatabaseTechnology,Lau-
sanne,Switzerland,March22-26,2010,Proceedings,
volume426ofACMInternationalConferencePro-
ceedingSeries,pages697–700.ACM.
YiXu,JunjieOu,HuiXu,andLuoyiFu.2023. Tem-
poralknowledgegraphreasoningwithhistoricalcon-
trastivelearning. InThirty-SeventhAAAIConference
on Artificial Intelligence, AAAI 2023, Thirty-Fifth
ConferenceonInnovativeApplicationsofArtificial
Intelligence, IAAI 2023, Thirteenth Symposium on
EducationalAdvancesinArtificialIntelligence,EAAI
2023,Washington,DC,USA,February7-14,2023,
pages4765–4773.AAAIPress.A EnvironmentsandParameters the Chief Mechanical Engineer , Peter
Drummond in 1903 .
TSDRE’s results were achieved using a Python
Prediction: [[’Peter Drummond’,
implementationrunningonaworkstationwithan
’employer’, ’Highland Railway’, ’start
Intel(R) Xeon(R) Gold 5222 CPU @ 3.80GHz,
time’, ’1903’], [’Peter Drummond’,
376GB RAM and 3 NVIDIA RTX3090 graphics
’employer’, ’Highland Railway’, ’end
cards. Llama2’sresultswereachievedonawork-
time’, ’1903’]]
stationwithanIntel(R)Xeon(R)Gold6248CPU
Error Analysis: Relation should be
@2.50GHz, 472GBRAM,and8NVIDIATesla
’director/ manager’.
V100graphicscards. Thehyperparametersettings
ofCubeRE,Flan-T5,Llama2andBARTareshown C.3 MissingFacts
in Table 6. When applying LoRA to fine-tune
Input: Sir Hubert Edward Henry Jerningham
Llama2, we used a rank of 8 and an alpha value
, ( 18 October 1842 - 3 April 1914 ) was
of 32. Besides, We set the temperature value of
a British Liberal Party politician and
ChatGPT3.5to0tofacilitatereproduction.
Governor of Mauritius 1892 - 1897 , then
Governor of Trinidad and Tobago between
B TSDREwithBART
1897 and 1900 .
In this paper, we propose a pipeline architecture Prediction: [[’Hubert Edward Henry
TSDREwhichisbackbone-free. Wehavereported Jerningham’, ’position held’, ’Governor
inSection6.2thatourmodelTSDREwFlan-T5 of Trinidad and Tobago’,’start
achievestheSOTAresults. Wetesttheperformance time’, ’1897’], [’Hubert Edward
ofreplacingFlan-T5Large(770M)withaslightly Henry Jerningham’, ’position held’,
weakergenerativemodelBARTLarge(340M).The ’Governor of Mauritius’, ’end time’,
resultsareshownintable7. Fromtheexperimen- ’1897’], [’Hubert Edward Henry
tal results, it can be seen that the performance of Jerningham’, ’position held’, ’Governor
BARTasbasemodelisindeedlowerthanthatof of Mauritius’,’start time’, ’1892’]]
Flan-T5 (mentioned in Table 3). However, TSD Error Analysis: [’Hubert Edward Henry
stillenhancestheperformanceofBART. Jerningham’, ’position held’, ’Governor
of Trinidad and Tobago’,’end time’,
C ErrorAnalysis ’1900’] missed.
Herewepresentspecificexamplesofthreemajor D Differencefromthetemporalrelation
typesoferrors. extractiontask
C.1 NERError Indeed,itiseasytoconfusetemporalrelationex-
tractionwithtemporalfactextractionbasedontheir
Input: Fifteen locomotives of British
names, but they are actually very different tasks.
Rail Class 83 were built between 1960 and
Specifically, temporal relation extraction aims to
1962 by English Electric at Vulcan Foundry
identifying the temporal relation (e.g., BEFORE,
, as part of British Rail ’s policy to
AFTER,OVERLAPS)betweeneventsandtimes,
develop a standard electric locomotive .
whiletemporalfactextractions(pleaserefertoSec-
Prediction: [[’British Rail Class 83’,
tion 3.1) aims to to extract facts with temporal
’manufacturer’, ’Vulcan Foundry’, ’end
attributes (e.g., start_time, end_time). Therefore,
time’, ’1962’], [’British Rail Class 83’,
datasetsusedfortemporalrelationextractiontasks
’manufacturer’, ’Vulcan Foundry’, ’start
such as TimeBank (Gast et al., 2016) and Tem-
time’, ’1960’]]
pEval(UzZamanetal.,2012)arenotsuitablefor
Error Analysis: The tail entity should be
evaluatingourtask.
’English Electric’.
E Prompt
C.2 RelationExtractionError
E.1 In-ContextChatGPT3.5Prompt
Input: He was appointed manager of the
Highland Railway ’ s Lochgorm Works in Task: Extract all the quintuples [subject,
1903 , and promoted to Assistant to relation, object, qualifier, time point]Models Data BatchSizes Warm-up LearningRates MaxEpochs
HyperRED 32 0.2 5e-5 30
CubeRE
ComplexTRED 32 0.2 5e-5 30
HyperRED 2 0.12 2e-5 4
Flan-T5
ComplexTRED 2 0.12 2e-5 4
HyperRED 4 default 1e-4 3
Llama2
ComplexTRED 4 default 1e-4 3
HyperRED 2 0.12 2e-5 4
BART
ComplexTRED 2 0.12 2e-5 4
Table6: HyperparametersforCubeRE,Flan-T5,Llama2andBART.
HyperRED-Temporal ComplexTRED
Methods
Pr Re F Pr Re F
1 1
BARTLarge(340M)(Lewisetal.,2019) 68.85 48.89 57.18 43.68 17.52 25.00
TSDRE† wBARTLarge(340M) 71.51 50.27 59.04 45.15 18.45 26.20
Table7: PerformanceofBARTasbasemodelonHyperRED-TemporalandComplexTRED.†denotesthatthe
correspondingfoundationmodelisaLLM.
from the input text. ’point in time’, ’1983’]]
Task requirements: Extract all the Input: Alexander Mackenzie , PC ( January
quintuples [subject, relation, object, 28 , 1822 ˘2013 April 17 , 1892 ) , was a
qualifier, time point] from the input building contractor and newspaper editor
text. , and was the second Prime Minister of
Here are some concrete examples, the Canada , from November 7 , 1873 to October
output format is a list of quintuples: 8 , 1878 .
Input: He received the 1921 Nobel Prize in Output: [[’Canada’, ’head of government’,
Physics for his ¨services to theoretical ’Alexander Mackenzie’, ’end time’,
physics ¨, in particular his discovery of ’October 8 , 1878’], [’Canada’, ’head
the law of the photoelectric effect , a of government’, ’Alexander Mackenzie’,
pivotal step in the evolution of quantum ’start time’, ’November 7 , 1873’],
theory . [’Alexander Mackenzie’, ’position held’,
Output: [[’Nobel Prize’, ’winner’, ’He’, ’Prime Minister’, ’end time’, ’October
’point in time’, ’1921’], [’He’, ’award 8 , 1878’], [’Alexander Mackenzie’,
received’, ’Nobel Prize’, ’point in time’, ’position held’, ’Prime Minister’, ’start
’1921’]] time’, ’November 7 , 1873’]]
Input: It won the 1991 Nebula Award for Input: There has been a resident Treasury
Best Novelette and was nominated for the or Downing Street cat employed as a mouser
1991 Hugo Award for Best Novelette . and pet since the reign of Henry VIII ,
Output: [[’It’, ’award received’, ’Nebula when Cardinal Wolsey placed his cat by his
Award’, ’point in time’, ’1991’], [’It’, side while acting in his judicial capacity
’nominated for’, ’Best Novelette’, ’point as Lord Chancellor , an office he assumed
in time’, ’1991’]] in 1515 .
Input: They are currently the only club Output: [[’Cardinal Wolsey’, ’position
in Ulster to have won an All - Ireland held’, ’Lord Chancellor’, ’start time’,
Senior Club Hurling Championship , which ’1515’]]
they first won in 1983 . Input: He had 24 caps for Japan , from 1974
Output: [[’All - Ireland Senior Club to 1984 , scoring 3 tries , 5 conversions
Hurling Championship’, ’winner’, ’They’, , 14 penalties and 3 drop goals , in anaggregate of 73 points . Input: He was International President of
Output: [[’He’, ’member of sports team’, WWF from 1996 to 1999 succeeding Prince
’Japan’, ’start time’, ’1974’], [’He’, Philip , the Duke of Edinburgh .
’member of sports team’, ’Japan’, ’end Output: [[’WWF’, ’chairperson’,
time’, ’1984’]] ’He’, ’end time’, ’1999’], [’WWF’,
Input: He later played professional ’chairperson’, ’He’, ’start time’,
football in the American Football League ’1996’]]
, appearing in 42 games as a tackle and Input: Timothy Fok Tsun - ting ( born 14
defensive end for the New York Titans ( February 1946 in Hong Kong ) , GBS , JP ,
later renamed the Jets ) from 1960 to 1962 the eldest son of Henry Fok , is a Member
. of the Legislative Council of Hong Kong
Output: [[’He’, ’member of sports team’, , representing the Sports , Performing
’New York Titans’, ’start time’, ’1960’], Arts , Culture and Publication functional
[’He’, ’member of sports team’, ’New York constituency .
Titans’, ’end time’, ’1962’]] Output: [[’Henry Fok’, ’child’, ’Timothy
Input: " Suedehead " is the debut solo Fok Tsun - ting’, ’start time’, ’14
single from Morrissey , released in February 1946’]]
February 1988 . Input: The 2013 Philadelphia Eagles
Output: [[’Suedehead’, ’performer’, season was the franchise ’ s 81st season
’Morrissey’, ’publication date’, in the National Football League , and the
’February 1988’]] first under head coach Chip Kelly .
Input: It closed on 1 December 2003 Output: [[’Chip Kelly’, ’coach of sports
when operation of the line was suspended team’, ’Philadelphia Eagles’, ’start
between Kabe Station and Sandanky˘014d time’, ’2013’]]
Station . Input: Shek Kip Mei Station served as a
Output: [[’It’, ’adjacent station’, terminus in the very early phase of the
’Kabe’, ’end time’, ’1 December Kwun Tong Line ( Shek Kip Mei to Kwun Tong
2003’], [’It’, ’adjacent station’, , 1 October 1979 to 31 December 1979 ) .
’Sandanky˘014d’, ’end time’, ’1 December Output: [[’Shek Kip Mei’, ’connecting
2003’]] line’, ’Kwun Tong Line’, ’start time’,
Input: The 2008 presidential campaign of ’1 October 1979’]]
Barack Obama , then junior United States Input: K˘00f6nigsberg was transferred to
Senator from Illinois , was announced Soviet control in 1945 after World War II
at an event on February 10 , 2007 in .
Springfield , Illinois . Output: [[’K˘00f6nigsberg’, ’country’,
Output: [[’Barack Obama’, ’candidacy in ’Soviet’, ’start time’, ’1945’]]
election’, ’2008 presidential campaign’, Input: She was the Director of the Walter
’start time’, ’February 10 , 2007’]] and Eliza Hall Institute of Medical
Input: Among his victories were in Research ( WEHI ) , from 1996 until 30
reconquering Ji ’ an in Jiangxi Province June 2009 and remains a faculty member
in 1856 , as well as leading the assault , having rejoined the institute ’ s
on the Taiping capital at Nanjing in 1864 Molecular Genetics of Cancer Division .
. Output: [[’WEHI’, ’director / manager’,
Output: [[’Nanjing’, ’capital of’, ’She’, ’end time’, ’30 June 2009’],
’Taiping’, ’end time’, ’1864’]] [’WEHI’, ’director / manager’, ’She’,
Input: He was also one of the original ’start time’, ’1996’]]
correspondents on Comedy Central ’ s The Input: He previously served as Commander ,
Daily Show from 1996 to 1998 . United States Transportation Command from
Output: [[’The Daily Show’, ’cast member’, September 2005 to August 2008 .
’He’, ’end time’, ’1998’], [’The Daily Output: [[’United States Transportation
Show’, ’cast member’, ’He’, ’start time’, Command’, ’director / manager’, ’He’,
’1996’]] ’end time’, ’August 2008’], [’UnitedStates Transportation Command’, ’director Input: For the start of the 1982 season
/ manager’, ’He’, ’start time’, , the Minnesota Vikings moved from
’September 2005’]] Metropolitan Stadium to the Hubert H .
Input: He graduated from Pennsylvania Humphrey Metrodome .
State University in State College , PA Output: [[’Metrodome’, ’occupant’,
in 1969 , and earned a J . D . ’Minnesota Vikings’, ’start time’,
Output: [[’He’, ’educated at’, ’1982’], [’Minnesota Vikings’, ’home
’Pennsylvania State University’, ’end venue’, ’Metrodome’, ’start time’,
time’, ’1969’]] ’1982’]]
Input: She married in 1936 , and took up Input: It was established as the official
her first post in Liverpool University legislature of Kampuchea on January 5 ,
, where she studied for the rest of her 1976 , consisting of 250 members .
working life . Output: [[’Kampuchea’, ’legislative
Output: [[’She’, ’employer’, ’Liverpool body’, ’It’, ’start time’, ’January 5 ,
University’, ’start time’, ’1936’]] 1976’]]
Input: Air Union was merged with four Input: They were designed by R . J .
other French airlines to become Air France Billinton and built at Brighton works from
on 7 October 1933 . 1895 to 1897 .
Output: [[’Air Union’, ’followed by’, Output: [[’They’, ’manufacturer’,
’Air France’, ’point in time’, ’7 October ’Brighton works’, ’start time’, ’1895’]]
1933’]] Input: Dee Palmer ( formerly David Palmer
Input: The Duchy of Magdeburg ( German : ; born 2 July 1937 ) is an English
Herzogtum Magdeburg ) was a province of composer , arranger , and keyboardist
Brandenburg - Prussia from 1680 to 1701 best known for having been a member of
and a province of the German Kingdom of the progressive rock group Jethro Tull
Prussia from 1701 to 1807 . from 1977 to 1980 .
Output: [[’Brandenburg - Prussia’, Output: [[’David Palmer’, ’member of’,
’followed by’, ’Prussia’, ’point in ’Jethro Tull’, ’end time’, ’1980’],
time’, ’1701’], [’Magdeburg’, ’located [’David Palmer’, ’member of’, ’Jethro
in the administrative territorial Tull’, ’start time’, ’1977’]]
entity’, ’Prussia’, ’start time’, Input: In 1931 , he joined Joseph
’1701’], [’Magdeburg’, ’located in the Lyons and several other members in
administrative territorial entity’, leaving the Labor Party and joining with
’Brandenburg - Prussia’, ’start time’, the Nationalists to create the United
’1680’]] Australia Party .
Input: In 1998 , the studio moved from Output: [[’Joseph Lyons’, ’member of
Studio City , California to Burbank in political party’, ’United Australia
celebration of a new facility , and was Party’, ’start time’, ’1931’], [’Joseph
renamed Nickelodeon Animation Studio . Lyons’, ’member of political party’,
Output: [[’Nickelodeon Animation Studio’, ’Labor Party’, ’end time’, ’1931’]]
’headquarters location’, ’Studio City Input: He was in the United States Army
, California’, ’end time’, ’1998’], during World War II , from 1943 to 1946 .
[’Nickelodeon Animation Studio’, Output: [[’He’, ’military branch’,
’headquarters location’, ’Burbank’, ’United States Army’, ’end time’, ’1946’],
’start time’, ’1998’]] [’He’, ’military branch’, ’United States
Input: Kimera Walusimbi was Kabaka of the Army’, ’start time’, ’1943’]]
Kingdom of Buganda between 1374 and 1404 Input: She was named after the title
. character of the 1866 opera Mignon ,
Output: [[’Kimera’, ’noble title’, written by her godfather , French composer
’Kabaka’, ’start time’, ’1374’], Ambroise Thomas .
[’Kimera’, ’noble title’, ’Kabaka’, Output: [[’Ambroise Thomas’, ’notable
’end time’, ’1404’]] work’, ’Mignon’, ’publication date’,’1866’]] Books’, ’parent organization’, ’Penguin
Input: It was best known as the home of Group’, ’start time’, ’1986’]]
the Detroit Red Wings hockey team of the Input: In 1990 , following the Iraqi
National Hockey League from its opening invasion of Kuwait , Saudi Arabia
until 1979 . participated in the Gulf War to expel
Output: [[’Detroit Red Wings’, ’home Iraqi forces from the country .
venue’, ’It’, ’end time’, ’1979’], [’It’, Output: [[’Gulf War’, ’participant’,
’occupant’, ’Detroit Red Wings’, ’end ’Saudi Arabia’, ’point in time’, ’1990’]]
time’, ’1979’]] Input: On November 18 , 1928 the first
Input: He became a solicitor in 1900 and Mickey Mouse cartoon released to the
a barrister in 1913 , being a member of public , Steamboat Willie , debuted at
both King ’ s Inns , Dublin , and Gray the Colony .
˘2019 s Inn , London .
Output: [[’Mickey Mouse’, ’present in
Output: [[’He’, ’occupation’,
work’, ’Steamboat Willie’, ’point in
’solicitor’, ’start time’, ’1900’],
time’, ’November 18 , 1928’]]
[’He’, ’occupation’, ’barrister’, ’start
Input: The Third Republic of South Korea
time’, ’1913’]]
was replaced in 1972 by the Fourth
Input: The Portuguese Air Force ( PoAF )
Republic of South Korea under the Third
operated 50 LTV A - 7 Corsair II aircraft
Republic of South Korea ’ s president Park
in the anti - ship , air interdiction and
Chung - hee .
air defense roles between 1981 and 1999 .
Output: [[’Fourth Republic of South
Output: [[’LTV A - 7 Corsair II’,
Korea’, ’replaces’, ’Third Republic of
’operator’, ’Portuguese Air Force’,
South Korea’, ’point in time’, ’1972’]]
’start time’, ’1981’], [’LTV A - 7 Corsair
Input: He returned to favor in 1942 and
II’, ’operator’, ’Portuguese Air Force’,
was recalled to Moscow .
’end time’, ’1999’]]
Output: [[’He’, ’residence’, ’Moscow’,
Input: Brickleberry is an American
’start time’, ’1942’]]
animated comedy that premiered on
Input: On 20 July 2012 , the Constable
September 25 , 2012 on Comedy Central
welcomed the Olympic Torch to London at
.
the Tower one week in advance of the
Output: [[’Brickleberry’, ’original
London 2012 Summer Olympic Games , as
broadcaster’, ’Comedy Central’, ’start
part of the Olympic torch relay .
time’, ’September 25 , 2012’]]
Output: [[’London’, ’significant event’,
Input: Shellen joined Google in 2003 when
’London 2012 Summer Olympic Games’,
the company acquired Pyra Labs , which
’point in time’, ’20 July 2012’]]
developed the Blogger blogging platform
Input: and Max Verstappen , who in 2015
.
became the youngest driver in Formula One
Output: [[’Pyra Labs’, ’owned by’,
history at just 17 years old .
’Google’, ’start time’, ’2003’]]
Output: [[’Max Verstappen’, ’sport’,
Input: Volkswagen purchased the Bugatti
’Formula One’, ’start time’, ’2015’]]
trademark in June 1998 and incorporated
Bugatti Automobiles S . A . S . Input: She was married to internationally
Output: [[’Bugatti’, ’owned by’, famous writer Jorge Amado from 1945 until
’Volkswagen’, ’start time’, ’June his death in 2001 .
1998’]] Output: [[’She’, ’spouse’, ’Jorge Amado’,
Input: In 1986 , the company was acquired ’start time’, ’1945’], [’She’, ’spouse’,
by Penguin Group and split into two ’Jorge Amado’, ’end time’, ’2001’]]
imprints : Dutton and Dutton Children ’ Input: It was first listed on the
s Books . London Stock Exchange in 2005 is now a
Output: [[’Dutton’, ’parent constituent of the FTSE 100 Index .
organization’, ’Penguin Group’, ’start Output: [[’It’, ’stock exchange’, ’London
time’, ’1986’], [’Dutton Children ’ s Stock Exchange’, ’start time’, ’2005’]]Input: The Valencia Street Circuit DECOMPOSITION: From 1680 to 1701: The
( Valencian : Circuit Urb˘00e0 de Duchy of Magdeburg was a province of
Val˘00e8ncia , Spanish : Circuito Urbano Brandenburg-Prussia. From 1701 to 1807:
de Valencia ) was a street circuit in The Duchy of Magdeburg was a province of
Valencia , Spain which hosted the Formula the German Kingdom of Prussia. </s>
One European Grand Prix for five years ( feedback: Precision: very good. Recall:
2008 ˘2013 2012 ) . very good.
Output: [[’Valencia Street Circuit’, EXAMPLE2:
’used by’, ’Formula One’, ’start time’, TEXT: From its foundation in 1921 until
’2008’], [’Valencia Street Circuit’, moving to Ashton Gate in 2014 the Memorial
’used by’, ’Formula One’, ’end time’, Ground was the home to Bristol Rugby .
’2012’]]
TIME:[’1921’, ’2014’]
Nowpleaseextractthequintuplesfromthefol-
DECOMPOSITION: 1921: The Memorial Ground
lowinginput:
was founded and became the home to Bristol
Input: Rugby. 2014: The Memorial Ground moved to
Ashton Gate. </s>
E.2 DecompositionPromptwithFeedback
feedback: Precision: Not good. It was
First, I will give you a TEXT, and not the Memorial Ground that was founded
secondly, I will give you the TIME in 1921 but Bristol Rugby. Besides, it
contained in the TEXT. You need to was not the Memorial Ground but Bristol
sort out the events that occurred at Rugby that moved to Ashton Gate. Recall:
each time I provided based on the TEXT. Not good. Two events are not recalled:
The DECOMPOSITION format is required to In 1921 Bristol Rugby was founded and in
contain a time point and corresponding 2014 Bristol Rugby moved to Ashton Gate.
events in each sentence. Each sentence A correct and complete decomposition is
contains the complete elements of the as follows: 1921: Bristol Rugby was
event, such as subject, predicate, and founded and the Memorial Ground the home
object. DECOMPOSITION ends with </s>. to Bristol Rugby. 2014: Bristol Rugby
There are two evaluation criteria for moved to Ashton Gate.
this task: Precision and Recall. The EXAMPLE3:
definitions of Precision and Recall under TEXT: Alexander Mackenzie , PC ( January
this task are as follows: 28 , 1822 ˘2013 April 17 , 1892 ) , was a
Precision: Whether the time at which building contractor and newspaper editor
each event occurred is given correctly. , and was the second Prime Minister of
Recall: Whether all events that Canada , from November 7 , 1873 to October
occurred at each time are given 8 , 1878 .
completely. TIME: [’January 28, 1822’, ’November 7,
I will show you some examples. At 1873’, ’October 8, 1878’, ’April 17,
the same time, i will give you human 1892’]
feedback for decomposition results in DECOMPOSITION: January 28, 1822:
these examples. I need you to learn Alexander Mackenzie, PC, was born.
from these examples and feedback how to November 7, 1873: Alexander Mackenzie
decompose TEXT correctly and completely: started serving as the second Prime
EXAMPLE1: Minister of Canada. October 8, 1878:
TEXT: The Duchy of Magdeburg ( German : Alexander Mackenzie ended his term as the
Herzogtum Magdeburg ) was a province of second Prime Minister of Canada. April
Brandenburg - Prussia from 1680 to 1701 17, 1892: Alexander Mackenzie passed
and a province of the German Kingdom of away. </s>
Prussia from 1701 to 1807 . Feedback: Precision: very good. Recall:
TIME:[’from 1680 to 1701’, ’from 1701 to very good.
1807’] EXAMPLE4:TEXT: Pope Benedict XV ( Latin : EXAMPLE6:
Benedictus XV ; Italian : Benedetto XV TEXT: It is one of the most famous old
) born Giacomo Paolo Giovanni Battista districts in Poland today and was the
della Chiesa , ( 21 November 1854 ˘2013 22 center of Poland ’ s political life
January 1922 ) , was Pope from 3 September from 1038 until King Sigismund III Vasa
1914 until his death in 1922 . relocated his court to Warsaw in 1596 .
TIME: [’21 November 1854’, ’22 January TIME:[’1038’, ’1596’]
1922’, ’3 September 1914’, ’1922’] DECOMPOSITION: 1038: It was the center
DECOMPOSITION: 21 November 1854: Pope of Poland’s political life. 1596: King
Benedict XV, born Giacomo Paolo Giovanni Sigismund III Vasa relocated his court to
Battista della Chiesa, was born. 22 Warsaw. </s>
January 1922: Pope Benedict XV passed feedback: Precision: very good. Recall:
away. 3 September 1914: Pope Benedict not good. In 1596 it was no longer
XV started serving as Pope. 1922: Pope the center of Poland ’ s political life.
Benedict XV ended serving as Pope. </s> A correct and complete decomposition is
feedback: Precision: very good. Recall: as follows: 1038: It was the center
not good. In 22 January 1922, Pope of Poland’s political life. 1596: It
Benedict XV passed away and was no longer was no longer the center of Poland ’ s
Pope. In 1922: Pope Benedict XV passed political life since King Sigismund III
away and ended serving as Pope. A correct Vasa relocated his court to Warsaw.
and complete decomposition is as follows: EXAMPLE7:
21 November 1854: Pope Benedict XV, born
TEXT: He played for Gloucestershire
Giacomo Paolo Giovanni Battista della
between 1920 and 1921 .
Chiesa, was born. 22 January 1922: Pope
TIME:[’1920’, ’1921’]
Benedict XV passed away and ended serving
DECOMPOSITION: 1920: He played for
as Pope. 3 September 1914: Pope Benedict
Gloucestershire. 1921: He played for
XV started serving as Pope. 1922: Pope
Gloucestershire. </s>
Benedict XV passed away and ended serving
feedback: Precision: not good. This
as Pope.
decomposition does not convey the
EXAMPLE5:
meaning that he started playing for
TEXT:He received : Serbian NIN Prize 1978
Gloucestershire in 1920 and ended
and Andri˘0107 Award 1987 , Swedish PEN
playing for Gloucestershire in 1921.
Tucholsky Award ( 1993 ) , German Herder
Recall: very good. A correct
Prize ( 1995 ) , Montenegrin Njego˘0161
and complete decomposition is as
Award ( 2009 ) and 13th July award ( 2004
follows: 1920: He started playing for
) , Slovenian Vilenica Award ( 2003 ) ,
Gloucestershire. 1921: He ended playing
Croatian Vladimir Nazor Award ( 2008 )
for Gloucestershire.
etc .
Now process the following TEXT and
TIME: [’1978’, ’1987’, ’1993’, ’1995’,
follow my examples above for the
’2003’, ’2004’, ’2008’, ’2009’]
DECOMPOSITION format:
DECOMPOSITION: 1978: He received the
Serbian NIN Prize. 1987: He received F RelationsDeletedfromHyperRED
the Andrić Award. 1993: He received the
Swedish PEN Tucholsky Award. 1995: He Thereare57typesofrelationshipswithtimequali-
received the German Herder Prize. 2003: fierintheHyperREDdataset. Wehavedeletedthe
He received the Slovenian Vilenica Award. followingrelations:
2004: He received the 13th July award. Distantsupervisionoftheserelationsresults
2008: He received the Croatian Vladimir inexcessivenoise:
Nazor Award. 2009: He received the Shares_Border_With,Country_Of_Citizenship,
Montenegrin Njegoš Award. </s> Instance_Of,League,Subclass_Of,Part_Of,Part-
feedback: Precision: very good. Recall: ner_In_Business_Or_Sport.
very good. Easilyconfusedrelations:Head_Of_State
(Head_Of_Government), Location (Lo-
cated_In_The_Administrative_Territorial_Entity),
Part_Of (Member_of), Participating_team
(Participant),Voice_Actor(Performer).
In ComplexTRED, we delete 5 relations from
HyperRED-Temporal:
Distantsupervisionoftheserelationsresults
inexcessivenoise:
Followed_By (Replaces), Award_Received
(Winner),Head_Of_Government(Positionheld),
Easilyconfusedrelations:
Connecting_Line,Stock_Exchange.