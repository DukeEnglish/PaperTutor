Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers
TuoZhang∗ JinyueYuan∗and SalmanAvestimehr
UniversityofSouthernCalifornia
{tuozhang,jinyueyu,avestime}@usc.edu
Abstract initially dependent on manual prompt creation,
recent developments in automated prompt engi-
Numerous recent works aim to enhance the
neering, such as APE (Zhou et al., 2022) and
efficacy of Large Language Models (LLMs)
APO(Pryzantetal.,2023),leverageLLMsfordy- throughstrategicprompting. Inparticular,the
OptimizationbyPROmpting(OPRO)approach namicpromptgenerationandrefinement. Thisiter-
providesstate-of-the-artperformancebylever- ativeprocessenhancesNLPtaskaccuracythrough
agingLLMsasoptimizerswheretheoptimiza- feedbackandselection. Buildingonthis,thepropo-
tiontaskistofindinstructionsthatmaximize sition of LLMs as optimizers (Yang et al., 2023;
the task accuracy (Yang et al., 2023). In this
Guoetal.,2023)presentsthecurrentstate-of-the-
paper,werevisitOPROforautomatedprompt-
art in automated prompt design, framing prompt
ingwithrelativelysmall-scaleLLMs,suchas
refinementasanoptimizationchallenge. Thisap-
LLaMa-2 family and Mistral 7B. Our inves-
proachiterativelyrefinespromptstomaximizetask
tigation reveals that OPRO shows limited ef-
fectivenessinsmall-scaleLLMs,withlimited accuracy, ceasing when performance plateaus or
inferencecapabilitiesconstrainingoptimization iterationlimitsaremet.
ability. Wesuggestfutureautomaticprompting ThemotivationforOPROisbasedontheLLMs’
engineering to consider both model capabili-
self-optimization ability. However, our empiri-
tiesandcomputationalcosts. Additionally,for
cal results reveal that smaller-scale LLMs like
small-scale LLMs, we recommend direct in-
LLaMa-2 (Touvron et al., 2023) do not have suf-
structions that clearly outline objectives and
ficientabilitytosupporttheself-optimization. We
methodologiesasrobustpromptbaselines,en-
suringefficientandeffectivepromptengineer- demonstratethatsuchoptimizationstrategiesoffer
inginongoingresearch. marginalbenefitsforsmaller-scaleLLMs,demand-
ingconsiderablecomputationalresourcesforslight
1 Introduction
performance gains, particularly when contrasted
Advancements in large language models (LLMs) with zero-shot CoT prompts. We summarize our
have catalyzed a shift towards prompting-based contributionsasfollows:
learning,distinguishingmodelswithcapacitiesex- • Wedemonstratethatthelimitedinferenceabil-
ceeding100billionparametersfortheirfew-shot ities of small-scale LLMs, such as LLaMa-2
learningabilitieswithoutextensiveretraining(Ope- family and Mistral 7B, restrict their self-
nAI,2020). In-contextlearning,facilitatedthrough optimizationefficiency,renderingOPROinef-
the strategic use of prompts, enables these mod- fectiveforthesemodels. (Section2,4).
elstogeneratetask-specificresponses,markinga • Our findings reveal OPRO’s substantial re-
departurefromtraditionalpre-trainandfine-tune lianceonmanualpromptdesigninsmall-scale
approaches(Liuetal.,2021;Wanetal.,2023). LLMs,suggestingthatitsautomationadvan-
The Chain of Thought (CoT) technique signifi- tageisminimalcomparedtotraditionalman-
cantlyadvancesLLMs’problem-solvingcapabili- ualpromptingefforts. (Section4)
tiesbyincorporatingintermediatereasoningsteps,
• Based on empirical evidence and analysis,
facilitating effective zero-shot reasoning and per-
we recommend future prompt engineering
formanceenhancementswithpromptslike"Let’s
efforts to account for the inference limita-
thinkstepbystep"(Weietal.,2022;Wangetal.,
tionsofsmall-scaleLLMsandconsidertradi-
2022;Yaoetal.,2023;Kojimaetal.,2022). While
tionalCoTpromptsaseffective,adaptive,and
∗Thefirsttwoauthorscontributedequallytothiswork. resource-efficientbaselines. (Section3.2,4)
4202
yaM
61
]LC.sc[
1v67201.5042:viXra2 MotivationalStudy: CanLLaMa13B
SolveLinearRegression?
OPRO (Yang et al., 2023) and EvoPrompt (Guo
etal.,2023)frameworkhavedemonstratedthesig-
nificantpotential ofLLMsinautomating prompt
design. However, the effectiveness appears to be
contingentupontheinherentoptimizationcapabili-
tiesoftheLLMsthemselves. Notably,evaluations
withintheOPROframeworkhavepredominantly
focusedonlarge-scalemodels,suchasGPT-4and
text-bison, leavingtheperformanceofsmaller-
scaleLLMsunexplored. Thisobservationprompts
a critical inquiry: Can small-scale LLMs also
serveasoptimizers?
Todelveintothisquestion,weattempttorepro-
ducethelinearregressionoptimizationexperiment
withLLaMa-2-13B,themotivatingexampleshown
inOPRO(Yangetal.,2023). Weadoptthesame
experimentsettingasinOPRO.Specifically,ourex- Figure1: Anexampleofthemeta-promptanditsoutput
forlinearregression. Forsimplicity,weonlyshowtwo
perimentaimstooptimizetwoparameters,wandb,
solution-scorepairsintheexample. Theorangetextare
inaone-dimensionallinearregressionmodelwith
meta-instructions;thebluetextaresolution-scorepairs;
aninterceptb,using50datapointsgeneratedfrom
thegreentextareoutputbyLLaMa-2-13B.
predefinedvalueofw andb withstandard
true true
Gaussiannoiseϵ. Startingfromfiveinitialrandom
pairsof(w,b),weengagedLLaMa-2-13Bthrough
3.1 Experimentsetup
ameta-promptstrategysimilartoOPRO,directing
the model to propose pairs that minimize the ob-
DatasetsandModels. Weselectedmodelsfrom
jectivefunction,basedonhistoricaldataofthetop
two distinct categories: small-scale and large-
20performingpairs. Arepresentativemeta-prompt
scale. Within the small-scale category, we fo-
anditsoutputisshowninFigure1.
cusedontheLlamafamily,evaluatingLLaMa-2-7b,
Thenegativeresult,particularlyitsself-reported LLaMa-2-13b,andLLaMa-2-70b. Wealsoconduct
difficultieswithgradientdescent,underscoresapo- experimentswithMistral 7B(Jiangetal.,2023)
tential shortfall in optimization capability within totestthegeneralizabilityofthefindings. Forin-
smaller-scaleLLMsforsolvingmathematicalprob- sightsintolarge-scaleLLMperformance,wecon-
lems. Thisobservationimpliesthattheefficacyof ductedparallelexperimentsonGemini-Pro(Gem-
self-evaluatingprompts,whichrelyheavilyonthe iniTeam,2023). FollowingtheOPROpaper,allex-
LLM’s optimization skills, diminishes in smaller perimentsinthispaperareconductedwithGSM8K,
models. Consequently,ourfurtherresearchfocuses abenchmarkofgradeschoolmathwordproblems,
ondissectingthesechallengesthroughtargetedex- with7,373trainingsamplesand1,319testsamples.
periments and analyses, aiming to elucidate and
Baselines and Implementations. We focus
potentiallymitigatetheconstraintsfacedbysmall-
on three well-adapted prompting designs in the
scaleLLMsinoptimizationtasks.
experiments, including Zero-shot-CoT (Kojima
etal.,2022),Few-shot-CoT(Weietal.,2022),and
3 Evaluation OPRO (Yangetal.,2023). Werigorouslyfollow
the original OPRO paper (Yang et al., 2023) for
Inthissection,weaimtoreplicatetheOPROframe- theimplementationdetails. Specifically, weonly
workwithsmall-scaleLLMstoassessitsefficacy usethesamemodelarchitecturesfortheoptimizer
in identifying optimal instruction words. The in- and scorer in the main experiment, but these are
structionpositionisaddedtothebeginningofthe two independent LLMs. More details about the
LLMoutput. implementationsareshownintheAppendix.Table1:EvaluationperformanceonGSM8KusingvariouspromptingmethodsacrossmodelsincludingLLaMa-2-7b,
Mistral 7B,LLaMa-2-13b,LLaMa-2-70b,andGemini-Pro. TheInstructionWordscolumndetailsthespecific
instructionsusedtoachievethereportedtestaccuracy.
Model Method Accuracy InstructionWords
LLaMa-2-7b Zero-shot-CoT 24.26% Let’sthinkstepbystep
Few-shot-CoT 24.87% twoexemplars+Let’sthinkstepbystep
OPRO 29.81% Thecorrelationispresent
Mistral 7B Zero-shot-CoT 37.52% Let’sthinkstepbystep
Few-shot-CoT 38.13% twoexemplars+Let’sthinkstepbystep
Usingtheprovidedinformation,wecanfindthesolu-
OPRO 32.13%
tion
LLaMa-2-13b Zero-shot-CoT 32.75% Let’sthinkstepbystep
Few-shot-CoT 37.15% twoexemplars+Let’sthinkstepbystep
OPRO 31.24% Let’sthinkabout
LLaMa-2-70b Zero-shot-CoT 39.35% Let’sthinkstepbystep
Few-shot-CoT 48.67% twoexemplars+Let’sthinkstepbystep
OPRO 27.98% Thecorrelationispresent
Gemini-Pro Zero-shot-CoT 71.29% Let’sthinkstepbystep
Few-shot-CoT 69.67% twoexemplars+Let’sthinkstepbystep
To attain the utmost precision in solving diverse
gradeschoolmathematicalproblems,meticulously
OPRO 76.92%
adheretothiscomprehensiveandrigorouslydevel-
opedmethodology:
3.2 MainResults AnalysisofGeneratedInstructionWords. A
closerexaminationofOPRO’sinstructiongenera-
Weevaluatedvariouspromptingstrategiesacross
tionrevealssignificantinsightsintoitsoptimization
different LLM scales, detailed in Table 1, main-
efficacy. In LLaMa-2-13B, the instructions gen-
tainingconsistentmodelarchitecturesforbothop-
erated by OPRO resemble the traditional "Let’s
timizerandscorer. TheGemini-Promodeldemon-
think step by step" prompt, showcasing some op-
strates OPRO’s effectiveness, notably surpassing
timization capacity but failing to yield the opti-
CoTbaselines,inlinewithpreviousfindings(Yang
mal solution. This scenario underscores the in-
etal.,2023). ThisunderscoresOPRO’sadvantage
adequateself-optimizationskillsofsmaller-scale
with large-scale LLMs in optimizing task perfor-
LLMs, contrasting sharply with OPRO’s perfor-
mance.
mance in Gemini-Pro. For Gemini-Pro, OPRO
Conversely, OPRO’s results with Mistral 7B,
craftsinstructionsthataptlyinclude"gradeschool
LLaMa-2-13B,andLLaMa-2-70BfallshortofZero-
mathematical problems", indicating superior op-
shot-CoTandFew-shot-CoTbenchmarks,reveal-
timization and understanding that aligns closely
ingthesemodels’limitationsinoptimizationand
withthetask. Thedisparityinoutputbetweenthe
theirinabilitytooutperformbasic"Let’sthinkstep
smaller and larger-scale models corroborates the
bystep"prompts. Notably,thehighestperformance
preliminaryhypothesis: OPRO’soptimizationap-
isobservedwithFew-shot-CoT,suggestingthatfor
proach falls short in smaller-scale LLMs due to
small-scale LLMs, direct instructions providing
theirlimitedself-optimizationabilities.
clearguidanceonboththeobjectivesandmethod-
ologiesaremosteffective. Thisalignswithearlier
4 LimitationsofSelf-Optimization
discussions in Section 2, highlighting the insuffi-
PromptinginSmall-ScaleLLMs
cientself-optimizationcapabilitiesofsmaller-scale
LLMsingeneratingoptimalinstructionwords. The Small-scale LLMs could not support self-
results with Mistral 7B validate our argument optimization. Ouranalysis,presentedinTable1,
amongdifferentmodelarchitectures. assesseshowsmall-scaleLLMsfarewhenserving(a) Gemini-Pro optimizer (scorer: (b) LLaMa-2-13b optimizer (scorer: (c) LLaMa-2-13b optimizer (scorer:
Gemini-Pro) Gemini-Pro) LLaMa-2-13b)
Figure2: PromptoptimizationcurveonGSM8KusingGemini-ProandLLaMa-2-13b.
dualrolesinoptimizationandscoring. Further,Fig- asthetraditionalZero-shot-CoTapproaches. This
ure2illustratesthepromptoptimizationtrajectory relianceisechoedinpreviousresearch(Zhouetal.,
for LLaMa-2-13b and Gemini-Pro. OPRO’s effi- 2023), which found that manual prompting typi-
cacy in large-scale LLMs like Gemini-Pro (Fig- callysurpassesautomatedapproaches,aconclusion
ure 2a), consistent with previous studies (Yang consistentwithourobservationsinTable1.
etal.,2023). Notably,transitioningthescorerfrom
Table2: OPROevaluationperformancewithdifferent
LLaMa-2-13b to Gemini-Pro, while maintaining
metainstructionsusingLLaMa-2-13basoptimizer. The
LLaMa-2-13b as the optimizer, yields a 5% accu-
detailedtextsareshowninTable4inAppendix.
racyincrease(Figures2cand2b). Thishighlights
LLaMa-2-13b’sinadequacyasascorertoformulate
MetaInstruction Accuracy InstructionWords
effectiveoptimizationtargets,therebyconstraining
Congratulations!
Text1 17.59%
optimalsolutiondiscovery. You’reamathgenius!
This finding is in line with recommendations Now,let’stryanother
Text2 10.39%
problem:
fromexistingliterature(Hsiehetal.,2023),where
leveraging outputs from larger LLMs to enhance Text3 22.82% Thepreciseansweris
smallermodelsreflectsourexperimentalobserva- Text4 31.24% Let’sthinkabout
tions. Furthermore,recentliteratureindicatesthat
without additional inputs, LLMs struggle to self-
Table 3: Approximate input and output tokens with
improve (Huang et al., 2023). Interestingly, up-
Gemini Pro until optimal instruction words was
grading the scorer model only minimally affects
reached,andapproximatecomputationtimeinhours.
performance,implyingtheoptimizermaynotfully
leverage the advanced capabilities of a superior
Zero-shot-CoT Few-shot-CoT OPRO
scorer in OPRO’s context, leading to suboptimal
Input 6 130 96,289
promptgeneration. Asaresult,duetothelimited
Output 0 0 170,448
inferenceability,small-scaleLLMscouldnotsup- Time(hrs) 4 5 21
portself-optimizationforpromptingparadigms.
Human-CraftedElementsandTheirImpacts. Analysis of System Efficiency. Recent auto-
OPROaimstoautomateinstructionworddiscov- maticpromptworks(Fernandoetal.,2023;Yang
ery,minimizinghumaninterventionthroughLLM et al., 2023; Ma et al., 2024) have largely over-
capabilities. Yet,ourfindingsindicatesignificant lookedsystemefficiencyforsearchinginstructions.
variability in performance tied to manually de- In Table 3, we examine the efficiency of using
signedmeta-instructionswithinOPRO,especially Gemini Pro API across three methodologies by
in small-scale LLMs. We evaluated four distinct comparing input and output tokens and compu-
meta-instruction texts as shown in Table 4 in the tational time required to achieve the accuracies
AppendixwithLLaMa-2-13b,withresultsdetailed listed in Table 1. The token counts are based on
inTable2. Hugevarianceonaccuracyunderscores a word-based tokenization approach. OPRO in-
the critical influence of human-crafted elements cursanotablyhighertokencount,attributedtothe
on OPRO performance. Despite OPRO’s goal of scorer’s evaluation process in each meta-prompt
streamlining prompt optimization, it remains re- generationcycle. Additionally,OPRO’scomputa-
liantonhuman-craftedmeta-instructions,thesame tionaltimefarexceedsthatofalternativemethods.Theseresultssuggestthattheefficiencytrade-offs Jie Huang, Xinyun Chen, Swaroop Mishra,
associatedwithOPRO,givenitsextensivecompu- Huaixiu Steven Zheng, Adams Wei Yu, Xiny-
ing Song, and Denny Zhou. 2023. Large
tationaldemands,maynotalignwiththemarginal
language models cannot self-correct reasoning
performanceenhancementsitoffers.
yet. arXiv:2310.01798.
5 Conclusion AlbertQiaochuJiang,AlexandreSablayrolles,Arthur
Mensch, Chris Bamford, Devendra Singh Chap-
Withempiricalresults,wedemonstratethatsmall-
lot, Diego de Las Casas, Florian Bressand, Gi-
scaleLLMsarelimitedinself-optimizationcapac- annaLengyel, GuillaumeLample, LucileSaulnier,
ity,whichcausesOPROisnoteffectiveforsmall- L’elioRenardLavaud,Marie-AnneLachaux,Pierre
Stock,TevenLeScao,ThibautLavril,ThomasWang,
scaleLLMs. Inaddition,ourfindingsunderscore
TimothéeLacroix,andWilliamElSayed.2023. Mis-
OPRO’s dependency on scorer performance and
tral7b. ArXiv,abs/2310.06825.
manually designed prompts, despite the effort to
automatetheprocess. Wesuggestthefutureauto- TakeshiKojima,ShixiangShaneGu,MachelReid,Yu-
takaMatsuo,andYusukeIwasawa.2022. Largelan-
maticpromptingengineeringconsiderbothmodel
guagemodelsarezero-shotreasoners. Advancesin
capabilitiesandsystemefficiencies.
NeuralInformationProcessingSystems,35:22199–
Limitation and Future Study. Our study’s 22213.
scopewaslimitedbycomputationalresources,ex-
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,
cludingotherself-optimizationstrategieslikeEvo-
Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt
Prompt and APO due to their extensive prompt
understands,too. ArXiv,abs/2103.10385.
generation time. Our future research will extend
to enhancing the interpretability and depth of er- Ruotian Ma, Xiaolei Wang, Xin Zhou, Jian Li, Nan
Du,TaoGui,QiZhang,andXuanjingHuang.2024.
roranalysis,alternativeoptimizationmetrics,bias
Arelargelanguagemodelsgoodpromptoptimizers?
considerations,orhyperparametertuningimpacts
ArXiv,abs/2402.02101.
basedonourcurrentfindings.
OpenAI.2020. Languagemodelsarefew-shotlearners.
6 Acknowledgement NeurIPS.
Wethankthereviewersfortheirhelpfulcomments. Adam Paszke, Sam Gross, Francisco Massa, Adam
This work is in part supported by research gifts Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
from the USC Amazon Center for Secure and
Antiga, Alban Desmasion, Andreas Kopf, Edward
TrustedAIandIntel.
Yang,ZacharyDeVito,MartinRaison,AlykhanTe-
jani,SasankChilamkurthy,BenoitSteiner,LuFang,
JunjieBai,andSoumithChintala.2019. Pytorch: An
References imperativestyle,highperformancedeeplearningli-
brary. InAdvancesinNeuralInformationProcessing
Chrisantha Fernando, Dylan S. Banarse, Henryk
Systems32,pages8024–8035.
Michalewski, Simon Osindero, and Tim Rock-
täschel. 2023. Promptbreeder: Self-referential
Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-
self-improvement via prompt evolution. ArXiv,
guang Zhu, and Michael Zeng. 2023. Automatic
abs/2309.16797.
prompt optimization with "gradient descent" and
beamsearch. InConferenceonEmpiricalMethods
Google Gemini Team. 2023. Gemini: A family
inNaturalLanguageProcessing.
of highly capable multimodal models. ArXiv,
abs/2312.11805.
Hugo Touvron, Louis Martin, Kevin R. Stone, Peter
QingyanGuo,RuiWang,JunliangGuo,BeiLi,Kaitao Albert,etal.2023. Llama2: Openfoundationand
Song,XuTan,GuoqingLiu,JiangBian,YujiuYang, fine-tunedchatmodels. ArXiv,abs/2307.09288.
TsinghuaUniversity,andMicrosoftResearch.2023.
Connectinglargelanguagemodelswithevolutionary Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam,
algorithmsyieldspowerfulpromptoptimizers. ArXiv, Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan,
abs/2309.08532. YiZhu, QuanluZhang, MosharafChowdhury, and
MiZhang.2023. Efficientlargelanguagemodels: A
Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, survey. ArXiv,abs/2312.03863.
HootanNakhost,YasuhisaFujii,AlexanderJ.Ratner,
Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,
2023. Distillingstep-by-step! outperforminglarger Ed Huai hsin Chi, and Denny Zhou. 2022. Self-
languagemodelswithlesstrainingdataandsmaller consistencyimproveschainofthoughtreasoningin
modelsizes. ArXiv,abs/2305.02301. languagemodels. ArXiv,abs/2203.11171.JasonWei,XuezhiWang,DaleSchuurmans,Maarten
Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.
Chainofthoughtpromptingelicitsreasoninginlarge
languagemodels. arXiv:2201.11903.
ChengrunYang,XuezhiWang,YifengLu,HanxiaoLiu,
QuocVLe, DennyZhou, andXinyunChen.2023.
Largelanguagemodelsasoptimizers. arXivpreprint
arXiv:2309.03409.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,
Thomas L. Griffiths, Yuan Cao, and Karthik
Narasimhan. 2023. Tree of thoughts: Deliberate
problemsolvingwithlargelanguagemodels. ArXiv,
abs/2305.10601.
Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,
KeiranPaster,SilviuPitis,HarrisChan,andJimmy
Ba.2022. Largelanguagemodelsarehuman-level
promptengineers. ArXiv,abs/2211.01910.
YulinZhou,YirenZhao,IliaShumailov,RobertMullins,
andYarinGal.2023. Revisitingautomatedprompt-
ing: Are we actually doing better? In The 61st
AnnualMeetingoftheAssociationofComputational
Linguistics.A ExperimentalDetails
A.1 ModelsandTestEnvironment
WeimplementedtheexperimentsusingPyTorch(Paszkeetal.,2019),andconductedourexperimentson
twoNVIDIAA100GPUs. WetestedLLaMa-2-7b,LLaMa-2-13b,LLaMa-2-70b,andGemini-Prointhe
experiments. WedownloadedLLaMamodelsfromHuggingFaceandtestedthemlocallyonGPUs. For
Gemini-Pro,wereferencedthemodelviatheGeminiAPI.Thelinksforthemodelsareshownbelow.
LLaMa-2-7blink:
https://huggingface.co/meta-llama/Llama-2-7b-chat-hf
LLaMa-2-13blink:
https://huggingface.co/meta-llama/Llama-2-13b-chat-hf
LLaMa-2-70blink:
https://huggingface.co/meta-llama/Llama-2-70b-chat-hf
Gemini-Prolink:
https://ai.google.dev/models/gemini
A.2 PromptingMethods
1. Zero-shot-CoT:Thezero-shotinstruction"Let’sthinkstepbystep"(Kojimaetal.,2022)wouldbe
addedbeforeeachanswers.
2. Few-shot-CoT:Werandomlyselecttwosampleswithprocedures(Weietal.,2022)fromthetraining
setservingastheproblemdescriptionbeforethetestquestion.
3. OPRO:Werigorouslyfollowtheoriginalpaper(Yangetal.,2023)fortheimplementationdetails.
Our experiment utilized a meta-prompt, as illustrated in Figure 3, with the optimization process
spanning 100 iterations. In each iteration, we sampled 3.5% of GSM8K training examples as a
validationsetforscorerLLM.Weusedthemeta-prompttogenerateeightnewinstructionswiththe
optimizerLLM,updatingthetrajectorywiththeseinstructionsandtheirscoresineachinteraction.
Themeta-promptincludedthetop20instructionsandthreerandomtrainingexemplars.B Meta-PromptDesign
Figure 3 shows an example of the meta-prompt used in our implementation of OPRO. We rigorously
followed the original open source code provided by Google Deep Mind (https://github.com/google-
deepmind/opro). Thetwoexampleproblemsareexemplarsrandomlyselectedfromthetrainingsetof
GSM8Ktosupportthemeta-promptastheproblemdescription.
Figure3: Anexampleofthemeta-promptanditsoutputforGSM8Kdataset. Theorangetextaremeta-instructions;
thebluetextaresolution-scorepairs;thepurpletextareexemplarquestionsandoptimizationtargets;thegreentext
areoutputbyLLM.C Meta-InstructionDesign
Totesttherobustnessofmeta-prompt,weexperimentedwithseveralslightlyadjustedmeta-instructions.
Text4strictlyfollowsYangetal.’sdesign(Yangetal.,2023). Topreventhumaninventionontheprompt
design,weinputText4intoChatGPT(https://openai.com/chatgpt)togeneratethethreeotherprompts.
Precisionscoresarereplacedwiththescoresproducedbythescorerinthelaterevaluationstepsduring
thecomputationofOPRO.Table2showstheresultsofthedifferentmeta-instructionsonperformances.
Table4: MetainstructionsusedinOPRO
MetaInstruction Text
Createanewpieceoftextasaninstructionatthebeginningoftheanswertoenhance
theprecisioninsolvingdiversegradeschoolmathproblems. Wewanttheprecision
Text1 of the text to be higher as possible. Range of Precision is 0 to 100. For example,
Precision: 4<Text>Adime</Text>,Precision: 17<Text>Theanswerisafunction. It
is</Text>.
WriteanewtextforinstructionusebeforetheanswerintheQ&Apairtohelpsolving
the grade school math problems. We want to precision of the text to be as high as
Text2
possible,rangingfrom0to100. Forexample,Precision: 4<Text>Aquarter</Text>,
Precision: 25<Text>Nowfindtheanswer.</Text>.
Create a line of instruction, with precision 0 to 100. The text will be placed at the
startoftheanswer,toassistinsolvinggradeschoolmathematicalproblems. Some
Text3
example text and score pairs are: Precision: 29 <Text>The numeric answer to this
questionis:</Text>
Yourtaskistogeneratetheanswerstartingsentence<Start>. Belowaresomeprevious
startingsentenceswiththeirscores. Thescorerangesfrom0to100. Precision: 37
Text4
<Start>Thesolutiontothisinvolves</Start>,Precision: 39<Start>Thenumberasked
foris</Start>