On Partially Unitary Learning
Mikhail Gennadievich Belov∗
Lomonosov Moscow State University, Faculty of Mechanics and Mathematics,
GSP-1, Moscow, Vorob’evy Gory, 119991, Russia
Vladislav Gennadievich Malyshkin†
Ioffe Institute, Politekhnicheskaya 26, St Petersburg, 194021, Russia
(Dated: May, 14, 2024)
$Id: PartiallyUnitaryLearning.tex,v 1.268 2024/05/16 17:05:38 mal Exp $
The problem of an optimal mapping between Hilbert spaces IN of |ψ⟩ and OUT
of |ϕ⟩ based on a set of wavefunction measurements (within a phase) ψ → ϕ ,
l l
l = 1...M, is formulated as an optimization problem maximizing the total fidelity
(cid:80)M ω(l)|⟨ϕ |U|ψ ⟩|2 subject to probability preservation constraints on U (partial
l=1 l l
unitarity). Constructed operator U can be considered as a IN to OUT quantum
channel; it is a partially unitary rectangular matrix of the dimension dim(OUT)×
dim(IN) transforming operators as AOUT = UAINU†. An iteration algorithm finding
theglobalmaximumofthisoptimizationproblemisdevelopedandit’sapplicationtoa
number of problems is demonstrated. A software product implementing the algorithm
is available from the authors.
∗ mikhail.belov@tafs.pro
† malyshki@ton.ioffe.ru
4202
yaM
61
]GL.sc[
1v36201.5042:viXra2
I. INTRODUCTION
The progress in machine learning (ML) knowledge representation from linear regression
coefficients, perceptron weights[1], statistical learning[2], and logical approaches[3] to support
vector machines[4], rules and decision trees[5], fuzzy logic[6, 7], and deep learning[8] has
been the direction of ML development within the last four decades. Recently knowledge
representation in the form ofa unitary operatorstartedto attracta lotofattention[9–11]. The
problem of learning unitary matrices is also useful in other fields. For example in quantum
mechanics inverse problem[12, 13], quantum computing[14], market dynamics[15] and others.
The techniques used for unitary learning differ in unitary matrix representation, input
data, and quality criteria. A substantial number of existing works[16, 17] use Frobenius L2
norm of a difference between target and current matrix ∥U −V∥2. A limitation of this type of
approach is that it cannot determine operator U from wavefunction measurements — they are
measured without phase information. A better option is to use fidelity of the source/target
states |⟨ϕ|U |ψ⟩|2; this approach is used in [9, 18] and many others. An important advantage
of it is that multiplication of source |ψ⟩ or target |ϕ⟩ by a random phase does not change the
fidelity. A disadvantage — the operator U itself can be determined only within a phase.
Inourpreviouswork[19]theproblemofunitarylearningwasgeneralizedtopartiallyunitary
operators. This operator maps two Hilbert spaces of different dimensions (a unitary operator
maps a Hilbert space into itself). The problem is to maximize the fidelity of a mapping
between Hilbertspaces IN of|ψ⟩ andOUT of|ϕ⟩ basedon a setofwavefunction measurement
(within a phase) observations ψ → ϕ , l = 1...M, as an optimization problem maximizing
l l
the total fidelity (cid:80)M ω(l)|⟨ϕ |U |ψ ⟩|2 subject to probability preservation constraints on U
l=1 l l
(partial unitarity). This problem is reduced to a problem of maximization of a quadratic
form on U subject to quadratic form constraints. This is a variant of QCQP problem. This
problem is non-convex with local extremums and multiple saddle points. In this work an
iteration algorithm finding the global maximum of this optimization problem is developed
and it’s application to a number of problems is demonstrated. For software availability see
Appendix B; all references to a code in the paper correspond to this software.3
II. FORMULATION OF THE PROBLEM
Consider quantum mechanics inverse problem. It may be broadly described as a problem
of determining the internal structure (e.g. Hamiltonian) of a system from wavefunction
measurements. A number of other problems in statistics, machine learning, data analysis,
etc. can be converted to a problem of the following form: Let there are two Hilbert spaces of
the dimensions n and D, corresponding states are ψ(x) and ϕ(f) in some x- and f- bases x
k
and f of n ≥ D
j
n−1
(cid:88)
ψ(x) = α x (1a)
k k
k=0
D−1
(cid:88)
ϕ(f) = β f (1b)
j j
j=0
There is a scalar product operation ⟨·⟩ in each Hilbert space allowing to calculate scalar
product inside the basis ⟨ψ|ψ′⟩ = (cid:80)n−1 α∗⟨x |x ⟩α′ and ⟨ϕ|ϕ′⟩ = (cid:80)D−1 β∗⟨f |f ⟩β′,
k,q=0 k k q q j,i=0 j j i i
but not across the bases: the ⟨x |f ⟩ cannot be calculated! Assume we have l = 1...M
k j
wavefunction pairs (typically M ≫ n) as “observations”:
ψ (x) → ϕ (f) weight ω(l) (2)
l l
1 = ⟨ψ |ψ ⟩ = ⟨ϕ |ϕ ⟩ (3)
l l l l
these are M mapping from Hilbert space x to Hilbert space f. The inverse problem is to find
partially unitary operator U (a matrix u of the dimension D×n) acting from x to f
jk
n−1
(cid:88)
f = u x j = 0...D−1 (4)
j jk k
k=0
that maximizes the total probability (fidelity)
(cid:88)M (cid:12) (cid:12)2
F = ω(l)(cid:12)⟨ϕ |U |ψ ⟩(cid:12) −→ max (5)
(cid:12) l l (cid:12)
U
l=1
n−1
(cid:88)
⟨f |f ⟩ = u ⟨x |x ⟩u∗ j,j′ = 0...D−1 (6)
j j′ jk k k′ j′k′
k,k′=0
subject to scalar product preservation (6). The operator U is acting from Hilbert space x to
Hilbert space f, it can be viewed as a memoryless quantum channel. The functional (5) has
matrix element ⟨ϕ |U |ψ ⟩ absolute value squared, this allows to express the total probability
l l4
(fidelity) only with ⟨f |f ⟩, ⟨x |x ⟩ and fourth order terms ⟨f x |f x ⟩, without using
j j′ k k′ j k j′ k′
unavailable “cross-moments” ⟨x |f ⟩. The functional also has proper wavefunction phase
k j
invariance. A specific of quantum measurement is that a wavefunction cannot be measured
itself, only wavefunction squared (probability density) can be possibly obtained from a
measurement operation. The ψ2 is observable whereas the ψ is not. Measured wavefunction
pairs (2) should be considered as measured ψ2 and ϕ2 with the square root applied, obtained
result is possibly multiplied by an unknown random phase exp(iξ ). The optimization problem
l
(5)offindingoptimalquantumchannelU isinvariantwithrespecttorandomphasesintroduced
to measured wavefunctions (2). Without loss of generality we can consider Hilbert space
bases orthogonal:
δ = ⟨f |f ⟩ (7a)
jj′ j j′
δ = ⟨x |x ⟩ (7b)
kk′ k k′
if this is not the case — an orthogonalizing procedure can be applied, see Appendices A and
E of [20] or plain apply an orthogonalization of Gram–Schmidt type, see com/polytechnik/
kgo/DemoRecoverMapping.java:GramSchmidtTest for an implementation. For orthogonal
bases the condition (6) is exactly unitary condition if n = D, and if n > D we name it — the
condition of partial unitarity.
n−1
(cid:88)
δ = u u∗ j,j′ = 0...D−1 (8)
jj′ jk j′k
k=0
The meaning of F (5) is weighted (with ω(l) weights) sum of possible similarity between |ψ ⟩
l
and |ϕ ⟩. By construction we cannot directly project the states belonging to different Hilbert
l
spaces as ⟨ϕ |ψ ⟩, not to mention that such a direct projection will not be invariant with
l l
respect to random phases of measured wavefunctions. The only possible way to transform
a state from x to f is to apply operator U (4), this is a quantum channel that links two
different Hilbert spaces. The states ψ (x) and ϕ (f) in (1) basis are defined with l = 1...M
l l
coefficients α(l) and β(l). For (7) orthogonalized basis the functional (5) takes the form
k j
D−1 n−1
(cid:88) (cid:88)
F = u S u∗ (9)
jk jk;j′k′ j′k′
j,j′=0k,k′=0
M
(cid:88)
S = ω(l)β(l)α(l)β(l)∗α(l)∗ (10)
jk;j′k′ j k j′ k′
l=15
this expression is obtained with (4) conversion of ϕ (f) to a function in x-space and then pro-
l
jecting it to ψ (x). For non-orthogonal bases (7) the expression for S is more complicated
l jk;j′k′
and less convenient to use. The tensor S = S∗ is Hermitian by construction. The
jk;j′k′ j′k′;jk
original problem is now reduced to: maximize (9) functional subject to (8) partial unitarity
constraint. There are a number of practical problems that can be reduced to this optimization
problem.
A. A Quantum System Time Evolution
Consider a quantum system with time-independent Hamiltonian H. It’s time evolution
(cid:20) (cid:21)
t
U = exp −i H (11)
ℏ
(cid:12) (cid:12)ψ(t)(cid:11) = (cid:12) (cid:12)U|ψ(t=0)(cid:11) (12)
Assume we have a long sequence l = 1...M of system observations made equidistantly at
time moments t = τl
l
ψ (x) → ψ (x) weight ω(l) = 1 (13)
l l+1
measured wavefunctions can possibly have random phases. Given the basis (1a) measured
sample is a sequence of l = 1...M coefficients α(l) that define actual wavefunction within
k
a random phase exp(iξ ). To obtain the optimization problem of previous section we put
l
D = n, β(l) = α(l+1), and U is a unitary operator of wavefunction time shift τ.
j k
|ψ ⟩ = |U|ψ ⟩ (14)
l+1 l
The S is then
jk;j′k′
M
(cid:88)
S = ω(l)α(l+1)α(l)α(l+1)∗α(l)∗ (15)
jk;j′k′ j k j′ k′
l=1
The result of optimization problem (9) subject to unitary (n = D) constraint (8) is a unitary
time shift operator U. To obtain Hamiltonian from U — this requires taking the logarithm of
a unitary matrix, this is a different problem that requires separate consideration[21].
ℏ
H = i lnU (16)
τ6
B. A Classic System x → f Mapping
Consider classic x(l) → f(l) vectors mapping, l = 1...M, all measurements are real
numbers.
(x ,x ,...,x ,...,x )(l) → (f ,f ,...,f ,...,f )(l) weight ω(l) (17)
0 1 k n−1 0 1 j D−1
This is a mapping of some observable to observable classic measurements of real vectors,
for example let x be attributes and f being class label of some dataset used in machine
learning classification problem of vector type. A few example of f(x) constructed model:
linear regression, Radon-Nikodym approximation[20], a logical function, a neural network
model, etc.
We want to convert this data to ψ (x) → ϕ (f) form (2) to construct partially unitary
l l
operation U converting a state from x to f. Let us define an average in both Hilbert spaces
as the sum over data sample (17)
M
(cid:88)
⟨h⟩ = ω(l)h (18)
l
l=1
Gx = ⟨x x ⟩ (19)
kk′ k k′
Gf = ⟨f f ⟩ (20)
jj′ j j′
where h is a function on x or f. When (18) is applied to h = x(l)x(l) or f(l)f(l) obtain Gram
l k k′ j j′
matrix (19) or (20). As we discussed above the x and f bases can be always orthogonalized (7)
with basis linear transform; Gram matrix is equal to unit matrix in the case of an orthogonal
basis. Note, that with this classic data we can calculate “cross-moments” ⟨x |f ⟩, an example
k j
of a model that uses cross-moments is linear regression
(cid:42)(cid:12) (cid:12)2(cid:43)
(cid:12) (cid:88)n−1 (cid:12)
(cid:12)f − γ x (cid:12) → min (21)
(cid:12) j k k(cid:12)
(cid:12) (cid:12)
k=0
n−1
(cid:88)
f (x) ≈ x Gx;−1⟨f x ⟩ (22)
j k kk′ j k′
k,k′=0
As we use quantum channel ideology, our model should not depend on “cross-moments”, only
partially unitary operator U (4) can link x and f.7
To construct a wavefunction ψ (x) localized at x = y consider localized state ψ (x)
y y
n (cid:80)−1
y Gx;−1x
n (cid:80)−1
ψ[i](y)ψ[i](x)
n−1 i ik k
ψ (x) = (cid:112) K(y) (cid:88) y Gx;−1x = i,k=0 = i=0 (23)
y i ik k (cid:115) (cid:115)
n−1 n−1
i,k=0 (cid:80) y Gx;−1y (cid:80) [ψ[i](y)]2
i ik k
i,k=0 i=0
1 1
K(x) = = (24)
n−1 n−1
(cid:80) x Gx;−1x (cid:80) [ψ[i](x)]2
i ik k
i,k=0 i=0
The ψ (x) is a function on x localized at given y, it is just normalized reproducing kernel,
y
1 = ⟨ψ |ψ ⟩. In (23) it is written in two bases: original x , for which ⟨x x ⟩ = Gx , and in
y y k q k qk
some orthogonalized basis (cid:12) (cid:12)ψ[i](cid:11) such that (cid:10) ψ[q](cid:12) (cid:12)ψ[k](cid:11) = δ . The K(x) is Christoffel function.
qk
Localized states in f-space can be obtained with n to D and x to f replacement. With these
localized states obtain ψ (x) → ϕ (f) mapping for classic data (17), l = 1...M
l l
(cid:113) n−1 (cid:113) D−1
(cid:88) (cid:88)
K(x(l)) x(l)Gx;−1x → K(f(l)) f(l)Gf;−1f weight ω(l) (25)
i ik k i ik k
i,k=0 i,j=0
This mapping is actually an original x → f mapping (17) with x and f linearly transformed
and normalized. Contrary to an original mapping — the (25) can be considered as a mapping
of two Hilbert space states with (18) scalarproduct. It is convenient to introduce the moments
of Christoffel functions product:
(cid:10) x kf j(cid:12) (cid:12)K(x)K(f)(cid:12) (cid:12)x k′f j′(cid:11) =
(cid:88)M
ω(l)
x( kl)x( kl ′)
·
f j(l)f j( ′l)
(26)
n−1 D−1
l=0
(cid:80) x(l)Gx;−1x(l) (cid:80) f(l)Gf;−1f(l)
q qq′ q′ s ss′ s′
q,q′=0 s,s′=0
Assuming the x and f bases are orthogonalized (7), the tensor (10) is then
S
jk;j′k′
= (cid:10) x kf j(cid:12) (cid:12)K(x)K(f)(cid:12) (cid:12)x k′f j′(cid:11) (27)
We obtained, now for classic data, an optimization problem (9) subject to (8) partial uni-
tarity constraint. The result is operator U maximizing the F. This problem has the same
mathematical form as quantum problem of Section IIA above. But there is one important
difference: where Hilbert space inner product comes from. In quantum problem it is an
original property of Hilbert spaces used to formulate the problem, but the functional F is
obtained from measurement data. In classic problem both scalar product in Hilbert space
and functional F are obtained from measurement data.8
To evaluate the result at some given point y construct localized density (23) and transform
it to f-space with (4). Projecting it to a g-localized state in f-space obtain the probability of
a given outcome g (here ψ (f) is a f-space state localized at f = g, similarly to x-state (23),
g
i.e. in g Gf;−1f put f from (4) after that obtained expression is a function on x that can
i ij j j k
be coupled with (23); for orthogonal bases (7) obtain (27))
(cid:12) (cid:12)2
(cid:12)n−1 D−1 (cid:12)
(cid:12)(cid:80) (cid:80) g Gf;−1u y (cid:12)
(cid:12) j js sk k(cid:12)
(cid:12)k=0j,s=0 (cid:12)
⟨ψ |U |ψ ⟩2 = (28)
g y
D−1 n−1
(cid:80) g Gf;−1g (cid:80) y Gx;−1y
j jj′ j′ k kk′ k′
j,j′=0 k,k′=0
The optimization result is u matrix, j = 0...D−1;k = 0...n−1. This operator, given
jk
some input state (such as localized state |ψ ⟩), uniquely (within a phase) finds the function
x
in f-space |U|ψ ⟩ (coefficients a (x)) that predicts the probability (28) of outcome f:
x j
(cid:12) (cid:12)2
(cid:12)D−1 (cid:12)
(cid:12)(cid:80) a f (cid:12)
(cid:12) j j(cid:12)
(cid:12) (cid:12)j=0 (cid:12)
P(f)(cid:12) = ⟨ψ |U |ψ ⟩2 = (29)
(cid:12) f x
x D (cid:80)−1 f Gf;−1f
j jj′ j′
j,j′=0
D−1n−1
(cid:88)(cid:88) (cid:112)
a = Gf;−1u x K(x) (30)
j js sk k
s=0 k=0
the f is equal to the value of the outcome we are interested to determine the probability
of. Given x the probability of an outcome f is squared linear function on f multiplied by
j
Christoffel function K(f). This form of probability, a linear function on f squared divided
j
by a quadratic form on f , can be obtained from many different considerations; the difference
j
between them is in coefficients a . The maping (25) is a pure states mapping. For mixed
j
states mapping the probability will be a ratio of two general quadratic forms instead of rank
one matrix in the nominator (29).
For a simple demonstration of creating x → f classic mapping see Section VI below.
C. Learning Unitary Dynamics
Innaturemostofdynamicequationsareequivalenttoasequenceofunitarytransformations
(Newton, Maxwell, Schr¨odinger equations). Consider a simple classic problem. Let there is
an initial state vector X(0) of unit L2 norm and a unitary matrix U. The operator is applied9
to X(0) M times:
X(l+1) = UX(l) (31)
From this sequence M observations (x,f) (17) are created by taking (l,l+1) elements of the
sequence and multiplying them by a random phase (or ±1 for real space).
x(l) = exp(iξ )X(l) (32a)
l
f(l) = exp(iζ )X(l+1) (32b)
l
The random phases make any x ↔ f regression-type methods inapplicable. From unitary
propertyofoperatorU we immediatelyget(6),itdoes notdependon random phases. Whereas
in quantum problem we have a Hilbert space with an inner product ⟨·⟩, in this classic problem
there is no built in inner product available. The only available average is the sum (18) over
M observations, there is no any other average such as ensemble, quantum measurement, etc.
For unitary dynamic with ω(l) = 1 this sum is regular time-average. The problem becomes: to
determine operatorU from sample (32),l = 0...M−1,thatundergoes unitarytime-evolution
(31). The goal is to determine a matrix u (with D = n) maximizing quality criterion F (5).
jk
Gram matrices Gx (19) and Gf (20) are time-average. The functional F is also time-
kk′ jj′
average of (31) data
(cid:12) (cid:12)2
(cid:88)M (cid:12)D (cid:88)−1 (cid:88)n−1 (cid:12)
F = ω(l)(cid:12) X(l+1)u X(l)(cid:12) −−→ max (33)
(cid:12) j jk k (cid:12)
(cid:12) (cid:12) u jk
l=1 j=0 k=0
Using (32) obtain (for real space)
M
(cid:88)
S = ω(l)f(l)x(l)f(l)x(l) (34)
jk;j′k′ j k j′ k′
l=1
Random phases (±1 for real space) in (32) do not affect S and Gram matrices ⟨x x ⟩,
jk;j′k′ k k′
⟨f f ⟩ as the phases cancel each other in probabilities. The Gram matrices are not neces-
j j′
sary unit matrices. This can be changed by basis regularization. Introduce regularization
transformations Rx and Rf such that
x = Rxx (35a)
f = Rff (35b)10
produce unit Gram matrices; this can be for example Rx = Gx;−1/2, Rf = Gf;−1/2 or any
other, e.g. Gram-Schmidt with pivoting or QR decomposition. Then the transform (4) can
be written in the form
f = RfURx;−1x (36)
and we can consider an optimization problem for operator U(cid:101) = RfURx;−1 instead of for
U. The solution in the original basis is then U = Rf;−1U(cid:101)Rx, see com/polytechnik/kgo
/DemoRecoverUnitaryMatrixFromSeq.java:getUFromConfigGramMatrixChannel for an
implementation.
A question can be asked whether in the D = n case of data (32) an operator of system
dynamics U having the matrix u maximizing F subject to (6) constraints will always
jk
be unitary? It depends on the data. For a data of unitary dynamics (31) Gram matrices
⟨x x ⟩ and ⟨f f ⟩ must be the same since unitary U preserves scalar product. But if the
k k′ j j′
data contains non-unitary contribution — Gram matrices ⟨x x ⟩, ⟨f f ⟩ can be different and
k k′ j j′
this difference contributes to non-unitarity of u ; the constraints (6) preserve Gram matrix
jk
passing through quantum channel. This is the meaning of the constraints — Gram matrix
must to transform from Hilbert space IN into Hilbert space OUT as any other operator. The
idea is to measure Gram matrix from sample in both Hilbert spaces, construct u by solving
jk
optimization problem and then generalize the model stating that any other operator converts
the same (37). If other than Gram matrix operators are available in both Hilbert spaces —
they can be used to build a quantum channel in exactly the same manner. Consider unit
matrix transformation what corresponds to traditional unitary learning.
D. Traditional Unitary Learning
Consider “traditional” unitary learning. For (32) data sample it is postulated that operator
U (31) is unitary hence Gram matrix properties are irrelevant to the task. All observation
vectors x and f are of unit L2 norm and of the same dimension. The problem becomes:
maximize F (9) subject to unitary U, i.e. (8) constraints with D = n. This is exactly the
problem considered in Section IIC above but with a quantum channel transforming (37)
a unit matrix from Hilbert space IN into a unit matrix in Hilbert space OUT (instead of
Gram matrix). Practically this unit matrix quantum channel can be implemented exactly as11
considered above, the only difference — no regularization (35) should be performed at all
since it is postulated that U must always be unitary. The f and x should be used directly
as is (without regularization) when constructing S (34), see com/polytechnik/kgo
jk;j′k′
/DemoRecoverUnitaryMatrixFromSeq.java:getUFromConfigUnitMatrixChannel for an
implementation. Contrary to Gram matrix quantum channel of previous section the U
obtained with unit matrix quantum channel is always unitary. For a simple demonstration of
recovering u from unitary dynamics data (32) see Section IV below.
jk
This unitary learning considers x and f vectors being of the same dimension. The approach
can be directly generalized to partial unitarity. Assume a dataset is of x → f mapping having
all vectors x and f of unit L2 norm, but the vectors x and f are now of different dimensions:
n and D respectively. We want to build a partially unitary operator U of the dimension
D × n that converts a vector from x to f preserving probability. The model has direct
assumption about U and dataset hence Gram matrix properties are irrelevant to the task.
The problem becomes: maximize F (9) subject to partial unitary constraints (8), now with
D < n. The calculations are identical to the problem D = n we just considered. Calculate
S (34) using x and f directly, without regularization. Then optimize F subject to (8)
jk;j′k′
with corresponding D and n. Obtained partially unitary operator U is a quantum channel
transforming a unit matrix of the dimension n in Hilbert space x to a unit matrix of the
dimension D in Hilbert space f. This quantum channel maximizes the fidelity of mapping
between Hilbert spaces of different dimensions This can be a completely new look into unitary
ML models.
E. Algebraic Structure of the Optimization Problem
Formulated optimization problem maximizes (9) subject to partial unitarity constraint
(8). This is a variant of QCQP problem. We find an operator U optimally transforming (on
given measurement data) a state |ψ(x)⟩ from Hilbert space IN (of dimension n) into a state
|ϕ(f)⟩ from Hilbert space OUT (of dimension D). The operator is a rectangular matrix of
the dimension D×n transforming an operator A from IN to OUT as
AOUT = UAINU† (37)12
This transform converts any operator between two Hilbert spaces, for example a pure state
AIN = |ψ⟩⟨ψ| into a pure state AOUT = |U|ψ⟩⟨ψ|U|, for a more general form see Kraus
operators (77) below. There always should be an operator known in both Hilbert spaces,
it is used to create the constraints on U when maximizing the fidelity — these constraints
(probability preservation) determine the specific form of partial unitarity. We consider two
such operators: Gram matrix (most of this paper) and unit matrix (traditional unitary
learning in Section IID). Constrained optimization problem on U is reduced to a new
algebraic problem (this is a variation of Lagrangian L (48) set to zero)
SU = λU (38)
which is remotely similar to an eigenvalue problem, but S is a Hermitian tensor, “eigenvector”
U is partially unitary D × n matrix, and “eigenvalues” λ is a Hermitian D × D matrix;
functional extremal value F is equal to λ spur (the sum of diagonal elements). The algebraic
structure of this eigenvalue–like problem, let us call it an “eigenoperator” problem, requires
a separate study. Currently we only have a numeric solution algorithm.
III. NUMERICAL SOLUTIOIN
The problem becomes: optimize (39) subject to (40) constraints
D−1 n−1
(cid:88) (cid:88)
F = u S u∗ −→ max (39)
jk jk;j′k′ j′k′
u
j,j′=0k,k′=0
n−1
(cid:88)
δ = u u∗ j,j′ = 0...D−1 (40)
jj′ jk j′k
k=0
The tensor S = S∗ is Hermitian, for simplicity we consider it and u to be real
jk;j′k′ j′k′;jk jk
and do not write complex conjugated ∗ below and use the terms unitary and orthogonal
interchangeably; a generalization to complex values is straightforward. If we consider a subset
of all (40) constraints the optimization problem can be readily solved. Consider the squared
Frobenius norm of matrix u to be a “simplified constraint”:
jk
D−1n−1
(cid:88)(cid:88)
u2 = D (41)
jk
j=0 k=0
This is a partial constraint (it is the sum of all (40) diagonal elements). For this partial
constraint the optimization problem (39) is equivalent to an eigenvalue problem — it can13
be directly solved by considering a vector of Dn dimension obtained from u operator by
jk
saving all it’s components to a single vector, row by row.
The main idea of [19] was to modify partial constraint solution to satisfy the full set of the
constraints (40). There are several options to perform solution adjustment from partial to full
set of constraints. The one producing the minimal change to the solution is an application of
Gram matrix
n−1
(cid:88)
Gu = u u (42)
jj′ jk j′k
k=0
√
inverse square root Gu;−1/2 = 1/ Gu to u .1 There are 2D square roots of a positively
jk
definite symmetric matrix dimension D, different within the ± signs. The simplest method
to calculate it — convert Gu to diagonal form in the basis of Gu eigenvectors
jj′ jj′
(cid:12) (cid:12)Gu|g[i](cid:11) = λ[i](cid:12) (cid:12)g[i](cid:11) (43)
G
(cid:46)(cid:113)
then change the eigenvalues to ±1 λ[i] and convert the matrix back to initial basis
G
D−1
(cid:13) (cid:13)Gu;−1/2(cid:13) (cid:13) = (cid:88) (cid:113)±1 (cid:12) (cid:12)g[i](cid:11)(cid:10) g[i](cid:12) (cid:12) (44)
i=0
λ[i]
G
By checking the result one can verify that for any u producing non-degenerated Gram
jk
matrix (42) the vector
D−1
(cid:88)
u = Gu;−1/2u (45)
(cid:101)jk ji ik
i=0
satisfies all the constraints (40) (the most general form of an adjustment is an application of
BGu;−1/2 to u where B is an arbitrary unitary operator, the ± square root signs can be
ik
included into B; below we consider all signs in (44) to be “+”), see com/polytechnik/kgo/
AdjustedStateToUnitaryWithEigenproblem.java for an implementation. Thus multiple
constraints optimization problem (39) can be reduced to an unconstrained optimization
1 See Appendix A of [19] for adjustments in different bases and for an approach that uses SVD instead of
eigenproblem (43). Also note that both SVD-based and eigenproblem-based transforms convert a single
u state satisfying partial constraint (41) to the state satisfying the full set of the constraints (40). A
jk
problem of converting several u[s] states satisfying partial constraint to a single state satisfying the full set
jk
of the constraints can greatly improve algorithm’s initial convergence, this is a subject of future research.14
problem (we use D = (cid:80)D−1(cid:80)n−1u2 = (cid:80)D−1 (cid:80)n−1u Gu;−1u identity):
j=0 k=0 jk j,j′=0 k=0 jk jj′ j′k
D−1 n−1
(cid:80) (cid:80) u Gu;−1/2S Gu;−1/2u
jk ji ik;i′k′ j′i′ j′k′
F = j,j′,i,i′=0k,k′=0 −→ max (46)
1
D (cid:80)−1n (cid:80)−1
u2
u
D jk
j=0 k=0
However, this unconstrained problem2
• cannot be reduced to an eigenvalue problem since Gu;−1/2 itself depends on u .
ji jk
• is degenerate: there are multiple u producing the same F. Convert a solution u
jk jk
to Gram matrix basis (43), change the eigenvalues, then convert back to the initial
basis, this is similar to (44) transform. Hence the Hessian matrix is degenerated, this
prevents us from direct application of Newton’s and quasi-Newton optimizations. One
can possibly use a penalty function like (cid:80) 1/λ
G
D−1
(cid:88) 1
SpurGu;−1 = (47)
λ[i]
i=0 G
that has the minimal value D when all the constraints (40) are satisfied.
Alternatively an iteration approach with Lagrange multipliers can be used. Iterations are
required since we cannot simultaneously solve the equation for u and λ . A single iteration
jk jj′
consists in solving an eigenvalue problem, adjusting obtained solution to satisfy the full set
of the constraints, and calculating the new values of Lagrange multipliers; there are three
building blocks of the algorithm:
• Eigenproblem solution (56) to solve partially constrained optimization problem. An
important feature is that any additional linear constraints on u of (57) form can be
jk
easily incorporated — obtain the eigenproblem (63).
• Solution adjustment operation (45) that converts a partial constraint solution (41) to
a full set one (40).
2 Therearealternativewaystoobtainanunconstrainedoptimizationproblem.IntheunitarycaseD =none
canuseD(D+1)/2parameterstoparameterizeaHermitianmatrix,aunitarymatrixisthenobtainedwith
matrix exponentiation (11); functional optimization, however, requires the derivatives of matrix exponent
what complicates the problem[22, 23]. There is an option to parameterize a unitary matrix with Cayley
transform U =(I−A)(I+A)−1 for a skew-Hermitian matrix A† =−A. Both methods are problematic
to use, especially in the case of rectangular u , with D < n. See also [24] about other ways of unitary
jk
parametrization.15
• Linear system solution (52) to obtain new values of Lagrange multipliers.
In it’s na¨ıve form a convergence of the iteration algorithm turned out to be poor. The major
new result of the current paper is an iteration algorithm with a good convergence. A good
convergence was achieved by considering additional linear constraints to eigenproblem at
iteration step. Consider Lagrange multipliers λ , a matrix of D×D dimension, to approach
jj′
optimization problem (39) with the constraints (40)
(cid:34) (cid:35)
D−1 n−1 D−1 n−1
(cid:88) (cid:88) (cid:88) (cid:88)
L = u S u + λ δ − u u −→ max (48)
jk jk;j′k′ j′k′ jj′ jj′ jk′ j′k′
u
j,j′=0k,k′=0 j,j′=0 k′=0
Variating L over u obtain (50)
sq
D−1 n−1
(cid:88)(cid:88)
b = S u (49)
sq sq;j′k′ j′k′
j′=0k′=0
D−1
(cid:88)
0 = b − λ u (50)
sq sj′ j′q
j′=0
Here and below we consider Lagrange multipliers matrix to be Hermitian λ = λ , this is a
jj′ j′j
condition of extremal solution existence[19]. The variation (50) contains Dn equations. The
Hermitian Lagrange multipliers matrix λ has D2 real parameters (D(D+1)/2 independent
jj′
ones) for real space and 2D2 real parameters (D2 independent ones) for complex space. Thus
for a general u the variation (50) cannot be satisfied in full. The most straightforward way
jk
to obtain Lagrange multipliers for a given u is to take L2 norm of variation (50) and obtain
jk
the λ minimizing the sum of squares.
ij
(cid:12) (cid:12)2
D (cid:88)−1 (cid:88)n−1(cid:12) D (cid:88)−1 λ +λ (cid:12)
(cid:12)b − ij ji u (cid:12) −→ min (51)
(cid:12) (cid:12) iq 2 jq(cid:12) (cid:12) λij
i=0 q=0 j=0
n−1 D−1 n−1
(cid:88) (cid:88) (cid:88)
λ = Herm u b = Herm u S u (52)
ij ik jk ik jk;j′k′ j′k′
k=0 j′=0k,k′=0
The minimization gives Hermitian matrix λ which is obtained as a linear system solution;
ij
see com/polytechnik/kgo/LagrangeMultipliersPartialSubspace.java:calculateReg
ularLambda for an implementation. A more general form of λ is presented in Appendix A
ij
below. The problem can now be considered as maximizing a quadratic form with the matrix
S
jk;j′k′
S = S −λ δ (53)
jk;j′k′ jk;j′k′ jj′ kk′16
D−1 n−1
(cid:88) (cid:88)
u S u −→ max (54)
jk jk;j′k′ j′k′
u
j,j′=0k,k′=0
subject to constraints (40). Consider the eigenproblem3
D−1 n−1
(cid:80) (cid:80)
u S u
jk jk;j′k′ j′k′
j,j′=0k,k′=0 −→ max (55)
D (cid:80)−1 n (cid:80)−1 u
u Q u
jk jj′ j′k
j,j′=0k=0
D−1 n−1 D−1 D−1
(cid:88)(cid:88) (cid:88) (cid:88)
S u[s] − λ u[s] = µ[s] Q u[s] (56)
sq;j′k′ j′k′ sj′ j′q sj′ j′q
j′=0k′=0 j′=0 j′=0
with additional N linear constraints added (their specific form providing a good convergence
d
of the algorithm is discussed below in Appendix A1)
D−1n−1
(cid:88)(cid:88)
0 = C u d = 0...N −1 (57)
d;jk jk d
j=0 k=0
A common method of solving eigenproblem (55) with homogeneous linear constraints (57)
is Lagrange multipliers method[25]. This approach, however, creates a difficulty when both
linear and quadratic constraints are present, especially when the number of constraints is
large. A better approach to deal with linear constraints is to convert (57) to a form that
expresses rank(C ) components of u via it’s other components, the coefficients by the
d;jk jk
selected components C(cid:101) are zero (we plain moved some of (57) terms from right to left
d;jk
hand side).
D−1n−1
(cid:88)(cid:88)
u = C(cid:101) u d = 0...rank(C )−1 (58)
j[d]k[d] d;jk jk d;jk
j=0 k=0
Then any u satisfying (57) linear constraints can be expresses as a linear combination of
jk
selected components, let us denote them as some general variables V , p = 0...N −1
p V
N = Dn−rank(C ) (59)
V d;jk
N (cid:88)V−1
u = M V (60)
jk jk;p p
p=0
3 From the variation (50) it follows that the most interesting u[s] states have the value of functional (55)
jk
close to zero thence the matrix Q in the denominator can be chosen as an arbitrary positively definite
jj′
Hermitian matrix to improve convergence. The Q = δ gives familiar results; the other choice can
jj′ jj′
be Q =λ , i.e. have Lagrange multipliers in denominator instead of adding them to (53): consider a
jj′ jj′
D−1 n−1 (cid:46) D−1 n−1
(cid:80) (cid:80) (cid:80) (cid:80)
variation of u S u u λ u over u to obtain the same (50); see com/p
jk jk;j′k′ j′k′ jk jj′ j′k jk
j,j′=0k,k′=0 j,j′=0k=0
olytechnik/kgo/KGOIterationalLagrangeMultipliersInDenominatorU.java.17
Substitute (60) back to (55) to obtain an unconstrained generalized eigenproblem (62)
relatively N generalized variables V ; linear constraints (57) are incorporated into new
V p
variables V , the number of variables is reduced by rank(C ).4 The (60) transform is
p d;jk
actually a regular Gaussian elimination, a special form of LU decomposition. A simple
implementation with row and column pivoting is used in com/polytechnik/utils/Elim
inateLinearConstraints_HomegrownLUFactorization.java; for a very large number of
linear constraints a transform like RRQR factorization is probably more numerically stable.
The new eigenproblem has the matrices S and Q in nominator and denominator
p;p′ p;p′
D−1 n−1
(cid:88) (cid:88)
S = M S M (61a)
p;p′ jk;p jk;j′k′ j′k′;p′
j,j′=0k,k′=0
D−1 n−1
(cid:88) (cid:88)
Q = M Q M (61b)
p;p′ jk;p jj′ j′k;p′
j,j′=0k=0
the λ converts the same. We can write the eigenproblem (55) in the form
p;p′
N (cid:80)V−1
V S V
p p;p′ p′
p,p′=0 −→ max (62)
N (cid:80)V−1 V
V Q V
p p;p′ p′
p,p′=0
N (cid:88)V−1 N (cid:88)V−1 N (cid:88)V−1
S V[s] − λ V[s] = µ[s] Q V[s] (63)
p;p′ p′ p;p′ p′ p;p′ p′
p′=0 p′=0 p′=0
The cost of this conversion is that if originally we put Q = δ then in V basis the
jj′ jj′ p
denominator in (62) is no longer a unit matrix. This is not an issue since any modern linear
algebra package internally converts a generalized eigenproblem to a regular one, see e.g.
DSYGST, DSPGST, DPBSTF and similar subroutines.
Let the original eigenproblem (56) be already solved and extremal u[s] satisfying partial
jk
quadratic constraint (41) are obtained. Consider Lagrangian variation δL/δu . If the state
sq
u is (55) extremal then the variation (50) is zero
jk
D−1 n−1 D−1
(cid:88)(cid:88) (cid:88)
0 = S u − λ˘ u (64)
sq;j′k′ j′k′ sj′ j′q
j′=0k′=0 j′=0
4 Note that at this stage some of the vectors that were removed with the constraints can be possibly injected
back into V basis; they can be directly added as additional column(s) to M . This only changes the size
p jk;p
of V basis N =Dn−rank(C )+N . A better option, however, is to modify the C in the first
p V d;jk inj d;jk
place and avoid basis injection.18
λ˘ = λ +µ[s]Q (65)
jj′ jj′ jj′
It is exactly zero (for all Dn equations) if u is one of u[s] and Lagrange multipliers are (65).
jk jk
This u , however, may not satisfy the full set of the constraints (40). It is adjusted with
jk
(45) to satisfy all the constraints, and a new λ is calculated to use in the problem (56). A
jj′
difficulty we met in [19, 20] is that this iteration algorithm when λ (52) is used — it does
jj′
not converge to a solution, only given large enough iterations number it produces a good
enough solution among iterations seen. In this paper this difficulty is overcame by solving
the eigenproblem (56) with extra linear constraints (57). The improved iteration algorithm
becomes:
1. Take initial λ and linear constraints C to solve optimization problem (62) with
ij d;jk
respect to V . Solution method — an eigenvalue problem of N dimension, the number
p V
of columns in M matrix. A new u is obtained from V using (60). The result:
jk;p jk p
s = 0...N −1 eigenvalues µ[s] and corresponding matrices u[s] reconstructed from
V jk
V[s]. The value of N is typically Dn−(D−1)(D+2)/2.
p V
2. A heuristic is required to select the u among all N eigenstates. Trying a number of
jk V
them and selecting the maximum (i.e. from all s = 0...N −1 select the best state
V
among top eigenvalues µ[s]: try all positive and 10 highest negative) providing a large
value of the original functional (46) typically gives only a local maximum. Numerical
experiments show that the index of eigenstate is a good invariant: always selecting the
stateV[s] of: largestµ[s],secondlargestµ[s],thirdlargestµ[s],etc. convergestoadifferent
p
solution of the original problem. The global maximum of F typically corresponds to
the largest µ[s] selection. A good heuristic is to run algorithm n times, always selecting
the state of n-th largest µ[s]. Then select the global maximum among these n runs; the
remaining n − 1 solutions are also good — this way we managed to obtain up to a
dozen of different solutions. At worst — a cycle without convergence (usually with a
period of 2 iterations) is observed; this was a problem back in [19]. With the linear
constraints technique of Appendix A1 this difficulty is overcame.
3. Obtained u is not partially unitary as the constraint (41) is a subset of the full ones
jk
(40). Apply the adjustment (45) and calculate the λ (52) in the adjusted state, this is
ij
the Lagrange multipliers for the next iteration.19
4. For a good convergence, in addition to λ , we need to select a subspace for u next
ij jk
iteration variation. Using full size Dn basis gives a poor convergence [19]. There are
two feasible options to improve it: either use some advanced method of λ calculation,
ij
see Appendix A below, or constraint the subspace of u variation, see Appendix A1
jk
below; the later technique of additional linear constraints C (57) provides superior
d;jk
results.
5. Put this new λ to (53) and, using the basis V obtained (60) from C , calculate
ij p d;jk
generalized eigenproblem nominator and denominator matrices (61) to be used for the
next iteration. Repeat iteration process until converged to a maximum (presumably
global) of F with u satisfying the constraints (40). If a convergence is achieved the
jk
λ stops changing from iteration to iteration and the µ[s] of the selected state in step 2
ij
above is close to zero. On the first iteration take initial values of Lagrange multipliers
λ = 0 and no linear constraints.
ij
Based on a number of numerical experiments we can possibly conclude that this iteration
algorithm always converges. A determination of exact convergence domain is a subject of
futureresearch. Adistinctivefeatureofthisalgorithmisthatinsteadofusualiterationinternal
state in the form of a pair: approximation, Lagrange multipliers, (u ,λ ) it uses iteration
jk ij
internal state in the form of a triple: approximation, Lagrange multipliers, homogeneous
linear constraints, (u ,λ ,C ). Whereas most optimization algorithms have linear system
jk ij d;jk
solution (Newtonian iteration) as a building block,the algorithm in question has eigenproblem
solution as the building block. This allows us to develop a much more fine-grained solution
selection in step 2 above, what makes the algorithm less sensitive to degeneracy and more
likely to converge to the global maximum. The drawback of using eigenproblem solution as
a building block is that it is more computationally costly than a linear system solution or
a first order gradient method. However the main goal of the paper is to present a working
proof of concept algorithm capable to solve a new algebraic “eigenoperator” problem (38);
computational complexity of optimization is a separate task. Among evident optimizations
— since in step 2 it is usually sufficient always to select the state of maximal eigenvalue
a replacement of a general purpose eigenproblem solver by a one finding only the largest
eigenvalue is expected greatly to increase algorithm performance.
A reference implementation of this algorithm is available at com/polytechnik/kgo/K20
TABLE I. Convergence comparison for this paper algorithm and the one from our previous work
[19] on data sample with D = 4, n = 19, M = 13540 for the first i = 0...17 iterations. We present:
µ – the eigenvalue of selected state, F – the value of (46) functional, and an indicator of unitarity
(cid:80)
1/λ .
G
KGOIterationalSubspaceLinearConstraints KGOIterationalSimpleOptimizationU
(cid:80) (cid:80)
i µ F 1/λ µ F 1/λ
G G
0 2296.97 7864.08 38.86 2296.97 7864.08 38.86
1 201.08 7936.32 20.02 239.84 8020.58 34.94
2 113.98 8010.69 27.04 110.62 8032.89 17.70
3 100.82 8178.92 7.07 53.94 8014.30 14.31
4 64.91 7890.37 26.06 150.60 8138.49 15.82
5 204.71 8078.93 32.47 57.33 8113.74 11.39
6 134.24 7873.87 34.30 109.05 8071.24 17.90
7 201.39 8112.94 24.76 38.24 8005.98 11.93
8 90.78 7869.54 34.38 92.91 8059.80 18.31
9 159.77 8057.35 12.58 49.79 8101.24 6.51
10 83.15 8212.67 8.06 148.46 8094.14 29.02
11 28.15 8194.82 6.22 86.11 8035.41 16.24
12 25.06 8292.05 4.03 156.60 8060.77 26.22
13 3.08 8302.52 4.02 95.37 8112.07 17.72
14 0.23 8303.46 4.00 73.73 8016.44 23.42
15 5.63·10−4 8303.46 4.00 202.11 8040.93 32.05
16 1.05·10−8 8303.46 4.00 93.91 7963.84 21.20
17 2.39·10−13 8303.46 4.00 192.02 8030.50 24.14
GOIterationalSubspaceLinearConstraints.java. There are dozens of other algorithms
implemented, but only this one provides iterations convergence. There is a driver in com/
polytechnik/kgo/KGOSolutionVectorXVectorF.java:main class for an algorithm quick
test. The driver generates a deterministic random sample from which the tensor S is
jk;j′k′
calculated and then given algorithm (taken from command argument) is applied to the21
problem of finding optimal partially unitary operator. Let us compare a convergence of this
paper iteration algorithm and the result of [19] (vanilla Lagrange multipliers) on driver’s
deterministic randomly generated data with default settings: D = 4 and n = 19 of M = 13540
observations.
java com/polytechnik/kgo/KGOSolutionVectorXVectorF ITERATIONS_KGOIterationa
lSubspaceLinearConstraints 2>&1 | grep Selected
and
java com/polytechnik/kgo/KGOSolutionVectorXVectorF ITERATIONS_KGOIterationa
lSimpleOptimizationU 2>&1 | grep Selected
The output is grep-filtered to show only iteration status. The result is presented in Table I.
From the table we see that the algorithm is cycling a number of initial iterations without a
convergence, then when it hits an area of contraction mapping — the convergence becomes
very fast. Plain vanilla Lagrange multipliers method does not converge at all: full Dn space
optimization of (55) with λ severely breaks the constraints (40) and problem degeneracy
ij
impede the convergence. This can be demonstrated by running the KGOIterationalSubspa
ceLinearConstraints in a standard way for 20 iterations to obtain a perfect solution, then
starting at iteration 21 turn off linear constraints (A8), i.e. internally switch it to vanilla
Lagrange multipliers method. The result — for the next 4 to 7 iterations the solution stays
almost the same, but after 7-10 iterations it is starting to diverge (due to accumulated
floating point errors) and an irregular cycling without convergence is observed. This shows a
critical importance of linear constraints (A8) for iteration algorithm convergence and intrinsic
instability of vanilla methods.
There is a number of QCQP software packages, both commercial and open source: [26],
python qcqp, NAG QCQP among many others. A question arize how well existing software
can handle a QCQP optimization problem of this paper? We did not test third party software
much (in the future we are going to test existing software much more extensively), but
generally — the results were unsatisfactory, especially in finding the global maximum. The
reason is that existing QCQP software packages were designed as general purpose solvers
intended to solve any QCQP problem, many of them are heuristic Newton-style solvers with
Lagrange multipliers often combined with some form of linear programming and convex
optimization. The optimization problem considered in this paper is special: it has only
quadratic equality constraints, the problem is non-convex, it has multiple solutions, local22
maximums, and multiple saddle points. For such a problem the solvers based on a single
solution iteration (e.g. Newtonian type (second order), gradient type (first order), etc.) are
unlikely to determine the global maximum. One may try solvers of Genetic programming
type, but they lack the knowledge about underline algebraic structure of the problem. The
problem of finding an operator optimally converting an operator from one Hilbert space
to another requires special solver. Our use of generalized eigenvalue problem as algorithm
building block has an advantage of obtaining at once many solution candidates (eigenvectors);
with proper selection we can greatly increase the chances of finding the global maximum.
Compared to [19] the quality of our optimization algorithm was greatly improved and we
are going to use it as a “black box” to apply to several problems for demonstration. Let us
demonstrate the value of finding a partially unitary operator optimally transforming a vector
from Hilbert space IN to Hilbert space OUT.
IV. A DEMONSTRATION OF RECOVERING UNITARY DYNAMICS FROM
PHASE-STRIPPED DATA
Consider an inverse problem. Let a dynamic system to evolve with X(l+1) = UX(l)
equation (31). The problem is to recover orthogonal operator U from an observed sample
X(l). The problem would be trivial if X(l) are directly observed — a regression analysis
can reveal U, see e.g. Section “An application of LRR representation solution to dynamic
system identification problem” of [20]. What greatly complicates the matter — we consider
observations of quantum channel type hence the X(l) are observable only within a phase.
This means that any wavefunction mapping (2) can be observed only within an unknown
phase; this can be modeled by a multiplication of actually measured classic values by random
phase factors. The problem becomes: from (32) sample to determine an operator U.5
In this section a few simple examples are presented. All the calculations are performed in
real space, thus a ±1 factor is used instead of a complex phase factor. A timeserie is initially
generated with (31) then obtained X(l) are multiplied by random ±1 factors, these new
vectors (32) are now considered as observables. The problem is to recover U from observable
data. We present a recovery with two quantum channels: transforming Gram matrix of
5 The operator itself is defined within a phase. More degeneracy may come form the data: e.g. if some
components X and X are always stay the same in the sample — then we cannot distinguish a unit
j j+1
matrix from a permutation matrix.23
Section IIC and transforming unit matrix of Section IID. Since the unitarity of test data is
exact — both methods recover underline operator U exactly, thus only Gram matrix quantum
channel is presented below.
Let us start with a simple SO(3) rotation group. Rotation matrix can be represented via
Euler angles (φ,θ,ψ) (in a space of dimension d there are total d(d−1)/2 angles).
   
cosφ −sinφ 0 1 0 0 cosψ −sinψ 0
   
g(φ,θ,ψ) = sinφ cosφ 00 cosθ −sinθsinψ cosψ 0 (66)
   
   
0 0 1 0 sinθ cosθ 0 0 1
Take U = g(φ = 0.1,θ = 0.4,ψ = 0.7)
 
0.7018 −0.7113 0.0389
 
U = 0.6668 0.6366 −0.3875 (67)
 
 
0.2509 0.2978 0.9211
andapply1000 transformations to initialvector(0.0921,0.5523,0.8285). The datafile SO3.csv
is generated with
java com/polytechnik/algorithms/PrintOrthogonalSeq
Writing 1000 points to /tmp/SO3.csv .
There is a simple demo-program that recovers operator U from sampled data (32) with
random phases ±1 possibly introduced into observations; this program maximizes (33) with a
crude regularization (36) (numerical stability depends on regularization, the result is not[20])
and without taking care of possible data degeneracy, but it’s simplicity makes it’s internals
clear.
java com/polytechnik/kgo/DemoRecoverUnitaryMatrixFromSeq --data_file_to_bui
ld_unitarymodel_from=/tmp/SO3.csv --data_cols=6:1,3:0
The program recovers the U identically
 
−0.7018 0.7113 −0.0389
 
U = −0.6668 −0.6366 0.3875  (68)
 
 
−0.2509 −0.2978 −0.9211
As we discussed above operator U is defined within a ±1 factor in real space. The same test
was run on randomly generated orthogonal matrices of the dimensions 3,5,7,17,40. All tests
can be run automatically as24
java com/polytechnik/algorithms/PrintOrthogonalSeq\$TestAuto
The maximal difference in matrix elements (for all the dimensions tried) is less that 10−13
what is about floating point errors. This makes us to conclude that developed numerical
algorithm of Section III can recover system dynamics from wavefunction measurements
(without a phase) for dynamic systems of high dimension. This algorithm is a powerful
method to solve quantum mechanics inverse problem.
V. A DEMONSTRATION OF POLYNOMIAL MAPPING RECOVERING
Consider polynomials in [−1 : 1] interval. Let there are l = 1...M points y(l) split
equidistantly in the interval. Define the data
x(l) = ξ(l)T (y(l)) k = 0...n−1 (69a)
k k
f(l) = ζ(l)P (y(l)) j = 0...D−1 (69b)
j j
Here T is Chebyshev polynomial and P is Legendre polynomial. The ξ(l) and ζ(l) are
k j
deterministicrandomfunctionsonl thattakethevalue±1. Theproblemistobuildu matrix
jk
maximizing (5) subject to (6) constraints. The mapping (4) for (69) data has the meaning of
finding the coefficients expanding D Legendre polynomials in n Chebyshev polynomials. The
problem would be trivial (reduced to a linear system solution) were it not for random ±1
factors ξ(l) and ζ(l) in (69). With random phases presence any direct projection of one basis
on another becomes unavailable, the only feasible way is to consider quantum channel (4)
mapping. The solution is similar to the one of previous section. Calculate Gram matrices
(19), (20) and S tensor (34). After any regularization of input data, for example (36),
jk;j′k′
apply the algorithm of Section III. Specifically, generate a datafile ChebyshevLegendre.csv
by running
java com/polytechnik/algorithms/PrintChebyshevToLegendreMapping
Writing 500 points to /tmp/ChebyshevLegendre.csv
This file has 500 [−1 : 1] points of (69) data with T ...T and P ...P randomly multiplied
0 10 0 5
by ±1 factors. Consider a simple task of converting T ...T to P ...P . Use a simple demo-
0 4 0 4
program that recovers the mapping (4) from sampled data (69) with random phases ±1
possibly introducedinto observations,the approachis phase-agnostic; this program maximizes
F; program’s simplicity makes it’s internals clear.25
java com/polytechnik/kgo/DemoRecoverMapping --data_file_to_build_model_from
=/tmp/ChebyshevLegendre.csv --data_cols=21:2,6:14,18:-1:0
The program recovers the mapping identically (the values below 10−15 are replaces by 0).
 
−1.0 0 0 0 0
 
 0 −1.0 0 0 0 
 
U =   −0.25 0 −0.75 0 0   (70)
 
 
 0 −0.375 0 −0.625 0 
 
−0.140625 0 −0.3125 0 −0.546875
Exactvaluescanbeobtainedusingapolynomiallibraryfrom[27],itisincludedwiththispaper
software [28]. Run in jshell new Legendre().convertBasisToPBASIS(5,new Chebyshev())
to obtain P over T expansion
j k
 
1.0
 
 0 1.0 
 
  (71)
 0.25 0 0.75 
 
 
 0 0.375 0 0.625 
 
0.140625 0 0.3125 0 0.546875
i.e. P = 0.375T +0.625T . This matches (70) within a ± sign. In this n = D case we have
3 1 3
a perfect recovery. Now consider the same problem of polynomial mapping for D < n, let us
run it with D = 4 and n = 5.
java com/polytechnik/kgo/DemoRecoverMapping --data_file_to_build_model_from
=/tmp/ChebyshevLegendre.csv --data_cols=21:2,6:14,17:-1:0
Obtained mapping
 
0 −1.294340 0 0.560225 0
 
−0.639496 0 −0.449476 0 0.075960 
U =   (72)
 
 0 −0.676776 0 −0.692739 0 
 
−0.149669 0 −0.620827 0 −0.502329
is not a subset of D = 5, n = 5 mapping (70), a subset can be obtained by running with
D = 4, n = 4. We previously discussed [19] the difficulties of the D < n case. A mapping
between two Hilbert spaces of different dimensions sometimes leads to an unusual behavior
since the probability, not the value, is maximized. The value of F increases when going from26
D = 4, n = 4 to D = 4, n = 5 but the mapping is no longer a subset. The behavior is
determined by S choice. In this section a product (34) is used. There are alternative
jk;j′k′
options, for example (27) that uses Christoffel function factor; a few other are discussed in
[19], see e.g. dimension adjusted Christoffel function in the Appendix C therein.
VI. A DEMONSTRATION OF FUNCTION INTERPOLATION
In two previous sections we considered examples of constructing u when the data of
jk
x → f form has vectors x and f already to belong to corresponding Hilbert spaces. For classic
measurement this is often not the case, a transformation x → ψ, f → ϕ is required to build
Hilbert space states ψ and ϕ from original observations x and f. The most straightforward
method to construct such states is to consider localized at given x (or f) states (23) to
use them as wavefunctions mapping (25). Considered in section IIB above localized states
ψ (x) use for scalar product the same Gram matrix (19), which is obtained from l = 1...M
y
sampled data. Effectively this is sampled average (18) with weight multiplied by a localized at
y non-negative function ψ2(x) (it is normalized as 1 = (cid:10) ψ2(x)(cid:11)), it gives a Radon-Nikodym
y y
approximation of some characteristic g
g (y) = (cid:10) gψ2(cid:11) (73)
RN y
Here approximatedvalue is a superposition ofobservedg withpositive densityψ2(x). Familiar
y
least squares interpolation (22) that is expanding the value, not a probability, has similar
form
g (y) = ψ (y)⟨gψ ⟩ (74)
LS y y
Here the average is taken with ψ (x) density — it is not always positive as the ψ2(x) in
y y
(cid:112)
Radon-Nikodym (73) is; the ψ (y) is normalizing factor 1/ K(y). The (73) and (74) differ
y
in how they represent delta function δ(x−y): as ψ2(x) or as ψ (y)ψ (x).
y y y
Consider scalar function interpolation problem in the form of two Hilbert space mapping
with u . Let there is a scalar f(x) and there are M observation points f(l) = f(x(l)),
jk
l = 1...M. Convert this scalar mapping to a vector one (the ξ(l) and ζ(l) are deterministic
random functions on l that take the value ±1)
x(l) = ξ(l)(cid:0) x(l)(cid:1)k k = 0...n−1 (75a)
k27
f(l) = ζ(l)(cid:0) f(l)(cid:1)j j = 0...D−1 (75b)
j
For numerical stability it is better, instead of monomials (powers of argument), to use
Chebyshev or Legendre polynomials as x(l) = ξ(l)T (ax(l)+b) and f(l) = ζ(l)T (cf(l)+d) with
k k j j
a,b,c,d chosen to bring argument into [−1 : 1] interval. This greatly increases numerical
stability of calculations; the result itself, however, is invariant relatively polynomial basis
choice — the result will be the same with any polynomial basis. From obtained vector to
vector mapping construct x(l) and f(l) localized states ψ (x) → ψ (f) mapping (2) with
x(l) f(l)
the former one defined in (23) and the later one obtained from it with argument and index
replacement (25). Put them to (28) and obtain (27). After solving the problem for u an
jk
evaluation of interpolated f(x) can be performed as following: from a given x construct the
vector x (75a). Substitute it to (29) to obtain the probability of a given vector of outcome
f. In scalar case the value of outcome can be evaluated, for example, by constructing (75b)
vector f from f and then finding the value of f providing the maximal value of probability
(29). For scalar f this problem can be reduced to finding roots of a single variable polynomial.
This approach creates a number of issues. An attempt to fit a scalar function f(x) to
Hilbert spaces mapping using the moments like (cid:10) fjxkfj′xk′K(x)K(f)(cid:11) (26) create difficulties
both in computations and in mapping back from Hilbert space to function value. A simple
demonstration Letf(x) = xp withp = 2. Thisrequires tocalculate thex-moments ofmaximal
degree 2(n−1)+2p(D−1) that for, say n = D = 6 gives the maximal polynomial degree of
30 what creates a numerical instability difficulty. Also the problem of converting back from
Hilbert space to the value of f by checking all extremums of (29) to find the f providing
the maximal probability is a “switching” function that creates a non-continuous solution
when polynomial root changes. An example is presented in Fig. 1. For f = x2 (red line) we
calculated Radon-Nikodym approximation (73) (least squares is identical to the original f
since the x2 is in the basis thus it is not presented in the plot), and the f corresponding
to the maximum of probability (blue line) and corresponding to it probability (pink). In
the plot one can see staring numerical instability (plot asymmetry) and discontinuity in f
corresponding to the changing of the selected root. As expected for the x of P(fmaxP(x)) = 1
the value of fmaxP is equal to exact f(x).
Alternatively[19] we can consider (29) without (75b) requirement that all components of28
1
1.25
0.75
f
1
f
RN
fmaxP 0.5
0.75
maxP
P(f )
max
P
0.5
0.25
0.25
0
0
-1 -0.5 0 0.5 1
x
FIG. 1. Scalar function f = x2 (red) interpolation with: green: Radon-Nikodym (73), blue: f
corresponding to maximal probability (29), the dependence has discontinuities; some numerical
instability presents even in this D = n = 6 case. Pink: probabilities corresponding to (29) and (76a).
f are obtained from a single scalar f.
D−1
(cid:88)
fmaxP = Gf a (76a)
j jj′ j′
j′=0
(cid:12) D (cid:88)−1
P(fmaxP)(cid:12) = a Gf a (76b)
(cid:12) j jj′ j′
x
j,j′=0
The maximal value of probability (29) at (76a) is an important characteristic of obtained
solution. If problem dimensions are balanced – the value is equal to 1 for all x (pink line Pmax
in Fig. 1), it can be lower in D < n case and this requires separate consideration. In [19] we
evaluated f(x) from first two elements of vector f: for a polynomial basis P (f) (in (75b)
j
P (f) = fj) we can obtain the value of f from the ratio of P = fmaxP and P (f) = fmaxP.
j 0 0 1 1
Back in [19] we thought that observed singularities are caused by a poor solution to the
optimization problem. Whereas the algorithm developed in this paper provides a very good
solution to the optimization problem of Hilbert spaces mapping, the method of obtaining the
f P29
value of scalar f from basis functions ratio does not work well.
Currently we do not have a good solution to the problem of converting two Hilbert spaces
mapping u into a scalar function f(x). This is a subject of future research.
jk
VII. CONCLUSION
A novel QCQP problem of an optimal mapping between Hilbert spaces IN of |ψ⟩ and
OUT of |ϕ⟩ based on a set of wavefunction measurement (within a phase) observations
ψ → ϕ , l = 1...M, is formulated as an optimization problem maximizing the total fidelity
l l
(cid:80)M ω(l)|⟨ϕ |U |ψ ⟩|2 subject to probability preservation constraints on U. It is a linear
l=1 l l
operator transforming states between IN and OUT Hilbert spaces as (37). The operator
U, a rectangular matrix u of dim(OUT)×dim(IN) dimensions with D = dim(OUT) ≤
jk
n = dim(IN), can be viewed as a quantum channel. Time evolution is a special case of this
problem. An optimization problem of a quadratic function on U with quadratic constraints is
solved with a numerical method outlined in Section III. The method is an iteration method
with generalized eigenproblem as it’s building block, this is different from commonly used
optimization methods that deploy Newtonian iteration or gradient iteration as a building
block; this allows us to find the global maximum for almost any input. In addition, instead
of usual iteration internal state in the form of a pair: approximation, Lagrange multipliers:
(u ,λ ), the algorithm uses iteration internal state in the form of a triple: approximation,
jk ij
Lagrange multipliers, homogeneous linear constraints (u ,λ ,C ); it is the very linear
jk ij d;jk
constraints that provide a fast convergence of the algorithm.
This numerical method can be applied to a number of classic and quantum problems,
especially to quantum mechanics inverse problem: recover a Hamiltonian form a sequence of
wavefunction observations; an operator U is obtained from observations then apply (16). The
equation is quadratic on U what makes the results the same if measured wavefunctions are
multiplied by random phase factors.
A demonstration of unitary dynamics X(l+1) = UX(l) system identification: determine U
from measured X(cid:101)(l) that are actual X(l) multiplied by random phase factors, is presented for
a number of orthogonal operators U of the dimensions n = D = {3,5,7,17,40}. An exact
recovery of U was observed. The technique was also applied to the problems of polynomial
bases mapping and scalar function interpolation. For polynomial bases mapping an exact30
solution was obtained. The problem of scalar function interpolation is difficult to solve with
the developed technique since there is no good equivalence between the problem of Hilbert
spaces mapping and scalar function mapping.
Considered problem of two Hilbert spaces optimal mapping is reduced to a new algebraic
“eigenoperator” problem (38), currently we have only a numerical solution to it. An important
generalization of Unitary Learning was made in this paper by considering Hilbert spaces of
different dimensions D ≤ n. A question arise about further generalization. An important
topic of future research can be Kraus’ theorem that determines the most general form of
Hilbert spaces mapping[29]:
(cid:88)
AOUT = B AINB† (77)
s s
s
with Kraus operators B satisfying the constraints that unit AIN is converted to unit AOUT
s
(cid:88)
B B† = 1 (78)
s s
s
We can further generalize Kraus operators B by considering them to be rectangular D×n
s
matrices with a combined constraints sum similar to (8) for a single operator. This is a
generalization of (37) transform; it is applicable to systems with quantum decoherence. In
the Appendix I of [20] corresponding optimization problem is formulated, but it’s numerical
solution is a subject of future research.
ACKNOWLEDGMENTS
This research was supported by Autretech group[30], www.атретек.рф. We thank our
colleagues from Autretech R&D department who provided insight and expertise that greatly
assisted the research. Our grateful thanks are also extended to Mr. Gennady Belov for his
methodological support in doing the data analysis.
Appendix A: On Lagrange Multipliers Calculation With Selected States Variation
In this work we do not use iterations for Lagrange multipliers. Instead, on every iteration,
for current solution approximation u we calculate Lagrange multipliers that correspond to
jk
an extremum in this state. This makes the algorithm more stable and allows a procedure31
of solution adjustment (45) from partial (41) to full set (40) of constraints to be directly
applied. The variation of Lagrangian L (48) must be zero in the state u
jk
D−1 n−1 D−1
1 δL (cid:88)(cid:88) (cid:88)
0 = = S u − λ u (A1)
iq;j′k′ j′k′ ij′ j′q
2δu
iq
j′=0k′=0 j′=0
These are Dn equations. Lagrange multipliers λ is a Hermitian D×D matrix of D(D+1)/2
ij
independent elements. Thus for a general u all Dn equations (A1) cannot be simultaneously
jk
satisfied. They are satisfied in (64) for eigenstate u[s] of (56), but after (45) adjustment this is
jk
no longer the case. A trivial solution is to take L2 norm of variation vector (A1) and minimize
the sum of squares (51) to obtain (52) as linear system solution. The problem which arise
is that there are too many equations Dn, the variation is a vector of Dn components, and
minimizing L2 norm of it with just D(D+1)/2 parameters makes the convergence poor. The
idea is to calculate the sum of squares only in specific states. Consider the problem
(cid:12) (cid:34) (cid:35)(cid:12)2
N (cid:88)v−1(cid:12)D (cid:88)−1 (cid:88)n−1 D (cid:88)−1 (cid:88)n−1 D (cid:88)−1 λ +λ (cid:12)
(cid:12) v[s] S u − ij ji u (cid:12) −→ min (A2)
(cid:12) (cid:12) iq iq;j′k′ j′k′ 2 jq (cid:12) (cid:12) λij
s=0 i=0 q=0 j′=0k′=0 j=0
The variation (A1) is projected on v[s], s = 0...N −1, states and the sum of projection
iq v
squares is taken. If v[s] is a full basis, e.g. all the s = 0...Dn−1 eigenvectors u[s] of (56)
iq jk
then the (A2) is exactly (51). One may think about (A1) as Lagrangian (48) variation
δL/δu . The sum of squares (51) is (cid:10) δL (cid:12) (cid:12) δL(cid:11), the sum of squares in v[s]-projected states
iq δu δu iq
(A2) is (cid:80)Nv−1(cid:10) δL (cid:12) (cid:12)v[s](cid:11)(cid:10) v[s](cid:12)
(cid:12)
δL(cid:11), for any full basis v[s] they are the same. In practice the
s=0 δu δu iq
v[s] may not be necessary full or orthogonal. If they are a subset of original eigenstates u[s]
iq jk
(56) of iteration algorithm – they are orthogonal, if (45) adjustment is applied to them –
they are not. One may consider cross states with Dn×Dn Gram matrix, similar to (23) as
(cid:80)D−1 (cid:80)n−1 u G−1 v , but this is not usually necessary since the selection of vectors
j,j′=0 k,k′=0 jk jk;j′k′ j′k′
v[s] is performed only for algorithm’s better convergence.
iq
Once the vectors v[s] are selected – in the problem (A2) the D×D Hermitian matrix λ
iq ij
should be expressed via λ vector of the dimension D(D+1)/2. A variation over λ gives
r r
a linear system of the dimension D(D + 1)/2 that can be readily solved. The number of
v[s] vectors should be at least D(D+1)/2, otherwise the linear system will be degenerated.
iq
For matrix to vector conversion and linear system solution see com/polytechnik/kgo/La
grangeMultipliersPartialSubspace.java:getLambdaForSubspace that implements this
functionality to solve (A2) minimization problem and obtain λ from a subspace chosen as
ij32
the states of high eigenvalues of problem (56). The success of this approach is moderate:
The number of top-µ[s] states to select is not precisely clear, linear system may become
degenerated, etc. One can check these our attempts at com/polytechnik/kgo/LagrangeM
ultipliersPartialSubspace.java:getLambdaForSubspace and usage in com/polytechn
ik/kgo/KGOIterationalLagrangeMultipliersPartialSubspace.java. This makes us to
conclude that instead of considering a subspace for constructing λ we should consider a
ij
subspace for variation of u .
jk
1. Linear Constraints On Variation
Degeneracyoftheproblemandquadraticconstraintsrequirenotonlyagoodapproximation
for Lagrange multipliers, but also a restricted subspace for variation of u in (55) problem.
jk
The difficulty comes from partial unitarity constraints (40), optimization (55) preserves
D−1n−1
only partial constraint (41). Consider a full orthogonal basis v[s], δ = (cid:80) (cid:80) v[s]v[s′],
jk ss′ jk jk
j=0 k=0
s = 0...Dn−1, and a variation vector δu expanded over this basis
jk
Dn−1
(cid:88)
δu = a v[s] (A3)
jk s jk
s=0
D−1n−1
(cid:88)(cid:88)
a = v[s]δu (A4)
s jk jk
j=0 k=0
Were we work in a regular vector space the only available operation is scalar product
D−1n−1
(cid:88)(cid:88)
⟨v|u⟩ = v u (A5)
jk jk
j=0 k=0
Now, when we study partially unitary operators (37), we have a tensor
n−1
(cid:88)
Gv|u = v u (A6)
ij ik jk
k=0
The scalar product corresponds to ⟨v|u⟩ = SpurGv|u, Gram matrix (42) is Gu = Gu|u.
ij ij
Consider a variation Gu+δu|u+δu. To preserve partial unitarity constraints (40) on u within
ij jk
linear terms (on δu) we must have all off-diagonal elements of HermGu|δu to be zero 0 =
ij
Gu|δu +Gu|δu, i ̸= j (homogeneous) and diagonal elements to be one (inhomogeneous). The
ij ji
partial constraint (41) does preserve matrix spur thus it is sufficient to have diagonal elements33
just to be equal, and this is a homogeneous constraint 0 = Gu|δu −Gu|δu , i = 1...D−1.
ii i−1i−1
Expanding δu in basis (A3) obtain the constraints
Dn−1
0 = (cid:88) a (cid:16) Gu|v[s] +Gu|v[s](cid:17) j < i, i = 0...D−1 (A7a)
s ij ji
s=0
Dn−1
0 = (cid:88) a (cid:16) Gu|v[s] −Gu|v[s] (cid:17) i = 1...D−1 (A7b)
s ii i−1i−1
s=0
There are (D−1)(D+2)/2 total linear homogeneous constraints on expansion coefficients
a . The a now became not all Dn independent, there are just Dn − (D − 1)(D + 2)/2
s s
independent of them. The constraints (A7) are homogeneous
N = (D−1)(D+2)/2 (A8a)
d
d : 0...D(D−1)/2−1 (A8b)
offd
d : D(D−1)/2...(D−1)(D+2)/2−1 (A8c)
diag
s : 0...Dn−1 (A8d)

 Gu|v[s] +Gu|v[s] if d ∈ d for j < i, i = 0...D−1
C = ij ji offd (A8e)
d;s
 Gu|v[s] −Gu|v[s] if d ∈ d for i = 1...D−1
ii i−1i−1 diag
and can be put directly to (57) after basis transformation
Dn−1
(cid:88)
C = C v[s] (A9)
d;jk d;s jk
s=0
There are D(D−1)/2 constraints for zero off-diagonal elements (A8b) and D−1 constraints
(one less the dimension) of diagonal elements equal to each other (A8c), there are total
(D −1)(D +2)/2 homogeneous constraints. The constraints C , the same as Lagrange
d;jk
multipliers λ , are calculated solely from current iteration u ; see com/polytechnik/kgo/L
ij jk
inearConstraints.java:getOrthogonalOffdiag0DiagEq for an implementation.
The result of the consideration above is: if, instead of a full basis v[s] of the dimension
jk
Dn, we take the basis V (60) that has rank(C ) = (D −1)(D +2)/2 fewer elements —
p d;jk
the variation of (62) will preserve partial unitarity of u within first order. This drastically
jk
changes algorithm convergence. It starts perfectly converging to a true solution if both sets
of the constraints (A7a) and (A7b) are satisfied; a single set does not provide convergence.
Iteration algorithm finding a partially unitary operator optimally converting an operator34
from IN Hilbert space to OUT Hilbert space (37) is the main result of this paper. The
result was achieved by considering on each iteration not a pair: approximation, Lagrange
multipliers: (u ,λ ), but a triple: approximation, Lagrange multipliers, homogeneous linear
jk ij
constraints: (u ,λ ,C ). This is the cost of working with a quadratically constrained
jk ij d;jk
degenerate problem having multiple local extremums.
A problem of quadratic form optimization (cid:80)D−1 u M u −→ max with a single quadratic
i,j=0 i ij j
u
constraint 1 = (cid:80)D−1 u Q u with a positively definite matrix Q can be reduced to an
i,j=0 i ij j ij
eigenvalue problem. In the Appendix F of [31] and later in [32] a quadratic form optimization
problem with two quadratic form constraints was considered. An extra constraint was in
the from 0 = (cid:80)D−1 u C u . This is a much simpler problem than the one considered in the
i,j=0 i ij j
current paper. For this problem an algorithm with vanilla Lagrange multipliers iterations
converges without extra linear constraints added, see com/polytechnik/utils/IstatesCon
ditionalV2.java for an implementation. However, even in this simple case, a number of
iteration starting values should be tried to find the global maximum. Numerical experiments
show that adding on every iteration a single linear constraint on u : 0 = (cid:80)D−1 u C u(cur),
j i,j=0 i ij j
where u(cur) is the value of u used to calculate current iteration Lagrange multipliers, greatly
j j
increases the chances to find the global maximum. A reference implementation com/polyte
chnik/utils/IstatesConditionalSubspaceLinearConstraints.java on every iteration
solves an eigenproblem of the dimension D−1 (a single constraint reduces the dimension
by 1) and always selects the maximal eigenvalue, this is similar to the heuristic used in the
algorithm above. The result is almost always better than that of IstatesConditionalV2.j
ava which solves an eigenproblem of the dimension D and tries a large number of vectors to
select the next iteration from. This shows an advantage of considering iteration state as a
triple (solution, Lagrange multipliers, linear constraints) even in a simple case of a single
additional quadratic constraint.
Appendix B: Software description
• Install java 22 or later.
• Download the latest version of the source code code_polynomials_quadratures.zip
from [28] or from alternative location.35
• Decompress and recompile the program. Run a simple test recovering orthogonal
matrices of the dimensions 3,5,7,17,40.
unzip code_polynomials_quadratures.zip
javac -g com/polytechnik/*/*java
java com/polytechnik/algorithms/PrintOrthogonalSeq\$TestAuto >/tmp/diag 2>&1
The diagnostics is saved to the file /tmp/diag
• Check the maximal absolute difference between elements of original and recovered
orthogonal matrices, do grep DIFF /tmp/diag
GRAM DIFF for dim=3 is 2.3314683517128287E-15
UNIT DIFF for dim=3 is 4.440892098500626E-16
GRAM DIFF for dim=5 is 6.439293542825908E-15
UNIT DIFF for dim=5 is 7.105427357601002E-15
GRAM DIFF for dim=7 is 5.064698660461886E-14
UNIT DIFF for dim=7 is 1.6431300764452317E-14
GRAM DIFF for dim=17 is 4.6851411639181606E-14
UNIT DIFF for dim=17 is 3.2807090377673376E-14
GRAM DIFF for dim=40 is 4.5630166312093934E-14
UNIT DIFF for dim=40 is 3.907985046680551E-14
Since the unitarity of test data is exact — both quantum channels: invariant Gram
matrix of Section IIC and invariant unit matrix of Section IID recover the operator
u exactly.
jk
[1] F. Rosenblatt, The perceptron: a probabilistic model for information storage and organization
in the brain., Psychological review 65, 386 (1958).
[2] V. Vapnik and A. Y. Chervonenkis, The method of ordered risk minimization, I, Avtomatika i
Telemekhanika 8, 21 (1974).
[3] P. H´ajek and T. Havr´anek, On generation of inductive hypotheses, International Journal of
Man-Machine Studies 9, 415 (1977).
[4] V. Vapnik, The nature of statistical learning theory (Springer science & business media, 2013).36
[5] I. H. Witten and E. Frank, Data mining: practical machine learning tools and techniques with
Java implementations, Acm Sigmod Record 31, 76 (2002).
[6] L. A. Zadeh, Fuzzy sets, Information and control 8, 338 (1965).
[7] P. Ha´jek, Fuzzy logic and arithmetical hierarchy, Fuzzy sets and Systems 73, 359 (1995).
[8] Y.Bengio,A.Courville,andP.Vincent,Representationlearning: Areviewandnewperspectives,
IEEE transactions on pattern analysis and machine intelligence 35, 1798 (2013).
[9] A.Bisio,G.Chiribella,G.M. D’Ariano,S.Facchini,andP.Perinotti,Optimalquantumlearning
of a unitary transformation, Physical Review A 81, 032324 (2010).
[10] M. Arjovsky, A. Shah, and Y. Bengio, Unitary evolution recurrent neural networks, in Interna-
tional conference on machine learning (PMLR, 2016) pp. 1120–1128.
[11] S. Hyland and G. R¨atsch, Learning unitary operators with help from u(n), in Proceedings of
the AAAI Conference on Artificial Intelligence, Vol. 31 (2017).
[12] M. Razavy, An introduction to inverse problems in physics (World Scientific, 2020).
[13] J. R. Johansson, P. D. Nation, and F. Nori, QuTiP: An open-source Python framework for the
dynamics of open quantum systems, Computer Physics Communications 183, 1760 (2012).
[14] S. B. Ramezani, A. Sommers, H. K. Manchukonda, S. Rahimi, and A. Amirlatifi, Machine
learning algorithms in quantum computing: A survey, in 2020 International joint conference on
neural networks (IJCNN) (IEEE, 2020) pp. 1–8.
[15] V. G. Malyshkin and M. G. Belov, Market Directional Information Derived From (Time,
Execution Price, Shares Traded) Sequence of Transactions. On The Impact From The Future,
arXiv preprint arXiv:2210.04223 10.48550/arXiv.2210.04223 (2022).
[16] B. T. Kiani, S. Lloyd, and R. Maity, Learning unitaries by gradient descent, arXiv preprint
arXiv:2001.11897 10.48550/arXiv.2001.11897 (2020).
[17] B. Kiani, R. Balestriero, Y. LeCun, and S. Lloyd, projUNN: efficient method for training deep
networks with unitary matrices, Advances in Neural Information Processing Systems 35, 14448
(2022).
[18] S. Lloyd and R. Maity, Efficient implementation of unitary transformations, arXiv preprint
arXiv:1901.03431 10.48550/arXiv.1901.03431 (2019).
[19] V. G. Malyshkin, On Machine Learning Knowledge Representation In The Form Of Par-
tially Unitary Operator. Knowledge Generalizing Operator, arXiv preprint arXiv:2212.14810
10.48550/arXiv.2212.14810 (2022).37
[20] V. G. Malyshkin, On The Radon-Nikodym Spectral Approach With Optimal Clustering, arXiv
preprint arXiv:1906.00460 10.48550/arXiv.1906.00460 (2019).
[21] T. A. Loring, Computing a logarithm of a unitary matrix with general spectrum, Numerical
Linear Algebra with Applications 21, 744 (2014).
[22] I. Najfeld and T. F. Havel, Derivatives of the matrix exponential and their computation,
Advances in applied mathematics 16, 321 (1995).
[23] A. Raza, Differentiating exponentials of Hamiltonians (2020), https://araza6.github.io/
posts/hamiltonian-differentiation/.
[24] A. Raza, Learning unitary matrices (2020), https://araza6.github.io/posts/
unitary-learning/.
[25] G. H. Golub, Some modified matrix eigenvalue problems, Siam Review 15, 318 (1973).
[26] G. Frison, J. Frey, F. Messerer, A. Zanelli, and M. Diehl, Introducing the quadratically-
constrainedquadraticprogrammingframeworkinHPIPM,in2022 European Control Conference
(ECC) (IEEE, 2022) pp. 447–453.
[27] V. G. Malyshkin and R. Bakhramov, Mathematical Foundations of Realtime Equity Trad-
ing. Liquidity Deficit and Market Dynamics. Automated Trading Machines, arXiv preprint
arXiv:1510.05510 10.48550/arXiv.1510.05510 (2015).
[28] V. G. Malyshkin, The code for polynomials calculation (2014), http://www.ioffe.ru/LNEPS/
malyshkin/code.html and an alternative location.
[29] K. Kraus, States, Effects, and Operations: Fundamental Notions of Quantum Theory, Lecture
Notes in Physics, Vol. 190 (Springer-Verlag, 1983) Lectures in Mathematical Physics at the
University of Texas at Austin.
[30] Autretech LLC, www.атретек.рф, is a resident of the Skolkovo Technopark. In May 2019, the
company became the winner of the Skolkovo Cybersecurity Challenge competition.
[31] V. G. Malyshkin, Market Dynamics: On Directional Information Derived From (Time,
Execution Price, Shares Traded) Transaction Sequences, arXiv preprint arXiv:1903.11530
10.48550/arXiv.1903.11530 (2019).
[32] L.Boudjemila,V.V. Davydov,andV.G. Malyshkin,OnQuadraticFormOptimizationProblem
With Multiple Constraints of the Quadratic Form Type, in The 5th International Conference
on Future Networks & Distributed Systems (2021) pp. 568–571.