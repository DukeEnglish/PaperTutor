[
    {
        "title": "Toon3D: Seeing Cartoons from a New Perspective",
        "authors": "Ethan WeberRiley PeterlinzRohan MathurFrederik WarburgAlexei A. EfrosAngjoo Kanazawa",
        "links": "http://arxiv.org/abs/2405.10320v1",
        "entry_id": "http://arxiv.org/abs/2405.10320v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10320v1",
        "summary": "In this work, we recover the underlying 3D structure of non-geometrically\nconsistent scenes. We focus our analysis on hand-drawn images from cartoons and\nanime. Many cartoons are created by artists without a 3D rendering engine,\nwhich means that any new image of a scene is hand-drawn. The hand-drawn images\nare usually faithful representations of the world, but only in a qualitative\nsense, since it is difficult for humans to draw multiple perspectives of an\nobject or scene 3D consistently. Nevertheless, people can easily perceive 3D\nscenes from inconsistent inputs! In this work, we correct for 2D drawing\ninconsistencies to recover a plausible 3D structure such that the newly warped\ndrawings are consistent with each other. Our pipeline consists of a\nuser-friendly annotation tool, camera pose estimation, and image deformation to\nrecover a dense structure. Our method warps images to obey a perspective camera\nmodel, enabling our aligned results to be plugged into novel-view synthesis\nreconstruction methods to experience cartoons from viewpoints never drawn\nbefore. Our project page is https://toon3d.studio/.",
        "updated": "2024-05-16 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10320v1"
    },
    {
        "title": "Text-to-Vector Generation with Neural Path Representation",
        "authors": "Peiying ZhangNanxuan ZhaoJing Liao",
        "links": "http://arxiv.org/abs/2405.10317v1",
        "entry_id": "http://arxiv.org/abs/2405.10317v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10317v1",
        "summary": "Vector graphics are widely used in digital art and highly favored by\ndesigners due to their scalability and layer-wise properties. However, the\nprocess of creating and editing vector graphics requires creativity and design\nexpertise, making it a time-consuming task. Recent advancements in\ntext-to-vector (T2V) generation have aimed to make this process more\naccessible. However, existing T2V methods directly optimize control points of\nvector graphics paths, often resulting in intersecting or jagged paths due to\nthe lack of geometry constraints. To overcome these limitations, we propose a\nnovel neural path representation by designing a dual-branch Variational\nAutoencoder (VAE) that learns the path latent space from both sequence and\nimage modalities. By optimizing the combination of neural paths, we can\nincorporate geometric constraints while preserving expressivity in generated\nSVGs. Furthermore, we introduce a two-stage path optimization method to improve\nthe visual and topological quality of generated SVGs. In the first stage, a\npre-trained text-to-image diffusion model guides the initial generation of\ncomplex vector graphics through the Variational Score Distillation (VSD)\nprocess. In the second stage, we refine the graphics using a layer-wise image\nvectorization strategy to achieve clearer elements and structure. We\ndemonstrate the effectiveness of our method through extensive experiments and\nshowcase various applications. The project page is\nhttps://intchous.github.io/T2V-NPR.",
        "updated": "2024-05-16 17:59:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10317v1"
    },
    {
        "title": "Analogist: Out-of-the-box Visual In-Context Learning with Image Diffusion Model",
        "authors": "Zheng GuShiyuan YangJing LiaoJing HuoYang Gao",
        "links": "http://arxiv.org/abs/2405.10316v1",
        "entry_id": "http://arxiv.org/abs/2405.10316v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10316v1",
        "summary": "Visual In-Context Learning (ICL) has emerged as a promising research area due\nto its capability to accomplish various tasks with limited example pairs\nthrough analogical reasoning. However, training-based visual ICL has\nlimitations in its ability to generalize to unseen tasks and requires the\ncollection of a diverse task dataset. On the other hand, existing methods in\nthe inference-based visual ICL category solely rely on textual prompts, which\nfail to capture fine-grained contextual information from given examples and can\nbe time-consuming when converting from images to text prompts. To address these\nchallenges, we propose Analogist, a novel inference-based visual ICL approach\nthat exploits both visual and textual prompting techniques using a\ntext-to-image diffusion model pretrained for image inpainting. For visual\nprompting, we propose a self-attention cloning (SAC) method to guide the\nfine-grained structural-level analogy between image examples. For textual\nprompting, we leverage GPT-4V's visual reasoning capability to efficiently\ngenerate text prompts and introduce a cross-attention masking (CAM) operation\nto enhance the accuracy of semantic-level analogy guided by text prompts. Our\nmethod is out-of-the-box and does not require fine-tuning or optimization. It\nis also generic and flexible, enabling a wide range of visual tasks to be\nperformed in an in-context manner. Extensive experiments demonstrate the\nsuperiority of our method over existing approaches, both qualitatively and\nquantitatively.",
        "updated": "2024-05-16 17:59:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10316v1"
    },
    {
        "title": "CAT3D: Create Anything in 3D with Multi-View Diffusion Models",
        "authors": "Ruiqi GaoAleksander HolynskiPhilipp HenzlerArthur BrusseeRicardo Martin-BruallaPratul SrinivasanJonathan T. BarronBen Poole",
        "links": "http://arxiv.org/abs/2405.10314v1",
        "entry_id": "http://arxiv.org/abs/2405.10314v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10314v1",
        "summary": "Advances in 3D reconstruction have enabled high-quality 3D capture, but\nrequire a user to collect hundreds to thousands of images to create a 3D scene.\nWe present CAT3D, a method for creating anything in 3D by simulating this\nreal-world capture process with a multi-view diffusion model. Given any number\nof input images and a set of target novel viewpoints, our model generates\nhighly consistent novel views of a scene. These generated views can be used as\ninput to robust 3D reconstruction techniques to produce 3D representations that\ncan be rendered from any viewpoint in real-time. CAT3D can create entire 3D\nscenes in as little as one minute, and outperforms existing methods for single\nimage and few-view 3D scene creation. See our project page for results and\ninteractive demos at https://cat3d.github.io .",
        "updated": "2024-05-16 17:59:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10314v1"
    },
    {
        "title": "4D Panoptic Scene Graph Generation",
        "authors": "Jingkang YangJun CenWenxuan PengShuai LiuFangzhou HongXiangtai LiKaiyang ZhouQifeng ChenZiwei Liu",
        "links": "http://arxiv.org/abs/2405.10305v1",
        "entry_id": "http://arxiv.org/abs/2405.10305v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10305v1",
        "summary": "We are living in a three-dimensional space while moving forward through a\nfourth dimension: time. To allow artificial intelligence to develop a\ncomprehensive understanding of such a 4D environment, we introduce 4D Panoptic\nScene Graph (PSG-4D), a new representation that bridges the raw visual data\nperceived in a dynamic 4D world and high-level visual understanding.\nSpecifically, PSG-4D abstracts rich 4D sensory data into nodes, which represent\nentities with precise location and status information, and edges, which capture\nthe temporal relations. To facilitate research in this new area, we build a\nrichly annotated PSG-4D dataset consisting of 3K RGB-D videos with a total of\n1M frames, each of which is labeled with 4D panoptic segmentation masks as well\nas fine-grained, dynamic scene graphs. To solve PSG-4D, we propose PSG4DFormer,\na Transformer-based model that can predict panoptic segmentation masks, track\nmasks along the time axis, and generate the corresponding scene graphs via a\nrelation component. Extensive experiments on the new dataset show that our\nmethod can serve as a strong baseline for future research on PSG-4D. In the\nend, we provide a real-world application example to demonstrate how we can\nachieve dynamic scene understanding by integrating a large language model into\nour PSG-4D system.",
        "updated": "2024-05-16 17:56:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10305v1"
    }
]