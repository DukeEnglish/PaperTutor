[
    {
        "title": "How Far Are We From AGI",
        "authors": "Tao FengChuanyang JinJingyu LiuKunlun ZhuHaoqin TuZirui ChengGuanyu LinJiaxuan You",
        "links": "http://arxiv.org/abs/2405.10313v1",
        "entry_id": "http://arxiv.org/abs/2405.10313v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10313v1",
        "summary": "The evolution of artificial intelligence (AI) has profoundly impacted human\nsociety, driving significant advancements in multiple sectors. Yet, the\nescalating demands on AI have highlighted the limitations of AI's current\nofferings, catalyzing a movement towards Artificial General Intelligence (AGI).\nAGI, distinguished by its ability to execute diverse real-world tasks with\nefficiency and effectiveness comparable to human intelligence, reflects a\nparamount milestone in AI evolution. While existing works have summarized\nspecific recent advancements of AI, they lack a comprehensive discussion of\nAGI's definitions, goals, and developmental trajectories. Different from\nexisting survey papers, this paper delves into the pivotal questions of our\nproximity to AGI and the strategies necessary for its realization through\nextensive surveys, discussions, and original perspectives. We start by\narticulating the requisite capability frameworks for AGI, integrating the\ninternal, interface, and system dimensions. As the realization of AGI requires\nmore advanced capabilities and adherence to stringent constraints, we further\ndiscuss necessary AGI alignment technologies to harmonize these factors.\nNotably, we emphasize the importance of approaching AGI responsibly by first\ndefining the key levels of AGI progression, followed by the evaluation\nframework that situates the status-quo, and finally giving our roadmap of how\nto reach the pinnacle of AGI. Moreover, to give tangible insights into the\nubiquitous impact of the integration of AI, we outline existing challenges and\npotential pathways toward AGI in multiple domains. In sum, serving as a\npioneering exploration into the current state and future trajectory of AGI,\nthis paper aims to foster a collective comprehension and catalyze broader\npublic discussions among researchers and practitioners on AGI.",
        "updated": "2024-05-16 17:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10313v1"
    },
    {
        "title": "Fine-Tuning Large Vision-Language Models as Decision-Making Agents via Reinforcement Learning",
        "authors": "Yuexiang ZhaiHao BaiZipeng LinJiayi PanShengbang TongYifei ZhouAlane SuhrSaining XieYann LeCunYi MaSergey Levine",
        "links": "http://arxiv.org/abs/2405.10292v1",
        "entry_id": "http://arxiv.org/abs/2405.10292v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10292v1",
        "summary": "Large vision-language models (VLMs) fine-tuned on specialized visual\ninstruction-following data have exhibited impressive language reasoning\ncapabilities across various scenarios. However, this fine-tuning paradigm may\nnot be able to efficiently learn optimal decision-making agents in multi-step\ngoal-directed tasks from interactive environments. To address this challenge,\nwe propose an algorithmic framework that fine-tunes VLMs with reinforcement\nlearning (RL). Specifically, our framework provides a task description and then\nprompts the VLM to generate chain-of-thought (CoT) reasoning, enabling the VLM\nto efficiently explore intermediate reasoning steps that lead to the final\ntext-based action. Next, the open-ended text output is parsed into an\nexecutable action to interact with the environment to obtain goal-directed task\nrewards. Finally, our framework uses these task rewards to fine-tune the entire\nVLM with RL. Empirically, we demonstrate that our proposed framework enhances\nthe decision-making capabilities of VLM agents across various tasks, enabling\n7b models to outperform commercial models such as GPT4-V or Gemini.\nFurthermore, we find that CoT reasoning is a crucial component for performance\nimprovement, as removing the CoT reasoning results in a significant decrease in\nthe overall performance of our method.",
        "updated": "2024-05-16 17:50:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10292v1"
    },
    {
        "title": "Timeline-based Sentence Decomposition with In-Context Learning for Temporal Fact Extraction",
        "authors": "Jianhao ChenHaoyuan OuyangJunyang RenWentao DingWei HuYuzhong Qu",
        "links": "http://arxiv.org/abs/2405.10288v1",
        "entry_id": "http://arxiv.org/abs/2405.10288v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10288v1",
        "summary": "Facts extraction is pivotal for constructing knowledge graphs. Recently, the\nincreasing demand for temporal facts in downstream tasks has led to the\nemergence of the task of temporal fact extraction. In this paper, we\nspecifically address the extraction of temporal facts from natural language\ntext. Previous studies fail to handle the challenge of establishing\ntime-to-fact correspondences in complex sentences. To overcome this hurdle, we\npropose a timeline-based sentence decomposition strategy using large language\nmodels (LLMs) with in-context learning, ensuring a fine-grained understanding\nof the timeline associated with various facts. In addition, we evaluate the\nperformance of LLMs for direct temporal fact extraction and get unsatisfactory\nresults. To this end, we introduce TSDRE, a method that incorporates the\ndecomposition capabilities of LLMs into the traditional fine-tuning of smaller\npre-trained language models (PLMs). To support the evaluation, we construct\nComplexTRED, a complex temporal fact extraction dataset. Our experiments show\nthat TSDRE achieves state-of-the-art results on both HyperRED-Temporal and\nComplexTRED datasets.",
        "updated": "2024-05-16 17:48:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10288v1"
    },
    {
        "title": "Revisiting OPRO: The Limitations of Small-Scale LLMs as Optimizers",
        "authors": "Tuo ZhangJinyue YuanSalman Avestimehr",
        "links": "http://arxiv.org/abs/2405.10276v1",
        "entry_id": "http://arxiv.org/abs/2405.10276v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10276v1",
        "summary": "Numerous recent works aim to enhance the efficacy of Large Language Models\n(LLMs) through strategic prompting. In particular, the Optimization by\nPROmpting (OPRO) approach provides state-of-the-art performance by leveraging\nLLMs as optimizers where the optimization task is to find instructions that\nmaximize the task accuracy. In this paper, we revisit OPRO for automated\nprompting with relatively small-scale LLMs, such as LLaMa-2 family and Mistral\n7B. Our investigation reveals that OPRO shows limited effectiveness in\nsmall-scale LLMs, with limited inference capabilities constraining optimization\nability. We suggest future automatic prompting engineering to consider both\nmodel capabilities and computational costs. Additionally, for small-scale LLMs,\nwe recommend direct instructions that clearly outline objectives and\nmethodologies as robust prompt baselines, ensuring efficient and effective\nprompt engineering in ongoing research.",
        "updated": "2024-05-16 17:33:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10276v1"
    },
    {
        "title": "A Tale of Two Languages: Large-Vocabulary Continuous Sign Language Recognition from Spoken Language Supervision",
        "authors": "Charles RaudeK R PrajwalLiliane MomeniHannah BullSamuel AlbanieAndrew ZissermanGül Varol",
        "links": "http://arxiv.org/abs/2405.10266v1",
        "entry_id": "http://arxiv.org/abs/2405.10266v1",
        "pdf_url": "http://arxiv.org/pdf/2405.10266v1",
        "summary": "In this work, our goals are two fold: large-vocabulary continuous sign\nlanguage recognition (CSLR), and sign language retrieval. To this end, we\nintroduce a multi-task Transformer model, CSLR2, that is able to ingest a\nsigning sequence and output in a joint embedding space between signed language\nand spoken language text. To enable CSLR evaluation in the large-vocabulary\nsetting, we introduce new dataset annotations that have been manually\ncollected. These provide continuous sign-level annotations for six hours of\ntest videos, and will be made publicly available. We demonstrate that by a\ncareful choice of loss functions, training the model for both the CSLR and\nretrieval tasks is mutually beneficial in terms of performance -- retrieval\nimproves CSLR performance by providing context, while CSLR improves retrieval\nwith more fine-grained supervision. We further show the benefits of leveraging\nweak and noisy supervision from large-vocabulary datasets such as BOBSL, namely\nsign-level pseudo-labels, and English subtitles. Our model significantly\noutperforms the previous state of the art on both tasks.",
        "updated": "2024-05-16 17:19:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.10266v1"
    }
]