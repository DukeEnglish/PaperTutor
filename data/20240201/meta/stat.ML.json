[
    {
        "title": "Effect of Weight Quantization on Learning Models by Typical Case Analysis",
        "authors": "Shuhei KashiwamuraAyaka SakataMasaaki Imaizumi",
        "links": "http://arxiv.org/abs/2401.17269v1",
        "entry_id": "http://arxiv.org/abs/2401.17269v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17269v1",
        "summary": "This paper examines the quantization methods used in large-scale data\nanalysis models and their hyperparameter choices. The recent surge in data\nanalysis scale has significantly increased computational resource requirements.\nTo address this, quantizing model weights has become a prevalent practice in\ndata analysis applications such as deep learning. Quantization is particularly\nvital for deploying large models on devices with limited computational\nresources. However, the selection of quantization hyperparameters, like the\nnumber of bits and value range for weight quantization, remains an\nunderexplored area. In this study, we employ the typical case analysis from\nstatistical physics, specifically the replica method, to explore the impact of\nhyperparameters on the quantization of simple learning models. Our analysis\nyields three key findings: (i) an unstable hyperparameter phase, known as\nreplica symmetry breaking, occurs with a small number of bits and a large\nquantization width; (ii) there is an optimal quantization width that minimizes\nerror; and (iii) quantization delays the onset of overparameterization, helping\nto mitigate overfitting as indicated by the double descent phenomenon. We also\ndiscover that non-uniform quantization can enhance stability. Additionally, we\ndevelop an approximate message-passing algorithm to validate our theoretical\nresults.",
        "updated": "2024-01-30 18:58:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17269v1"
    },
    {
        "title": "Adaptive Experiment Design with Synthetic Controls",
        "authors": "Alihan HüyükZhaozhi QianMihaela van der Schaar",
        "links": "http://arxiv.org/abs/2401.17205v1",
        "entry_id": "http://arxiv.org/abs/2401.17205v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17205v1",
        "summary": "Clinical trials are typically run in order to understand the effects of a new\ntreatment on a given population of patients. However, patients in large\npopulations rarely respond the same way to the same treatment. This\nheterogeneity in patient responses necessitates trials that investigate effects\non multiple subpopulations - especially when a treatment has marginal or no\nbenefit for the overall population but might have significant benefit for a\nparticular subpopulation. Motivated by this need, we propose Syntax, an\nexploratory trial design that identifies subpopulations with positive treatment\neffect among many subpopulations. Syntax is sample efficient as it (i) recruits\nand allocates patients adaptively and (ii) estimates treatment effects by\nforming synthetic controls for each subpopulation that combines control samples\nfrom other subpopulations. We validate the performance of Syntax and provide\ninsights into when it might have an advantage over conventional trial designs\nthrough experiments.",
        "updated": "2024-01-30 17:45:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17205v1"
    },
    {
        "title": "Dynamical Survival Analysis with Controlled Latent States",
        "authors": "Linus BleisteinVan-Tuan NguyenAdeline FermanianAgathe Guilloux",
        "links": "http://arxiv.org/abs/2401.17077v1",
        "entry_id": "http://arxiv.org/abs/2401.17077v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17077v1",
        "summary": "We consider the task of learning individual-specific intensities of counting\nprocesses from a set of static variables and irregularly sampled time series.\nWe introduce a novel modelization approach in which the intensity is the\nsolution to a controlled differential equation. We first design a neural\nestimator by building on neural controlled differential equations. In a second\ntime, we show that our model can be linearized in the signature space under\nsufficient regularity conditions, yielding a signature-based estimator which we\ncall CoxSig. We provide theoretical learning guarantees for both estimators,\nbefore showcasing the performance of our models on a vast array of simulated\nand real-world datasets from finance, predictive maintenance and food supply\nchain management.",
        "updated": "2024-01-30 14:57:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17077v1"
    },
    {
        "title": "Gower's similarity coefficients with automatic weight selection",
        "authors": "Marcello D'Orazio",
        "links": "http://arxiv.org/abs/2401.17041v1",
        "entry_id": "http://arxiv.org/abs/2401.17041v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17041v1",
        "summary": "Nearest-neighbor methods have become popular in statistics and play a key\nrole in statistical learning. Important decisions in nearest-neighbor methods\nconcern the variables to use (when many potential candidates exist) and how to\nmeasure the dissimilarity between units. The first decision depends on the\nscope of the application while second depends mainly on the type of variables.\nUnfortunately, relatively few options permit to handle mixed-type variables, a\nsituation frequently encountered in practical applications. The most popular\ndissimilarity for mixed-type variables is derived as the complement to one of\nthe Gower's similarity coefficient. It is appealing because ranges between 0\nand 1, being an average of the scaled dissimilarities calculated variable by\nvariable, handles missing values and allows for a user-defined weighting scheme\nwhen averaging dissimilarities. The discussion on the weighting schemes is\nsometimes misleading since it often ignores that the unweighted \"standard\"\nsetting hides an unbalanced contribution of the single variables to the overall\ndissimilarity. We address this drawback following the recent idea of\nintroducing a weighting scheme that minimizes the differences in the\ncorrelation between each contributing dissimilarity and the resulting weighted\nGower's dissimilarity. In particular, this note proposes different approaches\nfor measuring the correlation depending on the type of variables. The\nperformances of the proposed approaches are evaluated in simulation studies\nrelated to classification and imputation of missing values.",
        "updated": "2024-01-30 14:21:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17041v1"
    },
    {
        "title": "Bayesian Optimization with Noise-Free Observations: Improved Regret Bounds via Random Exploration",
        "authors": "Hwanwoo KimDaniel Sanz-Alonso",
        "links": "http://arxiv.org/abs/2401.17037v1",
        "entry_id": "http://arxiv.org/abs/2401.17037v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17037v1",
        "summary": "This paper studies Bayesian optimization with noise-free observations. We\nintroduce new algorithms rooted in scattered data approximation that rely on a\nrandom exploration step to ensure that the fill-distance of query points decays\nat a near-optimal rate. Our algorithms retain the ease of implementation of the\nclassical GP-UCB algorithm and satisfy cumulative regret bounds that nearly\nmatch those conjectured in arXiv:2002.05096, hence solving a COLT open problem.\nFurthermore, the new algorithms outperform GP-UCB and other popular Bayesian\noptimization strategies in several examples.",
        "updated": "2024-01-30 14:16:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17037v1"
    }
]