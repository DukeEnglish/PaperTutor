[
    {
        "title": "Weaver: Foundation Models for Creative Writing",
        "authors": "Tiannan WangJiamin ChenQingrui JiaShuai WangRuoyu FangHuilin WangZhaowei GaoChunzhao XieChuou XuJihong DaiYibin LiuJialong WuShengwei DingLong LiZhiwei HuangXinle DengTeng YuGangan MaHan XiaoZixin ChenDanjun XiangYunxia WangYuanyuan ZhuYi XiaoJing WangYiru WangSiran DingJiayang HuangJiayi XuYilihamu TayierZhenyu HuYuan GaoChengfeng ZhengYueshu YeYihang LiLei WanXinyue JiangYujie WangSiyu ChengZhule SongXiangru TangXiaohua XuNingyu ZhangHuajun ChenYuchen Eleanor JiangWangchunshu Zhou",
        "links": "http://arxiv.org/abs/2401.17268v1",
        "entry_id": "http://arxiv.org/abs/2401.17268v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17268v1",
        "summary": "This work introduces Weaver, our first family of large language models (LLMs)\ndedicated to content creation. Weaver is pre-trained on a carefully selected\ncorpus that focuses on improving the writing capabilities of large language\nmodels. We then fine-tune Weaver for creative and professional writing purposes\nand align it to the preference of professional writers using a suit of novel\nmethods for instruction data synthesis and LLM alignment, making it able to\nproduce more human-like texts and follow more diverse instructions for content\ncreation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver\nBase (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for\ndifferent applications and can be dynamically dispatched by a routing agent\naccording to query complexity to balance response quality and computation cost.\nEvaluation on a carefully curated benchmark for assessing the writing\ncapabilities of LLMs shows Weaver models of all sizes outperform generalist\nLLMs several times larger than them. Notably, our most-capable Weaver Ultra\nmodel surpasses GPT-4, a state-of-the-art generalist LLM, on various writing\nscenarios, demonstrating the advantage of training specialized LLMs for writing\npurposes. Moreover, Weaver natively supports retrieval-augmented generation\n(RAG) and function calling (tool usage). We present various use cases of these\nabilities for improving AI-assisted writing systems, including integration of\nexternal knowledge bases, tools, or APIs, and providing personalized writing\nassistance. Furthermore, we discuss and summarize a guideline and best\npractices for pre-training and fine-tuning domain-specific LLMs.",
        "updated": "2024-01-30 18:58:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17268v1"
    },
    {
        "title": "Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks",
        "authors": "Andy ZhouBo LiHaohan Wang",
        "links": "http://arxiv.org/abs/2401.17263v1",
        "entry_id": "http://arxiv.org/abs/2401.17263v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17263v1",
        "summary": "Despite advances in AI alignment, language models (LM) remain vulnerable to\nadversarial attacks or jailbreaking, in which adversaries modify input prompts\nto induce harmful behavior. While some defenses have been proposed, they focus\non narrow threat models and fall short of a strong defense, which we posit\nshould be effective, universal, and practical. To achieve this, we propose the\nfirst adversarial objective for defending LMs against jailbreaking attacks and\nan algorithm, robust prompt optimization (RPO), that uses gradient-based token\noptimization to enforce harmless outputs. This results in an easily accessible\nsuffix that significantly improves robustness to both jailbreaks seen during\noptimization and unknown, held-out jailbreaks, reducing the attack success rate\non Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find\nthat RPO has a minor effect on normal LM use, is successful under adaptive\nattacks, and can transfer to black-box models, reducing the success rate of the\nstrongest attack on GPT-4 from 92% to 6%.",
        "updated": "2024-01-30 18:56:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17263v1"
    },
    {
        "title": "Weak-to-Strong Jailbreaking on Large Language Models",
        "authors": "Xuandong ZhaoXianjun YangTianyu PangChao DuLei LiYu-Xiang WangWilliam Yang Wang",
        "links": "http://arxiv.org/abs/2401.17256v1",
        "entry_id": "http://arxiv.org/abs/2401.17256v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17256v1",
        "summary": "Although significant efforts have been dedicated to aligning large language\nmodels (LLMs), red-teaming reports suggest that these carefully aligned LLMs\ncould still be jailbroken through adversarial prompts, tuning, or decoding.\nUpon examining the jailbreaking vulnerability of aligned LLMs, we observe that\nthe decoding distributions of jailbroken and aligned models differ only in the\ninitial generations. This observation motivates us to propose the\nweak-to-strong jailbreaking attack, where adversaries can utilize smaller\nunsafe/aligned LLMs (e.g., 7B) to guide jailbreaking against significantly\nlarger aligned LLMs (e.g., 70B). To jailbreak, one only needs to additionally\ndecode two smaller LLMs once, which involves minimal computation and latency\ncompared to decoding the larger LLMs. The efficacy of this attack is\ndemonstrated through experiments conducted on five models from three different\norganizations. Our study reveals a previously unnoticed yet efficient way of\njailbreaking, exposing an urgent safety issue that needs to be considered when\naligning LLMs. As an initial attempt, we propose a defense strategy to protect\nagainst such attacks, but creating more advanced defenses remains challenging.\nThe code for replicating the method is available at\nhttps://github.com/XuandongZhao/weak-to-strong",
        "updated": "2024-01-30 18:48:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17256v1"
    },
    {
        "title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation",
        "authors": "Yuan ChiangChia-Hong ChouJanosh Riebesell",
        "links": "http://arxiv.org/abs/2401.17244v1",
        "entry_id": "http://arxiv.org/abs/2401.17244v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17244v1",
        "summary": "Reducing hallucination of Large Language Models (LLMs) is imperative for use\nin the sciences where reproducibility is crucial. However, LLMs inherently lack\nlong-term memory, making it a nontrivial, ad hoc, and inevitably biased task to\nfine-tune them on domain-specific literature and data. Here we introduce LLaMP,\na multimodal retrieval-augmented generation (RAG) framework of multiple\ndata-aware reasoning-and-acting (ReAct) agents that dynamically interact with\ncomputational and experimental data on Materials Project (MP). Without\nfine-tuning, LLaMP demonstrates an ability to comprehend and integrate various\nmodalities of materials science concepts, fetch relevant data stores on the\nfly, process higher-order data (such as crystal structures and elastic\ntensors), and summarize multi-step procedures for solid-state synthesis. We\nshow that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge,\nreducing a 5.21% MAPE on frequently-documented bandgaps and a significant\n1103.54% MAPE on formation energies -- errors that GPT-3.5 seems to derive from\nmixed data sources. Additionally, LLaMP substantially reduces the hallucinated\nvolumetric strain in a diamond cubic silicon structure from 66.3% to 0. The\nproposed framework offers an intuitive and nearly hallucination-free approach\nto exploring materials informatics and establishes a pathway for knowledge\ndistillation and fine-tuning other language models. We envision the framework\nas a valuable component for scientific hypotheses and a foundation for future\nautonomous laboratories where multiple LLM agents communicate and cooperate\nwith robotics to drive material synthesis and chemical reactions without\nhard-coded human logic and intervention.",
        "updated": "2024-01-30 18:37:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17244v1"
    },
    {
        "title": "Morality is Non-Binary: Building a Pluralist Moral Sentence Embedding Space using Contrastive Learning",
        "authors": "Jeongwoo ParkEnrico LiscioPradeep K. Murukannaiah",
        "links": "http://arxiv.org/abs/2401.17228v1",
        "entry_id": "http://arxiv.org/abs/2401.17228v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17228v1",
        "summary": "Recent advances in NLP show that language models retain a discernible level\nof knowledge in deontological ethics and moral norms. However, existing works\noften treat morality as binary, ranging from right to wrong. This simplistic\nview does not capture the nuances of moral judgment. Pluralist moral\nphilosophers argue that human morality can be deconstructed into a finite\nnumber of elements, respecting individual differences in moral judgment. In\nline with this view, we build a pluralist moral sentence embedding space via a\nstate-of-the-art contrastive learning approach. We systematically investigate\nthe embedding space by studying the emergence of relationships among moral\nelements, both quantitatively and qualitatively. Our results show that a\npluralist approach to morality can be captured in an embedding space. However,\nmoral pluralism is challenging to deduce via self-supervision alone and\nrequires a supervised approach with human labels.",
        "updated": "2024-01-30 18:15:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17228v1"
    }
]