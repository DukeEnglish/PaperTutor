[
    {
        "title": "A simple, strong baseline for building damage detection on the xBD dataset",
        "authors": "Sebastian GerardPaul Borne-PonsJosephine Sullivan",
        "links": "http://arxiv.org/abs/2401.17271v1",
        "entry_id": "http://arxiv.org/abs/2401.17271v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17271v1",
        "summary": "We construct a strong baseline method for building damage detection by\nstarting with the highly-engineered winning solution of the xView2 competition,\nand gradually stripping away components. This way, we obtain a much simpler\nmethod, while retaining adequate performance. We expect the simplified solution\nto be more widely and easily applicable. This expectation is based on the\nreduced complexity, as well as the fact that we choose hyperparameters based on\nsimple heuristics, that transfer to other datasets. We then re-arrange the\nxView2 dataset splits such that the test locations are not seen during\ntraining, contrary to the competition setup. In this setting, we find that both\nthe complex and the simplified model fail to generalize to unseen locations.\nAnalyzing the dataset indicates that this failure to generalize is not only a\nmodel-based problem, but that the difficulty might also be influenced by the\nunequal class distributions between events.\n  Code, including the baseline model, is available under\nhttps://github.com/PaulBorneP/Xview2_Strong_Baseline",
        "updated": "2024-01-30 18:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17271v1"
    },
    {
        "title": "YOLO-World: Real-Time Open-Vocabulary Object Detection",
        "authors": "Tianheng ChengLin SongYixiao GeWenyu LiuXinggang WangYing Shan",
        "links": "http://arxiv.org/abs/2401.17270v1",
        "entry_id": "http://arxiv.org/abs/2401.17270v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17270v1",
        "summary": "The You Only Look Once (YOLO) series of detectors have established themselves\nas efficient and practical tools. However, their reliance on predefined and\ntrained object categories limits their applicability in open scenarios.\nAddressing this limitation, we introduce YOLO-World, an innovative approach\nthat enhances YOLO with open-vocabulary detection capabilities through\nvision-language modeling and pre-training on large-scale datasets.\nSpecifically, we propose a new Re-parameterizable Vision-Language Path\nAggregation Network (RepVL-PAN) and region-text contrastive loss to facilitate\nthe interaction between visual and linguistic information. Our method excels in\ndetecting a wide range of objects in a zero-shot manner with high efficiency.\nOn the challenging LVIS dataset, YOLO-World achieves 35.4 AP with 52.0 FPS on\nV100, which outperforms many state-of-the-art methods in terms of both accuracy\nand speed. Furthermore, the fine-tuned YOLO-World achieves remarkable\nperformance on several downstream tasks, including object detection and\nopen-vocabulary instance segmentation.",
        "updated": "2024-01-30 18:59:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17270v1"
    },
    {
        "title": "Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks",
        "authors": "Andy ZhouBo LiHaohan Wang",
        "links": "http://arxiv.org/abs/2401.17263v1",
        "entry_id": "http://arxiv.org/abs/2401.17263v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17263v1",
        "summary": "Despite advances in AI alignment, language models (LM) remain vulnerable to\nadversarial attacks or jailbreaking, in which adversaries modify input prompts\nto induce harmful behavior. While some defenses have been proposed, they focus\non narrow threat models and fall short of a strong defense, which we posit\nshould be effective, universal, and practical. To achieve this, we propose the\nfirst adversarial objective for defending LMs against jailbreaking attacks and\nan algorithm, robust prompt optimization (RPO), that uses gradient-based token\noptimization to enforce harmless outputs. This results in an easily accessible\nsuffix that significantly improves robustness to both jailbreaks seen during\noptimization and unknown, held-out jailbreaks, reducing the attack success rate\non Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find\nthat RPO has a minor effect on normal LM use, is successful under adaptive\nattacks, and can transfer to black-box models, reducing the success rate of the\nstrongest attack on GPT-4 from 92% to 6%.",
        "updated": "2024-01-30 18:56:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17263v1"
    },
    {
        "title": "You Only Need One Step: Fast Super-Resolution with Stable Diffusion via Scale Distillation",
        "authors": "Mehdi NorooziIsma HadjiBrais MartinezAdrian BulatGeorgios Tzimiropoulos",
        "links": "http://arxiv.org/abs/2401.17258v1",
        "entry_id": "http://arxiv.org/abs/2401.17258v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17258v1",
        "summary": "In this paper, we introduce YONOS-SR, a novel stable diffusion-based approach\nfor image super-resolution that yields state-of-the-art results using only a\nsingle DDIM step. We propose a novel scale distillation approach to train our\nSR model. Instead of directly training our SR model on the scale factor of\ninterest, we start by training a teacher model on a smaller magnification\nscale, thereby making the SR problem simpler for the teacher. We then train a\nstudent model for a higher magnification scale, using the predictions of the\nteacher as a target during the training. This process is repeated iteratively\nuntil we reach the target scale factor of the final model. The rationale behind\nour scale distillation is that the teacher aids the student diffusion model\ntraining by i) providing a target adapted to the current noise level rather\nthan using the same target coming from ground truth data for all noise levels\nand ii) providing an accurate target as the teacher has a simpler task to\nsolve. We empirically show that the distilled model significantly outperforms\nthe model trained for high scales directly, specifically with few steps during\ninference. Having a strong diffusion model that requires only one step allows\nus to freeze the U-Net and fine-tune the decoder on top of it. We show that the\ncombination of spatially distilled U-Net and fine-tuned decoder outperforms\nstate-of-the-art methods requiring 200 steps with only one single step.",
        "updated": "2024-01-30 18:49:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17258v1"
    },
    {
        "title": "SLIC: A Learned Image Codec Using Structure and Color",
        "authors": "Srivatsa PrativadibhayankaramMahadev Prasad PandaThomas RichterHeiko SparenbergSiegfried FößelAndré Kaup",
        "links": "http://arxiv.org/abs/2401.17246v1",
        "entry_id": "http://arxiv.org/abs/2401.17246v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17246v1",
        "summary": "We propose the structure and color based learned image codec (SLIC) in which\nthe task of compression is split into that of luminance and chrominance. The\ndeep learning model is built with a novel multi-scale architecture for Y and UV\nchannels in the encoder, where the features from various stages are combined to\nobtain the latent representation. An autoregressive context model is employed\nfor backward adaptation and a hyperprior block for forward adaptation. Various\nexperiments are carried out to study and analyze the performance of the\nproposed model, and to compare it with other image codecs. We also illustrate\nthe advantages of our method through the visualization of channel impulse\nresponses, latent channels and various ablation studies. The model achieves\nBj{\\o}ntegaard delta bitrate gains of 7.5% and 4.66% in terms of MS-SSIM and\nCIEDE2000 metrics with respect to other state-of-the-art reference codecs.",
        "updated": "2024-01-30 18:39:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17246v1"
    }
]