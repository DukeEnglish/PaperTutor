[
    {
        "title": "Weaver: Foundation Models for Creative Writing",
        "authors": "Tiannan WangJiamin ChenQingrui JiaShuai WangRuoyu FangHuilin WangZhaowei GaoChunzhao XieChuou XuJihong DaiYibin LiuJialong WuShengwei DingLong LiZhiwei HuangXinle DengTeng YuGangan MaHan XiaoZixin ChenDanjun XiangYunxia WangYuanyuan ZhuYi XiaoJing WangYiru WangSiran DingJiayang HuangJiayi XuYilihamu TayierZhenyu HuYuan GaoChengfeng ZhengYueshu YeYihang LiLei WanXinyue JiangYujie WangSiyu ChengZhule SongXiangru TangXiaohua XuNingyu ZhangHuajun ChenYuchen Eleanor JiangWangchunshu Zhou",
        "links": "http://arxiv.org/abs/2401.17268v1",
        "entry_id": "http://arxiv.org/abs/2401.17268v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17268v1",
        "summary": "This work introduces Weaver, our first family of large language models (LLMs)\ndedicated to content creation. Weaver is pre-trained on a carefully selected\ncorpus that focuses on improving the writing capabilities of large language\nmodels. We then fine-tune Weaver for creative and professional writing purposes\nand align it to the preference of professional writers using a suit of novel\nmethods for instruction data synthesis and LLM alignment, making it able to\nproduce more human-like texts and follow more diverse instructions for content\ncreation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver\nBase (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for\ndifferent applications and can be dynamically dispatched by a routing agent\naccording to query complexity to balance response quality and computation cost.\nEvaluation on a carefully curated benchmark for assessing the writing\ncapabilities of LLMs shows Weaver models of all sizes outperform generalist\nLLMs several times larger than them. Notably, our most-capable Weaver Ultra\nmodel surpasses GPT-4, a state-of-the-art generalist LLM, on various writing\nscenarios, demonstrating the advantage of training specialized LLMs for writing\npurposes. Moreover, Weaver natively supports retrieval-augmented generation\n(RAG) and function calling (tool usage). We present various use cases of these\nabilities for improving AI-assisted writing systems, including integration of\nexternal knowledge bases, tools, or APIs, and providing personalized writing\nassistance. Furthermore, we discuss and summarize a guideline and best\npractices for pre-training and fine-tuning domain-specific LLMs.",
        "updated": "2024-01-30 18:58:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17268v1"
    },
    {
        "title": "Proactive Detection of Voice Cloning with Localized Watermarking",
        "authors": "Robin San RomanPierre FernandezAlexandre DéfossezTeddy FuronTuan TranHady Elsahar",
        "links": "http://arxiv.org/abs/2401.17264v1",
        "entry_id": "http://arxiv.org/abs/2401.17264v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17264v1",
        "summary": "In the rapidly evolving field of speech generative models, there is a\npressing need to ensure audio authenticity against the risks of voice cloning.\nWe present AudioSeal, the first audio watermarking technique designed\nspecifically for localized detection of AI-generated speech. AudioSeal employs\na generator/detector architecture trained jointly with a localization loss to\nenable localized watermark detection up to the sample level, and a novel\nperceptual loss inspired by auditory masking, that enables AudioSeal to achieve\nbetter imperceptibility. AudioSeal achieves state-of-the-art performance in\nterms of robustness to real life audio manipulations and imperceptibility based\non automatic and human evaluation metrics. Additionally, AudioSeal is designed\nwith a fast, single-pass detector, that significantly surpasses existing models\nin speed - achieving detection up to two orders of magnitude faster, making it\nideal for large-scale and real-time applications.",
        "updated": "2024-01-30 18:56:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17264v1"
    },
    {
        "title": "Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks",
        "authors": "Andy ZhouBo LiHaohan Wang",
        "links": "http://arxiv.org/abs/2401.17263v1",
        "entry_id": "http://arxiv.org/abs/2401.17263v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17263v1",
        "summary": "Despite advances in AI alignment, language models (LM) remain vulnerable to\nadversarial attacks or jailbreaking, in which adversaries modify input prompts\nto induce harmful behavior. While some defenses have been proposed, they focus\non narrow threat models and fall short of a strong defense, which we posit\nshould be effective, universal, and practical. To achieve this, we propose the\nfirst adversarial objective for defending LMs against jailbreaking attacks and\nan algorithm, robust prompt optimization (RPO), that uses gradient-based token\noptimization to enforce harmless outputs. This results in an easily accessible\nsuffix that significantly improves robustness to both jailbreaks seen during\noptimization and unknown, held-out jailbreaks, reducing the attack success rate\non Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find\nthat RPO has a minor effect on normal LM use, is successful under adaptive\nattacks, and can transfer to black-box models, reducing the success rate of the\nstrongest attack on GPT-4 from 92% to 6%.",
        "updated": "2024-01-30 18:56:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17263v1"
    },
    {
        "title": "LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation",
        "authors": "Yuan ChiangChia-Hong ChouJanosh Riebesell",
        "links": "http://arxiv.org/abs/2401.17244v1",
        "entry_id": "http://arxiv.org/abs/2401.17244v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17244v1",
        "summary": "Reducing hallucination of Large Language Models (LLMs) is imperative for use\nin the sciences where reproducibility is crucial. However, LLMs inherently lack\nlong-term memory, making it a nontrivial, ad hoc, and inevitably biased task to\nfine-tune them on domain-specific literature and data. Here we introduce LLaMP,\na multimodal retrieval-augmented generation (RAG) framework of multiple\ndata-aware reasoning-and-acting (ReAct) agents that dynamically interact with\ncomputational and experimental data on Materials Project (MP). Without\nfine-tuning, LLaMP demonstrates an ability to comprehend and integrate various\nmodalities of materials science concepts, fetch relevant data stores on the\nfly, process higher-order data (such as crystal structures and elastic\ntensors), and summarize multi-step procedures for solid-state synthesis. We\nshow that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge,\nreducing a 5.21% MAPE on frequently-documented bandgaps and a significant\n1103.54% MAPE on formation energies -- errors that GPT-3.5 seems to derive from\nmixed data sources. Additionally, LLaMP substantially reduces the hallucinated\nvolumetric strain in a diamond cubic silicon structure from 66.3% to 0. The\nproposed framework offers an intuitive and nearly hallucination-free approach\nto exploring materials informatics and establishes a pathway for knowledge\ndistillation and fine-tuning other language models. We envision the framework\nas a valuable component for scientific hypotheses and a foundation for future\nautonomous laboratories where multiple LLM agents communicate and cooperate\nwith robotics to drive material synthesis and chemical reactions without\nhard-coded human logic and intervention.",
        "updated": "2024-01-30 18:37:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17244v1"
    },
    {
        "title": "ESPnet-SPK: full pipeline speaker embedding toolkit with reproducible recipes, self-supervised front-ends, and off-the-shelf models",
        "authors": "Jee-weon JungWangyou ZhangJiatong ShiZakaria AldenehTakuya HiguchiBarry-John TheobaldAhmed Hussen AbdelazizShinji Watanabe",
        "links": "http://arxiv.org/abs/2401.17230v1",
        "entry_id": "http://arxiv.org/abs/2401.17230v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17230v1",
        "summary": "This paper introduces ESPnet-SPK, a toolkit designed with several objectives\nfor training speaker embedding extractors. First, we provide an open-source\nplatform for researchers in the speaker recognition community to effortlessly\nbuild models. We provide several models, ranging from x-vector to recent\nSKA-TDNN. Through the modularized architecture design, variants can be\ndeveloped easily. We also aspire to bridge developed models with other domains,\nfacilitating the broad research community to effortlessly incorporate\nstate-of-the-art embedding extractors. Pre-trained embedding extractors can be\naccessed in an off-the-shelf manner and we demonstrate the toolkit's\nversatility by showcasing its integration with two tasks. Another goal is to\nintegrate with diverse self-supervised learning features. We release a\nreproducible recipe that achieves an equal error rate of 0.39% on the Vox1-O\nevaluation protocol using WavLM-Large with ECAPA-TDNN.",
        "updated": "2024-01-30 18:18:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17230v1"
    }
]