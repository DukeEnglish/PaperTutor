[
    {
        "title": "Effect of Weight Quantization on Learning Models by Typical Case Analysis",
        "authors": "Shuhei KashiwamuraAyaka SakataMasaaki Imaizumi",
        "links": "http://arxiv.org/abs/2401.17269v1",
        "entry_id": "http://arxiv.org/abs/2401.17269v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17269v1",
        "summary": "This paper examines the quantization methods used in large-scale data\nanalysis models and their hyperparameter choices. The recent surge in data\nanalysis scale has significantly increased computational resource requirements.\nTo address this, quantizing model weights has become a prevalent practice in\ndata analysis applications such as deep learning. Quantization is particularly\nvital for deploying large models on devices with limited computational\nresources. However, the selection of quantization hyperparameters, like the\nnumber of bits and value range for weight quantization, remains an\nunderexplored area. In this study, we employ the typical case analysis from\nstatistical physics, specifically the replica method, to explore the impact of\nhyperparameters on the quantization of simple learning models. Our analysis\nyields three key findings: (i) an unstable hyperparameter phase, known as\nreplica symmetry breaking, occurs with a small number of bits and a large\nquantization width; (ii) there is an optimal quantization width that minimizes\nerror; and (iii) quantization delays the onset of overparameterization, helping\nto mitigate overfitting as indicated by the double descent phenomenon. We also\ndiscover that non-uniform quantization can enhance stability. Additionally, we\ndevelop an approximate message-passing algorithm to validate our theoretical\nresults.",
        "updated": "2024-01-30 18:58:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17269v1"
    },
    {
        "title": "Weaver: Foundation Models for Creative Writing",
        "authors": "Tiannan WangJiamin ChenQingrui JiaShuai WangRuoyu FangHuilin WangZhaowei GaoChunzhao XieChuou XuJihong DaiYibin LiuJialong WuShengwei DingLong LiZhiwei HuangXinle DengTeng YuGangan MaHan XiaoZixin ChenDanjun XiangYunxia WangYuanyuan ZhuYi XiaoJing WangYiru WangSiran DingJiayang HuangJiayi XuYilihamu TayierZhenyu HuYuan GaoChengfeng ZhengYueshu YeYihang LiLei WanXinyue JiangYujie WangSiyu ChengZhule SongXiangru TangXiaohua XuNingyu ZhangHuajun ChenYuchen Eleanor JiangWangchunshu Zhou",
        "links": "http://arxiv.org/abs/2401.17268v1",
        "entry_id": "http://arxiv.org/abs/2401.17268v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17268v1",
        "summary": "This work introduces Weaver, our first family of large language models (LLMs)\ndedicated to content creation. Weaver is pre-trained on a carefully selected\ncorpus that focuses on improving the writing capabilities of large language\nmodels. We then fine-tune Weaver for creative and professional writing purposes\nand align it to the preference of professional writers using a suit of novel\nmethods for instruction data synthesis and LLM alignment, making it able to\nproduce more human-like texts and follow more diverse instructions for content\ncreation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver\nBase (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for\ndifferent applications and can be dynamically dispatched by a routing agent\naccording to query complexity to balance response quality and computation cost.\nEvaluation on a carefully curated benchmark for assessing the writing\ncapabilities of LLMs shows Weaver models of all sizes outperform generalist\nLLMs several times larger than them. Notably, our most-capable Weaver Ultra\nmodel surpasses GPT-4, a state-of-the-art generalist LLM, on various writing\nscenarios, demonstrating the advantage of training specialized LLMs for writing\npurposes. Moreover, Weaver natively supports retrieval-augmented generation\n(RAG) and function calling (tool usage). We present various use cases of these\nabilities for improving AI-assisted writing systems, including integration of\nexternal knowledge bases, tools, or APIs, and providing personalized writing\nassistance. Furthermore, we discuss and summarize a guideline and best\npractices for pre-training and fine-tuning domain-specific LLMs.",
        "updated": "2024-01-30 18:58:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17268v1"
    },
    {
        "title": "ReacLLaMA: Merging chemical and textual information in chemical reactivity AI models",
        "authors": "Aline HartgersRamil NugmanovKostiantyn ChernichenkoJoerg Kurt Wegner",
        "links": "http://arxiv.org/abs/2401.17267v1",
        "entry_id": "http://arxiv.org/abs/2401.17267v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17267v1",
        "summary": "Chemical reactivity models are developed to predict chemical reaction\noutcomes in the form of classification (success/failure) or regression (product\nyield) tasks. The vast majority of the reported models are trained solely on\nchemical information such as reactants, products, reagents, and solvents, but\nnot on the details of a synthetic protocol. Herein incorporation of procedural\ntext with the aim to augment the Graphormer reactivity model and improve its\naccuracy is presented. Two major approaches are used: training an adapter\nGraphormer model that is provided with a GPT-2-derived latent representation of\nthe text procedure (ReacLLaMA-Adapter) and labeling an unlabeled part of a\ndataset with the LLaMA 2 model followed by training the Graphormer on an\nextended dataset (Zero-Shot Labeling ReacLLaMA). Both methodologies enhance the\ndiscernment of unpromising reactions, thereby providing more accurate models\nwith improved specificity.",
        "updated": "2024-01-30 18:57:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17267v1"
    },
    {
        "title": "Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks",
        "authors": "Andy ZhouBo LiHaohan Wang",
        "links": "http://arxiv.org/abs/2401.17263v1",
        "entry_id": "http://arxiv.org/abs/2401.17263v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17263v1",
        "summary": "Despite advances in AI alignment, language models (LM) remain vulnerable to\nadversarial attacks or jailbreaking, in which adversaries modify input prompts\nto induce harmful behavior. While some defenses have been proposed, they focus\non narrow threat models and fall short of a strong defense, which we posit\nshould be effective, universal, and practical. To achieve this, we propose the\nfirst adversarial objective for defending LMs against jailbreaking attacks and\nan algorithm, robust prompt optimization (RPO), that uses gradient-based token\noptimization to enforce harmless outputs. This results in an easily accessible\nsuffix that significantly improves robustness to both jailbreaks seen during\noptimization and unknown, held-out jailbreaks, reducing the attack success rate\non Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find\nthat RPO has a minor effect on normal LM use, is successful under adaptive\nattacks, and can transfer to black-box models, reducing the success rate of the\nstrongest attack on GPT-4 from 92% to 6%.",
        "updated": "2024-01-30 18:56:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17263v1"
    },
    {
        "title": "ReAlnet: Achieving More Human Brain-Like Vision via Human Neural Representational Alignment",
        "authors": "Zitong LuYile WangJulie D. Golomb",
        "links": "http://arxiv.org/abs/2401.17231v1",
        "entry_id": "http://arxiv.org/abs/2401.17231v1",
        "pdf_url": "http://arxiv.org/pdf/2401.17231v1",
        "summary": "Despite the remarkable strides made in artificial intelligence, current\nobject recognition models still lag behind in emulating the mechanism of visual\ninformation processing in human brains. Recent studies have highlighted the\npotential of using neural data to mimic brain processing; however, these often\nreply on invasive neural recordings from non-human subjects, leaving a critical\ngap in our understanding of human visual perception and the development of more\nhuman brain-like vision models. Addressing this gap, we present, for the first\ntime, \"Re(presentational)Al(ignment)net\", a vision model aligned with human\nbrain activity based on non-invasive EEG recordings, demonstrating a\nsignificantly higher similarity to human brain representations. Our innovative\nimage-to-brain multi-layer encoding alignment framework not only optimizes\nmultiple layers of the model, marking a substantial leap in neural alignment,\nbut also enables the model to efficiently learn and mimic human brain's visual\nrepresentational patterns across object categories and different neural data\nmodalities. Furthermore, we discover that alignment with human brain\nrepresentations improves the model's adversarial robustness. Our findings\nsuggest that ReAlnet sets a new precedent in the field, bridging the gap\nbetween artificial and human vision, and paving the way for more brain-like\nartificial intelligence systems.",
        "updated": "2024-01-30 18:18:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.17231v1"
    }
]