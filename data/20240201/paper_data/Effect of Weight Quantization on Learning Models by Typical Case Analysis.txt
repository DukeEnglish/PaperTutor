EFFECT OF WEIGHT QUANTIZATION ON LEARNING MODELS
BY TYPICAL CASE ANALYSIS
SHUHEIKASHIWAMURA1,AYAKASAKATA2,MASAAKIIMAIZUMI1,3
1 2 3
TheUniversityofTokyo ,TheInstituteofStatisticalMathematics ,RIKENAIP
Abstract. Thispaperexaminesthequantizationmethodsusedinlarge-scaledataanalysismodels
andtheirhyperparameterchoices. Therecentsurgeindataanalysisscalehassignificantlyincreased
computational resource requirements. To address this, quantizing model weights has become a
prevalent practice in data analysis applications such as deep learning. Quantization is particularly
vital for deploying large models on devices with limited computational resources. However, the
selectionofquantizationhyperparameters,likethenumberofbitsandvaluerangeforweightquan-
tization, remains an underexplored area. In this study, we employ the typical case analysis from
statisticalphysics,specificallythereplicamethod,toexploretheimpactofhyperparametersonthe
quantization of simple learning models. Our analysis yields three key findings: (i) an unstable
hyperparameterphase,knownasreplicasymmetrybreaking,occurswithasmallnumberofbitsand
alargequantizationwidth;(ii)thereisanoptimalquantizationwidththatminimizeserror;and(iii)
quantizationdelaystheonsetofoverparameterization,helpingtomitigateoverfittingasindicatedby
thedoubledescentphenomenon. Wealsodiscoverthatnon-uniformquantizationcanenhancestabil-
ity. Additionally,wedevelopanapproximatemessage-passingalgorithmtovalidateourtheoretical
results.
1. Introduction
Thedevelopmentofdata-drivenscience,whichinvolvesanalyzinglarge-scaleobservationaldata
using models with a vast number of parameters, has led to a significant increase in computational
costs. Aprimeexampleisthelarge-scaleneuralnetworksusedinartificialintelligencetechnologies.
Forinstance,GPT(GenerativePre-trainedTransformer)-3[BMR+20]possessestensofbillionsof
parameters, requiring substantial computational resources for storage and updates in computer
memory. Moreover, with the growing need to implement these models in small-edge devices,
it’s crucial to minimize their computational resource requirements. Practical applications, such as
implementing neural networks in smartphones and in-vehicle sensors, are examples of this trend
[LGW+21]. Reducingcomputationalcostsisessentialtoachieveaccurateandsustainableinference
onthesedevices.
Quantizationisoneofthemostcommontechniquesforcompressinglargemodels(forasurvey,
see [GKD+22]). Quantization involves rounding the parameters or activation values of a model to
discretevalueswithlowerbitprecision. Specifically,itconvertsoriginalvaluesstoredwithhigher
bits, e.g., 32 bits, to lower bits, such as 4 or 8 bits. This operation not only reduces the amount
ofmemoryrequiredtostoreparametersbutalsodecreasesthecomputationalresourcesneededfor
operations like matrix products. In practice, it is empirically known that quantization minimally
impactsthepredictionaccuracyofneuralnetworks,therebyenhancingtheutilityofthistechnology
Date:January31,2024.
1
4202
naJ
03
]LM.tats[
1v96271.1042:viXra(see [BHHS18] for an example). Additionally, quantization can be integrated with other model
compressiontechniques,suchasdistillationandpruning[LGW+21]. Itssignificanceisanticipated
togrowasmodels,includingnewversionsofGPT,continuetoincreaseinsize.
Anongoingchallengeinquantizationistheselectionofhyperparameters,includingthenumber
ofbitsandthequantizationrange. Whileneuralnetworksexhibitsomerobustnesstoquantization,
a significant reduction in the number of bits can make them susceptible to outliers and data
shifts. Consequently, it is crucial to choose appropriate hyperparameters that balance accuracy
with computational load. Nevertheless, the comprehension of quantization remains an evolving
issue. The characteristics of optimal hyperparameters and their very existence are still not fully
understood.
In this study, we investigate the effects of hyperparameters on quantization, namely the number
of bits and the quantization range, and their impact on its generalization performance. To achieve
this, we employ typical case analysis from statistical physics, the replica method, to analyze
the characteristics of a simple learning model. Specifically, we focus on the learning model in
the proportionally high-dimensional regime, where the number of parameters infinitely increases
whilemaintainingaconstantratiotothesamplesize. Throughthismethod,wederivethestability
condition for the replica symmetric ansatz, which can be a stability measure for algorithms under
quantization,detectingexponentialnumberoflocalminima[RTWZ01],andsubsequentlyascertain
the precise value of the generalization error in the high-dimensional regime. Additionally, we
exploretwoquantizationpatterns,uniformandnon-uniformquantization.
Asforthetheoreticalcontributions,thisstudyelucidatedthefollowingpoints:
• When the number of bits is small and the quantization range is broad, unstable phases
intermittently appear, leading to algorithmic instability. Moreover, these unstable phases
canbemitigatedthroughnon-uniformquantization.
• Regarding the selection of hyperparameters, greater bit numbers result in higher accuracy.
Conversely, there exists an optimal quantization range that is neither excessively large nor
toosmall.
• Quantization alters the peak of the double descent in the generalization error. In simpler
terms, quantization slightly hinders reaching a phase where the learning model mitigates
overfitting.
Additionally, we applied Approximate Message Passing (AMP) algorithm to the quantization
that corresponds to the replica analysis. The simulation results from this algorithm validate our
theoreticalfindings.
1.1. Related Studies. Papers on quantization, especially for neural networks, are numerous and
have a long history. Only a few are mentioned here; the techniques of quantization prior to
the 2000s are summarized in the paper [GN98]. A seminal study [BHHS18] demonstrated the
utility of quantization in more modern deep learning and discussed the effectiveness of 8-bit
quantization. Thispolicyandtheimportantroleofbitsindeeplearningwereactivelydiscussedin
[WCB+18,WJZ+20,CBD14,BLS+21]. Morepracticaluseofthequantizationisalsoinvestigated:
a study [CBUS+20] demonstrated the effectiveness of 6-bit quantization techniques in improving
performancewithnaturalgradientmethods. Theseresearchstreamsofquantizationareexhaustively
summarizedin[GKD+22].
Thetheoreticalstudyofquantizationisstilladevelopingfield. Thisstudy[DLXS19]investigated
the universal approximability of neural network models with the quantization, i.e., any continuous
function can be approximated under any accuracy, as well as the magnitude of overfitting when
2(cid:2)(cid:3) (cid:2)(cid:3)
(a) (b)
(cid:4) (cid:4)
(cid:3) (cid:3)
(cid:1)(cid:4) (cid:1)(cid:4)
(cid:1)(cid:2)(cid:3) (cid:1)(cid:2)(cid:3)
(cid:1)(cid:2)(cid:3) (cid:1)(cid:4) (cid:3) (cid:4) (cid:2)(cid:3) (cid:1)(cid:2)(cid:3) (cid:1)(cid:4) (cid:3) (cid:4) (cid:2)(cid:3)
(cid:6)(cid:3) (c) (cid:6)(cid:3) (d)
(cid:5)(cid:3) (cid:5)(cid:3)
(cid:2)(cid:3) (cid:2)(cid:3)
(cid:3) (cid:3)
(cid:1)(cid:2)(cid:3) (cid:1)(cid:4) (cid:3) (cid:4) (cid:2)(cid:3) (cid:1)(cid:2)(cid:3) (cid:1)(cid:4) (cid:3) (cid:4) (cid:2)(cid:3)
𝑤 𝑤
Figure 1. (a) and (b): Quanzied values 𝑤 = 𝜑(𝑤) as a function of continuous
(cid:98)
parameter 𝑤 at 𝜔 = 8 and 𝑛 𝑝 = 6 for (a) uniform and (b) non-uniform cases. The
diagonal lines represent the identity map. (c) and (d): Loss function at 𝑁 = 𝑀 = 1
for 𝑦 = 0 and 𝑥 = 1 with 𝜆 = 0 for (c) uniform and (d) non-uniform quantization
correspondingto(a)and(b),respectively.
making predictions with the quantized neural network. Another paper [HTL+23] discussed theo-
retically the impact of the training process with the quantization on this prediction performance.
Thispaper[GBGR23]quantifiedtheimpactofquantizationontheapproximationabilityofneural
networks. This study [LDX+17] analyzed theoretically the difficulty of learning by the quanti-
zation, and another study [CEKL16] proposed a new loss function and methodology to prevent
performancedegradationduetothequantization.
Quantization has been a subject of investigation in statistical physics, particularly in classical
spin models like the Ising model with binary variables and the Potts model with 𝐾-state values.
The variations in phase transition phenomena arising from distinct values of variables have been
discussed[MPV87]. Statisticalphysicsmodelsandmethodshavebeenincorporatedintostatistical
learning, and have successfully revealed conditions for both the failure and success of inference,
as well as the development of the efficient algorithms [KWT09, KMS+12a, KMS+12b, ZK16]. In
line with recent trends, quantization settings are incorporated in statistical physics-based machine
learning studies, such as Hopfield model with discrete coupling [SA21], restricted Boltzmann
machine with binary synapses [Hua17], and single layer neural network with multiple states
[BGL+16]. However, there is still a lack of discussion on proper quantization, especially in
termsofgeneralization,whichisfocusofourstudy.
2. ProblemSetup
2.1. Quantization. Quantization is a procedure that converts real values 𝑤 ∈ R into their closest
discretecounterparts𝑤withinapredefinedsetofcandidatevaluesΩ. Werepresentthequantization
(cid:98)
as𝑤 = 𝜑(𝑤),usingaquantizationfunction𝜑 : R → ΩthatoutputsthevalueinΩclosestto𝑤. This
(cid:98)
3
𝑤
𝜑
=
𝑤#
𝑤
𝐸quantizationprocessisdeterminedbyclippingandpartitioning[WJZ+20]. Intheclippingprocess,
continuous values whose absolute magnitude exceeds 𝜔 are capped at ±𝜔, while their respective
signs are preserved. Further, the range [−𝜔,𝜔] is divided into 𝑛 subintervals following the
𝑝
partitioning procedure, and possible discrete values are located at the edges of these subintervals,
hence |Ω| = 𝑛 𝑝+1. Here,wedefinethenumberofbits,𝑏 := log 2(𝑛 𝑝+2),capableofassumingreal
values. We investigate two types of quantization: uniform and non-uniform quantizations, which
areillustratedinFig.1(a)and(b).
2.1.1. UniformQuantization[WJZ+20]. Inuniformquantization,thesubintervalshaveequalwidth
2𝜔/𝑛 .
𝑝
2.1.2. Non-Uniform Quantization. Non-uniform quantization is introduced to finely partition the
significant value regions [HMD15, ZHMD16]. We consider a non-uniform quantization based on
a uniform partition in the log-domain, which is a slightly updated version of [MLM16, ZYG+16].
Here,thesubintervalwidthclosesttozeroissettoΔ0 givenby,
𝜔
Δ0 = (1)
2(𝑛 𝑝+𝑘)/2 − 𝑘
where 𝑘 = 2 for an even 𝑛 𝑝 and 𝑘 = 3 for an odd 𝑛 𝑝. The widths of other subintervals are larger
thanΔ0 andaretwiceaswideastheirnearestneighborsubintervalwidthtowardzero1.
2.2. Learning Problem with Linear Model. We formulate the problem setting. Let 𝑀 be a
sample size and 𝑁 be a dimension. Suppose that we have 𝑀 pairs of an predictor and output
D := {(x𝜇,𝑦 𝜇)} 𝜇𝑀 =1 ⊂ R𝑁 × R, which is independently generated from the linear model 𝑦 𝜇 =
x⊤w0 + 𝜀0 with the true parameter w0 ∈ R𝑁 and the independent noise 𝜀0 ∼ N(0,𝜎2). The
𝜇 𝜇 𝜇
components of the true parameter and predictors are independently drawn from N(0,𝜌) with
𝜌 > 0 and N(0,1/𝑁), respectively 2. We also introduce a notation y = (𝑦 1,...,𝑦 𝑀)⊤ ∈ R𝑀 and
X = (x1,...,x𝑀)⊤ ∈ R𝑀×𝑁.
We formulate the estimation procedure for parameters with quantization. We define a quanti-
zation map φ : R𝑁 → Ω𝑁 for a vector w = (𝑤 1,...,𝑤 𝑁) ∈ R𝑁 as φ(w) = (𝜑(𝑤 1),...,𝜑(𝑤 𝑁))⊤.
Then,weconsidertheempiricalriskofw ∈ R𝑁 withtheℓ
2
regularizationwithacoefficient𝜆 > 0:
1 𝜆
𝐸(w;D) := ∥y −Xφ(w)∥2 + ∥φ(w)∥2. (2)
2 2 2 2
Thelossfunctionisnonconvexduetothequantization,asshowninFig.1. Thetrainedcontinuous
parameter
w∗(D) := argmin𝐸(w;D), (3)
w∈R𝑁
constitutecontinuoussetsthatcoversubintervalatthebottomofloss(Fig.1(c)and(d)). Here,we
denotequantizedvaluesof (3)asw∗(D) := φ(w∗(D)).
(cid:98)
1We define this partition width using 𝑛 and 𝜔 to compare it with uniform quantization. For various designs of
𝑝
non-uniformquantization,seethecomprehensivereviewby[BLS+21].
2Thesesettingsarenotrestrictiveasitseems. IthasbeenshownthatasymptoticbehaviorsofGaussianuniversality
regression problems with a large class of features can be computed to leading order under a simpler model with
Gaussianfeatures[LGC+21,GLR+22,MS22,GKL+23].
4Our interest is a generalization error, which measures a prediction performance of the trained
parameter w∗(D). With a newly generated pair (𝑦 new,xnew) from the identical distribution of the
observations D,wedefineanexpectationofthegeneralizationerroras
1
𝐸 𝑔 := 2E D[E (𝑦 new,xnew)[(𝑦 new −x⊤ neww (cid:98)∗(D))2 ]]. (4)
Simplecalculationsyieldtheexpression
1 (cid:16) (cid:17)
𝐸 𝑔 = 𝑄 𝜑 −2𝑚 𝜑 + 𝜌 +𝜎2 (5)
2
with𝑄 𝜑 := 𝑁1E D[∥w (cid:98)∗(D)∥2 2] and 𝑚 𝜑 := 𝑁1E D[w0⊤ w (cid:98)∗(D)].
3. ReplicaAnalysis
We employ replica method [MPV87, CMP+23] to compute the terms 𝑄 𝜑 and 𝑚 𝜑 in (5). In
preparation,weintroducetheposteriordistributionwithaparameter 𝛽 > 0as
𝑝 𝛽(w|D) = exp(−𝛽𝐸(w;D))/𝑍 𝛽(D), (6)
∫
where 𝑍 𝛽(D) = 𝑑wexp(−𝛽𝐸(w)) is the normalization constant. At 𝛽 → ∞, the posterior
distributionconvergestotheuniformdistributionoverthesolution(3). Denotingtheposteriormean
at𝛽 → ∞as⟨w⟩ ,itsquantizedvalueφ(⟨w⟩ )isequivalenttow∗(D),whenthequantizedvalue
D D (cid:98)
of (3)isunique. Further, ⟨𝜑(w)⟩ D = 𝜑(⟨w⟩ D) holds,henceweobtain𝑄 𝜑 = 𝑁1E D (cid:2) ⟨∥𝜑(w)∥2 2⟩ D(cid:3)
and 𝑚 𝜑 = 𝑁1E D[w0⊤ ⟨φ(w)⟩ D]. Since the posterior mean has 𝑍 𝛽(D) in its denominator, which
requires summations over exponential number of terms, we need the following procedure. As we
set
1
(cid:34) ∫ 𝑑w∥𝜑(w)∥2exp(−𝛽𝐸(w))(cid:35)
𝑄 𝜑(𝑛) = 𝛽l →im ∞𝑁E
D
𝑍 𝛽𝑛 (D)
𝑍
𝛽2
(D)
, (7)
it is obvious that 𝑄 𝜑 = lim 𝑛→0𝑄 𝜑(𝑛) holds. Assuming that 𝑛 is an integer larger than 1, the
denominatoriscanceled,andtheterm 𝑍𝑛−1(D)exp(−𝛽𝐸(w)) canbeexpressedwith𝑛-replicated
𝛽
systems with w1,··· ,w𝑛. At 𝑀 → ∞ and 𝑁 → ∞ with keeping 𝛼 = 𝑀/𝑁 ∼ 𝑂(1), the
expectation E [·] is implemented by the saddle point evaluation. The resulting expression for the
D
integer 𝑛 is analytically continued to 𝑛 ∈ R to take the limit 𝑛 → 0 under the replica symmetric
(RS)ansatz,whereweassumethatthedominantsaddlepointisinvariantunderanypermutationof
replica indices 𝑎 = 1,2,··· ,𝑛. Finally, 𝑄 𝜑 and 𝑚 𝜑 are given by 𝑄 and 𝑚 that satisfy saddle point
equations
∫ ∫
𝑄 = 𝐷𝑧(𝜑∗(ℎ𝑧,Θ(cid:98)))2, 𝑚 = 𝑚 (cid:98)𝜌 𝐷𝑧𝜕𝜑∗(ℎ𝑧,Θ(cid:98)), (8)
where 𝐷𝑧 = √𝑑 2𝑧 𝜋𝑒−1 2𝑧2, ℎ = √︁ 𝑚 (cid:98)2𝜌 + (cid:98)𝜒 andΘ(cid:98) = 𝑄 (cid:98)+𝜆 satisfy
𝛼 2𝛼E ∫
𝑔
𝑄 (cid:98)= 𝑚
(cid:98)
=
1+
𝜒, (cid:98)𝜒 =
(1+
𝜒)2, 𝜒 = 𝐷𝑧𝜕𝜑∗(ℎ𝑧,Θ(cid:98)). (9)
530 30
(a) (b)
20 20
10 10
2 3 4 5 2 3 4 5
30 30
(c) (d)
20 20
10 10
2 3 4 5 2 3 4 5
b b
Figure 2. Phase diagrams on 𝑏 − 𝜔 plane for quantized regression at (𝜎,𝛼) =
(0.01,1.5). Shaded and blighted regions are RS and RSB phase, respectively. (a)
and(b)areunderuniformquantizationat𝜆 = 0.0and𝜆 = 1.0,respectively. (c)and
(d)areundernon-uniformquantizationat𝜆 = 0.0and𝜆 = 1.0,respectively.
Here, E 𝑔 = 21 (𝑄 −2𝑚 + 𝜌 +𝜎2) coincides with 𝐸 𝑔 at the saddle point. The function 𝜑∗(ℎ𝑧,Θ(cid:98)) is
thesolutionoftheproblem
(cid:26)1 (cid:27)
𝜑∗(ℎ𝑧,Θ(cid:98)) = argmin Θ(cid:98)𝑑2 − ℎ𝑧𝑑 , (10)
2
𝑑∈Ω
whichcorresponds 𝜑(ℎ𝑧/Θ(cid:98)),namelythethesolutionofthequantization-restrictedridgeoptimiza-
tion problem under RS assumption3[KWT09], and 𝜕𝜑∗(ℎ𝑧,Θ(cid:98)) = 𝜕(𝜕 ℎ𝑧)𝜑∗(ℎ𝑧,Θ(cid:98)). The random
variable 𝑧 effectively represents the randomness induced by data D, and the solution of (10) is
statisticallyequivalenttothesolutionoftheoriginalproblem(3)[BM11].
4. Result
4.1. Phase Diagram. We study the RS ansatz for replica analysis. The RS solution loses local
stability against symmetry breaking perturbations when the following condition is not satisfied
[DAT78]:
𝛼 ∫ (cid:16) (cid:17)2
𝐷𝑧 𝜕𝜑∗(ℎ𝑧,Θ(cid:98)) < 1. (11)
(1+ 𝜒)2
Thephasessatisfying(11)andthatnotsatisfyingitaretermedasRSandreplicasymmetrybreaking
(RSB) phases, respectively. In the RSB phase, algorithmic instabilities arise due to exponential
number of local minima [AHW95, MPV87], necessitating quantization hyperparameter setting to
avoidtheRSBphase.
3This correspondence does not mean that the obtained quantized posterior mean is equivalent to the quantized
solutionforridgeregression. Thesaddlepointsdependonthequantizationfunction,hencethevaluesof ℎandΘ(cid:98)for
quantizedregressiondifferfromthoseinusualridgeregression.
6
ω
ω
ω
ω(cid:3)(cid:5)(cid:8) (cid:3)(cid:5)(cid:8)
(a) (b)
(cid:3)(cid:5)(cid:7) (cid:3)(cid:5)(cid:7)
(cid:3)(cid:5)(cid:6) (cid:3)(cid:5)(cid:6)
(cid:3)(cid:5)(cid:2) (cid:3)(cid:5)(cid:2)
−𝜔 𝑤$ 𝜔 −𝜔 𝑤$ 𝑤$ 𝜔
! ! "
(cid:3) (cid:3)
(cid:1)(cid:2)(cid:3) (cid:1)(cid:4) (cid:3) (cid:4) (cid:2)(cid:3) (cid:1)(cid:2)(cid:3) (cid:1)(cid:4) (cid:3) (cid:4) (cid:2)(cid:3)
Figure 3. Comparison between the distribution of continuous value ℎ𝑧/Θ(cid:98) (solid
line)andquantizedvaluesinΩwith𝜔 = 10(dots)for(a)𝑛 𝑝 = 2and(b)𝑛 𝑝 = 3under
uniformquantization. Here,weset ℎ/Θ(cid:98) = 1forsimplicity,andareascorresponding
tothediscretevaluesareshadedseparately.
Figure 2 shows the phase diagrams for uniform quantization ((a), (b)) and non-uniform quanti-
zation((c),(d)). Weobtainseveralimplicationsasfollows.
(I): Irrespective of the quantization method, large bits 𝑏 and small 𝜔 result in RS phase. This
alignswiththefactthatalarge𝑏 makesquantizedvaluesclosertocontinuousvalues. Additionally,
a small 𝜔 induces shrinkage in estimates, which tends to be the RS phase as the effective model
complexityisreduced[ZHT07,Sak23].
(II): The stronger regularization with large𝜆 makes the RS phase larger. This is consistent with
the general trend that regularization improves the stability of the model by reducing the variance
of weights. (III): In regions with small bits 𝑏 and large range 𝜔, the RS and RSB phases alternate
periodically along the 𝑏 direction, resulting in a striped phase diagram. The striped pattern is
caused by the dependence of Ω on 𝑛 𝑝 (or 𝑏), leading to RS for even 𝑛 𝑝 and RSB for odd 𝑛 𝑝. As
illustrated in Fig. 3, for even 𝑛 𝑝, Ω includes zero (𝑤 (cid:98)1 of (a)), while for odd 𝑛 𝑝, it does not. For
large𝜔 withsmall𝑛 𝑝,theintervalsbetweenquantizedvaluescanbesubstantialcomparedto ℎ/Θ(cid:98),
which represents the variance of the continuous value to be quantized in (10). Hence, for even
𝑛 , continuous values are highly likely to be quantized to zero, while for odd 𝑛 , they tend to be
𝑝 𝑝
quantized to the non-zero value closest to zero (𝑤 (cid:98)1 or 𝑤 (cid:98)2 in Fig.3 (b) where |𝑤 (cid:98)1| = |𝑤 (cid:98)2|). The
closest values can be substantial relative to ℎ/Θ(cid:98) at large 𝜔 and small 𝑛 𝑝, causing the quantization
at odd 𝑛 to extend the continuous value. This parameter extension increases effective degrees of
𝑝
freedom[ZHT07],resultingintheinstabilityoftheRSsolution.
(IV): The non-uniform quantization exhibits a larger RS phase compared to uniform one, in-
dicating a broader range for quantization settings. In non-uniform quantization, the subintervals
near the origin are small, effectively suppressing the extension caused by quantization for odd 𝑛 ,
𝑝
leadingtoawiderRSphase.
4.2. Optimal Parameters in RS Phase. We now investigate the generalization error 𝐸 𝑔 in RS
phase to consider optimal quantization hyperparameters. Figure 4 shows the generalization errors
underRSansatzwithdifferentvaluesof𝑏and𝜔for(a)lowand(b)highnoisecases. Insightsderived
from the behavior of generalization errors are as follows. First, in most cases, the generalization
error is a convex function of 𝜔 and has a unique minimum. This characteristics implies that there
existstheoptimalchoicefortherange𝜔,whichshouldbeappropriatelyselected. Especiallywhen
7(a) (b)
σ = 0.01,λ = 0.01,α = 1.4 σ = 1.0,λ = 0.01,α = 1.4
0.6
b=1 b=1
1.4
b=2 b=2
RSB
b=3 b=3
0.4
b=4 1.2 b=4
b=5 b=5
0.2
1.0
0.0 0.8
1 2 3 4 0.5 1.0 1.5
ω ω
Figure 4. Expected generalization error under RS assumption as a function of 𝜔
for (a) (𝜎,𝜆,𝛼) = (0.01,0.01,1.4) and (b) (𝜎,𝜆,𝛼) = (1.0,0.01,1.4). Solid lines
and dashesd lines represent the result of uniform and non-uniform quantization,
respectively. RSBphaseisindicatedbyblackarrows.
(a) (b)
b = 9.0,σ = 1.0,λ = 0.01 b = 9.0,σ = 1.0,λ = 0.01
ω=1 ω=1
2.5 ω=2 2.5 ω=2
ω=3 ω=3
ω=4 ω=4
2.0 2.0
ω=5 ω=5
ω=6 ω=6
1.5 ω=7 1.5 ω=7
1.0 1.0
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
α = M/N α = M/N
Figure 5. Expected generalization error under RS assumption as a function of
𝛼 = 𝑁/𝑀 for (a) uniform quantization and (b) non-uniform quantization. The
dashedlinesrepresenttheresultofridgeregression.
the number of bits is small, 𝐸 exhibits a stronger dependence on 𝜔 compared to cases with a
𝑔
larger number of bits, resulting in a sharper minimum value of 𝐸 . Consequently, the appropriate
𝑔
selectionof𝜔becomesmorecrucialforscenarioswithasmallnumberofbits. Second,asthenoise
𝜎 increases with the large bit 𝑏 increases, the generalization error of non-uniform and uniform
quantizationdisappears.
8
E
E
g
g
E
g
E
g(a) (b)
σ = 0.01,λ = 0.01,α = 1.4 σ = 0.01,λ = 0.01,α = 1.4
0.5
ω=1 b=1
0.4 ω=2 b=2
0.6 b=3
ω=3
b=4
RSB ω=4
0.3 b=5
RSB
ω=5
0.4
0.2
0.2
0.1
0.0 0.0
3.0 3.5 4.0 4.5 5.0 1 2 3
b ω
Figure 6. Comparison of results by replica analysis (solid line) and AMP (◦) in
terms of generalization error dependence on (a) 𝑏 and (b) 𝜔. The error bars denote
standarderrorsassessedby100independentrunsofAMP,wheretheeachcomponent
of initial conditions m0 is generated by N(0,1). RSB phase is indicated by black
arrows.
4.3. Effect on Double Descent. We investigate the effect of quantization on the double-descent
phenomenon of the generalization error 𝐸 . The double-descent refers to the phenomenon in
𝑔
whichthegeneralizationerrorincreasesonceandthendecreaseswhenthenumberofparametersis
extremely large[BHMM19, LVM+20]. This is an important notion in modern data analysis, since
it suggests that an excess number of parameters enters a regime where the low performance from
overfitting can be avoided [BHMM19, HMRT22]. Ridge regression with continuous parameter
exhibits the double descent phenomena, where generalization error has a peak at 𝛼 = 𝑀/𝑁 = 1
undersufficientlysmall𝜆 andsufficientlylarge 𝜎 [HMRT22,KH92].
Fig.5 shows the generalization errors under quantization as a function of 𝛼 for (a) uniform and
(b) non-uniform cases. The quantization shifts the peak toward smaller 𝛼 compared with the
usual ridge regression, which is indicated by dashed line. This means that more parameters are
needed to reach the regime of overparameterization under the quantization. This implies that it
is slightly difficult to benefit from the overparameterization, since the quantization reducing an
effective number of parameters. This tendency is marginally more pronounced for non-uniform
quantization. However,thisdiscrepancydisappearswiththelarger𝜔.
5. AMPAlgorithmandExperiments
As a numerical method that can potentially achieve the theoretical results under the replica
symmetry (RS) ansatz, we employ the approximate message passing (AMP) algorithm [DMM09,
Ran11]. AMP for quantized regression includes the procedures outlined in Algorithm 1, which
utilizeaquantizationfunction(10)anditsderivative. Asinreplicaanalysis,thequantizedestimate
byAMPinstep𝑡,m¯ 𝑡,correspondstothequantizedsolutionoftheridgeregression[SO21]. Inthe
quantized regression, the following relationship between AMP and replica analysis holds, as with
continuous regression [ZK16, KMS+12a]. First, the typical trajectory of AMP can be described
bythestateevolutionequation,whichcorrespondstothesaddlepointequationsderivedbyreplica
9
E
g
E
ganalysisundertheRSansatz. Second,themicroscopicconvergenceconditionofAMPcorresponds
tothestabilityconditionoftheRSsolution(11). Thispropertyindicatesthatthetheoreticalresult
forquantizedregressioncanbeachievedbyAMP,inprinciple.
Fig. 6shows 𝑏 and𝜔-dependenceof 𝐸 𝑔 at 𝑁 = 2500,computedwiththeform(4)usingAMP’s
estimatem¯ 𝑡 atsufficientlylarge𝑡 asw∗(D). Theobservednumericalresultsareingoodagreement
(cid:98)
with the theory in the RS phase, and indicate the validity of the AMP for the quantized regression
problems.
Algorithm1AMPforquantizedregression
m¯ 𝑡 = {𝑚¯ 𝑖𝑡} 𝑖𝑁 =1,v𝑡 = {𝑣 𝑖𝑡} 𝑖𝑁 =1,x𝜇 = {𝑋 𝜇𝑖} 𝑖𝑁 =1.
Initialize {(𝑉0,𝜃0)}𝑀 , {(Σ0,𝑅0)}𝑁 ,m¯ 0,v0.
𝜇 𝜇 𝜇=1 𝑖 𝑖 𝑖=1
for𝑡 = 1,...,𝑇 do
𝑉 𝜇𝑡 ← (v𝑡−1)⊤x𝜇
(Σ𝑡)−1 ← (cid:205)𝑀 𝑋2 /(𝑉𝑡 +1)
𝑖 𝜇=1 𝜇𝑖 𝜇
𝜃𝑡
𝜇
← (m¯ 𝑡−1)⊤x𝜇 −𝑉 𝜇𝑡(𝑦
𝜇
−𝜃𝑡 𝜇−1)/(𝑉 𝜇𝑡−1 +1),
𝑅 𝑖𝑡 ← 𝑚¯ 𝑖𝑡−1 +Σ 𝑖𝑡(cid:205) 𝜇𝑀
=1
𝑋 𝜇𝑖(𝑦
𝜇
−𝜃𝑡 𝜇)/(𝑉 𝜇𝑡−1 +1)
𝑚¯𝑡 ← 𝜑∗(𝑅𝑡/Σ𝑡,𝜆+1/Σ𝑡)
𝑖 𝑖 𝑖 𝑖
𝑣𝑡 ← 𝜕𝜑∗(𝑅𝑡/Σ𝑡,𝜆+1/Σ𝑡)
𝑖 𝑖 𝑖 𝑖
endfor
6. DiscussionandConclusion
Inthisstudy,weprovidedthetheoreticalresultsoftheeffectofhyperparametersofquantization.
Weinvestigatedtwotypesofquantization: uniformandnon-uniform. First,wederivedtheRS-RSB
phase diagram that describes the validity of our theoretical results. We showed that non-uniform
quantizationandregularizationcanreducetheextentoftheRSBphase. Second,wedemonstrated
thatanoptimalquantizationrangeexists,whichminimizesthegeneralizationerror. Lastly,wefound
that more parameters are required to diminish the generalization error in the overparametrization
regime when applying quantization. Our numerical experiments by AMP are in good agreement
withtheoreticalresults.
It is reported that AMP fail to achieve the theoretical limit for nonconvex problems [SO21].
Investigating the existence of this divergence in quantized regression is future work. Additionally,
extendingourresultstodevelopapracticalmethodforselectinghyperparametersisalsothesubject
offuturework.
Acknowledgment
SK is supported by JSPS KAKENHI (23KJ0723) and JST CREST (JPMJCR21D2). AS is
supportedbyJSPSKAKENHI(22H05117)andJSTPRESTO(JPMJPR23J4). MIissupportedby
JSPSKAKENHI(21K11780),JSTCREST(JPMJCR21D2),andJSTFOREST(JPMJFR216I).
References
[AHW95] Peter Auer, Mark Herbster, and Manfred KK Warmuth. Exponentially many local minima for single
neurons.Advancesinneuralinformationprocessingsystems,8,1995.
10[BGL+16] CarloBaldassi,FedericaGerace,CarloLucibello,LucaSaglietti,andRiccardoZecchina.Learningmay
needonlyafewbitsofsynapticprecision.PhysicalReviewE,93:052313,2016.
[BHHS18] RonBanner,ItayHubara,EladHoffer,andDanielSoudry.Scalablemethodsfor8-bittrainingofneural
networks.Advancesinneuralinformationprocessingsystems,31,2018.
[BHMM19] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning
practice and the classical bias–variance trade-off. Proceedings of the National Academy of Sciences,
116(32):15849–15854,2019.
[BLS+21] Chaim Baskin, Natan Liss, Eli Schwartz, Evgenii Zheltonozhskii, Raja Giryes, Alex M Bronstein, and
Avi Mendelson. Uniq: Uniform noise injection for non-uniform quantization of neural networks. ACM
TransactionsonComputerSystems(TOCS),37(1-4):1–15,2021.
[BM11] MohsenBayatiandAndreaMontanari.Thedynamicsofmessagepassingondensegraphs,withapplica-
tionstocompressedsensing.IEEETransactiononinformationtheory,57(2):764–785,2011.
[BMR+20] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,etal.Languagemodelsarefew-shotlearners.
Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
[CBD14] MatthieuCourbariaux,YoshuaBengio,andJean-PierreDavid.Trainingdeepneuralnetworkswithlow
precisionmultiplications.arXivpreprintarXiv:1412.7024,2014.
[CBUS+20] Brian Chmiel, Liad Ben-Uri, Moran Shkolnik, Elad Hoffer, Ron Banner, and Daniel Soudry. Neural
gradients are near-lognormal: improved quantized and sparse training. In International Conference on
LearningRepresentations,2020.
[CEKL16] Yoojin Choi, Mostafa El-Khamy, and Jungwon Lee. Towards the limit of network quantization. In
InternationalConferenceonLearningRepresentations,2016.
[CMP+23] Patrick Charbonneau, Enzo Marinari, Giorgio Parisi, Federico Ricci-tersenghi, Gabriele Sicuro,
FrancescoZamponi,andMarcMezard.SpinGlassTheoryandFarBeyond: ReplicaSymmetryBreaking
after40Years.WorldScientific,2023.
[DAT78] JRLDeAlmeidaandDavidJThouless.Stabilityofthesherrington-kirkpatricksolutionofaspinglass
model.JournalofPhysicsA:MathematicalandGeneral,11(5):983,1978.
[DLXS19] YukunDing,JinglanLiu,JinjunXiong,andYiyuShi.Ontheuniversalapproximabilityandcomplexity
bounds of quantized relu neural networks. In International Conference on Learning Representations.
InternationalConferenceonLearningRepresentations,ICLR,2019.
[DMM09] David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algorithms for compressed
sensing.ProceedingsoftheNationalAcademyofSciences,106(45):18914–18919,2009.
[GBGR23] AntoineGonon,NicolasBrisebarre,Re´miGribonval,andElisaRiccietti.Approximationspeedofquan-
tizedvs.unquantizedreluneuralnetworksandbeyond.IEEETransactionsonInformationTheory,2023.
[GKD+22] AmirGholami,SehoonKim,ZhenDong,ZheweiYao,MichaelWMahoney,andKurtKeutzer.Asurvey
of quantization methods for efficient neural network inference. In Low-Power Computer Vision, pages
291–326.ChapmanandHall/CRC,2022.
[GKL+23] FedericaGerace,FlorentKrzakala,BrunoLoureiro,LudovicStephan,andLenkaZdeborova´.Gaussian
universalityofperceptronswithrandomlabels.2023.hal-04019749.
[GLR+22] SebastianGoldt,BrunoLoureiro,GalenReeves,FlorentKrzakala,MarcMe´zard,andLenkaZdeborova´.
Thegaussianequivalenceofgenerativemodelsforlearningwithshallowneuralnetworks.InMathematical
andScientificMachineLearning,pages426–471.PMLR,2022.
[GN98] Robert M. Gray and David L. Neuhoff. Quantization. IEEE transactions on information theory,
44(6):2325–2383,1998.
[HMD15] SongHan,HuiziMao,andWilliamJDally.Deepcompression: Compressingdeepneuralnetworkswith
pruning,trainedquantizationandhuffmancoding.arXivpreprintarXiv:1510.00149,2015.
[HMRT22] TrevorHastie,AndreaMontanari,SaharonRosset,andRyanJTibshirani.Surprisesinhigh-dimensional
ridgelessleastsquaresinterpolation.Annalsofstatistics,50(2):949,2022.
[HTL+23] Charles Hernandez, Bijan Taslimi, Hung Yi Lee, Hongcheng Liu, and Panos M Pardalos. Training
generalizablequantizeddeepneuralnets.ExpertSystemswithApplications,213:118736,2023.
[Hua17] HaipingHuang.Statisticalmechanicsofunsupervisedfeaturelearninginarestrictedboltzmannmachine
withbinarysynapses.JournalofStatisticalMechanics: TheoryandExperiment,2017(5):053302,2017.
11[KH92] AndersKroghandJohnAHertz.Generalizationinalinearperceptroninthepresenceofnoise.Journal
ofPhysicsA:MathematicalandGeneral,25(5):1135,1992.
[KMS+12a] FKrzakala, MMe´zard, FSausset, YF.Sun, andLZdeborova´.Statistical-physics-basedreconstruction
incompressedsensing.PhysicalReviewX,2:021005,2012.
[KMS+12b] FlorentKrzakala,MarcMe´zard,FrancoisSausset,YifanSun,andLenkaZdeborova´.Probabilisticrecon-
structionincompressedsensing: algorithms,phasediagrams,andthresholdachievingmatrices.Journal
ofStatisticalMechanics: TheoryandExperiment,2012(08):P08009,2012.
[KWT09] Yoshiyuki Kabashima, Tadashi Wadayama, and Toshiyuki Tanaka. A typical reconstruction limit for
compressed sensing based on lp-norm minimization. Journal of Statistical Mechanics: Theory and
Experiment,2009(09):L09003,2009.
[LDX+17] HaoLi,SohamDe,ZhengXu,ChristophStuder,HananSamet,andTomGoldstein.Trainingquantized
nets: Adeeperunderstanding.AdvancesinNeuralInformationProcessingSystems,30,2017.
[LGC+21] BrunoLoureiro,CedricGerbelot,HugoCui,SebastianGoldt,FlorentKrzakala,MarcMezard,andLenka
Zdeborova´.Learningcurvesofgenericfeaturesmapsforrealisticdatasetswithateacher-studentmodel.
AdvancesinNeuralInformationProcessingSystems,34:18137–18151,2021.
[LGW+21] TailinLiang,JohnGlossner,LeiWang,ShaoboShi,andXiaotongZhang.Pruningandquantizationfor
deepneuralnetworkacceleration: Asurvey.Neurocomputing,461:370–403,2021.
[LVM+20] Marco Loog, Tom Viering, Alexander Mey, Jesse H Krijthe, and David MJ Tax. A brief prehistory of
doubledescent.ProceedingsoftheNationalAcademyofSciences,117(20):10625–10626,2020.
[MLM16] DaisukeMiyashita,EdwardHLee,andBorisMurmann.Convolutionalneuralnetworksusinglogarithmic
datarepresentation.arXivpreprintarXiv:1603.01025,2016.
[MPV87] MarcMe´zard,GiorgioParisi,andMiguelVirasoro.Spinglasstheoryandbeyond: AnIntroductiontothe
ReplicaMethodandItsApplications,volume9.WorldScientificPublishingCoInc,1987.
[MS22] Andrea Montanari and Basil N Saeed. Universality of empirical risk minimization. In Conference on
LearningTheory,pages4310–4312.PMLR,2022.
[Ran11] Sundeep Rangan. Generalized approximate message passing for estimation with random linear mixing.
In Information Theory Proceedings (ISIT), 2011 IEEE International Symposium on, pages 2168–2172.
IEEE,2011.
[RTWZ01] FedericoRicci-Tersenghi,MartinWeigt,andRiccardoZecchina.Simplestrandomk-satisfiabilityproblem.
PhysicalReviewE,63(2):026702,2001.
[SA21] Ryuta Sasaki and Toru Aonishi. Analysis of the hopfield model with discrete coupling. Journal of the
PhysicalSocietyofJapan,90(9):094602,2021.
[Sak23] Ayaka Sakata. Prediction errors for penalized regressions based on generalized approximate message
passing.JournalofPhysicsA:MathematicalandTheoretical,56(4):043001,2023.
[SO21] AyakaSakataandTomoyukiObuchi.Perfectreconstructionofsparsesignalswithpiecewisecontinuous
nonconvexpenaltiesandnonconvexitycontrol.JournalofStatisticalMechanics: TheoryandExperiment,
2021(9):093401,2021.
[WCB+18] NaigangWang,JungwookChoi,DanielBrand,Chia-YuChen,andKailashGopalakrishnan.Trainingdeep
neural networks with 8-bit floating point numbers. Advances in neural information processing systems,
31,2018.
[WJZ+20] HaoWu,PatrickJudd,XiaojieZhang,MikhailIsaev,andPauliusMicikevicius.Integerquantizationfor
deeplearninginference: Principlesandempiricalevaluation.arXivpreprintarXiv:2004.09602,2020.
[ZHMD16] ChenzhuoZhu,SongHan,HuiziMao,andWilliamJDally.Trainedternaryquantization.InInternational
ConferenceonLearningRepresentations,2016.
[ZHT07] H.Zou,T.Hastie,andR.Tibshirani.Onthedegreesoffreedomofthelasso.Annal.Stat.,35(5):2173–
2192,2007.
[ZK16] Lenka Zdeborova´ and Florent Krzakala. Statistical physics of inference: Thresholds and algorithms.
AdvancesinPhysics,65(5):453–552,2016.
[ZYG+16] AojunZhou,AnbangYao,YiwenGuo,LinXu,andYurongChen.Incrementalnetworkquantization: To-
wardslosslesscnnswithlow-precisionweights.InInternationalConferenceonLearningRepresentations,
2016.
12AppendixA. AdditionalResult
(a) (b)
σ = 0.01,λ = 0.01,α = 0.7 σ = 1.0,λ = 0.01,α = 0.7
0.6
b=1 b=1
1.4
0.5 b=2 b=2
RSB
b=3 b=3
0.4 b=4 1.2 b=4
b=5 b=5
0.3
1.0
0.2
0.8
0.1
1 2 3 4 0.5 1.0 1.5
ω ω
Figure7. ExpectedgeneralizationerrorunderRSassumptionasafunctionof𝜔for
(a)(𝜎,𝜆,𝛼) = (0.01,0.01,0.7)and(b)(𝜎,𝜆,𝛼) = (1.0,0.01,0.7). Theoutcomeof
uniform quantization is represented by solid lines, while non-uniform quantization
isrepresentedbydashedlines. TheblackarrowsindicatetheRSBphase.
We present an additional finding regarding the analysis of the expected generalization error.
Figure7showsthegeneralizationerrorsforboththelownoisecase(a)andthehighnoisecase(b)
when𝜆 = 0.01and𝛼 = 0.7. Itisevidentthatthegeneralizationperformanceissignificantlyworse
due to the impact of overparameterization, where 𝛼 < 1, compared to the performance shown in
Figure4ofthemaintext.
Figure 8 shows the generalization errors with (a) low and (b) high noise cases at 𝜆 = 1.0 and
𝛼 = 0.7. We observe that regularization can mitigate the sharpness of the optimality condition for
𝜔.
Figure 9 shows the generalization errors under quantization as a function of 𝛼 for (a) uniform
and(b)non-uniformcases. Thedoubledecentpeakisdiminishedinthelow-noisecase.
Figure 10 illustrates the generalization errors associated with quantization as a function of the
numberofbits𝑏for(a)uniformand(b)non-uniformcases. Inthecaseofuniformquantization,the
generalization performance asymptotically approaches that of the ridge regression as the number
of bits increases. This is not the case, however, for non-uniform quantization. In non-uniform
quantization, since the partition width increases exponentially, the approximation accuracy is
inferiorcomparedtothatofuniformquantization.
AppendixB. StateEvolutionandLocalStability
The typical performance of AMP can be analyzed by state evolution (SE). SE describes the
statistical properties of the trajectory of AMP, which depend on the randomness of the data. The
typical trajectory of AMP is tracked by two variables: 𝑉𝑡 := 1 (cid:205)𝑁 𝑣𝑡 and 𝐸𝑡 := 1 (cid:205)𝑁 (𝑤0 −𝑚¯𝑡).
𝑁 𝑖 𝑖 𝑁 𝑖 𝑖 𝑖
13
E
g
E
g(a) (b)
σ = 0.01,λ = 1.0,α = 0.7 σ = 1.0,λ = 1.0,α = 0.7
0.7
b=1 b=1
1.4
b=2 b=2
0.6
b=3 b=3
b=4 1.2 b=4
0.5
b=5 b=5
0.4
1.0
0.3
0.8
1 2 3 4 0.5 1.0 1.5
ω ω
Figure 8. Expected generalization error under RS assumption as a function of 𝜔
for (a) (𝜎,𝜆,𝛼) = (0.01,1.0,0.7) and (b) (𝜎,𝜆,𝛼) = (1.0,1.0,0.7). Solid lines
and dashed lines represent the result of uniform and non-uniform quantization,
respectively. RSBphaseisindicatedbyblackarrows.
(a) (b)
b = 9.0,σ = 0.01,λ = 0.01 b = 9.0,σ = 0.01,λ = 0.01
0.4 0.4
ω=1 ω=1
ω=2 ω=2
0.3 ω=3 0.3 ω=3
ω=4 ω=4
ω=5 ω=5
0.2 0.2
ω=6 ω=6
ω=7 ω=7
0.1 0.1
0.0 0.0
0.5 1.0 1.5 2.0 0.5 1.0 1.5 2.0
α = M/N α = M/N
Figure 9. Expected generalization error under RS assumption as a function of
𝛼 = 𝑁/𝑀 for (a) uniform quantization and (b) non-uniform quantization. The
dashedlinesrepresenttheresultofridgeregression.
Assumingthesamesettingsasthereplicaanalysisinthemaintext,theSEequationsaregivenas
∫ 𝜕𝜑∗ (cid:0)𝜉𝑡𝑧,Λ𝑡(cid:1)
𝑉𝑡+1 = 𝐷𝑧 , (12)
𝜕(𝜉𝑡𝑧)
𝜌𝛼 ∫ 𝜕𝜑∗(𝜉𝑡𝑧,Λ𝑡) ∫
𝐸𝑡+1 = 𝜌 −2 𝐷𝑧 + 𝐷𝑧𝜑∗(𝜉𝑡𝑧,Λ𝑡 )2, (13)
𝜉𝑡(1+𝑉𝑡) 𝜕𝑧
14
E
E
g
g
E
g
E gσ = 0.01,λ = 0.01,α = 1.4 σ = 0.01,λ = 0.01,α = 1.4
0.5
ω=1 ω=1
0.15
0.4 ω=2 ω=2
ω=3 ω=3
RSB
0.3 ω=4 ω=4
0.10
ω=5 ω=5
0.2 ridge ridge
0.05
0.1
0.0
0.00
3.0 3.5 4.0 4.5 5.0 3.0 3.5 4.0 4.5 5.0
b b
Figure10. TheexpectedgeneralizationerrorundertheRSassumptionispresented
as a function of 𝑏 for (a) uniform quantization and (b) non-uniform quantization.
Theyellowlinesrepresenttheresultsoftheridgeregression.
with 𝜉𝑡 := √︁ 𝛼2𝜌/(1+𝑉𝑡)2 +𝛼(𝜎2 +𝐸𝑡)/(1+𝑉𝑡)2 and Λ𝑡 := 𝜆 + 𝛼/(1+𝑉𝑡). The SE equations
and the saddle point equations of the replica analysis in the main text are equivalent based on the
correspondences
𝑉𝑡 ↔ 𝜒, (14)
𝐸𝑡 ↔ 𝑄 −2𝑚 + 𝜌. (15)
Therefore,thefixedpointofSEandtheRSsolutionofthereplicaanalysisareequivalent. Further-
more,thelinearstabilityconditionofthefixedpointofAMPisderivedas
𝛼 ∫
𝐷𝑧(𝜕𝜑∗(𝜉𝑧,Λ))2 < 1. (16)
(1+𝑉)2
SincethisconditionisequivalenttothestabilityconditionoftheRSsolutioninreplicaanalysis,it
isexpectedthattheoreticalresultsobtainedbythereplicamethodintheRSphasecanbevalidated
throughnumericalexperimentsemployingAMP.
15
E
g
E
g