GazeGPT: Augmenting Human Capabilities using Gaze-contingent
Contextual AI for Smart Eyewear
ROBERTKONRAD1,NITISHPADMANABAN1,J.GABRIELBUCKMASTER1,KEVINC.BOYLE1,and
GORDONWETZSTEIN1,2
1ZinnLabs,Inc.,2StanfordUniversity
“Will this plant be happy indoors?”
Will this plant be
happy indoors?
Yes, this plant will be happy indoors if it
receives bright indirect sunlight and proper care.
Fig.1: WeintroduceGazeGPT,ahuman-centricinterfacetogenerativeAImodels. CurrentAImodelsareexceptionalatingestingmultimodaldata
andprovidingreasonableresponses,butoftenlackthefundamentalinformationtoidentifytheobjectofinteresttothehumanuser. GazeGPTusesa
combinationofagazetrackerandaworld-facingcameratoprovidecontexttouserqueries.Thequery,alongwithamultiscalecroparoundtheobjectof
interest,isuploadedtoamultimodallargelanguagemodel,likeGPT-4V,whichcanprovidebetterresponseswiththeincludedcontext.Thisnewinterface
toAIhasthepotentialtofundamentallychangehowhumansaccessinformation.
ABSTRACT 1 INTRODUCTION
Therecentemergenceofmultimodalgenerativeartificialin-
Multimodal large language models (LMMs) excel in world telligence (AI) is heralded as a significant step towards ar-
knowledge and problem-solving abilities. Through the use tificial general intelligence [Achiam et al. 2023; Anil et al.
ofaworld-facingcameraandcontextualAI,emergingsmart 2023; Moon et al. 2023; You et al. 2023; H. Liu, C. Li, Q.
accessoriesaimtoprovideaseamlessinterfacebetweenhu- Wu, et al. 2023; H. Liu, C. Li, Y. Li, et al. 2023]. Indeed,
mans and LMMs. Yet, these wearable computing systems someofthesemodelsalreadyoutperformhumanexpertson
lackanunderstandingoftheuser’sattention. Weintroduce world knowledge and problem-solving abilities across mul-
GazeGPTasanewuserinteractionparadigmforcontextual tiplesubjects,suchasmath,physics,history,law,medicine,
AI.GazeGPTusesgazetrackingtohelptheLMMunderstand and ethics [Anil et al. 2023]. These large multimodal mod-
whichobjectintheworld-facingcameraviewauserispay- els (LMMs) represent the foundation of future personal as-
ingattentionto. Usingextensiveuserevaluations,weshow sistantsthatusecontextualAItounderstandauser’sintent
that this gaze-contingent mechanism is a faster and more andaugmenthumancapabilities.
accuratepointingmechanismthanalternatives, thatitaug- Oneofthekeyquestionsinthiscontextiswhataneffec-
ments human capabilities by significantly improving their tive interface between an LMM and a human should look
accuracyinadog-breedclassificationtask,andthatitiscon- like. Today’s models primarily take a user-defined text
sistentlyrankedasmorenaturalthanhead-orbody-driven prompt and a specified image as input and generate text
selectionmechanismsforcontextualAI.Moreover,weproto- output. Through the use of world-facing cameras, micro-
typeavarietyofapplicationscenariosthatsuggestGazeGPT phones, open-ear headphones, and speech-to-text capabili-
could be of significant value to users as part of future AI- ties,emerging“smart”glassesandotheraccessoriesprovide
drivenpersonalassistants. amoreseamless,all-daywearableinterfacebetweenhumans
UploadedtoarXivFebruary2024
4202
naJ
13
]CH.sc[
2v71271.1042:viXra2 • Konradetal.
and LMMs. Examples include Ray-Ban Meta Stories, Ama- • an evaluation of selection mechanisms, showing that
zon Echo Frames, Snap Spectacles, and Humane’s AI Pin, gazeisfasterandmoreaccuratethanbody-andhead-
among others. Unlike conventional augmented- or mixed- basedselectionwithoutvisualfeedback;
reality systems, however, smart accessories usually do not • the demonstration of augmented human capabilities,
useadisplayforvisualfeedback. improvingobjectclassificationaccuracyofhumanstoa
The core problem with smart accessories is that LMMs levelthatcloselymatchestheperformanceofanLMM.
havelittletonounderstandingofauser’svisualspatialatten-
tion,hamperingtheirunderstandingofuserintent. Imagine 2 RELATEDWORK
a cluttered environment in which a user asks the question,
Ourworkisattheconvergenceoftwoseeminglydisparate
“What is this?” The best guess of which object the user is
fields: largemultimodalmodelsandgaze-contingentgraph-
referring to may be the one centered in the camera image;
icstechniques. Here, webrieflyreviewthemostcloselyre-
however, theopticalaxisofabody-orhead-mountedcam-
latedworkinbothareas.
eradoesnotnecessarilycoincidewiththeuser’sattention.
Gaze is an intuitive selection mechanism and has been
Multimodal Large Language Models (LMMs) A com-
shown to be more effective than other ways of pointing in
prehensive overview of LMMs and multimodal foundation
certain conditions (see Sec. 2). Motivated by this insight,
models in general can be found in the recent survey by Li
we introduce GazeGPT as a new user interaction paradigm
et al. [2023]. These models extend the capabilities of large
forcontextualAI.ContextualAIusesmultimodalinput,in-
language models by accepting not only text but also image
cluding text, images, and video, to understand a user’s in-
dataasinput. TheperformanceofLMMsistypicallybench-
tent and enhance their capabilities. GazeGPT adds another
markedinavarietyoftasks,includingsimulatedexamsand
modalitytotheAI’sinput:thevisualspatialattentionofthe
mathematicalreasoningaswellasnaturalimage,audio,and
userasindicatedbytheirpointofgaze. Thepointofgazeis
videounderstanding.
measured by an eye tracker and used in conjunction with
world-facing cameras to identify the object or region that
the user fixates. A multiscale image crop centered around Gaze-basedUserInterfaces(UI) Withtheemergenceof
thispointisprovidedtotheLMMasinput. Toevaluatethe eyetrackingtechnology,gaze-basedhuman–computerinter-
ideaofgaze-contingentcontextualAIandassessitseffective- action techniques have gained much popularity [R. J. Jacob
ness, we build a prototype wearable-computing evaluation 1990; Majaranta and Bulling 2014; Plopski et al. 2022]. For
platformthatincludesahigh-resolution,wide-field-of-view, example, gaze-based selection was shown to be faster than
world-facingcamera,amicrophoneandspeech-to-textcapa- mouse-basedselection[SibertandR.J.K.Jacob2000]andto
bilities, a GPT-4V(ision) LMM backend, and text-to-speech outperformhead-basedselectionintermsofspeed,taskload,
output. Moreover,wedesignandconductanumberofuser required head movement, and user preference [Blattgerste
studies. These studies demonstrate that, in the absence of et al. 2018] when used as an interface with a monitor. In
visualfeedback,gazeisafasterandmoreaccurateselection theabsenceofvisualfeedback,gaze-basedselectionwasalso
mechanism than body- or head-based alignment. This in- showntobefasterthanhand-basedpointing[Tanriverdiand
sight indicates that GazeGPT may be a more effective in- R.J.K.Jacob2000],althoughthistrendseemstobereversed
terface for contextual AI than current solutions. We also with visual feedback [Cournia et al. 2003]. Because gaze is
showthatourGazeGPTsystemcanclosethegapintaskper- suchanintuitiveandeffectiveselectionmechanism,itisthe
formance between humans and generative AI. Specifically, primarymeansbywhichusersselectobjectsandUIelements
GazeGPTenablesahumantoachieveclose-to-AI-levelper- inApple’supcomingVisionProMixedRealityHeadset.
formanceindifficultobjectclassificationtasks.Finally,users
consistentlyrankagaze-directedinterfacetoanLLMasmore Eye Tracking and Smart Eyewear Eye tracking and
naturalthanhead-orbody-directedinterfacesinourstudy. gaze-contingentgraphicstechniquesbeyondUIapplications
GazeGPT is an example of human-centric AI: a technol- areacoreaspectofemergingaugmented-andvirtual-reality
ogythatenhances,notreplaces,people’snaturalabilities.We (AR/VR)systems.Forexample,eyetrackinginAR/VRisrou-
showthatitprovidesanintuitiveinterfacebetweenhumans tinelyusedforfoveatedrendering[Fristonetal.2019;Geisler
and LMMs, representing a crucial part of future contextual and Perry 1998; Guenter et al. 2012; Kaplanyan et al. 2019;
AIandsmarteyewear. Patneyetal.2016;Tariqetal.2022;Tursunetal.2019;Kra-
Specificcontributionsofourworkinclude: jancichetal.2023],varifocaldisplaytechniques[Akşitetal.
2017; Dunn et al. 2017; S. Liu et al. 2008; Padmanaban et
• the introduction of the gaze-contingent contextual AI al. 2017], enhancing depth perception [Konrad et al. 2020;
paradigmalongwithaprototypeplatform; Krajancichetal.2020],visioncorrection[Padmanabanetal.
UploadedtoarXivFebruary2024GazeGPT:AugmentingHumanCapabilitiesusingGaze-contingentContextualAIforSmartEyewear • 3
3.2 Implementation
World-facing
camera
Hardware The core hardware of the GazeGPT system is
a Zinn Labs DK1 Evaluation Kit. This tethered pair of
glasses includes an event-sensor-based eye tracker, world-
facing camera, microphone, and speaker (Fig. 2). The eye
trackeroperatesat120Hzandisratedbythemanufacturer
Microphone
ashaving1°accuracyand0.4°precision.Theglassesareteth-
location
Eye-tracking
eredtoalaptopcomputer,inourcaseaRazerBlade15′′2019
cameras
withanIntelI7-10750Hprocessor,whichperformsthegaze
Speaker estimationviatheprovidedZinnLabsSDK.Theworld-facing
location
camera comprises a Sony IMX179 sensor integrated into a
78°diagonal-field-of-viewmoduleandinterfacestotheteth-
ered computer via a USB 2.0 connection. The camera cap-
turesupto8MP(3264×2448)imagesat15Hzandappears
as a webcam on the laptop. The Zinn Labs DK1 provides
Fig.2: TheZinnLabsDK1EvaluationKit. Themajorcomponentsused
intheGazeGPTsystem(microphone,speaker,eyetrackingcameras,and its gaze estimate as a pupil center position and gaze vector
world-facingcamera)arelabeled. relativetoitsworld-facingcamera. Wecalibratethecamera
and project the 3D gaze into the 2D camera image for our
analysis. ALogitechR800PowerpointClickerisusedasthe
2019],andmanyotherapplicationssurveyedbyDuchowski user-interactionbutton.
etal.[2004]andKoulierisetal.[2019].
The work closest to ours is perhaps Meta’s Project
Speechtranscriptionandsynthesis GazeGPTusesOpe-
Aria[Somasundarametal.2023]:adisplay-lesssmartglasses
nAI’s Whisper v2-large model for the speech-to-text
prototype equipped with several microphones and sensors,
transcription. Fortext-to-speechsynthesis,thesystemuses
includingeyetrackersandworld-facingcameras. Whilethe
ElevenLabs’TurboV2model. Atthecostoflatency,wealso
capabilities of this hardware platform are somewhat simi-
supportElevenLabs’MultilingualV2modelincasethetran-
lar to our evaluation platform, the core contribution of our
scribedspeechisnotEnglish.
workis notaspecific hardwareimplementation butthein-
troductionandevaluationofthegaze-contingentcontextual
AIparadigm.Tothebestofourknowledge,thisideahasnot Multimodal Large Language Model By handling the
beendiscussedorevaluatedinpriorwork. text-to-speech and speech-to-text conversions separately,
theGazeGPTsystemonlyrequiresanLMMthatacceptstext
andvisualdata. ThesysteminterfaceswithOpenAI’sGPT-
3 GAZEGPTSYSTEM
4VLMMviaOpenAI’sPythonAPIbyuploadingtextandthe
gaze-centric world view images encoded in base 64 format.
GazeGPT is a hardware and software system that captures
TheGPT-4VLMMreturnsitsresponseastext.Inthefuture,
a world-facing image, the user’s gaze, and the user’s query
as LMMs become more powerful, they may be able to ac-
and then submits these data to a LMM for processing and
cept spoken audio and images, process them, and output a
response.
responseasaudiodirectly.
3.1 UserInterface
MultiscaleCropping Whentheuserposesaquerytothe
A GazeGPT query begins with a button press. At that mo- GazeGPTsystem,an8MPimageiscapturedfromtheworld-
ment,thesystemcapturestheuser’sgazeandanimagefrom facing camera. This image is cropped and scaled to form
the world-facing camera. The two are combined to create three512×512pixelimagescenteredontheuser’sgazewith
a gaze-centric image (described below). While keeping the increasinglybroaderfieldsofview,whicharethenuploaded
buttonhelddown,theuserspeakstheirquery,whichisthen totheGPT-4Vmodel(Fig.3).Thesemultiscaleimagesdirect
transcribed via speech to text upon release. The query and GPT-4V’s attention to the object that the user is looking at
gaze-centricworldviewareuploadedtoGPT-4VviatheOpe- andenableGPT-4Vtoprovideusefulinformationaboutob-
nAIAPI.GPT-4V’sresponseissynthesizedintospeechviaa jectsofvaryingsizeanddistance,aswellasreasonaboutob-
text-to-speechalgorithmandplayedbacktotheuseroverthe jectsbasedontheircontext. Thistechniquegreatlyreduces
speakerintheframes. dataprocessingandtransferrequirements,andresultsinan
UploadedtoarXivFebruary20244 • Konradetal.
approximately 10× reduction in data as compared to using
Full-resolution capture
thefull8MPimage. 3264×2448 px (8.0 Megapixels)
Inasystemthatseekstoachievehuman-levelvisualacu-
ityovertheentirenaturalgazerange,thedatasavingsmay
beevenmoredramatic.Peakhumanvisualacuityisapprox-
imately60cyclesperdegreeatthecenterofthefovea,which
wouldrequireadigitalcamerasystemwith120pxperdegree
tomatch.Maintainingthisangularresolutionovertheentire
naturalgazerange(approximately44°horizontal×33°ver-
ticalforthe75thpercentileofgazeangles[Aizenmanetal.
2023]) plus 2° on each side to account for the radius of the
fovea results in a 5760×4320 px (24.9 MP) image. Instead,
supposeagaze-basedsystemonlycapturesthe±2degreere-
Multiscale foveated capture
gionaroundtheuser’sgazeatthefullresolutionoftheimage 3×512×512 px (0.8 Megapixels)
sensor.This480×480pximagecanbeaugmentedwiththree
successivelyincreasingfieldsofviewthatareeachalsoscaled
to480×480pxandtheresultingmultiscaleimagewouldbe
only 4×480×480 px (0.9 MP). This represents a savings of
over25×,whilestillprovidingtheLMMsufficientinforma-
tiontoanswertheuser’squery. Fig.3:Anillustrationofthemultiscalecaptureconcept.Thenarrowestfield
ofviewgivesadetailedviewofthecarthattheuserislookingat,whilethe
widerfieldofviewimagesprovidehelpfulcontext. Atthesametime,the
totalimagesizeisreducedbyanorderofmagnitude.
Evaluating Latency System latency was measured from
theendoftheuserquestiontothebeginningofthespoken 4.1 Evaluating Selection Mode Speed and Accu-
response. This time was broken into three parts (speech to racy
text, GPT-4V response time, and text to speech) and mea-
Inthisexperiment,weaimtoevaluatetheemergingcontex-
suredinseveraltypicalusecases. Thesystemwasrunning
tual AI selection modes—gaze-, head-, and body-based—in
on200Mbpsdownload/20Mbpsuploadbusiness-classcable
termsoftheiraccuracyandspeedofselectingobjects.Specif-
internetoverWiFi. GPT-4Vresponsetimedominatedover-
ically,weaimtocomparethesemodesintermsofhoweffec-
alllatencyat4.0saverage,whilespeechtotexttook0.9son
tive they are for use in the absence of visual feedback and
averageandtexttospeechtook0.3sonaverage.
without training or learning effects. We compare these se-
lectionmodesagainstsmartphones,thestatusquoinmobile
computinginterfaces. Whilepriorartexistscomparingsub-
4 EXPERIMENTS setsoftheseselectionmodes[SibertandR.J.K.Jacob2000;
Blattgersteetal.2018;TanriverdiandR.J.K.Jacob2000],we
We evaluate gaze-tracking alongside head- and body-based arenotawareofanyworkencompassingthissetspecifically
selection, used by Ray-Ban Meta Glasses and Humane’s AI intheabsenceofvisualfeedback—alikelyrequirementuntil
pinrespectively,tounderstandwhich,ifany,ofthesemodes the emergence of all-day-wearable head-worn displays like
canbeusedtoaugmenthumancapabilities. Tothisend,we mixed-realityheadsets.
evaluateeachselectionmodeintermsofhowaccuratelyand
quickly users can select objects of interest, how accurately
Selection Mode Hardware We used three different sys-
asystemcontrolledbyeachselectionmodecanidentifydog
tems to evaluate the selection modes: for the gaze- and
breedsbasedonimagesalone,andinaqualitativepreference
head-based modes, we used the Zinn Labs DK1; for the
study.
body-basedandsmartphone-likemodes,webuiltcustomde-
Twelveadultsparticipated(agerange18–37,5male)inthe vicesusingthesameIMX179cameraastheZinnLabsDK1
experiments. Participants who wore glasses for vision cor- to ensure comparable image quality and resolution across
rection were asked to remove them for the study, but each modes. The body-based device consisted of a custom 3D-
participantwasscreenedtomakesurethattheycouldseea printed IMX179 camera mount attached to a GoPro Perfor-
1°crossrenderedonthescreenfrom1maway. Eachpartic- mance Chest Mount allowing for vertical tilt of the cam-
ipantgaveinformedconsent. era.Thesmartphone-likedevicecombinedaWaveShare5.5′′
UploadedtoarXivFebruary2024GazeGPT:AugmentingHumanCapabilitiesusingGaze-contingentContextualAIforSmartEyewear • 5
capacitive touch AMOLED display with the IMX179 cam-
Selection Target
era, mounted together via a custom 3D-printed enclosure.
We streamed a live camera feed to the display, mimicking
asmartphonecameraapp.Werenderedagreencircleinthe
centerofthecamerafeed,spanninghalfthenarrowerdisplay
dimension.Thecircleservestoindicatetotheuserthatthey
shouldcenteranobjectandadditionallymakesitslightlyeas-
iertodoso. Apicturecouldbetakenbypressinganywhere
Selection Error Selection Time
onthedisplay. Formoredetails, pleaserefertothesupple- 12 3
ment. (all comparisons)
8 2
StudySetupandStimuli Allvisualstimuliwererendered
on an LG UQ7590 4k 75′′ television placed on a height-
adjustable standing desk, 1 m in front of users. The height
4 1
ofthestandingdeskwasadjustedsuchthattheTVwascen-
teredattheuser’seyeswhiletheywerestanding.Thestudy
wasperformedinadarkroomwithoverheadlightingturned
0 0
offsothatthemainlightsourcewastheTV. Phone- Body- Head- Gaze- Phone- Body- Head- Gaze-
based based based based based based based based
For the duration of the study, users wore the Zinn Labs
Fig.4: Resultsofselectionevaluationforaccuracy(left)andspeed(right)
DK1systemandthebody-mountedGoProchestmount. We
fortheselectiontargetshown(top). Boththephone-andgaze-basedse-
checkedthatthecablesrunningfromtheglassesandbody- lectionmodesachievedhighaccuracy(justunder2°),whilethegaze-based
mounted system did not inhibit motion and adjusted them selectionmodewasthefastestofallthemodes.Significanceisindicatedat
asnecessary.UsersusedaLogitechR800Powerpointclicker
the**p=0.01and***p=0.001levels.ErrorsbarsindicateSE.
toindicateintentforgaze-,head-,andbody-basedselection.
Users were handed the phone-like device when evaluating
selectiondeviceorgazetothecentralcross—orinthecase
phone-based selection. Instead of using the clicker, users
ofthephone,toreturnittotheirside(toaccountforthefact
tappedthetouchscreentoindicateintent.
thatinareal-worldscenario,theirphonewouldlikelybein
Theselectiontargetconsistedofa1.06°cross(×)thatap- theirpocketoronatable). Theuserthentappedtheinter-
pearedblackwhenfirstrendered(Fig.4).Thecrosswassur- actionbuttonorscreen. Thetextwoulddisappear,andafter
roundedby4ArUcomarkersthatwereusedtoregisterthe a random interval (2, 3, or 4 s), the central cross would be
head/body/phonecamerarelativetothedisplaytocompute replacedbyatesttargetatarandomizedlocation.Whenthe
accuracystatistics.Thecrossappearedinoneof25locations user selected the test target, it would first turn yellow, ac-
in a 5×5 grid, spaced 11° apart and centered relative to the knowledgingtheirselection,andthengreenoncetheimages
TV. andgazewerecaptured—userswereinstructedtoremainin
placeuntilthetargetturnedgreen.Thenexttrialwouldthen
Procedure We evaluated the speed and accuracy of the begin.
selection modes with 25 trials in each mode. The order of Once all 25 trials for a given selection mode were com-
modesandthetrialswererandomized, withthetargetsap- pleted,weproceededtothenextselectionmodeuntilallfour
pearingonceineachpossiblelocationforeachmode. werecompleted.
Atthestartofeachselectionmode,userswereinstructed
on how to use that mode, given the phone-like device Analysis Duringthestudy,werecordedthepixellocations
if needed, and allowed to try a few practice selections. inthecameraimageofthetesttarget(ascalculatedbasedon
For the gaze-based mode, the user followed the 5-point the ArUco markers) and the user’s selection. For the gaze-
manufacturer-providedgazecalibrationimmediatelybefore based mode, the selected pixel was given by the Zinn Labs
the trial set to minimize eye-tracker error. They were in- DK1; for the other modes, we took the center pixel of the
structedtolooknaturallytotheobject. Formoredetailsre- camera image. These pixel locations were converted to de-
gardingtheexactinstructionsusedforeachmode,pleasere- greesrelativetotheuser’sheadbasedontheArUcomarkers
fer to the supplement. Once they understood the selection andtheknownlocationofthecurrenttargetontheTV.The
mode,theywouldthencontinuetothemainsetof25trials. user’sselectionerrorwascalculatedasthedistancebetween
Eachtrialbeganwith awaitingscreenreading, “Pressto thetargetandselectionpointsindegrees.
start...” and a central cross. They were asked to reset their We also recorded the time taken for the user to make
UploadedtoarXivFebruary2024
seergeD sdnoceS6 • Konradetal.
their selection. The timer started when the test target ap- inthisexperimentassumingthat, withvisualfeedback, the
pearedandstoppedwhentheyselectedit(i.e.,whenthetar- human will be able to accurately select the dog of interest.
getturnedyellow). Weusethesamestudysetupasinthepreviousstudy.
Foreachuserandmode,theselectionerrorandtimewere
averagedacrossall25trials.Weanalyzedtheselectionerror
Stimuli We selected a set of 81 officially recognized dog
averages and selection time averages each with a one-way breedsaccordingtotheAmericanKennelClub1forthistask.
repeated-measures analysis of variance (ANOVA) with the
Eachdogbreedhadanassociated700×700pximageofthe
independent variable of selection mode having four levels.
dog on a white background. Of the 81 dog breeds, 25 were
Greenhouse–Geissersphericitycorrectionwasapplied.Post
randomlyselectedtobepartofthetestset.
hoctestswereconductedaspairwiset-testswithBonferroni
Foreachtrial,adogimagefromthetestsetappearedatone
correction applied to the p-values—reported p-values have
of25possiblelocationsina5×5grid,centeredrelativetothe
theBonferronicorrectionfactorapplied.
TVandwith8°spacing(Fig.5).Theremaining80dogimages
paddedthecentralimageequallyonallsidescreatinga9×9
Results The average selection error and time for each grid to emulate natural environments that often have com-
mode can be found in Fig. 4. In terms of error, phone- and peting objects of interest. Each dog image subtended 8° of
gaze-based selection are just below 2° of error, while body- visualangle. Weadded2°ofverticalseparationbetweenad-
andhead-basedareabove7°.Ontheotherhand,forselection jacentimagestomaintainvisuallyuniformverticalanduni-
time,phone-basedistheslowestataround2.5s,body-based formspacingbetweendogs.
isabout1.5s,head-based≈1.1s,andgaze-based≈0.8s.
TheANOVAshowsasignificationeffectofmodeforboth Procedure Weevaluatedeachselectionmodewithsetsof
selection error (F = 36.12, p < 0.001) and time 25trials.Theorderofthemodesandtrialswasrandomized,
1.93,21.19
(F = 81.01, p < 0.001). Therefore we conduct withthecentraldogimagelocationappearingonceineach
1.36,14.94
follow-uppairwiset-testsfortheposthocanalyses. possiblelocationforeachmode.
For selection error, the t-tests show the following signif- The procedure for this experiment was largely identical
icant effects: phone-based selection has lower error than to that of the previous experiment with the only difference
body-andhead-based(bothp<0.001),gaze-basedselection being the choice of stimulus. At the start of each selection
has lower error than body-based (p < 0.001), and gazed- mode,userswereinstructedonhowtousethatmode,were
based selection lower error than head-based (p < 0.01). allowed to try practice selections, and then proceeded onto
For selection time, all pairwise comparisons are significant themaintrialset. Forthegaze-basedmode,userscalibrated
(p<0.001). theirgaze.
Overall,phone-andgaze-basedselectionhavemuchlower Whenusersclickedawayfromthewaitingscreen,weren-
errorthantheothertwomodes.However,gaze-andphone- deredthegridofdogimagesatoneofthepossible25loca-
based selection are differentiated by the selection time re- tions.Wehighlightedthedogimageofinterestfortheusers
sults, in which phone-based selection is slower than gaze- toselect. Oncetheselectionwasmade,weremovedanyin-
based selection by more than a factor of three. Body- dicationofwhichdogimagewasthetargetandcapturedthe
and head-based selection have unacceptable error, at least image—userswereinstructedtoremaininplaceuntiltheim-
without visual feedback. Additionally, they are also both agewascapturedandthewaitscreenreturned. Onceall25
slower than gaze-based selection, corroborating Blattgerste trials for a given selection mode were completed, we pro-
et al.’s [2018] finding that head-based is slower than gaze- ceededtothenextselectionmodeuntilallthreewerecom-
basedselection. pleted.
After users evaluated each mode, they identified the dog
4.2 AugmentingHumanCapabilities breedsfromthetestsetbasedonimagesalone.Wecreateda
quizthatrenderedeachdogimagefromthetestsetalongside
In this experiment, we evaluate how an LMM paired with
alistofthe81possibledogbreeds,thesameinformationthat
thedifferentselectionmodesfaresonamultimodaltask.We
wasprovidedtoGPT-4Vduringanalysis(seebelow).Partic-
comparetheend-to-enddog-breedidentificationcapabilities
ipants were instructed to identify each dog breed, using a
oftheGazeGPTsystemtothatofhumansonthesametask.
keyboardtonavigatethroughthelistandmakeaselection.
Weexplorehowthechoiceofselectionmodeaffectsthesys-
We did not enforce a time constraint. They identified each
tem’scapabilitiesandwhichmodesareviableforaugment-
dog breed one at a time until all dogs in the test set were
inghumancapabilitiesonthistaskandothers.
classified.
Weusethesamegaze-,head-,andbody-basedsystemsas
inthepreviousexperiment. Weomitthephone-basedmode 1https://www.akc.org/
UploadedtoarXivFebruary2024GazeGPT:AugmentingHumanCapabilitiesusingGaze-contingentContextualAIforSmartEyewear • 7
Image grid Classification Accuracy ofthedogbreedscorrectlybasedontheseimages,showing
thedegradationinperformanceduetothecameraquality.
Image-centered baseline
60% We also evaluated the LMM classification baseline when
Face-centered baseline the camera was pointed directly at each dog’s face. Faces
aresalientfeaturesandweobservedthatmostofourusers
40%
fixatedonthedogs’faces. Withthismodification,theLMM
classified only 52% of the dog breeds correctly. While the
20% multiscaleimagescollectivelycapturethefulldog,thepartof
thedogcapturedinthemostzoomed-inviewclearlyaffects
LMMperformance.
0%
User Body- Head- Gaze-
based based based
Fig.5:Exampleimagesusedfortheclassificationevaluation(left)andthe Results Theaverageclassificationaccuracyforeachmode
resultsoftheevaluation(right).Eachtrialdisplayeda9×9gridofdogim- canbefoundinFig.5.WithoutthehelpofanLMM,userson
agesonawhitebackgroundtoemulateanaturalenvironmentthatmay average identified 22% of dog breeds. Gaze-based selection
havemanycompetingobjectsofinterest.Gaze-basedselectionconsistently
hasthebestaccuracyat40%andbody-andhead-basedare
outperformstheotherselectionmodesandistheonlyonetooutperform
around15%.
the users themselves. Significance is indicated at the **p = 0.01 and
***p=0.001levels.ErrorsbarsindicateSE. TheANOVAshowsasignificationeffectofmode(F =
3,44
36.12, p < 0.001). We conduct follow-up t-tests for the
posthocanalysis,whichshowthatgaze-basedselectionhas
Analysis Asinthepreviousexperiment,werecordedthe significantlyhigheraccuracythanallthreeothermodes(all
pixellocationsoftheuser’sselection(basedongazeorcen- p<0.001).
tralpixel)inthecameraimage.Wegenerateamultiscalecrop Fromthisresultwecanconcludethatgaze-basedselection
(Sec.3.2)aroundtheselectedpixel. Thiscroppedimageand istheonlymodethatcaneffectivelyaugmenthumancapa-
aqueryaskingaboutthedogbreedareuploadedtoGPT-4V. bilities.Theotherselectionmodes,likelyduetotheirhigher
We then count the number of dog breeds identified by the selectionerror,oftenassumetheuserislookingatadifferent
LMMineachmodeandbytheuserandcalculateaccuracies. objectthanintended.
ThequeryaboutdogbreedspresentedtotheLMMlistsall
81 possible choices of dog breed and asks it to choose one 4.3 UserPreference
initsresponse(seeSupplement). Weincludedthebreedsin
Inthisexperiment,usersexperiencedtheend-to-endcontex-
thequeryfortworeasons:first,itsidestepstheissueofwhat
tualAIsystemsinareal-worldenvironmentwiththetaskof
shouldbedoneiftheresponseiscorrectbuttoogeneric(e.g.,
qualitativelyrankingthemalongdifferentmetrics.
“terrier” instead of “Boston terrier”), and second, the user
We populated a desk and wall roughly 1 m in front of
choosesfromthesamelist.
theuserwithcommonobjectslikeplants,tissueboxes,and
We analyzed the classification accuracies with a one-
paintings (see supplement) and users freely inquired about
way ANOVA with the independent variable of selection
theseobjects. Usingasimilarinterfacetotheonedescribed
mode/user having four levels. Post hoc tests were con-
inSec.3.1,usersselectedobjectsusingthegaze-,head-,and
ducted as t-tests with Bonferroni correction applied to the
body-basedmodesandspokeaquestion. ThecontextualAI
p-values—reportedp-valueshavetheBonferronicorrection
would transcribe their speech, upload the query with asso-
factorapplied.
ciatedimagestoGPT-4V,andrespondwithaudio. Afterthe
userselectedanobject,wedisplayedthemultiscaleimageof
Baseline The original set of 25 test dog images was di- thatobjectsotheusercouldadjustbehaviorforsubsequent
rectly uploaded to GPT-4V for baseline analysis with the queries.
query, “Please identify the dog breeds in the following im- Theorderofselectionmodesthattheuserexperiencedin
ages:”.Thedogimageswererenamedsothatthebreedname this setting was randomized per user. Users were asked to
didnotappearinthefilename. Onthisset,theLMMclassi- performmultiplequeriesbeforemovingontothenextmode.
fied84%ofthedogbreedscorrectly. Afterexperiencingallthreemodes,userscouldasktoreturn
Tomeasuretheeffectofcameraquality,wecapturedmul- toanyindividualmode.
tiscale images of the 25 test dog images. The camera was We gave users a forced-choice ranking task between the
placed1mfromthestudyTVtomatchtheusers’studydis- selectionmodesondifferentmetrics:1)whichwasthemost
tance.WeuploadedtheseimagestoGPT-4Vforclassification naturaltoselecttheobjectofinterest,2)howusefultheuser
usingthesamequeryasinthestudy.TheLMMclassified64% could imagine it to be in their daily life, and 3) which pro-
UploadedtoarXivFebruary20248 • Konradetal.
videdthefastestresponseaftertheselectionwasmade. The Naturalness Usefulness Perceived Latency
thirdcriterionwasacontrolquestionsincetheGPT-4Vback-
endwasusedforallmodes.Weexplainedthemetricstousers
beforethestartoftheevaluationandweremindedthemof
thecriteriawhenstartingeachnewmode. Afterexperienc-
ingallofthemodes,usersfilledoutaquestionnaireindicat-
ingtheirrankingsalongwithanyadditionalcomments.
Results AlluserpreferencerankingscanbefoundinFig.6
(blackdots).Thereisacleartrendofgaze-basedselectionbe-
Body- Head- Gaze- Body- Head- Gaze- Body- Head- Gaze-
ingpreferredoverhead-andbody-basedfornaturalnessand
based based based based based based based based based
usefulness. For latency, body-based is perceived as slower,
Fig.6:Preferencestudyresults. Gaze-basedselectionisconsistentlyrated
perhapsduetouserfrustration,buthead-andgaze-basedare
asmorenaturalandusefulthantheothers.Whilethelatencyofthethree
similar. modesshouldbeidentical,itappearsthatuserfrustrationwithbody-based
Friedman tests were significant at the 0.001 level for all selectionaffectedtheirperceptionoflatencyaswell. Significanceisindi-
preferencequestions.Wilcoxonsigned-ranktestswereused
catedatthe*p=0.05and**p=0.01levels.Eachdotrepresentsauser
ranking.
forposthocanalysis,withBonferronicorrectionappliedto
thep-values.Weseethatgaze-basedispreferredoverbody-
real-world object selection in the image domain can be an
based(allquestionsp < 0.01),head-basedispreferredover effectivemethodofprovidingcontexttouserqueries,partic-
body-based (naturalness p < 0.01, usefulness and latency ularlyinon-the-gowearabledevices.
p<0.05),andgaze-basedispreferredoverhead-based(nat-
We evaluated multiple object-selection modes on speed
uralnessp<0.01,usefulnessp<0.05).
and accuracy. The gaze-based selection mode was—with-
outvisualfeedback—asaccurateasaphone-likesystemwith
5 APPLICATIONS
visual feedback and roughly 3× faster. We expect even
GazeGPT is most useful in on-the-go environments where larger differences in speed in real-world use, as a phone in
usersarelookingtolearnmoreinformationaboutanobject apocketisevenlessaccessible.Head-andbody-basedselec-
intheirlineofsight. Thisisparticularlyusefulinscenarios tionmodes,whilefast,didnotmaintaintheaccuracylikely
whereusersdon’thavetimetopullouttheirphone,launch neededforreal-worlduse.
anLMMapp,andtakeaphoto. Thiswascorroboratedinafollow-upstudyinwhichusers
For example, when traveling to a foreign country, a user useddifferentselectionmodestoclassifydogbreedsbasedon
couldglanceatastreetsign,menu,oradvertisementandsay imagesalone. Whilegaze-basedselection(40%correct)out-
“What does this mean?” and the contextual AI would re- performedhumancapabilities(22%correct)onthistask,the
spondintheirear.Wheneatingatarestaurant,ausercould head-andbody-basedselectionmodesfaredworseincom-
askwhichwinecouldpairbestwithameal,actingasasom- parison(botharound15%correct).Inauserpreferenceeval-
melierforthetable. uation,thegaze-basedmodewasheavilyfavoredcompared
GazeGPTcanalsobeusefulforday-to-daylife.Auserata tothehead-andbody-basedmodes.
grocerystorecouldinquireaboutthecaloriesorhealthben-
efitsofavegetable,andthengetrecommendationsformeals
Limitations and Future Work. We observed that the
with that vegetable. Out in the forest, a user could inquire
world-facing camera quality has a significant effect on the
whether a mushroom or berry is edible. For those that are
classificationabilityofthesystem.Occasionally,motionblur
colorblind,GazeGPTcouldbeusedtoansweraquestionsuch
orfocuserrordegradedtheimages.TheIMX179-basedworld
as“Domyclothesmatch?” Severalotherreal-worldscenar-
facingcameraisfairlybasic,andwell-establishedtechniques
iosaredepictedinFig.7.
suchasopticalimagestabilizationandphasedetectionaut-
ofocus could greatly improve performance. At other times,
6 DISCUSSION
thequalityoftheLMM’sresponsewasdegradedsimplydue
Generative AI is already capable of ingesting various types tothelimitedresolutionoftheIMX179sensor,whichcould
of complex data and outputting reasonable responses—and bemitigatedbyusingahigherresolutionsensor.
we expect that to accelerate in coming years. One funda- Secondly, before a GazeGPT system can achieve mass
mental challenge that remains, however, is providing these adoption,theLMMresponselatencywilllikelyneedtobere-
modelswithcontextaboutwhatisimportanttothehuman ducedsignificantly. ThecurrentGazeGPTsystemmaintains
user. Much like prompt engineering can guide AI models, a latency of approximately 5.2 s. This includes speech-to-
UploadedtoarXivFebruary2024
gniknaR
tseB
tsroWGazeGPT:AugmentingHumanCapabilitiesusingGaze-contingentContextualAIforSmartEyewear • 9
text(STT),LMMresponse,andtext-to-speech(TTS)models, Geisler, Wilson S and Jeffrey S Perry (1998). “Real-time foveated
allprocessedinthecloud. Bringingthesemodelsondevice multiresolution system for low-bandwidth video communica-
to reduce latency may be feasible for the smaller STT and tion”.In:HumanvisionandelectronicimagingIII.Vol.3299.SPIE,
TTSmodels,thoughtheLMMwillnecessarilyremaincloud- pp.294–305.
basedinthenearfuture. Guenter,Brianetal.(2012).“Foveated3Dgraphics”.In:ACMtrans-
Finally, a relatively simple dog-breed-classification task
actionsonGraphics(tOG)31.6,pp.1–10.
Jacob, Robert JK (1990). “What you look at is what you get:
wasusedtodemonstratehowGazeGPTaugmentshumanca-
eyemovement-basedinteractiontechniques”.In:Proceedingsof
pabilities. FutureworkcouldusestandardLMMevaluation
the SIGCHI conference on Human factors in computing systems,
benchmarks, such as the MMMU (Massive Multi-discipline
pp.11–18.
MultimodalUnderstanding)[Yueetal.2023],toevaluatethe Kaplanyan,AntonSetal.(2019).“DeepFovea:Neuralreconstruc-
benefitsofgaze-contingentAIonabroadersetoftasks.The tionforfoveatedrenderingandvideocompressionusinglearned
MMMUcontains11.5kquestionsacross30subjects,however, statistics of natural videos”. In: ACM Transactions on Graphics
soanadequateevaluationonmanyuserswouldbewellbe- (TOG)38.6,pp.1–13.
yondthescopeofthispaper. Konrad,Robertetal.(2020).“Gaze-contingentocularparallaxren-
dering for virtual reality”. In: ACM Transactions on Graphics
Conclusion Human-centric AI technologies, such as
(TOG)39.2,pp.1–12.
Koulieris, George Alex et al. (2019). “Near-eye display and track-
GazeGPT,provideanopportunityforuserstodirectlybenefit
ingtechnologiesforvirtualandaugmentedreality”.In:Computer
fromthequicklyevolvingcapabilitiesofgenerativeAImod-
GraphicsForum.Vol.38.2.WileyOnlineLibrary,pp.493–519.
els in everyday tasks. One can only imagine the myriad of
Krajancich, Brooke et al. (2020). “Optimizing depth perception in
creativeapplicationsGazeGPTmayenablewhenusersstart virtual and augmented reality through gaze-contingent stereo
toseetheworldthroughthelensofcontextualAI. rendering”.In:ACMTransactionsonGraphics(TOG)39.6,pp.1–
10.
REFERENCES – (2023).“TowardsAttention–awareFoveatedRendering”.In:ACM
Achiam,Joshetal.(2023).GPT-4TechnicalReport.arXiv:2303.08774
TransactionsonGraphics(TOG)42.4,pp.1–10.
Li,Chunyuanetal.(2023).“Multimodalfoundationmodels:From
[cs.CL].
Aizenman, Avi M. et al. (Jan. 2023). “The Statistics of Eye Move- specialists to general-purpose assistants”. In: arXiv preprint
mentsandBinocularDisparitiesduringVRGaming:Implications
arXiv:2309.100201.2,p.2.
forHeadsetDesign”.In:ACMTrans.Graph.42.1.issn:0730-0301.
Liu,Haotian,ChunyuanLi,YuhengLi,etal.(2023).ImprovedBase-
doi:10.1145/3549529.url:https://doi.org/10.1145/3549529.
lineswithVisualInstructionTuning.
Liu,Haotian,ChunyuanLi,QingyangWu,etal.(2023).“VisualIn-
Akşit,Kaanetal.(2017).“Near-eyevarifocalaugmentedrealitydis-
playusingsee-throughscreens”.In:ACMTransactionsonGraph-
structionTuning”.In:NeurIPS.
Liu,Shengetal.(2008).“Anopticalsee-throughheadmounteddis-
ics(TOG)36.6,pp.1–13.
Anil,Rohanetal.(2023).“Gemini:afamilyofhighlycapablemulti- play with addressable focal planes”. In: 2008 7th IEEE/ACM In-
modalmodels”.In:arXivpreprintarXiv:2312.11805. ternational Symposium on Mixed and Augmented Reality. IEEE,
pp.33–42.
Blattgerste,Jonasetal.(2018).“AdvantagesofEye-GazeoverHead-
Majaranta,PäiviandAndreasBulling(2014).“Eyetrackingandeye-
Gaze-Based Selection in Virtual and Augmented Reality under
Varying Field of Views”. In: Workshop on Communication by
basedhuman–computerinteraction”.In:Advancesinphysiologi-
GazeInteraction.COGAIN’18.NewYork,NY,USA:Association
calcomputing.Springer,pp.39–65.
Moon,Seungwhanetal.(2023).“Anymal:Anefficientandscalable
for Computing Machinery. isbn: 9781450357906. doi: 10.1145/
3206343.3206349.url:https://doi.org/10.1145/3206343.3206349. any-modality augmented language model”. In: arXiv preprint
Cournia,Nathanetal.(2003).“Gaze-vs.hand-basedpointinginvir-
arXiv:2309.16058.
Padmanaban,Nitishetal.(2017).“Optimizingvirtualrealityforall
tualenvironments”.In:CHI’03extendedabstractsonHumanfac-
usersthroughgaze-contingentandadaptivefocusdisplays”.In:
torsincomputingsystems,pp.772–773.
Duchowski, Andrew T et al. (2004). “Gaze-contingent displays: A
ProceedingsoftheNationalAcademyofSciences114.9,pp.2183–
2188.
review”.In:Cyberpsychology&behavior7.6,pp.621–634.
Padmanaban, Nitish et al. (2019). “Autofocals: Evaluating gaze-
Dunn,Davidetal.(2017).“WideFieldOfViewVarifocalNear-Eye
DisplayUsingSee-ThroughDeformableMembraneMirrors”.In:
contingenteyeglassesforpresbyopes”.In:ScienceAdvances5.6,
pp.1–10.
IEEETransactionsonVisualizationandComputerGraphics 23.4,
Patney,Anjuletal.(2016).“Towardsfoveatedrenderingforgaze-
pp.1322–1331.doi:10.1109/TVCG.2017.2657058.
Friston,Sebastianetal.(2019).“Perceptualrasterizationforhead-
trackedvirtualreality”.In:ACMTransactionsonGraphics(TOG)
35.6,pp.1–12.
mounteddisplayimagesynthesis.”In:ACMTrans.Graph.38.4,
Plopski,Alexanderetal.(Mar.2022).“TheEyeinExtendedReality:
pp.97–1.
ASurveyonGazeInteractionandEyeTrackinginHead-Worn
UploadedtoarXivFebruary202410 • Konradetal.
ExtendedReality”.In:ACMComput.Surv.55.3.issn:0360-0300.
doi:10.1145/3491207.url:https://doi.org/10.1145/3491207.
Sibert, Linda E. and Robert J. K. Jacob (2000). “Evaluation of Eye
Gaze Interaction”. In: Proceedings of the SIGCHI Conference on
HumanFactorsinComputingSystems.CHI’00.TheHague,The
Netherlands: Association for Computing Machinery, pp. 281–
288.isbn:1581132166.doi:10.1145/332040.332445.url:https:
//doi.org/10.1145/332040.332445.
Somasundaram, Kiran et al. (2023). “Project Aria: A New Tool
for Egocentric Multi-Modal AI Research”. In: arXiv preprint
arXiv:2308.13561.
Tanriverdi,VildanandRobertJ.K.Jacob(2000).“Interactingwith
EyeMovementsinVirtualEnvironments”.In:Proc.SIGCHI.CHI
’00.NewYork,NY,USA:AssociationforComputingMachinery,
pp.265–272.isbn:1581132166.doi:10.1145/332040.332443.url:
https://doi.org/10.1145/332040.332443.
Tariq,Taimooretal.(2022).“Noise-basedenhancementforfoveated
rendering”.In:ACMTransactionsonGraphics(TOG)41.4,pp.1–
14.
Tursun, Okan Tarhan et al. (2019). “Luminance-contrast-aware
foveated rendering”. In: ACM Transactions on Graphics (TOG)
38.4,pp.1–14.
You,Haoxuanetal.(2023).Ferret:ReferandGroundAnythingAny-
whereatAnyGranularity.arXiv:2310.07704[cs.CV].
Yue,Xiangetal.(2023).“MMMU:AMassiveMulti-disciplineMul-
timodal Understanding and Reasoning Benchmark for Expert
AGI”.In:arXivpreprintarXiv:2311.16502.
UploadedtoarXivFebruary2024GazeGPT:AugmentingHumanCapabilitiesusingGaze-contingentContextualAIforSmartEyewear • 11
Scene Egocentric view Multiscale cropping
gaze location
Query Contextual AI Response
What does this say? The text says “kein zutritt
ausser für befugte” which
translates to “No entry except
to authorized persons” in
English.
Query Contextual AI Response
What’s a wine that would A light-bodied Chardonnay
pair well with this? or a crisp Sauvignon Blanc
would pair well with this
soft cheese.
Query Contextual AI Response
[in Romanian] [in Romanian]
What is this character’s name? This is Zuma, a character from
the Paw Patrol cartoon.
Query Contextual AI Response
How tall do these typically get? Sequoia trees can grow over
200 feet tall.
Fig.7: ApplicationsofGazeGPT.TheGazeGPTsystemexcelsatawidevarietyoftasks, includinggeneralknowledge, contextualrecommendations,
translation,andevenoperatesinmultiplelanguages.
UploadedtoarXivFebruary2024
noitalsnarT
gniriap
eniW
laugnilitluM
stcaf
erutaN