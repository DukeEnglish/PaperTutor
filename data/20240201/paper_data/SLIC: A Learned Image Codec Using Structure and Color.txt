SLIC: A Learned Image Codec Using Structure and Color
Srivatsa Prativadibhayankaram§†⋆, Mahadev Prasad Panda§†⋆ , Thomas Richter§,
Heiko Sparenberg§¶, Siegfried Fo¨ßel§, Andr´e Kaup†
§Fraunhofer Institute for Integrated Circuits IIS, Erlangen, Germany
†Friedrich-Alexander-Universit¨at Erlangen-Nu¨rnberg, Erlangen, Germany
¶RheinMain University of Applied Sciences, Wiesbaden, Germany
first.last@iis.fraunhofer.de first.last@fau.de first.last@hs-rm.de
Abstract
We propose the structure and color based learned image codec (SLIC) in which the task
of compression is split into that of luminance and chrominance. The deep learning model
is built with a novel multi-scale architecture for Y and UV channels in the encoder, where
the features from various stages are combined to obtain the latent representation. An au-
toregressive context model is employed for backward adaptation and a hyperprior block for
forward adaptation. Various experiments are carried out to study and analyze the perfor-
manceoftheproposedmodel, andtocompareitwithotherimagecodecs. Wealsoillustrate
theadvantagesofourmethodthroughthevisualizationofchannelimpulseresponses, latent
channels and various ablation studies. The model achieves Bjøntegaard delta bitrate gains
of 7.5% and 4.66% in terms of MS-SSIM and CIEDE2000 metrics with respect to other
state-of-the-art reference codecs.
Introduction
Development of learned image compression methods has accelerated of late. There
are some methods that outperform traditional image codecs such as JPEG [1] or
the intra-frame coding mode of traditional video codecs such as HEVC [2] and VVC
[3]. But the complexity and energy consumption of learned image codecs are many
orders of magnitude higher than that of traditional codecs [4]. A large number of
learned image codecs follow the non-linear transform coding approach introduced in
[5]. The analysis transform converts an image from the data space to a latent space.
This latent representation is quantized to perform irrelevancy reduction and then
compressed to a compact form by an entropy coder. The synthesis transform decodes
and reconstructs the image. The rate-distortion optimization of such a model can be
represented as
min {L},with L(θ,ϕ) = R(θ)+λ·D(θ,ϕ), (1)
θ,ϕ
where L represents the loss term, R is the rate measured in bits per pixel, D is the
distortion term and λ is the Lagrangian multiplier. The symbols θ and ϕ indicate
the learnable parameters of the analysis and the synthesis transforms respectively.
Several works targeting various aspects of learned image compression have been
developed recently. While some focus on architecture, others develop better context
modeling and entropy coding methods. The work in [6] outperforms many state-
of-the-art image codecs including VVC all-intra mode. A codec that makes use of
transformers is developed in [7]. A novel implicit neural representation based codec is
⋆ Equal contribution
4202
naJ
03
]VI.ssee[
1v64271.1042:viXraintroduced in [8], but the results are not on par with state-of-the-art codecs. A multi-
scale skip connection based encoder can be seen in [9]. The work in [10] employs
a Gaussian mixture model for better entropy coding, including an autoregressive
context model.
Most learned image codecs operate in the RGB color space. However, there are a
fewlearnedcodecsthatoperateinYUVcolorspace[11,12]. Inourpriorwork[12], we
developed a model that contains two branches - one for capturing structure from the
luminance or Y channel, and color from chrominance or UV channels. In this work,
we adapt the split luma and chroma branches from the color learning model [12] to
the Cheng2020 [10] model architecture and make various improvements. Firstly, we
haveamulti-scaleencoderblock, wherefeaturesfromvariousstagesintheencoderare
combined. Secondly,wereplacesomeoftheconvolutionallayersinthehypersynthesis
transform by sub-pixel convolution layers that help with better prediction of the
latent distribution. Thirdly, we make use of an autoregressive context model, along
with an entropy parameter estimation module for backward adaptation, resulting in
significant bitrate savings. Finally, instead of the parameter heavy residual attention
blocks used in Cheng2020 [10], we use shuffle attention [13] blocks.
Our main contributions in this work can be outlined as reduction in model com-
plexity with a novel architecture and a better structural as well as color fidelity in
reconstruction of images resulting in competitive performance. We illustrate the ben-
efits of our model through various experimental results and ablation studies. We
also compare the performance of proposed SLIC model with other codecs – both
traditional and learned, and report our findings.
Structure and Color Based LIC
In this section, we look into the details of the proposed structure and color based
learned image codec (SLIC). As mentioned, our model is built based on [10] and our
prior work in [12]. The model has an asymmetric architecture, where the encoder has
a higher number of parameters in comparison to the decoder. Additionally, there is
an autoregressive context model added to both luminance and chrominance branches.
The block diagram is shown in Fig.1. It should be noted that all the components are
instanced twice - once for luminance (Y) and once for chrominance (UV) channels.
Network Architecture: In the analysis transform blocks, we make use of a multi-
scale architecture. The features from various stages of the analysis transform layers
are tapped and finally combined. The residual up and down convolution blocks are
the same as in Cheng2020 model. In contrast to [10], we make use of shuffle attention
[13] layers instead of the residual attention in both analysis and synthesis transforms.
Residual attention consists of 337,536 parameters in comparison to shuffle attention
layer that has only 48 parameters. It has been experimentally shown in [13] that,
shuffle attention layer behaves as a lightweight plug-and-play block, that improves
the model performance in various convolutional neural network architectures. In the
hyper synthesis transform, we make use of sub-pixel convolution in addition to con-
volution layers. The sub-pixel convolution is an implementation of deconvolution
layer where, a shuffling operation is performed after a standard convolution in low-
resolution space. Our autoregressive context block consists of a masked convolutionAnalysis transform (𝑔#) Hyper analysis transform (ℎ#)
Conv(5, 8),
GDN
Conv(3, 4),
GDN
𝑧!/𝑧"
Conv(1, 2), Q
GDN
AE
𝑥!/𝑥" Q Context
𝑦’!/𝑦’" Model Hyper synthesis transform (ℎ$)
AD
AE
Synthesis transform (𝑔$)
𝑧!̂/𝑧"̂
𝑥#!/𝑥#" AD
𝑦’!/𝑦’"
Figure 1: Network architecture of the proposed SLIC model. Q represents the quan-
tizer, AE and AD indicate arithmetic encoder and decoder respectively.
layer with a kernel of size 5×5, similar to the model in [10]. However, we do not use
a Gaussian mixture model for estimating the latent probability distribution. The en-
tropy parameter estimation block consists of three convolutional layers and generates
the predicted mean (µ) and scale (σ) of the latent yˆ.
Loss Function: As distortion metrics, we use mean squared error (MSE) and
multi-scale structural similarity index measure (MS-SSIM) [14] for structural fidelity.
Similar to our prior work, we use the color difference metric CIEDE2000 (∆E12) [15]
00
to optimize our model for color fidelity. This metric operates in LAB color space with
three components, namely luminosity, color, and hue to compute the color difference
between two given pixel triplet values. The final loss function based on (1), to train
the model is :
min {L},with L(θ,ϕ) = R+λ ·MSE(·)+λ ·(1.0−MS-SSIM(·))+λ ·∆E12(·), (2)
θ,ϕ 1 2 3 00
where λ ,λ , and λ are the Lagrangian multipliers for the metrics MSE, MS-SSIM
1 2 3
and CIEDE2000 respectively. It should be noted that MSE and MS-SSIM are es-
timated in the RGB color space. R indicates the total bitrate and consists of four
components, namely luma and chroma hyperprior bits, as well as luma and chroma
latent bits.
Implementation Details
The SLIC model was implemented in Python programming language using PyTorch1
and CompressAI2 frameworks. The training data comprised around 118,000 images
from the COCO20173 training dataset. As validation data, 5,000 randomly cho-
sen images from the ImageNet dataset were used. The model was trained for var-
1https://pytorch.org
2https://github.com/InterDigitalInc/CompressAI
3https://cocodataset.org
nwoD
laudiseR
kcolB
vnoC
)1,1(vnoC
nwoD
laudiseR
kcolB
vnoC
pU
laudiseR
kcolB
vnoC
noitnettA
elffuhS
pU
laudiseR
nwoD
laudiseR
kcolB
vnoC
kcolB
vnoC
noitnettA
elffuhS
nwoD
laudiseR
kcolB
vnoC
pU
laudiseR
kcolB
vnoC
pU
laudiseR
etanetacnoC
kcolB
vnoC
noitnettA
elffuhS
NDG
,)1 ,1(vnoC
noitnettA
elffuhS
)"𝜎/!𝜎,"𝜇/!𝜇(
"𝜏/!𝜏
yportnE retemaraP
"𝛾/!𝛾
,)2
,3(vnoC
lexiPbuS
ULeRykaeL
,)1
,3(vnoC
ULeRykaeL
ULeRykaeL
,)1
,3(vnoC
ULeRykaeL
,)1
,3(vnoC
ULeRykaeL
,)2
,3(vnoC
,)2
,3(vnoC
lexiPbuS
ULeRykaeL
,)1
,3(vnoC
ULeRykaeL
ULeRykaeL
,)1
,3(vnoC
ULeRykaeL
,)2
,3(vnoC
ULeRykaeL
,)1
,3(vnoC
prior
Factorizedious bitrate configurations with the ReduceOnPlateau learning rate scheduler, star-
ing with 1e − 4. For every configuration, the model was trained for 120 epochs,
with images cropped to 256 × 256 and a batch size of 32. The Lagrangian mul-
tiplier values were chosen experimentally, based on the range of metric values as:
λ = {0.001,0.005,0.01,0.02} for MSE, λ = {0.01,0.12,2.4,4.8} for MS-SSIM and
1 2
λ = {0.024,0.12,0.24,0.48} for CIEDE2000, similar to [12]. Additionally, since the
3
color difference metric CIEDE2000 considers two pixel values, it was modified to work
with large batches of image data in the form of tensors efficiently.
The total number of parameters in our SLIC model is around 15 million, whereas
Cheng2020 model consists of approximately 30 million parameters in the highest
bitrate configuration. In terms of kilo multiply-accumulate operations (kMACs) for
each pixel, SLIC needs 829.72 kMAC/pixel and Cheng2020 model needs 1033.75
kMAC/pixel for an end-to-end forward pass.
Model workflow
The encoder comprises the analysis, hyper analysis and hyper synthesis transform
blocks, context model, and the entropy parameter estimation module. The input
image x is converted from RGB to YUV color space. The YUV image is split into two
components x and x , which are the luminance and chrominance components. The
L C
non-linear analysis transform g transforms the inputs into the latent representations
a
y and y respectively. In order to estimate the distribution of the latents, the
L C
hyperanalysis transform h transforms them into hyperlatents z and z . These
a L C
hyperlatents are quantized and entropy coded with the factorized prior that is learnt
during training. The autoregressive context models generate τ and τ to help encode
L C
eachlatentvalueandarethencombinedwiththeoutputofhypersynthesistransforms
γ and γ by the entropy parameter estimation modules to obtain mean and scale
L C
values. They are then used to perform arithmetic coding of the quantized latents yˆ
L
and yˆ .
C
The decoder consists of hyperprior blocks, context models, entropy parameter es-
timation blocks and the synthesis transforms. The decoding process starts with the
recovery of hyperlatents zˆ and zˆ . They are decoded by the hyper synthesis trans-
L C
form h to obtain γ and γ . The context models start with all zeros and iteratively,
s L C
contexts are estimated for each latent pixel based on the previously decoded latent
values. The estimated mean and scale values are then used for entropy decoding and
obtain the quantized latents yˆ and yˆ . Followed by this, they are transformed back
L C
into the image space by the synthesis transform g . The reconstructed luma channel
s
isxˆ andthechromacomponentisxˆ . Finally, theyareconcantenatedandconverted
L C
from YUV to RGB color space, which gives us the final reconstructed image xˆ.
Experiments and Results
In this section, we provide details about the various experiments that were con-
ducted and illustrate the properties of the proposed SLIC model. We start with the
rate-distortion performance and compare SLIC’s performance with other methods.
Followed by this, we make a visual comparison of image patches reconstructed by
various codecs. Then we present visualization of the predicted latent distributions.38 20 5
36 18
34 16 4 C Hh ypen eg rp2 r0 i2 o0 r
32 14 FactorizedPrior
3 VTM
30 12 CL(prior)
SLIC(ours)
28 10 2
26 8
0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1
Rate(bpp) Rate(bpp) Rate(bpp)
Figure 2: RD curves compared with various codecs for the Kodak dataset.
Finally, we discuss the channel impulse response computed for each latent channel
and compare it to that of Cheng2020 model.
Rate-distortion performance
The model was trained for various bitrate configurations. We measured the rate
and distortion values for the Kodak dataset (24 images) spanning various bitrates
in the range 0 to 1 bits per pixel (bpp). The distortion metrics PSNR, MS-SSIM,
and CIEDE2000 are considered for comparison. The RD values are measured and
averaged over all the images for each bitrate configuration. A comparison is made
with Factorized Prior [5], Hyperprior [16], Cheng2020 [10], CL model [12], and VVC
referencesoftwareVTM1 [3]. TheRDcurvesareshowninFig. 2. Notethatforbetter
readability, the MS-SSIM values are converted using −10×log (1−MS-SSIM) to a
10
decibel (dB) scale.
In terms of PSNR, our model is comparable to the Hyperprior model and worse
than VTM and Cheng2020. But with MS-SSIM curves, our model is comparable
to VTM and Cheng2020 at bitrates less than 0.5 bpp. For the range between 0.5
and 0.8 bpp, we see our model clearly outperforming the rest of the codecs under
consideration. Looking at the CIEDE2000 curves, it can be inferred that our model
has the best performance at bitrates larger than 0.2 bpp. This highlights the benefit
of optimizing the model for color fidelity.
We also compared the Bjøntegaard delta bitrate (BD-BR) [17] and distortion
values with VTM as the baseline. The comparison is made for the metrics considered
above and reported in Table I. In terms of PSNR, Cheng2020 seems to perform the
best having 3.4% gain in BD-BR and 0.15 dB in BD-PSNR values. But in case of
MS-SSIM, see a gain of 7.5% in BD-BR and 0.21 in with BD-MS-SSIM. The BD-BR
gain for the proposed SLIC model is the highest for CIEDE2000, with a value of
4.66%. This is significantly better when compared to the other codecs.
Visual comparison of images
The main goal of the split luma and chroma architecture is to optimize for structural
and color fidelity. Here we illustrate it with an image for visual comparison of quality.
We use the image ClassD APPLE BasketBallScreen 2560x1440p 60 8b sRGB.png,
an image composed of natural and synthetic regions, taken from JPEG XL test data.
WecomparethedecodedimagesfromCheng2020 andourSLICmodel, compressedat
abitrateofaround0.3bpp. Twopatchesofsize128×128intheimageareconsidered,
1https://vcgit.hhi.fraunhofer.de/jvet/VVCSoftware_VTM
↑)Bd(RNSP ↑)Bd(MISS-SM ↓0002EDEICTable I: BD-Rate and BD-Distortion comparison with different codecs for the Kodak
dataset.
PSNR MS-SSIM CIEDE2000
Codec Name
BD-BR BD-PSNR BD-BR BD-MS- BD-BR BD-1
(%) (dB) (%) SSIM (%) /CIEDE2000
SLIC (Ours) 21.74 -0.83 -7.50 0.21 -4.66 0.0081
Cheng2020 [10] 3.40 -0.15 -3.32 0.13 20.82 -0.0175
Hyperprior [16] 38.18 -1.39 7.91 -0.34 67.68 -0.0539
Factorized Prior [5] 78.16 -2.35 15.05 -0.59 91.62 -0.0765
Cheng2020 Original SLIC (ours)
37.93, 0.9845, PSNR ↑, MS-SSIM ↑, 37.97, 0.9853,
0.88 CIEDE2000↓ 0.81
33.18, 0.9974, 33.73, 0.9978,
1.00 1.03
Figure 3: Comparison of reconstructed image patches from SLIC and Cheng2020,
compressedatabitrateofaround0.3bpp. (Bestwhenviewedenlargedonamonitor.)
shown in blue and green boxes in Fig. 3. The quality metrics are provided with the
reconstructed patches. Looking at the image crop in blue, which mainly consists
of text and icons, it can be seen that the text is reconstructed fairly well by both
models. However, on closer inspection, the tiny box with words “abc” are smudged
in the Cheng2020 image, but are legible in the SLIC image. We also consider a
region with natural content indicated by the green box, which is a cat face. Here we
observe that the highly textured regions are smoothed in both cases. But the complex
textures are better preserved by our model in comparison to Cheng2020.
Visualization of predicted distributions of latents
Similar to the visualization in [10], we have illustrated the effect of different entropy
models in Fig. 4. We used the kodim21.png from Kodak dataset as a test image.
Here we visualize the latent channels and entropy of the proposed SLIC, Hyperprior
[16], and that of Cheng2020 [10] models depicted in each row. The most contributing
latent channel in terms of bitrate, or in other words the channel with highest entropy
is visualized for each codec. The first two rows represent the luma and chroma
branches of our SLIC model. The Cheng2020 results are shown in the third row. The
fourth row consists of results from the Hyperprior [16] model with mean and scale
hyperprior.
The latent channel (yˆ) for each codec is visualized in the first column. The pre-
dicted mean µ and variance σ values for the latent channel are shown in the second
and third columns respectively. We see that the predicted mean µ has structure
similar to the latent yˆ. The regions not captured by the predicted mean, appear in
the visualization of scale σ, shown in column 3. The scale visualization shows lowerLatent Mean Scale Remaining redundancy Required bits Average bits
Figure 4: Latent visualization of proposed SLIC, Cheng2020[10], and Hyperprior [16]
models for the image kodim21.png. (Best when viewed enlarged on a monitor.)
values at smoother regions and higher values at edges and highly textured areas. It
can be clearly observed, that our model, as well as Cheng2020 have sparse and lower
values in the scale visualization in column 3. But they are higher and denser for the
Hyperprior model. This can be attributed to the causal context modeling used in
both SLIC and Cheng2020 models.
The normalized values representing the remaining redundancy not captured by
the mean or scale predictions are visualized in the fourth column. Their values are
measured by yˆ−µ. The required bits for encoding each pixel in the latent channel is
σ
computed as −log (p (yˆ|zˆ)) using the predicted probability distribution and visual-
2 yˆ|zˆ
ized in the fifth column, where zˆ represents the decoded hyperlatent. It provides an
insight into the number of bits required to encode the remaining redundancy. Lower
redundancy enables lesser number of bits for coding. Finally, the average number of
bits required per channel for each latent pixel, shown in the last column is computed
using −1 (cid:80) log (p (yˆ|zˆ)) where, i = {0,1,...,N − 1} and N is the number of
N i 2 yˆi|zˆi i i
latent channels.
Although column 5 gives an overview with regards to the required bits, it is
specific to the channel with the highest entropy. In order to get a complete picture ,
we compute the average bits for each latent pixel. In column 6, it can be seen that
more bits are required to encode highly textured regions. We observe that structured
regions require higher number of bits in the luma part, shown in row 1. However, the
regions with large change in color values need more bits in the chroma component,
as seen in row 2. The benefits of encoding the luma and chroma latents individually
with separate entropy models can thus be seen. We observe this behavior with all
bitrate configurations.
Channel Impulse Response
Based on our prior work [12], we compute the channel impulse response of the pro-
posed SLIC model and compare it with that of Cheng2020. The channels are sorted
in decreasing order of their bitrate contributions, measured using R = −log (p ) for
n 2 n
a channel n using the prior probability p . The low frequency components appear
n
CILS
CILS
0202gnehC
elacS
naeM
)sruo(
amuL
)sruo(
amorhC
roirprepyHCheng2020
Luma (ours) Chroma (ours)
Luma (ours)
Figure 5: Impulse responses of image ClassA 8bit BIKE 2048x2560 8b RGB.png.
first, followed by the higher frequencies. We observe a mixture of color and structure
in the impulse response of the Cheng2020 model. Whereas, in our model we have
a separation into structure in luminance and color in chrominance components. For
luma channel impulse response, we see similarity with linear orthogonal transforms
such as discrete cosine transform (DCT).
Ablation Study
We report two ablation studies on our model. Firstly, we studied the effect of various
loss functions on the model performance. Secondly, we evaluated variants of the
context model. For all the experiments, we used the same model architecture and
training environment as described in the previous section, unless stated. The Kodak
dataset was used for evaluating the experiments.
Effect of loss function
We initially trained our model with the MSE distortion metric. Followed by this,
we trained our model with a combination of MSE and CIEDE2000 metrics. Finally,
we trained the model with MSE, MS-SSIM, and CIEDE2000 metrics, as in (2). The
findings on the effect of loss function on RD performance is shown in Fig. 6. It
can be observed that using the color difference metric in the loss function not only
improves the color fidelity, but also the structural fidelity. This is evident from the
MS-SSIM curves. However, using MS-SSIM in addition to the other two metrics,
further improves the performance. Having MS-SSIM additionally in the loss term
does not seem to impact PSNR or CIEDE2000 values.
Effect of Context Model
We compare three configurations of the SLIC model, namely without context model,
context block in luma branch only, and context in both luma and chroma branches.
A total of 12 models (four per variant) were trained. We report the RD performance34 18
4
32 16
30 14 3
MSE 12
28 MSE+CIEDE2000 2
MSE+CIEDE2000+MS-SSIM 10
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.1 0.2 0.3 0.4 0.5 0.6 0.7
Rate(bpp) Rate(bpp) Rate(bpp)
Figure 6: RD performance for different loss functions.
34 18 3.5
32 16 3
30 14 2.5
W/OContext
28 LumaContext 12 2
LumaChromaContext
10
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Rate(bpp) Rate(bpp) Rate(bpp)
Figure 7: RD performance for different configurations of context model.
Table II: BD-Rate and BD-Distortion comparison with different codecs.
PSNR MS-SSIM CIEDE2000
Configuration
BD-BR BD- BD-BR BD-MS- BD-BR BD-1
(%) PSNR (dB) (%) SSIM (%) /CIEDE2000
W/O Context 39.89 -1.345 0.68 -0.098 6.96 -0.002
Luma Context 27.90 -0.997 -4.53 0.115 3.32 0.001
Luma Chroma Context 21.74 -0.827 -7.50 0.205 -4.66 0.008
in Fig. 7. It can be observed that adding context improves performance in all three
metrics, due to the backward adaptation, where predictions are based on a causal
context. The third variant with context model in both branches performs the best.
However, with the context modeling blocks, additional time is required to encode and
decode, due to the causal nature of context modeling. Table.II lists the comparison
of BD-Rate and BD-Distortion values made with VTM as the baseline. It shows that
the context model in both luma and chroma branches provides the most gains.
Conclusion
A learned image codec that uses structure and color separately, called SLIC is pro-
posed. We show that splitting the image compression task based on luminance and
chrominance components not only improves performance, but also reduces the model
complexity significantly. The asymmetric architecture makes for more practical im-
age compression, with BD-BR gains of 7.5% for MS-SSIM. Although we outperform
various codecs in terms of MS-SSIM and CIEDE2000, we still lack in terms of PSNR,
which we plan to address in a future work. As continuation of this work, we plan
to speed up context modeling through parallelization and also compare with other
learned image codecs operating in YUV color space, such as JPEG AI.
References
[1] G.K. Wallace, “The jpeg still picture compression standard,” IEEE Transactions on
Consumer Electronics, vol. 38, no. 1, pp. xviii–xxxiv, 1992.
↑)Bd(RNSP
↑)Bd(RNSP
↑)Bd(MISS-SM
↑)Bd(MISS-SM
↓0002EDEIC
↓0002EDEIC[2] Gary J. Sullivan, Jens-Rainer Ohm, Woo-Jin Han, and Thomas Wiegand, “Overview
of the high efficiency video coding (hevc) standard,” IEEE Transactions on Circuits
and Systems for Video Technology, vol. 22, no. 12, pp. 1649–1668, 2012.
[3] Benjamin Bross, Ye-Kui Wang, Yan Ye, Shan Liu, Jianle Chen, Gary J. Sullivan, and
Jens-Rainer Ohm, “Overview of the versatile video coding (vvc) standard and its
applications,” IEEE Transactions on Circuits and Systems for Video Technology, vol.
31, no. 10, pp. 3736–3764, 2021.
[4] Christian Herglotz, Fabian Brand, Andy Regensky, Felix Rievel, and Andr´e Kaup,
“Processing energy modeling for neural network based image compression,” in 2023
IEEE International Conference on Image Processing (ICIP), Oct 2023, pp. 2390–2394.
[5] Johannes Ball´e, Valero Laparra, and Eero P Simoncelli, “End-to-end optimized image
compression,” in 5th International Conference on Learning Representations, ICLR
2017, 2017.
[6] Dailan He, Ziming Yang, Weikun Peng, Rui Ma, Hongwei Qin, and Yan Wang, “Elic:
Efficient learned image compression with unevenly grouped space-channel contextual
adaptive coding,” in Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition, 2022, pp. 5718–5727.
[7] Ming Lu, Peiyao Guo, Huiqing Shi, Chuntong Cao, and Zhan Ma, “Transformer-based
image compression,” in 2022 Data Compression Conference (DCC), March 2022, pp.
469–469.
[8] BharathBhushanDamodaran, MuhammetBalcilar, FranckGalpin, andPierreHellier,
“Rqat-inr: Improved implicit neural image compression,” in 2023 Data Compression
Conference (DCC), March 2023, pp. 208–217.
[9] Lei Zhou, Zhenhong Sun, Xiangji Wu, and Junmin Wu, “End-to-end optimized image
compression with attention mechanism,” in Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition (CVPR) Workshops, June 2019.
[10] Z. Cheng, H. Sun, M. Takeuchi, and J. Katto, “Learned Image Compression With Dis-
cretized Gaussian Mixture Likelihoods and Attention Modules,” in 2020 IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR), June 2020, pp.
7936–7945.
[11] Panqi Jia, Ahmet Burakhan Koyuncu, Georgii Gaikov, Alexander Karabutov, Elena
Alshina, and Andr´e Kaup, “Learning-based conditional image coder using color sepa-
ration,” in 2022 Picture Coding Symposium (PCS), Dec 2022, pp. 49–53.
[12] Srivatsa Prativadibhayankaram, Thomas Richter, Heiko Sparenberg, and Siegfried
Foessel, “Color learning for image compression,” in 2023 IEEE International Confer-
ence on Image Processing (ICIP), 2023, pp. 2330–2334.
[13] Qing-Long Zhang and Yu-Bin Yang, “Sa-net: Shuffle attention for deep convolutional
neuralnetworks,” inICASSP2021-2021IEEEInternationalConferenceonAcoustics,
Speech and Signal Processing (ICASSP), 2021, pp. 2235–2239.
[14] Z. Wang, E.P. Simoncelli, and A.C. Bovik, “Multiscale structural similarity for image
quality assessment,” in The Thrity-Seventh Asilomar Conference on Signals, Systems
& Computers, 2003, Pacific Grove, CA, USA, 2003, pp. 1398–1402, IEEE.
[15] Gaurav Sharma, Wencheng Wu, and Edul N Dalal, “The CIEDE2000 color-difference
formula: Implementation notes, supplementary test data, and mathematical observa-
tions,” Color Research & Application, vol. 30, no. 1, pp. 21–30, 2005.
[16] Johannes Ball´e, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston,
“Variational image compression with a scale hyperprior,” in International Conference
on Learning Representations, 2018.
[17] Gisle Bjontegaard, “Calculation of average psnr differences between rd-curves,” ITU
SG16 Doc. VCEG-M33, 2001.