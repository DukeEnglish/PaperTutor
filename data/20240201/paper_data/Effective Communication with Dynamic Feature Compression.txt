This paper has been submitted to IEEE Transactions on Communications. Copyright may change without notice. 1
Effective Communication with Dynamic Feature
Compression
Pietro Talli, Student Member, IEEE, Francesco Pase, Graduate Student Member, IEEE,
Federico Chiariotti, Member, IEEE, Andrea Zanella, Senior Member, IEEE, and Michele Zorzi, Fellow, IEEE
Abstract—The remote wireless control of industrial systems meaning of a message is often related to the intentions of
is one of the major use cases for 5G and beyond systems: in the receiver.
these cases, the massive amounts of sensory information that
While the Level B and C problems attracted limited at-
need to be shared over the wireless medium may overload even
tention for decades, the explosion of Industrial Internet of
high-capacity connections. Consequently, solving the effective
communication problem by optimizing the transmission strategy Things (IIoT) systems has drawn the research and indus-
todiscardirrelevantinformationcanprovideasignificantadvan- trial communities toward semantic and effective communi-
tage,but is often a verycomplex task.In thiswork, weconsider cation [3], optimizing remote control processes under severe
a prototypal system in which an observer must communicate
communication constraints beyond Shannon’s limits on Level
its sensory data to a robot controlling a task (e.g., a mobile
A performance [2]. In particular, the effectiveness problem is
robot in a factory). We then model it as a remote Partially
Observable Markov Decision Process (POMDP), considering the highly relevant to robotic applications, in which independent
effectofadoptingsemanticandeffectivecommunication-oriented mobile robots, such as drones or rovers, must operate based
solutionsontheoverallsystemperformance.Wesplitthecommu- on information from remote sensors. In this case the sensors
nication problem by considering an ensemble Vector Quantized
and the cameras act as the transmitter in a communication
Variational Autoencoder (VQ-VAE) encoding, and train a Deep
problem, while the robot is the receiver: by solving the Level
Reinforcement Learning (DRL) agent to dynamically adapt the
quantization level, considering both the current state of the C problem, the sensors can transmit the information that best
environment and the memory of past messages. We tested directs the robot’s actions toward the optimal policy [4]. We
the proposed approach on the well-known CartPole reference can also consider a case in which the robot is the transmitter,
control problem, obtaining a significant performance increase
while the receiver is a remote controller, which must get the
over traditional approaches.
most relevant information to decide the control policy [5].
Index Terms—Effective communication, Networked control, The rise of communication metrics that take the content
Semantic communication, Information bottleneck
of the message into account, such as the Value of Infor-
mation (VoI) [6], represents an attempt to approach the
I. INTRODUCTION problem in practical scenarios, and analytical studies have
THE main goal of classical communication theory is to exploited information theory to define a semantic accuracy
metric and minimize distortion [7]. In particular, information
build reliable systems for the accurate transmission of
bottleneck theory [8] has been widely used to characterize
arbitrary data through a constrained communication channel
Level B optimization [9]. However, translating a practical
while using as few symbols as possible. However, in the pref-
system model into a semantic space is a non-trivial issue,
acetoShannon’sseminalwork[1],WarrenWeaveralreadyen-
and the semantic problem is a subject of active research [10],
visioned two more complex Levels of communication beyond
[11]. The effectiveness problem is even more complex, as it
the simple transmission of bits. Classical communications are
implicitly depends on estimating the effect of communication
then included in Level A, or the technical problem, which
distortion on the control policy and, consequently, on its per-
concerns itself with the accurate and efficient transmission
formance [12]. While the effect of simple scheduling policies
of arbitrary raw data. Level B, or the semantic problem, is
is relatively easy to compute [13], and linear control systems
to find the best way to convey the meaning of the message,
can be optimized explicitly [14], realistic control tasks are
even when irrelevant details are lost or misunderstood, while
highly complex, complicating an analytical approach to the
Level C, also called the effectiveness problem, deals with the
Level C problem. Pure learning-based solutions that consider
resulting behavior of the receiver [2]: as long as the receiver
communication as an action in a multi-agent DRL problem,
takestheoptimaldecision,theeffectivenessproblemissolved,
such as emergent communication, also have limitations [15],
regardlessofthequalityofthereceivedinformation.TheLevel
as they can only deal with very simple scenarios due to
B and C problems are tightly intertwined, as defining the
significant convergence and training issues. In some cases,
Pietro Talli (corresponding author, pietro.talli@phd.unipd.it), Francesco the information bottleneck approach can also be exploited to
Pase (pasefrance@dei.unipd.it), Federico Chiariotti (chiariot@dei.unipd.it), determine state importance [16], but the existing literature on
AndreaZanella(zanella@dei.unipd.it)andMicheleZorzi(zorzi@dei.unipd.it)
optimizingLevelCcommunicationisverysparse,andlimited
are with the Department of Information Engineering, University of Padova,
35131Padua,Italy. to simpler scenarios [17]. Another possible approach to the
ThisworkwassupportedbytheEuropeanUnionundertheItalianNational remote tracking of Markov sources is addressed in zero-delay
RecoveryandResiliencePlanofNextGenerationEU,underthepartnershipon
codingtheory[18].However,thistheoryconsiderstheerroron
“TelecommunicationsoftheFuture”(PE0000001-program“RESTART”)and
the“SoEYoungResearchers”grantREDIAL. the hidden state estimate as the objective of the optimization,
4202
naJ
92
]GL.sc[
1v63261.1042:viXra2
which is similar to what we could consider a semantic (or Sec. VI concludes the paper.
Level B) approach. In this work, we show that it is possible
to optimize the system with respect to other metrics which
II. RELATEDWORK
cannot be explicitly derived such as cumulative rewards in
DRL: by accepting a higher distortion at the semantic Level The specific requirements of distributed and remotely con-
when it is not relevant to the task, we can further reduce the trolled systems have focused the research community’s at-
required bitrate without sacrificing the control performance. tention towards communication systems that must provide
In this work, we consider a dual model which combines updated information to enable real-time high-level tasks such
concepts from DRL and semantic source coding: we consider as inference, tracking or control. Although metrics such as
an ensemble of VQ-VAE models [19], each of which learns Age of Information (AoI) [6] represent a major improvement
to represent observations using a different codebook. A DRL withrespecttolatencyandpacketloss,theyarestilllimited,as
agent can then select the codebook to be used for each theyassumethatthequalityoftheinformationavailableatthe
transmission, controlling the trade-off between accuracy and receiverdegradesdeterministicallywithtime,mostcommonly
compression.Dependingonthetaskofthereceiver,thereward (but not necessarily [21]) in a linear fashion. However, more
totheDRLagentcanbetunedtosolvetheLevelA,B,andC sophisticated systems can also take into consideration the
problems, optimizing the performance for each specific task. current state of the system in order to decide whether and
In order to test the performance of the proposed framework when to update the status of the receiver. Metrics such as
in a relevant example scenario, we consider the well-known Urgency of Information (UoI) [14] and VoI [22] incorporate
CartPole problem, whose state can be easily converted into state information in their definition and are thus aware of
a semantic one, as its dynamics depend on a limited set of the intrinsic value of potential updates. Other context-aware
physical quantities. The problem we selected is purposefully indices to measure the nonlinear time-varying importance and
simple, as this allows for a better explainability and an the non-uniform context dependence of the status information
easier training, but the solution is not limited to the CartPole have also been proposed [14]. The authors of [23] considered
problem,andcanbeadaptedtomorecomplextasks.Themain asysteminwhichatransmittermonitorsthestatusofasystem
contributions of this paper are then given by the following: and updates the controller, providing status information. Then
aconstrainedMarkovdecisionprocess(MDP)isformulatedto
• We model a remote-control system as a remote POMDP minimize the cost of actuation and simultaneously guarantee
problem and present an efficient solution for learning ef-
a target communication rate. Both works show significant
fectivecommunicationthroughthedynamiccompression
improvements with respect to other metrics such as AoI.
of learnable features;
Atthesametime,thedevelopmentoflearning-basedcoding
• We show that dynamic codebook selection outperforms
schemes has allowed communication system designers to
static strategies for all three Levels, and that considering
movebeyondpacketerrorasthekeycodingperformancemet-
the Level C task can significantly improve the control
ric, exploiting semantic considerations. Joint source-channel
performance without increasing the bitrate;
codingforwirelessimagetransmissionisimplementedin[24],
• We adopt an explainability framework to understand
[25],andtheencoder-decoderpairisparameterizedbyaneural
the choices of the agent in this simple problem, and
network (NN), whose architecture may vary. This approach
verify that the Level C dynamic compression captures
can be used to maintain the semantic information contained
the receiver’s uncertainty in the state estimation and its
in the transmitted data, while improving the compression
impact on the expected reward, transmitting only when
performance.
necessary.
Semantic information at the receiver can be used to solve
We remark that the dynamic codebook selection policy is different tasks. Effective communication [12] can be seen as
not limited to the VQ-VAE ensemble we consider, but is an extension of this, in which the task involves the receiver
a general technique that can be applied to any compression taking actions and possibly altering the information that the
algorithm with adaptable quality parameters, helping deliver transmitteriscommunicating.Effectivecommunicationdiffers
more accurate information when it is relevant to do so. The from semantic communication mostly because the “semantic”
results and policy analysis lead to significant insights for the content which has to be preserved in the communicated mes-
design of communication strategies for remote control. A par- sages is not explicit. Moreover, control tasks have a temporal
tialversionofthisworkwaspresentedattheIEEEINFOCOM component that must be taken into account, as investigated
WiSARN 2023 workshop [20]. This paper includes a more in [12]. The scenario considered in the work is a two-agent
completetheoreticalcharacterizationoftheproblem,aswellas POMDPinwhichoneagentcommunicatesandtheotheragent
an updated learning architecture, additional results, and an in- interacts with the environment, using DRL to solve the joint
depthanalysisoftheDRL-baseddynamiccompressionpolicy. problem and encoding the information. A distributed percep-
The rest of the paper is organized as follows: first, we tion scenario, in which multiple sensors communicate to a
analyze the state of the art on semantic and effective commu- singlerobot,isconsideredin[26]andsolvedusingmulti-agent
nication in Sec. II. Sec. III then presents the general system reinforcement learning (MARL), showing that joint training
model and the three Levels of communication we consider. improves the performance of the system, particularly when
Wethendescribethedynamicfeaturecompressionsolutionin communication is severely constrained. While past works
Sec. IV, which is evaluated by simulation in Sec. V. Finally, aimedatspecificscenariosandobjectives,thispaperproposes3
TABLE I: Main notation and definitions.
a novel DRL approach that combines status updates with an
adaptive coding scheme and can be easily adapted to operate
Symbol Definition
on any of the three Levels of communication (A, B, or C).
S Setofsystemstates
A Setoffeasibleactions
III. SYSTEMMODEL O Setofobservations
P Statetransitionprobabilityfunction
The recent interest in semantic and effective communica- ω Observationfunction
tions from the research community has driven the develop- R Rewardfunction
γ Discountfactor
ment of a wide array of models and conceptualizations, as
h Historyofstochasticobservations
highlighted in the previous section. At the highest level of h(r) Historyofreceivedmessagesattherobot
abstraction,ourpurposeistodefineamodelinwhicheffective π Policy
communicationismeaningful,andthedifferencesbetweenthe G Expectedcumulativediscountedreward
Φ(·) Spaceofprobabilitydistributionsoveraset
three problems in Weaver’s formulation become clear.
ξ BeliefdistributionoverthestatespaceS
Let us then consider a simple example: we have a remote ξ(o) Beliefdistributionattheobserver
actuator performing a control task, while a camera observes ξpri,(r) Priorbeliefdistributionattherobot
the results and transmits its observation through a wireless ξ(r) Beliefdistributionattherobot
M Setofmessages
channel. The actuator might have its own sensors, but it relies
Λ Encodingfunction
on the video feed to improve its performance and maintain m Messagecommunicated
stable and efficient control. The classical, Level A approach ℓ(m) Lengthofmessagem
ζ Vectorquantizer
to the problem would be to compress the video as efficiently
P Picturespace
as possible, minimizing the reconstruction error on frames by β Communicationcost
using an appropriate codec. The difference between Level A
andLevelBsolutionsisthenobvious:theformerencodesnew
frames so that the reconstruction fidelity is preserved, while a tuple ⟨S,A,O,P,ω,R,γ⟩, where S represents the set of
thelattermapselementsintheframetotheirimportancewhen system states, A is the set of feasible actions, and O is
estimating the physical state of the system. In some control the observation set. The function P : S × A → Φ(S),
applications, the state can be defined trivially, while in others where Φ(·) represents the space of probability distributions
it may be more complex, but in general, the translation of the over a set, gives the state transition probability function.
videotothestatespaceisunaffectedbyirrelevantinformation We denote the conditional probability distribution of the
(such as, e.g., movements in the background). next state, given the current state and the selected action,
If we consider Level C, we target control performance as P(s′|s,a) = Pr[S =s′|S =s,A =a]. Then, ω :
t+1 t t
directly, and thus further restrict the definition of relevant S → Φ(O) is an observation function representing the
information:whileLevelBconcernsitselfwithestimatingthe probability of an observation conditioned on the system state:
systemstatecorrectly,aLevelCsolutiononlyconsiderserrors ω(o|s)=Pr[O =o|S =s], i.e., the probability of receiving
t t
in the state estimation when they cause performance drops. observation o, given that the system is in state s. Finally,
If the control action is the same over a wide set of states, function R : S × A → R represents the expected reward
accuracythenbecomesunnecessary,astheactuatoronlyneeds received by the agent when taking action a in state s, denoted
a rough estimate of the state to decide what to do; the same as R(s,a), and the scalar γ ∈[0,1) is a discount factor used
happens if there are multiple actions with almost equivalent to compute the long-term reward. We notice that functions P,
performance, i.e., if the optimality gap caused by imperfect R, and ω do not depend on the time instant t, thus focusing
information remains small. on homogeneous Markov processes.
These natural observations represent the core concepts of The POMDP proceeds in discrete steps indexed by t: at
effective communication, but implementing them in practical each step t, the agent can infer the system state s only
t
systems is often complex as actions have long-term con- from the partial information given by the history of stochastic
sequences, and state estimates are based on a history of observations h = (o ,o ,...,o ) ∈ Ot. Based on these
t t t−1 1
observations, so that transmitting a message may affect future observations,andonitspolicyπ :Ot →Φ(A),whichoutputs
performanceincomplexways.Inthefollowing,weprovidean a probability distribution over the action space for each pos-
analytical framework using the remote POMDP approach to sible observation history, the agent interacts with the system
objectively evaluate these choices and implement a solution by selecting an action A ∼π(h ). The sampled action a is
t t t
for effective communication in cyber-physical systems. We then performed in the real environment, whose hidden state
willdenoterandomvariableswithcapitalletters,theirpossible s is unknown to the agent, which then receives a feedback
t
values with lower-case letters, and sets with calligraphic or from the environment in the form of a (potentially stochastic)
Greekcapitals.TableIreportsthemainsymbolsweintroduce reward r , with expected value R =R(s ,a ).1 The goal for
t t t t
in the following sections for the reader’s convenience. the agent is then to optimize its policy π to maximize the
(cid:104) (cid:105)
expected cumulative discounted reward G = E (cid:80) γtR .
t t
A. POMDP Definition and Solution
In the infinite horizon POMDP formulation [27], one agent
1As the reward signal is only provided during training, it cannot be used
needs to optimally control a stochastic process defined by toinferthevalueofst.4
Havinganoptimalpolicyisequivalenttoknowingtheoptimal a case in which communication and control are designed
state-actionvalues,alsoknownasQ-values,andtakingaction separately. Joint control and communication approaches [26]
canoutperformseparateapproachesbytuningthetwoagents’
a =π(h )=argmax Q(h ,a),
t t t policies to each other, but they introduce additional training
a∈A
complexity, and will not be considered in this work. In the
where Q(h ,a) = E[(cid:80)∞ γτ−tr |h ,a] is the expected
t τ=t τ t following, we will refer to variable x related to the robot as
cumulative reward starting from (h ,a).
t x(r), while the corresponding variable on the observer side
However,consideringthefullhistoryofobservationsmakes
will be denoted by x(o).
the solution highly complex, as the length of h is potentially
t 1) The Robot-Side POMDP: We denote the message com-
unbounded. We then define an estimator ξ : Ot → Φ(S),
municated to the robot at time step t as m ∈ M. The
which outputs the a posteriori belief distribution over the t
set of possible messages M forms the set of observations
state space. We can then recast the original POMDP as a
that are available to the robot, and the history of these
standardMDP,whosestatespaceisS′ =Φ(S),i.e.,thespace
observations is given by h(r) = {m ,...,m }, which is the
of possible belief distributions. Solving the POMDP in this t t 1
sequence of messages received up to time t. We can then see
modified belief space has been proved to be optimal in [28].
the robot as an agent with its own POMDP, in which the
The policy over this modified MDP is then π : S′ → A,
observations are filtered by both the partial knowledge of the
whichcanbeoptimizedusingstandardtools[29].Wecanalso
observer and the further distortion produced by the fact that
computethenewtransitionprobabilityandexpectedrewardas
these observations are encoded and communicated through a
in [28]. Given ξ (s)=Pr(S =s|h ), which represents the
t t t constrained channel. The robot-side POMDP is then defined
maximumlikelihoodestimateofthestategivenallthehistory
by the tuple
(cid:10) S,A,M,P,π(o),R,γ(cid:11)
, as observations depend
of observations h , and A =a, we define the a priori belief
t t on the observer’s policy.
over the state at time t+1 as
The message transmitted from the observer to the robot
ξ tp +ri 1(s|ξ t,a)=Pr[S t+1 =s|ξ t,A t =a] modifies the belief distribution over the next state as a
= (cid:88) ξ (s′)P(s|s′,a). Bayesianupdate.Letusdefinethedistributionoverthecurrent
t state,giventhatthemessagem hasbeenreceived,asξ(r).For
s′∈S t t
example,ifthecommunicatedmessagem containsthecorrect
The a posteriori belief ξ , which also includes the new t
t+1 state S = s, the belief distribution becomes deterministic,
observation o , can then be obtained by performing a t
t+1 i.e., ξ(s′ | m ) = δ , where δ is the Kronecker delta
Bayesian update using the a priori belief as a prior: t s,s′ m,n
function, equal to 1 if the two arguments are the same
ξ t+1(s|ξ t,a,o)=Pr[S t+1 =s|ξ t,A t =a,O t+1 =o] and 0 otherwise. Ideally, an intelligent observer will allocate
=Pr(cid:104) S =s|ξpri (s|ξ ,a),O =o(cid:105) morecommunication resourcesandthusprovide moreprecise
t+1 t+1 t t+1 messages if the a priori distribution of the robot is far from
ω(o|s)ξpri (s|ξ ,a) the one estimated by the observer. The modified MDP is
= (cid:80) ξpri (t+ s′1 |ξ ,at )ω(o|s′). then defined by the tuple (cid:10) Φ(S),A,P(r),R,γ(cid:11) , where P(r)
s′∈S t+1 t (1) represents the Bayesian update function. According to the
These update equations allow us to compute the modified previous notation, we can express the optimal action at time
transition probability matrix P′ for the belief MDP, which t+1 as
(cid:16) (cid:17)
is then defined by the tuple ⟨Φ(S),A,P′,R,γ⟩. a =argmax Q ξ(r),a ,
t+1 t
a∈A
B. The Remote POMDP whereξ(r) isthecurrentbeliefattherobotside(aftermessage
t
In this paper we consider a variant of the POMDP, that we m t is received). The robot’s reward is simply given as the
defineremotePOMDP,inwhichtwoagentsareinvolvedinthe reward of the original POMDP, i.e., the control performance
process.Thefirstagent,i.e.,theobserver,receivesobservation in the environment. The optimal policy can be reached by
O ∈ O, and needs to convey such information to a second using standard DRL tools.
t
agent, i.e., the robot, through a constrained communication 2) The Observer-Side POMDP: On the other side, the
channel,whichlimitsthenumberofbitstheobservercansend observer needs to encode its belief ξ(o) in a message m ∈
t t
reliably.Consequently,theamountofinformationtheobserver M and transmit it. We can then consider the observer-side
can send to the robot is limited. The robot then chooses POMDP, in which the action set corresponds to the set of
and takes an action in the physical environment. This system messages M and the state space is represented by the belief
can formalize many control problems in future IIoT systems, from the observed results. The tuple defining this POMDP is
as sensors and actuators may potentially be geographically
(cid:10) S,M,O,P,ω,R(o),γ(cid:11)
. We assume that the observer knows
distributed, and the amount of information they can exchange the robot’s policy, i.e., it can know the actions that the
toaccomplishataskislimitedbythesharedwirelessmedium, robot takes in the environment and use them to improve
which has to be allocated to the many devices installed in the its estimate of the state. This can also be accomplished if
factory, as well as by the energy limitations of the sensors. the robot transmits the actions it takes as feedback to the
Similar systems have been analyzed in [12], [23]. We will observer. As described above for the robot-side problem, we
now analyze the problems for the two agents, considering can transform this POMDP into the belief MDP given by5
(cid:10) Φ(S)×Φ(S),M,P(o),R(o),γ(cid:11) ,whereP(o) istheBayesian CartPole problem. The Level B reward function R(o) is then
B
update given in (1). We highlight that the observer needs given as follows:
to keep track of both its own and the robot’s belief, as the (cid:16)(cid:68) (cid:69) (cid:17) (cid:16) (cid:17)
R(o) ξ(o),ξpri,(r) ,m =−d ξ(o),ξ(r) −βℓ(m). (3)
effectiveness of communication depends on the difference B t t B t t
between the two, and the state of the observer is given by
(cid:68) (cid:69) In our CartPole case, this may be simply represented by the
ξ(o),ξpri,(r) .
t t MSE between the best estimate of the state at the transmitter
Theobjectiveoftheobserveristominimizechannelusage, and the receiver.
i.e., communicate as few bits as possible, while maintaining Finally, we can consider the Level C system. In this case,
the highest possible performance in the control task: the thedistortionmetricisnotneeded,asthecontrolperformance
expected reward R(o) depends on both components. We then can be used directly, and the reward R(o) is:
C
consider a simple linear combination approach: if it transmits
(cid:16)(cid:68) (cid:69) (cid:17) (cid:16) (cid:16) (cid:17)(cid:17)
message m, whose length in bits is ℓ(m), the observer then R(o) ξ(o),ξpri,(r) ,m =R ξ(r),π(r) ξ(r) −βℓ(m).
C t t t t
gets a penalty βℓ(m), where β ∈ R+ is a cost parameter. In (4)
(cid:16) (cid:17)
order to optimize its policy, the observer also needs to have The VoI of message m, V ξpri,(r),m , can then be given by
t
a way to gauge the value of information, which is a complex
the difference between the expected performance of the robot
problem: information theory, and in particular rate-distortion
with and without this information:
theory,haveprovidedthefundamentallimitswhenoptimizing
(cid:16) (cid:17) (cid:16) (cid:16) (cid:17)(cid:17)
for the technical problem, i.e., Level A, where the goal is V ξpri,(r),m =Q ξ(r),π(r) ξ(r)
t t t
to reconstruct the source signals with the highest fidelity [30]. (cid:16) (cid:16) (cid:17)(cid:17) (5)
−Q ξpri,(r),π(r) ξpri,(r) .
WewilldiscussthedefinitionofVoIinthefollowingsections. t t
More complex modeling choices for the transmission cost are
Thus, the optimal Level C observer policy π(o) will balance
also possible, e.g., considering energy constraints for a sensor C
the trade-off between the performance at the receiver and the
with energy harvesting capabilities, but are beyond the scope
communication cost not only in the current time step but also
of this work.
in the long term. This foresighted behavior is essential when
As the complexity of the problem is massive, we restrict
considering that the belief distributions incorporate the mem-
ourselves to a smaller action space by making a simplifying
ory of previously received messages. Providing information
assumption, which allows us to separate the problem: the
that does not improve the expected reward in the next step
observerdoesnottransmittheentirebeliefdistribution,which
might still be worth the cost if it allows the robot to improve
may be implicit, but rather the observation O . We then
t
its estimate, reducing the need for future communication.
consider the encoding function Λ : O×Φ(S) → M, which
(cid:16) (cid:17)
will generate a message M =Λ O |ξ(o) to be sent to the
t t t IV. PROPOSEDSOLUTION
robot at each step t.
In this section, we introduce the architecture we used to
represent Λ, the VQ-VAE, and discuss the remote POMDP
C. Observer Reward in Remote POMDPs
solution. As the VQ-VAE model is not adaptive, we consider
The first and simplest way to solve the remote POMDP an ensemble model with different quantization levels, limiting
problem is to blindly apply standard Level A rate-distortion the choice of the observer to which VQ-VAE model to use
metrics to compress the sensor observations into messages to in the transmission. As we mentioned, directly learning the
be sent to the agent. As an example, in the CartPole problem encoding is highly complex, with a vast action space, and
analyzed in this work (see Sec. V), one sensor observation techniques such as emergent communication that learn it
is given by two consecutive 2D camera acquisitions. The explicitly are limited to scenarios with very simple tasks and
observer’s policy is then independent of the robot’s task, and immediate rewards. By restricting the problem to a smaller
canbecomputedseparately.TheLevelArewardfunctionR(o) action space, we find a potentially sub-optimal solution, but
A
is then given as follows: we can deal with much more complex problems.
(cid:16)(cid:68) (cid:69) (cid:17)
R(o) ξ(o),ξpri,(r) ,m =−d (o ,oˆ)−βℓ(m). (2)
A t t A t t A. Deep VQ-VAE Encoding
In the CartPole case, a natural distortion metric is the image InordertorepresenttheencodingfunctionΛ,andtorestrict
Peak Signal to Noise Ratio (PSNR), an image quality metric theobserver-sidePOMDPtoamoremanageableactionspace,
proportional to the logarithm of the normalized Mean Square the observer exploits the VQ-VAE architecture introduced
Error (MSE) between the images. Naturally, encoding the in [19]. The VQ-VAE is built on top of the more common
observation with a higher precision will require more bits, as Variational Autoencoder (VAE) model, with the additional
the set of messages needs to be bigger. feature of finding an optimal discrete representation of the
The Level B problem considers the projection of the raw latent space. The VAE is used to reduce the dimensionality
observations into a significantly smaller semantic space, over of an input vector X ∈ RI, by mapping it into a stochastic
which we measure distortion using function d , explicitly latent representation Z ∈ RL ∼ q (Z|X), where L < I.
B ν
capturing the error over the needed physical system informa- The stochastic encoding function q (Z|X) is a parameterized
ν
tion, e.g., the angular position and velocity of the pole in the probability distribution represented by a neural network with6
parameter vector ν. To find optimal latent representations Z,
the VAE jointly optimizes a decoding function p (Xˆ|Z) that Observer Ensemble Actor
θ VQ-VAE
aimstoreconstructX fromasampleXˆ ∼p (Xˆ|Z).Thisway,
θ
the parameter vectors ν and θ are usually jointly optimized to Ψ∅
minimize the distortion d(X,Xˆ) between the input and its Observation Decoder oˆt
reconstruction, given the constraint on Z, while reducing the Ψ1
distance between q ν(Z|X), and some prior q(Z) [31] used to ot ... mt Semantic sˆt
impose some structure or complexity budget. estimate
However, in practical scenarios, one needs to digitally ΨV
rt
encode the input X into a discrete latent representation. To DRL agent
do this, the VQ-VAE quantizes the latent space by using N DRL agent at
K-dimensionalcodewordsz ,...,z ∈RK,formingadictio-
1 N
nary with N entries. Moreover, to better represent 3D inputs,
the VQ-VAE quantizes the latent representation Z using a set Fig. 1: Dynamic feature compression architecture.
of F blocks, each quantizing one feature f(X) of the input,
andchosenfromasetofN possiblecodewords.Wedenotethe
described in the previous section, the type of reward depends
set containing all the NF possible concatenated blocks with
on the communication problem that the observer is trying to
M(N), as it represents the set of all possible messages the
solve: at Levels A and B, the observer aims at minimizing
observercanusetoconveytotherobottheinformationonthe
distortion in the observation and semantic space, respectively.
observation O, by using F discrete N-dimensional features.
At Level C, the objective is to maximize the robot’s reward.
The peculiarity of the VQ-VAE architecture is that it jointly
We remark that the only Level at which the decision of the
optimizesthecodewordsinM(N)togetherwiththestochastic
receiver matters is Level C. The semantic estimate describes
encoding and decoding functions q and p , instead of simply
ν θ thephysicalprocess,whichmodelsthedynamicsofthecontrol
applyingfixedvectorquantizationontopoflearnedcontinuous
processthattheObservercansense.Optimizingthetransmitter
latent variables Z. When the communication budget is fixed,
to minimize the reconstruction error in the semantic estimate
i.e.,thevalueofLisconstant,theprotocoltosolvetheremote
does not consider the decision of the actor (receiver). In
POMDP is rather simple: first, the observer trains the VQ-
VAE with N = 2F−1L to minimize the technical, semantic, general, the physical state of the system can carry redundant
or irrelevant information with respect to the agent’s decision,
or effective distortion d , depending on the problem; then, at
α and is not equivalent to Level C optimization.
each step t, the observer computes mˆ ∼ q (·|o ), and finds
ν t In all three cases, memory is important: representing snap-
m =argmin ∥m−mˆ∥ . The message m is sent to
t m∈M(N) 2 t shots of the physical system in consecutive instants, subse-
the robot, which can optimize its decision accordingly.
quent observations have high correlations, and the robot can
gleanasignificantamountofinformationfrompastmessages.
B. Dynamic Feature Compression This is an important advantage of dynamic compression, as it
canadaptmessagestotheestimatedknowledgeatthereceiver
WecanthenconsiderthearchitectureshowninFig.1,con-
side.
sistingofasetofVQ-VAEsV ={ζ∅,ζ 1,...,ζ V},whereeach
While the observer is adapting its transmissions to the
VQ-VAE ζ compresses each feature using v bits. We also
v
robot’s task, the robot’s algorithms are fixed. They could
includeanullactionζ∅,whichcorrespondstonottransmitting
themselves be adapted to the dynamic compression strategy,
anything. As we only consider the communication side of the
but this joint training is significantly more complex, and we
problem, the actor is trained beforehand using the messages
consider it as a possible extension of this work.
with the finest-grained quantization, which are compressed
with the VQ-VAE ζ with the largest codebook. This choice
V
ensuresthattheactorcandealwithfiner-grainedinputs,while C. RL implementation
still being robust to lower-precision features. The robot can There are two policies in the considered system, one for
then perform three different tasks, corresponding to the three the observer and one for the robot and in both cases the
communicationproblems:itcandecodetheobservation(Level policies are learned through the Actor Critic algorithm. This
A) with the highest possible accuracy, using the decoder part means that an agent learns a parametric policy π and a Q-
λ
of the VQ-VAE architecture; it can estimate the hidden state values estimator. Both the policy and the Q-values are neural
(Level B) using a supervised learning solution; or it can networks. In order to take into account the past observations
perform a control action based on the received information the two networks share a Long Short-Term Memory (LSTM)
and observe its effects (Level C). layerwhichestimatesalatentstatewhichisthengivenasinput
In all three cases, the dynamic compression is performed to both the policy and the values estimator. This architecture
by the observer, based on the feedback from the robot. The avoids explicitly modeling the belief distribution which may
observer side of the remote POMDP, whose reward is given be complicated to treat in continuous settings like the one
in (2)-(4), is restricted to the choice of ζ , i.e., to selecting considered in this work. This practical choice is also useful
v
one of the possible codebooks learned by each VQ-VAE in to avoid decoding the latent features discovered by the VQ-
the ensemble model, or to avoid any transmission. As we VAE back in the observation space O or in the physical7
TABLE II: Simulation Parameters.
Parameter Value Description
H×W 160×360 Imagesize
V 7 Numberofquantizers
F 8 Numberoflatentfeatures (a)Original.
D emb 8 Embeddingdimensionoffeatures
B 256 Batchsize
γ 0.95 Discountfactor
T 500 Maximumnumberofstepsforanepisode
αenc 10−3 VQ-VAElearningrate
αReg 10−4 Regressorlearningrate
α A2C 10−4 A2Clearningrate
D 5×104 SizeoftheVQ-VAEtrainingdataset
(b)Reconstructed.
Tenc 100 Encodertrainingepochs
E rob 2×104 Robotpolicytrainingepisodes Fig.2:Exampleoftheoriginalandreconstructedobservation.
E obs 1×105 Observerpolicytrainingepisodes
Etest 1000 Numberoftestepisodes
35
30
state space S, increasing the potential for errors. Indeed, the
quantizedfeaturescommunicatedwiththemessagem contain 25
t
astructuredrepresentationoftheobservationspacewhichcan
20
be used effectively by an LSTM to estimate the true state.
The training algorithm is the standard Advantage Actor Critic 15
0 20 40 60
(A2C), but the replay buffer is appropriately modified to take
Epoch
into account the history of previously received messages.
(a)PSNR.
V. SIMULATIONSETTINGSANDRESULTS
The underlying use case analyzed in this work is the well- 60
knownCartPoleproblem,asimplementedintheOpenAIGym
library.2 In this problem, a pole is installed on a cart, and the 40
task is to control the cart position and velocity to keep the
20 Trainingperplexity
pole in equilibrium. The physical state of the system is fully
Maximumperplexity
describedbythecartpositionx andvelocityx˙ ,andthepole 0
t t
angle ψ and angular velocity ψ˙ . Consequently, the true state 0 20 40 60
t t
of the system is s =(x ,x˙ ,ψ ,ψ˙ ), and the semantic state Epoch
t t t t t
space is S ⊂ R4 (because of physical constraints, the range (b)Perplexity.
of each value does not actually span the whole real line). The
Fig. 3: Training of the VQ-VAE model ζ , with 6 bits per
main simulation parameters are reported in Table II. 6
feature.
At each step t, the observer senses the system by taking
a black and white picture of the scene, which is in a space
P = {0,...,255}180×360. To take the temporal element into thustomaximizethecumulativediscountedsumofthereward
account, an observation O t includes two subsequent pictures, R t, while limiting the communication cost.
at times t−1 and t, so that the observation space is O =
P ×P. An example of the transmission process is given in
A. The Coding and Decoding Functions
Fig.2,whichshowstheoriginalversionsensedbytheobserver
As mentioned in Sec. III-C, the observer can optimize its
(above) and the reconstructed version at the receiver (below)
coding function Λ according to different criteria depending
whenusingatrainedVQ-VAEζ ,i.e.,anencodertrainedwith
6
on the considered communication problem. However, as we
6 bits per feature, the maximum we consider in this study.
explained in Sec. IV, optimizing Λ without any parameters
In the CartPole problem, the action space A contains
is usually not feasible due to the curse of dimensionality on
just two actions Left and Right, which push the cart to
the action space. Consequently, we rely on a pre-trained set V
the left or to the right, respectively. At the end of each
of VQ-VAE models, whose codebooks are optimized to solve
step, depending on the true state s , and on the taken ac-
t
the technical problem, i.e., minimizing the distortion on the
tion a , the environment will return a deterministic reward
t
R = −x−1|x | − ψ−1|ψ |, ∀t, where x = 4.8 m and observation measured using the MSE: d A(o,oˆ)=MSE(o,oˆ).
ψt = 2πma rx adt (equivm aa lx entt to 24◦) are them max aximum values The training performance of the VQ-VAE with ζ 6, i.e., using
max 15 themaximumvalueV of6bitsperfeature,isshowninFig.3:
for the two quantities. If the angle or cart position go outside
the encoder converges to a good reconstruction performance,
the boundaries, the episode is over, and the agents do not
which can be measured by its perplexity. The perplexity is
accumulate any more reward. The goal for the two agents is
simply 2H(p), where H(p) is the entropy of the codeword
2https://www.gymlibrary.dev/environments/classic control/cart pole/ selection frequency, and a perplexity equal to the number of
]Bd[RNSP
ytixelpreP8
TABLE III: Encoder-Decoder parameters.
codewords is the theoretical limit, which is only reached if
all codewords are selected with the same probability. The
Layertype Size Kernelsize Stride
perplexity at convergence is 54.97, which is close to the
Encoder
theoretical limit for a real application.
Conv2d+ReLU 64 10×11 8×9
The observer then uses DRL to foresightedly choose the Conv2d+ReLU 64 12×12 10×10
best VQ-VAE ζ at each time step, maximizing the expected Conv2d+ReLU 128 3×3 1×1
v
ResidualStack 2 3×3 1×1
long-term reward for each communication problem. We train
Conv2d 8 3×3 1×1
the observer to solve each specific coding problem by de-
Decoder
signing three different rewards, depending on the considered
ResidualStack 2 3×3 1×1
communication Level (A, B, or C): Conv2d+ReLU 128 3×3 1×1
Conv2d+ReLU 64 12×12 10×10
1) Level A (technical problem): The distortion metric for
Conv2d 64 10×11 8×9
the observer is d (o ,oˆ) = −PSNR(o ,oˆ), as part of
A t t t t
the reward definition from (2). The PSNR is an image
TABLE IV: Recurrent architectures.
fidelity measure proportional to the logarithm of the
normalizedMSEbetweentheoriginalandreconstructed Layertype Inputs Outputs Description
image;
Regressor
2) Level B (semantic problem): The distortion metric is LSTM+ReLU 64 64 Singlerecurrentlayer
d (sˆ(o),sˆ(r)) = MSE(sˆ(o),sˆ(r)), as part of the reward Linear+ReLU 64 128 Hiddenlayer
B t t t t Linear 128 1 Outputlayer
defined in (3), and the decoder needs to estimate the
underlyingphysicalstates byminimizingtheMSE,i.e., Actor-critic
t LSTM+ReLU 64 64 Singlerecurrentlayer
thedistancebetweensˆ( to) andsˆ( tr) inthesemanticspace. Linear+ReLU 64 128 Hiddenlayer
In our case, the estimator used to obtain the estimates Linear 128 1 Outputlayer(Value)
Linear+softmax 128 |A| Outputlayer(Policy)
is a pre-trained supervised LSTM neural network;
3) Level C (effective problem): In this case, there is no
direct distortion metric, and the control performance is
A unique observer policy is trained for different values of the
used directly as in (4). The policy π(r) is given by an
trade-offparameterβ andfordifferentcommunicationLevels.
actor-critic agent implementing an LSTM architecture,
For further details on the implementation, training and testing
pre-trainedusingdatawiththehighestavailablemessage
process, we refer to the publicly available simulation code.3
quality (6 bits per feature).
Theadditionalcomputationalcostofthearchitectureonthe
In this case, the task depends on all the semantic features
observer side is well within the computational capabilities of
containedinS .However,the4componentsofthestatedonot
t even relatively simple embedded devices [32], and even more
carry the same amount of information to the robot: depending
complex problems can be dealt with by Edge devices. At the
on the system conditions, i.e., the state S , some pieces of
t same time, training the actor with compressed representations
information are more relevant than others.
actually reduces its computational burden, as the feature
extraction is performed by the sender. However, if the sender
B. Neural Network Architecture and Training issignificantlycomputationallyconstrained,replacingtheVQ-
TheVQ-VAEarchitectureismadewithConvolutionalNeu- VAEwithaclassicalcompressionschemesuchasJPEGmight
ral Network (CNN) layers to extract latent features and it is be a good way to reduce the cost and still deliver the benefits
trained separately before the training of the control policy. of dynamic compression.
To this end, a dataset of observations is collected through
a random policy. We then train an encoding network, the C. Results
vector quantization layer and the decoder jointly as in the
We assess the performance of the three different tasks in
standard VQ-VAE [19]. The first vector quantization layer
theCartPolescenariobysimulation,measuringtheresultsover
learned contains the highest number of codewords. Finally,
1000episodesafterconvergence.Fig.4showstheperformance
we fix the encoder and the decoder and just train the other
of the various schemes over the three problems, compared
vector quantization layers, obtaining multiple quantizers over
with a static VQ-VAE solution with a constant compression
the same latent space discovered by a common encoder.
level.IntheLevelCevaluation,wealsoconsiderastaticVQ-
The hyperparameters used to train the VQ-VAE are reported
VAE solution in which the robot is not retrained for each
in Table II. After obtaining the V quantizers, we train the
v, but is only trained for v = 6 bits per feature (i.e., 48
policy using the standard A2C algorithm. Table III shows
bits per message, as the VQ-VAE considers 8 features) as
the Encoder-Decoder layers of the VQ-VAE. In Table IV,
for the dynamic scheme. We trained the dynamic schemes
the layers of the implemented Regressor and the Actor-critic
with different values of the communication cost β, so as to
neural networks are reported. All the NNs are implemented
provideafullpictureoftheadaptationtothetrade-offbetween
through the Pytorch library. Once the robot policy has been
performanceandcompression.Wealsointroducethenotionof
obtained,wecantraintheobserverpolicy.Theobserverlearns
Pareto dominance: an n-dimensional tuple η = (η ,...,η )
1 n
a policy through the same A2C algorithm, but in this case
the input to the policy are the features before quantization. 3https://www.github.com/pietro-talli/tmlcn code9
LevelA LevelB LevelC
StaticVQ-VAE StaticVQ-VAE(withoutretraining)
35 0.2
0.15
30
0.1
25
0.05
20 0
1 2 3 4 5 6 1 2 3 4 5 6
Averagemessagelengthℓ¯[B] Averagemessagelengthℓ¯[B]
(a)Technicalproblem. (b)Semanticproblem.
400
200
0
1 2 3 4 5 6
Averagemessagelengthℓ¯[B]
(c)Effectiveproblem.
Fig. 4: Performance of the communication schemes on the three Levels of the remote POMDP.
Pareto dominates η′ (which we denote as η ≻η′) if and only the robot to deal with the specific VQ-VAE used may provide
if: a slight performance advantage. In general, almost perfect
η ≻η′ ⇐⇒ ∃i:η >η′∧η ≥η′ ∀j. (6) control can be achieved with less than half of the average
i i j j
bitrateofthestaticcompressor,whichcanonlyreachasimilar
We can extend this to schemes with multiple possible config- performance at a much higher communication cost. We also
urations. The definition of Pareto dominance for schemes x note that, in this case, the Level B solution performs worst:
and y is: x ≻ y ⇐⇒ ∃η(x) ≻ η(y)∀η(y), i.e., for each choosing the solution that minimizes the semantic distortion
configuration of scheme y, there is a setting of x that Pareto is not always matched to the task, as it considers the state
dominates it. In other words, we can always tune scheme x variables as having equal weight, while a higher precision
so that it outperforms any configuration of scheme y on all mayberequiredwhenthequantizationerroraffectstherobot’s
metrics. action.
Wefirstconsiderthetechnicalproblemperformance,shown Another analysis is conducted on the way the CartPole is
in Fig. 4a: as expected, the Level A dynamic compression controlled with the different communication policies. Fig. 5
outperforms all other solutions, and its performance is Pareto shows the Angular Root Mean Squared Deviation (RMSD)
dominant with respect to static compression. Interestingly, the (Fig. 5a) and the Position RMSD (Fig. 5b), defined as:
Level B and Level C solutions perform worse than static
(cid:118)
compression: by concentrating on features in the semantic (cid:117) (cid:117)1(cid:88)t
space or the task space, these solutions remove information
RMSD(x)=(cid:116)
t
(x i−x target)2,
that could be useful to reconstruct the full observation, but is i=1
meaningless for the specified task. where x is the desired value of the controlled process
target
In the semantic problem, shown in Fig. 4b, a lower MSE and x is the recorded value of the process at time step i.
i
on the reconstructed state is better, and the Level B solution Both RMSD are computed with respect to the central and
is Pareto dominant with respect to all others. The Level A vertical position of the CartPole: x = 0 and ψ = 0.
target target
solution also Pareto dominates static compression, while the These results help to evaluate how well the control dynamics
Level C solution only outperforms it for higher compression keep the CartPole near the optimal central position and to
levels, i.e., on the left side of the graph. assess the smoothness of the resulting pole oscillations. It
Finally, Fig. 4c shows the performance of the effectiveness is possible to see that, in general, a higher rate allows to
problem (Level C), summarized by how long the CartPole keep the angular RMSD smaller. In particular, in the Level C
systemmanagestoremainwithinthepositionandanglelimits. system,thevaluesarethesmallest.However,thiscomesatthe
The Level C solution significantly outperforms all others, but cost of deviating more from the central position, as shown in
is not strictly Pareto dominant: when the communication con- the figure. The policy prioritizes the stabilization of the pole
straintisverytight,settingastaticcompressionandretraining oscillations, though this requires deviating from the central
]Bd[RNSP.sbO
]spets[htgneledosipE
ESMetatS10
LevelA LevelB LevelC
StaticVQ-VAE StaticVQ-VAE(withoutretraining)
0.04π
0.03π 4
0.02π
2
0.01π
0 0
1 2 3 4 5 6 1 2 3 4 5 6
Averagemessagelengthℓ¯[B] Averagemessagelengthℓ¯[B]
(a)AngularRMSDfromthecentralposition. (b)PositionRMSDfromthecentralposition.
Fig. 5: Other performance metrics relative to the CartPole control problem.
LevelA LevelB LevelC scheme on CartPole pictures, it is possible to obtain lower
CompressAI(retrained) CompressAI JPEG bitrates while improving the resulting control performance.
Our approach, which is directly trained on the CartPole task,
outperformsallothers;however,wedonotclaimthatVQ-VAE
is the best compression technique for all scenarios. The main
400
contribution of this work is not in the specific compression
scheme,butratherinthedynamicandgoal-orientedadaptation
200
of the compression parameters for each transmitted update,
which could be directly applied to different NN architectures
and even JPEG.
0 0.5 1 1.5 2
log (ℓ) [B]
10 E. Analysis of the communication policy
Fig. 6: Comparison with other methods. We can then use an explainability approach to gain further
insights on how effective communication operates. Fig. 7
shows the distribution of the quantization level selected by
position.Thisisbecauseswingsinthepole’sangleareharder
an observer trained for each communication Level (A, B, and
to control due to the instability of the inverted pendulum, and
C).WenotethatweadaptedthescaleofβforthethreeLevels,
there is a significant risk that the pole might go out of the
soastoobtaincomparableresults:astherewardprocesstakes
acceptable range, ending the episode.
values in different ranges (e.g., the PSNR is in dB while the
reward of the MDP is between −1 and 1), using the same
D. Comparison with existing compression approaches
transmission cost would result in very different outcomes.
Inthissection,wecomparetheperformanceofourproposed We then chose to rescale the transmission cost parameter to
approach to that of other methods. Specifically, we show how have the full range of average bitrates at each communication
digital compression techniques, such as JPEG, perform in the Level. The similarity in the compression level distributions
samescenario.WealsocompareotherNN-basedcompression at the three Levels is striking. For lower values of β, the
models and evaluate their performance. For the digital com- observer selects ζ∅, which corresponds to no transmission,
pression,weusedifferentsetsofparameterscombiningimage more often. As β decreases, the communication cost becomes
resizing, the quality parameter of the JPEG standard, and the lower, and thus the observer chooses longer messages more
numberofcolorgrayscalelevels.Forlearning-basedcompres- often. Another common pattern is that quantizing features
sion,weusedtheCompressAIlibrarywhichimplementsthe using 1,2 or 3 bits is a rare choice. This shows that the
model proposed in [33]. memory implemented implicitly in the system through the
Fig.6showstheperformanceofothermethodswithrespect LSTM is powerful enough to obtain adequate beliefs based
to the proposed dynamic feature compression method. It is on past messages, so that the observer can rely on it and not
possible to see that the digital compression scheme does not sendanything.Aroughlyquantizedupdatehasarelativelylow
allowtheactortoeffectivelycontrolthesystem,asitsstability value, as its novelty is limited, and transmitting intermittent
is low even though the updates are bigger by two orders updates at a higher quality results in a better performance.
of magnitude. Obtaining a high control performance would However, the real difference between the three policies is
requireanextremelyhighbitrate.Ontheotherhand,theneural given by when they decide not to transmit. Therefore, we
compression scheme achieves a higher performance with a propose an analysis based on the visualization of the observer
limited bitrate, but since the model is a general compression policy and the receiver policy. In Fig. 8, four colormaps
technique designed to compress a wide variety of images, showdifferentpoliciesprojectedinthesamedomain:thepole
it cannot reach extremely low bitrates. After retraining the angle on the x-axis and the cart velocity on the y-axis. More
]spets[
htgnel
edlosipE
]dar[)ϕ(DSMR ]m[)x(DSMR11
1 1 1 1 1
0.8 0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2 0.2
0 0 0 0 0
0 2 4 6 0 2 4 6 0 2 4 6 0 2 4 6 0 2 4 6
ℓ ℓ ℓ ℓ ℓ
t t t t t
(a)LevelA,β=2. (b)LevelA,β=1.5. (c)LevelA,β=1. (d)LevelA,β=0.5. (e)LevelA,β=0.1.
1 1 1 1 1
0.8 0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2 0.2
0 0 0 0 0
0 2 4 6 0 2 4 6 0 2 4 6 0 2 4 6 0 2 4 6
ℓ ℓ ℓ ℓ ℓ
t t t t t
(f)LevelB,β=0.01. (g)LevelB,β=0.0075. (h)LevelB,β=0.005. (i)LevelB,β=0.0025. (j)LevelB,β=0.001.
1 1 1 1 1
0.8 0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2 0.2
0 0 0 0 0
0 2 4 6 0 2 4 6 0 2 4 6 0 2 4 6 0 2 4 6
ℓ ℓ ℓ ℓ ℓ
t t t t t
(k)LevelC,β=0.15. (l)LevelC,β=0.1. (m)LevelC,β=0.07. (n)LevelC,β=0.05. (o)LevelC,β=0.01.
Fig. 7: Distribution of the selected compression levels.
specifically,wequantizetheprojectedstateintocellsandshow moving towards. In these cases, the entropy is extremely low,
the policy of the robot and the observer in each cell. Fig. 8a andthetransmittercanavoidsendingnewupdatestotherobot.
shows the robot actions, averaged among 106 samples. Since This is due to the fact that, even if the estimated state at the
in the CartPole problem the actions are binary, we represent receiver differs from the observed one, the action to perform
the probability of choosing action Right in a range between remains the same and will be to push further the cart to try
0 and 1. Fig. 8b depicts the entropy of the robot policy. We to get the pole more vertical. Recalling (5), we note that if
(cid:16) (cid:16) (cid:17)(cid:17)
considered the action probability in the previous figure and Q ξ(r),π(r) ξ(r) is very sensitive to small variations in
t t
compute the action entropy as follows:
ξ(r), then the gap in (5) is going to be significant, leading the
t
H(a)=−p(a=0)log (p(a=0))−p(a=1)log (p(a=1)), observer to choose to send precise information. In principle, a
2 2
Level C transmitter could reduce the message length or even
wherep(a)isempiricallyestimatedbycountingthenumberof avoid transmission as long as the robot is able to choose the
times each action is chosen when the state is in the projected correct actions, even though its belief is incorrect. An optimal
cell.Fig.8candFig.8dshowtheaveragelengthoftheupdate communication scheme approximately follows
packets,i.e.,thenumberofBytestransmittedineachcellwhen
optimizing for Levels A and C, respectively. This can be seen
ℓ
∝V(cid:16) ξpri,(r),m(cid:17)
,
as the averagenumber of bits that the transmitter allocates for t t
each projected slice of the state space. Fig. 9 shows the same
which means that the message length is roughly proportional
results but for a different physical state projection, mapping
the angle ψ on the x-axis and the pole angular velocity ψ˙ on to VoI. This concept might be used when defining a heuristic
policy,whichbehavessimilarlytotheeffectivecommunication
the y-axis.
policybutismuchsimplertodesignandimplement.Notethat
In both figures, there is a strong correspondence between
this condition includes two separate cases in which a Level
the states where the robot entropy is higher and the states
C observer chooses not to transmit, while Level A and B
where the Level C policy allocates a higher number of bits.
transmitters would send precise data:
This confirms that an effective observer policy manages to
discriminate the uncertainty at the robot side. In regions of • The action corresponding to the prior belief is the same
thestatespacewhereitismoredifficulttoretrievethecorrect as the one after the updating message. In this case, the
action, i.e., the action entropy is higher, the observer will VoI of the communicated message is low and thus we
provide the robot with more precise information by sending can lower ℓ ;
t
longer messages. There are regions where the robot action is • The action is different after the communicated message,
always the same, e.g., whenever the cart is moving fast and butthelong-termrewardsarecloseenoughthattherobot
the tip of the pole is pointing to the same side the cart is is not going to benefit too much from choosing the other
]
ℓ[rP
]
ℓ[rP
]
ℓ[rP
t
t
t12
Left action Right action H(a t) ℓ¯ ℓ¯
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 2 4 6 0 2 4 6
1 1 1 1
0 0 0 0
−1 −1 −1 −1
−0.2−0.1 0 0.1 0.2 −0.2−0.1 0 0.1 0.2 −0.2−0.1 0 0.1 0.2 −0.2−0.1 0 0.1 0.2
ψ [rad] ψ [rad] ψ [rad] ψ [rad]
(a)ProbabilityoftakingactionRight. (b)Entropyofthepolicy. (c)LevelAbitrate. (d)LevelCbitrate.
Fig. 8: Analysis of the transmission policy as a function of the pole angle ψ and cart linear velocity x˙. The bitrate is measured
in Bytes per transmission.
Left Right H(a t) ℓ¯ ℓ¯
0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1 0 2 4 6 0 2 4 6
1 1 1 1
0 0 0 0
−1 −1 −1 −1
−0.2−0.1 0 0.1 0.2 −0.2−0.1 0 0.1 0.2 −0.2−0.1 0 0.1 0.2 −0.2−0.1 0 0.1 0.2
ψ [rad] ψ [rad] ψ [rad] ψ [rad]
(a)ProbabilityoftakingactionRight. (b)Entropyofthepolicy. (c)LevelAbitrate. (d)LevelCbitrate.
Fig. 9: Analysis of the transmission policy as a function of the pole angle ψ and angular velocity ψ˙. The bitrate is measured
in Bytes per transmission.
action. Even in this case, sending less information is not cause a poor estimation of the value function, which may in
going to affect the control performance significantly. turn cause the robot to choose a low-quality action.
In Fig. 10, we provide an analysis of the communication
These cases cannot be taken into account in Levels A and
strategy with respect to different AoI values. This allows to
B. Indeed, the Level A policy shown in Fig. 8c tends to
show how the memory of the robot and of the observer plays
allocate communication resources in the states where the
a crucial role on the communication decisions. In particular,
pictureischangingmorerapidly,sothatthememoryavailable
we consider five values of the AoI: AoI = 0 indicates that a
to the robot is less useful to estimate the current observation,
regardless of the correct action. As the cart speed x˙ increases message of any length was transmitted in the previous time
step. AoI =∆ with ∆∈{1,2,3,4} means that no messages
along the y-axis, the number of bits increases too. The same
have been received by the observer for ∆ time steps since
reasoning can be applied to the results in Fig. 9.
the last received message. This is a measure of how up to
Another general principle that we can deduce for an effec-
date the memory of the robot is, allowing us to evaluate
tivepolicyisthatitshouldbeawareofvariationsofthevalue
the next choice of the observer for a given age. We then
function with respect to the belief. If the value function is
consider the distribution of the observer actions (y-axis) with
strongly affected by small perturbations of the belief, then
respect to different ranges of the robot actions entropy (x-
the effective policy should communicate more information
in order to reduce the discrepancy between ξ(o) and ξ(r). axis).Thismeansthat,foreachentropyinterval,wecountthe
t t number of times each action is performed, in order to obtain
Thisreasoningcanbeintuitivelyunderstoodbylookingatthe
(cid:16) (cid:16) (cid:17)(cid:17) an empirical distribution. The columns are normalized so that
differential of the robot’s value function Q ξ(r),π(r) ξ(r)
t t each cell shows the probability that the observer chooses a
with respect to changes in its belief distribution ξ(r). When specific ℓ whenever the robot action entropy falls within the
t t
this value is big, an inaccurate estimation of the state would corresponding interval, for different values of the AoI.
]s/m[
x˙
]s/dar[
ψ˙
]s/dar[
ψ˙
]s/m[
x˙
]s/dar[
ψ˙
]s/m[
x˙
]s/m[
x˙
]s/dar[
ψ˙13
1
6 6 6 6 6
0.8
4 4 4 4 4 0.6
2 2 2 2 2 0.4
0.2
0 0 0 0 0
0
0 0.20.40.60.8 1 0 0.20.40.60.8 1 0 0.20.40.60.8 1 0 0.20.40.60.8 1 0 0.20.40.60.8 1
H(a ) H(a ) H(a ) H(a ) H(a )
t t t t t
(a)β=0.15,AoI=0 (b)β=0.15,AoI=1. (c)β=0.15,AoI=2. (d)β=0.15,AoI=3. (e)β=0.15,AoI=4.
1
6 6 6 6 6
0.8
4 4 4 4 4 0.6
2 2 2 2 2 0.4
0.2
0 0 0 0 0
0
0 0.20.40.60.8 1 0 0.20.40.60.8 1 0 0.20.40.60.8 1 0 0.20.40.60.8 1 0 0.20.40.60.8 1
H(a ) H(a ) H(a ) H(a ) H(a )
t t t t t
(f)β=0.1,AoI=0. (g)β=0.1,AoI=1. (h)β=0.1,AoI=2. (i)β=0.1,AoI=3. (j)β=0.1,AoI=4.
Fig. 10: Level C observer action distribution for different robot action entropy levels and values of β. The color of each cell
represents the empirical probability of choosing a packet length ℓ for a given entropy H(a ) and AoI. Each column of each
t t
subfigure then represents a conditional Probability Mass Function (PMF).
1
6 6 6 6 6
0.8
4 4 4 4 4 0.6
2 2 2 2 2 0.4
0.2
0 0 0 0 0
0
0 0.20.40.60.8 1 0 0.20.40.60.8 1 0 0.20.40.60.8 1 0 0.20.40.60.8 1 0 0.20.40.60.8 1
H(a ) H(a ) H(a ) H(a ) H(a )
t t t t t
(a)AoI=0. (b)AoI=1. (c)AoI=2. (d)AoI=3. (e)AoI=4.
Fig. 11: Level A observer action distribution for different robot action entropy levels with β =1.
Fig. 10a clearly shows that, if there was a transmission in system state is highly predictable, and the actor can rely on
the previous time step (AoI = 0), it is very unlikely that the its past knowledge to get a precise estimate of it, or the two
system is going to be updated again in the current time step. actions are almost equivalent, e.g., if the pole is balanced
As remarked above, Figs. 10a-e show the case with β =0.15, vertically. The former case is more likely to be a low-entropy
for which the observer almost always picks ζ to transmit. state,whilethelatterishigh-entropy,buthasasmalldifference
4
On the other hand, Figs. 10f-j show the case with β = 0.1, between the rewards for the two actions. We can see that the
in which the agent sometimes selects other codebooks due to trend holds for different values of β by looking at Figs. 10f-j:
the lower transmission costs. The observer often chooses to if we decrease the value of β, the observer tends to transmit
communicate if AoI = 1, with an exception if the system is moreoften,andusehighermessagelengthswhenittransmits,
in a very low entropy state, in which case the probability of but the general tendency to transmit more whenever the robot
communicatingusingζ issimilartotheonecorrespondingto action entropy is high clearly holds. This final analysis allows
4
action ζ∅. If we look at the behavior for higher values of the us to get an easy heuristic for effective communication when
AoI, we can notice a general trend: communication is more the value function is not available or cannot be learned.
likelytohappeninhigherentropystatesthaninlowerentropy
ones.Thisshowsthattheobserverpolicyunderstandsthecases Fig.11showsthattheLevelApolicyallocatescommunica-
where the state has to be precisely estimated by the robot tion resources without considering the entropy of the control
to choose its action correctly. Additionally, the probability actions. While Fig. 10 showed a clear monotonic trend in the
of transmitting an update actually decreases when the AoI probability of selecting ζ 4, which increased as the entropy
reaches higher values. The observer will only skip several increased, the pattern is much weaker in this case. The value
consecutivetransmissionopportunitiesintwocases:eitherthe ofβ forthiswaschosentogetasimilaroverallbitrate(and,as
wediscussed,asimilaroverallactiondistribution)totheLevel
ℓ
ℓ
ℓ
t
t
t
FMP
FMP
FMP14
Ccasewithβ =0.15.Aswediscussedabove,thereisaweak [11] F.Pase,S.Kobus,D.Gu¨ndu¨z,andM.Zorzi,“SemanticCommunication
correlationbetweentheactionentropyandfeaturessuchasthe ofLearnableConcepts,”Int.Symp.Inform.Theory(ISIT),Jun.2023.
[12] T.-Y. Tung, S. Kobus, J. P. Roig, and D. Gu¨ndu¨z, “Effective commu-
angular and cart velocities, but it is the latter that the Level A
nications: A joint learning and communication framework for multi-
policy considers: as the difficulty of accurately reconstructing agent reinforcement learning over noisy channels,” IEEE J. Sel. Areas
the image increases with the speed of the CartPole system, Commun.,vol.39,no.8,pp.2590–2603,Jun.2021.
[13] D.Kim,S.Moon,D.Hostallero,W.J.Kang,T.Lee,K.Son,andY.Yi,
more unstable states have more frequent transmissions.
“Learning to schedule communication in multi-agent reinforcement
learning,”in7thInt.Conf.LearningRepr.(ICLR),May2019.
VI. CONCLUSION [14] X.Zheng,S.Zhou,andZ.Niu,“UrgencyofInformationforContext-
Aware Timely Status Updates in Remote Control Systems,” IEEE T.
In this work, we presented a dynamic feature compression
WirelessCommun.,vol.19,no.11,pp.7237–7250,Jul.2020.
scheme that can exploit an ensemble VQ-VAE to solve the [15] J.N.Foerster,Y.M.Assael,N.deFreitas,andS.Whiteson,“Learning
semanticandeffectivecommunicationproblems.Thedynamic tocommunicatewithdeepmulti-agentreinforcementlearning,”in30th
Int.Conf.NeuralInf.Proc.Sys.(NeurIPS),Dec.2016.
scheme outperforms fixed quantization, and can be trained
[16] A.Goyal,R.Islam,D.Strouse,Z.Ahmed,H.Larochelle,M.Botvinick,
automaticallywithlimitedfeedback,unlikeemergentcommu- S.Levine,andY.Bengio,“Transferandexplorationviatheinformation
nication models that are unable to deal with complex tasks. bottleneck,”in7thInt.Conf.LearningRepr.(ICLR),May2019.
[17] F.Pase,D.Gu¨ndu¨z,andM.Zorzi,“Rate-constrainedremotecontextual
Thechoicesmadebytheobserverareclearlytiedtothecontrol
bandits,”IEEEJ.Sel.AreasInform.Theory,vol.3,no.4,pp.789–802,
policyoftherobotitaimstohelp,significantlyoutperforming Dec.2022.
a simpler optimization that does not take into account the [18] E. Akyol, K. B. Viswanatha, K. Rose, and T. A. Ramstad, “On zero-
delaysource-channelcoding,”IEEET.Inform.Th.,vol.60,no.12,pp.
semanticandeffectiveproblems.Wealsoanalyzedtheoptimal
7473–7489,Dec.2014.
policies to draw insights on their decisions, showing that the [19] A. Van Den Oord, O. Vinyals et al., “Neural discrete representation
Level C optimization indeed considers the robot’s policy. learning,”in31stInt.Conf.NeuralInf.Proc.Sys.(NeurIPS),Dec.2017.
[20] P. Talli, F. Pase, F. Chiariotti, A. Zanella, and M. Zorzi, “Semantic
A natural extension of this model is to consider more com-
and effective communication for remote control tasks with dynamic
plex tasks and wider communication channels, correspond- feature compression,” in 16th Int. Worksh. Wireless Sensing Actuating
ing to realistic control scenarios, or scenarios with multiple Rob.Networks(INFOCOMWiSARN). IEEE,May2023.
[21] A. Kosta, N. Pappas, A. Ephremides, and V. Angelakis, “The age of
transmitters with partial information about each other and the
information in a discrete time queue: Stationary distribution and non-
robot. Considering a more realistic channel model, which has linearagemeananalysis,”IEEEJ.Sel.AreasCommun.,vol.39,no.5,
a loss probability and time-varying statistics in addition to pp.1352–1364,May2021.
[22] Z.Wang,M.-A.Badiu,andJ.P.Coon,“Aframeworkforcharacterizing
the transmission cost, would also be an interesting direction,
the value of information in hidden Markov models,” IEEE T. Inform.
joining our model with Joint Source Channel Coding (JSCC) Theory,vol.68,no.8,pp.5203–5216,Aug.2022.
theory. Dynamically adapting JSCC parameters with the goal [23] E. Fountoulakis, N. Pappas, and M. Kountouris, “Goal-oriented poli-
cies for cost of actuation error minimization in wireless autonomous
ofhelpingaremoteDRLagentwouldbeanaturalextensionof
systems,”IEEECommun.Lett.(EarlyAccess),Jun.2023.
our proposed approach for more realistic wireless scenarios. [24] E.Bourtsoulatze,D.BurthKurka,andD.Gu¨ndu¨z,“Deepjointsource-
Another interesting direction for future work is to consider channel coding for wireless image transmission,” IEEE T. Cognitive
Commun.&Networking,vol.5,no.3,pp.567–579,May2019.
joint training of the robot and the observer, or cases with
[25] H. Xie, Z. Qin, G. Y. Li, and B.-H. Juang, “Deep learning enabled
partial information available at both transmitter and receiver. semantic communication systems,” IEEE T. Signal Proc., vol. 69, pp.
2663–2675,Apr.2021.
REFERENCES [26] F. Mason, F. Chiariotti, A. Zanella, and P. Popovski, “Multi-agent
reinforcement learning for pragmatic communication and control,”
[1] C. E. Shannon and W. Weaver, The mathematical theory of communi- arXiv:2302.14399,Feb.2023.
cation. UniversityofIllinoisPress,Sep.1949. [27] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, “Planning and
[2] D. Gu¨ndu¨z, Z. Qin, I. E. Aguerri, H. S. Dhillon, Z. Yang, A. Yener, actinginpartiallyobservablestochasticdomains,”Artif.Intell.,vol.101,
K.K.Wong,andC.-B.Chae,“Beyondtransmittingbits:Context,seman- no.1,pp.99–134,May1998.
tics,andtask-orientedcommunications,”IEEEJ.Sel.AreasCommun., [28] E. J. Sondik, “The optimal control of partially observable Markov
vol.41,pp.5–41,Jan.2023. processes over the infinite horizon: Discounted costs,” Oper. Res.,
[3] P. Popovski, O. Simeone, F. Boccardi, D. Gu¨ndu¨z, and O. Sahin, vol.26,no.2,pp.282–304,Apr.1978.
“Semantic-effectiveness filtering and control for post-5G wireless con- [29] R.S.SuttonandA.G.Barto,Reinforcementlearning:Anintroduction.
nectivity,”J.IndianInst.Sci.,vol.100,no.2,pp.435–443,Apr.2020. MITpress,Nov.2018.
[4] P. A. Stavrou and M. Kountouris, “A rate distortion approach to goal- [30] T.M.CoverandJ.A.Thomas,ElementsofInformationTheory,Second
orientedcommunication,”inInt.Symp.Inform.Theory(ISIT). IEEE, Edition. Wiley-Interscience,Sep.2006.
Jun.2022,pp.590–595. [31] D. P. Kingma and M. Welling, “Auto-encoding variational Bayes,” in
[5] S.Wan,Z.Gu,andQ.Ni,“Cognitivecomputingandwirelesscommu- 2ndInt.Conf.LearningRepr.(ICLR),May2014.
nicationsontheedgeforhealthcareservicerobots,”Comput.Commun., [32] P.KangandJ.Jo,“BenchmarkingmodernedgedevicesforAIapplica-
vol.149,pp.99–106,Jan.2020. tions,”IEICET.Inf.&Sys.,vol.104,no.3,pp.394–403,Mar.2021.
[6] R. D. Yates, Y. Sun, D. Richard Brown, S. K. Kaul, E. Modiano, and [33] Z. Cheng, H. Sun, M. Takeuchi, and J. Katto, “Learned image com-
S.Ulukus,“AgeofInformation:AnIntroductionandSurvey,”IEEEJ. pression with discretized Gaussian mixture likelihoods and attention
Sel.AreasCommun.,vol.39,no.5,pp.1183–1210,Mar.2021. modules,”inProceedingsoftheIEEEConferenceonComputerVision
[7] Y.Shao,Q.Cao,andD.Gu¨ndu¨z,“ATheoryofSemanticCommunica- andPatternRecognition(CVPR),Jun.2020.
tion,”arXiv:2212.01485,Dec.2022. [34] V.Mnih,K.Kavukcuoglu,D.Silveretal.,“Human-levelcontrolthrough
[8] E. Beck, C. Bockelmann, and A. Dekorsy, “Semantic communication: deepreinforcementlearning,”Nature,vol.518,no.7540,pp.529–533,
Aninformationbottleneckview,”arXiv:2204.13366,Apr.2022. Feb.2015.
[9] J.Shao,Y.Mao,andJ.Zhang,“Learningtask-orientedcommunication
for edge inference: An information bottleneck approach,” IEEE J. Sel. APPENDIX
AreasCommun.,vol.40,no.1,pp.197–211,Jan.2022.
[10] E.Uysal,O.Kaya,A.Ephremides,J.Gross,M.Codreanu,P.Popovski, THEINFORMATIONBOTTLENECKVIEW
M. Assad, G. Liva, A. Munari, B. Soret et al., “Semantic communi-
We can also consider another perspective on the observer’s
cations in networked systems: A data significance perspective,” IEEE
Network,vol.36,no.4,pp.233–240,Oct.2022. choices, using information bottleneck theory. We define a15
sufficient statistic i(s) of any given state s ∈ S, which is on its memory of past messages. Finally, to be even more
enough to determine the robot’s performance in that state. efficientandspecificwithrespecttothetask,theobservermay
Denotingthenumberofbitsrequiredtorepresentarealization optimize the message M to minimize a distortion measure
(cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17) t
of random variable X as b(X), we consider a case in which: d i ξ(o) ,i ξ(r) between the effective representation
C t t
b(i(S))<b(S)<b(O). of the observer’s belief on the state, which contains only the
task-specific information, and the knowledge available to the
Indeed, the observation may contain much more information robot. Naturally, any message instance m ∈ M must be at
t
thanneededtoestimatethestate[30],andlossilycompressing most L bits long, in order to respect the constraint. However,
the message to preserve the relevant information, removing definingthesufficientstatistici(cid:16) ξ(o)(cid:17)
maybehighlycomplex
redundant or irrelevant details, can ease communication re- t
and problem-dependent, and using the robot’s reward as a
quirementswithoutanyperformanceloss.Wecanalsoobserve
direct performance measure is significantly more direct, with
that i(S)→S →O is a Markov chain. The random quantity
the same guarantees.
i(S) represents the minimal description of the system with
respect to the robot’s task, i.e., no additional data computed
fromSaddsmeaningfulinformationfortherobot’spolicy.The
state S may also include task-irrelevant physical information
on the system. However, both S and i(S) are unknown
quantities, as the observer only receives a noisy and high-
dimensional representation of S through O. This is a well-
knownissueinDRL:intheoriginalpaperpresentingtheDeep
Q-Network (DQN) architecture [34], the agent could only
observe the screen while playing classic arcade videogames,
and did not have access to the much more compact and
precise internal state representation of the game. Introducing
communication and dynamic encoding adds another layer of
complexity.
We can then consider the case in which communication
is limited to a maximum length of L bits, i.e., to 2L+1 −1
messages,consideringallpossiblelengthslowerthanorequal
to L, including no communication. Naturally, this assumes
that the receiver has a way to discriminate between messages
of different lengths, e.g., through a MAC layer header. The
channel is ideal, i.e., instantaneous and error-free, but it
includes a constant cost per bit β, as in the observer reward
we gave in the previous section. Consequently, the problem
introduces an information bottleneck between the observation
O and the estimate oˆ that the robot can make, based on
t t
the message M conveyed through the channel. If we define a
t
distortionmeasureovertheobservationspaced :O2 →R+,
A
any communication introduces a non-zero distortion d (o,oˆ)
A
whenever b(o) > L, whose theoretical asymptotic limits
are given by rate-distortion theory [30]. If we also consider
memory, i.e., the use of past messages in the estimation of oˆ,
the mutual information between o and the previous messages
can be used to reduce the distortion, improving the quality of
the estimate.
In the semantic problem, the aim is to extrapolate the
real physical state of the system S from the compressed
t
observation M , which can be a complex stochastic function.
t
In general, the real state lies in a low-dimensional semantic
space S. The term semantic is motivated by fact that, in
this case, the observer is not just transmitting pure sensory
data,butsomemeaningfulpieceofphysicalinformationabout
the system. Consequently, the distortion to be considered in
this case can be represented by a measure d : S2 → R
B
over the semantic space, so that the distortion d (sˆ(o),sˆ(r))
B t t
is computed between the observer’s best estimate of the
state and the one performed by the robot based on M , and
t