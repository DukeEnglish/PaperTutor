PlantoGraphy:IncorporatingIterativeDesignProcessintoGenerativeArtificial
IntelligenceforLandscapeRendering
RONGHUANG,
TheHongKongUniversityofScienceandTechnology(Guangzhou),China
HAI-CHUANLIN,
TheHongKongUniversityofScienceandTechnology(Guangzhou),China
CHUANZHANGCHEN,
TheHongKongUniversityofScienceandTechnology(Guangzhou),China
KANGZHANG,
TheHongKongUniversityofScienceandTechnology(Guangzhou),ChinaandTheHongKong
UniversityofScienceandTechnology,China
WEI ZENG∗,
The Hong Kong University of Science and Technology (Guangzhou), China and The Hong Kong
UniversityofScienceandTechnology,China
Landscaperenderingsarerealisticimagesoflandscapesites,allowingstakeholderstoperceivebetterandevaluatedesignideas.While
recentadvancesinGenerativeArtificialIntelligence(GAI)enableautomatedgenerationoflandscaperenderings,theend-to-end
methodsarenotcompatiblewithcommondesignprocesses,leadingtoinsufficientalignmentwithdesignidealizationsandlimited
cohesionofiterativelandscapedesign.Informedbyaformativestudyforcomprehendingdesignrequirements,wepresentPlantoGraphy,
aniterativedesignsystemthatallowsforinteractiveconfigurationofGAImodelstoaccommodatehuman-centereddesignpractice.
Atwo-stagepipelineisincorporated:first,concretizationmoduletransformsconceptualideasintoconcretescenelayoutswitha
domain-orientedlargelanguagemodel;andsecond,illustrationmoduleconvertsscenelayoutsintorealisticlandscaperenderings
usingafine-tunedlow-rankadaptationdiffusionmodel.PlantoGraphyhasundergoneaseriesofperformanceevaluationsanduser
studies,demonstratingitseffectivenessinlandscaperenderinggenerationandthehighrecognitionofitsinteractivefunctionality.
CCSConcepts:•Human-centeredcomputing→Interactivesystemsandtools;Interactiondesign;•Computingmethodolo-
gies→Artificialintelligence.
AdditionalKeyWordsandPhrases:Landscaperendering,largelanguagemodel,scenegraph,generativeartificialintelligence
1 INTRODUCTION
WhileGoethereferstoarchitectureasfrozenmusic,Filordrawsananalogybetweenlandscapedesignandballet,which
aimsatdiscoveringhowtheelementsofnaturecanberecombinedresponsivetobothplannedandunforeseenuses
onadailyandseasonaltimescale[14].Thetraditionallandscapedesignprocessreliesonthecreativityandexpertise
ofdesignerstocreateuseful,comfortableandattractivespaces[3],whichcanbedividedintofourmainstages:a)
initialconceptualization,b)designdevelopment,c)3Dmodeling,d)rendering,asdemonstratedinFig.1(bottom).
Specifically,designersdevelopapreliminaryconcepttakingintoaccountsitecharacter,useracquirementanddesign
vision,thenrefinethedesignconceptintoadetailedplanthatincludesspecificdesignelements,suchasplantselection
andconfiguration.Theprocessoutputshuman-perspectivelandscaperenderingsthatdepictrealisticimagesofa
plannedsitewithallplants,allowingstakeholderstoevaluatethevisualqualityoflandscapedesigns.
∗WeiZengisthecorrespondingauthor.
Thisistheauthor’sversionofthework.Itispostedhereforyourpersonaluse.Notforredistribution.ThedefinitiveversionwaspublishedinACM
CHI’24,https://doi.org/10.1145/3613904.3642824.
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
ManuscriptsubmittedtoACM
1
4202
naJ
03
]CH.sc[
1v02171.1042:viXraCHI’24,May11–16,2024,Honolulu,HI,USA RongHuang,Hai-ChuanLin,ChuanzhangChen,KangZhang,andWeiZeng
Text to Image Model
AI-assisted
Design Process Sketch to Image Model
Depth to Image Model
Initial Design 3D Mastetplan
Conceptualisation Development Modeling Rendering
Traditional
Design Process Site analysis Space Organization
Human Perspective
Idea generation Plant Configuration Rendering
Fig.1. ComparisonoftraditionalandAI-aideddesignprocessforlandscaperendering.VariousAIGCmodelscanbeappliedto
differentstages,yetthesemethodsoperateinanend-to-endmannerwithoutintegratingiterativedesign.
Withtheadvancesingenerativeartificialintelligence(GAI),AI-basedimagegenerationtoolshaveemergedto
facilitatethelandscapedesignprocess.Pre-trainedtext-to-imagegenerationmodels,suchasStableDiffusion[37]and
DALL-E-2[36],cantakedescriptionsforagardenlike“arealisticpictureofalandscapedesignwithtrees,including
adogwoodwithpinkflowers,floweringplantssuchaswhitetulipsanddaisies”,andproducecorrespondinglandscape
rendering.Duetotheflexibility,GAItoolscanplayvaryingrolesatdifferentstagesofthedesignprocess,asillustratedin
Fig.1(top).Withinthecontextofhuman-perspectivelandscaperendering,AIisactingasco-creatorswiththedesigners,
refiningtheabstractconceptandgeneratinghumanperspectiverenderingsthroughtext-to-imagemodels[36,37].
However,existingGAItoolsprimarilyworkinanend-to-endmanner,lackingtheflexibilitynecessaryfordesigners
toincorporatecommondesignpracticesthatiterativelyrefinetheoutputs.Inparticular,weidentifythefollowing
limitationsbyexistingAI-supporteddesigningprocessfromliteraturereview(Sect.2)andaformativestudywith
landscapedesigners(Sect.3).1)Absenceofinteractivefunctionalityforiterativedesign.Landscapedesignisaniterative
processshapedbybothconceptandform,aimingatfindingtheapproachesthatbestrespondtoexpectations[14].
Existingend-to-endapproachesfocusondesigncontrolwithintextediting,alongwithin-coherentresultsinmultiple
generations,causingabsenceofinteractivefunctionalityforiterativedesign.2)Insufficientsensitivitytodesignarrange-
mentofelements.Landscapedesignersaretaskedwithselectingplantspeciesanddeterminingtheoptimalcombination
expressedusingdirectionalwordsandcommonlyusedplantwords,whichmaynotbeaccuratelyinterpretedby
text-to-imagemodels.Forexample,existingAIGCtoolsarenotcompatiblewithdescriptionslike“thedaisyislocated
belowthedogwood,andthewhitetulipispositionedtotherightofthedaisy.”
Toaddresstheserequirementsfromlandscapedesigners,wepresentPlantoGraphy,anintelligentsystemwith
interactivefunctionsforiterativerefinementandanenhancedcomprehensionoflandscapescenariodescriptions.As
illustratedinFig.2,PlantoGraphy hastwomainmodules:1)Concretizationmodule(Sect.4.3)thattransformsidea
descriptionstoscenariolayout.Themoduleintroducestheconceptofthescenegraph,whichrepresentslandscape
designasastructured,semanticdescriptionoftheobjectsandtheirrelationshipswithinascene.Employingadataset
comprisingscenedescriptions,scenegraphs,andlayouts,thismoduleutilizesalargelanguagemodel(LLM)totransform
thedesigners’descriptionsintolayoutswiththeguidanceofscenegraphs,whichprovidemoreinterpretableand
controllableinformationaboutthelandscapesceneforguidingtherenderinggenerationtask.2)Illustrationmodule
(Sect. 4.4) that generates layout-guided landscape rendering. We harvest a vegetation dataset commonly used in
landscapedesign,withtheparticipationofexpertdesigners.Utilizingthisdataset,thelatentdiffusionmodel[28,37]is
fine-tunedthroughaLow-rankAdaptation(LoRA)model[20],whichenhancesthemodel’sabilitytogeneratespecific
2PlantoGraphy CHI’24,May11–16,2024,Honolulu,HI,USA
Concretization Module Illustration Module
Design Concept Landscape Design
(Description) Rendering
Pairs Landscape Plants Images
(Desc-Graph) Knowledge Dataset
scA
a
p pi ec t wu ir te
h
o trf ea
e
sr e ia nl
c
ll ua dn id n-
g
F lee aw r- ns ih no gt F lee aw r- ns ih no gt LoRA
a dogwood and birch,
flowering plants like red
tulips b. aF cl ko gw re or us n a dr .e .. .in the Fine L- Lt Muned Scene graph Fine L- Lt Muned Layout Lay Do iu fft u-g siu oi nded C Lo am tep no tsed
Fig.2. Overviewofthesystemworkflow.PlantoGraphyincorporatesatwo-stageframeworktotransformdesignconceptsdescribed
intextualcontentintorealisticimagesoflandscaperendering.First,theconcentizationmoduleleveragesagraph-enhancedLLMto
transformdesigndescriptionsintolayouts,usingscenegraphstoimprovethecomprehensionofuserinput.Next,theillustration
moduleemploysafine-tunedLoRAmodeltogeneraterealisticlandscaperenderingsbasedonthelayout.
plantswithinthelandscapescenariowithhighprecisionandaccuracy.PlantoGraphyalsoincorporatesaweb-based
userinterface(Sect.4.2)thatenablesuserstoeasilygeneratelandscapedesignrenderingsanditerativelyrefinethemby
interactingwiththegraphandlayoutcomponents.
AthoroughassessmentofPlantoGraphyhasbeenconductedfromvariousperspectives.Forconcretizationmodule
assessment(Sect.5.1),twosubjectiveexperimentswithquantitativemetricspertainingtolayoutwereconducted,
highlightingtheessentialroleofincoperatingscenegraphinlayoutpredictionandinjectingdomainknowledgeusing
reasoningmethods.Forillustrationmoduleevaluation(Sect.5.2),threeexperimentsinbothobjectiveandsubjective
perspectiveswerecarriedout,withresultsdemonstratingtheeffectivenessofPlantoGraphyinimprovingcoherence
intheresultsofmultiplegenerationsandcomprehendingdesigners’intents.TofurtherevaluatetheproposedAI-
assistedlandscapedesignprocess,weconductedawithin-subjectsstudy(Sect.6)thatinvolvescreatinglandscape
renderingsusingPlantoGraphyandtraditionaldesignsoftwareseparately.TheresultssuggestthatPlantoGraphyholds
varyingdegreesofvalueatdifferentdesignstages.Thefindingsalsohighlighthuman-centeredAIassistanceindesign,
emphasizingtheimportanceofsupportingiterativerefinementofdesignsateachstageofthedesignprocess.
Themajorcontributionsandnovelaspectsofthisworkinclude:
• Framework.WeproposeanovelframeworkthatincorporatesaniterativedesignprocessintotheAI-assisted
designingpipelineforlandscaperendering.Thisframeworkmaximizescontroloverthecoherenceinmultiple
generationsofresults,andaddressesthechallengeofthegenerativemodel’sinsensitivitytocommonlyused
orientationwordsandplanttermsinlandscapedesignscenedescriptions.
• System.Weintroduceaninteractivesystem,namelyPlantoGraphy,tosupportthelandscaperenderinggeneration
anditerativerefinement.Thesystemisbuiltuponacustomdatasetcreatedincollaborationwithexpertdesigners
thatcomprisesrealisticplantsrenderingimages.PlantoGraphyspeedsuptheiterationbyallowingdesignersto
setconstraintsthroughinteraction,promotingtheoutputdesigntobemoreconsistentwiththeirexpectations
andincreasingtheinvolvementoflandscapedesignersintheAI-assisteddesigningprocess.
2 RELATEDWORK
2.1 GenerativeAIforCreativeDesign
Generativemodelsarecapableofgeneratingnewdatapointsthataresimilartothetrainingdataset[15].Therecent
surgeinGAIhasignitedgrowinginterestacrossvariouscreativedesignfields,includingfashiondesign[10,50],UI
design[24],andvisualizationdesign[52].GAIservestoeitherautomatespecifictasksortofostertheexploration
3CHI’24,May11–16,2024,Honolulu,HI,USA RongHuang,Hai-ChuanLin,ChuanzhangChen,KangZhang,andWeiZeng
ofcreativeideas[26]duringthecreationphaseamongvariousdesignprocesses[16,41].Recently,therehasbeena
growinginterestindiffusionmodels[18,42]amongdesignersandartists.Theseincludetext-to-image(T2I)diffusion
models[33,36,37]thatleveragetextualpromptstodirecttheimagegenerationprocess,andotherconditionalgenerative
modelslikeControlNet[56],T2Iadapter[32]andGLIGEN[28],whichallowmorefine-grainedinputconditions.While
theremarkableefficiencyandgenerationabilityhavebeenshownintechniquemanner,somelimitationsimpedethe
usabilityofGAIinpracticalwayofdesign.Thefirstandforemostreasonisthat,mostdiffusionmodelsarerealizedin
anend-to-endmannerwithinputtextpromptsandoutputimages,whichoverlookthe“exploration”stageasoneofthe
mostimportantpartsincreativeprocess[21].Inaddition,thelackofcontrollabilitycannotsupportstylisticconsistency
outputs,whichiscrucialforiterativedesigninpracticalscenarios[43].
ThisstudyaimstodevelopanAI-assisteddesignsystemforlandscaperendering.Threespecificchallengesare
addressed:integratingthedesignprocessintothemodel,obtainingsuitabletrainingdatasetsforthisspecificcontext,
andexertingcontroloverfactorssuchaslayout,plantpositioning,andsizing.WhileLLM-groundeddiffusion[29],
alongwithsomeothermodels[13,59],claimtooffercontrolovertheshape,position,andappearanceofgenerated
objects,ourexperimentshavedemonstratedthattheiroutcomesarehardtomeetexpectationsofdomainexperts.
Furthermore,thesemethodsdonotencompassthefinalstageofrenderingrealisticlandscapeviews.Tobridgethisgap,
wecontributeaLoRAfinetuningmodeltrainedonalandscapedatasetthatwehavecuratedourselves.
2.2 ReasoninginLargeLanguageModel
LLMs have demonstrated promising results across a range of downstream tasks [38]. The capabilities have been
exemplifiedbytheGPTseries[4,34],revealingemergentabilitiesthatbecomeapparentasthescaleoftrainingreaches
acertainthreshold[47].WhileLLMsareoriginallydesignedfortext-basedtasks,researchershavealsoventuredinto
otherdomainslikevision-languagereasoning.Forexample,visualGPT[49]fusesChatGPTwithvisualfoundation
modelstotacklevision-languagetasksinaninteractivemanner.Nevertheless,onesignificantchallengeimpedingthe
practicalapplicationofLLMsistheirlimitedabilityforreasoning.In-contextlearning(ICL)[12]addressesthischallenge
byenablingLLMstogenerateexpectedoutputswhengiveninputtext.Studieshaveshowcasedtheeffectivenessof
LLMsinsolvingcomplexreasoningproblemsthroughICL[45].Chain-of-thought(CoT)[48]approachesareintroduced
tobolsterthereasoningcapabilitiesincomplextasksbyintroducingintermediatereasoningstepsthatleadtothefinal
output.CoTcanbeusedwithICLintwomainways:Few-shotCoTandZero-shotCoT[27].Few-shotCoTapplies
thestep-by-stepreasoningintheformof<𝑖𝑛𝑝𝑢𝑡,𝑜𝑢𝑡𝑝𝑢𝑡 >→<𝑖𝑛𝑝𝑢𝑡,𝐶𝑜𝑇,𝑜𝑢𝑡𝑝𝑢𝑡 >,whilstZero-shotCoTdirectly
generatesintermediatereasoningstepstoderivetheanswers,asexemplifiedbythephrase“let’sthinkstepbystep”.
BuildingupontheCoTframework,severalworkssuchasAuto-CoT[57],Tree-of-Thought[55],multimodal-CoT[58],
andmultilingual-CoT[39]havebeenproposedtoelicitreasoningabilitiesofLLMsinvarioustasks.
LLM-groundeddiffusion[29]combinesGPTanddiffusionmodelstoperformconditionedtext-to-imagetasks,which
canbeappliedtolandscaperenderingtasks.However,thismodeldoesnotinherentlysupportiterativedesignprocess
thatisrequiredbylandscapedesigners.Thecentralchallengerevolvesaroundfindinganeffectivemethodtointegrate
designrationalesinthedesignprocess,intoLLM-baseddesign.Toaddressthislimitation,weleveragethescenegraph
concept,torepresentplantswithinalandscapeasentitiesandtheirspatialrelationshipsasedges.Experimentalresults
demonstratethatthisinnovativeapproachcanproducepreciseandrealisticlandscaperenderingsbasedondesigners’
intentions,andmoreimportantly,facilitateiterativedesignprocess.
4PlantoGraphy CHI’24,May11–16,2024,Honolulu,HI,USA
2.3 SceneGraph
Scenegraphisastructureddatamodelemployedtorepresentspatialrelationshipsandsemanticinformationofobjects
withinascene[22,23].Theabilitytointuitivelydepictspatialrelationshasmadescenegraphsanemergingtopicinboth
computervisionandAIresearch[6].Asagraph-basedrepresentation,scenegraphscanbeseamlesslyintegratedwith
deep-learning-basedgenerativemodels.Withadvancementsinconditionalimagesynthesis,techniqueshaveevolvedto
renderimagesbyconditioninggenerativemodelsonscenegraphs,suchasGANs[2,44]anddiffusionmodels[53].
Thesemethodsallowdesignersandartiststoworkinamoreabstractandintuitivemannerbymanipulatingobjects
andtheirrelationshipsratherthandirectlyeditingpixelsorvertices.
Notably,theconceptofthescenegraphishighlyalignedwiththeabstractbubblemapscommonlyemployedin
designsketch[11,17].Theintegrationofscenegraphswithgenerativemodelshasthepotentialtoenhancereasoning
capabilitiesandprovideuserswithgreatercontrol.Scenegraphshavebeeneffectivelyemployedinimagegeneration
throughthepredictionofboundingboxesandsegmentationmasks[2,22].However,thesemethodsprimarilyrely
onGAN-basedapproaches,whichcanpresentchallengesinachievinghigh-resolutionresultsandrecognitionofthe
renderingorders.WeaddressthesechallengeswithLoRAdiffusionmodelsforhighresolutionrenderingsandan
instance-basedlatentcompositionforcontrollingtheoverlappingorderofobjects.Conversely,thisstudyseeksto
explorethepossibilityofleveragingscenegraphsinconjunctionwithLLMsanddiffusionmodels.Thisrepresentsa
newandinherentlychallengingavenueofresearch,particularlyforcreativedesign.
3 DESIGNSTUDY
This section presents a group interview with landscape design experts aimed at gaining insights into the design
prerequisitesforcollaborativeeffortsbetweenhumansandAIinlandscaperendering(Sect.3.1).Inlightofthefindings,
weconsolidatedesigngoalstobeachievedforPlantoGraphy(Sect.3.2).
3.1 GroupInterview
Weconductedonlineinterviewswithfivelandscapedesignprofessionals(2designersinemploymentU1andU2,2
landscapedesignstudentsU3andU4,1landscapedesignresearcherU5).Alldesignershavemorethanthreeyearsof
landscapedesignexperience.Eachinterviewlasted40-60minutes.Specifically,asparticipantsU2andU4hadnoprior
experienceusingAItools,weprovidedanintroductiontoexistingtoolsandbasicinstructionsbeforeinterviews.We
designedaquestionoutlinemainlyfocusingonthreetopicsbasedontherelevantliteraturereviewandaskedeach
designertoanswerquestionsaccordingtotheirdesignexperiencewithnospecificlimitation:1)thegeneralworkflow
oflandscapedesign,2)collaborationwithartificialintelligence,and3)theprosandconsofexistingAI-aidedlandscape
designingtoolsaccordingtotheirusingexperience.
Attheendoftheinterview,wesummarizedthecoreinsightsbasedonthedesigners’feedbackasfollows:
Landscape designing process. We first surveyed relevant literature and industrial standard documentation to
summarizethetraditionallandscapedesignprocessandwhatcontentsareusuallyincluded.Withdetailssupplemented
by the designers’ practice experience, the traditional landscape designing process mainly comprises four stages
(Fig.1(bottom)):First,designersdeterminetheinitialdesignconceptsaccordingtotheinformationcollectedfromsite
analysisandculturalstudy.Second,theydevelopdesignsbyorganizingspaceandflowonthesketchofmasterplan.
Third,designswillbemodeledinprofessional3Dmodelingsoftwareandrefinetheconstructiondetail.Finally,designers
5CHI’24,May11–16,2024,Honolulu,HI,USA RongHuang,Hai-ChuanLin,ChuanzhangChen,KangZhang,andWeiZeng
willrenderthe3Dmodelintoarealscenethroughrenderingenginestotestthevisualexperiencefromahuman
perspective.It’sworthnotingthatparticipantsmentionedthatinthepracticeproject,
"...designrequirementsarealwaysadjusted,theaforementionedprocessiscyclicalratherthansequential,untilanoptimal
fitbetweenconceptandapproachisachieved,asrefereediterativedesign."-U1&U2
CollaborationwithAI.AIcanbeutilizedtoassistatdifferentstagesoflandscapedesign,asillustratedinFig.1.When
theinputisascenariosketchoradepthmap,thedesigniscompletedbydesignerswhiletheAIservesasanacceleration
toolinthedrawingprocess.AnothersituationisthattheAIactsasaco-creatorforthelandscapedesignerwhenthe
inputisaninitialconceptualdescriptionexpressedintextualprompts.ParticipantsU3andU4expressedapreference
forthisco-creativeapproach,whichallowsdesignerstoimparttheirideasbydeliveringdiverseinterpretationsof
thetextualconcepts.Thisalignswithcommondesignpracticeswheredesignersoftenreceivevagueandabstract
descriptionsfromclients.However,U1andU2expressedconcernsaboutthepotentiallossofcontrolfordesigners
whenusinggraphicaltoolstobypassessentialstepsinthelandscapedesignprocess.Theyallagreethatwhendesigners
aredeprivedofmulti-dimensionalcontrolovertheoutcome,themeaningofdesignislost.
"IfeellikeI’mmoreofauserthanadesigner."-U1
Existingtoolsforlandscapedesign.Allparticipantsagreedthatcurrenttoolscanprovidedesignerswithmultiple
solutionstoinspirethem,buttheyrelyheavilyontextinput,whichisnotalwaysaccuratelyunderstoodbythemodel,
particularlywithregardtoorientationdescriptions.Besides,participantsexpressedapreferenceformoreinteractive
functionsofgraphicalinterfacescomplementedwithtextualpromptsthatconveythelogicofdesigners’thinking,
allowingdesignerstoexpresstheirdesignintentinmultipleways(U3).U1andU2notedthateachtimetheyrefined
thedescription,theoutputsbyexistingmodelsvarieddrastically,farawayfromdesigners’needstoiterativelyrefine
thedesigntoaccommodatechangingconditions.Additionally,designersneedtoselectplantspeciesconsideringthe
environmentalconditionsandcharacteristicsofspeciesanddeterminingthebestlocationforeachplantbasedonits
environmentalrequirementsandaestheticconsiderations.
"Currentmodelhaslimitedplantspeciesavailableresultinginincorrectplantspeciesgeneration."-U1&U3&U4
3.2 DesignGoals
Ourobjectiveistoharnesshuman-AIcollaborationstosupportthecreativedesignprocessforlandscaperenderings.
Thisprocessentailsprogressingfromgeneralconceptstodetailedelements,withiterativeadjustmentsandrefinements
madealongtheway[41].InanAI-assisteddesignapproach,thegoalistominimizerepetitivetaskswhileenhancingthe
designer’screativity,ratherthanreplacingthem[1,5].Toachievethis,thesystemneedstoenhanceitsabilitytointeract
withdesigners,acceptingtheirguidanceandactivelysuggestingpossiblesolutions,thuscompletingafeedbackloopin
thecreationprocess[40,41,54].InlinewiththeHCIcommunity’sexplorationofaugmentingusercontrolovermodels,
enhancingdesignerinvolvementintheAI-supportedcreatingstagecanfullyleveragespecializedknowledge[40].The
graphicaluserinterface(GUI)playsacrucialroleasauser-friendlyandefficientinteractionapproachtoacceptinputs
fromdesigners.Thisincludesusingbuttonsandslidersforinputtingbasicsettings[9],texteditorsforexpressingdesign
concepts[7,51],andgraphicalcontrolformodification[8,46].
Basedonthefeedbackgatheredfromthedesignstudyandexistingdesignpractices,weformulateasetofdesign
goalsforourintelligentlandscapedesignsystem.
• G1:Improvementoftheresult’sgraphicalcoherencetosupportiterativedesign.Tomeettheiterative
modificationneedsofdesigners,oursystemshouldenhancethegraphicalconsistencyofmultiplegenerating
6PlantoGraphy CHI’24,May11–16,2024,Honolulu,HI,USA
resultswiththeexceptionofthemodifiedportionwhenuserinputismicro-changed,e.g.,slightlychangingthe
plants’positionandsizes.
• G2:Enhancedmodel’scomprehensionoflandscapedesigndescriptions.Oursystemshouldbeequipped
withamodelcapableofcomprehendingdesigndescriptionsinlandscapescenarios,particularlyorientation
wordsthatarecommonlyusedbydesigners.Additionally,itshouldsupportaccurategenerationforexpert
commonly-usedplantspecies.
• G3:Interactiveeditingfunctionforin-depthdesignerparticipation.Oursystemshouldoffermultiple
waysforlandscapedesignerstoexpresstheirdesignrequirements.Inlinewiththethinkinglogicofdesigners,
enablingtextualpromptsasinputisnecessary.Agraphic-basededitingfunctionisasuitableaddition,enabling
designerstointeractwiththesysteminamoreintuitiveandvisualmanner.
4 PLANTOGRAPHYSYSTEM
ThissectionintroducesPlantoGraphy,anovelsystemthatsupportsthesteerablegenerationoflandscaperendering
fromscenedescriptionandfacilitatesiterativedesignbyinteractivefunctionality.Wesummarizehowthesystemdesign
respondstothedesigngoalsinSect.4.1,followedbyadetaileddescriptionoftheinterfacedesign(Sect.4.2).Thenwe
introducethemainmodulesofthesystem:aconcretizationmodule(Sect.4.3)andanillustrationmodule(Sect.4.4).
4.1 SystemOverview
PlantoGraphyiscraftedforexperiencedlandscapedesignersproficientingeneratinglandscaperenderingsusingthe
systemanditerativelyrefiningtheresultsaccordingtodesignrequirementsthroughtheinteractiveeditingmodule.To
accomplishthegoal,wedesignatwo-stageapproachasdemonstratedinFig.2:
• Concretizationmodule.Themoduleharnessesdomain-specificLLMstotranslateusers’conceptualideas,as
depictedintextualdescriptions,intoconcretescenelayouts.Aninnovationintroducedhereistheincorporation
ofscenegraphasanintermediarylinkbetweentextandlayouts.ScenegraphrepresentationenhancestheLLM’s
abilitytocomprehendlandscapedesigndescriptions(G2)whencomparedtodirectconversion.Furthermore,the
scenegraphactsasamediumforuserstocustomizetheirdesigns,complementingtext-onlyuserinteractionsto
improvegraphicalcoherence(G1)andenhanceinteractiveediting(G3).Themoduleoffersscenegraphandlayout
visualrepresentations,tofacilitatetheiterativeideaexpressioninanintuitiveandinteractivemanner.Inthe
backend,twoLLM-poweredgeneratorsguidedbywell-structuredprompttemplatessupportthetransformation
betweentext,scenegraph,andlayout.Thesepromptsincorporatebothreasoningenhancementtechniquesand
domain-specificknowledgerelevanttolandscapedesign.Furthermore,interactivefeaturesenabledesignersto
makeadjustmentsintuitivelybyeditingthegraphandlayout.Thisincludesactionssuchasaddingorremoving
nodesandresizingelementstoaccommodateevolvingdesignneeds.
• Illustrationmodule.ThemoduleemploysaLoRAfinetuningmodeltoconvertscenelayoutsintorealistic
landscaperenderings.Oneofthesignificantchallengesfacedinthisprocessisthescarcityofavailabletraining
samples.Toaddressthischallenge,wehavecuratedavegetationdatasetcommonlyutilizedinlandscapedesign
withtheinvaluableinputofexpertdesigners.Thedatasetencompassesawiderangeofplanttypes,alongwith
detailedattributeslikecommonplantcompositionpatterns.Thismakesthegeneratedoutputsexhibitgreater
graphicalcoherencewithuserdescriptions,aligningwithG1.Furthermore,wetackletheissueofoverlapping
objectorder,byimplementinganinstance-basedlatentcompositionprocess.Thismoduleoffersfullautomation,
7CHI’24,May11–16,2024,Honolulu,HI,USA RongHuang,Hai-ChuanLin,ChuanzhangChen,KangZhang,andWeiZeng
therebysavingdesignerstimeindesigndevelopmentand3Dmodelingandempoweringthemtovisualizehow
theirconceptualideastranslateintohuman-perspectiveviews.
Fig.3. InteractivevisualinterfaceforPlantoGraphy.UserscaninputtextualdescriptionsofthesceneintheTextPanelandcustomize
thedesignbymanipulatingthescenegraphintheGraphPanelandupdatingthelayoutintheLayoutPanel.Therenderingresultsare
presentedintheRenderingPanel.
4.2 UserInterface
Aninteractiveinterfaceisstrategicallycraftedtobridgethegapbetweentherapidgenerationcapabilitiesofcurrent
generativemodelsandthecontrolled,iterativenatureoftraditionaldesignprocesses.Itempowersprofessionaldesigners
toharnessthetime-savingadvantagesoftext-to-imagetechnologywhileretainingasmuchcontroloverthedesign
developmentastraditionalmethodsallow,ensuringaworkflowthatisbothefficientandalignedwiththenatural
progressionofideadevelopment.TheinterfacecomprisesfourpanelsdepictedinFig.3,allowingusertodeveloptheir
ideainaprocedural,controllableandinteractiveway:text,graph,layout,andrendering.
• TextPanel.TheTextPanelinitiatesthedesignprocess.Designerscanenterdescriptivetext,includingplant
species,quantities,andpositionalrelationshipsbetweenobjectsinthedesiredscene.Forexample,"Arealistic
pictureofalandscapedesignwithtrees,includingadogwoodwithpinkflowers,floweringplantssuchaswhitetulips
anddaisies.Thedaisyislocatedbelowthedogwood,andthewhitetulipispositionedtotherightofthedaisy."To
assistdesignersinavoidingtheneedtorepeatedlyinputcommonconstrainprompts,weoffercommonlyused
optionsrelatedtolandscapedesign,includingslidersfortimeandseason,aswellasrenderingstyles.Additionally,
atthebottomofthepanel,weprovideahistoricalmoduletostorepreviousdesignflows,allowingdesignersto
iteratedesignsacrossdifferenttimesandprojects
• GraphPanel.TheGraphPanelorganizesandvisualizesthescenegraphgeneratedfromdesigners’descriptions,
whilealsoenablingiterativeadjustmentstothescenegraph.Eachindividualplantisrepresentedbyanodeon
8PlantoGraphy CHI’24,May11–16,2024,Honolulu,HI,USA
User
Prompt Text Graphical control
Template Design
�)) �)) “Scene graph is a Design Design
d ri er pe rc et se ed n tg sr a sp ch e nw eh i ic nh the Requirement Requirement
following format... ”
Scene Graph Interaction function Layout
Generator Generator
“Plants’ layout in a Landscape "objects": ["banya", Banya: ('banya', Illustration
s r Rc u ee l ln e ae s t : if vo A el s l p so e iw c zi t en g r r a ut t lh i er o ,e e r ule, Knowledge " "d ro eg lw ao to id o" n] ships": [0, [ D1 o0 g0 w, o o2 d0 :6 , ( '4 d0 o, g w1 o0 o0 d] ') , Module
Scaling rule.....” "on the right of", [0, 156, 90, 180])
1], .... ...
Designer
Fig.4. Frameworkofideaconcretizationviaexpert-engagedinteractionwithLLMsforscenegraphgeneratorandlayoutgenerator.
thegraph,andtheorientededgerepresentstherelationshipbetweennodes.Thelabelontheedgeisapreposition
indicatingtheorientationrelationshipbetweennodes.Designerscancreateandmanipulatenodesandedgesto
representplantsandtheirrelationships.Aplantdatabasepanelisprovidedontherightside,allowingdesigners
todirectlydragtheplantimageasinput.
• LayoutPanel.TheLayoutPanelpresentstheinferredlayoutofthescenebasedontextualinputsandthe
scenegraph,finalizingspatialdetailsbeforetransitioningtoconcretelandscaperenderings.Thepaneldisplays
eachnodeinthegraphasabox,withabsolutepositionalcoordinatesandaccurateplantsize,inferredfromthe
concretizationmodule.Designerscanadjustpositionsandsizesofelementstofinalizethespatiallayout.
• RenderingPanel.TheRenderingPanelshowcasesthefinallandscaperenderingsgeneratedbythesystem
basedontheintermediatescenegraphandlayout.Thegeneratedimagesbytheiterativemodificationsarealso
storedatthehistoryviewinthebottom,whichcanbeeasilyclickedonbytheusertomakecomparisons.
4.3 IdeaConcretizationviaLargeLanguageModel
Theconcretizationmoduleisdesignedtotransformtextualscenedescriptionsintoconcretelayoutsofdesignelement
boundingboxes,servingasvisualguidanceandconditionalinputforthegenerativemodelinsubsequentsections.
Drawinginsightsfromthegroupinterview(Sect.3.1),wheredesignersexpressedtheexpectationsofsupporting
iterativedesignandimprovingthecomprehensionofdesignintentions,weproposetointegratescenegraphasan
intermediatecomponentintheprocess.Thescenegraphaidsinorganizingdesignelementsatanabstractlevel,avoiding
earlyconstraintsimposedbyconcretedetailsandseamlesslybridgingthetransitionfromtextualconceptualideasto
finallayouts.Assuch,wedeveloptwoLLMgenerators:thefirstonetransformsscenedescriptionstoascenegraph,and
thesecondonetransformsthescenegraphtoalayoutseparately,throughpromptengineering.Specifically,thesecond
generatorisreasoning-intensive,requiringspatialanddomain-specificreasoninginlandscapedesign.TheLLMneeds
toinfertherelativepositionsofplantsinatwo-dimensionalspace,deducethesizeofeachplantbasedonlandscape
knowledge,andrefinethelayoutconsideringvariousconstraints.TheoverallframeworkisillustratedinFig.4.
Consequently,wehavedevelopedtwodistinctprompttemplates,asshowninFig.5.Bothtemplatescomprise
fourmaincomponents:thetaskdescription,constraints,contextualinformation,anddemonstrations.Toequipthe
LLM’swithdomain-specificreasoningcapabilities,thepromptforthegraph-to-layoutgeneratorincludesalandscape
knowledgecomponentascontextualinformation.Below,weusethegraph-to-layoutgeneratorasanexampleto
illustrateeachcomponent.ThecompleteprompttemplatesareshownintheSupplementaryMaterial.
9CHI’24,May11–16,2024,Honolulu,HI,USA RongHuang,Hai-ChuanLin,ChuanzhangChen,KangZhang,andWeiZeng
a
Prefix
You are an expert in bounding box generation.
b
General description
Giving you a scene graph and a set of generation rules, your task is to generate bounding box for each object
in the given scene graph based on the spatial relation descripted in scene graph and generation rules.
Each bounding box should be in the format of (object name, [top -left x coordinate, top -left y coordinate,
box width, box height]) and include exactly one object.
c Step by step instruction
Your task should be done in three steps.
Step 1: Determine the position of each object based on scene graph. From left to right, the x position decreases.
Step 2: Determine the size of each object based on the aspect ratio rule and relative size rule.
Step 3: Adjust the size of each object based on the scaling rule. (determine the distance relationship
between objects, then scale the object behind)
d
Constraints
The images are of size 512x512, and the bounding boxes should not overlap or go beyond the image boundaries.
e
Contextual information
Aspect ratio rule: Relative size rule: Scaling rule:
1.Banya have aspect 1. The height of If there is relation that present the front and back relation,
ratio as 4:10 dogwood is twice as such as [a, in front of, b], the size between a and b should
2.Dogwood have aspect high as banya. have a scaling relation. The object behind should scale the
ratio as 2:4 width and height smaller than before, keeping the aspect ratio.
... ... ...
f
Demonstration
Input scene graph: "objects": ["banya", "dogwood"]
"relationships": [0, "on the right of", 1], [0, "in front of", 1]
...
Fig.5. Prompttemplateforlandscapescenelayoutgeneration.Thetemplateconsistsoffourmaincomponents:taskdescription,
constraints,contextualinformation,anddemonstrations.
• TaskDescription:Thepromptcommenceswithaprefixprompt(Fig.5(a))followedbyageneraldescription
(Fig. 5 (b)). The prefix prompt improves LLM’s ability to perform specialized tasks with role conditioning.
Thegeneraldescriptionelucidatestheinput,output,andoutputformats.Wetransformnon-naturallanguage
entities, such as scene graphs and layouts, into a sequential format to enhance comprehensibility for the
LLM.Thescenegraphstructureislinearizedasaseriesoftriples<𝑎,𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛,𝑏 >,where𝑎and𝑏represent
nodesinthegraph,and𝑟𝑒𝑙𝑎𝑡𝑖𝑜𝑛 correspondstotheedgeconnecting𝑎 and𝑏.Thelayoutisrepresentedas
[𝑜𝑏𝑗𝑒𝑐𝑡𝑛𝑎𝑚𝑒,[𝑥,𝑦,𝑤𝑖𝑑𝑡ℎ,ℎ𝑒𝑖𝑔ℎ𝑡]].Tobolsterreasoningcapabilitiesduringthegenerationprocess,weemploya
chain-of-thoughtstrategyasastep-by-stepinstruction(Fig.5(c)).
10
noitpircseD
ksaTPlantoGraphy CHI’24,May11–16,2024,Honolulu,HI,USA
Fig.6. Frameworkoflandscapeillustrationvialayout-guidedlandscaperenderinggeneration.
• Constraints:Theconstraints(Fig.5(d))definetheboundarieswithinwhichtheLLMisauthorizedtoperform
reasoning.Ourpromptsincorporatevarioustypesofconstraints,suchasrestrictionsinvisiontasksandlimitations
onthenumberofgeneratedelementsingraphs.
• Contextualinformation:WhileLLMspossessreasoningabilities,directlyexpectingthemtoreasonwithunfamiliar
professionalknowledgeabsentinthepre-trainedmodelcanbechallenging.Therefore,weintroducethreekey
rulesinlandscaperendergenerationforlandscapedesignascontextualinformationfortheLLMtoreference
duringreasoning.Theserulesencompasstheaspectratioofeachplant,therelativesizesbetweenplants,andthe
scalingeffectforperspectiverelationship(Fig.5(e)).
• Demonstration:Weprovide5question-answerexamplestofacilitatetheLLM’sreasoningabilityandensure
alignmentwiththedesiredansweringformatthroughfew-shotlearning(Fig.5(f)).
4.4 LandscapeIllustrationviaCustomizedDiffusionModel
The illustration module takes the layout combined with textual prompts as inputs to produce realistic landscape
renderings.Thelayoutdictatestheplacementofplants,whilethepromptspecifiesoverallsceneconditionslikeseason
andweather.Toaddressthelimitationsofreliablygeneratingspecificplantsandaccuratelydeducingoverlappingorder
facedbyexistinggenerativemodels,wehavedevelopedamorecontrollableframework,asillustratedinFig.6.This
frameworkiscomposedofseveralcomponents.First,weemployGLIGEN[28]toactasthefoundationalmodelfor
open-set,layout-guidedimagegeneration.WetrainLoRAmodels[20]tofine-tuneGLIGEN,enhancingitsabilityto
generatedomain-specificplants.Additionally,aninstance-basedlatentcompositionprocessisemployedtomanagethe
depthrelationshipsamongobjects.Thisintegratedapproachenhancesthemodel’scapacitytoproducecontrollableand
consistentlandscaperenderings,tailoredtotherequirementsofprofessionallandscapedesign.
4.4.1 Layout-guideddiffusionmodel. GLIGENisusedasitsbasemodelforhigh-quality,layout-guidedimagegeneration.
GLIGENenhancespre-trainedmodelsbyintegratinglocationinputsthroughanewtrainablegated-attentionlayer,
preservingtheoriginalpre-trainedweights.Thisenhancementallowstheframeworktoeffectivelygenerateimages
basedonbothtextualdescriptionsandlayoutinformation,whilemaintainingrobustzero-shotcapabilities.
4.4.2 Modelfine-tuningforbetterplantgenerationcontrol. Toimproveplantgeneration,wefurtherperformfine-tuning
ofthebasemodelusingaself-curateddatasetwiththehelpoflandscapedesigners,whoprovidetheirexpertiseand
creativityinselectingthebestplantsandarrangementsforeachscene.Thedatasetcompriseshundredsofrealistic
renderingimagesinvariousscenesandincludes11typesoftreesand11typesofbushes,witheachplanttypeconsisting
11CHI’24,May11–16,2024,Honolulu,HI,USA RongHuang,Hai-ChuanLin,ChuanzhangChen,KangZhang,andWeiZeng
Fig.7. Resultsofwith(bottom)andwithout(top)instance-basedlatentcomposition.
Fig.8. Comparisonofgeneratedresultsbetweenourmethodtoexistingapproaches.
ofabout20images.Thedatasetisusedtofine-tuneLoRA,aneffectivefine-tuningmethodthatupdatesonlyasmall
setofparametersinlargepre-trainedmodels.WetrainLoRAmodelsbasedonStableDiffusionUNetandmergethe
LoRAweightswiththefrozenStableDiffusionUNetpartofGLIGEN.Experimentsshowthatthenewtypesofplants
introducedbyourLoRAmodelcanbecorrectlygeneratedwithlayoutconditions,evenforneverencounteredconcepts.
12PlantoGraphy CHI’24,May11–16,2024,Honolulu,HI,USA
4.4.3 Instance-basedlatentcomposition. Drawinginspirationfrom[29],weadoptaninstance-basedlatentcomposition
method,toaddressthelimitationofGLIGEN’sinsensitivitytothefrontandbackpositions.Themethodtreatseach
plantasaseparateforegroundinstance,distinctfromthebackground.Thegenerationprocessisoutlinedasfollows:
(1) UtilizeGLIGENtogenerateimagesforeachplantinthecorrespondingboundingboxseparately.
(2) EmploySAM[25]tosegmentthelargestobjectinsidetheboundingboxforeachplant.Typically,thelargest
objectcorrespondstothetargetplantduetothecharacteristicsofGLIGEN.Storethesegmentedobjectmasks.
(3) UtilizeDDIMinversiontoretrievethelatentrepresentationofeachplantimage.
(4) Determinetheorderoftheplantobjectswithinthescenegraph,prioritizingthemfromthebacktothefront.
(5) Extractthelatentwithinthesegmentedobjectmaskforeachlatentrepresentationoftheplantimage.Sequentially
replacetheextractedlatenttothecorrespondingarea(e.g.,thesegmentedobjectmask)witharandomGaussian
noise.Thelatentoftheforemostplantobjectwillbethelastonetooverlap.Afterthisreplacingprocess,the
resultinglatentrepresentationisreferredtoascomposedlatent.
(6) UtilizeGLIGENonceagaintogeneratethelandscapewiththecomposedlatentasinitiallatent.Duringthe
reference,alltheareascontainingplantswillbefrozenforcertainsteps.
Fig.7comparestheresultsbythemodelswithandwithoutinstance-basedlatentcomposition,usingthesameprompts
andseeds.Theresultsindicatethatthelandscapesbyoursadheretoappropriateforegroundandbackgroundrelations,
asguidedbythescenegraphandthefront-to-backorderoftheplantobjects.Unlikesimplyoverlayingobjectsinthe
RGBpixelspace,theoverlappinginlatentspaceensuresgreatercoherencewithseveralinferencesteps.Fig.8presents
generatedlandscapesbasedontheinputpromptsandlayout,byourmethod(row3)andtwobaselines(rows4&5).
Theresultsdemonstratethenecessityofincorporatinglayoutguidanceandmodelfine-tuninginourframework.
5 EXPERIMENTSFORMODULEEVALUATION
Weconductexperimentstoevaluatetheperformanceoftheproposedmodules.1)Concretizationmoduleevaluation
(Sec.5.1):Three-dimensionalquantitativemetricsareemployedtoassesstheaccuracyoftheLLM-poweredgenerator
intransformingtextintothescenelayout.2)Illustrationmoduleevaluation(Sec.5.2):Weutilizestructuralsimilarity
indexmetric(SSIM)[19]andconductacontrolledwithin-subjectsuserstudy,toevaluatethegraphicalcoherenceofthe
resultsgeneratedduringiterativedesign(Sect.5.2.1).Wefurtherconductauserstudytoverifywhetherourmethod
succeedsinenhancingthemodels’comprehensionofplant-relatedknowledge(Sect.5.2.2).
5.1 Experiment1:Concretizationmoduleevaluation
Metrics.Following[29],weexaminetheperformanceoftheconcretizationmoduleforlayoutpredictionfromthree
dimensions:object-attributeassignment,spatialreasoning,andperspectiverelationalreasoning,asfollows:
• Object-attribute assignment is evaluated by two metrics: First, correctness of aspect ratio [60] measures the
correctnessofaspectratioofasingleplant,toassessobjectdistortioninthegeneratedrenderings;andsecond,
correctnessofrelativeareasmeasuresthecorrectnessofrelativesizebetweenplants,toassessvisualcoherence,
balance,andlogicalstructureoftheplants.Specifically,tomeasurecorrectnessofaspectratio,wecalculateL1
errorbetweentheaspectratioofaground-truthplant(denotedas𝐴𝑅 𝑔𝑡)andtheoneofthegeneration(denoted
as𝐴𝑅 𝑔𝑒𝑛).Asingleplantaspectratioiscorrectif∥𝐴𝑅 𝑔𝑡 −𝐴𝑅 𝑔𝑒𝑛∥ 1 <𝜃,where𝜃 issetto0.05.Werandomly
sample100scenelayoutsasgroundtruths,andcorrespondinggenerationresultsfromthegenerationmethod.A
13CHI’24,May11–16,2024,Honolulu,HI,USA RongHuang,Hai-ChuanLin,ChuanzhangChen,KangZhang,andWeiZeng
Table1. Thequantitativecomparisonbetweenourstoablationmethods.
Correctnessof Correctnessof Correctnessof Applicationof
Methods
aspectratio relativeareas relativepositions scalingrule
Zero-shotGPT3.5(w/domainknowledge) 51 56 58 66
Few-shotGPT3.5(w/domainknowledge) 100 100 91 92
Zero-shotGPT4.0(w/odomainknowledge) 0 48 79 70
Zero-shotGPT4.0(w/domainknowledge) 82 94 72 70
Few-shotGPT4.0(w/domainknowledge) 100 100 92 93
generatedscenelayouthavecorrectaspectratioifallplantsinthegeneratedscenelayouthavecorrectaspect
ratiosasthegroundtruths.Thecorrectnessofrelativeareasiscomputedsimilarly.
• Spatialreasoningreferstothecomprehensionoftherelativepositionsofplants,whichisevaluatedbythemetric
correctnessofrelativepositions.Specifically,weutilizesixcommonrelativepositionsof‘left’,‘right’,‘top’,‘bottom’,
‘behind’,and‘infrontof’.Forthemetric,weusethesame100scenelayoutsamplesandmeasuretherelative
positionsofallpairsofplantsbasedontheirboundingboxes,andcheckifthecorrespondingresultsbythe
generationmethodhavethesamerelativeposition.Specifically,ifboundingboxesoftwoplantsoverlap,we
manuallychecktheirrelativepositionandcategorizetherelationaseither‘behind’or‘infrontof’.
• Perspectiverelationalreasoningfurtherconsidersthecorrectnessofscalingsizeduetoperspectiveeffects,for
whichobjectsthatarefurtheraway(fromhumanperspective)shouldhavesmallerrelativesizes.Here,we
firstcomputetheground-truthrelativesizeratio𝑆𝑅
𝑔𝑡
=𝑠𝑖𝑧𝑒(𝐴 𝑔𝑡)/𝑠𝑖𝑧𝑒(𝐵 𝑔𝑡)betweentwoplants𝐴
𝑔𝑡
and𝐵
𝑔𝑡
basedonapre-defineddictionaryofexpectedsizesaccordingtodomainknowledge.Then,wecomputethe
actualrelativesizeratio𝑆𝑅 𝑔𝑒𝑛 =𝑠𝑖𝑧𝑒(𝐴 𝑔𝑒𝑛)/𝑠𝑖𝑧𝑒(𝐵 𝑔𝑒𝑛)fromthetwogeneratedplants.Finally,wecheckthe
consistencyofz-orderrelationbetweenplants𝐴and𝐵withtheorderrelationbetween𝑆𝑅 𝑔𝑡 and𝑆𝑅 𝑔𝑒𝑛,namely
𝐴infrontof𝐵⇔𝑆𝑅 𝑔𝑡 <𝑆𝑅 𝑔𝑒+𝑛,or𝐴behind𝐵⇔𝑆𝑅 𝑔𝑡 >𝑆𝑅 𝑔𝑒𝑛.Forthemetric,weusethesame100scene
layoutsamplesandcorrespondinggeneratedresults.Ageneratedscenelayouthavecorrectperspectiverelation
ifallpairsofplantsinthescenelayouthavecorrectperspectiverelation.
Thesuccessofscenelayoutgenerationdependsonwhethertheseaspectsarecorrectinthegeneratedlayout.Inthe
experiment,thehighernumberofsuccessfulresultsineachaspect,thebetterperformanceofthemodel.
Conditions.Weexaminetheperformanceofourmethodincomparisoninfiveconditions:1)zero-shotGPT3.5with
landscapeknowledge,2)few-shotGPT3.5withlandscapeknowledge,3)zero-shotGPT4.0withlandscapeknowledge,
4)zero-shotGPT4.0withoutlandscapeknowledge,and5)few-shotGPT4.0withlandscapeknowledge.Toensurea
faircomparison,eachconditionusesbasicallythesamepromptarchitectureinFig.5whileonlyadding/removingthe
demonstrationorbackgroundknowledgeforthetesting.Foreachcondition,wegenerate100samplesformeasurement,
basedonsamplepromptsgeneratedthroughatemplatethattakesrandomplantsandrelationsastextdescription.
Results.Table1presentsthecountofsuccessfulresultsinfouraspectsbetweendifferentmodelsandmethods.First,it
isobservedthatbothGPT-4.0andGPT-3.5achievehighgenerationperformancewithonlyafewerrorsinthegenerated
results.ThegapbetweenGPT-4.0andGPT-3.5isnotprominent,possiblybecauseourpromptsenabletheGPTmodels
toreachacertainhighlevelatwhichthedifferencesbecomenegligible.Forthezero-shotmethod,thereisasignificant
gapbetweenGPT-4.0andGPT-3.5,suggestingthatGPT-4.0hassuperiorzero-shotcapabilities.Focusingoneachaspect,
wenotethatthezero-shotGPT-4.0,withoutlandscapeknowledgeforreference,generateszerocorrectaspectratiosand
hasalowcorrectnessrateforrelativeareaswhenrelyingsolelyoncommonsense.Thishighlightstheeffectivenessof
injectingdomainknowledgeintoourmethod.Ontheotherhand,forpositionreasoningandperspectivereasoning,
whichinvolvecommonsensereasoning,thezero-shotGPT-4.0outperformsthezero-shotGPT-3.5.Itisevidentthatthe
14PlantoGraphy CHI’24,May11–16,2024,Honolulu,HI,USA
Table2. MeanSSIMofimagesgeneratedbytwomodels.
C1 C2 C3 C4 C5 All
Baseline 0.0956 0.1017 0.1123 0.1138 0.1387 0.1091
Ours 0.1201 0.1466 0.1858 0.1520 0.1545 0.1499
few-shotmodelachievesasignificantimprovementoveritszero-shotcounterpart.Insummary,theresultsdemonstrate
theeffectivenessofreasoningmethods,suchasfew-shotlearning,whichareincorporatedintoourapproachand
highlighttheessentialroleoflandscapeknowledgeforlanguagemodelstoperformdomain-orientedreasoning.
5.2 Experiment2:Illustrationmoduleperformance
5.2.1 G1:Ensuringgraphicalcoherenceoftheresultsgeneratedduringiterativedesign.
Quantitativeevaluation.Wefirstconductaquantitativeexperimenttoassesstheefficacyofincorporatinglayoutasa
conditionwithinrenderinggenerationtosustaingraphicalcoherencebycomparingthesimilarityofimagesgenerated
inmultipleiterations.
Baselinemodel.Forcomparison,wecreateabaselinemodelbyremovingthelayout-conditionalmodulebut
keepingothermodelconfigurationsincludingthesameLoRAmodel.Thisensuresconsistencyintheknowledgelevel
betweenthemodelsandminimizestheriskofmisinterpretationsthatcouldinfluencetheexperiment.
Datapreparation.Wedevelopthetestdatasetfromtwosources:randomlygeneratedandexpertgenerated,with
considerationsoftwovariables:planttypeandplantnumber.First,werandomlygenerate4800landscaperenderings
including5compositionsoflandscapescenesthatcover20differentplantcombinations:TreeA(C1),TreeA+TreeA
(C2),TreeA+TreeB(C3),TreeA+ShrubA(C4),ShrubA(C5).Second,werecruit7designerswithoverthreeyearsof
experienceinlandscapedesign.Theyareinstructedtoexperiencetheinteractivefunctionalityofoursystemanduseit
togeneratelandscaperenderings,fullysimulatingactualadjustmentsaccordingtotheirprofessionalexperience.The
generationprocessmentionedaboveemploysthesamepromptwiththesameseedsgroup.Eachcombinationutilizes
thesamepromptandobjectlayoutwhilegeneratedimagesdifferaccordingtorandomseeds.
Metric.WeemploySSIM[19]toevaluatetheimages’graphicalcoherenceduringiterativegeneration.SSIMisa
widelyusedmetricformeasuringthesimilaritybetweentwoimages.Ittakesintoaccountthestructuralinformationof
theimagesinsteadofonlythepixelvalues,whichismoreinlinewithhumanperceptionandsuitablefortheassessment
oflayout-guidedgenerationmodels.Thesimilarityofthegeneratedimageswithinthegroupscanbequantifiedby
comparingthemeanvalueoftheSSIM.AgreaterSSIMscoresignifiesaheightenedcoherenceamongtheimages
generatedunderidenticalprompts,therebyreflectingtherobustnessofthemodeliniterativedesign.
Results.AsshowninTable2,ourmodelconsistentlyoutperformsthebaselineinallcategories.Theresultsignifies
theeffectivenessofintroducinglayoutcontrolinlandscaperenderinggeneration,particularlyinenhancingimage
structuralcoherencethroughouttheiteration.Notable,theimprovementsaremoresignificantinC2&C3.Thismaybe
becauseplanttypesinC2andC3arebothtwotrees,typicallyoccupyalargerareathanothergroupsintheimage,
makingtheconsistencyprovidedbythelayoutguidancemoreprominentacrosstestsets.
Subjectiveevaluation.Inlandscapedesign,subjectiveperceptionofvisualsimilaritycanbeaffectedbyfactorssuch
asspatialambiance,colorandspaceharmony,whichisnotabletobeevaluatedusingquantitativemetrics.Asa
complementfromsubjectiveperspective,wefurtherconductasubjectiveuserstudytoevaluatethegraphicalcoherence
consideringhumanperception.
15CHI’24,May11–16,2024,Honolulu,HI,USA RongHuang,Hai-ChuanLin,ChuanzhangChen,KangZhang,andWeiZeng
7 ** ** ** ** ** **
6
5
4
3 * P<0.05
** P<0.01
2 *** P<0.001
1 Ours
Baseline
0
C1 C2 C3 C4 C5 All
Tree A Tree A & Tree A Tree A & Tree B Tree A & Shrub A Shrub A
Fig.9. Resultsoftheuserstudycomparingthegraphicalcoherenceofthegeneratedimageswithinthegroups.
Participants.45participantsarerecruitedbypostingrecruitmentmessagesononlinesocialmedia.Amongthem,
20participantshavedesignexperienceandtheremaining25donothave.Theaverageageoftheparticipantsis24.9
(2.22%in[0,18],53.33%in(18,25],35.56%in(26,30],8.89%in(31,40]).
Procedure.Werandomlyselect55groupsofgeneratedimagesusingeachmodelwiththesameseedfromthetest
setmentionedabove.Participantsaretaskedtocomparethegraphicalcoherenceofimagegroups,andgiveratingsform
an8-scalequestionnaire.Intheinstructionsgivenpriortotheexperiment,theyareinformedthat1)thequestionnaire
optionsrepresentedperceptionsrangingfrom"verydissimilar(0scale)"to"verysimilar(7scale)";2)thefocusison
visualsimilarity,regardlessofsemantics;and3)eachquestionneedstobeansweredwithin15seconds.
Results.Nooutlierisidentifiedfromtheratings.WeruntheFriedmantestonratingsforourapproachandthe
baselineforeachgroup.AsdemonstratedinFig.9(ALL),ourmethodoutperformsthebaselinemodelintermsofimage
coherencefromasubjectiveperspectiveacrossalltestgroups.Thisisparticularlyevidentingeneratingmultipleobjects,
suchasingroupsC2,C3andC4,whereacomprehensiveunderstandingoftherelationshipsbetweentheseobjects
becomescrucialforanalysis.Forsingleobjectgeneration,ourmethodpresentsanarrowerperformancegapcompared
tothebaselinemodel,possiblybecausethebaselinemodelpossessestheabilitytogenerateasingleobjectwithhigh
robustness.However,eveninsingleobjectgenerationtasks,ourmethodexhibitsasignificantlyhighercoherence
comparedtothebaselinemodel.Asthecomplexityofthegeneratedimagesincreases,theperformancegapbetween
ourmethodandthebaselinemodelwidens.Thisindicatesthatourmethodisbettersuitedfordesigntasksthatrequire
ahigherlevelofcoherence,especiallywhendealingwithcomplexobjectrelationshipsandcompositions.
5.2.2 G2:Ensuringthemodel’sabilitytocomprehendlandscapedesigndescriptions.
Weconductauserstudytoevaluatethemodels’comprehensionofdesigners’intentions,whichisfacilitatedbythe
layout-basedconditionalinputandLoRAmoduleinintegratingplant-relatedknowledgeintothegenerationprocess.
Baselinemodel.Toeliminatetheimpactofthelayoutconditiononthegeneratedresults,weselectourlayout-guided
modelwithouttheLoRAmoduleasthebaselinemodel.
Datapreparation.Fortestsetpreparation,werandomlygenerate50imagesfromeachmodelcovering20plant
combinationsthesameasabove.Thegeneratedimagesarepairedwithcorrespondinggenerationseedstocreatea
5-pointLikertquestionnaire.10landscapedesignerswithmorethanthreeyearsofdesignexperiencearerecruitedto
evaluatethealignmentbetweendesignintentandimages.
Results.AsdepictedinFig.10,ourmethodexhibitsahighertext-imagealignmentforplanttypefactorsacross
allcombinations,demonstratingthattheLoRAmodulesuccessfullyintegratesplantknowledgeintothemodeland
enhancesitsabilitytoaccuratelygenerateplanttypes.Intermsofplanttype,groupsC2,C3,C4byourmodelall
exhibitbetterperformancethanthebaseline,indicatingthatourmodelisbetteratgeneratingtheappropriateplantsin
multi-objectscenes.AsshowninFig.11(02),thebaselinemodelfailstogeneratethecorrectplants.Specifically,C5has
16PlantoGraphy CHI’24,May11–16,2024,Honolulu,HI,USA
Plant Type Plant Number Plant Location
100%
C1: Tree A
80% C2: Tree A + Tree A
C3: Tree A + Tree B
60% C4: Tree A+ Shrub A
C5: Shrub A
40%
Ours < Base
20% Ours = Base
Ours > Base
0%
All C1 C2 C3 C4 C5 All C1 C2 C3 C4 C5 All C1 C2 C3 C4 C5
Fig.10. Resultoftheuserstudycomparingthemodel’scomprehensionoflandscapedesigndescriptions.
01 Scene Description 02 Scene Description 03 Scene Description
A realistic photo of a field of africanlily in A realistic photo of a weeping willow and a A realistic photo of a dogwood and a japanese
front of a river in the park with blue sky. japanese pine in a park. The weeping willow is pine in a park. The dogwood is on the right of
Sunset, grass, cloud. on the right of japanese pine. ... japanese pine. ...
Ours Comparison Ours Comparison Ours Comparison
04 Scene Description 05 Scene Description 06 Scene Description
A realistic photo of a weeping willow in front A realistic photo of two weeping willow in the A realistic photo of two crape myrtle in the
of a river in the park. The weeping willow is park, with blue sky. Sunset, grass, cloud. park with blue sky. Sunset, grass, cloud.
on the right of the scene. ...
A
Ours Comparison Ours Comparison Ours Comparison
Fig.11. Caseswithasignificantdifferencebetweentheevaluationofourmodelandthebaselinemodel.
thesmallestgapbetweenourmodelandthebaselinemodel.Thiscouldbeduetothefactthattheshrubsinthebaseline
modeloverlapwiththeshrubspeciesselectedinourLoRA,resultinginbothmodelsperformingwellinthisscenario.
Unexpectedly,ourmethodshowsabettercomprehensionofplantnumbersandplantlocations,demonstratingthat
thelayout-basedconditionalinputcanefficientlyenhancethemodel’scomprehensionofplantnumbersandlocations.
Incontrast,incomplexscenes(C3,C4),ourmodelisabletobettercontrolthecorrectnumberandlocationofplants.
Itisnoteworthythatafteraddingplantspecies(C2/C3),thebaselinemodel’sabilitytounderstandthenumberof
plantsdecreasedsignificantly,possiblyduetothefactthatpre-trainedsemanticinformationdoesnotsupportsuch
complexrepresentations.Fig.11(04,05)revealsthatthebaselinemodelcancomprehend"aweepwillow"butfailsto
generatethecorrectscenefor"twoweepingwillow".Intermsofplantlocation,ourmodelstilldemonstratessuperiority
especiallyincomplexscenes(C3,C4).AsshowninFig.11(03),thegeneratedresultomitsthe"Japanesepine"and
locatesthe"dogwood"onthewrongside.Theseunderlinetheneedtoincludelayoutcontrolinmodelstoincrease
graphicalcoherenceandproducevisuallycaptivatingimages.
17
sesnopser
fo
noitcarFCHI’24,May11–16,2024,Honolulu,HI,USA RongHuang,Hai-ChuanLin,ChuanzhangChen,KangZhang,andWeiZeng
6 USERSTUDYOFPLANTOGRAPHY
TofurtherevaluatetheeffectivenessofPlantoGraphy,weconductedawithin-subjectsstudywith6expertdesignersto
comparetheAI-supportedlandscapedesignprocessfacilitatedbyPlantoGraphywiththeconventionaldesignprocess
usingwidelyutilizedindustrysoftware.
6.1 Participants
Werecruited6experts(female:3andmale:3,Meanage=25.84)fromsocialnetworkstoparticipateinourstudy.To
ensureavalidcomparison,weselectedparticipantswhohadexperienceusingAItoolsmeanwhilehadmorethanthree
yearsofexperiencewithconventionaldesignsoftware.ThedemographicsofalltheparticipantsareshowninTable3.
Theparticipantswereinvitedtoconducttheexperimentsoffline.Giventhecomplexityoftheexperimenttasks,we
coveredthetravelexpensesforallparticipantsandcompensatedthematarateofabout$13.5/hour.
Table3. Demographicsoftheparticipantsinsystemevaluation.
UID Gender Landscapedesignexperience Frequencyonindustrysoftware FrequencyonAItools
E1 Female 5years VeryFrequently Occasionally
E2 Female 7years Frequently Rarely
E3 Male 7years Occasionally Frequently
E4 Male 6years VeryFrequently Occasionally
E5 Male 5years VeryFrequently Occasionally
E6 Female 3years Frequently Occasionally
6.2 ExperimentSetup
Fortheuserstudy,wedevisedalandscapedesigntasksimulatingareal-worlddesigncreativeprocess.Thetaskwas
designedtoassessparticipants’abilitytoutilizeconventionaldesigningtoolsvs.PlantoGraphyincreatinglandscape
designs.Thetaskinvolvedthreescenariodescriptionsasinitialdesignrequirements,asfollows:
• Scenario1:WoodedPath:Atrailwithtalltreesonbothsides.
• Scenario2:LakeandCherryTrees:Thepathborderedbyalakeononesideandarowofcherrytreesonthe
other,withsomefloweringplantsaroundthelake.
• Scenario3:HousesandFieldsofFlowers:Smallhousesinthedistance,withlargefieldsofflowersnearby.
Theparticipantsweregiventhefollowingtask.
"Youreceivevaguescenariodescriptionsfromaclient.
Prepareaconcretelandscaperenderingforthenextdebriefingsession,tohelpconfirmtheprojectrequirements."
6.3 ExperimentProcedures
Priortothestudy,theparticipantssignedaconsentform,agreeingtojointheexperiment,andallowingustocollect
basicdemographicinformationandrecordtheirbehavioraldata,includingthefrequencyofoperationsoneachfunction
andthetimetakentocompleteeachtask.Then,participantswereinstructedtogothroughthefollowingsteps:
(1) Participantswerebrieflyintroducedtothebackgroundoftheprojectandreceivedanintroductoryaboutthe
usageofPlantoGraphyinterfaceforabout5minutes.ThentheywereallowedtofreelyexplorePlantoGraphyfor
about10minutes.
(2) Participantswereaskedtocompletethedesigntaskusingconventionallandscapedesigningtoolsvs.usingPlan-
toGraphy.Forconventionaldesignprocess,participantswereprovidedwiththeflexibilitytochoosecommonly
18PlantoGraphy CHI’24,May11–16,2024,Honolulu,HI,USA
Usability Q1: It is easy to understand. 1 1 4
Q2: Switching between different functions is smooth. 2 3 1
Q3: It is easy to use it. 3 3
Enjoyability Q4: Interacting with it is a pleasant experience. 4 2
Q5: Using this it makes design tasks more interesting. 2 4
Q6: It is visually appealing and engaging. 1 3 2
Effectiveness Q7: It effectively assists me in designing tasks. 1 2 3
Q8: I am satisfied with the performance and output of it. 1 3 2
Q9: It meets my expectations for function. 2 2 2
System
Q1 6 Q1 1 2 3 Q1 6
Q2 1 4 1 Q2 1 4 Q2 1 1 3 1
Q3 2 1 3 Q3 3 3 Q3 3 3
Q4 3 3 Q4 1 1 4 Q4 1 3 2
Q5 1 4 1 1 Q5 3 3 Q5 3 3
Strongly Agree Q6 2 1 2 1 1 1 Q6 1 2 2 1 Q6 1 1 4
Agree
Neutral Q7 1 2 2 1 Q7 3 3 Q7 2 4
Disagree Q8 4 2 Q8 3 3 Q8 1 2 3
Strongly Disagree Q9 1 2 3 1 Q9 2 4 Q9 1 1 4
Text Panel Graph Panel Layout Panel
Fig.12. ParticipantratingsonthePlantoGraphysystemandtheinteractivepanelsintermsofusability(Q1-Q3),enjoyability(Q4-Q6),
andeffectiveness(Q7-Q9).
60
50
40 Time taken (min)
30 Conven�onal: Modeling
Conven�onal: Rendering
20 AI-supported: Text panel
AI-supported: Graph panel
10
AI-supported: Layout panel
0 AI-supported: Rendering panel
E1 E2 E3 E4 E5 E6
Fig.13. TimetakenondesigntasksusingconventionalandAI-supportedlandscapedesignprocessbyeachparticipant.
usedsoftwarethatfacilitatestheirdesigngoalsincludingmodelingsoftware(e.g.,Rhino,Sketchup,C4D)and
renderingtools(e.g.,V-Ray,Lumion,UnrealEngine,AdobeSuite).
(3) Participantswereaskedtofillapost-studyquestionnariewith5-pointLikertscalequestionsfocusingonusability,
enjoyabilityandeffectivenessratingsofeachinteractivepanelandthewholePlantoGraphysystem.Additionally,
participantswereaskedtoanswerfouropen-endedquestions:
• DoestheinteractivesystemimprovetheefficiencyofcollaboratingwithgenerativeAI?Ifso,how?
• WhatisthedifferencebetweenGAI-supporteddesignprocessandtraditionaldesignprocess?
• WhatpossibleimprovementscanbemadetothePlantoGraphysystem?
Toresolvebiasbytaskfamiliarity,werandomizedtheorderinwhicheachparticipantusingconventionaltoolsor
PlantoGraphy.Wedidnotimposeanytimeconstraintstoencourageparticipantstofocusontheiterativedesignand
prioritizethequalityoftheirresults.Theaveragetimeforcompletingthestudywas103.5minutesforeachparticipant.
6.4 Results
Belowwefirstreportthequantitativeratingsandqualitativefeedbackfromtheparticipantsonsystemdesign,followed
byalessonslearnedonAI-assisteddesignprocess.
19CHI’24,May11–16,2024,Honolulu,HI,USA RongHuang,Hai-ChuanLin,ChuanzhangChen,KangZhang,andWeiZeng
6.4.1 SystemDesign. UserratingsregardingdesignoftheoverallsystemandinteractivepanelsaredepictedinFig.12.
• Overall System. Participants generally expressed satisfaction with the overall system’s usability (Q1-Q3),
enjoyability(Q4-Q6),andeffectiveness(Q7-Q9).Specificly,themajorityofusersratedthesystemwiththehighest
scoreineasytounderstand(Q1)andtouse(Q3).E4&E6emphasizedtheimportanceofanintuitiveinterfacefor
usersengagingindesignprocess.MostusersenjoyusingthePlantoGraphysystemduringcreativeprocess,with
thehighestscoreonmakingthedesigntasksmoreinteresting(Q5).Theparticipantsunanimouslylaudedthe
system’sinteractivefeatures,notingthat"graphicaleditingpanelsalignmoreeffectivelywithtypicaldesignlogic
comparedtoconventionaldesignprocessthatrequiresseveraldifferentsoftwares"(E3).Participant’response
onthesystemeffectivenessarerelativelymoderate.Notably,wecanseeaconcernonassistanceindesigning
taskswhilesomeusersgavealowscore(Q7).Participant(E6)suggestedthatfine-tuningoperationscouldbe
moreintuitiveifthelayoutweretobeoverlaidontherenderingpanel.Additionally,E1observedthatgenerating
certainplantpairings,likeweepwillowandbanyan,canresultinunusualedgeformationsintheplants.More
discussionsareprovidedinthelessonslearnedbelow.
• TextPanel.ParticipantsshowedalowlevelofinterestontheTextPanelsincethetaskswerefinalizedanddid
notrequireanymodifications.Interestingly,E3raisedahighpointinenjoyablefortextpanel:"Iliketheweather
andtimesliderthatallowmetochangetheenvironmenteffectofgeneratedimageintuitively."Forimprovement,
participantsuggestedaddingsketchesasamorenaturalalternativeforinput,favoringtheirintuitivenatureover
traditionaltext-basedmethods(E6).
• GraphPanel.TheratingsontheGraphPanelaremostlypositive.E2foundthegraphrepresentationsand
interactionsespeciallybeneficial,highlightingtheirpotentialtodeepenunderstandingofcomplexstructures.
"Plantcombinationsareinfluencedbyvariousfactors,includinglocalclimaticconditions,applicableplants,andthe
relationshipsbetweenplantpopulations....Graphcanabstracttheserelationshipsanditisintuitive(E2)".E3&E6
suggestedenhancementsinthegraphpanelforsmoothernodeadditionandautomatedoptimalnodeplacement
recommendations.
• LayoutPanel.ParticipantsperceivedtheLayoutPaneltobethemostenjoyableandefficientamongthethree
panels.Thelayoutpanelwaspredominantlyutilized,overshadowingthefrequencyofthegraphandtextpanels,
aligningwiththesubjectivescoringprovidedbyusers.E3explainedthat"designersprefertousegraphic-oriented
approachesconveyingdesignconceptswithstakeholders."E5alsohighlightedthat"usingagraphicaleditingfunction
forminormodificationismoreeffectiveforiterativedesign".However,E4gavethelowestscoreonswitching
betweendifferentfunctions(Q2),astheeffectoflayoutmodificationwasonlyvisibleontheoutputbutnot
reflectedontheothertwopanels.
TimeAnalysis.Fig.13illustratesthetimespentbyeachparticipantonthetasksusingconventionalsoftwareandthe
PlantoGraphysystem.Throughoutthestudy,thetraditionalprocessdemandedmoretimethanAI-assistedones.The
reductionintimeisquitesignificant(mean:20.17minutes,max:27minutes,min:11minutes),showcasingtheefficiency
ofPlantoGraphyinstreamliningdesignprocesses.Notably,thetimereductionsaremoresignificantbyE1∼E3thanthose
byE4∼E6.Thefollow-upinterviewsrevealedthereasons,asE1∼E3tooklongertimesforconceptualizationthatsolely
reliesondesigners’knowledgeandexperienceusingconventionaltools,whilstGAIcanquicklygivesomeprototype
designsandhelpdesignersbrainstorm.Interestingly,E3exclusivelyutilizedPhotoshoptomergeonline-retrievedplant
imagesandcreaterenderings.However,thisapproachprovedtime-consuming,assubstantialeffortwasrequiredto
manuallyadjustplantsizesandpositionstoaccuratelyportrayhuman-perspectiveviews.
20PlantoGraphy CHI’24,May11–16,2024,Honolulu,HI,USA
Wealsoobserveddistinctpatternsinthetemporaldistributionofuserengagementwiththeinteractivepanelsin
PlantoGraphyduringcreativetasks(Fig.13).Here,timeontheRenderingPanelpertainstothewaitingdurationsfor
themodeltogeneraterenderings.Undoubtedly,participantsdevotedtheleastamountoftimetotheTextPanelandthe
mosttimetotheLayoutPanel,underscoringtheirinclinationforiterativedesigns.
6.4.2 LessonsLearned. Participantshighlypraisedthesystem’sabilitytosupporttheminiterativedesign.PlantoGraphy
providesopportunitiesforapplyingdesigners’expertiseandcollaboratingwithGAI,facilitatingthedevelopmentof
distinctiveandcaptivatingdesigns.However,theimprovementisconstrainedbyPlantoGraphy’slimitedflexibilityin
accommodatingmorediverseandcomplexdesignrequirements.Participantsattemptedtoincorporateadditionalplants
thatharmonizedwellwiththoseinthegivenscenariosbutencounteredchallenges,particularlywhenthenumber
ofplantsexceedssix.Thisislikelyattributabletotherestrictedquantityofplantsinthefine-tuningdataset,andthe
LLM’sreasoningcapabilitywhendealingwithintricategraphs.
UponconductingadetailedanalysisofthetimereductiondisparitybetweenE1∼E3andE4∼E6,wediscovered
thatE1∼E3primarilyfocusedontheinitialphaseofthedesignprocess,whereasE4∼E6shiftedtheirattentiontothe
moreadvancedandin-depthdesignphase.E4∼E6notedthat,"forthemoreadvancedstage,thecontentandqualityof
theimagesgeneratedbettermatchedmygraphicsketches,"necessitatingadditionaliterativeadjustmentswhenusing
PlantoGraphy.Theelongationoftimeinthetraditionalprocessislesspronounced,astheintermediate‘3Dmodel’
outcomeempowersdesignerswithenhancedcontrolforviewingandadjustingthedesignfromvariousangles.
ThisobservationrevealsacompellingaspectofAIintegrationindesignprocesses:asAIbecomesmoredeeply
embeddedinthedesignworkflow,itsefficiencygainsfordesignersseemtodiminish.Thisphenomenonmaystem
fromtheescalatingcomplexityofdecisionsandcreativeinputsrequiredatadvancedstages,arealmwhereAI-assisted
designtoolsmaynotyetexhibitthesameefficacyasininitialstagetasks.WeanticipatethattheprogressofAIwill
enhancetheeffectivenessofAI-assisteddesigntoolsformoreadvancedtasks.Importantly,thisalsoimpliesapotential
reevaluationoftheroleofAI:fromatoolthatexpeditestaskstoonethatevolvestoenhance,ratherthanovershadow,
humancreativityandexpertise.
7 DISCUSSION
7.1 Findings
7.1.1 Domainknowledgeembedding. Inourworkflow,theend-to-endprocessoftext-imagegenerationisbrokendown
intomultiplesteps,allowingdomainknowledgetobeintroducedintothemodeldevelopmentprocessinvariousforms:
buildingprompttemplates,translatingexperienceintorules,etc.Forexample,theconcretizationincorporatesdesigners’
experienceandknowledgeintothedomain-orientedLLMthroughprompttemplates.Anadvantageisthatweonly
needtoconstructasmallnumberofdatasetswithexpertsandembeddedtheexpertiseintothemodelbymeansof
few-shotlearning,whichisprovedtobeeffectiveinExperiment1(Sect.5.1).Inthisway,weutilizethepowerful
languageprocessingandreasoningcapabilitiesofLLMstofacilitateAI-assisteddesign.Itrequireslessspecificationsof
domainknowledgemeanwhileretainshighcompatibility,whichisfriendlytonon-AIpractitioners.
7.1.2 InteractiondesignforgenerativeAI. Oursystemintroducesscenegraphandlayoutastwointermediategraphical
representationsinthetext-imagegenerationprocess.Theserepresentationstransformtext-basedscenedescriptionsinto
intuitive,easy-to-modifygraphsfromabstractandfigurativeperspectives.Inpost-studyinterviews,usershighlightedthe
benefitofmultimodalinputs(e.g.,textandgraphicalcontrol)enabledbytheGUIandwidgetinterface,allowingformore
21CHI’24,May11–16,2024,Honolulu,HI,USA RongHuang,Hai-ChuanLin,ChuanzhangChen,KangZhang,andWeiZeng
Fig.14. NarrativemapgeneratedbyaparticipantwiththehelpofPlantoGraphy,illustratinghisimaginationofthehumanperspective
keyplotsofthedesignsite.
effortlessandintuitiveexpressionofdesignrequirements.Thisfacilitatestheaccuratetranslationandcommunication
ofthedesigner’sneedstothegenerativeAImodels.Positiveuserfeedbackoninteractionmodessuggeststhatdesigning
functionstailoredtothetargetpopulation’sthoughtprocessesandoperationallogicreducesthelearningcurveand
facilitatessmootherinformationtransfer.Forexample,designersinourstudyexpressedfamiliaritywithabstractgraphs,
aidingtheminvisualizingthesystem’spotentialfordiversedesignneeds.Thesefindingsalignwiththeguidelinesfor
craftinghuman-centeredgenerativeAIsystemsasoutlinedin[40],mirroringfindingsfromrecentHCI-centricGAI
studiesforvariousdomains(e.g.,[43,50]).
7.1.3 Controlfromdesignerstomodels. Controlfromhumanstomodelsisregardedasakeydimensionforhuman-
centeredgenerativeAIsystems[40].Inourapproach,theuseoflayoutasaconditionalinputenhancesthegraphical
coherenceofthegenerationmodelacrossiterations.Usersfoundthisimprovementtobehighlypracticalforreal-world
designscenarios,indicatingthattheAItooleffectivelyintegratesintotheiterativedesignprocessandco-createswith
designers.Furthermore,controllableAIgenerationtoolscanempowerdesignerstofullyleveragetheirexpertiseand
experiencetostimulatecreativity.Throughoutthedesignprocess,designerscanfreelyexploreandexperimentwith
variouscreativesolutions,withAIservingasanassistivetooltoofferadditionalpossibilitiesandinspiration.Inthe
future,AIgenerationtoolsincorporatingmultiplemediumsofcontrolwillbecomeincreasinglycrucial.Suchtoolswill
enabledesignerstoachievehigh-qualitydesignoutcomesmorerapidlyandenhancetheirworkefficiency,allwhile
capitalizingontheboundlesscreativepotentialofferedbyAItechnology.However,it’sessentialtoviewAItoolsas
complementary,notsubstitutes,forprofessionallandscapedesigners,ashumanexpertiseremainsindispensablefor
ethicalconsiderations,nuancedunderstanding,andclientprioritizationinlandscapedesign.
7.1.4 AI-aidedcreativity. ManystudieshavedemonstratedthecapabilityofGAIsystemsinfosteringcreativity,by
controllingparameterslikerandomseedorpenaltytocreaterandomnessintheoutputs[30,35].PlantoGraphyalsoallow
userstocontroltheinputstocreatediverselandscaperenderings.Moreover,oursystemfurtherintroducescreativity
byintroducingmultiplestepswhenutilizingGAIforlandscaperenderings.Forinstance,creativityenhancement
22PlantoGraphy CHI’24,May11–16,2024,Honolulu,HI,USA
isevidentduringbrainstorming,wherePlantoGraphy allowsdesignerstoinputbasicenvironmentalrequirements
andincrementallyintroduceplantcombinationswithoutaspecificscenedesign.Inaddition,theabilitytoefficiently
generatinglandscaperenderingsenablesuserstostringtheserenderingstogetherinanabstractstreamlineandforma
narrativemap,asshowninFig.14.InthisnovelAI-assisteddesignapproach,therendering,previouslyonlypresentin
thefinalresultpresentation,nowservesasasubstituteforthescenesketchandisincorporatedintotheinitialconcept
development.Wewerepleasedtodiscoverthatsystemsfeaturingintuitiveandwell-tailoredinteractivecapabilities
exhibitedgreateruser-friendlinessandweremorereadilyembracedbydesigners,whocouldfreelyexplorenovel
applicationscenarios.Thisrealizationleadustoconsiderthatredefiningtheinputsandoutputsofthetraditionaldesign
processcouldserveasastartingpointforenvisioninganewdesignparadigmforcreativedesign,particularlyinlight
oftheefficientgenerativecapabilitiesofferedbyAI.
7.1.5 Methodgeneralizability. Whilethisstudycentersonlandscapedesign,theproposedpipelineandfine-tuning
strategiesarereadilyadaptabletodiversecreativedesigns.Breakingdownanend-to-endprocessintoamulti-step
pipelineandintegratingitwithLLMsisapplicabletootherGAI-assisteddesignswith“iteratingpromptsandout-
puts”[43].Specifically,amulti-steppipelinecanenhanceelementrelationarrangementandlayoutadjustmentin
thegenerationprocess,acharacteristicoftenseeninarchitectural,fashion,andUIdesign.Inthesefields,textual
descriptionsalonemayinadequatelyconveytheintricaciesofdesignintentions[26].Ourmethod,incorporatinggraph
andlayoutasastructuredforwardprocessandinteractiveapproach,enablesamorenuancedinterpretationofdesign
intentions,simplifyingthechallengeofconstructingalarge-scalefine-tuningdataset.Thisencapsulationofdomain
knowledgeintothepipelineprovesessentialfortaskswherepreparingafine-tuningdatasetischallenging.
7.2 Limitations
Generationperformance.Whileinmostcases,oursystemiscapableofgeneratingthecorrecttypeofplantinthe
intendedlocation.However,thereareinstanceswherecertaintypesofplantscaninfluenceeachother.Forexample,the
generatedplantmaybeofthesametypeeveniftheuserinputspecifiesadifferenttype.Thisissuemaystemfroman
inherentlimitationofGLIGEN.Inourexperiment,weobservedthatwhencertainobjectsaregeneratedtogether,one
objectisconvertedintoanotherwithintheGLIGENbasemodel.Toaddressthissituation,weattempttomitigatethe
effectbyusinglatentcompositionandfreezingthegeneratedsingleobjectlatent.Whilethisapproachhelpsmaintain
thecorrecttypeofgeneratedplant,itdoesresultinadecreaseinthecoherencebetweentheobjectandthebackground.
Infuturework,weaimtoinvestigatehowandwhendifferentconceptscausethisconversiontoavoidsuchconditions
andimproveoursystem’susability.
Biascausedbymodelfine-tuning.Pre-trainedmodels,whileequippedwithabroadknowledgebase,caninadver-
tentlycarrybiasesiftheirtrainingdatasetslackdiversityorexhibitcertainpreferences.Weobservedthatwhilesome
popularstyleslikeJapaneseZenGardensandEnglishCottageGardensarerenderedwithhighfidelity,othersare
limitedtoanarrowselectionofplanttypesorevenincorrectspecies.Fine-tuningwithLoRAisdesignedtocustomize
themodel’soutputmorepreciselyforlandscapedesigntasks.However,ifthefine-tuningdatasetisnotsufficiently
diverse,themodelmaybecomeoverlytailoredtotheparticularstyles,plants,andmaterialswithinthatdataset,at
theexpenseofotherviableandinnovativedesignoptions.Thissituationpresentsacriticaltrade-off:thespecificity
andaccuracyofgenerateddesignsversusthediversityandversatilityrequiredbydesigners.Tobalancethistrade-off,
theweightofLoRAmodel,fine-tuningdatasetcollectionandothermethodslikeregularizationshouldbecarefully
consideredinaccordancewiththedesigners’objectives.
23CHI’24,May11–16,2024,Honolulu,HI,USA RongHuang,Hai-ChuanLin,ChuanzhangChen,KangZhang,andWeiZeng
AnotherlimitationisreferredtoastheconceptpollutionofLoRA.WhenaLoRAmodelismergedwiththebase
model,certainconceptsinthebasemodelcanbealteredorcontaminatedbytheLoRAmodel.Forinstance,ifweloada
LoRAmodelfocusingon"banya"treesintothebasemodel,allthetreesinthebasemodelmaybecome"banya"trees.
Toalleviatethisissue,weemployregularizationtechniques.However,itisworthnotingthatwhenencounteringstrong
features,suchascolorfulflowers,itispossiblefortheflowersinthebasemodeltobecontaminatedandappearasthe
sametypeofcolorfulflower.Infuturework,weaimtoaugmenttheregularizationdatasettopreventsuchconcept
pollutionandfurtherenhancethesystem’sperformance.
Systemdesign.Thecurrentsystem’sselectionofplantspeciesislimitedduetotheconceptpollutionproblem.
Resolvingthisissuecouldenablethesystemtosupportawiderrangeofspecies.Furthermore,plantselectioninour
systemreliesonthedesigner’sexperienceforconfiguration,withtheecologicalharmonyofconfiguredplantsjudged
solelybythedesigner,whichraisesthesystem’sthreshold.Inthefuture,incorporatingagraphrecommendationmodel
withexpertknowledgeinputcouldenableautomaticplantspeciesrecommendationsandnewgraphgenerationonthe
graphpanel,betteraligningwithlandscapedesignpractice.
AI-assistedsystemwithmulti-interactions.Duringthepost-studyinterviewswithdesigners,welearnedthattheyare
accustomedtousinghand-drawnsketchesandexpressedtheirdesireforafunctionallowingrequirementtransformation
intheformofdrawingsdirectlyonthedrawingsurface.Thisinsighthighlightedtheimportanceofconsideringusers’
habitswhenselectingdifferentinteractionmethodswiththeconsiderationofdiverseuserinput(text,voice,drawing,
graphicalcontrol,etc.).Itcanmaximizethesmoothnessofusercommandtransformationwithinthesystem.
8 CONCLUSIONANDFUTUREWORK
Thispaperhasexploredthepotentialofintegratinglargepre-trainedgenerativemodelswithinvariousdesignphases
tofacilitateahuman-AIiterativedesignprocess.Addressingthelimitationsofexistingend-to-endrenderinggeneration
methods, PlantoGraphy was developed through a formative study and support scene graph and layout as visual
interactioncomponentstoensuremorecompatibilitywithcommondesignprocesses.Thesystemincorporatesa
two-stagepipeline,consistingofaconcretizationmodulefortranslatingconceptualideasintoconcretescenelayouts,
andanillustrationmodulefortransformingscenelayoutsintorealisticlandscaperenderings.Performanceevaluations
haveattestedtoPlantoGraphy’seffectivenessinlandscaperenderinggeneration.Awithin-subjectsstudycomparing
conventionaldesignprocesswiththeAI-assisteddesignprocesshasdemonstratedtheeffectivenessofPlantoGraphy
andalsouncoveredpotentialareasforimprovements.
FutureworkwillfocusonenhancingthegenerationcontrollabilityofAI-assistedlandscapedesigningtoolsby
addressingcurrentlimitations,suchasconceptpollutionandunexpectedobjecttypeconversion.Toaugmentuser
experienceandefficiency,oursystemwillalsobeequippedwithsupplementaryfeatures,suchasplantpairing.Building
uponthisresearchfoundation,weaimtodelvedeeperintonoveldesignparadigmsthatfostercollaborationbetween
artificialintelligenceandhumandesigners.Additionally,weintendtoinvestigateeffectiveinteractionpatternsand
pertinentinformationcircuitstailoredfortheenhancementoffutureAI-supporteddesignprocesses.
REFERENCES
[1] SaleemaAmershi,DanWeld,MihaelaVorvoreanu,AdamFourney,BesmiraNushi,PennyCollisson,JinaSuh,ShamsiIqbal,PaulNBennett,Kori
Inkpen,etal.2019.Guidelinesforhuman-AIinteraction.InProceedingsofthe2019chiconferenceonhumanfactorsincomputingsystems.1–13.
[2] OronAshualandLiorWolf.2019. Specifyingobjectattributesandrelationsininteractivescenegeneration.InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision.4561–4569.
[3] ElizabethBoultsandChipSullivan.2010.Illustratedhistoryoflandscapedesign.JohnWiley&Sons.
24PlantoGraphy CHI’24,May11–16,2024,Honolulu,HI,USA
[4] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,
AmandaAskell,etal.2020.Languagemodelsarefew-shotlearners.AdvancesinNeuralInformationProcessingSystems33(2020),1877–1901.
[5] TaraCapelandMargotBrereton.2023.WhatisHuman-CenteredaboutHuman-CenteredAI?AMapoftheResearchLandscape.InProceedingsof
the2023CHIConferenceonHumanFactorsinComputingSystems.1–23.
[6] XiaojunChang,PengzhenRen,PengfeiXu,ZhihuiLi,XiaojiangChen,andAlexHauptmann.2021.AComprehensiveSurveyofSceneGraphs:
GenerationandApplication.IEEETransactionsonPatternAnalysisandMachineIntelligence45,1(2021),1–26.
[7] YuCheng,ZheGan,YitongLi,JingjingLiu,andJianfengGao.2020.SequentialattentionGANforinteractiveimageediting.InProceedingsofthe
28thACMinternationalconferenceonmultimedia.4383–4391.
[8] JohnJoonYoungChung,WooseokKim,KangMinYoo,HwaranLee,EytanAdar,andMinsukChang.2022. TaleBrush:Sketchingstorieswith
generativepretrainedlanguagemodels.InProceedingsofthe2022CHIConferenceonHumanFactorsinComputingSystems.1–19.
[9] HaiDang,LukasMecke,andDanielBuschek.2022.GANSlider:HowUsersControlGenerativeModelsforImagesusingMultipleSliderswithand
withoutFeedforwardInformation.InProceedingsofthe2022CHIConferenceonHumanFactorsinComputingSystems.1–15.
[10] RichardLeeDavis,ThiemoWambsganss,WeiJiang,KevinGonyopKim,TanjaKäser,andPierreDillenbourg.2023.FashioningtheFuture:Unlocking
theCreativePotentialofDeepGenerativeModelsforDesignSpaceExploration.InExtendedAbstractsofthe2023CHIConferenceonHumanFactors
inComputingSystems.NewYork,NY,USA,Article136,9pages.
[11] EllenYi-LuenDo.2005.Designsketchesandsketchdesigntools.Knowledge-BasedSystems18,8(2005),383–405.
[12] QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,BaobaoChang,XuSun,JingjingXu,andZhifangSui.2022.Asurveyforin-context
learning.arXivpreprintarXiv:2301.00234(2022).
[13] DaveEpstein,AllanJabri,BenPoole,AlexeiAEfros,andAleksanderHolynski.2023.Diffusionself-guidanceforcontrollableimagegeneration.
arXivpreprintarXiv:2306.00986(2023).
[14] SeamusWFilor.1994.Thenatureoflandscapedesignanddesignprocess.LandscapeandUrbanPlanning30,3(1994),121–129.
[15] DavidFoster.2023.GenerativeDeepLearning,2ndEdition.O’ReillyMedia,Inc.
[16] GabrielaGoldschmidt.2014.Linkography:unfoldingthedesignprocess.MitPress.
[17] DanielMHerbert.1993.Architecturalstudydrawings.JohnWiley&Sons.
[18] JonathanHo,AjayJain,andPieterAbbeel.2020.Denoisingdiffusionprobabilisticmodels.AdvancesinNeuralInformationProcessingSystems33
(2020),6840–6851.
[19] AlainHoreandDjemelZiou.2010. Imagequalitymetrics:PSNRvs.SSIM.In201020thInternationalConferenceonPatternRecognition.IEEE,
2366–2369.
[20] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.2021.Lora:Low-rankadaptation
oflargelanguagemodels.arXivpreprintarXiv:2106.09685(2021).
[21] NannaInie,JeanetteFalk,andSteveTanimoto.2023.DesigningParticipatoryAI:CreativeProfessionals’WorriesandExpectationsaboutGenerative
AI.InExtendedAbstractsofthe2023CHIConferenceonHumanFactorsinComputingSystems.1–8.
[22] JustinJohnson,AgrimGupta,andLiFei-Fei.2018.Imagegenerationfromscenegraphs.InProceedingsoftheIEEEConferenceonComputerVision
andPatternRecognition.1219–1228.
[23] JustinJohnson,RanjayKrishna,MichaelStark,Li-JiaLi,DavidShamma,MichaelBernstein,andLiFei-Fei.2015.Imageretrievalusingscenegraphs.
InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition.3668–3678.
[24] TaeSooKim,DaEunChoi,YoonseoChoi,andJuhoKim.2022. Stylette:Stylingthewebwithnaturallanguage.InProceedingsofthe2022CHI
ConferenceonHumanFactorsinComputingSystems.1–17.
[25] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,TeteXiao,SpencerWhitehead,AlexanderCBerg,
Wan-YenLo,etal.2023.Segmentanything.arXivpreprintarXiv:2304.02643(2023).
[26] Hyung-KwonKo,GwanmoPark,HyeonJeon,JaeminJo,JuhoKim,andJinwookSeo.2023.Large-scaletext-to-imagegenerationmodelsforvisual
artists’creativeworks.InProceedingsofthe28thInternationalConferenceonIntelligentUserInterfaces.919–933.
[27] TakeshiKojima,ShixiangShaneGu,MachelReid,YutakaMatsuo,andYusukeIwasawa.2022.Largelanguagemodelsarezero-shotreasoners.URL
https://arxiv.org/abs/2205.11916(2022).
[28] YuhengLi,HaotianLiu,QingyangWu,FangzhouMu,JianweiYang,JianfengGao,ChunyuanLi,andYongJaeLee.2023.Gligen:Open-setgrounded
text-to-imagegeneration.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.22511–22521.
[29] LongLian,BoyiLi,AdamYala,andTrevorDarrell.2023.LLM-groundedDiffusion:EnhancingPromptUnderstandingofText-to-ImageDiffusion
ModelswithLargeLanguageModels.arXivpreprintarXiv:2305.13655(2023).
[30] RyanLouie,AndyCoenen,ChengZhiHuang,MichaelTerry,andCarrieJ.Cai.2020.Novice-AIMusicCo-CreationviaAI-SteeringToolsforDeep
GenerativeModels.InProceedingsofthe2020CHIConferenceonHumanFactorsinComputingSystems.1–13. https://doi.org/10.1145/3313831.3376739
[31] Merriam-Webster.2023.DefinitionofGenerativeAI. https://www.merriam-webster.com/dictionary/generative%20AIAccessed:11December2023.
[32] ChongMou,XintaoWang,LiangbinXie,JianZhang,ZhongangQi,YingShan,andXiaohuQie.2023.T2i-adapter:Learningadapterstodigout
morecontrollableabilityfortext-to-imagediffusionmodels.arXivpreprintarXiv:2302.08453(2023).
[33] AlexNichol,PrafullaDhariwal,AdityaRamesh,PranavShyam,PamelaMishkin,BobMcGrew,IlyaSutskever,andMarkChen.2021.Glide:Towards
photorealisticimagegenerationandeditingwithtext-guideddiffusionmodels.arXivpreprintarXiv:2112.10741(2021).
[34] OpenAI.2023.GPT-4TechnicalReport. arXiv:2303.08774[cs.CL]
25CHI’24,May11–16,2024,Honolulu,HI,USA RongHuang,Hai-ChuanLin,ChuanzhangChen,KangZhang,andWeiZeng
[35] HanQiao,VivianLiu,andLydiaChilton.2022.InitialImages:UsingImagePromptstoImproveSubjectRepresentationinMultimodalAIGenerated
Art.InProceedingsofthe14thConferenceonCreativityandCognition(Venice,Italy).15–28.
[36] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen.2022.Hierarchicaltext-conditionalimagegenerationwithcliplatents.
arXivpreprintarXiv:2204.06125(2022).
[37] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer.2022.High-resolutionimagesynthesiswithlatentdiffusion
models.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.10684–10695.
[38] MurrayShanahan.2022.Talkingaboutlargelanguagemodels.arXivpreprintarXiv:2212.03551(2022).
[39] FredaShi,MiracSuzgun,MarkusFreitag,XuezhiWang,SurajSrivats,SoroushVosoughi,HyungWonChung,YiTay,SebastianRuder,DennyZhou,
etal.2022.Languagemodelsaremultilingualchain-of-thoughtreasoners.arXivpreprintarXiv:2210.03057(2022).
[40] JingyuShi,RahulJain,HyungjunDoh,RyoSuzuki,andKarthikRamani.2023.AnHCI-CentricSurveyandTaxonomyofHuman-Generative-AI
Interactions.arXivpreprintarXiv:2310.07127(2023).
[41] YangShi,TianGao,XiaohanJiao,andNanCao.2023. UnderstandingDesignCollaborationBetweenDesignersandArtificialIntelligence:A
SystematicLiteratureReview.ProceedingsoftheACMonHuman-ComputerInteraction7,CSCW2(2023),1–35.
[42] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli.2015.Deepunsupervisedlearningusingnonequilibriumthermody-
namics.InInternationalConferenceonMachineLearning.PMLR,2256–2265.
[43] VeeraVimpari,AnnakaisaKultima,PerttuHämäläinen,andChristianGuckelsberger.2023. "AnAdapt-or-DieTypeofSituation":Perception,
Adoption,andUseofText-To-Image-GenerationAIbyGameIndustryProfessionals.arXivpreprintarXiv:2302.12601(2023).
[44] BoWang,TaoWu,MinfengZhu,andPengDu.2022.Interactiveimagesynthesiswithpanopticlayoutgeneration.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition.7783–7792.
[45] XinyiWang,WanrongZhu,andWilliamYangWang.2023. Largelanguagemodelsareimplicitlytopicmodels:Explainingandfindinggood
demonstrationsforin-contextlearning.arXivpreprintarXiv:2301.11916(2023).
[46] ZeyuWang,CuongNguyen,PaulAsente,andJulieDorsey.2023.PointShopAR:SupportingEnvironmentalDesignPrototypingUsingPointCloud
inAugmentedReality.InProceedingsofthe2023CHIConferenceonHumanFactorsinComputingSystems.1–15.
[47] JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud,DaniYogatama,MaartenBosma,DennyZhou,DonaldMetzler,
etal.2022.Emergentabilitiesoflargelanguagemodels.arXivpreprintarXiv:2206.07682(2022).
[48] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,DennyZhou,etal.2022.Chain-of-thoughtprompting
elicitsreasoninginlargelanguagemodels.AdvancesinNeuralInformationProcessingSystems35(2022),24824–24837.
[49] ChenfeiWu,ShengmingYin,WeizhenQi,XiaodongWang,ZechengTang,andNanDuan.2023.Visualchatgpt:Talking,drawingandeditingwith
visualfoundationmodels.arXivpreprintarXiv:2303.04671(2023).
[50] DiWu,ZhiwangYu,NanMa,JiananJiang,YuetianWang,GuixiangZhou,HanhuiDeng,andYiLi.2023.StyleMe:TowardsIntelligentFashion
GenerationwithDesignerStyle.InProceedingsofthe2023CHIConferenceonHumanFactorsinComputingSystems.1–16.
[51] TongshuangWu,EllenJiang,AaronDonsbach,JeffGray,AlejandraMolina,MichaelTerry,andCarrieJCai.2022.Promptchainer:Chaininglarge
languagemodelpromptsthroughvisualprogramming.InCHIConferenceonHumanFactorsinComputingSystemsExtendedAbstracts.1–10.
[52] ShishiXiao,SuiziHuang,YueLin,YilinYe,andWeiZeng.2023.Letthechartspark:Embeddingsemanticcontextintochartwithtext-to-image
generativemodel.IEEETransactionsonVisualizationandComputerGraphics(2023).
[53] LingYang,ZhilinHuang,YangSong,ShendaHong,GuohaoLi,WentaoZhang,BinCui,BernardGhanem,andMing-HsuanYang.2022.Diffusion-
basedscenegraphtoimagegenerationwithmaskedcontrastivepre-training.arXivpreprintarXiv:2211.11138(2022).
[54] QianYang,AaronSteinfeld,CarolynRosé,andJohnZimmerman.2020.Re-examiningwhether,why,andhowhuman-AIinteractionisuniquely
difficulttodesign.InProceedingsofthe2020chiconferenceonhumanfactorsincomputingsystems.1–13.
[55] ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,ThomasLGriffiths,YuanCao,andKarthikNarasimhan.2023. Treeofthoughts:Deliberate
problemsolvingwithlargelanguagemodels.arXivpreprintarXiv:2305.10601(2023).
[56] LvminZhangandManeeshAgrawala.2023.Addingconditionalcontroltotext-to-imagediffusionmodels.arXivpreprintarXiv:2302.05543(2023).
[57] ZhuoshengZhang,AstonZhang,MuLi,andAlexSmola.2022.AutomaticChainofThoughtPromptinginLargeLanguageModels.arXivpreprint
arXiv:2210.03493(2022).
[58] ZhuoshengZhang,AstonZhang,MuLi,HaiZhao,GeorgeKarypis,andAlexSmola.2023.Multimodalchain-of-thoughtreasoninginlanguage
models.arXivpreprintarXiv:2302.00923(2023).
[59] GuangcongZheng,XianpanZhou,XueweiLi,ZhongangQi,YingShan,andXiLi.2023. LayoutDiffusion:ControllableDiffusionModelfor
Layout-to-imageGeneration.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.22490–22499.
[60] ZhaohuiZheng,PingWang,DongweiRen,WeiLiu,RongguangYe,QinghuaHu,andWangmengZuo.2021.Enhancinggeometricfactorsinmodel
learningandinferenceforobjectdetectionandinstancesegmentation.IEEETransactionsonCybernetics52,8(2021),8574–8586.
26PlantoGraphy CHI’24,May11–16,2024,Honolulu,HI,USA
A GLOSSARY
Term Definition
ArtificialIntelligenceGeneratedContent Anewcreationmodelincorporatingartificialintelligenceandhumancollaborationforcontent
generation.
(AIGC)
Boundingbox Arectangularboxrepresentsanobject’sspatiallocationanddimensionalinformation.
Denoising Diffusion Implicit Model Atechniqueisproposedforiterativelytransformingdatadistributions,suchasimages,intorandom
noise.Itenablesthemodeltoreconstructorgeneratespecificinstancesofdata,suchasimages,by
(DDIM)inversion
retracingbackitscorrespondinglatentspaceinthediffusionprocess.
EndtoEnd Anideathatonlyfocusesoninputsandoutputswithoutconsideringintermediateprocesses.
Fewshots Anothermethodtotrainmodelsforspecifictaskswithafewsamples.
Fine-tune Atechniquetotrainapre-trainedmodeltooptimizeitsperformanceonaspecifictask.
GatedAttention Itisacomponentofneuralnetworksreferredtoastheattentionmechanismthatplaysacrucial
roleinselectivelyfocusingontargetedregions,therebyenhancingthemodel’sabilitytocapture
andextractkeyfeatures.
GenerativeArtificialIntelligence(GAI) Artificialintelligencecangeneratenewcontent(e.g.,textandimages)throughtrainingonan
extensivedataset[31].
GraphicalUserInterface(GUI) Aninterfacethatprovidesvisualinformationandallowsinteraction.
LargeLanguageModel(LLM) Thelanguagemodelcancomprehendandproducehuman-likeresponsesbasedonalargeamount
oftextualdatatraining.
LatentDiffusionModel Basedonthetraditionaldiffusionmodel,thismodelappliesthediffusionprocesstothelatent
encodingratherthanthepixels,thusgeneratinghigh-qualityimagesmoreefficiently[28,37].
Low-RankAdaptation(LoRA)Diffusion Amethodoffine-tuninglargepre-trainedmodelsfortext-to-imagegenerationenablesthetraining
withfewerparameterswhileensuringhighperformance,reducingcomputationalcost,andallowing
Model
largemodelstoexcelinspecifictasks[20].
Pre-trained text-to-image generation Generativeartificialintelligence(GAI)modelsthathavebeenpreviouslytrainedcanbefurther
tunedtoperformtext-to-imagetasks.
models
PromptEngineering Onepracticeistomodifythepromptsgiventothelargelanguagemodelsforbetteroutputsthat
meetuserneeds.
Prompts Instructionsorrequestsgiventogenerativeartificialintelligence.
StableDiffusionUNet Asymmetricalstructureofthecentralcomponentinthestablediffusionmodelplaysacrucialrole
ineffectivelydenoisingimages.
Zeroshot Atrainingapproachusingamodeldirectlytoperformtaskswithoutadditionaltraining.
27