Bayesian Optimization with Noise-Free Observations:
Improved Regret Bounds via Random Exploration
Hwanwoo Kim and Daniel Sanz-Alonso
University of Chicago
Abstract
This paper studies Bayesian optimization with noise-free observations. We introduce new
algorithms rooted in scattered data approximation that rely on a random exploration step to
ensure that the fill-distance of query points decays at a near-optimal rate. Our algorithms retain
the ease of implementation of the classical GP-UCB algorithm and satisfy cumulative regret
bounds that nearly match those conjectured in [Vak22], hence solving a COLT open problem.
Furthermore, the new algorithms outperform GP-UCB and other popular Bayesian optimization
strategies in several examples.
1 Introduction
Bayesian optimization [JSW98, Moc98, Fra18] is an attractive strategy for global optimization of
black-box objective functions. Bayesian optimization algorithms sequentially acquire information
on the objective by observing its value at carefully selected query points. In some applications,
these observations are noisy, but in many others the objective can be noiselessly observed; examples
include hyperparameter tuning for machine learning algorithms [BS96], parameter estimation for
computer models [CJBGP16, Pou20], goal-driven dynamics learning [BCX+17], and alignment of
density maps in Wasserstein distance [SY23]. While most Bayesian optimization algorithms can be
implemented with either noisy or noise-free observations, few methods and theoretical analyses are
tailored to the noise-free setting.
This paper introduces two new algorithms rooted in scattered data approximation for Bayesian
optimizationwithnoise-freeobservations. Thefirstalgorithm,whichwecallGP-UCB+,supplements
query points obtained via the classical GP-UCB algorithm [SKKS10] with randomly sampled query
points. The second algorithm, which we call EXPLOIT+, supplements query points obtained by
maximizing the posterior mean of a Gaussian process surrogate model with randomly sampled
query points. Both algorithms retain the simplicity and ease of implementation of the GP-UCB
algorithm, but introduce an additional random exploration step to ensure that the fill-distance
of query points decays at a near-optimal rate. The new random exploration step has a profound
impact on both theoretical guarantees and empirical performance. On the one hand, GP-UCB+ and
EXPLOIT+ satisfy regret bounds that improve upon existing and refined rates for the GP-UCB
algorithm. Indeed, the new algorithms nearly achieve the cumulative regret bounds for noise-free
bandits conjectured in [Vak22]. On the other hand, GP-UCB+ and EXPLOIT+ explore the state
space faster, which leads to an improvement in numerical performance across a range of benchmark
and real-world examples.
1
4202
naJ
03
]GL.sc[
1v73071.1042:viXra1.1 MainContributions
• We introduce two new algorithms, GP-UCB+ and EXPLOIT+, which attain the cumulative
regret bounds for noise-free bandits conjectured in [Vak22], and far improve existing and
refined rates for the classical GP-UCB algorithm. Our theory covers both deterministic
assumptions on the objective (Theorem 4.4) and probabilistic ones (Theorem 4.8). En route
to compare the cumulative regret bounds for our new algorithms with those for GP-UCB, we
establish in Theorem 3.1 a regret bound for GP-UCB with squared exponential kernels that
refines the one in [LYT19].
• Our algorithms and regret bounds bring ideas from the literature on scattered data approxi-
mation [Wen04] to the design and analysis of Bayesian optimization algorithms. Such ideas
are particularly powerful in the noise-free setting, where the maximum information gain used
in the analysis of the noisy setting is not well defined [SKKS10].
• We numerically demonstrate that GP-UCB+ and EXPLOIT+ outperform GP-UCB and other
popular Bayesian optimization algorithms across many examples, including optimization of
several 10-dimensional benchmark objective functions, hyperparameter tuning for random
forests, and optimal parameter estimation of a garden sprinkler computer model.
• We showcase that both GP-UCB+ and EXPLOIT+ share the simplicity and ease of implemen-
tation of the GP-UCB algorithm. In addition, EXPLOIT+ requires fewer input parameters
than GP-UCB or GP-UCB+, and achieves competitive empirical performance without any
tuning.
1.2 Outline
Section 2 formalizes the problem of interest and provides necessary background. We review related
work in Section 3. Our new algorithms are introduced in Section 4, where we establish regret bounds
under deterministic and probabilistic assumptions on the objective. Section 5 contains numerical
examples, and we close in Section 6. Proofs and additional numerical experiments are deferred to
an appendix.
2 Preliminaries
2.1 ProblemStatement
We want to find the global maximizer of an objective function 𝑓 : 𝒳 → R by leveraging the observed
values of 𝑓 at carefully chosen query points. We are interested in the setting where the observations
of the objective are noise-free, i.e. for query points 𝑋 = {𝑥 ,...,𝑥 } we can access observations
𝑡 1 𝑡
𝐹 = [𝑓(𝑥 ),...,𝑓(𝑥 )]⊤. The functional form of 𝑓 is not assumed to be known. For simplicity, we
𝑡 1 𝑡
assume throughout that 𝒳 ⊂ R𝑑 is a 𝑑-dimensional hypercube.
Our theory will cover both deterministic and probabilistic assumptions on the objective 𝑓. In
the deterministic case, we assume that 𝑓 ∈ ℋ (𝒳) belongs to the Reproducing Kernel Hilbert
𝑘
Space (RKHS) associated with a kernel 𝑘 : 𝒳 ×𝒳 → R. In the probabilistic case, we assume that
𝑓 ∼ 𝒢𝒫(0,𝑘) is a draw from a centered Gaussian process with covariance function 𝑘.
22.2 GaussianProcessesandBayesianOptimization
Many Bayesian optimization algorithms, including the ones introduced in this paper, rely on a
Gaussian process surrogate model of the objective function to guide the choice of query points.
Here, we review the main ideas. Denote generic query locations by 𝑋 = {𝑥 ,...,𝑥 } and the
𝑡 1 𝑡
corresponding noise-free observations by 𝐹 = [𝑓(𝑥 ),...,𝑓(𝑥 )]⊤. Gaussian process interpolation
𝑡 1 𝑡
with a prior 𝒢𝒫(0,𝑘) yields the following posterior predictive mean and variance:
𝜇 (𝑥) = 𝑘 (𝑥)⊤𝐾−1𝐹 ,
𝑡,0 𝑡 𝑡𝑡 𝑡
𝜎2 (𝑥) = 𝑘(𝑥,𝑥)−𝑘 (𝑥)⊤𝐾−1𝑘 (𝑥),
𝑡,0 𝑡 𝑡𝑡 𝑡
where 𝑘 (𝑥) = [𝑘(𝑥,𝑥 ),...,𝑘(𝑥,𝑥 )]⊤ and 𝐾 is a 𝑡×𝑡 matrix with entries (𝐾 ) = 𝑘(𝑥 ,𝑥 ).
𝑡 1 𝑡 𝑡𝑡 𝑡𝑡 𝑖,𝑗 𝑖 𝑗
Our interest lies in Bayesian optimization with noise-free observations. However, we recall for
later reference that if the observations are noisy and take the form 𝑦 = 𝑓(𝑥 )+𝜂 , 1 ≤ 𝑖 ≤ 𝑡, where
𝑖 𝑖 𝑖
i.i.d.
𝜂 ∼ 𝑁(0,𝜆), then the posterior predictive mean and variance are given by
𝑖
𝜇 (𝑥) = 𝑘 (𝑥)⊤(𝐾 +𝜆𝐼)−1𝑌 ,
𝑡,𝜆 𝑡 𝑡𝑡 𝑡
𝜎2 (𝑥) = 𝑘(𝑥,𝑥)−𝑘 (𝑥)⊤(𝐾 +𝜆𝐼)−1𝑘 (𝑥),
𝑡,𝜆 𝑡 𝑡𝑡 𝑡
where 𝑌 = [𝑦 ,...,𝑦 ]⊤.
𝑡 1 𝑡
To perform Bayesian optimization, one can sequentially select query points by optimizing a Gaus-
sian Process Upper Confidence Bound (GP-UCB) acquisition function. Let 𝑋 = {𝑥 ,...,𝑥 }
𝑡−1 1 𝑡−1
denote the query points at the (𝑡−1)-th iteration of the algorithm. Then, at the 𝑡-th iteration, the
classical GP-UCB algorithm [SKKS10] sets
1
𝑥 = argmax𝜇 (𝑥)+𝛽2𝜎 (𝑥), (2.1)
𝑡 𝑡−1,𝜆 𝑡 𝑡−1,𝜆
𝑥∈𝒳
where 𝛽 is a user-chosen positive parameter. The posterior predictive mean provides a surrogate
𝑡
model for the objective; hence, one expects the maximum of 𝑓 to be achieved at a point 𝑥 ∈ 𝒳 where
𝜇 (𝑥) is large. However, the surrogate model 𝜇 (𝑥) may not be accurate at points 𝑥 ∈ 𝒳
𝑡−1,𝜆 𝑡−1,𝜆
where 𝜎2 (𝑥) is large, and selecting query points with large predictive variance helps improve the
𝑡−1,𝜆
accuracy of the surrogate model. The GP-UCB algorithm finds a compromise between exploitation
(maximizingthemean)andexploration(maximizingthevariance). Theweightparameter𝛽 balances
𝑡
this exploitation-exploration trade-off. For later discussion, Algorithm 2.1 below summarizes the
approach with noise-free observations 𝐹 and 𝜆 = 0.
𝑡
2.3 PerformanceMetric
The performance of Bayesian optimization algorithms is often analyzed through bounds on their
cumulative regret, 𝑅 , given by
𝑇
𝑇
𝑅 = ∑︁ 𝑟 , 𝑟 = 𝑓*−𝑓(𝑥 ),
𝑇 𝑡 𝑡 𝑡
𝑡=1
where 𝑓* is the maximum of the objective 𝑓, 𝑥 is the query location selected at the 𝑡-th iteration,
𝑡
and 𝑟 is called the instantaneous regret. Algorithms that satisfy lim 𝑅 /𝑇 = 0 are known as
𝑡 𝑇→∞ 𝑇
3Algorithm 2.1 GP-UCB with noise-free observations .
1: Input: Kernel 𝑘; Total number of iterations 𝑇; Initial design points 𝑋 0; Initial noise-free
observations 𝐹 . Weights {𝛽 }𝑇 .
0 𝑡 𝑡=1
2: Construct 𝜇 0,0(𝑥) and 𝜎 0,0(𝑥) using 𝑋 0 and 𝐹 0.
3: For 𝑡 = 1,...,𝑇 do:
1. Set
1
𝑥 = argmax𝜇 (𝑥)+𝛽2𝜎 (𝑥).
𝑡 𝑡−1,0 𝑡 𝑡−1,0
𝑥∈𝒳
2. Set 𝑋 = 𝑋 ∪{𝑥 } and 𝐹 = 𝐹 ∪{𝑓(𝑥 )}.
𝑡 𝑡−1 𝑡 𝑡 𝑡−1 𝑡
3. Update 𝜇 (𝑥) and 𝜎 (𝑥) using 𝑋 and 𝐹 .
𝑡,0 𝑡,0 𝑡 𝑡
4: Output: argmax 𝑥∈𝑋𝑇 𝑓(𝑥).
zero-regret algorithms. In the large 𝑇 asymptotic, zero-regret algorithms successfully recover the
maximum of the objective since 0 ≤ 𝑓*−max 𝑓(𝑥 ) ≤ 𝑅𝑇.
𝑡=1,...,𝑇 𝑡 𝑇
2.4 ChoiceofKernel
We will consider the well-specified setting where Gaussian process interpolation for surrogate
modeling is implemented using the same kernel 𝑘 which specifies the deterministic or probabilistic
assumptions on 𝑓, namely 𝑓 ∈ ℋ or 𝑓 ∼ 𝒢𝒫(0,𝑘). The impact of kernel misspecification on
𝑘
Bayesian optimization algorithms is studied in [BK21, KSAY22].
For concreteness, we focus on Matérn kernels with smoothness parameter 𝜈 and lengthscale
parameter ℓ, given by
1 (︂‖𝑥−𝑥′‖)︂𝜈 (︂‖𝑥−𝑥′‖)︂
𝑘(𝑥,𝑥′) = 𝐵 ,
Γ(𝜈)2𝜈−1 ℓ 𝜈 ℓ
where 𝐵 is a modified Bessel function of the second kind, and on squared exponential kernels with
𝜈
lengthscale parameter ℓ, given by
(︃ )︃
‖𝑥−𝑥′‖2
𝑘(𝑥,𝑥′) = exp − .
2ℓ2
We recall that the Matérn kernel converges to the squared exponential kernel in the large 𝜈
asymptotic. Both types of kernel are widely used in practice, and we refer to [WR06, Wen04, Ste12]
for further background.
3 Related Work
3.1 ExistingRegretBounds: NoisyObservations
Numerous works have established cumulative regret bounds for Bayesian optimization with noisy
observations under both deterministic and probabilistic assumptions on the objective function
[SKKS10, CG17, VKP21, BK21, RVR14, KKSP18]. These bounds involve a quantity known
as the maximum information gain, which under a Gaussian noise assumption is given by 𝛾 =
𝑡
1 log|𝐼 +𝜆−1𝐾 𝑡|, where 𝜆 > 0 represents the noise level. In particular, under a deterministic
2 𝑡
4objective function assumption, [CG17] showed a cumulative regret bound for GP-UCB of the form
𝒪(︁
𝛾
√ 𝑇)︁
,
whichimprovestheoneobtainedin[SKKS10]byafactorof𝒪(︀ log3/2(𝑇))︀
. Bytightening
𝑇
existing upper bounds on the maximum information gain, [VKP21] established a cumulative regret
bound for GP-UCB of the form
⎧ (︂ )︂
2𝜈+3𝑑 2𝜈
⎪⎨𝒪 𝑇4𝜈+2𝑑 log2𝜈+𝑑 𝑇 ,
𝑅 =
𝑇 ⎪⎩𝒪(︁ 𝑇1
2
log𝑑+1𝑇)︁
,
for Matérn and squared exponential kernels.
3.2 ExistingRegretBounds: Noise-FreeObservations
In contrast to the noisy setting, few works have obtained regret bounds with noise-free observations.
With an expected improvement acquisition function and Matérn kernel, [Bul11] provides a simple
(︁ )︁
regret bound of the form 𝒪˜ 𝑡−min{𝜈,1}/𝑑 , where 𝒪˜ suppresses logarithmic factors. [DFSZ12]
introduced a branch and bound algorithm that achieves an exponential rate of convergence for the
instantaneous regret. However, unlike the standard GP-UCB algorithm, the algorithm in [DFSZ12]
requires many observations in each iteration to reduce the search space, and it further requires
solving a constrained optimization problem in the reduced search space.
To the best of our knowledge, [LYT19] presents the only cumulative regret bound available for
GP-UCBwithnoise-freeobservationsunderadeterministicassumptionontheobjective. Specifically,
they consider Algorithm 2.1, and, noticing that 𝜎 (𝑥) ≤ 𝜎 (𝑥) for any 𝜆 ≥ 0, they deduce that
𝑡,0 𝑡,𝜆
existing cumulative regret bounds for Bayesian optimization with noisy observations remain valid
with noise-free observations. Furthermore, in the noise-free setting, the cumulative regret bound is
√
improved by a factor of 𝛾 , which comes from using a constant weight parameter 𝛽 := ‖𝑓‖2
𝑇 𝑡 ℋ (𝒳)
𝑘
given by the squared RKHS norm of the objective. This leads to a cumulative regret bound with
√
rate 𝒪( 𝛾 𝑇), which gives
𝑇
⎧ (︂ )︂
𝜈+𝑑 𝜈
⎪⎨𝒪 𝑇2𝜈+𝑑 log2𝜈+𝑑 𝑇 ,
𝑅 = (3.1)
𝑇 ⎪⎩𝒪(︁ 𝑇1
2
log𝑑+ 21 𝑇)︁
,
for Matérn and squared exponential kernels. A similar improvement has not been established under
a probabilistic assumption on the objective.
3.3 TighterBoundforSquaredExponentialKernels
[Vak22] sets as an open problem whether one can improve the cumulative regret bounds in (3.1) for
the GP-UCB algorithm with noise-free observations. For squared exponential kernels, we claim that
√
one can further improve the cumulative regret bound in (3.1) by a factor of log𝑇.
Theorem3.1. Let 𝑓 ∈ ℋ 𝑘(𝒳), where 𝑘 is a squared exponential kernel. GP-UCB with noise-free
observations and 𝛽 := ‖𝑓‖2 satisfies the cumulative regret bound
𝑡 ℋ
𝑘
(︁ 1 𝑑 )︁
𝑅
𝑇
= 𝒪 𝑇2 log2 𝑇 .
5Remark3.2. Our improvement in the bound comes from a constant term 1 , which was ignored
log(1+𝜆−1) √
in existing analyses with noisy observations. By letting 𝜆 → 0, the constant offsets a log𝑇 growth
in the cumulative regret bound.
Remark3.3. For Matérn kernels, a similar approach to improve the rate is not feasible. A state-of-the-
art, near-optimal upper bound on the maximum information gain with Matérn kernels obtained in
[VKP21]introducesapolynomialgrowthfactorasthenoisevariance𝜆decreasestozero. Minimizing
the rate of an upper bound in [VKP21] one can match the rate obtained in [LYT19].
Remark3.4. Unlike cumulative regret bounds with noisy observations, Theorem 3.1 and the results in
[LYT19] are deterministic.
3.4 ConjecturedRegretBounds
Theorem 3.1 refines the rate bound in (3.1) for GP-UCB with noise-free observations using squared
exponential kernels. In the rest of the paper, we will design new algorithms that achieve drastically
faster rates. In particular, we are motivated by [Vak22], which conjectured that, for Matérn kernels,
a cumulative regret bound
⎧ 𝑑−𝜈
⎪⎪⎨𝒪(𝑇 𝑑 ), for 𝑑 > 𝜈
𝑅 = 𝒪(log𝑇), for 𝑑 = 𝜈
𝑇
⎪⎪⎩𝒪(1),
for 𝑑 < 𝜈
could be obtained. Our new algorithms nearly achieve this bound while preserving the ease of
implementation of GP-UCB algorithms. The recent preprint [SVZ23] proposes an alternative batch-
based approach, which combines random sampling with domain shrinking to attain the conjectured
rates with high probability.
4 Exploitation with Accelerated Exploration
4.1 HowWellDoesGP-UCBExplore?
TheGP-UCBalgorithmselectsquerypointsbyoptimizinganacquisitionfunctionwhichincorporates
the posterior mean to promote exploitation and the posterior standard deviation to promote
exploration. Our new algorithms are inspired by the desire to improve the exploration of GP-UCB.
Before introducing the algorithms in the next subsection, we heuristically explain why such an
improvement may be possible.
A natural way to quantify how well data 𝑋 = {𝑥 ,...,𝑥 } cover the search space 𝒳 is via the
𝑡 1 𝑡
fill-distance, given by
ℎ(𝒳,𝑋 ) := sup inf ‖𝑥−𝑥 ‖.
𝑡 𝑖
𝑥∈𝒳𝑥𝑖∈𝑋𝑡
The fill-distance appears in error bounds for Gaussian process interpolation and regression [Wen04,
Tec20, ST18, TW20]. For quasi-uniform points, it holds that ℎ(𝒳,𝑋 𝑡) =
Θ(︁
𝑡−
𝑑1)︁
, which is the
fastest possible decay rate for any sequence of design points. The fill-distance of the query points
selected by our new Bayesian optimization algorithms will (nearly) decay at this rate.
[WSH21] introduced a stabilized greedy algorithm to obtain query points by maximizing the
posterior predictive standard deviation at each iteration. Their algorithm sequentially generates a
6Figure1Average fill-distance of a set of query points obtained using four different algorithms over 100
independent experiments. The results are based on a 10-dimensional Rastrigin function. The discrete
subset 𝒳 consists of 100 Latin hypercube samples.
𝐷
set of query points whose fill-distance decays at a rate
Θ(︁
𝑡−
𝑑1)︁
by sequentially solving constrained
optimization problems, which can be computationally demanding. Since GP-UCB simply promotes
explorationthroughtheposteriorpredictivestandarddeviationtermintheUCBacquisitionfunction,
one may heuristically expect the fill-distance of query points selected by the standard GP-UCB
algorithm to decay at a slower rate. On the other hand, a straightforward online approach to obtain
a set of query points whose fill-distance nearly decays at a rate
Θ(︁
𝑡−
𝑑1)︁
is to sample randomly from
a probability measure 𝑃 with a strictly positive Lebesgue density on 𝒳. Specifically, [OCBG19]
shows that, in expectation, the fill-distance of independent samples from such a measure decays at
a near-optimal rate: for any 𝜖 > 0, E 𝑃[︀ ℎ(𝒳,𝑋 𝑡)]︀ = 𝒪(𝑡− 𝑑1+𝜖).
Figure 1 compares the decay of the fill-distance for query points selected using four strategies.
For a 10-dimensional Rastrigin function, we consider: (i) GP-UCB; (ii) EXPLOIT (i.e., maximizing
the posterior mean at each iteration); (iii) EXPLORE (i.e., maximizing the posterior variance); and
(iv) UNIFORM (i.e. independent uniform random samples on 𝒳). The results were averaged over
100 independent experiments. The fill-distance for GP-UCB lies in between those for EXPLORE
and EXPLOIT; whether it lies closer to one or the other depends on the choice of weight parameter,
which here we choose based on a numerical approximation of the max-norm of the objective,
1
𝛽2 = max |𝑓(𝑥)| where 𝒳 is a discretization of the search space 𝒳. Note that UNIFORM
𝑡 𝑥∈𝒳𝐷 𝐷
yields a drastically smaller fill-distance even when compared with EXPLORE. Our new algorithms
willleveragerandomsamplingtoenhanceexplorationinBayesianoptimizationandachieveimproved
regret bounds.
4.2 ImprovedExplorationviaRandomSampling
Inthissubsection,weintroducetwoBayesianoptimizationalgorithmsthatleveragerandomsampling
as a tool to facilitate efficient exploration of the search space. While the GP-UCB algorithm selects
a single query point per iteration, our algorithms select two query points per iteration.
The first algorithm we introduce, which we call GP-UCB+, selects a query point using the GP-
UCB acquisition function and another query point by random sampling. We outline the pseudocode
7in Algorithm 4.1. The second algorithm we introduce, which we call EXPLOIT+, decouples the
exploitation and exploration goals, selecting one query point by maximizing the posterior mean to
promote exploitation, and another query point by random sampling to promote exploration. We
outline the pseudocode in Algorithm 4.2.
Algorithm 4.1 GP-UCB+.
1: Input: Kernel 𝑘; Total number of iterations 𝑇; Initial design points 𝑋full; Initial noise-free
0
observations 𝐹full; Probability distribution 𝑃 on 𝒳. Weights {𝛽 }𝑇 .
0 𝑡 𝑡=1
2: Construct posterior mean 𝜇full(𝑥) and standard deviation 𝜎full(𝑥) using 𝑋full and 𝐹full.
0,0 0,0 0 0
3: For 𝑡 = 1,...,𝑇 do:
1. Exploitation + Exploration: Set
1
𝑥 = argmax𝜇full (𝑥)+𝛽2𝜎full (𝑥).
𝑡 𝑡−1,0 𝑡 𝑡−1,0
𝑥∈𝒳
2. Exploration: Sample 𝑥˜ ∼ 𝑃.
𝑡
3. Set
𝑋full = 𝑋full ∪{𝑥 ,𝑥˜ },
𝑡 𝑡−1 𝑡 𝑡
𝐹full = 𝐹full ∪{𝑓(𝑥 ),𝑓(𝑥˜ )}.
𝑡 𝑡−1 𝑡 𝑡
4. Update 𝜇full(𝑥) and 𝜎full(𝑥) using 𝑋full and 𝐹full.
𝑡,0 𝑡,0 𝑡 𝑡
4: Output: argmax 𝑥∈𝑋full𝑓(𝑥).
𝑇
Notably, EXPLOIT+ does not require input weight parameters {𝛽 }𝑇 . As mentioned in Section
𝑡 𝑡=1
3, many regret bounds for GP-UCB algorithms rely on choosing the weight parameters as the
squared RKHS norm of the objective or in terms of a bound on it. The performance of GP-UCB
and GP-UCB+ can be sensitive to this choice, which in practice is often based on empirical tuning
or heuristic arguments rather than guided by the theory. In contrast, EXPLOIT+ achieves the
same regret bounds as GP-UCB+ and drastically faster rates than GP-UCB without requiring the
practitioner to specify weight parameters. Additionally, EXPLORE+ shows competitive empirical
performance.
Remark 4.1. In the exploration step, one can acquire a batch of points to further enhance the
exploration of GP-UCB+ and EXPLOIT+. As long as the number of points sampled at each
iteration does not grow with respect to the iteration index 𝑡, the regret bounds stated in Theorems
4.4 and 4.8 below remain valid.
Remark 4.2. A common heuristic strategy to expedite the performance of Bayesian optimization
algorithms is to acquire a moderate number of initial design points by uniformly sampling the search
space. Since the order of the exploration and exploitation steps can be swapped in our algorithms,
such heuristic strategy can be interpreted as an initial batch exploration step.
Remark4.3. A natural choice for 𝑃 is the uniform distribution on the search space 𝒳. Our theory,
which utilizes bounds on the fill-distance of randomly sampled query points from [OCBG19], holds
8Algorithm 4.2 EXPLOIT+.
1: Input: Kernel 𝑘; Total number of iterations 𝑇; Initial design points 𝑋full; Initial noise-free
0
observations 𝐹full; Probability distribution 𝑃 on 𝒳.
0
2: Construct posterior mean 𝜇full(𝑥) and standard deviation 𝜎full(𝑥) using 𝑋full and 𝐹full.
0,0 0,0 0 0
3: For 𝑡 = 1,...,𝑇 do:
1. Exploitation: Set 𝑥 = argmax 𝜇full (𝑥).
𝑡 𝑥∈𝒳 𝑡−1,0
2. Exploration: Sample 𝑥˜ ∼ 𝑃.
𝑡
3. Set
𝑋full = 𝑋full ∪{𝑥 ,𝑥˜ },
𝑡 𝑡−1 𝑡 𝑡
𝐹full = 𝐹full ∪{𝑓(𝑥 ),𝑓(𝑥˜ )}.
𝑡 𝑡−1 𝑡 𝑡
4. Update 𝜇full(𝑥) using 𝑋full and 𝐹full.
𝑡,0 𝑡 𝑡
4: Output: argmax 𝑥∈𝑋full𝑓(𝑥).
𝑇
as long as 𝑃 has a strictly positive Lebesgue density on 𝒳. In what follows, we assume throughout
that 𝑃 satisfies this condition.
4.3 RegretBounds: DeterministicSetting
We first obtain regret bounds under the deterministic assumption that 𝑓 belongs to the RKHS of
a kernel 𝑘. Our algorithms are random due to sampling from 𝑃, and we show cumulative regret
bounds in expectation with respect to such randomness.
Theorem4.4. Let 𝑓 ∈ ℋ 𝑘(𝒳). GP-UCB+ with 𝛽 𝑡 := ‖𝑓‖2 ℋ (𝒳) and EXPLOIT+ attain the following
𝑘
cumulative regret bounds. For Matérn kernels with parameter 𝜈 > 0,
{︃ 𝑑−𝜈+𝜀
E [𝑅 ] = 𝒪(𝑇 𝑑 ), for 𝑑 > 𝜈 −𝜀
𝑃 𝑇
𝒪(1), for 𝑑 ≤ 𝜈 −𝜀
where 𝜀 > 0 can be arbitrarily small. For squared exponential kernels,
E [𝑅 ] = 𝒪(1).
𝑃 𝑇
Remark4.5. In expectation, the proposed algorithms nearly attain the optimal rate conjectured in
[Vak22]. Moreover, one can obtain the exact rate conjectured in [Vak22] by replacing the random
sampling step in GP-UCB+ and EXPLOIT+ with a more computationally expensive quasi-uniform
sampling scheme.
Remark 4.6. Compared with the GP-UCB algorithm with noise-free observations, the proposed
algorithms attain improved cumulative regret bounds in expectation for both Matérn and squared
exponential kernels. For the Matérn kernel, the new cumulative regret bound satisfies a slower
polynomial growth rate and completely removes the log factor. For the squared exponential kernel,
the proposed algorithms have bounded cumulative regret, whereas the improved bound for the
1 𝑑
GP-UCB algorithm in Theorem 3.1 grows at rate 𝑇2 log2(𝑇).
93
Remark4.7. For Matérn kernels, compared with the recent preprint [SVZ23], which attains 𝒪(log2 𝑇)
rate when 𝑑 < 𝜈, our algorithms achieve bounded cumulative regret. When 𝑑 ≥ 𝜈, [SVZ23] attains
the rate conjectured in [Vak22] up to a logarithmic factor, while we attain the conjectured rate up
to a factor of 𝒪(𝑇𝜖), for arbitrarily small 𝜖 > 0. Our result additionally covers squared exponential
kernels, for which we show bounded cumulative regret.
4.4 RegretBounds: ProbabilisticSetting
Next, we provide regret bounds under the probabilistic assumption that 𝑓 ∼ 𝒢𝒫(0,𝑘) is a draw from
a Gaussian process with covariance 𝑘. To this end, let 𝒳 := [−𝑟,𝑟]𝑑 ⊂ R𝑑 and assume the following
high probability bound on the derivatives of GP sample paths 𝑓: for some constants 𝑎,𝑏 > 0,
P(sup|𝜕𝑓/𝜕𝑥 | > 𝐿) ≤ 𝑎exp(−𝐿2/𝑏2), 𝑗 = 1,...,𝑑.
𝑗
𝑥∈𝒳
For squared exponential and Matérn kernels with smoothness parameter 𝜈 > 2, the above condition
is met, while it is violated for the Ornstein-Uhlenbeck kernel [GR06, Ste12].
Theorem 4.8. Suppose 𝑓 ∼ 𝒢𝒫(0,𝑘). For any 𝛿 > 0, GP-UCB+ with 𝛽 𝑡 :=
2log(︀ 2𝑡2𝜋2/(3𝛿))︀
+
2𝑑log(2𝑡2𝑑𝑏𝑟√︀
log(4𝑑𝑎/𝛿)) and EXPLOIT+ satisfy the following cumulative regret bounds with
probability at least 1−𝛿. For Matérn kernels with parameter 𝜈 > 2,
E 𝑃[𝑅 𝑇] = 𝒪(︁ 𝑇𝑑− 𝑑𝜈+𝜀 log1 2 𝑇)︁ ,
where 𝜀 > 0 can be arbitrarily small. For squared exponential kernels,
E 𝑃[𝑅 𝑇] = 𝒪(︁ log1 2 𝑇)︁ .
√
Remark4.9. Recall that under the RKHS assumption, [LYT19] shows an 𝒪( 𝛾 𝑇) factor improvement
in the noise-free observation setting compared with the noisy case thanks to the constant exploration
parameter 𝛽 in the GP-UCB algorithm. However, under the Gaussian process assumption, we need
𝑡
to increase the exploration parameter. Therefore, no additional improvement can be made under
the noise-free setting over the noisy case.
Remark4.10. Compared with GP-UCB with noisy observations, where [VKP21] shows regret bounds
𝒪(︂ 𝑇2𝜈 𝜈+ +𝑑
𝑑
log44 𝜈𝜈 ++ 2𝑑 𝑑(𝑇))︂
and
𝒪(︁ 𝑇21 log𝑑 2+1(𝑇))︁
for Matérn and squared exponential kernels, the
proposed algorithms achieve significantly faster rates in expectation.
5 Numerical Experiments
This section explores the empirical performance of our methods on three benchmark objective
functions, on hyperparameter tuning for a machine learning model, and on optimizing a black-
box objective function designed to guide engineering decisions. We compare the new algorithms
(GP-UCB+, EXPLOIT+) with GP-UCB and two other popular Bayesian optimization strategies:
Expected Improvement (EI) and Probability of Improvement (PI). We also compare with the
10Figure2Simple regret vs number of noise-free observations.
EXPLOIT approach outlined in Subsection 4.1, but not with EXPLORE as this method did not
achieve competitive performance. Throughout, we choose the distribution 𝑃 which governs random
exploration in the new algorithms to be uniform on 𝒳. For the weight parameter of UCB acquisition
1/2
functions, we considered a well-tuned constant value 𝛽 = 2 that achieves good performance
𝑡
1/2
in our examples, and the approach in [CG19], which sets 𝛽 = max |𝑓(𝑥)| where 𝒳 is a
𝑡 𝑥∈𝒳𝐷 𝐷
discretization of the search space. All the hyperparameters of the kernel function were iteratively
updated through maximum likelihood estimation. Since the new algorithms need two noise-free
observations per iteration but the methods we compare with only need one, we run the new
algorithms for half as many iterations to ensure a fair comparison.
5.1 BenchmarkObjectiveFunctions
We consider three 10-dimensional benchmark objective functions: Ackley, Rastrigin, and Levy. Each
of them has a unique global maximizer but many local optima, posing a challenge to standard
first and second-order convex optimization algorithms. Following the virtual library of simula-
tion experiments: https://www.sfu.ca/ ssurjano/, we respectively set the search space to be
[−32.768,32.768]10,[−5.12,5.12]10, and [−10,10]10. We used a Matérn kernel with the default initial
smoothness parameter 𝜈 = 2.5 and initial lengthscale parameter ℓ = 1. For each method and objec-
tive, we obtain 400 noise-free observations and average the results over 20 independent experiments.
1/2
For GP-UCB and GP-UCB+, we set 𝛽 = 2. Figure 2 shows the average simple regrets, given
𝑡
by 𝑓*−max 𝑓(𝑥 ). We report the regret as a function of the number of observations rather
𝑡=1,...,𝑇 𝑡
than the number of iterations to ensure a fair comparison. For all three benchmark functions,
GP-UCB+ and EXPLOIT+ outperform the other methods. To further demonstrate the strength of
the proposed algorithms, Table 5.1 shows the average simple regret at the last iteration, normalized
so that for each benchmark objective the worst-performing algorithm has unit simple regret. Table
B.1 in Appendix B shows results for the standard deviation, indicating that the new methods are
not only more accurate, but also more precise.
To illustrate the sensitivity of UCB algorithms to the choice of weight parameters, we include
1/2
numerical results with 𝛽 = max |𝑓(𝑥)| in Appendix B. In particular, since GP-UCB+ has
𝑡 𝑥∈𝒳𝐷
an additional exploration step through random sampling, using a smaller weight parameter for
GP-UCB+ than for GP-UCB tends to work more effectively. Remarkably, the parameter-free
EXPLOIT+ algorithm achieves competitive performance compared with UCB algorithms with
well-tuned weight parameters.
11Table5.1 Normalized average simple regret with 400 function evaluations for benchmark objectives in
dimension 𝑑=10.
Method Ackley Rastrigin Levy
GP-UCB+ 0.222 0.576 0.146
GP-UCB 0.583 0.930 0.768
EXPLOIT+ 0.342 0.505 0.126
EXPLOIT 1.000 1.000 1.000
EI 0.832 0.644 0.142
PI 0.891 0.698 0.507
5.2 RandomForestHyperparameterTuning
Here we use Bayesian optimization to tune four hyperparameters of a random forest regression
model for the California housing dataset [PB97]. The parameters of interest are (i) three integer-
valued quantities: the number of trees in the forest, the maximum depth of the tree, and the
minimum number of samples required to split the internal node; and (ii) a real-valued quantity
between zero and one: the transformed maximum number of features to consider when looking
for the best split. For the discrete quantities, instead of optimizing over a discrete search space,
we performed the optimization over a continuous domain and truncated the decimal values when
evaluating the objective function. We split the dataset into training (80%) and testing (20%)
sets. To define a deterministic objective function, we fixed the random state parameter for the
RandomForestRegressor function from the Python scikit-learn package and built the model using
the training set. We then defined our objective function to be the negative mean-squared test
error of the built model. We used a Matérn kernel with initial smoothness parameter 𝜈 = 2.5
and initial lengthscale parameter ℓ = 1. For the GP-UCB and GP-UCB+ algorithms, we set
1/2
𝛽 = max |𝑓(𝑥)| where 𝒳 consists of 40 Latin hypercube samples. We conducted 20
𝑡 𝑥∈𝒳𝐷 𝐷
independent experiments with 80 noise-free observations. From Table 5.2 and Figure 3, we see that
both GP-UCB+ and EXPLOIT+ algorithms led to smaller cumulative test errors. An instantaneous
test error plot with implementation details can be found in Appendix C.
Table5.2 Cumulative test error averaged over 20 experiments.
Method Mean ± SD
GP-UCB+ 28.552±1.971
GP-UCB 35.226±1.467
EXPLOIT+ 26.346±1.404
EXPLOIT 35.026±1.092
EI 34.294±0.987
PI 33.438±1.085
12Figure3Cumulative test error vs number of noise-free observations.
5.3 GardenSprinklerComputerModel
The Garden Sprinkler computer model simulates the range of a garden sprinkler that sprays water.
The model contains eight physical parameters that represent vertical nozzle angle, tangential nozzle
angle, nozzleprofile, diameterofthesprinklerhead, dynamicfrictionmoment, staticfrictionmoment,
entrance pressure, and diameter flow line. First introduced in [SHvB10] and later formulated into
a deterministic black-box model by [Pou20], the goal is to maximize the accessible range of a
garden sprinkler over the domain of the eight-dimensional parameter space. In this problem, the
observations of the objective are noise-free. Following [PL21], for GP-UCB and GP-UCB+ we set
𝛽1/2 = 2 and used a squared exponential kernel with an initial lengthscale parameter ℓ2 = 50. We
𝑡
ran 30 independent experiments, each with 100 noise-free observations. The results in Table 5.3
and Figure 4 demonstrate that the new algorithms achieve competitive performance. In particular,
EXPLOIT+ attains on average the largest maximum value, while also retaining a moderate standard
deviation across experiments.
Table5.3 Maximumattainedvalueofthegardensprinklerobjectivefunctionaveragedover30experiments.
Method Mean ± SD
GP-UCB+ 17.511 ± 1.603
GP-UCB 18.038 ± 2.026
EXPLOIT+ 18.427±1.825
EXPLOIT 17.352 ± 2.537
EI 18.061 ± 1.657
PI 17.105 ± 2.329
6 Conclusion
This paper has introduced two Bayesian optimization algorithms, GP-UCB+ and EXPLOIT+, that
supplement query points obtained via UCB or posterior mean maximization with query points
13Figure4Maximum attained value of the garden sprinkler objective function vs number of noise-free
observations.
obtained via random sampling. The additional sampling step in our algorithms promotes search
space exploration and ensures that the fill-distance of the query points decays at a nearly optimal
rate. From a theoretical viewpoint, we have shown that GP-UCB+ and EXPLOIT+ satisfy regret
bounds that improve upon existing and refined rates for the classical GP-UCB algorithm with
noise-free observations, and nearly match the rates conjectured in [Vak22]. Indeed, at the price of
a higher computational cost, one can obtain the exact rates in [Vak22] by replacing the random
sampling step in GP-UCB+ and EXPLOIT+ with a quasi-uniform sampling scheme. From an
implementation viewpoint, both GP-UCB+ and EXPLOIT+ retain the appealing simplicity of the
GP-UCB algorithm; moreover, EXPLOIT+ does not require specifying input weight parameters.
From an empirical viewpoint, we have demonstrated that the new algorithms outperform existing
ones in a wide range of examples.
References
[BCX+17] Somil Bansal, Roberto Calandra, Ted Xiao, Sergey Levine, and Claire J Tomlin.
Goal-driven dynamics learning via Bayesian optimization. In 2017 IEEE 56th Annual
Conference on Decision and Control (CDC), pages 5168–5173. IEEE, 2017.
[BK21] IlijaBogunovicandAndreasKrause. MisspecifiedGaussianprocessbanditoptimization.
Advances in Neural Information Processing Systems, 34:3004–3015, 2021.
[BS96] Christopher J Burges and Bernhard Schölkopf. Improving the accuracy and speed of
support vector machines. Advances in Neural Information Processing Systems, 9, 1996.
[Bul11] Adam D Bull. Convergence rates of efficient global optimization algorithms. Journal
of Machine Learning Research, 12(10), 2011.
[CG17] Sayak Ray Chowdhury and Aditya Gopalan. On kernelized multi-armed bandits. In
International Conference on Machine Learning, pages 844–853. PMLR, 2017.
14[CG19] Sayak Ray Chowdhury and Aditya Gopalan. Bayesian optimization under heavy-tailed
payoffs. Advances in Neural Information Processing Systems, 32, 2019.
[CJBGP16] DanielLClarkJr, Ha-RokBae, KooroshGobal, andRaviPenmetsa. Engineeringdesign
explorationusinglocallyoptimizedcovariancekriging. AIAA Journal,54(10):3160–3175,
2016.
[DFSZ12] Nando De Freitas, Alex J Smola, and Masrour Zoghi. Exponential regret bounds
for Gaussian process bandits with deterministic observations. In Proceedings of the
29th International Coference on International Conference on Machine Learning, pages
955–962, 2012.
[Fra18] Peter I Frazier. A tutorial on Bayesian optimization. arXiv preprint arXiv:1807.02811,
2018.
[GR06] Subhashis Ghosal and Anindya Roy. Posterior consistency of Gaussian process prior
for nonparametric binary regression. The Annals of Statistics, pages 2413–2429, 2006.
[HSTZ23] Tapio Helin, Andrew M Stuart, Aretha L Teckentrup, and Konstantinos Zygalakis. In-
troductiontoGaussianprocessregressioninBayesianinverseproblems,withnewresults
on experimental design for weighted error measures. arXiv preprint arXiv:2302.04518,
2023.
[JSW98] Donald R Jones, Matthias Schonlau, and William J Welch. Efficient global optimization
of expensive black-box functions. Journal of Global optimization, 13(4):455, 1998.
[KHSS18] Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath K Sriperumbudur.
Gaussian processes and kernel methods: A review on connections and equivalences.
arXiv preprint arXiv:1807.02582, 2018.
[KKSP18] Kirthevasan Kandasamy, Akshay Krishnamurthy, Jeff Schneider, and Barnabás Póczos.
Parallelised Bayesian optimisation via Thompson sampling. InInternational Conference
on Artificial Intelligence and Statistics, pages 133–142. PMLR, 2018.
[KSAY22] Hwanwoo Kim, Daniel Sanz-Alonso, and Ruiyi Yang. Optimization on manifolds via
graph Gaussian processes. arXiv preprint arXiv:2210.10962, 2022.
[LYT19] Yueming Lyu, Yuan Yuan, and Ivor W Tsang. Efficient batch black-box optimization
with deterministic regret bounds. arXiv preprint arXiv:1905.10041, 2019.
[Moc98] JonasMockus. TheapplicationofBayesianmethodsforseekingtheextremum. Towards
Global Optimization, 2:117, 1998.
[OCBG19] Chris J Oates, Jon Cockayne, François-Xavier Briol, and Mark Girolami. Convergence
rates for a class of estimators based on Stein’s method. Bernoulli, 25(2):1141–1159,
2019.
15[PB97] RKelleyPaceandRonaldBarry. Sparsespatialautoregressions. Statistics & Probability
Letters, 33(3):291–297, 1997.
[PL21] Tony Pourmohamad and Herbert KH Lee. Bayesian Optimization with Application to
Computer Experiments. Springer, 2021.
[Pou20] Tony Pourmohamad. Compmodels: Pseudo computer models for optimization. R
package version 0.2. 0, 2020.
[RVR14] Daniel Russo and Benjamin Van Roy. Learning to optimize via information-directed
sampling. Advances in Neural Information Processing Systems, 27, 2014.
[SHvB10] Karl Siebertz, Thomas Hochkirchen, and D van Bebber. Statistische Versuchsplanung.
Springer, 2010.
[SKKS10] Niranjan Srinivas, Andreas Krause, Sham Kakade, and Matthias Seeger. Gaussian
process optimization in the bandit setting: no regret and experimental design. In
Proceedings of the 27th International Conference on Machine Learning, pages 1015–
1022, 2010.
[ST18] Andrew Stuart and Aretha Teckentrup. Posterior consistency for Gaussian process
approximations of Bayesian posterior distributions. Mathematics of Computation,
87(310):721–753, 2018.
[Ste12] Michael L Stein. Interpolation of Spatial Data: Some Theory for Kriging. Springer
Science & Business Media, 2012.
[SVZ23] Sudeep Salgia, Sattar Vakili, and Qing Zhao. Random exploration in Bayesian
Optimization: order-optimal regret and computational efficiency. arXiv preprint
arXiv:2310.15351, 2023.
[SY23] Amit Singer and Ruiyi Yang. Alignment of density maps in Wasserstein distance. arXiv
preprint arXiv:2305.12310, 2023.
[Tec20] Aretha L Teckentrup. Convergence of Gaussian process regression with estimated
hyper-parameters and applications in Bayesian inverse problems. SIAM/ASA Journal
on Uncertainty Quantification, 8(4):1310–1337, 2020.
[TW20] Rui Tuo and Wenjia Wang. Kriging prediction with isotropic Matérn correlations: Ro-
bustness and experimental designs. Journal of Machine Learning Research, 21(1):7604–
7641, 2020.
[Vak22] Sattar Vakili. Open problem: Regret bounds for noise-free kernel-based bandits. In
Conference on Learning Theory, pages 5624–5629. PMLR, 2022.
[VKP21] Sattar Vakili, Kia Khezeli, and Victor Picheny. On information gain and regret bounds
in Gaussian process bandits. In International Conference on Artificial Intelligence and
Statistics, pages 82–90. PMLR, 2021.
16[Wen04] Holger Wendland. Scattered Data Approximation, volume 17. Cambridge University
Press, 2004.
[WR06] Christopher KI Williams and Carl Edward Rasmussen. Gaussian Processes for Machine
Learning, volume 2. MIT Press Cambridge, MA, 2006.
[WS93] Zong-min Wu and Robert Schaback. Local error estimates for radial basis function
interpolation of scattered data. IMA Journal of Numerical Analysis, 13(1):13–27, 1993.
[WSH21] Tizian Wenzel, Gabriele Santin, and Bernard Haasdonk. A novel class of stabilized
greedy kernel approximation algorithms: Convergence, stability and uniform point
distribution. Journal of Approximation Theory, 262:105508, 2021.
[WTJW20] Wenjia Wang, Rui Tuo, and CF Jeff Wu. On prediction properties of kriging: Uni-
form error bounds and robustness. Journal of the American Statistical Association,
115(530):920–930, 2020.
A Proofs
Proof of Theorem 3.1. Let 𝑓* = 𝑓(𝑥*) = max 𝑓(𝑥) and let 𝑟 = 𝑓*−𝑓(𝑥 ) be the instantaneous
𝑥∈𝒳 𝑡 𝑡
regret. Then,
𝑟 = 𝑓*−𝜇 (𝑥*)+𝜇 (𝑥*)−𝜇 (𝑥 )+𝜇 (𝑥 )−𝑓(𝑥 )
𝑡 𝑡−1,0 𝑡−1,0 𝑡−1,0 𝑡 𝑡−1,0 𝑡 𝑡
(i)
≤ ‖𝑓‖ 𝜎 (𝑥*)+𝜇 (𝑥*)−𝜇 (𝑥 )+‖𝑓‖ 𝜎 (𝑥 )
ℋ (𝒳) 𝑡−1,0 𝑡−1,0 𝑡−1,0 𝑡 ℋ (𝒳) 𝑡−1,0 𝑡
𝑘 𝑘 (A.1)
(ii)
≤ ‖𝑓‖ 𝜎 (𝑥 )+𝜇 (𝑥 )−𝜇 (𝑥 )+‖𝑓‖ 𝜎 (𝑥 )
ℋ (𝒳) 𝑡−1,0 𝑡 𝑡−1,0 𝑡 𝑡−1,0 𝑡 ℋ (𝒳) 𝑡−1,0 𝑡
𝑘 𝑘
= 2‖𝑓‖ 𝜎 (𝑥 ),
ℋ (𝒳) 𝑡−1,0 𝑡
𝑘
where for (i) we use twice that, for any 𝑥 ∈ 𝒳, it holds that |𝑓(𝑥)−𝜇 (𝑥)| ≤ ‖𝑓‖ 𝜎 (𝑥)
𝑡−1,0 ℋ (𝒳) 𝑡−1,0
𝑘
—seeforinstanceCorollary3.11in[KHSS18]—andfor(ii)weusethedefinitionof𝑥 intheGP-UCB
𝑡
algorithm. Thus, for any 𝜆 > 0,
𝑅2
( ≤i)
𝑇
∑︁𝑇
𝑟2
( ≤ii)
4𝑇‖𝑓‖2
∑︁𝑇
𝜎2 (𝑥 )
( ≤iii)
4𝑇‖𝑓‖2
∑︁𝑇
𝜎2 (𝑥 ),
𝑇 𝑡 ℋ (𝒳) 𝑡−1,0 𝑡 ℋ (𝒳) 𝑡−1,𝜆 𝑡
𝑘 𝑘
𝑡=1 𝑡=1 𝑡=1
where (i) follows by the Cauchy-Schwarz inequality, (ii) from the bound on 𝑟 , and (iii) from the
𝑡
fact that 𝜎 (𝑥 ) ≤ 𝜎 (𝑥 ) for any 𝜆 > 0. Since the function 𝑥 is strictly increasing
𝑡−1,0 𝑡 𝑡−1,𝜆 𝑡 log(1+𝑥)
in 𝑥 and for the squared exponential kernel it holds that 𝜆−1𝜎2 (𝑥 ) ≤ 𝜆−1, we have that
𝑡−1,𝜆 𝑡
𝜆−1𝜎2 (𝑥 ) ≤ 𝜆−1 log(︀ 1+𝜆−1𝜎2 (𝑥 ))︀ . Therefore,
𝑡−1,𝜆 𝑡 log(1+𝜆−1) 𝑡−1,𝜆 𝑡
𝑅2 ≤ 8𝑇‖𝑓‖2 ℋ 𝑘(𝒳)(︃ 1 ∑︁𝑇 log(︁ 1+𝜆−1𝜎2 (𝑥 ))︁)︃ ≤ 8𝑇‖𝑓‖2 ℋ 𝑘(𝒳) 𝛾 , (A.2)
𝑇 log(1+𝜆−1) 2 𝑡−1,𝜆 𝑡 log(1+𝜆−1) 𝑇,𝜆
𝑡=1
17where the last inequality follows from Lemma 5.3 in [SKKS10]. Since (A.2) holds for any 𝜆 > 0, by
plugging 𝜆 = 𝑇−𝛼, for some 𝛼 > 0, we conclude that
8𝑇‖𝑓‖2
𝑅2 ≤ ℋ 𝑘(𝒳) 𝛾 . (A.3)
𝑇 log(1+𝑇𝛼) 𝑇,𝑇−𝛼
For squared exponential kernels, Corollary 1 in [VKP21] implies that
(︂(︁ )︁𝑑 )︂ (︁ )︁ (︁ )︁
𝛾 ≤ 2(1+𝛼)log𝑇 +𝐶˜(𝑑) +1 log 1+𝑇1+𝛼 ≲ log𝑑(𝑇)log 1+𝑇1+𝛼 ,
𝑇,𝑇−𝛼
where 𝐶˜(𝑑) = 𝒪(𝑑log𝑑) is independent of 𝑇 and 𝜆. Hence, using that log(1+𝑇𝛼+1) ≤ 𝛼+1 for
log(1+𝑇𝛼) 𝛼
𝛼 > 0,𝑇 ≥ 1, we obtain
log(1+𝑇1+𝛼)
𝑅2 ≲ 𝑇 log𝑑(𝑇) ≲ 𝑇 log𝑑(𝑇),
𝑇 log(1+𝑇𝛼)
concluding the proof.
Proof of Theorem 4.4. We first prove the cumulative regret bound for GP-UCB+. As in (A.1), one
can show that
𝑟 ≤ 2‖𝑓‖ 𝜎full (𝑥 ) ≤ 2‖𝑓‖ sup𝜎full (𝑥).
𝑡 ℋ 𝑘(𝒳) 𝑡−1,0 𝑡 ℋ 𝑘(𝒳) 𝑡−1,0
𝑥∈𝒳
For Matérn kernels, [WS93] shows that sup 𝜎full (𝑥) ≤ ℎ(𝒳,𝑋full)𝜈 —see also Lemma 2 in
𝑥∈𝒳 𝑡−1,0 𝑡
[WTJW20]. Moreover, we have the trivial bound ℎ(𝒳,𝑋full) ≤ ℎ (𝒳) := sup inf ‖𝑥−
𝑡 𝑡 𝑥∈𝒳 𝑥˜𝑖∈{𝑥˜1,...,𝑥˜𝑡}
𝑥˜ ‖. Hence, for any 𝜖 > 0,
𝑖
E 𝑃[𝑅 𝑇] ≲ ∑︁ supE 𝑃[︁ 𝜎 𝑡fu −l 1l ,0(𝑥)]︁ ≲ ∑︁ supE 𝑃[︁ ℎ 𝑡(𝒳)𝜈]︁ ( ≲⋆) ∑︁𝑇 𝑡−𝜈 𝑑+𝜖 ≲ 𝑇𝑑− 𝑑𝜈+𝜖 ,
𝑡=1𝑥∈𝒳 𝑡=1𝑥∈𝒳 𝑡=1
where (⋆) follows from Proposition 4 in [HSTZ23] —see also Lemma 2 in [OCBG19].
For squared exponential kernels, Theorem 11.22 in [Wen04] shows that, for some 𝑐 > 0,
sup 𝜎full (𝑥) ≤ exp(︀ −𝑐/ℎ(𝒳,𝑋full))︀ . Hence, for any 𝜖 ≤ 1 ,
𝑥∈𝒳 𝑡−1,0 𝑡 2𝑑
E 𝑃[𝑅 𝑇] ≲ ∑︁𝑇 supE 𝑃[︁ 𝜎 𝑡fu −l 1l ,0(𝑥)]︁ ≲ ∑︁𝑇 E 𝑃[︁ exp(︀ −𝑐/ℎ 𝑡(𝒳))︀]︁ ( ≲⋆) ∑︁𝑇 exp(︁ −𝑐𝑡𝑑1−𝜖)︁ ≲ ∫︁ ∞ exp(︁ −𝑐𝑡21 𝑑)︁ 𝑑𝑡,
𝑡=1𝑥∈𝒳 𝑡=1 𝑡=1 0
where (⋆) follows from Proposition 4 in [HSTZ23] —see also Lemma 2 in [OCBG19]. Since
∫︀ 0∞exp(︁ −𝑐𝑡21 𝑑)︁ 𝑑𝑡 = 𝑐2 2𝑑 𝑑Γ(2𝑑) < ∞, we conclude that E 𝑃[𝑅 𝑇] = 𝒪(1).
18For the EXPLOIT+ algorithm, we have that
𝑟 = 𝑓*−𝑓(𝑥 )
𝑡 𝑡
= 𝑓*−𝜇full (𝑥*)+𝜇full (𝑥*)−𝜇full (𝑥 )+𝜇full (𝑥 )−𝑓(𝑥 )
𝑡−1,0 𝑡−1,0 𝑡−1,0 𝑡 𝑡−1,0 𝑡 𝑡
(i)
≤ ‖𝑓‖ 𝜎full (𝑥*)+𝜇full (𝑥*)−𝜇full (𝑥 )+‖𝑓‖ 𝜎full (𝑥 )
ℋ 𝑘 𝑡−1,0 𝑡−1,0 𝑡−1,0 𝑡 ℋ 𝑘 𝑡−1,0 𝑡
(ii)
≤ ‖𝑓‖ 𝜎full (𝑥*)+‖𝑓‖ 𝜎full (𝑥 )
ℋ 𝑘(𝒳) 𝑡−1,0 ℋ 𝑘(𝒳) 𝑡−1,0 𝑡
≤ 2‖𝑓‖ sup𝜎full (𝑥),
ℋ 𝑘(𝒳) 𝑡−1,0
𝑥∈𝒳
where for (i) we use twice that, for any 𝑥 ∈ 𝒳, it holds that |𝑓(𝑥)−𝜇 (𝑥)| ≤ ‖𝑓‖ 𝜎 (𝑥),
𝑡−1,0 ℋ (𝒳) 𝑡−1,0
𝑘
and for (ii) we use the definition of 𝑥 in the EXPLOIT+ algorithm. The rest of the proof proceeds
𝑡
exactly as the one for GP-UCB+, and we hence omit the details.
Proof of Theorem 4.8. By Lemma 5.5 in [SKKS10], the choice of 𝛽 in the statement of Theorem
𝑡
4.8 ensures that, with probability at least 1−𝛿/2,
1
|𝑓(𝑥 )−𝜇 (𝑥 )| ≤ 𝛽2𝜎 (𝑥 ), for all 𝑡 ≥ 1.
𝑡 𝑡−1,0 𝑡 𝑡 𝑡−1,0 𝑡
Furthermore, Lemma 5.7 in [SKKS10] implies that, with probability at least 1−𝛿/2,
1 1
|𝑓(𝑥*)−𝜇 ([𝑥*] )| ≤ 𝛽2𝜎 ([𝑥*] )+ , for all 𝑡 ≥ 1,
𝑡−1,0 𝑡 𝑡 𝑡−1,0 𝑡 𝑡2
where[𝑥*] isthepoint𝑥in𝒟 closestto𝑥*and𝒟 isadiscretizationofsize𝜏𝑑 := (2𝑟𝑑𝑡2𝑏√︀ log(2𝑑𝑎/𝛿))𝑑
𝑡 𝑡 𝑡 𝑡
inside [−𝑟,𝑟]𝑑 satisfying that, for all 𝑥 ∈ 𝒟 , ‖𝑥−[𝑥] ‖ ≤ 2𝑟𝑑/𝜏 .
𝑡 𝑡 1 𝑡
For the GP-UCB+ algorithm,
𝑟 = 𝑓(𝑥*)−𝑓(𝑥 )
𝑡 𝑡
(i) 1 1
≤ 𝜇 ([𝑥*] )+𝛽2𝜎 ([𝑥*] )+ −𝑓(𝑥 )
𝑡−1,0 𝑡 𝑡 𝑡−1,0 𝑡 𝑡2 𝑡
(ii) 1 1
≤ 𝜇 (𝑥 )+𝛽2𝜎 (𝑥 )+ −𝑓(𝑥 )
𝑡−1,0 𝑡 𝑡 𝑡−1,0 𝑡 𝑡2 𝑡
(iii) 1 1
≤ 2𝛽2𝜎 (𝑥 )+ ,
𝑡 𝑡−1,0 𝑡 𝑡2
1 1
≤ 2𝛽2 sup𝜎 (𝑥)+ ,
𝑡 𝑡−1,0 𝑡2
𝑥∈𝒳
where (i) follows from Lemma 5.7 in [SKKS10], (ii) from the definition of 𝑥 in the GP-UCB+
𝑡
algorithm, and (iii) from Lemma 5.5 in [SKKS10]. Therefore, with probability at least 1−𝛿,
1 1
𝑟 ≤ 2𝛽2 sup𝜎 (𝑥)+ , for all 𝑡 ≥ 1,
𝑡 𝑡 𝑡−1,0 𝑡2
𝑥∈𝒳
19from which we deduce that
1 ∑︁𝑇 𝜋2
𝑅 ≤ 2𝛽2 sup𝜎 (𝑥)+ .
𝑇 𝑇 𝑡−1,0 6
𝑡=1𝑥∈𝒳
Using the same argument as in the proof of Theorem 4.4 and the choice of 𝛽 , we obtain the desired
𝑡
result.
For the EXPLOIT+ algorithm,
𝑟 = 𝑓(𝑥*)−𝑓(𝑥 )
𝑡 𝑡
(i) 1 1
≤ 𝜇 ([𝑥*] )+𝛽2𝜎 ([𝑥*] )+ −𝑓(𝑥 )
𝑡−1,0 𝑡 𝑡 𝑡−1,0 𝑡 𝑡2 𝑡
(ii) 1 1
≤ 𝜇 (𝑥 )+𝛽2𝜎 ([𝑥*] )+ −𝑓(𝑥 )
𝑡−1,0 𝑡 𝑡 𝑡−1,0 𝑡 𝑡2 𝑡
(iii) 1 1 1
≤ 𝛽2𝜎 (𝑥 )+𝛽2𝜎 ([𝑥*] )+ ,
𝑡 𝑡−1,0 𝑡 𝑡 𝑡−1,0 𝑡 𝑡2
1 1
≤ 2𝛽2 sup𝜎 (𝑥)+ ,
𝑡 𝑡−1,0 𝑡2
𝑥∈𝒳
where (i) follows from Lemma 5.7 in [SKKS10], (ii) from the definition of 𝑥 in the EXPLOIT+
𝑡
algorithm, and (iii) from Lemma 5.5 in [SKKS10]. The rest of the proof proceeds exactly as the one
for GP-UCB+, and we hence omit the details.
B Additional Experiments and Implementation Details: Benchmark Functions
This appendix provides detailed descriptions of the numerical experiments conducted in Section 5.1.
The functional forms of the three objective functions we considered and their respective search space
are provided below. For all three benchmark functions we denote 𝑥 = (𝑥1,...,𝑥𝑑) and set 𝑑 = 10.
• Ackley function:
𝑓(𝑥) =
−20exp⎛ ⎝−1⎯ ⎸ ⎸ ⎷1 ∑︁𝑑 (𝑥𝑖)2⎞ ⎠−exp(︃ 1 ∑︁𝑑 cos(2𝜋𝑥𝑖))︃
+20+exp(1), 𝑥 ∈ [−32.768,32.768]𝑑.
5 𝑑 𝑑
𝑖=1 𝑖=1
• Rastrigin function:
𝑑
𝑓(𝑥) = 10𝑑+∑︁ [(𝑥𝑖)2−10cos(2𝜋𝑥𝑖)], 𝑥 ∈ [−5.12,5.12]𝑑.
𝑖=1
• Levy function: With 𝜔 = 1+
𝑥𝑖−1,
for all 𝑖 ∈ {1,...,𝑑}
𝑖 4
𝑑−1
𝑓(𝑥) = sin2(𝜋𝜔 )+∑︁ (𝜔 −1)2[1+10sin2(𝜋𝜔 +1)]+(𝜔 −1)2[1+sin2(2𝜋𝜔 )], 𝑥 ∈ [−10,10]𝑑.
1 𝑖 𝑖 𝑑 𝑑
𝑖=1
20TableB.1 Normalized average standard deviation of simple regret with 400 function evaluations for
different benchmark objectives in dimension 𝑑=10.
Method Ackley Rastrigin Levy
GP-UCB+ 0.075 0.797 0.131
GP-UCB 1.000 1.000 0.719
EXPLOIT+ 0.306 0.577 0.127
EXPLOIT 0.733 0.976 1.000
EI 0.466 0.609 0.160
PI 0.329 0.360 0.312
Recall that Figure 2 portrayed the average simple regret of the six Bayesian optimization
strategies we consider: GP-UCB+ (proposed algorithm), GP-UCB ([SKKS10]) (both with the
choice of 𝛽 = 2), EXPLOIT+ (proposed algorithm), EXPLOIT (GP-UCB with 𝛽 = 0), EI
𝑡 𝑡
(Expected Improvement), and PI (Probability of Improvement). The simple regret values at the last
iteration were displayed in Table 5.1. Furthermore, Table B.1 shows the standard deviations of the
last simple regret values over 20 independent experiments. From Figure 2 and Table B.1, one can
see that not only were the proposed methods (GP-UCB+ and EXPLOIT+) able to yield superior
simple regret performance, but also their standard deviations were substantially smaller than those
of the other methods, indicating superior stability.
Additionally, Figure 5 shows the cumulative regret for GP-UCB+ and GP-UCB with different
1/2
choices of 𝛽 . All results were averaged over 20 independent experiments. We considered 𝛽 = 2
𝑡 𝑡
1/2
and 𝛽 = max |𝑓(𝑥)| where 𝒳 is a set of 100 Latin hypercube samples. In all experiments,
𝑡 𝑥∈𝒳𝐷 𝐷
max |𝑓(𝑥)| was significantly larger than 2. Figure 5 demonstrates that the choice of 𝛽 can
𝑥∈𝒳𝐷 𝑡
significantly influence the cumulative regret. In particular, we have observed that the GP-UCB+
algorithmtendstoworkbetterwithsmaller𝛽 values,asthealgorithmcontainsadditionalexploration
𝑡
steps through random sampling; this behavior can also be seen in Figure 5. In all three benchmark
functions, GP-UCB exhibits sensitivity to the choice of parameter 𝛽 ; in contrast, our EXPLOIT+
𝑡
algorithm does not require specifying weight parameters and consistently achieves competitive or
improved performance across all our experiments.
Figure5Simple regret plots for benchmark functions with 𝛽 𝑡1/2 =2 (TWO) and 𝛽 𝑡1/2 =max 𝑥∈𝒳𝐷|𝑓(𝑥)|
(SUP).
21C Additional Figures and Implementation Details: Hyperparameter Tuning
To train the random forest regression model for California housing dataset [PB97], we first divided
the dataset into test and train datasets. 80 percent of (feature vector, response) pairs were
assigned to be the training set, while the remaining 20 percent were treated as a test set. In
constructing the deterministic objective function, we defined it to be a mapping from the vector of
four hyperparameters to a negative test error of the model built based on the input and training
set. As the model construction may involve randomness coming from the bootstrapped samples, we
fixed the random state parameter to remove any such randomness in the definition of the objective.
We tuned the following four hyperparameters:
• Number of trees in the forest ∈ [10,200].
• Maximum depth of the tree ∈ [1,20].
• Minimum number of samples requires to split the internal node ∈ [2,10].
• Maximum proportion of the number of features to consider when looking for the best split
∈ [0.1,0.999].
For the first three parameters we conducted the optimization task in the continuous domain and
rounded down to the nearest integers. Figure 6 shows that the proposed algorithms attained smaller
cumulative and instantaneous test errors.
Figure6Test errors vs number of noise-free observations.
D Implementation Details: Garden Sprinkler Computer Model
For the Garden Sprinkler computer model, the eight-dimensional search space we considered was
given by:
• Vertical nozzle angle ∈ [0,90].
• Tangential nozzle angle ∈ [0,90].
• Nozzle profile ∈ [2×10−6,4×10−6].
22• Diameter of the sprinkler head ∈ [0.1,0.2].
• Dynamic friction moment ∈ [0.01,0.02].
• Static friction moment ∈ [0.01,0.02].
• Entrance pressure ∈ [1,2].
• Diameter flow line ∈ [5,10].
23