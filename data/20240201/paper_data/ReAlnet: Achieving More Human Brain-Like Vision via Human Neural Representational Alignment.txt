ReAlnet: Achieving More Human Brain-Like Vision
via Human Neural Representational Alignment
ZitongLu1 YileWang2 JulieD.Golomb1
Abstract
Despite the remarkable strides made in artifi-
cialintelligence,currentobjectrecognitionmod-
els still lag behind in emulating the mecha-
nism of visual information processing in hu-
manbrains. Recentstudieshavehighlightedthe
potential of using neural data to mimic brain
processing; however, these often reply on in-
vasive neural recordings from non-human sub-
jects, leaving a critical gap in our understand-
ing of human visual perception and the devel-
opment of more human brain-like vision mod-
els. Addressing this gap, we present, for the
firsttime,‘Re(presentational)Al(ignment)net’,a
visionmodelalignedwithhumanbrainactivity
basedonnon-invasiveEEGrecordings, demon-
stratingasignificantlyhighersimilaritytohuman
brainrepresentations. Ourinnovativeimage-to-
brainmulti-layerencodingalignmentframework
notonlyoptimizesmultiplelayersofthemodel,
marking a substantial leap in neural alignment,
butalsoenablesthemodeltoefficientlylearnand
mimichumanbrain’svisualrepresentationalpat-
ternsacrossobjectcategoriesanddifferentneural
data modalities. Furthermore, we discover that
alignmentwithhumanbrainrepresentationsim-
proves the model’s adversarial robustness. Our
Figure1.ReAlnetalignedwithhumanneuralsignalsasamore
findings suggest that ReAlnet sets a new prece-
human brain-like vision model. (A) An overview of ReAlnet
dent in the field, bridging the gap between arti-
alignmentframework.Addinganadditionalmulti-layerencoding
ficialandhumanvision,andpavingthewayfor
moduletoanImageNetpre-trainedCORnet-S,theoutputscontain
morebrain-likeartificialintelligencesystems. thecategoryclassificationresultsandthegeneratedEEGsignals.
UsingtrainingEEGdata,weaimtominimizebothclassification
lossandgenerationloss,enablingCORnettonotonlystabilize
1.Introduction theclassificationperformancebutalsoeffectivelylearnhuman
brainfeaturesandtransformintoReAlnet. (B)UsingtestEEG
Whilecurrentvisionmodelsinartificialintelligence(AI)are data,wemeasuretherepresentationalsimilaritybetweenthemodel
advanced,theystillfallshortofcapturingthefullcomplexity RDMandtimepoint-by-timepointEEGneuralRDMsforearlyand
andadaptabilityinherentinthehumanbrain’sinformation latelayersinReAlnet,CORnet-S,ResNet-101,andCLIP(witha
ResNet-101backbone)respectively(earlylayer:thefirstlayer;late
1DepartmentofPsychology,TheOhioStateUniversity,Colum-
layer:thelayerbeforetheclassificationlayerinReAlnet,CORnet,
bus,OH,USA2DepartmentofNeuroscience,SchoolofBehavioral
andResNet,andthelastvisuallayerinCLIP),andReAlnetshows
andBrainSciences,TheUniversityofTexasatDallas,Dallas,TX,
thehighestsimilaritytothehumanbrain.
USA.Correspondenceto:ZitongLu<lu.2637@osu.edu>.
1
4202
naJ
03
]VC.sc[
1v13271.1042:viXraReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
processing. Deepconvolutionalneuralnetworks(DCNNs) pecially deep learning models. The earliest attempt was
havereachedaperformancelevelinobjectrecognitionthat to apply human fMRI signals to amend the classification
rivals human capabilities (Lecun et al., 2015), and many boundary of SVMs and CNNs to achieve better category
studieshaveidentifiedsimilaritiesinthehierarchicalstruc- classificationperformance(Fongetal.,2018). Somemore
turebetweenDCNNsandtheventralvisualstream(Cichy recentstudiesstartedtoletthemodelslearnneuralrepre-
etal.,2016;Gu¨c¸lu¨ &vanGerven,2015;Kietzmannetal., sentations. Onecommonwayistoaddasimilaritylossto
2019;Lu&Golomb,2023a;Yaminsetal.,2014). However, increasetherepresentationalsimilaritybetweenmodelsand
the alignment between DCNNs and human neural repre- neuralactivity(neuralrecordingsfrommouseV1,monkey
sentationsremainsdeeplyinadequate, whethercompared V1orIT)duringthetraining(Dapelloetal.,2023;Federer
with human electroencephalography (EEG) or functional et al., 2020; Li et al., 2019; Pirlot et al., 2022). Another
magneticresonanceimaging(fMRI)data. Enhancingthe strategyfrom(Safaranietal.,2021)istoaddanadditional
resemblancebetweenvisualmodelsandthehumanbrain taskbasedonanencodingmoduletopredictmonkeyV1
hasbecomeacriticalconcernforbothcomputerscientists neuralactivity.Bothsimilarity-basedmethodandmulti-task
andneuroscientists. Fromacomputervisionperspective, frameworkcanachievemorebrain-likerepresentationsand
brain-inspiredmodelsoftenexhibithigherrobustnessand improve model robustness. However, these neural align-
generalization, crucial for realizing true brain-like intelli- mentstudieshavetwokeychallenges: (a)Dependenceon
gence;meanwhile,fromacognitiveneuroscienceperspec- animalinsteadofhumanneuralactivity. Thislimitsthedi-
tive,modelsthatmorecloselymirrorbrainrepresentations rectapplicabilityandrelevanceoffindingstohumanvisual
cansignificantlyaidinourexplorationofthebrain’svisual processing,anditishardertoenablemodelstoeffectively
processingmechanisms. learnthehumanbrain’srepresentationalpatternsbasedon
thelowdataquality. (b)Singlebrainregionorsinglemodel
Giventhesechallengesandlimitations,thepivotalquestion
layeralignment. Ontheonehand,previousstudiescould
arisesishowwecanleverageourunderstandingofthe
onlyalignasingleearlyorlatelayerinCNNand/oralign
humanbraintoenhancecurrentAIvisionmodels. Con-
themodelwithacertainbrainregion,V1orIT.Ontheother
ventionalapproacheshavelimitationsinemulatingthecom-
hand,itremainsunclearwhichspecificbrainregionshould
plexityofthehumanbrain’svisualinformationprocessing,
alignwithwhichparticularlayerofthemodel,leadingto
evenwithincreasedmodeldepthandlayers(Rajalingham
potentialmisalignmentandinaccuracies.
etal.,2018). Thislimitationhaspromptedtheexploration
of new methodologies. Researchers have attempted vari- Additionally, a recent study focused on video emotion
ousstrategies,includingalteringthemodel’sarchitecture recognitionfirstappliedarepresentationalsimilarity-based
(addingrecurrentstructures(Karetal.,2019; Kietzmann methodtoalignCNNwithhumanfMRIactivity(Fuetal.,
etal.,2019;Kubiliusetal.,2019;Spoereretal.,2017;Tang 2023). However,itisnoteworthythattheyfocusedonsim-
etal.,2018),dual-pathwaymodels(Baietal.,2017;Choi pleremotionrecognitiontasks,mayfallshortinthemore
etal.,2023;Han&Sereno,2022;2023;Sunetal.,2017), complexanddiversedomainofobjectrecognitionwhich
topographicconstraints(Finzietal.,2022;Leeetal.,2020; haslargerspaceandmultitudeofobjectcategories. There-
Luetal.,2023;Margalitetal.,2023)orfeedbackpathways fore,ourworkaddressedthisbyemployinganadditional
(Konkle&Alvarez,2023))andchangingthetrainingtask encoding module that goes beyond mere similarity. This
(using self-supervised training (Konkle & Alvarez, 2022; modulepredictshumanneuralactivityandistrainedtoau-
Prince et al., 2023) or 3D task models (O’Connell et al., tonomouslyextractcomplexvisualfeatures,offeringamore
2023)). However,limitedstudieshavefocusedondirectly effectiveapproachforaligningthemodelwithhumanneural
using neural responses to complex visual information as representationsinobjectrecognition.
feedbacktoimprovethemodel’ssimilaritytohumanbrains.
Contributions. TobridgethegapbetweenAIvisionand
Our research focuses on a third approach – utilizing hu-
humanvision,weproposeamorehumanbrain-likevision
manbrainneuralactivitydatatorealizebrain-likemodels.
model,ReAlnet,effectivelyalignedwithhumanbrainrep-
Thisapproachrepresentsamoredirectalignmentstrategy
resentationsbasedonanovelandeffectiveencoding-based
between models and the human brain, unconstrained by
multi-layeralignmentframework. Wesummarizeourcon-
variationsinmodelstructureorpre-trainingmethods,po-
tributionsandnovelfindingsasfollows:
tentiallymarkingacrucialsteptowardsachievinggreater
resemblancetothehumanbrain. Thus,ourcentralresearch
questionemerges: Canweusehumanbrainactivityto
alignANNsonobjectrecognitionandachievemorehu- • Tothebestofourknowledge,wearethefirsttodirectly
manbrain-likevisionmodels? alignobjectrecognitionmodelsusingnon-invasiveneu-
raldatarecordedfromhumanbrains,whichopensnew
Related Work. Several previous studies have already
possibilitiesforenhancingbrain-likerepresentations
startedtotrytoapplyneuraldatatomachinelearninges-
inmodelsbasedonhumanbrainactivity.
2ReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
• We propose a novel image-to-brain encoding-based Itisworthnotingthatthetrainingandtestsetsdonotoverlap
representationalalignmentframeworkthatoptimizes intermsofobjectcategories(concepts),whichmeansthat
multiplelayersofthenetworksimultaneously,which theperformanceofReAlnettrainedonthetrainingset,when
effectivelyimprovesthemodel’ssimilaritytohuman evaluatedonthetestset,caneffectivelyrevealthemodel’s
brainrepresentationsacrossdifferentmodalities(both generalizationcapabilityacrossdifferentobjectcategories.
humanEEGandfMRI).
2.2.HumanfMRIdataforcross-modalitytesting
• Ourrepresentationalalignmentframeworkallowsus
toobtainapersonalizedvisionmodelbyaligningwith Todemonstratethatourapproachofaligningwithhuman
individual’sneuraldata. EEG not only enhances the model’s similarity to human
EEGbutindicatesthatReAlnethaseffectivelylearnedthe
• Aligning with human neural representations can im-
humanbrain’srepresentationalpatternsmorebroadly,we
provethemodeladversarialrobustness.
alsoperformedcross-modaltesting,testingReAlnetondata
from a different modality (fMRI), from a different set of
2.Re(-presentational)Al(-ignment)net subjects,viewingadifferentsetofimages. ThefMRIdata
originatefrom(Shenetal.,2019). ThisShenfMRIdataset
Herewedescribethehumanneuraldata(EEGdataforthe
recorded human brain fMRI signals from three subjects
alignment,andbothEEGandfMRIdatafortestingthesim-
whiletheyfocusedonthecenterofthescreenviewingnat-
ilaritybetweenmodelsandhumanbrains)weusedinthis
ural images sourced from ImageNet. We selected he test
study,thealignmentpipeline(includingthestructure,the
setfromShenfMRIdataset,whichcomprisesfMRIsignals
lossfunctions,andtrainingandtestmethods)foraligning
ofeachsubjectviewing50imagesofdifferentcategories,
CORnetrepresentationswithhumanneuralrepresentations
witheachimagebeingviewed24times. Weaveragedthe
toobtainReAlnet,andtheevaluationmethodsformeasur-
fMRIsignalsacrossthe24repeatedtrialstoobtainmoresta-
ingrepresentationalsimilaritybetweenmodelsandhuman
blebrainactivityforeachimageobservationandextracted
brainsandadversarialrobustness.
signalsfromfiveregions-of-interest(ROIs)forsubsequent
comparisonofmodelandhumanfMRIsimilarity: V1,V2,
2.1.HumanEEGdataforrepresentationalalignment
V3,V4,andthelateraloccipitalcomplex(LOC).
HumanEEGdatawereobtainedfromanEEGopendataset,
THINGSEEG2(Giffordetal.,2022),includingEEGdata 2.3.Image-to-brainencoding-basedalignmentpipeline
from 10 healthy human subjects in a rapid serial visual
BasicarchitectureofReAlnet: Wehavechosenthestate-
presentation(RSVP)paradigm. Stimuliwereimagessized
of-the-art CORnet-S model (Kubilius et al., 2018; 2019)
500×500pixelsfromTHINGSdataset(Hebartetal.,2019),
asthefoundationalarchitectureforReAlnet,incorporating
whichconsistsofimagesofobjectsonanaturalbackground
recurrentconnectionsakintothoseinthebiologicalvisual
from1854differentobjectconcepts. Beforeimputingthe
systemandproventomorecloselyemulatethebrain’svisual
images to the model, we reshaped image sizes to 224 ×
processing.BothCORnetandReAlnetconsistoffourvisual
224 pixels and normalized the pixel values of images to
layers(V1,V2,V4,andIT)andacategorydecoderlayer.
ImageNet statistics. Subjects viewed one image per trial
LayerV1performsa7×7convolutionwithastrideof2,
(100ms). Each participant completed 66160 training set
followed by a 3 × 3 max pooling with a stride of 2, and
trials (1654 object concepts × 10 images per concept ×
another 3 × 3 convolution. Layer V2, V4, and IT each
4 trials per image) and 16000 test set trials (200 object
perform two 1 × 1 convolutions, a bottleneck-style 3 ×
concepts×1imageperconcept×80trials).
3convolutionwithastrideof2,anda1×1convolution.
EEGdatawerecollectedusinga64-channelEASYCAPand ApartfromtheinitialLayerV1,theotherthreevisuallayers
a BrainVision actiCHamp amplifier. We use already pre- includerecurrentconnections,allowingoutputsofacertain
processeddatafrom17channels(O1,Oz,O2,PO7,PO3, layer to be passed through the same layer several times
POz,PO4,PO8,P7,P5,P3,P1,Pz,P2)overlyingoccipital (twiceinLayerV2andIT,andfourtimesinLayerV4).
andparietalcortex. Were-epochedEEGdatarangingfrom
EEG generation module: In addition to the original re-
stimulusonsetto200msafteronsetwithasamplefrequency
currentCNNstructure,wehaveaddedanEEGgeneration
of100Hz. Thus,theshapeofourEEGdatamatrixforeach
moduledesignedtoconstructanimage-to-brainencoding
trial is 17 channels × 20 time points. and we reshaped
modelforgeneratingrealistichumanEEGsignals. Eachvi-
the EEG data as a vector including 340 values for each
suallayerisconnectedtoanonlinearN×128layer-encoder
trial. Beforethemodeltrainingandtest,weaveragedallthe
(Enc-V1,Enc-V2,Enc-V4,andEnc-ITcorrespondtoLayer
repeatedtrials(4trialsperimageinthetrainingsetand80
V1,V2,V4,andIT)thatprocessesthroughafullyconnected
trialsperimageinthetestset)toobtainmorestableEEG
networkwithaReLUactivation. Thesefourlayer-encoders
signals.
3ReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
N
arethendirectlyconcatenatedtoformanN ×512Multi- LCont = 1 (cid:88) [1−p(S ,Sˆ)]
LayerVisualEncoder,whichissubsequentlyconnectedto N i i
i=1
anN ×340EEGencoderthroughalinearlayertogenerate (5)
N N
thepredictedEEGsignals. HereN isthebatchsize. − 1 (cid:88) (cid:88) [1−p(S ,Sˆ)]
N(N −1) i j
Therefore, we aim for the model to not only perform the i=1j=1,j̸=i
objectclassificationtaskbutalsotogeneratehumanEEG
signalswhichcanbehighlysimilartotherealEEGsignals Here,S iandSˆ irepresentthegeneratedandrealEEGsignals
when a person views the certain image through the EEG correspondingtothei-thimage.
generationmodulewithaseriesofencoders. Duringthis
Training procedures: Unlike CORnet which trained on
processofgeneratingbrainactivity,ReAlnet’svisuallayers
thesameImageNetdataset,ReAlnetadditionallytrainedon
arepoisedtoeffectivelyextractfeaturesmorealignedwith
individualEEGdataconsistsof10personalizedReAlnets,
neuralrepresentations.
1 per EEG subjects. Each network were trained to mini-
AlignmentLoss: Accordingly,thetraininglossLAofour mize the alignment loss including both classification and
alignmentframeworkconsistsoftwoprimarylosses,aclas- generationlosseswithastatictrainingrateof0.00002for
sificationlossandagenerationlosswithaparameterβ that 30epochsusingtheAdamoptimizer. Weusedabatchsize
determinestherelativeweighting: of16, meaningthecontrastivelosscomputeddissimilari-
ties of 256 pairs for each gradient step. Also, we trained
variousReAlnetsusingfourdifferentβ weights(β =1,10,
LA =LC +β·LG (1) 100,1000)separately. Intotal,wetrained40ReAlnets(4β
weights×10subjects).
LC representsthestandardcategoricalcrossentropyloss
Representationalsimilarityanalysis(RSA):RSAisused
formodelpredictionsonImageNetlabels:
forrepresentationalcomparisonsbetweenmodelsandhu-
manbrains(Kriegeskorteetal.,2008)basedonfirstcomput-
ingrepresentationaldissimilaritymatrices(RDMs)formod-
N
(cid:88)
LC =− y log(p ) (2) elsandhumanneuralsignals,andthencalculatingSpearman
i i
correlationcoefficientsbetweenRDMsfromtwosystems.
i=1
ToevaluatethesimilaritybetweenmodelsandhumanEEG,
Here, y represents the i-th image, and p represents the
i i theshapeofeachRDMis200×200,correspondingto200
probability that model predicts the i-th image belongs to
images in THINGS EEG2 test set. For EEG RDMs, we
classioutof1000categories. However, thecorrectIma-
applieddecodingaccuracybetweentwoimageconditions
geNet category labels for images in THINGS dataset are
asthedissimilarityindextoconstructEEGRDMforeach
notavailable. Therefore,weadoptthesamestrategyasin
timepointandeachsubject. FormodelRDMs,weinput200
(Dapello et al., 2023), using the labels obtained from the
imagesintoeachmodelandobtainedlatentfeaturesfrom
ImageNetpre-trainedCORnetwithoutneuralalignmentas
eachvisuallayer. Then,weconstructedeachlayer’sRDM
thetruelabelstostabilizetheclassificationperformanceof
bycalculatingthedissimilarityusing1minusPearsoncorre-
ReAlnet.
lationcoefficientbetweenflattenedvectorsoflatentfeatures
LGisthegenerationloss,whichincludesameansquareder- correspondingtoanytwoimages. Tocomparetherepresen-
ror(MSE)lossLMSE andacontrastivelossLContbetween tations,wecalculatedtheSpearmancorrelationcoefficient
thegeneratedandrealEEGsignals. Thiscontrastiveloss asthesimilarityindexbetweenlayer-by-layermodelRDMs
iscalculatedbasedonthedissimilarity(1minusSpearman andtimepoint-by-timepointneuralEEGRDMs.
correlationcoefficient)betweengeneratedandrealsignals,
ToevaluatethesimilaritybetweenmodelsandhumanfMRI,
aiming to bring the generated signals from the same im-
the shape of each RDM is 50 × 50, corresponding to 50
age(positivepairs)closertothecorrespondingrealhuman
images in Shen fMRI dataset test set. For fMRI RDMs,
EEGsignalsandmakethegeneratedsignalsfromdifferent
we calculated 1 minus Pearson correlation coefficient be-
images(negativepairs)moredistinct. LG iscalculatedas
tweenvoxel-wiseactivationpatternscorrespondingtoany
followed:
twoimagesasthedissimilarityindexintheRDMforeach
ROI and each subject. For model RDMs, similar to the
EEGcomparisonsabove,weobtainedthe50×50RDM
LG =LMSE +LCont (3)
foreachlayerfromeachmodel. Then, wecalculatedthe
Spearmancorrelationcoefficientasthesimilarityindexbe-
N
LMSE = 1 (cid:88) (S −Sˆ)2 (4) tweenlayer-by-layermodelRDMsandneuralfMRIRDMs
N i i fordifferentROIs,assigningthefinalsimilarityforacertain
i=1
4ReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
Figure2.ReAlnetsshowhighersimilaritytohumanEEGrepresentations.(A)Representationalsimilaritytimecoursesbetweenhuman
EEGandmodelsfordifferentlayersrespectively.BlacksquaredotsatthebottomindicatethetimepointswhereReAlnetvs.CORnetwere
significantlydifferent(p<.05).Shadedareareflects±SEM.(B)SimilarityimprovementandsimilarityimprovementratioofReAlnets
comparedtoCORnetatthesimilaritypeaktimepointandtheaverageduring50to200mstime-window. Eachcircledotindicatesan
individualReAlnet.
brain region as the highest similarity result across model V2: 60-200ms;LayerV4: 60-200ms;LayerIT:70-160ms)
layers due to the lack of a clear correspondence between thantheoriginalCORnetwithouthumanneuralalignment
differentmodellayersandbrainregions. AllRSAanalyses (Figure2A).Furtherstatisticalanalysisofeachlayer’ssimi-
wereimplementedbasedonNeuroRAtoolbox(Lu&Ku, larityimprovement(ReAlnet-CORnet)andimprovement
2020). ratio((ReAlnet-CORnet)/CORnet)alsoindicatethatat
thesimilaritypeaktimepoint,thereisamaximumofan8%
Adversarialattacks: Forperformingwhiteboxadversarial
similarityimprovementandan80%improvementratio,with
attacks, we used Fast Gradient Sign Attack (FGSA). We
theaverageimprovementforthe50-200mstime-window
evaluatedtop-5classificationaccuraciesonImageNetwith
beingover5%andtheaverageimprovementratioover40%
epsilonrangedfrom0to0.06foreachmodel.
(Figure2B).AdditionalcomparisonsalsoshowthatReAl-
netismorehumanbrain-likethannotonlyCORnet,butalso
3.Results ResNetandCLIP(Figure1B).
Ourcorefocusinthisstudyistoinvestigatewhetheralign- These results suggest three findings: (1) Our multi-layer
ingthemodelwithindividualneuralrepresentationsofhu- alignmentframeworkindeedimprovesalllayers’similarity
manscanenhancethemodel’ssimilaritytothehumanbrain to human EEG representations. (2) Every ReAlnet with
anditsadversarialrobustnessinwhite-boxtestingscenarios. individualneuralalignmentexhibitsimprovedsimilarityto
InSection3.1to3.4,weprimarilypresenttheresultsofthe humanEEGcomparedtothebasicCORnet. (3)ReAlnets
alignedReAlnetswithβ =100. InSection3.5,wecompare demonstratethegeneralizationofimprovementinhuman
the performance of ReAlnet under different β values. In brain-likesimilaritycrossobjectcategories, astheimage
Section3.6,weshowtheresultsofourcontrolexperiments. categoriesusedfortestingwereentirelyabsentduringthe
alignmenttraining.
3.1.ImprovedsimilarityinReAlnetstoHumanEEG
3.2.ImprovedsimilarityinReAlnetstoHumanfMRI
Here, for each of the 10 human subjects, we calculated
(1) the similarity between their EEG data and the single AlthoughReAlnetdemonstrateshighersimilaritytohuman
CORnet,and(2)thesimilaritybetweentheirEEGdataand EEG,aquestionarises: DoesReAlnetlearnrepresentations
thesubject-matchedReAlnet. ReAlnetsshowsignificantly specifictoEEG,ormoregeneralneuralrepresentationsof
highersimilaritytohumanEEGneuraldynamicsforallfour thehumanbrain? Toensurethatouralignmentframework
visuallayers(LayerV1: 70-130msand160-200ms;Layer enablesthemodeltolearnrepresentationsbeyondthesin-
5ReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
Figure3.ReAlnetsshowhighersimilaritytohumanfMRIrepresentations.(A)Representationalsimilaritybetweenthreesubjects’fMRI
activityoffivedifferentbrainregionsandmodelsrespectively.AsterisksindicatesignificantlyhighersimilarityofReAlnetthanthatof
CORnet(p<.05).(B)CorrelationofsimilarityimprovementbetweenReAlnetvs.humanEEGandReAlnetvs.humanfMRI.Each
circledotindicatesanindividualReAlnet.
glemodalityofEEG,weutilizedadditionalhumanfMRI individualvariabilityindex.
datatoevaluatethemodel’scross-modalityrepresentational
Ourresultsshow: (1)PersonalizedReAlnetsindeedexhibit
similaritytohumanfMRI.
individualvariability(Figure4AandFigure4B).(2)This
Excitingly, we indeed observed an increase in this cross- variabilityincreaseswiththedepthofthelayers(fromLayer
modalbrain-likesimilarity. Theresultsindicatethateven V1toLayerIT,Figure4AandFigure4B).Thismayalso
thoughReAlnetswerealignedbasedonhumanEEGdata, suggest a trend of increasing individual variability from
theystillresemblethehumanbrainmorecloselyonfMRI primarytohighervisualcorticalareasinhumanbrains.
data compared to CORnet (Figure 3A). Also, there is a
significantcorrelationofReAlnets’similarityimprovement
compared to CORnet between EEG and fMRI (r=.9204,
p<.001)(Figure3B).
These findings further highlight three points: (1) Across
multipleROIs,ReAlnetsexhibitshigherhumanfMRIsim-
ilarity than CORnet. (2) Despite not being trained with
the EEG data of subjects in the fMRI dataset, almost ev-
eryReAlnetshowshigherfMRIsimilarity,suggestingthat
ReAlnetlearnsconsistentbraininformationprocessingpat-
terns across subjects. (3) Images from fMRI dataset for
evaluationwereneverpresentedduringthealignmenttrain-
ing,reaffirmingthegeneralizationofReAlnetsinimproving
brain-likesimilarityacrossobjectcategoriesandimages.
Figure4.IndividualvariabilityacrosstenpersonalizedReAlnets.
(A)ReAlnetindividualvariabilitymatricesoffourvisuallayers.
3.3.HierarchicalindividualvariabilitiesinReAlnets (B)ReAlnetindividualvariabilityalongthemodellayers. Each
circledotindicatesapairoftwodifferentReAlnets.
Unliketraditionalmodelsincomputervision,ReAlnetisa
personalizedmodeltrainedbasedondifferentindividual’s
neuraldata. Thissparkedourinterestinexploringwhether
3.4.IncreasedadversarialrobustnessinReAlnets
thesepersonalizedReAlnetsexhibitintra-modelindividual
variabilitiesandhowsuchvariabilitieschangeacrossdiffer- Usingwhite-boxFGSA,wealsodiscoveredthatReAlnets,
entlayersofthemodel. Toinvestigatethis,weconducted alignedwithhumanneuralrepresentations,haveincreased
comparisonsbetweenmodelRDMsbasedon200images adversarialrobustnessagainstadversarialattacks(Figure5).
inTHINGSEEG2testsetacrossdifferentlayers,usingthe The left panel of Figure 5 shows a slight increase in ad-
dissimilarity(1minustheSpearmancorrelationcoefficient) versarial robustness in ReAlnets compared to CORnet at
betweentwoRDMscorrespondingtotwoReAlnetsasan aroundEpsilon=0.02. However,theoriginalclassification
6ReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
performance(Epsilon=0)ofReAlnetsislowerthanthat
ofCORnet,duetotheabsenceofcorrectlabelsforimages
in THINGS EEG2 dataset. To make a fairer comparison,
wealignedtheclassificationaccuracyatEpsilon=0asthe
baselinetoobservetherelativedeclineinaccuracyforboth
ReAlnetsandCORnetasEpsilonvalueincreases. Thecor-
rectedresults(Figure5right)demonstratemorepronounced
adversarialstabilityinReAlnets.
Figure6.Figure6.ReAlnetperformanceacrossdifferentβvalues.
(A)ImprovementinhumanEEGsimilarityofReAlnetscompared
toCORnet(averagingfourvisuallayersandtimepointsina50-
200mstime-winodw).(B)ImprovementinhumanfMRIsimilarity
ofReAlnetscomparedtoCORnet(averagingthreesubjectsand
fivebrainregions).(C)ReAlnetindividualvariability(averaging
fourvisuallayers). (D)AdversarialrobustnessofReAlnetsand
CORnet when Epsilon = 0.02. Asterisks indicate significantly
Figure5.IncreasedadversarialrobustnessinReAlnets. Original
higheradversarialrobustnessofReAlnetsthanthatofCORnet
(left)andbaseline-aligned(right)adversarialrobustnessforRe-
(p<.05).
AlnetsandCORnetasafunctionofEpsilon. Asterisksindicate
significantlyhigheradversarialrobustnessofReAlnetsthanthat
ofCORnet(p<.05).
3.6.Controlexperiments
Forthecontrolexperiments,wetestedtwoaspects: (1)How
does contrastive learning influence model-to-brain align-
ment? (2)Ifwedisruptthepairingofeachimagewiththe
3.5.ReAlnetperformanceacrossdifferentweights
EEGsignalfromthesamesubjectbutelicitedbyviewinga
Theresultspresentedabovearebasedonagenerationloss differentimage,canthemodelstilllearntheneuralrepresen-
weightβ setto100. Wefurtherexploredtheimpactofthis tationpatternsofthehumanbrain? Accordingly,wetrained
β value on the performance of ReAlnet. Theoretically, a twoadditionalsetsofReAlnetsbasedonhumanEEGdata
higherβ shouldleadtostrongerlearningofhumanneural fromtensubjectsinTHINGSEEG2dataset,termedasW/o
representations. However,isalargerβ alwaysbetter? Our ContLossmodels(withouttheconstrastivelosscomponent)
findingssuggestotherwise. andUnpairedmodels(wherethepairingbetweenimages
andEEGsignalswasdisrupted).
We observed that with an increase in β, ReAlnets show
greater similarity to human EEG and fMRI (Figure 6A Theresultsofthecontrolexperimentsreveal: (1)W/oCon-
andFigure6B)andmorepronouncedindividualvariabil- tLossmodelsstillexhibitanimprovementinhumanbrain
itywithinmodels(Figure6C).However,anincreaseinβ similaritycomparedtoCORnet. However,whilethesimi-
alsoreducestheimprovementofadversarialstability(the laritytohumanEEGdidnotdecreasecomparedtoReAlnet,
improvement at β = 100 was less significant than at β = thesimilaritytocross-modalityhumanfMRIsignificantly
1 or 10) (Figure 6D). Moreover, at excessively high val- decreased.Thissuggeststhatthecontrastivelosscomponent
ues(β =1000),ReAlnet’sadversarialrobustnesswaseven inouralignmentframeworkaidsReAlnetinextractingmore
lowerthantheoriginalCORnetwithoutneuralalignment cross-modalitybrainvisualrepresentationfeatures. (2)Un-
(Figure 6D). Therefore, this analysis suggests that: (1) It pairedmodelsfailedtoenhancebrainsimilarity,whichshow
justifies our use of β = 100 as a weight that balances the nosignificantimprovementinbrainsimilaritycomparedto
trade-offsandmaximizesadvantagesofReAlnet. (2)β is CORnet, indicating that the training process requires the
aparameterthatcouldbemanipulateddifferentlyinfuture model to effectively learn the specific neural visual fea-
researchdependingonresearchgoals. turescorrespondingtoeachimage. Onlyinthiswaycan
7ReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
themodelbecomemorehumanbrain-likeandthenexhibit encodecategory-specificinformation,mayhavebeenopti-
highersimilaritytothehumanbrainacrossdifferentobject mized(Federeretal.,2020). Moreanalysesoftheneural
images,categories,andhumanneuroimagingdatamodali- network’sinternalrepresentationsmaybeneededtodelve
ties. intothis. Also,fromareverse-engineeringperspective,at-
temptingtounderstandthebrain-likeoptimizationprocess
ofthemodelcouldfurtheraidinunravelingthemechanisms
bywhichourbrainsprocessvisualinformation(Ayzenberg
etal.,2023;Cic,2019;Doerigetal.,2023;Kanwisheretal.,
2023;Lu&Ku,2023;Lu&Golomb,2023b).
Certainly, it is important to highlight that ReAlnet tran-
scendsbeingmerelyaspecificvisionmodel;itrepresents
apioneeringframeworkpotentiallyapplicableforaligning
anyAImodelwithbrainactivity.Ontheonehand,thisalign-
mentframeworkcanbeextendedtootherneuralmodalities,
Figure7.Resultsofcontrolexperiments.(A)Improvementinhu- such as fMRI and MEG (dimensionality reduction might
manEEGsimilarityofReAlnetsandcontrolmodelscomparedto benecessaryforextensiveneuraldatafeatures),pavingthe
CORnet.(B)ImprovementinhumanfMRIsimilarityofReAlnets wayforthedevelopmentofvariantslikeReAlnet-fMRIand
andcontrolmodelscomparedtoCORnet.Asterisksindicatethe ReAlnet-MEG.Ontheotherhand,theambitionistoadapt
significance(p<.05). this framework to a wider range of models and tasks in
thefuture,includinglanguageandauditoryprocessingand
self-supervisedorunsupervisedmodels,leadingtoinnova-
tions such as ReAlnet-Language, ReAlnet-Auditory, and
4.Discussion
self-supervisedorunsupervisedversionsofReAlnet.
Building upon previous research utilizing neural data for Limitations: Fromadataperspective,theprimarylimita-
aligning object recognition models, we have proposed a tionsofourcurrentstudystemfrom(1)therelativelylower
novelandmoreeffectiveframeworkforhumanneuralrepre- samplesizeofneuraldatasetscomparedtoimagedatasets
sentationalalignment,alongwiththecorrespondinghuman withhugesamples,and(2)thelackofsharedlabelsbetween
brain-like model, ReAlnet. Unlike previous studies that differentdatasets,suchastheabsenceofcorrespondingIma-
focusedonusinganimalneuralsignalstooptimizemodels geNetcategorylabelsforimagesinTHINGSdataset. These
orwereunabletouseglobalneuralactivityforcomprehen- limitationsrestrictfurtherenhancementofReAlnet’ssimi-
sivemodeloptimization(Dapelloetal.,2023;Federeretal., laritytothehumanbrainandreduceitsclassificationperfor-
2020;Lietal.,2019;Pirlotetal.,2022;Safaranietal.,2021), manceonImageNet. Fromatechnicalperspective,future
our approach efficiently utilizes human neural activity to researchmayneedtofocuson(1)moreeffectivelylearning
simultaneouslyoptimizemultiplelayersofthemodel,en- the alignment of models with the human brain on small-
ablingittolearnthehumanbrain’sinternalrepresentational sampleneuraldata, and(2)employingself-supervisedor
patternsforobjectvisualprocessing. Notably,unlikeprior unsupervisedlearningmethodsthatdonotrequirecategory
research relying on behavioral or single modality neural labelsformodeltraining.
recordingdataformodelevaluation(Dapelloetal.,2023;
Federeretal.,2020;Fuetal.,2023;Lietal.,2019;Pirlot
5.Conclusion
etal.,2022;Safaranietal.,2021),weemployeddifferent
modalitiesofhumanneuroimagingdataformodelevalua- Ourstudytranscendstraditionalboundariesbyemployinga
tiontoensurethatReAlnetlearnsbroader,cross-modalbrain groundbreakingalignmentframeworkthatpioneerstheuse
representational patterns. Additionally, we observed that ofhumanneuraldatatoachievingamorehumanbrain-like
ReAlnet exhibits individual representational variabilities visionmodel,ReAlnet. Demonstratingsignificantadvances
akintohumanbrain’shierarchicalprocessingandadversar- inbio-inspiredAI,ReAlnetnotonlyalignscloselywithhu-
ialstabilitysimilartothefindingsinotherbrain-inspired manEEGandfMRIbutalsoexhibithierarchicalindividual
models(Dapelloetal.,2023;Konkle&Alvarez,2023). variabilities and increased adversarial robustness, mirror-
inghumanvisualprocessing. Wehopethatouralignment
RegardingReAlnetitself,itwarrantsfurtherexplorationto
framework stands as a testament to the potential synergy
ascertain what specific information has learned from the
between computational neuroscience and machine learn-
alignmentwithhumanbrains. Thefactthatdifferentgener-
ing and enables the enhancement of any AI model to be
ationlossweightsdonotsignificantlyimpactthebehavioral
morehumanbrain-like,openingupexcitingpossibilitiesfor
performancebutdoenhanceitssimilaritytohumanbrains
futureresearchinbrain-likeAIsystems.
suggeststhatnodesinthemodel,whichoriginallydidnot
8ReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
6.Acknowledgements Federer,C.,Xu,H.,Fyshe,A.,andZylberberg,J. Improved
objectrecognitionusingneuralnetworkstrainedtomimic
This work was supported by grants from the National In-
thebrain’sstatisticalproperties. NeuralNetworks,131:
stitutesofHealth(R01-EY025648)andNationalScience
103–114,2020. doi: 10.1016/j.neunet.2020.07.013.
Foundation(NSF1848939)toJulieD.Golomb. Wethank
theOhioSupercomputerCenterandGeorgiaStuartforpro- Finzi,D.,Margalit,E.,Kay,K.,Yamins,D.L.K.,andGrill-
vidingtheessentialcomputingresourcesandsupport. We Spector, K. Topographic DCNNs trained on a single
thankYuxuanZengforthe‘ReAlnet’namesuggestion. We self-supervisedtaskcapturethefunctionalorganization
thankTianyuZhang,ShuaiChen,JiaqiLi,andsomeother ofcortexintovisualprocessingstreams. NeurIPS2022
membersinMemory&PerceptionReviewsReadingGroup WorkshopSVRHM,2022.
(RRG) for helpful discussions about the methods and re-
Fong, R. C., Scheirer, W. J., and Cox, D. D. Us-
sults. WethankYuxinWangforconstructivefeedbackon
ing human brain activity to guide machine learning.
themanuscript.
Scientific Reports, 8(1):1–10, 2018. doi: 10.1038/
s41598-018-23618-6.
References
Fu,K.,Du,C.,Wang,S.,andHe,H. ImprovedVideoEmo-
Deep Neural Networks as Scientific Models. Trends in tion Recognition with Alignment of CNN and Human
CognitiveSciences,23(4):305–317,2019. doi: 10.1016/j. BrainRepresentations. IEEETransactionsonAffective
tics.2019.01.009. Computing, 14(8):1–15, 2023. doi: 10.1109/TAFFC.
2023.3316173.
Ayzenberg,V.,Blauch,N.,andBehrmann,M. Usingdeep
neuralnetworkstoaddressthehowofobjectrecognition. Gifford,A.T.,Dwivedi,K.,Roig,G.,andCichy,R.M. A
PsyArXiv,2023. doi: 10.31234/osf.io/6gjvp. largeandrichEEGdatasetformodelinghumanvisual
objectrecognition. NeuroImage,264:119754,2022. doi:
Bai,S.,Li,Z.,andHou,J. Learningtwo-pathwayconvo-
10.1016/J.NEUROIMAGE.2022.119754.
lutionalneuralnetworksforcategorizingsceneimages.
MultimediaToolsandApplications,76(15):16145–16162, Gu¨c¸lu¨, U. and van Gerven, M. A. Deep Neural Net-
2017. doi: 10.1007/s11042-016-3900-6. works Reveal a Gradient in the Complexity of Neu-
ral Representations across the Ventral Stream. Jour-
Choi, M., Han, K., Wang, X., Zhang, Y., and Liu, Z. A nal of Neuroscience, 35(27):10005–10014, 2015. doi:
Dual-Stream Neural Network Explains the Functional 10.1523/JNEUROSCI.5023-14.2015.
SegregationofDorsalandVentralVisualPathwaysinHu-
manBrains. AdvancesinNeuralInformationProcessing Han,Z.andSereno,A. ModelingtheVentralandDorsal
Systems(NeurIPS),36,2023. Cortical Visual Pathways Using Artificial Neural Net-
works. NeuralComputation,34(1):138–171,2022. doi:
Cichy, R. M., Khosla, A., Pantazis, D., Torralba, A., and 10.1162/NECO A 01456.
Oliva, A. Comparison of deep neural networks to
Han,Z.andSereno,A. IdentifyingandLocalizingMulti-
spatio-temporalcorticaldynamicsofhumanvisualobject
pleObjectsUsingArtificialVentralandDorsalCortical
recognitionrevealshierarchicalcorrespondence. Scien-
VisualPathways. NeuralComputation,35(2):249–275,
tific Reports, 6(1):1–13, 2016. ISSN 2045-2322. doi:
2023. doi: 10.1162/neco a 01559.
10.1038/srep27755.
Hebart, M. N., Dickter, A. H., Kidder, A., Kwok, W. Y.,
Dapello, J., Kar, K., Schrimpf, M., Geary, R. B., Fer-
Corriveau,A.,VanWicklin,C.,andBaker,C.I.THINGS:
guson, M., Cox, D. D., and DiCarlo, J. J. Aligning
Adatabaseof1,854objectconceptsandmorethan26,000
Model and Macaque Inferior Temporal Cortex Repre-
naturalisticobjectimages.PLoSONE,14(10):1–24,2019.
sentationsImprovesModel-to-HumanBehavioralAlign-
doi: 10.1371/journal.pone.0223792.
mentandAdversarialRobustness. InternationalConfer-
enceonLearningRepresentations(ICLR),12,2023. doi: Kanwisher,N.,Khosla,M.,andDobs,K. Usingartificial
10.1101/2022.07.01.498495. neural networks to ask ‘why’ questions of minds and
brains. TrendsinNeurosciences,46(3):240–254,2023.
Doerig, A., Sommers, R., Seeliger, K., Richards, B., Is-
doi: 10.1016/j.tins.2022.12.008.
mael, J., Lindsay, G., Kording, K., Konkle, T., Van
Gerven, M. A. J., Kriegeskorte, N., and Kietzmann, Kar,K.,Kubilius,J.,Schmidt,K.,Issa,E.B.,andDiCarlo,
T.C. Theneuroconnectionistresearchprogramme. Na- J. J. Evidence that recurrent circuits are critical to the
ture Reviews Neuroscience, 24:431–450, 2023. doi: ventralstream’sexecutionofcoreobjectrecognitionbe-
10.1038/s41583-023-00705-w. URLhttp://arxiv. havior. NatureNeuroscience,22(6):974–983,2019. doi:
org/abs/2209.03718. 10.1038/s41593-019-0392-5.
9ReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
Kietzmann, T. C., Spoerer, C. J., So¨rensen, L. K., Cichy, Lu,Z.andGolomb,J.D. HumanEEGandartificialneural
R.M.,Hauk,O.,andKriegeskorte,N. Recurrenceisre- networks reveal disentangled representations of object
quiredtocapturetherepresentationaldynamicsofthehu- real-worldsizeinnaturalimages. bioRxiv,2023b. doi:
manvisualsystem. ProceedingsoftheNationalAcademy 10.1101/2023.04.26.538469.
of Sciences of the United States of America, 116(43):
Lu, Z. and Ku, Y. NeuroRA: A Python Toolbox of
21854–21863,2019. doi: 10.1073/PNAS.1905544116/
Representational Analysis From Multi-Modal Neural
SUPPL FILE/PNAS.1905544116.SM06.MP4.
Data. FrontiersinNeuroinformatics,14:61,2020. doi:
10.3389/FNINF.2020.563669.
Konkle, T. and Alvarez, G. Cognitive Steering in Deep
NeuralNetworksviaLong-RangeModulatoryFeedback
Lu, Z. and Ku, Y. Bridging the gap between EEG and
Connections.AdvancesinNeuralInformationProcessing
DCNNsrevealsafatiguemechanismoffacialrepetition
Systems(NeurIPS),(36),2023.
suppression. iScience,26:108501,2023. doi: 10.1016/j.
isci.2023.108501.
Konkle,T.andAlvarez,G.A. Aself-superviseddomain-
generallearningframeworkforhumanventralstreamrep- Lu,Z.,Doerig,A.,Bosch,V.,Krahmer,B.,Kaiser,D.,Ci-
resentation. NatureCommunications,13(1):1–12,2022. chy,R.M.,andKietzmann,T.C. End-to-endtopographic
doi: 10.1038/s41467-022-28091-4. networksasmodelsofcorticalmapformationandhuman
visualbehaviour: movingbeyondconvolutions. arXiv,
Kriegeskorte, N., Mur, M., and Bandettini, P. Represen- 2023. doi: 10.48550/arXiv.2308.09431.
tationalsimilarityanalysis-connectingthebranchesof
Margalit,E.,Lee,H.,Finzi,D.,DiCarlo,J.J.,Grill-Spector,
systemsneuroscience.FrontiersinSystemsNeuroscience,
K., and Yamins, D. L. A Unifying Principle for the
4,2008. doi: 10.3389/NEURO.06.004.2008.
FunctionalOrganizationofVisualCortex. bioRxiv,2023.
Kubilius,J.,Schrimpf,M.,Nayebi,A.,Bear,D.,Yamins, doi: 10.1101/2023.05.18.541361.
D.,andDiCarlo,J. CORnet: ModelingtheNeuralMech-
O’Connell, T. P., Bonnen, T., Friedman, Y., Tewari, A.,
anismsofCoreObjectRecognition. bioRxiv,2018. doi:
Tenenbaum,J.B.,Sitzmann,V.,andKanwisher,N. Ap-
10.1101/408385.
proaching human 3D shape perception with neurally
mappable models. arXiv, 2023. doi: 10.48550/arXiv.
Kubilius,J.,Schrimpf,M.,Kar,K.,Rajalingham,R.,Hong,
2308.11300.URLhttp://arxiv.org/abs/2308.
H.,Majaj,N.J.,Issa,E.B.,Bashivan,P.,Prescott-Roy,
11300.
J.,Schmidt,K.,Nayebi,A.,Bear,D.,Yamins,D.L.K.,
and Dicarlo, J. J. Brain-Like Object Recognition with Pirlot,C.,Gerum,R.C.,Efird,C.,Zylberberg,J.,andFyshe,
High-PerformingShallowRecurrentANNs. Advances A. Improving the Accuracy and Robustness of CNNs
inNeuralInformationProcessingSystems(NeurIPS),32, UsingaDeepCCANeuralDataRegularizer. 2022.
2019.
Prince,J.S.,Alvarez,G.A.,andKonkle,T. Acontrastive
Lecun,Y.,Bengio,Y.,andHinton,G.Deeplearning.Nature, codingaccountofcategoryselectivityintheventralvisual
521(7553),2015. doi: 10.1038/nature14539. stream. bioRxiv,2023. doi: 10.1101/2023.08.04.551888.
Rajalingham,R.,Issa,E.B.,Bashivan,P.,Kar,K.,Schmidt,
Lee,H.,Margalit,E.,Jozwik,K.M.,Cohen,M.A.,Kan-
K.,andDiCarlo,J.J. Large-Scale,High-ResolutionCom-
wisher, N., Yamins, D. L. K., and DiCarlo, J. J. To-
parisonoftheCoreVisualObjectRecognitionBehavior
pographicdeepartificialneuralnetworksreproducethe
ofHumans,Monkeys,andState-of-the-ArtDeepArtifi-
hallmarks of the primate inferior temporal cortex face
cialNeuralNetworks. JournalofNeuroscience,38(33):
processingnetwork. bioRxiv,2020. doi: 10.1101/2020.
7255–7269,2018. doi: 10.1523/JNEUROSCI.0388-18.
07.09.185116.
2018.
Li,Z.,Brendel,W.,Walker,E.Y.,Cobos,E.,Muhammad,
Safarani,S.,Nix,A.,Willeke,K.,Cadena,S.A.,Restivo,
T.,Reimer,J.,Bethge,M.,Sinz,F.H.,Pitkow,X.,and
K.,Denfield,G.,Tolias,A.S.,andSinz,F.H. Towards
Tolias, A. S. Learning from brains how to regularize
robustvisionbymulti-tasklearningonmonkeyvisualcor-
machines. AdvancesinNeuralInformationProcessing
tex. AdvancesinNeuralInformationProcessingSystems
Systems(NeurIPS),32,2019.
(NeurIPS),34,2021.
Lu, Z. and Golomb, J. D. Generate your neural signals Shen,G.,Horikawa,T.,Majima,K.,andKamitani,Y. Deep
frommine:individual-to-individualEEGconverters. Pro- imagereconstructionfromhumanbrainactivity. PLoS
ceedingsoftheAnnualMeetingoftheCognitiveScience computationalbiology,15(1):e1006633,2019. doi: 10.
Society45,2023a. 1371/journal.pcbi.1006633.
10ReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
Spoerer,C.J.,McClure,P.,andKriegeskorte,N. Recurrent
convolutionalneuralnetworks: Abettermodelofbiolog-
icalobjectrecognition. FrontiersinPsychology,8:1551,
2017. doi: 10.3389/FPSYG.2017.01551.
Sun,T.,Wang,Y.,Yang,J.,andHu,X. ConvolutionNeural
Networks With Two Pathways for Image Style Recog-
nition. IEEETransactionsonImageProcessing,26(9):
4102–4113,2017. doi: 10.1109/TIP.2017.2710631.
Tang,H.,Schrimpf,M.,Lotter,W.,Moerman,C.,Paredes,
A., Caro, J. O., Hardesty, W., Cox, D., and Kreiman,
G. Recurrentcomputationsforvisualpatterncompletion.
ProceedingsoftheNationalAcademyofSciencesofthe
UnitedStatesofAmerica,115(35):8835–8840,2018. doi:
10.1073/PNAS.1719397115.
Yamins, D. L., Hong, H., Cadieu, C. F., Solomon, E. A.,
Seibert, D., and DiCarlo, J. J. Performance-optimized
hierarchical models predict neural responses in higher
visualcortex. ProceedingsoftheNationalAcademyof
SciencesoftheUnitedStatesofAmerica,111(23):8619–
8624,2014. doi: 10.1073/PNAS.1403112111.
11ReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
A.Appendix.
A.1. ImageNetclassificationperformancesofReAlnetsatdifferentβ values
WetestedtheclassificationaccuracyofReAlnetsonImageNetatdifferentβ values(Figure8). Importantly,toascertainthat
theobserveddecreaseinaccuracywasnotduetotheadditionalgenerationtaskcompromisingclassificationperformance,
butrathertheabsenceofcorrectImageNetlabelsforimagesinTHINGSEEG2dataset,wetrainedaReAlnetwithβ =0.
ThisReAlnetexcludedtheEEGsignalgenerationmodulebutunderwentfine-tuningwithimagesfromTHINGSEEG2
dataset. TheresultsindicatedthattheReAlnetwithβ =0alsoexperiencedasimilarlevelofdecline.
Figure8.ImageNetclassificationaccuracyofdifferentReAlnets. Left: Top-1accuracy. Right: Top-5accuracy. Thebluedottedline
indicatestheaccuracyofCORnet,andthegreydottedlineindicatestheaccuracyofReAlnetatβ=0.
12ReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
A.2. EEGgenerationperformancesofouralignmentframeworkatdifferentβ values
WeevaluatedtheEEGgenerationperformanceofthealignmentframeworksatdifferent(valuesbycalculatingtheSpearman
correlationbetweenthegenerativeEEGsignalsandtheactualEEGsignals. Figure9showstheEEGgenerationperformance
andsomeexamplesofgeneratedresults.
Figure9.(A)EEGgenerationperformanceofdifferentalignmentframeworks.(B)FourexamplesofEEGgenerationresults(fromthe
modelatβ=100ofSub-01).Foreachexample,theleftimageindicatestheimageinputtotheReAlnetandtheimageviewedbythe
subject.ThegreycurvesrepresenttherealEEGsignals,andthegreencurvesrepresentthegeneratedEEGsignalscorrespondingtothe
sameimage.
13ReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
A.3. RepresentationalsimilaritybetweenhumanEEGandReAlnetsatdifferentβ values
Figure10showstherepresentationalsimilaritybetweenhumanEEGandReAlnetsatdifferentβ values.
Figure10.RepresentationalsimilaritytimecoursesbetweenhumanEEGanddifferentReAlnetsfordifferentlayersrespectively.Black
squaredotsatthebottomindicatesignificanttimepoints(p<.05).Shadedareareflects±SEM.
14ReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
A.4. RepresentationalsimilaritybetweenhumanfMRIandReAlnetsatdifferentβ values
Figure11showstherepresentationalsimilaritybetweenhumanfMRIandReAlnetsatdifferentβ values.
Figure11.Representational similarity between three subjects’ fMRI activity of five different brain regions and different ReAlnets
respectively.AsterisksindicatesignificantlyhighersimilarityofReAlnetthanthatofCORnet(p<.05).
15ReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
A.5. IndividualvariabilityacrosspersonalizedReAlnetsatdifferentβ values
Figure12showstherepresentationalsimilaritybetweenhumanfMRIandReAlnetsatdifferentβ values.
Figure12. IndividualvariabilitymatricesoffourvisuallayersofdifferentReAlnets.
16ReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
A.6. AdversarialrobustnessofReAlnetsatdifferentβ values
Figure13showstheadversarialrobustnessofReAlnetsatdifferentβ values.
Figure13.Baseline-alignedadversarialrobustnessfordifferentReAlnetsasafunctionofEpsilon.Asterisksindicatesignificantlyhigher
adversarialrobustnessofReAlnetsthanthatofCORnet(p<.05).
17ReAlnet:AchievingMoreHumanBrain-LikeVisionviaHumanNeuralRepresentationalAlignment
A.7. Representationalsimilaritybetweenhumanbrainsandcontrolledmodels
Figure14AshowstherepresentationalsimilaritybetweenhumanEEGandcontrolledmodels,andFigure14Bshowsthe
representationalsimilaritybetweenhumanfMRIandcontrolledmodels.
Figure14.(A)RepresentationalsimilaritytimecoursesbetweenhumanEEGandReAlnetsandcontrolmodels(βfordifferentlayers
respectively. Shadedareareflects±SEM.(B)Representationalsimilaritybetweenthreesubjects’fMRIactivityoffivedifferentbrain
regionsandReAlnetsandcontrolmodels(β=100)respectively.
18