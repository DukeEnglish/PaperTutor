A simple, strong baseline for building damage detection on the xBD dataset
Sebastian Gerard1, Paul Borne-Pons1,2, and Josephine Sullivan1
1KTH Royal Institute of Technology, Stockholm, Sweden
{sgerard,paulbp,sullivan}@kth.se
2CentraleSup´elec (Universit´e Paris-Saclay), Paris, France
paul.borne-pons@student-cs.fr
January 31, 2024
Abstract
We construct a strong baseline method for building damage detection by starting with the highly-engineered win-
ning solution of the xView2 competition, and gradually stripping away components. This way, we obtain a much
simplermethod,whileretainingadequateperformance. Weexpectthesimplifiedsolutiontobemorewidelyandeasily
applicable. This expectation is based on the reduced complexity, as well as the fact that we choose hyperparameters
based on simple heuristics, that transfer to other datasets. We then re-arrange the xView2 dataset splits such that
the test locations are not seen during training, contrary to the competition setup. In this setting, we find that both
the complex and the simplified model fail to generalize to unseen locations. Analyzing the dataset indicates that this
failuretogeneralizeisnotonlyamodel-basedproblem,butthatthedifficultymightalsobeinfluencedbytheunequal
class distributions between events.
Code, including the baseline model: https://github.com/PaulBorneP/Xview2_Strong_Baseline
1 Introduction
With progressing climate change, extreme weather events are projected to occur more often across many regions of
the world. [2] Since these events typically impact large areas, it is useful to incorporate large-scale observations from
remote sensing systems in the disaster response. Modern computer vision methods can be used to process such large-
scale observations automatically and provide various analysis results in the area of humanitarian assistance and disaster
response (HADR).
As part of the xView2 competition, the xBD building damage prediction dataset[3] was published. It contains
high-resolution satellite image pairs from before and after various types of disasters, paired with high-quality building
damage annotations for the post-disaster images. The natural starting point for research on this kind of problem now is
thesolutionthatwonthexView2competition.[1]However,sincethatsolutionwasconstructedwiththegoalofwinning
a competition, it is highly engineered and fine-tuned in many small ways, that make it hard to tweak and extend.
In this paper, we construct a simple, yet strong, baseline method by stripping away components of the xView2-
winning solution step by step. It retains most of the performance of the original solution, while being much simpler, and
thereforeeasiertouseandextendinfurtherresearch. Wethenshowthatbothmethodssuffergreatlyfromgeneralization
issues, when tested on a set of events of xView2 that is not seen during training. This is contrary to the original setup
of the xView2 competition, in which images of each event are seen during training and testing.
We publish the simplified baseline as a LightningModule, as well as providing a LightningDataModule to load the
xBD dataset itself. Both of these are part of the PyTorch Lightning framework [4], that makes it easy to encapsulate
all relevant steps for training and inference into one object, instead of having these details spread across various files and
function calls, interspersed with the training logic.
In summary, our contributions are the following:
• WeproposeastrongbaselinemethodbasedonthexView2-winningsolution. Wedemonstratethatthestep-by-step
removal of various components only has a small influence on the performance of the method and the final method
is only about 2 percentage points worse than our reproduction of the original method.
• Wedemonstratethatboththecompetitionwinner,aswellasthestrongbaseline,suffersfromastronggeneralization
failure in two of the four damage classes. This only becomes clear once we construct a dataset split, in which the
events do not overlap between train and test set.
• We publish the strong baseline in an easily accessible form, to facilitate further research.
Structure In section 2, we first describe the xBD dataset, including class balance issues that might contribute to
generalizationproblems. Then,wedescribeourstrongbaselinemodelinsection3. Insection5,weshowtheexperimental
1
4202
naJ
03
]VC.sc[
1v17271.1042:viXraFigure1: Publishedweights, ourreproduction, oursimplifiedmodel, generalization. Wecomparethefollowing
models: Theleftmost(brown)barrepresentsthepublishedweightsofthecompetitionwinningsolution.[1]The2nd (dark
green) bar uses the published code of the winning solution to retrain the model on our hardware. We can see a drop in
performance that is not based on any intentional changes to the code. The 3rd (dark blue) bar shows our strong baseline
model,derivedfromthewinningsolutionbyvarioussimplificationsteps. Itperformsslightlyworsethanourreproduction.
The two rightmost bar are, in order, the winning model (light green) and our strong baseline (light blue), retrained on a
data split where the test disasters are not seen in training. Generalization proves difficult for both models. Although the
strong baseline yields worse results, this difference is small, compared to the performance drop between the two splits.
The drop is especially steep for ’minor damage’ and ’major damage’. While the ’no damage’ and ’destroyed’ classes are
easy to distinguish, it is difficult to clearly distinguish the two damage levels in between, so seeing the performance drop
strongest in those two classes is not surprising. The results are based on individual ResNet34-U-Net models.
results of simplifying the complex competition-winning model step by step until we arrive at our baseline model. This
includes the description of the complexities of the winning solution. We move on to the question of generalization
in section 6, comparing the generalization performance of competition-winning model and the simplified model on a
non-overlapping dataset split. section 6.
2 xBD: A Dataset for Assessing Building Damage from Satellite Imagery
The xBD dataset [3] is a satellite image dataset for building damage prediction. It contains images covering 22 disasters
in 15 countries, including hurricanes, tornadoes, wildfires, earthquakes, floods and volcano outbreaks. It represents the
task to detect buildings and classify them into one of four building damage classes, given one satellite image from before
and one from after the disaster. The respective labels are high-quality annotations created by humans. Each building
is consistently labelled with one class, even if only a part of the building is actually damaged. Figure 2 shows example
images. ThexBDdatasetwasthebasisforthexView2challenge. Fortrainingintheablationexperiments, wecombine
thetrainsubset(2799imagepairs)andthetier3subset(6369imagepairs). Intesting, weusethetestsubset, containing
933 image pairs from events that are already part of the training subsets. All images have a size of 1024² pixels.
2.1 Generalization difficulty on the dataset-level
In this section we explore dataset properties that make it harder for the trained models to generalize well to unseen
events. In Figure 3, we see how strongly the class distribution differs between events. For example, wildfire damages
are rarely annotated as ’minor damage’ or ’major damage’, making it very difficult to learn to predict this class for
this disaster type. While almost all distributions are dominated by the ’no damage’ class, more than half of hurricane
Matthew’s building pixels are labeled as ’minor damage’. This is not the case for other hurricanes or tornadoes. This
imbalance is based on the dataset composition and makes it hard for the model to perform equally well on all damage
classes for all events, and also to generalize to events that might have very different class distributions.
Part of the generalization problem is also that the way that damage classes look changes based on the geo-location.
Minor damage looks different when it affects a large, expensive house in Asia than when it affects low-income housing
in North America. This difficulty is inherent to the general task of building damage classification itself. Additionally,
2(a) Pre-disaster (b) Post-disaster (c) Building damage annotation
Figure2: xBD: Example images from the datasetTheimagesshowalocationimpactedbythe2013Mooretornado.
Thedatasetcontainsoneimagefrombeforeandoneafterthedisaster. Theannotationindicatesthelocationofbuildings
and the respective building damage class.
the distinction between damage classes from satellite images is no easy task in annotation either, especially given the
differentappearanceoftheseclassesacrossgeographiesanddisastertypes. Whilethe’nodamage’and’destroyed’classes
are clearly distinguishable, the distinction between ’minor damage’, ’major damage’ and their respective neighboring
classes is likely difficult, based only on the satellite images. Labeling with in-situ visitations could improve upon this,
but this approach is of course completely impractical on the scale of this dataset.
2.2 Assessing generalization via a new dataset split
To be able to investigate how well models generalize, we want to test them on data that they have not seen during
training. Since the original xBD dataset splits have images from all test events in the training set, we need to create
a new split, that assigns each event exclusively to one of the subsets. At the same time, we want to retain at least one
disaster event per event type in each of the subsets. For example, it does not make much sense to test on wildfires if the
model has never seen wildfires during training. When plotting the images’ locations on a map, we further found that
several of the disaster events overlap spatially (see Figure 4) or are in close spatial proximity (see Figure 5). To ensure
a fair generalization test, we keep all subgroups of spatially close events contained in the same subset. The new split is
shown in Table 1.
Train set Test set
event name count event name count
lower-puna-volcano 291 tuscaloosa-tornado 343
palu-tsunami 155 guatemala-volcano 23
mexico-earthquake 159 sunda-tsunami 138
socal-fire 1130 santa-rosa-wildfire 300
woolsey-fire 878 hurricane-matthew 311
portugal-wildfire 1869 — —
pinery-bushfire 1845 — —
nepal-flooding 619 — —
midwest-flooding 359 — —
moore-tornado 277 — —
joplin-tornado 149 — —
hurricane-florence 427 — —
hurricane-harvey 427 — —
hurricane-michael 441 — —
Table 1: New dataset splits To fairly evaluate the generalization performance of the models, we rearrange the dataset
to be free of overlaps between the train and test set. This includes ensuring that disaster events that happened in close
proximity are contained in the same data subset. Those disaster are in bold in the table
3 The strong baseline method
The strong baseline is a streamlined version of the xView2 challenge winning solution [1], where we have kept the most
importantcomponentsoftrainingandarchitecture. Theablationexperimentsfortheintermediatestepsbetweenwinning
3Figure 3: Damage class distributions per event. The distribution between events and event categories can differ
greatly. For example, wildfire damages are mostly annotated as destroyed, while floods are mostly annotated as minor
or major damage. This imbalance adds to the difficulty of training a model to perform well on all classes of all events.
4Figure4: Spatialoverlapbetween”socalfire”and”woolseyfire”,twodisastersthatcanbefoundinourdatasetandthat
actually describe the same fire that occurred around Los Angeles in November 2018. Map by OpenStreetMap.
solution and simplified baseline are shown in section 5. The strong baseline is built around an encoder-decoder network
f with skip connections and convolutional layers akin to a U-Net [5]. f is parameterized by Θ . This network
enc-dec enc-dec 1
is applied independently to the before image X and after image X of the disaster site, each of which has width
before after
W, height H and depth D =3:
Z =f (X ,Θ ), Z =f (X ,Θ ) (1)
before enc-dec before 1 after enc-dec after 1
with Z before,Z
after
∈ RW×H×D1. These outputs are concatenated Z
cat
= [Z before,Z after] and input into a final small
convolutional network g consisting of 1×1×2D convolutions, parameterized by Θ :
1 2
Z =g(Z ,Θ ) (2)
cat 2
The final output is Z ∈ RW×H×Cˆ, where Cˆ is the number of damage classes (Cˆ = 4: no damage, minor damage, major
damage, destroyed). Z contains the score for each pixel of belonging to each class. Each score in Z is transformed to a
scalar between 0 and 1 by applying the sigmoid function:
P =σ(Z) (3)
where σ(·) denotes the sigmoid function. It is applied to each entry of Z independently and results in P ∈[0,1]W×H×Cˆ.
One can interpret the entry P of P as the probability of pixel at location (i,j) belonging to damage class cˆ.
ijcˆ
Note: We represent the multi-class classification problem, where each pixel should ultimately be assigned to just one
class,asmultiplebinaryclassificationproblemsduringtraining. Whilenotstrictlythecorrectapproach(aseachpixelcan
have only one label), this modelling has been shown [6], [7] to lead to better accuracy for supervised image classification.
3.1 Building localization
To discriminate building pixels from non-building pixels, we use a localization mask L ∈ {0,1}W×H. We compute the
final segmentation mask M ∈{0,1,2,3,4}W×H as the output of our method by masking the damage segmentation with
the localization mask. Class 0 indicates the background class, while classes 1−4 indicate the different damage classes:
5Figure 5: Spatial proximity between ”joplin tornado”, ”moore tornado” and ”midwestern floodings”. Those disaster
occurred in geographically close areas in the state of Oklahoma and were kept in the same set as prevention. Map by
OpenStreetMap.
M =(1+argmax(P ))1 . (4)
ijcˆ Lij=1
The localization mask L can be computed in a number of ways, all of which yielded similar localization performance
in our ablation studies:
1. In the winning solution, the localization mask L is created by thresholding the prediction output P of a separate
loc
localization model, that has the same architecture as the damage classification model.
2. Ourstrongbaselinepredictsanadditionalclassthatdistinguishesbetweenbuildingandno-building,independently
of the damage classification.
3. Anotherapproachweusedonthewaytowardsthestrongbaselinemakesuseofthebuildingdamagepredictionsthat
are already part of the model. The localization map can be created from the damage predictions by thresholding
them (we use OTSU’s method [8]). If at least one of the binary classifications indicates that any level of building
damage is present, it means there is also a building at this position. This is possible, because we formulated the
problem as several independent binary classification problems, instead of using a softmax function. We used this
methodfortheearlyversionsofoursolution,sinceabuginthecodeledustobelievethatthepreviouslymentioned
method was not working well enough.
3.2 Loss function
ThefinalversionofourbaselinepredictsC =5channels: fourtopredictbuildingdamageandonetopredictthepresence
or absence of buildings. We keep the same notations as above but with C instead of Cˆ.
Learning the parameters, Θ = (Θ ,Θ ), of our complete network from labelled training data requires optimizing a
1 2
lossfunctionw.r.t. Θ, givenatrainingdataset. Thelossinthewinningsolutionisacombinationofseveralstandardloss
functions used in semantic segmentation. Here, we review the mathematical definition of these losses and in turn also
introduce our notation.
LetY =(Y ,Y ,...,Y ),whereeachY ∈{0,1}W×H,for1≤c≤C,isthe2dbinarymatrixslicethroughtheground
1 2 C c
truth Y, representing which pixel locations have label c. Similarly let P =(P ,P ,...,P ) be the respective predictions.
1 2 C
For easier definition of the focal loss function, let P˜ represent the predicted probability for the presence or absence
c
of class c at each pixel location as indicated by Y :
c
P˜ =(1−Y )⊙(1−P )+Y ⊙P (5)
c c c c c
where ⊙ represents element-wise multiplication. For a ground truth segmentation map Y and a predicted segmentation
c
map P (and thus in turn also P˜ ), the standard loss functions used to train the strong baseline network for one input
c c
6are:
L focal(P c,Y c)=− W1
H
(cid:88) (1−P˜ ijc)γ log(P˜ ijc) Focal loss (6)
i,j
(cid:80)
2 Y P
L dice(P c,Y c)=1− (cid:80) Yi,j +ij (cid:80)c ij Pc Soft dice loss (7)
i,j ijc i,j ijc
where γ ≥0. The combination loss, for non-negative scalars w ,w , and w , is then defined as:
bce focal dice
L (P ,Y )=w L (P ,Y )+w L (P ,Y ). (8)
combo c c focal focal c c dice dice c c
Finally, the overall training loss over all classes, for one labelled input image pair is:
C
(cid:88)
L(P,Y)= w L (P ,Y ), (9)
c combo c c
c=1
where each w is a non-negative scalar.
c
The strong baseline uses γ = 2, (w ,w ) = (1,1) and w = 1 for c ∈ {1,2,3,4,5}, where f is the relative
focal dice c fc c
frequencyofclasscinthewholetrainingdataset. Aschannel0representsthepresenceofabuildingandnotthepresence
of background, the different classes are relatively balanced (compared to other segmentation tasks).
3.3 Further implementation details
WeuseasingleResNet-34-basedU-Net,whichisthesmallestamongthewinningsolution’sarchitectures. Duringtraining,
the following geometric data augmentations are applied to the whole images:
• horizontal flipping
• rotations of 0, 90, 180, 270 degrees;
• random rotations in the range [−10◦,10◦] + random global scaling in the range [.9,1.1]
For each labelled example the same augmentation is applied to before and after images. Afterwards, random square
patches of side length in the range [529,715] are cropped from the data-augmented images with a bias towards patches
that cover the building pixels from less frequent damage classes (using inverse frequency) and resized to side length 608.
4 Experimental setup
The experiments are mainly based on the xView2 competition winner’s code [1]. It uses Python and PyTorch [9] to
implement the models and all of the training and evaluation code. We additionally use Weights&Biases [10] for tracking
experiments.
Hyperparameters For training, we used an AdamW optimizer, a learning rate of 2e-4 and weight decay of 1e-6.
The learning rate was iteratively halved on epoch 5, 11, 17, 23, 29 and 33. Furthermore, we used half-precision and
DataParallel training on three RTX 2080Ti GPUs with a global batch size of 14.
Evaluation metric The competition score is computed as a weighted average of the localization F1 score F1
loc
and the harmonic mean of all building damage F1 scores F1 = 0.3 F1 +0.7 F1 . Since localization and damage
dmg loc dmg
estimation are evaluated separately, the damage predictions are only evaluated where the ground truth labels show a
building. Otherwise, localization errors would influence the damage prediction score.
Dataset splitsFortheablationstudiesinsection5,weusethetrainandtier3subsetsastrainingdata,andrandomly
split off 10% into a validation set. The split is stratified over the disaster events, which is different from the completely
random split used in the winning solution. To compute the test performance, we use the test set. The holdout set is not
used. For the generalization experiments, we use the non-overlapping splits described in subsection 2.1. The validation
dataset is split off in the same way as in the ablation experiments.
5 Ablations: The path to a strong, but simple, baseline
We want to find out which components of the xView2 competition’s winning solution (#1 solution) are important for
its success, and which can be left out, without losing a lot of performance.
Large ensemble The #1 solution consists of an ensemble of four architectures, each trained on three random seeds.
We investigate the effect of this ensemble approach by using the published model weights [1] of the original author.
Table 2 shows that if we simply pick the best performing seed of the smallest model, a ResNet-34-based U-Net, we only
lose 1.5pp (percentage points) in the competition score. This way, we greatly reduce the cost of training and inference.
In the following ablation experiments, we will retrain the model with various changes to the initial setting.
To have a fair point of comparison, we re-run the training of the best individual model among the published models.
This model’s performance will be used as the upper bound of performance that we compare all ablated model versions
7Table 2: Performance of the winning solution for the xView2 challenge
F on main tasks F score on damage classes
1 1
Model Score Local. Damage None Minor Major Destroyed
Ensemble (4 architectures `a 3 seeds) 0.804 0.862 0.779 0.929 0.615 0.778 0.872
ResNet-34 (3 seeds) 0.792 0.859 0.764 0.921 0.592 0.765 0.866
ResNet-34 (best seed) 0.789 0.858 0.759 0.916 0.583 0.769 0.862
ResNet-34 (best seed, our rerun) 0.770 0.854 0.734 0.911 0.544 0.741 0.860
Table 3: Ablation studies: We start from the re-run of the xView2 winner’s solution (top row), and reduce its
complexity step by step. We reach a much simpler baseline approach, while the xView2 competition score only drops
from 0.75 to 0.73. Our model does not use a separate finetuning step with a jump back up in learning rate. We use a
single model for localization and classification, instead of two separate ones. This includes not validating the model on
the competition metric, while using the localization model, but on the loss function instead. We use an equal-weighted
combination of Focal and soft Dice loss. We use class weights for the losses and crop-selection that reflect the class
distribution, instead of optimized weights. We only use geometric augmentations. We do not dilate building classes in
training,northe’minordamage’classintesting. Thecompetitionscoresarethemeanandstandarddeviationoverthree
repetitions with varying random seeds.
Tuned weights Optimized Optimized
Classif. Separate Loc Photom. Dilation Competition
in combo loss loss cropping
pretraining Finetuning method augs train test score
function class weights class weights
Localization ✓ Loc model ✓ ✓ ✓ ✓ ✓ ✓ 0.75 ± 0.02
ImageNet ✓ Loc model ✓ ✓ ✓ ✓ ✓ ✓ 0.75 ± 0.01
ImageNet - Loc model ✓ ✓ ✓ ✓ ✓ ✓ 0.74 ± 0.0
ImageNet - OTSU ✓ ✓ ✓ ✓ - ✓ 0.76 ± 0.01
ImageNet - OTSU - - ✓ ✓ - ✓ 0.74 ± 0.01
ImageNet - OTSU - - - ✓ - ✓ 0.74 ± 0.00
ImageNet - OTSU - - - - - ✓ 0.73 ± 0.01
ImageNet - OTSU - - - - - - 0.73 ± 0.01
ImageNet - Loc channel - - - - - - 0.73 ± 0.01
to. Notably, the retrained model has 1.9pp lower performance than the published weights. One factor that might cause
thisisthechangefromrandomlysplittrain/valsetsintheoriginalcode, toastratifiedsplit, inourexperiment. Another
factor is that the original hyperparameters might be optimized for the used random seeds, which ensure reproducibility
on the same machine, but lead to different behavior when moving to our hardware.
Single modelStartingfromtheResNet-34U-net,weremovedifferentaspectsofthemodel,stepbystep. Theresults
are shown in Table 3. The first row represents the re-run of the best individual model as a reference point. We will walk
through the results here, to also explain the intricacies and tricks used to squeeze out every last bit of performance, and
win the xView2 competition. The resulting competition scores can be seen in Table 2, but won’t be mentioned in the
text, since the performance differences are all rather small.
Pretraining & finetuning The original approach uses two separate models, one for binary building localization,
one for damage classification. The localization model is trained first, and then used as a starting point for the classifi-
cation model. We use an ImageNet-pretrained model as the starting point for classification instead. After training the
classification model, the #1 solution then performs something the original author calls tuning. This consists of training
for 3 more epochs, with a learning rate that is halved every epoch and slightly changed augmentations. The learning
rate is set to a value that’s higher than the one at the end of the training, constituting a slight warm restart. We remove
this separate finetuning step. We increase the number of training epochs from 20 to 40, to account for the fact that
ImageNet pretraining is less close to the building damage segmentation than the localization pretraining. Additionally,
we simplify the thresholding of the localization prediction, which in the #1 solution is a combination of three different
thresholdconditions, appliedtolocalizationandbuildingdamagechannels. Weonlyretainthesimplestthreshold, which
is a fixed threshold applied only to the localization prediction.
IntegratinglocalizationandclassificationAsanextstep,weremovetheseparatelocalizationmodel. Instead,we
usethebuildingdamagepredictions, thataremodeledasfourindependentbinaryclassificationproblems. Ifanyofthem
indicatesthepresenceofbuildingdamage,therespectivepixelmustcontainabuilding. WeapplyOTSU-thresholding[8]
to each building damage channel and combine them with logical OR operations, to compute the localization mask. At
the same time, we also remove test-time dilation, which dilates ’minor damage’ predictions to overwrite ’no damage’
predictions as a post-processing step.
Tuned class and loss weights The #1 solution uses a weighted combination of multiple loss functions (see sub-
section 3.2). For the ResNet34-based model, these are focal and soft dice loss. For other architectures, they also mix
in a cross-entropy loss. We replace the weights for the loss functions with equal weights for both losses. Furthermore,
8each per-class loss is weighted in a way that is likely optimized via search and/or hand-tuned. We replace these weights
with the inverse relative frequency of each class in the training set. The original class weights do not quite follow this
distribution, and also overemphasize the difficult ’minor damage’ class.
Training-time augmentations The #1 solution uses both geometric augmentations (see subsection 3.3), as well
as a range of photometric augmentations: Channel-wise addition in RGB and HSV space, CLAHE, Gaussian noise,
blurring, and changes in saturation, brightness and contrast. We remove the photometric augmentations and retain
only the geometric ones. The #1 solution also dilates all building damage classes during training, again preferring
the ’minor damage’ class if multiple dilated classes overlap. We remove this training-time dilation. During random
cropping, the #1 solution considers multiple candidate crops and computes a preference score, again based on per-class
weights multiplied with the number of pixels of each class in the candidate crop. The crop with the highest preference
score is used. Oversampling buildings makes sense, so we keep this aspect, but again replace the per-class weights with
distribution-based weights.
Localization as fifth channel Lastly, we replace the localization via OTSU thresholding with simply predicting
the presence of buildings as a separate channel. This had been our first choice, since it is simple and clean, but due to a
probleminthecode,itseemedlikethisoptionwasnotworkingwell. InsteadoftheadaptiveOTSUthreshold,wealsogo
back to the simple fixed threshold value set in the original solution. Both options should work in practice. We make this
last change and receive a much simpler method than the one we started with, while only losing two percentage points in
competition score across all of the changes made.
5.1 Aspects we kept
During the ablation process, we attempted some modifications that resulted in a severe drop in performance. Notably,
when we removed the dice part of the combo loss. The dice part in the loss appears to be necessary for learning good
building localization.
Another critical aspect was the importance of choosing the right batch size during training. A batch size that is too
smallcannegativelyimpactthestabilityoftrainingandultimatelyaffectthefinalscore. Thisissuebecomesparticularly
problematic when using a weighted loss function, as some classes may be underrepresented in the dataset. Consequently,
computing the loss for a few images can lead to encountering edge cases, hence making the training unstable.
Otheraspectsthatwedidnottrytochangeweretheinitiallearningrate,learningratescheduleandoptimizer,aswell
as the fixed threshold used for the localization prediction. The building damage predictions are aggregated via argmax.
We also did not try to change the relatively standard geometric data augmentations.
6 Generalization: difficulties on the model-level
To assess the generalization performance of both models, and whether the simpler model is maybe less overfit to the
dataset, we train the model on the new data splits shown in Table 1. We use the same hyperparameters as in section 5,
except that the class weights have been recomputed for the new training set. The results are shown in the respective two
rightmost bars in the columns of Figure 1.
We see that both models overall perform much worse than on the original data split. The localization, as well as
the detection of the ’no damage’ and ’destroyed’ class are only a few percentage points worse than on the old split,
confirming that they are distinct enough to be easily distinguishable. However, the performance on the ’minor damage’
class drops roughly 20% and the ’major damage’ class even drops roughly 50% for both models. These large gaps
in performance between the old, overlapping split, and the new, non-overlapping split, demonstrates a big problem of
generalization. Notably, the ’minor damage’ class is the one that received special attention in the xView2 winner’s
solution. Forexample,itreceivedadisproportionatelyhighclassweightintraining,andpredictionsonitwereartificially
dilated as a post-processing step. This can explain some of the higher performance of the winning solution. Comparing
the simplified baseline with the winning solution, the winning solution is again clearly better than the simplified model,
but the differences between the models’ performances is much smaller than the generalization gap.
Based on the observations about class distributions differing between events (see subsection 2.1), we also investigate
if the performance on the individual events in the test set are associated with the distance between either the overall
training set’s class distribution or the specific event type’s class distribution in training. In both cases, we do not find a
strong association. The detailed results are not reported here.
7 Conclusion
WehaveshownthatagreatlysimplifiedversionofthexView2competitionwinner’sapproachstillachievesanadequate
performance. In the simplified approach, various hyperparameter choices are made in clear, principled ways, making it
easy to train the model on different datasets or different splits of the same dataset. Other parts of the complex approach
areremovedwithoutlosingmuchperformance. Byretrainingbothmodelsonanon-overlappingdatasetsplit,wefindthat
generalizationtounseenlocationsisaproblemforbothmodels. Duetothesimplifiedversion,weknowthatthisproblem
is not caused by the winning model being overfit to the original dataset. After showing that the class distributions vary
greatly between events, we assume that this imbalance in the dataset composition likely contributes to the difficulty of
generalizing to unseen events.
9References
[1] V.Durnov,Xview2 1st place solution,Feb.2020.[Online].Available:https://github.com/vdurnov/xview2_1st_
place_solution (visited on 01/10/2022).
[2] I.P.o.C.Change(IPCC),“WeatherandClimateExtremeEventsinaChangingClimate,”Climate Change 2021 –
ThePhysicalScienceBasis:WorkingGroupIContributiontotheSixthAssessmentReportoftheIntergovernmental
Panel on Climate Change, Cambridge University Press, 2023, pp. 1513–1766. doi: 10.1017/9781009157896.013.
[3] R. Gupta, R. Hosfelt, S. Sajeev, N. Patel, B. Goodman, J. Doshi, E. Heim, H. Choset, and M. Gaston, xBD: A
DatasetforAssessingBuildingDamagefromSatelliteImagery,arXiv:1911.09296[cs],Nov.2019.[Online].Available:
http://arxiv.org/abs/1911.09296 (visited on 03/08/2023).
[4] W.FalconandThePyTorchLightningteam,PyTorchLightning,doi:10.5281/zenodo.3828935,https://www.pytorchlightning.ai,
Lastvisited2021-09-17,Mar.2019.[Online].Available:https://www.pytorchlightning.ai(visitedon09/17/2021).
[5] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks for biomedical image segmentation,”
International Conference on Medical image computing and computer-assisted intervention,Springer,2015,pp.234–
241.
[6] L. Beyer, O. J. H´enaff, A. Kolesnikov, X. Zhai, and A. v. d. Oord, Are we done with ImageNet? arXiv:2006.07159
[cs], Jun. 2020. [Online]. Available: http://arxiv.org/abs/2006.07159 (visited on 01/30/2024).
[7] S. Kornblith, T. Chen, H. Lee, and M. Norouzi, “Why do better loss functions lead to less transferable features?”
Advances in Neural Information Processing Systems, vol. 34, pp. 28648–28662, 2021.
[8] N. Otsu, “A Threshold Selection Method from Gray-Level Histograms,” IEEE Transactions on Systems, Man,
and Cybernetics, vol. 9, no. 1, pp. 62–66, Jan. 1979, Conference Name: IEEE Transactions on Systems, Man, and
Cybernetics. doi: 10.1109/TSMC.1979.4310076.
[9] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga,
A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai,
and S. Chintala, “PyTorch: An Imperative Style, High-Performance Deep Learning Library,” Advances in Neural
Information Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer, F. d. Alch´e-Buc, E. Fox, and R.
Garnett, Eds., Curran Associates, Inc., 2019, pp. 8024–8035. [Online]. Available: http://papers.neurips.cc/
paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.
[10] L. Biewald, Experiment Tracking with Weights and Biases, 2020. [Online]. Available: https://www.wandb.com/.
10A Contributions according to the CRediT system
Description of the CRediT system: https://credit.niso.org/
• Sebastian Gerard: Conceptualization, Formal analysis, Investigation, Methodology, Software, Writing – original
draft, Writing – review & editing
• Paul Borne-Pons: Formal analysis, Investigation, Methodology, Software, Validation, Visualization, Writing – orig-
inal draft
• Josephine Sullivan: Conceptualization, Funding acquisition, Project administration, Resources, Supervision, Writ-
ing – review & editing
11