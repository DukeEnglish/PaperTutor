YOLO-World: Real-Time Open-Vocabulary Object Detection
TianhengCheng3,2,∗, LinSong1,∗,✉, YixiaoGe1,2,†, WenyuLiu3, XinggangWang3,✉, YingShan1,2
∗equalcontribution †projectlead ✉correspondingauthor
1 TencentAILab 2 ARCLab,TencentPCG
3 SchoolofEIC,HuazhongUniversityofScience&Technology
Code&Models: YOLO-World
Abstract
The You Only Look Once (YOLO) series of detectors
haveestablishedthemselvesasefficientandpracticaltools.
However, their reliance on predefined and trained ob-
ject categories limits their applicability in open scenar-
ios. Addressingthislimitation,weintroduceYOLO-World, 20×Speedup
an innovative approach that enhances YOLO with open-
vocabulary detection capabilities through vision-language
modelingandpre-trainingonlarge-scaledatasets. Specif-
ically, we propose a new Re-parameterizable Vision-
Language Path Aggregation Network (RepVL-PAN) and
region-text contrastive loss to facilitate the interaction be-
tweenvisualandlinguisticinformation. Ourmethodexcels
in detecting a wide range of objects in a zero-shot man-
ner with high efficiency. On the challenging LVIS dataset,
YOLO-World achieves 35.4 AP with 52.0 FPS on V100, Figure 1. Speed-and-Accuracy Curve. We compare YOLO-
which outperforms many state-of-the-art methods in terms Worldwithrecentopen-vocabularymethodsintermsofspeedand
accuracy.AllmodelsareevaluatedontheLVISminivalandin-
of both accuracy and speed. Furthermore, the fine-tuned
ferencespeedsaremeasuredononeNVIDIAV100w/oTensorRT.
YOLO-Worldachievesremarkableperformanceonseveral
Thesizeofthecirclerepresentsthemodel’ssize.
downstream tasks, including object detection and open-
vocabularyinstancesegmentation.
scenarios.
Recent works [7, 12, 49, 54? ] have explored the
1.Introduction prevalentvision-languagemodels[18,36]toaddressopen-
vocabulary detection [54] through distilling vocabulary
Objectdetectionhasbeenalong-standingandfundamental knowledgefromlanguageencoders,e.g.,BERT[5]. How-
challengeincomputervisionwithnumerousapplicationsin ever,thesedistillation-basedmethodsaremuchlimiteddue
image understanding, robotics, and autonomous vehicles. to the scarcity of training data with a limited diversity of
Tremendous works [15, 26, 40, 42] have achieved signif- vocabulary, e.g., OV-COCO [54] containing 48 base cate-
icant breakthroughs in object detection with the develop- gories.Severalmethods[23,29,52,53,55]reformulateob-
mentofdeepneuralnetworks. Despitethesuccessofthese ject detection training as region-level vision-language pre-
methods,theyremainlimitedastheyonlyhandleobjectde- trainingandtrainopen-vocabularyobjectdetectorsatscale.
tection with a fixed vocabulary, e.g., 80 categories in the However,thosemethodsstillstrugglefordetectioninreal-
COCO[25]dataset. Onceobjectcategoriesaredefinedand worldscenarios, which sufferfromtwo aspects: (1) heavy
labeled,traineddetectorscanonlydetectthosespecificcat- computation burden and (2) complicated deployment for
egories, thus limiting the ability and applicability of open edge devices. Previous works [23, 29, 52, 53, 55] have
1
4202
naJ
03
]VC.sc[
1v07271.1042:viXrademonstrated the promising performance of pre-training to connect vision and language features and an open-
large detectors while pre-training small detectors to en- vocabulary region-text contrastive pre-training scheme
dow them with open recognition capabilities remains un- forYOLO-World.
explored. • The proposed YOLO-World pre-trained on large-scale
In this paper, we present YOLO-World, aiming for datasets demonstrates strong zero-shot performance and
high-efficiency open-vocabulary object detection, and ex- achieves35.4APonLVISwith52.0FPS.Thepre-trained
plore large-scale pre-training schemes to boost the tradi- YOLO-Worldcanbeeasilyadaptedtodownstreamtasks,
tional YOLO detectors to a new open-vocabulary world. e.g.,open-vocabularyinstancesegmentationandreferring
Compared to previous methods, the proposed YOLO- object detection. Moreover, the pre-trained weights and
World is remarkably efficient with high inference speed codesofYOLO-Worldwillbeopen-sourcedtofacilitate
and easy to deploy for downstream applications. Specif- morepracticalapplications.
ically, YOLO-World follows the standard YOLO archi-
tecture [19] and leverages the pre-trained CLIP [36] text 2.RelatedWorks
encoder to encode the input texts. We further propose
2.1.TraditionalObjectDetection
theRe-parameterizableVision-LanguagePathAggregation
Network (RepVL-PAN) to connect text features and im- Prevalent object detection research concentrates on fixed-
agefeaturesforbettervisual-semanticrepresentation. Dur- vocabulary (close-set) detection, in which object detectors
ing inference, the text encoder can be removed and the are trained on datasets with pre-defined categories, e.g.,
text embeddings can be re-parameterized into weights of COCO dataset [25] and Objects365 dataset [43], and then
RepVL-PAN for efficient deployment. We further inves- detect objects within the fixed set of categories. During
tigate the open-vocabulary pre-training scheme for YOLO the past decades, the methods for traditional object de-
detectorsthroughregion-textcontrastivelearningonlarge- tection can be simply categorized into three groups, i.e.,
scaledatasets,whichunifiesdetectiondata,groundingdata, region-based methods, pixel-based methods, and query-
and image-text data into region-text pairs. The pre-trained basedmethods. Theregion-basedmethods[10,11,15,26,
YOLO-Worldwithabundantregion-textpairsdemonstrates 41], such as Faster R-CNN [41], adopt a two-stage frame-
astrongcapabilityforlargevocabularydetectionandtrain- work for proposal generation [41] and RoI-wise (Region-
ing more data leads to greater improvements in open- of-Interest) classification and regression. The pixel-based
vocabularycapability. methods [27, 30, 39, 45, 57] tend to be one-stage detec-
In addition, we explore a prompt-then-detect paradigm tors, which perform classification and regression over pre-
tofurtherimprovetheefficiencyofopen-vocabularyobject defined anchors or pixels. DETR [1] first explores object
detection in real-world scenarios. As illustrated in Fig. 2, detection through transformers [46] and inspires extensive
traditional object detectors [15, 19, 22, 38–40, 48] con- query-based methods [60]. In terms of inference speed,
centrate on the fixed-vocabulary (close-set) detection with Redmonetal.presentsYOLOs[37–39]whichexploitsim-
predefined and trained categories. While previous open- ple convolutional architectures for real-time object detec-
vocabularydetectors[23,29,52,55]encodethepromptsof tion. Several works[9,22,32,48,51]propose variousar-
a user for online vocabulary with text encoders and detect chitectures or designs for YOLO, including path aggrega-
objects. Notably, those methods tend to employ large de- tion networks [28], cross-stage partial networks [47], and
tectorswithheavybackbones,e.g.,Swin-L[31],toincrease re-parameterization [6], which further improve both speed
theopen-vocabularycapacity. Incontrast,theprompt-then- and accuracy. In comparison to previous YOLOs, YOLO-
detect paradigm (Fig. 2 (c)) first encodes the prompts of a Worldinthispaperaimstodetectobjectsbeyondthefixed
usertobuildanofflinevocabularyandthevocabularyvaries vocabularywithstronggeneralizationability.
with different needs. Then, the efficient detector can infer
2.2.Open-VocabularyObjectDetection
the offline vocabulary on the fly without re-encoding the
prompts. For practical applications, once we have trained
Open-vocabularyobjectdetection(OVD)[54]hasemerged
the detector, i.e., YOLO-World, we can pre-encode the
as a new trend for modern object detection, which aims
prompts or categories to build an offline vocabulary and
to detect objects beyond the predefined categories. Early
thenseamlesslyintegrateitintothedetector.
works [12] follow the standard OVD setting [54] by train-
Our main contributions can be summarized into three ing detectors on the base classes and evaluating the novel
folds: (unknown)classes. Nevertheless,thisopen-vocabularyset-
• We introduce the YOLO-World, a cutting-edge open- ting can evaluate the capability of detectors to detect and
vocabulary object detector with high efficiency for real- recognize novel objects, it is still limited for open sce-
worldapplications. narios and lacks generalization ability to other domains
• WeproposeaRe-parameterizableVision-LanguagePAN due to training on the limited dataset and vocabulary.
2Train-only
User User User
Fixed Text Text Offline
Vocabulary Encoder Online Encoder Vocabulary
Vocabulary
Re-parameterize
Text Encoder
Object Detector Large Detector Lightweight Detector
(a) Traditional Object Detector (b) PreivousOpen-Vocabulary Detector (c) YOLO-World
Figure 2. Comparison with Detection Paradigms. (a) Traditional Object Detector: These object detectors can only detect objects
withinthefixedvocabularypre-definedbythetrainingdcaatpatsieotns,, en.go.u,n8 0pchartaesgeosr,i ecsaotefgCoOryC…Odataset[25]. Thefixedvocabularylimitsthe
extensionforopenscenes. (b)PreviousOpen-VocabularyDetectors: Previousmethodstendtodeveloplargeandheavydetectorsfor
Text Embeddings
open-vocabularydetectionwhichintuitivelyhavestrongcapacity. Inaddition,thesedetectorssimultaneouslyencodeimagesandtextsas
inputforprediction,whichistime-consumingforpracticalapplications. (c)YOLO-World: Wedemonstratethestrongopen-vocabulary
performanceoflightweightdetectors,e.g.,YOLOdetectors[19,39],whichisofgreatsignificanceforreal-worldapplications.Ratherthan Dog
usingonlinevocabulary,wepresentaprompt-then-detectparadigmforefficientinference,inwhichtheusergeneratesaseriesofprompts
Detector
accordingtotheneedandthepromptswillbeencodedintoanofflinevocabulary. Thenitcanbere-parameterizedasthemodelweights
fordeploymentandfurtheracceleration.
Vocabulary Embeddings Image-aware Embeddings Region-Text Matching
Training: Online Vocabulary
man man
A manand a Text
womanare skiing woman woman
Encoder
with a dog dog dog
Extract Nouns
Deployment: Offline Vocabulary
User’s
Vocabulary Multi-scale Object Embeddings
User
Image Features
Text
Contrastive Head
YOLO
Backbone
Box Head
InputImage
Figure3.OverallArchitectureofYOLO-World.ComparedtotraditionalYOLOdetectors,YOLO-Worldasanopen-vocabularydetector
adoptstextasinput.TheTextEncoderfirstencodestheinputtextinputtextembeddings.ThentheImageEncoderencodestheinputimage
Multi-scale Image Features
intomulti-scaleimagefeaturesandtheproposedRepVL-PANexploitsthemulti-levelcross-modalityfusionforbothimageandtextfeatures.
Finally,YOLO-Worldpredictstheregressedboundingboxesandtheobjectembeddingsformatchingthecategoriesornounsthatappeared
intheinputtext.
Inspired by vision-language pre-training [18, 36], recent these methods often use heavy detectors like ATSS [57]
works [7, 21, 49, 58, 59] formulate open-vocabulary ob- or DINO [56] with Swin-L [31] as a backbone, lead-
ject detection as image-text matching and exploit large- ing to high computational demands and deployment chal-
scale image-text data to increase the training vocabulary lenges. In contrast, we present YOLO-World, aiming for
at scale. GLIP [23] presents a pre-training framework for efficient open-vocabulary object detection with real-time
open-vocabulary detection based on phrase grounding and inference and easier downstream application deployment.
evaluates in a zero-shot setting. Grounding DINO [29] DifferingfromZSD-YOLO[50],whichalsoexploresopen-
incorporates the grounded pre-training [23] into detection vocabulary detection [54] with YOLO through language
transformers[56]withcross-modalityfusions. model alignment, YOLO-World introduces a novel YOLO
framework with an effective pre-training strategy, enhanc-
Severalmethods[24,52,53,55]unifydetectiondatasets
ingopen-vocabularyperformanceandgeneralization.
and image-text datasets through region-text matching and
pre-traindetectorswithlarge-scaleimage-textpairs,achiev-
ing promising performance and generalization. However,
3
Vision-Language
PAN3.Method transformationwiththelearnablescalingfactorαandshift-
ingfactorβ. BoththeL2normsandtheaffinetransforma-
3.1.Pre-trainingFormulation: Region-TextPairs
tionsareimportantforstabilizingtheregion-texttraining.
The traditional object detection methods, including the
YOLO-series [19], are trained with instance annotations Training with Online Vocabulary. During training, we
Ω = {B i,c i}N i=1, which consist of bounding boxes {B i} construct an online vocabulary T for each mosaic sample
andcategorylabels{c i}. Inthispaper,wereformulatethe containing 4 images. Specifically, we sample all positive
instance annotations as region-text pairs Ω = {B i,t i}N i=1, nouns involved in the mosaic images and randomly sam-
wheret iisthecorrespondingtextfortheregionB i. Specif- ple some negative nouns from the corresponding dataset.
ically, the text t i can be the category name, noun phrases, ThevocabularyforeachmosaicsamplecontainsatmostM
orobjectdescriptions.Moreover,YOLO-Worldadoptsboth nouns,andM issetto80asdefault.
theimageIandtextsT (asetofnouns)asinputandoutputs
predictedboxes{Bˆ }andthecorrespondingobjectembed-
k Inference with Offline Vocabulary. At the inference
dings{e }(e ∈RD).
k k stage,wepresentaprompt-then-detectstrategywithanof-
3.2.ModelArchitecture flinevocabularyforfurtherefficiency. AsshowninFig.3,
theusercandefineaseriesofcustomprompts,whichmight
TheoverallarchitectureoftheproposedYOLO-Worldisil-
includecaptionsorcategories. Wethenutilizethetexten-
lustrated in Fig. 3, which consists of a YOLO detector, a
coder to encode these prompts and obtain offline vocabu-
Text Encoder, and a Re-parameterizable Vision-Language
laryembeddings. Theofflinevocabularyallowsforavoid-
PathAggregationNetwork (RepVL-PAN).Giventheinput
ingcomputation foreach inputand providesthe flexibility
text,thetextencoderinYOLO-Worldencodesthetextinto
toadjustthevocabularyasneeded.
textembeddings. TheimageencoderintheYOLOdetector
extractsthemulti-scalefeaturesfromtheinputimage.Then 3.3.Re-parameterizableVision-LanguagePAN
weleveragetheRepVL-PANtoenhancebothtextandim-
Fig. 4 shows the structure of the proposed RepVL-PAN
age representation by exploiting the cross-modality fusion
whichfollowsthetop-downandbottom-uppathsin[19,28]
betweenimagefeaturesandtextembeddings.
to establish the feature pyramids {P ,P ,P } with the
3 4 5
multi-scale image features {C ,C ,C }. Furthermore,
YOLO Detector. YOLO-World is mainly developed 3 4 5
we propose the Text-guided CSPLayer (T-CSPLayer) and
based on YOLOv8 [19], which contains a Darknet back-
Image-Pooling Attention (I-Pooling Attention) to further
bone[19,40]astheimageencoder,apathaggregationnet-
enhance the interaction between image features and text
work (PAN) for multi-scale feature pyramids, and a head
features, which can improve the visual-semantic represen-
forboundingboxregressionandobjectembeddings.
tationforopen-vocabularycapability. Duringinference,the
offlinevocabularyembeddingscanbere-parameterizedinto
Text Encoder. Given the text T, we adopt the Trans-
weightsofconvolutionalorlinearlayersfordeployment.
formertextencoderpre-trainedbyCLIP[36]toextractthe
correspondingtextembeddingsW=TextEncoder(T)∈
RC×D, where C is the number of nouns and D is the em- Text-guided CSPLayer. As Fig. 4 illustrates, the cross-
stage partial layers (CSPLayer) are utilized after the top-
bedding dimension. The CLIP text encoder offers better
down or bottom-up fusion. We extend the CSPLayer
visual-semantic capabilities for connecting visual objects
(also called C2f) of [19] by incorporating text guidance
with texts compared to text-only language encoders [5].
into multi-scale image features to form the Text-guided
When the input text is a caption or referring expression,
CSPLayer. Specifically, giventhetextembeddingsW and
we adopt the simple n-gram algorithm to extract the noun
image features X ∈ RH×W×D (l ∈ {3,4,5}), we adopt
phrasesandthenfeedthemintothetextencoder. l
the max-sigmoid attention after the last dark bottleneck
blocktoaggregatetextfeaturesintoimagefeaturesby:
Text Contrastive Head. Following previous works [19],
weadoptthedecoupledheadwithtwo3×3convstoregress X′ =X ·δ( max (X W⊤))⊤, (2)
boundingboxes{b k}K
k=1
andobjectembeddings{e k}K k=1, l l j∈{1..C} l j
whereK denotesthenumberofobjects. Wepresentatext
wheretheupdatedX′ isconcatenatedwiththecross-stage
contrastiveheadtoobtaintheobject-textsimilaritys k,j by: l
featuresasoutput. Theδindicatesthesigmoidfunction.
s =α·L2-Norm(e )·L2-Norm(w )⊤+β, (1)
k,j k j
where L2-Norm(·) is the L2 normalization and w ∈ W Image-Pooling Attention. To enhance the text embed-
j
is the j-th text embeddings. In addition, we add the affine dings with image-aware information, we aggregate image
4man
woman
dog=
Text Embeddings Image-aware Embeddings noisyboxes,weonlycalculatetheregressionlossforsam-
pleswithaccurateboundingboxes.
C T-CSPLayer P
5 5
1 PseudoLabelingwithImage-TextData. Ratherthandi-
×2 ×
2 rectlyusingimage-textpairsforpre-training,weproposean
C 4 T-CSPLayer T-CSPLayer P 4 automatic labeling approach to generate region-text pairs.
1 Specifically,thelabelingapproachcontainsthreesteps: (1)
×2 ×
2
extract noun phrases: we first utilize the n-gram algo-
C 3 T-CSPLayer P 3 rithmtoextractnounphrasesfromthetext; (2)pseudola-
Text to Image Image to Text beling: we adopt a pre-trained open-vocabulary detector,
e.g., GLIP [23], to generate pseudo boxes for the given
S Split C Concat Text 3×3 Text noun phrases for each image, thus providing the coarse
S Dark Bottleneck Max-Sigmoid C C MHCA region-text pairs. (3) filtering: We employ the pre-trained
CLIP[36]toevaluatetherelevanceofimage-textpairsand
region-text pairs, and filter the low-relevance pseudo an-
T-CSPLayer (C2f Block) I-PoolingAttention
notations and images. We further filter redundant bound-
Figure4.IllustrationoftheRepVL-PAN.TheproposedRepVL- ingboxesbyincorporatingmethodssuchasNon-Maximum
PANadoptstheText-guidedCSPLayer(T-CSPLayer)forinjecting Suppression(NMS).Wesuggestthereadersrefertotheap-
languageinformationintoimagefeaturesandtheImagePooling pendixforthedetailedapproach. Withtheaboveapproach,
Attention (I-Pooling Attention) for enhancing image-aware text
we sample and label 246k images from CC3M [44] with
embeddings.
821kpseudoannotations.
features to update the text embeddings by proposing the
4.Experiments
Image-PoolingAttention. Ratherthandirectlyusingcross-
attention on image features, we leverage max pooling on
In this section, we demonstrate the effectiveness of the
multi-scale features to obtain 3×3 regions, resulting in a
totalof27patchtokensX˜ ∈R27×D. Thetextembeddings proposed YOLO-World by pre-training it on large-scale
datasetsandevaluatingYOLO-Worldinazero-shotmanner
arethenupdatedby:
onbothLVISbenchmarkandCOCObenchmark(Sec.4.2).
W′ =W +MultiHead-Attention(W,X˜,X˜) (3) We also evaluate the fine-tuning performance of YOLO-
WorldonCOCO,LVISforobjectdetection.
3.4.Pre-trainingSchemes
4.1.ImplementationDetails
In this section, we present the training schemes for pre-
trainingYOLO-Worldonlarge-scaledetection,grounding, The YOLO-World is developed based on the MMYOLO
andimage-textdatasets. toolbox [3] and the MMDetection toolbox [2]. Following
[19], weprovidethreevariantsofYOLO-Worldfordiffer-
entlatencyrequirements,e.g.,small(S),medium(M),and
Learning from Region-Text Contrastive Loss. Given
large(L).Weadopttheopen-sourceCLIP[36]textencoder
the mosaic sample I and texts T, YOLO-World outputs
with pre-trained weights to encode the input text. Unless
K object predictions {B ,s }K along with annotations
k k k=1 specified,wemeasuretheinferencespeedsofallmodelson
Ω={B ,t }N . Wefollow[19]andleveragetask-aligned
i i i=1 oneNVIDIAV100GPUwithoutextraaccelerationmecha-
labelassignment[8]tomatchthepredictionswithground- nisms,e.g.,FP16orTensorRT.
truthannotationsandassigneachpositivepredictionwitha
textindexastheclassificationlabel. Basedonthisvocabu-
4.2.Pre-training
lary,weconstructtheregion-textcontrastivelossL with
con
region-textpairsthroughcrossentropybetweenobject-text ExperimentalSetup. Atthepre-trainingstage,weadopt
(region-text)similarityandobject-textassignments. Inad- the AdamW optimizer [33] with an initial learning rate
dition, we adopt IoU loss and distributed focal loss for of 0.002 and weight decay of 0.05. YOLO-World is pre-
bounding box regression and the total training loss is de- trainedfor100epochsonon32NVIDIAV100GPUswith
fined as: L(I) = L + λ · (L + L ), where λ is a total batch size of 512. During pre-training, we follow
con I iou dfl I
anindicatorfactorandsetto1wheninputimageI isfrom previousworks[19]andadoptcoloraugmentation,random
detection or grounding data and set to 0 when it is from affine,randomflip,andmosaicwith4imagesfordataaug-
the image-text data. Considering image-text datasets have mentation. Thetextencoderisfrozenduringpre-training.
5
I-Pooling
Attention
I-Pooling
Attention4.3.AblationExperiments
Dataset Type Vocab. Images Anno.
Objects365V1[43] Detection 365 609k 9,621k We provide extensive ablation studies to analyze YOLO-
GQA[16] Grounding - 621k 3,681k World from two primary aspects, i.e., pre-training and ar-
Flickr[35] Grounding - 149k 641k chitecture. Unless specified, we mainly conduct ablation
CC3M†[44] Image-Text - 246k 821k experiments based on YOLO-World-L and pre-train Ob-
jects365withzero-shotevaluationonLVISminival.
Table 1. Pre-training Data. The specifications of the datasets
usedforpre-trainingYOLO-World.
Pre-training Data. In Tab. 3, we evaluate the perfor-
mance of pre-training YOLO-World using different data.
Pre-training Data. For pre-training YOLO-World, we Compared to the baseline trained on Objects365, adding
mainlyadoptdetectionorgroundingdatasetsincludingOb- GQA can significantly improve performance with an 8.4
jects365(V1)[43],GQA[16],Flickr30k[35],asspecified AP gain on LVIS. This improvement can be attributed to
in Tab. 1. Following [23], we exclude the images from therichertextualinformationprovidedbytheGQAdataset,
the COCO dataset in GoldG [20] (GQA and Flickr30k). which can enhance the model’s ability to recognize large
The annotations of the detection datasets used for pre- vocabulary objects. Adding part of CC3M samples (8%
training contain both bounding boxes and categories or ofthefulldatasets)canfurtherbring0.5APgainwith1.3
noun phrases. In addition, we also extend the pre-training AP on rare objects. Tab. 3 demonstrates that adding more
datawithimage-textpairs,i.e.,CC3M†[44],whichwehave
data can effectively improve the detection capabilities on
labeled 246k images through the pseudo-labeling method large-vocabularyscenarios. Furthermore,astheamountof
discussedinSec.3.4. dataincreases,theperformancecontinuestoimprove,high-
lighting the benefits of leveraging larger and more diverse
datasetsfortraining.
Zero-shot Evaluation. After pre-training, we di-
rectly evaluate the proposed YOLO-World on the LVIS Ablations on RepVL-PAN. Tab. 4 demonstrates the ef-
dataset [13] in a zero-shot manner. The LVIS dataset fectivenessoftheproposedRepVL-PANofYOLO-World,
contains 1203 object categories, which is much more including Text-guided CSPLayers and Image Pooling At-
than the categories of the pre-training detection datasets tention, for the zero-shot LVIS detection. Specifically, we
and can measure the performance on large vocabulary adopt two settings, i.e., (1) pre-training on O365 and (2)
detection. Following previous works [20, 23, 52, 53], we pre-training on O365 & GQA. Compared to O365 which
mainly evaluate on LVIS minival [20] and report the only contains category annotations, GQA includes rich
Fixed AP [4] for comparison. The maximum number of texts, particularly in the form of noun phrases. As shown
predictionsissetto1000. in Tab. 4, the proposed RepVL-PAN improves the base-
line(YOLOv8-PAN[19])by1.1APonLVIS,andtheim-
provements are remarkable in terms of the rare categories
(AP )ofLVIS,whicharehardtodetectandrecognize. In
MainResultsonLVISObjectDetection. InTab.2, we r
addition, theimprovementsbecomemoresignificantwhen
compare the proposed YOLO-World with recent state-of-
YOLO-World is pre-trained with the GQA dataset and ex-
the-artmethods[20,29,52,53,55]onLVISbenchmarkina
periments indicate that the proposed RepVL-PAN works
zero-shotmanner. Consideringthecomputationburdenand
betterwithrichtextualinformation.
modelparameters,wemainlycomparewiththosemethods
basedonlighterbackbones,e.g.,Swin-T[31]. Remarkably,
YOLO-World outperforms previous state-of-the-art meth- Text Encoders. In Tab. 5, we compare the performance
odsintermsofzero-shotperformanceandinferencespeed. of using different text encoders, i.e., BERT-base [5] and
ComparedtoGLIP,GLIPv2,andGroundingDINO,which CLIP-base (ViT-base) [36]. We exploit two settings dur-
incorporate more data, e.g., Cap4M (CC3M+SBU [34]), ing pre-training, i.e., frozen and fine-tuned, and the learn-
YOLO-World pre-trained on O365 & GolG obtains bet- ing rate for fine-tuning text encoders is a 0.01× factor of
ter performance even with fewer model parameters. Com- the basic learning rate. As Tab. 5 shows, the CLIP text
paredtoDetCLIP,YOLO-Worldachievescomparableper- encoder obtains superior results than BERT (+10.1 AP for
formance (35.4 v.s. 34.4) while obtaining 20× increase in rare categories in LVIS), which is pre-trained with image-
inferencespeed. Theexperimentalresultsalsodemonstrate textpairsandhasbettercapabilityforvision-centricembed-
that small models, e.g., YOLO-World-S with 13M parame- dings. Fine-tuningBERTduringpre-trainingbringssignifi-
ters, can be used for vision-language pre-training and ob- cantimprovements(+3.7AP)whilefine-tuningCLIPleads
tainstrongopen-vocabularycapabilities. toasevereperformancedrop. Weattributethedroptothat
6Method Backbone Params Pre-trainedData FPS AP AP AP AP
r c f
MDETR[20] R-101[14] 169M GoldG - 24.2 20.9 24.3 24.2
GLIP-T[23] Swin-T[31] 232M O365,GoldG 0.12 24.9 17.7 19.5 31.0
GLIP-T[23] Swin-T[31] 232M O365,GoldG,Cap4M 0.12 26.0 20.8 21.4 31.0
GLIPv2-T[55] Swin-T[31] 232M O365,GoldG 0.12 26.9 - - -
GLIPv2-T[55] Swin-T[31] 232M O365,GoldG,Cap4M 0.12 29.0 - - -
GroundingDINO-T[29] Swin-T[31] 172M O365,GoldG 1.5 25.6 14.4 19.6 32.2
GroundingDINO-T[29] Swin-T[31] 172M O365,GoldG,Cap4M 1.5 27.4 18.1 23.3 32.7
DetCLIP-T[52] Swin-T[31] 155M O365,GoldG 2.3 34.4 26.9 33.9 36.3
YOLO-World-S YOLOv8-S 13M(77M) O365,GoldG 74.1(19.9) 26.2 19.1 23.6 29.8
YOLO-World-M YOLOv8-M 29M(92M) O365,GoldG 58.1(18.5) 31.0 23.8 29.2 33.9
YOLO-World-L YOLOv8-L 48M(110M) O365,GoldG 52.0(17.6) 35.0 27.1 32.8 38.3
YOLO-World-L YOLOv8-L 48M(110M) O365,GoldG,CC3M† 52.0(17.6) 35.4 27.6 34.1 38.0
Table2. Zero-shotEvaluationonLVIS.WeevaluateYOLO-WorldonLVISminival[20]inazero-shotmanner. WereporttheFixed
AP [4] for a fair comparison with recent methods. † denotes the pseudo-labeled CC3M in our setting, which contains 246k samples.
The FPS is evaluated on one NVIDIA V100 GPU w/o TensorRT. The parameters and FPS of YOLO-World are evaluated for both the
re-parameterizedversion(w/obracket)andtheoriginalversion(w/bracket).
Pre-trainedData AP AP AP AP TextEncoder Frozen? AP AP AP AP
r c f r c f
O365 23.5 16.2 21.1 27.0 BERT-base Frozen 14.6 3.4 10.7 20.0
O365,GQA 31.9 22.5 29.9 35.4 BERT-base Fine-tune 18.3 6.6 14.6 23.6
O365,GoldG 32.5 22.3 30.6 36.0 CLIP-base Frozen 22.4 14.5 20.1 26.0
O365,GoldG,CC3M† 33.0 23.6 32.0 35.5 CLIP-base Fine-tune 19.3 8.6 15.7 24.8
Table3. AblationsonPre-trainingData. Weevaluatethezero- Table5.TextEncoderinYOLO-World.Weablatedifferenttext
shotperformanceonLVISofpre-trainingYOLO-Worldwithdif- encodersinYOLO-Worldthroughthezero-shotLVISevaluation.
ferentamountsofdata.
GQA T→I I→T AP AP AP AP
r c f todemonstratetheeffectivenessofthepre-training.
✗ ✗ ✗ 22.4 14.5 20.1 26.0
✗ ✓ ✗ 23.2 15.2 20.6 27.0 Experimental Setup. We use the pre-trained weights to
✗ ✓ ✓ 23.5 16.2 21.1 27.0 initializeYOLO-Worldforfine-tuning.Allmodelsarefine-
✓ ✗ ✗ 29.7 21.0 27.1 33.6 tunedfor80epochswiththeAdamWoptimizerandtheini-
tiallearningrateissetto0.0002. Inaddition,wefine-tune
✓ ✓ ✓ 31.9 22.5 29.9 35.4
theCLIPtextencoderwithalearningfactorof0.01.Forthe
LVISdataset,wefollowpreviousworks[7,12,59]andfine-
Table 4. Ablations on Re-parameterizable Vision-Language
tuneYOLO-WorldontheLVIS-base(common&frequent)
Path Aggregation Network. We evaluate the zero-shot perfor-
andevaluateitontheLVIS-novel(rare).
manceonLVISoftheproposedVision-LanguagePathAggrega-
tionNetwork. T→IandI→TdenotetheText-guidedCSPLayers
andImage-PoolingAttention,respectively. COCO Object Detection. We compare the pre-trained
YOLO-World with previous YOLO detectors [19, 22, 48]
in Tab. 6. For fine-tuning YOLO-World on the COCO
fine-tuningonO365maydegradethegeneralizationability dataset, we remove the proposed RepVL-PAN for fur-
ofthepre-trainedCLIP,whichcontainsonly365categories ther acceleration considering that the vocabulary size of
andlacksabundanttextualinformation. the COCO dataset is small. In Tab. 6, it’s evident that
ourapproachcanachievedecentzero-shotperformanceon
4.4.Fine-tuningYOLO-World
the COCO dataset, which indicates that YOLO-World has
Inthissection,wefurtherfine-tuneYOLO-Worldforclose- strong generalization ability. Moreover, YOLO-World af-
setobjectdetectionontheCOCOdatasetandLVISdataset ter fine-tuning on the COCO train2017 demonstrates
7Method Pre-train AP AP AP FPS Method AP AP AP AP
50 75 r c f
Trainingfromscratch. ViLD[12] 27.8 16.7 26.5 34.2
YOLOv6-S[22] ✗ 43.7 60.8 47.0 442 RegionCLIP[58] 28.2 17.1 - -
YOLOv6-M[22] ✗ 48.4 65.7 52.7 277 Detic[59] 26.8 17.8 - -
YOLOv6-L[22] ✗ 50.7 68.1 54.8 166 FVLM[21] 24.2 18.6 - -
YOLOv7-T[48] ✗ 37.5 55.8 40.2 404 DetPro[7] 28.4 20.8 27.8 32.4
YOLOv7-L[48] ✗ 50.9 69.3 55.3 182 BARON[49] 29.5 23.2 29.3 32.5
YOLOv7-X[48] ✗ 52.6 70.6 57.3 131 YOLOv8-S 19.4 7.4 17.4 27.0
YOLOv8-S[19] ✗ 44.4 61.2 48.1 386 YOLOv8-M 23.1 8.4 21.3 31.5
YOLOv8-M[19] ✗ 50.5 67.3 55.0 238 YOLOv8-L 26.9 10.2 25.4 35.8
YOLOv8-L[19] ✗ 52.9 69.9 57.7 159 YOLO-World-S 23.9 12.8 20.4 32.7
Zero-shottransfer. YOLO-World-M 28.8 15.9 24.6 39.0
YOLO-World-S O+G 37.6 52.3 40.7 - YOLO-World-L 34.1 20.4 31.1 43.5
YOLO-World-M O+G 42.8 58.3 46.4 -
YOLO-World-L O+G 44.4 59.8 48.3 - Table 7. Comparison with Open-Vocabulary Detectors on
LVIS.WetrainYOLO-WorldontheLVIS-base(includingcom-
YOLO-World-L O+G+C 45.1 60.7 48.9 -
monandfrequent)reportthebboxAP.TheYOLO-v8aretrained
Fine-tunedw/RepVL-PAN.
onthefullLVISdatasets(includingbaseandnovel)alongwiththe
YOLO-World-S O+G 45.9 62.3 50.1 - classbalancedsampling.
YOLO-World-M O+G 51.2 68.1 55.9 -
YOLO-World-L O+G+C 53.3 70.1 58.2 -
4.5.Open-VocabularyInstanceSegmentation
Fine-tunedw/oRepVL-PAN.
YOLO-World-S O+G 45.7 62.3 49.9 373 In this section, we further fine-tune YOLO-World for
segmenting objects under the open-vocabulary setting,
YOLO-World-M O+G 50.7 67.2 55.1 231
which can be termed open-vocabulary instance segmenta-
YOLO-World-L O+G+C 53.3 70.3 58.1 156
tion (OVIS). Previous methods [17] have explored OVIS
with pseudo-labelling on novel objects. Differently, con-
Table 6. Comparison with YOLOs on COCO Object Detec-
sidering that YOLO-World has strong transfer and gener-
tion. Wefine-tunetheYOLO-WorldonCOCOtrain2017and
alization capabilities, we directly fine-tune YOLO-World
evaluateonCOCOval2017. TheresultsofYOLOv7[48]and
onasubsetofdatawithmaskannotationsandevaluatethe
YOLOv8[19]areobtainedfromMMYOLO[3].‘O’,‘G’,and‘C’
denotepertainingusingObjects365,GoldG,andCC3M†,respec- segmentationperformanceunderlarge-vocabularysettings.
tively.TheFPSismeasuredononeNVIDIAV100w/TensorRT. Specifically, we benchmark open-vocabulary instance seg-
mentationundertwosettings:
• (1)COCOtoLVISsetting,wefine-tuneYOLO-Worldon
higherperformancecomparedtopreviousmethodstrained the COCO dataset (including 80 categories) with mask
fromscratch. annotations, under which the models need to transfer
from80categoriesto1203categories(80→1203);
• (2)LVIS-basetoLVISsetting,wefine-tuneYOLO-World
on the LVIS-base (including 866 categories, common &
LVIS Object Detection. In Tab. 7, we evaluate the fine-
frequent)withmaskannotations,underwhichthemodels
tuningperformanceofYOLO-WorldonthestandardLVIS
need to transfer from 866 categories to 1203 categories
dataset. Firstly, compared to the oracle YOLOv8s [19]
(866→1203).
trained on the full LVIS datasets, YOLO-World achieves
We evaluate the fine-tuned models on the standard LVIS
significantimprovements,especiallyforlargermodels,e.g.,
val2017 with 1203 categories, in which 337 rare cate-
YOLO-World-L outperforms YOLOv8-L by 7.2 AP and
gories are unseen and can be used to measure the open-
10.2 AP . The improvements can demonstrate the effec-
r
vocabularyperformance.
tiveness of the proposed pre-training strategy for large-
vocabulary detection. Moreover, YOLO-World, as an effi-
cientone-stagedetector,outperformspreviousstate-of-the- Results. Tab.8showstheexperimentalresultsofextend-
arttwo-stagemethods[7,12,21,49,59]ontheoverallper- ing YOLO-World for open-vocabulary instance segmenta-
formancewithoutextradesigns,e.g.,learnableprompts[7] tion. Specifically, we adopt two fine-tuning strategies: (1)
orregion-basedalginments[12]. only fine-tuning the segmentation head and (2) fine-tuning
8all modules. Under strategy (1), the fine-tuned YOLO- correspondingboundingboxes,demonstratingthatthepre-
World still retains the zero-shot capabilities acquired from trained YOLO-World has the referring or grounding capa-
the pre-training stage, allowing it to generalize to unseen bility. This ability can be attributed to the proposed pre-
categories without additional fine-tuning. Strategy (2) en- trainingstrategywithlarge-scaletrainingdata.
ables YOLO-World fit the LVIS dataset better, but it may
resultinthedegradationofthezero-shotcapabilities. 5.Conclusion
Tab. 8 shows the comparisons of fine-tuning YOLO-
We present YOLO-World, a cutting-edge real-time open-
World with different settings (COCO or LVIS-base) and
vocabularydetectoraimingtoimproveefficiencyandopen-
different strategies (fine-tuning seg. head or fine-tuning
vocabularycapabilityinreal-worldapplications. Inthispa-
all). Firstly,fine-tuningonLVIS-baseobtainsbetterperfor-
per, we have reshaped the prevalent YOLOs as a vision-
mancecomparedtothatbasedonCOCO.However,thera-
language YOLO architecture for open-vocabulary pre-
tiosbetweenAPandAP (AP /AP)arenearlyunchanged,
r r
training and detection and proposed RepVL-PAN, which
e.g., the ratios of YOLO-World on COCO and LVIS-base
connectsvisionandlanguageinformationwiththenetwork
are 76.5% and 74.3%, respectively. Considering that the
and can be re-parameterized for efficient deployment. We
detector is frozen, we attribute the performance gap to the
further present the effective pre-training schemes with de-
factthattheLVISdatasetprovidesmoredetailedanddenser
tection, grounding and image-text data to endow YOLO-
segmentation annotations, which are beneficial for learn-
World with a strong capability for open-vocabulary de-
ing the segmentation head. When fine-tuning all mod-
tection. Experiments can demonstrate the superiority of
ules, YOLO-World obtains remarkable improvements on
YOLO-World in terms of speed and open-vocabulary per-
LVIS,e.g.,YOLO-World-Lachieves9.6APgain.However,
formanceandindicatetheeffectivenessofvision-language
the fine-tuning might degrade the open-vocabulary perfor-
pre-trainingonsmallmodels,whichisinsightfulforfuture
manceandleadtoa0.6boxAP dropforYOLO-World-L.
r
research. WehopeYOLO-Worldcanserveasanewbench-
4.6.Visualizations markforaddressingreal-worldopen-vocabularydetection.
We provide the visualization results of pre-trained YOLO-
References
World-L under three settings: (a) we perform zero-shot
inference with LVIS categories; (b) we input the custom [1] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas
promptswithfine-grainedcategorieswithattributes;(c)re- Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-
ferring detection. The visualizations also demonstrate that to-endobjectdetectionwithtransformers. InECCV,pages
213–229,2020. 2
YOLO-World has a strong generalization ability for open-
vocabularyscenariosalongwithreferringability. [2] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu
Xiong,XiaoxiaoLi,ShuyangSun,WansenFeng,ZiweiLiu,
JiaruiXu,ZhengZhang,DazhiCheng,ChenchenZhu,Tian-
Zero-shot Inference on LVIS. Fig. 5 shows the visual- hengCheng,QijieZhao,BuyuLi,XinLu,RuiZhu,YueWu,
izationresultsbasedontheLVIScategorieswhicharegen- Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang,
erated by the pre-trained YOLO-World-L in a zero-shot Chen Change Loy, and Dahua Lin. MMDetection: Open
manner.Thepre-trainedYOLO-Worldexhibitsstrongzero- mmlab detection toolbox and benchmark. arXiv preprint
shot transfer capabilities and is able to detect as many ob- arXiv:1906.07155,2019. 5
jectsaspossiblewithintheimage. [3] MMYOLOContributors. MMYOLO:OpenMMLabYOLO
seriestoolboxandbenchmark. https://github.com/
open-mmlab/mmyolo,2022. 5,8
InferencewithUser’sVocabulary. InFig.6,weexplore [4] Achal Dave, Piotr Dolla´r, Deva Ramanan, Alexander Kir-
thedetectioncapabilitiesofYOLO-Worldwithourdefined illov, and Ross B. Girshick. Evaluating large-vocabulary
categories. The visualization results demonstrate that the object detectors: The devil is in the details. CoRR,
pre-trainedYOLO-World-Lalsoexhibitsthecapabilityfor abs/2102.01066,2021. 6,7
(1) fine-grained detection (i.e., detect the parts of one ob- [5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
ject)and(2)fine-grainedclassification(i.e.,distinguishdif- Toutanova. BERT:pre-trainingofdeepbidirectionaltrans-
ferentsub-categoriesofobjects.). formersforlanguageunderstanding. InNAACL-HLT,pages
4171–4186,2019. 1,4,6
[6] XiaohanDing,XiangyuZhang,NingningMa,JungongHan,
ReferringObjectDetection. InFig.7,weleveragesome
Guiguang Ding, and Jian Sun. Repvgg: Making vgg-style
descriptive(discriminative)nounphrasesasinput,e.g.,the convnetsgreatagain. InCVPR,pages13733–13742,2021.
standing person, to explore whether the model can locate 2
regions or objects in the image that match our given in- [7] YuDu,FangyunWei,ZiheZhang,MiaojingShi,YueGao,
put. Thevisualizationresultsdisplaythephrasesandtheir andGuoqiLi. Learningtopromptforopen-vocabularyob-
9Model Fine-tuneData Fine-tuneModules AP AP AP AP APb APb
r c f r
YOLO-World-M COCO SegHead 12.3 9.1 10.9 14.6 22.3 16.2
YOLO-World-L COCO SegHead 16.2 12.4 15.0 19.2 25.3 18.0
YOLO-World-M LVIS-base SegHead 16.7 12.6 14.6 20.8 22.3 16.2
YOLO-World-L LVIS-base SegHead 19.1 14.2 17.2 23.5 25.3 18.0
YOLO-World-M LVIS-base All 25.9 13.4 24.9 32.6 32.6 15.8
YOLO-World-L LVIS-base All 28.7 15.0 28.3 35.2 36.2 17.4
Table8.Open-VocabularyInstanceSegmentation.WeevaluateYOLO-Worldforopen-vocabularyinstancesegmentationunderthetwo
settings. Wefine-tunethesegmentationheadorallmodulesofYOLO-WorldandreportMaskAPforcomparison. APb denotesthebox
AP.
Figure 5. Visualization Results on Zero-shot Inference on LVIS. We adopt the pre-trained YOLO-World-L and infer with the LVIS
vocabulary(containing1203categories)ontheCOCOval2017.
jectdetectionwithvision-languagemodel. InCVPR,pages Deep residual learning for image recognition. In CVPR,
14064–14073,2022. 1,3,7,8 pages770–778,2016. 7
[8] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew R. Scott, [15] Kaiming He, Georgia Gkioxari, Piotr Dolla´r, and Ross B.
andWeilinHuang.TOOD:task-alignedone-stageobjectde- Girshick. MaskR-CNN. InICCV,pages2980–2988,2017.
tection. InICCV,pages3490–3499.IEEE,2021. 5 1,2
[9] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian [16] DrewA.HudsonandChristopherD.Manning.GQA:Anew
Sun. YOLOX: exceeding YOLO series in 2021. CoRR, dataset for real-world visual reasoning and compositional
abs/2107.08430,2021. 2 questionanswering. InCVPR,pages6700–6709,2019. 6
[10] RossB.Girshick. FastR-CNN. InICCV,pages1440–1448, [17] Dat Huynh, Jason Kuen, Zhe Lin, Jiuxiang Gu, and Ehsan
2015. 2 Elhamifar. Open-vocabulary instance segmentation via ro-
[11] RossB.Girshick,JeffDonahue,TrevorDarrell,andJitendra bust cross-modal pseudo-labeling. In CVPR, pages 7010–
Malik. Richfeaturehierarchiesforaccurateobjectdetection 7021,2022. 8
andsemanticsegmentation.InCVPR,pages580–587,2014. [18] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,
2 HieuPham,QuocV.Le,Yun-HsuanSung,ZhenLi,andTom
[12] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Duerig.Scalingupvisualandvision-languagerepresentation
Open-vocabulary object detection via vision and language learningwithnoisytextsupervision. InICML,pages4904–
knowledgedistillation. InICLR,2022. 1,2,7,8 4916,2021. 1,3
[13] AgrimGupta,PiotrDolla´r,andRossB.Girshick. LVIS:A [19] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Ultralyt-
datasetforlargevocabularyinstancesegmentation.InCVPR, ics yolov8. https://github.com/ultralytics/
pages5356–5364,2019. 6 ultralytics,2023. 2,3,4,5,6,7,8
[14] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. [20] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel
10{men, women, boy, girl} {elephant, ear, leg, trunk, ivory} {golden dog, black dog, spotted dog} {grass, sky, zebra, trunk, tree}
Figure6. VisualizationResultsonUser’sVocabulary. WedefinethecustomvocabularyforeachinputimageandYOLO-Worldcan
detecttheaccurateregionsaccordingtothevocabulary.ImagesareobtainedfromCOCOval2017.
the person in red the brown animal the tallest person person with a white shirt
the jumping person holding a person holding a toy the standing person moon
person baseball bat
Figure7. VisualizationResultsonReferringObjectDetection. Weexplorethecapabilityofthepre-trainedYOLO-Worldtodetect
objectswithdescriptivenounphrases.ImagesareobtainedfromCOCOval2017.
Synnaeve,IshanMisra,andNicolasCarion.MDETR-mod- [24] ChuangLin,PeizeSun,YiJiang,PingLuo,LizhenQu,Gho-
ulateddetectionforend-to-endmulti-modalunderstanding. lamreza Haffari, Zehuan Yuan, and Jianfei Cai. Learning
InICCV,pages1760–1770,2021. 6,7 object-language alignments for open-vocabulary object de-
[21] Weicheng Kuo, Yin Cui, Xiuye Gu, A. J. Piergiovanni, tection. InICLR,2023. 3
andAneliaAngelova. F-VLM:open-vocabularyobjectde- [25] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James
tection upon frozen vision and language models. CoRR, Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and
abs/2209.15639,2022. 3,8 C. Lawrence Zitnick. Microsoft COCO: common objects
[22] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei incontext. InProceedingsoftheEuropeanConferenceon
Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng, ComputerVision(ECCV),pages740–755,2014. 1,2,3,13
Weiqiang Nie, Yiduo Li, Bo Zhang, Yufei Liang, Linyuan [26] Tsung-YiLin,PiotrDolla´r,RossB.Girshick,KaimingHe,
Zhou, Xiaoming Xu, Xiangxiang Chu, Xiaoming Wei, and BharathHariharan,andSergeJ.Belongie. Featurepyramid
XiaolinWei. Yolov6:Asingle-stageobjectdetectionframe- networks for object detection. In CVPR, pages 936–944,
work for industrial applications. CoRR, abs/2209.02976, 2017. 1,2
2022. 2,7,8 [27] Tsung-YiLin,PriyaGoyal,RossB.Girshick,KaimingHe,
[23] LiunianHaroldLi,PengchuanZhang,HaotianZhang,Jian- andPiotrDolla´r. Focallossfordenseobjectdetection. In
wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu ICCV,pages2999–3007,2017. 2
Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang, and [28] Shu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia.
Jianfeng Gao. Grounded language-image pre-training. In Path aggregation network for instance segmentation. In
CVPR,pages10955–10965,2022. 1,2,3,5,6,7,13 CVPR,pages8759–8768,2018. 2,4
11[29] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao [44] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Zhang,JieYang,ChunyuanLi,JianweiYang,HangSu,Jun Soricut. Conceptualcaptions: Acleaned,hypernymed,im-
Zhu,andLeiZhang.GroundingDINO:marryingDINOwith agealt-textdatasetforautomaticimagecaptioning. InACL,
groundedpre-trainingforopen-setobjectdetection. CoRR, pages2556–2565,2018. 5,6,13
abs/2303.05499,2023. 1,2,3,6,7 [45] ZhiTian,ChunhuaShen,HaoChen,andTongHe. FCOS:
[30] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian fully convolutional one-stage object detection. In ICCV,
Szegedy,ScottE.Reed,Cheng-YangFu,andAlexanderC. pages9626–9635,2019. 2
Berg. SSD:singleshotmultiboxdetector. InECCV,pages [46] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
21–37,2016. 2 reit,LlionJones,AidanN.Gomez,LukaszKaiser,andIllia
[31] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,Zheng Polosukhin. Attention is all you need. In NeurIPS, pages
Zhang, Stephen Lin, and Baining Guo. Swin transformer: 5998–6008,2017. 2
Hierarchical vision transformer using shifted windows. In [47] Chien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu,
ICCV,pages9992–10002,2021. 2,3,6,7 Ping-YangChen,Jun-WeiHsieh,andI-HauYeh. Cspnet:A
[32] XiangLong,KaipengDeng,GuanzhongWang,YangZhang, newbackbonethatcanenhancelearningcapabilityofCNN.
QingqingDang,YuanGao,HuiShen,JianguoRen,Shumin InCVPRW,pages1571–1580,2020. 2
Han, Errui Ding, and Shilei Wen. PP-YOLO: an effec- [48] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-
tiveandefficientimplementationofobjectdetector. CoRR, Yuan Mark Liao. Yolov7: Trainable bag-of-freebies sets
abs/2007.12099,2020. 2 newstate-of-the-artforreal-timeobjectdetectors. InCVPR,
[33] IlyaLoshchilovandFrankHutter. Decoupledweightdecay pages7464–7475,2023. 2,7,8
regularization. InICLR,2019. 5 [49] Size Wu, Wenwei Zhang, Sheng Jin, Wentao Liu, and
Chen Change Loy. Aligning bag of regions for open-
[34] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.
vocabularyobjectdetection. InCVPR,pages15254–15264,
Im2text: Describingimagesusing1millioncaptionedpho-
2023. 1,3,8
tographs. InNeurIPS,pages1143–1151,2011. 6
[50] Johnathan Xie and Shuai Zheng. ZSD-YOLO: zero-shot
[35] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes,
YOLO detection using vision-language knowledgedistilla-
JuanC.Caicedo,JuliaHockenmaier,andSvetlanaLazebnik.
tion. CoRR,2021. 3
Flickr30k entities: Collecting region-to-phrase correspon-
dencesforricherimage-to-sentencemodels. Int.J.Comput. [51] Shangliang Xu, Xinxin Wang, Wenyu Lv, Qinyao Chang,
Vis.,pages74–93,2017. 6 Cheng Cui, Kaipeng Deng, Guanzhong Wang, Qingqing
Dang, Shengyu Wei, Yuning Du, and Baohua Lai.
[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
PP-YOLOE: an evolved version of YOLO. CoRR,
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
abs/2203.16250,2022. 2
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
[52] LeweiYao,JianhuaHan,YoupengWen,XiaodanLiang,Dan
Krueger, and Ilya Sutskever. Learning transferable visual
Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu.
modelsfromnaturallanguagesupervision. InICML,pages
Detclip: Dictionary-enrichedvisual-conceptparalleledpre-
8748–8763,2021. 1,2,3,4,5,6,13
trainingforopen-worlddetection.InNeurIPS,2022.1,2,3,
[37] JosephRedmonandAliFarhadi. YOLO9000: better,faster,
6,7
stronger. InCVPR,pages6517–6525,2017. 2
[53] Lewei Yao, Jianhua Han, Xiaodan Liang, Dan Xu, Wei
[38] Joseph Redmon and Ali Farhadi. Yolov3: An incremental
Zhang, Zhenguo Li, and Hang Xu. Detclipv2: Scal-
improvement. CoRR,abs/1804.02767,2018. 2
ableopen-vocabularyobjectdetectionpre-trainingviaword-
[39] JosephRedmon,SantoshKumarDivvala,RossB.Girshick, regionalignment.InCVPR,pages23497–23506,2023.1,3,
andAliFarhadi. Youonlylookonce: Unified,real-timeob- 6
jectdetection. InCVPR,pages779–788,2016. 2,3
[54] AlirezaZareian,KevinDelaRosa,DerekHaoHu,andShih-
[40] JosephRedmon,SantoshKumarDivvala,RossB.Girshick, FuChang.Open-vocabularyobjectdetectionusingcaptions.
andAliFarhadi. Youonlylookonce: Unified,real-timeob- InCVPR,pages14393–14402,2021. 1,2,3
jectdetection. InCVPR,pages779–788,2016. 1,2,4
[55] HaotianZhang, PengchuanZhang, XiaoweiHu, Yen-Chun
[41] ShaoqingRen,KaimingHe,RossB.Girshick,andJianSun. Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu
Faster R-CNN: towards real-time object detection with re- Yuan, Jenq-NengHwang, andJianfengGao. Glipv2: Uni-
gionproposalnetworks.IEEETransactionsonPatternAnal- fying localization and vision-language understanding. In
ysisandMachineIntelligence,pages1137–1149,2017. 2 NeurIPS,2022. 1,2,3,6,7
[42] ShaoqingRen,KaimingHe,RossB.Girshick,andJianSun. [56] HaoZhang,FengLi,ShilongLiu,LeiZhang,HangSu,Jun
Faster R-CNN: towards real-time object detection with re- Zhu,LionelM.Ni,andHeung-YeungShum. DINO:DETR
gionproposalnetworks.IEEETransactionsonPatternAnal- withimproveddenoisinganchorboxesforend-to-endobject
ysisandMachineIntelligence,pages1137–1149,2017. 1 detection. InICLR,2023. 3
[43] ShuaiShao,ZemingLi,TianyuanZhang,ChaoPeng,Gang [57] ShifengZhang, ChengChi, YongqiangYao, ZhenLei, and
Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: Stan Z. Li. Bridging the gap between anchor-based and
A large-scale, high-quality dataset for object detection. In anchor-freedetectionviaadaptivetrainingsampleselection.
ICCV,pages8429–8438,2019. 2,6 InCVPR,pages9756–9765,2020. 2,3
12[58] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan B. Automatic Labeling on Large-scale Image-
Li,NoelCodella,LiunianHaroldLi,LuoweiZhou,Xiyang TextData
Dai, Lu Yuan, Yin Li, and Jianfeng Gao. Regionclip:
Region-basedlanguage-imagepretraining. InCVPR,pages In this section, we add details procedures for labeling
16772–16782,2022. 3,8 region-text pairs with large-scale image-text data, e.g.,
[59] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp CC3M [44]. The overall labeling pipeline is illustrated in
Kra¨henbu¨hl, and Ishan Misra. Detecting twenty-thousand Fig. 8, which mainly consists of three procedures, i.e., (1)
classesusingimage-levelsupervision.InECCV,pages350–
extractobjectnouns, (2)pseudolabeling, and(3)filtering.
368,2022. 3,7,8
AsdiscussedinSec.3.4,weadoptthesimplen-gramalgo-
[60] XizhouZhu,WeijieSu,LeweiLu,BinLi,XiaogangWang,
rithmtoextractnounsfromcaptions.
andJifengDai.DeformableDETR:deformabletransformers
forend-to-endobjectdetection. InICLR,2021. 2
Region-TextProposals. Afterobtainingthesetofobject
A.AdditionalDetails nouns T = {t k}K from the first step, we leverage a pre-
trainedopen-vocabularydetector,i.e.,GLIP-L[23],togen-
A.1.Re-parameterizationforRepVL-PAN eratepseudoboxes{B }alongwithconfidencescores{c }:
i i
During inference on an offline vocabulary, we adopt re-
{B ,t ,c }N =GLIP-Labeler(I,T), (7)
parameterizationforRepVL-PANforfasterinferencespeed i i i i=1
and deployment. Firstly, we pre-compute the text embed- where{B ,t ,c }N arethecoarseregion-textproposals.
i i i i=1
dingsW ∈RC×D throughthetextencoder.
CLIP-based Re-scoring & Filtering. Considering the
Re-parameterizeT-CSPLayer. ForeachT-CSPLayerin region-text proposals containing much noise, we present
RepVL-PAN,wecanre-parameterizeandsimplifythepro- a restoring and filtering pipeline with the pre-trained
cess of adding text guidance by reshaping the text embed- CLIP [36]. Given the input image I, caption T, and
dingsW ∈ RC×D×1×1 intotheweightsofa1×1convo- thecoarseregion-textproposals{B ,t ,c }N ,thespecific
i i i i=1
lutionlayer(oralinearlayer),asfollows: pipelineislistedasfollows:
X′ =X⊙Sigmoid(max(Conv(X,W),dim=1)), (4) • (1)ComputeImage-TextScore: weforwardtheimageI
with its caption T into CLIP and obtain the image-text
where X× ∈ RB×D×H×W and X′ ∈ RB×D×H×W are similarityscoresimg.
theinputandoutputimagefeatures. ⊙isthematrixmulti-
• (2)ComputeRegion-TextScore: wecroptheregionim-
plicationwithreshapeortranspose.
agesfromtheinputimageaccordingtotheregionboxes
{B }. Then we forward the cropped images along with
i
Re-parameterize I-Pooling Attention. The I-Pooling theirtexts{t }intoCLIPandobtaintheregion-textsimi-
i
Attentioncanbere-parameterizeorsimplifiedby: laritySr ={sr}N .
i i=1
X˜ =cat(MP(X ,3),MP(X ,3),MP(X ,3)), (5) • (3) [Optional] Re-Labeling: we can forward each
3 4 5
cropped image with all nouns and assign the noun with
where cat is the concentration and MP(·, 3) denotes the
maximum similarity, which can help correct the texts
max pooling for 3×3 output features. {X ,X ,X } are
3 4 5 wronglylabeledbyGLIP.
themulti-scalefeaturesinRepVL-PAN.X˜ isflattenedand • (4) Rescoring: we adopt the region-text similarity Sr to
hastheshapeofB×D×27. Thenwecanupdatethetext rescoretheconfidencescoresc˜ =(cid:112) c ∗sr.
embeddingsby: i i i
• (5)Region-levelFiltering: wefirstdividetheregion-text
W′ =W +Softmax(W ⊙X˜),dim=-1)⊙W, (6) proposalsintodifferentgroupsaccordingtothetextsand
then perform non-maximum suppression (NMS) to fil-
A.2.Fine-tuningDetails.
tertheduplicatepredictions(theNMSthresholdissetto
We remove all T-CSPLayers and Image-Pooling Atten- 0.5).Thenwefilterouttheproposalswithlowconfidence
tion in RepVL-PAN when transferring YOLO-World to scores(thethresholdissetto0.3).
COCO [25] object detection, which only contains 80 cat- • (6) Image-level Filtering: we compute the image-level
egories and has a relatively low dependency on visual- region-text scores sregion by averaging the kept region-
language interaction. During fine-tuning, we initialize text scores. Th√en we obtain the image-level confidence
YOLO-Worldusingpre-trainedweights. Thelearningrate score by s = simg∗sregion and we keep the images
offine-tuningissetto0.0002withtheweightdecaysetto withscoreslargerthan0.3.
0.05. Afterfine-tuning, wepre-computetheclasstextem- Thethresholdsmentionedaboveareempiricallysetaccord-
beddingswithgivenCOCOcategoriesandstoretheembed- ing to the part of labeled results and the whole pipeline is
dingsintotheweightsoftheclassificationlayers. automatic without human verification. Finally, the labeled
13Automatic Labeling Pipeline
“A photography of a man
and a woman”
nouns Open-Vocabulary boxes
n-gram CLIP Labeler
Labeler
caption image image, object nouns
Extracting Object Nouns Pseudo Labeling CLIP-based Filtering
Figure8.LabelingPipelineforImage-TextDataWefirstleveragethesimplen-gramtoextractobjectnounsfromthecaptions.Weadopt
apre-trainedopen-vocabularydetectortogeneratepseudoboxesgiventheobjectnouns, whichformsthecoarseregion-textproposals.
Thenweuseapre-trainedCLIPtorescoreorrelabeltheboxesalongwithfiltering.
objects
samples are used for pre-training YOLO-World. We will
provide the pseudo annotations of CC3M for further re-
search.
C.Pre-trainingYOLO-WorldatScale
When pre-training small models, e.g., YOLO-World-S, a
natural question we have is: how much capacity does a
smallmodelhave,andhowmuchtrainingdataorwhatkind
ofdatadoesasmallmodelneed? Toanswerthisquestion,
weleveragedifferentamountsofpseudo-labeledregion-text
pairstopre-trainYOLO-World. AsshowninTab.9,adding
more image-text samples can increase the zero-shot per-
formance of YOLO-World-S. Tab. 9 indicates: (1) adding
image-text data can improve the overall zero-shot perfor-
mance of YOLO-World-S; (2) using an excessive amount
ofpseudo-labeleddatamayhavesomenegativeeffectsfor
smallmodels(YOLO-World-S),thoughitcanimprovethe
on rare categories (AP ). However, using fine-grained an-
r
notations(GoldG)forsmallmodelscanprovidesignificant
improvements,whichindicatesthatlarge-scalehigh-quality
annotateddatacansignificantlyenhancethecapabilitiesof
small models. And Tab. 3 in the main text has shown that
pre-training with the combination of fine-annotated data
andpseudo-annotateddatacanperformbetter. Wewillex-
plore more about the data for pre-training small models or
YOLOdetectorsinfuturework.
14Method Pre-trainedData Samples AP AP AP AP
r c f
YOLO-World-S O365 0.61M 16.3 9.2 14.1 20.1
YOLO-World-S O365+GoldG 1.38M 24.2 16.4 21.7 27.8
YOLO-World-S O365+CC3M-245k 0.85M 16.5 10.8 14.8 19.1
YOLO-World-S O365+CC3M-520k 1.13M 19.2 10.7 17.4 22.4
YOLO-World-S O365+CC3M-750k 1.36M 18.2 11.2 16.0 21.1
Table9.Zero-shotEvaluationonLVIS.Weevaluatetheperformanceofpre-trainingYOLO-World-Swithdifferentamountsofdata,the
image-textdata.
15