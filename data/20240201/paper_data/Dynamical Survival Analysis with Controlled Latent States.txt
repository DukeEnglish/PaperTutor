Dynamic Survival Analysis with Controlled Latent States
LinusBleistein*123 Van-TuanNguyen*45 AdelineFermanian4 AgatheGuilloux12
Abstract ofaseriesofseizures(Rasheedetal.,2020)andWi sum-
marizes unchanging characteristics of the individual (age,
We consider the task of learning individual-
gender,ethnicity,...).Thephysician’sgoalistodetermine
specific intensities of counting processes from whetheranindividualhasahighprobabilitytoexperiencea
a set of static variables and irregularly sampled
seizuregiventheircharacteristics.Suchataskismostoften
time series. We introduce a novel modelization
addressedbymodellingtheindividual-specificintensity
approachinwhichtheintensityisthesolutionto
a controlled differential equation. We first de- λi(t|Wi):= lim 1 E(cid:0) Ni(t+h)−Ni(t)|Fi(cid:1)
signaneuralestimatorbybuildingonneuralcon- ⋆ h→0+ h t
trolled differential equations. In a second time, oftheunderlyingcountingprocess,where
weshowthatourmodelcanbelinearizedinthe (cid:88)
Ni(t):= 1
signaturespaceundersufficientregularitycondi- Ti≤t
j
tions,yieldingasignature-basedestimatorwhich j≥1
we call CoxSig. We provide theoretical learn- isthestochasticprocesscountingthenumberofeventsof
ingguaranteesforbothestimators,beforeshow- individual i up to time t, and Fi is the past information
t
casing the performance of our models on a vast availableattimetwhichincludesWi(Aalenetal.,2008).
array of simulated and real-world datasets from The intensity corresponds to the instantaneous probability
finance,predictivemaintenanceandfoodsupply of experiencing an event. It is classically modelled using
chainmanagement. Coxmodels(Cox,1972;Aalenetal.,2008;Kvammeetal.,
2019)orHawkesprocessesinthecaseofself-excitingpro-
cesses (Bacry et al., 2015). Recent advances in the field
1.Introduction havealsoenrichedthesemodelsusingvariousdeeparchi-
tectures (Mei & Eisner, 2017; Omi et al., 2019; Kvamme
Time-to-event data is ubiquitous in numerous fields such et al., 2019; Chen et al., 2020; Groha et al., 2020; Shchur
as meteorology, economics, healthcare and finance. We et al., 2021; De Brouwer et al., 2022; Tang et al., 2022).
typically want to predict when an event - which can be a Oncelearnt,theintensityoftheprocesscanthenbeusedto
catastrophicearthquake,theburstofahousingbubble,the predict survival in the future or rank individuals based on
onset of a disease or a financial crash - will occur by us- theirrelativerisks.
ingsomepriorhistoricalinformation(Ogata,1988;Bacry
etal.,2015;Bussyetal.,2019). Thisgeneralproblemen- Learning with Time-Dependant Data. More realisti-
compasses many settings and in particular survival analy- cally, in addition to the static features Wi, we often also
sis, where every individual experiences at most one event haveaccesstime-dependentfeaturesalongwiththeirsam-
(Cox,1972). pling times Xi =: {(Xi(t ),t ),...,(Xi(t ),t )} ∈
1 1 K K
Rd×K, where D = {t ,...,t } is a set of measurement
For an individual i, we have typically access to several 1 K
event times Ti < ··· < Ti and features Wi ∈ Rs. For times. Taking again the example of seizure prediction,
1 q thetime-dependantfeaturesmayrepresentsomemeasure-
instance,inneurology,onemightconsidertheonsettimes
mentsmadebyawearabledevice,asdoneforinstanceby
*Equal contribution 1Inria Paris, F-75015 Paris, France Dumanis et al. (2017). Taking both the static and time-
2Centre de Recherche des Cordeliers, INSERM, Universite´ de dependantinformationintoaccountiscrucialwhenmaking
Paris, Sorbonne Universite´, F-75006 Paris, France 3LaMME,
predictions. Thissettingcallsforhighlyflexiblemodelsof
UEVEandUMR8071, ParisSaclayUniversity, F-91042, Evry,
the intensity which take into account the stream of infor-
France 4LOPF, Califrais’ Machine Learning Lab, Paris, France
5LaboratoiredeProbabilite´s,StatistiqueetMode´lisation,LPSM, mationcarriedbythelongitudinalfeatures.
Univ. ParisCite´,F-75005,Paris,France. Correspondenceto: Li-
This problem has been tackled by the bio-statistics com-
nusBleistein<linus.bleistein@inria.fr>.
munity, in particular using joint models that concurrently
fit parametric models to the trajectory of the longitudinal
1
4202
naJ
03
]LM.tats[
1v77071.1042:viXraDynamicSurvivalAnalysiswithControlledLatentStates
featuresandtheintensityofthecountingprocess(Ibrahim time,followingFermanianetal.(2021)andBleisteinetal.
et al., 2010; Crowther et al., 2013; Long & Mills, 2018; (2023), we propose to linearize the unknown dynamic la-
Nguyen et al., 2023). While being highly interpretable, tent state zi(·) in the signature space. Informally, this
⋆
they rarely scale to high-dimensional and frequently mea- meansthatatanytimet,wehavethesimplifiedexpression
sureddata.
zi(t)≈α⊤ S (Xi )
⋆ ⋆,N N [0,t]
Moderndeepmethods,thatcanencodecomplexandmean-
where α is an unknown finite-dimensional vector and
ingful patterns from complex data in latent states, offer a ⋆,N
S (Xi ) is a deterministic transformation of the time-
particularly attractive alternative for this problem. How- N [0,t]
ever,theliteraturebridgingthegapbetweendeeplearning series Xi [0,t] observed up to time t called the signature
and survival analysis is scarce. Notably, Lee et al. (2019) transform. Notice that in this form, the vector α ⋆,N does
tacklethisproblembyembeddingthetime-dependantdata
notdependontandcanhencebelearnedatanyobservation
through a recurrent neural network combined with an at- time. Forthissecondmodel, westateaprecisedecompo-
tention mechanism. They then use this embedding in a sitionofthevarianceandthediscretizationbiasofoures-
discrete-time setting to maximize the likelihood of dying timator, which crucially depends on the coarseness of the
inagiventime-frameconditionalonhavingsurviveduntil samplinggridD. Finally,webenchmarkbothmethodson
thistime.Moonetal.(2022)combineaprobabilisticmodel simulatedandreal-worlddatasetsfromfinance,healthcare
with a continuous-time neural network, namely the ODE- and digital food retail, in a survival analysis setting. Our
RNNSofRubanovaODE-RNNs(Rubanovaetal.,2019)in signature-basedestimatorprovidestate-of-the-artresults.
asimilarsetup.
Summary. Section 2 details our theoretical framework.
InSection3,westatetheoreticalguaranteesforourmodel.
Modelling Time-Series with Controlled Latent States.
Lastly,weconductaseriesofexperimentsinSection4that
Building on the increasing momentum of differential
displaysthestrongperformancesofourmodelsagainstan
equation-based methods for learning (Chen et al., 2018;
arrayofbenchmarks. Allproofsaregivenintheappendix.
De Brouwer et al., 2019; Rubanova et al., 2019; Chen
et al., 2020; Moon et al., 2022; Marion et al., 2022), we
propose a novel modelling framework in which the un- 2.ModellingPointProcesseswithControlled
known intensity of the counting process is parameterized LatentStates
by a latent state driven by a controlled differential equa-
tion(CDE).Formally,welettheunknownintensityofthe 2.1.TheData
countingprocessofindividualidependontheircovariates
Inpractice,anindividualcanbecensored(forexampleaf-
Wi andanunobservedprocessxi(t)∈Rd thatisthecon-
ter dropping out from a study) or cannot experience more
tinuous unobserved counterpart of the time series Xi i.e.
than a given number of events. To take this into account,
(Xi(t),t) = xi(t) for all t ∈ D. We model the intensity
we introduce Yi : [0,τ] → {0,1} the at-risk indica-
bysetting
tor function, which equals 1 when the individual i is still
λi(cid:0) t|Wi,(xi(s)) (cid:1) =exp(cid:0) zi(t)+β⊤Wi(cid:1) , (1) at risk of experiencing an event. The observed counting
⋆ s≤t ⋆ ⋆ process is now t → (cid:82)t Yi(s)dNi(s), that we also de-
0
note Ni for the sake of simplicity. The integral against
wherethedynamicallatentstatezi(t) ∈ Risthesolution
⋆ the counting process Ni is to be understood is to be un-
totheCDE derstood as a Stieltjes integral i.e. (cid:82)t λi(s)dNi(s) =
dz ⋆i(t)=G ⋆(cid:0) z ⋆i(t)(cid:1)⊤ dxi(t) (2) (cid:80) Ti≤tλi ⋆(T i) — see Aalen et al. (20080 , p⋆ .55-56). Its in-
tensitywritesλi(t|Wi,(xi(s)) )Yi(t),whichwesim-
⋆ s≤t
with initial condition zi(0) = 0 driven by xi. Here, the plywriteλi(t)Yi(t)toalleviatenotations. Ourdataset
⋆ ⋆
vector field G : R → Rd and β ∈ Rs are both un-
⋆ ⋆ D :={Xi,Wi,Yi(t),Ni(t), 0≤t≤τ}
known. This means that the latent dynamics are common n
between individuals, but are driven by individual-specific consists of n i.i.d. historical observations up to time τ.
data, yielding individual-specific intensities. Our frame- Our setup can be extended to individual-dependant grids
workisintroducedingreaterdetaillateron. (Di)n , butwechoosetofocusontheformersettingfor
i=1
the sake of clarity. The vector Wi ∈ Rs is a set of static
Contributions. In an effort to provide scalable and effi- baseline features. The individual specific time series are
cientmodelsforevent-dataanalysis,weproposetwonovel onlyobservedaslongastheindividualisatrisk. Theprob-
estimatorsforthisproblem. WefirstleverageneuralCDEs abilisticframeworkdefiningthefiltrationassociatedtothe
(Kidger etal., 2020), whichdirectly approximatethe vec- observations Fi is described in Appendix A.1. We first
tor field G with a neural vector field G . In a second makeanassumptiononthetimeseries.
⋆ ψ
2DynamicSurvivalAnalysiswithControlledLatentStates
Assumption 1. For every individual i = 1,...,n, there Remarkthat∥G (0)∥ < ∞sincethevectorfieldisLip-
⋆ op
existsacontinuouspathofboundedvariationxi :[0,τ]→ schitzandhencecontinuous.
Rpsatisfying
Remark2.2. Bydifferentiation,onecanseethattheinten-
(cid:13) (cid:13)xi(cid:13)
(cid:13)
1-var,[s,t]
≤L x|t−s|
s tii aty
l
eit qs ue alf tis oa nti (s Lfi ie ns &as Yo- oc na gll ,e 2d 0c 2o 0n )t .ro Il nle dd eeV do ,l dte ifr fr ea red nif tf ie ar tie nn g-
for all 0 ≤ s < t ≤ τ such that the time series Xi is a theintensityλi ⋆yieldstheCDE
discretizationofxi. dλi(s)=λi(s)G (zi(s))dxi(s)
⋆ ⋆ ⋆ ⋆
RemarkthatthisassumptionimpliesthatthepathsareL - withinitialconditionλi(0)=exp(β⊤Wi). Notethatthis
x ⋆ ⋆
Lipschitz. We now state a supplementary assumption on CDEispathdependanti.e. itsvectorfielddependsonthe
thestaticfeatures. pathzi :[0,τ]→R.
⋆
Assumption2. ThereexistsaconstantB >0suchthat
W
(cid:13) (cid:13)
foreveryi=1,...,n,(cid:13)Wi(cid:13)
2
≤B W. 2.3.NeuralControlledDifferentialEquations
Following the ideas of continuous time models, our first
2.2.ModellingIntensitieswithControlledDifferential
approachtolearningthedynamicsistofitaparameterized
Equations
intensitytothismodelbysetting
Controlled differential equations are a theoretical frame- λi(s)=exp(α⊤zi(s)+β⊤Wi), (3)
θ θ
work that allows to generalize ODEs beyond the non-
autonomousregime(Lyonsetal.,2007). Recallthatanon- where z θi(s) ∈ Rp is an embedding of the time series Xi
autonomousODEisthesolutionto parameterized by θ ∈ Rv and α ∈ Rp is a learnable pa-
rameter. We propose to use Neural Controlled Differen-
dz(t)=F(z(t),t)dt tial Equations (NCDEs), a powerful tool for embedding
with a given initial value z(0) = z ∈ Rp. Here, the irregular time series introduced by Kidger et al. (2020).
vector field F : Rp × [0,+∞[→ 0 Rp depends explic- NCDEs work by first embedding a time series Xi in the
spaceofcontinuousfunctionsofboundedvariation,yield-
itly on t ≥ 0, allowing for time-varying dynamics un-
ing x˜i : [0,τ] → Rp, before defining a representation of
likeautonomousODEswhosedynamicsremainunchanged
thedatathrough
throughtime. Controlleddifferentialequationscanbeseen
asageneralizationofnon-autonomousODEs. Theyallow dz (t)=G (cid:0) z (s)(cid:1) dx˜i(s) (4)
θ ψ θ
for the vector field to depend explicitly on the values of
anotherRd-valuedfunction,thusencodingevenricherdy- withinitialconditionz θ(0) = 0. Itiscommonpracticeto
set G : Rp → Rp×d to be a small feed-forward neural
namics. Formally,aCDEwrites ψ
networkparameterizedbyψ. Thelearnableparametersof
(cid:0) (cid:1)
dz(t)=G z(t) dx(t) thismodelarethusθ =(α,ψ,β). Inoursetup,theembed-
z(0)=z ∈Rp dingx˜i :[0,τ]→Rpmustbecarefullychoseninordernot
0
to leak information from the future observations. Hence
where G is a Rp×d-valued vector field. Existence and naturalcubicsplines,usedintheoriginalpaperbyKidger
uniquenessofthesolutionisensuredunderregularitycon- etal.(2020),cannotbeusedandweresorttothepiecewise
ditionsonGandxbythePicard-Lindelho¨fTheorem(see constant interpolation scheme proposed by Morrill et al.
TheoremA.1). Thefollowingassumptionisneededinor- (2021)anddefinedas
dertoensurethatthismodeliswell-defined.
x˜i(s)=Xi(t )foralls∈[t ,t [.
Assumption3. ThevectorfieldG : Rp → Rp×d defin- k k k+1
⋆
ing λi in Equation (2) is L -Lipschitz; β is such that Thisyieldsadiscretelyupdatedlatentstateequalto
⋆ G⋆ ⋆
B∥β ⋆∥ ,2 B≤ B ,Bβ,2, ∥ >β ⋆ 0∥ 1 ar≤ ecB onβ s,1 taa nn tsd
.
∥β ⋆∥ 0 ≤ B β,0, where z θi,D(t k)=z θi,D(t k−1)+G ψ(z θi,D(t k−1))∆Xi(t k)
β,2 β,1 β,0
where ∆Xi(t ) = Xi(t )−Xi(t ). This architecture
k k k−1
Under these assumptions, the intensity is bounded at all hasbeenstudiedunderthenameofcontrolledResNet be-
times. cause of its resemblance with the popular ResNet (Cirone
Lemma 2.1 (A bound on the intensity). For every indi- etal.,2023;Bleistein&Guilloux,2023).
vidual i = 1,...,n and all t ∈ [0,τ], the log intensity
Inordertoprovidetheoreticalguarantees, werestrictour-
logλi(t)isupperboundedby
⋆ selvestoaboundedsetofNCDEsi.e. weconsiderasetof
B B +∥G (0)∥ L texp(cid:0) L L t(cid:1) NCDEpredictors
β,2 W ⋆ op x G⋆ x
Θ ={θ ∈Rvs.t.∥α∥ ≤B ,∥ψ∥≤B ,∥β∥ ≤B }
almostsurely. 1 2 α ψ 2 β,2
3DynamicSurvivalAnalysiswithControlledLatentStates
wherethenormonψ referstothesumofℓ 2 normsofthe 1.5 x x( (1 2) )( (t t) )
weights and biases of the neural vector field G ψ. This 1.0 x(3)(t)
restriction is fairly classical in statistical learning theory 0.5
0.0
(Bach,2021).
0.5
0.0 0.2 0.4 0.6 0.8 1.0
2.4.LinearizingCDEsintheSignatureSpace 1.0 I = (3,2) tSI(x[0,t]) 0.06 tSI(x[0,t]) 0.000 tSI(x[0,t])
0.5 0.005 0.04 I = (1,1,3)
TheSignatureTransform. Whileneuralcontrolleddif- 0.0 0.010
0.02
ferential equations allow for great flexibility in represen- 0.015 I = (3,3,3,2)
0.5
tation of the time series, they are difficult to train and re- 0.0 0.2 0.4 0.6 0.8 1.0 0.00 0.0 0.2 0.4 0.6 0.8 1.0 0.020 0.0 0.2 0.4 0.6 0.8 1.0
Time
quire significant computational resources. The signature
is a promising and theoretically well-grounded tool from Figure 1: Sample path x(t) of a 3-dimensional fractional
stochastic analysis, that allows for a parameter-free em- Brownian motion on top, and three signature coefficients
bedding of the time series. Mathematically, the signature SI(x [0,t])associatedtodifferentwordsonthebottom.
coefficientofafunction
x:t∈[0,τ](cid:55)→(cid:0) x(1)(t),...,x(d)(t)(cid:1) Themathematicaldefinitionofα ⋆istechnicalandwerefer
toAppendixA.4foraformalstatementandadiscussionof
associatedtoawordI =(i ,...,i )∈{1,...,d}kofsize
1 k the regularity assumptions. Hence, under the correspond-
kisthefunction
ingregularityconditions,thetrueintensityforindividuali
(cid:90)
writes
SI(x [0,t]):= dx(i1)(u 1)...dx(ik)(u k)
0<u1<···<uk<t λi(t)=exp(cid:0) α⊤S(xi )+β⊤Wi(cid:1) . (5)
⋆ ⋆ [0,t] ⋆
which maps [0,τ] to R. The integral is to be understood
as the Riemann-Stieltjes integral. While the definition of Thismotivatestheuseofthesignature-basedestimator
the signature is technical, it can simply be seen as a fea- λi,D(s):=exp(cid:0) α⊤S (Xi )+β⊤Wi(cid:1) ,
ture extraction step. We refer to Figure 1 for an illustra- θ N [0,s]
tion. The truncated signature of order N ≥ 1, which we where θ = (α,β) ∈ Rq × Rs, N ≥ 1 is treated
writeS N(x [0,t]), isequaltothecollectionofallsignature as a hyperparameter and Xi corresponds to the fill-
[0,s]
coefficients associated to words of size k ≤ N sorted by forwardembeddingoftheobservedvalues ofthetimese-
lexicographical order. Finally, the infinite signature is the ries(Xi(t ),t ),...,(Xi(t ),t )uptothelastobservation
1 1 s s
sequencedefinedthrough timet precedings. Theintegerq = dN−1−1 isthesizeof
s d−1
S(x )= lim S (x ). the signature truncated at depth N ≥ 1. The superscript
[0,t] N [0,t]
N→+∞ in D emphasises the dependence of this estimator on the
observationgridD.
Learning with Signatures. Signatures are a prominent
tool in stochastic analysis since the pioneering work of Similarly to the NCDE-based estimator, we restrict our-
Chen (1958) and Lyons et al. (2007). They have re- selvestotheboundedsetofestimators
cently found successful applications in statistics and ma-
Θ ={θs.t. ∥α∥≤B ,∥β∥≤B }.
2 α β,2
chinelearningasafeaturerepresentationforirregulartime
series (Kidger et al., 2019; Morrill et al., 2020; Ferma-
2.5.ConnectionstoCoxModelswithTime-Varying
nian, 2021; Salvi et al., 2021; Fermanian, 2022; Lyons &
Covariates
McLeod,2022;Bleisteinetal.,2023;Horvathetal.,2023)
andatoolforanalyzingresidualneuralnetworksinthein- Cox model with time-varying covariates are the classical
finitedepthlimit(Fermanianetal.,2021). classofmodels(Lin&Zelterman,2002;Aalenetal.,2008;
Zhang et al., 2018), where the individual specific hazard
Signatures and CDEs. An appealing feature of signa- ratehastheformλi(t)=λ (t)exp(α⊤Xi(t)+β⊤Wi),
θ 0,θ ⋆ ⋆
tures is their connection to controlled differential equa- whereλ :[0,τ]→R iscalledthebaselinehazard.
0 +
tions. Indeed,undersufficientregularityassumptions(Friz
For signature-based embeddings, recall that we compute
& Victoir, 2010; Fermanian et al., 2021; Bleistein et al.,
the signature of a time-embedded time series Xi =
2023; Cirone et al., 2023), the generative CDE (1) can be
{(Xi(t ),t ),...,(Xi(t ),t )}. Infact,thisamountsto
linearized in the signature space. Informally, this means 1 1 k k
that there exists a sequence α such that for all t ∈ [0,τ] N
⋆ (cid:88) (cid:88)
wehave α⊤S N(Xi [0,t])= α ktk+α I⊤ 1Xi(t)+ α ISI(Xi [0,t])
z ⋆i(t)=α ⋆⊤S(xi [0,t]). (cid:124)k=0 (cid:123)(cid:122) (cid:125) (cid:124) I∈ (cid:123)I (cid:122)2 (cid:125)
=logλ0(t) =log ofindividualspecifichazardrate
4
)t(x
erutangiSDynamicSurvivalAnalysiswithControlledLatentStates
whereα isasubvectorofαandI ⊂(cid:81)N {1,...,d}k. Proposition3.1. Foreveryθ ∈ Θ,thedifferenceinlikeli-
I1 2 k=2
Hence our model can be interpreted as a generalized ver- hoodsℓD(θ)−ℓ⋆ decomposesas
n n
sionofCoxmodelswithtime-varyingcovariates. Asimi-
lar interpretation holds for NCDEs. We detail this link in 1 (cid:88)n (cid:90) λi,D(s)
KL (λ ,λD)− log θ dMi(s),
AppendixA.6. n ⋆ θ n λi(s)
i=1 ⋆
3.TheoreticalGuarantees whereMi :[0,τ]→Risalocalsquareintegrablemartin-
gale.
3.1.TheLearningProblem
This proposition is a consequence of the Doob-Meyer de-
Forbothmodels,theparameterθcanbefittedbylikelihood
composition Ni(t) = (cid:82)t λi(s)Yi(s)ds + Mi(t) of the
maximizationbysolving 0 ⋆
countingprocess(Aalenetal.,2008). Wenowfurthermore
θˆ∈argminℓD(θ)+pen(θ), (6) definethetotalvariationdivergenceas
n
θ∈Θ
1 (cid:88)n (cid:90) τ
whereΘ ∈ {Θ ,Θ }dependingonwhetheroneusessig- TV (λ ,λD):= |λi(s)−λi,D(s)|Yi(s)ds.
1 2 n ⋆ θ n ⋆ θ
nature or NCDE-based embeddings, pen : Θ → R + is a i=1 0
penaltyandℓD(θ)isequaltothenegativelog-likelihoodof
thesampleDn nevaluatedatθ. andthequadraticlogdivergenceD2 n(λ ⋆,λD θ )as
U bon tl hes Ns Csp De Ec sifi ae nd do st ih ge nr, att uh re e-f bo al slo edwi en mg bs et dat de im nge snt (s upho tl od df io fr
-
1 (cid:88)n (cid:90) τ (cid:0) logλi,D(s)−logλi(s)(cid:1)2 λi(s)Yi(s)ds.
n θ ⋆ ⋆
ferent constants given explicitly in the proofs). Following i=1 0
Aalenetal.(2008),thenegativeloglikelihoodℓD(θ)ofthe
n Proposition3.2. Thereexisttwoconstantsc ,c >0such
samplewrites 1 2
that
1 (cid:88)n (cid:90) τ λi,D(s)Yi(s)ds−(cid:90) τ
logλi,D(s)dNi(s), c TV (λ ,λD)2 ≤KL (λ ,λD)≤c D2(λ ,λD).
n θ θ 1 n ⋆ θ n ⋆ θ 2 n ⋆ θ
i=1 0 0
More precisely, the constants c ,c are functions of
1 2
andwelet
Θ,L ,τ andL andaregivenexplicitlyinAppendixB.2.
x G⋆
1 (cid:88)n (cid:90) (cid:90)
ℓ⋆ = λi(s)Yi(s)ds− logλi(s)dNi(s) This bound is obtained by combining a Pinsker-type in-
n n ⋆ ⋆
equality (Proposition B.1) and a self-concordance bound
i=1
(Proposition B.2). It is informative in two ways. First,
bethetruelikelihoodofthedata. Ourgoal,inthissection, it shows that minimizing the negative empirical log-
istoobtainabias-variancedecompositionofthedifference likelihood and hence the KL-divergence between the true
andparameterizedintensitywillleadtoaminimizationof
ℓD(θˆ)−ℓ⋆
n n thetotalvariationbetweenthetwointensities. Secondly,it
showsthattheKL-divergenceisupperboundedbyaterm
betweenthetruelikelihoodandthelikelihoodofthelearnt
involvingthedifferenceofthelogarithmsoftheintensities.
model. We first define the empirical KL-divergence be-
Wemakeuseofthissecondboundtoobtainabias-variance
tweenthetrueandparameterizedintensityassociatedtothe
decomposition.
sampleD as
n
1 (cid:88)n (cid:90) τ λi(s) 3.2.ARiskBound
KL (λ ,λ ):= log ⋆ λi(s)Yi(s)ds
n ⋆ θ n λi(s) ⋆ Proposition3.3(InformalBiasDecomposition). Wehave
i=1 0 θ
− n1 (cid:88)n (cid:90) τ (cid:0) λi ⋆(s)−λi θ(s)(cid:1) Yi(s)ds. D2 n(λ ⋆,λD θˆ)≤Discretizationbias+Approximationbias.
i=1 0
Moreprecisely,itholdsthat
This definition is classical for intensities of counting pro-
Discretizationbias=O(|D|)
cesses (Aalen et al., 2008; Lemler, 2016). We now show
that minimizing the empirical KL-divergence between the and the approximation bias depends on the regularity of
trueandtheparameterizedintensity,ignoringanoiseterm G resp. the approximation capacities of the vector field
⋆
thatwillbecanceledbysettingthepenaltyaccordingly. G whenusingsignaturesresp. NCDEs.
ψ
5DynamicSurvivalAnalysiswithControlledLatentStates
AformalstatementofthispropositionisgiveninAppendix 1.0
t
B.3. Thelinearityofthesignaturemodelallowstogoone 0.5
stepfurtherandtocontrolthestochasticintegralofPropo- 0.0
sition3.1. 0.5
t increases
Theorem 3.4 (Informal Risk Bound for the Signature 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Model). Consider the signature-based embedding. Let θˆ 0.4 t t tS S SI I I1 2 3( ( (x x x[ [ [0 0 0, , ,t t t] ] ]) )
)
0.4 t t tS S SI I I1 2 3( ( (x x xF F FF F F, , ,[ [ [0 0 0, , ,t t t] ] ]) )
)
12 .. 50 ||S(x(t))S(xFF(t))||2
0.2 0.2 be the solution of (6) with pen(θ) = η ∥α∥ +η ∥β∥ .
1 1 2 1 1.0
ForanyN ≥ 1,wehavewithhighprobabilityandanap- 0.0 0.0
0.5
propritechoiceofη 1,η 2that 0.20.0 0.2 0.4 0.6 0.8 1.0 0.20.0 0.2 0.4 0.6 0.8 1.0 0.0 0.0 0.2 0.4 0.6 0.8 1.0
Time
ℓD(θˆ)−ℓ⋆ ≤Discretizationbias+Approximationbias
n n Figure2: Onthetop, observedtimeseriesuptotimetin
(cid:32)(cid:114) (cid:33) (cid:32)(cid:114) (cid:33)
logNdN logs boldcolorsandtruetimeseriesinfadedcolors.Wheneval-
+O +O .
n n uatingourmodels, wefill-forwardthelastobservedvalue
fromton.Onthebottom,signaturesofthetruepath(left),
of the observed path (center) and difference in ℓ norm
Foraformalstatement,seeAppendixB.4.Remarkably,we 2
obtainclassicalratesinn−1/2forthevarianceterm. (right)—x FF(t)denotesthefilled-forwardtimeseries.
4.ExperimentalEvaluation WedescribeitsdetailedcomputationinAppendixC.2.
Wenowfocusonthesurvivalanalysissetup. Wehencelet Time-Dependant Concordance Index. Following Lee
Ti be the unique time-of-event, which may eventually be et al. (2019), we measure the discriminative power of
censored, of individual i. ∆i is the censorship indicator, our models by using a time-dependant concordance index
equal to 1 if the individual experiences the event and to 0 C(t,δt) that captures our models ability to correctly rank
otherwise. individuals on their predicted probability of survival. The
concordanceindexC(t,δt)isthenfinallycomputedas
4.1.TrainingSetup
n n
(cid:80) (cid:80)1 1
We train on a dataset D
n
of the same structure than de-
j=1i=1
r θi(t,δt)>r θj(t,δt) Ti>Tj,Tj∈[t,t+δt],∆j=1
scribedinSection2.1andlearntheparameterθˆbysolving
n n
.
(cid:80) (cid:80)1
the optimization problem (6). NCDEs are trained without Ti>Tj,Tj∈[t,t+δt],∆j=1
j=1i=1
penalization,whileweslightlydeviatefromourtheoretical
settinganduseamixtureofelastic-netpenalties Thismetriccapturesthecapacityofourmodeltodiscrimi-
natebetweenj andanotherindividualithroughthecondi-
pen(θ):=η 1pen EN(α)+η 2pen EN(β) tionalprobabilityofsurvival.
for training the signature-based model, where pen (·) =
EN Brier Score. While the concordance index is a ranking-
γ∥·∥ + (1 − γ)∥·∥ . The hyperparameters (η ,η ,N)
1 2 1 2 based measure, the Brier Score measures the accuracy
are chosen by cross-validation of a mixed metric equal to
in predictions by comparing the estimated survival func-
thedifferencebetweentheC-indexandtheBrierscore(see
tion and the survival indicator function (Lee et al., 2019;
below) and we set γ = 0.1. We refer to Appendix C.1
Kvamme et al., 2019; Kvamme & Borgan, 2023). For-
for a detailed description of the training procedures. We
mally,wedefinetheBrierscoreBS(t,δt)as
evaluateourmodel’scapacitytopredicteventsin[t,t+δt]
byleveragingvaluesofthelongitudinalfeaturesuptot(see 1 (cid:88)n
1 ri(t,δt)2
Figure2)througharankingmetricandacalibrationmetric. n Ti≤t+δt,∆i=1 θ
Thisevaluationprocedureisstandard(Leeetal.,2019). i=1
n
1 (cid:88)
+ 1 (1−ri(t,δt))2.
4.2.Metrics n Ti>t+δt θ
i=1
Wecomputefourmetricsusingtheindividualspecificsur- Contrarily to the C-index, the Brier score is a measure
vivalfunctionsasestimatedbyourmodelwithparameters of calibration of the predictions: it measures the distance
θ. Attimet+δtforδt > 0conditionalonsurvivalupto between the estimated survival function and the indicator
time t, and on observation of the longitudinal features up functionofsurvivalontheinterval[t,t+δt].
totimet,itisdefinedas
Additional metrics. We furthermore report AUC and
(cid:16) (cid:17)
ri(t,δt)=P Ti >t+δt|Ti >t,Xi ,Wi . weightedBrierscoreinAppendixD.
θ [0,t]
6
)t(x
erutangiSDynamicSurvivalAnalysiswithControlledLatentStates
ingapaymenttotheholderincaseofanevent(Cheridito
&Xu,2015;Corcuera&Valdivia,2016). Buildingonthis
6
4 problem,weconsidertheOrnstein-UhlenbeckSDE
w
2
0
-2 (cid:88)d
-4 dwi(t)=−ω(wi(t)−µ)dt+ dx(i,j)(t)+σdBi(t)
0 2 T 4 6 8 10
j=1
Observed Unobserved
2
where d = 5, σ = 1, µ = 0.1 and ω = 0.1 are fixed pa-
0
rameters. xi(t) = (x(i,1)(t),...,x(i,d−1)(t)) is a sample
-2 x xtt( (1 2)) x xt t( (3 4) ) path of a fractional Brownian motion with Hurst parame-
0 2 4 Time 6 8 10 terH = 0.6,andBi(t)isanindividualspecificBrownian
noise term. In this setup, our data consists of Xi which
Figure 3: Time series Xi of a randomly picked individual on is a downsampled version of xi and the Brownian part is
bottomandunobservedSDEwi(t)onthetop.Theredstarindi-
unobserved. Our goal is to predict the first hitting time
catesthefirsthittingtimeofthethresholdvaluew =2.5.
⋆
min{t > 0|w ≥ w }ofathresholdvaluew = 2.5. We
t ⋆ ⋆
train on n = 500 individuals. Figure 3 shows the sample
4.3.Methods pathsandSDEofarandomlyselectedindividual.
We propose three distinct methods. In addition to the
Tumor Growth. We similarly aim at predicting the hit-
signature-basedmodel,whichwecallCoxSig,wealsocon-
tingtimeofastochasticprocessmodellingthegrowthofa
siderCoxSig+whichaddsthefirstvalueofthetimeseries
tumor (Simeoni et al., 2004), where xi represents a drug-
to the static features. This is motivated by the translation
intake. Crucially, inthisexperiment, thetimeseriesXi is
invariance of signatures (see discussion below). Our last
very-lowdimension(d=2,whichincludesthetimechan-
method is the NCDE embedding of the longitudinal fea-
nel).
tures. We benchmark our three models against a set of
competingmethods. AllmethodsaredetailedinAppendix
4.5.Real-WorldDatasets
C.1.
Predictive Maintenance (Saxena et al., 2008). This
Time-Independent Cox Model. As a sanity check, dataset collects simulations of measurements of sensors
we implement a simple Cox model with elastic-net placed on aircraft gas turbine engines run until a thresh-
penalty which uses the parameterized intensity λi(t) = old value is reached. In this context, the time-to-event is
θ
λ (t)exp(β⊤Wi) using scikit-survival (Po¨lsterl, thefailuretime.
0
2020). This baseline allows to check whether our pro-
posed methods can make use of the supplementary time- Churnprediction. Weuseaprivatedatasetfromafood
dependant information. If no static features are available, supply chain company1 that delivers gross products to
we use the first observed value of the time series, i.e., restaurants. The company has access to a variety of fea-
Wi =Xi(0). turesobservedthroughtimeforeverycustomer. Itsgoalis
forexampletopredictwhenthecustomerwillchurn.
RandomSurvivalForest(RSF). WeuseRSF(Ishwaran
etal.,2008)withstaticfeaturesWiastheonlyinput.Sim- 4.6.Results
ilarlytoourimplementationoftheCoxmodel,weusethe
GeneralperformanceofCoxSig. Overall,thesignature-
first value of the time series as static features if no other
based estimators outperform competing methods. We
featuresareavailable.
observe that CoxSig and CoxSig+ improve over to the
strongest baselines in terms of Brier scores. Contrarily to
DynamicDeepHit(Leeetal.,2019). DDHisastate-of-
the strong baseline DDH, this improvement is consistent
the-art method for dynamical survival analysis, that com-
over larger prediction windows [t,t+ δt] as δt increases
binesanRNNwithanattentionmechanismandusesboth
(see Figure 5). They provide even stronger improvements
timedependantandstaticfeatures.
intermsofC-indexes(seeFigure4andAppendixD).This
suggests that they are particularly well-tailored for rank-
4.4.SyntheticExperiments
ing tasks, such as identifying the most-at-risk individual.
Hitting time of a partially observed SDE. Predicting
1We will provide more details on this dataset upon publica-
hitting times is a crucial problem in finance — for in- tion to preserve anonymity of the authors during the reviewing
stance,whenpricingso-calledcatastropheboundstrigger- process.
7
tw
txDynamicSurvivalAnalysiswithControlledLatentStates
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=0.23, t=0.10 t=3.00, t=2.00 t=166.60, t=16.20
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.25 0.25 0.25
0.20 0.20 0.20
0.15 0.15 0.15
0.10 0.10 0.10
0.05 0.05 0.05
t=0.23, t=0.10 t=3.00, t=2.00 t=166.60, t=16.20
0.00 0.00 0.00
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure 4: C-Index(higherisbetter)ontopandBrierscore(lowerisbetter)onbottomforhittingtimeofapartiallyobservedSDE
(left),churnprediction(center)andpredictivemaintenance(right)evaluatedatchosenpoints(t,δt). tischosenasthefirstdecileof
theeventtimesi.e.90%oftheeventsoccuraftert.Hollowdotsindicateoutliers,anderrorbarsindicate80%oftheinterquartilerange.
Wereportdetailedresultsfornumerouspoints(t,δt)inAppendixD.
manian(2021)foranempiricalstudy). Apossiblesolution
0.5 to handle low-dimensional data is to use embeddings be-
CoxSig Cox
CoxSig+ RSF forecomputingsignaturestomakethemmoreinformative
0.4 NCDE DDH (Morrilletal.,2020).
0.3
NCDEs. Ontheotherside, NCDEsgenerallytieorper-
0.2 formworsethancompetingmethods. Notably, whencon-
sideringC-indexes, theyevenperformworsethanrandom
0.1 onthepredictivemaintenancedataset. Thisstandsinstark
contrasttotheirgoodperformancesonclassificationorre-
0.0
0.2 0.4 0.6 0.8 1.0 gression tasks (Kidger et al., 2020; Morrill et al., 2021;
t Vanderschuerenetal.,2023).
Figure 5: Brier score δt (cid:55)→ BS(t,δt), evaluated at t = 0.23,
forthepartiallyobservedSDEexperiment. Confidenceintervals
Running times. Finally, we observe that our methods
indicate1standarddeviation.
run in similar times than DDH, while including cross-
validation(seeFigure12intheappendix). Modelsthatdo
Includingthefirstobservedvalueofthetime-seriesgener- notusetimedependantfeatures(RSFandCox)are2orders
ally improves CoxSig’s performance: this is possibly due ofmagnitudetimesfastertotrain.
to the fact that signatures are invariant by translation (i.e.
the signature of x : t (cid:55)→ x(t) is equal to the signature of 5.Conclusion
x : t (cid:55)→ x(t)+a), and hence including the first value of
thetimeseriesprovidesnon-redundantinformation. Wehavedesignedandanalyzedamodelforgenericcount-
ingprocessesdrivenbyacontrolledlatentstate,whichcan
Performance on low-dimensional data. A notable ex- bereadilyestimatedusingeitherneuralCDEorsignature-
ception is the tumor growth simulation, in which CoxSig based estimators. CoxSig in particular offers a parsimo-
is generally outperformed (see Figures 14 and 15 in the nious alternative to deep models and yields excellent per-
Appendix). This is probably due to the low-dimensional formanceforsurvivalanalysis. Futureresearcheffortswill
nature of the time series (d = 2), which causes the sig- betargetedatextendingourmodeltocompetingrisksand
natureofthetimeseriestocollapsestoapolynomialmap multimodaldata.
of the last observed point carrying little information. The
competitiveperformanceofsignaturesformoderatetohigh Acknowledgements. For this work, AG has benefited
dimensionaldatastreamsisawell-studiedfeature(seeFer- fromthesupportoftheNationalAgencyforResearchun-
8
xednI-C
erocS
reirB
erocS
reirB
xednI-C
erocS
reirB
xednI-C
erocS
reirBDynamicSurvivalAnalysiswithControlledLatentStates
dertheFrance2030programwiththereferenceANR-22- Chen, K.-T. Integration of paths—a faithful representa-
PESN-0016. LB and AG thank Killian Bakong-Epoune´, tion of paths by non-commutative formal power series.
who performed preliminary analysis of the NCDE model TransactionsoftheAmericanMathematicalSociety,89:
during an internship at Inria. LB, VTN and AF thank the 395–407,1958.
Sorbonne Center for Artificial Intelligence (SCAI) and its
teamfortheirsupport. LBthanksClaireBoyerandGe´rard Chen, R. T., Amos, B., and Nickel, M. Neu-
Biauforsupervisingapreviousinternshipthatsparkedhis ral spatio-temporal point processes. arXiv preprint
interestinsignatures. arXiv:2011.04583,2020.
Chen, R. T. Q., Rubanova, Y., Bettencourt, J., and Duve-
Impactstatement. Thispaperpresentsworkwhosegoal
naud, D. K. Neural ordinary differential equations. In
istorankindividualsortopredicttheirriskofeventbyre-
Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
lyingonpastinformation. Thisisacommongoalinmany
Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in
diverse fields, including healthcare (which patient is most
NeuralInformationProcessingSystems,volume31,pp.
at risk of dying ?), insurance policy pricing (given a per-
6572–6583.CurranAssociates,Inc.,2018.
son’s history, what is her probability of experiencing an
eventcoveredbyherpolicy?) andhumanresourcesman-
Cheridito,P.andXu,Z. Pricingandhedgingcocos. Avail-
agement(whichemployeeismostlikelytoquit?).
ableatSSRN2201364,2015.
Ourproposedmethodcanbeusedinanyofthesesettingsto
personalizepredictions,andhencetargetinterventions. As Cirone,N.M.,Lemercier,M.,andSalvi,C. Neuralsigna-
anypredictionalgorithm,itmaycausesomedisadvantaged ture kernels as infinite-width-depth-limits of controlled
groupstosufferfrombiaseddecisions. resnets. arXivpreprintarXiv:2303.17671,2023.
Corcuera,J.M.andValdivia,A. Pricingcocoswithamar-
References
kettrigger. SpringerInternationalPublishing,2016.
Aalen,O.,Borgan,O.,andGjessing,H. Survivalandevent
Cox, D.R. Regressionmodelsandlife-tables. Journalof
historyanalysis: aprocesspointofview. SpringerSci-
theRoyalStatisticalSociety:SeriesB(Methodological),
ence&BusinessMedia,2008.
34(2):187–202,1972.
Bach, F. Self-concordant analysis for logistic regression.
Crowther, M.J., Abrams, K. R., and Lambert, P. C. Joint
2010.
modeling of longitudinal and survival data. The Stata
Bach, F. Learningtheoryfromfirstprinciples. Draftofa Journal,13(1):165–184,2013.
book,versionofSept,6:2021,2021.
DeBrouwer,E.,Simm,J.,Arany,A.,andMoreau,Y.GRU-
Bacry, E., Mastromatteo, I., andMuzy, J.-F. Hawkespro- ODE-Bayes: Continuous modeling of sporadically-
cessesinfinance. MarketMicrostructureandLiquidity, observed time series. In Wallach, H., Larochelle, H.,
1(01):1550005,2015. Beygelzimer, A., d'Alche´-Buc, F., Fox, E., andGarnett,
R. (eds.), Advances in Neural Information Processing
Bleistein,L.andGuilloux,A. Onthegeneralizationcapac- Systems,volume32,pp.7379–7390.CurranAssociates,
ities of neural controlled differential equations. arXiv Inc.,2019.
preprintarXiv:2305.16791,2023.
De Brouwer, E., Gonzalez, J., and Hyland, S. Predict-
Bleistein, L., Fermanian, A., Jannot, A.-S., and Guilloux, ing the impact of treatments over time with uncertainty
A. Learningthedynamicsofsparselyobservedinteract- aware neural differential equations. In International
ingsystems. arXivpreprintarXiv:2301.11647,2023. Conference on Artificial Intelligence and Statistics, pp.
4705–4722.PMLR,2022.
Boyd, S. P. and Vandenberghe, L. Convex optimization.
Cambridgeuniversitypress,2004. Dumanis,S.B.,French,J.A.,Bernard,C.,Worrell,G.A.,
and Fureman, B. E. Seizure forecasting from idea to
Bussy, S., Veil, R., Looten, V., Burgun, A., Ga¨ıffas, S., reality.outcomesofthemyseizuregaugeepilepsyinno-
Guilloux,A.,Ranque,B.,andJannot,A.-S. Comparison vationinstituteworkshop. Eneuro,4(6),2017.
of methods for early-readmission prediction in a high-
dimensionalheterogeneouscovariatesandtime-to-event Fermanian, A. Embedding and learning with signatures.
outcomeframework. BMCmedicalresearchmethodol- ComputationalStatistics&DataAnalysis,157:107148,
ogy,19:1–9,2019. 2021.
9DynamicSurvivalAnalysiswithControlledLatentStates
Fermanian, A. Functional linear regression with trun- Kvamme,H.,Borgan,Ø.,andScheel,I.Time-to-eventpre-
catedsignatures. JournalofMultivariateAnalysis,192: dictionwithneuralnetworksandcoxregression. arXiv
105031,2022. preprintarXiv:1907.00825,2019.
Fermanian,A.,Marion,P.,Vert,J.-P.,andBiau,G. Fram- Lee, C., Yoon, J., and Van Der Schaar, M. Dynamic-
ing RNN as a kernel method: A neural ODE approach. deephit: Adeeplearningapproachfordynamicsurvival
In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, analysiswithcompetingrisksbasedonlongitudinaldata.
P.,andVaughan,J.W.(eds.),AdvancesinNeuralInfor- IEEE Transactions on Biomedical Engineering, 67(1):
mationProcessingSystems,volume34,pp.3121–3134. 122–133,2019.
CurranAssociates,Inc.,2021.
Lemler, S. Oracle inequalities for the lasso in the high-
Friz, P. K. and Victoir, N. B. Multidimensional Stochas- dimensionalaalenmultiplicativeintensitymodel. 2016.
ticProcessesasRoughPaths: TheoryandApplications,
volume 120 of Cambridge Studies in Advanced Mathe- Lin,H.andZelterman,D. Modelingsurvivaldata: extend-
matics. CambridgeUniversityPress,Cambridge,2010. ingthecoxmodel,2002.
Groha,S.,Schmon,S.M.,andGusev,A. Ageneralframe- Lin, P. and Yong, J. Controlled singular volterra integral
work for survival analysis and multi-state modelling. equations and pontryagin maximum principle. SIAM
arXivpreprintarXiv:2006.04893,2020. Journal on Control and Optimization, 58(1):136–164,
2020.
Horvath, B., Lemercier, M., Liu, C., Lyons, T., and
Salvi, C. Optimal stopping via distribution regres- Long, J. D. and Mills, J. A. Joint modeling of multivari-
sion: a higher rank signature approach. arXiv preprint ate longitudinal data and survival data in several obser-
arXiv:2304.01479,2023. vational studies of huntington’s disease. BMC medical
researchmethodology,18(1):1–15,2018.
Ibrahim, J. G., Chu, H., and Chen, L. M. Basic concepts
and methods for joint models of longitudinal and sur-
Lyons, T. and McLeod, A. D. Signature methods in ma-
vival data. Journal of Clinical Oncology, 28(16):2796,
chinelearning. arXivpreprintarXiv:2206.14674,2022.
2010.
Lyons,T.,Caruana,M.,andLe´vy,T.DifferentialEquations
Ishwaran,H.,Kogalur,U.B.,Blackstone,E.H.,andLauer,
drivenbyRoughPaths,volume1908ofLectureNotesin
M.S. Randomsurvivalforests. 2008.
Mathematics. Springer,Berlin,2007.
Kidger,P.andLyons,T. Signatory: differentiablecompu-
Marion, P., Fermanian, A., Biau, G., and Vert, J.-P. Scal-
tations of the signature and logsignature transforms, on
ing resnets in the large-depth regime. arXiv preprint
bothcpuandgpu.InInternationalConferenceonLearn-
arXiv:2206.06929,2022.
ingRepresentations,2020.
Mei, H. and Eisner, J. M. The neural hawkes process:
Kidger, P., Bonnier, P., Perez Arribas, I., Salvi, C., and
A neurally self-modulating multivariate point process.
Lyons, T. Deep signature transforms. In Wallach, H.,
Advancesinneuralinformationprocessingsystems,30,
Larochelle, H., Beygelzimer, A., d'Alche´-Buc, F., Fox,
2017.
E., and Garnett, R. (eds.), Advances in Neural Infor-
mationProcessingSystems,volume32,pp.3099–3109.
Moon,I.,Groha,S.,andGusev,A. Survlatentode: Aneu-
CurranAssociates,Inc.,2019.
ralodebasedtime-to-eventmodelwithcompetingrisks
Kidger,P.,Morrill,J.,Foster,J.,andLyons,T. Neuralcon- forlongitudinaldataimprovescancer-associatedvenous
trolleddifferentialequationsforirregulartimeseries. In thromboembolism (vte) prediction. In Machine Learn-
Larochelle,H.,Ranzato,M.,Hadsell,R.,Balcan,M.F., ing for Healthcare Conference, pp. 800–827. PMLR,
andLin,H.(eds.),AdvancesinNeuralInformationPro- 2022.
cessingSystems,volume33,pp.6696–6707.CurranAs-
Morrill, J., Fermanian, A., Kidger, P., and Lyons, T. A
sociates,Inc.,2020.
generalised signature method for multivariate time se-
Kingma, D.P.andBa, J. Adam: Amethodforstochastic riesfeatureextraction.arXivpreprintarXiv:2006.00873,
optimization. arXivpreprintarXiv:1412.6980,2014. 2020.
Kvamme,H.andBorgan,Ø. Thebrierscoreunderadmin- Morrill,J.,Kidger,P.,Yang,L.,andLyons,T. Neuralcon-
istrativecensoring: Problemsandasolution. Journalof trolleddifferentialequationsforonlinepredictiontasks.
MachineLearningResearch,24(2):1–26,2023. arXivpreprintarXiv:2106.11028,2021.
10DynamicSurvivalAnalysiswithControlledLatentStates
Nguyen, V. T., Fermanian, A., Guilloux, A., Barbieri, A., in xenograft models after administration of anticancer
Zohar,S.,Jannot,A.-S.,andBussy,S. Flash: afastjoint agents. Cancerresearch,64(3):1094–1101,2004.
modelforlongitudinalandsurvivaldatainhighdimen-
Tang, W., Ma, J., Mei, Q., and Zhu, J. Soden: A scal-
sion. arXivpreprintarXiv:2309.03714,2023.
able continuous-time survival model through ordinary
Ogata, Y. Statistical models for earthquake occurrences differentialequationnetworks. TheJournalofMachine
andresidualanalysisforpointprocesses. Journalofthe LearningResearch,23(1):1516–1544,2022.
AmericanStatisticalassociation,pp.9–27,1988.
Van de Geer, S. Exponential inequalities for martingales,
Omi, T., Aihara, K., et al. Fully neural network based with application to maximum likelihood estimation for
model for general temporal point processes. Advances countingprocesses. TheAnnalsofStatistics,pp.1779–
inneuralinformationprocessingsystems,32,2019. 1801,1995.
Po¨lsterl,S.scikit-survival:Alibraryfortime-to-eventanal- Vanderschueren, T., Curth, A., Verbeke, W., and van der
ysis built on top of scikit-learn. Journal of Machine Schaar, M. Accounting for informative sampling when
Learning Research, 21(212):1–6, 2020. URL http: learningtoforecasttreatmentoutcomesovertime. arXiv
//jmlr.org/papers/v21/20-729.html. preprintarXiv:2306.04255,2023.
Rasheed, K., Qayyum, A., Qadir, J., Sivathamboo, S., Virmaux, A.andScaman, K. Lipschitzregularityofdeep
Kwan,P.,Kuhlmann,L.,O’Brien,T.,andRazi,A. Ma- neural networks: analysis and efficient estimation. Ad-
chinelearningforpredictingepilepticseizuresusingeeg vances in Neural Information Processing Systems, 31,
signals: A review. IEEE Reviews in Biomedical Engi- 2018.
neering,14:139–155,2020.
Zhang,Z.,Reinikainen,J.,Adeleke,K.A.,Pieterse,M.E.,
Reizenstein,J.andGraham,B. Theiisignaturelibrary: ef- and Groothuis-Oudshoorn, C. G. Time-varying covari-
ficientcalculationofiterated-integralsignaturesandlog atesandcoefficientsincoxregressionmodels. Annalsof
signatures. arXivpreprintarXiv:1802.08252,2018. translationalmedicine,6(7),2018.
Rubanova,Y.,Chen,R.T.Q.,andDuvenaud,D.K. Latent
ordinary differential equations for irregularly-sampled
timeseries.InWallach,H.,Larochelle,H.,Beygelzimer,
A.,d'Alche´-Buc,F.,Fox,E.,andGarnett,R.(eds.),Ad-
vances in Neural Information Processing Systems, vol-
ume32,pp.5320–5330.CurranAssociates,Inc.,2019.
Salvi, C., Lemercier, M., Liu, C., Horvath, B., Damoulas,
T.,andLyons,T. Higherorderkernelmeanembeddings
to capture filtrations of stochastic processes. Advances
in Neural Information Processing Systems, 34:16635–
16647,2021.
Saxena,A.,Goebel,K.,Simon,D.,andEklund,N. Dam-
age propagation modeling for aircraft engine run-to-
failure simulation. In 2008 international conference
onprognosticsandhealthmanagement, pp.1–9.IEEE,
2008.
Shchur, O., Tu¨rkmen, A. C., Januschowski, T., and
Gu¨nnemann,S. Neuraltemporalpointprocesses: Are-
view. arXivpreprintarXiv:2104.03528,2021.
Shorack,G.R.andWellner,J.A. Empiricalprocesseswith
applicationstostatistics. SIAM,2009.
Simeoni, M., Magni, P., Cammia, C., De Nicolao,
G., Croci, V., Pesenti, E., Germani, M., Poggesi,
I., and Rocchetti, M. Predictive pharmacokinetic-
pharmacodynamic modeling of tumor growth kinetics
11DynamicSurvivalAnalysiswithControlledLatentStates
Supplementary Material
A.SupplementaryMathematicalElements
A.1.Formaldefinitionofthefiltration
Totheobservations,weassociatethefiltrationF,withallσ-fieldsat0≤t≤τ definedas
(cid:91)
F = Fi
t t
i=1,...,n
whereFi =σ(cid:0) Xi(t),Wi,Ni(t),Yi(t)≤t≤τ(cid:1) . . WeassumeinadditionthatYiisFi-predictable.
t
A.2.Picard-Lindelho¨fTheorem
TheoremA.1. Letxbeacontinuouspathofboundedvariation,andassumethatG:Rp →Rp×disLipschitzcontinuous.
ThentheCDE
dz =G(z )dx
t t t
withinitialconditionz ∈Rphasauniquesolution.
0
AfullproofcanbefoundinFermanianetal.(2021). Remarkthatsinceinoursetting,NCDEsareLipschitzsincetypical
neuralvectorfields,suchasfeed-forwardneuralnetworks,areLipschitz(Virmaux&Scaman,2018). Thisensuresthatthe
solutionstoNCDEsarewelldefined.
A.3.ContinuityoftheFlowofCDEs
WestatearesultonthecontinuityoftheflowadaptedfromBleistein&Guilloux(2023).
Theorem A.2. Let F,G : Rp → Rp×d be two Lipschitz vector fields with Lipschitz constants L ,L > 0. Let x,r
F G
be either continuous or piecewise constant paths of total variations bounded by L and L . Consider the controlled
x r
differentialequations
dw(t)=F(w(t))dx(t) and dv(t)=G(v(t))dr(t)
withinitialconditionsw(0)=v(0)=0respectively. Itthenfollowsthat
(cid:32) (cid:33)
(cid:0) (cid:1)
∥w(t)−v(t)∥≤ ∥x−r∥ 1+L L K +max∥F(v)−G(v)∥ L exp(L L ),
∞,[0,t] F r op r F x
v∈Ω
where
(cid:34) (cid:35)
(cid:0) (cid:1)
K= L ∥F(0)∥ L exp(L L )+∥F(0)∥ exp(L L ) (7)
F op x F x op F x
and
Ω=(cid:8) u∈Rp| ∥u∥≤(∥G(0)∥ L )exp(L L )(cid:9) (8)
op r G r
Proof. SeeBleistein&Guilloux(2023),TheoremB.5.
A.4.LinearizationintheSignatureSpace
A.4.1.GENERALRESULT
Inthissection,wegiveadditionaldetailsonthelinearizationofCDEsinthesignaturespace.Wefirstdefinethedifferential
product.
12DynamicSurvivalAnalysiswithControlledLatentStates
Definition A.3. Let F,G : Rp → Rp be two C∞ vector fields and let J(·) be the Jacobian matrix. Their differential
productF ⋆G:Rp →Rpisthesmoothvectorfielddefinedforeveryh∈Rpby
e
(cid:88) ∂G
(F ⋆G)(h)= (h)F (h)=J(G)(h)F(h)
∂h j
j
j=1
WenowconsideratensorfieldF:Rp →Rp×dwhichwewrite
 
| ... |
F=F1 ... Fd 
| ... |
whereforevery1≤i≤d,Fi :Rp →Rp,andwedefine
Γ k(F):= sup (cid:13) (cid:13)Fi1 ⋆···⋆Fik(h)(cid:13) (cid:13) 2. (9)
∥h∥≤M,i1≤···≤ik≤d
Considerthesolutionz :[0,τ]→RptotheCDE
dz(t)=F(z(t))dx(t) (10)
z(0)=0∈Rp (11)
wherex : [0,τ] → Rd isacontinuouspathoffinitetotalvariationboundedbyL τ > 0. Werecallthefollowingresult
x
fromFermanianetal.(2021),Proposition4.
PropositionA.4(Fermanianetal.(2021),Proposition4.). Wehave
(cid:13) (cid:13)z ⋆i(t)−α ⋆⊤ ,NS N(x [0,t])(cid:13) (cid:13)≤ (d (NL x +t)N 1)+ !1 Γ N+1(F)
Asaconsequence,wehavethefollowingTheorem.
TheoremA.5. LetF:Rp →Rp×dbeaC∞tensorfield. If
(dL t)N+1
x Γ (F)→0
(N +1)! N+1
asN →+∞,thenthesolutionztoCDE(10)canbewrittenas
(cid:88) (cid:88)
z(t)= S(i1,...,ik)(x )Fi1 ⋆···⋆Fik(0).
[0,t]
k≥1 1≤i1,...,ik≤d
A.4.2.APPLICATIONTOOURMODEL
RecallthatwehavedefinedourgenerativemodelthroughtheCDE
dzi(t)=G (zi(t))dxi(t) (12)
⋆ ⋆ ⋆
withinitialconditionzi(0) = 0,whereG : R → Rp isaL -Lipschitzvectorfield. Sinceinourcase,thevectorfield
⋆ ⋆ G⋆
G mapsRtoRd,itcanbewrittenas
⋆
G (z)=(cid:2) G1(z) ... Gd(z)(cid:3)
⋆ ⋆ ⋆
whereforevery1≤i≤d,Gi :R→R. Inthissetup,for1≤i ,i ≤dthedifferentialproductcollapsesto
⋆ 1 2
(Gi1 ⋆Gi2)(h)=(Gi2)′(h)×Gi1(h)∈R. (13)
⋆ ⋆ ⋆ ⋆
13DynamicSurvivalAnalysiswithControlledLatentStates
For1≤i ,i ,i ≤d,itwrites
1 2 3
(Gi1 ⋆Gi2 ⋆Gi3)(h)=(Gi2 ⋆Gi3)′(h)×Gi1(h) (14)
⋆ ⋆ ⋆ ⋆ ⋆ ⋆
=(cid:0) (Gi3)′(h)×Gi2(h)(cid:1)′ ×Gi1(h) (15)
⋆ ⋆ ⋆
=(cid:0) (Gi3)′′(h)×Gi2(h)+(Gi3)′(h)×(Gi2)′(h)(cid:1) ×Gi1(h)∈R. (16)
⋆ ⋆ ⋆ ⋆ ⋆
Onecanderivesimilarexplicitexpressionfor1≤i ,...,i ≤d.
1 k
InlinewithTheoremA.5,wemakethefollowingAssumptiononthevectorfieldG .
⋆
Assumption4. ThevectorfieldG satisfies
⋆
(L τd)N+1
x Γ (G )→0,
(N +1)! N+1 ⋆
whereforallk ≥1,
Γ k(G ⋆):= sup |Gi ⋆1 ⋆···⋆Gi ⋆k(h)|
i1≤···≤ik≤d
AsaconsequenceofAssumption4,wecanwritetheℓ andℓ normsofα asfunctionsofthedifferentialproductof
2 1 ⋆,N
G .
⋆
LemmaA.6. UnderAssumption4,onehas
N
(cid:16)(cid:88) (cid:17)1/2
∥α ∥ ≤ dkΓ (G )2
⋆,N 2 k ⋆
k=1
and
N
(cid:88)
∥α ∥ ≤ dkΓ (G ).
⋆,N 1 k ⋆
k=1
Proof. Startingwiththeℓ norm,onehas
2
N
(cid:16)(cid:88) (cid:88) (cid:17)1/2
∥α ⋆,N∥
2
= Gi ⋆1 ⋆···⋆Gi ⋆k(0)2 (17)
k=1 1≤i1,i2,...,ik≤d
N
(cid:16)(cid:88) (cid:17)1/2
≤ dk max |Gi1 ⋆···⋆Gik(0)|2 (18)
⋆ ⋆
k=1
1≤i1,i2,...,ik≤d
N
(cid:16)(cid:88) (cid:17)1/2
≤ dkΓ (G )2 . (19)
k ⋆
k=1
Movingontotheℓ norm,wesimilarlyobtain
1
N
(cid:16)(cid:88) (cid:88) (cid:17)
∥α ⋆,N∥
1
= |Gi ⋆1 ⋆···⋆Gi ⋆k(0)| (20)
k=1 1≤i1,i2,...,ik≤d
N
(cid:16)(cid:88) (cid:17)
≤ dk max |Gi1 ⋆···⋆Gik(0)| (21)
⋆ ⋆
k=1
1≤i1,i2,...,ik≤d
N
(cid:88)
≤ dkΓ (G ). (22)
k ⋆
k=1
14DynamicSurvivalAnalysiswithControlledLatentStates
A.5.SignatureofaDiscretizedPath
WerecallthefollowingresultfromBleisteinetal.(2023).
Theorem A.7. Let x : [0,τ] (cid:55)→ x(t) be a path satisfying Assumption 1. Let D = {t ,...,t } ⊂ [0,τ] be a grid of
1 K
samplingpoints. Forallα∈Rq,whereq := dN−1,wehave
d−1
|α⊤(cid:0) S (x )−S (xD )(cid:1) |≤c (N)∥α∥|D|,
N [0,t] N [0,t] 3
where
(L t)N−1−1
c (N)=2e x L .
3 L t−1 x
x
A.6.TheCoxConnections
Signature-basedembeddings. Consideracontinuouspathofboundedvariationx : t (cid:55)→ (x(t),t) ∈ Rd. First,remark
thatforeverywordofsizeoneI ∈{1,...,d},thesignaturewrites
(cid:90)
SI(x )= dx(I)(s)=x(I)(t).
[0,t]
0<u1<t
Furthermore,foranywordI ={d,...,d}ofsizekmadeonlyoftheletterdi.e. wordsthatonlyincludethetimechannel,
wehave
(cid:90) 1
SI(x )= du ...du = tk.
[0,t] 1 k k!
0<u1<···<uk<t
Thisprovesourclaim.
NCDEs. ForanyN ≥1,considertheaugmentedvectorfield
(cid:20) (cid:21)
G 0
G˜ (z)= ψ N×d ∈R(N+p)×(N+d)
ψ 0 I
p×(N+1) N×N
takingasaninputtheaugmentedpathx˜(t) := (x(t),t,t2,...,tN) ∈ Rd+N. ThelatentstateoftheNCDEmodelisnow
updatedas
z˜i,D(t )=z˜i,D(t )+G˜ (z˜i,D(t ))∆X˜ (23)
θ k+1 θ k ψ θ k tk+1
 .   . 
. .
. .
  G ψ(z θi,D)∆Xi(t k+1) 

  z θi,D(t k)+G ψ(z θi,D)∆Xi(t k+1) 

 . .   . . 
=z˜i,D(t )+ . = . . (24)
θ k    
 ∆t   t 
 k+1   k+1 
 . .   . . 
 .   . 
∆tN tN
k+1 k+1
Thisprovesourclaim.
A.7.Self-concordance
Wenowstateaself-concordancebound,whichcanbefoundalongwithitsproofinBach(2010).
LemmaA.8. Letg :R→Rbeaconvex,threetimesdifferentiablefunctionsuchthat
|g(3)|≤Mg(2)(x)
forallx∈RforsomeM ≥0. Thenitfollowsthat
g(2)(0) g(2)(0)
Φ(−Mt)≤g(t)−g(0)−tg′(0)≤ Φ(Mt)
M2 M2
forallt≥0,where
Φ:t(cid:55)→exp(t)−t−1.
15DynamicSurvivalAnalysiswithControlledLatentStates
B.Proofs
B.1.ProofofProposition3.1
Proof. UsingtheDoob-Meyerdecompositionofcountingprocesses-seeAalenetal.(2008,p. 52-60)-wehave
(cid:90) t
Ni(t)= λi(s)Yi(s)ds+Mi(t)
⋆
0
whereMiislocalsquareintegrablemartingale,andhencethelog-likelihoodassociatedtoindividualiis
(cid:90) τ (cid:90) τ
ℓD(θ)= λi,D(s)Yi(s)ds− logλi,D(s)dNi(s) (25)
i θ θ
0 0
(cid:90) τ(cid:16) (cid:17) (cid:90) τ
= λi,D(s)−logλi,D(s)λi(s) Yi(s)ds− logλi,D(s)dMi(s). (26)
θ θ ⋆ θ
0 0
Similarly,thelog-likelihoodassociatedtothetrueintensityλi writes
⋆
(cid:90) τ (cid:90) τ
ℓ⋆ = λi(s)Yi(s)ds− logλi(s)dNi(s) (27)
n ⋆ ⋆
0 0
(cid:90) τ(cid:16) (cid:17) (cid:90) τ
= λi(s)−logλi(s)λi(s) Yi(s)ds− logλi(s)dMi(s). (28)
⋆ ⋆ ⋆ ⋆
0 0
Hence,weget
ℓD(θ)−ℓ⋆ (29)
n n
1 (cid:88)n (cid:90) (cid:104) λi(s)(cid:105) 1 (cid:88)n (cid:90) λi(s)
= λi(s)−λi(s)−λi(s)log θ Yi(s)ds− log θ dMi(s) (30)
n θ ⋆ ⋆ λi(s) n λi(s)
i=1 ⋆ i=1 ⋆
=KL (λ ,λ )−
1 (cid:88)n (cid:90)
log
λ θ(s)
dMi(s). (31)
n ⋆ θ n λi(s)
i=1 ⋆
Thisconcludestheproof.
B.2.ProofofProposition3.2
ToproveProposition3.2,weessentiallycombineaPinsker-typeinequalityandaself-concordancebound. Weprovethese
two bounds separatly bellow. Combining them yields the double inequality of Proposition 3.2. In the following, for all
t∈[0,τ],welet
(cid:90) t
Λi,D(t)= λi,D(s)Yi(s)ds
θ θ
0
and
(cid:90) t
Λi(t)= λi(s)Yi(s)ds
⋆ ⋆
0
bethecumulativehazardfunctions.
PropositionB.1(Pinsker’sinequality.). Letλ bethetrueintensitydefinedinEquations1and2λ beaintensityparam-
⋆ θ
eterizedbyθ ∈Θ. UnderAssumptions1,2,and3,wehavethat
c TV (λ ,λD)2 ≤KL (λ ,λD) (32)
1 n ⋆ θ n ⋆ θ
with
1 (cid:104)4 (cid:0) (cid:1) 2 (cid:105)−1
c := exp(−B B ) exp(∥G (0)∥ L τexp L L τ )+ exp(B exp(L τ))
1 τ β,2 W 3 ⋆ op x G⋆ x 3 α x
16DynamicSurvivalAnalysiswithControlledLatentStates
forsignature-basedembeddingsand
1 (cid:104)4 (cid:0) (cid:1) 2 (cid:104) (cid:0) (cid:105)(cid:105)−1
c := exp(−B B ) exp(∥G (0)∥ L τexp L L τ )+ exp ∥G (0)∥ L τexp L L τ)
1 τ β,2 W 3 ⋆ op x G⋆ x 3 ψ op x Gψ x
forNCDEs.
Proof. Wehave
1 (cid:88)n (cid:90) τ 1 (cid:88)n (cid:90) τ λi(s)
TV (λ ,λD)= |λi(s)−λi,D(s)|Yi(s)ds= | ⋆ −1|λi(s)Yi(s)ds (33)
n ⋆ θ n ⋆ θ n λi,D(s) θ
i=1 0 i=1 0 θ
(cid:118)
(cid:90) τ(cid:117) (cid:117)(cid:32) λi(s) (cid:33)2
= (cid:116) ⋆ −1 λi,D(s)Yi(s)ds, (34)
λi,D(s) θ
0 θ
√
wherewehaveusedthat|x|= x2.
Bydefinition,λi,D(s)=exp(cid:0) zi,D(s)+β⊤Wi(cid:1)
>0foralls∈[0,τ]: wecanthus
θ θ
safelydividebythisterm. Now,sinceforallx≥0
(cid:16)4 2 (cid:17)
(x−1)2 ≤ + x Φ(x) (35)
3 3
with
Φ(x):=xlogx−x+1, (36)
oneobtains
1 (cid:88)n (cid:90) τ(cid:115) (cid:16)4 2 λi(s) (cid:17) (cid:16) λi(s) (cid:17)
TV (λ ,λD)≤ + ⋆ Φ ⋆ λi,D(s)Yi(s)ds. (37)
n ⋆ θ n 3 3λi,D(s) λi,D(s) θ
i=1 0 θ θ
UsingtheCauchy-Schwarzinequalityyields
TV (λ ,λ ) (38)
n ⋆ θ
1 (cid:88)n (cid:34) (cid:90) τ(cid:16)4 2λi(s)(cid:17) (cid:35)1/2(cid:34) (cid:90) τ (cid:16)λi(s)(cid:17) (cid:35)1/2
≤ + ⋆ λi(s)Yi(s)ds Φ ⋆ λi(s)Yi(s)ds (39)
n 3 3λi(s) θ λi(s) θ
i=1 0 θ 0 θ
1 (cid:88)n (cid:34) 4 2 (cid:35)1/2(cid:34) (cid:90) τ(cid:16) λi(s) (cid:17) (cid:35)1/2
≤ Λi,D(τ)+ Λi(τ) λi(s)log ⋆ −λi(s)+λi(s) Yi(s)ds (40)
n 3 θ 3 ⋆ ⋆ λi(s) θ ⋆
i=1 0 θ
(cid:34) (cid:35)1/2
≤ max 4 Λi,D(τ)+ 2 Λi(τ) (cid:112) KL (λ ,λ ). (41)
i=1,...,n 3 θ 3 ⋆ n ⋆ θ
Takingthesquareonbothsidesyields
(cid:34) (cid:35)
4 2
TV (λ ,λD)2 ≤ max Λi,D(τ)+ Λi(τ) KL (λ ,λ ). (42)
n ⋆ θ i=1,...,n 3 θ 3 ⋆ n ⋆ θ
Boundingthetruecumulativehazardfunction. Wehave
(cid:34) (cid:35)
4 2 4 2
max Λi,D(τ)+ Λi(τ) ≤ max Λi,D(τ)+ max Λi(τ) (43)
i=1,...,n 3 θ 3 ⋆ 3i=1,...,n θ 3i=1,...,n ⋆
and
logλi(s)≤B B +∥G (0)∥ L sexp(cid:0) L L s(cid:1) (44)
⋆ β,2 W ⋆ op x G⋆ x
(cid:0) (cid:1)
≤B B +∥G (0)∥ L τexp L L τ (45)
β,2 W ⋆ op x G⋆ x
17DynamicSurvivalAnalysiswithControlledLatentStates
foralls∈[0,τ]followingLemma2.1. Henceforalli=1,...,n,itholdsthat
(cid:90) τ
Λi(τ)= λi(s)Yi(s)ds≤τ sup λi(s)
⋆ ⋆ ⋆
0 s∈[0,τ]
(cid:32) (cid:33)
(cid:0) (cid:1)
≤τexp B B +∥G (0)∥ L τexp L L τ .
β,2 W ⋆ op x G⋆ x
Sincethislastbounddoesnotdependoni,thisgivesusthat
(cid:32) (cid:33)
2 max Λi(τ)≤ 2τ exp B B +∥G (0)∥ L τexp(cid:0) L L τ(cid:1) . (46)
3i=1,...,n ⋆ 3 β,2 W ⋆ op x G⋆ x
Similarly,oneobtainsthat
4 4τ
max Λi,D(τ)≤ max sup λi(s). (47)
3i=1,...,n θ 3 i=1,...,ns∈[0,τ] θ
Signature-basedembeddings. Forsignature-basedembeddings,wehave
(cid:104) (cid:105)
sup λi(s)≤exp(B B ) exp(B exp(L τ)) . (48)
θ β,2 W α x
s∈[0,τ]
NCDEs. ForNCDEs,oneobtains
sup
λi(s)≤exp(cid:0)
B B
)exp(cid:104)
∥G (0)∥ L
τexp(cid:0)
L L
τ)(cid:105)
. (49)
θ β,2 W ψ op x Gψ x
s∈[0,τ]
FinalBound. Puttingeverythingtogether,onefinallyhasthat
c TV (λ ,λD)2 ≤KL (λ ,λ ), (50)
1 n ⋆ θ n ⋆ θ
where
1 (cid:104)4 (cid:0) (cid:1) 2 (cid:105)−1
c = exp(−B B ) exp(∥G (0)∥ L τexp L L τ )+ exp(B exp(L τ)) (51)
1 τ β,2 W 3 ⋆ op x G⋆ x 3 α x
whenusingsignaturesand
1 (cid:104)4 (cid:0) (cid:1) 2 (cid:104) (cid:0) (cid:105)(cid:105)−1
c := exp(−B B ) exp(∥G (0)∥ L τexp L L τ )+ exp ∥G (0)∥ L τexp L L τ) (52)
1 τ β,2 W 3 ⋆ op x G⋆ x 3 ψ op x Gψ x
whenusingNCDEs.
Wealsoprovethefollowingself-concordancebound,acloseresultcanbefoundin(Lemler,2016).
PropositionB.2(Aself-concordancebound.). Letλ bethetrueintensitydefinedinEquations1-2andλ beaintensity
⋆ θ
parameterizedbyθ ∈Θ. UnderAssumptions1,2,and3,itholdsthat
KL (λ ,λ )≤c D2(λ ,λD)
n ⋆ θ 2 n ⋆ θ
where
expM −M −1
c := ,
2 M
18DynamicSurvivalAnalysiswithControlledLatentStates
and
(cid:34) (cid:35)
(cid:16) (cid:0) (cid:1)(cid:17)
M :=exp(B B ) exp ∥G (0)∥ L τexp L L τ +exp(B exp(L τ))
β,2 W ⋆ op x G⋆ x α x
whenusingsignaturesand
(cid:34) (cid:35)
(cid:16) (cid:0) (cid:1)(cid:17) (cid:104) (cid:0) (cid:105)
M :=exp(B B ) exp ∥G (0)∥ L τexp L L τ +exp ∥G (0)∥ L τexp L L τ)
β,2 W ⋆ op x G⋆ x ψ op x Gψ x
whenusingNCDEs.
Proof. Define
g :t(cid:55)→ 1 (cid:88)n (cid:90) τ(cid:104) exp(cid:0) t(logλi(s)−logλi(s))(cid:1) −tlog λi θ(s) −1(cid:105) λi(s)Yi(s)ds.
n θ ⋆ λi(s) ⋆
i=1 0 ⋆
This function satisfies all assumptions needed in Lemma A.8. The function f : t (cid:55)→ exp(ξt)−ξt−1 is convex for all
ξ ∈ R. Convexity is preserved by integration against a positive function: indeed, if f(t,s) is convex in t for all s and
h(s)≥0foralls∈A,then
(cid:90)
f(t,s)h(s)ds
A
is convex in t (see Boyd & Vandenberghe, 2004, Page 79). The function is also clearly C∞. Finally, remark that by
differentiationtheintegral,oneobtains
g′(t)= 1 (cid:88)n (cid:90) τ(cid:104) (logλi(s)−logλi(s))exp(cid:0) t(logλi(s)−logλi(s))(cid:1) −log λi θ(s)(cid:105) λi(s)Yi(s)ds,
n θ ⋆ θ ⋆ λi(s) ⋆
i=1 0 ⋆
g′′(t)= 1 (cid:88)n (cid:90) τ (logλi(s)−logλi(s))2exp(cid:0) t(logλi(s)−logλi(s))(cid:1) λi(s)Yi(s)ds,
n θ ⋆ θ ⋆ ⋆
i=1 0
g′′′(t)= 1 (cid:88)n (cid:90) τ (logλi(s)−logλi(s))3exp(cid:0) t(logλi(s)−logλi(s))(cid:1) λi(s)Yi(s)ds.
n θ ⋆ θ ⋆ ⋆
i=1 0
Hence
|g′′′(t)| (53)
≤ 1 (cid:88)n (cid:90) τ (cid:13) (cid:13)λi −λi(cid:13) (cid:13) logλi(s)−logλi(s))2exp(cid:0) t(logλi(s)−logλi(s))(cid:1) λi(s)Yi(s)ds (54)
n θ ⋆ ∞ θ ⋆ θ ⋆ ⋆
i=1 0
=Mg′′(t) (55)
forallt∈Rwith
M := max (cid:13) (cid:13)λi −λi(cid:13) (cid:13) .
i=1,...,n θ ⋆ ∞
UsingnowLemmaA.8,wegetatt=1
g′′(0) g′′(0)
Φ(−M)≤g(1)−g(0)−g′(0)≤ Φ(M).
M2 M2
(cid:124) (cid:123)(cid:122) (cid:125)
=0
Wehave
1 (cid:88)n (cid:90) τ
g′′(0)= (logλi(s)−logλi(s))2λi(s)Yi(s)ds.
n θ ⋆ ⋆
i=1 0
19DynamicSurvivalAnalysiswithControlledLatentStates
Finally,remarkthat
g(1)=KL (λ ,λ )
n ⋆ θ
hence
expM −M −1(cid:88)n (cid:90) τ
KL (λ ,λ )≤ (logλi(s)−logλi(s))2λi(s)Yi(s)ds.
n ⋆ θ nM θ ⋆ ⋆
i=1 0
TurningtotheconstantM,remarkthat
M := max (cid:13) (cid:13)λi −λi(cid:13) (cid:13) ≤ max (cid:13) (cid:13)λi(cid:13) (cid:13) + max (cid:13) (cid:13)λi(cid:13) (cid:13) . (56)
i=1,...,n θ ⋆ ∞ i=1,...,n θ ∞ i=1,...,n ⋆ ∞
SimilarlytowhatisdoneintheproofofPropositionB.1,wehave
(cid:32) (cid:33)
i=m 1,a ..x .,n(cid:13) (cid:13)λi ⋆(cid:13) (cid:13)
∞
≤exp B β,2B W+∥G ⋆(0)∥ opL xτexp(cid:0) L G⋆L xτ(cid:1) , (57)
and
i=m 1,a ..x .,n(cid:13) (cid:13)λi θ(cid:13) (cid:13)
∞
≤exp(B β,2B W)(cid:104) exp(B αexp(L xτ))(cid:105) (58)
whenusingsignaturesand
i=m 1,a ..x .,n(cid:13) (cid:13)λi θ(cid:13) (cid:13)
∞
≤exp(cid:0) B β,2B W)exp(cid:104) ∥G ψ(0)∥ opL xτexp(cid:0) L GψL xτ)(cid:105) (59)
whenusingNCDEs. Puttingeverythingtogether,oneobtains
KL (λ ,λ )≤c D2(λ ,λD), (60)
n ⋆ θ 2 n ⋆ θ
with
exp(M)−M −1
c := (61)
2 M
and
(cid:34) (cid:35)
(cid:16) (cid:0) (cid:1)(cid:17)
M :=exp(B B ) exp ∥G (0)∥ L τexp L L τ +exp(B exp(L τ)) (62)
β,2 W ⋆ op x G⋆ x α x
whenusingsignaturesand
(cid:34) (cid:35)
(cid:16) (cid:0) (cid:1)(cid:17) (cid:104) (cid:0) (cid:105)
M :=exp(B B ) exp ∥G (0)∥ L τexp L L τ +exp ∥G (0)∥ L τexp L L τ) (63)
β,2 W ⋆ op x G⋆ x ψ op x Gψ x
whenusingNCDEs.
B.3.ProofandFormalStatementofProposition3.3
WefirstgiveapreciseandformalstatementofProposition3.3.
PropositionB.3(FormalStatementofProposition3.3). Letλ bethetrueintensitydefinedinEquations1-2andλ bea
⋆ θ
intensityparameterizedbyθ ∈Θ. UnderAssumptions1,2,3and4,forsignature-basedembeddings,itholdsthat
(cid:32) (cid:33)(cid:34) (cid:35)2
D2(λ ,λD)≤2τexp B B +∥G (0)∥ L τexp(cid:0) L L τ(cid:1) (dL x)N+1 Λ (G ) τ2N+3
n ⋆ θˆ β,2 W ⋆ op x G⋆ x (N +1)! N+1 ⋆ 2N +3
(cid:124) (cid:123)(cid:122) (cid:125)
Approximationbias
(cid:32) (cid:33) N
+2τexp B B +∥G (0)∥ L τexp(cid:0) L L τ(cid:1) c (N)2(cid:88) dkΓ (G )2|D|2
β,2 W ⋆ op x G⋆ x 3 k ⋆
k=1
(cid:124) (cid:123)(cid:122) (cid:125)
Discretizationbias
20DynamicSurvivalAnalysiswithControlledLatentStates
whereasforNCDEs,itholdsthat
(cid:34) (cid:35)
D2 n(λ ⋆,λD
θ
)≤2τexp(2L GψL x) (cid:16) B α(cid:0) 1+L GψL xK(cid:1) L x|D|(cid:17)2 +(cid:16) m v∈a Ωx(cid:13) (cid:13)α⊤G ψ(v)−G ⋆(v)(cid:13) (cid:13)(cid:17)2
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Discretizationbias Approximationbias
(cid:32) (cid:33)
(cid:0) (cid:1)
×exp B B +∥G (0)∥ L τexp L L τ .
β,2 W ⋆ op x G⋆ x
Proof. Wehavethefollowing.
NCDEs. WefirstconsiderthecasewheretheindividualtimeseriesareembeddedusingNCDEs. Onethenhas
D2(λ ,λD)= 1 (cid:88)n (cid:90) τ (cid:0) α⊤zi,D(s)+β⊤Wi−zi(s)+β⊤Wi(cid:1)2 λi(s)Yi(s)ds (64)
n ⋆ θ n θ ⋆ ⋆ ⋆
i=1 0
= 1 (cid:88)n (cid:90) τ (cid:0) α⊤zi,D(s)−zi(s)+(β−β )⊤Wi(cid:1)2 λi(s)Yi(s)ds (65)
n θ ⋆ ⋆ ⋆
i=1 0
≤ 2 (cid:88)n (cid:90) τ (cid:0) α⊤zi,D(s)−zi(s)(cid:1)2 λi(s)Yi(s)ds+ 2 (cid:88)n (cid:90) τ (cid:0) (β−β )⊤Wi(cid:1)2 λi(s)Yi(s)ds (66)
n θ ⋆ ⋆ n ⋆ ⋆
i=1 0 i=1 0
wherethelastinequalityisobtainedusing(a+b)2 ≤ 2a2+2b2. Sincethisistrueforallθ ∈ Θ,wefirstchoseβ = β ,
⋆
hencecancellingthesecondterm. Turningtotheremainingterm,wehave
α⊤zi,D(s)−zi(s)=α⊤zi,D(s)−α⊤zi(s)+α⊤zi(s)−zi(s), (67)
θ ⋆ θ θ θ ⋆
wherezi(s)isthesolutiontotheCDEdrivenbycontinuousunobservedpath
θ
dzi(t)=G (zi(t))dxi(t) (68)
θ ψ θ
withinitialconditionzi(0)=0∈R. Usingthecontinuityoftheflow,oneobtains
(cid:13) (cid:13)
|α⊤zi,D(s)−α⊤zi(s)|≤∥α∥(cid:13)zi,D(s)−zi(s)(cid:13) (69)
θ θ (cid:13) θ θ (cid:13)
≤B αexp(L GψL
x)(cid:0)
1+L GψL
xK(cid:1)(cid:13) (cid:13)xi−xi,D(cid:13)
(cid:13) ∞. (70)
Usingthefactthat
(cid:13) (cid:13)xi−xi,D(cid:13)
(cid:13)≤L x|D|, (71)
onefinallyobtains
|α⊤zi,D(s)−α⊤zi(s)|≤B exp(L L )(cid:0) 1+L L K(cid:1) L |D|. (72)
θ θ α Gψ x Gψ x x
Usingagainthecontinuityoftheflow,onealsohasthat
|α⊤z θi(s)−z ⋆i(s)|≤exp(L GψL x)m v∈a Ωx(cid:13) (cid:13)α⊤G ψ(v)−G ⋆(v)(cid:13) (cid:13) (73)
sinceα⊤zi(s)isthesolutiontotheCDE
θ
dui(t)=α⊤G (ui(t))dxi(t) (74)
ψ
withinitialconditionui(0)=0. Puttingeverythingtogether,oneobtains
(cid:34) (cid:35)
(α⊤z θi,D(s)−z ⋆i(s))2 ≤2exp(2L GψL x) (cid:16) B α(cid:0) 1+L GψL xK(cid:1) L x|D|(cid:17)2 +(cid:16) m v∈a Ωx(cid:13) (cid:13)α⊤G ψ(v)−G ⋆(v)(cid:13) (cid:13)(cid:17)2 . (75)
21DynamicSurvivalAnalysiswithControlledLatentStates
Pluggingthisresultintheoriginalinequalityonthesquaredlog-divergenceyields
(cid:34) (cid:35)
D2 n(λ ⋆,λD
θ
)≤2τexp(2L GψL x) (cid:16) B α(cid:0) 1+L GψL xK(cid:1) L x|D|(cid:17)2 +(cid:16) m v∈a Ωx(cid:13) (cid:13)α⊤G ψ(v)−G ⋆(v)(cid:13) (cid:13)(cid:17)2 (76)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Discretizationbias Approximationbias
(cid:32) (cid:33)
(cid:0) (cid:1)
×exp B B +∥G (0)∥ L τexp L L τ . (77)
β,2 W ⋆ op x G⋆ x
Signature-basedembeddings. Weproceedinasimilarfashion. Wehave
(cid:0) logλi,D(s)−logλi(s)(cid:1)2 =(cid:16) α⊤S (xi,D )+β⊤Wi−α⊤S(xi,D )−β⊤Wi(cid:17)2 (78)
θ ⋆ N [0,s] ⋆ [0,s] ⋆
(cid:16) (cid:17)2
= α⊤S (xi,D )−α⊤S(xi,D +(β−β )⊤Wi . (79)
N [0,s] ⋆ [0,s] ⋆
Now,onefurthermorehas
α⊤S (xi,D )−α⊤S(xi ) (80)
N [0,s] ⋆ [0,s]
=α⊤S (xi,D )−α⊤ S (xi )+α⊤ S (xi )−α⊤S(xi ). (81)
N [0,s] ⋆,N N [0,s] ⋆,N N [0,s] ⋆ [0,s]
Inparticular,forα=α ,wehave
⋆,N
(cid:16) (cid:17)
α⊤ S (xi,D )−α⊤S(xi )=α⊤ S (xi,D )−S (xi ) (82)
⋆,N N [0,s] ⋆ [0,s] ⋆,N N [0,s] N [0,s]
+α⊤ S (xi )−α⊤S(xi ). (83)
⋆,N N [0,s] ⋆ [0,s]
UsingPropositionA.4,oneobtains
(dL s)N+1
|α⋆,N⊤S (xi )−α⊤S(xi )|≤ x Λ (G ). (84)
N [0,s] ⋆ [0,s] (N +1)! N+1 ⋆
Additionally,usingTheoremA.7,wehave
(cid:16) (cid:17)
|α⊤ S (xi,D )−S (xi ) |≤∥α ∥c (N)|D |. (85)
⋆,N N [0,s] N [0,s] ⋆,N 3 [0,s]
Oneobtainsforθ =(α ,β )that
⋆,N ⋆
(cid:0) logλi,D(s)−logλi(s)(cid:1)2 ≤2(cid:104)(dL xs)N+1 Λ (G )(cid:105)2 +2∥α ∥2C2|D |2 (86)
θ0 ⋆ (N +1)! N+1 ⋆ ⋆,N [0,s]
usingthefactthat(a+b)2 ≤2a2+2b2. Finally,integratingyields
D2(λ ,λD)≤ 1 (cid:88)n exp(cid:32) B B +∥G (0)∥ L τexp(cid:0) L L τ(cid:1)(cid:33) 2τ(cid:104)(dL x)N+1 Λ (G )(cid:105)2(cid:90) τ s2(N+1)ds
n ⋆ θ n β,2 W ⋆ op x G⋆ x (N +1)! N+1 ⋆
i=1 0
(87)
n (cid:32) (cid:33)
+ 1 (cid:88) exp B B +∥G (0)∥ L τexp(cid:0) L L τ(cid:1) 2τ∥α ∥2c (N)2|D|2 (88)
n β,2 W ⋆ op x G⋆ x ⋆,N 3
i=1
(cid:32) (cid:33)(cid:34) (cid:35)2
≤2τexp B B +∥G (0)∥ L
τexp(cid:0)
L L
τ(cid:1) (dL x)N+1
Λ (G )
τ2N+3
(89)
β,2 W ⋆ op x G⋆ x (N +1)! N+1 ⋆ 2N +3
(cid:32) (cid:33)
+2τexp B B +∥G (0)∥ L τexp(cid:0) L L τ(cid:1) ∥α ∥2c (N)2|D|2. (90)
β,2 W ⋆ op x G⋆ x ⋆,N 3
22DynamicSurvivalAnalysiswithControlledLatentStates
UsingLemmaA.6,wecanfurthermoresimplifytheboundto
(cid:32) (cid:33)(cid:34) (cid:35)2
D2(λ ,λD)≤2τexp B B +∥G (0)∥ L τexp(cid:0) L L τ(cid:1) (dL x)N+1 Γ (G ) τ2N+3 (91)
n ⋆ θ β,2 W ⋆ op x G⋆ x (N +1)! N+1 ⋆ 2N +3
(cid:124) (cid:123)(cid:122) (cid:125)
Approximationbias
(cid:32) (cid:33) N
+2τexp B B +∥G (0)∥ L τexp(cid:0) L L τ(cid:1) c (N)2(cid:88) dkΓ (G )2|D|2. (92)
β,2 W ⋆ op x G⋆ x 3 k ⋆
k=1
(cid:124) (cid:123)(cid:122) (cid:125)
Discretizationbias
B.4.ProofofTheorem3.4
TheoremB.4(FormalstatementofTheorem3.4). Considerthesignature-basedembedding.Letθˆ=(αˆ,βˆ)bethesolution
of (6)withpen(θ) = η ∥α∥ +η ∥β∥ . UnderAssumptions1,2,3and4,andwritingβ = (β(1),...,β(s)),wehave
1 1 2 1 ⋆ ⋆ ⋆
foranyN ≥1andanyx>0thatwithprobabilitygreaterthan1−4e−x
(cid:32) (cid:33)(cid:34) (cid:35)2
ℓD(θˆ)−ℓ⋆ ≤2τexp B B +∥G (0)∥ L τexp(cid:0) L L τ(cid:1) (dL x)N+1 Γ (G ) τ2N+3
n n β,2 W ⋆ op x G⋆ x (N +1)! N+1 ⋆ 2N +3
(cid:124) (cid:123)(cid:122) (cid:125)
Approximationbias
(cid:32) (cid:33) N
+2τexp B B +∥G (0)∥ L τexp(cid:0) L L τ(cid:1) c (N)2(cid:88) dkΓ (G )2|D|2
β,2 W ⋆ op x G⋆ x 3 k ⋆
k=1
(cid:124) (cid:123)(cid:122) (cid:125)
Discretizationbias
(cid:115)
+2(L xτ)k⋆ 2(cid:0) x+log(NdN)(cid:1) λ
∞
(cid:88)N
dkΓ (G )+2B sup |β(k)|B
(cid:114) 2τλ ∞(x+logs)
.
k⋆! n k ⋆ β,0 ⋆ W n
k=1,...,s
k=1
Byoptimalityofθˆ,wehaveforallθ ∈Θthat
ℓD(θˆ)+pen(θˆ)−ℓ⋆ ≤ℓD(θ)+pen(θ)−ℓ⋆ (93)
n n n n
andhence,usingProposition3.1,oneobtains
KL (λ ,λD)≤KL (λ ,λD)+pen(θ)−pen(θˆ)+
1 (cid:88)n (cid:90) logλi θˆ,D(s)
dMi(s). (94)
n ⋆ θˆ n ⋆ θ n λi,D(s)
i=1 θ
UsingProposition3.2,theKL-divergenceontherighthandsidecanbeboundedbythesquaredlogdivergence,yielding
KL (λ ,λD)≤C D2(λ ,λD)+pen(θ)−pen(θˆ)+
1 (cid:88)n (cid:90) logλi θˆ,D(s)
dMi(s). (95)
n ⋆ θˆ 2 n ⋆ θ n λi,D(s)
i=1 θ
Wenowstudytheterm
1 (cid:88)n (cid:90) τ logλi θ,D(s) dMi(s)= 1 (cid:88)n (cid:90) τ (cid:0) α⊤S (Xi )+β⊤Wi−α⊤ S(xi )−β⊤Wi(cid:1) dMi(s)
n λi,D(s) n N [0,s] ⋆,N [0,t] ⋆
i=1 0 θ⋆ i=1 0
N
1 (cid:88)n (cid:90) τ
=(α−α )⊤ S (Xi )dMi(s)
⋆,N n N [0,s]
i=1 0
n
1 (cid:88)
+(β−β )⊤ WiMi(t).
⋆ n
i=1
whichappearsinProposition3.1whenconsideringsignaturebasedembeddings. Wemakearepeateduseofthefollowing
lemmasinourderivationofaboundforthisterm.
23DynamicSurvivalAnalysiswithControlledLatentStates
LemmaB.5. LetS (Xi ) ∈ Rbethesignaturecoefficientassociatedtothej-thwordofthek-thsignaturelayerof
[k],j [0,t]
atimeseriesXi evaluatedattimet≤τ. Thenwehave
[0,t]
(cid:13) (cid:13) (L t)k (L τ)k
(cid:13)S (Xi )(cid:13) = max |S (Xi )|≤ x ≤ x .
(cid:13) [k],j [0,·] (cid:13) ∞,[0,t] s∈[0,t] [k],j [0,s] k! k!
Proof. Foralls∈[0,t],wehave
(cid:13) (cid:13)
(cid:13) (cid:13)Xi(cid:13) (cid:13)k
|S (Xi )|≤(cid:13)S (Xi )(cid:13)≤ 1-var,[0,t] (96)
[k],j [0,s] (cid:13) [k] [0,s] (cid:13) k!
whereS (Xi )referstothefullsignaturelayerofdepthk,andthelastinequalitycanbefoundinFermanian(2021).
[k] [0,s]
UsingAssumption1,wehave
(cid:13) (cid:13)Xi(cid:13) (cid:13)k (cid:13) (cid:13)xi(cid:13) (cid:13)k
(L t)k (L τ)k
1-var,[0,t] ≤ 1-var,[0,t] ≤ x ≤ x . (97)
k! k! k! k!
Thefollowingdeviationinequalityisadirectconsequencefromtheonein(VandeGeer,1995)andderivesfrominequali-
tiesforgeneralmartingalesthatcanbefoundin(Shorack&Wellner,2009)forinstance.
LemmaB.6(Deviationinequalityforamartingale). LetΥbealocallysquareintegrablemartingale.Then,foranyx>0
andt≥0,thefollowingholdstruefor
P(cid:16)(cid:12) (cid:12)Υ(t)(cid:12) (cid:12)≥(cid:112) 2v(t)x+ B(t)x ,⟨Υ(t)⟩≤v(t), sup |∆Υ(s)|≤B(t)(cid:17) ≤2e−x, (98)
3
s∈[0,t]
where⟨Υ(t)⟩isthepredictablevariationofM and∆Υ(t)itsjumpattimet.
Decomposingonthesignaturelayers,wecanwrite
(cid:12) 1 (cid:88)n (cid:90) τ (cid:12) (cid:12)(cid:88)N 1 (cid:88)n (cid:90) τ (cid:12)
(cid:12)(α−α )⊤ S (Xi )dMi(s)(cid:12)=(cid:12) (α −α )⊤ S (Xi )dMi(s)(cid:12)
(cid:12) ⋆,N n N [0,s] (cid:12) (cid:12) [k] ⋆,[k] n [k] [0,s] (cid:12)
i=1 0 k=1 i=1 0
(cid:88)N (cid:12)1 (cid:88)n (cid:90) τ (cid:12)
≤ ∥α −α ∥ sup (cid:12) S (Xi )dMi(s)(cid:12).
[k] ⋆,[k] 1 (cid:12)n [k],j [0,s] (cid:12)
k=1 1≤j≤dk i=1 0
BecausethemartingalesMiareindependent,andthesignaturecoefficientsbounded,theterm
1 (cid:88)n (cid:90)
χ (τ)= S (Xi )dMi(s)
n n [k],j [0,s]
i=1
itselfamartingale. MoreoveraseachMi comesfromacountingprocessviaaDoobMeierdecomposition,itsjumpsare
bounded by 1 and, at a given time, there is (almost surely) at most one Mi that jumps. As a consequence, we get the
followingboundwithjumpsboundedby
(cid:12) (cid:12) 1 (L τ)k
sup(cid:12)∆χ (t)(cid:12)≤ sup ∥S (Xi )∥ ≤ x
(cid:12) n (cid:12) n [k],j [0,·] ∞,[0,τ] nk!
[0,τ] i=1,...,n
andquadraticvariationgivenattimeτ ≥0by
⟨χ(t)⟩=(cid:10)1 (cid:88)n (cid:90) τ S (Xi )dMi(s)(cid:11) = 1 (cid:88)n (cid:90) τ S (Xi )2λi(s)Yi(s)ds
n [k],j [0,s] n2 [k],j [0,s] ⋆
i=1 0 i=1 0
n
1 (cid:88) 1
≤ sup ∥S (Xi )∥2 Λi(t)≤ sup ∥S (Xi )∥2 sup Λi(t)
[k],j [0,·] ∞,[0,τ]n2 ⋆ n [k],j [0,·] ∞,[0,τ] ⋆
i=1,...,n i=1,...,n i=1,...,n
i=1
L2kτ2k+1
≤ x , (99)
n(k!)2λ
∞
24DynamicSurvivalAnalysiswithControlledLatentStates
thankstoPropositionsB.5,thefactthat
(cid:90)
Λi(t)= λi(s)ds≤tsupλi(s)≤τsupλi(s)≤τλ
⋆ ⋆ ⋆ ⋆ ∞
[0,t] [0,t] [0,τ]
andwhere
(cid:0) (cid:0) (cid:0) (cid:1)(cid:1)
λ =exp B B )exp ∥G (0)∥ L τexp L L τ
∞ β,2 W ⋆ op x G⋆ x
accordingtoLemma2.1. LemmaB.6nowwarrantsthatforanyx>0withaprobabilitygreaterthan1−2e−x
(cid:12) (cid:12)1 (cid:88)n (cid:90) τ S (Xi )dMi(s)(cid:12) (cid:12)≤(cid:115) 2xL2 xk+1τ2k+2λ ∞(cid:1) + x(L xτ)k ≤ (L xτ)k⋆(cid:16)(cid:114) 2xλ ∞ + x (cid:17)
(cid:12)n [k],j [0,s] (cid:12) n(k!)2 3nk! k⋆! n 3n
i=1 0
where
(L τ)k
k⋆ =argmax x .
k≥1 k!
Adoubleunionboundonthesignaturelayersandthesignaturecoefficientswithineachlayerensuresthatforanyx > 0
withaprobabilitygreaterthan1−2e−x
(cid:115)
(L τ)k⋆(cid:16) 2(cid:0) x+log(NdN)(cid:1) λ x(cid:0) x+log(NdN)(cid:1) (cid:17)
sup sup ≤ x ∞ + .
k⋆! n 3n
1≤k≤N1≤j≤dk
Asaconsequence
(cid:115)
(cid:12)
(cid:12)(αˆ−α
)⊤1 (cid:88)n (cid:90) τ
S (Xi
)dMi(s)(cid:12)
(cid:12)≤∥α−α ∥
(L xτ)k⋆(cid:16) 2(cid:0) x+log(NdN)(cid:1) λ
∞ +
x+log(NdN)(cid:17)
(cid:12) ⋆,N n N [0,s] (cid:12) ⋆,N 1 k⋆! n 3n
i=1 0
foranyx>0withaprobabilitygreaterthan1−2e−x.
Weapplythesamelineofreasoningto
(cid:12) 1 (cid:88)n (cid:12) (cid:12)1 (cid:88)n (cid:12)
(cid:12)(β−β )⊤ WiMi(t)(cid:12)≤∥β−β ∥ sup (cid:12) Wi Mi(t)(cid:12).
(cid:12) ⋆ n (cid:12) ⋆ 1 (cid:12)n m (cid:12)
1≤m≤s
i=1 i=1
n
Foreachm,theterm (cid:80) Wi Mi(·)isamartingalewithpredictablevariationlessthat
m
i=1
n
(cid:88)
(Wi )2Λi(t)≤B2 τλ .
m ⋆ W ∞
i=1
Its jumps are bounded by B . Lemma B.6 applies. Via an union bound, we deduce that for any x > 0 and with a
W
probabilitygreaterthat1−2e−x
(cid:12)
(cid:12)(βˆ−β
)⊤1 (cid:88)n WiMi(t)(cid:12)
(cid:12)≤∥βˆ−β ∥
≤(cid:114) 2B W2 τλ ∞(x+logs)
+
B W(x+logs)
.
(cid:12) ⋆ n (cid:12) ⋆ 1 n 3n
i=1
Nowdefiningthepenalty
(cid:115)
(L τ)k⋆(cid:16) 2(cid:0) x+log(NdN)(cid:1) λ x+log(NdN)(cid:17)
pen(θ)=∥α∥ x ∞ +
1 k⋆! n 3n
(cid:114)
(cid:16) 2B2 τλ (x+logs) B (x+logs)(cid:17)
+∥β∥ W ∞ + W ,
1 n 3n
25DynamicSurvivalAnalysiswithControlledLatentStates
wejustshowedthat
pen(θ )−pen(θˆ)+
1 (cid:88)n (cid:90) logλi θˆ,D(s)
dMi(s)
⋆,N n λi,D(s)
i=1 θ
(cid:115)
(L τ)k⋆(cid:16) 2(cid:0) x+log(NdN)(cid:1) λ x+log(NdN)(cid:17)
≤∥α ∥ x ∞ +
⋆,N 1 k⋆! n 3n
(cid:114)
(cid:16) 2B2 τλ (x+logs) B (x+logs)(cid:17)
+∥β ∥ W ∞ + W
⋆ 1 n 3n
withaprobabilitygreaterthat1−4e−xforanyx>0. Fornlargeenough,wecanwrite
(cid:115)
(L τ)k⋆(cid:16) 2(cid:0) x+log(NdN)(cid:1) λ x+log(NdN)(cid:17)
∥α ∥ x ∞ +
⋆,N 1 k⋆! n 3n
(cid:114)
2τλ (x+logs) B (x+logs)
+∥β ∥ B ∞ + W
⋆ 1 W n 3n
(cid:115)
(L τ)k⋆ 2(cid:0) x+log(NdN)(cid:1) λ (cid:114) 2τλ (x+logs)
≤2∥α ∥ x ∞ +2∥β ∥ B ∞ .
⋆,N 1 k⋆! n ⋆ 1 W n
Finally,usingLemmaA.6andAssumption3,wecanboundtheℓ normsofα andβ andobtainthatforlargen,for
1 ⋆,N ⋆,N
anyx>0wehavewithprobabilitygreaterthan1−4e−xthat
(cid:115)
(L τ)k⋆ 2(cid:0) x+log(NdN)(cid:1) λ (cid:114) 2τλ (x+logs)
2∥α ∥ x ∞ +2∥β ∥ B ∞
⋆,N 1 k⋆! n ⋆ 1 W n
(cid:115)
≤2(L xτ)k⋆ 2(cid:0) x+log(NdN)(cid:1) λ
∞
(cid:88)N
dkΓ (G )+2B sup |β(k)|B
(cid:114) 2τλ ∞(x+logs)
.
k⋆! n k ⋆ β,0 ⋆ W n
k=1,...,s
k=1
26DynamicSurvivalAnalysiswithControlledLatentStates
C.AlgorithmicandImplementationDetails
InthisSection,weprovideextrainformationaboutlearningalgorithmsdescribedinthemainpaperandtheirhyperparam-
etersoptimizationbygridsearchmethod.
C.1.DescriptionofCompetingMethods
C.1.1.COXSIGANDCOXSIG+
Implementation. Weuseiisignature(Reizenstein&Graham,2018)tocomputesignatures. Alternativesforcom-
putingsignaturesincludethesignatorylibrary(Kidger&Lyons,2020).
Training. Weminimizethepenalizednegativelog-likelihood(definedin6inthemainpaper)usingavanillaproximal
pointalgorithm(Boyd&Vandenberghe,2004).
Hyperparameters. Theinitiallearningrateoftheproximalgradientalgorithmissettoe−3andthelearningrateforeach
iterationischosenbybacktrackinglinesearchmethod(Boyd&Vandenberghe,2004).Thehyperparametersofpenalization
strength (η ,η ) and truncation depth N are chosen by 1-fold cross-validation of a mixed metric equal to the difference
1 2
between the C-index and the Brier score. We select the best hyperparameters that minimizes the average of this mixed
metriconthevalidationset. Welistthehyperparameterssearchspaceofthisalgorithmbelow.
• η : {1,e−1,e−2,e−3,e−4,e−5};
1
• η : {1,e−1,e−2,e−3,e−4,e−5};
2
• N:{2,3}.Largervalueswereconsideredinthebeginningofexperimentsbutwereremovedfromthecross-validation
gridbecausetheyyieldedbadperformanceandnumericalinstabilities.
C.1.2.NCDE
Implementation. Weimplementthefill-forwarddiscreteupdateofNCDEsinPytorch.
Structure. The neural vector field is a feed-forward network composed of two fully connected hidden layers whose
hidden dimension is set to 128. We choose to represent the latent state in 4 dimensions, then the number of node in the
inputlayerissetto4. Thedimensionoftheoutputlayerisequaltomultiplicationofhiddenlayerdimensionwhichis128
andthedimensionofsamplepathsofaspecificdataset. tanhissettobetheactivationfunctionforallthenodesinthe
network.
Training. Themodelwastrainedfor50epochsusingAdamoptimizer(Kingma&Ba,2014)withbatchsizeof32and
crossvalidatedlearningratesettoe−4.
C.1.3.COXMODEL
ImplementationandTraining. WeuseaclassicalCoxmodelwithelastic-netpenaltyasabaseline,whichisgiveneither
thefirstmeasuredvalueoftheindividualtimeseriesorthestaticfeaturesiftheyareavailable. Theintensityofthismodel
istheninform
λi(t)=λ (t)exp(β⊤Wi), (100)
θ 0
whereWi =Xi(0)ifthereisnostaticfeaturesareavailable. WeusetheimplementationprovidedinthePythonpackage
scikit-survivalandcalledCoxnetSurvivalAnalysis(Po¨lsterl,2020).
Hyperparameters. The ElasticNet mixing parameter γ is set to 0.1. The hyperparameter of penalization strength η is
chosen by cross-validation similarly to the one described above. We crossvalidate over the set {1, e−1, e−2, e−3, e−4,
e−5}toselectthebestvalue.
C.1.4.RANDOMSURVIVALFOREST
Implementation. We use the implementation of RSF (Ishwaran et al., 2008) provided in the Python package
scikit-survival(Po¨lsterl,2020).
27DynamicSurvivalAnalysiswithControlledLatentStates
Training. We train this model with static features Wi as the only input. Similarly to our implementation of the Cox
model,weusethefirstvalueofthetimeseriesasstaticfeaturesifnootherfeaturesareavailable.
Hyperparameters. Wecrossvalidatetwohyperparametersonthefollowinggrid.
• max features: {None,sqrt};
• min samples leaf: {1,5,10};
C.1.5.DYNAMICDEEP-HIT(LEEETAL.,2019)
DDHisadynamicalsurvivalanalysisalgorithmwhichframesdynamicalsurvivalanalysisasaclassificationproblem. It
divides the considered time period [0,τ] into a set of contiguous time intervals. The network is then trained to predict a
timeintervalofeventforeverysubject,whichisamulticlassclassificationtask.
NetworkArchitecture. Beingadaptedtocompetingevents,DynamicDeep-Hitcombinesasharednetworkwithacause-
specific network. The shared network is a combination of a RNN-like network that processes the longitudinal data and
anattentionmechanism,whichhelpsthenetworkdecidewhichpartofthehistoryofthemeasurementsisimportant. The
cause-specificnetworkisafeed-forwardnetworktakingasaninputthehistoryofembeddedmeasurementsandlearninga
cause-specificrepresentation. See6foragraphicalrepresentationofthenetwork’sstructure.
Figure6: NetworkstructureofDynamicDeepHit. FiguretakenfromLeeetal.(2019).
LossFunction. ThelossfunctionofDDHisasumofthreelossfunctions
ℓ =ℓ +ℓ +ℓ .
DynamicDeepHit log-likelihood ranking prediction
Thefirstlossmaximizestheconditionallikelihoodofdyingininterval[t ,t [giventhattheindividualhassurvivedup
k k+1
to time t . On a side note, we notice that the claim of Lee et al. (2019) that this loss corresponds to the ”the negative
k
log-likelihoodofthejointdistributionofthefirsthittingtimeandcorrespondingeventconsideringtheright-censoring”of
thedataishenceinexact. ThismightexplaintheresultsobservedinFigure5:DDH’sperformance,intermsofBrierscore,
stronglydegradesasδtincreasesbecausethemodelisonlytrainedtopredictonestepahead, insteadofmaximizingthe
fulllikelihood.
Thesecondlossfavorscorrectrankingsamongatriskindividuals: anindividualexperiencinganeventattimeTi should
haveahigherriskscoreattimet<Tithananindividualj forwhichTj >Ti.
Thethirdlossisapredictionloss,whichmeasuresthedifferencebetweenthevalueofthetime-dependantfeaturesanda
predictionofthisvaluemadebythesharednetwork. ThelossisminimizedusingAdam(Kingma&Ba,2014).
Hyperparameters. In our setting, we use the network in its original structure. The learning rate is set to e−4 and the
numberofepochsto300.
28DynamicSurvivalAnalysiswithControlledLatentStates
C.2.ComputationoftheDifferentMetrics
ThefollowingLemmadetailsthecomputationoftheconditionalsurvivalfunction.
LemmaC.1. Foranyi∈{1,...,n},
(cid:90) t+δt
ri(t,δt)=exp(− λi(u,Xi )du),
θ θ [0,u∧t]
t
(cid:16) (cid:17)
whereri(t,δt) = P Ti > t+δt|Ti > t,Xi isthesurvivalfunctionofindividuali,asestimatedbythemodelwith
θ [0,t]
parametersθ,attimet+δtforδt>0conditionalonsurvivaluptotimet,andonobservationofthelongitudinalfeatures
up to time t, and the notation λi(u,Xi ) means that the intensity at time u is computed by using the longitudinal
θ [0,u∧t]
featuresuptotimeu∧t=min(u,t).
Proof. SinceBayesrulegives
(cid:16) (cid:17)
P Ti >t+δt|Xi ,Wi
ri(t,δt)=P(cid:16)
Ti >t+δt|Ti >t,Xi
,Wi(cid:17)
=
[0,t]
,
θ [0,t] (cid:16) (cid:17)
P Ti >t|Xi ,Wi
[0,t]
wecancomputethisscorebyusingthefactthat
(cid:16) (cid:17)
P Ti >t|Xi ,Wi =exp(−Λi,D(t)),
[0,t] θ
wherewerecallthatΛi,D(t)isthecumulativehazardfunction
θ
(cid:90) t
Λi,D(t):= λi,D(s)Yi(s)ds.
θ θ
0
WereferthereaderunfamiliarwithsurvivalanalysistoAalenetal.(2008,Chapter1,p. 6)foraproofofthisexpression
ofthesurvivalfunction. Thisthenyields
exp(−(cid:82)t+δt λi(u,Xi )du)
ri(t,δt)= 0 θ [0,u∧t]
θ exp(−(cid:82)t λi(u,Xi )du)
0 θ [0,u∧t]
(cid:90) t+δt
=exp(− λi(u,Xi )du).
θ [0,u∧t]
t
Besidethetwometricsdescribedinthemainpaper,wereportourresultsintermoftwomoremetricsnamelytheweighted
Brier Score and the area under the receiver operating characteristic curve (AUC). The details of these metrics are given
below.
WeightedBrierScore. TheweightedversionoftheBrierscore,whichwewriteWBS(t,δt),isdefinedas
(cid:88)n ri(t,δt))2 (1−ri(t,δt))2
1 θ +1 θ ,
Ti≤t,∆i=1 Gˆ(Ti) Ti≥t Gˆ(t)
i=1
whereGˆ(·)istheprobabilityofcensoringweight,estimatedbytheKaplan-Meierestimator.
AUC. WedefinethetheareaunderthereceiveroperatingcharacteristiccurveAUC(t,δt)as
n n
(cid:80) (cid:80) 1 1 w
ri(t,δt)>rj(t,δt) Ti>t+δt,Tj∈[t,t+δt] j
i=1j=1 θ θ
,
n n
((cid:80)1 )((cid:80)1
w )
Ti>t+δt Ti∈[t,t+δt] i
i=1 i=1
wherew areinverseprobabilityofcensoringweights,estimatedbytheKaplan-Meierestimator.
i
29DynamicSurvivalAnalysiswithControlledLatentStates
D.DetailsofExperimentsandDatasets
ThemaincharacteristicsofthedatasetsusedinthepaperaresummarizedinTable1andweprovidemoredetailedinfor-
mation of these datasets in subsections below. For the experiments, each dataset is randomly divided into a training set
(80%)andtestset(20%). Hyperparameteroptimizationisperformedasfollows. Wesplitthethetrainingset, using4/5
fortrainingand1/5forvalidation. Wethenre-fitonthewholetrainingsetwiththebesthyperparametersandreportthe
resultsonthetestsetfor10runs. Notethattheperformanceisevaluatedatnumerouspoints(t,δ ),wheretissettothe
t
5th,10th,and20thpercentileofthedistributionofeventtimes.
Name n d StaticFeatures Censoring Avg.ObservationTimes Source
Hittingtime 500 5 ✗ Terminal(3.2%) 177 Simulation
TumorGrowth 500 2 ✗ Terminal(8.4%) 250 Simeonietal.(2004)
PredictiveMaintenance 200 17 ✗ Online(50%) 167 Saxenaetal.(2008)
Churn 1043 14 ✗ Terminal(38.4%) 25 Privatedataset
Table 1: Description of the 4 datasets we consider. The integer d is the dimension of the time series including the time
channel. Terminalcensoringmeansthattheindividualsarecensoredattheendoftheoverallobservationperiod[0,τ]if
theyhavenotexperiencedanyevent. Itisopposedtoonlinecensoringthatcanhappenatanytimein[0,τ]. Thereported
percentageindicatesthecensoringleveli.e. theshareofthepopulationthatdoesnotexperiencetheevent.
D.1.HittingTimeofapartiallyobservedSDE
Timeseries. Thepathsx = (x(1),...,x(d−1))are(d−1)-dimensionalsamplepathsofafractionalBrownianmotion
t t t
withHurstparameterH = 0.6,andBi(t)isaBrowniannoiseterm. Wesetd = 5. Thepathsaresampledat1000times
overthetimeinterval[0,10].Allsimulationsaredoneusingthestochasticpackage2.ThetimeseriesXiareidentical,
uptoobservationtime,totheonesusedforsimulations.
EventdefinitionWeconsiderthestochasticdifferentialequation
d
dw =−ω(w −µ)dt+(cid:88) dx(i)+σdB ,
t t t t
i=1
wherew istrajectoryofeachindividualwith(σ,µ,ω) ∈ R3 arefixedparameters. Inourexperiment,theparametersare
t
chosentobeσ =1,µ=0.1andω =0.1. Wethendefinethetime-of-eventasthetimewhentrajectorycrossthethreshold
w ∈Rduringtheobservationperiod[t t ],whichis
⋆ 0 N
T⋆ =min{t ≤t≤t |w ≥w }.
0 N t ⋆
Inourexperiments,weusethethresholdvaluew = 2.5. ThetargetSDEissimulatedusinganEulerdiscretization. We
⋆
trainonn=500individuals.
CensorshipWecensorindividualswhosetrajectorydoesnotcrossthethresholdduringtheobservationperiod.Thismeans
that individuals are never censored during the observation period, but only at the end. The simulated censoring level is
3.2%.
SupplementaryFigures. Figure7providesanexampleoffullsamplepathofanindividualanddistributionoftheevent
timesofthewholepopulation. WeaddadditionalresultsonthetestsetinFigures8,9,10,11and12.
D.2.TumorGrowth
Time-series. Similarly to the partially observed SDE experiment described above, we set d = 2 which includes 1-
dimensional sample path x of a fractional Brownian motion with Hurst parameter H = 0.6. The paths are sampled at
t
1000timesoverthetimeinterval[0,10]. Allsimulationsaredoneusingthestochasticpackage. ThetimeseriesXi
areidentical,uptoobservationtime,totheonesusedforsimulations.
2Availableathttps://github.com/crflynn/stochastic
30DynamicSurvivalAnalysiswithControlledLatentStates
4 xt(1) xt(3)
xt(2) xt(4) 80
2
60
0
2 40
4
20
6
0
0 2 4 6 8 10 0 2 4 6 8 10
Time Time
Figure7: Fullsamplepathofanindividual(left)anddistributionoftheeventtimes(left)forthepartiallyobservedSDEexperiment.
Thesurgeineventsatterminaltimeindicatesterminalcensorship.
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=0.17, t=0.06 t=0.17, t=0.10 t=0.17, t=0.17
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=0.23, t=0.06 t=0.23, t=0.10 t=0.23, t=0.17
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=0.31, t=0.06 t=0.31, t=0.10 t=0.31, t=0.17
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure8: C-Index(higherisbetter)forhittingtimeofapartiallyobservedSDEfornumerouspoints(t,δt).
Eventdefinition. FollowingSimeonietal.(2004),weconsiderthedifferentialequations
du(1) λ u(1)
t = 0 t −κ x u(1)
dt (cid:2) 1+(λ λ0 1w t)Ψ(cid:3)1/Ψ 2 t t
du(2)
t =κ x u(1)−κ u(2)
dt 2 t t 1 t
du(3)
t =κ (u(2)−u(3))
dt 1 t t
du(4)
t =κ (u(3)−u(4))
dt 1 t t
w =u(1)+u(2)+u(3)+u(4),
t t t t t
31
xednI-C
xednI-C
xednI-C
tx
xednI-C
xednI-C
xednI-C
tnuoC
xednI-C
xednI-C
xednI-CDynamicSurvivalAnalysiswithControlledLatentStates
0.25 0.25 0.25
0.20 0.20 0.20
0.15 0.15 0.15
0.10 0.10 0.10
0.05 0.05 0.05
t=0.17, t=0.06 t=0.17, t=0.10 t=0.17, t=0.17
0.00 0.00 0.00
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.25 0.25 0.25
0.20 0.20 0.20
0.15 0.15 0.15
0.10 0.10 0.10
0.05 0.05 0.05
t=0.23, t=0.06 t=0.23, t=0.10 t=0.23, t=0.17
0.00 0.00 0.00
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.25 0.25 0.25
0.20 0.20 0.20
0.15 0.15 0.15
0.10 0.10 0.10
0.05 0.05 0.05
t=0.31, t=0.06 t=0.31, t=0.10 t=0.31, t=0.17
0.00 0.00 0.00
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure9: Brierscore(lowerisbetter)forhittingtimeofapartiallyobservedSDEfornumerouspoints(t,δt).
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
t=0.17, t=0.06 t=0.17, t=0.10 t=0.17, t=0.17
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
t=0.23, t=0.06 t=0.23, t=0.10 t=0.23, t=0.17
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
t=0.31, t=0.06 t=0.31, t=0.10 t=0.31, t=0.17
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure10: WeightedBrierscore(lowerisbetter)forhittingtimeofapartiallyobservedSDEfornumerouspoints(t,δt).
32
erocS
reirB
erocS
reirB
erocS
reirB
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
erocS
reirB
erocS
reirB
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
erocS
reirB
erocS
reirB
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
dethgieWDynamicSurvivalAnalysiswithControlledLatentStates
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=0.17, t=0.06 t=0.17, t=0.10 t=0.17, t=0.17
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=0.23, t=0.06 t=0.23, t=0.10 t=0.23, t=0.17
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=0.31, t=0.06 t=0.31, t=0.10 t=0.31, t=0.17
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure11: AUC(higherisbetter)forhittingtimeofapartiallyobservedSDEfornumerouspoints(t,δt).
104 103
103
102
102
101
101
100 100
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure12:RunningtimesonthepartiallyobservedSDEexperiment(log-scale)averagedover10runsincludingcross-validationofthe
hyperparametersonCoxSig,CoxSig+,CoxandRSF(left)andover1runwithoutcross-validationofthehyperparametersonCoxSig,
CoxSig+,CoxandRSF(right).
where w is trajectory of each individual with initial status of (u(1),u(2),u(3),u(4)) = (0.8,0,0,0) and
t 0 0 0 0
(λ ,λ ,κ ,κ ,Ψ) ∈ R5 are fixed parameters. In our experiment, the parameters are chosen to be λ = 0.9, λ = 0.7,
0 1 1 2 0 1
κ =10,κ =0.15andΨ=20. Wethendefinethetime-of-eventasthetimewhentrajectorycrossthethresholdw ∈R
1 2 ⋆
duringtheobservationperiod[t t ],whichis
0 N
T⋆ =min{t ≤t≤t |w ≥w }.
0 N t ⋆
Inourexperiments, weusethethresholdvaluew = 1.7. ThetargetdifferentialequationsaresimulatedusinganEuler
⋆
discretization. Wetrainonn=500individuals.
Censorship. Similarly to the partially observed SDE experiment, we consider terminal censorship: individuals that do
notexperiencetheeventwithintheobservationperiodarecensored. Thecensoringlevelis8.4%.
33
CUA
CUA
CUA
).ces(
emiT
gninnuR
CUA
CUA
CUA
).ces(
emiT
gninnuR
CUA
CUA
CUADynamicSurvivalAnalysiswithControlledLatentStates
xt 100
2.5
2.0 80
1.5 60
1.0
40
0.5
20
0.0
0
0 2 4 6 8 10 0 2 4 6 8 10
Time Time
Figure13: Fullsamplepathofanindividual(left)anddistributionoftheeventtimes(left)forthetumorgrowthexperiment.Thesurge
ineventsatterminaltimeindicatesterminalcensorship.
SupplementaryFigures. Figure13providesanexampleoffullsamplepathofanindividualanddistributionoftheevent
timesofthewholepopulation. WeaddadditionalresultsonthetestsetinFigures14,15,16and17.
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=1.04, t=0.05 t=1.04, t=0.10 t=1.04, t=0.13
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=1.09, t=0.05 t=1.09, t=0.10 t=1.09, t=0.13
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=1.17, t=0.05 t=1.17, t=0.10 t=1.17, t=0.13
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure14: C-Index(higherisbetter)forTumorGrowthfornumerouspoints(t,δt).
D.3.PredictiveMaintenance
Time-series. Thisdatasetdescribesthedegradationof200aircraftgasturbineengines,where22measurementsofsen-
sorsand3operationalsettingsarerecordedeachoperationalcycleuntilitsfailure. Afterremovinglow-variancefeatures,
16longitudinalfeaturesareselectedfortrainingmodels. Theaveragetimelengththesefeaturesisabout25cycles. Note
thatweapplystandardizationforselectedfeaturesbeforetraining.
Eventdefinition. Thetimesofeventaregivenas-isinthedataset. WerefertoSaxenaetal.(2008)foraprecisedescrip-
tionofthedatageneration.
34
xednI-C
xednI-C
xednI-C
tx
xednI-C
xednI-C
xednI-C
tnuoC
xednI-C
xednI-C
xednI-CDynamicSurvivalAnalysiswithControlledLatentStates
0.25 0.25 0.25
0.20 0.20 0.20
0.15 0.15 0.15
0.10 0.10 0.10
0.05 0.05 0.05
t=1.04, t=0.05 t=1.04, t=0.10 t=1.04, t=0.13
0.00 0.00 0.00
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.25 0.25 0.25
0.20 0.20 0.20
0.15 0.15 0.15
0.10 0.10 0.10
0.05 0.05 0.05
t=1.09, t=0.05 t=1.09, t=0.10 t=1.09, t=0.13
0.00 0.00 0.00
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.25 0.25 0.25
0.20 0.20 0.20
0.15 0.15 0.15
0.10 0.10 0.10
0.05 0.05 0.05
t=1.17, t=0.05 t=1.17, t=0.10 t=1.17, t=0.13
0.00 0.00 0.00
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure15: Brierscore(lowerisbetter)forTumorGrowthfornumerouspoints(t,δt).
0.25 0.25 0.25
0.20 0.20 0.20
0.15 0.15 0.15
0.10 0.10 0.10
0.05 0.05 0.05
t=1.04, t=0.05 t=1.04, t=0.10 t=1.04, t=0.13
0.00 0.00 0.00
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.25 0.25 0.25
0.20 0.20 0.20
0.15 0.15 0.15
0.10 0.10 0.10
0.05 0.05 0.05
t=1.09, t=0.05 t=1.09, t=0.10 t=1.09, t=0.13
0.00 0.00 0.00
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.25 0.25 0.25
0.20 0.20 0.20
0.15 0.15 0.15
0.10 0.10 0.10
0.05 0.05 0.05
t=1.17, t=0.05 t=1.17, t=0.10 t=1.17, t=0.13
0.00 0.00 0.00
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure16: WeightedBrierscore(lowerisbetter)forTumorGrowthfornumerouspoints(t,δt).
35
erocS
reirB
erocS
reirB
erocS
reirB
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
erocS
reirB
erocS
reirB
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
erocS
reirB
erocS
reirB
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
dethgieWDynamicSurvivalAnalysiswithControlledLatentStates
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=1.04, t=0.05 t=1.04, t=0.10 t=1.04, t=0.13
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=1.09, t=0.05 t=1.09, t=0.10 t=1.09, t=0.13
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=1.17, t=0.05 t=1.17, t=0.10 t=1.17, t=0.13
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure17: AUC(higherisbetter)forTumorGrowthfornumerouspoints(t,δt).
Censorship. Censorshipisgivenas-ininthedataset. Thecensoringlevelofthisdatasetis50%,whichisahighcensor-
shiprateinsurvivalanalysis. WereferagaintoSaxenaetal.(2008)formoredetails.
SupplementaryFigures. Figure18providesanexampleofseveralrandomlypickedsamplepathsofanindividualand
distributionoftheeventtimesofthewholepopulation. WeaddadditionalresultsinFigures19,20,21and22.
D.4.ChurnPrediction
Forthisdataset,theamountofdetailsthatwecanreleaseislimitedbothbecauseofthesensitivenatureofthedataandof
theanonymityrequirementsofthereviewingprocess.
Time-series. Alllongitudinalfeatureshavebeencomputedonatemporalwindowofoneweek,therawdatacorrespond-
ingtoallproductordersplacedontheplatformfrom06-12-2021to12-11-2023. Forclientswhohavenoorderduringthe
week,wefillzerovaluefortheiralllongitudinalmeasurementsthisweek. Afterremovingfeatureswithmorethan90%
ofmissingness,14longitudinalfeaturesof1043clientsareselectedfortrainingstep. Notethatweapplystandardization
forselectedfeaturesbeforetraining.
Event definition. We consider that a customer has churned if she has no passed any order in the last 4 weeks. If the
customerstartsorderingagainafterachurn,weregisterherasanewcustomer.
Censorship. Censorshipisterminalbasedonthedatacollectionperiod(givedateshere). Henceanycustomerthathas
notchurnedbythe12-11-2023iscensored. Inthisdataset,38.4%oftheclientsareterminallycensored.
SupplementaryFigures. Figure23providesanexampleoffoursamplepathsoffourrandomlychosenindividual. We
addadditionalresultsinFigures24,25,26and27.
36
CUA
CUA
CUA
CUA
CUA
CUA
CUA
CUA
CUADynamicSurvivalAnalysiswithControlledLatentStates
3 14
2 setting1 s7 12
s2 s9
1 10
8 0
6
1
4
2
2
3
0
0 50 100 150 200 250 300 350 0 50 100 150 200 250 300 350
Time Time
Figure18:Partialsamplepathofanindividual(left)anddistributionoftheeventtimes(left)forthepredictivemaintenanceexperiment.
Ontheleft,thetimeseriesisfilledwiththelastobservedvaluefromthetimeofeventon.
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=153.90, t=9.40 t=153.90, t=16.20 t=153.90, t=22.10
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=166.60, t=9.40 t=166.60, t=16.20 t=166.60, t=22.10
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=191.60, t=9.40 t=191.60, t=16.20 t=191.60, t=22.10
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure19: C-Index(higherisbetter)forpredictivemaintenancefornumerouspoints(t,δt).
37
xednI-C
xednI-C
xednI-C
tx
xednI-C
xednI-C
xednI-C
tnuoC
xednI-C
xednI-C
xednI-CDynamicSurvivalAnalysiswithControlledLatentStates
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
t=153.90, t=9.40 t=153.90, t=16.20 t=153.90, t=22.10
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
t=166.60, t=9.40 t=166.60, t=16.20 t=166.60, t=22.10
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
t=191.60, t=9.40 t=191.60, t=16.20 t=191.60, t=22.10
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure20: BrierScore(lowerisbetter)forpredictivemaintenancefornumerouspoints(t,δt).
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
t=153.90, t=9.40 t=153.90, t=16.20 t=153.90, t=22.10
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
t=166.60, t=9.40 t=166.60, t=16.20 t=166.60, t=22.10
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
t=191.60, t=9.40 t=191.60, t=16.20 t=191.60, t=22.10
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure21: WeightedBrierScore(lowerisbetter)forpredictivemaintenancefornumerouspoints(t,δt).
38
erocS
reirB
erocS
reirB
erocS
reirB
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
erocS
reirB
erocS
reirB
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
erocS
reirB
erocS
reirB
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
dethgieWDynamicSurvivalAnalysiswithControlledLatentStates
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=153.90, t=9.40 t=153.90, t=16.20 t=153.90, t=22.10
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=166.60, t=9.40 t=166.60, t=16.20 t=166.60, t=22.10
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=191.60, t=9.40 t=191.60, t=16.20 t=191.60, t=22.10
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure22: AUC(higherisbetter)forpredictivemaintenancefornumerouspoints(t,δt).
client_id = 0 client_id = 4 7 client_id = 0 client_id = 4
20 client_id = 1 client_id = 6 client_id = 1 client_id = 6
6
15 5
4
10
3
2
5
1
0 0
0 20 40 60 80 0 20 40 60 80
Time Time
12.5 1600
client_id = 0 client_id = 4 client_id = 0 client_id = 4
client_id = 1 client_id = 6 1400 client_id = 1 client_id = 6
10.0
1200
7.5 1000
800
5.0
600
400
2.5
200
0.0 0
0 20 40 60 80 0 20 40 60 80
Time Time
Figure23:Valuesof4differenttime-dependantfeaturesfor4randomlychosenindividualsfromthechurnpredictiondataset.Individ-
ualtime-to-eventanddistributionoftheeventtimescannotbedisplayedtoprotectconsumerandbusinessprivacy.Aprecisedescription
ofthedifferenttime-dependantfeatureswillbeprovideduponpublication.
39
CUA
CUA
CUA
ecirp
egareva
dethgieW
redro
rep
smeti
fo
rebmuN
CUA
CUA
CUA
eulav
redro
egarevA
keew
rep
redro
fo
rebmuN
CUA
CUA
CUADynamicSurvivalAnalysiswithControlledLatentStates
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=2.00, t=1.00 t=2.00, t=2.00 t=2.00, t=4.00
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=3.00, t=1.00 t=3.00, t=2.00 t=3.00, t=4.00
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=4.00, t=1.00 t=4.00, t=2.00 t=4.00, t=4.00
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure24: C-Index(higherisbetter)forchurnpredictionfornumerouspoints(t,δt).
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
t=2.00, t=1.00 t=2.00, t=2.00 t=2.00, t=4.00
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
t=3.00, t=1.00 t=3.00, t=2.00 t=3.00, t=4.00
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
t=4.00, t=1.00 t=4.00, t=2.00 t=4.00, t=4.00
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure25: Brierscore(lowerisbetter)forchurnpredictionfornumerouspoints(t,δt).
40
xednI-C
xednI-C
xednI-C
erocS
reirB
erocS
reirB
erocS
reirB
xednI-C
xednI-C
xednI-C
erocS
reirB
erocS
reirB
erocS
reirB
xednI-C
xednI-C
xednI-C
erocS
reirB
erocS
reirB
erocS
reirBDynamicSurvivalAnalysiswithControlledLatentStates
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
t=2.00, t=1.00 t=2.00, t=2.00 t=2.00, t=4.00
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
t=3.00, t=1.00 t=3.00, t=2.00 t=3.00, t=4.00
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
0.5 0.5 0.5
0.4 0.4 0.4
0.3 0.3 0.3
0.2 0.2 0.2
0.1 0.1 0.1
t=4.00, t=1.00 t=4.00, t=2.00 t=4.00, t=4.00
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure26: WeightedBrierscore(lowerisbetter)forchurnpredictionfornumerouspoints(t,δt).
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=2.00, t=1.00 t=2.00, t=2.00 t=2.00, t=4.00
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=3.00, t=1.00 t=3.00, t=2.00 t=3.00, t=4.00
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
t=4.00, t=1.00 t=4.00, t=2.00 t=4.00, t=4.00
0.0 0.0 0.0
CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH CoxSig CoxSig+ NCDE Cox RSF DDH
Figure27: AUC(higherisbetter)forchurnpredictionfornumerouspoints(t,δt).
41
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
dethgieW
CUA
CUA
CUA
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
dethgieW
CUA
CUA
CUA
erocS
reirB
dethgieW
erocS
reirB
dethgieW
erocS
reirB
dethgieW
CUA
CUA
CUA