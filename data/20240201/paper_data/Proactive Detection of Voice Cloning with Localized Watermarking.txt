Proactive Detection of Voice Cloning with Localized Watermarking
RobinSanRoman*12 PierreFernandez*12 HadyElsahar*1
AlexandreDe´fossez3 TeddyFuron2 TuanTran1
Abstract Proactively watermarked speech generator
Speech
In the rapidly evolving field of speech gener- Speech Watermark editing
Model Generator
ative models, there is a pressing need to en-
AI-Generated Watermarked
sureaudioauthenticityagainsttherisksofvoice
cloning. We present AudioSeal, the first audio ‘AI generated?’
watermarkingtechniquedesignedspecificallyfor ✔ Published
✗ Watermark
localizeddetectionofAI-generatedspeech. Au-
Detector
dioSeal employs a generator / detector architec-
ture trained jointly with a localization loss to
Figure1. ProactivedetectionofAI-generatedspeech. Weem-
enable localized watermark detection up to the
bedanimperceptiblewatermarkintheaudio,whichcanbeused
sample level, and a novel perceptual loss in- todetectifaspeechisAI-generatedandidentifythemodelthat
spired by auditory masking, that enables Au- generatedit.ItcanalsopreciselypinpointAI-generatedsegments
dioSeal to achieve better imperceptibility. Au- inalongeraudiowithasamplelevelresolution(1/16kseconds).
dioSeal achieves state-of-the-art performance in
terms of robustness to real life audio manipu- traceability, including forensics and watermarking – see
lations and imperceptibility based on automatic Chi(2023);Eur(2023);TheWhiteHouse(2023).
andhumanevaluationmetrics. Additionally,Au-
Themainforensicsapproachtodetectsynthesizedaudiois
dioSealisdesignedwithafast,single-passdetec-
to train binary classifiers to discriminate between natural
tor, that significantly surpasses existing models
and synthesized audios, a technique highlighted in stud-
in speed—achieving detection up to two orders
ies by Borsos et al. (2022); Kharitonov et al. (2023); Le
of magnitude faster, making it ideal for large-
et al. (2023). We refer to this technique as passive detec-
scaleandreal-timeapplications.
tionsinceitdoesnotalteroftheaudiosource. Albeitbeing
a straightforward mitigation, it is prone to fail as gener-
1.Introduction ative models advance and the difference between synthe-
sizedandauthenticcontentdiminishes.
Generative speech models are now capable of synthesiz-
Watermarking emerges as a strong alternative. It embeds
ing voices that are indistinguishable from real ones (Arik
a signal in the generated audio, imperceptible to the ear
etal.,2018;Kimetal.,2021;Casanovaetal.,2022;Wang
but detectable by specific algorithms. There are two wa-
etal.,2023). Thoughspeechgenerationandvoicecloning
termarking types: multi-bit and zero-bit. Zero-bit water-
arenotnovelconcepts,theirrecentadvancementsinqual-
markingdetectsthepresenceorabsenceofawatermarking
ity and accessibility have raised new security concerns.
signal, which is valuable for AI content detection. Multi-
These technologies now pose greater risks to individual
bit watermarking embeds a binary message in the con-
and corporate safety by enabling sophisticated scams and
tent, allowing to link content to a specific user or genera-
spreadingmisinformationmoreeffectively. Anotableinci-
tivemodel. Mostdeep-learningbasedaudiowatermarking
dent occurred where a deepfake audio misleadingly urged
methods(Pavlovic´etal.,2022;Liuetal.,2023;Chenetal.,
US voters to abstain, showcasing the potential for misus-
2023) are multi-bit and train a generator taking a sample
ingthesetechnologiestospreadfalseinformation(Murphy
and a message to output the watermarked sample, and an
etal.,2024).Consequently,regulatorsandgovernmentsare
extractorretrievingthehiddenmessagefromthatsample.
implementing measures for AI content transparency and
Currentwatermarkingmethodshavelimitations.First,they
*Equal contribution 1FAIR, Meta 2Inria 3Kyutai (work done
are not adapted for detection. The initial applications as-
whileatFAIR).
Correspondenceto: <robinsr,hadyelsahar,pfz@meta.com>. sumed any sound sample under scrutiny is watermarked
Code:github.com/facebookresearch/audioseal (e.g. IP protection). As a result, the decoders were never
4202
naJ
03
]DS.sc[
1v46271.1042:viXraProactiveDetectionofVoiceCloningwithLocalizedWatermarking
trainedonnon-watermarkedsamples.Thisdiscrepancybe- Ouroverallcontributionsare:
tween the training of the models and their practical use
• We introduce AudioSeal, the first audio watermark-
leads to poor or overestimated detection rates, depending
ing technique designed for localized detection of AI-
ontheembeddedmessage(seeApp.B).Ourmethodaligns
generatedspeechuptothesample-level;
morecloselywiththeconcurrentworkbyJuvela&Wang
(2023),whichtrainsadetector,ratherthanadecoder. • A novel perceptual loss inspired by auditory masking,
thatenablesAudioSealtoachievebetterimperceptibility
Second, they are not localized and consider the entire au-
ofthewatermarksignal;
dio, making it difficult to identify small segments of AI-
generatedspeechwithinlongeraudioclips.Theconcurrent • AudioSeal achieves the state-of-the-art robustness to a
WavMark’sapproach(Chenetal.,2023)addressesthisby widerangeofreallifeaudiomanipulations(section5);
repeating at 1-second intervals a synchronization pattern
• AudioSealsignificantlyoutperformsthestateofartaudio
followed by the actual binary payload. This has several
watermarkingmodelincomputationspeed,achievingup
drawbacks. It cannot be used on spans less than 1 second
totwoordersofmagnitudefasterdetection(section5.5);
and is susceptible to temporal edits. The synchronization
bitsalsoreducethecapacityfortheencodedmessage, ac- • Insightsonthesecurityandintegrityofaudiowatermark-
counting for 31% of the total capacity. Most importantly, ingtechniqueswhenopensourcing(section6).
the brute force detection algorithm for decoding the syn-
chronization bits is prohibitively slow especially on non- 2.RelatedWork
watermarkedcontent,asweshowinSec.5.5.Thismakesit
unsuitable for real-time and large-scale traceability of AI- In this section we give an overview of the detection and
generated content on social media platforms, where most watermarking methods for audio data. A complementary
contentisnotwatermarked. descritionofpriorworkscanbefoundintheAppendixA.
To address these limitations, we introduce AudioSeal, a Synthetic speech detection. Detection of synthetic
methodforlocalizedspeechwatermarking. Itjointlytrains speech is traditionally done in the forensics community
twonetworks: agenerator thatpredictsanadditivewater- by building features and exploiting statistical differences
mark waveform from an audio input, and a detector that between fake and real. These features can be hand-
outputs the probability of the presence of a watermark at crafted (Sahidullah et al., 2015; Janicki, 2015; AlBadawy
each sample of the input audio. The detector is trained to et al., 2019; Borrelli et al., 2021) and/or learned (Mu¨ller
preciselyandrobustlydetectsynthesizedspeechembedded etal.,2022;Barringtonetal.,2023). Theapproachofmost
inlongeraudioclipsbymaskingthewatermarkinrandom audio generation papers (Borsos et al., 2022; Kharitonov
sections of the signal. The training objective is to maxi- etal.,2023;Borsosetal.,2023;Leetal.,2023)istotrain
mizethedetector’saccuracywhileminimizingthepercep- end-to-end deep-learning classifiers on what their models
tual difference between the original and watermarked au- generate,similarlyasZhangetal.(2017). Accuracywhen
dio. WealsoextendAudioSealtomulti-bitwatermarking, comparing synthetic to real is usually good, although not
so that an audio can be attributed to a specific model or performingwellonoutofdistributionaudios(compressed,
versionwithoutaffectingthedetectionsignal. noised,slowed,etc.).
We evaluate the performance of AudioSeal to detect and
Imperceptible watermarking. Unlike forensics, water-
localize AI-generated speech. AudioSeal achieves state-
markingactivelymarksthecontenttoidentifyitonceinthe
of-the-art results on robustness of the detection, far sur-
wild.Itisenjoyingrenewedinterestinthecontextofgener-
passingpassivedetectionwithnearperfectdetectionrates
ativemodels,asitprovidesameanstotrackAI-generated
overawiderangeofaudioedits. Italsoperformssample-
content,beitfortext(Kirchenbaueretal.,2023;Aaronson
leveldetection(atresolutionof1/16ksecond),outperform-
& Kirchner, 2023; Fernandez et al., 2023a), images (Yu
ing WavMark in both speed and performance. In terms
etal.,2021b;Fernandezetal.,2023b;Wenetal.,2023),or
ofefficiency,ourdetectorisrunonceandyieldsdetection
audio/speech(Chenetal.,2023;Juvela&Wang,2023).
logits at every time-step, allowing for real-time detection
of watermarks in audio streams. This represents a major Traditionalmethodsforaudiowatermarkingreliedonem-
improvement compared to earlier watermarking methods, bedding watermarks either in the time or frequency do-
whichrequiressynchronizingthewatermarkwithinthede- mains (Lie & Chang, 2006; Kalantari et al., 2009; Natgu-
tector, thereby substantially increasing computation time. nanathan et al., 2012; Xiang et al., 2018; Su et al., 2018;
Finally, in conjunction with binary messages, AudioSeal Liuetal.,2019),usuallyincludingdomainspecificfeatures
almost perfectly attributes an audio to one model among to design the watermark and its corresponding decoding
1,000,eveninthepresenceofaudioedits. function. Deep-learning audio watermarking methods fo-
cus on multi-bit watermarking and follow a generator/de-ProactiveDetectionofVoiceCloningwithLocalizedWatermarking
coder framework (Tai & Mansour, 2019; Qu et al., 2023; s ∈ RT and outputs a watermark waveform δ ∈ RT
Pavlovic´ et al., 2022; Liu et al., 2023; Ren et al., 2023). ofthesamedimensionality,whereT isthenumberof
Fewworkshaveexploredzero-bitwatermarking(Wuetal., samplesinthesignal. Thewatermarkedaudioisthen
2023; Juvela & Wang, 2023), which is better adapted for s =s+δ.
w
detectionofAI-generatedcontent. Ourrationaleisthatro-
bustnessincreasesasthemessagepayloadisreducedtothe (ii) To enable sample-level localization, we adopt an
bareminimum(Furon,2007). augmentation strategy focused on watermark mask-
ing with silences and other original audios. This is
In this work we compare with the current state-of-the-art
achieved by randomly selecting k starting points and
onaudiowatermarkingWavMark(Chenetal.,2023),that
altering the next T/2k samples from s in one of
exhibits superior performance over previous works. It is w
4 ways: revert to the original audio (i.e. s (t) =
based on invertible networks that hide 32 bits across 1- w
s(t)) with probability 0.4; replacing with zeros (i.e.
secondspansoftheinputaudio. Thedetectionisdoneby
s (t) = 0) with probability 0.2; or substituting with
sliding along the audio with a time step of 0.05s, and de- w
a different audio signal from the same batch (i.e.
codingthemessageforeachslidingwindow. Ifthe10first
s (t) = s′(t))withprobability0.2,ornotmodifying
decoded bits match a synchronization pattern the rest of w
thesampleatallwithprobability0.2.
thepayloadissaved(22bits),andthewindowcandirectly
slide 1s (instead of the 0.05). This brute force detection
(iii) Thesecondclassofaugmentationensurestherobust-
algorithmisprohibitivelyslowespeciallywhenthewater-
ness against audio editing. One of the following sig-
markisabsent,sincethealgorithmwillhavetoattemptand
nalalterationsisapplied: bandpassfilter,boostaudio,
failtodecodeawatermarkforeachslidingwindowinthe
duck audio, echo, highpass filter, lowpass filter, pink
inputaudio(duetotheabsenceofwatermark).
noise, gaussian noise, slower, smooth, resample (full
details in App. C.2). The parameters of those aug-
3.Method
mentations are fixed to aggressive values to enforce
maximalrobustnessandtheprobabilityofsamplinga
The method jointly trains two models. The generator cre-
given augmentation is proportional to the inverse of
ates a watermark signal that is added to the input audio.
its evaluation detection accuracy. We implemented
The detector outputs local detection logits. The training
theseaugmentationsinadifferentiablewaywhenpos-
optimizes two concurrent classes of objectives: minimiz-
sible,andotherwise(e.g. MP3compression)withthe
ing the perceptual distortion between original and water-
straight-through estimator (Yin et al., 2019) that al-
marked audios and maximizing the watermark detection.
lowsthegradientstoback-propagatetothegenerator.
To improve robustness to modifications of the signal and
localization,weincludeacollectionoftraintimeaugmen-
(iv) Detector D processes the original and the water-
tations. Atinferencetime,thelogitspreciselylocalizewa-
markedsignals, outputtingforeachasoftdecisionat
termarkedsegmentsallowingfordetectionofAI-generated
every time step, meaning D(s) ∈ [0,1]T. Figure 3
content. Optionally, shortbinaryidentifiersmaybeadded
illustrates that the detector’s outputs are at one only
ontopofthedetectiontoattributeawatermarkedaudioto
whenthewatermarkispresent.
aversionofthemodelwhilekeepingasingledetector.
3.1.Trainingpipeline
0.2
Figure 2 illustrates the joint training of the generator and 0.1
thedetectorwithfourcriticalstages:
0.0
(i) The watermark generator takes as input a waveform 0.1
0.2
1.0
0.5
Perceptual Localization WM Labels ↔ Predictions
Losses Loss 0.0
4 5 6 7 8
Time (s)
Watermark Watermark
Generator Detector Figure3. (Top) A speech signal (gray) where the watermark is
Original Watermarked Augmented presentbetween5and7.5seconds(orange,magnifiedby5).(Bot-
& Masked
tom)Theoutputofthedetectorforeverytimestep. Anorange
Figure2. Generator-detectortrainingpipeline. backgroundcolorindicatesthepresenceofthewatermark.
langiS
rotceteD
edutilpmA
ytilibaborPProactiveDetectionofVoiceCloningwithLocalizedWatermarking
Watermark Generator Watermark Detector
✗✔
bit bit
1 b
010111011011
Message embeddings
…
δ
w
Encoder optional Decoder Encoder optional
Figure4. Architectures. The generator is made of an encoder and a decoder both derived from EnCodec’s design, with optional
messageembeddings.TheencoderincludesconvolutionalblocksandanLSTM,whilethedecodermirrorsthisstructurewithtransposed
convolutions. Thedetectorismadeofanencoderandatransposeconvolution,followedbyalinearlayerthatcalculatessample-wise
logits.Optionally,multiplelinearlayerscanbeusedforcalculatingk-bitmessages.MoredetailsinApp.C.3.
The architectures of the models are based on En- lw =Loudness(δw)−Loudness(sw). (1)
b b b
Codec(De´fossezetal.,2022). TheyarepresentedinFig-
This measure quantifies the discrepancy in loudness be-
ure4anddetailedintheappendixC.3.
tween the watermark and the original signal within a spe-
cific time window w, and a particular frequency band b.
3.2.Losses
Thefinallossisaweightedsumoftheloudnessdifferences
Our setup includes multiple perceptual losses, we balance usingsoftmaxfunction:
them during training time by scaling their gradients as (cid:88)
L = (softmax(l)w∗lw). (2)
inDe´fossezetal.(2022). Thecompletelistofusedlosses loud b b
isdetailedbellow. b,w
Thesoftmaxpreventsthemodelfromtargetingexcessively
Perceptual losses enforce the watermark imperceptibil- lowloudnesswherethewatermarkisalreadyinaudible.
ity to the human ear. These include an ℓ loss on the wa-
1
termarksignaltodecreaseitsintensity,themulti-scaleMel Maskedsample-leveldetectionloss. Alocalizationloss
spectrogram loss of (Gritsenko et al., 2020), and discrim- ensuresthatthedetectionofwatermarkedaudioisdoneat
inative losses based on adversarial networks that operate the level of individual samples. For each time step t, we
onmulti-scaleshort-term-Fourier-transformspectrograms. compute the binary cross entropy (BCE) between the de-
De´fossez et al. (2022) use this combination of losses for tector’soutputD(s) tandthegroundtruthlabel(0fornon-
trainingtheEnCodecmodelforaudiocompression. watermarked,1forwatermarked). Overall,thisreads:
T
Inaddition,weintroduceanoveltime-frequencyloudness 1 (cid:88)
L = BCE(D(s′) ,y ), (3)
loss TF-Loudness, which operates entirely in the wave- loc T t t
t=1
form domain. This approach is based on ”audotiry mask- where s′ might be s or s , and where time step labels y
w t
ing”, a psycho-acoustic property of the human auditory
aresetto1iftheyarewatermarked,and0otherwise.
system already exploited in the early days of watermark-
ing (Kirovski & Attias, 2003): the human auditory sys-
3.3.Multi-bitwatermarking
temfailsperceivingsoundsoccurringatthesametimeand
at the same frequency range (Schnupp et al., 2011). TF- We extend the method to support multi-bit watermarking,
Loudness is calculated as follows: first, the input signal which allows for attribution of audio to a specific model
s is divided into B signals based on non overlapping fre- version. Atgeneration,weaddamessageprocessinglayer
quency bands s ,...,s . Subsequently, every signal inthemiddleofthegenerator.Ittakestheactivationmapin
0 B−1
is segmented using a window of size W, with an overlap Rh,t′ andabinarymessagem∈{0,1}bandoutputsanew
amount denoted by r. This procedure is applied to both activationmaptobeaddedtotheoriginalone. Weembed
theoriginalaudiosignalsandtheembeddedwatermarkδ. m into e = (cid:80) E ∈Rh, where E ∈ R2k,h
i=0..b−1 2i+mi
As a result, we obtain segments of the signal and water- is a learnable embedding layer. e is then repeated t times
markintime-frequencydimensions,denotedassw andδw along the temporal axis to match the activation map size
b b
respectively. For every time-frequency window we com- (t,h). At detection, we add b linear layers at the very end
pute the loudness difference, where loudness is estimated ofthedetector. Eachofthemoutputsasoftvalueforeach
using ITU-R BS.1770-4 recommendations (telecommuni- bit of the message at the sample-level. Therefore, the de-
cationUnion,2011)(seeApp.C.1fordetails): tectoroutputsatensorofshapeRt,1+b(1forthedetection,
Conv
1D
ResNet
Conv
block
…
ResNet
Conv
block
LSTM
LSTM
LSTM
LSTM
Conv
1D
+
Conv
1D
LSTM
LSTM
LSTM
LSTM
ResNet
Conv
block
…
ResNet
Conv
block
Conv
1D
Conv
1D
ResNet
Conv
block
…
ResNet
Conv
block
LSTM
LSTM
LSTM
LSTM
Conv
1D
Conv
Transpose
1DProactiveDetectionofVoiceCloningwithLocalizedWatermarking
b for the message). At training, we add a decoding loss
Table1. Audio quality metrics. Compared to traditional wa-
L tothelocalizationlossL . ThislossL averages
dec loc dec termarkingmethodsthatminimizetheSNRlikeWavMark, Au-
the BCE between the original message and the detector’s
dioSealachievessameorbetterperceptualquality.
outputsoverallpartswherethewatermarkispresent.
Methods SI-SNR PESQ STOI ViSQOL MUSHRA
3.4.Trainingdetails WavMark 38.25 4.302 0.997 4.730 71.52±7.18
AudioSeal 26.00 4.470 0.997 4.829 77.07±6.35
Our watermark generator and detector are trained on a
4.5KhourssubsetfromtheVoxPopuli(Wangetal.,2021) than watermarking methods like WavMark (Chen et al.,
dataset. It’s important to emphasize that the sole purpose 2023) that try to minimize the SI-SNR. In practice, high
of our generator is to generate imperceptible watermarks SI-SNRisindeednotnecessarilycorrelatedwithgoodper-
given an input audio; without the capability to produce or ceptualquality. AudioSealisnotoptimizedforSI-SNRbut
modifyspeechcontent. Weuseasamplingrateof16kHz ratherforperceptualqualityofspeech. Thisisbettercap-
andone-secondsamples,soT = 16000inourtraining. A turedbytheothermetrics(PESQ,STOI,ViSQOL),where
fulltrainingrequires600ksteps,withtheAdam,alearning AudioSeal consistently achieves better performance. Put
rateof1e-4,andabatchsizeof32. Forthedropaugmen- differently, our goal is to hide as much watermark power
tation,weusek =5windowsof0.1sec. hissetto32,and aspossiblewhilekeepingitperceptuallyindistinguishable
thenumberofadditionalbitsbto16(notethathneedsto fromtheoriginal. Figure3alsovisualizeshowthewater-
behigherthanb,forexampleh = 8isenoughinthezero- marksignalfollowstheshapeofthespeechwaveform.
bitcase). Theperceptuallossesarebalancedandweighted
ThemetricusedforoursubjectiveevaluationsisMUSHRA
as follows: λ = 0.1, λ = 2.0, λ = 4.0,
ℓ1 msspec adv test(Series,2014).Thecompletedetailsaboutourfullpro-
λ = 10.0. The localization and watermarking losses
loud tocolcanbefoundintheAppendixC.4. Inthisstudyour
areweightedbyλ =10.0andλ =1.0respectively.
loc dec samplesgotratingsveryclosetothegroundtruthsamples
thatobtainedanaveragescoreof80.49.
3.5.Detection,localizationandattribution
Atinference,wemayusethegeneratoranddetectorfor: 5.ExperimentsandEvaluation
• Detection: To determine if the audio is watermarked or
Thissectionevaluatesthedetectionperformanceofpassive
not. Toachievethis,weusetheaveragedetector’soutput
classifiers, watermarking methods, and AudioSeal. using
over the entire audio and flag it if the score exceeds a
TruePositiveRate(TPR)andFalsePositiveRate(FPR)as
threshold(default: 0.5).
key metrics for watermark detection. TPR measures cor-
• Localization: Topreciselyidentifywherethewatermark rectidentificationofwatermarkedsamples, whileFPRin-
is present. We utilize the sample-wise detector’s output dicates the rate of genuine audio clips falsely flagged. In
and mark a time step as watermarked if the score sur- practicalscenarios,minimizingFPRiscrucial. Forexam-
passesathreshold(default: 0.5). ple, on a platform processing 1 billion samples daily, an
FPR of 10−3 and a TPR of 0.5 means 1 million samples
• Attribution: Toidentifythemodelversionthatproduced
requiremanualrevieweachday,yetonlyhalfofthewater-
theaudio,enablingdifferentiationbetweenusersorAPIs
markedsamplesaredetected.
with a single detector. The detector’s first output gives
thedetectionscoreandtheremainingkoutputsareused
5.1.Comparisonwithpassiveclassifier
for attribution. This is done by computing the average
messageoverdetectedsamplesandreturningtheidenti- We first compare detection results on samples generated
fierwiththesmallestHammingdistance. with Voicebox (Le et al., 2023). We compare to the pas-
sive setup where a classifier is trained to discriminate be-
4.Audio/SpeechQuality tween Voicebox-generated and real audios. Following the
approachintheVoiceboxstudy,weevaluate2,000approx-
We first evaluate the quality of the watermarked audio imately 5-second samples from LibriSpeech, These sam-
using: Scale Invariant Signal to Noise Ratio (SI-SNR): ples have masked frames (90%, 50%, and 30% of the
SI-SNR(s,s ) = 10log (cid:0) ∥αs∥2/∥αs−s ∥2(cid:1) , where phonemes) pre-Voicebox generation. We evaluate on the
w 10 2 w 2
α = ⟨s,s ⟩/∥s∥2; as well as PESQ (Rix et al., 2001), sametasks,i.e. distinguishingbetweenoriginalandgener-
w 2
ViSQOL (Hines et al., 2012) and STOI (Taal et al., 2010) ated,orbetweenoriginalandre-synthesized(createdbyex-
whichareobjectiveperceptualmetricsmeasuringthequal- tractingtheMelspectrogramfromoriginalaudioandthen
ityofspeechsignals. vocodingitwiththeHiFi-GANvocoder).
Table1reportthesemetrics.AudioSealbehavesdifferently Both active and passive setups achieve perfect classifi-ProactiveDetectionofVoiceCloningwithLocalizedWatermarking
(faster, resample), filtering (bandpass, highpass, lowpass),
Table2. ComparisonwithVoiceboxbinaryclassifier.Percent-
audio effects (echo, boost audio, duck audio), noise (pink
agereferstothefractionofmaskedinputframes.
noise, random noise), and compression (MP3, AAC, En-
AudioSeal(Ours) VoiceboxClassif. Codec). These attacks cover a wide range of transforma-
%Mask Acc. TPR FPR Acc. TPR FPR tionsthatarecommonlyusedinaudioeditingsoftware.For
alleditsexceptEncodeccompression, evaluationwithpa-
OriginalaudiovsAI-generatedaudio
rametersinthetrainingrangewouldbeperfect. Inorderto
30% 1.0 1.0 0.0 1.0 1.0 0.0
showgeneralization,wechosestrongerparametertotheat-
50% 1.0 1.0 0.0 1.0 1.0 0.0
90% 1.0 1.0 0.0 1.0 1.0 0.0 tacksthanthoseusedduringtraining(detailsinApp.C.2).
Re-synthesizedaudiovsAI-generatedaudio Detection is done on 10k ten-seconds audios from our
30% 1.0 1.0 0.0 0.704 0.680 0.194 VoxPopuli validation set. For each edit, we first build a
50% 1.0 1.0 0.0 0.809 0.831 0.170 balanced dataset made of the 10k watermarked/ 10k non-
90% 1.0 1.0 0.0 0.907 0.942 0.112
watermarked edited audio clips. We quantify the perfor-
mancebyadjustingthethresholdofthedetectionscore,se-
Table3. Detection resuts for different edits applied before de-
lectingthevaluethatmaximizesaccuracy(weprovidecor-
tection. Acc. (TPR/FPR)istheaccuracy(andTPR/FPR)obtained
respondingTPRandFPRatthisthreshold).TheROCAUC
forthethresholdthatgivesbestaccuracyonabalancedsetofaug-
(AreaUndertheCurveoftheReceiverOperatingCharac-
mentedsamples.AUCistheareaundertheROCcurve.
teristics) gives a global measure of performance over all
AudioSeal(Ours) WavMark threshold levels, and captures the TPR/FPR trade-off. To
Edit Acc.TPR/FPR AUC Acc.TPR/FPR AUC adapt data-hiding methods (e.g. WavMark) for proactive
detection, we embed a binary message (chosen randomly
None 1.001.00/0.00 1.00 1.001.00/0.00 1.00
beforehand) in the generated speech before release. The
Bandpass 1.001.00/0.00 1.00 1.001.00/0.00 1.00
detectionscoreisthencomputedastheHammingdistance
Highpass 0.610.82/0.60 0.61 1.001.00/0.00 1.00
between the original message and the one extracted from
Lowpass 0.990.99/0.00 0.99 0.501.00/1.00 0.50
Boost 1.001.00/0.00 1.00 1.001.00/0.00 1.00 thescrutinizedaudio.
Duck 1.001.00/0.00 1.00 1.001.00/0.00 1.00
WeobserveinTab.3thatAudioSealisoverallmorerobust,
Echo 1.001.00/0.00 1.00 0.930.89/0.03 0.98
withanaverageAUCof0.97vs. 0.84forWavMark. The
Pink 1.001.00/0.00 1.00 0.880.81/0.05 0.93
performanceforlowpassandhighpassfiltersindicatesthat
White 0.910.86/0.04 0.95 0.500.54/0.54 0.50
AudioSealembedswatermarksneitherinthelownorinthe
Fast(1.25x) 0.990.99/0.00 1.00 0.500.01/0.00 0.15
highfrequencies(WavMarkfocusesonhighfrequencies).
Smooth 0.990.99/0.00 1.00 0.940.93/0.04 0.98
Resample 1.001.00/0.00 1.00 1.001.00/0.00 1.00
Generalization. We evaluate how AudioSeal general-
AAC 1.001.00/0.00 1.00 1.001.00/0.00 1.00
izes on various domains and languages. Specifically, we
MP3 1.001.00/0.00 1.00 1.000.99/0.00 0.99
translate speech samples from a subset of the Expresso
EnCodec 0.980.98/0.01 1.00 0.510.52/0.50 0.50
dataset (Nguyen et al., 2023) (studio-quality recordings),
Average 0.960.98/0.04 0.97 0.850.85/0.14 0.84
with the SeamlessExpressive translation model (Seamless
Communication et al., 2023). We select four target lan-
cation in the case when trained to distinguish between guages: Mandarin Chinese (CMN), French (FR), Italian
natural and Voicebox. Conversely, the second part of (IT), and Spanish (SP). We also evaluate on non-speech
Tab. 2 highlights a significant drop in performance when AI-generatedaudios: musicfromMusicGen(Copetetal.,
the classifier is trained to differentiate between Voicebox 2023) and environmental sounds from AudioGen (Kreuk
and re-synthesized. It suggests that the classifier is de- et al., 2023). We use medium-sized pretrained models
tecting vocoder artifacts, since the re-synthesized samples available in AudioCraft (Copet et al., 2023), with default
aresometimeswronglyflagged. Theclassificationperfor- samplingparameters.ForMusicGen,weuseunconditional
mancequicklydecreasesasthequalityoftheAI-generated generation,whileforAudioGen,weusepromptsfromthe
sample increases (when the input is less masked). On the testsetofAudioCaps(Kimetal.,2019). Resultsarevery
otherhand,ourproactivedetectiondoesnotrelyonmodel- similartoourin-domaintestsetandcanbefoundinApp.5.
specific artifacts but on the watermark presence. This al-
lowsforperfectdetectionoveralltheaudioclips. 5.3.Localization
We evaluate localization with the sample-level detection
5.2.Comparisonwithwatermarking
accuracy, i.e. the proportion of correctly labeled samples,
We evaluate the robustness of the detection a wide range andtheIntersectionoverUnion(IoU).Thelatterisdefined
ofrobustnessandaudioeditingattacks: timemodification as the intersection between the predicted and the groundProactiveDetectionofVoiceCloningwithLocalizedWatermarking
truthdetectionmasks(1whenwatermarked,0otherwise),
Table4. Attributionresults.Wereporttheaccuracyoftheattri-
dividedbytheirunion.IoUisamorerelevantevaluationof
bution(Acc.) andfalseattributionrate(FAR).Detectionisdone
thelocalizationofshortwatermarksinalongeraudio.
atFPR=10−3andattributionmatchesthedecodedmessagetoone
Thisevaluationiscarriedoutonthesameaudioclipsasfor ofNversions.WereportaveragedresultsovertheeditsofTab.3.
detection. Foreachoneofthem,wewatermarkarandomly N 1 10 102 103 104
placedsegmentofvaryinglength. LocalizationwithWav-
WavMark 0.0 0.20 0.98 1.87 4.02
Markisabrute-forcedetection: awindowof1sslidesover FAR(%)↓
AudioSeal 0.0 2.52 6.83 8.96 11.84
the10sofspeechwiththedefaultshiftvalueof0.05s. The
WavMark 58.4 58.2 57.4 56.6 54.4
Hammningdistancebetweenthe16patternbitsisusedas Acc.(%)↑
AudioSeal 68.2 65.4 61.4 59.3 56.4
thedetectionscore.Wheneverawindowtriggersapositive,
we label its 16k samples as watermarked in the detection
maskin{0,1}t. put of the detector exceeds a threshold, corresponding to
FPR=10−3. Next,wecalculatetheHammingdistancebe-
Figure 5 plots the sample-level accuracy and IoU for dif- tweenthedecodedwatermarkandallN originalmessages.
ferentproportionsofwatermarkedspeechintheaudioclip. The message with the smallest Hamming distance is se-
AudioSealachievesanIoUof0.99whenjustonesecondof lected. It’s worth noting that we can simulate N > N′
speech is AI-manipulated, compared to WavMark’s 0.35. modelsbyaddingextramessages. Thismayrepresentver-
Moreover,AudioSealallowsforprecisedetectionofminor sionsthathavenotgeneratedanysample.
audioalterations: itcanpinpointAI-generatedsegmentsin
FalseAttributionRate(FAR)isthefractionofwrongattri-
audio down to the sample level (usually 1/16k sec), while
butionamongthedetectedaudioswhiletheattributionac-
theconcurrentWavMarkonlyprovidesone-secondresolu-
curacyistheproportionofdetectionsfollowedbyacorrect
tionandthereforelagsbehindintermsofIoU.Thisises-
attributions over all audios. AudioSeal has a higher FAR
peciallyrelevantforspeechsamples,whereasimpleword
butoverallgivesabetteraccuracy,whichiswhatultimately
modificationmaygreatlychangemeaning.
matters. Insummary,decouplingdetectionandattribution
achieves better detection rate and makes the global accu-
5.4.Attribution
racybetter,atthecostofoccasionalfalseattributions.
Given an audio clip, the objective is now to find if any of
N versions of our model generated it (detection), and if 5.5.EfficiencyAnalysis
so, which one (identification). For evaluation, we create
N′ =100random16-bitsmessagesandusethemtowater- To highlight the efficiency of AudioSeal, we conduct a
performance analysis and compare it with WavMark. We
mark1kaudioclips,eachconsistingof5secondsofspeech
apply the watermark generator and detector of both mod-
(not 10s to reduce compute needs). This results in a total
els on a dataset of 500 audio segments ranging in length
of 100k audios. For WavMark, the first 16 bits (/32) are
from1to10seconds,usingasingleNvidiaQuadroGP100
fixedandthedetectionscoreisthenumberofwelldecoded
GPU. The results are displayed in Fig. 6 and Tab. 6. In
patternbits,whilethesecondhalfofthepayloadhidesthe
terms of generation, AudioSeal is 14x faster than Wav-
modelversion. Anaudioclipisflaggediftheaverageout-
Mark. For detection, AudioSeal outperforms WavMark
with two orders of magnitude faster performance on av-
1.0 erage, notably 485x faster in scenarios where there is no
watermark(Tab.6). Thisremarkablespeedincreaseisdue
0.8
to our model’s unique localized watermark design, which
0.6
bypasses the need for watermark synchronization (recall
0.4 thatWavMarkrelieson200passforwardsforaone-second
snippet).AudioSeal’sdetectorprovidesdetectionlogitsfor
1.0
eachinputsampledirectlywithonlyonepasstothedetec-
0.8 tor, significantly enhancing the detection’s computational
0.6 efficiency. Thismakesoursystemhighlysuitableforreal-
Ours
timeandlarge-scaleapplications.
0.4 Wavmark
2 4 6 8
6.AdversarialWatermarkRemoval
x seconds of watermarked signal in a 10 seconds audio
Figure5.Localization results across different durations of wa- Wenowexaminemoredamagingdeliberateattacks,where
termarkedaudiosignalsintermsofSample-LevelAccuracyand attackers might either “forge” the watermark by adding
IntersectionOverUnion(IoU)metrics(↑isbetter). it to authentic samples (to overwhelm detection systems)
.ccA
level-elpmaS
UoIProactiveDetectionofVoiceCloningwithLocalizedWatermarking
cesstoanAPIthatproduceswatermarkedsamples, and
AudioSeal (ours) tonegativespeechsamplesfromanypublicdataset.They
100
Wavmark firstcollectsamplesandtrainaclassifiertodiscriminate
betweenwatermarkedandnot-watermarked.Theyattack
10 1 thisclassifierasifitwerethetruedetector.
For every scenario, we watermark 1k samples of 5 sec-
10 2
onds, then attack them. The gradient-based attack opti-
mizes an adversarial noise added to the audio, with 100
10 3 steps of Adam. During the optimization, we control the
norm of the noise to trade-off attack strength and audio
WM Generation WM Detection
quality. When training the classifier for the black-box at-
Figure6.Mean runtime (↓ is better). AudioSeal is one order
tack, we use 80k/80k watermarked/genuine samples of 8
ofmagnitudefasterforwatermarkgenerationandtwoordersof
seconds and make sure the classifier has 100% detection
magnitudefasterforwatermarkdetectionforthesameaudioin-
accuracyonthevalidationset. MoredetailsinApp.C.6.
put.SeeAppendixDforfullcomparison.
Figure 7 contrasts various attacks at different intensities,
or “remove” it to avoid detection. Our findings suggest
usingGaussiannoiseasareference. Thewhite-boxattack
that in order to maintain the effectiveness of watermark-
isbyfarthemosteffectiveone,increasingthedetectioner-
ing against such adversaries, the code for training water-
ror by around 80%, while maintaining high audio quality
marking models and the awareness that published audios
(PESQ>4). Otherattacksarelesseffective,requiringsig-
arewatermarkedcanbemadepublic. However,thedetec-
nificantaudioqualitydegradationtoachieve50%increase
tor’sweightsshouldbekeptconfidential.
thedetectionerror,thoughtheyarestillmoreeffectivethan
Wefocusonwatermark-removalattacksandconsiderthree randomnoiseaddition. Insummary,themoreisdisclosed
typesofattacksdependingontheadversary’sknowledge: about the watermarking algorithm, the more vulnerable it
is. The effectiveness of these attacks is limited as long as
• White-box: theadversaryhasaccesstothedetector(e.g.
thedetectorremainsconfidential.
becauseofaleak),andperformsagradient-basedadver-
sarial attack against it. The optimization objective is to
7.Conclusion
minimizethedetector’soutput.
• Semi black-box: the adversary does not have access to Inthispaper,weintroducedAudioSeal,aproactivemethod
any weights, but is able to re-train generator/detector for the detection, localization, and attribution of AI-
pairs with the same architectures on the same dataset. generatedspeech. AudioSealrevampsthedesignofaudio
They perform the same gradient-based attack as before, watermarking to be specific to localized detection rather
butusingthenewdetectorasproxyfortheoriginalone. than data hiding. It is based on a generator/detector ar-
chitecture that can generate and extract watermarks at the
• Black-box: the adversary does not have any knowledge
audiosamplelevel. Thisremovesthedependencyonslow
on the watermarking algorithm being used, but has ac-
bruteforcealgorithms,traditionallyusedtoencodeandde-
code audio watermarks. The networks are jointly trained
throughanovelloudnessloss,differentiableaugmentations
1.0 andmaskedsampleleveldetectionlosses. Asaresult,Au-
dioSeal achieves state-of-the-art robustness to various au-
0.8
dioeditingtechniques,veryhighprecisioninlocalization,
0.6 and orders of magnitude faster runtime than methods re-
lying on synchronization. Through an empirical analysis
White-box
0.4 ofpossibleadversarialattacks,weconcludethatforwater-
Semi black-box
marking to still be an effective mitigation, the detector’s
Black-box
0.2
weights have to be kept private – otherwise adversarial
Gaussian noise
attacks might be easily forged. A key advantage of Au-
0.0
4.5 4.0 3.5 3.0 2.5 2.0 1.5 dioSealisitspracticalapplicability. Itstandsasaready-to-
PESQ ( stronger attacks) deploysolutionforwatermarkinginvoicesynthesisAPIs.
This is pivotal for large-scale content provenance on so-
Figure7. Watermark-removalattacks. PESQismeasuredbe-
tweenattackedaudiosandgenuineones(PESQ<4stronglyde- cialmediaandfordetectingandeliminatingincidents,en-
gradestheaudioquality). Themoreknowledgetheattackerhas abling swift action on instances like the US voters’ deep-
overthewatermarkingalgorithm,thebettertheattackis. fakecase(Murphyetal.,2024)longbeforetheyspread.
)elacS
goL
,ces(
tnemges
rep
noitaruD
etaR
rorrE
noitceteDProactiveDetectionofVoiceCloningwithLocalizedWatermarking
EthicalStatement. Thisresearchaimstoimprovetrans- Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E.,
parency and traceability in AI-generated content, but wa- Pietquin,O.,Sharifi,M.,Roblek,D.,Teboul,O.,Grang-
termarking in general can have a set of potential misuses ier, D., Tagliasacchi, M., and Zeghidour, N. Audi-
suchasgovernmentsurveillanceofdissidentsorcorporate olm: A language modeling approach to audio genera-
identification of whistle blowers. Additionally, the water- tion. IEEE/ACM Transactions on Audio, Speech, and
marking technology might be misused to enforce copy- LanguageProcessing,31:2523–2533,2022.
right on user-generated content, and its ability to detect
AI-generatedaudiocouldincreaseskepticismaboutdigital Borsos, Z., Sharifi, M., Vincent, D., Kharitonov, E.,
communicationauthenticity, potentiallyunderminingtrust Zeghidour, N., and Tagliasacchi, M. Soundstorm:
in digital media and AI. However, despite these risks, en- Efficient parallel audio generation. arXiv preprint
suring the detectability of AI-generated content is impor-
arXiv:2305.09636,2023.
tant, along with advocating for robust security measures
Casanova, E., Weber, J., Shulby, C. D., Junior, A. C.,
andlegalframeworkstogovernthetechnology’suse.
Go¨lge,E.,andPonti,M.A. Yourtts: Towardszero-shot
multi-speakerttsandzero-shotvoiceconversionforev-
References eryone. InInternationalConferenceonMachineLearn-
ing,pp.2709–2720.PMLR,2022.
Chinese ai governance rules, 2023. URL
http://www.cac.gov.cn/2023-07/13/
Chen, G., Wu, Y., Liu, S., Liu, T., Du, X., and Wei, F.
c_1690898327029107.htm. Accessed on August
Wavmark: Watermarking for audio generation. arXiv
29,2023.
preprintarXiv:2308.12770,2023.
European ai act, 2023. URL https://
Copet,J.,Kreuk,F.,Gat,I.,Remez,T.,Kant,D.,Synnaeve,
artificialintelligenceact.eu/. Accessed
G., Adi, Y., and De´fossez, A. Simple and controllable
onAugust29,2023.
music generation. arXiv preprint arXiv:2306.05284,
Aaronson, S. and Kirchner, H. Watermarking gpt out- 2023.
puts,2023.URLhttps://www.scottaaronson.
Defossez,A.,Synnaeve,G.,andAdi,Y. Realtimespeech
com/talks/watermark.ppt.
enhancementinthewaveformdomain,2020.
AlBadawy, E. A., Lyu, S., and Farid, H. Detecting ai-
synthesized speech using bispectral analysis. In CVPR De´fossez, A., Copet, J., Synnaeve, G., and Adi, Y.
workshops,pp.104–109,2019. High fidelity neural audio compression. arXiv preprint
arXiv:2210.13438,2022.
Arik,S.,Chen,J.,Peng,K.,Ping,W.,andZhou,Y. Neural
voice cloning with a few samples. Advances in neural Fernandez, P., Chaffin, A., Tit, K., Chappelier, V., and
informationprocessingsystems,31,2018. Furon, T. Three bricks to consolidate watermarks for
largelanguagemodels. 2023IEEEInternationalWork-
Bai, H., Zheng, R., Chen, J., Ma, M., Li, X., and
shop on Information Forensics and Security (WIFS),
3
Huang,L. A t: Alignment-awareacousticandtextpre- 2023a.
training for speech synthesis and editing. In Chaud-
huri, K., Jegelka, S., Song, L., Szepesva´ri, C., Niu, Fernandez, P., Couairon, G., Je´gou, H., Douze, M., and
G., and Sabato, S. (eds.), International Conference Furon, T. The stable signature: Rooting watermarks in
on Machine Learning, ICML 2022, 17-23 July 2022, latentdiffusionmodels. ICCV,2023b.
Baltimore, Maryland, USA, volume 162 of Proceed-
Furon,T. Aconstructiveandunifyingframeworkforzero-
ings of Machine Learning Research, pp. 1399–1411.
PMLR,2022. URLhttps://proceedings.mlr. bit watermarking. IEEE Transactions on Information
press/v162/bai22d.html. ForensicsandSecurity,2(2):149–163,2007.
Barrington, S., Barua, R., Koorma, G., and Farid, Gritsenko, A., Salimans, T., van den Berg, R., Snoek, J.,
H. Single and multi-speaker cloned voice detection: andKalchbrenner,N.Aspectralenergydistanceforpar-
From perceptual to learned features. arXiv preprint allelspeechsynthesis. AdvancesinNeuralInformation
arXiv:2307.07683,2023. ProcessingSystems,33:13062–13072,2020.
Borrelli, C., Bestagini, P., Antonacci, F., Sarti, A., and Hines, A., Skoglund, J., Kokaram, A., and Harte, N.
Tubaro, S. Synthetic speech detection through short- Visqol: Thevirtualspeechqualityobjectivelistener. In
termandlong-termpredictiontraces. EURASIPJournal IWAENC2012;internationalworkshoponacousticsig-
onInformationSecurity,2021(1):1–14,2021. nalenhancement,pp.1–4.VDE,2012.ProactiveDetectionofVoiceCloningwithLocalizedWatermarking
Hsu, W.-N., Akinyemi, A., Rakotoarison, A., Tjandra, A., Kong, J., Kim, J., and Bae, J. Hifi-gan: Generative ad-
Vyas,A.,Guo,B.,Akula,B.,Shi,B.,Ellis,B.,Cruz,I., versarial networks for efficient and high fidelity speech
Wang,J.,Zhang,J.,Williamson,M.,Le,M.,Moritz,R., synthesis. In Larochelle, H., Ranzato, M., Hadsell, R.,
Adkins,R.,Ngan,W.,Zhang,X.,Yungster,Y.,andWu, Balcan, M., and Lin, H. (eds.), Advances in Neural In-
Y.-C. Audiobox: Unifiedaudiogenerationwithnatural formation Processing Systems, volume 33, pp. 17022–
languageprompts. arXivpreprintarXiv:...,2023. 17033.CurranAssociates,Inc.,2020.
Janicki,A. Spoofingcountermeasurebasedonanalysisof Kreuk,F.,Synnaeve,G.,Polyak,A.,Singer,U.,De´fossez,
linear prediction error. In Sixteenth annual conference A., Copet, J., Parikh, D., Taigman, Y., and Adi, Y.
of the international speech communication association, Audiogen: Textually guided audio generation. In The
2015. Eleventh International Conference on Learning Repre-
sentations,2023.
Juvela, L. and Wang, X. Collaborative watermark-
ing for adversarial speech synthesis. arXiv preprint Kumar, K., Kumar, R., de Boissie`re, T., Gestin, L., Teoh,
arXiv:2309.15224,2023. W. Z., Sotelo, J. M. R., de Bre´bisson, A., Bengio, Y.,
and Courville, A. C. Melgan: Generative adversarial
Kalantari, N. K., Akhaee, M. A., Ahadi, S. M., and
networksforconditionalwaveformsynthesis. InNeural
Amindavar,H. Robustmultiplicativepatchworkmethod
InformationProcessingSystems,2019.
for audio watermarking. IEEE Trans. Speech Au-
dio Process., 17(6):1133–1141, 2009. doi: 10.1109/ Kumar,R.,Seetharaman,P.,Luebs,A.,Kumar,I.,andKu-
TASL.2009.2019259.URLhttps://doi.org/10. mar,K. High-fidelityaudiocompressionwithimproved
1109/TASL.2009.2019259. rvqgan. ArXiv,abs/2306.06546,2023.
Kharitonov, E., Vincent, D., Borsos, Z., Marinier, R., Le, M., Vyas, A., Shi, B., Karrer, B., Sari, L., Moritz,
Girgin, S., Pietquin, O., Sharifi, M., Tagliasacchi, M., R.,Williamson,M.,Manohar,V.,Adi,Y.,Mahadeokar,
and Zeghidour, N. Speak, read and prompt: High- J., et al. Voicebox: Text-guided multilingual uni-
fidelitytext-to-speechwithminimalsupervision. ArXiv, versal speech generation at scale. arXiv preprint
abs/2302.03540,2023. arXiv:2306.15687,2023.
Kim, C., Min, K., Patel, M., Cheng, S., and Yang, Y. Lie, W. and Chang, L. Robust and high-quality time-
Wouaf: Weight modulation for user attribution and fin- domain audio watermarking based on low-frequency
gerprinting in text-to-image diffusion models. arXiv amplitude modification. IEEE Trans. Multim., 8
preprintarXiv:2306.04744,2023. (1):46–59, 2006. doi: 10.1109/TMM.2005.861292.
URL https://doi.org/10.1109/TMM.2005.
Kim, C. D., Kim, B., Lee, H., and Kim, G. Audiocaps:
861292.
Generating captions for audios in the wild. In NAACL-
HLT,2019. Liu,C.,Zhang,J.,Fang,H.,Ma,Z.,Zhang,W.,andYu,N.
Dear:Adeep-learning-basedaudiore-recordingresilient
Kim, J., Kong, J., and Son, J. Conditional variational au-
watermarking. InWilliams,B.,Chen,Y.,andNeville,J.
toencoderwithadversariallearningforend-to-endtext-
(eds.),Thirty-SeventhAAAIConferenceonArtificialIn-
to-speech. In International Conference on Machine
telligence,AAAI2023,Thirty-FifthConferenceonInno-
Learning,pp.5530–5540.PMLR,2021.
vativeApplicationsofArtificialIntelligence,IAAI2023,
Kirchenbauer, J., Geiping, J., Wen, Y., Katz, J., Miers, I., ThirteenthSymposiumonEducationalAdvancesinArti-
andGoldstein,T. Awatermarkforlargelanguagemod- ficial Intelligence, EAAI 2023, Washington, DC, USA,
els. arXivpreprintarXiv:2301.10226,2023. February 7-14, 2023, pp. 13201–13209. AAAI Press,
2023. doi: 10.1609/aaai.v37i11.26550.
Kirovski, D. and Attias, H. Audio watermark robustness
to desynchronization via beat detection. In Petitcolas, Liu, Z., Huang, Y., and Huang, J. Patchwork-based
F.A.P.(ed.), InformationHiding, pp.160–176, Berlin, audio watermarking robust against de-synchronization
Heidelberg, 2003. Springer Berlin Heidelberg. ISBN and recapturing attacks. IEEE Trans. Inf. Foren-
978-3-540-36415-3. sics Secur., 14(5):1171–1180, 2019. doi: 10.1109/
TIFS.2018.2871748. URLhttps://doi.org/10.
Kirovski, D. and Malvar, H. S. Spread-spectrum water-
1109/TIFS.2018.2871748.
marking of audio signals. IEEE Trans. Signal Pro-
cess., 51(4):1020–1033, 2003. doi: 10.1109/TSP.2003. Luo, Y. and Mesgarani, N. Conv-tasnet: Surpassing ideal
809384.URLhttps://doi.org/10.1109/TSP. time–frequency magnitude masking for speech separa-
2003.809384. tion. IEEE/ACM Transactions on Audio, Speech, andProactiveDetectionofVoiceCloningwithLocalizedWatermarking
Language Processing, 27(8):1256–1266, 2019. doi: Seamless Communication, Barrault, L., Chung, Y.-A.,
10.1109/TASLP.2019.2915167. Meglioli, M.C., Dale, D., Dong, N., Duppenthaler, M.,
Duquenne, P.-A., Ellis, B., Elsahar, H., Haaheim, J.,
Mu¨ller, N.M., Czempin, P., Dieckmann, F., Froghyar, A., Hoffman, J., Hwang, M.-J., Inaguma, H., Klaiber, C.,
andBo¨ttinger,K. Doesaudiodeepfakedetectiongener- Kulikov, I., Li, P., Licht, D., Maillard, J., Mavlyutov,
alize? arXivpreprintarXiv:2203.16263,2022. R.,Rakotoarison,A.,Sadagopan,K.R.,Ramakrishnan,
A., Tran, T., Wenzek, G., Yang, Y., Ye, E., Evtimov,
Murphy,M.,Metz,R.,Bergen,M.,andBloomberg. Biden
I., Fernandez, P., Gao, C., Hansanti, P., Kalbassi, E.,
audio deepfake spurs ai startup elevenlabs—valued
Kallet, A., Kozhevnikov, A., Mejia, G., Roman, R. S.,
at $1.1 billion—to ban account: ‘we’re going to see
Touret, C., Wong, C., Wood, C., Yu, B., Andrews, P.,
a lot more of this’. Fortune, January 2024. URL
Balioglu, C., Chen, P.-J., Costa-jussa`, M. R., Elbayad,
https://fortune.com/2024/01/27/ai-
M.,Gong,H.,Guzma´n,F.,Heffernan,K.,Jain,S.,Kao,
firm-elevenlabs-bans-account-for-
J.,Lee,A.,Ma,X.,Mourachko,A.,Peloquin,B.,Pino,
biden-audio-deepfake/.
J.,Popuri,S.,Ropers,C.,Saleem,S.,Schwenk,H.,Sun,
A., Tomasello, P., Wang, C., Wang, J., Wang, S., and
Natgunanathan,I.,Xiang,Y.,Rong,Y.,Zhou,W.,andGuo,
Williamson, M. Seamless: Multilingual expressive and
S. Robust patchwork-based embedding and decoding
streamingspeechtranslation. 2023.
scheme for digital audio watermarking. IEEE Trans.
Speech Audio Process., 20(8):2232–2239, 2012. doi:
Series, B. Method for the subjective assessment of inter-
10.1109/TASL.2012.2199111. URL https://doi. mediate quality level of audio systems. International
org/10.1109/TASL.2012.2199111.
TelecommunicationUnionRadiocommunicationAssem-
bly,2014.
Nguyen, T. A., Hsu, W.-N., d’Avirro, A., Shi, B., Gat, I.,
Fazel-Zarani, M., Remez, T., Copet, J., Synnaeve, G., Shen, K., Ju, Z., Tan, X., Liu, Y., Leng, Y., He, L., Qin,
Hassid, M., etal. Expresso: Abenchmarkandanalysis T., Zhao, S., and Bian, J. Naturalspeech 2: Latent
ofdiscreteexpressivespeechresynthesis. arXivpreprint diffusion models are natural and zero-shot speech and
arXiv:2308.05725,2023. singingsynthesizers. CoRR,abs/2304.09116,2023. doi:
10.48550/ARXIV.2304.09116. URL https://doi.
Pavlovic´, K., Kovacˇevic´, S., Djurovic´, I., and Woj-
org/10.48550/arXiv.2304.09116.
ciechowski,A. Robustspeechwatermarkingbyajointly
trainedembedderanddetectorusingadnn. DigitalSig- Su, Z., Zhang, G., Yue, F., Chang, L., Jiang, J., and
nalProcessing,122:103381,2022. Yao, X. Snr-constrained heuristics for optimizing the
scaling parameter of robust audio watermarking. IEEE
Qu,X.,Yin,X.,Wei,P.,Lu,L.,andMa,Z. Audioqr: Deep
Trans.Multim.,20(10):2631–2644,2018. doi: 10.1109/
neuralaudiowatermarksforqrcode. IJCAI,2023. TMM.2018.2812599.URLhttps://doi.org/10.
1109/TMM.2018.2812599.
Ren, Y., Zhu, H., Zhai, L., Sun, Z., Shen, R., and Wang,
L. Who is speaking actually? robust and versatile Taal, C.H., Hendriks, R.C., Heusdens, R., andJensen, J.
speakertraceabilityforvoiceconversion. arXivpreprint A short-time objective intelligibility measure for time-
arXiv:2305.05152,2023. frequency weighted noisy speech. In 2010 IEEE inter-
nationalconferenceonacoustics,speechandsignalpro-
Rix, A. W., Beerends, J. G., Hollier, M. P., and Hekstra,
cessing,pp.4214–4217.IEEE,2010.
A. P. Perceptual evaluation of speech quality (pesq)-a
newmethodforspeechqualityassessmentoftelephone Tai, Y.-Y. and Mansour, M. F. Audio watermarking over
networks and codecs. In 2001 IEEE international con- the air with modulated self-correlation. In ICASSP
ferenceonacoustics,speech,andsignalprocessing.Pro- 2019-2019 IEEE International Conference on Acous-
ceedings (Cat. No. 01CH37221), volume 2, pp. 749– tics,SpeechandSignalProcessing(ICASSP),pp.2452–
752.IEEE,2001. 2456.IEEE,2019.
Sahidullah, M., Kinnunen, T., and Hanilc¸i, C. A compar- telecommunicationUnion,I. Algorithmstomeasureaudio
ison of features for synthetic speech detection. ISCA programme loudness and true-peak audio level. Series,
(theInternationalSpeechCommunicationAssociation), BS,2011.
2015.
The White House. Ensuring safe, secure, and trustwor-
Schnupp, J., Nelken, I., and King, A. Auditory neuro- thy ai. https://www.whitehouse.gov/wp-
science: Makingsenseofsound. MITpress,2011. content/uploads/2023/07/Ensuring-ProactiveDetectionofVoiceCloningwithLocalizedWatermarking
Safe-Secure-and-Trustworthy-AI.pdf, Quenneville-Be´lair,V.,andShi,Y.Torchaudio:Building
July2023. Accessed: [july2023]. blocks for audio andspeech processing. arXiv preprint
arXiv:2110.15018,2021.
van den Oord, A., Dieleman, S., Zen, H., Simonyan, K.,
Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., Yin, P., Lyu, J., Zhang, S., Osher, S., Qi, Y., and Xin,
andKavukcuoglu,K. Wavenet: Agenerativemodelfor J. Understanding straight-through estimator in train-
rawaudio. InArxiv,2016. ing activation quantized neural nets. arXiv preprint
arXiv:1903.05662,2019.
Wang, C., Rivie`re, M., Lee, A., Wu, A., Talnikar, C.,
Haziza, D., Williamson, M., Pino, J. M., and Dupoux, Yu, N., Skripniuk, V., Abdelnabi, S., andFritz, M. Artifi-
E. Voxpopuli: A large-scale multilingual speech cor- cialfingerprintingforgenerativemodels: Rootingdeep-
pus for representation learning, semi-supervised learn- fake attribution in training data. In Proceedings of the
ingandinterpretation. InZong,C.,Xia,F.,Li,W.,and IEEE/CVFInternationalconferenceoncomputervision,
Navigli,R.(eds.),Proceedingsofthe59thAnnualMeet- pp.14448–14457,2021a.
ingoftheAssociationforComputationalLinguisticsand
Yu, N., Skripniuk, V., Chen, D., Davis, L. S., and Fritz,
the11thInternationalJointConferenceonNaturalLan-
M. Responsible disclosure of generative models using
guageProcessing,ACL/IJCNLP2021,(Volume1: Long
scalable fingerprinting. In International Conference on
Papers),VirtualEvent,August1-6,2021,pp.993–1003.
LearningRepresentations,2021b.
Association for Computational Linguistics, 2021. doi:
10.18653/V1/2021.ACL-LONG.80. URL https://
Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and
doi.org/10.18653/v1/2021.acl-long.80.
Tagliasacchi, M. Soundstream: An end-to-end neural
audiocodec.IEEE/ACMTransactionsonAudio,Speech,
Wang, C., Chen, S., Wu, Y., Zhang, Z., Zhou, L., Liu, S.,
and Language Processing, 30:495–507, 2022. doi: 10.
Chen, Z., Liu, Y., Wang, H., Li, J., et al. Neural codec
1109/TASLP.2021.3129994.
languagemodelsarezero-shottexttospeechsynthesiz-
ers. arXivpreprintarXiv:2301.02111,2023.
Zhang, C., Yu, C., and Hansen, J. H. An investigation of
deep-learning frameworks for speaker verification anti-
Wen, Y., Kirchenbauer, J., Geiping, J., and Goldstein,
spoofing.IEEEJournalofSelectedTopicsinSignalPro-
T. Tree-ring watermarks: Fingerprints for diffusion
cessing,11(4):684–694,2017.
images that are invisible and robust. arXiv preprint
arXiv:2305.20030,2023.
Wu,S.,Liu,J.,Huang,Y.,Guan,H.,andZhang,S. Adver-
sarial audio watermarking: Embedding watermark into
deepfeature. In2023IEEEInternationalConferenceon
MultimediaandExpo(ICME),pp.61–66.IEEE,2023.
Xiang,Y.,Natgunanathan,I.,Guo,S.,Zhou,W.,andNaha-
vandi,S. Patchwork-basedaudiowatermarkingmethod
robusttode-synchronizationattacks. IEEEACMTrans.
Audio Speech Lang. Process., 22(9):1413–1423, 2014.
doi: 10.1109/TASLP.2014.2328175. URL https://
doi.org/10.1109/TASLP.2014.2328175.
Xiang, Y., Natgunanathan, I., Peng, D., Hua, G., and
Liu, B. Spread spectrum audio watermarking using
multiple orthogonal PN sequences and variable embed-
ding strengths and polarities. IEEE ACM Trans. Au-
dio Speech Lang. Process., 26(3):529–539, 2018. doi:
10.1109/TASLP.2017.2782487. URLhttps://doi.
org/10.1109/TASLP.2017.2782487.
Yang, Y.-Y., Hira, M., Ni, Z., Chourdia, A., Astafurov,
A., Chen, C., Yeh, C.-F., Puhrsch, C., Pollack, D.,
Genzel, D., Greenberg, D., Yang, E. Z., Lian, J., Ma-
hadeokar, J., Hwang, J., Chen, J., Goldsborough, P.,
Roy, P., Narenthiran, S., Watanabe, S., Chintala, S.,ProactiveDetectionofVoiceCloningwithLocalizedWatermarking
A.Extendedrelatedwork B.FalsePositiveRates-TheoryandPractice
Zero-shotTTSandvocalstylepreservation. Therehas Theoretical FPR. When doing multi-bit watermarking,
beenanemergenceofmodelsthatimitateorpreservevocal previous works (Yu et al., 2021a; Kim et al., 2023; Fer-
style using only a small amount of data. One key exam- nandezetal.,2023b;Chenetal.,2023)usuallyextractthe
pleiszero-shottext-to-speech(TTS)models. Thesemod- messagem′ fromthecontentxandcompareittotheorig-
elscreatespeechinvocalstylestheyhaven’tbeenspecifi- inalbinarysignaturem ∈ {0,1}k embeddedinthespeech
callytrainedon. Forinstance,modelslikeVALL-E(Wang sample. Thedetectiontestreliesonthenumberofmatch-
et al., 2023), YourTTS (Casanova et al., 2022), Natural- ingbitsM(m,m′):
Speech2 (Shen et al., 2023) synthesize high-quality per-
sonalizedspeechwithonlya3-secondrecording. Ontop, ifM(m,m′)≥τ where τ ∈{0,...,k}, (4)
zero-shot TTS models like Voicebox (Le et al., 2023),
A3T (Bai et al., 2022) and Audiobox (Hsu et al., 2023), thentheaudioisflagged. Thisprovidestheoreticalguaran-
withtheirnon-autoregressiveinference,performtaskssuch teesoverthefalsepositiverates.
astext-guidedspeechinfilling,wherethegoalistogenerate
Formally,thestatisticalhypothesesareH :“Theaudiosig-
1
masked speech given its surrounding audio and text tran-
nal x is watermarked”, and the null hypothesis H : “The
0
script. Itmakesthemapowerfultoolforspeechmanipula-
audio signal x is genuine”. Under H (i.e., for unmarked
0
tion. In the context of speech machine translation, Seam- audio), if the bits m′,...,m′ are independent and iden-
lessExpressive (Seamless Communication et al., 2023) is 1 k
tically distributed (i.i.d.) Bernoulli random variables with
a model that not only translates speech, but also retains parameter 0.5, then M(m,m′) follows a binomial distri-
thespeaker’suniquevocalstyleandemotionalinflections,
bution with parameters (k, 0.5). The False Positive Rate
therebybroadeningthecapabilitiesofsuchsystems. (FPR) is defined as the probability that M(m,m′) ex-
ceeds a given threshold τ. A closed-form expression can
Audio generation and compression. Early models are
be given using the regularized incomplete beta function
autoregressive like WaveNet (van den Oord et al., 2016),
I (a;b)(linkedtotheCDFofthebinomialdistribution):
with dilated convolutions and waveform reconstruction as x
objective. Subsequent approaches explore different au-
FPR(τ)=P(M >τ|H )=I (τ +1,k−τ). (5)
diolosses,suchasscale-invariantsignal-to-noiseratio(SI- 0 1/2
SNR) (Luo & Mesgarani, 2019) or Mel spectrogram dis-
tance (Defossez et al., 2020). None of these objectives Empirical study. We empirically study the FPR of
are deemed ideal for audio quality, leading to the adop- WavMark-based detection on our validation dataset. We
tionofadversarialmodelsinHiFi-GAN(Kongetal.,2020) use the same parameters as in the original paper, i.e. k =
or MelGAN(Kumaret al.,2019). Ourtraining objectives 32-bitsareextractedfrom1sspeechsamples. Wefirstex-
andarchitecturesareinspiredbymorerecentneuralaudio tract the soft bits (before thresholding) from 10k genuine
compression models (De´fossez et al., 2022; Kumar et al., samplesandplotthehistogramofthescoresinFig.8(left).
2023; Zeghidour et al., 2022), that focus on high-quality WeshouldobserveaGaussiandistributionwithmean0.5,
waveformgenerationandintegrateacombinationofthese whileempiricallythescoresarecenteredaround0.38.This
diverseobjectivesintheirtrainingprocesses. makesthedecisionheavilybiasedtowardsbit0ongenuine
samples. It is therefore impossible to theoretically set the
Synchronization and Detection speed. To accurately FPRsincethiswouldlargelyunderestimatetheactualone.
extract watermarks, synchronization between the encoder
and decoder is crucial. However, this can be disrupted by
desynchronization attacks such as time and pitch scaling.
Toaddressthisissue, varioustechniqueshavebeendevel- 3 10 1
oped. Oneapproachisblockrepetition, whichrepeatsthe
10 3
watermark signal along both the time and frequency do- 2
mains(Kirovski&Malvar,2003;Kirovski&Attias,2003). 10 5
Another method involves implanting synchronization bits 1 10 7
Empirical
into the watermarked signal (Xiang et al., 2014). During
10 9 Theoretical
decoding,thesesynchronizationbitsservetoimprovesyn-
0
chronizationandmitigatetheeffectsofde-synchronization 0.0 0.5 1.0 0.0 0.5 1.0
Output score Threshold on bit accuracy
attacks. Detection of those synchronization bits for wa-
termarkdetectionusuallyinvolvesexhaustivesearchusing Figure8. (Left) Histogram of scores output by WavMark’s ex-
bruteforcealgorithms,whichsignificantlyslowsdownde- tractoron10kgenuinesamples.(Right)Empiricalandtheoretical
codingtime. FPRwhenthechosenhiddenmessageisall0.
ytisneD
etar
evitisop
eslaFProactiveDetectionofVoiceCloningwithLocalizedWatermarking
Forinstance,Figure8(right)showsthetheoreticalandem- and0.5; (E)fixeddelayof0.5seconds, fixedvolumeof
piricalFPRfordifferentvaluesofτ whenthechosenhid- 0.5.
den message is full 0. Put differently, the argument that
• PinkNoise: Addspinknoiseforabackgroundnoiseef-
saysthathidingbitsallowsfortheoreticalguaranteesover
fect. (T)standarddeviationfixedat0.01;(E)fixedat0.1.
thedetectionratesisnotvalidinpractice.
• WhiteNoise: Addsgaussiannoisetothewaveform. (T)
standarddeviationfixedat0.001;(E)fixedat0.05.
C.Experimentaldetails
• Smooth: Smoothstheaudiosignalusingamovingaver-
C.1.Loudness age filter with a variable window size. (T) window size
randombetween2and10;(E)fixedat40.
Our loudness function is based on a simplification of the
implementation in the torchaudio (Yang et al., 2021) li- • AAC: Encodes the audio in AAC format. (T) bitrate of
brary.Itiscomputedthroughamulti-stepprocess.Initially, 128kbps;(E)bitrateof64kbps.
the audio signal undergoes K-weighting, which is a filter- • MP3: Encodes the audio in MP3 format. (T) bitrate of
ing process that emphasizes certain frequencies to mimic 128kbps;(E)bitrateof32kbps.
the human ear’s response. This is achieved by applying a
• EnCodec: Resamplesat24kHz,encodestheaudiowith
treblefilterandahigh-passfilter.Followingthis,theenergy
EnCodec with nq = 16 (16 streams of tokens), and re-
oftheaudiosignaliscalculatedforeachblockofthesignal.
samplesitbackto16kHz.
Thisisdonebysquaringthesignalandaveragingovereach
block. Theenergyisthenweightedaccordingtothenum- Implementation of the augmentations is done with the
berofchannelsintheaudiosignal, withdifferentweights juliuspythonlibrarywhenpossible.
applied to different channels to account for their varying
Weplotinthefigure9formultipleaugmentationsthede-
contributions to perceived loudness. Finally, the loudness
tection accuracy with respect to the strength of the mod-
is computed by taking the logarithm of the weighted sum
ification. As can be seen for most augmentations our
ofenergiesandaddingaconstantoffset.
method outperforms Wavmark for every parameters. We
remark that for Highpass filters well above our training
C.2.RobustnessAugmentations
range (500Hz) Wavmark has a much better detection ac-
Herearethedetailsoftheaudioeditingaugmentationsused curacy.
attraintime(T),andevaluationtime(E):
C.3.Networksarchitectures(Fig.4)
• BandpassFilter: Combineshigh-passandlow-passfil-
teringtoallowaspecificfrequencybandtopassthrough. The watermark generator is composed of an encoder
(T)fixedbetween300Hzand8000Hz;(E)fixedbetween and a decoder, both incorporating elements from En-
500Hzand5000Hz. Codec (De´fossez et al., 2022). The encoder applies a 1D
convolution with 32 channels and a kernel size of 7, fol-
• HighpassFilter: Usesahigh-passfilterontheinputau-
lowed by four convolutional blocks. Each of these blocks
diotocutfrequenciesbelowacertainthreshold.(T)fixed
includes a residual unit and down-sampling layer, which
at500Hz;(E)fixedat1500Hz.
uses convolution with stride S and kernel size K = 2S.
• Lowpass Filter: Applies a low-pass filter to the input
Theresidualunithastwokernel-3convolutionswithaskip-
audio, cuttingfrequenciesaboveacutofffrequency. (T)
connection,doublingchannelsduringdown-sampling.The
fixedat5000Hz;(E)fixedat500Hz.
encoder concludes with a two-layer LSTM and a final
• Speed: Changesthespeedoftheaudiobyafactorclose 1D convolution with a kernel size of 7 and 128 channels.
to1. (T)randombetween0.9and1.1;(E)fixedat1.25. StridesSvaluesare(2,4,5,8)andthenonlinearactivation
inresidualunitsistheExponentialLinearUnit(ELU).The
• Resample: Upsamples to intermediate sample rate and
decoder mirrors the encoder but uses transposed convolu-
thendownsamplestheaudiobacktoitsoriginalratewith-
tionsinstead,withstridesinreverseorder.
outchangingitsshape. (T)and(E)32kHz.
Thedetector comprisesanencoder, a transposedconvolu-
• Boost Audio: Amplifies the audio by multiplying by a
tionandalinearlayer. Theencodersharesthegenerator’s
factor. (T)factorfixedat1.2;(E)fixedat10.
architecture (but with different weights). The transposed
• DuckAudio:Reducesthevolumeoftheaudiobyamul-
convolutionhashoutputchannelsandupsamplestheacti-
tiplyingfactor. (T)factorfixedat0.8;(E)fixedat0.1.
vationmaptotheoriginalaudioresolution(resultinginan
• Echo:Appliesanechoeffecttotheaudio,addingadelay activation map of shape (t,h)). The linear layer reduces
andlessloudcopyoftheoriginal. (T)randomdelaybe- the h dimensions to two, followed by a softmax function
tween0.1and0.5seconds,randomvolumebetween0.1 thatgivessample-wiseprobabilityscores.ProactiveDetectionofVoiceCloningwithLocalizedWatermarking
Gaussian Noise Lowpass Filter Highpass Filter
1.0 1.0 1.0
0.9 0.9 0.9
0.8 0.8 0.8
0.7 0.7 0.7
0.6 0.6 0.6
0.5 0.5 0.5
10 3 10 2 10 1 100 500 1000 1500 2000 2500 1000 2000 30004000
Standard deviation Cutoff Frequency (Hz) Cutoff Frequency (Hz)
Encodec Compression Mp3 Compression Pink Noise
1.0 1.0 1.0
0.9
0.9 0.9
0.8
0.8
0.8
0.7
0.7
0.6 0.7 AudioSeal AudioSeal
0.5 WavMark 0.6 WavMark
1.5 3.0 6.0 12.0 24.0 12 24 32 64 128 256 10 2 10 1 100
Bitrate (kbps) Bitrate (kbps) Standard deviation
Figure9. Accuracyofthedetectoronaugmentedsampleswithrespecttothestrengthoftheaugmentation.
C.4.MUSHRAprotocoledetail basedattack. Itstartsbyinitializingadistortionδ with
adv
random gaussian noise. The algorithm iteratively updates
The Mushra protocole consist in a crowd sourced test
the distortion for a number of steps n. For each step,
where participants have to rate out of 100 the quality of
the distortion is added to the original audio via x = x+
some samples. The ground truth is given. We used 100
α.tanh(δ ),passedthroughthemodeltogetpredictions.
adv
speechsamplesof10seceach. Everysampleisevaluated
A cross-entropy loss is computed with label either 0 (for
byatleast20participants. Weincludedinthestudyalow
removal) or 1 (for forging), and back-propagated through
anchorverylossycompressionat1.5kbps(usingEncodec).
thedetectortoupdatethedistortion,usingtheAdamopti-
Participant that do not manage to rate the lowest score to
mizer. At the end of the process, the adversarial audio is
the low anchor for at least 80% of their assignments are
x++α.tanh(δ ). In our attack, we use a scaling fac-
adv
removed from the study. As a comparison Ground truth
torα = 10−3,anumberofstepsn = 100,andalearning
samplesobtained80.49andlowanchor’s53.21.
rateof10−1. Thetanhfunctionisusedtoensurethatthe
distortionremainssmall,andgivesanupperboundonthe
C.5.Outofdomain(ood)evaluations
SNRoftheadversarialaudio.
Asdescribedinpreviouslytoensurethatourmodelwould
workwellonanytypeofvoicecloningmethodwetestedit
onoutputsofseveralvoicecloningmodelsandotheraudio
modalities. Onthe10ksamplesthatcomposedeachofour Training of the malicious detector. Here, we are inter-
ooddatasets,nosamplewasmisclassifiedbyourproactive ested in training a classifier that can distinguish between
detection method. On top of that we performed the same watermarked and non-watermarked samples, when access
set of augmentations and observed very similar numbers. to many samples of both types is available. To train the
Weshowtheresultsintable5. Evenifwedidnottrainon classifier,weuseadatasetmadeofmorethan80ksamples
AIgeneratedspeechweobserveaincreaseinperformance of8secondsspeechfromVoicebox(Leetal.,2023)water-
comparedtoourtestdatawh marked using our proposed method and a similar amount
of genuine (un-watermarked) speech samples. The classi-
C.6.Attacksonthewatermark fier shares the same architecture as AudioSeal’s detector.
The classifier is trained for 200k updates with batches of
Adversarial attack against the detector. Given a sam-
64 one-second samples. It achieves perfect classification
ple x and a detector D, we want to find x′ ∼ x such
ofthesamples. ThisiscoherentwiththefindingsofVoice-
that D(x′) = 1−D(x). To that end, we use a gradient-
box(Leetal.,2023).
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccAProactiveDetectionofVoiceCloningwithLocalizedWatermarking
Table5. EvaluationofAudioSealGeneralizationAcrossDomainsandLanguages. Namely, translationsofspeechsamplesfromthe
Expresso dataset (Nguyen et al., 2023) to four target languages: Mandarin Chinese (CMN), French (FR), Italian (IT), and Spanish
(SP), using the SeamlessExpressive model (Seamless Communication et al., 2023). Music from MusicGen (Copet et al., 2023) and
environmentalsoundsfromAudioGen(Kreuketal.,2023).
Aug Seamless(Cmn) Seamless(Spa) Seamless(Fra) Seamless(Ita) Seamless(Deu) Voicebox(Eng) AudioGen MusicGen
None 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
Bandpass 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
Highpass 0.71 0.68 0.70 0.70 0.70 0.64 0.52 0.52
Lowpass 1.00 0.99 1.00 1.00 1.00 1.00 1.00 1.00
Boost 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
Duck 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
Echo 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
Pink 0.99 1.00 0.99 1.00 0.99 1.00 1.00 1.00
White 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
Fast(x1.25) 0.97 0.98 0.99 0.98 0.99 0.98 0.87 0.87
Smooth 0.96 0.99 0.99 0.99 0.99 0.99 0.98 0.98
Resample 1.00 1.00 1.00 1.00 1.00 1.00 1.00 1.00
AAC 0.99 0.99 0.99 0.99 0.99 0.97 0.99 0.98
MP3 0.99 0.99 0.99 0.99 0.99 0.97 0.99 1.00
Encodec 0.97 0.98 0.99 0.99 0.98 0.96 0.95 0.95
Average 0.97 0.97 0.98 0.98 0.98 0.97 0.95 0.95
Table6. Theaverageruntime(ms)persampleofourproposedAudioSealmodelagainstthestate-of-the-artWavmark(Chenetal.,2023)
method.Ourexperimentswereconductedonadatasetofaudiosegmentsspanning1secto10secs,usingasingleNvidiaQuadroGP100
GPU.Theresults,displayedinthetable,demonstratesubstantialspeedenhancementsforbothWatermarkGenerationandDetectionwith
andwithoutthepresenceofawatermark.Notably,forwatermarkdetection,AudioSealis485×fasterthanWavmarkduringtheabsence
ofawatermark,moredetailsinsection5.5.
Model Watermarked Detectionms(speedup) Generationms(speedup)
Wavmark No 1710.70±1314.02 –
AudioSeal(ours) No 3.25±1.99 (485×) –
Wavmark Yes 106.21±66.95 104.58±65.66
AudioSeal(ours) Yes 3.30±2.03 (35×) 7.41±4.52 (14×)
D.ComputationalEfficiency
We show in Figure 10 the mean runtime of the detection
and generation depending on the audio duration. Corre- Watermark Generation Watermark Detection
spondingnumbersaregiveninTable6. 100 AudioSeal (ours) 100
wavmark
10 1 10 1
10 2 10 2
10 3 10 3
10 4 10 4
10 5 10 5
2 4 6 8 10 2 4 6 8 10
Segment Duration (seconds) Segment Duration (seconds)
Figure10.Meanruntime(↓isbetter)ofAudioSealversusWav-
Mark. AudioSealisoneorderofmagnitudefasterforwatermark
generationandtwoordersofmagnitudefasterforwatermarkde-
tection for the same audio input, signifying a considerable en-
hancementinreal-timeaudiowatermarkingefficiency.
)elacS
goL
,ces(
tnemges
rep
noitaruD