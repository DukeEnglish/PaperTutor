ESPnet-SPK: full pipeline speaker embedding toolkit with reproducible
recipes, self-supervised front-ends, and off-the-shelf models
Jee-weonJung∗,1,WangyouZhang∗,1,JiatongShi∗,1,ZakariaAldeneh2,TakuyaHiguchi2,
Barry-JohnTheobald2,AhmedHussenAbdelaziz2,andShinjiWatanabe1
1CarnegieMellonUniversity,USA 2Apple,USA
jeeweonj@ieee.org, shinjiw@ieee.org
Abstract Table 1: Open-source speaker verification toolkits. △: pre-
trainedmodelsexist,butnotpreparedforoff-the-shelfusage.†:
This paper introduces ESPnet-SPK, a toolkit designed with currentlydoesnotsupportaSSLfeature-basedspeakerverifi-
several objectives for training speaker embedding extractors. cationrecipe,butpossiblewithmodifications.
First, we provide an open-source platform for researchers in
Name Datasets SSLfront-end Off-the-shelf
the speaker recognition community to effortlessly build mod-
ASV-subtools[16] Multiple ✗ ✗
els. We provide several models, ranging from x-vector to re-
NeMo[19] VoxCeleb ✗ ✗
cent SKA-TDNN. Through the modularized architecture de- Kaldi[13] Multiple ✗ △
sign,variantscanbedevelopedeasily. Wealsoaspiretobridge VoxCelebTrainer[14] VoxCeleb ✗ △
developed models with other domains, facilitating the broad Wespeaker[15] Multiple ✗ △
researchcommunitytoeffortlesslyincorporatestate-of-the-art 3D-Speaker[20] Multiple ✗ △
PaddleSpeech[17] VoxCeleb ✓† ✓
embedding extractors. Pre-trained embedding extractors can
SpeechBrain[18] VoxCeleb ✓† ✓
beaccessedinanoff-the-shelfmannerandwedemonstratethe
ESPnet-SPK Multiple ✓ ✓
toolkit’sversatilitybyshowcasingitsintegrationwithtwotasks.
Anothergoalistointegratewithdiverseself-supervisedlearn-
developmentofspeakerembeddingextractorsremainunavail-
ingfeatures. Wereleaseareproduciblerecipethatachievesan able.
equal error rate of 0.39% on the Vox1-O evaluation protocol
Tothisend,weintroduceESPnet-SPK,anewopen-source
usingWavLM-LargewithECAPA-TDNN.
speaker embedding toolkit designed to meet the evolving de-
IndexTerms:speakerverification,speakerrecognition,toolkit
mands of the research community. The objective behind the
creationofESPnet-SPKismultifaceted. Primarily,weprovide
1. Introduction
speakerrecognitionresearcherswithaplatformtodevelopand
compare models using a unified environment across multiple
Inrecentyears, therapiddevelopmentofspeakerrecognition,
datasets. Weprovideacomprehensiverepertoireofcontempo-
in conjunction with the application of deep learning has re-
rarymodels,spanningfromx-vectortothesophisticatedSKA-
sultedinrobustspeakerembeddings[1–5].Theseembeddings,
TDNN[25].1 Well-structuredmodeldevelopment(detailedin
which represent unique speaker characteristics in a compact
Section2)makesdevelopingnewmodelseasy. Secondly, the
form,havefoundapplicationsacrossawiderangeoftasks,in-
toolkit enables researchers from the broader speech commu-
cluding speaker verification, diarization [6,7], text-to-speech
nitytoeasilyaccessandutilizethelateststate-of-the-artmodels
(TTS) [8], and other speech tasks [9–12]. As the demand for
witheasefortheirstudies. Forthispurpose,ESPnet-SPKpro-
speakerembeddingextractorscontinuestogrow,severalopen-
videseasy-to-usepre-trainedspeakerembeddingextractorsand
sourcetoolkitsthatfacilitateboththetraininganddeployment
theassociatedtoolingforinstantiatingandrunningthemodels,
ofsuchembeddingextractormodelshaveemerged[13–20].
whichwerefertoasoff-the-shelfuse. Weshowcaseexamples
Table 1 presents some well-known speaker recognition
intwotaskswithreproducibleESPnetrecipes: TTSandtarget
toolkits. Compared to fast-evolving requirements across the
speaker extraction (TSE). Finally, we integrate SSL front-end
community,weobserveafewlimitationsintheexistingtoolk-
featuresintothedevelopmentofspeakerembeddingextractors
its. Manycurrenttoolkitslackaunifiedenvironmentfordata
to further boost the performance in speaker-related tasks. We
preprocessing,featureextraction,andthedesignoftraining/in-
release a recipe which achieves an equal error rate (EER) of
ference for researchers. The accessibility of state-of-the-art
0.39%ontheVox1-Oevaluationprotocol,employingWavLM-
models for researchers within the broader research commu-
Large[23]coupledwithECAPA-TDNN[26].
nity is often restricted. As a result, less competitive speaker
embeddings are often used in downstream tasks [8,21]. De- Among existing toolkits, PaddleSpeech and Speech-
spite the impressive performances shown by employing self- Brain[18]arethemostrelevanttoESPnet-SPK.ButESPnet-
supervisedlearning(SSL)models’representationsasinputfea- SPKisnotlimitedtoonlyasinglerecipeontheVoxCelebcor-
tures[22–24],therecipesonintegratingthesefeaturesintothe pus, andESPnet-SPKsupportsmodelswithmorecompetitive
performance. For example, WavLM-Large features combined
∗Equalcontribution. withECAPA-TDNN[23,26]demonstratingEERof0.39%on
Experiments of this work used the Bridges2 system at PSC and Vox1-O,whichhasservedasarepresentativebenchmarksuc-
DeltasystematNCSAthroughallocationsCIS210014andIRI120008P cessfullyemployingSSLfeatureinspeakerrecognition[27].
fromtheAdvancedCyberinfrastructureCoordinationEcosystem: Ser-
vices & Support (ACCESS) program, supported by National Sci-
enceFoundationgrants#2138259,#2138286,#2138307,#2137603,and 1ESPnet-SPKcurrentlysupportsfivemodelsbutmoremodelswill
#2138296. beadded.
4202
naJ
03
]DS.sc[
1v03271.1042:viXraFigure2: Sub-componentsconstitutingthespeakerembedding
extractor.Bycombiningdifferentsub-componentsintheconfig-
Figure1: OverviewofESPnet-SPK’sprocesspipeline. Itcon- urationfile,dozensofmodelscanbeeasilydeveloped.
sistsofmultiplestages(resemblingtheKaldi[13]speechpro-
cessing toolkit). Stages 1 through 10 described in the upper Modularized sub-components. ESPnet-SPK divides the
partillustratethespeakerverificationprocedure,wheretrained speakerembeddingextractorintofoursub-components: front-
speakerembeddingscanbeoptionallymadepubliclyavailable end,encoder,pooling,andprojector. Thefront-enddigestsraw
viastages9and10. Publiclyavailableembeddingextractors waveforms and produces a sequence of input features.2 The
canbeeasilyusedinanoff-the-shelfmanner. encoder is the main frame-level feature modeling block con-
sisting of a stack of convolution, or Conformer [33] blocks.
2. Framework The pooling refers to the aggregation methods, such as the
widelyadoptedattentivestatisticspooling[34]. Theprojector
2.1. Design
referstoagroupofadditionallayers,usuallyacompositionof
TheESPnet-SPKcanbedividedintotwoparts: speakerveri- fully-connected, batchnormalization[35], orlayernormaliza-
ficationrecipeandoff-the-shelfspeakerembeddingextraction. tion[36]layers.Hencepredefinedmodelscanbeablatedeasily
Section 2.2 describes the eight stages of our speaker verifica- byalteringoneortwosub-components. Figure2illustratesthe
tion recipes. Stages 1 through 4 pre-process and prepare the sub-componentsdescribed.
data,stage5conductstraining,andstages6through8inferthe
Objective functions, optimizers, and learning rate sched-
results.
ulers. ESPnet-SPKsupportsdiverseobjectivefunctionsspan-
Section 2.3 describes the process for uploading and us-
ning from conventional cross-entropy to AAM-Softmax [37]
ing trained speaker embedding extractors. The former part,
with sub-center and inter top-k applied [38]. The optimizers
stages 9 and 10 of a speaker verification recipe, uploads the
andlearningrateschedulersaresharedwiththerestoftheESP-
trainedspeakerembeddingextractortomakeitpubliclyavail-
netfamily,providingaselectionofwidelyusedconfigurations.
able. Thelatterpartisdesignedforresearchersoutsideofthe
speaker recognition community on top of speaker recognition
Inference. Theinferencephasecomprisesstages6, 7, and8,
researchers.Figure1illustratestheoverallESPnet-SPK.
whereeachstagehastheroleof:
2.2. Speakerverificationrecipe • Stage6: speakerembeddingsextractionfromallutterances
usedininference;
• Stage7:postnormalizationand/orenhancements;
Preprocessing. The preprocessing phase comprises Stages 1
through4,whereeachstagehastheroleof: • Stage8:metriccalculation.
• Stage1: datasetsdownloadandpreparation(includingthose Stage6extractsspeakerembeddingsfromallutterancesin
usedforaugmentation); thetestset. Inaddition, Stage6alsoextractsembeddingsfor
utterancesthatwillbeusedinStage7.Atypicalexamplewould
• Stage2:speedperturbation[28];
bethecohortsetsthatcanbeusedforscorenormalizationand
• Stage3:dataformatting;
qualitymeasurefunction.
• Stage4:statisticscalculation.
Stage 7 applies additional processing in the score level.
ThesestagesaresharedwiththerestofESPnet; weguidethe Onceallscoresfortrialsarederived, scorenormalizationand
readers to [29] for full details. One difference with the other qualitymeasurefunctionscanbeappliedbasedontheconfig-
tasks is that we follow [4,30], where the speakers of speed uration. For score normalization, we support AS-norm [39]
perturbed utterances are considered new speakers, not identi- andforqualitymeasurefunction,inwhichweimplementthose
cal.Thisaugmentationhasbeenshowntobemoreeffectiveon mentionedin[40].
speakerverification. Stage 8 lastly calculates the metrics, EER, and minimum
detection cost function (minDCF). For the minDCF, we use a
Training. Stage5conductsthemaintraining. Withacentral-
p ,C ,andC of0.05,1,and1respectively.
izedyaml-styleconfigurationfile,wedefinethemodelarchi- trg falsealarm miss
tecture,objectivefunction,optimizer,andothertraininghyper-
2.3. Off-the-shelfusage
parameters,whicharefurtherusedtotrainthespeakerembed-
dingextractor. Oncethetrainingandinferencestagesarefinished,thetrained
modelcanbemadepubliclyavailableviastages9and10:
Supported models. ESPnet-SPK currently supports five pre-
• Stage9:modelfitpreparation;
defined models: x-vector [1], MFA-Conformer [31], ECAPA-
TDNN [26], RawNet3 [32], and SKA-TDNN [25]. We skip 2In the case of models that directly digest raw waveform, e.g.,
architecturaldetailsduetospacelimitationsandguidereaders RawNet3,thefront-endincludeslayerslikeSinc-convwithnormaliza-
totheindividualpapersforfurtherdetails. tionstoprovideanintegratedformatfortheencoderinput.Table2: ComparisonofmodelsontheVox1-Oevaluationpro-
1 import numpy as np
2 from espnet2.bin.spk_inference import tocol. Lowerisbetterforbothequalerrorrates(EER,%)and
Speech2Embedding minimumdetectioncostfunction(minDCF).
3
4 # from uploaded models Model EER(%)↓ minDCF↓
5 speech2spk_embed = Speech2Embedding.from_pretrained
(model_tag="espnet/voxcelebs12_rawnet3") x-vector[1] 1.81 0.1251
6 embedding = speech2spk_embed(np.zeros(16500)) MFA-Conformer[31] 0.86 0.0627
7
8 # from checkpoints trained by oneself ECAPA-TDNN[26] 0.85 0.0666
9 speech2spk_embed = Speech2Embedding(model_file=" RawNet3[32] 0.73 0.0581
model.pth", train_config="config.yaml")
SKA-TDNN[25] 0.72 0.0457
10 embedding = speech2spk_embed(np.zeros(32000))
Table3:ComparisonofECAPA-andSKA-TDNNwithdifferent
Figure 3: Example code for using publicly available or self-
front-endsontheVox1-Oevaluationprotocol.Threeconditions
developedspeakerembeddingextractorsinESPnet-SPK.
are compared, mel-spectrogram, WavLM (fixed) and WavLM
(jointlyfine-tuned). ReportedinEERs(%). “Mel-spec”: mel-
• Stage10:Huggingfacemodelupload. spectrogram,“tuned”:jointlyfine-tuned.
For the models made publicly available, we provide off- Front-end
Model
the-shelf usage. Users can load a pre-trained speaker embed- Mel-spec WavLM(fixed) WavLM(tuned)
ding extractor simply by setting the model name.3 This part
ECAPA-TDNN 0.85 0.60 0.39
oftheframeworkistargetedprimarilyforthoseoutsideofthe SKA-TDNN 0.72 0.56 0.51
speakerrecognitioncommunity. Figure3displaysanexample
ofextractingaspeakerembeddingfromapre-trainedRawNet3
modelwithlessthan10linesofPythoncode. usedasinputfeaturesexceptforRawNet3, whichdirectlydi-
gestsrawwaveforms. AAM-softmaxwithsub-centerandinter
top-k penalty loss functions were adopted. We employed an
3. Experiments
Adamoptimizerthathasawarm-upwitharestart,usingapeak
ESPnet-SPK has been developed using PyTorch, a Python- andalowestlearningrateof1e−3and1e−7[44,45].
baseddeeplearningtoolkit,andlivesathttps://github. Among the five currently supported models, SKA-TDNN
com/espnet/espnet. andRawNet3performedthebest. Notethatallfivemodelsare
availableoff-the-shelfviafewlinesofcode.4
3.1. Corpus
3.3. SSLfront-endreplacements
Throughout this paper, the experiments span VoxCelebs1&2
datasets[41,42]andtherecentlyintroducedVoxBlink[43]cor- Table 3 shows the results of replacing the mel-spectrogram
pora.4 front-end with a WavLM-Large [23] SSL model. It is worth
noting that in ESPnet-SPK, thanks to the integration with
VoxCelebs. TheVoxCeleb1&2corpusisalarge-scalepublicly S3PRL5 [22], users can easily access over 100 SSL models
availabledatasetcomprising1,211+6,122speakersandover by changing only a few lines in the yaml configuration file.
1.2millionutterances.Unlessmentionedotherwise,weusethe In this paper, we select WavLM as an example because of its
combineddevsetsofVoxCelebs1&2,whichconsistofapproxi- superiorperformanceinspeaker-relatedtasksintheSUPERB
mately2,690hoursofspeechfrom7,205speakers.Weemploy benchmark[22]. Here, weemployECAPA-TDNNandSKA-
theVox1-Oevaluationprotocolwhichusesapproximately37k TDNN,eachwiththreeconfigurations:mel-spectrogramfront-
trialsfrom40speakersintheVoxCeleb1testset,whichiscon- end, WavLMfront-end(fixed), andWavLMfront-end(jointly
sideredthemostwidelyusedevaluationprotocolintherecent fine-tuned). All unmentioned hyperparameters are equal to
speakerrecognitionfield. thoseinSection3.2.
Results show consistent tendencies across both architec-
VoxBlink.TheVoxBlinkcorpushasbeenintroducedrelatively
tures,WavLMoutperformingmel-spectrogramandjointlyfine-
recently and comprises two subsets: full- and clean-set. The
tuning the SSL front-end outperforming freezing it. For
full set consists of 1.45 million utterances from 38k speakers
ECAPA-TDNN, we reproduced an EER of 0.38% in the
whereasthecleanset,asubsetofthefullset,consistsof1.02
WavLM paper6 with an EER of 0.39% in ESPnet-SPK. For
million utterances from 18k speakers. We use VoxBlink be-
SKA-TDNN jointly fine-tuning the WavLM brought less im-
causeitcanbecomplementarytoVoxCeleb, whereVoxCeleb
provement. Wehypothesizethislimitedimprovementmaybe
focusesoncelebritieswhereasVoxBlinkfocusesmoreonindi-
relatedtoWavLMfeaturedimensionsnothavingaserialitylike
vidualsingeneralYouTubevideos.ReaderscanrefertoLenet
thefrequencybandsinthemel-spectrogramandhencetheSKA
al.[43]forfulldetails.
headmaynotshowanoptimalcombinationwiththeSSLfea-
ture.
3.2. Mainresultswithsupportedmodelarchitectures
Table2showsthemainresultsontherepresentativeVoxCeleb 3.4. RecipesonVoxBlink
recipe with various supported models. Mel-spectrograms ex-
Table 4 displays the results using VoxBlink dataset [43]. Al-
tractedfromcroppedorduplicatedutterancesof3secondswere
though two sets, full and clean, were introduced, no perfor-
3Available list of models live at https://github.com/ manceonthemodelstrainedonthefullsethasbeenreported
espnet/espnet/egs2/[dataset]/spk1/README.md.
4Morerecipesonpubliccorporawillbeavailable,butareexcluded 5https://github.com/s3prl/s3prl
fromthispaperduetospacelimitations. 6Onlythetrainedmodelweightsarerevealed,notthetrainingrecipe.Table4:ResultsofrecipesinvolvingtheVoxBlink[43]dataset. Table6:TSEresultsonWSJ0-2mix(minmode)usingdifferent
Forthefirsttime,wereporttheresultoftrainingonlywiththe speakerembeddings.
VoxBlink-fullset. ThemodelarchitectureisfixedtoRawNet3.
Embeddingextractor PESQ↑ STOI↑ SDR(dB)↑
“Vb-full”:VoxBlink-full,“Vb-cln”:VoxBlink-clean.
x-vector[1,18] 2.77 0.83 10.61
VoxcelebsVb-full Vb-cln EER(%)↓ minDCF↓ RawNet3[32] 2.81 0.85 11.18
WavLM-Large(fixed)+ECAPA 2.80 0.84 10.66
✓ - - 0.73 0.0581
WavLM-Large(tuned)+ECAPA 2.88 0.85 11.53
- ✓ - 2.68 0.1893
- - ✓ 2.51 0.1858 Table7: Ablationsonselectedcomponents. Objectivefunction
✓ ✓ - 0.78 0.0655 and train data is ablated on RawNet3. Model architecture is
✓ - ✓ 0.77 0.0556 ablatedonECAPA-TDNN.
Model EER(%)↓ minDCF↓
Table 5: TTS Evaluation Results on VCTK with Different
RawNet3 0.73 0.0581
SpeakerEmbeddings.MOSofreferencesis3.88. →w/osub-center&intertop-k 0.79 0.0626
Embedding MCD↓ WER(%)↓ UTMOS↑ MOS↑ SMOS↑ →w/oVoxCeleb1dev 0.85 0.0698
x-vector[1,18] 6.76 9.8 3.80 3.35 3.75 WavLM-Large(fixed)+ECAPA 0.60 0.0446
Rawnet3[32] 6.76 7.7 3.84 3.40 3.79
→w/oVoxCeleb2dev 1.33 0.1157
WavLM(tuned)+ECAPA 6.75 8.0 3.86 3.42 3.75
→w/identityencoder 1.88 0.1363
yet.7 To complement the results, we conducted experiments
providedinESPnetfordatapreparationandtraining9.
on both sets (rows 2 and 3), respectively, as well as combin-
Results in Table 6 demonstrates that the ECAPA-TDNN
ingthemwiththeVoxCelebs1&2developmentsets(rows4and
with a jointly fine-tuned WavLM front-end achieves the best
5). We find that the quality of data is important as the clean
TSEperformance,followedbyRawNet3embeddings.Theper-
setoutperformedthefullsetcounterpartbyasmallmarginde-
formancesareconsistentwiththoseinTable2,wherewecon-
spitehavingsignificantlymoredata(2.1kvs1.6khours). For
firm that better speaker embeddings can improve TSE perfor-
the results of enlarging the train data with VoxCeleb, we find
mance.
that training with only VoxCeleb data results in better perfor-
mance,whereasin[43],combineddatabroughtover10%im- 3.7. Ablations
provement.Thisdifferenceintendencymayhavebeenaffected
Finally, wepresentasetofablationsonselectedcomponents.
byseveralfactors,includingthatweemployVoxCeleb1devset
ResultsaredemonstratedinTable7. First,weshowtwoabla-
whiletheoriginalpaperonlyusedVoxCeleb2devset.
tions on RawNet3 by ablating either the objective function or
3.5. TTSresults reducingthetrainingdata. Whenremovingtherecentlywidely
adopted sub-center AAM-softmax and inter top-k and using
Inpreviousmulti-speakerTTSresearch, thereexistsasignifi-
an AAM-softmax as the sole loss function, EER increased to
cantrelianceonpre-trainedspeakerembeddings[8,21]. How-
0.79%. WhenVoxCeleb1-devwasremoved,trainingonlywith
ever,thereisalackofinvestigationintotheimpactofdifferent
VoxCeleb2-dev,EERfurtherincreasedto0.85%. Throughthe
pre-trainedspeakerembeddingextractorsinTTS.Inthiswork,
results, were-confirmedtheimportanceofbothadvancedob-
we employ three pre-trained speaker embeddings, integrating
jectivefunctionsandadditionaldata.
themwithaVITSmodel[46]toassesstheireffectivenessinthe
Next, we ablated with the model adopting SSL front-end
VCTKcorpus[47].
features and report the results in rows 4 to 6 motivated from
Forevaluation,weusebothobjectiveandsubjectivemea-
[52].WhentrainedwithonlyVoxCeleb1-dev,anEERof1.33%
sures. This includes mean cepstral distortion (MCD), word
could be obtained. Further removing the ECAPA-TDNN en-
error rate (WER) analyzed by Whisper-small [48], UTMOS
coder(i.e.,thearchitecturebecomesWavLM,attentivestatistics
[49],meanopinionscore(MOS),andspeakersimilarityMOS
pooling, andaprojectionlayer)resultedinanEERof1.88%.
(SMOS).Wereceived6votesperutterancefromEnglishspeak-
TheseresultsagainconfirmthatSSLfront-endscombinedwith
ersforMOSandSMOS.Thefindingsoftheseevaluationsare
relativelylesscomplexarchitecturesanddatacanstillbedevel-
detailed in Table 5. We observe the MOS scores are consis-
opedtoobtaincompetitiveperformance.
tentwithTable2,whereSMOSwouldfavortheRawNet3-based
embeddingextractor. 4. Conclusion
3.6. TSEresults WeintroducedESPnet-SPK,anopen-sourcetoolkitforspeaker
AnotherimportantapplicationofspeakerembeddingsisTSE, verificationandembeddingextraction. Thetoolkitfocuseson
wherethespeakerembeddingsserveasananchortodetectand providingafullpipelineofdiversecorporaaswellasconnec-
extractthespeakerofinterestfromthemixturespeechofmul- tionswithSSLfeatures.Asaresult,itsupportsfivemodelsthat
tiple speakers. Therefore, it is vital to select a representative arewidelyusedinrecentworksaswellasamethodtoeasily
speakerembeddinginTSE.Wecomparefourdifferentspeaker developnewmodelsbyalteringoneormoresub-components.
embeddingsontheireffectivenessinTSEbasedonthewidely- ESPnet-SPKfurtherprovidesanoff-the-shelfexperienceforin-
usedWSJ0-2mixbenchmark[50]. Here,theTD-SpeakerBeam terdisciplinaryusagesaimingatvariousdownstreamtasksthat
model[51]isadoptedasthebasicarchitecture,andweusethe requirespeakerembeddings.
minversion8ofdatasetsforexperiments.Wefollowtherecipe
7ESPnet-SPKisthefirstopen-sourcetoolkitprovidingrecipeson 9https://github.com/espnet/espnet/tree/
theVoxBlinkdataset. master/egs2/wsj0_2mix/tse1.
8Eachsampleistrimmedtoonlycontainedfullyoverlappedspeech. 9Theexperimentsarestillongoing.5. References
[27] Z.Chenetal.,“Large-scaleself-supervisedspeechrepresentation
learning for automatic speaker verification,” in Proc. ICASSP,
[1] D.Snyder, D.Garcia-Romeroetal., “X-vectors: RobustDNN
2022.
embeddingsforspeakerrecognition,”inProc.ICASSP,2018.
[28] T.Ko,V.Peddinti,D.PoveyandS.Khudanpur, “Audioaugmen-
[2] J.-w. Jung, H.-S. Heo, J.-h. Kim et al., “RawNet: Advanced tationforspeechrecognition,”inProc.Interspeech,2015.
end-to-end deep neural network using raw waveforms for text-
independentspeakerverification,”inProc.Interspeech,2019. [29] S.Watanabe,T.Hori,S.Karitaetal.,“ESPnet:End-to-endspeech
processingtoolkit,”inProc.Interspeech,2018.
[3] L.Wan,Q.Wang,A.PapirandI.L.Moreno, “Generalizedend-
[30] Z.Chen,B.Han,X.Xiangetal., “BuildaSREchallengesys-
to-endlossforspeakerverification,”inProc.ICASSP,2018.
tem: LessonsfromVoxSRC2022andCNSRC2022,” inProc.
[4] H.Yamamoto,K.A.Lee,K.OkabeandT.Koshinaka, “Speaker Interspeech,2023.
augmentationandbandwidthextensionfordeepspeakerembed-
[31] Y.Zhang, Z.Lv, H.Wuetal., “MFA-Conformer: Multi-scale
ding.,”inProc.Interspeech,2019.
featureaggregationconformerforautomaticspeakerverification,”
[5] J.Villalba,Y.ZhangandN.Dehak, “x-vectorsmeetadversarial inProc.Interspeech,2022.
attacks:Benchmarkingadversarialrobustnessinspeakerverifica-
[32] J.-w.Jung,Y.Kim,H.-S.Heoetal., “Pushingthelimitsofraw
tion.,”inProc.Interspeech,2020.
waveformspeakerrecognition,”inProc.Interspeech,2022.
[6] T.J.Parketal., “Areviewofspeakerdiarization: Recentad-
[33] A.Gulatietal.,“Conformer:Convolution-augmentedtransformer
vanceswithdeeplearning,” ComputerSpeech&Language,vol.
forspeechrecognition,”inProc.Interspeech,2020.
72,2022.
[34] K. Okabe, T. Koshinaka and K. Shinoda, “Attentive statistics
[7] J.-w.Jung,H.-S.Heoetal., “Insearchofstrongembeddingex- poolingfordeepspeakerembedding,”inProc.Interspeech,2018.
tractorsforspeakerdiarisation,”inProc.ICASSP.IEEE,2023.
[35] S.IoffeandC.Szegedy,“Batchnormalization:Acceleratingdeep
[8] E.Casanova,J.Weber,C.D.Shulbyetal., “YourTTS:Towards networktrainingbyreducinginternalcovariateshift,” inProc.
zero-shotmulti-speakerTTSandzero-shotvoiceconversionfor ICML.pmlr,2015.
everyone,”inProc.ICML,2022.
[36] J.L.Ba,J.R.KirosandG.E.Hinton, “Layernormalization,” in
[9] Q. Wang et al., “VoiceFilter: Targeted voice separation by NeuralIPS-DeepLearningSymposium,2016.
speaker-conditionedspectrogrammasking,”inProc.Interspeech,
[37] J.Deng, J.Guo, N.XueandS.Zafeiriou, “ArcFace: Additive
2019.
angularmarginlossfordeepfacerecognition,” inProc.CVPR,
[10] C.Xu,W.Raoetal., “SpEx: Multi-scaletimedomainspeaker 2019.
extractionnetwork,”IEEE/ACMTrans.ASLP.,vol.28,2020.
[38] M.Zhao,Y.Ma,M.LiuandM.Xu,“TheSpeakInsystemforVox-
[11] T.-h.Huang,J.-h.LinandH.-y.Lee,“Howfararewefromrobust Celebspeakerrecognitionchallange2021,” arXiv:2109.01989,
voiceconversion:Asurvey,”inProc.SLT,2021. 2021.
[12] W.-C.Huang,L.P.Violeta,S.Liuetal., “Thesingingvoicecon- [39] P.Matejka,O.Novotny`,O.Plchotetal., “Analysisofscorenor-
versionchallenge2023,”arXiv:2306.14422,2023. malizationinmultilingualspeakerrecognition.,” inProc.Inter-
speech,2017.
[13] D.Povey, A.Ghoshal, G.Boulianneetal., “TheKaldispeech
recognitiontoolkit,”inProc.ASRU,2011. [40] N.Torgashov,R.Makarov,I.Yakovlevetal.,“TheIDR&DVox-
Celeb speaker recognition challenge 2023 system description,”
[14] J.S.Chung,J.Huh,S.Munetal., “Indefenceofmetriclearning
arXiv:2308.08294,2023.
forspeakerrecognition,”inProc.Interspeech,2020.
[41] A.Nagrani,J.S.ChungandA.Zisserman, “VoxCeleb:Alarge-
[15] H.Wangetal., “Wespeaker:Aresearchandproductionoriented scalespeakeridentificationdataset,”inProc.Interspeech,2017.
speakerembeddinglearningtoolkit,”inProc.ICASSP,2023.
[42] J.S.Chung,A.NagraniandA.Zisserman, “VoxCeleb2: Deep
[16] F.Tong,M.Zhao,J.Zhouetal., “ASV-Subtools: Opensource speakerrecognition,”inProc.Interspeech,2018.
toolkitforautomaticspeakerverification,”inProc.ICASSP,2021.
[43] Y.Lin,X.Qin,G.Zhaoetal., “VoxBlink: Alargescalespeaker
[17] H.Zhang,T.Yuan,J.Chenetal.,“PaddleSpeech:Aneasy-to-use verificationdatasetoncamera,”inProc.ICASSP,2024.
all-in-onespeechtoolkit,”inProc.NAACL,2022.
[44] D.P.KingmaandJ.Ba, “Adam: amethodforstochasticopti-
[18] M.Ravanelli,T.Parcollet,P.Plantingaetal., “SpeechBrain: A mization,”inProc.ICLR,2015.
general-purposespeechtoolkit,”arXiv:2106.04624,2021.
[45] I.LoshchilovandF.Hutter, “SGDR:Stochasticgradientdescent
[19] O.Kuchaiev,J.Li,H.Nguyenetal.,“NeMo:atoolkitforbuilding withwarmrestarts,”inProc.ICLR,2017.
aiapplicationsusingneuralmodules,”arXiv:1909.09577,2019.
[46] J.Kimetal.,“Conditionalvariationalautoencoderwithadversar-
[20] S. Zheng, L. Cheng et al., “3D-Speaker: A large-scale multi- iallearningforend-to-endtext-to-speech,”inProc.ICML,2021.
device,multi-distance,andmulti-dialectcorpusforspeechrepre-
[47] C. Veaux, J. Yamagishi and K. MacDonald, “CSTR VCTK
sentationdisentanglement,”arXiv:2306.15354,2023.
Corpus: EnglishMulti-speakerCorpusforCSTRVoiceCloning
[21] T.Hayashi,R.Yamamoto,T.Yoshimuraetal., “ESPnet2-TTS: Toolkit,” TheCentreforSpeechTechnologyResearch(CSTR),
ExtendingtheedgeofTTSresearch,”arXiv:2110.07840,2021. UniversityofEdinburgh,2019.
[22] S.wenYangetal.,“SUPERB:SpeechProcessingUniversalPER- [48] A.Radford,J.W.Kim,T.Xuetal., “Robustspeechrecognition
formanceBenchmark,”inProc.Interspeech,2021. vialarge-scaleweaksupervision,”inProc.ICML.PMLR,2023.
[23] S.Chen,C.Wangetal., “WavLM:Large-scaleself-supervised [49] T.Saeki,D.Xinetal., “UTMOS:UTokyo-SaruLabSystemfor
pre-trainingforfullstackspeechprocessing,” IEEEJournalof VoiceMOSChallenge2022,”inProc.Interspeech,2022.
SelectedTopicsinSignalProcessing,vol.16,no.6,2022. [50] J.R.Hersheyetal.,“Deepclustering:Discriminativeembeddings
forsegmentationandseparation,”inProc.ICASSP,2016.
[24] C.Wangetal.,“UniSpeech:Unifiedspeechrepresentationlearn-
ingwithlabeledandunlabeleddata,”inProc.ICML,2021. [51] M. Delcroix, T. Ochiai, K. Zmolikova et al., “Improving
speaker discrimination of target speech extraction with time-
[25] S.H.Mun,J.-w.Jungetal.,“Frequencyandmulti-scaleselective
domainSpeakerBeam,”inProc.ICASSP,2020.
kernelattentionforspeakerverification,”inProc.SLT,2022.
[52] J.Peng,T.Stafylakis,R.Guetal., “Parameter-efficienttransfer
[26] B.Desplanques,J.Thienpondtetal., “ECAPA-TDNN:Empha-
learningofpre-trainedtransformermodelsforspeakerverification
sized channel attention, propagation and aggregation in TDNN
usingadapters,”inProc.ICASSP,2023.
basedspeakerverification,”inProc.Interspeech,2020.