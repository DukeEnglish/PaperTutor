[
    {
        "title": "How Numerical Precision Affects Mathematical Reasoning Capabilities of LLMs",
        "authors": "Guhao FengKai YangYuntian GuXinyue AiShengjie LuoJiacheng SunDi HeZhenguo LiLiwei Wang",
        "links": "http://arxiv.org/abs/2410.13857v1",
        "entry_id": "http://arxiv.org/abs/2410.13857v1",
        "pdf_url": "http://arxiv.org/pdf/2410.13857v1",
        "summary": "Despite the remarkable success of Transformer-based Large Language Models\n(LLMs) across various domains, understanding and enhancing their mathematical\ncapabilities remains a significant challenge. In this paper, we conduct a\nrigorous theoretical analysis of LLMs' mathematical abilities, with a specific\nfocus on their arithmetic performances. We identify numerical precision as a\nkey factor that influences their effectiveness in mathematical tasks. Our\nresults show that Transformers operating with low numerical precision fail to\naddress arithmetic tasks, such as iterated addition and integer multiplication,\nunless the model size grows super-polynomially with respect to the input\nlength. In contrast, Transformers with standard numerical precision can\nefficiently handle these tasks with significantly smaller model sizes. We\nfurther support our theoretical findings through empirical experiments that\nexplore the impact of varying numerical precision on arithmetic tasks,\nproviding valuable insights for improving the mathematical reasoning\ncapabilities of LLMs.",
        "updated": "2024-10-17 17:59:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.13857v1"
    },
    {
        "title": "From Gradient Clipping to Normalization for Heavy Tailed SGD",
        "authors": "Florian HüblerIlyas FatkhullinNiao He",
        "links": "http://arxiv.org/abs/2410.13849v1",
        "entry_id": "http://arxiv.org/abs/2410.13849v1",
        "pdf_url": "http://arxiv.org/pdf/2410.13849v1",
        "summary": "Recent empirical evidence indicates that many machine learning applications\ninvolve heavy-tailed gradient noise, which challenges the standard assumptions\nof bounded variance in stochastic optimization. Gradient clipping has emerged\nas a popular tool to handle this heavy-tailed noise, as it achieves good\nperformance in this setting both theoretically and practically. However, our\ncurrent theoretical understanding of non-convex gradient clipping has three\nmain shortcomings. First, the theory hinges on large, increasing clipping\nthresholds, which are in stark contrast to the small constant clipping\nthresholds employed in practice. Second, clipping thresholds require knowledge\nof problem-dependent parameters to guarantee convergence. Lastly, even with\nthis knowledge, current sampling complexity upper bounds for the method are\nsub-optimal in nearly all parameters. To address these issues, we study\nconvergence of Normalized SGD (NSGD). First, we establish a parameter-free\nsample complexity for NSGD of\n$\\mathcal{O}\\left(\\varepsilon^{-\\frac{2p}{p-1}}\\right)$ to find an\n$\\varepsilon$-stationary point. Furthermore, we prove tightness of this result,\nby providing a matching algorithm-specific lower bound. In the setting where\nall problem parameters are known, we show this complexity is improved to\n$\\mathcal{O}\\left(\\varepsilon^{-\\frac{3p-2}{p-1}}\\right)$, matching the\npreviously known lower bound for all first-order methods in all problem\ndependent parameters. Finally, we establish high-probability convergence of\nNSGD with a mild logarithmic dependence on the failure probability. Our work\ncomplements the studies of gradient clipping under heavy tailed noise improving\nthe sample complexities of existing algorithms and offering an alternative\nmechanism to achieve high probability convergence.",
        "updated": "2024-10-17 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.13849v1"
    },
    {
        "title": "Artificial Kuramoto Oscillatory Neurons",
        "authors": "Takeru MiyatoSindy LöweAndreas GeigerMax Welling",
        "links": "http://arxiv.org/abs/2410.13821v1",
        "entry_id": "http://arxiv.org/abs/2410.13821v1",
        "pdf_url": "http://arxiv.org/pdf/2410.13821v1",
        "summary": "It has long been known in both neuroscience and AI that ``binding'' between\nneurons leads to a form of competitive learning where representations are\ncompressed in order to represent more abstract concepts in deeper layers of the\nnetwork. More recently, it was also hypothesized that dynamic (spatiotemporal)\nrepresentations play an important role in both neuroscience and AI. Building on\nthese ideas, we introduce Artificial Kuramoto Oscillatory Neurons (AKOrN) as a\ndynamical alternative to threshold units, which can be combined with arbitrary\nconnectivity designs such as fully connected, convolutional, or attentive\nmechanisms. Our generalized Kuramoto updates bind neurons together through\ntheir synchronization dynamics. We show that this idea provides performance\nimprovements across a wide spectrum of tasks such as unsupervised object\ndiscovery, adversarial robustness, calibrated uncertainty quantification, and\nreasoning. We believe that these empirical results show the importance of\nrethinking our assumptions at the most basic neuronal level of neural\nrepresentation, and in particular show the importance of dynamical\nrepresentations.",
        "updated": "2024-10-17 17:47:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.13821v1"
    },
    {
        "title": "Discrete distributions are learnable from metastable samples",
        "authors": "Abhijith JayakumarAndrey Y. LokhovSidhant MisraMarc Vuffray",
        "links": "http://arxiv.org/abs/2410.13800v1",
        "entry_id": "http://arxiv.org/abs/2410.13800v1",
        "pdf_url": "http://arxiv.org/pdf/2410.13800v1",
        "summary": "Markov chain samplers designed to sample from multi-variable distributions\noften undesirably get stuck in specific regions of their state space. This\ncauses such samplers to approximately sample from a metastable distribution\nwhich is usually quite different from the desired, stationary distribution of\nthe chain. We show that single-variable conditionals of metastable\ndistributions of reversible Markov chain samplers that satisfy a strong\nmetastability condition are on average very close to those of the true\ndistribution. This holds even when the metastable distribution is far away from\nthe true model in terms of global metrics like Kullback-Leibler divergence or\ntotal variation distance. This property allows us to learn the true model using\na conditional likelihood based estimator, even when the samples come from a\nmetastable distribution concentrated in a small region of the state space.\nExplicit examples of such metastable states can be constructed from regions\nthat effectively bottleneck the probability flow and cause poor mixing of the\nMarkov chain. For specific cases of binary pairwise undirected graphical\nmodels, we extend our results to further rigorously show that data coming from\nmetastable states can be used to learn the parameters of the energy function\nand recover the structure of the model.",
        "updated": "2024-10-17 17:38:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.13800v1"
    },
    {
        "title": "Change Detection in Multivariate data streams: Online Analysis with Kernel-QuantTree",
        "authors": "Michelangelo Olmo Nogara NotarianniFilippo LeveniDiego StucchiLuca FrittoliGiacomo Boracchi",
        "links": "http://arxiv.org/abs/2410.13778v1",
        "entry_id": "http://arxiv.org/abs/2410.13778v1",
        "pdf_url": "http://arxiv.org/pdf/2410.13778v1",
        "summary": "We present Kernel-QuantTree Exponentially Weighted Moving Average (KQT-EWMA),\na non-parametric change-detection algorithm that combines the Kernel-QuantTree\n(KQT) histogram and the EWMA statistic to monitor multivariate data streams\nonline. The resulting monitoring scheme is very flexible, since histograms can\nbe used to model any stationary distribution, and practical, since the\ndistribution of test statistics does not depend on the distribution of\ndatastream in stationary conditions (non-parametric monitoring). KQT-EWMA\nenables controlling false alarms by operating at a pre-determined Average Run\nLength ($ARL_0$), which measures the expected number of stationary samples to\nbe monitored before triggering a false alarm. The latter peculiarity is in\ncontrast with most non-parametric change-detection tests, which rarely can\ncontrol the $ARL_0$ a priori. Our experiments on synthetic and real-world\ndatasets demonstrate that KQT-EWMA can control $ARL_0$ while achieving\ndetection delays comparable to or lower than state-of-the-art methods designed\nto work in the same conditions.",
        "updated": "2024-10-17 17:17:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.13778v1"
    }
]