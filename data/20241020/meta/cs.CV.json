[
    {
        "title": "Fluid: Scaling Autoregressive Text-to-image Generative Models with Continuous Tokens",
        "authors": "Lijie FanTianhong LiSiyang QinYuanzhen LiChen SunMichael RubinsteinDeqing SunKaiming HeYonglong Tian",
        "links": "http://arxiv.org/abs/2410.13863v1",
        "entry_id": "http://arxiv.org/abs/2410.13863v1",
        "pdf_url": "http://arxiv.org/pdf/2410.13863v1",
        "summary": "Scaling up autoregressive models in vision has not proven as beneficial as in\nlarge language models. In this work, we investigate this scaling problem in the\ncontext of text-to-image generation, focusing on two critical factors: whether\nmodels use discrete or continuous tokens, and whether tokens are generated in a\nrandom or fixed raster order using BERT- or GPT-like transformer architectures.\nOur empirical results show that, while all models scale effectively in terms of\nvalidation loss, their evaluation performance -- measured by FID, GenEval\nscore, and visual quality -- follows different trends. Models based on\ncontinuous tokens achieve significantly better visual quality than those using\ndiscrete tokens. Furthermore, the generation order and attention mechanisms\nsignificantly affect the GenEval score: random-order models achieve notably\nbetter GenEval scores compared to raster-order models. Inspired by these\nfindings, we train Fluid, a random-order autoregressive model on continuous\ntokens. Fluid 10.5B model achieves a new state-of-the-art zero-shot FID of 6.16\non MS-COCO 30K, and 0.69 overall score on the GenEval benchmark. We hope our\nfindings and results will encourage future efforts to further bridge the\nscaling gap between vision and language models.",
        "updated": "2024-10-17 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.13863v1"
    },
    {
        "title": "UniDrive: Towards Universal Driving Perception Across Camera Configurations",
        "authors": "Ye LiWenzhao ZhengXiaonan HuangKurt Keutzer",
        "links": "http://arxiv.org/abs/2410.13864v1",
        "entry_id": "http://arxiv.org/abs/2410.13864v1",
        "pdf_url": "http://arxiv.org/pdf/2410.13864v1",
        "summary": "Vision-centric autonomous driving has demonstrated excellent performance with\neconomical sensors. As the fundamental step, 3D perception aims to infer 3D\ninformation from 2D images based on 3D-2D projection. This makes driving\nperception models susceptible to sensor configuration (e.g., camera intrinsics\nand extrinsics) variations. However, generalizing across camera configurations\nis important for deploying autonomous driving models on different car models.\nIn this paper, we present UniDrive, a novel framework for vision-centric\nautonomous driving to achieve universal perception across camera\nconfigurations. We deploy a set of unified virtual cameras and propose a\nground-aware projection method to effectively transform the original images\ninto these unified virtual views. We further propose a virtual configuration\noptimization method by minimizing the expected projection error between\noriginal cameras and virtual cameras. The proposed virtual camera projection\ncan be applied to existing 3D perception methods as a plug-and-play module to\nmitigate the challenges posed by camera parameter variability, resulting in\nmore adaptable and reliable driving perception models. To evaluate the\neffectiveness of our framework, we collect a dataset on Carla by driving the\nsame routes while only modifying the camera configurations. Experimental\nresults demonstrate that our method trained on one specific camera\nconfiguration can generalize to varying configurations with minor performance\ndegradation.",
        "updated": "2024-10-17 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.13864v1"
    },
    {
        "title": "DepthSplat: Connecting Gaussian Splatting and Depth",
        "authors": "Haofei XuSongyou PengFangjinhua WangHermann BlumDaniel BarathAndreas GeigerMarc Pollefeys",
        "links": "http://arxiv.org/abs/2410.13862v1",
        "entry_id": "http://arxiv.org/abs/2410.13862v1",
        "pdf_url": "http://arxiv.org/pdf/2410.13862v1",
        "summary": "Gaussian splatting and single/multi-view depth estimation are typically\nstudied in isolation. In this paper, we present DepthSplat to connect Gaussian\nsplatting and depth estimation and study their interactions. More specifically,\nwe first contribute a robust multi-view depth model by leveraging pre-trained\nmonocular depth features, leading to high-quality feed-forward 3D Gaussian\nsplatting reconstructions. We also show that Gaussian splatting can serve as an\nunsupervised pre-training objective for learning powerful depth models from\nlarge-scale unlabelled datasets. We validate the synergy between Gaussian\nsplatting and depth estimation through extensive ablation and cross-task\ntransfer experiments. Our DepthSplat achieves state-of-the-art performance on\nScanNet, RealEstate10K and DL3DV datasets in terms of both depth estimation and\nnovel view synthesis, demonstrating the mutual benefits of connecting both\ntasks. Our code, models, and video results are available at\nhttps://haofeixu.github.io/depthsplat/.",
        "updated": "2024-10-17 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.13862v1"
    },
    {
        "title": "PUMA: Empowering Unified MLLM with Multi-granular Visual Generation",
        "authors": "Rongyao FangChengqi DuanKun WangHao LiHao TianXingyu ZengRui ZhaoJifeng DaiHongsheng LiXihui Liu",
        "links": "http://arxiv.org/abs/2410.13861v1",
        "entry_id": "http://arxiv.org/abs/2410.13861v1",
        "pdf_url": "http://arxiv.org/pdf/2410.13861v1",
        "summary": "Recent advancements in multimodal foundation models have yielded significant\nprogress in vision-language understanding. Initial attempts have also explored\nthe potential of multimodal large language models (MLLMs) for visual content\ngeneration. However, existing works have insufficiently addressed the varying\ngranularity demands of different image generation tasks within a unified MLLM\nparadigm - from the diversity required in text-to-image generation to the\nprecise controllability needed in image manipulation. In this work, we propose\nPUMA, emPowering Unified MLLM with Multi-grAnular visual generation. PUMA\nunifies multi-granular visual features as both inputs and outputs of MLLMs,\nelegantly addressing the different granularity requirements of various image\ngeneration tasks within a unified MLLM framework. Following multimodal\npretraining and task-specific instruction tuning, PUMA demonstrates proficiency\nin a wide range of multimodal tasks. This work represents a significant step\ntowards a truly unified MLLM capable of adapting to the granularity demands of\nvarious visual tasks. The code and model will be released in\nhttps://github.com/rongyaofang/PUMA.",
        "updated": "2024-10-17 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.13861v1"
    },
    {
        "title": "VLM-Grounder: A VLM Agent for Zero-Shot 3D Visual Grounding",
        "authors": "Runsen XuZhiwei HuangTai WangYilun ChenJiangmiao PangDahua Lin",
        "links": "http://arxiv.org/abs/2410.13860v1",
        "entry_id": "http://arxiv.org/abs/2410.13860v1",
        "pdf_url": "http://arxiv.org/pdf/2410.13860v1",
        "summary": "3D visual grounding is crucial for robots, requiring integration of natural\nlanguage and 3D scene understanding. Traditional methods depending on\nsupervised learning with 3D point clouds are limited by scarce datasets.\nRecently zero-shot methods leveraging LLMs have been proposed to address the\ndata issue. While effective, these methods only use object-centric information,\nlimiting their ability to handle complex queries. In this work, we present\nVLM-Grounder, a novel framework using vision-language models (VLMs) for\nzero-shot 3D visual grounding based solely on 2D images. VLM-Grounder\ndynamically stitches image sequences, employs a grounding and feedback scheme\nto find the target object, and uses a multi-view ensemble projection to\naccurately estimate 3D bounding boxes. Experiments on ScanRefer and Nr3D\ndatasets show VLM-Grounder outperforms previous zero-shot methods, achieving\n51.6% Acc@0.25 on ScanRefer and 48.0% Acc on Nr3D, without relying on 3D\ngeometry or object priors. Codes are available at\nhttps://github.com/OpenRobotLab/VLM-Grounder .",
        "updated": "2024-10-17 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.13860v1"
    }
]