Transformer Guided Coevolution: Improved Team Formation in
Multiagent Adversarial Games
PranavRajbhandari PrithvirajDasgupta DonaldSofge
CarnegieMellonUniversity NavalResearchLaboratory NavalResearchLaboratory
Pittsburgh,PA,UnitedStates Washington,D.C.,UnitedStates Washington,D.C.,UnitedStates
prajbhan@alumni.cmu.edu prithviraj.dasgupta.civ@us.navy.mil donald.a.sofge.civ@us.navy.mil
ABSTRACT Researchershaveaddressedtheteamselectionprobleminmulti-
Weconsidertheproblemofteamformationwithinmultiagentad- agentteamformationusingevolutionarycomputation-basedap-
versarialgames.WeproposeBERTeam,anovelalgorithmthatuses proaches[14,31],albeitfornon-adversarialsettingslikesearchand
atransformer-baseddeepneuralnetworkwithMaskedLanguage reconnaissance.Inthispaper,weconsidertheuseofatransformer
Modeltrainingtoselectthebestteamofplayersfromatrainedpop- basedneuralnetworktopredictthesetofagentswhichformateam.
ulation.Weintegratethiswithcoevolutionarydeepreinforcement WenamethistechniqueBERTeam,andinvestigateitssuitability
learning,whichtrainsadiversesetofindividualplayerstochoose forteamformationinmultiagentadversarialgames.
teamsfrom.Wetestouralgorithminthemultiagentadversarial Incontrasttoevolution-basedapproachesinliterature,ourtech-
gameMarineCapture-The-Flag,andwefindthatBERTeamlearns nique considers team selection as a token sequence generation
non-trivialteamcompositionsthatperformwellagainstunseenop- process.Wegenerateateambybeginningwithamaskedsequence
ponents.Forthisgame,wefindthatBERTeamoutperformsMCAA, ofmembers,anditerativelyqueryingthetransformertopredictthe
analgorithmthatsimilarlyoptimizesteamformation. nextmaskedagent’sidentity.Wecontinueuntilthespecifiedteam
sizeisreached.WetrainBERTeamonadatasetofwell-performing
KEYWORDS teams,sothetransformerwillattempttomatchthisdistribution.
AlongsidetrainingBERTeam,weevolveapopulationofagents
Multiagentreinforcementlearning,TeamFormation,Adversarial
usingCoevolutionaryDeepReinforcementLearning[12,24].This
games,Coevolution,Transformers,SequenceGeneration
methodutilizesself-play,samplinggamesbetweenteamsselected
ACMReferenceFormat: fromthepopulation.Thetrainingdatafromthesegamesupdatesa
PranavRajbhandari,PrithvirajDasgupta,andDonaldSofge.2025.Trans- setofReinforcementLearning(RL)algorithmsandguidesastan-
formerGuidedCoevolution:ImprovedTeamFormationinMultiagentAd- dardevolutionaryalgorithm.
versarialGames.InProc.ofthe24thInternationalConferenceonAutonomous Weempiricallyvalidateourproposedtechniquewiththe𝑘-v-𝑘
AgentsandMultiagentSystems(AAMAS2025),Detroit,Michigan,USA,May adversarial game Marine Capture-the-Flag (MCTF). Our results
19–23,2025,IFAAMAS,11pages. showthatBERTeamisaneffectivemethodforteamselectionin
thisdomain.WefoundthatBERTeamisabletolearnanon-trivial
1 INTRODUCTION distribution,favoringwellperformingteams.Wealsofindthatfor
MCTF,BERTeamoutperformsMultiagentCoevolutionforAsym-
Weinspectmultiagentadversarialgames,characterizedbyanenvi-
metricAgents(MCAA),anotherteamformationalgorithm.
ronmentwithmultipleteamsofagents,eachworkingtoachievea
teamgoal.Theirperformanceisevaluatedbyanoutcome,areal
numberassignedtoeachteamattheendofagame(episode). 2 RELATEDWORKS
Variouscomplexteamgamescanbeformulatedasamultiagent Self-Play:Self-playisacentralconceptfortrainingautonomous
adversarialgame,includingpursuit-evasiongames[10,18,40,49], agentsforadversarialgames.Themainideaofself-playistokeep
roboticfootball[17,23,41],androboticcapture-the-flag[32].The trackofasetofpoliciestoplayagainstduringtraining[20].These
problemofcreatingacooperativeteamofrobotsisusefulforsolving policiesareusuallycurrentorpastversionsofagentsbeingtrained.
thesegames,aswellasapplicationssuchassearchandrescue. Extendingtheconceptofself-play,Alpha-Star[21]utilizedleague
A crucial problem in adversarial multiagent team formation play,atechniquewhereadiversesetofagentswithdifferentper-
is the selection of teams. Given a set of agents and potentially formancelevelsplayinatournamentstylegamestructure.League
informationabouttheenvironment,ateammustbeselectedto playimprovedtheadaptabilityoftrainedagentstoplayagainst
performbestagainstopponents.Thisproblemisdifficultsincea differentopponentdifficultiesintheStarcraft-IIreal-timestrategy
goodteamformationalgorithmmustconsiderbothintra-teamand videogame.Similartotheleague-playconcept,inourproposed
inter-teaminteractionstoselectanoptimalteam.Additionally,the technique,weusecoevolutionofagentstoimprovetheiradaptabil-
setofagentsmustoftenlearntheirindividualpolicies,increasing ityagainstvaryingopponentstrategiesanddifficultylevels.
thecomplexityoftheproblem. TeamFormationinAdversarialGames:In[27],researchers
proposedTeam-PSRO(PolicySpaceResponseOracle),atechnique
Proc.ofthe24thInternationalConferenceonAutonomousAgentsandMultiagentSystems withinaframeworkcalledTMECor(TeamMax-minEquilibrium
(AAMAS2025),A.ElFallahSeghrouchni,Y.Vorobeychik,S.Das,A.Nowe(eds.),May19 withCorrelationdevice),formulti-playeradversarialgames.In
–23,2025,Detroit,Michigan,USA.©2025InternationalFoundationforAutonomous
Team-PSRO,agentsimprovetheirpoliciesiterativelythroughre-
AgentsandMultiagentSystems(www.ifaamas.org).Thisworkislicencedunderthe
CreativeCommonsAttribution4.0International(CC-BY4.0)licence. peatedgame-play.Ineachiteration,thepolicyforeachagentina
4202
tcO
71
]IA.sc[
1v96731.0142:viXrateamisselectedasacomponentofabest-responsepolicyforthe Transformers:Transformersareasequence-to-sequencedeep
entireteam,calculatedbyabestresponseoracle.Inourtechnique, neuralnetworkarchitecturedesignedtocreatecontext-dependent
theroleofthisoracleisperformedbyBERTeam’steamselection. embeddingsofinputtokensinasequence[45].Theyuseanencoder-
Anotherapproachforlearningtoplayadversarialteamgames decoderarchitecture[1,11,42],takingasinputtwosequences.The
trainsagentsincrementallyviacurriculumlearning,thenadapts outputisanembeddedsequencecorrespondingtoeachelementof
thoselearnedstrategiesforadversarialsettingsviaself-play[25]. oneinitialsequence.Afinallayercanbeaddedconvertingeachele-
Strategiesarestochasticallymodifiedtointroducediversityandto mentintoaprobabilitydistributionovertokensinavocabularyand
increaseadaptabilityagainstunseenopponentstrategies.Whilethis allowingforsequencegeneration.Thisarchitectureiswidelyused
techniqueadaptspreviouslytrainedpoliciesforadversarialplay,our inNaturalLanguageProcessing(NLP),intaskssuchasgeneration,
approachcreatesapopulationofpoliciestrainedinanadversarial classification,andtranslation[8,9,22,26].
environment,thenselectsthebestteamfromthispopulation.The BidirectionalEncoderRepresentationsfromTransformers(BERT)
largenumberofpotentialteamstoselectfromallowsustoadapt [13]isanupdatetotheoriginaltransformer‘nexttokenprediction’
ourteamselectiontoneweropponentstrategies. trainingstructure.BERTisinsteadtrainedwithMaskedLanguage
EvolutionaryAlgorithmsforMultiagentGames:Evolution- Modeling(MLM),inspiredbytheClozetask[43].Thisforcesitto
aryalgorithmshavebeenusedfordecadesformultiagentgames predictrandomlymaskedtokensgivenbidirectionalcontext,and
duetotheiradaptabilityandperformanceindomainslikesoccer thusimprovesrobustnessinthemodel.Weuseasimilarapproach,
[48].Coevolutioninparticular[46,47]hastheadvantageofevolv- associatingasetofagentstotokensandusinganMLMtraining
ingindependentpopulationsofagentsforspecializedskills(e.g. schemetoproducesequencesofagentstoformstrongteams.
defending,passing,shooting).However,adownsidetothisisthat
thecorrectspecializedagentsmustbechosenforeachenvironment. 3 TEAMSELECTIONINADVERSARIAL
Toaddressthis,authorsin[49]proposedatechniquetouseanas- GAMES
sessortochooseteamcompositionsintelligently.Theassessorwas
Preliminaries:AMarkovDecisionProcess(MDP)isaframework
amodeltrainedtopredicttheoutcomeofgamesbetweenknownop-
capturingabroadrangeofoptimizationtasks.AnMDPisdescribed
ponents.Thisallowedthemtoevolvespecializedagentswhilealso byatuple(𝑆,𝐴,T,R,𝛾),containingastatespace𝑆,anactionspace
beingabletochooseoptimalteamsagainstknownopponents.How- 𝐴,atransitionfunctionT(𝑆 | 𝑆 ×𝐴),arewardfunctionR:𝑆 ×
ever,toselecttheoptimalteamagainstaknownopponentteam, 𝐴×𝑆 →R,andadiscountfactor𝛾 ∈ [0,1).Anagentisaplayer
theymustsearchthespaceofallpossibleteams,queryingtheir inanMDPandisdescribedbyitspolicy𝜋(𝐴 |𝑆).Thesequence
modeleachtime.Toselectagoodteamagainstapartiallyknown (𝑠 0,𝑎 0,𝑟 0),(𝑠 1,𝑎 1,𝑟 1),... isreferredtoasatrajectory,where𝑎 𝑖 is
opponentteam,theyrepeatedlyupdatethecurrentandopponent sampledfrom𝜋(𝑎 |𝑠 𝑖−1),𝑠 𝑖issampledfromthetransitionfunction
teams,continuinguntilconvergence.Thus,thiscausesdifficultyin T(𝑠 | 𝑠 𝑖−1,𝑎 𝑖−1),and𝑟 𝑡 = R(𝑠 𝑡,𝑎 𝑡,𝑠 𝑡+1).Anagent’sobjectiveis
scalabilitytolargerpopulations,andreducesadaptabilityincases optimizingthesumofdiscountedrewardsinatrajectory:E[(cid:205)𝛾𝑖𝑟 𝑖].
whereopponentpoliciesarenotknownapriori. 𝑖
Intheirresearch,Dixitetal.achieveasimilargoalofdiversifying ReinforcementLearning(RL)isonemethodofoptimizingapol-
individualpoliciesandselectinganoptimalteamcompositionfrom icyforanMDP.Whiletherearevariousdifferenttechniquesfor
thesediverseagents[14].Todiversifyagents,theyusedQualityDi- different classes of MDPs, most of them keep track of a policy
versity(QD)methodslikeMAP-Elites,anevolutionaryalgorithm
𝜋 𝜃(𝐴 |𝑆)andarewardestimator𝑟
𝜙
:𝑆 →R.Aftersamplingan
that ensures the population evolved has sufficiently diverse be-
episode,therewardparameters𝜙areupdatedtowardstheobserved
havior [31]. MAP-Elites works by projecting each policy into a discountedrewardsfromeachstate.Thepolicyparameters𝜃 are
lowdimensionalbehaviorspace,thenconsideringonlyfitnessesof updatedusingtherewardestimatortooptimizetheexpecteddis-
agentsthatbehavesimilarlywhenupdatingthepopulation. countedrewardsfromeachstate.DeepRLutilizesDeepNeural
DixitusedQDonafewindependentislandsofRLagents.To Networkstocreatepolicyandrewardnetworks𝜋 𝜃,𝑟 𝜃.
selectoptimalteamcompositions,theirmainlandalgorithmkeeps We are concerned with multiagent 𝑘-v-𝑘 adversarial games.
trackofadistributionoftheproportionofmembersfromeach Giventhepoliciesofallagentsplaying,thisscenariocanbeformally
islandthatcomposeanoptimalteam.Totrainthisdistribution,they definedasaninstanceofanMDPforeachagent,withadistinct
repeatedlyevaluateagentsinteamgames,thenusetheoutcomesto rewardfunctionforeachagent.Ateachtimestep,theactionsof
ranktheteams.Theyusethecompositionsofthebestfewteamsto allotheragentsareconsideredinthetransitionfunctionT.We
updatethedistribution,andtheindividualfitnessesandRLtraining dividetheagentsintoteamsandadditionallydefineanoutcome
examplestoupdatetheindividualpoliciesoneachisland. evaluationthatconsidersthetrajectoriesofagameandreturnsa
MCAAisdesignedandevaluatedforcooperativetaskslikevis-
setofteamsthat‘won’.1WeassumetheMDPrewardsofanagent
iting a set of targets with robots that have different navigation correlatewithitsteam’soutcome,soagentsthatgethighrewards
capabilities(e.g.dronesandrovers).SimilartoMCAA,ourpro- inagamearelikelytobeonwinningteams.Weusethisframework
posedalgorithmusestwoseparatecomponentsforevolvingagent asopposeddec-MDPs,aformalizationusedinteamreach-avoid
skillsandevaluatingagentperformance.However,wedivergefrom games[5,10,18,39]sincewewouldlikeeachagenttohavetheir
MCAAasweconsideradversarialteamsandcaseswheretheagents ownrewardstructure,andfortheserewardstobeseparatefromthe
areuniformandonlydifferentiatedbythebehavioroftheirlearned gameoutcomes.Withinourframework,theproblemweconsider
policies.Wecomparethemaintechniquesofourproposedalgo-
1Anaturalextensionofthisistohavetheoutcomebearealnumberforeachteam.
rithmwithanalogouscomponentsofMCAA. WediscussawaytopotentiallyhandlethisinAppendixD.1ishowtobestcreateateamofagentswhosepoliciesarelikelyto
winagainstavarietyofopponents.
Thisproblemisdifficulttosolveduetothecooperationrequired
ofpolicieswithinateamandthevarietyofpotentialopponent
policies.Therearetwogeneraldirectionstoaddressthisproblem.
Thefirstmethodmaintainsafixednumberofagentsperteamcorre-
spondingtotheteamsize;eachagent’spolicyistrainediteratively
viatechniqueslikeself-play[25,27].However,maintainingafixed
setofagentpolicieslimitstheadaptabilityofeachagentaswellas
thediversityoftheteam.Itmightbedifficulttoquicklyformateam
thatcanplaysuccessfullyagainstanopponentstrategythatmay
havebeen‘forgotten’duringtraining.Thesecondapproach[21], Figure1:BERTeam’score,atransformernetwork
whichweadoptinthisresearch,istomaintainalargersetofagent
policiesandselectafewagentpoliciestoformateam.Thiscreates
vastdiversityinpossibleteamswiththedownsideofintroducing ofvectorsonwhichtoconditionBERTeam’soutput.4 Sincethe
theadditionaloverheadofteamselection.Thisisanon-trivialprob- architectureofthisinputembeddingdependsontheformofthe
lemastheteamselectionmustselectpoliciesthatcooperatewell, observations,thismodelmustbetailoredtoeachusecase.Asingle
whileremainingcognizantoftheopponent’spossiblestrategies.At queryofthemodelwilltakeasinputapartiallymaskedteamand
thesametime,policiesmustbecontinuouslyimprovedviaself-play anyenvironmentobservations.Theoutputwillbeapredicteddistri-
learningtoremaincompetitiveagainstneweropponentstrategies. butionoveragentsforeachelementofthesequence,asillustrated
Ourproposedtechniqueistousecoevolutiontotrainalarge inFigure1.
setorpopulationofagentpolicies,anduseatransformer-based Duringevaluation,themodelwillbeusedasagenerativepre-
sequencegenerationtechniquetoselectthebestteamsfromthe trainedmodel(asin[45])tosampleateam.Forthegeneralcase,
population.ThesetwotechniquesareillustratedinFigure2and itcouldbeusefultoeitherincludetheteamindexasinputforthe
describedinmoredetailinthefollowingsections. inputembedding,ormakeseparateinstancesofBERTeamforeach
team.However,thereisoftensymmetrybetweenteams,sothe
3.1 TransformerbasedSequenceCompletion sameinstanceofBERTeamcanbeused.
forTeamSelection 3.1.2 TrainingProcedure. TheBERTeammodelusesMLMtraining,
WhyTransformers? Transformersaregenerativemodelsthatcanbe whichrequiresadatasetof‘correctsequences’.Themodelistrained
usedtoquery‘nexttoken’conditionaldistributions.Asize𝑘team tocompletemaskedsequencestomatchthisdistribution.
canbegeneratedin𝑘queries,givingusefficientuseofthemodel. Togeneratethisdataset,weconsidertheoutcomesofgames
Theformoftheoutputalsoallowsusmakespecializedqueriesfor playedbetweenvariousteams.Sinceourgoalisforthemodelto
caseswherevalidteamsmayhavespecificconstraints.2Transform- predictgoodteams,wefillthedatasetwithonlyteamsthatwin
ersarealsoabletoconditiontheiroutputoninputembeddings, games,alongwiththeobservationsoftheirplayers.Wealsoinclude
allowingustoformteamsdependentonobservations(ofanyform) examplesofwinningteamswithoutanyobservationstotrainthe
oftheopponent.Wemayalsopassinanemptyinputsequencefor modeltogenerateunconditionedoutput.
unconditionalteamformation.Weexpectthatthroughtheirinitial Sinceitisusuallyinfeasibletosampleallpossibleteampairs
embeddings,transformerswillencodesimilaritiesbetweenagents uniformly,wemustdecidewhichgamestosampletoefficiently
andallowthemodeltoinfermissingdata.Thisissupportedbythe createadataset.WedothisbyusingBERTeam’ssamplingtocreate
behaviorofwordembeddingsinthedomainofNLP[28]. adatasetofteamsthatwinagainstBERTeam’scurrentdistribution
ThemainissuewithtransformersistheirΘ(𝑘2)complexityfor ofopponents.Themotivationofthisisthatisisnaturaltofavorgen-
asequenceofsize𝑘.However,inpractice,sequencelengthsofup eratingteamsthatwinagainstpowerfulopponents,asopposedto
to512areeasilycalculated[13].Thisindicatesthatouralgorithm teamsthatwinagainstpoorlycoordinatedopponents.Additionally,
canscaleuptothisteamsize.3 Evenforteamsizesbeyondthis undercertainassumptionsanddatasetweights,BERTeamimitating
limit,thereexistworkaroundslikesliding-windowattention[3]. adatasetformedinthiswaywillconvergetoaNashEquilibrium.
WeproposeBERTeam,amethodforselectingateamofagents WedescribethisinAppendixD.1,butdonotimplementthisinour
inamulti-agentadversarialgameutilizingatransformermodel. experiment,asitmaycausepoorlybehavedlearning.Thisappendix
alsodescribesastrategyforgameswithgeneraloutcomes,where
3.1.1 ModelArchitecture. ThecoreofBERTeamisatransformer cannotnecessarilydistinguisha‘winning’team.
modelwhosetokensrepresenteachpossibleagentinthepopu- Thus,asinFigure2,wewilltrainBERTeamalongsidegenerating
lation,alongwitha[MASK]token.Aseparateinputembedding itsdataset,andutilizethepartiallytrainedmodeltosamplegames.
modeltransformsobservationsofanyformintoashortsequence Ourdatasetwilltaketheformofareplaybuffersothatoutdated
examples are eventually replaced by newer ones. One concern
is that filling the dataset with teams generated from BERTeam
2Anexampleofthiswouldbeateamthatmustbepartitionedbyagenttypes.Inthis
case,wewouldusetheoutputtosampleeachmemberfromasetofvalidchoices. 4WhilethisisacapabilityofBERTeam,wechoosetoanalyzeBERTeamasanuncondi-
3Thiswouldcomewithanincreaseintrainingdatarequiredforsensibleoutput. tionalteamgenerator.Weplantoexploreinputembeddingsinfuturework.mayresultinstagnation,asthedatasetwillmatchtrendsinthe Algorithm1:Algorithmfortrainingapopulationofagents
distribution.Tomitigatethis,weuseinverseprobabilityweighting viacoevolutionself-play
[19],weightingraresamplesmore.WediscussthisinAppendixD. input :𝑃𝑜𝑝:agentpopulation
𝑘:sizeofeachteam
output:𝑃𝑜𝑝:updatedagentpopulationviacoevolution
1
Proceduretrain-pop-coevolution(𝑃𝑜𝑝,𝑘)
2 𝑓 𝑖 ←1000 ∀𝑖 ∈𝑃𝑜𝑝
3 for1...𝑛 𝑒𝑝𝑜𝑐ℎ𝑠 do
4
T𝑖 ←∅ ∀𝑖 ∈𝑃𝑜𝑝
5 for1,...,𝑛 𝑔𝑎𝑚𝑒𝑠 do
6
(𝑇,𝑇′)←sample𝑘agentsperteamfrom𝑃𝑜𝑝
7
𝑐𝑎𝑝,𝑐𝑎𝑝′ ←selectcaptainsforteams𝑇,𝑇′
8
𝑔←gameplayedbetweenteams𝑇,𝑇′
9
𝑂 ←outcomesforteams𝑇,𝑇′fromgame𝑔
10
T𝑖 ←T𝑖∪trajectoriesfrom𝑔foragent𝑖,
∀𝑖 ∈{𝑇 ∪𝑇′}
Figure2:TrainingofBERTeamalongsidecoevolutionaryRL
11
𝑓 𝑐𝑎𝑝,𝑓
𝑐𝑎𝑝′
←updatefitness(𝑐𝑎𝑝,𝑐𝑎𝑝′,𝑂)
12 end
3.1.3 TrainingalongwithCoevolution. TheBERTeammodelisable
tobetrainedalongsidecoevolution,asitspastknowledgeofthe 13 Update𝜋 𝑖 usingRLalgorithmonexperienceinT𝑖
agentpoliciescanbeutilizedandupdatedwithrecentinformation. 14 P𝑐𝑙𝑜𝑛𝑒 ←Clone𝑛 𝑟𝑒𝑚 agentsfrom𝑃𝑜𝑝selected
usingEqn.1
The outcomes of games sampled in coevolution can be used in
thedatasetofBERTeam,andtheBERTeampartiallytrainedmodel 15 P𝑟𝑒𝑚 ←Select𝑛 𝑟𝑒𝑚 agentsfrom𝑃𝑜𝑝usingEqn.2
canbeusedtosamplebetterteams,acycledisplayedinFigure2. 16
𝑃𝑜𝑝 ←{𝑃𝑜𝑝\P𝑟𝑒𝑚}∪P𝑐𝑙𝑜𝑛𝑒
WhilewedetailthecoevolutionalgorithminSection3.2,themain 17 end
thingwemustdefineusingBERTeamisanindividualagentfitness 18 return𝑃𝑜𝑝
function.Thisistricky,aswehaveonlyassumedtheexistenceofa
comparativeteamoutcome.WesolvethisbyutilizingElo,amethod
ofassigningeachplayeravaluefromtheresultsofpairwise1v1
ThepartsthatdeviatefromthecoevolutionaryRLalgorithm
games[15].Givenateamselector(i.e.BERTeam)thatcansample
in[12]arethefitnessupdates(lines7and11)andthegeneration
fromthesetofallpossibleteamscontainingsomespecifiedagent
update(lines14-16).Thedeviationinfitnessupdatesisaresultof
(thecaptain),wedefinethefitnessofeachagentastheexpectedElo
onlyassumingtheexistenceofateamevaluationfunction,andis
ofateamchosenwiththatagentascaptain.Toupdatethesevalues,
discussedinSection3.1.3.Thedeviationinpopulationupdatesis
wesampleteamstoplaytraininggames,choosingourcaptains
duetoconsiderationswithtrainingalongsideBERTeam.BERTeam
byaddingnoisetoBERTeam’sinitialdistribution.Weupdatethe
assumessimilaritiesinbehavioroftheagentassignedtoeachtoken,
fitnessesofeachteamcaptainwithastandardEloupdate(Appendix
andifwereplaceorrearrangeeverymemberofthepopulation,
A).Sinceitisconfusingtodistinguishtheseindividualfitnesses
thetrainingoftheBERTeammodelwouldberendereduseless.By
fromteamElos,wewillrefertothemasfitnessvaluesfromnow
controllingreplacementrate,weensuremostoftheinformation
on.
learnedbyBERTeamretainsrelevance.Todothiswhilebestim-
3.2 CoevolutionForTrainingAgentPolicies itating[12],westochasticallychoose𝑛 𝑟𝑒𝑚 agentstoreplaceand
𝑛 𝑟𝑒𝑚 toclone5usingthefollowingequations:
Themainideaincoevolutionisthatinsteadofoptimizingasingle
team,apopulationofagentslearnsstrategiesplayingingames
betweensampledteams.WechoosetouseCoevolutionaryDeepRL P{cloneagent𝑖}=
exp(𝑓 𝑖)
(1)
sinceitallowsagentstobetrainedagainstavarietyofopponent (cid:205) exp(𝑓 𝑗)
𝑗∈𝑃𝑜𝑝
policies,addressingperformanceagainstanunseenopponent.
Thus,weuseAlgorithm1,heavilyinspiredby[12],toproducea P{removeagent𝑖}=
exp(−𝑓 𝑖)
(2)
populationoftrainedagents.Theinputisaninitializedpopulation (cid:205) exp(−𝑓 𝑗)
𝑗∈𝑃𝑜𝑝
ofagents,aswellasparametersliketeamsize.Ineachepoch,we
sample𝑛 𝑔𝑎𝑚𝑒𝑠 games,whicheachconsistofselectingteamsand 4 EXPERIMENTS
captainsusingateamselector(lines6,7),playingthegame(line8),
andcollectingtrajectoriesandoutcomes(lines9,10).Theoutcomes Tobetteranalyzetheeffectivenessofthisalgorithm,weconsider
aresenttotheteamselectorfortrainingandalsousedtoupdate 2v2 team games. This small team size allows us to more easily
individualagentfitnessesinline11(seeFigure1).Attheendof analyzethetotaldistributionlearnedbyBERTeam.
eachepoch,thetrajectoriescollectedupdateeachagentpolicyin
placeinline13,andthepopulationisupdatedinlines14-16. 5Anagentmightbeselectedforreplacementandcloning,resultinginnochange.4.1 Aquaticus 4.3 TeamSelectionwithCoevolutionofAgents
WetrainBERTeamalongsideAlgorithm1,asillustratedinFigure
2.6Foreachindividualagent,weuseProximalPolicyOptimization
(PPO),anon-policyRLalgorithm[38].Wedetailexperimentpa-
rametersinAppendixE.TofindtheElosofeachpossibleteam,we
utilizetheEloscalculatedforthefixedpolicyteamsasabaseline.
Foreachofthe1275possibleteams,7wetestagainstallpossible
teamsoffixedpolicyagents.Wethenusetheresultsofthesegames
tocalculatethetrueElosofourteamsoftrainedagents.Wedonot
updatetheElosofourfixedpolicyagentsduringthiscalculation.
(a)MOOS-IvPEnvironment (b)Pyquaticusenvironment
Forpolicyoptimization,weusestable_baselines3[33],astan-
dardRLlibrary.Sincethislibraryissingle-agent,wecreateunsta-
Figure3:Aquaticusgame,anditssimulatedversion ble_baselines3,8awrapperallowingasetofindependentlearning
algorithmstotraininaPettingZoomultiagentenvironment.
AquaticusisaCapture-the-Flagcompetitionplayedwithteams 4.3.1 Aggression Metric. The BERTeam model is difficult to in-
ofautonomousboats[32].Weareinterestedinthistaskbecauseit terpretinthecaseoflearnedpolicies,astheteamcompositionis
isanexampleofateam-basedmultiagentadversarialgame.Each unclear.Thus,wecreateanaggressionmetrictoestimatethebehav-
agenthaslow-levelmotorcontrol,andthusanycomplexbehaviors
iorofeachagentinagame.Specifically,thismetricforagent𝑎is
mustbelearnedthroughamethodlikeRL.Additionally,thereare
1+2(#𝑎capturesflag)+1.5(#𝑎grabsflag)+(#𝑎istagged)
,where (#𝑎event)
1+(#𝑎tagsopponent)
variousstrategies(e.g.offensive/defensive)thatagentsmayadopt denotesthenumberoftimesthateventhappenedtoagent𝑎inthe
thatperformwellincompetition.Finally,webelievethatoptimal game.Weevaluatetheaggressivenessofeachtrainedagentbycon-
teamcompositioninthisgameisnon-trivial,andexpectthatagood sideringagamewhereoneteamiscomposedofonlythatagent.We
teamiscomposedofabalancedsetofstrategies. evaluatetheaverageaggressionmetricagainstallpossibleteamsof
fixedpolicyagents.Weexpectaggressiveagentstohaveametric
4.1.1 Pyquaticus. Duetothedifficultyoftestingonrealrobotic largerthan1,anddefensiveagentstohaveametriclessthan1.9
platforms,wetestandevaluateourmethodsonPyquaticus,asim-
ulatedversionofAquaticus[2].Theplatformisimplementedasa
4.4 ComparisonwithMCAA
PettingZooenvironment[44],thestandardmultiagentextensionof
OpenAIGymnasium[6].Inexperiments,weusetheMDPstructure WenoticethattheMCAAmainlandteamselectionalgorithmis
implementedinPyquaticus.Weterminateanepisodewhenateam playingananalogousroletoBERTeamteamformation,andthatthe
capturesaflag,oraftersevenin-gameminutes. MAP-Elitespolicyoptimizationoneachislandisanalogoustoour
coevolutionaryRLalgorithm.SincebothouralgorithmandMCAA
distinguishteamformationandindividualpolicyoptimizationas
4.2 TeamSelectionwithFixedPolicyAgents
separatealgorithms,wemayhybridizethemethodsandcompare
TotesttheeffectivenessofBERTeamindependentofcoevolution,
theresultsoffouralgorithms.Thealgorithmsaredistinguishedby
weusefixedpolicyagentspredefinedinPyquaticus:arandomagent,
choosingMAP-ElitesorCoevolutionaryDeepRLforpolicyopti-
threedefendingagents,andthreeattackingagents.Theattacking
mization,andBERTeamorMCAAforteamselection.Thehybrid
anddefendingagentseachcontainaneasy,medium,andhardpolicy.
trialswillallowustoindividuallyevaluateBERTeamasateam
Theeasypoliciesmoveinafixedpath,andthemediumpolicies
selectionmethod,independentofthepolicyoptimization.
utilizepotentialfieldcontrollerstoeitherattacktheopponentflag
WemustmakesomechangestobeabletoimplementMCAAand
whileavoidingopponents,orcaptureopponentsbyfollowingthem.
MAP-Elitesinanadversarialscenario.First,astepoftheMCAAal-
Thehardpoliciesarethemediumpolicieswithfastermovement
gorithmranksasetofgeneratedteamswithateamfitnessfunction.
speed.WefollowthetrainingalgorithminFigure2withoutthe
Toapproximatethisinanadversarialenvironment,weplayasetof
coevolutionupdate.Weconductanexperimentwiththe7agents,
gameseachepoch,andconsidertheteamsthatwonas‘topranked’,
anddetailexperimentparametersinAppendixE.
andincludethemintheMCAAtrainingdata.Similarly,MCAA
Throughouttraining,werecordtheoccurrenceprobabilityof
assumesanevaluationfunctionforfitnessofindividualagents.We
all possible teams using BERTeam. We do this exhaustively, by
approximatethisbyconsideringtheteamseachindividualhasbeen
consideringallpossiblesequencesdrawnfromthesetofagentswith
includedin.Wetakeamovingaverageofthecomparativeteam
replacement.WeexpectthatasBERTeamtrains,thiswillgradually
evaluations,andassignthisaveragetotheselectedagent.
approachadistributionthatfavorswellperformingteams.
ForMAP-Elites,weadaptouraggressionmetricasabehavior
projection.Whilethiscouldbemademorecomplex,webelievethis
4.2.1 EloCalculation. Sincewehaveafixedsetofpolicies,wecan
issufficient,asourworkisfocusedontheteamselectionaspect.
computetrueElosofall28unordered2agentteams(seeAppendix
B),obtainedfromanexhaustivetournamentofallpossibleteam
pairings.Fortheseresults,weperform10experimentsforeach
6Ourimplementation,alongwithexperiments,isavailableat[34,35]
7Weuseapopulationof50agentsandateamsizeof2,seeAppendixB.
choiceoftwoteams.WeusethescalingofstandardchessElo[15], 8Codeavailableat[36]
andshiftallElossothatthemeanis1000. 9Wetunedourmetrictodistinguishourfixedpolicyagents.Additionally,weadapttheMAP-Elitesalgorithmtoremove𝑛
𝑟𝑒𝑚
poorlyperformingagentsfromthepopulation:
• Projecteachpolicyintoalowdimensionalbehaviorspace.
Letthebehaviorvectorofagent𝑎be𝐵 𝑎.
• Protect‘unique’agentsfromdeletion.Givenaneighborhood
size𝜆,weconsideran𝑎uniqueif𝐵 𝑎is𝜆-farfromanyother
𝐵 𝑎′.Wemayincrease𝜆iftoomanyagentsareunique.
• Obtainfitnessscores𝑓 𝑎 foreachagent,andfitnesspredic-
tions𝑓 𝑎′usingeachagentsneighborhoodaverage.
• Delete𝑛 𝑟𝑒𝑚 agentsstochasticallyusingEquation2onrela- (a)Totaldistribution (b)Captaindistribution
tivefitness𝑓 𝑎−𝑓 𝑎′.
WedothistokeepthespiritofMAP-Eliteswhileallowingitto Figure4:BERTeamdistributionsthroughouttraining,sorted
maintainafixedpopulationsize,whichisnecessaryforthehybrid byprobability(largestonbottom)
trial with BERTeam. The original MAP-Elites algorithm can be
recoveredbyremovingstochasticityandrepeatedlyrunningour teamscontainingitbecameheavilyfavoredinthefirstfewepochs.
versionwithfixed𝜆untileveryagentisunique. Initially,itseemsBERTeamfavored(2,2),theteamcomposedof
AnotherconsiderationisthatintheMCAApaper,islandswere onlyhardattackingagents.However,aroundepoch500,thedis-
distinguishedbyhavingdifferentproportionsofvarioustypesof tributionshiftedandteam(2,2)sharplydecreasedinoccurrence
robots(i.e.dronesandrovers).Inourcase,wecannotdothisasall probability,infavorofteam(2,5),thestrongbalancedteam.After
agentsarehomogeneous.Toimitatethisvariation,weimplement this,therewerenomajorchangesinBERTeam’soutputdistribution.
adifferentRLalgorithmoneachisland,varyingthealgorithmtype
aswellasthenetworksizeused.Wedescribethesealgorithms, True True Predicted BERTeam
Team
alongwithotherexperimentparametersinAppendixE. Rank Elo Rank Occurrence
Onceallfouralgorithmshavebeentrained,wefixthelearned (2,5) 1 1388 1 0.14
agentpoliciesandteamselectionpolicies.WedefinetheEloofan (2,2) 2 1337 2 0.13
algorithmastheexpectedEloofateamsampledfromthealgo- (2,3) 3 1135 7 0.06
rithm’steamselector.Weevaluatetherelativeperformanceoftwo (1,2) 4 1112 4 0.10
algorithmsbysamplingteamsandevaluatingthegamesplayed. (0,2) 5 1097 3 0.10
Wesample10000gamesforeachofthe6algorithmcomparisons. (2,4) 6 1087 5 0.10
WeadditionallygroundourEloestimatesbysampling1000games (2,6) 7 1035 6 0.07
againstourfixedpolicyteams.WhendoingElocalculations,wedo
(0,5) 8 975 13 0.03
notupdatethefixedpolicyteams.Wesamplesinceforapopulation
Table1:Comparisonoftrueranksandpredictedranks
sizeof𝑛,thereareΘ(𝑛2)possibleteams,resultinginaninfeasible
Θ(𝑛4)possiblepairingsbetweenalgorithms.
ToinspecttheperformanceofBERTeam’sfavoredteamcompo-
5 RESULTS
sitions,weconsiderthetrueElosofeachteam.InTable1,welist
InMCTF,reorderingthemembersofateamhasnoeffectonthe theeightbestperformingteamsandtheirtrueElosalongsidethe
team’sperformance.However,BERTeamisasequencegeneration rankingsandoccurrenceprobabilitiesfromBERTeam.Wefindthat
model,soitdoesdistinguishorder.Tomakeresultsmorereadable, thetoptwochoicesmadebyBERTeamarecorrect,beingthebal-
we assume any reordering of a given team is equivalent to the anced(2,5),andtheaggressive(2,2)respectively.Theiroccurrence
originalteam,andcalculatedistributionsandElosaccordingly.A probabilities(14%,13%)arealsoreasonablylargerthanthethird
caveattothisisthatteamswithtwodistinctagentsarecounted rankat10%.BERTeamdoescorrectlyselectthenextfive,though
twiceinatotaldistribution,whileteamswithtwocopiesofone theorderisshuffled.Withtheexceptionofteam(2,3),theyallare
agentarecountedonce.Tocompensateforthisduringanalysis, reasonablyclosetotheircorrectpositions.
wedoublethedistributionvalueofthesecondtypeofteam,and InFigure4(b),weweconsiderthedistributionofteamcaptains
normalize.ThisisrelevantmainlyinFigure4(a)andTable1,where chosenbyBERTeam.Thisistheexpectedoutputdistributiongiven
weinspectthetotaldistributionofasmallnumberofteams.For acompletelymaskedsequence.Wefindthatagent2isstrongly
theanalysisofcaseswithmanyagents,thiseffectisnegligible. favoredthroughouttraining,indicatingthatitisalikelychoice
in a top performing team. This can be related to NLP, with an
5.1 TeamSelectionwithFixedPolicyAgents
analogousproblemofgeneratingthefirstwordofanunknown
Throughouttraining,weinspectthetotaldistributionofteams sentence.Justasarticlesandprepositions(e.g.‘The’)arestrong
learnedbyBERTeam.FromFigure4(a),wenoticethatourproposed choicesforthistask,BERTeambelievesagent2isastrongchoice
trainingalgorithmcertainlyseemstolearnsomethingnon-uniform. toleadateam.ThisissupportedbythetopseventeamsinTable1
Thetopsevenoutof21possibleteamcompositionsaccountfor beingallteamscontainingagent2.Thus,justasinNLP,BERTeamis
about75%ofthetotaldistribution.ItseemslikeBERTeamimme- abletoaccuratelydetermineagentslikelytobeinwell-performing
diatelyfavoredteamscontainingthehardattackingagent,asthe teams,andchoosethemasteamcaptains.TheseresultsindicateBERTeamisabletolearnanon-trivialteam 5.2.1 Team Elos. We calculate the true Elos for all 1275 teams,
composition,asitpredictedtopteamswithreasonableaccuracy. usingthefixedpolicyagentteamsasabaseline.Weplotthesein
Figure6,alongwithBERTeam’soccurrenceprobability.Weparti-
5.2 TeamSelectionwithCoevolutionofAgents tionallteamsinto‘Defensive’,‘Balanced’,and‘Aggressive’based
onwhethertheyhavezero,one,ortwoaggressiveagents.Wealso
conductalinearregressiononall1275teamsandplottheline.
WefindthatthereisacorrelationwiththetrueEloofateam
and BERTeam’s probability of outputting that team. Our linear
regressionhadacorrelationcoefficient𝑅2 ≈ .25,implyingthat
about25%ofthevarianceinBERTeam’soutputisexplainedbythe
performanceoftheteam.Thisindicatesthedistributionlearnedby
BERTeam,whilenoisy,favorsteamsthatperformwell.
WefindthatthetrueEloofthebestperformingteamisaround
1017,indicatingaperformanceslightlybetterthananaveragefixed
policyteam.Thisspecificteam(composedoftwodistinctaggressive
(a)Clusteringbasedonaggression (b)BERTeamteamcomposition(ordered) agents)isalsothemostprobableoutputofBERTeam.
Thus,wefindthatBERTeam,trainedalongsidecoevolution,was
Figure5:BERTeamlearneddistributionontrainedagents abletoproduceandrecognizeateamthatperformedcompetitively
againstpreviouslyunseenopponents.Infact,fromtherankingsin
Table1,weseethatthetopchoicefromBERTeamoutperformsany
After training, we evaluate the evolved agents based on the
teamthatdoesnotcontainagent2(thehardoffensiveagent).
aggressionmetricdescribedinSection4.3.1.WeobservefromFigure
Whiletheperformanceoftheteamslearnedfromself-playare
5(a)thatthemetricclusterstheevolvedagentsintotwocleargroups.
lowerthanthetopfixedpolicyagents,thismaybearesultfrom
Fromourpopulationof50agents,weclassify36asdefensiveand14
difficultiesintheenvironment,suchasalackofcorrelationbetween
asaggressive.Thesimilardistributionofagentfitnessacrosseach
gameoutcomesandMDPrewards.Thiscouldalsopotentiallybe
populationindicatesthatthisdiversityisnotcorrelatedwithagent
improvedbyhyperparametertuningineitherthebaseRLalgorithm,
performance.Itisinsteadlikelyduetospecializationfordifferent
thecoevolutionalgorithm,orinBERTeam.
subtasks.Thisindicatesthatourtrainingschemesupportsdiversity
Overall,thereasonableperformanceoftopcoevolvedteams,as
inagentbehaviorsduringcoevolution,evenwhentheMDPreward
wellasthepositivecorrelationinFigure6,indicatethatBERTeam
structureforeachagentisidentical.
trainedalongsidecoevolutionissuccessfulatoptimizingpolicies
Weusethegroupingofagentsbyaggressiontopartitionthe
foramultiagentadversarialgameagainstunknownopponents.
teamdistributionlearnedbyBERTeam.WenoticefromFigure5(b)
thatthetotaldistributionofBERTeamheavilyfavorsabalanced
5.2.2 AgentEmbeddings. Recallthatthefirststepofatransformer
team,composedofadefensiveandanaggressivemember.This
istoassigneachtokenavectorembedding.Wedirectlyinspectthe
pairingaccountsforabout75%ofthetotaldistribution.Thesecond
agentembeddingslearnedbyBERTeam.Forasubsetofthetotal
mostcommoncompositionistwoaggressivemembers,accounting
population,weconsidertheaveragecosinesimilarityofeachpair
forabout20%ofthetotaldistribution.
chosenfromthesubset.Weusethisasanestimateofhowsimilar
ThedistributionlearnedbyBERTeamalignswithourobserva-
BERTeambelievesagentsinthatsubsetare.
tioninthefixedpolicyexperiment,whereabalancedteamperforms
Foroursubsetchoices,wedividethetotalpopulationintoag-
thebest(Table1).ThisresultalsoimpliesthatBERTeamislearning
gressiveanddefensiveagents,asinFigure5(a).Wefurtherdivide
anon-trivialteamcomposition,sinceasolutionsuchas‘always
eachsubsetinto‘strong’and‘weak’basedonwhethertheirElo
choosethebestagent’favorsateamcomposedofoneagenttype.
isabovethepopulationaverage.WeexpectthatBERTeam’sem-
beddingsofeachclassofagentshavemoresimilaritythanasubset
chosenuniformlyatrandom.Wecalculatethecosinesimilarityof
auniformrandomsubsetinAppendixC.
FromtheresultsinTable2,wecanseethatthemajorityofthe
subsetswechosehaveastrongersimilaritythanarandomsubset
ofthesamesize.Theonlysubsetsthatdonotsupportthisarethe
‘Defensive’and‘WeakDefensive’subsets,whichareslightlylower.
This suggests that the initial vector embeddings learned by
BERTeamarenotsimplyuniquelydistinguishingeachagent.The
agentsthatperformsimilarlyareviewedassimilarbyBERTeam.
Thisindicatesknownpropertiesoftokenembeddingsinthedomain
ofNLP(suchaswordvectorslearnedin[28])applyinourcase
aswell.ThevectorslearnedbyBERTeamencodeaspectsofeach
agent’sbehavior,andsimilaritiesinagentscanbeinferredthrough
Figure6:BERTeamtotaldistributionandElos
similaritiesintheirinitialembeddings.ThissuggestsBERTeamcanSubset AvgCosineSimilarity Size significantcostisjustifiedbyitsstrongerperformance,andthat
UniformRandom -0.00306 Any itcanbetrainedofflinewithoutinterferingwiththeflowofthe
Aggressive 0.0153 14 restofthealgorithms.Finally,wefindourimplementationofMAP-
Elitesiscostly,aswesampleseparategamestoperformbehavior
Defensive -0.00449 36
projection.WecouldmitigatethisbyimplementingMAP-Elites
StrongAggressive 0.01984 8
moresimilartotheoriginalimplementation.
WeakAggressive 0.03846 6
StrongDefensive 0.00741 17
6 CONCLUSION
WeakDefensive -0.00842 19
Table2:AveragecosinesimilarityofBERTeam’slearnedini- In this paper, we propose BERTeam, an algorithm and training
tialembeddingsacrossvariouspopulationsubsets procedurethatisabletolearnteamcompositioninmultiagentad-
versarialgames.Thealgorithmiseffectivebothinchoosingteamsof
fixedpolicyagentsandwhenbeingtrainedalongwiththepolicies
ofindividualagents.BERTeamcanalsotakeininput,allowingitto
accountforincompletetrainingdatabylearningwhichagentshave generateteamsconditionalonobservationsofopponentbehavior.
similarbehaviors,andmaybeinterchangeableinateam. WeevaluatethisalgorithmonPyquaticus,asimulatedcapture-
the-flaggameplayedbyboatrobots.WetestBERTeambothwith
5.3 ComparisonwithMCAAandMAP-Elites
fixedpolicyagentsandtrainingalongsideacoevolutionarydeepRL
Wedirectlycomparethetrainedpoliciesusingouralgorithm,MCAA, algorithm.Wefindthatinbothcases,BERTeameffectivelylearns
andthehybridmethods.Weproduceteamsusingthespecifiedteam strongnon-trivialteamcomposition.Forfixedpolicyagents,we
formationmethod,andusetheoutcomesofthesegamestoestimate findthatBERTeamlearnsthecorrectoptimalteam.Inthecoevolu-
theircomparativeexpectedElosinTable3. tioncase,wefindthatBERTeamlearnstoformabalancedteamof
agentsthatperformscompetitively.Uponfurtherinspection,we
Policy Team Avg.updatetimeof findthatlikeitsinspirationinthefieldofNLP,BERTeamlearns
Elo
Optimizer Selection Agents TeamDist. similaritiesbetweenagentbehaviorsthroughinitialembeddings.
Coevolution BERTeam 919 13s/epoch 46s/update Thisallowsittoaccountformissingdatabyinferringthebehav-
Coevolution MCAA 817 13s/epoch ≈0s/update iorofagents.WealsofindthatBERTeamoutperformsMCAA,an
MAP-Elites BERTeam 883 36s/epoch 45s/update algorithmdesignedforteamselection.
MAP-Elites MCAA 809 35s/epoch ≈0s/update Overall, BERTeam is a strong team selection algorithm with
Table3:RelativeperformanceofMCAA,ouralgorithm,and rootsinspiredbyNLPtextgenerationmodels.BERTeam’sability
hybridalgorithms tolearnsimilaritiesinagentbehaviorresultsinefficienttraining,
andallowsBERTeamtotrainalongsideindividualagentpolicies.
6.1 FutureResearch
We find that trials that used BERTeam as a team formation
methodoutperformedtrialsthatusedtheMCAAmainlandalgo- • WedonotexplorethecapabilitytoconditionBERTeam’s
rithm.Onepossibleexplanationforthisisthelackofspecificityin outputonobservationsoftheopponent.Infutureresearch,
theMCAAmainlandalgorithm.WhileBERTeamlearnsthedistri- weplantoshowthatteamsgeneratedthroughconditioning
butiononanindividualagentlevel,MCAAchoosestheproportion outperformteamsgeneratedwithnoinformation.
ofeachislandincludedinateam.Thismethodseemstobemost • Wetestonlysize2teamstoeasilyanalyzetheoutputof
effectivewhentheislandsaredistinct(i.e.intheoriginalMCAA BERTeam.Weplantotestlargerteamsinfutureresearch.
paper,islandshaddifferentproportionsofrobottypes).However, • WecanchangeourweightingandtrainingsothatBERTeam
inourcase,itseemsvaryingtheRLalgorithmdidnothaveasimilar willconvergeonaNashEquilibrium,assumingcertainprop-
effect.AnotherpossibleexplanationiswhileBERTeammaylearn ertiesofthegameoutcomes(AppendixD.1).Wedonotmake
anarbitrarilyspecifictotaldistribution,MCAAcanonlylearnato- thesechangesbecausetheymaybeincompatiblewithMLM
taldistributionthatisindependentforeachposition.Thisrestricts training.Weplantoexplorethisinfutureresearch.
MCAAfromlearningdistributionsthatdonotfactorinthisway. • BERTeamisapplicabletogameswithmorethantwoteams.
Additionally,thissupportspreviousresults.Whentakingthe Infutureexperiments,itwouldbeinterestingtoevaluate
weightedaverageofElosinFigure6,wefindBERTeam’sexpected theperformanceofBERTeamonmulti-teamgames.
Eloisabout930.Thisisclosetotheresultof917,andtheminor • Forcoevolution,weonlyconsiderabasicevolutionaryalgo-
differencecanbeexplainedbythefactwetrainedforlessepochs. rithmwithreproductionthroughcloning,asin[12].How-
Asforruntimecomplexity,weseparatelyinspecttheclocktime ever,thereisavastliteratureofevolutionaryalgorithmvari-
ofteamtrainingandofagentpolicyupdates.Thedifferencein antsthatcouldreplacethis.Itwouldbeinterestingtoexplore
usingBERTeamorMCAAtogenerateteamsforpolicyupdateswas whicharemostcompatiblewithourtrainingscheme.
negligable.Theruntimewasdominatedbyconductingthesample • Wedonotfocusonoptimizinghyperparametersinoural-
gamesandconductingtheRLupdates.Forteamtraining,wefind gorithms or their interactions. It would be interesting to
theupdateofMCAAtookalmostnotime.Incontrast,trainingthe optimizetheseacrossawidevarietyofprobleminstances,
BERTeammodeltookabout46secondsonabatchsizeof512.This andinspecttheirrelationwithaspectsofeachinstance.REFERENCES
[29] DovMondererandLloydShapley.1996.FictitiousPlayPropertyforGameswith
[1] DzmitryBahdanauetal.2016.NeuralMachineTranslationbyJointlyLearning IdenticalInterests.JournalofEconomicTheory68,1(1996),258–265.
toAlignandTranslate. arXiv:1409.0473[cs.CL] [30] DovMondererandLloydS.Shapley.1996.PotentialGames.GamesandEconomic
[2] Jordan Beason et al. 2024. Evaluating Collaborative Autonomy in Behavior14,1(1996),124–143.
Opposed Environments using Maritime Capture-the-Flag Competitions. [31] Jean-BaptisteMouretandJeffClune.2015.Illuminatingsearchspacesbymapping
arXiv:2404.17038[cs.RO] elites. arXiv:1504.04909[cs.AI]
[3] Iz Beltagy et al. 2020. Longformer: The Long-Document Transformer. [32] MichaelNovitzkyetal.2019. Aquaticus:PubliclyAvailableDatasetsfroma
arXiv:2004.05150[cs.CL] MarineHuman-RobotTeamingTestbed.In201914thACM/IEEEInternational
[4] UlrichBerger.2007.Brown’soriginalfictitiousplay.JournalofEconomicTheory ConferenceonHuman-RobotInteraction(HRI).InstituteofElectricalandElectron-
135,1(2007),572–578. icsEngineers,392–400.
[5] AurélieBeynieretal.2013.DEC-MDP/POMDP.JohnWiley&Sons,Ltd,Chapter9, [33] AntoninRaffinetal.2021.Stable-Baselines3:ReliableReinforcementLearning
277–318. Implementations.JournalofMachineLearningResearch22,268(2021),1–8.
[6] GregBrockmanetal.2016.OpenAIGym. arXiv:1606.01540[cs.LG] [34] Pranav Rajbhandari. 2024. BERTeam. https://github.com/pranavraj575/
[7] GeorgeBrown.1951.IterativeSolutionofGamesbyFictitiousPlay.InActivity BERTeam.
AnalysisofProductionandAllocation,T.C.Koopmans(Ed.).Wiley. [35] PranavRajbhandari.2024.TransformerbasedCoevolver.https://github.com/
[8] TomBrownetal.2020.Languagemodelsarefew-shotlearners.InProceedingsof pranavraj575/coevolution.
the34thInternationalConferenceonNeuralInformationProcessingSystems(NIPS [36] PranavRajbhandari.2024.unstable_baselines3.https://github.com/pranavraj575/
’20).CurranAssociatesInc.,Article159,25pages. unstable_baselines3.
[9] Wei-ChengChangetal.2020.TamingPretrainedTransformersforExtremeMulti- [37] JuliaRobinson.1951.AnIterativeMethodofSolvingaGame.AnnalsofMathe-
labelTextClassification.InProceedingsofthe26thACMSIGKDDInternational matics54,2(1951),296–301.
ConferenceonKnowledgeDiscovery&DataMining(KDD’20).Associationfor [38] John Schulman et al. 2017. Proximal Policy Optimization Algorithms.
ComputingMachinery,3163–3171. arXiv:1707.06347[cs.LG]
[10] MoChenetal.2017.MultiplayerReach-AvoidGamesviaPairwiseOutcomes. [39] LloydShapley.1953.Stochasticgames.ProceedingsoftheNationalAcademyof
IEEETrans.Automat.Control62,3(2017),1451–1457. Sciences39,10(1953),1095–1100.
[11] KyunghyunChoetal.2014.LearningPhraseRepresentationsusingRNNEncoder- [40] DaigoShishikaetal.2019. TeamCompositionforPerimeterDefensewith
DecoderforStatisticalMachineTranslation.InProceedingsofthe2014Conference PatrollersandDefenders.In2019IEEE58thConferenceonDecisionandControl
onEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).Associationfor (CDC).InstituteofElectricalandElectronicsEngineers,7325–7332.
ComputationalLinguistics,1724–1734. [41] YanSongetal.2024.BoostingStudiesofMulti-AgentReinforcementLearning
[12] DavidCottonetal.2020.CoevolutionaryDeepReinforcementLearning.In2020 onGoogleResearchFootballEnvironment:ThePast,Present,andFuture.In
IEEESymposiumSeriesonComputationalIntelligence(SSCI).InstituteofElectrical Proceedingsofthe23rdInternationalConferenceonAutonomousAgentsandMulti-
andElectronicsEngineers,2600–2607. agentSystems(AAMAS’24).InternationalFoundationforAutonomousAgents
[13] JacobDevlinetal.2019.BERT:Pre-trainingofDeepBidirectionalTransformers andMultiagentSystems,1772–1781.
forLanguageUnderstanding.InProceedingsofNAACL-HLT,Vol.1.Association [42] IlyaSutskeveretal.2014.Sequencetosequencelearningwithneuralnetworks.In
forComputationalLinguistics,2. Proceedingsofthe27thInternationalConferenceonNeuralInformationProcessing
[14] GauravDixitetal.2022. Diversifyingbehaviorsforlearninginasymmetric Systems-Volume2(NIPS’14).MITPress,3104–3112.
multiagentsystems.InProceedingsoftheGeneticandEvolutionaryComputation [43] WilsonTaylor.2016."ClozeProcedure":ANewToolForMeasuringReadability.
Conference(GECCO’22).AssociationforComputingMachinery,350–358. InJournalismQuarterly.SageJournals,415–433.
[15] ArpadElo.1978.TheRatingofChessplayers,PastandPresent.ArcoPub. [44] J.K.Terryetal.2021.PettingZoo:GymforMulti-AgentReinforcementLearning.
[16] PhilippeFlajoletandRobertSedgewick.2013.AnalyticCombinatorics.Cambridge InAdvancesinNeuralInformationProcessingSystems,Vol.34.CurranAssociates,
UniversityPress. Inc.,15032–15043.
[17] VanessaFrías-MartínezandElizabethSklar.2004.Ateam-basedco-evolutionary [45] AshishVaswanietal.2017. Attentionisallyouneed.InProceedingsofthe
approachtomultiagentlearning.InProceedingsofthe2004AAMASWorkshop 31stInternationalConferenceonNeuralInformationProcessingSystems(NIPS’17).
onLearningandEvolutioninAgentBasedSystems.Citeseer,AutonomousAgents CurranAssociatesInc.,6000–6010.
andMultiagentSystems. [46] ChernHanYongandRistoMiikkulainen.2001. CooperativeCoevolutionof
[18] EloyGarciaetal.2020.OptimalStrategiesforaClassofMulti-PlayerReach-Avoid Multi-AgentSystems.
DifferentialGamesin3DSpace.IEEERoboticsandAutomationLetters5,3(2020), [47] ChernHanYongandRistoMiikkulainen.2009. CoevolutionofRole-Based
4257–4264. CooperationinMultiagentSystems.IEEETransactionsonAutonomousMental
[19] MorrisH.HansenandWilliamN.Hurwitz.1943.OntheTheoryofSamplingfrom Development1,3(2009),170–186.
FinitePopulations.TheAnnalsofMathematicalStatistics14,4(1943),333–362. [48] HaoyuZhaoetal.2021.Multi-ObjectiveOptimizationforFootballTeamMember
[20] JohannesHeinrichandDavidSilver.2016.DeepReinforcementLearningfrom Selection.IEEEAccess9(2021),90475–90487.
Self-PlayinImperfect-InformationGames. arXiv:1603.01121[cs.LG] [49] YueZhao,LushanJu,andJosèHernández-Orallo.2024.Teamformationthrough
[21] MaxJaderbergetal.2019.Human-levelperformancein3Dmultiplayergames anassessor:choosingMARLagentsinpursuit-evasiongames.Complex&Intelli-
withpopulation-basedreinforcementlearning.Science364,6443(2019),859–865. gentSystems10,3(2024),3473–3492.
[22] MarcinJunczys-Dowmunt.2019.MicrosoftTranslatoratWMT2019:Towards
Large-ScaleDocument-LevelNeuralMachineTranslation.InProceedingsofthe
FourthConferenceonMachineTranslation(Volume2:SharedTaskPapers,Day1).
AssociationforComputationalLinguistics,225–233.
[23] HiroakiKitanoetal.1997.TheRoboCupsyntheticagentchallenge97.InProceed-
ingsofthe15thInternationalJointConferenceonArtificalIntelligence-Volume1
(IJCAI’97).MorganKaufmannPublishersInc.,24–29.
[24] DaanKlijnandA.E.Eiben.2021. Acoevolutionaryapproachtodeepmulti-
agentreinforcementlearning.InProceedingsoftheGeneticandEvolutionary
ComputationConferenceCompanion(GECCO’21).AssociationforComputing
Machinery,283–284.
[25] FanqiLinetal.2023.TiZero:MasteringMulti-AgentFootballwithCurriculum
LearningandSelf-Play.InProceedingsofthe2023InternationalConferenceonAu-
tonomousAgentsandMultiagentSystems(AAMAS’23).InternationalFoundation
forAutonomousAgentsandMultiagentSystems,67–76.
[26] XiaodongLiuetal.2020.VeryDeepTransformersforNeuralMachineTranslation.
arXiv:2008.07772[cs.CL]
[27] StephenMcAleeretal.2023.Team-PSROforLearningApproximateTMECor
inLargeTeamGamesviaCooperativeReinforcementLearning.InAdvances
inNeuralInformationProcessingSystems,A.Oh,T.Naumann,A.Globerson,
K.Saenko,M.Hardt,andS.Levine(Eds.),Vol.36.CurranAssociates,Inc.,45402–
45418.
[28] TomasMikolovetal.2013.Efficientestimationofwordrepresentationsinvector
space. arXiv:1301.3781[cs.CL]A ELOUPDATEEQUATION D DATASETWEIGHTING/SAMPLING
Consider a 1v1 game where outcomes for each player are non- Considerthegeneralcasewith𝑚teamsinanadversarialgame.We
negativeandsumto1.Elosareamethodofassigningvaluesto assumeBERTeamhasacurrentdistributionforeachteam,andwe
eachagentinapopulationbasedontheirabilityinthegame[15]. wouldliketosampleadatasetfortheteamin𝑖thposition.Wealso
Ifagents1and2withelos 𝑓 1 and 𝑓 2 playagame,weexpect assumewehaveanotionofa‘winning’teaminacertaingame.Our
agent1towinwithprobability10𝑌 1:= 1+exp(1
𝑓 2−𝑓
1).Whenagame goalisthattheoccurrenceofateaminthedatasetisproportional
betweenagents1and2issampled,weupdatetheEloofagent𝑖 toitswinprobabilityagainstopponentsselectedbyBERTeam.
usingthegameoutcome𝑆 𝑖 withthefollowingequation: Forteam𝐴,denotethiswinprobability𝑊(𝐴).Letthesetofall
validteamsforthe𝑖thpositionbeT𝑖.
𝑓 𝑖′ =𝑓 𝑖 +𝑐(𝑆 𝑖 −𝑌 𝑖). (3) ThenaïveapproachistosimplysampleuniformlyfromT𝑖 and
includeteamsthatwinagainstopponentssampledfromBERTeam.
Notethatiftheoutcome𝑆 𝑖 islarger(resp.smaller)thanourexpec- Whilethisresultsinthecorrectdistribution,thismethodwillrarely
tation𝑌 𝑖,weincrease(resp.decrease)ourEloestimate.Weset𝑐as findasuccessfulteam,asweassumeBERTeam’schoicesarestrong.
thescaletodeterminethemagnitudeoftheupdates. To increase the probability of finding a successful team, we
maysampleusingBERTeaminsteadofuniformlyfrom T𝑖.This
B NUMBEROFPOSSIBLETEAMSOFSIZEK increasesoursuccessrate,butfailstogeneratethecorrectdistri-
bution.Inparticular,theoccurrenceofteam𝐴isproportionalto
Wewillfindthenumberofpossiblesize𝑘teamswithindistinguish-
P{𝐴 ∼ BERTeam}·𝑊(𝐴).Tofixthis,weuseinverseprobability
ablemembersfrom𝑛totalpolicies.
weighting[19],weightingeachinclusionofteam𝐴bytheinverseof
Wefirstpartitionallpossibleteamsof𝑘membersbasedontheir
itsselectionprobability.Thisensuresthattheweightedoccurrence
number of distinct policies𝑖. In the case of𝑖 policies, we must
choosewhichofthe𝑛policiestoinclude((cid:0)𝑛 𝑖(cid:1)
choices).Wethen
oft Te ha im sa𝐴 ddin itit oh ne ad lla yta cs re et ati es s𝑊 sy( m𝐴) m.
etry,sinceeachteamissampled
assignanagenttoeachofthe𝑘 teammembers.Sincewedonot
fromBERTeam.Thus,wemayconsider𝑚datasetsandeachgame
careaboutorder,wemustconsiderthenumberofwaystoassign𝑘
updatethedatasetscorrespondingtothewinningteams.Thisin-
indistinguishableobjects(members)into𝑖distinctbins(policies).
Thereare(cid:0)𝑘 𝑖−− 11(cid:1)
waystodothisby‘starsandbars’[16].Thus,overall
c or ue ra ese xs peth rie mn eu nm tsb ,e wr ho ef rs eam thp ele ps law ye erg se at np der og pa pm one eb ny tsa af ra ect so yr mo mf𝑚 et. rI icn
,
thereare(cid:0)𝑛 𝑖(cid:1)(cid:0)𝑘 𝑖−− 11(cid:1) teamchoiceswith𝑖distinctmembers.Wesum
wekeeponedatasetanddothisimplicitly.
𝑘
thisoverallpossiblevaluesof𝑖toobtain 𝑖(cid:205) =1(cid:0)𝑛 𝑖(cid:1)(cid:0)𝑘 𝑖−− 11(cid:1) .
Ifwedocareaboutorder(i.e.membersaredistinguishable),there D.1 RelationtoNashEquilibria
aretrivially𝑛𝑘 waystochooseasequenceof𝑘agentsfrom𝑛with Weanalyzethegeneralcasewherethereare𝑚teamsinagame,and
repeats.Anyintermediateorderconsiderationsmustfallbetween thesetsofvalidteamsareT 1,...,T𝑚.Theactofchoosingteams
thesetwoextremes.Ineithercase,withfixed𝑘,thereareΘ(𝑛𝑘)
toplaymultiagentadversarialmatchessuggeststhestructureof
possiblechoicesofateamofsize𝑘from𝑛totalagents. a normal form game with𝑚 players. The𝑖th player’s available
actionsareteamsinT𝑖,andtheutilitiesofachoiceof𝑚teamsare
C COSINESIMILARITYOFRANDOMSUBSET theexpectedoutcomesofeachteaminthematch.
Arandomsubsetofsize𝑘chosenuniformlyfromapopulationof ThedistributionofBERTeamdefinesamixedstrategyofaplayer
𝑖 in this game (i.e. a distribution over all teams, an element of
vectorswillhavethesameaveragecosinesimilarityasthewhole
thesimplex Δ(T𝑖)).Ideally,wewouldlikethetrainingtocause
population.
BERTeam to approach a Nash Equilibrium of the game. This is
Foraproof,wemayconsiderafullyconnectedgraphwhere
eachnodeisoneof𝑛vectors,andeachedgejoiningtwoagentsis possible,undersomeassumptions.
Consideraweighteddataset𝑆ofteamssampleduniformlyfrom
weightedbytheircosinesimilarity.Theabovestatementisequiva-
lenttosayingtheexpectedaverageweightofedgesinarandom T𝑖.Weconstructavector𝑣 𝑆 ∈R|T𝑖| suchthatthedimensioncor-
inducedsubgraphofsize𝑘istheoverallaverageedgeweight. respondingtoteam𝑇 ∈ T𝑖 istheweightedoccurrenceof𝑇 in𝑆.
Considertakingtheexpectationacrossallpossible𝑘 subsets. AssumeBERTeam’sdistributionis𝑝 ∈Δ(T𝑖).Wedefine𝑣 𝑆,𝑝 asthe
E ofa 𝑘ch sued bg see tw sce oig nh tat iw nii nll gb ie tc (co hu on ot se ed t(cid:0) h𝑛 𝑘 e−− r22 e(cid:1) mti am ine is n, gas 𝑘t −hi 2s fis rot mhe tn hu em 𝑛b −e 2r p ar bo oje uc nt dio an ryof o𝑣 f𝑆 Δo (Tn 𝑖t )o at nh det 𝑣a 𝑆n ,𝑝ge en xt its sp ta hc ee so imf𝑝 plw exrt ,. wΔ e(T m𝑖) a. yIf n𝑝 eeis do tn
o
othernodes).Whentakingtheedgeaverageineach𝑘subset,we insteadproject𝑣 𝑆 ontothetangentspaceof𝑝 withrespecttoa
divideby(cid:0)𝑘(cid:1) ,andwhentakingtheexpectationacrossall𝑘subsets,
lowerdimensionalfaceofΔ(T𝑖).
2 OurfirstassumptionisthatthereisaBERTeamarchitectureand
wedivideby(cid:0)𝑛 𝑘(cid:1) .Thus,anedgecontributes (( 𝑘𝑛 𝑘 )−− (22 𝑛)
)
= (cid:0)𝑛 2(cid:1)−1 times trainingschemesuchthatupdatingwithdataset𝑆isequivalent(in
itsweighttotheexpectation,thesameasitw2 ou𝑘
ldinanaverage
expectation)toupdating𝑝inthedirectionof𝑣 𝑆,𝑝.
(cid:0)𝑛(cid:1) Nowconsiderweightingeachteamin𝑆accordingtoitsexpected
acrossall edges.
2 outcomeagainstadistribution𝑞 ∈ (cid:206)Δ(T𝑗).Weclaimthatthe
𝑗≠𝑖
resultingvector𝑣 𝑆,𝑝 isinthedirectionofanupdatethatimproves
10Elosarescaledinchessbyafactorof 400 ,butignoringthisiscleaner theexpectedutilityof𝑝against𝑞unless𝑝isoptimal.
log10Considerthevectorin𝑅|T𝑖| wherethedimensioncorresponding Parameter Value Justification
toteam𝑇 istheexpectedoutcomeof𝑇 against𝑞.Byconstruction Epochs 3000 Plotconverged
of𝑆,thisisexactly𝑣 𝑆.Sinceexpectationsarelinear,thechange Gamesperepoch 25 Consistency
inexpectedutilitybyupdating𝑝 inthe𝑣 𝑆,𝑝 directionisexactly BERTeamTransformer
𝑣 𝑆 ·𝑣 𝑆,𝑝.However,since𝑣 𝑆,𝑝 isaprojectionof𝑣 𝑆,thisisalways Encoder/Decoderlayers 3/3 1 PyTorchdefault
non-negative.Additionally,𝑣 𝑆 ·𝑣 𝑆,𝑝 =0onlywhen𝑣 𝑆 iszero,or Embeddingdim 128 2 1 PyTorchdefault
when𝑝 cannotincreaseprobabilityfortheteamswithmaximal Feedforwarddim 512 4 1 PyTorchdefault
expectedoutcome.Thus,𝑣
𝑆,𝑝
improvestheexpectedutilityof𝑝
Attentionheads 4
4
1 PyTorchdefault
against𝑞unless𝑝isoptimal. 2
Dropout 0.1 PyTorchdefault
Thisimpliesupdatingwith𝑆weightedinthiswaywillresultin
Trainfrequency Every10epochs Consistency
animprovingupdateto𝑝basedonempiricalevidenceofopponent
Batch/Minibatchsize 1024/256
strategies.Thesameholdsforanyofthe𝑚players.
InputEmbedding(Unused)
This process is equivalent to fictitious play, a process where
playersupdatetheirstrategybasedonempiricalestimationsof Networkarchitecture LSTM
opponentstrategies[4,7].Therehasbeenmuchresearchintofor Layers 2
whatclassesofgamesfictitiousplayresultsinconvergencetoa Embeddingdim 128 SameasBERTeam
Nashequilibrium(e.g.zero-sumgameswithfinitestrategies[37], Dropout 0.1 SameasBERTeam
orpotentialgames[29,30]).Ifourdefinedgamehappenstofall Table4:BERTeamexperimentparameters(Section4.2)
inanyofthesecategories,thisprocesswillconvergetoaNash
equilibrium.Thisisoursecondassumption.
Toformadataset𝑆suchthattheweightedoccurrenceofeach
Parameter Value Justification
teamisitsexpectedoutcomeagainstadistribution𝑞,wecanfill
Epochs 8000
𝑆 with teams in T𝑖 weighed by sampled outcomes against𝑞. In
Gamesperepoch 25
expectation,thiswillachieveourdesiredweighting.11
ChangesinBERTeamParameters
Weimplicitlyassumethatthebehaviorsofagentsarefixed,so
Batch/Minibatchsize 512/64 Loweredforspeed
thatgamesbetweentwoteamshaveconstantexpectedoutcome.
Thisiscertainlynottrueforconfigurationswhereweupdateagent Coevolution
policies,butcanbe‘trueenough’ifthepolicieshaveconverged. PopulationSize 50
Oursecondassumptiondependsontheactualoutcomesdefined Replacements Drasticchangesmay
1
inthemultiagentadversarialgame.However,eveninsituations pergeneration destabilizeBERTeam
wherethisassumptiondoesnothold,updatingBERTeaminthis Protectionof Decentpoliciesrequire
500epochs
waywillresultinfictitiousplay,whichseemslikeareasonable newagents ∼500gamesoftraining
updatestrategy.InthecaseofPyquaticus,wecansimplyredefine Eliteagents 3 Protectbestpolicies
ouroutcomevaluestoformazero-sumgame. ReinforcementLearning
Our first assumption is untrue for MLM training. Instead of RLalgorithm PPO
updatingtowardsavector,MLMtrainingusescross-entropyloss, Networkhiddenlayers (64,64) stable_baselinesdefault
encouragingthemodeltomatchadistribution.Whilesomething
Table5:Coevolutionexperimentparameters(Section4.3)
similartoMSElosswouldachievethedesiredgradient,thereis
littlesupportinliteratureforusinglossesotherthancross-entropy
losswhenworkingwithdistributions.Thus,usingadifferentloss
Parameter Value Justification
orchangingthemodelmayresultinpoorlybehavedtraining.
Epochs 4000 Decreasedforspeed
E EXPERIMENTPARAMETERS GamesperEpoch 16 Decreasedforspeed
Islands 4 Sameasexperimentsin[14]
IslandSize 15 4·15≈50
EliteAgentsperIsland 1 1·4≈3
Island0
RLalgorithm PPO
Networkhiddenlayers (64,64) stable_baselinesdefault
Island1
RLalgorithm PPO
11IfwealsomustweighttoaccountforsamplingbiasasinAppendixD,wecansimply Networkhiddenlayers (96,96) Slightlymorecomplex
multiplytheinverseprobabilityweightwiththeoutcomeweighttoensurebothgoals. Island2
RLalgorithm DQN
Networkhiddenlayers (64,64) stable_baselinesdefault
Island3
RLalgorithm DQN
Networkhiddenlayers (96,96) Slightlymorecomplex
Table6:Comparisonexperimentparameters(Section4.4)