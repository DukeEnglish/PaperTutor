AUTOAL: AUTOMATED ACTIVE LEARNING WITH DIF-
FERENTIABLE QUERY STRATEGY SEARCH
YifengWang1,XueyingZhan2∗,SiyuHuang3
1DepartmentofElectricalandComputerEngineering,CarnegieMellonUniversity
2RayandStephanieLaneComputationalBiologyDepartment,CarnegieMellonUniversity
3VisualComputingDivision,SchoolofComputing,ClemsonUniversity
yifengw3@cs.cmu.edu, xueyingz@andrew.cmu.edu, siyuh@clemson.edu
ABSTRACT
Asdeeplearningcontinuestoevolve,theneedfordataefficiencybecomesincreas-
inglyimportant. Consideringlabelinglargedatasetsisbothtime-consumingand
expensive, activelearning(AL)providesapromisingsolutiontothischallenge
by iteratively selecting the most informative subsets of examples to train deep
neuralnetworks,therebyreducingthelabelingcost. However,theeffectiveness
ofdifferentALalgorithmscanvarysignificantlyacrossdatascenarios,anddeter-
miningwhichALalgorithmbestfitsagiventaskremainsachallengingproblem.
This work presents the first differentiable AL strategy search method, named
AutoAL,whichisdesignedontopofexistingALsamplingstrategies. AutoAL
consists of two neural nets, named SearchNet and FitNet, which are optimized
concurrentlyunderadifferentiablebi-leveloptimizationframework. Foranygiven
task, SearchNet and FitNet are iteratively co-optimized using the labeled data,
learninghowwellasetofcandidateALalgorithmsperformonthattask. With
the optimal AL strategies identified, SearchNet selects a small subset from the
unlabeled poolfor querying theirannotations, enabling efficienttraining ofthe
taskmodel. ExperimentalresultsdemonstratethatAutoALconsistentlyachieves
superior accuracy compared to all candidate AL algorithms and other selective
AL approaches, showcasing its potential for adapting and integrating multiple
existingALmethodsacrossdiversetasksanddomains. Codewillbeavailableat:
https://github.com/haizailache999/AutoAL.
1 INTRODUCTION
Withthedevelopmentofdeeplearning(DL)techniques,largevolumesofhigh-qualitylabeleddata
havebecomeincreasinglycrucialasthemodeltrainingprocessesareusuallydatahungry Renetal.
(2021);Zhanetal.(2022). Activelearning(AL)addressesthischallengebyiterativelyselecting
themostinformativeunlabeledsamplesforannotation,therebyboostingmodelefficiencyXieetal.
(2021);Wangetal.(2023). Deepactivelearningstrategiesaretypicallydividedintotwocategories:
uncertainty-basedandrepresentativeness/diversity-basedapproachesSener&Savarese(2017);Zhu
&Bento(2017);Ashetal.(2019). Uncertainty-basedmethodsfocusonqueryingsamplesthemodel
ismostuncertainabout,whilediversity-basedstrategiesaimtoselectadiversesubsetofsamples
thateffectivelyrepresenttheentiredatasetCitovskyetal.(2021). Bothstrategieshavelimitations,
especiallyinbatchselectionandvaryingdatasetcomplexities,leadingtotheinconsistencyofthe
strategyefficiencyRenetal.(2021). Hybridstrategies,whichbalanceuncertaintyanddiversityZhan
et al. (2021a;b), offer partial solutions. However, these strategies rely on strategy choices and
subjectiveexperimentsettings,whichcanbeineffectiveinreal-lifeapplications.
PreviousworksBarametal.(2004);Hsu&Lin(2015);Pangetal.(2018)haveexploredselecting
amongdifferentALstrategiestoachieveoptimalselectionswithouthumaneffort. SelectALHacohen
&Weinshall(2024)introducesamethodfordynamicallychoosingALstrategiesfordifferentdeep
learningtasksandbudgetsbyestimatingtherelativebudgetsizeoftheproblem.Itinvolvesevaluating
theimpactofremovingsmallsubsetsofdatapointsfromtheunlabeledpooltopredicthowdifferent
∗CorrespondingAuthor.
1
4202
tcO
71
]GL.sc[
1v35831.0142:viXraALstrategieswouldperform. However, thisstrategydependsonapproximatingthereductionin
generalizationerroronsmallsubsets,whichwouldnotfullycapturethecomplexitiesofreal-world
tasks. Zhang et al. (2024) proposes to select the optimal batch of AL strategy from hundreds of
candidates,aimingtomaximizefuturecumulativerewardsbasedonnoisypastrewardobservations
ofeachcandidatealgorithm. However,boththecomputationalcostandthelackofdifferentiability
maketheoptimizationoftheseworksinefficient.
To address the challenges mentioned above, we propose AutoAL, an automated active learning
frameworkthatleveragesdifferentiablequerystrategysearch. AutoALofferstwokeyadvantages:
1) it enables automatic and efficient updates during optimization, allowing the model to adapt
seamlessly;2)itisinherentlydata-driven,leveragingtheunderlyingdatadistributiontoguidethe
ALstrategyselection. However,integratingmultipleALstrategiesintothecandidatepoolmakes
achievingdifferentiabilitydifficult. Toovercomethisissue,AutoALleveragesabi-leveloptimization
frameworkthatsmoothlyintegratesandoptimizesdifferentALstrategies. Wedesigntwoneural
networks: SearchNet and FitNet. FitNet is trained on the labeled dataset to adaptively yield the
informativenessofeachunlabeledsample. GuidedbyFitNet’staskloss,SearchNetisefficiently
optimized to accurately select the best AL strategy. Instead of searching over a discrete set of
candidate AL strategies, we relax the search space to a continuous domain, allowing SearchNet
to be optimized via gradient descent based on FitNet’s output. The gradient-based optimization,
whichismoredata-efficientthantraditionalblack-boxsearches,enablesAutoALtoachievestate-of-
the-artperformanceinstrategyselectionwithsignificantlyreducedcomputationalcosts. Ourkey
contributionsareasfollows:
• WeintroduceanovelAutoALframeworkfordifferentiableALquerystrategysearch. Tothebest
ofourknowledge,AutoAListhefirstautomaticquerystrategysearchalgorithmthatcanbetrained
inadifferientiableway,achievinghighlyefficientupdateofquerystrategyinadata-drivenmanner.
• AutoALisbuiltuponexistingALstrategies(i.e.,uncertainty-basedanddiversity-basedapproaches).
ByincorporatingmostALstrategiesintoitssearchspace, AutoALleveragestheadvanagesof
thesestrategiesthroughfindingthemosteffectiveonesforspecificALsettings,objectives,ordata
distributions.
• WeevaluateAutoALacrossvariousALtasks.GiventheimportanceofALindomain-specifictasks,
particularlywheredataqualityandimbalanceareconcerns,weconducteextensiveexperimentson
bothnatureimagedatasetsandmedicaldatasets. TheresultsdemonstratethatAutoALconsistently
outperformsallcandidatestrategiesandotherstate-of-the-artALmethodsacrossthesedatasets.
2 RELATED WORK
ActiveLearning. ActiveLearninghasbeenstudiedfordecadestoimprovethemodelperformance
usingsmallsetsoflabeleddata,significantlyreducingtheannotationcostCohnetal.(1996);Settles
(2009); Zhan et al. (2021b). Most AL research focuses on pool-based AL, which identifies and
selectsthemostinformativesamplesfromalargeunlabeledpooliteratively. Pool-basedALsampling
strategiesaregenerallycategorizedintotwoprimarybranches: diversity-basedanduncertainty-based.
Thediversity-basedmethodsselectthebatchofsamplesthatcanbestrepresenttheentiredataset
distribution. CoreSetSener&Savarese(2017)selectsabatchofrepresentativepointsfromacoreset,
whichisasubsampleofthedatasetthateffectivelyservesasaproxyfortheentireset. Incontrast,
uncertainty-basedmethodsfocusonselectingthesampleswithhighuncertainty, whichisdueto
thedatagenerationorthemodeling/learningprocess. BayesianActiveLearningbyDisagreements
(BALD)Galetal.(2017)choosesdatapointsthatareexpectedtomaximizethemutualinformation
betweenpredictionsandmodelposterior.Yoo&Kweon(2019)incorporatesalosspredictionstrategy
byappendingacompactparametricmoduledesignedtoforecastthelossofunlabeledinputsrelative
to the target model. This is achieved by minimizing the discrepancy between the predicted loss
andtheactualtargetloss. Itrequiressubjectivejudgmentandstrategicselectionofactivelearning
methodstobeeffectiveonspecificdatasetsinreal-worldsettings.
AdaptiveSampleSelectioninAL. Neitherdiversity-basednoruncertainty-basedmethodsare
perfect in all data scenarios. Diversity-based methods tend to perform better when the dataset
contains rich category content and large batch size, while uncertainty-aware methods typically
performbetterinoppositesettingsZhanetal.(2021b);Citovskyetal.(2021). Severalpastworks
2haveexploredadaptiveselectioninactivelearningtosolvethisproblem. Acommonapproachisto
choosethebestALstrategyfromasetofcandidatemethodsanduseittoqueryunlabeleddata,such
asHacohen&Weinshall(2024)andZhangetal.(2024)introducedinSection1. ActiveLearningBy
Learning(ALBL)Hsu&Lin(2015)adaptivelyselectsfromasetofexistingalgorithmsbasedon
theirestimatedcontributiontolearningperformance. Itframeseachcandidateasabanditmachine,
convertingtheproblemintoamulti-armedbanditscenario. However,noneofthesemethodsleverage
thedifferentiableframeworkforautomaticALselection. Comparedtothemethodsmentionedabove,
AutoALfirstrelaxesthesearchspacetobecontinuous,thuscanoptimizeautomaticallyviagradient
descent. ThisframeworkenablesAutoALtoseamlesslyintegratemultipleexistingALstrategies
andrapidlyadapttoselecttheoptimalstrategybasedonthelabeledpool. Thesimplegradient-based
optimizationismuchmoredata-efficientthantraditionalblack-boxsearches. Therefore,AutoALcan
significantlyreducecomputationalcostscomparedtothesemethods.
BilevelOptimization. Bileveloptimizationisahierarchicalmathematicalframeworkwherethe
feasibleregionofoneoptimizationtaskisconstrainedbythesolutionsetofanotheroptimization
taskLiuetal.(2021). Ithasgainedsignificantattentionduetoitsnestedproblemstructure,which
allowsthetwotaskstooptimizejointly. BileveloptimizationadaptstomanyDLapplicationsChen
etal.(2022),suchashyperparameteroptimizationChenetal.(2019);MacKayetal.(2019),meta-
knowledgeextractionFinnetal.(2017),neuralarchitecturesearchLiuetal.(2018);Huetal.(2020),
andactivelearningSener&Savarese(2017). Forinstance,Sener&Savarese(2017)formulatesAL
asabi-levelcoresetselectionproblemanddesignsa2-approximationmethodtoselectasubsetof
datathatrepresentstheentiredataset. Inthiswork,wenovellyformulatetheAlquerystrategysearch
asabi-leveloptimizationproblem,i.e.,iterativelyupdatingFitNetandusingitstasklosstoguidethe
optimizationofSearchNet. Wefurtherreformulateitasadifferentiablelearningparadigmtoenable
amoreefficientselectionofexamplesacrossvariousobjectivesanddatadistributions.
3 METHODOLOGY
3.1 PROBLEMSETTING
Pool-based AL focuses on selecting the most informative data iteratively from a large pool of
unlabeledindependentandidenticallydistributed(i.i.d.)datasamplesuntilafixedbudgetisexhausted
ortheexpectedmodelperformancehasbeenreached. Specifically,inALprocesses,assumingthat
wehaveaninitialseedlabeledsetL={(x ,y )}M andalargeunlabeleddatapoolU ={x ,}N ,
j j j=1 i i=1
where M ≪ N, y ∈ {0,1} is the class label of x for binary classification and y ∈ {1,...,p}
i i i
for multi-class classification. In each iteration, we select a batch of the most informative data
Q∗ = argmaxb α(x,Ω)withbatchsizebfromU basedonthebasiclearnedmodelΩandthe
x∈U
acquisitionfunctionα(x,Ω),andquerytheirlabelsfromoracle/annotator. Withthesesamples,the
expectedlossfunctionf ofthebasiclearnedmodelcanbeminimized. LandU arethenupdated.
TheperformanceofexistingALmethodsreliesonthechoiceofquerystrategies,whichshouldbe
carefullyadaptedtodifferenttasksorapplications. ToachievearobustandadaptiveALperformance,
thisworknovellycreatesanautomatedALstrategyselectionframework. AutoALintegratestwo
neuralnets,FitNetΩ andSearchNetΩ uponeachbasiclearnedmodelΩtoselectthebestAL
F S
strategy before selecting Q. To facilitate automatic selection, AutoAL incorporates them into a
bi-leveloptimizationframeworkandrelaxesthesearchspacetoenabledifferentiableupdates. We
discussmoredetailsofAutoALframeworkinthefollowingsections.
3.2 AUTOMATEDACTIVELEARNING
WeproposeAutomatedActiveLearning(AutoAL),whichaimstoadaptivelydelivertheoptimal
query strategy for each sample in a given unlabeled data pool. Specifically, AutoAL consists of
two neural networks: FitNet Ω and SearchNet Ω . Ω selects the optimal AL strategy from a
F S S
set A, which contains K candidate AL sampling strategies {A } (e.g., BALD, Maximum
κ κ∈[K]
Entropy,etc). Ω modelsthedatadistributionwithintheunlabeleddatasetandguidesthetrainingof
F
SearchNetΩ . SinceannotationsfortheunlabeleddatacannotbeaccesseddirectlybythemodelΩ,
S
AutoALrequiresnodatafromtheunlabeledpoolU,usingonlythelabeleddatasetLfortraining.
3LossDetach Samples
Half FitNet AutoAL
Queue
After Training
Batch FitNet
B+1 (Cycle c-2) Loss LF
La Pb oe ole ld QH ua el uf e Batch (CycleS e ca -1rc &h N Ce yt cle c) AL Weights …00 0.. 52 RS ei lg am xao tii od n …00 0.. .90 054 2 Score LS
B 1 0 0.02
1 0 Probabilistic 0.02
0 0.73 Query 0.99
… … … AL Score
0 Bi-Level
0
Update 2 Optimization
AL
Candidates
Annotator After Training
Top-b
Samples
Labeled
AutoAL Unlabeled
Pool Selection Pool
Figure1: Overallframeworkofdifferentiablequerystrategysearchforautomatedactivelearning
(AutoAL).AutoALleverageslabeledpooltotraintheFitNetandSearchNetinabi-leveloptimization
mode. Datasampleswiththelargestsearchscorearethenselectedfromtheunlabeledpool.
InAutoAL,eachALiterationconsistsofC cycles. Ineachcyclec,thelabeleddataLisrandomly
splitintotwoequal-sizedsubsets: trainingandvalidationsets. FitNetΩ computesthecross-entropy
F
lossL Zhang&Sabuncu(2018)fordata.Meanwhile,SearchNetΩ generatesscoresforthesamples,
S
whichareusedtorankthem. Thisraisesakeyquestion: Ifonlythetrainingsetisavailable,without
accesstotheunlabeledpool,canΩ stilleffectivelyselecttheoptimalsamples?
S
Our key design is intuitive: SearchNet Ω treats the training batch as the labeled pool and the
S
validation batch as the unlabeled pool. This allows Ω to simulate the process of AL selection,
S
withoutdirectaccesstotheactualunlabeleddatapool. TheoptimizationgoalofΩ istoselectdata
S
withhigherlosses,astheyareexpectedtoprovidemoreinformativeupdatestotheneuralnetwork.
For Ω , the training objective is to minimize the loss of the selected samples, particularly those
F
prioritizedbyΩ . AutoALisformulatedasabi-leveloptimizationproblem,astheoptimizationof
S
Ω isdependentonlyonitself,buttheoptimizationofΩ dependsontheoptimalΩ .
F S F
(cid:88)M/2
Ω∗ =argmax L ((x ,y ),Ω ,Ω∗), (1)
S S j j S F
ΩS j=1
(cid:88)M/2
s.t. Ω∗ =argmin L ((x ,y ),Ω ).
F F j j F
ΩF j=1
L and L represent the losses of Ω and Ω , respectively. The neural nets Ω and Ω are
F S F S F S
optimizedjointlyasoutlinedinEq.1. Atthelowerlevelofthenestedoptimizationframework,Ω
F
isoptimizedtoapproachthedistributionofthedataset. Attheupperlevel,Ω isoptimizedtooutput
S
theinformativenessofeachdata,basedontheoptimaldistributionreturnedfromΩ .
F
3.3 DIFFERENTIABLEQUERYSTRATEGYOPTIMIZATION
AlthoughEq. 1showsaneffectiveframeworkforautomaticallyderivingtheoptimalquerystrategy,
solving the bi-level optimization problem is inefficient in practice. To address this, we derive a
probabilisticquerystrategycoupledwithadifferentiableoptimizationframework,improvingthe
efficiencyoftheoptimizationprocess.
Probabilisticquerystrategy. Duringtraining,foreachlabeleddatax ,wequeryascoreSκ(x )
j j
fromeachcandidateALsamplingstrategyAi,whereSκ(x )∈0,1indicateswhetherthesample
j
isselected. TomodeltheoverallscoreS(x ),whichisacombinationofalltheALsearchscores
j
Sκ(x ),weadoptaGaussianMixtureapproachReynoldsetal.(2009):
j
(cid:88)k
p(S)= π N(S|µ ,Σ ), (2)
k k k
k=1
4M
{πˆ ,µˆ ,Σˆ }=argmax (cid:89) p(S(x )), (3)
k k k πk,µk,Σk j
j=1
whereπ istheweightofthek-thgaussiancomponentandN istheprobabilitydensityfunction.
k
ThenwegeneratesamplesaccordingtotheGaussianMixturemodelandgetthet-thmaximumvalue,
wheretistheratioofbatchsizebtothetotalpoolsizeM+N. W istheprobabilityofeachquery
κ,j
strategyforeachsamplepredictedfromtheneuralnetworkΩ .
S
S
∼(cid:88)K
πˆ N(S|µˆ ,Σˆ ), (4)
sample k k k
k=1
Sˆ (x ,Ω )=(S (x )−ϑ (S ))∗W . (5)
κ j S κ j t sample κ,j
AsamplewithahigherscoreSˆindicatesthatithasbeenselectedbymorecandidateALstrategies,
andthereforeshouldbegivenhigherpriorityforlabeling.
Differentiableacquisitionfunctionoptimization. IfwelimittheindividualqueryscoreS (x )
κ j
todiscretevalues{0,1},thebi-leveloptimizationobjectiveofEq. 1becomesnon-differentiableand
stilldifficulttooptimize. Toenabledifferentiableoptimization,werelaxthecategoricalselectionof
strategiesintoacontinuousspace,weapplyaSigmoidfunctionoverallpossiblestrategies.:
S¯(x )= (cid:88) λ Sˆ (x ,Ω ), (6)
j 1+exp(−Θ(j)) κ j S
κ∈K S′
κ
wherethestrategymixingweightsforeachsamplej isparameterizedbyavectorΘ(j)ofdimension
|A|. λisascalingvector. S¯(x )isthefinaldifferentiablelearningobjectivefunction,whichcanbe
j
optimizedwithback-propagation. Therefore,Ω andΩ canbeefficientlyoptimizedunderEq. 1.
F S
3.4 LEARNINGANDALGORITHMOFAUTOAL
WithdifferentiablequerystrategyoptimizationinSec.3.3,wereformulatethebi-leveloptimization
probleminSec.3.2asefficientoptimizationobjectivesasfollows:
(n+2)B
L = 1 (cid:88) S¯ (x′)·L(x′,y′)+λ¯L (t,B), (7)
F B detach j j j re
j′=1+(n+1)B
(n+1)B
L =−1 (cid:88) S¯(x )·L (x ,y )−λ¯L (t,B), (8)
S B j detach j j re
j=1+nB
Asdescribedinsection3.2,incyclec-2,Ω isoptimizedtogetthedatadistribution. Thefinallossis
F
calculatedaccordingtoEq.7. Incyclec-1,AutoALoptimizesΩ togetthedatasamplewithhighest
S
loss. ThelossfunctionfromΩ isdetachedtoavoidupdatingitself,andthefinallossiscalculated
F
accordingtoEq.8. Hereweusenegativelosstoachievegradientascent.
Forn∈(cid:8) 0,1,...,(cid:4)M(cid:5)(cid:9)
,
2B
AutoALadditionallyintegratesalosspredictionmoduleaccordingtoYoo&Kweon(2019)tohelp
updateΩ ,as
S
(cid:18) (cid:19)
1
L (t,B)= −0.5 . (9)
re 1+exp(0.5·|α−t·B|)
L representsaregularizationlossthatlimitsthenumberofselectedsamples,asshowninEq.9.
re
Here,αisthenumberofselectedsamples,tistheratioofthequerybatchsizebineachALiteration
tothetotalpoolsizeM +N,andλ¯isascalingvector. Thefinallearningobjectiveistoselectthe
mostinformativesampleswhileeffectivelyleveragingtheunderlyingdatadistribution.
TheAutoALalgorithmisillustratedinAlgorithm1. IneachALiteration,thelabeledsetisfirstused
totraintheFitNetandSearchNetasdescribedinsection3.2. ThentheoptimalSearchNetΩ∗ is
S
appliedtotheunlabeledpooltoselectanewbatchofdata,whereΩ∗ isusedtogeneratescoreS¯for
S
eachsamplex ,andQ∗ =argmaxb S¯(x )withtop-bhighestscoringsamplesareselectedand
i x∈U i
labeled. ThelabeledsetLandtheunlabeledsetU areupdatedasL=L+Q∗ andU =U −Q∗,
respectively. Theupdatedlabeledsetisthenusedfortaskmodeltraining.
5Algorithm1AutoAL:AutomatedActiveLearningwithDifferentiableQueryStrategySearch
Input: K candidatealgorithmsA = {A } ,labeledpoolL = {(x ,y )}M andunlabledpoolU =
κ κ∈[K] j j j=1
{x }N ,totalnumberofALroundsR,batchsizeb,taskmodelT.
i i=1
Output: taskmodelT
1: InitializeT
2: forc=1,...,Rdo
3: OptimizeΩ∗,Ω∗ accordingtoEq.1;
F S
4: CalculateS¯(x )byEq.6foralli∈N;
i
5: SelectthetopbsampleswiththehighestscoreS¯(x );
j
6: Querylabely foralli∈b;UpdateLandU;
i
7: TraintaskmodelT byusingL;
8: endfor
9: returnT
Table1: Overviewofdatasetinformation.
Dataset #oftrainingset #oftestset #ofclasses
Cifar-10 50,000 10,000 10
Natureimage
Cifar-100 50,000 10,000 100
SVHN 73,257 26,032 10
OrganCMNIST 12,975 8,216 11
Medicalimage
PathMNIST 89,996 7,180 9
TissueMNIST 165,466 47,280 8
4 EXPERIMENTS
WeempiricallyevaluatetheperformanceofAutoALonawiderangeofdatasetswithbothnature
andmedicalimages. TheseexperimentsdemonstratethatAutoALoutperformsthebaselinesbyfully
automatingthetrainingwithinthedifferentiablebi-leveloptimizationframework. Additionally,we
conductablationstudiestoanalyzethecontributionofeachmoduleandexaminetheeffectsofthe
candidateALstrategies,includingdifferentnumbersofcandidates.
4.1 EXPERIMENTSETTINGS
Datasets and Baselines. We conduct AL experiments on six datasets: Cifar-10 and Cifar-
100 Krizhevsky et al. (2009), SVHN Netzer et al. (2011) in the nature image domain, and Or-
ganCMNIST,PathMNIST,andTissueMNISTfromMedMNISTYangetal.(2023)inthemedical
imagedomain.WecompareourmethodagainstmultipleALsamplingstrategies,includingMaximum
EntropySamplingShannon(1948),MarginSamplingNetzeretal.(2011),LeastConfidenceWang&
Shang(2014),KMeansSamplingAhmedetal.(2020),BayesianActiveLearningbyDisagreements
(BALD)Galetal.(2017),VariationRatios(VarRatio)Freeman(1965)andMeanStandardDeviation
(MeanSTD)Kampffmeyeretal.(2016). Wealsoconsiderthestate-of-the-artdeepALmethods
as baselines, including Batch Active learning by Diverse Gradient Embeddings (BADGE) Ash
etal.(2019)andLossPredictionActiveLearning(LPL)Yoo&Kweon(2019). Wealsoinclude
ALBLHsu&Lin(2015)tocompareAutoALwithexistingselectiveALstrategies.
Implementation Details. For Ω and Ω , we build the backbone using ResNet-18 He et al.
F S
(2016). We also employ ResNet-18 as the classification model on all baselines and AutoAL for
faircomparison. SinceAutoALisbuiltuponexistingALstrategiesandfocusesonselectingthe
optimalstrategy,weintegratesevenALmethodsintoAutoAL:MaximumEntropy,MarginSampling,
LeastConfidence,KMeans,BALD,VarRatio,andMeanSTD.FortheoptimizationofΩ andloss
S
predictionmodule,weuseSGDoptimizerRuder(2016). FortheoptimizationofΩ ,weuseAdam
F
optimizer Kingma (2014). Both use 0.005 as the learning rate. While training, FitNet will first
updatefor200epochsusingthevalidationqueue,thenΩ ,Ω andthelosspredictionmodulewill
F S
6 ( Q W U R S \ 6 D P S O L Q J  / H D V W & R Q I L G H Q F H  0 H D Q 6 7 '  % $ / '  / 3 /  $ X W R $ /
 . P H D Q V 6 D P S O L Q J  0 D U J L Q 6 D P S O L Q J  9 D U 5 D W L R  % D G J H 6 D P S O L Q J  $ / % /
  D   & L I D U      E   & L I D U       F   6 9 + 1
    
    
    
    
         
    
              
              
              
                                                                                                                 
   R I  O D E H O H G  G D W D    R I  O D E H O H G  G D W D    R I  O D E H O H G  G D W D
  G   2 U J D Q & 0 1 , 6 7   H   3 D W K 0 1 , 6 7   I   7 L V V X H 0 1 , 6 7
    
         
    
         
    
         
    
              
         
         
    
    
                                                                                                              
   R I  O D E H O H G  G D W D    R I  O D E H O H G  G D W D    R I  O D E H O H G  G D W D
Figure2: Overallperformanceonsixbenchmarkdatasets: naturalimagedatasets(top)andmedical
imagedatasets(bottom).
updateiterativelywithatotalof400epochs. Allexperimentsarerepeatedthreetimeswithdifferent
randomlyselectedinitiallabeledpools,reportingmeanandstandarddeviation.
4.2 EVALUATIONOFACTIVELEARNINGPERFORMANCE
Fig.2showstheoverallperformancecomparisonofdifferentALmethods,whereAutoALconsistently
outperformsthebaselinesacrossalldatasets. Additionally,wehavethefollowingobservations:
1. Ourmethodconsistentlyoutperformstheotherapproachesintermsofrounds,acrucialattribute
forasuccessfulactivelearningmethod. Thisadaptabilityisparticularlyvaluableinreal-world
applications,wherethelabelingbudgetmayvarysignificantlyacrossdifferenttasks. Forinstance,
onemightonlyhavetheresourcestoannotate10%ofthedata,ratherthan20%or40%.
2. Our method demonstrates robust performance not only on easier datasets but also on more
challengingones,suchasCifar-100andOrganCMNIST.Cifar-100hassignificantlymoreclasses
thanotherdatasets,whileOrganCMNISThassmallerdatapool,withonly12,975images. These
challengesmakeactivelearningmoredifficult, yetourmethod’ssuperiorperformanceacross
thesedatasetshighlightsitsrobustness.
3. The labeled data vs. accuracy curves of our method are relatively smooth compared to those
of other methods. The baseline methods sometimes show significant accuracy drops during
certainactivelearningrounds. ThismainlyduestoharmfuldataselectionKoh&Liang(2017)or
overfittingproblem. However,byselectingtheoptimalstrategythusselectingtheinformativedata,
AutoALcanalleviatetheproblemandmakethecurvemuchmoresmooththanbaselinestrategies.
4. Ourmethodshowssmallerstandarddeviations,indicatingthatitisrobustacrossrepeatedexperi-
ments. ThisrobustnessiscrucialforALapplications,especiallyinareal-worldsetting,wherethe
ALprocessistypicallyconductedonlyonce.
5. DifferentALstrategiesproducevaryingresultsacrossdatasets. Forinstance,MarginSampling
underperforms on the SVHN dataset, while KMeans Sampling yields the worst performance
7
     \ F D U X F F $  Q D H 0
     \ F D U X F F $  Q D H 0
     \ F D U X F F $  Q D H 0
     \ F D U X F F $  Q D H 0
     \ F D U X F F $  Q D H 0
     \ F D U X F F $  Q D H 0 5 H V 1 H W  % D F N E R Q H  / R V V  3 U H G L F W L R Q  0 R G X O H  5 H V 1 H W  / R V V  3 U H G L F W L R Q
  D   & L I D U       E   6 9 + 1   F   2 U J D Q & 0 1 , 6 7
    
         
         
    
         
         
         
    
         
         
    
    
    
                                                                                                                 
   R I  O D E H O H G  G D W D    R I  O D E H O H G  G D W D    R I  O D E H O H G  G D W D
Figure3: AblationstudyonthreecomponentsofAutoAL.‘ResNetBackbone’: withouttheloss
predictionmodule,updatingonlytheResNet.‘LossPredictionModule’:withoutupdatingtheResNet
backbone,optimizingsolelywiththelosspredictionmodule. ‘ResNet+LossPrediction’: thefull
AutoALpipeline.
   F D Q G L G D W H    F D Q G L G D W H V    F D Q G L G D W H V    F D Q G L G D W H V
  D   & L I D U       E   6 9 + 1   F   2 U J D Q & 0 1 , 6 7
    
         
    
         
    
    
         
    
         
    
         
    
                                                                                                                 
   R I  O D E H O H G  G D W D    R I  O D E H O H G  G D W D    R I  O D E H O H G  G D W D
Figure4: AblationstudyonthesizeofthecandidatepoolinAutoALstrategyselection.
ontheOrganCMNISTandCifar-10datasets. Thisvariabilityhighlightsthesignificanceofour
approach,whichaimstoidentifyandselecttheoptimalstrategyforeachdataset.
4.3 ABLATIONSTUDIES
WeperformtheablationstudyontwokeycomponentsofAutoAL:theSearchNetarchitectureand
theALcandidatestrategies. Experimentsareconductedonthreedatasets: Cifar-100,SVHN,and
OrganCMNIST,whicharechosenfortheirmixofnaturalandmedicalimages,aswellastheincreased
difficultytheypresentforactivelearningapplications.
SearchNetArchitecture. OurSearchNetconsistsoftwomaincomponents: aResNet-18backbone
andalosspredictionmoduleforsamplelossprediction Yoo&Kweon(2019). Inthisablationstudy,
wedisableonecomponentatatimeandrecordtheresults.Fig.3illustratestheperformance,revealing
theeffectivenessofbothResNet-18backboneandthelosspredictionmodule. However,whenonly
thelosspredictionmoduleisusedtooptimizethenetwork,performancedropssignificantly. This
alignswithourintuition,asalthoughthelosspredictionmodulecanassistwithnetworkoptimization,
themainfocusshiftsfromselectingthebestALstrategytomerelyminimizingthesampleloss. This
misalignmentwillresultinincorrectALstrategyweightestimation(SeeFig.1).
SizeofActiveLearningCandidatePool. SinceAutoALisbuiltonmultipleALstrategies,we
studyhowthesizeoftheALcandidatepoolimpactsperformance. WestartfromtheMaximum
Entropyasthesolecandidatefortheinitialbaseline,thenexpandthecandidatepooltothreestrategies
by adding Margin Sampling and Least Confidence. For the pool of five candidates, we further
includeKMeansandBALD.AsshowninFig.4,forallthreedatasets,thesinglecandidatevariation
performstheworstbutstilloutperformstheEntropySamplingbaselineinFig.2. Thisisbecause
AutoAL does not directly query data samples with maximum entropy but queries the data with
thehighestscorefromSearchNet, whichhasbeenoptimizedwiththehelpofthelossprediction
8
     \ F D U X F F $  Q D H 0
     \ F D U X F F $  Q D H 0
     \ F D U X F F $  Q D H 0
     \ F D U X F F $  Q D H 0
     \ F D U X F F $  Q D H 0
     \ F D U X F F $  Q D H 0moduleandFitNet. ForCifar-100andOrganCMNIST,AutoALcanperformwellwhenthereare
atleastthreecandidatesintheselectionpool. However,forSVHN,moreALcandidatesresultin
betterperformance. Thisindicatesthattheupperboundofcandidatepoolsizevariesacrossdifferent
datasets. Incertaindatasets,incorporatingmoreactivelearningstrategiesyieldsbetterresults,but
thisisnotthecaseforotherdatasets. Additionally,poolswithmorecandidatesexhibitlowerstandard
deviations. This demonstrates strong capability of AutoAL in selecting the best active learning
strategy. Itcanconsistentlychoosetheoptimaloneandexcludethelesseffectiveoptions.
4.4 ALSELECTIONSTRATEGY
Here,weexaminewhichALstrategyisprioritizedbyAutoALindifferentactivelearningrounds.
ForeachALstrategycandidate,wecalculatetheALscoreofeachimageaccordingtoEq.6. Notice
thatwedon’tsumallthescoresaccordingtothedimensionofeachactivelearning,butcomputethe
averagevalueofallimagesoveraround,andnormalizetheresultsforvisualization. Asillustratedin
Fig.5,inbothexperiments,KMeansdominatethesampleselectionduringtheinitialrounds,such
as the first round, but be de-prioritized in subsequent rounds. This indicates that diversity-based
measures, which select samples that represent diverse regions of the data distribution, are more
criticalintheearlystagesofactivelearning. Intheearlyrounds,whenonlyasmallpercentageofthe
datahasbeenlabeled,themodel’sunderstandingoftheentiredatadistributionislimited. Asaresult,
selectingsamplesthatcoverabroadspectrumofthedatasetbecomescrucialtoprovidethemodel
withamorecomprehensiveviewofthedata.
OnCifar-100dataset,bothLeastConfidenceandMeanSTD,twouncertainty-basedmeasures,demon-
strateimprovedperformanceinthefinalrounds. AutoALassignsthemhigherprioritytoenhance
overallperformance. AstheALprocessprogressesandmorediversedatabecomeavailable, the
model attempts to develop a better understanding of the underlying structure of the target data.
Uncertainty-basedmeasuresaremoreeffectivenowbecausethemajorchallengeswitchestorefining
thedecisionboundaries. Thetransitionfromdiversitytouncertainty-basedstrategiesisconsistent
with the model’s evolving needs. In early rounds, the model requires diversity to build a broad
understandingofthedata,butoncethatfoundationisestablished,itsfocusshiftstowarduncertainty,
whichhelpsfine-tunemodeldecision-makingability. Thisadaptiveapproachensuresthatthemodel
targetsthemostdiversesubsetsateachstageofthelearningprocess,frombroadexplorationinthe
earlyroundstofine-grainedexploitationinthelaterrounds. Byautomaticallyadjustingthepriorities
ofthesestrategies,AutoALimprovestheoverallaccuracyandshowsstrongerrobustness.
  D   & L I D U       E   2 U J D Q & 0 1 , 6 7
   
 ( Q W U R S \ 6 D P S O L Q J  ( Q W U R S \ 6 D P S O L Q J
   
 0 D U J L Q 6 D P S O L Q J  0 D U J L Q 6 D P S O L Q J
   
 / H D V W & R Q I L G H Q F H  / H D V W & R Q I L G H Q F H
   
 . P H D Q V 6 D P S O L Q J  . P H D Q V 6 D P S O L Q J    
 % $ / '  % $ / '    
 9 D U 5 D W L R  9 D U 5 D W L R    
   
 0 H D Q 6 7 '  0 H D Q 6 7 '
                                 
 $ /  5 R X Q G V  $ /  5 R X Q G V
Figure5: ALstrategyscoresacrossdifferentALroundsonCIFAR-100andOrganCMNISTdatasets.
5 CONCLUSION
Inthispaper, wehaveintroducedAutoAL,thefirstautomatedsearchframeworkfordeepactive
learning. AutoAL employs two neural networks, SearchNet and FitNet, integrated into a bilevel
optimizationframeworktoenablejointoptimizations. Toefficientlysolvethebi-leveloptimization
problem,wehaveproposedaprobabilisticquerystrategythatrelaxesthesearchspacefromdiscreteto
9continuous,enablingadifferentiableanddata-drivenlearningparadigmforAutoAL.Thisframework
notonlyacceleratesthesearchprocessbutalsoensuresthatAutoALcangeneralizeacrossawide
rangeoftasksanddatasets. Furthermore,AutoALisflexible,allowingforeasyintegrationofmostof
theexistingactivelearningstrategiesintothecandidatepool,makingitadaptabletoevolvingactive
learningtechniques. Ourextensiveempiricalevaluation,conductedacrossbothnaturalandmedical
imagedatasets,hasdemonstratedtherobustnessandgeneralizabilityofAutoAL.Theresultsshowthat
AutoALconsistentlyoutperformsbaselines,highlightingitscapabilitytooptimizesampleselection
andenhancemodelperformance. Infuture,weplantoapplyAutoALtoboardermachinelearning
taskssuchasstructuredpredictionandconsidermoresophisticatedALmethodcombinations.
REFERENCES
MohiuddinAhmed,RaihanSeraj,andSyedMohammedShamsulIslam. Thek-meansalgorithm: A
comprehensivesurveyandperformanceevaluation. Electronics,9(8):1295,2020.
JordanTAsh,ChichengZhang,AkshayKrishnamurthy,JohnLangford,andAlekhAgarwal. Deep
batchactivelearningbydiverse,uncertaingradientlowerbounds.arXivpreprintarXiv:1906.03671,
2019.
YoramBaram,RanElYaniv,andKobiLuz. Onlinechoiceofactivelearningalgorithms. Journalof
MachineLearningResearch,5(Mar):255–291,2004.
CanChen,XiChen,ChenMa,ZixuanLiu,andXueLiu. Gradient-basedbi-leveloptimizationfor
deeplearning: Asurvey. arXivpreprintarXiv:2207.11719,2022.
YihongChen,BeiChen,XiangnanHe,ChenGao,YongLi,Jian-GuangLou,andYueWang. λopt:
Learntoregularizerecommendermodelsinfinerlevels. InProceedingsofthe25thACMSIGKDD
InternationalConferenceonKnowledgeDiscovery&DataMining,pp.978–986,2019.
Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand Rajagopalan, Afshin
Rostamizadeh,andSanjivKumar. Batchactivelearningatscale. AdvancesinNeuralInformation
ProcessingSystems,34:11933–11944,2021.
DavidACohn,ZoubinGhahramani,andMichaelIJordan. Activelearningwithstatisticalmodels.
Journalofartificialintelligenceresearch,4:129–145,1996.
ChelseaFinn,PieterAbbeel,andSergeyLevine. Model-agnosticmeta-learningforfastadaptationof
deepnetworks. InInternationalconferenceonmachinelearning,pp.1126–1135.PMLR,2017.
LintonCFreeman. Elementaryappliedstatistics: forstudentsinbehavioralscience. (NoTitle),1965.
YarinGal,RiashatIslam,andZoubinGhahramani. Deepbayesianactivelearningwithimagedata.
InInternationalconferenceonmachinelearning,pp.1183–1192.PMLR,2017.
GuyHacohenandDaphnaWeinshall. Howtoselectwhichactivelearningstrategyisbestsuitedfor
yourspecificproblemandbudget. AdvancesinNeuralInformationProcessingSystems,36,2024.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pp.770–778,2016.
Wei-Ning Hsu and Hsuan-Tien Lin. Active learning by learning. In Proceedings of the AAAI
ConferenceonArtificialIntelligence,volume29,2015.
YiboHu,XiangWu,andRanHe. Tf-nas: Rethinkingthreesearchfreedomsoflatency-constrained
differentiableneuralarchitecturesearch. InComputerVision–ECCV2020: 16thEuropeanCon-
ference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XV 16, pp. 123–139. Springer,
2020.
MichaelKampffmeyer,Arnt-BorreSalberg,andRobertJenssen. Semanticsegmentationofsmall
objectsandmodelingofuncertaintyinurbanremotesensingimagesusingdeepconvolutional
neuralnetworks.InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition
workshops,pp.1–9,2016.
10DiederikPKingma. Adam: Amethodforstochasticoptimization. arXivpreprintarXiv:1412.6980,
2014.
PangWeiKohandPercyLiang. Understandingblack-boxpredictionsviainfluencefunctions. In
Internationalconferenceonmachinelearning,pp.1885–1894.PMLR,2017.
AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiplelayersoffeaturesfromtinyimages. 2009.
HanxiaoLiu,KarenSimonyan,andYimingYang. Darts: Differentiablearchitecturesearch. arXiv
preprintarXiv:1806.09055,2018.
RishengLiu,JiaxinGao,JinZhang,DeyuMeng,andZhouchenLin. Investigatingbi-leveloptimiza-
tionforlearningandvisionfromaunifiedperspective: Asurveyandbeyond. IEEETransactions
onPatternAnalysisandMachineIntelligence,44(12):10045–10067,2021.
Matthew MacKay, Paul Vicol, Jon Lorraine, David Duvenaud, and Roger Grosse. Self-tuning
networks: Bileveloptimizationofhyperparametersusingstructuredbest-responsefunctions. arXiv
preprintarXiv:1903.03088,2019.
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Baolin Wu, Andrew Y Ng, et al.
Readingdigitsinnaturalimageswithunsupervisedfeaturelearning. InNIPSworkshopondeep
learningandunsupervisedfeaturelearning,volume2011,pp. 4.Granada,2011.
KunkunPang,MingzhiDong,YangWu,andTimothyMHospedales. Dynamicensembleactive
learning: Anon-stationarybanditwithexpertadvice. In201824thInternationalConferenceon
PatternRecognition(ICPR),pp.2269–2276.IEEE,2018.
PengzhenRen,YunXiao,XiaojunChang,Po-YaoHuang,ZhihuiLi,BrijBGupta,XiaojiangChen,
andXinWang. Asurveyofdeepactivelearning. ACMcomputingsurveys(CSUR),54(9):1–40,
2021.
DouglasAReynoldsetal. Gaussianmixturemodels. Encyclopediaofbiometrics,741(659-663),
2009.
Sebastian Ruder. An overview of gradient descent optimization algorithms. arXiv preprint
arXiv:1609.04747,2016.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. arXivpreprintarXiv:1708.00489,2017.
BurrSettles. Activelearningliteraturesurvey. 2009.
Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical
journal,27(3):379–423,1948.
DanWangandYiShang. Anewactivelabelingmethodfordeeplearning. In2014International
jointconferenceonneuralnetworks(IJCNN),pp.112–119.IEEE,2014.
YifengWang,ZhiTu,YiwenXiang,ShiyuanZhou,XiyuanChen,BingxuanLi,andTianyiZhang.
Rapidimagelabelingvianeuro-symboliclearning. InProceedingsofthe29thACMSIGKDD
ConferenceonKnowledgeDiscoveryandDataMining,pp.2467–2477,2023.
Yichen Xie, Masayoshi Tomizuka, and Wei Zhan. Towards general and efficient active learning.
arXivpreprintarXiv:2112.07963,2021.
JianchengYang, RuiShi, DonglaiWei, ZequanLiu, LinZhao, BilianKe, HanspeterPfister, and
BingbingNi. Medmnistv2-alarge-scalelightweightbenchmarkfor2dand3dbiomedicalimage
classification. ScientificData,10(1):41,2023.
DonggeunYooandInSoKweon. Learninglossforactivelearning. InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pp.93–102,2019.
XueyingZhan,QingLi,andAntoniBChan. Multiple-criteriabasedactivelearningwithfixed-size
determinantalpointprocesses. arXivpreprintarXiv:2107.01622,2021a.
11XueyingZhan,HuanLiu,QingLi,andAntoniBChan. Acomparativesurvey: Benchmarkingfor
pool-basedactivelearning. InIJCAI,pp.4679–4686,2021b.
XueyingZhan,QingzhongWang,Kuan-haoHuang,HaoyiXiong,DejingDou,andAntoniBChan.
Acomparativesurveyofdeepactivelearning. arXivpreprintarXiv:2203.13450,2022.
JifanZhang,ShuaiShao,SaurabhVerma,andRobertNowak. Algorithmselectionfordeepactive
learningwithimbalanceddatasets. AdvancesinNeuralInformationProcessingSystems,36,2024.
ZhiluZhangandMertSabuncu. Generalizedcrossentropylossfortrainingdeepneuralnetworks
withnoisylabels. Advancesinneuralinformationprocessingsystems,31,2018.
Jia-JieZhuandJose´Bento. Generativeadversarialactivelearning. arXivpreprintarXiv:1702.07956,
2017.
12