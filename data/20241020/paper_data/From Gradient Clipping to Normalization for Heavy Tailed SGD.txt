From Gradient Clipping to Normalization
for Heavy Tailed SGD
Florian Hübler∗† Ilyas Fatkhullin∗ Niao He
ETH Zurich, Switzerland
Abstract
Recentempiricalevidenceindicatesthatmanymachinelearningapplicationsinvolveheavy-tailedgra-
dient noise, which challenges the standard assumptions of bounded variance in stochastic optimization.
Gradientclippinghasemergedasapopulartooltohandlethisheavy-tailednoise,asitachievesgoodper-
formanceinthissettingboththeoreticallyandpractically. However,ourcurrenttheoreticalunderstanding
of non-convex gradient clipping has three main shortcomings. First, the theory hinges on large, increas-
ingclippingthresholds,whichareinstarkcontrasttothesmallconstantclippingthresholdsemployedin
practice. Second, clipping thresholds require knowledge of problem-dependent parameters to guarantee
convergence. Lastly,evenwiththisknowledge,currentsamplingcomplexityupperboundsforthemethod
are sub-optimal in nearly all parameters. To address these issues, we study convergence of Normalized
SGD (NSGD). First, we establish a parameter-free sample complexity for NSGD of O(cid:16) ε− p2 −p 1(cid:17) to find
anε-stationarypoint. Furthermore,weprovetightnessofthisresult,byprovidingamatchingalgorithm-
specific lower bound. In the setting where all problem parameters are known, we show this complexity
is improved to
O(cid:16) ε−3 pp −− 12(cid:17)
, matching the previously known lower bound for all first-order methods in
all problem dependent parameters. Finally, we establish high-probability convergence of NSGD with a
mild logarithmic dependence on the failure probability. Our work complements the studies of gradient
clipping under heavy tailed noise improving the sample complexities of existing algorithms and offering
an alternative mechanism to achieve high probability convergence.
1 Introduction
We study the stochastic optimization problem
minF(x), F(x):=E [f(x,ξ)], (1)
ξ∼D
x∈Rd
where F: Rd →R is a potentially non-convex, L-smooth objective function, and ξ is a random variable with
an unknown distribution D. Such problems are pervasive in machine learning applications, where obtaining
exact gradients is often infeasible, necessitating reliance on stochastic gradients (Bottou et al., 2018).
Traditionally, stochastic gradient methods rely on the assumption that the variance of the gradient
noise is bounded. Under this assumption, it is well established that first-order algorithms require at least
Ω(cid:0) ∆ Lε−2+∆ Lσ2ε−4(cid:1) stochastic gradient oracle queries in the worst case to find an ε-stationary point,
1 1
i.e., x ∈ Rd with E[∥∇F(x)∥] ≤ ε (Arjevani et al., 2023). Here ∆ denotes the initialization gap, and σ2
1
thevariance. StochasticGradientDescent(SGD)withanappropriatelychosenstep-sizeachievesthisoptimal
sample complexity (Ghadimi and Lan, 2013).
However, new insights in machine learning suggest that the bounded variance (BV) assumption may be
overly restrictive. Empirical evidence from fields such as image classification (Simsekli et al., 2019; Battash
et al., 2024), training large language models (LLMs) (Zhang et al., 2020; Ahn et al., 2024), and policy
optimizationinreinforcementlearning(RL)(Gargetal.,2021)indicatesthatstochasticgradientsoftenfollow
∗EqualContribution
†florian.huebler@inf.ethz.ch
1
4202
tcO
71
]CO.htam[
1v94831.0142:viXraheavy-tailed distributions. These findings challenge the standard assumption, suggesting a shift towards
weaker noise models which only assume boundedness of the p-th central moment of the gradient noise for
some p∈(1,2], i.e.
E[∥∇f(x,ξ)−∇F(x)∥p]≤σp (p-BCM)
with σ = σ ≥ 0, where p denotes the tail index. Specifically, the aforementioned works estimate the tail
p
indexofstochasticgradientsusingstatisticaltests(e.g.,Mohammadietal.(2015))andfindp<2. Evenwhen
the bounded variance assumption holds, the resulting constant σ(2) can be prohibitively large compared to
σ(p) for some p<2.
WhileSGDachievestheoptimalsamplecomplexityunderfinitevariance, empiricalevidencesuggeststhat
adaptive algorithms become crucial in the presence of heavy tailed noise (Zhang et al., 2020). All works1
which are able to prove convergence under these conditions employ the gradient clipping mechanism (Zhang
et al., 2020; Gorbunov et al., 2020; Cutkosky and Mehta, 2021; Sadiev et al., 2023; Gorbunov et al., 2024;
Li and Liu, 2023; Nguyen et al., 2023a; Kornilov et al., 2024; Liu et al., 2024). This mechanism replaces the
stochastic gradient in optimization algorithms by its clipped counterpart
(cid:26) (cid:27)
γ
∇(cid:98)f(x t,ξ t)=min 1, ∥∇f(xt
,ξ )∥
∇f(x t,ξ t), (2)
t t
where {γ } is the sequence of clipping thresholds.
t t≥1
Perhaps, the most popular scheme is Clip-SGD,2 which updates the iterates as x
t+1
=x t−η t∇(cid:98)f(x t,ξ t),
where {η } is a predefined sequence of step-sizes.
t t≥1
1.1 Drawbacks of Gradient Clipping Theory
Despite its popularity in the literature, we want to outline several drawbacks of current clipping theory.
Misalignment between theoretical and practical insights. Existingtheoreticalanalysesof Clip-SGD
(and its variants) under the (p-BCM) assumption hinge on using a large, p-dependent sequence of increasing
1
clipping thresholds (e.g., γ
t
= γ ·t3p−2) (Zhang et al., 2020; Cutkosky and Mehta, 2021; Li and Liu, 2023;
Nguyen et al., 2023b,a). This choice of clipping thresholds is based on the following two ideas. First,
clipping allows to control the variance of the clipped gradient estimator ∇(cid:98)f(x t,ξ t), even in cases where
the original gradient oracle has infinite variance. Second, it ensures that the probability of gradients being
clipped decreases over time as γ increases, thereby reducing the bias introduced by clipping and facilitating
t
convergence. However,thistheoreticalrecommendationcontradictscommonpracticeforclippinginmachine
learning, where small, constant thresholds (e.g., γ ≡ 0.25) are typically used instead (Merity et al., 2018;
t
Zhang et al., 2022; Touvron et al., 2023).
In contrast, one can observe that the clipping thresholds commonly used in practice lead to an increasing
probability of clipping gradients, eventually resulting in gradients being clipped at every iteration. This
observation runs counter to theoretical insights, which suggest clipping is becoming less frequent as training
progresses. Specifically, we observe this phenomenon in language modelling tasks in Section 4, and notice
the same effect on simpler, synthetic examples in Appendix E. This aggressive clipping behaviour essentially
transforms Clip-SGD into a variant of Normalized SGD:
g
x =x −η t , (NSGD)
t+1 t t∥g ∥
t
where g = ∇f(x ,ξ ) in this case. It is worth noting, however, that unlike Clip-SGD, NSGD only requires
t t t
tuning a single parameter η, highlighting its simplicity in comparison.
1Except for (Wang et al., 2021), which studies convergence of SGD in the strongly convex case under additional p-positive
definitenessassumptionontheHessian.
2ManyvariantsandmodificationsofClip-SGDexistincludingitscombinationswithNesterov’sacceleration(Gorbunovetal.,
2020),normalization(CutkoskyandMehta,2021),zero-order(Kornilovetal.,2024)andcoordinate-wisevariants(Zhangetal.,
2020),butgradientclippingisthekeybuildingblockofthesemethods.
2Need for tuning. To our knowledge, all existing convergence results for clipping require knowledge of
all problem parameters to set the clipping thresholds {γ } and other hyper-parameters of the underlying
t t≥1
algorithm. As these problem-dependent parameters are not known in practice, this corresponds to the need
for extensive hyper-parameter tuning. In particular, for Clip-SGD, there are 2 hyper-parameters which
potentially require tuning. In Appendix E we observe that even in simple scenarios, tuning both parameters
may be needed to match the performance of NSGD, which only has 1 parameter. Additionally, in Section 4
we empirically observe that even while requiring extensive hyper-parameter tuning, Clip-SGD is not able to
outperform vanilla NSGD on language modelling tasks.
Suboptimal sample complexities. None of the existing convergence analysis of non-convex Clip-SGD
(anditsvariants)achievethesamplecomplexitylowerboundbyZhangetal.(2020),Ω(cid:16) ∆1L + ∆1L(cid:0)σ(cid:1) p−p 1(cid:17) ,
ε2 ε2 ε
in all problem parameters, even when problem parameters are known. In particular, prior to this work, the
optimal heavy-tailed sample complexity remained an open question.
1.2 Our Contributions
Our work seeks to remove the drawbacks listed above by diving into the convergence analysis of NSGD with
different gradient estimators3 under heavy tailed noise. We summarize our contributions as follows:
1. We prove in-expectation convergence of NSGD using either mini-batches or momentum under the (p-
BCM) assumption for p∈(1,2] in two settings.
a) Without any knowledge of problem specific parameters, we show that the algorithms require at most
O(cid:16) ∆4 1+L4 +(cid:0)σ(cid:1) p2 −p 1(cid:17)
queriestostochasticgradientoracletoreachanε-stationarypointinexpectation,
ε4 ε
providing the first parameter-free heavy-tailed convergence guarantee. Furthermore, we construct an
algorithm specific lower bound showing that this sample complexity cannot be uniformly improved for
NSGD with polynomial step-size and batch-size.
b) When problem parameters are known, we improve the sample complexity to O(cid:16) ∆1L + ∆1L(cid:0)σ(cid:1) p−p 1(cid:17) ,
ε2 ε2 ε
improving the previously best known heavy-tailed sample complexity. This sample complexity exactly
matches the mini-max lower bound in all parameters for the class of first-order algorithms under our
assumptions.
To our knowledge, NSGD is the first algorithm which achieves either a) or b) in the heavy tail regime p<2.
2. We provide a high probability convergence guarantee for minibatch-NSGD, removing the need for
clipping, thereby extending our understanding of high-probability guarantees under heavy-tailed noise. The
sample complexity in this case corresponds to the same complexity as its in-expectation counterpart with a
mild multiplicative log(1/δ) factor.
1.3 Related Work
Gradient clipping is widely used to stabilize the training in various fields of machine learning (Pascanu
et al., 2013; Schulman et al., 2017; Zhang et al., 2020). Recently a number of works provide convergence
guarantees for Clip-SGD and its variants in different settings, e.g., (Nazin et al., 2019; Gorbunov et al.,
2020; Davis et al., 2021; Gorbunov et al., 2024; Liu and Zhou, 2023; Puchkin et al., 2024) to name a few.
However, the results in the non-convex stochastic setting are relatively scarce. In particular, Zhang et al.
(2020) study in-expectation and Sadiev et al. (2023); Nguyen et al. (2023a) investigate high probability
convergence of Clip-SGD under (p-BCM). All above mentioned works use increasing (iteration dependent)
1
clipping parameters, e.g., γ
t
=γ·t3p−2, and derive suboptimal convergence rates, see Section 3.1 for a more
detaileddiscussion. Amomentumversionof Clip-SGDwasanalyzedin(MaiandJohansson,2021)assuming
the bounded second moment of stochastic gradients. However, their proof crucially relies on setting the
clipping threshold larger than the expected gradient norm. Recently, (Koloskova et al., 2023) offer a new
analysisof Clip-SGDwithconstantclippingthresholdunderBVsetting. However,theirproofcruciallyrelies
3Our results in the main body are stated for minibatch-NSGD, the corresponding results for NSGD with momentum can be
foundinAppendixC.
3on bounded variance and seems challenging to extend to (p-BCM) setting. It is worth mentioning that
gradient clipping is also used to tackle heavy tailed noise in bandits and RL literature, e.g., (Bubeck et al.,
2013; Cayci and Eryilmaz, 2024). Moreover, Clip-SGD is the key mechanism to ensure differential privacy
(Abadi et al., 2016; Sha et al., 2024).
Normalized SGDwasfirstproposedbyNesterov(1984,2018)andanalyzedinthedeterministicconvex
case. Later the analysis was extended to smooth (Levy, 2017) and stochastic (Hazan et al., 2015) settings.
In the non-convex case, Cutkosky and Mehta (2020) show how to remove large mini-batch requirement for
NSGD by incorporating Polyak’s momentum. Later, Yang et al. (2024) derive a tight lower bound for NSGD
without momentum and Hübler et al. (2024) study the parameter agnosticity of momentum NSGD under a
relaxed smoothness assumption. In a different line of works, Levy (2016) study the ability of NSGD to escape
from saddle points. However, all above mentioned works make strong noise assumptions such as BV. The
most closely related to our work are (Cutkosky and Mehta, 2021; Liu et al., 2023), which study variants of
NSGDunderheavytailednoise. Unfortunately,theseworksusebothnormalizationandgradientclippingwith
increasing clipping parameter, which necessitates tuning γ . Moreover, Cutkosky and Mehta (2021) assume
t
bounded non-central moment assumption, i.e., E[∥∇f(x,ξ)∥p] ≤ Gp, which is stronger than our (p-BCM).
This assumption is relaxed in (Liu et al., 2023) to (p-BCM) at the cost of imposing an additional (almost
sure) individual smoothness assumption for each f(x,ξ).
More recently, the role of normalization was investigated for sharpness aware minimization (Dai et al.,
2024), and the variants of NSGD showed an impressive empirical and theoretical success in more structured
non-convexproblemsinRL(Fatkhullinetal.,2023;Barakatetal.,2023;Ganeshetal.,2024). However,these
works are also restricted to benign BV noise assumption. Some recent works also make connections with
SignSGD algorithm (Bernstein et al., 2018; Karimireddy et al., 2019; Crawshaw et al., 2022), which applies
a coordinate-wise normalization. Indeed, the convergence analysis of SignSGD and NSGD are closely related
and our techniques can be extended to its sign variants (Liu et al., 2019; Sun et al., 2023).
2 Preliminaries
Let us introduce basic notations, definitions and assumptions needed in the upcoming analysis.
Notation. We adopt the common conventions N={0,1,...}, [n]={1,2,...,n} and that empty sums and
products are given by their corresponding neutral element. Throughout this paper, d ∈ N denotes the
≥1
dimension of the variable to be optimized, F: Rd → R the objective and ∇f(·,·) the stochastic gradient
oracle. Unlessstatedotherwise,L≥0denotestheL-smoothnessparameterandη >0thestepsizes. Weuse
t
the standard O(·),Ω(·),ω(·) complexity notations (Howell, 2008), O(cid:101)(·) additionally hides poly-logarithmic
factors in log(1/δ), where δ is the failure probability.
ProblemSetup. Sincesolvingmin F(x)toglobaloptimalityiscomputationallyintractable(Nemirovskij
x∈Rd
andYudin,1983),ourgoalistoinsteadfindanε-stationarypoint,i.e.,x∈Rdsuchthat 1 (cid:80)T ∥∇F(x)∥≤ε
T t=1
in expectation or with high probability. Furthermore, we assume the access to first order information is lim-
ited to a (potentially noisy) gradient oracle, ∇f(·,ξ) of ∇F, where ξ is a random variable. The sample
complexity is defined as the number of calls the algorithm makes to this oracle to find an ε-stationary point.
Throughout the paper we work under the following standard assumptions.
Assumption 1 (Lower Boundedness). The objective function F is lower bounded by F∗ >−∞.
Assumption 2 (L-smoothness). The objective function F is L-smooth, i.e. F is differentiable and for all
x,y ∈Rd we have ∥∇F(x)−∇F(y)∥≤L∥x−y∥.
Instead of the classical bounded variance assumption, we adopt the weaker concept of the bounded p-th
central moment, as discussed in the introduction.
Assumption 3 (p-BCM). The gradient oracle is unbiased and has a finite p-th central moment, i.e. there
exists σ ≥0 such that, for all x∈Rd,
p
i) E[∇f(x,ξ)]=∇F(x), and
ii) E[∥∇f(x,ξ)−∇F(x)∥p]≤σp.
p
4In this work, we focus on the case p ∈ (1,2]. It is worth noting that, by Jensen’s inequality, any oracle
satisfying (p-BCM) also satisfies the assumption for all p′ ≤ p, with σ ≤ σ . Notably, (p-BCM) is weaker
p′ p
thantheboundedvarianceassumption,anditispossibleforσ tobemuchsmallerthanσ . Wewillomitthe
p′ p
subscript throughout the work to improve readability, though the dependence of σ on p remains important
to keep in mind.
3 Main Results
In this section, we present our convergence results for normalized stochastic gradient methods under the
(p-BCM) assumption. In order to guarantee a consistent presentation, we will present the results for
minibatch-NSGD,4 i.e., NSGD with the mini-batch gradient estimator
g =
1
(cid:88)Bt
∇f(cid:16)
x
,ξ(j)(cid:17)
, (3)
t B t t
t
j=1
where ξ(1),...,ξ(Bt) are independent copies of ξ . All results but Corollary 5 can equivalently be derived
t t t
for NSGD with momentum and are presented in Appendix C. Furthermore, for vanilla NSGD (i.e. using g =
t
∇f(x ,ξ )),theresultsimplyconvergencetoaσ-neighbourhood,inlinewithcorrespondingalgorithmspecific
t t
lower bound (Yang et al., 2024, Theorem 3).
In Section 3.1 we first examine the convergence of minibatch-NSGD for unknown problem-parameters,
providing a parameter-free convergence guarantee under the (p-BCM) assumption. Afterwards, we examine
the performance for optimally tuned parameters. In Section 3.2, we derive a high-probability convergence
result for minibatch-NSGD. Finally, in Section 3.3, we examine the importance of different convergence
measures for our analysis.
3.1 Normalized SGD can Handle Heavy Tailed Noise
We first theoretically confirm the robustness of minibatch-NSGD, by providing a parameter-free convergence
guarantee. This is in stark contrast to current Clip-SGD analyses, which hinge on the knowledge of all
parameters.
Proposition 1. Assume(LowerBoundedness), (L-smoothness)and(p-BCM)withp∈(1,2]. Letη,B,q >0
and r ∈ (0,1). Then the iterates generated by minibatch-NSGD with parameters η ≡ ηT−r and B ≡
t t
⌈max{1,BTq}⌉ satisfy
T
1 (cid:88) E[∥∇F(x )∥]≤ ∆ 1 + ηL + 4σ .
T t ηT1−r 2Tr p−1
t=1 max{1,BTq} p
(cid:18) (cid:19)
In particular, the sample complexity is bounded by O
(cid:0)∆1(cid:1) 11 −+q
r
+(cid:0)L(cid:1)1+ rq +(cid:0)σ(cid:1) qp (( p1 −+q 1)
) .
ε ε ε
This result characterises the sample complexity of minibatch-NSGD under the (p-BCM) assumption for
differentordersofstep-sizesandbatch-sizes. Rememberthatanyoraclesatisfying(p-BCM),alsosatisfiesthe
assumptionforallp′ ≤pwithσ ≤σ . Inparticular,itispossiblethatσ ≪σ andapplyingProposition1
p′ p p′ p
with p′ may yield a smaller sample complexity. Hence the result also implies a potentially better sample
complexity bound for a specific oracle by taking the infimum over all p′ ∈(1,p] of our result.
TheproofofProposition1followsasimilarstructuretothecasewhenp=2,thoughitdemandsadditional
attention to the noise term. Notably, we employ a vectorized version of the von Bahr and Esseen inequality
(seeLemma10),whichprovidesamoregeneralfoundationcomparedtothead-hocapproachusingadditional
gradientclippingCutkoskyandMehta(2021),whoanalyzedNSGDwithmomentumandgradientclipping. In
the special case of p=2, our Proposition 1 can recover the previous rates for NSGD in (Cutkosky and Mehta,
2020).
4Wefurthermorepresenttheresultsforknownhorizon(T-dependent)parameters. Notethatallconvergenceguaranteesalso
holdfordecaying(t-dependent)parametersatthemildcostofamultiplicativelog(T)term.
5Tightness of Proposition 1. While lower bounds on the sample complexity for general first-order algo-
rithms are well-established (Arjevani et al., 2023; Zhang et al., 2021), there are no algorithm-specific lower
boundsspecifyingtheoptimaloraclecomplexityofminibatch-NSGDwithgeneralparameters. Asaresult, it
isunclearwhether theparameter-dependence— inparticular thedependence on r and q —in Proposition1
is tight. To address this, we establish an algorithm-specific lower bound, demonstrating that Proposition 1
is indeed tight in all parameters.
Theorem 2 (Simplified). Under the setting of Proposition 1, consider minibatch-NSGD with parameters
η ≡ ηT−r and B ≡ ⌈max{1,BTq}⌉. Then there exists a function F that satisfies (Lower Boundedness),
t t
(L-smoothness), and an oracle ∇f(·,·) that satisfies (p-BCM) such that A requires at least
Ω(cid:32)(cid:18) ∆ 1(cid:19) 11 −+q r +(cid:18) L(cid:19)1+ rq +(cid:16)σ(cid:17) qp (( pq −+1 1) )(cid:33)
ε ε ε
samples to generate an iterate with E[∥∇F(x )∥]≤ε.
t
The extended result, which includes the dependence on η and B, can be found in Appendix D.2. Its
proofisbasedontwokeyideas. First, inthedeterministicsetting, weconstructahardfunctionthatexactly
satisfies (L-smoothness) and (Lower Boundedness), penalizing excessively small and large step sizes within
(cid:16) (cid:17)
a single function. This yields an iteration complexity lower bound of Ω (∆1/ε)1/(1−r)+(L/ε)1/r . Second,
we construct an oracle that points in the opposite direction of the true gradient with maximal probability,
while adhering to the (p-BCM) assumption. This oracle leads to a lower bound on the required batchsize,
(cid:16) p (cid:17)
which in turn implies an iteration complexity lower bound of Ω (σ/ε)q(p−1) . Combining these iteration
complexity lower bounds with the samples per iteration concludes the proof. The formal proof can be found
in Appendix D.
Parameter-free convergence. Whenconsideringtheparametersr =1/2andq =1,Proposition1implies
the sample complexity
(cid:18) ∆4+L4 (cid:16)σ(cid:17) 2p (cid:19)
O 1 + p−1 , (4)
ε4 ε
without requiring knowledge of any problem-dependent parameters, including the tail index p. It turns out,
that thischoiceof step-size and batch-size parametersis the bestparameter-free choice possible, in thesense
that (4) cannot be uniformly improved for all p∈(1,2]. More precisely, (4) is tight for all p∈(1,2], as can
be seen by plugging r =1/2 and q =1 into Theorem 2. Furthermore, while a different choice of r and q may
improve the sample complexity for some p, the complexity would get strictly worse for p = 2. That is any
other choice of (r,q) ̸= (1/2,1) implies a sample complexity lower bound of
ω(cid:0) ε−4(cid:1)
, which is strictly worse
than the
O(cid:0) ε−4(cid:1)
we get from (4).
Optimalsamplecomplexitywithtuning. Foralgorithmswithknowledgeofproblemparameters,Zhang
et al. (2020) provide a sample complexity lower bound for our setting of
(cid:18) ∆ L ∆ L(cid:16)σ(cid:17) p (cid:19)
Ω 1 + 1 p−1 . (5)
ε2 ε2 ε
Tothebestofourknowledge,therearenoupperboundsexactlymatchingthislowerbound,leavingthetight-
ness of (5) an open question. The following result closes this question, by improving the sample complexity
of (4) — when given access to problem-parameters — to tightly match the lower bound in all parameters.
Corollary 3 (Optimal Sample Complexity). Assume (Lower Boundedness), (L-smoothness) and (p-BCM)
(cid:112)
with p ∈ (1,2]. Then the iterates generated by minibatch-NSGD with parameters η
t
≡ ∆1/LT and B
t
≡
(cid:24) (cid:26) (cid:16) (cid:17) p (cid:27)(cid:25)
max 1, σ2T 2p−2 satisfy
∆1L
√
T
1 (cid:88)
E[∥∇F(x )∥]≤6
√∆ 1L
.
T t T
t=1
6In particular the sample complexity is bounded by O(cid:16) ∆1L + ∆1L(cid:0)σ(cid:1) p−p 1(cid:17) .
ε2 ε2 ε
For comparison, Zhang et al. (2020) derived in-expectation convergence for Clip-SGD with a sample
complexity of5
 
O∆
1Lσpp −2
1
+
(∆
1Lσp)23 pp −− 22 +σ3 pp −− 12

ε2 3p−2
εp−1
which is suboptimal in all parameters besides ε.
3.2 Convergence with High-Probability
While in-expectation results guarantee small gradient norms given sufficiently many optimization runs, com-
putationalconstraintsoftenprecluderunningenoughprocedures. Therefore,resultsoftheformwithprobabil-
ityatleast1−δ,asingleoptimizationrunachievesacertaingradientnorm,oftencalledin-probabilityresults,
are more desirable. While the Markov inequality can convert in-expectation guarantees to in-probability
guarantees, the poor polynomial dependence on 1/δ renders these results impractical.
Therefore, the gold standard are so called high-probability results with a mild log(1/δ) dependence. To
achieve such results, existing literature relies on either light tail noise assumptions (e.g., Ghadimi and Lan
(2013);LiandOrabona(2020);Maddenetal.(2024);LiandLiu(2022)),orthegradientclippingmechanism
(e.g., Cutkosky and Mehta (2021); Nguyen et al. (2023a)). In contrast, Sadiev et al. (2023) prove that SGD
√
without clipping cannot achieve a better δ dependence than Ω(1/ δ) under heavy-tailed noise.
The following theorem provides an unified high-probability guarantee for NSGD with a general gradient
estimator. The result will imply high-probability convergence for minibatch-NSGD, and high-probability
convergence to a σ-neighbourhood for vanilla NSGD.
Theorem 4 (High-Probability). Let δ ∈ (0,1). Assume (Lower Boundedness), (L-smoothness) and ∞ >
σ := E[∥g −∇F(x )∥|F ], where F := σ(g ,...,g ). Additionally let ηmax := max η and
t t t t−1 t−1 1 t−1 T t∈[T] t
C :=max η
(cid:80)t−1
η . Then, with probability at least 1−δ, the iterates generated by NSGD satisfy
T t∈[T] t τ=1 τ
(cid:88)T
w ∥∇F(x )∥≤
2∆ 1+L(cid:80)T t=1η t2+4(cid:80)T t=1η tσ
t +
12(η Tmax∥∇F(x 1)∥+C TL)log(1/δ)
,
t t (cid:80)T
η
(cid:80)T
η
t=1 τ=1 τ τ=1 τ
where w := ηt .
t (cid:80)T τ=1ητ
Themainideabehindtheproofistoreducethestatementtolowerboundingtheexpectedcosinebetween
g and ∇F(x ). Since the cosine is bounded within [−1,1], we can apply a high-probability concentration
t t
inequalityonitandobtainthemildlog(1/δ)dependence. Wewouldliketopointoutthatourprooftechnique
for establishing this high probability result significantly deviates from the existing high probability analysis
of methods using gradient clipping. The formal proof can be found in Appendix B.2.
Note that Theorem 4 does not make any unbiasedness or decreasing stepsize assumptions. Furthermore,
when comparing this result with its in-expectation counterpart (see Proposition 14), the bound can be
interpreted as a concentration inequality around the expected value. Crucially, compared to (Cutkosky and
Mehta, 2021), Theorem 4 does not require the additional clipping step, effectively reducing the need to tune
an additional parameter and aligning theory with practice. Additionally, we do not require their (stronger)
bounded non-central moment assumption.
We next apply Theorem 4 to minibatch-NSGD with optimal parameters. A parameter-free version can
be found in Appendix B.2.3. To the best of our knowledge, we are the first work to show a high-probability
result without requiring strong noise assumptions or clipping.6
5Weignorenon-leadingtermsandsimplifytherateintheirfavour.
6Note that there are works such as (Armacki et al., 2023) that provide high-probability guarantees for NSGD under strong
noise assumptions.
7Corollary 5. Assume(LowerBoundedness), (L-smoothness)and(p-BCM)withp∈(1,2]. Thentheiterates
(cid:113) (cid:24) (cid:26) (cid:16) (cid:17) p (cid:27)(cid:25)
generated by minibatch-NSGD with parameters η t ≡ L∆ T1 and B t ≡ max 1, ∆σ2 1T L 2p−2 satisfy
√
T
T1 (cid:88)
∥∇F(x t)∥≤(11+30log(1/δ))
√∆ T1L
t=1
with probability at least 1−δ. This corresponds to O(cid:101)(cid:16) ∆1L + ∆1L(cid:0)σ(cid:1) p−p 1(cid:17) sample complexity.
ε2 ε2 ε
This result is optimal in ∆ ,L,σ and ε. We are not aware of any lower bounds specifying the optimal
1
δ-dependence. Forcomparison,Nguyenetal.(2023a)derivedhigh-probabilityconvergenceof Clip-SGDwith
a sample complexity of
 
3p−2 (cid:16) σ2p (cid:17)3 pp −− 12 +(cid:0) ∆ Lσ2(cid:1) 43 pp −− 42
O(cid:101) (∆ 1L)4p−4
+
(∆1L)2−p 1 
,
 ε6 2p p− −4
1
ε3 pp −− 12 
which is suboptimal in all parameters besides ε in the stochastic case. In the deterministic case, even the
dependence on ε appears suboptimal. In contrast, our result is noise adaptive in the sense that, for σ = 0,
the optimal deterministic iteration complexity is obtained. In particular, this result closes open questions
posed by Liu et al. (2023).
WhileTheorem4canbeappliedtoshowhigh-probabilityconvergenceofvanillaNSGDtoaσ-neighbourhood,
technical difficulties prevent us from extend it to NSGD with momentum. We discuss these difficulties empir-
ically in Section 4, and theoretically in Appendix C.3.
3.3 Can we Improve the Convergence Measure of Normalized SGD?
One may observe that the convergence of NSGD above is stated in terms of the average gradient norm, which
is different from the average of squared gradient norm that is commonly used in non-convex optimization.
By Jensen’s inequality it is straightforward to see that
(cid:118)
(cid:117) T T
(cid:117) (cid:116)1 (cid:88) ∥∇F(x )∥2 ≥ 1 (cid:88) ∥∇F(x )∥. (6)
T t T t
t=1 t=1
This raises a natural question whether this different convergence measure is a limitation of our analysis or
an intrinsic property of the algorithm. The following result shows that this is indeed an intrinsic property of
the algorithm, by providing a lower bound on the second moment of the gradient norm.
Theorem 6. There exists an L-smooth function F: R → R such that if minibatch-NSGD with parameters
as in Corollary 3 is run from any initial point x >0 for T ≥18 iterations, then we have
1
(cid:118) (cid:117) T √
(cid:117) (cid:116)1 (cid:88) ∥∇F(x )∥2 ≥ 2 √∆ 1L ·T1/4, while
T t 3 T
t=1
√
T
1 (cid:88)
∥∇F(x )∥≤6
√∆ 1L
(by Corollary 3).
T t T
t=1
The above result implies that even if we select the optimal step-size parameter (to minimize the upper
bound on 1 (cid:80)T ∥∇F(x )∥), minibatch-NSGD does not achieve optimal rates in terms of the stronger
T t=1 t
(cid:113)
measure 1 (cid:80)T ∥∇F(x )∥2. Specifically, the convergence rate for the latter measure is worse at least by
T t=1 t
(cid:113)
a factor of Θ(T1/4). Our Theorem 20 implies that the step-size η t = L∆ T1 is essentially the only order of
step-size to attain the optimal convergence rate in terms of 1 (cid:80)T ∥∇F(x )∥ (up to a numerical constant).
T t=1 t
Combined with inequality (6) and Theorem 6, it implies that there is no predefined constant step-size for
(cid:113)
NSGD that can guarantee optimal convergence when the rate is measured by 1 (cid:80)T ∥∇F(x )∥2.
T t=1 t
84 Experiments
Inthissection, wepresentexperimentsdesignedtoempiricallymotivateandvalidatethetheoreticalfindings
of this paper. Since heavy tails have prominently been observed in language modelling tasks (Zhang et al.,
2020), our experiments target this task.
Experimental Setup. We conduct training on the Penn Treebank (PTB) (Marcus et al., 1993) and
WikiText-2 (Merity et al., 2017) datasets using the AWD-LSTM architecture (Merity et al., 2018). Hy-
perparameters of the model and batchsizes were chosen according to (Merity et al., 2018). To observe the
exact optimization behaviour of algorithms, the averaging mechanism of the model was disabled. Additional
licensing and compute information can be found in Appendix E.
In order to examine the behaviour of Clip-SGD and compare it to NSGD, we tuned their respective pa-
rameters using a course grid search in a 50 epoch training. For NSGD we considered the stepsizes η = ηt−r
t
and tuned η and r. For Clip-SGD we considered the same stepsizes and additionally tuned the clipping
threshold γ. The parameters resulting on the above described tuning scheme on the PTB dataset were
(η,r,γ) = (50,0.1,0.25) for Clip-SGD and (η,r) = (50,0.25) for NSGD. It should be noted that the observed
optimal clipping threshold γ = 0.25 is in line with the previous empirical work by Merity et al. (2018) that
introducedtheAWD-LSTM.TheresultingparametersontheWikiText-2datasetwere(η,r,γ)=(30,0,0.15)
for Clip-SGD and (η,r) = (15,0.1) for NSGD. The final training was then carried out for 300 epochs on the
seeds 0,1970,2000,2024 and 2112.
Motivation and Validation. Figure1showsthebehaviourof Clip-SGDandNSGDwiththeircorrespond-
ing tuned parameters on both datasets. Dashed lines represent the proportion of stochastic gradients that
got clipped by Clip-SGD per epoch. We want to discuss two observations. First, perhaps surprisingly, we
observeonbothdatasetsthatthepercentageofclippinggradientsincreasesforClip-SGD,contradictingtheo-
reticalinsightsasdiscussedinSection1.1. Interestingly, Clip-SGDeventuallyclipseveryiteration, becoming
equivalent to NSGD after a certain number of epochs. Second, it can be noted that both algorithms perform
similarlywhenmeasuredwiththeircorrespondingtrainingloss,depictedwithsolidlines,despiteNSGDhaving
one less parameter and hence requiring substantially less time to tune.
6.0 5.0
1.0 1.0
4.8
5.5
Clip-SGD 0.8 Clip-SGD 0.8
NSGD 4.6 NSGD
5.0
0.6 0.6
4.4
4.5
0.4 4.2 0.4
4.0
4.0
0.2 0.2
3.5 3.8
0.0 0.0
0 50 100 150 200 250 300 0 50 100 150 200 250 300
Epoch Epoch
(a) PTB (b) WikiText-2
Figure 1: All plots consider Clip-SGD and NSGD with tuned parameters. Solid lines represent the training
loss and correspond to the left y-axis. The dashed line correspond to the right y-axis, and represent the
percentage of clipped gradients by Clip-SGD in an epoch. Shaded areas represent the minimal and maximal
value within 5 seeds, the line the median.
High probability convergence. In this set of experiments, we verify high probability convergence of
NSGD, and NSGD-M on a strongly convex quadratic function under heavy tailed noise. We run each algorithm
√ √
(cid:112)
for k = 105 times with default parameters: η = 1/ t for NSGD and η = α /t, α = 1/ t for NSGD-M see
t t t t
(20)inAppendixC.Figure2(left)visualizestheconvergencebehaviourbyselectingthemedianalongwithδ
9
ssoL
niarT
deppilc
%
ssoL
niarT
deppilc
%ParetoOracle, d: 1000
NSGD 0.525 NSGD
1.4 NSGDM NSGDM
0.500
1.2 0.475
1.0
0.450
0.8 0.425
0.6 0.400
0.375
0.4
0.350
0 20 40 60 80 100 2 4 6 8
T log(1
)
Figure2: VerifyinghighprobabilityconvergenceofNSGDandNSGD-M.Weusef(x,ξ)= 1∥x∥2+⟨x,ξ⟩,where
2 2
ξ is a random vector with i.i.d.components drawn from a symmetrized Pareto distribution with tail index
p=2.5. For NSGD-M, the quantiles of average gradient norm (left plot) exhibit a super-linear dependence on
log(1/δ) indicating the lack of high probability convergence.
and1−δ quantilesofthealgorithmrunsbasedontheaveragegradientnormatT =100,whereδ :=1−10−4.
We observe that the δ quantile run of NSGD-M deviates significantly from the median compared to that of
NSGD from its own median. Figure 2 (right) plots the average gradient norm at T = 100 corresponding
to different values of quantiles δ. NSGD-M shows a super-linear dependence on log(1/δ), indicating that it
may not achieve high probability convergence as effectively as NSGD even in the presence of BV noise. This
suggests that extending high probability bounds to NSGD-M is more challenging due to momentum’s impact
on the dynamics. We refer to Appendix E.3 for additional experiments including comparison with light tail
noise and SGD.
5 Conclusion
This work analyzes Normalized SGD under heavy-tailed noise. Our theoretical analysis reveals several in-
teresting insights. First, we extend our understanding of high-probability convergence under heavy tailed
noise, providing the first such guarantee with an algorithm that does not require gradient clipping. Second,
we tightly characterize the optimal sample complexity in all parameters under the (p-BCM) assumption.
Lastly, our results for parameter-free NSGD suggest the robustness of the algorithm to misspecification of its
parameters. Additionally, our algorithm specific lower bounds in Theorem 2 and Theorem 6 allow additional
insights into the behaviour of NSGD.
Several open questions arise from this work for future research. For instance, it remains unclear whether
ourhigh-probabilityresultcanbeextendedtoNSGDwithmomentumorvariancereducedgradientestimators.
More importantly, it remains open whether the sample complexity that is optimal for parameter-dependent
algorithms (5) can be achieved by any algorithm without knowledge of problem parameters.
Acknowledgements
The work is supported by ETH research grant, ETH AI Center Doctoral Fellowship and Swiss National
Science Foundation (SNSF) Project Funding No. 200021-207343.
References
M.Abadi, A.Chu, I.Goodfellow, H.B.McMahan, I.Mironov, K.Talwar, andL.Zhang. Deeplearningwith
differentialprivacy. InProceedingsofthe2016ACMSIGSACconferenceoncomputerandcommunications
security, pages 308–318, 2016.
10
T
||)tx(F
||
1 T
1=t
T
||)tx(F
||
1 T
fo
selitnauQ
1=tK. Ahn, X. Cheng, M. Song, C. Yun, A. Jadbabaie, and S. Sra. Linear attention is (maybe) all you need (to
understand transformer optimization). In International Conference on Learning Representations, 2024.
Y. Arjevani, Y. Carmon, J. C. Duchi, D. J. Foster, N. Srebro, and B. Woodworth. Lower bounds for non-
convex stochastic optimization. Mathematical Programming, 199(1-2):165–214, 2023.
A.Armacki,P.Sharma,G.Joshi,D.Bajovic,D.Jakovetic,andS.Kar. High-probabilityConvergenceBounds
for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise. arXiv preprint arXiv:2310.18784,
2023.
A.Barakat,I.Fatkhullin,andN.He.Reinforcementlearningwithgeneralutilities: Simplervariancereduction
and large state-action space. In International Conference on Machine Learning, pages 1753–1800, 2023.
B. Battash, L. Wolf, and O. Lindenbaum. Revisiting the noise model of stochastic gradient descent. In
International Conference on Artificial Intelligence and Statistics, pages 4780–4788. PMLR, 2024.
J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar. Signsgd: Compressed optimisation for
non-convex problems. In International Conference on Machine Learning, pages 560–569. PMLR, 2018.
L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale machine learning. SIAM
review, 60(2):223–311, 2018.
S. Bubeck, N. Cesa-Bianchi, and G. Lugosi. Bandits with heavy tail. IEEE Transactions on Information
Theory, 59(11):7711–7717, 2013.
Y. Carmon, J. C. Duchi, O. Hinder, and A. Sidford. Lower Bounds for Finding Stationary Points I. Mathe-
matical Programming, 184(1):71–120, Nov 2020. ISSN 1436-4646.
S. Cayci and A. Eryilmaz. Provably robust temporal difference learning for heavy-tailed rewards. Advances
in Neural Information Processing Systems, 36, 2024.
Y.Cherapanamjeri,N.Tripuraneni,P.Bartlett,andM.Jordan.Optimalmeanestimationwithoutavariance.
In Conference on Learning Theory, pages 356–357. PMLR, 2022.
M. Crawshaw, M. Liu, F. Orabona, W. Zhang, and Z. Zhuang. Robustness to unbounded smoothness of
generalized signsgd. Advances in Neural Information Processing Systems, 35:9955–9968, 2022.
A.CutkoskyandH.Mehta. MomentumimprovesnormalizedSGD. InInternational Conference on Machine
Learning, volume 119, pages 2260–2268. PMLR, 2020.
A.CutkoskyandH.Mehta. High-probabilityboundsfornon-convexstochasticoptimizationwithheavytails.
Advances in Neural Information Processing Systems, 34:4883–4895, 2021.
Y. Dai, K. Ahn, and S. Sra. The crucial role of normalization in sharpness-aware minimization. Advances in
Neural Information Processing Systems, 36, 2024.
D. Davis, D. Drusvyatskiy, L. Xiao, and J. Zhang. From low probability to high confidence in stochastic
convex optimization. The Journal of Machine Learning Research, 22(1):2237–2274, 2021.
R.Durrett.Probability: TheoryandExamples.CambridgeSeriesinStatisticalandProbabilisticMathematics.
Cambridge University Press, 5 edition, 2019.
I. Fatkhullin, A. Barakat, A. Kireeva, and N. He. Stochastic policy gradient methods: Improved sample
complexity for Fisher-non-degenerate policies. In International Conference on Machine Learning, pages
9827–9869, 2023.
S.Ganesh,W.U.Mondal,andV.Aggarwal. Variance-reducedpolicygradientapproachesforinfinitehorizon
average reward markov decision processes. arXiv preprint arXiv:2404.02108, 2024.
S. Garg, J. Zhanson, E. Parisotto, A. Prasad, Z. Kolter, Z. Lipton, S. Balakrishnan, R. Salakhutdinov, and
P. Ravikumar. On proximal policy optimization’s heavy-tailed gradients. In International Conference on
Machine Learning, pages 3610–3619. PMLR, 2021.
11S. Ghadimi and G. Lan. Stochastic first-and zeroth-order methods for nonconvex stochastic programming.
SIAM Journal on Optimization, 23(4):2341–2368, 2013.
I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2016.
E.Gorbunov,M.Danilova,andA.Gasnikov. Stochasticoptimizationwithheavy-tailednoiseviaaccelerated
gradient clipping. Advances in Neural Information Processing Systems, 33:15042–15053, 2020.
E.Gorbunov,A.Sadiev,M.Danilova,S.Horváth,G.Gidel,P.Dvurechensky,A.Gasnikov,andP.Richtárik.
High-probability convergence for composite and distributed stochastic minimization and variational in-
equalities with heavy-tailed noise. In International Conference on Machine Learning, pages 15951–16070,
2024.
E.Hazan,K.Levy,andS.Shalev-Shwartz.Beyondconvexity: Stochasticquasi-convexoptimization.Advances
in Neural Information Processing Systems, 28, 2015.
R. Howell. On Asymptotic Notation with Multiple Variables. Tech. Rep., 2008.
F. Hübler, J. Yang, X. Li, and N. He. Parameter-Agnostic Optimization under Relaxed Smoothness. In
International Conference on Artificial Intelligence and Statistics, pages 4861–4869, 2024.
S. P. Karimireddy, Q. Rebjock, S. Stich, and M. Jaggi. Error feedback fixes SignSGD and other gradient
compression schemes. In International Conference on Machine Learning, 2019.
A. Koloskova, H. Hendrikx, and S. U. Stich. Revisiting gradient clipping: Stochastic bias and tight conver-
gence guarantees. In International Conference on Machine Learning, pages 17343–17363. PMLR, 2023.
N. Kornilov, O. Shamir, A. Lobanov, D. Dvinskikh, A. Gasnikov, I. Shibaev, E. Gorbunov, and S. Horváth.
Accelerated zeroth-order method for non-smooth stochastic convex optimization problem with infinite
variance. Advances in Neural Information Processing Systems, 36, 2024.
K. Levy. Online to offline conversions, universality and adaptive minibatch sizes. Advances in Neural Infor-
mation Processing Systems, 30, 2017.
K. Y. Levy. The power of normalization: Faster evasion of saddle points. arXiv preprint arXiv:1611.04831,
2016.
S. Li and Y. Liu. High probability guarantees for nonconvex stochastic gradient descent with heavy tails. In
International Conference on Machine Learning, pages 12931–12963, 2022.
S.LiandY.Liu. HighProbabilityAnalysisforNon-ConvexStochasticOptimizationwithClipping. InECAI
2023, pages 1406–1413. IOS Press, 2023.
X. Li and F. Orabona. A high probability analysis of adaptive sgd with momentum. arXiv preprint
arXiv:2007.14294, 2020.
L. Liu, Y. Wang, and L. Zhang. High-Probability Bound for Non-Smooth Non-Convex Stochastic Optimiza-
tion with Heavy Tails. In International Conference on Machine Learning, pages 32122–32138, 2024.
S. Liu, P.-Y. Chen, X. Chen, and M. Hong. Signsgd via zeroth-order oracle. In International Conference on
Learning Representations, 2019.
Z. Liu and Z. Zhou. Stochastic nonsmooth convex optimization with heavy-tailed noises. arXiv preprint
arXiv:2303.12277, 2023.
Z.Liu,J.Zhang,andZ.Zhou.BreakingtheLowerBoundwith(Little)Structure: AccelerationinNon-Convex
Stochastic Optimization with Heavy-Tailed Noise. In Conference on Learning Theory, pages 2266–2290,
2023.
L. Madden, E. Dall’Anese, and S. Becker. High probability convergence bounds for non-convex stochastic
gradient descent with sub-weibull noise. Journal of Machine Learning Research, 25(241):1–36, 2024.
12V. V. Mai and M. Johansson. Stability and convergence of stochastic gradient clipping: Beyond lipschitz
continuity and smoothness. In International Conference on Machine Learning, pages 7325–7335. PMLR,
2021.
M. P. Marcus, M. A. Marcinkiewicz, and B. Santorini. Building a large annotated corpus of English: the
penn treebank. Comput. Linguist., 19(2):313–330, jun 1993. ISSN 0891-2017.
S. Merity, C. Xiong, J. Bradbury, and R. Socher. Pointer Sentinel Mixture Models. In International Confer-
ence on Learning Representations, 2017.
S. Merity, N. S. Keskar, and R. Socher. Regularizing and Optimizing LSTM Language Models. In Interna-
tional Conference on Learning Representations, 2018.
M. Mohammadi, A. Mohammadpour, and H. Ogata. On estimating the tail index and the spectral measure
of multivariate α-stable distributions. Metrika, 78(5):549–561, 2015.
A. V. Nazin, A. S. Nemirovsky, A. B. Tsybakov, and A. B. Juditsky. Algorithms of robust stochastic
optimization based on mirror descent method. Automation and Remote Control, 80:1607–1627, 2019.
A. Nemirovskij and D. Yudin. Problem complexity and method efficiency in optimization. SIAM Review,
1983.
Y. Nesterov. Lectures on convex optimization, volume 137. Springer, 2018.
Y. E. Nesterov. Minimization methods for nonsmooth convex and quasiconvex functions. Matekon, 29(3):
519–531, 1984.
T. D. Nguyen, T. H. Nguyen, A. Ene, and H. Nguyen. Improved convergence in high probability of clipped
gradient methods with heavy tailed noise. In Advances in Neural Information Processing Systems, vol-
ume 36, pages 24191–24222, 2023a.
T. D. Nguyen, T. H. Nguyen, A. Ene, and H. L. Nguyen. High probability convergence of clipped-sgd under
heavy-tailed noise. arXiv preprint arXiv:2302.05437, 2023b.
R. Pascanu, T. Mikolov, and Y. Bengio. On the difficulty of training recurrent neural networks. In Interna-
tional Conference on Machine Learning, pages 1310–1318. Pmlr, 2013.
I. Pinelis. On the von bahr–esseen inequality. arXiv preprint arXiv:1008.5350, 2010.
I.Pinelis. Bestpossibleboundsofthevonbahr–esseentype. Annals of Functional Analysis, 6(4):1–29, 2015.
N.Puchkin,E.Gorbunov,N.Kutuzov,andA.Gasnikov. Breakingtheheavy-tailednoisebarrierinstochastic
optimizationproblems. InInternationalConferenceonArtificialIntelligenceandStatistics,pages856–864.
PMLR, 2024.
A.Sadiev,M.Danilova,E.Gorbunov,S.Horváth,G.Gidel,P.Dvurechensky,A.Gasnikov,andP.Richtárik.
High-probability bounds for stochastic optimization and variational inequalities: the case of unbounded
variance. In International Conference on Machine Learning, 2023.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization algorithms.
arXiv preprint arXiv:1707.06347, 2017.
H. Sha, Y. Cao, Y. Liu, Y. Wu, R. Liu, and H. Chen. Clip body and tail separately: High probability
guarantees for DPSGD with heavy tails. arXiv preprint arXiv:2405.17529, 2024.
U. Simsekli, L. Sagun, and M. Gurbuzbalaban. A tail-index analysis of stochastic gradient noise in deep
neural networks. In International Conference on Machine Learning, pages 5827–5837. PMLR, 2019.
T.Sun,Q.Wang,D.Li,andB.Wang. Momentumensuresconvergenceofsignsgdunderweakerassumptions.
In International Conference on Machine Learning, pages 33077–33099. PMLR, 2023.
13H.Touvron,T.Lavril,G.Izacard,X.Martinet,M.-A.Lachaux,T.Lacroix,B.Rozière,N.Goyal,E.Hambro,
F.Azhar,etal.Llama: OpenandEfficientFoundationLanguageModels.arXivpreprintarXiv:2302.13971,
2023.
B. von Bahr and C.-G. Esseen. Inequalities for the r-th absolute moment of a sum of random variables,
1≤r ≤2. The Annals of Mathematical Statistics, pages 299–303, 1965.
H. Wang, M. Gurbuzbalaban, L. Zhu, U. Simsekli, and M. A. Erdogdu. Convergence Rates of Stochastic
Gradient Descent under Infinite Noise Variance. In Advances in Neural Information Processing Systems,
volume 34, pages 18866–18877, 2021.
J. Yang, X. Li, I. Fatkhullin, and N. He. Two sides of one coin: the limits of untuned SGD and the power of
adaptive methods. Advances in Neural Information Processing Systems, 36, 2024.
J. Zhang, S. P. Karimireddy, A. Veit, S. Kim, S. Reddi, S. Kumar, and S. Sra. Why are adaptive methods
good for attention models? Advances in Neural Information Processing Systems, 33:15383–15393, 2020.
J.Zhang,C.Ni,C.Szepesvari,M.Wang,etal. Ontheconvergenceandsampleefficiencyofvariance-reduced
policy gradient method. Advances in Neural Information Processing Systems, 34:2228–2240, 2021.
S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin, et al.
Opt: Open Pre-trained Transformer Language Models. arXiv preprint arXiv:2205.01068, 2022.
S.-Y. Zhao, Y.-P. Xie, and W.-J. Li. On the convergence and improvement of stochastic normalized gradient
descent. Science China Information Sciences, (3):132103, 2021.
14Contents
1 Introduction 1
1.1 Drawbacks of Gradient Clipping Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Our Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Preliminaries 4
3 Main Results 5
3.1 Normalized SGD can Handle Heavy Tailed Noise . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 Convergence with High-Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3.3 Can we Improve the Convergence Measure of Normalized SGD? . . . . . . . . . . . . . . . . . 8
4 Experiments 9
5 Conclusion 10
A Technical Results 16
B Upper-Bounds for NSGD 19
B.1 In-Expectation Upper-Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.1.1 Proof of Proposition 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.1.2 Proof of Corollary 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.2 High-Probability Upper-Bounds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.2.1 Proof of Theorem 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.2.2 Proof of Corollary 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.2.3 Parameter-Free High-Probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C Upper-Bounds for NSGD with Momentum 25
C.1 Parameter-Free . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
C.2 Optimal Sample Complexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
C.3 Technical Difficulties of Proving High Probability Convergence . . . . . . . . . . . . . . . . . 28
D Lower-Bounds for NSGD 29
D.1 Deterministic Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
D.2 Stochastic Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
D.3 Lower-Bound on the Convergence Measure. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
E Additional Experiments and Details 36
E.1 Additional Details on Language Modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
E.2 Illustrating Drawbacks of Gradient Clipping Theory . . . . . . . . . . . . . . . . . . . . . . . 36
E.3 Verifying High Probability Bounds for SGD and NSGD(M) . . . . . . . . . . . . . . . . . . . 37
15A Technical Results
Thissectioncontainsvarioustechnicalresultsrequiredforouranalysis. Westartwithtwolemmasthatarise
due to the normalization in NSGD. Slightly different formulations of these were used by Zhao et al. (2021).
Lemma 7. For all a,b∈Rd with b̸=0 we have
a⊤b
≥∥a∥−2∥a−b∥.
∥b∥
Proof. We calculate
a⊤b (a−b)⊤b
= +∥b∥≥−∥a−b∥+∥b∥≥∥a∥−2∥a−b∥,
∥b∥ ∥b∥
where we used Cauchy-Schwarz in the first, and ∥a∥≤∥a−b∥+∥b∥ in the second inequality.
Lemma 8 (Expected Angle Bound). Let (Ω,A,P) be a probability space and X: Ω → Rd a random vector.
Furthermore let µ∈Rd\{0},σ :=E[∥X−µ∥] and suppose that X ̸=0 almost surely. Then it holds that
(cid:20) µ⊤X (cid:21) σ
E ≥1−2 .
∥µ∥∥X∥ ∥µ∥
Proof. We apply Lemma 7 with a←µ and b←X to derive
µ⊤X
≥∥µ∥−2∥µ−X∥.
∥X∥
Dividing both sides by ∥µ∥ and taking expectations yields the claim.
Thenextlemmashowsthatt-dependentparametershavethesame (uptoconstants)behaviorasconstant,
T-dependent, parameters in NSGD.
Lemma 9 (see Hübler et al. (2024, Lemma 10)). Let q ∈(0,1),r ∈[0,1] and T ∈N . Then
≥2
T T (cid:18) (cid:19)
(cid:88) t−r (cid:89) (cid:0) 1−τ−q(cid:1) ≤2exp 1 (T +1)q−r.
1−q
t=1 τ=t+1
To control the error of momentum or mini-batch gradient estimator, we use a von Bahr and Esseen type
inequality stated in Lemma 10. This result was initially proved by von Bahr and Esseen (1965) for d = 1.
Later, it was studied and generalized in various ways, e.g., by Pinelis (2010, 2015). An alternative proof
was rediscovered by Cherapanamjeri et al. (2022) for the one dimensional i.i.d. case and extended to higher
dimensions by Kornilov et al. (2024). The extension in the latter work has some inaccuracies, which we fix
below.
Lemma10. Letp∈[1,2],andX ,...,X ∈Rd beamartingaledifferencesequence(MDS),i.e.,E[X |X ,...,X ]=
1 n j j−1 1
0 a.s. for all j =1,...,n satisfying
E[∥X ∥p]<∞ for all j =1,...,n.
j
Define S
:=(cid:80)n
X , then
n j=1 j
n
E[∥S ∥p]≤2(cid:88) E[∥X ∥p].
n j
j=1
16Proof. Theclaimistrueford=1,see(vonBahrandEsseen,1965,Equation(4)). Namely,lety ,...,y ∈R
1 n
be a MDS, i.e., E[y |y ,...,y ]=0 a.s. for all j =1,...,n, satisfying
j j−1 1
E[|y |p]<∞ for all j =1,...,n. (7)
j
Then
(cid:12) (cid:12)p
(cid:12) n (cid:12) n
E (cid:12) (cid:12)(cid:88) y j(cid:12) (cid:12) ≤2(cid:88) E[|y j|p]. (8)
(cid:12) (cid:12)
(cid:12)j=1 (cid:12) j=1
Following(Kornilovetal.,2024),defineg ∼N(0,I)andy :=g⊤X ,whereg isindependentofX . Weneed
j j j
to verify that y ,...,y defined this way is indeed a MDS. Define the sigma algebra H := σ(y ,...,y )
1 n 1 j−1 1
and H :=σ(X ,...,X ,g). Observe that H ⊂H and by the tower property
2 j−1 1 1 2
E(cid:2) g⊤X |H (cid:3) =E(cid:2)E(cid:2) g⊤X |H (cid:3) |H (cid:3) =E(cid:2) g⊤E[X |H ]|H (cid:3) =0,
j 1 j 2 1 j 2 1
where the second equality holds because g is H -measurable and the last equality holds by independence
2
of X and g, and the assumption that E[X |X ,...,X ]=0 a.s. Next, we need to verify that E[|y |p]<
j j j−1 1 j
∞. We know that g⊤a ∼ N(0,∥a∥2) for any vector a ∈ Rd. Therefore, E[|y |p|X ] = E(cid:2) |g⊤X |p|X (cid:3) =
j j j j
C(p)∥X
j∥p,withC(p):=2p/2Γ( √p+
2
π1)
,whereweusedthep-thabsolutemomentofnormaldistributionapplied
to a random variable g⊤X given X . Taking full expectation, we get
j j
E[|y |p]=C(p)E[∥X ∥p]<∞. (9)
j j
We have verified that the sequence y ,...,y is a MDS and property (7) holds, thus we are ready to apply
1 n
(8). Using the p-th moment of normal distribution applied to g⊤S given S , we have
n n
(cid:12) (cid:12)p
(cid:12) n (cid:12) n
C(p)E[∥S n∥p]=E(cid:2)E(cid:2) |g⊤S n|p|S n(cid:3)(cid:3) =E (cid:12) (cid:12)(cid:88) y j(cid:12) (cid:12) ≤2(cid:88) E[|y j|p],
(cid:12) (cid:12)
(cid:12)j=1 (cid:12) j=1
where in the last step we used (8). It remains to use (9) to conclude the proof.
Weusethefollowingmartingaleconcentrationinequalityforourhigh-probabilityguarantees, seee.g.,(Li
and Orabona, 2020, Lemma 1).
Lemma11. Let(F ) beaFiltrationand(D ) aMartingaleDifferenceSequencewithrespectto(F ) .
t t∈N t t∈N t t∈N
Furthermore, for each t ∈ N ≥1, let σ
t
be F t−1-measurable and assume that
E(cid:104) exp(cid:16)
D
σ2t2(cid:17)(cid:12)
(cid:12) (cid:12)F
t−1(cid:105)
≤ e. Then,
for all T ∈N, t
(cid:32) T T (cid:18) (cid:19)(cid:33)
(cid:88) 3 (cid:88) 1 1
∀λ>0,δ ∈(0,1): P D ≤ λ σ2+ log ≥1−δ.
t 4 t λ δ
t=1 t=1
In order to apply Lemma 10, we require the following lemma on conditional expectations.
Lemma12(c.f.Durrett(2019,Example4.1.7)). Let(Ω,F,P)beaprobabilityspaceandX,Y beindependent
random variables mapping to measurable spaces (E ,Σ ) and (E ,Σ ) respectively. Furthermore let h: E ×
1 1 2 2 1
E →Rd be a (Lebesgue-)measurable function with E[∥h(X,Y)∥]<∞. Then
2
E[h(X,Y)|X]a =.s. g(X), where g(x):=E[h(x,Y)].
Proof. Firstnotethat,byFubini’sTheorem,g isΣ /Bd measurableandhenceg(X)isσ(X)/Bd measurable.
1
Therefore it suffices to show that
E[h(X,Y)1 ]=E[g(X)1 ]
A A
17for all A ∈ σ(X). First note that, by definition of σ(X) = (cid:8) X−1(C): C ∈Σ (cid:9) , there exists B ∈ Σ with
1 1
A = X−1(B). Next, by independence of X and Y, their joint induced measure is a product measure µ×ν
on E ×E . Combining, we get
1 2
(cid:90) (cid:90)
E[h(X,Y)1 ]= h(X(ω),Y(ω))dP(ω)= h(x,y)1 (x)d(µ×ν)(x,y).
A B
A E1×E2
ByourassumptionE[∥h(X,Y)∥]<∞weknowthathisµ×ν integrableandFubini’sTheoremhenceyields
(cid:90) (cid:90) (cid:90) (cid:90)
h(x,y)1 (x)d(µ×ν)(x,y)= h(x,y)dν(y)1 (x)dµ(x)= g(x)1 (x)dµ(x)=E[g(X)1 ].
B B B A
E1×E2 E1 E2 E1
This completes the proof.
18B Upper-Bounds for NSGD
This section contains the proofs that are missing in the main part of the paper. Throughout this section
we denote the iterates generated by NSGD with (x ) . Furthermore, we denote the natural filtration of our
t t∈N
iterates by F :=σ(g ,...,g ).
t 1 t
We start by deriving a descent lemma. While such descent lemmas are well studied for NSGD in the
literature — to the best of our knowledge — none highlight the importance of the cosine between g and
t
∇F(x ). As this term will play a crucial role in our high-probability result, we will provide our version of
t
the descent lemma and its proof below.
Lemma 13 (Descent Lemma). Assume (Lower Boundedness) and (L-smoothness). Furthermore let
∇F(x )⊤g
ϕ := t t
t ∥∇F(x )∥∥g ∥
t t
denote the cosine between g and ∇F(x ). Then the iterates of NSGD satisfy
t t
T T
(cid:88) L(cid:88)
η ϕ ∥∇F(x )∥≤∆ + η2.
t t t 1 2 t
t=1 t=1
Proof. By the definition of x , (L-smoothness) implies
t+1
g L ∇F(x )⊤g L
F(x )−F(x )≤−η ∇F(x )⊤ t + η2 =−η t t ∥∇F(x )∥+ η2.
t+1 t t t ∥g ∥ 2 t t∥∇F(x )∥∥g ∥ t 2 t
t t t
Summing up over t∈[T] and telescoping now yields
T T
(cid:88) L(cid:88)
F∗−F(x )≤F(x )−F(x )≤− η ϕ ∥∇F(x )∥+ η2,
1 T+1 1 t t t 2 t
t=1 t=1
where we used (Lower Boundedness) in the first inequality. This completes the proof.
Thus,ifwecouldguaranteethattheanglebetweenthegradientoracleandtruegradientremainsbounded
away from zero, we would be done. Since this can however, even in expectation, not be guaranteed, we need
a more detailed analysis to prove our results.
B.1 In-Expectation Upper-Bounds
To prove our in-expectation results, we start with a unified analysis for normalized algorithms. This result
does not specify the exact gradient estimator, allowing to incorporate different gradient estimators and noise
assumptions afterward. This result was first derived by Cutkosky and Mehta (2020) in a slightly different
formulation.
Proposition14(c.f.CutkoskyandMehta(2020,Lemma2)). Assume(LowerBoundedness),(L-smoothness)
and ∞>σ :=E[∥g −∇F(x )∥]. Then the iterates (x ) generated by NSGD satisfy
t t t t t∈N
≥1
(cid:88)T η
t E[∥∇F(x )∥]≤
∆ 1+ L
2
(cid:80)T t=1η t2+2(cid:80)T t=1η tσ
t .
(cid:80)T
η
t (cid:80)T
η
t=1 τ=1 τ τ=1 τ
Note that for constant parameters η ≡η and σ ≡σ this result reduces to
t t
T
1 (cid:88) E[∥∇F(x )∥]≤ ∆ 1 + ηL +2σ. (10)
T t ηT 2
t=1
WeprovideaslightlydifferentproofofProposition14whencomparedto(CutkoskyandMehta,2020)below.
19Proof. Let ϕ :=
∇F(xt)⊤gt
denote the cosine between ∇F(x ) and g . Then, by Lemma 13, we have
t ∥∇F(xt)∥∥gt∥ t t
T T
(cid:88) L(cid:88)
η ϕ ∥∇F(x )∥≤∆ + η2. (11)
t t t 1 2 t
t=1 t=1
Next we apply Lemma 7 to get
E[ϕ ∥∇F(x )∥]≥E[∥∇F(x )∥−2∥g −∇F(x )∥]≥E[∥∇F(x )∥]−2σ ,
t t t t t t t
where we applied our assumption σ ≥E[∥g −∇F(x )∥] in the last inequality. Therefore, by taking expec-
t t t
tations in (11), we get
T T T
(cid:88) L(cid:88) (cid:88)
η E[∥∇F(x )∥]≤∆ + η2+2 η σ .
t t 1 2 t t t
t=1 t=1 t=1
Dividing by
(cid:80)T
η yields the claim.
τ=1 τ
B.1.1 Proof of Proposition 1
Now we are ready to prove Proposition 1.
Proof of Proposition 1. To shorten the notation we write η¯:= ηT−r and B¯ := ⌈max{1,BTq}⌉. Remember,
that we are considering the mini-batch gradient-estimator
B¯
g =
1 (cid:88) ∇f(cid:16)
x
,ξ(j)(cid:17)
.
t B¯ t t
j=1
We start by controlling the (conditional) expected deviation of g from ∇F(x ) using Lemma 10. Let x∈Rd
t t
and define X
j(x):=∇f(cid:16)
x,ξ
t(j)(cid:17)
−∇F(x) for all j
∈(cid:2) B¯(cid:3)
. Now note that X 1(x),...,X B¯(x) are independent
randomvariableswithmeanzeroandhenceaMartingaleDifferenceSequence(MDS).Furthermorenotethat
E[∥X (x)∥p]≤σp by (p-BCM) and we can hence apply Lemma 10 to get
j
(cid:13) (cid:13)p
(cid:13) B¯ (cid:13) B¯
g(x):=E (cid:13) (cid:13)(cid:88) X j(x)(cid:13) (cid:13) ≤2(cid:88) E[∥X j(x)∥p]≤2B¯σp. (12)
(cid:13) (cid:13)
(cid:13)j=1 (cid:13) j=1
Next we calculate
(cid:13) (cid:13)(cid:12) 
(cid:13) B¯ (cid:13)(cid:12)
E[∥g t−∇F(x t)∥|x t]=E (cid:13) (cid:13) (cid:13)B1 ¯ (cid:88)(cid:16) ∇f(x t,ξ t(j))−∇F(x t)(cid:17)(cid:13) (cid:13) (cid:13)(cid:12) (cid:12) (cid:12)x t
(cid:13) j=1 (cid:13)(cid:12)
(13)
(cid:13)
(cid:13) B¯
(cid:13) (cid:13)p(cid:12)
(cid:12)
1/p
≤ B1 ¯E (cid:13) (cid:13) (cid:13)(cid:88)(cid:16) ∇f(x t,ξ t(j))−∇F(x t)(cid:17)(cid:13) (cid:13)
(cid:13)
(cid:12) (cid:12) (cid:12)x t
(cid:13)j=1 (cid:13) (cid:12)
where we applied Jensen in the last inequality. Next define
(cid:13) (cid:13)p
Y =(cid:18) ξ t(1),...,ξ t(B¯)(cid:19) and h(x t,Y)=(cid:13) (cid:13) (cid:13) (cid:13)(cid:88)B¯ (cid:16) ∇f(x t,ξ t(j))−∇F(x t)(cid:17)(cid:13) (cid:13) (cid:13)
(cid:13)
.
(cid:13)j=1 (cid:13)
and note that x and Y are independent. Hence we may apply Lemma 12 which yields
t
(13) 1 1 (12) σ
E[∥g −∇F(x )∥|x ] ≤ E[h(x ,Y)|x ]1/p Lem =. 12 g(x )1/p ≤ 2 (14)
t t t B¯ t t B¯ t B¯p− p1
20almost surely. By the tower property we get E[∥g t−∇F(x t)∥] = E[E[∥g t−∇F(x t)∥|x t]] ≤ 2σB¯−p− p1 and
plugging into (10) yields
T
1 (cid:88) E[∥∇F(x )∥]≤ ∆ 1 + η¯L + 4σ .
T
t=1
t η¯T 2 B¯p− p1
Using the definitions of η¯and B¯ we get
T
1 (cid:88) E[∥∇F(x )∥]≤ ∆ 1 + ηL + 4σ ≤ ∆ 1 + ηL + 4σ
T t ηT1−r 2Tr p−1 ηT1−r 2Tr p−1
t=1 ⌈max{1,BTq}⌉ p max{1,BTq} p
This implies an iteration complexity of
O(cid:32)(cid:18) ∆ 1(cid:19) 1−1 r +(cid:18) ηL(cid:19) r1
+
1 (cid:16)σ(cid:17) q(pp −1)(cid:33)
ηε ε B1/q ε
and hence a sample complexity of
O(cid:32)(cid:18) ∆ 1(cid:19) 1−1 r +(cid:18) ηL(cid:19) r1 +B(cid:18) ∆ 1(cid:19) 11 −+q r +B(cid:18) ηL(cid:19)1+ rq
+
1 (cid:16)σ(cid:17) qp (( p1 −+q 1) )(cid:33)
. (15)
ηε ε ηε ε Bq1 ε
This completes the proof.
B.1.2 Proof of Corollary 3
Finally we provide a slightly more refined analysis to prove Corollary 3.
Proof of Corollary 3. Applying Proposition 1 to our choice of parameters yields
√
T
1 (cid:88)
E[∥∇F(x )∥]≤2
√∆ 1L
+
4σ
.
T
t=1
t T max(cid:26) 1,(cid:16) σ2T(cid:17) 2pp −2(cid:27)p− p1
∆1L
We proceed with a case distinction.
√
Case 1: σ2T ≤1. In this case we get σ ≤ √∆1L and hence
∆1L T
(cid:32) (cid:40) (cid:18) σ2T (cid:19) 2pp −2(cid:41)(cid:33)−p− p1 √ ∆ L
4σ max 1, =4σ ≤4 √1 .
∆ 1L T
Case 2: σ2T >1. We calculate
∆1L
(cid:32) (cid:40) (cid:18) σ2T (cid:19) 2pp −2(cid:41)(cid:33)−p− p1 (cid:18) σ2T (cid:19)− 21 √ ∆ L
4σ max 1, =4σ =4 √1 .
∆ 1L ∆ 1L T
This implies an iteration complexity of
O(cid:0)
∆
Lε−2(cid:1)
and hence a sample complexity of
1
(cid:32) (cid:38) (cid:40) (cid:18) σ2(cid:19) 2pp −2(cid:41)(cid:39)(cid:33) (cid:18) ∆ L ∆ L(cid:16)σ(cid:17) p (cid:19)
O ∆ Lε−2· max 1, =O 1 + 1 p−1 .
1 ε2 ε2 ε2 ε
21B.2 High-Probability Upper-Bounds
Thissubsectioncontainstheproofsforourhigh-probabilityresults. WestartoffwiththeproofofTheorem4.
Theproofhingesontheobservationthat
∇F(xt)⊤gt
∈[−1,1]isboundedandhenceconcentrateswell. This
∥∇F(xt)∥∥gt∥
will allow us to apply Lemma 11 to get the mild log(1/δ) dependence.
B.2.1 Proof of Theorem 4
Proof of Theorem 4. Let ϕ :=
∇F(xt)⊤gt
denote the cosine between ∇F(x ) and g . Then, by Lemma 13,
t ∥∇F(xt)∥∥gt∥ t t
we have
T T
(cid:88) L(cid:88)
η ϕ ∥∇F(x )∥≤∆ + η2.
t t t 1 2 t
t=1 t=1
Next,weusethefactthatϕ isboundedandhencesharplyconcentratesaroundits(conditional)expectation.
t
Formally,letψ :=E[ϕ |F ]andnotethatD :=−η (ϕ −ψ )∥∇F(x )∥isamartingaledifferencesequence
t t t−1 t t t t t
with respect to (F ) . Furthermore, noting that
t t∈N
(cid:32) (cid:33) (cid:32) (cid:33)
D2 (ϕ −ψ )2
exp t =exp t t ≤e
4η2∥∇F(x )∥2 4
t t
implies that we may apply Lemma 11 with σ2 = 4η2∥∇F(x )∥2 (note the abuse of notation, this σ is
t t t t
unrelated to σ defined in the statement). Doing so yields, for all λ>0,
t
T T
(cid:88) L(cid:88) 1
η t(ψ t−3λη t∥∇F(x t)∥)∥∇F(x t)∥≤∆ 1+
2
η t2+ λlog(1/δ)
t=1 t=1
with probability at least 1−δ. Using (L-smoothness) we get ∥∇F(x )∥≤∥∇F(x
)∥+L(cid:80)t−1
η and hence
t 1 τ=1 τ
choosing λ:= 1 yields, with probability at least 1−δ,
6(η Tmax∥∇F(x1)∥+CTL)
T (cid:18) (cid:19) T
(cid:88) 1 L(cid:88)
η
t
ψ t−
2
∥∇F(x t)∥≤∆ 1+
2
η t2+6(η Tmax∥∇F(x 1)∥+C TL)log(1/δ). (16)
t=1 t=1
Finally we are left with the challenge of guaranteeing that ψ is large enough. Therefore we use Lemma 7
t
(cid:104) (cid:12) (cid:105)
to get ψ ∥∇F(x )∥ = E ∇F(xt)⊤gt (cid:12)F ≤ ∥∇F(x )∥−2E[∥g −∇F(x )∥|F ] = ∥∇F(x )∥−2σ and
t t ∥gt∥ (cid:12) t−1 t t t t−1 t t
hence
T T T
1(cid:88) L(cid:88) (cid:88)
2
η t∥∇F(x t)∥≤∆ 1+
2
η t2+2 η tσ t+6(η Tmax∥∇F(x 1)∥+C TL)log(1/δ).
t=1 t=1 t=1
Dividing by
1(cid:80)T
η yields the claim.
2 τ=1 τ
B.2.2 Proof of Corollary 5
Next we apply Theorem 4 to derive the high-probability result for tuned minibatch-NSGD.
Proof of Corollary 5. Toshortenthenotationwewriteη ≡ηandB ≡B. Firstnotethatx isσ(x )⊆F
t t t t t−1
measurable and ξ(1),...,ξ(B) are independent of F . In particular we have E[∥g −∇F(x )∥|F ] =
t t t−1 t t t−1
E[∥g −∇F(x )∥|x ] and hence may apply (14) to get
t t t
(14) 2σ
E[∥g −∇F(x )∥|F ]=E[∥g −∇F(x )∥|x ] ≤ , (17)
t t t−1 t t t p−1
B p
22Plugging into Theorem 4 now yields
T (cid:18) (cid:19)
T1 (cid:88) ∥∇F(x t)∥≤ 2 η∆ T1 +ηL+8σB−p− p1 +12 ∥∇F T(x 1)∥ +ηL log(1/δ)
t=1
√ √ √
(cid:18) (cid:19)
∆ L ∆ L ∥∇F(x )∥ ∆ L
= 2 √1 + √1 +8σB−p− p1 +12 1 + √1 log(1/δ) (18)
T T T T
√
∆ L
≤ (3+30log(1/δ)) √1 +8σB−p− p1 ,
T
√
where we used ∥∇F(x )∥≤ 2∆ L in the last inequality. We now proceed with a case distinction.
1 1
(cid:113)
Case 1: B =1. This implies σ ≤ ∆1L and hence
T
√
T
T1 (cid:88)
∥∇F(x t)∥≤(11+30log(1/δ))
√∆ T1L
.
t=1
(cid:16) (cid:17) p (cid:113)
Case 2: B = σ2T 2p−2. In this case we have σB1− pp = ∆1L and plugging into (18) yields
∆1L T
√
T
T1 (cid:88)
∥∇F(x t)∥≤(11+30log(1/δ))
√∆ T1L
.
t=1
This finishes the convergence result. To prove the oracle complexity, note that each iteration requires 1 and
(cid:16) (cid:17) p (cid:16) (cid:17)
∆σ2 1T
L
2p−2 oracle calls in Case 1 and 2 respectively. To reach an ε-stationary point, O ∆ 1Lε−2log(1/δ)2
iterations are required. Plugging into the oracle complexity per iteration yields the second claim.
B.2.3 Parameter-Free High-Probability
Similar to Proposition 1, we can also derive a parameter-free high-probability result for minibatch-NSGD.
Corollary 15. Assume (Lower Boundedness), (L-smoothness) and (p-BCM) with p ∈ (1,2]. Furthermore
let η,B,q > 0 and δ,r ∈ (0,1). Then the iterates generated by minibatch-NSGD with parameters η ≡ ηT−r
t
and B ≡⌈max{1,BTq}⌉ satisfy, with probability at least 1−δ,
t
√
T
T1 (cid:88) ∥∇F(x t)∥≤ η2 T∆ 1−1
r
+ η TL r(1+12log(1/δ))+17 ∆ T1L log(1/δ)+ 8σ
p−1
t=1 max{1,BTq} p
(cid:18) (cid:19)
In particular, the sample complexity is bounded by O(cid:101)
(cid:0)∆1(cid:1) 11 −+q
r
+(cid:0)L(cid:1)1+ rq +(cid:0)σ(cid:1) qp (( p1 −+q 1)
) .
ε ε ε
Proof. Toshortenthenotationwewriteη ≡η¯andB ≡B¯. Firstweapply(17)togetE[∥g −∇F(x )∥|F ]≤
t t t t t−1
2σ and plugging into Theorem 4 yields
p−1
B p
T (cid:18) (cid:19)
T1 (cid:88) ∥∇F(x t)∥≤ 2 η¯∆ T1 +η¯L+8σB¯−p− p1 +12 ∥∇F T(x 1)∥ +η¯L log(1/δ)
t=1
√
2∆ ηL 8σ ∆ L
≤ 1 + (1+12log(1/δ))+ +17 1 log(1/δ) (19)
ηT1−r Tr ⌈max{1,BTq}⌉p− p1 T
√
2∆ ηL ∆ L 8σ
≤ 1 + (1+12log(1/δ))+17 1 log(1/δ)+
ηT1−r Tr T p−1
max{1,BTq} p
√
where we used ∥∇F(x )∥≤ 2∆ L in the second inequality. This corresponds to an iteration complexity of
1 1
O(cid:101)(cid:32)(cid:18) ∆ 1(cid:19) 1−1 r +(cid:18) ηL(cid:19) r1
+
1 (cid:16)σ(cid:17) q(pp −1)(cid:33)
,
ηε ε Bq1 ε
23√ (cid:16)√ (cid:17) 1 (cid:16) √ (cid:17)1 (cid:18)(cid:16) (cid:17) 1 (cid:16) (cid:17)1(cid:19)
whereweused ∆1L ≤ ∆1 1−r + η L r =O ∆1 1−r + ηL r byYoung’sinequality. Finally,this
ε ηε ε ηε ε
implies a sample complexity of
O(cid:101)(cid:32)(cid:18) ∆ 1(cid:19) 1−1 r +(cid:18) ηL(cid:19) r1 +B(cid:18) ∆ 1(cid:19) 11 −+q r +B(cid:18) ηL(cid:19)1+ rq
+
1 (cid:16)σ(cid:17) qp (( p1 −+q 1) )(cid:33)
.
ηε ε ηε ε Bq1 ε
24C Upper-Bounds for NSGD with Momentum
In this section we discuss the version of our results for NSGD with momentum (NSGD-M), i.e., NSGD with the
gradient estimator
g =β g +(1−β )∇f(x ,ξ ), (20)
t t t−1 t t t
where g =0. Throughout this section we use the notation α :=1−β an β
:=(cid:81)b
β .
0 t t a:b κ=a κ
We first derive a deviation bound for g from ∇F(x ), similar to (14) but for the momentum estimator,
t t
generalising the bound in (Cutkosky and Mehta, 2020) to p<2.
Lemma 16. Let β = 0 and assume (L-smoothness), (p-BCM) with p ∈ (1,2]. Then the iterates generated
1
by NSGD-M satisfy
t (cid:32) t (cid:33)1/p
E[∥g −∇F(x )∥]≤L(cid:88) η β +2σ (cid:88)(cid:0) β (1−β )(cid:1)p .
t t τ−1 τ:t (τ+1):t τ
τ=2 τ=1
Proof. To simplify notation we first define
µ :=g −∇F(x ),
t t t
ε :=∇f(x ,ξ )−∇F(x ),
t t t t
S :=∇F(x )−∇F(x ).
t t−1 t
Now we calculate
g =β g +(1−β )∇f(x ,ξ )
t t t−1 t t t
=β (∇F(x )+µ )+(1−β )(ε +∇F(x ))
t t−1 t−1 t t t
=∇F(x )+(1−β )ε +β S +β µ
t t t t t t t−1
and unrolling yields
t t t t
(cid:88) (cid:88) (cid:88) (cid:88)
µ =β γ + β α ε + β S = β α ε + β S ,
t 2:t 1 (τ+1):t τ τ τ:t τ (τ+1):t τ τ τ:t τ
τ=2 τ=2 τ=1 τ=2
where we used β =0 in the second equality. Therefore
1
(cid:34)(cid:13) (cid:13)(cid:88)t (cid:13) (cid:13)(cid:35) (cid:88)t (cid:34)(cid:13) (cid:13)(cid:88)t (cid:13) (cid:13)p(cid:35)1/p (cid:88)t
E[∥µ ∥]≤E (cid:13) β α ε (cid:13) + β E[∥S ∥]≤E (cid:13) β α ε (cid:13) + β E[∥S ∥], (21)
t (cid:13) (τ+1):t τ τ(cid:13) τ:t τ (cid:13) (τ+1):t τ τ(cid:13) τ:t τ
(cid:13) (cid:13) (cid:13) (cid:13)
τ=1 τ=2 τ=1 τ=2
whereweappliedJenseninthesecondinequality.
ThesecondsumcanbeupperboundedbyL(cid:80)t
η β .
τ=2 τ−1 τ:t
To control the first sum we want to apply Lemma 10.
Therefore, to simplify notation, let C := β α and X := C ε . To check whether X ,...,X
τ (τ+1):t τ τ τ τ 1 t
satisfies the assumptions of Lemma 10, first note that, for all τ ∈[t],
E[X |X ,...,X ]=C E[∇f(x ,ξ )−∇F(x )|X ,...,X ]
τ 1 τ−1 τ τ τ τ 1 τ−1
and furthermore, as x is σ(X ,...,X ) measurable and ξ independent of X ,...,X , we have
τ 1 τ−1 τ 1 τ−1
E[∇f(x ,ξ )−∇F(x )|X ,...,X ]=E[∇f(x ,ξ )−∇F(x )|x ]=0,
τ τ τ 1 τ−1 τ τ τ τ
where we applied our unbiasedness assumption in conjunction with Lemma 12 in the last equality. By a
similar argument, using (p-BCM), we get
E[∥X ∥p]=CpE[E[∥∇f(x ,ξ )−∇F(x )∥p|x ]]≤Cpσp <∞.
τ τ τ τ τ τ τ
25Hence we may apply Lemma 10 to get
E(cid:34)(cid:13) (cid:13) (cid:13)(cid:88)t
β α ε
(cid:13) (cid:13) (cid:13)p(cid:35)1/p ≤(cid:32) 2(cid:88)t Cpσp(cid:33)1/p ≤2σ(cid:32) (cid:88)t
(cid:0)
β α
(cid:1)p(cid:33)1/p
(cid:13) (τ+1):t τ τ(cid:13) τ (τ+1):t τ
(cid:13) (cid:13)
τ=1 τ=1 τ=1
Combining these bounds with (21) yields
E[∥µ ∥](2 ≤1) E(cid:34)(cid:13) (cid:13) (cid:13)(cid:88)t β α ε (cid:13) (cid:13) (cid:13)p(cid:35)1/p +(cid:88)t β E[∥S ∥]≤2σ(cid:32) (cid:88)t (cid:0) β α (cid:1)p(cid:33)1/p +L(cid:88)t η β
t (cid:13) (τ+1):t τ τ(cid:13) τ:t τ (τ+1):t τ τ−1 τ:t
(cid:13) (cid:13)
τ=1 τ=2 τ=1 τ=2
and hence the claim.
C.1 Parameter-Free
Next we derive the NSGD-M counterpart to the parameter-free result (4). Additionally, the result is phrased
for decreasing stepsizes, outlining how results can be extended to those.
Corollary17(Parameter-AgnosticConvergence). LetT ≥3andassume(LowerBoundedness),(L-smoothness)
and (p-BCM) with p∈(1,2]. Then the iterates generated by NSGD with g =β g +(1−β )∇f(x ,ξ ) and
t t t−1 t t t
parameters β =1−t−1/2 and η =ηt−3/4 satisfy
t t
(cid:16) (cid:17)
1 (cid:88)T E[∥∇F(x )∥]≤ ∆ η1 +120ηLlog(T)+120σ 24 −p p T2 4− pp −1 .
T
t=1
t T1
4
(cid:16) (cid:17)
In particular, this corresponds to a rate of convergence of O(cid:101) (∆
1+L)T−1/4+σT−p 2− p1
and hence a sample
complexity of
O(cid:101)(cid:16) ∆4 1+L4 +(cid:0)σ(cid:1) p2 −p 1(cid:17)
.
ε4 ε
The proof follows similar steps as in (Cutkosky and Mehta, 2020), but requires additional attention to
the noise term to handle the case p<2.
Proof. To shorten notation we define r := 3/4,q := 1/2, and hence η
t
= ηt−r,β
t
= 1−t−q. Furthermore let
σ :=E[∥g −∇F(x )∥]. From Proposition 14 we get
t t t
T (cid:32) T (cid:33)−1(cid:32) T T (cid:33)
(cid:88) η t E[∥∇F(x )∥]≤ (cid:88) η ∆ + L(cid:88) η2+2(cid:88) η σ
(cid:80)T η t t 1 2 t t t
t=1 τ=1 τ t=1 t=1 t=1 (22)
(cid:32) T (cid:33)
≤Tr−1 ∆ 1 + 3 ηL+2(cid:88) t−rσ ,
η 2 t
t=1
where we used (cid:80)T η ≥ηT1−r and (cid:80)T η2 ≤3η2 in the second inequality. To control the third term, we
t=1 t t=1 t
apply Lemma 16 and Lemma 9 to get
(cid:88)T
t−rσ
t
≤4exp(cid:18) 1−1 q(cid:19) (cid:88)T (cid:16)
σt−r−qp− p1
+ηLt−2r+q(cid:17)
t=1 t=1
T
=4e2(cid:88)(cid:16)
σt−5p 4− p2
+ηLt−1(cid:17)
.
t=1
(cid:32) T (cid:33)
≤4e2 σ(cid:88) t−5p 4− p2 +ηL(1+log(T)) .
t=1
26In order to bound (cid:80)T t−5p 4− p2 we note that 5p−2 =1 iff p=2 and hence
t=1 4p

(cid:88) t=T 1t−5p 4− p2 ≤1+(cid:90) 1T t−5p 4− p2 dt≤ 1 1+ +l 1o −g 51( pT −2),
(cid:16)
T1−5p 4− p2
−1(cid:17)
,
i of thp e= rw2
ise.
4p
Now note that, due to L’Hôspital, lim 1 (cid:0) T1−q−1(cid:1) = log(T) and hence we can unify the cases by
q→1 1−q
writing the second expression and using continuous extensions. Plugging into (22) yields
(cid:88)T (cid:80)Tη
t
η
E[∥∇F(x t)∥]≤
Tr−1(cid:18) ∆
η1
+8e2ηL(1+log(T))+8e2σ(cid:18)
1+
24 −p p(cid:16)
T2 4− pp
−1(cid:17)(cid:19)(cid:19)
t=1 τ=1 τ
(cid:18) ∆ 4p (cid:16) (cid:17)(cid:19)
≤ T−1/4 1 +120ηLlog(T)+120σ T2 4− pp −1 ,
η 2−p
(cid:16) (cid:17)
where we used that 4p T2 4− pp −1 ≥ 1 for T ≥ 3 in the last inequality. The other statements follow from
2−p
the observation lim 1 (cid:0) T1−q−1(cid:1) =log(T).
q→1 1−q
C.2 Optimal Sample Complexity
Finally we provide the NSGD-M version of Corollary 3.
Corollary 18 (Optimal Oracle Complexity). Assume (Lower Boundedness), (L-smoothness) and (p-BCM)
withp∈(1,2]. ThentheiteratesgeneratedbyNSGD-Mwithparametersβ 1 :=0,β t ≡β
:=1−min(cid:110) 1,(cid:0)∆
σ21
TL(cid:1) 3pp −2(cid:111)
(cid:113)
for t≥2 and η ≡ ∆1(1−β) satisfy
t LT
1 (cid:88)T
E[∥∇F(x
)∥]≤6√ √∆ 1L +6(cid:32) ∆ 1Lσp−p 1(cid:33) 3p p− −1 2
.
T t T T
t=1
In particular, this corresponds to an oracle complexity of O(cid:16) ∆1L + ∆1L(cid:0)σ(cid:1) p−p 1(cid:17) .
ε2 ε2 ε
Proof. To shorten the notation we write η ≡ η,β ≡ β and α := 1−β. Combining (10) with Lemma 16
t t
yields
T
T1 (cid:88) E[∥∇F(x t)∥]≤ ∆ ηT1 + η 2L +2σαp− p1 + 2L αη
t=1
(cid:114) √ (cid:114)
=
∆ 1L
+
∆ √1Lα +2σαp− p1
+2
∆ 1L (23)
αT 2 T αT
(cid:114)
∆ L
≤4 1 +2σαp− p1 .
αT
(cid:113)
Case 1: α=1. This implies σ ≤ ∆1L and hence
T
T (cid:114)
1 (cid:88)
E[∥∇F(x )∥]≤6
∆ 1L
.
T t T
t=1
Case 2:
α=(cid:0)∆1L(cid:1) 3pp
−2. Plugging into (23) yields
σ2T
T (cid:18) (cid:19)p−1 (cid:18) (cid:19)p−1 (cid:18) (cid:19)p−1
T1 (cid:88) E[∥∇F(x t)∥]≤4σ3pp −2 ∆ T1L 3p−2 +2σ3pp −2 ∆ T1L 3p−2 =6σ3pp −2 ∆ T1L 3p−2 .
t=1
27Therefore we get
T (cid:40)(cid:114) (cid:18) (cid:19)p−1 (cid:41)
T1 (cid:88) E[∥∇F(x t)∥]≤6max ∆ T1L ,σ3pp −2 ∆ T1L 3p−2
t=1
and hence the claim.
Additionally, this result recover those in Cutkosky and Mehta (2020)7 with improved constants when
p=2.
C.3 Technical Difficulties of Proving High Probability Convergence
IntheprevioussectionweshowedthatEquation(4)andCorollary3alsoholdforNSGDwithmomentum. For
Theorem5ontheotherhand, whileitstillholdsfortime-varyingandconstantparameters, wewerenotable
to prove the result for NSGD-M. We shortly want to discuss the technical difficulty of extending Corollary 5
to the momentum version.
The proof of Theorem 4 hinges on two parts: Firstly, one shows that the angle ϕ sharply concentrates
t
around its conditional expectation ψ = E[ϕ |F ]. This step only requires the boundedness of ϕ and is
t t t−1 t
hence applicable for both the minibatch and momentum version of NSGD. In the next step however, we have
to lower bound ψ . Our current proof technique — and to some extend intuition — tells us that such lower
t
bounds involves the term
E[∥g −∇F(x )∥|F ]. (24)
t t t−1
In the case of minibatch NSGD, g only depends on randomness sampled in iteration t, and (24) can hence be
t
upper bounded by a constant as seen in (14). However, in the case of NSGD with momentum, g consists of
t
random samples from all previous iterations. This results in (24) being a random variable instead, and it is
not clear how to uniformly control it. Our empirical evidence indicates that an extension to NSGD-M might
not be possible since quantiles of average gradient norms of NSGD-M behave super-linearly in log(1/δ), see
Section 4 and appendix E.3 for more details.
7Notethattheauthorsdidnotuseβ1=0,resultinginanadditionalterm. Howeverthistermisnotleadingandhencedoes
notaffecttheoraclecomplexity.
28D Lower-Bounds for NSGD
In this section we prove that Proposition 1 is tight and the sample complexity achieved in Equation (4)
is optimal, in the sense that no other choice of parameters can lead to a uniformly better guarantee for
minibatch-NSGD when problem-dependent parameters are unknown. To do so, we first derive a lower bound
for the deterministic setting which might be of independent interest. Afterwards, we will equip this hard
function with a stochastic gradient oracle to prove the lower bound for the stochastic setting.
D.1 Deterministic Setting
The following Lemma derives a lower bound for NGD with arbitrary stepsizes. We will use it afterwards to
show optimality of our polynomial stepsize order.
Lemma 19 (Lower Bounds for Deterministic Setting). Let F be the set of functions that satisfy (Lower
∆1,L
Boundedness) and (L-smoothness) and let ε > 0. Denote NGD with stepsizes η ≥ 0 as A . Then there
t (ηt)
exists a function F ∈F such that the iterates of A satisfy
∆1,L (ηt)
∥∇F(x )∥>ε
t
for all t∈[T∗], where T∗ :=inf(cid:110) T ∈N(cid:12) (cid:12)ε> ∆1− Lε + L(cid:80)T t=1η t2(cid:111) .
(cid:12) 2(cid:80)T t=1ηt 8(cid:80)T t=1ηt
The proof extends ideas from (Hübler et al., 2024) by also including the (L-smoothness) assumption into
the function construction.
Proof. We first define
(cid:40)
−2ε+Lx, x≤ η
g : [0,η]→R,x(cid:55)→ 2
η −2ε+ηL−Lx, x> η,
2
whichwillcorrespondtothegradientofourconstructedfunctionbetweentwoconsecutivepoints. Toformalise
this idea, define
η2L (cid:90) η
A :=−2η ε+ t = g (x)dx
t t 4 ηt
0
and T∗ according to the statement. Furthermore, let τ := (cid:80)t−1 η . Then we define our hard function via
t κ=1 κ
its derivative

−2ε, x<0
g
(x−τ ), x∈[τ ,τ ), t∈[T∗−1]
F′(x):= ηt t t t+1 (25)
− 0,2ε+L(x−τ T∗), ox t∈ he( rτ
wT i∗
s, eτ
T∗
+ 2 Lε]
and F(x):=∆ +(cid:82)x F′(λ)dλ. Note that, by definition of F, we have
1 0
t−1 t−1 t−1
(cid:88) (cid:88) L(cid:88)
F(τ )=∆ + A =∆ −2ε η + η2
t 1 t 1 τ 4 τ
τ=1 τ=1 τ=1
and hence F(τ )> ε for all t≤T∗8 by our definition of T∗. In particular we have inf F(x)≥0 and hence
t L x
F(0)−inf F(x)≤∆ . Furthermore F is L-smooth by definition and hence F ∈F . Next we will show
x 1 ∆1,L
that NGD, when started at x = 0, produces the iterates x = τ . Therefore note that τ = 0 = x and,
1 t t 1 1
assuming x = τ ,t < T∗, we have x = x −sgn(F′(x ))η = x +η = τ . By induction we hence get
t t t+1 t t t t t t+1
|F′(x )|=|F′(τ )|≡2ε for all t∈[T∗], which completes the proof.
t t
8Duetotheshiftt−1inthesum. Otherwise,ifwewouldstartthealgorithmatx0,t≤T∗−1.
29ThefollowingTheoremisaconsequenceofLemma19andimpliesthatr =1/2isthe(only)optimalchoice
of polynomial stepsize decay in the deterministic setting. To formulate the result, we will use the complexity
definitionintroducedbyCarmonetal.(2020),i.e.,foranalgorithmA,afunctionclassF andε>0wedefine
T ε(A,F):= sup
inf(cid:8) t∈N(cid:12)
(cid:12)∥∇F(x t)∥≤ε, (x t)
t∈N
=A(F)(cid:9)
.
F∈F
Theorem 20 (Optimality in Deterministic Setting). Let F be the set of functions that satisfy (Lower
∆ √1,L
Boundedness)and(L-smoothness). Supposeε≤ ∆1L andε≤ ∆1L. Furthermoreletη,r >0anddenoteNGD
2 3
with decaying stepsizes η =ηt−r as Ar, and with constant stepsizes η ≡ηT−r as Ar. Then Ar ∈{Ar,Ar}
t d t c d c
satisfies
(cid:18) (cid:19)1 (cid:18) (cid:19) 1
T (Ar,F )≥ ηL r + (1−r)∆ 1 1−r
ε ∆1,L 4ε 8ηε
for r ∈(0,1). For r ≥1 and small enough ε we have T ε(Ar,F ∆1,L)≥exp(∆1/(8ηε)).
Proof. First note that the definition of T starts with x instead of x . For the sake of consistency with
ε 0 1
other works, we will also apply this convention in our result by denoting (x ,x ,...)←(x ,x ,...). We first
0 1 1 2
consider constant stepsizes η ≡ηT−r. Setting x =0 and applying Lemma 19 yields
t 0
T∗ =inf(cid:26) T ∈N(cid:12) (cid:12) (cid:12)ε> ∆ 1− Lε + ηL (cid:27)
(cid:12) 2ηT1−r 8Tr
(cid:26) (cid:12) (cid:27)
≥inf T ∈N(cid:12) (cid:12)ε> ∆ 1 + ηL ,
(cid:12) 4ηT1−r 8Tr
where we used our assumption ε≤ ∆1L in the last line. For r ∈(0,1) we calculate
2
(cid:26) (cid:27)
∆ ηL ∆ ηL
ε> 1 + ⇒ε>max 1 ,
4ηT1−r 8Tr 4ηT1−r 8Tr
(cid:40)(cid:18) (cid:19) 1 (cid:18) (cid:19)1(cid:41)
⇔T >max ∆ 1 1−r , ηL r .
4ηε 8ε
(cid:26)(cid:16) (cid:17) 1 (cid:16) (cid:17)1(cid:27)
In particular we have T∗ ≥max ∆1 1−r, ηL r and hence
4ηε 8ε
(cid:40)(cid:18) (cid:19) 1 (cid:18) (cid:19)1(cid:41)
T (A ,F )≥max ∆ 1 1−r , ηL r .
ε c ∆1,L 4ηε 8ε
For r ≥1 we have
∆ ηL (cid:26) ∆ Tr−1 ηL (cid:27)
ε> 1 + ⇒ε>max 1 ,
4ηT1−r 8Tr 4η 8Tr
(cid:18) (cid:19)1
ηL r 4ηε
⇔T > and Tr−1 >
8ε ∆
1
In the case r = 1 this implies T ε(Ar c,F ∆1,L) ≥ ∞ for all ε < ∆ 4η1. For r > 1 the same holds for ε <
(cid:16) (cid:17)β
(ηL)α ∆1 , where α:= r−1 and β := r .
η 2r−1 2r−1
Next we consider decreasing stepsizes η =ηt−r. Therefore note that, for q ̸=1,
(cid:16) (cid:17)
η (T +1)1−q−1 (cid:90) T+1 (cid:88)T (cid:32) (cid:90) T (cid:33) (cid:18) T1−q−1(cid:19)
=η t−qdt≤η t−q ≤η 1+ t−qdt =η 1+ .
1−q 1−q
1 t=1 1
30In particular we have
(cid:88)T (cid:18) T1−r−1(cid:19)
2ε η ≤2εη 1+ and
t 1−r
t=1
L(cid:88)T
η2 ≥
η2L (cid:16)
(T
+1)1−2r−1(cid:17)
.
4 t 4(1−2r)
t=1
We first consider r ∈(0,1)\{1/2}. Plugging into Lemma 19 yields
(cid:40) (cid:12)
(cid:12) ∆ (1−r)
ηL(1−r)(cid:0) T1−2r−1(cid:1)(cid:41)
T∗ ≥inf T ∈N(cid:12)ε> 1 + .
(cid:12) 4ηT1−r 8(1−2r)T1−r
(cid:12)
ByourassumptionsonεwegetT∗ ≥2andhencecanassumeT ≥2whichinturnimplies T1−2r−1 ≥ T1−2r.
1−2r 2
Therefore we get
(cid:26) (cid:12) (cid:27)
T∗ ≥inf T ∈N(cid:12) (cid:12)ε> ∆ 1(1−r) + ηL(1−r)
(cid:12) 4ηT1−r 16Tr
(cid:40)(cid:18) (cid:19) 1 (cid:18) (cid:19)1(cid:41)
≥max
∆ 1(1−r) 1−r
,
ηL(1−r) r
.
4ηε 16ε
Finally we have to consider the edge cases r ∈{1/2,1}. Let r =1/2, then
(cid:88)T (cid:16) √ (cid:17) √
η ≤η 2 T −1 ≤2η T,
t
t=1
T (cid:90) T+1
(cid:88)
η2 ≥η2 t−1dt≥η2log(T)
t
t=1 1
and hence
(cid:26) (cid:12) (cid:27)
T∗ ≥inf T ∈N(cid:12) (cid:12)ε> ∆ √1 + ηLlo √g(T)
(cid:12) 8η T 16 T
(cid:26) (cid:12) (cid:27)
≥inf T
∈N(cid:12)
(cid:12)ε>
∆ 1(1−r)
+
ηL(1−r)
(cid:12) 4ηTr 16Tr
andwecanhenceproceedasbefore. Notethatamorecarefulanalysiscanadditionallyshowtightnessofthe
log(T) dependence. Now let r =1 and note that
T T
(cid:88) (cid:88)
η =η t−1 ≤η(1+log(T)).
t
t=1 t=1
In particular
(cid:26) (cid:12) (cid:27) (cid:18) (cid:19)
T∗ ≥inf T ∈N(cid:12) (cid:12)ε> ∆ 1 ≥exp ∆ 1 −1
(cid:12) 4η(1+log(T)) 4ηε
and hence T ε(Ar d,F ∆1,L)≥e8∆ η1 ε for ε≤ ∆ 8η1.
D.2 Stochastic Setting
Finally we will extend the above result to the stochastic setting.
31Theorem 21. Let F be the set of functions that satisfy (Lower Boundedness) and (L-smoothness).
∆1,L
Furthermore let O
σ,p
denote the set of stochastic gradient oracles that satisfy (p-BCM). Suppose ε ≤ ∆ 21L
and let η,B,q > 0,r ∈ (0,1). Let A denote NGD with parameters η ≡ ηT−r,B ≡ max{1,BTq} and the
t t
mini-batch gradient estimator g = 1 (cid:80)Bt ∇f(x ,ξ(j)), where ξ(1),...,ξ(Bt) i. ∼i.d. ξ . Then there exists a
t Bt j=1 t t t t t
function F ∈F and oracle ∇f(·,·)∈O such that A requires at least
∆1,L σ,p
mE ≥max(cid:40)(cid:18) ∆ 1(cid:19) 1−1 r ,(cid:18) ηL(cid:19) r1 ,B(cid:18) ∆ 1(cid:19) 11 −+q r ,B(cid:18) ηL(cid:19)1+ rq
,
1 (cid:16) σ (cid:17) qp (( pq −+1 1) ).(cid:41)
ε 6ηε 12ε 6ηε 12ε Bq1 28ε
oracle calls to generate an iterate with E[∥∇F(x )∥]≤ε.
t
When comparing to the corresponding upper bound (15) we can see that both bounds are tight in all
parameters. This is due to 1 (cid:80)n a ≤max{a ,...,a }≤(cid:80)n a , hence the maximum and sum notation
n i=1 i 1 n i=1 i
are equivalent up to constants.
Proof. The idea behind the proof is the following. We will again use a very similar construction to (25) and
add a noise oracle on top. The goal of this noise oracle will be to point in the wrong direction with the
highest possible probability, effectively slowing down the progress we make even more. As this noise oracle
may however lead to iterates going below x , we need to slightly modify the construction of F.
1
Construction of the hard function F. To this end, let η¯:=ηT−r, τ :=kη¯for k ∈Z,
k
T∗ :=inf(cid:26) T ∈N(cid:12) (cid:12) (cid:12)ε> ∆ 1− Lε + Lη¯(cid:27)
d (cid:12) 3Tη¯ 12
(cid:40)(cid:18) (cid:19) 1 (cid:18) (cid:19)1(cid:41)
≥max ∆ 1 1−r , ηL r .
6ηε 12ε
where we used ε≤ ∆1L in the last inequality as before, and define
2

g (x−τ ), x∈[τ ,τ ), t+1≤T∗
 η¯ t t t+1 d
F′(x):= −2ε+L(cid:0) x−τ (cid:1) , x∈(cid:0) τ ,τ + 3ε(cid:3) (26)
0,
T d∗ otherwT id∗ se,T d∗∗ L
where
(cid:40)
−3ε+Lx, x≤ η
g : [0,η]→R is given by x(cid:55)→ 2
η −3ε+ηL−Lx, x> η.
2
As before, we define the hard function as F(x) := ∆ +(cid:82)x F′(t)dt. In the following we will denote the
1 0
derivative of F using ∇F to align with the gradient oracle notation. Now firstly note that we can use the
deterministic Lemma 19 to rule out any stepsize that satisfies η¯≥ 8ε: In this case we would have
L
∆ − ε LTη¯2 ∆ − ε Lη¯
1 L + = 1 L + ≥0+ε,
2Tη¯ 8Tη¯ 2Tη¯ 8
where we used ε ≤ ∆1L in the last inequality. Hence we may assume ηT−r ≤ 8ε. Under this assumption9
2 L
we again have F(0)−inf F(x)≤∆ and that F is L-smooth.
x 1
Construction of the noise oracle ∇f(x,ξ). Wewillconstructthementionedoracle,whichaimstopoint
inthewrongdirectionwiththemaximalprobability. Thisoracleconstructionfollowsasimilarideaas(Yang
et al., 2024, Theorem 3). To construct the oracle, let ρ>0 to be defined later and define α:= p ,
p−1
(cid:26) (cid:18) 2(1+ρ)∥∇F(x)∥(cid:19)α(cid:27)
δ(x):=min 1, .
σ
9NotethatforηT−r ≥ 1 L2ε wewouldgetlimx→−∞F(x)=−∞forF definedby(26).
32This will be the probability of the oracle returning the correct direction. Now let
(cid:40)
−ρ∇F(x), ξ ≥δ(x)
∇f(x,ξ):= (cid:16) (cid:17)
1+ (1−δ(x))(1+ρ) ∇F(x), ξ <δ(x)
δ(x)
and ξ ∼Unif([0,1]). Straightforward calculations yield
(cid:18) (cid:18) (cid:19)(cid:19)
(1−δ(x))(1+ρ)
E[∇f(x,ξ)]=∇F(x) (1−δ(x))(−ρ)+δ(x) 1+
δ(x)
=∇F(x)((1−δ(x))(−ρ)+δ(x)+(1−δ(x))(1+ρ))
=∇F(x)
and
(cid:18)
(1−δ(x))(1+ρ)
(cid:19)p
E[∥∇f(x,ξ)−∇F(x)∥p]=(1−δ(x))(1+ρ)p∥∇F(x)∥p+δ(x) ∥∇F(x)∥
δ(x)
(cid:32) (cid:33)
(1−δ(x))p
=(1+ρ)p∥∇F(x)∥p (1−δ(x))+
δ(x)p−1
(cid:32) (cid:33)
1
≤(1+ρ)p∥∇F(x)∥p 1+
δ(x)p−1
2(1+ρ)p∥∇F(x)∥p
≤ ≤σp.
δ(x)p−1
In particular ∇f(·,·)∈O .
σ,p
The behaviour of NSGD on the constructed function and oracle. Finally we are able to show the
lower bound by analysing the behaviour of NSGD on our constructed objects. Firstly it is clear that we can
upper bound the stochastic progress by the deterministic progress, i.e. x ≤ η¯t. In particular, with the
t+1
same arguments as in Lemma 19 and theorem 20, we get that ∥∇F(x )∥ > ε for all t ∈ [T∗] and we hence
t d
can lower bound the iteration complexity by T ≥T∗. We next differentiate two cases.
d
Case 1: max{1,B(T∗)q}≥ σα . In this case nothing else needs to be done and we can lower bound the
d 2(7ε)α
number of oracle calls required to find an ε-stationary point by
(cid:40) (cid:18) (cid:19)1+q (cid:18) (cid:19)1+q(cid:41)
T∗·max{1,B(T∗)q}=max T∗,B ∆ 1 1−r ,B ηL r .
d d d 6ηε 12ε
Case 2: max{1,B(T∗)q} < σα . In this case we will make use of the gradient oracle to construct
d 2(7ε)α
a stronger lower bound T∗ > T∗. Therefore first note that, due to the constant stepsize and x = 0, the
s d 1
iterations x always stay on the lattice Γ=η¯Z. Furthermore, by the construction of F, we have that
t
∀x∈Γ: ∇F(x)∈[−3ε,0],
which, for ρ≤ 1, in particular implies
6
(cid:18) 6(1+ρ)ε(cid:19)α (cid:18) 7ε(cid:19)α
∀x∈Γ: δ(x)≤ ≤ =:δ. (27)
σ σ
(cid:40)
1, ω ∈A
Now define the random variable ζ :=1 , where 1 (ω)= , and compute
t {gt<0} A
0, o.w.
t
(cid:88)
x =x +η¯(2ζ −1)=η¯ (2ζ −1)=:2η¯S −η¯t, (28)
t+1 t t τ t
τ=1
33where S
t
:=(cid:80)t τ=1ζ τ. Furthermore, let B¯ =max{1,BTq} and ρ≤min{1/6,1/B¯}, then we have
 
B¯
P(g t <0)=P  B1 ¯ (cid:88) ∇f(x t,ξ t(j))<0=1−(1−δ(x t))B¯ (2 ≤7) 1−(1−δ)B¯ . (29)
j=1
By definition
(1−δ)B¯ =exp(cid:0) B¯log(1−δ)(cid:1)
and
−δ (7ε)α 2(7ε)α
log(1−δ)≥ =− ≥−
1−δ σα−(7ε)α σα
where we used log(1+x)≥ x for x∈(−1,0] in the first, and 1< σα by our case 2 assumption in the
1+x 2(7ε)α
last inequality. Plugging into (29) yields
(cid:18) 2(7ε)α(cid:19) (cid:26) 2(7ε)α(cid:27)
P(g <0)≤1−exp −B¯ ≤min 1,B¯ , (30)
t σα σα
where we used ex ≥max{0,1+x} in the last inequality. Next up let T ∈N such that B¯ ≤ σα , then
4(7ε)α
(30)
(cid:26) 2(7ε)α(cid:27)
1
P(g <0) ≤ min 1,B¯ ≤
t σα 2
andζ
t
ishenceaBernoullirandomvariablewithprobabilityatmost1/2. Inparticular,wehavemedian(S t)≤
⌊t/2⌋ and hence
(cid:18) (cid:19)
1 ≤P S ≤ t =P(2η¯S ≤η¯t)(2 =8)P(x ≤0).
2 t 2 t t+1
Finally note that all x in (Γ∩(−∞,0]) satisfy ∇F(x)=−3ε and hence
E[∥∇F(x )∥]=3εP(x ≤0)+E(cid:2) ∥∇F(x )∥1 (cid:3) >ε+0
t t t {xt>0}
for all t ∈ [T]. Summing up the results in Case 2, we so far proved the auxiliary result that for any T with
max{1,BTq}≤ σα alliteratest∈[T]satisfyE[∥∇F(x )∥]>ε. Bytheassumptionofcase2, thisimplies
4(7ε)α t
∀T ≤T∗: ∀t∈[T]: E[∥∇F(x )∥]>ε,
s t
where T∗
:=(cid:16)
σα
(cid:17)1/q
>T∗ with α= p . In particular, we can lower bound the number of oracle calls
s 4B(7ε)α d p−1
required to reach an expected ε-stationary point by
1 (cid:16) σ (cid:17)p(q+1)
T∗·B(T∗)q = q(p−1).
s s Bq1 28ε
Combining. Finally we are able to combine everything into our lower bound. Therefore, let
T∗
:=max(cid:40) T∗,B(cid:18) ∆ 1(cid:19) 11 −+q r ,B(cid:18) ηL(cid:19)1+ rq
,
1 (cid:16) σ (cid:17) qp (( pq −+1 1) )(cid:41)
d 6ηε 12ε Bq1 28ε
and note that for T ≤T∗, one of the above cases applies, showing
∀t∈[T]: E[∥∇F(x )∥]>ε.
t
This completes the proof.
34D.3 Lower-Bound on the Convergence Measure
(cid:113)
Proof. Inthisproof,weconsideraslightlymoregeneralstep-sizeγ = ∆1 1 foranya<1. TakeF(x)= 1x2
√L Ta 2
and x
1
> 0 (w.l.g.), then the step-size is γ = Tx0 a. Denote by N := 2Ta. For the first ⌈N⌉ iteratio √ns the
updateruleisx =x −γ(t−1)=x (1+ 1 − t )≥0,fort=0,...,⌈N⌉. WecomputeforT ≥⌈N⌉=⌈ 2Ta⌉
t 1 1 N N
(cid:88)T
∥∇F(x )∥2 ≥
(cid:88)⌈N⌉
∥∇F(x )∥2
=x2(cid:88)⌈N⌉(cid:18)
1+
1
−
t
(cid:19)2
t t 1 N N
t=1 t=1 t=1
(cid:18) 1 (cid:19)2 (cid:18) 1 (cid:19) 1 (cid:88)⌈N⌉ x2 (cid:88)⌈N⌉
= x2⌈N⌉ 1+ −2x2 1+ t+ 1 t2
1 N 1 N N N2
t=1 t=1
(cid:18)
1
(cid:19)2 (cid:18)
1
(cid:19)
1 ⌈N⌉(⌈N⌉+1)
= x2⌈N⌉ 1+ −2x2 1+
1 N 1 N N 2
x2 (cid:18) ⌈N⌉3 ⌈N⌉2 ⌈N⌉(cid:19)
+ 1 + +
N2 3 2 6
⌈N⌉(N +1)(N −⌈N⌉) x2⌈N⌉3
≥ x2 + 1
1 N2 3N2
x2⌈N⌉2 x2⌈N⌉3
≥ − 1 + 1
N2 3N2
√
x2⌈N⌉3 (cid:18) 6 (cid:19) x2⌈N⌉3 x2N 2L∆ Ta
≥ 1 2− ≥ 1 ≥ 1 = 1 ,
6N2 ⌈N⌉ 6N2 6 3
where in the second inequality we dropped the last two terms, in the third inequality we used ⌈N⌉−N ≤1
and N +1 ≤ ⌈N⌉2 for N ≥ 6. The forth inequality holds by the assumption N ≥ 6. It remains to divide
both sides by T and verify that in case a = 1/2, the assumption T ≥ 18 implies the assumed conditions
T ≥⌈N⌉≥6. Rearranging, we get
(cid:115)√
(cid:114)
(cid:104) (cid:105) 2L∆
E ∥∇F(x¯ )∥2 ≥ 1 ·T1/4.
T 3T
(cid:113)√
Noting that 2 ≥ 2 completes the proof.
3 3
35E Additional Experiments and Details
In this section we provide additional information and experiments for Section 4.
E.1 Additional Details on Language Modelling
Additional Details All experiments were carried out on Nvidia RTX 3090 GPUs in an internal cluster.
The total compute including preliminary experiments were approximately 380 GPU hours. Roughly 200 of
these were required for preliminary experiments and parameter-tuning, 180 for the final experiments.
The AWD-LSTM (Merity et al., 2018) is released under a BSD 3-Clause License, the Penn Treebank
dataset (Marcus et al., 1993) under the LDC User Agreement for Non-Members and the WikiText-2 dataset
(Merity et al., 2017) under the Creative Commons BY-SA 3.0 license.
The below experiments all follow the general structure outlined in Section 4.
ReasonsfortheClippingBehaviour. Weadditionallywanttounderstandwhythepercentageofclipped
gradientsincreasesovertime. ThereforeFigure3examinetheaverageminibatch-gradientnormperepoch,i.e.
1 (cid:80)t0+E−1∥g ∥ where the epoch consists of E mini-batches and starts at iteration t . The plot shows that,
E t=t0 t 0
while the training loss decreases, the stochastic gradient norms increase. This observation is in line with
previous observations on different tasks (Goodfellow et al., 2016, Chapter 8). Therefore, while surprising
at first, the increase in stochastic gradient norms is able to explain the increasing clipping percentage in
hindsight.
6.0 5.0 0.40
Clip-SGD 0.7 Clip-SGD
NSGD 4.8 NSGD 0.35
5.5 0.6
0.30
0.5 4.6
5.0 0.25
0.4 4.4
0.20
4.5
0.3 4.2 0.15
4.0 0.2 4.0 0.10
0.1 0.05
3.5 3.8
0.0 0.00
0 50 100 150 200 250 300 0 50 100 150 200 250 300
Epoch Epoch
(a) PTB (b) WikiText-2
Figure3: AllplotsconsiderClip-SGDandNSGDwithtunedparameters. Solidlinesrepresentthetrainingloss
andcorrespondtothelefty-axis. Thedashedlinecorrespondstotherighty-axis,andrepresentstheaverage
mini-batch gradient norm in an epoch. Shaded areas represent the minimal and maximal value within 5
seeds, the line the median.
E.2 Illustrating Drawbacks of Gradient Clipping Theory
In the two sets of experiments below we compare several algorithms with and without parameter tuning
on a simple quadratic model to better understand the influence of heavy tailed noise and the necessity of
parameter tuning.
Comparison with tuned parameters. In this set of experiments, we compare NSGD using step-sizes
√ √ √
η =η/ t (Yang et al., 2024) and Clip-SGD with η =η/ t, γ =γ· 4t (Zhang et al., 2020; Nguyen et al.,
t t t
2023a) and tune the pair (η,γ) over a grid. We report the optimal parameters, η for NSGD and the pair (η,γ)
for Clip-SGD in Table 1. Convergence plots for different noise distributions are presented in Figures 4a, 5a
and 6a.
36
ssoL
niarT
||tg||
ssoL
niarT
||tg||Clip-SGD (opt)
NSGD (opt)
100 100
SGD
Clip-SGD
10 1 10 1 NSGD
NSGDM
0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 5000
Iterations Iterations
(a) Tuned Clip-SGD vs. NSGD. When parameters are (b) Comparison without parameter tuning.
tuned, clipping is triggered at every iteration.
Figure4: Performanceofdifferentalgorithmsonaquadraticproblemf(x,ξ)= 1∥x∥2+⟨x,ξ⟩,d=10,where
2 2
ξ is a random vector with i.i.d.components drawn from a symmetrized Pareto distribution with tail index
p=1.5.
We observe that in both heavy tailed and light tailed settings, when parameters are tuned, the two
algorithms exhibit comparable convergence rate. However, for all three noise distributions we tested, the
optimal parameter for NSGD appeared to be η = 0.5, while Clip-SGD required different parameters to reach
similarperformance. ThisillustratesthatClip-SGDcanbemoresensitivetomisspecificationofitsparameters
compared to NSGD. Moreover, Clip-SGD requires two parameters for tuning compared to only one parameter
for NSGD.
Table 1: Comparison of Tuned Parameters for NSGD and Clip-SGD under Different Noise Distributions
Noise Distribution Algorithm Optimal η Optimal γ
Heavy tailed (p=1.5) NSGD 0.5 –
(Figure 4a) Clip-SGD 0.1 1
Heavy tailed (p=1.8) NSGD 0.5 –
(Figure 5a) Clip-SGD 100 0.001
Light tailed NSGD 0.5 –
(Figure 6a) Clip-SGD 5 0.1
Comparison without parameter tuning. In Figures 4b, 5b and 6b, we compare several adaptive algo-
√
rithms with default parameter sequences, i.e., γ = η = 1. Specifically, we use η = 1/ t for SGD and NSGD;
√ t √ √
(cid:112)
η = α /t, α = 1/ t (where α is momentum sequence) (Yang et al., 2024), and η = 1/ t, γ = 4t
t t t t t t
for Clip-SGD. This order of step-sizes is selected based on the theory for each algorithm under BV setting,
where this order is known to give an asymptotically optimal convergence rate as T → ∞. We observe that
the performance of Clip-SGD significantly degrades when γ and η are not tuned, while the performance of
NSGDremains nearly the same.
We also see that under heavier tailed noise such as Pareto distribution, the performance of untuned SGD
and Clip-SGD degrades substantially compared to the light tail noise setting, confirming the sensitivity of
these algorithms to different noise distributions. On the other hand, NSGD and its momentum variant (see
(20) in Appendix C) are more stable and converge to a smaller neighbourhood around the optimal solution
even under heavy tailed noise.
E.3 Verifying High Probability Bounds for SGD and NSGD(M)
In this section, we conduct experiments to verify high probability convergence for three algorithms: SGD,
NSGD, NSGD-M. High probability convergence refers to the convergence rate of the average gradient norm
37
)tx(f )tx(fClip-SGD (opt)
NSGD (opt)
100 100
SGD
Clip-SGD
NSGD
10 1 10 1 NSGDM
0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 5000
Iterations Iterations
(a) Tuned Clip-SGD vs. NSGD. When parameters are (b) Comparison without parameter tuning.
tuned, clipping is triggered at every iteration.
Figure5: Performanceofdifferentalgorithmsonaquadraticproblemf(x,ξ)= 1∥x∥2+⟨x,ξ⟩,d=10,where
2 2
ξ is a random vector with i.i.d.components drawn from a symmetrized Pareto distribution with tail index
p=1.8.
Clip-SGD (opt) SGD
NSGD (opt) Clip-SGD
NSGD
NSGDM
100 100
10 1 10 1
0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 5000
Iterations Iterations
(a) Tuned Clip-SGD vs. NSGD. When parameters are (b) Comparison without parameter tuning.
tuned, clipping is triggered at every iteration.
Figure6: Performanceofdifferentalgorithmsonaquadraticproblemf(x,ξ)= 1∥x∥2+⟨x,ξ⟩,d=10,where
2 2
ξ is distributed according standard Normal distribution.
depending linearly on log(1/δ) as demonstrated in our Corollary 5, where δ ∈ (0,1) is a failure probability.
Previous theoretical results have shown that vanilla SGD does not exhibit high probability convergence. In
particular, Sadiev et al. (2023) demonstrated that under a bounded variance setting, SGD fails to achieve
this property, mainly due to noise injected in the final iteration. In contrast, for adaptive methods such
as Clip-SGD (Nguyen et al., 2023a) or NSGD Corollary 5, one can establish a high probability convergence
with a mild linear dependence on log(1/δ). However, extending our result to NSGD-M is challenging due to
correlation issues introduced by momentum. Thus, the primary objective of this section is to empirically
investigate whether NSGD-M might still exhibit high probability convergence similar to NSGD.
To achieve this, we evaluate the performance of the three algorithms on a simple quadratic function
F(x) = 1∥x∥2, x ∈ Rd using dimensions d = 1 and d = 1000. We introduce three types of noise during
2
training: (standard)Normal,(component-wise)Paretowithp=1.5andp=2.5,tosimulatebothlight-tailed
and heavy-tailed noise environments. Each algorithm is run k =105 times over T =100 iterations, and the
convergence criterion is the average gradient norm over T iterations. We present two plots for each set of
experiments. The left plot visualizes the convergence behavior by selecting the median, δ and 1−δ quantiles
(where δ := 10−4) of the algorithm runs based on the average gradient norm at T = 100. These quantile-
based trajectories are plotted against iteration t = 1,...,T. The right plot shows the quantiles of average
38
)tx(f
)tx(f
)tx(f
)tx(fNormalOracle, d: 1
1.4 NSGD NSGD 0.50
NSGDM NSGDM
1.2
SGD SGD
0.45 1.0
0.8 0.40
0.6 0.35
0.4
0.30
0.2
0 20 40 60 80 100 1 2 3 4 5 6 7
T log(1
)
Figure 7: Light tail noise, ξ ∼N(0,I).
t
NormalOracle, d: 1000
1.4 NSGD 0.37
NSGDM
1.2 SGD 0.36
1.0 0.35 NSGD
NSGDM
0.8 0.34 SGD
0.6
0.33
0.4
0.32
0 20 40 60 80 100 1 2 3 4 5 6 7
T log(1
)
Figure 8: Light tail noise, ξ ∼N(0,I).
t
gradient norm at T = 100 plotted against log(1/δ). For algorithms with high probability convergence, this
plot should have a sub-linear dependence on log(1/δ).
Lighttailednoise. OurresultsrevealthatfortheNormalnoisedistribution,whichhaslighttails,allthree
algorithms exhibit sub-linear curves Figures 7 and 8, indicating high probability convergence. However, for
Pareto noise Figures 9 to 12 (particularly with p = 1.5 and p = 2.5), which corresponds to infinite and
finite variance regimes respectively, SGD exhibits a super-linear curve, confirming its lack of high probability
convergence, consistent with theoretical predictions.
Heavy tailed noise. Most importantly, we observe that while both NSGD and NSGD-M exhibit similar
qualitativebehaviorswhenthenoisehaslighttails,Figures7and8; thequantiledependenceonlog(1/δ)can
be super-linear under heavy tailed noise Figures 2 and 13, strongly suggesting that the momentum version
of NSGD (NSGD-M) may not possess a high probability convergence as NSGD.
39
T
T
||)tx(F
||
1 T
||)tx(F
||
1 T
1=t
1=t
T
T
||)tx(F
||
1 T
fo
selitnauQ
||)tx(F
||
1 T
fo selitnauQ
1=t
1=tParetoOracle, d: 1
0.9
NSGD NSGD
2.5 NSGDM NSGDM
0.8
SGD SGD
2.0 0.7
1.5 0.6
1.0 0.5
0.4
0.5
0.3
0 20 40 60 80 100 1 2 3 4 5 6 7
T log(1
)
Figure 9: Heavy tailed noise with finite variance. Pareto with p=2.5.
ParetoOracle, d: 1000
1.6
0.65 NSGD
1.4 NSGDM
0.60 SGD
1.2
NSGD 0.55
1.0 NSGDM
0.50
SGD
0.8
0.45
0.6
0.40
0.4
0.35
0 20 40 60 80 100 1 2 3 4 5 6 7
T log(1
)
Figure10: Heavytailednoisewithfinitevariance. Paretowithp=2.5. SeeFigure2forthesameexperiment,
but without SGD on the plot.
ParetoOracle, d: 1
60 NSGD 7 NSGD
NSGDM NSGDM
50 SGD 6 SGD
40 5
30 4
3
20
2
10
1
0
0
0 20 40 60 80 100 1 2 3 4 5 6 7
T log(1
)
Figure 11: Heavy tailed noise with infinite variance. Pareto with p=1.5.
40
T
T
T
||)tx(F
||
1 T
||)tx(F
||
1 T
||)tx(F
||
1 T
1=t
1=t
1=t
||)tx(F
||
1T
=t1
T
fo
selitnauQ
||)tx(F
||
1T =t1
T
fo
selitnauQ
||)tx(F
|| 1T
=t1
T fo
selitnauQParetoOracle, d: 1000
25
120 NSGD NSGD
NSGDM NSGDM
100 SGD 20 SGD
80
15
60
10
40
20 5
0 0
0 20 40 60 80 100 1 2 3 4 5 6 7
T log(1
)
Figure 12: Heavy tailed noise with infinite variance. Pareto with p = 1.5. See Figure 13 for the same
experiment, but without SGD on the plot.
ParetoOracle, d: 1000
1.8
1.6 NSGD
1.6 NSGDM
1.4
1.4
1.2
1.2 NSGD
1.0 NSGDM 1.0
0.8 0.8
0.6 0.6
0.4
0 20 40 60 80 100 2 4 6 8
T log(1
)
Figure 13: Heavy tailed noise with infinite variance. Pareto with p=1.5.
41
T
T
||)tx(F
||
1 T
||)tx(F
||
1 T
1=t
1=t
T
T
||)tx(F
||
1=t1
T fo
selitnauQ
||)tx(F
||
1=t1
T
fo
selitnauQ