[
    {
        "title": "Implicit Regularization for Tubal Tensor Factorizations via Gradient Descent",
        "authors": "Santhosh KarnikAnna VeselovskaMark IwenFelix Krahmer",
        "links": "http://arxiv.org/abs/2410.16247v1",
        "entry_id": "http://arxiv.org/abs/2410.16247v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16247v1",
        "summary": "We provide a rigorous analysis of implicit regularization in an\noverparametrized tensor factorization problem beyond the lazy training regime.\nFor matrix factorization problems, this phenomenon has been studied in a number\nof works. A particular challenge has been to design universal initialization\nstrategies which provably lead to implicit regularization in gradient-descent\nmethods. At the same time, it has been argued by Cohen et. al. 2016 that more\ngeneral classes of neural networks can be captured by considering tensor\nfactorizations. However, in the tensor case, implicit regularization has only\nbeen rigorously established for gradient flow or in the lazy training regime.\nIn this paper, we prove the first tensor result of its kind for gradient\ndescent rather than gradient flow. We focus on the tubal tensor product and the\nassociated notion of low tubal rank, encouraged by the relevance of this model\nfor image data. We establish that gradient descent in an overparametrized\ntensor factorization model with a small random initialization exhibits an\nimplicit bias towards solutions of low tubal rank. Our theoretical findings are\nillustrated in an extensive set of numerical simulations show-casing the\ndynamics predicted by our theory as well as the crucial role of using a small\nrandom initialization.",
        "updated": "2024-10-21 17:52:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16247v1"
    },
    {
        "title": "Asymmetries in Financial Spillovers",
        "authors": "Florian HuberKarin KlieberMassimiliano MarcellinoLuca OnoranteMichael Pfarrhofer",
        "links": "http://arxiv.org/abs/2410.16214v1",
        "entry_id": "http://arxiv.org/abs/2410.16214v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16214v1",
        "summary": "This paper analyzes nonlinearities in the international transmission of\nfinancial shocks originating in the US. To do so, we develop a flexible\nnonlinear multi-country model. Our framework is capable of producing\nasymmetries in the responses to financial shocks for shock size and sign, and\nover time. We show that international reactions to US-based financial shocks\nare asymmetric along these dimensions. Particularly, we find that adverse\nshocks trigger stronger declines in output, inflation, and stock markets than\nbenign shocks. Further, we investigate time variation in the estimated dynamic\neffects and characterize the responsiveness of three major central banks to\nfinancial shocks.",
        "updated": "2024-10-21 17:14:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16214v1"
    },
    {
        "title": "Theoretical Limitations of Ensembles in the Age of Overparameterization",
        "authors": "Niclas DernJohn P. CunninghamGeoff Pleiss",
        "links": "http://arxiv.org/abs/2410.16201v1",
        "entry_id": "http://arxiv.org/abs/2410.16201v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16201v1",
        "summary": "Classic tree-based ensembles generalize better than any single decision tree.\nIn contrast, recent empirical studies find that modern ensembles of\n(overparameterized) neural networks may not provide any inherent generalization\nadvantage over single but larger neural networks. This paper clarifies how\nmodern overparameterized ensembles differ from their classic underparameterized\ncounterparts, using ensembles of random feature (RF) regressors as a basis for\ndeveloping theory. In contrast to the underparameterized regime, where\nensembling typically induces regularization and increases generalization, we\nprove that infinite ensembles of overparameterized RF regressors become\npointwise equivalent to (single) infinite-width RF regressors. This\nequivalence, which is exact for ridgeless models and approximate for small\nridge penalties, implies that overparameterized ensembles and single large\nmodels exhibit nearly identical generalization. As a consequence, we can\ncharacterize the predictive variance amongst ensemble members, and demonstrate\nthat it quantifies the expected effects of increasing capacity rather than\ncapturing any conventional notion of uncertainty. Our results challenge common\nassumptions about the advantages of ensembles in overparameterized settings,\nprompting a reconsideration of how well intuitions from underparameterized\nensembles transfer to deep ensembles and the overparameterized regime.",
        "updated": "2024-10-21 17:03:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16201v1"
    },
    {
        "title": "A Trust-Region Method for Graphical Stein Variational Inference",
        "authors": "Liam PavlovicDavid M. Rosen",
        "links": "http://arxiv.org/abs/2410.16195v1",
        "entry_id": "http://arxiv.org/abs/2410.16195v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16195v1",
        "summary": "Stein variational inference (SVI) is a sample-based approximate Bayesian\ninference technique that generates a sample set by jointly optimizing the\nsamples' locations to minimize an information-theoretic measure of discrepancy\nwith the target probability distribution. SVI thus provides a fast and\nsignificantly more sample-efficient approach to Bayesian inference than\ntraditional (random-sampling-based) alternatives. However, the optimization\ntechniques employed in existing SVI methods struggle to address problems in\nwhich the target distribution is high-dimensional, poorly-conditioned, or\nnon-convex, which severely limits the range of their practical applicability.\nIn this paper, we propose a novel trust-region optimization approach for SVI\nthat successfully addresses each of these challenges. Our method builds upon\nprior work in SVI by leveraging conditional independences in the target\ndistribution (to achieve high-dimensional scaling) and second-order information\n(to address poor conditioning), while additionally providing an effective\nadaptive step control procedure, which is essential for ensuring convergence on\nchallenging non-convex optimization problems. Experimental results show our\nmethod achieves superior numerical performance, both in convergence rate and\nsample accuracy, and scales better in high-dimensional distributions, than\nprevious SVI techniques.",
        "updated": "2024-10-21 16:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16195v1"
    },
    {
        "title": "Theoretical Insights into Line Graph Transformation on Graph Learning",
        "authors": "Fan YangXingyue Huang",
        "links": "http://arxiv.org/abs/2410.16138v1",
        "entry_id": "http://arxiv.org/abs/2410.16138v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16138v1",
        "summary": "Line graph transformation has been widely studied in graph theory, where each\nnode in a line graph corresponds to an edge in the original graph. This has\ninspired a series of graph neural networks (GNNs) applied to transformed line\ngraphs, which have proven effective in various graph representation learning\ntasks. However, there is limited theoretical study on how line graph\ntransformation affects the expressivity of GNN models. In this study, we focus\non two types of graphs known to be challenging to the Weisfeiler-Leman (WL)\ntests: Cai-F\\\"urer-Immerman (CFI) graphs and strongly regular graphs, and show\nthat applying line graph transformation helps exclude these challenging graph\nproperties, thus potentially assist WL tests in distinguishing these graphs. We\nempirically validate our findings by conducting a series of experiments that\ncompare the accuracy and efficiency of graph isomorphism tests and GNNs on both\nline-transformed and original graphs across these graph structure types.",
        "updated": "2024-10-21 16:04:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16138v1"
    }
]