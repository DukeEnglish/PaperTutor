[
    {
        "title": "xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs",
        "authors": "Michael S. RyooHonglu ZhouShrikant KendreCan QinLe XueManli ShuSilvio SavareseRan XuCaiming XiongJuan Carlos Niebles",
        "links": "http://arxiv.org/abs/2410.16267v1",
        "entry_id": "http://arxiv.org/abs/2410.16267v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16267v1",
        "summary": "We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for\nvideos, particularly designed to efficiently capture temporal information over\nmultiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in\naddition to the conventional visual tokenizer, which maps a sequence of tokens\nover multiple frames into a compact set of visual tokens. This enables\nBLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32\nvs. 4608 tokens). We explore different types of temporal encoders, including\nlearnable spatio-temporal pooling as well as sequential models like Token\nTuring Machines. We experimentally confirm that BLIP-3-Video obtains video\nquestion-answering accuracies comparable to much larger state-of-the-art models\n(e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using\nfewer visual tokens. The project website is at\nhttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html",
        "updated": "2024-10-21 17:59:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16267v1"
    },
    {
        "title": "CompassJudger-1: All-in-one Judge Model Helps Model Evaluation and Evolution",
        "authors": "Maosong CaoAlexander LamHaodong DuanHongwei LiuSongyang ZhangKai Chen",
        "links": "http://arxiv.org/abs/2410.16256v1",
        "entry_id": "http://arxiv.org/abs/2410.16256v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16256v1",
        "summary": "Efficient and accurate evaluation is crucial for the continuous improvement\nof large language models (LLMs). Among various assessment methods, subjective\nevaluation has garnered significant attention due to its superior alignment\nwith real-world usage scenarios and human preferences. However, human-based\nevaluations are costly and lack reproducibility, making precise automated\nevaluators (judgers) vital in this process. In this report, we introduce\n\\textbf{CompassJudger-1}, the first open-source \\textbf{all-in-one} judge LLM.\nCompassJudger-1 is a general-purpose LLM that demonstrates remarkable\nversatility. It is capable of: 1. Performing unitary scoring and two-model\ncomparisons as a reward model; 2. Conducting evaluations according to specified\nformats; 3. Generating critiques; 4. Executing diverse tasks like a general\nLLM. To assess the evaluation capabilities of different judge models under a\nunified setting, we have also established \\textbf{JudgerBench}, a new benchmark\nthat encompasses various subjective evaluation tasks and covers a wide range of\ntopics. CompassJudger-1 offers a comprehensive solution for various evaluation\ntasks while maintaining the flexibility to adapt to diverse requirements. Both\nCompassJudger and JudgerBench are released and available to the research\ncommunity athttps://github.com/open-compass/CompassJudger. We believe that by\nopen-sourcing these tools, we can foster collaboration and accelerate progress\nin LLM evaluation methodologies.",
        "updated": "2024-10-21 17:56:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16256v1"
    },
    {
        "title": "Can Knowledge Editing Really Correct Hallucinations?",
        "authors": "Baixiang HuangCanyu ChenXiongxiao XuAli PayaniKai Shu",
        "links": "http://arxiv.org/abs/2410.16251v1",
        "entry_id": "http://arxiv.org/abs/2410.16251v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16251v1",
        "summary": "Large Language Models (LLMs) suffer from hallucinations, referring to the\nnon-factual information in generated content, despite their superior capacities\nacross tasks. Meanwhile, knowledge editing has been developed as a new popular\nparadigm to correct the erroneous factual knowledge encoded in LLMs with the\nadvantage of avoiding retraining from scratch. However, one common issue of\nexisting evaluation datasets for knowledge editing is that they do not ensure\nLLMs actually generate hallucinated answers to the evaluation questions before\nediting. When LLMs are evaluated on such datasets after being edited by\ndifferent techniques, it is hard to directly adopt the performance to assess\nthe effectiveness of different knowledge editing methods in correcting\nhallucinations. Thus, the fundamental question remains insufficiently\nvalidated: Can knowledge editing really correct hallucinations in LLMs? We\nproposed HalluEditBench to holistically benchmark knowledge editing methods in\ncorrecting real-world hallucinations. First, we rigorously construct a massive\nhallucination dataset with 9 domains, 26 topics and more than 6,000\nhallucinations. Then, we assess the performance of knowledge editing methods in\na holistic way on five dimensions including Efficacy, Generalization,\nPortability, Locality, and Robustness. Through HalluEditBench, we have provided\nnew insights into the potentials and limitations of different knowledge editing\nmethods in correcting hallucinations, which could inspire future improvements\nand facilitate the progress in the field of knowledge editing.",
        "updated": "2024-10-21 17:55:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16251v1"
    },
    {
        "title": "Analyzing Context Contributions in LLM-based Machine Translation",
        "authors": "Emmanouil ZaranisNuno M. GuerreiroAndré F. T. Martins",
        "links": "http://arxiv.org/abs/2410.16246v1",
        "entry_id": "http://arxiv.org/abs/2410.16246v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16246v1",
        "summary": "Large language models (LLMs) have achieved state-of-the-art performance in\nmachine translation (MT) and demonstrated the ability to leverage in-context\nlearning through few-shot examples. However, the mechanisms by which LLMs use\ndifferent parts of the input context remain largely unexplored. In this work,\nwe provide a comprehensive analysis of context utilization in MT, studying how\nLLMs use various context parts, such as few-shot examples and the source text,\nwhen generating translations. We highlight several key findings: (1) the source\npart of few-shot examples appears to contribute more than its corresponding\ntargets, irrespective of translation direction; (2) finetuning LLMs with\nparallel data alters the contribution patterns of different context parts; and\n(3) there is a positional bias where earlier few-shot examples have higher\ncontributions to the translated sequence. Finally, we demonstrate that\ninspecting anomalous context contributions can potentially uncover pathological\ntranslations, such as hallucinations. Our findings shed light on the internal\nworkings of LLM-based MT which go beyond those known for standard\nencoder-decoder MT models.",
        "updated": "2024-10-21 17:51:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16246v1"
    },
    {
        "title": "ToW: Thoughts of Words Improve Reasoning in Large Language Models",
        "authors": "Zhikun XuMing ShenJacob DineenZhaonan LiXiao YeShijie LuAswin RRVChitta BaralBen Zhou",
        "links": "http://arxiv.org/abs/2410.16235v1",
        "entry_id": "http://arxiv.org/abs/2410.16235v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16235v1",
        "summary": "We introduce thoughts of words (ToW), a novel training-time data-augmentation\nmethod for next-word prediction. ToW views next-word prediction as a core\nreasoning task and injects fine-grained thoughts explaining what the next word\nshould be and how it is related to the previous contexts in pre-training texts.\nOur formulation addresses two fundamental drawbacks of existing next-word\nprediction learning schemes: they induce factual hallucination and are\ninefficient for models to learn the implicit reasoning processes in raw texts.\nWhile there are many ways to acquire such thoughts of words, we explore the\nfirst step of acquiring ToW annotations through distilling from larger models.\nAfter continual pre-training with only 70K ToW annotations, we effectively\nimprove models' reasoning performances by 7% to 9% on average and reduce model\nhallucination by up to 10%. At the same time, ToW is entirely agnostic to tasks\nand applications, introducing no additional biases on labels or semantics.",
        "updated": "2024-10-21 17:41:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16235v1"
    }
]