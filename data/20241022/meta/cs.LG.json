[
    {
        "title": "xGen-MM-Vid (BLIP-3-Video): You Only Need 32 Tokens to Represent a Video Even in VLMs",
        "authors": "Michael S. RyooHonglu ZhouShrikant KendreCan QinLe XueManli ShuSilvio SavareseRan XuCaiming XiongJuan Carlos Niebles",
        "links": "http://arxiv.org/abs/2410.16267v1",
        "entry_id": "http://arxiv.org/abs/2410.16267v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16267v1",
        "summary": "We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for\nvideos, particularly designed to efficiently capture temporal information over\nmultiple frames. BLIP-3-Video takes advantage of the 'temporal encoder' in\naddition to the conventional visual tokenizer, which maps a sequence of tokens\nover multiple frames into a compact set of visual tokens. This enables\nBLIP3-Video to use much fewer visual tokens than its competing models (e.g., 32\nvs. 4608 tokens). We explore different types of temporal encoders, including\nlearnable spatio-temporal pooling as well as sequential models like Token\nTuring Machines. We experimentally confirm that BLIP-3-Video obtains video\nquestion-answering accuracies comparable to much larger state-of-the-art models\n(e.g., 34B), while being much smaller (i.e., 4B) and more efficient by using\nfewer visual tokens. The project website is at\nhttps://www.salesforceairesearch.com/opensource/xGen-MM-Vid/index.html",
        "updated": "2024-10-21 17:59:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16267v1"
    },
    {
        "title": "Revisiting Deep Feature Reconstruction for Logical and Structural Industrial Anomaly Detection",
        "authors": "Sukanya PatraSouhaib Ben Taieb",
        "links": "http://arxiv.org/abs/2410.16255v1",
        "entry_id": "http://arxiv.org/abs/2410.16255v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16255v1",
        "summary": "Industrial anomaly detection is crucial for quality control and predictive\nmaintenance, but it presents challenges due to limited training data, diverse\nanomaly types, and external factors that alter object appearances. Existing\nmethods commonly detect structural anomalies, such as dents and scratches, by\nleveraging multi-scale features from image patches extracted through deep\npre-trained networks. However, significant memory and computational demands\noften limit their practical application. Additionally, detecting logical\nanomalies-such as images with missing or excess elements-requires an\nunderstanding of spatial relationships that traditional patch-based methods\nfail to capture. In this work, we address these limitations by focusing on Deep\nFeature Reconstruction (DFR), a memory- and compute-efficient approach for\ndetecting structural anomalies. We further enhance DFR into a unified\nframework, called ULSAD, which is capable of detecting both structural and\nlogical anomalies. Specifically, we refine the DFR training objective to\nimprove performance in structural anomaly detection, while introducing an\nattention-based loss mechanism using a global autoencoder-like network to\nhandle logical anomaly detection. Our empirical evaluation across five\nbenchmark datasets demonstrates the performance of ULSAD in detecting and\nlocalizing both structural and logical anomalies, outperforming eight\nstate-of-the-art methods. An extensive ablation study further highlights the\ncontribution of each component to the overall performance improvement. Our code\nis available at https://github.com/sukanyapatra1997/ULSAD-2024.git",
        "updated": "2024-10-21 17:56:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16255v1"
    },
    {
        "title": "Distribution Learning with Valid Outputs Beyond the Worst-Case",
        "authors": "Nick RittlerKamalika Chaudhuri",
        "links": "http://arxiv.org/abs/2410.16253v1",
        "entry_id": "http://arxiv.org/abs/2410.16253v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16253v1",
        "summary": "Generative models at times produce \"invalid\" outputs, such as images with\ngeneration artifacts and unnatural sounds. Validity-constrained distribution\nlearning attempts to address this problem by requiring that the learned\ndistribution have a provably small fraction of its mass in invalid parts of\nspace -- something which standard loss minimization does not always ensure. To\nthis end, a learner in this model can guide the learning via \"validity\nqueries\", which allow it to ascertain the validity of individual examples.\nPrior work on this problem takes a worst-case stance, showing that proper\nlearning requires an exponential number of validity queries, and demonstrating\nan improper algorithm which -- while generating guarantees in a wide-range of\nsettings -- makes an atypical polynomial number of validity queries. In this\nwork, we take a first step towards characterizing regimes where guaranteeing\nvalidity is easier than in the worst-case. We show that when the data\ndistribution lies in the model class and the log-loss is minimized, the number\nof samples required to ensure validity has a weak dependence on the validity\nrequirement. Additionally, we show that when the validity region belongs to a\nVC-class, a limited number of validity queries are often sufficient.",
        "updated": "2024-10-21 17:56:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16253v1"
    },
    {
        "title": "Implicit Regularization for Tubal Tensor Factorizations via Gradient Descent",
        "authors": "Santhosh KarnikAnna VeselovskaMark IwenFelix Krahmer",
        "links": "http://arxiv.org/abs/2410.16247v1",
        "entry_id": "http://arxiv.org/abs/2410.16247v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16247v1",
        "summary": "We provide a rigorous analysis of implicit regularization in an\noverparametrized tensor factorization problem beyond the lazy training regime.\nFor matrix factorization problems, this phenomenon has been studied in a number\nof works. A particular challenge has been to design universal initialization\nstrategies which provably lead to implicit regularization in gradient-descent\nmethods. At the same time, it has been argued by Cohen et. al. 2016 that more\ngeneral classes of neural networks can be captured by considering tensor\nfactorizations. However, in the tensor case, implicit regularization has only\nbeen rigorously established for gradient flow or in the lazy training regime.\nIn this paper, we prove the first tensor result of its kind for gradient\ndescent rather than gradient flow. We focus on the tubal tensor product and the\nassociated notion of low tubal rank, encouraged by the relevance of this model\nfor image data. We establish that gradient descent in an overparametrized\ntensor factorization model with a small random initialization exhibits an\nimplicit bias towards solutions of low tubal rank. Our theoretical findings are\nillustrated in an extensive set of numerical simulations show-casing the\ndynamics predicted by our theory as well as the crucial role of using a small\nrandom initialization.",
        "updated": "2024-10-21 17:52:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16247v1"
    },
    {
        "title": "MoRE: Multi-Modal Contrastive Pre-training with Transformers on X-Rays, ECGs, and Diagnostic Report",
        "authors": "Samrajya ThapaKoushik HowladerSubhankar BhattacharjeeWei le",
        "links": "http://arxiv.org/abs/2410.16239v1",
        "entry_id": "http://arxiv.org/abs/2410.16239v1",
        "pdf_url": "http://arxiv.org/pdf/2410.16239v1",
        "summary": "In this paper, we introduce a novel Multi-Modal Contrastive Pre-training\nFramework that synergistically combines X-rays, electrocardiograms (ECGs), and\nradiology/cardiology reports. Our approach leverages transformers to encode\nthese diverse modalities into a unified representation space, aiming to enhance\ndiagnostic accuracy and facilitate comprehensive patient assessments. We\nutilize LoRA-Peft to significantly reduce trainable parameters in the LLM and\nincorporate recent linear attention dropping strategy in the Vision\nTransformer(ViT) for smoother attention. Furthermore, we provide novel\nmultimodal attention explanations and retrieval for our model. To the best of\nour knowledge, we are the first to propose an integrated model that combines\nX-ray, ECG, and Radiology/Cardiology Report with this approach. By utilizing\ncontrastive loss, MoRE effectively aligns modality-specific features into a\ncoherent embedding, which supports various downstream tasks such as zero-shot\nclassification and multimodal retrieval. Employing our proposed methodology, we\nachieve state-of-the-art (SOTA) on the Mimic-IV, CheXpert, Edema Severity, and\nPtbXl downstream datasets, surpassing existing multimodal approaches. Our\nproposed framework shows significant improvements in capturing intricate\ninter-modal relationships and its robustness in medical diagnosis that\nestablishes a framework for future research in multimodal learning in the\nhealthcare sector.",
        "updated": "2024-10-21 17:42:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.16239v1"
    }
]