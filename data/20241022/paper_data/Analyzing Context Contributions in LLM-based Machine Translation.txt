Analyzing Context Contributions in LLM-based Machine Translation
EmmanouilZaranis1,2,NunoM.Guerreiro1,2,3,4,AndréF.T.Martins1,2,3
1InstitutodeTelecomunicações,2InstitutoSuperiorTécnico,3Unbabel,4MICS
emmanouil.zaranis@tecnico.ulisboa.pt
Abstract 2023;Alvesetal.,2023;Garciaetal.,2023). Yet,
there is a gap in the literature on understanding
Largelanguagemodels(LLMs)haveachieved
theinternalworkingsofLLM-basedMT.Previous
state-of-the-artperformanceinmachinetrans-
interpretability research on MT has been limited
lation (MT) and demonstrated the ability to
totraditional,specializedencoder-decodermodels
leveragein-contextlearningthroughfew-shot
examples. However,themechanismsbywhich (Dingetal.,2017;Ferrandoetal.,2022a,b;Voita
LLMsusedifferentpartsoftheinputcontext etal.,2021;Sartietal.,2024;MohammedandNic-
remain largely unexplored. In this work, we ulae,2024),andwhilesubstantialworkhasinvesti-
provide a comprehensive analysis of context gatedICLinothertasks,suchasclassification(Min
utilizationinMT,studyinghowLLMsusevar- etal.,2022;Luetal.,2022;Yooetal.,2022;Wang
iouscontextparts,suchasfew-shotexamples
et al., 2023) and question answering (Liu et al.,
and the source text, when generating transla-
2022; Liu et al., 2023; Si et al., 2023; Wei et al.,
tions. We highlight several key findings: (1)
2023),themechanismsbywhichLLMsleverage
thesourcepartoffew-shotexamplesappears
tocontributemorethanitscorrespondingtar- partsofcontextinMTremainlargelyunexplored.
gets, irrespective of translation direction; (2) Inthiswork,weaimtofillthisresearchgapby
finetuningLLMswithparalleldataaltersthe contributingtowardsabetterunderstandingofhow
contributionpatternsofdifferentcontextparts; LLMs utilize different parts of the provided con-
and (3) there is a positional bias where ear-
text (e.g., few-shot examples, the source text, or
lier few-shot examples have higher contribu-
previouslygeneratedtargettokens)inMT.While
tions to the translated sequence. Finally, we
previousworkconductedonunderstandingtheim-
demonstratethatinspectinganomalouscontext
contributionscanpotentiallyuncoverpatholog- pactofcontextinMTlargelyfocusesonperform-
ical translations, such as hallucinations. Our ingmodificationsontheLLMinputandmeasuring
findingsshedlightontheinternalworkingsof performancedrop(Zhuetal.,2023;Raunaketal.,
LLM-basedMTwhichgobeyondthoseknown 2023), we take instead an attribution-based ap-
forstandardencoder-decoderMTmodels.
proach(Ferrandoetal.,2022a),trackingtheinput
tokens’relevanceinallpartsofthecontext—this
1 Introduction
allowsustoestimatehowdifferentpartsofcontext
Largelanguagemodels(LLMs)havereachedstate- contributetothegeneratedtranslations,providing
of-the-artperformanceinmachinetranslation(MT) amorefine-grainedanalysisofcontextutilization.
andaremakingsignificantstridestowardbecoming We study several key aspects of context uti-
thedefactosolutionforneuralMT(Kocmietal., lization in MT using general purpose LLaMA-2
2023;Alvesetal.,2024). Comparedtotheclassi- models(Touvronetal.,2023)and TOWERmodels
calstandardapproachusingencoder-decodermod- (Alvesetal.,2024)—asuiteofmodelsspecifically
els (Bahdanau et al., 2016; Vaswani et al., 2017), adaptedfortranslationtasks. First,weinvestigate
LLMs are typically decoder-only models param- how different input parts contribute to the trans-
eterized by billions of parameters. Remarkably, latedsequence. Next,weexplorewhetherthepro-
LLMs have demonstrated the ability to perform videdfew-shotexamplescontributeequallytothe
translation tasks without being explicitly trained translated sequence. We also analyze if undergo-
for them, instead leveraging in-context learning ing adaptation via continuous pretraining (Gupta
(ICL)throughdemonstrationsofthetask(Zhang et al., 2023; Çag˘atay Yıldız et al., 2024; Alves
et al., 2022; Agrawal et al., 2023; Hendy et al., etal.,2024)onrelevantmultilingualandparallel
4202
tcO
12
]LC.sc[
1v64261.0142:viXraMatrix of contributionsMℓ E1 E2 SRC
y
MTy1
MTy2
MTy3
MTy4
MTy5
MTy6
E1 E1 E1 E2 E2 E2 SRC SRC SRC SRC MT MT MT MT MT 0 0.5 1 0 0.5 1 0 0.5 1
x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 y1 y2 y3 y4 y5
Context Part Contributions
0.30 0.14 0.35
Figure1: Illustrationofsyntheticpart-leveltotalcontributionscomputationgiven2examplesascontext. Fromthe
token-to-tokenlevelcontributionmatrixMℓ,wecomputethetotalcontributionofeachinputparttoeachgenerated
y
token,bysummingthecorrespondingtoken-levelcontributions. Subsequently,wecomputethepart-leveltotal
contributionofeachinputparttothetranslatedsequence,byaveragingoverthegeneratedtokens.
dataleadstoachangeinthesecontributionpatterns. 2.1 In-ContextLearning(ICL)
Moreover,tofurtherunderstandthetranslationdy-
ICL is a paradigm where LLMs "learn" to solve
namics,weexaminehowcontextcontributionsvary
new tasks at inference time by being provided
at different stages of the generation process. Fi-
with a few task demonstrations as part of the
nally, we also assess whether anomalous context
input prompt, without requiring any updates to
contributionscanuncovercatastrophictranslations,
theirparametersorfine-tuning(Brownetal.,2020;
suchashallucinations(Daleetal.,2023a).
Agrawal et al., 2023; Hendy et al., 2023). More
Ouranalysisrevealsseveralkeyinsightsoncon-
broadly, for MT, few-shot examples can also be
textutilizationbyLLMsfortranslation,including:
usedforinferencetimeadaptation,e.g. todifferent
• Irrespective of the translation direction, the domains,terminology,orotherelementsoftrans-
sourceofeachfew-shotexamplecontributes lation, guidingthemodeltoproduceoutputsthat
morethanitscorrespondingtarget; aremoresuitableforthegivencontext(Alvesetal.,
2023;AycockandBawden,2024).
• The examined models exhibit a positional
bias—earlierfew-shotexamplestendtohave 2.2 ALTIforautoregressivelanguagemodels
higher contributions to the translated se-
For our analysis, we choose the ALTI (Aggrega-
quence. Additionally,thebiasismaintained
tion of Layer-Wise Token-to-Token Interactions)
acrossdifferentgenerationstages;
method(Ferrandoetal.,2022a)foritssimplicity
• Training on task-specific data reduces the andprovensuccessinvariousapplications. ALTI
influence of few-shot examples and conse- hasbeensuccessfullyemployedfordetectinghal-
quentlyshrinksthepositionalbiasobserved; lucinations in MT (Dale et al., 2023b; Guerreiro
et al., 2023), identifying toxicity in multilingual
• Lowsourcecontributionscanpotentiallyun- text (Team et al., 2022; Costa-jussà et al., 2023),
coverpathologicaltranslations. and explaining information flows in LLMs (Fer-
randoandVoita,2024;Tufanovetal.,2024).
We release all our code, and make our results
availableacrossalltestedmodelsandlanguages.1 ALTIisaninputattributionmethodthatquanti-
fies the mixing of information in the transformer
2 ProblemFormulation architecture (Vaswani et al., 2017). It follows
the modeling approach proposed by Abnar and
In this section, we introduce ICL and describe
Zuidema(2020),wheretheinformationflowinthe
howweemploytheALTImethod(Ferrandoetal.,
modelissimplifiedasadirectedacyclicgraph,with
2022a) to measure the contribution of each input
nodesrepresentingtokenrepresentationsandedges
partinthecontexttothetranslatedsequence.
representingtheinfluenceofeachinputtokenrep-
1https://github.com/deep-spin/interp_llm resentationontheoutputtokenrepresentation(foreachlayerofthetransformer). ALTIproposesus- pretrainedcheckpointof LLAMA-2 7Bonamix-
ing token contributions instead of raw attention tureofmonolingualandparalleldata(Alvesetal.,
weights,andcomputestheamountofinformation 2024). WealsoexperimentwithTOWERINSTRUCT
flowingfromonenodetoanotherindifferentlay- 7B,whichisobtainedviafinetuning TOWER ona
ersbysummingoverthedifferentpathsconnecting setofinstructionsfortranslation-relatedtasks.5
both nodes, where each path is the result of the
multiplicationofeveryedgeinthepath. Formally, Datasets. Weconductourstudyonthepublicly
givenaninputsequenceoflengthS andanoutput availableWMT22testsets,examiningEnglishto
sequenceoflengthT,wecomputeatoken-to-token
German(en-de),GermantoEnglish(de-en),En-
contributionmatrixCℓ ∈ R(S+T)×(S+T),whereℓ glish to Russian (en-ru) and Russian to English
istheℓ-thlayerofthemodel.2 Theelementcℓ of (ru-en)languagepairs.6
i,j
the matrix represents the contribution of the j-th
Few-shotsettingandpromptselection. Wecon-
inputtokenatlayerℓ−1tothei-thoutputtoken
ductouranalysisundera5-shotsetting,usingthe
at layer ℓ. By multiplying the layer-wise coeffi-
few-shotexamplesprovidedbyHendyetal.2023,
cient matrices, Mℓ = Cℓ ·Cℓ−1···C1 we can
which were selected to be high-quality examples
describerepresentationsofintermediatelayers(and
andrelevant—accordingtoembeddingsimilarity—
final layer) as a linear combination of the model
tothesourcetext. Wemakesurethattheexamples
inputtokens—anexampleofacontributionmatrix
in the context are shuffled and not sorted by rele-
isshowninFigure1.3 Thismatrixcanbeusedto
vancetothesource.7 Weusetheprompttemplates
interpretthemodel’sbehaviorandstudyhowdiffer-
suggestedinZhangetal.2023. Additionaldetails
entpartsoftheinputinfluencegeneratedoutputs.
areprovidedinAppendixA.1.
Formoredetails,seeFerrandoetal.(2022a).
2.3 Part-levelcontributions Filtering. DuetothehighGPUmemoryrequire-
ments of the attribution method when applied to
Toquantifythecontributionofeachinputpartto
a 7B parameter model, we had to filter samples
thetranslatedsequence,weperformatwo-stepag-
withlargecontextlength. Weprovidemoredetails
gregationprocess,illustratedinFigure1. First,we
aboutthefilteringprocessinAppendixA.2.
computethetotalcontributionofeachparttoeach
generated token by summing the corresponding
4 HowDoDifferentContextParts
token-level contributions within each part (right
ContributetotheTranslatedSequence?
hand-sideofFigure1). Then,weaveragethepart-
to-tokencontributionsacrossthegeneratedtokens
In this section, we conduct a top-level analysis
tocomputethecontributionsofeachcontextpart
bymeasuringandcomparingthecontributionsof
totheentiretranslatedsequence. Similarlyto(Fer-
differentinputpartstothegeneratedtranslation.
randoetal.,2022a;Daleetal.,2023a,b;Guerreiro
etal.,2023),thesepart-levelcontributionsareused 4.1 Analysissetup
fortheanalysisinthefollowingsections.4
Toinvestigatethecontributionofdifferentprompt
3 ExperimentalSetup partstothetranslatedsequence,wefirstdividethe
contextintothefollowingparts: sourceandtarget
Weprovideanoverviewofthemodelsanddatasets
sideofeachfew-shotexample,sourcetext,andtar-
used throughout our study, as well as important
getprefix. Then,wefollowtheapproachdescribed
considerationsonhowwepromptthemodels.
inSection2.3andobtainpart-levelcontributions
Models. We experiment with two families of thatareusedforanalysis.
models: the general-purpose LLAMA-2 7B base
5We use the following HuggingFace checkpoints:
model(Touvronetal.,2023),andthestate-of-the-
LLAMA-2 (meta-llama/Llama-2-7b-hf), TOWER
art TOWER 7B base model, which is a continued (Unbabel/TowerBase-7B-v0.1), and TOWERINSTRUCT
(Unbabel/TowerInstruct-7B-v0.2).
2Notethatthismatrixiscausalmasked. 6GermanisthesecondmostfrequentlanguageinLLAMA-
3Forsimplicity,wewillconsiderM yℓ asthematrixcon- 2(Touvronetal.,2023),justbehindEnglish,whileRussian
tainingthelastT rowsofMℓ—theserowscontainthecontri- accountsforapproximately0.13%ofthetrainingdata.
butionsoftheinputpartstotheoutputtokens. 7Weincludeexperimentswithadifferentshufflingseedin
4Wefollowpreviousworkandanalyzethelast-layercon- AppendixB—trendsinresultsaresimilartothosereportedin
tributions. themaintext.Llama-2 Tower-Mono Tower TowerInstruct
0.25
0.20
0.15
0.10
0.05
0.00
ExSrc E1 xTrgt E1 xSrc E2 xTrgt E2 xSrc E3 xTrgt E3 xSrc E4 xTrgt E4 xSrc E5 xTrgt5 Sour Tc ae rgetPrefix ExSrc E1 xTrgt E1 xSrc E2 xTrgt E2 xSrc E3 xTrgt E3 xSrc E4 xTrgt E4 xSrc E5 xTrgt5 Sour Tc ae rgetPrefix
GermantoEnglish EnglishtoGerman
Figure2: Illustrationofcontext’spart-levelcontributionstothetranslatedsequence,foralltheexaminedmodels.
Contribution Ratio
to E1|SRC
E1|SRC EsgibtauchzweischöneParksinderNähe,denEspanyaIndustrialParkunddenParcdeJoanMiró.
E1|TGT Therearealsotwobeautifulparksnearby,theEspanyaIndustrialParkandtheParcdeJoanMiró.
E2|SRC DasFrühstückistimPreis(10C)enthalten,esistaberoptional.
[...]
SRC Diegibteszwarauch(anscheinend?)beidenMarathonPlusReifen,aberderGroßteilistschonbreiter.
MT Therearealsotwobeautifulparksnearby,theEspanyaIndustrialParkandtheParcdeJoanMiró.
0 0.5 1
Figure3: ExampleofanomaloussourcecontributionsforTOWERwhichhallucinates,copyinginformationfromthe
firstexample. Weshowcontributionratiosto E1|SRC—1beingthecontributionof E1|SRC.
4.2 Results muchgreaterfor LLAMA-2 thanforboth TOWER
models. Onehypothesisisthatthecontinuedpre-
In Figure 2, we show, for all the examined mod-
els, the total contribution of each context part to
trainingwithparalleldataonTOWERmakesitrely
less on the examples since it is not required to
thetranslatedsequenceforGermantoEnglishand
“learn” the task “on-the-fly”. This leads to an in-
EnglishtoGermanlanguagepairs.
terestingquestion: whatifwereplacetheparallel
The source of each few-shot example consis- data and instead only use monolingual data for
tentlycontributesmorethanitscorresponding multiplelanguages? Toinvestigatethis,weexam-
target. Foreachoftheexaminedmodels,weno- inethe TOWER-MONO model.8 Interestingly,we
tice that the source of each provided example is findthat TOWER-MONO behavesmuchmoresimi-
moreinfluentialthanthecorrespondingtargetfor larlyto LLAMA-2than TOWER. Thissuggeststhat
generating the translation. This finding is consis- continual pretraining with task-specific data may
tentacrosslanguagepairs. Aligningwithfindings leadthemodeltorelylessonexamplestoperform
inclassicalencoder-decoderMTmodels(Ferrando thetask. Exploringhowtotraindedicatedmodels
etal.,2022a;Guerreiroetal.,2023),whereitwas to be better guided by in-context examples is an
foundthatmodelstendtohavehighersourcetext interestingdirectionforfuturework.
contributionwhentranslatingintoEnglishthanout
of English, we find that the source contribution, Closeinspectionofcontextcontributionscanun-
bothattheexampleandtestsourcelevel,ishigher coveranomaloustranslations. Previousworks
forGermantoEnglishthaninEnglishtoGerman. in neural MT have connected trends in context
Training on parallel data reduces the impact 8TOWER-MONOwastrainedfollowingthesametraining
of the provided examples on the translated se- procedureasTOWER(Alvesetal.,2024).Theonlydifference
totheformeristhat,insteadofusing20Btokensoftextsplit
quence. Weobservethatthecontributionsoffew-
in2/3monolingualdataand1/3paralleldata,itwastrained
shotexamples,particularlythefirstexamples,are with20Btokensofmonolingualdata.
)ITLA(noitubirtnoClatoTcontributions, particularly low source contribu- Llama-2 TowerInstruct
tions,topathologicaltranslationssuchashalluci- Tower-Mono Random
Tower
nations(Ferrandoetal.,2022a;Daleetal.,2023b; 100
Guerreiroetal.,2023). Throughcloseinspection
80
of our analyzed samples, we indeed find a series
ofpathologicaltranslations. Figure3presentsone 60
suchexample—here,thesourcecontributionispar-
40
ticularlylow,representingonlyabout25%ofthe
contributionofthefirstexample;interestingly,the 20
generated translation is, in fact, an exact copy of
0
thetranslationfromthatfirstexample. Weprovide K=1 K=2 K=3 K=4
additionalexamplesinAppendixB.3. Wewillre-
(a)
turntotheseandothersalientcasesinSection6to
100
examinehowcontributionsevolveforsuchcases
80
duringthegenerationprocess.
60
Aclearpositionaltrendemergesinfew-shotex-
40
amplecontributions. Figure2showsaremark-
able “stair-like” trend in the contribution of few- 20
shot examples to the translated sequence. On av-
0
erage,theinfluenceofeachexampleappearstobe
K=1 K=2 K=3 K=4
stronglycorrelatedwithitspositioninthecontext,
(b)
with earlier examples exhibiting higher contribu-
tionsthanlaterones. Thissuggeststheremaybe Figure4: Proportionofde-ensamplesthatfollowposi-
a positional bias in how the models leverage the tionalbias,fordifferentvaluesofK,inthe(a)original
providedexamplesduringthetranslationprocess. and(b)replace-last-exsettings.
5 ExaminingPositionalBiasoverthe
ples.9 WeconsiderdifferentvaluesofK torepre-
ProvidedFew-shotExamples
sentdifferenttypesofpositionalbias. Forinstance,
Motivatedbythefindingsfromtheprevioussection, when K = 1, the first few-shot example attains
wenowcloselyinspectpropertiesofthepositional the highest level of contribution. When K = 4,
thefew-shotexamplesexhibitgloballymonotonic
biasinfew-shotexamplecontributions.
contributions, indicating a strong positional bias
5.1 Areexamplesthatoccurearlyinthe acrossallexamples. Examplesforeachbiastype
contextmoreinfluentialthanlaterones? areprovidedinAppendixC.
To quantify the prevalence of each type of po-
Hereweperformasample-levelanalysistoobtaina
sitional bias, we measure the proportion of sam-
betterunderstandingoftherelationshipbetweenex-
plesthatsatisfytheaforementionedconditionfor
amples’contributionsandtheirrespectiveposition.
each value of K. We then compare these propor-
Specifically,weaimtoexplorewhetherthereisa
tions to the probability, under a permutation of
systematicandmonotonicrelationshipbetweenthe
theexamplesdrawnuniformlyatrandom(denoted
orderoffew-shotexamplesandtheircontributions.
as RANDOM), of the first K few-shot examples
monotonicallydominatingtheremainingN −K
5.1.1 Analysissetup
examples,whichisgivenasp = (N −K)!/N!.
Weexaminewhetherthecontributionsofthefirst
K few-shotexamplesmonotonicallydominatethe 5.1.2 Results
remaining N − K examples, where N is the to- WeshowresultsforGermantoEnglishtranslation
tal number of examples used in the context. In inFigure4a.10
otherwords,foreachsample,wecheckifthecon-
tributionsofthefirstK examplesaresortedinde- 9We do not require the contributions of the remaining
N −Kexamplestobemonotonicallysorted.
scendingorderandiftheyarestrictlyhigherthan
10Weincluderesultsfortherestlanguagepairsexamined
the contributions of the remaining N −K exam- inAppendixC—trendsarelargelysimilar.
selpmasfo%
selpmasfo%Llama-2 Tower-Mono Tower TowerInstruct 5.2.1 Isitallaboutposition?
0.25
First,weexaminetheimpactofaddingataskde-
0.20 scriptionbeforetheexamples.12 Ifthebiasissolely
0.15 position-dependent, we might expect the task de-
0.10 scription to receive higher contribution due to its
0.05 placement at the beginning of the context. This
0.00 analysiswillhelpusunderstandwhethertheposi-
TaskDesc Er xSrc E1 xTrgt E1 xSrc E2 xTrgt E2 xSrc E3 xTrgt E3 xSrc E4 xTrgt E4 xSrc E5 xTrgt5 Sourc Te argetPrefix t oi ron ifa il tb ii sas sti rs icin tlfl yu pe on sc ie tid ob ny -bt ah se en da .tureofthecontent
Taskdescriptionreceivesminimalcontribution
Figure 5: Illustration of context’s part-level contribu-
tions,whenthetaskdescriptionisadded. Translation despite its position. The results of our first ex-
direction: GermantoEnglish periment, shown in Figure 5, reveal that, despite
appearing at the beginning of the input text, the
taskdescriptionreceivessignificantlylowercontri-
Positionalbiasisprevalentandfollowsamono-
butioncomparedtotheexamplesandotherpartsof
tonic pattern. Our analysis reveals that posi-
thecontext. Thissuggeststhatthepositionalbiasis
tionalbiasissignificantlymorecommonthanthe
notmerelyafunctionofabsoluteposition,butmay
RANDOMbaselineforallvaluesofK,suggesting
ratherdependonthenatureofthecontent. Interest-
thatitisaprevalentphenomenonintheexamined
ingly,eventhoughanewpartofcontextwasadded,
models. Additionally,weobserveamonotonicre-
thepositionalbiasovertheexamples—“stair-like”
lationship: the bias is more frequent for the first
trendinthecontributions—isstillpresent.
fewexamplesthanforlaterones. Thisimpliesthat
theinfluenceofpositionalbiasgraduallydecreases 5.2.2 Canrelevancetothetestexamplebreak
aswemovefurtherdownthecontext. thebias?
We now investigate whether an overwhelmingly
The bias is particularly stark for the first few-
relevantexamplecanbreakthepositionalbias,even
shotexamples. Allmodelstendtoassignhigher
whenitappearslaterinthecontext.
contribution to the first example, with this bias
To test this, we create an artificial setup—
being more prevalent for models not trained on
replace-last-ex—where a copy of the test ex-
parallel data. For these models, over 95% of the
ample(sourceandtranslation)isplacedasthelast
analyzedsamplesexhibitthehighestcontribution
exampleinthecontext. Intuitively,ifthemodelis
for the first example.11 Models trained with par-
shownasourcetextalongwithitscorresponding
alleldata,eitherthroughcontinuedpretrainingor
translationinthecontext,themoststraightforward
additionalfinetuning,showaslightdecreaseinthe
approachwouldbetocopythetranslation. Assuch,
first-examplebias,butitremainssignificantcom-
weexpectthemodeltoassignhighercontribution
paredtothe RANDOMbaseline.
tothislastexample,overridingthepositionalbias.
Theobservedpositionalbiasraisesanimportant
question: are contributions merely a function of The bias is shrunk significantly. Figure 4b
position or are they connected to content of the shows that this intervention significantly reduces
context parts? We will conduct two additional thepositionalbias,particularlyfortheTOWERand
experimentsinthenextsectiontoinspectthisphe- TOWERINSTRUCTmodels. Incontrast,formodels
nomenoncloser. nottrainedonparalleldata,thefirstexamplestill
contributes more than all other examples—even
5.2 Howstrongisthepositionalbias? whenacopyispresentinthecontext—waymore
frequentlythanrandomchance. Interestingly,the
We now turn to a more detailed investigation of
biasisalmostentirelybrokenforallotherexample
thepositionaltrendwefoundintheresultsabove.
positions. These findings suggest that while rele-
Specifically,weinvestigatehowtheintroductionof
vantcontentcanindeedshrinkthebias,thefirstex-
othercontextpartsandtherelevanceoftheexam-
plesinteractwiththetrend. 12We can assume the "task description" as an additional
part of the context. We use the following description tem-
11We remark again that the examples in the context are plate: Translate the following text from [SRC_LANG] to
shuffledandnotsortedbyrelevancetothesource. [TGT_LANG]\n.
)ITLA(noitubirtnoClatoTGermantoEnglish EnglishtoGerman
0.2 ExSrc1 ExSrc4
ExTrgt1 ExTrgt4
ExSrc2 ExSrc5
0.1 ExTrgt2 ExTrgt5
ExSrc3 Source
ExTrgt3 TargetPrefix
0.0
b1 b2 b3 b4 b5 b6 b7 b8 b9 b10 b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
GeneratedSeqinBins(b) GeneratedSeqinBins(b)
Figure6: IllustrationofhowcontextcontributionsevolveacrossdifferentgenerationstagesfortheTOWERmodel.
Eachgeneratedbinaccountsfor10%ofthegeneratedsequence.
amplesinfluencethetranslationgenerationbeyond findadecreaseinthesourcecontributionatthelast
simply“solvingthetask.” Theylikelyprovideaddi- stageofgeneration,suggestingthatthemodelrelies
tionalcues,suchasthelanguagepairandexpected lessonthesourcewhengeneratingthefinaltokens.
outputformat,thatshapethemodel’sbehavior. Interestingly, both these observations align with
findings in traditional neural MT models, which
6 HowDoContextContributionsEvolve haveshownsimilarpatternsintherelativecontri-
duringtheGenerationProcess? butionsofsourceandtargetinformationduringthe
generationprocess(Voitaetal.,2021).
Intheprevioussections,weexaminedwhichparts
oftheprovidedcontexthavethegreatestinfluence Translation direction impacts the evolution of
onthetranslatedsequence. Wenowshiftourfocus contextcontributions. Whiletheoverallranking
toexplorehowthesecontextcontributionsevolve of context part contributions remains similar, we
acrossdifferentstagesofthegenerationprocess. observenotabledifferenceswhentranslatinginto
or out of English. As noted earlier in Section 4,
6.1 Analysissetup
thesourcecontributionishigherwhentranslating
To investigate this, we divide the generated se- into English (de-en) compared to when translat-
quence into 10 bins of equal length and compute ingoutofEnglish(en-de). Interestingly,inde-en
thetotalcontributionofeachcontextparttoeach translation,thesourceofeachexamplealsoconsis-
bin. We then average these contributions across tentlycontributesmorethanitscorrespondingtar-
samples to obtain a comprehensive view of how get,resultingina“stacked”appearanceofsource
theinfluenceofdifferentcontextpartschangesas contributions—the contribution from any exam-
thetranslationprogresses. ple’s source is bigger than that of any example’s
targettext. Incontrast,en-detranslationexhibits
Results. InFigure6,wepresenttheaveragetotal
analternatingcontributionranking,withthesource
contributionofeachindividualparttoeachgener-
and target of each example interleaved (e.g., src
atedbin,forthe TOWERmodels.
example 1 > tgt example 1 > src example 2 >
Relativerankingofcontextparts’contributions tgtexample2,andsoon). Moreover,wealsoob-
remainsstablethroughoutgeneration. Weob- servethatthetargetprefixcontributiongrowsmuch
servethattherelativerankingofcontributionsfrom more steeply in en-de than in de-en, suggesting
differentcontextpartsislargelypreservedthrough- thatwhentranslatinganon-Englishtext,themodel
outthegenerationprocess. Specifically,thesource reliesmoreheavilyonthecontext(examplesand
textconsistentlyexhibitsthehighestcontribution source)throughoutthegenerationprocess.
across all bins, followed by the few-shot exam-
ples in descending order of their position—this Highlightingtheimportanceofsource-partcon-
reinforcesthenotionofpositionalbias. Theonly tributionsinanomalouscases. Buildingonour
exceptiontothispatternisthetargetprefix,which findingsfromSection4,whichshowedthatclose
attains higher contribution as it grows in length. inspectionofcontextcontributionscanpotentially
This is expected: with a longer prefix, the model uncoveranomaloustranslations,wefurtheranalyze
increasinglyreliesonthepreviouslygeneratedto- such cases in terms of how context contributions
kenstoinformitspredictions. Moreover,wealso evolveduringthegenerationprocess. Wecompare
)ITLA(noitubirtnoClatoTE1|SRC EsgibtauchzweischöneParksinderNähe,denEspanyaIndus- ExSrc1 ExSrc3 ExSrc5
ExTrgt1 ExTrgt3 ExTrgt5
trialParkunddenParcdeJoanMiró.
ExSrc2 ExSrc4 Source
E1|TGT Therearealsotwobeautifulparksnearby,theEspanyaIndustrial ExTrgt2 ExTrgt4 TargetPrefix
ParkandtheParcdeJoanMiró.
Llama-2
0.4
E2|SRC DasFrühstückistimPreis(10 C)enthalten,esistaberoptional.
E2|TGT Breakfastisincludedintheprice(10 C),butitisoptional. 0.3
E3|SRC EsgibtauchkostenloseInternet24/7andWiFiinallenZimmern.
0.2
E3|TGT Thereisalsofreeinternet24/7andwifiinallrooms.
E4|SRC BishergibtesnochkeineBewertungenfürS-PlusCompany! 0.1
E4|TGT TherearenoreviewsforS-PlusCompanyyet!
0.0
E5|SRC Die Größe der Wohnung ist 15 m2, es ist klein, aber sehr b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
gemütlich. GeneratedSeqinBins(b)
E5|TGT Thesizeoftheapartmentis15m2,it’ssmallbutverycosy. Tower
0.4
SRC Diegibteszwarauch(anscheinend?) beidenMarathonPlus
Reifen,aberderGroßteilistschonbreiter. 0.3
LLAMA-2✓
0.2
MT Therearealso(apparently?) atMarathonPlusTyres, butthe
majorityiswider. 0.1
TOWER✗
0.0
MT Therearealsotwobeautifulparksnearby,theEspanyaIndustrial
b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
ParkandtheParcdeJoanMiró.
GeneratedSeqinBins(b)
Table1: IllustrationofanexampleexhibitinganomaloussourcecontributionsforTOWER—whichhallucinates,
followedbyLLAMA-2’scontributions,whichperformsnormally.
thebehaviorofLLAMA-2andTOWERmodelsus- Lowsourcecontributionsare,insomecases,pre-
ing the example presented in Table 1 (the same dictiveofhallucinations. Ourpreviousobserva-
presented in Section 4). For LLAMA-2, which tionsmaypotentiallyalignwellwithpreviousneu-
generates a correct translation, the context con- ralMTresearchlinkingpathologicaltranslations
tribution trends align with the average case for tolowsourcecontributions(Ferrandoetal.,2022a;
German to English translation (see Figure 19 in Dale et al., 2023b; Guerreiro et al., 2023). Note
Appendix D.1). In contrast, TOWER, which pro- again that classical encoder-decoder MT models
ducesanincorrecttranslationbycopyingthefirst andlargelanguagemodels(LLMs)aredistinctin
example, exhibits anomalous contribution trends termsofthepartsofcontexttheyoftensupport: in
(compared to Figure 6). Specifically, we ob- classicalencoder-decoderNMTmodels,the"con-
serve a steeply increasing contribution from the text" for generation typically comprises only the
first example, while the source contribution de- sourcesentenceandpreviouslygeneratedtokens;
creasessignificantly,highlightingthecopyingbe- LLMs, however, often maintain a much broader
havior. AdditionalsalientcasesarediscussedinAp- context, potentially including various other rele-
pendixD.2.13 Crucially,wefindthatinsuchcases, vantinformation. Thisdistinctionmeansthatlow
sourcecontributions—bothattheexampleandtest source contribution in LLMs may not be so pre-
sourcelevels—canpotentiallyindicatepathologi- dictive of pathological translations, as the model
caltranslationsandalsoprovideinsightsintothe might be drawing from other relevant contextual
factorsdrivingthegeneration. information. Toexplorethisfurther,weconducta
quantitativeanalysistoassesstheextenttowhich
LanguagePair Model AUROC low-source contribution can be associated with
en-ru LLAMA-2 52.3 hallucinations. Initially, for each model and lan-
de-en TOWER 97.3 guage pair combination, we identify instances of
en-ru TOWER 88.7
"fully-detached"hallucinationsbyannotatingthe
Table2: AUROCoflowsourcecontributionscores. generated translations using the LLAMA-3-70B-
INSTRUCTmodel(Dubeyetal.,2024),following
the exact approach outlined by Benkirane et al.,
13Here, we not only provide examples of other halluci-
nations,butalsoofothercorrecttranslationsforwhichthe
contextcontributionsfollowinterestingnon-typicalpatterns.
)ITLA(noitubirtnoClatoT
)ITLA(noitubirtnoClatoT2024.14 For each model-language pair combina- Limitations
tionforwhichweobservedareasonablenumber15
Whileourstudyprovidesavaluableinsightofhow
of "fully-detached" hallucinations, we report the
contextisutilizedbyLLMsinMT,thereareafew
AUROCofthelowsourcecontributionscoreinTa-
limitationsthatshouldbeacknowledged.
ble2. Ourfindings,suggestthatwhileforTOWER
Firstly, duetolimitationsintermsofcomputa-
lowsourcecontributionsareparticularlyassociated
tionalresourcespairedwiththefactthattheALTI
withhallucinations,itisnotthecasefor LLAMA2.
method employed in our study can be computa-
Uponcloserinspection,wefindthatthelowsource
tionallyintensive,werestrictedouranalysisto7B
contributionisparticularlypredictiveofhallucina-
parametermodels. Thisconstraintraisestheques-
tionsthatcomeintheformofexactcopiesofthe
tion of whether our findings still hold true when
providedfew-shotexamples16. Investigatingthese
largerLLMsareconsidered,makingitapotential
trendsfurther,notonlyinmachinetranslationbut
directionforfuturestudying.
alsoinothertaskswherecontextisrelevant,isan
Secondly, it should be noted that we focused
interestingdirectionforfutureresearch.
exclusivelyon LLAMA-basedmodels,particularly
aimingonanalyzingthe TOWER-familyofmodels,
7 Conclusion
whicharespecificallyorientedforMT.Thisselec-
tionenabledustostudyhowcontinuedpretraining
We have comprehensively studied context contri-
andfinetuningontask-specificdataimpactscontext
butionsinLLM-basedMTusingthegeneralpur-
utilization. However,thisdecisionmakesitsothat
poseLLAMA-2andtranslation-specializedTOWER
itisstillunclearwhetherourfindingsgeneralizeto
models,exploringabroadrangeofkeyaspects,in-
otherLLMfamilies.
cludinginvestigatinghowdifferentpartsofcontext
Despitetheselimitations,webelieveourstudy
contributetogeneratedtranslations,andhowthese
canleadtoabetterunderstandingofthedynamics
contributionsevolveduringthegenerationprocess.
ofcontextutilizationinLLM-basedMT,providing
Our findings reveal a strong positional bias,
keyinsightsthatcanmotivatefutureworkonthe
whereearlierfew-shotexamplesinthecontexthave
fieldandinspireotherresearchdirections.
higher contributions to the translated sequence,
bothatthesentencelevelandacrossdifferentgen- EthicalConsiderations&PotentialRisks
erationstages. Interestingly,ourexperimentsshow
UtilizingLLMsforMTmightraisepotentialrisks
that this bias is shrunk by continuous pretraining
thatshouldbepointedout,particularlyregarding
ontask-specificdata. Moreover,werevealthatthe
pathological translationsand the ethical usage of
source part of each few-shot example has higher
contextualdata.
contributioncomparedtoitscorrespondingtarget,
Firstly, one of the critical risks which arises
irrespective of the translation direction. Finally,
when using LLMs for MT is the phenomenon of
we stress the importance of source-part contribu-
pathological translations, such as hallucinations.
tions by demonstrating that anomalous contribu-
As our study reveals, anomalous context contri-
tions can potentially uncover pathological trans-
butionscanpotentiallyindicatethesepathological
lations, such as hallucinations. We believe our
translations, especially when low reliance on the
work not only provides insights into the internal
source text is noticed. Despite the potential of
workingsofLLM-basedMT,butalsodrawsimpor-
detectingthesepathologicaltranslations,theiroc-
tantconnectionstoclassicalencoder-decoderNMT
currenceremainsanimportantconcern,asmisinter-
models.
pretationsandincorrecttranslationsmightleadto
Tosupportfutureresearchonthistopic,weare
significantconsequencesinspecificdomainssuch
open-sourcingourcodeandreleasingalldataused
as healthcare, law etc. Thus ensuring that LLMs
inouranalysis.
providereliabletranslationsiscrucial.
Secondly,therelianceofLLMsinspecificparts
14Inthispaper,theauthorsshowthatthisLLMcanachieve
ofthecontextwhentranslating,introducesethical
performancecomparableorevenbetterthanpreviouslypro-
poseddetectors. considerations that should be taken into account
15Weprovidefurtherquantitativeresulsonthenumberof
regardingthechoiceofsomecontextparts,suchas
detectedhallucinationsinAppendixD.3.
thefew-shotexamples. Theprovidedcontextmight
16TOWER’spathologicaltranslationsareusuallycopiesof
thefew-shotexamples,whilethisisnotthecaseforLLAMA2. containbiasesandmisleadingorinappropriatecon-tent and as a result this might be propagated into Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua
thegeneratedtranslations. Ourresearchcansignifi- Bengio. 2016. Neural machine translation by
jointly learning to align and translate. Preprint,
cantlycontributetomitigatethisriskbyidentifying
arXiv:1409.0473.
whichpartsoftheprovidedcontextareresponsible
forpropagatingbiasesorinappropriatecontentto KenzaBenkirane,LauraGongas,ShaharPelles,Naomi
thetranslatedsequence. Fuchs,JoshuaDarmon,PontusStenetorp,DavidIfe-
oluwa Adelani, and Eduardo Sánchez. 2024. Ma-
Toconclude,addressingtheserisksandethical
chinetranslationhallucinationdetectionforlowand
considerationsisimportanttofosterabetterusage highresourcelanguagesusinglargelanguagemodels.
ofthesesystemsandpreventpotentialharms. Preprint,arXiv:2407.16470.
Acknowledgements TomB.Brown,BenjaminMann,NickRyder,Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
This work was supported by the Portuguese Neelakantan,PranavShyam,GirishSastry,Amanda
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Recovery and Resilience Plan through project
Gretchen Krueger, Tom Henighan, Rewon Child,
C645008882-00000055 (Center for Responsible
Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
AI), by EU’s Horizon Europe Research and In- Clemens Winter, Christopher Hesse, Mark Chen,
novationActions(UTTER,contract101070631), EricSigler,MateuszLitwin,ScottGray,Benjamin
Chess, Jack Clark, Christopher Berner, Sam Mc-
by the project DECOLLAGE (ERC-2022-CoG
Candlish, AlecRadford, IlyaSutskever, andDario
101088763), and by Fundação para a Ciência e
Amodei.2020. Languagemodelsarefew-shotlearn-
TecnologiathroughcontractUIDB/50008/2020. ers. Preprint,arXiv:2005.14165.
Marta Costa-jussà, Eric Smith, Christophe Ropers,
Daniel Licht, Jean Maillard, Javier Ferrando, and
References
CarlosEscolano.2023. Toxicityinmultilingualma-
SamiraAbnarandWillemZuidema.2020. Quantify- chinetranslationatscale. InFindingsoftheAssoci-
ing attention flow in transformers. In Proceedings ationforComputationalLinguistics: EMNLP2023,
of the 58th Annual Meeting of the Association for pages9570–9586,Singapore.AssociationforCom-
Computational Linguistics, pages 4190–4197, On- putationalLinguistics.
line.AssociationforComputationalLinguistics.
DavidDale, ElenaVoita, LoicBarrault, andMartaR.
Sweta Agrawal, Chunting Zhou, Mike Lewis, Luke Costa-jussà. 2023a. Detecting and mitigating hal-
Zettlemoyer, andMarjanGhazvininejad.2023. In- lucinations in machine translation: Model internal
contextexamplesselectionformachinetranslation. workingsalonedowell,sentencesimilarityEvenbet-
In Findings of the Association for Computational ter. In Proceedings of the 61st Annual Meeting of
Linguistics: ACL2023,pages8857–8873,Toronto, theAssociationforComputationalLinguistics(Vol-
Canada.AssociationforComputationalLinguistics. ume1: LongPapers),pages36–50,Toronto,Canada.
AssociationforComputationalLinguistics.
DuarteAlves,NunoGuerreiro,JoãoAlves,JoséPom-
bal, Ricardo Rei, José de Souza, Pierre Colombo, David Dale, Elena Voita, Janice Lam, Prangthip
andAndreMartins.2023. Steeringlargelanguage Hansanti,ChristopheRopers,ElaheKalbassi,Cyn-
modelsformachinetranslationwithfinetuningand thia Gao, Loic Barrault, and Marta Costa-jussà.
in-contextlearning. InFindingsoftheAssociation 2023b. HalOmi: Amanuallyannotatedbenchmark
forComputationalLinguistics: EMNLP2023,pages for multilingual hallucination and omission detec-
11127–11148,Singapore.AssociationforComputa- tion in machine translation. In Proceedings of the
tionalLinguistics. 2023ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages638–653,Singapore.As-
DuarteM.Alves,JoséPombal,NunoM.Guerreiro,Pe- sociationforComputationalLinguistics.
droH.Martins,JoãoAlves,AminFarajian,BenPe-
ters,RicardoRei,PatrickFernandes,SwetaAgrawal, YanzhuoDing,YangLiu,HuanboLuan,andMaosong
Pierre Colombo, José G. C. de Souza, and André Sun.2017. Visualizingandunderstandingneuralma-
F.T.Martins.2024. Tower: AnOpenMultilingual chinetranslation. InProceedingsofthe55thAnnual
LargeLanguageModelforTranslation-RelatedTasks. Meeting of the Association for Computational Lin-
arXive-prints,arXiv:2402.17733. guistics(Volume1: LongPapers),pages1150–1159,
Vancouver,Canada.AssociationforComputational
SethAycockandRachelBawden.2024. Topic-guided Linguistics.
example selection for domain adaptation in LLM-
based machine translation. In Proceedings of the AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,
18thConferenceoftheEuropeanChapteroftheAs- AbhishekKadian,AhmadAl-Dahle,AieshaLetman,
sociationforComputationalLinguistics: StudentRe- Akhil Mathur, Alan Schelten, Amy Yang, Angela
searchWorkshop,pages175–195,St.Julian’s,Malta. Fan,AnirudhGoyal,AnthonyHartshorn,AoboYang,
AssociationforComputationalLinguistics. ArchiMitra, ArchieSravankumar, ArtemKorenev,ArthurHinsvark,ArunRao,AstonZhang,Aurelien Yan,ZhengxingChen,ZoePapakipos,AadityaSingh,
Rodriguez, Austen Gregerson, Ava Spataru, Bap- AaronGrattafiori,AbhaJain,AdamKelsey,Adam
tiste Roziere, Bethany Biron, Binh Tang, Bobbie Shajnfeld,AdithyaGangidi,AdolfoVictoria,Ahuva
Chern,CharlotteCaucheteux,ChayaNayak,Chloe Goldstand,AjayMenon,AjaySharma,AlexBoesen-
Bi,ChrisMarra,ChrisMcConnell,ChristianKeller, berg,AlexVaughan,AlexeiBaevski,AllieFeinstein,
Christophe Touret, Chunyang Wu, Corinne Wong, Amanda Kallet, Amit Sangani, Anam Yunus, An-
CristianCantonFerrer,CyrusNikolaidis,DamienAl- drei Lupu, Andres Alvarado, Andrew Caples, An-
lonsius,DanielSong,DaniellePintz,DannyLivshits, drew Gu, Andrew Ho, Andrew Poulton, Andrew
David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Ryan, Ankit Ramchandani, Annie Franco, Apara-
DiegoGarcia-Olano,DiegoPerino,DieuwkeHupkes, jitaSaraf,ArkabandhuChowdhury,AshleyGabriel,
EgorLakomkin,EhabAlBadawy,ElinaLobanova, Ashwin Bharambe, Assaf Eisenman, Azadeh Yaz-
EmilyDinan,EricMichaelSmith,FilipRadenovic, dan,BeauJames,BenMaurer,BenjaminLeonhardi,
FrankZhang,GabrielSynnaeve,GabrielleLee,Geor- BernieHuang,BethLoyd,BetoDePaola,Bhargavi
gia Lewis Anderson, Graeme Nail, Gregoire Mi- Paranjape,BingLiu,BoWu,BoyuNi,BradenHan-
alon,GuanPang,GuillemCucurell,HaileyNguyen, cock,BramWasti,BrandonSpence,BraniStojkovic,
Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Brian Gamido, Britt Montalvo, Carl Parker, Carly
Zarov,ImanolArrietaIbarra,IsabelKloumann,Ishan Burton,CatalinaMejia,ChanghanWang,Changkyu
Misra,IvanEvtimov,JadeCopet,JaewonLee,Jan Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu,
Geffert,JanaVranes,JasonPark,JayMahadeokar, ChrisCai,ChrisTindal,ChristophFeichtenhofer,Da-
Jeet Shah, Jelmer van der Linde, Jennifer Billock, monCivin,DanaBeaty,DanielKreymer,DanielLi,
Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, DannyWyatt,DavidAdkins,DavidXu,DavideTes-
Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, tuggine,DeliaDavid,DeviParikh,DianaLiskovich,
Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph DidemFoss,DingkangWang,DucLe,DustinHol-
Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, land, Edward Dowling, Eissa Jamil, Elaine Mont-
Kalyan Vasuden Alwala, Kartikeya Upasani, Kate gomery,EleonoraPresani,EmilyHahn,EmilyWood,
Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, ErikBrinkman,EstebanArcaute,EvanDunbar,Evan
KhalidEl-Arini,KrithikaIyer,KshitizMalik,Kuen- Smothers, Fei Sun, Felix Kreuk, Feng Tian, Firat
leyChiu,KunalBhalla,LaurenRantala-Yeary,Lau- Ozgenel, Francesco Caggioni, Francisco Guzmán,
rensvanderMaaten,LawrenceChen,LiangTan,Liz FrankKanayet,FrankSeide,GabrielaMedinaFlo-
Jenkins,LouisMartin,LovishMadaan,LuboMalo, rez,GabriellaSchwarz,GadaBadeer,GeorgiaSwee,
Lukas Blecher, Lukas Landzaat, Luke de Oliveira, GilHalpern,GovindThattai,GrantHerman,Grigory
MadelineMuzzi,MaheshPasupuleti,MannatSingh, Sizov, Guangyi, Zhang, Guna Lakshminarayanan,
Manohar Paluri, Marcin Kardas, Mathew Oldham, HamidShojanazeri,HanZou,HannahWang,Han-
Mathieu Rita, Maya Pavlova, Melanie Kambadur, wen Zha, Haroun Habeeb, Harrison Rudolph, He-
Mike Lewis, Min Si, Mitesh Kumar Singh, Mona lenSuk,HenryAspegren,HunterGoldman,Ibrahim
Hassan,NamanGoyal,NarjesTorabi,NikolayBash- Damlaj, Igor Molybog, Igor Tufanov, Irina-Elena
lykov,NikolayBogoychev,NiladriChatterji,Olivier Veliche, Itai Gat, Jake Weissman, James Geboski,
Duchenne,OnurÇelebi,PatrickAlrassy,Pengchuan James Kohli, Japhet Asher, Jean-Baptiste Gaya,
Zhang, Pengwei Li, Petar Vasic, Peter Weng, Pra- JeffMarcus,JeffTang,JenniferChan,JennyZhen,
jjwal Bhargava, Pratik Dubal, Praveen Krishnan, JeremyReizenstein,JeremyTeboul,JessicaZhong,
Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill,
Dong,RagavanSrinivasan,RajGanapathy,Ramon Jon Shepard, Jonathan McPhie, Jonathan Torres,
Calderer, Ricardo Silveira Cabral, Robert Stojnic, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou
Roberta Raileanu, Rohit Girdhar, Rohit Patel, Ro- U, Karan Saxena, Karthik Prasad, Kartikay Khan-
mainSauvestre,RonniePolidoro,RoshanSumbaly, delwal, Katayoun Zand, Kathy Matosich, Kaushik
RossTaylor,RuanSilva,RuiHou,RuiWang,Saghar Veeraraghavan, Kelly Michelena, Keqian Li, Kun
Hosseini, Sahana Chennabasappa, Sanjay Singh, Huang,KunalChawla,KushalLakhotia,KyleHuang,
Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Lailin Chen, Lakshya Garg, Lavender A, Leandro
Shaoliang Nie, Sharan Narang, Sharath Raparthy, Silva,LeeBell,LeiZhang,LiangpengGuo,Licheng
Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Yu, Liron Moshkovich, Luca Wehrstedt, Madian
Zhang,SimonVandenhende,SoumyaBatra,Spencer Khabsa,ManavAvalani,ManishBhatt,MariaTsim-
Whitman,StenSootla,StephaneCollot,SuchinGu- poukelli,MartynasMankus,MatanHasson,Matthew
rurangan,SydneyBorodinsky,TamarHerman,Tara Lennie, Matthias Reso, Maxim Groshev, Maxim
Fowler,TarekSheasha,ThomasGeorgiou,Thomas Naumov,MayaLathi,MeghanKeneally,MichaelL.
Scialom,TobiasSpeckbacher,TodorMihaylov,Tong Seltzer, Michal Valko, Michelle Restrepo, Mihir
Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Patel, Mik Vyatskov, Mikayel Samvelyan, Mike
Gupta,VigneshRamanathan,ViktorKerkez,Vincent Clark,MikeMacey,MikeWang,MiquelJubertHer-
Gonguet, Virginie Do, Vish Vogeti, Vladan Petro- moso, Mo Metanat, Mohammad Rastegari, Mun-
vic,WeiweiChu,WenhanXiong,WenyinFu,Whit- ish Bansal, Nandhini Santhanam, Natascha Parks,
neyMeers,XavierMartinet,XiaodongWang,Xiao- NatashaWhite,NavyataBawa,NayanSinghal,Nick
qing Ellen Tan, Xinfeng Xie, Xuchao Jia, Xuewei Egebo,NicolasUsunier,NikolayPavlovichLaptev,
Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Ning Dong, Ning Zhang, Norman Cheng, Oleg
Babaei, YiWen, YiwenSong, YuchenZhang, Yue Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem
Li,YuningMao,ZacharieDelpierreCoudert,Zheng Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pa-van Balaji, Pedro Rittner, Philip Bontrager, Pierre Johnson,andOrhanFirat.2023. Theunreasonable
Roux,PiotrDollar,PolinaZvyagina,PrashantRatan- effectivenessoffew-shotlearningformachinetrans-
chandani,PritishYuvraj,QianLiang,RachadAlao, lation. Preprint,arXiv:2302.01398.
RachelRodriguez, RafiAyub, RaghothamMurthy,
RaghuNayani,RahulMitra,RaymondLi,Rebekkah NunoM.Guerreiro,DuarteAlves,JonasWaldendorf,
Hogan, Robin Battey, Rocky Wang, Rohan Mah- Barry Haddow, Alexandra Birch, Pierre Colombo,
eswari,RussHowes,RutyRinott,SaiJayeshBondu, and André F. T. Martins. 2023. Hallucinations in
Samyak Datta, Sara Chugh, Sara Hunt, Sargun Large Multilingual Translation Models. arXiv e-
Dhillon,SashaSidorov,SatadruPan,SaurabhVerma, prints,arXiv:2303.16104.
SeijiYamamoto,SharadhRamaswamy,ShaunLind-
say, Shaun Lindsay, Sheng Feng, Shenghao Lin, Kshitij Gupta, Benjamin Thérien, Adam Ibrahim,
Shengxin Cindy Zha, Shiva Shankar, Shuqiang Mats L. Richter, Quentin Anthony, Eugene
Zhang,ShuqiangZhang,SinongWang,SnehaAgar- Belilovsky, Irina Rish, and Timothée Lesort. 2023.
wal, Soji Sajuyigbe, Soumith Chintala, Stephanie Continual pre-training of large language mod-
Max,StephenChen,SteveKehoe,SteveSatterfield, els: How to (re)warm your model? Preprint,
Sudarshan Govindaprasad, Sumit Gupta, Sungmin arXiv:2308.04014.
Cho,SunnyVirk,SurajSubramanian,SyChoudhury,
Amr Hendy, Mohamed Abdelrehim, Amr Sharaf,
SydneyGoldman,TalRemez,TamarGlaser,Tamara
VikasRaunak,MohamedGabr,HitokazuMatsushita,
Best, Thilo Kohler, Thomas Robinson, Tianhe Li,
YoungJinKim,MohamedAfify,andHanyHassan
TianjunZhang,TimMatthews,TimothyChou,Tzook
Awadalla. 2023. How good are gpt models at ma-
Shaked, VarunVontimitta, VictoriaAjayi, Victoria
chinetranslation?acomprehensiveevaluation. arXiv
Montanez,VijaiMohan,VinaySatishKumar,Vishal
preprintarXiv:2302.09210.
Mangla,VítorAlbiero,VladIonescu,VladPoenaru,
VladTiberiuMihailescu, VladimirIvanov, WeiLi,
Tom Kocmi, Eleftherios Avramidis, Rachel Bawden,
WenchenWang,WenwenJiang,WesBouaziz,Will
Ondˇrej Bojar, Anton Dvorkovich, Christian Fed-
Constable,XiaochengTang,XiaofangWang,Xiao-
ermann, Mark Fishel, Markus Freitag, Thamme
jianWu,XiaolanWang,XideXia,XilunWu,Xinbo
Gowda, Roman Grundkiewicz, Barry Haddow,
Gao,YanjunChen,YeHu,YeJia,YeQi,YendaLi,
Philipp Koehn, Benjamin Marie, Christof Monz,
YilinZhang,YingZhang,YossiAdi,YoungjinNam,
MakotoMorishita,KentonMurray,MakotoNagata,
Yu,Wang,YuchenHao,YundiQian,YuziHe,Zach
Toshiaki Nakazawa, Martin Popel, Maja Popovic´,
Rait,ZacharyDeVito,ZefRosnbrick,ZhaoduoWen,
andMariyaShmatova.2023. Findingsofthe2023
ZhenyuYang,andZhiweiZhao.2024. Thellama3
conferenceonmachinetranslation(WMT23): LLMs
herdofmodels. Preprint,arXiv:2407.21783.
areherebutnotquitethereyet. InProceedingsofthe
Eighth Conference on Machine Translation, pages
Javier Ferrando, Gerard I. Gállego, Belen Alastruey,
1–42,Singapore.AssociationforComputationalLin-
CarlosEscolano, andMartaR.Costa-jussà.2022a.
guistics.
Towards opening the black box of neural machine
translation: Sourceandtargetinterpretationsofthe
Tom Kocmi, Rachel Bawden, Ondˇrej Bojar, Anton
transformer. InProceedingsofthe2022Conference
Dvorkovich, Christian Federmann, Mark Fishel,
onEmpiricalMethodsinNaturalLanguageProcess-
Thamme Gowda, Yvette Graham, Roman Grund-
ing,pages8756–8769,AbuDhabi,UnitedArabEmi-
kiewicz,BarryHaddow,RebeccaKnowles,Philipp
rates.AssociationforComputationalLinguistics.
Koehn,ChristofMonz,MakotoMorishita,Masaaki
Nagata,ToshiakiNakazawa,MichalNovák,Martin
JavierFerrando,GerardI.Gállego,andMartaR.Costa-
Popel,andMajaPopovic´.2022. Findingsofthe2022
jussà. 2022b. Measuring the mixing of contextual
conference on machine translation (WMT22). In
information in the transformer. In Proceedings of
ProceedingsoftheSeventhConferenceonMachine
the2022ConferenceonEmpiricalMethodsinNat-
Translation(WMT),pages1–45,AbuDhabi,United
uralLanguageProcessing,pages8698–8714,Abu
ArabEmirates(Hybrid).AssociationforComputa-
Dhabi,UnitedArabEmirates.AssociationforCom-
tionalLinguistics.
putationalLinguistics.
Javier Ferrando, Gerard I. Gállego, Ioannis Tsiamas, JiachangLiu,DinghanShen,YizheZhang,BillDolan,
and Marta R. Costa-jussà. 2023. Explaining how Lawrence Carin, and Weizhu Chen. 2022. What
transformers use context to build predictions. In makes good in-context examples for GPT-3? In
Proceedings of the 61st Annual Meeting of the As- ProceedingsofDeepLearningInsideOut(DeeLIO
sociationforComputationalLinguistics(Volume1: 2022): The 3rd Workshop on Knowledge Extrac-
Long Papers), pages 5486–5513, Toronto, Canada. tionandIntegrationforDeepLearningArchitectures,
AssociationforComputationalLinguistics. pages100–114,Dublin,IrelandandOnline.Associa-
tionforComputationalLinguistics.
JavierFerrandoandElenaVoita.2024. Informationflow
routes: Automaticallyinterpretinglanguagemodels NelsonF.Liu,KevinLin,JohnHewitt,AshwinParan-
atscale. Arxiv. jape,MicheleBevilacqua,FabioPetroni,andPercy
Liang. 2023. Lost in the Middle: How Lan-
Xavier Garcia, Yamini Bansal, Colin Cherry, George guage Models Use Long Contexts. arXiv e-prints,
Foster, Maxim Krikun, Fangxiaoyu Feng, Melvin arXiv:2307.03172.YaoLu,MaxBartolo,AlastairMoore,SebastianRiedel, Chenglei Si, Dan Friedman, Nitish Joshi, Shi Feng,
and Pontus Stenetorp. 2022. Fantastically ordered Danqi Chen, and He He. 2023. Measuring induc-
promptsandwheretofindthem: Overcomingfew- tive biases of in-context learning with underspeci-
shotpromptordersensitivity. InProceedingsofthe fieddemonstrations. InProceedingsofthe61stAn-
60thAnnualMeetingoftheAssociationforCompu- nualMeetingoftheAssociationforComputational
tationalLinguistics(Volume1: LongPapers),pages Linguistics(Volume1: LongPapers),pages11289–
8086–8098,Dublin,Ireland.AssociationforCompu- 11310,Toronto,Canada.AssociationforComputa-
tationalLinguistics. tionalLinguistics.
NLLBTeam,MartaR.Costa-jussà,JamesCross,Onur
SewonMin,XinxiLyu,AriHoltzman,MikelArtetxe,
Çelebi,MahaElbayad,KennethHeafield,KevinHef-
MikeLewis,HannanehHajishirzi,andLukeZettle-
fernan, Elahe Kalbassi, Janice Lam, Daniel Licht,
moyer.2022. Rethinkingtheroleofdemonstrations:
JeanMaillard,AnnaSun,SkylerWang,Guillaume
Whatmakesin-contextlearningwork? InProceed-
Wenzek, Al Youngblood, Bapi Akula, Loic Bar-
ingsofthe2022ConferenceonEmpiricalMethodsin
rault,GabrielMejiaGonzalez,PrangthipHansanti,
NaturalLanguageProcessing,pages11048–11064,
John Hoffman, Semarley Jarrett, Kaushik Ram
AbuDhabi,UnitedArabEmirates.Associationfor
Sadagopan, Dirk Rowe, Shannon Spruit, Chau
ComputationalLinguistics.
Tran, Pierre Andrews, Necip Fazil Ayan, Shruti
Bhosale, Sergey Edunov, Angela Fan, Cynthia
WafaaMohammedandVladNiculae.2024. Onmeasur-
Gao,VedanujGoswami,FranciscoGuzmán,Philipp
ingcontextutilizationindocument-levelMTsystems.
Koehn, Alexandre Mourachko, Christophe Rop-
InFindingsoftheAssociationforComputationalLin-
ers, Safiyyah Saleem, Holger Schwenk, and Jeff
guistics: EACL2024,pages1633–1643,St.Julian’s,
Wang. 2022. No language left behind: Scal-
Malta.AssociationforComputationalLinguistics.
inghuman-centeredmachinetranslation. Preprint,
arXiv:2207.04672.
KishorePapineni,SalimRoukos,ToddWard,andWei-
JingZhu.2002. Bleu: amethodforautomaticevalu- Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
ationofmachinetranslation. InProceedingsofthe bert, Amjad Almahairi, Yasmine Babaei, Nikolay
40thAnnualMeetingoftheAssociationforCompu- Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
tational Linguistics, pages 311–318, Philadelphia, Bhosale,DanBikel,LukasBlecher,CristianCanton
Pennsylvania,USA.AssociationforComputational Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
Linguistics. JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
CynthiaGao,VedanujGoswami,NamanGoyal,An-
VikasRaunak,ArulMenezes,andHanyAwadalla.2023. thonyHartshorn,SagharHosseini,RuiHou,Hakan
Dissectingin-contextlearningoftranslationsinGPT- Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
3. InFindingsoftheAssociationforComputational IsabelKloumann,ArtemKorenev,PunitSinghKoura,
Linguistics: EMNLP 2023, pages 866–872, Singa- Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
pore.AssociationforComputationalLinguistics. anaLiskovich,YinghaiLu,YuningMao,XavierMar-
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
Ricardo Rei, José G. C. de Souza, Duarte Alves, bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
ChrysoulaZerva,AnaCFarinha,TaisiyaGlushkova, stein, Rashi Rungta, Kalyan Saladi, Alan Schel-
AlonLavie,LuisaCoheur,andAndréF.T.Martins. ten, Ruan Silva, Eric Michael Smith, Ranjan Sub-
2022a. COMET-22: Unbabel-IST2022submission ramanian, Xiaoqing Ellen Tan, Binh Tang, Ross
for the metrics shared task. In Proceedings of the Taylor, Adina Williams, Jian Xiang Kuan, Puxin
SeventhConferenceonMachineTranslation(WMT), Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, An-
pages578–585,AbuDhabi,UnitedArabEmirates gelaFan,MelanieKambadur,SharanNarang,Aure-
(Hybrid).AssociationforComputationalLinguistics. lienRodriguez,RobertStojnic,SergeyEdunov,and
Thomas Scialom. 2023. Llama 2: Open Founda-
tion and Fine-Tuned Chat Models. arXiv e-prints,
Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro,
arXiv:2307.09288.
ChrysoulaZerva,AnaCFarinha,ChristineMaroti,
José G. C. de Souza, Taisiya Glushkova, Duarte
IgorTufanov,KarenHambardzumyan,JavierFerrando,
Alves, Luisa Coheur, Alon Lavie, and André F. T.
andElenaVoita.2024. Lmtransparencytool:Interac-
Martins.2022b. CometKiwi: IST-unbabel2022sub-
tivetoolforanalyzingtransformerlanguagemodels.
mission for the quality estimation shared task. In
Arxiv.
ProceedingsoftheSeventhConferenceonMachine
Translation (WMT), pages 634–645, Abu Dhabi,
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
UnitedArabEmirates(Hybrid).AssociationforCom-
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
putationalLinguistics.
Kaiser,andIlliaPolosukhin.2017. Attentionisall
youneed. Advancesinneuralinformationprocessing
GabrieleSarti,GrzegorzChrupała,MalvinaNissim,and systems,30.
AriannaBisazza.2024. Quantifyingtheplausibility
ofcontextrelianceinneuralmachinetranslation. In ElenaVoita,RicoSennrich,andIvanTitov.2021. Ana-
The Twelfth International Conference on Learning lyzingthesourceandtargetcontributionstopredic-
Representations. tionsinneuralmachinetranslation. InProceedingsof the 59th Annual Meeting of the Association for Following prior work (Zhang et al., 2023), we
ComputationalLinguisticsandthe11thInternational usethein-contexttemplateillustratedinTable3.
JointConferenceonNaturalLanguageProcessing
(Volume1: LongPapers),pages1126–1140,Online.
SRC_LANG: E1|SRC
AssociationforComputationalLinguistics.
TGT_LANG: E1|TGT
LeanWang,LeiLi,DamaiDai,DeliChen,HaoZhou,
SRC_LANG: E2|SRC
FandongMeng,JieZhou,andXuSun.2023. Label
TGT_LANG: E2|TGT
wordsareanchors: Aninformationflowperspective
for understanding in-context learning. In Proceed- [...]
ingsofthe2023ConferenceonEmpiricalMethods SRC_LANG: SRC
inNaturalLanguageProcessing,pages9840–9855,
TGT_LANG:
Singapore.AssociationforComputationalLinguis-
tics.
Table3: Prompttemplateforfew-shotinference.
Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert
Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,
Da Huang, Denny Zhou, and Tengyu Ma. 2023. A.2 Filteringdetails
Largerlanguagemodelsdoin-contextlearningdif-
ferently. arXive-prints,arXiv:2303.03846. Duetoourresourceconstraints,coupledwiththe
highGPUmemoryrequirementsoftheattribution
KangMinYoo,JunyeobKim,HyuhngJoonKim,Hyun-
method when applied to a 7B parameter model,
sooCho,HwiyeolJo,Sang-WooLee,Sang-gooLee,
andTaeukKim.2022. Ground-truthlabelsmatter: A wehadtofiltersampleswithlargecontextlength.
deeperlookintoinput-labeldemonstrations. InPro- Morespecifically,weexcludesamplesexceeding
ceedingsofthe2022ConferenceonEmpiricalMeth-
400 tokens, when considering the concatenation
ods in Natural Language Processing, pages 2422–
of the input prompt with the generated sequence.
2437,AbuDhabi,UnitedArabEmirates.Association
forComputationalLinguistics. Weadditionallyfilteroutthesamplesforwhichthe
generatedsequencedoesnotexceedthelengthof
BiaoZhang,BarryHaddow,andAlexandraBirch.2023.
10tokens.17 Wereportthesizesofthesets—over
Promptinglargelanguagemodelformachinetransla-
tion: acasestudy. InProceedingsofthe40thInter- 1000samplesforeachlanguagepair—examinedin
nationalConferenceonMachineLearning,ICML’23. ouranalysisinTable4.
JMLR.org.
Susan Zhang, Stephen Roller, Naman Goyal, Mikel LanguagePair SampleSize
Artetxe,MoyaChen,ShuohuiChen,ChristopherDe-
De-En 1021
wan,MonaDiab,XianLi,XiVictoriaLin,TodorMi-
haylov,MyleOtt,SamShleifer,KurtShuster,Daniel Ru-En 1017
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu En-De 1174
Wang, and Luke Zettlemoyer. 2022. Opt: Open En-Ru 1107
pre-trainedtransformerlanguagemodels. Preprint,
arXiv:2205.01068.
Table4:Samplesizesforeachlanguagepairconsidered
inouranalysis.
WenhaoZhu,HongyiLiu,QingxiuDong,JingjingXu,
Lingpeng Kong, Jiajun Chen, Lei Li, and Shujian
Huang.2023. Multilingualmachinetranslationwith
largelanguagemodels: Empiricalresultsandanaly- A.3 EvaluationDetails
sis. ArXiv,abs/2304.04675.
We evaluate the models used in our work on
Çag˘atay Yıldız, Nishaanth Kanna Ravichandran, Pr- all language directions examined to ensure high
ishruit Punia, Matthias Bethge, and Beyza Ermis.
translation quality. We report BLEU (Papineni
2024. Investigatingcontinualpretraininginlargelan-
guagemodels: Insightsandimplications. Preprint, et al., 2002), COMET-22 (Rei et al., 2022a), and
arXiv:2402.17400. COMETKiwi(Reietal.,2022b)inTable5.
A FurtherDetailsonExperimentalSetup A.4 Inference
A.1 Few-shotsetting&Promptselection Weusedgreedydecodingatinferencetime,setting
300tokensasthemaximumlengthforthegener-
We conduct our experiments using the few-shot
atedsequence.
examples provided by Hendy et al. 2023, which
wereselectedtobeofhigh-qualityandrelevantto
17InouranalysisinSection6,weseparatethegenerated
thesource. sequencesinto10bins.A.5 Hardwarespecifications B.3 Exampleswithanomalouspart-level
contributions
All our experiments were conducted using 3
NVIDIARTXA6000GPUs. InFigures10and11,weincludesomeadditional
caseswherethemodelshallucinatebycopyingone
A.6 Discussiononartifacts of the provided few-shot examples. We observe
thatinallcasesthemodelsexhibitanomalouscon-
Thedatausedforanalysisinthispaperwasinitially
tributionsandparticularlythecontributionofthe
releasedfortheWMT22GeneralMTtask(Kocmi
sourceisminimal. Wealsocloselyinspectsimilar
etal.,2022)andcanbefreelyusedforresearchpur-
casesinAppendixD.2,whereweanalyzethecon-
poses. Alltranslationdemonstrations(few-shotex-
textdynamicsacrossthegenerationstagesandwe
amples)usedinourpaperwerereleasedin(Hendy
discussourfindings.
etal.,2023)underaMITlicense.
OurcodewasdevelopedontopoforiginalALTI
C PositionalBiasAnalysis
repositories(Ferrandoetal.,2022a,2023),which
havebeenreleasedunderApache-2.0License.
C.1 Detailsonanalysissetupandexamplesof
positionalbiastypes
B Top-levelAnalysis
In the analysis conducted in Section 5.1, we as-
In the top-level analysis conducted in Section 4, sesstheprevalenceandtheextentofthepositional
weexaminedthecontributionsofindividualparts bias observed. Particularly, we examine whether
ofthecontexttothetranslatedsequenceandhigh- thecontributionsofthefirstK few-shotexamples
lighted several findings. In addition, we provide monotonicallydominatetheremainingN −K ex-
results for the Russian to English and English to amples. WeconsiderdifferentvaluesofK torep-
Russian language pairs (§ B.1). As supplemen- resent the different types of positional bias. For
tarymaterial,weincludeanadditionalexperiment instance,whenK = 1,thefirstfew-shotexample
(§B.2)toenhancethevalidityofourfindings,and attainsthehighestlevelofcontribution. Inthecase
we also present examples exhibiting anomalous whereK = 2,thefirsttwoexamplesexhibitsorted
part-levelcontributions(§B.3)forcompleteness. contributionsinadescendingorderandtheremain-
ingthreehavelowercontributionsthanthefirsttwo,
B.1 Context’spart-levelcontributionsfor
buttheyarenotnecessarilysortedinadescending
additionallanguagepairs
order. Similarly, in the case where K = 3, the
In Figure 7, we show, for all the examined mod- firstthreefew-shotexamplesexhibitsortedcontri-
els, the total contribution of each context part to butions in a descending order and the remaining
thetranslatedsequenceforRussiantoEnglishand two have lower contributions than the first three,
English to Russian language pairs. We observe buttheyarenotnecessarilysortedinadescending
thatresultsarelargelysimilarwiththosepresented order. Finally,whenK = 4,thefew-shotexamples
in the main text for the German to English and exhibitgloballymonotoniccontributions,indicat-
EnglishtoGermanlanguagepairs. ingastrongpositionalbiasacrossallexamples. We
visuallyillustrateexamplesoftheaforementioned
B.2 Additionalexperimentbyreshufflingthe casesinFigure12.
orderoffew-shotexamples
C.2 Additionalplots
Toensureourfindingsholdagainstanypotential,
yethighlyunlikely,content-relatedbiasstemming Isitallaboutposition? InFigures13,14and15
from the position of the few-shot examples, we we show the context’s part-level contributions,
conductasupplementaryexperiment. Putsimply, whenthetaskdescriptionisaddedfortheEnglish
wereshuffletheorderofthefew-shotexamplesfor toGerman,EnglishtoRussianandRussiantoEn-
eachsampleandrepeattheanalysis. Wereportthe glishtranslationdirectionsrespectively. Wenotice
resultsinFigures8and9forGermanandRussian that in all translation directions the task descrip-
languages respectively. The top-level part-level tionreceivessignificantlylowercontributioncom-
contributionsremainlargelyconsistentwiththose paredtotheexamplesandotherpartsofthecontext,
presentedinthemaintext. Thisresultunderscores suggestingthatthepositionalbiasisnotmerelya
thevalidityofthefindingspresentedinSection4. functionofabsoluteposition.De-En En-De
BLEU COMET-22 COMETKiwi BLEU COMET-22 COMETKiwi
LLAMA-2 28.42 82.25 78.82 21.12 78.79 74.95
TOWER-MONO 28.19 82.45 78.90 23.42 80.99 77.88
TOWER 30.19 83.22 79.60 29.39 84.40 81.58
TOWERINSTRUCT 35.24 85.72 81.43 42.66 88.11 83.11
Ru-En En-Ru
BLEU COMET-22 COMETKiwi BLEU COMET-22 COMETKiwi
LLAMA-2 32.99 82.53 78.84 20.03 80.78 76.80
TOWER-MONO 33.47 83.04 79.16 23.19 83.26 79.31
TOWER 37.78 83.84 79.79 28.33 86.10 82.03
TOWERINSTRUCT 44.48 86.53 81.51 40.02 89.72 83.41
Table5: TranslationperformanceofeachexaminedmodelontheWMT22testset.
Llama-2 Tower-Mono Tower TowerInstruct
0.3
0.2
0.1
0.0
ExSrc E1 xTrgt E1 xSrc E2 xTrgt E2 xSrc E3 xTrgt E3 xSrc E4 xTrgt E4 xSrc E5 xTrgt5 Sour Tc ae rgetPrefix ExSrc E1 xTrgt E1 xSrc E2 xTrgt E2 xSrc E3 xTrgt E3 xSrc E4 xTrgt E4 xSrc E5 xTrgt5 Sour Tc ae rgetPrefix
RussiantoEnglish EnglishtoRussian
Figure7: Illustrationofcontext’spart-levelcontributionstothetranslatedsequence,foralltheexaminedmodels.
Can relevance to the test example break the pairsexamined. Weadditionallyshowexamplesof
bias? In Figures 16a and 16b, we present the anomalouscontextcontributionsandothersalient
proportionofen-desamplesthatfollowpositional casesandwediscusstheresults.
bias,fordifferentvaluesofK,intheoriginaland
replace-last-examplesettingsrespectively. We D.1 Additionalplots
additionallyprovidethecorrespondingresultsfor In Figure 19, we present how context contribu-
theRussiantoEnglishandEnglishtoRussiantrans- tionsevolveacrossdifferentgenerationstagesfor
lationdirectionsinFigures17and18respectively. LLAMA-2,TOWER-MONOandTOWERINSTRUCT
Inallsettingsexamined,weobservethatresultsare models,forthede-enanden-detranslationdirec-
largelysimilarwiththosepresentedinSections5.1 tions. Forcompleteness,weprovideinFigures20
and5.2. and 21 the corresponding plots for the ru-en and
en-rulanguagepairsrespectively.
D ContextContributionsacross
GenerationStages D.2 Examplesofanomalouscontext
contributionsandothersalientcases
In Section 6, we explored how context contribu-
tions evolve across different stages of the gener- In Section 6, we highlighted the importance of
ation process for the TOWER model. In the fol- anomaloussource-partcontributionsasindicators
lowingpart,weincludeadditionalplotsexamining ofpathologicaltranslations. Here,weincludemore
howcontextcontributionsevolveacrossthegenera- suchexamplesaswellasinstancesofothersalient
tionprocessfortherestofthemodelsandlanguage cases.
)ITLA(noitubirtnoClatoTLlama-2 Tower-Mono Tower TowerInstruct
0.25
0.20
0.15
0.10
0.05
0.00
ExSrc E1 xTrgt E1 xSrc E2 xTrgt E2 xSrc E3 xTrgt E3 xSrc E4 xTrgt E4 xSrc E5 xTrgt5 Sour Tc ae rgetPrefix ExSrc E1 xTrgt E1 xSrc E2 xTrgt E2 xSrc E3 xTrgt E3 xSrc E4 xTrgt E4 xSrc E5 xTrgt5 Sour Tc ae rgetPrefix
GermantoEnglish EnglishtoGerman
Figure8: Illustrationofcontext’spart-levelcontributionstothetranslatedsequence,whenreshufflingtheorderof
providedfew-shotexamples.
Llama-2 Tower-Mono Tower TowerInstruct
0.25
0.20
0.15
0.10
0.05
0.00
ExSrc E1 xTrgt E1 xSrc E2 xTrgt E2 xSrc E3 xTrgt E3 xSrc E4 xTrgt E4 xSrc E5 xTrgt5 Sour Tc ae rgetPrefix ExSrc E1 xTrgt E1 xSrc E2 xTrgt E2 xSrc E3 xTrgt E3 xSrc E4 xTrgt E4 xSrc E5 xTrgt5 Sour Tc ae rgetPrefix
RussiantoEnglish EnglishtoRussian
Figure9: Illustrationofcontext’spart-levelcontributionstothetranslatedsequence,whenreshufflingtheorderof
providedfew-shotexamples.
In Tables 7, 8 and 9, we present 3 examples agecasetrendswepresentedforGermantoEnglish
where one of the examined models hallucinates, translation(seeFigures6and19for TOWER and
exhibitinganomalouscontributions. Theexample LLAMA-2 respectively).
showninTable7isparticularlyinteresting,asboth
modelsinthebeginningofthetranslationprocess
exhibit low source contributions — compared to
thesource-partcontributionofthefirstexample— Let’snowturntosomeothersalientcases. Inpar-
indicating that they primarily rely on the first ex- ticular,wenowturntoexampleswherethemodels
ample. However,asthetranslationprogresses,the donotproduceanypathologicaltranslations(see
source contributions of the examined models fol- Tables 10 and 11). Note that the models exhibit
lowcompletelyoppositetrends. TOWER exhibits low source contributions in the early steps of the
extremelyanomalouscontributions—asteeplyin- translationprocess(comparedtothecontributions
creasing contribution from the source-part of the ofthefew-shotexamples)indicatingagreaterin-
firstexampleandadecreasingonefromthesource fluencefromthefew-shotexamplesthatareseman-
—producinginthiswayahallucination,bycopying ticallysimilar. Then,asthetranslationprogresses,
thefirstexample. Incontrast, LLAMA-2 produces theyexhibitincreasedsourcecontributionsbeing
acorrecttranslation,withitscontributionsfollow- verysimilarwiththeaveragecasetrendsforGer-
ingtheaveragecasetrendsforGermantoEnglish man to English translation (see Figures 6 and 19
translation. Importantly,inalltheprovidedexam- for TOWER and LLAMA-2 respectively), indicat-
ples,themodelsthatproduceacorrecttranslation ingtherelianceonthesourcetoproduceacorrect
exhibitcontributiontrendsthatalignwiththeaver- translation.
)ITLA(noitubirtnoClatoT
)ITLA(noitubirtnoClatoTContribution Ratio
to E1|SRC
E1|SRC LeiderkonntenwirkeineShopsfinden,dieFolgendesanbieten:BuchmitISBN’9789635487899’.
E1|TGT UnfortunatelywecouldnotfindanystoresofferingtheBookwithISBN’9789635487899’.
E2|SRC DeezeraufXboxOne–DeezerSupport
E2|TGT DeezeronXboxOne–DeezerSupport
E3|SRC InstallierenSiedieMercedesPROAdapterApp2aufIhremSmartphone.
E3|TGT InstalltheMercedesPROAdapterApp2onyoursmartphone.
E4|SRC SpielenMetalStorm:OnlineaufIhremmobilenGerät.
E4|TGT PlayMetalStorm:Onlineonyourmobiledevice.
E5|SRC support@vivago.com(TechnischerSupport)
E5|TGT support@vivago.com(TechnicalSupport)
SRC LeiderwarteichvergeblichaufdieemailvonihremSupport.
MT Unfortunately,wecouldnotfindanystoresofferingtheBookwithISBN’9789635487899’.
0 0.5 1
Figure10: ExampleofanomaloussourcecontributionsforTOWERwhichhallucinates,copyinginformationfrom
thefirstexample. Weshowcontributionratiosto E1|SRC—1beingthecontributionof E1|SRC.
LanguagePair Model #ofhall. by Benkirane et al., 2024.18 Specifically, each
instance is annotated into one of four categories:
En-De LLAMA-2 3
En-De TOWER-MONO 4 "Nohallucination","Smallhallucination","Partial
En-De TOWER 1 hallucination", and"Fullhallucination". Onlyin-
En-De TOWERINSTRUCT 1
De-En LLAMA-2 2 stancesclassifiedas"Fullhallucination"areconsid-
De-En TOWER-MONO 2 ered"fully-detached"hallucinationsinouranalysis.
De-En TOWER 11
Wereportthenumberoffullhallucinationsforeach
De-En TOWERINSTRUCT 0
En-Ru LLAMA-2 23 ofmodelandlanguagepaircombinationinTable6.
En-Ru TOWER-MONO 4
En-Ru TOWER 10 E AIAssistants
En-Ru TOWERINSTRUCT 1
Ru-En LLAMA-2 1 We have used Github Copilot19 during develop-
Ru-En TOWER-MONO 5
mentofourresearchwork.
Ru-En TOWER 2
Ru-En TOWERINSTRUCT 1
Table6: Numberoffullydetachedhallucinationcases
bylanguagepairandmodel.
D.3 DetailsofQuantitativeAnalysis
InSection6,weexaminedwhetheranomalouscon-
textcontributionscanserveasindicatorsofhalluci-
nations. Specificallywefocusedonhowlowsource
contributions,byconductingaquantitativeanaly-
sis to assess the extent to which low-source con-
tributionscanbeassociatedwith"fully-detached"
hallucinations. Inthissection,weprovidefurther
detailsregardingtheannotationprocess.
Foreachmodelandlanguagepaircombination,
weidentifyinstancesof"fully-detached"hallucina-
tions by annotating the generated translations us- 18We used the "Severity Ranking Prompt 2" as this
was shown to be the optimal prompt for LLAMA-3-70B-
ingthe LLAMA-3-70B-INSTRUCT model(Dubey
INSTRUCT.
etal.,2024),followingtheexactapproachoutlined 19https://github.com/features/copilotContribution Ratio
to MT
E1|SRC WirwünschenIhneneinenangenehmenAufenthaltinMaribor.
E1|TGT WewishyouapleasantstayinMaribor.
E2|SRC WirwünschenIhneneinenangenehmenAufenthaltinOlomouc.
E2|TGT WewishyouapleasantstayinOlomouc.
E3|SRC WirwünschenIhneneinenangenehmenAufenthaltinDebrecen.
E3|TGT WewishyouapleasantstayinDebrecen.
E4|SRC WirwünschenIhneneinenangenehmenAufenthaltinPoznan.
E4|TGT WewishyouapleasantstayinPoznan.
E5|SRC BusbudhilftIhnen,einenBusvonLübecknachWismarzufinden.
E5|TGT BusbudhelpsyoufindabusfromLübecktoWismar.
SRC WirverratenIhnen,woSieimRaumLübeckdochnocheinenWeihnachtsbratenherbekommen.
MT BusbudhelpsyoufindabusfromLübecktoWismar.
0 0.5 1
Figure11: ExampleofanomaloussourcecontributionsforTOWERwhichhallucinates,copyinginformationfrom
thelastexample. Weshowcontributionratiosto MT—1beingthecontributionof MT.
E1|SRC IchinteressieremichfürdasObjekt08867inSalzburg-Parsch ExSrc1 ExSrc3 ExSrc5
ExTrgt1 ExTrgt3 ExTrgt5
E1|TGT Iaminterestedintheobject08867inSalzburg-Parsch
ExSrc2 ExSrc4 Source
ExTrgt2 ExTrgt4 TargetPrefix
E2|SRC IchinteressieremichfürdasObjekt55057inSalzburg-Itzling
Llama-2
E2|TGT Iaminterestedintheobject55057inSalzburg-Itzling 0.4
E3|SRC Ichinteressieremichfür’2bedroomsApartmentinLosAngeles. 0.3
E3|TGT Iaminterestedin’2bedroomsApartmentinLosAngeles. 0.2
E4|SRC Ichinteressieremichfür’ApartmentforrentinSANDIEGO....’. 0.1
E4|TGT Iaminterestedin’ApartmentforrentinSANDIEGO....’. 0.0
b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
E5|SRC IchinteressieremichfürdasObjekt33405inSalzburg-Herrnau GeneratedSeqinBins(b)
E5|TGT Iaminterestedintheobject33405inSalzburg-Herrnau Tower
0.4
SRC ichinteressieremichfürden#PRS_ORG#Stuhl. 0.3
LLAMA-2✓
0.2
MT Iaminterestedinthe#PRS_ORG#Chair.
0.1
TOWER✗
0.0
MT Iaminterestedintheobject08867inSalzburg-Parsch b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
GeneratedSeqinBins(b)
Table7: IllustrationofanexampleexhibitinganomaloussourcecontributionsforTOWER—whichhallucinates,
followedbyLLAMA-2’scontributions,whichperformsnormally.
)ITLA(noitubirtnoClatoT
)ITLA(noitubirtnoClatoT0.33 0.17 0.24 0.16 0.10 0.33 0.24 0.16 0.17 0.10
Example Example Example Example Example Example Example Example Example Example
1 2 3 4 5 1 2 3 4 5
0.24 0.33 0.16 0.17 0.10 0.33 0.17 0.24 0.16 0.10
Example Example Example Example Example Example Example Example Example Example
1 2 3 4 5 1 2 3 4 5
(a)Thetopsamplefollowstheexaminedpositionalbias(K = (b) The top sample follows the examined positional bias
1)asthefirstexampleattainsthehighestcontribution. The (K = 2)asthefirsttwoexamplesmonotonicallydominate
bottomsampledoesnotfollowthebias,asthesecondexample theremainingthreeandthelastthreehavelowercontributions
hasgreatercontributionthanthefirst. thanthefirsttwo.Notethatthelastthreeexamplesdonotnec-
essarilyexhibitsortedcontributionsindecreasingorder. The
bottomsampledoesnotfollowthebias,asthethirdexample
hasgreatercontributionthanthesecond.
0.33 0.24 0.17 0.10 0.16 0.33 0.24 0.17 0.16 0.10
Example Example Example Example Example Example Example Example Example Example
1 2 3 4 5 1 2 3 4 5
0.33 0.24 0.16 0.17 0.10 0.33 0.24 0.17 0.10 0.16
Example Example Example Example Example Example Example Example Example Example
1 2 3 4 5 1 2 3 4 5
(c) The top sample follows the examined positional bias (d) The top sample follows the examined positional bias
(K = 3)asthefirstthreeexamplesmonotonicallydominate (K =4)asthecontributionsofalltheexamplesaresortedin
theremainingtwoandthelasttwohavelowercontributions decreasingorder.Thebottomsampledoesnotfollowthebias,
thanthefirstthree.Notethatthelasttwoexamplesdonotnec- asthefourthexamplebreaksthemonotonicity.
essarilyexhibitsortedcontributionsindecreasingorder. The
bottomsampledoesnotfollowthebias,asthefourthexample
hasgreatercontributionthanthethird.
Figure12: Foreachoftheexaminedpositionalbiastypesweillustrate2examples. Onethatfollowstheexamined
typeofpositionalbiasandonethatdoesnot. Wenotethatthedemonstratedexamplesareprovidedforpurely
illustrativepurposesanddonotdepictanyrealdata.
Llama-2 Tower-Mono Tower TowerInstruct
0.25
0.20
0.15
0.10
0.05
0.00
r 1 1 2 2 3 3 4 4 5 5 e x
Task
Desc
Ex
Src
Ex
Trgt
Ex
Src
Ex
Trgt
Ex
Src
Ex
Trgt
Ex
Src
Ex
Trgt
Ex
Src
Ex
Trgt Sourc
rget
Prefi
a
T
Figure13: Illustrationofcontext’spart-levelcontributions,whenthetaskdescriptionisadded. Translationdirection:
EnglishtoGerman
)ITLA(
noitubirtnoC
latoT
1=K
3=K
2=K
4=KLlama-2 Tower-Mono Tower TowerInstruct
0.30
0.25
0.20
0.15
0.10
0.05
0.00
r 1 1 2 2 3 3 4 4 5 5 e x
Task
Desc
Ex
Src
Ex
Trgt
Ex
Src
Ex
Trgt
Ex
Src
Ex
Trgt
Ex
Src
Ex
Trgt
Ex
Src
Ex
Trgt Sourc
rget
Prefi
a
T
Figure14: Illustrationofcontext’spart-levelcontributions,whenthetaskdescriptionisadded. Translationdirection:
EnglishtoRussian
Llama-2 Tower-Mono Tower TowerInstruct
0.30
0.25
0.20
0.15
0.10
0.05
0.00
r 1 1 2 2 3 3 4 4 5 5 e x
Task
Desc
Ex
Src
Ex
Trgt
Ex
Src
Ex
Trgt
Ex
Src
Ex
Trgt
Ex
Src
Ex
Trgt
Ex
Src
Ex
Trgt Sourc
rget
Prefi
a
T
Figure15: Illustrationofcontext’spart-levelcontributions,whenthetaskdescriptionisadded. Translationdirection:
RussiantoEnglish
)ITLA(
noitubirtnoC
latoT
)ITLA(
noitubirtnoC
latoTLlama-2 TowerInstruct
Tower-Mono Random
Tower
100
100
80
80
60 60
40 40
20 20
0 0
K=1 K=2 K=3 K=4 K=1 K=2 K=3 K=4
(a) (b)
Figure16: Proportionofen-desamplesthatfollowpositionalbias,fordifferentvaluesofK,inthe(a)originaland
(b)replace-last-exsettings.
Llama-2 TowerInstruct
Tower-Mono Random
Tower
100
100
80
80
60 60
40 40
20 20
0 0
K=1 K=2 K=3 K=4 K=1 K=2 K=3 K=4
(a) (b)
Figure17: Proportionofru-ensamplesthatfollowpositionalbias,fordifferentvaluesofK,inthe(a)originaland
(b)replace-last-exsettings.
selpmas
fo
%
selpmas
fo
%
selpmas
fo
%
selpmas
fo
%Llama-2 TowerInstruct
Tower-Mono Random
Tower
100
100
80
80
60 60
40 40
20 20
0 0
K=1 K=2 K=3 K=4 K=1 K=2 K=3 K=4
(a) (b)
Figure18: Proportionofen-rusamplesthatfollowpositionalbias,fordifferentvaluesofK,inthe(a)originaland
(b)replace-last-exsettings.
ExSrc1 ExTrgt2 ExSrc4 ExTrgt5
ExTrgt1 ExSrc3 ExTrgt4 Source
ExSrc2 ExTrgt3 ExSrc5 TargetPrefix
LLaMa2 Tower-Mono TowerInstruct
0.2
0.1
0.0
b1 b2 b3 b4 b5 b6 b7 b8 b9b10 b1 b2 b3 b4 b5 b6 b7 b8 b9b10 b1 b2 b3 b4 b5 b6 b7 b8 b9b10
GeneratedSeqinBins(b) GeneratedSeqinBins(b) GeneratedSeqinBins(b)
(a)GermantoEnglish
LLaMa2 Tower-Mono TowerInstruct
0.2
0.1
0.0
b1 b2 b3 b4 b5 b6 b7 b8 b9b10 b1 b2 b3 b4 b5 b6 b7 b8 b9b10 b1 b2 b3 b4 b5 b6 b7 b8 b9b10
GeneratedSeqinBins(b) GeneratedSeqinBins(b) GeneratedSeqinBins(b)
(b)EnglishtoGerman
Figure19: Illustrationofhowcontextcontributionsevolveacrossdifferentgenerationstages,fortheLLAMA-2,
TOWER-MONOandTOWERINSTRUCTmodels. Eachgeneratedbinaccountsfor10%ofthegeneratedsequence.
)ITLA(noitubirtnoClatoT
)ITLA(noitubirtnoClatoT
selpmas
fo
%
selpmas
fo
%ExSrc1 ExTrgt2 ExSrc4 ExTrgt5
ExTrgt1 ExSrc3 ExTrgt4 Source
ExSrc2 ExTrgt3 ExSrc5 TargetPrefix
LLaMa2 Tower-Mono
0.3
0.2
0.1
0.0
b1 b2 b3 b4 b5 b6 b7 b8 b9 b10 b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
Tower TowerInstruct
0.3
0.2
0.1
0.0
b1 b2 b3 b4 b5 b6 b7 b8 b9 b10 b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
GeneratedSeqinBins(b) GeneratedSeqinBins(b)
Figure20: Illustrationofhowcontextcontributionsevolveacrossdifferentgenerationstages,foralltheexamined
models. Eachgeneratedbinaccountsfor10%ofthegeneratedsequence. Translationdirection: RussiantoEnglish
E1|SRC WielangedauertesvonCefalùnachTaorminazukommen? ExSrc1 ExSrc3 ExSrc5
ExTrgt1 ExTrgt3 ExTrgt5
E1|TGT HowlongdoesittaketogetfromCefalùtoTaormina?
ExSrc2 ExSrc4 Source
ExTrgt2 ExTrgt4 TargetPrefix
E2|SRC WielangedauertesvonOslonachHaugesundzukommen?
Llama-2
E2|TGT HowlongdoesittaketogetfromOslotoHaugesund? 0.4
E3|SRC WielangedauertesvonBaselnachMontpellierzukommen? 0.3
E3|TGT HowlongdoesittaketogetfromBaseltoMontpellier?
0.2
E4|SRC WielangedauertesvonFlensburgnachÅrhuszukommen?
0.1
E4|TGT HowlongdoesittaketogetfromFlensburgtoÅrhus?
0.0
E5|SRC WielangedauertesvonOslonachHammerfestzukommen? b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
E5|TGT HowlongdoesittaketogetfromOslotoHammerfest? GeneratedSeqinBins(b)
Tower
SRC wielangedauertesdiegelbenzubestellenmitundohnearm- 0.4
lehne?
0.3
LLAMA-2✗
0.2
MT HowlongdoesittaketogetfromOslotoHammerfest?
0.1
TOWER✓
MT how long does it take to order the yellow with and without 0.0
b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
armrest?
GeneratedSeqinBins(b)
Table8: IllustrationofanexampleexhibitinganomaloussourcecontributionforLLAMA-2—whichhallucinates,
followedbyTOWER’scontributions,whichperformsnormally.
)ITLA(noitubirtnoClatoT
)ITLA(noitubirtnoClatoT
)ITLA(noitubirtnoClatoT
)ITLA(noitubirtnoClatoTExSrc1 ExTrgt2 ExSrc4 ExTrgt5
ExTrgt1 ExSrc3 ExTrgt4 Source
ExSrc2 ExTrgt3 ExSrc5 TargetPrefix
LLaMa2 Tower-Mono
0.3
0.2
0.1
0.0
b1 b2 b3 b4 b5 b6 b7 b8 b9 b10 b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
Tower TowerInstruct
0.3
0.2
0.1
0.0
b1 b2 b3 b4 b5 b6 b7 b8 b9 b10 b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
GeneratedSeqinBins(b) GeneratedSeqinBins(b)
Figure21: Illustrationofhowcontextcontributionsevolveacrossdifferentgenerationstages,foralltheexamined
models. Eachgeneratedbinaccountsfor10%ofthegeneratedsequence. Translationdirection: EnglishtoRussian
E1|SRC WirwünschenIhneneinenangenehmenAufenthaltinMaribor. ExSrc1 ExSrc3 ExSrc5
ExTrgt1 ExTrgt3 ExTrgt5
E1|TGT WewishyouapleasantstayinMaribor.
ExSrc2 ExSrc4 Source
ExTrgt2 ExTrgt4 TargetPrefix
E2|SRC WirwünschenIhneneinenangenehmenAufenthaltinOlomouc.
E2|TGT WewishyouapleasantstayinOlomouc. 0.4 Llama-2
E3|SRC WirwünschenIhneneinenangenehmenAufenthaltinDebrecen. 0.3
E3|TGT WewishyouapleasantstayinDebrecen.
0.2
E4|SRC WirwünschenIhneneinenangenehmenAufenthaltinPoznan.
E4|TGT WewishyouapleasantstayinPoznan. 0.1
E5|SRC Busbud hilft Ihnen, einen Bus von Lübeck nach Wismar zu 0.0
b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
finden.
GeneratedSeqinBins(b)
E5|TGT BusbudhelpsyoufindabusfromLübecktoWismar.
Tower
0.4 SRC WirverratenIhnen,woSieimRaumLübeckdochnocheinen
Weihnachtsbratenherbekommen. 0.3
LLAMA-2✓
0.2
MT We tell you where you can still get a Christmas roast in the
Lübeckarea. 0.1
TOWER✗ 0.0
MT BusbudhelpsyoufindabusfromLübecktoWismar. b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
GeneratedSeqinBins(b)
Table9: IllustrationofanexampleexhibitinganomaloussourcecontributionforTOWER—whichhallucinates,
followedbyLLAMA-2’scontributions,whichperformsnormally.
)ITLA(noitubirtnoClatoT
)ITLA(noitubirtnoClatoT
)ITLA(noitubirtnoClatoT
)ITLA(noitubirtnoClatoTE1|SRC TelefónicaDeutschlandhatdenSABREAwardEMEAgewon- ExSrc1 ExSrc3 ExSrc5
ExTrgt1 ExTrgt3 ExTrgt5
nen.
ExSrc2 ExSrc4 Source
E1|TGT TelefónicaDeutschlandhaswontheSABREAwardEMEA. ExTrgt2 ExTrgt4 TargetPrefix
E2|SRC NewYorkCity(BundesstaatNewYork,USA):Promenadeim Llama-2
CentralPark. 0.40
E2|TGT NewYorkcity(NewYorkState,USA):PromenadeinCentral 0.35
Park. 0.30
0.25
E3|SRC NewYorkCityFCoderNewEnglandRevolution
0.20
E3|TGT NewYorkCityFCorNewEnglandRevolution
0.15
E4|SRC 25.08 02:30 LA Galaxy - Los Angeles FC (Fußball,Major
0.10
LeagueSoccer)
0.05
E4|TGT 25.0802:30LAGalaxy-LosAngelesFC(Calcio,MajorLeague
0.00
Soccer)
b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
E5|SRC FCSchalke04hat2vondenletzten3SpielegegenVfLWolfs- GeneratedSeqinBins(b)
burggewonnen Tower
0.40
E5|TGT FCSchalke04haswon2outoftheirlast3matchesagainstVfL
0.35
Wolfsburg
0.30
SRC NewYorkCityFChatzumerstenMaldenTitelinderMajor
0.25
LeagueSoccergewonnen.
0.20
LLAMA-2✓
0.15
MT NewYorkCityFChaswonthetitleintheMajorLeagueSoccer
0.10
forthefirsttime.
0.05
TOWER✓
0.00
MT NewYorkCityFChaswonthetitleintheMajorLeagueSoccer b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
forthefirsttime. GeneratedSeqinBins(b)
Table10: IllustrationofanexamplewherebothLLAMA-2andTOWERproducecorrecttranslations. Weobserve
thattheircontributionsfollowtheaveragecasetrendsforGermantoEnglishtranslation.
E1|SRC ArminiaBielefeld-UnionBerlin2.Bundesliga. ExSrc1 ExSrc3 ExSrc5
ExTrgt1 ExTrgt3 ExTrgt5
E1|TGT ArminiaBielefeld-UnionBerlin2ndBundesliga.
ExSrc2 ExSrc4 Source
ExTrgt2 ExTrgt4 TargetPrefix
E2|SRC HerthaBSC:Gewinnerder2.Bundesliga2010/2011
E2|TGT HerthaBSC:2.Bundesligawinners2010/2011 0.4 Llama-2
E3|SRC Samstag,9.März2019SVDarmstadt98HolsteinKiel 0.3
E3|TGT Saturday,9March2019SVDarmstadt98HolsteinKiel
0.2
E4|SRC DarmstadtReisenvonSaarbrückennachDarmstadtin4stunden
und59minuten 0.1
E4|TGT DarmstadtTravelfromSaarbrückentoDarmstadtin4hoursand
0.0
59minutes b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
GeneratedSeqinBins(b)
E5|SRC DasWasserdarfnichtheißerals60°Csein.
Tower
E5|TGT Thewatermustnotbehotterthan60°C. 0.4
SRC Darmstadt98darfvonderRückkehrindieFußball-Bundesliga 0.3
träumen.
0.2
LLAMA-2✓
MT Darmstadt98candreamofreturningtotheBundesliga. 0.1
TOWER✓ 0.0
MT Darmstadt98candreamofareturntotheBundesliga. b1 b2 b3 b4 b5 b6 b7 b8 b9 b10
GeneratedSeqinBins(b)
Table11: IllustrationofanexamplewherebothLLAMA-2andTOWERproducecorrecttranslations. Weobserve
thattheircontributionsfollowtheaveragecasetrendsforGermantoEnglishtranslation.
)ITLA(noitubirtnoClatoT
)ITLA(noitubirtnoClatoT
)ITLA(noitubirtnoClatoT
)ITLA(noitubirtnoClatoT