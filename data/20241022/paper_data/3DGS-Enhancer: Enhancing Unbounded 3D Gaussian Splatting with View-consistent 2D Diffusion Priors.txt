3DGS-Enhancer: Enhancing Unbounded 3D Gaussian
Splatting with View-consistent 2D Diffusion Priors
XiLiu* ChaoyiZhou* SiyuHuang
VisualComputingDivision
SchoolofComputing
ClemsonUniversity
{xi9, chaoyiz, siyuh}@clemson.edu
3DGS(PSNR:16.29) Ours(PSNR:26.04) Ground-truth
3DGS(PSNR:12.83) Ours(PSNR:22.26) Ground-truth
Figure1: The3DGS-Enhancerimproves3DGaussiansplattingrepresentationsonunboundedscenes
withsparseinputviews.
Abstract
Novel-viewsynthesisaimstogeneratenovelviewsofascenefrommultipleinput
imagesorvideos, andrecentadvancementslike3DGaussiansplatting(3DGS)
haveachievednotablesuccessinproducingphotorealisticrenderingswithefficient
pipelines.However,generatinghigh-qualitynovelviewsunderchallengingsettings,
such as sparse input views, remains difficult due to insufficient information in
under-sampledareas,oftenresultinginnoticeableartifacts. Thispaperpresents
3DGS-Enhancer, a novel pipeline for enhancing the representation quality of
3DGS representations. We leverage 2D video diffusion priors to address the
challenging3Dviewconsistencyproblem,reformulatingitasachievingtemporal
consistency within a video generation process. 3DGS-Enhancer restores view-
consistent latent features of rendered novel views and integrates them with the
input views through a spatial-temporal decoder. The enhanced views are then
used to fine-tune the initial 3DGS model, significantly improving its rendering
performance. Extensiveexperimentsonlarge-scaledatasetsofunboundedscenes
demonstratethat3DGS-Enhanceryieldssuperiorreconstructionperformanceand
high-fidelityrenderingresultscomparedtostate-of-the-artmethods. Theproject
webpageishttps://xiliu8006.github.io/3DGS-Enhancer-project.
Correspondingauthor:SiyuHuang
38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).
4202
tcO
12
]VC.sc[
1v66261.0142:viXra1 Introduction
Novel-viewsynthesis(NVS)hasdecadesofhistoryincomputervisionandgraphicscommunities,
aimingtogenerateviewsofascenefrommultipleinputimagesorvideos. Recently,3DGaussian
splatting(3DGS)[18]hasexcelledinproducingphotorealisticrenderingswithahighlyefficient
renderingpipeline.However,renderinghigh-qualitynovelviewsfarfromexistingviewpointsremains
verychallenging, asoftenencounteredinsparse-viewsettings, duetoinsufficientinformationin
under-sampledareas. AsshowninFigure1,noticeableellipsoid-likeandhollowartifactsmanifest
whenthereareonlythreeinputviews. Duetothesecommonlow-qualityrenderingresultsinpractice,
itisessentialtoenhance3DGStoensureitsviabilityforreal-worldapplications.
Toourknowledge,fewpriorstudieshavespecificallyfocusedonenhancementmethodsaimedat
improvingtherenderingqualityofNVS.MostexistingenhancementworkforNVS[19,43]focuses
onincorporatingadditionalgeometricconstraintssuchasdepthandnormalintothe3Dreconstruction
processtofulfillthegapbetweentheobservedandunobservedregions. Forexample,DNGaussian
[19]appliesahard-and-softdepthregularizationtothegeometryofradiancefields. However,these
methodsheavilyrelyontheeffectivenessofadditionalconstraintandareoftensensitivetonoises.
Another line of work leverages generative priors to regularize the NVS pipeline. For instance,
ReconFusion[40]enhancesNeuralRadianceFields(NeRFs)[25]bysynthesisingthegeometryand
texturefortheunobservedregions. Althoughitcangeneratephoto-realisticnovelviews,theview
consistencyisstillchallengingwhenthegeneratedviewsarefarawayfromtheinputones.
In this work, we exploit the 2D generative priors, e.g., the latent diffusion models (LDMs) [31],
for 3DGS representation enhancement. LDM has demonstrated powerful and robust generation
capabilities in various image generation [31] and restoration tasks [42]. Nevertheless, the main
challengelies inthe poor3D view consistency amonggenerated 2Dimages, which significantly
hindersthe3DGStrainingprocessthatrequireshighlypreciseviewconsistency. Althoughsome
efforts have been made, such as the Score Distillation Sampling (SDS) loss [29] that distills the
optimizationobjectiveofapre-traineddiffusionmodel, itfailstogeneratethe3Drepresentation
allowingrenderinghigh-fidelityimages
Motivatedbytheanalogyofthevisualconsistencybetweenmulti-viewimagesandthetemporal
consistencybetweenvideoframes,weproposetoreformulatethechallenging3Dconsistencyproblem
asaneasiertaskofachievingtemporalconsistencywithinvideogeneration,sowecanleveragethe
powerfulvideodiffusionmodelsforrestoringhigh-qualityandview-consistentimages. Weproposea
novel3DGSenhancementpipeline,dubbed3DGS-Enhancer. Thecoreof3DGS-Enhancerisavideo
LDMconsistingofanimageencoderthatencodeslatentfeaturesofrenderedviews,avideo-based
diffusionmodelthatrestorestemporallyconsistentlatentfeatures,andaspatial-temporaldecoder
thateffectivelyintegratesthehigh-qualityinformationinoriginalrenderedimageswiththerestored
latentfeatures. Theinitial3DGSmodelwillbefinetunedbytheseenhancedviewstoimproveits
rendering performance. The proposed 3DGS-Enhancer can be trajectory-free to reconstruct the
unboundscenesfromsparseviewsandgeneratethenatural3Drepresentationfortheinvisiblearea
betweentwoknownviews. AcocurrentworkV3D[7]alsoleverageslatentvideodiffusionmodels
[4]forgeneratingobject-level3DGSmodelsfromsingleimages. Incontrast,our3DGS-Enhancer
focusesonenhancinganyexisting3DGSmodelsandthuscanbeappliedtomoregeneralizedscenes,
e.g.,theunboundedoutdoorscenes.
Inexperiments,wegeneratelarge-scaledatasetswithpairsoflow-qualityandhigh-qualityimages
on hundreds of unbounded scenes, based on DL3DV [20], for comprehensively evaluating the
novellyinvestigated3DGSenhancementproblem. Empiricalresultsdemonstratethattheproposed
3DGS-Enhancermethodachievessuperiorreconstructionperformanceonvariouschallengingscenes,
yieldingmoredistinctandvividrenderingresults. Thecodeandthegenerateddatasetwillbepublicly
available. Thecontributionsofthispaperaresummarizedasfollows.
1. Tothebestofourknowledge,thisisthefirstworktotackletheproblemofenhancinglow-
quality3DGSrenderingresults,anissuethatwidelyexistsinpractical3DGSapplications.
2. Weproposeanovelpipeline3DGS-Enhancerthataddressesthe3DGSenhancementprob-
lem. 3DGS-Enhancerreformulatesthe3D-consistentimagerestorationtaskastemporally
consistentvideogeneration,suchthatpowerfulvideoLDMscanbeleveragedforgener-
atingbothhigh-qualityand3D-consistentimages. Novel3DGSfine-tuningstrategiesare
2also devised for an effective integration of the enhanced views with the original 3DGS
representation.
3. Weconductextensiveexperimentsonlarge-scaledatasetsofunboundedscenestodemon-
stratetheeffectivenessoftheproposedmethodsoverexistingstate-of-the-artfew-shotNVS
methods.
2 RelatedWork
Radiancefieldsfornovelviewsynthesis. Novelviewsynthesis(NVS)aimstogenerateunseen
viewpointsfromasetofinputimagesandcamerainformation. Radiancefieldsmethods,likeNeRFs
[25], encode 3D scenes as radiance fields and use volume rendering for novel views, achieving
high-fidelity results but at the cost of lengthy training and inference times. Improvements such
as Mip-NeRF [1, 2] enhance rendering quality through anti-aliasing, while others [6, 9, 46, 26]
focus on speeding up the processes. Recently, 3D Gaussian splatting (3DGS) [18] has emerged,
offeringcompetitiverenderingqualityandsignificantlyhigherefficiencybyrepresentingscenesas3D
Gaussianspheresandusingafastdifferentiablesplattingpipeline[49]. However,3DGSstillrequires
high-qualityandnumerousinputviewsforoptimalreconstruction,whichisoftenimpractical.
Few-shotnovelviewsynthesis. Leveragingadditionalinformationisessentialforgeneratingnovel
viewsfromsparseinputimages. Variousapproachesincorporatedifferentregularizationtechniquesto
prevent3Dgeometryfromoverfittingtothetrainingviews. [19,10,27,22]introduceextrageometric
information,suchasdepthmapsorcoarsemesh,toenhancetherobustnessandperformanceof3D
reconstructionfromsparseviews. [5,8]leveragethelearnedpriorsfrommulti-viewstereodatasetsas
generalpriorstoimproveperformanceinsparseviewreconstructiontasks. FreeNeRF[43]integrates
frequencyandocclusionregularizationduringtrainingtomitigateoverfittingissuesinfew-shotneural
rendering. Similarly,DietPixelNeRF[16]employsasemanticviewconsistencylosstoensurethat
allviewsshareconsistentsemantics,therebyalleviatingoverfitting. However,thesemethodsare
highlysensitivetothenetworkâ€™sperformance,whereincorrectdepthestimationsorinaccuratemesh
reconstructionscansignificantlydegradethefinaloutput.
Diffusionpriorsfornovelviewsynthesis. Recently,utilizingdiffusionmodelsaspriorsforfew-shot
novel view synthesis has proven to be an effective approach. DreamFusion [29] employs Score
DistillationSampling(SDS)withapre-traineddiffusionmodeltoguide3Dobjectgenerationfrom
textprompts[35,32,45]. Someworks[21,33,34]embed3Dawarenessinto2Ddiffusionmodelsto
generatemulti-viewimages,thoughthesemethodstypicallyrequirelargedatasets[48]andsignificant
trainingresources[16,27]. ReconFusion[40]leavargethe2Ddiffusionpriorstorecoverahigh-
fidelityNeRFfromsparseinputviews. Moreadvancedapproachesleveragevideodiffusionmodels
[4,12,13,23]forfew-shotNVS.Forinstance,AnimateDiff[11]fine-tunesdiffusionmodelswith
additionalcameramotionsusingLoRA[14],whilemethodslikeSVD-MV[4],V3D[36]andIM-3D
[23]proposecamera-controlledvideodiffusionmodelsforobject-level3Dgeneration. Incontrast,
ourapproachoffersgreatergeneralizabilityforunboundedoutdoorscenes.
Radiancefieldsenhancement. SeveralexistingstudiesfocusonenhancingNeRFsbyaddressing
thelimiteddetailpreservationissuecausedbyinsufficientorlow-qualityinputdata. NeRF-SR[37]
andRefsr-nerf[15]useasuper-resolutionnetworktoupscalethetrainingviewimages,allowing
novelviewstobesynthesizedathigherresolutionswithappropriatedetails. Alignerf[17]introduce
optical-flownetworktosolvethemisalignmentproblemtoenhancetheperformance. Someother
approachesincorporate2Ddiffusionpriorsinto3Dreconstructions. Forinstance,DiffusionNeRF
[41]leveragesadiffusionmodeltolearngradientsoflogarithmsofRGBDpatchpriors,servingas
regularizedgeometryandcolorforascene. Nerfbusters[39]usediffusionpriorstoremoveghostly
artifactsinthe3Dgaussians. Ourworkaimtoaddressestheradiancefieldsenhancementproblem
byproposinganovelframework3DGS-Enhancer,achievingsuperiorenhancementperformancefor
low-qualityunbounded3DGSrepresentations.
3 Preliminaryof3DGaussianSplatting
Here,webrieflyreviewtheformulationandrenderingprocessof3DGS[18]. 3DGSrepresentsa
sceneasasetofanisotropic3DGaussianspheres,allowinghigh-fidelityNVSwithextremelylow
renderinglatency. A3DGaussiansphereincludesacenterpositionÂµâˆˆR3,ascalingfactorsâˆˆR3,
3Video diffusion prior for 3D representation
novel view enhancement fine-tuning
Temporal
Denosing U-Net
c
n
E
E
A
V
CLIP
Fine-tuned Frozen
Input views and rendered novel views ResBlock-3D Spatio-temporal attention Enhanced novel views
Figure2:Anoverviewoftheproposed3DGS-Enhancerframeworkfor3DGSrepresentationenhance-
ment. Welearn2Dvideodiffusionpriorsonalarge-scalenovelviewsynthesisdatasettoenhancethe
novelviewsrenderedfromthe3DGSmodelonanovelscene. Then,theenhancedviewsandinput
viewsjointlyfine-tunethe3DGSmodel.
andarotationquaternionqâˆˆR4,suchthattheGaussiandistributionis
G(x)=eâˆ’ 21(xâˆ’Âµ)TÎ£âˆ’1(xâˆ’Âµ), (1)
whereÎ£=RSSTRT,SisthescalingmatrixdeterminedbysandRistherotationmatrixdetermined
by q. To additionally model the view-dependent appearance, the Gaussian sphere also includes
sphericalharmonics(SH)coefficientsC âˆˆRk,wherekisthenumberofSHfunctions,andanÎ±âˆˆR
foropacity. ThecolorandopacityarealsocalculatedbytheGaussiandistributionillustratedinEq. 1.
Forrendering,allthe3DGaussianspheresareprojectedontothe2Dcameraplanesviaadifferentiable
Gaussiansplattingpipeline[49]. GiventheviewingtransformmatrixW andJacobianmatrixJ ofthe
affineapproximationoftheprojectivetransformation,thecovariancematrixÎ£â€²incameracoordinates
iscalculatedas
Î£â€² =JWÎ£WTJT. (2)
The differentiable splatting method efficiently projects the 3D Gaussian spheres to 2D Gaussian
distributions,ensuringfastÎ±-blendingforrenderingandcolorsupervision. Foreachpixel,thecolor
isrenderedbyM Gaussianspheresthatoverlapwiththepixelonthe2Dcameraplanes,sortedinthe
depthdistanceas
iâˆ’1
(cid:88) (cid:89)
C = C Î± (1âˆ’Î± ). (3)
i i i
iâˆˆM j=1
4 Method
4.1 3DGS-Enhancer: AnOverview
This work studies the 3DGS enhancement problem. More specifically, given a 3DGS model
trained on a scene consisting of input views {Iref,Iref,...,Iref } and corresponding camera
1 2 Nref
poses {pref,pref,...,pref }, the goal of this work is to enhance a set of low-quality novel views
1 2 Nref
{I ,I ,I ,...,I }renderedbythe3DGSmodel. Theenhancedimagesfurtherfine-tunethe
1 2 3 Nnew
3DGSmodeltoimproveitsreconstructionandrenderingquality.
Thisworknovellyreformulatesthechallengingtaskof3D-consistentimagerestorationasthetaskof
videorestoration,inlightoftheanalogybetweenthemulti-viewconsistencyandthevideotemporal
consistency. Weproposeanovelframeworknamed3DGS-EnhancerthatemploysavideoLDM
comprising an image encoder, a video-based diffusion model, and a spatial-temporal decoder to
enhancetherenderedimageswhilepreservingahigh3Dconsistency. 3DGS-Enhanceralsoadopts
novelfine-tuningstrategiestoselectivelyintegratetheviewsenhancedbythevideoLDMintothe
3DGSfine-tuningprocess. Anillustrationofthe3DGS-EnhancerframeworkisshowninFigure2.
Wediscussmoredetailsoftheframeworkinthefollowing.
44.2 VideoDiffusionPriorforTemporalInterpolation
In this section, we introduce the video diffusion model for achieving 3D-consistent 2D image
restoration. To lift the consistency between the generated 2D video frames and the high-quality
reference views, we further propose to formulate the video restoration task as a video interpo-
lation task, where the first frame and the last frame of inputs to the video diffusion model are
two reference views. This formulation provides stronger guidance for the video restoration pro-
cess. Let {pref ,ps,ps,...,ps,pref} be the camera poses sampled from the trajectory fitted be-
iâˆ’1 1 2 T i
tween two reference views, the images rendered accordingly are v = {Iref ,I ,I ,...,I ,Iref}.
iâˆ’1 1 2 T i
v âˆˆ R(T+2)Ã—3Ã—HÃ—W serves as the input to the video diffusion model, e.g., a pre-trained image-
guidedstablevideodiffusion(SVD)model[4]thatadoptscross-framespatio-temporalattention
moduleand3DresidualconvolutioninthediffusionU-Net. UnlikeSVD,whichrepeatsthesingle
inputimagefeatureextractedbyCLIP[30]forT timesastheconditionalinputs,weinputvtothe
CLIPencodertogetasequenceofconditionalinputsc andaddittothevideodiffusionmodel
clip
throughcrossattention. Meanwhile,weinputv totheVAEencodertogetlatentfeaturec and
vae
additintothediffusionmodelthroughaclassifier-freeguidancestrategytoincorporaterichercolor
information. ThediffusionU-NetÏµ predictsthenoiseÏµforeachdiffusionstept,andthetraining
Î¸
objectiveis
L =E[âˆ¥Ïµâˆ’Ïµ (z ,t,c ,c )âˆ¥]. (4)
diffusion Î¸ t clip vae
wherez =Î± z+Ïƒ Ïµâˆ—..wherezisthegtlatent,ÏµâˆˆN(0,I),Î± andÏƒ defineanoiseattimestept.
t t t t t
Thelearnedvideodiffusionmodelgeneratesasequenceofenhancedimagelatentsz corresponding
v
totherenderedlow-qualityviewsv.
4.3 Spatial-TemporalDecoder
Althoughthevideodiffusionmodelcangenerateenhancedimagelatentsz ,weobservethatthere
v
areartifactssuchastemporalinconsistency,blurring,andcolorshiftinoutputsoftheoriginaldecoder
ofvideoLDM.Toaddressthisissue,weproposeamodifiedspatial-temporaldecoder(STD).STD
makesthefollowingimprovementsovertheoriginalVAEDecoder: 1)Temporaldecodingmanner.
STD adopts additional temporal convolution layers to ensure the temporal consistency between
decodedoutputs. Similartoourvideodiffusionmodel, thefirstandthelastinputframesarethe
referenceviewimages,andtheintermediateinputsarethegeneratedviews;2)Effectiveintegration
ofrenderedviews. STDadoptsadditionalconditionalinputs,sameasthoseofthevideodiffusion
model,allowingthedecodertobetterleveragetheoriginalrenderedimages. Inspiredby[44,47],
theseconditionalinputsarefedintoSTDthroughControllableFeatureWarping(CFW)modules
[38],suchthattheirhigh-frequencypatternsarebetterpreserved. 3)Colorcorrection. Toaddress
thecolorshiftissue,weapplycolornormalizationtothedecodedimagesbyfollowingStableSR
[38]. However,weobservethathighlyblurredandlow-qualityimagesintheconditionalinputscan
underminethecolorcorrectioneffects. Tomitigatethis,weusethefirstreferenceviewtocalculate
themeanandvariance,andthenalignalltheotherdecodedimageswiththisreferenceview. Let
Ig bethei-thdecodedimageswithameanu andavarianceÏƒ ,IË†g bethereferenceviewwitha
i IË†g IË†g 0
meanÂµ Ig andavarianceÏƒ
Ig,thecorrectedim0
ageI
iciscomputed0
by:
i i
Ic = I igâˆ’Âµ I ig Â·Ïƒ +u . (5)
i Ïƒ Ig IË† 0g IË† 0g
i
TheoptimizationobjectiveofSTDconsistsofanL1reconstructionlossandanLPIPSperceptual
lossbetweenIg andground-truthIË†g,andanadversarialloss,as
L =L (Ig,IË†g)+L (Ig,IË†g)+L (Ig). (6)
STD rec LPIPS adv
whereL istheadversariallossthatdiscriminatesbetweenrealimageIË†g andfakeimageIg.
adv
4.4 Fine-tuningStrategiesof3DGaussianSplatting
Confidence-aware3DGaussiansplatting. Unlikeexistingsparse-viewNVSmethods,ourap-
proachdoesnotrelyondepthestimationnetworksfordepthregularization. Instead,wetakeapurely
2Dvisualmethodbyutilizingavideodiffusionmodeltoenhanceimagesrenderedfromalow-quality
3DGSmodel. Despitethissignificantenhancementinthequalityoftherenderedviews,wepropose
5Figure 3: The red circle indicates the area with high confidence, meaning the generated videos
cancontributemoreinformation. Conversely,thegreenquadrilateral highlightstheareawithlow
confidence,suggestingthatthegeneratedvideoshouldnottendtooptimizethisarea.
torelymoreonthereferenceviewsratherthantherestorednovelviewswhenfine-tuningthe3DGS
model,sincethe3DGSmodelishighlysensitivetoslightinaccuraciesintherestoredviews. These
inaccuraciescouldbeamplifiedduringthefine-tuningprocess.
TominimizethenegativeimpactofgeneratedimagesonGaussiantraining,weproposeconfidence-
aware3DGaussiansplatting. Thisstrategyinvolvestwolevelsofconfidence,imagelevelandpixel
level. Fortheimagelevel,thegeneratedimagesthatareclosertorealimageshavelowerconfidence.
For pixel level, the larger the mean covariance of all the Gaussians used to render this pixel, the
higheritsconfidence.
Imagelevelconfidence. Inthetaskofnovelviewsynthesis,ifnoiseexistsintwoimageviews,
aclosedistancebetweenthemincreasesthelikelihoodofgeneratingconflictsanddisruptingthe
3Dconsistencyofthescene. Therefore,fornovelviewsthatareclosetothereferenceview,itis
crucialtocarefullyoptimizethe3DGaussianstomitigatetheadverseeffectsofnoise. Conversely,
when a novel view is far from all known views, it has a smaller likelihood of disturbing already
well-reconstructedareas. Basedonthisreasoning,wenormalizethedistancefromnovelviewsto
referenceviewsbetween0and1. Thefartheraviewpointisfromthereferenceview,thehigherits
confidence.
Pixellevelconfidence. InspiredbyActiveNeRF[28],whichusesGaussiandistributionsinNeRF
to estimate uncertainty and identify views with the highest information gain, we aim to find the
pixelsthatcanprovidethehighestinformationgainfromthegeneratedimages. AsshowninFig3,
weobservedthatwell-reconstructedareasaretypicallyrepresentedbyGaussianswithverysmall
volumes,calculatedusingthescalingvectorsâˆˆR. Basedonthisobservation,weproposeamethod
tocalculatepixel-levelconfidence.
Theuniquerepresentationof3DGaussiansallowsustorenderanHÃ—WÃ—3imageusingaprocess
similartorenderingcolors,whereeachchannelcorrespondstooneofthethreecomponentsofthe
scalingvectors. In3DGS-Enhancer,wemultiplythesethreechannelsofthescalemaptoobtain
pixel-levelconfidence. Foreachpixelinthegeneratedimages,higherconfidenceresultsingreater
weightinsupervisingthetrainingofthe3DGSmodel.
Given a set of 3D Gaussian, the 3-channel C confidence map is rendered as same as colour
conf
rendering,andtheformulaisdefinedasfollows
iâˆ’1
(cid:88) (cid:89)
C = s Î± (1âˆ’Î± ). (7)
conf i i i
iâˆˆM j=1
6Table1: Aquantitativecomparisonoffew-shot3Dreconstruction. ExperimentsonDL3DVand
LLFFfollowthesettingof[43]. ExperimentsonMip-NeRF360followthesettingof[40].
3views 6views 9views
Method PSNRâ†‘ SSIMâ†‘ LPIPSâ†“ PSNRâ†‘ SSIMâ†‘ LPIPSâ†“ PSNRâ†‘ SSIMâ†‘ LPIPSâ†“
DL3DV(130trainingscenes,20testscenes)
Mip-NeRF[1] 10.92 0.191 0.618 11.56 0.199 0.608 12.42 0.218 0.600
RegNeRF[27] 11.46 0.214 0.600 12.69 0.236 0.579 12.33 0.219 0.598
FreeNeRF [43] 10.91 0.211 0.595 12.13 0.230 0.576 12.85 0.241 0.573
3DGS[18] 10.97 0.248 0.567 13.34 0.332 0.498 14.99 0.403 0.446
DNGaussian[19] 11.10 0.273 0.579 12.67 0.329 0.547 13.44 0.365 0.539
3DGS-Enhancer(ours) 14.33 0.424 0.464 16.94 0.565 0.356 18.50 0.630 0.305
(cid:112)
Andthe1channelpixellevelconfidencemapP = 3 C [0]Ã—C [1]Ã—C [2]. Overall,in
c conf conf conf
ourtrainingprocessfor3DGaussians,thelossfunctionsweredefinedas
L =I Â·(P âŠ™âˆ¥Câˆ’CË†âˆ¥ +SSIM(C,CË†)) (8)
3DGS c c 1
whereSSIMistheStructuralSimilarityIndexandâŠ™isHadamardâ€™sproduct,I istheimage-level
c
confidencemapandCË† istherealpixelvalue.
5 Experiments
5.1 3DGS-EnhanceDataset
Giventhattheenhancementof3DGSrepresentationsisanewtask,wecreateadatasettosimulate
various artifactsof the 3DGS representations. This dataset also serves as a more comprehensive
benchmark for evaluating the performance of few-shot NVS methods. Existing few-shot NVS
algorithms [43, 19] primarily focus on face-forward evaluations [24], where the test views have
significantoverlapwiththeinputviews. However,thisevaluationmethodisnotsuitableforlarge-
scaleunboundedoutdoorscenes. Therefore,weproposeadatasetprocessingstrategythatallowsus
topost-processanyexistingmulti-viewdatasettogeneratealargenumberoftrainingimagepairs
thatincludetypicalartifactscausedbyfew-shotNVS.
More specifically, for each scene, we have n views I = {I ,I ,...,I }, which serve as the
train 1 2 n
input for a high-quality 3DGS model. We uniformly sample a small number m of views I
low
fromI ,whichserveastheinputforthelow-quality3DGSmodel. Bylinearlyfittingthehigh-
train
quality camera poses ptrain = {ptrain,ptrain,...,ptrain}, we randomly sample a camera trajectory
i 1 2 nâˆ—
prender ={prender,prender,...,prender}onptrainandrendertheimagepairsusingbothhigh-qualityand
i 1 2 nâˆ— i
low-quality3DGSmodels. Thiscreatesasetofhigh-qualityandlow-qualityimagepairsusedforthe
trainingofourvideodiffusionmodel.
WeapplythisdatasetprocessingstrategytoDL3DV[20],alarge-scaleoutdoordatasetcontaining
10Kscenes. Werandomlyselect130scenesfromtheoriginalDL3DVdatasetandformmorethan
150,000 image pairs. We randomly select another 20 scenes from DL3DV to form the test sets,
evaluatingthecorss-scenecapabilityofourmethod. Moreimplementationdetailsofthemethodcan
befoundinthesupplementarymaterial.
5.2 ComparisonwithState-of-the-Arts
ThequantitativeandqualitativeresultsontheDL3DVtestsetwith36and9inputviewsareshownin
Table1andFigure4. OurapproachoutperformsalltheotherbaselinesinPSNR,SSIM,andLPIPS
scores. NeRF-basedmethodsincludingMip-NeRF[1]andFreeNeRF[43]produceblurrynovel
viewsduetosmoothinginconsistencies.Incontrast,3DGS[18]generateselongatedellipticalartifacts
duetolocalminimaconvergence. DNGuassian[19]reducesartifactswithdepthregularizationbut
resultsinblurryandnoisynovelviews.
ThefirstexampleinFigure4demonstrates3DGS-Enhacerâ€™scapabilitytoremoveartifactswhile
preserving view consistency. By interpolating input views using a video diffusion model, we
incorporatemoreinformationwhileenrusingahighviewconsistency,enablinghigh-qualitynovel
7MipNerf[1] FreeNerf[43] 3DGS[18]
DNGaussian[19] Ours Ground-truth
MipNerf[1] FreeNerf[43] 3DGS[18]
DNGaussian[19] Ours Ground-truth
MipNerf[1] FreeNerf[43] 3DGS[18]
DNGaussian[19] Ours Ground-truth
Figure4: AvisualcomparisonofrenderedimagesonscenesfromDL3DV[20]testsetwiththe
3-viewsetting.
viewsandavoidinglocalminima. Thesecondexamplehighlights3DGS-Enhancerâ€™sadvantagein
recoveringhigh-frequencydetails. Ourdatasetprocessingstrategyandvideodiffusionmodelenable
anunderstandofstrongmulti-viewprioracrossvariousscenes. Asaresult,verychallengingcases
suchasthetreescanberestoredwithsharpdetails. Insummary,comparisonswithbaselinemethods
demonstrateourapproachâ€™spotentialtosignificantlyimprovetheunbounded3DGSrepresentations,
synthesizinghigh-fidelitynovelviewsforopenenvironments.
Todemonstratethegeneralizabilityofourmethodforout-of-distributiondataset,wetrainthemethods
on the DL3DV-10K dataset [20] and test them on the Mip-NeRF360 dataset [2]. The results, as
83DGS(PSNR:16.15) Ours(PSNR:22.32) Ground-truth
3DGS(PSNR:13.90) Ours(PSNR:22.32) Ground-truth
Figure5: Avisualcomparisonofcross-datasetgeneralizationability,wherethemethodsaretrained
ontheDL3DV-10Kdataset[20]andtestedontheMip-NeRF360dataset[2].
summarized in Table 2 and Fig 5, show that our method outperforms the baseline approaches,
highlightingitsremarkablegeneralizationcapabilitiesinunboundedenvironments.
Table2: AquantitativecomparisonofmethodsontheunseenMip-NeRF360dataset[2].
6views 9views
Method
PSNRâ†‘ SSIMâ†‘ LPIPSâ†“ PSNRâ†‘ SSIMâ†‘ LPIPSâ†“
Mip-NeRF360(alltestscenes)
Mip-NeRF 13.08 0.159 0.637 13.73 0.189 0.628
RegNeRF 12.69 0.175 0.660 13.73 0.193 0.629
FreeNeRF 12.56 0.182 0.646 13.20 0.198 0.635
3DGS 11.53 0.144 0.651 12.65 0.187 0.607
DNGaussian 11.81 0.208 0.689 12.51 0.228 0.683
3DGS-Enhancer(ours) 13.96 0.260 0.570 16.22 0.399 0.454
5.3 AblationStudy
Realimageasreferenceviews. Table3showsthequantitativecomparisonsofdifferentcompo-
nentsin3DGS-Enhancerframework. Thevideodiffusionmodelprovidesstrongmulti-viewpriors.
However, due to its native restrictions, we directly feed the original input views into the 3DGS
fine-tuningprocess. Thisresultsinmorereliableandview-consistentinformationfromtheinput
domaintofacilitate3DGSfine-tuning,asdemonstratedbythe"Realimage"inTable3.
Confidenceawarereweighting. Distantviewsarelesslikelytocauseartifacts,sowenormalize
theirdistancetoreferenceviewsbetween[0,1],givinghigherconfidenceofvideodiffusionresultsto
fartherviewpoints. Thisstrategyisdenotedby"Imageconfidence"inTable3. Pixel-levelconfidence,
as denoted by "Pixel confidence" in Table 3, is based on the density of small-volume Gaussians
inwell-reconstructedareas,usingacolorrenderingpipelinetocalculatevolumes. Bothpixeland
image-levelconfidencestrategiesimproveresultsindividually,andtheircombinationyieldsthebest
performance.
Video diffusion and STD. Figure 6 visualizes the effects of video diffusion and STD module,
respectively. Videodiffusionremovesmostoftheartifacts,andSTDmoduleenhancesfine-grained
andhigh-frequencytextures,resultinginmorevividnovelviewrenderings,whichareclosertothe
groundtruth. Table4showstheimprovmentforeachmodules.
9Table3: Anablationstudyofthefourmodulesofour3DGS-Enhancerframework,whereallresults
areaveragedacross3,6,9,and12inputviewsonDL3DVdataset[20].
Videodiffusion Realimage Imageconfidence Pixelconfidence PSNRâ†‘ SSIMâ†‘ LPIPSâ†“
âœ“ - - - 14.33 0.476 0.422
âœ“ âœ“ - - 17.01 0.553 0.361
âœ“ âœ“ âœ“ - 17.29 0.570 0.354
âœ“ âœ“ - âœ“ 17.16 0.564 0.351
âœ“ âœ“ âœ“ âœ“ 17.34 0.574 0.351
Table4: AnablationstudyofSTD(temporallayers)andcolorcorrectionmoduleontheDL3DVtest
datasetwitha9-viewsetting.
Videodiffusion STD(temporallayers) colorcorrection PSNRâ†‘ SSIMâ†‘ LPIPSâ†“
âœ“ - - 18.11 0.591 0.312
âœ“ âœ“ - 18.44 0.625 0.306
âœ“ âœ“ âœ“ 18.50 0.630 0.305
Input Diffusion STD Ground-truth
Figure 6: An ablation study of the video diffusion model components in our 3DGS-Enhancer
framework.
6 Conclusions,Limitations,andFutureWork
This paper has introduced 3DGS-Enhancer, a unified framework that applies view-consistency
prior from video diffusion and use trajectory interpolation method to enhance unbounded 3DGS
representations. Bycombiningimageandpixel-levelconfidencewith3DGSfine-tuning,wehave
achievedstate-of-the-artperformanceinNVSenhancement.However,ourapproachreliesonadjacent
viewsforcontinuousinterpolation,itcannotbeeasilyadaptedtosingle-view3Dmodelgeneration.
Moreover,theconfidence-aware3DGSfine-tuningstrategiesarerelativelysimpleandstraightforward.
Inthefuture,itisinterestingtointegrateconfidencemapsdirectlywiththevideogenerationmodel,
enablingthegenerationofimagesthataremoreinlinewiththereal3Dworldwithouttheneedfor
post-processing. Meanwhile,utilizingtheefficientdatagenerationcapabilityof3DGStoconstructa
massivelyscaleddatasetforourvideogenerationmodelpresentsaprimeopportunitytoenhancethe
modelâ€™s3Dconsistency. Thisapproachalsofacilitatesthe2Dmodelstounderstandthe3Dworld
directlyfrom2Dimageswithoutadditionalgeometricconstraints. Regardingthesocialimpact,the
goalofthisworkistoadvancethefieldsof3DreconstructionandNVS.Therearemanypotential
societalconsequencesofourwork,nonewhichwefeelmustbespecificallyhighlightedhere.
7 Acknowledgement
The authors gratefully acknowledge the Clemson University Palmetto Cluster for providing the
high-performancecomputingresourcesthatsupportedthecomputationsofthiswork.
10References
[1] JonathanTBarron,BenMildenhall,MatthewTancik,PeterHedman,RicardoMartin-Brualla,
andPratulPSrinivasan. Mip-nerf: Amultiscalerepresentationforanti-aliasingneuralradiance
fields. InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,pages
5855â€“5864,2021.
[2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman.
Mip-nerf360: Unboundedanti-aliasedneuralradiancefields. CVPR,2022.
[3] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman.
Zip-nerf: Anti-aliasedgrid-basedneuralradiancefields. ICCV,2023.
[4] AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,MaciejKilian,Do-
minikLorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal. Stablevideodiffusion:
Scalinglatentvideodiffusionmodelstolargedatasets. arXivpreprintarXiv:2311.15127,2023.
[5] DavidCharatan,SizheLesterLi,AndreaTagliasacchi,andVincentSitzmann. pixelsplat: 3d
gaussiansplatsfromimagepairsforscalablegeneralizable3dreconstruction. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages19457â€“19467,
2024.
[6] AnpeiChen,ZexiangXu,AndreasGeiger,JingyiYu,andHaoSu. Tensorf: Tensorialradiance
fields. InEuropeanconferenceoncomputervision,pages333â€“350.Springer,2022.
[7] ZilongChen,YikaiWang,FengWang,ZhengyiWang,andHuapingLiu. V3d: Videodiffusion
modelsareeffective3dgenerators. arXivpreprintarXiv:2403.06738,2024.
[8] JulianChibane,AayushBansal,VericaLazova,andGerardPons-Moll. Stereoradiancefields
(srf): Learning view synthesis from sparse views of novel scenes. In IEEE Conference on
ComputerVisionandPatternRecognition(CVPR).IEEE,jun2021.
[9] SaraFridovich-Keil,AlexYu,MatthewTancik,QinhongChen,BenjaminRecht,andAngjoo
Kanazawa.Plenoxels:Radiancefieldswithoutneuralnetworks.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages5501â€“5510,2022.
[10] Guangcong, Zhaoxi Chen, Chen Change Loy, and Ziwei Liu. Sparsenerf: Distilling depth
rankingforfew-shotnovelviewsynthesis. IEEE/CVFInternationalConferenceonComputer
Vision(ICCV),2023.
[11] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh
Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image
diffusionmodelswithoutspecifictuning.InternationalConferenceonLearningRepresentations,
2024.
[12] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,
Diederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, and Tim Salimans.
Imagenvideo: Highdefinitionvideogenerationwithdiffusionmodels,2022.
[13] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and
DavidJ.Fleet. Videodiffusionmodels,2022.
[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In
InternationalConferenceonLearningRepresentations,2022.
[15] XudongHuang,WeiLi,JieHu,HantingChen,andYunheWang. Refsr-nerf: Towardshigh
fidelityandsuperresolutionviewsynthesis. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition(CVPR),pages8244â€“8253,June2023.
[16] AjayJain, MatthewTancik, andPieterAbbeel. Puttingnerfonadiet: Semanticallyconsis-
tentfew-shotviewsynthesis. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision(ICCV),pages5885â€“5894,October2021.
11[17] YifanJiang,PeterHedman,BenMildenhall,DejiaXu,JonathanT.Barron,ZhangyangWang,
andTianfanXue. Alignerf: High-fidelityneuralradiancefieldsviaalignment-awaretraining.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR),pages46â€“55,June2023.
[18] BernhardKerbl,GeorgiosKopanas,ThomasLeimkÃ¼hler,andGeorgeDrettakis. 3dgaussian
splattingforreal-timeradiancefieldrendering. ACMTransactionsonGraphics,42(4),July
2023.
[19] JiaheLi,JiaweiZhang,XiaoBai,JinZheng,XinNing,JunZhou,andLinGu. Dngaussian:
Optimizingsparse-view3dgaussianradiancefieldswithglobal-localdepthnormalization. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
20775â€“20785,2024.
[20] LuLing,YichenSheng,ZhiTu,WentianZhao,ChengXin,KunWan,LantaoYu,QianyuGuo,
ZixunYu,YawenLu,etal. Dl3dv-10k: Alarge-scalescenedatasetfordeeplearning-based3d
vision. arXivpreprintarXiv:2312.16256,2023.
[21] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl
Vondrick. Zero-1-to-3: Zero-shotoneimageto3dobject. InProceedingsoftheIEEE/CVF
internationalconferenceoncomputervision,pages9298â€“9309,2023.
[22] XiaoxiaoLong,ChengLin,PengWang,TakuKomura,andWenpingWang. Sparseneus: Fast
generalizableneuralsurfacereconstructionfromsparseviews. InEuropeanConferenceon
ComputerVision,pages210â€“227.Springer,2022.
[23] LukeMelas-Kyriazi,IroLaina,ChristianRupprecht,NataliaNeverova,AndreaVedaldi,Oran
Gafni, and Filippos Kokkinos. Im-3d: Iterative multiview diffusion and reconstruction for
high-quality3dgeneration. InternationalConferenceonMachineLearning,2024,2024.
[24] BenMildenhall,PratulP.Srinivasan,RodrigoOrtiz-Cayon,NimaKhademiKalantari,Ravi
Ramamoorthi,RenNg,andAbhishekKar. Locallightfieldfusion: Practicalviewsynthesis
withprescriptivesamplingguidelines. ACMTransactionsonGraphics(TOG),2019.
[25] BenMildenhall,PratulP.Srinivasan,MatthewTancik,JonathanT.Barron,RaviRamamoorthi,
andRenNg. Nerf: Representingscenesasneuralradiancefieldsforviewsynthesis. InECCV,
2020.
[26] MuhammadHusnainMubarik,RamakrishnaKanungo,TobiasZirr,andRakeshKumar. Hard-
wareaccelerationofneuralgraphics. InProceedingsofthe50thAnnualInternationalSympo-
siumonComputerArchitecture,pages1â€“12,2023.
[27] MichaelNiemeyer,JonathanTBarron,BenMildenhall,MehdiSMSajjadi,AndreasGeiger,
and Noha Radwan. Regnerf: Regularizing neural radiance fields for view synthesis from
sparseinputs. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages5480â€“5490,2022.
[28] XuranPan,ZihangLai,ShijiSong,andGaoHuang. Activenerf: Learningwheretoseewith
uncertaintyestimation. InEuropeanConferenceonComputerVision,pages230â€“246.Springer,
2022.
[29] BenPoole,AjayJain,JonathanT.Barron,andBenMildenhall. Dreamfusion: Text-to-3dusing
2ddiffusion. InTheEleventhInternationalConferenceonLearningRepresentations,2023.
[30] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,
pages8748â€“8763.PMLR,2021.
[31] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjÃ¶rnOmmer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),pages10684â€“10695,June
2022.
12[32] KyleSargent,ZizhangLi,TanmayShah,CharlesHerrmann,Hong-XingYu,YunzhiZhang,
EricRyanChan,DmitryLagun,LiFei-Fei,DeqingSun,andJiajunWu. ZeroNVS:Zero-shot
360-degreeviewsynthesisfromasinglerealimage. CVPR,2024,2023.
[33] RuoxiShi,HanshengChen,ZhuoyangZhang,MinghuaLiu,ChaoXu,XinyueWei,Linghao
Chen,ChongZeng,andHaoSu. Zero123++: asingleimagetoconsistentmulti-viewdiffusion
basemodel,2023.
[34] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. MVDream:
Multi-viewdiffusionfor3dgeneration. InTheTwelfthInternationalConferenceonLearning
Representations,2024.
[35] JiaxiangTang,JiaweiRen,HangZhou,ZiweiLiu,andGangZeng. Dreamgaussian: Generative
gaussiansplattingforefficient3dcontentcreation. arXivpreprintarXiv:2309.16653,2023.
[36] VikramVoleti,Chun-HanYao,MarkBoss,AdamLetts,DavidPankratz,DmitriiTochilkin,
ChristianLaforte,RobinRombach,andVarunJampani. SV3D:Novelmulti-viewsynthesisand
3Dgenerationfromasingleimageusinglatentvideodiffusion. arXiv,2024.
[37] Chen Wang, Xian Wu, Yuan-Chen Guo, Song-Hai Zhang, Yu-Wing Tai, and Shi-Min Hu.
Nerf-sr: Highqualityneuralradiancefieldsusingsupersampling. InProceedingsofthe30th
ACMInternationalConferenceonMultimedia,pages6445â€“6454,2022.
[38] Jianyi Wang, Zongsheng Yue, Shangchen Zhou, Kelvin CK Chan, and Chen Change
Loy. Exploiting diffusion prior for real-world image super-resolution. In arXiv preprint
arXiv:2305.07015,2023.
[39] FrederikWarburg,EthanWeber,MatthewTancik,AleksanderHolynski,andAngjooKanazawa.
Nerfbusters: Removingghostlyartifactsfromcasuallycapturednerfs. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision(ICCV),pages18120â€“18130,October
2023.
[40] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson,
Pratul P Srinivasan, Dor Verbin, Jonathan T Barron, Ben Poole, et al. Reconfusion: 3d
reconstructionwithdiffusionpriors. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages21551â€“21561,2024.
[41] Jamie Wynn and Daniyar Turmukhambetov. DiffusioNeRF: Regularizing Neural Radiance
FieldswithDenoisingDiffusionModels. InCVPR,2023.
[42] BinXia,YulunZhang,ShiyinWang,YitongWang,XinglongWu,YapengTian,WenmingYang,
andLucVanGool. Diffir: Efficientdiffusionmodelforimagerestoration. ICCV,2023.
[43] JiaweiYang,MarcoPavone,andYueWang.Freenerf:Improvingfew-shotneuralrenderingwith
freefrequencyregularization. InProc.IEEEConf.onComputerVisionandPatternRecognition
(CVPR),2023.
[44] XiYang,ChenhangHe,JianqiMa,andLeiZhang.Motion-guidedlatentdiffusionfortemporally
consistentreal-worldvideosuper-resolution. InEuropeanConferenceonComputerVision,
pages224â€“242.Springer,2025.
[45] TaoranYi,JieminFang,JunjieWang,GuanjunWu,LingxiXie,XiaopengZhang,WenyuLiu,
QiTian,andXinggangWang. Gaussiandreamer: Fastgenerationfromtextto3dgaussiansby
bridging2dand3ddiffusionmodels. InCVPR,2024.
[46] AlexYu,RuilongLi,MatthewTancik,HaoLi,RenNg,andAngjooKanazawa. Plenoctreesfor
real-timerenderingofneuralradiancefields. InarXiv,2021.
[47] ShangchenZhou,PeiqingYang,JianyiWang,YihangLuo,andChenChangeLoy. Upscale-
a-video: Temporal-consistentdiffusionmodelforreal-worldvideosuper-resolution. InPro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition, pages
2535â€“2545,2024.
13[48] TinghuiZhou,RichardTucker,JohnFlynn,GrahamFyffe,andNoahSnavely. Stereomagnifi-
cation: Learningviewsynthesisusingmultiplaneimages. InSIGGRAPH,2018.
[49] MatthiasZwicker,HanspeterPfister,JeroenBaar,andMarkusGross. Surfacesplatting. Pro-
ceedingsoftheACMSIGGRAPHConferenceonComputerGraphics,2001,082001.
148 Appendix
8.1 Detailsof3DGSEnhancementDataset
Forour3DGSEnhancementDataset,constructedbasedonDL3DV,werandomlyselect120scenes
tocreatethetrainingsetforourvideodiffusionmodeland30scenesasthetestset. Byfollowing
previousworks,weusethestandardtrain/testsplit,selectingevery8thframeoftheremainingframes
forevaluation.
Tocreateimagepairssimulatingtheartifactsduetothelackofinputviewsinnovelviewsynthesis
problem,werendertheimagepairsfrompairsoflow-highquality3DGSmodels. Specifically,the
inputviewsforthehigh-qualitymodelconsistofallimagesintheoriginaldataset,whiletheinputs
forthelow-qualitymodelareasubsetuniformlysampledfromtheoriginaldataset. Toaddmore
complexity, we sample the subset according to a certain number (e.g., 3, 6, 9) or a certain ratio
(e.g., 5%). With the aim to fully capture the distribution of artifacts created by the sparse input
viewsandtrainthevideodiffusionmodelwithsmootherinputs,weproposeaheuristictrajectory
fittingalgorithm,asshowninFigure7,provingasequenceofcamerasbyinterpolatingthelowor
high-qualitymodelâ€™sinputviews. Specifically,iftheoriginalcameratrajectoriesaresmoothand
simple, such as those of DL3DV, we use the high-quality input views as the reference to fit the
trajectories. Forcomplextrajectories,suchasthoseinMip-NeRF360,weusethelow-qualityinput
toavoidsignificantlypoorrenderingresults,whichwouldleadtounreasonableartifactdistributions.
Asaresult,werenderalargenumberofimagepairswithandwithoutartifacts,asshowninFigure8,
ataresolutionof512Ã—512,leadingtopowerfulvideodiffusionpriorswithhighviewconsistency
andphoto-realism.
Input:3 Input:6 Input:9
Figure7: Thefittingtrajectoriesunderdifferentnumberofinputviews.
8.1.1 Trainingdetails
Ourvideodiffusionmodelincludesapre-trainedVAEtoencodeanimagesequenceintoalatent
sequenceanddecodethelatentsequencebackintotheimagesequence. ItalsoincludesaU-Netwith
learnabletemporallayers,whichemployscross-frameattentionmodulesand3DCNNmodulesto
ensureframe-consistentoutputs. Theinputofvideodiffusionmodelisaimagesequencesegment
thatincludes25imageswithdifferentsamplestepsfromtheimagesequencesrenderedfromthe
low-quality3DGSmodel. Thefirstandthelastframesinthissegmentarereplacedwithimages
rendered from the high-quality 3DGS model. During fine-tuning, our video diffusion model is
conditionedontheseimagesequencesegmentsandtrainedtosynthesizethecorrespondingsegments
renderedfromthehigh-quality3DGSmodel.
Ourvideodiffusionmodelisfine-tunedwithalearningrateof0.0001,incorporating500stepsfor
warm-up,followedbyatotalof80,000trainingsteps. Thebatchsizeissetto1ineachGPU,where
each batch consisted of 25 images at 512x512 resolution. To optimize the training process, the
Adamoptimizerisemployed. Additionally,adropoutrateof0.1isappliedtotheconditionsbetween
thefirstandlastframesandthetrainingprocessutilizeCFG(classifier-freeguidance)totrainthe
diffusionmodel. Thetrainingisconductedon2NVIDIAA100-80GGPUsover3days. TheSTD
isfine-tunedwithalearningrateof0.0005and50,000trainingsteps. Thebatchsizeissetto1in
eachGPU,whereeachbatchconsistsof5imagesat512x512resolution,butforinference,itwas
increasedto25. Thefine-tuningprocessisconductedon2NVIDIAA100-80GGPUsin2days. The
entirepipelineâ€™sinferenceandtrainingspeedswereevaluatedandarepresentedinTable5.
15Input:3 Ground-truth Input:6 Ground-truth
Input:9 Ground-truth Input:5% Ground-truth
Figure8: Thelowandhighqualityimagepairscreatedinour3DGSEnhancementdataset.
Table 5: A comparison of per-scene training time and rendering FPS between methods. For our
method,theLQ-3DGSreconstructiontakes10.5minutes,stablevideodiffusioninferencefor50
novelviewsrequires2.0minutes,andtheHQ-3DGSreconstructiontakes12.0minutes.
Method Per-scenetrainingtimeâ†“ RenderingFPSâ†‘
Mip-NeRF 10.7h 0.09
RegNeRF 2.5h 0.09
FreeNeRF 3.8h 0.09
3DGS 10.5min 100
DNGaussian 3.3min 100
3DGS-Enhancer(ours) 24.5min 100
8.2 DetailsofComparisonBaselines
Fortheevaluationdatasets,wecompareagainstthestandard3DGaussianSplatting[18](whichisalso
thereconstructionpipelineusedinourwork),andthestate-of-the-artfew-viewNVSregularization
methods, including Mip-NeRF [1], FreeNeRF [43], Zip-NeRF [3], and RegNeRF [27]. We also
compare to some few-shot NVS methods using generative priors including ZeroNVS [32], and
ReconFusion[40].
FortheevaluationofMipNeRF,FreeNeRF,RegNeRF,andDNGaussianonDL3DVandMip-NeRF
360 dataset, we follow the original configurations and code shared by the authors. Additionally,
we use random point cloud as the initialization for 3DGS, following the implementations from
DNGaussian. WealsodecreasethebatchsizeforRegNeRFfrom4096to512accordingtothelimited
computationresource.
16