TechnicalReport
CompassJudger-1: All-in-one Judge Model Helps Model Eval-
uation and Evolution
MaosongCao,AlexanderLam,HaodongDuan,HongweiLiu
SongyangZhang∗,KaiChen∗
ShanghaiAILaboratory
opencompass@pjlab.org.cn
∗ correspondingauthors
Abstract
Efficientandaccurateevaluationiscrucialforthecontinuousimprovement
oflargelanguagemodels(LLMs). Amongvariousassessmentmethods,
subjectiveevaluationhasgarneredsignificantattentionduetoitssuperior
alignmentwithreal-worldusagescenariosandhumanpreferences. How-
ever,human-basedevaluationsarecostlyandlackreproducibility,making
preciseautomatedevaluators(judgers)vitalinthisprocess. Inthisreport,
weintroduceCompassJudger-1,thefirstopen-sourceall-in-onejudgeLLM.
CompassJudger-1isageneral-purposeLLMthatdemonstratesremarkable
versatility. Itiscapableof: 1. Performingunitaryscoringandtwo-model
comparisons as a reward model; 2. Conducting evaluations according
to specified formats; 3. Generating critiques; 4. Executing diverse tasks
likeageneralLLM.Toassesstheevaluationcapabilitiesofdifferentjudge
modelsunderaunifiedsetting,wehavealsoestablishedJudgerBench,a
newbenchmarkthatencompassesvarioussubjectiveevaluationtasksand
coversawiderangeoftopics. CompassJudger-1offersacomprehensive
solutionforvariousevaluationtaskswhilemaintainingtheflexibilityto
adapttodiverserequirements. BothCompassJudgerandJudgerBenchare
releasedandavailabletotheresearchcommunityatCompassJudger. We
believethatbyopen-sourcingthesetools,wecanfostercollaborationand
accelerateprogressinLLMevaluationmethodologies.
1 Introduction
Theevaluationoflargelanguagemodels(LLMs)typicallyfallsintotwomajorcategories:
objectiveevaluationandsubjectiveevaluation. ObjectiveevaluationevaluatesLLMsusing
questionswithground-truthanswers(multiple-choicequestions,fill-in-the-blankquestions,
etc.). GivenLLMresponsesandanswers,rule-basedapproachesareadoptedtocheckif
theresponsesarecorrectornot. Subjectiveevaluation,ontheotherhand,oftenrequiresa
morecomprehensiveassessmentofmodels’responsesfrommultipleperspectivessuchas
helpfulness,honesty,orcreativity(Liuetal.,2023). Mostquestionsinsubjectiveevaluation
donothaveaground-truthanswer,thusrequiringhumanjudgementforresponsequality
assessment.
Despite the effectiveness, human-based subjective evaluation is time-consuming, labor-
intensive, as well as hard to reproduce. Therefore, in early stages of subjective evalua-
tion, using the best-performing model (such as GPT4 (Achiam et al., 2023)) to evaluate
the responses of LLMs became the most adopted approach, which has been applied to
multiple subjective evaluation benchmarks including AlpacaEval (Dubois et al., 2024a),
ArenaHard(Lietal.,2024),WildBench(Linetal.,2024),andAlignBench(Liuetal.,2023).
However,thosejudgemodelswithstrongercritiquecapabilitiesareoftencommercialAPIs
with limited transparency and charge per use. The cost can be prohibitive for research
studieswhenevaluationsarefrequentlyconducted.
Recenteffortsintheresearchcommunityhavefocusedondevelopingopen-sourcejudge
models for subjective evaluation of large language models (LLMs). For example, Cri-
1
4202
tcO
12
]LC.sc[
1v65261.0142:viXraTechnicalReport
tiqueLLM(Keetal.,2023)hasbeendevelopedtoevaluateAlignBenchinsteadofGPT-4.
Works like Auto-J (Li et al., 2023) and Prometheus (Kim et al., 2024) also explore differ-
ent training paradigms for judge models. However, most open-source judge LLMs are
constrainedbytheirspecifictrainingtasksandcanonlyadheretocertainformats. This
limitationrestrictstheirapplicabilityacrossmultiplesubjectivebenchmarkssimultaneously.
Wepositthat,ratherthandevelopingLLMssolelyforjudgingpurposes,itiscrucialfora
judgeLLMtoexcelinbothjudgingtasksandgeneraltaskstoachievegoodgeneralization
capability,similartoGPT-4. Therefore,thedevelopmentofanall-in-onemodelwithstrong
judgecapabilitiesisparamount. Suchamodelshoulddemonstrate: 1. Robustgeneraliza-
tionabilities;2. Applicabilitytodiversesubjectiveevaluationsettings;3. Adaptabilityto
emergingevaluationtasks.
Inaddition,evaluatingtheeffectivenessofjudgemodelsonsubjectiveevaluationtasksis
crucial. However,amongthenumerousexistingLLMbenchmarks,veryfewarespecifically
designedforassessingjudgemodelsorrewardmodels. RewardBench(Lambertetal.,2024)
representsaninitialattemptinthisdirection,comprisingapproximately3,000questions
with corresponding answers from two models and annotated ground truth preferences
fromhumans. Thisbenchmarkaimstotestwhetherjudgemodelsorrewardmodelscan
accuratelyidentifyrelativeresponsequality. Despitebeingacommendablefirststep,the
scenarioscoveredbyRewardBencharerelativelysimplified. Italsolacksanexaminationof
themodel’scritiquecapabilitiesandtheabilitytogenerateformattedjudgement. Given
these constraints, there is a clear need for a more comprehensive benchmark to assess
judgmentcapabilities.
Inresponsetotheaforementionedchallenges,weconductextensiveinvestigationsontwo
keycomponents: all-in-onejudgertrainingandjudgemodelevaluation. High-qualitydata
isparamounttotheeffectivetrainingofmodels,therefore,wefirstexplorethedatarecipe
fortrainingCompassJudger-1,whichincludesvariousdatasourcessuchaspair-wisedata
usedforrewardmodeltuning,open-sourcecritiquedata,andself-collecteddata. Wefurther
developseveraldatafilteringandsamplingstrategiesforthetrainingprocess. Additionally,
wealsoinvestigatetheinfluenceofthegeneralSFT(G-SFT)dataontrainingajudgemodel.
We also introduce the JudgerBench in this work, which includes the bi-lingual realistic
humanannotationfromtheLLMarena(ChatBotArena(Chiangetal.,2024)andCompas-
sArena1),aswellasGPTannotationsonthetypicalsubjectiveBenchmarks. Therelated
codeandmodelsarereleasedinhttps://github.com/open-compass/CompassJudger.
2 CompassJudger-1
Inthissection,wefirstintroducethetrainingdatausedtobuildCompassJudger-1inSec.2.1,
followedbythedetailsoftrainingstrategiesinSec.2.2
2.1 DataCollection
High-qualitydataisessentialfortraininganeffectivemodel,andthejudgemodelisno
exception.Therefore,wefocusondatacollectionandthecompositionofthetrainingdataset.
Ourtrainingdataisprimarilydividedintothreecategories: publiclyavailablejudgedata,
self-collectedsubjectiveevaluationdata,andrewarddatausedfortrainingrewardmodels.
WepresenttherelevantinformationofallthedatainTable1. Thetableclearlyshowsthat
the number of entries in the training data pool from different sources can vary greatly,
makingithighlyimbalanced. Duetotheuniquenatureofjudgedata,theoutputformat
fromthesamedatasetisoftenuniform. Thiscancausethemodel’sresponsepatternsto
becomerigid,whichisakeyreasonwhymostjudgemodelsstrugglewithout-of-domain
evaluation. Therefore,appropriatesamplingstrategyisnecessarywhenincorporatingthe
dataintothefinaltrainingset.
Additionally,theproportionofgenerativedataintheoveralldatapoolisrelativelysmall,
withmostdatacontainingonlyjudgmentresultsratherthanthereasoningbehindthem.
1LLMArenareleasedbyOpenCompass:https://arena.opencompass.org.cn/
2TechnicalReport
Figure1: TrainingDataCollectionofCompassJudger-1.
Table1: TrainingDataConstructionofCompassJudger-1,Pointwiseindicatesthatthedata
containsonlyonemodel’sresponsealongwiththescoregivenbytheJudgemodel/Reward
model. Pairwise indicates that the data includes responses from two models and the
comparisonresultgivenbytheJudgemodel/Rewardmodel. Generativeindicatesthatthe
dataincludestheJudgeresultsaswellasthereasoningprocessoftheJudge. Thenumber
ofeachdatasetreferstothenumberofcandidatesintheTrainingDatapool,notthefinal
amountoftrainingdata.
Attribution DatasetName DataFormat Number Language
Open-sourceJudgeData AlpacaFarm(Duboisetal.,2024b) Pairwise 39k EN
Auto-J(Lietal.,2023) Pointwise,Pairwise,Generative 9k ZH,EN
PandaLM(Wangetal.,2023) Pairwise 287k EN
JudgeLM(Zhuetal.,2023) Pointwise 100k EN
LLM-Eval2(Zhangetal.,2023) Pointwise,Generative 10k ZH
CritiqueBench(Lanetal.,2024) Generative 1k EN
UltraFeedback(Cuietal.,2023) Pointwise,Generative 380k EN
Open-sourceRewardData OffsetBias(Parketal.,2024) Pairwise 8k EN
Hendrydong(Dongetal.,2024) Pairwise 700k EN
SkyWorker(Shiwenetal.,2024) Pairwise 80k EN
Airoboros Pairwise 36k EN
Anthropic Pairwise 161k EN
PKUAlignment Pairwise 82k EN
SelfCollectJudgeData CJ-Judge-Data-v1 Pointwise,Pairwise,Generative 60k ZH,EN
SelfCollectRewardData MathCodePreference Pairwise 11k EN
ChineseMath Pairwise 76k ZH
LengthControl Pairwise 0.6k EN
LanguageMatch Pairwise 0.5k ZH,EN
Therefore, to strengthen the model’s critique capabilities, it is essential to expand the
generativecritiqueportionoftheexistingdata. Toaddressthis,weintroducethefollowing
strategiesfordatasetsfromvarioussources. Thespecificdataprocessingproceduresare
alsoillustratedinFigure1.
Public Judge Data: We collect public available judge data from below datasets: Pan-
daLM (Wang et al., 2023), JudgeLM (Zhu et al., 2023), AutoJ (Li et al., 2023), Alpaca-
Farm(Duboisetal.,2024b),CritiqueBench(Lanetal.,2024),Ultrafeedback(Cuietal.,2023),
etc. Then,weintroducethreemethodstoimprovethetrainingdataqualityofthem.
•Re-evaluate. Westartbyupdatingoutdatedjudgedata,whichusesoldermodelslike
ChatGPTforjudgingandhaslostrelevance. Therefore,weusethemorepowerfulQwen-
2.5-72B (Yang et al., 2024), to re-evaluate the original tasks and generate new training
data.
•DataCategorization. Wefurthercategorizethedatatofacilitatesubsequentsampling.
Thequestioncomponentsareextracted,andeachquestionisassignedtoacategoryusing
LLMswithspecificallydesignedprompts. Detailedpromptsandcategorylabelscanbe
foundintheA.1.
3TechnicalReport
Table2: AblationStudyAbouttheProportionofRewardData.
Models ProportionofRewardData RewardBench JudgerBench Average
CompassJudger-1-7B 25% 0.810 0.633 0.722
33% 0.812 0.646 0.729
50% 0.823 0.665 0.744
66% 0.831 0.697 0.764
75% 0.833 0.612 0.723
83% 0.834 0.438 0.636
•BalanceSampling. Duringthetrainingofgeneralmodels(Caietal.,2024),weachieved
betterresultswhenthevariousclasslabelsinthetrainingdatamaintainacertainlevelof
balance. Thustoensurecategorybalance,weappliedcategory-balancedsamplingtothe
judgedata. Usingtheclasslabelsobtainedfromthepreviousprocess,weperformedunder-
samplingoncategorieswithampledataandoversamplingoncategorieswithinsufficient
data,ultimatelymaintainingabalanceacrossallcategoriesinthedataset.
Self-collectData DuringtheiterativetrainingofInternLM2.5(Caietal.,2024),wecon-
ductedextensiveevaluationonourinternalsubjectivetestsets. Wecollectedthisinternal
data, which covers a wide range of question types, including pointwise, pairwise, and
single-turndialogueevaluations,andmulti-turndialogueevaluations. Sincethejudgments
are made by models with strong evaluation capabilities, they are directly added to the
trainingdatapoolafterbalancingthequantities,resultinginapproximately20kentriesin
thefinaltrainingset.
RewardData Ourearlyexperimentsindicatesthatexcessiverewarddata,whichlackeda
critiqueprocessandcontainedonlyjudgeresults,ledtomodelcollapse,necessitatingstrict
limitationsonitsquantitytomaintainbalancedtrainingdata. Toenhancethejudgemodel’s
critiquecapabilities, particularlyforreasoningandmathematicsproblems, wereformat
questionsandimplementedaprocessingsystemtogeneratedetailedcritiqueprocesses,
ultimatelycuratingapproximately500,000entriesofhigh-qualitydata. Thepromptsfor
obtainingcritiquesaredetailedinthe A.2.
2.2 TrainingStrategyandAblationStudy
Inthissection, wepresentourtrainingconfigurationanddiscussourablationstudyon
differentdatasources.
TrainingConfiguration Basedontheprocessedtrainingdatapool,weconductexperi-
mentswithvaryingdataratios. WeemployXtuner(Contributors,2023b)asourtraining
frameworkand,throughextensiveexperimentation,determinethatanepochof2anda
learningrateof2e-5areoptimalparameters. Weperformablationexperimentsspecifically
ontheproportionsofrewarddataandgeneralSFTdata.
AblationStudyonRewardData Wefirstexaminetheproportionoftherewarddata. As
thepairwiseevaluationtasksinrewarddataonlyrequirethecategoricalannotation,we
caneasilycollectalargenumberofdatafromthepubliccommunity. Whileincorporating
reward data benefits the training of an all-in-one judge model, our experiments reveal
thatexcessiverewarddatacanleadtomodeloverfitting, resultinginsimplisticoutputs
resembling a reward model and compromising the ability to perform complex critique
tasks. Throughexperimentation(SeeinTable2),wedeterminethattheoptimalproportion
ofrewarddataliesbetween50%and70%,enablingthemodeltoachievestrongjudging
capabilitieswhilemaintaininggeneralizability.
Influence of General SFT Data We then investigate the impact of general SFT data
proportions. Sinceourgoalistocreateapowerfulall-in-onemodelwithjudgingcapabilities
rather than a model specific to particular datasets, incorporating general SFT (G-SFT)
4TechnicalReport
Table3: AblationStudyofGeneralSFTData. ”JudgeAverage”referstotheevaluation
scorethatencompassesthejudgingcapabilitiesofbothRewardBenchandJudgerBench,
while”SubjectiveAverage”istheevaluationscoreonseveralsubjectivedatasetslistedin
thetable. TherelevantevaluationresultsareobtainedusingOpenCompass(Contributors,
2023a). Allresultsfromthecorrespondingdatasetshavebeennormalizedtopercentages.
Models JudgeAverage AlignBench ArenaHard Fofo WildBench Sub.Average
CJ-1-7B-w/oG-SFT 0.693 0.590 0.487 0.750 -0.071 0.490
CJ-1-7B-w.G-SFT 0.697 0.624 0.562 0.740 0.015 0.528
trainingdataisessentialformaintaininggeneralizability. OurG-SFTdatacomesfromthe
internaltrainingdataforSFT.Ourexperimentsshowthatanappropriateamountnotonly
preservesthemodel’sgeneralcapabilitiesbutalsoenhancesitsjudgingperformance. We
presenttheseresultsinTable3.
DataRecipes Basedonourfindings,weestablishtheoptimaltrainingdataratioas:
{critique data : reward data : general SFT data = 1 : 3 : 1} (1)
Our final training dataset comprises approximately 900k entries. We select the recently
releasedopen-sourceQwen2.5series(Yangetal.,2024)asthefoundationmodelsforour
judgeSFTtraining. Wedetailtherelatedresultsinthefollowingsections.
3 JudgerBench
Inthissection,weintroduceJudgerBench,aspecializedevaluationdatasetdesignedtoeval-
uatethejudgemodels. Toreplicaterealisticjudgemodelapplicationscenarios,JudgerBench
incorporatestwodistincttypesofannotations: humanannotationsforthearenapartand
LLM annotations for the benchmark component. We detail the construction process of
thesecomponentsinSec.3.1andpresentCompassJudger’sperformanceonJudgerBenchin
Sec.3.2.
3.1 JudgerBenchConstruction
JudgerBenchconsistsoftwoprimarycomponents:theArenacomponent(denotedasJDB-A)
andtheBenchmarkcomponent(denotedasJDB-B).JDB-A,similartoRewardBench,focuses
onalignmentwithhumanpreferencesandrequiresonlysimplejudgeoutputssuchas[[A]]
or[[B]].JDB-B,ontheotherhand,assessesthemodel’scritiquecapabilitiesanditsability
toprovidejudgmentsfollowingspecificformats.
JudgerBenchA(ArenaPart)
•DataSource. JDB-Aconsistsoftwosections: EnglishandChinese. TheEnglishsectionis
derivedfromthereleaseddataofChatbotArena(Chiangetal.,2024),whiletheChinese
sectioncomesfromthecollecteddataofCompassArena(Contributors,2023a). Thesedata
aretheresultsofhumanvoting,witheachentrycontainingaquestionandthecorresponding
responsesfromtwomodels,alongwiththevoteonwhichmodelisthewinner.BothChinese
and English include single-turn and multi-turn dialogue data, with approximately 500
single-turndialoguesand250multi-turndialogues,totalingaround1500piecesofdata. We
introducedinnextparagraphhowwescreenandobtainthesedata.
•ScreeningProcess. The1,500datapointsinJDB-Awereobtainedthroughthefollowing
screeningprocess: Wefirstperformedunsupervisedclustering(specifically,k-meanswithk
setto50inourimplementation)onallthedatatogetroughcategories. Then,theProcessor
model(Qwen2.5-72B)summarizedspecificcategorynamesbasedontypicalcaseswithin
these categories (detailed category names can be found in the results of Table 7). After
obtainingthetop10categorynamessummarizedbytheprocessor,weusedtheprocessorto
processeachdatapointindividually,assigningthemtotheirrespectivecategories.Following
5TechnicalReport
Table4: DetailedIntroductionofSubjectiveEvaluationDatasetsinJDB-BTheofficial
FoFodatasetincludesonlyEnglish,andwecreatedtheChineseportion. Additionally,due
totheoutdatedreferencesinAlignBench,wechangeditsevaluationmethodfromPointwise
toPairwise.
DatasetName DataFormat Turns ScenarioLabel Language
AlignBench Pairwise SingleTurn DailyChat,ChineseCulture ZH
ArenaHard Pairwise SingleTurn DailyChat,Reasoning,Math,Code EN
FoFo Pointwise SingleTurn InstructionFollowing ZH,EN
WildBench Pairwise SingleTurn,MultiTurn DailyChat EN
Table5: ResultsonRewardBenchandJudgerBench,WhichJDB-AmeansJudgerBench
partA,JDB-BmeansJudgerBenchpartB.
Models RewardBench JDB-AEN JDB-ACN JDB-BAcc JDB-BCorr JudgerBench
Qwen2.5-7B-Chat 0.789 0.567 0.535 0.590 0.874 0.641
Qwen2-72B-Chat 0.822 0.588 0.584 0.625 0.935 0.683
Qwen2.5-72B-Chat 0.832 0.615 0.590 0.681 0.937 0.706
GPT-4o-0806 0.867 0.664 0.608 1 1 0.818
Skywork-llama3.1-8B 0.890 0.630 0.605 - - -
Selftaught-llama3.1-70B 0.900 0.443 0.570 0.598 0.869 0.620
CJ-1-1.5B 0.724 0.553 0.527 0.629 0.905 0.654
CJ-1-7B 0.831 0.570 0.583 0.687 0.948 0.697
CJ-1-14B 0.842 0.599 0.615 0.699 0.959 0.718
CJ-1-32B 0.854 0.614 0.612 0.720 0.963 0.727
thedetailedcategorization,weconductedmanualscreeningtoensurethecorrectnessofthe
judgingdata.
Inaddition,togiveJDB-Aadifficultygradingsystem,besidessettingcategoriesandhan-
dlingsingle-turnandmulti-turndialogues,wealsoperformeddifficultygradingbasedon
thelengthofmodelresponses. Accordingtotheexperienceofsubjectiveevaluation(Dubois
etal.,2024a),Judgersoftenexhibitalengthbias,meaningtheytendtofavorlongerresponses
whenthequalityofresponsesfromtwomodelsissimilar. Toavoidfittingthislengthbiasof
theJudgers,weselectedabatchofshorterresponsesthatweremarkedaswinnerstoserve
asdifficultdata,thusincreasingthedifficultyofJDB-A.
JudgerBenchB(BenchmarkPart)
•ConstructionMethods. JDB-Bprimarilyincludesfourdatasets(AlignBench(Liuetal.,
2023),ArenaHard(Lietal.,2024),FoFo(Xiaetal.,2024),andWildBench(Linetal.,2024)),
which are very commonly used in subjective evaluations, covering different subjective
scenarios(suchasdailychat,instructionfollowing),differentevaluationmethods(suchas
scoring,head-to-headcompetition),differentlanguages(ChineseandEnglish),andsingle
and multi-turn dialogues. We detail the relevant properties of these datasets in Table 4.
Forthesefoursubjectivedatasets,wesampled100questionsfromeachdatasetaccording
totheirrespectivesubcategories,totaling400questions. Then,weusedthetop10closely
rankedmodelsfromtheOpenCompassleaderboard2toobtaintheirresponsestothese400
questions(specificmodelinformationcanbefoundinthedatajson),thusacquiringatotal
of4000QApairs,notethatthese10modelshaveverysimilarandhighcapabilities,which
alsodemonstratesthedifficultyofjudgingwithJDB-B.WethenusedGPT-4o3tojudgethese
pairs,usingthisjudgmentasabenchmarktocheckwhetherthejudgmentresultsofother
modelsalignwiththoseofGPT-4o.
•EvaluationMetrics. Tofacilitatetheresearch,weadopttheGPT-4o’sjudgementasthe
referenceground-truth(thoughtheremayexistnoiseanderrors). Wecalculatefromtwo
metricsforJDB-B,accuracyandcorrelation.
2Weusethedataof202407version.
3Weusegpt-4o-2024-08-06asdefaultifnotspecified.
6TechnicalReport
Table6: DetailedResultsonRewardBench.
Models Chat ChatHard Safety Reasoning Average
Qwen2.5-7B-Chat 0.961 0.567 0.831 0.797 0.789
Qwen2-72B-Chat 0.955 0.640 0.843 0.848 0.822
Qwen2.5-72B-Chat 0.961 0.680 0.838 0.850 0.832
GPT-4o-0806 0.961 0.761 0.881 0.866 0.867
Skywork-llama3.1-8B 0.936 0.814 0.911 0.898 0.890
Selftaught-llama3.1-70B 0.969 0.851 0.896 0.884 0.900
CJ-1-1.5B 0.964 0.495 0.781 0.656 0.724
CJ-1-7B 0.978 0.605 0.847 0.895 0.831
CJ-1-14B 0.975 0.623 0.845 0.925 0.842
CJ-1-32B 0.978 0.656 0.861 0.922 0.854
• Theaccuracyperquestion,i.e.,whethereachmodel’sjudgmentoneachquestion
matchesGPT-4o’sjudgment,whichisaverystringentindicator,especiallywhen
dealingwithdatasetslikeWildBenchthatrequirefive-categoryjudgments(A++,
A+,A=B,B+,B++). Theaccuracyrateofjudgemodelstendstobelower.
• Forthecorrectionmetric,weadoptPearsonproduct-momentcorrelationcoeffi-
cients. Wefirstobtainthescoringresultsof10referencemodelsandthensortthis
result. IftheseaveragescoresonwholedatasetaligncloselywithGPT-4o’sscores,
theneveniftherearediscrepanciesinthescoringoneachquestion,itindicatesthat
theJudgeModelcanreflectgoodjudgmenteffectsoverall.
3.2 JudgerBenchResults
OverallResults
WetestbaselineChatmodels,currentSOTAJudgeModels,andourCompassJudgerserieson
RewardBenchandJudgerBench,reportingtheoverallresultsinTable5,withmoredetailed
resultspresentedinsubsequenttables. Notably, manyjudgemodelsfailedtoadhereto
theprompttemplatesofthesubjectivedatasets,leadingtotestfailures,andweshowcase
one of these failure cases in Appendix A.3. From the table, it can be observed that our
CompassJudgerseriesoutperformsallopen-sourcemodelsandachievesover95%ofGPT-
4o’sjudgingcapabilityintherelevancetestsonJDB-B.WhileGPT-4odemonstrateshigh
consistencywithhumanevaluationresultsacrossdifferentdatadomains,somemodels(such
asSelftaught,skyworker),despiteachievinghighscoresinonedomainlikeRewardBench,
loseacertaindegreeofgeneralizationinotherdomainsanddonotpossessgooduniversal
judgingcapabilities. Eventhoughtheyaregenerativemodels,theyarenolongerableto
followinstructionstoevaluatecommonsubjectivedatasets. Incontrast,CompassJudger
v1achievedrelativelybalancedresultsonbothRewardBenchandJDB-A,andshoweda
significantleadonJDB-B.
RewardBenchResults
We present the detailed results on RewardBench in Table 6. For RewardBench, some
existingJudgeModelsperformwell(e.g. SkyworkandSelftaught),evensurpassingGPT-4o.
However,uponcloserinspection,themaingapisevidentintheChatHardcategory. The
number of questions in this category does not constitute a large proportion of the total
questions on RewardBench. There is a possibility of over-training with respect to these
JudgeModels,andourCompassJudgerseriesalsoshowsimprovingscoresinthiscategory
asthemodelsizeincreases.
JudgerBenchAResults
ThedetailedresultsforJudgerBenchAarepresentedinTables 7and 8,showingthatthere
are slight differences in the judging capabilities of various models in both Chinese and
Englishdomains. Forinstance,theEnglishreasoningjudgingperformanceofQwen2.5-7B-
ChatislowerthanitsChinesereasoningperformance,andtheEnglishscoresforhumanities
arealsomuchlowerthantheChinesescoresforallmodels. Ontheotherhand,GPT-4o
7TechnicalReport
Table7: DetailedResultsonJDB-A-EN.
Models Teaser AI Roleplay Chat Math Reasoning Creation Code Science Humanities
Qwen2.5-7B-Chat 0.54 0.59 0.59 0.46 0.69 0.43 0.61 0.65 0.58 0.52
Qwen2-72B-Chat 0.63 0.59 0.54 0.49 0.62 0.64 0.60 0.74 0.51 0.52
Qwen2.5-72B-Chat 0.68 0.57 0.57 0.47 0.78 0.64 0.58 0.75 0.61 0.52
GPT-4o-0806 0.82 0.53 0.62 0.61 0.83 0.67 0.67 0.73 0.64 0.55
Skywork-llama3.1-8B 0.69 0.61 0.54 0.62 0.63 0.64 0.60 0.69 0.74 0.53
Selftaught-llama3.1-70B 0.47 0.45 0.47 0.37 0.45 0.43 0.36 0.58 0.48 0.36
CJ-1-1.5B 0.42 0.56 0.56 0.43 0.66 0.47 0.55 0.78 0.64 0.44
CJ-1-7B 0.56 0.56 0.51 0.47 0.68 0.58 0.58 0.75 0.58 0.43
CJ-1-14B 0.66 0.51 0.57 0.54 0.72 0.61 0.56 0.74 0.61 0.47
CJ-1-32B 0.66 0.57 0.56 0.59 0.78 0.58 0.55 0.75 0.60 0.49
Table8: DetailedResultsonJDB-A-CN.
Models Teaser AI Roleplay Chat Math Reasoning Creation Code Science Humanities
Qwen2.5-7B-Chat 0.46 0.58 0.36 0.45 0.70 0.53 0.52 0.53 0.52 0.64
Qwen2-72B-Chat 0.62 0.54 0.34 0.55 0.68 0.63 0.58 0.58 0.62 0.64
Qwen2.5-72B-Chat 0.65 0.47 0.49 0.47 0.71 0.60 0.57 0.58 0.69 0.60
GPT-4o-0806 0.77 0.56 0.51 0.53 0.67 0.66 0.63 0.58 0.62 0.58
Skywork-llama3.1-8B 0.62 0.58 0.58 0.59 0.63 0.58 0.60 0.61 0.60 0.61
Selftaught-llama3.1-70B 0.62 0.56 0.55 0.48 0.67 0.55 0.57 0.57 0.51 0.61
CJ-1-1.5B 0.54 0.58 0.38 0.38 0.62 0.63 0.54 0.52 0.55 0.54
CJ-1-7B 0.62 0.54 0.41 0.58 0.70 0.60 0.59 0.56 0.59 0.60
CJ-1-14B 0.69 0.61 0.51 0.55 0.71 0.68 0.60 0.58 0.61 0.65
CJ-1-32B 0.69 0.58 0.53 0.52 0.71 0.53 0.60 0.61 0.61 0.69
Table9: DetailedAccuracyResultsonJDB-B.
Models AlignBench Fofo WildBench ArenaHard Average
Qwen2.5-7B-Chat 0.777 0.670 0.470 0.444 0.590
Qwen2-72B-Chat 0.867 0.692 0.564 0.376 0.625
Qwen2.5-72B-Chat 0.878 0.677 0.599 0.570 0.681
Selftaught-llama3.1-70B 0.755 0.627 0.538 0.472 0.598
CJ-1-1.5B 0.822 0.712 0.550 0.430 0.629
CJ-1-7B 0.816 0.783 0.564 0.586 0.687
CJ-1-14B 0.839 0.787 0.566 0.602 0.699
CJ-1-32B 0.857 0.806 0.596 0.621 0.720
leadsinbothChineseandEnglishacrossalldomains,particularlyintheTeasercategory,
whereGPT-4osignificantlyoutperformsothermodels,indicatingitsbetterunderstanding
and discernment of human teasers. In the Chat category, the scores of the models are
relativelycloseinbothChineseandEnglish. FromthescoringsituationofCompassJudger,
itsstrengthsareevidentinMath,Reasoning,andCode,andthesesub-domainsareclosely
related to the model’s judging capabilities. Therefore, the high judging performance of
CompassJudgerisalsoattributedtotheenhancedreasoningandmathematicalabilitiesin
itstrainingdata,whichalsoprovidesinsightsforthedevelopmentofsubsequentJudge
Modelsandgeneral-purposemodels.
JudgerBenchBResults
OnJudgerBenchB,wereporttheresultsbasedontheaccuracyrateperquestionandthe
correlationresultsbasedontheoverallmodelscores,showninTables9and 10,respectively.
Many models, although they are generative Judge Models, are fundamentally unable
to evaluate according to the instructions given by the dataset, and therefore we cannot
reporttheirscoresonJudgerBenchB.Thisisalsooneoftheoriginalintentionsbehindour
proposal for an All-in-one judge model. From the tables, we can see that even though
Selftaught-llama3.1-70BisamoregeneralizableJudgeModelthanSkywork-llama3.1-8B,
theperformanceofthisspecializedJudgeModelinsubjectiveevaluationsiseveninferiorto
thatofgeneralmodelsliketheQwen-chatseries. Thisalsoindicatesthatthedomainsfor
trainingcurrentJudgeModelsaretoonarrow,whileCompassJudgermaintainsexcellent
generalizabilityandjudgingcapabilitiesamongthem.
8TechnicalReport
Table10: DetailedCorrelationResultsonJDB-B.
Models AlignBench Fofo WildBench ArenaHard Average
Qwen2.5-7B-Chat 0.916 0.681 0.967 0.931 0.874
Qwen2-72B-Chat 0.937 0.889 0.976 0.936 0.935
Qwen2.5-72B-Chat 0.964 0.916 0.958 0.912 0.937
Selftaught-llama3.1-70B 0.918 0.667 0.950 0.942 0.869
CJ-1-1.5B 0.928 0.851 0.981 0.858 0.905
CJ-1-7B 0.956 0.936 0.970 0.932 0.948
CJ-1-14B 0.966 0.956 0.965 0.951 0.959
CJ-1-32B 0.973 0.951 0.954 0.975 0.963
4 ConclusionandDiscussion
Inthisreport,weproposetheall-in-oneJudgemodel,CompassJudger,andtheJudgerBench
forevaluatingmodeljudgingcapabilities. OurCompassJudgermodelhasachievedthe
bestresultsamongopen-sourcemodelsonJudgerBenchandistrulycapableofreplacing
GPT-4oforevaluatingcommonsubjectivedatasets,greatlyreducingthecostofsubjective
evaluations. Inaddition,wehavemanyissuesforfurtherdiscussion.
HowcantheJudgeModelassistintheiterationofmodels? Thepotentialofagoodjudge
modelisnotlimitedtojustjudgingandcritiquingcapabilities;itcanalsohelpmodelsiterate
andevolve. Itshouldpointouttheshortcomingsofthemodelwhenansweringquestions
andprovideguidance,whichismoreconducivetothemodelimprovingitsresponsesor
achievingamorestandardizedanswer—somethingthataregularrewardmodelcannot
accomplish. Thishasbeenvalidatedinrelatedexperiments,andourinternalexperimental
resultsarecomingsoon,whichwilldemonstratehowweusethejudgemodeltofacilitate
betteriterationofthemodel.
Ifthemodel’sjudgingcapabilityispartofitsgeneralabilities,canjudgetrainingenhance
themodel’soverallgeneralcapabilities? JustasGPT-4canhandlealljudgetasks,we
believe that judge capability is just a part of the model’s general abilities, with a focus
onreasoningandinstructionfollowing. Ourexperimentshavealsoobservedthatgood
training in instruction following and reasoning abilities can significantly improve the
model’sjudgingcapability. Conversely,relevantjudgedatacanfurtherenhancethemodel’s
reasoningandinstructionfollowingabilities.
5 Acknowledgement
WewouldliketoexpressoursinceregratitudetoInternLM’spost-trainingteamforgener-
ouslyprovidingthedatausedinthisresearch. Theircontributionisessentialtothesuccess
ofthisproject. WealsoextendourthankstoJiayeGefortheirinvaluablesupport,guidance,
andcoordinationthroughouttheproject.
9TechnicalReport
References
JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
Gpt-4technicalreport. arXivpreprintarXiv:2303.08774,2023.
ZhengCai,MaosongCao,HaojiongChen,KaiChen,KeyuChen,XinChen,XunChen,Zehui
Chen,ZhiChen,PeiChu,etal. Internlm2technicalreport. arXivpreprintarXiv:2403.17297,
2024.
Wei-LinChiang, LianminZheng, YingSheng, AnastasiosNikolasAngelopoulos, Tianle
Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E Gonzalez, et al.
Chatbotarena: Anopenplatformforevaluatingllmsbyhumanpreference. arXivpreprint
arXiv:2403.04132,2024.
OpenCompassContributors.Opencompass:Auniversalevaluationplatformforfoundation
models. https://github.com/open-compass/opencompass,2023a.
XTunerContributors. Xtuner: Atoolkitforefficientlyfine-tuningllm. https://github.com/
InternLM/xtuner,2023b.
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie,
ZhiyuanLiu,andMaosongSun. Ultrafeedback: Boostinglanguagemodelswithhigh-
qualityfeedback,2023.
HanzeDong,WeiXiong,BoPang,HaoxiangWang,HanZhao,YingboZhou,NanJiang,
DoyenSahoo,CaimingXiong,andTongZhang. Rlhfworkflow: Fromrewardmodeling
toonlinerlhf,2024.
YannDubois,Bala´zsGalambosi,PercyLiang,andTatsunoriBHashimoto.Length-controlled
alpacaeval: Asimplewaytodebiasautomaticevaluators. arXivpreprintarXiv:2404.04475,
2024a.
YannDubois,ChenXuechenLi,RohanTaori,TianyiZhang,IshaanGulrajani,JimmyBa,
CarlosGuestrin,PercySLiang,andTatsunoriBHashimoto. Alpacafarm: Asimulation
frameworkformethodsthatlearnfromhumanfeedback. AdvancesinNeuralInformation
ProcessingSystems,36,2024b.
Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang,
AohanZeng,YuxiaoDong,HongningWang,etal. Critiquellm: Scalingllm-as-criticfor
effectiveandexplainableevaluationoflargelanguagemodelgeneration. arXivpreprint
arXiv:2311.18702,2023.
Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun
Yoon,GuijinSon,YejinCho,SheikhShafayat,JinheonBaek,etal. Thebiggenbench: A
principled benchmark for fine-grained evaluation of language models with language
models. arXivpreprintarXiv:2406.05761,2024.
Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khy-
athi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench:
Evaluatingrewardmodelsforlanguagemodeling. arXivpreprintarXiv:2403.13787,2024.
TianLan,WenweiZhang,ChenXu,HeyanHuang,DahuaLin,KaiChen,andXian-lingMao.
Criticbench: Evaluatinglargelanguagemodelsascritic. arXivpreprintarXiv:2402.13764,
2024.
JunlongLi,ShichaoSun,WeizheYuan,Run-ZeFan,HaiZhao,andPengfeiLiu. Generative
judgeforevaluatingalignment. arXivpreprintarXiv:2310.05470,2023.
TianleLi,Wei-LinChiang,EvanFrick,LisaDunlap,TianhaoWu,BanghuaZhu,JosephE
Gonzalez,andIonStoica. Fromcrowdsourceddatatohigh-qualitybenchmarks: Arena-
hardandbenchbuilderpipeline. arXivpreprintarXiv:2406.11939,2024.
10TechnicalReport
BillYuchenLin,YuntianDeng,KhyathiChandu,FaezeBrahman,AbhilashaRavichander,
ValentinaPyatkin,NouhaDziri,RonanLeBras,andYejinChoi. Wildbench: Benchmark-
ingllmswithchallengingtasksfromrealusersinthewild. arXivpreprintarXiv:2406.04770,
2024.
XiaoLiu,XuanyuLei,ShengyuanWang,YueHuang,ZhuoerFeng,BosiWen,JialeCheng,
PeiKe,YifanXu,WengLamTam,etal. Alignbench: Benchmarkingchinesealignmentof
largelanguagemodels. arXivpreprintarXiv:2311.18743,2023.
JunsooPark,SeungyeonJwa,MeiyingRen,DaeyoungKim,andSanghyukChoi. Offsetbias:
Leveragingdebiaseddatafortuningevaluators,2024.
TuShiwen,ZhaoLiang,ChrisYuhaoLiu,LiangZeng,andYangLiu. Skyworkcriticmodel
series. https://huggingface.co/Skywork,September2024. URLhttps://huggingface.
co/Skywork.
Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen,
ChaoyaJiang,RuiXie,JindongWang,XingXie,etal. Pandalm: Anautomaticevaluation
benchmarkforllminstructiontuningoptimization. arXivpreprintarXiv:2306.05087,2023.
CongyingXia,ChenXing,JiangshuDu,XinyiYang,YihaoFeng,RanXu,WenpengYin,and
CaimingXiong. Fofo: Abenchmarktoevaluatellms’format-followingcapability. arXiv
preprintarXiv:2402.18667,2024.
AnYang,BaosongYang,BinyuanHui,BoZheng,BowenYu,ChangZhou,ChengpengLi,
ChengyuanLi,DayihengLiu,FeiHuang,etal. Qwen2technicalreport. arXivpreprint
arXiv:2407.10671,2024.
MingZhang,YueZhang,ShichunLiu,HaipengYuan,JunzheWang,YuruiDong,Jingyi
Deng, Tao Gui, Qi Zhang, and Xuanjing Huang. LLMEval-2, July 2023. URL https:
//github.com/llmeval/llmeval-2.
LianghuiZhu,XinggangWang,andXinlongWang. Judgelm: Fine-tunedlargelanguage
modelsarescalablejudges. arXivpreprintarXiv:2310.17631,2023.
11TechnicalReport
A Appendix
A.1 Promptandcategorylabelfordatacategorizationofpublicjudgedata.
Promptforcategorization
CATEGORY MAP = [”General Q&A”, ”Tech Consulting”, ”Education Tutoring”,
”Healthcare”, ”Travel”, ”Finance&Investment”, ”LegalAdvice”, ”Psychological
Counseling”,”EntertainmentGossip”,”Cuisine&Cooking”,”HomeImprovement”,
”AutoMaintenance”,”VideoGames”,”Sports&Fitness”,”Literature&Art”,”His-
tory&Humanities”,”Politics&CurrentEvents”,”Religion&Faith”,”ParentingEd-
ucation”,”PetCare”,”CareerPlanning”,”ShoppingRecommendations”,”Lifestyle
Services”,”Relationships&Emotions”,”SocialNetworking”,”Programming&De-
velopment”,”DataAnalysis”,”Marketing”,”BusinessManagement”,”Entrepreneur-
shipGuidance”,”ScientificExploration”,”EnvironmentalProtection”,”Other”]
CATEGORYPROMPT=Ineedtocategorizetheuser’squestion. Belowisacategory
map. Pleasehelpmecategorizetheuser’squestionintooneofthesecategories.
Thecategorymapisasfollows: {categorymap}
[StartofUserQuestion]
{question}
[EndofUserQuestion]
Pleaseprovidethecategorystrictlyinthefollowingformatanddonotoutputany
otherstatements:
[StartofQuestionCategorization]
xxxx
[EndofQuestionCategorization]
A.2 Promptforgettingcritiqueofrewarddata.
Promptforgettingcritique
Pleaseactasanimpartialjudgeandevaluatethequalityoftheresponsesprovided
bytwoAIassistantstotheuserquestiondisplayedbelow. Youshouldchoosethe
assistantthatfollowstheuser’sinstructionsandanswerstheuser’squestionbetter.
Yourevaluationshouldconsiderfactorssuchasthehelpfulness,relevance,accuracy,
depth, creativity, andlevelofdetailoftheirresponses. Beginyourevaluationby
comparingthetworesponsesandprovideashortexplanation. Avoidanyposition
biasesandensurethattheorderinwhichtheresponseswerepresenteddoesnot
influenceyourdecision. Donotallowthelengthoftheresponsestoinfluenceyour
evaluation. Donotfavorcertainnamesoftheassistants. Beasobjectiveaspossible.
Afterprovidingyourexplanation,outputyourfinalverdictbystrictlyfollowingthis
format: ”[[A]]”ifassistantAisbetter,”[[B]]”ifassistantBisbetter.
[UserQuestion]
xxx
[TheStartofAssistantA’sAnswer]
xxx
[TheEndofAssistantA’sAnswer]
[TheStartofAssistantB’sAnswer]
xxx
[TheEndofAssistantB’sAnswer]
A.3 FailureCaseforJudgerBenchB
12TechnicalReport
JudgePrompt:
Pleaseactasanimpartialjudgeandevaluatethequalityoftheresponsesprovided
bytwoAIassistantstotheuserpromptdisplayedbelow. Youwillbegivenassistant
A’sanswerandassistantB’sanswer. Yourjobistoevaluatewhichassistant’sanswer
isbetter.
Beginyourevaluationbygeneratingyourownanswertotheprompt. Youmust
provideyouranswersbeforejudginganyanswers.
When evaluating the assistants’ answers, compare both assistants’ answers with
youranswer. Youmustidentifyandcorrectanymistakesorinaccurateinformation.
Thenconsideriftheassistant’sanswersarehelpful,relevant,andconcise. Helpful
meanstheanswercorrectlyrespondstothepromptorfollowstheinstructions. Note
whenuserprompthasanyambiguityormorethanoneinterpretation, itismore
helpfulandappropriatetoaskforclarificationsormoreinformationfromtheuser
thanprovidingananswerbasedonassumptions. Relevantmeansallpartsofthe
responsecloselyconnectorareappropriatetowhatisbeingasked. Concisemeans
theresponseisclearandnotverboseorexcessive.
Thenconsiderthecreativityandnoveltyoftheassistant’sanswerswhenneeded.
Finally,identifyanymissingimportantinformationintheassistants’answersthat
wouldbebeneficialtoincludewhenrespondingtotheuserprompt.
Afterprovidingyourexplanation,youmustoutputonlyoneofthefollowingchoices
asyourfinalverdictwithalabel:
1. AssistantAissignificantlybetter: [[A>>B]]
2. AssistantAisslightlybetter: [[A>B]]
3. Tie,relativelythesame: [[A=B]]
4. AssistantBisslightlybetter: [[B>A]]
5. AssistantBissignificantlybetter: [[B>>A]]
Exampleoutput: Myfinalverdictistie: [[A=B]].
[UserPrompt]
Can you come up with a 12 bar chord progression in C that works in the lydian
mode?
[TheStartofAssistantA’sAnswer]
Sure! IntheLydianmode,thechordsarebuiltonthescaledegreesoftheLydian
scale. Here’sa12barchordprogressioninCLydian:|Cmaj7|Dmaj7|Em7|F#m7b5
||Cmaj7|Dmaj7|Em7|F#m7b5||Cmaj7|Dmaj7|Em7|Cmaj7|Thisprogression
usestheI,II,iii,andivchordsoftheCLydianmode.
[TheEndofAssistantA’sAnswer]
[TheStartofAssistantB’sAnswer]
Certainly! TheLydianmodeisthefourthmodeofthemajorscaleandisknownfor
its#4interval,whichgivesitadistinctive,brightquality. Tocreatea12barchord
progression in C Lydian, we’ll use chords that are diatonic to the C Lydian scale
(CDEF#GAB).Here’sasimple12-barchordprogressioninCLydian,focusing
onthecharacteristic#4(F#)togiveitthatLydianflavor: |Cmaj7|Cmaj7|Cmaj7|
Cmaj7||F#m7|F#m7|G7|G7||Cmaj7|Am7|D7|G7||Cmaj7|F#m7|G7|G7
|Explanation:
Bars1to4: StartwithfourbarsofCmaj7,establishingthetonic.
Bars5to6: UseF#m7,whichisaminorchordbuiltonthe#4(F#)oftheCLydian
scale. ThisisthekeychordthatgivestheprogressionitsLydiancharacter.
Bars7to8: TransitionwithG7,whichisthedominantchordinCLydian. Itsetsupa
nicetensionbeforeresolvingbacktothetonic.
Bars9to10: ReturntoCmaj7,thenuseAm7,whichisaiichordinCLydian,andD7,
whichisaV7chord. Thisii-Vmovementcreatesabitoftensionleadingbacktothe
tonic.
Bars11to12: UseF#m7andG7againtoreinforcetheLydianmodeandcreateanice
tension-resolutiondynamicbeforeendingonG7,whichcanleadbacktoCmaj7ifyou
wanttolooptheprogression. Thisprogressionhighlightstheuniquequalityofthe
Lydianmodewhilemaintainingacoherentharmonicflow. Feelfreetoexperiment
withdifferentvoicingsandextensionstofurtherexploretheLydiansound!
[TheEndofAssistantB’sAnswer]
13TechnicalReport
JudgeResponsesofDifferentModels:
GPT-4oJudgeResponse:
Tocreatea12-barchordprogressionintheLydianmodeinC,weneedtounderstand
thecharacteristicsoftheLydianmode. TheLydianmodeisthefourthmodeofthe
majorscaleandischaracterizedbyaraisedfourthdegree. InthekeyofCLydian,
thenotesareC,D,E,F#,G,A,andB.Thechordsbuiltonthesescaledegreeswould
beasfollows:
1. Cmaj7(I)
2. Dmaj7(II)
3. Em7(iii)
4. F#dim7(iv)
5. G7(V)
6. Am7(vi)
7. Bm7(vii)
Here’sa12-barchordprogressionthatemphasizestheLydiansoundbyhighlighting
theraisedfourth(F#):
|Cmaj7|Dmaj7|Cmaj7|Em7|
|F#dim7|G7|Cmaj7|Dmaj7|
|Cmaj7|Em7|F#dim7|Cmaj7|
ThisprogressionusestheI,II,iii,andivchords,whilealsoincorporatingtheVchord
toaddsometensionandresolutionwithintheprogression. TheuseofF#dim7helps
tohighlighttheLydiancharacteristicoftheraisedfourthdegree.
**EvaluationoftheResponses:**
**AssistantA:**
1. **Accuracy**: Assistant A initially explains the F Lydian mode instead of the
C Lydian mode, which is a misunderstanding of the prompt. The progression it
providesismorerelatedtoCmajorwithemphasisonFmajor,ratherthanatrue
explorationofCLydian,whichshouldfocusontheraisedfourth(F#).
2. **Helpfulness and Relevance**: While Assistant A provides a progression, it
doesn’taccuratelyreflecttheLydiancharacteristicsinCasrequested. Theexplana-
tionoftheLydianmodeisslightlyoffbecauseitreferencestheFLydianmode.
3. **ClarityandConciseness**: Theresponseisdetailedbutincludessomeirrelevant
informationabouttheFLydianmode,whichcouldconfusetheuser.
**AssistantB:**
1. **Accuracy**: AssistantBcorrectlyusesthechordsfromtheCLydianscale,which
includestheraisedfourth(F#),thoughitincorrectlylabelsF#m7b5asivinsteadof
iv7.
2. **HelpfulnessandRelevance**: Theresponseismorerelevanttothepromptsince
itfocusesonCLydian. ItprovidesaprogressionthatmorecloselyreflectstheLydian
mode,thoughitcouldusemoreexplanationonwhythesechordsarechosen.
3. **ClarityandConciseness**: Theresponseisconciseanddirectlyaddressesthe
promptwithacorrectLydianmodeprogression.
**FinalVerdict**: AssistantB’sresponseismoreaccurateandrelevanttotheprompt,
althoughbothassistantshaveareasforimprovement.AssistantBcorrectlymaintains
theCLydianmodefocus,despiteaminorerrorinchordlabeling. Thus,myfinal
verdictis: [[B>A]]
Skywork-Critic-Llama-3.1-8BJudgeResponse:
[[A]]
14