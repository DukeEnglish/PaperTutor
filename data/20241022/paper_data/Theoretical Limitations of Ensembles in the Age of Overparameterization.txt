Preprint
THEORETICAL LIMITATIONS OF ENSEMBLES IN THE
AGE OF OVERPARAMETERIZATION
NiclasDern∗ JohnP.Cunningham
TechnicalUniversityofMunich ColumbiaUniversity,ZuckermanInstitute
niclas.dern@tum.de jpc2181@columbia.edu
GeoffPleiss
UniversityofBritishColumbia,VectorInstitute
geoff.pleiss@stat.ubc.ca
ABSTRACT
Classic tree-based ensembles generalize better than any single decision tree. In
contrast, recent empirical studies find that modern ensembles of (overparame-
terized) neural networks may not provide any inherent generalization advantage
over single but larger neural networks. This paper clarifies how modern overpa-
rameterizedensemblesdifferfromtheirclassicunderparameterizedcounterparts,
usingensemblesofrandomfeature(RF)regressorsasabasisfordevelopingthe-
ory. In contrast to the underparameterized regime, where ensembling typically
inducesregularizationandincreasesgeneralization,weprovethatinfiniteensem-
blesofoverparameterizedRFregressorsbecomepointwiseequivalentto(single)
infinite-widthRFregressors.Thisequivalence,whichisexactforridgelessmodels
andapproximateforsmallridgepenalties,impliesthatoverparameterizedensem-
blesandsinglelargemodelsexhibitnearlyidenticalgeneralization. Asaconse-
quence,wecancharacterizethepredictivevarianceamongstensemblemembers,
anddemonstratethatitquantifiestheexpectedeffectsofincreasingcapacityrather
thancapturinganyconventionalnotionofuncertainty. Ourresultschallengecom-
mon assumptions about the advantages of ensembles in overparameterized set-
tings, prompting a reconsideration of how well intuitions from underparameter-
izedensemblestransfertodeepensemblesandtheoverparameterizedregime.
1 INTRODUCTION
Ensemblingisoneofthemostwell-establishedtechniquesinmachinelearning(e.g.Schapire,1990;
Hansen&Salamon,1990;Opitz&Maclin,1999;Dietterich,2000). Historically, mostensembles
aggregatedcomponentmodelsthataresimplebytoday’sstandards. Commontechniqueslikebag-
ging(Breiman,1996),featureselection(Breiman,2001),randomprojections(Kaba´n,2014;Thanei
et al., 2017), and boosting (Freund, 1995; Chen & Guestrin, 2016) were developed and analyzed
assumingdecisiontrees,least-squaresregressors,andotherunderparameterizedcomponentmodels
incapable of achieving near-zero training error. Crucially, the resulting ensembles achieve better
generalizationthanwhatcouldbeachievedbyanyindividualcomponentmodel.
Recently,researchersandpractitionershaveturnedtoensemblinglargeoverparameterizedmodels,
suchasneuralnetworks,whichhavemorethanenoughcapacitytomemorizetrainingdatasetsand
are typically trained with little to no regularization. Like ensembles of underparameterized mod-
els, ensembles of large neural networks are often used to reduce generalization error (Lee et al.,
2015;Fortetal.,2019). Motivatedbypracticaleffectivenessandheuristicsfromclassicensembles
(Mentch&Hooker,2016), somehavefurtherarguedthatthepredictivevarianceamongstcompo-
nentmodelsintheseso-calleddeepensemblesisawell-calibratednotionofuncertainty(Lakshmi-
narayananetal.,2017;Ovadiaetal.,2019;Gustafssonetal.,2020)thatcanbeusedondownstream
decision-makingtasks(Galetal.,2017;Yuetal.,2020).
∗WorkdonewhileattheVectorInstitute.
1
4202
tcO
12
]LM.tats[
1v10261.0142:viXraPreprint
Whiletherearefewtheoreticalworksanalyzingthesemodernoverparameterizedensembles,recent
empiricalevidencesuggeststhatintuitionsfromtheirunderparameterizedcounterpartsdonothold
inthisnewregime. Forexample,classicmethodstoincreasediversityamongstcomponentmodels,
suchasbagging,areharmfulfordeepensembles(Nixonetal.,2020;Jeffaresetal.,2024;Abeetal.,
2024) despite being nearly universally beneficial for underparameterized ensembles. Moreover,
severalrecentstudiesquestionwhetherdeepensemblesoffersignificantimprovementsinrobustness
and uncertainty quantification over what can be achieved by a single (but larger) neural network
(Abeetal.,2022; Theisenetal.,2024;Chenetal., 2024). Theseresultssuggestthat anensemble
of(large)overparameterizednetworksmaynotdifferfundamentallyfromasingle(extremelylarge)
neuralnetwork,incontrasttotheunderparameterizedregimewhereensemblesareafundamentally
differentclassofpredictorsSchapire(1990);Breiman(2001);Kaba´n(2014).
Toaddressthisdivergenceandverifyrecentempiricalfindingsondeepensembles,wedevelopathe-
oreticalcharacterizationofensemblesintheoverparameterizedregime,withthegoalofcontrasting
against(traditional)underparameterizedensembles. Weanswerthefollowingquestions:
1. Do large ensembles of overparameterized models differ from single (very large) models
trainedonthesamedata?Doesthecapacityofthecomponentmodelsaffectthisdifference?
2. Underafixedparameter/computationbudget,doesanensembleofoverparameterizedmod-
elsprovideadditionalgeneralizationorrobustnessbenefitsoverasingle(larger)model?
3. What does the predictive variance of overparameterized ensembles measure, and does it
relatetodifferentnotionsofuncertainty?
Toanswerthesequestions,weanalyzeensemblesofoverparameterizedrandomfeature(RF)linear
regressors,awidelyusedtheoretically-tractableapproximationofneuralnetworks(e.g.Belkinetal.,
2018; Bartlett et al., 2020; Mei & Montanari, 2022). These models can be interpreted as neural
networkswhereonlythelastlayeristrained(e.g.Rudi&Rosasco,2017;Belkinetal.,2019)oras
first-orderTaylorapproximationsofneuralnetworks(e.g.Jacotetal.,2018). Byaveragingmodels
that differ solely in their random features, we emulate the common practice of ensembling neural
networks that differ only by random initialization (Lakshminarayanan et al., 2017). Our analysis
focusesonthepracticallyrelevantregimewhereregressorsaretrainedwithlittletonoregularization.
1.1 RELATEDWORK
Random feature models. RF models perform regression on a random subset or projection of a
high-(orinfinite-)dimensionalfeaturerepresentation. Originallyintroducedasascalableapprox-
imation to kernel machines (Rahimi & Recht, 2007; 2008a;b), RF regressors have seen growing
theoretical interest as simplified models of neural networks (e.g. Belkin et al., 2019; Jacot et al.,
2018; Bartlett et al., 2020; Mei & Montanari, 2022; Simon et al., 2024). This approximation of
neuralnetworksbecomesexactinthelimitofinfinitewidth(e.g.Jacotetal.,2018;Leeetal.,2019).
Underparameterizedrandomfeaturemodelsandensembles. Therearemanyworkstheoreti-
callycharacterizingensemblesoftree-basedmodels(e.g.Schapire&Singer,1998;Sexton&Laake,
2009;Wageretal.,2014;Mentch&Hooker,2016). Here,werestrictourdiscussiontoanalysesof
(ensemblesof)RFregressors.Mostworksofthisnatureanalyzeunderparameterizedmodels,where
thenumberofrandomfeatures(i.e.,thewidth)isassumedtobefarfewerthanthenumberofdata
points. In the underparameterized fixed design setting, the infinite ensemble of unregularized RF
regressors achieves the same generalization error as ridge regression on the original (unprojected)
inputs (Kaba´n, 2014; Thanei et al., 2017; Bach, 2024a). We emphasize the distinction between
underparameterized component models and their aggregated prediction: i.e., the ensemble of un-
regularized regressors is equivalent to a regularized predictor. (We provide theoretical analysis in
Appx.Dthatfurtherdemonstratesridge-likebehaviourofunderpameterizedRFensembles.)
Overparameterizedrandomfeaturemodels. RecentworksonRFmodelshavefocusedonthe
overparameterizedregime,oftenusinghigh-dimensionalasymptoticstocharacterizegeneralization
error(Adlam&Pennington,2020;Hastieetal.,2022;Mei&Montanari,2022;Bach,2024a). Im-
plicitinmanyworksisanassumptionofGaussianuniversality,inwhichthemarginaldistributions
overtherandomfeaturesarereplacedbymoment-matchedGaussians. Whilesuchassumptionsare
2Preprint
1.5 1.5
RFModels EnsembleModel
KernelModel KernelModel
1.0 TrainingData 1.0 TrainingData
0.5 0.5
0.0 0.0
0.5
0.5
1.0
4 2 0 2 4 4 2 0 2 4
Figure 1: An infinite ensemble of overparameterized RF models is equivalent to a single
infinite-width RF model. (Left) We show a sample of 100 finite-width RF models (blue) with
ReLUactivationstrainedonthesameN =6datapoints. Additionally,weshowthesingleinfinite-
width RF model (pink). The finite-width predictions concentrate around the infinite-width model.
(Right) We again show the single infinite-width RF model (pink) and the “infinite” ensemble of
M =10,000RFmodels(blue). Wenotenoperceptibledifferencebetweenthetwo.
commonthroughoutasymptoticrandommatrixtheory(e.g.Tao,2012),ourworkaimstoestablish
more general results that hold for more general random features. We demonstrate—both theoreti-
callyandempirically—thatGaussianitymaybeaninappropriateapproximationforneuralnetwork
featureswhencomparingthepointwisebehaviourofensemblesversussinglemodels. Mostrelated
toourworkisJacotetal.(2020),whocharacterizethepointwiseexpectationandvarianceofridge-
regularizedRFmodelswherethefeaturesaredrawsfromGaussianprocesses(GP).Ourresultsfor
ensembles similarly characterize the pointwise expectation and variance of overparameterized RF
modelsandensembles;however,wesignificantlyweakentheassumptionsontherandomfeatures.
1.2 CONTRIBUTIONS
We consider ensembles of overparameterized RF regressors in both the ridgeless and small ridge
regimes. Unlike prior work, we make minimal assumptions about the distribution of the random
features.Therefore,ourresultscanbeassumedtoholdformostRFensemblesratherthanonlythose
thatarecompositionsofGP-randomfeatures. Concretely,wemakethefollowingcontributions:
To answer Question 1: we show that the average ridgeless RF regressor is pointwise equivalent
to its corresponding ridgeless kernel regressor (Theorem 1), implying that an infinite ensemble of
overparameterizedRFmodelsisexactlyequivalenttoasingleinfinite-widthRFmodel. Wefurther
showthatthisequivalenceapproximatelyholdsinthesmallridgeregime(Theorem2).
To answer Question 2: we use rates established in prior work to demonstrate that the variance
reductionfromensemblingoverparameterizedRFregressorsisverysimilartoincreasingthenumber
of features in a single model. This shows that ensembles do not offer additional generalization or
robustnessadvantagesoversinglemodelsunderfixedparameterbudgets(seeSec.3.2).
To answer Question 3: we show that the predictive variance in an overparameterized ensemble is
theexpectedsquareddifferencebetweenthepredictionsfroma(finite-width)RFregressorandits
corresponding kernel regressor (i.e., the infinite-width model). With this finding, we demonstrate
that ensemble variance differs from conventional uncertainty quantifications, except in practically
unrealisticcaseswheretherandomfeaturesaresampledfromaGP(seeSec.3.2).
Altogether, these results support recent empirical findings that deep ensembles offer few general-
izationanduncertaintyquantificationbenefitsoversinglemodels(Abeetal.,2022;Theisenetal.,
2024). Our theory and experiments demonstrate that these phenomena are not specific to neural
networksbutaremoregeneralpropertiesofensemblesintheoverparameterizedregime.
3Preprint
2 SETUP
Weworkinaregressionsettingwithafixedtrainingdataset = (x ,y ) N ( R)N ofsize
N. Thevectory RN representstheconcatenationofalltrD aining{ resi poi ns} ei s= .1 ∈ X ×
∈
Weconsiderrandomfeature(RF)modelsoftheform:
h W(x)= √1
D
(cid:80)D j=1ϕ(ω j,x)θ j,
whereθ arelearnedparameters, = ω D ΩD arei.i.d. samplesfromadistributionπ(),
and ϕ :j Ω R is the feaW ture ex{ traj c} tj i= o1 n∈ function. For a ReLU-based RF model with· p-
dimensional× inX put→ s, we have = Ω = Rp and ϕ(ω ,x) = max(0,ω⊤x). Although RF models
X j j
cannotfullyexplainthebehaviorofneuralnetworks(e.g.Ghorbanietal.,2019;Lietal.,2021;Pleiss
&Cunningham,2021),theyserveasusefulproxiesforstudyingtheeffectsofoverparameterization
andmodelcapacityongeneralization(e.g.Belkinetal.,2019;Adlam&Pennington,2020;Mallinar
etal.,2022).
Notation. For any x,x′ , let k(x,x′) = E [ϕ(ω,x)ϕ(ω,x′)] denote the second moment of
ω
∈ X
the feature extraction function ϕ(ω, ). We note that the function k is a positive definite kernel
function,andwewillrefertoitassuc· h. WewillusethematricesK := [k(x ,x )] RN×N and
i j ij
Φ :=[ϕ(ω ,x )]
RN×Dtodenotethekernelfunctionappliedtoalltrainingdata∈
pairsandthe
W j i ij
∈
featureextractionfunctionappliedtoalldata/featurecombinations, respectively. Wewilldropthe
subscript whenthesetofrandomfeaturesisclearfromcontext. WeassumethatK isinvertible.
W
Throughoutouranalysis,itwillbeusefultoconsiderthe“whitened”featurematrixW =R−⊤Φ
RN×D whereR⊤R=K istheCholeskydecompositionofthekernelmatrixK. Whenconsiderin∈ g
atestpointx∗ (orequivalentlyasetoftestpoints),weextendtheK,R,Φ,W notationby
∈X
(cid:20)
K [k(x ,x∗)]
(cid:21) (cid:20)
R c
(cid:21)⊤(cid:20)
R c
(cid:21) (cid:20)
W
(cid:21) (cid:20)
R c
(cid:21)−⊤(cid:20)
Φ
(cid:21)
i i = , = . (1)
[k(x∗,x )] k(x∗,x∗) 0 r 0 r w⊤ 0 r [ϕ(ω ,x∗)]
j j ⊥ ⊥ ⊥ ⊥ j j
For fixed training/test points, E [WW⊤] = D I, E [w⊤w ] = D and E [w⊤W⊤] = 0
whichcanbedirectlyderivedfrW omE [ΦΦ⊤] =· D Kw⊥ (and⊥ sim⊥ ilarpropertiesW fo,w r⊥ ϕ∗)⊥ . Moreover,
Φ
·
thecolumns[w ;w ]of[W;w ]arei.i.d. sincetheyaretransformationsofthei.i.d. columnsofΦ.
i ⊥i ⊥
Overparameterizedridge/ridgelessregressorsandensembles. Asourfocusistheoverparame-
terizedregime,weassumeacomputationalbudgetofD >N features( = ω ,...,ω πD)
1 D
toconstructanRFregressorh W(x) = √1 ϕ W(x)⊤θ. WetraintheregrW essor{ parametersθ} t∼ omin-
D
imize the loss ∥√1 DΦ Wθ
−
y ∥2
2
+ λ ∥θ ∥2
2
for some ridge parameter λ
≥
0. When λ > 0, this
optimization problem admits the closed-form solution θ W(R ,R λ) = √1 DΦ⊤ W(cid:0) D1 ·Φ WΦ⊤
W
+λI(cid:1)−1 y.
Althoughthelearningproblemisunderspecifiedwhenλ=0(i.e.,intheridgelesscase),theimplicit
biasof(stochastic)gradientdescentinitializedatzeroleadstotheminimumnorminterpolatingso-
lutionθ(LN) = √1 (Φ)⊤(cid:0)1 ΦΦ⊤(cid:1)−1 y.Wedenotetheresultingridge(less)regressorsas
W D D ·
h( WLN)( ·):= √1
D
[ϕ(ω j, ·)] jθ W(LN), h( WR ,R λ)( ·):= √1
D
[ϕ(ω j, ·)] jθ W(R ,R λ).
We also consider ensembles of M ridge(less) regressors. We assume that each is trained on the
training same dataset but using different sets of i.i.d. random features ,..., πD, with
1 M
W W ∼
D >N. Thus,theonlysourceofrandomnessintheseensemblescomesfromtherandomselection
offeatures ,analogoustothestandardtrainingprocedureofdeepensembles(Lakshminarayanan
i
W
etal.,2017). Theensemblepredictionisgivenbythearithmeticaverageoftheindividualmodels
h¯ ()= 1 (cid:80)M h ()= 1 (cid:80)M (cid:104) 1 [ϕ(ω , )] Φ⊤ (cid:0)1 Φ Φ⊤ +λI(cid:1)−1(cid:105) y.
W1:M · M m=1 Wm,λ · M m=1 D mj · j Wm D · Wm Wm
Assumptions. A key difference between this paper and prior literature is the set of assumptions
about the random feature distribution π(). It is commonly assumed that entries in the extended
·
whitenedfeaturematrix[W;w ]arei.i.d. drawsfromazero-meansub-Gaussiandistribution(e.g.
⊥
4Preprint
ReLU Features Gaussian Error Function Features
12000 300
Mean:0.00 Mean:0.00
10000 250
8000 200
6000 150
4000 100
2000 50
0 0
0.08 0.06 0.04 0.02 0.00 0.02 0.04 0.06 0.08 0.20 0.15 0.10 0.05 0.00 0.05 0.10 0.15
[w W (WW ) 1]1 (whitened finite model residual) [w W (WW ) 1]1 (whitened finite model residual)
Figure 2: Empirically, the term E[w⊤W⊤(WW⊤)−1] is consistently zero. We plot the distri-
⊥
butionofthefirstindexofw⊤W⊤(WW⊤)−1,whichcapturesthedifferencebetweentheinfinite-
⊥
widthsinglemodelandasmalleroverparameterizedRFmodel(seeEq.(2)). (Left)WeuseReLU
asactivationfunction,x R,andN =6,D =200. (Right)WeusetheGaussianErroractivation
i
∈
function,theCaliforniaHousingdataset(KelleyPace&Barry,1997),andN =12,D =200.
Bartlett et al., 2020; Bach, 2024b), which implicitly places constraints on ϕ(, ) and π(). Many
· · ·
worksfurtherassumeGaussianuniversality—i.e. thatthedistributionofW,w canbemodeledby
⊥
i.i.d. standard Gaussian random variables (Adlam & Pennington, 2020; Mei & Montanari, 2022;
Simonetal.,2024)—implyingthatϕ(ω , )aredrawsfromaGaussianprocesswithcovariancek.1
i
·
Wearguethisassumptionisunrealisticwhenconsideringfeaturesthatresemblethosefromneural
networks. For example, ReLU features are always non-negative; thus, the mean of W = R−⊤Φ
is almost surely non-zero. Moreover, if Rp with p < N, then feature extraction functions
of the form σ(ω⊤x) are fully specified bX y a⊆ p-dimensional random variable. Thus, knowing N
evaluationsofω⊤x allowsonetoinferω ,makingw deterministicgivenW. Weinsteadconsider
j i j ⊥
thefollowinglessrestrictiveassumptionsaboutW,w ,whichimplicitlyspecifypropertiesofπ():
⊥
·
Assumption1(Assumptionofsubexponentiality). Wehavethat
1. w w (wherew istheithcolumnofW)issub-exponential i 1,...,D and
i ⊥i i
∀ ∈{ }
2. (cid:80)D w w⊤isalmostsurelypositivedefiniteforanyD N.
i=1 i i ≥
Thefirstconditionisinparticularfulfilledwheneverw andw⊤ aresub-Gaussianbutpotentially
⊥i i
dependent, a condition that is true for bounded activation functions or piecewise linear activation
functions with standard normal weights. The second condition is equivalent to Φ having almost
surely full rank. We note that the second condition will be violated by ReLUs and leaky-ReLUs;
however,itholdsforarbitrarilypreciseapproximations.2 Notethatwemakenoassumptionsabout
themean,independence,oreventheuncorrelationofentriesinagivencolumnof[W,w ].
⊥
3 MAIN RESULTS
3.1 EQUIVALENCEOFINFINITEENSEMBLESANDTHEINFINITE-WIDTHSINGLEMODELS
Weatfirstassumeaninfinitecomputationalbudgetandconsiderthefollowingtwolimitingpredic-
tors,forwhichwewillshowpointwiseequivalenceinpredictions:
1. Aninfinite-widthleastnormpredictor,h(LN),thea.s. limitofh(LN)as =D
∞ W |W| →∞
2. Aninfiniteensembleoffinite-widthleastnormpredictors,h¯(LN),whichisthealmostsure
∞
limitofh¯(LN) asM ,withN <D < remainingconstant.
W1:M →∞ ∞
1AssumetheentriesofW andw arei.i.d. Gaussian. Thentheith featureappliedtotraining/testinputs
⊥
([R⊤w ;c⊤w +r w ])ismultivariateGaussian. Thisfactholdsforanytrain/testdata;thus,theithfeature
i i ⊥ ⊥i
isaGPbydefinition(e.g.Rasmussen&Williams,2006,Ch.2).
2E.g.,ϕ (ω,x)= 1 log(1+eαω⊤x),α>0yieldsana.s.full-rankΦ,andϕ (ω,x)α→→∞ReLU(ω⊤x).
α α α
5
ycneuqerF ycneuqerFPreprint
ReLU Features Softplus Features
1.75
1.50
0.3
1.25
1.00
0.2 N=D N=D
0.75
0.1 0.50
0.25
0.0 0.00
0 10 20 30 40 50 0 10 20 30 40 50
Number of features per model Number of features per model
Figure3: Infiniteoverparameterizedensemblesareequivalenttoasingleinfinite-widthmodel
regardless of width, while underparameterized ensembles behave fundamentally differently.
We present the average absolute difference between the infinite ensemble and the single infinite
modelfordifferentfeaturecountsD. (Left)ReLUactivations,N = 6,dataarefromthesettingin
Fig. 1. (Right) softplus activations, N = 12, California Housing dataset. Both exhibit a “hockey
stick” pattern: there is a substantial difference between the underparameterized ensemble and the
infinite-widthmodel;however,thisdifferencevanishesintheoverparameterizedregime.
These limiting predictors do not only serve as approximations to large ensembles and very large
single models but will also prove useful in characterizing the variance and generalization error of
finiteoverparameterizedensembles,asdiscussedinSec.3.2.
Define k () : RN as the vector of kernel evaluations with the training data k () =
N N
· X → ·
[k(x , ) k(x , )]⊤ RN. AsD ,theminimumnorminterpolatingmodelconverges
1 N
· ··· · ∈ →∞
pointwisealmostsurelytotheridgelesskernelregressorbytheStrongLawofLargeNumbers:
h(LN)() a.s. h(LN)(), h(LN)():=k ()⊤K−1y.
W · −→ ∞ · ∞ · N ·
On the other hand, using W and w as introduced in Sec. 2 we can rewrite the infinite ensemble
⊥
predictionh¯(LN)(x∗)as(foraderivationofthis,seeAppx.B.1)
∞
h¯(LN)(x∗)=h(LN)(x∗) + r E (cid:104) w ⊤W⊤(cid:0) WW⊤(cid:1)−1(cid:105) R−⊤y (2)
∞ ∞ ⊥ W,w⊥ ⊥
Toprovethepointwiseequivalenceoftheinfiniteensembleandinfinite-widthsinglemodel,weneed
toshowthatE [w⊤W⊤(WW⊤)−1]terminEq.(2)iszero. Notethatthisresulttriviallyholds
W,w⊥ ⊥
whentheentriesofW andw arei.i.d.,asassumedinpriorwork(e.g.Jacotetal.,2020). Here,we
⊥
showthatthistermiszeroevenwhenw andW aredependent,which—asdescribedinSec.2—is
⊥
amorerealisticassumptionforneuralnetworkfeatures. Empirically,inFig.2weobservethatthe
entriesoftherandomvariablew⊤W⊤(WW⊤)−1 RN haveameanofzeroforbothReLUand
⊥ ∈
Gaussianerrorfunctionfeatures,bothofwhichviolateindependenceassumptionsbetweenw and
⊥
W (asnotedinSec.2). Weformalizethisobservationinthefollowinglemma:
Lemma1. UnderAssumption1,itholdsthatE [w⊤W⊤(WW⊤)−1]=0.
W,w⊥ ⊥
Proofsketch. (See Appx. B.1 for a full proof.) We start by applying the Woodbury formula to
expressthematrixinverse(WW⊤)−1 asadecompositioninvolvingthematrixA = (WW⊤
−i
w w⊤)andtheindividualcolumnw ofW. Thisdecompositionyieldstheexpression: −
i i i
w⊤W⊤(WW⊤)−1 =(cid:80)D (w w⊤)/(1+w⊤A−1w )A
⊥ i=1 ⊥i i i −i i −i
Next,usingsub-exponentialconcentrationinequalitiesinconjunctionwiththeWeakLawofLarge
Numbers, we show that the conditional expectation E (cid:2) (w w⊤)/(1+w⊤A−1w ) A (cid:3)
w⊥i,wi ⊥i i i −i i | −i
exists and is zero for all invertible A . This result implies that: E[w W⊤(WW⊤)−1] =
−i ⊥
(cid:80)D E [E [(w w⊤)/(1+w⊤A−1w )]A−1]=0.
i=1 A−i w⊥i,wi ⊥i i i −i i −i
CombiningLemma1andEq.(2)yieldsthepointwiseequivalenceofh¯(LN)andh(LN):
∞ ∞
6
1L)NL(h
)NL(h
1L)NL(h
)NL(hPreprint
Ensemble Variance GP Variance
0.016
0.014 V Tra ar iπ nD i[ nh g( SL DN a)( t· a)] Points 0.012
0.012 0.010
0.010 0.008
r2
0.008 0.006 T⊥rainingDataPoints
0.006
0.004
0.004
0.002 0.002
0.000 0.000
4 2 0 2 4 4 2 0 2 4
Figure4:Ensemblevariance(left)andBayesiannotionsofuncertainty(right)candiffersignif-
icantly. ForN =6andD =200withReLUactivations,weshowtheoverparameterizedensemble
variance(left)andtheposteriorvarianceofaGaussianprocesswithpriorcovariancek(, )(right)
· ·
acrosstheinputrange. Empirically,weobservesubstantialdifferencesbetweenthetwoquantities.
Theorem 1 (Equivalence of infinite-width single model and infinite ensembles). Under Assump-
tion1,theinfiniteensembleoffinite-width(butoverparameterized)RFregressorsh¯(LN)ispointwise
∞
almostsurelyequivalenttothe(single)infinite-widthRFregressorh(LN).
∞
This result shows that, in the case of overparameterized RF regression, ensembling yields exactly
thesamepredictionsaswhatcouldbeachievedbysimplyincreasingthecapacityofasinglemodel
(see Fig. 1 for a visualization). As a result, we should not expect substantial differences in gener-
alizationbetweenlargesinglemodelsandoverparameterizedensembles. Weemphasizeacontrast
withtheunderparameterizedregime,whereRFensemblesmatchthegeneralizationerrorofkernel
ridgeregression(seeAppx.DorBach,2024a,Sec.10.2.2). Whilewidthcontrolstheimplicitridge
parameterintheunderparameterizedregime(seeSec.1.1),widthdoesnotaffecttheensemblepre-
dictorintheoverparameterizedregime. WeconfirmthisdifferenceinFig.3whichshowsthatRF
ensemblesareequivalenttotheridgelesskernelregressorwhenD >N butnotwhenD <N.
3.2 VARIANCEOFENSEMBLEPREDICTIONS
We now analyze the predictive variance amongst component models in an overparameterized RF
ensemble,aquantityusedtoquantifypredictiveuncertaintyandprovideinsightsaboutthegeneral-
izationerror. UsingTheorem1,thevarianceofthepredictionsofasingleRFmodelwithrespectto
itsrandomfeaturescanbeexpressedas(seeAppx.B.2foraderivation)
Var [h(LN)(x∗)]=r2 (cid:0) y⊤R−1E [(WW⊤)−TWw w⊤W⊤(WW⊤)−1]R−⊤y(cid:1) . (3)
W W ⊥ W,w⊥ ⊥ ⊥
InthespecialcasewhereW andw arei.i.d. standardnormal,thisexpressionsimplifiesto
⊥
Var W[h( WLN)(x∗)]=r ⊥2 (cid:16) ∥ Dh −( ∞L NN) −∥ 12 k(cid:17) , (4)
where h(LN) 2 represents the squared norm of h(LN) in the RKHS defined by the kernel k(, ).
∥ ∞ ∥K ∞ · ·
Fromthisequation,wenotethatthevariancedecreaseswithRFregressorwidthas 1/D,scales
∼
with the complexity of h(LN), and only depends on x∗ through the quantity r2 (a term we will
∞ ⊥
analyzelater).
Unfortunately, E [(WW⊤)−TWw w⊤W⊤(WW⊤)−1] generally does not have simple ex-
W,w⊥ ⊥ ⊥
pressionforarbitraryW,w satisfyingAssumption1. WithoutassumingGaussianuniversality,the
⊥
variancedependsonx∗ throughbothr2 aswellasthroughtheexpectationfromEq.(3)involving
⊥
w . Still,priorworksandempiricalresults(seeFig.5andAppx.A.3)suggestthatthevarianceof
⊥
RFmodelsdecayswith 1/Dunderavarietyofdistributions(e.g.Adlam&Pennington,2020).
∼
Implicationsforuncertaintyquantification. Acommonapproachtouncertaintyquantification
with ensembles is to examine the predictive variance of their members at a specific test point x∗
7Preprint
100 2.2 Singlemodel
Kernelmodel
2.1 Ensemble
101
2.0
102 1.9
Point1 1.8
103 Point2
Point3 1.7
104 P Po oi in nt t4 5 1.6
103 0 1000 2000 3000 4000 5000 6000 7000
Number of features of random feature models Total number of features used
Figure 5: The variance and generalization error of overparameterized ensembles and single
largemodelsscalesimilarlywiththetotalnumberoffeatures. (Left)Weshowthatthevariance
ofasinglemodelwithMD featuresdecaysas 1 ,consistentwiththescalingbehaviorofthe
∼ MD
varianceinanensemble. (Right)WepresentthegeneralizationerrorofanensembleofM models,
each with D = 200 features, compared to a single model with MD total features. Both exhibit
nearlyidenticaldependenceonthetotalfeaturebudget. TheresultsuseaReLUactivationfunction,
theCaliforniaHousingdataset,andN =12.
(Lakshminarayanan et al., 2017). Before diving into an analysis of Eqs. (3) and (4), it is worth
reflectingontheimplicationsthatTheorem1hasforensemblevarianceasuncertaintyquantification.
BecausetheexpectedoverparameterizedRFmodelistheinfinite-widthRFmodel,wecanexactly
characterizetheensemblevarianceastheexpectedsquareddifferencebetweenthepredictionsofa
large (i.e., infinite-width) model versus a smaller (finite-width but still overparameterized) model.
This reveals that ensemble variance provides a non-standard notion of uncertainty, differing from
bothconventionalfrequentistandBayesianinterpretations.
AnotableexceptioniswhenW andw arei.i.d.standardnormal.RecallbyEq.(4)thatthevariance
⊥
undertheGaussianuniversalityassumptiononlydependsonx∗throughthequantityr2.ByEq.(1)
⊥
weseethatr2 isequaltok(x∗,x∗) k (x∗)⊤K−1k (x∗),whichisexactlytheGaussianprocess
⊥ − N N
posterior variance with prior covariance k(, ) (e.g. Rasmussen & Williams, 2006). In this case,
· ·
ensemblesprovideascaledversionofaclassicBayesianestimateofuncertainty.
However,relaxingfromGaussianitytoAssumption1makestherelationshipbetweenensemblevari-
anceandr2 morecomplex, astheindependencebetweenW andw isnolongerguaranteed. As
⊥ ⊥
can be seen in Eq. (3), the variance generally depends on x∗ through both r2 and a complicated
⊥
expectation involving W and w . In Appx. B.2 we demonstrate with a simple example that this
⊥
expectationcanindeeddependonx∗,implyingthatensemblevariancedoesnotexactlycorrespond
to a scalar multiple of r2. In our numerical experiments using realistic (i.e., non-Gaussian) ran-
⊥
dom feature distributions, (Fig. 4 and Appx. A.3), we observe significant deviations between the
ensemble variance and the Gaussian process posterior variance, further implying that one cannot
view ensembles through a classic framework of uncertainty. These discrepancies are particularly
importantforuncertaintyestimationinsafety-criticalapplicationsoractivelearning(e.g.Galetal.,
2017;Beluchetal.,2018).
Ensemblesversuslargersinglemodelsunderafinitefeaturebudget. Ourcharacterizationof
ensemble variance also holds implications for the generalization error of ensembles versus single
modelsunderafinitecomputationalbudget. WecompareensemblesofM modelswithDfeatures
each(h¯(LN) = 1 (cid:80)M h(LN))tosinglemodelswithMDfeaturesh(LN)()(i.e.,here =D
W1:M M m=1 Wm W∗ · |Wm |
forall m and ∗ = MD). Theexpected generalizationerrorof either predictorcanbe decom-
|W |
posedintostandardbiasandvarianceterms:
E [L(h)]:=E [E [(h(x) E[y x])2]=L(E [h])+E [Var (h(x))].
h h x h x h
− |
Since h(LN) and h(LN),...,h(LN) share the same expected predictor (as established in Theo-
W∗ W1 WM
rem 1), the only difference in the generalization of h(LN) and h¯(LN) arises from their vari-
W∗ W1:M
ances. Duetotheindependencebetweenensemblemembers,wehavethatVar [h¯(LN)(x)] =
W1:M W1:M
8
])()NL
S(h[DraV
rorre
noitazilareneGPreprint
Ridge/ridgeless Ensemble Diff. Ridge/ridgeless Single Model Diff.
2
1
1
0
0
1 1
2
2
0.0000 0.0002 0.0004 0.0006 0.0008 0.0010 0.0000 0.0002 0.0004 0.0006 0.0008 0.0010
Figure6:Lipschitzcontinuityofpredictionsforaninfiniteensembleandkernelregressorwith
respecttotheridgeparameter. (Left)Weplotthe h¯(RR)(x∗) h¯(LS)(x∗) asafunctionofλfor
| ∞,λ − ∞ |
500testpoints. (Right)Weshowtheevolutionof h(RR)(x∗) h(LS)(x∗) forthesametestpoints.
| ∞,λ − ∞ |
BothplotsusetheReLUactivationfunctionandtheCaliforniaHousingDatasetwithN = 12and
D =200.Whilethedirectdifference h¯(RR)(x∗) h(RR)(x∗) isnotshownduetoreasonsoutlined
| ∞,λ − ∞,λ |
inAppx.A.4,itcanbeboundedbyasumoftheshowndifferences(seeAppx.C.1).
1 Var [h(LN)(x)]. Moreover, sincethevarianceofasingleRFmodelisinverselyproportional
M Wm Wm
tothenumberoffeatures(exactlyinthecaseofGaussianfeaturesandapproximatelyinthegeneral
case, as discussed above), we have that Var [h(LN)(x)]/Var [h(LN)] 1/M. Altogether,
W∗ W∗ Wm Wm ≍
thissuggeststhatthegeneralizationerroroffiniteensemblesandfinite-widthsinglemodelsdecay
atsimilarrates. WeconfirmthissimilarrateofdecayinFig.5andAppx.A.3,whichcompareen-
semblesversussinglemodelsundervariousfeaturebudgets. Theseresultsfurtherdemonstratethat
ensemblesdonotprovideanymeaningfulgeneralizationadvantageoversinglemodels.
3.3 EQUIVALENCEOFTHELIMITINGPREDICTORSINTHESMALLRIDGEREGIME
Havingestablishedthepointwiseequivalencebetweeninfiniteensemblesandinfinite-widthsingle
models in the ridgeless regime, we now extend our analysis to the case where a small ridge pa-
rameter λ > 0 is introduced. While h(RR), the infinite-width limit of h(RR) as = D ,
∞,λ W,λ |W| → ∞
almostsurelyconvergestothekernelridgeregressorwithridgeλ, theinfiniteensembleh¯(RR) :=
∞,λ
E [h(RR)(x)]doesnotgenerallymaintainpointwiseequivalencewithh(RR). Thisdivergenceoc-
W W,λ ∞,λ
cursevenundertheGaussianuniversalityassumption(Jacotetal.,2020). However,wehypothesize
thatthedifferencebetweentheselimitingpredictorsissmallwhenλisclosetozero,whichiscom-
moninpracticalapplications. Toanalyzethisregime,weintroduceaminoradditionalassumption:
Assumption2. WeassumethatE [(cid:0) Φ Φ⊤(cid:1)−1 ]isfiniteforall =D >N.
W W W |W|
UnderAssumptions1and2,weshowthatthedifferenceisLipschitz-continuouswithrespecttoλ:
Theorem 2 (The difference between ensembles and large single models is smooth with respect
to λ.). Under Assumptions 1 and 2, the difference h¯(RR)(x∗) h(RR)(x∗) between the infinite
| ∞,λ − ∞,λ |
ensemble and the single infinite-width model trained with ridge λ is Lipschitz-continuous in λ for
λ 0. TheLipschitzconstantisindependentofx∗forcompact .
≥ X
Proofsketch. We first prove a lemma that shows the predictions of infinite-width RF regressors
h(RR)(x∗)areLipschitz-continuousinλ(seeLemma3). Usingasimilarproofstrategyandnoting
∞,λ
thatanequivalentstatementtoLemma1holdsintheridgeregime,wealsoprovethatthepredictions
ofinfiniteensemblesh¯(RR)(x∗)areLipschitz-continuousinλ(seeLemma4). InFig.6,weshow
∞,λ
howthedifferencesbetweenthesepredictionsandtheirridgelesscounterpartsevolvewithrespect
toλforvarioustestpoints. Combiningthesetworesultsandusingatriangleinequalityyieldsour
theorem. Forthefullproof,seeAppx.C.1.
9
|)()SL(h
)()R
,R(h|
|)()SL(h
)()RR
,(h|Preprint
SinceTheorem1ensuresthat h¯(RR)(x∗) h(RR)(x∗) = 0forλ = 0, wecanconcludethatthe
| ∞,λ − ∞,λ |
pointwisedifferencegrowsatmostlinearlywithλ. Specifically,wehavethefollowingbound
(cid:12) (cid:12)
(cid:12)h¯(RR) h(RR)(x)(cid:12) C λ,
(cid:12) ∞,λ − ∞,λ (cid:12) ≤ ·
forsomeconstantC independentofx∗,providedthat iscompact. Inpracticalterms,thisresult
X
indicatesthatforsufficientlysmallvaluesofλ,thepredictionsoflargeensemblesandlargesingle
modelsremainnearlyindistinguishable,reinforcingourfindingsfromtheridgelessregime.
4 CONCLUSION
ForQuestion1,wedemonstratedthatunderweakconditions,infiniteensembles,andsingleinfinite-
width models are pointwise equivalent in the ridgeless regime and nearly identical with a small
ridge, significantly expanding on prior results (e.g. Jacot et al., 2020). These results verify recent
empirical findings (e.g. Abe et al., 2022) that much of the benefit attributed to overparameterized
ensembles,suchasimprovedpredictiveperformanceandrobustness,canbeexplainedbytheirsim-
ilaritytolargersinglemodels. Wecontrastthesefindingstotheunderparameterizedregime,where
ensemblingtypicallyinducesregularizationandimprovesgeneralization. Similarly,forQuestion2,
we argued that the variance reduction from ensembling is asymptotically equivalent to increasing
thenumberoffeaturesofasinglemodel. ThisresultfurtherstrengthensourfindingsonQuestion1
anddemonstratesfunctionalsimilaritiesunderrelativelysmallcomputationalbudgets.ForQuestion
3, we found that the ensemble variance measures the expected difference to a single larger model
and is thus a non-standard measure for uncertainty. Significant deviations from the Gaussian pro-
cessposteriorvarianceindicatethatcautionisneededwhenusingensemblevarianceforuncertainty
quantification,especiallyinsafety-criticalsettings. Again,theseresultsreinforceempiricalfindings
from(Abeetal.,2022)aboutoverparameterizedneuralnetworkensembles.
Overall,whileourresultsdonotcontradicttheutilityofoverparameterizedensembles,theysuggest
thattheirbenefitsmayoftenbeexplainedbytheirsimilaritytolargermodelsandthatfurtherresearch
isneededtoimproveuncertaintyquantificationmethods.
Limitations. Thepracticalimplicationsofourworkarelimitedbythetheoreticalabstractionswe
employ. While these abstractions provide valuable theoretical insights, they may not always hold
inreal-world,finitesettings. Mostnotably,weapproximateneuralnetworksusingRFmodelsand
focusoninfinitesinglemodelsandinfiniteensemblesasapproximationsforlargemodelsandlarge
ensembles. Nevertheless, we emphasize that our theoretical results on RF regressors align with
recent empirical observations on deep ensembles (Abe et al., 2022; Theisen et al., 2024), further
supporting the growing body of work that uses RF models to provide insights into deep learning
phenomena(e.g.Belkinetal.,2019;Hastieetal.,2022;Simonetal.,2024).
Inadditiontothesetheoreticalassumptions,ourempiricalresultsareconstrainedintermsofscale
andcomplexity. Duetonumericalstabilityissues(seeAppx.A.2),weprimarilyconsideredasmall
numberofsamples,arelativelylargenumberofrandomfeatures,andsimpledata-generatingfunc-
tions.Again,wereferthereaderstotheafformentionedempiricalworkforlarger-scaleexperiments.
REPRODUCIBILITY AND ETHICS STATEMENTS
Reproducibility. Theprimarycontributionofthispaperisatheoreticalanalysistoexplainempir-
ical phenomena studied in the recent works of Abe et al. (2022; 2024); Theisen et al. (2024). All
proofsandderivationsarelargelyself-contained,eitherinthemaintextortheappendix. Wesupply
referencestoallbackgroundmaterialwhereapplicable.
Empiricalresultsarenotthemainfocusofthiswork. Nevertheless,weprovidethesimulationcode
usedtogenerateallfiguresinthetext,andacompletedescriptionoftheexperimentscanbefoundin
Appx.A.1.WealsoincludeadiscussiononthenumericalstabilityofourexperimentsinAppx.A.2.
Ethics. We believe there are no significant ethical concerns stemming from this work, as it is
largely a theoretical analysis of previous empirical results. However, we do note that this work
studiesensemblesofneuralnetworksandtheiruncertaintyestimates,whichhavethepotentialtobe
usedinsafety-criticalapplications(Lakshminarayananetal.,2017;Ovadiaetal.,2019).
10Preprint
ACKNOWLEDGEMENTS
Resourcesusedinthisresearchwereprovided,inpart,bytheProvinceofOntario,theGovernment
ofCanada, andcompaniessponsoringtheVectorInstitute. JPCissupportedbytheGatsbyChari-
tableFoundation(GAT3708), theSimonsFoundation(542963), theNSFAIInstituteforArtificial
andNaturalIntelligence(ARNI:NSFDBI2229929)andtheKavliFoundation. GPacknowledges
supportfromNSERCandtheCanadaCIFARAIChairprogram.
REFERENCES
Taiga Abe, E. Kelly Buchanan, Geoff Pleiss, Richard Zemel, and John P. Cunningham. Deep en-
sembleswork, butaretheynecessary? InAdvancesinNeuralInformationProcessingSystems,
2022.
Taiga Abe, E. Kelly Buchanan, Geoff Pleiss, and John P. Cunningham. Pathologies of predictive
diversityindeepensembles. TransactionsonMachineLearningResearch,2024.
Ben Adlam and Jeffrey Pennington. Understanding double descent requires a fine-grained bias-
variancedecomposition. InAdvancesinNeuralInformationProcessingSystems,2020.
F.Bach. LearningTheoryfromFirstPrinciples. MITPress,2024a.
FrancisBach. High-dimensionalanalysisofdoubledescentforlinearregressionwithrandompro-
jections. SIAMJournalonMathematicsofDataScience,6(1):26–50,2024b.
PeterL.Bartlett,PhilipM.Long,Ga´borLugosi,andAlexanderTsigler. Benignoverfittinginlinear
regression. ProceedingsoftheNationalAcademyofSciences,117(48):30063–30070,2020.
MikhailBelkin, SiyuanMa, andSoumikMandal. Tounderstanddeeplearningweneedtounder-
standkernellearning. InInternationalConferenceonMachineLearning,pp.541–549,2018.
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-
learningpracticeandtheclassicalbias–variancetrade-off. ProceedingsoftheNationalAcademy
ofSciences,116(32):15849–15854,2019.
William H. Beluch, Tim Genewein, Andreas Nurnberger, and Jan M. Kohler. The power of en-
semblesforactivelearninginimageclassification. InComputerVisionandPatternRecognition,
2018.
LeoBreiman. Baggingpredictors. MachineLearning,24(2):123–140,1996.
LeoBreiman. Randomforests. MachineLearning,45(1):5–32,2001.
Lin Chen, Michal Lukasik, Wittawat Jitkrittum, Chong You, and Sanjiv Kumar. On bias-variance
alignmentindeepmodels. InInternationalConferenceonLearningRepresentations,2024.
Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In International
ConferenceonKnowledgeDiscoveryandDataMining,pp.785–794,2016.
Youngmin Cho and Lawrence Saul. Kernel methods for deep learning. In Advances in Neural
InformationProcessingSystems,2009.
ThomasGDietterich. Ensemblemethodsinmachinelearning. InInternationalWorkshoponMul-
tipleClassifierSystems,pp.1–15.Springer,2000.
Stanislav Fort, Huiyi Hu, and Balaji Lakshminarayanan. Deep ensembles: A loss landscape per-
spective. arXivpreprintarXiv:1912.02757,2019.
YoavFreund. Boostingaweaklearningalgorithmbymajority. InformationandComputation,121
(2):256–285,1995.
YarinGal,RiashatIslam,andZoubinGhahramani. DeepBayesianactivelearningwithimagedata.
InInternationalConferenceonMachineLearning,pp.1183–1192,2017.
11Preprint
BehroozGhorbani, SongMei, TheodorMisiakiewicz, andAndreaMontanari. Limitationsoflazy
training of two-layers neural network. In Advances in Neural Information Processing Systems,
2019.
FredrikKGustafsson,MartinDanelljan,andThomasBSchon. Evaluatingscalablebayesiandeep
learningmethodsforrobustcomputervision. InComputerVisionandPatternRecognitionWork-
shops,pp.318–319,2020.
L.K.HansenandP.Salamon.Neuralnetworkensembles.PatternAnalysisandMachineIntelligence,
12(10):993–1001,1990.
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-
dimensionalridgelessleastsquaresinterpolation. AnnalsofStatistics,50(2):949,2022.
ArthurJacot,FranckGabriel,andCle´mentHongler. Neuraltangentkernel: Convergenceandgen-
eralizationinneuralnetworks. AdvancesinNeuralInformationProcessingSystems,2018.
Arthur Jacot, Berfin Simsek, Francesco Spadaro, Cle´ment Hongler, and Franck Gabriel. Implicit
regularization of random feature models. In International Conference on Machine Learning,
2020.
AlanJeffares, TennisonLiu, JonathanCrabbe´, andMihaelavanderSchaar. Jointtrainingofdeep
ensemblesfailsduetolearnercollusion. InAdvancesinNeuralInformationProcessingSystems,
2024.
Ata Kaba´n. New bounds on compressive linear least squares regression. In Artificial Intelligence
andStatistics,pp.448–456,2014.
R.KelleyPaceandRonaldBarry. Sparsespatialautoregressions. Statistics&ProbabilityLetters,
33(3):291–297,1997.
BalajiLakshminarayanan,AlexanderPritzel,andCharlesBlundell. Simpleandscalablepredictive
uncertainty estimation using deep ensembles. In Advances in Neural Information Processing
Systems,2017.
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
undergradientdescent. InAdvancesinNeuralInformationProcessingSystems,2019.
Stefan Lee, Senthil Purushwalkam, Michael Cogswell, David Crandall, and Dhruv Batra. Why
m heads are better than one: Training a diverse ensemble of deep networks. arXiv preprint
arXiv:1511.06314,2015.
MufanLi,MihaiNica,andDanRoy. Thefutureislog-Gaussian: ResNetsandtheirinfinite-depth-
and-widthlimitatinitialization. AdvancesinNeuralInformationProcessingSystems,34:7852–
7864,2021.
NeilMallinar,JamesB.Simon,AmirhesamAbedsoltan,ParthePandit,MikhailBelkin,andPreetum
Nakkiran. Benign,tempered,orcatastrophic: Ataxonomyofoverfitting. InAdvancesinNeural
InformationProcessingSystems,2022.
SongMeiandAndreaMontanari. Thegeneralizationerrorofrandomfeaturesregression: Precise
asymptoticsandthedoubledescentcurve. CommunicationsonPureandAppliedMathematics,
75(4):667–766,2022.
LucasMentchandGilesHooker. Quantifyinguncertaintyinrandomforestsviaconfidenceintervals
andhypothesistests. JournalofMachineLearningResearch,17(26):1–41,2016.
JeremyNixon,BalajiLakshminarayanan,andDustinTran. Whyarebootstrappeddeepensembles
notbetter? InNeurIPS“ICan’tBelieveIt’sNotBetter!” Workshop,2020.
David Opitz and Richard Maclin. Popular ensemble methods: An empirical study. Journal of
ArtificialIntelligenceResearch,11:169–198,1999.
12Preprint
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian Nowozin, Joshua
Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty?
evaluatingpredictiveuncertaintyunderdatasetshift. InAdvancesinNeuralInformationProcess-
ingSystems,2019.
Geoff Pleiss and John P. Cunningham. The limitations of large width in neural networks: A deep
Gaussian process perspective. Advances in Neural Information Processing Systems, 34:3349–
3363,2021.
AliRahimiandBenjaminRecht. Randomfeaturesforlarge-scalekernelmachines. InAdvancesin
NeuralInformationProcessingSystems,2007.
AliRahimiandBenjaminRecht. Uniformapproximationoffunctionswithrandombases. In46th
AnnualAllertonConferenceonCommunication,Control,andComputing,pp.555–561,2008a.
AliRahimiandBenjaminRecht. Weightedsumsofrandomkitchensinks: Replacingminimization
withrandomizationinlearning. InAdvancesinNeuralInformationProcessingSystems,2008b.
CarlEdwardRasmussenandChristopherK.I.Williams.GaussianProcessesforMachineLearning.
MITPress,2006.
AlessandroRudiandLorenzoRosasco. Generalizationpropertiesoflearningwithrandomfeatures.
AdvancesinNeuralInformationProcessingSystems,2017.
RobertE.Schapire. Thestrengthofweaklearnability. MachineLearning,5(2):197–227,1990.
RobertESchapireandYoramSinger. Improvedboostingalgorithmsusingconfidence-ratedpredic-
tions. InConferenceonComputationalLearningTheory,pp.80–91,1998.
JosephSextonandPetterLaake. Standarderrorsforbaggedandrandomforestestimators. Compu-
tationalStatistics&DataAnalysis,53(3):801–811,2009.
James B. Simon, Dhruva Karkada, Nikhil Ghosh, and Mikhail Belkin. More is better in modern
machinelearning: wheninfiniteoverparameterizationisoptimalandoverfittingisobligatory. In
InternationalConferenceonLearningRepresentations,2024.
TerenceTao. TopicsinRandomMatrixTheory. AmericanMathematicalSoc.,2012.
Gian-Andrea Thanei, Christina Heinze, and Nicolai Meinshausen. Random projections for large-
scaleregression. BigandComplexDataAnalysis: MethodologiesandApplications, pp.51–68,
2017.
RyanTheisen,HyunsukKim,YaoqingYang,LiamHodgkinson,andMichaelWMahoney. When
areensemblesreallyeffective? AdvancesinNeuralInformationProcessingSystems,36,2024.
Stefan Wager, Trevor Hastie, and Bradley Efron. Confidence intervals for random forests: The
jackknife and the infinitesimal jackknife. Journal of Machine Learning Research, 15(1):1625–
1651,2014.
MartinJWainwright. High-DimensionalStatistics: ANon-AsymptoticViewpoint. CambridgeUni-
versityPress,2019.
ChristopherK.I.Williams. Computingwithinfinitenetworks. InAdvancesinNeuralInformation
ProcessingSystems,1996.
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Y Zou, Sergey Levine, Chelsea
Finn,andTengyuMa. MOPO:Model-basedofflinepolicyoptimization. InAdvancesinNeural
InformationProcessingSystems,2020.
13Preprint
1.00 0.25 D Tra at ia niG ngen Der aa tt aingFunction
0.00
0.75
0.25
0.50
0.50
0.25 DataGeneratingFunction
TrainingData 0.75
0.00
1.00
0.25
1.25
0.50
1.50
0.75
1.75
4 2 0 2 4 4 2 0 2 4
Figure7: Truefunctionf(x) = sin(5 b⊤x)withdifferentrandomseeds. Thebluelineshows
·
thetruefunction,whilereddotsrepresenttrainingsamplesfortwodistinctrandomseeds.
Intheappendix,wewillprovidethefollowingadditionalresults:
1. InAppx.A,wewilldescribeourexperimentalsetupinmoredetail,difficultiesweencoun-
teredwhendevelopingtheexperiments,andprovidetheresultsofadditionalexperiments.
2. InAppx.BwewillgivetheproofsforSecs.3.1and3.2inthemainpaper.
3. InAppx.CwewillgivetheproofsforSec.3.3inthemainpaper.
4. Finally, in Appx. D, we prove (under mild assumptions) that infinite underparameterized
RFensemblesareequivalenttokernelridgeregressionundersometransformedkernel.
A EXPERIMENTAL SETUP AND ADDITIONAL RESULTS
The code to run all our experiments can be found on GitHub: https://github.com/
nic-dern/theoretical-limitations-overparameterized-ensembles. It con-
tainsaREADME.mdfilethatexplainshowtosetupandruntheexperiments.
A.1 EXPERIMENTALSETUP
Wehadtwosetupsusingwhichweperformedmostofourexperiments:
1. Wegeneratetrainingandtestpointsuniformlyatrandomfrom[ 5,5]d usingthefunction
f(x) = sin(5 b⊤x), where b is a vector (depending on the ra− ndom seed) and the noise
·
parameter is σ = 0.05 (we assume Gaussian noise with mean 0). In this setting, we use
N = 6,D = 200,anddatafromR(i.e.,d = 1)ifnotspecifiedotherwise. Youcanfinda
plotofanexampletruefunctioninFig.7.
2. We use the California Housing (Kelley Pace & Barry, 1997) dataset and sample distinct
trainingandtestpointsfromit(randomlypermutatingthedatasetinitially). Inthissetting,
weuseN = 12,D = 200ifnotdifferentlyspecified. ThedatadimensionisR8 here. In
contrasttothefirstsetting,weemployadatanormalizationusingamax-minnormalization
ontheentiredatasetsinceweexperimentallyfoundthismakesourmethodsmorestable.
WecalculatethegeneralizationerrorusingN =1000testpointsinbothsettings.Inthefirstsetting,
wecalculatethevarianceofthepredictionsofasinglemodelusingM = 20,000models,whilein
thesecondsetting,weuseM = 4,000models. ApartfromFig.2whereweuse100,000samples,
“infinite”ensemblesconsistofM =10,000models.
As distribution τ() of the elements ω we always use (0,I). As activation functions, we
i
use ReLU, the Ga· ussian error function,∈ anW d the softplus funcN tion 1 log(1+exp(β ω⊤x)) with
β · ·
β = 1. Forthefirsttwoactivationfunctions, thereexistanalyticallycalculatablelimitingkernels,
thearc-cosinekernel(Cho&Saul,2009)andtheerf-kernel(Williams,1996). Theclosedformsfor
theseare
1
k (x,x′)= x x′ (sinθ+(π θ)cosθ),
arc-cosine 2π∥ ∥∥ ∥ −
14Preprint
TrainingPointsaftertransformation
Linex0=1
Hyperplaneofω1
Hyperplaneofω2
x0
Figure8: Visualizationofhyperplanesseparatingtrainingpoints. Weillustratehowaseriesof
hyperplanescanseparateagrowingsubsetofthetrainingpoints, leadingtoatriangular, invertible
matrixstructureasasubsetofΦ.
(cid:16) (cid:17)
whereθ =cos−1 x⊤x′ and
∥x∥∥x′∥
(cid:32) (cid:33)
2 2x⊤x′
k (x,x′)= sin−1 .
erf π (cid:112) (1+2 x 2)(1+2 x′ 2)
∥ ∥ ∥ ∥
Forthesoftplusfunction,weapproximatethekernelbyestimatingthesecondmomentk(x,x′) =
E[ϕ(ω,x)ϕ(ω,x′) x,x′]ofthefeatureextractionusing107samplesfromτ().ForsamplingGaus-
| ·
sianfeatures(i.e., testingundertheassumptionGaussianuniversality), weusethesameapproach
asdescribedbyJacotetal.(2020).
Beforetrainingondata,wealwaysappenda1inthezeroeth-dimensionofthedatabeforecalculating
thedotproductwithω (correspondingly, thedimensionofω isd+1)andapplyingtheactivation
function. Intheridgelesscase,weuseλ=10−8toavoidnumericalissues.
A.2 NOTESONSTABILITY
Duringourexperiments,weencounteredchallengesrelatedtobothmathematicalstability(i.e.,ma-
tricesbeingtrulysingularratherthannearlysingular)andnumericalstability. Thissectionoutlines
theseissuesanddescribesthestepswetooktomitigatethem.
Mostimportantly,thematrixΦ Φ⊤ isnotalmostsurelyinvertiblewhenusingtheReLUactivation
W W
function, meaning that technically, the second condition of our Assumption 1 is not fulfilled. In
numericalexperiments,thisresultsincaseswhere(Φ Φ⊤)−1isnearlysingular(thoughstabilized
W W
withλ=10−8).
Ontheotherhand, whenD issufficientlylargerelativetoN, Φ isfullrankwithhighprobabil-
W
ity, which implies that Φ Φ⊤ is invertible with high probability. Given our data transformation
W W
of appending a 1 in the zeroeth dimension, one can see this as there exists a series of (non-zero
probability sets of) hyperplanes separating an increasing subset of the training points, leading to
a subset of Φ ’s columns that form a triangular, invertible matrix (see Fig. 8 for a visualization).
W
Intuitively,higherdatadimensionalityandbetterseparabilityofthepointsincreasetheprobability
ofΦ havingfullrank.
W
As an example of the discussed instabilities, see the adversarial scenario shown in Fig. 9, where
N = 15 and many training points are placed very close to each other. In this case, individual
RFregressorsexhibitrelativelyhighvarianceoutputvalues(duetonumericalinstabilities), which
arenotaveragedoutinthe“infinite”ensemble. Similarissueswerealsoobservedwhenusingthe
Gaussianerrorfunctionastheactivationfunction,althoughtheyweregenerallylesspronounced.
Toalleviatetheseissues,weusedthefollowingapproaches:
• Weusedarelativelylownumberofsamples, N = 6orN = 12,comparedtoD = 200.
AsshowninFig.1,evenwithD =200,thereisstillaconsiderableamountofvariancein
theRFregressors(i.e.,theindividualRFregressorsarenotyetcloselyapproximatingthe
limitingkernelridgeregressor).
15
1xPreprint
2
EnsembleModel
0 KernelModel
0 TrainingData
2
4 1
6
8 2
10
12 RFModels 3
KernelModel
14 TrainingData
4
4 2 0 2 4 4 2 0 2 4
Figure9:AnadversarialexamplewheretheinfiniteensembleofoverparameterizedRFmodels
is numerically not equivalent to a single infinite-width RF model. (Left) We show a sample
of 100 RF models (blue) with ReLU activations trained on the same N = 15 densely clustered
data points. Additionally, we show the single infinite-width RF model (pink). (Right) We again
show the single infinite-width RF model (blue) and the “infinite” ensemble of M = 10,000 RF
models(pink). Asignificantdifferencebetweenthetwomodelsisobservedinthisadversarialcase,
indicatinginstability.
ReLU Features Softplus Features
1.75
0.9
1.50
0.8
1.25
0.7
1.00
N=D N=D
0.6
0.75
0.5
0.50
0.4
0.25
0.3
0.00
0 10 20 30 40 50 0 10 20 30 40 50
Number of features per model Number of features per model
Figure10: UsingsoftplusactivationsinsteadofReLUactivationsreducesinstabilitiesinover-
parameterizedRFensembles. Theplotsshowtheaverageabsolutedifferencebetweenthepredic-
tionsofaninfiniteensembleandasingleinfinite-widthmodelforvaryingfeaturecountsD,using
N = 12 training samples from the California Housing dataset. (Left) ReLU activations exhibit
significant instability, especially for D > N,D N, and do not consistently show the expected
≈
pointwise equivalence between the infinite ensemble and the single infinite-width model. (Right)
Softplus activations — as equivalently shown in Fig. 3 — smooth out these instabilities and more
consistentlyshowtheexpectedpointwiseequivalence.
16
1L)NL(h
)NL(h
1L)NL(h
)NL(hPreprint
Ensemble Variance GP Variance
0.016 VarπD[h( SLN)( ·)] 0.030
0.014 TrainingDataPoints
0.025
0.012
0.010 0.020
r2
0.008 0.015 T⊥rainingDataPoints
0.006
0.010
0.004
0.005
0.002
0.000 0.000
4 2 0 2 4 4 2 0 2 4
Ensemble Variance GP Variance
V Tra ar iπ nD i[ nh g( SL DN a)( t· a)] Points r T⊥2 rainingDataPoints
0.8
0.4
0.6
0.3
0.2 0.4
0.1 0.2
0.0 0.0
4 4
2 2
4 0 4 0
2 2 2 2
0 0
2 4 2 4
4 4
Figure 11: Variance and r2 for different activations and dimensions. (Top left) Variance of
⊥
RF model predictions across the input range for D = 200 and N = 6, using the erf activation
function. (Topright)Correspondingr2 valuesacrosstheinputrangeusingtheerfkernel. (Bottom
⊥
left) Variance of RF model predictions across the input range for D = 200, p = 2, and N = 12,
usingtheReLUactivationfunction. (Bottomright)Correspondingr2 valuesacrosstheinputrange
⊥
usingthearc-cosinekernel.
• We appended a 1 in the zeroeth dimension of the data before calculating the dot product
withω.
• Weperformedadditionalexperimentsusingthesoftplusfunctionwithβ = 1asasmooth
approximationoftheReLUactivationfunction. Thisoftenhelpedstabilizethenumerical
computations, asseeninFig.10, wherewerepeatedapartoftheexperimentfromFig.3
using the ReLU function as activation function which increased the numerical instability
forlowDvalues.
• Weusedaridgetermλ=10−8intheridgelesscasetostabilizetheinversionofΦ Φ⊤.
W W
• We used double precision for all computations and used the torch.linalg.lstsq
functionwiththedrivergelsd(fornot-well-conditionedmatrices)tosolvelinearsystems.
• We applied max-min normalization to the entire California Housing dataset to improve
stability.
A.3 ADDITIONALEXPERIMENTSFORTHERIDGELESSCASE
Additionalexperimentsontheensemblevariance. WeobservedadifferentbehavioroftheRF
regressorvarianceandr2 asshowninFig.4consistentlyacrossdifferentrandomseedsanddimen-
⊥
sionsforbothReLUandtheGaussianerrorfunctionactivationsasactivationfunctions. InFig.11,
we present additional examples for the Gaussian error function in one dimension and the ReLU
activationintwodimensions.
Additional experiment on generalization error and variance scaling. In Fig. 5, we demon-
strated variance and generalization error decay for the ReLU activation function. To verify the
17Preprint
100 Singlemodel
Kernelmodel
3.8 Ensemble
101
3.6
102
Point1 3.4
103 Point2
Point3
Point4 3.2
104
Point5
103 0 1000 2000 3000 4000 5000 6000 7000
Number of features of random feature models Total number of features used
Figure12: Varianceandgeneralizationerrorscalesimilarlywiththenumberoffeatures,con-
sistent with Fig. 5. In (a), the variance of a single model with MD features decays as 1 ,
∼ MD
matchingtheensemble’sbehavior. In(b), thegeneralizationerrorofanensemblewithM models
andD = 200featuresshowsasimilardecaytothatofasinglemodelwithMD features. Results
usetheGaussianerrorfunction,CaliforniaHousingdataset,andN =12.
ReLU Features Gaussian Error Function Features
1000 Mean:0.00 Mean:-0.00
250
800
200
600
150
400 100
200 50
0 0
0.20 0.15 0.10 0.05 0.00 0.010 0.005 0.000 0.005 0.010
[w W (WW + D R R 1) 1]3 [w W (WW + D R R 1) 1]3
Figure 13: Empirically, the term E (cid:104) w⊤W⊤(cid:0) WW⊤+D λ R−⊤R−1(cid:1)−1(cid:105) is consis-
W,w⊥ ⊥ · ·
tently zero. We show the empirical distribution of the third—just because it looks more
interesting—indexofw⊤W⊤(cid:0) WW⊤+D λ R−⊤R−1(cid:1)−1 RN,whichcapturesthedifference
inpredictionsbetweenc⊥ ⊤E (cid:104) WW⊤(cid:0) W· W· ⊤+D λ R−∈ ⊤R−1(cid:1)−1(cid:105) R−⊤yandafinite-sized
W,w⊥
· ·
overparameterized RF model (see Eq. (6)). We use λ = 1.0 in both plots. (Left) We use a ReLU
activationfunction,x R,andN = 6,D = 200. (Right)WeusetheGaussianErrorFunctionas
i
∈
activationfunction,theCaliforniaHousingdataset,andN =12,D =200.
consistency of these trends, we repeated the experiment using the Gaussian error function and the
correspondingerf-kernel. Theresultsareverysimilar,showninFig.12.
A.4 MOREEXPERIMENTSFORTHERIDGECASE
Additionalexperimentsfortheconvergenceoftheexpectedvalueterm. InAppx.C,weshow
thatavariantofLemma1alsoholdsintheridgecase. Moreprecisely,weshowthat
E (cid:104) w⊤W⊤(cid:0) WW⊤+D λ R−⊤R−1(cid:1)−1(cid:105) =0
W,w⊥ ⊥ · ·
under Assumption 1. We repeated the experiment from Fig. 2 for the ridge case to verify this
experimentally. TheresultsareshowninFig.13.
Additionalnotes. InFig.6,weillustratetheLipschitzcontinuityofthepredictionsforaninfinite
ensembleandakernelregressorwithrespecttotheridgeparameter. Ratherthandirectlypresenting
(cid:12) (cid:12) (cid:12) (cid:12)
the difference (cid:12)h¯(RR)(x∗) h(RR)(x∗)(cid:12), we show the evolution of (cid:12)h¯(RR)(x∗) h¯(LS)(x∗)(cid:12) and
(cid:12) ∞,λ − ∞,λ (cid:12) (cid:12) ∞,λ − ∞ (cid:12)
18
])()NL
S(h[DraV
ycneuqerF
rorre
noitazilareneG
ycneuqerFPreprint
(cid:12) (cid:12)
(cid:12)h(RR)(x∗) h(LS)(x∗)(cid:12). This choice was made because the upper bound we obtained was not
(cid:12) ∞,λ − ∞ (cid:12)
consistently tight for settings with large D. In particular, the pointwise predictions of the infinite
ensembleh¯(RR) andthesingleinfinite-widthmodelh(RR) trainedwithridgeλwerealreadyvery
∞,λ ∞,λ
closefornon-zeroλ. Weoptedtodisplaytheupperboundsratherthanthedirectdifferencetoavoid
cherry-pickingfavorablesettings.
Our best explanation for this phenomenon is that infinite ensembles under Assumption 1 in the
ridgeregimeoftenbehavesimilarlytothesingleinfinite-widthmodelh(RR) withanimplicitridge
∞,λ˜
parameterλ˜,whichsolvestheequation
λ˜ =λ+
λ˜ (cid:88)N d
i
D λ˜+d
i=1 i
whered aretheeigenvaluesofthekernelmatrixK,asshownbyJacotetal.(2020)underGaussian
i
universality. Intuitivelyandempirically, forlargeD, theimplicitridgeλ˜ tendstobeverycloseto
thetrueridgeλ. UsingLemma3,thissuggeststhatforsmallvaluesofλ,thedifferencebetweenthe
infiniteensembleandtheinfinite-widthsinglemodelh(RR) withridgeλisalreadyminimalbefore
∞,λ
λapproacheszero.
Interestingly, ourfindings(seeFig.3)suggestthatintheridgelesscase, thesimilaritytotheridge
regressorwiththeimplicitridgeonlyholdsintheoverparameterizedregime. Notethatthisdoesnot
violatetheresultsfromJacotetal.(2020)sincetheconstantsintheirboundsblowupasλ 0in
→
boththeunderparameterizedandoverparameterizedregimes.
B PROOFS FOR OVERPARAMETERIZED RIDGELESS REGRESSION
B.1 EQUIVALENCEOFINFINITEENSEMBLEANDINFINITESINGLEMODEL.
WestartbyprovingtheequivalentformulationoftheinfiniteensemblepredictionstatedinEq.(2)
usingthetermsW andw asintroducedinSec.2:
⊥
Proof. Definingϕ∗ =[ϕ(ω ,x∗)] RD,wehave
W i i ∈
h¯ (x∗)=E (cid:104) 1ϕ∗ Φ⊤ (cid:0)1 Φ Φ⊤(cid:1)−1(cid:105) y
∞ W D W W D · W W
=E (cid:104)(cid:0) c⊤W +r w⊤(cid:1) W⊤R(cid:0) R⊤WW⊤R(cid:1)−1(cid:105) y
W,w⊥ ⊥ ⊥
=E (cid:104)(cid:0) c⊤W +r w⊤(cid:1) W⊤(cid:0) WW⊤(cid:1)−1(cid:105) R−⊤y
W,w⊥ ⊥ ⊥
=c⊤R−⊤y + r E (cid:104) w⊤W⊤(cid:0) WW⊤(cid:1)−1(cid:105) R−⊤y, (5)
⊥ W,w⊥ ⊥
wherec,R,r areasdefinedinEq.(1). TheleftterminEq.(5)isequaltoh(LN)(x):
⊥ ∞
h(LN)(x∗)=[k(x ,x∗)]N K−1y =c⊤RR−1R−⊤y =c⊤R−⊤y.
∞ i i=1
Inthecaseofλ>0,wecansimilarlyseethat
h¯(RR)(x∗)=E (cid:104) 1ϕ∗ Φ⊤ (cid:0)1 Φ Φ⊤ +λI(cid:1)−1(cid:105) y
∞,λ W D W W D · W W
=E (cid:104)(cid:0) c⊤W +r w⊤(cid:1) W⊤R(cid:0) R⊤WW⊤R+D λ R⊤R−⊤R−1R(cid:1)−1(cid:105) y
W,w⊥ ⊥ ⊥ · ·
=E (cid:104)(cid:0) c⊤W +r w⊤(cid:1) W⊤(cid:0) WW⊤+D λ R−⊤R−1(cid:1)−1(cid:105) R−⊤y
W,w⊥ ⊥ ⊥ · ·
=c⊤E (cid:104) WW⊤(cid:0) WW⊤+D λ R−⊤R−1(cid:1)−1(cid:105) R−⊤y
W,w⊥
· ·
+r E (cid:104) w⊤W⊤(cid:0) WW⊤+D λ R−⊤R−1(cid:1)−1(cid:105) R−⊤y. (6)
⊥ W,w⊥ ⊥ · ·
19Preprint
Nextup,weshowthattheexpectedvalueE (cid:104) w⊤W⊤(cid:0) WW⊤(cid:1)−1(cid:105) iszerounderAssumption1.
W,w⊥ ⊥
Thisdirectlyimpliesthepointwiseequivalenceoftheinfiniteensembleandthesingleinfinite-width
model(seeTheorem1).
Lemma1(Restated). UnderAssumption1,itholdsthatE [w⊤W⊤(WW⊤)−1]=0.
W,w⊥ ⊥
Proof. Define A = (WW⊤ w w⊤). Note that A is almost surely invertible and positive
−i − i i −1
definitebyassumptionAssumption1.
BytheWoodburyformula,foralmosteveryWW⊤wehavethat
(WW⊤)−1 =(A +w w⊤)−1 =A−1 A− −1 iwiw i⊤A− −1 i,
−i i i −i − 1+w i⊤A− −1 iwi
whichimpliesthat
(cid:18) (cid:19)
w⊤W⊤(WW⊤)−1 =(cid:80)D w w⊤ A−1 A− −1 iwiw i⊤A− −1 i
⊥ i=1 ⊥i i −i − 1+w i⊤A− −1 iwi
(cid:18) (cid:19)
=(cid:80)D
w
w⊤A− −1 i+w i⊤A− −1 iw i⊤A− −1 iwi w i⊤A− −1 iwiw i⊤A− −1
i
i=1 ⊥i i 1+w i⊤A− −1 iwi − 1+w i⊤A− −1 iwi
=(cid:80)D w⊥iw i⊤ A−1.
i=1 1+w i⊤A− −1 iwi −i
For any positive definite matrix B RN×N and any vector v RN; v = 1 and any
∈ ∈ ∥ ∥
i 1,...,D ,wehave
∈{ }
(cid:12) (cid:12) (cid:12)E w⊥i,wi(cid:104) 1+w w⊥ i⊤iw Bi⊤ wi(cid:105) v(cid:12) (cid:12)
(cid:12)
≤E w⊥i,wi(cid:104)(cid:12) (cid:12) (cid:12)1w +⊥ wi i⊤w Bi⊤ wv i(cid:12) (cid:12) (cid:12)(cid:105)
=(cid:90) ∞ P(cid:104)(cid:12) (cid:12) w⊥iw i⊤v (cid:12) (cid:12) t(cid:105) dt
0
(cid:12)1+w i⊤Bwi(cid:12)
≥
(cid:90) ∞
= P(cid:2)(cid:12) (cid:12)w ⊥iw i⊤v(cid:12) (cid:12) ≥(cid:0) 1+w i⊤Bw i(cid:1) t(cid:3) dt
0
(cid:90) ∞
≤
P(cid:2)(cid:12)
(cid:12)w ⊥iw
i⊤(cid:12) (cid:12)>t(cid:3)
dt
0
(cid:90) ν2/α 2exp(cid:16) t2(cid:17) dt+(cid:90) ∞ 2exp(cid:0) t (cid:1) dt, (7)
≤ −2ν −2α
0 ν2/α
wherethelastinequalityisastandardsub-exponentialboundappliedtow w . Notethatwehere
usethefactthatE[w ⊥iw i⊤]=0andthe(ν2,α)-sub-exponentialityof(cid:12) (cid:12)w ⊥i⊥ wi i⊤(cid:12) (cid:12)i .
SincethelasttwointegralsinEq.(7)arefinite,theexpectationE (cid:2) (w w⊤)/(1+w⊤Bw )(cid:3) v
W,w⊥ ⊥i i i i
isfinite.Bytheweaklawoflargenumbers,fori.i.d.randomvariablesw(j)andw(j)acrossdifferent
i ⊥i
j’s,wehave
P(cid:20)(cid:12) (cid:12) (cid:12) (cid:12)M1 (cid:80)M
j=1
1+w (⊥( wj i) i(( jw ))i( ⊤j) B)⊤ wv
i(j)
−E W,w⊥(cid:104) 1+w w⊥ i⊤iw Bi⊤ wi(cid:105) v(cid:12) (cid:12) (cid:12) (cid:12)>t(cid:21) →0,
for any t > 0 and v RN such that v = 1 as M . At the same time, repeating the
∈ ∥ ∥ → ∞
sub-exponentialargumentabove,wehavethat
P(cid:20)(cid:12) (cid:12) (cid:12) (cid:12)M1 (cid:80)M
j=1
1+w (⊥( wj i) (( jw ))i( ⊤j) B)⊤ wv (j)(cid:12) (cid:12) (cid:12) (cid:12)>t(cid:21) ≤P(cid:104)(cid:12) (cid:12) (cid:12)M1 (cid:80)M i=1w ⊥(j i)(w i(j))⊤v(cid:12) (cid:12) (cid:12)>t(cid:105)
i i
(cid:40) (cid:16) (cid:17)
2exp Mt2 0<t ν2/α
− 2ν ≤
≤ 2exp(cid:0) Mt(cid:1) t>ν2/α
−2α
0
→
asM . HereweusethepropertythatthesumofM (ν2,α)-sub-exponentialrandomvariables
is(Mν→2,∞
α)-sub-exponential.
20Preprint
Together,theseresultsimplythatE (cid:2) (w w⊤)/(1+w⊤Bw )(cid:3) =0foreverypositivedefinite
W,w⊥ ⊥i i i i
B. Since the random matrix A is positive semidefinite, almost surely invertible (by the second
−i
halfofAssumption1),andindependentofw ,w ,wehavethat
i ⊥i
E
w⊥,W
(cid:104) w ⊥W⊤(cid:0) WW⊤(cid:1)−1(cid:105) =(cid:80)D i=1E w⊥i,wi,A−i(cid:20) 1+w w⊥ i⊤i Aw
−
−i⊤
1
iwiA− −1 i(cid:21)
(cid:20) (cid:20) (cid:21) (cid:21)
=(cid:80)D i=1E
A−i
E
w⊥i,wi
1+w w⊥ i⊤i Aw
−
−i⊤
1 iwi
A− −1
i
=0.
We remark that this proof equivalently holds for the ridge-regression case, i.e.,
E (cid:104) w⊤W⊤(cid:0) WW⊤+D λ R−⊤R−1(cid:1)−1(cid:105) = 0 since the proof does not rely on the
W,w⊥ ⊥ · ·
specificformofthematrixA otherthanitbeingpositivedefinite. ThusbyEq.(6)wedirectlyget
−i
thatunderAssumption1itholdsthat
h¯(RR)(x∗)=c⊤E (cid:104) WW⊤(cid:0) WW⊤+D λ R−⊤R−1(cid:1)−1(cid:105) R−⊤y. (8)
∞,λ W,w⊥ · ·
B.2 VARIANCEOFENSEMBLEPREDICTIONS.
Inthenextstep,weshowtheformulaforthevarianceofasinglemodelpredictionunderGaussian
universality. Note that one could also get this result by slightly extending proofs by Jacot et al.
(2020).
Lemma2(Varianceofsinglemodelpredictions). UnderGaussianuniversalityandassumingD >
N +1,thevarianceofsinglemodelpredictionatatestpointx∗isgivenby
h(LN) 2
Var [h(LN)(x∗)]=r2 ∥ ∞ ∥H , (9)
W W ⊥D N 1
− −
where isnormdefinedbytheRKHSassociatedwithkernelk(, ).
H
∥·∥ · ·
Proof. Westartbywritingdownthevarianceofthepredictionofasinglemodel:
Var [h(LN)(x∗)]=E [h(LN)(x∗)2] E [h(LN)(x∗)]2
W W W W − W W
UsingTheorem1,thedefinitionofthepredictionofasinglemodelandthedefinitionofW andw ,
⊥
wecanexpandthisexpression:
=E [ϕ∗ Φ⊤(Φ Φ⊤)−1yy⊤(Φ Φ⊤)−⊤Φ ϕ∗⊤] (h(LN)(x∗))2
W W W W W W W W W − ∞
=E [(r w⊤+c⊤W)W⊤R(cid:0) R⊤WW⊤R(cid:1)−1 yy⊤(cid:0) R⊤WW⊤R(cid:1)−⊤ R⊤W(r w⊤+c⊤W)⊤]
W,w⊥ ⊥ ⊥ ⊥ ⊥
(h(LN)(x))2
− ∞
=E [(r w⊤+c⊤W)W⊤(cid:0) WW⊤(cid:1)−1 R−⊤yy⊤R−1(cid:0) WW⊤(cid:1)−⊤ W(r w⊤+c⊤W)⊤]
W,w⊥ ⊥ ⊥ ⊥ ⊥
(h(LN)(x))2
− ∞
=(c⊤R−⊤y)2 (h(LN)(x))2
− ∞
+2 r⊤E [w⊤W⊤(WW⊤)−1]R−⊤yy⊤R−1c
· ⊥ W,w⊥ ⊥
+r2E [w⊤W⊤(WW⊤)−1R−⊤yy⊤R−1(WW⊤)−TWw ]
⊥ W,w⊥ ⊥ ⊥
Nowwecanseethatthefirsttwotermscancelout(sinceh(LN)(x) = c⊤R−⊤y)andthethirdterm
∞
iszerobyLemma1. Weareleftwiththefourthterm,whichwecanslightlyrewrite:
Var [h(LN)(x∗)]=r2E [w⊤W⊤(WW⊤)−1R−⊤yy⊤R−1(WW⊤)−TWw ]
W W ⊥ W,w⊥ ⊥ ⊥
=r2y⊤R−1E [(WW⊤)−TWw w⊤W⊤(WW⊤)−1]R−⊤y (10)
⊥ W,w⊥ ⊥ ⊥
21Preprint
Usingthetowerruleforconditionalexpectations,wehave:
Var [h(LN)(x)]=r2y⊤R−1E [(WW⊤)−TWw w⊤W⊤(WW⊤)−1]R−⊤y
W W ⊥ W,w⊥ ⊥ ⊥
=r2y⊤R−1E [(WW⊤)−TWE [w w⊤ W]W⊤(WW⊤)−1]R−⊤y
⊥ W w⊥|W ⊥ ⊥|
SincetheGaussianuniversalityassumptionimpliesW andw areindependent,weget:
⊥
Var [h(LN)(x)]=r2y⊤R−1E [(WW⊤)−TWE [w w⊤]W⊤(WW⊤)−1]R−⊤y
W W ⊥ W w⊥ ⊥ ⊥
Moreover, since by Gaussian universality w and W are multivariate Gaussians with the identity
⊥
matrixascovariance,weget(viatheexpectedvalueofaWishartandaninverseWishartdistribution;
notethatforgettingthisexpectedvalue,weneedtoassumethatD >N +1):
Var [h(LN)(x)]=r2y⊤R−1E [(WW⊤)−T(WW⊤)(WW⊤)−1]R−⊤y
W W ⊥ W
=r2y⊤R−1E [(WW⊤)−T]R−⊤y
⊥ W
=r2 y⊤R−1R−⊤y
⊥ D−N−1
=r2 y⊤K−1y.
⊥D−N−1
Recognizingthaty⊤K−1y = h(LN) 2 (e.g.Wainwright,2019,Ch.12)completestheproof.
∥ ∞ ∥H
An equivalent argument does not work under the more general Assumption 1 since w and W
⊥
arenotnecessarilyindependent. Eveninthecaseofindependence, E [(WW⊤)−1]mightnotbe
W
known.
Counterexampleforsubexponentialcase. Wenowgiveanexplicitcounterexampleshowingthat
whenonlyassuminguncorrelatednessbetweenW andw theterm
⊥
E :=E [(WW⊤)−TWw w⊤W⊤(WW⊤)−1]
W,w⊥ ⊥ ⊥
fromEq.(10)dependsonx∗implyingthatthevariancedoesnotonlydependonx∗viar2.
⊥
Let us assume N = D = 1 and let W be uniformly distributed across the set
(cid:110) (cid:111)
√4 , √3 ,√3 ,√4 . ThenwehaveE[W]=0andE[W2]= 1 16 + 1 9 =1.
− 12.5 − 12.5 12.5 12.5 2 · 12.5 2 · 12.5
(cid:110) (cid:111)
Now consider an x∗ that produces a w
⊥
so that w
⊥
= √2 when W = √3 ,√3 and
− 12.5 12.5
w =0otherwise. ThenwehaveE[w⊤W]=0andE[w2]=1. ThevalueofE isnow 12.5.
⊥ ⊥ ⊥ 9
(cid:110) (cid:111)
Furthermore, consideranx∗ thatproducesaw
⊥
sothatw
⊥
= √2whenW = √4 ,√4
− 12.5 12.5
andw =0otherwise. ThenwehaveE[w⊤W]=0andE[w2]=1. ThevalueofE isnow 12.5.
⊥ ⊥ ⊥ 16
C PROOFS FOR OVERPARAMETERIZED RIDGE REGRESSION
C.1 DIFFERENCEBETWEENTHEINFINITEENSEMBLEANDINFINITESINGLEMODEL.
Webeginwithalemma,whichshowsthatthepredictionofkernelregressorsisLipschitz-continuous
inλforanyx∗ andλ 0. Wewilldenotethekernelridgeregressorwithregularizationparameter
≥
λash(RR),asintroducedinSec.3.3.
∞,λ
Lemma 3 (Bound on the difference between the kernel ridge regressors). Let λ,λ′ 0 be two
regularizationparameters. Then,foranyx∗ itholdsthat: ≥
∈X
h(RR)(x∗) h(RR)(x∗) √n C λ′ λ (cid:112) yTK−4y
| ∞,λ′ − ∞,λ |≤ · 1 ·| − |·
whereweassumek(x ,x∗) C foralli [N].
i 1
≤ ∈
22Preprint
Proof. We can write the kernel ridge regressors as h(RR)(x∗) = (cid:80)n α k(x ,x∗) and
∞,λ i=1 1,i i
h(RR)(x∗)=(cid:80)n α k(x ,x∗)withcoefficientsα andα givenby:
∞,λ′ i=1 2,i i 1 2
α =(K+λI)−1y
1
α =(K+λ′I)−1y
2
WenowwriteyintheorthonormalbasisoftheeigenvectorsofK,i.e. y
=(cid:80)n
a v . Wecallthe
i=1 i i
correspondingeigenvaluesofK d ,...,d >0.
1 n
Thematrix(K+λI)−1hasthesameeigenvectorsasKandtheeigenvaluesare0<d˜ = 1 1.
Thus,wecanwriteα =(cid:80)n a 1 v andα =(cid:80)n a 1 v . i di+λ ≤ λ
1 i=1 idi+λ i 2 i=1 idi+λ′ i
Inthenextstep,webound α α 2: Usingtheorthonormalityoftheeigenvectors,weget:
∥ 1 − 2 ∥2
(cid:16) (cid:16) (cid:17)(cid:17)2
α α 2 =(cid:80)n a 1 1
∥ 1 − 2 ∥2 i=1 i di+λ − di+λ′
(cid:12) (cid:12) (cid:12) (cid:12)
Nowwebound(cid:12) 1 1 (cid:12) (cid:12) λ′−λ (cid:12) |λ′−λ| whichgivesus:
(cid:12)λ+di − λ′+di(cid:12) ≤(cid:12)λλ′+(λ+λ′)di+d2 i(cid:12) ≤ d2 i
α α 2 (cid:80)n
(cid:16) ai|λ′−λ|(cid:17)2
λ′ λ2yTK−4y
∥ 1 − 2 ∥2 ≤ i=1 d2 i ≤| − |
Usingthisresult,wecanboundthedifferencebetweenthepredictionsofthetwokernelregressors
atasinglepointx∗:
h(RR)(x∗) h(RR)(x∗) = (cid:80)n (α α )k(x ,x∗) (cid:80)n α α k(x ,x∗)
| ∞,λ − ∞,λ′ | | i=1 1,i − 2,i i |≤ i=1| 1,i − 2,i | i
Sincek(x ,x∗) C ,weget(usingtherelationbetweenthe1-normandthe2-norm):
i 1
≤
f (x∗) f (x∗) C (cid:80)n α α C α α √n √n C λ′ λ (cid:112) y⊤K−4y
| λ − λ′ |≤ 1 i=1| 1,i − 2,i |≤ 1 ∥ 1 − 2 ∥2 ≤ · 1 ·| − |·
Usingsimilararguments,wenowshowthattheexpectedpredictionofRFregressors,i.e.,thepre-
dictionoftheinfiniteensembleofRFregressors,isLipschitz-continuousforanyx∗andλ 0:
≥
Lemma 4 (Bound on the difference between expected RF Regressors). Under Assumption 1 and
Assumption2,theexpectedvalueofthepredictionofRFregressorsisLipschitz-continuousinλfor
anyx∗andλ 0,i.e.,foranyx∗itholdsthat:
≥
h¯(RR)(x∗) h¯(RR)(x∗) c⊤R−⊤ y DC λ′ λ
| ∞,λ′ − ∞,λ |≤∥ ∥∥ ∥ 2 | − |
whereC isaconstantdependingonthedistributionofΦ.
2
Proof. Weusethecharacterizationofh¯(RR)(x∗)fromEq.(8),whichgivesusthedifferenceas
∞,λ
(cid:12) (cid:12)c⊤E (cid:104) WW⊤(cid:16)(cid:0) WW⊤+D λ′ R−⊤R−1(cid:1)−1 (cid:0) WW⊤+D λ′ R−⊤R−1(cid:1)−1(cid:17)(cid:105) R−⊤y(cid:12) (cid:12).
(cid:12) W,w⊥
· · − · ·
(cid:12)
WecannowreversesomestepswemadetogetthischaracterizationandwriteitintermsofΦagain:
(cid:12) (cid:12)c⊤R−⊤E (cid:104) Φ Φ⊤ (cid:16)(cid:0) Φ Φ⊤ +D λ′ I(cid:1)−1 (cid:0) Φ Φ⊤ +D λ I(cid:1)−1(cid:17)(cid:105) y(cid:12) (cid:12).
(cid:12) W W W W W · · − W W · · (cid:12)
Andnow,usingJensen’sinequalityandtheconvexityofthetwo-norm,wecanpullouttheexpected
valuetotheoutsideofthedifference:
c⊤R−⊤ E (cid:104) Φ Φ⊤ (cid:16)(cid:0) Φ Φ⊤ +D λ′ I(cid:1)−1 (cid:0) Φ Φ⊤ +D λ I(cid:1)−1(cid:17) y (cid:105) .
∥ ∥· W ∥ W W W W · · − W W · · ∥
SimilarlytotheproofofLemma3, wecanwritey intheorthonormalbasisoftheeigenvectorsof
ΦΦ⊤(notethatwedropthesubscript fornotationalsimplicity),i.e.y =(cid:80)n a v .Furthermore
W i=1 i i
23Preprint
wedefinetheeigenvaluesofΦΦ⊤ asd ,...,d >0. Thematrix(ΦΦ⊤+D λI)−1 againhasthe
1 n
sameeigenvectorsasΦΦ⊤andtheeigenvaluesare0< 1 1 . ·
di+D·λ ≤ D·λ
MultiplyingywithΦΦ⊤(ΦΦ⊤+D λI)−1andΦΦ⊤(ΦΦ⊤+D λ′I)−1thengivesus:
· ·
ΦΦ⊤(ΦΦ⊤+D ·λI)−1y =(cid:80)n i=1a idi+d Di ·λv i
ΦΦ⊤(ΦΦ⊤+D ·λ′I)−1y =(cid:80)n i=1a idi+d Di ·λ′v i
Wecannowcalculatethedifferenceofthesetwovectorsusingtheorthonormalityoftheeigenvec-
tors:
(cid:16) (cid:16) (cid:17)(cid:17)2
∥ΦΦ⊤(ΦΦ⊤+D ·λ′I)−1y −ΦΦ⊤(ΦΦ⊤+D ·λI)−1y ∥2 2 =(cid:80)n i=1 a i di+d Di ·λ − di+d Di ·λ′
Nowwelookatthedifferencebetweenthetwocoefficientsandseethatforeachi,wehave:
(cid:12) (cid:12)
(cid:12) di di (cid:12) D·|λ′−λ|
(cid:12)di+D·λ − di+D·λ′(cid:12) ≤ di
Thus,wehavethatthedifferenceisboundedby:
ΦΦ⊤(ΦΦ⊤+D λ′I)−1y ΦΦ⊤(ΦΦ⊤+D λI)−1y 2 D2·|λ−λ′|2 y 2.
∥ · − · ∥2 ≤ d2 N ∥ ∥2
All together, we can now bound the difference of the expected values of the predictions of RF
regressorsvia:
(cid:104) (cid:105)
h¯(RR)(x∗) h¯(RR)(x∗) c⊤R−⊤ y D λ′ λE 1
| ∞,λ′ − ∞,λ |≤∥ ∥∥ ∥ | − | dN dN
Sincetr((ΦΦ⊤)−1)=(cid:80)n 1,andthetraceisalinearoperator,wecanwrite:
i=1 di
E (cid:104) 1 (cid:105) E (cid:2) (tr(Φ Φ⊤)−1)(cid:3) =tr(E (cid:2) (Φ Φ⊤)−1(cid:3) )=:C
dN dN ≤ W W W W W W 2
whichisfinitewheneverE (cid:2) (Φ Φ⊤)−1(cid:3) isfinite,i.e. Assumption2holds.
W W W
UsingLemma3andLemma4wecannowshowthatthedifferencebetweentheinfiniteensemble
whereeachmodelhasridgeλandtheinfinitesinglemodelwithridgeλisLipschtiz-continuousin
λforλ 0:
≥
Theorem2(Restated). UnderAssumptions1and2, thedifference h¯(RR)(x∗) h(RR)(x∗) be-
| ∞,λ − ∞,λ |
tween the infinite ensemble and the single infinite-width model trained with ridge λ is Lipschitz-
continuousinλforλ 0. TheLipschitzconstantisindependentofx∗forcompact .
≥ X
(cid:12) (cid:12)
Proof. Webounddifference(cid:12)h¯(RR)(x∗) h(RR)(x∗) h¯(RR)(x∗) h(RR)(x∗)(cid:12)byusingfirst
(cid:12) | ∞,λ′ − ∞,λ′ |−| ∞,λ − ∞,λ |(cid:12)
theinverse,thenthenormaltriangleinequality:
(cid:12) (cid:12)
(cid:12)h¯(RR)(x∗) h(RR)(x∗) h¯(RR)(x∗) h(RR)(x∗)(cid:12)
(cid:12) | ∞,λ′ − ∞,λ′ |−| ∞,λ − ∞,λ |(cid:12)
h¯(RR)(x∗) h¯(RR)(x∗)+h(RR)(x∗) h(RR)(x∗)
≤| ∞,λ′ − ∞,λ ∞,λ − ∞,λ′ |
h¯(RR)(x∗) h¯(RR)(x∗) + h(RR)(x∗) h(RR)(x∗)
≤| ∞,λ′ − ∞,λ | | ∞,λ − ∞,λ′ |
UsingtheboundfromLemma3andLemma4(andsummarizingthethecorrespondingconstantsas
c andc )wecanboundthisby:
1 2
h¯(RR)(x∗) h(RR)(x∗) h¯(RR)(x∗) h(RR)(x∗) c λ′ λ +c λ′ λ
| ∞,λ′ − ∞,λ′ |−| ∞,λ − ∞,λ |≤ 1 | − | 2 | − |
ThuswehaveLipschitz-continuityinλforλ 0.
≥
The Lipschitz constant is independent of x∗ for compact since the Lipschitz constants from
Lemma3andLemma4dependonx∗inacontinuoX
usfashion.
NotethatanequivalentargumentincombinationwithJacotetal.(2020)[Proposition4.2],i.e. λ˜
γ λ,directlygivestheLipschitz-continuityinλforλ 0forthedifferencebetweentheinfini≤ te
γ−1 ≥
ensembleandtheinfinite-widthsinglemodelwitheffectiveridgeintheoverparameterizedregime.
24Preprint
D UNDERPARAMETERIZED ENSEMBLES
Here,weofferaproofthatinfinite,unregularized,underparameterizedRFensemblesareequivalent
tokernelridgeregressionunderatransformedkernelfunction.Weemphasizethedifferencefromthe
overparameterizedcase—thecentralfocusofourpaper—inwhichtheinfiniteensembleisequivalent
to a ridgeless kernel regressor. Thus, underparameterized ensembles induce regularization, while
overparameterizedensemblesdonot.
OtherworkshaveexploredtheridgebehaviorofunderparameterizedRFensembles(Kaba´n,2014;
Thaneietal.,2017;Bach,2024a);however,theseworksoftenfocusonanequivalenceingeneraliza-
tionerrorwhereasweestablishapointwiseequivalence.Tothebestofourknowledge,thefollowing
resultisnovel:
Lemma 5. If the expected orthogonal projection matrix E (cid:104) R⊤W˜ (cid:0) W⊤RR⊤W(cid:1)−1 W˜⊤R(cid:105) is
W˜
well defined, and a contraction (i.e., singular values strictly less than 1), then the infinite under-
parameterizedRFensembleh¯(LN)(x∗)isequivalenttokernelridgeregressionundersomekernel
∞
functionk˜(, ).
· ·
Proof. WhenD <N,theinfiniteridgelessRFensembleisgivenby
h¯(LN)(x∗)=E (cid:104) 1 (cid:80)D ϕ(ω ,x∗)(cid:0)1Φ⊤Φ (cid:1)−1 Φ⊤(cid:105) y
∞ W D j=1 j D W W W
=E (cid:104)(cid:0) r w⊤+c⊤W(cid:1)(cid:0) W⊤RR⊤W(cid:1)−1 W⊤(cid:105) Ry, (11)
W,w⊥ ⊥ ⊥
whereW,w ,r ,c,RareasdefinedinSec.2. Definingthefollowingblockmatrices:
⊥ ⊥
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21)
W R c
W˜ = R(N+1)×D, R˜ = R(N+1)×N, c˜= R(N+1),
w ⊥⊤ ∈ 0 ∈ r ⊥ ∈
wecanrewriteEq.(11)as
h¯(LN)(x∗)=c˜⊤(cid:16) E (cid:104) W˜ (cid:0) W⊤RR⊤W(cid:1)−1 W˜⊤(cid:105)(cid:17) R˜y.
∞ W˜
ByaddingandsubtractingR˜R˜⊤ insidetheouterparenthesis, wecanmassagethisexpressioninto
kernelridgeregressioninatransformedcoordinatesystem:
 −1
h¯(LN)(x∗)=c˜⊤R˜R˜⊤+(cid:16) E (cid:104) W˜ (cid:0) W⊤RR⊤W(cid:1)−1 W˜⊤(cid:105)(cid:17)−1 R˜R˜⊤ R˜y.
∞  W˜ − 
:=A˜
(cid:16) (cid:17)−1
=c˜⊤A˜−1R˜ R˜⊤A˜−1R˜+I y. (12)
ApplyingtheWoodburyinversionlemmatoA˜−1,wehave:
A˜−1 =E (cid:104) W˜ (cid:0) W⊤RR⊤W(cid:1)−1 W˜⊤(cid:105)
W˜
+E (cid:104) W˜ (cid:0) W⊤RR⊤W(cid:1)−1 W⊤R(cid:105) (I E [P ])−1E (cid:104) R⊤W (cid:0) W⊤RR⊤W(cid:1)−1 W˜⊤(cid:105) ,
W˜
−
W W W˜
(13)
whereP isthe(random)orthogonalprojectionmatrixontothespanofthecolumnsofR⊤W:
W
P =R⊤W (cid:0) W⊤RR⊤W(cid:1)−1 W⊤R.
W
Because P is an orthogonal projection matrix, we have that P = 1, and thus (by Jensen’s
W W 2
inequality) E [P ] 1. Ifthisinequalityisstrictsothat∥ I E∥ [P ]isinvertible,wehave
W W 2 W W
byinspectio∥ nofEq.(1∥ 3)≤ thatA˜ispositivedefinite. Therefore,the− blockmatrix
(cid:20) R˜⊤(cid:21)
A˜−1(cid:2) R˜ c˜(cid:3)
=(cid:20) R˜⊤A˜−1R˜ R˜⊤A˜−1c˜(cid:21)
(14)
c˜⊤ c˜⊤A˜−1R˜ c˜⊤A˜−1c˜
25Preprint
isalsopositivedefiniteandthustherealizationofsomekernelfunctionk˜(, );i.e.
· ·
 k˜(x ,x ) k˜(x ,x ) k˜(x ,x∗)
1 1 1 N 1
(cid:20) R˜⊤A˜−1R˜ R˜⊤A˜−1c˜(cid:21) =  . . . · .· .· . . . . . . .  .
c˜⊤A˜−1R˜ c˜⊤A˜−1c˜  k˜(x N,x 1) k˜(x N,x N) k˜(x N,x∗) 
k˜(x∗,x ) ··· k˜(x∗,x ) k˜(x∗,x∗)
1 N
···
NotethatifA˜=I thenbyEq.(1)werecovertheoriginalkernelmatrix
k(x ,x ) k(x ,x ) k(x ,x∗)
1 1 1 N 1
(cid:20) R c˜˜ ⊤⊤ RR ˜˜ R c˜˜ ⊤⊤ c˜c˜(cid:21) = 
k(x
. . .
,x )
· .· .· .
k(x
. . .
,x ) k(x
. . . ,x∗)  .
N 1 N N N
k(x∗,x ) ··· k(x∗,x ) k(x∗,x∗)
1 N
···
Thus,theunderparameterizedensembleinEq.(12)simplifiesto
 k˜(x ,x ) k˜(x ,x ) −1
1 1 1 N
h¯( ∞LN)(x∗)=(cid:2) k˜(x∗,x 1) k˜(x∗,x N)(cid:3)   . . . · .· .· . . . .  +I  y,
···
k˜(x ,x ) k˜(x ,x )
N 1 N N
···
whichiskernelridgeregressionwithrespecttothekernelk˜(, ).
· ·
26