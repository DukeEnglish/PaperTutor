Yeah, Un, Oh: Continuous and Real-time Backchannel Prediction
with Fine-tuning of Voice Activity Projection
KojiInoue,DiveshLala,GabrielSkantze*,TatsuyaKawahara
GraduateSchoolofInformatics,KyotoUniversity,Japan,
*KTHRoyalInstituteofTechnology,Sweden
Correspondence:inoue@sap.ist.i.kyoto-u.ac.jp
Abstract I went to that museum But it was closed on the day
Inhumanconversations,shortbackchannelut-
terances such as “yeah” and “oh” play a cru-
User
cial role in facilitating smooth and engaging (Speaker)
dialogue. Thesebackchannelssignalattentive- Continuous prediction
nessandunderstandingwithoutinterruptingthe
speaker, making their accurate prediction es-
sentialforcreatingmorenaturalconversational System
Yeah Oh
agents. This paper proposes a novel method (Listener)
forreal-time,continuousbackchannelpredic-
Figure1: Conceptualdiagramofcontinuousbackchan-
tionusingafine-tunedVoiceActivityProjec-
nelprediction
tion(VAP)model. Whileexistingapproaches
have relied on turn-based or artificially bal-
anceddatasets,ourapproachpredictsboththe
linguistictokenssuchas“Isee,”thisworkfocuses
timing and type of backchannels in a contin-
uous and frame-wise manner on unbalanced, onshorttokensthatarefrequentlyanddynamically
real-worlddatasets. Wefirstpre-traintheVAP usedbylisteners.
modelonageneraldialoguecorpustocapture
Backchannel modeling remains a significant
conversationaldynamicsandthenfine-tuneit
challengeduetotheirsubtleandcontext-dependent
onaspecializeddatasetfocusedonbackchan-
characteristics. Given the dynamic nature of
nelbehavior. Experimentalresultsdemonstrate
thatourmodeloutperformsbaselinemethodsin backchannels, it is essential to predict them on
bothtimingandtypepredictiontasks,achiev- a frame-by-frame basis and in real-time for live
ing robust performance in real-time environ- spokendialoguesystems,asdepictedinFigure1.
ments. This research offers a promising step However, most previous studies have focused on
toward more responsive and human-like dia-
utterance-basedsystems,or,inthecaseofframe-
loguesystems,withimplicationsforinteractive
based systems, have artificially balanced the test
spoken dialogue applications such as virtual
databyreducingnon-backchannelsamples. This
assistantsandrobots.
data manipulation introduces a discrepancy be-
1 Introduction
tweenthetrainingmodelsandreal-worldsystems.
Consequently,forpracticalapplications,itisnec-
In natural human conversations, short backchan-
nels, such as “yeah” and “right,” play a crucial essarytodevelopmodelscapableofreal-time,con-
tinuous frame-wise prediction and evaluate them
role in facilitating smooth and engaging inter-
usingunbalanced,real-worlddatasets.
actions (Clark, 1996; Clancy et al., 1996; Mad-
drell and Watson, 2012). They function as feed- Transformer-basedarchitectureshaveemerged
backmechanisms,signalingattentiveness,under- as powerful tools for a broad range of sequen-
standing, and agreement without interrupting the tial prediction tasks, such as language modeling
speaker. Accurate prediction and generation of and speech recognition. Among these, the Voice
backchannels in spoken dialogue systems are es- Activity Projection (VAP) model, a Transformer-
sential for creating more natural and human-like based architecture, has shown its efficacy in pre-
interactions(Schroderetal.,2011;DeVaultetal., dictingfuturevoiceactivitywithindialogues(Ek-
2014; Inoue et al., 2020b). Although some defi- stedt and Skantze, 2022b,a). Since voice activ-
nitions of backchannels include longer and more ityiscloselyintertwinedwithturn-takingdynam-
4202
tcO
12
]LC.sc[
1v92951.0142:viXraics and backchannel behaviors, the VAP model have focused exclusively on predicting the tim-
seemstohavethepotentialtoenablemoreaccurate ing and types of backchannels. The definition
backchannelprediction. Furthermore,asaprevious and functions of backchannel types have been
studyimplementedthereal-timeVAPmodel(Inoue explored in conversational analysis and linguis-
etal.,2024b),aVAP-basedbackchannelprediction ticstudies(DrummondandHopper,1993;Wong
model could serve as a promising candidate for and Peters, 2007; Tang and Zhang, 2009; Den
real-time backchannel generation systems. In its et al., 2011). Despite the critical role of prosody
initialapplication,theVAPmodelwasemployed in entrainment, existing research on this remains
forbackchannelpredictioninazero-shotmanner. scarce (Kawahara et al., 2015; Ochi et al., 2024).
However, this approach exhibited a sensitivity to Thisreviewprimarilysummarizesthecurrentstate
threshold selection and depended on a balanced of research on predicting the timing and form of
dataset. ThesefindingssuggestthatwhiletheVAP backchannels.
modelshowspromiseforbackchannelprediction, Beforerecentadvancesinmachinelearningtech-
further research is necessary to refine its training nologies,backchannelpredictionmodelswerepri-
methodologyforimprovedperformance. marilybasedonhand-craftedfeatureswithheuris-
In this study, we propose a novel approach to tic rules or simpler models (Koiso et al., 1998;
backchannelpredictionbyutilizingtheVAPmodel Ward and Tsukahara, 2000; Fujie et al., 2005;
asafoundationalframework. WefirsttraintheVAP Morencyetal.,2008,2010;OzkanandMorency,
modelonalargecorpusofgeneraldialoguedatato 2011; Blache et al., 2020). With the advent of
capturethefundamentalpatternsofconversational dataset creation paradigms and machine learn-
dynamics. Subsequently, we fine-tune the model ingadvancements,deeplearningmodelsbeganto
on a specialized dataset focused on backchannel be employed for backchannel prediction. Initial
prediction. Thistwo-stagetrainingprocessisanal- modelswerebuiltusinglong-short-termmemory
ogoustothepre-trainingandfine-tuningparadigm (LSTM) networks (Ruede et al., 2017a,b; Adiba
employedbymodelslikeBERT,aimingtodemon- etal.,2021;Jainetal.,2021),whilemorerecentap-
strate the VAP model’s versatility as a general- proachesleverageTransformer-basedmodels(Jang
purposebasemodel. Moreover,tothebestofour etal.,2021;Liermannetal.,2023).
knowledge,ourmodelisthefirsttopredictboththe Mostpreviousstudiesfocusedontimingpredic-
timing and type of backchannels in a continuous tion, with some also addressing type prediction.
andreal-timemanner. Themostconventionalapproachinvolvesframing
Thecontributionsofthispaperaretwofold. the prediction task as a three-class classification
problem: non-backchannel,continuer,andassess-
• Real-Time Continuous Backchannel Pre-
ment(Choietal.,2024),asdescribedinSection4.2.
diction: Amethodforreal-time,continuous
The present work adopts this three-class classifi-
backchannel prediction based on the VAP
cation scheme for both timing and type predic-
model is developed and evaluated on real-
tion. Otherresearchhasexploredafour-typeclas-
world,unbalancedtestdata.
sificationsystem,encompassingnon-backchannel,
• Two-StageTrainingforGeneralization: A
continuer,understanding,andempathybackchan-
two-stage training process is introduced for
nels(Jangetal.,2021). Anotherstudyintroduced
theVAPmodel,demonstratingitspotentialas
afive-typeclassificationforsinglecontinuer,dou-
afundamentalmodelforpredictingconversa-
blecontinuers(e.g.,"yeahyeah"),triplecontinuers,
tionaldynamics.
assessment,andnon-backchannel(Kawaharaetal.,
2016). Furthermore,adifferentapproachproposed
Notethatthesourcecodesandtrainedmodelsare
publiclyavailable1. a two-step classification method where the first
model predicts the timing, followed by a second
2 RelatedWork modelthatdeterminesthetype(Adibaetal.,2021).
In terms of prediction unit, utterance-based or
Effectivebackchannelgenerationnecessitatesac-
continuous,utterance-basedmodelstendtoincor-
curate prediction of three key elements: tempo-
porate linguistic features such as word embed-
ralplacement(timing),linguisticform(type),and
dings (Jang et al., 2021; Park et al., 2024). Con-
prosodic patterns. The majority of prior studies
versely,previouscontinuousmodelsweregenerally
1https://github.com/inokoj/VAP-Realtime restrictedtousingprosodicfeatures(Ruedeetal.,numerousbackchannelresponsesbyERICA.The
User Operator
participants comprised two demographic groups:
studentsandolderadults. Eachgroupwasprovided
with a prompt to guide their conversation; for in-
stance,studentsdiscussed“challengesduringthe
COVID-19 pandemic,” while the elderly partici-
ERICA
pantsreflectedon“memorabletravelexperiences
Figure2: Setupfordialoguerecording andrecentfavoritemeals.” ERICA’soperatorswere
threeexperiencedactresses,withoneparticipating
ineachsession. Intotal,werecorded109dialogue
2017a,b). Recentmodelshavebegunutilizingau-
sessions,eachlastingapproximately7to8minutes.
dio encoders that can theoretically capture both
The data were randomly divided into 87, 11, and
linguistic and prosodic information in an end-to-
11sessionsfortraining,validation,andtestingpur-
end manner (Park et al., 2024; Choi et al., 2024).
poses, respectively. Wesubsequently transcribed
The VAP model used in this study similarly em-
thedialoguesandannotatedERICA’sbackchannel
ploys a pre-trained contrastive predictive coding
responses.
(CPC)modelasitsaudioencoder.
To address the issue of imbalanced data, re- 3.2 Pre-trainingDataforVAP
cent studies have integrated multi-task learning
Inthisstudy,weintroduceatwo-steptrainingap-
with the primary task of backchannel prediction.
proach where the original VAP model is initially
Forinstance,subtaskssuchasturn-takingpredic-
trained, followed by fine-tuning for backchannel
tion (Hara et al., 2018; Ishii et al., 2021), senti-
prediction. Thefirststeprequiresalargerdataset
mentscoreanalysis(Jangetal.,2021),dialogueact
totraintheVAPmodeleffectivelytocontinuously
recognition(Liermannetal.,2023),andstreaming
predictfuturevoiceactivities. Tosupportthis, in
automatic speech recognition (ASR) (Choi et al.,
additiontotheattentivelisteningdialoguedataset
2024) have been considered. Notably, a recent
mentionedearlier,weincorporatedadditionaltrain-
model(Choietal.,2024)wasevaluatedonareal-
ingdataatthisstage. Usingthesameconfiguration
world imbalanced dataset, demonstrating reason-
asERICA,werecordeddataacrossvariousscenar-
ableperformancewithF1-scoresof26%and22%
ios,suchasjobinterviews(Inoueetal.,2020a)and
forcontinuerandassessmentbackchannels,respec-
first-meetingdialogues(Inoueetal.,2022). These
tively, in a frame-wise manner. This work also
diversetasksprovidedifferentdialoguestyles,en-
proposestheincorporationofmulti-tasklearning
hancingtheVAPmodel’srobustnessandenabling
forbothbackchannelpredictionandVAPtasks.
ittoadapttovariousbehaviors,includingbackchan-
nels. In total, the pre-training data amounted to
3 Dataset
about 35 hours, which includes the training set
Weemploytwotypesofdatasets: onespecialized from the aforementioned backchannel prediction
for backchannel prediction and the other for pre- dataset.
trainingtheproposedmodel. Notethatallthedia-
4 TaskDefinition
loguedatasetswereinJapanese.
Inthiswork,weaddresstwodistinctbackchannel
3.1 AttentiveListeningDataset
predictiontasksasoutlinedbelow.
We have collected spoken dialogue data using a
4.1 TimingPrediction
Wizard-of-Oz (WOZ) setup. In this experiment,
the android ERICA (Inoue et al., 2016) was em- Theprimaryobjectiveofthistaskistopredictthe
ployed,withahumanoperatorremotelycontrolling occurrenceofabackchannel,framingitasabinary
it,whichwastransmittedandplayedthroughER- classification problem. We manually annotated
ICA’s speaker system (Figure 2). The dialogue thebackchannelsintheaforementioneddialogue
task focused on attentive listening, where human dataset, identifyingtwodistincttypesofshortto-
participantssharedpersonalexperiences,andER- kens as backchannels: continuer and assessment.
ICA actively engaged as a listener (Inoue et al., Thecontinuertokensincludeexpressionssuchas
2020b; Lala et al., 2017). This task was advan- “un” and “hai” in Japanese, which correspond to
tageous because it allowed for the collection of “yeah”and“right”inEnglish. Ontheotherhand,t Original VAP This work
Backchannel Backchannel
Transcript “Un” “Oh” Current voice activity Future voice activity Future backchannel
(VAD) (VAP) (BC)
500 msec.
Positive Positive
Label Negative Negative Negative
(continuer) (assessment)
VAD linear VAP linear BClinear
Figure3: Definitionofpositive(backchannel)andnega-
tive(non-backchannel)frames
Cross-attention Cross-attention
transformer transformer
the assessment tokens include utterances such as
Self-attention Transformer Self-attention Transformer
“he-” and “oh” in Japanese, equivalent to “wow”
and“oh”inEnglish. Itisimportanttonotethatin
CPC CPC
the current task, we do not differentiate between
thesetwotokentypes,whereassuchadistinctionis
madeinthesecondtask. Tofacilitatetheimplemen- Participant 1 audio signal Participant 2 audio signal
tation of the model in real-time spoken dialogue Figure4: ArchitectureoftheVAPmodel
systems, we marked the positive sample frames
as occurring 500 milliseconds before the actual
backchannels. The definition of timing remains
backchannelutterances,asillustratedinFigure3.
consistentwiththeprevioustask,asillustratedin
Thetotalnumberofannotatedbackchannelut-
Figure3.
terances amounted to 13,601, with a cumulative
durationof5,912.6seconds. Theseweresplitinto
5 ProposedMethod
11,371utterancesfortraining,1,139forvalidation,
and1,091fortesting. Forthenegativesamples,the In this section, we begin by explaining the voice
cumulativetimeofnon-backchannelsegmentswas activityprojection(VAP)model,whichservesas
56,467.3 seconds, resulting in a ratio of approxi- the foundational model and is trained to predict
mately10%forpositivesamples. futurevoiceactivitiesbyusingthelargestspoken
dialoguedatasetexplainedinSection3.2. Follow-
4.2 TimingandTypePrediction ingthat,wediscusshowtoadaptapre-trainedVAP
modelforuseinthecurrentbackchannelprediction
Inthesecondtask,thepredictionprocessbecomes
task,usingthedataintroducedinSection3.1.
more refined by incorporating different types of
backchannels. Although numerous definitions of
5.1 VoiceActivityProjection
backchannel types or categories exist in prior re-
search, we adopt the two basic types: continuers The VAP model employed in this study is con-
and assessments, as defined earlier. This distinc- structeduponaTransformer-basedarchitecturede-
tion is crucial for conveying different listener in- signedtoemulatehuman-likepredictivecapabili-
tentions,andmostpreviousstudieshaveprimarily ties. AsillustratedinFigure4,thearchitectureof
addressedcontinuers,asassessmentbackchannels theVAPmodelprocessesstereoaudiosignalsorigi-
may require comprehension of both the prosodic natingfromtwoparticipants2. Notethatthismodel
andlinguisticaspectsoftheuser’sutterances. integrates the listener’s audio as one of the input
channels. This allows self-generated backchan-
After reclassifying the previously annotated
nelstobefedbackintothemodel,distinguishing
backchannelsintothesetwocategories,wefound
it from other existing models. This functionality
that there were 10,081 instances of continuers
playsacrucialroleinpreventingmultipleconsecu-
and 3,506 instances of assessment backchannels.
tivebackchannels,whichmayappearunnatural.
Thisratiomeansthatthepredictionofassessments
Each audio channel is initially processed inde-
seemstobemoredifficultthanthoseforcontinuers.
pendentlythroughaContrastivePredictiveCoding
Itisimportanttonotethat14tokenscouldnotbe
(CPC) audio encoder and a channel-wise Trans-
classifiedintoeithercategory,andthustheywere
former. The CPC was pre-trained with the Lib-
excludedfromthistask. Consequently,theclassifi-
cationprobleminthistaskbecomesathree-class
2Adetailedexplanationofthemodel’sarchitectureispro-
classification: continuers, assessments, and non- videdinapreviouswork(Inoueetal.,2024a)rispeechdataset(Riviereetal.,2020)andisfrozen in binary classification tasks, such as predicting
during the VAP training. The resulting outputs the presence or absence of a backchannel (Sec-
are then input into a cross-attention Transformer, tion4.1andSection6.1),thedimensionalitywould
where one channel serves as the query, while the be2. Conversely,inmulti-classclassificationtasks,
other functions as the key and value. The output, whichinvolvepredictingboththetimingandtype
aconcatenationofbothchannels,producesa512- ofbackchannel(Section4.2andSection6.2),the
dimensional vector. Note that we used the same dimensionalitywouldexceed3.
parametersasdefinedintheoriginalwork(Inoue
etal.,2024a)wherethenumbersoflayersforthe 6 Experiment
channel-wiseandthecross-attentionTransformers
To evaluate the effectiveness and applicability of
were 1 and 3, and the number of attention heads
theproposedmethod,weconductedthefourexper-
was4,respectively.
imentsdescribedbelow.
Theconcatenatedvectorissubsequentlypassed
throughlinearlayersfortwodistincttasks: voice
6.1 TimingPrediction
activityprojectionandvoiceactivitydetection. The
primary task, voice activity projection, is repre- The first experiment focuses on the initial tasks
sented as a 256-dimensional vector that predicts outlined in Section 4.1, which involve a binary
the future two-second voice activity of the two classificationofbackchannelsornon-backchannels.
participants(EkstedtandSkantze,2022b). Incon- Wepreparedseveralcomparativemethods,includ-
trast, the voice activity detection task focuses on ingrandomclassification(alwaysoutputspositive),
the current frame, generating two binary vectors as detailed below: (i) Baseline consists solely of
thatrepresentthepresentvoiceactivity. the audio encoder CPC and a linear head. While
othermethodsinthisstudyfreezetheCPCduring
5.2 Fine-TuningforBackchannelPrediction
training,thisbaselineapproachfine-tunestheCPC
Following the pre-training of the VAP model, an modelitself. Asthetwoinputaudiochannelsare
additional trainingphase isconducted using data separately fed into the CPC, their distinct output
specific to backchannel prediction. To facilitate vectorsareconcatenatedandthenpassedtothelin-
this,anewlinearlayerisintroducedontopofthe earhead. (ii)STw/oPTreferstothesingle-task
VAPmodel,complementingtheexistinglayersfor (ST)model,wherethelossfunctiononlyincludes
VAP and VAD, as depicted in Figure 4. The loss L fromEquation(1). Moreover,thismodeldoes
bc
functionforthisfine-tuningprocess,denotedasL, notinvolveanypre-training(PT)oftheVAPmodel.
isformulatedasfollows: (iii)STw/PTintroducespre-trainingoftheVAP
modelinadditiontothesingle-tasklearning. (iv)
L = αL +β L +γL , (1)
vap vad bc MTw/PTrepresentstheproposedmethod,which
incorporatesbothmulti-task(MT)learning,asde-
where α, β, and γ are the hyperparameters used
scribedinEquation1, andthepre-trainingofthe
toadjustthebalancebetweenthethreetasks,with
VAP model. The evaluation metrics used are F1-
γ typicallyassignedahighervalueduetothepri-
score,precision,andrecall,calculatedinaframe-
mary focus on backchannel prediction. The first
wisemanner,andtheF1-scoreisthemostpriority
two terms are consistent with those used in the
indicator. ForthehyperparametersinEquation(1),
originalVAPmodel(EkstedtandSkantze,2022b),
to emphasizethe importanceof backchannelpre-
while the final term, L , is newly introduced in
bc diction,weempiricallysetthemasα = 1,β = 1,
thiswork. Thistermrepresentsthecross-entropy
and γ = 5. Additionally, to mitigate the impact
lossassociatedwithbackchannelpredictionandis
of the imbalanced dataset, we adjusted the loss
definedas:
weight,assigningaweightfivetimeslargertoposi-
tive(backchannel)samplescomparedtonegative
L = −logσ(o (r )), (2)
bc bc bc
(non-backchannel)samples.
where o represents the output from the linear Table1presentsasummaryoftheresultsforthis
bc
headassociatedwithbackchannelprediction,and task. Firstly,alltrainedmethodsdemonstratedsig-
r denotes the reference label. It is important nificantlyhighterscoreswhencomparedtotheran-
bc
to note that the dimensionality of these vectors domapproach. Inparticular,theproposedmethod
is dependent on the specific task. For instance, achievedthehighestscoresinboththeF1scoreandFigure5: Exampleofbackchanneltimingprediction. Thetopsectionrepresentsthelistener’sactivity(orange:
backchannel), the middle shows the speaker’s speech, and the bottom section illustrates the model’s predicted
probabilitiesforbackchanneloccurrence.
Table1: Resultontimingprediction(ST:Single-task, Table2: Resultontimeandtypepredictiononcontin-
MT:Multi-task,PT:Pre-training) uer(ST:Single-task,MT:Multi-task,PT:Pre-training)
Method F1-score Precision Recall Method F1-score Precision Recall
Random 13.76 7.39 100.00 Random 10.19 5.37 100.00
Baseline 36.37 26.43 58.32 Baseline 34.13 26.59 47.63
STw/oPT 36.34 25.04 66.24 STw/oPT 36.10 28.65 48.77
STw/PT 41.65 31.31 62.18 STw/PT 36.47 29.08 48.90
MTw/PT 42.85 32.52 62.80 MTw/PT 38.11 29.89 52.58
Table 3: Result on time and type prediction on as-
precisionmetrics. Thesefindingssuggestthatboth
sessment (ST: Single-task, MT: Multi-task, PT: Pre-
pre-trainingandmulti-tasklearningplayacrucial
training)
role in improving backchannel prediction perfor-
mance, indicating that this task requires a more Method F1-score Precision Recall
robustapproachthanconventionaltarget-specific Random 3.57 1.82 100.00
trainingorfine-tuning. Moreover,theVAPmodel, Baseline 19.74 32.71 14.13
alongwithitsoriginallossfunction, exhibitsbet- STw/oPT 23.72 26.11 21.73
tergeneralizabilityandapplicabilitytoothernon- STw/PT 30.09 30.36 29.82
linguisticbehaviorpredictions,suchasthecurrent MTw/PT 31.76 29.95 33.81
backchannelpredictiontask.
Figure5illustratesasampleoutputgeneratedby
themodel. Asshowninthegraph,evenwhilethe performance, with the proposed method achiev-
Bluespeakerisstillspeaking,themodeliscapable ing the highest F1-score. When comparing these
ofpredictingmultiplebackchannelpointsjustprior twotypes,asanticipated,thepredictionofassess-
totheiroccurrence. ment backchannels yielded lower scores. While
random prediction offers no meaningful insight,
6.2 TimingandTypePrediction theproposedmethodexceededanF1-scoreof30.
Thesecondexperimentinvolvesthepredictionof Compared to a recent similar study (Choi et al.,
backchanneltypes,asoutlinedinSection4.2. We 2024),themodeldemonstratedahigherandmore
employedthesamecomparativemethodsasinthe promising score. However, given the variations
previousexperiment,butadjustedtheoutputdimen- in the datasets employed and the differences in
sionofthelinearheadfrom2to3toaccommodate model sizes, a direct comparison under identical
the classification of continuers, assessments, and conditionsischallenging. Nevertheless,suchcom-
non-backchannels. Theevaluationmetricremained parisonsshouldbepursuedinfutureresearch.
unchanged;however,weconductedseparateevalu- Figure 6 illustrates a sample output generated
ationsforcontinuersandassessments. bythemodel. Inthisexample,thelisteneruttered
Table 2 and Table 3 present the outcomes of twocontinuerbackchannels(orange)followedby
this task. As with the previous results, both ta- anassessment(pink). Themodelcancorrectlypre-
blesdemonstratethatthecombinationofmulti-task dictthefirsttwocontinuesandthenalsoproperly
learning and pre-training significantly enhanced predicttheassessment. Fromthisresult,themodelFigure6: Exampleofmodelpredictionsforbackchanneltimingandtype. Thetoptwosectionsshowthelistener
(orange: continuer, pink: assessment)andspeakeractivities. Thethirdsectiondisplaysthemodel’sprediction
probabilitiesforcontinuer(orange),whilethebottomsectionshowstheprobabilitiesforassessment(pink).
Original(before) Pitchflattened(after) Table4: Pitchandintensityflatteningresult
4096 4096
2048 2048 F1-score
1024 1024
512 512 Manipulation
256 256 Continuer Assessment
128 128
64 64 None(original) 38.11 31.76
0 0 0.5 1 1.5 2 2.5 3 3.5 4 0 0 0.5 1 1.5 2 2.5 3 3.5 4
Time Time
Pitchflat 37.20(-0.91) 31.09(-0.67)
4096 4096
2048 2048 Intensityflat 35.48(-2.63) 28.73(-3.03)
1024 1024
512 512
256 256
128 128
64 64
0 0 0.5 1 1.5 2 2.5 3 3.5 4 0 0 0.5 1 1.5 2 2.5 3 3.5 4
Time Time beforeandafterapplyingtheflatteningmanipula-
Figure 7: Input example of pitch flattening test (Top: tionsacrossthethreeclassificationmodels. Dueto
Spectrogram,Bottom: AutomaticallyestimatedF0)
space limitations, we report only the F1-score of
theproposedmodel(MTw/PT).
Original(before) Intensityflattened(after)
Table 4 presents the results of this experiment.
0.15 0.15
0.10 0.10 Overall, neither manipulation significantly de-
0.05 0.05
0.00 0.00 graded performance, suggesting that the model
0.05 0.05
may rely more heavily on other factors, such as
0.10 0.10
0.15 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 0.15 0 0.5 1 1.5 2 2.5 3 3.5 4 4.5 linguisticinformation. Bothtypesofbackchannels
Time (s) Time (s)
exhibitedasimilartrend,withintensityflattening
Figure8: Inputexampleofintensityflatteningtest
causing greater degradation than pitch flattening.
Thisfindingindicatesthatthecurrentbackchannel
can be trained properly to predict both two types predictionmodelcapturestheintensitydynamicsof
ofbackchannels. precedinguserutterancesmoreeffectively. When
comparingthetwotypesofbackchannels,theas-
6.3 ProsodySensitivity
sessmentrevealedahigherdependenceonintensity,
Wefurtherexaminedtheextenttowhichthemodel thoughthedifferencewasnotsubstantial.
dependsonprosodicinformation. Previouswork
ontheVAPmodelconductedasimilarexperiment 6.4 Real-timeProcessingPerformance
byflatteningthepitch(Figure7)andintensity(Fig-
To validate the applicability of live spoken dia-
ure8)ofthetestinput(EkstedtandSkantze,2022a).
loguesystems,wealsoexaminedtherelationship
Inthisstudy,wesimilarlyutilizedPraat3 toflatten
between the model’s input context length and its
bothpitchandintensity,respectively. Ifsuchma-
predictionperformance. AstheCPCaudioencoder
nipulations significantly degrade performance, it
is composed of an autoregressive model, we pro-
would suggest that the model both relies on and
videdtheentirecontextaudioinputtotheencoder.
effectivelycapturestheprosodicinformation. We
Subsequently,weconstrainedtheinputlengthfor
subsequently analyzed the performance changes
the Transformer layers. In addition, we adjusted
3https://www.fon.hum.uva.nl/praat/ theframerateto10Hz,whichissufficientforreal-
zH
zH
edutilpmA
zH
zH
edutilpmATable5: Real-timeprocessingperformance(RTF:Real-
timefactor)
Context F1-score
RTF
[sec.] Continuer Assessment
20 36.17 28.75 0.229
10 36.51 30.46 0.220
5 36.57 30.08 0.194
3 35.79 29.51 0.172
1 35.25 27.67 0.157
Input
timepredictionsystems,andretrainedthemodels
Output (Continuer)
accordingly. Therefore,notethattheresultsinthis
section would be different from the ones so far.
Inthisexperiment,weemployedthesecondtask,
Output (Assessment)
whichinvolvespredictingboththetimingandtype
ofbackchannels. Forthisevaluation,onlyaCPU
wasutilized,specificallyanIntelCorei7-11700@
2.50GHz.
Figure 9: A conversational agent with VAP-based
The result for the continuer and assessment backchannelpredictionanditsGUIvisualization
backchannelsinthedifferentinputcontextlengths
ispresentedinTable5. Also,inthisexperiment,we
experiment with this dialogue system to evaluate
onlyreportedtheF1-scoreoftheproposedmodel
thenaturalnessandeffectivenessofthebackchan-
(MT w/PT). Overall, due to the compact design
nelgenerationsystem.
oftheVAPmodel,thereal-timefactor(RTF)was
consistentlybelow1.0inallcases,indicatingthat
8 Conclusion
real-time processing is achieved. Regarding the
effectofinputcontextlengthontheTransformer This paper presents a method for real-time, con-
layers, approximately 5 seconds of input context tinuousbackchannelpredictionusingafine-tuned
yieldedoptimalresultsforbothtypesofbackchan- Voice Activity Projection (VAP) model. Our ap-
nels. When comparing the two types, while the proachcombinespre-trainingonalargerdialogue
continuer backchannels could be predicted even datasetwithfine-tuningonaspecializedbackchan-
witha1-secondinputcontext,theperformancefor neldataset,leveragingtheVAParchitecture’sgen-
assessment backchannels decreased significantly eralizability. Experimentalresultsshowedthatour
withshortercontexts,suchas1or3seconds. This two-stageandmulti-tasktrainingprocessimproves
disparitysuggeststhatthepredictionofassessment themodel’sabilitytopredictboththetimingand
backchannelsrequiresalongerinputcontext. typeofbackchannels,demonstratingitsadaptabil-
ity to real-world, unbalanced data. We also vali-
7 IntegrationwithaCGAgent dated the model for real-time use, confirming its
effectivenessinlivesystemswithoutcompromis-
Based on the results from the previous experi-
ingaccuracy,especiallyforcontinuerbackchannels.
ment, we have developed a VAP-based real-time
The results also highlight the need for longer in-
backchannel prediction system and implemented
putcontextsforaccurateassessmentbackchannel
itwithaconversationalCGagent4. Figure9illus-
predictions.
trates the system in operation with the agent, as
Thisstudyrepresentsastepforwardinenhanc-
wellasitsreal-timeGUIvisualizationtool. Note
ingconversationalagents’interactivitybyprovid-
thatsincethisbackchannelgenerationsystemoper-
ing a more human-like and responsive backchan-
atesindependentlyoftheinterface,itcanbeapplied
nelingsystem. Futureresearchwillconcentrateon
tovariousotherinterfaces,includingphysicallyem-
evaluatingtheeffectivenessofthebackchannelgen-
bodiedrobots. Weplantoconductauserdialogue
erationsystemthroughuserdialogueexperiments,
aswellasfurtherrefiningbackchannelprediction
4CG-CATakumi(c)2023byNagoyaInstituteofTechnol-
ogy,MoonshotR&DGoal1AvatarSymbioticSociety formorecomplexconversationalcontexts.Acknowledgments reactivetokensinEnglish,Japanese,andMandarin.
Journalofpragmatics,26(3):355–387.
This work was supported by JST PREST JP-
MJPR24I4,JSTMoonshotR&DJPMJPS2011,and HerbertHClark.1996. Usinglanguage. Cambridge
UniversityPress.
JSPSKAKENHIJP23K16901.
YasuharuDen,NaoYoshida,KatsuyaTakanashi,and
Limitations
HanaeKoiso.2011. AnnotationofJapaneseresponse
tokensandpreliminaryanalysisontheirdistribution
ThisstudywasevaluatedsolelyonaJapanesedia-
inthree-partyconversations. InInternationalConfer-
loguedataset,whichlimitsthegeneralizabilityof
enceonSpeechDatabaseandAssessments(Oriental
themodeltootherlanguages. Futureworkshould COCOSDA),pages168–173.
assessitsperformanceoncommonotherdatasets
like Switchboard to ensure broader applicability. David DeVault, Ron Artstein, Grace Benn, Teresa
Dey,EdFast,AlesiaGainer,KallirroiGeorgila,Jon
Additionally,whileourmodelshowspromisefor
Gratch,ArnoHartholt,MargauxLhommet,GaleLu-
real-time backchannel prediction, it has not been
cas,StacyMarsella,FabrizioMorbini,AngelaNazar-
evaluatedinpracticalsettingswithconversational ian,StefanScherer,GiotaStratou,AparSuri,David
agents or robots. Future experiments involving Traum,RachelWood,YuyuXu,AlbertRizzo,and
Louis P. Morency. 2014. SimSensei Kiosk: A vir-
userinteractionswithsuchsystemsarenecessary
tualhumaninterviewerforhealthcaredecisionsup-
toevaluatethemodel’seffectivenessanduserim-
port. In International Conference on Autonomous
pressionsinreal-worldscenarios. Agents and Multi-Agent Systems (AAMAS), pages
1061–1068.
EthicalConsiderations
KentDrummondandRobertHopper.1993. Backchan-
Intheprocessofcollectingdialoguedata,allpar- nelsrevisited: Acknowledgmenttokensandspeak-
ticipants were informed about the purpose of the ershipincipiency. ResearchonlanguageandSocial
Interaction,26(2):157–177.
research,andtheirexplicitconsentwasobtainedfor
the use of their data. The data collection process
ErikEkstedtandGabrielSkantze.2022a. Howmuch
was designed to ensure the protection of partici- doesprosodyhelpturn-taking? Investigationsusing
pants’privacy,andanypersonalinformationwas voice activity projection models. In Annual Meet-
ingoftheSpecialInterestGrouponDiscourseand
anonymized or excluded from the dataset to pre-
Dialogue(SIGdial),pages541–551.
vent identification. The study was conducted in
accordance with ethical guidelines, and approval ErikEkstedtandGabrielSkantze.2022b. VoiceActivity
wasobtainedfromtheappropriateethicscommit- Projection: Self-supervisedlearningofturn-taking
events. InINTERSPEECH,pages5190–5194.
tee prior to data collection. The participants’ pri-
vacy and confidentiality were strictly maintained
Shinya Fujie, Kenta Fukushima, and Tetsunori
throughouttheresearchprocess. Kobayashi. 2005. Back-channel feedback genera-
tion using linguistic and nonlinguistic information
and its application to spoken dialogue system. In
References INTERSPEECH,pages889–892.
AmaliaIstiqlaliAdiba,TakeshiHomma,andToshinori
KoheiHara,KojiInoue,KatsuyaTakanashi,andTatsuya
Miyoshi.2021. Towardsimmediatebackchannelgen-
Kawahara. 2018. Prediction of turn-taking using
erationusingattention-basedearlypredictionmodel.
multitasklearningwithpredictionofbackchannels
In International Conference on Acoustics, Speech
andfillers. InINTERSPEECH,pages991–995.
andSignalProcessing(ICASSP),pages7408–7412.
KojiInoue,KoheiHara,DiveshLala,KentaYamamoto,
Philippe Blache, Massina Abderrahmane, Stéphane
ShizukaNakamura,KatsuyaTakanashi,andTatsuya
Rauzy, andRoxaneBertrand.2020. Anintegrated
Kawahara.2020a. Jobinterviewerandroidwithelab-
modelforpredictingbackchannelfeedbacks. InIn-
oratefollow-upquestiongeneration. InInternational
ternationalConferenceonIntelligentVirtualAgents
ConferenceonMultimodalInteraction(ICMI),pages
(IVA),pages1–3.
324–332.
Yong-SeokChoi,Jeong-UkBang,andSeungHiKim.
2024. Jointstreamingmodelforbackchannelpredic- KojiInoue,Bing’erJiang,ErikEkstedt,TatsuyaKawa-
tionandautomaticspeechrecognition. ETRIJournal, hara,andGabrielSkantze.2024a. Multilingualturn-
46(1):118–126. takingpredictionusingvoiceactivityprojection. In
Joint International Conference on Computational
PatriciaMClancy,SandraAThompson,RyokoSuzuki, Linguistics, Language Resources and Evaluation
andHongyinTao.1996. Theconversationaluseof (LREC-COLING),pages11873–11883.KojiInoue,Bing’erJiang,ErikEkstedt,TatsuyaKawa- 2017. Attentivelisteningsystemwithbackchannel-
hara, and Gabriel Skantze. 2024b. Real-time and ing,responsegenerationandflexibleturn-taking. In
continuousturn-takingpredictionusingvoiceactiv- AnnualMeetingoftheSpecialInterestGrouponDis-
ityprojection. InInternationalWorkshoponSpoken courseandDialogue(SIGDIAL),pages127–136.
DialogueSystemsTechnology(IWSDS).
WenckeLiermann,Yo-HanPark,Yong-SeokChoi,and
KojiInoue,DiveshLala,andTatsuyaKawahara.2022.
Kong Lee. 2023. Dialogue act-aided backchannel
Canarobotlaughwithyou?: Sharedlaughtergener-
predictionusingmulti-tasklearning. InInternational
ationforempatheticspokendialogue. Frontiersin
Conference on Empirical Methods in Natural Lan-
RoboticsandAI,9.
guageProcessing(EMNLP)Findings,pages15073–
15079.
Koji Inoue, Divesh Lala, Kenta Yamamoto, Shizuka
Nakamura,KatsuyaTakanashi,andTatsuyaKawa-
JenniferAMaddrellandGingerSWatson.2012. Thein-
hara.2020b. Anattentivelisteningsystemwithan-
fluenceofbackchannelcommunicationoncognitive
droidERICA:ComparisonofautonomousandWOZ
load. InTheNextGenerationofDistanceEducation:
interactions. InAnnualMeetingoftheSpecialInter-
UnconstrainedLearning,pages171–180.Springer.
est Group on Discourse and Dialogue (SIGDIAL),
pages118–127.
L. P. Morency, I. D. Kok, and J. Gratch. 2010. A
Koji Inoue, Pierrick Milhorat, Divesh Lala, Tianyu probabilisticmultimodalapproachforpredictinglis-
Zhao, and Tatsuya Kawahara. 2016. Talking with tenerbackchannels. InternationalConferenceonAu-
ERICA, an autonomous android. In Annual Meet- tonomousAgentsandMulti-AgentSystems(AAMAS),
ingoftheSpecialInterestGrouponDiscourseand 20(1):70–84.
Dialogue(SIGDIAL),pages212–215.
Louis-PhilippeMorency,IwanDeKok,andJonathan
RyoIshii,XutongRen,MichalMuszynski,andLouis- Gratch.2008. Predictinglistenerbackchannels: A
PhilippeMorency.2021. Multimodalandmultitask probabilisticmultimodalapproach. InInternational
approachtolistener’sbackchannelprediction: Can workshoponintelligentvirtualagents(IVA),pages
prediction of turn-changing and turn-management 176–190.
willingnessimprovebackchannelmodeling? InIn-
ternationalConferenceonIntelligentVirtualAgents
KeikoOchi,KojiInoue,DiveshLala,andTatsuyaKawa-
(IVA),pages131–138.
hara.2024. Entrainmentanalysisandprosodypre-
dictionofsubsequentinterlocutor’sbackchannelsin
ViditJain,MaitreeLeekha,RajivRatnShah,andJainen-
dialogue. InINTERSPEECH,pages462–466.
draShukla.2021. Exploringsemi-supervisedlearn-
ing for predicting listener backchannels. In Con-
DeryaOzkanandLouis-PhilippeMorency.2011. Mod-
ference on Human Factors in Computing Systems
elingwisdomofcrowdsusinglatentmixtureofdis-
(CHI).
criminativeexperts. InAnnualMeetingoftheAsso-
Jin Yea Jang, San Kim, Minyoung Jung, Saim Shin, ciationforComputationalLinguistics(ACL),pages
and Gahgene Gweon. 2021. BPM_MT: Enhanced 335–340.
backchannelpredictionmodelusingmulti-tasklearn-
ing. In International Conference on Empirical Yo-HanPark,WenckeLiermann,Yong-SeokChoi,and
MethodsinNaturalLanguageProcessing(EMNLP), Kong Joo Lee. 2024. Improving backchannel pre-
pages3447–3452. diction leveraging sequential and attentive context
awareness. In Conference of the European Chap-
TatsuyaKawahara,MikiUesato,YoshinoKoichiro,and teroftheAssociationforComputationalLinguistics
KatsuyaTakanashi.2015. Towardadaptivegenera- (EACL)Findings,pages1689–1694.
tionofbackchannelsforattentivelisteningagents. In
InternationalWorkshoponSpokenDialogueSystems Morgane Riviere, Armand Joulin, Pierre-Emmanuel
Technology(IWSDS). Mazaré, and Emmanuel Dupoux. 2020. Unsuper-
visedpretrainingtransferswellacrosslanguages. In
TatsuyaKawahara,TakashiYamaguchi,KojiInoue,Kat-
InternationalConferenceonAcoustics,Speechand
suya Takanashi, and Nigel G Ward. 2016. Predic-
SignalProcessing(ICASSP),pages7414–7418.
tionandgenerationofbackchannelformforattentive
listeningsystems. InINTERSPEECH,pages2890–
Robin Ruede, Markus Müller, Sebastian Stüker, and
2894.
Alex Waibel. 2017a. Enhancing backchannel pre-
dictionusingwordembeddings. InINTERSPEECH,
Hanae Koiso, Yasuo Horiuchi, Syun Tutiya, Akira
pages879–883.
Ichikawa, and Yasuharu Den. 1998. An analysis
ofturn-takingandbackchannelsbasedonprosodic
andsyntacticfeaturesinJapanesemaptaskdialogs. Robin Ruede, Markus Müller, Sebastian Stüker, and
Languageandspeech,41(3-4):295–321. Alex Waibel. 2017b. Yeah, right, uh-huh: A deep
learning backchannel predictor. In International
DiveshLala, PierrickMilhorat, KojiInoue, Masanari WorkshoponSpokenDialogueSystemsTechnology
Ishida, Katsuya Takanashi, and Tatsuya Kawahara. (IWSDS),pages247–258.Marc Schroder, Elisabetta Bevacqua, Roddy Cowie,
Florian Eyben, Hatice Gunes, Dirk Heylen, Mark
Ter Maat, Gary McKeown, Sathish Pammi, Maja
Pantic,etal.2011. Buildingautonomoussensitive
artificiallisteners. IEEETransactionsonAffective
Computing,3(2):165–183.
Chen-HsinTangandGraceQiaoZhang.2009. Acon-
trastivestudyofcomplimentresponsesamongAus-
tralianEnglishandMandarinChinesespeakers. Jour-
nalofpragmatics,41(2):325–345.
Nigel Ward and Wataru Tsukahara. 2000. Prosodic
featureswhichcueback-channelresponsesinEnglish
and Japanese. Journal of pragmatics, 32(8):1177–
1207.
Deanna Wong and Pam Peters. 2007. A study of
backchannelsinregionalvarietiesofEnglish,using
corpusmark-upasthemeansofidentification. Inter-
nationalJournalofCorpusLinguistics,12(4):479–
510.