XGEN-MM-VID (BLIP-3-VIDEO): YOU ONLY NEED
32 TOKENS TO REPRESENT A VIDEO EVEN IN VLMS
MichaelS.Ryoo,HongluZhou,ShrikantKendre,CanQin,LeXue,ManliShu,
SilvioSavarese,RanXu,CaimingXiong,JuanCarlosNiebles
SalesforceAIResearch
mryoo@salesforce.com
ABSTRACT
We present xGen-MM-Vid (BLIP-3-Video): a multimodal language model for
videos, particularly designed to efficiently capture temporal information over
multiple frames. BLIP-3-Video takes advantage of the ‘temporal encoder’ in
additiontotheconventionalvisualtokenizer,whichmapsasequenceoftokens
over multiple frames into a compact set of visual tokens. This enables BLIP-
3-Video to use much fewer visual tokens than its competing models (e.g., 32
vs. 4608 tokens). We explore different types of temporal encoders, including
learnablespatio-temporalpoolingaswellassequentialmodelslikeTokenTuring
Machines. WeexperimentallyconfirmthatBLIP-3-Videoobtainsvideoquestion-
answering accuracies comparable to much larger state-of-the-art models (e.g.,
34B),whilebeingmuchsmaller(i.e.,4B)andmoreefficientbyusingfewervisual
tokens.Theprojectwebsiteisathttps://www.salesforceairesearch.com/
opensource/xGen-MM-Vid/index.html
1 INTRODUCTION
LargeVision-LanguageModels(VLMs),benefitingfromlarge-scaleimage-texttraining,havebeen
dominatingthefieldofcomputervision. Recently,open-sourceVLMsareobtainingstrongresults,
despitehavingmuchsmallersizethanthecommercialmodels(e.g.,4Bvs. Trillions).
Further, in addition to such VLMs trained with images, VLMs for videos are becoming increas-
ingly popular. The key component in a VLM for videos is the temporal abstraction of tokens
over multiple frames. Models like Video-ChatGPT (Maaz et al., 2024) and PLLaVA (Xu et al.,
2024a) rely on a simple spatial/temporal pooling on top of image frame-level tokens to repre-
sent the entire video. Some models rely on a separate video encoder to capture temporal infor-
mation in videos (Lin et al., 2023). Similarly, some models use additional convolutional lay-
ers (or Transformer layers) over frames to reduce their representation size (e.g., Video-LLaMA
(Zhang et al., 2023), Kangaroo (Liu et al., 2024)). Approaches that simply collect all the vi-
sualtokensfromalltheframes(e.g.,MiniGPT4-video(Ataallahetal.,2024),LLaVA-NeXT(Li
etal.,2024b),Tarsier(Wangetal.,2024a)andLLaVA-OneVision(Wangetal.,2024a))alsohave
been very popular recently, as they allow capturing all the details from the frame-level tokens.
However,thisoftenmakesthenumber
Tarsier-34B Tarsier-34B
of tokens for video to be very huge
(e.g., thousands even for 8 frames). 78 BLIP-3-Video-4B-32t 78 BLIP-3-Video-4B-32t
Such large number of video tokens BLIP-3-Video-4B-16t BLIP-3-Video-4B-16t
76 76
couldbecriticalforlongervideosas
theLLMcomputationisquadraticto 74 LLaVA-NeTa Xr Ts -Vie -r 3- 27 BB 74 Tarsier-7B LLaVA-NeXT-V
thenumberoftotaltokens. LLaVA-OneVision-7B LLaVA-OneVision-7B
72 Video-LLaVA 72 Video-LLaVA
Inthispaper,weintroducexGen-MM-
Vid(BLIP-3-Video),whichisaneffi- 70 LLLLaaMMAA--VVIIDD--71B3B 70 LLaMAL-VLaIDM-A7B-VID-13B
Chat-UniVi Chat-UniVi
cientcompactvision-languagemodel 102 103 104 0 10 20 30 40
Number of tokens Number of parameters (billion)
withanexplicittemporalencoder,de-
Figure1: SOTAvideoVLMmodelcomparison: (Left)Num-
signedparticularlyforvideos. BLIP- berofvisualtokensvs. video-QAaccuracy. (Right)Model
3-Videoparticularlyfocusesonincor- sizevs. video-QAaccuracy.
1
4202
tcO
12
]VC.sc[
1v76261.0142:viXra
)%(
ycarucca
AQ-DVSM
)%(
ycarucca
AQ-DVSM(pre-trained) LLM
M=32
Temporal encoder
N=128
image image image image … image image Text encoder
encoder encoder encoder encoder encoder encoder
How does the subject's interaction with the
surrounding tools and objects contribute to the
primary goal of the video?
Figure2: AnillustrationoftheBLIP-3-Videomodelarchitecture. Ithastheexplicittemporalencoder
insertedtoBLIP-3.
poratingalearnable‘temporalencoder’withinit. Weexploredifferenttypesoftemporalencoder,
anddemonstratethatthemodelcanabstracteachvideointomuchfewervisualtokens(e.g.,16)while
beingsuccessfulinopen-endedquestion-answeringandcaptioningtasks. Weincludeaspace-time
attentionalpoolingaswellasasequentialmodelasourtemporalencoder,relyingontokenoperations
toiterativelyabstractaseriesofframe-leveltokensintoalearnablememory.
Therehasbeenpriorworkinvestigatingtheroleofpooling(Jinetal.,2024),convolutions,andcross
attentionlayers(Zhangetal.,2023;Liuetal.,2024;Lietal.,2024c),butstudyonfullspace-time
attentionalpoolingorsequentialmodeltothisextenthasbeenlimitedinthepast. Ourobjective
inthispaperistoprovideafundamentalalternativetomorebrute-forcewayofcollectingallthe
visualtokenswhichhavebeenincreasingpopularrecently. Weexperimentallyconfirmthat16∼32
videotokensabstractedbythetemporalencoderisoftensufficienttorepresenttheentirevideofor
question-answering(Figure1).
2 BLIP-3-VIDEO
2.1 MODELARCHITECTURE
WebuildtheBLIP-3-Videomodelbasedontheimage-basedvision-languagemodel(VLM),BLIP-3
(Xueetal.,2024).
Themodelarchitectureiscomposedofthefollowingfourcomponents: (1)thevisionencoder(ViT)
takingeachframeinput,(2)theframe-leveltokenizertoreducethenumberoftokens,(3)thetemporal
encodertobuildvideo-leveltokenrepresentations,and(4)theautoregressiveLLMgeneratingoutput
textcaptionsbasedonsuchvideotokensandtextprompttokens. Figure2showsanoverview.
First,weapplyapretrainedSigLIPasthevisionencoder,designedtotakeonesingleimageframeat
atime. Perceiver-ResampleristhenappliedtomapsuchvisualtokensintoN =128visualtokens
perframe,independently. Oncethemodelhassuchvisualtokensovertime(i.e.,overmultipleframes
inthevideo),theyareprovidedtoanexplicit‘temporalencoder’. Theroleofthetemporalencoderis
tobuildavideo-leveltokenrepresentationfromsuchsequenceofimage-leveltokens,servingasa
mappingfunctionbetweenasetofN ·T imagetokenstoM videotokenswhereT isthenumberof
framesandM isaconstantnumberoftokens. Weexplorevariousformsofthetemporalencoder,
includingtemporalpoolingaswellassequentialmodels,whichwediscussfurtherinthefollowing
subsection. TheresultingtokensaregiventotheLLMtogetherwiththeencodedtexttokensina
prefixmanner,asinmanystandardVLMs.
Forcomputationalefficiency,themodeltakesuniformlysampled8framespervideo. Asaresult,in
ourmodel,ViTfirstmapsavideointo8∗729visualtokens,whichisthenmappedto8∗128visual
tokensusingPerceiver-Resampler,andthento16∼128videotokensusingthetemporalencoder.
WeusePhi-3(Abdinetal.,2024)asourLLMbackbonetakingsuchvideotokensinadditiontothe
textprompttokens. Thisenablesthemodeltotaketext+videoasaninputandgeneratetextsentences
asanoutput.
2Time
Mean pool
x x x
1 2 3
v x
(1,1) 1
Transformer
x
2
v (3,4) x 3 v (1,1) … v (3,4)
(a) Temporal pooling (b) Transformer-based
Learnable soft x x x
space-time selection 1 2 3
Sequential model
x
1
v v v v
x (1,1) (1,2) (1,3) (1,4)
2
v v v v
(2,1) (2,2) (2,3) (2,4)
x
3
v v v v
(3,1) (3,2) (3,3) (3,4)
(c) Attentional pooling (TokenLearner) (d) Sequential model (TTM)
Figure 3: Visually comparing different types of temporal encoders we explored in our model
architecture. (c)and(d)areparticularlyeffective,aswediscussfurtherintheexperiments.
2.2 TEMPORALENCODERS
Atemporalencoderisafunctionoftokens,takingN ·T tokensasaninputandreturningM tokens
asanoutput: x =f(v ).
1,...,M (1,1),...,(N,T)
Weexploredifferenttypesofencodersaspartofourmodel.Thesimplestformofthetemporalencoder
(cid:8)(cid:80) (cid:9)M
willbetemporalpooling,e.g.,summatingper-frametokensovertime: x = (v )
1,...,M t (i,t) i=1
whereM isalwaysrestrictedtobeidenticaltoN,whichwasalsousedin(Maazetal.,2024).Another
possible implementation would be the use of a temporal Transformer, modeling the entire token
sequenceandselectingthelastmtokenssimilartoMirasol3B(Piergiovannietal.,2024):
x ={Transformer(v)}N·T (1)
1,...,M N·T−M+1
Inadditiontothestraightforwardtemporalencodersmentionedabove,weexploretwoimportant
temporalencodersconsideringspace-timenatureoftokens: spatio-temporalattentionalpoolingand
sequentialmodels(Figure3).
Spatio-temporal attentional pooling: Attentional pooling allows learnable ‘soft selection’ of
multipletokensgivenalargersetoftokens. Suchattentionalpoolinghavebeenpreviouslydeveloped
forTransformers(e.g.,Perceiver(Jaegleetal.,2022)andTokenLearner(Ryooetal.,2021)),andalso
usedinearlierfoundationmodels(e.g.,CoCa(Yuetal.,2022))forimages.
Inourmodel,weuseTokenLearner(Ryooetal.,2021),makingitexplicitlyserveasourspace-time
awaretemporalencoder. Unlikepreviousper-image-frameusageofpoolingswherespatialpooling
andtemporalpoolingareappliedseparately(e.g.,Video-ChatGPT),ourtemporalencoderdirectly
takesallN ·T tokensand‘learns’tosoft-selectM informativetokensspatio-temporally. Here,N
tokenscouldbeviewedasspatialrepresentationsofaframeandwehaveT ofthem,suggestingitis
aspatio-temporalrepresentationselection.
Ourattentionalpoolinginitssimplestformisexpressedas:
x =A(V)·V
=softmax(cid:0) α(VT)(cid:1)
·V (2)
i
whereV isamatrixformedbyconcatenatinginputtokensv .ThefunctionA(·)computes
(1,1),...,(N,T)
thesummationweightsforV,performingsoftselectionoftokens. Thisisfurtherdecomposedtothe
softmaxandthefunctionα(·). InPerceiver,amatrixmultiplicationwithalatentquerytokens(i.e.,
3
snekot
Ncrossattentionwhere|Q|=m)havebeenusedtoimplementthis: α(V)=Q·VT/c. TokenLearner
usesaconvolution/MLPontopofV: α(V)=MLP (VT),whichweuseinourmodel. Thisallows
m
selectingasmallernumberoftokens(e.g.,M =32tokens).
Weexperimentallyconfirmthatsuchlearnablespatio-temporalattentionalpoolinghasadvantages
overtheconventionalapproachofnon-learnablespatialpoolingandtemporalpooling,inSection3.3.
SequentialModel: WealsodeployTokenTuringMachines(TTM)(Ryooetal.,2023)asatemporal
encoder,whichisasequentialmodelcapableoftakinganynumberofframestogenerateavideo-level
tokenrepresentation(e.g.,M =32regardlessthenumberofframes). OuruseofTTMissimilarto
itsusageinMirasol3B(Piergiovannietal.,2024),exceptthatourmodelusesTTMdirectlytoencode
asequenceofimagetokenswhileMirasol3BusesTTMtoencodeasequenceofsetsofvideotokens.
Here,wealsofurtherextendTTMbyaddingtime-stampedpositionalencodingstoembdedtheframe
indexofeachtokeninthelatentspace. Thisenablesthetokensinthe‘memory’ofTTMtopreserve
thetemporalorderinginformation,whichiscrucialwhenrepresentingcomplicatedorlongvideo
scenes. Inaddition,weuseTTMtemporalencoderina‘grouped’fashion,maintainingaseparate
memoryofsizeG=4foreachofN =128tokensovertime. Thememoryismaintainedtohave
thesizeisN ·G,andthefinaloutputfromthesequencemodelisattentionallypooledfromthefinal
memorytogiveM tokens.
2.3 TRAININGRECIPE
BLIP-3-Videofollowsathree-stagecurriculumlearning: (1)imagecaptionpretraining,(2)video
captionpretraining,and(3)videoinstructiontuning. Inallitstrainingwefreezethevisionencoder,
onlytrainingthemodelparametersafterthevisionencoder. First, wedirectlyusethepretrained
weightsfromBLIP-3(Xueetal.,2024). BLIP-3isforimagesanditdoesnotcontainweightsforthe
temporalencoder,sowerandomlyinitializethoseweights.
Asits2ndstage,themodelisthentrainedonLLaVA-Hound-DPO’svideocaptiondata(Zhangetal.,
2024b), featuring over 900k video captions. Instead of directly using the text captions provided
inLLaVA-Hound-DPO,weusedGPT-4torephrasesuchtextcaptionssothattheybecomemore
GPT-stylecaptions.
Finally,wetunedthemodelusingamixofvideoquestion-answeringdatasets,includingVideoChat-
GPT’s99k-samplevideoinstructiontuningdata(Maazetal.,2024),alongwiththetrainingsplitsof
theMSVD-QA(Xuetal.,2017),MSRVTT-QA(Xuetal.,2017),ActivityNet-QA(Yuetal.,2019),
TGIF-QA(Jangetal.,2017),andNExT-QA(Xiaoetal.,2021)datasets,whichcontain30k,149k,
32k,71k,and34ksamples,respectively. ForTGIF-QA,weonlyusedthetrainingdataassociated
with the Repeating Action and State Transition tasks. In our video instruction tuning recipe, we
employbothopen-endedandmultiple-choicevideoQAformatsforTGIF-QAandNExT-QA.For
theopen-endedvideoQAtrainingdatasourcedfromtheMSVD-QA,MSRVTT-QA,TGIF-QA,and
NExT-QAtrainingsets,weusedGPT-3.5torephrasetheoriginalsingle-wordorsingle-phraseanswer
intoanaturallanguagesentence,providingthequestionintheLLMpromptcontext. Foropen-ended
TGIF-QAandNExT-QA,wealsodoublethesamplesizebyusingboththeoriginalshort-phrase
answersandtherephrasedsentence-basedanswers. Inaddition,weaddedafilteredversionofthe
Miracaptiondataset(Juetal.,2024)forourvideoinstructiontuning. Thatis,weareusingboth
videoquestion-answeringandvideocaptioningforourfinaltraining. WeexcludedcaptionsforMira
videoslongerthanoneminute,totaling935kvideocaptionsamples.
Wetrainedourmodelwith8×H100GPUs. Forthevideocaptionpretraining,weusethebatchsize
of16perGPU,500warmupsteps,andthelearningrateof2e-5withthecosinedecay. Wetrainedthe
modelfor1epoch. ThevideoQAsft(i.e.,instructiontuning)wasdonewiththebatchsizeof4per
gpu,500warmupsteps,andthelearningrateof1e-5withthecosinedecay. Wetrainedthemodelfor
1epochinthiscaseaswell. Theentiretraining(combiningbothvideopretrainingandthesft)takes
around12hours,confirmingtheefficiencyofourmodel.
43 EXPERIMENTS AND RESULTS
3.1 MODELIMPLEMENTATIONDETAILS
WesharethemodeldetailswithBLIP-3(4B),exceptthatBLIP-3-Videohasthenewtemporalencoder
componentinitsarchitecture. Thismodeltakesthevideowiththeinputresolutionof384×384,using
SigLIPencodertomapitto729tokensperframewiththechannelsize1152. Perceiver-Resampleris
implementedwithmultiplecross-attentionlayerswiththesamechanneldim,whichisthengivento
thetemporalencoder.
TokenLearnerservingasthespatio-temporalattentionalpoolingwasimplementedusingaMLPas
theattentionfunction. Thesizeofitsinnerdimwasthenumberoftargettokens*2. Thegrouped
TTMservingasthesequentialmodeltemporalencoderwasimplementedusing4Transformerlayers
(withthechanneldimof1152)astheprocessormodulewhileusingTokenLearnersforread/write
modules. MemorysizewassettoN ∗4=512tokenstotal.
Theresulting16 ∼ 128tokensaremappedtothetextembeddingwiththechanneldimensionof
3072,beforegiventotheLLM(Phi-3).
3.2 PUBLICBENCHMARKS
We conducted experiments measuring video question-answering accuracies on multiple public
datasets. Thisincludesopen-endedanswergenerationtaskslikeMSVD-QA,aswellasmultiple
choicequestionslikeNExT-QA.Wefollowtheirstandardsettingsinallcases.
Table 1 compare open-ended question answering accuracies of BLIP-3-Video against reported
numbersofothermodels. Weusefourcommonlyusedpublicdatasets,MSVD-QA,MSRVTT-QA,
ActivityNet-QA,andTGIF-QA,followingstandardVideoLLMevaluationsettings. Notethatour
MSVD-QA and MSRVTT-QA accuracy was measured by training our model with a subset (i.e.,
Video-ChatGPTdataset-only)ofourtrainingdata,asthisallowsmoredirectcomparisontosome
ofthepriorworkandenablesmorestableresultsduetoitsdatadistribution. Weareincludingthe
modelsizeaswellasthenumberofvisualtokensinthetable. Weareabletoobservethat,despiteits
smallersize(i.e.,4Bvs. 7Bor34B),ourmodelisobtainingsuperiororcomparableperformance.
Withthetemporalencoder,BLIP-3-Videowasabletoretaintheperformancewithmuchfewertokens,
whichwediscussmoreinthefollowingsubsection. Ourresultssuggestthatnottoomanyvisual
tokensarereallynecessarytobesuccessfulontheseopen-endedquestionansweringbenchmarks,as
longaswehaveacarefullydesignedtemporalencoder.
Inaddition,weevaluatedBLIP-3-Video’sabilitytosolvemultiplechoicequestions(MCQ).Table
2 shows the results on NExT-QA. Due to the nature of its questions requiring understanding of
multipleframes,manypriormodelsusequieabitoftokens. Forinstance,GPT-4usesaminimum
of255tokensperframe. ItisinterestingthatBLIP-3-Videoachievescomparableaccuracywhile
representingtheentirevideowithonly32(or128)tokens.
3.3 ABLATIONS
Weconductedanablationcomparingdifferenttemporalencoders. Theseinclude: (1)thebasesingle
framemodel(i.e.,BLIP-3trainedwithvideos),(2)meanpoolingsimilartoVideo-ChatGPT,and
(3)transformertemporalencodersimilartoMirasol3B.Wealsotriedthe(4)vanillaTokenTuring
Machines,whichisnotthegroupedversionweuseasourtemporalencoder.
Table3showstheresult,comparingthequestion-answeringaccuraciesofdifferenttypesoftemporal
encoderswhenabstractingavideointo128tokens.Weareabletoobservethattheyalldoareasonable
job,whilesometemporalencodersaremoreeffective.
Inaddition,wecompareddifferentpoolingapproachessimilartotheonestriedinpriorworks,when
they are required to select a much smaller number of tokens (e.g., 32) from a large set of visual
tokens. Wecompareourspatio-temporalattentionalpoolingaswellasthesequentialmodelagainst
itsalternatives, including(1)fixed-window(non-learnable)space-timepoolingand(2)learnable
‘per-frame’ pooling. In particular, (2) is similar to the approach taken in LLaMA-VID (Li et al.,
2024c),whichindependentlyselectedafixednumberoftokens(e.g.,2)perframe. Table4shows
5Method Size #tokens MSVD-QA MSRVTT-QA ActivityNet-QA TGIF-QA
VideoChat(Lietal.,2023b) 7B 32 56.3/2.8 45.0/2.5 -/2.2 34.4/2.3
Video-LLaMA(Zhangetal.,2023) 7B 32 51.6/2.5 29.6/1.8 12.4/1.1 -/-
Video-ChatGPT(Maazetal.,2024) 7B 264+ 64.9/3.3 49.3/2.8 34.2/2.8 51.4/3.0
Chat-UniVi(Jinetal.,2024) 7B 112 69.3/3.7 55.0/3.1 46.1/3.3 69.0/3.8
LLaMA-VID(Lietal.,2024c) 7B 32 69.7/3.7 57.7/3.2 47.4/3.3 -
LLaMA-VID(Lietal.,2024c) 13B 32 70.0/3.7 58.9/3.3 47.5/3.3 -
Video-LLaVA(Linetal.,2023) 7B 2048 71.8/3.9 59.2/3.5 45.3/3.3 70.0/4.0
MiniGPT4-Video(Ataallahetal.,2024) 7B 2880+ 73.9/4.1 59.7/3.3 46.3/3.4 72.2/4.1
PLLaVA(Xuetal.,2024a) 7B 576+ 76.6/4.1 62.0/3.5 56.3/3.5 77.5/4.1
SlowFast-LLaVAXuetal.(2024b) 7B 3680 79.1/4.1 65.8/3.6 56.3/3.4 78.7/4.2
LLaVA-Hound-DPOZhangetal.(2024b) 7B 2048 80.7/4.1 70.2/3.7 -/- 61.4/3.5
LLaVA-OneVision∗(Wangetal.,2024a) 7B 1568 72.9/3.9 57.8/3.4 55.3/3.6 41.1/3.1
Tarsier(Wangetal.,2024a) 7B 4608+ 77.0/4.1 62.0/3.5 59.5/3.6 79.2/4.2
Tarsier∗(Wangetal.,2024a) 7B 4608 74.4/4.0 59.1/3.4 54.3/3.5 -/-
PLLaVA(Xuetal.,2024a) 34B 576+ 79.9/4.2 68.7/3.8 60.9/3.7 80.6/4.3
LLaVA-NeXT-Video∗(Lietal.,2024b) 32B 1152 73.6/4.0 56.8/3.4 58.4/3.6 73.5/4.1
Tarsier(Wangetal.,2024a) 34B 4608+ 80.3/4.2 66.4/3.7 61.6/3.7 82.5/4.4
Tarsier∗(Wangetal.,2024a) 34B 4608 79.3/4.1 62.2/3.5 61.5/3.7 -/-
BLIP-3-Video 4B 32 77.7/4.2 60.0/3.6 55.7/3.5 76.5/4.3
BLIP-3-Video 4B 128 77.9/4.3 59.7/3.6 56.9/3.6 77.1/4.3
Table1: Comparisonagainstreportednumbersofothermodelsonopen-endedquestionanswering
evaluation. Thenumberofvisualtokensarealsoreported. Thenumbersafter‘/’areanswerquality
scores. ∗ indicatesourevaluationusingthecheckpointandinferencecodeprovidedbytheauthor,
withtheidenticalvideosusedinourmodel(8framesof384×384resolution).
Method Size #tokens NExT-QA
LangRepo(Kahatapitiyaetal.,2024) 7B 3136+ 54.6
LangRepo(Kahatapitiyaetal.,2024) 12B 3136+ 60.9
Tarsier(Wangetal.,2024a) 7B 4608+ 71.6
LLoVi(Zhangetal.,2024a) 157B 1000s 67.7
IG-VLM(Kimetal.,2024) 34B 1536+ 70.9
VideoAgent(Wangetal.,2024b) GPT-4 2091+ 71.3
VideoTree(Wangetal.,2024c) GPT-4 3978+ 73.5
Tarsier(Wangetal.,2024a) 34B 4608+ 79.2
BLIP-3-Video 4B 32 76.4
BLIP-3-Video 4B 128 77.1
Table2:Comparisonagainstreportednumbersofothermodelsonmultiplechoicequestion-answering
(MCQ)benchmark.
theresults. Table5explicitlycomparestheimpactofhavingsmallervisualtokens. Weareableto
observethat32visualtokensormoregivesareasonablevideoQAaccuracy.
Speed: Reducingthenumberofvisualtokensincreasesthecomputationalefficiencyofthemodels,
asthetotalcomputationisquadratictothenumberoftokensfedtotheLLM.Wemeasuretheruntime
ofourmodelsinthetrainingsettingforthefaircomparison. Here,wereport‘samplespersecondper
GPU’.Withoutthetemporalencoder(i.e.,directlyusing1024visualtokens),themodelprocessed
3.3samplespersecond. With16/32/128tokensusingthetemporalencoder,themodelwasableto
process8.5/8.2/7.5samplespersecond.
Encoder MSVD-QA TGIF-QA ActivityNet-QA NExT-QA
1frame 71.49/4.01 72.74/4.16 51.83/3.39 72.79
Meanpooling 76.75/4.17 77.01/4.30 55.89/3.53 76.24
Transformer 76.24/4.15 76.33/4.28 55.59/3.50 76.34
VanillaTokenTuringMachine 76.42/4.15 75.80/4.26 54.45/3.48 75.42
Ours(Space-time) 77.49/4.18 76.90/4.29 56.94/3.56 76.27
Ours(Sequential) 77.86/4.20 77.10/4.31 56.66/3.56 77.07
Table3: Ablationscomparingdifferenttemporalencoders: 128tokens. ∗Aslightlydifferenttraining
recipeusingasubsetoftheentiredataset(withoutMiradata)wasusedfortheablations.
6Encoder MSVD-QA #tokens MSVD-QA TGIF-QA NExT-QA
Space-timepooling(4*8) 76.04 16tokens 76.17/4.16 76.19/4.28 75.8
Per-frame(4*8) 76.78 32tokens 77.11/4.17 77.07/4.30 76.4
Ours(Space-time) 77.71 128tokens 77.86/4.20 77.10/4.31 77.07
Ours(Sequential) 77.11 256tokens 77.67/4.18 77.35/4.31 77.06
Table4: AblationscomparingdifferentTable5: Ablationscomparingdifferent#oftokens. Ours
poolingstrategiesfor32tokens. withsequentialmodelasatemporalencoderwasused.
Question: Please describe the primary object or subject in the video, capturing their attributes, actions, positions, and movements.
GT: A white jellyfish, is seen floating and gently pulsating as it moves through the water. Its bell contracts and expands rhythmically, propelling it forward. The
tentacles trail behind, undulating with the water's movement. Throughout the sequence, the jellyfish maintains a steady, graceful motion, drifting from left to right
and slightly rotating as it progresses.
Prediction: A jellyfish, is seen swimming in the water. It moves in a slow, undulating motion, with its bell-shaped body contracting and expanding to propel itself
forward. The jellyfish's tentacles trail behind it, creating a delicate, flowing effect. Throughout the video, the jellyfish maintains a consistent upward and forward
movement, occasionally adjusting its direction slightly.
Question: Please provide a dense description of the video, covering the main subject, movements, style, backgrounds, and cameras.
GT: The video provides a breathtaking aerial view of a historic castle perched on the edge of a steep cliff. As the drone camera glides over the scene, viewers
can appreciate the castle's ancient stone walls and the surrounding lush greenery, which contrasts sharply with the rugged cliff faces. The castle, a relic of
medieval architecture, is shown in various states of preservation, with some parts reduced to ruins while others stand robustly. The video captures the castle's
strategic location, offering panoramic views of the surrounding landscape and the churning waters below.
Prediction: The video provides an aerial view of a historic fortification perched on a rugged cliff overlooking the ocean. The fort, characterized by its robust
stone walls and strategic positioning, includes various defensive structures such as cannons and barricades. As the video progresses, the camera captures the
changing light of the setting sun, casting long shadows and highlighting the textures of the stone and the surrounding landscape. The fort's layout, including
pathways and small buildings, is revealed in greater detail, showcasing its historical significance and architectural complexity.
Question: Please provide a dense description of the video, covering the main subject, movements, style, backgrounds, and cameras.
GT: A step-by-step guide on assembling a wooden bed frame, featuring a person methodically working through the construction process. Initially, the person is
seen preparing the wooden planks and arranging the necessary tools. As the video progresses, the individual meticulously attaches and secures the slats to the
frame, ensuring stability and precision. The final scenes depict the person tightening the last screws and reviewing the completed structure, demonstrating a
sense of accomplishment and attention to detail.
Prediction: A step-by-step process of assembling a piece of furniture, specifically a bed frame. The person involved is seen working on a wooden bed frame,
using various tools and components to construct the bed. The video likely serves as a tutorial, providing viewers with a visual guide on how to assemble the bed
frame themselves. The person's actions are deliberate and precise, indicating a level of expertise or familiarity with the task.
Question: Please provide a dense description of the video, covering the main subject, movements, style, backgrounds, and cameras.
GT: Two men engaged in the task of loading a barrel onto a small, yellow floatplane near a body of water. The men are dressed in casual attire suitable for
outdoor work, with one wearing gloves for handling the barrel. The plane is docked at a wooden structure, possibly a jetty or makeshift dock, and the men
coordinate their efforts to lift and maneuver the barrel into the cargo area of the aircraft. The surrounding environment is serene and natural, with a forested
backdrop suggesting a remote or wilderness setting.
Prediction: A sequence of events where two men are engaged in loading a barrel onto a small, yellow seaplane parked on a wooden dock beside a serene
lake. The men, dressed in casual work attire, appear to be coordinating their efforts to secure the barrel, which is large and cylindrical, onto the aircraft. The
setting is tranquil, with the calm waters of the lake reflecting the clear blue sky and the lush greenery of the surrounding landscape. The video conveys a sense
of teamwork and the meticulous nature of preparing an aircraft for a journey.
Figure4: ExamplevideocaptioningresultsonMiradataset,formedinquestion-answeringstyle.
7Question: What’s the video about in brief?
Prediction: A person is using a citrus juicer to extract juice from a lemon on a wooden cutting board. The person’s hands are visible, and they are wearing a
black sleeve. The lemon is placed on the cutting board, and the person squeezes it to extract the juice, which is collected in a white bowl. The process is
methodical and focused, with the person ensuring to extract as much juice as possible.
Question: Can you briefly tell me what's in the video?
Prediction: A person is preparing shrimp for cooking. They start by placing the shrimp in a glass bowl and adding a white powdery substance, likely salt or
flour. The person then mixes the shrimp and the powder together with their hands, ensuring the shrimp are evenly coated. Finally, the person transfers the
shrimp onto a cutting board, ready for the next step in the cooking process.
Question: What’s the main gist of the video?
Prediction: The video depicts a dramatic encounter between a scuba diver and an octopus in the ocean. The octopus, with its tentacles spread wide, appears
to be approaching the diver, who is equipped with a camera and other diving gear. The octopus's movements are fluid and deliberate, showcasing its ability to
maneuver through the water with ease. The diver remains relatively stationary, observing the octopus's behavior.
Question: Can you briefly tell me what's in the video?
Prediction: A wrestling match between two athletes on a red mat. The wrestlers are engaged in a series of competitive maneuvers, attempting to gain control
over each other. The sequence shows the wrestlers in various positions, from standing to on the ground, as they grapple and execute moves. The intensity of
the match is evident through their physical exertion and strategic positioning.
Figure5: ExamplevideocaptioningresultsonMSVDandMSRVTTcaptiondataset.
Method Size #tokens MSVD-Cap MSRVTT-Cap Mira-Cap
LLaVA-OneVision 7B 1152 61.62/3.31 38.60/2.71 48.83/3.10
Tarsier 7B 4608 62.26/3.37 40.27/2.77 40.55/2.87
BLIP-3-Video 4B 32 63.59/3.38 42.06/2.82 80.67/3.96
BLIP-3-Video 4B 128 64.17/3.41 43.05/2.85 81.13/3.97
BLIP-3-Video(captioning-onlymodel) 4B 128 69.50/3.52 50.45/2.98 81.76/4.00
Table 6: Video caption evaluation results using 8 frames. We employ VideoChatGPT’s LLM
evaluationandreportAverageAccuracy/AverageScoreinthistable. The‘captioning-onlymodel’
was trained only using Mira video caption data (without QA data), making it specialized for the
captioning.
3.4 VIDEOCAPTIONINGEVALUATION
Weevaluateourmodelonthevideocaptioningtaskbycomparingitagainststate-of-the-artmodelson
thetestsplitsofMSVD-CaptionandMSRVTT-Caption,aswellasacustomevaluationsplitfromthe
Miradataset. FortheMiradataset,werandomlyselected6,000samplesfromourfull,filtereddata
tocreatetheevaluationsplit,withtheremainderusedfortraining. WeemployedVideo-ChatGPT’s
LLMevaluation,specificallyusingGPT-3.5tocomparemodel-predictedcaptionswithgroundtruth
captions. TheLLMassessesaccuracybycheckingifthepredictedcaptionmatchesthegroundtruth,
andassignsascoreonascaleof0to5foreachsample.
Table 6 demonstrates the results. All three models were provided with 8 frames per video, and
consistentvisualinputandpromptswereensuredacrossthemodels. OurBLIP-3-Videoconsistently
8outperformsLLaVA-OneVision-7BandTarsier-7Bacrossallthreevideocaptioningbenchmarks,
withparticularlynotableimprovementsontheMiravideocaptioningtask.
We present qualitative video captioning results for the Mira dataset in Figure 4 (more details in
Appendix)andfortheMSVDandMSRVTTdatasetsinFigure5. BLIP-3-Videogenerateshigh-
quality,detailedcaptions.
4 RELATED WORKS
4.1 IMAGE-TEXTLLMS
Among recent advances in image-text multimodal models (Li et al., 2023a; Alayrac et al., 2022;
Liuetal.,2023;Daietal.,2023;Xueetal.,2024;Laurençonetal.,2024;Deitkeetal.,2024),one
commonstrategyenableimageunderstandinginLLMistostartwithapre-trainedimageencoder
(e.g.,ViT(Radfordetal.,2021;Zhaietal.,2023))andapre-trainedlanguage-onlyLLM(Abdinetal.,
2024;Baietal.,2023;Dubeyetal.,2024). Thetwocomponentsareconnectedviaavision-language
connector,whichistrainedtoprojectvisionembeddingsoutputfromthevisionencoderinto“vision
tokens" that can be ingested by the LLM. The vision tokens are of the same shape as language
embeddings, sotheimage-textLLMcanbetrainedinthesamewayasregularlanguagemodels
using the next token prediction loss. There are many design choices for the VL connector, for
example, BLIP-2 (Li et al., 2023a) chooses to use a Q-Former to extract vision tokens from the
vision embeddings, Flamingo (Alayrac et al., 2022) uses “perceiver resampler" as the connector
pluscross-attentionlayersthroughoutthelanguagemodel,whileasimplerchoiceistouseMLP
layers to transform the vision embeddings. Image-text LLMs are usually trained with a multi-
stage training strategy, including pre-training, instruction tuning, and sometimes, post-training
(e.g.,DPO(Rafailovetal.,2024)). Inadditiontosimplestructuredimage-textdatasuchasimage
captioningandsingle-imageVQA,recentworksalsoexplorefree-fromimage-textdataformodel
trainingsuchasinterleavedimage-textunderstanding(Laurençonetal.,2023;Awadallaetal.,2024)
andmulti-imageVQA(Jiangetal.,2024;Lietal.,2024a).
4.2 VIDEOLLMS
VideoLLMsextendthearchitectureofimage-basedLLMstohandlevideoinput. Zhangetal.(2023)
integratespre-trainedencodersandfrozenLLMstoprocessmultimodalinputthroughaVideoQ-
FormerandAudioQ-Former,generatingvideoandaudioembeddingscompatiblewithLLMwithout
retrainingencoders. Maazetal.(2024)adaptstheCLIPvisualencoderforvideobyincorporating
temporalfeaturesandfine-tunesthemodelusingvideo-instructionpairscollectedbytoolslikeBLIP-
2(Lietal.,2023a)andGRiT(Wuetal.,2022). Lietal.(2024c)generatesframe-levelembeddings
using a visual encoder but condenses visual information into two tokens per frame. However, it
does not account for temporal recency across frames. Similarly, models like Video-LLaVA (Lin
etal.,2023)andLLaVa-OneVision(Lietal.,2024a)treatvideosaslongmulti-imagecontextsbut
lack token efficiency optimization, making them computationally costly. SlowFast-LLaVA (Xu
etal.,2024b)adoptsatwo-streamarchitecture—SlowandFastpathways—tocapturebothspatial
andtemporalvideosemanticswithoutextrafine-tuning. Finally,LLaVa-hound-DPO(Zhangetal.,
2024b)usesDirectPreferenceOptimization(DPO)(Rafailovetal.,2024)withGPT-4Vtoannotate
preferencedata,enhancingvideoquestion-answeringperformancebydetectinginconsistenciesor
hallucinationsinmodelresponses.
4.3 TOKENPRUNING
TokenpruningisawidelyusedtechniquetoreduceredundantandoverlappinginformationinVision
Transformers(ViTs)andlargelanguagemodels(LLMs). Bolyaetal.(2022)mergessimilartokens
withinViTs,combiningredundantcontentwhileretainingtask-relevantinformationacrosstaskslike
image,video,andaudioprocessing. Similarly,Renetal.(2023)employstheTemporalAggregation
Module to combine redundant consecutive video frames and the Spatial Aggregation Module to
mergesimilarpatcheswithineachframe,reducingthenumberofprocessedtokensbyupto75%.
Shenetal.(2024)focusontemporalredundancyandprogressivelymergestokensacrossneighboring
clips,whichreducesthenumberoftokensbypreservingimportantvideo-levelfeatures. Allthese
9methodsfocusonvisualtokenmerginginViTs,wheretokenprocessingischallenginginvideo-based
LLMs. Inaddition,Chenetal.(2024)improvesattentionefficiencyindeeperlayersbydynamically
pruningormergingredundantimagetokensbasedonattentionscoreswithoutextratraining. Shang
etal.(2024)introducesadaptivetokenreductionthroughtheAdaptiveImportantTokenSelection
andTokenSupplement,whichcanbeintegratedintoVLMmodelswithoutfine-tuning. InLLMs,
KVcachepruningispopularforefficientmodelserving,asseenin(Fuetal.,2024),whichuses
attentionmapstoprogressivelyprunetokensandreducethetime-to-first-token(TTFT).Wanetal.
(2024)extends KVcachepruningtoVLMs, employingdifferenttoken mergingstrategiesto cut
computationalcostsandsupportlongermultimodalcontexts.
5 CONCLUSION
WeintroduceBLIP-3-Video,whichisanefficient,compactvision-languagemodelforvideoswith4B
parameters.BLIP-3-Videoincorporatesatemporalencoderinitsarchitecture,whichallowsthemodel
toabstracttheentirevideowithasfewas16or32tokens. Incontrasttomanystate-of-the-artvideo
VLMstakingadvantageofthousandsofvisualtokenstorepresentavideo(e.g.,4608),BLIP-3-Video
showsacompetitiveperformancewhileutilizingmuchfewervisualtokens(e.g.,32).
REFERENCES
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,
NguyenBach,AmitBahree,ArashBakhtiari,JianminBao,HarkiratBehl,AlonBenhaim,MishaBilenko,
JohanBjorck,SébastienBubeck,QinCai,MartinCai,CaioCésarTeodoroMendes,WeizhuChen,Vishrav
Chaudhary, Dong Chen, Dongdong Chen, Yen-Chun Chen, Yi-Ling Chen, Parul Chopra, Xiyang Dai,
Allie Del Giorno, Gustavo de Rosa, Matthew Dixon, Ronen Eldan, Victor Fragoso, Dan Iter, Mei Gao,
Min Gao, Jianfeng Gao, Amit Garg, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng
Hao,RussellJ.Hewett,JamieHuynh,MojanJavaheripi,XinJin,PieroKauffmann,NikosKarampatziakis,
DongwooKim,MahoudKhademi,LevKurilenko,JamesR.Lee,YinTatLee,YuanzhiLi,YunshengLi,
ChenLiang,LarsLiden,CeLiu,MengchenLiu,WeishungLiu,EricLin,ZeqiLin,ChongLuo,Piyush
Madan,MattMazzola,ArindamMitra,HardikModi,AnhNguyen,BrandonNorick,BarunPatra,Daniel
Perez-Becker,ThomasPortet,ReidPryzant,HeyangQin,MarkoRadmilac,CorbyRosset,SambudhaRoy,
OlatunjiRuwase,OlliSaarikivi,AminSaied,AdilSalim,MichaelSantacroce,ShitalShah,NingShang,
HiteshiSharma,SwadheenShukla,XiaSong,MasahiroTanaka,AndreaTupini,XinWang,LijuanWang,
ChunyuWang,YuWang,RachelWard,GuanhuaWang,PhilippWitte,HaipingWu,MichaelWyatt,BinXiao,
CanXu,JiahangXu,WeijianXu,SonaliYadav,FanYang,JianweiYang,ZiyiYang,YifanYang,Donghan
Yu,LuYuan,ChengruidongZhang,CyrilZhang,JianwenZhang,LiLynaZhang,YiZhang,YueZhang,
YunanZhang,andXirenZhou. Phi-3technicalreport: Ahighlycapablelanguagemodellocallyonyour
phone,2024. URLhttps://arxiv.org/abs/2404.14219.
Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,Arthur
Mensch,KatieMillican,MalcolmReynolds,RomanRing,ElizaRutherford,SerkanCabi,TengdaHan,Zhitao
Gong, SinaSamangooei, MarianneMonteiro, JacobMenick, SebastianBorgeaud, AndrewBrock, Aida
Nematzadeh,SahandSharifzadeh,MikolajBinkowski,RicardoBarreira,OriolVinyals,AndrewZisserman,
andKarenSimonyan. Flamingo: avisuallanguagemodelforfew-shotlearning. InAdvancesinneural
informationprocessingsystems,2022.
KirolosAtaallah,XiaoqianShen,EslamAbdelrahman,EssamSleiman,DeyaoZhu,JianDing,andMohamed
Elhoseiny. Minigpt4-video: Advancingmultimodalllmsforvideounderstandingwithinterleavedvisual-
textualtokens. arXivpreprintarXiv:2404.03413,2024.
AnasAwadalla,LeXue,OscarLo,ManliShu,HannahLee,EtashKumarGuha,MattJordan,ShengShen,
MohamedAwadalla,SilvioSavarese,CaimingXiong,RanXu,YejinChoi,andLudwigSchmidt. Mint-1t:
Scalingopen-sourcemultimodaldataby10x: Amultimodaldatasetwithonetrilliontokens,2024. URL
https://arxiv.org/abs/2406.11271.
JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,WenbinGe,YuHan,Fei
Huang, BinyuanHui, LuoJi, MeiLi, JunyangLin, RunjiLin, DayihengLiu, GaoLiu, ChengqiangLu,
KemingLu,JianxinMa,RuiMen,XingzhangRen,XuanchengRen,ChuanqiTan,SinanTan,JianhongTu,
PengWang,ShijieWang,WeiWang,ShengguangWu,BenfengXu,JinXu,AnYang,HaoYang,JianYang,
ShushengYang,YangYao,BowenYu,HongyiYuan,ZhengYuan,JianweiZhang,XingxuanZhang,Yichang
Zhang,ZhenruZhang,ChangZhou,JingrenZhou,XiaohuanZhou,andTianhangZhu. Qwentechnical
report,2023. URLhttps://arxiv.org/abs/2309.16609.
10DanielBolya,Cheng-YangFu,XiaoliangDai,PeizhaoZhang,ChristophFeichtenhofer,andJudyHoffman.
Tokenmerging:Yourvitbutfaster. arXivpreprintarXiv:2210.09461,2022.
LiangChen,HaozheZhao,TianyuLiu,ShuaiBai,JunyangLin,ChangZhou,andBaobaoChang. Animageis
worth1/2tokensafterlayer2:Plug-and-playinferenceaccelerationforlargevision-languagemodels. arXiv
preprintarXiv:2403.06764,2024.
WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,BoyangLi,
PascaleFung,andStevenC.H.Hoi. Instructblip: Towardsgeneral-purposevision-languagemodelswith
instructiontuning. InNeurIPS,2023.
MattDeitke,ChristopherClark,SanghoLee,RohunTripathi,YueYang,JaeSungPark,MohammadrezaSalehi,
NiklasMuennighoff,KyleLo,LucaSoldaini,JiasenLu,TairaAnderson,ErinBransom,KianaEhsani,Huong
Ngo,YenSungChen,AjayPatel,MarkYatskar,ChrisCallison-Burch,AndrewHead,RoseHendrix,Favyen
Bastani,EliVanderBilt,NathanLambert,YvonneChou,ArnaviChheda,JennaSparks,SamSkjonsberg,
MichaelSchmitz,AaronSarnat,ByronBischoff,PeteWalsh,ChrisNewell,PiperWolters,TanmayGupta,
Kuo-HaoZeng,JonBorchardt,DirkGroeneveld,JenDumas,CrystalNam,SophieLebrecht,CaitlinWittlif,
CarissaSchoenick,OscarMichel,RanjayKrishna,LucaWeihs,NoahA.Smith,HannanehHajishirzi,Ross
Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and pixmo: Open weights and open data for
state-of-the-artmultimodalmodels,2024. URLhttps://arxiv.org/abs/2409.17146.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,AieshaLetman,
AkhilMathur,AlanSchelten,AmyYang,AngelaFan,AnirudhGoyal,AnthonyHartshorn,AoboYang,Archi
Mitra,ArchieSravankumar,etal. Thellama3herdofmodels,2024. URLhttps://arxiv.org/abs/
2407.21783.
QichenFu,MinsikCho,ThomasMerth,SachinMehta,MohammadRastegari,andMahyarNajibi. Lazyllm:
Dynamictokenpruningforefficientlongcontextllminference. arXivpreprintarXiv:2407.14057,2024.
AndrewJaegle,SebastianBorgeaud,Jean-BaptisteAlayrac,CarlDoersch,CatalinIonescu,DavidDing,Skanda
Koppula,DanielZoran,AndrewBrock,EvanShelhamer,OlivierJ.Hénaff,MatthewM.Botvinick,Andrew
Zisserman,OriolVinyals,andJoãoCarreira. PerceiverIO:Ageneralarchitectureforstructuredinputs&
outputs. InICLR,2022.
YunseokJang,YaleSong,YoungjaeYu,YoungjinKim,andGunheeKim. Tgif-qa: Towardspatio-temporal
reasoninginvisualquestionanswering. InProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pp.2758–2766,2017.
DongfuJiang,XuanHe,HuayeZeng,CongWei,MaxW.F.Ku,QianLiu,andWenhuChen. Mantis:Interleaved
multi-imageinstructiontuning. arXiv2405.01483,2024.
PengJin,RyuichiTakanobu,WancaiZhang,XiaochunCao,andLiYuan. Chat-univi:Unifiedvisualrepresenta-
tionempowerslargelanguagemodelswithimageandvideounderstanding. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.13700–13710,2024.
XuanJu,YimingGao,ZhaoyangZhang,ZiyangYuan,XintaoWang,AilingZeng,YuXiong,QiangXu,and
YingShan. Miradata:Alarge-scalevideodatasetwithlongdurationsandstructuredcaptions,2024. URL
https://arxiv.org/abs/2407.06358.
KumaraKahatapitiya,KanchanaRanasinghe,JongwooPark,andMichaelS.Ryoo. Languagerepositoryfor
longvideounderstanding,2024. URLhttps://arxiv.org/abs/2403.14622.
WonkyunKim,ChanginChoi,WonseokLee,andWonjongRhee.Animagegridcanbeworthavideo:Zero-shot
videoquestionansweringusingavlm,2024. URLhttps://arxiv.org/abs/2403.18406.
HugoLaurençon,LucileSaulnier,LéoTronchon,StasBekman,AmanpreetSingh,AntonLozhkov,Thomas
Wang,SiddharthKaramcheti,AlexanderM.Rush,DouweKiela,MatthieuCord,andVictorSanh. OBELICS:
anopenweb-scalefiltereddatasetofinterleavedimage-textdocuments. InNeurIPS,2023.
Hugo Laurençon, Andrés Marafioti, Victor Sanh, and Léo Tronchon. Building and better understanding
vision-languagemodels:insightsandfuturedirections.,2024.
BoLi,YuanhanZhang,DongGuo,RenruiZhang,FengLi,HaoZhang,KaichenZhang,YanweiLi,ZiweiLiu,
andChunyuanLi. Llava-onevision:Easyvisualtasktransfer. arXivpreprintarXiv:2408.03326,2024a.
FengLi,RenruiZhang,HaoZhang,YuanhanZhang,BoLi,WeiLi,ZejunMa,andChunyuanLi. Llava-next:
Tacklingmulti-image,video,and3dinlargemultimodalmodels,June2024b. URLhttps://llava-vl.
github.io/blog/2024-06-16-llava-next-interleave/.
11JunnanLi,DongxuLi,SilvioSavarese,andStevenC.H.Hoi. BLIP-2: bootstrappinglanguage-imagepre-
trainingwithfrozenimageencodersandlargelanguagemodels. InICML,volume202ofProceedingsof
MachineLearningResearch,pp.19730–19742.PMLR,2023a.
KunChangLi,YinanHe,YiWang,YizhuoLi,WenhaiWang,PingLuo,YaliWang,LiminWang,andYuQiao.
Videochat:Chat-centricvideounderstanding. arXivpreprintarXiv:2305.06355,2023b.
YanweiLi,ChengyaoWang,andJiayaJia. Llama-vid:Animageisworth2tokensinlargelanguagemodels.
2024c.
Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual
representationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning,2023.
JiajunLiu,YibingWang,HanghangMa,XiaopingWu,XiaoqiMa,XiaomingWei,JianbinJiao,EnhuaWu,
andJieHu. Kangaroo:Apowerfulvideo-languagemodelsupportinglong-contextvideoinput,2024. URL
https://arxiv.org/abs/2408.15542.
MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFahadShahbazKhan.Video-chatgpt:Towardsdetailed
videounderstandingvialargevisionandlanguagemodels. InProceedingsofthe62ndAnnualMeetingofthe
AssociationforComputationalLinguistics(ACL2024),2024.
AJPiergiovanni,IsaacNoble,DahunKim,MichaelS.Ryoo,VictorGomes,andAneliaAngelova. Mirasol3b:A
multimodalautoregressivemodelfortime-alignedandcontextualmodalities.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,2024.
AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,
AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever. Learningtransferable
visualmodelsfromnaturallanguagesupervision. InICML,volume139ofProceedingsofMachineLearning
Research,pp.8748–8763.PMLR,2021.
RafaelRafailov,ArchitSharma,EricMitchell,ChristopherDManning,StefanoErmon,andChelseaFinn.Direct
preferenceoptimization:Yourlanguagemodelissecretlyarewardmodel. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
ShuhuaiRen,SishuoChen,ShichengLi,XuSun,andLuHou. Testa:Temporal-spatialtokenaggregationfor
long-formvideo-languageunderstanding. arXivpreprintarXiv:2310.19060,2023.
Michael Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. Tokenlearner:
Adaptivespace-timetokenizationforvideos. InNeurIPS,volume34,pp.12786–12797,2021.
MichaelS.Ryoo,KeerthanaGopalakrishnan,KumaraKahatapitiya,TedXiao,KanishkaRao,AustinStone,Yao
Lu,JulianIbarz,andAnuragArnab. Tokenturingmachines. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,2023.
YuzhangShang,MuCai,BingxinXu,YongJaeLee,andYanYan. Llava-prumerge:Adaptivetokenreduction
forefficientlargemultimodalmodels. arXivpreprintarXiv:2403.15388,2024.
LeqiShen,TianxiangHao,SichengZhao,YifengZhang,PengzhangLiu,YongjunBao,andGuiguangDing.
Tempme:Videotemporaltokenmergingforefficienttext-videoretrieval. arXivpreprintarXiv:2409.01156,
2024.
Zhongwei Wan, Ziang Wu, Che Liu, Jinfa Huang, Zhihong Zhu, Peng Jin, Longyue Wang, and Li Yuan.
Look-m:Look-onceoptimizationinkvcacheforefficientmultimodallong-contextinference. arXivpreprint
arXiv:2406.18139,2024.
Jiawei Wang, Liping Yuan, and Yuchen Zhang. Tarsier: Recipes for training and evaluating large video
descriptionmodels. arXivpreprintarXiv:2407.00634,2024a.
XiaohanWang,YuhuiZhang,OrrZohar,andSerenaYeung-Levy. Videoagent:Long-formvideounderstanding
withlargelanguagemodelasagent,2024b. URLhttps://arxiv.org/abs/2403.10517.
Ziyang Wang, Shoubin Yu, Elias Stengel-Eskin, Jaehong Yoon, Feng Cheng, Gedas Bertasius, and Mohit
Bansal. Videotree:Adaptivetree-basedvideorepresentationforllmreasoningonlongvideos,2024c. URL
https://arxiv.org/abs/2405.19209.
JialianWu,JianfengWang,ZhengyuanYang,ZheGan,ZichengLiu,JunsongYuan,andLijuanWang. Grit:A
generativeregion-to-texttransformerforobjectunderstanding. arXivpreprintarXiv:2212.00280,2022.
12JunbinXiao,XindiShang,AngelaYao,andTat-SengChua. Next-qa: Nextphaseofquestion-answeringto
explainingtemporalactions. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pp.9777–9786,2021.
DejingXu,ZhouZhao,JunXiao,FeiWu,HanwangZhang,XiangnanHe,andYuetingZhuang. Videoquestion
answeringviagraduallyrefinedattentionoverappearanceandmotion. InACMMultimedia,2017.
LinXu,YilinZhao,DaquanZhou,ZhijieLin,SeeKiongNg,andJiashiFeng. Pllava: Parameter-freellava
extensionfromimagestovideosforvideodensecaptioning,2024a.
MingzeXu,MingfeiGao,ZheGan,Hong-YouChen,ZhengfengLai,HaimingGang,KaiKang,andAfshin
Dehghan. Slowfast-llava:Astrongtraining-freebaselineforvideolargelanguagemodels. arXivpreprint
arXiv:2407.15841,2024b.
LeXue,ManliShu,AnasAwadalla,JunWang,AnYan,SenthilPurushwalkam,HongluZhou,VirajPrabhu,
YutongDai,MichaelSRyoo,etal. xgen-mm(blip-3): Afamilyofopenlargemultimodalmodels. arXiv
preprintarXiv:2408.08872,2024.
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca:
Contrastivecaptionersareimage-textfoundationmodels. arXivpreprintarXiv:2205.01917,2022.
ZhouYu,DejingXu,JunYu,TingYu,ZhouZhao,YuetingZhuang,andDachengTao. Activitynet-qa:Adataset
forunderstandingcomplexwebvideosviaquestionanswering. InProceedingsoftheAAAIConferenceon
ArtificialIntelligence,volume33,pp.9127–9134,2019.
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image
pre-training. InICCV,pp.11941–11952.IEEE,2023.
CeZhang,TaixiLu,MdMohaiminulIslam,ZiyangWang,ShoubinYu,MohitBansal,andGedasBertasius.
Asimplellmframeworkforlong-rangevideoquestion-answering,2024a. URLhttps://arxiv.org/
abs/2312.17235.
HangZhang,XinLi,andLidongBing. Video-llama: Aninstruction-tunedaudio-visuallanguagemodelfor
videounderstanding. arXivpreprintarXiv:2306.02858,2023.
RuohongZhang,LiangkeGui,ZhiqingSun,YihaoFeng,KeyangXu,YuanhanZhang,DiFu,ChunyuanLi,
AlexanderHauptmann, YonatanBisk, andYimingYang. Directpreferenceoptimizationofvideolarge
multimodalmodelsfromlanguagemodelreward,2024b.
13Question: Please describe the primary object or subject in the video, capturing their attributes, actions, positions, and movements.
GT: A white jellyfish, is seen floating and gently pulsating as it moves through the water. Its bell contracts and expands rhythmically, propelling it forward. The
tentacles trail behind, undulating with the water's movement. Throughout the sequence, the jellyfish maintains a steady, graceful motion, drifting from left to right
and slightly rotating as it progresses.
Tarsier-7b: The primary object in the video is a large jellyfish. It pulsates and swims through the water. The jellyfish changes its position and orientation in the
water.
LLaVA-OneVision-7b: The primary object in the video is a jellyfish, characterized by its translucent body with a white and purple coloration. It has long, thin
tentacles that trail behind it as it moves through the water. The jellyfish is seen floating near the ocean floor, which is covered with small rocks and some debris.
The water surrounding the jellyfish is clear blue, and there are strands of seaweed visible in the background.
BLIP-3-Video: A jellyfish, is seen swimming in the water. It moves in a slow, undulating motion, with its bell-shaped body contracting and expanding to propel
itself forward. The jellyfish's tentacles trail behind it, creating a delicate, flowing effect. Throughout the video, the jellyfish maintains a consistent upward and
forward movement, occasionally adjusting its direction slightly.
Question: Please provide a dense description of the video, covering the main subject, movements, style, backgrounds, and cameras.
GT: The video provides a breathtaking aerial view of a historic castle perched on the edge of a steep cliff. As the drone camera glides over the scene, viewers
can appreciate the castle's ancient stone walls and the surrounding lush greenery, which contrasts sharply with the rugged cliff faces. The castle, a relic of
medieval architecture, is shown in various states of preservation, with some parts reduced to ruins while others stand robustly. The video captures the castle's
strategic location, offering panoramic views of the surrounding landscape and the churning waters below.
Tarsier-7b: The video begins with an aerial view of a grassy field with several rectangular structures. The camera pans left, revealing more of the field and
structures. A lighthouse and a metal walkway become visible. The camera continues to pan left, showing the coastline and additional structures. The video ends
with a black screen.
LLaVA-OneVision-7b: The video begins with an aerial view of a historical site, showcasing a series of rectangular stone structures and pathways. The
structures are surrounded by lush green grass, and the pathways are lined with metal railings. A small building with a sloped roof is visible in the foreground.
The perspective shifts to reveal more of the site, including a steep cliff with a staircase leading up to a platform. The cliffside features several large, rectangular
stone formations arranged in a linear pattern. The video continues to focus on these formations, highlighting their arrangement along the cliff edge. The
background reveals a body of water, possibly the ocean, adding to the scenic beauty of the location. The video wraps up with a close-up view of the stone
formations, emphasizing their historical significance and the meticulous craftsmanship involved in their construction.
BLIP-3-Video: The video provides an aerial view of a historic fortification perched on a rugged cliff overlooking the ocean. The fort, characterized by its robust
stone walls and strategic positioning, includes various defensive structures such as cannons and barricades. As the video progresses, the camera captures the
changing light of the setting sun, casting long shadows and highlighting the textures of the stone and the surrounding landscape. The fort's layout, including
pathways and small buildings, is revealed in greater detail, showcasing its historical significance and architectural complexity.
Figure6: ExamplevideocaptioningresultsonMiradataset,formedinquestion-answeringstyle. We
comparetheoutputsofBLIP-3-Video,Tarsier,andLLaVA-OneVision. GTstandsfortheground
truth. Differentcoloredtextsaredifferentpartsofgroundtruthcaptionsandtheircorresponding
sentencesinthemodeloutputs. Underlinedtextsarehallucinations.
APPENDIX
Figures6and7comparesthevideocaptioningresultsofBLIP-3-VideoagainstTarsierandLLaVA-OneVision.
Themodelstaketheexactsamevideoinputwiththesamenumberofframes,forafaircomparison.Weobserve
thatBLIP-3-Videogeneratecaptioningmuchmoresimilartothegroundtruth.Italsoisabletoprovidemore
holisticunderstandingofthevideo(ratherthanfocusingonunnecessarydetailssuchastextsinthebackground)
whilesufferingfromlesshallucinations.
14Question: Please provide a dense description of the video, covering the main subject, movements, style, backgrounds, and cameras.
GT: A step-by-step guide on assembling a wooden bed frame, featuring a person methodically working through the construction process. Initially, the person is
seen preparing the wooden planks and arranging the necessary tools. As the video progresses, the individual meticulously attaches and secures the slats to the
frame, ensuring stability and precision. The final scenes depict the person tightening the last screws and reviewing the completed structure, demonstrating a
sense of accomplishment and attention to detail.
Tarsier-7b: The video starts with a person assembling a piece of furniture. The scene then transitions to a website showcasing different types of beds, including
a bed with a canopied frame and a minimalist bed with a white mattress. The video concludes with social media and website icons for 'dactylomoovis' and
'dactylomoveis'.
LLaVA-OneVision-7b: The video begins with a person in a pink shirt with the number 18 on it, working on assembling a wooden bed frame. The individual is
seen using a black brush to apply paint or stain to the wooden parts of the bed frame. The text 'Utilize espacador para colar os estrados' (Use an spacer to glue
the slats) appears at the bottom of the screen, providing instructions for the assembly process. The scene transitions to a top-down view of the person
continuing to work on the bed frame, with the text 'Parafuse os estrados nas barras' (Screw the slats into the bars) appearing at the bottom, offering further
guidance. The background remains consistent throughout, showing a white wall and some tools on the floor. The video then shifts to a promotional website for
Dactylo.com.br, featuring images of different bed frames and a brief description of the company's mission to create high-quality furniture using sustainable
materials. The website also includes social media icons for Facebook, Twitter, Google+, and YouTube, along with the Dactylo logo and the tagline 'Movimento
de Madeira' (Wood Movement). The video concludes with a promotional image for Dactylo.com.br, showcasing a white bed frame with the text 'COMO
MANTER A BICAMA MISSION' (How to maintain the Bicama Mission), emphasizing the importance of proper care for the furniture.
BLIP-3-Video: A step-by-step process of assembling a piece of furniture, specifically a bed frame. The person involved is seen working on a wooden bed
frame, using various tools and components to construct the bed. The video likely serves as a tutorial, providing viewers with a visual guide on how to assemble
the bed frame themselves. The person's actions are deliberate and precise, indicating a level of expertise or familiarity with the task.
Question: Please provide a dense description of the video, covering the main subject, movements, style, backgrounds, and cameras.
GT: Two men engaged in the task of loading a barrel onto a small, yellow floatplane near a body of water. The men are dressed in casual attire suitable for
outdoor work, with one wearing gloves for handling the barrel. The plane is docked at a wooden structure, possibly a jetty or makeshift dock, and the men
coordinate their efforts to lift and maneuver the barrel into the cargo area of the aircraft. The surrounding environment is serene and natural, with a forested
backdrop suggesting a remote or wilderness setting.
Tarsier-7b: The video shows two men near a seaplane by a lakeside. One man is wearing a red shirt and the other is in a blue shirt. They are handling a barrel.
The scene changes to a construction site with heavy machinery and road signs. The camera captures a vehicle moving forward on a dirt road. The scene
switches back to the men by the seaplane, now with a view of the lake and mountains in the background. The video includes on-screen text commentary about
living in Yukon and the challenges faced by the community.
LLaVA-OneVision-7b: The video begins with two individuals standing next to a yellow and brown seaplane on a wooden dock by a body of water. The seaplane
has the registration number C-GFPR and is adorned with red and white stripes. One person, wearing a maroon shirt and jeans, is holding a red barrel, while the
other, in a blue shirt and shorts, is also handling a barrel. In the background, there are red barrels and a small boat. The scene transitions to a view from inside
a vehicle, showing a dirt road under construction with orange cones and construction equipment visible. The road is surrounded by dense green forests and
mountains in the distance. The video then returns to the seaplane, where the same two individuals are now standing near the open door of the plane. The text
'Living up YUKON is tough' appears at the bottom of the screen, followed by 'Collecting resources is the biggest issue for the people up here.' The video wraps
up with the two individuals standing near the seaplane, with the serene backdrop of the lake and forested mountains still visible.
BLIP-3-Video: A sequence of events where two men are engaged in loading a barrel onto a small, yellow seaplane parked on a wooden dock beside a serene
lake. The men, dressed in casual work attire, appear to be coordinating their efforts to secure the barrel, which is large and cylindrical, onto the aircraft. The
setting is tranquil, with the calm waters of the lake reflecting the clear blue sky and the lush greenery of the surrounding landscape. The video conveys a sense
of teamwork and the meticulous nature of preparing an aircraft for a journey.
Figure7: ExamplevideocaptioningresultsonMiradataset,formedinquestion-answeringstyle. We
comparetheoutputsofBLIP-3-Video,Tarsier,andLLaVA-OneVision. GTstandsfortheground
truth. Differentcoloredtextsaredifferentpartsofgroundtruthcaptionsandtheircorresponding
sentencesinthemodeloutputs. Underlinedtextsarehallucinations.
15