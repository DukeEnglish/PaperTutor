Distribution Learning with Valid Outputs Beyond the
Worst-Case
NickRittler KamalikaChaudhuri
UniversityofCalifornia-SanDiego UniversityofCalifornia-SanDiego
nrittler@ucsd.edu kamalika@cs.ucsd.edu
Abstract
Generativemodelsattimesproduce“invalid”outputs,suchasimageswithgen-
erationartifactsandunnaturalsounds. Validity-constraineddistributionlearning
attemptstoaddressthisproblembyrequiringthatthelearneddistributionhavea
provablysmallfractionofitsmassininvalidpartsofspace–somethingwhichstan-
dardlossminimizationdoesnotalwaysensure. Tothisend,alearnerinthismodel
canguidethelearningvia“validityqueries”,whichallowittoascertainthevalidity
of individual examples. Prior work on this problem takes a worst-case stance,
showingthatproperlearningrequiresanexponentialnumberofvalidityqueries,
anddemonstratinganimproperalgorithmwhich–whilegeneratingguarantees
in a wide-range of settings – makes an atypical polynomial number of validity
queries. Inthiswork,wetakeafirststeptowardscharacterizingregimeswhere
guaranteeingvalidityiseasierthanintheworst-case. Weshowthatwhenthedata
distributionliesinthemodelclassandthelog-lossisminimized,thenumberof
samplesrequiredtoensurevalidityhasaweakdependenceonthevalidityrequire-
ment. Additionally,weshowthatwhenthevalidityregionbelongstoaVC-class,a
limitednumberofvalidityqueriesareoftensufficient.
1 Introduction
When sampling from a generative model, it is highly desirable that its outputs meet some basic
criteriaofquality. Inthecaseoftext,thismaymeanthatgeneratedsentencesrespectgrammarrules,
oravoidtheuseofbiasedoroffensivelanguage[1,2]. Whengeneratingcode,acriterionmaybethat
thegeneratedcodesuccessfullycompiles[3]. Inimagegeneration,wemightwishtoavoidblurry
outputs,orthosepossessinggenerationartifactswhichclearlydistinguishthemfromnaturalimages
[4,5].
In this paper, we examine the statistical cost of ensuring that learned distributions produce such
“valid”outputs. Todoso,weconsideranelegantformulationoftheproblemoflearningsuchvalid
modelsdueto[3]. Intheirwork,trainingdataaregeneratedaccordingtoaprobabilitydistributionP,
andthebinary“validity”ofexamplesisdeterminedbysomeunknown“validityfunction”v. Given
sampleaccesstoP andqueryaccesstov,alearnerattemptstoidentifyaprobabilitydistribution
whichoutputsinvalidexampleswithprobabilityatmostϵ . Atthesametime,thedistributionshould
2
havealosswhichisatmostϵ worsethanthatoftheminimumlossmodelinaclassQwhichoutputs
1
validexampleswithprobability1. Here,queryaccesstovcapturestheideathatcollectingsamplesis
oftencheap,butverifyingvalidityisoftenlessso,possiblyrequiringahuman-in-the-loop.
Theinitialworkof[3]suggeststhatchoosingsuchalow-loss,high-validitydistributionqˆmayrequire
alargenumberofvalidityqueries. UndertheassumptionthatP is“fully-valid”,i.e. outputsavalid
examplewithprobability1,theyshowthatintheworstcase,2Ω(1/ϵ1)validityqueriesarerequiredto
choosesuchamodelqˆfromtheclassQ. Theyfollowthisresultwithanimproperlearningalgorithm
38thConferenceonNeuralInformationProcessingSystems(NeurIPS2024).
4202
tcO
12
]GL.sc[
1v35261.0142:viXraforchoosingqˆwhich,whileachievingpolynomialboundsonthenumberofvalidityqueries,usesa
relativelylargenumberofvalidityqueriesO˜(cid:0) log(|Q|)/ϵ2ϵ (cid:1) .
1 2
Thesomewhatpessimisticpicturepaintedbythesefascinatingcomplexity-theoreticresultscanbe
trackedtotheirgenerality. Firstly,it’spossiblethatQandP aresignificantly“mismatched”,i.e. the
supportofeachmodelq ∈QhasonlyasmalloverlapwiththesupportofthedistributionP,inwhich
case the validity information contained in valid training samples is unhelpful to a proper learner.
Secondly,theirimproperlearningalgorithmislargelyloss-agnostic,inthatitgeneratesguarantees
forawideclassofboundedlossfunctions. Finally,nothingisassumedabouttheformofthevalidity
functionv,precludingprovableestimation.
Inthiswork,weofferacounterbalancetothispicture,beginninganinvestigationintolearningsettings
whereguaranteeingvalidityischeaperthansuchresultsmightindicate. Wefirstconsiderlearning
undercompleteeliminationofmodelclassmismatch,whereQisrichenoughtocontainthefully-
validdatadistributionP,andthelossisthelog-lossl(f(x))=log(1/f(x)). Itisintuitivethatinthis
setting,lossminimizationaloneshouldguaranteevalidity. Somewhatlessintuitively,wedemonstrate
analgorithmcloselyrelatedtoempiricalriskminimizationwhichusesjustO˜(cid:0) log(|Q|)/min(ϵ2,ϵ )(cid:1)
1 2
samplestoguaranteeitsoutputmeetslossandvalidityrequirements–inotherwords,validitycomes
quicklyunderrandomsamplingfromP inthissetting.
Secondly,weconsiderlearningunderadifferentrealizabilityassumption,namelythatthevalidity
regionisamemberofaVC-classofdimensionD. Inthissetting, weprovideananalysisofthe
naturalschemeofrestrictingtheempiricalriskminimizertoanestimateofthevalidpartofspace. We
showthatwhensmall-lossmodelsq ∈Qhaveatleastconstantvalidity,thisschemeusesO˜(D/ϵ )
2
validityqueries,implyingaquerycostreductionoverthegeneral-purposealgorithmof[3]. Wealso
showthatlearningunderthecappedlog-losscanbeusedtorelaxtheassumptionofconstantvalidity
atthecostofanextrafactorof1/ϵ .
1
Ourresultssuggesttheexistenceofarichwebofsettingsinwhichvaliditymaybecheaperthanin
thegeneralcase. Theyalsosuggestthatthechoiceofthelossplaysanimportantrollinguaranteeing
validoutputs,compellingfurtherinvestigationofthelog-lossinparticular.
2 RelatedWork
The framing of learning distributiuons in terms of PAC guarantees similar to [6] dates back to
[7],whoconsiderthelearnabilityofspecificclassesofdiscretedistributionsunderarealizability
assumption. A significant body of work on distribution learning has been developed overtime,
often focusing on algorithms for learning over parametric families or under specific “structural”
assumptions [8, 9, 10, 11]. The only theoretical contribution to validity-constrained distribution
learningundertheformulationposedby[3]thatweareawareofisthatworkitself.
The study of loss functions for the evaluation of probabilistic models has often been studied the
lens of “scoring rules” in the forecasting literature [12, 13, 14]. There are some notable recent
contributionstowardsexpandingtheunderstandingofwhenlossfunctionsfordistributionlearning
displaydesirableproperties,e.g. “properness”,whichdesignatesthatthelossisminimizedbythe
truedatadistribution[8,15].
Thefirsthalfofthispaperdrawsonintuitionfromhypothesistestingtoevaluatetheperformanceof
empiricalriskminimization. Hypothesistestingisamajorfocusoftheclassicalstatisticsliterature
[16]. TheboundsinthefirsthalfofthepaperareduetoanalysisinspiredbytheNeyman-Pearson
lemma[17,18],andrelyontheapproximationoftotalvariationaldistancebetweenproductmeasures
[19].
Theappliedliteratureongenerativemodelinghasconsistentlynotedtheproblemoflearnedmodels
producing“invalid”examples[20,21,22,4]. Varioustechniqueshavebeenproposedformitigating
invaliditygenerally,andindomainspecificsettings[23,20,24].Whileworkingundertheassumption
thatthevalidityfunctionliesinaVC-class,thestrategyweintroducehassomeroughsemblancetoa
“post-editing”procedureproposedby[24].
23 Preliminaries
3.1 ProblemSetup
Let X be a subset of Euclidean space Rd with finite Lebesgue measure λ. Let P denote the set
ofallprobabilitydistributionsonthemeasurablespace(X,F ),whereF arisesfromLebesgue
X X
measurablesetsintersectedwithX. LetP ∈P bethedata-generatingdistribution.
Intheeyesofthelearner,thefunctionv :X →{0,1}isafixedandunknown“validityfunction”,
measurablewithrespecttotherelevantdistributions. Thevalidityfunctiondenoteswhetherornot
anexamplex ∈ X isconsideredavalidoutputforalearnedapproximationofP. Thelearneris
givenamodelclassofQ⊂P ofprobabilitydistributionsonX,eachwithdensityf withrespectto
q
λ,andaffordedwiththeknowledgethatatleastoneq ∈Qis“fully-valid”,i.e. thatthereissome
q ∈QwithinvalidityV(q):=Pr (v(X)=1)=1. Weattimesusethenotionof“invalidity”of
X∼q
amodel,bywhichwemeanI(q)=1−V(q). Followingthemainexpositionof[3],weassumeQis
offinitecardinality.
Thegoodness-of-fitofamodelq ∈P isgovernedbyadecreasing“local”lossfunctionl:R≥0 →
R ∪ {∞}. Such a loss function gives rise to loss of model via L (q;l) := E [l(f (X))].
P X∼P q
Givenani.i.dsampleS fromP,welettheempiricalestimateofthelossofamodelbeL (q;l)=
S
(cid:80)
l(f (x ))/|S|.WeusetheshorthandL (q)andL (q)todenotethetrueandempiricallosses
xi∈S q i P S
ofqunderthelog-lossl(q)=log(1/f (x)),wherelogdenotesthenaturallogarithm. Wetakethe
q
log-losstobeinfiniteatpointswheref (x)=0.
q
3.2 GoalofLearning
The goal of the learner is to choose some qˆ∈ P which has a loss L (qˆ;l) similar to that of the
P
lowest-lossfully-validmodelinQ,whilesimultaneouslymaintainingnearfull-validity. Explicitly,
considerthemodel
q∗ := argmin L (q;l).
P
q∈Q:V(q)=1
Todescribethequalityofanoutputtedmodel,weconsidertwolearningparametersϵ andϵ ,where
1 2
ϵ isusedtocontrolthelosssub-optimality,andϵ tocontroltheinvalidity. Formallythen,thegoal
1 2
ofthelearneristooutputqˆ∈ P satisfyingL(qˆ) ≤ L(q∗)+ϵ andI(qˆ) ≤ ϵ . Toaccomplishthis
1 2
goal,thelearnerhassampleaccesstoP,andqueryaccesstov,i.e. alearnercandrawanyfinite
numberofi.i.d.samplesfromP,andanyrequestthevalueofthevalidityfunctionvatanyfinite
numberofinputsinX.
Ataminimum,weareinterestedinalgorithmswhichrequireanumberofsamplesfromP andnumber
validityqueriesthatispolynomialinlog(|Q|),1/ϵ and1/ϵ . Ideally,wewouldliketominimizethe
1 2
numberofvalidityqueriesgivensomepolynomialnumberofsamplesfromP. Themotivationfor
thisgoalissimilartotheminimizationoflabelqueriesinactivelearningforclassification[25],where
samplesfromthemarginaloverinstancesareoftencheap,butlabelingsuchexamplesisassumed
expensive.
3.3 Full-ValidityofP
Weassumethatallsamplesfromthedata-generatingdistributionP arevalid,i.e. thatV(P) = 1.
Undersuchanassumption,thequerydemandofalearningalgorithmcanbeconceptualizedasthe
overheadnumberofqueriessufficientforchoosingagoodmodelunderthestandardprocedureof
removinginvalidexamplesfromthetrainingset.
Ifthedatadistributionisnotfully-valid,andvalidsamplesarerequiredbyanalgorithm,thequestion
ofminimizingtheoverallnumberofqueriesisdependentonthesamplecomplexityoflearning–
if one assumes that P has been constructed by “accepting” valid samples from some underlying
distribution which outputs a valid sample with constant probability, then the overall query cost
incurredbyanalgorithmisontheorderofthelargerofthenumberofsamplesandthenumberof
“overhead”validityqueriesituses.
Inthispaper,weareprimarilyinterestedinthe“overhead”numberofqueries,whichwerefertoas
the“numberofvalidityqueries”ofagivenscheme. Inmostcases,algorithmsamplerequirements
aresimilartoO(log(|Q|)/ϵ2),whichallowsforaccuratelossestimationinmanysettings.
1
33.4 SummaryofPreviousResults
The learning problem above is due to [3], who considered the possibility of specifying learning
algorithmsmeetingtheabovebi-criteriaobjectiveforanychoiceofbounded,decreasing,localloss
function.
Thisworkgivessomeinterestinginsightintothedifficultyofselectingsuchalow-loss,high-validity
model. Theybeginbygivinganegativeresult,namelythatanyproperlearningalgorithmoutputting
qˆ∈Q,mustmake2Ω(1/ϵ1)validityqueriesintheworstcase,regardlessofthenumberofsamples
availablefromP. Thisresultarisesfromaspecificprobleminstancewhereineveryq ∈ Qhasa
significantamountofmassoutsideofthesupportofP,inwhichcasesamplesfromafully-validP
donotgiveinformationaboutvinpartsofspacerelevanttothechoiceofqˆ∈Q.
Ontheotherhand, theydemonstrateanimproperlearningalgorithmwhichachievespolynomial
bounds on samples and validity queries for any choice of loss meeting the above criteria. Their
algorithmharnessesaconstrainedERMoracle,iterativelyqueryingthevalidityofsamplesfromthe
modelq ∈Qwhichistheempiricallossminimizerputtingnomassonpointsknowntobeinvalid.
Inparticular,theirschemeusesO˜(log(|Q|)/ϵ2)samplesandO˜(log(|Q|)/ϵ2ϵ )validityqueries.
1 1 2
4 LearningWithoutModelClassMismatchUndertheLog-Loss
Wefirstconsidertheproblemofselectingalow-loss,high-validitymodelunderarelaxationoftwo
ofthemainsourcesofdifficultyinoriginalproblemformulation: themisalignmentofthemodel
classQwiththedatadistributionP,andthelackofassumptionsontheloss.
Inparticular,weconsidertheproblemunderarealizabilityassumption,namelythatP ∈Q,further
investigating the power of the log-loss. Such a setting is arguably more closely aligned with
contemporarylearningsettingswithrichmodelclassesthatappropriatelycapturefeaturesofthe
underlyingdatadistribution, wherethevalidityinformationcontainedinsamplesfromP canbe
exploitedbyconvergencetothebestinformation-theoreticrepresentationofP inQ.
Thelog-lossisbyfarthemostwidely-usedlossinpractice[8]. Itisaclassicresultoftheproper
scoringruleliteraturethatthelog-lossistheuniquestrictly-properlocalloss,i.e. theonlylocalloss
underwhichforalldistributionsq ̸= P,itholdsthatL (P;l) < L (q;l). Thishighlydesirable
P P
property –implying thatconvergence tothe optimumover P coincideswith convergence to P –
makes the choice of an alternative outside of capped variants preferable only under specialized
circumstances.
4.1 TowardsValiditywithoutValidityQueries
Given that samples are assumed to be valid, and the log-loss permits convergence to the data
generatingdistribution,onewouldhopethatsimplyselectingamodelqˆ∈Qwhichisasufficiently
goodrepresentationofP underthelog-losswouldyieldvalidityguaranteesinthissetting. Simply
utilizingempiricalriskminimization(ERM)isthecanonicalapproachtothisend,andonewhich,
givensufficientdatafromP,usesexactlyzerovalidityqueries.
NotethatanymodelqwithinvalidityI(q)>ϵ necessaryhasd (q,P)>ϵ . Inthiscase,qmust
2 TV 2
have at least ϵ mass in the invalid part of space, where P has none. Thus, if one can guarantee
2
that qˆ has d (qˆ,P) ≤ ϵ , the validity requirement is met. Recalling the Pinsker inequality
TV 2
(cid:112)
d (q,q′)≤O( d (q,q′))relatingtotalvariationaldistanceandKL-divergence,itfollowsthat
TV KL
obtainingamodelqˆwhichisatmostϵ2sub-optimalinlog-lossyieldsamodelmeetingthevalidity
2
requirement.
Whilethisillustratesusefulintuitionforthesetting,itglossesovertwomainissues. Firstly,empirical
estimatesofthelog-lossdonotadmitconcentrationguarantees–onecanconstructsimpleexamples
whereE [log(1/f (X))]isunboundedabove,butwithhighprobability,theempiricalestimate
X∼P q
(cid:80)
L (q)= log(1/f (x))/|S|isapproximatelythatofP [8]. Thus,selectinglow-empiricalloss
S x∈S q
modelscanneveryieldlossguarantees. Secondly,thisapplicationofPinsker’sinequalitydemands
ϵ2losssub-optimality,suggestingthatensuringvalidityviatheselectionofagoodmodelunderthe
2
log-lossisevenharderthanguaranteeingasmallloss.
4Algorithm1ModifyingERMtoYieldLog-LossGuarantees
1: procedurefinite_log_loss(DistributionClassQ,S,ϵ ∧ =min(ϵ 1,ϵ 2))
(cid:80)
2: qˆ
ERM
←argmin
q∈Q
xi∈Slog(1/f q(x i))
3: returnqˆ=(1−ϵ ∧/8)·qˆ ERM+ϵ ∧/8·u ▷MixERM,uniformdistribution
4: endprocedure
We would hope that in the case that zero-query learning is possible, that guaranteeing validity
arisessomewhatcoincidentlywithconvergencetoP, meaningthatthesamplecomplexityisnot
much worse given a validity requirement than without one. Thus, the path towards satisfaction
ofthelearningobjectivesrequiressubtlehandling,andcompelsparticularattentiontothesample
complexitydependenceonthevalidityparameterϵ .
2
4.2 AnalysisofEmpiricalRiskMinimization
Asindicatedabove,itisnotpossibletoguaranteethatempiricalriskminimization(ERM)outputsa
modelwithsmalllog-loss. Itis,howeverpossibletoguaranteethatitoutputsamodelwhichclosely
resemblesP andinheritsvalidityguaranteeswithasmallnumberofsamples.
Inparticular,it’spossibletoshowthatgivensufficientsamples,ERMyieldsamodelwithsmalltotal
variationtoP whenP ∈Q. Thisisduetothefollowingfolkloretheorem[8],whichweproveunder
assumptionofdensityexistenceintheAppendix.
Lemma4. Fix0<ϵ,δ <1arbitrarily,andletP,q ∈P bedistributionswithdensitieswithrespect
toλ. Thenifd (q,P)≥ϵ,andS ∼Pnforn≥Ω(log(1/δ)/ϵ2),itholdswithprobability≥1−δ
TV
that
L (P)<L (q).
S S
Thus,atthestatisticalcostofestimatingacoinbias,anydistributionqwithtotalvariation≥ϵfrom
thedatadistributionwillrevealitselftobeempiricallyinferiorwhenthelog-lossisused. Thiscanbe
easilyleveragedtogenerateguaranteesforERMoverQintermsoftotalvariation.
Itistemptingtothinkthatthisistheentirestorywhenitcomestoguaranteeingvalidity. Afterall,
wearguedabovethatsmalltotalvariationfromP issufficientforϵ invalidity. Thatsaid,simply
2
lookingattotalvariationignoresaparticularstructuralfeatureofdistributionsqwithI(q)>ϵ –in
2
particular,suchdistributionshavemassinpartsofspaceinwhichP doesnot.
Thisobservationcanbeusedtoconstructtightlowerboundsonthetotalvariationaldistancebetween
productmeasuresarisingfromP andq withI(q) > ϵ . Thisleadstothefollowingresult,which
2
statesthatERMyieldsafaithfulrepresentationofthedatageneratingdistributionthatisatmostϵ
2
invalidgivenanumberofsampleswithamodestdependenceonthevalidityparameterϵ .
2
Lemma5. Fix0 < δ,ϵ ,ϵ < 1arbitrarily,andsupposeP ∈ Q. IfP isfully-validunderv,and
1 2
(cid:16) (cid:17)
S ∼ Pn for n ≥ Ω log(|Q|)+log(1/δ) , then with probability ≥ 1−δ over S ∼ Pn, the ERM
min(ϵ2 1,ϵ2)
solution
(cid:88)
qˆ=argmin log(1/q(x )),
i
q∈Q
xi∈S
satisfiesboth
d (qˆ,P)≤ϵ and I(qˆ)≤ϵ .
TV 1 2
Notethatthisguaranteeisnotredundant–havingd (qˆ,P)≤ϵ doesnotimplyI(q)≤ϵ when
TV 1 2
ϵ <ϵ .
2 1
4.3 AttainingLog-LossGuarantees
Thisresultcanbeinterpretedasavoteofconfidenceforthenaivetrainingofgenerativemodelsunder
thelog-loss. Nevertheless,fromalearning-theoreticperspective,thereisaquestionwhetherornotit
ispossibletoguaranteelowlog-losswhilemaintainingvaliditywithzerovalidityqueries.
5WhileERMcannotpossiblyfurnishlog-lossguarantees,itturnsoutthatitispossibletomodifythe
outputofERMtogeneratelog-lossguaranteesatthecostofanextrapolylogarithmicfactorinthe
samplecomplexity,atleastwhenthedensitiesf areboundedaboveandbelowintheirsupport.1
q
Theidea,formalizedinAlgorithm1,issimplytomixtheoutputofERMwiththeuniformdistribution.
Givingtheuniformdistributionamixturecomponentontheorderofmin(ϵ ,ϵ )canbeshownto
1 2
ensurethatthevalidityguaranteesoftheERMoutputarepreserved,whilealsogivingtheoutputted
distributionsupportacrosstheentirespace. Thisleadstothefollowingtheorem.
Theorem1. Fix0<δ,ϵ ,ϵ <1arbitrarily,andsupposeP ∈QandthatP isfully-validunderv.
1 2
Ifitholdsthatforeachq ∈Qthatα≤f (x)≤β forallx∈supp(q),thenthereisan
q
(cid:32) log2(1/min(ϵ ,ϵ ,α))·(cid:0) log(|Q|)+log(1/δ)(cid:1)(cid:33)
N ≤O˜ 1 2 ,
min(ϵ2,ϵ )
1 2
suchthatforalln≥N,withprobability≥1−δ,theoutputqˆofAlgorithm1satisfies
L (qˆ)≤L (q∗)+ϵ and I(qˆ)≤ϵ .
P P 1 2
Here the O˜ notation hides a polylogarithmic dependence on 1/β, which is insignificant in most
regimes, and treats the density of the uniform distribution over X as a constant, which would
otherwisealsoenterpolylogarithmically.
Theorem1showsthatguaranteeswithrespecttotheunboundedlog-lossareattainableimproperly,i.e.
whenthelearnercanchooseq ∈/ Q. It’saninterestingquestionwhetherthelogarithmicdependence
onmin(ϵ ,ϵ )canberemovedwithamoresubtlestrategy.
1 2
4.4 DiscussionofOptimality
Onemightsuspectthatachievingasmallerdependencethan1/ϵ onthevalidityparametershould
2
beimpossible. Weconfirmthisistrueatleastforproperlearners,showingthattheanalysisofERM
inLemma5istightinitsdependenceonϵ . Thislemmaisusedtogeneratethevalidityguaranteein
2
Theorem1.
Theorem2. Forallϵ < 1/4andforallproperlearnersL : S → P,ifthesampleS ∼ Pn isof
2
sizen≤1/8ϵ ,thenthereexistsatriple(P,Q,v)withP ∈QandP fully-valid,onwhichL(S)has
2
invalidityI(L(S))>ϵ withprobability≥1/4.
2
The intuition here is that while any invalid q ∈ Q has at least ϵ total variation from P, in the
2
worst case, the total variation between q and P is upper bounded by O(ϵ ) as well. This makes
2
distinguishingbetweenP andsomeϵ invaliddistributionhardenoughtogeneratesuchalower
2
bound.
Thesamplerequirementof1/ϵ2,bothinourguaranteesandinpreviouswork,isastandardoffshoot
1
of loss estimation, irrespective of the search of a valid model. In general, one cannot expect
improvementstothisend–thisisthestandarddependenceonefindsforestimatingthemeansbounded
randomvariables. Thissuggeststhatthe“realizablecomplexity”forthissettingis1/min(ϵ2,ϵ )–
1 2
whilenon-zerolossesshouldnotbegenerallyestimableusing“realizable”techniques,guaranteeing
smallinvaliditycanwhenP ∈Q.
5 UtilizingEstimatesoftheValidityFunctioninTraining
Inthegeneralformulationoftheproblem,thelearnerisgivenanarbitrarybounded,decreasingloss,
amodelclassQwhichismismatchedwithP, andhasnoaprioriinformationaboutthevalidity
functionv. Insuchasetting,itisclearthatvalidityqueriesarenecessary.
In this section, we consider a setting where it is known to the learner that v can be found in a
hypothesisclassV ofboundedcomplexity. Undersuchanassumption,wewouldhopetobeableto
lowerthenumberofvalidityqueriesbeyondtheboundsof[3].
6Algorithm2Post-HocRestrictionofERMtoanEstimateofValidOutputs
1: procedurevalid_restriction(DistributionClassQ,ValidityClassV,ϵ 1,ϵ 2,δ,γ)
(cid:16) (cid:17)
2: S ←Ω
M2(log(|Q|)+log(1/δ))
i.i.d. samples∼P
ϵ2
1
(cid:80)
3: qˆ ←argmin l(q(x))
ERM q∈Q x∈S
(cid:16) (cid:17)
4: S P ←Ω M(Dlog(M/ ϵϵ 11)+log(1/δ)) i.i.d. samples∼P,
(cid:16) (cid:17)
S ←Ω Dlog(1/γϵ2)+log(1/δ) i.i.d. samples∼qˆ
qˆERM γϵ2 ERM
5: vˆ←argmin h∈V(cid:80) x∈SP∪SqˆERM 1[h(x)̸=v(x)] ▷LabelS qˆERM viaqueriestov
6: returnf qˆ∝f qˆERM(x)·1[vˆ(x)=1]ifqˆ ERM({vˆ(x)=1})>0elsef qˆ=f qˆERM
7: endprocedure
5.1 Algorithm
Anaturalalgorithminthissettingisto“correct”theinvalidityoftheempiricalriskminimizer–to
restricttheempiricalriskminimizertopartsofspacewhicharevalidwithrespecttoanestimateof
thevalidityvˆ. ThisisthepreciselytheideaformalizedinAlgorithm2.
Togenerateguaranteesforsuchastrategy,onemustdeterminethedistributionwithrespecttowhich
theestimatevˆshouldbeaccurate. Inourcase,wegenerateaccuracyguaranteesoverbothP andthe
ERMmodelqˆ byselectinganestimatevˆthathas0empiricalerroroverbothdistributions. The
ERM
sourceofthequerycomplexityofthealgorithmcomesfromthefactthatsamplesarisingfromqˆ
ERM
mustbelabeledbyoraclecallstov. NotingthatsamplesfromP canbeautomaticallylabeledas
validbythefull-validityofP savesaconstantfactorovernaivelylabelingallexamplesacquiredin
thesecondhalfofthealgorithm.
AccuracyundersamplesfromP allowsonetocontrolthelossofqˆbyinvokingtheboundedness
ofthelossinthedisagreementregionofvˆandv,andguaranteeswithrespecttoqˆ allowusto
ERM
boundtheinvalidityoftherestriction. BecauseP isfully-valid,losscontributionsfromtheagreement
regionofvandvˆcorrespondtopartsofspacewherevˆ(x)=1–asthelossisnon-increasing,placing
moremassinsuchpartsofspacecanneverincreasethelosscontributionattributabletointegrating
overthisregion.
Algorithm2alsorequiresaparameterγ >0. Thisparametershouldbeavaliditylowerboundonthe
modelsq ∈Q,providingasafeguardonthepossibilityofan“invalidityblowup”whenrestrictingthe
ERMoutputtoacertainregionofspace–onemustnormalizetherestrictiontooutputaprobability
distribution,whichinthiscasemeansincreasingmassinpartsofspacethatareestimatedtobevalid.
Anapriorilowerboundonthevalidityallowsforpreciseenoughestimationofvˆthatincreasingthe
massinsuchregionsisunlikelytoleadtoappreciableinvalidityinthefinalmodel.
It’spossiblethattherestrictionoftheERMestimatedvalidregionisundefined–thishappensifand
onlyiftheestimatedvalidregionhaszeromassundertheERM.Givenvaliditylowerboundsfor
modelsq ∈Q,thisisalowprobabilityeventwhichcanoccuronlywhenestimationofthevalidity
functionisverypoorrelativetothequerycomplexity.Asonemightimagine,thehandlingofthiscase
isimmaterialforPAC-guarantees. Wechoosetoarbitrarilydefinebehaviorinthiscasebyoutputting
theERMmodel.
5.2 Guarantees
TherestrictedoutputqˆofAlgorithm2admitsthefollowingguaranteeoverlosssub-optimalityand
invalidity.
1ThisdoesnotyielduniformconvergenceoverQgiventhatthesupportofq∈QneednotalignwithP
7Theorem3. Supposev ∈V withVC-dimensionVC(V)≤D,andthatforeachq ∈Q,thevalidity
V(q) ≥ γ > 0. For all 0 < ϵ ,ϵ ,δ ≤ 1 and for all choices of non-increasing loss functions
1 2
l:R≥0 →[0,M],Algorithm2requiresanumberofsamples
(cid:18) M2(log(|Q|)+log(1/δ)) M(Dlog(M/ϵ )+log(1/δ))(cid:19)
≤O + 1 ,
ϵ2 ϵ
1 1
andanumberofvalidityqueries
(cid:18) (cid:19)
Dlog(1/γϵ )+log(1/δ)
≤O 2 ,
γϵ
2
toensurethatwithprobability≥1−δ,itsoutputenjoys
L (qˆ;l)≤L (q∗;l)+ϵ and I(qˆ)≤ϵ .
P P 1 2
Thus, in regimes where e.g. γ ≥ Ω(ϵ ), D = Θ(log(|Q|)), this guarantee represents a reduced
1
numberofqueriesundertheO˜(M2log(|Q|)/ϵ2ϵ )boundof[3]. Italsoimpliesa“decoupling”of
1 2
thequerycomplexityfromϵ .
1
WenotethatthesamplerequirementfromP isincreasedincertainregimesovertheO˜(log(|Q|)/ϵ2)
1
requirement of [3]. This is, however, not a concern in most settings where validity queries are
expensive. Ifsamplesfromafully-validP arereadilyobtainable,thesettingisanalogoustothatof
activelearning,wherefocusisdirectedtothenumberoflabelsrequestedintraining.
EvenifP mustbeconstructedby“accepting”validsamplesfromsomeunfilteredP′,acomparison
betweenthequerycomplexityofTheorem3andthequeryboundof[3]isoftenstillrepresentativeof
therelativedatacostsoftheschemes. SupposingP′producesvalidsampleswithconstantprobability,
thetotalnumberofvalidityqueriesmadebyeachschemeisproportionaltothescheme’ssample
requirementsfromP,plusthenumberofvalidityqueriesusedinitsexecution. Essentially,toyield
validityqueryspeedups,ourschemerequiresaVCboundonV whichdoesnotdwarflog(|Q|). Thus,
inmostcasesofinterest,thequeryingthevalidityofO˜(MD/ϵ )extrasamplesisasymptotically
1
inconsequentialrelativetoO(M2log(|Q|)/ϵ2),andtheO˜(D/γϵ )querybudgetrequiredtoexecute
1 2
thealgorithmgivenaccesstoafully-validP.
5.3 BetterQueryComplexityBounds
5.3.1 ExploitingthePowerofActiveLearning
Theorem3presentsasomewhatpessimisticviewofthepotentialofsucha“post-filtering”scheme.
Firstly,itignoresthepotentialofactivelearnerstoimprovequerycomplexitiesoverpassivesampling.
Querycomplexitiesinactivelearningofclassifiersareoftenexpressedintermsofthe“disagreement
coefficient”[26],oftendenotedviaθ. Intherealizablesetting,querycomplexitiesofactivelearning
looklikeO˜(Dθlog(1/ϵ))[27]. Definitionally,itcanbeshownthatθ ≤1/ϵ. Thus,provingthegains
ofactivelearningalgorithmsusuallyreliesonboundingthedisagreementcoefficientnon-trivially,i.e.
showingθ <o(1/ϵ),orideally,θ ≤O(1).
Whilethisischallenging,asθisbothaclassanddistribution-dependentquantity,thereisaliterature
thataddressesthispotentialinvarioussettings–seethereferencesin[25]. Inprinciple,onecoulduse
suchananalysistoshowthatthequerycomplexityofanactivelearningmodificationofAlgorithm
2isonalowerorderthantheguaranteeofTheorem3whenconditionsarefavorable. Tothisend,
itmaybeusefultonotethatamodificationofAlgorithm2whereinvˆisselectedastheERMona
datasetgeneratedbyamixtureofP andqˆ admitsguaranteesaswell.
ERM
5.3.2 OnlyLow-LossModelsNeedAppreciableValidity
Another source of potential looseness in the statement of Theorem 3 is that it phrases the query
complexityintermsoftheworst-casevalidityovermodelsq ∈Q. Thisisunnecessary–withhigh
probability,inthefirststepofthealgorithm,oneselectsamodelq ∈QwithO(ϵ )trueloss. Thus,
1
whatreallymattersforsuchastrategyisthatmodelsthathaverelativelysmalllossldonothave
invaliditynearing1.
8Thisisarealisticscenariointhecasethatthelossfunctionl –despitepossiblynotbeingproper
–prioritizesmodelswhichinsomesenseresemblethedatageneratingdistributionP. Indeed,it’s
somewhatdifficulttoenvisionasituationwherealosswouldbechosenthatprioritizesmodelswith
norelationtothedatageneratingdistribution.Tothisend,wegivethefollowingcorollarytoTheorem
3.
Corollary 1. Under the conditions of Theorem 3, if in addition it holds that all models q ∈ Q
withlosssub-optimalityϵ havevaliditygreaterthansomeconstant,thenthequerycomplexityof
1
Algorithm2canbeimprovedto
(cid:18) (cid:19)
Dlog(1/ϵ )+log(1/δ)
≤O 2 .
ϵ
2
5.3.3 RemovingthePositiveValidityRequirement
Using an idea found in the algorithm of [3], one can show that if a learner has access to single
distributionDwithadensityandatleastsomenon-zeroconstantvalidity,andthedensitiesf are
q
boundedabove,thatAlgorithm2canbemodifiedsoastodroptherequirementofpositivevalidity
overmodelswhenlearningunderthecappedlog-loss.
By mixing the qˆ with D, giving mixture component O(ϵ ) to D, one can generate similar
ERM 1
guaranteesasthoseofTheorem3. ThemodificationcanbefoundintheAppendixasAlgorithm3,
andenjoysthefollowingguarantee.
Theorem4. Supposev ∈ V whereVC(V) ≤ D, andthatforeachq ∈ Q, wehavef (x) ≤ β.
q
SupposefurtherthatthereissomeknownD ∈P withdensityf whichhasV(D)≥c>0forsome
D
constantc. Thenforallchoicesof0<ϵ ,ϵ ,δ <1/2andM >0,Algorithm3requiresanumber
1 2
ofsamples
(cid:18) M2(log(|Q|)+log(1/δ)) M(Dlog(M/ϵ )+log(1/δ))(cid:19)
≤O˜ + 1 ,
ϵ2 ϵ
1 1
andanumberofvalidityqueries
(cid:18) (cid:19)
Dlog(1/ϵ ϵ )+log(1/δ)
≤O 1 2 ,
ϵ ϵ
1 2
toensurethatwithprobability≥1−δ,itsoutputenjoys
(cid:20) (cid:21) (cid:20) (cid:21)
E min(M, log(1/f (X))) ≤E min(M, log(1/f (X))) +ϵ and I(qˆ)≤ϵ .
X∼P qˆ X∼P q∗ 1 2
Here,theO˜ notationagainhidesfactorspolylogarithmicin1/β.
NotethattheO˜nowappearsinthesamplecomplexity.ThissimplyreflectsthefactthattheM-capped
log-losscanrangebetweengapM andlog(1/β)whenworkingwithdensitiesboundedaboveby
β ≥ 1. Inthecasethatdensitiescanbeboundedaboveby1,asinthediscretesettingof[3],this
dependencedisappears.
6 Conclusion
Thisworkisintendedasafirst-lookintosettingsclosertothecommon-case,whereensuringvalidity
mayberelativelycheap.
Amorethoroughinvestigationofthelog-loss,aswellascappedvariants,seemsaveryrelevantlineof
furtherinquiry,giventhewidespreaduseofthisfamilyinpracticeanditsusefulinformation-theoretic
properties. Anaturalextensiontothefirstpartofthisworkwouldbetoconsiderlearninginthe
agnosticcaseP ∈/ Qunderthelog-loss,whereonewouldhopetobeabletoexploittheseproperties
andthevalidityoftrainingsamplestokeepthenumberofvalidityquerieslow.
Ingeneral,characterizingthesampleandquerydemandsofvalidity-constraineddistributionlearning
ischallenging,giventhatprovinglowerboundsingeneralrequiresarguingagainstlearnerswithtwo
toolsattheirdisposal–samplingandactivelyqueryingvalidity. Workinthisdirectionwilllikely
requiresomecreativeconstructions.
9References
[1] EthanPerez,SaffronHuang,FrancisSong,TrevorCai,RomanRing,JohnAslanides,Amelia
Glaese, Nat McAleese, and Geoffrey Irving. Red teaming language models with language
models. arXivpreprintarXiv:2202.03286,2022.
[2] AbubakarAbid,MaheenFarooqi,andJamesZou. Persistentanti-muslimbiasinlargelanguage
models. InProceedingsofthe2021AAAI/ACMConferenceonAI,Ethics,andSociety,pages
298–306,2021.
[3] Steve Hanneke, Adam Kalai, Gautam Kamath, and Christos Tzamos. Actively avoiding
nonsenseingenerativemodels. CoRR,abs/1802.07229,2018. URLhttp://arxiv.org/abs/
1802.07229.
[4] TakuhiroKanekoandTatsuyaHarada. Blur,noise,andcompressionrobustgenerativeadversar-
ialnetworks. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages13579–13589,2021.
[5] AugustusOdena,VincentDumoulin,andChrisOlah. Deconvolutionandcheckerboardartifacts.
Distill,1(10):e3,2016.
[6] L.G.Valiant. Atheoryofthelearnable. CommunicationsoftheACM,pages1134–1142,1984.
[7] MichaelKearns,YishayMansour,DanaRon,RonittRubinfeld,RobertE.Schapire,andLinda
Sellie. Onthelearnabilityofdiscretedistributions. InProceedingsoftheTwenty-SixthAnnual
ACMSymposiumonTheoryofComputing,STOC’94,page273–282,NewYork,NY,USA,
1994.AssociationforComputingMachinery. ISBN0897916638. doi: 10.1145/195058.195155.
URLhttps://doi.org/10.1145/195058.195155.
[8] NikaHaghtalab,CameronMusco,andBoWaggoner.Towardacharacterizationoflossfunctions
fordistributionlearning. CoRR,abs/1906.02652,2019. URLhttp://arxiv.org/abs/1906.
02652.
[9] ConstantinosDaskalakis,IliasDiakonikolas,RyanODonnell,RoccoAServedio,andLi-Yang
Tan. Learning sums of independent integer random variables. In 2013 IEEE 54th Annual
SymposiumonFoundationsofComputerScience,pages217–226.IEEE,2013.
[10] AdamTaumanKalai,AnkurMoitra,andGregoryValiant. Efficientlylearningmixturesoftwo
gaussians. InProceedingsoftheforty-secondACMsymposiumonTheoryofcomputing,pages
553–562,2010.
[11] ConstantinosDaskalakis,IliasDiakonikolas,andRoccoAServedio. Learningpoissonbinomial
distributions.InProceedingsoftheforty-fourthannualACMsymposiumonTheoryofcomputing,
pages709–728,2012.
[12] GlennW.Brier. VerificationofForecastsExpressedinTermsofProbability. MonthlyWeather
Review,78(1),January1950. doi: 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2.
[13] Irving John Good. Rational decisions. Journal of the Royal Statistical Society: Series B
(Methodological),14(1):107–114,1952.
[14] Tilmann Gneiting and Adrian Raftery. Strictly proper scoring rules, prediction, and es-
timation. Journal of the American Statistical Association, 102:359–378, 03 2007. doi:
10.1198/016214506000001437.
[15] RafaelFrongillo,DhammaKimpara,andBoWaggoner. Properlossesfordiscretegenerative
models,2022. URLhttps://arxiv.org/abs/2211.03761.
[16] ErichLeoLehmann, JosephPRomano, andGeorgeCasella. Testingstatisticalhypotheses,
volume3. Springer,1986.
[17] Jerzy Neyman and Egon Sharpe Pearson. On the Problem of the Most Efficient Tests of
Statistical Hypotheses. Phil. Trans. Roy. Soc. Lond. A, 231(694-706):289–337, 1933. doi:
10.1098/rsta.1933.0009.
10[18] Patrick Rebeschini. Minimax lower bounds and hypothesis testing, December
2021.URLhttps://www.stats.ox.ac.uk/~rebeschi/teaching/AFoL/22/material/
lecture16.pdf.
[19] R.D.Reiss. Approximationofproductmeasureswithanapplicationtoorderstatistics. The
AnnalsofProbability,9(2):335–341,1981. doi: 10.1214/aop/1176994477.
[20] Matt J Kusner, Brooks Paige, and José Miguel Hernández-Lobato. Grammar variational
autoencoder. InInternationalconferenceonmachinelearning,pages1945–1954.PMLR,2017.
[21] DavidJanz,JosvanderWesthuizen,BrooksPaige,MattJKusner,andJoséMiguelHernández-
Lobato. Learningagenerativemodelforvalidityincomplexdiscretestructures. arXivpreprint
arXiv:1712.01664,2017.
[22] ZhuJYIsolaP,TZhou,etal. Imagetoimagetranslationwithconditionaladversarialnetworks.
Proceedingsofthe IEEEConferenceonComputerVisionandPatternRecognition. Hawaii, USA,
1125:1134,2017.
[23] AndrewAitken,ChristianLedig,LucasTheis,JoseCaballero,ZehanWang,andWenzheShi.
Checkerboard artifact free sub-pixel convolution: A note on sub-pixel convolution, resize
convolutionandconvolutionresize. arXivpreprintarXiv:1707.02937,2017.
[24] ZhifengKongandKamalikaChaudhuri. Dataredactionfrompre-trainedgans. In2023IEEE
ConferenceonSecureandTrustworthyMachineLearning(SaTML),pages638–677.IEEE,
2023.
[25] Steve Hanneke. Theory of disagreement-based active learning. Foundations and Trends in
MachineLearning,7:131–309,012014. doi: 10.1561/2200000037.
[26] SteveHanneke. Aboundonthelabelcomplexityofagnosticactivelearning. InProceedingsof
the24thInternationalConferenceonMachineLearning,page353–360,2007.
[27] Sanjoy Dasgupta. Two faces of active learning. Theoretical Computer Science, 412(19):
1767–1781,2011. ISSN0304-3975.
[28] BruceDriver. Probabilitytoolswithexamples, June2010. URLhttps://mathweb.ucsd.
edu/~tkemp/280A/Driver-Probability-Lecture-Notes.pdf.
[29] TonSteerneman. Onthetotalvariationandhellingerdistancebetweensignedmeasures;an
applicationtoproductmeasures. ProceedingsoftheAmericanMathematicalSociety,88(4):
684–688,1983.
[30] Puning Zhao and Lifeng Lai. Analysis of knn density estimation. IEEE Transactions on
InformationTheory,2022. doi: 10.1109/TIT.2022.3195870.
117 Appendix
7.1 ProbabilityDistributionsandMeasureTheoreticFormalism
WeworkoverEuclideanspaceRdandletX beaLebesguemeasurablesetwithLebesguemeasure
λ(X) < ∞. ByλandLebesguemeasurableset,werefertothemeasureandσ-algebraF arising
fromtheusualconstructionofLebesguemeasureonRd. By“distribution”,wemeanaprobability
measureonthemeasurablespace(X,F ), whereF = {E ∩X : E ∈ F}. Letutheuniform
X X
distributionbethemeasuregivenbyu(E)=λ(E)/λ(X)forE ∈F . LetP denotethesetofall
X
probabilitymeasureson(X,F ).
X
We assume that P ∈ P and q ∈ Q ⊂ P have densities f and f with respect to the reference
P q
measureλ. Attimes,itwillbeusefultoassumethatdensitiesareboundedawayfromzeroincertain
partsofspace. Bysayingdensitiesareboundedintheirsupportbyβ > α > 0,wemeanthatfor
all x ∈ supp(q) := cl{x : f (x) > 0}, we have α ≤ f (x) ≤ β, where the closure is defined
q q
throughopenballsintheEuclideanmetric. Notethatinthissetting,wehaveq(supp(q)) = 1,as
q(supp(q)c)=(cid:82) 1[x∈supp(q)c]f (x)dλ(x)=(cid:82) 0dλ=0.
q
Denoteviaqn =q⊗···⊗qtheproductmeasureoverthemeasurablespace(X⊗n,F⊗n). Sucha
X
measurecorrespondstotheprocessoftakingni.i.d. samples∼ q. Denotethedensityofqn with
respecttoλnviafn.
q
Wedefinelog(1/0) = ∞. Followingtheconventionsin[28],wesaythat1[x ∈ E]·g(x) = 0if
g(x)<∞forx∈E andg(x)=∞forsomex∈Ec. Thisallowsustointegrateoverthefinitepart
offunctionsandgetafiniteresult.
Tofacilitatedigestibility,werefrainfrommeasuretheoreticnotationasmuchaspossible. Itisat
timesuseful,particularlyindealingwithtotalvariation. Weassumethroughoutthatallfunctionswe
encounterintheAppendix–includingthefixedvalidityfunctionvandfunctionsinthevalidityclass
V –are(X,F )-measurable.
X
7.2 EstimatesofValidity,Invalidity
Wefixthevalidityfunctionvasanarbitraryfunctionv :X →{0,1}measurablewithrespecttoeach
distributionqarisingintheAppendix. Asdiscussedabove,foragivenmodelq ∈P,the“validity”of
qisthequantityV(q)=Pr (v(X)=1),andthe“invalidity”I(q)=1−V(q). Wewillattimes
X∼q
beinterestedinestimatingthevalidityofamodelqusingsamplesfromqalongwithvalidityqueries.
Givenani.i.d. sample{X }n fromq,weletVˆ(q)=(cid:80)n v(X )/nbethenaturalestimateofthe
i i=1 i=1 i
validityofq.
At times, we will be interested in the validity of a model under an estimate of the underlying
validity function. To this end, given a model q ∈ P and a function g : X → {0,1}, we let
V (q)=Pr (g(X)=1). Givenasample{X }n ,letVˆ (q)=(cid:80)n g(X )/n. Notethatinthe
g X∼q i i=1 g i=1 i
languageofthisnotation,wehaveV(q)=V (q)andVˆ(q)=Vˆ (q). Weextendthisnotationinthe
v v
naturalwaytoinvalidityquantities.
7.3 AnalysisofEmpiricalRiskMinimization,ImproperAlgorithminRealizableSetting
Tobeginouranalysisoftherealizablesetting,wefirstobservethatmodelswithappreciableinvalidity
lookverydifferentfromafully-validdatageneratingdistribution–becausetheymusthavemass
inpartsofspacewhereP doesnot,theyareseparatedintotalvariationfromP byamargin. We
formalizethisideaviathefollowing.
Lemma1. Fix0<ϵ<1arbitrarily. Foranyvalidityfunctionv,ifq ∈P hasI(q)>ϵ,andP ∈P
hasI(P)=0,then
d (P,q)>ϵ.
TV
Proof. Fixthevalidityfunctionarbitrarily. ConsidertheeventE ={x∈X :v(x)=0}. Then
¬v
wehaved (P,q)=sup |P(E)−q(E)|≥q(E )−P(E )>ϵ,wherewehaveusedthat
TV E⊆X ¬v ¬v
I(P)=0impliesP(E )=0.
¬v
12Wenextextendthisobservationtotheassociatedproductmeasures,whicharethemaintargetof
analysisunderi.i.d. samplingfromP. Theideaistolowerboundthetotalvariationbetweenproduct
measures by the difference in probabilities on the event that at least one example from a sample
ofsizenisinvalid. Ofcourse,foreachsample,thishappenswithprobability0underP andwith
probabilityatleastϵunderanymodelqwithatleastϵinvalidity. Thus,identicallytohowoneshows
realizableratesforclassificationtasks,weattainalargetotalvariationgapbetweenP andanyqwith
appreciableinvalidity–mistakesinclassificationarethusanalogoustothegenerationofan“invalid”
samplesinoursetting.
Lemma2. Fix0 < ϵ < 1andn ∈ N\{0}arbitrarily. Foranyvalidityfunctionv,ifq ∈ P has
I(q)>ϵ,andP ∈P hasI(P)=0,then
d (Pn,qn)>1−e−nϵ.
TV
Proof. Fixthevalidityfunctionarbitrarily. Considerlowerboundingthetotalvariationbetweenthe
productmeasuresviathemagnitudeofthedifferenceoftheirmeasuresontheevent
(cid:26) (cid:27)
E = (x ,...,x )∈Xn :∃is.t.v(x )=0 .
≥1 1 n i
BecauseP hasperfectvalidity, anygivendrawfromP hasprobability0ofbeinginvalid. Thus,
Pn(E ) = 0. On the other hand, the invalidity of q states that for any 1 ≤ i ≤ n, we have
≥1
q({x∈X :v(x)=0})>ϵ. LetE ={x∈X :v(x)=1},andnotethatq(E )<1−ϵ. Thenwe
v v
have
qn(E )=1−q(E )n
≥1 v
>1−(1−ϵ)n
≥1−e−nϵ,
wherethefinalinequalityfollowsfromthefactthat(1+x/n)n ≤exforx≤n.
We can then borrow from classical analysis of hypothesis testing given by the Neyman-Pearson
lemmatoleveragethisgapintotalvariationbetweenproductmeasuresintoaboundontheprobability
thatafternsamples,amodelwithappreciableinvalidityhasasmallerlossthanP.
Lemma3. Fix0 < ϵ < 1andn ∈ N\{0}arbitrarily. Foranyvalidityfunctionv,ifq ∈ P has
I(q)>ϵ,P ∈P hasI(P)=0,andqandP havedensitieswithrespecttothereferencemeasureλ,
then
(cid:18) (cid:19)
Pr qn(S)≥Pn(S) ≤e−nϵ.
S∼Pn
Proof. TheprooffollowsthatoftheNeyman-Pearsonlemma’sclaimthattheLikelihoodRatioTest
achievesthelowerboundonthesumofTypeIandTypeIIerrors[18],combinedwithLemma2. We
givethefullargumentforcompleteness.
Fixthevalidityfunctionarbitrarily,andnotethefollowingstringofrelations:
(cid:18) (cid:19) (cid:90)
Pr qn(S)≥Pn(S) = 1[fn(x)≥fn(x)]fn(x)dλn(x)
S∼Pn q P P
(cid:90)
≤ min(cid:0) fn(x),fn(x)(cid:1) dλn(x)
q P
=1−d (Pn,qn)
TV
≤e−nϵ,
wheretheswitchtototalvariationinthesecondtolastlineistheresultofaclassiccharacterization
oftotalvariationgivenby“Scheffé’sTheorem”,andthefinallinecomesfromLemma2.
13Togeneratelossguarantees,weneedtobeabletoreasonaboutmodelswhichdonothaveappreciable
invalidity. Thenextlemmaisananalogueofthepreviousone–itisidenticaluptothereplacement
oftheconditionthatqhaveappreciableinvaliditywithaweakeronethatthetotalvariationbetween
amodelqandthedatadistributionP isappreciable. Whenqdoesnothaveappreciableinvalidity,we
cannolongerrelyonthestructureofthesupportsofqandP togeneratelargemarginsforthetotal
variationseparationofproductmeasures,andhavetofallbackonmoregeneralestimatesfortotal
variationbetweenproductmeasures.
Lemma4. Fix0<ϵ,δ <1arbitrarily,andletP,q ∈P bedistributionswithdensitieswithrespect
toλ. Thenifd (q,P)≥ϵ,andS ∼Pnforn≥Ω(log(1/δ)/ϵ2),itholdswithprobability≥1−δ
TV
that
L (P)<L (q).
S S
Proof. WhenqandP bothpossessdensities,wecanrelatedthetheprobabilitythatthelikelihoodof
qisatleastthatofP totheirtotalvariation,asintheNeyman-PearsonLemma.
(cid:18) (cid:19) (cid:90)
Pr Pn(S)≤qn(S) = 1[fn(x)≤fn(x)]fn(x)dλn(x)
S∼Pn P q P
(cid:90)
≤ min(cid:0) fn(x),fn(x)(cid:1) dλn(x)
P q
=1−d (Pn,qn)
TV
≤e−ndTV(p,q)2/2
≤e−nϵ2/2.
Here, the second to last inequality is the consequence of powerful result of [19] (and later [29]),
namelythatforanytwocollectionsofprobabilitymeasures{q }n and{p }n overmeasurable
i i=1 i i=1
spaces{(X ,F )}n ,theproductmeasuresovertherespectivecollectionsqnandpnsatisfy
i i i=1
(cid:32) n (cid:33)
1(cid:88)
1−exp − d (q ,p )2 ≤d (qn,pn).
2 TV i i TV
i=1
ThefinalinequalityfollowsfromassumedgapintotalvariationbetweenP andq.
TheprevioustworesultsconcernthetestingofindividualmodelsagainstP. Inthestandardway,we
nowleveragethefinitecardinalitytoargueviaaunionboundthatgivenenoughsamplesfromP,it’s
unlikelythattheERMmodelhasalargetotalvariationdistancefromP.
Lemma5. Fix0 < δ,ϵ ,ϵ < 1arbitrarily,andsupposeP ∈ Q. IfP isfully-validunderv,and
1 2
(cid:16) (cid:17)
S ∼ Pn for n ≥ Ω log(|Q|)+log(1/δ) , then with probability ≥ 1−δ over S ∼ Pn, the ERM
min(ϵ2 1,ϵ2)
solution
(cid:88)
qˆ=argmin log(1/q(x )),
i
q∈Q
xi∈S
satisfiesboth
d (qˆ,P)≤ϵ and I(qˆ)≤ϵ .
TV 1 2
Proof. LetQ ={q ∈Q:d (q,P)>ϵ }. ByLemma4andaunionbound,itholdsthat
dTV>ϵ1 TV 1
(cid:18) (cid:19) (cid:18) (cid:19)
Pr d (qˆ,P)>ϵ ≤Pr ∃q ∈Q s.t.L (q)≤L (P)
S∼Pn TV 1 S∼Pn dTV>ϵ1 S S
(cid:18) (cid:19)
≤|Q |· max Pr L (q)≤L (P)
dTV>ϵ1
q∈QdTV>ϵ1
S∼Pn S S
≤|Q|e−nϵ2 1/2.
14Inthesamemanner,letv beanarbitraryvalidityfunction,andletQ = {q ∈ Q : I(q) > ϵ }.
I>ϵ2 2
Thenwehave
(cid:18) (cid:19) (cid:18) (cid:19)
Pr I(qˆ)>ϵ ≤Pr ∃q ∈Q s.t.L (q)≤L (P)
S∼Pn 2 S∼Pn I>ϵ2 S S
(cid:18) (cid:19)
≤|Q |· max Pr L (q)≤L (P)
I>ϵ2
q∈QI>ϵ2
S∼Pn S S
≤|Q|e−nϵ2,
wherethefinalinequalityholdsbyLemma3. Thenfinally,
(cid:18) (cid:19)
Pr
S∼Pn
d TV(qˆ,P)>ϵ
1
∨ I(qˆ)>ϵ
2
≤|Q|e−nϵ2 1/2+|Q|e−nϵ2,
andsochoosingn≥2(log(|Q|)+log(1/δ))/min(ϵ2,ϵ )ensuresthatthesumofthesefinalterms
1 2
is≤δ.
Before we can prove Theorem 1, we need one final intermediate result, which we now give. It
statesthatthevalueofthelog-lossatanygivenxforamixturedistributionconstructedbyheavily
weightingoneoftwodistributionsisnotmuchdifferentthanthevalueofthelossfortheheavily
weightedcomponent. Thisfollowsfromthefactthatthenaturallogiswell-approximatedbyalinear
functionnear1. WewillusethisresulttoarguethatthelossoftheoutputofAlgorithm1isnot
significantlydifferentthanthatoftheERMinthesupportoftheERM.
Lemma 4. Fix 0 < ϵ < 1. For any q ∈ P having a density with respect to λ, the mixture
M =(1−ϵ/2)q+ϵu/2hasdensityf (x)=(1−ϵ/2)f (x)+ϵf (x)/2andthisdensitysatisfies
M q u
(cid:18) (cid:19) (cid:18) (cid:19)
log 1/f (x) ≤ϵ+log 1/f (x) ,
M q
forallx∈X.
Proof. Theexistenceclaimonthedensitiesisimmediategiventhedefinitionofuastheuniform
distributionwithrespecttothereferencemeasureλ. Toseetheinequality,fixx∈X arbitrarily,and
notethatf (x)≥(1−ϵ/2)f (x). Thus,wemaywrite
M q
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
1
log 1/f (x) ≤log +log 1/f (x)
M 1−ϵ/2 q
(cid:18) (cid:19) (cid:18) (cid:19)
1
≤ −1 +log 1/f (x)
1−ϵ/2 q
(cid:18) (cid:19)
ϵ/2
≤ +log 1/f (x)
1−ϵ/2 q
(cid:18) (cid:19)
≤ϵ+log 1/f (x) ,
q
wheresecondequalitycomesfromthefactthatlog(z)≤z−1,thefinalfollowsfromthefactthat
z/(1−z)≤2zforz ≤1/2.
Togetguaranteesforthelog-loss,wemakeheavyuseofthepreviouslemma. Beingabletoguarantee
asmalltotalvariationaldistancefromtheERMtoP andsmallinvaliditymeansthattheERMoutput
isalreadylikelytobeafaithfulrepresentationofP withsmallinvalidity. Allthatisthenneededisto
eliminatethepossibilitythattheERMhasalargelog-lossbecauseofsmallmismatchesinsupport
withP.
Todealwiththispossibility,wemixtheERMmodelwiththeuniformdistributioninaccordancewith
Algorithm1. BecausetheweightgiventotheuniformdistributionisO(min(ϵ2,ϵ )),theinvalidity
1 2
isclosetothatoftheERM.Toshowthatsuchamovedoesnotincreasethelosssignificantly,we
splitthecontributiontothelossoftheoutputtedmodelintotwothatarisingfromsupp(qˆ )and
ERM
it’s complement. We first observe that the ERM always has an empirical risk which is a faithful
estimatoroftheintegralE [1[X ∈supp(qˆ )]·log(1/f (x))],allowingustoboundthis
X∼P ERM qˆERM
15integralaboveintermsofthelossofP. Theintegralofthelossoversupp(qˆ )ccanbeshownto
ERM
besmallusingthelowerboundonthedensityoftheoutputmodelqˆaffordedbymixingwiththe
uniformdistribution,andthefactthatsupp(qˆ )c musthaveasmallmeasureunderP whenthe
ERM
ERMisclosetoP intotalvariation.
Theorem1. Fix0<δ,ϵ ,ϵ <1arbitrarily,andsupposeP ∈QandthatP isfully-validunderv.
1 2
Ifitholdsthatforeachq ∈Qthatα≤f (x)≤β forallx∈supp(q),thenthereisan
q
(cid:32) log2(1/min(ϵ ,ϵ ,α))·(cid:0) log(|Q|)+log(1/δ)(cid:1)(cid:33)
N ≤O˜ 1 2 ,
min(ϵ2,ϵ )
1 2
suchthatforalln≥N,withprobability≥1−δ,theoutputqˆofAlgorithm1satisfies
L (qˆ)≤L (q∗)+ϵ and I(qˆ)≤ϵ .
P P 1 2
Proof. Fixvarbitrarily,andletϵ =min(ϵ ,ϵ ). Toseetheclaimonthevalidity,notethatbythe
∧ 1 2
guaranteeofLemma5,theassymptoticcomplexityofLemma5yieldstheguaranteeI(qˆ )≤ϵ /2
ERM 2
withprobability≥1−δ/3,inwhichcase
Pr (cid:0) v(X)=0(cid:1) =Pr (cid:0) v(X)=0(cid:1) ·(cid:16) 1− ϵ ∧(cid:17) +Pr (cid:0) v(X)=0(cid:1) · ϵ ∧
X∼qˆ X∼qˆERM 8 X∼U 8
ϵ (cid:16) ϵ (cid:17) ϵ
≤ 2 · 1− ∧ +1· ∧
2 8 8
<ϵ .
2
The loss of the outputted model qˆ can be decomposed into the contributions from the loss in
supp(qˆ )andit’scomplement:
ERM
(cid:20) (cid:21) (cid:20) (cid:21)
L (qˆ)=E 1[X ∈supp(qˆ )]·log(1/f (X)) +E 1[X ∈supp(qˆ )c]·log(1/f (X)) .
P X∼P ERM qˆ X∼P ERM qˆ
Toboundthefirstterm,foreachq ∈Q,considerthefunction
(cid:26)
log(1/f (x)) ifx∈supp(q),
B (x)= q
q 0 else.
These functions are bounded above by log(1/α) and below by log(1/β) (if β < 1, simply
loosen the density upper bound), and thus for a sample X ∼ P, define bounded random vari-
ables B (X). By Hoeffding’s inequality and a union bound, it holds that a sample S of size
q
n ≥ Ω˜(log2(1/α)(log(|Q|)+log(1/δ))/ϵ2) from P is large enough such that with probability
1
≥1−δ/3,foreachq ∈Q,itholdsthat
(cid:12) (cid:12)
(cid:12) (cid:12)E [B (X)]− 1 (cid:88) B (x )(cid:12) (cid:12)≤ ϵ 1.
(cid:12) X∼P q n q i (cid:12) 8
(cid:12) (cid:12)
xi∈S
Notethatforeachq ∈QwithL (q)<∞,itholdsthatL (q)coincideswiththeempiricalestimates
S S
ofE [B (X)],namely
X∼P q
1 (cid:88) 1 (cid:88)
B (x )= log(1/f (x )).
n q i n q i
xi∈S xi∈S
Furthermore, this coincidence takes place for qˆ with probability 1, as P ∈ Q implies that
ERM
L (qˆ )≤L (P)<∞withprobability1–notethatL (P)isagoodestimatorforL (P)in
S ERM S S P
thesensearisingfromanapplicationofHoeffding,aslog(1/f (X))isboundedalmostsurelyfor
P
X ∼P giventhatP hasadensitythatisboundedinit’ssupport(asamemberofQ). Thus,wemay
write
(cid:20) (cid:21) (cid:20) (cid:21)
ϵ
E 1[X ∈supp(qˆ )]·log(1/f (X)) ≤E 1[X ∈supp(qˆ )]·log(1/f (X)) + 1
X∼P ERM qˆ X∼P ERM qˆERM 4
ϵ
=E [B (X)]+ 1
X∼P qˆERM 4
3ϵ
≤L (qˆ )+ 1
S ERM 8
ϵ
≤L (q∗)+ 1,
P 2
16whereweinvokeLemma4inthefirststep,andinthelaststepusethatq∗ =P bythestrictproperness
ofthelog-loss.
To bound the second term, note that by the argument in given in Lemma 5, when
n ≥ Ω(cid:0) log2(1/ϵ )·(log(|Q|+log(1/δ))/ϵ2(cid:1) , it holds with probability ≥ 1 − δ/3 that
∧ 1
d (qˆ ,P) ≤ ϵ /8log(8/ϵ ). Because qˆhas density at least ϵ /8 everywhere in X,2 we
TV ERM 1 ∧ ∧
have
(cid:20) (cid:21) (cid:20) (cid:21)
E 1[X ∈supp(qˆ )c]·log(1/f (X)) ≤log(8/ϵ )·E 1[X ∈supp(qˆ )c]
X∼P ERM qˆ ∧ X∼P ERM
≤log(8/ϵ )·d (P,qˆ )
∧ TV ERM
<ϵ /2,
1
whereinthesecondlineweusethefactthatqˆ (supp(qˆ ))=1toarguethatd (P,qˆ )≥
ERM ERM TV ERM
P(supp(qˆ )c). Combiningtheboundsonthetwosummandsandunionboundingtheconfidence
ERM
yieldsthefullguarantee.
Wenotethatthisargumentimpliesaslightmoreprecisesamplecomplexityboundgivenby
(cid:32) (cid:32) log(|Q|)+log(1/δ) log2(1/min(ϵ ,ϵ ,α))(cid:0) log(|Q|)+log(1/δ)(cid:1)(cid:33)(cid:33)
O˜ max , 1 2 ,
ϵ ϵ2
2 1
whichaffordsaminorimprovementincertainregimeswhereϵ <ϵ2.
2 1
7.4 AZero-QueryLowerBound
Theorem2. Forallϵ < 1/4andforallproperlearnersL : S → P,ifthesampleS ∼ Pn isof
2
sizen≤1/8ϵ ,thenthereexistsatriple(P,Q,v)withP ∈QandP fully-valid,onwhichL(S)has
2
invalidityI(L(S))>ϵ withprobability≥1/4.
2
Proof. WegiveanargumentinspiredbytheproofofalowerboundinTheorem2of[30].
Let X = [0,1] ⊂ R. Fix the proper learner L and ϵ < 1/4 arbitrarily. Consider a model class
2
definedbyQ={P,P˜},whereP andP˜ havedensitieswithrespecttotheLebesguemeasure
1 1
f (x)=1[x∈[0,1−2ϵ ]] , f (x)=1[x∈[2ϵ ,1]] .
P 2 1−2ϵ P˜ 2 1−2ϵ
2 2
This Q gives rise to two realizable problem instances of interest. Under the first, P is the data
generatingdistributionandv(x)=1everywhereexceptforin(1−2ϵ ,1],wherev(x)=0. Under
2
thesecond, P˜ isthedatageneratingdistributionandv(x) = 1everywhereexceptforin[0,2ϵ ),
2
where v(x) = 0. Assume by contradiction that for both problem instances, given a sample of
sizen ≤ 1/8ϵ ,wehavethatI(L(S)) ≤ ϵ withprobability> 3/4. Inbothcases,wehavethat
2 2
I(q)=2ϵ /(1−2ϵ )>ϵ forthemodelqwhichisnotthedatageneratingdistribution. Thus,L(S)
2 2 2
isamodelwithI(L(S))≤ϵ withprobability>3/4overbothprobleminstancesifandonlyifit
2
identifiesthedatageneratingdistributionwithprobability>3/4overbothprobleminstances.
Consider the simple hypothesis tester defined by T (S) = L(S), which by the above, outputs
L
thecorrectdatageneratingdistributiongivenachoiceofP orP˜ –andgivenn ≤ 1/8ϵ samples
2
fromeitherP orP˜ –withprobability> 3/4. NotethatthedistributionsP,P˜ haved (P,P˜) =
TV
2ϵ /(1−2ϵ ) ≤ 4ϵ , where we use ϵ < 1/4 and z/(1−z) ≤ 2z for small enough z in the
2 2 2 2
inequality. Byaclassicupperboundonthetotalvariationbetweenproductmeasures[19],wehave
thatd (Pn,P˜n)≤4nϵ . Then,byLeCam’smethodandthisupperboundond (Pn,P˜n),we
TV 2 TV
have
1 1 1
inf max Pr (T (S)̸=q)≥ − ·d (Pn,P˜n)≥ −2nϵ .
T:S→{P,P˜}q∈{P,P˜} S∼qn L 2 2 TV 2 2
Thus,ifn≤1/8ϵ ,thethereisachoiceofdatageneratingdistributionsuchthatT (S)incurserror
2 L
probability≥1/4,whichisacontradiction.
2Whentheuniformdistributionhasadensitysmallerthan1,thereisanextralogfactortoaccountforhere.
WecanWLOGthisawaybyaddingtheconditionthatλ(X)=1,e.g.arisingfromnormalizeddata.
177.5 StrategiesArisingfromEstimationoftheValidityFunction
Whenthevalidityfunctionv isknowntolieinaclassofboundedcomplexity,itislearnable,and
learnedestimatesmaybeutilizedintheselectionofalow-loss,high-validitymodel. Aninteresting
featureoftheproblemsetuphereisthatunlikeinmostlearningsettings,thelearnercanactually
choosewhichdistributionsitwouldlikeestimatevwithrespectto,i.e. decideunderwhichmarginal
distributionsoverX theestimatevˆshouldsmalldisagreementwithv.
WebeginwithalemmaarguingthatwhenevertheERMhaspositiveprobabilityofoutputtingan
example with vˆ(x) = 1, and the disagreement of vˆ and v under a proposal distribution is small
enough,therestrictionwillindeedyieldalow-validitymodel.
Lemma5. Fix0<ϵ<1andadistributionq ∈Pabsolutelycontinuouswithrespecttoλarbitrarily.
Further,fixVˆ(q)suchthat0<Vˆ(q)≤V(q),andsupposethatforsomevˆ:X →{0,1}wehave
Vˆ(q)ϵ
Pr (vˆ(X)̸=v(X))≤ .
X∼q 2
Thenwheneverthereisadistributionqˆcorrespondingto
f (x)∝f (x)1[vˆ(x)=1],
qˆ q
ithasinvalidityI(qˆ)≤ϵ.
Proof. LetV (q) = Pr (vˆ(X)=1)denotethenormalizingconstantfortherestrictiontoesti-
vˆ X∼q
matedvalidregion. Notethattherestrictioncorrespondstoaprobabilitydistributionifandonlyif
V (q)>0,andthatinthiscase
vˆ
f (x)1[vˆ(x)=1]
f (x)= q .
qˆ V (q)
vˆ
Itholdsfurtherthatqˆisabsolutelycontinuouswithrespecttoλ,andthatwecanwritethefollowing
chainofrelations:
(cid:90)
I(qˆ)= 1[v(x)=0]f (x)dλ(x)
qˆ
1 (cid:90)
≤ 1[vˆ(x)̸=v(x)]f (x)dλ(x)
V (q) q
vˆ
Vˆ(q) ϵ
≤
V (q)2
vˆ
V(q) ϵ
≤ ,
V (q)2
vˆ
wherethefirstinequalityfollowsafterinputtingthedefinitionoff ,andthefinaltwoinequalities
qˆ
comebyassumption. It’sfurtherpossibletoshowthatvalidityofqcanbeapproximatedfromabove
byaconstantmultipleofV (q),whichcanbeconceptualizedasthevalidityofqifvˆwerethetrue
vˆ
validityfunction. Inparticular,
(cid:18) (cid:19)
V(q)= V (q)−V (q) +V(q)
vˆ vˆ
(cid:90) (cid:18) (cid:19)
=V (q)+ 1[v(x)=1]−1[vˆ(x)=1] f (x)dλ(x)
vˆ q
(cid:90)
≤V (q)+ 1[v(x)̸=vˆ(x)]f (x)dλ(x)
vˆ q
Vˆ(q)ϵ
≤V (q)+
vˆ 2
V(q)ϵ
≤V (q)+ .
vˆ 2
ThisimpliesthatV(q) ≤ V (q)/(1−ϵ/2),whichyieldsV(q) ≤ 2V (q)asϵ < 1. Utilizingthis
vˆ vˆ
inequalityinthelastlineofthefirststringofinequalitiesgivestheguarantee.
18Theneedforalowerestimateonthevalidityintheprecisionoftheestimateforvˆcanbeunderstood
asfollows: whentheproposaldistributionhasverysmallvalidity,therestrictiontotheestimation
ofthevalidpartsofspaceundervˆmaycreatehuge(orinfinite)increasesinmassovertheproposal
distribution–inthelanguageofLemma5,V (q)maybeverysmallforaproposaldistributionq. If
vˆ
thisisthecase,theestimatevˆmustbemoreprecise,assmallerrorsintheestimationofthevalidity
functionmayleadtoinvalidpartsofspacehavinglargemassunderqˆ.
WeintroduceanotherlemmabeforeprovingTheorem3. Itarguesthatforamodelqˆconstructedby
acceptingsamplesfromsome"proposaldistribution"qthatfallinthevalidpartofspaceundervˆ,the
contributiontothelossfromthepartofspacewherevˆagreeswithvcanneverexceedthetotallossof
theproposaldistributionq. Essentially,weareexploitingthefull-validityofP here–inthesubregion
oftheagreementregion{v(x)=vˆ(x)}onwhichthelossiscomputed,wehavethatvˆ(x)=1bythe
factthatX ∼P isvalid. Thismeansthatthedensityofqˆcanonlybelargerthanthedensityofqin
thisregion,whichunderanon-increasinglosscannotincreasethelossoverthatincurredbyq.
Lemma6. Fix0 < ϵ,δ < 1,avalidityfunctionestimatevˆ : X → {0,1},andq ∈ Qarbitrarily.
Supposel:R≥0 →[0,M]isanon-increasinglossfunction. Thenwhenever
f (x)∝f (x)1[vˆ(x)=1]
qˆ q
correspondstoaprobabilitydistribution,itenjoys
E (cid:2) l(f (X))·1[vˆ(X)=v(X)](cid:3) ≤L (q;l).
X∼P qˆ P
Proof. LetV (q)=Pr (vˆ(x)=1)bethenormalizingconstantforqˆ,wherewenotethatV (q)>
vˆ X∼q vˆ
0whenf (x)1[vˆ(x)=1]correspondstoaprobabilitydistribution.
q
GiventhatP isfully-valid,wehavePr (v(X)=1)=1. Bythefactthatintegrationisdefined
X∼P
uptonullsets,itholdsthat
(cid:20) (cid:21) (cid:20) (cid:21)
E l(f (X))·1[vˆ(X)=v(X)] =E l(f (X))·1[vˆ(X)=v(X)∧v(X)=1] .
X∼P qˆ X∼P qˆ
Further,wemaywrite
(cid:20) (cid:21)
E l(f (X))·1[vˆ(X)=v(X)∧v(X)=1]
X∼P qˆ
(cid:20) (cid:18)
f
(X)1[vˆ(X)=1](cid:19) (cid:21)
≤E l q ·1[vˆ(X)=1]
X∼P V (q)
vˆ
(cid:20) (cid:18) (cid:19)(cid:21)
f (X)
≤E l q
X∼P V (q)
vˆ
(cid:20) (cid:21)
≤E l(f (X))
X∼P q
=L (q;l).
P
Here, the second to last inequality comes from the non-negativity of the loss along with the fact
thatwhenevervˆ(X) = 0,theintegrandiszero–whenvˆ(X) = 1,thelossisjustevaluatedatthe
normalizeddensity,andsotheintegrandintroducedinthislineisanupperboundfortheprevious
integrand. Thefinalinequalitycomesfromthenon-increasingnessofthelossfunctionalongwiththe
observationthatV (qˆ)≤1–inremovingthenormalizingconstant,wecanonlymakethevalueat
vˆ
whichthelossisevaluatedatsmaller,whichcannotdecreasethevalueoftheloss.
We are now ready to prove the main result of the second half of the paper – the guarantee for
Algorithm2. Itcombinesthepreviouslemmas,notingfurtherthatthenumberofsamplesinS is
P
sufficienttomakethedisagreementofvandvˆsmallenoughunderP suchthatthecontributiontothe
lossinthatpartofspacecanbecontrolledbytriviallyapplyingthelossupperboundM.
19Theorem3. Supposev ∈V withVC-dimensionVC(V)≤D,andthatforeachq ∈Q,thevalidity
V(q) ≥ γ > 0. For all 0 < ϵ ,ϵ ,δ ≤ 1 and for all choices of non-increasing loss functions
1 2
l:R≥0 →[0,M],Algorithm2requiresanumberofsamples
(cid:18) M2(log(|Q|)+log(1/δ)) M(Dlog(M/ϵ )+log(1/δ))(cid:19)
≤O + 1 ,
ϵ2 ϵ
1 1
andanumberofvalidityqueries
(cid:18) (cid:19)
Dlog(1/γϵ )+log(1/δ)
≤O 2 ,
γϵ
2
toensurethatwithprobability≥1−δ,itsoutputenjoys
L (qˆ;l)≤L (q∗;l)+ϵ and I(qˆ)≤ϵ .
P P 1 2
Proof. Given that the loss is bounded, Hoeffding’s inequality applied to the random variables
l(f (X)) for X ∼ P, and a union bound, imply that S is large enough that with probability
q
≥1−δ/3,wehavethatforallq ∈Q,theempiricallossestimatesL (q;l)areatmostϵ /4away
S 1
fromtruelossesL (q;l). Foranychoiceofqˆ ,becausewehavev ∈ V,itmustholdthatany
P ERM
minimizer vˆis consistent with the labeling under v of both S and S . The standard rates of
P qˆERM
convergencewhenchoosinganarbitraryconsistenthypothesisthusimplythatthesizesofS and
P
S arelargeenoughtoguaranteethat,withprobability≥1−2δ/3,wehave
qˆERM
(cid:18) (cid:19) (cid:18) (cid:19)
ϵ γϵ
Pr vˆ(X)̸=v(X) ≤ 1 ∧ Pr vˆ(X)̸=v(X) ≤ 2.
X∼P 2M X∼qˆERM 2
Byaunionbound,withprobability≥1−δ,alloftheseestimationaccuracyeventstakeplace. We
conditiononthesefavorableeventstakingplacegoingforwards.
Notethatconditionedonthesefavorableevents,thenormalizingconstantqˆ ({vˆ(x)=1})>0,
ERM
asforanyERM,wehave
(cid:90)
qˆ ({vˆ(x)=1})=E [1[v(x)=1]]− (1[v(x)=1]−1[vˆ(x)=1])dqˆ (x)
ERM X∼qˆERM ERM
(cid:90)
≥E [1[v(x)=1]]− 1[v(x)̸=vˆ(x)]dqˆ (x)
X∼qˆERM ERM
γϵ
≥γ− 2
2
>0.
Thus,therestrictionoftheERMtotheestimatedvalidityregionisaviableprobabilitydistribution,
andisoutputtedbythealgorithmasqˆ. Foranyestimateofthevalidityfunctionvˆ,wecandecompose
thelossofqˆas
(cid:20) (cid:21) (cid:20) (cid:21)
L (qˆ;l)=E l(f (X))·1[vˆ(X)=v(X)] +E l(f (X))·1[vˆ(X)̸=v(X)] .
P X∼P qˆ X∼P qˆ
FirstusingLemma6,andthenusingtheuniformconvergenceofthelossestimates,wecanbound
thefirsttermas
(cid:20) (cid:21)
E l(f (X))·1[vˆ(X)=v(X)] ≤L (qˆ ;l)
X∼P qˆ P ERM
ϵ
≤L (q∗;l)+ 1
P l 2
ϵ
≤L (q∗;l)+ 1,
P 2
whereq∗ =argmin L (q;l)isthelowest-lossmodelintheclassQ. Toupperboundthesecond
l q∈Q P
terminthelossdecomposition,wecanusethefactthatPr (vˆ(X)̸=v(X))≤ϵ /2M andthe
X∼P 1
upperboundonthelosstowrite
(cid:20) (cid:21) (cid:20) (cid:21)
ϵ
E l(f (X))·1[vˆ(X)̸=v(X)] ≤M ·E 1[vˆ(X)̸=v(X)] ≤ 1,
X∼P qˆ X∼P 2
20Algorithm3RestrictiontoERMunderLog-LosswithoutValidityAssumption
1: procedurevalid_restriction_log(DistributionClassQ,ValidityClassV,D,δ,ϵ 1,ϵ 2)
(cid:16) (cid:17)
2: S ←Ω
M2(log(|Q|)+log(1/δ))
i.i.dsamples∼P
ϵ2
1
(cid:80)
3: qˆ
ERM
←argmin
q∈Q
x∈Smin(M,−log(f q(x)))
4: q˜
ERM
←(1−ϵ 1/8)·qˆ ERM+ϵ 1/8·D ▷MixwithconstantvalidityD
(cid:16) (cid:17)
5: S P ←Ω M(Dlog(M/ ϵϵ 11)+log(1/δ)) i.i.d. samples∼P,
(cid:16) (cid:17)
S ←Ω Dlog(1/ϵ1ϵ2)+log(1/δ) i.i.d. samples∼q˜
q˜ERM ϵ1ϵ2 ERM
6: vˆ←argmin h∈V(cid:80) x∈SP∪Sq˜ERM 1[h(x)̸=v(x)] ▷Labelx∈S q˜ERM viav
7: returnf qˆ∝f q˜ERM(x)·1[vˆ(x)=1]ifq˜ ERM({vˆ(x)=1})>0elsef qˆ=f qˆERM
8: endprocedure
yieldingthelossguarantee.
ThevalidityguaranteefollowsdirectlyfromthefactthatPr (vˆ(X)̸=v(X)) ≤ γϵ /2and
X∼qˆERM 2
Lemma5,whereγ furnishesthelowerestimateforthevalidityofthemodelqˆ .
ERM
ThecorollarytoTheorem3statingthatonlylow-lossmodelsneedappreciablevalidityisstraightfor-
wards. OnecansimplyaddanextralinetotheproofofTheorem3,arguingthatwhentheintersection
ofgoodestimationeventstakesplace,thelossoftheERMdistributioniswithinO(ϵ )oftheoptimal
1
lossacrossmodelsinQ,meaningthatithasvaliditygreaterthansomeconstantc. Thus,onecanrun
Algorithm2withanS largeenoughtoachieveO(ϵ )disagreementratebetweenvˆandvunder
qˆERM 2
samplesfromqˆ ,loweringthelabelcomplexity.
ERM
TheproofofTheorem4isverysimilartothatofTheorem3. Themaindifferenceisthatwhenthe
lossisthecappedlog-loss,wecanexploitastabilitypropertyundermixturesimilartothatintroduced
inLemma4. Thisallowsustomixqˆ withadistributionofconstantvaliditytogetavalidity
ERM
lowerboundonthefinalproposaldistributionq˜ withoutincreasingthelossmorethanO(ϵ ).
ERM 1
ThevaliditylowerboundcanthenbeusedasinTheorem3.
Theorem4. Supposev ∈ V whereVC(V) ≤ D, andthatforeachq ∈ Q, wehavef (x) ≤ β.
q
SupposefurtherthatthereissomeknownD ∈P withdensityf whichhasV(D)≥c>0forsome
D
constantc. Thenforallchoicesof0<ϵ ,ϵ ,δ <1/2andM >0,Algorithm3requiresanumber
1 2
ofsamples
(cid:18) M2(log(|Q|)+log(1/δ)) M(Dlog(M/ϵ )+log(1/δ))(cid:19)
≤O˜ + 1 ,
ϵ2 ϵ
1 1
andanumberofvalidityqueries
(cid:18) (cid:19)
Dlog(1/ϵ ϵ )+log(1/δ)
≤O 1 2 ,
ϵ ϵ
1 2
toensurethatwithprobability≥1−δ,itsoutputenjoys
(cid:20) (cid:21) (cid:20) (cid:21)
E min(M,log(1/f (X))) ≤E min(M,log(1/f (X))) +ϵ and I(qˆ)≤ϵ .
X∼P qˆ X∼P q∗ 1 2
Proof. WLOGassumeβ ≥1,andconsiderlearningover¯l(z)=min(M,log(1/z))−log(1/β),a
translationofthecappedlog-lossboundedbelowby0forallinputstof ∈Q,andboundedabove
q
byM¯ =M −log(1/β).
SimilartotheproofofTheorem3,withprobability≥ 1−δ/2overthesampleS ∼ Pn,itholds
that for all q ∈ Q that (cid:12) (cid:12)L S(q;¯l)−L P(q;¯l)(cid:12) (cid:12) ≤ ϵ 1/8; in this case, we use the fact that f q ≤ β to
ensurethattherandomvariables¯l(f (X))forX ∼P arebounded,allowingforanapplicationof
q
21Hoeffding’sinequalityoverempiricalestimatesofalossunboundedbelow. Asabove,foranychoice
ofq˜ ,thesizesofS andS aresuchthatwithprobability≥1−2δ/3,
ERM P q˜ERM
(cid:18) (cid:19) (cid:18) (cid:19)
ϵ cϵ ϵ
Pr vˆ(X)̸=v(X) ≤ 1 ∧ Pr vˆ(X)̸=v(X) ≤ 1 2.
X∼P 2M X∼q˜ERM 16
Asbefore,byaunionbound,theseboundsbothholdsimultaneouslywithprobability≥1−δ. We
conditiononthisintersectionoffavorableeventsgoingforwards.
Note that when this intersection of events takes place, we have q˜ ({vˆ(x)=1}) > 0. In this
ERM
case,wehavethatV(q˜ )≥ϵ V(D)/8≥ϵ c/8asV(D)≥c,andsoidenticallytoourworkin
ERM 1 1
Theorem3,wemaywrite
(cid:90)
q˜ ({vˆ(x)=1})≥E [1[v(X)=1]]− 1[vˆ(x)̸=v(x)]dq˜ (x)
ERM X∼q˜ERM ERM
cϵ cϵ ϵ
≥ 1 − 1 2
8 16
>0.
Thus, the restriction of q˜ to the estimate of the valid region is defined and outputted by the
ERM
algorithmasqˆ. Toseethatthelossguaranteethenholdsforsuchaqˆ,considerthelossdecomposition
usedintheproofofTheorem3:
(cid:20) (cid:21) (cid:20) (cid:21)
L (qˆ;¯l)=E ¯l(f (X))1[vˆ(X)=v(X)] +E ¯l(f (X))1[vˆ(X)̸=v(X)] .
P X∼P qˆ X∼P qˆ
WeupperboundthesecondtermexactlyasinTheorem3. Toupperboundthefirstterm,consideran
argumentsimilartothatoftheproofofLemma6. LetV (q˜ )>0bethenormalizingconstantfor
vˆ ERM
qˆ. Wecanwrite
(cid:20) (cid:21) (cid:20) (cid:18)
f
(X)1[vˆ(X)=1](cid:19) (cid:21)
E ¯l(f (X))1[vˆ(X)=v(X)] ≤E ¯l q˜ERM ·1[vˆ(X)=1]
X∼P qˆ X∼P V (q˜ )
vˆ ERM
(cid:20) (cid:18) (cid:19)(cid:21)
f (X)
≤E ¯l q˜ERM
X∼P V (q˜ )
vˆ ERM
(cid:20) (cid:21)
≤E ¯l(f (X)) .
X∼P q˜ERM
Here,thenon-increasingnessofthelossstillholds,leadingtothefinalstep. Now,fixsomex∈X
arbitrarily,andnotethefollowing,inthestyleofLemma4:
(cid:18) (cid:19)
¯l(f (x))+log(1/β)=log 1/f (x) ∧M
q˜ERM q˜ERM
(cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
1
≤ log +log 1/f (x) ∧M
1−ϵ /8 qˆERM
1
(cid:18) (cid:18) (cid:19)(cid:19)
≤ ϵ /4+log 1/f (x) ∧M
1 qˆERM
(cid:18) (cid:19)
ϵ
≤log 1/f (x) ∧M + 1.
qˆERM 4
Thus,itholdsthat¯l(f (x))≤¯l(f (x))+ϵ /4,andsowemaywrite
q˜ERM qˆERM 1
(cid:20) (cid:21) (cid:20) (cid:21)
ϵ
E ¯l(f (X)) ≤E ¯l(f (X)) + 1
X∼P qˆ X∼P qˆERM 4
ϵ
=L (qˆ ;¯l)+ 1.
P ERM 4
Thus,wehavewrittenthelossintermsoftheERM,andsowehave,asinTheorem3,thatthefirst
termofthelossdecompositioncanbeboundedbyL (q∗)+ϵ /2.
P 1
GivenPr (vˆ(X)̸=v(X))≤cϵ ϵ /16,andthelowerboundonthevalidityofq˜ derived
X∼q˜ERM 1 2 ERM
inthethirdparagraphabove,wecanagainapplyLemma5togetthevalidityguarantee.
22