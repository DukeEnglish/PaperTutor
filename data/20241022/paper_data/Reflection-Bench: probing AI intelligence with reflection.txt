Reflection-Bench: probing AI intelligence with reflection
LingyuLi1,2,3,YixuWang1,HaiquanZhao1,ShuqiKong1,2,3,
YanTeng†1,ChunboLi†2,3,YingchunWang1
1ShanghaiArtificialIntelligenceLaboratory
2ShanghaiJiaoTongUniversity
3ShanghaiMentalHealthCenter
†Correspondence:tengyan@pjlab.org.cn&licb@smhc.org.cn
Abstract
The ability to adapt beliefs or behaviors in
response to unexpected outcomes, reflection,
is fundamental to intelligent systems’ inter-
action with the world. From a cognitive sci-
enceperspective,thisservesasacoreprinciple
of intelligence applicable to both human and
AI systems. To address the debate on the in-
telligence of large language models (LLMs),
we propose Reflection-Bench, a comprehen-
sive benchmark comprising 7 tasks spanning Figure 1: Reflection, a fundamental process of intel-
corecognitivefunctionscrucialforreflection, ligence, integrates various cognitive components. To
including perception, memory, belief updat- achieve desired outcomes, an intelligent agent must
ing, decision-making, prediction, counterfac- predicttheexternalworldstatesandbehavioralconse-
tual thinking, and meta-reflection. We evalu- quencesbasedonpriorbeliefs. Post-action,discrepan-
ate the performances of 13 prominent LLMs ciesbetweenpredictionandobservationareperceived,
suchasOpenAIo1,GPT-4,Claude3.5Sonnet, prompting an update of prior belief. This update in-
etc. TheresultsindicatethatcurrentLLMsstill volvesrecallingthepreviousdecisionprocessanden-
lacksatisfactoryreflectionability. Wediscuss gagingincounterfactualthinkingaboutun-chosenalter-
theunderlyingcausesoftheseresultsandsug- natives.
gestpotentialavenuesforfutureresearch. In
conclusion,Reflection-Benchoffersbotheval-
uationtoolsandinspirationfordevelopingAI
tionsoftenlackaunifyingframeworkgroundedin
capableofreliablyinteractingwiththeenviron-
ment.Ourdataandcodeareavailableathttps: afundamentaltheoryofintelligence,hinderinga
//github.com/YabYum/ReflectionBench. comprehensiveunderstandingofLLMcapabilities.
Our work aims to clarify this enigma and estab-
1 Introduction
lishefficientmetricsforassessingintelligenceby
drawing insights from cognitive science into the
Large language models (LLMs) have sparked
fundamentalprocessofintelligence,reflection.
intense debate regarding their true capabilities:
are they genuinely intelligent or simply sophis- From a first-principles perspective, cognitive
ticated statistical engines mimicking human lan- scienceconceptualizesintelligentsystemsaspre-
guage? (Bender et al., 2021) This question has dictivemachinesthatconstantlyanticipatefuture
profound implications for trust in AI and the de- eventsusinginternalmodels(Friston,2010). They
velopmentofappropriateregulations. Proponents cancopewithuncertainenvironmentsbyminimiz-
of AI’s human-level intelligence often advocate ingsurprises(mismatchbetweenpredictionsand
forstricterregulationsduetopotentialexistential observations)throughiterativeupdatestothoughts
risks,whileskepticsarguethatexcessiveregulation andactions,asshowninFigure1. Weoperational-
couldhinderinnovation. Numerousstudiesprobe ize this process as reflection, a cyclic process of
specific facets of LLM intelligence, such as rea- predictingbasedonpriorbelief,makingdecisions
soning(Clarketal.,2018),planning(Valmeekam toachievedesiredstates,perceivingsurprises,and
et al., 2024), and cognitive flexibility (Kennedy subsequentlyupdatingbeliefs. Reflectionencom-
andNowak,2024),etc. However,theseinvestiga- passescrucialcognitivecomponentsincludingper-
1
4202
tcO
12
]IA.sc[
1v07261.0142:viXraception,memory,beliefupdating,decision-making, • We conduct an assessment of 13 prominent
prediction,counterfactualthinking,andevenmeta- LLMs. ThestudyrevealsthatcurrentLLMs
reflection. Leveraging this framework, we intro- exhibit significant limitations in their ca-
duce Reflection-Bench, a benchmark designed to pacityforhuman-likereflection,especially
evaluatethereflectioncapabilitiesofLLMs. theuniversallackofmeta-reflectionabilities.
Baseduponwell-establishedcognitivescience
paradigms, we select 6 paradigms and design 7 2 Relatedwork
tasksadaptedforLLMsevaluation,includingodd-
Emergentability ‘Emergentabilities’inLLMs
ball paradigm, n-back task, probabilistic reversal
aredefinedascapabilities‘notpresentinsmaller
learningtask,Wisconsincardsortingtest,weather
models but present in larger models’(Wei et al.,
predictiontask,double-choiceIowagamblingtask,
2022). These manifest as the ability to ‘perform
andmeta-bandittask. Reflection-Benchleverages
abovetherandombaselineoncorrespondingtasks
these7taskstoofferacomprehensiveevaluation
without explicit training on those same tasks’,
of different aspects while engaging in reflection.
suchasreasoningandplanning. (Luetal.,2024).
Importantly, the difficulty of these tasks can be
Someresearchersarguethattheseemergentabili-
adjustedtoaccommodatevaryingcognitiveloads,
tiesmightdisappearwithbettermetricsorstatistics
ensuringReflection-Bench’sadaptabilitytomore
(Schaefferetal.,2023). Thisunderscorestheim-
advancedAImodelsandmaintainingitslong-term
portance of designing valid and efficient metrics
relevance.
forevaluatingintelligence,oneofthemostdebated
We evaluated 13 diverse LLMs on Reflection-
emergentpropertiesofLLMs.
Benchusingrelativelyeasysettingsandconducted
an in-depth analysis of their performance. The
Evaluating intelligence Various benchmarks
resultsdemonstrateReflection-Bench’sdiscrimina-
havebeendevelopedtoassessdifferentaspectsof
tivepower: o1-previewachievedthehighestscores,
LLMs’intelligence. TheAI2ReasoningChallenge
followedbytheotherstate-of-the-artLLMs,while
(ARC) evaluates reasoning, common sense, and
modelswithsmallersizesscoredlower. Wehigh-
text comprehension using multiple-choice ques-
lightanotablecontrastbetweeno1-preview’sper-
tions (Clark et al., 2018). PlanBench tests plan-
formancesintheoddballparadigmanditsperfor-
ning and reasoning with open-ended questions
mance in other tasks. Additionally, we observed
(Valmeekam et al., 2024). CELLO assesses four
a universal absence of meta-reflection across all
levelsofcausalinference-discovery,association,
models. Thepotentialcausesandimplicationsare
intervention,andcounterfactual-withmulti-choice
discussed. Inconclusion,ourfindingsindicatethat
questions(Chenetal.,2024). Beyondthesingle-
currentLLMsstillfallconsiderablyshortofhuman-
turnevaluations,researchersalsoinvestigateLLMs’
levelintelligence.
intelligence in uncertain environments through
To summarize, our main contributions are as
multi-turn dialogues. These include tasks such
follows:
asmulti-armedbanditsforprobabilityestimation
• We introduce reflection, a core feature of (Krishnamurthyetal.,2024),Wisconsincardsort-
intelligent systems, into the AI field as a ingtestsforruleinference(KennedyandNowak,
biologically-inspiredcriterionforprobingAI 2024),Minecraftforspatialreasoning(Madgeand
intelligence. Thisapproachoffersamorenu- Poesio, 2024), etc. These aspects are intercon-
anced understanding of LLMs’ intelligence nected in ways related to the epistemology of AI
thatalignswithhumans. systems-howtheylearnaboutandadapttotheex-
ternalworld. Whilethisfeaturehasbeenvaguely
• We propose Reflection-Bench, a compre-
recognizedbutnotclearlydefinedinAIfield.
hensive benchmark comprising seven tasks
adapted from cognitive science paradigms Firstprincipleofintelligence Afirst-principles
toassessthereflectioncapabilitiesofLLMs. perspectiveincognitivescienceconceptualizesin-
This benchmark decomposes reflection into telligentsystemsaspredictivemachinesthatcon-
crucial cognitive components: perception, tinuouslypredictfutureeventswithinternalmod-
memory,beliefupdating,decision-making, els(Friston,2010). Theycanadapttoenvironments
prediction, counterfactual thinking, and flexiblybyminimizingunexpectedeventsthrough
meta-reflection. updatingthoughtsoractions. Thistheoryhasbeen
2appliedtointerpretintelligentsystemsacrossdif- tratedinFigure2.C,participantsengageinatwo-
ferentscales,suchasinvitroneuralnetworks(Iso- armbandittask,makingchoicesbetweentwoop-
muraetal.,2023),sensorysystem(HuangandRao, tions with different reward probabilities, which
2011), cognitive process (Spratling, 2016), con- they need to infer to maximize rewards. At the
sciousness(Solms,2019),andself-identity(Liand midpoint of the task, the reward probabilities are
Li,2024). Therefore,thistheoryservesasthefirst reversed without notification. This task focuses
principle of intelligence. Considering the cyclic on the ability to update beliefs in response to the
nature of this process as showcased in Figure 1 - changingenvironment.
prediction, verification, and update - we term it
Decision-making WeemploytheWisconsincard
reflection. Throughreflection,wecancomprehen-
sorting test (WCST) to assess the flexibility in
sivelyassessvariousaspectsofintelligenceencom-
decision-making(NyhusandBarceló,2009). Par-
passingtheaforementionedresearchinterests.
ticipantsarepresentedwithasetofcardsvarying
3 Reflection-Bench in color, shape, and number of figures, and must
matchthegivencardaccordingtoanundisclosed
3.1 Taskselectionforevaluatingreflection rule (e.g., by color) which they must infer from
feedback on their choices. (Figure 2.D). After a
As discussed, reflection is a complex capabil-
certainnumberoftrials,thematchingrulechanges
ity collaborated by cognitive components includ-
withoutwarning. Both PRLTand WCST require
ing perception, working memory, belief updat-
flexiblebeliefupdating,whileWCSTfocusesmore
ing, decision-making, prediction, counterfactual
on inferring which latent rule one should obey
thinking,andmeta-reflection. Drawingfromwell-
whenmakingdecisions.
establishedcognitivescienceparadigms,weselect
6paradigmsanddesign7tasksadaptedforLLMs
Prediction Computationally, prediction is re-
evaluationinReflection-Bench,asfollows:
alized via transition probability (Friston, 2010).
There are no tests directly evaluating transition
Perception For perceiving surprise signals, we
probabilisticlearningincognitivescience. There-
selecttheoddballparadigm(Näätänenetal.,2007),
forewedesignedanadaptedversionoftheweather
amethodtostudythebrain’sautomaticprocessing
predictiontask(WPT)originallydesignedtostudy
of novel stimuli. As shown in Figure 2.A, par-
probabilisticclassificationlearning(Shohamyetal.,
ticipants are presented with a sequence of audi-
2008). In the original version, subjects need to
torystimuli,includingfrequentstandardandrare
predicttheweatherbasedoncardcuesandgradu-
deviant stimuli (e.g., different tones). Electroen-
allylearntheprobabilityrelationshipbetweencard
cephalogramrecordingsshowthatthebrainisnat-
cuesandweather. Weconvertthisprobabilityinto
urally sensitive to deviant stimuli, as reflected in
theexplicitlydefinedtransitionprobabilitymatrix
adeviance-relatednegativewaveformcalledMis-
asshowninFigure2.E.Theevaluatedmodelmust
match Negativity (MMN) (Garrido et al., 2009).
learnhowthecues(sensors)influencethetransition
This automatic phenomenon represents the basic
ofweather,throughwhichtheirflexibleprediction
ability to perceive surprise signals, which aligns
abilityisassessed.
withourintentionappropriately.
Counterfactualthinking Counterfactualthink-
Memory Recallingthepreviousdecisionprocess
ing refers to creating an alternative to reality by
requires active memory retrieval. We select the
considering ’what if’ (Byrne, 2016). Therefore
n-backtaskforevaluatingthisability(Jaeggietal.,
we test this ability by forcing models to consider
2010). AsshowninFigure2.B,presentedwithse-
theun-chosenoptionswithanopportunityto‘turn
quentialstimuli(typicallysymbols),subjectsneed
back time’. We select Iowa gambling task (IGT)
to indicate whether the current stimulus matches
for our design (Buelow and Suhr, 2009). In IGT,
the one from n steps earlier. The process of con-
participantsarepresentedwithfourdecksofcards
tinuouslyupdatingandmaintaininginformationin
(A,B,C,andD)withdistinctexpectedrewardsand
then-backtaskfitsourobjectivewell.
losses,showninFigure2.F.Theymustmaximize
Beliefupdating Focusingonbeliefupdating,we theirprofitbyselectingcardsfromthesedecksover
involveprobabilisticreversallearningtask(PRLT) a series of trials. We implement a double-choice
(Costa et al., 2015) in our benchmark. As illus- IGT(DC-IGT)whereaftereachinitialchoice,mod-
3Figure2: TasksinReflection-Bench. A:Oddballparadigm. B:N-back(2-back). C:Probabilisticreversallearning
task. D:Wisconsincardsortingtest. E:Weatherpredictiontask. F:Iowagamblingtest. G:Meta-bandittask.
elsaregiventhepossibleoutcomesofthechoice (stimulus A) and one random sentence (stimulus
and the opportunity to go back in time to make B)causingcontentconsistencyinterruption(exam-
theirchoiceagain. pleinAppendixA).Thepromptsarepresentedto
LLMswithonlytheinstructionto‘justmakesome
Meta-reflection AsillustratedinFigure1,meta-
briefcomments’,allowingustoevaluatetheircapa-
reflectionistheprocessofreflectingtheprevious
bilitytoautomaticallydetectsurprisesignals. We
reflection,therefore,wedesignameta-bandittask
compile50suchpromptswiththehelpoftheo1-
(MBT)basedonthePRLT.InMBT,thethereward
preview model. Model’s responses are manually
probabilities are reversed every n trials, creating
scoredfrom0to3(exampleinAppendixA),cor-
apredictablepatternofrulechanges(Figure2.G).
respondingto:
This design introduces a “rule of rule changing”,
requiring participants to not only adapt to rever- • 0: forcedexplanationorneglecttoB
salsbutalsotorecognizeandanticipatethemeta-
structureofthesechanges,i.e.,meta-reflection. • 1: simpleenumerationofAandB
Throughthese7tasks,Reflection-Benchoffers
• 2: pointingoutthatAandBaredifferent
a comprehensive evaluation of different aspects
while engaging reflection. Notably, there are no
• 3: clearly stating B is nonsensical in this se-
strictcorrespondencesbetweenthesetasksandspe-
quence
cific cognitive components, only with certain as-
pectsbeingmoreprominentlyfeaturedorassessed. N-back For n-back implementation, we set a
fixed sequence consisting of several letters (e.g.
3.2 TaskdesignforassessingLLMs
E,F,G,andH)andsendthemtoevaluatedmodels
Theselectedsixparadigmsarewidelyusedincog- one by one. The models are instructed to deter-
nitivescienceforinvestigatinghumanintelligence. mineifthecurrentletteristhesameastheonethat
We apply them to evaluate LLMs in Reflection- appears n steps earlier. The model’s accuracy is
Benchthroughthefollowingadaption: calculatedasitsscore.
Oddball paradigm We design a prompt-based Probabilisticreversallearningtask Wedesign
oddball task for evaluating LLMs. The prompt a40-trialPRLT.Ineachtrial,theLLMispresented
consistsofsevenshortsentencesaboutonetopic withtwooptionsandaskedtomakeachoice. The
4reward for the chosen option is sampled from a Task Trials Parameters
Bernoullidistribution. TheLLMisthenprompted Oddball 50 NA
tomakeitsnextchoicebasedontherewardfeed- N-back 52 n=2
back. Therewardprobabilitiesofthetwooptions PRLT 40 p=0.9
(leftandrightarms)arepand1-pinitiallyandre- WCST 108 x=108
versedatthe21sttrial. Themodels’beliefsonthe WPT 100 p=0.9
rewardprobabilityareestimatedaccordingtotheir DC-IGT 100 P = 0.5,0.1,0.5,0.1
loss
decisionsovertimebyamovingaverage(window MBT 60 n=3,p=1
size = 3). The overall score is calculated by the
Table1: Experimentsettingsofourexperimentinclud-
mean absolute errors (MAE) between estimated
ingtrialsandparametersettingsineachtask.
andtrueprobability:
Score = (1−MAE/Max )∗100
MAE $260, $1250, $50, and $200 with possibilities of
P = p ,p ,p ,p , respectively. In each trial,
Wisconsincardsortingtest Similartothedes- loss a b c d
the LLM is presented with four card decks and
ignation of (Kennedy and Nowak, 2024), we im-
askedtomakeaninitialchoice. Thegainandloss
plementWCSTinatext-basedversionconsisting
ofitschoicearecalculatedwiththeaboverule. Af-
of x trials, and the matching rule changes every
ter receiving feedback on the gain and loss, the
x/6trials,whichmeanseachrulecanbeapplied2
modelispromptedtomakeasecondchoice,with
times. In each trial, the LLM is presented with a
theoptiontostickwithorchangetheirinitialdeci-
descriptionofonetestingcard(e.g.,‘trianglegreen
sion. TheLLM’sperformancesarethecomposite
4’). Themodelisinstructedtomatchthetargetcard
ofshort-term(beneficialswitchesthatavoidlosses)
amongfourchoiceswithoutbeingtoldthematch-
and long-term (final overages) metrics. Notably,
ing rule. After each choice, the model receives
thescoresinDC-IGTarenormalizedfromtheraw
feedback on whether the match was correct. We
scores,sothemarksonlyrepresentperformances
evaluated the LLMs’ performance based on their
relativetoothermodels.
overallaccuracy.
Meta-bandit task The MBT consists of 20
Weatherpredictiontask Ineachtrial,LLMis
blocksofntrialseach. Ineachtrial,theLLMispre-
presented with the current day’s weather and the
sentedwithtwooptionsandaskedtomakeachoice.
sensor state ([0,1] or [1,0]), and required to pre-
Therewardforthechosenoptionissampledfrom
dict the next day’s weather based on the two in-
Bernoulli distribution and then communicated to
formation. Actual weather is calculated with the
themodel(either0or1). Therewardprobabilities
correspondingtransitionmatrix:
forthetwooptionsarepand1-p,respectively,and
(cid:20) (cid:21)
p 1−p reversedeveryntrialswithoutnotification. Byset-
T
sensors=[1,0]= 1−p p tingp=1,weanalyzethemodel’srewardsoverthe
tasktojudgewhetheritsuccessfullyrecognizesthe
(cid:20) 1−p p (cid:21) fixed pattern of reversals - if so, the model could
T
sensors=[0,1]= p 1−p getarewardinthereversaltrials.
At the next trial, LLM is informed of the actual
4 Experiment
weather which is also the current day’s weather
forthistrialandthestateofsensors,andrequired 4.1 ExperimentalSetup
to make anewprediction again. We estimate the
We evaluate the performances of 13 LLMs on
transitionprobabilitymatricesofmodelsaccording
above 7 tasks, including o1-preview, o1-mini,
to their last 20 predictions in 100 trials. Models’
GPT-4, GPT-4o, GPT-4o-mini (OpenAI, 2023),
performances are evaluated in the same way as
Claude-3.5-Sonnet (Anthropic, 2023), Gemini-
PRLT,i.e.,basedonMAEbetweenestimatedand
1.5-pro (Reid et al., 2024), Llama-3.1-405B-
truetransitionmatrices.
Instruct, Llama-3.1-70B-Instruct, Llama-3.1-8B-
DoublechoiceIowagamblingtask Weimple- Instruct (Dubey et al., 2024), Qwen-2.5-72B-
mentDC-IGTfor100trials. Gainsoffourdecks Instruct, Qwen-2.5-32B-Instruct, and Qwen-2.5-
are$100,$100,$50,and$50,andtheirlossesare 14B-Instruct(Yangetal.,2024). Theevaluations
5areimplementedusingthecorrespondingAPI.Ta-
ble1showsthetrialsandrepeatingsessionsofeach
task. Tomitigatepotentialscorerbiasintheman-
ualscoringprocess,werepeatoddballparadigms
3 times with the temperature parameter set to 0.
Toensuredeterministicresponses,thetemperature
parameterintheother6taskstestedandscoredau-
tomaticallywassetto0. Consideringthefeedback
in several tasks is sampled probabilistically, they
were repeated 2 sessions and calculated average
performancestoreducerandomness. Intotal,we
utilizedAPI1,470timesforeachmodel. Thetotal Figure 3: MMN-like waveforms demonstrating the
response of LLMs to deviant stimuli in an oddball
costs of API calls were approximately $500, and
paradigm. Adeepercurvemeansahigherresponse.
thecostofo1-previewwasaround60%higherthan
thesummationofallother12models’cost.
4.2 ExperimentResults
Mainresults TheoverallresultsarelistedinTa-
ble2. Ingeneral,Reflection-Benchshowsgooddis-
criminativepower. o1-previewstandsoutamong
all 13 evaluated models, followed by the current
top-tier models such as GPT-4, Llama-3.5-405B,
Gemini-1.5-pro, and Claude-3.5-sonnet. Smaller
modelstendtoperformworseincludingLlama-3.1-
8B,GPT-4o-mini,Qwen-2.5-32B,ando1-mini. In
MBT, all models fail to recognize the pattern of
Figure4: Probabilisticreversallearningtask. Theblack
rewardreversalsandthusgetzeropoints,therefore, dashed line represents the true reward possibility of
MBTisnotincludedinthecalculationofthetotal the bandit’s left arm over trials, and other solid lines
score. Inthefollowingparagraphs,weanalyzethe representtheaverageratiooftheleftarmchosen.
resultstaskbytask.
Oddball paradigm As showcased in Figure 3, 2 steps earlier can be difficult for some models.
mostLLMscandetectcontextualinconsistencies Onlyo1-previewgotfullmarks,andtheaccuracy
automatically to some extent without explicit in- ofmodelslikeGemini-1.5-proandGPT-4o-miniis
structions. Llama-3.1-405B & 70B and GPT-4 evenaround50%.
demonstratethestrongestresponsestodeviantstim-
Probabilistic reversal learning task In PRLT,
ulus,indicatingarobustcapabilitytoperceiveun-
Qwen-2.5-14B,andLlama-3.1-8Bshowlittlelearn-
expectedinformation. GPT-4o-mini,however,does
ing behavior, as illustrated in Figure 4, and o1-
notdemonstratethisabilitygiventhatitsaverage
scoreofeachresponseis34.44∗4.5/150 = 1.03 mini, Qwen-2.5-32B, and GPT-4o-mini demon-
straterigidbeliefsthatdonotconvergetothetrue
whichmeansthesimpleenumerationoftwodiffer-
reward possibility after reversed. Other models
enttopics. Interestingly,MMNdeficitsareidenti-
couldupdatebeliefsrelativelyflexibly. Addition-
fiedasarobustfeatureofmentaldisorderssuchas
ally,wefindthatmostmodelsadopteda‘win-stay-
schizophrenia suggesting cognitive impairments
lose-switch’ strategy. This might be the reason
(Umbricht and Krljes, 2005). And the ‘MMN
whythesemodelsperformsimilarlywith7models
deficits’ofGPT-4o-miniarealignedwithpoorper-
scoringover80points.
formancesinallothertasks. DifferentfromGPT-
4o-mini, despite the ‘MMN deficits’, o1-preview
Wisconsincardsortingtest InWCST,thescore
stillachievessuperiorperformancesinothertasks.
ofo1-previewis25pointshigherthanthesecond
2-back Althoughweprovideallconversationhis- place, Gemini-1.5-pro, so we further analyze the
torytoevaluatedmodelswhentesting,determining accuracyofmodelsby6rulegroupsover108trials,
whetherthecurrentstimulusmatchestheonefrom showninFigure5. Tosummarize,Llama-3.1-405B
6Model Oddball 2-back PRLT WCST WPT DC-IGT Overall MBT
o1-preview 58.22 100 87.07 85.29 71.48 83.78 80.97 0
o1-mini 64.89 86.54 63.73 53.92 35.38 14.20 53.11 0
GPT-4 90.00 81.73 87.05 53.43 40.93 61.48 69.10 0
GPT-4o 62.22 75.00 84.42 56.37 29.70 81.05 64.79 0
GPT-4o-mini 34.44 54.81 68.43 50.98 41.08 43.99 48.96 0
Claude-3.5-Sonnet 86.22 77.88 81.28 53.43 26.73 76.57 67.02 0
Gemini-1.5-pro 80.00 48.08 88.46 60.29 53.31 81.82 68.66 0
Llama-3.1-405B-Instruct 94.67 70.19 79.38 29.90 67.22 70.95 68.72 0
Llama-3.1-70B-Instruct 92.22 57.69 90.30 53.92 50.33 51.29 65.96 0
Llama-3.1-8B-Instruct 50.67 57.69 60.12 49.51 29.52 39.91 47.90 0
Qwen-2.5-72B-Instruct 57.33 73.08 89.42 52.94 29.38 55.48 59.61 0
Qwen-2.5-32B-Instruct 45.56 75.00 65.48 52.94 23.71 43.66 51.06 0
Qwen-2.5-14B-Instruct 54.22 66.35 56.67 51.96 46.75 67.48 57.24 0
Table2: Performancesof13modelsonReflection-Bench.
Figure5: Wisconsincardsortingtest. Accuracyby6
Figure 6: Weather prediction task. True and models’
rulegroupsover108trials.
estimatedtransitionmatricesofthehighest(o1-preview)
andlowestscoringmodels(Qwen-2.5-32B).
&8Bfailedtoobeyanyruleinthetest. Mostmod-
elsfellintothe‘shapesink’forthewholetestafter
tremes. Qwen-2.5-32Btendstopredicttheweather
the first rule group (shape). Gemini-1.5-pro suc-
accordingtoafixedtransitionprobability,missing
cessfully transited the matching rule from shape
thedifferencesbetweenthetwosensorstates. On
tocolorinthesecondrulegroupbuttheninsisted
theotherside,theo1-previewshowsabettergrasp
on the color rule for the rest trials. Only the o1-
oftheactualtransitionprobabilities,althoughnot
previewkeptmatchingbasedonhiddenandchang-
accurately.
ingrulesuntiltheend,althoughitsaccuracyinthe
DoublechoiceIowagamblingtask o1-minihas
thirdrulegroupwasonly55%.
thelowestscoresinDC-IGT,becauseitkeptchoos-
Weatherpredictiontask Wesetthequiteclear ing the deck of card (B) with expected negative
transitionprobabilitymatrices(p=0.9)tolowerthe reward,evenwhentoldthatthechoicewouldcause
difficultyofWPT.Forexampleonasunnyday,the high losses. Llama-3.1-70B, despite varying se-
nextdaywouldmostlikelybesunnyforcue‘[1,0]’ lections,insistedonitsinitialchoicenomatterits
and rainy for cue ‘[0,1]’. However, most mod- possiblegainorloss,hencegettingaslowashort-
elsstruggledwithlearningtwooppositetransition term score as o1-mini. GPT-4o-mini and Llama-
probabilities,whilesomemodelsgraspedtheprob- 3.1-8B show rigid switching patterns leading to
abilisticrelationshipstosomeextent. InFigure6, highershort-termscoresbychance. Othermodels
wepresenttheactualandmodels’estimatedtransi- suchaso1-preview,GPT-4o,andGemini-1.5-pro
tionmatricesoftheo1-previewandQwen-2.5-32B can modify their initial choice according to pos-
todemonstratethedifferencesinperformanceex- siblegainandloss,demonstratingsomeextentof
7tectionandworkingmemory,mostmodelsstruggle
to adapt flexibly to changing environments. This
inflexibilitymanifestsinrigidbeliefupdating,rule
inference,predictivelearning,andcounterfactual
thinking. Most strikingly, all models lack meta-
reflectionability. InMBT,allmodelsmerelyalter
their choices based on immediate feedback, fail-
ing to recognize the task’s meta-structure. This
indicates an absence of reflection on their adap-
tion strategies, i.e., lack of meta-reflection. It is
not just an advanced cognitive skill but a corner-
Figure 7: Rewards of models in the meta-bandit task
stone of intelligence, crucial for rational reason-
over60trialsand20reversals.
ing and learning, reliable decision-making, and
self-development(Griffiths,2020;Boureauetal.,
counterfactualthinking. 2015; Li and Li, 2024). These results suggest
thatcurrentLLMsstilllacksatisfactoryreflection
Meta-bandittask Interestingly,inthistask,no
ability. While LLMs may excel in certain tasks,
modelcouldrecognizethepatternofrewardrever-
theymaylackcorefeaturesofhuman-levelintelli-
sals,asillustratedinFigure7. Allmodelsperform
gence. Thesefindingsunderscoretheneedfornew
period or irregular mistakes across the 60 trials
paradigmsandthisfieldmaybenefitfromdrawing
which contain 20 reversals. This result indicates
inspirationfrombiologicalintelligence.
thegeneraldifficultyforcurrentmodelsingrasping
themeta-structureofthistaskwhichrepresentsthe
5 Conclusion
lackofmeta-reflectionability.
Thisstudyaimstoestablishvalidandefficientmet-
4.3 Discussion
ricsforassessingintelligence,oneofthemostde-
CoT is not cost-effective o1-preview signifi- bated emergent abilities of LLMs. We introduce
cantly outperforms the other 12 models, likely the concept of reflection, drawing from the first-
demonstratingtheeffectsofchainofthought(CoT) principles perspective of cognitive science. Re-
in enhancing reflection abilities 1. We encour- flection, a general process inherent in both bio-
age further research into the impacts of CoT on logical and artificial intelligence, is the capabil-
LLM’s reflection capability. Intriguingly, while ity to adapt beliefs or behaviors in response to
o1-previewexcelsinmosttasksitexhibits’MMN unexpectedoutcomes,facilitatinglearningabout,
deficits’intheoddballparadigm. Thismaybeat- and adaption to the environments. To probe AI
tributedtothelong-hiddenCoTweakeningimme- systems’ intelligence through reflection, we pro-
diateresponsestounexpectedinformation. More- poseReflection-Bench. Thiscomprehensivebench-
over,despitetheo1-previewAPIcostbeingaround markdecomposesreflectionintocrucialcognitive
60%higherthanthecombinedcostofallother12 components: perception,memory,beliefupdating,
models,theperformancegapdidnotsignificantly decision-making,prediction,counterfactualthink-
widenexceptin2-backandWCST.Consequently, ing, and meta-reflection. We design seven corre-
althoughCoThasbeenproventodramaticallyim- spondingtasksforLLMs,spanningcorecognitive
proveLLMs’reasoningandproblem-solvingabili- functions with adjustable difficulties to maintain
ties(Prabhakaretal.,2024;Lietal.,2024),itmay long-term relevance. In this work, we employed
notbetheoptimalsolutiontogenuineintelligence relatively easy experiment settings to assess the
duetoitsexcessivecosts. Thisfindingunderscores reflection capabilities of 13 LLMs. The results
thenecessityforfutureAIsystemstobalancedif- indicate that current LLMs still lack satisfactory
ferent cognitive demands through ‘thinking fast reflectionability. Inconclusion,Reflection-Bench
andslow’(Kahneman,2011). offersbothevaluationtoolsandinspirationforde-
veloping AI capable of reliably interacting with
LLM lacks human-level reflection While
the environment. We hope this benchmark will
demonstratingbasicautomaticsurprisesignalde-
contribute to advancing the field of artificial in-
1https://openai.com/index/learning-to-reason-with-llms/ telligence,particularlyinenhancingthereflection
8capabilitiesofAIsystems. MeiqiChen,BoPeng,YanZhang,andChaochaoLu.
2024. Cello: Causal evaluation of large vision-
Limitations languagemodels. Preprint,arXiv:2406.19131.
We acknowledge the limitations of the current PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,
Reflection-Bench that can be improved in future AshishSabharwal,CarissaSchoenick,andOyvind
Tafjord.2018. Thinkyouhavesolvedquestionan-
work. Firstly,text-basedtasksforLLMsmaynot
swering? tryarc,theai2reasoningchallenge. arXiv
fully capture their original effectiveness. For in- preprintarXiv:1803.05457.
stance,thetextualversionoftheoddballparadigm
mightbeinefficientinassessingthemodel’sability Vincent D Costa, Valery L Tran, Janita Turchi, and
Bruno B Averbeck. 2015. Reversal learning and
todetectsurprisesignalsautomatically,becausethe
dopamine: abayesianperspective. JournalofNeuro-
fine-tuningprocessandsystempromptsinfluence
science,35(6):2407–2416.
the outcomes. Secondly, our analysis of model’s
performancesisrelativelysuperficial,primarilyfo- AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,
AbhishekKadian,AhmadAl-Dahle,AieshaLetman,
cusing on metrics like accuracy. This risks over-
Akhil Mathur, Alan Schelten, Amy Yang, Angela
lookingtheirinternalgenerationprocesses. More
Fan,etal.2024. Thellama3herdofmodels. arXiv
nuancedanalysiscouldrevealinvaluableinsights preprintarXiv:2407.21783.
into LLMs’ reflection abilities. Thirdly, except
Karl Friston. 2010. The free-energy principle: a uni-
for the oddball paradigm, we required the eval-
fied brain theory? Nature reviews neuroscience,
uated model to only respond with their options.
11(2):127–138.
Whilethisfacilitatesresultanalysis,itmightlimit
the model’s real capability by restricting explicit MartaIGarrido,JamesMKilner,KlaasEStephan,and
thoughtprocesses. Karl J Friston. 2009. The mismatch negativity: a
reviewofunderlyingmechanisms. Clinicalneuro-
physiology,120(3):453–463.
Ethicalconsiderations
ThomasL.Griffiths.2020. Understandinghumanintel-
Reflection-Bench provides useful metrics of AI
ligencethroughhumanlimitations. TrendsinCogni-
system’sintelligenceandcanadapttoadvancedAI
tiveSciences,24(11):873–883.
in the future by adjustable difficulty. No privacy-
relatedandpersonallyidentifiabledataareincluded YanpingHuangandRajeshPNRao.2011. Predictive
inourbenchmarks. Consideringsomelimitations, coding. WileyInterdisciplinaryReviews: Cognitive
Science,2(5):580–593.
itsvalidityandreliabilityinassessingintelligence
stillrequireconfirmationbyfurtherstudies. TakuyaIsomura,KiyoshiKotani,YasuhikoJimbo,and
KarlJFriston.2023. Experimentalvalidationofthe
free-energyprinciplewithinvitroneuralnetworks.
References NatureCommunications,14(1):4547.
Anthropic. 2023. Claude. https://claude.ai/ SusanneMJaeggi,MartinBuschkuehl,WalterJPerrig,
chats.
andBeatMeier.2010. Theconcurrentvalidityofthe
n-backtaskasaworkingmemorymeasure. Memory,
EmilyMBender, TimnitGebru, AngelinaMcMillan-
18(4):394–412.
Major, and Shmargaret Shmitchell. 2021. On the
dangersofstochasticparrots: Canlanguagemodels
DanielKahneman.2011. Thinking,fastandslow. Far-
betoobig? InProceedingsofthe2021ACMconfer-
rar,StrausandGiroux.
enceonfairness,accountability,andtransparency,
pages610–623.
SeanMKennedyandRobertDNowak.2024. Cognitive
flexibilityoflargelanguagemodels. InICML2024
Y-LanBoureau,PeterSokol-Hessner,andNathanielD.
WorkshoponLLMsandCognition.
Daw.2015. Decidinghowtodecide:Self-controland
meta-decisionmaking. TrendsinCognitiveSciences,
AkshayKrishnamurthy,KeeganHarris,DylanJFoster,
19(11):700–710.
Cyril Zhang, and Aleksandrs Slivkins. 2024. Can
MelissaTBuelowandJulieASuhr.2009. Construct largelanguagemodelsexplorein-context? InICML
validityoftheiowagamblingtask. Neuropsychology 2024WorkshoponIn-ContextLearning.
review,19:102–114.
Lingyu Li and Chunbo Li. 2024. Enabling self-
RuthM.J.Byrne.2016. Counterfactualthought. Annual identification in intelligent agent: insights from
Review of Psychology, 67(Volume 67, 2016):135– computational psychoanalysis. arXiv preprint
157. arXiv:2403.07664.
9Zhiyuan Li, Hong Liu, Denny Zhou, and Tengyu 2024. Planbench: Anextensiblebenchmarkforeval-
Ma. 2024. Chain of thought empowers transform- uatinglargelanguagemodelsonplanningandreason-
ers to solve inherently serial problems. Preprint, ingaboutchange. AdvancesinNeuralInformation
arXiv:2402.12875. ProcessingSystems,36.
ShengLu,IrinaBigoulaeva,RachneetSachdeva,Har- Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,
ish Tayyar Madabushi, and Iryna Gurevych. 2024. Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Areemergentabilitiesinlargelanguagemodelsjust MaartenBosma,DennyZhou,DonaldMetzler,EdH.
in-contextlearning? Preprint,arXiv:2309.01809. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy
Liang,JeffDean,andWilliamFedus.2022. Emer-
ChrisMadgeandMassimoPoesio.2024. Allmbench- gentabilitiesoflargelanguagemodels. Transactions
markbasedontheminecraftbuilderdialogagenttask. on Machine Learning Research. Survey Certifica-
Preprint,arXiv:2407.12734. tion.
RistoNäätänen, PetriPaavilainen, TeemuRinne, and An Yang, Baosong Yang, Binyuan Hui, Bo Zheng,
KimmoAlho.2007. Themismatchnegativity(mmn) BowenYu,ChangZhou,ChengpengLi,Chengyuan
inbasicresearchofcentralauditoryprocessing: are- Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2
view. Clinicalneurophysiology,118(12):2544–2590. technicalreport. arXivpreprintarXiv:2407.10671.
Erika Nyhus and Francisco Barceló. 2009. The wis- A Appendix1
consincardsortingtestandthecognitiveassessment
of prefrontal executive functions: a critical update. A.1 ExamplesofOurUsedPrompts
Brainandcognition,71(3):437–451.
Here,weshowthepromptsweuseindetailbelow.
OpenAI.2023. Chatgpt. https://chat.openai.com/
chat. Systemprompt Youareplayingagameandwill
be presented with a sequence of sentences about
AksharaPrabhakar,ThomasL.Griffiths,andR.Thomas aspecifictopic. Justmakeshortcommentsonthe
McCoy. 2024. Deciphering the factors influenc-
material.
ing the efficacy of chain-of-thought: Probabil-
ity, memorization, and noisy reasoning. Preprint,
Oddballprompt TheGreatWallofChinaisan
arXiv:2407.01687.
ancient structure. It stretches over 13,000 miles.
Machel Reid, Nikolay Savinov, Denis Teplyashin, The wall was built for defense purposes. Many
Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste tourists visit it each year. Bananas are rich in
Alayrac,RaduSoricut,AngelikiLazaridou,OrhanFi-
potassium. Parts of the wall date back to the 7th
rat,JulianSchrittwieser,etal.2024. Gemini1.5: Un-
century BCE. Some sections are well-preserved.
lockingmultimodalunderstandingacrossmillionsof
tokensofcontext. arXivpreprintarXiv:2403.05530. Thewallisvisiblefromspace.
RylanSchaeffer,BrandoMiranda,andSanmiKoyejo. Forced explanation (Gemini-1.5-pro) Wow,
2023. Areemergentabilitiesoflargelanguagemod- 13,000 miles is long! Ancient defense against
els a mirage? In Advances in Neural Information
potassium-rich invaders? Not sure about being
ProcessingSystems,volume36,pages55565–55581.
visiblefromspace,that’samyth. It’sstillamazing
CurranAssociates,Inc.
though!
D Shohamy, CE Myers, J Kalanithi, and MA Gluck.
2008. Basal ganglia and dopamine contributions Simpleenumeration(GPT-4o) TheGreatWall
toprobabilisticcategorylearning. Neuroscience& of China is truly ancient and impressive. Over
BiobehavioralReviews,32(2):219–236.
13,000milesisanastonishinglength! Defensewas
a primary reason for its construction. It’s clearly
MarkSolms.2019. Thehardproblemofconsciousness
andthefreeenergyprinciple. FrontiersinPsychol- a major tourist attraction. Interesting note about
ogy,9:2714. bananas being rich in potassium. Some sections
datingbacktothe7thcenturyBCEaddstoitshis-
Michael W Spratling. 2016. Predictive coding as a
toricalsignificance. It’sgoodtoknowsomeparts
modelofcognition. Cognitiveprocessing,17:279–
305. arewell-preserved. There’sdebateaboutitsvisibil-
ityfromspace,butitremainsapopularclaim.
Daniel Umbricht and Sanya Krljes. 2005. Mis-
matchnegativityinschizophrenia: ameta-analysis. Pointing out differences (o1-mini) The Great
Schizophreniaresearch,76(1):1–23.
WallofChinaisindeedaremarkableancientstruc-
ture. Stretching over 13,000 miles showcases its
KarthikValmeekam,MatthewMarquez,AlbertoOlmo,
Sarath Sreedharan, and Subbarao Kambhampati. immensescale. Itwasprimarilybuiltfordefense,
10highlightingitshistoricalsignificance. Itspopular-
ityamongtouristsreflectsitsculturalimportance.
Interestingfactaboutbananasbeingrichinpotas-
sium, though unrelated to the Great Wall. Parts
datingbacktothe7thcenturyBCEemphasizeits
longhistory. Well-preservedsectionsallowvisitors
to appreciate its original construction. The idea
that the wall is visible from space is a common
misconception.
Clearlystatingnonsense(Llama-3.1-405b) In-
teresting fact about the wall’s age... wait, what’s
withthebananas? Thatseemsoutofplace. Any-
way,7thcenturyBCEisimpressive. I’mnotsureif
it’sentirelyvisiblefromspace,though-I’veheard
that’samyth.
The13models’responseswereannotatedbyone
Master’sstudentincognitivescience.
11