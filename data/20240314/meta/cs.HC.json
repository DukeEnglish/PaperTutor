[
    {
        "title": "Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment",
        "authors": "Paraskevas PegiosManxi LinNina WengMorten Bo Søndergaard SvendsenZahra BashirSiavash BigdeliAnders Nymark ChristensenMartin TolsgaardAasa Feragen",
        "links": "http://arxiv.org/abs/2403.08700v1",
        "entry_id": "http://arxiv.org/abs/2403.08700v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08700v1",
        "summary": "Obstetric ultrasound image quality is crucial for accurate diagnosis and\nmonitoring of fetal health. However, producing high-quality standard planes is\ndifficult, influenced by the sonographer's expertise and factors like the\nmaternal BMI or the fetus dynamics. In this work, we propose using\ndiffusion-based counterfactual explainable AI to generate realistic\nhigh-quality standard planes from low-quality non-standard ones. Through\nquantitative and qualitative evaluation, we demonstrate the effectiveness of\nour method in producing plausible counterfactuals of increased quality. This\nshows future promise both for enhancing training of clinicians by providing\nvisual feedback, as well as for improving image quality and, consequently,\ndownstream diagnosis and monitoring.",
        "updated": "2024-03-13 17:04:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08700v1"
    },
    {
        "title": "Non-discrimination Criteria for Generative Language Models",
        "authors": "Sara SterlieNina WengAasa Feragen",
        "links": "http://arxiv.org/abs/2403.08564v1",
        "entry_id": "http://arxiv.org/abs/2403.08564v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08564v1",
        "summary": "Within recent years, generative AI, such as large language models, has\nundergone rapid development. As these models become increasingly available to\nthe public, concerns arise about perpetuating and amplifying harmful biases in\napplications. Gender stereotypes can be harmful and limiting for the\nindividuals they target, whether they consist of misrepresentation or\ndiscrimination. Recognizing gender bias as a pervasive societal construct, this\npaper studies how to uncover and quantify the presence of gender biases in\ngenerative language models. In particular, we derive generative AI analogues of\nthree well-known non-discrimination criteria from classification, namely\nindependence, separation and sufficiency. To demonstrate these criteria in\naction, we design prompts for each of the criteria with a focus on occupational\ngender stereotype, specifically utilizing the medical test to introduce the\nground truth in the generative AI context. Our results address the presence of\noccupational gender bias within such conversational language models.",
        "updated": "2024-03-13 14:19:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08564v1"
    },
    {
        "title": "Calibrating coordinate system alignment in a scanning transmission electron microscope using a digital twin",
        "authors": "Dieter WeberDavid LandersChen HuangEmanuela LibertiEmiliya PoghosyanMatthew BryanAlexander ClausenDaniel G. StroppaAngus I. KirklandElisabeth MüllerAndrew StewartRafal E. Dunin-Borkowski",
        "links": "http://arxiv.org/abs/2403.08538v1",
        "entry_id": "http://arxiv.org/abs/2403.08538v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08538v1",
        "summary": "In four-dimensional scanning transmission electron microscopy (4D STEM) a\nfocused beam is scanned over a specimen and a diffraction pattern is recorded\nat each position using a pixelated detector. During the experiment, it must be\nensured that the scan coordinate system of the beam is correctly calibrated\nrelative to the detector coordinate system. Various simplified and approximate\nmodels are used implicitly and explicitly for understanding and analyzing the\nrecorded data, requiring translation between the physical reality of the\ninstrument and the abstractions used in data interpretation. Here, we introduce\na calibration method where interactive live data processing in combination with\na digital twin is used to match a set of models and their parameters with the\naction of a real-world instrument.",
        "updated": "2024-03-13 13:52:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08538v1"
    },
    {
        "title": "A Picture Is Worth a Thousand Words: Exploring Diagram and Video-Based OOP Exercises to Counter LLM Over-Reliance",
        "authors": "Bruno Pereira CiprianoPedro AlvesPaul Denny",
        "links": "http://arxiv.org/abs/2403.08396v1",
        "entry_id": "http://arxiv.org/abs/2403.08396v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08396v1",
        "summary": "Much research has highlighted the impressive capabilities of large language\nmodels (LLMs), like GPT and Bard, for solving introductory programming\nexercises. Recent work has shown that LLMs can effectively solve a range of\nmore complex object-oriented programming (OOP) exercises with text-based\nspecifications. This raises concerns about academic integrity, as students\nmight use these models to complete assignments unethically, neglecting the\ndevelopment of important skills such as program design, problem-solving, and\ncomputational thinking. To address this, we propose an innovative approach to\nformulating OOP tasks using diagrams and videos, as a way to foster\nproblem-solving and deter students from a copy-and-prompt approach in OOP\ncourses. We introduce a novel notation system for specifying OOP assignments,\nencompassing structural and behavioral requirements, and assess its use in a\nclassroom setting over a semester. Student perceptions of this approach are\nexplored through a survey (n=56). Generally, students responded positively to\ndiagrams and videos, with video-based projects being better received than\ndiagram-based exercises. This notation appears to have several benefits, with\nstudents investing more effort in understanding the diagrams and feeling more\nmotivated to engage with the video-based projects. Furthermore, students\nreported being less inclined to rely on LLM-based code generation tools for\nthese diagram and video-based exercises. Experiments with GPT-4 and Bard's\nvision abilities revealed that they currently fall short in interpreting these\ndiagrams to generate accurate code solutions.",
        "updated": "2024-03-13 10:21:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08396v1"
    },
    {
        "title": "ShareYourReality: Investigating Haptic Feedback and Agency in Virtual Avatar Co-embodiment",
        "authors": "Karthikeya Puttur VenkatrajWo MeijerMonica Perusquía-HernándezGijs HuismanAbdallah El Ali",
        "links": "http://dx.doi.org/10.1145/3613904.3642425",
        "entry_id": "http://arxiv.org/abs/2403.08363v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08363v1",
        "summary": "Virtual co-embodiment enables two users to share a single avatar in Virtual\nReality (VR). During such experiences, the illusion of shared motion control\ncan break during joint-action activities, highlighting the need for\nposition-aware feedback mechanisms. Drawing on the perceptual crossing\nparadigm, we explore how haptics can enable non-verbal coordination between\nco-embodied participants. In a within-subjects study (20 participant pairs), we\nexamined the effects of vibrotactile haptic feedback (None, Present) and avatar\ncontrol distribution (25-75%, 50-50%, 75-25%) across two VR reaching tasks\n(Targeted, Free-choice) on participants Sense of Agency (SoA), co-presence,\nbody ownership, and motion synchrony. We found (a) lower SoA in the free-choice\nwith haptics than without, (b) higher SoA during the shared targeted task, (c)\nco-presence and body ownership were significantly higher in the free-choice\ntask, (d) players hand motions synchronized more in the targeted task. We\nprovide cautionary considerations when including haptic feedback mechanisms for\navatar co-embodiment experiences.",
        "updated": "2024-03-13 09:23:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08363v1"
    }
]