[
    {
        "title": "FastMAC: Stochastic Spectral Sampling of Correspondence Graph",
        "authors": "Yifei ZhangHao ZhaoHongyang LiSiheng Chen",
        "links": "http://arxiv.org/abs/2403.08770v1",
        "entry_id": "http://arxiv.org/abs/2403.08770v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08770v1",
        "summary": "3D correspondence, i.e., a pair of 3D points, is a fundamental concept in\ncomputer vision. A set of 3D correspondences, when equipped with compatibility\nedges, forms a correspondence graph. This graph is a critical component in\nseveral state-of-the-art 3D point cloud registration approaches, e.g., the one\nbased on maximal cliques (MAC). However, its properties have not been well\nunderstood. So we present the first study that introduces graph signal\nprocessing into the domain of correspondence graph. We exploit the generalized\ndegree signal on correspondence graph and pursue sampling strategies that\npreserve high-frequency components of this signal. To address time-consuming\nsingular value decomposition in deterministic sampling, we resort to a\nstochastic approximate sampling strategy. As such, the core of our method is\nthe stochastic spectral sampling of correspondence graph. As an application, we\nbuild a complete 3D registration algorithm termed as FastMAC, that reaches\nreal-time speed while leading to little to none performance drop. Through\nextensive experiments, we validate that FastMAC works for both indoor and\noutdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while\nmaintaining high registration success rate on KITTI. Codes are publicly\navailable at https://github.com/Forrest-110/FastMAC.",
        "updated": "2024-03-13 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08770v1"
    },
    {
        "title": "3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surface",
        "authors": "Linyi JinNilesh KulkarniDavid Fouhey",
        "links": "http://arxiv.org/abs/2403.08768v1",
        "entry_id": "http://arxiv.org/abs/2403.08768v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08768v1",
        "summary": "This paper introduces 3DFIRES, a novel system for scene-level 3D\nreconstruction from posed images. Designed to work with as few as one view,\n3DFIRES reconstructs the complete geometry of unseen scenes, including hidden\nsurfaces. With multiple view inputs, our method produces full reconstruction\nwithin all camera frustums. A key feature of our approach is the fusion of\nmulti-view information at the feature level, enabling the production of\ncoherent and comprehensive 3D reconstruction. We train our system on\nnon-watertight scans from large-scale real scene dataset. We show it matches\nthe efficacy of single-view reconstruction methods with only one input and\nsurpasses existing techniques in both quantitative and qualitative measures for\nsparse-view 3D reconstruction.",
        "updated": "2024-03-13 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08768v1"
    },
    {
        "title": "MonoOcc: Digging into Monocular Semantic Occupancy Prediction",
        "authors": "Yupeng ZhengXiang LiPengfei LiYuhang ZhengBu JinChengliang ZhongXiaoxiao LongHao ZhaoQichao Zhang",
        "links": "http://arxiv.org/abs/2403.08766v1",
        "entry_id": "http://arxiv.org/abs/2403.08766v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08766v1",
        "summary": "Monocular Semantic Occupancy Prediction aims to infer the complete 3D\ngeometry and semantic information of scenes from only 2D images. It has\ngarnered significant attention, particularly due to its potential to enhance\nthe 3D perception of autonomous vehicles. However, existing methods rely on a\ncomplex cascaded framework with relatively limited information to restore 3D\nscenes, including a dependency on supervision solely on the whole network's\noutput, single-frame input, and the utilization of a small backbone. These\nchallenges, in turn, hinder the optimization of the framework and yield\ninferior prediction results, particularly concerning smaller and long-tailed\nobjects. To address these issues, we propose MonoOcc. In particular, we (i)\nimprove the monocular occupancy prediction framework by proposing an auxiliary\nsemantic loss as supervision to the shallow layers of the framework and an\nimage-conditioned cross-attention module to refine voxel features with visual\nclues, and (ii) employ a distillation module that transfers temporal\ninformation and richer knowledge from a larger image backbone to the monocular\nsemantic occupancy prediction framework with low cost of hardware. With these\nadvantages, our method yields state-of-the-art performance on the camera-based\nSemanticKITTI Scene Completion benchmark. Codes and models can be accessed at\nhttps://github.com/ucaszyp/MonoOcc",
        "updated": "2024-03-13 17:59:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08766v1"
    },
    {
        "title": "VLOGGER: Multimodal Diffusion for Embodied Avatar Synthesis",
        "authors": "Enric CoronaAndrei ZanfirEduard Gabriel BazavanNikos KolotourosThiemo AlldieckCristian Sminchisescu",
        "links": "http://arxiv.org/abs/2403.08764v1",
        "entry_id": "http://arxiv.org/abs/2403.08764v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08764v1",
        "summary": "We propose VLOGGER, a method for audio-driven human video generation from a\nsingle input image of a person, which builds on the success of recent\ngenerative diffusion models. Our method consists of 1) a stochastic\nhuman-to-3d-motion diffusion model, and 2) a novel diffusion-based architecture\nthat augments text-to-image models with both spatial and temporal controls.\nThis supports the generation of high quality video of variable length, easily\ncontrollable through high-level representations of human faces and bodies. In\ncontrast to previous work, our method does not require training for each\nperson, does not rely on face detection and cropping, generates the complete\nimage (not just the face or the lips), and considers a broad spectrum of\nscenarios (e.g. visible torso or diverse subject identities) that are critical\nto correctly synthesize humans who communicate. We also curate MENTOR, a new\nand diverse dataset with 3d pose and expression annotations, one order of\nmagnitude larger than previous ones (800,000 identities) and with dynamic\ngestures, on which we train and ablate our main technical contributions.\n  VLOGGER outperforms state-of-the-art methods in three public benchmarks,\nconsidering image quality, identity preservation and temporal consistency while\nalso generating upper-body gestures. We analyze the performance of VLOGGER with\nrespect to multiple diversity metrics, showing that our architectural choices\nand the use of MENTOR benefit training a fair and unbiased model at scale.\nFinally we show applications in video editing and personalization.",
        "updated": "2024-03-13 17:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08764v1"
    },
    {
        "title": "Segmentation of Knee Bones for Osteoarthritis Assessment: A Comparative Analysis of Supervised, Few-Shot, and Zero-Shot Learning Approaches",
        "authors": "Yun Xin TeohAlice OthmaniSiew Li GohJuliana UsmanKhin Wee Lai",
        "links": "http://arxiv.org/abs/2403.08761v1",
        "entry_id": "http://arxiv.org/abs/2403.08761v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08761v1",
        "summary": "Knee osteoarthritis is a degenerative joint disease that induces chronic pain\nand disability. Bone morphological analysis is a promising tool to understand\nthe mechanical aspect of this disorder. This study proposes a 2D bone\nmorphological analysis using manually segmented bones to explore morphological\nfeatures related to distinct pain conditions. Furthermore, six semantic\nsegmentation algorithms are assessed for extracting femur and tibia bones from\nX-ray images. Our analysis reveals that the morphology of the femur undergoes\nsignificant changes in instances where pain worsens. Conversely, improvements\nin pain may not manifest pronounced alterations in bone shape. The\nfew-shot-learning-based algorithm, UniverSeg, demonstrated superior\nsegmentation results with Dice scores of 99.69% for femur and 99.60% for tibia.\nRegarding pain condition classification, the zero-shot-learning-based\nalgorithm, CP-SAM, achieved the highest accuracy at 66% among all models.\nUniverSeg is recommended for automatic knee bone segmentation, while SAM models\nshow potential with prompt encoder modifications for optimized outcomes. These\nfindings highlight the effectiveness of few-shot learning for semantic\nsegmentation and the potential of zero-shot learning in enhancing\nclassification models for knee osteoarthritis diagnosis.",
        "updated": "2024-03-13 17:58:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08761v1"
    }
]