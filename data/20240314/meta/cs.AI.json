[
    {
        "title": "FastMAC: Stochastic Spectral Sampling of Correspondence Graph",
        "authors": "Yifei ZhangHao ZhaoHongyang LiSiheng Chen",
        "links": "http://arxiv.org/abs/2403.08770v1",
        "entry_id": "http://arxiv.org/abs/2403.08770v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08770v1",
        "summary": "3D correspondence, i.e., a pair of 3D points, is a fundamental concept in\ncomputer vision. A set of 3D correspondences, when equipped with compatibility\nedges, forms a correspondence graph. This graph is a critical component in\nseveral state-of-the-art 3D point cloud registration approaches, e.g., the one\nbased on maximal cliques (MAC). However, its properties have not been well\nunderstood. So we present the first study that introduces graph signal\nprocessing into the domain of correspondence graph. We exploit the generalized\ndegree signal on correspondence graph and pursue sampling strategies that\npreserve high-frequency components of this signal. To address time-consuming\nsingular value decomposition in deterministic sampling, we resort to a\nstochastic approximate sampling strategy. As such, the core of our method is\nthe stochastic spectral sampling of correspondence graph. As an application, we\nbuild a complete 3D registration algorithm termed as FastMAC, that reaches\nreal-time speed while leading to little to none performance drop. Through\nextensive experiments, we validate that FastMAC works for both indoor and\noutdoor benchmarks. For example, FastMAC can accelerate MAC by 80 times while\nmaintaining high registration success rate on KITTI. Codes are publicly\navailable at https://github.com/Forrest-110/FastMAC.",
        "updated": "2024-03-13 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08770v1"
    },
    {
        "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
        "authors": "Adam IbrahimBenjamin ThérienKshitij GuptaMats L. RichterQuentin AnthonyTimothée LesortEugene BelilovskyIrina Rish",
        "links": "http://arxiv.org/abs/2403.08763v1",
        "entry_id": "http://arxiv.org/abs/2403.08763v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08763v1",
        "summary": "Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by final loss and\nlanguage model (LM) evaluation benchmarks. Specifically, we show this for a\nweak but realistic distribution shift between two commonly used LLM\npre-training datasets (English$\\rightarrow$English) and a stronger distribution\nshift (English$\\rightarrow$German) at the $405$M parameter model scale with\nlarge dataset sizes (hundreds of billions of tokens). Selecting the weak but\nrealistic shift for larger-scale experiments, we also find that our continual\nlearning strategies match the re-training baseline for a 10B parameter LLM. Our\nresults demonstrate that LLMs can be successfully updated via simple and\nscalable continual learning strategies, matching the re-training baseline using\nonly a fraction of the compute. Finally, inspired by previous work, we propose\nalternatives to the cosine learning rate schedule that help circumvent\nforgetting induced by LR re-warming and that are not bound to a fixed token\nbudget.",
        "updated": "2024-03-13 17:58:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08763v1"
    },
    {
        "title": "DAM: Dynamic Adapter Merging for Continual Video QA Learning",
        "authors": "Feng ChengZiyang WangYi-Lin SungYan-Bo LinMohit BansalGedas Bertasius",
        "links": "http://arxiv.org/abs/2403.08755v1",
        "entry_id": "http://arxiv.org/abs/2403.08755v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08755v1",
        "summary": "We present a parameter-efficient method for continual video\nquestion-answering (VidQA) learning. Our method, named DAM, uses the proposed\nDynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable\nefficient adaptation to continually arriving datasets, (iii) handle inputs from\nunknown datasets during inference, and (iv) enable knowledge sharing across\nsimilar dataset domains. Given a set of continually streaming VidQA datasets,\nwe sequentially train dataset-specific adapters for each dataset while freezing\nthe parameters of a large pretrained video-language backbone. During inference,\ngiven a video-question sample from an unknown domain, our method first uses the\nproposed non-parametric router function to compute a probability for each\nadapter, reflecting how relevant that adapter is to the current video-question\ninput instance. Subsequently, the proposed dynamic adapter merging scheme\naggregates all the adapter weights into a new adapter instance tailored for\nthat particular test sample to compute the final VidQA prediction, mitigating\nthe impact of inaccurate router predictions and facilitating knowledge sharing\nacross domains. Our DAM model outperforms prior state-of-the-art continual\nlearning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA\ndatasets spanning various domains. We further extend DAM to continual image\nclassification and image QA and outperform prior methods by a large margin. The\ncode is publicly available at: https://github.com/klauscc/DAM",
        "updated": "2024-03-13 17:53:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08755v1"
    },
    {
        "title": "Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework",
        "authors": "Jingling LiZeyu TangXiaoyu LiuPeter SpirtesKun ZhangLiu LeqiYang Liu",
        "links": "http://arxiv.org/abs/2403.08743v1",
        "entry_id": "http://arxiv.org/abs/2403.08743v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08743v1",
        "summary": "Large language models (LLMs) can easily generate biased and discriminative\nresponses. As LLMs tap into consequential decision-making (e.g., hiring and\nhealthcare), it is of crucial importance to develop strategies to mitigate\nthese biases. This paper focuses on social bias, tackling the association\nbetween demographic information and LLM outputs. We propose a causality-guided\ndebiasing framework that utilizes causal understandings of (1) the\ndata-generating process of the training corpus fed to LLMs, and (2) the\ninternal reasoning process of LLM inference, to guide the design of prompts for\ndebiasing LLM outputs through selection mechanisms. Our framework unifies\nexisting de-biasing prompting approaches such as inhibitive instructions and\nin-context contrastive examples, and sheds light on new ways of debiasing by\nencouraging bias-free reasoning. Our strong empirical performance on real-world\ndatasets demonstrates that our framework provides principled guidelines on\ndebiasing LLM outputs even with only the black-box access.",
        "updated": "2024-03-13 17:46:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08743v1"
    },
    {
        "title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models",
        "authors": "Carlo NicoliniJacopo StaianoBruno LepriRaffaele Marino",
        "links": "http://arxiv.org/abs/2403.08739v1",
        "entry_id": "http://arxiv.org/abs/2403.08739v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08739v1",
        "summary": "A substantial gap persists in understanding the reasons behind the\nexceptional performance of the Transformer architecture in NLP. A particularly\nunexplored area involves the mechanistic description of how the distribution of\nparameters evolves over time during training. In this work we suggest that\nlooking at the time evolution of the statistic distribution of model\nparameters, and specifically at bifurcation effects, can help understanding the\nmodel quality, potentially reducing training costs and evaluation efforts and\nempirically showing the reasons behind the effectiveness of weights\nsparsification.",
        "updated": "2024-03-13 17:42:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08739v1"
    }
]