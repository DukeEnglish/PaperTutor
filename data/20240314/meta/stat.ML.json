[
    {
        "title": "Efficient Combinatorial Optimization via Heat Diffusion",
        "authors": "Hengyuan MaWenlian LuJianfeng Feng",
        "links": "http://arxiv.org/abs/2403.08757v1",
        "entry_id": "http://arxiv.org/abs/2403.08757v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08757v1",
        "summary": "Combinatorial optimization problems are widespread but inherently challenging\ndue to their discrete nature.The primary limitation of existing methods is that\nthey can only access a small fraction of the solution space at each iteration,\nresulting in limited efficiency for searching the global optimal. To overcome\nthis challenge, diverging from conventional efforts of expanding the solver's\nsearch scope, we focus on enabling information to actively propagate to the\nsolver through heat diffusion. By transforming the target function while\npreserving its optima, heat diffusion facilitates information flow from distant\nregions to the solver, providing more efficient navigation. Utilizing heat\ndiffusion, we propose a framework for solving general combinatorial\noptimization problems. The proposed methodology demonstrates superior\nperformance across a range of the most challenging and widely encountered\ncombinatorial optimizations. Echoing recent advancements in harnessing\nthermodynamics for generative artificial intelligence, our study further\nreveals its significant potential in advancing combinatorial optimization.",
        "updated": "2024-03-13 17:55:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08757v1"
    },
    {
        "title": "Neural reproducing kernel Banach spaces and representer theorems for deep networks",
        "authors": "Francesca BartolucciErnesto De VitoLorenzo RosascoStefano Vigogna",
        "links": "http://arxiv.org/abs/2403.08750v1",
        "entry_id": "http://arxiv.org/abs/2403.08750v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08750v1",
        "summary": "Studying the function spaces defined by neural networks helps to understand\nthe corresponding learning models and their inductive bias. While in some\nlimits neural networks correspond to function spaces that are reproducing\nkernel Hilbert spaces, these regimes do not capture the properties of the\nnetworks used in practice. In contrast, in this paper we show that deep neural\nnetworks define suitable reproducing kernel Banach spaces.\n  These spaces are equipped with norms that enforce a form of sparsity,\nenabling them to adapt to potential latent structures within the input data and\ntheir representations. In particular, leveraging the theory of reproducing\nkernel Banach spaces, combined with variational results, we derive representer\ntheorems that justify the finite architectures commonly employed in\napplications. Our study extends analogous results for shallow networks and can\nbe seen as a step towards considering more practically plausible neural\narchitectures.",
        "updated": "2024-03-13 17:51:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08750v1"
    },
    {
        "title": "Implicit Regularization of Gradient Flow on One-Layer Softmax Attention",
        "authors": "Heejune SheenSiyu ChenTianhao WangHarrison H. Zhou",
        "links": "http://arxiv.org/abs/2403.08699v1",
        "entry_id": "http://arxiv.org/abs/2403.08699v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08699v1",
        "summary": "We study gradient flow on the exponential loss for a classification problem\nwith a one-layer softmax attention model, where the key and query weight\nmatrices are trained separately. Under a separability assumption on the data,\nwe show that when gradient flow achieves the minimal loss value, it further\nimplicitly minimizes the nuclear norm of the product of the key and query\nweight matrices. Such implicit regularization can be described by a Support\nVector Machine (SVM) problem with respect to the attention weights. This\nfinding contrasts with prior results showing that the gradient descent induces\nan implicit regularization on the Frobenius norm on the product weight matrix\nwhen the key and query matrices are combined into a single weight matrix for\ntraining. For diagonal key and query matrices, our analysis builds upon the\nreparameterization technique and exploits approximate KKT conditions of the SVM\nassociated with the classification data. Moreover, the results are extended to\ngeneral weights configurations given proper alignment of the weight matrices'\nsingular spaces with the data features at initialization.",
        "updated": "2024-03-13 17:02:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08699v1"
    },
    {
        "title": "When can we Approximate Wide Contrastive Models with Neural Tangent Kernels and Principal Component Analysis?",
        "authors": "Gautham Govind AnilPascal EsserDebarghya Ghoshdastidar",
        "links": "http://arxiv.org/abs/2403.08673v1",
        "entry_id": "http://arxiv.org/abs/2403.08673v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08673v1",
        "summary": "Contrastive learning is a paradigm for learning representations from\nunlabelled data that has been highly successful for image and text data.\nSeveral recent works have examined contrastive losses to claim that contrastive\nmodels effectively learn spectral embeddings, while few works show relations\nbetween (wide) contrastive models and kernel principal component analysis\n(PCA). However, it is not known if trained contrastive models indeed correspond\nto kernel methods or PCA. In this work, we analyze the training dynamics of\ntwo-layer contrastive models, with non-linear activation, and answer when these\nmodels are close to PCA or kernel methods. It is well known in the supervised\nsetting that neural networks are equivalent to neural tangent kernel (NTK)\nmachines, and that the NTK of infinitely wide networks remains constant during\ntraining. We provide the first convergence results of NTK for contrastive\nlosses, and present a nuanced picture: NTK of wide networks remains almost\nconstant for cosine similarity based contrastive losses, but not for losses\nbased on dot product similarity. We further study the training dynamics of\ncontrastive models with orthogonality constraints on output layer, which is\nimplicitly assumed in works relating contrastive learning to spectral\nembedding. Our deviation bounds suggest that representations learned by\ncontrastive models are close to the principal components of a certain matrix\ncomputed from random features. We empirically show that our theoretical results\npossibly hold beyond two-layer networks.",
        "updated": "2024-03-13 16:25:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08673v1"
    },
    {
        "title": "Extracting Explanations, Justification, and Uncertainty from Black-Box Deep Neural Networks",
        "authors": "Paul ArdisArjuna Flenner",
        "links": "http://arxiv.org/abs/2403.08652v1",
        "entry_id": "http://arxiv.org/abs/2403.08652v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08652v1",
        "summary": "Deep Neural Networks (DNNs) do not inherently compute or exhibit\nempirically-justified task confidence. In mission critical applications, it is\nimportant to both understand associated DNN reasoning and its supporting\nevidence. In this paper, we propose a novel Bayesian approach to extract\nexplanations, justifications, and uncertainty estimates from DNNs. Our approach\nis efficient both in terms of memory and computation, and can be applied to any\nblack box DNN without any retraining, including applications to anomaly\ndetection and out-of-distribution detection tasks. We validate our approach on\nthe CIFAR-10 dataset, and show that it can significantly improve the\ninterpretability and reliability of DNNs.",
        "updated": "2024-03-13 16:06:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08652v1"
    }
]