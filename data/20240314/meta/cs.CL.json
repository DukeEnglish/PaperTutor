[
    {
        "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
        "authors": "Adam IbrahimBenjamin ThérienKshitij GuptaMats L. RichterQuentin AnthonyTimothée LesortEugene BelilovskyIrina Rish",
        "links": "http://arxiv.org/abs/2403.08763v1",
        "entry_id": "http://arxiv.org/abs/2403.08763v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08763v1",
        "summary": "Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by final loss and\nlanguage model (LM) evaluation benchmarks. Specifically, we show this for a\nweak but realistic distribution shift between two commonly used LLM\npre-training datasets (English$\\rightarrow$English) and a stronger distribution\nshift (English$\\rightarrow$German) at the $405$M parameter model scale with\nlarge dataset sizes (hundreds of billions of tokens). Selecting the weak but\nrealistic shift for larger-scale experiments, we also find that our continual\nlearning strategies match the re-training baseline for a 10B parameter LLM. Our\nresults demonstrate that LLMs can be successfully updated via simple and\nscalable continual learning strategies, matching the re-training baseline using\nonly a fraction of the compute. Finally, inspired by previous work, we propose\nalternatives to the cosine learning rate schedule that help circumvent\nforgetting induced by LR re-warming and that are not bound to a fixed token\nbudget.",
        "updated": "2024-03-13 17:58:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08763v1"
    },
    {
        "title": "DAM: Dynamic Adapter Merging for Continual Video QA Learning",
        "authors": "Feng ChengZiyang WangYi-Lin SungYan-Bo LinMohit BansalGedas Bertasius",
        "links": "http://arxiv.org/abs/2403.08755v1",
        "entry_id": "http://arxiv.org/abs/2403.08755v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08755v1",
        "summary": "We present a parameter-efficient method for continual video\nquestion-answering (VidQA) learning. Our method, named DAM, uses the proposed\nDynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable\nefficient adaptation to continually arriving datasets, (iii) handle inputs from\nunknown datasets during inference, and (iv) enable knowledge sharing across\nsimilar dataset domains. Given a set of continually streaming VidQA datasets,\nwe sequentially train dataset-specific adapters for each dataset while freezing\nthe parameters of a large pretrained video-language backbone. During inference,\ngiven a video-question sample from an unknown domain, our method first uses the\nproposed non-parametric router function to compute a probability for each\nadapter, reflecting how relevant that adapter is to the current video-question\ninput instance. Subsequently, the proposed dynamic adapter merging scheme\naggregates all the adapter weights into a new adapter instance tailored for\nthat particular test sample to compute the final VidQA prediction, mitigating\nthe impact of inaccurate router predictions and facilitating knowledge sharing\nacross domains. Our DAM model outperforms prior state-of-the-art continual\nlearning approaches by 9.1% while exhibiting 1.9% less forgetting on 6 VidQA\ndatasets spanning various domains. We further extend DAM to continual image\nclassification and image QA and outperform prior methods by a large margin. The\ncode is publicly available at: https://github.com/klauscc/DAM",
        "updated": "2024-03-13 17:53:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08755v1"
    },
    {
        "title": "Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework",
        "authors": "Jingling LiZeyu TangXiaoyu LiuPeter SpirtesKun ZhangLiu LeqiYang Liu",
        "links": "http://arxiv.org/abs/2403.08743v1",
        "entry_id": "http://arxiv.org/abs/2403.08743v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08743v1",
        "summary": "Large language models (LLMs) can easily generate biased and discriminative\nresponses. As LLMs tap into consequential decision-making (e.g., hiring and\nhealthcare), it is of crucial importance to develop strategies to mitigate\nthese biases. This paper focuses on social bias, tackling the association\nbetween demographic information and LLM outputs. We propose a causality-guided\ndebiasing framework that utilizes causal understandings of (1) the\ndata-generating process of the training corpus fed to LLMs, and (2) the\ninternal reasoning process of LLM inference, to guide the design of prompts for\ndebiasing LLM outputs through selection mechanisms. Our framework unifies\nexisting de-biasing prompting approaches such as inhibitive instructions and\nin-context contrastive examples, and sheds light on new ways of debiasing by\nencouraging bias-free reasoning. Our strong empirical performance on real-world\ndatasets demonstrates that our framework provides principled guidelines on\ndebiasing LLM outputs even with only the black-box access.",
        "updated": "2024-03-13 17:46:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08743v1"
    },
    {
        "title": "The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models",
        "authors": "Carlo NicoliniJacopo StaianoBruno LepriRaffaele Marino",
        "links": "http://arxiv.org/abs/2403.08739v1",
        "entry_id": "http://arxiv.org/abs/2403.08739v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08739v1",
        "summary": "A substantial gap persists in understanding the reasons behind the\nexceptional performance of the Transformer architecture in NLP. A particularly\nunexplored area involves the mechanistic description of how the distribution of\nparameters evolves over time during training. In this work we suggest that\nlooking at the time evolution of the statistic distribution of model\nparameters, and specifically at bifurcation effects, can help understanding the\nmodel quality, potentially reducing training costs and evaluation efforts and\nempirically showing the reasons behind the effectiveness of weights\nsparsification.",
        "updated": "2024-03-13 17:42:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08739v1"
    },
    {
        "title": "Improving Acoustic Word Embeddings through Correspondence Training of Self-supervised Speech Representations",
        "authors": "Amit MeghananiThomas Hain",
        "links": "http://arxiv.org/abs/2403.08738v1",
        "entry_id": "http://arxiv.org/abs/2403.08738v1",
        "pdf_url": "http://arxiv.org/pdf/2403.08738v1",
        "summary": "Acoustic word embeddings (AWEs) are vector representations of spoken words.\nAn effective method for obtaining AWEs is the Correspondence Auto-Encoder\n(CAE). In the past, the CAE method has been associated with traditional MFCC\nfeatures. Representations obtained from self-supervised learning (SSL)-based\nspeech models such as HuBERT, Wav2vec2, etc., are outperforming MFCC in many\ndownstream tasks. However, they have not been well studied in the context of\nlearning AWEs. This work explores the effectiveness of CAE with SSL-based\nspeech representations to obtain improved AWEs. Additionally, the capabilities\nof SSL-based speech models are explored in cross-lingual scenarios for\nobtaining AWEs. Experiments are conducted on five languages: Polish,\nPortuguese, Spanish, French, and English. HuBERT-based CAE model achieves the\nbest results for word discrimination in all languages, despite Hu-BERT being\npre-trained on English only. Also, the HuBERT-based CAE model works well in\ncross-lingual settings. It outperforms MFCC-based CAE models trained on the\ntarget languages when trained on one source language and tested on target\nlanguages.",
        "updated": "2024-03-13 17:42:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.08738v1"
    }
]