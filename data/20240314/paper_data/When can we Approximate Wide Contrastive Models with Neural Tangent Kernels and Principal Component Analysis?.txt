When can we Approximate Wide Contrastive Models with Neural
Tangent Kernels and Principal Component Analysis?
GauthamGovindAnil GAUTHAMGA.GGA@GMAIL.COM
IndianInstituteofTechnologyMadras
PascalEsser ESSER@CIT.TUM.DE
DebarghyaGhoshdastidar GHOSHDAS@CIT.TUM.DE
TechnicalUniversityofMunich
Abstract
Contrastivelearningisaparadigmforlearningrepresentationsfromunlabelleddatathathasbeen
highlysuccessfulforimageandtextdata. Severalrecentworkshaveexaminedcontrastivelosses
toclaimthatcontrastivemodelseffectivelylearnspectralembeddings,whilefewworksshowrela-
tionsbetween(wide)contrastivemodelsandkernelprincipalcomponentanalysis(PCA).However,
itisnotknowniftrainedcontrastivemodelsindeedcorrespondtokernelmethodsorPCA.Inthis
work, we analyze the training dynamics of two-layer contrastive models, with non-linear activa-
tion, andanswerwhenthesemodelsareclosetoPCAorkernelmethods. Itiswellknowninthe
supervised setting that neural networks are equivalent to neural tangent kernel (NTK) machines,
andthattheNTKofinfinitelywidenetworksremainsconstantduringtraining. Weprovidethefirst
convergence results of NTK for contrastive losses, and present a nuanced picture: NTK of wide
networksremainsalmostconstantforcosinesimilaritybasedcontrastivelosses,butnotforlosses
basedondotproductsimilarity. Wefurtherstudythetrainingdynamicsofcontrastivemodelswith
orthogonalityconstraintsonoutputlayer,whichisimplicitlyassumedinworksrelatingcontrastive
learningtospectralembedding. Ourdeviationboundssuggestthatrepresentationslearnedbycon-
trastive models are close to the principal components of a certain matrix computed from random
features.Weempiricallyshowthatourtheoreticalresultspossiblyholdbeyondtwo-layernetworks.
Keywords: ContrastiveLoss,Self-supervisedLearning,LearningDynamics,NeuralTangentKer-
nel,PrincipalComponentAnalysis
1. Introduction
The paradigm of self-supervised learning (SSL) builds on the idea of using knowledge about se-
manticsimilaritiesinthedatatodefinewhichdata-pointsshouldbemappedclosetoeachotherin
the latent representation. The goal of SSL is to learn a “good representation”. While there is no
uniquenotionof“good”withouttakingadownstreamtaskintoconsideration(Bengioetal.,2013),
in general one is interested in mapping semantically similar objects to close representations in the
latent space, but avoid “dimension collapse” that occurs when different dimensions in the latent
spacecollapsetothesamevalue. Dependingonthemechanismusedtopreventcollapseoflearned
embeddings, SSL strategies can be broadly categorised as contrastive or non-contrastive learning.
Contrastivelearningreliesonnegativesamplestoensurerepresentationsdonotcollapse(Oordetal.,
2018; Chen et al., 2020; He et al., 2020; HaoChen et al., 2021), whereas non-contrastive learning
avoidscollapsebyincorporatingarchitecturalasymmetry(Grilletal.,2020;ChenandHe,2021)or
reductionindimensionredundancy(Zbontaretal.,2021;Bardesetal.,2021). Inpractice,aplethora
ofSSLstrategies,includingdeepcontrastiveandnon-contrastivemodels,havebeenproposedover
© G.G.Anil,P.Esser&D.Ghoshdastidar.
4202
raM
31
]GL.sc[
1v37680.3042:viXraANILESSERGHOSHDASTIDAR
thepastyearsacrossmultipledomains;manyofthemdemonstratingexcellentperformanceempir-
ically(Assranetal.,2022;Wangetal.,2023). WhiletheseworksunderlinetheimportanceofSSL
and(non-)contrastivemodelsforapplications,theirtheoreticalunderstandingisstilllimited.
TheoreticalanalysisofSSLisinitsearlystages. Therehasbeenconsiderableeffortinderiving
generalizationerrorboundsfordownstreamtasksonlearnedembeddings(Aroraetal.,2019b;Wei
etal.,2021;Baoetal.,2022),andanalysingspectral/isoperimetricpropertiesofdataaugmentation
(Han et al., 2023; Zhuo et al., 2023). Results based on learning theoretic measures (Saunshi et al.,
2019; Wei et al., 2020; Nozawa and Sato, 2021), information theory (Tsai et al., 2020; Tosh et al.,
2021)andlosslandscapes(Pokleetal.,2022;Ziyinetal.,2022)havebeenstudied.
Generalisationbounds,however,providelittleunderstandingoftherepresentationslearnedvia
SSL. Balestriero and LeCun (2022) answer this by showing that various (non)contrastive learning
formulations result in learning spectral embedding, principal component analysis (PCA) or their
variants. In a similar vein, Munkhoeva and Oseledets (2023) relate contrastive learning with trace
maximizationproblemsandmatrixcompletion—allrelatedtoPCA.Equivalencesbetweentheop-
timization formulations of SSL and PCA do not necessarily imply that (non-)contrastive models,
trained with gradient descent, perform PCA. This requires analysing either the converged solution
orthetrainingdynamicsofSSL.
A number of works derive and study the training dynamics of (non)contrastive learning, albeit
mostly limited to linear neural networks (Wang and Isola, 2020; Tian et al., 2021; Wang and Liu,
2021; Tian, 2022; Esser et al., 2023b). In the context of non-linear networks, Simon et al. (2023)
suggest that for wide neural networks, that is, in the neural tangent kernel (NTK) regime (Jacot
et al., 2018; Lee et al., 2019), contrastive learning could be equivalent to kernel PCA (Scho¨lkopf
etal.,1997). Althoughnopriorworkexplicitlyanalysestheconvergenceofwidecontrastivemodels
to kernel (or NTK) machines, there has been a significant interest in training kernel models under
(non)contrastive losses (Kiani et al., 2022; Cabannes et al., 2023; Esser et al., 2023a). Depending
ontheproblemformulation,itcanindeedbeshownthatthesekernelcontrastivemodelsareclosely
relatedtokernelPCA(Esseretal.,2023a)orkernelsupportvectormachine(Shahetal.,2022).
Motivation and Contributions. In spite of strongly suggesting relations between constrative
learning,PCAandkernelmethods(orNTKs),existingtheoreticalworksdonotexplicitlyanswerif
trainedcontrastivemodelsareclosetokernelmethods,specificallywithafixeddeterministickernel
(as has been shown in the NTK regime for supervised models). There is also no theoretical evi-
denceonwhentrainedcontrastivemodelscanbeapproximatedbysolutionsofPCAorothertrace
maximizationproblems. Weanalysethetrainingdynamicsoftwo-layernon-linearnetworkstrained
undercontrastiveornon-contrastivelosses,andrigorouslyanswerbothquestions. Specifically:
1. In Section 3, we derive the NTK of two-layer networks of width M trained under (non)-
contrastiveloss,andstudythedeviationbetweenNTKafterseveralstepsofgradientdescent
fromtheNTKatinitialization. OurresultsaddressquestionsontheconstancyofNTK.
Observation1: (Non-)Contrastivelossesaredefinedintermsofsimilaritiesbetweenlearned
representations. We show that if the losses are in terms of dot-product similarity, then NTK
drasticallychangeswithinO(logM)trainingtime. Experimentsonnon-contrastivelearning
suggestthatNTKchanges(Simonetal.,2023),buttherewasnopriortheoreticalevidence.
Observation2: Incontrasttodotproductsimilarity,ifthelossesaredefinedintermsofcosine
similarity—consideredinInfoNCE(Oordetal.,2018)andSimCLR(Chenetal.,2020)—then
2WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
NTKafterO(M1/6)stepsisclosetoNTKatinitialisation. Thus,contrastivemodelstrained
undersuchlossescanbeapproximatedbykernelmethods,withafixedNTK.Unfortunately,
unlikesupervisedlearning—wheretrainedneuralnetworksinNTKregimeisthesolutionof
kernelregression—theremaynotbeaclosedformedanalyticalsolutionofthetrainedmodel.
2. In Section 4, we study the training dynamics of (Grassmannian) gradient descent under or-
thogonalityconstraintsoftheoutputlayerofthenetwork. Whileorthogonalityisnotimposed
in practical SSL approaches, it is often assumed in theoretical works to relate contrastive
learning to variants of PCA (Munkhoeva and Oseledets, 2023), in kernel SSL formulations
(Esseretal.,2023a),topreventdimensioncollapse(Esseretal.,2023b)etc.
Observation1: Wenotethat, withorthogonalityconstraint, somecontrastivelosses(ortheir
modifications)areequivalenttoPCAofaM×M matrixC(t)thatdependsonthenon-linear
featuresatthehiddenlayer,learnedaftertiterationsofgradientdescent.
Observation2: Forsomecosine-similaritybasedcontrastivelosses,theFrobeniusnormdevi-
√
ation∥C(t)−C(0)∥ = O(t/ M)suggestingthat,inthiscase,widecontrastivemodelsare
F
closetoPCAofarandomlyinitialisedmatrixC(0). Furthermore,therepresentationlearned
viaPCAfromC(t)andC(0)arealsoclose,uptoorthonormalrotations.
Empirical validation of our theoretical results are provided using MNIST dataset, and we further
showthatsomeoftheresultsmayalsoholdbeyondtwo-layernetworks(seeSection5). Allproofs
areprovidedinthesupplementarymaterial.
2. PreliminariesandProblemSetup
Before going into the main results of the paper, we first outline the contrastive learning setup, the
embeddingfunctionandNTKformulationunderconsideration,togetherwiththegeneralconditions
fortheNTKtoremainconstantduringtraining. Weusethefollowingnotationthroughoutthepaper:
Notation. Weuselowercaseboldletters(e.g. a)todenotevectorsanduppercaseboldletters(e.g.
A)todenotematrices. LetA denotetheith rowandA denotetheith columnofmatrixA. Let
i. .i
I be an appropriately sized identity matrix. ∥·∥ denotes the L norm, ∥·∥ denotes the Frobenius
p p F
normand∥A∥ := max {|A |}. WedenoteparameterΘattime-steptbyΘ(t);howeverthe
max ij ij
timeindexingissuppressedwhenitisclearfromthecontexttoimprovereadability.
2.1. (Non-)ContrastiveLearning
Inthiswork,ourprimaryfocusisonsample-contrastivemethodswhichusemultiplepositive/nega-
tivesamplepairs. ConsideradatasetofN datapoints: D := (cid:8)(cid:8) x ,x (cid:9)Q (cid:9)N ,wherex ∈ RD
n n,q q=1 n=1 n
denotesthenth D dimensionaldatasampleandx denotestheqth pairinrelationtox .1 Using
n,q n
thisformulation,wenowstateageneralformforthecontrastiveloss:
N
1 (cid:88) (cid:16) (cid:17)
L(D) := l {s(x ,x )}Q (1)
N n n,q q=1
n=1
1.Notethatthepaircouldinvolveapositiveornegativesample.Hence,thisframeworkencompassespopularexamples
suchasthecontrastivetripletsetting{x ,x+,x−}N andthenon-contrastivesetting{x ,x+}N .
n n n n=1 n n n=1
3ANILESSERGHOSHDASTIDAR
where l(·) is some function and s(x,x˜) is the similarity between representations of inputs x and
x˜ learned by a (non-)contrastive model. While softmax or its logarithm are typically used for l(·)
in practice, theoretical works often consider l(·) to be linear (Ji et al., 2023; Esser et al., 2023b).
While a wide range of similarity measures s(·,·) are considered, they often build on similar un-
derlying ideas. Losses such as MoCo (He et al., 2020) build on dot product similarity, while the
popularSimCLRandInfoNCE (Chenetal.,2020;Oordetal.,2018)lossesbuildoncosinesimilar-
ity. Therefore, we consider the following two similarity measures, where x (cid:55)→ f(x) denotes the
learnedrepresentation:2
s(x,x˜) := f(x)⊤f(x˜), (dotproduct)
f(x)⊤f(x˜)
s(x,x˜) := . (cosinesimilarity)
(∥f(x)∥+δ)(∥f(x˜)∥+δ)
Weconsiderthefollowingsetofassumptionsonthesimilaritymeasureandonthedata:
Assumption1(Constantforcosinesimilarity) δ isasmallstrictlypositiveconstant.
(cid:12) (cid:12)
Assumption2(Smoothness) (cid:12) ∂l(·) (cid:12) ≤ c foranyx,x˜.
(cid:12)∂s(x,x˜)(cid:12) l
(cid:8) (cid:9)
Assumption3(Boundedinputs) Inputvectorsarebounded,max ∥x ∥ ,∥x ∥ ≤ c .
n,q n ∞ n,q ∞ in
While δ is not typically considered in cosine similarity, assuming δ > 0 ensures that s(x,x˜) is
defined even when norms of the representations are zero. Furthermore, δ > 0 can be made arbi-
trarilysmall,makingAssumption1practicallyreasonable. Apartfrommakingthecosinesimilarity
computation numerically stable, this structure for cosine similarity helps to simplify the proofs by
providing a strictly positive lower bound on ∥f(x)∥ + δ for any x. Assumption 2 is evidently
satisfied for commonly considered losses where l(·) is linear or softmax . Assumption 3 is often
consideredfortheoreticalanalysisinNTKliterature(e.g. Jacotetal.(2018)).
2.2. EmbeddingFunction
The outlined setup for contrastive losses is stated for an arbitrary embedding function f(·). How-
ever, for our analysis, we focus on one hidden layer neural networks. We aim to find a mapping
f(x;θ) : RD → RZ parameterized by θ where typically D > Z. In particular, we consider a
two-layer fully connected non-linear neural network: f(x;θ) = W⊤ϕ(Vx) where x ∈ RD is an
inputvectorandϕisapointwisenon-linearactivationfunction. W ∈ RM×D andV ∈ RM×Z are
thetrainableweightmatricesandletθ bethevectorwhichcontainsallentriesofW andV. Inthe
contextof(infinite)widthanalysis,the‘appropriate’initializationoftheseweightsisessential. Ex-
istingNTKliteratureonsupervisedlearning(e.g. Jacotetal.(2018);Aroraetal.(2019a))considers
thefollowingparameterization:
1
f(x;θ) := √ W⊤ϕ(Vx) (2)
M
where each W ,V ∼ N(0,1). We consider this setup, termed NTK parametrization, for the
i,j i,j
remainderofthepaper. Inaddition,wealsoconsiderthefollowingassumptions:
2.Notethatf(·)isaparameterizedfunctionaswelaterdefinein(2). However,wesuppresstheparameterizationhere
foreaseofnotation.
4WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
Assumption4(Maxnormofweightsatinitialization) ∥W(0)∥ ,∥V(0)∥ ≤ c logM.
max max θ
Assumption5(Smoothnessofactivationfunction) ϕisL -Lipschitzandβ -smooth.
ϕ ϕ
Assumption6(Boundsongradientsandweights) Letb = ∂f,b = ∂f . Atinitialization,
√ 1 ∂x 2 ∂ϕ(Vx)
∥W(0)∥ 2,∥V(0)∥
2
≤ c
s
M andthereisconstants 0suchthat∥b i∥
∞
≤ √s M0 ∥b i∥
2
for i = 1,2.
Assumption4holdswithhighprobabilityforstandardGaussianinitialisationofweights. Assump-
tion5isausuallyconsideredsmoothnesscriterion(andholdsforsigmoid,tanhetc.). Assumption6
istypicallyneededtoproveconstancyofNTKforsupervisedmodels(seeLiuetal.,2020a,b).
2.3. ConditionsforConstancyofNTK
LetusstartbyoutliningtheNTKanalysisingeneralforafunctionf(x;θ(t)) : RD → RZ,where
Z ≥ 1. Forinputvectorsx ∈ RD andx˜ ∈ RD,wedefinetheempiricalNTK foraneuralnetwork
f(·)parameterizedbyθ(t)as:
∂f (x;θ(t))⊤∂f (x˜;θ(t))
i j
K (x,x˜;θ(t)) :=
ij
∂θ(t) ∂θ(t)
wheref (x;θ)representtheithentryoftheZdimensionalfunctionoutput. Ingeneral,K(x,x˜;θ(t))
i
varies with time t as the model is trained. However, under certain conditions, for infinitely wide
neuralnetworks,theNTKstaysconstantduringtraining(Jacotetal.,2018;Aroraetal.,2019a),i.e,
∀t |K (x,x˜;θ(t))−K (x,x˜;θ(0))| → 0asM → ∞. (3)
ij ij
Furthermore, under Gaussian initialisation of parameters, it holds that the NTK at initialisation
converges, as M → ∞, to an analytical NTK K∗ (x,x˜) := E [K (x,x˜;θ)]. As the NTK does
ij θ ij
notchangeduringtraining,thetrainingdynamicsofthenetworkatanytimestepcanbewrittenin
terms of K∗ — in the supervised setting, this leads to kernel regression at convergence. To prove
constancy of the form (3), several works have analyzed NTKs in the supervised setting (Arora
etal.,2019a;Leeetal.,2019;Chizatetal.,2019). Inparticular,Liuetal.(2020b)showedthatthe
constancyofNTKispredicatedonthespectralnormoftheHessian.
TostudytheNTKofcontrastivemodels,weconsiderf(x;θ)oftheform(2)andusethemachin-
erybuiltin(Liuetal.,2020b,a). DefineZ Hessianmatrices,oneforeachelementoftheoutputrep-
resentation.
ThezthHessianmatrixH(z)(evaluatedatinputx)isH(z)
(x) :=
∂2fz(x;θ(t))
, z ∈ [Z].
ij ∂θi(t)∂θj(t)
WeboundthechangeinthespectralnormoftheHessianintermsofthechangeinweightsbyadapt-
ingLiuetal.(2020a,Theorem7.1)3 toaccountformulti-dimensionaloutputs:
Lemma7(BoundonthenormoftheHessian) Under Assumptions 3, 5, 6, consider the neural
networkdefinedin(2). Ifthechangeinweightsduringtrainingisboundedas
∥W(t)−W(0)∥ +∥V(t)−V(0)∥ ≤ R, (4)
F F
then,∀ z ∈ [Z],withα = 4β c2 L andα = 4L c (1+β c s c ),thezthHessianisbounded
1 ϕ in ϕ 2 ϕ in ϕ in 0 s
(cid:13) (cid:13)
as: (cid:13)H(z)(x;θ(t))(cid:13) ≤ α1√R+α2.
(cid:13) (cid:13) 2 M
(cid:13) (cid:13) (cid:16) (cid:17)
3.Theorem7.1of(Liuetal.,2020a)givesaboundoftheform(cid:13) (cid:13)H(z)(x;θ(t))(cid:13)
(cid:13)
2
= O R√3 ML foranetworkwithL
layers.However,fortwo-layernetworks,itispossibletoreducethisboundtotheformgiveninLemma7.
5ANILESSERGHOSHDASTIDAR
With help of Lemma 7, we can now bound the change in NTK. Towards this, we extend Proposi-
tion2.3ofLiuetal.(2020b)tothemulti-dimensionalcase(Z > 1)toobtainthefollowinglemma:
Lemma8(BoundonthechangeinNTK) Define S := {s ∈ Rp;∥s−s(0)∥ ≤ R}, where p is
(cid:13) (cid:13)
the total number of learnable parameters in (2). Assume that for any input x, (cid:13)H(z)(x;s)(cid:13) ≤ ϵ
(cid:13) (cid:13)
2
and ∥∇ f (x;s)∥ ≤ c , ∀ z ∈ [Z] and ∀ s ∈ S. Then, for any inputs x,x˜, ∀ s ∈ S and
s z 2 0
∀ i,j ∈ [Z],|K (x,x˜;s)−K (x,x˜;s(0))| ≤ 2ϵc R.
ij ij 0
If K does not change during training, the analytical (expected) NTK K∗ models the behaviour of
the network not only at initialization, but also at convergence and therefore allows us to express
the network dynamics in a simple form. In supervised settings with squared loss, it is known that
condition(4)istruetillconvergenceforwideneuralnetworks(Liuetal.,2020a). Whileitispossible
touseLemmas7and8toexaminethebehaviourofNTKingeneral,notethatitisnotknownwhen
thecondition(4)holdsforanarbitrarylossfunction. InSection3, westudythevalidityofthis
conditionwhenlearningembeddingsusingaforementioned(non-)contrastivelosses.
3. OntheConstancyofNTKsunderContrastiveLosses
WenowexaminetheNTKevolutionforneuralnetworkstrainedundercontrastivelosses. Todoso,
usingtheabovepresentedsetup,wederivethedynamicsofaneuralnetworktrainedusinggradient
flowunderalossoftheform(1)intermsoftheNTKoftheneuralnetworkasdefinedin(2):
Lemma9(ContrastivelearningdynamicsintermsofNTK) Considertraininganeuralnetwork
oftheform(2)usingalossl(·)oftheform(1)undergradientflowondatasetD. Letg (x,x˜;θ(t)) :=
i
∂s(x,x˜) . Then,forz ∈ [Z],therepresentationofanarbitraryinputx˜ evolvesas:
∂fi(x;θ(t))
(cid:34) Z
∂f z(x˜;θ(t)) 1 (cid:88) ∂l(·) (cid:88)
= − [K (x˜,x ;θ(t))g (x ,x ;θ(t))
zi n i n n,q
∂t N ∂s(x ,x )
n n,q
n,q i=1
(cid:3)
+K (x˜,x ;θ(t))g (x ,x ;θ(t))] .
zi n,q i n,q n
While Lemma 9 holds for any loss of the form (1), we are interested in the behaviour of the NTK
when trained under losses which use dot product or cosine similarity as the similarity measure.
Numerical Simulation. Throughout the paper, we illus-
trate our theoretical findings numerically. If not otherwise
stated,allexperimentsareperformedon1000randomlysam-
pledpointsoftheMNISTdataset(Deng,2012)4. Thepositive
samples are obtained by randomly resizing and cropping the
corresponding data sample. Negative samples are obtained
by randomly sampling from the entire dataset. For all ex-
periments,wegenerateonepositivesampleandonenegative
sample for each data sample. All plots indicate the average Figure 1: Difference between NTK
over 5 runs and additional plots are provided in the supple- andGDdynamics.
mentarymaterial. Ifnototherwisestated,weconsiderReLU
4.Notethatincreasingnumberofsampleswouldnothaveaneffectontheoveralltrendsduetothepresenceofthe 1
N
terminthelossasdefinedin(1).
6WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
activationfunctionandlinearcontrastiveloss. Beforegoingintotheoreticalanalysis,weempirically
illustratethatthedynamicsderivedfromtheNTK(atinitialization)issimilartotheactualdynamics
obtained by training the network. We first train (2) using cosine similarity for 500 epochs. We re-
peatthesameusingtheNTKdynamicsinLemma9. Forthissimulation,weconsiderN = Z = 10.
InFigure1,weplotthefractionaldifferencebetweentherepresentationslearnedbythetwometh-
ods after taking the maximum across inputs and dimensions. We observe that as M increases, the
differencedecreasesandthedynamicsofLemma9alignwithtraining(2)usinggradientdescent.
3.1. NTKforDotproductLossesDoesNotNecessarilyStayConstant
We start with examining the change in weights (as defined in (4)) of a network trained using con-
trastivelosseswhichutilizedotproductasthesimilaritymeasure. Inparticular,weshowthatinthis
setting, thereexistscaseswherethechangeinweightsbecomearbitrarilylargeevenforarbitrarily
wide neural networks, implying that the NTK does not remain constant. To demonstrate this, we
consider a simple loss function of the form (1) under dot product similarity. Because there is no
normalization in the dot product similarity measure, the loss can be minimized arbitrarily by scal-
ingtheweightsandhenceweexpecttheweightstogrowarbitrarilylargewithtime. Weformalize
thisnotionanditsimplicationsontheconstancyoftheNTKinthefollowingproposition:
Proposition10(NTKunderdotproductdoesnotremainconstant) For D = Z = 1, linear
loss (l(a) := a), dot product similarity and triplet setting (D = {x ,x+,x−}N ) in (1), the
n n n n=1
optimizationis: min 1 (cid:80)N f(x ;θ)(f(x−;θ)−f(x+;θ)). Consideranetwork(2)withlinear
θ N n=1 n n n
activation(ϕ(a) := a),weightsinitialisedasindependentN(0,1),andtrainedviagradientflow.
There is a dataset such that, with probability at least 1− √25 , for a time step t˜∈ (0,logM) and
M
anyinputpairx,x˜withxx˜ ̸= 0,theNTKsatisfies|K(x,x˜;θ(t))−K(x,x˜;θ(0))| → ∞ast → t˜.
Proposition 10 shows there are cases where the NTK does not remain constant even for arbitrarily
widenetworksandlogarithmictrainingtimewhentrainedunderdotproductsimilaritybasedloss.
Numerical Simulation on MNIST. To show that Proposi-
tion10istrueinpractice,welookatthedeviationoftheNTK
with training across networks of varying widths. The results
are shown in Figure 2. We observe that, as expected, even
afterafewepochs,thedifferenceinNTKdiverges.
3.2. NTKforCosinesimilarityLossesRemainsConstant
Consideringthesamequestionasintheprevioussection,we
nowshiftourfocusontotheconstancyofNTKforlossesde- Figure2: ChangeinNTK.
finedintermsofthecosinesimilarity. Thekeydifferencebetweendotproductandcosinesimilarity
isthepresenceofnormalizationbynormsoftherepresentations. Wenowshowthatthisnormaliza-
tionplaysanimportantroleindecidingthelearningdynamicsandexamineitsimplicationsonthe
constancyoftheNTK.ToproveconstancyoftheNTK,wemakeuseofthefactthatthesimilarity
measureisnormalizedandfirstestablishaboundonthemaximumelement-wisechangeinweights.
Lemma11(Boundonelement-wisechangeinweightsundercosinesimilarity) UnderAssump-
tions1-5,considerlossesoftheform(1)wherecosinesimilarityisused. Ifaneuralnetworkf(·)
asdefinedin(2)istrainedusinggradientdescentwithlearningrateη,atanytimet,thechangein
7ANILESSERGHOSHDASTIDAR
weightsarebo √undedas: |∆V ij(t)| ≤ √β M1 ∥W(t)∥
max
and|∆W ij(t)| ≤ √β M2 ∥V(t)∥
max
where
β = 4c c Q ZL andβ = 4c c QDL areconstantsindependentofM.
1 δ l in ϕ 2 δ l in ϕ
From Lemma 11, it can be seen that bounds for change in V and W form a coupled system. To
studythediscrete-timedynamicsofthissystem,wedefineandcharacterizeausefulquantityc(t):
Lemma12(Boundonweightdifferenceduringtrainingundercosinesimilarity) Defineβ :=
(cid:16) (cid:17)t
max{β 1,β 2} and c(t) := c(0) 1+ √β where c(0) = c θlogM. Then, for any t, we have :
M
∥V(t)−V(0)∥ ≤ c(t)−c(0)and∥W(t)−W(0)∥ ≤ c(t)−c(0).
max max
WenowstatethemaintheoremregardingtheconvergenceofNTKforcosinesimilaritylosses:
Theorem13(BoundonthechangeinNTKundercosinesimilarity) Considerlossesoftheform
(1)withcosinesimilarity. Letc(0),β betheconstantinLemma12,Rbeasin(4)5,α ,α beasin
√ 1 2
Lemma7andγ := 2 2DL c . Ifaneuralnetworkf(·)oftheform(2)istrainedusinggradient
ϕ in
descent,thenunderAssumptions1-6,fort ≤ Mα iterations,thechangeinNTKisboundedas
(cid:16) (cid:17) α R2+α R
|K (x,x˜;θ(t))−K (x,x˜;θ(0))| ≤ γ
c(0)eβMα−0.5 1
√
2
.
ij ij
M
Inparticular,ifwesetα = 1 andassumeM ≥ max{1,β3},thentheabovestatementsimplifiesto
6
(cid:16) (cid:17)
max sup |K (x,x˜;θ(t))−K (x,x˜;θ(0))| = O M−1/6(logM)3 .
ij ij
t∈(0,M1/6] x,x˜
According to Theorem 13, wide neural networks trained under cosine similarity based contrastive
loss have a nearly constant NTK even after M1/6 iterations of gradient descent. This is in sharp
contrast to networks trained under dot product based losses where the change in weights become
arbitrarily large within logM gradient descent updates. Intuitively, this holds since normalization
ensuresthatthechangeinweightsremainsufficientlysmallinthecaseofcosinesimilarity.
Numerical Simulation on MNIST. To check the validity
ofTheorem13,welookatthethechangeinNTKwithtrain-
ingacrossnetworksofvaryingwidths. Theresultsareshown
in Figure 3. In general, we expect the change in each entry
of the NTK to decrease with an increase in depth. We see
that this is indeed the case. For a fixed number of iterations,
t = O(1), we expect α ≈ 0 and the change to go down
√
(roughly)as1/ M withincreasingwidth. Figure3: ChangeinNTK.
Remark14(ClosedformsolutionintermsofNTK) In the supervised setting, combining the
analyticalNTKwiththeclosedformsolutionforkernelregressionprovidesaclosedformexpression
ofthenetworktraineduntilconvergence(Jacotetal.,2018),orevenwhenearlystopped. Wetakea
steptowardssucharesultforcontrastivelosses. InLemma9,wepresentthelearningdynamicsin
thecontrastivelosssettingintermsoftheNTKandinTheorem13,weshowthattheNTKremains
constant during training for M1/6 steps under cosine similarity. While this is an important step
5.NotethatRhereisafunctionofM,withtherelationbeinggivenbyLemma12.Similarly,c(0)=c logM.
θ
8WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
towards a better theoretical understanding of contrastive models, it does not yet provide a closed
form solution of the model output in terms of the NTK. While prior works on kernel contrastive
methods suggest that, in the wide neural network regime, contrastive losses could be equivalent to
kernelPCA(Simonetal.,2023),thisconnectionhasnotbeenprovensofarandisnotapparentfrom
the dynamics derived in Lemma 9 even if the NTK is constant (for cosine similarity based losses).
Therefore,weshiftourviewpointfromNTKdynamicstoexplicitlyinvestigatingthisconnection.
4. OntheConnectionbetweenWideContrastiveModelsandPCA
We next study if there is a formal connection between PCA and representations learned by con-
trastivemodels, trainedwithgradientdescent. Priorworkshaveconnectedcontrastivemodelstoa
tracemaximizationproblemwithanorthogonalityconstraintontheoutputlayer,oftheform:
(cid:16) (cid:17)
maxTr W⊤C W s.t. W⊤W = I , (5)
ϑ Z
W,ϑ
whereC ∈ RM×M isasymmetricmatrixthathasa(possiblynon-linear)datadependencethrough
ϑ
afunctionparameterizedbyϑ. IfC staysconstantduringoptimization,optimizationisdoneonly
ϑ
overW andhencethe problemsimplifiestoPCAonC . ToconnectcontrastivelossesandPCA,
ϑ
it is then necessary to analyze the behaviour of C when a contrastive model is trained. Existing
ϑ
worksdonotexaminethisaspectofneuralnetworkdynamics. Morespecifically,Esseretal.(2023a)
considersakernelsettingwherelearningisdoneusingacontrastivelossoftheform(5),butdoesnot
linkittotheneuralnetworkdynamics. Esseretal.(2023b)considersneuralnetworkdynamicsfor
contrastivelossesoftheform(5)butonlyexaminesthelinearsetting. Simonetal.(2023)considers
thedynamicsforkernelmodelsbutdoesnotinvestigateifthekerneldynamicsareclosetotheneural
networkdynamics. MunkhoevaandOseledets(2023)reformulateslossessuchasSimCLRto(5).
Extendingpriorworks,weworktowardsformalizingtheconnectionbetweennon-linear,wide
networksandPCA,notonlythroughrewritingtheloss,butalsobytakinglearningintoconsider-
ation. Let us consider (1) with a linear loss such that we obtain (6). In addition, we consider (2)
underorthogonalityconstraintonthesecondlayertoobtainaneuralnetworkoftheform(7):
N Q
1 (cid:88)(cid:88)
L(D) := − α s(x ,x ) (6)
q n nq
N
n=1q=1
1
f⊥(x;θ) := √ W⊤ϕ(Vx) s.t. W⊤W = I . (7)
Z
M
whereα = 1if(x ,x )isapositivepairandα = −1foranegativepair. Observethatifweuse
q n nq q
the dot product similarity, s(x,x˜) = f⊥(x;θ)⊤f⊥(x˜;θ), then the minimisation of the contrastive
lossL(D)in(6)canbedirectlyposedasatracemaximizationproblem6
(5)withC
ϑ
= C(cid:101)V =
C
V
+C⊤
V ; C
V
=
1 (cid:88)N (cid:88)Q
α qϕ(Vx n,q)ϕ(Vx n)⊤. (8)
2 MN
n=1q=1
6.ThevalueoftheTracein(5)isthesameirrespectiveofwhetherweuseC
V
orC(cid:101)V. However,usingthesymmetric
C(cid:101)V allowsustomaketheconnectionofsolving(5)throughPCAmoredirect.
9ANILESSERGHOSHDASTIDAR
Remark15(FromcontrastivemodelstoPCAincaseofdotproductsimilarity) Proposition10
showsthattheNTKdivergeswithinlog(M)stepsfortrainingwithdotproductbasedlosses. While
wedonotexplicitlyprovethis,italsofollowsthat,asthefirstlayerV istrained,thematrixC (t)
V
divergesarbitrarilyfromtheinitialisationC (0). Hence,whileminimizingL(D)foranyfixedV
V
corresponds to PCA for finding W (see (8)), contrastive models trained with dot product based
lossesdonotseemtobeequivalenttoPCAforaconstantmatrixC .
ϑ
4.1. CosinesimilarityObjectiveisClosetoPCAforWideNetworks
Due to the near constancy of NTK under cosine similarity based losses, we investigate if it is pos-
sible to relate cosine similarity based trained contrastive models with PCA of a fixed matrix, po-
tentiallyC
ϑ
= C(cid:101)V(0). Adirectequivalenceseemscomplicatedduetothenormalizationtermsof
cosine similarity. However, we note that with orthogonality on W, the cosine similarity measure
canbeboundedfrombelowas
f⊥(x;θ)⊤f⊥(x˜;θ)
(cid:0) W⊤ϕ(Vx)(cid:1)⊤(cid:0) W⊤ϕ(Vx˜)(cid:1)
≥ (9)
(∥f⊥(x;θ)∥+δ)(∥f⊥(x˜;θ)∥+δ) (∥ϕ(Vx)∥+δ′)(∥ϕ(Vx˜)∥+δ′)
√
since(cid:13) (cid:13)W⊤ϕ(Vx)(cid:13)
(cid:13) ≤
(cid:13) (cid:13)W⊤(cid:13)
(cid:13) ∥ϕ(Vx)∥ and∥W∥ = 1(whereδ′ := Mδ). Theinequality
2 2 2 2
(9) can be used to define a modified loss L(D) where s(x,x˜) is defined as the right hand side of
(9). Minimizingthecorrespondingloss(6)with(7)cannowbewrittenas
(5)withC
ϑ
= C(cid:101)V =
C
V
+
2
C⊤
V ; C V=
N1 (cid:88)N (cid:88)Q (∥ϕ(Vxα qϕ( )V ∥+x
n δ, ′q
)) (ϕ ∥( ϕV (Vx
n
x)⊤
)∥+δ′)
(10)
n,q n
n=1q=1
Note that (10) is of the form (5), however, C is still dependent on V and hence the optimization
V
in (6) with (7) is performed over both V and W. Lemma 16 below shows that for wide networks
trainedwithcosinesimilaritybasedlosses,C V(0)isclosetoC V(t)inFrobeniusnorm. HenceC(cid:101)V
alsoremainsalmostconstant,whichintuitivelysuggeststhat(10)becomes“close”to(5).
Lemma16(ConstancyofC (t)) UnderAssumptions1–5andconstraintW⊤W = I ,consider
V Z
trainingf⊥(·)in(7)fortiterationsusingGrassmanniangradientdescent7underlossesoftheform
√
(10)withlearningrateη. Then∥C (t)−C (0)∥ ≤ κ√t ,whereκ := 16δ−2ηQ2L2c2 D.
V V F M ϕ in
Numerical Simulation on MNIST. We train a network of the form (7) using a loss of the form
(10). Wethenexaminetheevolutionofthequantity∥C (t)−C (0)∥ withtrainingacrossvary-
V V F
ing widths. The results are shown in Figure 4 (left), where colors indicate the epochs (t ∈ [500]).
Whilethedifferenceincreasesslightlywithtraining,itgoesdownroughlyas √1 withanincrease
M
in width, which is in line with the behaviour predicted in Lemma 16. In addition, we observe in
Figure4(middle)thatW(t)changessignificantlyfasterthanC (t)duringtraining; thissuggests
V
thattheW thatislearnedisindeedthePCAofaC(cid:101)V(t)thatisclosetoC(cid:101)V(0).
7.Inshort,followingEdelmanetal.(1998),thederivativeofafunctiong(·)restrictedtoaGrassmannianmanifoldcan
beobtainedbyleft-multiplying1−g(·)g(·)⊤totheunrestrictedderivativeofg(·).
10WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
Figure 4: (left) Change in C during training for varying depths M. Time steps are indicated
V
by color. (middle) Training rate of C and W. Plotted are
∥CV(t)−CV(0)∥
F and
∥W(t)−W(0)∥
F.
V ∥CV(0)∥
F
∥W(0)∥
F
(right)DifferenceinoutputwhenC isfrozenandwhenC istrainedforvaryingwidthsM.
V V
4.2. RepresentationslearnedfromPCAofC(cid:101)(0)andPCAofC(cid:101)(t)areclose
We characterize the difference between the representations learned by performing PCA on C(cid:101)V(0)
andonC(cid:101)V(t). Lemmas16–17suggestthattraining(7)under(6)couldbeclosetoPCAonC V(0).
Lemma17(Perturbationboundonrepresentation) Letu(x;W∗,C )betherepresentationob-
ϑ
tainedfrom(7)withW = W∗,whereW∗isobtainedbysolving(5)forafixedC . UnderAssump-
ϑ
∗
tions 1–5, let W∗,W(cid:102) be the solutions of (5) obtained through PCA on fixed C(cid:101)V(0) a √nd C(cid:101)V(t)
respectively. Letλ Z,λ
Z+1
beZth and(Z+1)th eigenvaluesofC(cid:101)V(0). Letζ = 4δ−1ηQ DL2 ϕc2
in
andξ = 27 2δ−1QDc in(L ϕc
θ
+|ϕ(0)|). ThereexistsanorthogonalmatrixO suchthat
(cid:13) ∗ (cid:13) t (cid:18) ξlogM (cid:19)
(cid:13) (cid:13)O⊤u(x;W(cid:102) ,C(cid:101)V(t))−u(x;W∗,C(cid:101)V(0))(cid:13)
(cid:13)
≤ ζ√
M
1+
λ Z −λ Z+1
.
Numerical Simulation on MNIST. Consider training (7) according to (10) under two settings:
with C(cid:101)V(t) as a trainable matrix and with C(cid:101)V(0) as a fixed matrix. We look at the fractional
differencebetweenthelearnedrepresentationsaftertraininginthetwosettings. InFigure4(right),
the mean fractional difference of learned representations computed across samples is plotted. As
expected,thedifferencegoesdowntozeroaswidthincreases.
5. Furtherdiscussionandopenproblems
Effect of initialization on dimension collapse: For the presented analysis, we only have a mild
assumptionontheinitializationofweights(Assumption4)thatholdswithhighprobability. While
this assumption is sufficient for proving the constancy of NTK results in Section 3, additional as-
sumptions may be needed to obtain meaningful representations. While learning representations, it
is desirable to avoid dimension collapse. Dimension collapse, in the context of linear contrastive
models, has been shown for dot product (Esser et al., 2023b). Using NTK, we show that certain
initializationschemescancausedimensioncollapseevenifcosinesimilaritybasedlossesareused:
Proposition18 Consideraneuralnetworkoftheform(2)trainedusingalossoftheform(1)using
gradientdescent. Thenforanyinputx,W (0) = W (0) ⇒ f (x;θ(t)) = f (x;θ(t)), ∀ t ≥ 0.
.i .j i j
Thus, collapse occurs irrespective of whether the NTK remains constant or not, and hence, this is
applicableirrespectiveofthewidthoftheneuralnetwork. Further,theresultholdsforbothdotprod-
uctandcosinesimilaritybasedlosses. TheresultalsoholdsforanalyticalNTKK∗;ifK∗ isused
inthedynamicsinLemma9,thenf (x;θ(0)) = f (x;θ(0)) ⇒ f (x;θ(t)) = f (x;θ(t)), ∀t ≥ 0.
i j i j
11ANILESSERGHOSHDASTIDAR
Figure 5: (left) Change in empirical NTK with cosine similarity for 3 layer networks of varying
widths (time steps indicated by color). (middle) Maximum entry-wise change in empirical NTK
withdotproductsimilarityfor3layernetworksofvaryingwidths. (right)Accuracyondownstream
taskforPCAonC(cid:101)(0),fullytrainedmodelandproposediterativealgorithmover5updates.
Empirical observations beyond the theory: (i) Deep networks: While Theorem 13 has been
shown to hold only in the case of neural networks with a single hidden layer, we expect it to hold
for deep networks as well. We experimentally examine the case with 3 hidden layers in Figure 5
(left). The results are similar to the case of a single hidden layer, as we again observe a decay
withwidththatisroughly √1 uptoscaling. Alongsimilarlines, weobserveinFigure5(middle)
M
that the NTK for three hidden layer networks optimized with dot product based losses diverges,
similar to the single hidden layer case. (ii) Iterative learning of the trace maximization problem:
Combining thefindings fromSection 4, we proposean alternative optimization procedure tosolve
(5) under A := C(cid:101)V(t). As C
V
updates slower than W as shown in Figure 4 (middle), and
since for a fixed C(cid:101)V the optimal W can be obtained using PCA, an alternative update could be
by iteratively (a) updating W by solving PCA on C(cid:101)V(t−1) and (b) updating V by running one
step of gradient descent. We validate this approach in the following. (iii) Predictive accuracy in
downstream tasks: Extending the results from Section 4.2, we show in Figure 5 (right) that the
representationsobtainedwithPCAonC(cid:101)V(0)andfullytraining(7)under(6)areclosetoeachother
withregardstodownstreamtestaccuracyforasimplelinearclassifiertrainedontherepresentations.
While Section 4.2 provides results on the behaviour of the PCA for different time-steps of C(cid:101)V,
Figure 5 (right) suggests that this similarity extends to PCA on C(cid:101)V(0) and fully trained networks
as well. In addition, we also observe that the iterative optimization (as proposed above) performs
well,especiallyforsmallerwidths(possiblyduetothelargerchangeinC(cid:101)V forsmallM).
Open problem (Missing link in the claim contrastive models perform PCA): In Lemma 17,
we compare the solutions obtained by PCA on C(cid:101)V(0) and C(cid:101)V(t). But in Figure 4 (middle), we
observe that C and W evolve simultaneously, even though at different rates. The open problem
V
then, is the exact connection between fully trained contrastive models and the PCA solution. We
believe that a direct analysis, such as the one considered for Theorem 13, may not work. This is
because the solutions might differ significantly at convergence even for closely initialized models
if the eigengap is not significant. Therefore, a spectral viewpoint combined with the analysis of
gradientdescentstepsisnecessarytoboundthedeviation.
Open problem (NTK and PCA at convergence): Theorem 13 shows that, for wide networks,
the NTK remains nearly constant for M1/6 time-steps (in fact, one can also show constancy till
O(Mα)stepsforα < 1). WhilesuchresultsareinlinewithinitialNTKanalysisinthesupervised
4
setting (e.g. Jacot et al. (2018)), it is an open question whether the constancy of NTK holds until
12WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
convergence. In fact, Figure 3 suggests that for wide networks, NTK remains constant beyond
Mα steps. A potential approach to prove constancy till convergence would be to investigate the
stationarypointsassociatedwithcosinesimilaritybasedlossesandverifyiftheyareattainedwithin
Mα steps. While this is a crucial open question, we believe that the presented results provide the
firstvaluableinsightsintotheconstancyofNTKundercontrastivelosses,beyondthesquarederror.
InLemma17,weshowthattheoutputsofthetwoconsideredtracemaximizationproblemsare
√
closeforO(t(logM)/ M)steps. Astheexpressionisintermsoftime-steps,thequestionremains
iftheyarestillcloseatconvergence. Whilewedonothaveaprecisecharacterizationofsuchresults,
apossibleapproachcouldbetoextendXuandLi(2021),whoshowthatforZ = 1,PCAconverges
inroughlyO(log(M))stepsunderRiemanniangradientdescent.
Acknowledgments
The work was done when G. G. Anil was at TU Munich, supported by the German Academic
Exchange Service (DAAD) through the DAAD KOSPIE fellowship, 2023. This work is also sup-
portedbytheGermanResearchFoundation(DFG)throughthePriorityProgramSPP2298(project
GH257/2-1).
References
Sanjeev Arora, Simon S Du, Wei Hu, Zhiyuan Li, Russ R Salakhutdinov, and Ruosong Wang. On
exactcomputationwithaninfinitelywideneuralnet. Advancesinneuralinformationprocessing
systems,2019a.
Sanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saun-
shi. A theoretical analysis of contrastive unsupervised representation learning. In International
ConferenceonMachineLearning,2019b.
MahmoudAssran,MathildeCaron,IshanMisra,PiotrBojanowski,FlorianBordes,PascalVincent,
Armand Joulin, Mike Rabbat, and Nicolas Ballas. Masked siamese networks for label-efficient
learning. InEuropeanConferenceonComputerVision.Springer,2022.
Randall Balestriero and Yann LeCun. Contrastive and non-contrastive self-supervised learning re-
coverglobalandlocalspectralembeddingmethods. AdvancesinNeuralInformationProcessing
Systems,2022.
Han Bao, Yoshihiro Nagano, and Kento Nozawa. On the surrogate gap between contrastive and
supervisedlosses. InInternationalConferenceonMachineLearning,2022.
Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regulariza-
tionforself-supervisedlearning. arXivpreprintarXiv:2105.04906,2021.
Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives. IEEEtransactionsonpatternanalysisandmachineintelligence,2013.
Vivien Cabannes, Bobak Kiani, Randall Balestriero, Yann LeCun, and Alberto Bietti. The ssl
interplay: Augmentations, inductive bias, and generalization. In International Conference on
MachineLearning.PMLR,2023.
13ANILESSERGHOSHDASTIDAR
TingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton. Asimpleframeworkfor
contrastive learning of visual representations. In International conference on machine learning.
PMLR,2020.
XinleiChenandKaimingHe. Exploringsimplesiameserepresentationlearning. InProceedingsof
theIEEE/CVFconferenceoncomputervisionandpatternrecognition,2021.
LenaicChizat,EdouardOyallon,andFrancisBach. Onlazytrainingindifferentiableprogramming.
Advancesinneuralinformationprocessingsystems,2019.
Li Deng. The mnist database of handwritten digit images for machine learning research. IEEE
SignalProcessingMagazine,2012.
AlanEdelman,Toma´sAArias,andStevenTSmith. Thegeometryofalgorithmswithorthogonality
constraints. SIAMjournalonMatrixAnalysisandApplications,1998.
Pascal Esser, Maximilian Fleissner, and Debarghya Ghoshdastidar. Non-parametric representation
learningwithkernels. arXivpreprintarXiv:2309.02028,2023a.
PascalEsser,SatyakiMukherjee,andDebarghyaGhoshdastidar. Representationlearningdynamics
ofself-supervisedmodels. arXivpreprintarXiv:2309.02011,2023b.
Jean-Bastien Grill, Florian Strub, Florent Altche´, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,
etal. Bootstrapyourownlatent-anewapproachtoself-supervisedlearning. Advancesinneural
informationprocessingsystems,2020.
LuHan,Han-JiaYe,andDe-ChuanZhan. Augmentationcomponentanalysis: Modelingsimilarity
viatheaugmentationoverlaps. InTheEleventhInternationalConferenceonLearningRepresen-
tations,2023.
Jeff Z HaoChen, Colin Wei, Adrien Gaidon, and Tengyu Ma. Provable guarantees for self-
supervised deep learning with spectral contrastive loss. Advances in Neural Information Pro-
cessingSystems,2021.
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition(CVPR),2020.
ArthurJacot,FranckGabriel,andCle´mentHongler. Neuraltangentkernel: Convergenceandgen-
eralizationinneuralnetworks. Advancesinneuralinformationprocessingsystems,2018.
WenlongJi,ZhunDeng,RyumeiNakada,JamesZou,andLinjunZhang. Thepowerofcontrastfor
featurelearning: Atheoreticalanalysis. JournalofMachineLearningResearch,2023.
Bobak T Kiani, Randall Balestriero, Yubei Chen, Seth Lloyd, and Yann LeCun. Joint embedding
self-supervisedlearninginthekernelregime. arXivpreprintarXiv:2209.14884,2022.
V.Yu.KorolevandI.G.Shevtsova.Ontheupperboundfortheabsoluteconstantintheberry–esseen
inequality. TheoryofProbability&ItsApplications,54(4):638–658,2010.
14WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
Jaehoon Lee, Lechao Xiao, Samuel Schoenholz, Yasaman Bahri, Roman Novak, Jascha Sohl-
Dickstein, and Jeffrey Pennington. Wide neural networks of any depth evolve as linear models
undergradientdescent. Advancesinneuralinformationprocessingsystems,2019.
Chaoyue Liu, Libin Zhu, and Mikhail Belkin. Toward a theory of optimization for over-
parameterized systems of non-linear equations: the lessons of deep learning. arXiv preprint
arXiv:2003.00307v1,2020a.
ChaoyueLiu,LibinZhu,andMishaBelkin. Onthelinearityoflargenon-linearmodels: whenand
whythetangentkernelisconstant. AdvancesinNeuralInformationProcessingSystems,2020b.
MarinaMunkhoevaandIvanOseledets.Neuralharmonics: Bridgingspectralembeddingandmatrix
completion in self-supervised learning. In Thirty-seventh Conference on Neural Information
ProcessingSystems,2023.
Kento Nozawa and Issei Sato. Understanding negative samples in instance discriminative self-
supervisedrepresentationlearning. AdvancesinNeuralInformationProcessingSystems,2021.
AaronvandenOord,YazheLi,andOriolVinyals. Representationlearningwithcontrastivepredic-
tivecoding. arXivpreprintarXiv:1807.03748,2018.
AshwiniPokle,JinjinTian,YuchenLi,andAndrejRisteski.Contrastingthelandscapeofcontrastive
andnon-contrastivelearning. arXivpreprintarXiv:2203.15702,2022.
NikunjSaunshi,OrestisPlevrakis,SanjeevArora,MikhailKhodak,andHrishikeshKhandeparkar.
Atheoreticalanalysisofcontrastiveunsupervisedrepresentationlearning. InInternationalCon-
ferenceonMachineLearning.PMLR,2019.
BernhardScho¨lkopf,AlexanderSmola,andKlaus-RobertMu¨ller.Kernelprincipalcomponentanal-
ysis. InInternationalconferenceonartificialneuralnetworks.Springer,1997.
Anshul Shah, Suvrit Sra, Rama Chellappa, and Anoop Cherian. Max-margin contrastive learning.
InThirty-SixthAAAIConferenceonArtificialIntelligence,.AAAIPress,2022.
James B Simon, Maksis Knutins, Liu Ziyin, Daniel Geisz, Abraham J Fetterman, and Joshua Al-
brecht. On the stepwise nature of self-supervised learning. arXiv preprint arXiv:2303.15438,
2023.
Yuandong Tian. Understanding deep contrastive learning via coordinate-wise optimization. Ad-
vancesinNeuralInformationProcessingSystems,2022.
YuandongTian,XinleiChen,andSuryaGanguli. Understandingself-supervisedlearningdynamics
withoutcontrastivepairs. InInternationalConferenceonMachineLearning.PMLR,2021.
Christopher Tosh, Akshay Krishnamurthy, and Daniel Hsu. Contrastive estimation reveals topic
posteriorinformationtolinearmodels. TheJournalofMachineLearningResearch,2021.
Yao-Hung Hubert Tsai, Yue Wu, Ruslan Salakhutdinov, and Louis-Philippe Morency. Self-
supervisedlearningfromamulti-viewperspective. arXivpreprintarXiv:2006.05576,2020.
15ANILESSERGHOSHDASTIDAR
Feng Wang and Huaping Liu. Understanding the behaviour of contrastive loss. In Proceedings of
theIEEE/CVFconferenceoncomputervisionandpatternrecognition,2021.
Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen, Xiyang Dai, Mengchen Liu, Lu Yuan,
and Yu-Gang Jiang. Masked video distillation: Rethinking masked feature modeling for self-
supervisedvideorepresentationlearning. InProceedingsoftheIEEE/CVFConferenceonCom-
puterVisionandPatternRecognition,2023.
TongzhouWangandPhillipIsola. Understandingcontrastiverepresentationlearningthroughalign-
ment and uniformity on the hypersphere. In International Conference on Machine Learning.
PMLR,2020.
ColinWei,KendrickShen,YiningChen,andTengyuMa. Theoreticalanalysisofself-trainingwith
deepnetworksonunlabeleddata. arXivpreprintarXiv:2010.03622,2020.
ColinWei,SangMichaelXie,andTengyuMa. Whydopretrainedlanguagemodelshelpindown-
stream tasks? an analysis of head and prompt tuning. In Advances in Neural Information Pro-
cessingSystems,2021.
ZhiqiangXuandPingLi. Acomprehensivelytightanalysisofgradientdescentforpca. Advances
inNeuralInformationProcessingSystems,2021.
Yi Yu, Tengyao Wang, and Richard J Samworth. A useful variant of the davis–kahan theorem for
statisticians. Biometrika,2015.
JureZbontar,LiJing,IshanMisra,YannLeCun,andSte´phaneDeny. Barlowtwins: Self-supervised
learning via redundancy reduction. In International Conference on Machine Learning. PMLR,
2021.
ZhijianZhuo,YifeiWang,JinwenMa,andYisenWang. Towardsaunifiedtheoreticalunderstand-
ing of non-contrastive learning via rank differential mechanism. In The Eleventh International
ConferenceonLearningRepresentations,2023.
Liu Ziyin, Ekdeep Singh Lubana, Masahito Ueda, and Hidenori Tanaka. What shapes the loss
landscapeofself-supervisedlearning? arXivpreprintarXiv:2210.00638,2022.
16WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
Appendix
Intheappendixweprovidethefollowingadditionalmaterial:
A Proofsforresultsinthemainpaper.
B Additionalplotsandexperimentalresults.
AppendixA. Proofs
Contents
A.1 Lemma7[BoundonthenormoftheHessian] . . . . . . . . . . . . . . . . . . . . 17
A.2 Lemma8[BoundonthechangeinNTK] . . . . . . . . . . . . . . . . . . . . . . 18
A.3 Lemma9[ContrastivelearningdynamicsintermsofNTK] . . . . . . . . . . . . . 19
A.4 Proposition10[NTKunderdotproductdoesnotremainconstant] . . . . . . . . . 20
A.5 Lemma11[Boundonelement-wisechangeinweightsundercosinesimilarity] . . 24
A.6 Lemma12[Boundonweightdifferenceduringtrainingundercosinesimilarity] . 27
A.7 Theorem13[BoundonthechangeinNTKundercosinesimilarity] . . . . . . . . 29
A.8 Lemma16[ConstancyofC (t)] . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
V
A.9 Lemma17[Perturbationboundonrepresentation] . . . . . . . . . . . . . . . . . . 36
A.10 Proposition18 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
A.1. Lemma7[BoundonthenormoftheHessian]
Under Assumptions 3, 5, 6, consider the neural network defined in (2). If the change in weights
duringtrainingisboundedas
∥W(t)−W(0)∥ +∥V(t)−V(0)∥ ≤ R,
F F
then,∀ z ∈ [Z],withα = 4β c2 L andα = 4L c (1+β c s c ),thezth Hessianisbounded
1 ϕ in ϕ 2 ϕ in ϕ in 0 s
(cid:13) (cid:13)
as: (cid:13)H(z)(x;θ(t))(cid:13) ≤ α1√R+α2.
(cid:13) (cid:13) 2 M
Proof Notethatthezth entryoftheembeddingf(·)canbewrittenas
1
f (x;θ) = √ (W⊤) ϕ(Vx).
z z.
M
Therefore, eachf (x;θ)canbethoughofasasingle-dimensionaloutputofaneuralnetworkwith
z
weights{V,W }. Further,
z.
∥V(t)−V(0)∥ +∥W(t)−W(0)∥ ≤ R
F F
(cid:13) (cid:13)
=⇒ ∥V(t)−V(0)∥ +(cid:13)W⊤(t)−W⊤(0)(cid:13) ≤ R
F (cid:13) z. z. (cid:13)
F
since
(cid:13) (cid:13)W⊤(t)−W⊤(0)(cid:13)
(cid:13) ≤ ∥W(t)−W(0)∥ . Let the Hessian corresponding to this neural
z. z. F F
networkbedenotedasH˜ (z) (x;θ(t)). NotethatH˜ (z) (x;θ(t))satisfiesallconditionsrequired(lin-
earfinallayerandGaussianinitialization)byTheorem7.1ofLiuetal.(2020a), andhencewecan
(cid:13) (cid:13) (cid:13) (cid:13)
use this theorem to bound (cid:13)H˜ (z) (x;θ(t))(cid:13) . To connect this to (cid:13)H(z)(x;θ(t))(cid:13) , note that after
(cid:13) (cid:13) (cid:13) (cid:13)
2 2
17ANILESSERGHOSHDASTIDAR
appropriatepermutationsofrowsandcolumns(notethatspectralnormisinvarianttopermutations),
wecanwrite:
(cid:34) (cid:35)
H(z)(x;θ(t)) = H˜ (z) (x;θ(t)) 0
0 0
(cid:13) (cid:13) (cid:13) (cid:13)
and therefore (cid:13)H(z)(x;θ(t))(cid:13) = (cid:13)H˜ (z) (x;θ(t))(cid:13) . Hence, we can apply Theorem 7.1 of Liu
(cid:13) (cid:13) (cid:13) (cid:13)
2 2
(cid:13) (cid:13)
et al. (2020a) to (cid:13)H(z)(x;θ(t))(cid:13) . Now, from the proof of Theorem 7.1 in Liu et al. (2020a), we
(cid:13) (cid:13)
2
have:
(cid:13) (cid:13) L2C′(R)
(cid:13)H(z)(x;θ(t))(cid:13) ≤ √
(cid:13) (cid:13)
2 M
whereListhenumberoflayersintheneuralnetworkand
L−1
C′(R) = β c2 (cid:88) L2l′−1(c +R)2l′−2C(l′) (R)+LL−1(c +R)L−2c
ϕ in ϕ 0 b ϕ 0 in
l′=1
ForL = 2,whichisthecaseweareinterestedin,thisreducesto:
C′(R) = β c2 L C(1) (R)+L c
ϕ in ϕ b ϕ in
(1)
Inoursetup,wehavec = c ,wherec isdefinedinAssumption6andC (R) = s c +R,where
0 s s b 0 0
s isdefinedinAssumption6. SoforeachHessianmatrix,wehave:
0
(cid:13) (cid:13) α R+α
(cid:13)H(z)(x;θ(t))(cid:13) ≤ 1 √ 2 (11)
(cid:13) (cid:13)
2 M
where α = 4β c2 L and α = 4L c (1+β c s c ). Note that in general, for L layers, the
1 ϕ in ϕ 2 ϕ in ϕ in 0 s
spectralnormhasaboundoftheform(cid:13) (cid:13)H(z)(x;θ(t))(cid:13)
(cid:13) ≤
O(√R3L
). However,forthespecialcase
2 M
ofL = 2,itispossibletofurtherreduceittotheformin(11).
A.2. Lemma8[BoundonthechangeinNTK]
DefineS := {s ∈ Rp;∥s−s(0)∥ ≤ R},wherepisthetotalnumberoflearnableparametersin(2).
(cid:13) (cid:13)
Assumethatforanyinputx,(cid:13)H(z)(x;s)(cid:13) ≤ ϵand∥∇ f (x;s)∥ ≤ c ,∀ z ∈ [Z]and∀ s ∈ S.
(cid:13) (cid:13) s z 2 0
2
Then,foranyinputsx,x˜,∀ s ∈ Sand∀ i,j ∈ [Z],|K (x,x˜;s)−K (x,x˜;s(0))| ≤ 2ϵc R.
ij ij 0
Proof We start by writing out and expanding the difference in the (i,j)th kernel entry when there
isachangeinsfroms(0)tos:
|K (x,y;s)−K (x,y;s(0))|
ij ij
(cid:12) (cid:12)
(cid:12)∂f (x,s)⊤∂f (y,s) ∂f (x,s(0))⊤∂f (y,s(0))(cid:12)
= (cid:12) i j − i j (cid:12)
(cid:12) ∂s ∂s ∂s(0) ∂s(0) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)(cid:18)
∂f (x,s) ∂f
(x,s(0))(cid:19)⊤
∂f (y,s) ∂f
(x,s(0))⊤(cid:18)
∂f (y,s) ∂f
(y,s(0))(cid:19)(cid:12)
= (cid:12) i − i j + i j − j (cid:12)
(cid:12) ∂s ∂s(0) ∂s ∂s(0) ∂s ∂s(0) (cid:12)
(cid:12) (cid:12)
(cid:13) (cid:13)(cid:13) (cid:13) (cid:13) (cid:13)(cid:13) (cid:13)
(cid:13)∂f i(x,s) ∂f i(x,s(0))(cid:13)(cid:13)∂f j(y,s)(cid:13) (cid:13)∂f i(x,s(0))(cid:13)(cid:13)∂f j(y,s) ∂f j(y,s(0))(cid:13)
≤ (cid:13) − (cid:13)(cid:13) (cid:13)+(cid:13) (cid:13)(cid:13) − (cid:13)
(cid:13) ∂s ∂s(0) (cid:13)(cid:13) ∂s (cid:13) (cid:13) ∂s(0) (cid:13)(cid:13) ∂s ∂s(0) (cid:13)
18WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
wherewehaveusedTriangleandCauchy-Schwartzinequalities. Usingthefactthat∥∇ f (x;s)∥,
s z
∥∇ f (y;s)∥ ≤ c , ∀ s ∈ S,z ∈ [Z] (from the lemma assumptions), we can simplify the
s z 0
expressionto:
|K (x,y;s)−K (x,y;s(0))|
ij ij
(cid:18)(cid:13) (cid:13) (cid:13) (cid:13)(cid:19)
(cid:13)∂f i(x,s) ∂f i(x,s(0))(cid:13) (cid:13)∂f j(y,s) ∂f j(y,s(0))(cid:13)
≤ c 0 (cid:13) − (cid:13)+(cid:13) − (cid:13) .
(cid:13) ∂s ∂s(0) (cid:13) (cid:13) ∂s ∂s(0) (cid:13)
From Proposition 2.3 of Liu et al. (2020b), we know that the remaining terms can be bounded in
termsoftheHessianandRasfollows:
(cid:13) (cid:13)
(cid:13)∂f i(x,s) ∂f i(x,s(0))(cid:13)
(cid:13) − (cid:13) ≤ max∥H i(x;s)∥R,
(cid:13) ∂s ∂s(0) (cid:13) s∈S
(cid:13) (cid:13)
(cid:13)∂f j(y,s) ∂f j(y,s(0))(cid:13)
(cid:13) − (cid:13) ≤ max∥H j(y;s)∥R.
(cid:13) ∂s ∂s(0) (cid:13) s∈S
Combiningtheaboveresults,wecanboundthekerneldifferenceas:
|K (x,y;s)−K (x,y;s(0))| ≤ 2ϵc R
ij ij 0
whichconcludestheproof.
A.3. Lemma9[ContrastivelearningdynamicsintermsofNTK]
Consider training a neural network of the form (2) using a loss l(·) of the form (1) under gradient
flow on dataset D. Let g (x,x˜;θ(t)) := ∂s(x,x˜) . Then, for z ∈ [Z], the representation of an
i ∂fi(x;θ(t))
arbitraryinputx˜ evolvesas:
(cid:34) Z
∂f z(x˜;θ(t)) 1 (cid:88) ∂l(·) (cid:88)
= − [K (x˜,x ;θ(t))g (x ,x ;θ(t))
zi n i n n,q
∂t N ∂s(x ,x )
n n,q
n,q i=1
(cid:3)
+K (x˜,x ;θ(t))g (x ,x ;θ(t))] .
zi n,q i n,q n
Proof We are interested in computing the dynamics of f (x˜;θ). Note that the dynamics can be
z
expressedas
(cid:18)
df
(x˜;θ)(cid:19)⊤
dθ
f˙ (x˜) = z . (12)
z
dθ dt
Sinceweareoptimizingusinggradientflow, dθ = −∇ L. Wenowcompute∇ Lasfollows:
dt θ θ
∇ L = 1 (cid:88) ∇ l(cid:16) {s(x ,x )}Q (cid:17)
θ N θ n n,q q=1
n
1 (cid:88) ∂l
= ∇ s(x ,x )
θ n n,q
N ∂s(x ,x )
n n,q
n,q
(cid:34) Z (cid:20) (cid:21)(cid:35)
1 (cid:88) ∂l (cid:88) ∂s(x n,x n,q) ∂f i(x n;θ) ∂s(x n,x n,q) ∂f i(x n,q;θ)
= · + · .
N ∂s(x ,x ) ∂f (x ;θ) ∂θ ∂f (x ;θ) ∂θ
n n,q i n i n,q
n,q i=1
19ANILESSERGHOSHDASTIDAR
Thereforeusing∇ Lin(12)weobtainthedynamicsas
θ
f˙ (x˜) =
−1 (cid:88) ∂l
(cid:34) (cid:88)Z (cid:18)
df
z(x˜;θ)(cid:19)⊤(cid:20)
∂s(x n,x n,q)
·
∂f i(x n;θ)
z
N ∂s(x ,x ) dθ ∂f (x ;θ) ∂θ
n n,q i n
n,q i=1
(cid:21)(cid:21)
∂s(x ,x ) ∂f (x ;θ)
n n,q i n,q
+ ·
∂f (x ;θ) ∂θ
i n,q
(cid:34) Z (cid:20)
−1 (cid:88) ∂l (cid:88) ∂s(x n,x n,q)
= ·K (x˜,x ;θ)
zi n
N ∂s(x ,x ) ∂f (x ;θ)
n n,q i n
n,q i=1
(cid:21)(cid:21)
∂s(x ,x )
n n,q
+ ·K (x˜,x ;θ)
zi n,q
∂f (x ;θ)
i n,q
which,aftersubstitutingg (x,x˜;θ(t)) := ∂s(x,x˜) ,providestheexpressioninLemma9andthus
i ∂fi(x;θ(t))
concludestheproof.
A.4. Proposition10[NTKunderdotproductdoesnotremainconstant]
For D = Z = 1, linear loss (l(a) := a), dot product similarity and triplet setting (D = {x ,x+,
n n
x−}N ) in (1), the optimization is: min 1 (cid:80)N f(x ;θ)(f(x−;θ)−f(x+;θ)). Consider a
n n=1 θ N n=1 n n n
network (2) with linear activation (ϕ(a) := a), weights initialised as independent N(0,1), and
trainedviagradientflow.
There is a dataset such that, with probability at least 1− √25 , for a time step t˜∈ (0,logM) and
M
anyinputpairx,x˜withxx˜ ̸= 0,theNTKsatisfies|K(x,x˜;θ(t))−K(x,x˜;θ(0))| → ∞ast → t˜.
Proof Letusfirstrecallthedefinitionofalinearneuralnetworkwithonedimensionalinputx ∈ R
andonedimensionaloutput(Z = 1):
M
1 (cid:88)
f(x;θ) = √ w v x.
m m
M
m=1
Pluggingthisintothelossfunction,weobtain:
N
L =
1 (cid:88)
f(x
;θ)(cid:0) f(x−;θ)−f(x+;θ)(cid:1)
N n n n
n=1
N M (cid:32) M M (cid:33)
1 (cid:88) 1 (cid:88) 1 (cid:88) 1 (cid:88)
= √ w v x √ w v x−− √ w v x+
N M m m n M m m n M m m n
n=1 m=1 m=1 m=1
(cid:32) M (cid:33)2(cid:32) N (cid:33)
−1 (cid:88) 1 (cid:88)
= w v x (x+−x−)
M m m N n n n
m=1 n=1
(cid:32) M (cid:33)2
−C (cid:88)
= w v
m m
M
m=1
whereC = 1 (cid:80)N x (x+−x−)isadatadependentconstant. Recallthatweaimtofindasetting
N n=1 n n n
underwhichtheNTKdoesnotremainconstantandassuchitissufficienttoconstructanexample
20WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
wherethisisthecase. WethereforechooseadatasetsuchthatC > 0(wedonothavetoexplicitly
construct this dataset for the proof, we just need the existence of such a dataset). Let us define the
followingforeaseofnotation:
(cid:32) M (cid:33)
1 (cid:88)
S = w v ,
m m
M
m=1
(cid:32) M (cid:33)
1 (cid:88)
P = (v2 +w2 ) .
M m m
m=1
WestartbynotingthatwecanwritetheNTKas:
(cid:32) M (cid:33)
1 (cid:88)
K(x,y;θ(t)) = (v2 +w2 ) xy
M m m
m=1
= P(t)xy.
Thereforeasxy isafixedconstant,tocomputethechangeinNTK,itissufficienttogetanexpres-
sionforthechangeofP withrespecttot,whichwecanwriteas:
(cid:32) M (cid:18) (cid:19)(cid:33)
dP 2 (cid:88) dv m dw m
= v +w , (13)
m m
dt M dt dt
m=1
Now,wehave,fromthegradientflowassumption:
(cid:32) M (cid:33)
dv m dL 2C (cid:88)
= − = w v w = 2CSw , (14)
m′ m′ m m
dt dv M
m
m′=1
(cid:32) M (cid:33)
dw m dL 2C (cid:88)
= − = w v v = 2CSv . (15)
m′ m′ m m
dt dw M
m
m′=1
Using(14)and(15)in(13),wecanfurtherrewrite dP intermsofS:
dt
(cid:32) M (cid:33)(cid:34) M (cid:35)
dP 2C (cid:88) 2 (cid:88)
= w v (v w +w v )
m′ m′ m m m m
dt M M
m′=1 m=1
= 8CS2. (16)
NowcomputingthechangeofS w.r.t. t,weobtain:
(cid:32) M (cid:18) (cid:19)(cid:33)
dS 1 (cid:88) dv m dw m
= w +v
m m
dt M dt dt
m=1
(cid:32) M (cid:33)(cid:34) M (cid:35)
2C (cid:88) 1 (cid:88)
= w v (v2 +w2 )
M m′ m′ M m m
m′=1 m=1
= 2CSP (17)
21ANILESSERGHOSHDASTIDAR
From(16)and(17),wehave:
dP 4S
=
dS P
P2(t)
=⇒ S2(t) = −α (18)
4
whereα = P2(0) −S2(0). Also,notethat:
4
P2(t)−4α ≥ 0 (19)
foralltsinceS2(t) ≥ 0. From(16)and(18),wehave:
dP
= 2C(P2−4α)
dt
dP
= 2Cdt (20)
P2−4α
Now,
4α = P2−4S2
 (cid:32) M (cid:33)2 (cid:32) M (cid:33)2
1 (cid:88) (cid:88)
= M2  (v2 m+w2 m) −4 v mw m 
m=1 m=1
(cid:34)(cid:32) M (cid:33)(cid:32) M (cid:33)(cid:35)
1 (cid:88) (cid:88)
= (v2 +w2 +2v w ) (v2 +w2 −2v w )
M2 m m m m m m m m
m=1 m=1
(cid:34)(cid:32) M (cid:33)(cid:32) M (cid:33)(cid:35)
1 (cid:88) (cid:88)
= (v +w )2 (v −w )2
M2 m m m m
m=1 m=1
≥ 0
Therefore, dP is of the form dx . (20) can then be integrated using partial fractions method
P2−4α x2−a2
toobtain:
√
(cid:12) (cid:12)
(cid:12) (cid:12)P −2 √α(cid:12) (cid:12) = βe8C√ αt
(cid:12)P +2 α(cid:12)
(cid:12) √ (cid:12) √
where β =
(cid:12)P(0)−2 √α(cid:12).
Now, using (19) and the fact that P,4α ≥ 0, we have P − 2 α ≥ 0.
(cid:12)P(0)+2 α(cid:12)
Therefore,wehave:
√
P −2 α √
√ = βe8C αt
P +2 α
(cid:32) √ (cid:33)
√ 1+βe8C αt
=⇒ P(t) = 2 α √
1−βe8C αt
Notethatsinceeachv ,w isinitializedasN(0,1),w⊤v ̸= 0withprobability1atinitialization.
m m √
Hence, S2(0) > 0 with probability 1. Therefore, P2(0)−4α > 0 and hence P(0)−2 α > 0.
22WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
√
Consequently, β > 0 with probability 1. Let t˜indicate the time t at which 1 − βe8C αt = 0.
Then, t˜ = 1√ log 1. Since 0 < β < 1, t˜ > 0. Clearly, as t → t˜, P(t) → ∞. Since
8C α β
|K(x,y;θ(t))−K(x,y;θ(0))| = |(P(t)−P(0))xy|,
|K(x,y;θ(t))−K(x,y;θ(0))| → ∞
ifxy ̸= 0.
Inthefinalpartoftheproof,weshowthatt˜∈ (0,logM)withprobabilityatleast1−√25 . Wefirst
M
compute a few moments for the random variables S(0) and P(0). Due to standard normal initial-
izationoftheweights,onecancomputethefollowing: E[P(0)] = 2,Var(P(0)) = 4 ,E[S(0)] =
M
0,E[S(0)2] = Var(S(0)) = 1 . andVar(S(0)2) = 2 + 6 . ByapplyingChebyshev’sinequal-
M M2 M3
ity,itimmediatelyfollowsthat
(cid:18) (cid:19) (cid:18) (cid:19)
1 16 1 4
P |P(0)−2| > ≤ 4·Var(P(0)) = ; P |S(0)| > ≤ 4·Var(S(0)) = .
2 M 2 M
We further need to show that S(0)2 > 1 with high probability, but Chebyshev’s inequality does
M2
notsufficetoprovethis(asVar(S(0)2)istoolarge). Tothisend,weuseBerry-Esseentheorem(see,
for instance, Korolev and Shevtsova, 2010, Corollary 1): Let Z ,...,Z be i.i.d. variables with
1 M
E[Z ] = 0,E[Z2] = σ2 andE[|Z |3] ≤ ρ < ∞. Thedeviationbetweencumulativedistribution,
m m m
F Y(·),ofY = σ√1
M
(cid:80) mZ
m
andstandardnormaldistribution,Φ(·),isboundedas
0.52ρ
sup|F (x)−Φ(x)| < √ .
Y
x σ3 M
In the present context, Z = w v satisfies E[Z ] = 0,E[Z2] = E[w2 ]E[v2 ] = 1 and
m m m m m m m
E[|Z |3] = (E[|w |3])2 ≤ E[w2 ]E[w4 ] = 3 (Cauchy-Schwartz inequality). Further, Y =
√ m m m m
MS(0). Hence,wecanbound:
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
1 1 1 −1
P S(0)2 ≤ = P |Y| ≤ √ = F √ −F √
M2 M Y M Y M
(cid:18) (cid:19) (cid:18) (cid:19)
1 1
≤ Φ √ −Φ −√ +2sup|F (x)−Φ(x)|
Y
M M x
2 1 1.56 5
< √ · √ + 2· √ < √ .
M 2π M M
Combiningallprobabilitybounds,wehavethat,withprobabilityatleast1− 20 − √5 ≥ 1− √25 ,
M M M
it holds that P(0) ∈ [1.5,2.5],S(0) ∈ [−0.5,0.5] and S(0)2 > 1 . Under this condition, it
M2
immediatelyfollowsthatα = P(0)2 −S(0)2 isboundedas 1 < α < 2,while
4 4
√
P(0)−2 α 4S(0)2 4S(0)2 1
β = √ = √ > > .
P(0)+2 α (P(0)+2 α)2 36 9M2
Using the lower bounds of α,β in the expression for t˜, it follows that t˜< 1 log(3M) with prob-
2C
ability at least 1− √25 . One can choose the dataset such that C > 0 is large enough to make the
M
upperboundatmostlogM.
23ANILESSERGHOSHDASTIDAR
A.5. Lemma11[Boundonelement-wisechangeinweightsundercosinesimilarity]
Under Assumptions 1 - 5, consider losses of the form (1) where cosine similarity is used. If a
neural network f(·) as defined in (2) is trained using gradient descent with learning rate η, at any
time t, the change in weights are boun √ded as: |∆V ij(t)| ≤ √β M1 ∥W(t)∥
max
and |∆W ij(t)| ≤
√β M2 ∥V(t)∥
max
whereβ
1
= 4 δc lc inQ ZL
ϕ
andβ
2
= 4 δc lc inQDL
ϕ
areconstantsindependentof
M.
Proof Wefirstobservethatthecosinesimilarityforaneuralnetworkf(·)asdefinedin(2)canbe
writtenoutas
f(x )⊤f(x )
k k,q
s(x ,x ) =
k k,q
(∥f(x)∥+δ)(∥f(y)∥+δ)
ϕ⊤(Vx )WW⊤ϕ(Vx )
k k,q
= √ √ .
((cid:13) (cid:13)W⊤ϕ(Vx k)(cid:13) (cid:13)+ Mδ)((cid:13) (cid:13)W⊤ϕ(Vx k,q)(cid:13) (cid:13)+ Mδ)
ToproveLemma11,wesplititintotwoparts. Wefirstanalyze ∂L andthenanalyze ∂L .
∂V ∂W
ANALYSIS FOR ∂L
∂V
Tobound ∂L,wefirstnotethat:
∂V
(cid:12) (cid:12) N Q (cid:12) (cid:12)(cid:12) (cid:12)
(cid:12) ∂L (cid:12) 1 (cid:88)(cid:88)(cid:12) ∂l (cid:12)(cid:12)∂s(x k,x k,q)(cid:12)
(cid:12) (cid:12) ≤ (cid:12) (cid:12)(cid:12) (cid:12)
(cid:12)∂V (cid:12) N (cid:12)∂s(x ,x )(cid:12)(cid:12) ∂V (cid:12)
ij k k,q ij
k=1q=1
(cid:12) (cid:12)
And note that (cid:12) ∂l (cid:12) is bound by c by Assumption 2. Therefore what remains is to bound
(cid:12)∂s(x ,x )(cid:12) l
(cid:12) k k,q (cid:12) √
the second term, (cid:12)∂s(x k,x k,q)(cid:12). To do so, let us first note that W⊤ϕ(Vx ) = Mf(x ;θ) and
(cid:12) √ ∂Vij (cid:12) k k
W⊤ϕ(Vx ) = Mf(x ;θ). Wedenotef(x ;θ)andf(x ;θ)byu andu respectively.
k,q k,q k k,q k k,q
Writingout ∂l ,weobtain:
∂s(x ,x )
k k,q
∂s(x ,x )
(cid:18)
∂s(x ,x
)(cid:19)⊤
∂ϕ(Vx )
(cid:18)
∂s(x ,x
)(cid:19)⊤
∂ϕ(Vx )
k k,q k k,q k k k,q k,q
= + (21)
∂V ∂ϕ(Vx ) ∂V ∂ϕ(Vx ) ∂V
ij k ij k,q ij
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
TermI TermII
Observe that Term I and Term II only differ in considering x and x . Therefore, we show the
n k,q
derivationforTermI inthefollowing-TermII followthesamestructure.
∂s(x ,x )
TocomputeTermI,westartwithcomputingtheterm k k,q andobtain
∂ϕ(Vx )
k
∂s(x ,x ) 1 (∥u ∥+δ)(∥u ∥+δ)∥u ∥Wu −u⊤u (∥u ∥+δ)Wu
k k,q = √ k k,q k k,q k k,q k,q k .
∂ϕ(Vx k) M (∥u k∥+δ)2(∥u k,q∥+δ)2∥u k∥
∂ϕ(Vx )
Itiseasytoseethatthevector k canbewrittenas:
∂Vij
(cid:20) (cid:21)
∂ϕ(Vx )
k = 1 ·ϕ′(V x )(x )
r=i i. k k j
∂V
ij r
24WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
andthereforewecanwriteTermI as
(cid:18)
∂s(x ,x
)(cid:19)⊤
∂ϕ(Vx )
k k,q k
∂ϕ(Vx ) ∂V
k ij
(cid:20) (cid:21)
∂s(x ,x )
= k k,q ·ϕ′(V x )(x )
i. k k j
∂ϕ(Vx )
k,q i
1 (∥u ∥+δ)(∥u ∥+δ)∥u ∥W u −u⊤u (∥u ∥+δ)W u
= √ k k,q k i. k,q k k,q k,q i. k ·ϕ′(V x )(x ) .
M (∥u k∥+δ)2(∥u k,q∥+δ)2∥u k∥ i. k k j
Fromtriangleinequality,wecanboundtheabsolutevalueofTermI as:
(cid:12) (cid:12)
(cid:12)(cid:18) ∂s(x ,x )(cid:19)⊤ ∂ϕ(Vx )(cid:12)
(cid:12) k k,q k (cid:12)
(cid:12) ∂ϕ(Vx ) ∂V (cid:12)
(cid:12) k ij (cid:12)
≤
√1 (∥u k∥+δ)(∥u k,q∥+δ)∥u k∥(cid:12) (cid:12)Wi.u k,q(cid:12) (cid:12)+(cid:12) (cid:12)u⊤ ku k,q(cid:12) (cid:12)(∥u k,q∥+δ)|W i.u k|
M (∥u k∥+δ)2(∥u k,q∥+δ)2∥u k∥
·(cid:12) (cid:12)ϕ′(V i.x k)(x k) j(cid:12) (cid:12).
FromCauchy-Schwartzinequality,weknowthatu⊤u ≤ ∥u ∥∥u ∥,|W u | ≤ ∥W ∥∥u ∥
k k,q k k,q i. k i. k
and|W u | ≤ ∥W ∥∥u ∥. Theexpressionthencanbesimplifiedas:
i. k,q i. k,q
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:18) ∂ ∂s( ϕx (k V,x xk k, )q)(cid:19)⊤ ∂ϕ ∂( VV ix jk)(cid:12) (cid:12)
(cid:12)
(cid:12)
≤ √1
M
(2 (∥∥ uu kk ∥∥ ++ δδ )) 2∥ (∥u uk k,q ,∥ q∥∥ +W δi ).∥ ·(cid:12) (cid:12)ϕ′(V i.x k)(x k) j(cid:12) (cid:12)
≤ √1 2∥W i.∥ ·(cid:12) (cid:12)ϕ′(V i.x k)(x k) j(cid:12) (cid:12)
M ∥u k∥+δ
≤ √1 2∥W i.∥ ·(cid:12) (cid:12)ϕ′(V i.x k)(x k) j(cid:12) (cid:12)
M δ
sinceδ > 0.
BythesameargumentwecanboundTermII as:
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:12)(cid:18) ∂ ∂s ϕ( (x Vk, xx kk ,q,q ))(cid:19)⊤ ∂ϕ( ∂V Vx ijk,q)(cid:12) (cid:12)
(cid:12) (cid:12)
≤ √1
M
2∥W
δ
i.∥ ·(cid:12) (cid:12)ϕ′(V i.x k,q)(x k) j(cid:12) (cid:12).
CombiningtheboundforTermI andTermII in(21),weobtain:
(cid:12) (cid:12)
(cid:12) (cid:12)∂s(x k,x k,q)(cid:12) (cid:12) ≤ √1 2∥W i.∥ (cid:0)(cid:12) (cid:12)ϕ′(V i.x k)(x k) j(cid:12) (cid:12)+(cid:12) (cid:12)ϕ′(V i.x k,q)(x k) j(cid:12) (cid:12)(cid:1) .
(cid:12) ∂V ij (cid:12) M δ
(cid:12) (cid:12)(cid:12) (cid:12)
Let k′ and q′ denote the values of k and q respectively at which (cid:12) ∂l (cid:12)(cid:12)∂s(x k,x k,q)(cid:12) is maxi-
(cid:12)∂s(x k,x k,q)(cid:12)(cid:12) ∂Vij (cid:12)
mum. Thenwehave:
(cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12)
(cid:12) ∂L (cid:12) (cid:12) ∂l (cid:12)(cid:12)∂s(x k′,x k′,q′)(cid:12)
(cid:12) (cid:12) ≤ Q(cid:12) (cid:12)(cid:12) (cid:12)
(cid:12)∂V ij(cid:12) (cid:12)∂s(x k′,x k′,q′)(cid:12)(cid:12) ∂V
ij
(cid:12)
25ANILESSERGHOSHDASTIDAR
Therefore:
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) ∂L (cid:12) (cid:12) ≤ √2Q (cid:12) (cid:12) ∂l (cid:12) (cid:12) ∥W i.∥ (cid:0)(cid:12) (cid:12)ϕ′(V i.x k)(x k) j(cid:12) (cid:12)+(cid:12) (cid:12)ϕ′(V i.x k,q)(x k,q) j(cid:12) (cid:12)(cid:1) (22)
(cid:12)∂V ij(cid:12) M (cid:12)∂s(x k′,x k′,q′)(cid:12) δ
Undergradientdescentwithlearningrateη,wehave:
(cid:12) (cid:12)
(cid:12) ∂L (cid:12)
|∆V ij| = η(cid:12) (cid:12)
(cid:12)∂V (cid:12)
ij
√
4Q Zc L c
l ϕ in
≤ √ max{|W |}
ij
Mδ ij
(cid:12) (cid:12)
wherewehaveusedAssumption2tobound(cid:12) ∂l (cid:12) ≤ c ,3tobound|(x ) |,|(x ) | ≤ c
(cid:12)∂s(x k′,x k′,q′)(cid:12) l √ k j k,q j in
and 5 to bound |ϕ′(V x )|,|ϕ′(V x )|. Setting β = 4Q Zc L c , we have the required
i. k i. k,q 1 δ l ϕ in
result.
ANALYSIS FOR ∂L
∂W
Theargumentfollowstheonepresentedfor ∂L closely. Notethat:
∂V
Tr(cid:0) ϕ⊤(Vx )WW⊤ϕ(Vx )(cid:1)
k k,q
s(x ,x ) = √ √
k k,q ((cid:13) (cid:13)W⊤ϕ(Vx k)(cid:13) (cid:13)+ Mδ)((cid:13) (cid:13)W⊤ϕ(Vx k,q)(cid:13) (cid:13)+ Mδ)
∂s(x ,x )
Again,wecanstartbycomputing k k,q as:
∂W
(cid:34) (cid:35)
∂s(x k,x k,q) 1 ϕ(Vx k)u⊤
k,q
+ϕ(Vx k,q)u⊤
k
= √
∂W M (∥u k∥+δ)(∥u k,q∥+δ)
1 (cid:20) u⊤u
− √ k k,q
M (∥u k∥+δ)2(∥u k,q∥+δ)2
(cid:32) (cid:33)(cid:35)
(∥u k,q∥+δ)ϕ(Vx k)u⊤
k +
(∥u k∥+δ)ϕ(Vx k,q)u⊤
k,q
∥u ∥ ∥u ∥
k k,q
Andtherefore,
∂s(x k,x k,q) 1
(cid:34) ϕi(Vx k)uj
k,q
+ϕi(Vx k,q)uj k(cid:35)
= √
∂W ij M (∥u k∥+δ)(∥u k,q∥+δ)
1 (cid:20) u⊤u
− √ k k,q
M (∥u k∥+δ)2(∥u k,q∥+δ)2
(cid:18) (cid:19)(cid:21)
(∥u ∥+δ)ϕ (Vx )(u ) (∥u ∥+δ)ϕ (Vx )(u )
k,q i k k j k i k,q k,q j
+
∥u ∥ ∥u ∥
k k,q
Using triangle and Cauchy-Schwartz inequalities, and the fact that δ > 0, the expression can be
simplifiedinto:
(cid:20) (cid:21)
∂s(x ,x ) 1 |ϕ (Vx )|+|ϕ (Vx )| 1
k k,q i k i k,q
≤ √ + (|ϕ (Vx )|+|ϕ (Vx )|)
i k i k,q
∂W ij M δ δ
26WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
Now,
(cid:12) (cid:12) N Q (cid:12) (cid:12)(cid:12) (cid:12)
(cid:12) ∂L (cid:12) 1 (cid:88)(cid:88)(cid:12) ∂l (cid:12)(cid:12)∂s(x k,x k,q)(cid:12)
(cid:12) (cid:12) ≤ (cid:12) (cid:12)(cid:12) (cid:12)
(cid:12)∂W (cid:12) N (cid:12)∂s(x ,x )(cid:12)(cid:12) ∂W (cid:12)
ij k k,q ij
k=1q=1
(cid:12) (cid:12)(cid:12) (cid:12)
Letk′′ andq′′ denotethevaluesofk andq respectivelyatwhich(cid:12) ∂l (cid:12)(cid:12)∂s(x k,x k,q)(cid:12)ismaxi-
(cid:12)∂s(x k,x kq)(cid:12)(cid:12) ∂Wij (cid:12)
mum. Thenwehave:
(cid:12) (cid:12) (cid:12) (cid:12)(cid:20) (cid:21)
(cid:12) ∂L (cid:12) 2Q (cid:12) ∂l (cid:12) |ϕ i(Vx k)|+|ϕ i(Vx k,q)|
(cid:12) (cid:12) ≤ √ (cid:12) (cid:12)
(cid:12)∂W ij(cid:12) M (cid:12)∂s(x k′′,x k′′q′′)(cid:12) δ
Therefore,undergradientdescent,
(cid:12) (cid:12)
(cid:12) ∂L (cid:12)
|∆W ij| = η(cid:12) (cid:12)
(cid:12)∂W (cid:12)
ij
4QDc c L
l in ϕ
≤ √ max{|V |}
ij
Mδ ij
wherewehaveagainusedAssumptions2,3and5. Settingβ = 4QDc c L ,wehavetherequired
2 δ l in ϕ
result.
A.6. Lemma12[Boundonweightdifferenceduringtrainingundercosinesimilarity]
(cid:16) (cid:17)t
Define β := max{β 1,β 2} and c(t) := c(0) 1+ √β where c(0) = c θlogM. Then, for any t,
M
wehave: ∥V(t)−V(0)∥ ≤ c(t)−c(0)and∥W(t)−W(0)∥ ≤ c(t)−c(0).
max max
Proof Weprovetheabovestatementbyinduction.
BaseCase. Notethatatt = 0,theseholdtrivially:
max{|V (0)−V (0)|} = 0
ij ij
ij
max{|W (0)−W (0)|} = 0
ij ij
ij
InductionStep. Letusassumethatattimet,wehave:
max{|V (t)−V (0)|} ≤ c(t)−c(0) (23)
ij ij
ij
max{|W (t)−W (0)|} ≤ c(t)−c(0) (24)
ij ij
ij
Then,att+1,wehave:
max{|V (t+1)−V (0)|} = max{|V (t+1)−V (t)+V (t)−V (0)|}
ij ij ij ij ij ij
ij ij
= max{|∆V (t)+V (t)−V (0)|}
ij ij ij
ij
≤ max{|∆V (t)|+|V (t)−V (0)|}
ij ij ij
ij
27ANILESSERGHOSHDASTIDAR
where we have used the definition of ∆V (t) (the change of V in one time-step at time t), tri-
ij ij
angleinequalityandthefactthatinequalitiesarepreservedundermaxoperation. Further,applying
triangleinequalityforthemaxoperation,wehave:
max{|V (t+1)−V (0)|} ≤ max{|∆V (t)|}+max{|V (t)−V (0)|}
ij ij ij ij ij
ij ij ij
β
1
≤ √ max{|W (t)|}+c(t)−c(0), (25)
ij
M ij
where we use |∆V ij(t)| ≤ √β M1 ∥W(t)∥
max
from Lemma 11 and (23) to obtain (25). Now by
(24),wehave:
max{|W (t)|−|W (0)|} ≤ c(t)−c(0)
ij ij
ij
(cid:12) (cid:12)
(cid:12) (cid:12)
=⇒ (cid:12)max{|W ij(t)|}−max{|W ij(0)|}(cid:12) ≤ c(t)−c(0)
(cid:12) ij ij (cid:12)
CASE 1: max ij{|W ij(t)|} < max ij{|W ij(0)|}
Westartbymultiplying √β1 tobothsidesoftheinequality:
M
β β
1 1
√ max{|W (t)|} < √ max{|W (0)|}
ij ij
M ij M ij
andthenusing(25),weobtain:
β
1
max{|V (t+1)−V (0)|} ≤ √ max{|W (0)|}+c(t)−c(0)
ij ij ij
ij M ij
Notethatmax {|W (0)|} ≤ c(0)andc(t) ≥ c(0)bydefinition. Therefore,wehave:
ij ij
β
max{|V (t+1)−V (0)|} ≤ √ c(t)+c(t)−c(0)
ij ij
ij M
(cid:18) (cid:19)
β
= 1+ √ c(t)−c(0)
M
(cid:16) (cid:17)
Nowfromthedefinitionofc(t),weknowthat 1+ √β c(t) = c(t+1)andtherefore,
m
max{|V (t+1)−V (0)|} ≤ c(t+1)−c(0)
ij ij
ij
CASE 2: max ij{|W ij(t)|} ≥ max ij{|W ij(0)|}
Wehave:
max{|W (t)|}−max{|W (0)|} ≤ c(t)−c(0)
ij ij
ij ij
=⇒ max{|W (t)|} ≤ c(t)
ij
ij
28WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
sincemax {|W (0)|} ≤ c(0). Usingthisinequalityin(25),wehave:
ij ij
β
1
max{|V (t+1)−V (0)|} ≤ √ c(t)+c(t)−c(0)
ij ij
ij M
=⇒ max{|V (t+1)−V (0)|} ≤ c(t+1)−c(0)
ij ij
ij
Bythesametoken,wehave:
max{|W (t+1)−W (0)|} ≤ c(t+1)−c(0)
ij ij
ij
Therefore,bytheprincipleofinduction,(23)and(24)holdforallt.
A.7. Theorem13[BoundonthechangeinNTKundercosinesimilarity]
Consider losses of the form (1) with cosine similarity. Let c(0),β be the constant in Lemma 12,
√
R be as in (4)8, α ,α be as in Lemma 7 and γ := 2 2DL c . If a neural network f(·) of the
1 2 ϕ in
form(2)istrainedusinggradientdescent,thenunderAssumptions1-6,fort ≤ Mα iterations,the
changeinNTKisboundedas
(cid:16) (cid:17) α R2+α R
|K (x,x˜;θ(t))−K (x,x˜;θ(0))| ≤ γ
c(0)eβMα−0.5 1
√
2
.
ij ij
M
Inparticular,ifwesetα = 1 andassumeM ≥ max{1,β3},thentheabovestatementsimplifiesto
6
(cid:16) (cid:17)
max sup|K (x,x˜;θ(t))−K (x,x˜;θ(0))| = O M−1/6(logM)3 .
ij ij
t∈(0,M1/6] x,x˜
Proof The main result we need is Lemma 8. Therefore, let us recall the statement of this lemma
andhighlightthequantitiesweneedtospecifytoobtainTheorem13:
DefineS := {s ∈ Rp;∥s−s(0)∥ ≤ R},wherepisthetotalnumberoflearnableparametersin(2).
Assumethatforanyinputx,
(cid:13) (cid:13)
1. (cid:13)H(z)(x;s)(cid:13) ≤ ϵand
(cid:13) (cid:13)
2
2. ∥∇ f (x;s)∥ ≤ c ,∀ z ∈ [Z]and∀ s ∈ S.
s z 2 0
Then, for any inputs x,x˜, ∀ s ∈ S and ∀ i,j ∈ [Z], |K (x,x˜;s)−K (x,x˜;s(0))| ≤ 2ϵc R.
ij ij 0
Letusnowconsiderthetwoquantitiesseparately:
HESSIAN (ASPECT 1)
LetusrecallLemma7:
Under Assumptions 3, 5, 6, consider the neural network defined in (2). If the change in weights
duringtrainingisboundedas
∥W(t)−W(0)∥ +∥V(t)−V(0)∥ ≤ R,
F F
8.NotethatRhereisafunctionofM,withtherelationbeinggivenbyLemma12.Similarly,c(0)=c logM.
θ
29ANILESSERGHOSHDASTIDAR
then,∀ z ∈ [Z],withα = 4β c2 L andα = 4L c (1+β c s c ),thezth Hessianisbounded
1 ϕ in ϕ 2 ϕ in ϕ in 0 s
(cid:13) (cid:13)
as: (cid:13)H(z)(x;θ(t))(cid:13) ≤ α1√R+α2.
(cid:13) (cid:13) 2 M
Now,letusassumewearetrainingf(·)oftheform(2)forMα iterations. Then,usingLemma12,
weknowthat:
max{|V (Mα)−V (0)|} ≤ c(Mα)−c(0)
ij ij
ij
max{|W (Mα)−W (0)|} ≤ c(Mα)−c(0)
ij ij
ij
andwecanfurtherboundtherighthandsideas:
(cid:34) (cid:18)
β
(cid:19)Mα (cid:35)
c(Mα)−c(0) = c(0) 1+ √ −1
M
(cid:104)(cid:16) (cid:17) (cid:105)
≤ c(0)
eβMα−0.5
−1
wherewehaveusedthefactthat1+x ≤ ex. Therefore:
√
∥V(Mα)−V(0)∥ ≤ MDmax{|V (Mα)−V (0)|}
F ij ij
ij
√ (cid:104)(cid:16) (cid:17) (cid:105)
≤ c(0) MD
eβMα−0.5
−1
andsimilarly,
√ (cid:104)(cid:16) (cid:17) (cid:105)
∥W(Mα)−W(0)∥ ≤ c(0) MZ eβMα−0.5 −1
F
√ √ √ (cid:104)(cid:16) (cid:17) (cid:105)
WecannowinvokeLemma7withR = c(0) M( D+ Z) eβMα−0.5 −1 .
GRADIENT (ASPECT 2)
TofurtheruseLemma8,wewouldneedboundson∥∇ f (x;θ)∥and∥∇ f (y;θ)∥,whereθisthe
θ z θ z
weightvectoratt = Mα. Now,
(cid:115)
(cid:13) (cid:13)∂f z(x;θ)(cid:13) (cid:13)2 (cid:13) (cid:13)∂f z(x;θ)(cid:13) (cid:13)2
∥∇ θf z(x;θ)∥
2
= (cid:13)
(cid:13) ∂V
(cid:13)
(cid:13)
+(cid:13)
(cid:13) ∂W
(cid:13)
(cid:13)
F F
Notethat:
∂f (x;θ) 1
z = √ W ϕ′(V x)x
iz i. j
∂V ij M
(cid:12) (cid:12)
(cid:12)∂f z(x;θ)(cid:12) 1
=⇒ (cid:12) (cid:12) ≤ √ |W iz|L ϕ|x j|
(cid:12) ∂V ij (cid:12) M
L c (cid:16) (cid:104)(cid:16) (cid:17) (cid:105) (cid:17)
≤
√ϕ in
c(0)
eβMα−0.5
−1 +|W (0)|
zi
M
L c c(0) (cid:16) (cid:17)
≤
ϕ √in eβMα−0.5
M
30WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
wherewehaveusedLemma12andthefactthatmax {|W (0)|} ≤ c(0). Therefore,
ij ij
(cid:13) (cid:13) (cid:13)∂f z(x;θ)(cid:13) (cid:13)
(cid:13) ≤ L
ϕ√
Dc
inc(0)(cid:16) eβMα−0.5(cid:17)
(cid:13) ∂V (cid:13)
F
Similarly,
∂f (x;θ) 1
z
= √ ϕ(V x)·1
i. j=z
∂W ij M
L
ϕ
≤ √ ∥V ∥∥x∥·1
i. j=z
M
L Dc
ϕ in
≤ √ max|V |·1
ij j=z
M ij
(cid:13) (cid:13)
=⇒
(cid:13) (cid:13)∂f z(x;θ)(cid:13)
(cid:13) ≤ L ϕDc
inc(0)(cid:16) eβMα−0.5(cid:17)
(cid:13) ∂W (cid:13)
F
Therefore,
√ (cid:16) (cid:17)
∥∇ f (x;θ)∥ ≤ 2DL c c(0)
eβMα−0.5
θ z 2 ϕ in
Finally, wecanusetheboundontheHessianandthegradientinLemma8toboundthechangein
thetangentkernel:
(cid:12) (cid:12)K ij(x,y;s′)−K ij(x,y;s(0))(cid:12) (cid:12) ≤ γ(cid:16) c(0)eβMα−0.5(cid:17) α 1R √2+α 2R
M
√
whereγ = 2 2DL c isapositiveconstantindependentofM.
ϕ in
Proof of second statement. Now, let us consider the specific case where α = 1 and M ≥
6
max{1,β3}andlookatthedependenceofthechangeinKernelonM. Notethat:
(cid:12) (cid:12)K ij(x,y;s′)−K ij(x,y;s(0))(cid:12) (cid:12) = O(cid:18) c(0)eβM−1/3(cid:18) α 1R √2+α 2R(cid:19)(cid:19) (26)
M
sinceγ isindependentofM. Now,wehave:
c(0) = O(logM) (27)
Further,notethat
βM−1/3 ≤ 1
sinceM ≥ max{1,β3}. Notethefactthatwhenz ≤ 1,
ez ≤ 1+2z ≤ 3
=⇒ eβM−1/3 ≤ 1+2βM−1/3 ≤ 3 (28)
Using(28),wehave:
eβM−1/3
= O(1) (29)
31ANILESSERGHOSHDASTIDAR
Now:
√ √ √ (cid:104)(cid:16) (cid:17) (cid:105)
α R = α ( D+ Z)c(0) M
eβMα−0.5
−1
2 2
Usingthefactthatα ,D,Z areindependentofM,wehave:
2
√ (cid:104)(cid:16) (cid:17) (cid:105)
α R = O( M logM
eβMα−0.5
−1 )
2
√
= O(( M logM)M−1/3)
1
= O(M6 logM) (30)
wherewehaveused(27)and(28). Hence,
α 1R2 = O(M31 (logM)2) (31)
Plugging(27),(29),(30)and(31)into(26),wehave:
(cid:12) (cid:12)K ij(x,y;s′)−K ij(x,y;s(0))(cid:12) (cid:12) =
O(cid:18)
(logM
1
)3(cid:19)
M6
A.8. Lemma16[ConstancyofC (t)]
V
UnderAssumptions1–5andconstraintW⊤W = I ,considertrainingf⊥(·)in(7)fortiterations
Z
using Grassmannian gradient descent9under losses of the form (10) with learning rate η. Then
√
∥C (t)−C (0)∥ ≤ κ√t ,whereκ := 16δ−2ηQ2L2c2 D.
V V F M ϕ in
Proof ToboundthechangeinC aftertiterations,weanalyze∥C (t)−C (0)∥ . Recallthat:
V V V F
N Q
1 (cid:88)(cid:88)
C (t) = (α s(x ,x ;t)) (32)
V q n nq
N
n=1q=1
where
ϕ(V(t)x)ϕ⊤(V(t)y)
s(x,y;t) :=
(∥ϕ(V(t)x)∥+δ′)(∥ϕ(V(t)y)∥+δ′)
√
andδ′ := Mδ. Therefore:
N Q
1 (cid:88)(cid:88)(cid:2) (cid:3)
∥C (t)−C (0)∥ ≤ ∥s(x ,x ;t)−s(x ,x ;0)∥ (33)
V V F N n n,q n n,q F
n=1q=1
9.Inshort,followingEdelmanetal.(1998),thederivativeofafunctiong(·)restrictedtoaGrassmannianmanifoldcan
beobtainedbyleft-multiplying1−g(·)g(·)⊤totheunrestrictedderivativeofg(·).
32WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
wherewehaveusedthetriangleinequalityandthefactthat|α | = 1. Now,
q
s(x,y;t)−s(x,y;0)
ϕ(V(t)x)ϕ⊤(V(t)y) ϕ(V(0)x)ϕ⊤(V(0)y)
= −
(∥ϕ(V(t)x)∥+δ′)(∥ϕ(V(t)y)∥+δ′) (∥ϕ(V(0)x)∥+δ′)(∥ϕ(V(0)y)∥+δ′)
ϕ(V(t)x)
(cid:18)
ϕ(V(t)y) ϕ(V(0)y)
(cid:19)⊤
= −
∥ϕ(V(t)x)∥+δ′ ∥ϕ(V(t)y)∥+δ′ ∥ϕ(V(0)y)∥+δ′
(cid:18) ϕ(V(t)x) ϕ(V(0)x) (cid:19) ϕ⊤(V(0)y)
+ −
∥ϕ(V(t)x)∥+δ′ ∥ϕ(V(0)x)∥+δ′ ∥ϕ(V(0)y)∥+δ′
OncomputingtheFrobeniusnormoftheaboveexpression,wehave:
∥s(x,y;t)−s(x,y;0)∥
F
(cid:13) (cid:13)
∥ϕ(V(t)x)∥ (cid:13) ϕ(V(t)y) ϕ(V(0)y) (cid:13)
≤ (cid:13) − (cid:13)
∥ϕ(V(t)x)∥+δ′ (cid:13)∥ϕ(V(t)y)∥+δ′ ∥ϕ(V(0)y)∥+δ′(cid:13)
(cid:13) (cid:13)
(cid:13) ϕ(V(t)x) ϕ(V(0)x) (cid:13) ∥ϕ(V(0)y)∥
+(cid:13) − (cid:13)
(cid:13)∥ϕ(V(t)x)∥+δ′ ∥ϕ(V(0)x)∥+δ′(cid:13) ∥ϕ(V(0)y)∥+δ′
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) ϕ(V(t)y) ϕ(V(0)y) (cid:13) (cid:13) ϕ(V(t)x) ϕ(V(0)x) (cid:13)
≤ (cid:13) − (cid:13)+(cid:13) − (cid:13)
(cid:13)∥ϕ(V(t)y)∥+δ′ ∥ϕ(V(0)y)∥+δ′(cid:13) (cid:13)∥ϕ(V(t)x)∥+δ′ ∥ϕ(V(0)x)∥+δ′(cid:13)
Now,wehave:
(cid:13) (cid:13)
(cid:13) ϕ(V(t)y) ϕ(V(0)y) (cid:13)
(cid:13) − (cid:13)
(cid:13)∥ϕ(V(t)y)∥+δ′ ∥ϕ(V(0)y)∥+δ′(cid:13)
(cid:13) (cid:13)
(cid:13)∥ϕ(V(0)y∥ϕ(V(t)y)−∥ϕ(V(t)y)∥ϕ(V(0)y)(cid:13)
≤ (cid:13) (cid:13)
(cid:13) (∥ϕ(V(t)y)∥+δ′)(∥ϕ(V(0)y)∥+δ′) (cid:13)
(cid:13) (cid:13)
+δ′(cid:13)
(cid:13)
∥ϕ(V(t)y)−ϕ(V(0)y)∥ (cid:13)
(cid:13)
(cid:13)(∥ϕ(V(t)y)∥+δ′)(∥ϕ(V(0)y)∥+δ′)(cid:13)
Now,
∥(∥ϕ(V(0)y∥ϕ(V(t)y)−∥ϕ(V(t)y)∥ϕ(V(0)y))∥
≤ |(∥ϕ(V(0)y)∥−∥ϕ(V(t)y)∥)|∥ϕ(V(t)y)∥+∥ϕ(V(t)y∥∥ϕ(V(t)y)−ϕ(V(0)y)∥
FromtheLipschitzcontinuityofϕ,wehave:
|(∥ϕ(V(t)y)∥−∥ϕ(V(0)y)∥)| ≤ ∥ϕ(V(t)y)−ϕ(V(0)y)∥ ≤ L ∥V(t)−V(0)∥∥y∥
ϕ
Therefore,
∥(∥ϕ(V(0)y∥ϕ(V(t)y)−∥ϕ(V(t)y)∥ϕ(V(0)y))∥ ≤ 2L ∥ϕ(V(t)y)∥∥V(t)−V(0)∥∥y∥
ϕ
Hence,
(cid:13) (cid:13) ϕ(V(t)y) ϕ(V(0)y) (cid:13) (cid:13) L ϕ∥V(t)−V(0)∥∥y∥(2∥ϕ(V(t)y)∥+δ′)
(cid:13) − (cid:13) ≤
(cid:13)∥ϕ(V(t)y)∥+δ′ ∥ϕ(V(0)y)∥+δ′(cid:13) (∥ϕ(V(t)y)∥+δ′)(∥ϕ(V(0)y)∥+δ′)
2L ∥V(t)−V(0)∥∥y∥
ϕ
≤
∥ϕ(V(0)y)∥+δ′
33ANILESSERGHOSHDASTIDAR
Similarly,
(cid:13) (cid:13)
(cid:13) ϕ(V(t)x) ϕ(V(0)x) (cid:13) 2L ϕ∥V(t)−V(0)∥∥x∥
(cid:13) − (cid:13) ≤
(cid:13)∥ϕ(V(t)x)∥+δ′ ∥ϕ(V(0)x)∥+δ′(cid:13) ∥ϕ(V(0)x)∥+δ′
Therefore,
∥s(x,y;t)−s(x,y;0)∥
F
(cid:18) (cid:19)
∥y∥ ∥x∥
≤ 2L ∥V(t)−V(0)∥ + (34)
ϕ ∥ϕ(V(0)y)∥+δ′ ∥ϕ(V(0)x)∥+δ′
BOUNDING∥V(t)−V(0)∥:
Wewillderive∥V(t)−V(0)∥throughtheanalysisof ∂L. Recallthatwecanwritethelossas:
∂V
N Q
1 (cid:88)(cid:88)
L = Tr(WTs(x ,x )W)
n n,q
N
n=1q=1
Letusdefineg(x,y) = Tr(WTs(x,y)W)andnotethat,
N Q
∂L 1 (cid:88)(cid:88) ∂g(x n,x n,q)
=
∂V N ∂V
ij ij
n=1q=1
Now,
∂g(x,y)
(cid:18) ∂g(x,y)(cid:19)⊤
∂ϕ(Vx)
(cid:18) ∂g(x,y)(cid:19)⊤
∂ϕ(Vy)
= +
∂V ∂ϕ(Vx) ∂V ∂ϕ(Vy) ∂V
ij ij ij
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
TermI TermII
WeboundTermI (TermII followsanalogously). First,weexpandthefirstexpressioninTermI:
√
∂g(x,y) M (∥ϕ(Vx)∥+δ′)2Wu −M2u⊤u ϕ(Vx)
= y x y
∂ϕ(Vx) (∥ϕ(Vx)∥+δ′)3(∥ϕ(Vy)∥+δ′)
whereu
x
= √1 W⊤ϕ(Vx)andu
y
= √1 W⊤ϕ(Vy)andthesecondexpressioninTermI as
M M
(cid:20) (cid:21)
∂ϕ(Vx)
= 1 ·ϕ′(V x)x .
r=i i. j
∂V
ij r
Combiningthem,TermI becomes:
√
(cid:18) ∂g(x,y)(cid:19)⊤ ∂ϕ(Vx) M (∥ϕ(Vx)∥+δ′)2W u −Mu⊤u ϕ (Vx)
= i. y x y i ·ϕ′(V x)x
∂ϕ(Vx) ∂V
ij
(∥ϕ(Vx)∥+δ′)3(∥ϕ(Vy)∥+δ′) i. j
Wecanbounditsnormas:
(cid:12) (cid:12)
(cid:12)(cid:18) ∂g(x,y)(cid:19)⊤ ∂ϕ(Vx)(cid:12)
(cid:12) (cid:12)
(cid:12) ∂ϕ(Vx) ∂V (cid:12)
(cid:12) ij (cid:12)
√
≤ M (∥ϕ(Vx) (∥ ∥ϕ+ (Vδ′) x2 )∥ ∥W +i δ.∥ ′)∥ 3u (∥y ϕ∥ (+ VM y)∥∥u +x δ∥ ′∥ )u y∥|ϕ i(Vx)| ·(cid:12) (cid:12)ϕ′(V i.x)x j(cid:12) (cid:12).
34WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
Tofurthersimplifytheexpression,wenotethat:
√
M ∥u ∥ ≤ ∥W∥ ∥ϕ(Vx)∥
x 2
≤ (∥ϕ(Vx)∥+δ′)
√
M ∥u ∥ ≤ (∥ϕ(Vy)∥+δ′)
y
(cid:12) (cid:12)ϕi(Vx)(cid:12) (cid:12) ≤ ∥ϕ(Vx)∥+δ′
∥W ∥ ≤ 1
i.
wherethelastinequalityfollowsfromthefactthatW⊤W = I (Toseethis,considertheM ×M
Z
square matrix W˜ formed by appending M −Z columns to W such that columns of W˜ form an
(cid:13) (cid:13)
orthonormal set. Then, it is clear that W˜ is an orthogonal square matrix and hence (cid:13)W˜ (cid:13) = 1.
(cid:13) i.(cid:13)
(cid:13) (cid:13)
Since∥W ∥ ≤ (cid:13)W˜ (cid:13)byconstruction,wehave∥W ∥ ≤ 1). Therefore,
i. (cid:13) i.(cid:13) i.
(cid:12) (cid:12)
(cid:12)(cid:18) ∂g(x,y)(cid:19)⊤ ∂ϕ(Vx)(cid:12) 2|ϕ′(V x)x |
(cid:12) (cid:12) ≤ i. j
(cid:12) ∂ϕ(Vx) ∂V (cid:12) ∥ϕ(Vx)∥+δ′
(cid:12) ij (cid:12)
(cid:18) (cid:19)
2 L c
ϕ in
≤ √
M δ
Similarly,wecanboundTermII as:
(cid:12) (cid:12)
(cid:12)(cid:18) ∂g(x,y)(cid:19)⊤ ∂ϕ(Vy)(cid:12) 2 (cid:18) L c (cid:19)
(cid:12) (cid:12) ≤ √ ϕ in .
(cid:12) (cid:12) ∂ϕ(Vy) ∂V ij (cid:12) (cid:12) M δ
(cid:12) (cid:12)
CombiningtheboundsforTermI andTermII,wecannowbound(cid:12)∂g(x,y)(cid:12)as:
(cid:12) ∂Vij (cid:12)
(cid:12) (cid:12) (cid:18) (cid:19)
(cid:12)∂g(x,y)(cid:12) 4 L ϕc
in
(cid:12) (cid:12) ≤ √
(cid:12) ∂V ij (cid:12) M δ
andhence,
(cid:12) (cid:12) (cid:18) (cid:19)
(cid:12) ∂L (cid:12) 4 QL ϕc in
(cid:12) (cid:12) ≤ √ .
(cid:12)∂V ij(cid:12) M δ
Assuminggradientdescentwithlearningrateη wehave:
(cid:12) (cid:12)
(cid:12) ∂L (cid:12)
|∆V ij| = η(cid:12) (cid:12)
(cid:12)∂V (cid:12)
ij
(cid:18) (cid:19)
4 ηQL c
ϕ in
≤ √
M δ
Therefore,aftertiterations,thechangeinV canbeboundedas:
ij
(cid:18) (cid:19)
t 4ηQL c
ϕ in
|V (t)−V (0)| ≤ √
ij ij
M δ
(cid:18) (cid:19)
4ηQL c
ϕ in
=⇒ ∥V(t)−V(0)∥ ≤ t . (35)
F δ
35ANILESSERGHOSHDASTIDAR
Substitutingthisin(34),weobtain:
(cid:32) √ (cid:33)
(cid:18) (cid:19)
4ηQL c 4L c D
ϕ in ϕ in
∥s(x,y;t)−s(x,y;0)∥ ≤ t √ ,
F δ Mδ
√
16ηQ2L2c2 D
andhencefrom(33),withκ = ϕ in ,wehave:
δ2
t
∥C (t)−C (0)∥ = κ√
V V F
M
thereforeconcludingtheproof.
A.9. Lemma17[Perturbationboundonrepresentation]
Letu(x;W∗,C )betherepresentationobtainedfrom(7)withW = W∗,whereW∗ isobtained
ϑ
∗
by solving (5) for a fixed C ϑ. Under Assumptions 1–5, let W∗,W(cid:102) be the solutions of (5) ob-
tained through PCA on fixed C(cid:101)V(0) and √C(cid:101)V(t) respectively. Let λ Z,λ
Z+1
be Zth and (Z +1)th
eigenvalues of C(cid:101)V(0). Let ζ = 4δ−1ηQ DL2 ϕc2
in
and ξ = 27 2δ−1QDc in(L ϕc
θ
+|ϕ(0)|). There
existsanorthogonalmatrixO suchthat
(cid:13) ∗ (cid:13) t (cid:18) ξlogM (cid:19)
(cid:13) (cid:13)O⊤u(x;W(cid:102) ,C(cid:101)V(t))−u(x;W∗,C(cid:101)V(0))(cid:13)
(cid:13)
≤ ζ√
M
1+
λ Z −λ Z+1
.
Proof NotethatsincewearedoingPCA,W⊤W = I andhencetherequirementsofLemma16
Z
aremet. Therefore:
t
∥C (t)−C (0)∥ = κ√
V V F
M
(cid:13) (cid:13) t
=⇒ (cid:13) (cid:13)C(cid:101)V(t)−C(cid:101)V(0)(cid:13)
(cid:13)
= κ√ (36)
F M
FromAssumptions4and5,wehave:
∥ϕ(V(0))∥ ≤ L ∥V(0)∥ +∥ϕ(0)∥
F ϕ F F
√ √ √
=⇒ ∥ϕ(V(0))∥ ≤ ( DL c )( M logM)+|ϕ(0)| DM (37)
F ϕ θ
√ √
≤ D(L c +|ϕ(0)|)( M logM) (38)
ϕ θ
Using(35),wehave:
∥ϕ(V(t))−ϕ(V(0))∥ ≤ L ∥V(t)−V(0)∥
F ϕ F
(cid:32) (cid:33)
4ηQL2c
ϕ in
≤ t (39)
δ
36WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
∗
We denote u(x;W(cid:102) ,C(cid:101)V(t)) as u and u˜(x;W∗,C(cid:101)V(0)) as u˜ to simplify notation. Now, for any
orthogonalmatrixO,using(37)and(39),wehave:
(cid:13) (cid:13)O⊤u−u˜(cid:13) (cid:13) = (cid:13) (cid:13) (cid:13)√1 (cid:18) O⊤W(cid:102)∗⊤ ϕ(V(t)x)−W∗⊤ ϕ(V(0)x)(cid:19)(cid:13) (cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) M (cid:13)
(cid:18)(cid:13) (cid:13) (cid:13) (cid:13)(cid:19)
≤
√1 (cid:13) (cid:13)O⊤W(cid:102)∗⊤ (ϕ(V(t)x)−ϕ(V(0)x))(cid:13) (cid:13)+(cid:13) (cid:13)(cid:16) W(cid:102)∗ O−W∗(cid:17)⊤ ϕ(V(0)x)(cid:13)
(cid:13)
M (cid:13) (cid:13) (cid:13) (cid:13)
1 (cid:16) (cid:13) ∗ (cid:13) (cid:17)
≤ √ ∥x∥∥ϕ(V(t))−ϕ(V(0))∥ +∥x∥∥ϕ(V(0))∥ (cid:13)W(cid:102) O−W∗(cid:13)
F F (cid:13) (cid:13)
M F
≤
√c 1t
+(c
2logM)(cid:13)
(cid:13)
(cid:13)W(cid:102)∗ O−W∗(cid:13)
(cid:13)
(cid:13)
M F
√
wherec
1
= 4ηQ D δL2 ϕc2 in andc
2
= Dc in(L ϕc θ+|ϕ(0)|). Now,notethatW(cid:102)∗ andW∗havethetopZ
eigenvectorsofC(cid:101)V(t)andC(cid:101)V(0)respectivelyastheircolumns. Toboundthedifferencebetween
these two matrices, we can use the Davis–Kahan theorem. In particular, we invoke Theorem 2 of
Yu et al. (2015) which (when adapted to our setup) says that there exists an orthogonal matrix
O˜
suchthat:
(cid:13) (cid:13)
(cid:13) ∗ (cid:13)
23
2
(cid:13) (cid:13)C(cid:101)V(t)−C(cid:101)V(0)(cid:13)
(cid:13)
(cid:13)W(cid:102) O(cid:101) −W∗(cid:13) ≤ F
(cid:13) (cid:13) F λ Z −λ Z+1
SinceO canbeanyorthogonalmatrix,letuschooseO = O˜ . Then,wehave:
(cid:13) (cid:13)O⊤u−u˜(cid:13)
(cid:13) ≤
√c 1t
+
23 2c 2logM ·κ√t
(cid:13) (cid:13) M λ Z −λ Z+1 M
wherewehaveused(36). Therefore:
(cid:13) (cid:13) t (cid:18) ξlogM (cid:19)
(cid:13)O⊤u−u˜(cid:13) ≤ ζ√ 1+
(cid:13) (cid:13) M λ Z −λ Z+1
√
whereζ = 4ηQ DL2 ϕc2 in andξ = 227 QDcin(L ϕc θ+|ϕ(0)|) concludingtheproof.
δ δ
A.10. Proposition18
Consideraneuralnetworkoftheform(2)trainedusingalossoftheform(1)usinggradientdescent.
Thenforanyinputx,W (0) = W (0) ⇒ f (x;θ(t)) = f (x;θ(t)), ∀ t ≥ 0.
.i .j i j
Proof WecomparetheoutputsbyanalyzingthelearningdynamicsasgiveninLemma9,weknow
that:
(cid:20) Z (cid:20) (cid:21)(cid:21)
−1 (cid:88) ∂l (cid:88)
f˙(x) = K (x,x ;θ)g (x ,x )+K (x,x ;θ)g (x ,x )
i ip n p n n,q ip n,q p n,q n
N ∂s(x ,x )
n n,q
n,q p=1
(cid:20) Z (cid:20) (cid:21)(cid:21)
−1 (cid:88) ∂l (cid:88)
f˙ (x) = K (x,x ;θ)g (x ,x )+K (x,x ;θ)g (x ,x )
j jp n p n n,q jp n,q p n,q n
N ∂s(x ,x )
n n,q
n,q p=1
37ANILESSERGHOSHDASTIDAR
whereg (x,y) = ∂s(x,y) . Notethatirrespectiveofthesimilaritymeasure,thequantities ∂l ,
p ∂fp(x;θ) ∂s(xn,xn,q)
g (x ,x ), g (x ,x ) are independent of i and j and hence are identical in the dynamics of
p n n,q p n,q n
bothf˙(x)andf˙ (x).
i j
Therefore,weonlyhavetocharacterizeandcomparetheNTKforaneuralnetworkf(·)oftheform
(2). RecallthattheNTKcanbedefinedas:
∂f (x;θ)⊤∂f (y;θ) ∂f (x;θ)⊤∂f (y;θ)
i j i j
K (x,y;θ) = +
ij
∂v ∂v ∂w ∂w
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
TermI TermII
where v and w are vectorized versions of weights V and W respectively. Now we can expand
TermI:
∂f (x;θ)
i = W ϕ′(V x)x
qi q. r
∂V
qr
andtherefore,
∂f i(x,θ)⊤∂f j(y;θ)
=
(cid:88)
W W ϕ′(V x)ϕ′(Vq.y)x y .
∂v ∂v qi qj q. r r
q,r
Similarly,forTermII:
∂f (x;θ)
i
= ϕ(V x)·1 ,
q′. i=r′
∂W
q′r′
and,
∂f i(x;θ)⊤∂f j(y;θ) (cid:88)
= ϕ(V x)ϕ(V y)·1
q′. q′. i=j
∂w ∂w
q′
We are interested in the evolution of f (x;θ(t)) and f (x;θ(t)) when W (0) = W (0). There-
i j .i .j
fore,
(cid:88) (cid:88)
K (x,x ;θ) = W W ϕ′(V x)ϕ′(V y)x y + ϕ(V x)ϕ(V y)·1
ip n qi qp q. q. r r q′. q′. i=p
q,r q′
(cid:88) (cid:88)
K (x,x ;θ) = W W ϕ′(W x)ϕ′(W y)x y + ϕ(V x)ϕ(V y)·1
jp n qj qp q. q. r r q′. q′. j=p
q,r q′
From these equations, it is clear that K (x,x ;θ) = K (x,x ;θ) whenever W = W .
ip n jp n .i .j
Therefore,ifweprovethatW (t) = W (t)forallt,thenf˙(x) = f˙ (x)andhencef (x) = f (x)
.i .j i j i j
forallt.
Let us assume W (t) = W (t) at some time t. Then, after an iteration of gradient descent, with
.i .j
learningrateη,wehave:
N Q
η (cid:88)(cid:88) ∂l ∂s(x k,x k,q)
W (t+1) = W (t)−
.i .i
N ∂s(x ,x ) ∂W (t)
k k,q .i
n=1q=1
N Q
η (cid:88)(cid:88) ∂l ∂s(x k,x k,q)
W (t+1) = W (t)−
.j .j
N ∂s(x ,x ) ∂W (t)
k k,q .j
n=1q=1
38WHENCANWEAPPROXIMATEWIDECONTRASTIVEMODELSWITHNTKSANDPCA?
CASE 1: DOT PRODUCT
Wehave:
∂s(x ,x ) 1 ∂Tr(cid:0) ϕ⊤(V(t)x )W(t)W(t)⊤ϕ(Vx )(cid:1)
k k,q k k,q
=
∂W(t) M ∂W(t)
1 (cid:16) (cid:17)
= ϕ(V(t)x )ϕ⊤(V(t)x )+ϕ(V(t)x )ϕ⊤(V(t)x ) W(t)
k,q k k k,q
M
Therefore:
∂s(x ,x ) 1 (cid:16) (cid:17)
k k,q = ϕ(V(t)x )ϕ⊤(V(t)x )+ϕ(V(t)x )ϕ⊤(V(t)x ) W (t)
k,q k k k,q .i
∂W (t) M
.i
∂s(x ,x ) 1 (cid:16) (cid:17)
k k,q = ϕ(V(t)x )ϕ⊤(V(t)x )+ϕ(V(t)x )ϕ⊤(V(t)x ) W (t)
k,q k k k,q .j
∂W (t) M
.j
∂s(x ,x ) ∂s(x ,x )
Clearly,sinceW (t) = W (t), k k,q = k k,q foralln,q andthereforeW (t+1) =
.i .j ∂W.i(t) ∂W.j(t) .i
W (t+1).
.j
CASE 2: COSINE SIMILARITY
FromtheproofofLemma11,wehave:
(cid:34) (cid:35)
∂s(x k,x k,q)
=
√1 ϕ(Vx k)u⊤
k,q
+ϕ(Vx k,q)u⊤
k
−
√1 (cid:20) u⊤ ku
k,q
∂W M (∥u k∥+δ)(∥u k,q∥+δ) M (∥u k∥+δ)2(∥u k,q∥+δ)2
(cid:32) (cid:33)(cid:35)
(∥u k,q∥+δ)ϕ(Vx k)u⊤
k +
(∥u k∥+δ)ϕ(Vx k,q)u⊤
k,q
∥u ∥ ∥u ∥
k k,q
Therefore,
∂s(x ,x ) 1 (cid:20) ϕ(Vx )(u ) +ϕ(Vx )(u ) (cid:21) 1 (cid:20) u⊤u
k k,q = √ k k,q i k,q k i − √ k k,q
∂W .i(t) M (∥u k∥+δ)(∥u k,q∥+δ) M (∥u k∥+δ)2(∥u k,q∥+δ)2
(cid:18) (cid:19)(cid:21)
(∥u ∥+δ)ϕ(Vx )(u ) (∥u ∥+δ)ϕ(Vx )(u )
k,q k k i k k,q k,q i
+
∥u ∥ ∥u ∥
k k,q
and
∂s(x ,x ) 1 (cid:20) ϕ(Vx )(u ) +ϕ(Vx )(u ) (cid:21) 1 (cid:20) u⊤u
k k,q = √ k k,q j k,q k j − √ k k,q
∂W .j(t) M (∥u k∥+δ)(∥u k,q∥+δ) M (∥u k∥+δ)2(∥u k,q∥+δ)2
(cid:18) (cid:19)(cid:21)
(∥u ∥+δ)ϕ(Vx )(u ) (∥u ∥+δ)ϕ(Vx )(u )
k,q k k j k k,q k,q j
+
∥u ∥ ∥u ∥
k k,q
Note that (u k,q)
i
= √1 MW⊤ .i(t)ϕ(Vx k,q) and (u k,q)
j
= √1 MW⊤ .j(t)ϕ(Vx k,q). Therefore, since
W (t) = W (t), (u ) = (u ) . Similarly, (u ) = (u ) . Since this is true for all k,q,
.i .j k,q i k,q j k i k j
W (t+1) = W (t+1).
.i .j
Therefore, for both dot product and cosine similarity based losses, W (t) = W (t) implies
.i .j
W (t + 1) = W (t + 1). Note that W (0) = W (0) is assumed to be true. Therefore, by
.i .j .i .j
the principle of induction, W (t) = W (t) for all t ≥ 0. Hence, f (x) = f (x) for all t ≥ 0
.i .j i j
whichconcludestheproof.
39ANILESSERGHOSHDASTIDAR
AppendixB. AdditionalPlots
We present in Figure 6 the same results as in Figure 3, 5(left) & 4(left), but the change is plotted
as a function of time instead of as a function of M. In the main paper, we analyze the behaviour
ofthequantitieswhenM changes, asthisallowsustoshowthatthechangedecreaseswithwidth.
Here, we see that the increase with t is also small for large widths. We also note that the standard
deviationissignificantlyhigherforsmallerwidths,whichistobeexpectedsincetheboundisloose
forsmallM.
(a) (b) (c)
Figure 6: . (a) The maximum entry-wise change in empirical NTK for single hidden layer neural
networkswithReLUactivationandLinearlossforvaryingwidthM. (b)Themaximumentry-wise
change in empirical NTK for three hidden layer neural networks of varying width. (c) Change in
C duringtrainingforvaryingwidthM.
V
Inaddition, wealsoobservethattheevolutionforReLUnetworks(Figure3&6(a))aresimilarto
theevolutionobservedforTanhactivationinFigure7.
Figure 7: The maximum entry-wise change in empirical NTK for single hidden layer neural net-
workswithTanhactivationandLinearlossforvaryingdepthsM.
40