A Picture Is Worth a Thousand Words:
Exploring Diagram and Video-Based OOP
Exercises to Counter LLM Over-Reliance
Bruno Pereira Cipriano1, Pedro Alves1, and Paul Denny2
1 Luso´fona University, COPELABS, Campo Grande, 376, Lisbon, Portugal
{bcipriano,pedro.alves}@ulusofona.pt
2 The University of Auckland, Auckland, New Zealand
paul@cs.auckland.ac.nz
Abstract. Much research has highlighted the impressive capabilities
of large language models (LLMs), like GPT and Bard, for solving in-
troductory programming exercises. Recent work has shown that LLMs
can effectively solve a range of more complex object-oriented program-
ming(OOP)exerciseswithtext-basedspecifications.Thisraisesconcerns
aboutacademicintegrity,asstudentsmightusethesemodelstocomplete
assignments unethically, neglecting the development of important skills
such as program design, problem-solving, and computational thinking.
Toaddressthis,weproposeaninnovativeapproachtoformulatingOOP
tasksusingdiagramsandvideos,asawaytofosterproblem-solvingand
deter students from a copy-and-prompt approach in OOP courses. We
introduce a novel notation system for specifying OOP assignments, en-
compassing structural and behavioral requirements, and assess its use
in a classroom setting over a semester. Student perceptions of this ap-
proach are explored through a survey (n=56). Generally, students re-
sponded positively to diagrams and videos, with video-based projects
being better received than diagram-based exercises. This notation ap-
pears to have several benefits, with students investing more effort in
understanding the diagrams and feeling more motivated to engage with
the video-based projects. Furthermore, students reported being less in-
clinedtorelyonLLM-basedcodegenerationtoolsforthesediagramand
video-basedexercises.ExperimentswithGPT-4andBard’svisionabili-
tiesrevealedthattheycurrentlyfallshortininterpretingthesediagrams
to generate accurate code solutions.
Keywords: object-orientedprogramming·largelanguagemodels·gpt-
4 · bard
1 Introduction
The advent of large language models (LLM) and their ability to generate com-
puter code from natural language descriptions has led to robust discussion in
the computing education community around the opportunities and challenges
4202
raM
31
]ES.sc[
1v69380.3042:viXra2 B. Pereira Cipriano et al.
they present to educators and students [5,11]. In fact, there is differing opin-
ion amongst educators regarding whether to resist and fight the usage of these
tools, or to try and find ways to embrace them [16,26]. While there have been
some initial attempts to integrate LLMs into teaching practice at the introduc-
tory level [22,14], very little is known about the efficacy of these approaches
and there is little discussion or consensus about how higher level courses should
adapt [23].
Given that LLMs are becoming an essential part of industry practice [4], it
is necessary for educators to explore approaches that promote the acquisition of
core computing knowledge and skills alongside authentic use of code-generation
tools. Tasks which are solvable through pure “copy-and-prompt” approaches
may not be engaging or motivating for students. Very recent research has sug-
gested using image-based exercises that illustrate expected behaviors through
diagrammatic descriptions of input/output pairs [16,9]. The goals of this ap-
proach, which has only been explored at the introductory programming level,
are twofold: first, students have to make an effort to understand the expected
transformation,and,second,theycan’tjustcopytheassignmentintoChatGPT,
Bard or similar tool.
Inthisresearch weproposeextendingtheideaofimage-basedproblem spec-
ifications to more complex design-oriented tasks suitable for Object-Oriented
Programming (OOP) courses. In such courses, the focus is not on implementing
stand-alone functions but rather on tasking students with designing and imple-
mentingmultipleclassesthatmaintainmutablestatesandcollaboratewitheach
othertoachievespecificobjectives.Ouraimisalsotodevelopanapproachwhere
problemscanbepresentedinsuchawaythatthattheirsolutionisnotuniquely
specified.Instead,ratherthanguidingstudentstowardsasinglemodelsolution,
wewantstudentstohavetheflexibilitytoinfertheirownobject-orienteddesign
byperformingadomainanalysis.Toachievethis,weproposeanovelnotationfor
expressingprogramrequirements.Problemsarethenpresentedtostudentsusing
this notation, rather than as highly descriptive plain text specifications, which
we hypothesize will help to guard against probable LLM-abuse as motivated in
prior work [10].
We conducted an experiment with diagram-based OOP exercises and video-
based OOP projects, both utilizing a custom notation, and evaluated it in a
classroom setting over the course of a semester. Our investigation is guided by
the following three research questions (RQs):
RQ1:Dostudentsexpressapreferencetowardsdiagram-basedexercisescom-
pared to exercises with more traditional text-based specifications?
RQ2: For larger-scale projects, are video-based specifications motivating for
students and do they find them easier for interpreting required behavior?
RQ3:Towhatextentdoestheproposednotationforspecifyingprogramming
tasks discourage students from inappropriate use of LLMs for code-generation?A Picture Is Worth a Thousand Words 3
Recently, both ChatGPT3 and Bard4 were updated with the capacity to
interpret image content (i.e. ‘vision’). We performed some experiments to de-
termine if these new capabilities could jeopardize our diagramming efforts. Al-
though these were ad-hoc experiments, the relevance to our study made us in-
clude them in a brief section of this paper.
Thispapermakesthefollowingcontributions:(1)Presentsanovelnotationto
represent OOP exercises; (2) presents the results of a student survey evaluating
diagram-based and video-based exercises, as well as the impact of these new
exercise formats on their LLM usage; and, (3) presents the results of several
ad-hoc experiences using ChatGPT-4 and Bard’s ‘vision’ capabilities to solve
diagram-based exercises.
2 Related work
SpecifyingProgrammingTasksManyprogrammingcoursesrequirestudents
to implement certain behaviours described in natural language (e.g. “Create a
function that receives an array of int(s) and returns the sum of all elements in
the array”). Students have to interpret the textual description of the problem,
devise an algorithm to solve it, and then create computer code that implements
the algorithm. This is particularly common for introductory programming exer-
cises [1]. GPT-based models have demonstrated great capacity for solving such
exercises described in natural language [12,25,24,8].
OOP Courses Some object-oriented programming exercises are based on
Unified Modeling Language (UML) diagrams: teachers first teach students how
to interpret UML class diagrams, and then ask them to create the code that
implements the classes described in the UML diagram [20]. Other educators
use text based instructions, with either strongly directed instructions [25,24], or
with less directed instructions which require that students partially decide the
objectmodelforthemselves [6].GPT-basedmodelshavealsodemonstratedthe
capability to solve text-based OOP exercises [7,25,24,20,6].
LLM-oriented exercises Several strands of work have explored ways to
limit student reliance on LLMs and foster new skillsets. Denny et al. proposed
Prompt Problems [9], a novel pedagogical approach to the teaching of pro-
gramming, in which diagrams are displayed to students, who must then derive
an effective LLM-prompt to obtain working code. Liffiton et al. describe Code-
Help [18], a LLM-powered tool which acts an intermediate between the student
and GPT, in order to prevent students from over-relying on the LLM by filter-
ingoutgeneratedsourcecode.Studentsreportedpositivefeedbackregardingits
usefulness for support while programming.
3 ChatGPTreceived‘vision’supportinSeptember25,2023(onlyavailableforpaying
subscribers) [19].
4 Google Bard started supporting image input in July 13, 2023 [15]4 B. Pereira Cipriano et al.
3 Notation: Diagrams and videos
3.1 Diagrams
Ourproposednotationaimstoaddressthevarietyofscenariosthattypicallyoc-
cur in OOP, such as functions that change the state of an object, functions that
receivemultipleobjectsandassociatethemwitheach-other,conceptswithcom-
monattributesand/orbehavioursthatshouldtakeadvantageoftheinheritance
and polymorphism mechanisms, and so on.
We propose 5 types of diagrams, and explain the relevant notation in the
following subsections. A given assignment may use a combination of several
diagram types.
Algorithmic function diagram These diagrams are used to present students
with single non-instance (i.e. static) functions that transform input into output,
similar to prior research adapting introductory programming courses [9]. See
Figure 1 for an example.
These diagrams depict the function, with a black box denoting its name,
input elements represented using inbound orange arrows, and output elements
in green (with outbound black arrows). In cases where inputs or outputs are
arrays,linesofsmallboxesshouldbeutilized,asillustratedintheFigure1.The
functionnamewithintheblackboxshouldbe“obfuscated,”soasnottoprovide
clues to the function’s purpose (e.g., ‘f()’). This is recommended as research has
shown GPT-based models can generate working solutions from only meaningful
function names [3,27].
A minimum of two examples of inputs/outputs should be provided to assist
inclarifyingtheexpectedbehavior.Incertaininstances,morethantwoexamples
maybenecessary,particularlytoillustratehowthefunctionshouldbehavewith
invalid inputs or boundary cases.
Fig.1. Example algorithmic exercise. Students should understand that the function
must create an array with elements from the two input arrays, in alternating fashion.
State-change function diagram These diagrams are similar to the algorith-
mic function diagrams described in the previous section but are used for non-
instance (static) functions that change the state of one or more objects
and possibly also return a value. While this may be considered bad practice in
imperative programming, where students should avoid implementing functionsA Picture Is Worth a Thousand Words 5
class Account {
public boolean widthdraw(int ammount) {
if (ammount < this.balance) ...
}
}
static boolean withdraw(Account account, int ammount) {
return account.withdraw(ammount);
}
Listing 1.1. Withdraw function and supporting class, in Java
with side-effects, it is common practice in OOP. Since these functions can have
two simultaneous effects, a different notation must be used for changing the ob-
jects and returning a value. The former is represented by a dashed arrow while
the latter is represented by a solid arrow (in concordance with the algorith-
mic diagrams that only return values). Given that the distinction between solid
and dashed arrows may not be immediately evident, a caption is provided to
highlight this difference for students. The remaining notation is expected to be
inferred by students.
The business rules can be directly written in the diagram as a side note
but we consider it more interesting to just provide several examples and allow
students to infer those rules. For more complex rules, it may not be feasible to
describethemthroughexamples-considerusingastatetransitionrulesdiagram
(see Section 3.1) in those cases.
Noticethatstudentsareexpectedtoimplementnotonlythefunctionbutalso
the classes and methods that support its behavior. Imagine you want students
toimplementthe‘withdraw’functioninJava,asillustratedinListing1.1.Using
our notation, the corresponding diagram is the one shown in Figure 2 with the
first example showing a successful withdrawal that returns true and reduces the
account’s balance and the second example showing a failed withdrawal due to
insufficient balance.
Fig.2. State-change function diagram for the withdraw function. It changes the ‘Ac-
count’ object’s state and returns true if the operation succeeded, or false otherwise.6 B. Pereira Cipriano et al.
Class declaration diagram These diagrams provide students with guidelines
for implementing predefined classes, attributes, and relationships (e.g., compo-
sition). Each class should be depicted within a box, enumerating its attributes
(optionally with their types), specifying the necessary constructors, and out-
lining the expected behavior of fundamental methods like ‘toString()’. The be-
havior can be described similarly to algorithmic diagrams, with examples of
input/output and corresponding arrows. It is important to note that the direct
representationofrelationshipsbetweenclassesisomitted.Specificattributesare
linked with an accompanying image, indicating that they are, in fact, objects
themselves.
See Figure 3 for an example where students had to implement two related
classes. In that example, students had to infer that the small house-like figure
withinthe‘Person’classdiagramrepresentedacompositionrelationshipbetween
the ‘Person’ and ‘Apartment’ classes. Note also the reference to the ‘toString()’
function, with a black arrow which indicates the expected return value, similar
to the previously discussed diagrams for algorithmic exercises.
Although this information could be partially represented by UML class dia-
grams, this proposal has two advantages: (1) it presents both structure (e.g. the
class’sattributes)andbehaviour(e.g.‘toString()”sexpectedreturn-value);and,
(2) the composition relationship is less explicit than it would be in UML.
Fig.3. Class declaration exercises. Students should infer that the ‘Person’ class has a
composition relationship with the ‘Apartment’ class. Some data-types were omitted,
since students should be able to derive them (e.g. name should be a ‘String’).
Inheritance diagram These diagrams are an extension (or adaptation) of the
previouslymentionedclassdeclarationdiagrams,specificallytailoredforexercis-
inginheritancerelationshipsbetweenclasses(i.e.generalizationviainheritance).
The notation is exactly the same, however the goal is different. Instead of de-
scribing all the classes that the students must implement (as in class diagrams),
they only describe the child classes and students must determine the structure
andbehaviouroftheparentclass(whichisabsentfromthediagram).Afterthis,
they must implement all classes (structure and behavior), both those explicitly
outlinedinthediagramandclassesthatarenotpresentinthediagrambuthave
been identified by the students.A Picture Is Worth a Thousand Words 7
Figure 4 presents one of these diagrams, in which two similar concepts are
represented: one representing ‘Managers’ and the other representing ‘IT Tech-
nicians’. Students are expected to infer that these two concepts should share a
common super-class since they have common attributes (e.g. name, salary) and
similar behaviours (e.g. salary calculation).
Fig.4.Exampleinheritanceexercise.Studentswereexpectedtounderstandthatsome
commonelementsexistbetweenthe‘Manager’andthe‘ITTechnician’andthuscreate
an inheritance relationship with a super-class above those.
State transition rules diagram These diagrams should be used when the
exercisehascertainclasseswhichcantransitionbetweendifferentinternalstates,
with rules which guide those transitions. In practical terms, these diagrams are
simplified state transition diagrams which only display valid transitions. See
Figure 6 for an example. That figure’s diagram explains the name of the action
which changes the state (e.g. ‘plan’, ‘start’), as well as what information should
exist inside the object in each state. For example, a newly created task only has
value for the ‘description’.
These diagrams are useful for providing complementary information to the
other exercises. For example, the state transition diagram of Figure 6 and the
state-changefunctiondiagramofFigure5werepartofthesameassignmentand
are complementary: for students to correctly implement the ‘f06()’ function, to
assigna‘Task’toan‘Employee’,theymustunderstandthattheassociationcan
only be done if the Task is in the ‘Planned’ state. The diagram explains this
connection to the student in the form of a small asterisk near the function’s
return value.
3.2 Videos
While some exercises are easy to describe with a diagram containing 2 or 3
example inputs, or even with 2 or 3 diagrams, in more complex assignments,
where there is a significant amount of user interaction or input validation, or
projects which span multiple weeks or months, creating diagrams to represent
all relevant behaviours and interactions is complex and time consuming.
In such cases, it is more practical to create video demonstrations of the
expected behaviours. The videos can include a mixture of behavioural demon-
strationsandstaticdiagrams.Moreconcretely,werecommendusingdemonstra-
tions for explaining user interactions, and diagrams for presenting any manda-
tory protocols which the students must follow. The videos should start with the8 B. Pereira Cipriano et al.
Fig.5.Examplestate-changefunctionexercise.Studentswereexpectedtounderstand
that the function would receive 3 objects (a ‘Company’, an ‘Employee’, and a ‘Task’)
and associate them if some rules were respected. The rules were described in another
diagram which can be seen in Figure 6.
Fig.6. State transition rules. This is a simplified state transition diagram which only
display valid transitions.A Picture Is Worth a Thousand Words 9
demonstrationoftheexpectedfunctionalityfromtheuser’sperspective,andonly
afterwards go into required implementation details. A video5 with an example
OOP course project is available online6.
Table 1. The quantitative questions used in the survey, organized by Research Ques-
tion.
RQQ. #Question
‘It is easier to understand the objective of the exercise when presented in
1 1
the form of a diagram, than when it is presented textually.’
‘When I come across a diagram-based exercise, I tend to think more carefully
1 2
before writing code, compared to what I do with exercises described textually.’
‘In general, I prefer exercises based on diagrams over exercises
1 3
described textually.’
‘Considering the interpretation of the exercise, the absence of the function
1 4
name in the diagrams ...’
‘It is easier to interpret the project when presented in video format,
2 1
due to the combination of text, images and audio.’
‘Video statements fall short when compared to statements in natural language,
2 2
because it is more difficult to take notes on paper’
‘I feel more motivated to do the project when the statement is in video
2 3
than when it is in natural language.’
‘I find it easier to develop the project with the video statement than with
2 4
the traditional model’.
‘I am more likely to use GPT/Bard with textual exercise descriptions than
3 1
with diagrams and/or videos’.
‘Diagram and/or video exercises effectively prevent abuse of GPT/Bard,
3 2 as they force me to prepare a prompt instead of simply copying the
description into GPT/Bard.’
‘I consider diagram and/or video exercises a good step towards making me more
3 3
prepared for a professional future where I have to interact with GPT/Bard.’
4 Method
4.1 Experimental context
To evaluate the proposed diagrams and videos as formats to present students
with OOP tasks, we applied them in the 2023/24 edition of a University course
focused on Object-Oriented Design and Programming. In this course, students
are exposed to OO analysis and design, as well as Java implementation details.
They are also challenged to solve a number of practical programming activities,
withthecoursefollowingamixedapproachwithbothexercise-basedandproject-
based learning [17], supported by an AAT [21].
5 Narrated in Portuguese, but auto-translated captions are available.
6 https://www.youtube.com/watch?v=LkyEaAVK6yU10 B. Pereira Cipriano et al.
The diagrammatic notation and videos proposed in this paper were applied
to the majority of these practical activities, which were originally described in
naturallanguage.Diagramswereusedtodescribetheexercises,whiletheproject
was described in video.
4.2 Evaluation
To assess the effectiveness of this novel notation, students were administered a
structuredanonymousquestionnaireduringthe12thweekofthecourse.Atthis
juncture,studentshadbeenexposedtomultiplediagramsofalltypespresented
inthispaper:thisincludes9assignments,eachcomposedofmultiplediagrams7.
Also, students were actively engaged in the project.
The questionnaire was composed of 11 quantitative questions, each associ-
ated with one RQ. Ten of the questions – presented in Table 1 – used a stan-
dard5-pointLikertscale(stronglydisagreetostronglyagree).Onequestionwas
categorical, with three possible options. Finally, the questionnaire included a
qualitative question for students to provide open-response comments.
5 Results
The course had 115 enrolled students. Among them, 84 students had some level
ofparticipation,submittingatleastonediagram-basedassignment.Forthose84
students, the average number of submitted assignments was 6.25 and the mean
was 7 (out of the 9 available assignments). The minimum number of submitted
assignments was 1, the maximum and mode were both 10, and 12 students
submitted all assignments. A total of 56 results participated in the survey.
5.1 RQ1: Preference for diagram-based exercises
This section of the survey was dedicated to understanding students opinions
on diagram-based exercises, in comparison to text-based exercises that they are
familiar with from previous courses (RQ1). Response distributions to the three
quantitative questions associated with RQ1 are presented in Figure 7. Although
the results are somewhat balanced between the agreement and disagreement
sides, in the first two questions, more students agree with the benefits of dia-
grammatic exercises. However, in the third question, results were skewed to the
disagreement side, indicating that most students do not prefer diagram-based
exercises over text-based ones.
Finally, we asked students to evaluate the impact of the absence of the func-
tion name from the diagrams (Table 1: Question 4, RQ1). This question was
categorical and had 3 possible options: ‘It causes me some difficulties’ (selected
by 37 students or 66.07%), ‘It doesn’t affect me’ (14 students or 25.00%), and,
‘It causes me a lot of difficulties’ (5 students or 8.93%). These results somewhat
surprised us, since we thought that the majority of students would report the
lack of function names to have caused them more difficulty.
7 The total number of unique diagrams was 72.A Picture Is Worth a Thousand Words 11
Fig.7. Students opinions with regards to diagram-based exercises when compared to
natural language exercises (RQ1).
Fig.8. Distribution of student replies to questions related with video-based exercises
(RQ2).
Fig.9.Student’sopinionsondiagramandvideoexercises’effectontheiracademicand
professional LLM usage (RQ3).12 B. Pereira Cipriano et al.
5.2 RQ2: Interpretability and motivation of video-based projects
Figure8presentstheresultsforeachquestionrelatedwithRQ2.Thevastmajor-
ity of students indicated that videos make projects easier to interpret. However,
with regards to the ease of note taking, most students feel that videos are not
as good as textual descriptions. Videos also seem to have a positive impact on
students’ motivation (Table 1: Question 3, RQ2). Finally, the majority of stu-
dents reported that developing the project with the video statement is easier
than with the traditional text-based assignments.
5.3 RQ3: Use and Reliance on LLMs
Figure 9 presents results for questions related with RQ3. The majority of stu-
dents indicate that they would be more likely to resort to an LLM when solving
textual exercises than with diagrammatic or video exercises. The vast majority
of students also agree that diagrams and videos are an effective way of prevent-
ing LLM abuse. Finally, students agree that these new formats will help them
be more prepared for a potential professional future where they have to interact
with an LLM to produce code [2].
6 Discussion
The proposed notation and diagrams present a new visual language relevant for
the specificities of teaching and learning OO design and programming. These
diagrams require that students first understand the problem by analysing the
diagrammatic sample sets, and then proceed to define an object model and/or
algorithm which solves it, with or without the help of a LLM.
Diagrams and videos can prescribe more or less directed tasks, according to
educators’ goals, by using clues such as ‘Implement in class Main’, ‘Represent
these concepts’, and so on. However, educators should avoid exposing too much
information textually or verbally, since some textual instructions can be parsed
from images [13] and audio interpretation tools are also emerging [19].
We consider this model to have three advantages: (1) it forces students to
interprettestcasesandinferaproblemdescription;(2),itshouldpreventdirect
‘copy-and-prompting’ from the assignment description to the LLM; and, (3)
it requires students that wish to use an LLM to create prompts that guide
it towards the goal. Although these last two ideas might seem contradictory,
we believe that both have some pedagogical value, since it is important that
students gain some experience with using LLMs in an authentic way, as helpers
for solving coding problems, due to the likelihood that they will use these tools
professionally [2,4].
During the experiment, we observed some interesting student behaviours
which we believe should guide future work on this topic. One interesting as-
pect was, at least in some cases, students’ difficulties in interpreting the dia-
grams prompted interesting discussions between students and teachers. WhenA Picture Is Worth a Thousand Words 13
the doubts were not obvious, we would engage with the student and help them
reach the expected interpretation by themselves. For example, when a student
asked “What does this ‘N + 1’ tasks mean?”, the teacher pointed out that, be-
forethefunctioncall,theobjectindicated‘Ntasks’,andthenaskedthestudent
what they thought was happening that resulted in the change from ‘N tasks’ to
‘N + 1 tasks’. As for the videos, at least one student transcribed the video to
a text document. We informally asked the student for their reasons, and they
indicated doing it to support offline work because they didn’t have an internet
connection at all times. One other student used a tool to download Youtube’s
automatic captions for the video assignment. Although these techniques could
be used to exploit the video assignment, since not all information is obtainable
from the narration, the videos should remain LLM-resistant for the time being.
BesidescounteringLLMover-reliance,thesurveyshowsthattheseapproaches
also seem to have other benefits, such as the apparent positive impact on stu-
dents’ motivation levels. Moreover, the qualitative question also allowed us to
identify another added benefit, since multiple students reported the need to
analyse and reflect on the diagram’s contents, in order to fully understand it.
For example, participant S7 indicated that “Not knowing specifically what the
function should do [...], I had to ‘spend’ some time trying to understand that”.
Anotherparticipant,S39,commentedthat“SinceIdon’thavea[function]name,
I don’t immediately know what I’m supposed to do, I have to think and reflect
on the diagram”.Finally,S56indicatedthat“Sometimes, just the diagram with-
out a brief contextual explanation made the exercise more difficult to understand
than solving it.”. As such, it appears that diagrams might also promote some
development of analytical skills, and this would be an interest avenue to explore
more rigorously in future work.
6.1 GPT-4 and Bard’s vision capabilities
Recently, both GPT-4 and Bard have become capable of interpreting images.
Given the potential threat to the proposed diagrams, we conducted initial ex-
plorations of the vision capabilities of these tools. To achieve this, GPT-4 and
Bard where supplied with the same information that students had: a diagram
containing some introductory text and Figure 6, and then a second diagram
which was equal to Figure 5. This experiment was repeated 3 times with the
same images, but with slightly different prompts and the generated code was
evaluated considering 3 compilation and 3 logical items. None of the attempts
yielded a correct implementation of the ‘f06()’ function, with both models gen-
eratingcodewithcompilationerrorsand/orlogicalerrors8.Initsbestattempt,
GPT-4 generated a function that compiled correctly, but the function’s logic
8 GPT-4’s diagram scores: Best compilation: 3/3 (experiment #3). Best logic: 1/3
(experiment #2). Worst compilation: 0/3 (experiments #1 and #2). Worst logic:
0/3(experiments#1and#3).Bard’sdiagramscores:Best/Worstcompilation:0/3
(all experiments). Best logic: 1/3 (experiment #2). Worst logic: 0/3 (experiments
#1 and #3).14 B. Pereira Cipriano et al.
was hardcoded to the example provided in the diagram and would not work
with different input values. Bard’s best attempt was also problematic: it failed
to create the function in the prescribed class, the argument order was wrong, it
wasnotdeclaredasfinalandthereturn-typewasvoidinsteadofboolean.Asfor
thelogic,thecodefailstocorrectlyvalidatethe‘Task”sstatetransitionandalso
fails to check if the ‘Employee’belongs to the company. The only sub-behaviour
that would work as expected was the assignment of the Task to the Employee.
Finally, instead of returning the requested boolean, an ‘Exception’ was being
thrown.Wealsosuppliedbothmodelswithanequivalenttext-baseddescription
of the exercise. With the textual input, GPT-4 was able to generate an almost
correct solution, failing only to declare the function as ‘static’ 9. Bard’s solution
tothetext-basedvariantwasalsobetterthanallitsdiagram-basedattempts10.
The logs of our experiments are available online11.
ThisexperimentshowsthatLLMsaremuchbetterathandlingOOPexercises
described textually, than diagram-based exercises. As such, diagrams seem a
good approach to limit students’ over-reliance on LLMs, at least for the time
being.
7 Limitations
One limitation is the significant proportion of students opting for the neutral
option in most quantitative questions. This could potentially have skewed some
of our interpretations.
In relation to the questions about the impact of diagrams and videos on the
utilization of LLMs, it is conceivable that some students may not have provided
honest responses to this question given concerns around academic integrity.
8 Conclusions
We believe that the biggest challenge in adapting courses and classes to this
bravenewworldwhereLLMswithcodegenerationcapacityareeasilyaccessible
is creating exercises that are hard for an LLM to solve, but are still accessible
and can be solved by students. In this paper we present a novel pedagogical
approach to describe OO programming exercises and projects. This notation
allowed us to replace the previously used natural language descriptions which
could be handled, to some degree, by GPT-3.5, GPT-4 and Bard. The proposed
notationwaswellreceivedbyourstudents,whoalsoagreethatithelpedmitigate
LLM over-reliance.
9 GPT-4’s textual scores: Compilation: 2/3. Logic: 3/3.
10 Bard’s textual scores: Compilation: 3/3. Logic: 1.5/3.
11 https://doi.org/10.5281/zenodo.10547278A Picture Is Worth a Thousand Words 15
References
1. Allen, J.M., Downey, K., Miller, K., et al.: Many small programs in cs1: Usage
analysis from multiple universities. In: 2019 ASEE Annual Conference & Exposi-
tion”.p.1–13.No.10.18260/1-2–33084,ASEEConferences,Tampa,Florida(June
2019), https://peer.asee.org/33084
2. Alves, P., Cipriano, B.P.: The centaur programmer – How Kasparov’s Advanced
Chess spans over to the software development of the future (2023)
3. Babe, H.M., Nguyen, S., Zi, Y., Guha, A., Feldman, M.Q., Anderson, C.J.: Stu-
denteval: A benchmark of student-written prompts for large language models of
code (2023)
4. Barke, S., James, M.B., Polikarpova, N.: Grounded copilot: How programmers
interact with code-generating models. Proc. ACM Program. Lang. 7(OOPSLA1)
(apr2023).https://doi.org/10.1145/3586030,https://doi.org/10.1145/3586030
5. Becker, B.A., Denny, P., Finnie-Ansley, J., Luxton-Reilly, A., Prather, J.,
Santos, E.A.: Programming is hard - or at least it used to be: Educa-
tional opportunities and challenges of ai code generation. In: Proceedings of
the 54th ACM Technical Symposium on Computer Science Education V. 1.
p. 500–506. SIGCSE 2023, Association for Computing Machinery, New York,
NY, USA (2023). https://doi.org/10.1145/3545945.3569759, https://doi.org/
10.1145/3545945.3569759
6. Cipriano,B.P.,Alves,P.:LLMsStillCan’tAvoidInstanceof:AnInvestigationInto
GPT-3.5, GPT-4 and Bard’s Capacity to Handle Object-Oriented Programming
Assignments http://arxiv.org/abs/2403.06254v1
7. Cipriano, B.P., Alves, P.: GPT-3 vs Object Oriented Programming As-
signments: An Experience Report. In: Proceedings of the 2023 Confer-
ence on Innovation and Technology in Computer Science Education V. 1.
p. 61–67. ITiCSE 2023, Association for Computing Machinery, New York,
NY, USA (2023). https://doi.org/10.1145/3587102.3588814, https://doi.org/
10.1145/3587102.3588814
8. Denny, P., Kumar, V., Giacaman, N.: Conversing with copilot: Exploring prompt
engineering for solving cs1 problems using natural language. In: Proceedings
of the 54th ACM Technical Symposium on Computer Science Education V. 1.
p. 1136–1142. SIGCSE 2023, Association for Computing Machinery, New York,
NY, USA (2023). https://doi.org/10.1145/3545945.3569823, https://doi.org/
10.1145/3545945.3569823
9. Denny, P., Leinonen, J., Prather, J., Luxton-Reilly, A., Amarouche, T., Becker,
B.A., Reeves, B.N.: Promptly: Using prompt problems to teach learners how to
effectively utilize ai code generators. arXiv preprint arXiv:2307.16364 (2023)
10. Denny, P., Leinonen, J., Prather, J., Luxton-Reilly, A., Amarouche, T., Becker,
B.A.,Reeves,B.N.:Promptproblems:Anewprogrammingexerciseforthegener-
ativeaiera.In:Proceedingsofthe55thACMTechnicalSymposiumonComputer
ScienceEducationV.1.p.296–302.SIGCSE2024,AssociationforComputingMa-
chinery, New York, NY, USA (2024). https://doi.org/10.1145/3626252.3630909,
https://doi.org/10.1145/3626252.3630909
11. Denny, P., Prather, J., Becker, B.A., Finnie-Ansley, J., Hellas, A., Leinonen, J.,
Luxton-Reilly, A., Reeves, B.N., Santos, E.A., Sarsa, S.: Computing Education
in the Era of Generative AI. Commun. ACM 67(2), 56–67 (Jan 2024), https:
//doi.org/10.1145/362472016 B. Pereira Cipriano et al.
12. Finnie-Ansley, J., Denny, P., Becker, B.A., Luxton-Reilly, A., Prather, J.: The
robots are coming: Exploring the implications of openai codex on introductory
programming.In:Proceedingsofthe24thAustralasianComputingEducationCon-
ference. pp. 10–19 (2022)
13. Hou, I., Man, O., Mettille, S., Gutierrez, S., Angelikas, K., MacNeil, S.: More
robots are coming: Large multimodal models (chatgpt) can solve visually diverse
images of parsons problems. arXiv preprint arXiv:2311.04926 (2023)
14. Kazemitabaar, M., Chow, J., Ma, C.K.T., Ericson, B.J., Weintrop, D., Gross-
man, T.: Studying the effect of ai code generators on supporting novice learners
in introductory programming. In: Proceedings of the 2023 CHI Conference on
Human Factors in Computing Systems. CHI ’23, Association for Computing Ma-
chinery, New York, NY, USA (2023). https://doi.org/10.1145/3544548.3580919,
https://doi.org/10.1145/3544548.3580919
15. Krawczyk, Jack: Bard’s latest update: more features, lan-
guages and countries. https://blog.google/products/bard/
google-bard-new-features-update-july-2023/ (2023), [Online; last accessed
16-December-2023]
16. Lau, S., Guo, P.: From” ban it till we understand it” to” resistance is futile”:
How university programming instructors plan to adapt as more students use ai
code generation and explanation tools such as chatgpt and github copilot. In:
Proceedings of the 2023 ACM Conference on International Computing Education
Research-Volume 1. pp. 106–121 (2023)
17. Lenfant, R., Wanner, A., Hott, J.R., Pettit, R.: Project-based and assignment-
based courses: A study of piazza engagement and gender in online courses. In:
Proceedings of the 2023 Conference on Innovation and Technology in Computer
Science Education V. 1. pp. 138–144 (2023)
18. Liffiton, M., Sheese, B.E., Savelka, J., Denny, P.: CodeHelp: Using Large Lan-
guage Models with Guardrails for Scalable Support in Programming Classes.
In: Proceedings of the 23rd Koli Calling International Conference on Comput-
ing Education Research. Koli Calling ’23, Association for Computing Machinery,
New York, NY, USA (2024). https://doi.org/10.1145/3631802.3631830, https:
//doi.org/10.1145/3631802.3631830
19. OpenAI: Chatgpt can now see, hear, and speak. https://openai.com/
blog/chatgpt-can-now-see-hear-and-speak (2023), [Online; last accessed 12-
December-2023]
20. Ouh, E.L., Gan, B.K.S., Shim, K.J., Wlodkowski, S.: Chatgpt, can you generate
solutions for my coding exercises? an evaluation on its effectiveness in an under-
graduate java programming course. arXiv preprint arXiv:2305.13680 (2023)
21. Paiva, J.C., Leal, J.P., Figueira, A.: Automated assessment in computer science
education:Astate-of-the-artreview.ACMTrans.Comput.Educ.22(3)(jun2022).
https://doi.org/10.1145/3513140, https://doi.org/10.1145/3513140
22. Porter, L., Zingaro, D.: Learn AI-Assisted Python Programming With GitHub
Copilot and ChatGPT. Manning, Shelter Island, NY, USA (2023), https://www.
manning.com/books/learn-ai-assisted-python-programming
23. Prather, J., Denny, P., Leinonen, J., Becker, B.A., Albluwi, I., Craig, M., Ke-
uning, H., Kiesler, N., Kohn, T., Luxton-Reilly, A., MacNeil, S., Petersen, A.,
Pettit, R., Reeves, B.N., Savelka, J.: The robots are here: Navigating the gener-
ative ai revolution in computing education. In: Proceedings of the 2023 Work-
ing Group Reports on Innovation and Technology in Computer Science Edu-
cation. p. 108–159. ITiCSE-WGR ’23, Association for Computing Machinery,A Picture Is Worth a Thousand Words 17
New York, NY, USA (2023). https://doi.org/10.1145/3623762.3633499, https:
//doi.org/10.1145/3623762.3633499
24. Savelka, J., Agarwal, A., An, M., Bogart, C., Sakr, M.: Thrilled by your progress!
large language models (gpt-4) no longer struggle to pass assessments in higher
education programming courses. In: Proceedings of the 2023 ACM Confer-
ence on International Computing Education Research V.1. ICER 2023, ACM
(Aug 2023). https://doi.org/10.1145/3568813.3600142, http://dx.doi.org/10.
1145/3568813.3600142
25. Savelka, J., Agarwal, A., Bogart, C., Song, Y., Sakr, M.: Can generative
pre-trained transformers (gpt) pass assessments in higher education program-
ming courses? In: Proceedings of the 2023 Conference on Innovation and
Technology in Computer Science Education V. 1. ITiCSE 2023, ACM (Jun
2023). https://doi.org/10.1145/3587102.3588792, http://dx.doi.org/10.1145/
3587102.3588792
26. Sheard, J., Denny, P., Hellas, A., Leinonen, J., Malmi, L., Simon: Instructor
perceptions of ai code generation tools - a multi-institutional interview study.
In: Proceedings of the 55th ACM Technical Symposium on Computer Science
Education V. 1. p. 1223–1229. SIGCSE 2024, Association for Computing Ma-
chinery, New York, NY, USA (2024). https://doi.org/10.1145/3626252.3630880,
https://doi.org/10.1145/3626252.3630880
27. Yeti¸stiren, B., O¨zsoy, I., Ayerdem, M., Tu¨zu¨n, E.: Evaluating the code quality of
ai-assisted code generation tools: an empirical study on github copilot, amazon
codewhisperer, and chatgpt. arxiv preprint arxiv: 230410778. 2023 (2023)