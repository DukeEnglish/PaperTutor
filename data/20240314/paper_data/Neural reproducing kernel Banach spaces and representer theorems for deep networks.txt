NEURAL REPRODUCING KERNEL BANACH SPACES
AND REPRESENTER THEOREMS FOR DEEP NETWORKS
F.BARTOLUCCI,E.DEVITO,L.ROSASCO,ANDS.VIGOGNA
ABSTRACT. Studying the function spacesdefined byneuralnetworks helpsto under-
standthecorrespondinglearningmodelsandtheirinductivebias. Whileinsomelim-
itsneuralnetworkscorrespondtofunctionspacesthatarereproducingkernelHilbert
spaces,theseregimesdonotcapturethepropertiesofthenetworksusedinpractice.In
contrast,inthispaperweshowthatdeepneuralnetworksdefinesuitablereproducing
kernel Banach spaces. These spaces are equipped with norms that enforce a form of
sparsity,enablingthemtoadapttopotentiallatentstructureswithintheinputdataand
theirrepresentations.Inparticular,leveragingthetheoryofreproducingkernelBanach
spaces,combinedwithvariationalresults,wederiverepresentertheoremsthatjustify
thefinitearchitecturescommonlyemployedinapplications. Ourstudyextendsanalo-
gousresultsforshallownetworksandcanbeseenasasteptowardsconsideringmore
practicallyplausibleneuralarchitectures.
1. INTRODUCTION
Neural networks define functions by composing linear and nonlinear maps in a
multi-layer (deep) architecture. While easy to implement, the corresponding mod-
els are hard to analyze since they are nonlinearly parameterized. Understanding the
propertiesofthefunctionspacesdefinedbydifferentneuralnetworkarchitecturescan
giveinsightsinthecorrespondinglearningmodels. Further,itcanprovideindications
on the underlying inductive bias, namely what functions can be approximated and
learnedefficiently by a given classof networks.
In some overparametrized regimes, neural networks can be seen to define Hilbert
spaces of functions and in particular reproducing kernel Hilbert spaces (RKHS) [1].
For instance, a classical observation is that shallow networks with infinitely many
random units correspond to RKHS, with reproducing kernels depending on the con-
sidered nonlinearity [15]. Thisregime, also known asthe Gaussian Process (GP) limit,
hasconnectionswithmodelssuchasrandomfeatures[19].Thelimitsofmorecomplex,
possibly non shallow architectures can also be derived and characterized in terms of
RKHS, see e.g. [12]. Another infinite-width limit in which neural networks are de-
scribed by RKHS is the so-called lazy training regime [9]. In this limit, the network
weightsevolvelittleduringtheoptimizationandcanbewellapproximatedbyalinear
approximation around a random initialization. Inthis case aswell, the corresponding
function spaces are RKHS, and the associated kernel is called neural tangent kernel
(NTK) [13]. Again, neural tangent kernels and corresponding RKHS can be derived
for a varietyof architectures, seee.g. [5].
In fact, neither the above GP/random features limit nor the NTK/lazy training
regimeseemtocapturekeyaspectsofneuralnetworksmodels[11,4]. Resultsforshal-
low networks suggest that neural networks might favor functions with small norms
that are not Hilbertian but rather associated to Banach spaces [2]. In turn, represen-
ter theorems associated to such norms allow to characterize finite-width networks
1
4202
raM
31
]LM.tats[
1v05780.3042:viXracommonly used in practice [22, 16, 24, 17, 18]. These observations have sparked con-
siderable interest in understanding the Banach spaces associated to neural networks.
Notably, one possibility is to consider extensions of classical splines [28, 27, 26, 25].
Another possibility is to consider reproducing kernel Banach spaces (RKBS) [30, 14],
see e.g. [3]and references therein.
In this paper, we develop the latter approach tackling the extension from shallow
to deep networks. The study of Banach spaces associated to deep architectures and
correspondingrepresentertheoremswasstartedin[18],wheredeeparchitectureswith
ReLU activations and finite rank constraints at each layer are considered. The latter
requirement is not natural and is mainly due to technical reasons. Indeed, the finite
rank constraint allows for the construction of layers as concatenation of real valued
functions studied forshallow networks.
Inourstudy,weproposeanapproachwhichallowstoconsidermoregeneralactiva-
tionsandespeciallytoavoidthefiniterankconstraints. Thisrequiresmoresubstantial
developments using vector Radon measures to address the challenges posed by po-
tentiallyinfinite-dimensional hiddenlayers. Ourfirstcontribution istodefinearepro-
ducing kernel Banach space which describes an infinite-width limit of a deep neural
network with an associated norm promoting sparsity. We dub such a space a neural
RKBS. Then, we provide a representer theorem for a large class of nonlinearities that
shows how optimal neural networks minimizing empirical objectives can be taken to
haveafinitestructure ateverylayer. Thisresultextendsanalogousresults forshallow
networks. It implies that commonly used networks are optimal in the sense that they
are solutions ofa suitable variational problem.
The rest of the paper is organized as follows. In Section 2 we review the main tech-
nical ingredients of our construction, namely reproducing kernel Banach spaces and
vector Radon measures. In Section 3 we introduce several notions of RKBS to model
functional properties ofneuralnetworks, leadingtothe construction ofdeepandneu-
ral RKBS. In Section 4 we prove our represent theorems on deep and neural RKBS. In
AppendixAwecollectvariational resultsandextremepointcharacterizationsusedto
prove ourrepresenter theorems.
TABLE 1. Notation
symbol definition symbol definition
B(X,Y) boundedlinearmapsX →Y M(Θ) M(Θ,R)
X′ continuousdualofX kµk totalvariationnormofmeasureµ
TV
Xh·,·i X′ pairingonX,X′ δ θ Diracdeltaatθ
h·,·i innerproductonX C (Θ,Y) continuousfunctionsΘ→Yvanishingat∞
X 0
k·k normonX C (Θ) C (Θ,R)
X 0 0
B(Θ) Borelσ-algebraonΘ B (r) ballonXofradiusr
X
M(Θ,Y) vectormeasuresonΘwithvaluesinY Ext(Q) extremalpointsofQ
2. PRELIMINARIES
In this section, we review the notion of an architecture and building blocks of deep
neural networks. Our perspective is to cast the construction of neural networks in
the theory of reproducing kernel Banach spaces, as functions parametrized by vector
measures.
2Neural networks. We start by setting up some notation, considering to the case of
fully connected, feed-forward architectures.
Definition 2.1 (Fully connected feed-forward neural network). Let σ : R → R be a
(nonlinear) function, L ≥ 1 an integer and d = d ,d ,...,d ,d = p ≥ 1 a family
0 1 L L+1
of integers. A (fully connected feed-forward) neural network from
Rd
into
Rp
with
activation function σ, depth L and widths d ,...,d is any function f : Rd → Rp of the
1 L
form f(x) = x(L+1) where
x(1) = W(1)x+b(1) ∈ Rd 1
(1)
(x(ℓ+1) = W(ℓ+1)σ(x(ℓ))+b(ℓ+1) ∈ Rdℓ+1 ℓ = 1,...,L
for some weights W(ℓ) ∈ Rdℓ×dℓ−1 and biases (or offsets) b(ℓ) ∈ Rdℓ, and where the
function σ isassumedtoapplyon vectors componentbycomponent. Wecall aneural
network shallow (or a one-hidden layer network) if L = 1, deep if L > 1. A neuron is a
functionoftheform φ(z) = σ(hz,wi+b). Thevectorσ(x(ℓ))forℓ = 1,...,Listheℓ -th
ℓ
hiddenlayerofthe network, hence dℓ isthe numberofneurons atthe -th hiddenlayer.
The input and output dimensions d,p are prescribed by the problem. Given the
problem, the user chooses an activation σ, a depth L and widths d ,...,d . After this
1 L
choice, the architecture is fixed, and the
parametersW(ℓ),b(ℓ)
are obtained optimizing
an empirical loss on the available data. Optimization is thus performed on the set of
functions (1), with
(W(ℓ),b(ℓ))
in
Rdℓ×dℓ−1 ×Rdℓ.
The set of neural networks with a
fixed architecture is parametric, albeit hence nonlinear and high-dimensional. Next,
we will define structured nonparametric linear spaces on which optimization has a
solution in the set of neural networks (1). This suggests that neural networks may
be seen as parametric solutions of a nonparametric model, where the architecture it-
self can be obtained by training, rather than arbitrarily chosen by the practitioner. To
define such a model, we need the notion of a reproducing kernel Banach space and
vector measures.
Reproducing kernel Banach spaces. We will assume the following minimal defini-
tion, that readily generalizes reproducing kernel Hilbert spaces to a Banach setting.
Werefer to[30, 14]fora comprehensive overview.
Definition 2.2 (Vector valued reproducing kernel Banach space). Let X be a set and
Y a Banach space. A reproducing kernel Banach space (RKBS) H on X with values in Y
is a Banach space of functions f : X → Y with continuous pointwise evaluation, that
is, for all x ∈ X there is C > 0 such that kf(x)k ≤ C kfk for all f ∈ H. In other
x Y x H
words, the map f 7→ f(x) isin B(H,Y) forall x ∈ X.
ThefollowingpropositioncharacterizesRKBSasBanachfeaturespaces,thatis,spaces
parametrized on Banach spaces.
Proposition 2.3. Let X be asetand Y aBanach space. Consider thefollowing statements.
(a) A space H of functions f : X → Y isaRKBS.
(b) Thereis aBanach space F and amap φ : X → B(F,Y) suchthat
(i) H = {f : µ ∈ F} where f = φ(·)µ ;
µ µ
(ii) kfk = inf{kµk : µ ∈ F, f = f } .
H F µ
(c) Thereis aBanach space F and amap ψ : X → B(Y′,F′) suchthat
3(i) H = {f : µ ∈ F} where hf (·),y′i = hµ,ψ(·)y′i .
µ Y µ Y′ F F′
(ii) kfk = inf{kµk : µ ∈ F, f = f } .
H F µ
Then (a) and (b) are equivalent and each one implies (c). Moreover, if Y is reflexive (in partic-
ular Hilbert),then(a), (b)and (c) are allequivalent.
Proof. To seethat (a)implies(b), take F = H anddefine
φ : X → B(F,Y) φ(x)f = f(x) .
Then (i) and (ii) of item (b) are clear. Let us prove that (b) implies (a). Clearly H is a
linearspace and k·k isa norm. Wethen show that the normed space H is complete.
H
Thelinearmapµ 7→ f haskernelN = kerφ(x). Sinceφ(x) isboundedforall x,
µ x∈X
kerφ(x) isclosed, hencesois N. Thus, F/N isaBanach space[20,Theorem 1.41],by
T
construction isomorphic to H, which is therefore complete. Next, we show that point
evaluations in H are continuous. For f ∈ H,let µ ∈ F such that f = f . Then
µ
kf(x)k = kφ(x)µk ≤ kφ(x)k kµk ,
Y Y B(F,Y) F
whence
kf(x)k ≤ inf kφ(x)k kµk = kφ(x)k kfk .
Y B(F,Y) F B(F,Y) H
µ∈F:f=fµ
Theimplicationfrom(b)to(c)followseasilyconsideringψ(x)asthedualmapofφ(x).
Finally,notethat(i)in(c)defines f asafunctionfromX toY′′. Hence,ifY isreflexive,
µ
it defines f : X → Y. From here, following the proof of the implication from (b) to
µ
(cid:3)
(a), one can prove that (c) implies(a).
Vector Radon measures. In view of Proposition 2.3, RKBS can be constructed choos-
ing a feature space parametrizing functions. Thinking of a neural network layeras an
atomic integration, we will defineRKBS parametrizedby measures. Ashiddenlayers
have vectorial outputs, we needthe concept of vector measure [10].
Definition 2.4 (Vector measures). Let Θ be a (Hausdorff) locally compact, second
countable topological space, and let Y be a Banach space. A vector (Radon) measure
on Θ withvaluesin Y isamapµ: B(Θ) → Y suchthat, forall A ∈ B(Θ) andall {A }
i
partitions of A,1
µ(A) = ∑ µ(A ) ,
i
i
where the sum converges absolutely in the k·k norm. We denote by M(Θ,Y) the
Y
spaceofvectormeasureson Θ withvaluesin Y. Ifµ ∈ M(Θ,Y), forall A ∈ B(Θ) we
define
|µ|(A) = sup∑kµ(A )k ,
i Y
{A i} i
where the supremum is taken over all partitions of A. Then, |µ| is a bounded positive
measure on Θ, which we call the total variation of µ. The space M(Θ,Y) is a Banach
space with respect to the total variationnorm
kµk = |µ|(Θ) .
TV
1 Apartitionof a Borelset A isanumerable familyof Borelsets {A } such that A ∩A = ∅ for all
i i j
i 6= jand A = A.
i i
4
SRemark2.5. Fixavectormeasureµ. SinceΘisasecondcountablespace,forally′ ∈ Y′,
hµ(·),y′i is a scalar regular (Radon) measure, hence µ is a regular vector measure
Y Y′
[21]. Since the elements of M(Θ,Y′) are regular measures, the space M(Θ,Y′) can
be identified with the dual of C (Θ,Y), the space of continuous functions from Θ to
0
Y that vanish at infinity, by (a generalization of) the Riesz representation theorem. In
particular, M(Θ,Y′) can be endowed with the product weak∗ topology, with respect
to which the closed balls are compact. This result was proved in [23] for compact Θ
(see also [21]), and then extended in [8] to locally compact Θ in the case where Y is
Hilbert. The proof in [8]holds true with the obvious modifications if Y isBanach.
3. NEURAL RKBS
Inthissection, weintroduce particularclassesofRKBS,incorporating generalprop-
erties inspired by neural networks architectures. In particular, we construct deep and
neuralRKBS,andshowthatneuralnetworksasintendedinDefinition2.1areelements
of neural RKBS. In this way, we provide neural networks with a Banach space func-
tional structure, viewing them as finite representations of an underlying nonparamet-
ricmodel.
IntegralRKBS. LetΘ,Ξ betwolocallycompact,secondcountabletopological spaces.
Let X bea set, and let Y be a RKBSon Ξ. Let
ρ : X ×Θ → R
be such that ρ(x,·) ∈ C (Θ) for all x ∈ X. For µ ∈ M(Θ,Y), let
0
f : X → Y f (x) = φ(x)µ = ρ(x,θ)dµ(θ) (2)
µ µ
Θ
Z
and
H = {f : X → Y : µ ∈ M(Θ,Y)}
µ
with
kfk = inf {kµk : f = f } . (3)
H TV µ
µ∈M(Θ,Y)
The space H thus defined is a (vector valued) RKBS, which we call an integral RKBS.
Moreover, we call Θ and Ξ the parameterspacesof H, and ρ its basisfunctions.
Since Y isaRKBS,foreveryξ ∈ Ξ there isδ ∈ Y′ suchthat y(ξ) = hy,δ i forall
ξ Y ξ Y′
y ∈ Y. Thus, definingthe (scalar)measure
µ(E | ξ) = hµ(E),δ i E ∈ B(Θ) ξ ∈ Ξ ,
Y ξ Y′
for all x ∈ X we have
f (x)(ξ) = ρ(x,θ)dµ(θ | ξ) .
µ
Θ
Z
DeepRKBS. LetLbeapositiveinteger. TakeasetX andBanachspacesX ,...,X .
0 1 L+1
For ℓ = 0,...,L, take RKBS Hℓ on Xℓ with valuesin Xℓ+1. The direct sum
H = H ⊕···⊕H
0 L
isa Banachspace with respectto the norm
kfk = kf k +···+kf k f = f ⊕···⊕ f .
H 0 H L H 0 L
0 L
To every f = f ⊕···⊕ f ∈ H we assign the function fdeep : X → X defined
0 L 0 L+1
by
fdeep = f ◦···◦ f .
L 0
5The linearspace H parametrizesthe nonlinearspace
Hdeep = {fdeep : X → X : f ∈ H} .
0 L+1
With slight abuse of language, we call the non-linear space Hdeep a RKBS of depth L
parametrized by the RKBS H = H ⊕···⊕H . If L > 1, we refer to Hdeep as a deep
0 L
RKBSp.
Moreover, we call Xℓ the layer spaces of H. In particular, X = X
0
is the input space,
Y = X
L+1
isthe output space,and Xℓ, ℓ = 1,...,L, are the hiddenlayerspaces.
DeepintegralRKBS. Let L bea positive integer. Ifthe RKBS Hℓ are integral,
Hℓ = {f
µℓ
: Xℓ → Xℓ+1 : µℓ ∈ M(Θ ℓ,Xℓ+1)} ,
with basis functions
ρℓ : Xℓ ×Θ ℓ → R ,
the corresponding nonlinearspace
Hdeep = {fdeep : X
0
→ X
L+1
: fdeep = f
L
◦···◦ f
0
, fℓ = f
µℓ
∈ Hℓ ,ℓ = 0,...,L}
iscalledanintegralRKBSofdepth Lparametrizedby H = H ⊕···⊕H and,if L > 1,
0 L
a deepintegralRKBS.
Afunction f : X → X ∈ Hdeep hasthe form f(x) = x(L+1) where
0 L+1
x(0) = x ∈ X
0
 x(ℓ+1) = ρℓ(x(ℓ) ,θℓ)dµℓ(θℓ) ∈ Xℓ+1 ℓ = 0,...,L .
 ZΘℓ

Neural RKBS. We are going to define a particular instance of deep integral RKBS
modeling on neural networks. In neural RKBS, the basis functions are defined by
activation functions. First, we fix the parameter spaces Θ ℓ and the layer spaces Xℓ as
follows:
Θ = {0,...,d} X = ℓ2({1,...,d}) = Rd
0 0
Θ ℓ = N Xℓ = ℓ2(N) ℓ = 1,...,L
Θ = {1,...,p} X = ℓ2({1,...,p}) = Rp ℓ = L+1 .
L+1 L+1
Note that X can be thought of as a function space on Θ by putting x(0) = 1 for all
0 0
x ∈ X .
0
Next, let σ : R → R be a Lipschitz activation function such that σ(0) = 0. This
impliesthat
σ(x) ∈ ℓ2(N) for all x ∈ ℓ2(N) , (4)
where σ applies on sequences component by component. Indeed, if C denotes the
σ
Lipschitz constant of σ, we have
|σ(x )| = |σ(x )−σ(0)| ≤ C |x −0| = C |x |.
n n σ n σ n
Inparticular, thisimpliesthat
lim σ(x ) = 0 for all x ∈ ℓ2(N) . (5)
n
n→∞
6Assumingσ(0) = 0isnotrestrictive. Infact, ifσ(0) 6= 0,thechoice oftheactivation
function σ′ = σ−σ(0) simplyimpliesascaling ofthe offsets. Then,we set
1 n = 0
ρ (x,n) = x ∈ Rd
0
(x
n
n = 1,...,d
1 n = 0
ρℓ(x,n) = x ∈ ℓ2(N) ℓ = 1,...,L .
(σ(x n−1) n ≥ 1
We call the resulting (deep) integral RKBS Hdeep a neural RKBS of depth L and, if
>
L 1, a deepneural RKBS.
Remark3.1. IfσisLipschitzwithLipschitzconstantC ,then,forallℓ = 1,...,L, x,x′ ∈
σ
ℓ2(N) and n ∈ N ,
|ρℓ(x,n)−ρℓ(x′ ,n)| ≤ |σ(x n−1)−σ(x n′ −1)| ≤ C σ|x n−1−x n′ −1|.
GeneralformofneuralRKBSfunctions. Anelement fdeep oftheneuralRKBSHdeep
ofdepth L isa composition of L+1 integral functions f ,...,f ,
0 L
Rd f 0 // ℓ2(N) // ··· // ℓ2(N) f L //44 R ,
fdeep
whereeach fℓ isdefinedviaameasureµℓ by fℓ = f µℓ. From layer0tolayer1,wehave
µ ∈ M({0,...,d},ℓ2(N)) ,
0
so that
d
µ = ∑ w(1) δ
0 m m
m=0
for a d+1-familyof vectors w(1) ,...w(1) ∈ ℓ2(N). Let b(1) = w(1) ∈ ℓ2(N) anddefine
0 d 0
the bounded operator
d
W(1) ∈ B(Rd,ℓ2(N)) W(1) x = ∑ w(1) x ,
m m
m=1
Then,for x ∈ Rd, the function f : Rd → ℓ2(N) is
0
d d
f (x) = ∑ w(1) ρ (x,m) = w(1) + ∑ w(1) x = W(1) x+b(1) ∈ ℓ2(N) ,
0 m 0 0 m m
m=0 m=1
and the scalarcomponents of f are
0
f (x) = hx,w(1) i +b(1) n ∈ N ,
0 n n· Rd n
where w(1) ∈ Rd with w(1) = (w(1) ) .
n· nm m n
For ℓ = 1,...,L−1, from layer ℓ to layer ℓ+1 we have
µℓ ∈ M(N ,ℓ2(N)) ,
so that
∞
µℓ = ∑ w( mℓ+1) δ
m
,
m=0
7for a countable familyof vectors
w(ℓ+1) ,...,w(ℓ+1)
,... ∈ ℓ2(N) such that
0 m
∞
kµℓk TV = ∑ kw( mℓ+1) k ℓ2(N) < +∞ . (6)
m=0
Asbefore, set b(ℓ+1) = w(ℓ+1) ∈ ℓ2(N) and
0
+∞
W(ℓ+1) ∈ B(ℓ2(N),ℓ2(N)) W(ℓ+1) x = ∑ w(ℓ+1) x ,
m m−1
m=1
where the seriesconverges absolutely in ℓ (N) due to (6). Hence,for x ∈ ℓ (N),
2 2
+∞ +∞
fℓ(x) = ∑ w( mℓ+1) ρℓ(x,m) = w 0(ℓ+1) + ∑ w( mℓ+1) σ(x m−1)
m=0 m=1
= W(ℓ+1)(σ(x))+b(ℓ+1) ∈ ℓ2(N) ,
where σ(x) ∈ ℓ2(N) by (4). Thecomponent of fℓ(x) are
fℓ(x) n = hσ(x),w( nℓ ·+1) i ℓ2 +b n(ℓ+1) n ∈ N ,
where
w(ℓ+1)
∈ ℓ2(N) and
w(ℓ+1)
=
(w(ℓ+1)
) .
n· nm m n
Finally, from layer L to layer L+1,we have
µ ∈ M(N ,Rp) ,
L
so that
∞
µ = ∑ w(L+1) δ ,
L m m
m=0
for a countable familyof vectors w(L+1) ,...,w(L+1) ,... ∈ Rp such that
0 m
∞
∑ kw( mℓ+1) kRp < +∞ . (7)
m=0
Asbefore, set b(L+1) = w(L+1) ∈ ℓ2(N) and
0
+∞
W(L+1) ∈ B(ℓ2(N),Rp) W(L+1) x = ∑ w(L+1) x ,
m m−1
m=1
where the seriesconverges absolutely inRp dueto (7). Hence,for x ∈ ℓ (N),
2
+∞ +∞
f (x) = ∑ w(L+1) ρ (x,m) = w(L+1) + ∑ w(L+1) σ(x )
L m L 0 m m−1
m=0 m=1
= W(L+1)(σ(x))+b(L+1) ∈ ℓ2(N) ,
where σ(x) ∈ ℓ2(N) by (4). Thecomponent of f (x) are
L
(L+1) (L+1)
f L(x) n = hσ(x),w n· i ℓ2 +b n n = 1,...,p , (8)
where w(L+1) ∈ ℓ2(N) and w(L+1) = (w(L+1) ) .
n· nm m n
8Byiteration
x(ℓ+1)
=
fℓ(x(ℓ)),
we obtain
x(0) = x ∈ Rd
x(1) = W(1)x(0) +b(1) ∈ ℓ2(N)


  x(ℓ+1) = W(ℓ+1) σ(x(ℓ)) +b(ℓ+1) ∈ ℓ2(N)


x(L+1) = W(L+1)(cid:16) σ(x(L))(cid:17) +b(L+1) ∈ Rp .



We call the neural   RKBS function f(cid:16) an infin(cid:17) ite-width neural network. Note that N
parametrizes the width of the L hidden layers ℓ = 1,...,L, while input and output
layers ℓ = 0 and ℓ = L+1 have fixed finite widths d and p, respectively. Also note
that infinite-width neural networks generalize finite-width neural networks, where
the innerlayersare generated by infinite-rank operators.
We can visualize the shallow (L = 1), and the simplest non-shallow case (L = 2),
considering a non-iterative expression. For L = 1, we have
f(x) = W(2) σ(W(1) x+b(1)) +b(2) .
(cid:16) (cid:17)
Inthe caseof L = 2 hiddenlayers, wecan write
f(x) = W(3) σ W(2) σ(W(1) x+b(1)) +b(2) +b(3) .
(cid:16) (cid:16) (cid:16) (cid:17) (cid:17)(cid:17)
Comparing our construction to the one proposed in [18], we remark that our net-
works are completely nonparametric, in the sense that every hidden layer has infinte
width. In contrast, the networks in [18] are compositions of vector-valued shallow
networks hence theyhave intermediate linearlayers offinite width, corresponding to
thefiniteoutputdimensionofeachshallownetwork. Suchanarchitecture canberein-
terpreted as an infinite-width network with finite-rank constraints. While imposing
such constraints mayenforce in practice some sort of beneficial stability, how toset or
bound the ranks isan arbitrary choice.
Finite form of neural RKBS functions. We now show that the neural functions de-
finedin2.1arecorrespondtomeasuresµ ,...,µ havingfinitesupport. Indeed,under
1 L
this assumption, for each ℓ = 1,...,L,
dℓ
µℓ = b(ℓ+1) δ 0+ ∑ w( kℓ+1) δ m(ℓ)
k=1 k
forsomefamilyoflocationm(ℓ) ,...,m(ℓ)
∈
N\{0}andsomefamilyofvectorsw(ℓ+1) ,...,w(ℓ+1)
∈
1 dℓ 1 k
ℓ 2(N) if ℓ < L (w 1(L+1) ,...,w k(L+1) ∈ Rp if ℓ = L). We define fℓ starting from the last
layer. Since the support of µ is
{0,mL,...,m(L)
}, by (8) f depends only on the vari-
L 1 d L
L
ables
m(L) ,...,m(L)
,so that wecan regard f asa function from
Rd
L to
Rp
given by
1 d L
L
f (x) = W(L+1) σ(x)+b ,
L
whereW(L+1) is the d ×d matrix(recall that d = p) with components
L L+1 L+1
(L+1) (L+1)
W = (w ) n = 1,...,p, k = 1,...,d .
nk (L) n−1 L
m
k
9Since f is defined on
Rd
L, regarded as finite dimensional subspace of
ℓ (N),
and
L 2
denoted by P : ℓ (N) → Rd L the corresponding projection
2
Px = (x ,...,x ) ,
(L) (L)
m m
1 dL
then for all x ∈ ℓ (N)
2
f (f (x)) = f (f (x)) = f (f (x)) .
L L−1 L µ L−1 L Pµ L−1
Then,withoutlossofgenerality,wecanassumethatthemeasureµ L−1isinM(N ,Rd L)
ℓ
and ithasa finite support. Byiterating this procedure, wecan assume that for all
µℓ ∈ M({0,...,dℓ},Rdℓ+1)
for some family d ,d ,...,d ,d ∈ N with d = d and d = p. Thismeansthat
0 1 L L+1 0 L+1
dℓ
µℓ = b(ℓ+1) δ 0+ ∑ w( kℓ+1) δ
k
.
k=1
Hence,for all ℓ = 1,...,L+1, letW(ℓ) bethe n ×n matrix
dℓ−1 dℓ
(ℓ) (ℓ)
W
nk
= (w
k
)
n
n = 1,...,dℓ, k = 1,...,dℓ−1 .
Then fℓ = f
µℓ
: Rdℓ−1 → Rdℓ isgiven by
W(1)x+b(1) ℓ = 0
fℓ(x) =
(W(ℓ+1)σ(x)+b(ℓ+1) ℓ >
1 ,
fdeep = f ◦...◦ f
L 0
isa neural deepfunction according to Definition 2.1,and we can rewrite fdeep as
x(1) = W(1)x+b(1) ∈ Rd 1
(x(ℓ+1) = W(ℓ+1)σ(x(ℓ))+b(ℓ+1) ∈ Rdℓ+1 ℓ = 1,...,L ,
that is, fdeep is aneural network of depth L and(finite) widths d ,...,d .
1 L
4. REPRESENTER THEOREMS
In this section, we state and prove representer theorems for deep and neural RKBS,
showing that neural networks can be seen asparametric solutions of a nonparametric
model. Westart bybriefly recallinga basicsupervised learningsetting.
Let X be a set and Y a Banach space, called input and output space, respectively.
Consider a Banach space H of functions f : X → Y, called hypothesis space, and a loss
function L : Y ×Y → [0,∞). Given N samples
(x ,y ) ∈ X ×Y i = 1,...,N ,
i i
we wantto solve the regularizedempiricalriskminimizationproblem
inf R(f)+kfk , (9)
H
f∈H
where
1 N
R(f) = ∑ L(f(x ),y ) ,
i i
N
i=1
istheempiricalerrorassociatedtothelossfunction Landthetrainingpoints(x ,y ) ∈
i i
X ×Y,i = 1,...,N.
10To prove a representer theorem when H is a deep integral RKBS, we will need the
following two fundamental assumptions on the basis functions ρℓ. We will see that
both assumptions are easilysatisfied byneural RKBS.
Assumption 1 (Pointwise form). There is ρℓ : R×Θ ℓ → R such that, for all x ∈ Xℓ
and θ ∈ Θ ℓ,
ρℓ(x,θ) = ρ eℓ(x(θ),θ) .
Assumption 2 (Lipschitz condition). There are Cℓ > 0, gℓ ∈ C b(Θ ℓ,Xℓ) and βℓ ∈
C 0(Θ ℓ) such that, for all x,x′ ∈ Xℓ and θ ∈eΘ ℓ,
|ρℓ(x,θ)−ρℓ(x′ ,θ)| ≤ Cℓ|hx−x′ ,gℓ(θ)i Xℓ||βℓ(θ)| .
ForneuralRKBSwithactivationfunctionσ,Assumption1issatisfiedwithρ (t,n) =
0
t and ρℓ(t,n) = σ(t) forall ℓ = 1,...,L. Furthermore, byRemark3.1, Assumption 2is
satisfied with Cℓ = C
σ
and gℓ(n) = δ
n−1
forall ℓ = 1,...,L.
e
Theoreem4.1 (Representertheorem for deepintegral RKBS). Let H be an integralRKBS
of depth L on X with values in Y. Assume that Xℓ+1, ℓ = 0,...,L, is Hilbert, Y = X
L+1
has finite dimension d L+1, and L is continuous in the first entry. Moreover, assume that ρℓ
satisfiesAssumption1forallℓ = 0,...,L andAssumption2forall ℓ = 1,...,L. Then,there
are d ,...,d < ∞, θ(ℓ) ∈ Θdℓ and w(ℓ) ∈ Xdℓ−1 such that f(x) = x(L+1) definedby
1 L ℓ ℓ
x(0) = x
 x(ℓ+1) = ∑dℓ w(ℓ+1) ρℓ(x(ℓ),θ(ℓ) ) ℓ = 0,...,L
 k=1 k k
isasolution topro blem(9). Moreover,for every ℓ = 1,...,L, dℓ ≤ Ndℓ+1,and
L dℓ
kfk ≤ ∑ ∑
kw(ℓ+1)
k .
H k Xℓ+1
ℓ=0k=1
Proof. Recall that in (9) the evaluation of an element f = f ⊗ f ∈ H at point x ∈ X
0 L
is given by f(x ) = fdeep(x) where fdeep is the composition of f ,..., f . Further-
i 0 L
more, by construction each fℓ ∈ Hℓ is parameterised by some measure µℓ ∈ Mℓ =
M(Θ ℓ,Xℓ+1)accordingto(2). Takingintoaccountaccount(3),theminimizationprob-
lem (9)isequivalentto
L
inf R(f µdeep )+ ∑ kµℓk
TV
=: inf S(µ), (10)
µ∈M ℓ=0 µ∈M
deep
where if µ = µ ⊗µ , f isthe composition of f ,..., f .
0 L µ µ 0 µ L
By a classical argument, we first show that Problem (10) admits a solution. Pick a
fixed ν ∈ M, and let R = S(ν). Then Problem (10)isequivalentto
inf S(µ) . (11)
µ∈∏ ℓL =0BMℓ(R)
Indeed,take µ outside ∏ ℓL =0B Mℓ(R), then for some ℓ = 0,...,L, kµℓk
TV
> R, so that
L
S(µ) = R(f µ)+ ∑ kµℓk
TV
≥ R(f µ)+S(ν) ≥ S(ν) ,
ℓ=0
which proves the equivalence. We now prove the existence of a minimizer of (11). By
LemmaA.7, for all x ∈ X the map
(µ ,...,µ ) 7→ f ◦···◦ f (x)
0 L µ L µ 0
11is jointly continuous from ∏L B (R), endowed with product topology induced by
ℓ=0 Mℓ
the weak-* topology of each B Mℓ(R) to Y. Since the µℓ → kµk
TV
is weak-* continu-
ous, the map µ 7→ S(µ) is also continuous. Moreover, thanks to the Banach–Alaoglu
theorem, the product ∏L B (R) is weakly∗ compact. Hence, by the extreme value
ℓ=0 Mℓ
theorem, the problem (11)hasatleasta minimizer.
Let µ∗ be anysuch solution. Denote x i(0) = x i and x i(ℓ+1) = f µ∗ ℓ(x i(ℓ) ) for ℓ = 0,...,L.
Then, in view of Lemma A.4, a solution to (11) can be found by solving the following
interpolation problems for all ℓ = 0,...,L:
(ℓ) (ℓ+1)
inf kµℓk
TV
subject to f µℓ(x
i
) = x
i
i = 1,...,N . (12)
µℓ∈Mℓ
Let us start from ℓ = L. We want to apply Theorem A.2. To this end, let U = M
L
endowed with the weak∗ topology. Wedefine A : U → RN×d L+1 by
Aµ = [f (x )] .
µ L i i=1,...,N
Then A is a surjective continuous linear operator from U onto H = RanA, with
dim(H) ≤ Nd . Moreover, the norm G = k · k is coercive on U. Indeed, by
L+1 TV
the Banach–Alaoglu theorem, the balls B (r) are weakly∗ compact for every r > 0.
M
L
Wedefine F : H → [0,∞] by
0 h = y forall i = 1,...,N
F(h) = i i
(∞ h
i
6= y
i
for some i = 1,...,N .
The function F is the indicator function associeated to the singleton {(y ,...,y )}, so
0 L
that it is convex, coercive and lower semi-continuous. Therefore, we can apply Theo-
rem A.2, whence we obtain that (12)hasasolution ofthe form
d
L
µ = ∑ c u
L k k
k=1
for some d ≤ Nd , c > 0 and u e∈ Ext(B (1)). By Lemma A.3, for each k there
L L+1 k k M
L
are y ∈ Ext(B (1)) and
θ(L)
∈ Θ such that
k Y k L
u = y ·δ .
k k (L)
θ
k
(L+1)
Thus, defining w = c y , we get
k k k
d
L
µ = ∑ w(L+1) δ .
L k θ(L)
k=1 k
We now come to ℓ = L−1. Theanks to Assumption 1, we can regard f as defined
µ
L
on
X = Rsupp(µ L) , e
L
and restrict (12)to e
e
(L−1) (L)
inf kµ k subject to f (x ) = x i = 1,...,N .
µ L−1∈M(Θ L−1,X L)
L−1 TV µ L−1 i i |supp(µ L)
12
e
eWe use again Theorem A.2, together with Lemma A.3, as in the previous step (ℓ = L).
Inparticular, thistime wehave dim(H) ≤ Nd . Thus, we find asolution ofthe form
L
d L−1
µ = ∑ w(L) δ ,
L−1 k θ(L−1)
k=1 k
for some d L−1 ≤ Nd L. e
Iterating the argument for ℓ = L−2,...,0, the main claim follows. The bound on
dℓ isalsoclearbyiteration. For the bound on kfk H, bydefinition wehave
L L L
kfk
H
= ∑ kfℓk
Hℓ
= ∑ kf µℓk
Hℓ
≤ ∑ kµℓk
TV
,
ℓ=0 ℓ=0 ℓ=0
where, once again byTheorem A.2, e e
dℓ
kµℓk
TV
= ∑ c
k
.
k=1
Butsince y ∈ Ext(B (1)), we havee ky k = 1,whence
k Xℓ+1 k Xℓ+1
dℓ dℓ dℓ
∑ c = ∑ kc y k = ∑
kw(ℓ+1)
k ,
k k k Xℓ+1 k Xℓ+1
k=1 k=1 k=1
(cid:3)
which concludes the proof.
Inthe special case ofa neural RKBS, Theorem 4.1 impliesthe following result.
Corollary 4.2 (Representer theorem for neural RKBS). Let H be a neural RKBS of depth
L on Rd withvaluedinRd L+1. Assumethat L iscontinuous inthe firstentry. Then, thereare
d 1,...,d
L
< ∞, W(ℓ) ∈ Rdℓ×dℓ−1 and b(ℓ) ∈ Rdℓ such that f(x) = x(L+1) definedby
x(1) = W(1)x
(x(ℓ+1) = W(ℓ+1)σ(x(ℓ) −b(ℓ)) ℓ = 1,...,L
isasolution toproblem(9). Moreover,for every ℓ = 1,...,L, dℓ ≤ Ndℓ+1,and
kfk ≤
∑L ∑dℓ d ∑ℓ+1
|W(ℓ+1)
|2
1/2
.
H jk
ℓ=0k=1(cid:18)j=1 (cid:19)
Our result shows that deep neural networks are solutions to regularized empiri-
cal risk minimization over neural RKBS, thus justifying neural RKBS as hypothesis
spacesfordeeplearning. Moreover,itprovidesanupperboundonthenetworkwidth
depending on sample size and input/output dimensions. Finally, it shows that the
regularization norm is controlled by the
ℓ1
norm of the
ℓ2
norms of the weights of the
network. Finer characterizations of the Banach structure can be obtained using the
specific form of the activation function and the functional properties that this induces
(see[18]for the ReLU).
5. CONCLUSION
Studying function spacesdefinedbyneuralnetworks provides anatural waytoun-
derstand their properties. Recently reproducing kernel Banach spaces have emerged
hasa useful concept tostudy shallow networks.
13In this paper, we take a step towards more complex architectures considering deep
networks. We allow for a wide class of activation functions and remove unnecessary
low rank constraints. Our main contribution is defining a class of neural RKBS ob-
tained composing vector valued RKBS and deriving corresponding representer theo-
remsborrowing ideasfrom [18].
Future developments include considering more structured architectures, for exam-
ple convolutional networks, as well as investigating the statistical and computational
properties of neuralRKBS
REFERENCES
[1] N. Aronszajn. “Theory of Reproducing Kernels”. In: Transactions of the American
MathematicalSociety 68.3(1950),pp. 337–404.
[2] F. Bach. “Breakingthe Curse ofDimensionality with ConvexNeural Networks”.
In:Journal of MachineLearning Research18.19(2017),pp. 1–53.
[3] F. Bartolucci, E.De Vito,L.Rosasco, andS.Vigogna. “Understandingneural net-
works with reproducing kernel Banach spaces”. In: Applied and Computational
HarmonicAnalysis62 (2023),pp. 194–236.
[4] A.BiettiandF.Bach.“DeepequalsshallowforReLunetworksinkernelregimes”.
In:International Conferenceon Learning Representations (ICLR) 9 (2021).
[5] A. Bietti and J. Mairal. “On the Inductive Bias of Neural Tangent Kernels”. In:
Advances inNeuralInformationProcessingSystems(NeurIPS)32 (2019).
[6] V. I.Bogachev. Measure theory.Vol. II.Springer-Verlag, 2007.
[7] K. Bredies and M. Carioni. “Sparsity of solutions for variational inverse prob-
lems with finite-dimensional data”. In: Calculus of Variations and Partial Differen-
tial Equations 59.14 (2020).
[8] C. Carmeli, E. De Vito, A. Toigo, and V. Umanità. “Vector valued reproducing
kernel Hilbert spaces and universality”. In: Analysis and Applications 8.01 (2010),
pp. 19–61.
[9] L. Chizat, E. Oyallon, and F. Bach. “On lazy training in differentiable program-
ming”. In:AdvancesinNeural InformationProcessingSystems(NeurIPS)32(2019).
[10] J. Diestel and J. Uhl.VectorMeasures. AmericanMathematical Society, 1977.
[11] B. Ghorbani, S. Mei, T. Misiakiewicz, and A. Montanari. “When Do Neural Net-
works Outperform Kernel Methods?” In: Advances in Neural Information Process-
ing Systems(NeurIPS). Vol. 33.2020,pp. 14820–14830.
[12] B. Hanin. “Random Neural Networks in the Infinite Width Limit as Gaussian
Processes”. In: Annals of AppliedProbability toappear(2023).
[13] A. Jacot, C. Hongler, and F. Gabriel. “Neural Tangent Kernel: Convergence and
Generalization in Neural Networks”. In: Advances in Neural Information Process-
ing Systems(NeurIPS). 2018,pp.8580–8589.
[14] R. R. Lin, H. Z. Zhang, and J. Zhang. “On Reproducing Kernel Banach Spaces:
Generic Definitions and Unified Framework of Constructions”. In: Acta Mathe-
maticaSinica38.8(2022), pp.1459–1483.
[15] R.M. Neal. BayesianLearning forNeuralNetworks. Vol. 118.Springer, 2012.
[16] G.Ongie,R.Willett,D.Soudry,andN.Srebro.“AFunctionSpaceViewofBounded
Norm Infinite WidthReLUNets: The Multivariate Case”. In:EighthInternational
Conference onLearning Representations (ICLR). 2020.
[17] R.ParhiandR.D.Nowak.“BanachSpaceRepresenterTheoremsforNeuralNet-
works and Ridge Splines”. In: Journal of Machine Learning Research 22.43 (2021),
pp. 1–40.
14[18] R.ParhiandR.D.Nowak.“WhatKindsofFunctionsDoDeepNeuralNetworks
Learn?InsightsfromVariationalSplineTheory”.In:SIAMJournalonMathematics
of DataScience4.2(2022), pp.464–489.
[19] A.RahimiandB.Recht.“RandomFeaturesforLarge-ScaleKernelMachines”.In:
Advances inNeuralInformationProcessingSystems(NeurIPS). Vol. 20.2007.
[20] W. Rudin. Functional Analysis. International Series in Pure and Applied Mathe-
matics. McGraw-Hill, NewYork, 1991.
[21] R. Ryan. “The F. and M. Riesz theorem for vector measures”. In: Indag. Math. 25
(1963),pp. 408–412.
[22] P.Savarese,I.Evron,D.Soudry,andN.Srebro.“Howdoinfinitewidthbounded
normnetworkslookinfunctionspace?”In:ConferenceonLearningTheory.PMLR.
2019,pp. 2667–2690.
[23] I. Singer. “Linear functionals on the space of continuous mappings of a com-
pact Hausdorff space into a Banach spaces”. In: Rev. Math. Pures Appl. 2 (1957),
pp. 301–315.
[24] M.Unser.“AUnifyingRepresenterTheorem forInverseProblems andMachine
Learning”. In:Foundations of Computational Mathematics(2020),pp. 1–20.
[25] M. Unser. “From kernel methodsto neural networks: Aunifying variational for-
mulation”. In: Foundations of Computational Mathematics(2023), pp.1–40.
[26] M. Unser. “Ridges, Neural Networks, and the Radon Transform”. In: Journal of
MachineLearning Research24.37(2023),pp. 1–33.
[27] M. Unser and J. Fageot. “Native Banach spaces for splines and variational in-
verse problems”. In:arXiv:1904.10818(2019).
[28] M. Unser, J. Fageot, and J. P. Ward. “Splines are universal solutions of linear in-
verseproblemswithgeneralizedTVregularization”.In:SIAMReview59.4(2017),
pp. 769–793.
[29] D.Werner.“Extremepointsinspacesofoperatorsandvector–valuedmeasures”.
In:Proceedingsof the12th Winter Schoolon AbstractAnalysis(1984),pp. 135–143.
[30] H.Zhang,Y.Xu,andJ.Zhang.“ReproducingKernelBanachSpacesforMachine
Learning”. In:Journal of MachineLearning Research 10.95(2009),pp. 2741–2775.
APPENDIX A. SPARSE SOLUTIONS TO FINITE-DIMENSIONAL VARIATIONAL
PROBLEMS
Thekeyingredienttoestablishourrepresentertheoremisgivenbyapowerfulvaria-
tional result proved in [7]. Thisresult dealswith general minimization problems with
finite-dimensional constraints and seminorm penalization. It states that such prob-
lems admit sparse solutions, namely finite linear combinations of extremal points of
the seminorm unit ball. We report the formal statement below, after recalling the defi-
nition ofextremal point.
DefinitionA.1(Extremalpoint). LetQbeaconvexsubsetofalocallyconvexspace. A
point q ∈ Q is called extremal if Q\{q} is convex, that is, there do not exist p,r ∈ Q,
p 6= r, such that q = tp+(1−t)r for some t ∈ (0,1). We denote the set of extremal
points of Q by Ext(Q).
TheoremA.2([7,Theorem 3.3]). Consider theproblem
argminF(Au)+G(u) , (13)
u∈U
where U is a locally convex topological vector space, A : U → H is a continuous, surjective
linearmapwithvaluesinafinite-dimensionalHilbertspace H, F : H → (−∞,+∞] isproper,
15convex, coercive and lower semi-continuous, and G : U → [0,+∞) is a coercive and lower
semi-continuous norm. Let B (1) denote the unit ball {u ∈ U : G(u) ≤ 1}. Then (13) has
U
solutions of the form
K
∑
c u
k k
k=1
with u ∈ Ext(B (1)), c > 0, K ≤ dimH,and ∑K c = G(u).
k U k k=1 k
TheoremA.2isasimplifiedversion of[7,Theorem3.3],where G isonlyassumedto
be aseminorm.
InviewofTheoremA.2,inordertodeterminetheform ofsparse solutionsweneed
tocharacterizethesetofextremalpointoftheunitballinU. Inourconstructionofinte-
gral RKBS,U isthespaceofvectormeasureswithtotal variation norm. Thefollowing
result provides the desired characterization for our case. It appears in [29] assuming
that Θ is compact, but the proof works analogously when Θ is locally compact. We
report the proof for the reader’sconvenience.
Lemma A.3 ([29, Theorem 2]). Let Θ be a locally compact, second countable topological
space,and let Y be aBanach space. Then
Ext(B (1)) = {y·δ : y ∈ Ext(B (1)),θ ∈ Θ}.
M(Θ,Y) θ Y
Proof. Let us denote E = {y · δ : y ∈ Ext(B (1)),θ ∈ Θ}. We start showing that
θ Y
Ext(B (1)) ⊆ E. Suppose that µ ∈ Ext(B (1)) but µ 6= y·δ for any y ∈ Ext(B (1))
M M θ Y
andθ ∈ Θ. Then|µ| 6= δ foranyθ ∈ Θ,andthereis A ∈ B(Θ)suchthat0 < |µ|(A) <
θ
1. Denote by χ the indicator function on A. Then, setting t = |µ|(A), µ = µχ /t
A 1 A
and µ = µχ /(1−t), we can write µ asaconvex combination
2 Θ\A
µ = tµ +(1−t)µ . (14)
1 2
Since t ∈ (0,1) and µ ,µ ∈ B (1), we get that µ ∈/ Ext(B (1)), leading to a contra-
1 2 M M
diction. Wenowshowtheconverseinclusion E ⊆ Ext(B (1)). Letµ = y·δ forsome
M θ
y ∈ Ext(B (1)) and θ ∈ Θ. Suppose there are t ∈ (0,1) and µ ,µ ∈ B (1) such that
Y 1 2 M
(14). We wantto showthat necessarily µ = µ = µ. Considerthe subspace
1 2
Z = {z·δ : z ∈ Y} ,
θ
and let P: M(Θ,Y) → Z be the projection onto Z defined by
Pν = ν({θ})·δ .
θ
Bydefinition oftotal variation, for every ν ∈ M(Θ,Y) wehave
kνk ≥ kPνk +kν−Pνk ,
TV TV TV
whilethe converse bound issimplytrue bytriangle inequality, hence
kνk = kPνk +kν−Pνk . (15)
TV TV TV
Note that µ ∈ Z and thus Pµ = µ. Hence,applying P to(14)we obtain
µ = tPµ +(1−t)Pµ . (16)
1 2
Now, consider the unitball in Z,
B (1) = {z·δ : z ∈ B (1)} .
Z θ Y
Then µ ∈ Ext(B (1)), and Pµ ∈ B for i = 1,2. Therefore, looking back to (16) we
Z i Z
must have Pµ = µ, and in particular kPµ k = kµk = 1. Moreover, kµ k ≤ 1
i i TV TV i TV
since µ ∈ B (1). Thus, applying (15) to ν = µ we get kµ − Pµ k = 0, hence
i M i i i TV
(cid:3)
µ = Pµ , which in turn implies µ = µ, concluding the proof.
i i i
16The following lemma is contained in the proof of [18, Theorem 3.2]. It allows to
reduceaminimizationproblemovercompositionalfunctionstoasequenceofinterpo-
lationproblemswithrespecttoagenericminimizer. Sincewefounditofindependent
interest, we thought to emphasizeit ina separate lemma.
Let L be a positive integer. Take a set X and Banach spaces X ,...,X . For
0 1 L+1
each ℓ = 0,...,L, fix a Banach space Hℓ of functions from Xℓ to Xℓ+1. To every
f= f
0
⊕···⊕ f
L
∈ ℓL =0Hℓ, recall that fdeep : X
0
→ X
L+1
isdefined as
L fdeep = f ◦···◦ f .
L 0
LemmaA.4. With theabove setting,fixa family x ,...,x ∈ X and set
1 N 0
L
A : Hℓ → RN A(f)
i
= fdeep(x i) .
Mℓ=0
For F : RN → (−∞,+∞], considerthe minimizationproblem
L
inf F(A(f)) + ∑ kfℓk
Hℓ
. (17)
f∈ ℓL =0Hℓ ℓ=0
Assume that (17) has a solutionL f∗ = ⊕f∗, and denote x(0) = x and x(ℓ+1) = f∗(x(ℓ) ) for
ℓ i i i ℓ i
ℓ = 0,...,L. Thenthereexistsaminimizer f = ⊕ ℓL =0fℓ of (17)suchthatforall ℓ = 0,...,L
the function fℓ isthe solutionof
e e
(ℓ) (ℓ+1)
e
inf kfℓk
Hℓ
subjectto fℓ(x
i
) = x
i
i = 1...,N . (18)
fℓ∈Hℓ
and kfℓk
Hℓ
= kf ℓ∗k Hℓ.
Proof.eLet f be a solution to (18). The solution f∗ satisfies the constraints
fℓ(x(ℓ)
) =
i
x
i(ℓ+1)
, hence kfℓk
Hℓ
≤ kf ℓ∗k
Hℓ
and fℓ(x i) = f ℓ∗(x i) for all i = 1,...,N. This implies
e
∑ ℓL =0kfℓk
Hℓ
≤ ∑ ℓL =0kf ℓ∗k
Hℓ
and A(f) = A(f∗), sothat
e e
L L
e F(A(f))+ ∑ kfℓek
Hℓ
≤ F(A(f∗))+ ∑ kf ℓ∗k
Hℓ
.
ℓ=0 ℓ=0
Butsince f∗ isa minimize er,soi sf ae nd kfℓk
Hℓ
= kf ℓ∗k Hℓ. (cid:3)
Thenextlemmasareneededtoestablish continuity inthe setting ofourrepresenter
e e
theorem 4.1. A continuity result akin to Lemma A.7 is also needed for the proof of
[18, Theorem 3.2]. We remark that, in order for the argument to go thorough, joint
continuity is required, while the proof in [18, Theorem 3.2] only establishes separate
continuity. Thefinalresultremainsvalidsincejointcontinuityholdstruenevertheless.
ThiscanbeseenasaspecialcaseofourLemmaA.7,wherethelaststepscanbesimpli-
fied in view of the fact that, in finite-dimensional spaces, weak and strong continuity
coincide.
Lemma A.5. Let X be a set, Y a Hilbert space, and Θ a locally compact, second countable
topological space. Let ρ : X ×Θ → R such that ρ(x,·) ∈ C (Θ) for all x ∈ X, and define
0
φ(x) : M(Θ,Y) → Y by
φ(x)µ = ρ(x,θ)dµ(θ) x ∈ X µ ∈ M(Θ,Y) .
Θ
Z
17Then, for all x ∈ X, φ(x) is continuous from M(Θ,Y) endowed with the weak∗ topology to
Y endowed withthe weaktopology.
Proof. Notethat,sinceY isHilbert,theRieszrepresentationtheoremsaysthatM(Θ,Y) =
C (Θ,Y)′. Now, for all x ∈ X, µ ∈ M(Θ,Y) and y ∈ Y, we have
0
hφ(x)µ,yi = hρ(x,·)y,µi .
Y C (Θ,Y) M(Θ,Y)
0
Thus hφ(x)·,yi defines an element of the predual C (Θ,Y), and hence it is weakly∗
Y 0
continuous from M(Θ,Y) to R . But since this is true for all y ∈ Y, it is weakly∗
continuous from M(Θ,Y) to Y endowedwith the weaktopology. (cid:3)
The following known result is a direct consequence of Prokhorov theorem. We re-
port the proof for completeness.
Lemma A.6 (Joint dominated convergence theorem). Let Θ be a Polish space. For all
n ∈ N , let λ ∈ M(Θ) and f : Θ → R satisfyingthe followingconditions:
n n
(i) for each n ∈ N , the function f is λ integrable;
n n
(ii) the sequence (f ) convergesto some f : Θ → R uniformlyon allcompactsets;
n n
(iii) the sequence (f ) is uniformlybounded;
n n
(iv) the sequence (λ ) converges to some λ ∈ M(Θ) with respect to the narrow topology,
n
i.e.
lim ϕ(θ)dλ (θ) = ϕ(θ)dλ(θ) ∀ϕ ∈ C (Θ) ;
n b
n→+∞ Θ Θ
Z Z
(v) the function f is λ-integrable;
Then
f (θ)dλ (θ) −−−→ f(θ)dλ(θ) .
n n
Θ n→∞ Θ
Z Z
Proof. For any compact K ⊂ Θ, we have
f (θ)dλ (θ)− f(θ)dλ(θ)
n n
Θ Θ
(cid:12)Z Z (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
=
(cid:12)
(f n(θ)− f(θ))dλ n(θ) +
(cid:12)
(f n(θ)− f(θ))dλ n(θ) + (f n(θ)− f(θ))dλ(θ)
K Θ\K Θ
(cid:12)Z (cid:12) (cid:12)Z (cid:12) (cid:12)Z (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
≤s(cid:12) (cid:12)up|f n − f||λ n(K)|+sup(cid:12) (cid:12) |f n(cid:12) (cid:12)− f||λ n(Θ\K)|+ (f n((cid:12) (cid:12)θ)−(cid:12) (cid:12) f(θ))dλ(θ) . (cid:12) (cid:12)
K Θ\K (cid:12)ZΘ (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Thelasttermgoestozerobydominatedconvergenc(cid:12)esince f n− f isuniform(cid:12)lybounded.
The first term goes to zero because f → f uniformly on K and, since (λ ) is conver-
n n
gent, |λ (K)| ≤ |λ (Θ)| ≤ sup kλ k < ∞. For the middle term, fix an arbitrary
n n n n TV
ε > 0. Again, f − f is uniformly bounded. Moreover, since (λ ) isconvergent and Θ
n n
is Polish, by the Prokhorov theorem [6, Theorem 8.6.2] there is a compact set K ⊂ Θ
ε
such that |λ (Θ\K )| < ε for all n. By taking K = K we get
n ǫ ε
limsup f (θ)dλ (θ)− f(θ)dλ(θ) ≤ supsup|f (θ)− f(θ)| ǫ .
n n n
n→∞ (cid:12)ZΘ ZΘ (cid:12) n Θ !
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:3)
The claimfollo(cid:12)ws because ǫ isarbitrary. (cid:12)
18Lemma A.7. Let X be a set, X ,X separable Hilbert spaces, and Θ ,Θ locally compact,
0 1 2 0 1
second countable topological spaces. For ℓ = 0,1, let ρℓ : Xℓ ×Θ ℓ → R such that ρℓ(x,·) ∈
C 0(Θ ℓ) for all x ∈ Xℓ, and define φℓ(x) : M(Θ ℓ,Xℓ+1) → Xℓ+1 by
φℓ(x)µ = ρℓ(x,θ)dµ(θ) x ∈ Xℓ µ ∈ M(Θ ℓ,Xℓ+1) .
ZΘℓ
Assume there are C > 0, g ∈ C (Θ ,X ) and β ∈ C (Θ ) such that, for all x,x′ ∈ X and
b 1 1 0 1 1
θ ∈ Θ ,
1
|ρ (x,θ)−ρ (x′ ,θ)| ≤ C|hx−x′ ,g(θ)i ||β(θ)| . (19)
1 1 X
1
Letr ,r > 0. Then,for all x ∈ X , the map
0 1 0
Γ (µ,ν) = φ ((φ (x)µ))ν
x 1 0
is jointly weakly∗ continuous from B (r )× B (r ) to X endowed with the
M(Θ ,X ) 0 M(Θ ,X ) 1 2
0 1 1 2
weaktopology.
Proof. BytheBanach–Alaoglutheorem,theproductB = B (r )×B (r )
M(Θ ,X ) 0 M(Θ ,X ) 1
0 1 1 2
is compact. Moreover, since Xℓ+1 (ℓ = 0,1) is separable, so is C 0(Θ ℓ,Xℓ+1) by the
Stone–Weierstrasstheorem. Also,Xℓ+1isHilbert,henceM(Θ ℓ,Xℓ+1) = C 0(Θ ℓ,Xℓ+1)′
by the Riesz representation theorem. Therefore, B is metrizable [20, Theorem 3.16].
Thus, itis enough to prove the (weak∗-weak)sequential continuity of Γ .
x
To this end, let (µ ,ν ) → (µ,ν) (weakly∗). We want to show that Γ (µ ,ν ) →
n n x n n
Γ (µ,ν) (weakly). Wehave
x
Γ (µ ,ν )−Γ (µ,ν) = [φ (φ (x)µ )−φ (φ (x)µ)]ν +φ (φ (x)µ)(ν −ν) .
x n n x 1 0 n 1 0 n 1 0 n
The second term goes to zero by Lemma A.5. Let us call I the first term, and let
z = φ (x)µ , z = φ (x)µ. For all y ∈ X , byassumption (19)we have
n 0 n 0 2
|hI,yi | ≤ |ρ (z ,θ)−ρ (z,θ)||β(θ)|d|[ν ] |(θ)
X 1 n 1 n y
2 Θ
Z 1
≤ C |hz −z,g(θ)i ||β(θ)|d|[ν ] |(θ) ,
n X n y
Θ 1
Z 1
where [ν ] (E) = hν (E),yi for all E ∈ B(Θ ). We want to apply Lemma A.6 with
n y n X 1
2
f (θ) = |hz −z,g(θ)i | and λ = |β||[ν ] |, soto conclude that |hI,yi | → 0.
n n X n n y X
1 2
First, letusverify that (f ) isuniformly bounded. We have
n
f n(θ) ≤ kz n −zk X kg(θ)k X ≤ supkz n −zk X kgk ∞ ,
1 1 1
n
where sup nkz n − zk X
1
< ∞ because (z n − z) is convergent, and kgk ∞ < ∞ by as-
sumption. Next, we show that (λ ) converges pointwise on C (Θ ). Let λ = |β||[ν] |
n b 1 y
where [ν] (E) = hν(E),yi for all E ∈ B(Θ ). Then, for every h ∈ C (Θ ) we have
y X 1 b 1
2
h|β| ∈ C (Θ ), andhence, since [ν ] → [ν] pointwise on C (Θ ),
0 1 n y y 0 1
h(θ)dλ (θ) = h(θ)|β(θ)|d[ν ] (θ) → h(θ)|β(θ)|d[ν] (θ) = h(θ)dλ(θ) .
n n y y
Θ Θ Θ Θ
Z 1 Z 1 Z 1 Z 1
Finally, we show that f → 0 uniformly on compact sets. Let K ⊂ Θ be compact,
n 1
and fix an arbitrary ε > 0. Since g is continuous, g(K) is compact in X , and thus it
1
can be covered by a finite number of closed balls. Let w ,...,w ∈ X be the centers
1 q 1
of such balls, and define P : X → X as the projection onto span{w ,...,w }. Then
1 1 1 q
19sup kw−Pwk ≤ ε. Hence,forevery θ ∈ K there isw ∈ PX such that kg(θ)−
w∈g(K) X ! 1
wk ≤ ε. Thus, we have
X
1
|f (θ)| = |hz −z,g(θ)i |
n n X
1
≤ |hz −z,g(θ)−wi |+|hP(z −z),wi |
n X n X
1 1
≤ kz −zk kg(θ)−wk +kP(z −z)k kwk
n X X n X X
1 1 1 1
≤ kz −zk kg(θ)−wk +kP(z −z)k kg(θ)−wk +kg(θ)k
n X X n X X X
1 1 1 1 1
≤ kz
n
−zk
X
ε+kP(z
n
−z)k
X
(ε+kgk ∞) .
(cid:0) (cid:1)
1 1
Now, since z → z weakly, sup kz −zk < ∞, and kP(z −z)k → 0 because P
n n n X 1 n X 1
hasfinite rank.
All the assumptions of Lemma A.6 are therefore satisfied, and its application con-
(cid:3)
cludesthe proof.
(F.Bartolucci)ANALYSISGROUP-DELFTINSTITUTEOFAPPLIEDMATHEMATICS,TUDELFT,NETHER-
LANDS
Emailaddress: f.bartolucci@tudelft.nl
(E.DeVito)MALGA- DIMA,UNIVERSITY OF GENOA, ITALY
Emailaddress: ernesto.devito@unige.it
(L.Rosasco)MALGA- DIBRIS, UNIVERSITY OF GENOA, ITALY& CBMM,MIT& IIT
Emailaddress: lorenzo.rosasco@unige.it
(S. Vigogna) ROMADS - DEPARTMENT OF MATHEMATICS, UNIVERSITY OF ROME TOR VERGATA,
ITALY
Emailaddress: vigogna@mat.uniroma2.it
20