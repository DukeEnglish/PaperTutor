3DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surfaces
LinyiJin1,NileshKulkarni1,DavidF.Fouhey2
UniversityofMichigan1,NewYorkUniversity2
{jinlinyi,nileshk}@umich.edu, david.fouhey@nyu.edu
Abstract
View 1 View 2
This paper introduces 3DFIRES, a novel system for
scene-level3Dreconstructionfromposedimages.Designed 3DFIRES ?
to work with as few as one view, 3DFIRES reconstructs
the complete geometry of unseen scenes, including hid-
den surfaces. With multiple view inputs, our method pro-
duces full reconstruction within all camera frustums. A
key feature of our approach is the fusion of multi-view in-
formation at the feature level, enabling the production of
coherent and comprehensive 3D reconstruction. We train
our system on non-watertight scans from large-scale real
scene dataset. We show it matches the efficacy of single-
view reconstruction methods with only one input and sur-
passes existing techniques in both quantitative and quali-
View 1 Frustum Prediction Full Prediction
tativemeasuresforsparse-view3Dreconstruction. Project
page: https://jinlinyi.github.io/3DFIRES/
Figure 1. Reconstructing 3D from sparsely posed images.
Given a sparse set of posed image views, our method is able to
reconstruct the full 3D of the scene. On the top, we show two
1.Introduction sparseviewsofthesceneinView1andView2. Onthebottom
left is the 3D reconstruction from our network in the frustum of
ConsidertwoviewsofthesceneinFig.1. Partofthebed-
View1. Weshowthatourmethodcangeneratetheoccludedside
room in View 1 is occluded by the wall, and so you may table(zoomin).Onthebottomrightisthefullreconstruction.We
be uncertain what is behind it, although you might guess coloroccludedsurfaceswithsurfacenormals.
the wall continues. Now consider adding in View 2. You
can see a bedside table, but little else. However, you can 20, 27, 42, 44], one can predict both visible and occluded
fusethesepiecestogethertocreateaconsistent3Dsenseof 3Dstructurefromanimage,butstackingsuchoutputsfrom
thesceneviewedbytheimages, includingboththevisible multiple images can produce inconsistent outputs. When
and invisible parts. We use this sense when shopping for handled independently, methods cannot identify the best
realestateorlookingatafriend’sphotos. Weestimatethe viewtoreasonaboutanoccludedregion. Non-line-of-sight
structureofthescenefrompartsthatarevisibletoallviews; imaging involves transmitting and receiving signals to re-
integrateinformationacrossimagesforpartsthatvisiblein vealhiddenscenes,incompatiblewithstandardcameraim-
oneviewbutnotothers;andtakeeducatedguessesforcom- ages[14]. Sparseviewreconstructionmethods [1,17,40]
pletelyoccludedregions. Importantly,astheavailabledata cancreateconsistentreconstructionsfromtwoviews;how-
increasesfromonecameratoahandful,wecanseamlessly ever,theseapproachesarelimitedtothevisiblepartsofthe
integratetheevidenceacrossviews. scene that can decomposed into planes. Moreover, these
Thistaskposesachallengeforcurrentcomputervision methods are usually specialized to a particular number of
since it requires making judgments about visible and oc- imagesthatcanbeaccepted.
cluded3Dstructuresandintegratinginformationacrossim- Recently, there has been considerable progress in gen-
ages with large pose change. These abilities are usually eralized radiance fields, which produce full 3D represen-
independently investigated in two separate strands of re- tations. This occupancy representation and per-scene op-
search. With single image reconstruction techniques [15, timization has shown promising results by optimizing for
4202
raM
31
]VC.sc[
1v86780.3042:viXranovel view synthesis on single scenes from posed images onmultipleimagesofthesamescene[21]. Ourmethodcan
sets [7, 24, 37, 41]. Extending this line of work, methods reconstruct hidden geometry from at least a single image
like [33, 47] have shown an ability to predict novel views using implicit representation from [20]. Instead of naively
forunseenscenesfromafewimages. However,sincethese fusingpointcloudsfromdifferentimages,wefusefeatures
methodsoptimizeforperceptualquality,theunderlyingge- when predicting a multi-view consistent point cloud with
ometry often has artifacts. Like them we also require one fewinputimages.
ormoreimageviewsatinput,butinsteadwepredictanim- 3D from dense views. Traditional multi-view 3D recon-
plicitfunction[20]thatcanreliablyreconstructbothvisible structionmethodscanproduceaccurateandcoherentpoint-
andoccludedpartsofpreviouslyunseenscenes. cloudsfrompixelcorrespondences[34]. Classicalmethods
We propose 3DFIRES, Few Image 3D-REconstruction in computer vision use approaches like Multi-view stereo
of Scenes, which integrates information from a variable (MVS)toconstructonlyvisiblepartsofthesceneinallthe
number of images to produce a full reconstruction of the images. Thereisalonglineofworkintryingtoreconstruct
scene. 3DFIRES integrates information in the features scenesfromvideosequences[6,35]wheretheyreconstruct
space across a varying number of images, enabling it to visible scenes and camera poses. Learning-based methods
identifyhowtobestusetheavailableimagedatatoproduce forMVSestimategeometryforscenes[18,25,39,46]also
anaccuratereconstructionatapoint. Asoutput,3DFIRES require an input video to explicitly predict scene geome-
produces a pixel-aligned implicit field based on a gener- try. Instead of requiring high overlap inputs such as video
alization of the Directed Ray Distance Function [20, 21], frames,ourmethodworksonwide-baselineimages.
which enables high quality reconstructions. Thanks to in- 3D from sparse view inputs. Our approach operates
tegration in feature space, the results are more consistent in a multi-view setting with a sparse set of views. We
than handling images independently: this is what enables haveasimilarsettingaswide-baselinereconstruction[28].
reconstructingthebed-sidetablein Fig.1,eventhoughitis Associative3D [29] reconstructs the whole scene but re-
hiddenbythewallinoneimage. Wefoundanddocument quiresvoxelizedscenestotrain,ourmethodworksonnon-
several design decisions in terms of training and network watertightscenedata. Priorworkalsoexploresplanarrep-
architectureneededtoproducetheseresults. resentation [1, 17, 40] for coherent 3D surfaces in non-
Weevaluateourmethodoncomplexinteriorscenesfrom watertightscenes. Theyusefeed-forwardnetworkstopre-
Omnidata[8,34]datasetcollectedwitharealscanner. We dictvisible3Dsurfacesforeachviewandmergethemus-
compare3DFIRESwiththepoint-spacefusionofstate-of- ingpredictedcorrespondences. Ourapproachleveragesan
the-artmethodsforscene-levelfull3Dreconstructionmeth- implicit representation that accommodates non-watertight
ods from a single image [21, 44]. Our experiments show data, enabling the reconstruction of both visible and oc-
several key results. First, 3DFIRES produces more accu- cludedsurfaces. Wefusedeepfeaturesfrommultipleviews
rateresultscomparedtoexistingworks. Theimprovements to predict DRDF representation from Kulkarni et al. [20],
arelargerinhiddenregions,andespeciallysubstantialwhen producingacoherentreconstruction.
measuring consistency of prediction from multiple views. Novel view synthesis. NeRF [24] and its extensions [43,
Second, ablative analysis reveals the key design decisions 47, 49] optimizes per-scene radiance fields for novel-view
responsible for 3DFIRES’s success. Third, 3DFIRES can synthesis,thisrequiresmanyviewsandtest-timeoptimiza-
generalizetovariableviews: wetrainon1,2,and3views tion. Duetoitsoccupancy-basedrepresentation,extracting
and generalize to 5 views. Finally, 3DFIRES can recon- geometry often requires thresholding the density function,
structwhengivenLoFTR[38]estimatedposeswithknown which leads to cloudy geometry with sparse input views.
translationscale. Ourmethoddirectlypredictsgeometryfromunseenimages
withouttheneedfortest-timeoptimization. PixelNerf[47]
2.RelatedWorks or SRT [33] can generalize to new scenes but their objec-
tivesoptimizeforphotometriclosses.
We aim to produce a coherent 3D scene reconstruction
givenasingleorafewimageswithwidebaselines.
3.Method
3D from Single Image. Predicting a complete 3D scene
fromasingleimageisinherentlyambiguous. Recentlydif- Ourgoalistopredictanaccurateandconsistent3Drecon-
ferent3Drepresentationshavebeenproposedtoreconstruct struction from one or more sparsely spaced camera views
complete 3D scenes (including occluded surfaces) such as andknownposes. Withoneimage,themethodshouldpre-
layereddepth[36],voxels[3,11,19,42],planes[16],point- dictallsurfacesinthecamerafrustum,includingvisibleand
clouds[9,44], meshes[10,12,26], orimplicitrepresenta- occluded regions. With more images, the method should
tionforobjects[23,27]andscenes[2,4,20,21,37]. While predictthesurfacesintheunionofthefrustum.
theyhavestrongperformanceonsingleimage,theydonot Wetacklethisproblemwith3DFIRES,asimpleandef-
necessarilyproducecoherentresultswhenrequiredtoinfer fectiveapproachdesignedforthissetting. Wefirstdiscuss(a) Single Image DRDF (b) Sparse View DRDF 3D FIRES
Query Encoder
~r<latexit sha1_base64="WCih1HuVbgsdeJD/Jxk0LyZ5EiQ=">AAAB+3icbVDLSsNAFL3xWesr1qWbYBFclaT4WhbduKxgH9CEMJlO2qGTSZiZFEvIr7hxoYhbf8Sdf+OkzUJbDwwczrmXe+YECaNS2fa3sba+sbm1Xdmp7u7tHxyaR7WujFOBSQfHLBb9AEnCKCcdRRUj/UQQFAWM9ILJXeH3pkRIGvNHNUuIF6ERpyHFSGnJN2vulODMjZAaB2Em8txv+mbdbthzWKvEKUkdSrR988sdxjiNCFeYISkHjp0oL0NCUcxIXnVTSRKEJ2hEBppyFBHpZfPsuXWmlaEVxkI/rqy5+nsjQ5GUsyjQk0VIuewV4n/eIFXhjZdRnqSKcLw4FKbMUrFVFGENqSBYsZkmCAuqs1p4jATCStdV1SU4y19eJd1mw7lqXD5c1Fu3ZR0VOIFTOAcHrqEF99CGDmB4gmd4hTcjN16Md+NjMbpmlDvH8AfG5w+F25TE</latexit>
 <latexit sha1_base64="G01Afc33Oi/D7UWC3hHpG9gxs9M=">AAACAXicbVDLSsNAFJ3UV62vqBvBzWARXJVEfC2LunBZwT6gCWUyvWmHTh7MTAolxI2/4saFIm79C3f+jZM2C209cOFwzr3ce48XcyaVZX0bpaXlldW18nplY3Nre8fc3WvJKBEUmjTikeh4RAJnITQVUxw6sQASeBza3ugm99tjEJJF4YOaxOAGZBAyn1GitNQzD5xb4Io4Y6CpExA19PxUZFnP7plVq2ZNgReJXZAqKtDomV9OP6JJAKGinEjZta1YuSkRilEOWcVJJMSEjsgAupqGJADpptMPMnyslT72I6ErVHiq/p5ISSDlJPB0Z36knPdy8T+vmyj/yk1ZGCcKQjpb5CccqwjnceA+E0AVn2hCqGD6VkyHRBCqdGgVHYI9//IiaZ3W7Iva+f1ZtX5dxFFGh+gInSAbXaI6ukMN1EQUPaJn9IrejCfjxXg3PmatJaOY2Ud/YHz+AAP5l0U=</latexit> ~r1  <latexit sha1_base64="jrxlEyRgmJkiW2Z+5eNgHrYDpm8=">AAACCHicbVDJSgNBEO2JW4zbqEcPDgbBU5gRt2NQD54kglkgE0JPpyZp0rPQXRMMwxy9+CtePCji1U/w5t/YWQ6a+KDg8V4VVfW8WHCFtv1t5BYWl5ZX8quFtfWNzS1ze6emokQyqLJIRLLhUQWCh1BFjgIasQQaeALqXv9q5NcHIBWPwnscxtAKaDfkPmcUtdQ2991rEEjdAbDUDSj2PD+VWdZ2ER4wvc3aZtEu2WNY88SZkiKZotI2v9xOxJIAQmSCKtV07BhbKZXImYCs4CYKYsr6tAtNTUMagGql40cy61ArHcuPpK4QrbH6eyKlgVLDwNOdo1vVrDcS//OaCfoXrZSHcYIQsskiPxEWRtYoFavDJTAUQ00ok1zfarEelZShzq6gQ3BmX54nteOSc1Y6vTspli+nceTJHjkgR8Qh56RMbkiFVAkjj+SZvJI348l4Md6Nj0lrzpjO7JI/MD5/ADRqmsE=</latexit> ~rN
x
2
x
n<latexit sha1_base64="frXz0wLR2F2rv06ZTN4WrOUxHDg=">AAACAXicbVDLSsNAFJ3UV62vqBvBzWAR6qYk4mtZdOOygn1AW8pkOmmHTiZh5kZaQtz4K25cKOLWv3Dn3zhpu9DWAxcO59zLvfd4keAaHOfbyi0tr6yu5dcLG5tb2zv27l5dh7GirEZDEaqmRzQTXLIacBCsGSlGAk+whje8yfzGA1Oah/IexhHrBKQvuc8pASN17YM2sBEkskfTrltqBwQGnp+M0pOuXXTKzgR4kbgzUkQzVLv2V7sX0jhgEqggWrdcJ4JOQhRwKlhaaMeaRYQOSZ+1DJUkYLqTTD5I8bFRetgPlSkJeKL+nkhIoPU48ExndqKe9zLxP68Vg3/VSbiMYmCSThf5scAQ4iwO3OOKURBjQwhV3NyK6YAoQsGEVjAhuPMvL5L6adm9KJ/fnRUr17M48ugQHaESctElqqBbVEU1RNEjekav6M16sl6sd+tj2pqzZjP76A+szx+3vZcS</latexit> dc1(x) n<latexit sha1_base64="AGnVijE+w0+ytMTQA+K3+R2t95w=">AAACCHicbZDLSsNAFIYnXmu9RV26MFiEuimJeFsW3biSCvYCTSmT6aQdOpmEmRNpCVm68VXcuFDErY/gzrdx0mahrT8MfPznHOac34s4U2Db38bC4tLyymphrbi+sbm1be7sNlQYS0LrJOShbHlYUc4ErQMDTluRpDjwOG16w+us3nygUrFQ3MM4op0A9wXzGcGgra554AIdQSJ6JO1O8TYtuwGGgecno/S4a5bsij2RNQ9ODiWUq9Y1v9xeSOKACiAcK9V27Ag6CZbACKdp0Y0VjTAZ4j5taxQ4oKqTTA5JrSPt9Cw/lPoJsCbu74kEB0qNA093Ziuq2Vpm/ldrx+BfdhImohioINOP/JhbEFpZKlaPSUqAjzVgIpne1SIDLDEBnV1Rh+DMnjwPjZOKc145uzstVa/yOApoHx2iMnLQBaqiG1RDdUTQI3pGr+jNeDJejHfjY9q6YOQze+iPjM8f7Wiajg==</latexit> dcN(x)
<latexit sha1_base64="dasf5QzN237RWZiN+UItoDGYkUc=">AAAB8XicbVDLSgMxFL3js9ZX1aWbYBFclRnxtSy6cVnBPrAtJZPeaUMzmSHJiGXoX7hxoYhb/8adf2OmnYW2HggczrmXnHv8WHBtXPfbWVpeWV1bL2wUN7e2d3ZLe/sNHSWKYZ1FIlItn2oUXGLdcCOwFSukoS+w6Y9uMr/5iErzSN6bcYzdkA4kDzijxkoPnZCaoR+kT5NeqexW3CnIIvFyUoYctV7pq9OPWBKiNExQrdueG5tuSpXhTOCk2Ek0xpSN6ADblkoaou6m08QTcmyVPgkiZZ80ZKr+3khpqPU49O1kllDPe5n4n9dOTHDVTbmME4OSzT4KEkFMRLLzSZ8rZEaMLaFMcZuVsCFVlBlbUtGW4M2fvEgapxXvonJ+d1auXud1FOAQjuAEPLiEKtxCDerAQMIzvMKbo50X5935mI0uOfnOAfyB8/kDAIiRJg==</latexit> <latexit sha1_base64="dasf5QzN237RWZiN+UItoDGYkUc=">AAAB8XicbVDLSgMxFL3js9ZX1aWbYBFclRnxtSy6cVnBPrAtJZPeaUMzmSHJiGXoX7hxoYhb/8adf2OmnYW2HggczrmXnHv8WHBtXPfbWVpeWV1bL2wUN7e2d3ZLe/sNHSWKYZ1FIlItn2oUXGLdcCOwFSukoS+w6Y9uMr/5iErzSN6bcYzdkA4kDzijxkoPnZCaoR+kT5NeqexW3CnIIvFyUoYctV7pq9OPWBKiNExQrdueG5tuSpXhTOCk2Ek0xpSN6ADblkoaou6m08QTcmyVPgkiZZ80ZKr+3khpqPU49O1kllDPe5n4n9dOTHDVTbmME4OSzT4KEkFMRLLzSZ8rZEaMLaFMcZuVsCFVlBlbUtGW4M2fvEgapxXvonJ+d1auXud1FOAQjuAEPLiEKtxCDerAQMIzvMKbo50X5935mI0uOfnOAfyB8/kDAIiRJg==</latexit>
Pos-Enc
~r<latexit sha1_base64="oP73xHA6Y//ggR7Js6yMcrLdveE=">AAAB+3icbVDLSsNAFL2pr1pfsS7dDBbBVUnE17LoxmUF+4AmhMl00g6dPJiZFEvIr7hxoYhbf8Sdf+OkzUJbDwwczrmXe+b4CWdSWda3UVlb39jcqm7Xdnb39g/Mw3pXxqkgtENiHou+jyXlLKIdxRSn/URQHPqc9vzJXeH3plRIFkePapZQN8SjiAWMYKUlz6w7U0oyJ8Rq7AeZyHPP9syG1bTmQKvELkkDSrQ988sZxiQNaaQIx1IObCtRboaFYoTTvOakkiaYTPCIDjSNcEilm82z5+hUK0MUxEK/SKG5+nsjw6GUs9DXk0VIuewV4n/eIFXBjZuxKEkVjcjiUJBypGJUFIGGTFCi+EwTTATTWREZY4GJ0nXVdAn28pdXSfe8aV81Lx8uGq3bso4qHMMJnIEN19CCe2hDBwg8wTO8wpuRGy/Gu/GxGK0Y5c4R/IHx+QOEV5TD</latexit> 1 ~r<latexit sha1_base64="rKr49T9SCmirA5CGw4/7vHIpSdc=">AAAB+3icbVDLSsNAFJ3UV62vWJdugkVwVRLfy6IblxXsA5oQJtObduhkEmYmxRLyK25cKOLWH3Hn3zhps9DqgYHDOfdyz5wgYVQq2/4yKiura+sb1c3a1vbO7p65X+/KOBUEOiRmsegHWAKjHDqKKgb9RACOAga9YHJb+L0pCElj/qBmCXgRHnEaUoKVlnyz7k6BZG6E1TgIM5Hn/plvNuymPYf1lzglaaASbd/8dIcxSSPgijAs5cCxE+VlWChKGOQ1N5WQYDLBIxhoynEE0svm2XPrWCtDK4yFflxZc/XnRoYjKWdRoCeLkHLZK8T/vEGqwmsvozxJFXCyOBSmzFKxVRRhDakAothME0wE1VktMsYCE6XrqukSnOUv/yXd06Zz2by4P2+0bso6qugQHaET5KAr1EJ3qI06iKBH9IRe0KuRG8/Gm/G+GK0Y5c4B+gXj4xuHX5TF</latexit> 3 q<latexit sha1_base64="ON5lJl3gy8FVbSRkBSbwQ6R6Bcs=">AAAB83icbVDLSgMxFL1TX7W+qi7dBIvgqsyIr2XRjcsK9gGdoWTSTBuaScYkI5Shv+HGhSJu/Rl3/o2ZdhbaeiBwOOde7skJE860cd1vp7Syura+Ud6sbG3v7O5V9w/aWqaK0BaRXKpuiDXlTNCWYYbTbqIojkNOO+H4Nvc7T1RpJsWDmSQ0iPFQsIgRbKzk+zE2ozDKHqd9r1+tuXV3BrRMvILUoECzX/3yB5KkMRWGcKx1z3MTE2RYGUY4nVb8VNMEkzEe0p6lAsdUB9ks8xSdWGWAIqnsEwbN1N8bGY61nsShncwz6kUvF//zeqmJroOMiSQ1VJD5oSjlyEiUF4AGTFFi+MQSTBSzWREZYYWJsTVVbAne4peXSfus7l3WL+7Pa42boo4yHMExnIIHV9CAO2hCCwgk8Ayv8Oakzovz7nzMR0tOsXMIf+B8/gAi75HD</latexit> 1 f<latexit sha1_base64="2dwfFXT52wdn3Sh6Efi0/rcDkNQ=">AAAB/HicbVDLSsNAFJ3UV62vaJdugkVwVRLxtSy6cVnBPqAJYTKdtEMnkzBzI5YQf8WNC0Xc+iHu/BsnbRbaemDgcM693DMnSDhTYNvfRmVldW19o7pZ29re2d0z9w+6Kk4loR0S81j2A6woZ4J2gAGn/URSHAWc9oLJTeH3HqhULBb3ME2oF+GRYCEjGLTkm3U3wjAOwizMfRfoI2RO7psNu2nPYC0TpyQNVKLtm1/uMCZpRAUQjpUaOHYCXoYlMMJpXnNTRRNMJnhEB5oKHFHlZbPwuXWslaEVxlI/AdZM/b2R4UipaRToySKqWvQK8T9vkEJ45WVMJClQQeaHwpRbEFtFE9aQSUqATzXBRDKd1SJjLDEB3VdNl+AsfnmZdE+bzkXz/O6s0bou66iiQ3SETpCDLlEL3aI26iCCpugZvaI348l4Md6Nj/loxSh36ugPjM8favaVSA==</latexit>1 f<latexit sha1_base64="SsEXFX3XD2T+N4puBLockWx2+TE=">AAAB/HicbVDLSsNAFJ3UV62vaJdugkVwVRLxtSy6cSUV7AOaECbTSTt0MgkzN2IJ8VfcuFDErR/izr9x0mah1QMDh3Pu5Z45QcKZAtv+MipLyyura9X12sbm1vaOubvXVXEqCe2QmMeyH2BFORO0Aww47SeS4ijgtBdMrgq/d0+lYrG4g2lCvQiPBAsZwaAl36y7EYZxEGZh7rtAHyC7yX2zYTftGay/xClJA5Vo++anO4xJGlEBhGOlBo6dgJdhCYxwmtfcVNEEkwke0YGmAkdUedksfG4damVohbHUT4A1U39uZDhSahoFerKIqha9QvzPG6QQXngZE0kKVJD5oTDlFsRW0YQ1ZJIS4FNNMJFMZ7XIGEtMQPdV0yU4i1/+S7rHTeeseXp70mhdlnVU0T46QEfIQeeoha5RG3UQQVP0hF7Qq/FoPBtvxvt8tGKUO3X0C8bHN5cHlWU=</latexit>N q<latexit sha1_base64="Vupu/QUeN1zsS0xe1k+AbszCWzc=">AAAB/HicbVDLSsNAFJ3UV62vaJdugkVwVRLxtSy6cSUV7AOaECbTSTt08nDmRgwh/oobF4q49UPc+TdO2iy09cDA4Zx7uWeOF3MmwTS/tcrS8srqWnW9trG5tb2j7+51ZZQIQjsk4pHoe1hSzkLaAQac9mNBceBx2vMmV4Xfe6BCsii8gzSmToBHIfMZwaAkV6/bAYax52f3uWsDfYTsJnf1htk0pzAWiVWSBirRdvUvexiRJKAhEI6lHFhmDE6GBTDCaV6zE0ljTCZ4RAeKhjig0smm4XPjUClDw4+EeiEYU/X3RoYDKdPAU5NFVDnvFeJ/3iAB/8LJWBgnQEMyO+Qn3IDIKJowhkxQAjxVBBPBVFaDjLHABFRfNVWCNf/lRdI9blpnzdPbk0brsqyjivbRATpCFjpHLXSN2qiDCErRM3pFb9qT9qK9ax+z0YpW7tTRH2ifP6ghlXA=</latexit> N
MLP
f<latexit sha1_base64="fNAhuIJmuxGOUAWxc3gYL3NpBko=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5CRbRVUnE17LoxmUF+4AmhMl00g6dTMLMjVhD8FfcuFDErf/hzr9x2mahrQcGDufcyz1zgoQzBbb9bZQWFpeWV8qrlbX1jc0tc3unpeJUEtokMY9lJ8CKciZoExhw2kkkxVHAaTsYXo/99j2VisXiDkYJ9SLcFyxkBIOWfHPPjTAMgjAL8yPfBfoAmZP7ZtWu2RNY88QpSBUVaPjml9uLSRpRAYRjpbqOnYCXYQmMcJpX3FTRBJMh7tOupgJHVHnZJH1uHWqlZ4Wx1E+ANVF/b2Q4UmoUBXpynFXNemPxP6+bQnjpZUwkKVBBpofClFsQW+MqrB6TlAAfaYKJZDqrRQZYYgK6sIouwZn98jxpndSc89rZ7Wm1flXUUUb76AAdIwddoDq6QQ3URAQ9omf0it6MJ+PFeDc+pqMlo9jZRX9gfP4A0MSVeQ==</latexit>10 f<latexit sha1_base64="iUqmZNYA14b1Q4Ign+slxiGbFzo=">AAAB/XicbVDLSsNAFJ34rPUVHzs3g0V0VRLxtSy6cSUV7AOaECbTSTt08mDmRqwh+CtuXCji1v9w5984abvQ1gMDh3Pu5Z45fiK4Asv6NubmFxaXlksr5dW19Y1Nc2u7qeJUUtagsYhl2yeKCR6xBnAQrJ1IRkJfsJY/uCr81j2TisfRHQwT5oakF/GAUwJa8sxdJyTQ94MsyA89B9gDZDe5Z1asqjUCniX2hFTQBHXP/HK6MU1DFgEVRKmObSXgZkQCp4LlZSdVLCF0QHqso2lEQqbcbJQ+xwda6eIglvpFgEfq742MhEoNQ19PFlnVtFeI/3mdFIILN+NRkgKL6PhQkAoMMS6qwF0uGQUx1IRQyXVWTPtEEgq6sLIuwZ7+8ixpHlfts+rp7Umldjmpo4T20D46QjY6RzV0jeqogSh6RM/oFb0ZT8aL8W58jEfnjMnODvoD4/MH/NWVlg==</latexit>N0
x
<latexit sha1_base64="dasf5QzN237RWZiN+UItoDGYkUc=">AAAB8XicbVDLSgMxFL3js9ZX1aWbYBFclRnxtSy6cVnBPrAtJZPeaUMzmSHJiGXoX7hxoYhb/8adf2OmnYW2HggczrmXnHv8WHBtXPfbWVpeWV1bL2wUN7e2d3ZLe/sNHSWKYZ1FIlItn2oUXGLdcCOwFSukoS+w6Y9uMr/5iErzSN6bcYzdkA4kDzijxkoPnZCaoR+kT5NeqexW3CnIIvFyUoYctV7pq9OPWBKiNExQrdueG5tuSpXhTOCk2Ek0xpSN6ADblkoaou6m08QTcmyVPgkiZZ80ZKr+3khpqPU49O1kllDPe5n4n9dOTHDVTbmME4OSzT4KEkFMRLLzSZ8rZEaMLaFMcZuVsCFVlBlbUtGW4M2fvEgapxXvonJ+d1auXud1FOAQjuAEPLiEKtxCDerAQMIzvMKbo50X5935mI0uOfnOAfyB8/kDAIiRJg==</latexit> Backbone Self Attention
w<latexit sha1_base64="SsapNaAZwYYPqevoKoVNPS22/WU=">AAAB6nicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr2PQi8eI5gHJEmYnvcmQ2dllZlYJSz7BiwdFvPpF3vwbJ8keNFrQUFR1090VJIJr47pfTmFpeWV1rbhe2tjc2t4p7+41dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6nvqtB1Sax/LejBP0IzqQPOSMGivdPfa8XrniVt0ZyF/i5aQCOeq98me3H7M0QmmYoFp3PDcxfkaV4UzgpNRNNSaUjegAO5ZKGqH2s9mpE3JklT4JY2VLGjJTf05kNNJ6HAW2M6JmqBe9qfif10lNeOlnXCapQcnmi8JUEBOT6d+kzxUyI8aWUKa4vZWwIVWUGZtOyYbgLb78lzRPqt559ez2tFK7yuMowgEcwjF4cAE1uIE6NIDBAJ7gBV4d4Tw7b877vLXg5DP78AvOxzcNao2p</latexit> 1 w<latexit sha1_base64="JoCDypO3YFcOoOQhkYWep7sqdjE=">AAAB8XicbVDJSgNBEO2JW4xb1KOXxiB4CjPidgx68SQRzILJEHo6NUmTnp6hu0YNQ/7CiwdFvPo33vwbO8tBow8KHu9VUVUvSKQw6LpfTm5hcWl5Jb9aWFvf2Nwqbu/UTZxqDjUey1g3A2ZACgU1FCihmWhgUSChEQwux37jHrQRsbrFYQJ+xHpKhIIztNLdQ6eN8IjZ9ahTLLlldwL6l3gzUiIzVDvFz3Y35mkECrlkxrQ8N0E/YxoFlzAqtFMDCeMD1oOWpYpFYPxscvGIHlilS8NY21JIJ+rPiYxFxgyjwHZGDPtm3huL/3mtFMNzPxMqSREUny4KU0kxpuP3aVdo4CiHljCuhb2V8j7TjKMNqWBD8OZf/kvqR2XvtHxyc1yqXMziyJM9sk8OiUfOSIVckSqpEU4UeSIv5NUxzrPz5rxPW3PObGaX/ILz8Q3/npEl</latexit> N
MLP
Weighted Pooling
<latexit sha1_base64="dasf5QzN237RWZiN+UItoDGYkUc=">AAAB8XicbVDLSgMxFL3js9ZX1aWbYBFclRnxtSy6cVnBPrAtJZPeaUMzmSHJiGXoX7hxoYhb/8adf2OmnYW2HggczrmXnHv8WHBtXPfbWVpeWV1bL2wUN7e2d3ZLe/sNHSWKYZ1FIlItn2oUXGLdcCOwFSukoS+w6Y9uMr/5iErzSN6bcYzdkA4kDzijxkoPnZCaoR+kT5NeqexW3CnIIvFyUoYctV7pq9OPWBKiNExQrdueG5tuSpXhTOCk2Ek0xpSN6ADblkoaou6m08QTcmyVPgkiZZ80ZKr+3khpqPU49O1kllDPe5n4n9dOTHDVTbmME4OSzT4KEkFMRLLzSZ8rZEaMLaFMcZuVsCFVlBlbUtGW4M2fvEgapxXvonJ+d1auXud1FOAQjuAEPLiEKtxCDerAQMIzvMKbo50X5935mI0uOfnOAfyB8/kDAIiRJg==</latexit>x ~r<latexit sha1_base64="oP73xHA6Y//ggR7Js6yMcrLdveE=">AAAB+3icbVDLSsNAFL2pr1pfsS7dDBbBVUnE17LoxmUF+4AmhMl00g6dPJiZFEvIr7hxoYhbf8Sdf+OkzUJbDwwczrmXe+b4CWdSWda3UVlb39jcqm7Xdnb39g/Mw3pXxqkgtENiHou+jyXlLKIdxRSn/URQHPqc9vzJXeH3plRIFkePapZQN8SjiAWMYKUlz6w7U0oyJ8Rq7AeZyHPP9syG1bTmQKvELkkDSrQ988sZxiQNaaQIx1IObCtRboaFYoTTvOakkiaYTPCIDjSNcEilm82z5+hUK0MUxEK/SKG5+nsjw6GUs9DXk0VIuewV4n/eIFXBjZuxKEkVjcjiUJBypGJUFIGGTFCi+EwTTATTWREZY4GJ0nXVdAn28pdXSfe8aV81Lx8uGq3bso4qHMMJnIEN19CCe2hDBwg8wTO8wpuRGy/Gu/GxGK0Y5c4R/IHx+QOEV5TD</latexit> 1 <latexit sha1_base64="dasf5QzN237RWZiN+UItoDGYkUc=">AAAB8XicbVDLSgMxFL3js9ZX1aWbYBFclRnxtSy6cVnBPrAtJZPeaUMzmSHJiGXoX7hxoYhb/8adf2OmnYW2HggczrmXnHv8WHBtXPfbWVpeWV1bL2wUN7e2d3ZLe/sNHSWKYZ1FIlItn2oUXGLdcCOwFSukoS+w6Y9uMr/5iErzSN6bcYzdkA4kDzijxkoPnZCaoR+kT5NeqexW3CnIIvFyUoYctV7pq9OPWBKiNExQrdueG5tuSpXhTOCk2Ek0xpSN6ADblkoaou6m08QTcmyVPgkiZZ80ZKr+3khpqPU49O1kllDPe5n4n9dOTHDVTbmME4OSzT4KEkFMRLLzSZ8rZEaMLaFMcZuVsCFVlBlbUtGW4M2fvEgapxXvonJ+d1auXud1FOAQjuAEPLiEKtxCDerAQMIzvMKbo50X5935mI0uOfnOAfyB8/kDAIiRJg==</latexit>x ~r<latexit sha1_base64="WCih1HuVbgsdeJD/Jxk0LyZ5EiQ=">AAAB+3icbVDLSsNAFL3xWesr1qWbYBFclaT4WhbduKxgH9CEMJlO2qGTSZiZFEvIr7hxoYhbf8Sdf+OkzUJbDwwczrmXe+YECaNS2fa3sba+sbm1Xdmp7u7tHxyaR7WujFOBSQfHLBb9AEnCKCcdRRUj/UQQFAWM9ILJXeH3pkRIGvNHNUuIF6ERpyHFSGnJN2vulODMjZAaB2Em8txv+mbdbthzWKvEKUkdSrR988sdxjiNCFeYISkHjp0oL0NCUcxIXnVTSRKEJ2hEBppyFBHpZfPsuXWmlaEVxkI/rqy5+nsjQ5GUsyjQk0VIuewV4n/eIFXhjZdRnqSKcLw4FKbMUrFVFGENqSBYsZkmCAuqs1p4jATCStdV1SU4y19eJd1mw7lqXD5c1Fu3ZR0VOIFTOAcHrqEF99CGDmB4gmd4hTcjN16Md+NjMbpmlDvH8AfG5w+F25TE</latexit> 2 <latexit sha1_base64="dasf5QzN237RWZiN+UItoDGYkUc=">AAAB8XicbVDLSgMxFL3js9ZX1aWbYBFclRnxtSy6cVnBPrAtJZPeaUMzmSHJiGXoX7hxoYhb/8adf2OmnYW2HggczrmXnHv8WHBtXPfbWVpeWV1bL2wUN7e2d3ZLe/sNHSWKYZ1FIlItn2oUXGLdcCOwFSukoS+w6Y9uMr/5iErzSN6bcYzdkA4kDzijxkoPnZCaoR+kT5NeqexW3CnIIvFyUoYctV7pq9OPWBKiNExQrdueG5tuSpXhTOCk2Ek0xpSN6ADblkoaou6m08QTcmyVPgkiZZ80ZKr+3khpqPU49O1kllDPe5n4n9dOTHDVTbmME4OSzT4KEkFMRLLzSZ8rZEaMLaFMcZuVsCFVlBlbUtGW4M2fvEgapxXvonJ+d1auXud1FOAQjuAEPLiEKtxCDerAQMIzvMKbo50X5935mI0uOfnOAfyB8/kDAIiRJg==</latexit>x ~r<latexit sha1_base64="rKr49T9SCmirA5CGw4/7vHIpSdc=">AAAB+3icbVDLSsNAFJ3UV62vWJdugkVwVRLfy6IblxXsA5oQJtObduhkEmYmxRLyK25cKOLWH3Hn3zhps9DqgYHDOfdyz5wgYVQq2/4yKiura+sb1c3a1vbO7p65X+/KOBUEOiRmsegHWAKjHDqKKgb9RACOAga9YHJb+L0pCElj/qBmCXgRHnEaUoKVlnyz7k6BZG6E1TgIM5Hn/plvNuymPYf1lzglaaASbd/8dIcxSSPgijAs5cCxE+VlWChKGOQ1N5WQYDLBIxhoynEE0svm2XPrWCtDK4yFflxZc/XnRoYjKWdRoCeLkHLZK8T/vEGqwmsvozxJFXCyOBSmzFKxVRRhDakAothME0wE1VktMsYCE6XrqukSnOUv/yXd06Zz2by4P2+0bso6qugQHaET5KAr1EJ3qI06iKBH9IRe0KuRG8/Gm/G+GK0Y5c4B+gXj4xuHX5TF</latexit> 3
f<latexit sha1_base64="KTn/HLBLodNTChBNwnVEDvLPgGk=">AAAB9HicbVDJSgNBEO2JW4xb1KOXwSB4CjPidgwaxGMUs0AyhJ5OTdKkp2fsrgmGId/hxYMiXv0Yb/6NneWgiQ8KHu9VUVXPjwXX6DjfVmZpeWV1Lbue29jc2t7J7+7VdJQoBlUWiUg1fKpBcAlV5CigESugoS+g7vevx359AErzSD7gMAYvpF3JA84oGskL2i2EJ0zL9+WbUTtfcIrOBPYicWekQGaotPNfrU7EkhAkMkG1brpOjF5KFXImYJRrJRpiyvq0C01DJQ1Be+nk6JF9ZJSOHUTKlER7ov6eSGmo9TD0TWdIsafnvbH4n9dMMLj0Ui7jBEGy6aIgETZG9jgBu8MVMBRDQyhT3Nxqsx5VlKHJKWdCcOdfXiS1k6J7Xjy7Oy2UrmZxZMkBOSTHxCUXpERuSYVUCSOP5Jm8kjdrYL1Y79bHtDVjzWb2yR9Ynz+fOJIE</latexit>DRDF
3D FIRES 3D FIRES 3D FIRES
MLP
Backbone
D<latexit sha1_base64="fe8riY/uTMwNUuW9LKSYMVFWhNc=">AAAB8nicbVDLSgNBEJyNrxhfUY9eBoPgKeyKr2PQIB6jmAdsljA7mSRDZmeWmV4xLPkMLx4U8erXePNvnCR70MSChqKqm+6uMBbcgOt+O7ml5ZXVtfx6YWNza3unuLvXMCrRlNWpEkq3QmKY4JLVgYNgrVgzEoWCNcPh9cRvPjJtuJIPMIpZEJG+5D1OCVjJbwN7grR6X70Zd4olt+xOgReJl5ESylDrFL/aXUWTiEmgghjje24MQUo0cCrYuNBODIsJHZI+8y2VJGImSKcnj/GRVbq4p7QtCXiq/p5ISWTMKAptZ0RgYOa9ifif5yfQuwxSLuMEmKSzRb1EYFB48j/ucs0oiJElhGpub8V0QDShYFMq2BC8+ZcXSeOk7J2Xz+5OS5WrLI48OkCH6Bh56AJV0C2qoTqiSKFn9IreHHBenHfnY9aac7KZffQHzucPIcaRKw==</latexit> RDF
D<latexit sha1_base64="OGHYbwlaNfxZB2E9BNiciYUFjtg=">AAACAHicbVDLSsNAFJ34rPUVdeHCTbAIdVMS8bUsWsRlFfuAJpTJdNIOnTyYuZGWkI2/4saFIm79DHf+jZM2C209cOFwzr3ce48bcSbBNL+1hcWl5ZXVwlpxfWNza1vf2W3KMBaENkjIQ9F2saScBbQBDDhtR4Ji3+W05Q6vM7/1SIVkYfAA44g6Pu4HzGMEg5K6+r4NdARJ7b52k5ZtH8PA9ZJRetzVS2bFnMCYJ1ZOSihHvat/2b2QxD4NgHAsZccyI3ASLIARTtOiHUsaYTLEfdpRNMA+lU4yeSA1jpTSM7xQqArAmKi/JxLsSzn2XdWZnShnvUz8z+vE4F06CQuiGGhApou8mBsQGlkaRo8JSoCPFcFEMHWrQQZYYAIqs6IKwZp9eZ40TyrWeeXs7rRUvcrjKKADdIjKyEIXqIpuUR01EEEpekav6E170l60d+1j2rqg5TN76A+0zx+Kr5Zj</latexit> RDF(x) D<latexit sha1_base64="7B9cLwSfZzJXX+8Byuq7QaZOqi8=">AAACFHicbVDLSgNBEJz1bXxFPXoZDEJECLviC7yIBvEYxaiQDWF20muGzD6Y6Q0Jy36EF3/FiwdFvHrw5t84iRHUWNBQVHXT3eXFUmi07Q9rbHxicmp6ZjY3N7+wuJRfXrnSUaI4VHkkI3XjMQ1ShFBFgRJuYgUs8CRce+2Tvn/dAaVFFF5iL4Z6wG5D4QvO0EiN/JaL0MW0fFE+zYpuwLDl+Wk3O6RuB3j6LagsazibjXzBLtkD0FHiDEmBDFFp5N/dZsSTAELkkmldc+wY6ylTKLiELOcmGmLG2+wWaoaGLABdTwdPZXTDKE3qR8pUiHSg/pxIWaB1L/BMZ/9K/dfri/95tQT9g3oqwjhBCPnXIj+RFCPaT4g2hQKOsmcI40qYWylvMcU4mhxzJgTn78uj5Gq75OyVds93CkfHwzhmyBpZJ0XikH1yRM5IhVQJJ3fkgTyRZ+veerRerNev1jFrOLNKfsF6+wQdG57g</latexit> RDF(x;~r ) D<latexit sha1_base64="MGfc5aZ1lb1ZRFXYpnZ9lnuj/H0=">AAACFHicbVDLSgNBEJyNrxhfUY9eBoOgCGFXfIGXoEE8qpgoZEOYnfTGwdkHM72SsOxHePFXvHhQxKsHb/6NkxhBowUNRVU33V1eLIVG2/6wcmPjE5NT+enCzOzc/EJxcamuo0RxqPFIRurKYxqkCKGGAiVcxQpY4Em49G6O+v7lLSgtovACezE0A9YJhS84QyO1ipsuQhfT6nn1OFt3A4bXnp92swPq3gJPvwWVZa2tjVaxZJftAehf4gxJiQxx2iq+u+2IJwGEyCXTuuHYMTZTplBwCVnBTTTEjN+wDjQMDVkAupkOnsromlHa1I+UqRDpQP05kbJA617gmc7+lXrU64v/eY0E/f1mKsI4QQj51yI/kRQj2k+ItoUCjrJnCONKmFspv2aKcTQ5FkwIzujLf0l9q+zslnfOtkuVw2EcebJCVsk6ccgeqZATckpqhJM78kCeyLN1bz1aL9brV2vOGs4sk1+w3j4BHqCe4Q==</latexit> RDF(x;~r ) D<latexit sha1_base64="PccmhB1+nX/97sfKNPL7SN7hozM=">AAACFHicbVDJSgNBEO1xjXGLevTSGARFCDPu4CWoiEcVo0ImhJ5OTdKkZ6G7JiQM8xFe/BUvHhTx6sGbf2MnRnB7UPB4r4qqel4shUbbfrdGRsfGJyZzU/npmdm5+cLC4pWOEsWhwiMZqRuPaZAihAoKlHATK2CBJ+Haax/1/esOKC2i8BJ7MdQC1gyFLzhDI9ULGy5CF9Pji+OTbM0NGLY8P+1mB9TtAE+/BJVl9a31eqFol+wB6F/iDEmRDHFWL7y5jYgnAYTIJdO66tgx1lKmUHAJWd5NNMSMt1kTqoaGLABdSwdPZXTVKA3qR8pUiHSgfp9IWaB1L/BMZ/9K/dvri/951QT9/VoqwjhBCPnnIj+RFCPaT4g2hAKOsmcI40qYWylvMcU4mhzzJgTn98t/ydVmydkt7ZxvF8uHwzhyZJmskDXikD1SJqfkjFQIJ7fknjySJ+vOerCerZfP1hFrOLNEfsB6/QAgJZ7i</latexit> RDF(x;~r ) DRDF Predictor
1 2 3
Figure2.(a)ArchitectureforsingleviewDRDF[20].Givenanimageandaquerypixellocation,itpredictsDRDFalongtherayfromthe
querypixel. (b)weextend(a)toworkonsparseviews. Middle:GivenNimages,aquerypointx,andaquerydirection⃗r ,weaggregate
q
featuresfrommultipleimagesandoutputDRDFalongthequeryray. Right: Weshowdetailednetworkarchitectureof3DFIRESwhich
consistsofaQueryEncoderandaDRDFPredictor.
view implicit reconstruction method aims to produce the
full 3D reconstruction for the scene from this image. At
inference,whenconditionedonimagefeatures,themethod
outputsadistancefunctionforapre-definedsetof3Dpoints
in the camera frustum. It then decodes this predicted dis-
tance function to a surface to recover the 3D geometry of
the scene. For instance, if the predicted 3D distance func-
tionisanunsigneddistancefunction[2], thepointsonthe
surfacearewithdistancesclosetozero.
Kulkarni et al. [20] solve the single image 3D recon-
struction with the DRDF function and show that using the
DRDF outperforms the standard unsigned distance func-
SV-DRDF 3DFIRES tion. TheDRDFisaray-baseddistancefunctionmeasuring
the distance of a point x to the nearest intersection with a
Figure 3. Predictions in the blue camera frustum. Occluded
surface along a ray⃗r. In [20], the ray on which distances
surfacesarecoloredwithsurfacenormals. Asingleimageto3D
method like DRDF [20] is unable to reconstruct the parts of the aremeasuredistherayfromthecameracenterctox.
scenebehindthewallwithcertaintyandhenceerroneouslyaddsa Fig. 2 (a) shows the DRDF for one such ray. Now, any
fullwallinfrontofthehallway(redbox). 3DFIRESwhichfuses 3D point x can be represented as its distance towards the
featuresfrommultipleviews(GreenandPurplecamerainFig.2) camera times a unit ray direction, or z⃗r, where z ∈ R
predictsemptyspacefortheentrance(blackbox). and⃗r = norm(x − c) where norm(p) = p/||p||. The
tacklingscenereconstructioninasingleimagecasein§3.1 DRDF, d (z⃗r), furthermore includes a sign that deter-
DR
usingtheDirectedRayDistanceFunction(DRDF)[20]and mines for the point the direction along the ray towards the
scalethisapproachtomultipleimageviewsin§3.2.In§3.3, nearest intersection (i.e., forwards or backwards). There-
weshowhowwecanoperationalizeourmulti-viewrecon- fore(z+d (z⃗r))⃗rcorrespondstoapointonthesurface.
DR
structiongoalwithanattention-basedmodelarchitecture.
The DRDF can be used to create a system that infers
singleimage3Dbypairingthedistancefunctionatapoint
3.1.BackgroundSingleViewReconstruction
xwithpixel-alignedfeatures. Atinferencetime, asshown
We begin by revisiting the DRDF formulation for a single inFig.2(a),givenapointxinthecamerafrustumwecan
image reconstruction. Consider a single image I, a single extractcorrespondingpixel-alignedimagefeaturesusinganimagebackboneBB[π(x)], anduseanMLPtopredictthe per-image, the model must learn to aggregate information
DRDFvaluecorrespondingtothepointxalongthe⃗r.Since across cameras. This is done with our second component
DRDFisaray-basedfunction,itsvalueonlydependsonthe QueryEncoderthatprovidesgeometricinformationforag-
intersectionsalongtheray. Foranyraycorrespondingtoa gregating appearance features. Specifically, the query en-
pixel on the image, the prediction of DRDF for the point coder uses the information about the relative positions of
dependsontheimagefeatures,andthelocationofthepoint querypointxandquerydirection⃗r w.r.t.cameras{π }N .
q i i=1
on the ray. This parameterization allows DRDF to learn ThefinalmoduleistheDRDFPredictorthattakesappear-
sharp 3D reconstructions of the scene from a single RGB anceandqueryfeaturestoproducesaDRDFvaluealongthe
image. At training time, we train a model to predict the querydirection⃗r byincorporatingtheappearancefeatures
q
DRDFbysupervisingitwiththeground-truthDRDFvalues (evidence for geometry) and query encoder features (evi-
computedusingthemeshgeometry. dencethatrelatesdifferentfeatures). Fig.3showsanexam-
ple on how integrating information across multiple views
3.2.ExtendingDRDFstoMultipleViews
leadstobetterpredictionforoccludedpartsofthescene.
Now,withmultipleviewswehave:Nimages{I }N ,rela- Backbone Feature Extractor. Our backbone features ex-
i i=1
tivecameratransforms{π }N ,andcorrespondingcamera tractor BB(·) aims to create appearance features from an
fc ue ln lte scrs en{ ec .i} WN
i= h1
i, leo tu hr eg tao sa kli i csi o= ut1 o ldr pec eo rhn as ptr suc bt et ah ce co3 mD po lif shth ede aim ga rg ide. ofIt Da -c dc ie mp ets nsa in oni am la fg ee atI uri es∈ FR iH ∈×W RH× ′3 ×a Wn ′d ×p Dr imo gd .u Wce es
by simply predicting individual 3D for each camera, and use a pre-trained depth estimating vision transformer [30].
assemblingthemtogether. Ourinsightisthatifthecamera Feature extraction for each image proceeds independently
frustumshaveconsiderableoverlap,foroverlappingregions using the same network. With extracted per-camera back-
wecanachieveabetterandmoreconsistentreconstruction bone features, f i, for point x by interpolating features in
byallowingthenetworktoreasonaboutwhichcamerapro- {F i}N i=1attheprojection{π i(x)}N i=1correspondingly.
vides the best view for each point. This can be achieved Query Encoder. Our query encoder q(·) aims to enable
byallowingthenetworktofusefeaturesacrosscamerasfor a predictor to decide how to aggregate information across
the points in feature space rather than by concatenating in images. As input, the encoder takes a query 3D point
point space. We propose to improve the feature quality of x and a query direction⃗r q. It additionally considers the
any point x by fusing the features from multiple cameras. backbone features, camera centers {c i}N i=1 and transforms
Since we are now dealing with the multi-view settings, a {π i}N i=1.Ourqueryencodingistheconcatenationof:(i)the
multi-view DRDF formulation is necessary to allow us to relative viewing direction in camera i’s space ∆⃗r i(⃗r q) =
predict the DRDF value along each of the query rays,⃗r q, [⃗r q −norm(x−c i),⃗r q ·norm(x−c i)] ∈ R4; and(ii)the
originatingfromtherespectivecameracenters. normalizeddevicecoordinates(NDC),coordinatesofpoint
In the case of multiple views, the image feature cor- xinthecameraframendc i(x) ∈ R3. Intuitivelythisquery
responding to a point x should be a fusion of features representation,q i = {∆⃗r i,ndc i(x)} ∈ R7 enablesreason-
{f [π (x)]}N . The feature should support predicting ingsuchas: informationaboutsurfacesnearxindirection
θ i i=1
the N DRDF values along all the camera directions as ⃗r q is likely not visible in camera i due to either angle or
{d (z⃗r )}N . The intuition of our key idea is that distance,sothisfeatureoughttobeweightedlow. Theray
DR i i i=1
multiple-image views provide more information about the queryvectorisencodedinapositionalencodinglayer[41]
3Dsceneandhencepotentiallybetterfeatures.Wecanlearn withoutputdimensionD query.
these better features by fusing features to predict a consis- DRDFPredictor. Foraqueryrayandpointtuple,{⃗r q,x},
tentoutput.Thisrequiresanovelarchitecturethatattendsto thismodelconsiderstheimagefeatures{f i}N i=1,andquery
featuresandrays,{⃗r i}N i=1,originatingfromalltheavailable features {q i}N
i=1
yielding a joint camera specific feature,
imageviews. UnderthisformulationsingleviewDRDFis {f i,q i}N i=1,ofdimensionD img+D query. Ourself-attention
aspecialcaseofourformulationwhereNis1. attends over all these features to produce a weight w i per
feature. Weaggregatethefeaturesusingthisweighttopro-
3.3.NetworkArchitecture duceafusedfeatureforthepointx. Wethenusethefused
feature to predict a DRDF value between [−1,1] with the
TowardsthegoalofpredictingDRDFsalongmultiplequery
help of an MLP. This is akin to selecting cameras that are
rays⃗r ∈ {⃗r }N , we present a simple and effective net-
q i i=1 likely to contain the geometry information about the ray
work3DFIRESthataccomplishesthistask. 3DFIREScon-
pointtupleandpredictingthegeometryinformation.
sistsofthreemodules: ThefirstmoduleisaBackboneFea-
ture Extractor that obtains pixel-aligned appearance fea-
3.4.Training3DFIRES
tures; byprojectingthequerypointxontothecamera, we
can obtain a per-point and per-camera appearance feature The effectiveness of 3DFIRES is improved by getting de-
as in [20, 24, 32, 43, 47]. Since the appearance feature is tailsrightduringtraining. Oneobservationisthatsamplingpointsnearintersectionsgivesimprovementsoveruniform cameraposesfromtheTaskonomy[48]Mediumsubset,in-
sampling because the scene-level space is predominantly cluding 98/20/20 training/validation/test buildings. Since
empty. By increasing the density of sampled points near ourmultiviewsettingisdifferentfromthesingle-viewset-
surface,thenetworkcanbetterlearnthescenestructure.We tingof[21],theprecisesamplesaredifferent. Oursettingis
sample points along the ray as per a Gaussian distribution also similar to [17, 40] in that images have wide baselines
centered at the intersection. Prior work [43] involves ap- (median 2.8m translation, 63.9◦ rotation), unlike methods
plying ray attention which allows for samples along a ray using video frames [39] where images have high overlap.
to attend with each other before the final prediction. This Ourapproachdivergesfrom[17,40]inalsoreconstructing
has been shown to be effective. However, combining ray occludedregionsandusingreal(notrendered)images.
attention with Gaussian sampling during training enables To curate our image sets, we use a sampling process
thenetworkto‘cheat’. RayAttentionexploitsatrain-time like [17]. For a set of k images, after picking an image
shortcut(querypointdensity)toinferintersections. Atin- atrandom,eachnewimageisselectedtohaveatmost70%
ference as point density is uniform and this shortcut fails. overlapwithanyexistingimageintheset,andatleast30%
Empirically we find Gaussian sampling alone to be more overlap with at least one other image in the set. The pro-
effectivethanrayattention. cessbalancesdiversityandcoherenceintheviewpoints.We
cropimagestoafixedfieldofview. Wecollect3781train-
3.5.ImplementationDetails
ing sets among ≥ 10K images. We also sample 300 sets
of3-viewimagesand100setsof5-viewimagesforevalu-
Training. Our image feature backbone is vision trans-
ationfromtheheld-outtestscenes. Seethesupplementary
former [30] dpt beit large 384 pretrained by MiDaS [31].
for dataset generation details. The 3 view and 5 view test
We use ℓ loss on log-space truncated DRDF [5, 20, 39].
1
setcontainconsiderableoccluded3Dgeometry(41.9%and
Duringtraining,werandomlysample1,2,3viewswith80
43.7%respectively).
raysperimageand512pointsalongeachray. Ourmethod
is trained for 300K iteration on NVIDIA A100 GPU with
4.2.Baselines
batchsizeof1. Moredetailsinsupp.
Inference. Given N images, we extract backbone features To the best of our knowledge, no prior work reconstructs
foreachimage. Wegeneraten = 128×128queryrays occluded regions from sparse-view images at scene scale.
ray
from each camera. Along each ray, we sample n = 256 Wethuscreatestrongbaselinesfromexistingmethodsthat
pt
points that have uniformly spaced depth from 0 to 8m. In handlepartsofoursetting. Eachmethodisthestrongestin
total, wegetN×n ×n querypairs{x,⃗r }, whichare itslineofwork.
ray pt q
fedto3DFIRESinparalleltogetDRDFvalue.Wecalculate For instance, the visible surface upper-bound includes
positive-to-negative zero-crossings along each ray [20] to all methods that reconstruct visible surfaces from sparse
geta3Dpointandaggregatetheresults. views [17, 39, 40]. The DRDF method [20, 21] has been
shown to be more effective for scene-level 3D reconstruc-
4.Experiment tion compared to many other implicit functions like den-
sity [47], occupancy [32], unsigned distance functions on
Inthissection, wepresenttheexperimentalframeworkfor
scenesandrays[2]. MCC[44]islikewiseSOTAforpoint
3DFIRES,oursystemdesignedtoreconstructfullscenege-
cloudcompletion.
ometryfromwide-baseline,sparseimages. Consideringthe
Depth Only [8, 30] Prior state-of-the-art works on sparse
novelty of our problem, there is no prior work that does
scene reconstruction [39, 40] predict visible surfaces from
thisexactsetting. Toaddressthis,wecuratedadatasetand
multiple views, but cannot recover hidden surfaces. To
developed testing metrics specifically tailored to the prob-
showthenear-oraclereconstructionofvisiblesurfaces,we
lem’srequirements.Weconductcomprehensiveevaluations
useMiDaS[30]depthmodeltrainedonOmnidata[8]with
of3DFIRESusingrealsceneimages,comparingitsperfor-
ground-truth scale and shift. This baseline is an upper
manceagainstalternativemethodsinthefield.
boundontheperformanceofmethodslike[1,17,39,40].
MultiviewCompressiveCoding(MCC)[44]Thismethod
4.1.Dataset
predicts occupancy probability from RGB-D partial point-
Following [21], we use the dataset from the Gibson clouds. MCCworksonscene-levelreconstructionsinclud-
database [45], which contains real images of complex and ing non-watertight meshes. We train MCC on the same
diverse scenes such as multi-floor villas and expansive trainingsetasours.Thismethodrequiresdepthasinputand
warehouses. Thescaleoftheassetsinthedatasetpresents at inference we provide it with ground truth depth. Since
challenging reconstruction problem, which is desirable for MCC only works on a single point cloud, to produce pre-
evaluatingtheabilitytorecoveroccludedsurfaces. Weuse dictions from multiple images, we infer each image inde-
the images sampled by Omnidata [8] for a diverse set of pendentlyandaggregatethepredictedpointcloudinpointInput 3-view Depth Only MCC SV-DRDF 3DFIRES Ground Truth
Figure 4. Comparison between different methods on held-out test scene. Occluded surfaces are colored with the computed surface
normals. “Depthonly”leavesholeswithsparseinputviews,e.g.absentfloorsandwalls. Occupancy-basedmethodMCC[44]produces
cloudyresults,failingtogetthedetailslikepillow,tables. ConcatenationofsingleviewDRDF(SV-DRDF)[20]producesinconsistent
results,e.g.missingwallinrow2,thedoublewallinrow3. Ourmethodproducesmoreconsistentpredictionsacrossdifferentviewsand
alsorecoversthehiddensurface,resultinginacompletemesh.Weurgethereadertoseeresultsprovidedinthesupplementaryvideos.
cloudspace. within ρ from a predicted point), and their F-score (F1).
Single-view DRDF (SV-DRDF) [20] This method recon- This gives an overall summary of scene-level reconstruc-
structsbothvisibleandhiddensurfacesfromasingleinput tion. Weclassifythesceneinto(1)visible: pointsthatare
image. Weusethisbaselinetoshowthebenefitofourpro- visible from any one of the input views; and (2) hidden:
posed multi-view feature aggregation. For a fair compari- points that are hidden from all of the input views. Due to
son,weupgradetheoriginalbackbonefromResNet34[13] thespacelimit,weonlyshowF-scoreatρ=0.2. Afullta-
to the same BEiT [30] and use the same training strategy blewithaccuracy,completeness,F-scoreatdifferentρisin
suchasGaussiansamplingofpoints. Bothimproveresults. thesupp. Trendsarethesameacrossvaluesofρandthere
Since this baseline only supports single image reconstruc- is no significant accuracy/completeness imbalance for the
tion,weproducepredictionsindependentlyfromeachinput baselines(MCC,SV-DRDF).
imageandaggregateallthepointclouds.
Multiview consistency. Only measuring the F-score does
4.3.EvaluationMetrics not measure the consistency of 3D reconstruction when
generating results from multiple views. Doubled predic-
Weusetwometricstoevaluateoursystem. tionsofsurfacesdonotchangetheSceneFscoreresultsif
Scene F score. Following [20, 44], we compute the scene they are within ρ. Prior work [17] used a detection-based
accuracy(fractionofpredictedpointswithinρofaground method that penalized double surfaces on planar predic-
truth point), completeness (fraction of ground truth points tions,buttheirmetricisnotapplicablesinceitrequirespla-Input Views Ours Ground Truth Input Views Ours Ground Truth
Ours Ground Truth Ours Ground Truth
5-Views
Figure5.Qualitativeresultsonheld-outtestscenes.Toprow:Reconstructionfrom3imagesandcomparedwithgroundtruth.Ourmethod
canreconstructacompletescenestructurewithinallthecamerafrustums,includingtheoccludedsurfaces.Bottomrow:Predictionsfrom
5inputimagescomparedwithgroundtruth.Forthe2ndand3rdexamples,ceilingsareremovedtorevealthedetailsofthescene.
narinstances.Werequireametricthatcanmeasurethecon- as the missing surfaces behind chairs in Row 1; and ab-
sistency of 3D reconstruction of points in individual frus- sent floor sections in Row 4. MCC [44] tends to produce
tums. Specifically, we would like to ensure that points P cloudy volumes and misses details like pillows and tables.
i
generatedfromallqueryraysoriginatingfromc ofπ are Single-viewDRDF(SV-DRDF)producesoccludedregions
i i
consistent with points, P , generated from by ray queries andsharpsurfacesbutlacksconsistencywhenaggregating
j
from c of π at the intersection of frustums of both the resultsfrommultipleviews. Thisisnoticeableinitsinabil-
j j
cameras. For every point, p ∈ P and within the field of ity to reconstruct the occluded wall in Row 2, the creation
j
view of camera i, we compute their minimum distance to ofadoubledceilinginRow3duetoocclusions. 3DFIRES,
pointsinP . Ourmetricmeasurespercentofpointsinthe effectively merges observations from multiple images, re-
i
setP thathaveminimumdistancewithinthethresholdof sultinginsharpandaccuratereconstructionsofbothvisible
j
ρ. We evaluate this metric bidirectionally to ensure com- andhiddensurfaces. Byfusinginformationacrossviewsin
pleteresults. thefeaturespace,ourmethodovercomesthelimitationsof
otherapproaches. Thisensurescomprehensiveandconsis-
4.4.Results tentscene-levelreconstructionfromfewsparseviews.
InFig.5weshowadditionalalongsidethegroundtruth.
QualitativeResults. Fig.3showsreconstructionfromus-
3DFIRES successfully reconstructs large occluded areas,
ing query rays from the blue camera in Fig. 2. Occluded
floors hidden by foreground objects (colored in pink), and
surfaces are colored with surface normals. DRDF [20] is
unseen sides of objects such as the back of chairs in the
unabletoreconstructthepartsofthescenebehindthewall
first example and the kitchen islands in the second exam-
with certainty and erroneously adds a full wall in front of
ple. The reconstruction from multiple views demonstrates
thehallway. 3DFIRESfusesfeaturesfrommultipleimages
consistencyandcoherentsurfacesinoverlappingregions.
(GreenandPurplecamerainFig.2)accuratelypredictsthe
emptyspace. While our method is trained with up to three views, it
Fig. 4 shows results unseen test scenes, and compares seamlessly extends to five views. This adaptability stems
reconstructionofbaselines. Redboxcropshowhighlighted fromourarchitecture’sinherentflexibilitytothenumberof
differences and provide a zoomed-in view for detailed ex- input views. With increasing views it predicts clean and
amination. Depthonly(MiDaSwithgroundtruthscaleand coherentreconstructionswithinallthecamerafrustums.
shift)reconstructsonlyvisibleregionsthisleavesholessuch QuantitativeResults.Weevaluateourmethodonsetsof1,Table1. QuantitativeresultsonSceneF-score(ρ = 0.2)forHiddenpoints,Visiblepoints,Allpoints. For3and5views,weevaluate
Consistency. Depthonly: visiblesurfaceupperboundisseparatedtoindicateithasoracleinformation. Despiteaccuratereconstructions
on visible surfaces, these lines of work cannot recover hidden surfaces, causing low overall performance. With 1 view, 3DFIRES is
comparabletosingleviewDRDF.Withmoreviews,3DFIRESoutperformsalltheotherbaselinesinF-score.Thereislargeimprovement
inconsistencymetriccomparedtosingleviewDRDF,showingthataggregatingfeaturesproducesamorecoherentreconstruction. Full
tablesshowingaccuracyandcompletenessareinthesupplemental.
1view 3views 5views
Hidden↑ Visible↑ All↑ Hidden↑ Visible↑ All↑ Consistency↑ Hidden↑ Visible↑ All↑ Consistency↑
Depthonly - 85.31 60.12 - 87.84 63.90 72.79 - 91.29 69.40 72.57
MCC 40.27 56.40 50.25 42.91 62.02 54.78 70.20 38.51 64.44 55.94 66.57
SV-DRDF 53.36 73.45 65.21 48.02 76.19 65.61 76.44 47.51 81.31 70.54 78.13
3DFIRES 53.34 74.29 65.71 49.99 76.74 66.56 85.48 49.52 81.74 71.41 85.92
Table2.Ablationstudyontrainingstrategies.GS:Gaussiansam- lationstudy (Tab.2) toinvestigatethe effectivenessof dif-
pling near intersection along the ray during training. Ray Attn: ferenttrainingstrategiesforourmethod. WithoutGaussian
pointsalongaqueryrayattendtoeachother.
sampling or ray attention (-GS), the method has degraded
performance (-7% in hidden F score). With ray attention
Hidden Visible All Consistency
only(+RayAttn. -GS),themethodisabletobetterrecon-
-GS 43.07 77.05 64.81 83.45 structthehiddensurfacebutisstillworsethanours(-3%).
+RayAttn.-GS 47.09 77.60 65.58 83.27
WithbothrayattentionandGaussiansampling(+RayAttn.
+RayAttn.+GS 14.85 3.36 13.29 33.56
+GS), the network finds shortcut during training and does
Ours 50.20 77.30 66.46 85.45
notworkduringtesting. WithGaussiansamplingstrategy,
ourmethodperformsthebest.
Table3. Quantitativeresultsonnoisycameraposesgeneratedby
LoFTR,evaluatedon3viewcasesatρ=0.2.3DFIRESassumes Robustness with noisy camera poses. Our method re-
accuratepixel-alignedfeaturesbutstillproducesmoreconsistent quiresaccuratecameraposestoaggregatepixel-alignedfea-
reconstructionscomparedtonotaggregatingfeatures. tures.Thissettingischallengingwithsparseviewdatasince
camera estimation can be noisy. We test if the misalign-
3-View Hidden Visible All Consistency ment of image features caused by noisy camera projection
matricesdegradesoursystem. WeuseLoFTR[38]toesti-
SV-DRDF 37.39 62.71 52.93 57.65
matethecamerarotationandtranslationangleandevaluate
Ours 38.85 62.40 53.19 65.71
the reconstruction within all the camera frustums. Since
LoFTRdoesnotprovideatranslationscale,weuseground
3,5viewsrespectively,asdetailedinTab.1. Ourapproach, truth instead. Tab. 3 shows results on 3-view cases. Our
designed for flexible input views, matches prior works in methodstillhassignificantlyhigherconsistencyoversingle
single-viewscenereconstructionandachievesstate-of-the- viewDRDFbaseline.Weprovideananalysiswithsynthetic
artresultswithmultipleinputviews. Insingle-imagecases, Gaussiancameranoiseinthesupplementary.
itiscomparabletothesingle-viewDRDFbaseline.
5.Conclusions
For 3-view sets, our method outperforms MCC [44] or
DRDF[21]. AlthoughMiDaSwithgroundtruthscaleand
We present 3DFIRES, a scene-level 3D reconstruction
shift demonstrates optimal visible surface reconstruction,
method that requires only one or a few posed images of a
it falls short in overall scene reconstruction because of no scene. Our method takes in an arbitrary number of input
reconstruction on occluded surfaces. When evaluated on views, fuses multi-view information in the features space
scene consistency, 3DFIRES shows a large absolute im- and predicts DRDF given a 3D point and query direction.
provement of > 9%, over the second-best baseline, show- Wetrainourmethodonalarge-scalescenedatasetandshow
ing3DFIRES’sabilitytoaggregatefeaturesacrossviewsto itsstrongabilitytoreconstructbothvisibleandhiddensur-
produceconsistentresults. facescoherentlywithinallthecamerafrustumsonchalleng-
ingwide-baselineimages. Currently,ourmethodsrequires
Thetrendpersistswith5-viewinputs,whereourmethod
pose input from off-the-shelf estimation methods, solving
hasthehighestFscoreandconsistency. Ourmethodisnot
for3Dreconstructionandadaptingtheposesisachalleng-
trainedon5-viewssubsetbutstillremainsrobusttomorein-
ingnextstepandlefttofuturework.
putviewsenhancingthereconstructionqualityinbothvisi-
bleandhiddensurfacereconstructions. Acknowledgments. Thanks to Mohamed Banani, Richard Hig-
gins,ZiyangChenfortheirhelpfulfeedback.ThankstoUMARC
4.5.AblationsandAnalysis forcomputingsupport. ToyotaResearchInstituteprovidedfunds
tosupportthiswork.
Ablation study on training strategy. We conduct an ab-References [19] NileshKulkarni,IshanMisra,ShubhamTulsiani,andAbhi-
navGupta. 3d-relnet:Jointobjectandrelationalnetworkfor
[1] Samir Agarwala, Linyi Jin, Chris Rockwell, and David F
3dprediction. InICCV,2019. 2
Fouhey. Planeformers: From sparse view planes to 3d re-
[20] NileshKulkarni, JustinJohnson, andDavidFFouhey. Di-
construction. InECCV,2022. 1,2,5
rectedraydistancefunctionsfor3dscenereconstruction. In
[2] JulianChibane,AymenMir,andGerardPons-Moll. Neural
ECCV,2022. 1,2,3,4,5,6,7
unsigned distance fields for implicit function learning. In
[21] Nilesh Kulkarni, Linyi Jin, Justin Johnson, and David F
NeurIPS,2020. 2,3,5
Fouhey. Learning to predict scene-level implicit 3d from
[3] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin posedrgbddata. InCVPR,2023. 2,5,8
Chen,andSilvioSavarese. 3d-r2n2: Aunifiedapproachfor
[22] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Si-
single and multi-view 3d object reconstruction. In ECCV,
monLucey. Barf: Bundle-adjustingneuralradiancefields.
2016. 2
InICCV,2021. 1
[4] ManuelDahnert,JiHou,MatthiasNießner,andAngelaDai. [23] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-
Panoptic 3d scene reconstruction from a single rgb image. bastianNowozin,andAndreasGeiger.Occupancynetworks:
NeurIPS,2021. 2 Learning 3d reconstruction in function space. In CVPR,
[5] AngelaDai,ChristianDiller,andMatthiasNießner. Sg-nn: 2019. 2
Sparsegenerativeneuralnetworksforself-supervisedscene [24] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
completionofrgb-dscans. InCVPR,2020. 5 JonathanT.Barron,RaviRamamoorthi,andRenNg. Nerf:
[6] Andrew J Davison, Ian D Reid, Nicholas D Molton, and Representingscenesasneuralradiancefieldsforviewsyn-
Olivier Stasse. Monoslam: Real-time single camera slam. thesis. InECCV,2020. 2,4
TPAMI,2007. 2 [25] ZakMurez,TarrenceVanAs,JamesBartolozzi,AyanSinha,
[7] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra- VijayBadrinarayanan,andAndrewRabinovich.Atlas:End-
manan.Depth-supervisednerf:Fewerviewsandfastertrain- to-end3dscenereconstructionfromposedimages.InECCV,
ingforfree. InCVPR,2022. 2 2020. 2
[8] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir [26] YinyuNie,XiaoguangHan,ShihuiGuo,YujianZheng,Jian
Zamir. Omnidata:Ascalablepipelineformakingmulti-task Chang,andJianJunZhang.Total3dunderstanding:Jointlay-
mid-levelvisiondatasetsfrom3dscans. InICCV,2021. 2, out, objectposeandmeshreconstructionforindoorscenes
5,1 fromasingleimage. InCVPR,2020. 2
[9] HaoqiangFan,HaoSu,andLeonidasJGuibas. Apointset [27] Jeong Joon Park, Peter Florence, Julian Straub, Richard
generationnetworkfor3dobjectreconstructionfromasingle Newcombe,andStevenLovegrove. Deepsdf:Learningcon-
image. InCVPR,2017. 2 tinuous signed distance functions for shape representation.
InCVPR,2019. 1,2
[10] Justin Johnson Georgia Gkioxari, Nikhila Ravi. Learning
3dobjectshapeandlayoutwithout3dsupervision. CVPR, [28] PhilipPritchettandAndrewZisserman.Widebaselinestereo
2022. 2 matching. InICCV,1998. 2
[29] Shengyi Qian, Linyi Jin, and David F Fouhey. Associa-
[11] RohitGirdhar, DavidFFouhey, MikelRodriguez, andAb-
tive3d: Volumetric reconstruction from sparse views. In
hinav Gupta. Learning a predictable and generative vector
ECCV,2020. 2
representationforobjects. InECCV,2016. 2
[30] Rene´ Ranftl,AlexeyBochkovskiy,andVladlenKoltun. Vi-
[12] GeorgiaGkioxari,JitendraMalik,andJustinJohnson. Mesh
sion transformers for dense prediction. ICCV, 2021. 4, 5,
r-cnn. InICCV,2019. 2
6
[13] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
[31] Rene´ Ranftl, Katrin Lasinger, David Hafner, Konrad
Deep residual learning for image recognition. In CVPR,
Schindler, and Vladlen Koltun. Towards robust monocular
2016. 6
depthestimation:Mixingdatasetsforzero-shotcross-dataset
[14] MarikoIsogawa,DorianChan,YeYuan,KrisM.Kitani,and
transfer. TPAMI,2022. 5
MatthewO’Toole. Efficientnon-line-of-sightimagingfrom
[32] ShunsukeSaito,TomasSimon,JasonSaragih,andHanbyul
transientsinograms. InECCV,2020. 1
Joo. Pifuhd: Multi-levelpixel-alignedimplicitfunctionfor
[15] HamidIzadinia,QiShan,andStevenM.Seitz. Im2cad. In
high-resolution3dhumandigitization. InCVPR,2020. 4,5
CVPR,2017. 1
[33] Mehdi S. M. Sajjadi, Henning Meyer, Etienne Pot, Urs
[16] Ziyu Jiang, Buyu Liu, Samuel Schulter, Zhangyang Wang, Bergmann, Klaus Greff, Noha Radwan, Suhani Vora,
andManmohanChandraker. Peek-a-boo:Occlusionreason- MarioLucic,DanielDuckworth,AlexeyDosovitskiy,Jakob
ing in indoor scenes with plane representations. In CVPR, Uszkoreit, Thomas Funkhouser, and Andrea Tagliasacchi.
2020. 2 Scene Representation Transformer: Geometry-Free Novel
[17] Linyi Jin, Shengyi Qian, Andrew Owens, and David F ViewSynthesisThroughSet-LatentSceneRepresentations.
Fouhey. Planarsurfacereconstructionfromsparseviews. In CVPR,2022. 2
ICCV,2021. 1,2,5,6 [34] Daniel Scharstein and Richard Szeliski. A taxonomy and
[18] AbhishekKar,ChristianHa¨ne,andJitendraMalik.Learning evaluation of dense two-frame stereo correspondence algo-
amulti-viewstereomachine. NeurIPS,2017. 2 rithms. IJCV,2002. 2[35] JohannesLSchonbergerandJan-MichaelFrahm. Structure-
from-motionrevisited. InCVPR,2016. 2
[36] Jonathan Shade, Steven Gortler, Li-wei He, and Richard
Szeliski. Layereddepthimages. InSiggraph,1998. 2
[37] VincentSitzmann,JulienMartel,AlexanderBergman,David
Lindell, andGordonWetzstein. Implicitneuralrepresenta-
tionswithperiodicactivationfunctions. NeurIPS,2020. 2
[38] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and
XiaoweiZhou.LoFTR:Detector-freelocalfeaturematching
withtransformers. CVPR,2021. 2,8
[39] JiamingSun,YimingXie,LinghaoChen,XiaoweiZhou,and
HujunBao. Neuralrecon:Real-timecoherent3dreconstruc-
tionfrommonocularvideo. InCVPR,2021. 2,5
[40] Bin Tan, Nan Xue, Tianfu Wu, and Gui-Song Xia. Nope-
sac: Neural one-plane ransac for sparse-view planar 3d re-
construction. IEEE Transactions on Pattern Analysis and
MachineIntelligence,2023. 1,2,5
[41] MatthewTancik,PratulP.Srinivasan,BenMildenhall,Sara
Fridovich-Keil,NithinRaghavan,UtkarshSinghal,RaviRa-
mamoorthi, Jonathan T. Barron, and Ren Ng. Fourier fea-
turesletnetworkslearnhighfrequencyfunctionsinlowdi-
mensionaldomains. NeurIPS,2020. 2,4
[42] Shubham Tulsiani, Saurabh Gupta, David F Fouhey,
AlexeiAEfros,andJitendraMalik. Factoringshape,pose,
andlayoutfromthe2dimageofa3dscene. InCVPR,2018.
1,2
[43] QianqianWang,ZhichengWang,KyleGenova,PratulSrini-
vasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-
Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet:
Learningmulti-viewimage-basedrendering.InCVPR,2021.
2,4,5
[44] Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph
Feichtenhofer, and Georgia Gkioxari. Multiview compres-
sivecodingfor3Dreconstruction.CVPR,2023.1,2,5,6,7,
8
[45] FeiXia,AmirRZamir,ZhiyangHe,AlexanderSax,Jitendra
Malik,andSilvioSavarese. Gibsonenv:Real-worldpercep-
tionforembodiedagents. InCVPR,2018. 5
[46] Yiming Xie, Matheus Gadelha, Fengting Yang, Xiaowei
Zhou, and Huaizu Jiang. Planarrecon: Real-time 3d plane
detectionandreconstructionfromposedmonocularvideos.
InCVPR,2022. 2
[47] AlexYu,VickieYe,MatthewTancik,andAngjooKanazawa.
pixelNeRF:Neuralradiancefieldsfromoneorfewimages.
InCVPR,2021. 2,4,5
[48] Amir R. Zamir, Alexander Sax, William Shen, Leonidas J.
Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy:
Disentanglingtasktransferlearning. InCVPR,2018. 5
[49] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Dis-
tillingview-conditioneddiffusionfor3dreconstruction. In
CVPR,2023. 23DFIRES: Few Image 3D REconstruction for Scenes with Hidden Surfaces
Supplementary Material
The supplemental material shows: details about the dataset mainpaperacrossallthresholds.Depth-onlyoraclebaselinehave
(Sec.6),detaileddescriptionsofimplementation(Sec.7),andad- aslightlyhigherFscorewhenevaluatedwithsmallρbecauseit
ditionalresults(Sec.8). hasgroundtruthscaleandshift.However,theycannotreconstruct
anyofthemandthusget0forrecallonhiddensurfaces.
6.DatasetCuration Ourapproach,designedforflexibleinputviews,matchesprior
works in single-view scene reconstruction and achieves state-of-
To effectively test the ability to reconstruct hidden surfaces, we the-artresultswithmultipleinputviews. Insingle-imagecases,it
needimagesthatcontainlargeoccludedareas. Omnidataalready iscomparabletothesingle-viewDRDFbaseline.
provides posed images with corresponding statistics such as oc- For 3-view sets, our method outperforms MCC [44] or
clusion score, camera pitch, walkability score, objectness score DRDF[20]. MCChashigherrecallinthelowthresholdregime
etc. Weusethesestatisticstofilterdatabettersuitedforourtask. butitspredictionisblobbyduetoitsoccupancyprediction, thus
Weremoveimageswithinthreecriteria.1)imagesmainlycontain havinglowprecisionandresultinginalowoverallFscore. SV-
walls (do not contain foreground objects) by a simple criterion: DRDFhasacomparablescorewithusonvisiblesurfaces,butis
occlusion boundaries less than 1% of the image. The occlusion worse on hidden surfaces, resulting in a lower score for the full
boundaryisprovidedbytheOmnidata[8]. 2)Imageswithlarge scene. Whenevaluatedonsceneconsistency, ourmethodshows
invalidareas(≥10%ofinvaliddepthvalue,e.g.windowsormir- alargeabsoluteimprovementoverallbaselinesonallthresholds
rors)3)Imageslookingupwardtotheceiling(pitch≥0).Wecrop (>11%absoluteimprovementwhenρ=0.05,0.1),demonstrat-
allimagestofixedFoVof63.4◦following[17].Aftercleaningthe ingthatfusingfeaturesfrommultipleviewsenablesamorecoher-
images,wesampleimagesetsfromthesesingleimages. Foraset entprediction.
ofkimages,afterpickinganimageatrandom,eachnewimageis The trend persists with 5-view inputs, where our method has
selectedtohaveatmost70%overlapwithanyexistingimagein thehighestFscoreandconsistency. Ourmethodisnottrainedon
theset,andatleast30%overlapwithatleastoneotherimagein 5-viewssubsetbutstillremainsrobusttomoreinputviewsenhanc-
theset.Theprocessbalancesdiversityandcoherenceintheview- ing the reconstruction quality in both visible and hidden surface
points. Wecollect3781trainingsetsamong≥ 10Kimages. We reconstructions.
sample300setsof3-viewimagesand100setsof5-viewimages
8.3. Robustness analysis with synthetic camera
forevaluationfromtheheld-outtestscenes.
noise.
7.ImplementationDetails
Howrobustisourmethodifthecameraposeisnoisy? Wetestif
themisalignedimagefeaturescausedbynoisycameraprojection
NetworkArchitectureTab.4showsadetailednetworkarchitec-
matricescandegradetheperformanceofoursystem.Wekeepone
ture.
camera at the origin and add random noise to other camera pro-
TrainingOurmethodisimplementedinPyTorchandthe3Dpart
jectionmatrices. Weevaluatethereconstructionwithinthecam-
isinPyTorch3D.Weusemultisteplearningratewithbaselearning
eraattheorigin. Specifically, weparametrizetherotationso(3)
rate of 0.001. The learning rate decays by a factor of 0.1 after
Lie algebra, similar to [22] and synthetically perturb the cam-
100kand200kiterations.WeusetheStochasticGradientDescent
eraposeswithadditivenoiseδR ∼ N(0,σ )andtranslationin
(SGD) optimizer with a momentum of 0.9. The whole network R
δt∼N(0,σ ),whereσ in(0.02,0.04,0.06,0.08,0.10),corre-
istrainedonasingleNVIDIAA100GPUwithbatchsize1for2 t R
spondingtoa95percentileerrorat(3.20,6.37,9.55,12.82,15.97)
days.
[◦]inrotationangleandσ in(0.1,0.2,0.3,0.4,0.5),correspond-
t
ingtoa95percentileerrorat(0.28,0.56,0.84,1.12,1.40)[m]in
8.AdditionalResults
translation. WeevaluatetheFscoreunderdifferentnoiselevels.
ResultsareshowninFig.7. Ourmethodisrobusttomulti-view
8.1.SinglecamerafrustumsinFigure3
features with small noisy camera poses. It is better than single
InFig.6, weshowourpredictionsingreenandpurplecameras, view DRDF baseline when σ ≤ 0.06, corresponding to 9.55◦
R
extendingFig.3. SV-DRDFconcatenatesthesingleviewpredic- rotationerror;orσ ≤ 0.3correspondingtoabig0.84mtransla-
t
tions,soanyinconsistencyamongthepredictionsaffectstheover- tionerror.
allresult.
8.4.Morequalitativeresults
8.2.AdditionalQuantitativeResults
Please check the video or the interactive demo at https://
Extending Tab. 1, we include full evaluation metrics (Scene F jinlinyi.github.io/3DFIRES/.
score and Multiview Consistency) with different thresholds ρ ∈
{0.05,0.1,0.2,0.5}. We also show accuracy and completeness
foreachmethod.WeshowsingleviewinTab.5;3-viewinTab.6;
5-view in Tab. 7. The results are consistent with Tab. 1 of theSV-DRDF [Full] 3DFIRES [Full]
Figure6.ResultsonsinglecamerafrustumofFig.3.SV-DRDFconcatenatesthesingleviewpredictions,soanyinconsistencyamongthe
predictionsaffectstheoverallresult.
Table4. ModelArchitecture. Step(1)isthebackbonefeatureextractor,whichusesaViTtoextractimagefeaturesfromallinputviews.
(2)givenaquery3Dpointsx,wegetitspixelalignedfeaturethroughinterpolation.(3)OurQueryEncoder,whichenablesapredictorto
decidehowtoaggregateinformationacrossimages. (4)-(6)istheDRDFpredictor,whichconcatenatesthefeaturesin(4);self-attention
attendsallfeaturesacrosscamerastoproduceaweightperfeaturein(5);andusestheweighttoproduceafusedfeatureforthepointx
anduseafinallinearlayertoextracta1DDRDFvaluein(6).
Index Inputs Operation OutputShape
(1) NInputImages ViTBackbone N×H×W ×Dimg
(2) queryx,(1) Pixelalignedfeatures{fi}N
i=1
N×Dimg
QueryEncoderq(·):
(3) querypair(x,⃗rq) ∆⃗ri←concat[⃗rq−norm(x−ci),⃗rq·norm(x−ci)], N×Dquery
qi←concat[PE(∆⃗ri),PE(ncd(xi))],
(4) (2)(3) c Lo inn ec aa rt( (q Di im,f gi +),
Dquery,Dfeat)
Dfeat
Linear(Dfeat,Dfeat)
SelfAttention(·,·,·)
(5) (4)
Linear(Dfeat,Dfeat)
Dweight
Softmax()
WeightedPooling()
(6) (4)(5) 1
Linear(Dfeat,1)
Figure7. Fscorewithrespecttodifferentnoiselevel. Ourmethodisrobusttomulti-viewfeatureswithsmallnoisycameraposes. It
isbetterthansingleviewDRDFbaselinewhenσ ≤ 0.06,correspondingto6.44◦ rotationerror;orσ ≤ 0.3correspondingto0.56m
R t
translationerror.Table5.Quantitativeresultsforsingleviewtestscenes.
Hidden
p@0.05 p@0.1 p@0.2 p@0.5 r@0.05 r@0.1 r@0.2 r@0.5 f@0.05 f@0.1 f@0.2 f@0.5
Depthonly - - - - 0 0 0 0 - - - -
MCC 11.84 25.90 45.57 74.42 4.96 20.37 40.87 64.81 6.27 20.87 40.27 66.09
SV-DRDF 22.57 44.07 62.46 82.04 12.82 30.45 50.77 76.10 15.07 33.77 53.36 76.64
Ours 21.13 41.18 58.77 80.01 13.03 31.32 52.85 79.23 15.11 33.79 53.34 77.36
Visible
p@0.05 p@0.1 p@0.2 p@0.5 r@0.05 r@0.1 r@0.2 r@0.5 f@0.05 f@0.1 f@0.2 f@0.5
Depthonly 46.27 71.41 84.59 93.20 36.94 66.63 87.39 97.35 40.14 68.05 85.31 94.96
MCC 14.00 30.76 55.67 86.51 10.38 37.91 59.67 76.45 11.25 32.87 56.40 80.44
SV-DRDF 35.42 60.35 78.45 92.20 25.25 49.56 71.15 89.38 28.44 52.99 73.45 90.03
Ours 35.95 61.44 79.04 92.36 25.53 50.48 72.10 90.20 28.75 54.05 74.29 90.57
FullScene
p@0.05 p@0.1 p@0.2 p@0.5 r@0.05 r@0.1 r@0.2 r@0.5 f@0.05 f@0.1 f@0.2 f@0.5
Depthonly 46.27 71.41 84.59 93.20 21.64 38.15 48.90 53.45 28.21 47.92 60.12 66.11
MCC 13.17 29.02 52.54 82.98 7.83 29.84 50.59 70.13 9.23 28.31 50.25 74.84
SV-DRDF 31.28 55.09 73.20 88.94 19.41 40.41 61.03 82.35 22.96 45.15 65.21 84.49
Ours 30.71 54.38 71.87 87.99 19.80 41.47 62.61 84.35 23.09 45.73 65.71 85.18
Table6.Quantitativeresultsfor3-viewtestscenes.
Hidden[41.9%]
p@0.05 p@0.1 p@0.2 p@0.5 r@0.05 r@0.1 r@0.2 r@0.5 f@0.05 f@0.1 f@0.2 f@0.5
Depthonly - - - - 0 0 0 0 - - - -
MCC 11.47 26.35 46.97 77.15 18.54 32.69 43.92 60.11 12.99 27.23 42.91 65.33
SV-DRDF 17.93 38.14 59.03 84.10 11.33 25.51 42.93 67.26 13.16 29.12 48.02 73.28
Ours 18.41 39.05 60.08 84.95 11.88 26.42 44.97 70.61 13.79 30.22 49.99 75.87
Visible[58.1%]
p@0.05 p@0.1 p@0.2 p@0.5 r@0.05 r@0.1 r@0.2 r@0.5 f@0.05 f@0.1 f@0.2 f@0.5
Depthonly 45.32 73.84 87.95 95.98 40.54 69.31 88.19 97.33 42.26 71.13 87.84 96.56
MCC 15.29 34.53 60.98 90.10 35.50 53.68 64.33 78.90 20.99 41.49 62.02 83.72
SV-DRDF 33.10 60.83 79.35 93.39 30.63 54.27 73.94 90.12 31.24 56.73 76.19 91.50
Ours 34.35 62.45 80.60 93.98 30.57 53.96 73.89 90.28 31.81 57.33 76.74 91.85
FullScene
p@0.05 p@0.1 p@0.2 p@0.5 r@0.05 r@0.1 r@0.2 r@0.5 f@0.05 f@0.1 f@0.2 f@0.5
Depthonly 45.32 73.84 87.95 95.98 24.50 41.38 52.01 56.85 30.69 51.55 63.90 69.99
MCC 14.28 32.37 57.43 86.91 27.46 43.75 54.43 69.64 18.26 36.28 54.78 76.39
SV-DRDF 30.02 56.18 75.15 91.44 22.16 41.26 59.48 79.08 24.79 46.61 65.61 84.25
Ours 30.81 57.21 75.94 91.91 22.26 41.42 60.33 80.72 25.27 47.24 66.56 85.47
RayConsistency
@0.05 @0.1 @0.2 @0.5
Depthonly 27.92 52.27 72.79 91.24
MCC 40.91 59.93 70.20 85.32
SV-DRDF 28.68 54.23 76.44 94.00
Ours 40.94 66.91 85.48 97.24Table7.Quantitativeresultsfor5-viewtestscenes.
Hidden[43.7%]
p@0.05 p@0.1 p@0.2 p@0.5 r@0.05 r@0.1 r@0.2 r@0.5 f@0.05 f@0.1 f@0.2 f@0.5
Depthonly - - - - 0 0 0 0 - - - -
MCC 10.91 25.24 44.69 73.89 6.34 21.64 36.60 53.50 7.30 21.85 38.51 60.05
SV-DRDF 15.99 35.63 57.35 84.27 10.73 25.00 42.58 67.83 12.14 28.16 47.51 73.95
Ours 16.87 37.17 58.95 85.79 11.16 25.97 44.68 71.14 12.77 29.40 49.52 76.58
Visible[56.3%]
p@0.05 p@0.1 p@0.2 p@0.5 r@0.05 r@0.1 r@0.2 r@0.5 f@0.05 f@0.1 f@0.2 f@0.5
Depthonly 46.91 77.30 91.35 98.08 46.18 74.41 91.41 98.48 46.19 75.60 91.29 98.26
MCC 15.87 36.05 63.99 91.93 17.09 47.97 65.55 81.10 16.25 40.88 64.44 85.96
SV-DRDF 35.68 64.60 82.66 95.35 37.00 62.11 80.27 93.99 35.94 62.97 81.31 94.62
Ours 36.01 65.94 84.00 95.89 36.10 61.16 79.86 93.83 35.64 63.12 81.74 94.79
FullScene
p@0.05 p@0.1 p@0.2 p@0.5 r@0.05 r@0.1 r@0.2 r@0.5 f@0.05 f@0.1 f@0.2 f@0.5
Depthonly 46.91 77.30 91.35 98.08 29.53 47.26 57.62 61.79 35.30 57.33 69.40 74.65
MCC 14.94 34.03 60.47 88.66 12.68 37.12 53.35 69.36 13.40 34.86 55.94 77.16
SV-DRDF 32.75 60.36 78.95 93.79 26.51 47.22 64.74 82.75 28.71 52.21 70.54 87.58
Ours 33.00 61.42 80.00 94.28 25.99 46.91 65.28 84.03 28.49 52.47 71.41 88.57
RayConsistency
@0.05 @0.1 @0.2 @0.5
Depthonly 27.27 51.77 72.57 90.47
MCC 13.63 47.26 66.57 83.07
SV-DRDF 29.48 56.04 78.13 94.31
Ours 40.20 67.10 85.92 97.23