Efficient Combinatorial Optimization via Heat Diffusion
HengyuanMa1,WenlianLu1,2,3,4,5,6,JianfengFeng1,2,3,4,7∗
1InstituteofScienceandTechnologyforBrain-inspiredIntelligence,FudanUniversity,Shanghai200433,China
2KeyLaboratoryofComputationalNeuroscienceandBrain-InspiredIntelligence(FudanUniversity),Ministryof
Education,China
3SchoolofMathematicalSciences,FudanUniversity,No. 220HandanRoad,Shanghai,200433,Shanghai,China
4ShanghaiCenterforMathematicalSciences,No. 220HandanRoad,Shanghai,200433,Shanghai,China
5ShanghaiKeyLaboratoryforContemporaryAppliedMathematics,No. 220HandanRoad,Shanghai,200433,
Shanghai,China
6KeyLaboratoryofMathematicsforNonlinearScience,No. 220HandanRoad,Shanghai,200433,Shanghai,China
7DepartmentofComputerScience,UniversityofWarwick,Coventry,CV47AL,UK
∗jffeng@fudan.edu.cn
ABSTRACT
Combinatorialoptimizationproblemsarewidespreadbutinherentlychallengingduetotheirdiscrete
nature. Theprimarylimitationofexistingmethodsisthattheycanonlyaccessasmallfractionof
thesolutionspaceateachiteration,resultinginlimitedefficiencyforsearchingtheglobaloptimal.
Toovercomethischallenge,divergingfromconventionaleffortsofexpandingthesolver’ssearch
scope,wefocusonenablinginformationtoactivelypropagatetothesolverthroughheatdiffusion.
Bytransformingthetargetfunctionwhilepreservingitsoptima,heatdiffusionfacilitatesinformation
flowfromdistantregionstothesolver,providingmoreefficientnavigation. Utilizingheatdiffusion,
weproposeaframeworkforsolvinggeneralcombinatorialoptimizationproblems. Theproposed
methodologydemonstratessuperiorperformanceacrossarangeofthemostchallengingandwidely
encounteredcombinatorialoptimizations. Echoingrecentadvancementsinharnessingthermodynam-
icsforgenerativeartificialintelligence,ourstudyfurtherrevealsitssignificantpotentialinadvancing
combinatorialoptimization.
1 Introduction
Combinatorialoptimizationproblemsareprevalentinvariousapplications,encompassingcircuitdesign[1],machine
learning[2],computervision[3],moleculardynamicssimulation[4],trafficflowoptimization[5],andfinancialrisk
analysis[6]. Thiswidespreadapplicationcreatesasignificantdemandforacceleratedsolutionstotheseproblems.
Alongsideclassicalalgorithms,whichencompassbothexactsolversandmetaheuristics[7],recentyearshaveseen
remarkableadvancementsinaddressingcombinatorialoptimization. Theseincludequantumadiabaticapproaches[8,
9,10],simulatedbifurcation[11,12,13],coherentIsingmachine[14,15],high-orderIsingmachine[16],anddeep
learningtechniques[17,18]. However,duetotheexponentialgrowthofthesolutionnumber,findingtheoptimawithin
alimitedcomputationalbudgetremainsadauntingchallenge.
Akeycomponentofpracticalsolutionsforcombinatorialoptimizationaretheiterativeapproximationsolvers. They
typically begin with an initial solution and iteratively refine it by assessing potentially better solutions within the
neighborhoodofthecurrentsolution, knownasthesearchscopeormorevividly, receptivefield. However, dueto
combinatorialexplosion,asthescopeofthereceptivefieldincreases,thenumberofsolutionstobeassessedgrows
exponentially,makingathoroughevaluationofallthesesolutionscomputationallyunfeasible. Asaresult,current
approachesarelimitedtoanarrowreceptivefieldperiteration,renderingthemblindtodistantregionsinthesolution
spaceandhenceheighteningtheriskofgettingtrappedinlocalminimasorareaswithbumpylandscapes. Although
methodslikelargeneighborhoodsearch[19],variableneighborhoodsearch[20]andpathauxiliarysampling[21]are
designedtobroadenthesearchscope, theycanonlygatheramodestincrementofinformationfromtheexpanded
searchscope. Consequently,thecurrentsolvers’receptivefieldremainssignificantlyconstrained,impedingtheirsearch
efficiency.
Inthisstudy,weapproachtheprevalentlimitationfromauniqueperspective. Insteadofexpandingthesolver’sreceptive
fieldtoacquiremoreinformationfromthesolutionspace,weconcentrateonpropagatinginformationfromdistant
areasofthesolutionspacetothesolverviaheatdiffusion[22]. Toillustrate,imagineasolversearchingfortheoptima
inthesolutionspaceakintoapersonsearchingforakeyinadarkroom,asdepictedinFig.1. Withoutlight,theperson
iscompelledtorelysolelyontouchinghissurroundingspace. Thetactileprovidesonlylocalizedinformation,leading
toinefficientnavigation. Thismirrorsthecurrentsituationincombinatorialoptimization,whereinthereceptivefieldis
predominantlyconfinedtolocalinformation. However,ifthekeyweretoemitheat,itsradiatingwarmthwouldbe
perceptiblefromadistance,actingasadirectionalbeacon. Thiswouldsignificantlyenhancenavigationalefficiencyfor
findingthekey.
4202
raM
31
]LM.tats[
1v75780.3042:viXraoriginal problem problem under heat diffusion
time
high
Where can I I feel a heat flow I know where
find the key？ over here. to go next!
key
low
cooperative optimization
Figure1: TheHeatdiffusionoptimization(HeO)framework. Theefficiencyofsearchingakeyinadarkroomis
significantlyimprovedbyemployingnavigationthatutilizesheatemissionfromthekey. Incombinatorialoptimization,
heat diffusion transforms the target function into multiple versions, each directing information to the solver while
preservingtheoriginal’soptimalsolutions. Theseversionscanbejointlyoptimizedtosolvetheoriginalproblem..
Motivatedbytheabovemetaphor,weproposeasimplebutefficientframeworkutilizingheatdiffusiontosolvevarious
combinatorialoptimizationproblems. Heatdiffusiontransformsthetargetfunctionintodifferentversions,withinwhich
theinformationfromdistantregionsactivelyflowtowardthesolver. Crucially,thebackwarduniquenessoftheheat
equation[23]guaranteesthattheoriginalproblem’soptimaareunchangedunderthesetransformations. Therefore,
informationoftargetfunctionsunderdifferentheatdiffusiontransformationscanbecooperativelyemployedforoptimize
theoriginalproblem(Fig.1). Empirically,ourframeworkdemonstratessuperiorperformancecomparedtoadvanced
algorithmsacrossadiverserangeofcombinatorialoptimizationinstances,spanningquadratictopolynomial,binaryto
ternary,unconstrainedtoconstrained,anddiscretetomixed-variablescenarios. Mirroringtherecentbreakthroughs
ingenerativeartificialintelligencethroughdiffusionprocesses[24],ourresearchfurtherrevealsthepotentialofheat
diffusion,arelatedthermodynamicphenomenon,inenhancingcombinatorialoptimization.
2 Gradient-basedcombinatorialoptimization
Variouscombinatorialoptimizationproblemscanbenaturallyformalizedasapseudo-Booleanoptimization(PBO)
problem[25],inwhichweaimtofindtheminimaofareal-valuetargetfunctionf ∈Rn (cid:55)→Rsubjectingtoabinary
constraints
min f(s), (1)
s∈{−1,1}n
wheresisbinaryconfiguration,andf(·)isthetargetfunction. Throughthetransformation(s+1)/2,ourdefinition
alignswiththatin[26],whereelementsofstake0or1. Giventheadvanceddevelopmentofgradient-basedoptimizers,
weareinterestinginconvertingdiscreteoptimizationproblemsintodifferentiableformats,therebyenablinggradient-
basedoptimizationmethods. Toachievethispurpose,weencodethebitss ,i = 1,...,nasindependentBernoulli
i
variables Bern(θ ) with θ ∈ [0,1]: p(s = ±1|θ) = 0.5±(θ −0.5). In this way, the original combinatorial
i i i i
optimizationproblemisconvertedintoadifferentiableoptimizationproblem
minh(θ), (2)
θ∈I
whereI :=[0,1]n,andh(θ)=E [f(s)]. Theminimaθ∗ofEq.(2)isθ∗ =0.5(sgn(s∗)+1),giventhats∗isa
p(s|θ)
minimaoftheoriginalproblemEq.(1),heresgn(·)isthesignfunction. NowEq.(2)canbesolvedthroughgradient
descentstartingfromsomeθ
0
θ =θ −γ▽ h(θ ), (3)
t+1 t θt t
fort=1,...,T,giventhelearningrateγ andminimumiterationnumberT.
2.1 Parameteruncertainty
AprimaryconcernarisesastheoptimizationprocedureEq.(3)givesadistributionp(s|θ)overthewholeconfiguration
space{−1,1}ninsteadofadeterministicbinaryconfigurations. Theuncertaintyofp(s|θ),whichismeasuredbyits
2
{
temperaturetotalvariance
n
(cid:88)
V(θ)= θ (1−θ ), (4)
i i
i=1
makesitstillambiguouswhichbinaryconfigurationisthebestone. Althoughwecanmanuallybinarizeθthrough
B(θ) := sgn(θ−0.5) to get the binary configuration which maximizes probability p(s|θ), the outcome f(B(θ))
may be much higher than h(θ), resulting in significant performance degradation [27]. This suggests that a good
gradient-basedalgorithmshouldefficientlyreducetheuncertaintyV(θ)tozeroafteroptimization.
2.2 MonteCarlogradientestimation
Conventionally,wecansolvetheproblemEq.(2)byapproximatingthegradientofh(θ)viaMonteCarlogradient
estimation(MCGE)[28],inwhichweestimatethegradientinEq.(3)as
▽ h(θ)=▽ E [f(s)]=E [f(s)▽ logp(s|θ)]
θ θ p(s|θ) p(s|θ) θ
1 (cid:88)M (5)
≈ f(s(m))▽ logp(s(m)|θ), s(m) ∼ p(s|θ), m=1,...,M.,
M θ i.i.d.
m=1
asshowninAlg.1. Noticedthatweclamptheparameterθ inthe[0,1]fornumericalstability;additionally,webinarize
t
theθ toobtaintheoptimizedbinaryconfigurations intheend.
T T
Algorithm1MonteCarlogradientestimationforcombinatorialoptimization(MCGE)
Input: targetfunctionf(·),stepsizeγ,samplenumberM,iterationnumberT
initializeelementsofθ as0.5
0
fort=0toT −1do
samples(m) ∼ p(s|θ ), m=1,...,M
i.i.d. t
g ← 1 (cid:80)M f(s(m))▽ logp(s(m)|θ )
t M m= (cid:0)1 θt
(cid:1)
t
θ ←clamp θ −γg ,0,1
t+1 t t
endfor
s ←sgn(θ −0.5)
T T
Output: binaryconfigurations
T
ItturnsoutthattheMCGE(Alg.1)performspoorlycomparedtoexistingsolverssuchassimulatedannealingand
Hopfieldneuralnetwork,asshowninFig.2.WeexplainthisinferiorperformanceofMCGEasfollows.AlthoughMCGE
turnsthecombinatorialoptimizationproblemintoadifferentiableone,itdoesnotreducetheinherentcomplexityofthe
originalproblem,andthechallengeshiftstonavigatingthecomplexenergylandscape. Thisissueisnotameliorated
even when applying advanced gradient-based solver such as momentum method. As gradient only provides local
information,MCGEmightbeeasilyensnaredinlocalminima. WealsofindthattheMCGEarestrugglingtominimize
theuncertaintyV(θ),asshowninFig.2. Thisobservationclarifieswhy,notwithstandingthewidespreaduseofMCGE
inmachinelearning,thereseemstobeanabsence,tothebestofourknowledge,ofstudiesthatdirectlyemployMCGE
specificallyforcombinatorialoptimization.
3 Heatdiffusionoptimization
ThesuboptimalperformanceoftheMCGEcanbeattributedtoitsrelianceonanarrowreceptivefieldateachiteration,
alimitationsharedwithotherexistingmethods. Wemanagetoprovidemoreefficientnavigationtothegradient-based
solverbyemployingheatdiffusion,whichpropagatesinformationfromdistantregiontothesolver. Intuitively,we
considertheparameterspaceasathermodynamicsystem,whereeachparameter−θisreferredtoasalocationandis
associatedwithaninitialtemperaturevalueh(θ),asshowninFig.1. Thenoptimizationprocedurecanbedescribed
astheprocessthatthesolveriswalkingaroundtheparameterspacetosearchforthelocationθ∗ withthehighest
temperature(equivalently,thelowesth(θ)). Astimeprogresses,heatflowsobeyingtheprinciplesofheattransfer,
leadingtoadynamictemperaturedistributionatvariousmoments. Theheatatthemaximaθ∗flowstootherregions,
reachingthelocationofthesolver. Thisheatflowconveysusefulinformationforthesolvertofindthemaxima,asthe
solvercanfollowthedirectionthatheatcomefromtogettothemaxima.
Now we introduce heat diffusion for solving the combinatorial optimization. We extent the parameter space of
θ from [0,1]n to R¯n with R¯ = R ∪ {−∞,+∞}. To keep the probabilistic distribution p(s|θ) meaningful for
3θ ∈/ [0,1]n,wenowdefinep(s = ±1|θ) = clamp(0.5±(θ −0.5),0,1),wheretheclampfunctionisdefinedas
i i
clamp(x,0,1)=max(0,min(x,1)). Duetoheatexchangebetweenlocationsattheparameterspace,thetemperature
distributionh(θ)variesacrosstimeτ. Denotethetemperaturedistributionatτ asu(τ,θ),thenu(τ,θ)isthesolution
tothefollowingunboundedheatdiffusionequation
(cid:26) ∂ u(τ,θ) = ∆ u(τ,θ), τ >0, θ ∈Rn
uτ (τ,θ) = θ h(θ), τ =0, θ ∈Rn . (6)
Forθ ∈R¯n/Rn,wecomplementthedefinitionbyu(τ,θ)=lim u(τ,θ ),where{θ }isasequenceinRnwith
θn→θ n n
limitationθ. Theheatdiffusionequationexhibitstwocrucialcharacteristicsthatfacilitateitsapplicationinefficient
optimization. Firstly,thepropagationspeedofheatisinfinite[22],implyingthatinformationcarriedbyheatfrom
distantlocationsreachesthesolverinstantaneously. Secondly,heatalwaysspontaneouslyflowsfromareasofhigher
temperaturetolowertemperature. Thissuggeststhatthelocationofeachlocalmaxima(includingtheglobalmaxima)
doesnotchangeacrosstimeτ. Infact,wehavethefollowingtheorem.
Theorem1. Foranyτ >0,thefunctionu(τ,θ)andh(θ)hasthesameglobalmaximainR¯n
argmax θ∈R¯nu(τ,θ)=argmax θ∈R¯nh(θ) (7)
Consequentially,wecangeneralizethegradientdescentapproachEq.(3)bysubstitutingthefunctionh(θ)withu(τ,θ)
withdifferentτ >0atdifferentsteps,asfollows
θ =θ −γ▽ u(τ ,θ ), (8)
t+1 t θt t t
where the subscript ’ ’ in τ means that τ can vary across different steps. In this way, the optimizer receives the
t t t
gradientscontaininginformationaboutdistantregionofthelandscapethatispropagatedbytheheatdiffusion,resulting
inamoreefficientnavigation. However,theiterationofEq.(8)willconvergetoθˇwhichisdefinedas
(cid:26) +∞, s∗ =+1
θˇ = i . (9)
i −∞, s∗ =−1
i
NoticedthatprojectingtheθˇbacktoI givesthemaximaofh(θ)inI: θ∗ = Proj (cid:0) θˇ(cid:1) . Tomaketheoptimization
I
processEq.(8)practicable,weprojecttheθ backtoI aftergradientdescentateachiteration
t
(cid:0) (cid:1)
θ =Proj θ −γ▽ u(τ ,θ ) , (10)
t+1 I t θt t t
sothatθ ∈I alwaysholds. Importantly,duetothepropertyoftheprojection,ifthesolvermovesalongthetowards
t
θˇ, it also gets closer to θ∗. Therefore, Eq. (10) is a reasonable update rule for finding the maxima θ∗ within in I.
Additionally,sincetheconvergenceofEq.(8)isθˇ,whosecoordinationareallinfinity,theconvergentpointofEq.(10)
mustwithintheverticesofI,i.e.,{0,1}n. ThissuggeststhatEq.10isguaranteedtogiveanoutputθdiminishingthe
uncertaintyV(θ).
3.1 Heatequationestimation
Motivated by the above theoretical analysis, we now develop an efficient algorithm for solving the combinatorial
optimizationproblem. ThefirstissueistosolvetheheatdiffusionequationEq.(6)giventhetargetfunctionf(·). When
thedimensionnishigh,thisiscomputationalexpensiveingeneral. ThesolutionofEq.(6)canbewritteninaform
whichhasaobviouslyprobabilisticmeaning
√
u(τ,θ)=E [h(θ+ 2τz)], p(z)=N(0,I). (11)
p(z)
Combinedwiththedefinitionofh(θ),wehave
u(τ,θ)=E [E √ [f(s)]]. (12)
p(z) p(s|θ+ 2τz)
Toconstructalow-varianceestimation,insteadofdirectlyusingtheMonteCarlogradientestimationbysamplingfrom
p(z)andp(s|θ+z)forgradientestimation,wemanagetointegrateoutthestochasticityrespecttoz.Wereparameterize
thep(s|θ)byintroducinganotherrandomvectorx∈Rn,whichisuniformlydistributedon(0,1)n. Itiseasytosee
thatsgn(θ −x )∼Bern(θ ). Replacings byx inEq.(2),wehaveanewexpressionofh(θ)
i i i i i
h(θ)=E [f(sgn(θ−x))], x∼Unif(0,1)n, (13)
p(x)
wheresgn(·)iselement-wisesignfunction. CombineEq.(11)andEq.(13)andexchangetheorderofintegration,we
have
√
u(τ,θ)=E [E [f(sgn(θ+ 2τz−x))]]. (14)
p(x) p(z)
4√
NowwecalculatetheinnertermE [f(sgn(θ+ 2τz−x))]. Assumethetargetfunctionf(s)canbewrittenasa
p(z)
K-ordermultilinearpolynomialofs
(cid:88) (cid:88) (cid:88) (cid:88)
f(s)=a + a s + a s s + a s s s +···+ a s ···s ,
0 1,i1 i1 2,i1i2 i1 12 3,i1i2i3 i1 12 i3 K,i1...iK i1 iK
i1 i1<i2 i1<i2<i3 i1<···<iK
(15)
whichisthecasethatforawiderangeofcombinatorialoptimizationproblems[16]. Wehave
√
E [f(sgn(θ+ 2τz−x))] (16)
p(z)
(cid:88) (cid:88) (cid:88) (cid:88)
=a + a s˜ + a s˜ s˜ + a s˜ s˜ s˜ +···+ a s˜ ···s˜ , (17)
0 1,i1 i1 2,i1i2 i1 12 3,i1i2i3 i1 12 i3 K,i1...iK i1 iK
i1 i1<i2 i1<i2<i3 i1<···<iK
where
√ θ −x
s˜ =E [sgn(θ + 2τz −x )]=erf( i√ i), (18)
i p(zi) i i i
2τ
whereerf(·)istheerrorfunction. Therefore,wehave
θ−x
u(τ,θ)=E [f(erf( √ ))], (19)
p(x)
2τ
whereerf(·)istheelement-wiseerrorfunction. Forthecasethatthetargetfunctionf(s)isnotamultilinearpolynomial
ofs,westillusetheapproximationEq.(19),asweempiricallyfindthattheapproximationworkswell.
3.2 Proposedalgorithm
BasdeonEq.(19),weestimatethegradientofu(τ,θ)as
1 (cid:88)M θ−x(m)
▽ u(τ,θ)≈ ▽ f(erf( √ )), x(m) ∼ Unif[0,1]n. (20)
θ M θ 2τ i.i.d.
m=1
This can be accelerated by GPUs, making the framework scalable for high-dimensional cases. We illustrate the
algorithmforcombinatorialoptimizationinAlg.2,whichwecallHeatdiffusionoptimization(HeO),whereweset
thesamplenumberM as1. OurHeOcanbeequippedwithmomentum,whichisshowninAlg.3inAppendix. In
contrasttothosemethodsspeciallydesignedforsolvingcombinatorialoptimizationwithspecialformsuchasquadratic
unconstrainedbinaryoptimizationwhichonlyinvolve2-orderinteractionsbetweenindividualbitss ,ourHeOcan
i
directlysolvePBOproblemswithgeneralform. AlthoughPBOcanberepresentedasQUBO[29],thisnecessitates
theintroductionofauxiliaryvariables,whichmayconsequentlyincreasetheproblemsizeandleadingtoadditional
computationaloverhead. Infact,thereexistsomePBOthatrequireatleastO(2n/2)auxiliaryvariablestoreformulate
themintoQUBO[30],makingitunfeasibleforsolvingintheseways.
Algorithm2Heatdiffusionoptimization(HeO)
Input: targetfunctionf(·),stepsizeγ,τ schedule{τ },iterationnumberT
t
initializeelementsofθ as0.5
0
fort=0toT −1do
samplex ∼Unif[0,1]n
t
g
t
←▽ θtf(er (cid:0)f(θ√t− 2τx tt))
(cid:1)
θ ←Proj θ −γg
t+1 I t t
endfor
s ←sgn(θ −0.5)
T T
Output: binaryconfigurations
T
3.3 Errorboundanalysis
Onecounter-intuitivethingisthatduetoEq.(10),theHeOactuallyareoptimizeu(τ ,θ)ateachstept,asthesolver
t
movesalongthegradient▽ u(τ ,θ). Itseemsweirdthatweareoptimizingh(θ)byconcurrentlyoptimizingdifferent
θ t
functionsu(τ ,θ). Actually,theoptimizationofdifferentu(τ,θ)coordinatelyhelptotheoptimizationofh(θ). We
t
illustratethisbyprovideatheoreticalupperboundfortheoptimizationlossh(θ)−h(θ∗)relatedtou(τ,θ)−u(τ,θ∗),
theoptimizationlossforu(τ,θ),τ >0.
5Theorem2. Givenτ >0andϵ>0,thereexistsτ ∈(0,τ ),suchthat
2 1 2
h(θ∗)−h(θ)≤(f∗−f )1/2(cid:2)(cid:0) u(τ ,θ∗)−u(τ ,θ)+
n(cid:90) τ2 u(τ,θ∗)−u(τ,θ)
dτ(cid:1)1/2 +ϵ(cid:3) . (21)
min 2 2 2 τ
τ1
Consequently,wecansimultaneouslyoptimizeaseriesσ-diffusionproblemtosolvetheoriginalPBOproblem,which
werefertoasacooperativeoptimizationparadigm,asillustratedinFig.1.
4 Experiments
WeapplyourHeOourseveralchallengingcombinatorialoptimizationproblems. Unlessexplicitlystatedotherwise,
√
weemploytheτ scheduleas 2τ =1−t/T forAlg.2throughoutthiswork. Thischoiceismotivatedbytheaim
t t
toreversethedirectionofheatflowback,guidingthesolvertowardstheoriginofthesource,i.e.,theglobalminima.
Noticedthatthischoiceisnottheoreticallynecessary,aselaboratedinDiscussion.
4.1 Toyexample
Figure 2: Performance of HeO, Monte Carlo gradient estimation (MCGE), Hopfield neural network (HNN) and
simulatedannealing(SA)withdifferenttimestepsonthenonlinearbinaryoptimizationproblem(Eq.(22)). Toppanel:
theenergy(targetfunctionf(·)). Bottompanel: theuncertaintyV(θ)(Eq.(4))ofHeOandMCGE.
To illustrate the efficacy of our HeO for handling general combinatorial optimization problem, we compare the
performanceofourHeOagainstseveralrepresentativemethods: theconventionalprobabilisticparameterizationsolver
MCGE[28],thesimulatedannealing[31],andtheHopfieldneuralnetwork[32]. Weconsiderthefollowingrandomly
constructedneuralnetworktargetfunction
f(s)=aTsigmoid(Ws+a ) (22)
2 1
where sigmoid(x) = 1 is the element-wise sigmoid function, a ∈ Rn, a ∈ Rm, W ∈ Rm,n are randomly
1+e−x 1 2
sampled network parameters. The elements of a ,a ,W and uniformly sampled from [−1,1]. According to the
1 2
universalapproximationtheory[33], f(s)canapproximateanymeasurablefunctionwithsufficientlylargemand
n, thereby representing a general PBO. We apply HeO with momentum (Alg. 3). As shown in Fig. 2, where we
setn,m = 10000,ourHeOdemonstratesexceptionalsuperiorityoverallothermethods,andefficientlyreducesits
uncertaintycomparedtoMCGE.
WeapplyourHeOtoavarietyofcombinatorialoptimizationproblems,includingQUBO,polynomialunconstrained
binaryoptimization(PUBO),ternaryoptimization,mixedcombinatorialoptimization,andconstrainedbinaryoptimiza-
tion,demonstratingitsversatilityandbroadapplicability.
6a b
graph
node partition
examples of cut
cut number: 3 cut number: 2
example of max cut
cut number: 5
Figure 3: a, Illustration of the max-cut problem. b, Performance of HeO and representative approaches including
LQA [10], aSB [12], bSB [13], dSB [13], CIM [34] and SIM-CIM [15] on Max-cut problems from the Biq Mac
Library[35]. Toppanel:averagerelativelossforeachalgorithmoverallproblems. Bottompanel:thecountofinstances
whereeachalgorithmendedupwithoneofthebottom-2worstresultsamongthe7algorithms.
4.2 Quadraticunconstrainedbinaryoptimization
TheQUBOisthecombinatorialoptimizationproblemwithquadraticformtargetfunction
f(s)=sTJs, (23)
whereJ ∈Rn×nisasymmetricmatrixwithzerodiagonals. Awell-knownclassofQUBOismax-cutproblem[27],
whichinvolvesdividingtheverticesofagraphintotwodistinctsubsetsinsuchawaythatthenumberofedgesbetween
thetwosubsetsismaximized. Themax-cutproblemisNP-hard,andcanbeequivalentlyformulatedasthequadratic
formEq.(23),whereJ isdeterminedbytheadjacencymatrixofthegraph,andmaximizingthecut-valueisequivalent
tominimizef(s).
We compare our HeO with representative methods especially developed for solving QUBO including LQA [10],
aSB [12], bSB [13], dSB [13], CIM [34], and SIM-CIM [15] on the 3 sets of max-cut problems in the Biq Mac
Library[35]1. Toreducethefluctuationsoftheresults,foreachalgorithmalgandinstanti,therelativelossiscalculated
as|Ci,alg−Ci |/|Ci |,whereCi,alg isthelowestoutputofthealgorithmalgontheinstanceiover10tries,and
min min
Ci isthelowestoutputofall7thealgorithmontheinstancei. Wereporttherelativelossaveragedoverallinstances
min
andthecountoftheinstanceswhereeachalgorithmendedupwithoneofthebottom-2worseresultsamongthe7
algorithmsinFig.3. ItisobservedthatourHeOissuperiortoothermethodsintermsoftherelativeloss,andstandsout
astheonlyonethatconsistentlyavoidsproducingthebottom-2worstresults. ThisperformanceemphasizesourHeO’s
significantpotentialinhandlingQUBO,acriticalcategoryofcombinatorialoptimization.
4.3 Polynomialunconstrainedbinaryoptimization
PUBOisaclassofcombinatorialoptimizationproblems,inwhichhigher-orderinteractionsbetweenbitss appearsin
i
thetargetfunction.ExistingmethodsforsolvingPUBOfallintotwocategories:thefirstapproachinvolvestransforming
PUBOintoQUBObyaddingauxiliaryvariablesthroughaquadratizationprocess,andthensolvingitasaQUBO
problem [37], and the one directly solves PUBO [16]. The quadratization process may dramatically increases the
dimensionoftheproblem,hencebringsheaviercomputationaloverhead. Onthecontrary,ourHeOcanbedirectlyused
forsolvingPUBO.
A well-known class of PUBO is the Boolean 3-satisfiability (3-SAT) problem [27], which is NP-complete. The
3-SATprobleminvolvesdeterminingthesatisfiabilityofaBooleanformulaovernBooleanvariablesb ,...,b where
1 n
b ∈{0,1}. TheBooleanformulaisstructuredinConjunctiveNormalForm(CNF)consistingofH conjunction(∧)
i
ofclauses,andeachclausehisadisjunction(∨)ofexactlythreeliterals. AliteraliseitheraBooleanvariableorits
negation. Analgorithmof3-SATaimstofindtheBooleanvariablesthatmakesasmanyasclausessatisfied.
1https://biqmac.aau.at/biqmaclib.html
7a b
Conjunctive normal form (CNF)
Circuit representation
OR
AND
satisfying or not
Figure4: a,IllustrationoftheBoolean3-satisfiability(3-SAT)problem. b,The3-SATproblemswithvariousnumber
ofvariablesfromtheSATLIB[36]. Toppanel: ThemeanpercentofconstraintssatisfiedofHeO,2-orderand3-order
oscillationIsingmachine(OIM)[16]undervariousnumbersofthevariablesinthe3-SAT.Bottompanel:Theprobability
ofsatisfyingallconstraintsofeachalgorithmversusthenumberofthevariablesinthe3-SATproblems.
ToapplyourHeOtothe3-SAT,weformulate3-SATasaPBO.WeencodeeachBooleanvariableb ass ,whichis
i i
assignedwithvalue1ifb =1,otherwises =−1. Foraliteral,wedefineavaluec ,whichis−1iftheliteralisthe
i i hi
negationofthecorrespondingBooleanvariable,elsewiseitis1. ThenfindingtheBooleanvariablesthatmakesasmany
asclausessatisfiedisequivalenttominimizethetargetfunction
H 3
f(s)=(cid:88)(cid:89)1−c his
hi.
(24)
2
h=1i=1
WecomparedourHeOwiththesecond-orderoscillatorIsingmachines(OIM)solverthatusingquadratizationandthe
state-of-arthigh-orderOIMproposedin[16]on3-SATinstanceinSATLIB2.AsshowninFig.4,ourHeOissuperior
to other methods in attaining higher number of satisfied solutions. Notably, our method is able to find satisfiable
solutions tothe 250-variable 3-SAT problems, whichis just achieved by thehigher-orderoscillator Ising machine
recently,whilethe2-orderoscillatorIsingmachineshavebeenunabletofindsolutionssatisfyingallclausesbefore[16].
ThishighlightsourHeOsignificantpotentialinhandlinghigher-ordercombinatorialoptimizationproblems.
4.4 Ternaryoptimization
Neural networks excel in learning and modeling complex functions. However, neural networks comprising a vast
numberofparameterscanbringaboutconsiderablecomputationaldemands. Apromisingstrategytomitigatethis
issueinvolvesconstrainingparameterstodiscretevalues,significantlyreducingmemoryusageduringinferenceand
enhancingimplementationefficiency[38]. Amongsucheffortsistheadoptionofternaryvalueparameters(−1,0,1),
whichsimplifiescomputationsbyeliminatingtheneedformultipliers. Yet,trainingnetworkswithdiscreteparameters
introducesasignificantchallengeduetothehigh-dimensionalcombinatorialoptimizationproblemitpresents.
We apply our HeO to train neural networks with ternary value. Supposed that we have an input-output dataset
D generated by a ground-truth ternary single-layer perceptron Γ(v;W ) = Relu(W v), where Relu(x) =
GT GT
max{0,x}istheelement-wiseReluactivation,W ∈{−1,0,1}m×nistheground-truthternaryweightparameter,
GT
v∈{−1,0,1}nistheinput. WeaimtofindtheternaryconfigurationW ∈{−1,0,1}m×nminimizingtheMSE
MSE(W,D)= 1 (cid:88) ∥Γ(v;W)−y∥2. (25)
|D|
(v,y)∈D
wherey=Γ(v;W )∈Rmisthemodeloutput.
GT
We generalized our HeO from the binary to the ternary case by representing a ternary variable s ∈ {−1,0,1} as
t
s = 1(s +s )withtwobitss ,s ∈{−1,1}. Inthisway,eachelementofW canberepresentedasafunction
t 2 b,1 b,2 b,1 b,2
ofs∈Rm×n×2. Wedenotethisrelationasamatrix-valuefunctionW =W(s)
1
W (s)= (s +s ), i=1,...,m, ,j =1,...,n, (26)
ij 2 ij,1 ij,2
2https://www.cs.ubc.ca/~hoos/SATLIB/benchm.html
8a
b
v
y
W
x =
ReLU
accuracy
W’
training by HeO
{(v, y),...}
dataset
Figure5: a,Traininganetworkwithternary-value(−1,0,1)weightbasedoninput-outputpairs. b,Theweightvalue
accuracyoftheHeOunderdifferentsizesoftrainingset. (n=100,m=1,5,20,50). Foreachtest,weestimatethe
meanfrom10runs,andthestandarddeviationissmallhenceimperceptible.
Figure6: Thevariableselectionof400-dimensionallinearregressionsusingHeOandLasso(L )regression[39]and
1
L regression[40]. Weconsiderthecasesofq =0.05,0.1andreporttheaccuracyofeachalgorithmindetermining
0.5
whethereachvariableshouldbeignoredforpredictionandtheirMSEonthetestset. Themean(dots)andstandard
deviation(bars)areestimatedover10runs.
andthetargetfunctionisdefinedas
f(s)= 1 (cid:88) ∥Γ(v;W(s))−y∥2. (27)
|D|
(v,y)∈D
AsshowninFig.5,withtheincreasingin|D|,theaccuracyofthelearnedweightvaluesquicklyincreasesto1. This
highlightsthepotentialofourHeOinlearningintricatediscretestructuresunderdifferentoutputsizem=1,5,20,50.
Noticedthatthelowertheoutputdimensionmandtrainingsetsizeare,theharderthetaskis.
4.5 Mixedcombinatorialoptimization
Inhigh-dimensionallinearregression,thepresenceofnumerousvariables,amongwhichonlyasmallfractionsignifi-
cantlycontributetoprediction,isacommonscenario. Therefore,identifyingandselectingasubsetofvariableswith
strongpredictivepower—aprocessknownasvariableselection—iscrucial. Thispracticenotonlyimprovesthemodel’s
generalizability and computational efficiency but also enhances its interpretability. [41]. However, direct variable
selectionisacombinatorialoptimizationmixedwithcontinuousvariables,entailingtacklingNP-hardproblems[42].
Asapracticalalternative,regularizationmethodslikelassoalgorithmarecommonlyemployed[39].
9Consideradatasetgeneratedfromalinearmodel,inwhichtheinputvobeysastandardGaussiandistribution,andthe
outputisgeneratedbyy =β∗·v+ϵ,whereβ∗istheground-truthlinearcoefficientandϵisindependentGaussian
noise with standard deviation σ . Suppose that a substantial proportion of its elements β∗ are zero, hence only a
ϵ i
smallproportion(denotedasq ∈(0,1))ofvariablesshouldbeconsideredforprediction. Ourgoalistoidentifythese
variablesthroughanindicatorvectors∈{−1,1}n(where1indicatesselectionand−1indicatesnon-selection)and
estimatethecoefficientβ. Thetargetfunctionoftheproblemcanbewrittenas
1 (cid:88) (cid:12) (cid:12)(cid:0) s+1(cid:1) (cid:12) (cid:12)2
f(s;β)= (cid:12) β⊙ ·v−y(cid:12) , (28)
|D| (cid:12) 2 (cid:12)
(v,y)∈D
where1∈Rnisthevectorwhoseelementsareall1.
WedevelopanalgorithmbasedonHeOforthismixedcombinatorialoptimization(Alg.5,Appendix). Weparameterize
thedistributionofsasθasbefore,andminimizethefittinglossrelativetoθwhileslowingchangingβviaitserror
gradient. Afterbinarizingtheresultingθtogettheindicators,weimplementanordinaryleastsquaresregressionon
thevariablesselectedbythestodeterminethenon-zerocoefficients. AsshowninFig.6,ourHeOoutperformsboth
LassoregressionandthemoreadvancedL regression[40]intermsofproducingmoreaccurateindicatorssand
0.5
achievinglowertestpredictionerrorsacrossvariousqandσ settings. ThisrevealstheefficiencyofourHeOformixed
ϵ
combinatorialoptimization.
4.6 Constrainedbinaryoptimization
covered vertex
graph examples of vertex cover examples of minimum vertex cover
Figure7: TheHeatdiffusionoptimization(HeO)framework. Theillustrationofminimumvertexcover.
Table1: TheattributesofrealworlddatasetandthevertexcoversizeofHeOandFastVConthem.
Graphname Vertexnumber Edgenumber FastVC HeO
tech-RL-caida 190914 607610 78306 77372.4(17.4)
soc-youtube 495957 1936748 149458 148875.2(25.0)
inf-roadNet-PA 1087562 1541514 588306 587401.0(103.5)
inf-roadNet-CA 1957027 2760388 1063352 1061338.5(31.9)
socfb-B-anon 2937612 20959854 338724 312531.4(194.0)
socfb-A-anon 3097165 23667394 421123 387730.3(355.3)
socfb-uci-uni 58790782 92208195 869457 867863.0(36.3)
We now consider the combinatorial optimization problem involving multiple constraints, typically expressed as
g (s)≤0fork =1,...,K. Oneclassoftheconstrainedcombinatorialoptimizationisminimumvertexcover(MVC),
k
aconstrainedbinaryoptimizationproblemwithwideapplications[43],asillustratedinFig.7. Givenanundirected
graph G with vertex set V and edge set E, the MVC is to find the minimum subset V ⊂ V, so that for each edge
c
e ∈ E,atleastoneofitsendpointsbelongstoV . ThemostpopularapproachesfortacklingtheMVCareheuristic
c
algorithms[44].
WedevelopanalgorithmleveragingHeOfortacklingconstrainedMVC(Alg.6,Appendix),wherewetransformthese
constraintsintoanunconstrainedformatbyemployingpenaltycoefficientsλ.WecomparetheperformanceofHeOwith
theFastVC[44],apowerfulMVCalgorithmbasedonheuristicsearchonseveralrealworldgraphdatasetswithmassive
vertexandedgenumber3. Forafaircomparison,wecontroltheruntimeoftwoalgorithmsasthesame. Asshown
inTab.1e,ourHeOidentifiessmallercoversetsinthesameruntimeasFastVConmassivereal-worldgraphs. This
3http://networkrepository.com/
10indicatesthepotentialofourHeOforconstrainedbinaryoptimizationproblemsandhighlightsitslowcomputational
complexity.
5 Discussion
5.1 Relatedwork
SeveralstudiesexhibitsimilaritiestoourHeO.AnalogoustoourHeO,thecross-entropy(CE)method[45]models
thesolutionspaceofcombinatorialoptimizationviaaparameterizeddistribution,withiterativeparameterupdates.
However,unlikeHeOwhichrequiresonlyasinglesampleperiteration,theCEmethodupdatesparametersbyincreasing
sampleprobabilitiesofimprovingthetargetfunctionvalues,necessitatingalargenumberofsampleseachiteration.
ParalleltoHeO,othermethodsintegrategradientinformationincombinatorialoptimization. Forexample,theGibbs-
With-Gradientalgorithm[46]employsgradientsofthelikelihoodfunctionrelativetodiscreteinputsformoreefficient
explorationinthesolutionspace. Incontrast, ourHeOexploretheparameterspaceviagradientinformationfrom
the target function. Sun et al. (2023) employ a denoising diffusion model (DDM) [24] for solving combinatorial
optimizationproblems[47]. ThoughtheDDMincorporatesadiffusionprocessakintotheheatdiffusioninourHeO,it
divergesinkeyaspects,asdiscussedlater. Moreover,akintootherdeeplearning-basedsolvers,thismethodnecessitates
asubstantialdatasetfortraining. Incontrast,ourHeOoperatesdirectlywithouttraining. Additionally,ourHeOemerges
asavariantofrandomizedsmoothing,atechniqueappliedtonon-smoothconvexoptimizationproblems[48],known
foritseffectivenessinacceleratingoptimizationprocedure. AdistinctivefeatureofourHeOisthatunderdifferentτ,
thefunctionu(τ,θ)preservestheoptimaoftheoriginaltargetfunctionh(θ). Thisuniquepropertydistinguishesthe
τ-diffusion(fromh(θ)tou(τ,θ))fromthetransformationinexistingmethodssuchasquantumadiabatictheory[9],
bifurcationtheory[12]andotherrelaxationstrategies[49],inwhichthereisinconsistentontheoptimabetweenthe
transformedandoriginalone[27,50]. Thisisalsodifferentfrompriortechniquesthatsmooththeenergylandscapeby
introducingnoisewhileintroducingbiastotheoptimasimultaneously[51].
5.2 Relationtodenoisingdiffusionmodels
Ourapproach,whilebearingsimilaritiestotheDDM—ahighlyregardedandextensivelyutilizedartificialgenerative
model[24]thatreliesonthereversediffusionprocessfordatageneration—differsinkeyaspects.TheDDMnecessitates
reversingthediffusionprocesstogeneratedatathatalignswiththetargetdistribution. Incontrast,itisunnecessary
forourHeOtostrictlyadheringtothereversetimesequenceτ intheoptimizationprocess,asunderdifferentτ,the
t
functionu(τ,θ)sharesthesameoptimawiththatoftheoriginalproblemh(θ). ThisclaimiscorroboratedinFig.S1,
Appendix,whereHeOapplyingnonmonotonicschedulesofτ stilldemonstratessuperiorperformance. Hence,itis
t
possibletoexplorediverseτ schedulestofurtherenhancetheperformanceoftheframework.
t
5.3 Generalparabolicdifferentialequations
Inthisstudy,theheatdiffusionequationformsthecornerstoneoftheproposedHeO.Broadly,HeOcanbenaturally
extendedtoencompassgeneralparabolicdifferentialequations,giventhatabroadspectrumoftheseequationsadhere
tothepropertyofbackwarduniqueness[52]. Forexample,wecanapplyaparabolicpartialdifferentialequationwith
constantcoefficienttoreplacetheEq.(6):
(cid:26) ∂ u(τ,θ)=▽ [A▽ u(τ,θ)], τ >0, θ ∈Rn
τ u(τ,θ)θ =h(θθ ), τ =0, θ ∈Rn , (29)
where A is a real positive definite matrix. Our prior research demonstrated that the optimization of the denoising
diffusionmodelcanbesignificantlyexpeditedbyincorporatinganisotropicGaussiannoise[53]. Thisimpliesthat
judiciously choosing a suitable matrix A could substantially improve the efficacy of the proposed framework in
combinatorialoptimization.
5.4 Limitations
Despite the effectiveness of HeO on the problems we have considered, it does have its limitations. Specifically,
HeO demonstrates inefficiency in solving integer linear programming and routing problems, primarily due to the
challenges in encoding integer variables through the probabilistic parameterization employed in our framework.
However,integratingheatdiffusionprincipleswithadvancedMetropolis-Hastingsalgorithm[54]presentsapossible
pathtobroadentheapplicabilityofourconcepttoawiderrangeofcombinatorialoptimizationproblems.
116 Conclusion
In conclusion, grounded in the heat diffusion, we present a probabilistic parameterization framework called Heat
diffusion optimization (HeO) to solve various combinatorial optimization problems. The heat diffusion facilitates
thetransmissionofinformationfromdistantregionstothesolver,therebyenhancingtheefficiencyinsearchingfor
theglobaloptima. Demonstratingexceptionalperformanceinnumerouscombinatorialoptimizationscenarios,our
HeO underscores the potential of leveraging heat diffusion to overcome challenges associated with combinatorial
explosion.
References
[1] FranciscoBarahona,MartinGrötschel,MichaelJünger,andGerhardReinelt. Anapplicationofcombinatorial
optimizationtostatisticalphysicsandcircuitlayoutdesign. OperationsResearch,36(3):493–513,1988.
[2] JunWang,TonyJebara,andShih-FuChang. Semi-supervisedlearningusinggreedymax-cut. TheJournalof
MachineLearningResearch,14(1):771–800,2013.
[3] Chetan Arora, Subhashis Banerjee, Prem Kalra, and SN Maheshwari. An efficient graph cut algorithm for
computervisionproblems. InComputerVision–ECCV2010: 11thEuropeanConferenceonComputerVision,
Heraklion,Crete,Greece,September5-11,2010,Proceedings,PartIII11,pages552–565.Springer,2010.
[4] HayatoUshijima-Mwesigwa,ChristianFANegre,andSusanMMniszewski. Graphpartitioningusingquantum
annealing on the d-wave system. In Proceedings of the Second International Workshop on Post Moores Era
Supercomputing,pages22–29,2017.
[5] Florian Neukart, Gabriele Compostella, Christian Seidel, David Von Dollen, Sheir Yarkoni, and Bob Parney.
Trafficflowoptimizationusingaquantumannealer. FrontiersinICT,4:29,2017.
[6] RománOrús,SamuelMugel,andEnriqueLizaso. Quantumcomputingforfinance: Overviewandprospects.
ReviewsinPhysics,4:100028,2019.
[7] RafaelMartiandGerhardReinelt. ExactandHeuristicMethodsinCombinatorialOptimization,volume175.
Springer,2022.
[8] EdwardFarhi,JeffreyGoldstone,SamGutmann,JoshuaLapan,AndrewLundgren,andDanielPreda. Aquantum
adiabaticevolutionalgorithmappliedtorandominstancesofannp-completeproblem. Science,292(5516):472–
475,2001.
[9] JohnASmolinandGraemeSmith. Classicalsignatureofquantumannealing. Frontiersinphysics,2:52,2014.
[10] JosephBowles,AlexandreDauphin,PatrickHuembeli,JoséMartinez,andAntonioAcín. Quadraticunconstrained
binaryoptimizationviaquantum-inspiredannealing. PhysicalReviewApplied,18(3):034016,2022.
[11] MáriaErcsey-RavaszandZoltánToroczkai. Optimizationhardnessastransientchaosinananalogapproachto
constraintsatisfaction. NaturePhysics,7(12):966–970,2011.
[12] HayatoGoto,KosukeTatsumura,andAlexanderRDixon. Combinatorialoptimizationbysimulatingadiabatic
bifurcationsinnonlinearhamiltoniansystems. Scienceadvances,5(4):eaav2372,2019.
[13] HayatoGoto,KotaroEndo,MasaruSuzuki,YoshisatoSakai,TaroKanao,YoheiHamakawa,RyoHidaka,Masaya
Yamasaki,andKosukeTatsumura. High-performancecombinatorialoptimizationbasedonclassicalmechanics.
ScienceAdvances,7(6):eabe7953,2021.
[14] TakahiroInagaki,YoshitakaHaribara,KojiIgarashi,TomohiroSonobe,ShuheiTamate,ToshimoriHonjo,Alireza
Marandi, Peter L McMahon, Takeshi Umeki, Koji Enbutsu, et al. A coherent ising machine for 2000-node
optimizationproblems. Science,354(6312):603–606,2016.
[15] EgorSTiunov,AlexanderEUlanov,andAILvovsky. Annealingbysimulatingthecoherentisingmachine. Optics
express,27(7):10288–10295,2019.
[16] Connor Bybee, Denis Kleyko, Dmitri E Nikonov, Amir Khosrowshahi, Bruno A Olshausen, and Friedrich T
Sommer. Efficientoptimizationwithhigher-orderisingmachines. NatureCommunications,14(1):6033,2023.
[17] MartinJASchuetz,JKyleBrubaker,andHelmutGKatzgraber. Combinatorialoptimizationwithphysics-inspired
graphneuralnetworks. NatureMachineIntelligence,4(4):367–377,2022.
[18] BernardinoRomera-Paredes,MohammadaminBarekatain,AlexanderNovikov,MatejBalog,MPawanKumar,
EmilienDupont, FranciscoJRRuiz, JordanSEllenberg, PengmingWang, OmarFawzi, etal. Mathematical
discoveriesfromprogramsearchwithlargelanguagemodels. Nature,pages1–3,2023.
12[19] DavidPisingerandStefanRopke. Largeneighborhoodsearch. Handbookofmetaheuristics,pages99–127,2019.
[20] PierreHansen, NenadMladenovic´, andJoseAMorenoPerez. Variableneighbourhoodsearch: methodsand
applications. AnnalsofOperationsResearch,175:367–407,2010.
[21] HaoranSun,HanjunDai,WeiXia,andArunRamamurthy. Pathauxiliaryproposalformcmcindiscretespace. In
InternationalConferenceonLearningRepresentations,2021.
[22] LawrenceCEvans. Partialdifferentialequations,volume19. AmericanMathematicalSociety,2022.
[23] Jean-MichelGhidaglia. Somebackwarduniquenessresults. NonlinearAnalysis: Theory,Methods&Applications,
10(8):777–790,1986.
[24] LingYang,ZhilongZhang,YangSong,ShendaHong,RunshengXu,YueZhao,WentaoZhang,BinCui,and
Ming-HsuanYang. Diffusionmodels: Acomprehensivesurveyofmethodsandapplications. ACMComputing
Surveys,56(4):1–39,2023.
[25] EndreBorosandPeterLHammer.Pseudo-booleanoptimization.Discreteappliedmathematics,123(1-3):155–225,
2002.
[26] YvesCramaandPeterLHammer.Booleanfunctions:Theory,algorithms,andapplications.CambridgeUniversity
Press,2011.
[27] BernhardHKorte,JensVygen,BKorte,andJVygen. Combinatorialoptimization,volume1. Springer,2011.
[28] Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in
machinelearning. TheJournalofMachineLearningResearch,21(1):5183–5244,2020.
[29] AvradipMandal,ArnabRoy,SarvagyaUpadhyay,andHayatoUshijima-Mwesigwa. Compressedquadratization
ofhigherorder binaryoptimizationproblems. In Proceedingsofthe 17th ACMInternational Conference on
ComputingFrontiers,pages126–131,2020.
[30] MartinAnthony,EndreBoros,YvesCrama,andAritananGruber. Quadraticreformulationsofnonlinearbinary
optimizationproblems. MathematicalProgramming,162:115–144,2017.
[31] MichelGendreau,Jean-YvesPotvin,etal. Handbookofmetaheuristics,volume2. Springer,2010.
[32] JohnJHopfieldandDavidWTank. “neural”computationofdecisionsinoptimizationproblems. Biological
cybernetics,52(3):141–152,1985.
[33] GeorgeCybenko. Approximationbysuperpositionsofasigmoidalfunction. Mathematicsofcontrol,signalsand
systems,2(4):303–314,1989.
[34] ZheWang,AlirezaMarandi,KaiWen,RobertLByer,andYoshihisaYamamoto. Coherentisingmachinebased
ondegenerateopticalparametricoscillators. PhysicalReviewA,88(6):063853,2013.
[35] AngelikaWiegele. Biqmaclibrary—acollectionofmax-cutandquadratic0-1programminginstancesofmedium
size. Preprint,51,2007.
[36] HolgerHHoosandThomasStützle. Satlib: Anonlineresourceforresearchonsat. Sat,2000:283–292,2000.
[37] AndrewLucas. Isingformulationsofmanynpproblems. Frontiersinphysics,2:5,2014.
[38] AmirGholami,SehoonKim,ZhenDong,ZheweiYao,MichaelWMahoney,andKurtKeutzer. Asurveyof
quantizationmethodsforefficientneuralnetworkinference. InLow-PowerComputerVision,pages291–326.
ChapmanandHall/CRC,2022.
[39] RobertTibshirani. Regressionshrinkageandselectionviathelasso. JournaloftheRoyalStatisticalSocietySeries
B:StatisticalMethodology,58(1):267–288,1996.
[40] ZongbenXu, HaiZhang, YaoWang, XiangYuChang, andYongLiang. L1/2regularization. ScienceChina
InformationSciences,53:1159–1169,2010.
[41] Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. The elements of statistical
learning: datamining,inference,andprediction,volume2. Springer,2009.
[42] Xiaoping Li, Yadi Wang, and Rubén Ruiz. A survey on sparse learning models for feature selection. IEEE
transactionsoncybernetics,52(3):1642–1660,2020.
[43] Alexander C Reis, Sean M Halper, Grace E Vezeau, Daniel P Cetnar, Ayaan Hossain, Phillip R Clauer, and
HowardMSalis. Simultaneousrepressionofmultiplebacterialgenesusingnonrepetitiveextra-longsgrnaarrays.
Naturebiotechnology,37(11):1294–1301,2019.
[44] ShaoweiCai,JinkunLin,andChuanLuo. Findingasmallvertexcoverinmassivesparsegraphs: Construct,local
search,andpreprocess. JournalofArtificialIntelligenceResearch,59:463–494,2017.
13[45] Pieter-TjerkDeBoer,DirkPKroese,ShieMannor,andReuvenYRubinstein. Atutorialonthecross-entropy
method. Annalsofoperationsresearch,134:19–67,2005.
[46] WillGrathwohl,KevinSwersky,MiladHashemi,DavidDuvenaud,andChrisMaddison. Oopsitookagradient:
Scalablesamplingfordiscretedistributions. InInternationalConferenceonMachineLearning,pages3831–3841.
PMLR,2021.
[47] ZhiqingSunandYimingYang. Difusco: Graph-baseddiffusionsolversforcombinatorialoptimization. Advances
inneuralinformationprocessingsystems,2023.
[48] JohnCDuchi,PeterLBartlett,andMartinJWainwright. Randomizedsmoothingforstochasticoptimization.
SIAMJournalonOptimization,22(2):674–701,2012.
[49] NikolaosKaraliasandAndreasLoukas. Erdosgoesneural: anunsupervisedlearningframeworkforcombinatorial
optimizationongraphs. AdvancesinNeuralInformationProcessingSystems,33:6659–6672,2020.
[50] Haoyu Peter Wang, Nan Wu, Hang Yang, Cong Hao, and Pan Li. Unsupervised learning for combinatorial
optimizationwithprincipledobjectiverelaxation. AdvancesinNeuralInformationProcessingSystems,35:31444–
31458,2022.
[51] MoZhou,TianyiLiu,YanLi,DachaoLin,EnluZhou,andTuoZhao. Towardunderstandingtheimportanceof
noiseintrainingneuralnetworks. InInternationalConferenceonMachineLearning,pages7594–7602.PMLR,
2019.
[52] JieWuandLiqunZhang. Backwarduniquenessforgeneralparabolicoperatorsinthewholespace. Calculusof
VariationsandPartialDifferentialEquations,58:1–19,2019.
[53] HengyuanMa, LiZhang, XiatianZhu, andJianfengFeng. Acceleratingscore-basedgenerativemodelswith
preconditioneddiffusionsampling. InEuropeanConferenceonComputerVision,pages1–16.Springer,2022.
[54] Haoran Sun, Katayoon Goshvadi, Azade Nova, Dale Schuurmans, and Hanjun Dai. Revisiting sampling for
combinatorialoptimization. InInternationalConferenceonMachineLearning, pages32859–32874.PMLR,
2023.
[55] JuntaoWang,DanielEbler,KYMichaelWong,DavidShuiWingHui,andJieSun. Bifurcationbehaviorsshape
howcontinuousphysicaldynamicssolvesdiscreteisingoptimization. NatureCommunications,14(1):2510,2023.
14Appendix
1 Proofoftheorems
ToprovetheThm.1,werecallthebackwarduniquenessoftheheatequation[23].
TheoremS3. Giventwoboundedfunctionf ,f withdomainonRn. Denote
1 2
√
u (τ,x)=E[f (x+ 2τz)] (S1)
i i
fori=1,2,withz∼N(0,I). If
u (τ,x)=u (τ,x), ∀x∈Rn (S2)
1 2
forsomeτ >0,wehavef =f .
1 2
ProofoftheThm.1.
Proof. Wefirstshowthattheglobalmaximaofu(τ,θ)isalsotheglobalmaximaofh(θ). Thecornerstoneofthe
prooftheisbackwarduniquenessoftheheatequation[23],whichassertsthattheinitialstateofaheatequationcan
beuniquelydeterminedbyitsstateatatimepointτ undermildconditions. Toutilizethebackwarduniqueness,we
considerthereparameterizep(s|θ),sothath(θ)canbewrittenasE [f(sgn(θ−x))]withx∼Unif[0,1]n. Using
p(x)
thedefinitionoftheheatkernel,wehave
√
u(τ,θ)=E [h(θ+ 2τz)], p(z)=N(0,I). (S3)
p(z)
Therefore,wehave
√ √
u(τ,θ)=E E [[f(sgn(θ+ 2τz−x)]]=E [E [f(sgn(θ−(x+ 2τz))]]. (S4)
p(z) p(x) p(x) p(z)
√
Denote u(τ,x;θ) = E [f(sgn(θ −(x+ 2τz)))], x ∈ (0,1)n. Noticed that u(τ,x;θ) is the solution of the
p(z)
followingunboundedheatdiffusionequationrestrictedon(0,1)n
(cid:26) ∂ u(τ,x;θ) = ∆ u(τ,x;θ), τ >0, x∈Rn
uτ (τ,x;θ) = f(sx gn(θ−x)), τ =0, x∈Rn , (S5)
hencethelattercanbeconsideredasanextensionoftheformer. Sinceu(τ,x;θ)isanalyticrespecttox∈(0,1)nfor
τ >0,thisextensionisunique. Therefore,thevalueofu(τ,x;θ)on(0,1)n,τ >0uniquelydeterminesthesolutionof
Eq.(S5). Denoteθˇas
(cid:26) +∞, s∗ =+1
θˇ = i (S6)
i −∞, s∗ =−1.
i
Thenu(τ,x;θˇ)=f∗,foranyτ ≥0andx∈Rn,andwehave
u(τ,θˇ)=E [u(τ,x;θ˘)]=f∗. (S7)
p(x)
Noticedthat
√
u(τ,θ)=E [u(τ,x;θ)]=E [E [f(sgn(θ−(x+ 2τz)))]]≤E [E [f∗]]=f∗, θ ∈R¯n. (S8)
p(x) p(x) p(z) p(x) p(z)
wheretheequalityistrueifandonlyifu(τ,x;θ)=f∗istrueforx∈Rn. Therefore,ifθˆistheoneofthemaximasof
u(τ,θ),wehaveu(τ,θˆ)=f∗and
u(τ,x;θˆ)=f∗ =u(τ,x;θ∗), x∈Rn. (S9)
Duetothebackwarduniquenessoftheheatdiffusionequation,wehave
u(0,x;θˆ)=u(0,x;θ∗), x∈Rn, (S10)
thatis
h(θˆ)=h(θ∗)=f∗. (S11)
As a result, θˆis the one of maximas of h(θ). Conversely, using Eq. (S7), it is obviously to see that if θˆis one of
maximasofh(θ),itisalsooneofmaximasofu(τ,θˆ).
1ProofoftheThm.2.
Proof. Definethesquarelossofθas
e(θ)=(h(θ)−h(θ∗))2. (S12)
Accordingtothedefinitionofh(θ),wehave
e(θ)=E [f(sgn(θ−x))−f(sgn(θ∗−x))]2 ≤E [(f(sgn(θ−x))−f(sgn(θ∗−x)))2]. (S13)
p(x) p(x)
Definetheerrorfunction
r(τ,x;θ)=u(τ,x;θ)−u(τ,x;θ∗). (S14)
Thentheerrorfunctionsatisfiesthefollowingheatequation
(cid:26)
∂ r(τ,x;θ)=▽ r(τ,x;θ)
τ x . (S15)
r(0,x;θ)=f(sgn(θ−x))−f(sgn(θ∗−x)).
Definetheenergyfunctionoftheerrorfunctionr(τ,x;θ)as
(cid:90)
E(τ;θ)= r2(τ,x;θ)p(x)dx. (S16)
Rn
Thenapplyingtheheatequationandtheintegrationbyparts,wehave
d (cid:90)
E(τ;θ)=−2 ∥▽r(τ,x;θ)∥2p(x)dx. (S17)
dτ
Rn
Hencewehavefor0<τ <τ
1 2
(cid:90) τ2(cid:90)
E(τ ;θ)=E(τ ;θ)+2 ∥▽r(τ,x;θ)∥2p(x)dxdτ. (S18)
1 2
τ1 Rn
UsetheHarnack’sinequality[22],wehave
n
∥▽r(τ,x;θ)∥2 ≤r(τ,x;θ)∂ r(τ,x;θ)+ r2(τ,x;θ), (S19)
τ 2τ
hence,wehave
n(cid:90) τ2 E(τ;θ)
E(τ ;θ)≤E(τ ;θ)+ dτ. (S20)
1 2 2 τ
τ1
UsingtheMinkowskiinequalityonthemeasurep(x),wehave
(cid:90) (cid:90)
e1/2(θ)≤(cid:0)
(f(sgn(θ−x))−u(τ
;x;θ))2p(x)dx(cid:1)1/2 +(cid:0)
(u(τ ;x;θ)−u(τ
;x;θ∗))2p(x)dx(cid:1)1/2
+
1 1 1
Rn Rn
(cid:90)
(cid:0) (f(sgn(θ∗−x))−u(τ ;x;θ∗))2dx(cid:1)1/2
1
Rn
(cid:90) (cid:90)
=(cid:0) (f(sgn(θ−x))−u(τ ;x;θ))2p(x)dx(cid:1)1/2 +(cid:0) (f(sgn(θ∗−x))−u(τ ;x;θ∗))2dx(cid:1)1/2
1 1
Rn Rn
+E1/2(τ ;θ).
1
(S21)
Recallthecontinuityoftheheatequation:
(cid:90)
lim (u(τ,x;θ)−f(sgn(θ−x)))2p(x)dx=0. (S22)
τ→0 Rn
Therefore,givenϵ>0,thereexistsaτ >0,suchthat
1
(cid:90) (cid:90)
(cid:0) (f(sgn(θ−x))−u(τ ;x;θ))2p(x)dx(cid:1)1/2 +(cid:0) (f(sgn(θ∗−x))−u(τ ;x;θ∗))2p(x)dx(cid:1)1/2 <ϵ. (S23)
1 1
Rn Rn
Wethenhavetheerrorcontrolfore(θ):
e1/2(θ)≤E1/2(τ ;θ)+ϵ≤(cid:0) E(τ ;θ)+ n(cid:90) τ2 E(τ;θ) dτ(cid:1)1/2 +ϵ. (S24)
1 2 2 τ
τ1
Noticedthat
E(τ;θ)≤(f∗−f )E [u(τ,x;θ∗)−u(τ,x;θ)]=(f∗−f )(u(τ,θ∗)−u(τ,θ)), (S25)
min p(x) min
weprovethetheorem.
230000
25000
LQA
bSB
20000
dSB
aSB
15000
CIM
SIM-CIM
10000
HeO
5000 Best known
0 0.25 0.5 0.75 1
FigureS1: VerifyingthecooperativeoptimizationmechanismofHeO.Thebestcutvalueover10runsforeach
algorithm on the K-2000 problem [14] when the control parameters are randomly perturbed by different random
perturbationlevelδ. Thereddashlineisthebestcutvalueeverfind.
2 Cooperativeoptimization
Our study suggests that HeO exhibits a distinct cooperative optimization mechanism, setting it apart from current
methodologies. Specifically,HeObenefitsfromthefactthatvariousτ-diffusionproblems(u(τ,θ))sharethesame
optimaastheoriginalproblem(h(θ)). Thischaracteristicallowsthesolvertotransitionbetweendifferentτ values
duringtheproblem-solvingprocess,eliminatingthenecessityforamonotonicτ schedule. Incontrast,traditional
t
methodssuchasthosebasedonquantumadiabatictheoryorbifurcationtheoryrequirealinearincreaseofacontrol
parametera from0to1. Thisparameterisanalogoustoτ intheHeOframework.
t t
Toempiricallyverifytheaboveclaim,weintroducearandomperturbationtotheτ scheduleinAlg.1,renderingit
non-monotonic: τ˜ =c2τ ,wherec isuniformlydistributedon[1−δ,1+δ]withδcontrollingtheamplitudeofthe
t t t t
perturbation. Forothermethodsbasedonquantumadiabatictheoryorbifurcationtheory,wecorrespondinglyintroduce
theperturbationasa˜ =clamp(c a ,0,1). Ifanalgorithmcontainscooperativeoptimizationmechanism,itstillworks
t t t
wellevenwhenthecontrolparameterisnotmonotonic,asoptimizingthetransformedproblemsunderdifferentcontrol
parameterscooperativelycontributestooptimizingtheoriginalproblem. AsshowninS1,theperformanceofother
methodsarealldramaticallydeteriorated. Incontrast,HeOshowsnosubstantialdeclineinperformance,corroborating
thatHeOemploysacooperativeoptimizationmechanism.
3 Implementationdetails
Gradientdescentwithmomentum. WeprovideHeOwithmomentuminAlg.3.
Algorithm3Heatdiffusionoptimization(HeO)withmomentum
Input: targetfunctionf(·),stepsizeγ,momentumκ,τ schedule{τ },iterationnumberT
t
initializeelementsofθ as0.5
0
fort=0toT −1do
samplex ∼Unif[0,1]n
t
w
t
←▽ θtf(erf(θ√t− 2τx tt))
g ←κg +γw
t t−1 (cid:0) t (cid:1)
θ ←Proj θ +g
t+1 I t t
endfor
s ←sgn(θ −0.5)
T T
Output: binaryconfigurations
T
Toyexample. Wesetthemomentumκ=0.9999,learningrateγ =2anditerationnumberT =5000,andM =1for
HeO(Alg.3). ForMCGE,wesetT = 50000,γ =1e-6,momentumκ = 0,andM = 10. Empirically,wefindthat
non-zeromomentumresultsinworseresultsofMCGE.
Max-cutproblem. Forsolvingthemax-cutproblemsfromtheBiqMacLibrary[35],wesetthestepsT = 5000
√
forallthealgorithms. ForHeO,wesetγ =2,M =1,and 2τ linearlydecreasesfrom1to0forHeO,andweset
t
3
eulav
tucmomentumaszero. ForLQAandSIM-CIM,weusethesettingin[10]. ForbSH,dSH,aSB,andCIM,weapplythe
settingsin[55]. Foreachtest,weestimatethemeanandstdfrom10runs.
3-SATproblem. ForBoolean3-satisfiability(3-SAT)problem,wesetthemomentumκ=0.9999,T =5000,γ =2,
√
M =1,and 2τ linearlydecreasesfrom1to0forHeO.Weconsiderthe3-SATproblemswithvariousnumberof
t
variablesfromtheSATLIB[36]. Foreachnumberofvariablesinthedataset,weconsiderthefirst100instance. We
applythesameconfigurationofthatin[16]forboth3-ordersolverand2-orderoscillationIsingmachinesolver. The
energygapofthe2-ordersolverissetas1. Foreachtest,weestimatethemeanandstdfrom100runs.
Ternary-valueneuralnetworklearning. WedesignthetrainingalgorithmforbasedonHeOinAlg.4. Theinput
vofthedatasetDisgeneratedfromtheuniformdistributionon{−1,0,1}n. ForHeO.wesetT =10000,γ =0.5,
√
κ=0.999,M =1,and 2τ linearlydecreasingfrom1to0. Foreachtest,weestimatethemeanandstdfrom10runs.
t
Algorithm4HeOfortrainingternary-valueneuralnetwork.
Input: datasetD,stepsizeγ,momentumκ,τ schedule{τ },iterationnumberT
t
initializeelementsofθ as1,initializeelementsofβ˜ as0.
0 0
fort=0toT −1do
samplex ∼Unif[0,1]n
t
W
t
←W(erf(θ√t− 2τx tt))(Eq.(26))
MSE← 1 (cid:80) ∥Γ(v;W )−y∥2
|D| (v,y)∈D t
w ←▽ MSE
t θt
g ←κg +γw
t t−1 (cid:0) t (cid:1)
θ ←Proj θ −g
t+1 I t t
endfor
s =sgn(θ −0.5)
T T
Output: W =W(s )
T T
Variableselectionproblem. WeconstructanalgorithmforvariableselectionproblembasedonHeOasshownin
Alg.5,wherethefunctionf(s,β)isdefinedinEq.(28).
Algorithm5HeOforlinearregressionvariableselection
Input: datasetD,stepsizeγ,momentumκ,τ schedule{τ },iterationnumberT
t
initializeelementsofθ as1,initializeelementsofβ˜ as0.
0 0
fort=0toT −1do
samplex ∼Unif[0,1]n
t
w tθ ←▽ θtf(erf(θ√t 2− τx t),β)(Eq.(28))
gθ ←κgθ +γwθ
t t−1 t
w tβ ←▽
β˜
tf(erf(θ√t 2− τx t),β)(Eq.(28))
gθ ←κgθ +fγwβ
t t−1 T t
θ ←Proj
(cid:0)
θ
−γgθ(cid:1)
t+1 I t t
β˜ ←β˜ −gβ
t+1 t t
endfor
s ←sgn(θ −0.5)
T T
Output: s
T
Werandomlygenerate400-dimensionaldatasetswith1000trainingsamples.Theelementoftheground-truthcoefficient
β∗isuniformlydistributedon[−2,−1]∪[1,2],andeachelementhaves1−qprobabilityofbeingsetaszeroandthus
shouldbeignoredfortheprediction. Weapplyafive-foldcross-validationforallofmethods. ForourHeO,weset
T =2000andγ =1,κ=0.999,andM =1. Wegenerateanensembleofindicatorssofsize100. Foreachsinthe
ensemble,wefitalinearmodelbyimplementinganOLSonthenon-zerovariablesindicatedbysandcalculatethe
averageMSElossofthelinearmodelonthecross-validationsets. WethenselectthelinearmodelwithlowestMSEon
thevalidatesetsastheoutputlinearmodel. ForLassoandL regression,wefollowtheimplementationin[40]with
0.5
10iterations. theregularizationparameterisselectedbycross-validationfrom{0.05,0.1,0.2,0.5,1,2,5}. Foreach
test,weestimatethemeanandstdfrom10runs.
4Minimumvertexcoverproblem. Forconstrainedbinaryoptimization
min f(s), (S26)
s∈{1,1}n
g (s)≤0, ,k =1,...,K, (S27)
k
weputtheconstrainsasthepenaltyfunctionwithcoefficientλintothetargetfunction
K
(cid:88)
f (s)=f(s)−λ g (s), (S28)
λ k
k=1
proposethecorrespondingalgorithmbasedonHeO,asshowninAlg.6.
Algorithm6HeOforconstrainedbinaryoptimization
Input: targetfunctionwithpenaltyf ,stepsizeγ,τ schedule{τ },penaltycoefficientsschedule{λ },iteration
λ t t
numberT
initializeelementsofθ as0.5
0
fort=0toT −1do
samplex fromUnif[0,1]n
t
w
t
←▽ θtf λt(θ√t− 2x τt)
g ←κg +γw
t t−1 (cid:0) t (cid:1)
θ ←Proj θ −γg
t+1 I t t
endfor
s ←sgn(θ −0.5)
T T
Output: s
T
Algorithm7RefinementoftheresultofMVC
Input: theresultofHeOs
T
fori=1tondo
sets as0ifs isstillavertexcover
T,i T
endfor
Output: s
T
TableS2: TheattributesoftherealworldgraphsandtheparametersettingsofHeO.
graphname |V| |E| T γ λ σ
t t
tech-RL-caida 190914 607610 200 2.5 linearlyfrom0to2.5
soc-youtube 495957 1936748 200 2.5 linearlyfrom0to2.5
inf-roadNet-PA 1087562 1541514 200 2.5 linearlyfrom0to7.5
inf-roadNet-CA 1957027 2760388 200 5 linearlyfrom0to7.5 linearlyfrom1to0
socfb-B-anon 2937612 20959854 50 2.5 linearlyfrom0to5
socfb-A-anon 3097165 23667394 50 2.5 linearlyfrom0to5
socfb-uci-uni 58790782 92208195 50 2.5 linearlyfrom0to5
WeimplementtheHeOonasingleNVIDIARTX3090GPUforalltheminimumvertexcover(MVC)experiments.
Letsbetheconfigurationtobeoptimized,inwhichs is1ifweselecti-thvertexintoV ,otherwisewedonotselect
i c
i-vertexintoV c. ThetargetfunctiontobeminimizeisthesizeofV c: f(s)=(cid:80)n
i=1
si 2+1,andtheconstrainsare
s +1 s +1
g (s)=(1− i )(1− j )=0, ∀i,j,e ∈E, (S29)
ij 2 2 ij
where e represent the edge connecting the i and j-th vertices. We construct the target function f (s) = f(s)+
ij λ
(cid:80)
λ g (s). Thetermwiththepositivefactorλpenalizesvectorswhenthereareuncoverededges. Afterthe
eij∈E ij
HeOoutputstheresults ,weempiricallyfindthatitssubsetmayalsoformavertexcoverforthegraphG,sowe
T
implementthefollowingrefinementontheresults ,asshowninAlg.7. Wereportthevertexnumber,edgenumber
T
andsettingsofHeOinTab.S2. ForFastVC,wefollowthesettingsin[44]anduseitscodebase,andsetthecut-offtime
asthesameasthetimecostofHeO.Foreachtest,weestimatethemeanandstdfrom10runs.
5