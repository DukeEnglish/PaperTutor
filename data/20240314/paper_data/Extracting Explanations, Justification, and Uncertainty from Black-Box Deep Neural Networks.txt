Extracting Explanations, Justification, and Uncertainty
from Black-Box Deep Neural Networks
PaulArdisa and ArjunaFlennerb
a GEAerospaceResearch,1ResearchCircle,Niskayuna,NY12309,USA
b GEAerospace,3290PattersonAvenueSE,GrandRapids,MI49512,USA
ABSTRACT
Deep Neural Networks (DNNs) do not inherently compute or exhibit empirically-justified task confidence. In mission
critical applications, it is important to both understand associated DNN reasoning and its supporting evidence. In this
paper,weproposeanovelBayesianapproachtoextractexplanations,justifications,anduncertaintyestimatesfromDNNs.
Ourapproachisefficientbothintermsofmemoryandcomputation,andcanbeappliedtoanyblackboxDNNwithoutany
retraining, including applications to anomaly detection and out-of-distribution detection tasks. We validate our approach
ontheCIFAR-10dataset,andshowthatitcansignificantlyimprovetheinterpretabilityandreliabilityofDNNs.
INTRODUCTION
Deepneuralnetworks(DNNs)arepowerfulmachinelearningmodelsthatcanlearncomplexpatternsfromdata.However,
theircomplexitycanmakeitdifficulttounderstandwhyaDNNmakesaparticularpredictionpurelyfromtheirmathemat-
icalconstruction.WebuilduponthepriorExplainableAI(XAI)workfromViranietal.1 (seeFigure1)whicheffectively
explainsifthereispropertrainingdatatosupportthedecisions.However,theirmemoryandcomputationalfootprintpre-
vents the approach from being effective for very large data sets and on edge devices. We exploit the concept of Sparse
Gaussian processes2–5 to overcome these two computational challenges while maintaining the their method’s accuracy
andexplainability.OurapproachisacomputationallyefficientXAImethodthatextractsexample-basedjustificationsand
uncertaintyestimatesthatcanbeappliedtoanypre-trainedDNN.
Explainableartificialintelligenceaddressestheblack-boxprobleminmanyways:itdecomposesthemodelconstruc-
tion for more intuitive human understanding; it supplements models with tools that explain their decision-making; or a
structuredcombinationoftheprevioustwo.ByprovidinginsightsintohowAIsystemsmakedecisions,XAIcanhelphu-
manstoidentifyandcorrectpotentialerrorsbeforetheycauseharm.Assuch,XAIisacriticaltoolforensuringthesafety
and human trust in mission-critical tasks such as jet engine maintenance and airport safety. In the context of jet engine
maintenance,XAIcanbeusedtoprovidedetailedreasoningabouttheevidencesuggestingpotentialproblemsbeforethey
can cause failures, recommend corrective actions to prevent potential issues, and monitor the performance of engines to
ensurethattheycontinuetooperatesafely.Inthecontextofairportsafety,XAIcanbeusedtodetectandexplainpotential
hazards on the airport grounds, expedite human understanding of the evidence behind security threats, and monitor the
movementofaircrafttoensurethattheyfollowsafetakeoffandlandingdirections.
Whilethereareanumberofcompetingstandardsandmeta-studiesofXAI,6primaryseminalpapersexploreXAIusing
one of three main approaches. The first approach is to use gradient-based methods to identify the input features that are
mostimportantforaparticularprediction.7Thiscanbedonebycalculatingthegradientofthelossfunctionwithrespectto
theinputfeatures-thefeatureswiththelargestgradientsaretheonesthathavethemostinfluenceontheprediction.The
secondapproachtoexplainabilityistotraintheDNNtooutputtextthatexplainsitsreasoning.Thiscanbedonebyusing
a technique called attention, which allows the DNN to focus on specific input features when making a prediction.8 The
DNNcanthenbetrainedtooutputtextthatdescribesthefeaturesthatitfocusedon.Thethirdapproachtoexplainability
istotraintheDNNusingmetriclearning.9 ThisinvolvestrainingtheDNNtolearnadistancemetricthatcanbeusedto
measurethesimilaritybetweeninputfeatures.Themetriccanthenbeusedtoexplicitlyidentifytrainingsetsamplesthat
aresimilartoanygiventestsample.TheDNNcanalsothenbeusedtomakepredictionsforthetestingsamplebasedsolely
onthepredictionsthatitmadeforthesimilartrainingsamples.
Eachoftheseapproacheshasitsownadvantagesanddisadvantages.Gradient-basedmethodsarerelativelysimpleto
implement, but they can be difficult to interpret in their raw form. Text-based explanations can be more informative, but
theycanbemoredifficulttogeneratewithoutaddingacomplexmodelofrelevanthumanlanguage.Metriclearningcan
4202
raM
31
]GL.sc[
1v25680.3042:viXraInput
Epistemic classifier
Layer 1 ?
Example-based
justification
Class 1 training sample
Layer k ?
Class 2 training sample
Test sample
Output Support neighborhood
Figure1:Anillustrationoftheexample-basedXAIapproachofViranietal.1Theirapproachusesthetransformationspaces
from intermittent layers of a pre-trained DNN to evaluate the model’s justification and builds a support neighborhood
aroundeachtestsample’stransformedpoint.
beusedtogeneratebothsimpleandinformativeexplanations,butitcanbemorecomputationallyexpensiveasitrequires
retrainingtheneuralnetwork.
Weaddressthelimitationsofexistingmethodsbypresentinganapproachtoextractexample-basedjustificationsand
uncertaintyestimatesfrompre-trainedDNNs.Ourexample-basedjustificationscanbeusedtodeterminethetrainingset
samplesthatwerethemostrelevanttoanygiventestsample.Inscenarioswherethereareaninsufficientnumberofrelevant
training set samples, we can conclude that the neural network is extrapolating and has a high potential of misclassifica-
tion. In addition, our uncertainty estimates provide explicit information about the model’s prediction confidence based
upon local exemplar density and coherence, and this uncertainty can be thresholded to limit operation to avoid potential
misclassifications.Ourcontributionsareasfollows:
• WepresentanapproachthatusessparseGaussianprocessestotakethelatentembeddingsfromapre-trainedDNN
and predict the model output. Such an approach will allow us to estimate the prediction uncertainty while also
leveragingtheDNN’srepresentationpower.
• Weobtainexample-basedjustificationsforourmodelpredictionsbyselectingthetrainingsetsamplesthatarehighly
correlatedwiththetestsamples.WeusetheSGP’skernelfunctionstodeterminehowthesamplesarecorrelated.
Advantagesofourapproach
• BayesianuncertaintyquantificationusingsparseGaussianprocess(SGPs3,10)
– SGPshaveO(nm2+m3)computationcost,4 wherenisthenumberoftrainingsamplesandmisthenumber
ofSGPinducingpoints
– SGPscanbetrainedonlargedatasetsusingstochasticgradientdescent11
– SGPssupportnon-Gaussianlikelihoods(e.g.,softmaxlikelihood)
– SGPscanbedefinedonRiemannianmanifoldsandleveragegeodesicstodeterminecorrelations12
• Predictionjustificationfromexample-basedexplanations
– Reduceneighborhoodoperators’computationcostbyleveragingSGPinducingpointsBACKGROUND:GAUSSIANPROCESSES
Gaussianprocesses(GP),13canbeusedaspriorsinanon-parametricBayesianapproachforregressionandclassification
problems.Letx ∈ Rd,thenaGaussianprocessisuniquelydefinedusingameanfunctionm : Rd → Randcovariance
functionk :Rd×Rd →R.Thecovariancefunctionkmustbeapositive-definitekernelfunctionwhichmaydependona
setofparametersΘ.Giventhesetwofunctions,astochasticprocessf ∈Rindexedbyx∈Rdsuchthat
m(x) = E[f(x)],
k(x ,x ) = E[(f(x )−m(x ))(f(x )−m(x ))]
1 2 1 1 2 2
isaGP.Iff isaGaussianprocesswithmeanm(·)andcovariancekernelk(·,·)write
f(x) ∼ GP(f|m(x),k(x,x)).
Forregressionproblems,theGPisoftenusedtodefineagenerativemodeloftheform:
y = f(x )+ϵ ,
i i i
ϵ ∼ N(ϵ|0,σ2 ),
i noise
f(x ) ∼ GP(f|m(x,k(x,x)).
i
An essential concept in the following discussion is the number of variables x used in the calculations. The follow-
ing notation is used to highlight the set sizes with the goal of illuminating the computational complexity. Let X =
n
[x ,x ,...,x ]⊤ be a set of locations. Define 1 ≤ i,j ≤ n and let K be a matrix with elements k(x ,x ). Note that
1 2 n nn i j
thesubscriptnnreferstothenumberofrowsandnumberofcolumnsandnotaroworcolumnindex.Definetherandom
vectorf =[f(x ),f(x ),...,f(x )]⊤.Giventhelocationx,definek =[k(x,x ),...,k(x,x )]⊤.
n 1 2 n xn 1 n
Bayesian methods assume a prior data model and calculate a posterior distribution once data is observed. Consider
an observed data set D = {(x ,y ) | 1 ≤ i ≤ n, x ∈ Rn, y ∈ R}. Define the vector y = [y ,...,y ]⊤ and
n i i i i 1 n
assumethatthemeanfunctionm(·)=0.Withthismeanfunction,recallthatthedefinitionofaGPdesignatesp(f|X )=
n
N(f|0,K ).SincealltherandomvariablesintheGPregressionmodelareGaussiandistributed,theposteriordistribution
nn
isalsoaGPwithmeanfunctionm (·)andcovariancekernelk (·,·)givenby
y y
m (x)=k⊤ (K +σ2 I)−1y,
y xn nn noise
(1)
k (x,x′)=k(x,x′)−k⊤ (K +σ2 I)−1k .
y xn nn noise x′n
Theparameters{Θ,σ }canbeestimatedbymaximizingthelogmarginallikelihood
noise
logp(y)=log[N(y|0,K +σ2 I)].
nn noise
Inspectionoftheposteriorparametersillustratesacomputationalbottleneck.Specifically,computationisviaaninver-
sionofamatrixofsizen×n,wherenisthecardinalityofthedataset,whichisaO(n3)operation.Forlargedatasets,
thisiscomputationallyinfeasible.ThisobservationhasmotivatedthedevelopmentofSparseGaussianprocesses.
Sparse Gaussian processes (SGPs)2–5 are designed to overcome the cubic nature of calculating GP posteriors by
approximatingtheGPusinganotherGaussianprocesssupportedwithm ≪ ninducingpoints.Thisreducestheposterior
calculationtoamatrixinversionusingam×mmatrix;SGPsreducethecomputationstoO(m3).
This paper uses the sparse variational GP (SVGP).4,14,15 This approach implements a variational Bayesian method
thatapproximatesthetrueposteriordistributionwithavariationaldistributionq definedusinginducingpointsX ,mean
m
parameterµ,andcovarianceparameterAasvariationalparameters.
Oncetheseparametersarecalculated,theycanbeusedtodefineanapproximateGPwithmeanparametermq(·)and
y
covaraincefunctionkq(·,·)givenby
y
mq(x)=k⊤ K−1 µ,
y xm mm
kq(x,x′)=k(x,x′)−k⊤ K−1 k (2)
y xm mm mx′
+k⊤ K−1 AK−1 k .
xm mm mm mx′GiventheinducingpointsX ,theparametersµandAcanbecalcualtedas
m
µ = σ−2 K ΣK y,
noise mm mn
A = K ΣK ,
mm mm
Σ = (K +σ−2 K K )−1.
mm noise mn nm
TherearemanytechniquestodeterminetheinducingpointsX ;SeeTitsiasetal.4formoredetails.
m
WepointoutthatthesubscriptsofK highlightsiftheinducingpointsorthedatacorrespondtothefirstandsecond
mn
component of the covariance function. For example, K varies the inducing point across the rows and varies the data
mn
pointsacrossthecolumns.
Baueretal.15 providesanin-depthanalysisoftheSVGP’slowerboundwhichilluminatesroleoftheinducingpoints.
Thevariationalapproximationmaximizestheevidencelowerbound(ELBO)F withregardtothevariationaldistribution:
n 1
F = log(2π)+ y⊤(Q +σ2 I)−1y
2 2 nn noise
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
constant datafit
(3)
1 1
+ log|Q +σ2 I|− Tr(K −Q ),
2 nn noise 2σ2 nn nn
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) noise (cid:123)(cid:122) (cid:125)
complexity trace
whereQ = K K−1 K istheNystro¨mapproximationtoK .Theseparatetermshavethefollowinginterpreta-
nn nm mm mn nn
tions. The data fit term penalizes inaccuracies in outputs. The complexity term penalizes local density in inducing point
selection. When the trace term approaches zero, the m inducing points become a sufficient statistic for the n training
samples,meaningthatSGPwithinducingpointsX hasthesamedistributionontheX samplespoints.
m n
METHOD
OurapproachreplacesthesupportneighborhoodsofViranietal.1 withaSGP;thus,reducingthecomputationalcostand
memoryfootprintfromndatapointstominducingpoints.Thisgreatlyreducestherequiredcomputationalresourceswhen
m≪n,withmtunablebaseduponvalidationperformance.
AreviewoftheconceptsinViranietal.1 motivatesourSGPapproach.Theyfirstcalculatetheactivationsofeachof
thelayersforthetrainingdatapointsX .Givenanewdatapoint,ifthereareenoughtrainingpointactivationswithinan
n
ϵ-neighborhood around the new data point, then there is local evidence to support the final classification decision. If the
number of points within the ball is defined as a decision supporting set, then there is support-set-sufficiency to trust the
output.Atwrost,though,thisrequiresacomparisonofallnewoperatingpointstoalltrainingpoints.
Insteadofusingϵ-neighborhoodstocalculatesufficientevidence,ourapproachincludesaprobabilitybasedmeasure
for sufficient evidence. Specifically, if the training set activation density is estimated using a Gaussian process, then the
Gaussian process’s training index set and covariance provides an estimate of the local supporting evidence. However,
the computations required for a Gaussian process requires a large memory and computational load. We overcome this
computationalburdenusingSGPs.
The SGP’s inducing points X are a set of locations that can be used to effectively approximates the Gaussian dis-
m
tributionofthetrainingdatasetD.TheSGPcalculatesthecovarianceusingonlytheseinducingpointsandatestsample
exploitsthiscomputationalefficientcovariancetopredictthesupport-set-sufficiency.
Thecovariancesdeterminesthelimitsofthesupportneighborhood;itgivesexample-basedjustificationforthemodel’s
predictionsusingtheinducingpoints.ComparedwithGPs,SGPsreducethecomputationcostfromO(n3)toO(nm2 +
m3);Thus,oursupportneighborhooddeterminationissignificantlyfasterthanthepriorapproachforpracticaluse.Figure2
illustratesourapproach.
Todefinethespecificcalculations,considerr observationsX ,definea = min(K ),b = max(K ),andX =
r mm mm
concat[X ,X ]. We estimate the distance to each inducing point, the covariance and posterior likelihood of generation
r mSGP
Input
Embedding space Covariance matrix Example-based
justification
Layer 1
?
Prediction
distribution
Class 1 training sample
Layer k
Class 2 training sample
SGP inducing point
Output Test sample
Figure2:Anillustrationoftheproposedexample-basedXAIapproach.
correspondingtotheembeddingaroundthatinducingpoint,andacovariance-adjusteddistance:
D =pairwise-distance(X ,X ),
rm r m
K=kq(X,X),
y
K−1[:r,r :]−a
K = , (4)
rm b−a
P =clip(K ,0,1)
rm rm
Dcov =D +λP
rm rm rm
Withϵasathresholdparameter,foreachnewsampleinX countingthenumberofinducingpointssuchthatD <ϵor
r rm
Dcov <ϵprovideameanstodefineϵ-neighborhoodsandsupportsets.Theseneighborhoodsgivelocalevidencetosupport
rm
thefinalclassificationdecision.AsinViranietal.,thisproceduredefinesepistemicoperatingpointsbasedlabel-coherent
support from the estimated local evidence. At a threshold level, epistemic uncertainty becomes apparent in the presence
of label-conflicting inclusions. Moreover, assuming a sufficient number of samples, for inducing points with conflicting
labelsthefunctionP directlyestimatelabelpredictionuncertainty.Weshowcasetheseusesbelow.
rm
EXPERIMENTS
WecomparetheefficacyofourapproachtoViranietal.’sperformance.UsingtheCIFAR-10dataset,16weappliedViraniet
al.’smethodthatusesallndatapointsagainstourmethodthatusesSGPsandm<<ninducingpointsusingthreemetrics:
finallabelingaccuracy;newlabelconfidence;andcomputationtime.Thenewlabelconfidencerequiresexplanation.
Newlabelconfidencemeasuresthesupportingevidenceofanewdatapoint’slabelandismeasuredusingtunable
ϵ-neighborhoods.ForViranietal.,theϵ-neighborhoodwasanϵ-ballaroundthenewpoints.Forourcase,theϵ-neighborhood
isdefinedbycountingthenumberofinducingpointswithDcov orD inEquation4.Forbothmethods,iftherearesignif-
rm rm
icant(thresholded)numberoflabel-coherentpointswithintherespectivelydefinedϵ-neighborhoods,thenthereissufficient
trainingsamplestosupportthedecisionandthesamplepointpassesthe”IKnow”(IK)requirement.
The metrics are shown in three figures. The accuracy of sample points that pass the IK requirement is in Figure 3,
the fraction of points that pass the IK requirement is in figure 4, and execution time is shown in Figure 5. In each case,
performancecomparedwiththeViranietal.”Baseline”isshownviaablackdashedline.The”SGP”methodusesD
rm
and”Cov-SGP”usesDcov,and,insteadofusingvariationalmethodstofindtheinducingpoints,”RandomSubset”usesa
rm
randomlyselectedsetofinducingpoints.TheadditionofarandomsubsethighlightstheutilityofincludingtheinducingFigure3:LabelaccuracyforCIFAR-10
Figure4:EpistemicoperationforCIFAR-10Figure5:InferencetimeforCIFAR-10
points as variational parameters in the Sparse Gaussian Process versus less nuanced embeddings. Performance variance
over10randomrestartsisshownasashadedregionaroundeachoperatingcurvetohelpclarifyconfidenceintheappli-
cationoftheapproachevenwithvariedinstantiations.Weobservedthattherewasnosignificantmeasurablebenefitifthe
numberofinducingpointswasincrasedpast2500.Foreaseofvisualcomparison,thisplateaueffectofepistemicinclusion
thresholdingisindicatedwithadashedredline.
As shown, our SGP or Cov-SGP solution provides assured label performance slightly improving upon the baseline
regardlessofthenumberofinducingpointsselected.UsingtheIKrequirementsasanepistemicselectioncriteriarestricts
theCIFAR-10datasettolabel79%ofallvalidationpointsvs.82%forViranietal.Ofmostimportance,however,isthe
reduction of time required for inference by two orders of magnitude; this goes hand-in-hand with the reduction of data
pointswhichneedtoberetainedfromtheoriginal50,000byatleastanorderofmagnitude.
CONCLUSIONS
Based upon our findings, we believe that there is strong potential in deploying epistemic uncertainty-aware systems for
safepracticeeveninextremelyconfinedsize,weight,andpowerenvironments.TheefficacyofSGPsforuseinembedding
trainingexperiencesuggestsaninitialpathtohigh-confidenceuse,whileweintendtofurtherexplorealternateembeddings
andotherimprovementsinfuturework.
REFERENCES
[1] Virani,N.,Iyer,N.,andYang,Z.,“Justification-basedreliabilityinmachinelearning,”in[ProceedingsoftheAAAI
ConferenceonArtificialIntelligence],34(04),6078–6085(2020).
[2] Snelson,E.andGhahramani,Z.,“SparseGaussianProcessesusingPseudo-inputs,”in[AdvancesinNeuralInforma-
tionProcessingSystems],Weiss,Y.,Scho¨lkopf,B.,andPlatt,J.,eds.,18,MITPress(2006).
[3] Bui,T.D.,Yan,J.,andTurner,R.E.,“AUnifyingFrameworkforGaussianProcessPseudo-PointApproximations
UsingPowerExpectationPropagation,”JournalofMachineLearningResearch18(104),1–72(2017).[4] Titsias,M.,“VariationalLearningofInducingVariablesinSparseGaussianProcesses,”in[ProceedingsoftheTwelth
International Conference on Artificial Intelligence and Statistics], van Dyk, D. and Welling, M., eds., 567–574,
PMLR,Florida,USA(2009).
[5] Hoang, T. N., Hoang, Q. M., and Low, B. K. H., “A Unifying Framework of Anytime Sparse Gaussian Process
Regression Models with Stochastic Variational Inference for Big Data,” in [Proceedings of the 32nd International
ConferenceonMachineLearning],Bach,F.andBlei,D.,eds.,37,569–578,PMLR,Lille,France(2015).
[6] Schwalbe,G.andFinzel,B.,“Acomprehensivetaxonomyforexplainableartificialintelligence:asystematicsurvey
ofsurveysonmethodsandconcepts,”DataMiningandKnowledgeDiscovery(2023).
[7] Ancona,M.,Ceolini,E.,O¨ztireli,C.,andGross,M.,[Gradient-BasedAttributionMethods],169–191,SpringerIn-
ternationalPublishing,Cham(2019).
[8] Neely, M., Schouten, S. F., Bleeker, M. J. R., and Lucic, A., “Order in the court: Explainable AI methods prone to
disagreement,”CoRRabs/2105.03287(2021).
[9] Suarez, J. L., Garcia, S., and Herrera, F., “Ordinal regression with explainable distance metric,” Machine Learn-
ing110,2729–2762(2021).
[10] Quinonero-Candela, J., Rasmussen, C. E., and Williams, C. K. I., “Approximation Methods for Gaussian Process
Regression,”in[Large-ScaleKernelMachines],203–223,MITPress(2007).
[11] vanderWilk,M.,Dutordoir,V.,John,S.,Artemev,A.,Adam,V.,andHensman,J.,“AFrameworkforInterdomain
andMultioutputGaussianProcesses,”ArXiv(2020).
[12] Borovitskiy, V., Terenin, A., Mostowsky, P., and Deisenroth (he/him), M., “Mate´rn Gaussian processes on rieman-
nianmanifolds,”in[AdvancesinNeuralInformationProcessingSystems],Larochelle,H.,Ranzato,M.,Hadsell,R.,
Balcan,M.,andLin,H.,eds.,33,12426–12437,CurranAssociates,Inc.(2020).
[13] Rasmussen,C.E.andWilliams,C.K.I.,[GaussianProcessesforMachineLearning],MITPress,Cambridge,USA
(2005).
[14] Burt, D., Rasmussen, C. E., and Van Der Wilk, M., “Rates of Convergence for Sparse Variational Gaussian Pro-
cess Regression,” in [Proceedings of the 36th International Conference on Machine Learning], Chaudhuri, K. and
Salakhutdinov,R.,eds.,97,862–871,PMLR(Jun2019).
[15] Bauer,M.,vanderWilk,M.,andRasmussen,C.E.,“UnderstandingProbabilisticSparseGaussianProcessApproxi-
mations,”in[AdvancesinNeuralInformationProcessingSystems],1533–1541(2016).
[16] Krizhevsky,A.,“Learningmultiplelayersoffeaturesfromtinyimages,”(2009).