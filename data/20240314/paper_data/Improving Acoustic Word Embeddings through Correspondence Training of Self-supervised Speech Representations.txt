Improving Acoustic Word Embeddings through Correspondence Training
of Self-supervised Speech Representations
AmitMeghanani,ThomasHain
SpeechandHearingResearchGroup
DepartmentofComputerScience,TheUniversityofSheffield,UnitedKingdom
{ameghanani1,t.hain}@sheffield.ac.uk
Abstract in the context of learning acoustic word embed-
dings(AWEs). AWEsarefixed-dimensionalvector
Acoustic word embeddings (AWEs) are vec-
representationsofspokenwordsthatfindapplica-
tor representations of spoken words. An ef-
tionsinvariousdownstreamtasks,suchasquery-
fective method for obtaining AWEs is the
by-examplesearch(Settleetal.,2017;Yuanetal.,
CorrespondenceAuto-Encoder(CAE).Inthe
2018;Huetal.,2021),keywordspotting(Barakat
past, the CAE method has been associated
with traditional MFCC features. Representa- et al., 2011), providing clues for human lexical
tions obtained from self-supervised learning processing(Matusevychetal.,2020),hatespeech
(SSL)-basedspeechmodelssuchasHuBERT, detection in low resource settings (Jacobs et al.,
Wav2vec2, etc., are outperforming MFCC in 2023),etc.
manydownstreamtasks. However,theyhave
Recently, the work (Sanabria et al., 2023) pro-
not been well studied in the context of learn-
posed extracting AWEs from SSL-based speech
ing AWEs. This work explores the effective-
representationsusingameanpoolingmechanism.
ness of CAE with SSL-based speech repre-
sentations to obtain improved AWEs. Addi- The authors suggest that SSL-based speech rep-
tionally,thecapabilitiesofSSL-basedspeech resentations,whicharecontextualized,canbeef-
models are explored in cross-lingual scenar- fectivelyconvertedintoAWEsusingastraightfor-
iosforobtainingAWEs. Experimentsarecon- wardpoolingmechanism. Ontheotherhand,Cor-
ductedonfivelanguages: Polish,Portuguese,
respondenceAuto-Encoder(CAE)basedtraining
Spanish,French,andEnglish. HuBERT-based
strategiesforAWEs(Kamper,2019)usingMFCC
CAEmodelachievesthebestresultsforword
(DavisandMermelstein,1980)featuresareshown
discrimination in all languages, despite Hu-
tobepromisingintheliterature. Correspondence
BERTbeingpre-trainedonEnglishonly. Also,
theHuBERT-basedCAEmodelworkswellin traininginvolvesanauto-encoderwhereaspoken
cross-lingualsettings. ItoutperformsMFCC- word serves as the input to the encoder, and the
based CAE models trained on the target lan- targetoutputofthedecoderisadifferentinstance
guageswhentrainedononesourcelanguage of the same spoken word. This approach helps
andtestedontargetlanguages.
to preserve acoustic-phonetic information while
filtering out unnecessary details such as speaker,
1 Introduction
acousticenvironment,andduration,etc. Bothen-
Self-supervisedlearning(SSL)-basedspeechrep- coder and decoder are typically recurrent neural
resentationsare becomingpopular inspeechpro- networks (RNNs). More details about the model
cessing and producing state-of-the-art results in willbepresentedinSec. 2andSec. 4.2. Correspon-
manydownstreamtaskssuchasautomaticspeech dencetraininghasalsobeenexploredinthework
recognition,speakerverification,keywordspotting, (Meghanani and Hain, 2024) to improve content
voice conversion, etc (Yang et al., 2021). These representationsofSSL-basedspeechmodels.
representationsareobtainedusingself-supervised The work (Lin et al., 2023) uses a Correspon-
learning on large amounts of unlabelled speech dence Transformer Encoder (CTE) for obtaining
data. Wav2vec2 (Baevski et al., 2020), HuBERT robust AWEs, trained from scratch and a large-
(Hsuetal.,2021),andWavLM(Chenetal.,2022) scale unlabelled speech corpus. In contrast, in
are a few examples of such SSL-based speech thiswork,pre-trainedSSLspeechmodelsarecou-
models. However, representations obtained from pled with a simple RNN based auto-encoder for
these models have not been extensively explored correspondence training to obtain robust AWEs.
4202
raM
31
]LC.sc[
1v83780.3042:viXraThisworkattemptstousethecorrespondencetrain-
Y
X 1
ingofauto-encodertoobtaintheAWEsbylever- 1 e Y 2
X
aging SSL-based speech representations instead Input: 2 NC D E Target output:
of MFCC features as input features to the CAE X = {X 1,X 2,...,X m } E C Xꞌ = {Xꞌ 1,Xꞌ 2,..,Xꞌ n }
X
model. Further,cross-lingualcapabilitiesarealso m-1 AWE Y n-1
examinedforSSL-basedAWEstrainedwithCAE X m Y n
method. The SSL models (HuBERT, Wav2vec2,
Figure1:CAE-RNNtrainingsetupforextractingAWEs
andWavLM)usedinthisworkarepre-trainedon (Kamper,2019).
Englishdata. However,ithasbeendemonstrated
thatthesemodelsworkwellasfeatureextractors
2 Methodology
for the all the languages considered in this study.
Theperformancesontheword-discriminationtask
Correspondence auto-encoder is trained with in-
forallthelanguages(Polish,Portuguese,Spanish,
put as a spoken word and target output as a dif-
andFrench)areasgoodasontheEnglishlanguage
ferent instance of the same spoken word. Typ-
(Sec. 5). Adetailedanalysisisalsoconductedto
ically, Recurrent Neural Network (RNN) based
assess the importance of contextual information
encoder and decoder are used, hence the model
inspokenwordsbycomparingfeatureextraction
isreferredtoasCAE-RNN.Therationalebehind
withandwithoutcontext. Forthiswork,weobtain
this training method is that CAE-RNN will pre-
spokenwordsforallfivelanguagesfromthesubset
serveonlytheacoustic-phoneticinformationand
ofMultilingualLibriSpeech(MLS)(Pratapetal.,
filterouttheotherunnecessaryinformationfactors
2020)dataset. Thederiveddatasetconsistsoffive
such as speaker, duration, acoustic environment,
languages (Polish, Portuguese, Spanish, French,
etc(Kamper,2019). Fig. 1showstheCAE-RNN
English)withstartandendtimestampsofspoken
modelsetup. TheinputtotheENCisasequenceof
words 1. We chose MLS dataset for our experi-
acousticfeaturevectors(X = X ,X ,...,X )of
1 2 m
mentsasmanypreviousworks(Matusevychetal.,
aspokenword. Thetargetoutputisthesequence
2020;Abdullahetal.,2021a,2022,2021b;Kamper
ofacousticfeaturevectorsofthedifferentinstance
etal.,2021)onAWEsrelyonGobalPhone(Schultz
of the same spoken word (X′ = X′,X′,...,X′).
1 2 n
etal.,2013)dataset,whichisnotfreelyavailable.
TheencoderproducestheAWE(e)ofthespoken
The main contributions of this work are as fol-
word X, which is then fed to the decoder to re-
lows:
construct X′. The output of the decoder is repre-
1. Utilizing corresponding training with SSL- sented as Y = Y ,Y ,...,Y . The mean squared
1 2 n
basedspeechrepresentationstoobtainhighly lossfunctionforasingletrainingpair(X,X′)can
discriminativeAWEs. bedescribedasfollowing:
2. Showing effectiveness of SSL models, pre- n
(cid:88)
trainedonlyonEnglish,asfeatureextractors L = ||X′ −Y ||2 (1)
k k
incross-lingualscenariosforobtaininghigh- k=1
qualityAWEs.
, where X′ = X′,X′,...,X′ is the target output
1 2 n
3. Quantitativelydemonstratingthatincorporat- andY = Y 1,Y 2,...,Y n istheoutputofthedecoder
ing the context of the spoken word in SSL- asshowninFig. 1.
basedspeechrepresentationsleadstothepro-
ductionofmorerobustAWEs. 3 DataPreparation
The rest of the paper is as follows: Sec. 2 de- The Multilingual LibriSpeech (MLS) dataset
scribesthecorrespondenceauto-encodermethod- (Pratap et al., 2020) is utilized to obtain spoken
ology to obtain AWEs; Sec. 3 describes the data words. Fivelanguages,namelyPolish,Portuguese,
preparation and data statistics; Sec. 4 describes Spanish, French, and English, are selected from
thedetailsoftheexperiments;Sec. 5describesthe MLS. For each language, approximately 25,000
results and analysis; Sec. 6 concludes the work utterancesareselectedforthetrainingset,500for
with possible future directions. Sec. 7 describes thedevelopmentset,and500forthetestset. These
thelimitationsofthework. selectedutterancesareforce-alignedtoobtainthe
1https://github.com/Trikaldarshi/SSL_AWE spokenwordboundariesusingtheMontrealForcedData Polish Portuguese Spanish French English
Statistics Train Dev Test Train Dev Test Train Dev Test Train Dev Test Train Dev Test
#SpokenWords 104448 4595 4563 117820 4964 4659 82258 3721 3601 84267 3004 3114 92352 3147 3192
#UniqueSpokenWords 9346 3818 3887 9785 3696 3539 7085 2678 2769 7221 2394 2433 7157 2448 2527
#Speaker 11 4 4 42 10 10 79 20 20 120 18 18 182 42 41
TotalDuration(hours) 18.5 0.8 0.8 21.9 0.93 0.87 14.7 0.67 0.65 14.9 0.55 0.56 16.6 0.56 0.57
Table1: Asummaryofthedatastatisticsforallfivelanguagesacrossthetrain,dev,andtestsplits
Alignertoolkit(McAuliffeetal.,2017). Onlyspo- 2 (allwith≈95Mparameters)areusedforfeature
kenwordswithadurationof0.5secondsorlonger extraction. Allthesemodelsarepre-trainedon960
areincludedinthederiveddataset, followingthe hoursofLibriSpeechdata(Panayotovetal.,2015).
standardpracticeintheliterature(Heetal.,2017). A“BASE"architecturetypicallyhasamulti-layer
Spokenwordswithafrequencygreaterthan50or CNN-basedfeatureencoderfollowedby12Trans-
lessthan5areexcludedfromthederiveddataset. formerlayers. Inthiswork,representationsfrom
Table 1 presents a summary of the statistics for eachmodelareextractedfromthefinal(i.e. 12th)
thefinalextracteddataset,encompassingallfive Transformer layer. For all the above mentioned
languages. The speakers across different sets are SSLmodels,768-dimensionalfeaturevectorsare
non-overlapping,whichisadesirablecharacteris- obtainedforeachspokenwordataframerateof20
ticforevaluatingAWEsastheyshouldberobust ms.
to speaker variations. Polish language had lim- SSL-basedspeechrepresentationsareextracted
itedavailabledataandconsequentlyhasthefewest in two different ways: the first one is extracted
numberofspeakers,whileEnglishhasthehighest using the context around the spoken word, and
number. ThedurationinTable1representsthetotal the other one is extracted without the context, as
timedurationofspokenwordsacrossthedifferent describedhere:
sets.
1. Withcontext: Inthiscase,firsttheSSL-based
4 ExperimentalSetup
speechrepresentationsoftheentirespokenut-
Experiments are conducted on all the five lan- terancearecomputedandthenthetimebound-
guageswithSSL-basedspeechrepresentationsas ariesofthespokenwordisusedtogettherep-
inputfeaturesextractedfromWav2vec2,HuBERT, resentationsofthesegmentbelongingtothe
andWavLM.Experimentsarealsoconductedwith spokenword. Thisensuresthattheextracted
MFCCasinputfeatures. First,thefeatureextrac- representationscapturethecontextaroundthe
tionmethodsforvariousSSL-basedspeechrepre- spoken word as the entire utterance is pro-
sentations and MFCCs are described. Then, the cessed by the SSL model. Let us assume U
configurationoftheCAE-RNNmodelisexplained, represents an utterance and X represents a
along with the mean pooling baseline (Sanabria spokenwordinstancepresentintheutterance
et al., 2023) and the AE-RNN method (without U withstartandendtimestampsdenotedast
1
correspondence training), for comparison. Next, andt . Iff representstheSSLmodel,thenthe
2
the word discrimination task is described, which SSL-basedspeechrepresentationfortheentire
isusedforevaluatingthequalityoftheextracted utterance is computed Z = f(U). Then the
AWEs. Finally, the training details of the CAE- speechrepresentationsforthespokenwordX
RNNandothermodelsareprovided. willbeZ .
t1:t2
4.1 FeatureExtraction
2. Withoutcontext: Inthiscase,nocontextis
4.1.1 SSL-basedSpeechRepresentations
consideredandSSL-basedspeechrepresenta-
SSL models are pre-trained on large amount of tions are extracted by inputting only speech
unlabelled speech data. The task defined for the segments belonging to the spoken words to
pre-training is known as the pretext task. Each theSSLmodels. Hence,inthiscase,theSSL-
modeldiffersbasedonhowthepretexttaskwasde- based speech representation for the spoken
fined,thedatausedforpre-training,andthemodel wordX willbeZ = f(U ).
t1:t2
architecture. In this work, the “BASE” architec-
turesofWav2vec2,HuBERT,andWavLMmodel 2https://github.com/pytorch/fairseq4.1.2 MFCCFeatures SpokenWord
Polish Portuguese Spanish French English
Pairs(inmillion)
Foreachspokenword,20-dimensionalMFCCfea- test 10.4M 10.8M 6.4M 4.8M 5.1M
test′ 4.7 2.6M 1.8M 1.3M 1.4M
turesareextractedwith30mswindowsizeand20
msshiftalongwithdeltaanddelta-deltafeatures, Table2: Thetotalnumberofspokenwordpairsgener-
whichleadsto60-dimensionalMFCCfeaturevec- atedforthetestandtest′sets.
tors.
4.2 ModelDetails spokenwordpairsforboththetestandtest′ setsis
describedinTable2foralllanguages.
A4-layerBidirectionalGRUwithahiddendimen-
sionof256isusedforboththeencoderanddecoder
4.4 TrainingDetails
intheCAE-RNNmodel. Dropoutrateissetto0.2.
Thefinalhiddenstateoftheencoder-GRUisfedto The total number of generated correspondence
afullyconnectedlayertoobtain128-dimensional training pairs (X,X′) for each language is as
AWE (e) as shown in Fig. 1. This embedding is follows: 9,55,106 for Polish, 11,63,468 for Por-
thenfedtothedecoderateachtimestepasinput tuguese,7,80,197forSpanish,7,95,613forFrench,
tothedecoder(Kamper,2019). Theoutputofthe and9,72,532forEnglish. Theremainingtraining
decoder is then fed to a fully connected layer to detailsareasfollowsforvariousinputs:
producethetargetoutput.
Aregularauto-encoderRNN(AE-RNN)model • SSL-based Speech Representations as In-
is also used as one of the baselines with similar put: CAE-RNN models are trained for 30
configurations. AE-RNNmodelisanauto-encoder epochs, usingalearningrateof0.0001with
modelwhereinputandtargetoutputisexactlythe Adam optimizer and a batch size of 512. In
samespokenword,i.e. input-outputtrainingpair eachrun,themodelwiththebestperformance
is(X,X). Ameanpoolingmodelisalsousedas on the development set in terms of word-
baseline (Sanabria et al., 2023), which does not discrimination is selected as the final model
require any training. This method computes the forevaluationonthetestset. AE-RNNmod-
meanoftheSSL-basedspeechrepresentationsto elsaretrainedfor50epochs,keepingallother
getthe768-dimensionalAWEofaspokenword. parameterssameasmentionedaboveforthe
CAE-RNNmodels.
4.3 WordDiscriminationTask
To evaluate the AWEs, the same-different word- • MFCC as Input: Both AE-RNN and CAE-
discrimination task is used (Kamper et al., 2015; RNNmodelswithMFCCasinputsaretrained
Carlinetal.,2011). First,allpossiblespokenword for100epochs,usingalearningrateof0.0001
pairsaregenerated. Forexample,iftherearetotal withAdamoptimizer. Thebatchsizeforthe
N spoken words, then the total generated spoken AE-RNNmodelwaschosenas64,whilefor
wordpairsforcomparisonwillbe(cid:0)N(cid:1)
=
N(N−1)
. theCAE-RNNmodelitwassetto256based
2 2
Afterthat,thecosinedistancebetweentheAWEs onpreliminaryexperimentsforbetterconver-
of these pairs are computed and compared with gence. Similarly to the previous case, the
a threshold to decide whether the spoken words modelwiththebestperformanceonthedevel-
aresameordifferent. Theaverageprecision(AP) opmentsetintermsofword-discriminationis
iscalculatedbyvaryingallthepossiblethreshold selectedasthefinalmodelforevaluationon
values,whichistheareaundertheprecision-recall thetestset.
curve. APisreportedforthesame-differentword
discrimination task. Word-discrimination task is
appliedonthetestset,whichhasunseenspeakers Model Polish Portuguese Spanish French English
AE-RNN 0.20 0.10 0.17 0.01 0.01
during training. Also, a subset of the test set is
CAE-RNN 0.56 0.41 0.57 0.43 0.24
created for all five languages in such a way that
none of the words in the subset are encountered Table3: APonthetestsetforword-discriminationtask
duringtraining. Thisparticularsubsetisreferred usingMFCCsasinputfeaturesforAE-RNNandCAE-
to as test′. The word-discrimination task is also RNNmodelsindifferentlanguages.
conductedontest′. ThetotalnumberofgeneratedModel Polish Portuguese Spanish French English based AWEs are 768-dimensional. Based on the
AE-RNN 0.21 0.10 0.24 0.03 0.01
results presented in Table 5, it is evident that the
CAE-RNN 0.54 0.47 0.63 0.57 0.33
HuBERTfeaturesconsistentlyachievethebestper-
Table 4: AP on the test′ set for word-discrimination formanceacrossallconfigurationsandlanguages.
taskusingMFCCsasinputfeaturesforAE-RNNand Specifically, when using the CAE-RNN method
CAE-RNNmodelsindifferentlanguages. forAWEextractionandSSL-basedspeechrepre-
sentations extracted ‘with context’, the HuBERT
achieves the highest AP on the test set: 0.90 for
5 ResultsandAnalysis
Polish,0.88forPortuguese,0.95forSpanish,0.74
Table3showsthebaselineresultswithMFCCfea- forFrench,and0.86forEnglish. Theperformance
tures as input for the AE-RNN and CAE-RNN order can be sorted as HuBERT > Wav2vec2 >
models. Thisdemonstratestheeffectivenessofthe WavLM > MFCCs when using the CAE-RNN-
CAE-RNNmodelovertheAE-RNNmodelforthe basedAWEmodelandSSL-basedspeechrepresen-
word-discriminationtask,astheCAE-RNNconsis- tationsextracted‘withcontext’.
tentlyoutperformstheAE-RNNforalllanguages. Table 6 presents the results for the test′ set,
Table4presentstheresultsforthederivedsubset which includes unseen words and speakers. The
ofthetestset(test′)withsimilartrends. Itisworth models exhibit similar trends in performance in
notingthattheAPonthetest′setisrelativelybetter thiscaseaswell. Thisprovidesevidencethatthe
thanthatoftheoriginaltestsetinmostcases. This proposed methodology performs equally well on
islikelyduetothefactthatthenumberofspoken unseen words. One interesting finding is that the
wordpairsgeneratedfortheevaluationonthetest′ SSL-based speech representations considered in
issignificantlyfewercomparedtotheoriginaltest this work were pre-trained solely on English lan-
set,asmentionedinTable2. guage. Despitethis,theyarecapableofgenerating
Table 5 displays the results obtained from meaningfulfeaturesforotherlanguages,resulting
using various SSL-based speech representations in good performance as demonstrated in Table 5
(Wav2vec2,WavLM,andHuBERT)asinputfea- and6fortheword-discriminationtask.
tures, combined with different AWE extraction
5.1 Cross-lingualAnalysis
methods (mean pooling, CAE-RNN, and AE-
RNN).TheresultspresentedinTable5represent To assess the effectiveness of SSL speech
the AP for the word-discrimination task on the representation-basedCAE-RNNmodelsincross-
testset,employingdifferentSSL-basedspeechrep- lingualsettings,aCAE-RNNmodeltrainedonone
resentation feature extraction setups (‘with con- sourcelanguage(Englishinthiscase)isevaluated
text’ and ‘without context’). From Table 5, it is onfourdifferenttargetlanguages. Thisevaluation
evident that the AWEs derived ‘with context’ ex- can be considered a ‘zero-shot’ evaluation, as no
hibit greater robustness. The AP on the test set trainingdatafromthetargetlanguagesisrequired.
foralllanguagesissignificantlybetterwhenutiliz- Table7displaystheresultsintermsofAPforthe
ing SSL-based speech representations ‘with con- word-discriminationtaskonthetestsetandtest′set
text’ compared to the feature extraction ‘without for the four target languages (Polish, Portuguese,
context’. As shown in Table 5, the CAE-RNN Spanish, and French). In this scenario as well,
model demonstrates superior performance when theHuBERT-basedCAE-RNNmodelachievesthe
using SSL-based speech representations as input bestperformanceacrossalllanguages,exceptfor
features compared to the MFCC-based baseline French where Wav2vec2 performs the best. It is
model(Table3)acrossalllanguages. Furthermore, worthnotingthattheCAE-RNNmodelinthe‘zero-
Table 5 provides a comparison of the CAE-RNN shot’settingoutperformsthemeanpoolingmethod
modelwithotherbaselinemodels(meanpooling (Table5and6)(Sanabriaetal.,2023)andtheCAE-
and AE-RNN) when utilizing SSL-based speech RNN model trained on the target language with
representationsasinputfeatures. CAE-RNNcon- MFCCfeatures(Table3and4). Themeanpooling
sistentlyoutperformsboththeAE-RNNandmean method(Sanabriaetal.,2023)canbeconsidered
poolingmethodsforalllanguagesandSSLmodels. a ‘zero-shot’ AWE extraction method, as it does
AnotheradvantageoftheCAE-RNNmodelover not involve additional training on top of the pre-
meanpoolingisthattheAWEsobtainedfromCAE- trained SSL models. In a ‘zero-shot’ setup for
RNNhaveadimensionof128,whilemeanpooling- targetlanguages,usingaCAE-RNNtrainedonaAWE Input Polish Portuguese Spanish French English
Extraction Features with without with without with without with without with without
Method context context context context context context context context context context
Wav2vec2 0.01 0.00 0.00 0.00 0.02 0.00 0.03 0.01 0.02 0.00
MeanPooling WavLM 0.07 0.00 0.01 0.00 0.03 0.00 0.05 0.02 0.07 0.03
HuBERT 0.17 0.33 0.10 0.15 0.14 0.32 0.22 0.31 0.15 0.24
Wav2vec2 0.10 0.00 0.08 0.00 0.15 0.01 0.13 0.01 0.07 0.00
AE-RNN WavLM 0.34 0.14 0.21 0.11 0.43 0.20 0.34 0.23 0.26 0.17
HuBERT 0.44 0.40 0.36 0.27 0.58 0.52 0.45 0.40 0.36 0.34
Wav2vec2 0.86 0.71 0.86 0.63 0.93 0.79 0.71 0.61 0.82 0.52
CAE-RNN WavLM 0.86 0.72 0.76 0.63 0.92 0.85 0.70 0.61 0.66 0.51
HuBERT 0.90 0.82 0.88 0.71 0.95 0.89 0.74 0.65 0.86 0.65
Table5: APscoresfortheword-discriminationtaskonthetestsetusingSSL-basedspeechrepresentationsasinput
featuresforallfivelanguages. AWEextractionmethodsincludemeanpooling(Sanabriaetal.,2023),AE-RNN,
andCAE-RNN.
AWE Input Polish Portuguese Spanish French English
Extraction Features with without with without with without with without with without
Method context context context context context context context context context context
Wav2vec2 0.01 0.01 0.01 0.00 0.04 0.01 0.06 0.01 0.03 0.00
MeanPooling WavLM 0.05 0.00 0.02 0.01 0.05 0.01 0.10 0.05 0.09 0.04
HuBERT 0.18 0.31 0.11 0.17 0.19 0.40 0.30 0.40 0.18 0.29
Wav2vec2 0.09 0.01 0.09 0.01 0.24 0.02 0.21 0.03 0.08 0.01
AE-RNN WavLM 0.33 0.12 0.25 0.12 0.55 0.28 0.44 0.33 0.29 0.23
HuBERT 0.44 0.41 0.41 0.31 0.69 0.64 0.55 0.50 0.43 0.41
Wav2vec2 0.87 0.72 0.90 0.65 0.96 0.85 0.84 0.73 0.89 0.67
CAE-RNN WavLM 0.85 0.72 0.81 0.68 0.95 0.90 0.83 0.75 0.75 0.62
HuBERT 0.90 0.83 0.91 0.75 0.97 0.93 0.86 0.81 0.93 0.75
Table6: APscoresfortheword-discriminationtaskonthetest′setusingSSL-basedspeechrepresentationsasinput
featuresforallfivelanguages. AWEextractionmethodsincludemeanpooling(Sanabriaetal.,2023),AE-RNN,
andCAE-RNN..
well-resourcedsourcelanguagecanofferanadvan- Polish Portuguese Spanish French
Input
tageoverthemeanpoolingmethod. Inconclusion, Features test test′ test test′ test test′ test test′
SSL-based CAE-RNN models have fairly good
Wav2vec2 0.57 0.57 0.48 0.54 0.60 0.68 0.52 0.68
performancewhenusedcrosslingually. Therehave WavLM 0.48 0.47 0.36 0.40 0.54 0.63 0.50 0.64
beenearlierstudies(Kamperetal.,2021)onacous- HuBERT 0.59 0.60 0.50 0.56 0.62 0.69 0.48 0.65
MFCC 0.18 0.20 0.11 0.15 0.22 0.29 0.22 0.35
ticwordembeddingsforzero-resourcelanguages
using multilingual transfer with MFCC features, Table7: APfortheword-discriminationtaskwithCAE-
whichworkedwell. Also,intuitively,somegener- RNNmodeltrainedonEnglishlanguagewithvarious
alisationwasexpectedastheaimofmodellingis inputfeaturesandtestedonotherfourlanguages.
tocompressasmallsegmentofspeechintoafixed
dimensional vector. There might be a language
effectonpre-trainedSSLspeechmodelsbuttheba- letterorderonAWErepresentation. RobustAWEs
sicspeechpropertiesarestillinvarianttochanges should capture the letter order in spoken words.
inthelanguage. Thecross-lingualabilityofSSL- Forthisanalysis,samespokenwordpairsandana-
basedCAE-RNNmodelstoobtainAWEscansup- grampairsarechosenfromdifferentspeakers. Ide-
portmanyapplicationssuchasspeechsearch,in- ally, the cosine distance between the same word
dexinganddiscoverysystemsforlanguageswith pairs should be close to 0, while anagram word
low-resources(Kamperetal.,2021). pairs should be close to 1. In Table 8, HuBERT-
based CAE-RNN AWEs demonstrate cosine dis-
tances of approximately 0.01, 0.11, and 0.02 for
5.2 AnalysisofAnagramPairs
the same spoken word pairs ‘aside’, ‘this’, and
Anagrams are words that can be formed by rear- ‘no’,respectively. Theanagrampairsofthewords
ranging the letters of another word. Analysing ‘aside’,‘this’,and‘no’(i.e.,‘ideas’,‘hits’,and‘on’)
anagrampairsprovidesinsightsintotheimpactof havedistancesof0.99,0.50,and0.69,respectively,CosineDistance CosineDistance
Word1 Word2 Description
(MeanPooling) (CAE-RNN)
aside aside 0.23 0.01 Sameword
aside ideas 0.56 0.99 Anagrampair
this this 0.33 0.11 Sameword
this hits 0.38 0.50 Anagrampair
no no 0.30 0.02 Sameword
no on 0.63 0.69 Anagrampair
Table8:ComparisonofcosinedistancesbetweenAWEs
ofsamespokenwordpairsandanagrampairs.HuBERT
featuresareusedforbothCAE-RNNandmeanpooling
method.
fortheHuBERT-basedCAE-RNNmodel. These
values are significantly better for both the same
word pairs and anagram word pairs when com-
paredtotheHuBERT-basedmeanpoolingmethod
(Sanabria et al., 2023). This indicates that the
HuBERT-basedCAE-RNNmodelaccuratelycap-
tures the letter order in a word compared to the
meanpoolingbaseline(Sanabriaetal.,2023).
5.3 AWEVisualisation
t-SNE visualization is used to plot the 2-
dimensionalrepresentationsofthederivedAWEs
forallfivelanguages. Fromeachlanguage,allspo-
keninstancesofthetop7wordswiththehighest
frequencycountfromthetestsetarechosen. The
plotsdemonstratedistinctandwell-separatedclus-
tersforeachspokenwordacrossalllanguages. One
interestingpatterncanbeobservedforthePolish
language,wheretheclustersofthespokenwords
‘Owadów’ and ‘Owady’ share the boundary and
arecloselyrelatedintheAWEspace. Thisislikely
duetothefactthatthefirstfourlettersofboththe
words(o,w,a,d)aresharedandthesewordsonly
differintheirendings.
6 ConclusionsandFutureWork
It has been demonstrated that SSL-based speech
representations with CAE-RNN models outper-
form mean pooling and AE-RNN models across
alllanguages. TheyalsooutperformMFCC-based
models. Among all the SSL models, HuBERT
performs the best when used as input for the
CAE-RNNmodel,outperformingmodelssuchas
Wav2vec2andWavLM.Notably,despitebeingpre-
trained on English data, the SSL models exhibit
Figure2:t-SNEvisualisationoftheAWEsderivedfrom
HuBERT-basedCAE-RNNmodelforallfivelanguages. excellent performance on other languages, show-
Fromeachlanguage,allspokeninstancesofthetop7 casingtheircross-lingualgeneralizationcapability
wordswiththehighestfrequencycountfromthetestset forAWEextraction.
arechosen. Furthermore,quantitativeanalysisrevealsthatin-
corporatingcontextinformationofthespokenwordleadstomorerobustAWEs. TheHuBERT-based References
CAE-RNNmodeltrainedonEnglishlanguageand
Badr Abdullah, Iuliia Zaitova, Tania Avgustinova,
tested on other target languages outperforms the
BerndMöbius,andDietrichKlakow.2021a. Howfa-
mean pooling method and the CAE-RNN model miliardoesthatsound?cross-lingualrepresentational
trainedonthetargetlanguageusingMFCCfeatures. similarityanalysisofacousticwordembeddings. In
ProceedingsoftheFourthBlackboxNLPWorkshop
This‘zero-shot’methodtoobtainrobustAWEsfor
onAnalyzingandInterpretingNeuralNetworksfor
the target language can be useful in applications
NLP,pages407–419,PuntaCana,DominicanRepub-
for low-resource languages (Jacobs et al., 2023). lic.AssociationforComputationalLinguistics.
An analysis was also conducted to show that the
Badr M. Abdullah, Marius Mosbach, Iuliia Zaitova,
CAE-RNNmodeleffectivelycapturestheorderof
Bernd Möbius, and Dietrich Klakow. 2021b. Do
lettersinaword.
Acoustic Word Embeddings Capture Phonological
In future work, experiments will be conducted Similarity? An Empirical Study. In Proc. Inter-
with the “LARGE" variation of SSL models, as speech2021,pages4194–4198.
wellasmultilingualpre-trainedSSLmodelssuch
BadrM.Abdullah,BerndMöbius,andDietrichKlakow.
asWav2vec2-XLSR(Conneauetal.,2020). Addi-
2022. IntegratingFormandMeaning: AMulti-Task
tionally,aninterestingexperimentwouldinvolve
LearningModelforAcousticWordEmbeddings. In
training a single universal AWE model on all Proc.Interspeech2022,pages1876–1880.
languages and comparing its performance with
AlexeiBaevski,YuhaoZhou,AbdelrahmanMohamed,
language-specificAWEmodels. Furtherresearch
andMichaelAuli.2020. wav2vec2.0: Aframework
will focus on measuring the performance gains
forself-supervisedlearningofspeechrepresentations.
ofSSL-basedCAE-RNNmodelsondownstream InAdvancesinNeuralInformationProcessingSys-
taskssuchasquery-by-examplesearch(Settleetal., tems,volume33,pages12449–12460.CurranAsso-
ciates,Inc.
2017;Yuanetal.,2018;Huetal.,2021)andkey-
wordspotting(Shinetal.,2022).
M. S. Barakat, C. H. Ritz, and D. A. Stirling. 2011.
Keywordspottingbasedontheanalysisoftemplate
7 Limitations matchingdistances. In20115thInternationalCon-
ference on Signal Processing and Communication
This work is focused on the extraction of AWEs Systems(ICSPCS),pages1–6.
and measuring their quality solely based on the
word discrimination task. No downstream appli- MichaelA.Carlin,SamuelThomas,ArenJansen,and
HynekHermansky.2011. Rapidevaluationofspeech
cationssuchasquery-by-examplesearchandkey-
representationsforspokentermdiscovery. InProc.
word spotting, have been discussed using the im- Interspeech2011,pages821–824.
provedAWEs. Inthiswork,onlythe“BASE”ver-
sionsoftheSSL-basedspeechmodelsareexplored Sanyuan Chen, Chengyi Wang, Zhengyang Chen,
YuWu,ShujieLiu,ZhuoChen,JinyuLi,Naoyuki
forexperimentsandanalysis. Thereareothervari-
Kanda,TakuyaYoshioka,XiongXiao,JianWu,Long
ations,suchas“LARGE”version,forwhichthis
Zhou,ShuoRen,YanminQian,YaoQian,JianWu,
studycanbeextended. Allthelanguagesconsid- MichaelZeng,XiangzhanYu,andFuruWei.2022.
eredinthisworkbelongtotheIndo-Europeanlan- Wavlm: Large-scaleself-supervisedpre-trainingfor
full stack speech processing. IEEE Journal of Se-
guagefamily. Thisworkdoesnotcontaintheanal-
lectedTopicsinSignalProcessing,16(6):1505–1518.
ysisoflanguagesthatbelongtoanotherlanguage
family,suchasDravidianorAfroasiaticlanguage AlexisConneau,AlexeiBaevski,RonanCollobert,Ab-
families. Thisworkdoesnotdealwithlayer-wise delrahmanMohamed,andMichaelAuli.2020. Un-
supervisedcross-lingualrepresentationlearningfor
analysis,whichcanprovidebetterinsightsforfur-
speechrecognition. InInterspeech.
therimprovingtheAWEs.
SDavisandPMermelstein.1980. Comparisonofpara-
8 Acknowledgments
metricrepresentationsformonosyllabicwordrecog-
nitionincontinuouslyspokensentences. IEEETrans.
This work was supported by the Centre for Doc- Acoust.,Speech,SignalProcess.,28(4):357–366.
toral Training in Speech and Language Tech-
nologies (SLT) and their Applications funded Wanjia He, Weiran Wang, and Karen Livescu. 2017.
Multi-view recurrent neural acoustic word embed-
by UK Research and Innovation [grant number
dings. In5thInternationalConferenceonLearning
EP/S023062/1]. Thisworkwasalsofundedinpart
Representations,ICLR2017,Toulon,France,April
byLivePerson,Inc. 24-26,2017,ConferenceTrackProceedings.Wei-NingHsu,BenjaminBolte,Yao-HungHubertTsai, VineelPratap,QiantongXu,AnuroopSriram,Gabriel
KushalLakhotia,RuslanSalakhutdinov,andAbdel- Synnaeve,andRonanCollobert.2020. Mls: Alarge-
rahman Mohamed. 2021. Hubert: Self-supervised scalemultilingualdatasetforspeechresearch. Inter-
speechrepresentationlearningbymaskedprediction speech2020.
ofhiddenunits. IEEE/ACMTransactionsonAudio,
Speech,andLanguageProcessing,29:3451–3460. Ramon Sanabria, Hao Tang, and Sharon Goldwa-
ter. 2023. Analyzing acoustic word embeddings
frompre-trainedself-supervisedspeechmodels. In
Yushi Hu, Shane Settle, and Karen Livescu. 2021.
ICASSP 2023 - 2023 IEEE International Confer-
Acoustic span embeddings for multilingual query-
ence on Acoustics, Speech and Signal Processing
by-examplesearch. In2021IEEESpokenLanguage
TechnologyWorkshop(SLT),pages935–942.
(ICASSP),pages1–5.
TanjaSchultz,NgocThangVu,andTimSchlippe.2013.
ChristiaanJacobs,NathanaëlCarrazRakotonirina,Ev-
Globalphone: Amultilingualtext&speechdatabase
erlynAsikoChimoto,BruceA.Bassett,andHerman
in20languages. In2013IEEEInternationalCon-
Kamper. 2023. Towards hate speech detection in
ferenceonAcoustics,SpeechandSignalProcessing,
low-resource languages: Comparing asr to acous-
pages8126–8130.
ticwordembeddingsonwolofandswahili. ArXiv,
abs/2306.00410. ShaneSettle,KeithLevin,HermanKamper,andKaren
Livescu.2017. Query-by-examplesearchwithdis-
Herman Kamper. 2019. Truly unsupervised acoustic criminative neural acoustic word embeddings. In
wordembeddingsusingweaktop-downconstraints INTERSPEECH.
in encoder-decoder models. ICASSP 2019 - 2019
IEEEInternationalConferenceonAcoustics,Speech Hyeon-KyeongShin,HyewonHan,DoYeonKim,Soo-
andSignalProcessing(ICASSP),pages6535–3539. WhanChung,andHong-GooKang.2022. Learning
audio-textagreementforopen-vocabularykeyword
Herman Kamper, Micha Elsner, Aren Jansen, and spotting. InInterspeech.
SharonGoldwater.2015. Unsupervisedneuralnet-
Shu-Wen Yang, Po-Han Chi, Yung-Sung Chuang,
workbasedfeatureextractionusingweaktop-down
Cheng-ILai,KushalLakhotia,YistY.Lin,AndyT.
constraints. In 2015 IEEE International Confer-
Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin,
ence on Acoustics, Speech and Signal Processing
TzuhsienHuang,Wei-ChengTseng,KotikLee,Da-
(ICASSP),pages5818–5822.
RongLiu,ZiliHuang,ShuyanDong,Shang-WenLi,
ShinjiWatanabe,AbdelrahmanMohamed,andHung
Herman Kamper, Yevgen Matusevych, and Sharon
yiLee.2021. Superb: Speechprocessinguniversal
Goldwater. 2021. Improved acoustic word embed-
performancebenchmark. InInterspeech.
dingsforzero-resourcelanguagesusingmultilingual
transfer. IEEE/ACMTrans.Audio,SpeechandLang.
Yougen Yuan, Cheung-Chi Leung, Lei Xie, Hongjie
Proc.,29:1107–1118.
Chen, Bin Ma, and Haizhou Li. 2018. Learning
AcousticWordEmbeddingswithTemporalContext
Jingru Lin, Xianghu Yue, Junyi Ao, and Haizhou Li. forQuery-by-ExampleSpeechSearch. InProc.In-
2023. Self-Supervised Acoustic Word Embedding terspeech2018,pages97–101.
LearningviaCorrespondenceTransformerEncoder.
InProc.INTERSPEECH2023,pages2988–2992.
Yevgen Matusevych, Herman Kamper, and Sharon
Goldwater. 2020. Analyzing autoencoder-based
acousticwordembeddings. CoRR,abs/2004.01647.
Michael McAuliffe, Michaela Socolof, Sarah Mihuc,
Michael Wagner, and Morgan Sonderegger. 2017.
Montrealforcedaligner: Trainabletext-speechalign-
mentusingkaldi. InInterspeech.
AmitMeghananiandThomasHain.2024. Score: Self-
supervisedcorrespondencefine-tuningforimproved
content representations. In ICASSP 2024 - 2024
IEEEInternationalConferenceonAcoustics,Speech
andSignalProcessing(ICASSP).
VassilPanayotov,GuoguoChen,DanielPovey,andSan-
jeevKhudanpur.2015. Librispeech: Anasrcorpus
basedonpublicdomainaudiobooks. In2015IEEE
InternationalConferenceonAcoustics,Speechand
SignalProcessing(ICASSP),pages5206–5210.