Implicit Regularization of Gradient Flow on One-Layer
Softmax Attention
Heejune Sheen Siyu Chen Tianhao Wang Harrison H. Zhou
Department of Statistics and Data Science, Yale University
{heejune.sheen, siyu.chen.sc3226, tianhao.wang, huibin.zhou}@yale.edu
March 14, 2024
Abstract
We study gradient flow on the exponential loss for a classification problem with a one-
layersoftmax attention model, where the key andquery weightmatrices aretrainedseparately.
Under a separability assumption on the data, we show that when gradient flow achieves the
minimal loss value, it further implicitly minimizes the nuclear norm of the product of the key
and query weight matrices. Such implicit regularization can be described by a Support Vector
Machine(SVM)problemwithrespecttotheattentionweights. Thisfindingcontrastswithprior
results showing that the gradient descent induces an implicit regularization on the Frobenius
norm on the product weight matrix when the key and query matrices are combined into a
single weight matrix for training. For diagonal key and query matrices, our analysis builds
upon the reparameterization technique and exploits approximate KKT conditions of the SVM
associated with the classification data. Moreover, the results are extended to general weights
configurations given proper alignment of the weight matrices’ singular spaces with the data
features at initialization.
1 Introduction
Transformer-based models have become state-of-the-art in various domains of machine learning ap-
plications,especiallyfortasksrelatedtonaturallanguageprocessingandcomputervision(Vaswani et al.,
2017; Brown et al., 2020; Dosovitskiy et al., 2020; Khan et al., 2022; Yuan et al., 2021). The em-
pirical success of these models relies on the attention mechanism (Vaswani et al., 2017), a crucial
component of transformers that provides the ability to evaluate and then exploit the correlation
across tokens in the input sequence. Such ability unlocks the unprecedented performance of recent
large language models that have transformed many academic and industrial fields, as well as many
aspects of daily lives (Brown et al., 2020; Achiam et al., 2023; Anthropic, 2023; Team et al., 2023).
Yet, our understanding of the training of such attention-based models remains limited.
A number of recent studies have initiated the effort to develop a theoretical understanding
of the attention mechanism in transformers. One of the key questions here is to investigate the
training process of attention-based models and thus to understand the properties of the trained
models. In particular, for the setting of classification tasks, there are recent results characterizing
different aspects of the training dynamics of attention-based models (Li et al., 2023b; Jelassi et al.,
2022;Li et al.,2023a;Tian et al.,2024;Oymak et al.,2023;Tarzanagh et al.,2023b,a;Deora et al.,
2023). Interestingly, it was shown by Tarzanagh et al. (2023a,b) that the attention mechanism has
an intriguing connection with a certain support vector machine (SVM) problem. This connection
provides a new perspective on the attention mechanism and has been used to characterize the
1
4202
raM
31
]GL.sc[
1v99680.3042:viXraimplicit regularization of gradient descent on the classification loss. Specifically, for training a
one-layer softmax attention model with a fixed linear prediction head, Tarzanagh et al. (2023a)
showed that gradient descent implicitly minimizes the Frobenious norm of the attention weights
and meanwhile, it maximizes the margin between the attention scores of the optimal tokens and
non-optimal ones, which can be described by an associated SVM problem for separating the tokens.
This type of implicit regularization of gradient descent is closely related to a previous line of
workonmargin-maximization ofgradient-basedoptimization algorithmsforneuralnetworksinclas-
sification tasks (Soudry et al., 2018; Gunasekar et al., 2018b,a; Nacson et al., 2019a,b; Lyu and Li,
2019; Ji and Telgarsky, 2018, 2020, 2021; Moroshko et al., 2020). In the context of neural networks
without the attention module, the margin measures the separation between different classes by the
decision boundary. However, in the case of classification with an attention model, the margin refers
to the extent to which the attention mechanism separates the optimal tokens in the input sequence
from the non-optimal ones. Notably, the convergence analysis in Tarzanagh et al. (2023a) applies
only to the case where the key and query matrices are combined into a single weight matrix, and
it remains open how to track the dynamics when the key and query matrices are trained sepa-
rately. In the latter case, by analyzing the regularization path associated with the empirical loss,
Tarzanagh et al. (2023a) suggested that the gradient descent would instead implicitly minimize the
nuclear normofthecombined attention weights. Inthispaper, weconfirmthisby providing adirect
analysis on gradient flow for training the attention model with separate key and query matrices.
We focus on a one-layer softmax attention model formulated as follows: Let X RL×d and
∈
z Rd be the input sequence and query respectively, then the output of the model is
∈
Φ(K,Q;X,z) =v⊤X⊤Softmax(XKQ⊤z),
where K,Q Rd×de are the key and query weight matrices and v Rd is a fixed linear prediction
∈ ∈
head. For such a model, our main result is summarized in the following informal theorem.
Theorem 1.1 (Informal). For binary classification with a one-layer softmax attention model and
the exponential loss, gradient flow on (K,Q) at convergence perfectly separates the optimal token
from non-optimal ones for each input sequence. Meanwhile, it implicitly minimizes the nuclear norm
of the combined attention weights W = KQ⊤.
Main contributions. Specifically, our contributions are as follows:
1. For the exponential loss for binary classification, we analyze the gradient flow for training
a one-layer softmax attention model with separate key and query matrices. Under proper
conditions on the initialization and the data, we show that the empirical loss converges to the
infimum loss value as the parameter norm of the attention model grows to infinity. Assuming
the existence of a certain margin-normalized direction of the combined attention weights
W = KQ⊤, we then investigate the implicit regularization of the gradient flow.
2. Restrictingthekeyandquerymatricestobediagonal,weshowthattheimplicitregularization
diag
of gradient flow can be described by a nuclear norm minimizing SVM problem (W -SVM)
∗
for separating the optimal tokens from the non-optimal ones. Our result confirms that when
K and Q are trained separately, gradient flow implicitly minimizes the nuclear norm of the
combined attention weights, in contrast to the Frobenious norm regularization when the key
and query matrices are combined into a single weight matrix. Technically, we exploit the
reparametrization technique to analyze the intertwined dynamics of K and Q, and prove the
optimality by verifying the approximate KKT conditions of the associated SVM problem.
23. We further extend the result to more general weights configurations for the key and query
matrices (K,Q). We show with additional conditions on data that the gradient flow is equiv-
alent to the diagonal case up to certain rotation transformations, and the convergence point
is characterized by (W -SVM).
∗
2 Related work
Training dynamics of attention-based models Recent works have studied the training dy-
namics of attention-based models from various perspectives (Li et al., 2023b; Jelassi et al., 2022;
Li et al., 2023a; Tian et al., 2024; Oymak et al., 2023; Tarzanagh et al., 2023b,a). Specifically,
it was shown in Li et al. (2023b) that certain topic structure can be learned by a single-layer
transformer trained by gradient descent. Jelassi et al. (2022) showed that vision transformers
(ViT) with softmax attention can learn the spatial structure of the data via gradient descent,
and Li et al. (2023a) studied the generalization of ViT. The training dynamics of a single-layer
transformer for the next token prediction task was studied in Tian et al. (2024). The training dy-
namics of prompt-tuning for attention-based models was investigated in Oymak et al. (2023). See
alsoZhang et al.(2023);Huang et al. (2023);Nichani et al.(2024);Chen et al.(2024);Huang et al.
(2024); Thrampoulidis (2024) for recent studies on the training dynamics of transformers for in-
context learning and beyond.
The most relevant works to ours are Tarzanagh et al. (2023b,a); Vasudeva et al. (2024). The
intriguing connections between the attention mechanism and the support vector machine (SVM)
were initially derived in Tarzanagh et al. (2023b,a), where the authors characterized the implicit
regularization of gradient descent via an associated SVM problem. Vasudeva et al. (2024) extended
their results by showing the global directional convergence, as well as the convergence rate, of
gradient descent under certain data assumptions. Notably, these works considered the case where
the key and query matrices are combined into a single weight matrix, while we focus on training
the key and query matrices separately. Though not changing the expressiveness of the attention
model, the difference in the architecture indeed leads to significantly different training dynamics
and the corresponding implicit regularization. We also remark that the separate training of the key
and query matrices is a common practice (Vaswani et al., 2017).
Implicit regularization There have been many recent studies on the implicit regularization
of optimization for neural networks for both regression and classification tasks. For the regression
settings, ithasbeenshownthatgradient descentimplicitly minimizes certainparameter norms even
withoutanyexplicitregularizationintheempiricalloss(Gunasekar et al.,2018a;Azizan and Hassibi,
2018; Gidel et al., 2019; Woodworth et al., 2020; Yun et al., 2020; Li et al., 2021b; Azulay et al.,
2021; Zhou and Ge, 2023; HaoChen et al., 2021; Fan et al., 2023). For classification tasks, the
margin-maximizationphenomenonhasbeenobservedandstudiedforgradientdescentonexponential-
tailed loss Soudry et al. (2018); Gunasekar et al. (2018b,a); Nacson et al. (2019a,b); Lyu and Li
(2019); Ji and Telgarsky (2018, 2020, 2021); Ji et al. (2021); Yun et al. (2020); Moroshko et al.
(2020). These results have also been extended to other optimization algorithms (Gunasekar et al.,
2018b; Ji et al., 2021; Ji and Telgarsky, 2021), different training objectives (Nacson et al., 2019b;
Ji and Telgarsky, 2021), and homogeneous neural networks (Gunasekar et al., 2018b; Lyu and Li,
2019; Nacson et al., 2019a; Ji and Telgarsky, 2020, 2018; Yun et al., 2020; Moroshko et al., 2020).
Compared tothese works, the architecture ofthe attention model is indeed different. Inparticu-
lar, the softmax operation makes the model non-homogeneous, and consequently, previous analyses
and results on homogeneous models do not directly apply to our model. Another major distinction
3is that the implicit regularization of the attention model is characterized by SVM separating the
tokens, not the labeled data. This is due to the special structure of the attention model, which is
designed to simultaneously process a sequence of tokens.
Matrix factorization and rank minimization Finally, we note that our results are also re-
lated to the literature on implicit regularization of gradient descent for matrix factorization. It
has been observed that gradient descent exhibits certain implicit low-rank bias in the context of
matrix sensing problems (Gunasekar et al., 2017; Li et al., 2018; Arora et al., 2019; Li et al., 2020;
Stöger and Soltanolkotabi, 2021). Gunasekar et al. (2017) showed that, with small initialization,
gradient flow on matrix factorization implicitly minimizes the nuclear norm, which was further ex-
tended to deep matrix factorization models in Arora et al. (2019). Later, Li et al. (2018, 2020);
Stöger and Soltanolkotabi (2021) showed that gradient descent with small initialization can recover
low-rank solutions for matrix factorization. While our analysis involves the combined attention
weights W = KQ⊤ as a product of two parameter matrices and yields similar implicit regulariza-
tion results as in matrix factorization, our problem setting is completely different.
3 Preliminaries
We first introduce the conventions and notation used throughout the paper. For any positive
integer n, we denote [n] := 1,...,n . For a function x(t) on t, we denote by x˙(t) its derivative
{ }
with respect to t. For a matrix X, we denote by X and X its Frobenius norm and nuclear
F ∗
k k k k
norm, respectively. For a vector x Rd, we write Diag(x) Rd×d as the diagonal matrix with x
∈ ∈
being its diagonal entries. For a matrix X Rd×d, we define diag(X) Rd as the vector containing
∈ ∈
the diagonal entries. For vectors x,y Rd, x y denotes the Hadamard product between x and
∈ ⊙
y, i.e., x y = (x y ,...,x y )⊤, and we write x⊙2 := x x. For a vector x Rd, we define the
1 1 d d
softmax o⊙ peration as σ(x) := (ex1/ d exi,...,exd/ d ⊙ exi)⊤. ∈
i=1 i=1
P P
3.1 One-layer transformer for binary classification
We consider the binary classification data (X ,z ,y ) n , where for the i-th sample, (X ,z )
{ i i i }i=1 i i ∈
RL×d Rd is the feature and y 1 is the label. To solve the classification task, we consider
i
× ∈ {± }
a one-layer softmax attention model with a fixed linear prediction head v Rd. Specifically, let a
∈
key matrix K Rd×de and a query matrix Q Rd×de be the trainable model parameters, where
∈ ∈
d is the embedding dimension. Denote θ = (K,Q) for simplicity. Then, given an input sequence
e
X RL×d and a query vector z Rd, the output of the model is
∈ ∈
Φ(θ;X,z) = v⊤X⊤σ(XKQ⊤z),
where the softmax function σ() is applied row-wise. This attention model can be viewed as a
·
simplified one-layer transformer, omitting a feed-forward neural network component. We refer to
W := KQ⊤ as the combined attention weights.
For the classification task, each query vector z can be interpreted as a feature that is closely
i
related to the significance of the token within the input sequence X for determining the label y .
i i
We compute the inner product between the query vector z and the embedding of each token in the
i
input sequence X , and then apply the softmax function to obtain the “importance” of each token.
i
Then by taking the weighted sum of the tokens based on the token importance, the model applies
the linear prediction head v to make the final prediction. Following this intuition, we introduce the
notion of token score and optimal token.
4Definition 3.1 (Token score and optimal token, Tarzanagh et al. 2023b,a). For the binary classi-
fication data (X ,z ,y ) n where each sequence X = [x ,...,x ]⊤ RL×d, the score of each
{ i i i }i=1 i i1 iL ∈
token x is defined as γ := y v⊤x . Moreover, for each i [n], the index of an optimal token is
il il i il
∈
defined to be opt(i) argmax γ .
∈ l∈[L] il
With a fixed linear prediction head v, the token score reflects the importance of each token for
predicting the label, with the optimal token having the highest score. As such, the performance of
the attention model is determined by its ability to separate the optimal token from the non-optimal
ones. Throughout the paper, we consider classification data satisfying the following separability
assumption (Tarzanagh et al., 2023b,a; Vasudeva et al., 2024).
Assumption 1 (Separability). For the binary classification data (X ,z ,y ) n , the optimal to-
{ i i i }i=1
ken opt(i) is unique for all i [n]. Furthermore, there exists a matrix W Rd×d such that
∗
∈ ∈
⊤
x x W z 1 for all l = opt(i), i [n].
i,opt(i) il ∗ i
− ≥ 6 ∈
(cid:0) For any to(cid:1)ken x , the quantity x⊤Wz represents the corresponding attention score prior to
il il i
applying the softmax function. The separability condition ensures that the attention score of the
optimal token x is strictly larger than those of the non-optimal tokens. In particular, it has
i,opt(i)
been shown that the separability assumption holds for data with gaussian noise when d is larger
than L and n (Tarzanagh et al., 2023a, Theorem 1).
3.2 Gradient flow and implicit regularization
We consider the exponential loss ℓ(x) = exp( x) for the classification task. Correspondingly, the
−
empirical loss defined over the data (X ,z ,y ) n is given by
{ i i i }i=1
n
1
(θ)= ℓ(y Φ(θ;X ,z )).
i i i
L n ·
i=1
X
Wetrainthemodelbyminimizingtheaboveobjectiveoverθ. Inparticular, weconsiderthegradient
flow defined as
θ˙(t) = (θ(t)). (1)
−∇L
This is the continuous-time version of the commonly used gradient descent algorithm.
Note that the empirical loss (θ), when it is trained by the gradient descent method, may not
converge to the global infimum vL alue due to its non-convexity. Let ∗ , 1 n ℓ(γ ), then
L n i=1 i,opt(i)
P
n L
1
∗ = inf (θ)= inf exp γ σ(X Wz ) ,
il i i l
L θ L W∈Rd×d n −
i=1 (cid:18) l=1 (cid:19)
X X
where the infimum is achieved when the combined attention weights W separate the optimal tokens
from the non-optimal tokens and the norm W diverges to infinity. For any finite θ, it holds that
F
k k
(θ)> ∗ since the softmax scores for non-optimal tokens are always positive, i.e., σ(X KQ⊤z ) >
i i l
L L
0 for all l = opt(i) by the nature of softmax function.
6
To facilitate the convergence analysis of gradient flow, we consider the following assumption.
Assumption 2 (Assumption B.2, Tarzanagh et al. 2023a). For each i [n], there exists a unique
∈
optimal token and the non-optimal tokens have the same scores, i.e., γ > γ and γ = γ for
i,opt(i) il il il′
all l,l′ = opt(i), where the non-optimal token score is denoted by γ .
i,nopt
6
5We remark that Assumption 2 can be relaxed by allowing small perturbations for non-optimal
scores, and a slight modification of the current proof suffices to handle the relaxed condition. We
adhere to Assumption 2 in this work for ease of presentation.
Under Assumption 2, it can be shown that the parameter norm diverges to infinity and the
empirical loss converges as t . In such a regime, we are interested in certain directional
→ ∞
convergence of the parameters, and the implicit regularization of the gradient flow corresponds
to the specific direction to which the parameters converge. In particular, for the direction of the
combined attention weights W(t) = K(t)Q(t)⊤, one can relate it to the following SVM problem:
min W s.t. (x x )⊤Wz 1, l = opt(i),i [n]. (W -SVM)
∗ i,opt(i) il i ∗
rank(W)≤dek k − ≥ ∀ 6 ∈
This formulation was first proposed in Tarzanagh et al. (2023a). The problem (W -SVM) includes
∗
a non-convex rank constraint on W, which is trivially satisfied when d d, rendering (W -SVM)
e ∗
≥
a convex problem.
The implicit regularization of gradient flow can be characterized by showing that the direction
of the combined attention weights W(t) converges to the solution of (W -SVM). This implies
∗
that without any explicit regularization in the empirical loss, the combined attention weights are
implicitly regularized to have a small nuclear norm among the feasible points of (W -SVM).
∗
4 Main results for diagonal weights
We first consider restricting the model parameters K and Q to be diagonal matrices and show that
gradient flow converges in the direction to the optimal solution of (W -SVM) under appropriate
∗
assumptions on the data. The results will then beextended to the caseof general weights in Section
5. In the rest of this section, we present our main results and proof ideas for diagonal weights and
then compare the implicit regularization with prior work.
4.1 Gradient flow implicitly regularizes nuclear norm
Under the gradient flow (1), the key and query weight matrices K(t) and Q(t) admit the following
dynamics:
K˙(t) = (K(t),Q(t)), Q˙(t) = (K(t),Q(t)).
K Q
−∇ L −∇ L
Recall that W(t) = K(t)Q(t)⊤. It can be shown that W(t) diverges as t under As-
F
k k → ∞
sumption 2. Therefore, to characterize the directional convergence of W(t), we consider a certain
normalized version of W(t), which we call the margin-normalized direction.
Definition 4.1. For the binary classification data (X ,z ,y ) n under Assumption 2, the margin
{ i i i }i=1
of any W Rd×d is defined as
∈
µ(W):= min (x x )⊤Wz .
i,opt(i) il i
i∈[n],l6=opt(i) −
Correspondingly, the margin-normalized direction of W is defined as
x W
W := .
µ(W)
6x W(t)
When W(t) diverges, we examine the limit of the margin-normalized direction W(t)=
k kF µ(t)
W(t)
as t , under the assumption that the limit exists. Note that, if the limit of exists, then
→ ∞ kW(t)kF
x
the limit of W(t) shares the same direction. We use the normalization factor as a margin to demon-
x
strate that the limit of W(t) is an exact global solution to (W -SVM). In existing works, it is often
∗
assumed that the ℓ -normalized direction converges and the margin of the limit is strictly positive,
2
which induces the existence of the limit of margin-normalized direction (Gunasekar et al., 2018b;
Nacson et al., 2019a; Moroshko et al., 2020). For homogeneous models, a different set of assump-
tions can be used to show the convergence of the margin-normalized direction (Ji and Telgarsky,
2020; Lyu and Li, 2019).
Consider a subclass of the attention model where the key and query weight matrices admit the
following form:
ξ 0 ... 0 0 ... 0 ξ 0 ... 0 0 ... 0
k,1 q,1
0 ξ ... 0 0 ... 0 0 ξ ... 0 0 ... 0
k,2 q,2
K =  . .
.
. .
.
... . .
.
. .
.
... . . ., Q =  . .
.
. .
.
... . .
.
. .
.
... . . .,
   
 0 0 ... ξ 0 ... 0  0 0 ... ξ 0 ... 0
 k,d   q,d 
   
where ξ ,ξ Rd are trainable parameters. Here k and q in the subscript stand for key and query,
k q
∈
respectively. Then the combined weight matrix W = KQ⊤ is diagonal, and by letting β = ξ ξ ,
k q
⊙
we have
β 0 ... 0
1
0 β ... 0
2
W =  . .
.
. .
.
... . .
.
.
 
0 0 ... β 
 d
 
For the diagonal weights, it suffices to consider the dynamics of only (ξ (t),ξ (t)), and the corre-
k q
x p
spondingmargin-normalizeddirection,W(t),canbecharacterizedbyβ(t) = β(t)/µ(t). Accordingly,
we consider (W -SVM) with an additional constraint that W is diagonal. We introduce diagonal
∗
SVM:
min W s.t. (x x )⊤Wz 1 l = opt(i),i [n]. (Wdiag -SVM)
∗ i,opt(i) il i ∗
rank(W)≤de,k k − ≥ ∀ 6 ∈
W isdiagonal
diag
When d d, (W -SVM) can be expressed as following vector-form ℓ - SVM:
e ∗ 1
≥
min β s.t. β⊤(z (x x )) 1 l = opt(i), i [n], (ℓ -SVM)
1 i i,opt(i) il 1
β∈Rdk k ⊙ − ≥ ∀ 6 ∈
Bil
where we denote B := z (x | x{z). }
il i i,opt(i) il
⊙ −
w⊙2−w⊙2
To analyze the dynamics of β, we consider an alternative parametrization w = + − with
2
parameters w ,w Rd. These two parametrizations are equivalent and have been studied as the
+ −
∈
so-called diagonal linear network in existing literature (Woodworth et al., 2020; Moroshko et al.,
2020; Vaskevicius et al., 2019; Li et al., 2021a; HaoChen et al., 2021; Yun et al., 2021; Li et al.,
2022a,b;Chou et al.,2023;Wind et al.,2023;Even et al.,2024;Pesme and Flammarion,2024). For
example, we can reparametrize β with ξ = (w +w )/√2,ξ = (w w )/√2. More specifically,
k + − q + −
−
we have the following lemma on the equivalence of the two parametrizations.
7Lemma 4.1. For the loss (ξ ,ξ ) = 1 n ℓ(y v⊤X⊤σ(Xdiag(β)z)), consider the gradient flow
L k q n i=1 i
on ξ and ξ :
k q
P
ξ˙ (t)= (ξ (t),ξ (t)), ξ˙ (t)= (ξ (t),ξ (t)). (2)
k −∇ξkL k q q −∇ξqL k q
Moreover, for the loss (w ,w )= 1 n ℓ(y v⊤X⊤σ(Xdiag(w)z)), consider the gradient flow on
L + − n i=1 i
w and w :
+ −
P
w˙ (t) = (w (t),w (t)), w˙ (t) = (w (t),w (t)) (3)
+ −∇w+L + − − −∇w−L + −
Suppose the following holds at the initialization:
β(0) = w(0), ξ⊙2(0)+ξ⊙2(0) = w⊙2(0)+w⊙2(0). (4)
k q + −
Then, the dynamics of β and w are equivalent, i.e., β(t) = w(t) for all t 0.
≥
The alternative parametrization in (w ,w ) is useful for analyzing the gradient flow dynamics
+ −
of β as we can formulate the dynamics of w as described in Appendix A.3. To employ the results of
Lemma 4.1, weconsider theinitialization ξ (0) andξ (0) satisfyingtheconditions (4). Additionally,
k q
we ensure that β(0) = 0 and that all entries of ξ⊙2(0) +ξ⊙2(0) are nonzero. More precisely, for
k q
any vector ω = w (0) = w (0), where ω = 0 for all j [d], we align the initialization by letting
0 + − 0,j
6 ∈
ξ (0) and ξ (0) satisfy
k q
ξ (0) ξ (0) = 0, ξ (0)⊙2+ξ (0)⊙2 = 2ω⊙2. (5)
k ⊙ q k q 0
Setting β(0) = 0 ensures that there is no directional bias at initialization. Moreover, we require all
entries ofξ⊙2(0)+ξ⊙2(0)tobenonzerosothatthegradientflowwouldnotbestuckatinitialization.
k q
Lemma 4.2. Under Assumption 2, suppose the initialization ξ (0) and ξ (0) satisfy (5) for the
k q
dynamics of ξ (t) and ξ (t) in Lemma 4.1. Then lim β(t) = lim ξ (t) ξ (t) = .
k q t→∞ 2 t→∞ k q 2
k k k ⊙ k ∞
TheproofofLemma4.2isgiveninAppendixA.4. Lemma4.2impliesthatgradientflowdoesnot
converge to a finite point. Instead, we consider the convergence of the margin-normalized direction
p
β(t) = β(t)/µ(β(t)) as t . Before we present our results, we introduce another assumption on
→ ∞
the loss value at initialization.
Assumption 3. The loss at initialization is upper bounded as follows:
exp( γ )+ exp γ
(θ(0)) min − j,nopt i6=j − i,opt(i) .
L ≤ j∈[n]( Pn (cid:0) (cid:1))
Assumption 3 ensures that the initial loss is small enough. A similar assumption was used in
Ji and Telgarsky (2020); Lyu and Li (2019) to analyze the directional convergence of gradient flow
on homogeneous neural networks. If we start with β(0) = 0, Assumption 3 holds when the gap
between the optimal token score and the non-optimal token score is sufficiently large. Now, we are
ready to state the main results for the diagonal weights.
Theorem 4.1. Suppose d d, and consider the gradient flow dynamics of ξ (t) Rd and
e k
ξ (t) Rd in (2) with initia≥ lization satisfying (5). Under Assumptions 2 and 3, su∈ ppose the
q
p∈ p
limit β∞ := lim β(t) exists. Let K(t) = diag(ξ (t)) and Q(t) = diag(ξ (t)), correspondingly
t→∞ k q
W(t)= diag(β(t)). Then Wx ∞ = diag(βp ∞) is a global solution of (Wdiag -SVM).
∗
8The key message delivered by the above theorem is two-fold:
p
1. Global convergence of loss. Indeed, we first show that under gradient flow, the limit β∞
is a global solution of (ℓ -SVM). Note that for the diagonal weights, (ℓ -SVM) is equivalent
1 1
diag
to (W -SVM) as mentioned earlier. The theorem shows that if the limit of the margin-
∗
normalized direction exists, then the limit must be a globally optimal direction such that
gradient flow achieves minimal loss as t . More precisely, the gradient flow converges
→ ∞ diag
in the direction of a feasible diagonal matrix W of (W -SVM) that separates the opti-
f ∗
mal tokens from the non-optimal tokens for all input sequences. This further implies that
lim (K(t),Q(t)) = ∗.
t→∞
L L
2. Implicit regularization It is shown that gradient flow implicitly regularizes the nuclear
norm of combined attention weights. In the case of diagonal weights, this would potentially
p
give rise to a sparse β∞ as minimizing ℓ -norm induces sparsity. Accordingly, the diagonal
1
x p
matrix W∞ =diag(β∞) is encouraged to be a low-rank matrix.
The detailed proof of Theorem 4.1 can be found in Appendix A.5. Below we provide the proof
sketch to illustrate the main ideas.
Proof sketch of Theorem 4.1. By Lemma 4.1, we analyze the alternative parametrization w which
mirrors the dynamics of the original parametrization β under the initialization conditions. We
introduce a shorthand µ(t) µ(β(t)). By taking integral from 0 to t on (3), we have
≡
1 t n
w (t) = w exp exp γ⊤σ(g (τ)) diag(X⊤Σ(g (τ))γ z⊤)dτ ,
+ 0 ⊙ n − i i i i i i
Z0
Xi=1 (cid:16) (cid:17)
!
1 t n
w (t) = w exp exp γ⊤σ(g (τ)) diag(X⊤Σ(g (τ))γ z⊤)dτ ,
− 0 ⊙ −n − i i i i i i
Z0
Xi=1 (cid:16) (cid:17)
!
where w := w (0) = w (0) Rd is an initialization such that w = 0 for all j [d]. Here,
0 + − 0,j
∈ 6 ∈
we define Σ(g ) := Diag (σ(g )) σ(g )σ(g )⊤ RL×L, γ = y X⊤v RL, and g (t) :=
X diag(β(t))z i RL for t 0. i Th− en, thi e dyni amic∈ s of the mai rgin-ni ormi aliz∈ ed direction i βp (t) =
i i
∈ ≥
β(t)/µ(t) can be expressed as
p w⊙2 2 n t
β(t) = 0 sinh exp( γ⊤σ(g (τ)))(Σ(g (τ))γ ) B dτ . (6)
µ(t) ⊙ − n − i i i i l il
(cid:18) i=1l6=opt(i)Z0 (cid:19)
X X
p
Our goal is to show that β∞ satisfies the KKT conditions of (ℓ -SVM): There exist λ 0 for
1 il
≥
all i [n] and l = opt(i), such that
∈ 6
(a) (Primal feasibility) for all i [n] and any l = opt(i), β⊤B 1;
il
∈ 6 ≥
(b) (Stationarity) λ B ∂ β , where ∂ β denotes the Clarke subdifferential;
i∈[n],l6=opt(i) il il ∈ k k1 k k1
(c) (ComplementaPry slackness) for all i [n] and any l = opt(i), λ (1 β⊤B ) = 0.
il il
∈ 6 −
p p p
Since β∞ = lim β(t), we show that for all sufficiently large t, β(t) approximately satisfies the
t→∞
KKT conditions.
For the primal feasibility condition (a), we employ (6) to derive that the empirical loss decreases
over time. Furthermore, we show that the loss converges to the minimal loss value ∗ under
p L
Assumption 3, which then implies the primal feasibility of β(t) for sufficiently large t.
9Forthestationarity condition (b),giventhattheobjectivefunctionof(ℓ -SVM)isanon-smooth
1
p
function, we carefully construct λ (t) 0 for β(t) by analyzing (6) as follows. For all l = opt(i),
il
≥ 6
i [n], and t 0, we let
∈ ≥
2 t
λ (t) := − exp γ⊤σ(g (τ)) (Σ(g (τ))γ ) dτ.
il nlnµ(t) − i i i i l
Z0
(cid:16) (cid:17)
p
Then, we show that (λ (t)B ) 1 (or 1) when the j-th entry of β∞ is positive (or
i∈[n],l6=opt(i) il il j → −
p
negative), respectively. Moreover, for the entries of β∞ that are zero, the corresponding entries of
P
λ (t)B are guaranteed to be in [ 1,1] for sufficiently large t.
i∈[n],l6=opt(i) il il −
Regarding the complementary slackness condition (c), we show that for all i [n], l = opt(i)
P ∈ 6
and sufficiently large t, λ (t) is bounded from above:
il
t
σ (g (τ))dτ
λ (t) C 0 l i ,
il ≤ t n σ (g (τ))dτ
0 i=1R l6=opt(i) l i
R P P
where C > 0 is some constant. Then, we demonstrate that λ (t) 0 as t for all i [n],l =
il
p → → ∞ ∈ 6
opt(i) such that (β∞)⊤B > 1. It can be obtained by comparing the ratio between λ (t) and
il il
p
λ (t), where indices i′ and l′ satisfy (β∞)⊤B = 1.
i′l′ i′l′
p
Combining the above arguments, β(t) approaches to a point satisfying the KKT conditions as
t increases. Finally, we apply the KKT approximation point theorem in Dutta et al. (2013) to get
p
that β∞ satisfies the KKT conditions.
4.2 Comparison with implicit Frobenius norm regularization
The implicit regularization of gradient descent on W was derived in Tarzanagh et al. (2023a) un-
der Assumption 2 with appropriate initialization, and further studied in Vasudeva et al. (2024).
Specifically, for the loss (W) = 1 n ℓ(y Φ(W;X ,z )), they showed that gradient descent on
L n i=1 i i i
W converges in the direction to the solution to the following SVM problem:
P
min W s.t. (x x )⊤Wz 1, l = opt(i), i [n]. (W -SVM)
F i,opt(i) il i F
W∈Rd×dk k − ≥ ∀ 6 ∈
Notably, comparing (W -SVM) and (W -SVM), we observe that gradient descent on the combined
F ∗
weight matrix W implicitly minimizes the Frobenius norm of W, while gradient flow on (K,Q)
implicitly minimizes instead the nuclear norm of W = KQ⊤ by Theorem 4.1. This suggests that
gradient descent exhibits different implicit regularization behavior under different parametrizations
of the attention weights.
Similar discrepancy in implicit regularization under different model parametrizations has also
been observed in the context of linear model. Specifically, given a dataset (x ,y ) n , where
{ i i }i=1
x Rd, y R, the parameter β Rd of the linear regression model is learned by minimizing the
i i
squ∈ ared loss∈ 1 n (y x⊤β)2. H∈ ere, each y = x⊤β∗+ǫ , where β∗ Rd is a true parameter, and
n i=1 i − i i i i ∈
iid
ǫ (0,1) is a Gaussian noise. When the model is overparametrized, i.e., d >n, it is known that
i P
∼ N
gradient descent on β space implicitly minimizes the ℓ norm of β (Gunasekar et al., 2018a). In
2
contrast, if we considerthediagonal linearnetwork where β isparametrized as β = w⊙2 w⊙2, then
+ − −
for a small initialization, it has been shown that gradient descent on (w ,w ) induces an implicit
+ −
regularizer on β that behaves like the ℓ norm (Woodworth et al., 2020; Vaskevicius et al., 2019;
1
Chou et al., 2023). See also a general framework for understanding the implicit regularization of
gradient descent on reparametrized models in Li et al. (2022b).
10These results indicate how the choice of model parametrization can impact the training process,
which further affects the generalization performance of the learned model. In particular, our results
offerinsights intotheintricate properties oftheattentionmechanism, whereinpractice, thekeyand
query matrices are indeed trained separately instead of being combined into a single weight matrix
(Vaswani et al., 2017). Note that the nuclear norm is a convex relaxation of the rank function,
thus the nuclear norm minimization in (W -SVM) encourages a low-rank structure of the combined
∗
attention weights. Our findings are consistent with the empirical evidence demonstrating that
pre-trained transformer weights tend to be low-rank (Aghajanyan et al., 2020; Wang et al., 2020;
Choromanski et al., 2020).
5 Main results for general weights
We proceed to study the gradient flow for the general case where the key and query matrices
K Rd×de and Q Rd×de are not necessarily diagonal. Below we first introduce some additional
∈ ∈
assumptions on the binary classification data needed for general weights configurations, and then
present our main result.
For general weights configurations, the dynamics of individual entries of K and Q exhibit intri-
cate dependencies on other entries and data, making their analysis more challenging. In this case,
we consider the following assumption for the data (X ,z ,y ) n .
{ i i i }i=1
Assumption 4. The binary classification data (X ,z ,y ) n satisfies the following properties:
{ i i i }i=1
(i) For all i= j, x⊤ x = 0 and z⊤z = 0.
6 i,opt(i) j,opt(j) i j
(ii) For all i [n], x⊤ z = x z .
∈ | i,opt(i) i | k i,opt(i) k2 k i k2
(iii) For each i [n], the set of the indices of non-optimal tokens [L] opt(i) can be partitioned
∈ \{ }
(L−1)/2
into a pairing (x ,x ) , where for each of the pairs, there exists a constant
{ i,l2k−1 i,l2k }k=1
c [ 1,1) such that
∈ −
x⊤ (x x ) = 0, x +x = 2cx .
i,opt(i) i,l2k−1 − i,l2k i,l2k−1 i,l2k i,opt(i)
In the above assumption, condition (i) states that the optimal tokens of different sequences are
orthogonal to each other, and the query tokens are also orthogonal to each other. Condition (ii)
states that for each sequence, the query token and the optimal token are parallel. In addition,
condition (iii) implies that for each sequence, the non-optimal tokens can be paired such that their
projections onto the optimal token are equal, and their sum is in the direction of the optimal token.
Moreover, by the orthogonality of the optimal tokens and query tokens, we can formulate an
alignmentpropertyofthekeyandquerymatricesK andQ,definedasfollows. UnderAssumption4,
there exist two orthogonal matrices U ,U Rd×d, whose columns are denoted by uk,...,uk
q q K Q ∈ { 1 d}
and u ,...,u respectively, such that for all i [n],
{ 1 d} ∈
x z
i,opt(i) = uk uk,...,uk , i =uq uq ,...,uq , (7)
x π(i) ∈{ 1 d} z π(i) ∈ { 1 d}
i,opt(i) 2 i 2
k k k k
for some one-to-one mapping π :[n] [d].
→
11Definition 5.1. (Alignment Property) Under Assumption 4 on the binary classification dataset
(X ,z ,y ) n , let U ,U Rd×d be orthogonal matrices such that (7) holds for all i [n]. We
{ i i i }i=1 K Q ∈ ∈
say that the key andquery weight matrices K Rd×de andQ Rd×de satisfy thealignmentproperty
∈ ∈
with respect to (X ,z ,y ) n if K and Q can be decomposed into the following structure:
{ i i i }i=1
K = U Λ V⊤, Q = U Λ V⊤,
K K Q Q
where V Rde×de is an orthogonal matrix and Λ ,Λ Rd×de are diagonal matrices.
K Q
∈ ∈
Definition 5.1 requires that optimal (query) tokens align with the left eigenvectors of K (Q),
respectively. When the initialization K(0) and Q(0) satisfy the alignment property with respect
to (X ,z ,y ) n , we can show that the alignment is preserved along the whole trajectory, thus
{ i i i }i=1
significantly simplifying the dynamics of K(t) and Q(t). This is formalized in the following lemma.
Lemma 5.1 (Gradient flow preserves alignment). Under Assumption 4, suppose the initialization
of the gradient flow satisfies the alignment property with respect to (X ,z ,y ) n in the sense of
{ i i i }i=1
Definition 5.1, that is,
K(0) = U Λ (0)V⊤, Q(0) = U Λ (0)V⊤.
K K Q Q
Then for all t 0, K(t) and Q(t) given by the gradient flow (1) can be decomposed into
≥
K(t)= U Λ (t)V⊤, Q(t) = U Λ (t)V⊤.
K K Q Q
Based on the characterization given by Lemma 5.1, we can analyze the implicit regularization
of the gradient flow for general weights. The main result is presented in the following theorem.
Theorem 5.1. Under Assumptions 2, 3, and 4, suppose that the limit Wx ∞ , lim Wx (t) exists.
t→∞
Assume that the initialization of the gradient flow satisfies the alignment property with respect to
(X ,z ,y ) n in the sense of Definition 5.1, that is,
{ i i i }i=1
K(0) = U Λ (0)V⊤, Q(0) = U Λ (0)V⊤,
K K Q Q
where d d. Let Λ (t) = diag(ξ (t)), Λ (t) = diag(ξ (t)) and β(t) = ξ (t) ξ (t) such that
e K k Q q k q
≥ x p ⊙
(ξ (0),ξ (0)) satisfy (5). Then, the gradient flow limit W∞ = U diag(β∞)U⊤ is a global optimal
k q K Q
solution of (W -SVM).
∗
Theorem 5.1 shows that under appropriate conditions, the results of Theorem 4.1 can be ex-
tended to general matrices K and Q. Similar to the case of diagonal weights, the empirical loss
achieved by the gradient flow converges to the global infimum value ∗ as t , and the gradient
L → ∞
flow also implicitly regularizes the nuclear norm of the combined attention weights. We conclude
this section by providing a proof sketch of Lemma 5.1 and Theorem 5.1, and the detailed proofs are
deferred to Appendix A.6 and Appendix A.7.
Proof sketch of Theorem 5.1. We first present the main idea of the proof for the preservation of the
alignment property along the gradient flow trajectory (Lemma 5.1), and then utilize the simplified
x
dynamics to show the optimality of W∞.
By direct calculation, we have
n
1
K˙(t) = (K(t),Q(t)) = exp( γ⊤σ(g (t)))X⊤Σ(g (t))γ z⊤Q(t),
−∇K L n − i i i i i i
i=1
X
n
1
Q˙(t) = (K(t),Q(t)) = exp( γ⊤σ(g (t)))z γ⊤Σ(g (t))X K(t),
−∇Q L n − i i i i i i
i=1
X
12where Σ(g ) = Diag (σ(g )) σ(g )σ(g )⊤ and g (t) = X K(t)Q(t)⊤z . Denote a vector ψ ,
i i i i i i i i
−
X⊤Σ(g )γ Rd for each i [n]. Observe that the singular vector spaces of K(t) and Q(t) are
i i i ∈ ∈
invariantalongthegradientflowtrajectorywhenthealignmentpropertyinDefinition5.1issatisfied
at initialization, in the sense that ψ / ψ = uk for all i [n]. Hence, it suffices to verify this
i k i k2 π(i) ∈
property.
For any i [n], ψ can be expressed as
i
∈
ψ = (x x )σ(g ) σ(g ) (γ γ ). (8)
i i,opt(i) − il i opt(i) i l i,opt(i) − i,nopt
l∈[L],l6=opt(i)
X
WhenK andQsatisfythealignmentpropertywithdata (X ,z ,y ) n ,itfollowsfromAssumption
{ i i i }i=1
4 (iii) that for each x , there exists a unique l′ [L] such that
il
∈
x x
x⊤Wz =x⊤U Λ Λ U⊤z =x⊤ i,opt(i) diag(Λ Λ ) =x⊤ i,opt(i) diag(Λ Λ ) =x⊤Wz .
il i il K K Q Q i il x K Q π(i) il′ x K Q π(i) il′ i
i,opt(i) 2 i,opt(i) 2
k k k k
This implies that the paired x and x have the same attention probability given by the softmax
il il′
operation, i.e., σ(g ) = σ(g ) . Then, again by Assumption 4 (iii), all components of x and x
i l i l′ il il′
in the directions orthogonal to x can be canceled out in (8), and only the components in the
i,opt(i)
direction of x remain. Consequently,
i,opt(i)
ψ x
i = i,opt(i) = uk .
ψ x π(i)
i 2 i,opt(i) 2
k k k k
Hence, we can apply Lemma 5.1 to simplify the gradient flow dynamics of K and Q and focus on
the dynamics of the spectral domain of K and Q.
p
By employing the analogous argument as the proof of Theorem 4.1, it can be shown that β∞ is
a global solution to
min β s.t. β⊤ sign( x ,z ) x x ,z e 1 l = opt(i), i [n], (9)
1 i,opt(i) i i,opt(i) il i π(i)
β∈Rdk k h i h − i ≥ ∀ 6 ∈
(cid:0) (cid:1)
where sign() is a sign function and e Rd is a vector with 1 in the ith entry and 0 elsewhere.
i
· p ∈ x p
Utilizing the fact that β∞ satisfies the KKT condition of (9), we verify that W∞ = U diag(β∞)U⊤
K Q
satisfies the KKT conditions of (W -SVM). For details, we refer to Appendix A.7.
∗
6 Conclusion and future work
Wehavestudiedimplicitregularization oftheone-layersoftmaxattentionmodeltrainedbygradient
flowforbinaryclassificationtasks. Whilepreviousworkprimarilyfocusedonthetrainingonasingle
weight matrix that combines key and query matrices, we have analyzed the training dynamics on
the key and query matrices. The distinction in parameterization has enabled us to characterize
the different implicit regularizations, minimizing the nuclear norm of the product of key and query
matrices.
For future work, it would be interesting to extend our results to more general settings, and
attention models. Additionally, investigating how the implicit regularizations influence the model’s
generalization performance would be an intriguing direction.
13References
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D.,
Altenschmidt, J., Altman, S., Anadkat, S. et al. (2023). Gpt-4 technical report. arXiv preprint
arXiv:2303.08774.
Aghajanyan, A., Zettlemoyer, L. and Gupta, S. (2020). Intrinsic dimensionality explains the effec-
tiveness of language model fine-tuning. arXiv preprint arXiv:2012.13255.
Anthropic (2023). Model card and evaluations for claude models.
Arora, S., Cohen, N., Hu, W. and Luo, Y. (2019). Implicit regularization in deep matrix factoriza-
tion. Advances in Neural Information Processing Systems, 32.
Azizan, N. and Hassibi, B. (2018). Stochastic gradient/mirror descent: Minimax optimality and
implicit regularization. arXiv preprint arXiv:1806.00952.
Azulay, S., Moroshko, E., Nacson, M. S., Woodworth, B. E., Srebro, N., Globerson, A. and
Soudry, D. (2021). On the implicit bias of initialization shape: Beyond infinitesimal mirror
descent. In International Conference on Machine Learning. PMLR.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A.,
Shyam, P., Sastry, G., Askell, A. et al. (2020). Language models are few-shot learners. Advances
in neural information processing systems, 33 1877–1901.
Chen, S., Sheen, H., Wang, T. and Yang, Z. (2024). Training dynamics of multi-head softmax
attention for in-context learning: Emergence, convergence, and optimality. arXiv preprint
arXiv:2402.19442.
Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P.,
Davis, J., Mohiuddin, A., Kaiser, L. et al. (2020). Rethinking attention with performers. arXiv
preprint arXiv:2009.14794.
Chou, H.-H., Maly, J. and Rauhut, H. (2023). More is less: inducing sparsity via overparameteri-
zation. Information and Inference: A Journal of the IMA, 12 iaad012.
Deora, P., Ghaderi, R., Taheri, H. and Thrampoulidis, C. (2023). On the optimization and gener-
alization of multi-head attention. arXiv preprint arXiv:2310.12680.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,
Dehghani, M., Minderer, M., Heigold, G., Gelly, S. et al. (2020). An image is worth 16x16
words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.
Dutta, J., Deb, K., Tulshyan, R. and Arora, R. (2013). Approximate kkt points and a proximity
measure for termination. Journal of Global Optimization, 56 1463–1499.
Even, M., Pesme, S., Gunasekar, S. and Flammarion, N. (2024). (s) gd over diagonal linear net-
works: Implicit bias, large stepsizes and edge of stability. Advances in Neural Information Pro-
cessing Systems, 36.
Fan, J., Yang, Z. and Yu, M. (2023). Understanding implicit regularization in over-parameterized
single index model. Journal of the American Statistical Association, 118 2315–2328.
14Gidel, G., Bach, F. and Lacoste-Julien, S. (2019). Implicit regularization of discrete gradient dy-
namics in linear neural networks. Advances in Neural Information Processing Systems, 32.
Gunasekar, S., Lee, J., Soudry, D. and Srebro, N. (2018a). Characterizing implicit bias in terms of
optimization geometry. In International Conference on Machine Learning. PMLR.
Gunasekar, S., Lee, J. D., Soudry, D. and Srebro, N. (2018b). Implicit bias of gradient descent on
linear convolutional networks. Advances in neural information processing systems, 31.
Gunasekar, S., Woodworth, B. E., Bhojanapalli, S., Neyshabur, B. and Srebro, N. (2017). Implicit
regularization in matrix factorization. Advances in neural information processing systems, 30.
HaoChen, J. Z., Wei, C., Lee, J. and Ma, T. (2021). Shape matters: Understanding the implicit
bias of the noise covariance. In Conference on Learning Theory. PMLR.
Huang, Y., Cheng, Y.and Liang, Y. (2023). In-context convergence of transformers. arXiv preprint
arXiv:2310.05249.
Huang, Y., Wen, Z., Chi, Y. and Liang, Y. (2024). Transformers provably learn feature-position
correlations in masked image modeling. arXiv preprint arXiv:2403.02233.
Jelassi, S., Sander, M. and Li, Y. (2022). Vision transformers provably learn spatial structure.
Advances in Neural Information Processing Systems, 35 37822–37836.
Ji, Z., Srebro, N. and Telgarsky, M. (2021). Fast margin maximization via dual acceleration. In
International Conference on Machine Learning. PMLR.
Ji, Z. and Telgarsky, M. (2018). Gradient descent aligns the layers of deep linear networks. arXiv
preprint arXiv:1810.02032.
Ji, Z. and Telgarsky, M. (2020). Directional convergence and alignment in deep learning. Advances
in Neural Information Processing Systems, 33 17176–17186.
Ji, Z. and Telgarsky, M. (2021). Characterizing the implicit bias via a primal-dual analysis. In
Algorithmic Learning Theory. PMLR.
Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S. and Shah, M. (2022). Transformers in
vision: A survey. ACM computing surveys (CSUR), 54 1–41.
Li, H., Wang, M., Liu, S. and Chen, P.-Y. (2023a). A theoretical understanding of shallow vision
transformers: Learning, generalization, and sample complexity. arXiv preprint arXiv:2302.06015.
Li, J., Nguyen, T., Hegde, C.and Wong, K. W.(2021a). Implicit sparseregularization: The impact
ofdepthandearlystopping. Advances inNeuralInformationProcessingSystems,3428298–28309.
Li, Y., Li, Y. and Risteski, A. (2023b). How do transformers learn topic structure: Towards a
mechanistic understanding. arXiv preprint arXiv:2303.04245.
Li, Y., Ma, T. and Zhang, H.(2018). Algorithmic regularization in over-parameterized matrix sens-
ing and neural networks with quadratic activations. In Conference On Learning Theory. PMLR.
Li, Z., Luo, Y. and Lyu, K. (2020). Towards resolving the implicit bias of gradient descent for
matrix factorization: Greedy low-rank learning. arXiv preprint arXiv:2012.09839.
15Li, Z., Wang, T. and Arora, S. (2021b). What happens after sgd reaches zero loss?–a mathematical
framework. arXiv preprint arXiv:2110.06914.
Li, Z., Wang, T. and Arora, S. (2022a). What happens after SGD reaches zero loss? –a mathemat-
ical framework. In International Conference on Learning Representations.
Li, Z., Wang, T., Lee, J. D. and Arora, S. (2022b). Implicit bias of gradient descent on
reparametrized models: On equivalence to mirror descent. Advances in Neural Information Pro-
cessing Systems, 35 34626–34640.
Lyu, K.andLi, J.(2019). Gradient descentmaximizes themargin ofhomogeneous neuralnetworks.
arXiv preprint arXiv:1906.05890.
Moroshko, E., Woodworth, B. E., Gunasekar, S., Lee, J. D., Srebro, N. and Soudry, D. (2020). Im-
plicitbiasindeeplinearclassification: Initializationscalevstrainingaccuracy. Advances in neural
information processing systems, 33 22182–22193.
Nacson, M. S.,Gunasekar, S.,Lee, J.,Srebro, N.andSoudry, D.(2019a). Lexicographic anddepth-
sensitivemarginsinhomogeneousandnon-homogeneousdeepmodels.InInternationalConference
on Machine Learning. PMLR.
Nacson, M. S., Lee, J., Gunasekar, S., Savarese, P. H. P., Srebro, N. and Soudry, D. (2019b). Con-
vergenceofgradientdescentonseparabledata. InThe22ndInternationalConference on Artificial
Intelligence and Statistics. PMLR.
Nichani, E., Damian, A. and Lee, J. D. (2024). How transformers learn causal structure with gra-
dient descent. arXiv preprint arXiv:2402.14735.
Oymak, S.,Rawat, A. S.,Soltanolkotabi, M.andThrampoulidis, C.(2023). Ontheroleofattention
in prompt-tuning. arXiv preprint arXiv:2306.03435.
Pesme, S. and Flammarion, N. (2024). Saddle-to-saddle dynamics in diagonal linear networks. Ad-
vances in Neural Information Processing Systems, 36.
Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S. and Srebro, N. (2018). The implicit bias of
gradient descent on separable data. The Journal of Machine Learning Research, 19 2822–2878.
Stöger, D. and Soltanolkotabi, M. (2021). Small random initialization is akin to spectral learning:
Optimization and generalization guarantees for overparameterized low-rank matrix reconstruc-
tion. Advances in Neural Information Processing Systems, 34 23831–23843.
Tarzanagh, D. A., Li, Y., Thrampoulidis, C. and Oymak, S. (2023a). Transformers as support vec-
tor machines. arXiv preprint arXiv:2308.16898.
Tarzanagh, D. A., Li, Y., Zhang, X. and Oymak, S. (2023b). Max-margin token selection in atten-
tion mechanism. In Thirty-seventh Conference on Neural Information Processing Systems.
Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J.,
Dai, A. M., Hauth, A. et al. (2023). Gemini: a family of highly capable multimodal models.
arXiv preprint arXiv:2312.11805.
Thrampoulidis, C. (2024). Implicit bias of next-token prediction. arXiv preprint arXiv:2402.18551.
16Tian, Y., Wang, Y., Chen, B. and Du, S. S. (2024). Scan and snap: Understanding training dy-
namics and token composition in 1-layer transformer. Advances in Neural Information Processing
Systems, 36.
Vaskevicius, T., Kanade, V. and Rebeschini, P. (2019). Implicit regularization for optimal sparse
recovery. Advances in Neural Information Processing Systems, 32.
Vasudeva, B., Deora, P. and Thrampoulidis, C. (2024). Implicit bias and fast convergence rates for
self-attention. arXiv preprint arXiv:2402.05738.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u. and
Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing
systems, 30.
Wang, S., Li, B. Z., Khabsa, M., Fang, H. and Ma, H. (2020). Linformer: Self-attention with linear
complexity. arXiv preprint arXiv:2006.04768.
Watson, G. A.(1992). Characterization of thesubdifferential ofsomematrix norms. Linear Algebra
Appl, 170 33–45.
Wind, J. S., Antun, V. and Hansen, A. C. (2023). Implicit regularization in ai meets generalized
hardness of approximation in optimization–sharp results for diagonal linear networks. arXiv
preprint arXiv:2307.07410.
Woodworth, B., Gunasekar, S., Lee, J. D., Moroshko, E., Savarese, P., Golan, I., Soudry, D. and
Srebro, N.(2020). Kernelandrichregimesinoverparametrizedmodels.InConferenceonLearning
Theory. PMLR.
Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Jiang, Z.-H., Tay, F. E., Feng, J. and Yan, S.
(2021). Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Pro-
ceedings of the IEEE/CVF international conference on computer vision.
Yun, C., Krishnan, S. and Mobahi, H. (2020). A unifying view on implicit bias in training linear
neural networks. arXiv preprint arXiv:2010.02501.
Yun, C., Krishnan, S. and Mobahi, H. (2021). A unifying view on implicit bias in training linear
neural networks. In International Conference on Learning Representations.
Zhang, R., Frei, S. and Bartlett, P. L. (2023). Trained transformers learn linear models in-context.
arXiv preprint arXiv:2306.09927.
Zhou, M. and Ge, R. (2023). Implicit regularization leads to benign overfitting for sparse linear
regression. arXiv preprint arXiv:2302.00257.
17A Appendix
Contents
A.1 Calculation of the gradients . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.2 Proof of Lemma 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.3 Gradient flow dynamics of β . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.4 Proof of Lemma 4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
A.5 Proof of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
A.6 Proof of Lemma 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
A.7 Proof of Theorem 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
A.1 Calculation of the gradients
Given the data (X ,z ,y ) n , where X RL×d,z Rd,y 1 , we minimize the following
{ i i i }i=1 i ∈ i ∈ i ∈ {± }
exponential loss function:
n
1
(K,Q) = ℓ γ⊤σ(g ) ,
L n i i
Xi=1 (cid:16) (cid:17)
where ℓ(x) = exp( x), γ = y X⊤v RL, g = X KQ⊤z RL with the parameter K,Q
− i i i ∈ i i i ∈ { } ∈
Rd×de (d d), and a fixed v Rd.
e
≥ ∈
The gradients of (K,Q) with respect to K and Q are given as follows:
L
n
1
(K,Q) = exp γ⊤σ(g ) X⊤Σ(g )γ z⊤Q,
∇K L −n − i i i i i i
Xi=1 (cid:16) (cid:17)
n
1
(K,Q) = exp γ⊤σ(g ) z γ⊤Σ(g )X K,
∇Q L −n − i i i i i i
Xi=1 (cid:16) (cid:17)
where Σ(g )= Diag (σ(g )) σ(g )σ(g )⊤ RL×L.
i i i i
− ∈
If we restrict K,Q Rd×de to be diagonal as in Section 4, it suffices to consider the parameters:
∈
ξ = diag(K) Rd,
k
∈
ξ = diag(Q) Rd,
q
∈
β = diag(W)= ξ ξ Rd.
k q
⊙ ∈
The gradient vectors evaluated at ξ and ξ are
k q
n
1
(ξ ,ξ ) = exp γ⊤σ(g ) diag X⊤Σ(g )γ z⊤ ξ ,
∇ξkL k q −n − i i i i i i ⊙ q
Xi=1 (cid:16) (cid:17) (cid:16) (cid:17)
n
1
(ξ ,ξ ) = exp γ⊤σ(g ) diag X⊤Σ(g )γ z⊤ ξ ,
∇ξqL k q −n − i i i i i i ⊙ k
Xi=1 (cid:16) (cid:17) (cid:16) (cid:17)
where with a slight abuse of notation, we let g = X (β z ).
i i i
⊙
18A.2 Proof of Lemma 4.1
For the reader’s convenience, we restate Lemma 4.1.
Lemma 4.1. For the loss (ξ ,ξ ) = 1 n ℓ(y v⊤X⊤σ(Xdiag(β)z)), consider the gradient flow
L k q n i=1 i
on ξ and ξ :
k q
P
ξ˙ (t)= (ξ (t),ξ (t)), ξ˙ (t)= (ξ (t),ξ (t)). (2)
k −∇ξkL k q q −∇ξqL k q
Moreover, for the loss (w ,w )= 1 n ℓ(y v⊤X⊤σ(Xdiag(w)z)), consider the gradient flow on
L + − n i=1 i
w and w :
+ −
P
w˙ (t) = (w (t),w (t)), w˙ (t) = (w (t),w (t)) (3)
+ −∇w+L + − − −∇w−L + −
Suppose the following holds at the initialization:
β(0) = w(0), ξ⊙2(0)+ξ⊙2(0) = w⊙2(0)+w⊙2(0). (4)
k q + −
Then, the dynamics of β and w are equivalent, i.e., β(t) = w(t) for all t 0.
≥
Proof. Considerthegradientflowdynamicsofβ andw. Letg (β(t)) = X (β(t) z ),andℓ (β(t)) =
i i i i
⊙
exp( γ⊤σ(g (β(t)))). For the parametrization β = ξ ξ , the gradient flow dynamics are
− i i k ⊙ q
ξ˙ (t) = (ξ (t),ξ (t))
k −∇ξkL k q
n
1
= ℓ′(β(t))diag(X⊤Σ(g (β(t)))γ z⊤) ξ (t),
−n i i i i i ⊙ q
i=1
X
ξ˙ (t) = (ξ (t),ξ (t))
q −∇ξqL k q
n
1
= ℓ′(β(t))diag(X⊤Σ(g (β(t)))γ z⊤) ξ (t),
−n i i i i i ⊙ k
i=1
X
β˙(t) = (ξ (t) ˙ ξ (t))
k q
⊙
= ξ˙ (t) ξ (t)+ξ (t) ξ˙ (t)
k q k q
⊙ ⊙
n
1
= ℓ′(β(t))diag(X⊤Σ(g (β(t)))γ z⊤) ξ⊙2(t)+ξ⊙2(t) . (10)
−n i i i i i ⊙ k q
i=1
X (cid:0) (cid:1)
w⊙2−w⊙2
For the alternative parametrization, w = + − , the gradient flow dynamics are expressed as
2
w˙ (t) = (w (t),w (t))
+ −∇w+L + −
n
1
= ℓ′(w(t))diag(X⊤Σ(g (w(t)))γ z⊤) w (t), (11)
−n i i i i i ⊙ +
i=1
X
w˙ (t) = (w (t),w (t))
− −∇w−L + −
n
1
= ℓ′(w(t))diag(X⊤Σ(g (w(t)))γ z⊤) w (t), (12)
n i i i i i ⊙ −
i=1
X
w⊙2(t) w⊙2(t)
w˙(t) = + − − = w˙ (t) w (t) w˙ (t) w (t)
+ + − −
2 ⊙ − ⊙
n
1
= ℓ′(w(t))diag(X⊤Σ(g (w(t)))γ z⊤) w⊙2(t)+w⊙2(t) . (13)
−n i i i i i ⊙ + −
i=1
X (cid:0) (cid:1)
19Moreover, we have the dynamics of ξ⊙2(t)+ξ⊙2(t) and w⊙2(t)+w⊙2(t):
k q + −
ξ⊙2(t)+˙ ξ⊙2(t) = 2ξ (t) ξ˙ (t)+2ξ (t) ξ˙ (t)
k q k ⊙ k q ⊙ q
n
(cid:0) (cid:1) 4
= ℓ′(β(t))diag(X⊤Σ(g (β(t)))γ z⊤) β(t), (14)
−n i i i i i ⊙
i=1
X
w⊙2(t)+˙ w⊙2(t) = 2w (t) w˙ (t)+2w (t) w˙ (t)
+ − + ⊙ + − ⊙ −
n
(cid:0) (cid:1) 4
= ℓ′(w(t))diag(X⊤Σ(g (w(t)))γ z⊤) w(t). (15)
−n i i i i i ⊙
i=1
X
Note that, if ξ⊙2(t)+ξ⊙2(t) = w⊙2(t)+w⊙2(t) and β(t) = w(t), then β˙(t) = w˙(t) follows by (10)
k q + −
and (13). On the other hand, if β(t) = w(t), then (ξ⊙2(t)+˙ ξ⊙2(t)) = (w⊙2(t)+˙ w⊙2(t)) by (14)
k q + −
and (15).
By taking the integral on both sides of (14) and (15) from 0 to t and combining them with (10)
and (13) respectively, we have
n
1
β˙(t) = ℓ′(β(t))diag(X⊤Σ(g (β(t)))γ z⊤)
−n i i i i i
i=1
X
t 4 n
ℓ′(β(τ))diag(X⊤Σ(g (β(τ)))γ z⊤) β(τ)dτ +ξ⊙2(0)+ξ⊙2(0) . (16)
⊙ −n i i i i i ⊙ k q
( Z0
i=1
)
X
n
1
w˙(t) = ℓ′(w(t))diag(X⊤Σ(g (w(t)))γ z⊤)
−n i i i i i
i=1
X
t 4 n
ℓ′(w(τ))diag(X⊤Σ(g (w(τ)))γ z⊤) w(τ)dτ +w⊙2(0)+w⊙2(0) . (17)
⊙ −n i i i i i ⊙ + −
( Z0
i=1
)
X
By (16) and (17), the dynamics of β and w are equivalent when initialized as follows:
β(0) = w(0), ξ⊙2(0)+ξ⊙2(0) = w⊙2(0)+w⊙2(0). (18)
k q + −
A.3 Gradient flow dynamics of β
In this section, we present the dynamics of w with initialization satisfying (18) which is equivalent
to the dynamics of β by Lemma 4.1. For w(0) = 0, it is equivalent to w (0) = w (0). We use the
+ −
w⊙2−w⊙2
notation β = w = + − for simplicity.
2
Lemma A.1. For all t 0, we have
≥
β(t) = ω⊙2 sinh(Bφ(t)), (19)
0 ⊙
where w := w (0) = w (0) Rd is an initialization such that w = 0 for all j [d], B
0 + − 0,j
∈ 6 ∈ ∈
Rd×n(L−1) is a matrix with columns B := (x x ) z , and φ Rn(L−1) is a vector such
il i,opt(i) il i
− ⊙ ∈
that its entries are expressed as
2 t
φ (t) = exp γ⊤σ(g (τ)) (Σ(g (τ))γ ) dτ
il −n − i i i i l
Z0
(cid:16) (cid:17)
20for all i [n] and l = opt(i). Also, we have
∈ 6
n
2
β˙(t) = β⊙2(t)+w⊙4 exp γ⊤σ(g (t)) (Σ(g (t))γ ) B .
n 0 ⊙ − − i i i i l il 
q Xi=1l6= Xopt(i) (cid:16) (cid:17)
 
Proof. For simplicity, we consider the initialization w = α1, where α R and 1 Rd denotes
0
∈ ∈
all one vector. For any initializations that satisfy (18), the dynamics can be derived similarly. Let
g (t)= X (β(t) z ). By taking the integral on both sides of (11) and (12) from 0 to t, we have
i i i
⊙
1 t n
w (t) = w (0) exp exp γ⊤σ(g (τ)) diag(X⊤Σ(g (τ))γ z⊤)dτ ,
+ + ⊙ n − i i i i i i
Z0
Xi=1 (cid:16) (cid:17)
!
1 t n
w (t) = w (0) exp exp γ⊤σ(g (τ)) diag(X⊤Σ(g (τ))γ z⊤)dτ .
− − ⊙ −n − i i i i i i
Z0
Xi=1 (cid:16) (cid:17)
!
With w (0) = α1 and w (0) = α1, we derive the dynamics of β(t) as follows:
+ −
w⊙2(t) w⊙2(t)
β(t) = + − −
2
2 t n
= α2sinh exp γ⊤σ(g (τ)) diag(X⊤Σ(g (τ))γ z⊤)dτ . (20)
n − i i i i i i
Z0
Xi=1 (cid:16) (cid:17)
!
Note that
L
diag(X⊤Σ(g )γ z⊤) = z X⊤Σ(g )γ = (z x )(Σ(g )γ ) , (21)
i i i i i ⊙ i i i i ⊙ il i i l
(cid:16) (cid:17) Xl=1
and
(Σ(g )γ ) = σ (g ) γ σ(g )⊤γ . (22)
i i l l i il i i
−
(cid:16) (cid:17)
Since 1⊤Σ(g ) = 0, we have L (Σ(g )γ ) = 1⊤Σ(g )γ = 0. Hence, we have
i l=1 i i l i i
P
(Σ(g )γ ) = (Σ(g )γ ) . (23)
i i opt(i) − i i l
l6=opt(i)
X
By (21) and (23), we can substitute x to each x in (21) and have for (20) that
i,opt(i) il
2 t n
β(t) = α2sinh exp γ⊤σ(g (τ)) (Σ(g (τ))γ ) z x z x dτ
−n − i i i i l i ⊙ i,opt(i) − i ⊙ il 
Z0
Xi=1l6= Xopt(i) (cid:16) (cid:17)
(cid:0) (cid:1)
 
2 t n ∞
= α2sinh exp γ⊤σ(g (τ)) (Σ(g (τ))γ ) B dτ . (24)
−n − i i i i l il 
Z0
Xi=1l6= Xopt(i) (cid:16) (cid:17)
=
α2sinh
(Bφ(t)),

21where we recall B := z (x x ). The derivative of β(t) can be written as
il i i,opt(i) il
⊙ −
2α2 2 t n
β˙(t) = cosh exp γ⊤σ(g (τ)) (Σ(g (τ))γ ) B dτ
n −n − i i i i l il 
Z0
Xi=1l6= Xopt(i) (cid:16) (cid:17)
 
n
exp γ⊤σ(g (t)) (Σ(g (t))γ ) B , (25)
⊙− − i i i i l il 
Xi=1l6= Xopt(i) (cid:16) (cid:17)
 
and by (24), we have
β(t) 2 t n
arcsinh = exp γ⊤σ(g (τ)) (Σ(g (τ))γ ) B dτ. (26)
α2 −n − i i i i l il
(cid:18) (cid:19) Z0 Xi=1l6= Xopt(i) (cid:16) (cid:17)
By substituting (26) into (25), we obtain
2α2 β(t) n
β˙(t) = cosh arcsinh exp γ⊤σ(g (t)) (Σ(g (t))γ ) B
n α2 ⊙− − i i i i l il 
(cid:18) (cid:18) (cid:19)(cid:19) Xi=1l6= Xopt(i) (cid:16) (cid:17)
 
n
2
= β⊙2(t)+α41 exp γ⊤σ(g (t)) (Σ(g (t))γ ) B ,
n ⊙− − i i i i l il 
p Xi=1l6= Xopt(i) (cid:16) (cid:17)
 
where the last inequality follows from that cosh(arcsinh(x)) = √x2+1.
A.4 Proof of Lemma 4.2
Lemma 4.2. Under Assumption 2, suppose the initialization ξ (0) and ξ (0) satisfy (5) for the
k q
dynamics of ξ (t) and ξ (t) in Lemma 4.1. Then lim β(t) = lim ξ (t) ξ (t) = .
k q t→∞ 2 t→∞ k q 2
k k k ⊙ k ∞
Proof. By Lemma 4.1, it suffices to consider the dynamics of w(t), denoted by β(t), for simplicity.
The gradient of the loss function (β) with respect to β is given by
L
n
1
(β) = exp γ⊤σ(g ) (Σ(g )γ ) B .
∇β L n − i i i i l il
Xi=1l6= Xopt(i) (cid:16) (cid:17)
Then, by the chain rule and the description of β˙(t) from Lemma (A.1), we have
d dβ(t) 2
L = (β(t)), = (β(t)), β⊙2(t)+w⊙4 (β(t))
dt ∇β L dt − ∇β L n 0 ⊙∇β L
(cid:28) (cid:29) (cid:28) q (cid:29)
2w2
min (β(t)) 2, (27)
≤ − n k∇β L k2
where w = min w . Taking the integral on both sides, we have
min i∈[d] 0,i
| |
∞ 2w2
(β(0)) lim (β(t)) min (β(t)) 2dt. (28)
L −t→∞L ≥ n k∇β L k2
Z0
Since lim (β(t)) 1 n exp( γ ) > 0, the left-hand side of (28) is bounded above
t→∞ L ≥ n i=1 − i,opt(i)
by (β(0)) and the integral on the right-hand side goes to a bounded positive constant by the
L P
22monotone convergence theorem. It implies that either (β(t)) 2 = 0 for some finite β(t) or
k∇β L k2
lim (β(t)) 2 = 0.
t→∞ k∇β L k2
First, we show that (β(t)) = 0 for any finite β(t). Under Assumption 2, for any finite
β 2
k∇ L k 6
time t and all l = opt(i),i [n], it holds that
6 ∈
(Σ(g (t))γ ) = σ(g (t)) γ σ(g (t))σ(g (t))⊤γ
i i l i ⊙ i − i i i l
= (cid:16) σ(g (t)) (σ(g (t))⊤γ γ ) (cid:17)
i l i i i,nopt
− −
= σ(g (t)) σ(g (t)) (γ γ ) < 0 (29)
i l i opt(i) i,opt(i) i,nopt
− −
Let β be a feasible solution of (ℓ -SVM) (i.e, β⊤B 1, l = opt(i),i [n]). By (29), it holds
f 1 f il ≥ ∀ 6 ∈
for any finite β(t) that
n
1
(β(t)),β = exp γ⊤σ(g ) (Σ(g )γ ) B ,β
h∇β L f i n − i i i i l il f
* +
Xi=1l6= Xopt(i) (cid:16) (cid:17)
n
1
exp γ⊤σ(g ) (Σ(g )γ ) < 0.
≤ n − i i i i l
Xi=1l6= Xopt(i) (cid:16) (cid:17)
Itimpliesthat (β(t)) = 0foranyfiniteβ(t). Hence,itholdsthatlim (β(t)) 2 = 0.
k∇β L k2 6 t→∞ k∇β L k2
In addition, we have
β (β(t)) (β(t)),β
−k f k2k∇β L k2 ≤ h∇β L f i
n
1
exp γ⊤σ(g ) (1 σ(g ) )(σ(g ) )(γ γ )< 0.
≤ −n − i i − i opt(i) i opt(i) i,opt(i) − i,nopt
Xi=1 (cid:16) (cid:17)
It follows from lim (β(t)),β = 0 that lim (1 σ(g ) )σ(g ) = 0 , i [n],
t→∞ β f t→∞ i opt(i) i opt(i)
h∇ L i − ∀ ∈
which cannot be achieved with any finite β(t) . Therefore, lim β(t) = .
2 t→∞ 2
k k k k ∞
A.5 Proof of Theorem 4.1
We recall Theorem 4.1:
Theorem 4.1. Suppose d d, and consider the gradient flow dynamics of ξ (t) Rd and
e k
≥ ∈
ξ (t) Rd in (2) with initialization satisfying (5). Under Assumptions 2 and 3, suppose the
q
p∈ p
limit β∞ := lim β(t) exists. Let K(t) = diag(ξ (t)) and Q(t) = diag(ξ (t)), correspondingly
t→∞ k q
W(t)= diag(β(t)). Then Wx ∞ = diag(βp ∞) is a global solution of (Wdiag -SVM).
∗
Note that (ℓ -SVM) is a convex problem and the KKT conditions are sufficient for global opti-
1
mality. We show that wp satisfies the KKT conditions of (ℓ -SVM). In particular, we analyze the
1
dynamics of w(t) and exploit the concept of the approximate KKT point, which is described in the
following subsection.
Approximate KKT conditions
We introduce the local Lipshitz functions and the Clarke subdifferential. A function f : Rd R is
→
locally Lipshitz if forany x Rd, there exists aneighborhood U(x) of x such that f is Lipshitz
U(x)
∈ |
23continuous. A locally Lipshitz function is differentiable almost everywhere. The non-differentiable
points of locally Lipshitz functions can be characterized by the Clarke subdifferential:
∂f(x), conv lim f(x ) lim x = x, f(x ) exists ,
n n n
{n→∞∇ |n→∞ ∇ }
where conv denotes the convex hull.
Consider the optimization problem:
min f(β) (30)
β∈Rd
s.t. g (β) 0 i [m],
i
≤ ∀ ∈
where f,g ,...,g :Rd R are locally Lipshitz functions.
1 m
→
Definition A.1 (KKT point). A point β is a KKT point of (30) if the following conditions are
satisfied. There exists λ ,...,λ 0 such that
1 n
≥
n
0 ∂f(β)+ λ ∂g (β),
i i
∈
i=1
X
g (β) 0 i [n],
i
≤ ∀ ∈
λ g (β) = 0 i [n].
i i
∀ ∈
Anylocalminimizerofproblem(30)isaKKTpointwhentheregularityconditionissatisfied. We
consideroneoftheregularityconditions, Mangasarian-Fromovitz ConstraintQualification(MFCQ),
which is defined as follows.
Definition A.2 (Mangasarian–Fromovitz constraint qualification (MFCQ) ). Consider a feasible
point β of (30). The MFCQ is satisfied at β if there exists v Rd such that for all i [n] such that
∈ ∈
g (β) = 0,
i
s,v > 0 s ∂g (β).
i
h i ∀ ∈
Now,weintroducetheconceptofapproximateKKTpointsandarelatedtheoremin(Dutta et al.,
2013).
Definition A.3 (Approximate KKT point). For ǫ,ǫ′,δ > 0, a point β is an (ǫ,ǫ′,δ)-KKT point of
q q q q
(30) if there exists β such that β β ǫ′ and there exists λ 0,I ∂f(β), s ∂g (β) for all
2 i i i
k − k ≤ ≥ ∈ ∈
i [n] such that
∈
n
q
I + λ s (β) ǫ
i i
(cid:13) (cid:13) ≤
(cid:13) Xi=1 (cid:13)2
(cid:13) (cid:13)
g(cid:13)i(β) 0 i (cid:13)[n],
(cid:13) ≤ ∀ ∈(cid:13)
λ g (β) δ i [n].
i i
≥ − ∀ ∈
The following theorem is a corollary of Theorem 3.6 in (Dutta et al., 2013). It shows that
(ǫ,ǫ′,δ)-KKT points can converge to a KKT point.
Theorem A.2. (Dutta et al.,2013) Let ǫ , ǫ′ and δ be sequences such that ǫ 0, ǫ′ 0,
{ k } { k} { k } k → k →
and δ 0. Suppose that β is an (ǫ ,ǫ′,δ )-KKT point of (30) for every k. Let MFCQ hold at β.
k → k k k k
If β β as k + , then β is a KKT point of (30).
k
→ → ∞
24Applying Theorem A.2 to our problem (ℓ -SVM)
1
Now, we prove Theorem 4.1. Inthis proof, thenotation is simplified by denoting w as β. Inessence,
for large enough t, we define βp (t) , β(t) and demonstrate that βp (1),...,βp (k),... is a sequence of
µ(t)
approximate KKT points of (ℓ -SVM) with some ǫ ,ǫ′,δ converging to zero. Therefore, if the
p 1 p { k k k }
limit β∞ exists, we conclude that β∞ is a KKT point and hence a global solution to (ℓ -SVM),
1
satisfying the MFCQ conditions.
p
Proof. We show that β(t) is an (ǫ(t),ǫ′(t),δ(t))-KKT point of (ℓ -SVM) satisfying the following
1
conditions: for all t > t′, βq (t) such that βp (t) βq (t) ǫ′(t), λ(t) Rn and I(t) ∂ βq (t)
∃ k − k2 ≤ ∈ ≥0 ∈ k k1
satisfying
(C1). I(t) n λ B ǫ(t),
k − i=1 il il k2 ≤
(C2). 1 βp (t)P⊤B 0 i [n], l = opt(i),
il
− ≤ ∀ ∈ 6
p
(C3). λ (1 β(t)⊤B ) δ(t) i [n], l = opt(i).
il il
− ≥ − ∀ ∈ 6
p p
We simplify β(t) as follows before we prove the conditions. By Lemma A.1, β(t) can be written as
p β(t) sinh(Bφ(t))
β(t) = = w⊙2
µ(t) 0 ⊙ µ(t)
Bφ(t) Bφ(t)
exp lnµ(t) exp lnµ(t)
= w⊙2 lnµ(t) − − lnµ(t)
0 ⊙ (cid:16) (cid:16) (cid:17)2µ(t) (cid:16) (cid:17)(cid:17)
Bφ(t) −Bφ(t)
µ(t)lnµ(t) µ(t) lnµ(t)
−
= w⊙2 (cid:18) (cid:19)
0 ⊙ 2µ(t)
=
w 0⊙2 µ(t)lB nφ µ( (t t) )−1 µ(t)− lB nφ µ( (t t) )−1
, (31)
2 ⊙ −
(cid:18) (cid:19)
wherew := w (0) = w (0) Rdisaninitializationsuchthatω = 0forallj [d],B Rd×n(L−1)
0 + − 0,j
∈ 6 ∈ ∈
isamatrixwithcolumnsB ,andφ Rn(L−1)isavectorwithentriesφ (t)= 2 t exp γ⊤σ(g (τ)) (Σ(g (τ))γ )
p il ∈ il −n 0 − i i i i
We show that β(t) satisfies conditions (C1), (C2) and (C3) for large enough t.
R (cid:0) (cid:1)
KKT Condition (C2).
Recall that
n
1
(β(t)) = exp γ⊤σ(g (t)) (Σ(g (t))γ ) B .
∇β L n − i i i i l il
Xi=1l6= Xopt(i) (cid:16) (cid:17)
Notice that exp( γ⊤σ(g )) exp( γ ) and lim (β(t)) = 0 by Lemma 4.2. Then,
− i i ≥ − i,opt(i) t→∞ k∇β L k2
for i,l =opt(i), it holds that lim ( Σ(g (t))γ ) = 0. By (22), it implies that either
∀ 6 t→∞ − i i l
lim σ(g (t)) = 0 or lim σ(g (t)) = 0 (32)
i l i opt(i)
t→∞ t→∞
should be satisfied for all l = opt(i) and i [n].
6 ∈
25Under Assumptions 2 and 3, for all i [n], we show that
∈
lim σ(g (t)) = 1,
i opt(i)
t→∞
which is equivalent to lim σ(g (t)) = 0 for all l = opt(i) and i [n].
t→∞ i l 6 ∈
Weprovetheabovebycontradiction. Supposethereexistsj [n]suchthatlim σ(g (t)) =
∈ t→∞ j opt(i) 6
1, which is equivalent to
lim σ(g (t)) = 0 (33)
j opt(i)
t→∞
by (32). Consider the loss:
n
1
lim (β(t)) = lim exp γ⊤σ(g (t))
t→∞L t→∞ n − i i
Xi=1 (cid:16) (cid:17)
n
(i) 1
= lim exp γ γ σ(g (t)) γ
t→∞ n − i,opt(i) − i,nopt i opt(i)− i,nopt
Xi=1 (cid:16)
(cid:0) (cid:1)
(cid:17)
exp( γ )+ exp γ
− i,nopt i6=j − i,opt(i)
≥ n
P (cid:0) (cid:1)
exp( γ )+ exp γ
min − j,nopt i6=j i,opt(i) ,
≥ j∈[n] Pn (cid:0) (cid:1)
where(i)followsfromγ = γ and L σ (g )= 1.ByAssumption3,wehavelim (β(t))
il i,nopt l=1 l i t→∞ L ≥
(β(0)). However, this contradicts to (27) that the loss decreases.
L P
By (33), it holds that for any i, l = opt(i),
6
σ(g (t))
lim i opt(i) = lim exp β(t)⊤ (x x ) z = ,
i,opt(i) il i
t→∞ σ(g i(t))
l
t→∞ − ⊙ ∞
(cid:16) (cid:17)
(cid:0) (cid:1)
equivalently,
lim β(t)⊤ x x z = lim β(t)⊤B = .
i,opt(i) il i il
t→∞ − ⊙ t→∞ ∞
(cid:0)(cid:0) (cid:1) (cid:1)
Since lim B⊤β(t) = for all l = opt(i) and i [n], we have lim µ(t) = and for large
t→∞ il ∞ 6 ∈ t→∞ ∞
enough t,
β(t)⊤B
il
min i,l6=opt(i)β(t)⊤B
il
= 1. (34)
µ(t) ≥ µ(t)
p p
It follows from(34) that β is a feasible solution to (ℓ -SVM). Moreover, for large enough t, β(t) is
1
a feasible point of (ℓ -SVM).
1
KKT Condition (C3).
We construct the dual variable of βp (t) as λ(t) Rn given by
∈ ≥0
2 t 1
λ (t) := − exp γ⊤σ(g (τ)) (Σ(g (τ))γ ) dτ = φ (t).
il nlnµ(t) − i i i i l lnµ(t) il
Z0
(cid:16) (cid:17)
26By (29), we have (Σ(g )γ ) > 0 for all i,l = opt(i). Hence φ (t) > 0 and λ (t) 0 for all
− i i l 6 il il ≥
finite time t. Next, we show that this construction gives us condition (C3). To do so, we define the
p
support vector index set for β∞ as
p
Γ = (i,l) [n] [L] l = opt(i), B⊤β∞ = 1 ,
{ ∈ × | 6 il
o
and the non-support vector index set as
p
Γc = (i,l) [n] [L] l =opt(i), B⊤β∞ > 1 .
∈ × | 6 il
n o
To prove condition (C3), we show that λ (t) is bounded above for all (i,l) Γ Γc, and
il
∈ ∪
limsup λ (t) = 0 (i,l) Γc.
t→∞ il ∀ ∈
In particular, for the upper bound λ (t), we derive a lower bound for lnµ(t) and an upper bound
il
for φ . Since exp γ⊤σ(g (t)) and σ(g (t)) are bounded above by 1,
il − i i i opt(i)
(cid:0) (cid:1)
2 γ γ t
i,opt(i) i,nopt
φ (t) − σ(g (τ)) dτ. (35)
il ≤ n i l
(cid:0) (cid:1) Z0
p
For the lower bound for lnµ(t), we note that the existence of limit point β∞ implies that µ(t) and
β(t) have the same order of t (i.e., µ(t) β(t) ). Hence, there exists a large enough t such
2 2 0
k k ≍ k k
that for t t ,
0
≥
lnµ(t) C ln β(t)
1 2
≥ k k
holds for some constant C > 0. It suffices to derive a lower bound for ln β(t) . β(t) can be
1 2 2
k k k k
bounded from below as follows:
1
β(t) = w⊙2(t) w⊙2(t)
k k2 2 + − − 2
(cid:13) 2 t n (cid:13)
c (cid:13)exp (cid:13) ℓ (τ)(γ γ )σ(g (τ)) σ(g (τ)) B dτ)
i i,opt(i) i,nopt i opt(i) i l il
≥ n −
(cid:13) (cid:18) Z0 i=1l6=opt(i)
(cid:13) X X
(cid:13) (cid:13) 2 t n
exp ℓ (τ)(γ γ )σ(g (τ)) σ(g (τ)) B dτ
i i,opt(i) i,nopt i opt(i) i l il
− − n −
(cid:18) Z0 i=1l6=opt(i) (cid:19)(cid:13)2
X X (cid:13)
(cid:13)
(cid:13)
(iii) 2 t n
C exp ℓ (τ)(γ γ )σ(g (τ)) σ(g (τ)) B dτ
i i,opt(i) i,nopt i opt(i) i l il
≥ n −
(cid:13) (cid:18)(cid:12) Z0 i=1l6=opt(i) (cid:12)(cid:13)2
(cid:13) (cid:12) X X (cid:12)(cid:13)
(cid:13) (cid:13)d (cid:12) (cid:12) 4 t n (cid:12) (cid:12)(cid:13) (cid:13) 1 2
= C exp ℓ (τ)(γ γ )σ(g (τ)) σ(g (τ)) B dτ ,
i i,opt(i) i,nopt i opt(i) i l il
n −
(cid:18)j=1 (cid:18)(cid:12) Z0 i=1l6=opt(i) (cid:12)j(cid:19)(cid:19)
X (cid:12) X X (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
where c > 0 and C > 0 are constants, and ℓ (τ) = exp γ⊤σ(g (τ)) . Inequality (iii) follows from
i − i i
that ex e−x ce|x| for some c >0 and sufficiently large x . Taking the logarithm on both sides,
| − |≥ (cid:0) | | (cid:1)
27we have
ln β(t)
2
k k
1 d 4 t n
ln exp ℓ (τ) γ γ σ(g (τ)) σ(g (τ)) B dτ +lnC
≥ 2  (cid:12)n i i,opt(i) − i,nopt i opt(i) i l il (cid:12) 
j=1 (cid:12) Z0 i=1l6=opt(i) (cid:12)
X (cid:12) X X (cid:0) (cid:1) (cid:12)j
 (cid:12) (cid:12) 
1 1 d (cid:12) (cid:12) 4 t n (cid:12) (cid:12)
= ln d exp ℓ (τ) γ γ σ(g (τ)) σ(g (τ)) B dτ +lnC
2  · d (cid:12)n i i,opt(i) − i,nopt i opt(i) i l il (cid:12) 
j=1 (cid:12) Z0 i=1l6=opt(i) (cid:12)
X (cid:12) X X (cid:0) (cid:1) (cid:12)j
 (cid:12) (cid:12) 
(iv) 1 d 2 t n (cid:12) (cid:12) 1 (cid:12) (cid:12)
ℓ (τ) γ γ σ(g (τ)) σ(g (τ)) B dτ + lnd+lnC
≥ 2d (cid:12)n i i,opt(i) − i,nopt i opt(i) i l il (cid:12) 2
j=1(cid:12) Z0 i=1l6=opt(i) (cid:12)
X(cid:12) X X (cid:0) (cid:1) (cid:12)j
(cid:12) (cid:12)
(v) 1 4(cid:12) (cid:12) t n (cid:12) (cid:12) 1
ℓ (τ) γ γ σ(g (τ)) σ(g (τ)) B dτ + lnC2d
≥ 2d (cid:13)n i i,opt(i) − i,nopt i opt(i) i l il (cid:13) 2
(cid:13) Z0 i=1l6=opt(i) (cid:13)
(cid:13) X X (cid:0) (cid:1) (cid:13)2
= 2 max(cid:13) (cid:13) (cid:13) t β (β(τ))dτ,a + 1 lnC2d (cid:13) (cid:13) (cid:13)
d kak2=1 (cid:28)Z0 ∇ L (cid:29) 2
(vi) 2 t s 1
max (β(τ)),β dτ + lnC2d
β f
≥ d ∇ L 2
Z0
2 t n (cid:10) (cid:11) 1
= ℓ (τ) γ γ σ(g (τ)) σ(g (τ)) m dτ + lnC2d
nd i i,opt(i) − i,nopt i opt(i) i l il 2
Z0
i=1l6=opt(i)
X X (cid:0) (cid:1)
(vii) t n 1
C (t) σ(g (τ)) dτ + lnC2d.
≥ 2 i l 2
Z0
i=1l6=opt(i)
X X
Inequality (iv) follows from the Jensen’s inequality, and (v) follows from x x . For (vi),
1 2
s s s k k ≥ k k
we choose β such that β = 1, β⊤B = m > 0 for all i,l = opt(i). For (vii), we choose
f k f k2 f il il 6
C (t) = min 2 ℓ (τ) γ γ σ(g (τ)) m .
2 i,l,τ∈[0,t] nd i i,opt(i) − i,nopt i opt(i) il
Since ℓ i(t) and σn(g i(t))
op(cid:0)t(i)
are bounded (cid:1)below by some poositive constant for any t, we have
lim C (t)= C > 0. Hence, we have
t→∞ 2 2
t n
ln β(t) C σ(g (τ)) dτ (36)
k k2 ≥ i l
Z0
i=1l6=opt(i)
X X
for sufficiently large t and some constant C > 0. Combining (35) and (36), for all i n and
∈
l = opt(i), we have
6
t
φ (t) σ(g (τ)) dτ
λ (t) = il C 0 i l
il lnµ(t) ≤ t n σ(g (τ)) dτ
0 i=1R l6=opt(i) i l
for sufficiently large t and some constant C >R 0P. P
Note that lim t n σ(g (τ)) dτ goes to infinity by lim β(t) = and
t→∞ 0 i=1 l6=opt(i) i l t→∞ k k2 ∞
p p
(24). Moreover, it follows from β(t)⊤B β∞(t)⊤B that there exists time t such that for t > t ,
R P P il il 0 0
βp (t)⊤B > 1 holds for all (i,l) Γc. → Then, we have lim σ(g i′(t)) l′ = (i.e., σ(g (t)) =
il ∈ t→∞ σ(gi(t)) l ∞ i l
o(σ(g (t)) )) for all (i,l) Γc and (i′,l′) Γ.
i′ l′
∈ ∈
28It implies that
t
σ(g (τ)) dτ
limsup 0 i l = 0 (i,l) Γc.
t→∞ 0t n i=1R l6=opt(i)σ(g i(τ)) ldτ ∀ ∈
Consequently, limsup λR (tP ) = 0 P for all (i,l) Γc.
t→∞ il ∈
KKT Condition (C1).
q p q
We prove the condition (C1) by constructing β(t) such that it satisfies β(t) β(t) ǫ′(t)
2
q p p k − q k ≤
and the condition (C1). Let β (t) = β (t) for all j j [d] β∞ = 0 and β (t) = 0 for
j j ∈ { ∈ | j 6 } j
p p p
j j [d] β∞ = 0 . Since β(t) β∞, we have ǫ′(t) 0 as t . We prove that ǫ(t)
∈ { ∈ | j } → → → ∞
converges to 0. As derived in (31), we have
βp
(t) =
w 0⊙2 µ(t)lB nφ µ( (t t) )−1 µ(t)− lB nφ µ( (t t) )−1
2 ⊙ −
(cid:18) (cid:19)
w⊙2
= 0 µ(t)Bλ(t)−1 µ(t)−Bλ(t)−1 .
2 ⊙ −
(cid:16) (cid:17)
p
Consider the index j j [d] β∞ = C > 0 . Let a (t) := (Bλ(t)) and b (t) := µ(t)aj(t).
∈ { ∈ | j j } j j j
q
The jth component of β(t) can be written as
w2(0)
βq (t) = βp (t)= j b (t)µ(t)−1 b (t)−1µ(t)−1 .
j j j j
2 −
(cid:0) (cid:1)
Equivalently,
p
2β (t)
b (t)µ(t)−1 b (t)−1µ(t)−1 = j := C (t), (37)
j − j w2(0) j
j
where lim C (t) = 2C /w2(0). If we represent (37) in terms of b (t),
t→∞ j j j j
C (t)µ(t)+ C2(t)µ2(t)+4
j j
b (t) = .
j q2
Taking the logarithm on both sides, we obtain
ln(C (t)µ(t)+ C2(t)µ2(t)+4) ln2
j j −
a (t) = .
j qlnµ(t)
Since lim µ(t)= , we have lim a (t) = 1. Note that for sufficiently large t > t , C (t) > 0
t→∞ t→∞ j 0 j
q ∞
and ∂ β(t) = 1. Then, it holds that
1
k k
q
lim ǫ (t)= lim a (t) ∂ β(t) = lim a (t) 1 = 0.
j j 1 j
t→∞ t→∞ − k k t→∞| − |
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
29p
Similarly, consider the index j j [d] β∞ = C < 0 . We have the following:
∈ { ∈ | j − j }
q
2β (t)
b (t)µ(t)−1 b (t)−1µ(t)−1 = j := C (t),
j − j w2(0) − j
j
C (t)µ(t)+ C2(t)µ2(t)+4
− j j 2
b (t) = = ,
j q2
C (t)µ(t)+ C2(t)µ2(t)+4
j j
q
ln C (t)µ(t)+ C2(t)µ2(t)+4 ln2
j j −
a (t) = .
j − (cid:16) qlnµ(t) (cid:17)
Then, it follows that lim a (t) = 1 and lim ǫ (t) = 0.
t→∞ j t→∞ j
− p q p
Finally, consider the index j j [d] β∞ = 0 . Since β (t) = 0 and lim β (t) =
∈ { ∈ | j } j t→∞ j
w2(0)
lim j µ(t)(Bλ(t))j−1 µ(t)−(Bλ(t))j−1 =0,itimplies thatforsufficiently large t,(Bλ(t))
t→∞ 2 q − j ∈
[ 1,1] = ∂ β(t) . Consequently, we have
− k (cid:0) k1 (cid:1)
q
lim ǫ (t) = lim a (t) ∂ β(t) = 0.
j j 1
t→∞ t→∞ − k k
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
A.6 Proof of Lemma 5.1
For reader’s convenience, we restate Lemma 5.1 here.
Lemma 5.1 (Gradient flow preserves alignment). Under Assumption 4, suppose the initialization
of the gradient flow satisfies the alignment property with respect to (X ,z ,y ) n in the sense of
{ i i i }i=1
Definition 5.1, that is,
K(0) = U Λ (0)V⊤, Q(0) = U Λ (0)V⊤.
K K Q Q
Then for all t 0, K(t) and Q(t) given by the gradient flow (1) can be decomposed into
≥
K(t)= U Λ (t)V⊤, Q(t) = U Λ (t)V⊤.
K K Q Q
Proof. For ease of presentation, we consider the case d = d . The same analysis can be applied to
e
the case d d . The gradient flow dynamics of K and Q are given by
e
≥
n
1
K˙(t) = (K(t),Q(t)) = exp γ⊤σ(g (t)) X⊤Σ(g (t))γ z⊤Q(t),
−∇K L n − i i i i i i
Xi=1 (cid:16) (cid:17)
n
1
Q˙(t) = (K(t),Q(t)) = exp γ⊤σ(g (t)) z γ⊤Σ(g (t))X K(t).
−∇Q L n − i i i i i i
Xi=1 (cid:16) (cid:17)
We denote a vector ψ , X⊤Σ(g )γ Rd. Then, it can be expressed as
i i i i ∈
L
ψ = x (Σ(g )γ )
i il i i l
l=1
X
(a)
= (x x )(Σ(g )γ )
il i,opt(i) i i l
−
l6=opt(i)
X
(b)
= (x x )σ(g ) σ(g ) (γ γ ), (38)
i,opt(i) − il i opt(i) i l i,opt(i) − i,nopt
l6=opt(i)
X
30where (a) follows from (23). Equality (b) follows from (29). Suppose that K and Q satisfy the
alignment property with data (X ,z ,y ) n . By Assumption 4 (iii), for each x , there exists a
{ i i i }i=1 il
x such that
il′
x⊤Wz = x⊤U Λ Λ U⊤z
il i il K K Q Q i
= x⊤U Λ Λ e
il K K Q π(i)
= x⊤uk diag(Λ Λ )
il π(i) K Q π(i)
x
= x⊤ i,opt(i) diag(Λ Λ )
il x K Q π(i)
i,opt(i) 2
k k
x
= x⊤ i,opt(i) diag(Λ Λ )
il′
x
K Q π(i)
i,opt(i) 2
k k
= x⊤Wz .
il′ i
Consequently, for paired x and x , we have
il il′
σ(g ) = σ(g ) . (39)
i l i l′
Together with (38), (39) and Assumption 4 (iii), it follows that for all i,
ψ x
i = i,opt(i) = uk . (40)
ψ x π(i)
i 2 i,opt(i) 2
k k k k
Observe the dynamics of K and Q as follows:
n
1
K˙ = exp γ⊤σ(g ) ψ z⊤Q
n − i i i i
Xi=1 (cid:16) (cid:17)
n
1
= exp γ⊤σ(g ) ψ z⊤U Λ V⊤
n − i i i i Q Q
Xi=1 (cid:16) (cid:17)
n
1
= exp γ⊤σ(g ) uk e⊤ ψ z Λ V⊤
n − i i π(i) π(i)k i k2 k i k2 Q
Xi=1 (cid:16) (cid:17)
n
1
= exp γ⊤σ(g ) ψ z diag(Λ ) uk v ⊤,
n − i i k i k2 k i k2 Q π(i) π(i) π(i)
Xi=1 (cid:16) (cid:17)
where e Rd is a vector with 1 at the ith component and 0 elsewhere.
i
∈
n
1
Q˙ = exp γ⊤σ(g ) z ψ⊤K
n − i i i i
Xi=1 (cid:16) (cid:17)
n
1
= exp γ⊤σ(g ) z ψ⊤U Λ V⊤
n − i i i i K K
Xi=1 (cid:16) (cid:17)
n
1
= exp γ⊤σ(g ) uq e⊤ ψ z Λ V⊤
n − i i π(i) π(i)k i k2 k i k2 K
Xi=1 (cid:16) (cid:17)
n
1
= exp γ⊤σ(g ) ψ z diag(Λ ) uq v ⊤.
n − i i k i k2 k i k2 K π(i) π(i) π(i)
Xi=1 (cid:16) (cid:17)
31The dynamics of K and Q do not alter the column spaces of U , U and V, respectively when
K Q
K and Q satisfy the alignment property with data (X ,z ,y ) n . In particular, consider the
initialization K(0) = U (0)Λ V(0)⊤ and Q(0) = U{ (0)i Λ i V(i 0)} ⊤i=1 such that xi,opt(i) = uk is
K K Q Q kxi,opt(i)k2 π(i)
the π(i)-th column of U (0) and zi = uq is the π(i)-th column of U (0) for all i [n] and the
K kzik2 π(i) Q ∈
restofthecolumnsofU (0)andU (0)arethezerovectors. Then, onlyΛ andΛ willbeupdated
K Q K Q
during the training, which leads to K(t) = U (0)Λ (t)V(0)⊤ and Q(t) = U (0)Λ (t)V(0)⊤ for all
K K Q Q
time t.
A.7 Proof of Theorem 5.1
We recall Theorem 5.1:
Theorem 5.1. Under Assumptions 2, 3, and 4, suppose that the limit Wx ∞ , lim Wx (t) exists.
t→∞
Assume that the initialization of the gradient flow satisfies the alignment property with respect to
(X ,z ,y ) n in the sense of Definition 5.1, that is,
{ i i i }i=1
K(0) = U Λ (0)V⊤, Q(0) = U Λ (0)V⊤,
K K Q Q
where d d. Let Λ (t) = diag(ξ (t)), Λ (t) = diag(ξ (t)) and β(t) = ξ (t) ξ (t) such that
e K k Q q k q
≥ x p ⊙
(ξ (0),ξ (0)) satisfy (5). Then, the gradient flow limit W∞ = U diag(β∞)U⊤ is a global optimal
k q K Q
solution of (W -SVM).
∗
Proof. Under the conditions of Lemma 5.1, it suffices to consider the dynamics of singular values
Λ and Λ . Similarly to our previous discussion, let
K Q
ξ = diag(Λ ) Rd,
k K
∈
ξ = diag(Λ ) Rd,
q Q
∈
β = ξ ξ Rd.
k q
⊙ ∈
The gradient evaluated at ξ and ξ are
k q
n
1
ξ˙ (t)= (K,Q) = exp γ⊤σ(g (t)) ψ (t) z e ξ (t),
k −∇ξkL n − i i k i k2 k i k2 π(i) ⊙ q
Xi=1 (cid:16) (cid:17)
n
1
ξ˙ (t) = (K,Q) = exp γ⊤σ(g (t)) ψ (t) z e ξ (t),
q −∇ξqL n − i i k i k2 k i k2 π(i) ⊙ k
Xi=1 (cid:16) (cid:17)
where ψ (t) = X⊤Σ(g (t))γ Rd and e Rd is a vector with 1 at the ith component and 0
i i i i ∈ i ∈
elsewhere.
w⊙2−w⊙2
Considerthedynamicsofthealternativereparameterization β = + − withtheinitialization
2
w = w (0) = w (0) that satisfies the conditions of Lemma 4.1. The corresponding dynamics of
0 + −
w and w are
+ −
n
1
w˙ (t) = exp γ⊤σ(g (t)) ψ (t) z e w (t),
+ n − i i k i k2 k i k2 π(i) ⊙ +
Xi=1 (cid:16) (cid:17)
n
1
w˙ (t) = exp γ⊤σ(g (t)) ψ (t) z e w (t).
− −n − i i k i k2 k i k2 π(i) ⊙ −
Xi=1 (cid:16) (cid:17)
32Then, the dynamics of β is
2 t n
β(t) = w⊙2 sinh exp γ⊤σ(g (τ)) ψ (τ) z e dτ .
0 ⊙ n − i i k i k2 k i k2 π(i)
Z0
Xi=1 (cid:16) (cid:17)
!
Since K(t) and Q(t) satisfy the alignment property for all t, by (40) and Assumption 4 (iv), it
holds that ψi(t) = xi,opt(i) = zi for all i and t. Then, we have
kψi(t)k2 kxi,opt(i)k2 ±kzik2
ψ (t) z
i 2 i 2
k k k k
= sign( x ,z ) ψ (t),z
i,opt(i) i i i
h i h i
= sign( x ,z ) σ (g (t))σ (g (t))(γ γ ) x x ,z , (41)
i,opt(i) i opt(i) i l i i,opt(i) i,nopt i,opt(i) il i
h i − h − i
l6=opt(i)
X
where sign() is a sign function. Let B = sign( x ,z ) x x ,z e Rd. By (41),
il i,opt(i) i i,opt(i) il i π(i)
· h i h − i ∈
the dynamics of β can be written as
2 t n
β(t) = w⊙2 sinh exp γ⊤σ(g (τ)) σ(g (τ)) σ(g (τ)) (γ γ )B dτ ,
0 ⊙ n − i i i opt(i) i l i,opt(i) − i,nopt il 
Z0
Xi=1l6= Xopt(i) (cid:16) (cid:17)
 
which has the same form as in (19). By the analogous argument in the proof of Theorem 4.1,
βp ∞ = lim β(t) is a global solution to
t→∞ mini,l6=opt(i)β(t)⊤Bil
min β s.t. β⊤B 1 l = opt(i), i [n], (42)
1 il
β∈Rdk k ≥ ∀ 6 ∈
where B = sign( x ,z ) x x ,z e Rd.
il i,opt(i) i i,opt(i) il i π(i)
h i h − i ∈
Now, we show that if
βp
∞ is a global solution to (42), then
Wx
∞ = lim
UK(0)Diag(β(t))UQ(0)⊤
t→∞ µ(t)
is a global solution to (W -SVM). First, for all i,l = opt(i) and t, it holds that
∗
6
β(t)⊤B = sign( x ,z ) x x ,z β (t)
il i,opt(i) i i,opt(i) il i π(i)
h i h − i
x
i,opt(i)
= x x , z β (t)
i,opt(i) il i 2 π(i)
− x k k
(cid:28) k i,opt(i) k2(cid:29)
= (x x )⊤U (0)Diag(β(t))U (0)⊤z
i,opt(i) il K Q i
−
= (x x )⊤W(t)z ,
i,opt(i) il i
−
and
U
(0)Diag(βp
∞)U (0)⊤ = lim
U K(0)Diag(β(t))U Q(0)⊤
K Q t→∞ min i,l6=opt(i)β(t)⊤B il
U (0)Diag(β(t))U (0)⊤
K Q
= lim
t→∞ min i,l6=opt(i)(x i,opt(i) x il)⊤W(t)z i
−
x
= W∞.
p x
Hence, the constraints of (42) and (W -SVM) are equivalent with β∞ and W∞, respectively.
∗
33The stationary conditons of (42) and (W -SVM) are derived as follows:
∗
∂ β λ B
1 il il
k k ∋
i,l6=opt(i)
X
∂ W λ (x x )z⊤.
k k∗ ∋ il i,opt(i) − il i
i,l6=opt(i)
X
The subdifferential nuclear norm is defined as
∂ W = UV⊤ +E P E = 0, P E = 0, E 1 ,
∗ U V spec
k k | k k ≤
n o
where P and P are the projection matrices onto the column spaces of U and V, respectively and
U V
E is the spectral norm of E.
spec
k k
We first show that λ (t)U (0)Diag(B )U (0)⊤ = λ (t)(x x )z⊤
i,l6=opt(i) il K il Q i,l6=opt(i) il i,opt(i) − il i
for all t. Recall we let λ (t) = 2 φ (t). By Assumption 4 (iii) and (39), for each x , there
Pil nlnµ(t) il P il
exists unique x such that σ (g (t)) = σ (g (t)) for all l = l′ and t. It follows from the definition of
il′ l i l′ i
6
λ (t) that λ (t)= λ (t) for all t. Then, for such x and x , we have
il il il′ il il′
λ (t)(x x )+λ (t)(x x )
il i,opt(i) il il′ i,opt(i) il′
− −
= λ (t)(2x x x )
il i,opt(i) il il′
− −
= λ (t)(2x P x P x )
il i,opt(i)
−
xi,opt(i) il
−
xi,opt(i) il′
= λ (t)(x P x )+λ (t)(x P x )
il i,opt(i)
−
xi,opt(i) il il′ i,opt(i)
−
xi,opt(i) il′
x x x x
i,opt(i) i,opt(i) i,opt(i) i,opt(i)
= λ (t) x x , +λ (t) x x , .
il i,opt(i) il il′ i,opt(i) il′
− x x − x x
(cid:28) k i,opt(i) k2(cid:29)k i,opt(i) k2 (cid:28) k i,opt(i) k2(cid:29)k i,opt(i) k2
(43)
By (43) and the definition of B , we have
il
x x
λ (t)U (0)Diag(B )U (0)⊤ = λ (t) x x , i,opt(i) i,opt(i) z⊤
il K il Q il i,opt(i) − il x x i
i,l6=opt(i) i,l6=opt(i) (cid:28) k i,opt(i) k2(cid:29)k i,opt(i) k2
X X
= λ (t)(x x )z⊤. (44)
il i,opt(i) − il i
i,l6=opt(i)
X
By Theorem 2 in Watson (1992), we have
∂ W =conv UDiag(d)V⊤ W = UDiag(β)V⊤ d ∂ β , (45)
∗ 1
k k | ∈ k k
n o
where conv denotes the convex hull of a set and U,V are orthogonal matrices. It follows from
{·} p
(44), (45) and Theorem 2 in Watson (1992) that if β∞ satisfies the stationary condition of (42),
x
then W∞ satisfies the stationary condition of (W -SVM).
∗
34