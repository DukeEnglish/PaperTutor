[
    {
        "title": "Aliasing and Label-Independent Decomposition of Risk: Beyond the bias-variance trade-off",
        "authors": "Mark K. TranstrumGus L. W. HartTyler J. JarvisJared P. Whitehead",
        "links": "http://arxiv.org/abs/2408.08294v1",
        "entry_id": "http://arxiv.org/abs/2408.08294v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08294v1",
        "summary": "A central problem in data science is to use potentially noisy samples of an\nunknown function to predict function values for unseen inputs. In classical\nstatistics, the predictive error is understood as a trade-off between the bias\nand the variance that balances model simplicity with its ability to fit complex\nfunctions. However, over-parameterized models exhibit counter-intuitive\nbehaviors, such as \"double descent\" in which models of increasing complexity\nexhibit decreasing generalization error. We introduce an alternative paradigm\ncalled the generalized aliasing decomposition. We explain the asymptotically\nsmall error of complex models as a systematic \"de-aliasing\" that occurs in the\nover-parameterized regime. In the limit of large models, the contribution due\nto aliasing vanishes, leaving an expression for the asymptotic total error we\ncall the invertibility failure of very large models on few training points.\nBecause the generalized aliasing decomposition can be explicitly calculated\nfrom the relationship between model class and samples without seeing any data\nlabels, it can answer questions related to experimental design and model\nselection before collecting data or performing experiments. We demonstrate this\napproach using several examples, including classical regression problems and a\ncluster expansion model used in materials science.",
        "updated": "2024-08-15 17:49:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08294v1"
    },
    {
        "title": "Derivative-Free Guidance in Continuous and Discrete Diffusion Models with Soft Value-Based Decoding",
        "authors": "Xiner LiYulai ZhaoChenyu WangGabriele ScaliaGokcen EraslanSurag NairTommaso BiancalaniAviv RegevSergey LevineMasatoshi Uehara",
        "links": "http://arxiv.org/abs/2408.08252v1",
        "entry_id": "http://arxiv.org/abs/2408.08252v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08252v1",
        "summary": "Diffusion models excel at capturing the natural design spaces of images,\nmolecules, DNA, RNA, and protein sequences. However, rather than merely\ngenerating designs that are natural, we often aim to optimize downstream reward\nfunctions while preserving the naturalness of these design spaces. Existing\nmethods for achieving this goal often require ``differentiable'' proxy models\n(\\textit{e.g.}, classifier guidance or DPS) or involve computationally\nexpensive fine-tuning of diffusion models (\\textit{e.g.}, classifier-free\nguidance, RL-based fine-tuning). In our work, we propose a new method to\naddress these challenges. Our algorithm is an iterative sampling method that\nintegrates soft value functions, which looks ahead to how intermediate noisy\nstates lead to high rewards in the future, into the standard inference\nprocedure of pre-trained diffusion models. Notably, our approach avoids\nfine-tuning generative models and eliminates the need to construct\ndifferentiable models. This enables us to (1) directly utilize\nnon-differentiable features/reward feedback, commonly used in many scientific\ndomains, and (2) apply our method to recent discrete diffusion models in a\nprincipled way. Finally, we demonstrate the effectiveness of our algorithm\nacross several domains, including image generation, molecule generation, and\nDNA/RNA sequence generation. The code is available at\n\\href{https://github.com/masa-ue/SVDD}{https://github.com/masa-ue/SVDD}.",
        "updated": "2024-08-15 16:47:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08252v1"
    },
    {
        "title": "Localized Sparse Principal Component Analysis of Multivariate Time Series in Frequency Domain",
        "authors": "Jamshid NamdariAmita ManatungaFabio FerrarelliRobert Krafty",
        "links": "http://arxiv.org/abs/2408.08177v1",
        "entry_id": "http://arxiv.org/abs/2408.08177v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08177v1",
        "summary": "Principal component analysis has been a main tool in multivariate analysis\nfor estimating a low dimensional linear subspace that explains most of the\nvariability in the data. However, in high-dimensional regimes, naive estimates\nof the principal loadings are not consistent and difficult to interpret. In the\ncontext of time series, principal component analysis of spectral density\nmatrices can provide valuable, parsimonious information about the behavior of\nthe underlying process, particularly if the principal components are\ninterpretable in that they are sparse in coordinates and localized in frequency\nbands. In this paper, we introduce a formulation and consistent estimation\nprocedure for interpretable principal component analysis for high-dimensional\ntime series in the frequency domain. An efficient frequency-sequential\nalgorithm is developed to compute sparse-localized estimates of the\nlow-dimensional principal subspaces of the signal process. The method is\nmotivated by and used to understand neurological mechanisms from high-density\nresting-state EEG in a study of first episode psychosis.",
        "updated": "2024-08-15 14:30:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08177v1"
    },
    {
        "title": "Extracting Sentence Embeddings from Pretrained Transformer Models",
        "authors": "Lukas StankevičiusMantas Lukoševičius",
        "links": "http://arxiv.org/abs/2408.08073v1",
        "entry_id": "http://arxiv.org/abs/2408.08073v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08073v1",
        "summary": "Background/introduction: Pre-trained transformer models shine in many natural\nlanguage processing tasks and therefore are expected to bear the representation\nof the input sentence or text meaning. These sentence-level embeddings are also\nimportant in retrieval-augmented generation. But do commonly used plain\naveraging or prompt templates surface it enough?\n  Methods: Given 110M parameters BERT's hidden representations from multiple\nlayers and multiple tokens we tried various ways to extract optimal sentence\nrepresentations. We tested various token aggregation and representation\npost-processing techniques. We also tested multiple ways of using a general\nWikitext dataset to complement BERTs sentence representations. All methods were\ntested on 8 Semantic Textual Similarity (STS), 6 short text clustering, and 12\nclassification tasks. We also evaluated our representation-shaping techniques\non other static models, including random token representations.\n  Results: Proposed representation extraction methods improved the performance\non STS and clustering tasks for all models considered. Very high improvements\nfor static token-based models, especially random embeddings for STS tasks\nalmost reach the performance of BERT-derived representations.\n  Conclusions: Our work shows that for multiple tasks simple baselines with\nrepresentation shaping techniques reach or even outperform more complex\nBERT-based models or are able to contribute to their performance.",
        "updated": "2024-08-15 10:54:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08073v1"
    },
    {
        "title": "BINDy -- Bayesian identification of nonlinear dynamics with reversible-jump Markov-chain Monte-Carlo",
        "authors": "Max D. ChampneysTimothy J. Rogers",
        "links": "http://arxiv.org/abs/2408.08062v1",
        "entry_id": "http://arxiv.org/abs/2408.08062v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08062v1",
        "summary": "Model parsimony is an important \\emph{cognitive bias} in data-driven\nmodelling that aids interpretability and helps to prevent over-fitting. Sparse\nidentification of nonlinear dynamics (SINDy) methods are able to learn sparse\nrepresentations of complex dynamics directly from data, given a basis of\nlibrary functions. In this work, a novel Bayesian treatment of dictionary\nlearning system identification, as an alternative to SINDy, is envisaged. The\nproposed method -- Bayesian identification of nonlinear dynamics (BINDy) -- is\ndistinct from previous approaches in that it targets the full joint posterior\ndistribution over both the terms in the library and their parameterisation in\nthe model. This formulation confers the advantage that an arbitrary prior may\nbe placed over the model structure to produce models that are sparse in the\nmodel space rather than in parameter space. Because this posterior is defined\nover parameter vectors that can change in dimension, the inference cannot be\nperformed by standard techniques. Instead, a Gibbs sampler based on\nreversible-jump Markov-chain Monte-Carlo is proposed. BINDy is shown to compare\nfavourably to ensemble SINDy in three benchmark case-studies. In particular, it\nis seen that the proposed method is better able to assign high probability to\ncorrect model terms.",
        "updated": "2024-08-15 10:03:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08062v1"
    }
]