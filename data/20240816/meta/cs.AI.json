[
    {
        "title": "Can Large Language Models Understand Symbolic Graphics Programs?",
        "authors": "Zeju QiuWeiyang LiuHaiwen FengZhen LiuTim Z. XiaoKatherine M. CollinsJoshua B. TenenbaumAdrian WellerMichael J. BlackBernhard Schölkopf",
        "links": "http://arxiv.org/abs/2408.08313v1",
        "entry_id": "http://arxiv.org/abs/2408.08313v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08313v1",
        "summary": "Assessing the capabilities of large language models (LLMs) is often\nchallenging, in part, because it is hard to find tasks to which they have not\nbeen exposed during training. We take one step to address this challenge by\nturning to a new task: focusing on symbolic graphics programs, which are a\npopular representation for graphics content that procedurally generates visual\ndata. LLMs have shown exciting promise towards program synthesis, but do they\nunderstand symbolic graphics programs? Unlike conventional programs, symbolic\ngraphics programs can be translated to graphics content. Here, we characterize\nan LLM's understanding of symbolic programs in terms of their ability to answer\nquestions related to the graphics content. This task is challenging as the\nquestions are difficult to answer from the symbolic programs alone -- yet, they\nwould be easy to answer from the corresponding graphics content as we verify\nthrough a human experiment. To understand symbolic programs, LLMs may need to\npossess the ability to imagine how the corresponding graphics content would\nlook without directly accessing the rendered visual content. We use this task\nto evaluate LLMs by creating a large benchmark for the semantic understanding\nof symbolic graphics programs. This benchmark is built via program-graphics\ncorrespondence, hence requiring minimal human efforts. We evaluate current LLMs\non our benchmark to elucidate a preliminary assessment of their ability to\nreason about visual scenes from programs. We find that this task distinguishes\nexisting LLMs and models considered good at reasoning perform better. Lastly,\nwe introduce Symbolic Instruction Tuning (SIT) to improve this ability.\nSpecifically, we query GPT4-o with questions and images generated by symbolic\nprograms. Such data are then used to finetune an LLM. We also find that SIT\ndata can improve the general instruction following ability of LLMs.",
        "updated": "2024-08-15 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08313v1"
    },
    {
        "title": "HyperTaxel: Hyper-Resolution for Taxel-Based Tactile Signals Through Contrastive Learning",
        "authors": "Hongyu LiSnehal DikhaleJinda CuiSoshi IbaNawid Jamali",
        "links": "http://arxiv.org/abs/2408.08312v1",
        "entry_id": "http://arxiv.org/abs/2408.08312v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08312v1",
        "summary": "To achieve dexterity comparable to that of humans, robots must intelligently\nprocess tactile sensor data. Taxel-based tactile signals often have low\nspatial-resolution, with non-standardized representations. In this paper, we\npropose a novel framework, HyperTaxel, for learning a geometrically-informed\nrepresentation of taxel-based tactile signals to address challenges associated\nwith their spatial resolution. We use this representation and a contrastive\nlearning objective to encode and map sparse low-resolution taxel signals to\nhigh-resolution contact surfaces. To address the uncertainty inherent in these\nsignals, we leverage joint probability distributions across multiple\nsimultaneous contacts to improve taxel hyper-resolution. We evaluate our\nrepresentation by comparing it with two baselines and present results that\nsuggest our representation outperforms the baselines. Furthermore, we present\nqualitative results that demonstrate the learned representation captures the\ngeometric features of the contact surface, such as flatness, curvature, and\nedges, and generalizes across different objects and sensor configurations.\nMoreover, we present results that suggest our representation improves the\nperformance of various downstream tasks, such as surface classification, 6D\nin-hand pose estimation, and sim-to-real transfer.",
        "updated": "2024-08-15 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08312v1"
    },
    {
        "title": "Benchmarking the Capabilities of Large Language Models in Transportation System Engineering: Accuracy, Consistency, and Reasoning Behaviors",
        "authors": "Usman SyedEthan LightXingang GuoHuan ZhangLianhui QinYanfeng OuyangBin Hu",
        "links": "http://arxiv.org/abs/2408.08302v1",
        "entry_id": "http://arxiv.org/abs/2408.08302v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08302v1",
        "summary": "In this paper, we explore the capabilities of state-of-the-art large language\nmodels (LLMs) such as GPT-4, GPT-4o, Claude 3.5 Sonnet, Claude 3 Opus, Gemini\n1.5 Pro, Llama 3, and Llama 3.1 in solving some selected undergraduate-level\ntransportation engineering problems. We introduce TransportBench, a benchmark\ndataset that includes a sample of transportation engineering problems on a wide\nrange of subjects in the context of planning, design, management, and control\nof transportation systems. This dataset is used by human experts to evaluate\nthe capabilities of various commercial and open-sourced LLMs, especially their\naccuracy, consistency, and reasoning behaviors, in solving transportation\nengineering problems. Our comprehensive analysis uncovers the unique strengths\nand limitations of each LLM, e.g. our analysis shows the impressive accuracy\nand some unexpected inconsistent behaviors of Claude 3.5 Sonnet in solving\nTransportBench problems. Our study marks a thrilling first step toward\nharnessing artificial general intelligence for complex transportation\nchallenges.",
        "updated": "2024-08-15 17:55:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08302v1"
    },
    {
        "title": "SLCA++: Unleash the Power of Sequential Fine-tuning for Continual Learning with Pre-training",
        "authors": "Gengwei ZhangLiyuan WangGuoliang KangLing ChenYunchao Wei",
        "links": "http://arxiv.org/abs/2408.08295v1",
        "entry_id": "http://arxiv.org/abs/2408.08295v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08295v1",
        "summary": "In recent years, continual learning with pre-training (CLPT) has received\nwidespread interest, instead of its traditional focus of training from scratch.\nThe use of strong pre-trained models (PTMs) can greatly facilitate knowledge\ntransfer and alleviate catastrophic forgetting, but also suffers from\nprogressive overfitting of pre-trained knowledge into specific downstream\ntasks. A majority of current efforts often keep the PTMs frozen and incorporate\ntask-specific prompts to instruct representation learning, coupled with a\nprompt selection process for inference. However, due to the limited capacity of\nprompt parameters, this strategy demonstrates only sub-optimal performance in\ncontinual learning. In comparison, tuning all parameters of PTMs often provides\nthe greatest potential for representation learning, making sequential\nfine-tuning (Seq FT) a fundamental baseline that has been overlooked in CLPT.\nTo this end, we present an in-depth analysis of the progressive overfitting\nproblem from the lens of Seq FT. Considering that the overly fast\nrepresentation learning and the biased classification layer constitute this\nparticular problem, we introduce the advanced Slow Learner with Classifier\nAlignment (SLCA++) framework to unleash the power of Seq FT, serving as a\nstrong baseline approach for CLPT. Our approach involves a Slow Learner to\nselectively reduce the learning rate of backbone parameters, and a Classifier\nAlignment to align the disjoint classification layers in a post-hoc fashion. We\nfurther enhance the efficacy of SL with a symmetric cross-entropy loss, as well\nas employ a parameter-efficient strategy to implement Seq FT with SLCA++.\nAcross a variety of continual learning scenarios on image classification\nbenchmarks, our approach provides substantial improvements and outperforms\nstate-of-the-art methods by a large margin. Code:\nhttps://github.com/GengDavid/SLCA.",
        "updated": "2024-08-15 17:50:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08295v1"
    },
    {
        "title": "Autonomous Behavior Planning For Humanoid Loco-manipulation Through Grounded Language Model",
        "authors": "Jin WangArturo LaurenziNikos Tsagarakis",
        "links": "http://arxiv.org/abs/2408.08282v1",
        "entry_id": "http://arxiv.org/abs/2408.08282v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08282v1",
        "summary": "Enabling humanoid robots to perform autonomously loco-manipulation in\nunstructured environments is crucial and highly challenging for achieving\nembodied intelligence. This involves robots being able to plan their actions\nand behaviors in long-horizon tasks while using multi-modality to perceive\ndeviations between task execution and high-level planning. Recently, large\nlanguage models (LLMs) have demonstrated powerful planning and reasoning\ncapabilities for comprehension and processing of semantic information through\nrobot control tasks, as well as the usability of analytical judgment and\ndecision-making for multi-modal inputs. To leverage the power of LLMs towards\nhumanoid loco-manipulation, we propose a novel language-model based framework\nthat enables robots to autonomously plan behaviors and low-level execution\nunder given textual instructions, while observing and correcting failures that\nmay occur during task execution. To systematically evaluate this framework in\ngrounding LLMs, we created the robot 'action' and 'sensing' behavior library\nfor task planning, and conducted mobile manipulation tasks and experiments in\nboth simulated and real environments using the CENTAURO robot, and verified the\neffectiveness and application of this approach in robotic tasks with autonomous\nbehavioral planning.",
        "updated": "2024-08-15 17:33:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08282v1"
    }
]