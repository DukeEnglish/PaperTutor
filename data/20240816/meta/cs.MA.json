[
    {
        "title": "Stochastic Semi-Gradient Descent for Learning Mean Field Games with Population-Aware Function Approximation",
        "authors": "Chenyu ZhangXu ChenXuan Di",
        "links": "http://arxiv.org/abs/2408.08192v1",
        "entry_id": "http://arxiv.org/abs/2408.08192v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08192v1",
        "summary": "Mean field games (MFGs) model the interactions within a large-population\nmulti-agent system using the population distribution. Traditional learning\nmethods for MFGs are based on fixed-point iteration (FPI), which calculates\nbest responses and induced population distribution separately and sequentially.\nHowever, FPI-type methods suffer from inefficiency and instability, due to\noscillations caused by the forward-backward procedure. This paper considers an\nonline learning method for MFGs, where an agent updates its policy and\npopulation estimates simultaneously and fully asynchronously, resulting in a\nsimple stochastic gradient descent (SGD) type method called SemiSGD. Not only\ndoes SemiSGD exhibit numerical stability and efficiency, but it also provides a\nnovel perspective by treating the value function and population distribution as\na unified parameter. We theoretically show that SemiSGD directs this unified\nparameter along a descent direction to the mean field equilibrium. Motivated by\nthis perspective, we develop a linear function approximation (LFA) for both the\nvalue function and the population distribution, resulting in the first\npopulation-aware LFA for MFGs on continuous state-action space. Finite-time\nconvergence and approximation error analysis are provided for SemiSGD equipped\nwith population-aware LFA.",
        "updated": "2024-08-15 14:51:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08192v1"
    },
    {
        "title": "EmBARDiment: an Embodied AI Agent for Productivity in XR",
        "authors": "Riccardo BovoSteven AbreuKaran AhujaEric J GonzalezLi-Te ChengMar Gonzalez-Franco",
        "links": "http://arxiv.org/abs/2408.08158v1",
        "entry_id": "http://arxiv.org/abs/2408.08158v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08158v1",
        "summary": "XR devices running chat-bots powered by Large Language Models (LLMs) have\ntremendous potential as always-on agents that can enable much better\nproductivity scenarios. However, screen based chat-bots do not take advantage\nof the the full-suite of natural inputs available in XR, including inward\nfacing sensor data, instead they over-rely on explicit voice or text prompts,\nsometimes paired with multi-modal data dropped as part of the query. We propose\na solution that leverages an attention framework that derives context\nimplicitly from user actions, eye-gaze, and contextual memory within the XR\nenvironment. This minimizes the need for engineered explicit prompts, fostering\ngrounded and intuitive interactions that glean user insights for the chat-bot.\nOur user studies demonstrate the imminent feasibility and transformative\npotential of our approach to streamline user interaction in XR with chat-bots,\nwhile offering insights for the design of future XR-embodied LLM agents.",
        "updated": "2024-08-15 13:48:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08158v1"
    },
    {
        "title": "Independent Policy Mirror Descent for Markov Potential Games: Scaling to Large Number of Players",
        "authors": "Pragnya AlaturAnas BarakatNiao He",
        "links": "http://arxiv.org/abs/2408.08075v1",
        "entry_id": "http://arxiv.org/abs/2408.08075v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08075v1",
        "summary": "Markov Potential Games (MPGs) form an important sub-class of Markov games,\nwhich are a common framework to model multi-agent reinforcement learning\nproblems. In particular, MPGs include as a special case the identical-interest\nsetting where all the agents share the same reward function. Scaling the\nperformance of Nash equilibrium learning algorithms to a large number of agents\nis crucial for multi-agent systems. To address this important challenge, we\nfocus on the independent learning setting where agents can only have access to\ntheir local information to update their own policy. In prior work on MPGs, the\niteration complexity for obtaining $\\epsilon$-Nash regret scales linearly with\nthe number of agents $N$. In this work, we investigate the iteration complexity\nof an independent policy mirror descent (PMD) algorithm for MPGs. We show that\nPMD with KL regularization, also known as natural policy gradient, enjoys a\nbetter $\\sqrt{N}$ dependence on the number of agents, improving over PMD with\nEuclidean regularization and prior work. Furthermore, the iteration complexity\nis also independent of the sizes of the agents' action spaces.",
        "updated": "2024-08-15 11:02:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08075v1"
    },
    {
        "title": "Time-Ordered Ad-hoc Resource Sharing for Independent Robotic Agents",
        "authors": "Arjo ChakravartyMichael X. GreyM. A. Viraj J. MuthugalaMohan Rajesh Elara",
        "links": "http://arxiv.org/abs/2408.07942v1",
        "entry_id": "http://arxiv.org/abs/2408.07942v1",
        "pdf_url": "http://arxiv.org/pdf/2408.07942v1",
        "summary": "Resource sharing is a crucial part of a multi-robot system. We propose a\nBoolean satisfiability based approach to resource sharing. Our key\ncontributions are an algorithm for converting any constrained assignment to a\nweighted-SAT based optimization. We propose a theorem that allows optimal\nresource assignment problems to be solved via repeated application of a SAT\nsolver. Additionally we show a way to encode continuous time ordering\nconstraints using Conjunctive Normal Form (CNF). We benchmark our new\nalgorithms and show that they can be used in an ad-hoc setting. We test our\nalgorithms on a fleet of simulated and real world robots and show that the\nalgorithms are able to handle real world situations. Our algorithms and test\nharnesses are opensource and build on Open-RMFs fleet management system.",
        "updated": "2024-08-15 05:34:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.07942v1"
    },
    {
        "title": "The Nah Bandit: Modeling User Non-compliance in Recommendation Systems",
        "authors": "Tianyue ZhouJung-Hoon ChoCathy Wu",
        "links": "http://arxiv.org/abs/2408.07897v1",
        "entry_id": "http://arxiv.org/abs/2408.07897v1",
        "pdf_url": "http://arxiv.org/pdf/2408.07897v1",
        "summary": "Recommendation systems now pervade the digital world, ranging from\nadvertising to entertainment. However, it remains challenging to implement\neffective recommendation systems in the physical world, such as in mobility or\nhealth. This work focuses on a key challenge: in the physical world, it is\noften easy for the user to opt out of taking any recommendation if they are not\nto her liking, and to fall back to her baseline behavior. It is thus crucial in\ncyber-physical recommendation systems to operate with an interaction model that\nis aware of such user behavior, lest the user abandon the recommendations\naltogether. This paper thus introduces the Nah Bandit, a tongue-in-cheek\nreference to describe a Bandit problem where users can say `nah' to the\nrecommendation and opt for their preferred option instead. As such, this\nproblem lies in between a typical bandit setup and supervised learning. We\nmodel the user non-compliance by parameterizing an anchoring effect of\nrecommendations on users. We then propose the Expert with Clustering (EWC)\nalgorithm, a hierarchical approach that incorporates feedback from both\nrecommended and non-recommended options to accelerate user preference learning.\nIn a recommendation scenario with $N$ users, $T$ rounds per user, and $K$\nclusters, EWC achieves a regret bound of $O(N\\sqrt{T\\log K} + NT)$, achieving\nsuperior theoretical performance in the short term compared to LinUCB\nalgorithm. Experimental results also highlight that EWC outperforms both\nsupervised learning and traditional contextual bandit approaches. This\nadvancement reveals that effective use of non-compliance feedback can\naccelerate preference learning and improve recommendation accuracy. This work\nlays the foundation for future research in Nah Bandit, providing a robust\nframework for more effective recommendation systems.",
        "updated": "2024-08-15 03:01:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.07897v1"
    }
]