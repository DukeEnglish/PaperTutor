Stochastic Semi-Gradient Descent for Learning Mean
Field Games with Population-Aware Function
Approximation
ChenyuZhang XuChen XuanDi
ColumbiaUniversity ColumbiaUniversity ColumbiaUniversity
NewYork,NY10025 NewYork,NY10025 NewYork,NY10025
cz2736@columbia.edu xc2412@columbia.edu sharon.di@columbia.edu
Abstract
Meanfieldgames(MFGs)modeltheinteractionswithinalarge-populationmulti-
agentsystemusingthepopulationdistribution. Traditionallearningmethodsfor
MFGsarebasedonfixed-pointiteration(FPI),whichcalculatesbestresponsesand
inducedpopulationdistributionseparatelyandsequentially. However,FPI-type
methodssufferfrominefficiencyandinstability,duetooscillationscausedbythe
forward-backwardprocedure. Thispaperconsidersanonlinelearningmethodfor
MFGs,whereanagentupdatesitspolicyandpopulationestimatessimultaneously
andfullyasynchronously,resultinginasimplestochasticgradientdescent(SGD)
typemethodcalledSemiSGD.NotonlydoesSemiSGDexhibitnumericalstability
andefficiency,butitalsoprovidesanovelperspectivebytreatingthevaluefunction
and population distribution as a unified parameter. We theoretically show that
SemiSGD directs this unified parameter along a descent direction to the mean
field equilibrium. Motivated by this perspective, we develop a linear function
approximation(LFA)forboththevaluefunctionandthepopulationdistribution,
resultinginthefirstpopulation-awareLFAforMFGsoncontinuousstate-action
space. Finite-timeconvergenceandapproximationerroranalysisareprovidedfor
SemiSGDequippedwithpopulation-awareLFA.
1 Introduction
Meanfieldgames(MFGs)[17,21]offeratractableframeworkformodelingmulti-agentsystems
withalargehomogeneouspopulation. InMFGs,theimpactofotheragentsonaparticularagent
isencapsulatedbyapopulationmass(ormeanfield),providingareliableapproximationofactual
interactions among agentswhen the number of agentsis large. Consequently, understanding the
populationisfundamentalinMFGs,aslearningthesegamesentailsconsideringboththeagent’s
policyandthepopulationdynamics.
PriorworkonlearningMFGshasmainlyfocusedonfixed-pointiteration(FPI)methodsandtheir
variations,whichsequentiallycalculatethebestresponsew.r.t. afixedpopulation,andtheinduced
populationdistribution(ormeasure)w.r.t. afixed policy[15,14,12,30,24]. However, FPI-type
methodsfaceseverallimitations: 1)Calculatingbestresponses(backward)andinducedpopulation
distributions (forward) typically involves distinct iterative processes. These processes are imple-
mentedseparatelyandexecutedsequentially,hinderingparallelcomputingandpotentiallyincreasing
theoverallcomputationalburden.2)FPImethodssufferfrominstability.Fixingapopulationmeasure
far from the equilibrium measure can lead to a corresponding best response that is also far from
theequilibriumpolicy,resultinginslowconvergence. Moreover,drasticdifferencesbetweenbest
responsesandinducedpopulationmeasuresinconsecutiveiterationscancauseoscillationsinthe
learningprocess,aphenomenoncommonlyobservedinpractice(see,e.g.,Figure1aand[12]).
Preprint.Underreview.
4202
guA
51
]GL.sc[
1v29180.8042:viXraFPI-type SGD-type
π µ π µ
This work delves into the 0 0 0 −g0(θ0);−g0(η0) 0
rapidlygrowingfieldofonline π 1
learning for MFGs, where an µ 1 ξ 1
online agent interacts with the π 2 µ ξ 2
2
environmentandgathersobser- ξ 3
vations on a Markov chain to
(π ,µ ) (π ,µ )
updateitspolicyandpopulation ∗ ∗ ∗ ∗
estimate. Notably,thisMarkov (a)Convergenceperformance. (b)Learningdynamics.
chain is parameterized by the Figure1: FPIversusSGD
policyandapopulationdistribu-
tion, which collectively determine the transition kernel. Thus, the goal of finding a mean field
equilibrium(MFE)canbetranslatedintofindinganoptimalparameter,whichcorrespondstothe
fixed point of the Bellman operator and the transition operator. This perspective unlocks a large
toolboxofstochasticparameterapproximationmethodsforlearningMFGs,suchassimplestochastic
gradient descent (SGD)-type methods. Specifically, we treat the policy and the population as a
unifiedparameterandupdatetheirestimatesfullyasynchronouslyusingthesamesamples,thereby
eliminatingtheforward-backwardprocess.
WetermourmethodSemiSGD,asitusessemi-gradientsinlieuofactualgradients. Albeitsimple,an
SGD-typeprocedureraisesseveralquestions: 1)Simultaneouslyandasynchronouslyupdatingthe
behaviorpolicyandthepopulationestimatecreatesastrongcouplingbetweenthetwo,rendering
theunderlyingMarkovchainnon-stationary. OnlineFPIavoidsthisissuebyfixingoneestimate
whileupdatingtheother. Howcanweanalyzethiscouplingandthenon-stationaryprocesstoensure
theconvergenceofSemiSGD?2)HowdoesSemiSGD,followinganon-stationaryMarkovchain,
converge? Futhermore,howcanitaddresstheinstabilityandoscillationissuesofFPI-typemethods?
For the first question, note that the Bellman and transition operators are contractive. Thus, with
sufficientlysmoothdynamics,thenon-stationarityisdominatedbythecontractionproperty. Remark-
ably,wediscoverthatSemiSGDindeedfollowsadescentdirectiontowardstheMFE(Lemma1),
providedthatthedynamicsareLipschitzcontinuouswithsamllLipschitzconstants. Byfollowing
adescentdirection,updatingparametersincrementally,anddroppingtheforward-backwardproce-
dure,SemiSGDachievesautomaticstabilizationwithoutadditionalmechanics(seeFigure1afora
numericalexampleandFigure1bforanintuitiveillustration).
TreatingthepolicyandpopulationasaunifiedparameteralsoilluminatesthegapinlearningMFGson
continuousstate-actionspaces: linearfunctionapproximation(LFA),atechniquewidelyemployedin
learningvaluefunctionsoncontinuousstate-actionspaces[26,19],canbeappliedtothepopulation
measureaswell. Asaframeworkforinfinite-playergames,commonMFGsareoftenoninfiniteor
continuousstate-actionspaces,suchasautonomousdriving[9],flocking[31]andcrowdmodelling
[20,7]. Equippedwithpopulation-awareLFA(PA-LFA),SemiSGDiscapableoflearningMFGson
continuousstate-actionspaces,fullyonlineandmodel-freewithstrongtheoreticalguarantees.
Mainresults. Wehighlightsomecontributionsandnoveltiesofthiswork.
• WeproposeSemiSGD,asimpleonlineSGD-typemethodforlearningMFGs. Weinnovatively
treat the policy and the population as a unified parameter. Algorithmically, we update both
simultaneouslyandasynchronouslyusingthesameonlineobservationsandlearningrate,thus
eliminatingtheforward-backwardprocesstypicalofFPImethods. SemiSGDenjoysautomatic
stabilization without additional mechanisms such as fictitious play or online mirror descent
(Figure1a). Theoretically,weshowthattheunifiedparameterinSemiSGDfollowsadescent
directiontowardstheMFE,whereasneitherthepolicynorpopulationaloneisguaranteedtodo
so(Lemma1).
• WeformulateanovelframeworkoflinearMFGs, characterizingaclassofMFGswithlinear
structureconcerningthepopulationmeasure.LinearMFGsaccommodatescontinuousstate-action
spacesandincludesMFGsonfinitestate-actionspacesasaspecialcase. Weprovethatlinear
MFGsenablelinearparameterizationofboththevaluefunctionandthepopulationmeasure.
• Weextendthelinearparameterizationtodevelopapopulation-awarelinearfunctionapproxima-
tion(PA-LFA)forgeneralMFGs. SemiSGDequippedwithPA-LFAisthefirstmethodtoapply
LFAtothepopulationmeasureinMFGs. Notably,updatesinSemiSGDwithPA-LFAmaintain
thesameoperationcomplexityasinthefinitestate-actionspacecase,highlightingthesimplicity
andefficiencyofourmethod.
2• SamplecomplexityandapproximationerroranalysisareprovidedforSemiSGDwithPA-LFA.
Wenovellyregardthelearningprocessasanon-stationaryMarkovdecisionprocessparameterized
bytheunifiedparameter. ThisperspectiveenablesastraightforwardSGD-typeanalysis,elegantly
handlingthestrongcouplingbetweenthepolicyandpopulationandofferinginsightsintothe
learningdynamicsofSemiSGD.
• Weconductvariousexperimentstodemonstratethepropertiesofourmethods: 1)Forcontinuous
state-actionspaces,PA-LFAismoreefficientandaccuratethandiscretization;2)SemiSGDis
inherentlysmootherandautomaticallystabilizeswithoutadditionalmechanics;3)Byeliminating
theforward-backwardprocedure,SemiSGDissignificantlyfasterthanotherFPI-typemethods.
Table1: ComparisonoflearningmethodsforMFGs
Algo. Additional Oracle- Functionapprox. Sample
Reference
type mechanism free Population Policy complex.
[14,15,35] FPI - - - - -
[30,24] FPI FP&OMD - - - -
[1,12] FPI regularization - - - -
[4,2] FPI multi-timescale ✓ - - -
[37] FPI FP ✓ - - O(cid:101)(ϵ−2)
[36] FPI two-timescale ✓ - - O(ϵ−4)
[25] FPI two-timescale - - ✓ O(cid:101)(ϵ−5)
[16] FPI - - - ✓ O(cid:101)(ϵ−2)
Thispaper SGD - ✓ ✓ ✓ O(cid:101)(ϵ−2)
Relatedwork. ThereisagrowingbodyofliteratureonlearningMFGs(summarizedinTable.1).
Readerscanreferto[23,13]foracomprehensivereview. ToaddresstheinstabilityissueofFPI,
researchershaveproposedvariousmethodstostabilizethelearningprocess,broadlycategorized
intotwoclasses: 1)regularization[1,12];2)fictitiousplay(FP)[30,8,35];and3)onlinemirror
descent(OMD)[29,24]. Amulti-timescale[2,4,36,25]updateruleisutilizedforthepopulation
andvaluefunctionintheforward-backwardprocess. Byfixingoneelement(ormakeitsupdatemuch
slower)whileupdatingtheother,thesemethodscircumventthestrongcouplingbetweenthevalue
functionandpopulationmeasure,andthusinherittheconvergencepropertiesofFPI-typemethods.
Todealwithlargestateandactionspacesandobtainstrongtheoreticalguarantees,[25,16]consider
linearfunctionapproximation(LFA)[6,26,19]forthepolicylearninginthebackwardprocessof
MFGs. The LFA scheme is not incorporated into the coupled forward-backward process, which
rendersthelearningalgorithmstillaFPI-typemethod,andsamplecomplexityresultsfailtocapture
thepopulationestimateduetotherequirementofpopulationoraclesintheforwardprocess.
Notation. AcompletelistofsymbolsandtheirmeaningsisprovidedinTable3. Wedenotetheset
ofprobabilitymeasuresonaspaceX asD(X),andthesetofsignedmeasuresasM(X). WhenX
isfinitewithdelements,theprobabilitysimplexonitisdenotedas∆d ⊂Rd. δ istheDiracdelta
x
measureatx. Withoutsubscript,theinnerproductdenotesthevectorinnerproduct⟨x,y⟩=xTy.
x⊕y is the direct sum of elements in linear spaces, which reduces to their concatenation (x;y)
when x and y are vectors. The norm without subscript denotes the ℓ norm for vectors, and ℓ
2 2
operatornormformatrices. For(vector-valued)functionsonS,theL innerproductisdefinedas
2
(cid:82) (cid:82)
⟨f,g⟩ = f(s)g(s)ds,andtheL normisdenotedas∥f∥ = ∥f(s)∥ ds.
L2 S 1 TV S 1
2 Stochasticsemi-gradientdescentforMFGsonfinitestate-actionspaces
2.1 RevisitonlinelearningforMFGsonfinitestate-actionspaces
Weconsideraninfinite-horizonMarkovdecisionprocess(MDP)denotedby(S,A,r,P,γ),withthe
statespaceS andactionspaceAbeingfinite. InMFGs,therewardfunctionrandtransitionkernel
P dependonthepopulationdistributionoverthestatespace. Specifically,foragivenstate-action
pair(s,a)∈S×Aandpopulationdistributionµ∈D(S),r(s,a,µ)andP(s′|s,a,µ)denotethe
rewardreceivedandtheprobabilityoftransitioningtostates′ ∈S. γ isthediscountfactor.
An agent in an MFG aims to find a policy π, which maps a state to a distribution on the action
spacedeterminingtheagent’sactions,thatmaximizesitsexpectedcumulativediscountedreward
whileinteractingwiththepopulation. Weutilizeavalue-basedapproachtocalculatepoliciesand
assume access to a policy operator Γ that returns a policy based on an (action-)value function
π
3Q: S ×A → R. Wewriteπ := Γ (Q). Todefinethelearninggoal,themeanfieldequilibrium
Q π
(MFE),wefirstintroducethefollowingtwooperatorsforanyvaluefunctionsQ,Q′ ∈R|S|×|A|and
populationmeasuresM,M′ ∈D(S):
T Q′(s,a):=E [r(s,a,M)+γQ′(s′,a′)], witha′∼π ,s′∼P(·|s,a,M) (Bellman)
(Q,M) (Q,M) Q
P M′(s′):=E [P(s′|s,a,M)], withs∼M′,a∼π . (Transition)
(Q,M) (M′,Q) Q
Definition1(Meanfieldequilibrium). Avaluefunction-populationdistributionpair(Q,M)isa
meanfieldequilibrium(MFE)ifitsatisfiesQ=T QandM =P M.
(Q,M) (Q,M)
AtypicalFPI-typemethod(approximately)calculatesthefixedpointoftheBellmanoperatorgiven
thecurrentpopulationdistribution,i.e.,thebestresponse,andthefixedpointofthetransitionoperator
giventhecurrentvaluefunction,i.e.,theinducedpopulationdistribution,alternately. Undercertain
conditions,FPIconvergestotheMFE[15].
This work focuses on online learning (model-free and stochastic) methods for MFGs. In such
methods,anagentmaintainsestimatesaboutthevaluefunctionandpopulationmeasure,anduses
onlineobservationstoupdateitsestimates. Forthevaluefunction,temporaldifference(TD)methods
arewidelyused[32]. GivenanonlineobservationtupleO =(s,a,s′,a′)andstep-sizeα,on-policy
TD(orSARSA[32]),updatestheQ-valuefunctionasfollows:
Q(s,a)←Q(s,a)−αg(Q;O), with g(Q;O):=Q(s,a)−r(s,a,M)−γQ(s′,a′). (1)
Forthepopulationdistribution,MonteCarlo(MC)sampling[22]isacommonchoice. Givenanew
statesamples′andusingageneralstep-sizeα,thepopulationmeasureisupdatedasfollows:
M ←M −αg(M;O), with g(M;O):=M −δ , (2)
s′
whereδ istheDiracdeltameasureandisaone-hotvectorforfinitestatespaces.
s′
TDandMCupdatesarewidelyusedinonlinelearningmethodsforMFGs[2,36,37]. Fixingthe
policyandpopulationmeasure(i.e.,fixingthetransitionkernel),TDconvergestotheoptimalvalue
function,andMCconvergestotheinducedpopulationdistribution.
2.2 Stochasticsemi-gradientdescent
Prior work on online learning for MFGs either uses two-layered loops with the policy and/or
populationmeasurefixedintheinnerloop[34,37], orusesatwo-timescaletoupdatethepolicy
andpopulationmeasureindifferentrates[2,3],essentiallyfollowingaFPI-typeforward-backward
process. Incontrast, weproposesimplycombining(1)and(2)inonepass, resultinginasimple
SGD-typemethodthatconvergestotheMFEefficiently.
Algorithm1:Stochasticsemi-gradientdescent(SemiSGD)
1 input: InitialvaluefunctionQ 0(=⟨ϕ,θ 0⟩),populationestimateM 0(=⟨ψ,η 0⟩),andstates 0.
2 fort=0,1,...,T do
3 Observea t∼Γ π(Q t)[s t],r t =r(s t,a t,M t),s t+1∼P(·|s t,a t,M t),a t+1∼Γ π(Q t)[s t+1].
4 UpdateforfiniteMFGs: (Q t+1;M t+1)=(Q t;M t)−α tg((Q t;M t);s,a,s′,a′);
5 orwithPA-LFA:ξ t+1 =Π(ξ t−α tg t(ξ t)).
6 return(Q T,M T)orξ T.
Toelaborate,gin(1)isreferredassemi-gradients,asitisnotarealgradientofanylossfunction,but
providesasimilardescentdirectiontothestationarypoint,i.e.,thezeropointofg.1 Pleasereferto
Section4forthederivationofthepopulationmeasuresemi-gradientin(2).Consequently,wepropose
updatingthevaluefunctionandpopulationmeasureestimatessimultaneouslyandasynchronously
using semi-gradients in one pass. We term our method SemiSGD and present it in Algorithm 1.
InAlgorithm1,wedenote(Q ;M )astheconcatenatedrepresentationofthevaluefunctionand
t t
populationmeasure,astheyarebothvectorsforfinitestate-actionspaces. Similarly,grepresentsthe
concatenationofthesemi-gradientsforthevaluefunctionandpopulationmeasure.
1Withaslightabuseofnotation,weuseasingleoperatorgtoreturnsemi-gradientsthroughoutthepaper,for
boththeaction-valuefunctionandpopulationmeasurewithorwithoutLFA;itshouldbeclearfromthecontext
anditsargumentswhichoneitrefersto.
4Algorithm1isanonlinesingle-loopmethodthatcompletelyremovestheneedforaforward-backward
process. NotonlyitupdatesthevaluefunctionandpopulationmeasureestimatesinanSGD-like
manner,thefollowinglemmashowsthattheadescentdirectionisprovidedbythemean-pathsemi-
gradient,definedasg¯ (·):=E g(·;O),whereOistheonlineobservationtuplefollowing
(Q,M) (Q,M)
thesteadydistributioninducedbythepolicyandtransitionkerneldeterminedbyQandM.
Lemma1(Descentdirection). Suppose(Q ;M )isanMFEandAssumptions1to3hold.2 Forany
∗ ∗
valuefunctionQandpopulationmeasureM,with∆Q:=Q−Q and∆M :=M −M ,wehave
∗ ∗
−(cid:10) ∆M,g¯ (M)(cid:11) ≤−∥∆M∥2+σL∥∆M∥(∥∆Q∥+∥∆M∥),
(Q,M)
−(cid:10) ∆Q,g¯ (Q)(cid:11) ≤−(1−γ)λ∥∆Q∥2+(σLH +L )∥∆Q∥(∥∆Q∥+∥∆M∥),
(Q,M) r
(cid:112)
whereλ:=min M (s)π (a|s),H =O( |S||A|R/(1−γ)),andotherconstantsaredefined
s,a ∗ Q∗
inAssumptions1to3.Neither−g¯ (M)nor−g¯ (Q)isguaranteedtobeadescentdirection,
(Q,M) (Q,M)
nomatterhowsmalltheLipschitzconstantsare. However,if3σLH +L ≤(1−γ)λ,itholdsthat
r
−(cid:10) (Q;M)−(Q ;M ),g¯ ((Q;M))(cid:11) ≤−1 (1−γ)λ∥(Q;M)−(Q ;M )∥2.
∗ ∗ (Q,M) 2 ∗ ∗
AgeneralizedversionofLemma1isprovedinAppendixF.1forlinearMFGs. Wenowdiscusssome
keyinsightsfromLemma1.
Descentdirection. Theconcatenatedsemi-gradientpointsadescentdirectionfortheconcatenated
estimate(Q;M),whileneitherthevaluefunctionnorpopulationmeasurealoneisguaranteedto
followadescentdirection. SeeFigure9foranillustrativeexample. Remarkably,wewillshowthat
Algorithm1followsthedirectionof−g¯withhigher-ordernoises(Theorem1). Therefore,Lemma1
arguesthatSemiSGDgenerallyfollowsadescentdirectiontotheMFE.
ExplainingoscillationsinFPI.Lemma1alsogivesanintuitiveexplanationfortheinstabilityin
FPI-typemethods. Whencalculatingthebestresponse,ifFPIfixesapopulationmeasurefarfrom
theequilibriummeasure(large∥M −M ∥),thesemi-gradientofthevaluefunctionmaynotgive
∗
a descent direction, potentially driving the best response away from the optimal policy. Similar
oscillationscanoccurwhenFPIcalculatestheinducedpopulationdistribution.
Incremental update by design. To stabilize FPI, fictitious play (FP) and online mirror descent
(OMD)incrementallyupdatethepolicyorpopulationmeasure[24],ratherthanswitchingtoentirely
newestimates. AsanSGD-typemethod,SemiSGDnaturallyupdatestheestimatesincrementally,
providinganotherlayerofstabilization,andthusdoesnotrequirethoseadditionalmechanisms.
LearnMFGonanon-stationaryMarkovchain. SemiSGDsamplesanon-stationaryMarkovian
trajectory,asitstransitionkerneldependsonthechangingvaluefunctionandpopulationmeasure.
Generally,rapidlychangingMarkovchainsexhibitchattering[39]. Thus,Lemma1requiresthat
the MDP components be Lipschitz continuous with sufficiently small Lipschitz constants. The
proof(AppendixF.1)showsthatthisdescentdirectionarisesfromthecontractionpropertiesofthe
Bellmanandtransitionoperators. WhentheMDPissmoothenough,non-stationarityisdominated
bythiscontraction. Thecontractionpropertyoftheoperatorstranslatestotheattractionpropertyof
theirfixedpoint,i.e.,theMFE,whichcorrespondstoastationaryparameterwherethemean-field
semi-gradientiszero(Proposition2). ThisfactagainunderscoresthatthelandscapeofanMFGis
bestdescribedbytheunifiedparameter,makingitmorenaturaltolearnthisparameterdirectly.
3 Linearmeanfieldgames
Fromthispoint,weremovetheassumptionthatthestate-actionspaceisfinite. InSection2,wehave
shownthatitisbeneficialtotreatthevaluefunctionandpopulationmeasureasaunifiedparameter
governingtheMDPdynamics. Asaresult,linearMDP[19]andLFAforthevaluefunctioncanbe
naturallyextendedtoaddresspopulationmeasureoncontinuousstatespaces, whichiscrucialin
variousapplicationssuchasautonomousdriving[9],flocking[31]andcrowdmodelling[20,7]. We
2Section3demonstrateslinearMFGsencompassfiniteMFGs(Example1).Thus,ouranalysisfocuseson
linearMFGs,withresultssubsequentlyreducedtofiniteMFGsinAppendixH.Asaresult,inthissectionwe
sometimesrefertotheanalysisforlinearMFGsinlatersections;insuchcases,finiteMFGsareconsideredas
linearMFGs.
5nowintroducelinearMFGs,aclassofMFGswithanunderlyinglinearMDPandalinearstructure
onthetransitionkernelw.r.t. thepopulationmeasure.
Definition2(Linearmeanfieldgames). AnMDP(S,A,P,r,γ)isalinearMDP[19]withfeature
mapϕ: S×A→Rd1 ifthereexistsd 1(signed)population-dependentmeasuresω
M
=(ω M(i))d i=1
1
∈
M(S)d1 andanunknownpopulation-dependentvectorν
M
∈Rd1,suchthatforanystate-actionpair
(s,a)∈S×AandpopulationdistributionM ∈D(S),wehave
P(s′|s,a,M)=⟨ϕ(s,a),ω (s′)⟩, r(s,a,M)=⟨ϕ(s,a),ν ⟩.
M M
AlinearMFGfurtherassumesameasurebasisψ ∈D(S)d2 suchthatforanypopulationM,there
existsanunknownmatrixΩ
M
∈ Rd1×d2 suchthatω
M
= Ω Mψ,indicatingthatP(s′|s,a,M) =
⟨ϕ(s,a),Ω ψ(s′)⟩. WerequireϕtobeL andψtobeL andL (thusitsGrammatrixexists).
M ∞ ∞ 2
Withoutlossofgenerality,weassumesup ∥ϕ(s,a)∥ ≤1andsup ∥ψ(s)∥ ≤F.
s,a 2 s 1
AlinearMFGassumestheMDPcomponents,thetransitionkernelandrewardfunction,lieinalinear
functionspacewithknownbasisfunctions. Thisindicatesthelinearstructureofthevaluefunction
andpopulationmeasure.
Proposition1. ForalinearMFG,foranypopulationdistributionM andpolicyπ,wedenoteQπ
M
astheaction-valuefunctionandµπ astheinducedpopulationmeasurew.r.t. theMDPdetermined
M
byM andπ. Then,thereexistvectorparametersθ ∈Rd1 andη ∈Rd2 suchthat
Qπ (s,a)=⟨ϕ(s,a),θ⟩, µπ (s′)=⟨ψ(s′),η⟩, ∀(s,a,s′)∈S×A×S.
M M
TheproofofProposition1isdeferredtoAppendixC.Remarkably,requiringthatthetransitionkernel
beinglinearw.r.t. ameasurebasisisessentialforthelinearstructureofthepopulationmeasure. In
thispaper,wereservelettersθ forthevaluefunctionparameterandη forthepopulationmeasure
parameter. And we denote the concatenated parameter as ξ = (η;θ). Additionally, we will use
parameters (η, θ, ξ) and their corresponding functions (Q, M, (Q,M)) interchangeably in our
analysis. Forexample,wesayξ ∈Rd1+d2 isameanfieldequilibrium(MFE)ifitscorresponding
valuefunctionQ=⟨ϕ,θ⟩andpopumeasureM =⟨ψ,η⟩satisfyDefinition1.
Example1(LinearMDPpluspopulation-independenttransitionkernel). MFGswithalinearMDP
andpopulation-independenttransitionkernelareatrivialexampleoflinearMFGs,whereΩ =I
M
forallM ∈D(S).
Example2(Finitestate-actionspace). ForfiniteMFGs,letthefeaturemapϕreturntheone-hot
vectorofeachstate-actionpair,i.e.,ϕ(s,a)=e ∈R|S||A|,andthemeasurebasisψreturnthe
(s,a)
Diracdeltameasureateachstate,i.e.,ψ(s′)=δ ∈∆|S|. ConstructthematrixΩ ∈R|S||A|×|S|
s′ M
such that (Ω ) = P(s′|s,a,M). Similarly, construct the vector ν ∈ R|S||A| such that
M (s,a),s′ M
(ν ) =r(s,a,M). Then,forany(s,a,s′)∈S×A×S,wehave
M (s,a)
P(s′|s,a,M)=eT Ω δ =(cid:10) e ,Ω δ(s′)(cid:11) , r(s,a,M)=eT ν =(cid:10) e ,ν (cid:11) .
(s,a) M s′ (s,a) M (s,a) M (s,a) M
Example2impliesthatalltheanalysisforlinearMFGsappliestofiniteMFGs(seealsoAppendixH).
4 SemiSGDwithpopulation-awarelinearfunctionapproximation
Section3introducethefirstlinearparameterizationofthepopulationmeasureinMFGs,necessitating
tailoredstochasticupdateruleforthepopulationmeasureestimate. Wenowderivethesemi-gradient
forthepopulationmeasurewithLFA.LetM betheobjectivepopulationmeasureanddefinethe
∗
lossfunctionL:= 1∥M −M ∥2 . Then,thegradientofthelossis
2 ∗ L2
∇ L=⟨∇ M,M −M ⟩ =⟨ψ,⟨ψ,η⟩−M ⟩ .
η η ∗ L2 ∗ L2
AstheMFEpopulationmeasureM isunknown,wereplaceitwiththeempiricalDeltadistribution
∗
(bootstrapping),givingthesemi-gradient
(cid:90) (cid:90)
g(η;s′)=⟨ψ,⟨ψ,η⟩−δ ⟩ = ψ(s)ψ(s)Tηds− ψ(s)δ (s)ds=:G η−ψ(s′), (3)
s′ L2 s′ ψ
S S
whereG :=(cid:82) ψ(s)ψ(s)TdsistheGrammatrixofmeasurebasisψ.
ψ S
6ForfiniteMFGs,(2)retainstheupdatedpopulationmeasureasaprobabilityvector,whichisnot
necessarilythecasewhenusing(3)asthesemi-gradient,duetothepresenceofgeneralG andψ(s′)
ψ
thatmaynotbeanidentitymatrixoraprobabilityvector. Therefore,weneedtoapplyaprojectionto
theupdatedparametertoensurethatitremainsaprobabilityvector,givingourstochasticupdaterule:
η =Π (η −α g (η )), (4)
t+1 ∆ t t t t
whereg t(η):=g(η;s t+1)andΠ ∆istheprojectionoperatorontotheprobabilitysimplex∆d2.
Remark1(Operationcomplexity). Thesimplexprojectionhasaworst-casecomplexityofO(d2)
2
[11]. Insemi-gradientevaluation,wenotethatG isaconstantmatrix,andthusweonlyneedto
ψ
evaluateψ atthenextstates′ andmultiplyparameterwithafixedmatrix,withthematrix-vector
multiplicationhavingacomplexityofO(d2). Therefore,thetotalworst-caseoperationcomplexity
2
oftheupdaterule(4)isO(d2). Additionally,thesimplexprojectionhasanexpectedcomplexityof
2
O(d )[11]. AndifG hasprecomputedpropertiesand/orspecialstructures,suchasbeingsparse
2 ψ
withO(d )non-zeroelements,thecomplexityofthesemi-gradientevaluationisO(d ),givingatotal
2 2
expectedcomplexityofO(d ),whichisthesameastheoperationcomplexityof(2)forfiniteMFGs
2
withastatespaceofsize|S|=O(d ). Insummary,(4)isassimpleandefficientasthepopulation
2
measureupdateforfiniteMFGs.
Similarly,wecangettheTDupdateruleforthevaluefunctionparameterθ[32]:
g(θ;O)=ϕ(s,a)(⟨ϕ(s,a)−γϕ(s′,a′),θ⟩−r)=:G (O)θ−φ(O),
ϕ
whereO =(s,a,r,s′,a′),andG (O):=ϕ(s,a)(ϕ(s,a)−γϕ(s′,a′))T andφ(O):=ϕ(s,a)r.The
ϕ
updaterulefortheaction-valuefunctionparameteris
θ =Π (θ −α g (θ )), (5)
t+1 D t t t t
wherewedenoteg (θ):=g(θ;O ),O =(s ,a ,r ,s ,a ),andΠ istheprojectionoperator
t t t t t t t+1 t+1 D
ontotheEuclideanballBd1 := {θ ∈ Rd1 : ∥θ∥ ≤ D}. Similartothepopulationmeasureupdate,
D
theprojectioniscommonlyusedwithLFAtoensurethatthevaluefunctionparameterremainsina
boundedregion[5,40],whichisautomaticallysatisfiedinfiniteMFGsAppendixH. Specifically,we
needD ≥∥θ ∥,whereθ istheMFEvaluefunctionparameter.
∗ ∗
Combining(4)and(5)givestheupdaterulefortheunifiedparameter:
ξ =Π(ξ −α g (ξ )):=Π (ξ −α ((G (O )⊕G )ξ −(φ⊕ψ)(O ))). (6)
t+1 t t t t Bd1×∆d2 t t ϕ t ψ t t
D
(6)updatestheaction-valuefunctionandpopulationmeasureusingthesameonlineobservations
withthesamelearningrate. ReplacingLine3inAlgorithm1with(6)givesSemiSGDwithLFA.
ThenextsectionshowsthatSemiSGDconvergestoazeropointofthemean-pathsemi-gradient
g¯ (ξ):=E g(ξ;O),whereµ istheobservationdistributioninducedbytheparameterξ. The
ξ O∼µξ ξ
nextpropositionstatesthatthispointisanMFE,thusvalidatingthederivationinthissection.
Proposition2(MFEasastationarypoint). ξisanMFEifandonlyifg¯ (ξ)=0.
ξ
WeproveanextendedversionofProposition2inAppendixD,showingthatthestationarypoint
actuallycorrespondstoaprojectedMFEforgeneral(non-linear)MFGs.
5 Samplecomplexityanalysis
This section provides a finite-sample analysis of the convergence of SemiSGD with LFA, thus
coveringfiniteMFGsasaspecialcase. WedenoteP (s′|s,a):=P(s′|s,a,M)andr (s,a):=
M M
r(s,a,M)forshort. Fortransitionkernels,wedefinethetotalvariationoperatornorm∥P∥ :=
TV
(cid:82)
sup ∥ P(·|s,a)p(ds,da)∥ .Wenowstatetheassumptionsfortheanalysis.
p∈M(S×A),∥p∥TV≤1 TV
Assumption1(LipschitzMDP). ThetransitionkernelandrewardfunctionareLipschitzcontinuous
w.r.t. thepopulationmeasure. Thatis,thereexistspositiveconstantsL andL suchthatforany
P r
state-actionpair(s,a)andpopulationmeasuresM ,M ,wehave
1 2
(cid:112) (cid:112)
∥P −P ∥ ≤(L / d )∥M −M ∥ , ∥r −r ∥ ≤(L / d )∥M −M ∥ .
M1 M2 TV P 2 1 2 TV M1 M2 ∞ r 2 1 2 TV
Assumption2(Lipschitzpolicyoperator). ThereexistsaconstantL suchthatforanystatesand
π
valuefunctionsQ ,Q ,wehave∥Γ (Q )(·|s)−Γ (Q )(·|s)∥ ≤L ∥Q −Q ∥ .
1 2 π 1 π 2 TV π 1 2 ∞
7Assumption3(Uniformergodicity). TheMDPisuniformlyergodicforanyparameteranyvalue
functionQandpopulationmeasureM. Thatis,thereexistsconstantsm≥1,ρ∈(0,1),andµ ,
(Q,M)
suchthatforanyinitialdistributionM ∈D(S),itholdsthat∥µ −Pt M ∥ ≤mρt.
0 (Q,M) (Q,M) 0 TV
Assumptions 1 and 2 are regularity conditions ensuring learning dynamics’ smoothness, i.e., the
updatesarenottoodrasticbetweeniterations. Assumption3isastandardassumptioninforonline
methods with Markovian sampling [5, 40, 2, 36], ensuring the environment is relatively easy to
explore. WecannowboundthemeansquarederrorofSemiSGDrecursively,whichdirectlygives
severalfinite-timeerrorbounds.
Theorem1(One-stepprogress). Letξ = (θ ;η )beanMFEparameter. Let{ξ = (θ ;η )}be
∗ ∗ ∗ t t t
a sequence of parameters generated by SemiSGD. Then, under Assumptions 1 to 3, there exists
w ∈(0,1/2]suchthatiftheLipschitzconstantsaresmallenough(3σmax{L ,L }H +L ≤2w,
P π r
seeLemma9inAppendixF.1),wehave
max{L ,L ,L }2H4
E∥ξ −ξ ∥2≤(1−α w)E∥ξ −ξ ∥2+H2·O(α2logα−1)+ P π r ·O(α3log4α−1),
t+1 ∗ t t ∗ t t w t t
whereH :=(1+γ)D+R+2F canbeseenasthescaleoftheproblem.
Corollary1(Constantstep-size). Givenaconstantstep-sizeα ≡α ,wehave
t 0
E∥ξ −ξ ∥2 ≤e−α0wT∥ξ −ξ ∥2+w−1H2·O(α logα−1).
T ∗ 0 ∗ 0 0
Letα =(logT)/(wT). ThenCorollary1statesE∥ξ −ξ ∥2 =O(cid:0) (H2log2T)/(w2T)(cid:1) ,giving
0 T ∗
anO(ϵ−2log2ϵ−1)samplecomplexityforanϵ-MFE(E∥ξ −ξ ∥≤ϵ).Alinearlydecayingstep-size
T ∗
sequenceimprovesthelogarithmicfactor,givingaconvergencerateofO(cid:0) (H2logT)/(w2T)(cid:1)
(see
(cid:16) (cid:17)
Corollary3). ReducingtofiniteMFGs,thecomplexitybecomesO(cid:101) SAR2 (seeCorollary4).
λ2(1−γ)4T
Remark2(SmallLipschitzconstants). SemiSGDfollowsanon-stationaryMarkovchainwithits
kerneldeterminedbythevaluefunctionandpopulationmeasure,whichareupdatedonthego. When
thekernelissensitivetotheparameterupdate(largeLipschitzconstants),theMarkovchainisrapidly
changing[39],whoseconvergenceisnotwellunderstood. Asaresult,fortheoreticalanalysis,we
requiretheLipschitzconstantstobesmall(seeLemma9),sothedescentdirection(attractionproperty
ofMFE)dominantsthenon-stationarity. Suchrequirementsarenotimplementedintheexperiments.
6 Approximationerrorfornon-linearMFGs
SemiSGDwithPA-LFAappliestonon-linearMFGsaslongasthefeaturemapϕandmeasurebasis
ψarespecified. However,fornon-linearMFGs,itremainsunclear: 1)WhatpointdoesSemiSGD
withPA-LFAconvergeto? 2)WhatfactorscharacterizetheapproximationerrorcausedbyPA-LFA?
Actually, the zero point of the mean-path semi-gradient developed in Section 4 corresponds to a
projectedMFEdefinedby⟨θ,ϕ⟩=Π T ⟨θ,ϕ⟩and⟨η,ψ⟩=Π P ⟨η,ψ⟩,whereΠ andΠ are
ϕ ξ ψ ξ ϕ ψ
orthogonalprojectionoperatorsontothelinearspanofϕandψ,respectively. Theformaldefinitions
andproofofthestatementaredeferredtoAppendixD. ForlinearMFGs,asimagesofT andP are
withinthelinearspanofϕandψ(AppendixC),theprojectedMFEistheMFEitself(Proposition2).
Letξ =(θ ;η )betheconvergencepointofSemiSGDwithPA-LFA(i.e.,thestationarypointw.r.t.
⋄ ⋄ ⋄
thesemi-gradient,ortheprojectedMFE).Let(q ,µ )betheactualMFE.WewriteP :=P for
∗ ∗ ⋄ ξ⋄
short,andsimilarlyforP ,T ,andT . Wehavethefollowingapproximationerrorcharacterization.
∗ ⋄ ∗
Theorem2(Approximationerror). Assumption3indicatestheexistenceofk ∈Nsuchthat
(cid:18) (cid:19) (cid:18) (cid:19)
γσRL γσRL
ϵ :=∥q −⟨ϕ,θ ⟩∥ ≤ γ+ π ϵ + L + √P ϵ +∥q −Π q ∥
q ∗ ⋄ ∞ 1−γ q r (1−γ) d µ ∗ ϕ ∗ ∞
2
(cid:18) (cid:19)
kL
ϵ :=∥µ −⟨ψ,η ⟩∥ ≤ ρk+ √ P ϵ +kL ϵ +k∥P −Π P ∥ .
µ ∗ ⋄ TV d µ π q ∗ ψ ∗ TV
2
IftheLipschitzconstantsaresmallenough: 2L (γσR+(1−γ)k)≤(1−γ)2 and2(L (γσR+
√ π √ P
(1−γ)k)+L (1−γ) d )≤(1+ρ−2ρk)(1−γ) d ,then
r 2 2
2
ϵ +ϵ ≤ (ϵ +kϵ ),
q µ 1−min{γ,ρ} ϕ ψ
8whereϵ :=∥q −Π q ∥ andϵ :=∥P −Π P ∥ areinherentapproximationerrorinduced
ϕ ∗ ϕ ∗ ∞ ψ ∗ ψ ∗ TV
by the projection onto the linear span of basis ϕ and ψ, which is independent of the algorithm.
Moreover,if∥P −P∞∥ =ρ(e.g.,P inducesareversibleMarkovchain),k =1.
∗ ∗ TV ∗
TheapproximationerrorinTheorem2scaleswiththeinherentapproximationerrordeterminedby
thechosenbasis,whichisconsistentwiththeapproximationerrorofLFAforvaluefunction[33].
7 Numericalexperiments
WeconductexperimentsontwoMFGexamplesinautonomousdrivingscenarios[37]: speedcontrol
on a ring road and routing game on a network. We equip FPI with entropy regularization (ER),
fictitiousplay(FP),andonlinemirrordescent(OMD).Thefollowingparametersaresharedforall
methodsandtwoexperiments: totalnumberofsamplesT = 105,withthenumberofouterloops
beingK =200forFPI-typemethods,constantstep-sizeα=10−3,policyoperatorΓ =softmax
π
withalargeinversetemperature(near-greedy). Theconvergenceperformanceincludeexploitability
andmeansquarederror(MSE).WecomparedistributionswiththereferenceMFEdistribution,which
iscalculatedbymodel-basedFPIwithFP.Allresultsareaveragedover10independentrunswith
randominitialization. 95%confidenceregionsareplotted. PleaserefertoAppendixAforthefull
setupoftheexperiments. Inthemainbody,wepresentonlytheresultsofthespeedcontrolproblem
duetopagelimit. Thedetailsoftheothergame(routing)canbefoundinAppendixA.
Speedcontrolonaringroad. WeconsideraspeedcontrolgameonaringroadS ∼ =S1. Thecost
functionr(s,a,µ)=−(b(s)+(1−µ(s)/µ )/2−a/a )2∆s/2,isavariantoftheLighthill-
jam max
Whitham-Richardsfunctionandabonustermbthatencodesthelocationpreference, µ isthe
jam
jamdensity,anda isthemaximumspeed. ERisimplementedwithalowinversetemperatureof
max
104,whileallothermethodsuseaninversetemperatureof109,guaranteeingthepoliciesarenear-
greedy. TheMSE,exploitabity[23],andlearnedpopulationcomparisonarereportedinFigure1a,
Figure2,andFigure3,respectively. TheMSEcomparisondemonstratesthatSemiSGD1)ismore
efficient, with a much faster convergence, 2) is more stable without any stabilization techniques,
3)andismoreaccurate,withalowerMSE.Fortheexploitability,SemiSGDseemstooscillateandbe
outperformedbyER.However,exploitabilityisanoperatordeterminedbytheunderlyingMDP,which
maynotbesmoothanddoesnotdirectlyreflectingthelearningdynamics. Thelearnedpopulation
distributionshowsthatFPIwithERisstillfarfromthereferenceMFE.Wealsoconductacomparison
betweenLFAanddiscretization,whichshowsthatLFAismoreaccuratethandiscretizationforsmall
dimensions(Table2;pleaserefertoAppendixAforthefullreport).
Dim.\Algo. Grid PA-LFA
2 0.25±0.02 0.16±0.04
5 0.27±0.15 0.15±0.01
10 0.17±0.16 0.11±0.04
25 0.22±0.1 0.40±0.04
Figure2: Exploitablity. Figure3: learneddistribution. Table2: LFAVS.DiscretizationonMSE
8 Conclusionanddiscussion
TheproposedSemiSGDmakeslearningMFGsmoreaccessibleandpracticalinreal-worldapplica-
tions,includingbutnotlimitedtoonlineautonomousvehicledriving,real-timepedestrianorcrowd
dynamics,internetpacketrouting,socialopiniondynamics,andepidemiology. Ourmethodologyis
generalizableandcanbeextendedto: (finite)populationgames,graphongames,policyoptimization
forMFGs,andevolutionaryMDPs.
AnopenchallengeistorelaxorremovethesmallLipschitzconstantsassumptioninourtheoretical
analysis. Asdemonstratedintheexperiments,SemiSGDisstablewithoutrequiringtheLipschitz
constantstobesmall. Bridgingthisgaprequiresadeeperunderstandingoftherapidlychanging
Markovchains(RCMCs)inducedbySemiSGD.Itisalsointerestingtoestablishsimilartheoretical
guaranteesformonotoneMFGs,aparallelconditiontothesmoothnessassumption.
9References
[1] B. Anahtarci, C. D. Kariksiz, and N. Saldi. Q-learning in regularized mean-field games.
DynamicGamesandApplications,13(1):89–117,2023.
[2] A.Angiuli,J.-P.Fouque,andM.Laurière. UnifiedreinforcementQ-learningformeanfield
gameandcontrolproblems. MathematicsofControl,Signals,andSystems,34(2):217–271,
2022.
[3] A. Angiuli, N. Detering, J.-P. Fouque, M. Lauriere, and J. Lin. Reinforcement Learning
AlgorithmforMixedMeanFieldControlGames,Feb.2023.
[4] A.Angiuli,J.-P.Fouque,M.Laurière,andM.Zhang. ConvergenceofMulti-ScaleReinforce-
mentQ-LearningAlgorithmsforMeanFieldGameandControlProblems,Dec.2023.
[5] J.Bhandari,D.Russo,andR.Singal. AFiniteTimeAnalysisofTemporalDifferenceLearning
WithLinearFunctionApproximation,Nov.2018.
[6] S.J.BradtkeandA.G.Barto. Linearleast-squaresalgorithmsfortemporaldifferencelearning.
Machinelearning,22(1):33–57,1996.
[7] M. Burger, M. D. Francesco, P. Markowich, and M.-T. Wolfram. Mean field games with
nonlinearmobilitiesinpedestriandynamics,2013.
[8] P.CardaliaguetandS.Hadikhanloo. Learninginmeanfieldgames: Thefictitiousplay. ESAIM:
Control,OptimisationandCalculusofVariations,23(2):569–591,2017.
[9] X.Chen,S.Liu,andX.Di. Learningdualmeanfieldgamesongraphs. Sept.2023.
[10] X.Chen,S.Liu,andX.Di.Ahybridframeworkofreinforcementlearningandphysics-informed
deeplearningforspatiotemporalmeanfieldgames. InProceedingsofthe2023International
ConferenceonAutonomousAgentsandMultiagentSystems,pages1079–1087,2023.
[11] L. Condat. Fast projection onto the simplex and the $$\pmb {l}_\mathbf {1}$$ l 1 ball.
MathematicalProgramming,158(1-2):575–585,July2016. ISSN0025-5610,1436-4646. doi:
10.1007/s10107-015-0946-6.
[12] K.CuiandH.Koeppl. ApproximatelySolvingMeanFieldGamesviaEntropy-Regularized
DeepReinforcementLearning,July2022.
[13] K.Cui,A.Tahir,G.Ekinci,A.Elshamanhory,Y.Eich,M.Li,andH.Koeppl. ASurveyon
Large-PopulationSystemsandScalableMulti-AgentReinforcementLearning,Sept.2022.
[14] R.Élie,J.P’erolat,M.Laurière,M.Geist,andO.Pietquin. Ontheconvergenceofmodelfree
learninginmeanfieldgames. InAAAIConferenceonArtificialIntelligence,2019.
[15] X. Guo, A. Hu, R. Xu, and J. Zhang. Learning mean-field games. Advances in Neural
InformationProcessingSystems,32,2019.
[16] J.Huang,B.Yardim,andN.He. Onthestatisticalefficiencyofmean-fieldreinforcementlearn-
ingwithgeneralfunctionapproximation. InInternationalConferenceonArtificialIntelligence
andStatistics,pages289–297.PMLR,2024.
[17] M.Huang,R.P.Malhamé,andP.E.Caines.Largepopulationstochasticdynamicgames:Closed-
loopMcKean-VlasovsystemsandtheNashcertaintyequivalenceprinciple. Communications
inInformationandSystems,6(3):221–252,2006.
[18] D.IsaacsonandG.R.Luecke. StronglyergodicMarkovchainsandratesofconvergenceusing
spectralconditions. StochasticProcessesandtheirApplications,7(1):113–121,1978.
[19] C.Jin,Z.Yang,Z.Wang,andM.I.Jordan. ProvablyEfficientReinforcementLearningwith
LinearFunctionApproximation,Aug.2019.
[20] A. Lachapelle and M.-T. Wolfram. On a mean field game approach modeling congestion
andaversioninpedestriancrowds. TransportationResearchPartB:Methodological,45(10):
1572–1589,2011. ISSN0191-2615.
10[21] J.-M.LasryandP.-L.Lions. Meanfieldgames. Japanesejournalofmathematics,2(1):229–260,
2007.
[22] K.Łatuszyn´ski,B.Miasojedow,andW.Niemiro. Nonasymptoticboundsontheestimation
errorofmcmcalgorithms. 2013.
[23] M.Laurière,S.Perrin,M.Geist,andO.Pietquin. LearningMeanFieldGames: ASurvey,June
2022.
[24] M.Lauriere,S.Perrin,S.Girgin,P.Muller,A.Jain,T.Cabannes,G.Piliouras,J.Perolat,R.Elie,
O.Pietquin,andM.Geist. ScalableDeepReinforcementLearningAlgorithmsforMeanField
Games. In Proceedings of the 39th International Conference on Machine Learning, pages
12078–12095.PMLR,June2022.
[25] W.Mao,H.Qiu,C.Wang,H.Franke,Z.Kalbarczyk,R.K.Iyer,andT.Basar. AMean-Field
GameApproachtoCloudResourceManagementwithFunctionApproximation.
[26] F.S.Melo,S.P.Meyn,andM.I.Ribeiro. Ananalysisofreinforcementlearningwithfunction
approximation. InProceedingsofthe25thInternationalConferenceonMachineLearning-
ICML’08,pages664–671,Helsinki,Finland,2008.ACMPress. ISBN978-1-60558-205-4.
doi: 10.1145/1390156.1390240.
[27] S.P.MeynandR.L.Tweedie. MarkovChainsandStochasticStability. SpringerScience&
BusinessMedia,2012.
[28] A. Yu. Mitrophanov. Sensitivity and Convergence of Uniformly Ergodic Markov Chains.
JournalofAppliedProbability,42(4):1003–1014,2005. ISSN0021-9002.
[29] J.Perolat,S.Perrin,R.Elie,M.Laurière,G.Piliouras,M.Geist,K.Tuyls,andO.Pietquin.
ScalingupMeanFieldGameswithOnlineMirrorDescent,Feb.2021.
[30] S.Perrin,J.Pérolat,M.Laurière,M.Geist,R.Elie,andO.Pietquin. Fictitiousplayformean
field games: Continuous time analysis and applications. Advances in Neural Information
ProcessingSystems,33:13199–13213,2020.
[31] S. Perrin, M. Laurière, J. Pérolat, M. Geist, R. Élie, and O. Pietquin. Mean Field Games
Flock! The Reinforcement Learning Way. In Proceedings of the Thirtieth International
Joint Conference on Artificial Intelligence, pages 356–362, Montreal, Canada, Aug. 2021.
InternationalJointConferencesonArtificialIntelligenceOrganization. ISBN978-0-9992411-9-
6. doi: 10.24963/ijcai.2021/50.
[32] R.S.SuttonandA.G.Barto. ReinforcementLearning: AnIntroduction. MITpress,2018.
[33] J.TsitsiklisandB.VanRoy. Ananalysisoftemporal-differencelearningwithfunctionapproxi-
mation. IEEETransactionsonAutomaticControl,42(5):674–690,May1997. ISSN00189286.
doi: 10.1109/9.580874.
[34] M. A. uz Zaman, K. Zhang, E. Miehling, and T. Bas¸ar. Reinforcement Learning in Non-
StationaryDiscrete-TimeLinear-QuadraticMean-FieldGames,Oct.2020.
[35] Q. Xie, Z. Yang, Z. Wang, and A. Minca. Learning While Playing in Mean-Field Games:
ConvergenceandOptimality. InProceedingsofthe38thInternationalConferenceonMachine
Learning,pages11436–11447.PMLR,July2021.
[36] M.A.U.Zaman,A.Koppel,S.Bhatt,andT.Basar. Oracle-freeReinforcementLearningin
Mean-Field Games along a Single Sample Path. In Proceedings of The 26th International
ConferenceonArtificialIntelligenceandStatistics,pages10178–10206.PMLR,Apr.2023.
[37] C.Zhang,X.Chen,andX.Di. ASingleOnlineAgentCanEfficientlyLearnMeanFieldGames,
May2024.
[38] C.Zhang,H.Wang,A.Mitra,andJ.Anderson. Finite-TimeAnalysisofOn-PolicyHeteroge-
neousFederatedReinforcementLearning,Jan.2024.
11[39] S. Zhang, R. T. D. Combes, and R. Laroche. On the Convergence of SARSA with Linear
FunctionApproximation. InProceedingsofthe40thInternationalConferenceonMachine
Learning,pages41613–41646.PMLR,July2023.
[40] S.Zou,T.Xu,andY.Liang. Finite-sampleanalysisforsarsawithlinearfunctionapproximation.
Advancesinneuralinformationprocessingsystems,32,2019.
12Appendix
Table of Contents
A Additionalexperiments 14
A.1 Generalsetupandremark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.2 Speedcontrolonaringroad. . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.3 Routinggameonanetwork. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
B Notation 17
C ProofofProposition1 18
D ProjectedMFEandstationarypoint 19
E Preliminarylemmas 20
F Samplecomplexityanalysis 24
F.1 Keylemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
F.2 ProofofTheorem1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
F.3 ProofofCorollaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
G Approximationerroranalysis 31
H Meanfieldgameswithfinitestate-actionspace 33
H.1 Implicitregularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
H.2 Convergencerate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
13A Additionalexperiments
A.1 Generalsetupandremark
Generalsetup. Wefocusontwoprimarymetrics: themeansquarederror(MSE)ofthepopulation
distributionandtheexploitabilityofthepolicy.
Reference MFE and mean squared error. We compare our results with the reference MFE
distributionM ,whichiscalculatedbymodel-basedFPIwithFP.Themodel-basedFPIconsistsof
ref
avalueiteration[32]forthebestresponsesanddirectcomputationoftheinducedpopulationmeasure
usingthetransitionoperator. Then,the(populationmeasure)meansquarederror(MSE)iscalculated
as
MSE(M):=∥M −M ∥2 =(cid:88) (M(s)−M (s))2.
ref 2 ref
s∈S
Notethatthisisequivalenttothe1-Wassersteindistancebetweenthetwomeasuresforfinitestate
spaces.
Exploitablity. Perrinetal.[30]definestheexploitabilityofapolicyasfollows:
exploitability(π):=maxE V(s;π′,µ )−E V(s;π,µ ),
π′
s∼µπ π s∼µπ π
whereV(s;π,µ)isthevaluefunctiondeterminedbypolicyπ andpopulationdistributionµ. The
exploitabilityresultsinourexperimentsarecalculatedusingmodel-basedvalueiteration. Weremark
thatalthoughexploitabilityisausefulmetrictoevaluatetheperformanceofthelearnedpolicy,it
isanoperatordeterminedbytheunderlyingMDP,whichmaynotbesmoothanddoesnotdirectly
reflectthelearningdynamics.
A.2 Speedcontrolonaringroad.
Weconsideraspeedcontrolgameofonaringroad,i.e.,theunitcircleS1 ∼ =[0,1).Atlocations∈S1,
therepresentativevehicleselectsaspeeda,andthenmovestothenextlocationfollowingtransition
s′ =s+a∆t (mod 1),where∆tisthetimeintervalbetweentwoconsecutivedecisions. Without
lossofgenerality,weassumethatthespeedisboundedby1,i.e.,thespeedspaceisalso[0,1). Then
wediscretizeboththelocationspaceandthespeedspaceusingagranularityof∆s=∆a=0.02.
Thus, both our discretized state (location) space and action (speed) space can be represented by
S = A = {0,0.02,...,0.98} ∼ = [50]. BytheCourant-Friedrichs-Lewycondition,wechoosethe
timeintervaltobe∆t=0.02≤∆s/maxa. Theobjectiveofavehicleistomaintainsomedesired
speedwhileavoidingcollisionswithothervehicles. Thus,itneedstoreducethespeedinareaswith
high population density. A classic cost function for this goal is the Lighthill-Whitham-Richards
function:
1(cid:18)(cid:18) µ(s)(cid:19)
a
(cid:19)2
r(LWR)(s,a,µ)=− 1− − ∆s,
2 µ a
jam max
whereµ isthejamdensity, anda isthemaximumspeed. However, inaninfinitehorizon
jam max
game,thiscostfunctioninducesatrivialMFNE,wheretheequilibriumpolicyandpopulationare
bothconstantacrossthestatespace. Therefore,weintroduceastimulustermbthatvariesacross
differentlocations:
1(cid:18) 1(cid:18) µ(s)(cid:19)
a
(cid:19)2
r(s,a,µ)=− b(s)+ 1− − ∆s,
2 2 µ a
jam max
wherethefactorofone-halfbeforethepopulationdistributiontermisincludedtoaccountforthe
presenceofthenewstimulusterm. ThisnewcostfunctionmakestheMFNEmorecomplexand
correspondstoreal-worldsituationswherevehiclesmayhavedistinctdesiredspeedsatdifferent
locations due to environmental variations. Specifically, we choose the stimulus term as b(s) =
0.2(sin(4πs)+1),andsetµ =3/S anda =1. Thediscountfactorissetasγ =0.98.
jam max
Theconvergenceofmodel-basedFPIwithFPisshowninFigure4,arguingthenear-optimalityofthe
referenceMFE.
14Figure4: Exploitablityofmodel-basedFPIwithFP.
A.2.1 ComparisonofPA-LFAandgriddiscretization
TodemonstratetheeffectivenessofPA-LFA,wecompareitwithgriddiscretization. Thereference
MFEiscalculatedusingagriddiscretizationwithagranularityof1/50. WeonlyapplyLFAtothe
populationmeasureestimate,asthisisourmainfocus. Themeasurebasisischosenas
ψ (s)=N(0)−N(tan((s−s )π)),
i i
whereN isthenormaldistribution,ands isthecenterofthebasisfunction. WerunSemiSGDd
i 2
states(griddiscretization)andd basisfunctions(PA-LFA)forT =104steps. Otherparametersare
2
thesameasthegeneralsetup. ThefinalMSE(accuracy)comparisonisreportedinTable2. TheMSE
curvesandlearnedpopulationdistributionsareshowninFigure6andFigure5,respectively. Aswe
cansee,whend issmall,PA-LFAgeneralizesbetterthangriddiscretization,astherepresentation
2
capacity of spanψ is larger than the grid discretization. However, when d is large, i.e., the
2
discretizationisfineenough,griddiscretizationperformsbetterthanPA-LFA.Thisisbecausethe
PA-LFAapproximationerrorislargerthanthediscretizationerrorwhenthediscretizationisfine
enough.
(a)d =2. (b)d =5. (c)d =10. (d)d =25.
2 2 2 2
Figure5: LFAversusdiscretizationonlearneddistributions.
(a)d =2. (b)d =5. (c)d =10. (d)d =25.
2 2 2 2
Figure6: LFAversusdiscretizationonMSE.
15A.3 Routinggameonanetwork.
WeconsideraroutinggameontheSiouxFallsnetwork,3agraphwith24nodesand74directededges.
Wedesignatenode1asthestartingpointandnode20asthedestination. Toconstructaninfinite-
horizongame,weaddarestartedgee fromthedestinationbacktothestartingpoint. Oneach
75
edge,avehicleselectsitsnextedgetotravelto. Weconsideradeterministicenvironment,meaning
thatthevehiclewillfollowthechosenedgewithoutanyrandomness. Therefore,boththestatespace
andtheactionspacecanberepresentedbytheedgeset,i.e.,S =A={e ,...,e }∼ =[75],where
1 75
e istherestartedge. Itisworthnotingthatavehiclecanonlyselectfromtheoutgoingedgesofits
75
currentlocationasitsnextedge.
Theobjectiveofavehicleistoreachthedestinationasfastaspossible. Duetocongestion,avehicle
spendsalongertimeonanedgewithhigherpopulationdistribution. Specifically,thecost(time)ona
non-restartedgeisr(cong.)(s,a,µ)=−c µ(s)21{s̸=e },wherec isacostconstant. Todrivethe
1 75 1
vehicletothedestination,weimposearewardattherestartedge: r(term.)(s,a,µ)=c 1{s=e }.
2 75
Together,wegetthecostfunction:
r(s,a,µ)=−c µ(s)21{s̸=e }+c 1{s=e }.
1 75 2 75
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
congestioncost terminalreward
Wesetc =105andc =10. Theotheralgorithmicparametersarechosenasfollows: thediscount
1 2
factorγ =0.8,theinitialstateisuniformlysampled,theinitialvaluefunctionissetasall-zero,the
initialpopulationisrandomlygenerated.
Similarly,ERisimplementedwithalowinversetemperatureof10−3,whileallothermethodsuse
aninversetemperatureof102. TheperformancecomparisonisreportedinFigure7. Notably,all
methods except FPI with ER have an oscillatory behavior in the exploitability around the same
value. Thisisduetothehighlynon-smoothnatureoftheunderlyingenvironmentandthechoice
ofnear-greedypolicies. ApplyingERtoallmethodsresultsinsimilarperformance,asshownin
Figure8.
(a)MSE. (b)Exploitablity. (c)Learnedpopulationdistribution.
Figure7: Networkrouting.
(a)MSEunderER. (b)ExploitablityunderER.
Figure8: Networkroutingwithregularization.
3The topology of the network is available at https://github.com/bstabler/
TransportationNetworks.
16B Notation
Table3providesasummaryofthesymbolnotationusedinthispaper. Weintroducesomesupple-
mentarynotationtoassistanalysis.
Table3: Notation
Notation Definition
D(X),M(X) SpaceofprobabilityandsignedmeasuresonspaceX
∆ Probabilitysimplex
δ Diracdeltameasure
a,A Actionandactionspace
s,S Stateandstatespace
r,R Rewardfunctionanditssupremumnormbound
γ Discountfactor
P,P Transitionkernelandoperator
T Bellmanoperator
Q,q Action-valuefunction
M,µ Populationmeasure
π,Γ Policyandpolicyoperator
π
ϕ State-actionfeaturemap
ψ Statemeasurebasis
F Normboundofψ
d Feature/measurespacedimension
ν Linearrewardfunctionparameter
ω,Ω Lineartransitionkernelparameter
η Populationmeasureparameter
θ Valuefunctionparameter
ξ,Ξ Concatenatedparameteranditsparameterspace
Π Projectionoperator
D Valuefunctionprojectionbound
H Problemscale(H =(1+γ)D+R+2F)
g Semi-gradient
G ,φ Temporaldifferenceoperator
ϕ
G Grammatrix
L Lipschitzconstant
w Contractionconstant
α Step-size
m,ρ,σ,k Ergodicityconstants
τ Backtrackingperiod
Concatenationanddirectsum. Weconsiderfinite-dimensionalEuclideanspacesastheparameter
spaces. Thus,foranyx∈Rd1 andy ∈Rd2,wedenotetheirconcatenationas(x;y)∈Rd1+d2. We
sometimeswriteitasthegeneraldirectsumbetweentwovectorsx⊕y ∈Rd1⊕Rd2 ∼ =Rd1+d2. For
matricesandoperators,wehave(A⊕B)(x⊕y)=Ax⊕By. Thisnotationisespeciallyusefulfor
handlingtheunifiedparameterξ =θ⊕η(see,e.g.,Lemma7). Additionally,weuseΞtodenotethe
unifiedparameterspaceΞ:=Rd1 ⊕Rd2.
Unprojectedparameters. Wedenotetheunprojectedparametersasξ˘,θ˘,andη˘. Thatis,
ξ =Π(ξ˘)=Π(ξ −α g (ξ )).
t+1 t t t t
Foranyparameterξ intheprojectedregion,wehave∥ξ −Π(ξ˘)∥≤∥ξ −ξ˘∥.
∗ ∗ ∗
Steadydistributions. Wedenoteµ ∈D(S)asthesteadystatedistributioninducedbyparameter
ξ
ξ =(θ;η),i.e.,bypolicyΓ (θ)andtransitionkernelandrewardfunctiondeterminedbypopulation
π
measureparameterη. µ isthemarginaldistributionofthefollowingtwosteadydistributions:
ξ
µ†(s,a):=µ (s)π (a|s), µ‡(s,a,s′,a′):=µ (s)π (a|s)P(s′|s,a,η)π (a′|s′).
ξ ξ θ ξ ξ θ θ
17WewriteE astheexpectationoverthesteadydistributioninducedbyξ;itshouldbeclearfromthe
ξ
contextwhichsteadydistributionisused.
Semi-gradients and temporal difference operators. Here we review the definition of semi-
gradients. Withaslightabuseofnotation,weuseasingleoperatorgtoreturnsemi-gradientsforboth
theaction-valuefunctionandpopulationmeasure. Itshouldbeclearfromtheargumentofgwhich
parameterthesemi-gradientisfor. Specifically,withasampletupleO =(s,a,r,s′,a′),wehave
g(ξ;O)=(g(θ;O);g(η;O))=(G (O)θ−φ(O);G η−ψ(s′)),
ϕ ψ
where G is the gram matrix of the measure basis ψ, and G and φ are the temporal difference
ψ ϕ
operatorsdefinedas
G (O)=ϕ(s,a)(ϕ(s,a)−γϕ(s′,a′))T, φ(O)=ϕ(s,a)r.
ϕ
Notably,G isaconstantmatrix,andG isaGram-likematrixthatdependsonthesampletuple.
ψ ϕ
Therefore,wesometimesdropthesubscriptϕinG anduseothersubscriptstoindicateitsdependence
ϕ
onthesampletuple. ItshouldbeclearthatanyGwithasubscriptotherthanψreferstoG . When
ϕ
thesampletupleisanonlineobservationattimestept, i.e., O = (s ,a ,r ,s ,a ), weuse
t t t t t+1 t+1
shorthand
g (·)=g(·;O ), G =G (O ), φ =φ(O ), ψ =ψ(O ).
t t t ϕ t t t t t
BacktrackingisananalysistechniqueintroducedtotacklerapidlychangingMarkovchains[40,39,
38]. ItconsidersavirtualstationaryMarkovchainbybacktrackingaperiodτ,fixingtheparameter
ξ ,andthensamplingtheMarkovianobservationswiththefixedparameter. Bytheergodicityof
t−τ
stationaryMarkovchains(Assumption3),thevirtualtrajectoryrapidlyconvergestothestationary
distributioninducedbyξ t−τ. WedenoteO(cid:101)tasthevirtualobservationtupleonthisvirtualtrajectory
attimet. Whenweconsiderthesemi-gradientsonthisvirtualtrajectory,wewriteoutitsdependence
onO(cid:101)texplicitly:
g t−τ(·;O(cid:101)t), G t−τ(O(cid:101)t), φ t−τ(O(cid:101)t), ψ t−τ(O(cid:101)t)
withthesubscriptindicatingthebacktrackingperiodτ.
Mean-pathsemi-gradientsaretheexpectationofsemi-gradientsoverasteadydistributioninducedby
aparameter:
g¯ =E g(ξ;O),
ξ ξ
wherethesubscriptξindicatesthattheobservationtupleOfollowsthesteadydistributioninduced
byξ =(θ;η). Moreexplicitly,thestatesfollowsthesteadydistributioncorrespondingtotransition
kernelP(·|·,·,η),andtheactionsfollowpolicyΓ (θ),andtherewardsaregeneratedbyr(·,·,η).
π
Similarly,wehave
G¯ =E G (O), φ¯ =E φ(O), ψ¯ =E ψ(s′).
ξ ξ ϕ ξ ξ ξ ξ
Whentheparameterhasasubscriptξ ,wealsouseshorthand
◦
g¯ =g¯ , A¯ =A¯ , ¯b =¯b , ψ¯ =ψ¯ .
◦ ξ◦ ◦ ξ◦ ◦ ξ◦ ◦ ξ◦
Forexample,g¯ =g¯ andg¯ =g¯ .
t ξt ∗ ξ∗
C ProofofProposition1
Proof. BytheBellmanequation,wehave
(cid:90)
Qπ (s,a)=r(s,a,M)+γ Qπ (s′,a′)π(a′|s′)P(s′|s,a,M)ds′da′
M M
S×A
(cid:90)
=⟨ϕ(s,a),ν ⟩+γ Qπ (s′,a′)π(a′|s′)⟨ϕ(s,a),Ω ψ(s′)⟩ds′da′
M M M
S×A
(cid:42) (cid:43)
(cid:90)
= ϕ(s,a),ν +γ Qπ (s′,a′)π(a′|s′)Ω ψ(s′)ds′da′ .
M M M
S×A
(cid:124) (cid:123)(cid:122) (cid:125)
θ
18Bythetransitionequation,wehave
(cid:90) (cid:90)
µπ (s′)= P(s′|s,a,M)π(a|s)µπ (s)dsda= ⟨ϕ(s,a),Ω ψ(s′)⟩π(a|s)µπ (s)dsda
M M M M
S×A S×A
(cid:42) (cid:43)
(cid:90) (cid:90)
= (cid:10) ψ(s′),ΩT ϕ(s,a)(cid:11) π(a|s)µπ (s)dsda= ψ(s′), ΩT ϕ(s,a)π(a|s)µπ (s)dsda .
M M M M
S×A S×A
(cid:124) (cid:123)(cid:122) (cid:125)
η
D ProjectedMFEandstationarypoint
Wecallavaluefunction-populationmeasurepairastationarypointifthemean-pathsemi-gradient
evaluated at this point is zero. Section 5 and Appendix F show that SemiSGD converges to a
stationarypoint. Ideally,wewantthestationarypointtobeanMFE(Definition1). Proposition1
andAppendixCshowthatforlinearMFGs,imagesofT andP arewithinthelinearspanofϕand
ψ,indicatingthelinearstructureoftheMFE.However,thisdoesnotholdforgeneral(non-linear)
MFGs,hintingthediscrepancybetweenthestationarypointandtheMFEfornon-linearMFGs.
WedefinetheprojectedMFEusingtheBellmanoperatorandtransitionoperatorcomposedwiththe
projectionoperators.
Definition3(ProjectedMFE). Wesayξ =(θ;η)constitutesaprojectedMFEif
⟨ϕ,θ⟩=Π T ⟨ϕ,θ⟩, ⟨ψ,η⟩=Π P ⟨ψ,η⟩,
ϕ ξ ψ ξ
whereΠ andΠ areorthogonalprojectionoperatorsontothelinearspanofϕandψ,respectively.
ϕ ψ
Itshouldbenotedthattheprojectionoperatorsaredeterminedbytheinnerproductstructureofthe
functionspaces. Theycanbeexplicitlyexpressedas
Π =ϕT(⟨ϕ,ϕT⟩ )−1⟨ϕ,·⟩ ,
ϕ ◦ ◦
whereϕisthefunctionbasisand⟨·,·⟩ isthechoseninnerproduct. Specifically,wechoosetheL
◦ 2
innerproductonM(S),giving
Π = ψTG−1⟨ψ,·⟩ .
ψ ψ L2
FortheprojectionactingonT ,wechoosetheinnerproductinducedbythesteadydistributionµ ,
ξ ξ
i.e.,⟨f,g⟩ =(cid:82) f(o)g(o)µ‡(do). Then,wehave
µξ ξ
Π ϕT
ξ
=ϕTG(cid:98)− ξ1⟨ϕ,T ξ·⟩ µξ,
whereG(cid:98)ξ istheGrammatrixofϕw.r.t. theinnerproduct⟨·,·⟩ µξ. NotethatG(cid:98)ξ isdifferentfromthe
TDoperatorG orG definedinAppendixB,whichisonlyGram-like.
ϕ ξ
WearenowreadytoproveageneralizedversionofProposition2. RecallthatforlinearMFGs,the
projectedMFEistheMFEitself.
Proposition3(ProjectedMFEasastationarypoint). ξisaprojectedMFEifandonlyifg¯ (ξ)=0.
ξ
Proof. Foraparameterξ =(θ;η),bythedefinitionofmean-pathsemi-gradients,wehave
g¯ (θ)=E (cid:2) ϕ(s,a)(cid:0) ϕT(s,a)θ−γϕT(s′,a′)θ−r(s,a,η)(cid:1)(cid:3) ,
ξ ξ
g¯ (η)=E [G η−ψ(s′)],
ξ ξ ψ
wheretheobservationtuple(s,a,s′,a′)followsthesteadydistributioninducedbyξ. Ontheother
hand,bythedefinitionoftheprojectionoperators,wehave
(Π ϕT ξ−Id)⟨ϕ,θ⟩=ϕTG(cid:98)− ξ1⟨ϕ,T ξ⟨ϕ,θ⟩⟩
µξ
−ϕTθ,
19whereIdistheidentityoperator. Supposeϕislinearlyindependent. Then,weget
(Π ϕT ξ−Id)⟨ϕ,θ⟩=0 ⇐⇒G(cid:98)− ξ1⟨ϕ,T ξ⟨ϕ,θ⟩⟩
µξ
=θ
⇐⇒⟨ϕ,T ξ⟨ϕ,θ⟩⟩
µξ
=G(cid:98)ξθ
⇐⇒ (cid:10) ϕ,E (cid:2) r(·,·,η)+γϕT(s′,a′)θ(cid:3)(cid:11) −E [ϕ(s,a)ϕT(s,a)]θ =0
ξ µξ ξ
⇐⇒E (cid:2) ϕ(s,a)(cid:0) r(s,a,η)+γϕT(s′,a′)θ(cid:1)(cid:3) −E [ϕ(s,a)ϕT(s,a)θ]=0
ξ ξ
⇐⇒E (cid:2) ϕ(s,a)(cid:0) ϕT(s,a)θ−γϕT(s′,a′)θ−r(s,a,η)(cid:1)(cid:3) =0
ξ
⇐⇒g¯ (θ)=0.
ξ
Similarly,fortheprojectedtransitionoperator,wehave
(Π P −Id)⟨ψ,η⟩=0 ⇐⇒ψTG−1⟨ψ,P ⟨ψ,η⟩⟩ −ψTη =0
ψ ξ ψ ξ L2
⇐⇒⟨ψ,P ⟨ψ,η⟩⟩ =G η
ξ L2 ψ
(cid:90)
⇐⇒ ψ(s′)P(s′|s,a,η)π (a|s)ψT(s)ηdsdads′−G η =0
θ ψ
S2×A
⇐⇒E [ψ(s′)]−G η =0
ξ ϕ
⇐⇒g¯ (η)=0.
ξ
Therefore,byDefinition3,ξisaprojectedMFEifandonlyifg (ξ)=0.
ξ
E Preliminarylemmas
Wepresentsomepreliminarylemmasthatareusedthroughouttheanalysis.
Lemma2(Normrelations). Foranyvectorsx,y,wehave
• ∥x⊕y∥ =∥x∥ +∥y∥ , ∥x⊕y∥2 ≤∥y∥2+∥x∥2.
1 1 1 1 1 1
• ∥x⊕y∥ ≤∥x∥ +∥y∥ , ∥x⊕y∥2 =∥y∥2+∥x∥2.
2 2 2 2 2 2
• ∥x⊕y∥ =max{∥x∥ ,∥y∥ }, ∥x⊕y∥2 ≤∥y∥2 +∥x∥2 .
∞ ∞ ∞ ∞ ∞ ∞
• ∥x∥ ∥y∥ ≤ 1∥x⊕y∥2.
1 1 4 1
• ∥x∥ ∥y∥ ≤ 1∥x⊕y∥2.
2 2 2 2
• ∥x∥ ∥y∥ ≤∥x⊕y∥2 .
∞ ∞ ∞
Proof. Allrelationsarebasicfactsofnormsandcanbeeasilyverified.
Lemma3(Gradientbounds). Foranyparameterξ =(θ;η)andanyobservationtupleO,wehave
∥g(θ;O)∥≤(1+γ)∥θ∥+R,
∥g(η;O)∥≤F∥η∥+F.
Moreover,suppose∥θ∥≤Dand∥η∥ =1. LetH :=(1+γ)D+R+2F. Then,wehave
1
∥g(ξ;O)∥≤H.
Proof. Bydefinition,wehave
∥g(θ;O)∥=∥G (O)θ−φ(O)∥≤∥G (O)∥∥θ∥+∥φ(O)∥
ϕ ϕ
≤∥ϕ(s,a)∥∥ϕ(s,a)−γϕ(s′,a′)∥∥θ∥+∥ϕ(s,a)∥|r(s,a,η)|
≤(1+γ)∥θ∥+R,
whereweusethefactthat∥ϕ(s,a)∥≤1. Similarly,wehave
∥g(η;O)∥≤∥g(η;O)∥ =∥G η−ψ(s′)∥ ≤∥G ∥ ∥η∥ +∥ψ(s′)∥ .
1 ψ 1 ψ op 1 1
20TheoperatornormofG satisfies
ψ
(cid:13)(cid:90) (cid:13) (cid:90)
∥G ψ∥ op = sup ∥G ψη∥ 1 = sup
(cid:13)
(cid:13)
ψ(s)ψT(s)ηds(cid:13)
(cid:13) ≤ sup ∥ψ(s)∥ 1⟨ψ,η⟩ds≤F,
(cid:13) (cid:13)
∥η∥1=1 ∥η∥1=1 S 1 ∥η∥1=1 S
wherethelastinequalityusesthenormboundofψandthefactthat⟨ψ,η⟩isaprobabilitymeasure.
Therefore,weget
∥g(η;O)∥≤2F.
Then,Lemma2indicatesthat∥g(ξ;O)∥≤H giventhat∥θ∥≤D.
To be more general, Assumptions 1 and 2 are stated in terms of the differences of population
measuresandvaluefunctions. Fortheeaseofpresentation,wewilldevelopourresultsintermsof
theparameters,andstatethemoregeneralresultsintermsofthedifferencesofpopulationmeasures
andvaluefunctionswithoutproof. WeneedtofirsttranslatetheLipschitznessassumptionsinterms
oftheparameters. Intherestofthepaper,werefertothefollowinglemmawhenweneedtousethe
Lipschitznessassumptionsintermsoftheparameters.
Lemma4(Lipschitznessinparameters). Assumption1andAssumption2implythatforanytwo
parametersξ =(θ ;η )andξ =(θ ;η ),wehave
1 1 1 2 2 2
∥P −P ∥ ≤L ∥η −η ∥, ∥r −r ∥ ≤L ∥η −η ∥,
η1 η2 TV P 1 2 η1 η2 ∞ r 1 2
∥π (·|s)−π (·|s)∥ ≤L ∥θ −θ ∥.
θ1 θ2 TV π 1 2
Proof. Theproofisstraightforwardnoticingthefactthat
∥⟨ψ,η⟩∥ TV
=(cid:90) |ηTψ(s)|ds=(cid:90)
∥η∥
1(cid:12) (cid:12)
(cid:12)
(cid:12)∥η ηT
∥
ψ(s)(cid:12) (cid:12)
(cid:12) (cid:12)ds≤∥η∥ 1
≤(cid:112)
d 2∥η∥,
S S 1
and
∥⟨ϕ,θ⟩∥ =∥θTϕ(s,a)∥ ≤∥∥θ∥∥ϕ(s,a)∥∥ ≤∥θ∥.
∞ ∞ ∞
Lemma5(Lipschitzsteadydistributions). Foranytwosteadydistributionsµ‡ andµ‡ inducedby
ξ1 ξ2
parametersξ =(θ ;η )andξ =(θ ;η ),wehave
1 1 1 2 2 2
∥µ‡ −µ‡ ∥ ≤σL(∥θ −θ ∥+∥η −η ∥),
ξ1 ξ2 TV 1 2 1 2
whereσ :=2+nˆ+mρnˆ/(1−ρ),nˆ :=(cid:6) log m−1(cid:7) ,andL=max{L ,L }. Involvedconstants
ρ P π
aredefinedinAssumptions1to3. Sinceµ† andµ aremarginaldistributionsofµ‡,asacorollary,
ξ ξ ξ
wehave
∥µ† −µ† ∥ ≤σL(∥θ −θ ∥+∥η −η ∥),
ξ1 ξ2 TV 1 2 1 2
∥µ −µ ∥ ≤σL(∥θ −θ ∥+∥η −η ∥).
ξ1 ξ2 TV 1 2 1 2
Proof. Wefirstprovethelastinequalityinthelemma. ByMitrophanov[28,Corollary3.1],wehave
∥µ −µ ∥ ≤(σ−2)∥P −P ∥ ,
ξ1 ξ2 TV ξ1 ξ2 TV
whereP representsthetransitionkerneldeterminedbypolicyΓ (θ)andpopulationmeasureη,and
ξ π
(cid:13)(cid:90) (cid:13) (cid:13)(cid:90) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
∥P ξ∥ TV := sup (cid:13) q(s)P(·|s,θ,η)ds(cid:13) = sup (cid:13) q(s)π θ(a|s)P(·|s,a,η)dsda(cid:13) .
(cid:13) (cid:13) (cid:13) (cid:13)
q∈M(S) S TV q∈M(S) S×A TV
∥q∥TV=1 ∥q∥TV=1
Bythetriangleinequality,wehave
∥µ −µ ∥ ≤σ∥P −P ∥ +σ∥P −P ∥ . (7)
ξ1 ξ2 TV (θ1;η1) (θ1;η2) TV (θ1;η2) (θ2;η2) TV
21Forthefirstterm,byAssumption1,wehave
(cid:13)(cid:90) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)P (θ1;η1)−P (θ1;η2)(cid:13)
TV
= sup (cid:13)
(cid:13)
q(s)π θ1(a|s)(P(·|s,a,η 1)−P(·|s,a,η 2))dsda(cid:13)
(cid:13)
q∈M(S) S×A TV
∥q∥TV=1
(cid:13)(cid:90) (cid:13)
(cid:13) (cid:13)
≤ sup (cid:13) q(s,a)(P(·|s,a,η 1)−P(·|s,a,η 2))dsda(cid:13)
(cid:13) (cid:13)
q∈M(S×A) S×A TV
∥q∥TV=1
=∥P −P ∥
η1 η2 TV
≤L ∥η −η ∥.
P 1 2
Similarly,forthesecondtermin(7),byAssumption2,wehave
(cid:13)(cid:90) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)P (θ1;η2)−P (θ2;η2)(cid:13)
TV
= sup (cid:13)
(cid:13)
q(s)P(·|s,a,η 2)(π θ1(a|s)−π θ2(a|s))dsda(cid:13)
(cid:13)
q∈D(S) S×A TV
∥q∥TV=1
(cid:90)
≤ sup q(s)P(s′|s,a,η )|π (a|s)−π (a|s)|dadsds′
2 θ1 θ2
q∈D(S) S2×A
∥q∥TV=1
(cid:90)
= sup q(s)∥π (·|s)−π (·|s)∥ ds
θ1 θ2 TV
q∈D(S) S
∥q∥TV=1
(cid:90)
≤ sup q(s)·L ∥θ −θ ∥ds
π 1 2
q∈D(S) S
∥q∥TV=1
=L ∥θ −θ ∥.
π 1 2
LetL:=max{L ,L }. Pluggingtheabovetwoinequalitiesinto(7)gives
P π
∥µ −µ ∥ ≤σL(∥η −η ∥+∥θ −θ ∥).
ξ1 ξ2 TV 1 2 1 2
Then,bythedefinitionofµ†,wehave
ξ
(cid:90)
∥µ† −µ† ∥ = |µ (s)π (a|s)−µ (s)π (a|s)|dsda
ξ1 ξ2 TV ξ1 θ1 ξ2 θ2
S×A
(cid:90)
≤ (|µ (s)−µ (s)|π (a|s)+µ (s)|π (a|s)−π (a|s)|)dsda
ξ1 ξ2 θ1 ξ2 θ1 θ2
S×A
(cid:90)
=∥µ −µ ∥ + µ (s)∥π (·|s)−π (·|s)∥ ds
ξ1 ξ2 TV ξ2 θ1 θ2 TV
S
≤(σ−2)L(∥η −η ∥+∥θ −θ ∥)+L ∥θ −θ ∥
1 2 1 2 π 1 2
≤(σ−1)L(∥η −η ∥+∥θ −θ ∥).
1 2 1 2
Similarly,bythedefinitionofµ‡,wehave
ξ
∥µ‡ −µ‡ ∥ ≤∥µ† −µ† ∥ +L ∥η −η ∥+L ∥θ −θ ∥
ξ1 ξ2 TV ξ1 ξ2 TV P 1 2 π 1 2
≤σL(∥η −η ∥+∥θ −θ ∥).
1 2 1 2
Corollary2. Similarly,forsteadydistributionsinducedbygeneralvaluefunctionsandpopulation
measuresµ :=µ ,µ :=µ ,wehave
1 (Q1,M1) 2 (Q2,M2)
(cid:110) (cid:111) (cid:18) L (cid:19)
max ∥µ −µ ∥ ,∥µ† −µ†∥ ,∥µ‡ −µ‡∥ ≤σ √P ∥M −M ∥+L ∥Q −Q ∥ .
1 2 TV 1 2 TV 1 2 TV d 1 2 π 1 2 ∞
2
Lemma 6 (Lipschitz temporal difference operators). For any two sets of mean-path temporal
differenceoperatorsG¯ ,φ¯ ,ψ¯ andG¯ ,φ¯ ,ψ¯ determinedbyparametersξ = (θ ;η )and
ξ1 ξ1 ξ1 ξ2 ξ2 ξ2 1 1 1
22ξ =(θ ;η ),wehave
2 2 2
∥G¯ −G¯ ∥≤σL(1+γ)(∥θ −θ ∥+∥η −η ∥),
ξ1 ξ2 1 2 1 2
∥φ¯ −φ¯ ∥≤σLR(∥θ −θ ∥+∥η −η ∥)+L ∥η −η ∥
ξ1 ξ2 1 2 1 2 r 1 2
∥ψ¯ −ψ¯ ∥≤σLF (∥θ −θ ∥+∥η −η ∥),
ξ1 ξ2 1 2 1 2
whereσandLaredefinedinLemma5,andL isdefinedinAssumption1.
r
Proof. Bydefinition,wehave
∥G¯
ξ1
−G¯ ξ2∥=(cid:13) (cid:13)
(cid:13)
(cid:13)(cid:90) ϕ(s,a)(ϕ(s,a)−γϕ(s′,a′))T (cid:16) µ‡ ξ1(s,a,s′,a′)−µ‡ ξ2(s,a,s′,a′)(cid:17) dsdads′da′(cid:13) (cid:13)
(cid:13)
(cid:13)
S2×A2
≤(cid:90) (cid:13) (cid:13)ϕ(s,a)(ϕ(s,a)−γϕ(s′,a′))T(cid:13) (cid:13)(cid:16) µ‡ (s,a,s′,a′)−µ‡ (s,a,s′,a′)(cid:17) dsdads′da′.
ξ1 ξ2
S2×A2
(cid:13) (cid:13)
Since∥ϕ(s,a)∥ ≤ 1,wehave(cid:13)ϕ(s,a)(ϕ(s,a)−γϕ(s′,a′))T(cid:13) ≤ 1+γ. Then,byLemma5,we
have
∥G¯ −G¯ ∥≤(1+γ)σL(∥θ −θ ∥+∥η −η ∥).
ξ1 ξ2 1 2 1 2
Similarly,bydefinition,wehave
(cid:13)(cid:90) (cid:13)
∥φ¯ ξ1 −φ¯ ξ2∥=(cid:13) (cid:13)
(cid:13)
ϕ(s,a)r(s,a,η 1)µ† ξ1(s,a)−ϕ(s,a)r(s,a,η 2)µ† ξ2(s,a)dsda(cid:13) (cid:13)
(cid:13)
S×A
(cid:90) (cid:16) (cid:12) (cid:12) (cid:17)
≤ r(s,a,η )(cid:12)µ† (s,a)−µ† (s,a)(cid:12)+µ† (s,a)|r(s,a,η )−r(s,a,η )| dsda
1 (cid:12) ξ1 ξ2 (cid:12) ξ2 1 2
S×A
≤R∥µ† −µ† ∥ +L ∥η −η ∥
ξ1 ξ2 TV r 1 2
≤σLR(∥θ −θ ∥+∥η −η ∥)+L ∥η −η ∥,
1 2 1 2 r 1 2
and
∥ψ¯ −ψ¯ ∥≤F∥µ −µ ∥ ≤σLF (∥θ −θ ∥+∥η −η ∥).
ξ1 ξ2 ξ1 ξ2 TV 1 2 1 2
Lemma7(Lipschitzsemi-gradient). Givenafixedsemi-gradientoperatorg(whichcanbeamean-
pathsemi-gradient),foranyparametersξ ,ξ ∈Ξ,wehave
1 2
∥g(ξ )−g(ξ )∥≤H∥ξ −ξ ∥.
1 2 1 2
Letξ =(θ ;η )andξ =(θ ;η ). Foranyξ =(θ;η)suchthat∥θ∥≤G,wehave
1 1 1 2 2 2
∥g¯ (ξ)−g¯ (ξ)∥≤σHmax{L ,L }(∥θ −θ ∥+∥η −η ∥)+L ∥η −η ∥.
ξ1 ξ2 P π 1 2 1 2 r 1 2
Proof. Wefirstshowthatthesemi-gradientoperatorisLipschitzinitsargument. Bydefinition,we
have
∥g(ξ ;O)−g(ξ ;O)∥=∥(G (O)⊕G )(ξ −ξ )∥≤∥G (O)⊕G ∥ ∥ξ −ξ ∥.
1 2 ϕ ψ 1 2 ϕ ψ op 1 2
Wegettheresultbynoticingthat
∥G (O)⊕G ∥ ≤max{∥G (O)∥ ,∥G ∥ }≤max{1+γ,F}≤H.
ϕ ψ op ϕ op ψ op
Next, weshowthatthemean-pathsemi-gradientoperatorisLipschitzintheparameter. Another
representationofthemean-pathsemi-gradientis
g¯ =(cid:0) G¯ θ−φ¯ ;G η−ψ¯ (cid:1) .
ξ1 ξ1 ξ1 ψ ξ1
Therefore,byLemma6,wehave
∥g¯ ξ1(ξ)−g¯ ξ2(ξ)∥≤(cid:13) (cid:13)G¯ ξ1θ−G¯ ξ2θ−¯b ξ1 +¯b ξ2(cid:13) (cid:13)+(cid:13) (cid:13)G ψη 1−G ψη 2−ψ¯ ξ1 +ψ¯ ξ2(cid:13) (cid:13)
≤D(cid:13) (cid:13)G¯
ξ1
−G¯ ξ2(cid:13) (cid:13)+(cid:13) (cid:13)¯b
ξ1
−¯b ξ2(cid:13) (cid:13)+(cid:13) (cid:13)ψ¯
ξ1
−ψ¯ ξ2(cid:13)
(cid:13)
≤σL((1+γ)D+R+F)(∥θ −θ ∥+∥η −η ∥)+L ∥η −η ∥.
1 2 1 2 r 1 2
23F Samplecomplexityanalysis
F.1 Keylemmas
Inthissection,wefirstdecomposethemeansquareerrorintodifferentterms,andthenboundeach
termseparately.
Lemma8(Errordecomposition). Letξ betheoptimalparameter. RecallthatbyProposition2,
∗
g¯ (ξ )=0. Wehave
∗ ∗
E∥ξ −ξ ∥2 ≤E∥ξ˘ −ξ ∥2
t+1 ∗ t+1 ∗
=E∥ξ −ξ ∥2−2α E⟨ξ −ξ ,g (ξ )⟩+α2∥g (ξ )∥2
t ∗ t t ∗ t t t t t
=E∥ξ −ξ ∥2+α2∥g (ξ )∥2
t ∗ t t t
−2α E⟨ξ −ξ ,g¯ (ξ )−g¯ (ξ )⟩ (descent)
t t ∗ t t ∗ ∗
−2α E⟨ξ −ξ ,g¯ (ξ )−g¯ (ξ )⟩ (progress)
t t ∗ t−τ t t t
−2α E⟨ξ −ξ ,g (ξ )−g¯ (ξ )⟩ (mix)
t t ∗ t−τ t t−τ t
−2α E⟨ξ −ξ ,g (ξ )−g (ξ )⟩. (backtrack)
t t ∗ t t t−τ t
Lemma 9 (Descent). Let ξ = (θ ;η ) be a projected MFE. For any parameter ξ = (θ;η) ∈
∗ ∗ ∗
B Dd1 ×∆d2,wedenote∆θ =θ−θ ∗and∆η =η−η ∗. Wehave
−⟨η−η ,g¯ (η)−g¯ (η )⟩≤−λ (G )∥∆η∥2+σLF∥∆η∥(∥∆θ∥+∥∆η∥),
∗ ξ ξ∗ ∗ min ψ
−⟨θ−θ ,g¯ (θ)−g¯ (θ )⟩≤−(1−γ)λ (Gˆ )∥∆θ∥2+σLH∥∆θ∥(∥∆θ∥+∥∆η∥)+L ∥∆η∥∥∆θ∥.
∗ ξ ξ∗ ∗ min ξ∗ r
That is, neither −g¯ (η) nor −g¯ (θ) is guaranteed to be a descent direction. Let w :=
ξ ξ
(cid:110) (cid:111)
1min λ (G ),(1−γ)λ (Gˆ ) . Suppose3σLH +L ≤2w. Then,wehave
2 min ψ min ξ∗ r
−⟨ξ−ξ ,g¯ (ξ)−g¯ (ξ )⟩≤−w∥ξ−ξ ∥2.
∗ ξ ξ∗ ∗ ∗
Proof. Wefirstfocusonthefirsttwoinequalities. Wedenote∆η :=η−η and∆θ :=θ−θ . For
∗ ∗
thepopulationmeasureparameter,bydefinition,wehave
−⟨∆η,g¯ (η)−g¯ (η )⟩=(cid:10) ∆η,−G ∆η+ψ¯ −ψ¯ (cid:11) .
ξ ξ∗ ∗ ψ ξ ξ∗
NotethatG isapositivedefiniteGrammatrixifψislinearlyindependent. Then,byLemma6,we
ψ
have
−⟨∆η,g¯ ξ(η)−g¯ ξ∗(η ∗)⟩≤−⟨∆η,G ψ∆η⟩+(cid:13) (cid:13)ψ¯ ξ−ψ¯ ξ∗(cid:13) (cid:13)∥∆η∥
≤−λ (G )∥∆η∥2+σLF∥∆η∥(∥∆η∥+∥∆θ∥). (8)
min ψ
Forthevaluefunctionparameter,wehave
−⟨∆θ,g¯ (θ)−g¯ (θ )⟩=(cid:10) ∆θ,G¯ θ −φ¯ −G¯ θ+φ¯ (cid:11)
ξ ξ∗ ∗ ξ∗ ∗ ξ∗ ξ ξ
=(cid:10) ∆θ,−G¯ ∆θ+(G¯ −G¯ )θ+φ¯ −φ¯ (cid:11)
ξ∗ ξ∗ ξ ξ ξ∗
≤−(cid:10) ∆θ,G¯ ∆θ(cid:11) +D∥G¯ −G¯ ∥∥∆θ∥+∥φ¯ −φ¯ ∥∥∆θ∥.
ξ∗ ξ∗ ξ ξ ξ∗
ByLemma6,wehave
−⟨∆θ,g¯ (θ)−g¯ (θ )⟩≤−(cid:10) ∆θ,G¯ ∆θ(cid:11) +∥∆θ∥((1+γ)σLD(∥∆θ∥+∥∆η∥)
ξ ξ∗ ∗ ξ∗
+σLR(∥∆θ∥+∥∆η∥)+L ∥∆η∥). (9)
r
ForamatrixG,wedefinew (G):=min ⟨x,Gx⟩. ForG¯ ,wehave
min ∥x∥=1 ξ∗
w (G¯ )= sup E (cid:2) θTϕ(s,a)(ϕ(s,a)−γϕ(s′,a′))Tθ(cid:3) =: sup E (cid:2) u2−γuu′(cid:3) ,
min ξ∗ ξ∗ ξ∗
∥θ∥=1 ∥θ∥=1
whereu:=θTϕ(s,a)andu′ :=θTϕ(s′,a′). Wehave
E[uu′]≤
1(cid:0)E[u2]+E[u′2](cid:1)
=
1(cid:0)E[u2]+E[u2](cid:1)
=E[u2].
2 2
24Therefore,
w (G¯ )≥(1−γ) sup E [u2]=(1−γ)w (cid:0)E [ϕϕT](cid:1) =(1−γ)w (cid:16) Gˆ (cid:17) =(1−γ)λ (cid:16) Gˆ (cid:17) ,
min ξ∗ ξ∗ min ξ∗ min ξ∗ min ξ∗
∥θ∥=1
whereGˆ istheGrammatrixofthefeaturemapϕunderthesteadydistributioninducedbyξ ,and
ξ∗ ξ∗
thelastequalityusesthepropertyofnormalmatrices. Pluggingtheabovederivationinto(9)gives
−⟨∆θ,g¯ (θ)−g¯ (θ )⟩≤−(1−γ)λ (Gˆ )∥∆θ∥2+σL(H−2F)∥∆θ∥(∥∆θ∥+∥∆η∥)+L ∥∆η∥∥∆θ∥.
ξ ξ∗ ∗ min ξ∗ r
(10)
It is clear that (8) and (10) are controlled by both ∥∆θ∥ and ∥∆η∥. When ∥∆η∥ ≫ ∥∆θ∥, (10)
suggeststhat−g¯ (θ)maynotbeadescentdirectionfor∥∆θ∥;when∥∆θ∥≫∥∆η∥,(8)suggests
ξ
that−g¯ (η)maynotbeadescentdirectionfor∥∆η∥.
ξ
However,combining(8)and(10)gives
−⟨∆ξ,g¯ (ξ)−g¯ (ξ )⟩≤−(1−γ)λ (Gˆ )∥∆θ∥2−λ (G )∥∆η∥2+σLF∥∆η∥2
ξ ξ∗ ∗ min ξ∗ min ψ
+σL(H −2F)∥∆θ∥2+(σLF +σL(H −2F)+L )∥∆θ∥∥∆η∥.
r
ByLemma2,wehave
−⟨∆ξ,g¯ (ξ)−g¯ (ξ )⟩
ξ ξ∗ ∗
1
≤−min{(1−γ)λ (Gˆ ),λ (G )}∥∆ξ∥2+σLH∥∆ξ∥2+ (σLH +L )∥∆ξ∥2
min ξ∗ min ψ 2 r
(cid:18) (cid:19)
1
≤ −min{(1−γ)λ (Gˆ ),λ (G )}+ (3σLH +L ) ∥∆ξ∥2.
min ξ∗ min ψ 2 r
(cid:110) (cid:111)
Letw := 1min (1−γ)λ (Gˆ ),λ (G ) . Suppose3σLH +L ≤2w. Then,wehave
2 min ξ∗ min ψ r
−⟨∆ξ,g¯ (ξ)−g¯ (ξ )⟩≤−w∥∆ξ∥2.
ξ ξ∗ ∗
Theaboveinequalitysuggeststhat−g¯ (ξ)isadescentdirectionfor∥∆ξ∥iftheLipschitzcontants
ξ
aresmallenough.
∆η
Rd1
∆θ
Rd1⊕Rd2 −g ξ(θ)
−g (η)
ξ
∆η
Rd2 ∆θ
−g (η)
ξ
−g (θ)
ξ −g (ξ)=−(g (θ)⊕g (η)) ∆ξ=∆θ⊕∆η
ξ ξ ξ
⟨−g (ξ),∆ξ⟩<0
ξ
Figure9:AnillustrativeexampleforLemma1,where−g¯ (ξ)givesadescentdirection,while−g¯ (θ)
ξ ξ
doesnot.
Lemma10(Progress). Foranyξ =(θ;η)inBd1 ×∆d2,andforanytimesteptandperiodτ,we
D
have
∥g¯ (ξ)−g¯ (ξ)∥≤α τC ,
t t−τ t−τ prog
whereC :=σmax{L ,L }H2+2L F.
prog π P r
Proof. ByLemma7,wehave
∥g¯ (ξ)−g¯ (ξ)∥≤σLH(∥θ −θ ∥+∥η −η ∥)+L ∥η −η ∥.
t t−τ t t−τ t t−τ r t t−τ
25Accordingtotheupdateruleofθandη,wehave
∥θ −θ ∥≤∥θ˘ −θ ∥=∥α g (θ )∥
t t−1 t t−1 t−1 t−1 t−1
∥η −η ∥≤∥η˘ −η ∥=∥α g (η )∥.
t t−1 t t−1 t−1 t−1 t−1
Since∥θ ∥≤Dand∥η ∥≤∥η ∥ =1,byLemma3,wehave
t−1 t−1 t−1 1
∥θ −θ ∥+∥η −η ∥≤α ((1+γ)D+R)+α ·2F =α H.
t t−1 t t−1 t−1 t−1 t−1
Bythetriangleinequality,weget
τ−1 τ−1
(cid:88) (cid:88)
∥g¯ (ξ)−g¯ (ξ)∥≤σLH (∥θ −θ ∥+∥η −η ∥)+L ∥η −η ∥
t t−τ t−l t−l−1 t−l t−l−1 r t−l t−l−1
l=0 l=0
≤α τσLH2+2α τL F
t−τ t−τ r
=:α τC ,
t−τ prog
where we require that the step size α is non-increasing, and C := σmax{L ,L }H2 +
t prog π P
2L F.
r
Lemma11(Mix). LetF bethefiltrationgeneratedbythehistoryuptotimet−τ. Forany
t−τ
ξ = (θ;η) ∈ B Dd1 ×∆d2 thatisindependentofg
t−τ
andg¯
t−τ
conditionedonF t−τ,andforany
timesteptandperiodτ,wehave
(cid:13) (cid:104) (cid:12) (cid:105)(cid:13)
(cid:13) (cid:13)E g t−τ(ξ;O(cid:101)t)−g¯ t−τ(ξ)(cid:12) (cid:12)F
t−τ
(cid:13) (cid:13)≤mρτH.
Proof. WedenoteP astheobservationdistributiononthevirtualtrajectorybyfixingthetransition
t−τ
kernelP attimet−τ. Wehave
ξt−τ
(cid:13) (cid:104) (cid:12) (cid:105)(cid:13)
(cid:13) (cid:13)E g t−τ(ξ,O(cid:101)t)−g¯ t−τ(ξ)(cid:12) (cid:12)F
t−τ
(cid:13)
(cid:13)
=(cid:13) (cid:13) (cid:13) (cid:13)(cid:90) (cid:16) ϕ(s,a)(cid:0) r(s,a)+γϕT(s′,a′)θ−ϕT(s,a)θ(cid:1) ;ψˆ(s′)(cid:17)(cid:16) P t−τ(O(cid:101)t =o|F t−τ)−µ‡ t−τ(o)(cid:17) do(cid:13) (cid:13) (cid:13)
(cid:13)
S2×A2
(cid:13) (cid:13)
≤((1+γ)D+R+F)(cid:13)P −µ‡ (cid:13) .
(cid:13) t−τ ξt−τ(cid:13)
TV
SinceP andµ‡ sharethesamepolicyπ andtransitionkernelP ,wehave
t−τ ξt−τ θt−τ ξt−τ
(cid:13) (cid:13) (cid:13)P t−τ −µ‡ ξt−τ(cid:13) (cid:13) (cid:13) TV =(cid:13) (cid:13)P t−τ(s t =·|F t−τ)−µ ξt−τ(cid:13) (cid:13) TV ≤mρτH,
wherethelastinequalityusesAssumption3.
Lemma12(Bakctrack). LetF bethefiltrationgeneratedbythehistoryuptotimet−τ. Forany
t−τ
ξ =(θ;η)∈B Dd1 ×∆d2 thatisindependentofg tandg
t−τ
conditionedonF t−τ,andforanytime
steptandperiodτ,wehave
(cid:13) (cid:104) (cid:12) (cid:105)(cid:13)
(cid:13) (cid:13)E g t(ξ,O t)−g t−τ(ξ,O(cid:101)t)(cid:12) (cid:12)F
t−τ
(cid:13) (cid:13)≤α t−ττC back(τ),
whereC (τ):=(τ +1)LH2+2L F.
back r
Proof. Bydefinition<wehave
(cid:13) (cid:104) (cid:12) (cid:105)(cid:13)
(cid:13) (cid:13)E g t(ξ,O t)−g t−τ(ξ,O(cid:101)t)(cid:12) (cid:12)F
t−τ
(cid:13)
(cid:13)
(cid:13)(cid:90) (cid:16) (cid:17)
=(cid:13)
(cid:13)
(G ϕ(o)⊕G ψ)ξ· P t(O
t
=o|F t−τ)−P t−τ(O(cid:101)t =o|F t−τ) do
S2×A2
(cid:90) (cid:13)
− (φ
ηt
⊕ψ)(o)P t(O
t
=o|F t−τ)−(cid:0) φ
ηt−τ
⊕ψ(cid:1) (o)P t−τ(O(cid:101)t =o|F t−τ)do(cid:13)
(cid:13)
S2×A2
≤((1+γ)D+F)∥P −P ∥
t t−τ TV
(cid:13) (cid:13)(cid:90) (cid:16) (cid:17) (cid:13) (cid:13)
+(cid:13)
(cid:13)
(φ
ηt
⊕ψ)(o) P t(O
t
=o|F t−τ)−P t−τ(O(cid:101)t =o|F t−τ) do(cid:13)
(cid:13)
S2×A2 TV
(cid:13)(cid:90) (cid:13)
(cid:13) (cid:0) (cid:1) (cid:13)
+(cid:13)
(cid:13)
φ
ηt
−φ
ηt−τ
(o)P t−τ(O(cid:101)t =o|F t−τ)do(cid:13)
(cid:13)
S2×A2 TV
≤((1+γ)D+F +R+F)∥P −P ∥ +L ∥η −η ∥
t t−τ TV r t t−τ
26whereP andP arethedistributionsofobservationattimesteptontheactualtrajectoryand
t t−τ
thevirtualtrajectory,respectively. BytheproofofLemma10,wehave∥η −η ∥≤α τ ·2F.
t t−τ t−τ
Therefore,wehave
(cid:13) (cid:104) (cid:12) (cid:105)(cid:13)
(cid:13) (cid:13)E g t(ξ,O t)−g t−τ(ξ,O(cid:101)t)(cid:12) (cid:12)F
t−τ
(cid:13) (cid:13)≤H∥P t−P t−τ∥ TV+2α t−ττL rF. (11)
LetΞbethesetofallparameters. WefirstexpandP withconditionalprobabilities:
t
P (O =(s,a,s′,a′)|F )
t t t−τ
(cid:90)
= P (s =s|ξ ,s )P (ξ =ξ|ξ ,s ,s =s)π (a|s)
t t t−τ t−τ t t−1 t−τ t−τ t θ
Ξ2
P(s′|s,a,η′)P (ξ =ξ′|ξ ,s ,ξ =ξ,s =s,a =a)π (a′|s′)dξdξ′,
t t t−τ t−τ t−1 t t θ′
whereξ =(θ;η)andξ′ =(θ′;η′). WethendecomposeP intoasimilarform:
t−τ
P t−τ(O(cid:101)t =(s,a,s′,a′)|F t−τ)
=P (s =s|ξ ,s )π (a|s)P(s′|s,a,η )π (a′|s′)
t−τ (cid:101)t t−τ t−τ θt−τ t−τ θt−τ
=P (s =s|ξ ,s )π (a|s)P(s′|s,a,η )π (a′|s′)
t−τ (cid:101)t t−τ t−τ θt−τ t−τ θt−τ
(cid:90)
· P (ξ =ξ|ξ ,s ,s =s)P (ξ =ξ′|ξ ,s ,ξ =ξ,s =s,a =a)dξdξ′.
t t−1 t−τ t−τ t t t t−τ t−τ t−1 t t
Ξ2
Therefore,wecandecomposethedistributiondifferenceintofourparts:
P −P
t t−τ
(cid:90)
= (P (s =s|F )−P (s =s|F ))P (ξ =ξ|ξ ,s ,s =s)
t t t−τ t−τ (cid:101)t t−τ t t−1 t−τ t−τ t
Ξ2
π (a|s)P(s′|s,a,η′)P (ξ =ξ′|ξ ,s ,ξ =ξ,s =s,a =a)π (a′|s′)dξdξ′
θ t t t−τ t−τ t−1 t t θ′
(S )
1
(cid:90)
(cid:0) (cid:1)
+ P (s =s|F )P (ξ =ξ|ξ ,s ,s =s) π (a|s)−π (a|s)
t−τ (cid:101)t t−τ t t−1 t−τ t−τ t θ θt−τ
Ξ2
P(s′|s,a,η′)P (ξ =ξ′|ξ ,s ,ξ =ξ,s =s,a =a)π (a′|s′)dξdξ′ (S )
t t t−τ t−τ t−1 t t θ′ 2
(cid:90)
+ P (s =s|F )P (ξ =ξ|ξ ,s ,s =s)π (a|s)(P(s′|s,a,η′)−P(s′|s,a,η ))
t−τ (cid:101)t t−τ t t−1 t−τ t−τ t θt−τ t−τ
Ξ2
P (ξ =ξ′|ξ ,s ,ξ =ξ,s =s,a =a)π (a′|s′)dξdξ′ (S )
t t t−τ t−τ t−1 t t θ′ 3
(cid:90)
+ P (s =s|F )P (ξ =ξ|ξ ,s ,s =s)π (a|s)P(s′|s,a,η )
t−τ (cid:101)t t−τ t t−1 t−τ t−τ t θt−τ t−τ
Ξ2
P (ξ =ξ′|ξ ,s ,ξ =ξ,s =s,a =a)(cid:0) π (a′|s′)−π (a′|s′)(cid:1) dξdξ′.
t t t−τ t−τ t−1 t t θ′ θt−τ
(S )
4
We now bound each part separately. By integrating out the later part, S becomes the marginal
1
differenceofthestatedistribution:
(cid:90) (cid:90)
S dsdads′da′ = (P (s =s|F )−P (s =s|F ))
1 t t t−τ t−τ (cid:101)t t−τ
S2×A2 S
(cid:32)
(cid:90)
· P (ξ =ξ|ξ ,s ,s =s)π (a|s)P(s′|s,a,η′)
t t−1 t−τ t−τ t θ
S×A2×Ξ2
(cid:33)
P (ξ =ξ′|ξ ,s ,ξ =ξ,s =s,a =a)π (a′|s′)dξdξ′dads′da′ ds
t t t−τ t−τ t−1 t t θ′
(cid:90)
= (P (s =s|F )−P (s =s|F ))ds
t t t−τ t−τ (cid:101)t t−τ
S
≤∥P (s =·|F )−P (s =·|F )∥
t t t−τ t−τ (cid:101)t t−τ TV
=∥P (s =·|F )−P (s =·|F )∥ .
t−1 t t−τ t−τ (cid:101)t t−τ TV
27ByJensen’sinequality,wehave
∥P (s =·|F )−P (s =·|F )∥
t−1 t t−τ t−τ (cid:101)t t−τ TV
(cid:13)(cid:90) (cid:13)
=(cid:13) (cid:13) P t−1(O t−1 =(s,a,·,a′)|F t−τ)−P t−τ(O(cid:101)t−1 =(s,a,·,a′)|F t−τ)dsdada′(cid:13) (cid:13)
(cid:13) (cid:13)
S×A2 TV
(cid:90) (cid:13) (cid:13)
≤ (cid:13) (cid:13)P t−1(O
t−1
=(s,a,·,a′)|F t−τ)−P t−τ(O(cid:101)t−1 =(s,a,·,a′)|F t−τ)(cid:13)
(cid:13)
dsdada′
S×A2 TV
(cid:13) (cid:13)
=(cid:13) (cid:13)P t−1(O
t
=·|F t−τ)−P t−τ(O(cid:101) =·|F t−τ)(cid:13)
(cid:13)
.
TV
Thatis,S isrecursivelyboundedby∥P −P ∥ .
1 t−1 t−τ TV
ForS ,wehave
2
(cid:90)
S dsdads′da′
2
S2×A2
(cid:90)
(cid:0) (cid:1)
= P (s =s|F )P (ξ =ξ|ξ ,s ,s =s) π (a|s)−π (a|s)
t−τ (cid:101)t t−τ t t−1 t−τ t−τ t θ θt−τ
S×A×Ξ
(cid:32) (cid:33)
(cid:90)
· P(s′|s,a,η′)P (ξ =ξ′|ξ ,s ,ξ =ξ,s =s,a =a)π (a′|s′)ds′da′dξ′ dsdadξ
t t t−τ t−τ t−1 t t θ′
S×A×Ξ
(cid:90)
(cid:0) (cid:1)
= P (s =s|F )P (ξ =ξ|ξ ,s ,s =s) π (a|s)−π (a|s) dsdadξ
t−τ (cid:101)t t−τ t t−1 t−τ t−τ t θ θt−τ
S×A×Ξ
(cid:90)
(cid:13) (cid:13)
≤ P t−τ(s
(cid:101)t
=s|F t−τ)P t(ξ
t−1
=ξ|ξ t−τ,s t−τ,s
t
=s)(cid:13)π θ(·|s)−π θt−τ(·|s)(cid:13) TVdsdξ.
S×Ξ
By Assumption 2 and Lemma 10, for any ξ ∈ Ξ such that P (ξ = ξ|F ) ̸= 0 (which is
t t−1 t−τ
equivalenttoP (ξ =ξ|ξ ,s ,s =s)̸=0asallpoliciesareergodic),wehave
t t−1 t−τ t−τ t
∥π (·|s)−π (·|s)∥ ≤α (τ −1)L ((1+γ)G+R).
θ θt−τ TV t−τ π
Therefore,weget
(cid:90) (cid:90)
S dsdads′da′ ≤α (τ −1)L ((1+γ)G+R) P (s =s|F )P (ξ =ξ|ξ ,s ,s =s)dsdξ
2 t−τ π t−τ (cid:101)t t−τ t t−1 t−τ t−τ t
S2×A2 S×Ξ
=α (τ −1)L ((1+γ)G+R).
t−τ π
ForS ,wehave
3
(cid:90)
S dsdads′da′
3
S2×A2
(cid:90)
= P (s =s|F )P (ξ =ξ|ξ ,s ,s =s)π (a|s)
t−τ (cid:101)t t−τ t t−1 t−τ t−τ t θt−τ
S2×A×Ξ2
·(P(s′|s,a,η′)−P(s′|s,a,η ))P (ξ =ξ′|ξ ,s ,ξ =ξ,s =s,a =a)
t−τ t t t−τ t−τ t−1 t t
(cid:18)(cid:90) (cid:19)
· π (a′|s′)da′ dsdads′ξdξ′
θ′
A
(cid:90)
≤ P (s =s|F )P (ξ =ξ|ξ ,s ,s =s)π (a|s)
t−τ (cid:101)t t−τ t t−1 t−τ t−τ t θt−τ
S×A×Ξ2
·∥P(·|s,a,η′)−P(·|s,a,η )∥ P (ξ =ξ′|ξ ,s ,ξ =ξ,s =s,a =a)dsdadξdξ′.
t−τ TV t t t−τ t−τ t−1 t t
Similar to the bound for S , by Assumption 1 and Lemma 10, for any ξ′ ∈ Ξ such that P (ξ =
2 t t
ξ′|F )̸=0,wehave
t−τ
∥P(·|s,a,η′)−P(·|s,a,η )∥ ≤2α τL .
t−τ TV t−τ P
Therefore,weget
(cid:90) (cid:90)
S dsdads′da′ ≤α τL H P (s =s|F )P (ξ =ξ|ξ ,s ,s =s)
3 t−τ P t−τ (cid:101)t t−τ t t−1 t−τ t−τ t
S2×A2 S×A×Ξ2
π (a|s)P (ξ =ξ′|ξ ,s ,ξ =ξ,s =s,a =a)dsdadξdξ′
θt−τ t t t−τ t−τ t−1 t t
=2α τL .
t−τ P
28SimilartotheboundforS ,forS ,wehave
2 4
(cid:90)
S dsdads′da′ ≤α τL ((1+γ)G+R).
4 t−τ π
S2×A2
PluggingbacktheaboveboundsforS ,S ,S ,andS ,weget
1 2 3 4
∥P −P ∥ ≤∥P −P ∥ +2τα (L ((1+γ)D+R)+L F)
t t−τ TV t−1 t−τ TV t−τ π P
.
Recursively,weget
τ
(cid:88)
∥P −P ∥ ≤2LH (lα )≤α τ(τ +1)LH,
t t−τ TV t−l t−τ
l=1
wherewerequirethestepsizetobenon-increasing. Pluggingtheaboveboundbackto(11)givesthe
desiredresult.
F.2 ProofofTheorem1
Theorem1. Letξ =(θ ;η )bea(projected)MFE.Let{ξ =(θ ;η )}beasequenceofparameters
∗ ∗ ∗ t t t
generatedbyAlgorithm1. Then,underAssumptions1to3,if3σLH +L ≤2w,wehave
r
(cid:18) α3log4α−1L2H4(cid:19)
E∥ξ −ξ ∥2 ≤(1−α w)E∥ξ −ξ ∥2+α2H2+O t t .
t+1 ∗ t t ∗ t w
Proof. Wedenote∆ξ =ξ −ξ . WefirstplugLemmas1and3intoLemma8toget
t t ∗
E∥∆ξ ∥2 ≤(1−2α w)E∥∆ξ ∥2+α2H2
t+1 t t t
+2α E⟨∆ξ,g¯ (ξ )−g¯ (ξ )⟩ (12)
t t t t−τ t
+2α E⟨∆ξ,g¯ (ξ )−g (ξ )⟩ (13)
t t−τ t t−τ t
+2α E⟨∆ξ,g (ξ )−g (ξ )⟩. (14)
t t−τ t t t
For(12),byYoung’sinequality,foranyβ >0,wehave
1
2E⟨∆ξ ,g¯ (ξ )−g¯ (ξ )⟩≤βE∥∆ξ ∥2+ E∥g¯ (ξ )−g¯ (ξ )∥2.
t t t t−τ t t β t−τ t t t
ByLemma10,weget
2E⟨∆ξ ,g¯ (ξ )−g¯ (ξ )⟩≤βE∥∆ξ ∥2+β−1α2 τ2C2 . (15)
t t t t−τ t t t−τ prog
For(13),notethatconditionedonF ,g¯ isdeterminedandg isonlydependentonthevirtual
t−τ t−τ t−τ
Markoviantrajectory. Thus,ξ isindependentofg andg¯ conditionedonF . Therefore,by
t t−τ t−τ t−τ
Lemma11andYoung’sinequality,wehave
2E⟨∆ξ ,g¯ (ξ )−g (ξ )⟩=2E[⟨E[∆ξ |F ],E[g (ξ )−g¯ (ξ )|F ]⟩]
t t−τ t t−τ t t t−τ t−τ t t−τ t t−τ
≤2E[E[∥∆ξ ∥ |F ]·∥E[g (ξ )−g¯ (ξ )|F ]∥]
t t−τ t−τ t t−τ t t−τ
≤2mρτHE∥∆ξ ∥
t
≤βE∥∆ξ ∥2+β−1m2ρ2τH2. (16)
t
For(14),wecannotdirectlyapplyLemma12asξ andg aredependentconditionedonF . To
t t t−τ
proceed,weemploythefollowingdecomposition:
E⟨∆ξ ,g (ξ )−g (ξ )⟩=E⟨∆ξ ,(g (ξ )−g (ξ ))−(g (ξ )−g (ξ ))⟩
t t−τ t t t t t−τ t t−τ t−τ t t t t−τ
(cid:124) (cid:123)(cid:122) (cid:125)
H1
+E⟨ξ −ξ ,g (ξ )−g (ξ )⟩+E⟨∆ξ ,g (ξ )−g (ξ )⟩.
t t−τ t−τ t−τ t t−τ t−τ t−τ t−τ t t−τ
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
H2 H3
29ForH ,bytheLipschitznessofthesemi-gradient(Lemma7)andLemma10,wehave
1
H ≤2α τH2·E∥∆ξ ∥.
1 t−τ t
ForH ,byLemmas3and10,weget
2
H ≤2H ·α τH.
2 t−τ
ForH ,ξ isindependentofg andg conditionedonF . Similartotheboundof(16),by
3 t−τ t t−τ t−τ
Lemma12,wehave
H =E[⟨E[∆ξ |F ],E[g (ξ )−g (ξ )|F ]⟩]
3 t−τ t−τ t−τ t−τ t t−τ t−τ
≤E[E[∥∆ξ ∥|F ]·∥E[g (ξ )−g (ξ )|F ]∥]
t−τ t−τ t t−τ t−τ t−τ t−τ
≤α τC E∥∆ξ ∥.
t−τ back t−τ
Then,applyingLemma10andYoung’sinequalitygives
2H ≤α τC (cid:0) α τH +E∥∆ξ ∥2(cid:1) ≤2α2 τ2HC +βE∥∆ξ ∥2+β−1α2 τ2C2 .
3 t−τ back t−τ t t−τ back t t−τ back
PluggingH ,H ,andH backintothedecompositionandapplyingYoung’sinequality,weget
1 2 3
2E⟨∆ξ ,g (ξ )−g (ξ )⟩≤2βE∥∆ξ ∥2+4α τH2+α2 τ2(cid:0) 2HC +β−1(cid:0) C2 +4H4(cid:1)(cid:1) .
t t−τ t t t t t−τ t−τ back back
(17)
Finally,pluggingtheboundsof(15–17)backinto(12–14)gives
E∥∆ξ ∥2 ≤(1−2α w+4α β)E∥∆ξ ∥2+α2H2+4α α τH2
t+1 t t t t t t−τ
+α (cid:0) β−1α2 τ2C2 +β−1m2ρ2τH2+α2 τ2(2HC +β−1(C2 +4H4))(cid:1) .
t t−τ prog t−τ back back
Nowwechooseτ =(cid:108) l lo og gα ρ−− t 11(cid:109) . Then,ρτ ≤α t. Werequirethestep-sizetobenon-increasing,and
nottoosamllsuchthat
∞
(cid:88)
α =+∞ and logα−1 =o(T).
t t
t=1
Thefirstconditionindicatesthatlimsup α /α := C < ∞. AndthereexistsC > 0
t→∞ t/2 t α,1 α,2
suchthatα /α ≤C ·limsup α /α foranytimestept. Thesecondconditionindicates
t/2 t α,2 t→∞ t/2 t
theexistenceofC >0suchthatα ≤C α foranyt. Inconclusion,wehave
α,2 t−τ α,3 t/2
α ≤C α ≤C C C α =:C α .
t−τ α,3 t/2 α,3 α,2 α,1 t α t
Then,wechooseβ =w/4. Together,weget
E∥∆ξ ∥2 ≤(1−α w)E∥∆ξ ∥2+α2(H2+4τC H2) (18)
t+1 t t t α
(cid:18) (cid:19)
+α3 4 (cid:0) τ2C2(C2 +C2 +4H4)+m2H2(cid:1) +2τ2C2C H
t w α prog back α back
=:(1−α w)E∥∆ξ ∥2+α2·C (α )+α3·C (α ),
t t t 2 t t 3 t
whereweuseC andC toencapsulatethetermsintheright-handsideof(18). Pluggingbackthe
2 3
definitionsofC andC givesthefinalresult
prog back
max{L ,L ,L }2H4
E∥∆ξ ∥2 =(1−α w)E∥∆ξ ∥2+H2·O(α2logα−1)+ π P r ·O(α3log4α−1),
t+1 t t t t w t t
wheretheasymptoticequivalenceholdsasα →0.
t
F.3 ProofofCorollaries
Corollary1. Foraconstantstep-sizeα ≡α ,wehave
t 0
α H2 (cid:18) L2H4α2(cid:19)
E∥ξ −ξ ∥2 ≤e−α0wt∥ξ −ξ ∥2+ 0 +O 0 .
t ∗ 0 ∗ w w2
30Proof. ByTheorem1,wehave
T
(cid:88)
E∥ξ −ξ ∥2 ≤(1−α w)T∥ξ −ξ ∥2+ (1−α w)T−t(α2C +α3C )
T ∗ 0 0 ∗ 0 0 2 0 3
t=0
α
≤e−α0wT∥ξ −ξ ∥2+ 0(C +α C )
0 ∗ w 2 0 3
≤e−α0wT∥ξ −ξ ∥2+w−1H2·O(α logα−1).
0 ∗ 0 0
Corollary3(Linearlydecayingstep-size). Wedefinealinearlydecayingstep-sizesequenceα =
t
4/(w(t+1)),andtheconvexcombinationξ˜ := 2 (cid:80)T tξ . Then,wehave
T T(T+1) t=0 t
(cid:18) H2logT(cid:19)
E∥ξ˜ −ξ ∥2 ≤O .
T ∗ w2T
Proof. RearrangingtheresultofTheorem1gives
1 (cid:18) 1 1(cid:19) 1 α α2
E∥∆ξ ∥2 ≤ − E∥∆ξ ∥2− E∥∆ξ ∥2+ tC + tC .
2 t α w 2 t α w t+1 w 2 w 3
t t
Substitutingα =4/(w(t+1))andmultiplyingbytgives
t
(t−1)t t(t+1) 8t 32t
tE∥∆ξ ∥2 ≤ E∥∆ξ ∥2− E∥∆ξ ∥2+ C + C . (19)
t 2 t 2 t+1 w2(t+1) 2 w3(t+1)2 3
ByJensen’sinequality,wehave
T
E∥ξ˜ −ξ ∥2 ≤ 1 (cid:88) tE∥∆ξ ∥2. (20)
T ∗ T(T +1) t
t=0
Combining(19)and(20)gives
E∥ξ˜ −ξ ∥2 ≤
4
(cid:32)
C
(cid:88)T
t
+
4C
3
(cid:88)T
t
(cid:33) =O(cid:18) H2logT(cid:19)
.
T ∗ w2T(T +1) 2 t+1 w (t+1)2 w2T
t=0 t=0
G Approximationerroranalysis
ThissectionaimstocharacterizetheapproximationerrorofSemiSGDforgeneral(non-linear)MFGs.
Recall that SemiSGD converges to the projected MFE (Appendix D). Let ξ = (θ ;η ) be the
⋄ ⋄ ⋄
convergencepointofSemiSGDwithPA-LFA.Let(q ,µ )betheactualMFE.Theapproximation
∗ ∗
errorisdefinedas
ϵ :=∥q −⟨ϕ,θ ⟩∥ , ϵ :=∥µ −⟨ψ,η ⟩∥ . (21)
q ∗ ⋄ ∞ µ ∗ ⋄ TV
Additionally,wedefinetheinherenterrorofthechosenbasisas
ϵ :=∥q −Π q ∥ , ϵ :=∥P −Π P ∥ .
ϕ ∗ ϕ ∗ ∞ ψ ∗ ψ ∗ TV
The next two lemmas bound the approximation errors in (21) separately, showing how they are
correlated. Theorem2isadirectcorollaryofLemmas13and14underthesmallLipschitzconstants
assumption.
Lemma13(Valuefunctionapproximationerror).
(cid:18) (cid:19) (cid:18) (cid:19)
γσRL γσRL
ϵ ≤ γ+ π ϵ + L + √P ϵ +ϵ .
q 1−γ q r (1−γ) d µ ϕ
2
31Proof. Wefirsthavethedecomposition:
ϵ ≤∥⟨ϕ,θ ⟩−Π q ∥ +∥Π q −q ∥ , (22)
q ⋄ ϕ ∗ ∞ ϕ ∗ ∗ ∞
whereΠ istheorthogonalprojectionoperatorontothelinearspanofbasisϕw.r.t. theinnerproduct
ϕ
inducedbytheξ . Sinceξ isaprojectedMFE,byDefinition3,weget
⋄ ⋄
∥⟨ϕ,θ ⟩−Π q ∥=∥Π T ⟨ϕ,θ ⟩−Π q ∥≤∥T ⟨ϕ,θ ⟩−q ∥, (23)
⋄ ϕ ∗ ϕ ⋄ ⋄ ϕ ∗ ⋄ ⋄ ∗
whereT :=T andtheinequalityusesthefactthatΠ isanon-expansiveoperator. Sinceq isan
⋄ ξ⋄ ϕ ∗
equilibriumvaluefunction,wehave
∥T ⟨ϕ,θ ⟩−q ∥ =∥T ⟨ϕ,θ ⟩−T q ∥ ≤∥T (⟨ϕ,θ ⟩−q )∥ +∥(T −T )q ∥ . (24)
⋄ ⋄ ∗ ∞ ⋄ ⋄ ∗ ∗ ∞ ⋄ ⋄ ∗ ∞ ⋄ ∗ ∗ ∞
Forthefirsttermin(24),theBellmanoperator’sdefinitiongives
∥T (⟨ϕ,θ ⟩−q )∥ ≤γ∥⟨ϕ,θ ⟩−q ∥ =γϵ . (25)
⋄ ⋄ ∗ ∞ ⋄ ∗ ∞ q
SimilartotheLipschitznessofTDoperators(Lemma6),thesecondtermin(24)canbeboundedas
follows:
∥(T −T )q (s,a)∥
⋄ ∗ ∗ ∞
=(cid:13) (cid:13) (cid:13) (cid:13)r(s,a,⟨ψ,η ⋄⟩)−r(s,a,µ ∗)+(cid:90) γq ∗(s′,a′)(cid:16) µ‡ ξ⋄(s,a,s′,a′)−µ‡ ∗(s,a,s′,a′)(cid:17) ds′da′(cid:13) (cid:13) (cid:13)
(cid:13)
S×A ∞
≤L ∥⟨ψ,η ⟩−µ ∥+γ∥q ∥ ∥µ‡ −µ‡∥ (26)
r ⋄ ∗ ∗ ∞ ξ⋄ ∗ TV
(cid:18) (cid:19)
L
≤L ϵ +γ∥q ∥ ·σ √P ϵ +L ϵ , (27)
r µ ∗ ∞ µ π q
d
2
where(26)usesAssumption1and(27)usesCorollary2. Sinceq isthebestresponsetoµ ,wehave
∗ ∗
(cid:13) (cid:13)
(cid:13) (cid:88)∞ (cid:13) (cid:88)∞ R
∥q ∥ =(cid:13)E γtr(s ,a ,µ )(cid:13) ≤ γtR= . (28)
∗ ∞ (cid:13) (q∗,µ∗) t t ∗ (cid:13) 1−γ
(cid:13) (cid:13)
t=0 ∞ t=0
Plugging(28)backinto(27)gives
(cid:18) (cid:19)
γσR L
∥(T −T )q ∥ ≤L ϵ + √P ϵ +L ϵ . (29)
⋄ ∗ ∗ ∞ r µ 1−γ d µ π q
2
Plugging(25)and(29)backinto(24)gives
(cid:18) (cid:19) (cid:18) (cid:19)
γσRL γσRL
∥T ⟨ϕ,θ ⟩−q ∥ ≤ γ+ π ϵ + L + √P ϵ . (30)
⋄ ⋄ ∗ ∞ 1−γ q r (1−γ) d µ
2
Plugging(30)backinto(23)andthen(22)givesthedesiredresult.
Lemma14(Populationmeasureapproximationerror).
Proof. ForuniformlyergodicMDPs,P∞ :=lim Pt exists,whichmapsanydistributiontoµ .
∗ t→∞ ∗ ∗
Theuniformergodicityisequivalenttostrongergodicity[27],whichimpliesfollowingrelationabout
thegeometricconvergencerate[18]:
ρ(P −P∞)≤ρ<1,
∗ ∗
whereρ(P)returnsthespectralradiusofP. Withoutlossofgenerality,weassumeρ>ρ(P −P∞).
∗ ∗
Then,byIsaacsonandLuecke[18,Corollary3.9],foranyρ>ρ(P −P∞),thereexistsk ∈Nsuch
∗ ∗
that
∥Pk−P∞∥ ≤ρk,
∗ ∗ TV
where the norm is the operator norm induced by the total variation norm. Now we apply the
decomposition:
ϵ µ ≤(cid:13) (cid:13)⟨ψ,η ⋄⟩−P ∗k⟨ψ,η ⋄⟩(cid:13) (cid:13) TV+(cid:13) (cid:13)P ∗k⟨ψ,η ⋄⟩−µ ∗(cid:13) (cid:13) TV. (31)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
E1 E2
32Sinceξ isaprojectedMFE,byDefinition3,wehave
⋄
(cid:13)(cid:16) (cid:17) (cid:13) (cid:13) (cid:13)
E =(cid:13) (Π P )k−Pk ⟨ψ,η ⟩(cid:13) ≤(cid:13)(Π P )k−Pk(cid:13) .
1 (cid:13) ψ ⋄ ∗ ⋄ (cid:13) (cid:13) ψ ⋄ ∗(cid:13)
TV TV
Afurtherdecompositiongives
(cid:13) (cid:16) (cid:17) (cid:13)
E ≤(cid:13)Π P (Π P )k−1−Pk−1 +Π (P −P )Pk−1+(Π −Id)Pk(cid:13) .
1 (cid:13) ψ ⋄ ψ ⋄ ∗ ψ ⋄ ∗ ∗ ψ ∗(cid:13)
TV
NotethatbothΠ andP arenon-expansiveoperators. Bythesub-multiplicativityofoperatornorms,
ψ
wehave
(cid:13) (cid:13)
E ≤(cid:13)(Π P )k−1−Pk−1(cid:13) +∥P −P ∥ +∥(Π −Id)P ∥ . (32)
1 (cid:13) ψ ⋄ ∗ (cid:13) ⋄ ∗ TV ψ ∗ TV
TV (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
E3 ϵψ
Wedenoteϵ :=∥P −Π P ∥ ;ϵ istheinherentapproximationerrorinducedbyψ. ForE ,by
ψ ∗ ψ ∗ TV ψ 3
definition,wehave
(cid:13)(cid:90) (cid:13)
(cid:13) (cid:13)
E 3 = sup (cid:13)
(cid:13)
(P(·|s,a,⟨ψ,η ⋄⟩)π θ⋄(a|s)−P(·|s,a,µ ∗)π q∗(a|s))µ(s)dsda(cid:13)
(cid:13)
∥µ∥TV≤1 S×A TV
L
≤L ϵ + √P ϵ .
π q µ
d
2
PluggingtheaboveboundofE backinto(32)givesthefollowingrecursion:
3
(cid:13) (cid:13) L
E ≤(cid:13)(Π P )k−1−Pk−1(cid:13) +L ϵ + √P ϵ +ϵ
1 (cid:13) ψ ⋄ ∗ (cid:13) TV π q d 2 µ ψ
(cid:18) (cid:19)
L
≤k L ϵ + √P ϵ +ϵ . (33)
π q µ ψ
d
2
ForE ,sinceµ istheequilibriumpopulationmeasure,andP∞ mapsanydistributiontoµ ,we
2 ∗ ∗ ∗
have
E 2 =(cid:13) (cid:13)P ∗k⟨ψ,η ⋄⟩−µ ∗+µ ∗−µ ∗(cid:13) (cid:13) TV
=(cid:13) (cid:13)P ∗k⟨ψ,η ⋄⟩−P ∗kµ ∗+P ∗∞⟨ψ,η ⋄⟩−P ∗∞µ ∗(cid:13) (cid:13)
TV
=(cid:13) (cid:13)(cid:0) P ∗k−P ∗∞(cid:1) (⟨ψ,η ⋄⟩−µ ∗)(cid:13) (cid:13)
TV
≤(cid:13) (cid:13)P ∗k−P ∗∞(cid:13) (cid:13) TVϵ µ
≤ρkϵ . (34)
µ
Plugging(33)and(34)backinto(31)gives
(cid:18) (cid:19)
kL
ϵ ≤ ρk+ √ P ϵ +kL ϵ +kϵ .
µ µ π q ψ
d
2
Furthermore,if∥P −P∞∥ ≤ρ,forexample,P correspondstoareversibleMarkovchain,then
∗ ∗ TV ∗
k =1.
H Meanfieldgameswithfinitestate-actionspace
RecallthatSemiSGDwithPA-LFA(Algorithm1)reducestotabularSemiSGD(Algorithm1)for
finitestate-actionspaces,whenthefeaturemapandmeasuremaparechosenas
ϕ(s,a)=e ∈R|S||A|, ψ(s′)=e ∈∆|S|.
(s,a) s′
Then,Q=θ ∈R|S||A|andM =η ∈∆|S|aretheparametersthemselves.
33H.1 Implicitregularization
WefirstshowthattabularSemiSGDdoesnotneedtheprojectionstepforregularizingtheparameters
(see(6)).Thatis,tabularSemiSGDenjoysimplicitregularization.For∥M∥,wefirsthave∥ψ(s′)∥ ≤
1
1=:F. RecallthestochasticupdateruleofM:
(cid:0) (cid:1)
M =M −α M −e =(1−α)M +αe .
t+1 t t st+1 t st+1
SupposeM ∈∆|S|andthestep-sizeissmallerthanone. ThemM ≥0. Futhermore,wehave
t t+1
(cid:88) (cid:88) (cid:88)
∥M ∥ = |(1−α)M (s)+αe (s)|=(1−α) M (s)+α e (s)=(1−α)+α=1,
t+1 1 t st+1 t st+1
s∈S s∈S s∈S
indicatingthatM ∈∆|S|,withoutanyprojection.
t+1
For∥Q∥,noticethatthetrueaction-valuefunctioninducedbyanypolicyπisboundedby∥q ∥ ≤
π ∞
(cid:80)∞ γtR=R/(1−γ)=:D Supposecurrentestimatedvaluefunctionsatisfiesthat∥Q ∥ ≤
t=0 ∞ t ∞
D ,thenwehave
∞
|Q (s,a)|=|Q (s ,a )+α(r +γQ (s ,a )−Q (s ,a ))|
t+1 t t t t t t+1 t+1 t t t
=|(1−α)Q (s ,a )+αγQ (s ,a )+αr |
t t t t t+1 t+1 t
≤(1−α)D +αγD +αR
∞ ∞
R
=(1−α+αγ) +αR
1−γ
R
= =D .
1−γ ∞
Therefore, if the bound holds for the initial estimated value function, it holds for all sequential
estimatedvaluefunctions. Then,thefollowingℓ normboundholdsforallvaluefunctions:
2
(cid:112)
(cid:112) |S||A|R
∥Q∥ ≤ |S||A|∥Q∥ ≤ =:D
2 ∞ 1−γ
Consequently,
(cid:112) (cid:32)(cid:112) (cid:33)
((1+γ) |S||A|+1−γ)R |S||A|R
H = +2=O .
1−γ 1−γ
H.2 Convergencerate
WenowfigureoutthescaleofthedescentparameterwforfiniteMFGs. First,fortabularSemiSGD,
G = I. AccordingtoLemma9,λ (G ) = 1. Second,Gˆ = diag(µ†(s,a)),whereµ† isthe
ψ min ψ ∗ ∗ ∗
steady state-action distribution induced by ξ . Thus, λ (Gˆ ) = min µ†(s,a) ≤ 1 . We
∗ min ∗ s,a ∗ |S||A|
defineλ:=min µ†(s,a)>0,theprobabilityofvisitingtheleastprobablestate-actionpairinthe
s,a ∗
MFE.Then,wehave
1 1
w = min{(1−γ)λ (Gˆ ),λ (G )}≥ (1−γ)λ.
2 min ξ∗ min ψ 2
WearenowreadytostatethesamplecomplexityoftabularSemiSGD.
Corollary4. Witheitheraconstantstepsizeα ≡ α = logT/(wT)oralinearlydecayingstep
t 0
sizeα
t
=1/(w(1+t)),thereexistsaconvexcombinationξ(cid:101)T oftheiterates{ξ t}T t=1suchthat
(cid:13) (cid:13)2 (cid:18) |S||A|R2 (cid:19)
E(cid:13) (cid:13)ξ(cid:101)T −ξ ∗(cid:13)
(cid:13)
=O(cid:101)
λ2(1−γ)4T
,
whereO(cid:101)suppresseslogarithmicfactors.
Notably,tabularSemiSGDhasthesamesamplecomplexityasTDLearningmethods[5,40].
34NeurIPSPaperChecklist
1. Claims
Question: Dothemainclaimsmadeintheabstractandintroductionaccuratelyreflectthe
paper’scontributionsandscope?
Answer: [Yes]
Justification: SeeabstractandSection1(Introduction).
Guidelines:
• TheanswerNAmeansthattheabstractandintroductiondonotincludetheclaimsmade
inthepaper.
• The abstract and/or introduction should clearly state the claims made, including the
contributionsmadeinthepaperandimportantassumptionsandlimitations. ANoorNA
answertothisquestionwillnotbeperceivedwellbythereviewers.
• The claims made should match theoretical and experimental results, and reflect how
muchtheresultscanbeexpectedtogeneralizetoothersettings.
• Itisfinetoincludeaspirationalgoalsasmotivationaslongasitisclearthatthesegoals
arenotattainedbythepaper.
2. Limitations
Question: Doesthepaperdiscussthelimitationsoftheworkperformedbytheauthors?
Answer: [Yes]
Justification: SeeSection8(Conclusionanddiscussion).
Guidelines:
• TheanswerNAmeansthatthepaperhasnolimitationwhiletheanswerNomeansthat
thepaperhaslimitations,butthosearenotdiscussedinthepaper.
• Theauthorsareencouragedtocreateaseparate"Limitations"sectionintheirpaper.
• Thepapershouldpointoutanystrongassumptionsandhowrobusttheresultsareto
violations of these assumptions (e.g., independence assumptions, noiseless settings,
modelwell-specification,asymptoticapproximationsonlyholdinglocally). Theauthors
should reflect on how these assumptions might be violated in practice and what the
implicationswouldbe.
• Theauthorsshouldreflectonthescopeoftheclaimsmade,e.g.,iftheapproachwasonly
testedonafewdatasetsorwithafewruns. Ingeneral,empiricalresultsoftendependon
implicitassumptions,whichshouldbearticulated.
• Theauthorsshouldreflectonthefactorsthatinfluencetheperformanceoftheapproach.
Forexample,afacialrecognitionalgorithmmayperformpoorlywhenimageresolution
isloworimagesaretakeninlowlighting. Oraspeech-to-textsystemmightnotbeused
reliablytoprovideclosedcaptionsforonlinelecturesbecauseitfailstohandletechnical
jargon.
• Theauthorsshoulddiscussthecomputationalefficiencyoftheproposedalgorithmsand
howtheyscalewithdatasetsize.
• Ifapplicable,theauthorsshoulddiscusspossiblelimitationsoftheirapproachtoaddress
problemsofprivacyandfairness.
• Whiletheauthorsmightfearthatcompletehonestyaboutlimitationsmightbeusedby
reviewersasgroundsforrejection,aworseoutcomemightbethatreviewersdiscover
limitations that aren’t acknowledged in the paper. The authors should use their best
judgmentandrecognizethatindividualactionsinfavoroftransparencyplayanimportant
roleindevelopingnormsthatpreservetheintegrityofthecommunity. Reviewerswillbe
specificallyinstructedtonotpenalizehonestyconcerninglimitations.
3. TheoryAssumptionsandProofs
Question: Foreachtheoreticalresult,doesthepaperprovidethefullsetofassumptionsand
acomplete(andcorrect)proof?
Answer: [Yes]
35Justification: SeeAssumptions1-3inSection5andproofinAppendicesC-H.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludetheoreticalresults.
• All the theorems, formulas, and proofs in the paper should be numbered and cross-
referenced.
• Allassumptionsshouldbeclearlystatedorreferencedinthestatementofanytheorems.
• Theproofscaneitherappearinthemainpaperorthesupplementalmaterial,butifthey
appearinthesupplementalmaterial,theauthorsareencouragedtoprovideashortproof
sketchtoprovideintuition.
• Inversely,anyinformalproofprovidedinthecoreofthepapershouldbecomplemented
byformalproofsprovidedinappendixorsupplementalmaterial.
• TheoremsandLemmasthattheproofreliesuponshouldbeproperlyreferenced.
4. ExperimentalResultReproducibility
Question: Doesthepaperfullydisclosealltheinformationneededtoreproducethemainex-
perimentalresultsofthepapertotheextentthatitaffectsthemainclaimsand/orconclusions
ofthepaper(regardlessofwhetherthecodeanddataareprovidedornot)?
Answer: [Yes]
Justification: SeeSection7(Numericalexperiments)andAppendixA.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• Ifthepaperincludesexperiments,aNoanswertothisquestionwillnotbeperceived
wellbythereviewers: Makingthepaperreproducibleisimportant,regardlessofwhether
thecodeanddataareprovidedornot.
• Ifthecontributionisadatasetand/ormodel,theauthorsshoulddescribethestepstaken
tomaketheirresultsreproducibleorverifiable.
• Dependingonthecontribution,reproducibilitycanbeaccomplishedinvariousways.
Forexample,ifthecontributionisanovelarchitecture,describingthearchitecturefully
mightsuffice,orifthecontributionisaspecificmodelandempiricalevaluation,itmay
benecessarytoeithermakeitpossibleforotherstoreplicatethemodelwiththesame
dataset, or provide access to the model. In general. releasing code and data is often
onegoodwaytoaccomplishthis,butreproducibilitycanalsobeprovidedviadetailed
instructionsforhowtoreplicatetheresults,accesstoahostedmodel(e.g.,inthecase
ofalargelanguagemodel),releasingofamodelcheckpoint,orothermeansthatare
appropriatetotheresearchperformed.
• WhileNeurIPSdoesnotrequirereleasingcode,theconferencedoesrequireallsubmis-
sionstoprovidesomereasonableavenueforreproducibility,whichmaydependonthe
natureofthecontribution. Forexample
(a) Ifthecontributionisprimarilyanewalgorithm,thepapershouldmakeitclearhow
toreproducethatalgorithm.
(b) Ifthecontributionisprimarilyanewmodelarchitecture,thepapershoulddescribe
thearchitectureclearlyandfully.
(c) Ifthecontributionisanewmodel(e.g.,alargelanguagemodel),thenthereshould
eitherbeawaytoaccessthismodelforreproducingtheresultsorawaytoreproduce
themodel(e.g.,withanopen-sourcedatasetorinstructionsforhowtoconstructthe
dataset).
(d) Werecognizethatreproducibilitymaybetrickyinsomecases,inwhichcaseauthors
arewelcometodescribetheparticularwaytheyprovideforreproducibility. Inthe
caseofclosed-sourcemodels,itmaybethataccesstothemodelislimitedinsome
way(e.g.,toregisteredusers),butitshouldbepossibleforotherresearcherstohave
somepathtoreproducingorverifyingtheresults.
5. Openaccesstodataandcode
Question: Doesthepaperprovideopenaccesstothedataandcode,withsufficientinstruc-
tionstofaithfullyreproducethemainexperimentalresults,asdescribedinsupplemental
material?
36Answer: [Yes]
Justification: Seethesupplementalmaterial.
Guidelines:
• TheanswerNAmeansthatpaperdoesnotincludeexperimentsrequiringcode.
• Please see the NeurIPS code and data submission guidelines (https://nips.cc/
public/guides/CodeSubmissionPolicy)formoredetails.
• Whileweencouragethereleaseofcodeanddata,weunderstandthatthismightnotbe
possible, so “No” is an acceptable answer. Papers cannot be rejected simply for not
includingcode, unlessthisiscentraltothecontribution(e.g., foranewopen-source
benchmark).
• Theinstructionsshouldcontaintheexactcommandandenvironmentneededtorunto
reproducetheresults. SeetheNeurIPScodeanddatasubmissionguidelines(https:
//nips.cc/public/guides/CodeSubmissionPolicy)formoredetails.
• Theauthorsshouldprovideinstructionsondataaccessandpreparation,includinghow
toaccesstherawdata,preprocesseddata,intermediatedata,andgenerateddata,etc.
• The authors should provide scripts to reproduce all experimental results for the new
proposedmethodandbaselines. Ifonlyasubsetofexperimentsarereproducible,they
shouldstatewhichonesareomittedfromthescriptandwhy.
• At submission time, to preserve anonymity, the authors should release anonymized
versions(ifapplicable).
• Providingasmuchinformationaspossibleinsupplementalmaterial(appendedtothe
paper)isrecommended,butincludingURLstodataandcodeispermitted.
6. ExperimentalSetting/Details
Question: Doesthepaperspecifyallthetrainingandtestdetails(e.g.,datasplits,hyper-
parameters, how they were chosen, type of optimizer, etc.) necessary to understand the
results?
Answer: [Yes]
Justification: SeeAppendixA.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• Theexperimentalsettingshouldbepresentedinthecoreofthepapertoalevelofdetail
thatisnecessarytoappreciatetheresultsandmakesenseofthem.
• Thefulldetailscanbeprovidedeitherwiththecode,inappendix,orassupplemental
material.
7. ExperimentStatisticalSignificance
Question:Doesthepaperreporterrorbarssuitablyandcorrectlydefinedorotherappropriate
informationaboutthestatisticalsignificanceoftheexperiments?
Answer: [Yes]
Justification: SeeSection7(Numericalexperiments)andAppendixA.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• Theauthorsshouldanswer"Yes"iftheresultsareaccompaniedbyerrorbars,confidence
intervals,orstatisticalsignificancetests,atleastfortheexperimentsthatsupportthe
mainclaimsofthepaper.
• Thefactorsofvariabilitythattheerrorbarsarecapturingshouldbeclearlystated(for
example,train/testsplit,initialization,randomdrawingofsomeparameter,oroverall
runwithgivenexperimentalconditions).
• Themethodforcalculatingtheerrorbarsshouldbeexplained(closedformformula,call
toalibraryfunction,bootstrap,etc.)
• Theassumptionsmadeshouldbegiven(e.g.,Normallydistributederrors).
• Itshouldbeclearwhethertheerrorbaristhestandarddeviationorthestandarderrorof
themean.
37• It is OK to report 1-sigma error bars, but one should state it. The authors should
preferablyreporta2-sigmaerrorbarthanstatethattheyhavea96%CI,ifthehypothesis
ofNormalityoferrorsisnotverified.
• For asymmetric distributions, the authors should be careful not to show in tables or
figuressymmetricerrorbarsthatwouldyieldresultsthatareoutofrange(e.g. negative
errorrates).
• Iferrorbarsarereportedintablesorplots,Theauthorsshouldexplaininthetexthow
theywerecalculatedandreferencethecorrespondingfiguresortablesinthetext.
8. ExperimentsComputeResources
Question: Foreachexperiment,doesthepaperprovidesufficientinformationonthecom-
puterresources(typeofcomputeworkers,memory,timeofexecution)neededtoreproduce
theexperiments?
Answer: [Yes]
Justification: SeeSection7(Numericalexperiments)andAppendixA.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotincludeexperiments.
• ThepapershouldindicatethetypeofcomputeworkersCPUorGPU,internalcluster,or
cloudprovider,includingrelevantmemoryandstorage.
• Thepapershouldprovidetheamountofcomputerequiredforeachoftheindividual
experimentalrunsaswellasestimatethetotalcompute.
• Thepapershoulddisclosewhetherthefullresearchprojectrequiredmorecomputethan
theexperimentsreportedinthepaper(e.g.,preliminaryorfailedexperimentsthatdidn’t
makeitintothepaper).
9. CodeOfEthics
Question: Doestheresearchconductedinthepaperconform, ineveryrespect, withthe
NeurIPSCodeofEthicshttps://neurips.cc/public/EthicsGuidelines?
Answer: [Yes]
Justification: Seethesupplementalmaterial.
Guidelines:
• TheanswerNAmeansthattheauthorshavenotreviewedtheNeurIPSCodeofEthics.
• IftheauthorsanswerNo,theyshouldexplainthespecialcircumstancesthatrequirea
deviationfromtheCodeofEthics.
• Theauthorsshouldmakesuretopreserveanonymity(e.g.,ifthereisaspecialconsidera-
tionduetolawsorregulationsintheirjurisdiction).
10. BroaderImpacts
Question: Does the paper discuss both potential positive societal impacts and negative
societalimpactsoftheworkperformed?
Answer: [Yes]
Justification: SeeSection8(Conclusionanddiscussion).
Guidelines:
• TheanswerNAmeansthatthereisnosocietalimpactoftheworkperformed.
• IftheauthorsanswerNAorNo, theyshouldexplainwhytheirworkhasnosocietal
impactorwhythepaperdoesnotaddresssocietalimpact.
• Examplesofnegativesocietalimpactsincludepotentialmaliciousorunintendeduses
(e.g., disinformation, generating fake profiles, surveillance), fairness considerations
(e.g.,deploymentoftechnologiesthatcouldmakedecisionsthatunfairlyimpactspecific
groups),privacyconsiderations,andsecurityconsiderations.
• The conference expects that many papers will be foundational research and not tied
toparticularapplications,letalonedeployments. However,ifthereisadirectpathto
anynegativeapplications,theauthorsshouldpointitout. Forexample,itislegitimate
topointoutthatanimprovementinthequalityofgenerativemodelscouldbeusedto
38generatedeepfakesfordisinformation. Ontheotherhand,itisnotneededtopointout
thatagenericalgorithmforoptimizingneuralnetworkscouldenablepeopletotrain
modelsthatgenerateDeepfakesfaster.
• Theauthorsshouldconsiderpossibleharmsthatcouldarisewhenthetechnologyisbeing
usedasintendedandfunctioningcorrectly,harmsthatcouldarisewhenthetechnologyis
beingusedasintendedbutgivesincorrectresults,andharmsfollowingfrom(intentional
orunintentional)misuseofthetechnology.
• Iftherearenegativesocietalimpacts,theauthorscouldalsodiscusspossiblemitigation
strategies (e.g., gated release of models, providing defenses in addition to attacks,
mechanismsformonitoringmisuse,mechanismstomonitorhowasystemlearnsfrom
feedbackovertime,improvingtheefficiencyandaccessibilityofML).
11. Safeguards
Question: Doesthepaperdescribesafeguardsthathavebeenputinplaceforresponsible
releaseofdataormodelsthathaveahighriskformisuse(e.g.,pretrainedlanguagemodels,
imagegenerators,orscrapeddatasets)?
Answer: [NA]
Justification: Thispaperposesnosuchrisks.
Guidelines:
• TheanswerNAmeansthatthepaperposesnosuchrisks.
• Releasedmodelsthathaveahighriskformisuseordual-useshouldbereleasedwith
necessarysafeguardstoallowforcontrolleduseofthemodel,forexamplebyrequiring
thatusersadheretousageguidelinesorrestrictionstoaccessthemodelorimplementing
safetyfilters.
• DatasetsthathavebeenscrapedfromtheInternetcouldposesafetyrisks. Theauthors
shoulddescribehowtheyavoidedreleasingunsafeimages.
• Werecognizethatprovidingeffectivesafeguardsischallenging,andmanypapersdonot
requirethis,butweencourageauthorstotakethisintoaccountandmakeabestfaith
effort.
12. Licensesforexistingassets
Question: Arethecreatorsororiginalownersofassets(e.g.,code,data,models),usedin
thepaper,properlycreditedandarethelicenseandtermsofuseexplicitlymentionedand
properlyrespected?
Answer: Yes
Justification: Wehaveproperlycitedthecreatorsofthemodels[10,37]inSection1and7.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotuseexistingassets.
• Theauthorsshouldcitetheoriginalpaperthatproducedthecodepackageordataset.
• Theauthorsshouldstatewhichversionoftheassetisusedand,ifpossible,includea
URL.
• Thenameofthelicense(e.g.,CC-BY4.0)shouldbeincludedforeachasset.
• For scraped data from a particular source (e.g., website), the copyright and terms of
serviceofthatsourceshouldbeprovided.
• Ifassetsarereleased,thelicense,copyrightinformation,andtermsofuseinthepack-
ageshouldbeprovided. Forpopulardatasets,paperswithcode.com/datasetshas
curatedlicensesforsomedatasets. Theirlicensingguidecanhelpdeterminethelicense
ofadataset.
• Forexistingdatasetsthatarere-packaged,boththeoriginallicenseandthelicenseofthe
derivedasset(ifithaschanged)shouldbeprovided.
• Ifthisinformationisnotavailableonline,theauthorsareencouragedtoreachouttothe
asset’screators.
13. NewAssets
Question:Arenewassetsintroducedinthepaperwelldocumentedandisthedocumentation
providedalongsidetheassets?
39Answer: [Yes]
Justification: SeeAppendixAandthesupplementalmaterial.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotreleasenewassets.
• Researchersshouldcommunicatethedetailsofthedataset/code/modelaspartoftheir
submissions via structured templates. This includes details about training, license,
limitations,etc.
• Thepapershoulddiscusswhetherandhowconsentwasobtainedfrompeoplewhose
assetisused.
• Atsubmissiontime,remembertoanonymizeyourassets(ifapplicable). Youcaneither
createananonymizedURLorincludeananonymizedzipfile.
14. CrowdsourcingandResearchwithHumanSubjects
Question: Forcrowdsourcingexperimentsandresearchwithhumansubjects,doesthepaper
includethefulltextofinstructionsgiventoparticipantsandscreenshots,ifapplicable,as
wellasdetailsaboutcompensation(ifany)?
Answer: [NA]
Justification: Thispaperdoesnotinvolvecrowdsourcingnorresearchwithhumansubjects.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
humansubjects.
• Includingthisinformationinthesupplementalmaterialisfine,butifthemaincontri-
butionofthepaperinvolveshumansubjects,thenasmuchdetailaspossibleshouldbe
includedinthemainpaper.
• AccordingtotheNeurIPSCodeofEthics,workersinvolvedindatacollection,curation,
or other labor should be paid at least the minimum wage in the country of the data
collector.
15. InstitutionalReviewBoard(IRB)ApprovalsorEquivalentforResearchwithHuman
Subjects
Question: Doesthepaperdescribepotentialrisksincurredbystudyparticipants,whether
suchrisksweredisclosedtothesubjects,andwhetherInstitutionalReviewBoard(IRB)
approvals(oranequivalentapproval/reviewbasedontherequirementsofyourcountryor
institution)wereobtained?
Answer: [NA]
Justification: Thispaperdoesnotinvolvecrowdsourcingnorresearchwithhumansubjects.
Guidelines:
• TheanswerNAmeansthatthepaperdoesnotinvolvecrowdsourcingnorresearchwith
humansubjects.
• Dependingonthecountryinwhichresearchisconducted,IRBapproval(orequivalent)
mayberequiredforanyhumansubjectsresearch. IfyouobtainedIRBapproval,you
shouldclearlystatethisinthepaper.
• Werecognizethattheproceduresforthismayvarysignificantlybetweeninstitutions
andlocations,andweexpectauthorstoadheretotheNeurIPSCodeofEthicsandthe
guidelinesfortheirinstitution.
• Forinitialsubmissions,donotincludeanyinformationthatwouldbreakanonymity(if
applicable),suchastheinstitutionconductingthereview.
40