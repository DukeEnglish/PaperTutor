ScalingFilter: Assessing Data Quality through Inverse Utilization of
Scaling Laws
RuihangLi1,2,YixuanWei2,3,MiaosenZhang2,4,
NenghaiYu1,HanHu2,HouwenPeng2*
1UniversityofScienceandTechnologyofChina,2MicrosoftResearchAsia,
3TsinghuaUniversity,4SoutheastUniversity
Abstract Assign
Quality Factors
Raw Data Select Train
High-qualitydataiscrucialforthepre-training (RedpajamaV2) Small High-quality Data Language Model
LM
performanceoflargelanguagemodels. Unfor-
tunately,existingqualityfilteringmethodsrely Large
LM
onaknownhigh-qualitydatasetasreference,
which can introduce potential bias and com-
Figure1: InScalingFilter,weassessthequalityoftext
promise diversity. In this paper, we propose
documentsbytheirscalingcharacteristicswithlanguage
ScalingFilter,anovelapproachthatevaluates
modelsindifferentsizes.
textqualitybasedontheperplexitydifference
between two language models trained on the
reference-free approaches. Reference-dependent
same data, thereby eliminating the influence
methods,suchasbinaryclassification(Brownetal.,
ofthereferencedatasetinthefilteringprocess.
2020;Gaoetal.,2020;Chowdheryetal.,2023)and AntheoreticalanalysisshowsthatScalingFilter
isequivalenttoaninverseutilizationofscaling DSIR(Xieetal.,2023),filteroutlow-qualitydata
laws. Throughtrainingmodelswith1.3Bpa- by comparing it with high-quality seed datasets.
rametersonthesamedatasourceprocessedby Whileeffective,thesemethodsinevitablyintroduce
variousqualityfilters,wefindScalingFiltercan biasespresentinthereferencedata,suchasspecific
improvezero-shotperformanceofpre-trained
writingstylesortopics,therebylimitingthediver-
modelsindownstreamtasks. Toassessthebias
sityandrepresentativenessoftrainingcorpus(Sol-
introduced by quality filtering, we introduce
dainietal.,2023). Incontrast,reference-freemeth-
semanticdiversity,ametricofutilizingtextem-
ods,suchasperplexitygating(Marionetal.,2023),
beddingmodelsforsemanticrepresentations.
Extensiveexperimentsrevealthatsemanticdi- assess data quality using predefined metrics like
versityisareliableindicatorofdatasetdiversity, perplexityscoresfrompre-trainedmodels. These
andScalingFilterachievesanoptimalbalance methods mitigate the biases introduced by refer-
betweendownstreamperformanceandseman-
encedatasetsbutencounterchallengesduetothe
ticdiversity.
indirect relationship between absolute perplexity
1 Introduction and document quality. This indirect relationship
inadvertentlyfavordatawithsimpleandrepetitive
The success of large language models (LLMs) is
content. Althoughsuchcontentiseasierformod-
significantlyinfluencedbythequalityandquantity
elstopredict,itcontributesminimallytolearning
of the pre-training corpus. Researchers have de-
diversityandcomplexity(Wettigetal.,2024).
velopedvariousdatacurationpipelinestoenhance
Toaddresstheseissues,weintroduceasimple
datasetquality,focusingonrawwebcrawling,text
yeteffectivequalityfilteringapproachnamedScal-
extraction, repetition and toxic content removal,
ingFilter,whichinverselyleveragesrecentscaling
and,notably,qualityfiltering(Brownetal.,2020;
lawsingenerativemodelingtoassessdataquality.
Raeetal.,2021;Penedoetal.,2023).
The core idea is to analyze the perplexity differ-
Quality filters aim to extract high-quality data
encesbetweentwopre-trainedmodelsonthesame
from a noisy raw corpus, thereby improving the
dataandassessthedataqualitybasedonthesedif-
languagemodel’sperformancewithoutincreasing
ferences. We find a positive correlation between
training costs. Existing filters are broadly classi-
dataqualityandperplexitydifferencesbyinversely
fiedintotwocategories: reference-dependentand
deriving Chinchilla scaling law (Hoffmann et al.,
*Correspondingauthor. 2022). Inotherwords,givenapairofpre-trained
1
4202
guA
51
]LC.sc[
1v01380.8042:viXramodels of different sizes, documents with higher anovelandreliablemetric. Extensiveexper-
perplexitydifferencesindicatehigherquality. iments demonstrate that ScalingFilter more
ScalingFilter involves utilizing the difference effectivelypreservestherichnessandvariety
between two separate models for data quality as- oftherawdatacomparedtoconventionalqual-
sessment,effectivelyaddressingthebiasissuein- ityfilteringapproaches.
ducedbyrelyingonasinglemodeltrainedonthe
reference data. This approach also mitigates the 2 Methodology
problem of selecting simple and repetitive texts
thatarisefromoverfittingtotheperplexitymetric, Overview. Existing quality filtering methods
thereby enhancing data diversity and complexity. dependoneitherareferencehigh-qualitydatasetor
Furthermore,ScalingFilteroffersatheoreticalanal- theabsoluteperplexityscoresofdocumentsfrom
ysis for using perplexity differences as a quality asinglelanguagemodel,whichcanintroducepo-
indicator for data filtering by inversely deriving tential bias or result in inferior performance. In
modelscalinglaws. thissection,wewillelaborateontheprinciplesof
OurexperimentsdemonstratethatScalingFilter ScalingFilterthroughmathematicalderivation. The
is superior to existing methods in improving fil- coreconceptofScalingFilterliesinestimatingthe
tereddataqualitywhilepreservingdatadiversity. qualityofdatasamplesbyinverselyapplyingthe
Specifically,weemployapairofmeta-modelswith scalinglaw. Specifically,thescalinglawrevealsa
sizesof124Mand774Mparameterstoassessthe power-lawdecreaseinlosswithincreasingmodel
perplexitydifferenceforeachdocumentintheraw size or data size (Hestness et al., 2017; Kaplan
dataset, and then select the high-quality ones us- et al., 2020; Hoffmann et al., 2022; Aghajanyan
ingatop-k strategy. Wetraina1.3Bmodelfrom et al., 2023). Ultimately, the scaling law yields
scratch using filtered high-quality data. We then theoptimalmodel/datascaling-upallocationstrat-
evaluateitszero-shotperformanceondownstream egy. Inotherwords,underthesamecomputational
tasks and assess the semantic diversity of the fil- budget(TFLOPS),itdeterminestheoptimalratio
tereddataset. ofmodelsizetothenumberoftrainingtokensto
The results demonstrate that ScalingFilter out- achieve the lowest loss, represented by a model
performstheunfilteredbaselineandpreviousstate- scalingexponentaandadatascalingexponentb.
of-the-art(SoTA)qualityfilteringmethods. Specif- Extensive experiments comparing multiple
ically, compared to the unfiltered baseline, Scal- datasets with known quality differences revealed
ingFilterachievesa+3.09%improvementindown- thathigh-qualitydataincreasesthemodelscaling
stream accuracy and a +2.23 increase in seman- exponent a (Bi et al., 2024). Specifically, the ex-
ticdiversity. Whencomparedwithperplexitygat- perimentscomparedtheearlyandfinalversionsof
ing(Marionetal.,2023;Wenzeketal.,2019),Scal- in-housedatatogetherwithOpenWebText2(Gao
ingFilter achieves a +1.12% improvement in per- et al., 2020), revealing that the final version of
formanceanda+4.7increaseinsemanticdiversity. OpenWebText2resultsinthehighesta,whilethe
Insummary, thecontributionsofthisworkare earlyversionwiththepoorestqualityleadstothe
threefold: lowesta. Intuitively,ahighervalueofaaccelerates
1. We introduce quality factor, a novel metric the rate at which the loss decreases as the model
that correlates directly with the quality of parametersincrease. Thispositiverelationshipwill
trainingdatathroughthelensofmodelscal- be demonstrated later. Such an observation sug-
inglaws,offeringamorepreciseandunbiased geststhathigh-qualitydataenhanceslogicalclarity
approachtodatacuration. and decreases predictive difficulty after adequate
training. Consequently,scalingupthemodelsize
2. WeproposeScalingFilter,anewqualityfilter-
becomes more beneficial when the compute bud-
ingmethodthatutilizesthequalityfactor to
getisincreased(Bietal.,2024;Aghajanyanetal.,
curate high-quality datasets without relying
2023;Kaplanetal.,2020;Hoffmannetal.,2022).
onreferencedata,therebymitigatingtherisk
Byinverselyapplyingthisprinciple,ScalingFilter
ofbiasandenhancingtherepresentativeness
estimatesdataqualitybasedontherateoflossde-
ofthetrainingcorpus.
creaseinmodelswithafixedparameterdifference,
3. To evaluate the data diversity of filtered therebyseparatinghigh-qualitydatafromtheraw
datasets, we introduce semantic diversity as dataset.
2LowQualityData UnfilteredCC
Wikipedia
HighQualityData
OpenWebText
Books3
3.5
l1 k1
k1
a1
k2
3.0 a1
l2
k2
a2
a2 a a3
4
2.5
N0 Np Nq 117 345 762 1542
ModelParametersN ModelParametersN(M)
(a) (b)
Figure2: (a)Avisualdiagramillustratesthetheoreticalresultthathigh-qualitydataacceleratestherateofloss
decreaseasmodelparametersincrease,resultinginlargermodelscalingexponentsa. (b)Wecalculatedtheaverage
lossofGPT-2modelsofdifferentsizesonseveraldatasetswithrecognizedqualitylevels: Wikipedia,OpenWebText,
andBooks3representhigh-qualitydata,whileUnfilteredCommonCrawlrepresentslow-qualitydata. Theresults
closelyalignwiththetheoreticalanalysisshownin(a),whichindicatesthathigh-qualitydataacceleratestherateof
lossdecreaseasmodelparametersincrease.
Toproceed,wewillfirstdefinethequalityfactor, Qualityfactorispositivelycorrelatedwithdata
which is the magnitude of loss reduction. Then, quality. Next,wewillintroducetheexpression
starting from the formula of the scaling law, we ofthescalinglaw(Hoffmannetal.,2022;Kaplan
willdemonstratethepositivecorrelationbetween et al., 2020; Henighan et al., 2020; Aghajanyan
thequalityfactorandthemodelscalingexponent etal.,2023)andtransformitintoaforminvolving
a. Furthermore,basedonthepositivecorrelation themodelscalingexponentawhich, asweintro-
betweenaanddataqualityobservedin(Bietal., ducedintheoverview,isknowntohaveapositive
2024),wecanultimatelyprovethepositivecorre- relationshipwithdataquality(Bietal.,2024).
lationbetweenthequalityfactoranddataquality. Given the number of model parameters N and
Qualityfactor. Webeginwithdefiningthequal- training tokens D, the expected model loss Lˆ is
formulated(Hoffmannetal.,2022)as:
ityfactor,whichwewilllaterdemonstratetohave
apositivecorrelationwithdataquality. Wedenote
A B
thesmallermeta-modelaspandthelargeroneas Lˆ(N,D) = E + + (2)
Nα Dβ
q. Bothmeta-modelssharethesamearchitecture
where E represents the minimal achievable loss,
andaretrainedonthesamedataset,withtheonly
correspondingtotheentropyofnaturaltext. The
difference being the parameter counts: N for p
p terms A and B account for the functional ap-
and N for q, with N < N . Let x be a given Nα Dβ
q p q i
proximationerrorandtheoptimizationorconver-
text sample, and denote the quality factor of this
genceerror,respectively(Aghajanyanetal.,2023).
sampleasd . Then,wehave:
i Here,A,B,α,andβ arehyperparametersrelated
tothemodelarchitectureandthetrainingdata. The
PPL (x )
d := p i (1) scaling law, indicating the optimized numbers of
i
PPL (x )
q i N andDunderagivencomputebudgetC,follows
apower-lawform(Kaplanetal.,2020;Hoffmann
wherePPL (x )andPPL (x )representtheper-
p i q i
etal.,2022):
plexityscoresofthetextsamplex whenevaluated
i
bypandq,respectively. It’simportanttonotethat
N ∝ Ca D ∝ Cb (3)
perplexityhasadirectrelationshipwiththecross- opt opt
entropylossLbecausePPL = 2L,indicatingthat wherea = β andb = α representthemodel
α+β α+β
theperplexityscoreispositivelyrelatedtotheloss anddatascalingexponents(Bietal.,2024;Hoff-
L. mannetal.,2022)respectively,indicatingthepro-
3
Lˆ
ssoLdetcepxE
L¯ssoLegarevAportionsofthetotalcomputationalbudgetallocated shown in Figure 2a, where the high-quality data
to model scaling and data scaling in the optimal shows a steeper secant (k > k ) compared to
2 1
computationallocation. low-quality data. It’s worth noting that a single
.
Consider setting η = α + β, then Lˆ can be casemightdeviatefromthetrainingdatadistribu-
presentedas: tionofthemeta-models,leadingtohigherabsolute
perplexityforvariousmodelsizes. Thus, relying
A B
Lˆ(N,D) = E + + (4) solely on single perplexity as a quality criterion
N(1−a)η Daη
canresultinmisjudgments. However,high-quality
Wefocusontherelationshipbetweenexpectedloss datafollowsalawwhereperplexitydecreasesmore
Lˆ andmodelscalingexponentaaswellasmodel withanincreaseinmodelparameters,indicatinga
sizeN,andthusdenoteLˆ asLˆ(a,N).
It’sobvious greaterperplexitydifference(i.e.,qualityfactor).
thatLˆ decreasesasN increases. Wefurtherprove
Selecting high-quality data with quality factor.
inAppendixA.2.1thatataspecificN ,theslope
0 Wehavedemonstratedthatthequalityfactor can
of the tangent to the Lˆ − N curve decreases as
directly characterize data quality above, so it’s
a increases (i.e., the larger the a, the steeper the
straightforward to directly use it to select high-
tangent,asillustratedinFigure2athatl issteeper
2 quality data from a noisy unfiltered dataset. We
thanl ). Duetothismonotonicrelationship,wecan
1 callthissimpleyeteffectivemethodasScalingFil-
inferthevalueofafromtheslopeofthetangent:
ter,asillustratedinFigure1. Consideraunfiltered
foragivenN ,asteepertangent(greaterabsolute
0 setofdocumentsS,containingbothhighandlow-
valueoftheslope)indicatesalargera,thatis:
quality documents. For each sample x ∈ S, we
i
(cid:12) calculatethequalityfactord forit. Asderivedpre-
∂L(cid:12) i
a ∝ − (cid:12) (5) viously,sampleswithhigherd areofbetterquality.
∂N(cid:12) i
N=N0 Thetop-k samplesarethenselectedbasedonthe
Furthermore,weproveinAppendixA.2.2that desiredcleaningratetoformtheresultingdataset.
the above conclusion can be extended from the
tangent slope at a given N to the slope of the 3 Experiments
0
secantlineforanygiven∆N (i.e. k inFigure2a).
i
Letting∆N = N −N ,theslopeofthesecantline In this section, we will demonstrate the effec-
q p
tivenessofScalingFilterthroughextensiveexper-
is always negative, and a is positively correlated
iments. Specifically, language models trained on
withthenegativeslopeofthesecantline. Sincethe
datafilteredbyScalingFilterconsistentlyachieved
quality factor d is also positively correlated with
superiorperformanceacrossvariousdownstream
thenegativeslopeofthesecantline,itfollowsthat
tasks,comparedtotheunfilteredbaselineandother
dispositivelycorrelatedwitha:
commonqualityfilteringapproaches,highlighting

a ∝
−Lˆ(Nq)−Lˆ(Np)
, thehigherqualityofthedata. Furthermore,bymea-
 Nq−Np
suringthesemanticdiversityofthefiltereddataset,

d =
2Lˆ(Np)−Lˆ(Nq)
=
2−(Lˆ(Nq)−Lˆ(Np))
we found that ScalingFilter effectively preserved
thediversitypresentintheoriginaldataset.
=⇒ d ∝ a
(6)
3.1 DataQualityEvaluation
Based on empirical observations from (Bi et al.,
2024), highervaluesofaareachievedwithhigh- Setup. We begin with five CommonCrawl
qualitydata,indicatingthatqualityfactordispos- dumpsfrom2019to2023,processedthroughthe
itivelycorrelatedwithdataquality. CCNet (Wenzek et al., 2019) pipeline, in accor-
Thisconclusionalignswithourpracticalcompar- dance with (Computer, 2023). From the prepro-
ativetests. AsshowninFigure2b,wecalculated cesseddataset,500GBoftextdataarerandomly
theaveragelossofGPT-2modelsofdifferentsizes selected as our baseline, yielding approximately
onseveraldatasetswithrecognizedqualitylevels: 125 billion tokens for additional quality filtering.
Wikipedia, OpenWebText, and Books3 represent Ineachexperiment,wetrainadecoder-onlymodel
high-qualitydata,whileUnfilteredCommonCrawl with 1.3B parameters, using the same model ar-
represents low-quality data, based on a random chitecture as (Peng et al., 2023). Each model is
sampleof10kdocumentsfromeachdataset. The trained on 25B tokens until performance levels
resultsaligncloselywiththetheoreticalestimates off, according to (Hoffmann et al., 2022; Penedo
4Table1: Zero-shotdownstreamaccuracyofmodelstrainedwithdifferentqualityfilters. Wecoveravarietyof
tasksandwidelyuseddatasets(Penedoetal.,2023;Brownetal.,2020;Chowdheryetal.,2023;Deyetal.,2023;
Biderman et al., 2023), including sentence completion, coreference resolution, natural language inference and
multiple-choicequestionanswering. Forbinaryclassification(Brownetal.,2020;Chowdheryetal.,2023;Touvron
et al., 2023) and importance resampling (Xie et al., 2023), we leverage the best results from various reference
datasets,whereasperplexitygating(Marionetal.,2023)utilizesthelargermodel’sperplexityinourmeta-models.
QualityFilter Hellaswag LAMBADA Winogrande PIQA ARC OpenbookQA BoolQ Avg
Random 45.40 41.96 51.07 69.80 39.88 32.40 56.76 48.18
BinaryClassification 48.13 48.96 54.30 69.75 41.66 30.40 61.38 50.65
ImportanceResampling 47.52 48.36 54.38 68.50 41.63 32.60 60.80 50.54
PerplexityGating 48.17 48.96 53.04 69.75 41.54 29.60 60.00 50.15
ScalingFilter(Ours) 49.07 48.42 55.09 70.57 42.67 31.40 61.68 51.27
etal.,2023;Marionetal.,2023),whichtakesap- we follow the settings in (Xie et al., 2023), with
proximately 4 days on 4 nodes with 8 NVIDIA OpenWebText(GokaslanandCohen,2019)asthe
TeslaV100GPUs. Weusepre-trainedGPT-2mod- target dataset. As for perplexity gating, we fol-
els(Radfordetal.,2019)asdefaultmeta-models low (Marion et al., 2023) as well as our cleaning
to calculate quality factors for each sample. The ratio,keepingdocumentswithperplexityranging
smaller and larger models have 124M and 774M from 15th to 85th percentiles, resulting in keep-
parameters, respectively. We later perform abla- ing the middle 70% documents of the unfiltered
tionstudiestodiscussimpactsbythepre-training dataset. Perplexity is computed by the larger of
data. Following (Penedo et al., 2023), we utilize themeta-models,theonewithhighercapacityand
the lm-evaluation-harness library (Gao et al., ability.
2023)toevaluatezero-shotperformanceacrossvar- Results. Table1showsthecomparisonbetween
ious downstream tasks of each model trained on variousdataqualityfiltermethods. Insummary:
documentsretainedthroughspecificqualityfilter-
ingmethod. Weencompassesavarietyoftasksand • Onaverage,ScalingFiltershowsa0.62%im-
widelyuseddatasets(Penedoetal.,2023;Brown provementoverthewidely-usedbinaryclassi-
et al., 2020; Chowdhery et al., 2023; Dey et al., ficationqualityfilteringmethodanda0.73%
2023; Biderman et al., 2023), including sentence improvement over importance resampling,
completion (Hellaswag (Zellers et al., 2019) and achievingthestate-of-the-artperformance.
LAMBADA (Paperno et al., 2016)), coreference
resolution(Winogrande(Sakaguchietal.,2021)), • ScalingFilterachievesa1.12%improvement
natural language inference (ARC (Clark et al., in average accuracy over perplexity gating,
2018)), and multiple-choice question answering a competing reference-free quality filtering
(PIQA(Bisketal.,2020),OpenbookQA(Mihaylov approach.
etal.,2018),BoolQ(Clarketal.,2019)).
Notably,forbinaryclassification(Brownetal.,
Baselines. WecompareScalingFilterwithran- 2020;Chowdheryetal.,2023;Touvronetal.,2023)
domselection,binaryclassification(Brownetal., andimportanceresampling(Xieetal.,2023),we
2020; Gao et al., 2020; Chowdhery et al., 2023; usethebestresultsfromvariousreferencedatasets,
Touvronetal.,2023),importanceresampling(Xie specifically OpenWebText. The results for per-
et al., 2023) and perplexity gating (Marion et al., plexitygating(Marionetal.,2023)usethelarger
2023). Allqualityfilterswillkeep70%oftheunfil- model’s perplexity in our meta-models for a fair
tereddocumentsinalignwith(Computer,2023),if comparison. Ablations concerning the reference
notspecifiedotherwise. Asforbinaryclassification, datasets of the aforementioned methods will be
we choose Wikipedia, books and OpenWebText discussedinsubsequentsections.
aspositivesamplesandunfilteredCommonCrawl Meta-modelstrainedonvariousdatasetsexhibit
documentsasnegativeones,following(Duetal., competitiveandcomparableperformance. We
2022;Chowdheryetal.,2023). Wesettheshape detailablationstudieswithmeta-modelstrainedon
parameter of Pareto distribution α = 9, follow- different datasets in Table 2. Besides the meta-
ing(Brownetal.,2020;Gaoetal.,2020;Chowd- models trained on WebText, results of which are
hery et al., 2023). As to importance resampling, shown in Table 1, we trained meta-models on a
5Table2: Ablationsoneffectsofmeta-modelstrainingdatawithintheScalingFilterframework. Theresultsreveal
thatmeta-modelstrainedonalternativedatasetsalsoshowcasecompetitiveperformance,indicatingthatthereisnot
anoverlystrongdependencyonmeta-modelspretrainedonWebText,emphasizingtherobustnessandflexibilityof
ScalingFiltervariants.
TrainingData Hellaswag LAMBADA Winogrande PIQA ARC OpenbookQA BoolQ Avg
UnfilteredCC 47.34 47.78 54.22 70.78 40.64 30.40 60.95 50.30
Wikipedia 48.81 47.64 56.67 69.31 41.71 32.60 61.07 51.12
OpenWebText 48.15 46.01 54.06 69.91 43.01 31.40 60.89 50.49
WebText† 49.07 48.42 55.09 70.57 42.67 31.40 61.68 51.27
†FormodelpairstrainedonWebText,wedirectlyuseOpenAIGPT-2modelsfromHuggingFace,whichisthemeta-modelsusedintheoriginalScalingFilter
framework.
subsetofWikipedia,OpenWebText,andunfiltered and books yields inferior results, possibly due to
CommonCrawldatawithnomorethan25Btokens. theclassifier’strainingrecipe,suchasthemixing
Eachdatasetwasusedtotrainmeta-modelsfor1 ratioofthethreedatasets. Surprisingly,importance
epoch. Theresultsdemonstratethatallexperiments resamplingwithWikipediaresultsinsimilaraver-
outperformthebaselineofrandomselection. Train- age accuracy to the random baseline, with much
ingonunfilteredCommonCrawlorOpenWebText betteraccuracyinARCandBoolQbutsignificantly
yieldedresultscompetitivewiththosefromother worse performance in sentence completion tasks
qualityfilteringmethods. Furthermore,trainingon like Hellaswag and LAMBADA, possibly due to
Wikipediaachievedresultsveryclosetothebest, the serious domain shift towards Wikipedia. In
withamarginalgapof0.15%. conclusion,thechoiceofreferencedatasetshasa
significant impact on the performance of quality
Ablations on different sizes of meta-models.
filtersthatrelyonreferences.
We perform experiments to investigate impacts
brought by using pairs of meta-models with dif-
3.2 DataDiversityEvaluation
ferent sizes. The results are briefly presented in
Table 4. When using a pair of meta-models with Training large language models requires diverse
relativelysmalldifferencesinthenumberofparam- data. Currentqualityfilters,byfavoringtextdata
eterstoestimatethequalityfactorsofdata, there similartothereferencedataset,maydiscarddocu-
isacertaindegreeofperformancedegradationin mentsoninformaltopicsorfromminorities,reduc-
the downstream tasks. Reducing the size of the ingthetrainedmodel’sknowledgediversity(Wen-
largermodelsinmeta-modelsfrom774Mto335M zek et al., 2019; Soldaini et al., 2023). How can
decreasestheaverageperformanceondownstream weassessadataset’sdatadiversity? Weintroduce
tasks by 0.96%. Conversely, increasing the size ametrictomeasureadocumentgroup’ssemantic
ofthesmallermodelsinmeta-modelsfrom124M diversity.
to335Mresultsinadecreaseof1.28%inperfor- Semantic diversity metric. Following (Fried-
mance. Thissuggeststhatalargerparametergap manandDieng,2022),wedefinesemanticdiver-
may more effectively amplify differences in how sityastheexponentialoftheShannonentropyof
modelsfittextualdata,allowingforamorereliable thesemanticsimilaritymatrix’seigenvalues. Fora
assessment of the quality factor. Detailed explo- setoftextdocumentsx ,x ,...,x andasemantic
1 2 n
rationofthishypothesisisleftasfuturework. similarityfunctionf,weobtainasimilaritymatrix
S, where each entry s = f(x ,x ). Denoting
Ablations on reference datasets. We also ex- i,j i j
λ ,λ ,...,λ astheeigenvaluesofS/n,wedefine
1 2 n
amine the impacts of different reference datasets
semanticdiversityasfollows.
onpopularqualityfilteringmethodsthatrelyona
reference. Results are shown in Table 3. Binary (cid:32) n (cid:33)
(cid:88)
classificationusingOpenWebTextasthepositive SemanticDiversity = exp − λ logλ
i i
class results in the best performance, similar to
i=1
importance resampling with the same dataset as (7)
a reference. This aligns with the findings in (Bi Apre-trainedlanguagemodelextractseachdoc-
etal.,2024),whichconfirmthatOpenWebTexthas ument’ssemanticembedding,usingcosinesimilar-
superiordataquality. Binaryclassificationwitha ityasthesimilarityfunction. Inourexperiments,
mixeddatasetincludingOpenWebText,Wikipedia, we utilize the bge-base-en-v1.5 model (Xiao
6Table3: Ablationstudiesontheeffectsofreferencedata. Wevariedthereferencedatasetsforbinaryclassifica-
tion(Brownetal.,2020;Chowdheryetal.,2023;Touvronetal.,2023)andimportanceresampling(Xieetal.,2023).
TheresultsindicatethatOpenWebTextistheoptimalreferencedatasetchoiceforbothreference-dependentquality
filteringmethods.
QualityFilter Hellaswag LAMBADA Winogrande PIQA ARC OpenbookQA BoolQ Avg
Random 45.40 41.96 51.07 69.80 39.88 32.40 56.76 48.18
BinaryClassification
OpenWebText 48.13 48.96 54.30 69.75 41.66 30.40 61.38 50.65
Wikipedia 46.80 46.96 53.35 69.15 41.40 32.00 61.31 50.14
Mixed† 47.10 47.68 53.43 68.61 42.19 32.20 57.71 49.85
ImportanceResampling
OpenWebText 47.52 48.36 54.38 68.50 41.63 32.60 60.80 50.54
Wikipedia 43.08 38.56 51.93 66.65 42.76 32.40 61.90 48.18
†Thisexperimentusesamixeddatasetasreferencedatasetfollowing(Duetal.,2022;Chowdheryetal.,2023),withOpenWebText,Wikipedia,andbooks.The
classificationscoresaredirectlyobtainedfromqualitysignalsprovidedbyRedpajamaV2.
   
       
   
       
   
  
      6 H P D Q W L F  ' L Y H U V L W \
 6 W D Q G D U G  ' H Y L D W L R Q
   
  
         
   
 1 X P E H U  R I  ' D W D V H W V   
Figure3: Positivecorrelationbetweenthenumberof       
datasetsandsemanticdiversity,demonstratingsemantic
                                       
diversityasareliablemeasureofdatadiversity.  1 X P E H U  R I  6 D P S O H V
Figure4: Resultsontherelationshipbetweensemantic
etal.,2023)withthesentence_transformersli-
diversityandsamplesize. Semanticdiversitystabilizes
braryduetoitsefficiencyandoutstandingperfor-
at a sample size of 10,000, with a standard deviation
manceinvarioustextembedding-relatedretrieval below0.2. Therefore,wechoose10,000asoursample
andclusteringtasks. sizeforcalculatingsemanticdiversity,asitrepresents
thedataset’sdiversityadequatelywhileensuringcom-
Selecting a proper size of documents. Com-
putationalefficiency.
putational constraints prevent calculating seman-
tic diversity for all documents in the dataset. Ex- dit 1), Wikipedia, and crawled web pages (Open-
periments on the unfiltered dataset help select an WebText (Gokaslan and Cohen, 2019)). We ex-
appropriatedocumentcountforcalculatingthese- tracted the same number of samples from one or
manticdiversitymetric. Foreachexperiment,we moreoftheabovedatasets,creatingamixedsubset
randomlyselectnsamples,calculatetheirsemantic of10,000samples. Wethenaveragedtherelation-
diversityscore,andrepeatthisprocess10timesto ship between semantic diversity and the number
computetheaveragescoreandstandarddeviation. of datasets (N). Figure 3 shows a positive corre-
ResultsaredisplayedinFigure4. Resultsindicate lationbetweensemanticdiversityandthenumber
that semantic diversity stabilizes when the group ofdatasets(N),indicatingthatsemanticdiversity
exceeds10,000samples,withastandarddeviation accuratelyreflectsdatadiversitywithindatasets.
of0.12. Wechoose10,000samplesforsubsequent Quality filtering with quality factor keeps the
experimentstobalanceaccuracyandefficiency. diversityoftheunfiltereddataset. Weassess
the semantic diversity of datasets resulting from
Theproposedmetriccanreflectdatasemantic
various quality filtering methods. The results are
diversity. Ourexperimentsshowedthatseman-
presentedinTable5. Mostqualityfiltersachieve
ticdiversityeffectivelyreflectsdatadiversityunder
higherdiversitythantheoriginalunfiltereddataset,
multi-datasets settings. We selected five datasets
likely due to the removal of a large number of
withdiversetopicsorwritingstyles,includingnews
machine-generated spams with similar semantic
articles(CC-News(Hamborgetal.,2017)),movie
reviews(IMDB(Maasetal.,2011)),forums(Red- 1https://www.reddit.com
7
 \ W L V U H Y L '  F L W Q D P H 6
 \ W L V U H Y L '  F L W Q D P H 6  Q R L W D L Y H '  G U D G Q D W 6Table4: Ablationsoneffectsofsizesofmeta-models. Tonote,theoriginalScalingFilterusesapairofmeta-models
with124Mand774Mparameters,respectively.
SmallModel LargeModel HellaSwag LAMBADA Winogrande PIQA ARC OpenbookQA BoolQ Avg
124M 335M 48.77 47.25 53.67 69.75 41.12 32.00 59.60 50.31
335M 774M 48.32 45.76 52.41 70.18 42.05 30.60 60.61 49.99
124M 774M 49.07 48.42 55.09 70.57 42.67 31.40 61.68 51.27
meanings. Theresultsindicatethatimportancere- studies(Wenzeketal.,2019;Soldainietal.,2023)
sampling achieves the highest diversity, at 56.25, keep all data to preserve diverse writing styles.
attributedtoitsresamplingstrategy. ScalingFilter Similarly,somefilterdatabasedonamodel’sper-
results in greater diversity compared to the most plexity,whichmightbiastowardseasilypredicted
commonlyusedbinaryclassification,thankstoits textsanddiscardchallengingbuthigh-qualitycon-
reference-free nature. Perplexity gating reduces tent(Marionetal.,2023). Ourapproachintroduces
thediversityoftheoriginaldataset,supportingthe aqualityfactorderivedfromtwolanguagemodels
conclusion from (Wenzek et al., 2019) that filter- toaddressthisissue.
ingdatabasedsolelyonperplexitythresholdscan ScalingLaws. Scalinglawsquantifyhowmodel
introduceunexpectedbiastodatadiversity. size, dataset size, compute budget, and perfor-
mance relate during neural network training. Ini-
QualityFilter Diversity
tial studies (Hestness et al., 2017; Kaplan et al.,
Random 52.50
0.12 2020) identified a power-law relationship among
BinaryClassification 53.99
0.19
thesefactors.(Hoffmannetal.,2022)introduceda
ImportanceResampling 56.25
0.21
unifiedformulaforscalinglawsthatincorporates
PerplexityGating 50.03
0.21
data-dependentscalingterms. Recentstudiesshow
ScalingFilter 54.73
0.14
variationsinscalinglawsacrossmultilingual(Con-
Table5: Qualityfiltermethodsandtheirimpactonse- neauetal.,2019)andmultimodal(Henighanetal.,
manticdiversity. Theresultsrepresentaveragesover10 2020; Cherti et al., 2023) settings. (Aghajanyan
attempts,withstandarddeviationsnotedassubscripts. etal.,2023)meticulouslyanalyzedthissubject,de-
rivingvariedscalinglawparametersforuni-modal
4 RelatedWork
andmixed-modalcontexts,highlightingthesignifi-
cantimpactofdatamodalityonscalingbehaviors.
Quality Filtering. Pretraining data for large
Thisdiscoverysuggestsahypothesisthatvarying
language models often includes low-quality con-
dataqualityinfluencesscalingbehaviors. Arecent
tent,suchasharmfulmachine-generatedspamor
study on large language model scaling laws (Bi
anomalousformats. Tofilterthisdata,researchers
et al., 2024) confirms data quality impacts both
typicallyscoredocumentsusinglinearclassifiersor
modelanddatascalingexponentsinscalinglaws.
languagemodels,thenfilterbasedonthesescores.
Thispaperdemonstratesthelinkbetweenscaling
High-quality data proxies like Wikipedia, books,
law parameters and data quality, facilitating the
andOpenWebTextarecommonlyused. Earlystud-
selection of high-quality samples based on their
ies,suchas(Brownetal.,2020;Chowdheryetal.,
scalingattributes.
2023),employedlinearbinaryclassifiers,compar-
ing curated datasets to unfiltered CommonCrawl
5 Conclusion
dataandusednoisythresholdingwithParetoran-
domness,which,whilepotentiallyenhancingdiver- We have presented ScalingFilter for data quality
sity,mightreducedataqualityassuggestedby(Xie filteringinareference-freemanner. Startingfrom
etal.,2023). Recentstudies,suchas(Touvronetal., thescalinglaw,wedemonstratethattheperplexity
2023),useWikipediaasthesolepositiveclassand differenceacrossagnatemodelsofdifferentsizes
applyhardthresholding,potentiallylimitingcorpus (i.e. meta-models)correlateswithdataqualitypos-
diversityandintroducingbiases. Anotherapproach itively. We select samples with higher perplexity
involveslanguagemodels. Forinstance,(Wenzek difference (i.e. quality factors) to form the pre-
etal.,2019)trainedann-grammodelonWikipedia training dataset. By eliminating the bias brought
andcategorizeddocumentsbyperplexityintohead, byreferencedatasets,ourmethodachievesbetter
middle, and tail, with low-perplexity documents downstreamperformanceoverseveralstrongbase-
retainedforpre-training(Computer,2023). Other lineswhilepreservingdatadiversity.
8Limitations AakankshaChowdhery,SharanNarang,JacobDevlin,
MaartenBosma,GauravMishra,AdamRoberts,Paul
TherearestillsomelimitationsofScalingFilterthat Barham,HyungWonChung,CharlesSutton,Sebas-
needtobeaddressed. First,itreliesonperplexity tianGehrmann,etal.2023. Palm: Scalinglanguage
modelingwithpathways. JournalofMachineLearn-
differencebetweentwoLLMs,whichmaymissnu-
ingResearch,24(240):1–113.
ancedaspectsoftextqualitylikefactualaccuracy
orbiaslikeracebias,socialclassbiasandgender Christopher Clark, Kenton Lee, Ming-Wei Chang,
bias,etc. Second,itrequiressignificantcomputa- Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019. Boolq: Exploring the surprising
tionalresourcestocomputeperplexitydifferences
difficultyofnaturalyes/noquestions. arXivpreprint
foralargedataset. Third,itsapplicabilitytoother
arXiv:1905.10044.
languages and data-limited domains is uncertain.
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,
Future research should address these limitations
AshishSabharwal,CarissaSchoenick,andOyvind
andfurtherexploretherelationshipbetweenseman-
Tafjord.2018. Thinkyouhavesolvedquestionan-
ticdiversityandmodelperformance, particularly swering? tryarc,theai2reasoningchallenge. arXiv
regardingfairnessandbias. preprintarXiv:1803.05457.
Together Computer. 2023. Redpajama v1: An open
sourcerecipetoreproducellamatrainingdataset.
References
AlexisConneau,KartikayKhandelwal,NamanGoyal,
Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Ning Hsu, Karen Hambardzumyan, Susan Zhang,
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
Stephen Roller, Naman Goyal, Omer Levy, and
moyer,andVeselinStoyanov.2019. Unsupervised
Luke Zettlemoyer. 2023. Scaling laws for genera-
cross-lingualrepresentationlearningatscale. arXiv
tivemixed-modallanguagemodels. arXivpreprint
preprintarXiv:1911.02116.
arXiv:2301.03728.
NolanDey,GurpreetGosal,HemantKhachane,William
XiaoBi,DeliChen,GuantingChen,ShanhuangChen,
Marshall,RibhuPathria,MarvinTom,JoelHestness,
DamaiDai,ChengqiDeng,HonghuiDing,KaiDong,
et al. 2023. Cerebras-gpt: Open compute-optimal
QiushiDu,ZheFu,etal.2024. Deepseekllm: Scal-
languagemodelstrainedonthecerebraswafer-scale
ingopen-sourcelanguagemodelswithlongtermism.
cluster. arXivpreprintarXiv:2304.03208.
arXivpreprintarXiv:2401.02954.
NanDu,YanpingHuang,AndrewMDai,SimonTong,
StellaBiderman,HaileySchoelkopf,QuentinGregory
Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun,
Anthony, Herbie Bradley, Kyle O’Brien, Eric Hal-
YanqiZhou,AdamsWeiYu,OrhanFirat,etal.2022.
lahan,MohammadAflahKhan,ShivanshuPurohit,
Glam: Efficient scaling of language models with
USVSN Sai Prashanth, Edward Raff, et al. 2023.
mixture-of-experts. InInternationalConferenceon
Pythia: A suite for analyzing large language mod-
MachineLearning,pages5547–5569.PMLR.
els across training and scaling. In International
ConferenceonMachineLearning,pages2397–2430.
DanFriedmanandAdjiBoussoDieng.2022. Thevendi
PMLR.
score: A diversity evaluation metric for machine
learning. arXivpreprintarXiv:2210.02410.
YonatanBisk,RowanZellers,JianfengGao,YejinChoi,
et al. 2020. Piqa: Reasoning about physical com- LeoGao,StellaBiderman,SidBlack,LaurenceGold-
monsenseinnaturallanguage. InProceedingsofthe ing,TravisHoppe,CharlesFoster,JasonPhang,Ho-
AAAIconferenceonartificialintelligence,volume34, raceHe, AnishThite, NoaNabeshima, etal.2020.
pages7432–7439. The pile: An 800gb dataset of diverse text for lan-
guagemodeling. arXivpreprintarXiv:2101.00027.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind LeoGao,JonathanTow,BaberAbbasi,StellaBiderman,
Neelakantan,PranavShyam,GirishSastry,Amanda SidBlack,AnthonyDiPofi,CharlesFoster,Laurence
Askell,etal.2020. Languagemodelsarefew-shot Golding,JeffreyHsu,AlainLeNoac’h,HaonanLi,
learners. Advancesinneuralinformationprocessing KyleMcDonell,NiklasMuennighoff,ChrisOciepa,
systems,33:1877–1901. Jason Phang, Laria Reynolds, Hailey Schoelkopf,
Aviya Skowron, Lintang Sutawika, Eric Tang, An-
Mehdi Cherti, Romain Beaumont, Ross Wightman, ishThite, BenWang, KevinWang, andAndyZou.
MitchellWortsman,GabrielIlharco,CadeGordon, 2023. A framework for few-shot language model
ChristophSchuhmann,LudwigSchmidt,andJenia evaluation.
Jitsev. 2023. Reproducible scaling laws for con-
trastive language-image learning. In Proceedings Aaron Gokaslan and Vanya Cohen. 2019. Open-
of the IEEE/CVF Conference on Computer Vision webtextcorpus. http://Skylion007.github.io/
andPatternRecognition,pages2818–2829. OpenWebTextCorpus.
9FelixHamborg,NormanMeuschke,CorinnaBreitinger, HouwenPeng,KanWu,YixuanWei,GuoshuaiZhao,
andBelaGipp.2017. news-please: Agenericnews Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang,
crawlerandextractor. InProceedingsofthe15thIn- BolinNi,JingchengHu,etal.2023. Fp8-lm: Train-
ternationalSymposiumofInformationScience,pages ing fp8 large language models. arXiv preprint
218–223. arXiv:2310.18313.
TomHenighan,JaredKaplan,MorKatz,MarkChen, AlecRadford,JeffreyWu,RewonChild,DavidLuan,
Christopher Hesse, Jacob Jackson, Heewoo Jun, DarioAmodei,IlyaSutskever,etal.2019. Language
TomBBrown,PrafullaDhariwal,ScottGray,etal. modelsareunsupervisedmultitasklearners. OpenAI
2020. Scaling laws for autoregressive generative blog,1(8):9.
modeling. arXivpreprintarXiv:2010.14701.
Jack W Rae, Sebastian Borgeaud, Trevor Cai, Katie
JoelHestness,SharanNarang,NewshaArdalani,Gre- Millican, Jordan Hoffmann, Francis Song, John
gory Diamos, Heewoo Jun, Hassan Kianinejad, Aslanides, Sarah Henderson, Roman Ring, Susan-
Md Mostofa Ali Patwary, Yang Yang, and Yanqi nah Young, et al. 2021. Scaling language models:
Zhou. 2017. Deep learning scaling is predictable, Methods,analysis&insightsfromtraininggopher.
empirically. arXivpreprintarXiv:1712.00409. arXivpreprintarXiv:2112.11446.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Men-
KeisukeSakaguchi,RonanLeBras,ChandraBhagavat-
sch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-
ula,andYejinChoi.2021. Winogrande: Anadver-
ford, Diego de Las Casas, Lisa Anne Hendricks,
sarialwinogradschemachallengeatscale. Commu-
Johannes Welbl, Aidan Clark, et al. 2022. Train-
nicationsoftheACM,64(9):99–106.
ingcompute-optimallargelanguagemodels. arXiv
preprintarXiv:2203.15556.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri,
PatrickLeGresley, JaredCasper, andBryanCatan-
JaredKaplan,SamMcCandlish,TomHenighan,TomB
zaro. 2019. Megatron-lm: Training multi-billion
Brown,BenjaminChess,RewonChild,ScottGray,
parameterlanguagemodelsusingmodelparallelism.
AlecRadford,JeffreyWu,andDarioAmodei.2020.
arXivpreprintarXiv:1909.08053.
Scaling laws for neural language models. arXiv
preprintarXiv:2001.08361.
Shaden Smith, Mostofa Patwary, Brandon Norick,
Patrick LeGresley, Samyam Rajbhandari, Jared
Andrew L. Maas, Raymond E. Daly, Peter T. Pham,
Casper, Zhun Liu, Shrimai Prabhumoye, George
DanHuang, AndrewY.Ng, andChristopherPotts.
Zerveas,VijayKorthikanti,etal.2022. Usingdeep-
2011. Learningwordvectorsforsentimentanalysis.
speed and megatron to train megatron-turing nlg
In Proceedings of the 49th Annual Meeting of the
530b,alarge-scalegenerativelanguagemodel. arXiv
AssociationforComputationalLinguistics: Human
preprintarXiv:2201.11990.
Language Technologies, pages 142–150, Portland,
Oregon, USA. Association for Computational Lin-
LucaSoldaini,RodneyKinney,AkshitaBhagia,Dustin
guistics.
Schwenk,DavidAtkinson,RussellAuthur,BenBo-
gin,KhyathiChandu,JenniferDumas,YanaiElazar,
Max Marion, Ahmet Üstün, Luiza Pozzobon, Alex
ValentinHofmann,AnanyaHarshJha,SachinKumar,
Wang, Marzieh Fadaee, and Sara Hooker. 2023.
Li Lucy, Xinxi Lyu, Ian Magnusson, Jacob Morri-
When less is more: Investigating data pruning
son,NiklasMuennighoff,AakankshaNaik,Crystal
for pretraining llms at scale. arXiv preprint
Nam, Matthew E. Peters, Abhilasha Ravichander,
arXiv:2309.04564.
KyleRichardson,ZejiangShen,EmmaStrubell,Nis-
TodorMihaylov,PeterClark,TusharKhot,andAshish hantSubramani,OyvindTafjord,EvanPeteWalsh,
Sabharwal.2018. Canasuitofarmorconductelec- Hannaneh Hajishirzi, Noah A. Smith, Luke Zettle-
tricity? anewdatasetforopenbookquestionanswer- moyer, Iz Beltagy, Dirk Groeneveld, Jesse Dodge,
ing. arXivpreprintarXiv:1809.02789. and Kyle Lo. 2023. Dolma: An Open Corpus of
ThreeTrillionTokensforLanguageModelPretrain-
DenisPaperno,GermánKruszewski,AngelikiLazari- ingResearch. arXivpreprint.
dou,QuanNgocPham,RaffaellaBernardi,Sandro
Pezzelle,MarcoBaroni,GemmaBoleda,andRaquel HugoTouvron,ThibautLavril,GautierIzacard,Xavier
Fernández.2016. Thelambadadataset: Wordpre- Martinet,Marie-AnneLachaux,TimothéeLacroix,
diction requiring a broad discourse context. arXiv Baptiste Rozière, Naman Goyal, Eric Hambro,
preprintarXiv:1606.06031. Faisal Azhar, et al. 2023. Llama: Open and effi-
cient foundation language models. arXiv preprint
GuilhermePenedo,QuentinMalartic,DanielHesslow, arXiv:2302.13971.
Ruxandra Cojocaru, Alessandro Cappelli, Hamza
Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
and Julien Launay. 2023. The refinedweb dataset Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
forfalconllm: outperformingcuratedcorporawith Kaiser,andIlliaPolosukhin.2017. Attentionisall
web data, and web data only. arXiv preprint youneed. Advancesinneuralinformationprocessing
arXiv:2306.01116. systems,30.
10GuillaumeWenzek,Marie-AnneLachaux,AlexisCon- A.2 DerivationofScalingFilter
neau, Vishrav Chaudhary, Francisco Guzmán, Ar-
A.2.1 Positivecorrelationbetweenaandthe
mandJoulin,andEdouardGrave.2019. Ccnet: Ex-
tractinghighqualitymonolingualdatasetsfromweb negativetangentslope
crawldata. arXivpreprintarXiv:1911.00359.
Let’sstartwiththeparametriclossfunctionintro-
ducedbyChinchilla(Hoffmannetal.,2022)scal-
AlexanderWettig,AatmikGupta,SaumyaMalik,and
inglaw.
DanqiChen.2024. Qurating: Selectinghigh-quality
data for training language models. arXiv preprint
A B
arXiv:2402.09739. Lˆ(N,D) = E + + (8)
Nα Dβ
Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas
where E represents the minimal achievable loss,
Muennighoff. 2023. C-pack: Packaged resources
correspondingtotheentropyofnaturaltext. The
to advance general chinese embedding. Preprint,
arXiv:2309.07597. scaling law, indicating the optimized numbers of
N andD,followsapower-lawform:
SangMichaelXie,ShibaniSanturkar,TengyuMa,and
PercyLiang.2023. Dataselectionforlanguagemod-
elsviaimportanceresampling. AdvancesinNeural N ∝ Ca D ∝ Cb
opt opt
InformationProcessingSystems(NeurIPS).
β α (9)
where a = , b =
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali α+β α+β
Farhadi, and Yejin Choi. 2019. Hellaswag: Can a
machinereallyfinishyoursentence? arXivpreprint whereaandbrepresentthemodelanddatascaling
arXiv:1905.07830. exponents,respectively. Inordertopresentαand
β withscalingexponents,wehave
A Appendix
α 1
= −1 (10)
A.1 Hyperparametersoftraininglanguage β a
models .
Let η = α + β, the parametric loss Lˆ can be
Wetraindecoder-onlytransformer(Vaswanietal., presentedas:
2017)modelsusingMegatron-DeepSpeed(Smith
et al., 2022; Shoeybi et al., 2019). The hyperpa- Lˆ(N,D) = E + A + B (11)
rameters used in the training process is listed in N(1−a)η Daη
TableA.1. Then,wecanobtainthepartialderivativesofLˆ
withrespecttoNexpressedintermsofaandb.
nparams 1.3B
nlayers 24
∂Lˆ
dmodel 2048 = A·(a−1)η·N(a−1)η−1 (12)
∂N
nheads 32
dhead 64 It’sobviousthat
Sequencelength 2048
∂Lˆ
Globalbatchsize 256 < 0 (13)
∂N
LRschedule cosine
Learningrate 2.5×10−4 whichmeansthattheexpectedlossdecreaseswhen
MinLR 2.5×10−5 modelsizeincreasesundersametrainingtokens.
Weightdecay 0.1 Wecanfurtherget
Optimizer Adam
Adamβ 0.9
1 ∂2L
Adamβ 2 0.95 = A·η·N(a−1)η−1
∂a∂N
Adamϵ 1.0×10−8
+A·(a−1)η·η·lnN ·N(a−1)η−1
Tokenizer cl100k_base
= A·η·N(a−1)η−1
TableA.1: Hyperparametersoftraininglanguagemod- ·[1+(a−1)η·lnN]
els. (14)
BecauseA,N,α,β,η > 0,wehave
11A.3 Samplingvs. top-kselection
A·η·N(a−1)η−1 > 0 (15) Previous works (Brown et al., 2020; Gao et al.,
2020;Xieetal.,2023;Wettigetal.,2024)typically
andsince1 > a > 0,η > 0,N ≫ 1,wehave
usesamplingwithoutreplacement,selectingdata
based on a rating score to balance quality and di-
1+(a−1)η·lnN < 0 (16)
versity. This approach often results in improved
downstream performance. We conducted exper-
Thus,wehave
iments to determine whether this sampling strat-
∂2L egy could enhance the downstream performance
< 0 (17)
of ScalingFilter. Following (Wettig et al., 2024),
∂a∂N
weintroducedatemperaturetermτ toadjustsam-
That means that at a specific N , the slope of
0 plediversity. Here,τ → 0meanstop-kselection,
thetangenttotheLˆ −N curve(∂Lˆ )decreasesas
∂N whileτ → ∞indicatesuniformsampling. Results
a increases (i.e., the larger the a, the steeper the
inTableA.3indicatethattop-kselectionistheop-
tangent). Inall,we’veproventhat
timaldataselectionmethodforScalingFilterdueto
itsreference-freenatureandtheunnecessaryuseof
∂(−Lˆ) ∂2(−Lˆ)
> 0, > 0 (18) noisysamplingstrategiestoenhancedatadiversity.
∂N ∂a∂N
A.4 AblationStudyonHyperparameters
Owing to this monotonic relationship, we can
inferthevalueofafromtheslopeofthetangent. TofurthervalidatetherobustnessofScalingFilter,
For a given N , a steeper tangent (with a greater weconductedablationexperimentsusingvarious
0
absolutevalueoftheslope)indicatesalargera: traininghyperparameterson1.3Bmodels. Ourfo-
cuswasprimarilyontwohyperparameters: learn-
(cid:12)
∂L(cid:12) ingrate(default2.5×10−4)andglobalbatchsize
a ∝ − (cid:12) (19)
∂N(cid:12) (default 256). We doubled the default values for
N=N0
eachintheablationstudy. Theresultsarepresented
A.2.2 Generalizingfromtangentslopeto inTableA.2. Theresultsindicatethatanincreasein
secantslope globalbatchsizesignificantlyreducesperformance
inbothsettingswithdifferentqualityfilters,asit
It’s impossible to calculate the slope of the tan-
gent ∂L intherealscenario,wecanonlyacquire halves the training steps. Conversely, increasing
∂N
thelearningrateslightlyaffectsdownstreamaccu-
theslopeofthesecantlinebyassessingthecross-
racy. Overall,ScalingFilterremainsrobustacross
entropylossontwomodelswithdifferentsizes(i.e.
a rangeof traininghyperparameters, consistently
a pair of meta-models). Next, we will prove that
surpassingbinaryclassification,itstopcompetitor,
theslopeofthetangenthasapositiverelationship
asshowninTable1.
withtheslopeofthesecantline. Therefore,wecan
builddirectrelationshipbetweentheslopeofthe
secantlineanda.
Given a pair of meta-models with N and N
p q
parameters where N < N , we can denote the
p q
slopeofthesecantlineas:
∆Lˆ
=
Lˆ(N q)−Lˆ(N p)
=
(cid:82) NN pq ∂∂ NLdN
(20)
∆N N −N N −N
q p q p
For a larger a,
∂L(cid:12)
(cid:12) < 0 is smaller for
∂N N=N0
every N . This lead to a smaller
∆Lˆ
, or a larger
0 ∆N
−∆Lˆ
,thatis
∆N
∆Lˆ
a ∝ − (21)
∆N
12TableA.2: Ablationsonhyperparametersusedintraining1.3Blanguagemodels. Numbersingrayrepresentthe
defaultvalues,asshowninTableA.1.
Abbreviations: BS.=BatchSize,Hella. =HellaSwag,Winog. =Winogrande.
LearningRate GlobalBS. Hella. LAMBADA Winog. PIQA ARC OpenbookQA BoolQ Avg
BinaryClassification
2.5×10−4 256 48.13 48.96 54.30 69.75 41.66 30.40 61.38 50.65
2.5×10−4 512 46.63 47.72 53.51 68.34 40.81 31.20 59.85 49.72
5.0×10−4 256 49.22 48.71 54.62 70.40 42.45 33.20 54.83 50.49
ScalingFilter
2.5×10−4 256 49.07 48.42 55.09 70.57 42.67 31.40 61.68 51.27
2.5×10−4 512 46.51 46.24 52.33 69.37 44.84 30.00 61.56 50.12
5.0×10−4 256 49.29 48.15 57.06 70.24 42.95 32.80 60.73 51.60
Table A.3: Ablations on sampling vs. top-k selection. Note that the top-k results are identical to the original
ScalingFilter results reported in Table 1. We compare top-k data selection to sampling without replacement
following(Xieetal.,2023;Wettigetal.,2024).
SamplingMethod Hellaswag LAMBADA Winogrande PIQA ARC OpenbookQA BoolQ Avg
τ =0 (Top-k) 49.07 48.42 55.09 70.57 42.67 31.40 61.68 51.27
τ =1 47.99 45.93 53.91 68.50 41.46 31.80 61.13 50.10
τ =2 46.93 48.34 54.06 69.75 41.01 32.60 60.28 50.42
τ =3 47.14 48.42 54.46 70.02 42.28 32.00 59.82 50.59
13