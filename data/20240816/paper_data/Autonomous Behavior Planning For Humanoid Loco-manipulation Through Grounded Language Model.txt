Autonomous Behavior Planning For Humanoid Loco-manipulation
Through Grounded Language Model
Jin Wang12∗, Arturo Laurenzi1, Nikos Tsagarakis1
Abstract—Enabling humanoid robots to perform au-
tonomously loco-manipulation in unstructured environments
is crucial and highly challenging for achieving embodied
intelligence. This involves robots being able to plan their
actions and behaviors in long-horizon tasks while using multi-
modality to perceive deviations between task execution and
high-level planning. Recently, large language models (LLMs)
have demonstrated powerful planning and reasoning capabili-
tiesforcomprehensionandprocessingofsemanticinformation
throughrobotcontroltasks,aswellastheusabilityofanalytical
judgmentanddecision-makingformulti-modalinputs.Tolever-
age the power of LLMs towards humanoid loco-manipulation,
we propose a novel language-model based framework that
enables robots to autonomously plan behaviors and low-level
executionundergiventextualinstructions,whileobservingand
correcting failures that may occur during task execution. To
systematically evaluate this framework in grounding LLMs,
we created the robot ‘action’ and ‘sensing’ behavior library
for task planning, and conducted mobile manipulation tasks
and experiments in both simulated and real environments
usingtheCENTAUROrobot,andverifiedtheeffectivenessand
application of this approach in robotic tasks with autonomous
behavioral planning.
Fig. 1: Humanoid robot CENTAURO picks objects with
the planning of the task graphs generated by the LLM.
I. INTRODUCTION
The ‘behavior lib’ consists of various action and sensing
Maintaining autonomy during the execution of a task in behaviors with ‘tags’ describing the semantic content of
a real-world environment is both essential and challenging different behaviors.
for robots, especially when performing tasks that require
interaction with surroundings and manipulation of objects.
Thisdemandsahighlevelofcapabilityfromrobotsthathave
However, due to the complexity of whole-body control
to perceive and make decisions during the task execution
and perceptual decision-making in humanoid robots, it is
and the ability to achieve autonomous planning based on
challenging to directly apply LLMs for action generation
these decisions. Furthermore, one of the main challenges
and planning, especially when it involves understanding the
lies in enabling robots to understand semantic instructions
current task based on environmental cues and interacting
from humans and apply them within the context of differ-
with objects. Previous work has shown that incorporating
ent scenarios and their current state. This process involves
language models into robotic tasks [1] and enabling in-
encoding textual information into a hierarchical sequence of
telligence to better reason and evaluate textual informa-
robot behaviors, as well as mapping high-level tasks to low-
tion can interact with the environment to accomplish long-
level robot control and generating reference trajectories that
horizon tasks that require complex planning of robot action
the robot can execute.
sequences. Leveraging this feature, we design a language
Integrating large language models (LLMs) has emerged model-based planner, which requires the pre-creation of a
as a promising avenue for enhancing the autonomy of behavior libfortherobot,includingmultipleactionsandper-
robots. These models have demonstrated great flexibility in ceptionsfordifferentmodalities.Thiscanbeusedtogenerate
understanding and processing semantic information, along direct reference trajectories for execution. After acquiring
withremarkablereasoninganddecision-makingcapabilities. the human instruction and the semantic indices from the
behavior lib, the LLM generates hierarchical task graphs,
†This work was supported by the European Union’s Horizon 2020
whichguidetherobottofollowthelogicalsequenceoftasks
research and innovation programme, euROBIN EPUE034001, and JL
LeonardoETCM058502. and make decisions according to different scenarios.
1Humanoids and Human-Centered Mechatronics (HHCM), Istituto Ital- Finally,theincreasingautonomyofrobotsduringtheexe-
ianodiTecnologia,ViaMorego30,Genoa,16163,Italy.
cutionofcomplextasks,aswellasunexpectedperturbations
2DIBRIS,UniversitadiGenova,Italy,16145.
∗Correspondingauthor:wang.jin@iit.it in long-term missions, make failures during the execution of
4202
guA
51
]OR.sc[
1v28280.8042:viXrahigh-level behavioral sequences in real-world environments simulation and real-world environments.
inevitable. Therefore, failure detection as well as correction
procedures, are needed to ensure alignment between low-
II. RELATEDWORKS
level execution and high-level semantic planning during Endowing robots with autonomy in locomotion and ma-
the programming of complex loco-manipulation tasks that nipulation tasks still represents a high-level challenge for
are composed of multiple subtasks. We attempt to fuse robotics. In recent years, prior research has focused on
multimodalsensordataandaddthemasperceptualbehaviors motion planning and trajectory optimization [3], covering
intotherobotbehavior lib,whichenablestheLLMtoselect systems with various morphologies and levels of autonomy.
the optimal combination of behaviors according to the task Thedata-drivenapproach[4]enablestheuseofexperienceto
scenario. Among them, the visual language model (VLM) make decisions online and generate appropriate multimodal
is used as one of the key metrics for determining whether reference trajectories for dexterous manipulation. Mean-
the task is completed or not as well as for failure determi- while,combiningdataandlearning,themodel-freereinforce-
nation, due to its accurate and effective image information ment learning approach [5][6] has demonstrated impressive
comprehension and reasoning capabilities. Once the VLM performance in several specific tasks and unstructured en-
perceptualbehaviordetectsafailureduringatask,ittriggers vironments. For long-term tasks, [7][8] introduced a novel
the behavioral planner to perform the corrective action. motion planning framework and task evaluation approach
This process is pre-planned by the LLM and stored in the that allowed robots to maintain dexterity while navigating
task graphs, and by combining the perceptual behavior of complex environments. Boston Dynamics [9] utilized an
different modalities, a closed loop of high-level feedback is offline trajectory optimization and model predictive control
realized, which improves the robustness of the autonomous strategy to codify the Atlas robot’s movements into a time
robotic system and increases the task success rate. series, achieving consistency between simulation planning
In this paper, we present a language model-based and motion execution. However, when faced with different
framework enabling the autonomous execution of loco- task scenarios and long-horizon planning involving multiple
manipulation tasks. This framework leverages the semantic subtasks, the ability to understand instructions and reason
understanding capabilities of LLM in reaction to human about tasks is often overlooked, making it challenging to
instructions, which is based on the knowledge of the task achieve autonomy and adaptability in task-driven mobile
scenarioandthebehavioralskills(“action”and“perception”) manipulation.
possessedbytherobot.AsshowninFigure1,whenahuman With the emergence of large language models (LLMs),
provides the instruction “find the bottle and pick it up”, severaltransformer-basedarchitecturalplanners[10][11][12]
the LLM will generate a task graphs consisting of different haveplayed apivotalrole inpredictingand generatingrobot
behavioral nodes based on the instruction and the behavior actions and attempting to derive robot policy code [13]
libasprompts,whichwillguidetheCENTAUROrobot[2]to guided by natural language instructions. However, such end-
performtheactionssuchasobjectdetection,objectgrasping, to-endstrategiesoftenrequiresubstantialamountsoftraining
and lifting, etc. While the VLM-based perceptual behavior data and expert demonstrations, complicating the model’s
node is used as a failure detector, it’s triggered when the training process, especially in unknown scenarios. In con-
stateofthecurrenttaskisrequiredtobedetectedandmakes trast,someapproachesleverageLLMs’semanticcomprehen-
an inferential judgment based on the images returned from sion capabilities as a higher-level task planner [1][14][15],
the robot onboard camera. transforming instructions into executable lower-level actions
Our summary of the main contributions of this work in a zero- or few-shot manner [16]. Nonetheless, these
includes: methods tend to assume the success of the task performed
and overlook the potential discrepancy between planned ex-
• We exploit the appropriateness of LLM in loco- pectationsandreal-worldexecution.Theyoftenfailtoenable
manipulation tasks and propose a novel framework for
whole-body control in multi-joint, high degree-of-freedom
autonomous behavior planning, enabling rapid deploy-
(DoF) mobile manipulation tasks, such as those involving
ment without additional training, which can be applied
humanoid robots. Moreover, studies such as [16][17][18]
to quadrupeds, humanoids, and mobile manipulators.
attempt to interpret textual and visual inputs simultaneously,
• We propose a new paradigm for robot behavior library, usingthemtoaddressdownstreamrobotictasks.Whilevisual
which enables to encoding of human instruction into
question answering (VQA) [19] can be effectively achieved
optimal action sequences by combining behaviors in
through understanding image descriptions and inferring the
a modular way and linking high-level tasks to inter-
robot’sstateandcurrentcontexttoguidesubsequentactions,
pretable low-level control.
relyingsolelyonhigh-levelvisualfeedbackhasbeenproved
• We incorporate multimodal failure detection as higher- to be insufficient. It compromises the rapid response to dy-
order feedback to facilitate the task graph in correcting
namic environments and is less effective than other methods
the misalignment between the intended goal and the
exploringproprioceptiveperception,suchasinteractionforce
actual robot’s actions during the execution of the task.
sensing when executing low-level tasks.
The proposed framework was experimentally validated Our study, on the other hand, applies the LLM to robots
to assess its feasibility in handling long-horizon tasks in by utilizing its ability to understand instructions and planwithexistingbehaviorlibrariesbygeneratingtaskgraphsthat Algorithm 1 Language model based robot behavior planner
sequentially instruct the high DoF humanoid robot’s whole- Given: Language model L, a human instruction i,
body actions. It detects and recovers from possible failures and a behavior lib Π with the language description
during tasks by using the VLM as a perceptual behavioral l π.
node and combining it with other perceptrons to enable 1: T i = L(i,l π)
multimodal feedback. This approach allows the robot to 2: B ←BehaviorTree(T i)
serializediscreteactionnodesandselectdifferentperceptual 3: Initialize the state s π, number of steps n
combinations according to the task scenario. Ultimately, a 4: while B(s πn)̸=”done” do
robust autonomous mobile manipulation skill is realized and 5: for π n ∈Π B do
demonstrated. 6: executeBehaviorπ n
III. AUTONOMOUSROBOTBEHAVIORPLANNING 7: s πn ←updateStatus
8: end for
A. Problem Statement
9: if allBehavior Completed then
The problem of performing autonomously loco- 10: return Done
manipulation task based on higher-level instructions 11: end if
can be described as follows. We assume that the human’s 12: end while
semantic description of the task, denoted as i articulates
a specific task to be executed by the robot. Additionally,
we consider that a behavior lib Π is provided to the robot, behavior, as well as a detailed semantic description of
consisting of a set of action and perception behaviors π ∈Π the behavior. In addition, prompts are defined in advance.
that can be directly executed by the robot, such as the Prompts include conditional information for input to the
grasp action would enable the robot to control the gripper large language model, a description of the robot’s features,
for grasping in Cartesian space, while the object detection the skills that the robot possesses (behavior lib), and a
behavior would utilize the camera to detect the pose of the description of the expected outputs. When a robot is called
target object. Each behavior is associated with a specific upontoperformatask,thehumanfirstgivestaskinstructions
semantic description l π (e.g., “open the griper and move as input, which are fed to the large language model together
to the target pose then close griper”). By invoking the with the prompts, along with the behavior tags from the
large language model L, the task graph T corresponding to behavior lib. Upon obtaining this information, the LLM
instruction i is generated: generatesasequencecomprisingtherobot’sbehaviorsbased
on the given task and stores it in an XML-formatted file,
T =L(i,l ,l ,...,l ) (1)
i π1 π2 πn which is used to generate a Behavior Tree (BT) [20] that
The task graph T , encapsulates a sequential arrangement controls the robot’s task execution. The behavior code in
i
of behaviors necessary for the robot to execute in various behavior libandtheBTgeneratedbytheLLMtogetherform
states s to accomplish the designated task requested by the a task graph, which instructs the robot to perform different
human. This graph is maintained in an XML file format behaviorsaccordingtothecurrentrobotstateandconditions,
and is operationalized through the Behavior Tree (BT) B. andultimatelycompletesthetask.Thetaskgraphalsoplansa
The robot state is the feedback by the execution results of correctionpolicyforfailuresduringthetask.Iftheperceptual
differentbehavioralnodes,including(running,success,fail). behaviordetectsaninconsistencyintheexecutionofthetask,
Depending on the current state of the robot, the BT guides the task graph will execute specific behaviors to try to fix
the robot to execute different behaviors. When a low-level the failure.
execution deviation from the task plan is detected, the BT While in the behavior execution phase, the CENTAURO
fixes this failure by resuming the behavior in an attempt to depends on Xbot[21] and Cartesian I/O[22] to execute the
correct the error. action commands issued by the task graph, the current state
The above process is described in Algorithm 1. In this of the robot, as well as the sensory information, are fed
way, the robot is enabled to encode human instructions into back to the task graph to guide the next action. During this
various sequences of behaviors and execute them according process, the host PC is responsible for behavioral planning,
tothetaskdemands,aswellastorecoverpossiblemisalign- including receiving human instructions and behavior tags,
ment between the instructions and the robot’s execution. and generating the task graph through the large language
model. Meanwhile, the robot pilot is tasked with receiving
B. Overview of the framework
and executing the low-level action commands.
Weproposearobotbehaviorplanningsystemforperform-
C. LanguageModelforBehaviorPlanningandModification
ingautonomouslyloco-manipulationtasksthatmakesuseof
languagemodels.Figure2showsanoverviewofthesystem. 1) Behavior Lib: Controlling a robot to accomplish com-
We first create a library of behaviors for the CENTAURO plex actions in a long-horizon task is difficult and challeng-
robot,dividedintoactionbehaviorsandperceptualbehaviors, ing. To link semantic behaviors with the actual execution of
eachcontainingacorrespondingbehaviortagandabehavior actions by the robot, we designed a behavior library for the
code. The behavior tag records the name and type of the CENTAURO robot such that each skill can directly controlFig. 2: Overview of the Framework. (a) Behavior Planner takes the human instruction as input, given the behavior lib and
prompts, LLM generates a hierarchical structure behavior tree, which forms the task graph along with the behavior code.
(b) The CENTAURO robot executes the lower action command and feeds back its current state. The entire process does not
require any additional training.
TABLE I: Behavior Lib Definition
the robot to complete basic actions. This approach improves
the interpretability of each step of the task process and
Behavior Type Tag
reducesthedeviationbetweenhigh-leveltasksandlow-level ‘bringingallofthejointsofrobot
Homing action
execution by partitioning the complex task into a sequence tohomingconfiguration’
of actions consisting of several behaviors skills. Approach action ‘movingrobottorsocloser
totargetbycertaindistance’
Weclassifythebehavior libintoactionbehaviorsandper- ‘movinggripperto
Grasp action
ceptualbehaviors.ActionbehaviorscontroltheCENTAURO agivenposeandcloseit’
‘raisinggripperto
robot to complete whole-body motions such as moving and Lift action
thechestandadjustingpose’
manipulating.Inspiredbyobjectmanipulationindailyoffice ‘movinggripperto
Place action
environments, we have designed several individual robot thegivenpositionandopenit’
‘measuringdistance
actions to compose the action lib using Cartesian I/O[22], Distance perception
betweenobjectandrobot’
includingApproach,Grasp,Lift,etc.,whichareusedto ‘obtainingtheactual
Grip force perception
implement subtasks such as navigating to various locations, torqueofgripper’
‘detectingandestimating
grasping a target object, and lifting a target object, etc. The Object detection perception
6-DoFofobjects’
action lib is a library of robot actions that can be added and ‘reasoningtaskstate
Visual Q&A perception
combineddependingonthedemandsofaloco-manipulation usingvisuallanguagemodel’
task and the interaction environment. Meanwhile, percep-
tual behaviors rely on the robot’s internal sensors ( torque
sensing, RGBD camera) to detect the position of the object,
haviortags,whichconsistofseveralcomponents.First,there
evaluate the robot’s state, and reason whether the current
is information about the current state of the CENTAURO
task is complete or if there are failures. Various algorithms
robot and its hardware configuration. Then, the concept of a
[23][24] are integrated into different perceptual behaviors,
behaviorlibrary,itscomponents,andsampleapplicationsare
and similar to action behaviors, perceptual behaviors can be
introduced. Lastly, there is the expected output in a format
designedindependentlyandaddedtotheperceptuallib.Mul-
thatincludes theconceptofa behaviortree,the definitionof
tiple sensors can be invoked in a single perceptual behavior
the nodes, and examples of applications.
and fused with data from different modalities according to
To convert high-level instructions into a sequence of
the requirements.
implementable low-level skills, we leverage Behavior Trees
2) LLM Generated Task Planner: While large language as both an intermediate bridge and an output of the LLM.
models can utilize their extensive knowledge of semantic The use of Behavior trees provides a hierarchical, tree-
data as well as their text comprehension reasoning capabil- structured framework for controlling the robot’s actions and
ities to provide answers to human instructions, the answers decision-making processes [25]. This framework consists of
can be diverse. To obtain the desired output, it is necessary nodes with different functions, including those controlling
to impose constraints on the instructions given as input. the execution process and conditional judgments, as well
One approach is to use prompt words, a linguistic construct as nodes that actually execute the robot’s actions. Having
designed to qualify a language model to give a specific previously defined the behavior lib, the LLM generates the
output. In our framework, prompts are used as input to the behavior tree framework based on the behavioral skills and
largelanguagemodelalongwithhumaninstructionsandbe- task instructions. This framework is stored in an XML file.Fig. 4: Failure detection using a combination of perception
behaviors. By asking VLM, the visual Q&A behavior can
reasonthestateofthetask,whileusingthetorquesensor,the
Grip forcebehaviorwillreturnthetorqueonthegripper.
Fig. 3: Behavior Planner Grounding LLM
the behavior tree will guide the execution of the behavior
The task graph is responsible for loading the behavior tree
according to the returned signals. After the action node is
and invoking behaviors from the behavior lib according to
executed,theconditionnodecanbeaddedtodecidewhether
the node guidance. This setup realizes decision reasoning
the current task is successful or returns the current state of
through the LLM. The task graph is used for behavior
the target object. For instance, in the process of grasping
planning, and the robot ultimately executes the tasks.
and lifting an object, after the completion of grasping, a
We access the gpt-4 model as the LLM through the
condition node IsObjectHeld can be added to decide
OpenAI API, which directly outputs the XML file used for
whether the object has been successfully grasped or not. In
generating the behavior tree, as shown in Figure 3.
this scenario, the node will activate the Grip force and
3) FailureDetectionandRecovery: Inordertodetermine
Visual Q&A perception behavior, which will obtain the
whether a task is successfully completed or deviates during
torqueofthegripperandasktheVLM”Isthe(TargetObject)
execution, we try to incorporate a failure detection and
held by the gripper”. Only if there is a torque on the gripper
recovery mechanism into the task graph. In our work, to
and the VLM answers ”Yes”, then the node will return a
take advantage of the visual language model’s capability of
success signal. The behavior tree will continue to execute
understanding and reasoning about images, we utilize visual
the subsequent nodes. If it returns a failure, the recovery
questions and answers (VQA) as perceptual behaviors to
node is activated and the robot will try to grasp the object
determine the current state of the robot performing the task,
again.
suchasinthetaskof‘pickingthebox’bygivingtherobot’s
cameraimageandasking“Istheboxbeingheld?”,theVLM IV. EXPERIMENTANDEVALUATION
will respond to the query by answering “Yes ” or “No”. We experimentally verify the capability of LLM as a
Proprioceptivesensingliketorqueanddistancehasalsobeen behaviorplannerbyimplementingitandassessingitsperfor-
developedasbehaviorstodetectthepotentialfailuresinspe- mance on CENTAURO robot executing long-horizon tasks
cifictasks,likeduringthetasksrequiringgrasping,detecting under semantic commands. Few studies have employed
the torque on the gripper can be a reference of whether the LLM to plan the behavior of humanoid type of robots like
object is being held. The perceptual behaviors we define in CENTAURO and conducted real-world experiments. It is
the behavior lib give multiple alternatives and combinations challenging to use different robots as a control group due
for the failure detection nodes, allowing the LLM to design to the variations in their functionalities and configurations.
the behavior tree based on the reasoning of different tasks. Therefore, we compare this method with our previous study
Somesimpletaskssuchas”findandapproachtoobject”only in terms of functional aspects as shown in Table 2, and
require the initiation of Object detection behavior to conduct preliminary experiments on applying LLM on the
determine if the object is available, while tasks that require CENTAURO robot.
multiple robotic actions often demand a combination of
different perceptual behaviors for failure detection. A. Experiment Setup
During the execution of the behavior tree, the node will WeconductedtheexperimentusingobjectsfromtheYCB
return three signals: success, failure, and running, and dataset [26] that are commonly found in an office kitchen.TABLE III: Behavior planning results for different tasks
including tasks with failure detection and recovery (FR).
Task Executable Success Time(s)
Findobject 100% 94% 14.93
Approachobject 98% 90% 16.15
Graspobject 96% 92% 16.27
Pickobject 96% 84% 17.11
Pickobject(FR) 90% 82% 17.91
Pickandplaceobject 92% 84% 18.23
Pickandplaceobject(FR) 84% 80% 19.07
Findandpickobject(FR) 86% 82% 17.86
Fig. 5: Experiment setup
TABLE II: Comparison of different methods
Method
Abilities Whole-bodyMPC LLMBehaviorPlanner
[3] (ours)
Autonomy low high
Whole-bodymotion ✔ ✔
Long-horizontask ✗ ✔
Failuredetection ✗ ✔
Real-timereplanning ✔ ✗
The test environment was an open area inside the lab, with Fig. 6: Task execution with failure detection and recovery in
objects randomly placed on a desk as shown in Figure 5. simulation. Images 1, 2, and 3 show the robot’s first attempt
TheCENTAUROrobot,ahybridwheelsandlegsquadruped topickupanobject.Aftertheperceptionbehaviorsdetected
robot with a humanoid upper body, features 37 degrees of thatthegripperdidnotsuccessfullygrasptheobjectinimage
freedom and a two-fingered claw gripper, enabling it to per- 3, then the robot tried again and successfully picked the
formawiderangeofloco-manipulationtasks.Equippedwith object as shown in image 4, 5, 6.
anRGBDcameraonitsheadandtorquesensorsinthejoints
throughoutitsbody,therobotpossessesextensiveperceptual
capabilities to measure joint efforts and interaction forces. tions only for the given objects in Figure. 5, and the
The action behaviors in the behavior lib were designed behaviors created in the behavior lib. These instructions are
basedonCartesianI/O,requiringnoextratraining.TheXbot simple descriptions of the task content, e.g., “Find the soup
functionsasmiddleware,providingreal-timecommunication can and pick it up.” If failure detection and recovery are
betweentherobot’svariousunderlyingactuatorsandthetask required during the task, this will need to be stated in the
commands through an API interface. For message transmis- instructions such as “Pick the cracker, place it aside. Detect
sion between the LLM, behavior lib, Behavior Tree, and the and recover the failure during the task.”. We then used the
robot, we utilized the Robot Operating System (ROS) and Behavior Tree to load the XML files generated by the LLM
conducted simulation experiments in the Gazebo simulator. and verified their feasibility. Finally, the appropriateness of
Experiments for both simulation and the real world are thebehavioralplanningandthesuccessfulcompletionofthe
demonstrated in the accompanying video. taskweremanuallyverified.Experimentsthatwereexecuted
and followed the requirements of the task instructions for
B. Autonomous Humanoid Loco-manipulation Task planningwerejudgedassuccessful.Eachtaskwasplanneda
1) Behavior Planning with LLM: We first tested the totalof50times,allusingthesamebehaviorlibandprompt.
LLM’s behavior planning capabilities for robot tasks of The time for each task graph generation was recorded, as
varyingcomplexity.Theexperimentwasconductedforeight well as the executable and success rate of the behavioral
different tasks, including tasks with failure detection and planning, as shown in Table 3.
recovery (FR), and the behaviors were planned using the 2) Long-horizon Task Execution: After verifying that the
method shown in Figure. 3. We provided standard instruc- behavioral plans generated by the LLM can be convertedFig. 7: Long-horizon task planning with CENTAURO robot. The left shows the task graph generated using LLM, the right
shows the robot executing human instruction according to the behavioral plan.
TABLE IV: Experiment results of simulation and real-world
intoanexecutableBehaviorTree,weconductedexperiments
environment
using the CENTAURO robot in both simulation and real-
worldenvironments.WeselectedthelastsixtasksfromTable
Simulation RealWorld
3totesttheactualperformanceoftherobotexecutingLLM- Task
Success Time(s) Success Time(s)
plannedbehaviors.Startingwithobjectgraspingandpicking
Graspobject 92% 85.7 96% 98.4
tasks, the initial positions of the robot were set at different
Pickobject 84% 104.9 80% 121.3
distances from the target objects, which were randomly
placed on the table. In the last task, the CENTAURO was Pickobject(FR) 88% 116.2 88% 167.1
placed relatively far to test the performance of the planner Pickandplaceobject 76% 132.7 72% 160.6
in tasks where “approach” behavior was necessary. For the Pickandplaceobject(FR) 84% 189.2 80% 203.2
same task, different descriptions of instruction and different Findandpickobject(FR) 80% 174.5 76% 197.8
targetobjectswereusedtoverifytheLLM’sabilitytoreason
about the simple task and plan the robot’s behavior. For
tasks that require in-process failure detection and recovery,
task process increases the difficulty of behavior planning,
theLLMincorporatesperceptualbehaviorsinthebehavioral
affecting the success rate of the generated task graphs. By
planning phase and attempts to recover if it detects that the
adding FR, it increased the time for task execution with the
planned action fails to complete the task. This is shown
robot as shown in Table 3, but not significantly increase the
in Figure. 6, where the recovery behaviors are selected
timeforLLMplanning.Finally,thebehavioralplanningtime
based on the task requirements and the current state of
depends on the feedback speed of the language model used
the robot. Finally, the experiments focus on verifying the
and the hardware device response time. The task complexity
LLM’s behavioral planning for long-horizon tasks and the
primarily affects the time taken to load the Behavior Tree,
robot’s ability to perform autonomous loco-manipulation,
leadingtominordifferencesinplanningtimeacrossdifferent
withmultipleperceptualandactionbehaviorsbeingselected
tasks.
and combined to achieve the task goal, as shown in Figure.
The results from robot task execution in both simulation
7. We conducted 25 experiments for each task separately
and real environments demonstrate that LLM can effectively
and recorded the success rate and execution time of the
plan humanoid robot loco-manipulation tasks to a consider-
robot to complete the task in both simulation and the real
able degree. By integrating perception and action behavior
environment, as shown in Table 4.
in the behavior lib through LLM, the CENTAURO robot
reaches a satisfactory level of success rate (≥ 72%) in
C. Results analysis
task execution. In long-horizon tasks, the incorporation of
In the experiments, we evaluated the behavioral planning failuredetectionandrecoverysignificantlybooststherobot’s
capabilities of the LLM for tasks with varying complexity execution success rate in both simulation and real-world
levels, applying it to the CENTAURO robot. With a de- settings, and the success rate can be increased by up to 8%
fined behavior lib and appropriate prompts, the LLM can in specific tasks. Additionally, increasing task complexity
generate corresponding behavior plans based on different and the addition of more robot behavioral nodes result in
task instructions, achieving a high planning success rate extended task implementation times as shown in Table 4.
and task execution rate. These rates vary with the task’s
V. CONCLUSION
complexityandthenumberofbehaviorsneededtocomplete
it. By comparing the original tasks with the tasks including In this work, we introduce an autonomous online behav-
FR, incorporating failure detection and recovery into the ioral planning framework utilizing a large language model(LLM)forperformingrobotloco-manipulationtasks,requir- [11] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choro-
ing only human language instructions. Within this frame- manski,T.Ding,D.Driess,A.Dubey,C.Finnetal.,“Rt-2:Vision-
language-action models transfer web knowledge to robotic control,”
work, we propose the concept of a behavior library and
arXivpreprintarXiv:2307.15818,2023.
design action and perception behaviors, which are both [12] A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Ir-
interpretable and pragmatically efficient, with corresponding pan, A. Khazatsky, A. Rai, A. Singh, A. Brohan et al., “Open
x-embodiment: Robotic learning datasets and rt-x models,” arXiv
behavioral tags provided for semantic interpretations. The
preprintarXiv:2310.08864,2023.
LLM organizes these behaviors into a task graph with a [13] J.Liang,W.Huang,F.Xia,P.Xu,K.Hausman,B.Ichter,P.Florence,
hierarchical structure, derived from the understanding of and A. Zeng, “Code as policies: Language model programs for em-
bodiedcontrol,”in2023IEEEInternationalConferenceonRobotics
given instructions. The robot then follows the nodes in
andAutomation(ICRA). IEEE,2023,pp.9493–9500.
this task graph to sequentially complete the task. Addi- [14] W.Huang,P.Abbeel,D.Pathak,andI.Mordatch,“Languagemodels
tionally, it detects and attempts to correct possible failures aszero-shotplanners:Extractingactionableknowledgeforembodied
agents,”inInternationalConferenceonMachineLearning. PMLR,
by integrating the visual language model with intrinsic
2022,pp.9118–9147.
perceptions throughout the task process, thus successfully [15] Y. Ding, X. Zhang, C. Paxton, and S. Zhang, “Task and motion
planningandexecutinglong-horizontasks.Experimentswith planning with large language models for object rearrangement,” in
2023 IEEE/RSJ International Conference on Intelligent Robots and
the CENTAURO robot validate the achieved performance
Systems(IROS). IEEE,2023,pp.2086–2092.
and practicality of this framework in robotic task planning. [16] W.Huang,C.Wang,R.Zhang,Y.Li,J.Wu,andL.Fei-Fei,“Voxposer:
Future work will focus on enriching the robot’s behavior Composable 3d value maps for robotic manipulation with language
models,”arXivpreprintarXiv:2307.05973,2023.
lib, as well as improving the prompts system, so that the
[17] D.Driess,F.Xia,M.S.Sajjadi,C.Lynch,A.Chowdhery,B.Ichter,
LLM can better plan and optimize behavioral sequences A.Wahid,J.Tompson,Q.Vuong,T.Yuetal.,“Palm-e:Anembodied
automatically based on the robot’s intrinsic mobility, manip- multimodallanguagemodel,”arXivpreprintarXiv:2303.03378,2023.
[18] A. Yang, A. Nagrani, P. H. Seo, A. Miech, J. Pont-Tuset, I. Laptev,
ulation, and perceptual strengths, thus enabling to perform
J.Sivic,andC.Schmid,“Vid2seq:Large-scalepretrainingofavisual
more complex mobile manipulation tasks. Another direction language model for dense video captioning,” in Proceedings of the
is to improve the dynamic planning and multiconditional IEEE/CVFConferenceonComputerVisionandPatternRecognition,
2023,pp.10714–10726.
reasoning capability of the framework. This includes behav-
[19] S.Antol,A.Agrawal,J.Lu,M.Mitchell,D.Batra,C.L.Zitnick,and
ioral replanning in response to external perturbations or the D. Parikh, “Vqa: Visual question answering,” in Proceedings of the
introduction of artificial subtasks during a task. IEEE international conference on computer vision, 2015, pp. 2425–
2433.
[20] M.ColledanchiseandP.O¨gren,BehaviortreesinroboticsandAI:An
introduction. CRCPress,2018.
REFERENCES
[21] L.Muratore,A.Laurenzi,E.M.Hoffman,andN.G.Tsagarakis,“The
xbotreal-timesoftwareframeworkforrobotics:Fromthedeveloperto
[1] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, theuserperspective,”IEEERobotics&AutomationMagazine,vol.27,
C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman et al., “Do as i no.3,pp.133–143,2020.
can,notasisay:Groundinglanguageinroboticaffordances,”arXiv [22] A. Laurenzi, E. M. Hoffman, L. Muratore, and N. G. Tsagarakis,
preprintarXiv:2204.01691,2022. “Cartesi/o:Arosbasedreal-timecapablecartesiancontrolframework,”
[2] N. Kashiri, L. Baccelliere, L. Muratore, A. Laurenzi, Z. Ren, E. M. in2019InternationalConferenceonRoboticsandAutomation(ICRA).
Hoffman, M. Kamedula, G. F. Rigano, J. Malzahn, S. Cordasco, IEEE,2019,pp.591–596.
P. Guria, A. Margan, and N. G. Tsagarakis, “Centauro: A hybrid [23] J.Tremblay,T.To,B.Sundaralingam,Y.Xiang,D.Fox,andS.Birch-
locomotion and high power resilient manipulation platform,” IEEE field, “Deep object pose estimation for semantic robotic grasping of
RoboticsandAutomationLetters,vol.4,no.2,pp.1595–1602,2019. householdobjects,”arXivpreprintarXiv:1809.10790,2018.
[3] I. Dadiotis, A. Laurenzi, and N. Tsagarakis, “Whole-body mpc for [24] R.OpenAI,“Gpt-4v(ision)systemcard,”Citekey:gptvision,2023.
highly redundant legged manipulators: Experimental evaluation with [25] M.Iovino,E.Scukins,J.Styrud,P.O¨gren,andC.Smith,“Asurveyof
a37dofdual-armquadruped,”in2023IEEE-RAS22ndInternational behaviortreesinroboticsandai,”RoboticsandAutonomousSystems,
ConferenceonHumanoidRobots(Humanoids),2023,pp.1–8. vol.154,p.104096,2022.
[4] D.Kappler,P.Pastor,M.Kalakrishnan,M.Wu¨thrich,andS.Schaal, [26] B. Calli, A. Walsman, A. Singh, S. Srinivasa, P. Abbeel, and
“Data-driven online decision making for autonomous manipulation.” A. M. Dollar, “Benchmarking in manipulation research: The ycb
inRobotics:scienceandsystems,vol.11,2015. object and model set and benchmarking protocols,” arXiv preprint
[5] Y. Ma, F. Farshidian, T. Miki, J. Lee, and M. Hutter, “Combining
arXiv:1502.03143,2015.
learning-basedlocomotionpolicywithmodel-basedmanipulationfor
leggedmobilemanipulators,”IEEERoboticsandAutomationLetters,
vol.7,no.2,pp.2377–2384,2022.
[6] J.-P. Sleiman, F. Farshidian, and M. Hutter, “Versatile multicontact
planningandcontrolforleggedloco-manipulation,”ScienceRobotics,
vol.8,no.81,p.eadg5014,2023.
[7] A. De Luca, L. Muratore, and N. G. Tsagarakis, “Autonomous
navigationwithonlinereplanningandrecoverybehaviorsforwheeled-
legged robots using behavior trees,” IEEE Robotics and Automation
Letters,vol.8,no.10,pp.6803–6810,2023.
[8] M.Murooka,I.Kumagai,M.Morisawa,F.Kanehiro,andA.Kheddar,
“Humanoid loco-manipulation planning based on graph search and
reachability maps,” IEEE Robotics and Automation Letters, vol. 6,
no.2,pp.1840–1847,2021.
[9] BostonDynamics,“Insidethelab:Takingatlasfromsimtoscaffold.”
[10] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn,
K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu et al., “Rt-1:
Robotics transformer for real-world control at scale,” arXiv preprint
arXiv:2212.06817,2022.