BINDY – BAYESIAN IDENTIFICATION OF NONLINEAR DYNAMICS
WITH REVERSIBLE-JUMP MARKOV-CHAIN MONTE-CARLO
PREPRINT
MaxD.Champneys∗ TimothyJ.Rogers
DynamicsResearchGroup DynamicsResearchGroup
UniversityofSheffield UniversityofSheffield
MappinSt,Sheffield MappinSt,Sheffield
max.champneys@sheffield.ac.uk tim.rogers@sheffield.ac.uk
ABSTRACT
Modelparsimonyisanimportantcognitivebiasindata-drivenmodellingthataidsinterpretability
andhelpstopreventover-fitting. Sparseidentificationofnonlineardynamics(SINDy)methodsare
abletolearnsparserepresentationsofcomplexdynamicsdirectlyfromdata,givenabasisoflibrary
functions. Inthiswork,anovelBayesiantreatmentofdictionarylearningsystemidentification,as
analternativetoSINDy,isenvisaged. Theproposedmethod–Bayesianidentificationofnonlinear
dynamics(BINDy)–isdistinctfrompreviousapproachesinthatittargetsthefulljointposterior
distributionoverboththetermsinthelibraryandtheirparameterisationinthemodel.Thisformulation
confers the advantage that an arbitrary prior may be placed over the model structure to produce
modelsthataresparseinthemodelspaceratherthaninparameterspace. Becausethisposterioris
definedoverparametervectorsthatcanchangeindimension,theinferencecannotbeperformedby
standardtechniques. Instead,aGibbssamplerbasedonreversible-jumpMarkov-chainMonte-Carlo
isproposed. BINDyisshowntocomparefavourablytoensembleSINDyinthreebenchmarkcase-
studies. Inparticular,itisseenthattheproposedmethodisbetterabletoassignhighprobabilityto
correctmodelterms.
Keywords BayesianStatistics·Nonlineardynamics·Reversible-jump·MCMC
1 Introduction
Describingthedynamicsofnonlinearprocessesanalyticallyisoffundamentalinteresttomanybranchesofscientific
modelling. Incaseswheregoverningdifferentialequationsareunavailable,thepractitionerhaslittlechoicebuttotry
anddiscerndifferentialequationsdirectlyfromdata. Thisdifficultyiscompoundedinsituationswherebyboththe
modelstructure(whichtermsshouldbeincluded)anditsparameterisationareunknown. Inhigh-noiseandlow-data
regimes,modeluncertaintyquantification(UQ)becomescritical. Limiteddata,measurementnoise,andanunknown
modelstructureallcontributesignificantuncertainty.
Animportantfacetofdata-drivenmodellingismodelparsimony. Recently,therehasbeenincreasinginterestinmethods
that are able to develop parsimonious models directly from measured data. Methods such as sparse identification
of nonlinear dynamics (SINDy) [1] have been extremely effective at learning sparse representations of complex
∗Correspondingauthor
4202
guA
51
]LM.tats[
1v26080.8042:viXraBINDy PREPRINT
dynamics. However,akeyfeatureofallSINDyapproachesisthespecificationofheuristicsparsity-inducingmethods
andhyperparametersthatcontroltheinevitabletrade-offbetweenfidelityandparsimonyindata-drivenmodelling.
In this paper, the authors take the view that developing parsimonious models from data is inherently an uncertain
task. Parsimoniousmodelselectioninpracticerequiresinterpretableanswerstoquestionsintheveinof‘whatisthe
probabilitythatagiventermshouldbeincludedgiventhedata?’ and‘ifoneweretoincludethisterm,howisitsvalue
distributed?’. Inordertoaccessprincipledanswerstotheabove,authorsproposeanalternativetoSINDywithina
Bayesianframework.
Bayesian uncertainty quantification is a natural framework for conducting UQ in data-driven modelling. Indeed,
muchhasbeenwrittenonthetopic,e.g.[2,3]. Physicalknowledgeandinductivebiases(suchasparsimony)canbe
incorporatedaspriorknowledge,makingsuchmodellingassumptionsexplicit. Indeed,manyliteraturecontributions
proposeBayesianmethodsforUQinbothmodelparameterisation[4–6]andmodelstructure[7–10]. Furtherdiscussion
ofspecificrelatedworksispresendedinSection3.4.
1.1 Contribution
Inthiswork,interpretablepriordistributionsareplacedovertermsinthemodel(independentlyoftheirparameterisation)
andoverparameters(independentlyoftheirinclusion). Thispriorstructureinvokesajointposteriordistributionthatcan
bemarginalisedtoobtainusefuldistributions,thatcanrobustlyaddressmodellingquestionssuchasthoseintroduced
above.
ThekeycontributionofthisworkisanovelBayesianapproachtotheidentificationofnonlineardynamicsfroma
libraryofbasisterms. Inparticular,
• Heuristic sparsity-inducing regression is replaced with interpretable prior distributions over models – all
modellingassumptionsaremadeup-front.
• Anefficientsamplerisproposedtoproducesamplesfromthetargetjointposteriordistributionovermodel
termsandparameters.
• Theproposedapproachisshowntocomparefavourablytoensemble-SINDyinthreecasestudiesincludinga
popularpopulationdynamicsdatasetconsistingofonly21datapoints.
Theremainderofthispaperisstructuredasfollows: ThefollowingsectionsummarisestheSINDymethodandholds
some discussion on parsimony in data-driven modelling. A third section reviews the reversible-jump theory and
introducestheproposedapproachanditspositionintheliterature. Afourthsectioncomparestheproposedapproachto
theensembleSINDymethodontwobenchmarkcasestudies. Afinalsectionpresentssomediscussionanddirections
forfuturework.
2 SINDymethods
TheSINDymethodproposedin2016byBruntonetal. [1]hasreceivedconsiderableattentioninthescientificliterature
asacomputationally-efficientwaytolearndifferentialequationsdirectlyfromdata. Sinceitwasfirstproposed,the
methodhasbeenproventobeeffectiveincorrectlyidentifyinggoverningequationsfrombothsimulatedandreal-world
data.
Attheircore,SINDymethodsmakethekeyassumptionthattheobserveddynamicsadmitafirst-orderformulation
thatislinearintheparametersofalibraryofuser-specifiedbasisfunctionsΘ. Letf denotethefirst-orderordinary
differentialequation(ODE)thatdescribestheevolutionofsomestate-spacedynamicsx(t),2
x˙(t)=f(x(t)) (1)
2ThemethodcantriviallybeextendedtosystemsofODEs,PDEsetc.butfornotationalsimplicity,theexpositioninthispaper
willconsiderscalarxandf only.
2BINDy PREPRINT
wheretheoverdotrepresentsatimederivative. NowthefoundationalassumptionoftheSINDymethodcanbestated,
x˙ ≈Θ(x)Ξ (2)
wherexisthestatevector,Θisalibraryofuser-selectedbasisfunctionscomputedfromx. Critically,thecoefficient
vectorΞisassumedtobesparse, wherebymanyelementsofΞareequalto0. Inpractice, itisnotoftenthecase
thatmeasurementsofbothxandx˙ areavailable. ItisusualintheSINDyliteraturetoassumethatonlythestatesx
aremeasuredandtheirderivativesx˙ canbecomputednumerically(withsufficientaccuracysuchthat(2)holds). The
SINDyframework(foridentificationofODEs)canthusbesummarisedbythreesteps:
1. Theselectionofalibraryofcandidatebasisterms.
2. Theselectionofanumericaldifferentiationschemetocomputex˙.
3. SparseregressiontothecoefficientvectorΞbyheuristicmeans.
Itisclearthatagreatdealofmethodscanbeappliedtoeachofthestepsabove. Inthisway,SINDy-typemethodshave
cometoencompassafamilyofapproaches. SincetheoriginalSINDyalgorithmwasintroducedin[1],extensionshave
beenproposedtopartialdifferentialequation(PDE)systems[11],implicitdynamics[12],discretedynamicsandweak
PDEsolutions[13]. Furthermore,manymethodshavebeenappliedtothesparseregressiontaskincludingsequentially-
thresholdedleast-squares[1],sparserelaxedregression[14]andforwardorthogonalleast-squaresregression(FROLS)
[15]. Severalapproachesalsoincludeuncertaintyquantificationsuchastheensembleformulationin[16]andsparsity-
inducingBayesianmethods[10,17,18]. Manyofthesemethodshavebeenmadereadilyavailabletopractitionersvia
anopen-sourcepythonlibrary[19].
3 Proposedapproach
OfthethreecomponentsinSINDymethodsdescribedintheprevioussection,thispaperisconcernedonlywiththe
third—determination of appropriate model terms and their parameters. At this stage, it is useful to present some
discussionastotheroleofparsimonyindata-drivenmodelling.
3.1 Parsimonyindata-drivenmodelling
Inherently, model parsimony is a cognitive bias, injected by modellers to promote interpretability and to prevent
overfitting. Inmanysparsity-promotingmethods(suchasSINDy),parsimonyisenforcedbyproxy;theimportance
ofeachterminthelibraryisrelatedtothecorrespondingsizeoftheparameterinΞ. Althoughthisisaconvenient
proxyforparsimony,itisnotwithoutlimitations. AcriticallimitationinpracticeisthatsmallparametersinΞdonot
necessarilycorrespondtosmalleffectsinthedynamics. Considertheeffectofneglectingasmallbutnegativedamping
terminthemodelleadingtoinstabilityinextrapolation. Practitionerscannotknowinadvancehowsmallparameters
willbehaveinregionsofthestatethatarenotobservedinthetrainingdata. NormalisationofthecolumnsofΘcanhelp
toaddressthisshortcoming,butcanalsointroduceambiguitybetweencorrelatedlibraryfunctions. Foramotivating
exampleconsidertheproblemofselectingbetweenthetermsx2andx4,x∈[0,1]inthepresenceofnoise. Ifallterms
inΘ(x)normalisedtounitstandarddeviation,thedifferencebetweenthetwotermsmightappeartrivialintheregime
ofthetrainingdata;thedifferenceinextrapolationisevident. Introducingcorrelationsofthisformcanultimatelyharm
modelparsimony.
Aparticularissuewithparametersizeasaproxyfortermimportanceistheselectionofathresholdvalue,belowwhich
modeltermsshouldbeexcluded. Inpractice,thislevelcanbedifficulttoselectinaprincipledmanner,leadingtoa
spectrumofmodelsatdifferentlevelsofsparsity.
Alternativeproxiesforterminclusionhavebeenconsideredbasedongreedyreductionofsomeerrormetric. Akey
exampleisforwardregressionleast-squaresoptimisation(FROLS)[15,19]thatattemptstogreedilyadd(orremove)
3BINDy PREPRINT
modeltermsthatmaximizeanerror-reductionratio. Suchapproachescircumventproblemsofsmallparametervalues
describedabove. Inpracticehowever,thesemethodsrequirethespecificationofaconvergencethreshold(ordirectly
numberthenumberoftermsinthemodel,asintheimplementationin[19])whichcanalsobedifficulttosetinadvance
inaprincipledmanner.
It is the position of the authors that data alone cannot inform model parsimony. In order to select models that are
parsimonious,cognitivebiasesmustbeapplied. Inthiswork,awiderprobabilisticviewofparsimonyinSINDy-type
methods is taken. Rather than use parameter magnitude as a proxy for sparsity, the authors propose to target the
parameterinclusionprobabilitydirectly. TakingaBayesianviewof(2)andassuminganadditiveGaussiannoisemodel
onehas,
x˙ =Θ(x)Ξ+ϵ, ϵ∼N(0,σ2) (3)
whereσ2isthenoisevarianceintheobservationmodel. Themodelidentificationtaskathandistoidentifyasubsetof
thecolumnsofΘ(x). Intuitively,thismodelsetincludesallsubsetsoflibraryfunctions(thepowerset)andthusall
possiblewaysthattheparametervectormaybesparsified. Let,
M={m
}2n
(4)
i i=1
bethesetofallsuchpossiblemodelsforΘ(x)consistingofnlibraryfunctions3. Keytotheformulationhere,isthat
eachmodelinm∈MisparametrisedbyacorrespondingparametervectorΞ,thedimensionofwhichdependsonthe
modelm.
Theobjectiveofthisworkistoinferthejointposteriordistributionp(Ξ,m|x˙,Θ(x)). Theobservationmodelin(3)
givesrisetoaGaussianlikelihoodp(Ξ,m|x˙,Θ(x))foreachmodelmandcorrespondingparametervectorΞ. Withan
appropriatelydefinedpriorp(Ξ,m),andapplyingBayesrule,onemaywrite,
p(Ξ,m)p(x˙|Ξ,m,Θ(x))
p(Ξ,m|x˙,Θ(x))= (5)
p(x˙)
Fornotationalsimplicityintheexpositionthatfollows,thedependenceofthelikelihoodandposterioronΘ(x)willbe
dropped,andtheobserved(orcomputed)statederivativesaredenotedbyD =x˙. Theabovecanthusbewritten,
p(Ξ,m)p(D|Ξ,m)
p(Ξ,m|x˙,D)= (6)
p(D)
Accesstotheposteriordistributionoverboththemodeltermsanditsparameterisationconfersanumberofadvantages
forthedata-drivendiscoveryofnonlineardynamics. Ifsamplesfromtheposteriorareavailable,amajoradvantageis
thattheycanbemarginalised(intheMonte-Carlosense)togivedirectaccesstoaposteriordistributionovermodels,
independentlyoftheirparameterisationi.e. p(m|D). Thisallowsthepractitionertoevaluatetheprobabilitythataterm
shouldbeincludedinamodel(giventheobserveddata). Thispermitsmorerobustwaystointroducemodelparsimony,
forexamplebyexcludingtermswithaninclusionprobabilitybelowacertainthreshold. Alternatively,thefullposterior
overmodeltermsandparameterscanbepropogatedtofurtheranalyses.
Thejointposteriordistributionp(Ξ,m|x˙,D)overbothmodelsandtheirparametersisaverychallengingobjectto
approach. AsisusualincomplexBayesianinferencetasks,themodelevidencetermp(D)isunavailableanalytically.
However,thisisnottheonlyobstacle. ThemajorinferentialchallengeisthatthedimensionoftheparametervectorΞ
3Themethodexposedinthisworkdoesnotrequirentobefiniteandcanbegeneralisedtothecaseofinfinitelibraries(for
exampleallpolynomials),howevertheexpositionandresultshereinconsiderfinitenonly.Someadditionaldiscussionisheldin
Section5.
4BINDy PREPRINT
hascometodependontheparticularmodelitparameterises. Inordertoovercomethesedifficulties,asamplingscheme
basedonreversible-jumpMarkov-chainMonte-Carlo(RJMCMC)[20]willbeemployed.
3.2 Reversible-jumpMCMC
ItwillbeusefulheretobrieflyreviewbothMetropolis-Hastings(MH)andreversible-jumpMarkov-chainMonte-Carlo
theoryinageneralsettinginordertomotivatetheproposedapproach. Muchoftheexpositionhereisavailablein
additionaldetailinreferences[21]forMHand[20]forreversible-jumpMCMC.Considerthewell-studiedproblemof
samplingfromatargetdistributionπ(θ)(thatisknownuptosomeconstant),byconstructingaMarkovchainwithπas
itsstationarydistribution. Asufficientconditionforconvergenceisthatofdetailedbalance,whereby,
π(θ′)κ(θ|θ′)=π(θ)κ(θ′|θ) (7)
Whereκ(θ′|θ)isthekerneloftheMarkovchainmovingfromstateθtonewlocationθ′. TheMHalgorithmfurther
dividesthiskernelintoatransitiondensityp(θ′|θ)andanacceptanceprobabilityα(θ →θ′),
k(θ′|θ)=p(θ′|θ)α(θ →θ′) (8)
ThefamiliarMHacceptanceprobabilitycanbefoundbymaximisingtheacceptanceprobabilitywhileretainingthe
conditionofdetailedbalance. Theoptimalchoiceisfoundtobe,
(cid:110) π(θ) p(θ|θ′)(cid:111)
α(θ →θ′)=min 1, (9)
π(θ) p(θ′|θ)
(cid:124)(cid:123)(cid:122)(cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
Target Proposal
ratio ratio
wherebyanunknownnormalisationconstantinπcanbecancelledfromthenumeratoranddenominator.
Inthecasethatθ′andθhavedifferentdimensions,detailedbalancewillnotholdforstandardchoicesofthetransition
density. Toovercomethisissue,in1995Greenintroducedthereversible-jumpMCMC(RJMCMC)algorithm[20]asa
methodtosamplefromdistributionsdefinedoverparametersofdifferentdimension. TheRJMCMCmethodovercomes
thisdifficultyviadimensionmatching. LetkbethedimensionofthestateofaMarkovchainatθ. Inordertomovetoa
newstatewithdimensionk′andpositionθ′,anauxiliaryvariableu′withdimensionj′issampled. Tocompensatefor
themismatchbetweendimensions,itisrequiredthatj+k =j′+k′,wherej isthecurrentdimensionoftheauxillary
variableu. Then,abijectivemapg :{u,θ}→{u′,θ′}betweeneachpairofdimensionsisdefined,suchthatthe
k→k′
dimensionof{u,θ}isunchangedbythebijection. Iftheprobabilityof‘jumping’fromonedimensiontoanother(via
theappropriatecorrespondingbijection)isgivenbyJ(k′,θ′|k,θ)q(u′|θ′)ThentheRJMCMCacceptanceprobability
canbereformulatedas,
α(θ
→θ′)=min(cid:110)
1,
π(θ′) J(k′,θ′|k,θ) q(u′|θ′)(cid:12) (cid:12) (cid:12)∂g k→k′(cid:12) (cid:12) (cid:12)(cid:111)
(10)
π(θ) J(k,θ|k′,θ′) q(u|θ) (cid:12)∂(u,θ)(cid:12)
(cid:124)(cid:123)(cid:122)(cid:125)(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
Target Jumpratio Auxillary Jacobian
ratio ratio determinant
In the general setting, working with RJMCMC can be cumbersome for the practitioner. The specification of the
bijectivemapsisnon-trivialandhasastrongeffectonthesamplingefficiencyofthescheme. Fortunately,asignificant
simplificationissometimesavailableandwillbeemployedhere. Thetrickistosamplemodelparametersindependently
betweenmodels. Thisisachievedbystettingu≜θ′andu′ ≜θineachtransition. Thus,eachbijectioncanbedefined,
g (u,θ)={u′,θ′}={θ,θ′} (11)
k→k′
5BINDy PREPRINT
Interrogatingtheabove,itisclearthatthedimensionmatchingconstraintissatisfiedandthattheJacobianisarow-wise
re-orderingoftheidentitymatrix. Therefore,theJacobiandeterminanttermintheacceptanceratiomustbeidentically
equaltoone. Theacceptanceprobabilitymaynowbesimplified,
(cid:110) π(θ′) J(k′,θ′|k,θ) q(θ|θ′)(cid:111)
α(θ →θ′)=min 1, (12)
π(θ) J(k,θ|k′,θ′) q(θ′|θ)
(cid:124)(cid:123)(cid:122)(cid:125)(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
Target Jumpratio Proposal
ratio ratio
3.3 Bayesianidentificationofnonlineardynamics
WiththerelevantRJMCMCtheoryestablished,attentioncanbereturnedtotheproblemofsamplingfromtheposterior
densityp(Ξ,m|D). Inordertodeveloptheposterioroverboththemodeltermsandtheirparameterisationpriorand
proposaldensitiesmustbespecified. Itisclearthattherearemanyappropriatechoicesfortheseobjectsandinpractice
itcanbeexpectedthatpriorselectionwillbeguidedbytheproblemathand. However,itcanbeshownthatcertain
choicesoftheparameterproposalcanleadtodrasticsimplificationoftheinferencescheme.
Consideringfornowthecasewherebythenoisevarianceσ2 in(3)isknownandsubstitutingtherequiredposterior
distributionintotheacceptanceprobabilityabove,onehas,
(cid:110) p(Ξ′,m′|D,σ2) J(m′|m) q(Ξ|Ξ′)(cid:111)
α(Ξ→Ξ′)=min 1, (13)
p(Ξ,m|D,σ2) J(m|m′) q(Ξ′|Ξ)
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
Posterior Jumpratio Proposal
ratio ratio
Theproposaldensityintheaboveisfreelychosenbytheuser. Anaturalchoice,(afterTroughtonandGodsill[22])isto
employthefullconditionaldensity,
q(Ξ′|Ξ)=p(Ξ′|m′,D,σ2) (14)
substitutingintotheabove,onefindsthatsince,
p(Ξ,m|D,σ2)
p(m|D,σ2)= (15)
p(Ξ|m,D,σ2)
theacceptanceratiocanbefurthersimplifiedto,
(cid:110) p(m′|D,σ2) J(m′|m)(cid:111)
α(Ξ→Ξ′)=min 1, (16)
p(m|D,σ2) J(m|m′)
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
Ratioofmodel Jumpratio
posteriors
Thissimplificationisonlyavailableforparticularchoicesofthepriorp(Ξ,m)thatareconjugate,e.g.theGaussian
likelihooddefinedabove(anddefinedindependentlytothepriorovermodels). However,itgivesdirectaccesstothe
marginalposteriorsp(m|D)(see[23,24]forexample). ConsideringhereaconjugateGaussianpriorovertheparameter
values(independentlyoftheirinclusioninthemodel),onehas,
p(Ξ|m)≜N(µ(0),Σ(0)) (17)
m m
6BINDy PREPRINT
p(m|D,σ2)∝p(m)p(D,σ2|m,Ξ)
(cid:18) 1 (cid:19) (18)
∝p(m)σ−k|Σ m|21 exp 2µT mΣ− m1µ
m
where,
(cid:104) (cid:105)−1
Σ =σ2 ΘT Θ +Σ(0)−1 (19)
m m m m
1
µ =Σ Σ(0)−1µ(0)+ ΣT ΘTx˙ (20)
m m m m σ2 m
aretheposteriormeanandcovarianceoftheparametersforagivenmodelm(see[23,24]forexample). Thus,the
overallacceptanceprobabilityofamovefromΞtoΞ′canbewritten,
α(Ξ→Ξ′)=min(cid:110)
1,
p(m′) J(m′|m) σ−k′|Σ m′|1 2 exp(cid:0)1 2µT m′Σ− m1 ′µ m′(cid:1) (cid:111)
(21)
p(m) J(m|m′) σ−k|Σ m|1
2
exp(cid:0)1 2µT mΣ− m1µ m(cid:1)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
Modelprior Jumpratio Modelposteriorratio
Theabovecannowbereadilyusedtosamplefromtherequiredposterior. Inpractice,thenoisevariancein(3)willbe
unknown. ThisproblemcanbesimplyaddressedbyincludingaGibbsmoveforthenoisevariancegiventhemodeland
theparameters,whichareavailablefollowingareversible-jumpmove. Inthiswork,aninverse-gammapriorisused.
TheGibbsstepistherefore,
p(σ2)≜IG(a(0),b(0)) (22)
Then by the conjugacy of the prior and likelihood, the posterior over the noise variance is available exactly as an
inverse-gammadistribution,
p(σ2|Ξ,m,D)=IG(a,b) (23)
where,
n
a=a(0)+ (24)
2
(cid:80)n(x˙ −Θ(x)Ξ)2
b=b(0)+ i (25)
2
wherenisthenumberofdatainx˙. TheoverallsamplerissummarisedinAlgorithm1.
3.4 Relatedwork
SeveralauthorshaveapproachedthejointmodelstructureandparameterisationproblemfromaBayesianperspective. A
keycontributionistheworkofAbdessalemetal. [8,9]whereinanapproximateBayesiancomputation(ABC)method
wasappliedtotheidentificationofanonlinearsystemfromwithinasmallnumberofcandidatemodels. Althoughthe
resultsofthemethodarecompelling(approximateposteriors,jointlyovermodelsandparameters),theutilityofthe
methodislimitedbytheextremecomputationalcostofABC.
7BINDy PREPRINT
Algorithm1Gibbssamplerwithreversible-jumpmodelmoves
Require: Parameterpriormeanandvarianceµ(0),Σ(0),variancepriorparametersa(0),b(0),priorovermodelsp(m),
m m
jumptransitionprobabilitiesJ(m′|m).
Initialisethechainatm=M(i.e. witheverytermincluded)andσ2
0
forNumberofsamplesN do
Proposeamodelmovem→m′bysamplingfromJ(m′|m)
Acceptthemodelmovewithprobabilityα(Ξ→Ξ′)givenby(21)elseremaininplace.
Proposeanewnoisevarianceσ2usingtheGibbsupdategivenby(23)
endfor
returnsamplesfromp(Ξ,m|D)
The proposed method is also not the first time that the SINDy method has been formulated in a Bayesian context.
Previousattemptshavemadeuseofsparsityinducingpriors[5,17,25–28]. Thesepriorsareplacedoverparametersin
Ξsuchthatmuchoftheposteriormassisconcentratedatoraroundzero(thusremovingthemfromthemodelwithhigh
probability). Althoughsparsity-inducingpriorsareshowntobeveryeffectiveincorrectlyidentifyingmodelterms(by
settingsuperfluousparameterstozero),themethodcannotobtainadistributionalestimateoverthetermsinthemodel
independentlyoftheirparameterisation4. Incontrast,themethodproposedinthisworkdiffersinthatpriordistributions
areplacedoverbothmodeltermsandtheirparametersindependently. Anotherchallengewithsparsity-inducingpriorsis
thattheinferenceisusuallyintractableandcanbeexpensivetocompute. Theworkinthispaperisabletosignificantly
reducethecomputationalburdenbymarginalisingtheparametersΞfromtheacceptanceratio(after[22])suchthata
newparameterneedonlybesampledwhenupdatingthenoisevariance.
ArelatedworkistherecentcontributionofFungetal. [18]. TheauthorsproposeaBayesianformulationoftheSINDy
methodbytracinguncertaintythroughthenonlinearlibraryandthedifferentialoperatorinordertoaccountfornoiseon
observedx. TheauthorsthenmakeGaussianapproximationstorecoverthemodelevidenceinaBayesianmanner. The
authorsthenproposetogreedilyselecttermsthatmaximallyincreasethemodelevidenceuntilnomoresuchtermsare
available. Thisresultsinasinglemodelstructureestimatewithaposteriordistributionoveritsparameterisation. The
methodproposedinthisdiffersinthatajointposteriordistributionoverbothmodeltermsandtheirparameterisation
isavailableexplicitly,thankstothereversible-jumpsampler. Theproposedapproachisalsonotlimitedtogreedily
maximisingthemodelevidence,whichcanbeimportantinsituationswherebytheposteriorismultimodalinmodel
space.
ThemethodpresentedhereinisalsonotthefirstuseofRJMCMCintheBayesianidentificationofdynamicmodels.
Troughton and Godsill [22] and Dahlin et al. [29] consider an RJMCMC scheme for the identification of linear
autoregressivemodels,andaRJMCMCschemeisemployedin[30]toselectbetweentwocandidatenonlinearmodels.
However,totheauthor’sbestknowledge,thisworkrepresentsthefirstapplicationofRJMCMCtotheSINDy-type
formulationof[1],andthefirsttoexplicitlyenumerateafullposteriordistributionoverbothmodeltermsandtheir
parameterisationjointly.
Anotherrelatedcontributionistheuncertaintyquantificationenabledbytheensemble-SINDy(E-SINDy)methodof
Faseletal. [16]. E-SINDyenablesheuristicUQthroughbootstrappingand(robust)averaging(b(r)agging)onsubsets
ofmodeltermsanddata. Inthisway,theauthorsareabletogiveheuristicestimatesofquantitiessuchasterminclusion
probabilitiesandbootstrappeddistributionalestimatesofmodelparameters. Whilethesequantitiesareundoubtably
usefultopractitioners,theirheuristicnaturemakestheirtheoreticalinterpretationdifficult. Nevertheless,E-SINDyisa
naturalbenchmarkcomparisonforthemethodproposedinthiswork. Inallforthcomingcasestudies,bothmethodsare
demonstrated.
4Becausesparsityinducingpriorsareusuallydefinedcontinuouslyovertheparameterrangeitisnotpossibletomarginaliseout
theparametervalueswithoutsettingaheuristicthresholdonwhichparametervaluesshouldbeconsideredinsignificant.
8BINDy PREPRINT
4 Results
Inordertodemonstratetheeffectivenessoftheproposedmethod,threebenchmarkcase-studiesarepresentedhere.
ResultsfromboththeproposedmethodandE-SINDy[16]arepresentedandcomparedintermsoftheirconfidencein
identifyingthetruedynamics.
Inallcase-studyexamples,thepriorstructureoftheinferenceissetuptobeweaklyuninformative. Inparticular,the
values,
µ ≜0(0), Σ(0) ≜103×I (26)
m m
a(0) ≜0, b(0) ≜0 (27)
arechosen. Thepriordistributionovermodelsisselectedinitiallytobeflatwithp(m)∝1∀m∈M. Theauthors
would note that this choice not promote model parsimony a priori. For the jump transitions J(m′|m) the authors
proposea‘bit-flipping’schemewherebyineachproposalsteparandomindexin[1,n](wherenisthenumberofrows
inΘ)isselectedwithequalprobability. Ifthattermispresentinthecurrentmodelthenitisremoved,ifnotthenitis
added. Ifamoveisrejectedthenanewindexisdrawn,againwithevenprobability. Thechainisinitialisedwithall
termsinΘpresentinthemodelandwiththenoisevariancesettoσ2 =1. TheGibbssamplerisrunfor6×103steps
intotal,withthefirstthousandsamplesdiscardedtoremovetheeffectsofanytransient‘burn-in’behavior. Theauthors
remarkthatthislengthofchainappearsmorethanadequateintermsofconvergencetothetargetdistribution.
TheE-SINDyresultspresentedhereafteraregeneratedusingthepubliclyavailable‘pysindy’library[19]. Inallcases,
5×103modelsaresampled(inlinewiththelengthofthechainsintheRJMCMC)andbothdataandlibraryensembling
areactivated,withthenumberofcandidatesdroppedineachsamplesetto1. Inbothcases,thesequentially-thresholded
least-squares(STLSQ)algorithmisusedtoperformthesparseregression. FormoredetailontheE-SINDymethodand
forinterpretationofthesehyperparameterstheinterestedreaderisdirectedtotheoriginalarticle[16]andthe‘pysindy’
documentation[19].
4.1 Legendrepolynomials
Thefirstcase-studyisastaticsparsepolynomialregressionprobleminwhichΘ(x)isstaticandissettobethefirst10
Legendrepolynomialsontheintervalx∈[−1,1]. Inordertodemonstratetherecoveryofsmall-valuedparameters,a
coefficientvectorisselectedwithbothzeroandnear-zeroelements. Thecoefficientvectorusedhereisgivenby,
Ξ∗ ≜[0.549,0,0.603,0.545,0.424,0.006,0,0,0,0.004]T (28)
Thetargetoftheregressionisthengeneratedas,
y ≜Θ(x)Ξ∗+ϵ ϵ∼N(0,σ2I) (29)
n
wherebyGaussiannoiseisaddedtothetargetdatawithstandarddeviationequalto5%oftheroot-mean-square(RMS)
ofthenose-freedata.
TheperformanceofbothE-SINDyandtheproposedmethodarecomparedinFigure1. Ascanbeseeninthefigure,
BoththeproposedmethodandE-SINDymethodsareabletocorrectlyassignlowinclusionprobabilitytothetermsin
Ξ∗equaltozero. However,theSTLSQalgorithmisunabletoidentifytermswithparameterslowerthanthethreshold
parameterλ,whichinthiscasestudyissettothedefaultvalueof0.1. Whilethisisasimplifiedexample,itservesto
demonstrateaparticularpathology,whichmayappearwhenusingparametervalueasaproxyfortermimportance.
However,theE-SINDymethodshowsmulti-modality,producingsomesamplesinclustersawayfromthetruevalue.
9BINDy PREPRINT
Figure1: ComparisonbetweenRJMCMCandSINDyinquantifyingmodeluncertaintyforthetoypolynomialdata. In
theplot,thebar-chartsdepictmodelinclusionprobability(leftaxis),pointplotsdepictsamplesofmodelparameters
(right,logaxis). Horizontalbarsdepictmedianparametervalues,blackdiamondsdepicttruevaluesofparametersin
theunderlyingdata-generatingmodel.
Thiscanbeattributedtothelibrarybaggingoperation. Everysubsetofthelibraryfunctionsproducesasample(none
arerejected-thereisnoMetropolisationstep). Thishastheeffectthatsomesamplesaregeneratedwithimportant
termsmissing,biasingtheparametervaluesoftheremainingterms.
4.2 Lynx-harepopulationdynamics
Thesecondcase-studyconsidersthewell-studiedlynx-harepopulationdynamicsdataset. Thisdatasetconsistsofthe
numberofpeltscollectedforharesandlynxesannuallybetween1900and1920bytheHudsonBayCompanyand
isconsideredtoreflectthepopulationlevelofthetwospecies[16]. Assuch,thedatasetisoftenusedasacanonical
example of data generated by the Lokta-Volterra predator-prey population model. Let L and H be the number of
LynxandHareinthepopulationrespectively,thentheirpopulationdynamicsaregovernedbythefirstorderordinary
differentialequation,
dL
=aL+bHL
dt
(30)
dH
=cH +dHL
dt
Wherea,b,c,dareunknownconstantsinthemodel. Thedatasetrepresentsasignificantchallengeinthatonly21
measurementsareavailable.
10BINDy PREPRINT
Figure2: Samplesfromtheidentifiedposteriordistribution(BINDy,green)andensemble(E-SINDy,orange)inboth
interpolationandextrapolationregimes.
Here, the derivatives of the lynx and hare populations are computed numerically in accordance with the approach
takenin[16], thecolumnsofΘarealsonormalisedandathresholdvalueof0.19forλisused. ForΘ(x), library
ofpolynomialterms,includinginteractionsuptothirdorderisused. Theauthorsnotethatthisconstitutesamore
challengingidentificationtask5thantheonepresentedin[16],whereonlytermsuptosecondorderareconsidered.
Thedistributionsidentifiedbythetwomethodsonthelynx-haredatasetaredepictedinFigure3. Inthefigure,itcanbe
seenthatbothmodelsareassigninghighprobabilitytotermsintheLokta-Volterramodel. However,theprobability
massismuchmoreconcentratedaroundthethesetermsintheBINDyposterior,comparedtothoseintheE-SINDy
ensemble. Onceagain,itcanbeseenthattheparametervaluesaretightlydistributedaroundthemaximumlikelihood
valuesforbothmethods(assumingLokta-Volterradynamics),althoughtheE-SINDymethodproducesanumberof
samplesclosetothecutoffthreshold. Particularlyconcerning,isthatthemedianparametervalueforalltermsinthe
hare evolution (as identified by ensemble SINDy) is zero. This corresponds to the STLSQ algorithm in E-SINDy
removingallparametersandreturningazeromodelmuchofthetime. Theseconstantdynamics(withnotermsinthe
model)identifiedbytheSINDyensembleareevidentinFigure4whichplotstimehistories(integratedforwardintime
numerically,fromknowninitialconditions)ofsamplesfrombothBINDyandE-SINDy. Bycomparison,thesamples
fromtheBINDyposteriorrepresentthedynamicswell,giventhescarcityofdataavailable. Inextrapolation,neither
modelisabletocloselyfollowthemeandynamics(computedastheleastsquaresestimateofa,b,c,d,assumingthe
modelstructuretobeknown). However,theuncertaintyinthedynamicsofferedbytheBINDyposteriorisvisuallyfar
morecompelling,expandinginboththelocationandmagnitudeofthepopulationpeaksforeachspecies–asmightbe
expected. Accesstomeaningfuluncertaintyquantificationforrealdatasetsinthelow-dataregimeisofhugeimportance,
forexamplewhenmodellingepidemiologicaldynamics,whereinpredictingtheheightandtimingofpeaksiscritical.
5Thesizesoftherespectivemodelspacesincreasefrom26to210whenconsideringtermsuptothirdorder.
11BINDy PREPRINT
Figure3: ComparisonbetweenRJMCMCandSINDyinquantifyingmodeluncertaintyforthelynx-harepopulation
example. In the plot, the bar-charts depict model inclusion probability (left axis), point plots depict samples of
modelparameters(right,logaxis). Horizontalbarsdepictmedianparametervalues,blackdiamondsdepictmaximum
likelihoodvaluesofparametersintheLokta-Volterradynamics.
4.3 Lorenzattractordynamics
ThethirdcasestudyconsidersthechaoticdynamicsoftheLorenzattractor. ThedynamicsoftheLorenzattractorare
wellstudiedintheSINDyliterature[1,16],andhavebecomeabenchmarksystemforadvancementstotheSINDy
method. Thedynamicsaredrivenbythefollowingsystemofordinarydifferentialequations.
dx
=10(x −x )
dt 2 1
dy
=x (28−x ) (31)
dt 1 3
dz 8
=x x − x
dt 1 2 3 3
Forthecasestudyinthiswork,theLorenzequationsabovearesimulatedfor10seconds(withafurther5seconds
ofunseendataforextrapolation)fromaninitialconditionof[x ,x ,x ]T =[−8,7,27]T atasamplerateof102 Hz.
1 2 3
Overall,103pointsareavailablefortraining. Asbefore,Θ(x)issetasapolynomiallibrary,consideringtermsupto
thirdorder. Beforetraining,noiseat2.5%RMSisaddedtothestatevariables. Thestatederivativesarecomputed
numericallyusingapolynomialsmoothedfinite-differenceschemewithawindowsizeof5andapolynomialorderof3.
FortheSINDyresults,thethresholdparameterissetto0.2(asisdonein[16]). Allotherparametersareunchanged
fromthepreviouscase-studies.
12BINDy PREPRINT
Figure4: Samplesfromtheidentifiedposteriordistribution(BINDy,green)andensemble(E-SINDy,orange)inboth
interpolationandextrapolationregimesforthelynx-harepopulationexample.
Forthisthirdcasestudy,theeffectofthepriorovermodelsp(m)isalsoconsidered. Inadditiontotheflatpriorusedin
thepreviouscase-studyexamples,aregularisinggeometricprioroverthetotalnumberofnon-zeroelementsinthe
modelisconsidered. Theprobabilitymassfunctionoftheregularisingpriorisgivenby,
p(m)=(1−θ)dθ (32)
wheredisthenumberoftermsinmodelm,andθisahyperparameter,settothestronglyregularisingvalueof0.99in
thiswork. Theeffectofthispriorovermodelsistostronglypenalise(byassigninglowpriorprobability)modelswith
manyterms.
TheresultoftheUQfortheLorenzequationsiscomparedinFigure5forBINDy(withandwithouttheregularising
prior)andE-SINDy. ItcanbeseeninthefigurethattheunregularisedBINDyassignshighposteriorprobabilityto
manyspuriousterms. Incontrast,BothE-SINDyandtheregularisedBINDyassignhighconfidencetotermsinthe
Lorenzequations. Thisisnotanunexpectedresult. ThenatureoftheLorenzequations,andtheuseofthepolynomial
libraryresultsinmany,highly-correlatedbasisfunctionsinΘ(x). This,plustheeffectofnoisepermitmanyincorrect
(butlowerror)parametrisationsoftheobserveddynamics. BothE-SINDyandregularisedBINDyareabletoovercome
thisdifficultybyexplicitlypromotingmodelparsimony. BecausetheunregularisedBINDyenforcesnosuchcognitive
bias, itisunsurprisingthattheidentifiedposteriorcontainsmanyterms. Summarystatisticsofthethreeidentified
posteriorsareprovidedinTable1.
Asbefore,sampledmodeltrajectories(integratedforwardintime)areplottedforallthreemethodsinFigure6. The
chaoticnatureoftheLorenzequationsmakestrackingthetruedynamicshighlychallenging. Thewell-known‘lobe-
switching’behaviorofthedynamicsmeansthatsmallerrorscanquicklycausesignificantdeviationsasthedynamics
becomegovernedbyadifferentlocalattractor. Thiseffectisclearlypresentintheidentifieddistributions,whichquickly
13BINDy PREPRINT
Figure5:ComparisonbetweenRJMCMCandSINDyinquantifyingmodeluncertaintyfortheLorenzattractorexample.
Intheplot,thebar-chartsdepictmodelinclusionprobability(leftaxis),pointplotsdepictsamplesofmodelparameters
(right,logaxis). Horizontalbarsdepictmedianparametervalues,blackdiamondsdepicttruevaluesofparametersin
theunderlyingdata-generatingmodel.
becomemultimodalafterafewoscillations. Qualitatively,itisobservedthattheregularisedBINDysamplesremainin
stepwiththetruedynamicsforthelongest,withE-SINDythefirsttodiverge. Thisdivergencecanbeseenatt<1s
whereoragelinesdeviatefromtheobserveddata. Indeed,thisissupportedbythenumericalresultsinTable2which
showlowererrorstatisticsfortheregularisedBINDyapproach. Intheextrapolationregion, thedistributionshave
becomeverydiffuse,inkeepingwithwhatmightbeexpectedforforecastingachaoticattractorsuchastheLorenz
equations.
5 Discussion
Inthefirsttwocasestudies,stronglyparsimoniousmodelswereidentifiedbytheproposedmethodology. Counterintu-
itively,thesemodelswereassignedhighposteriorprobabilitydespiteaflatprioroverthemodelspace. Itisinteresting
toconsiderhowthiscanbethecase,forwhichtheauthorsoffertwoexplanations. Onexaminationoftheacceptance
14BINDy PREPRINT
Figure6: Samplesfromtheidentifiedposteriordistribution(BINDy,green)andensemble(E-SINDy,orange)inboth
interpolationandextrapolationregimesfortheLorenzattractorexample.
Table1: PosteriorresultsfortheLorenzattractorcase-study. Terminclusionprobabilitiesp(Θ (x)|D)andexpected
i
parametervaluesE[Ξ |D](andstandarddeviations)foreachofthethreeconsideredmethods.
i
BINDy E-SINDy BINDy(regularised)
Equation Term
p(Θ |·) mean(std) p(Θ |·) mean(std) p(Θ |·) mean(std)
i i i
x 1.0 -10.4(0.754) 0.947 -8.76(3.88) 1.0 -10.1(0.0785)
10(x −x ) 1
2 1 x 1.0 8.6(0.594) 0.949 9.38(2.28) 1.0 10(0.0668)
2
x 1.0 27.3(0.473) 0.95 23.9(9.32) 1.0 27.4(0.31)
1
x (28−x ) x 0.998 -0.781(0.229) 0.851 1.22(5.79) 1.0 -0.845(0.107)
1 3 2
x x 1.0 -0.981(0.0173) 0.948 -0.888(0.266) 1.0 -0.983(0.0073)
1 3
x 0.998 -2.9(0.577) 0.922 -2.48(0.912) 1.0 -2.66(0.0238)
x x − 8x 3
1 2 3 3 x x 1.0 1.03(0.052) 0.931 0.936(0.262) 1.0 0.996(0.0039)
1 2
Table2: Median,Meanandstandarddeviationsofsamplemean-squarederrorscomputedfromthetrajectoriesinFigure
5.
BINDy E-SINDy BINDy(regularised)
Equation
median mean std median mean std median mean std
10(x −x ) 79.6 81.9 20.7 86.1 86.5 19.6 74.5 74.4 17.4
2 1
x (28−x ) 108 111 25.7 114 117 27.3 101 99 21.8
1 3
x x − 8x 69.4 77.5 27 85.4 102 110 60.6 66.7 22.8
1 2 3 3
15BINDy PREPRINT
ratioin(21),onefindsthattheratioisproportionaltoσ−k′/σ−k. Intuitively,thisratiohasaslightregularisingeffect
thatpenaliseslargermodelsthathavethesamefittothedata. Anotherimportantconsiderationisthecorrelations
betweenthecolumnsofΘ(x)andthestatederivativesx˙. Thesecorrelationsdependonthedata,thesystemunder
studyandchosenlibraryofbasisfunctions. Many,highlycorrelatedtermscancomplicatetheidentificationprocedure
withmanymodelsabletoproducepredictionsclosetotheobserveddata. Incontrast,inthecasewhereonlyafew
termshavehighcorrelation,theidentificationofmodelswithfewtermsissimplified. Thiseffectcouldalsoexplainthe
parsimoniousmodelsinthefirsttwocasestudies.
AswithanyBayesianapproach,thespecificationofpriordistributionsplaysanimportantrole. Alimitationofthe
currentapproachisthatthepriorsovertheparametersthemselvesarerequiredtobeconjugate. Formostapplications
thisisnotexpectedtobeanissue,althoughthereareseveralsituations(forexampleboundedparameterspacesor
monotonicfunctions)wherethisrequirementcouldbecomerestrictive. Theauthorscannotpresentlyenvisageasolution
tothisissuethatdoesnotcomeatconsiderablecomputationalcost,shouldalternative,non-conjugatepriorsbedesired.
Ashasbeendemonstratedinthethirdcase-studyofthiswork,thechoiceofpriorovermodelsp(m)maybecriticalin
enforcingparsimony. Here,astronglyregularisinggeometricdistributionparameterisedbythenumberoftermsinthe
modelhasbeenappliedsuccessfully. However,vastlymanymoreparametrisationsarepossible. Itisexpectedthatprior
selectioninpracticewillbedrivenbyexpertdomainknowledge,whereverpossible. Forexample,onecouldsetthe
priorovereachtermdirectly,assigninghighpriorprobabilitytotermsthatareexpectedtobeinthemodelandlow
probabilitiesotherwise. Alternativelyonecouldpromoteparsimonyinsometypesoflibraryfunctionswhilepermitting
densemodelsinothertypes. TheflexiblenatureoftheBayesianformulationpresentedinthiswork,allowsmodellers
toencodebothdomainknowledgeandcognitivebiasesapriori.
Onecouldevenimaginealibrarytemperingscheme(similarinsomewaystothesequentiallibraryandparameter
baggingin[16])wherebymodeltermswithlowprobabilityaretemperedoutofthepriorduringsampling. Practically
thiswouldenablemoreefficientsampling(asmoremodelmovesarelikelytobeaccepted)aswellaspermittheoperator
tostartthesamplerwithavastlygreater(oreveninfinite)libraryofterms,withtheexpectationthatallbutthoseinthe
typicalsetwouldbetemperedout.Suchaschemewouldbearsomeresemblancetowell-studiedsequentialMonte-Carlo
(SMC)samplers[31],forwhichagreatdealofanalysishasalreadybeenconducted. However,theapplicationofSMC
samplerstodomainswherebyparametersarepermittedtochange(suchasintheinteractingschemeofJasraetal. [32])
hasnotbeenaswidelystudiedandremainsanopenchallenge.
Arelatedareaforfutureinvestigationisthechoiceofthe‘jump’kernelJ(m′|m). The‘bit-flipping’methodapplied
hereinhasprovedtobeeffectiveinthecase-studiesconsideredsofar, butitmightbeineffectivewhensignificant
multi-modalityisexpectedintheposterior,forexamplewhenverydifferentlyparameterisedmodelsdescribethedata
well,lowprobabilityintermediatestepsmightinhibitexplorationoftheposterior. Instead,itisinterestingtoimagine
moresophisticatedschemesthatcouldbettersuitproblemsofthistype. Alsointerestingareso-called‘non-reversible’
jumpmethods[33]thatuseanadaptiveschemetoensurethattheposteriorisexploredefficiently.
AkeyassumptioninSINDymethodsisthatthedynamicsaredescribedasx˙ ≈Θ(x)Ξ. Implicitintheleast-squares
formulationof[1](aswellasalmosteveryotherapproachforlearningsparseΞ)istheGaussiannoisemodelin(3).
TheassumptionofGaussiannoiseonx˙ only,withx(oratleastΘ(x))noisefree,isunlikelytoholdformanypractical
applications–especiallyinthelow-data,high-noiseregime. Thecalculationofx˙ numericallyfromnoisyxislikelyto
onlycompoundthisissue,particularlyforsecond-orderdynamicalsystemswherethenumericalderivativemustbe
computedtwice. Itiscertainlypossibletoenvisagemoreadvancedstatisticaldescriptionsofthenoisemodel,whereby
noise enters the observed x is transformed nonlinearly by Θ and by the numerical differentiation in computing x˙
(suchanattemptismadein[18]). However,suchaformulationislikelytomaketheinferencedescribedinthiswork
impracticallyexpensive. Despitethestrongassumptionofthenoisemodelin3,theauthorsareencouragedbythe
strongperformanceoftheproposedmethodinbothnumericalandexperimentalcase-studies. Onepotentialexplanation
forthisperformanceisthatthenoisevarianceparameterσ2isfreelyabletoinflateduringthesampling. Thiscanhave
theeffectof‘swallowing’additionalvariancethatmaybepresentinamoreaccuratelikelihoodmodel. Theresultisa
Gaussianapproximationtothelikelihoodfunction(inamannersimilarto[18]).
16BINDy PREPRINT
6 Conclusion
Inthiswork,anovelmethodforBayesianidentificationofnonlineardynamicsystemsinaSINDy-likemannerhas
beenproposed,consideringajointinferenceoverbothmodeltermsandtheirparameterisation. Itistheargumentofthis
paperthatsuchaposteriorovermodelsismoreusefulthansingle(orevenanensembleof)sparsemodelsasitpermits
theoperatortomakeprobability-informedchoices,andpropagatemeaningfuluncertaintytofurtheranalyses.
Overalltheproposedmethodologyhasseveraladvantagesinheuristicquantificationofuncertaintyoverensemble-based
SINDy,includingpriorchoiceovermodelsandavaliddistributionalformulation. TheBayesianformulationhasbeen
showntobeeffectiveinthreebenchmarkcasestudies,correctlyassigninghighposteriorprobabilitytotermsinthe
underlyingdifferentialequations.
Authors’contributions
MDC,Conceptualisation(Equal),Methodology(Equal),Software(Lead),DataCuration(Lead),Writing-Original
Draft(Lead),Writing-Review&Editing(Supporting)
Conflictofinterestdeclaration
Theauthorsdeclaretherearenocompetinginterestsinthiswork.
Funding
TheauthorsofthispapergratefullyacknowledgethesupportoftheUKEngineeringandPhysicalSciencesResearch
Council(EPSRC)viagrantreferencesEP/W005816/1andEP/W002140/1. Forthepurposeofopenaccess,theauthors
haveappliedaCreativeCommonsAttribution(CCBY)licencetoanyAuthorAcceptedManuscriptversionarising.
References
[1] S.L.Brunton,J.L.Proctor,andJ.N.Kutz. Discoveringgoverningequationsfromdatabysparseidentificationof
nonlineardynamicalsystems. Proceedingsofthenationalacademyofsciences,2016.
[2] J.Schmidt,N.Krämer,andP.Hennig. Aprobabilisticstatespacemodelforjointinferencefromdifferential
equationsanddata. AdvancesinNeuralInformationProcessingSystems,2021.
[3] S.H.CheungandJ.L.Beck. BayesianmodelupdatingusinghybridMonteCarlosimulationwithapplicationto
structuraldynamicmodelswithmanyuncertainparameters. Journalofengineeringmechanics,2009.
[4] A.Wills,T.B.Schön,F.Lindsten,andB.Ninness. EstimationoflinearsystemsusingaGibbssampler. IFAC
ProceedingsVolumes,2012.
[5] R.Fuentes,N.Dervilis,K.Worden,andE.J.Cross. Efficientparameteridentificationandmodelselectionin
nonlineardynamicalsystemsviasparseBayesianlearning. InJournalofPhysics: ConferenceSeries,volume
1264.IOPPublishing,2019.
[6] J. D. Longbottom, M. D. Champneys, and T. J. Rogers. Probabilistic-Numeric SMC Sampling for Bayesian
NonlinearSystemIdentificationinContinuousTime. arXivpreprintarXiv:2404.12923,2024.
[7] L.Wasserman. Bayesianmodelselectionandmodelaveraging. Journalofmathematicalpsychology,2000.
[8] A.B.Abdessalem,N.Dervilis,D.Wagg,andK.Worden. Modelselectionandparameterestimationinstructural
dynamicsusingapproximateBayesiancomputation. MechanicalSystemsandSignalProcessing,2018.
[9] A.B.Abdessalem,N.Dervilis,D.Wagg,andK.Worden. Modelselectionandparameterestimationofdynamical
systemsusinganovelvariantofapproximateBayesiancomputation. MechanicalSystemsandSignalProcessing,
2019.
17BINDy PREPRINT
[10] T.TripuraandS.Chakraborty. AsparseBayesianframeworkfordiscoveringinterpretablenonlinearstochastic
dynamicalsystemswithGaussianwhitenoise. MechanicalSystemsandSignalProcessing,2023.
[11] S.H.Rudy,S.L.Brunton,J.L.Proctor,andJ.N.Kutz. Data-drivendiscoveryofpartialdifferentialequations.
Scienceadvances,2017.
[12] K.Kaheman,J.N.Kutz,andS.L.Brunton. SINDy-PI:arobustalgorithmforparallelimplicitsparseidentification
ofnonlineardynamics. ProceedingsoftheRoyalSocietyA,2020.
[13] D.A.MessengerandD.M.Bortz. WeakSINDyforpartialdifferentialequations. JournalofComputational
Physics,2021.
[14] P.Zheng, T.Askham, S.L.Brunton, J.N.Kutz, andA.Y.Aravkin. Aunifiedframeworkforsparserelaxed
regularizedregression: SR3. IEEEAccess,2018.
[15] S.A.Billings. Nonlinearsystemidentification: NARMAXmethodsinthetime,frequency,andspatio-temporal
domains. JohnWiley&Sons,2013.
[16] U.Fasel,J.N.Kutz,B.W.Brunton,andS.L.Brunton. Ensemble-SINDy: Robustsparsemodeldiscoveryinthe
low-data,high-noiselimit,withactivelearningandcontrol. ProceedingsoftheRoyalSocietyA,2022.
[17] S.M.Hirsh,D.A.Barajas-Solano,andJ.N.Kutz. SparsifyingpriorsforBayesianuncertaintyquantificationin
modeldiscovery. RoyalSocietyOpenScience,2022.
[18] L.Fung,U.Fasel,andM.P.Juniper. RapidBayesianidentificationofsparsenonlineardynamicsfromscarceand
noisydata. arXivpreprintarXiv:2402.15357,2024.
[19] B.M.deSilva,K.Champion,M.Quade,J.-C.Loiseau,J.N.Kutz,andS.L.Brunton. Pysindy: Apythonpackage
forthesparseidentificationofnonlineardynamicsfromdata. arXivpreprintarXiv:2004.08424,2020.
[20] P. J. Green. Reversible jump Markov chain Monte Carlo computation and Bayesian model determination.
Biometrika,1995.
[21] C.J.Geyer. PracticalMarkovchainMonteCarlo. Statisticalscience,1992.
[22] P. T. Troughton and S. J. Godsill. A reversible jump sampler for autoregressive time series, employing full
conditionalstoachieveefficientmodelspacemoves. UniversityofCambridge,DepartmentofEngineering,1997.
[23] K.P.Murphy. ProbabilisticMachineLearning: Anintroduction. MITPress,2022.
[24] K.P.Murphy. ProbabilisticMachineLearning: AdvancedTopics. MITPress,2023.
[25] R. Fuentes, R. Nayek, P. Gardner, N. Dervilis, T. Rogers, K. Worden, and E. Cross. Equation discovery for
nonlineardynamicalsystems: ABayesianviewpoint. MechanicalSystemsandSignalProcessing,2021.
[26] R.Nayek,R.Fuentes,K.Worden,andE.J.Cross. Onspike-and-slabpriorsforBayesianequationdiscoveryof
nonlineardynamicalsystemsviasparselinearregression. MechanicalSystemsandSignalProcessing,2021.
[27] A.Chen,Y.Du,L.Gao,andG.Lin. Bayesiandata-drivendiscoveryofpartialdifferentialequationswithvariable
coefficients. AvailableatSSRN4747393,2021.
[28] K.S.More,T.Tripura,R.Nayek,andS.Chakraborty. ABayesianframeworkforlearninggoverningpartial
differentialequationfromdata. PhysicaD:NonlinearPhenomena,2023.
[29] J.Dahlin,F.Lindsten,T.Schön,andA.Wills. RobustARXModelswithAutomaticOrderDeterminationand
Student’stInnovations. Technicalreport,2011.
[30] D.Tiboaca,P.Green,R.Barthorpe,andK.Worden. Bayesiansystemidentificationofdynamicalsystemsusing
reversiblejumpMarkovchainMonteCarlo. InTopicsinModalAnalysisII,Volume8: Proceedingsofthe32nd
IMAC,AConferenceandExpositiononStructuralDynamics,2014.Springer,2014.
[31] N.Chopin,O.Papaspiliopoulos,etal. AnintroductiontosequentialMonteCarlo,volume4. Springer,2020.
[32] A. Jasra, A. Doucet, D. A. Stephens, and C. C. Holmes. Interacting sequential Monte Carlo samplers for
trans-dimensionalsimulation. ComputationalStatistics&DataAnalysis,2008.
18BINDy PREPRINT
[33] P. Gagnon and A. Doucet. Nonreversible jump algorithms for Bayesian nested model selection. Journal of
ComputationalandGraphicalStatistics,2020.
19