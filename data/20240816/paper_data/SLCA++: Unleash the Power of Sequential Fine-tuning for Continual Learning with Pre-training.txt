JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 1
SLCA++: Unleash the Power of Sequential Fine-tuning
for Continual Learning with Pre-training
Gengwei Zhang, Liyuan Wang, Guoliang Kang, Ling Chen, Yunchao Wei, Member, IEEE
Abstract—Inrecentyears,continuallearningwithpre-training Frozen Learnable Learnable with smaller LR
(CLPT)hasreceivedwidespreadinterest,insteadofitstraditional Parameters
100 85
focus of training from scratch. The use of strong pre-trained Select Split CIFAR-100 Split ImageNet-R
models (PTMs) can greatly facilitate knowledge transfer and 95 80 79.78 78.09
Task t Parameters 91.6991.46
a ovll ee rv fiia ttt ie nc gat oa fstr po rp e-h ti rc af io nr eg det kti nn og w,b leu dt ga els io ns touff se pr es cf ir fio cm dp or wo ng sr te rs es aiv me (a) 90 85.5685.5986.56 75 72.6675.25
tasks. A majority of current efforts often keep the PTMs frozen
Parameters
85 82.76 70 66.4968.50
and incorporate task-specific prompts to instruct representation 80 65
Task t EMA update 40 35
learning, coupled with a prompt selection process for inference.
However, due to the limited capacity of prompt parameters, Parameters 35 33.61 30 28.87
this strategy demonstrates only sub-optimal performance in (b) 30 25
c P leo T an rMt nin is nu goa ,fl t mel ne aa kpr in r noi gn vg i sd. ee qI s un eth nc e to iam g lrp e fia a nr t ei es -so tt un n, p io nt tu gen n (i t Sn ia eg l qa f Foll r T)p ra e apra r fm ues ne e dt ne atr a ms ti eo no nf
-
Task t
(c)
Parameters L D2 uaP lPr Lo Am Ep -t A Cda Opt De Ar -Pro (m dp )t Seq FT Ours-F Ou ull rs-LoRA L D2 uaP lPr Lo Am Ep -t A Cda Opt De Ar -Prompt Seq FT Ours-F Ou ull rs-LoRA
tal baseline that has been overlooked in CLPT. To this end,
we present an in-depth analysis of the progressive overfitting Fig. 1. Comparison of recent methods and our proposal. (a) Prompt-based
problem from the lens of Seq FT. Considering that the overly methods (L2P [62], DualPrompt [61], CODA-Prompt [46], etc.) often con-
fast representation learning and the biased classification layer structandselectappropriatepromptparametersforeachtaskwhilekeeping
constitute this particular problem, we introduce the advanced thebackbonefrozen.(b)LAE[15]employsamomentumcopyofadditional
parameterstostabilizetheirupdatesinCL.(c)Ourproposalisimplemented
Slow Learner with Classifier Alignment (SLCA++) framework
withthesimplestbaseline,i.e.,sequentialfine-tuning(SeqFT),unleashingits
to unleash the power of Seq FT, serving as a strong baseline
powerwithSlowLearner(SL)andClassifierAlignment(CA).(d)Evaluation
approachforCLPT.OurapproachinvolvesaSlowLearner(SL)
of continual learning performance on Split CIFAR-100 with ImageNet-21K
to selectively reduce the learning rate of backbone parameters, supervisedpre-training.
andaClassifierAlignment(CA)toalignthedisjointclassification
layers in a post-hoc fashion. We further enhance the efficacy of
SLwithasymmetriccross-entropyloss(SCE),aswellasemploya
parameter-efficientstrategytoimplementSeqFTwithSLCA++. knowledge transfer and robustness to catastrophic forgetting
Across a variety of continual learning scenarios, including class- for continual learning of specific downstream tasks [58],
incremental learning on general datasets like CIFAR-100 and
which tend to be more significant as the scale of pre-training
ImageNet-R, fine-grained datasets like CUB-200 and Cars-196,
increases [37], [41]. Therefore, continual learning with pre-
and domain-incremental learning on DomainNet, our approach
provides substantial improvements and outperforms state-of- training (CLPT) turns out to be an emerging direction and
the-art methods by a large margin. Our code is available at: receives growing attention.
https://github.com/GengDavid/SLCA.
CLPT poses a particular challenge that the pre-trained
Index Terms—Continual Learning, Pre-trained Models, Fine- knowledge should be adapted to each incremental task while
tuning, Catastrophic Forgetting
maintaining generalizability for future tasks. In this regard,
recentprompt-basedmethods[46],[61],[62]proposetofreeze
I. INTRODUCTION the pre-trained backbone (i.e., the pre-trained knowledge car-
riedtherein)andintroduceafewpromptparameterstoinstruct
The purpose of continual learning (CL) is to learn contents
representation learning (see Fig 1), which often involve con-
that appear in sequence as if they were observed simulta-
struction and selection of appropriate prompt parameters for
neously. Previous efforts are mainly based on the premise
specific tasks. Additionally, a concurrent work [15] devises
of learning from scratch, attempting to mitigate catastrophic
a unified CLPT framework of parameter-efficient fine-tuning
forgetting [36] of previously-learned knowledge when ac-
(PEFT) techniques, such as prompt [33], LoRA [25], and
quiring new information. However, the success of large-
adapter [23], which employs the exponential moving average
scale pre-training has revolutionized the learning paradigm of
(EMA) of additional parameters to stabilize their updates.
deep neural networks. The use of pre-training brings positive
Despite some promising results, these methods remain clearly
sub-optimal in CL of specific tasks, mainly due to the limited
Gengwei Zhang, Ling Chen are with Australian Artificial Intelli- capacity of the additional parameters [52], [53]. In fact,
gence Institute, University of Technology Sydney, Sydney, NSW, Aus-
PTMs tend to have stronger adaptability as more parameters
tralia; Liyuan Wang is with Tsinghua University, Beijing, China; Guo-
liang Kang is with Beihang University, Beijing, China; Yunchao Wei can be adjusted, where tuning all parameters often provides
is with Institute of Information Science, Beijing Jiaotong University, the largest potential for representation learning. As a result,
Beijing, China (email: {zgwdavid, kgl.prml, wychao1987}@gmail.com;
sequential fine-tuning (Seq FT) may serve as a fundamental
wly19@mail.tsinghua.org.cn; ling.chen@uts.edu.au). Gengwei Zhang and
LiyuanWangareco-firstauthors.Correspondingauthors:YunchaoWei. baseline overlooked in CLPT, although it typically represents
4202
guA
51
]VC.sc[
1v59280.8042:viXra
)%(
ccA-tsaL
)%(
ccA-tsaLJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 2
theworstcaseofcontinuallearningfromscratch.Accordingly, performance, collectively referred to as SLCA++. As a result,
we retreat to a simple yet important question: What is the it reduces the learnable parameters from 85.80M to 0.64M,
sufficient inductive bias to make Seq FT robust in CLPT? with comparable or even better performance than before (e.g.,
To this end, we perform an in-depth analysis of CLPT in improving the performance on Split Cars-196 from 67.73%
terms of Seq FT. First, for a range of representative continual to 73.97%). In the experimental section, we include exten-
learning methods based on Seq FT, using a maximum stable sive explorations of parameter-efficient fine-tuning for SLCA,
learningrate[47]forallparametersleadstoextremelyinferior evaluate domain-incremental learning, as well as compare
performance. This problem is largely due to the progressive our method with more recent baselines, so as to justify the
overfitting of pre-trained knowledge into specific downstream generality of SLCA++.
tasks, thus gradually losing its generalizability and stability. Ourcontributionsincludefouraspects:(1)Wepresentanin-
We further validate this insight from the perspective of gradi- depth analysis of CLPT, and demonstrate that the progressive
ents and arouse our solution. We observe that this problem overfitting problem is the key challenge for representative
within the representation layer (i.e., the backbone) can be continual learning methods based on Seq FT. A principle
almostavoidedbyselectivelyreducingthelearningrate,which explanation from the gradient perspective based on the pro-
issufficienttobalancepre-trainedknowledgeandtask-specific gressive overfitting problem is included. (2) We propose a
knowledge. On the basis of a desirable representation layer, simple but effective approach named SLCA++ to unleash
weidentifythattheclassifier(i.e.,theoutputlayeringeneral) the power of sequential fine-tuning in CLPT, which clearly
suffers from remarkable bias between tasks or classes, which outperforms state-of-the-art competitors, serving as a strong
require further alignment after continual learning. In this baselinetore-evaluatethecurrentprogressandtechnicalroute.
regard, we propose Slow Learner with Classifier Alignment (3)Wedevisetwostrategiestofurtherimprovetheefficacyand
(SLCA++), a strong baseline approach for CLPT. The former efficiency of this strong baseline, making it more applicable
refers to the selectively reduced learning rate, while the latter tocontinuallearningscenarios.(4)Ourresultsfurtheridentify
employs class-wise distributions to rectify the classifier in a critical factors and promising directions for CLPT, such as
post-hoc fashion. To improve both efficacy and efficiency, we pre-training paradigm and downstream granularity, so as to
incorporates a SCE loss and a hybrid parameter-efficient fine- facilitate subsequent research.
tuningstrategycalledHybridSlowLearner(Hybrid-SL).SCE
furthur enhances the effect of slow learner while Hybrid-SL
II. RELATEDWORK
utilizes slow learner to tune all parameters by incorporating
the idea of low-rank adaptation. Continual Learning. Traditional works on continual learn-
Across a variety of continual learning benchmarks under ing mainly focus on sequential training of deep neural net-
supervisedandself-supervisedpre-training,ourapproachpro- work(s) from scratch, ensuring effective learning of new tasks
vides substantial improvements in CLPT, and significantly without catastrophic forgetting of old tasks. Representative
fills the gap of current progress from the joint training per- strategiesincluderegularization-basedmethods[2],[12],[28],
formance. Specifically, our approach consistently improves [34], [55], [69], which preserve the old model and selectively
the regular Seq FT by more than 45% on Split CIFAR- stabilize changes in parameters or predictions; replay-based
100, Split ImageNet-R, Split CUB-200, and Split Cars-196, methods [5], [40], [51], [54], [56], [66], which approxi-
thus outperforming the SOTA methods by a large margin. On mate and recover the previously-learned data distributions;
Split CIFAR-100 and Split ImageNet-R, the performance gap architecture-based methods [44], [45], [57], [67], which allo-
is shorten to less than 2% for supervised pre-training and catededicatedparametersub-spacesforeachtask.Inaddition,
less than 4% for self-supervised pre-training. In particular, currentadvancesincontinuallearningconcentrateonthefield
we find that other competitors suffer from remarkable per- ofcomputervisionandgraduallyextendtothefieldofnatural
formance drop under the more realistic self-supervised pre- language processing [58].
training and more challenging fine-grained datasets, which Continual Learning with Pre-training. In recent years,
makes our approach more advantageous and also points the thebenefitsofpre-trainingforcontinuallearninghavebeenin-
way to subsequent work on CLPT. creasinglyexplored.Forexample,therepresentationsobtained
Note that this work is built on a preliminary version fromsupervisedpre-traininghavebeenshowntofacilitatenot
presented at ICCV 2023 [70], which introduced the idea of onlyknowledgetransferbutalsorobustnesstocatastrophicfor-
Slow Learner with Classifier Alignment (SLCA) and revealed gettingincontinuallearningofspecificdownstreamtasks[37],
thepotentialofSeqFTforallparametersinCLPT.Thecurrent [41],[74].Also,learningalargenumberofbaseclassesinthe
version improves upon the previous one with more in-depth initial training phase allows the model to learn new classes
analysis, methodological enhancements and more extensive with only minor adaptations [65]. Inspired by the techniques
experiments. First, we provide a more in-depth analysis for of knowledge transfer in natural language processing, L2P
CLPT, and explain its particular challenge from a gradi- [62] employs an additional set of learnable parameters called
ent perspective. This explanation further reveals the intrinsic “prompts” that dynamically instruct the representation layer
mechanismsofSLCAandhelpstounderstanditsimplications for learning incremental tasks. DualPrompt [61] extends this
forCLPT.Onthemethodologyaspect,wedeviseaparameter- ideabyattachingcomplementarypromptstotherepresentation
efficient fine-tuning technique to implement SLCA, named layer for learning task-invariant and task-specific instructions.
as Hybrid Slow Learner, and the SCE loss to improve the Both L2P and DualPrompt require a prompt selection phaseJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 3
beforeadaptation.CODA-Prompt[46]improvestheutilization the class-incremental setting of continual learning [49]. In
of prompts by an attention operation instead of the hard details, D = (cid:83) {(x ,y )}Nc introduces a set of
selection. Recently, LAE [15] introduces a unified framework new classet s C , wc∈ hC ert e Nc,n dec n, on tesn= th1 e number of training
t c
that includes different forms of parameter sets such as LoRA samples (x ,y ) for class c, and all the classes ever seen
c,n c,n
[25],adapter[23]besidesprompt.Thesemethodsarereported are evaluated without task labels. Besides, in the experiments,
tobefarsuperiortorepresentativecontinuallearningmethods we also evaluate our approach under the domain-incremental
based on sequential training (refer to as Seq FT in the context setting [59], where D = {(x ,y )}Nt and N denotes the
t n n n=1 t
of continaul learning with pre-training), which potentially number of training samples belonging to domain t. Each D
t
challenges the current paradigm of using pre-trained knowl- introduce training samples from new domains, while the set
edge in computer vision. ofclassesCisdeterminedatthefirsttaskandthenkeepfixed
Parameter-efficient Fine-tuning. The pre-training then for all domains.
fine-tuning is the most adopted paradigm in transfer learning, ProgressiveOverfitting.ToachievetheobjectiveofCLPT,
where full fine-tuning guarantees a powerful performance. the network needs to (1) effectively transfer the pre-trained
As the growth of pre-trained model size, parameter-efficient knowledge to each incremental task while maintaining its
fine-tuning is introduced for improving efficiency. Partial tun- generalizability for future tasks, and (2) properly balance the
ing [20] is a straightforward way that freezes most backbone learning plasticity of new tasks with memory stability of old
parts and only updates a small portion of parameters. Another tasks, also knwon as overcoming the catastrophic forgetting
idea is to keep the whole backbone fixed and introduces problem [36]. The most straightforward baseline for CLPT
extra parameters like side-tuning [71], adapter [23], [42] and is to train the model M sequentially on each D , with
θ t
low-rank adaptation [25]. Recently, prompt-tuning [33] is f and h updated at a similar speed (i.e., using a
θrps θcls
introduced in language processing for fast adaptation and also larger learning rate for fast convergence). However, due to
introduces to computer vision [27]. However, visual prompt the lack of D and D , the performance is severely
pt 1:t−1
tuning [27] is find to struggle with adapting self-supervised restrictedbyaprogressiveoverfittingprobleminbothaspects.
pre-training. Specifically,theknowledgeofD islargelyinterferedbyD ,
pt t
Self-supervised Pre-training. Since the large amount of as θ is continually updated to accommodate incremental
rps
training samples required to construct strong PTMs are typ- tasks while its generalizability obtained from the pre-training
ically unlabeled and may also arrive incrementally, self- stage is progressively lost. Besides, the knowledge of D
1:t−1
supervised pre-training emerges as a more preferable choice isinterferedbyD ,asθ andθ catastrophicallyforgetthe
t cls rps
than supervised pre-training. Several recent studies discover old tasks when learning new tasks.
that continual learning in a self-supervised manner suffers To make a clear illustration, we follow the analysis of
from less catastrophic forgetting [14], [24], [35]. Indeed, self- stability gap in literature [9] and express the progressive
supervisedparadigmshavebeenshowntobebetteradaptedto overfitting problem from a gradient perspective:
upstream continual learning, i.e., continual learning of gener-
∇M =∇M +∇M +∇M , (1)
alized representations [8]. However, the effectiveness of self- plas stab gen
supervised pre-training for downstream continual learning, where ∇M , ∇M and ∇M represent three gradient
plas stab gen
i.e., continual learning based on a self-supervised pre-trained components for updating the model M in continual learning.
θ
model, remains to be investigated. Specifically, ∇M denotes the plasticity gradient, which
plas
aims to minimize the classification error on the current task
III. CONTINUALLEARNINGWITHPRE-TRAINING t. ∇M and ∇M represent the stability and general-
stab gen
In this section, we introduce the problem formulation of izability gradients for maintaining stability of old tasks and
continual learning with pre-training (CLPT) and perform an generalizability of pre-trained knowledge, respectively. Below
in-depth analysis of its particular challenge. We then present we will discuss their respective impacts on the challenges of
our approach based on the analysis. continual learning.
A. Problem Formulation B. Slow Learner is (Almost) All You Need?
CLPT Setup. Let’s consider a neural network M (·) = For continual learning from scratch, sequential fine-tuning
θ
h (f (·)) with parameters θ = {θ ,θ } for classifi- (Seq FT) represents the worst-case performance in general.
θcls θrps rps cls
cation tasks, which often consists of a representation layer This is because ∥∇M ∥ = 0 and ∥∇M ∥ is often
gen stab
f (·) that projects input images to feature representations, close to 0, and ∇M dominates the training gradients and
θrps plas
and a classification layer h (·) that projects feature rep- resultsincatastrophicforgetting.WhenM ispre-trainedwith
θcls θ
resentations to output predictions. θ is initialized on a D , the large-scale nature of D implicitly provides M a
rps pt pt θ
pre-training dataset D in a supervised or self-supervised large ∇M , making it success when fine-tuning on a wide
pt gen
manner (class labels are not necessary for the latter). Then, range of separate downstream tasks. Therefore, a question
M needs to learn a sequence of incremental tasks from is natually raised: whether the pre-trained weights can also
θ
their training sets D ,t = 1,...,T and tries to perform well implicitlysupplyusaproper∇M likeitdoesinproviding
t stab
on their test sets. Following previous efforts of CLPT in ∇M ? In previous efforts on CLPT [61], [62], the answer
gen
the field of computer vision [61], [62], we mainly focus on is possibly “No” since Seq FT still performs poorly in theirJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 4
Fig.3. Comparisonofpre-trainingparadigmsonImageNet-1K.DeiT[48]is
astrongsupervisedmethodfor(pre-)trainingvisiontransformer,whileMoCo
v3 [7], MAE [20] and BEiT [4] are representative self-supervised methods.
Fig.2. SlowLearner(SL)cangreatlyenhancetheperformanceofsequential Thepre-trainedcheckpointsareobtainedfromtheirofficialrelease.
fine-tuning(SeqFT)inCLPT.HereweadoptImageNet-21Ksupervisedpre-
trainingforallbaselineswithdefaultperformancereferencedfromprevious
efforts [61], [62], including prompt-based methods (L2P [62] and Dual-
It reveals that the use of pre-training indeed implicitly intro-
Prompt [61]), regularization-based methods (EWC [28] and LwF [34]), and
replay-basedmethods(GDumb[40],DER++[5]andBiC[66]). duces ∇M and ∇M to regularize modifications of the
stab gen
network, but was obscured and thus overlooked by previous
efforts. It is noteworthy that although the use of different
implementation. By careful dissecting, we find that they use
learning rates for different network layers has been explored
a relatively large learning rate (e.g., 0.005, which can make
in transfer learning [16], [21], [72], its specific benefits for
joint-trainingconvergeswellonalldatasets)forbothθ and
rps continual learning has not been discovered yet. In particular,
θ . We conjecture that ∇M is obscured in this case by a
cls stab while the upper bound performance (i.e., joint training) is
large∥∇M ∥,whichisindependentto∇M and∇M
plas stab gen similar or marginally improved by the SL, the performance
and can be explicitly modified by adjusting the learning rate.
gap between continual learning and joint training is greatly
Surprisingly,whenusingamuchsmallerlearningrate(0.0001)
filled (e.g., only 4.36% on Split CIFAR-100 and 7.80% on
for θ and a slightly larger learning rate (0.01) for θ , the
rps cls Split ImageNet-R for Seq FT w/ SL).
sequential fine-tuning (Seq FT) baseline is greatly enhanced.
Besides, we also apply SL to several representative con-
As shown in Fig. 21, Seq FT is improved by more than 40%
tinual learning methods that explicitly introduce ∇M by
for challenging continual learning benchmarks such as Split stab
either regularization loss [28], [34] or data replay [5], [40],
CIFAR-100 and Split ImageNet-R, respectively. Besides, we
[66] beyond Seq FT. As shown in Fig. 2, combining SL
find that such a simple change also makes Seq FT clearly
with these methods also achieves substantial improvements,
outperformtherecentprompt-basedmethodssuchasL2P[62]
demonstrating that the benefits of ∇M provided by pre-
andDualPrompt[61].Theseprompt-basedmethods[46],[61], stab
training remain significant even with anti-forgetting tech-
[62] fixes the representation layer and employes an additional
niques. On the other hand, these methods are based on the
set of learnable parameters θ to instruct the pre-trained
add assumption of training from scratch, and their performance
model.Fromthegradientoptimizationperspective,althoughit
is only comparable to Seq FT w/ SL. The results indicate
makes ∥∇M ∥ ≈ ∞ for the fixed representation layer, the
gen that ∇M provided by pre-training is already sufficient for
newly added parameters θ would suffer from even severe stab
add maintainingstability,andintroducingadditional∇M from
progressive overfitting problem with random initialization. stab
oldtaskswouldcauseunnecessarysuppressionof∇M and
We call the simple but remarkably effective strategy “Slow gen
∇M , thereby limiting the final performance.
Learner (SL)”, corresponding to slowing down the updating plas
Effect of Pre-training Paradigm. Given the effectiveness
speed, i.e., reducing ∥∇M ∥ of the representation layer.
plas ofSLinsupervisedpre-training,wearecuriousaboutwhether
this useful property also generalize well to self-supervised
1Amoreextensiveanalysisoftheimpactoflearningratesispresentedin
AppendixB. pre-training, which avoids any explicit labels during the pre-JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 5
of 100 classes in CIFAR-100 dataset). The performance of
theselinearprobesispresentedinFig.5,whichtendstogrow
with learning more tasks, indicating that the representation
layer is accumulating knowledge for better adaptation. After
learning all incremental tasks, it can be clearly seen that
using the continually learned representation layer to jointly
train an extra classifier for all classes can almost reach
the joint training performance of the entire model, and far
outperformitscounterpartwithacontinuallylearnedclassifier
(i.e.,SeqFTw/SLinFig.5).Therefore,theproposedSLcan
almost address the problem of the representation layer, yet
the classification layer remains sub-optimal. In particular, the
problem of classification layer becomes more severe for fine-
grainedcontinuallearningbenchmarkssuchasSplitCUB-200
andSplitCars-196.Thisphenomenonismainlycausedbythe
Fig.4. Similarityofthepre-trainedrepresentations(1)beforeandafterjoint
training(leftY-axis,yellowdot),and(2)afterjointtrainingandaftercontinual disjointoptimizationoftheclassifierh t ineachtask,i.e.,only
learning(rightY-axis,column).WeadoptCenteredKernelAlignment(CKA) the parameters θ
cls,t
∈ Rd×Ct (C
t
is the number of classes
[29]asthesimilaritymetric.Bestviewedincolor. in C ) that corresponds to the classes of the current task are
t
trained together with the representation layer f .
θrep
trainingphase.Consideringarchitecturalconsistencywithpre-
vious efforts of CLPT [61], [62], we select representative C. Slow Learner with Classifier Alignment
self-supervised methods (i.e., MoCo v3 [7], MAE [20] and
To further improve the classification layer, we propose to
BEiT [4]) that release checkpoints on ViT-B/16 in our exper-
save statistics of each class during the continual learning
iments. We also compare with DeiT [48], a strong supervised progress and then align all classifiers in a post-hoc fashion
method for (pre-)training vision transformer on ImageNet-1K
(see Fig. 6 and Algorithm 1), called Classifier Alignment
dataset.AsshowninFig.3,self-supervisedpre-training,while
(CA). Specifically, after learning each task, we collect feature
more realistic regarding labeling requirements and upstream representations F = {r ,...,r } for each class c ∈ C
continual learning, typically results in a larger performance within the task,
wc
here
rc,1
=
fc,Nc
(x ) and N denotes
itst
gap between Seq FT and joint training than supervised pre- amount.Insteadofsavingc,n theexθ tr raps ctedc,n featuresFc
oftraining
c
training. Similarly, the use of SL can effectively reduce this samples, CA preserves their mean µ ∈ Rd and covariance
c
gap, suggesting that self-supervised pre-training also provides Σ ∈Rd×d for each class c (d denotes the feature dimension)
c
∇M similarly as supervised pre-training.
stab for memory efficiency.
Interestingly, the performance of Seq FT w/ SL for MoCo Wheneverthemodelneedstobeevaluated,theclassification
v3[7]farexceedsthatofthemorerecentMAE[20],although layers are further aligned as follows. Given the preserved
their joint training performance is comparable. Meanwhile, mean µ and covariance Σ for each classes, we model the
c c
the use of SL allows MoCo v3 [7] to learn representation feature distribution as a Gaussian N(µ ,Σ ), since the use
c c
much closer to that of the joint training (Fig. 4, right Y- of pre-training provides well-distributed representations and
axis). In contrast, the continually learned representation with eachclasstendstobesingle-peaked.Consideringthepossible
MAEpre-trainingareneitherclosetothatofthejointtraining semantic drift [68] problem, where the feature distribution
nor to the initial pre-training. From the gradient perspective, after learning old tasks may not reflect the feature distribution
a larger ∥∇M plas∥ is required for the MAE pre-training, at inference time, we introduce a slight modification to the
while the overall strength ∥∇M stab∥ provided is very limited, featuremeanµ
c
inCA.Specifically,accordingtotheresearch
makingthechangeofrepresentationbehavesimilarlytothatof in open-set recognition [6], [11], optimizing softmax cross-
trainingfromscratchincontinuallearning.Inshort,theabove entropy loss within a finite category space would make un-
analysis suggests a new direction for designing the paradigms known samples to have lower feature magnitude. In continual
ofself-supervisedpre-training,i.e.,howtobenefitdownstream learning, training on classes of current task can lead to
continual learning and combine the advantages of SL. decrease of feature magnitude of old classes. Therefore, we
Evaluation of Representation. Moreover, how does SL scale down each feature mean µˆ = λ µ based on the
c t c
contribute to the representation learning over incremental learningprogresswithscalingfactorλ = 1 ,whereη
t 1+η∗(T−t)
tasks? What accounts for the remaining performance gap? controls the degree of scaling magnitude, which is set to 0.02
To answer these questions, here we perform a linear probing in all experiments, thus λ is dynamically determined by the
t
experiment [20] to evaluate the performance of the represen- incremental progress, and t is the the task identity that class
tation layer. Specifically, after learning each incremental task c belongs to.
(e.g., 10 classes per task for 10 tasks in Split CIFAR-100) via Next, we sample generated features Fˆ = {rˆ ,...,rˆ }
c c,1 c,Sc
Seq FT w/ SL, we fix the representation layer and employ fromthedistributionN(µˆ ,Σ )ofeachclassc∈C ,where
c c 1:T
an extra classification layer, called a linear probe, to learn all S istheamountofgeneratedfeaturesforeachclass(S isset
c c
classes of the corresponding benchmark dataset (e.g., a total to256inallexperiments),andthenumberoftaskseverseenTJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 6
Fig. 5. Linear probing results of the Slow Learner. All experiments are based on ImageNet-21K supervised pre-training. We report the averaged accuracy
ofallclassesinthecorrespondingbenchmarkdataset(e.g.,atotalof100classesinCIFAR-100dataset).Thedarkredarrowrepresentstheperformancegap
causedbyasub-optimalclassificationlayer.
Algorithm 1 Slow Learner with Classifier Alignment (SLCA++)
Classifier Alignment
Inputs: Pre-training dataset D ; training dataset D for task
Features Sampling pt t
Initialization
Pre-training Smaller
Repre Ls ae yn et ra tion t=1,...,T; network M θ(·)=h θcls(f θrps(·)) with parameters
θrps Le Ra arn teing θrps / {θrps, θLoRA} θ = {θ rps,θ cls}; LoRA parameters θ LoRA; learning rates α
(Full / Hybrid) for θ and β for θ (α < β); temperature hyperparameter
θw θrps
… … τ.
rps cls
LoRA Initialization
θw ={θattn_w, θmlp_w} Larger Classification Classification 1: # Initialization.
θLoRA Le Ra arn teing Layer θcls Layer θcls 2: Initialize θ rps by pre-training on D pt; initialize θ LoRA
Output Logit according to θ rep; initialize θ cls randomly.
Logits Normalization
3: if full version then
SCE Loss CE Loss
4: θ rep :=θ rep
5: else if hybrid version then
Fig.6. IllustrationoftheproposedSLCA++approachforCLPT.
6: θ rep :={θ rep,θ LoRA} with θ w not activated
7: end if
can be any positive integer without being known in advance. 8: # Sequential tasks.
The generated features Fˆ 1:T = {Fˆ 1,...,Fˆ C1:T} (C 1:T is the 9: for task t=1,...,T do
total number of classes in C 1:T) is feed to the classification 10: # different learning rates α, β for θ rep and θ cls.
layer h θcls as input, and a widely-used cross-entropy loss is 11: while not converged do
adopted to furthur optimze the classification layer. 12: Train M θ on D t with SCE loss in Eqn. 3.
However, a prolonged training of the classification layer 13: end while
canleadtoanoverconfidenceissue,whichpotentiallyimpairs 14: Collect F c ={r c,1,...,r c,Nc} for c∈C t.
generalizability to the test set(s). To overcome this issue, we 15: Save mean µ c and covariance Σ c of F c for c∈C t.
draw inspirations from out-of-distribution (OOD) detection
16: end for
[63] and normalize the magnitude of network outputs when
17: # Classifier alignment.
computing the cross-entropy. Let ˆl = h θcls(rˆ) denote the 18: Sample Fˆ c from N(µˆ c,Σ c) for c∈C 1:T.
logit (i.e., pre-softmax output) of a generated feature rˆ and
19: while not converged do
ˆl
c co
o∈
m rdp
iR
no
gnC le1 y:
n
,T
ts
∥,
:
ˆlw ∥ˆlh ==ich (cid:113)∥ˆlc
∥
(cid:80)an
·
c⃗ˆl ∈b
,
Ce
w
1:r
h
Te e- ∥rw
le
cr ∥i ∥t 2te
·
rn
∥
epa
d
rs
e en
st eoh nte
e
tssp tr
L
ho e2d -u
n
mc ot
ar
gmo nf
.
itAt uw dco
e-
2
2
20
1
2:
:
:
endC
T
wro
a
hm
in
ip leu ht θe clslo wgi it thˆl a nn od rmit as lim zea dgn loit gu id te in∥ˆl E∥ q.
n. 2.
of ˆl, and⃗ˆl represents its direction. Then we adopt a modified
cross-entropy loss with logit normalization to perform CA:
can alleviate the overconfidence issue in classifier alignment.
eˆly/(τ∥ˆl∥) In practice, we observe that τ is not sensitive and empirically
L(θ ;rˆ)=−log , (2)
cls (cid:80)
c∈C
1:T
eˆlc/(τ∥ˆl∥) find τ =0.1 to be a reasonable choice.
where ˆl denotes the y-th element of ˆl corresponding to D. SLCA with Parameter-efficient Fine-tuning (PEFT)
y
the ground-truth label y. τ is a temperature hyperparameter.
Our work reveals that tuning all parameters is clearly more
The intuition behind it is that normalizing ˆl with an input-
advantageous than tuning a few inserted short sequence. Be-
dependent constant τ∥ˆl∥ will not change the result of predic-
sides,withSLCA,atask-sharedmanneriseffectiveenoughto
tionargmax c∈C 1:T(l c),whileforcingthemagnitude∥ˆl∥before resolve the progressive overfitting problem. However, directly
softmax becomes 1, which can make the criterion only adjust
τ tuningallparametersisconsideredtobeexpensiveforalarge
the direction
⃗ˆl
[63]. Therefore, the normalization in Eqn. 2 model. Recently, low-rank adaptation (LoRA) [25] providesJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 7
an efficient way of adjusting a low-rank decomposition of is needed. Inspired by symmetric cross-entropy (SCE) [60],
parameter-intensive layers, such as attention and feed-forward originally introduced for solving noisy label problem, we add
layers in the transformer, providing an opportunity for imple- a reverse cross-entropy (RCE) loss along with the CE loss as:
menting SLCA in a parameter-efficient fashion.
Therefore, we introduce Hybrid Slow Learner, a L =αL +βL , (3)
SCE CE RCE
parameter-efficientversionthatadjustsallnetworkparameters
where the second term L = −plogq is the “reverse”
in an efficient way. In particular, first considering components RCE
version of L and two balancing factors α and β are in-
like bias, normalization layers and the class token, all these CE
cluded.Accordingtothepreviousanalysis[13],[60],SCEloss
parameters have already satisfied the nature of “parameter-
can balance the gradients between high and low confidence
efficient”.Wedirectlyfine-tunethemsequentiallyforeachtask
samples, thus benefit the optimization of continual learning.
withslowlearner.Instead,forthe“expensive”parameters,i.e.,
weightparametersθ intheattentionandfeed-forwardlayers,
w
an additional LoRA in used. Specifically, suppose we have IV. EXPERIMENT
a pre-trained weight matrix W ∈ Rd2×d1, LoRA constrains In this section, we first briefly describe the experimental
the learning of the weight with a low-rank decomposition setups, and then present the experimental results.
W + ∆W = W + BA, where A ∈ Rk×d1, B ∈ Rd2×k,
rank k ≪ min(d1,d2) and only two low-rank matrices A
A. Experimental Setups
and B are activated for updating during training. After all
learningstages,AandB canbeabsorbedintoW,makingthe Datasets. Following L2P [62] and DualPrompt [61], we
network architecture unchanged. Similar to other parameters, adopt pre-training from ImageNet-21K dataset [43], also
the injected LoRA layer is sequentially fine-tuned with Slow known as the full ImageNet [10] consisting of 14,197,122
Learner (SL) for all tasks in a task-shared manner. images with 21,841 classes. We also consider pre-training
Initialization of LoRA.Typically,thebasematrixAisini- from ImageNet-1K dataset [31], a subset of ImageNet-21K
tializedwithrandomvaluesdrawnformGaussiandistribution, introduced for the ILSVRC2012 visual recognition challenge,
while the weighting matrix B is initialized to zero to make consisting of 1000-class images.
BA = 0 at the beginning of the training. According to the Toevaluatetheperformanceofdownstreamcontinuallearn-
analysis of LoRA [25], the learning sub-space of LoRA has ing, for class-incremental setting, we consider four repre-
a stronger correlation with the space of pre-trained weight W sentative benchmark datasets and randomly split each of
compared to a random matrix. We utilize this property and them into 10 disjoint tasks: The first two follow previous
perform a singular value decomposition (SVD) on the pre- efforts [61], [62] and are relatively coarse-grained in terms
trained weight W to get W = UΣ sV⊤, where U ∈ Rd2×d2 of classification, while the last two are relatively fine-grained.
and V ∈Rd1×d1 are the left and right singular vectors of W Specifically, CIFAR-100 dataset [32] consists of 100-class
and Σ
s
∈ Rd2×d1 contains singular values of W. Although naturalimageswith500trainingsamplesperclass.ImageNet-
we can obtain the low-rank decomposition of W by keeping Rdataset[22]contains200-classimages,splitinto24,000and
top-k elements in U, Σ and V⊤, we only keep top-k rows in 6,000 images for training and testing (similar ratio for each
s
V⊤ to initialize matrix A while keeping B as zero, since our class),respectively.Notethatalthoughtheimagecategoriesof
goal is to find a proper initialization for the adaptation layer ImageNet-R are overlapped with ImageNet-21K, all images
rather than to reconstruct W. In this way, we obtain a LoRA are out-of-distribution samples for the pre-train dataset, i.e.,
layer starting at BA=0 but with a better initialization state, hard examples from ImageNet or newly collected data of
which works surprisingly well with the proposed SL. differentstyles.Itrequiresconsiderableadaptationsofthepre-
trained model, therefore serving as a challenging benchmark
for continual learning. CUB-200 dataset [50] includes 200-
E. Learning Objective
classbirdimageswitharound60imagesperclass,30ofwhich
Typically, we use standard softmax cross-entropy loss are used for training and the rest for testing. Cars-196 dataset
L =−qlogponcategoriesofthecurrenttaskforoptimize [30] includes 196 types of car images, split into 8,144 and
CE
thenetwork,whereqistheone-hotlabeldistribution,p=σ(l) 8,040 images for training and testing (similar ratio for each
is the post-softmax logits and σ(·) is the softmax function. class), respectively. We also consider the domain-incremental
However, training with cross-entropy loss is known to be settingfollowingS-Prompts[59]andevaluateourapproachon
dominant by gradients of low confidences samples [38]. In DomainNet[39],adatasetwith345categoriesfrom6different
joint fine-tuning or a single task training, it is beneficial as it domains, counting a total of more than 600,000 images.
accelerate the convergence of the network with hard samples. Evaluation Metrics. We present the average accuracy of
Incontrast,sincethenetworkissequentiallytrainedincontin- all classes after learning the last task, denoted as Last-Acc
ual learning, the dominant gradients from the low confidences (equivalent to “Avg. Acc” in [61], [62]). We also compute the
samples exacerbates the problem of progressive overfitting average accuracy of the classes ever seen after learning each
problem by suppressing ∇M and enlarging ∇M , and incremental task and then present their average after learning
stab plas
thus deteriorates the final continual learning performance. the last task, denoted as Inc-Acc.
Similartotheintuitionofslowlearner,astrategyforbalancing Implementations. Following previous efforts [61], [62],
the training speed (i.e., the magnitude of gradients here) we adopt a pre-trained ViT-B/16 backbone for all baselines.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 8
TABLEI
ExperimentalresultsforcontinuallearningonSplitCIFAR-100andSplitImageNet-R.IN21K-Sup:supervisedpre-trainingonImageNet-21K.IN1K-Self:
self-supervisedpre-trainingonImageNet-1KwithMoCov3[7].AllothercontinuallearningmethodsbasedonSeqFTarereproducedaccordingtotheir
publiccodeswiththeproposedSlowLearnerimplemented.SLCA++(Full):ourfullversionthatdirectlytuningallparametersinthebackbone.
SplitCIFAR-100 SplitImageNet-R
Type Method #Params Pre-trained
Last-Acc(%) Inc-Acc(%) Last-Acc(%) Inc-Acc(%)
Upper-Bound Joint-Training 85.80M IN21K-Sup 93.22±0.16 - 80.76±0.73 -
GDumb[40] 85.80M IN21K-Sup 81.92±0.15 89.46±0.94 24.23±0.35 43.48±0.49
DER++[5] 85.80M IN21K-Sup 84.50±1.67 91.49±0.61 67.75±0.93 78.13±1.14
Classical BiC[66] 85.80M IN21K-Sup 88.45±0.57 93.37±0.32 64.89±0.80 73.66±1.61
EWC[28] 85.80M IN21K-Sup 89.30±0.23 92.31±1.66 70.27±1.99 76.27±2.13
LwF[34] 85.80M IN21K-Sup 87.99±0.05 92.13±1.16 67.29±1.67 74.47±1.48
L2P[62] 0.46M IN21K-Sup 82.76±1.17 88.48±0.83 66.49±0.40 72.83±0.56
Param-Efficient
DualPrompt[61] 0.49M IN21K-Sup 85.56±0.33 90.33±0.33 68.50±0.52 72.59±0.24
LAE(Adapter)[15] 0.15M IN21K-Sup 85.59±0.46 89.96±0.44 72.66±0.63 78.91±0.89
CODA-Prompt[46] 3.84M IN21K+IN1K-Sup 86.56±0.77 90.61±0.36 75.25±0.56 81.26±0.76
SLCA[70] 85.80M IN21K-Sup 91.53±0.28 94.09±0.87 77.00±0.33 81.17±0.64
Ours
SLCA++(Full) 85.80M IN21K-Sup 91.69±0.15 94.47±0.72 79.78±0.16 84.31±0.73
SL++ 0.64M IN21K-Sup 89.86±0.31 92.88±0.92 76.41±0.52 82.05±0.88
SLCA++ 0.64M IN21K-Sup 91.46±0.18 94.20±0.71 78.09±0.22 82.95±0.78
Upper-Bound Joint-Training 85.80M IN1K-Self 89.11±0.06 - 72.80±0.23 -
GDumb[40] 85.80M IN1K-Self 69.72±0.20 80.95±1.19 28.24±0.58 43.64±1.05
DER++[5] 85.80M IN1K-Self 63.64±1.30 79.55±0.87 53.11±0.44 65.10±0.91
Classical BiC[66] 85.80M IN1K-Self 80.57±0.86 89.39±0.33 57.36±2.68 68.07±0.22
EWC[28] 85.80M IN1K-Self 81.62±0.34 87.56±0.97 64.50±0.36 70.37±0.41
LwF[34] 85.80M IN1K-Self 77.94±1.00 86.90±0.90 60.74±0.30 68.55±0.65
L2P[62] 0.46M IN1K-Self 68.35±0.48 79.28±0.82 51.53±0.67 63.96±0.78
Param-Efficient
DualPrompt[61] 0.49M IN1K-Self 74.29±0.64 83.36±0.61 59.31±0.77 68.38±1.06
LAE(Adapter)[15] 0.15M IN1K-Self 74.87±0.55 83.73±0.36 62.81±0.47 69.47±0.94
CODA-Prompt[46] 3.84M IN1K-Self 75.22±0.83 84.17±0.36 60.77±0.70 68.86±0.86
SLCA[70] 85.80M IN1K-Self 85.27±0.08 89.51±1.04 68.07±0.21 73.04±0.56
Ours
SLCA++(Full) 85.80M IN1K-Self 85.12±0.15 89.62±1.11 68.84±0.18 73.77±0.55
SL++ 0.64M IN1K-Self 81.83±0.29 87.64±1.09 66.63±0.95 72.93±0.96
SLCA++ 0.64M IN1K-Self 84.77±0.18 89.53±0.98 69.01±0.42 74.75±0.69
For recent works on CLPT, such as L2P [62], DualPrompt SplitCIFAR-100andSplitImageNet-RinTableI(alsoshown
[61], CODA-Prompt [46], etc., we follow their official im- inFig.2),theproposedSLcansubstantiallyenhancethefinal
plementation and employ the Adam optimizer for training. performance. With the help of Classifier Alignment (CA) and
For our approach and other continual learning methods based its Logit Normalization (LN), SLCA++ clearly outperforms
on Seq FT, an SGD optimizer is used, with the same batch state-of-the-art baselines [15], [46], [61], [62], and can almost
size of 128. Our SL adopts a learning rate of 0.0001 for the reach the joint training performance (the performance gap is
representation layer (0.001 for our hybrid version) and 0.01 lessthan2%undersupervisedpre-trainingand4%underself-
for the classification layer. supervised pre-training). For continual learning on relatively
Baselines. We adopt joint training as the upper bound fine-grained classification benchmarks, such as Split CUB-
performanceandconsidercontinuallearningbaselineswithor 200 and Split Cars-196 in Table II, SLCA++ consistently
without replaying old training samples. As for the former, a surpassesallotherbaselinesbyalargemargin,whichvalidates
memorybufferof1000imagesismaintained,andweevaluate its generality. In Table III, we also evaluate SLCA++ on Split
three representative replay-based methods such as BiC [66], DomainNet benchmark [39] for domain-incremental learning.
GDumb [40] and DER++ [5]. As for the latter, we evaluate Our SLCA++ with either full parameter tuning or parameter-
representativeregularization-basedmethodssuchasEWC[28] efficient fine-tuning achieves remarkably better performance
and LwF [34], and prompt-based methods such as L2P [62], than all other baselines.
DualPrompt [61] and CODA-Prompt [46]. We also evaluate Regarding the results of self-supervised pre-training (i.e.,
LAE[15],aconcurrentparameter-efficientfine-tuningmethod MoCov3 in our experiments), prompt-based methods per-
implemented with adapter. Note that Seq FT usually serves as form far below the joint training. This is not surprisingly
the lower bound performance of continual learning, but we as the visual prompt tuning [27] has been shown to per-
observe that simply adjusting the learning rate (i.e., using the form much worse than full parameter tuning when adapt-
proposedSL)makesitasurprisinglystrongbaselineforCLPT. ing self-supervised pre-trained models to downstream tasks.
LAE [15] with adapter show improvement over the prompt-
based method, but still underperform SLCA++, especially
B. Overall Performance
on fine-grained datasets. Under the same parameter-efficient
All continual learning methods based on Seq FT (i.e., the paradigm, our SLCA++ far exceeds other methods, while
classical baselines) in Table I and II are equipped with our showing comparable results on Split CIFAR-100 and Split
SlowLearner(SL)forfaircomparison.Forcontinuallearning ImageNet-RcomparedtoSLCA++withfullparametertuning.
onrelativelycoarse-grainedclassificationbenchmarks,suchas More interestingly, a better performance is achieved with ourJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 9
TABLEII
ExperimentalresultsforcontinuallearningonSplitCUB-200andSplitCars-196.IN21K-Sup:supervisedpre-trainingonImageNet-21K.IN1K-Self:
self-supervisedpre-trainingonImageNet-1KwithMoCov3[7].AllothercontinuallearningmethodsbasedonSeqFTarereproducedaccordingtotheir
publiccodeswiththeproposedSlowLearnerimplemented.SLCA++(Full):ourfullversionthatdirectlytuningallparametersinthebackbone.
SplitCUB-200 SplitCars-196
Method #Params Pre-trained
Last-Acc(%) Inc-Acc(%) Last-Acc(%) Inc-Acc(%)
Upper-Bound Joint-Training 85.80M IN21K-Sup 88.00±0.34 - 80.31±0.13 -
GDumb[40] 85.80M IN21K-Sup 61.80±0.77 79.76±0.18 25.20±0.84 49.48±0.74
DER++[5] 85.80M IN21K-Sup 77.42±0.71 87.61±0.09 60.41±1.76 75.04±0.57
Classical BiC[66] 85.80M IN21K-Sup 81.91±2.59 89.29±1.57 63.10±5.71 73.75±2.37
EWC[28] 85.80M IN21K-Sup 68.32±2.64 79.95±2.28 52.50±3.18 64.01±3.25
LwF[34] 85.80M IN21K-Sup 69.75±1.37 80.45±2.08 49.94±3.24 63.28±1.11
L2P[62] 0.46M IN21K-Sup 62.21±1.92 73.83±1.67 38.18±2.33 51.79±4.19
Param-Efficient
DualPrompt[61] 0.49M IN21K-Sup 66.00±0.57 77.92±0.50 40.14±2.36 56.74±1.78
LAE(Adapter)[15] 0.15M IN21K-Sup 77.48±0.94 85.83±0.68 52.47±1.46 64.08±1.01
CODA-Prompt[46] 3.84M IN21K+IN1K-Sup 72.63±0.76 80.54±0.54 44.89±0.61 58.91±0.37
SLCA[70] 85.80M IN21K-Sup 84.71±0.40 90.94±0.68 67.73±0.85 76.93±1.21
Ours
SLCA++(Full) 85.80M IN21K-Sup 86.07±0.06 91.52±0.67 72.20±0.44 79.24±0.58
SL++ 0.64M IN21K-Sup 79.80±0.67 87.51±0.74 63.68±0.29 72.85±0.66
SLCA++ 0.64M IN21K-Sup 86.59±0.29 91.63±0.72 73.97±0.22 79.46±0.80
Upper-Bound Joint-Training 85.80M IN1K-Self 79.55±0.04 - 74.52±0.09 -
GDumb[40] 85.80M IN1K-Self 45.29±0.97 66.86±0.63 20.95±0.42 45.40±0.66
DER++[5] 85.80M IN1K-Self 61.47±0.32 77.15±0.61 50.64±0.70 67.64±0.45
Classical BiC[66] 85.80M IN1K-Self 74.39±1.12 82.13±0.33 65.57±0.93 73.95±0.29
EWC[28] 85.80M IN1K-Self 61.36±1.43 72.84±2.18 53.16±1.45 63.61±1.06
LwF[34] 85.80M IN1K-Self 61.66±1.95 73.90±1.91 52.45±0.48 63.87±0.31
L2P[62] 0.46M IN1K-Self 46.11±1.09 67.27±0.93 36.29±1.37 50.47±1.02
Param-Efficient
DualPrompt[61] 0.49M IN1K-Self 48.47±1.31 68.36±1.22 36.99±1.64 51.03±1.63
LAE(Adapter)[15] 0.15M IN1K-Self 53.72±1.12 69.92±1.73 41.16±1.85 56.97±1.56
CODA-Prompt[46] 3.84M IN1K-Self 50.22±1.38 68.77±0.93 38.81±1.39 53.86±1.13
SLCA[70] 85.80M IN1K-Self 73.01±0.16 82.13±0.34 66.04±0.08 72.59±0.04
Ours
SLCA++(Full) 85.80M IN1K-Self 74.35±0.11 81.95±0.88 68.04±0.48 74.02±0.23
SL++ 0.64M IN1K-Self 65.30±1.15 75.64±1.81 58.99±0.91 68.30±0.28
SLCA++ 0.64M IN1K-Self 75.48±0.31 82.94±0.73 69.71±0.10 75.67±0.32
TABLEIII learningrateof0.005forbothrepresentationandclassification
Domain-incrementallearningonSplitDomainNet.Wefollowthesame layers, and continually learning classifier with a fixed repre-
implementationasS-iPrompts[59]forallexperiments.ImageNet-21K
sentation layer, denoted as Seq FT and (Seq FT) w/ Fixed
pre-trainingisusedforallmethods.
θ in Table IV, respectively. Although (Seq FT) w/ Fixed
rps
Method #Params Last-Acc (%) θ rps performs better than Seq FT, it is significantly inferior
EWC [28] 85.80M 47.62 to Seq FT w/ SL, indicating the necessity of updating the
LwF [34] 85.80M 49.19 representation layer while using a properly reduced learning
CaSSLe (Supervised) [14] 85.80M 55.90
rate to mitigate the progressive overfitting problem. Besides,
L2P [62] 0.46M 40.15
we can see that while the benefits of SL are significant,
S-iPrompts [59] 0.05M 50.62
the improvement is slightly limited in fine-grained datasets.
SLCA++ 0.64M 59.45
SLCA++ (Full) 85.80M 61.18 Specifically, for ImageNet-21K supervised pre-training, the
improvementsofSLare47.09%,44.85%onSplitCIFAR-100
hybridversiononfine-graineddatasetslikeSplitCUB-200and andSplitImageNet-Rbut28.05%and22.17%forSplitCUB-
Split Cars-196. 200 and Split Cars-196. As indicated by Fig. 5, a classifier
Besides,itisworthnotingthatdifferentreplay-basedmeth- alignment strategy is required.
ods (w/ SL) behave differently in CLPT. In general, BiC
EffectofClassifierAlignment(CA).WefirstapplyCA(as
[66], which adds a bias correction layer to mitigate imbalance
well as LN) on the Seq FT w/ Fixed θ baseline. Surpris-
rps
between old and new classes after the learning of representa-
ingly, it brings substantial improvements especially on fine-
tion layer using both newly added and old training samples,
grained datasets (60.44% and 28.92% on Split CUB-200 and
receives the most substantial improvements. While GDumb
SplitCars-196withIN21K-Suppre-training),demonstratingit
[40], which simply uses limited training samples storing in
strong capacity for solving the above mis-alignment problem.
thememorytotrainanewlyinitializedmodelattesttime,has
Moreover, CA+LN also works well with our SL, compensat-
difficulty in optimization and performs the worst (especially
ing improvement loss on fine-grained datasets. In details, it
on fine-grained datasets). Therefore, CLPT brings advantages
brings 2.67%, 5.20%, 19.64% and 17.99% on Split CIFAR-
but also challenges for the area of continual learning.
100, Split ImageNet-R, Split CUB-200 and Split Cars-196,
respectively over the Seq FT baseline with IN21K-Sup pre-
C. Ablation Study training. With IN1K-Self pre-training, our CA and CA+LN
Effect of Slow Learner (SL). We start from two basic exhibit a similar trend of improvement.
baselines, including sequential fine-tuning with a uniform Note that our CA is operated in a post-hoc fashion ratherJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 10
TABLEIV
AblationstudyofmajorcomponentsinSLCA++(Full).HerewepresenttheLast-Acc(%)aftercontinuallearningofalltasks.†Thereproduced
performanceofSeqFTisslightlydifferentfromthereportedonein[61],[62]duetotheuseofdifferentrandomseeds.SL:SlowLearner;LN:Logit
Normalization;CA:animplementationofClassifierAlignmentwithoutLN.SCE:SymmetricCross-Entropy.
Method Pre-trained SplitCIFAR-100 SplitImageNet-R SplitCUB-200 SplitCars-196
Baseline
SeqFT† IN21K-Sup 41.77±13.8 26.95±11.8 40.02±1.08 27.57±1.79
w/Fixedθrps IN21K-Sup 63.75±0.67 34.64±14.3 60.44±1.80 24.51±6.90
w/SL IN21K-Sup 88.86±0.83 71.80±1.45 68.07±1.09 49.74±1.25
w/Fixedθrps+CA IN21K-Sup 75.64±0.26 50.73±0.21 82.71±0.14 54.45±0.16
Ours w/Fixedθrps+CA+LN IN21K-Sup 75.62±0.21 51.83±0.34 83.65±0.18 53.43±0.09
w/SL+CA IN21K-Sup 90.70±0.52 74.41±0.51 83.20±0.19 67.90±0.53
w/SL+CA+LN IN21K-Sup 91.53±0.28 77.00±0.33 84.71±0.40 67.73±0.85
w/SL+SCE+CA+LN IN21K-Sup 91.69±0.15 79.78±0.16 86.07±0.06 72.20±0.44
Baseline
SeqFT IN1K-Self 27.99±5.16 45.84±4.19 45.35±1.38 35.96±2.04
w/Fixedθrps IN1K-Self 77.30±0.56 51.97±0.17 55.54±1.55 43.16±0.12
w/SL IN1K-Self 81.47±0.55 64.43±0.44 61.67±1.37 52.91±1.53
w/Fixedθrps+CA IN1K-Self 81.83±0.12 55.59±0.21 70.67±0.02 57.01±0.07
Ours w/Fixedθrps+CA+LN IN1K-Self 81.95±0.17 56.47±0.23 72.97±0.17 63.00±0.21
w/SL+CA IN1K-Self 84.64±0.21 67.54±0.29 72.52±0.06 64.80±0.20
w/SL+CA+LN IN1K-Self 85.27±0.08 68.07±0.21 73.01±0.16 66.04±0.08
w/SL+SCE+CA+LN IN1K-Self 85.12±0.15 68.84±0.18 74.35±0.11 68.04±0.48
TABLEV
Ablationstudyofvaryingtuningparameters.AllentriestunethebackboneinaSeqFTmanner.X-onlymeansonlyoptimizingparameterwithinallX
components.Partial-1meansonlytuningthelasttransformerblock.LoRA-xmeansusingLoRAadaptationwithrankx.Hybriddenotesourhybridslow
learner.Wechose Hybrid(LoRA-4) asourdefaultchoicewhencomparingwithotherworks.
TuningType Method #Params SplitCIFAR-100 SplitImageNet-R SplitCUB-200 SplitCars-196
Full 85.80M 91.69±0.15 79.78±0.16 86.07±0.06 72.20±0.44
Classical Mlp-only 56.67M 91.49±0.11 79.26±0.17 84.88±0.09 69.65±0.53
Attn-only 21.26M 91.61±0.09 77.88±0.21 84.66±0.16 70.21±0.49
Partial-1 7.09M 75.99±0.21 54.59±0.18 84.28±0.13 51.31±0.39
Norm-only 0.04M 89.47±0.16 73.10±0.10 84.98±0.15 64.97±0.51
bias-only 0.08M 91.12±0.11 73.95±0.23 84.81±0.28 66.84±0.44
Param-Efficient LoRA-only(LoRA-1) 0.13M 91.48±0.15 76.30±0.26 83.59±0.22 65.91±0.50
Hybrid(LoRA-1) 0.25M 91.61±0.19 76.69±0.19 85.39±0.19 69.41±0.37
Hybrid(LoRA-4) 0.64M 91.46±0.18 78.09±0.22 86.59±0.29 73.97±0.22
Hybrid(LoRA-4w/oinit.) 0.64M 90.16±0.38 77.47±0.14 85.19±0.34 65.37±0.27
thanaligningtheclassifierduringrepresentationlearning[18], providing a robust and strong PEFT-based solution for CLPT
[19], [73], as the latter would worsen the performance (e.g., through Seq FT without the need of parameter selection or
by 27.89% on Split CIFAR-100) in our experiments. incremental parameter addition.
Effect of Symmetric Cross-Entropy. With the Symmetric
Cross-Entropy (SCE) objective, we can further improve the D. Discussion
performance of our approach, especially on Split Cars-196,
Pre-training Paradigm. Constructing strong pre-trained
which is considered the most challenging benchmark in all
models typically requires large amounts of pre-training data,
experiments and has the largest domain shift compared to
while the extensive annotation is scarce and expensive, mak-
the pre-training dataset. The results are consistent with our
ing self-supervised pre-training a more realistic option than
analysis in Sec. III-E, where SCE balances the gradients of
supervised pre-training. However, most of the existing CLPT
high and low confidence samples to improve stability when
studiesonlyconsidertheuseofstrongsupervisedpre-training.
training on datasets with more hard samples.
Our initial investigation into continual learning with self-
Effect of Hybrid Slow Learner. Table V illustrates the supervised pre-training indicates that state-of-the-art methods
transformation of our approach into a parameter-efficient ver- for CLPT tend to face severe challenges with it. Given its
sion. We start with tuning different parts of the backbone practicalsignificanceandinherentdifficulty,wesuggestfuture
to transfer pre-trained knowledge for downstream continual CLPT studies to direct more effort into this avenue. On the
learning. Tuning different parts has varying effects on differ- other hand, we also suggest subsequent work to develop
ent datasets, demonstrating that there is no one-size-fits-all self-supervised pre-training methods that are better suited for
solution when only tuning a part of the backbone. Direcly downstream continual learning, and potentially use this as a
adopting advanced parameter-efficient fine-tuning techniques criterion to evaluate the progress of self-supervised learning.
likeLoRA[25]outperformsotherparameter-efficientpartson Scalability. We further discuss the scalability of our ap-
Split ImageNet-R but falls behind them on fine-grained tasks. proach. First, the generated features are only used to align the
By combining them together as Hybrid Slow Learner, we output layer at test time rather than training the entire back-
achievecomparableorevenbetterperformanceonfine-grained bone,thusthecomputationisefficientandnotaccumulatedin
datasets compared to the full parameter tuning counterpart, continuallearning(e.g.,rangingfromonly0.67%to5%oftheJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 11
totalrunningtimeforvariousbenchmarks).Second,theclass- REFERENCES
wisecovarianceΣ canbefurthersimplifiedasbyanglobalΣ
c
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, et al. Gpt-4 technical
forallclassesbymomentumupdatingΣ=γΣ +(1−γ)Σ
c−1 c report. arXivpreprintarXiv:2303.08774,2023.
where γ is the momentum value (e.g., 0.9), or the variance [2] Rahaf Aljundi, Francesca Babiloni, et al. Memory aware synapses:
σ2 ∈ Rd. When replacing Σ with σ2, it only results in a Learningwhat(not)toforget. InECCV,pages139–154,2018.
c c c [3] RohanAnil,AndrewMDai,OrhanFirat,etal. Palm2technicalreport.
tolerable performance degradation (e.g., up to only 0.61% on arXivpreprintarXiv:2305.10403,2023.
Split CIFAR-100 and 0.83% on Split ImageNet-R), while the [4] HangboBao,LiDong,andFuruWei. Beit:Bertpre-trainingofimage
storage of µ and σ2 for 100 classes corresponds to only transformers. arXivpreprintarXiv:2106.08254.
c c [5] PietroBuzzegaetal. Darkexperienceforgeneralcontinuallearning:a
0.18% parameters of the ViT-B/16 backbone, which is clearly strong,simplebaseline. InNeurIPS,volume33,2020.
[6] Guangyao Chen et al. Adversarial reciprocal points learning for open
lightweight.
setrecognition. IEEETPAMI,44(11):8065–8081,2021.
LearningRateforCL.Oneofourmostinterestingfindings [7] XinleiChenetal. Anempiricalstudyoftrainingself-supervisedvision
in this work is that, using a properly adapted learning rate transformers. InICCV,pages9640–9649,2021.
[8] Andrea Cossu et al. Continual pre-training mitigates forgetting in
has been sufficiently effective for representation learning on languageandvision. arXivpreprintarXiv:2205.09357,2022.
a continual basis, which can contribute to many relevant [9] Matthias De Lange et al. Continual evaluation for lifelong learning:
Identifyingthestabilitygap. arXivpreprintarXiv:2205.13452,2022.
topics. For example, as pre-training data is often massive
[10] JiaDengetal. Imagenet:Alarge-scalehierarchicalimagedatabase. In
and incrementally collected, the problem of continual pre- CVPR,pages248–255,2009.
training has received growing interest, especially for large [11] Akshay Raj Dhamija, Manuel Gu¨nther, and Terrance Boult. Reducing
networkagnostophobia. NeurIPS,31,2018.
language models (LLMs) [1], [3]. We note that a majority of [12] PrithvirajDhar,RajatVikramSingh,Kuan-ChuanPeng,etal. Learning
recent efforts on this topic proposed to selectively adjusting withoutmemorizing. InCVPR,pages5138–5146,2019.
[13] Mario Do¨bler et al. Robust mean teacher for continual and gradual
the learning rate for sequential training of all parameters
test-timeadaptation. InCVPR,pages7704–7714,2023.
insteadofusingspecializedregularizationstrategies[17],[26], [14] EnricoFini,VictorGTurrisidaCosta,etal.Self-supervisedmodelsare
[64], consistent with our results. This is possibly because the continuallearners. InCVPR,pages9621–9630,2022.
[15] QiankunGaoetal.Aunifiedcontinuallearningframeworkwithgeneral
distribution of pre-training data is extremely complex, which parameter-efficienttuning. InICCV,pages11483–11493,2023.
limits the application of continual learning methods that are [16] Yunhui Guo, Honghui Shi, Abhishek Kumar, et al. Spottune: transfer
learningthroughadaptivefine-tuning.InCVPR,pages4805–4814,2019.
specialydesigned.Incontrast,adjustingthelearningraterelies
[17] Kshitij Gupta et al. Continual pre-training of large language models:
on clearly simplified prior assumptions, making it work better Howtore-warmyourmodel? InICMLWorkshop,2023.
[18] Tyler L Hayes, Kushal Kafle, et al. Remind your neural network to
forcontinuallearninginthecontextofpre-training.Tothebest
preventcatastrophicforgetting. InECCV,pages466–483,2020.
of our knowledge, SLCA / SLCA++ is one of the first studies [19] TylerLHayesandChristopherKanan. Lifelongmachinelearningwith
to explicitly analyze the impact of learning rate on CLPT, deepstreaminglineardiscriminantanalysis.InCVPRWorkshops,pages
220–221,2020.
which may facilitate a series of subsequent work to develop
[20] KaimingHe,XinleiChen,SainingXie,etal. Maskedautoencodersare
more adaptive CLPT methods based on our approach. scalablevisionlearners. InCVPR,pages16000–16009,2022.
[21] KaimingHe,RossGirshick,andPiotrDolla´r. Rethinkingimagenetpre-
training. InICCV,pages4918–4927,2019.
[22] DanHendrycksetal. Themanyfacesofrobustness:Acriticalanalysis
V. CONCLUSION
ofout-of-distributiongeneralization. InICCV,pages8340–8349,2021.
[23] NeilHoulsby,AndreiGiurgiu,etal.Parameter-efficienttransferlearning
In this work, we present the advanced Slow Learner with
fornlp. InICML,pages2790–2799.PMLR,2019.
Classifier Alignment (SLCA++), a simple but effective ap- [24] DapengHuetal. Howwellself-supervisedpre-trainingperformswith
streamingdata? arXivpreprintarXiv:2104.12081,2021.
proach for the problem of continual learning with pre-training
[25] Edward J Hu, Yelong Shen, et al. Lora: Low-rank adaptation of large
(CLPT). This approach stems from our in-depth analysis of languagemodels. arXivpreprintarXiv:2106.09685,2021.
the progressive overfitting issue in CLPT, where using a [26] Adam Ibrahim, Benjamin The´rien, et al. Simple and scalable strate-
gies to continually pre-train large language models. arXiv preprint
uniformly large learning rate to update the entire model tends
arXiv:2403.08763,2024.
to compromise the benefits of both stability and generaliz- [27] Menglin Jia et al. Visual prompt tuning. In ECCV, pages 709–727.
Springer,2022.
ability from pre-training. In response, the Slow Learner (SL)
[28] James Kirkpatrick, Razvan Pascanu, et al. Overcoming catastrophic
demonstrates that using a selectively reduced learning rate forgettinginneuralnetworks. PNAS,114(13):3521–3526,2017.
for sequential fine-tuning can almost address this challenging [29] Simon Kornblith et al. Similarity of neural network representations
revisited. InICML,pages3519–3529.PMLR,2019.
issue in the representation layer, regardless of different pre- [30] Jonathan Krause et al. 3d object representations for fine-grained
training paradigms and downstream granularity. Also, the categorization. InICCVWorkshops,pages554–561,2013.
[31] AlexKrizhevskyetal. Imagenetclassificationwithdeepconvolutional
Classifier Alignment (CA) helps to resolve the problem of
neuralnetworks. InNeurIPS,volume25,pages1097–1105,2012.
sub-optimalclassificationlayerbyemployingfeaturestatistics. [32] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of
The combination of SL and CA unleash the hidden power of featuresfromtinyimages. 2009.
[33] Brian Lester et al. The power of scale for parameter-efficient prompt
sequentialfine-tuningforCLPT.Wehavefurtherimprovedthe tuning. InEMNLP,pages3045–3059,2021.
efficacyandefficiencyofourapproachwithSymmetricCross- [34] Zhizhong Li and Derek Hoiem. Learning without forgetting. IEEE
TPAMI,40(12):2935–2947,2017.
Entropy(SCE)andaparameter-efficientversioncalledHybrid
[35] Divyam Madaan et al. Rethinking the representational conti-
SlowLearner(HybridSL),respectively.Withallthesedesigns, nuity: Towards unsupervised continual learning. arXiv preprint
our approach achieves state-of-the-art results across a variety arXiv:2110.06976,2021.
[36] JamesLMcClellandetal. Whytherearecomplementarylearningsys-
ofCLPTbenchmarks.Possiblefutureworkincludesexploring temsinthehippocampusandneocortex:Insightsfromthesuccessesand
better self-supervised pre-training for downstream continual failuresofconnectionistmodelsoflearningandmemory.Psychological
Review,102(3):419,1995.
learningaswellasexploringSLCA++inmorescenarios,such
[37] Sanket Vaibhav Mehta et al. An empirical investigation of the role of
as continual learning of multi-modal and embodied tasks. pre-traininginlifelonglearning.arXivpreprintarXiv:2112.09153,2021.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 12
[38] Chaithanya Kumar Mummadi, Robin Hutmacher, et al. Test-time [68] LuYuetal.Semanticdriftcompensationforclass-incrementallearning.
adaptation to distribution shift by confidence maximization and input InCVPR,pages6982–6991,2020.
transformation. arXivpreprintarXiv:2106.14999,2021. [69] FriedemannZenke,BenPoole,andSuryaGanguli. Continuallearning
[39] Xingchao Peng et al. Moment matching for multi-source domain throughsynapticintelligence. InICML,pages3987–3995,2017.
adaptation. InICCV,2019. [70] GengweiZhang,LiyuanWang,GuoliangKang,LingChen,andYunchao
[40] Ameya Prabhu et al. Gdumb: A simple approach that questions our Wei. Slca:Slowlearnerwithclassifieralignmentforcontinuallearning
progressincontinuallearning. InECCV,pages524–540,2020. onapre-trainedmodel. InICCV,pages19148–19158,2023.
[41] VinayVenkateshRamasesh,AitorLewkowycz,andEthanDyer. Effect [71] Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and
ofscaleoncatastrophicforgettinginneuralnetworks. InICLR,2021. Jitendra Malik. Side-tuning: a baseline for network adaptation via
[42] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning additivesidenetworks. InECCV,pages698–714.Springer,2020.
multiplevisualdomainswithresidualadapters. NeurIPS,30,2017. [72] Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Weinberger, and
[43] TalRidniketal.Imagenet-21kpretrainingforthemasses.arXivpreprint Yoav Artzi. Revisiting few-sample bert fine-tuning. arXiv preprint
arXiv:2104.10972,2021. arXiv:2006.05987,2020.
[44] AndreiARusu,NeilCRabinowitz,etal. Progressiveneuralnetworks. [73] Fei Zhu, Xu-Yao Zhang, Chuang Wang, Fei Yin, and Cheng-Lin Liu.
arXivpreprintarXiv:1606.04671,2016. Prototype augmentation and self-supervision for incremental learning.
[45] JoanSerraetal. Overcomingcatastrophicforgettingwithhardattention InCVPR,pages5871–5880,2021.
tothetask. InICML,pages4548–4557,2018. [74] Hongguang Zhu, Yunchao Wei, et al. Ctp: Towards vision-language
[46] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, et al. Coda- continual pretraining via compatible momentum contrast and topology
prompt:Continualdecomposedattention-basedpromptingforrehearsal- preservation. InICCV,pages22257–22267,2023.
freecontinuallearning. InCVPR,pages11909–11919,2023.
[47] Leslie N Smith and Nicholay Topin. Super-convergence: Very fast Gengwei Zhang is currently working toward the
trainingofneuralnetworksusinglargelearningrates.InArtificialintel- Ph.D. degree at the Faculty of Engineering and
ligenceandmachinelearningformulti-domainoperationsapplications, Information Technology, University of Technology
volume11006,pages369–386.SPIE,2019. Sydney, Australia, supervised by Prof. Ling Chen
[48] Hugo Touvron, Matthieu Cord, et al. Training data-efficient image andProf.YunchaoWei.HereceivedhisBachelor’s
transformers & distillation through attention. In ICML, pages 10347– degreefromSunYat-SenUniversity,China,in2019.
10357.PMLR,2021. Hisresearchinterestsfocusonenhancingthelearn-
[49] GidoMvandeVenandAndreasSTolias.Threescenariosforcontinual ingcapabilitiesofcomputervisionmodelsusingma-
learning. arXivpreprintarXiv:1904.07734,2019. chinelearningtechniquessuchascontinuallearning,
[50] CatherineWah,SteveBranson,PeterWelinder,etal. Thecaltech-ucsd few-shotlearning,andself-supervisedlearning.
birds-200-2011dataset. 2011.
[51] LiyuanWang,BoLei,etal. Triple-memorynetworks:Abrain-inspired
LiyuanWangiscurrentlyapostdoctoralresearcher
methodforcontinuallearning. IEEETNNLS,2021.
inTsinghuaUniversity,workingwithProf.JunZhu
[52] Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang, Hang Su,
at the Department of Computer Science and Tech-
and Jun Zhu. Hierarchical decomposition of prompt-based continual
nology.Beforethat,hereceivedtheB.S.andPh.D.
learning:Rethinkingobscuredsub-optimality. NeurIPS,36,2024.
[53] LiyuanWang,JingyiXie,XingxingZhang,HangSu,andJunZhu.Hide- degrees from Tsinghua University. His research in-
pet: Continual learning via hierarchical decomposition of parameter- terestsincludecontinuallearning,incrementallearn-
efficienttuning. arXivpreprintarXiv:2407.05229,2024. ing, lifelong learning and brain-inspired AI. His
[54] Liyuan Wang, Kuo Yang, et al. Ordisco: Effective and efficient usage work in continual learning has been published in
ofincrementalunlabeleddataforsemi-supervisedcontinuallearning.In majorconferencesandjournalsinrelatedfields,such
CVPR,pages5383–5392,2021. asNatureMachineIntelligence,IEEETPAMI,IEEE
[55] LiyuanWang,MingtianZhang,ZhongfanJia,QianLi,ChenglongBao, TNNLS,NeurIPS,ICLR,CVPR,ICCV,ECCV,etc.
Kaisheng Ma, Jun Zhu, and Yi Zhong. Afec: Active forgetting of GuoliangKangiscurrentlyaProfessoratBeihang
negativetransferincontinuallearning. InNeurIPS,volume34,2021. University. He received his PhD degree from the
[56] Liyuan Wang, Xingxing Zhang, et al. Memory replay with data UniversityofTechnologySydneyin2019.Heused
compressionforcontinuallearning. InICLR,2021. tobeaPostdoctoralResearchAssociateatCarnegie
[57] LiyuanWang,XingxingZhang,QianLi,JunZhu,andYiZhong.Coscl:
Mellon University and the University of Texas,
Cooperation of small continual learners is stronger than a big one. In
Austin.Hisresearchinterestsincludedeeplearning,
ECCV,pages254–271.Springer,2022.
computervision,transferlearning,etc.
[58] LiyuanWang,XingxingZhang,HangSu,andJunZhu. Acomprehen-
sivesurveyofcontinuallearning:Theory,methodandapplication.arXiv
preprintarXiv:2302.00487,2023.
[59] Yabin Wang, Zhiwu Huang, and Xiaopeng Hong. S-prompts learning
withpre-trainedtransformers:Anoccam’srazorfordomainincremental LingChenisaProfessorwiththeAustralianArtifi-
learning. arXivpreprintarXiv:2207.12819,2022. cialIntelligenceInstitute(AAII),UniversityofTech-
[60] YisenWang,XingjunMa,ZaiyiChen,YuanLuo,JinfengYi,andJames nology Sydney. She received PhD from Nanyang
Bailey. Symmetriccrossentropyforrobustlearningwithnoisylabels. Technological University, Singapore. Her research
InICCV,pages322–330,2019. areaismachinelearninganddatamining.Herrecent
[61] Zifeng Wang et al. Dualprompt: Complementary prompting for researchfocusesondevelopingmoralalignedagents
rehearsal-free continual learning. arXiv preprint arXiv:2204.04799, for text-based games, improving the robustness of
2022. dialoguesystems,andanomalydetectionfromgraph
[62] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, et al. Learning to prompt structured data. Her papers appear in major jour-
forcontinuallearning. InCVPR,pages139–149,2022. nalsandconferencesincludingIEEETPAMI,IEEE
[63] HongxinWei,RenchunziXie,HaoCheng,LeiFeng,BoAn,andYixuan
TNNLS, NeurIPS and ICLR. She is an editorial
Li. Mitigatingneuralnetworkoverconfidencewithlogitnormalization.
member of the Elsevier Journal of Data and Knowledge Engineering, the
arXivpreprintarXiv:2205.09310,2022.
Springer Journal of Data Science and Analytics, and the IEEE Journal of
[64] Genta Winata, Lingjue Xie, Karthik Radhakrishnan, Shijie Wu, Xisen
SocialComputing.
Jin, Pengxiang Cheng, Mayank Kulkarni, and Daniel Preo¸tiuc-Pietro.
Overcomingcatastrophicforgettinginmassivelymultilingualcontinual YunchaoWeiiscurrentlyafullprofessoratBeijing
learning. InACL2023Findings,pages768–777,2023. JiaotongUniversity.Previously,heheldpositionsat
[65] Tz-Ying Wu, Gurumurthy Swaminathan, Zhizhong Li, et al. Class- theNationalUniversityofSingapore,theUniversity
incremental learning with strong pre-trained models. In CVPR, pages ofIllinoisatUrbana-Champaign,andtheUniversity
9601–9610,2022. of Technology Sydney. He has published over 100
[66] YueWu,YinpengChen,LijuanWang,YuanchengYe,ZichengLiu,etal. papers in top-tier conferences and journals, with
Largescaleincrementallearning. InCVPR,pages374–382,2019. more than 22,000 Google Scholar citations. His
[67] Binbin Yang, Xinchi Deng, Han Shi, Changlin Li, Gengwei Zhang, researchinterestsincludevisualrecognitionwithim-
HangXu,ShenZhao,LiangLin,andXiaodanLiang. Continualobject perfectdata,multi-modalperception,andgenerative
detectionviaprototypicaltaskcorrelationguidedgatingmechanism. In AI.
CVPR,pages9255–9264,2022.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 13
TABLEVI
Continuallearningperformancewithdifferentlearningratesoftherepresentationlayer.HerewepresenttheLast-Acc(%)aftercontinuallearningofall
classes.IN21K-Sup:supervisedpre-trainingonImageNet-21K.IN1K-Self:self-supervisedpre-trainingonImageNet-1KwithMoCov3[7].Thecolumn
labeledby† usesthesamelearningrateof0.005fortheentiremodel,whiletheothersusealearningrateof0.01fortheclassificationlayer.
Benchmark Pre-trained 0.005† 0.001 0.0001 0.00001 0.000001 Fixedθrps
SplitCIFAR-100 IN21K-Sup 44.77±13.8 83.04±1.46 88.86±0.83 88.81±0.46 85.11±0.42 63.75±0.67
SplitImageNet-R IN21K-Sup 26.95±11.8 70.38±0.80 71.80±1.45 62.64±2.35 53.57±4.33 34.64±14.3
SplitCUB-200 IN21K-Sup 40.02±1.08 60.02±1.24 68.07±1.09 66.58±3.93 64.38±3.36 60.44±1.80
SplitCars-196 IN21K-Sup 27.57±1.79 15.74±26.3 49.74±1.25 30.66±9.01 24.85±7.90 24.51±6.90
SplitCIFAR-100 IN1K-Self 27.99±5.16 81.49±0.75 81.47±0.55 81.57±0.14 78.61±0.29 77.30±0.56
SplitImageNet-R IN1K-Self 45.84±4.19 68.72±0.48 64.43±0.44 59.19±0.33 54.54±0.32 51.97±0.17
SplitCUB-200 IN1K-Self 45.35±1.38 68.58±1.16 61.67±1.37 56.46±1.86 55.10±2.13 55.54±1.55
SplitCars-196 IN1K-Self 35.96±2.04 58.39±2.31 52.91±1.61 43.64±0.73 41.74±0.23 43.16±0.12
Fig.7. CKAsimilarityofpre-trainedrepresentationsbeforeandafterlearning Fig.8. CKAsimilarityofpre-trainedrepresentationsafterjointtrainingand
downstreamtasks. aftercontinuallearning.
For domain-incremental setting on DomainNet benchmark,
APPENDIXA
we do not use classifier alignment since the class set is
IMPLEMENTATIONDETAILS
fixed. We adopt the same slow learner strategy used in
For class-incremental setting, all baselines follow an im- class-incremental setting, and the final results is obtained by
plementation similar to the one described in [61], [62]. averaging the classifier outputs of different domains of the
Specifically, a pre-trained ViT-B/16 backbone is adopt for same class.
all methods. Adam optimizer is used for prompting-based
methods [46], [61], [62] as well as for LAE [15]. An SGD
APPENDIXB
optimizerisutilizedforotherbaselinesandours,withthesame
EXTENDEDANALYSIS
batch size of 128. The original implementation of [61], [62]
adoptsaconstantlearningrateof0.005forallbaselines,while In this section, we provide extended ananlysis to support
our slow learner using 0.0001 for our full version and 0.001 the main claims in our paper. First, In Tab. VI, we provide
for hybrid version to update representation layer, and 0.01 for an analysis of the impact of learning rates on CLPT using
theclassificationlayer.Inpractice,weobservethatsupervised the ImageNet-21K supervised pre-training. We report Last-
pre-training usually converges faster than self-supervised pre- Acc for varying learning rates of the representation layer
training in downstream continual learning. Therefore, for rangingfrom0to0.005.Ahigherlearningrate(0.005)forthe
supervisedpre-training,wetrainallbaselinesfor20epochson representation layer results in even poorer performance than
SplitCIFAR-100and50epochsonotherbenchmarks.Forself- a fixed representation due to progressive overfitting problem.
supervised pre-training, we train all baselines for 90 epochs On the other hand, a too small learning rate for θ can not
rep
on all benchmarks. improve learning on challenging datasets like Split Cars-196,JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 14
making it perform similar to the fixed representation. A mod- SLCA++ with Prompt. We also explore the application
erate adjustment of learning rate for the representation layer of our framework with the prompt tuning. We append pre-
effectivelybalancesM ,∇M and∇M ,resolvingthe trained prompt (pre-trained on ImageNet-1K dataset) with
gen plas stab
progressive overfitting issue and obtaining a strong continual promptlength10inasamewayasVisualPromptTuning[27].
learning performance. We adopt the hybrid way of updating parameter-efficient part
We then diagnose the behavior of using different self- within the transformer. According to [15], prompts typically
supervised pre-training methods by presenting the CKA sim- receive significantly small gradient compared to network
ilarity of representations after each task (1) compared to the parameters, which naturally satisfy the property of “slow
pre-trained representation in Fig. 7, and (2) compared to the learner”. Therefore, we use a relatively large learning rate
joint trained representation in Fig. 8. The conclusion is inline (0.02) for prompts. Compared with CODA-Prompt [46], our
with the conclusion from Fig. 4. For MoCo v3 pre-training, SLCA++ with prompt show a better performance, especially
SL maintains the representation of each task close to the on fine-grained datasets, while requires much less learnable
pre-trained representation (i.e., preserves ∇M ), gradually parameters.
gen
increases its similarity to the joint-trained representation (i.e.,
balances ∇M and ∇M ). For MAE pre-training, al-
plas stab
though ∇M is increased with SL, ∇M from the pre-
gen stab
training is inadequate, resulting in a final representation that
is neither close to the pre-trained nor the joint-trained one.
TABLEVII
AblationsforCAcombiningwithEWCandBiC.
Method CIFAR-100 ImageNet-R CUB-200 Cars-196
EWC 47.01±0.29 35.00±0.43 51.28±2.37 47.02±3.90
EWCw/SL 89.30±0.23 70.27±1.99 81.62±0.34 64.50±0.36
EWCw/SL+CA 90.61±0.17 71.48±0.31 84.29±0.37 69.61±0.29
BiC 66.11±1.76 52.14±1.08 78.69±1.97 55.03±3.27
BiCw/SL 88.45±0.57 64.89±0.80 81.91±2.59 63.10±5.71
BiCw/SL+CA 91.57±0.13 74.49±0.08 86.82±0.69 73.90±0.38
APPENDIXC
EXTENDEDABLATIONS
Combinewithothermethods.Inthemaintext,theefficacy
of SL has been widely validated by combining it with all
baseline methods. We have further validated the efficacy of
CA, presenting representative non-replay and replay methods
on IN21K-Sup as shown in Table VII.
TABLEVIII
AblationstudiesaboutfeaturestatisticsinCA.sharedcov.computes
covariancewithmomentumupdateswhilepercls.cov.savescovariance
matrixforeachclass.
Method CIFAR-100 ImageNet-R CUB-200 Cars-196
sharedcov. 91.06±0.09 77.38±0.14 85.64±0.38 71.39±0.55
percls.cov. 91.46±0.18 78.09±0.22 86.59±0.29 73.97±0.22
Influence of Feature Statistic. In our implementation of
CA,wesavethecovariancematrixΣ ofeachclasstoestimate
c
theperclassfeaturedistribution.Asdiscussedinthemaintext,
the memory efficiency can be improved by using an global Σ
forallclassesbymomentumupdating.AsshowninTab.VIII,
only a slight decrease of performance is observed with this
modification.
TABLEIX
Resultsofourmethodwithprompt.WecomparewithCODA-Prompt[46]
thatalsousesImageNet-21k+ImageNet-1kpre-training.
Method #Params CIFAR-100 ImageNet-R CUB-200 Cars-196
CODA-Prompt[46] 3.84M 86.56±0.77 75.25±0.56 72.63±0.76 44.89±0.61
Ours(Prompt) 0.22M 90.91±0.15 76.46±0.13 81.28±1.91 66.83±0.54