[
    {
        "title": "iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries",
        "authors": "Adam CosciaLangdon HolmesWesley MorrisJoon Suh ChoiScott CrossleyAlex Endert",
        "links": "http://dx.doi.org/10.1145/3640543.3645142",
        "entry_id": "http://arxiv.org/abs/2403.04760v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04760v1",
        "summary": "The recent explosion in popularity of large language models (LLMs) has\ninspired learning engineers to incorporate them into adaptive educational tools\nthat automatically score summary writing. Understanding and evaluating LLMs is\nvital before deploying them in critical learning environments, yet their\nunprecedented size and expanding number of parameters inhibits transparency and\nimpedes trust when they underperform. Through a collaborative user-centered\ndesign process with several learning engineers building and deploying summary\nscoring LLMs, we characterized fundamental design challenges and goals around\ninterpreting their models, including aggregating large text inputs, tracking\nscore provenance, and scaling LLM interpretability methods. To address their\nconcerns, we developed iScore, an interactive visual analytics tool for\nlearning engineers to upload, score, and compare multiple summaries\nsimultaneously. Tightly integrated views allow users to iteratively revise the\nlanguage in summaries, track changes in the resulting LLM scores, and visualize\nmodel weights at multiple levels of abstraction. To validate our approach, we\ndeployed iScore with three learning engineers over the course of a month. We\npresent a case study where interacting with iScore led a learning engineer to\nimprove their LLM's score accuracy by three percentage points. Finally, we\nconducted qualitative interviews with the learning engineers that revealed how\niScore enabled them to understand, evaluate, and build trust in their LLMs\nduring deployment.",
        "updated": "2024-03-07 18:56:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04760v1"
    },
    {
        "title": "KnowledgeVIS: Interpreting Language Models by Comparing Fill-in-the-Blank Prompts",
        "authors": "Adam CosciaAlex Endert",
        "links": "http://dx.doi.org/10.1109/TVCG.2023.3346713",
        "entry_id": "http://arxiv.org/abs/2403.04758v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04758v1",
        "summary": "Recent growth in the popularity of large language models has led to their\nincreased usage for summarizing, predicting, and generating text, making it\nvital to help researchers and engineers understand how and why they work. We\npresent KnowledgeVis, a human-in-the-loop visual analytics system for\ninterpreting language models using fill-in-the-blank sentences as prompts. By\ncomparing predictions between sentences, KnowledgeVis reveals learned\nassociations that intuitively connect what language models learn during\ntraining to natural language tasks downstream, helping users create and test\nmultiple prompt variations, analyze predicted words using a novel semantic\nclustering technique, and discover insights using interactive visualizations.\nCollectively, these visualizations help users identify the likelihood and\nuniqueness of individual predictions, compare sets of predictions between\nprompts, and summarize patterns and relationships between predictions across\nall prompts. We demonstrate the capabilities of KnowledgeVis with feedback from\nsix NLP experts as well as three different use cases: (1) probing biomedical\nknowledge in two domain-adapted models; and (2) evaluating harmful identity\nstereotypes and (3) discovering facts and relationships between three\ngeneral-purpose models.",
        "updated": "2024-03-07 18:56:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04758v1"
    },
    {
        "title": "GNN-VPA: A Variance-Preserving Aggregation Strategy for Graph Neural Networks",
        "authors": "Lisa SchneckenreiterRichard FreinschlagFlorian SestakJohannes BrandstetterGünter KlambauerAndreas Mayr",
        "links": "http://arxiv.org/abs/2403.04747v1",
        "entry_id": "http://arxiv.org/abs/2403.04747v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04747v1",
        "summary": "Graph neural networks (GNNs), and especially message-passing neural networks,\nexcel in various domains such as physics, drug discovery, and molecular\nmodeling. The expressivity of GNNs with respect to their ability to\ndiscriminate non-isomorphic graphs critically depends on the functions employed\nfor message aggregation and graph-level readout. By applying signal propagation\ntheory, we propose a variance-preserving aggregation function (VPA) that\nmaintains expressivity, but yields improved forward and backward dynamics.\nExperiments demonstrate that VPA leads to increased predictive performance for\npopular GNN architectures as well as improved learning dynamics. Our results\ncould pave the way towards normalizer-free or self-normalizing GNNs.",
        "updated": "2024-03-07 18:52:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04747v1"
    },
    {
        "title": "LLMs in the Imaginarium: Tool Learning through Simulated Trial and Error",
        "authors": "Boshi WangHao FangJason EisnerBenjamin Van DurmeYu Su",
        "links": "http://arxiv.org/abs/2403.04746v1",
        "entry_id": "http://arxiv.org/abs/2403.04746v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04746v1",
        "summary": "Tools are essential for large language models (LLMs) to acquire up-to-date\ninformation and take consequential actions in external environments. Existing\nwork on tool-augmented LLMs primarily focuses on the broad coverage of tools\nand the flexibility of adding new tools. However, a critical aspect that has\nsurprisingly been understudied is simply how accurately an LLM uses tools for\nwhich it has been trained. We find that existing LLMs, including GPT-4 and\nopen-source LLMs specifically fine-tuned for tool use, only reach a correctness\nrate in the range of 30% to 60%, far from reliable use in practice. We propose\na biologically inspired method for tool-augmented LLMs, simulated trial and\nerror (STE), that orchestrates three key mechanisms for successful tool use\nbehaviors in the biological system: trial and error, imagination, and memory.\nSpecifically, STE leverages an LLM's 'imagination' to simulate plausible\nscenarios for using a tool, after which the LLM interacts with the tool to\nlearn from its execution feedback. Both short-term and long-term memory are\nemployed to improve the depth and breadth of the exploration, respectively.\nComprehensive experiments on ToolBench show that STE substantially improves\ntool learning for LLMs under both in-context learning and fine-tuning settings,\nbringing a boost of 46.7% to Mistral-Instruct-7B and enabling it to outperform\nGPT-4. We also show effective continual learning of tools via a simple\nexperience replay strategy.",
        "updated": "2024-03-07 18:50:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04746v1"
    },
    {
        "title": "How Far Are We from Intelligent Visual Deductive Reasoning?",
        "authors": "Yizhe ZhangHe BaiRuixiang ZhangJiatao GuShuangfei ZhaiJosh SusskindNavdeep Jaitly",
        "links": "http://arxiv.org/abs/2403.04732v1",
        "entry_id": "http://arxiv.org/abs/2403.04732v1",
        "pdf_url": "http://arxiv.org/pdf/2403.04732v1",
        "summary": "Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated\nincredible strides on diverse vision language tasks. We dig into vision-based\ndeductive reasoning, a more sophisticated but less explored realm, and find\npreviously unexposed blindspots in the current SOTA VLMs. Specifically, we\nleverage Raven's Progressive Matrices (RPMs), to assess VLMs' abilities to\nperform multi-hop relational and deductive reasoning relying solely on visual\nclues. We perform comprehensive evaluations of several popular VLMs employing\nstandard strategies such as in-context learning, self-consistency, and\nChain-of-thoughts (CoT) on three diverse datasets, including the Mensa IQ test,\nIntelligenceTest, and RAVEN. The results reveal that despite the impressive\ncapabilities of LLMs in text-based reasoning, we are still far from achieving\ncomparable proficiency in visual deductive reasoning. We found that certain\nstandard strategies that are effective when applied to LLMs do not seamlessly\ntranslate to the challenges presented by visual reasoning tasks. Moreover, a\ndetailed analysis reveals that VLMs struggle to solve these tasks mainly\nbecause they are unable to perceive and comprehend multiple, confounding\nabstract patterns in RPM examples.",
        "updated": "2024-03-07 18:35:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2403.04732v1"
    }
]