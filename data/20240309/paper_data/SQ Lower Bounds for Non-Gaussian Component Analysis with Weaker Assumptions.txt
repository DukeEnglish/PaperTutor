SQ Lower Bounds for Non-Gaussian Component Analysis with
Weaker Assumptions
Ilias Diakonikolas∗ Daniel M. Kane†
University of Wisconsin-Madison University of California, San Diego
ilias@cs.wisc.edu dakane@cs.ucsd.edu
Lisheng Ren ‡ Yuxin Sun §
University of Wisconsin-Madison University of Wisconsin-Madison
lren29@wisc.edu yxsun@cs.wisc.edu
March 8, 2024
Abstract
We study the complexity of Non-Gaussian Component Analysis (NGCA) in the Statistical
Query (SQ) model. Prior work developed a general methodology to prove SQ lower bounds for
thistaskthathavebeenapplicabletoawiderangeofcontexts. Inparticular,itwasknownthat
foranyunivariatedistributionAsatisfyingcertainconditions,distinguishingbetweenastandard
multivariate Gaussian and a distribution that behaves like A in a random hidden direction and
like a standard Gaussian in the orthogonal complement, is SQ-hard. The required conditions
were that (1) A matches many low-order moments with the standard univariate Gaussian, and
(2)thechi-squarednormofAwithrespecttothestandardGaussianisfinite. Whilethemoment-
matching condition is necessary for hardness, the chi-squared condition was only required for
technical reasons. In this work, we establish that the latter condition is indeed not necessary.
In particular, we prove near-optimal SQ lower bounds for NGCA under the moment-matching
conditiononly. Our resultnaturally generalizesto the setting ofa hidden subspace. Leveraging
our general SQ lower bound, we obtain near-optimal SQ lower bounds for a range of concrete
estimation tasks where existing techniques provide sub-optimal or even vacuous guarantees.
∗Supported by NSF Medium Award CCF-2107079, NSF Award CCF-1652862 (CAREER), a Sloan Research
Fellowship, and a DARPALearning with Less Labels (LwLL) grant.
†Supported byNSFMedium Award CCF-2107547, and NSFAward CCF-1553288 (CAREER).
‡Supported byNSFAward CCF-1652862 (CAREER).
§Supported byNSFAward CCF-1652862 (CAREER).
4202
raM
7
]GL.sc[
1v44740.3042:viXra1 Introduction
Non-Gaussian Component Analysis (NGCA) is a statistical estimation task first considered in
the signal processing literature [BKS+06]. As the name suggests, the objective is to find a non-
gaussian direction (or, more generally, low-dimensional subspace) in a high-dimensional dataset.
Sinceitsintroduction, theNGCAproblemhasbeenstudiedinarangeofworksfromanalgorithmic
standpoint; see[TV18,GS19]and references therein. Herewe explorethis problemfrom ahardness
perspective in the Statistical Query (SQ) model. Before we motivate and state our results, we
require basic background on the SQ model.
SQ Model SQ algorithms are a class of algorithms that are allowed to query expectations of
bounded functions on the underlying distribution through an (SQ) oracle rather than directly
access samples. The model was introduced by Kearns [Kea98] as a natural restriction of the PAC
model [Val84] in the context of learning Boolean functions. Since then, the SQ model has been
extensively studied in a range of settings, including unsupervised learning [Fel16]. The class of
SQ algorithms is broad and captures a range of known algorithmic techniques in machine learning
including spectral techniques, moment and tensor methods, local search (e.g., EM), and many
others (see, e.g., [FGR+17, FGV17] and references therein).
Definition 1.1 (SQ Model). Let D be a distribution on Rn. A statistical query is a bounded
function q :Rn [ 1,1]. We defineSTAT(τ) to betheoracle thatgiven any such queryq, outputs
→ −
a value v such that v E [q(x)] τ, where τ > 0 is the tolerance parameter of the query. A
x D
| − ∼ | ≤
statistical query (SQ) algorithm is an algorithm whose objective is to learn some information about
an unknown distribution D by making adaptive calls to the corresponding STAT(τ) oracle.
The following family of high-dimensional distributions forms the basis for the definition of the
NGCA problem.
Definition 1.2 (Hidden-Subspace Distribution). For a distribution A supported on Rm and a
matrix V Rn m with V⊺V = I , we define the distribution PA supported on Rn such that
∈ × m V
it is distributed according to A in the subspace span(v ,...,v ) and is an independent standard
1 m
Gaussian in the orthogonal directions, where v ,...,v denote the column vectors of V. In
1 m
particular, if A is a continuous distribution with probability density function A(y), then PA is the
V
distribution over Rn with probability density function
PA(x) = A( v ,x ,..., v ,x )exp( x VV⊺x 2/2)/(2π)(n m)/2 .
V h 1 i h m i −k − k2 −
That is, PA is the product distribution whose orthogonal projection onto the subspace V is A,
V
andonto thesubspaceperpendicularto V is thestandard(n m)-dimensionalnormaldistribution.
−
An important special case of the above definition corresponds to m = 1 (i.e., the case when the
hidden subspace is one-dimensional); for this setting, we will use the notation PA for such a
v
distribution, where A is a one-dimensional distribution and v Rn is a unit vector.
∈
Since weare focusingon establishing hardness,we will consider thefollowing hypothesis testing
version of NGCA (since the learning/search version typically reduces to the testing problem). We
use to denote the standard n-dimensional Gaussian distribution (0,I ). We use U(O ) to
n n n,m
N N
denote the uniform distribution over the set of all orthogonal matrices V Rn m; namely, this
×
∈
is the distribution obtained by taking RV , where R Rn n is a random rotation matrix and
′ ×
∈
V Rn m is an arbitrary orthogonal matrix.
′ ×
∈
Definition 1.3 (Hypothesis Testing Version of NGCA). Let n > m 1 be integers. For a
≥
distribution A supported on Rm, one is given access to a distribution D such that either:
1• H : D = ,
0 n
N
• H : D is given by PA, where V U(O ).
1 V ∼ n,m
The goal is to distinguish between these two cases H and H .
0 1
For the special case that m = 1 (i.e., for a univariate distribution A), prior work [DKS17]
established SQ-hardness of NGCA1 under the following condition:
Condition 1.4. Let d Z . The distribution A on R is such that (i) the first d moments of A
+
∈
agree with the first d moments of (0,1), and (ii) the chi-squared distance χ2(A, ) is finite, where
N N
the chi-squared distance of two distributions (with probability density functions) P,Q :Rn R is
+
→
defined as χ2(P,Q) d =ef P(x)2/Q(x)dx 1.
x Rn −
∈
Specifically, themain reRsultof [DKS17]shows thatany SQalgorithm thatsolves thetesting version
of NGCA requires either
2nΩ(1)
many SQ queries or at least one query with accuracy
n Ω(d) χ2(A, (0,1)).
−
N
p
It is worth noting that subsequent works (see [DK22a] and [DKPZ21]) generalized this result so
that it only requires that (i) A approximately matches moments with the standard Gaussian, and
(ii) A is a low-dimensional distribution embedded in a hidden low-dimensional subspace, instead
of a one-dimensional distribution.
The starting point of our investigation is a key technical limitation of this line of work. Specifi-
cally, ifχ2(A, (0,1))isverylarge(orinfinite),e.g.,ifAhasconstantprobabilitymassonadiscrete
N
set, the aforementioned SQ lower bound of [DKS17] can be very weak (or even vacuous). It is
thus natural to ask if the finite chi-squared assumption is in fact necessary for the corresponding
SQ lower bounds to hold.
AconcretemotivationtoanswerthisquestioncomesfromtheapplicationsofagenericSQlower
bound for NGCA to various learning problems. The SQ-hardness of NGCA can be used to obtain
similar hardness for a number of well-studied learning problems that superficially appear very dif-
ferent. Theseincludelearning mixturemodels [DKS17, DKPZ23, DKS23], robustmean/covariance
estimation [DKS17], robust linear regression [DKS19], learning halfspaces and other natural con-
cepts with adversarial or semi-random label noise [DKZ20, GGK20, DK22a, DKPZ21, DKK+22],
list-decodable mean estimation and linear regression [DKS18, DKP+21], learning simple neural
networks [DKKZ20], and even learning simple generative models [CLL22]. In several of these
applications, the requirement of bounded chi-squared distance is somewhat problematic, in some
cases leading to quantitatively sub-optimal results. Moreover, in certain applications, this restric-
tion leads to vacuous guarantees.
1.1 Our Results
1.1.1 Main Result
Our main result is a near-optimal SQ lower bound for the NGCA problem, assuming only the
moment-matching condition (i.e., without the chi-squared distance restriction). Informally, we
essentially show that in order to solve the NGCA in n dimensions with an m-dimensional distri-
bution A that (approximately) matches moments with the standard Gaussian up to degree d, any
1While the SQ lower bound result of [DKS17] was phrased for the search version of NGCA, it can be directly
translated to thetesting version; see, e.g., Chapter 8 of [DK23].
2SQ algorithm will either require one query with accuracy O (n Ω(d)) or exponential in n many
m,d −
queries.
Formally, we establish the following theorem.
Theorem 1.5 (Main SQ Lower Bound Result). Let λ (0,1) and n,m,d N with d even and
∈ ∈
m,d nλ/logn. Let 0 < ν < 2 and A be a distribution on Rm such that for any polynomial
≤
f :Rm R of degree at most d and E [f(x)2]= 1, the following holds:
→ x ∼Nm
E [f(x)] E [f(x)] ν .
| x ∼A − x ∼Nm | ≤
Let 0 < c < (1 λ)/4 and n be at least a sufficiently large constant depending on c. Then any
−
SQ algorithm solving the n-dimensional NGCA problem (as in Definition 1.3) with 2/3 success
probability requires either
(i) a query of tolerance O n ((1 λ)/4 c)d +(1+o(1))ν, or
m,d − − −
(ii)
2nΩ(c)
many queries.
(cid:0) (cid:1)
A few comments regarding the parameters are in order here. We first note that the constant
in O m,d(n −((1 −λ)/4 −c)d) is roughly the size of the Binomial coefficient (d+ mm /2)/2 1−1 . In addition, we
−
would like to point out that for most applications we will have m,d poly(n). Therefore, the
(cid:0)≪ (cid:1)
parameters λ,c can essentially be taken arbitrarily close to 0. So, informally, our lower bound on
the query accuracy can be roughly thought of as (d+ mm /2)/2 1−1 n −d/4. It is worth noting that the
−
constant of 1/4 appearing in the exponent is optimal in the worst-case; there is a problem instance
(cid:0) (cid:1)
of NGCA that can be solved by an SQ algorithm with a single query of tolerance n (d+2)/4 (see
−
Appendix D).
For all the applications given in this paper, the above theorem will be applied for the special
case that m = 1; namely, the case that the hidden subspace is a hidden direction and A is a
univariate distribution.
Relation to LLL-based Algorithms for NGCA Consider the special case of the NGCA
problemcorrespondingtom = 1, whereAisadiscretedistributionthatmatchesitsfirstdmoments
with the standard Gaussian. Theorem 1.5 implies that any SQ algorithm for this version of the
problem either uses a query with accuracy n Ω(d) or exponential many queries. On the other
−
hand, recent works [DK22b, ZSWB22] gave polynomial-time algorithms for this problem with
sample complexity O(n), regardless of the degree d. It is worth noting that the existence of these
algorithms does not contradict our SQ lower bound, as these algorithms are based on the LLL-
method for lattice basis-reduction that is not captured by the SQ framework. As an implication, it
follows that LLL-based methods surpass any efficient SQ algorithm for these settings of NGCA. A
similar observationwas previouslymadein[DH23]forthespecialcase thatthediscretedistribution
A is supported on 0, 1 ; consequently, [DH23] could only obtain a quadratic separation. Finally,
{ ± }
we note that this limitation of SQ algorithms is also shared by two other prominent restricted
families of algorithms (namely, SoS algorithms and low-degree polynomial tests).
1.1.2 Applications
We believe that Theorem 1.5 is interesting in its own right, as it elucidates the SQ complexity of a
well-studied statistical problem. Here we discuss concrete applications to some natural statistical
tasks. We note that the SQ lower bound in the prior work [DKS17] cannot give optimal (or even
nontrivial) lower bounds for these applications.
3List-decodable Gaussian Mean Estimation One can leverage our result to obtain a sharper
SQlower boundforthetaskoflist-decodableGaussianmeanestimation. Inthistask, thealgorithm
isgivenasinputpointsfromRn whereanα < 1/2fractionofthepointsaredrawnfromanunknown
mean and identity covariance Gaussian (µ,I), and the remaining points are arbitrary. The goal
N
of the algorithm is to output a list of O(1/α) many hypothesis vectors at least one of which is close
to µ in ℓ -norm with probability at least 2/3. [DKS18] established the following SQ lower bound
2
for this problem (see also [DK23] for a different exposition).
Fact 1.6 ([DKS18]). For each d Z and c (0,1/2), there exists c > 0 such that for any α > 0
+ d
∈ ∈
sufficiently small the following holds. Any SQ algorithm that is given access to a (1 α)-corrupted
−
Gaussian (µ,I) in n> d3/c dimensions and returns a list of hypotheses such that with probability
N
at least 2/3 one of the hypotheses is within ℓ -distance c α 1/d of the true mean µ, does one of
2 d −
the following: (i) Uses queries with error tolerance at most exp(O(α 2/d))Ω(n) (d+1)(1/4 c/2). (ii)
− − −
Uses at least exp(Ω(nc)) many queries. (iii) Returns a list of at least exp(Ω(n)) many hypotheses.
The above statement is obtained using the framework of [DKS17] by considering the distribu-
tion testing problem between (µ,I) and PA, where A is the one-dimensional moment-matching
v
N
distribution of the following form.
Fact 1.7 ([DKS18], see Lemma 8.21 in [DK23]). For d Z , there exists a distribution A =
+
∈
α (µ,1)+(1 α)E, for some distribution E and µ = 10c α 1/d, such that the first d moments of
d −
N −
A agree with those of (0,1). Furthermore, the probability density function of E can be taken to
N
be pointwise at most twice the pdf of the standard Gaussian.
We note that χ2(A, ) = O(exp(µ2)), and this results in the exp(O(α 2/d)) term in the error
−
N
tolerance of Fact 1.6. Consequently, if α log(n) d/2, the error tolerance is greater than one and
−
≪
Fact 1.6 fails to give a non-trivial bound. It is worth noting that the setting of “small α” (e.g., α is
sub-constant in the dimension) is of significant interest in various applications, including in mean
estimation. A concrete application is in the related crowdsourcing setting of [MV18] dealing with
this parameter regime.
We can circumvent this technical problem by combining our main result (Theorem 1.5) with
Fact 1.7 to obtain the following sharper SQ lower bound (see Appendix C for the proof).
Theorem 1.8 (SQ Lower Bound for List-Decoding the Mean). Let λ (0,1) and n,d N with
∈ ∈
d be even and d nλ/logn. Let c > 0 and n be at least a sufficiently large constant depending
≤
on c. There exists c > 0 such that for any α > 0 sufficiently small the following holds. Any SQ
d
algorithm that is given access to a (1 α)-corrupted Gaussian (µ,I) in n dimensions and returns
− N
a list of hypotheses such that with probability at least 2/3 one of the hypotheses is within ℓ -distance
2
c α 1/d of the true mean µ, does one of the following: (i) Uses queries with error tolerance at most
d −
O (n ((1 λ)/4 c)d). (ii) Uses at least 2nΩ(c) many queries. (iii) Returns a list of at least exp(Ω(n))
d − − −
many hypotheses.
Anti-concentration Detection Anti-concentration (AC) detection is the following hypothesis
testing problem: given access to an unknown distribution D over Rn, where the input distribution
D is promised to satisfy either (i) D is the standard Gaussian; or (ii) D has at least α < 1 of its
probability mass residing inside a dimension (n 1) subspace V Rn. The goal is to distinguish
− ⊂
between the two cases with success probability 2/3.
In order to use our main result to derive an SQ lower bound for this task, we require the
following lemma on univariate moment-matching (see Appendix C for the proof).
4Lemma 1.9. Let D denote the distribution that outputs 0 with probability 1. For d Z , there
0 +
∈
exists α (0,1) and a univariate distribution A = α D + (1 α )E for some distribution E,
d d 0 d
∈ −
such that the first d moments of A agree with those of (0,1). Furthermore, the pdf of E can be
N
taken to be pointwise at most twice the pdf of the standard Gaussian.
Using the above lemma and our main result, we can deduce the following SQ lower bound on
the anti-concentration detection problem (see Appendix C for the proof).
Theorem 1.10 (SQ Lower Bound for AC Detection). For any α (0,1/2), any SQ algorithm
∈
that has access to an n-dimensional distribution that is either (i) the standard Gaussian; or (ii) a
distribution that has at least α < 1 probability mass in an (n-1)-dimensional subspace V Rn, and
⊂
distinguishes the two cases with success probability at least 2/3, either requires a query with error
at most n ωα(1), or uses at least 2nΩ(1) many queries.
−
Learning Periodic Functions Another application is for the well-studied problem of learning
periodic functions (see, e.g., [SVWX17] and [SZB21]), which is closely related to the continuous
Learning with Errors (cLWE) problem [BRST21]. In the task of learning periodic functions, the
algorithm is given sample access to a distribution D of labeled examples (x,y) over Rn R. The
×
distribution D is such that x (0,I ) and y = cos(2π(δ w,x +ζ)) with noise ζ (0,σ2).
n
∼ N h i ∼ N
This implies that y is a periodic function of x along the direction of w with a small amount of
noise. While the frequency and noise scale parameters δ,σ R are known to the algorithm, the
+
∈
parameter w Sn 1 is unknown; the goal of the algorithm is to output a hypothesis h : Rn R
−
∈ →
such that E [(h(x) y)2] is minimized.
x D
∼ −
InordertouseourmaintheoremtoderiveanSQlower boundforthisproblem,weneedtoshow
that for any t [ 1,1], D conditioned on y = t approximately matches moments with (0,I ). To
n
∈ − N
this end, we introduce the following definition and fact of discrete Gaussian measurefrom [DK22a].
Definition 1.11. For s R and θ R, let G denote the measure of the “s-spaced discrete
+ s,θ
∈ ∈
Gaussian distribution”. In particular, for each n Z, G assigns mass sg(ns+θ) to the point
s,θ
∈
ns+θ, where g is the pdf function of (0,1).
N
Note that although G is not a probability measure (as the total measure is not one), it can
s,θ
be thought of as a probability distribution since the total measure is close to one for small σ. To
see this, we introduce the following fact.
Fact 1.12 (Lemma 3.12 from [DK22a]). For all k N, s > 0 and all θ R, we have that
∈ ∈
E [tk] E [tk] = k!O(s)kexp( Ω(1/s2)).
|
t ∼N(0,1)
−
t ∼Gs,θ
| −
Using the above fact, we are now ready to prove our SQ lower bound for learning periodic
functions (see Appendix C for the proof).
Theorem 1.13 (SQ Lower Bound for Learning Periodic Functions). Let c > 0 and n be at least
a sufficiently large constant depending on c. Let D be the distribution of (x,y) over Rn R that
×
is generated by x (0,I ) and y = cos(2π(δ w,x +ζ)) with noise ζ (0,σ2). Let δ > nc,
n
∼ N h i ∼ N
σ be known and w be unknown to the algorithm. Then any SQ algorithm that has access to the
distribution D and returns a hypothesis h : Rn R such that E [(h(x) y)2] = o(1), either
(x,y) D
requires a query with error at most exp(
nc′
)
f→
or c <
min(2c,1/10)∼
, or
uses−
at least
2nΩ(1)
many
′
−
queries.
51.2 Technical Overview
We start by noting that we cannot use the standard SQ dimension argument [FGR+17] to prove
our result, due to the unbounded chi-squared norm. To handle this issue, we need to revisit the
underlying ideas of that proof. In particular, we will show that for any bounded query function
f : Rn [ 1,1], with high probability over some V U(O ), the quantity E [f(x)]
→ − ∼ n,m | x ∼Nn −
E [f(x)] will be small. This allows an adversarial oracle to return E [f(x)] to every query
x ∼PA V | x ∼Nn
f regardless of which case we are in, unless the algorithm is lucky enough (or uses high accuracy)
to find an f that causes E [f(x)] E [f(x)] to be at least τ.
| x ∼Nn − x ∼PA V |
Our approach for calculating E [f(x)] will be via Fourier analysis. In particular, using
x PA
∼ V
the Fourier decomposition of f, we can write f(x) = ∞k=0hT k,H k(x) i, where H k(x) is the prop-
erly normalized degree-k Hermite polynomial tensor and T is the degree-k Fourier coefficient
k
P
of f. Taking the inner product with the distribution PA involves computing the expectation of
V
E [H (x)], which can be seen to be V kA , where A = E [H (x)]. Thus, we obtain (at
x PA k ⊗ k k x A k
∼ V ∼
least morally speaking) that
∞ ∞ ∞
E [f(x)]= V kA ,T A ,(V⊺) kT A (V⊺) kT . (1)
x ∼PA V h ⊗ k k i≤ |h k ⊗ k i|≤ k k k2 k ⊗ k k2
k=0 k=0 k=0
X X X
The k = 0 term of the above sum will be exactly T = E [f(x)]. By the moment-matching
0 x n
∼N
condition, the k = 1 through k = d terms will be very small. Finally, we need to argue that the
higher degree terms are small with high probability. To achieve this, we provide a non-trivial upper
bound for E
V
∼U(On,m)[ k(V⊺) ⊗kT
k
k2 2]. Combining with the fact that ∞k=0kT
k
k2
2
= kf k2
2
≤1, this
allows us to prove high probability bounds on each term, and thus their sum.
P
Furthermore, if we want higher probability estimates of the terms with small k, we can instead
boundE [ (V⊺) kT 2a]forsomeintegera. Unfortunately,therearenon-trivialtechnical
V ∼U(On,m) k ⊗ k k2
issues with the above approach, arising from issues with (1). To begin with, as no assumptions
other than moment-matching (for a few low-degree moments) were made on A, it is not guaranteed
that A is finite for larger values of k. To address this issue, we will truncate the distribution A.
k
In particular, we pick a parameter B (which will be determined carefully), and define A to be A
′
conditioned on the value in the ball Bm(B). Due to the higher-moment bounds, we can show that
A and A
′
are close in total variation distance, and thus that E
x ∼PA
V[f(x)] is close to E
x ∼PA
V′[f(x)]
for any bounded f.
Furthermore, using the higher moment bounds, we can show that A nearly matches the low-
′
degree moments of . The second issue arises with the interchange of summations used to
m
N
derive (1). In particular, although f(x) = ∞k=0hT k,H k(x) i, it does not necessarily follow that we
can interchange the infinite sum on the right-hand-side with taking the expectation over x PA.
P ∼ V
To fix this issue, we split f into two parts f ℓ (consisting of its low-degree Fourier components)
≤
and f>ℓ. We note that (1) does hold for f ℓ, as the summation there will be finite, and we can
≤
use the above argument to bound E [f(x)] E [f ℓ(x)] with high probability. To bound
x ∼Nn − x ∼PA V ≤ |
E [f>ℓ(x)], we note that by taking ℓ large, we can make f>ℓ < δ for some exponentially
| x ∼PA V | k k2
small δ > 0. We then bound E [E [f>ℓ ]] = E [f>ℓ ], where Q is the average
V ∼U(On,m) x ∼PA V | | x ∼Q | |
over V of PA (note that everything here is non-negative, so there is no issue with the interchange
V
of integrals). Thus, we can bound the desired quantity by noting that f>ℓ is small and that the
2
k k
chi-squared norm of Q with respect to the standard Gaussian is bounded.
n
N
62 Preliminaries
We will use lowercase boldface letters for vectors and capitalized boldface letters for matrices and
tensors. We use Sn 1 = x Rn : x = 1 to denote the n-dimensional unit sphere. For
− 2
{ ∈ k k }
vectors u,v Rn, we use u,v to denote the standard inner product. For u Rn, we use
∈ h i ∈
u = n uk 1/k to denote the ℓ -norm of u. For tensors, we will consider a k-tensor to be
k kk i=1 i k
an element in (Rn) k = Rnk . This can be thought of as a vector with nk coordinates. We will
(cid:0)P (cid:1)⊗ ∼
use A to denote the coordinate of a k-tensor A indexed by the k-tuple (i ,...,i ). By abuse
i1,...,ik 1 k
of notation, we will sometimes also use this to denote the entire tensor. The inner product and
ℓk-norm of a k-tensor are defined by thinking of the tensor as a vector with nk coordinates and
then using the definition of inner product and ℓ -norm of vectors. For a vector v Rn, we denote
k
by v k to be a vector (linear object) in Rnk . For a matrix V Rn m, we denote∈ by V , V
⊗ × 2 F
∈ k k k k
to be the operator norm and Frobenius norm respectively. In addition, we denote by V k to be a
⊗
matrix (linear operator) mapping
Rnk
to
Rmk
.
We use 1 to denote the indicator function of a set, specifically 1(t S) = 1 if t S and 0
∈ ∈
otherwise. We will use Γ : R
→
R to denote the gamma function Γ(z) = 0∞tz −1e −tdt. We use
B : R R R to denote the beta function B(z ,z ) = Γ(z )Γ(z )/Γ(z +z ). We use χ2 to
× → 1 2 1 2 1 R 2 k
denote the chi-squared distribution with k degrees of freedom. We use Beta(α,β) to denote the
Beta distribution with parameters α and β.
For a distribution D, we use Pr [S] to denote the probability of an event S. For a continuous
D
distribution D over Rn, we sometimes use D for both the distribution itself and its probability
density function. For two distributions D ,D over a probability space Ω, let d (D ,D ) =
1 2 TV 1 2
sup Pr (S) Pr (S) denote the total variation distance between D and D . For two
contS i⊆ nΩ uo| usD d1 istrib− utionD D2 ,D| over Rn, we use χ2(D ,D ) = D (x)2/D (1 x)dx 12 to denote
1 2 1 2 Rn 1 2
−
the chi-square norm of D w.r.t. D . For a subset S Rn with finite measure or finite surface
1 2
⊆ R
measure, we use U(S) to denote the uniform distribution over S (w.r.t. Lebesgue measure for the
volumn/surface area of S).
Basics of Hermite Polynomials We require the following definitions.
Definition 2.1 (Normalized Hermite Polynomial). For k N, we define the k-th probabilist’s
∈
Hermite polynomials He : R Ras He (t)= ( 1)ket2/2 dk e t2/2. We definethek-th normalized
k → k − ·dtk −
Hermite polynomial h : R R as h (t) = He (t)/√k!.
k k k
→
Furthermore, we will use multivariate Hermite polynomials in the form of Hermite tensors (as
the entries in the Hermite tensors are rescaled multivariate Hermite polynomials). We define the
Hermite tensor as follows.
Definition 2.2 (Hermite Tensor). For k N and x Rn, we define the k-th Hermite tensor as
∈ ∈
1
(H (x)) = ( I ) x .
k i1,i2,...,ik
√k! −
ia,ib ic
PartitionsP of[k] a,b P c P
X { O}∈ {O}∈
intosetsofsize1and2
We denote by L2(Rn, ) the space of all functions f : Rn R such that E [f2(v)] < .
For functions f,g
L2(RnN ,n
), we use f,g = E
[f(x)g→
(x)] to denote
thv e∼ irN in
nner
produ∞
ct.
We use f = ∈ f,f N ton denote ith s L2i -N nn orm. x F∼ oN rn a function f : Rn R and ℓ N, we
use f ℓk tok2 denoteh f ℓ(i xN )n = ℓ A ,H (x) , where A = E [f(x)H→ (x)], whic∈ h is the
degree≤ -ℓ approximp ati≤ on of f. Wk e=0 uh sek f>ℓk = i f f ℓ tok denotex ∼ itN sn residuek . We remark that
≤
P −
7normalized Hermite polynomials (resp. Hermite tensors) form a complete orthogonal system for
the inner product space L2(R, ) (resp. L2(Rn, )). This implies that for f L2(Rn, ),
n n
N N ∈ N
lim f>ℓ = 0. We also remark that both our definition of Hermite polynomial and Hermite
ℓ 2
→∞k k
tensor are “normalized” in the following sense: For Hermite polynomials, it holds h = 1. For
k 2
k k
Hermite tensors, given any symmetric tensor A, we have A,H (x) 2 = A,A .
kh k ik2 h i
The following claim states that for any orthonormal transformation B, the Hermite tensor
H (Bx) can be written as applying the linear transformation B k on the Hermite tensor H (x).
k ⊗ k
The proof is deferred to Appendix A.
Claim 2.3. Let 1 m < n. Let B Rm n with BB⊺ = I . It holds that H (Bx) =
× m k
≤ ∈
B kH (x),x Rn.
⊗ k
∈
3 SQ-Hardness of NGCA: Proof of Theorem 1.5
The main idea of the proof is the following. Suppose that the algorithm only asks queries with
tolerance τ, and let f bean arbitrary query function that the algorithm selects. The key ingredient
is to show that E [f(x)] E [f(x)] τ with high probability over V U(O ). If this
holds, then
whe|
n
tx h∼eP aA V lgorithm− qux e∼ rN ien
s f, if
t| h≤
e input is from the alternative
hyp∼ othesin s,,m
with high
probability, E [f(x)] is a valid answer for the query. Therefore, when the algorithm queries f,
x n
∼N
regardless of whether the input is from the alternative or null hypothesis, the oracle can just return
E [f(x)]. Then the algorithm will not observe any difference between the two cases with any
x n
∼N
small number of queries. Thus, it is impossible to distinguish the two cases with high probability.
To prove the desired bound, we introduce the following proposition.
Proposition 3.1. Let λ (0,1) and n,m,d N with d be even and m,d nλ. Let ν R
+
∈ ∈ ≤ ∈
and A be a distribution on Rm such that for any polynomial f : Rm R of degree at most d and
→
E [f(x)2]= 1,
x m
∼N
E [f(x)] E [f(x)] ν .
| x ∼A − x ∼Nm | ≤
Let 0 < c < (1 λ)/4 and n is at least a sufficiently large constant depending on c, then, for any
−
function f : Rn [ 1,1], it holds
→ −
Γ(d/2+m/2)
Pr E [f(x)] E [f(x)] n ((1 λ)/4 c)d +(1+o(1))ν 2 nΩ(c) .
V ∼U(On,m) | x ∼PA V − x ∼Nn | ≥ Γ(m/2) − − − ≤ −
(cid:20) (cid:18) (cid:19) (cid:21)
Assuming Proposition 3.1, the proof of our main theorem is quite simple.
Proof for Theorem 1.5. Suppose there is an SQ algorithm using q <
2nΩ(c)
many queries of
A
accuracy τ Γ(d/2+m/2) n ((1 λ)/4 c)d +(1+o(1))ν and succeeds with at least 2/3 probability.
≥ Γ(m/2) − − −
We prove by c(cid:16)ontradiction(cid:17)that such an cannot exist. Suppose the input distribution is n, and
A N
the SQ oracle always answers E [f(x)] for any query f. Then the assumption on implies
x ∼Nn A
that it answers “null hypothesis” with probability α > 2/3. Now consider the case that the input
distribution is PA and V U(O ). Suppose the SQ oracle still always answers E [f(x)] for
any query f.
LeV
t f ,
,∼
f be
tn h, em
queries the algorithm asks, where q =
2nΩ(c)
.
Bx y∼N Pn
roposition
1 q
···
3.1 and a union bound, we have
Pr [ i [q], E [f (x)] E [f (x)] τ] =o(1) .
V ∼U(On,m) ∃ ∈ | x ∼PA V i − x ∼Nn i | ≥
Therefore,withprobability1 o(1), theanswersgiven bytheoraclearevalid. From ourassumption
−
on , the algorithm needs to answer “alternative hypothesis” with probability at least 2(1 o(1)).
A 3 −
8But since the oracle always answers E [f(x)] (which is the same as the above discussed null
x n
∼N
hypothesis case), we know the algorithm will return “null hypothesis” with probability α > 2/3.
This gives a contradiction and completes the proof.
The rest of this section is devoted to the proof of Proposition 3.1.
3.1 Fourier Analysis using Hermite Polynomials
Themain idea of Proposition 3.1 is toanalyze E [f(x)] throughFourier analysis usingHermite
x PA
polynomials. Beforewedotheanalysis,wewillfir∼ sttV runcatethedistributionAinsideBm(B)which
is the ℓ -norm unit ball in m-dimensions with radius B for some B R to be specified. Namely,
2 +
∈
we will consider the truncated distribution A defined as the distribution of x A conditioned
′
∼
on x Bm(B). The following lemma shows that given any m-dimensional distribution A that
∈
approximately matches the first d moment tensor with the Gaussian, the truncated distribution A
′
is close to A in both the total variation distance and the first d moment tensors.
Lemma 3.2. Let m,d N with d be even. Let A be a distribution on Rm such that for any
∈
polynomial f of degree at most d and E [f(x)2] = 1,
x m
∼N
E [f(x)] E [f(x)] ν 2 .
| x ∼A − x ∼Nm | ≤ ≤
Let B R such that Bd c 2d/2 Γ(d+m/2) where c is at least a sufficiently large universal
∈ + ≥ 1 Γ(m/2) 1
constant and let A
′
be the trunc(cid:16)atedqdistribution(cid:17)defined as the distribution of x A conditioned
∼
on x Bm(B). Then d (A,A) = O 2d/2 Γ(d+m/2) B d, Furthermore, for any k N,
∈ TV ′ Γ(m/2) − ∈
(cid:16) q (cid:17)
2O(k) 2d/2 Γ(d+m/2) B (d k)
Γ(m/2) − −
kE
x
∼A′[H k(x)] −E
x
∼Nm[H k(x)]
k2
= 

+ 1+(cid:16) O q 2d/2 Γ( Γd (+ mm(cid:17) /2/ )2) B −d ν k < d;

 2O(cid:16)(k) 2d/2(cid:16) Γ(qd+m/2) Bk(cid:17)d (cid:17)
k d.
Γ(m/2) − ≥
  (cid:16) q (cid:17)
The proof of Lemma 3.2 is deferred to Appen dix B.1.
Since d (A,A) O 2d/2 Γ(d+m/2) B d and f is bounded in [ 1,1], it follows that
TV ′ ≤ Γ(m/2) − −
(cid:16) q (cid:17)
|E
x ∼PA
V′[f(x)] −E
x ∼PA
V[f(x)]
| ≤
2d TV(PA V′ ,PA V′ ) = 2d TV(A,A ′)= O 2d/2 sΓ( Γd (+ mm /2/ )2) !B −d
!
.
T prh oe pr ee rfo tyre t, hi af tsu Affic ises bf oo ur nu ds edto ina sn ida ely Bze m(E Bx )∼P wA V i′ ll[f b(x e) c] oi nn vs et nea ied ntof inE tx h∼ ePA V Fo[f u( rx ie) r]. anF au lr yt sh iser lm ato er re ., t Wh ee
′
introduce the following lemma which decomposes E ′[f(x)] using Hermite analysis.
x PA
∼ V
Lemma 3.3 (Fourier Decomposition Lemma). Let A be any distribution supported on Rm, V
′
Rn ×m and V⊺V = I m. Then for any ℓ
∈
N, E
x ∼PA
V′[f(x)] = ℓ k=0hV ⊗kA k,T
k
i+E
x ∼PA
V′[f>ℓ(x)∈ ],
where A
k
= E
x
A′[H k(x)] and T
k
= E
x
n[f(x)H k(x)].
P
∼ ∼N
Proof. Noting that E
x
PA′[f(x)] = E
x
PA′[f ≤ℓ(x)]+E
x
PA′[f>ℓ(x)], we will show that
∼ V ∼ V ∼ V
ℓ
E
x ∼PA
V′[f ≤ℓ(x)] = k=0hA k,H k( hv 1,x i,..., hv m,x i) i,f ≤ℓ
n
.
DX EN
9Notice that f ℓ is a polynomial of degree-ℓ. Let b , ,b be an orthonormal basis, where b =
≤ 1 n 1
v
w1
h, e. r. e.,b
m
n
= qv m. ℓ.T The hn erd ee fog rr ee ,e- iℓ tp suol ffiyn ceo sm ti oal ss ha or wes tp ha an t· n· fe· od rb ay nypoly nnom qials ℓo ,ftheform n i=1hb i,x iqi,
i=1 i ≤ i=1 i ≤ Q
P n ℓ P n
E
x ∼PA
V′ i=1hb i,x iqi = k=0hA k,H k( hv 1,x i,..., hv m,x i) i, i=1hb i,x iqi
n
,
hY i DX Y EN
which is equivalent to showing that for any m q ℓ,
i=1 i ≤
m ℓ P m
E
x ∼PA
V′ i=1hv i,x iqi = k=0hA k,H k( hv 1,x i,..., hv m,x i) i, i=1hv i,x iqi
m
,
hY i DX Y EN
which is the same as for any q = m i=1q
i
≤
ℓ, E
y
∼A′[H q(y)] = ℓ k=0E
y
∼Nm[ hA k,H k(y) iH q(y)].
One can see that the above holds from the definition of A and the orthornormal property of
k
P P
Hermite tensors. Therefore, by Claim 2.3, we have that
ℓ
E
x ∼PA
V′[f ≤ℓ(x)] =
(cid:28)
k=0hA k,H k( hv 1,x i,..., hv m,x i) i,f ≤ℓ
(cid:29) n
X N
ℓ
= A ,H (V⊺x) ,f ℓ
k k ≤
k=0h i
(cid:28) (cid:29) n
X N
ℓ
= V kA ,H (x) ,f ℓ
⊗ k k ≤
k=0h i
(cid:28) (cid:29) n
X N
ℓ ℓ
= V kA ,H (x) , T ,H (x)
⊗ k k k k
k=0h i k=0h i
(cid:28) (cid:29) n
X X N
ℓ
= V kA ,T ,
⊗ k k
k=0h i
X
where the last equality uses the orthonormal property of Hermite tensors. This completes the
proof.
Remark 3.4. Ideally, in Lemma 3.3, we would like to have E
x ∼PA
V′[f(x)] = ∞k=0hV ⊗kA k,T
k
i.
However, since we do not assume that χ2(A, ) < , this convergence may not hold.
′ m P
N ∞
Recall thatourgoal isto showthat E [f(x)] E [f(x)] is smallwithhigh probability.
| x ∼PA V − x ∼Nn |
Observe that E [f(x)] = T which is the first term in the summation of ℓ V kA ,T
(since A = 1).x ∼ TN hn erefore, giv0 en Lemma 3.3, it suffices to show that ℓ Vk= k0h A ⊗ ,T k ank di
0 | k=1Ph ⊗ k k i|
|E
x ∼PA
V′[f ≥ℓ(x)]
|
are both small with high probability. We ignore the |E
Px ∼PA
V′[f ≥ℓ(x)]
|
part for
now, as this is mostly a technical issue. To bound ℓ V kA ,T , it suffices to analyze
| k=1h ⊗ k k i|
ℓ V kA ,T bylookingateachterm V kA ,T = A ,(V⊺) kT A (V⊺) kT .
k=1|h ⊗ k k i| |h ⊗ k Pk i| |h k ⊗ k i| ≤ k k k2 k ⊗ k k2
To show that the summation is small, we need to prove that (with high probability):
P
1. A does not grow too fast w.r.t k;
k 2
k k
2. (V⊺) kT decays very fast w.r.t k (is small with high probability w.r.t the randomness of
⊗ k 2
k k
V).
We proceed to establish these in turn below.
10A does not grow too fast: We will use slightly different arguments depending on the size
k 2
k k
ofk. Weconsiderthreecases : k < d,d k n(1 λ)/4,andk n(1 λ)/4 (thevalueintheexponent
− −
≤ ≤ ≥
will deviate by a small quantity to make the proof go through). For k < d, A grows slowly by
k 2
k k
the approximate moment-matching property of A. For d k n(1 λ)/4, we require the following
′ −
≤ ≤
fact:
Fact 3.5. Let H be the k-th Hermite tensor for m dimensions. Suppose x B, then
k 2
k k ≤
H (x) 2kmk/4Bkk k/2exp k /B2 .
k k k2 ≤ − 2
(cid:16) (cid:17)
We provide the proof of Fact 3.5 i(cid:0)n(cid:1)Appendix B.1.
For k > n(1 λ)/4, we can show that A does not grow too fast by the following asymptotic
− k 2
k k
bound on Hermite tensors.
Fact 3.6. Let H be the k-th Hermite tensor for m dimensions. Then
k
1/2
k+m 1
H (x) 2O(m) − exp( x 2/4) .
k k k2 ≤ m 1 k k2
(cid:18) − (cid:19)
We provide the proof of Fact 3.6 in Appendix B.1.
(V⊺) kT decays very fast: We show that V k,T is small with high probability by
⊗ k 2 ⊗ k
k k |h i|
bounding its a-th moment for some even a. Notice that since H f 1, we can then
k 2 2
k k ≤ k k ≤
combine it with the following lemma:
Lemma 3.7. Let k Z , a Z be even, T Rnk and m Z satisfy m < n. Then there exists
+ + +
∈ ∈ ∈ ∈
a unit vector u Sn 1 such that
−
∈
E [ (V⊺) kT a] E V⊺u ak/2 T a .
V ∼U(On,m) k ⊗ k2 ≤ V ∼U(On,m) k k2 k k2
h i
Proof. Notice that
E [ (V⊺) kT a] =E [ (V⊺) ak/2Ta/2 2]
V ∼U(On,m) k ⊗ k2 V ∼U(On,m) k ⊗ k2
=E [ V ak/2(V⊺) ak/2,T a ]
V ∼U(On,m)
h
⊗ ⊗ ⊗
i
= E [V ak/2(V⊺) ak/2],T a
h
V ∼U(On,m) ⊗ ⊗ ⊗
i
E [V ak/2(V⊺) ak/2] T a .
≤k V ∼U(On,m) ⊗ ⊗ k2 k k2
Therefore, it suffices to bound the spectral norm E [V ak/2(V⊺) ak/2] .
k
V ∼U(On,m) ⊗ ⊗ k2
Let A = E [V ak/2(V⊺) ak/2], T be the eigenvector associated with the largest
V U(On,m) ⊗ ⊗ 0
∼
absolute eigenvalue, and let u = argmax u Sn−1 T 0,u ⊗ak/2 . Then, we have
∈ |h i|
A = AT ,u ak/2 / T ,u ak/2 = T ,Au ak/2 / T ,u ak/2
2 0 ⊗ 0 ⊗ 0 ⊗ 0 ⊗
k k |h i| |h i| |h i| |h i|
= T ,E [(VV⊺u) ak/2] / T ,u ak/2
|h
0 V ∼U(On,m) ⊗
i| |h
0 ⊗
i|
= E [ T ,(VV⊺u) ak/2 ]/ T ,u ak/2
|
V ∼U(On,m)
h
0 ⊗
i| |h
0 ⊗
i|
E [ T ,(VV⊺u) ak/2 ]/ T ,u ak/2
≤
V ∼U(On,m)
|h
0 ⊗
i| |h
0 ⊗
i|
E [ (VV⊺u) ak/2 T ,u ak/2 ]/ T ,u ak/2
≤
V ∼U(On,m)
k
⊗ k2
|h
0 ⊗
i| |h
0 ⊗
i|
=E [ (VV⊺u) ak/2 ]
V ∼U(On,m)
k
⊗ k2
=E V⊺u ak/2 ,
V ∼U(On,m) k k2
h i
11where we use u = argmax u Sn−1 T 0,u ⊗ak/2 in the second inequality. Using Lemma 3.8, and
∈ |h i|
plugging everything back, we get
E [ (V⊺) kT a] E V⊺u ak/2 T a .
V ∼U(On,m) k ⊗ k2 ≤ V ∼U(On,m) k k2 k k2
h i
Roughlyspeaking,thisisjusttheak/2-thmomentofthecorrelationbetweenarandomsubspace
and a random direction, which can be bounded above by the following lemma and corollary (see
Appendix B.1 for the proofs).
Lemma 3.8. For any even k N, and u Sn 1, E [ Vu k] = Θ
Γ(k+ 2m)Γ(n 2)
.
∈ ∈ − V⊺ ∼U(On,m) k k2 (cid:18)Γ(k+ 2n)Γ(m 2)
(cid:19)
Corollary 3.9. For any even k N, and u Sn 1,
−
∈ ∈
E [ V⊺u k] = O(2k/2(n/max(m,k)) k/2) .
V ∼U(On,m) k k2 −
In addition, if there exists some constant c (0,1) such that m nc < k, then
∈ ≤
E [ V⊺u k]= exp( Ω(nclogn))O
nc+n (n −m)/2
.
V ∼U(On,m) k k2 − k+n
!
(cid:18) (cid:19)
Tocombinetheaboveresultsandgivethehighprobabilityupperboundon ℓ A ,(V⊺) kT ,
k=1|h k ⊗ k i|
we require the following lemma.
P
Lemma 3.10. Under the conditions of Proposition 3.1, and further assuming d,m nλ/logn,
≤
ν < 2 and Γ(d/2+m/2) n ((1 λ)/4 c)d < 2, the following holds: For any n that is at least a
Γ(m/2) − − −
sufficiently la(cid:16)rge constan(cid:17)t depending on c, there is a B < n such that the truncated distribu-
tion A, defined as the distribution of x A conditioned on x Bm(B), satisfies d (A,A)
′ TV ′
∼ ∈ ≤
Γ(d/2+m/2) n ((1 λ)/4 c)d. Furthermore for any ℓ N, except with probability at most 2 nΩ(c)
Γ(m/2) − − − ∈ −
w(cid:16) .r.t. V U(cid:17) (O ), it holds ℓ A ,(V⊺) kT = Γ(d/2+m/2) n ((1 λ)/4 c)d +(1+o(1))ν
∼ n,m k=1|h k ⊗ k i| Γ(m/2) − − −
where A k = E x A′[H k(x)] and PT k = E x n[f(x)H k(x)].(cid:16) (cid:17)
∼ ∼N
Proof. Aswehavediscussed,wewillconsiderthreerangesofk. However, forsometechnicalreasons
and the ease of calculations, we will additionally break the second range into two ranges. We can
write
ℓ d 1 nλ
−
A ,(V⊺) kT = A ,(V⊺) kT + A ,(V⊺) kT
k ⊗ k k ⊗ k k ⊗ k
|h i| |h i| |h i|
k=1 k=1 k=d
X X X
T ℓ
+ A ,(V⊺) kT + A ,(V⊺) kT ,
k ⊗ k k ⊗ k
|h i| |h i|
k=Xnλ+1 k= XT+1
whereT isavaluewewilllaterspecify. Toanalyzeeach A ,(V⊺) kT ,recallthat A ,(V⊺) kT
k ⊗ k k ⊗ k
|h i| |h i| ≤
A
k 2
(V⊺) ⊗kT
k
2, where A
k
= E
x
A′[H k(x)] is a constant (not depending on the randomness
k of Vk ).k For (V⊺)k kT , we can sho∼ w it is small by bounding its a-th moment for even a using
⊗ k 2
k k
Lemma 3.7 which says
E [ (V⊺) kT a] E V⊺u ak/2 T a ,
V ∼U(On,m) k ⊗ k2 ≤ V ∼U(On,m) k k2 k k2
h i
12for some unit vector u Sn 1. We will apply this strategy on the four different ranges of k.
−
∈
Without loss of generality, we will assume that λ 4c. Suppose λ 4c is not true, then we
≥ ≥
can simply consider a new pair λ,c, where λ = λ+2c and c = c/2. Notice that (1 λ)/4 c =
′ ′ ′ ′
− −
(1 λ)/4 c; therefore, the SQ lower bound in the statement remains unchanged.
′ ′
− −
We start by picking the following parameters (the sufficiently close here only depends on c):
• We require d,m nλ/logn;
≤
• B = nα where α< (1 λ )/4 is sufficiently close;
3
−
• T = c B2 where c is a sufficiently large constant;
1 1
• We let λ > λ > λ > λ to be sufficiently close (the difference between these quantities will be
3 2 1
a sufficiently small constant fraction of c);
We now bound the summation ℓ A ,(V⊺) kT as follows:
k=1|h k ⊗ k i|
• d k−=1 1|hA k,(V⊺) ⊗kT
k
i|
is smP all with high probability:
PSince Γ(d+m/2) n ((1 λ)/4 c)d < 2 and B = nα, where α is sufficiently close to (1 λ)/4,
Γ(m/2) − − − −
the pa(cid:16) rameters s(cid:17) atisfy the condition Bd = ω 2d/2 Γ(d+m/2) in Lemma 3.2. Since k < d, by
Γ(m/2)
Lemma 3.2, we have (cid:16) q (cid:17)
kA
k k2
= kE
x
∼A′[H k(x)] −E
x
∼Nm[H k(x)]
k2
Γ(d+m/2) Γ(d+m/2)
= 2O(k) 2d/2 B (d k)+ 1+O 2d/2 B d ν .
− − −
s Γ(m/2) ! s Γ(m/2) ! !
Let a be the largest even number such that ak/2 nλ, where d = o(nλ) implies a 2. Then
≤ ≥
using Lemma 3.7 and Corollary 3.9, we have
E [ (V⊺) kT a] =E V⊺u ak/2 T a E V⊺u ak/2
V ∼U(On,m) k ⊗ k k2 V ∼U(On,m) k k2 k k k2 ≤ V ∼U(On,m) k k2
=O(2ak/4n (1hλ)ak/4) = Oi (n (1 λ1)ak/4) . h i
− − − −
Using Markov’s Inequality, this implies the tail bound
Pr[ (V⊺) kT n (1 λ2)k/4] 2 Ω(cnλ) = 2 nΩ(c) .
⊗ k 2 − − − −
k k ≥ ≤
Therefore, we have
d 1 d 1
− −
A ,(V⊺) kT A (V⊺) kT
k ⊗ k k 2 ⊗ k 2
|h i| ≤ k k k k
k=1 k=1
X X
d 1
− Γ(d+m/2) Γ(d+m/2)
n (1 λ2)k/4 2O(k) 2d/2 B (d k)+ 1+O 2d/2 B d ν
− − − − −
≤ s Γ(m/2) ! s Γ(m/2) ! ! !
k=1
X
Γ(d+m/2) Γ(d+m/2)
(1+o(1)) 2d/2 B d+ν = (1+o(1)) 2d/2 n αd+ν ,
− −
≤ s Γ(m/2) ! s Γ(m/2) !
except with probability 2 nΩ(c) , where the second inequality follows from B = nα = o(n(1 λ)/4).
− −
13• nλ A ,(V⊺) kT is small with high probability:
k=d|h k ⊗ k i|
PInthepreviouscase,wehavearguedthattheparameterssatisfytheconditionBd = ω 2d/2 Γ(d+m/2)
Γ(m/2)
in Lemma 3.2. Since k d, by Lemma 3.2, we have (cid:16) q (cid:17)
≥
Γ(d+m/2)
kA
k k2
= kE
x
∼A′[H k(x)] −E
x
∼Nm[H k(x)]
k2
= 2O(k) 2d/2
s Γ(m/2)
!Bk −d .
Let a be the largest even number that ak/2 nλ, where d = o(nλ) implies a 2. Applying
≤ ≥
Lemma 3.7 and Corollary 3.9 yields
E [ (V⊺) kT a] =E V⊺u ak/2 T a E V⊺u ak/2
V ∼U(On,m) k ⊗ k k2 V ∼U(On,m) k k2 k k k2 ≤ V ∼U(On,m) k k2
=O(2ak/4n (1hλ)ak/4) = Oi (n (1 λ1)ak/4) . h i
− − − −
Therefore, we have
nλ nλ nλ
Γ(d+m/2)
A ,(V⊺) kT A (V⊺) kT n (1 λ2)k/42O(k) 2d/2 Bk d
k ⊗ k k 2 ⊗ k 2 − − −
|h i| ≤ k k k k ≤ s Γ(m/2) !
k=d k=d k=d
X X X
=2O(d)n ((1 λ2)/4)d = n ((1 λ3)/4)d ,
− − − −
except with probability 2 nΩ(c) (the first equality above follows from B = nα = o(n(1 λ)/4) =
− −
o(n(1 λ2)/4)).
−
• T A ,(V⊺) kT is small with high probability:
k=nλ+1|h k ⊗ k
i|
PWe can assume without loss of generality that nλ < T, because otherwise this term is 0. Using
Lemma 3.5, we have
kA
k k2
= kE
x
∼A′[H k(x)] −E
x
∼Nm[H k(x)]
k2
= kE
x
∼A′[H k(x)]
k2
k
2kmk/4Bkk k/2exp /B2 2O(k)mk/4Bkk k/2 .
− −
≤ 2 ≤
(cid:18)(cid:18) (cid:19) (cid:19)
Then let a be the largest even number that ak/2 T, where k T implies a 2. Applying
≤ ≤ ≥
Lemma 3.7 and Corollary 3.9 yields
E [ (V⊺) kT a] =E V⊺u ak/2 T a E V⊺u ak/2
V ∼U(On,m) k ⊗ k k2 V ∼U(On,m) k k2 k k k2 ≤ V ∼U(On,m) k k2
h i n ak/4 h i
=O(2ak/4(ak/2n)ak/4) = O − ,
ak
(cid:16) (cid:17)
which implies the tail bound
Pr[ (V⊺) kT n ((1 λ)/4)kkk/4] n λak/4 2 Ω(T) = 2 Ω(n2α) = 2 nΩ(c) .
⊗ k 2 − − − − − −
k k ≥ ≤ ≤
Therefore, we have
T T T
A ,(V⊺) kT A (V⊺) kT 2O(k)mk/4Bkk k/4n ((1 λ)/4)k
k ⊗ k k 2 ⊗ k 2 − − −
|h i| ≤ k k k k ≤
k=Xnλ+1 k=Xnλ+1 k=Xnλ+1
=O Bnλ+1n ((1 λ)/4)(nλ+1) = n Ω(cnλ) = n Ω(cdlogn) n d ,
− − − − −
≤
(cid:16) (cid:17)
except with probability 2
nΩ(c)
.
−
14• ℓ A ,(V⊺) kT is small with high probability:
k=T+1|h k ⊗ k i|
Combining Fact 3.6 with the fact that A is bounded inside Bm(B), we have that
P ′
1/2
k+m 1
kA
k k2
= kE
x
∼A′[H k(x)] −E
x
∼Nm[H k(x)]
k2
= kE
x
∼A′[H k(x)]
k2
≤
2O(m)
m
−
1
exp(B2/4) .
(cid:18) − (cid:19)
We pick a = 2. Note that ak/2 > T = c n2α n2α. Applying Lemma 3.7 and Corollary 3.9
1
≥
yields
E [ (V⊺) kT a] =E V⊺u ak/2 T a = E V⊺u ak/2
V ∼U(On,m) k ⊗ k k2 V ∼U(On,m) k k2 k k k2 V ∼U(On,m) k k2
h i n2α+n (n −m)/2 h i
=exp( Ω(n2αlogn))O .
− k+n
!
(cid:18) (cid:19)
Applying Markov’s inequality yields the tail bound
Pr (V⊺) kT 2 Ω(n2αlogn)O
n2α+n (n −m)/4
2 Ω(n2α) = 2 Ω(n2c) = 2 nΩ(c) .
⊗ k 2 − − − −
k k ≥ k+n ≤
" !#
(cid:18) (cid:19)
Therefore, we have
ℓ
∞ ∞
A ,(V⊺) kT A ,(V⊺) kT A (V⊺) kT
k ⊗ k k ⊗ k k 2 ⊗ k 2
|h i| ≤ |h i| ≤ k k k k
k=T+1 k=T+1 k=T+1
X X X
∞ 2O(m) k+m −1 1/2 exp(B2/4)2 −Ω(n2αlogn)O n2α+n (n −m)/4
≤ m 1 k+n
!
k=T+1 (cid:18) − (cid:19) (cid:18) (cid:19)
X
∞
2
Ω(n2αlogn) k+m m/2 n2α+n n/8
,
−
≤ T +m k+n
k=T (cid:18) (cid:19) (cid:18) (cid:19)
X
where the last inequality follows from our choice of parameters. Therefore, we have that
ℓ |hA k,(V⊺) ⊗kT
k
i|≤
∞ 2 −Ω(n2αlogn) 1+ Tk − +mT m/2 1+ nk 2− α+n2 nα −n/8
k=T+1 k=T (cid:18) (cid:19) (cid:18) (cid:19)
X X
(m/2)(2n/T) n/8
∞
2
−Ω(n2αlogn)
1+
k −T
1+
k −T −
≤ n2α+n n2α+n
k=T (cid:18) (cid:19) (cid:18) (cid:19)
X
n/8+nm/T
∞
2
−Ω(n2αlogn)
1+
k −T −
≤ n2α+n
k=T (cid:18) (cid:19)
X
∞
2
−Ω(n2αlogn) n2α+n −T +k −n/16
≤ n2α+n
k=T (cid:18) (cid:19)
X
2
−Ω(n2αlogn) ∞
n2α+n −T +k −n/16
dk
≤ n2α+n
Zk=T −1(cid:18) (cid:19)
2
Ω(n2αlogn)(n2α+n)n/16
−
=
(n/16 1)(n2α +n 1)n/16 1
−
− −
=2
Ω(n2α)
,
−
except with probability 2
nΩ(c)
.
−
15Adding the four cases above together, we get for any d,m nλ/logn and n is at least a
≤
sufficiently large constant depending on c,
ℓ
Γ(d+m/2)
A ,(V⊺) kT (1+o(1)) 2d/2 n αd+ν +n ((1 λ3)/4)d +n d+2 Ω(n2α)
k ⊗ k − − − − −
|h i| ≤ s Γ(m/2) !
k=1
X
Γ(d/2+m/2)
n ((1 λ)/4 c/2)d +(1+o(1))ν
− − −
≤ Γ(m/2)
(cid:18) (cid:19)
Γ(d/2+m/2)
= n ((1 λ)/4 c)d +(1+o(1))ν ,
− − −
Γ(m/2)
(cid:18) (cid:19)
exceptwithprobability2 nΩ(c) ,wherethelastinequalityabovefollowsfrom Γ(d/2+m/2) 2 O(d) Γ(d+m/2) .
− Γ(m/2) ≥ − Γ(m/2)
q
3.2 Proof for Proposition 3.1
We are now ready to prove Proposition 3.1 which is the main technical ingredient of our main
result. Proposition 3.1 states that E [f(x)] E [f(x)] is small with high probability.
| x ∼PA V − x ∼Nn |
The main idea of the proof is to use Fourier analysis on E ′[f(x)] as we discussed in the last
x PA
section, where A is the the distribution obtained by truncat∼ ingV A inside Bm(B).
′
Proof for Proposition 3.1. For convenience, we let ζ = (1 λ)/4 c. We will analyze E [H (x)]
x A k
− − ∼
by truncating A. Therefore, we will apply Lemma 3.10 here. However, notice that Lemma 3.10
additionally assumes d,m nλ/logn, ν < 2 and Γ(d/2+m/2) n ζd < 2. We show that all these
≤ Γ(m/2) −
three conditions can be assumed true without loss(cid:16)of generalit(cid:17)y. If either the second or the third
condition is not true, then our lower bound here is trivialized and is always true since f is bounded
ib te it sw ee aen sy[
−
to1, s+ ee1] f. orFo ar nyd, sm
uffi≤
cin eλ n/ tll yog lan r, geco nns did ee pr ena dλ i′ n>
g
oλ ns (u 1cht λha )/t 4(1
−
ζλ ,′) w/ e4
−
haζ ve= d( ,1 m−λ 2)/4 − nζ λ′. /T loh ge nn
− − ≤
and ζ (1 λ)/4 ζ. Therefore, we can apply Lemma 3.10 for λ.
′
≤ − −
Now let B = nα where α < (1 λ)/4 is the constant in Lemma 3.10. Then we consider the
−
truncated distribution A defined as the distribution of x A conditioned on x Bm(B). By
′
∼ ∈
Lemma 3.10, we have d (A,A) Γ(d/2+m/2) n ζd. Given that f is bounded between [ 1,1],
TV ′ ≤ Γ(m/2) − −
thisimplies |E
x ∼PA
V[f(x)] −E
x ∼PA
V′[f(cid:16) (x)]
| ≤
2d TV(cid:17) (PA V,PA V′ ′) = 2d TV(A,A ′)
≤
2 Γ(d Γ/ (2 m+ /m 2)/2) n −ζd.
Thu Ls, etit ℓs =uffi ℓce (s n)for u Ns bto
e
a an fa ul ny cz te ioE nx d∼ eP pVA e′ n[f d( ix n) g] oin ns lytea od
n
o thf eE qx u∼ eP rA V y[f fu( nx c) t].
ion f
an(cid:16)
d the
dimen(cid:17)
sion n
f
∈
(ℓ to be specified later). By Lemma 3.3, we have that
ℓ
E
x ∼PA
V′[f(x)] = |hA k,(V⊺) ⊗kT
k
i|+E
x ∼PA
V′[f>ℓ(x)] .
k=0
X
Recall that we want to bound |E
x ∼PA
V′[f(x)] −E
x
∼Nn[f(x)]
|
with high probability, where we note
that E [f(x)] = A ,T . Therefore, we can write
x ∼Nn h 0 0 i
ℓ
|E
x ∼PA
V′[f(x)] −E
x
∼Nn[f(x)]
| ≤
(cid:12)
k=1hA k,(V⊺) ⊗kT
k i
(cid:12)+ |E
x ∼PA
V′[f>ℓ(x)]
|
.
(cid:12)X (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
16For the first term, by Lemma 3.10, we have that
ℓ
Γ(d/2+m/2)
A ,(V⊺) kT = n ζd+(1+o(1))ν ,
k ⊗ k −
(cid:12) h i(cid:12) Γ(m/2)
(cid:12)Xk=1 (cid:12) (cid:18) (cid:19)
(cid:12) (cid:12)
(cid:12) (cid:12)
except with probab(cid:12)ility 2 nΩ(c) . (cid:12)
−
Itnowremainsforustoshowthat E ′[f>ℓ(x)] isalsosmallwithhighprobability. Consider
the distribution D = E
[PA| ′ ].x ∼ TPA V
he
followin|
g lemma shows that D is continuous and
v U(On,m) V
χ2(D, ) is at most a con∼ stant only dependingon n (independentof the choice of the distribution
n
N
A).
Lemma 3.11. Let A be any distribution supported on Bm(n) for n N which is at least a suffi-
ciently large universal constant. Let D = E
[PA′
]. Then,
D∈
is a continuous distribution
V U(On,m) V
and χ2(D, ) = O (1). ∼
n n
N
Roughly speaking, the proof of the lemma follows by noting that the average over V of PA is
V
spherically symmetric. We defer its proof to Appendix B.2. Using this lemma, we can write
E V ∼U(On,m) E x ∼PA V′[f>ℓ(x)] ≤ E V ∼U(On,m) E x ∼PA V′ |f>ℓ(x) | = E x ∼D |f>ℓ(x) |
(cid:2)(cid:12) (cid:12)(cid:3) D((cid:2)x) (cid:2) (cid:3)(cid:3) (cid:2) (cid:3)
(cid:12) (cid:12) = E f>ℓ(x) 1+χ2(D, ) f>ℓ ,
x ∼Nn (cid:20)Nn(x) | |
(cid:21)
≤ Nn k k2
p
where the first equality holds due to Fubini’s theorem. Furthermore, since χ2(D, ) = O (1),
n n
N
there is a function δ :R R such that 1+χ2(D, ) δ(n). Therefore, we have that
n
→ N ≤
E
V ∼U(On,m)
E
x ∼PA
V′[f>ℓ(x)]
≤
1+χ2(D, Nn) kf>ℓ
k2 ≤
δ(n) kf>ℓ
k2
.
(cid:2)(cid:12) (cid:12)(cid:3) p
Wecan take ℓ = ℓ (n)(ℓ o(cid:12)nlydependson t(cid:12)hequeryfunctionf anddimensionn)tobeasufficiently
f
large function such that f>ℓ e−n Γ(d/2+m/2) n ζd. Then we get
k k2 ≤ δ(n) Γ(m/2) −
(cid:16) (cid:17)(cid:16) (cid:17)
Γ(d/2+m/2)
E
V ∼U(On,m)
E
x ∼PA
V′[f>ℓ(x)] ≤δ(n) kf>ℓ
k2 ≤
e −n
(cid:18)
Γ(m/2)
(cid:19)n −ζd .
(cid:2)(cid:12) (cid:12)(cid:3)
(cid:12) (cid:12)
This gives the tail bound Pr
V ∼U(On,m)
E
x ∼PA
V′[f>ℓ(x)]
≥
Γ(d Γ/ (2 m+ /m 2)/2) n −ζd
≤
e −n.
Using the above upper bounds, we (cid:2)h (cid:12)ave (cid:12) (cid:16) (cid:17) (cid:3)
(cid:12) (cid:12)
E
x ∼PA
V′[f(x)] −E
x
∼Nn[f(x)]
≤
ℓ k=1hA k,(V⊺) ⊗kT
k i
+ |E
x ∼PA
V′[f>ℓ(x)]
|
(cid:12)
(cid:12)
(cid:12)
(cid:12) =
2(cid:12) (cid:12)PΓ(d/2+m/2)
n
ζ(cid:12)
(cid:12)d+(1+o(1))ν
,
(cid:12) − (cid:12)
Γ(m/2)
(cid:18) (cid:19)
except with probability 2
nΩ(1)
. As we have argued at the beginning of the proof,
−
Γ(d/2+m/2)
|E
x ∼PA
V[f(x)] −E
x ∼PA
V′[f(x)]
| ≤
2
(cid:18)
Γ(m/2)
(cid:19)n −ζd .
Therefore,
Γ(d/2+m/2)
E [f(x)] E [f(x)] 3 n ζd+(1+o(1))ν ,
| x ∼PA V − x ∼Nn | ≤ Γ(m/2) −
(cid:18) (cid:19)
17except with probability 2
nΩ(1)
< 2
nΩ(c)
given c = O(1).
− −
Intheend,noticethattheaboveargumentisstilltrueifwetakeζ > ζ suchthat(1 λ)/4 ζ =
′ ′
− −
(1 λ)/4 ζ
− 2 − . Using the above argument for ζ ′ and given n is a sufficiently large constant depending
on (1 λ)/4 ζ = 2((1 λ)/4 ζ ), we get
′
− − − −
Γ(d/2+m/2)
E [f(x)] E [f(x)] n ζd+(1+o(1))ν ,
| x ∼PA V − x ∼Nn | ≤ Γ(m/2) −
(cid:18) (cid:19)
except with probability 2
nΩ((1−λ)/4−ζ′
)
= 2
nΩ(c)
. Replaceing ζ with (1 λ)/4 c completes the
− −
− −
proof.
References
[BKS+06] G. Blanchard, M. Kawanabe, M. Sugiyama, V. Spokoiny, and K.-R. Mu¨ller. In search
of non-gaussian components of a high-dimensional distribution. Journal of Machine
Learning Research, 7(9):247–282, 2006.
[BRST21] J. Bruna, O. Regev, M. J. Song, and Y. Tang. Continuous LWE. In 53rd Annual ACM
SIGACT Symposium on Theory of Computing, pages 694–707. ACM, 2021.
[CLL22] S.Chen,J.Li,andY.Li. Learning(very)simplegenerative modelsishard. InNeurIPS,
2022.
[DH23] J. Ding and Y. Hua. SQ lower bounds for random sparse planted vector problem. In
International Conference on Algorithmic Learning Theory, volume 201 of Proceedings
of Machine Learning Research, pages 558–596. PMLR, 2023.
[DK22a] I. Diakonikolas and D. Kane. Near-optimal statistical query hardness of learning halfs-
paces withmassartnoise. InConference on Learning Theory, volume178 of Proceedings
of Machine Learning Research, pages 4258–4282. PMLR, 2022. Full version available
at https://arxiv.org/abs/2012.09720.
[DK22b] I. Diakonikolas and D. Kane. Non-gaussian component analysis via lattice basis re-
duction. In Conference on Learning Theory, volume 178 of Proceedings of Machine
Learning Research, pages 4535–4547. PMLR, 2022.
[DK23] I. Diakonikolas and D. M. Kane. Algorithmic High-Dimensional Robust
Statistics. Cambridge university press, 2023. Full version available at
https://sites.google.com/view/ars-book.
[DKK+22] I. Diakonikolas, D. M. Kane, V. Kontonis, C. Tzamos, and N. Zarifis. Learning general
halfspaces with general massart noise under the gaussian distribution. In STOC ’22:
54th Annual ACM SIGACT Symposium on Theory of Computing, pages 874–885, 2022.
Full version available at https://arxiv.org/abs/2108.08767.
[DKKZ20] I. Diakonikolas, D. M. Kane, V. Kontonis, and N. Zarifis. Algorithms and SQ lower
bounds for PAC learning one-hidden-layer relu networks. In Conference on Learning
Theory, COLT 2020, volume 125 of Proceedings of Machine Learning Research, pages
1514–1539. PMLR, 2020.
18[DKP+21] I. Diakonikolas, D. M. Kane, A. Pensia, T. Pittas, and A. Stewart. Statistical query
lower bounds for list-decodable linear regression. In Advances in Neural Information
Processing Systems 34: Annual Conference on Neural Information Processing Systems
2021, NeurIPS 2021, pages 3191–3204, 2021.
[DKPZ21] I. Diakonikolas, D. M. Kane, T. Pittas, and N. Zarifis. The optimality of polynomial
regression for agnostic learning under gaussian marginals in the SQ model. In Confer-
ence on Learning Theory, COLT 2021, volume 134 of Proceedings of Machine Learning
Research, pages 1552–1584. PMLR, 2021.
[DKPZ23] I. Diakonikolas, D. M. Kane, T. Pittas, and N. Zarifis. SQ lower bounds for learning
mixtures of separated and bounded covariance gaussians. In The Thirty Sixth Annual
Conference on Learning Theory, COLT 2023, volume 195 of Proceedings of Machine
Learning Research, pages 2319–2349. PMLR, 2023.
[DKS17] I. Diakonikolas, D. M. Kane, and A. Stewart. Statistical query lower bounds for robust
estimation of high-dimensional gaussians and gaussian mixtures. In 58th IEEE Annual
Symposium on Foundations of Computer Science, FOCS 2017, pages 73–84, 2017. Full
version at http://arxiv.org/abs/1611.03473.
[DKS18] I. Diakonikolas, D. M. Kane, and A. Stewart. List-decodable robust mean estimation
and learning mixtures of spherical gaussians. In Proceedings of the 50th Annual ACM
SIGACT Symposium on Theory of Computing, STOC 2018, pages 1047–1060, 2018.
Full version available at https://arxiv.org/abs/1711.07211.
[DKS19] I. Diakonikolas, W. Kong, and A. Stewart. Efficient algorithms and lower bounds for
robustlinearregression. InProceedings of the Thirtieth Annual ACM-SIAMSymposium
on Discrete Algorithms, SODA 2019, pages 2745–2754, 2019.
[DKS23] I. Diakonikolas, D. M. Kane, and Y. Sun. SQ lower bounds for learning mixtures of
linear classifiers. CoRR, abs/2310.11876, 2023.
[DKZ20] I. Diakonikolas, D. M. Kane, and N. Zarifis. Near-optimal SQ lower bounds for agnos-
tically learning halfspaces and relus under gaussian marginals. CoRR, abs/2006.16200,
2020.
[Fel16] V.Feldman. Statisticalquerylearning. InEncyclopedia of Algorithms,pages2090–2095.
Springer New York, 2016.
[FGR+17] V. Feldman, E. Grigorescu, L. Reyzin, S. Vempala, and Y. Xiao. Statistical algorithms
and a lower bound for detecting planted cliques. J. ACM, 64(2):8:1–8:37, 2017.
[FGV17] V. Feldman, C. Guzman, and S. S. Vempala. Statistical query algorithms for mean
vector estimation and stochastic convex optimization. In Proceedings of the Twenty-
Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, SODA 2017, pages
1265–1277. SIAM, 2017.
[GGK20] S. Goel, A. Gollakota, and A. R. Klivans. Statistical-query lower bounds via func-
tional gradients. In Advances in Neural Information Processing Systems 33: Annual
Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020.
19[GS19] N. Goyal and A. Shetty. Non-gaussian component analysis using entropy methods. In
Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing,
STOC 2019, pages 840–851. ACM, 2019.
[Kea98] M. J. Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the
ACM, 45(6):983–1006, 1998.
[Kra04] I. Krasikov. New bounds on the Hermite polynomials. arXiv preprint math/0401310,
2004.
[MV18] M. Meister and G. Valiant. A data prism: Semi-verified learning in the small-alpha
regime. In Conference On Learning Theory, COLT 2018, volume 75 of Proceedings of
Machine Learning Research, pages 1530–1546. PMLR, 2018.
[SVWX17] L. Song, S. S. Vempala, J. Wilmes, and B. Xie. On the complexity of learning neural
networks. In Advances in Neural Information Processing Systems 30: Annual Confer-
ence on Neural Information Processing Systems 2017, NeurIPS 2017, pages 5514–5522,
2017.
[SZB21] M. J. Song, I. Zadik, and J. Bruna. On the cryptographic hardness of learning sin-
gle periodic neurons. In Advances in Neural Information Processing Systems 34: An-
nual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, pages
29602–29615, 2021.
[TV18] Y. S. Tan and R. Vershynin. Polynomial time and sample complexity for non-gaussian
component analysis: Spectral methods. In Conference On Learning Theory, COLT
2018, volume 75 of Proceedings of Machine Learning Research, pages 498–534. PMLR,
2018.
[Val84] L. Valiant. A theory of the learnable. Communications of the ACM, 27(11):1134–1142,
1984.
[ZSWB22] I. Zadik, M. J. Song, A. S. Wein, and J. Bruna. Lattice-based methods surpass sum-of-
squares in clustering. In Conference on Learning Theory, volume 178 of Proceedings of
Machine Learning Research, pages 1247–1248. PMLR, 2022.
20Appendix
A Omitted Proofs from Section 2
A.1 Proof of Claim 2.3
Proof. The proof of Claim 2.3 is obtained via the following calculation, using the definition of
Hermite tensor (Definition 2.2). We will use i,j for indexes in [d].
√k!H (Bx) = ( I ) (Bx)
k i1,...,ik
−
ia,ib ic
PartitionsP of[k] a,b P c P
intosetsoX fsize1and2{ O}∈ {O}∈
= ( (BIBT) ) (Bx)
−
ia,ib ic
PartitionsP of[k] a,b P c P
X { O}∈ {O}∈
intosetsofsize1and2
n n
= B I B⊺ B x
− ia,ja ja,jb jb,ib  ic,jc jc
Partitio XnsP of[k] {a O,b }∈P jaX,jb=1 {Oc }∈P j Xc=1
intosetsofsize1and2     
n
= B ( I )B⊺ (B x )
ia,ja − ja,jb jb,ib ic,jc jc
intP oa sr et ti sti oo X fns siP zeo 1f a[k n] d2j1,. X..,jk=1 {a O,b }∈P (cid:16) (cid:17) {Oc }∈P
n
= B ( I ) x
il,jl
−
ja,jb jc
Partitio XnsP of[k] j1,. X..,jk=1l O∈[k] {a O,b }∈P {Oc }∈P
intosetsofsize1and2
n
= B ( I ) x
il,jl
−
ja,jb jc
j1,. X..,jk=1l O∈[k] Partitio XnsP of[k] {a O,b }∈P {Oc }∈P
intosetsofsize1and2
n
= B √k!H (x)
il,jl k j1,...,jk
j1,. X..,jk=1l O∈[k]
n
=√k! (B k) H (x) ,
⊗ i1,...,ik,j1,...,jk k j1, ···,jk
j1,. X..,jk=1
where the fourth and fifth equalities follow from the fact that P is a partition of [k], so changing
the order of summation and multiplication gives exactly n and B . The seventh
equality follows from the definition of the Hermite tensor.
j T1,. h.. e,jk a= b1
ove is
eql ∈u[ ik v] alei nl, tjl
to H (Bx) =
k
P N
B kH (x). This completes the proof.
⊗ k
B Omitted Proofs from Section 3
B.1 Omitted Proofs from Section 3.1
B.1.1 Proof of Lemma 3.2
Proof. We construct the truncated distribution A as follows. We first sample x A, then we
′
∼
reject x unless x B. Let A be the distribution of the samples obtained from this process.
2 ′
k k ≤
21Firstnotice thatwewillusethemomentboundtoboundthetotal variation distance, as follows
E [ x d] E [ x d]+νE [ x 2d]1/2
x ∼A k k2 ≤ x ∼Nm k k2 x ∼Nm k k2
=E [td/2]+νE [td]1/2
t χ2(m) t χ2(m)
∼ ∼
Γ((d+m)/2) Γ((2d+m)/2)
=2d/2 +2d/2 ν
Γ(m/2) s Γ(m/2)
Γ(d+m/2)
c 2d/2 ,
2
≤ s Γ(m/2) !
where c is a universal constant. Using Markov’s inequality and the union bound, we have
2
Γ(d+m/2)
Pr [x Bm(B)] c 2d/2 B d .
x A 2 −
∼ 6∈ ≤ s Γ(m/2) !
By the definition of A, we have that d (A,A) c 2d/2 Γ(d+m/2) B d.
′ TV ′ ≤ 2 Γ(m/2) −
Then it only remains to verify kE x ∼A′[H k(x)] −(cid:16)E x ∼Nq m[H k(x)] k2(cid:17)for any k < d and k
≥
d
respectively. For k = 0, it is immediate that kE
x
∼A′[H k(x)]
−
E
x
∼Nm[H k(x)]
k2
= 0. There-
fore, we only consider 1 k < d. We first look at E [H (x)] E [H (x)] . Suppose
≤ k x ∼A k − x ∼Nm k k2
E [H (x)] E [H (x)] > ν. Then it is easy to see that the polynomial
k x ∼A k − x ∼Nm k k2
E [H (x)] E [H (x)]
f = Ex ∼A [Hk
(x)]
− Ex ∼Nm [Hk
(x)]
,H k(x)
(cid:28)k x ∼A k − x ∼Nm k k2 (cid:29)
satisfies the requirement, that f is at most degree-d, E [f(x)2] = 1 and E [f(x)]
x ∼Nm | x ∼A −
E [f(x)] > ν . This leads to a contradiction. Thus, E [H (x)] E [H (x)] ν.
Thx ∼ enNm
to
boun|
d E
x
A′[H k(x)] E
x
A[H k(x)] 2, let α =
Pk
r
xx ∼ AA
[x
k Bm(− B)].x ∼ WN em cank writk e2 ≤
k ∼ − ∼ k ∼ 6∈
E
x
A′[H k(x)] E
x
A[H k(x)]
2
k ∼ − ∼ k
1
= E [H (x)1(x Bm(B))] E [H (x)]
x A k x A k
1 α ∼ ∈ − ∼
(cid:13) − (cid:13)2
(cid:13) α 1 (cid:13)
=(cid:13)
(cid:13) E x A[H k(x)] E x A[H k(x)1(x /
Bm(cid:13)
(cid:13)(B))]
1 α ∼ − 1 α ∼ ∈
(cid:13) − − (cid:13)2
(cid:13) 1 α (cid:13)
(cid:13) E [H (x)1(x Bm(B))] + E [H ((cid:13)x)]
≤1(cid:13) α k x ∼A k 6∈ k2 1 αk x ∼A k(cid:13) k2
− −
1 α
E [H (x)1(x Bm(B))] + ν ,
x A k 2
≤1 αk ∼ 6∈ k 1 α
− −
wherethelastinequalityfollowsfrom E [H (x)] E [H (x)] ν andE [H (x)] = 0
k x ∼A k − x ∼Nm k k2 ≤ x ∼Nm k
foranyk > 0. SinceBd c 2d/2 Γ(d+m/2) (c isatleastasufficiently largeuniversalconstant),
≥ 1 Γ(m/2) 1
we have α c 2d/2 Γ(d+(cid:16)m/2) q B d 1/(cid:17) 2. Also, we have Bd c 2d/2 Γ(d+m/2) implies
≤ 2 Γ(m/2) − ≤ ≥ 1 Γ(m/2)
B2 m. We will(cid:16)needqthe followi(cid:17)ng fact: (cid:16) q (cid:17)
≥
Fact B.1. Let H be the k-th Hermite tensor for m-dimension. Suppose x m1/4, then
k 2
k k ≥
H (x) = 2O(k) x k.
k k k2 k k2
22Proof. For a degree-k tensor A, we use Aπ to denote thematrix that Aπ = A . Notice
A = Aπ . Then from the definition of Hermite tensor, we have
i1, ···,ik π(i1, ···,ik)
2 2
k k k k
k/2
1 ⌊ ⌋ 1 π
H(x) = ( I) tx (k 2t) .
⊗ ⊗ −
√k! 2tt!(k 2t)! −
Xt=0 Permuta Xtionπ of[k] − (cid:16) (cid:17)
This implies that
H (x)]
k 2
k k
k/2
1 ⌊ ⌋ 1 π
= ( I) tx (k 2t)
⊗ ⊗ −
(cid:13)√k! 2tt!(k 2t)! − (cid:13)
(cid:13) (cid:13) Xt=0 Permuta Xtionπ of[k] − (cid:16) (cid:17) (cid:13) (cid:13)2
(cid:13) (cid:13)
(cid:13)⌊k/2 ⌋ √k! (cid:13)
(cid:13) max( I t x k 2t,1) (cid:13)
≤ 2tt!(k 2t)! k ⊗ k2 k k2−
t=1 −
X
k/2
⌊ ⌋ √k!
= max(mt/2 x k 2t,1)
2tt!(k 2t)! k
k2−
t=1 −
X
k/2
⌊ ⌋ √k!
= x k .
k k2 2tt!(k 2t)!
t=1 −
X
One can see that the denominator is minimized when t = k/2 O(√k). Then it follows that the
−
sum is at most 2O(k) x k.
k k2
Since B2 m, applying Jensen’s inequality and Fact B.1, we have
≥
E [H (x)1(x Bm(B))] E [ H (x) 1(x Bm(B))]
x A k 2 x A k 2
k ∼ 6∈ k ≤ ∼ k k 6∈
E [2O(k)max(1, x k)1(x Bm(B))]
≤ x ∼A k k2 6∈
2O(k)E [ x k1(x Bm(B))]
≤ x ∼A k k2 6∈
2O(k) ∞ Pr[ x u x Bm(B)]duk
2
≤ k k ≥ ∧ 6∈
Z0
Γ(d+m/2)
2O(k) ∞ 2d/2 min(B d,u d)duk
− −
≤ Z0 s Γ(m/2) !
Γ(d+m/2) B
2O(k) 2d/2 B dduk + ∞ u dduk
− −
≤ s Γ(m/2) ! (cid:18)Z0 ZB (cid:19)
Γ(d+m/2)
2O(k) 2d/2 B (d k) .
− −
≤ s Γ(m/2) !
Plugging it back gives
E
x
A′[H k(x)] E
x
A[H k(x)]
2
k ∼ − ∼ k
Γ(d+m/2) Γ(d+m/2)
2O(k) 2d/2 B (d k)+ 1+O 2d/2 B (d k) ν ,
− − − −
≤ s Γ(m/2) ! s Γ(m/2) ! !
23for k < d.
Aπ
N toow dew noe tebo tu hn ed
mk
aE trx
i∼
xA t′ h[H atk A(x π)] −E x
=∼N
Ad[H k(x)] k2 anf dor k
A≥
d =. F Aor πa d .e Fg ore re t- hk ete cn ass eor thA a, tw ke us de
,
from the definition of Hermite
tei n1, s·o··r,i ,k
we
haπ v( ei1, ···,ik) k k2 k k2 ≥
k/2
1 ⌊ ⌋ 1 π
H(x) = ( I) tx (k 2t) .
⊗ ⊗ −
√k! 2tt!(k 2t)! −
Xt=0 Permuta Xtionπ of[k] − (cid:16) (cid:17)
This implies that
kE
x
∼A′[H k(x)] −E
x
∼Nm[H k(x)]
k2
= E
x
A′[H k(x)]
2
k ∼ k
k/2
1 ⌊ ⌋ 1 π
= (cid:13)√k! 2tt!(k 2t)! ( −I) ⊗tE x ∼A′ x ⊗(k −2t) (cid:13)
(cid:13) (cid:13) Xt=0 Permuta Xtionπ of[k] − (cid:16) h i(cid:17) (cid:13) (cid:13)2
(cid:13) (cid:13)
(cid:13)⌊k/2 ⌋ √k! (cid:13)
≤(cid:13) 2tt!(k 2t)!kI ⊗t k2 E x ∼A′ x ⊗(k −2t)
2
(cid:13)
Xt=1 − (cid:13) h i(cid:13)
(cid:13) (cid:13)
⌊k/2 ⌋ √k! (cid:13) (cid:13)
≤ 2tt!(k 2t)!kI ⊗t k2E x ∼A′ kx k2k −2t
Xt=1 − h i
k/2
⌊ ⌋ √k!
= 2tt!(k 2t)!mt/2Bmax(k −2t −d,0)E x ∼A′ kx k2min(k −2t,d)
Xt=1 − h i
k/2
⌊ ⌋ √k!
≤Bk −d 2tt!(k 2t)!mmax(0,2t −k+d)/4E x ∼A′ kx k2min(k −2t,d) ,
Xt=1 − h i
wherethelastinequalityfollowsfrom4(t/2)+max(k 2t d,0) = (k d)+4(max(0,2t k+d)/4)and
− − − −
B2 > m(whichisimpliedbyBd c 2d/2 Γ(d+m/2) ). SinceA isobtainedbytruncatinganyx
≥ 1 Γ(m/2) ′ 6∈
Bm(B) and Pr[x
6∈
Bm(B)]
≤
1/2, w(cid:16) e havq e E x ∼A′ k(cid:17)x km 2ax(k −2t,d) = O E x ∼A kx k2max(k −2t,d) .
Thus, we have h i (cid:16) h i(cid:17)
kE
x
∼A′[H k(x)] −E
x
∼Nm[H k(x)]
k2
k/2
⌊ ⌋ √k! Γ(min(k 2t,d)+m/2)
Bk −d mmax(0,2t −k+d)/4O 2min(k −2t,d)/2 −
≤ 2tt!(k 2t)! s Γ(m/2) !
t=1 −
X
k/2
Γ(d+m/2) ⌊ ⌋ √k!
Bk dO 2d/2 ,
−
≤ s Γ(m/2) ! 2tt!(k 2t)!
t=1 −
X
where the second inequality follows from max(0,2t k + d) + min(k 2t,d) = d. One can see
− −
the denominator is minimized when t = k/2 O(√k). Then it follows that the sum is at most
−
2O(k)Bk d 2d/2 Γ(d+m/2) .
− Γ(m/2)
This co(cid:16)mpleqtes the pro(cid:17)of.
24B.1.2 Proof of Fact 3.5
Proof. For a degree-k tensor A, we use Aπ to denote thematrix that Aπ = A . Notice
A = Aπ . Then from the definition of Hermite tensor, we have
i1, ···,ik π(i1, ···,ik)
2 2
k k k k
k/2
1 ⌊ ⌋ 1 π
H(x) ( I) tx (k 2t)
k k2 ≤ √k! (cid:13) 2tt!(k 2t)! − ⊗ ⊗ − (cid:13)
Xt=0 (cid:13) (cid:13)Permuta Xtionπ of[k] − (cid:16) (cid:17) (cid:13) (cid:13)2
(cid:13) (cid:13)
1 ⌊k/2 ⌋(cid:13) 1 (cid:13)
(cid:13) I t x k 2t (cid:13)
≤ √k! 2tt!(k 2t)! k k2k k2−
t=0 − Permutationπ of[k]
X X
k/2
1 ⌊ ⌋ k!
= mt/2 x k 2t
√k! 2tt!(k 2t)! k
k2−
t=0 −
X
mk/4 ⌊k/2 ⌋ k
2
t kx k2k −2t
≤ √k! t!
t=0 (cid:0) (cid:1)
X
k
2kmk/4Bkk k/2exp /B2 .
−
≤ 2
(cid:18)(cid:18) (cid:19) (cid:19)
B.1.3 Proof of Fact 3.6
Proof. We will need the following fact for the proof.
Fact B.2 ([Kra04]). Let h
k
be the k-th normalized Hermite polynomial. Then max
t
Rh2 k(t)e −t2/2 =
O(k 1/6). ∈
−
First note that using Lemma 2.3, we have for any orthogonal B Rm m,
×
∈
H (x) = B kH (x) = H (Bx) .
k 2 ⊗ k 2 k 2
k k k k k k
By taking the appropriate B, we can always have Bx = x e . Therefore, without loss of gener-
2 1
k k
ality, we can assume x = te for some t R.
1
∈
Notice that for any entry H (x) , let j for ℓ 1, ,m be the number of times ℓ appears
k i1, ···,ik ℓ
∈ ···
in i , ,i . Note that j = k. To bound the norm of H (x) notice
1 ··· k ℓ ℓ k
P 1/2 m
k −
H (x) = h (x ).
k i1, ···,ik
(cid:18)j 1,
···
,j
m(cid:19)
ℓ=1
jℓ ℓ
Y
25Therefore,
1 m 2
H (x) 2 = k − h (x )
k k k2 j , ,j jℓ ℓ
i1X, ···,ik(cid:18) 1 ··· m(cid:19) ℓ Y=1 !
m 2
= h (x )
jℓ ℓ
!
j1, ···,jm suc Xhthat P ℓjℓ=k ℓ Y=1
m
O(exp(x2/2))
≤ ℓ
j1, ···,jm suc Xhthat P ℓjℓ=kℓ Y=1
k+m 1
2O(m) − exp( x 2/2) ,
≤ m 1 k k2
(cid:18) − (cid:19)
where the first inequality follows from Fact B.2. Then we can take square root on both sides which
gives what we want.
B.1.4 Proof of Lemma 3.8
Proof. ToboundE [ Vu k],bysymmetry,wecaninsteadconsiderV = [e ,e , ,e ]⊺
and u U(Sn 1). TV⊺ h∼ eU n( wOn e,m h) avk e thk a2 t 1 2 ··· m
−
∼
E u ∼U(Sn−1) kVu kk 2 =E u ∼U(Sn−1) ( kVu k2/ ku k2)k = E u ∼U(Sn−1) kVu k2 2/ ku k2 2 k/2
h i h i h i
=E
t
Beta(m,n−m) tk/2 , (cid:0) (cid:1)
∼ 2 2
h i
wherethelastequationfollowsfromthestandardfactthatifX χ2 andY χ2 areindependent
∼ d1 ∼ d2
then XX +Y ∼ Beta(d 1/2,d 2/2). By Stirling’s formula, we can bound E t ∼Beta(m 2,n− 2m) tk/2 as
follows:
(cid:2) (cid:3)
E t ∼Beta(m 2,n− 2m) htk/2
i
= B m 2,1 n −2m Zt=1 0t(k+ 2m −1)(1 −t)(n− 2m −1)dt
=
B(cid:0)k+ 2m,n −2(cid:1)m
= Θ
Γ k+ 2m Γ n
2 .
B (cid:0) m 2,n −2m (cid:1) Γ (cid:0)k+ 2n (cid:1)Γ (cid:0)m 2(cid:1)!
(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
B.1.5 Proof of Corollary 3.9
Proof. We will proceed by case analysis. We first consider the case where both n and m are even
integers. By definition of the Γ function, we have that
Γ k+m Γ n n 1 n 2 m
E [ V⊺u k] =Θ 2 2 = Θ 2 − 2 − ··· 2
V ∼U(On,m) k k2 Γ (cid:0)k+ 2n (cid:1)Γ (cid:0)m 2(cid:1)! k+ 2(cid:0)n −1 (cid:1)(cid:0)k+ 2n −(cid:1)2 ··(cid:0)
·
k (cid:1)+ 2m !
k(cid:0)+m (cid:1)1(cid:0) k+(cid:1)m 2 (cid:0) m (cid:1)(cid:0) k+(cid:1)m (cid:0)k/2 (cid:1)
=Θ 2 − 2 − ··· 2 O
(cid:0) k+ 2n −1 (cid:1)(cid:0)k+ 2n −2 (cid:1) ··· (cid:0)n 2 (cid:1)! ≤ (cid:18)k+n (cid:19) !
k/2
(cid:0) max(cid:1)((cid:0)k,m) (cid:1) (cid:0) (cid:1)
=O 2k/2 . (2)
n
!
(cid:18) (cid:19)
26For m nc < k for some constant c (0,1), we have that
≤ ∈
Γ k+m Γ n
E [ V⊺u k]= Θ 2 2
V ∼U(On,m) k k2 Γ (cid:0)k+ 2n (cid:1)Γ (cid:0)m 2(cid:1)!
n 1 n 2 m(cid:0) (cid:1) (cid:0) (cid:1)nc+n 1 nc+n 2 nc+m
=Θ 2 − 2 − ··· 2 2 − 2 − ··· 2
nc 2+n (cid:0) −1 (cid:1)n (cid:0)c 2+n −(cid:1)2 ··(cid:0)
·
n (cid:1)c+ 2m × (cid:0) k+ 2n −1 (cid:1)(cid:0) k+ 2n −2 (cid:1)
···
(cid:0)k+ 2m (cid:1)!
=O(2n(cid:0)
c/2n (1
c(cid:1) )n(cid:0)
c/2)O
(cid:1)nc+(cid:0)n (n −(cid:1)m)/2 (cid:0) (cid:1)(cid:0) (cid:1) (cid:0) (cid:1)
− −
k+n
!
(cid:18) (cid:19)
=exp( Ω(nclogn))O
nc+n (n −m)/2
, (3)
− k+n
!
(cid:18) (cid:19)
where the third equation follows from (2) by taking k = nc.
For the case where n is even and m is odd, noting that Γ(x+1/2) = Θ(√xΓ(x)), by Stirling’s
approximation we have that
Γ k+m Γ n √mΓ k+m 1 Γ n
E [ V⊺u k]= Θ 2 2 = Θ 2− 2 .
V ∼U(On,m) k k2 Γ (cid:0)k+ 2n (cid:1)Γ (cid:0)m 2(cid:1)! √k+mΓ (cid:0) k+ 2n (cid:1)Γ (cid:0)m 2−(cid:1)1) !
By (2), we have that (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
E [ V⊺u k]= Θ
√mΓ k+m 2−1 Γ n
2 = O 2k/2
max(k,m) k/2
.
V ∼U(On,m) k k2 √k+mΓ (cid:0) k+ 2n (cid:1)Γ (cid:0)m 2−(cid:1)1) ! (cid:18) n (cid:19) !
For m nc < k for some constant c (0,1)(cid:0), by (cid:1)(3)(cid:0)we hav(cid:1)e that
≤ ∈
√mΓ k+m 1 Γ n
E [ V⊺u k] = Θ 2− 2
V ∼U(On,m) k k2 √k+mΓ (cid:0) k+ 2n (cid:1)Γ (cid:0)m 2−(cid:1)1) !
= exp(
Ω(nclog(cid:0) n))O(cid:1) (cid:0)nc+(cid:1)n (n −m+1)/2
− k+n
!
(cid:18) (cid:19)
exp( Ω(nclogn))O
nc+n (n −m)/2
.
≤ − k+n
!
(cid:18) (cid:19)
For the case where n is odd and m is even, by the Stirling approximation, we have that
Γ k+m Γ n √k+nΓ k+m Γ n+1
E [ V⊺u k] = Θ 2 2 = Θ 2 2 .
V ∼U(On,m) k k2 Γ (cid:0)k+ 2n (cid:1)Γ (cid:0)m 2(cid:1)! √nΓ k+ (cid:0)n 2+1 (cid:1)Γ (cid:0)m 2) (cid:1)!
By (2), we have that (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
√k+nΓ k+m Γ n+1
E [ V⊺u k]= Θ 2 2
V ∼U(On,m) k k2 √nΓ k+ (cid:0)n 2+1 (cid:1)Γ (cid:0)m 2) (cid:1)!
k/2
k+n(cid:0) (cid:1)ma(cid:0)x(k,(cid:1)m)
= O 2k/2
n n+1
r (cid:18) (cid:19) !
k/2
max(k,m)
= O 2k/2 .
n
!
(cid:18) (cid:19)
27For m nc < k for some constant c (0,1), by (3), we have that
≤ ∈
√k+nΓ k+m Γ n+1
E [ V⊺u k]= Θ 2 2
V ∼U(On,m) k k2 √nΓ k+ (cid:0)n 2+1 (cid:1)Γ (cid:0)m 2) (cid:1)!
= exp(
Ω(nc(cid:0) logn)O(cid:1) (cid:0) k+(cid:1) n nc+n+1 (n −m+1)/2
− n k+n+1
r (cid:18) (cid:19) !
exp( Ω(nclogn))O
nc+n (n −m)/2
.
≤ − k+n
!
(cid:18) (cid:19)
For the case where both n and m are odd integers, by the Stirling approximation, we have that
Γ k+m Γ n m(k+n)Γ k+m 1 Γ n+1
E [ V⊺u k] = Θ 2 2 = Θ 2− 2 .
V ∼U(On,m) k k2 Γ (cid:0)k+ 2n (cid:1)Γ (cid:0)m 2(cid:1)! pn(k+m)Γ (cid:0)k+n 2+1 (cid:1)Γ (cid:0)m 2−1) (cid:1) !
(cid:0) (cid:1) (cid:0) (cid:1) p (cid:0) (cid:1) (cid:0) (cid:1)
By (2) we have that
m(k+n)Γ k+m 1 Γ n+1
E [ V⊺u k] = Θ 2− 2
V ∼U(On,m) k k2 pn(k+m)Γ (cid:0)k+n 2+1 (cid:1)Γ (cid:0)m 2−1) (cid:1) !
pm(k+n) (cid:0) ma(cid:1)x(k(cid:0),m (cid:1)1) k/2
= O 2k/2 −
sn(k+m) n+1
!
(cid:18) (cid:19)
k/2
max(k,m)
= O 2k/2 .
n
!
(cid:18) (cid:19)
For m nc < k for some constant c (0,1), by (3) we have that
≤ ∈
m(k+n)Γ k+m 1 Γ n+1
E [ V⊺u k]= Θ 2− 2
V ∼U(On,m) k k2 pn(k+m)Γ (cid:0)k+n 2+1 (cid:1)Γ (cid:0)m 2−1) (cid:1) !
=
exp(p Ω(nclogn)O(cid:0) m(cid:1)(k(cid:0)+n) (cid:1)nc+n+1 (n −m+2)/2
− sn(k+m) k+n+1
!
(cid:18) (cid:19)
exp( Ω(nclogn))O
nc+n (n −m)/2
.
≤ − k+n
!
(cid:18) (cid:19)
B.2 Omitted Proofs from Section 3.2
B.2.1 Proof of Lemma 3.11
Proof. Notice that the distribution D is a symmetric distribution. Thus, if we can show that for
x D the distribution x 2 is a continuous distribution, then D is also a continuous distribution.
∼ k k2
Note that the distribution D can be thought of as generated by the following process. To generate
x D, we first sample t A, x . Let u , ,u be an orthonormal basis that
′ ′ n m 1 n m
spa∼ ns the orthogonal compl∼ ement of ∼ spaN n(V− ). We let x·· =· m i− =1t iv
i
+ d i=−11x iu i. Noting that
x 2 = t 2 + x 2, its distribution is the convolution sum of the distribution of t 2 and the
k k2 k k2 k ′ k2 P P k k2
χ2 distribution, which is continuous. Thus, as argued above, D is a continuous distribution.
n m
−
28It remains to argue that χ2(D, ) = O (1). We use D to denote the distribution of x 2
Nn n r2 k k2
aboveandP todenoteitspdf. WeuseS (rSn 1)todenotethesurfaceareaofthen-dimensional
r2 n 1 −
sphere rSn 1 with radius r. Then the pdf− function of D is
−
D(x) = P ( x 2)/S ( x Sn 1).
r2 k k2 n −1 k k2 −
Similarly, the pdf function of is
n
N
(x) = χ2( x 2)/S ( x Sn 1).
Nn n k k2 n −1 k k2 −
Then we have
D(x)2
1+χ2(D, )= dx
n
N Rn n(x)
Z N
P ( x 2)2
= r2 k k2 dx
ZRn χ2 n( kx k2 2)S n −1( kx k2Sn −1)
P ( x 2)2
= ∞ r2 k k2 dxdr
Z0 ZrSn−1 χ2 n( kx k2 2)S n −1( kx k2Sn −1)
P (r2)2
=
∞ r2
dr
χ2(r2)
Z0 n
P (r)2
=
∞ r2
dr .
2√rχ2(r)
Z0 n
Thus, it remains to show that 0∞ 2P √r r2 χ( 2r) (2 r)dr = O n(1).
n
We will first give a pointwise upper bound on P . Notice that D is the convolution sum
R r2 r2
of D and the χ2 distribution, where t 2 is inside [0,n2] since A is supported on Bm 1(n).
Thusk,t kw2 2
e can
writen −m k k2 −
n2
P (r) = P (s)χ2 (r s)ds max χ2 (s).
r2 Z0 kt k2 2 n −m − ≤ s ∈[r −n2,r] n −m
Plugging the pointwise upper bound back, we get
max χ2 (s) 2
1+χ2(D, Nn)
≤
∞ s ∈[ 2r − √n r2 χ,r 2] (rn )−m dr
Z0 (cid:0) n (cid:1)
max s(n m)/2 1e s/2 2
∞ s [r n2,r] − − −
= O n(1) ∈ − dr
rn/2 1/2e r/2
Z0 (cid:0) − − (cid:1)
max sn m 2e s
∞ s [r n2,r] − − −
= O n(1) ∈ − dr
rn/2 1/2e r/2
Z0 − −
O (1) ∞ rn/2 m 3/2e r/2en2 dr
n − − −
≤
Z0
= O (1) ∞ rn/2 m 3/2e r/2dr
n − − −
Z0
= O (1)Γ(n/2 m 1/2) = O (1) .
n n
− −
This completes the proof.
29C Omitted Details on Applications
In this section, we provide additional context on our applications and provide the proofs of Theo-
rems 1.8, 1.10, and 1.13.
C.1 Proof of Theorem 1.8
Proof. This is a direct application of Theorem 1.5. We will let the one-dimensional moment-
matching distribution be the distribution in Fact 1.7. Then any SQ algorithm distinguishing be-
tween
• A standard Gaussian; and
• The distribution PA for v U(Sn 1), where A = α (µ,1)+(1 α)E and µ = 10c α 1/d,
v − d −
∼ N −
with at least 2/3 probability must require either a query of tolerance at most O (n ((1 λ)/4 c)d) or
d − − −
2nΩ(1)
many queries. This proves the SQ lower bound for the NGCA testing problem. However, it
is not clear if there is a simple and optimal reduction from list-decodable Gaussian estimation to
the hypothesis testing problem. Therefore, we will need to directly prove an SQ lower bound for
the search problem.
Consider the following adversary for the search problem. The adversary will let X = PA for
v
v U(Sn 1) be the input distribution, and whenever possible, the adversary will answer a query
−
∼
with E [f(x)]. Given that the algorithm asks less than 2nΩ(1) queries, as we have shown in
x n
∼N
the proof of Theorem 1.5, with 1 o(1) probability, the adversary can always answer E [f(x)].
In such case, the algorithm will b− e left with 1 o(1) probability mess over v U(Sn x 1∼ )N tn hat are
−
− ∼
equally likely.
Then we argue that no hypothesis can be close to more than 2 Ω(n) probability mass. This can
−
be done by upper bounding the surface area of a spherical cap on a n-dimensional sphere, where
the sphereis unit radius and the polar angle of the cap is a sufficiently small constant Φ. Note that
the surface area of such a cap is Θ I n 1, 1 S (Sn 1), where I is the incomplete beta
sin2Φ −2 2 n 1 −
function and S (Sn 1) is the surface area of the n dim− ensional unit sphere. Thus, it suffices to
n 1 −
show that I − n 1,1 = 2 Ω(n). N(cid:0) otice t(cid:0) hat, by(cid:1)(cid:1) its definition, we have
sin2Φ −2 2 −
(cid:0)
n
1(cid:1)
1
sin2Φ
t(n 3)/2(1 t) 1/2dt
I − , = 0 − − −
sin2Φ 2 2 B n 1,1
(cid:18) (cid:19) R −2 2
=
0sin2Φ t(n −(cid:0)3)/2(1 −(cid:1)t) −1/2dt B n −21,1
B n 1,1 B n 1,1
R −2 (cid:0) −2 2(cid:1)
sin2Φ
(cid:0)t(n 3)/(cid:1)2(1 t)0dt (cid:0) (cid:1)
O(1) 0 − −
≤ B n 1,1
R −2
n 1
= O(1)I
sin2Φ
(cid:0) −
2
,1(cid:1) = O(1)(sin2Φ)(n −1)/2 = 2 −Ω(n) .
(cid:18) (cid:19)
Given that no hypothesis can be close to more than 2 Ω(n) probability mass, the only way to have
−
any constant probability of success would be to return 2Ω(n) many hypotheses. This completes the
proof.
30C.2 Proof of Lemma 1.9
Proof. We will take E to be the distribution with density E(t) = (t)+1(t [ 1,1])p(t), where
N ∈ −
p is a polynomial function that we truncate between [ 1,1]. In order to satisfy our requirements,
−
it will suffice to have p(t) 1/10 for all t [ 1,1] and for each integer 0 i d,
| | ≤ ∈ − ≤ ≤
1
E [ti] = (1 α )E [ti]= (1 α )E [ti]+(1 α ) P(t)tidt = E [ti] .
t A d t E d t d t
∼ − ∼ − ∼N − Z−1 ∼N
The second requirement is equivalently stated as follows: for each such i,
1 α
P(t)tidt = d E [ti].
t
Z −1 1 −α d ∼N
In order to satisfy these requirements, we need the following fact from [DK23].
Fact C.1 (Lemma 8.18 in [DK23]). Let C > 0 and m Z . For any a ,a , ,a R, there
+ 0 1 m
∈ ··· ∈
exists a unique degree at most m polynomial p : R R such that for each integer 0 t k we
→ ≤ ≤
have that
C
p(x)xtdx = a .
t
Z−C
Furthermore, for each x [ C,C] we have that p(x) O (max a C t 1).
m o t m t − −
∈ − | | ≤ ≤ ≤
We apply the fact and take C = 1. This implies that there is such a polynomial with p(x)
| | ≤
1/10 for sufficiently small α depending only on d.
d
C.3 Proof of Theorem 1.10
We provide a more detailed statement of Theorem 1.10 here.
Theorem C.2 (SQ Lower Bound for AC Detection). There exists a function f : (0,1/2) N
→
that lim f(α) = and satisfies the following. For any sufficiently small α (0,1/2), any
α 0
→ ∞ ∈
SQ algorithm that has access to a distribution that is either (i) a standard Gaussian; or (ii) a
distribution that has at least α probability mass in a (n 1)-dimensional subspace V Rn, and
− ⊂
distinguishes the two cases with success probability at least 2/3, either requires a query with error
at most O (n f(α)), or uses at least 2nΩ(1) many queries.
α −
Proof of Theorem C.2. Thisis adirectapplication of Theorem1.5. We willlettheone-dimensional
moment-matching distributionbethedistributioninLemma1.9,wherewewilltake donly depends
on α to be the largest integer such that α in Lemma 1.9 satisfies α α. Notice that d as
d d
≥ → ∞
α 0. Taking f(α) = d/32, it follows that any algorithm distinguishing between
→
• A standard Gaussian; and
• The distribution PA for v U(Sn 1), where A is the moment-matching distribution in
v −
∼
Lemma 1.9,
with at least 2/3 probability must require either a query of tolerance at most O (n d/32) =
d −
O (n f(α)) or 2nΩ(1) many queries. Notice that the distribution PA has at least α probability
α − v
mass resides insidetheorthogonal complement of span(v), which is a (n 1)-dimensional subspace.
−
Therefore, any SQ algorithm for solving the AC detection solves the hypothesis testing problem
above. This completes the proof.
31C.4 Proof of Theorem 1.13
Let G be the probability measure obtained by rescaling G such that the total measure is one.
′s,θ ′s,θ
We first show the following fact.
Fact C.3. For any polynomial p of degree at most k that E [p(t)2] = 1, s that is at most a
t (0,1)
∼N
sufficiently small universal constant, E t G′ [p(t)] E t (0,1)[p(t)] = k!2O(k)exp( Ω(1/s2)).
| ∼ s,θ − ∼N | −
Proof. Using Fact 1.12, we have that the total measure of G is 1 exp( Ω(1/s2)). Therefore,
s,θ
± −
for the rescaled G , for any k N, s> 0 and all θ R,
′s,θ
∈ ∈
E t (0,1)[tk] E t G′ [tk] = k!exp( Ω(1/s2)) .
| ∼N − ∼ s,θ | −
Then using the definition of Hermite polynomial,
k/2
1 ⌊ ⌋ k!
E t ∼N(0,1)[h k(t)] −E t ∼G′ s,θ[h k(t)] = √k! 2tt!(k 2t)! E t ∼N(0,1)[tk −2t] −E t ∼G′ s,θ[tk −2t]
Xt=0 − (cid:16) (cid:17)
k/2
⌊ ⌋ √k!
k!exp( Ω(1/s2)) tk 2t .
−
≤ − 2tt!(k 2t)!
t=0 −
(cid:0) (cid:1) X
Notice that the denominator is minimized when t = k/2 O(√k). Then it follows that the sum is
at most k!2O(k)exp( Ω(1/s2)). Now let p(t) = k w− h (t). Since E [p(t)2] = 1, it must
− i=0 i i t ∼N(0,1)
be w = 1 and it follows that w √k. Therefore, we have
k k2 k k1 ≤ P
k
E t (0,1)[p(t)] E t G′ [p(t)] w i E t (0,1)[h k(t)] E t G′ [h k(t)] = k!2O(k)exp( Ω(1/s2)) .
∼N − ∼ s,θ ≤ | || ∼N − ∼ s,θ | −
i=0
X
Now given Fact C.3, we can apply our main result Theorem 1.5. We will consider the following
distributiondistinguishingproblem. Inboththenullhypothesiscaseandthealternative hypothesis
case, the algorithm is given a joint distribution D of (x,y) over Rn R. In the null hypothesis case,
×
wehave x (0,I )andy U([ 1,+1]) independently. Whilein thealternative hypothesis case,
n
∼ N ∼ −
we have x (0,I ) and y = cos(2π(δ w,x +ζ)) with noise ζ (0,σ2) as in the definition
n
∼ N h i ∼ N
of learning periodic function. Notice that any SQ algorithm that can always returns a hypothesis
h such that E [(h(x) y)2] = o(1) can also be easily used to distinguish the two cases.
(x,y) D
∼ −
Therefore to lower bound such SQ algorithms, it suffices for us to give an SQ lower bound for this
distribution distinguishing problem.
Notice that in the alternative hypothesis, the distribution of x conditioned on any value of y is
thehiddendirectiondistributionPA w whereAis ∞ f y(ζ)1 2 G ′ 1/δ,arccos(y) ζ +G ′ 1/δ,2π−arccos(y) ζ dζ
and f
y
: R R is the PDF function of distRr− ib∞ ution of(cid:18) ζ condit2 iπ one− d on y. Notic2π e tha− t (cid:19) this
→
is a mixture of G with s = 1/δ and different θ. Thus applying Theorem 1.5 and Fact C.3
s,θ
yields that any SQ algorithm for solving the distinguishing problem, either requires a query of
error at most O (n ((1 λ)/4 β)k) + k!2O(k)exp( Ω(1/s2)), or at least 2nΩ(β) many queries for
k − − −
−
β > 0. Since k will have dependence on n, we will need to calculate the constant factor in
O (n ((1 λ)/4 β)k) which depends on k. According to Proposition 3.1, plug in the factor gives
k − − −
√k!n ((1 λ)/4 β)k +k!2O(k)exp( Ω(1/s2)).
− − −
−
32For convenience, let(1 λ)/4 β = γ. Itonlyremainstochoosethevalueofk andγ sothatk
nλ,γ < (1 λ)/4and√k!n− γk+k− !2O(k)exp( Ω(δ2))isminimized. Wewillchosek = nc′′ andγ = c≤
− ′′
− −
for c < c < min(2c,1/10). Then the error tolerance here is √k!n γk + k!2O(k)exp( Ω(δ2))
′ ′′ −
exp( k)+exp(O(c (logn)nc′′ )+O(nc′′ ) Ω(n2c)) = exp( Ω(nc′′ ))+exp( Ω(n2c)) =− exp( nc′≤ ).
′′
The−
number of queries here is
2nΩ(β)
=
2n− Ω((1−λ)/4−γ)
=
2nΩ(− (1−c′′ )/4−c′ )
=
2nΩ− (1)
. This
complet−
es the
proof.
C.5 SQ Lower Bounds as Information-Computation Tradeoffs
We note that both aforementioned results can be viewed as evidence of information-computation
tradeoffs for the application problems we discussed.
For the problem of list-decodable Gaussian mean estimation, the information-theoretically op-
timal error is Θ(log1/2(1/α)) and is achievable with poly(n/α) many samples ([DKS18]; see, e.g.,
Corollary 5.9 and Proposition 5.11 of [DK23]). Thebestknown algorithm for this problem achieves
ℓ -error guarantee O(α 1/d) using sample complexity and run time (n/α)O(d) ([DKS18], see The-
2 −
orem 6.12 of [DK23]). Notice that in order to achieve error guarantee even sub-polynomial in
1/α, the above algorithm will need super-polynomial runtime and sample complexity. Informally
speaking, Theorem 1.8 shows that no SQ algorithm can perform list-decodable mean estimation
with a sub-exponential in nΩ(1) many queries, unless using queries of very small tolerance — that
would require at least super-polynomially many samples to simulate. Therefore, it can be viewed
as evidence supporting an inherent tradeoff between robustness and time/sample complexity for
this problem.
For the AC detection hypothesis testing problem, the information-theoretically optimal sample
complexity is O(n/α). To see this, note that if the input distribution is a standard Gaussian, then
any n samples will almost surely be linearly independent. On the other hand, suppose that the
input distribution has α probability mass in a subspace. Then with O(n/α) many samples, with
highprobability, therewillbeasubsetofnsamplesallcomingfromthatsubspace,whichcannotbe
linearly independent. However, our SQ lower bound suggests that no efficient algorithm can solve
the problem with even nω(1) samples where the ω(1) is w.r.t. α 0. This suggests an inherent
→
tradeoff between the sample complexity and time complexity of the problem.
For the problem of learning periodic functions, the SQ lower bound given by our result will be
larger than the algorithmic upper bound in [SZB21](with sample complexity O(n) and run-time
2O(n)). However, the algorithms in [SZB21] are based on LLL lattice-basis-reduction which is not
captured by the SQ framework; therefore, this does not contradict our SQ lower bound result.
D The Optimality of Parameters in Theorem 1.5
In this section, we show that the lower bound in Theorem 1.5 is nearly optimal. Specifically, we
construct an NGCA problem instance that can be solved by an SQ algorithm with a single query
of tolerance n (d+2)/4 (as we explained, in most cases, we can take λ,c arbitrarily close to 0 and
−
our lower boundon tolerance is O (n d/4)). We recall the definition of hypothesis testing version
m,d −
of NGCA: For integers n > m 1 and a distribution A supported on Rm, one is given access to a
≥
distribution D such that either: (1) H : D = , (2) H : D is given by PA, where V U(O ).
0 Nn 1 V ∼ n,m
The goal is to distinguish between these two cases H and H .
0 1
To define our NGCA instance, we let A = (1 2ǫ) +ǫδ +ǫδ +p1[x [ 1,1]] for some
B B
polynomial p : R R of degree at most d, where− we aN ssign probabi− lity mass ǫ∈ on− some points B
→
and B respectively.
−
33Inordertomatch thefirstdmomentsofAwiththestandardGaussian, werequirethefollowing
technical result.
Lemma D.1 ([DK23]). Let C > 0 and d Z . For any a ,a ,...,a R, there exists a unique
+ 0 1 d
∈ ∈
degree at most d polynomial p : R R such that for each integer 0 t d we have that
→ ≤ ≤
C
p(x)xtdx = a .
t
Z−C
Furthermore, for each x [ C,C] we have that p(x) O (max a C t 1).
d 0 t d t − −
∈ − | | ≤ ≤ ≤ | |
In order to apply Lemma D.1, we pick C = 1 and a = 0. For any 1 t d, we let a =
0 t
≤ ≤
2ǫ((t 1)!! Bt)foreven tanda = 0foroddt. ByLemmaD.1,thereisapolynomialpofdegreeat
t
− −
mostdsuchthatAexactly matches thefirstdmomentsofthestandardGaussian. Furthermore,for
each x [ 1,1], we have that p(x) O (max a ). We take B = √n and ǫ = n (d+2)/2. In
d 0 t d t −
thisway∈ , f− oranyx [ 1,1], we| have| t≤ hat(1 2ǫ)≤ ≤ (x| ) | O (max a ) = O (ǫBd) = O (1/n).
d 0 t d t d d
∈ − − N ≥ ≤ ≤ | |
Therefore, A is well-defined.
x 2 n
Let F(x) =He (d+2)/2 k √k2 n− . The SQ algorithm is simple.
(cid:16) (cid:17)
• Ask the SQ oracle to obtain an estimate v of E [F(x)] with tolerance n (d+2)/4/4.
x D −
∼
• If v < n (d+2)/4/2 then return H , otherwise return H .
− 0 1
Weassumethedimensionnissufficientlylarge. Toprovethecorrectnessofouralgorithm,itsuffices
to show that E
x
∼PAv[F(x)]
≥
n −(d+2)/4. Note that kx √k2 2 n−n is sufficiently close to the standard
Gaussian, and we have that
x 2 n y2+z2 n
E [F(x)] = E He k k2 − = E He −
x ∼PAv x ∼PAv (d+2)/2
√n
y ∼χn−1,z ∼A (d+2)/2
√n
(cid:20) (cid:18) (cid:19)(cid:21) (cid:20) (cid:18) (cid:19)(cid:21)
z2 1 (d+2)/2
= E
z A
− = n −(d+2)/4E
z A
(z2 1)(d+2)/2 ,
∼ " √n # ∼ −
(cid:18) (cid:19)
h i
where we apply the identity He (x + y) = k k xk ℓHe (y) in the third equation. Since A
k ℓ=0 ℓ − ℓ
exactly matches the first d moments with the standard Gaussian and d is even, we have that
P (cid:0) (cid:1)
E [(z2 1)(d+2)/2] E [(z2 1)(d+2)/2]= E [zd+2] E [zd+2]
z A z z A z
∼ − − ∼N − ∼ − ∼N
1 1
= 2ǫBd+2 2ǫE [zd+2]+ p(x)xd+2dx = 2ǫBd+2 2ǫ(d+1)!!+ p(x)xd+2dx .
z
− ∼N Z−1 − Z−1
This gives that
1
E [(z2 1)(d+2)/2] 2ǫ(Bd+2 (d+1)!!) p(x)xd+2dx
z A
∼ − ≥ − −
Z
1| |
−
C
2ǫ(Bd+2 (d+1)!!) O (ǫBd) xd+2dx
d
≥ − −
Z0
= 2ǫ(Bd+2 (d+1)!!) O (1/n)
d
− −
1 .
≥
Therefore, we conclude that E [F(x)] = n (d+2)/4E (z2 1)(d+2)/2 n (d+2)/4.
x ∼PAv − z ∼A
− ≥
−
(cid:2) (cid:3)
34Remark D.2. By the standard definition of an SQ algorithm, we need to pick a boundedfunction
x 2 n
F(x) for the SQ queries. To address this, we can truncate the value of F(x) with k k2− more than
√n
M , where M is a parameter depending on d. Since n is sufficiently large, it suffices to show that
d d
E [He (x)1[x M]] is sufficiently close to 0 for any 1 k (d+2)/2. This can be achieved
x k
∼N | | ≤ ≤ ≤
since there is an M = M such that M (x)He (x)dx 1 ,1 k d.
d M N k ≤ O(d)d ≤ ≤
−
(cid:12) (cid:12)
(cid:12)R (cid:12)
(cid:12) (cid:12)
35