How Far Are We from Intelligent Visual Deductive
Reasoning?
YizheZhang∗, HeBai∗, RuixiangZhang∗, JiataoGu,
ShuangfeiZhai, JoshSusskind, NavdeepJaitly
Apple
{yizzhang,hbai7,ruixiangz,jgu32,szhai,jsusskind,ndjaitly}@apple.com
Abstract
Vision-Language Models (VLMs) such as GPT-4V have recently demonstrated
incrediblestridesondiversevisionlanguagetasks. Wedigintovision-basedde-
ductive reasoning, a more sophisticated but less explored realm, and find previ-
ouslyunexposedblindspotsinthecurrentSOTAVLMs.Specifically,weleverage
Raven’sProgressiveMatrices(RPMs),toassessVLMs’abilitiestoperformmulti-
hoprelationalanddeductivereasoningrelyingsolelyonvisualclues. Weperform
comprehensive evaluations of several popular VLMs employing standard strate-
giessuchasin-contextlearning,self-consistency,andChain-of-thoughts(CoT)on
threediversedatasets,includingtheMensaIQtest,IntelligenceTest,andRAVEN.
The results reveal that despite the impressive capabilities of LLMs in text-based
reasoning,wearestillfarfromachievingcomparableproficiencyinvisualdeduc-
tive reasoning. We found that certain standard strategies that are effective when
appliedtoLLMsdonotseamlesslytranslatetothechallengespresentedbyvisual
reasoningtasks.Moreover,adetailedanalysisrevealsthatVLMsstruggletosolve
thesetasksmainlybecausetheyareunabletoperceiveandcomprehendmultiple,
confoundingabstractpatternsinRPMexamples.
1 Introduction
RecentadvancementsinVision-LanguageModels(VLMs)haveshowcasedthesuccessofmodels
such as GPT4-V (OpenAI, 2023) and Gemini (Team et al., 2023) across various vision language
tasks. These tasks include captioning, object localization, multimodal world knowledge and com-
monsense,visualquestionanswering(VQA),andvision-basedcoding(Yangetal.,2023). Previous
evaluationsofthesemodelshaveproventhatstate-of-the-art(SOTA)VLMsarecapableofperform-
ingwellinnumerousvision-basedreasoningandunderstandingtasks(OpenAI,2023;Teametal.,
2023). Notably,priorworkshavedemonstratedthatstrongVLMscanaccuratelyextracttextfrom
images,understandandreasonwithchartsandtables,andsolvesimplevisualmathproblems(Yang
etal.,2023;NahidaAkteretal.,2023).
Inthisstudy,weaimtoevaluatethelimitationsofVLMsonchallengingtasksthatdemandsophisti-
catedvision-baseddeductionabilities,anareathathasbeenrelativelyunexplored. Specifically,we
askthemodelstocompleteasetofRaven’sProgressiveMatrices(RPMs)problems(Kundaetal.,
2013;Zhangetal.,2019),whicharefrequentlyusedtomeasurehumanintelligence,byidentifying
the correct pattern to fill in the blank from multiple options. See Figure 1 for illustration. This
requiresthemodelsto1)comprehendeachgivenpatternincludingthechoices,2)deduceunderly-
ingrulesandidentifyanytrendthatcanexplaintheevolutionofthesepatterns,and3)employthe
learnedrulestochoosethemissingpatternfromthegivenoptions. Themodel’scapacitytohandle
∗ Equalcontribution.
Preprint.Underreview.
4202
raM
7
]IA.sc[
1v23740.3042:viXraPrompt:
Grid:
You can see a grid of 9 boxes, one of which is empty (marked as ?). You
have to choosewhich of the 6 alternative shapes (A-F) to be placed in the
empty box in order to completethe pattern that connects the shapes.
Finally, provide your prediction as Answer: “X”
From the figure we can see:
The grid:
1. Top left box: square with a cross sign.
…
9. Bottom right box: ’?’.
The alternative shapes are:
A. Triangle with a star .
…
Alternative shapes: F. Triangle with a cross sign.
Analyzing the grid, there seems to be a pattern involving both
A B C
the shapes and the symbols inside them:
- The symbols inside the shapes alternate between a cross, a
star, and then a blank space as you move horizontally across
each row.
- The shapes themselves also alternate within each row – this
D E F is seen with the square, circle, and triangle repeating in each
row in that order.
Given the identified patterns, the correct shape should be a
triangle with a cross sign. Looking at the alternative shapes,
option "F" fits the described pattern perfectly.
Hence, the answer is: "F"
Figure1: IllustrationofthevisualdeductivereasoningforRaven’sProgressiveMatrices. Thetask
requires intricate coordination among perception, deductive reasoning, and hypothesis verification
capabilitiesexhibitedbyVision-LanguageModels.
eachaspectmustbeeffectivelycoordinatedtoprovidethecorrectanswer. Ourfindingsrevealthat
althoughsomeproblemsmayseemintuitivetohumans,theymightnotbeasintuitivetoVLMs.
Compared to standard image reasoning tasks like VQA (Antol et al., 2015), RPMs pose several
uniquechallenges:1)RPMsrequiresophisticateddeductivecapabilitiesthatinvolvemulti-hopcom-
parativereasoning,suchasdiscrimination,relation,andanalogy,whileVQAtypicallyrequiresonly
fewstepsofreasoning,2)RPMsrelysolelyonvisualcluestogeneratehypothesesandverifythem,
whileVQAofteninvolvesusingnaturallanguagetoinfertheobjectiveanddeterminewhichparts
to focus on, 3) RPMs are inherently few-shot (mostly 2-shot) learning tasks. Each RPM problem
may have different underlying rules, which demands strong generalization abilities to solve them.
Humanshavearemarkableabilitytolearnfromjustafewexamples,andpowerfullanguagemodels
likeLLMshavedemonstratedthisabilityintext-basedtasks. However,theabilityofstrongVLMs
tosolvefew-shotreasoningtasksbyrelyingsolelyonvisualcueshasnotbeenwellstudied.
As an emerging field, it is crucial to establish benchmarks and systematic evaluations in order to
pushthelimitsofthevisualdeductiveabilityofVLMs. Ourcontributionsinclude:
1. WesetupaframeworkforsystematicallyevaluatingVLMsonRPMproblems. Weevaluated
several SOTA open-source and closed-source VLMs on three diverse datasets, including
theMensaIQtest,IntelligenceTest,andRAVEN,providingacomprehensiveassessmentof
theirperformance. TheresultsindicatethatalthoughLLMsexhibitimpressivecapabilities
intext-basedreasoning,suchproficiencyhasnotbeenachievedinimage-basedreasoning.
The code and evaluation datasets have been released to facilitate future investigation and
improvementoverVLMs. 2
2. Weemployedstandardinference-timestrategiesinLLMssuchasin-contextlearning(Brown
etal.,2020)andself-consistency(Wangetal.,2022)toprobethepotentialofVLMs. We
foundthatsomestandardstrategiesthatareeffectiveinLLMsdonotseamlesslytranslate
totheVLMsweused.
2Ourcodeisavailableathttps://github.com/apple/ml-rpm-bench
2
Reasoning
Verification
Perception
Deductive
Hypothesis3. We finely diagnose the performance bottleneck of VLMs by breaking down their capability
intoperception,deductivereasoning,andhypothesisverification. Ouranalysisrevealsthat
perceptionisthelimitingfactorincurrentVLMs. Toscrutinizethisspecific“blindspot”
instrongVLMssuchasGPT-4V,weprovideacasestudyhighlightingwhereissuesoccur.
4. WeidentifiedandexaminedseveralissuesassociatedwiththecurrentVLMsinthistask.These
issues include overconfidence, sensitivity to prompt design and an inability to effectively
leverage in-context examples. We ablated the effects of different prompts on the overall
performanceofthemodelandfoundmodelscanbenefitfrommorestructuredprompts.
2 RelatedWork
General LLM Reasoning benchmarks Many text-based reasoning tasks and benchmarks have
been introduced to evaluate LLMs in various domains (Huang & Chang, 2022) such as general
knowledge(Hendrycksetal.,2020),mathreasoning(Cobbeetal.,2021),commonsensereasoning
(Gevaetal.,2021;Clarketal.,2018),factualreasoning(Labanetal.,2023),andcoding(Chenetal.,
2021). SomenoteworthyexamplesoftheseworksareBIG-bench(Srivastavaetal.,2022),HELM
(Liangetal.,2022),SuperGLUE(Sarlinetal.,2020),andLAMA(Petronietal.,2019).
Visual reasoning evaluation Previous work on visual reasoning tasks has primarily focused on
tasks such as visual question answering (VQA) Antol et al. (2015) and image captioning. These
tasksinvolveansweringquestionsaboutimagesorgeneratingnaturallanguagedescriptionsofvisual
content. Researchershavealsoexaminedtheabilityofmodelstounderstandtherelationalandcom-
positionalaspectsofobjectsinimages. DatasetslikeCLEVR(Johnsonetal.,2017)andSHAPES
(Andreasetal.,2016)assessvisualreasoningabilitiessuchascounting,comparing,logicalreason-
ing, and storing information in memory. As the VLMs abilities to perform visual reasoning have
evolvedsohavethebenchmarks. Newbenchmarks,likeMMMU(Yueetal.,2023)andMathVista
(Lu et al., 2023) have been developed that test the models’ ability to emulate human-like under-
standingofscenesandobjectsinimagesandvideos. Thesebenchmarksincludeareassuchasscene
text understanding (Sidorov et al., 2020), formulation (Lu et al., 2024), table and chart interpreta-
tion(Luetal.,2024),thecomprehensionofvisualstimuli(Yangetal.,2023),geometricreasoning
(Ahrabianetal.,2024),spatialreasoning(Chenetal.,2024),andfacialexpressioncomprehension
andreasoning(Yangetal.,2023).
Deductive reasoning Deductive reasoning evaluation and benchmarks have been conducted for
both textual and visual domains. Two notable examples are GuessWhat?! (De Vries et al., 2017)
andReferIt(Kazemzadehetal.,2014),whichassessthevisualreasoningabilitiesofthemodelsbe-
ingtested. Morerecently,LMRLGym(Abdulhaietal.,2023)andEntityDeductionArena(Zhang
etal.,2023)havebeenintroducedasmethodstoevaluatetheabilityofLLMstoperformmulti-turn
deductivereasoningtasks.AnotherrelevanttaskisARC(Acquavivaetal.,2022)whichsharessimi-
laritieswithRPMs,astheybothrequirecorrectlyinferringunseenoutputsbasedongivenexamples.
ComparingwithARC,RPMsareabstractandrequiresintricateanalogicalandrelationalreasoning.
3 ExperimentSetting
3.1 Dataset
In our paper, we employed three RPMs datasets. The Mensa Test3 consists of 35 questions with
progressivelevelsofdifficulty.Forthepurposeof1-shotlearning,weusedthefirstquestionasanin-
contextexampleandreservedtheremaining34questionsforevaluation. TheIntelligenceTest(IT)4
providedanIQtestencompassingverbal,patternrecognition,math,andstructuralcomponents. We
specifically focused on pattern recognition, which solely comprised RPMs problems and included
66examples. Additionally,weincorporatedtheRAVENdataset(Zhangetal.,2019)forevaluation.
The RAVEN dataset employs a generative model to create RPMs problems using a hierarchical
pipeline. ThetestdatasetofRAVENcontains14,000examples,covering7typesofdistinctfigural
3https://www.mensa.org/public/mensa-iq-challenge
4https://www.intelligencetest.com/questions
3configurationsthatincorporatedifferentlayouts,shapes,andrelationalstructures. Inthiswork,we
generate140newsamples,20samplesforeachfiguralconfiguration.
3.2 Models
We compared various VLMs that represent the state-of-the-art for both closed-source and open-
sourcemodels,includingGPT4-V(gpt-4-vision-preview)(OpenAI,2023),Gemini-pro(Teametal.,
2023),Qwen-VL-Max(Baietal.,2023)andLLaVA-1.5-13B(Liuetal.,2023). Weusethedefault
samplingmethodforeachofthetestedVLMsinourgenerationprocess,showinAppendixA.2.
3.3 Prompts
Wepromptthemodelwiththeinstructionfollowedbythequeryimage. FortheMensasetweused
thefollowingprompt:
You can see a grid of 9 boxed, one of which is empty (marked as ?). You have to choose
which of the 6 alternative shapes (A-F) should be placed in the empty box in order to
completethepatternthatconnectstheshapes. Finally,provideyourpredictionasAnswer:
“X”.{queryimage}
AppendixA.1containsthepromptsusedforallthreedatasetsfordifferentsettings. InSection5.4,
weexplainwhywechosetheparticularorderofthepromptandqueryimageorder.
4 EvaluationResults
4.1 EvaluationofVLMsonVisualDeductiveReasoning
Mensa IntelligenceTest(IT) RAVEN
Entropy Accuracy↑ Entropy Accuracy↑ Entropy Accuracy↑
GPT-4V 1.49 0.24±0.05 1.40 0.16±0.04 2.07 0.12±0.04
GeminiPro 1.24 0.15±0.04 1.18 0.18±0.03 1.37 0.11±0.04
QWen-VL-Max 1.13 0.17±0.01 0.97 0.13±0.02 0.48 0.10±0.03
LLaVA-1.5-13B 0.72 0.23±0.01 0.64 0.09±0.01 0.25 0.10±0.03
RandomGuess 2.58 0.16 2.58 0.16 3.00 0.12
Table 1: Benchmark of VLMs on three different datasets. “Entropy” denotes uncertainty of the
prediction,and“Accuracy”indicatesthepercentageofaccuratelyansweredquestions.
InTable1weshowhowdifferentVLMsperformedoneachdataset.Foreachmodelanddataset,we
computedthestatisticsbyaveragingthemover10repetitions.Fromthetable,itisevidentthatGPT-
4eitherslightlysurpassesorisonparwiththeothermodelsacrossallbenchmarks. However, the
accuracygapbetweenthemodelsisnotsubstantialintermsoftheirabilitytosolveRPMpuzzles.
Itisinterestingtonotethattheperformanceofthesemodelsiscomparabletorandomguessing(last
row),indicatingtheirlimitedeffectivenessinthisarea. Convertingtheaccuracyonthequestionsto
humanrankingscale,wefindthatthemodelsrankinthe2-8percentileontheMensatests. Onthe
IT dataset humans demonstrate a wide range of success rates per question, spanning from 30% to
93.4%, which is much higher than the highest accuracy of a mere 18% observed for Gemini Pro.
Similarly, ontheRavendatasethumansattainanimpressivesuccessrateof84.67%(Zhangetal.,
2019),starklyoutperformingVLMs,whichconsistentlyyieldresultsakintorandomguessing.
Uncertaintyoftheprediction Weanalyzetheentropyofmodelpredictionsinordertoassessthe
uncertaintyinherentintheirpredictivedistribution. ForthechoicessetC,theEntropyisdefinedas
(cid:80)
S = − p logp . If the model consistently predicts a single answer, it is has an entropy of
i∈C i i
0. Ifitrandomly guesses, theentropy reachestheupperboundshownintheTable1. Weseethat
GPT-4andGeminiProexhibitagreaterdiversityofanswers,whichisalsoreflectedinthegreater
4diversityinrecognizingandattemptingtoidentifyvariouspatterns. Ontheotherhand,LLaVAand
QWen-VLproducemoredeterministicpredictions,resultinginlowerentropy.
WeobservedthatalltheVLMstendtobeconfidentwhilebeingwrong. Interestingly,weobserved
that even when the entropy was high, models tried to provide a nonsensical rational, instead of
acknowledging their inability to perform the task; this was observed to happen more often with
models that had higher entropy. All the tested models never express any level of uncertainty by
using words like “likely” or “maybe”. This excessive confidence can presumably be attributed to
themodelpretrainingandinstructionfinetuningsteps,whichtypicallydonotinvolvecalibratingthe
modelforuncertainty. Instead,themodelsareencouragedtogenerateuncertaincontent,leadingto
moreerrorsinaggregatinginthegeneratedoutput.
4.2 DosomestandardstrategiesusedinLLMstranslateeffectivelytovisualdeductive
reasoning?
WetriedtwostrategiesthathavebeenshowntobeeffectiveinvarioustasksrelatedtotextLLMs:
1)One-shot(1-shot)(Brownetal.,2020)involvesprovidingasinglein-contextRPMexampleand
its solution to the VLMs. 2) Self-consistency (SC) (Wang et al., 2022) entails sampling multiple
responsesandselectingtheanswerthatreceivesthemajorityvote. TheresultsareshowninTable2.
Mensa IntelligenceTest RAVEN
Entropy Accuracy↑ Entropy Accuracy↑ Entropy Accuracy↑
GPT-4V(0-shot) 1.49 0.24±0.05 1.40 0.16±0.04 2.07 0.12±0.04
GPT-4V(1-shot) 1.41 0.22±0.06 1.31 0.17±0.04 2.03 0.12±0.04
GPT-4V(SC) 0.17 0.31±0.01 0.15 0.19±0.02 0.20 0.10±0.02
GeminiPro(0-shot) 1.24 0.15±0.04 1.18 0.18±0.03 1.37 0.11±0.04
GeminiPro(1-shot) 0.69 0.17±0.03 0.54 0.19±0.01 1.35 0.10±0.03
GeminiPro(SC) 0.03 0.18±0.01 0.03 0.18±0.01 0.08 0.10±0.01
Table 2: Expanded benchmark of VLMs on three different datasets, including the 1-shot and SC
variantsforbothGPT-4andGeminimodels. ThepromptsareprovidedinAppendixA.1.
VLMsstrugglewithreadingin-contextimage Theperformanceofthe1-shotevaluationdidnot
demonstrateasignificantimprovementcomparedtothe0-shotevaluation.Specifically,weobserved
onlyamarginal1%enhancementfortheIntelligenceTestdataset,whileencounteringadecreaseof
2-4% in accuracy for the Mensa test. Surprisingly, all the tested models, including GPT-4V and
Gemini,strugglewithahighfailurerateevenwhenthein-contextexampleisidenticaltothecurrent
taskbeingsolved. ThisispeculiarbecausepowerfulLLMsusuallyexhibittheabilitytoanalogize
andcopythein-contextexamplewhenprovidedwiththesamequery.Weobservedaccuracyranging
from 10% to 20% for these in-context examples across different datasets, which is comparable to
theaccuracywhenadifferentexampleisusedasthein-contextexample.
Inordertomakethisobservationconcretewepresentanablationexperimentwithaspecificexample
wecreatedmanuallyinthestyleofMensaproblems,whichwecallM-easy(SeeFigure2aforthe
problem and Table 3 for a summary of results). Here the same example is used as the in-context
example,andasthetaskbeingsolved,themodelonlyneedstobeabletodrawacomparisonbetween
thein-contextexampleandthequery,andcopyovertheanswerfromthein-contextsample5.
Wefirstcasttheproblemasatext-onlyproblemus-
ing appropriate descriptions for both the in-context In-context Query Accuracy
exampleandthequery(row1). Themodeldemon- Desc.+Rat.+Ans. Desc. 100%
Img.+Desc.+Rat.+Ans. Img.+Desc. 80%
strates a perfect accuracy of 100% showing that it
Img.+Desc.+Rat.+Ans. Img. 20%
is easy for it to solve this problem when it is rep- Img. Img.+Desc. 80%
resented as text. Next, we added the image to the Img. Img. 40%
textual description for both the in-context example
Table 3: GPT-4V analogizes better when
andthequery. Theaccuracynowdecreasesto80%,
solelybasedontextdescriptions.Desc.,Rat.,
even though additional visual information has been
Ans. andImg. representsdescription, ratio-
nale,answerandimage,respectively
5Theresultsarebasedon10repetitions
5provided(row2). Finally,whenthetextdescription
isremovedfromthequery,theaccuracysignificantly
drops to 20% (row 3). We hypothesize that the drop in accuracy arises because it is much harder
forthemodeltocompareimagetokensthanitistocomparetextualtokensandalsothatthemodel
utilizestextmorethanitdoestheimages.
5 WhatlimitstheperformanceoftheVLMs?
Grid: Grid: Grid:
"The grid:\n1. Top left box: square with a "The grid:\n1. Top left box: A downward- "The grid:\n1. Top left box: white circle,
cross sign.\n2. Top center box: circle with pointing triangle with three dots in it.\n2. white triangle, black square.\n2. Top
a star.\n3. Top right box: Empty Top center box: A leftward-pointing center box: white triangle, black circle,
triangle.\n4. Middle left box: empty triangle with two dots in it.\n3. Top right white square.\n3. Top right box: black
square. \n5. Middle center box: circle box: An upward-pointing triangle with square, black square, white triangle.\n4.
with a cross sign. \n6. Middle right box: one dot in it.\n4. Middle left box: A Middle left box: black circle, white
triangle with a star. \n7. Bottom left box: rightward-pointing triangle with two dots square, white triangle.\n5. Middle center
square with a star. \n8. Bottom center in it.\n5. Middle center box: A box: black square, white triangle, black
box: empty circle. \n9. Bottom right box: downward-pointing triangle with one dot square.\n6. Middle right box: white
"?".\nThe alternative shapes are:\nA. in it.\n6. Middle right box: A leftward- triangle, black square, white circle.\n7.
Triangle with a star .\nB. Triangle with a pointing triangle with three dots in it.\n7. Bottom left box: white triangle, black
plus sign.\nC. Circle with a cross sign.\nD. Bottom left box: An upward-pointing square, black square.\n8. Bottom center
Circle with a star.\nE. Empty triangle.\nF. triangle with one dot in it.\n8. Bottom box: black square, white circle, white
Triangle with a cross sign." center box: A rightward-pointing triangle triangle.\n9. Bottom right box: '?'.\nThe
Alternative shapes: Alternative shapes: Awlitthe rtnhraetei vdeot ss hina ipt.\ens9:. Bottom right alternative shapes are:\nA. white circle,
A B C A B C Abox: '?'.\nThe Balternative sha Cpes are:\nA. white triangle, black square.\nB. black
An upward-pointing triangle with two circle, white square, white triangle.\nC.
dots in it.\nB. A downward-pointing white circle, white square, black
triangle with one dot in it.\nC. A leftward- triangle.\nD. white circle, black square,
pointing triangle with one dot in it.\nD. A white triangle.\nE. black square, white
D E F D E F Drightward-poiEnting triangle wFith two dots triangle, white circle.\nF. white square,
in it.\nE. A leftward-pointing triangle with white triangle, black circle."
two dots in it.\nF. A downward-pointing
triangle with two dots in it."
(a)M-EasyRPMProblem (b)M-MediumRPMProblem (c)M-HardRPMProblem
Figure2: ThreemanuallycreatedRPMproblemsevaluatedfortextdescriptionaugmentation,illus-
tratingvaryinglevelsofdifficulty: easy,medium,andhard. Thecorrectanswersare“F,F,F”.
Position DescriptionofM-EasyRPM DescriptionofM-MediumRPM Desc.ofsegmentedM-MediumRPM
Triangle pointing down with three dots
Topleft AsquarewithanXinside. Invertedtrianglewiththreedotsinside.
formingaverticallineinthecenter.
Triangle pointing right with three dots
Topcenter Acirclewithastarinside. Right-pointingtrianglewithtwodots.
formingahorizontallinealongthecenter.
Trianglepointingupwithfourdotsform- Upright triangle with one dot in the
Topright Anemptytriangle.
ingaverticallineinthecenter. center.
Triangle pointing down with two dots
Middleleft AsquarewithanXinside. Right-pointingtrianglewithtwodots.
formingahorizontallineinthemiddle.
Middle Trianglepointingrightwithasingledotin Inverted triangle with one dot in the
AcirclewithanXinside.
center thecenter. center.
Middle Trianglepointingupwithtwodotsforming
AtrianglewithanXinside. Left-pointingtrianglewiththreedots.
right averticallinealongthecenter.
Bottom Trianglepointingdownwithonedotinthe Upright triangle with one dot in the
Asquarewithastarinside.
left center. center.
Bottom Trianglepointingrightwithtwodotsform-
Acircle. Right-pointingtrianglewiththreedots.
center ingahorizontallineinthemiddle.
Table 4: The M-Easy and M-Medium RPMs descriptions from GPT-4V for the patterns can con-
tain errors, including hallucinations and Chimera descriptions. When the model is provided with
segmentedRPMimages(i.e.,whenpatternsareseparatedintomultipleimageinputs),itleadstoa
reductionintheerror. Errorsareindicatedinred.
We investigate why VLMs fail to reach human-level performance in answering even simple ques-
tionsthatareintuitivetohumans.Forthispurpose,asacasestudy,wemanuallycreatedthreeRPMs
with varying degrees of difficulty, as depicted in Figure 2. To conduct a fine-grained analysis and
diagnosisoftheVLM’sinabilitytoperformthistaskofvisualdeductivereasoningwithRPMs,we
decomposetheevaluationintothreeconsecutivestages:
• Perception: assessifthemodelcanunderstandanddescribethepatternsintheRPMs.
6• Deductivereasoning: evaluateifthemodelcandiscernandarticulateunderlyingrules.
• Hypothesisverification: examinethemodel’sproficiencyinformulatingaplausiblehypoth-
esisforthemissingpatternandidentifyingamatchingoptionamongalternatives.
5.1 HowgoodistheVLM’sperceptiononthistask?
WefirstaskedthemodeltodescribetheRPMfigures,toassessiftheyunderstoodtheimagesthat
were provided as part of the problem. Surprisingly, even though VLMs are astoundingly accurate
indescribingcommonplaceimages, theyseemedtobequiteunsuccessfulataccuratelydescribing
even the simpler abstract patterns we gave them. The generated descriptions contained numerous
errorsacrossallthetestedmodels,asexemplifiedbyresultsfromGPT-4VinTable4.
WeidentifiedtwomajorissuesforthisblindspotofVLMs:
1. Compounding error: Models tend to replicate the descriptions of previous patterns, leading
toanautoregressiveamplificationofcompoundingerrorsinsuccessivedescriptions. This
resultsinanincreasinglyerroneousnarrativethroughoutthegenerationprocess. Forexam-
ple,inTable4(M-Medium),Whenthemodelfirstmakesamistakebyincluding“avertical
line”inthedescription, thesubsequenttextfollowsthesameerror. Wethinkthattheau-
toregressivenatureoftheVLMscausesittorepeatitself,withtheprecedingtextdictating
theentirefollow-uptext.
2. Confoundingerror:Thesimilaritiesbetweenpatternscauseconfusion,asthemodelstruggles
tomaintainfocusonasinglepattern. Consequently, weoftenobserve“Chimeradescrip-
tions”thaterroneouslycombineelementsfrommultiplepatterns. Forexample,inTable4
(M-Easy,middleright),thedescriptionseemstocombineelementsintwoadjacentpatterns
(middlecenter,middleright). Thiscouldbeattributedtothemodel’sfailuretoeffectively
focusitsattentiononthecorrespondingpatternwhenallthepatternsappearsimilar.
Thesetwoissuesareprevalentacrossallthetestedmethodsanddataset. Whenthepatternscontain
multipleelementsandaremoredetailed,theseissuesbecomemoresevere.
CandecomposingtheRPMsintoeachsinglepatternfrom
the grid enhance perception? Presumably, by decompos-
ingthepatternsintoindividualcomponents,wecaneliminate
theconfoundingerrors. Toinvestigatethis,wefirstsegmented
each of the three manual examples shown in Figure 2, into
9 individual question patterns and 6 candidate patterns. We
then used a new prompt A.1 for GPT-4V to read both the
fullimageandthesegmentedpatternstoinfertheanswer. In
thisway,wefoundGPT-4Vwoulddescribeeachpatternmore
accurately. The descriptions of the M-Medium RPM can be
found in Table 4. We conducted 10 tests for each RPM and
report the accuracy comparison with and without segmenta-
tioninFigure3. Wealsoverifythesegmentationimpactusing
theRavendataset(140examples). Wegot16.4%accuracyfor
segmented RPMs and 11.4% for non-segmented RPMs. The
resultsdemonstrateasignificantreductioninconfoundinger-
Figure 3: Accuracy of the origi-
rors,confirmingtheissuesdiscussedearlier.
nal RPM as input with that of the
segmented RPM as input. Results
Hallucination Wehavealsoobservedthemodelgenerating basedon10repetitions.
hallucinations in the descriptions, particularly when it comes
to counting. For instance, in Table 4 (M-Medium, top right),
themodelerroneouslystatesthattherearefourdotswhen,inreality,thereareonlythree.
Data distribution aspect VLMs are presumably trained primarily on real world images, which
maycausethemtobelesssensitivetoabstractpatterns. Wedidnottrytofinetunethemodeltotest
thishypothesisbecauseofthelimitedavailabilityofdiverseRPMdata. Webelievethatadditional
finetuningcouldpotentiallyimprovetheperformance. However,wehypothesizethatfinetuningthe
7M-Easy
Model Acc. Ent. A B C D E F
GPT-4V1-shot 50% 1.69 0 0 1 1 3 5
GPT-4V1-shot+Gen.Desc.(CoT) 50% 1.36 0 1 0 0 4 5
GPT-4V1-shot+OracleDesc. 60% 1.57 0 1 0 1 2 6
GPT-4V1-shot+OracleDesc.-Visual 60% 0.97 0 4 0 0 0 6
GPT-4V1-shot+OracleDesc.+Rationale 60% 1.57 1 0 0 1 2 6
M-Medium
Model Acc. Ent. A B C D E F
GPT-4V1-shot 20% 2.25 0 3 2 2 1 2
GPT-4V1-shot+Gen.Desc.(CoT) 50% 1.69 0 3 1 1 0 5
GPT-4V1-shot+OracleDesc. 80% 0.92 1 1 0 0 0 8
GPT-4V1-shot+OracleDesc.-Visual 60% 1.57 1 0 0 1 2 6
GPT-4V1-shot+OracleDesc.+Rationale 70% 0.88 3 0 0 0 0 7
M-Hard
Model Acc. Ent. A B C D E F
GPT-4V1-shot 20% 1.49 0 0 3 0 5 2
GPT-4V1-shot+Gen.Desc.(CoT) 30% 1.97 0 2 2 0 3 3
GPT-4V1-shot+OracleDesc. 40% 1.85 0 3 0 1 2 4
GPT-4V1-shot+OracleDesc.-Visual 10% 1.96 2 0 1 5 1 1
GPT-4V1-shot+OracleDesc.+Rationale 50% 1.49 0 2 0 0 3 5
Table5: AccuracyandbreakdownofGPT-4Vvariantswithaugmentedtextdescriptionacrossdif-
ferentRPMs. Eachcombinationisranfor10repetitions. Thecorrectanswer“F”ismarkedincolor.
model with RPMs might not entirely eliminate the compounding and confounding errors, as they
appeartobeinherentlimitationsoftheVLMsfromtraining.
5.2 HowgoodistheVLM’sdeductivereasoningonthistask?
Next,weassessthemodel’sabilitytoperformeffectivereasoningbyconditioningitontheground
truthtextdescriptionoftheRPMs. WeprovidethepromptsinAppendixA.1.
Does the oracle text description improve the model’s performance? The original evaluation
(Tables1and2)requiresthemodeltodirectlygeneratetheanswer,makingitdifficulttodisentangle
the understanding and deductive reasoning aspects. To examine the VLMs more closely, we pro-
videdeachevaluatedmodelwithoracletextdescriptionsthatweremanuallycreatedbytheauthors.
Wethenevaluatedthemodels’performanceonthethreeRPMproblemsandpresenttheresultsin
Table5(GPT-4V+OracleDesc.). TheoracletextdescriptionscanbefoundintheAppendixA.3.
WealsoprovidesampledrationalegeneratedbyGPT-4VintheAppendixA.4.
Itisevidentthatthemodel’sperformancehasbeensignificantlyimprovedwiththeadditionoforacle
descriptionsforeachpattern(Table5).Themodelsareabletoanalyzethegivenpatternsanddeduce
rulesfortheM-EasyandM-MediumRPMs,andproviderationalefortheproblem. FortheM-Hard
RPM, the models demonstrate some capability of reasoning, albeit with some challenges and is
far from human parity. We provide additional examples in the Appendix. However, it is not clear
whetherthemodelsstillrelyheavilyonvisualcuesoriftheirreasoningispurelytext-based.
Willremovingthevisualcuesharmthemodel? Next,weexaminewhethertextualinformation
alone is sufficient by removing the visual information. The results, shown in Table 5 (GPT-4V
+ Oracle Desc. - Visual), are intriguing. Without visual information, the models can maintain a
similar level of performance for M-Easy and M-Medium RPMs. Notably the result solely rely on
thetextualinformationoftheinputissuperiortotheGPT-4Vbaseline,whichmostlyrelyonvisual
information of the input. However, as the tasks become more challenging (M-Hard RPM), the
modelsstarttostruggle. TheperformanceisalsoworsethanGPT-4Vbaseline. Thissuggeststhat
fortasksthatinvolvecomplexspatiallayoutsandrelationalreasoning,textalonemaybeinsufficient
and potentially confusing, while visual cues may provide additional visual alignment and better
comparativeattention. Insuchcases,visualinformationandtextualclueswouldcomplementeach
other and work in synergy to achieve the optimal performance. Interestingly, when we provide
GPT-4Vwithanincorrectdescription,thereisaroundan80%chancethatthemodelrecognizesthe
mismatch between the text and the image and responds with statements such as: “I believe there
hasbeenamisinterpretationoftheprovidedimage”. Themodel,nevertheless,stillgeneratessome
rationalewhichseemsadheremorecloselytothetextdescriptionthantothevisualcues.
8Can the performance be improved by reasoning with noisy text descriptions generated by
the model itself? Drawing inspiration from Chain-of-Thoughts (CoT) in the text domain (Wei
et al., 2022) and the recent Self-Imagine work (Akter et al., 2024), we further investigate whether
VLMscanenhancetheirperformanceusingnoisytextdescriptionsthattheygenerateontheirown.
This also helps us understand the extent to which VLM reasoning relies on accurate descriptions
ofimagesandtheextenttowhichitcanrecoverfromerrorsinthedescriptions. Table5(GPT-4V
+ Gen Desc.) shows that incorrect text descriptions can still produce a gain. The gap between
self-generateddescriptionsandoracledescriptions,however,variesacrossthedifferentcases.
5.3 HowgoodistheVLM’shypothesisverificationonthistask?
Finally, We tested the performance of GPT-4V when it received both an oracle description and
an oracle rationale. The oracle rationale, which can be found in Appendix A.1, only includes the
explanationoftheunderlyingrulewithoutpredictingthefinalpatternoranswer. Theresultsfor10
repetitionsonmanualexamplesareshowninTable5(GPT-4V+OracleDesc. +Rationale).
Surprisingly,comparedtotherowrepresentingGPT-4V+OracleDesc.,theoraclerationaledidnot
significantly improve accuracy. In cases where the model failed, it sometimes directly generated
anincorrectanswerandatothertimesextendedtherationalebutstillgeneratedfalseanswers. For
example, for M-easy, GPT-4V continued to generate “the third row should have a star, as the first
two boxes of the third row (square and circle) already have a star.” This indicates that hypothesis
generationandverificationarecloselytiedtodeductivereasoning,andthemodelhasnotyetreached
human-levelperformanceinfollowinghintsandturninglearnedrulesintofuturepredictions.
Interestingly, strong models like GPT-4V exhibit some strategies similar to humans. For instance,
theyoftenusetheansweroptionsalongwiththegridtoformandtestshypotheses,ratherthangen-
eratingahypothesissolelybasedonthegridandthencheckingforanymatcheswiththealternative
shapes. 6 GPT-4Valsosometimesemploysastrategyofeliminationtoruleoutincorrectanswers
(e.g.,“therightshapeshouldhaveacrosssign,whichleavestheoptionstoCandF.”).
5.4 Howdoesthepromptformatinfluencethemodelprediction?
The format of the prompt can sometimes significantly im-
PromptingStructure Mensa
pacttheperformanceofVLM.Forexample, wefoundthe
arrangement of task instruction and images is crucial to
GeminiProImageFirst 2.3±1.3
GeminiProInstructionFirst 5.4±1.2
GeminiPro. WeshowtheresultsinTable6. Weobserved
GPT4V1-Shotw/oSentinelToken 6.1±1.5
a remarkable 200% increase in prediction accuracy when
GPT4V1-Shotw/SentinelToken 7.8±1.7
we simply altered the sequence of these elements. How-
ever,wedon’tobservesimilarconclusionfromothertested Table 6: Average number of cor-
models. rect predictions made by GPT4-V
and Gemini Pro on the Mensa test,
We also delves into the differences in how the model
demonstrating its sensitivity to the
performs under 0-shot and 1-shot evaluation setups. We
structureofpromptsused.
discovered that using special sentinel tokens, such as
[{BEGIN/END} OF EXAMPLE],toclearlyseparatetextpromptsfromimageshelpsthemodeldelin-
eatetaskinstructionsfromin-contextexamples. Thismethodofstructuringpromptsisparticularly
effective in aiding the model’s comprehension across all tested VLMs. For instance, we show the
resultsofGPT-4VinTable6.
This study underscores that VLMs, unlike their text-only counterparts, can benefit significantly
fromamorestructuredformatintheirtaskprompts. Furthermore,theinteractionbetweendifferent
modalities,suchastextandimage,needstobecarefullyconsideredandevaluated.
6 Conclusion
This work is a systematic evaluation of the performance of popular Vision-Language Models
(VLMs) in a variety of Raven’s Progressive Matrices (RPMs). These tasks serve as a challeng-
6Thisgenerate-then-verifystrategyaccountsforlessthan10%ofGPT-4V’sbehaviorinourobservation.
Insuchcasesthemodeloftenrejectstheoptionsprovidedandrespondsasfollows: “Unfortunately,thegiven
optionsdonotcorrespondwiththeidentifiedpattern.”
9ingbenchmarkforassessingthemodels’abilitytoreasonbasedonvisualclues. Weobservedthat
the current state-of-the-art VLMs still fall short of achieving human-level performance on these
tasks, with the best-performing models being close-sourced. Our analysis of the models’ perfor-
mance reveals that perceptual understanding may be the main bottleneck, as the models perform
better when provided with appropriate textual descriptions. In future work, it would be intriguing
tovalidateourhypothesisconcerningtheblindspotofVLMswhenitcomestodescribingpatterns.
Thisinvestigationhasthepotentialtoenhancethegeneralrecognitionandattentivenesscapabilities
ofVLMs.Additionally,exploringthedevelopmentofcontrastivelearningorreinforcementlearning
algorithmscouldfurtherimprovethemodel’svisualdeductivereasoningabilities.
7 Acknowledgement
WethankZijinGu,YusuQian,RussWebb,YinfeiYang,ZheGanfortheirexceptionalcontribution
toourwork.
10References
MarwaAbdulhai,IsadoraWhite,CharlieSnell,CharlesSun,JoeyHong,YuexiangZhai,KelvinXu,
andSergeyLevine. Lmrlgym: Benchmarksformulti-turnreinforcementlearningwithlanguage
models. arXivpreprintarXiv:2311.18232,2023.
Sam Acquaviva, Yewen Pu, Marta Kryven, Theodoros Sechopoulos, Catherine Wong, Gabrielle
Ecanow,MaxwellNye,MichaelTessler,andJoshTenenbaum. Communicatingnaturalprograms
to humans and machines. Advances in Neural Information Processing Systems, 35:3731–3743,
2022.
Kian Ahrabian, Zhivar Sourati, Kexuan Sun, Jiarui Zhang, Yifan Jiang, Fred Morstatter, and Jay
Pujara.Thecuriouscaseofnonverbalabstractreasoningwithmulti-modallargelanguagemodels.
arXivpreprintarXiv:2401.12117,2024.
Syeda Nahida Akter, Aman Madaan, Sangwu Lee, Yiming Yang, and Eric Nyberg. Self-imagine:
Effective unimodal reasoning with multimodal models using self-imagination. arXiv preprint
arXiv:2401.08025,2024.
Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In
CVPR,June2016.
StanislawAntol,AishwaryaAgrawal,JiasenLu,MargaretMitchell,DhruvBatra,CLawrenceZit-
nick,andDeviParikh. Vqa:Visualquestionanswering. InProceedingsoftheIEEEinternational
conferenceoncomputervision,pp.2425–2433,2015.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang
Zhou,andJingrenZhou.Qwen-vl:Afrontierlargevision-languagemodelwithversatileabilities.
arXivpreprintarXiv:2308.12966,2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
Boyuan Chen, Zhuo Xu, Sean Kirmani, Brian Ichter, Danny Driess, Pete Florence, Dorsa Sadigh,
Leonidas Guibas, and FeiXia. Spatialvlm: Endowingvision-language modelswith spatial rea-
soningcapabilities. arXivpreprintarXiv:2401.12168, 2024. URLhttps://arxiv.org/abs/
2401.12168.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
languagemodelstrainedoncode. arXivpreprintarXiv:2107.03374,2021.
PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,and
OyvindTafjord. Thinkyouhavesolvedquestionanswering? tryarc,theai2reasoningchallenge.
arXivpreprintarXiv:1803.05457,2018.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to
solvemathwordproblems. arXivpreprintarXiv:2110.14168,2021.
Harm De Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, and Aaron
Courville. Guesswhat?! visualobjectdiscoverythroughmulti-modaldialogue. InProceedingsof
theIEEEConferenceonComputerVisionandPatternRecognition,pp.5503–5512,2017.
MorGeva,DanielKhashabi,EladSegal,TusharKhot,DanRoth,andJonathanBerant. Didaristotle
usealaptop? aquestionansweringbenchmarkwithimplicitreasoningstrategies. Transactionsof
theAssociationforComputationalLinguistics,9:346–361,2021.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and
Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint
arXiv:2009.03300,2020.
11JieHuangandKevinChen-ChuanChang. Towardsreasoninginlargelanguagemodels: Asurvey.
arXivpreprintarXiv:2212.10403,2022.
JustinJohnson,BharathHariharan,LaurensVanDerMaaten,LiFei-Fei,CLawrenceZitnick,and
Ross Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual
reasoning. InCVPR,pp.2901–2910,2017.
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. ReferItGame: Referring
to objects in photographs of natural scenes. In Proceedings of the 2014 Conference on Em-
pirical Methods in Natural Language Processing (EMNLP), pp. 787–798, Doha, Qatar, Oc-
tober 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1086. URL
https://aclanthology.org/D14-1086.
MaithileeKunda,KeithMcGreggor,andAshokKGoel. Acomputationalmodelforsolvingprob-
lems from the raven’s progressive matrices intelligence test using iconic visual representations.
CognitiveSystemsResearch,22:47–66,2013.
Philippe Laban, Wojciech Krys´cin´ski, Divyansh Agarwal, Alexander R Fabbri, Caiming Xiong,
ShafiqJoty,andChien-ShengWu. Llmsasfactualreasoners: Insightsfromexistingbenchmarks
andbeyond. arXivpreprintarXiv:2305.14540,2023.
PercyLiang,RishiBommasani,TonyLee,DimitrisTsipras,DilaraSoylu,MichihiroYasunaga,Yian
Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language
models. arXivpreprintarXiv:2211.09110,2022.
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv
preprintarXiv:2304.08485,2023.
PanLu,HritikBansal,TonyXia,JiachengLiu,ChunyuanLi,HannanehHajishirzi,HaoCheng,Kai-
WeiChang,MichelGalley,andJianfengGao. Mathvista: Evaluatingmathematicalreasoningof
foundationmodelsinvisualcontexts. arXivpreprintarXiv:2310.02255,2023.
PanLu,HritikBansal,TonyXia,JiachengLiu,ChunyuanLi,HannanehHajishirzi,HaoCheng,Kai-
WeiChang,MichelGalley,andJianfengGao. Mathvista: Evaluatingmathematicalreasoningof
foundationmodelsinvisualcontexts. InICLR,2024.
Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Ba¨uerle, A´ngel Alexander
Cabrera, Krish Dholakia, Chenyan Xiong, and Graham Neubig. An in-depth look at gemini’s
languageabilities. arXive-prints,pp.arXiv–2312,2023.
OpenAI. Gpt-4technicalreport,2023.
FabioPetroni, TimRockta¨schel, PatrickLewis, AntonBakhtin, YuxiangWu, AlexanderHMiller,
andSebastianRiedel. Languagemodelsasknowledgebases? InEMNLP,2019.
Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue:
Learningfeaturematchingwithgraphneuralnetworks. InProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition,pp.4938–4947,2020.
Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for
imagecaptioningwithreadingcomprehension. InECCV,pp.742–758.Springer,2020.
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam
Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adria` Garriga-Alonso, et al. Beyond the
imitationgame:Quantifyingandextrapolatingthecapabilitiesoflanguagemodels.arXivpreprint
arXiv:2206.04615,2022.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,
RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighly
capablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdChi,SharanNarang,AakankshaChowdh-
ery,andDennyZhou. Self-consistencyimproveschainofthoughtreasoninginlanguagemodels.
arXivpreprintarXiv:2203.11171,2022.
12JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,Denny
Zhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. Advancesin
NeuralInformationProcessingSystems,35:24824–24837,2022.
ZhengyuanYang,LinjieLi,KevinLin,JianfengWang,Chung-ChingLin,ZichengLiu,andLijuan
Wang. theDawnofLmms: PreliminaryExplorationsWithGpt-4v(ision),2023.
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,
DongfuJiang,WeimingRen,YuxuanSun,etal. Mmmu: Amassivemulti-disciplinemultimodal
understandingandreasoningbenchmarkforexpertagi. arXivpreprintarXiv:2311.16502,2023.
ChiZhang,FengGao,BaoxiongJia,YixinZhu,andSong-ChunZhu.Raven:Adatasetforrelational
andanalogicalvisualreasoning. InCVPR,pp.5317–5327,2019.
Yizhe Zhang, Jiarui Lu, and Navdeep Jaitly. the Entity-deduction Arena: A Playground for
Probing the Conversational Reasoning and Planning Capabilities of Llms. arXiv preprint
arXiv:2310.01468,2023.
13A Appendix
A.1 Promptsfordifferentdatasetandtasks
Welistedallthepromptsweusedtogeneratetheresponsefromdifferentmodels.
Mensa. (0-shot)
Youcanseea3x3gridof9boxes,oneofwhichisempty(markedas?). Youhavetochoose
which of the 6 alternative shapes (A-F) should be placed in the empty box in order to
completethepatternthatconnectstheshapes. Finally,provideyourpredictionasAnswer:
“X”
{queryimage}
Mensa(1-shot)
Youcanseea3x3gridof9boxes,oneofwhichisempty(markedas?). Youhavetochoose
whichofthe6alternativeshapes(A-F)tobeplacedintheemptyboxinordertocomplete
thepatternthatconnectstheshapes. Thinkstepbystepbyfirstdescribetheeachboxinthe
3x3 grid, and each of the alternative shapes as the candidate answers. Then identify the
commonpattern. Finally,provideyourpredictionasAnswer: ”X”
Forexample,forthefollowingimage:
{in-contextimage}
{in-contextdescription}
{in-contextrationale}
{in-contextanswer}
Nowdothefollowingone:
{queryimage}
Mensa(1-shot)+OracleDesc.
Youcanseea3x3gridof9boxes,oneofwhichisempty(markedas?). Youhavetochoose
whichofthe6alternativeshapes(A-F)tobeplacedintheemptyboxinordertocomplete
the pattern that connects the shapes. Think step by step by first describe the each box in
the 3x3 grid, and each of the alternative shapes as the candidate answers. Then identify
thecommonpattern. Finally,provideyourpredictionasAnswer: ”X”
Forexample,forthefollowingimage:
{in-contextimage}
{in-contextdescription}
{in-contextrationale}
{in-contextanswer}
Nowdothefollowingone:
{queryimage}
{queryoracledescription}
14Mensa(1-shot)+OracleDesc. +OracleRationale
Youcanseea3x3gridof9boxes,oneofwhichisempty(markedas?). Youhavetochoose
whichofthe6alternativeshapes(A-F)tobeplacedintheemptyboxinordertocomplete
the pattern that connects the shapes. Think step by step by first describe the each box in
the 3x3 grid, and each of the alternative shapes as the candidate answers. Then identify
thecommonpattern. Finally,provideyourpredictionasAnswer: ”X”
Forexample,forthefollowingimage:
{in-contextimage}
{in-contextdescription}
{in-contextrationale}
{in-contextanswer}
Nowdothefollowingone:
{queryimage}
{queryoracledescription}
{queryoraclerationale}
IntelligenceTest(0-shot)
ThisimageisanIntelligenceTestQuestionaskingwhichfigure(A-F)belongsinthebottom
rightbox. Pleaseselectthecorrectanswer. Youmustfirstgiveyourexplanationandthen
outputtheanswerattheendofyourresponseintheformat: “Thecorrectansweris: ”.
{queryimage}
RAVEN(0-shot)
You can see a 3x3 grid of 9 boxes, one of which is empty (marked as ?). You have to
choosewhichofthe8alternativeshapes(A-H)shouldbeplacedintheemptyboxinorder
tocompletethepatternthatconnectstheshapes. Youmustfirstgiveyourexplanationand
thenoutputtheanswerattheendofyourresponseintheformat: “Thecorrectansweris:
”.
{queryimage}
15SegmentedPrompt(0-shot)
Inthefirstimage,youwillseea3x3gridof9boxes. Eachrowhasthreeimagesandisto
bereadfromlefttoright,withthelastimageinthethirdrowismissing(markedas?).Your
task is to infer the correct pattern that should complete each row based on the sequence
observedintheprecedingpatterns,andfinallyselecttherightoption(A,B,C,D,E,F)that
fitsthe3rdrow’slastimage.
{queryimage}
Foryourconvenience,Iprovide15segmentedfigures: thequestiongridissegmentedinto
9parts,andtheansweroptionsaresegmentedinto6parts. q0,q1,andq2arethefirstrow,
q3,q4,andq5arethesecondrow,andq6,q7,andq8arethethirdrow. A,B,C,D,E,and
Faretheansweroptions. Yourtaskistofindwhichoptionshouldbeplacedinq8.
q0:{q0image}
q1:{q1image}
q2:{q2image}
q3:{q3image}
q4:{q4image}
q5:{q5image}
q6:{q6image}
q7:{q7image}
q8:{q8image}
A:{Aimage}
B:{Bimage}
C:{Cimage}
D:{Dimage}
E:{Eimage}
F:{Fimage}
For each row, analyze the changes and relationships between the images. Consider the
numberofshapes,thetypesofshapes,theirpositions,theshading,andanyotherchanges
thatoccurfromonepatterntothenext. Onceyouhaveidentifiedtheruleorsequencethat
applies to the rows, select the option (A, B, C, D, E, F) that contains the pattern which
correctlycompletesthethirdrowsequence.
Pleasefirstgiveyourexplanationandthenwritetheanswerattheendofyourresponsein
theformat: “Thecorrectansweris: ”.
A.2 Samplingmethodsformodelevaluations
In the main text, we present evaluation results obtained by independently running each model 10
timesandestimatingtheirrespectivestatistics. ForGPT4-V,wefoundthatusingazerotemperature
setting along with different random seeds effectively balances prediction accuracy with reasoning
diversity. In the case of Gemini Pro Vision, the optimal performance is achieved at a temperature
settingof0.4. However,fortheself-consistencyevaluation,weincreasethesamplingtemperature
toamaximumof 0.7. Thisadjustmentfacilitatesmorevariedpredictionswhicharebeneficialfor
majority-voting processes. For all other models, we observed that a lower temperature range, be-
tween0and0.2,isnecessary. Thislowertemperaturehelpsthemodelsbetteradheretoinstructions
andimprovestheirtaskpredictionaccuracy,thoughitalsoresultsinsomewhatlessvariabilityinthe
models’predictions.
16M-Easy(Oracle)
Description
Thegrid:
1. Topleftbox: squarewithacrosssign.
2. Topcenterbox: circlewithastar.
3. Toprightbox: Emptytriangle.
4. Middleleftbox: emptysquare.
5. Middlecenterbox: circlewithacrosssign.
6. Middlerightbox: trianglewithastar.
7. Bottomleftbox: squarewithastar.
8. Bottomcenterbox: emptycircle.
9. Bottomrightbox: ’?’.
Thealternativeshapesare:
• A.Trianglewithastar.
• B.Trianglewithaplussign.
• C.Circlewithacrosssign.
• D.Circlewithastar.
• E.Emptytriangle.
• F.Trianglewithacrosssign.
Rationale
Analyzing the grid, there seems to be a pattern involving both the shapes and the
symbolsinsidethem:
• Thesymbolsinsidetheshapesalternatebetweenacross,astar,andthena
blankspaceasyoumovehorizontallyacrosseachrow.
• Theshapesthemselvesalsoalternatewithineachrow–thisisseenwiththe
square,circle,andtrianglerepeatingineachrowinthatorder.
17A.3 OracledescriptionandrationaleforeachRPMs
M-Medium(Oracle)
Description
Thegrid:
1. Topleftbox: Adownward-pointingtrianglewiththreedotsinit.
2. Topcenterbox: Aleftward-pointingtrianglewithtwodotsinit.
3. Toprightbox: Anupward-pointingtrianglewithonedotinit.
4. Middleleftbox: Arightward-pointingtrianglewithtwodotsinit.
5. Middlecenterbox: Adownward-pointingtrianglewithonedotinit.
6. Middlerightbox: Aleftward-pointingtrianglewiththreedotsinit.
7. Bottomleftbox: Anupward-pointingtrianglewithonedotinit.
8. Bottomcenterbox: Arightward-pointingtrianglewiththreedotsinit.
9. Bottomrightbox: ’?’.
Thealternativeshapesare:
• A.Anupward-pointingtrianglewithtwodotsinit.
• B.Adownward-pointingtrianglewithonedotinit.
• C.Aleftward-pointingtrianglewithonedotinit.
• D.Arightward-pointingtrianglewithtwodotsinit.
• E.Aleftward-pointingtrianglewithtwodotsinit.
• F.Adownward-pointingtrianglewithtwodotsinit.
Rationale
Analyzingthegrid,itappearsthatthere’sapatternrelatedtothedirectionthetriangle
ispointingandthenumberofdotswithinthetriangles.
First,let’sestablishthepatternsoftriangledirectionsanddotscount:
• Thefirstrowhasthetrianglespointingdownward,totheleft,andthenup.
• Thesecondrowhasthetrianglespointingrightward, downward, andthen
totheleft.
• This implies that the direction that the triangle is pointing to is rotating
clockwiseineachrow.
Nowlet’slookatthepatterninthenumberofdots:
• Thefirstrowhas3,2,1dots.
• Thesecondrowhas2,1,3dots.
• Thisimpliesapatternofadecreasingsequencefromlefttoright.
18Hard(Oracle)
Description
Thegrid:
1. Topleftbox: whitecircle,whitetriangle,blacksquare.
2. Topcenterbox: whitetriangle,blackcircle,whitesquare.
3. Toprightbox: blacksquare,blacksquare,whitetriangle.
4. Middleleftbox: blackcircle,whitesquare,whitetriangle.
5. Middlecenterbox: blacksquare,whitetriangle,blacksquare.
6. Middlerightbox: whitetriangle,blacksquare,whitecircle.
7. Bottomleftbox: whitetriangle,blacksquare,blacksquare.
8. Bottomcenterbox: blacksquare,whitecircle,whitetriangle.
9. Bottomrightbox: ’?’.
Thealternativeshapesare:
• A.whitecircle,whitetriangle,blacksquare.
• B.blackcircle,whitesquare,whitetriangle.
• C.whitecircle,whitesquare,blacktriangle.
• D.whitecircle,blacksquare,whitetriangle.
• E.blacksquare,whitetriangle,whitecircle.
• F.whitesquare,whitetriangle,blackcircle.
Rationale
Analyzingthegrid,itappearsthatthere’sapatternalongthediagonallinesfromtop
righttobottomleftdirection.
• Thethreeelementsinthefirstdiagonalline(topright,middlecenter,bot-
tomleft)consistofablacksquare,ablacksquare,andawhitetriangle. As
wefollowthediagonalline,theseelementsrotate120degreesclockwise.
• Thethreeelementsintheseconddiagonalline(topleft,middleright,bot-
tomcenter)consistofawhitecircle, awhitetriangle, andablacksquare.
As we follow the diagonal line, these elements rotate 120 degrees clock-
wise.
• This implies that the third diagonal line (top center, middle left, bottom
right)shouldalsofollowthispattern.
A.4 RationalegeneratedbyGPT-4V
In this paper, we present several examples that demonstrate the reasoning capabilities of GPT-4V
for solving certain RPM tasks. From the examples, it becomes evident that as the difficulty level
oftheproblemincreases,theGPT-4Vtendstostrugglewithitsreasoningabilitiesandprovidesan
explanationthatisnonsensical.
19Hard(rationale)
Analyzingthegrid, itappearsthatthere’sapatternalongthediagonallinesfromtopright
to bottom left direction. - The three elements in the first diagonal line (top right, middle
center, bottom left) consist of a black square, a black square, and a white triangle. As we
followthediagonalline,theseelementsrotate120degreesclockwise. -Thethreeelements
intheseconddiagonalline(topleft,middleright,bottomcenter)consistofawhitecircle,a
whitetriangleandablacksquare. Aswefollowthediagonalline,theseelementsrotate120
degreesclockwise. -Thisimpliesthatthethirddiagonalline(topcenter,middleleft,bottom
right)shouldalsofollowthispattern.
GeneratedrationaleforM-MediumRPM
Analyzingthegrid,itappearsthere’sapatternrelatedtothedirectionthetriangleispointing
andthenumberofdotswithinthetriangles.
First,let’sestablishthepatternsoftriangledirectionsanddotscount:
• Thefirstrowhasthetrianglespointingdownward,totheleft,andthenup.
• The second row has the triangles pointing rightward, downward, and then to the
left.
• It implies that for the third row, following the same pattern, the triangles should
pointupward,rightward,andthendownward.
Nowlet’slookatthepatterninthenumberofdots:
• Alongtheverticalcolumns,wecanobserveapatternindotcounts:
– Thefirstcolumnhas3,2,1dots.
– Thesecondcolumnhas2,1,3dots.
– Ifwefollowthispattern,thethirdcolumnshouldhave1,3,2dotsfromtopto
bottom.
Given these observed sequences, the third row should have a triangle pointing downward
with two dots in it, fitting the patterns of both direction and dot count. Looking at the
alternativeshapes,optionFisthedownward-pointingtrianglewithtwodotswithinit.
Answer: ”F”
20GeneratedrationaleforM-HardRPM
Analyzingthegrid,wecandeducethefollowingpattern:
1. The shapes in each box follow a sequence that changes from one cell to the next
horizontally.
2. Therearethreetypesofshapes: circle,square,andtriangle,andeachtypealsohas
twovariantsofcolor: blackorwhite.
Tounderstandhowthesequenceworks,wecanlayoutthepatternswesee:
• Inthetoprow,westartwithawhitecircle,whitetriangle,andblacksquare.Moving
right,theshapesrotatepositionsclockwise,andtheircolorsinvert(thewhitecircle
becomesablackcircleandsoon).
• Thesamepatternfollowsinthemiddlerow:blackcircle,whitesquare,whitetrian-
glebecomesblacksquare,whitetriangle,blacksquare,andagaininvertingcolors
androtatingtheshapesclockwise.
• Followingthispatterninthebottomrow,whitetriangle,blacksquare,blacksquare
shouldchangebyrotatingtheshapesclockwiseandinvertingthecolors. Theblack
squarewouldbecomeawhitesquare,theblacksquarewouldbecomeawhitecircle,
andthewhitetrianglewouldstaythesame,remaininginthebottomposition.
Matchingthisdescriptionwiththealternativesgiven,option”F”(whitesquare,whitetrian-
gle,blackcircle)fitstheobservedsequence.
Answer: ”F”
21