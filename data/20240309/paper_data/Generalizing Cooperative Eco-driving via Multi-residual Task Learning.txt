Generalizing Cooperative Eco-driving via Multi-residual Task Learning
Vindula Jayawardana1∗, Sirui Li1, Cathy Wu1, Yashar Farid2, and Kentaro Oguchi2
Abstract—Conventional control, such as model-based con-
trol, is commonly utilized in autonomous driving due to
its efficiency and reliability. However, real-world autonomous
driving contends with a multitude of diverse traffic scenarios
that are challenging for these planning algorithms. Model-
freeDeepReinforcementLearning(DRL)presentsapromising
avenue in this direction, but learning DRL control policies
that generalize to multiple traffic scenarios is still a challenge.
To address this, we introduce Multi-residual Task Learning
(MRTL), a generic learning framework based on multi-task
learning that, for a set of task scenarios, decomposes the
Fig. 1: In a signalized intersection, AVs lead platoons of
control into nominal components that are effectively solved
human-driven vehicles. As Lagrangian actuators, they re-
by conventional control methods and residual terms which
are solved using learning. We employ MRTL for fleet-level duce fleet emissions by controlling their own acceleration
emission reduction in mixed traffic using autonomous vehicles and shepherding the human drivers through car following
as a means of system control. By analyzing the performance dynamics.
of MRTL across nearly 600 signalized intersections and 1200
trafficscenarios,wedemonstratethatitemergesasapromising
models tends to result in poor generalization and, in some
approach to synergize the strengths of DRL and conventional
methods in generalizable control. cases, even increasing emissions levels.
On the other hand, DRL operates without the need for
I. INTRODUCTION
a predefined dynamics model, hence model-free. It has the
Autonomous vehicles (AVs) are surging in popularity due capacitytoaddresscontrolchallengesthatprovechallenging
to rapid technological advancements. Lately, AVs also have forconventionalmethodsasitspecifiescontrolobjectivesin-
been used as Lagrangian actuators for system-level traffic directly within a reward function rather than explicit control
control. Lagrangian control describes microscopic level traf- actions. Although limited in success, DRL has demonstrated
fic control techniques based on mobile actuators (e.g., vehi- the capacity to adapt to changes in the underlying environ-
cles)ratherthanfixed-locationactuators(e.g.,trafficsignals). mentalconditions[7],underscoringitspotentialtogeneralize
Therefore, it involves planning a fleet of AVs to accomplish across various traffic scenarios.
a given objective at the system level, which involves both However,developingDRLalgorithmsforLagrangiancon-
AVs and human drivers. These objectives include mitigating trol in diverse traffic settings still remains a challenge. Real-
traffic congestion [1], curbing emissions [2], and promoting worldroadsarecomplex,withcomplexitiesincludingvehicle
smoother traffic flows [3]. interactions,variedroadtopologies,andexternalcontrolslike
In particular, recent work explores the use of AVs as traffic signals and stop signs. These complexities are often
Lagrangian actuators for fleet-wide emission reduction [4]. unpredictable, introducing uncertainty. Devising eco-driving
As illustrated in Figure 1, the goal is to reduce emissions planning algorithms capable of handling multiple scenarios
of the fleet by controlling and coordinating AVs and ex- is thus demanding, and many existing studies focus on a
erting control over human-driven vehicles. Methods from few scenarios [4], leading to less meaningful insights and
heuristics[5],tomodel-basedmethods[6],tomodel-free[2] potential overfitting in evaluations [8].
methods, have been used in tackling this challenge.
In this study, we address this challenge of algorithmic
Although model-based control strategies like model-
generalization for DRL across various scenarios, such as
predictive control are frequently employed [4], they rely on
different traffic scenarios in cooperative eco-driving. We
the assumption of having a precise vehicle dynamics model.
introduce Multi-residual Task Learning (MRTL), a generic
Yet,devisingsuchamodel,includingthemultitudeoffactors
framework that combines DRL and conventional control
influencing driving dynamics, is challenging. In the absence
methods. MRTL divides each scenario into parts solvable
of them, these methods often fall short in terms of adapting
by conventional control and residuals solvable by DRL. The
to various traffic scenarios. The deployment of simplified
final control input for a scenario is thus the superposition of
these two control inputs.
1Massachusetts Institute of Technology, Cambridge, MA, USA
{vindula, siruil, cathywu}@mit.edu We employ MRTL in eco-driving at signalized intersec-
2 Toyota InfoTech Labs and Toyota Motor North America, tions,achievingbettergeneralizationinnearly600signalized
Mountain View, CA, USA {yashar.zeiynali.farid,
intersections across 1200 traffic scenarios. While many ex-
kentaro.oguchi}@toyota.com
∗ Workdoneduringtheauthor’sinternshipatToyotaInfoTechLabs. isting works focus on a few eco-driving scenarios [4], to the
4202
raM
7
]OR.sc[
1v23240.3042:viXrabest of our knowledge, we are the first to solve this problem operatesinlow-dimensionalstatespaces,leavingitssuitabil-
on a large scale. ity for high-dimensional spaces uncertain. It’s worth noting
Our key contributions are: that our approach differs fundamentally, focusing on multi-
task learning compared to their meta-learning approach.
• We present a generic learning framework called Multi-
Nevertheless, this work underscores the potential of RRL
residual Task Learning to enable algorithmic general-
in enhancing control policy generalization.
ization of DRL algorithms.
On the other hand, combining model-based methods with
• We employ the MRTL framework to devise generaliz-
learning (model-free) has been a topic of interest for some
able control policies for cooperative eco-driving.
time [14]. On the one hand, these methods often involve
• Analyzing nearly 1200 traffic scenarios across 600
learning a dynamics model and then using the model for
signalizedintersections,wedemonstratethatourMRTL
trajectoryoptimizationormodelpredictivecontrol[15,16]to
framework enables control generalization, outperform-
simulate experience [17] or to compute gradients for model-
ing baseline control methods by a large margin.
free updates [18]. In another line of work, learning is used
forcapturingtheobjectives[19]orconstraints[20].Recently,
II. RELATEDWORK
there has also been a line of work that looks at learning a
The use of residuals in learning has previously been corrective term to analytical physical models [21] with the
explored both in supervised learning and reinforcement purpose of performing better predictive control.
learning. He et al. [9] first propose residual neural networks In summary, while RRL has proven effective in single-
where they reformulate the layers of a neural network as agent,single-taskroboticmanipulations,noneoftheexisting
learning residual functions. In reinforcement learning, the studies have showcased its application to multi-agent coop-
use of residuals takes a slightly different form. The closest erative control with the capacity for generalization across a
toourworkisresidualreinforcementlearning(RRL),which range of scenarios, let alone in Lagrangian control. Alterna-
was first introduced simultaneously by Silver et al. [10] and tively, combining model-based and model-free methods has
Johannink et al. [11] for robot control. exhibited mixed results, and none of them adequately tackle
Silver et al. [10] primarily look at how RRL can improve the challenges of algorithmic generalization. In this study,
the performance of robotics manipulation tasks with various we aim to bridge this gap in the field.
nominal policies as backbones. They demonstrate that RRL
performs well when the environment is partially observable
III. PRELIMINARIES
andwhenthereissensornoise,modelmisspecifications,and A. Reinforcement Learning
controllermiscalibrations.Johanninketal.[11]furthershow
In reinforcement learning, an agent learns a control pol-
that RRL can be used to deal with the sim-to-real transfer.
icy by interacting with its environment, typically modeled
In particular, it can be used to modify the learned controller
as a Markov Decision Process (MDP) denoted as M =
such that it can react to the modeling inaccuracies observed
⟨S,A,p,r,ρ,γ⟩. Here, S represents the set of states, A
in the simulations to better adapt to the real world.
denotes the possible actions, p(s |s ,a ) denotes the tran-
t+1 t t
While these works lay a foundation, the focus is single- sition probability from the current state s to the next state
t
agent, single-task robotic manipulations characterized by s upon taking action a , the reward received for action
t+1 t
relatively simple control. In contrast, we look at multi- a at state s is r(s ,a ) ∈ R, a distribution over the initial
t t t t
agent,multi-taskscenarios,necessitatingnotonlyoptimizing statesisρ,andγ ∈[0,1]isadiscountingfactorthatbalances
performance on individual tasks but also extending to robust immediate and future rewards.
and generalization control synthesis. Given the MDP, we seek to find an optimal policy π∗ :
In autonomous driving, Zhang et al. [12] use RRL and S → A over the horizon H that maximizes the expected
reducethelaptimeinautonomousracing.Theyfurthershow cumulative discounted reward over the MDP.
thetransferabilityoflearnedpoliciesbytransferringapolicy (cid:34) H (cid:35)
from one track to another and to new tracks. However, this π∗(s)=argmaxE (cid:88) γtr(s ,a )|s ,π (1)
t t 0
work primarily looks at single-agent racing, and multi-agent π
t=0
racing poses different challenges. Furthermore, while some
B. Multi-task Reinforcement Learning
transferabilityresultshavebeenshown,itisstilllimitedtoa
Inmulti-taskreinforcementlearning,weextendthesingle-
few select racing tracks instead of a diverse set of scenarios.
MDP(singletask)reinforcementlearninginSectionIII-Ato
Moreover, autonomous racing and Lagrangian control have
multiple MDPs (multiple tasks). Accordingly, our objective
contrasting dynamics: racing involves competition, while
in finding optimal policy thus becomes,
Lagrangian control involves cooperation.
RRL is further utilized in synthesizing generalizable re- (cid:34) H (cid:35)
(cid:88)(cid:88)
π∗(s)=argmaxE γtr (s ,a )|s ,π (2)
inforcement learning controllers for robotics manipulations. τ t t 0
π
Hao et al. [13] introduce a meta-residual policy learning τ∈T t=0
method that performs in unseen peg-in-hole assembly tasks. where T is the set of MDPs (tasks). Also, note that what
It improves adaptation and sample efficiency but is limited we seek in multi-task reinforcement learning is a unified
to specific robotics skills and lacks task diversity. Further, it policy that is performant over all MDPs (tasks).IV. METHOD
In this section, we formalize the concept of algorithmic
generalization in DRL and detail our generic Multi-Residual
Task Learning framework.
A. Problem Formulation
In this work, we study the algorithmic generalization of
DRL algorithms across a family of MDPs (scenarios) that
originate from a single task, such as eco-driving. To formal-
izethisexploration,ourprimaryfocusrevolvesaroundsolv-
ing Contextual Markov Decision Processes (cMDPs) [22].
AcMDPexpandsupontheMDPdiscussedinSectionIII-
Abyincorporatinga’context’.Contextservesasameansto
parameterize the environmental variations encountered, such
as changes in lane lengths at different intersections, among
other factors in eco-driving. Mathematically, we denote a
cMDP as M=⟨S,A,C,p ,r ,ρ ,γ⟩. Compared to MDPs,
c c c
a context space C is introduced, and the action space A and
state space S remain unchanged. The transition dynamics
p c, rewards r c, and initial state distribution ρ c are changed Fig. 2: Multi-task learning trains a unified policy directly
based on the context c∈C. Essentially, a cMDP M defines with environments (intersections) sampled from a distribu-
a collection of MDPs, each differing based on the context, tionofenvironments(topfigure).Multi-residualtasklearning
such that M={M c} c∼C. building on multi-task learning decomposes the cMDP into
Solving a given cMDP leads to solving the problem parts solved by a nominal policy and residual parts solved
of algorithmic generalization within that task (i.e., finding by DRL, as shown in the bottom figure.
a policy that performs well in the cMDP overall). The
generalization objective where the goal is to find a unified fleetofvehicles(bothAVsandhuman-drivenvehicles)while
policyπ∗(·)thatperformswellonallM ∈Misasfollows. having a minimal impact on travel time across all signalized
c
(cid:34) H (cid:35) intersections. Given an instantaneous emission model E(·)
(cid:88)(cid:88)
π∗(s)=argmaxE γtr (s ,a )|sc,π (3) that measures vehicular emission, we seek an AV control
c t t 0
π c∈C t=0 policy such that,
The multi-task learning framework introduced in Sec- (cid:34) (cid:88)(cid:88)nc (cid:90) Ti (cid:35)
tion III-B emerges as a natural approach to tackle cMDPs. π∗ =argminE E(a i(t),v i(t))dt+T i (4)
Here, the contexts themselves define the different tasks, π c∈C i=1 0
effectively aligning with the notion that a specific context Here,n representsthetotalnumberofindexesofbothAV
c
c ∈ C in Equation 3 corresponds to a task τ ∈ T in and human-driven vehicles in intersection defined by c, T
i
Equation 2. denotes the travel time of vehicle i, v (t), and a (t) denote
i i
thespeedandaccelerationofvehicleiattimetandC denote
B. Cooperative Eco-driving cMDP
the contexts defining a set of signalized intersections.
In cooperative eco-driving at signalized intersections, a
C. Multi-residual Task Learning
wide array of context factors come into play, including
lane lengths, speed limits, lane count, vehicle inflows, and While multi-task reinforcement learning can be used for
the timings of green and red traffic signals. These factors solving cMDPs, it struggles when multiple MDPs are com-
collectivelyshapethecontextswithintheeco-drivingcMDP, bined within one learning framework. Simultaneous training
which encompasses a spectrum of signalized intersections can lead to competition among MDPs for the learning
(MDPs). Then, we seek a unified AV control policy that agent’s limited capacity, making it hard to balance MDP-
adeptly curbs emissions of the fleet across these signalized specific and shared knowledge. Moreover, varying dynamics
intersections. acrossMDPschallengerobustadaptationandgeneralization.
MDPs within a cMDP can manifest in both single-agent Catastrophic interference risk, where new learning disrupts
and multi-agent configurations. However, cooperative eco- prior performance, hinders effectiveness further.
driving adopts multi-agent control as coordination between In addressing these issues, we propose a generic learning
AVs to reduce emissions is required. This characteristic framework designed to enhance the algorithmic generaliza-
amplifies the complexity of solving eco-driving cMDP, ne- tionofDRLalgorithms,ultimatelyenablingsolvingcMDPs.
cessitating the implicit modeling of vehicle interactions and We introduce Multi-Residual Task Learning, a unified learn-
addressing the challenges posed by partial observability. ing approach that harnesses the synergy between multi-task
The overall objective of the cooperative eco-driving at learning and residual reinforcement learning [10, 11] (Fig-
signalized intersections is to minimize the emissions of a ure2).Atitscore,MRTLoperatesbyaugmentinganygivennominal policy, which exhibits sub-optimal and varying Algorithm 1 Nominal policy π n for eco-driving
performance in each MDP in a cMDP, by learning residuals 1: procedure GLIDE OR KEEP SPEED(ego-vehicle speed
on top of it. These residuals serve as corrective measures to v(t), ego-vehicle distance to intersection d(t), traffic
address suboptimalities within the nominal policy. signal timing plan T and green light duration T )
g
Consider eco-driving at signalized intersections. An AV’s 2: Calculate time to intersection T I ← vd( (t t) )
overallemission-reductionreward,r,canbesplitintor a and 3: Calculate time to green light T G from T
r b. r a rewards AV gliding at red signals, a known approach 4: Calculate time to end green light T E ←T G+T g
for emission reduction [4], and can be achieved through a 5: if T G ≤T I ≤T E then
straightforward model-based controller [5]. Conversely, r b 6: Target speed v target ←v(t)
rewardsadaptiveglidingbehaviorstoenvironmentalchanges 7: else if T G ≥T I then
(e.g., other vehicles lane changing), a challenge for model- 8: Calculate target speed based on gliding principle
b pa os lie cd yc fo on rtr ro bll ie srs fed au se ibt lo e,s ay ls lt oe wm inc gom thp ele nx oit my. inT ar lai pn oi ln ig cya tD oR fiL
x
19 0:
:
elsv etarget ← d T( Gt)
r a while learning targets the residual r b. 11: Target speed v target ←v IDM
Toputthisformally,MRTLisconcernedwithaugmenting
12: return v target
a given nominal policy π (s,c) : S ×C → A by learning
n 13: end procedure
residualsontopofit.Inparticular,weaimtolearntheMRTL
policy π(s,c) : S ×C → A by learning a residual function
f (s,c) : S × C → A on top of a given nominal policy
θ
at its current speed; if yes, it maintains that speed (lines 5
π (s,c):S×C →A such that,
n
and 6). If the time remaining to reach the intersection is
π(s,c)=π n(s,c)+f θ(s,c) greater than the time until the traffic light turns green, the
policy initiates a gliding maneuver to arrive precisely when
where s ∈ S and c ∈ C. The gradient of the π does
the light changes (lines 7, 8, and 9). If neither condition
not depend on the π . This enables flexibility with nominal
n applies, it defaults to natural driving behavior, following the
policy choice. The effectiveness of π can vary among dif-
n IDM car-following model [23] (lines 10 and 11).
ferent MDPs within a cMDP. Hence, the role of the residual
1) What makes the nominal policy suboptimal?: The
function f in each MDP depends on MDP characteristics
θ
nominalpolicyhasinherentlimitationsduetothesimplifica-
and nominal policy performance in that MDP. In some
tionsmadeforreal-timefeasibility.First,itfocusessolelyon
MDPs, π acts as a starting point for better exploration for
n
the ego vehicle’s dynamics, ignoring nearby vehicles, which
the residual function. In others, it can be nearly optimal,
compromises its effectiveness, especially in scenarios with
requiring fewer improvements by the residual function.
human-driven vehicles, lane changes, or intersection queues.
V. MRTLFORCOOPERATIVEECO-DRIVING Furthermore, the policy doesn’t account for appropriate
vehicle behaviors during unprotected left turns, leading
In this section, we discuss the application of the MRTL
to suboptimal control results and undermining intersection
framework on eco-driving at signalized intersections. We
queuesandlanechanges.Dedicatedleftturnlanesandtraffic
procedurally generate a synthetic dataset with nearly 600
signal phases introduce unmodeled lane changes, rendering
MDPs, which represent incoming approaches at signalized
the policy ineffective when these conditions arise, impacting
intersections. We simplify the eco-driving task to focus on
its emission reduction objective.
these incoming approaches since traffic signals coordinate
conflicting approaches [2].
B. MRTL Implementation Details
Approaches are described by six features with diverse
We employ centralized training and decentralized exe-
ranges: lane length (75-400 m), vehicle inflow (675-900
cution for training MRTL policies. We use actor-critic ar-
veh/hour), speed limit (10-15 m/s), lane count (1-3), and
chitecture with three hidden layers, each with 128 neurons
green and red signal phase times (25-30s). These features
in both the actor and critic, with a learning rate of 0.005.
define the context space for the eco-driving cMDP, and each
1200 traffic scenarios are modeled in 600 intersections and
environment is a realization of these features.
two AV penetration levels (20% and 100%) in the SUMO
A. Nominal Policy simulator. PPO algorithm [24] is used as the DRL algorithm
As the nominal policy, we design a model-based heuris- with 12 workers running for 400 iterations. We use a neural
tic controller inspired by the GLOSA algorithm for eco- surrogate emission model [25] replicating MOVES [26] as
driving [5]. While our nominal policy doesn’t perform real- our emission model to measure vehicular emissions.
time optimizations, its low computational demands and pre- Each MDP in eco-driving cMDP is defined as follows.
deployment verification appeal to practical applications. We • States: speed, position of the ego-vehicle, leading and
detail the nominal policy in Algorithm 1. following vehicles in the same lane and adjacent lanes,
The nominal policy operates on a simple set of criteria thecurrenttrafficsignalphasewithremainingtime,and
aimedatreducingidlingandtherebyreducingemissions[4]. contextfeaturesincludinglanelength,speedlimit,green
First, it checks if the ego-vehicle can pass the intersection and red phase times, and approach phase count.20%penetration 100%penetration
Method
Emission↓ Speed↑ Throughput↑ Emission↓ Speed↑ Throughput↑
Multi-tasklearning 64.08% -27.70% -34.70% 95.86% -30.87% -68.11%
Nominalpolicy 13.13% -21.11% -30.07% -25.09% 11.72% -3.90%
Multi-residualtasklearning(Ours) -13.95% 12.35% 7.95% -29.09% 17.10% 5.72%
TABLE I: Performance comparison of MRTL with other baselines for eco-driving at 20% and 100% AV penetration. The
percentages are calculated compared to the naturalistic human-like driving denoted by the IDM baseline. Evaluation metrics
involve emissions reduction and speed improvement of vehicles and throughput improvement at the intersection - where
lower emissions, higher speed, and higher throughput percentages indicate better performance.
(a) Emision benefits when 20% penetration (b) Emision benefits when protected left (c) Emision benefits when unprotected left
of AVs are used with nominal policy (top) turns are present with nominal policy (top) turns are present with nominal policy (top)
and multi-residual task learning (bottom) and multi-residual task learning (bottom) and multi-residual task learning (bottom)
Fig. 3: Visualization of t-SNE plots illustrating emission benefits (higher the better) in assessing the efficacy of MRTL
policy in mitigating nominal policy limitations. t-SNE is used for dimensionality reduction of vectors describing incoming
approaches to a two dimensional space (latent dimension 1 and 2 in the figures). Thus, each data point is an incoming
approach, and the color denotes the emission benefits (a) with partial guided AV penetration (20%), (b) in the presence of
protectedleftturns,and(c)whendealingwithunprotectedleftturns.Inallcases,theMRTLpolicyoutperformsthenominal
policy, evidenced by the predominance of blue data points in the lower-row figures as compared to the upper-row figures.
• Actions: longitudinal accelerations of the ego-vehicles. A. Baselines
LanechangesaredonebySUMOandnotbythepolicy.
InordertoassessthebenefitoftheMRTLframework,we
• Rewards:Ego-vehiclerewardsarecomputedasv i(t)+
leverage three baselines to compare the performance.
w e (t), where w = −7.57 is a hyperparameter, and
1 i 1
v (t) and e (t) represent the ego-vehicle’s speed and 1) Intelligent Driver Model (IDM) [23]: human-like
i i
emissions at time t, respectively. We use increasing driving baseline. The IDM [23] is used.
v (t) as a proxy for travel time reduction. 2) Multi-task reinforcement learning: Multi-task rein-
i
forcement learning from the scratch as introduced in
We adopt a neural network initialization method inspired
Section III-B.
by Silver et al. [10]. Initially, we set the last layer of the
3) Nominal policy: policy in algorithm 1.
policy network to zero (and hence f (s,c) = 0 at the
θ
start).ThispreventstheMRTLpolicyfrombeingworsethan We do not use exhaustive training (training a different
the nominal policy, especially when the nominal policy is model on each intersection) as a baseline since it is pro-
close to optimal. We also include a 30-iteration pre-training hibitively expensive given the large number of intersections
phase for the critic to align better with the nominal policy, and, hence, practically less useful for eco-driving.
improving value estimates early on.
B. Performance and generalization
VI. EXPERIMENTALRESULTS
In Table I, we analyze emission reduction and speed
Here, we present the experimental results of employing improvements of vehicles and throughput increase at the
MRTL for eco-driving at signalized intersections. intersection at 20% and 100% AV penetration across 600signalized intersections. Our findings highlight MRTL’s ef-
fectiveness in enhancing emission reductions due to better
generalization. Our MRTL policy improves benefits even in
partial penetration scenarios when the nominal policy falls
short. Furthermore, training multi-task reinforcement learn-
ing policies from scratch is challenging at both penetration
levels. Suboptimal individual agent performances in multi-
agentsettingscanleadtotrainingcollapse,especiallyinsce-
narioswherevehiclesfollowoneanother.Thiscanbeseenin
Fig. 5: Schematic interpretation of MRTL in policy search.
thesignificantemissionsincreaseinmulti-taskreinforcement
Left: MRTL enables better policy search initialization com-
learning when comparing 20% to 100% penetration.
paredtoinitializingfromscratch.Right:Aconcreteexample
C. Nominal policy limitations from eco-driving at signalized intersections.
In Section V-A.1, we discussed limitations in our nom-
initialization. MRTL allows searching within a high-quality
inal policy design. Here, in Figure 3, we explore how the
ball around π , yielding a good solution near the optimal
n
MRTL framework effectively addresses these limitations.
policy, while π remains far from optimal under the same
0
We focus on three settings: partial penetration, intersections
computational budget. In practice, the distance between
with protected left turns, and those with unprotected left
policies and the performance landscape may be highly non-
turns. Through t-SNE [27] distribution plots as performance
convexduetothenonconvexityoftheobjectivefunctionand
profiles, we show that MRTL significantly improves over
the policy learning process. However, the nominal policy
the nominal policy performance in all three settings, with
offers a favorable warm start to MRTL by initializing the
benefits extending across majority of intersections.
search closer to the optimal policy compared to random
initialization in learning from scratch.
D. Control noise and bias noise
As an example, consider a vehicle that eco-drives when
While conventional eco-driving controllers like
encounteringaredlight.InFigure5(right),wecontextualize
GLOSA [5] struggle with noise from communication
theaboveinterpretationwithacommonlyobservedbehavior
delays and sensor issues, MRTL policies can adapt well to
in a subset of our traffic experiments by considering a
such noise. Moreover, the nominal policies can be biased
generalMDPthatcanleadtomultipleMDPsbasedontraffic
toward certain cities or conditions, but DRL can learn to
signal timing plans. We take the known strategy of gliding
adapt on top of them. To test this adaptability, we introduce
(constant deceleration throughout) as the nominal policy
control gaussian noise ϵ = N(0,σ2), varying σ2 and
c π [4]. With potential deviations like human vehicles at the
n
a bias gaussian noise ϵ b = N(µ,0.3), varying µ to AV traffic light, the optimal policy π∗ may involve piecewise-
accelerations. In Figure 4 left, MRTL policies are more
constant acceleration (glide until the leading vehicle is met,
resilient to control noise, with only a 3% performance
then constant velocity to keep a constant headway, generally
decrease compared to a significant 18% drop in the nominal
for a short time period before crossing the intersection).
policy. Similar results can also be seen under bias noise in
MRTL from the gliding policy allows the search space
Figure 4 right.
(blue region) and the best policy within the search space
(bluesolidlines)tobeclosetotheoptimalpolicy.Incontrast,
random initialization from the entire action space
(cid:81)T
a ,
t=1 t
where a ∈ [−A,A], on average results in the zero acceler-
t
ation policy, which is further away from the optimal policy
thantheglidingresidualinitialization.Moreover,randomini-
tialization usually leads to non-smooth acceleration profiles
in practice, potentially making policy search challenging,
whereas the constant deceleration from the gliding policy
for MRTL allows a smoother learning landscape.
Fig.4:Effectofcontrolnoise(left)andbiasnoise(right)on
emissions.
VII. CONCLUSION
This study examines the algorithmic generalization of
E. Why does multi-residual task learning work?
DRL in solving contextual Markov decision processes. We
Intuitively, MRTL simplifies policy search by fine-tuning present MRTL as a generic framework for achieving this
from a nominal policy that is suboptimal yet not too far goal. MRTL uses DRL to acquire residual functions, im-
from the optimal policy, while learning a multi-task policy proving upon conventional controllers. We apply MRTL to
from scratch necessitates more computational effort due cooperativeeco-driving,showingimprovedgeneralizationin
to possibly distant random initialization. This contrast is emissionreductions.Potentialfutureworkincludesanalyzing
illustrated in Figure 5 (left) with π∗ as the optimal policy, MRTL to further understand the impact of different nominal
π as the nominal policy for MRTL, and π as the random policies on generalization.
n 0REFERENCES ingincontrol. AnnualReviewofControl,Robotics,andAutonomous
Systems,3:269–296,2020.
[1] CathyWu,AbdulRahmanKreidieh,KanaadParvate,EugeneVinitsky, [15] Igor Mordatch, Nikhil Mishra, Clemens Eppner, and Pieter Abbeel.
and Alexandre M. Bayen. Flow: A modular learning framework for Combiningmodel-basedpolicysearchwithonlinemodellearningfor
mixedautonomytraffic. IEEETransactionsonRobotics,2022. controlofphysicalhumanoids.In2016IEEEinternationalconference
[2] VindulaJayawardanaandCathyWu. Learningeco-drivingstrategies onroboticsandautomation(ICRA),pages242–248.IEEE,2016.
atsignalizedintersections. InEuropeanControlConference,2022. [16] Torsten Koller, Felix Berkenkamp, Matteo Turchetta, and Andreas
[3] Nathan Lichtle´, Eugene Vinitsky, Matthew Nice, Benjamin Seibold, Krause. Learning-basedmodelpredictivecontrolforsafeexploration.
Dan Work, and Alexandre M. Bayen. Deploying traffic smoothing In 2018 IEEE Conference on Decision and Control (CDC), pages
cruisecontrollerslearnedfromtrajectorydata. In2022International 6059–6066,2018.
ConferenceonRoboticsandAutomation(ICRA),2022. [17] Marc Peter Deisenroth, Dieter Fox, and Carl Edward Rasmussen.
[4] Yuhan Huang, Elvin CY Ng, John L Zhou, Nic C Surawski, Ed- Gaussianprocessesfordata-efficientlearninginroboticsandcontrol.
ward FC Chan, and Guang Hong. Eco-driving technology for sus- IEEE transactions on pattern analysis and machine intelligence,
tainableroadtransport:Areview. RenewableandSustainableEnergy 37(2):408–423,2013.
Reviews,93:596–609,2018. [18] NicolasHeess,GregoryWayne,DavidSilver,TimothyLillicrap,Tom
[5] Konstantinos Katsaros, Ralf Kernchen, Mehrdad Dianati, and David Erez, and Yuval Tassa. Learning continuous control policies by
Rieck. Performancestudyofagreenlightoptimizedspeedadvisory stochasticvaluegradients. Advancesinneuralinformationprocessing
(glosa) application using an integrated cooperative its simulation systems,28,2015.
platform. In 2011 7th International Wireless Communications and [19] Marcel Menner, Karl Berntorp, Melanie N Zeilinger, and Stefano
MobileComputingConference,pages918–923.IEEE,2011. DiCairano. Inverselearningforhuman-adaptivemotionplanning. In
[6] SeyedAminSajadi-Alamdari,HolgerVoos,andMohamedDarouach. 2019 IEEE 58th Conference on Decision and Control (CDC), pages
Nonlinear model predictive control for ecological driver assistance 809–815.IEEE,2019.
systemsinelectricvehicles.RoboticsandAutonomousSystems,2019. [20] Glen Chou, Necmiye Ozay, and Dmitry Berenson. Learning con-
[7] Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, and John straints from locally-optimal demonstrations under cost function un-
Schulman. Quantifying generalization in reinforcement learning. In certainty. IEEERoboticsandAutomationLetters,5(2),2020.
InternationalConferenceonMachineLearning.PMLR,2019. [21] Anurag Ajay, Jiajun Wu, Nima Fazeli, Maria Bauza, Leslie P Kael-
[8] Vindula Jayawardana, Catherine Tang, Sirui Li, Dajiang Suo, and bling, Joshua B Tenenbaum, and Alberto Rodriguez. Augmenting
Cathy Wu. The impact oftask underspecification in evaluating deep physical simulators with stochastic neural networks: Case study of
reinforcement learning. Advances in Neural Information Processing planar pushing and bouncing. In 2018 IEEE/RSJ International
Systems,35:23881–23893,2022. ConferenceonIntelligentRobotsandSystems(IROS),2018.
[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep [22] Carolin Benjamins, Theresa Eimer, Frederik Schubert, Aditya Mo-
residual learning for image recognition. In Proceedings of the IEEE han,Andre´ Biedenkapp,BodoRosenhahn,FrankHutter,andMarius
conferenceoncomputervisionandpatternrecognition,2016. Lindauer. Contextualize me–the case for context in reinforcement
[10] Tom Silver, Kelsey Allen, Josh Tenenbaum, and Leslie Kaelbling. learning. TransactionsonMachineLearningResearch,2022.
Residualpolicylearning. arXivpreprintarXiv:1812.06298,2018. [23] Treiber,Hennecke,andHelbing. Congestedtrafficstatesinempirical
[11] Tobias Johannink, Shikhar Bahl, Ashvin Nair, Jianlan Luo, Avinash observationsandmicroscopicsimulations. Physicalreview.E,2000.
Kumar,MatthiasLoskyll,JuanAparicioOjea,EugenSolowjow,and [24] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and
SergeyLevine. Residualreinforcementlearningforrobotcontrol. In OlegKlimov.Proximalpolicyoptimizationalgorithms.arXivpreprint
2019 International Conference on Robotics and Automation (ICRA), arXiv:1707.06347,2017.
pages6023–6029,2019. [25] Edgar Ramirez Sanchez, Catherine Tang, Vindula Jayawardana, and
[12] Ruiqi Zhang, Jing Hou, Guang Chen, Zhijun Li, Jianxiao Chen, and Cathy Wu. Learning surrogates for diverse emission models. In
Alois Knoll. Residual policy learning facilitates efficient model-
TacklingClimateChangewithMachineLearning,NeurIPS,2022.
free autonomous racing. IEEE Robotics and Automation Letters, [26] Moves and other mobile source emissions models. Environmental
7(4):11625–11632,2022. ProtectionAgency.
[27] LaurensVanderMaatenandGeoffreyHinton. Visualizingdatausing
[13] Peng Hao, Tao Lu, Shaowei Cui, Junhang Wei, Yinghao Cai, and
t-sne. Journalofmachinelearningresearch,9(11),2008.
Shuo Wang. Meta-residual policy learning: Zero-trial robot skill
adaptation via knowledge fusion. IEEE Robotics and Automation
Letters,7(2):3656–3663,2022.
[14] Lukas Hewing, Kim P Wabersich, Marcel Menner, and Melanie N
Zeilinger.Learning-basedmodelpredictivecontrol:Towardsafelearn-