Graph Machine Learning through the Lens of Bilevel Optimization
Amber Yijia Zheng1 Tong He Yixuan Qiu
Purdue University Amazon Web Services Shanghai University of
Finance and Economics
Minjie Wang David Wipf
Amazon Web Services Amazon Web Services
Abstract
1 Introduction
Bilevel optimization refers to scenarios
whereby the optimal solution of a lower-level Graph machine learning covers a wide range of model-
energy function serves as input features to ing tasks involving graph-structured data, where cross-
an upper-level objective of interest. These instance/nodedependenciesarereflectedbyedges. Asa
optimal features typically depend on tunable classical example, label propagation (Zhou et al., 2003)
parameters of the lower-level energy in such and its many offshoots represent a semi-supervised
a way that the entire bilevel pipeline can be learning approach whereby observed node labels are it-
trained end-to-end. Although not generally erativelyspreadacrossthegraphtounlabelednodes. In
presented as such, this paper demonstrates a related vein, various forms of graph-regularized MLP
howavarietyofgraphlearningtechniquescan models (Ando and Zhang, 2006; Hu et al., 2021; Zhang
be recast as special cases of bilevel optimiza- et al., 2023) share node representations across edges to
tion or simplifications thereof. In brief, build- penalize misalignment with network structure during
ing on prior work we first derive a more flexi- training. And as a third example more narrowly fo-
bleclassofenergyfunctionsthat,whenpaired cused on certain heterogeneous graphs, non-parametric
with various descent steps (e.g., gradient de- knowledge graph embedding (KGE) models (Bordes
scent, proximal methods, momentum, etc.), et al., 2013) produce node and relation-type embed-
form graph neural network (GNN) message- dings that have been trained to differentiate factual
passing layers; critically, we also carefully un- knowledge triplets, composed of head and tail nodes
pack where any residual approximation error connected by an edge relation, from spurious ones.
lieswithrespecttotheunderlyingconstituent
More recently, graph neural networks (GNNs) have
message-passing functions. We then probe
emerged as a promising class of predictive models for
several simplifications of this framework to
handling tasks such as node classification or link pre-
derivecloseconnectionswithnon-GNN-based
diction (Kipf and Welling, 2016; Hamilton et al., 2017;
graph learning approaches, including knowl-
Xu et al., 2019; Veličković et al., 2017; Zhou et al.,
edgegraphembeddings,variousformsoflabel
2020). Central to a wide variety of GNN architectures
propagation, and efficient graph-regularized
MLP models. And finally, we present sup- are layers composed of three components: a message
porting empirical results that demonstrate
function, which bundles information for sharing with
the versatility of the proposed bilevel lens, neighbors, an aggregation function that fuses all the
which we refer to as BloomGML, referenc- messages from neighbors, and an update function that
computes the layer-wise output embedding for each
ing that BiLevel Optimization Offers More
node. Collectively, these functions enable the layer-by-
Graph Machine Learning. Our code is avail-
layer propagation of information across the graph to
able at https://github.com/amberyzheng/
facilitate downstream tasks (Kipf and Welling, 2016;
BloomGML. Let graph ML bloom.
Hamilton et al., 2017; Kearnes et al., 2016).
1ContributionduringAWSShanghaiAILabinternship.
In the past, the graph ML models described above
Proceedings of the 27thInternational Conference on Artifi- have primarily been motivated from diverse perspec-
cial Intelligence and Statistics (AISTATS) 2024, Valencia, tives, without necessarily a single, transparent lens
Spain. PMLR: Volume 238. Copyright 2024 by the au- with which to evaluate their commonalities. To make
thor(s). strides towards a more cohesive graph ML narrative,
4202
raM
7
]GL.sc[
1v36740.3042:viXraGraph Machine Learning through the Lens of Bilevel Optimization
we propose to operate from a unifying vantage point 2.1 Message Passing Graph Neural Networks
afforded by bilevel optimization (Sinha et al., 2018;
Let G ={V,R,E} denote a heterogeneous graph with
Wangetal.,2016). Thelatterreferstoanoptimization
n = |V| nodes and m = |R| edge types. Each edge
problem characterized by two interconnected levels,
e ∈ E is composed of a triplet via e = (u,r,v), with
whereby the optimal solution of a lower-level objective
nodes u,v ∈ V and relation type r ∈ R. The more
serves as input features to an upper-level loss. Within
general heterogeneous case defaults to a homogeneous
the context of graph ML, we will demonstrate how the
graphwhenm=1. Foranygivennodev,wealsodefine
lower-levelinparticularcaninduceinterpretablemodel
thesetof(1-hop)neighborsasN :={(u,r):(u,r,v)∈
architectures, while exposing underappreciated simi- v
E}. Additionally, associated with each node is a d-
larities across seemingly disparate paradigms. After
dimensional feature vector and a c-dimensional label
providing relevant background material in Section 2,
vector which stack to form X∈Rn×d and Y ∈Rn×c,
our primary contributions throughout the remainder
respectively. The latter reflects our focus on node
of the paper can be summarized as follows:
classification tasks within this section for simplicity of
exposition; the extension to link prediction (whereby
• In Section 3 we introduce a broad class of lower-
labels correspond with supervision over edges rather
level energy functions that, when combined with
than individual nodes) follows naturally.
appropriate optimization steps, give rise to effi-
cient GNN message-passing layers inheriting in- When presented with a graph G so defined, a message-
terpretable inductive biases of energy minimizers. passing GNN (or MP-GNN) of depth L produces a
Moreover, subject to certain technical assump- layer-wise sequence of node embeddings {H(l)}L ,
l=1
tions, we demonstrate that these optimization- where H(l) = {h(l)} ∈ Rn×d represents the aggre-
v v∈V
inducedlayerscanreplicatearbitrarymessageand
gatedsetofembeddingsh(l)atthel-thlayer. Moreover,
aggregation functions, while providing a flexible v
for each l, H(l) is computed from H(l−1) using a com-
approximation for the update function. Hence we
position of three functions characteristic of MP-GNN
conditionally isolate any appreciable approxima-
architectures(Gilmeretal.,2017;Hamilton,2020). We
tion error to the latter, and in doing so, articulate
formalize this composition via the following definition:
a transparent design space for GNN architectures
produced by bilevel optimization.
Definition 1 An MP-GNN layer is formed via the
• Based on the above, in Section 4 we propose a composition of three functions:
broadly-applicable bilevel optimization framework
1. A message function f that computes µ(l) :=
called BloomGML, and establish that several no- M (u,r,v)
table special cases effectively reproduce and unify f (h(l−1),r,h(l−1)) ∈ Rd for any edge e =
M u v
non-GNN-based graph learning approaches. In (u,r,v) ∈ E, i.e., given the head and tail nodes
particular,wedemonstrateanequivalencebetween and the relation type, f produces an embedding
M
traditional KGE learning and the training of an for the corresponding edge;
implicitGNN,withparametersgivenbyrelational
embeddings and a graph formed from positive and 2. A permutation-invariant aggregation function f A
negative knowledge triplets. This association ex- that computes a(l) := f ({µ(l) : (u,r) ∈
v A (u,r,v)
plains recent empirical findings in the literature
N }) ∈ Rd for all v ∈ V, meaning for each node
while motivating new interpretable approaches to v
the set function f aggregates all messages associ-
modeling over knowledge graphs. A
ated with other nodes sharing an edge;
• Finally, in Section 5 we present experiments that
3. An update function f that computes the embed-
highlight the flexibility and explanatory power U
of candidate models motivated by our proposed dingsof the nextlayer as h( vl) =f U(h( vl−1),a( vl),x v)
bileveloptimizationframeworkandattendantanal- for all v ∈V, i.e., the embeddings from the previ-
ysis thereof. ous layer are combined with the aggregated mes-
sages and (optionally) the original input node fea-
2 Background tures x v ∈X.
In this section, we first introduce GNN architectures The permutation-invariance of f notwithstanding, all
A
and their constituent message-passing layers as applied three components defining the MP-GNN layer can in
to node classification tasks (more general tasks will be principle be any parameterized differentiable (almost
addressedlater). Wethendescribeanexisting,popular everywhere) functions suitable for learning with SGD,
special case induced by bilevel optimization, followed etc. Hence different MP-GNN architectures are more-
by a discussion of both advantages and limitations. or-less tantamount to different selections for f , f ,
M AZheng, He, Qiu, Wang, Wipf
and f . We adopt W to denote the bundled set of all this trajectory for each layer l, it is straightforward to
U
trainable parameters within {f ,f ,f }. show (see Appendix B) that the resultant updates for
M A U
each node v adhere to the stipulations of Definition 1,
Conditioned on executing L layers per the schema
and can be equivalently computed as
of Definition 1 culminating in the embeddings H(L),
model training proceeds by minimizing the loss
(cid:88) (cid:104) (cid:16) (cid:17) (cid:105) µ(l) =h(l−1)−h(l−1), a(l) = (cid:88) µ(l) , and
ℓ (W,Θ):= D g h(L)(W);Θ ,y , (1) (u,r,v) u v v (u,r,v)
up v v
u∈Nv
v∈V′
h(l) =(1−γ)h(l−1)+γλa(l)+γπ(x ;W), (4)
v v v v
where h(L)(W)≡h(L) reflects the explicit dependency
v v
of H(L) on W. In this expression, the output layer
g : Rd → Rc is a differentiable node-wise function where γ is the gradient step-size parameter. From
parameterized by Θ, V′ ⊂ V indicates the subset of this expression, we observe that the effective message-
training instances with observable labels, and D is a passing layer involves weighted skip connections from
discriminator function, e.g., cross-entropy for classifica- the previous layer and input model, along with a
tion. Finally, the “up” in ℓ reflects the fact that this permutation-invariant summation of the embeddings
up
function will later serve as an upper-level loss when from neighboring nodes. And provided π is differen-
embedded within a bilevel optimization setting to be tiable w.r.t. W, the update for h(l) computed via (4)
v
described next. is differentiable as well. Hence we can substitute into
(1) and train the entire system end-to-end using SGD
2.2 Graph-Centric Bilevel Optimization over W and Θ.
Our generic MP-GNN setting thus far narrows to the
Using various preconditioners and reparameterizations,
realm of bilevel optimization (Sinha et al., 2018) when
it has been widely noted that model layers constructed
we infuse the latent embeddings {H(l)}L from above
l=1 via (4), or variations thereof, directly correspond with
with additional layer-wise structure determined by a
various popular MP-GNN architectures (Chen et al.,
second, lower-level loss ℓ ( · ;W,G), which likewise
low 2021;Maetal.,2021;Panetal.,2020;Xueetal.,2023;
depends on W and G. Specifically, suppose that for all
Yang et al., 2021; Zhang et al., 2020; Zhu et al., 2021b;
layers l, we enforce that
Zhou et al., 2021; Di Giovanni et al., 2023). Hence
(cid:16) (cid:17) (cid:16) (cid:17) this particular instance of bilevel optimization can be
ℓ H(l);W,G ≤ℓ H(l−1);W,G ,
low low interpretedastrainingaGNNnodeclassificationmodel
(2)
and ideally H(L) ≈argmin ℓ (H;W,G) with layers designed to minimize (3).
low
H
forLsufficientlylarge. Inotherwords,ateachlayerthe
Why Construct GNN Architectures this Way?
composite embedding model f U◦f A◦f M that updates As one notable example, across customer-facing ap-
H(l) per Definition 1 dually serves as an algorithm
plications it is often desirable to know which factors
producing descent steps along the loss surface of ℓ low; influencetheoutputpredictionsofaGNNmodel(Ying
hence the coincident dependency of both ℓ low and f U ◦ et al., 2019). Relevant to providing such explanatory
f A◦f M on W and G. When combined with (1), we details, the embeddings produced by bilevel optimiza-
arrive at our bilevel destination, with embeddings that
tion as in Section 2.2 contain additional information
iteratively minimize ℓ low forming predictive features when contextualized w.r.t. their role in the ℓ . For
for end-to-end training within ℓ up. instance, if some node embedding h(L) withil now (3) is
v
very far from π(x ;W) relative to other nodes, it in-
A Representative Special Case. Motivated by creasesourconfidev ncethatsubsequentpredictionsmay
Zhou et al. (2003) in the context of homogeneous
be based on network effects rather than an irrelevant
graphs, we consider the energy ℓ low(H;W,G):= local feature x . We will empirically explore this pos-
v
sibility in Section 5. Alternatively, in the context of
(cid:88) 1∥h −π(x ;W)∥2+ λ (cid:88) ∥h −h ∥2, (3)
2 v v 2 2 u v 2 heterophilic graphs, we can examine the distribution
v∈V (u,v)∈E of ∥h −h ∥ across nodes sharing an edge to loosely
v u 2
where π is a parameterized input model (e.g., an MLP calibrate the degree of heterophily, and possibly coun-
with weights W), and λ>0 is a trade-off parameter. teractitsimpactviaappropriatemodifications. Andas
This factor balances local consistency w.r.t. the input a final dimension of motivation, the bilevel perspective
model π and global smoothness across the graph. provides natural entry points for the scalable training
of models with arbitrary depth (Xue et al., 2023) or
Next, starting from some H(0), we may iterate L gradi- interpretable integration with offline sampling (Jiang
ent descent steps along (3) to obtain H(L). And along
et al., 2023).Graph Machine Learning through the Lens of Bilevel Optimization
2.3 Unresolved Limitations 3.1 Canonical Form of MP-GNN Layers
Despite the widespread use of bilevel optimization in
Given the composite function f ◦f ◦f , there is
forming and/or analyzing GNN layers, prior work is U A M
no unique decomposition into its constituent parts,
largely confined to the exploration of narrow instances
even within the constraints of Definition 1. To this
that follow from quite specific (usually quadratic)
end, we introduce the following simplification of MP-
choices for ℓ paired with vanilla gradient descent,
low GNN layers that, as we will later show, leads to a
with application to homogeneous graphs; the example
unique decomposition (up to inconsequential linear
presented in Section 2.2 follows this paradigm. As of
transformations) while maintaining expressiveness:
yet, there has been no clear delineation of a broad
design space of possibilities. We take initial steps in
Definition 2 An MP-GNN layer in canonical form is
this direction next.
defined as the composition of three functions:
3 Towards More Flexible GNNs from
Bilevel Optimization
1. A message function f(cid:101)M that computes µ ((l u)
,r,v)
:=
f(cid:101)M(h( ul−1),r,h( vl−1)) ∈ Rd(cid:101) for any edge e =
In the previous section, we applied a specific iterative (u,r,v)∈E;
algorithm,gradientdescentwithstep-sizeγ,toapartic-
u rela sur lte in ne grg dy esf cu en nc tti uo pn d, aℓ tl eow s( pH ro; dW uc, eG d) tf hr eom GN(3 N), man esd sat gh ee
-
2. The aggregation function f(cid:101)A that computes a( vl) :=
passing layers given by (4). Within this relatively f(cid:101)A({µ( (l u)
,r,v)
:(u,r)∈N v})=(cid:80) (u,r)∈Nvµ( (l u) ,r,v);
narrow design space, the only algorithmic flexibility
is in the choice of γ, and on the energy side, we are 3. An update function f(cid:101)U that computes the embed-
l ii nm pi ut ted mt oo det lun πi .ng Noth te sut rr pad rie s- io nff glypa tr ha em n,et (e 4r )λ isa fn ad
r
lt eh se
s
dingsof the nextlayer as h( vl) =f(cid:101)U(h( vl−1),a( vl),x v)
for all v ∈V.
flexible/expressive relative to the general form from
Definition 1. We now seek to reduce this gap via the
We remark that there still remains a degree of non-
following formulation.
uniqueness in Definition 2 in that a linear transforma-
T Lo (Gb )eg bi en awe funre cq tu ioi nre ss po am cee oa fdd init ti eo rn ea stl n do et pa et nio dn e. ntL oe nt tion of µ( (l u)
,r,v)
introduced by f(cid:101)M could pass through
graph G, where each function ℓ ∈L(G) is specified
f(cid:101)A and be absorbed into f(cid:101)U without changing the com-
as ℓ : H → R over some
nodlo ew
embedding domain
positefunctionf(cid:101)U◦f(cid:101)A◦f(cid:101)M. However,thisambiguityis
low inconsequential for our purposes, and nonlinear trans-
H ⊆ Rn×d. We then define A : L(G)×H → H as an
arbitrary iterative mapping/algorithm that satisfies
formationscannotanalogouslypassthroughf(cid:101)Awithout
changing the form of the resulting composite function.
ℓ ( A[ℓ ,H] ) ≤ ℓ (H) ∀ℓ ∈L(G), ∀H∈H. Additionally, although it may superficially appear as
low low low low
(5) though we have lost expressive power in moving to the
This expression merely entails that if we apply A to canonical form, this is actually not the case:
ℓ initialized at H, the resulting iteration produces
low
embeddings that reduce (or leave unchanged) ℓ low. We Proposition 1 For any f U◦f A◦f
M
adhering to Def-
are then poised to ask a central motivating question: inition 1, there exists a canonical form f(cid:101)U ◦f(cid:101)A◦f(cid:101)M
following Definition 2 that provides an arbitrarily close
Can we find a sufficiently flexible pairing of some A
approximation.
and compatible ℓ ∈L(G) such that A[ℓ ,H] closely
low low
approximates, to the extent possible, any composite
Appendix D contains the proof. So indeed, we can
message-passing layer f ◦f ◦f which follows from
U A M work with the canonical form without any appreciable
the stipulations of Definition 1?
loss of generality. From here, we will first narrow our
To make initial progress in answering this question, we scope in Section 3.2 to reproducing f
M
or f(cid:101)M using
first convert Definition 1 to what we will refer to as some A[ℓ , · ]; later in Section 3.3 we zoom out and
low
a canonical form designed to minimize, without loss address approximations to the full canonical form.
of generality, the non-uniqueness of the decomposition
of f ◦f ◦f into respective message, aggregation,
U A M 3.2 A Priori Constraint Approximating
and update functions. In so doing we obtain a more
transparent entry point for isolating precisely where
Message Functions f
M
or f(cid:101)M
more-or-less exact alignment between A[ℓ,H] and f U ◦ To begin, we introduce the following definition:
f ◦f is possible, and where non-trivial gaps remain.
A MZheng, He, Qiu, Wang, Wipf
Definition 3 We say that a message function f sat- Motivated by these considerations, we propose the
M
isfies a gradient representation criteria if family of energies given by
f (h ,r,h ) =
∂ζ(h u,h v;r) ℓ low(H;W,G):= (8)
M u v ∂h v (cid:88) f(h u,h v;r)+(cid:88) [κ(h v;x v)+η(h v)],
∂ζ(h ,h ;r)
f (h ,r−1,h ) = u v (6) (u,r,v)∈E v∈V
M v u ∂h
u
where f :Rd×Rd →R and κ:Rd →R are arbitrary
for some function ζ :Rd×Rd →R with Lipschitz con- differentiable functions, while η :Rd →R has no such
tinuous gradients, where r−1 is the inverse relation of requirements, e.g., it can be nonsmooth and discontin-
r. Moreover, we extend this definition to the canonical uous. Additionally, W refers to all parameters that
form f(cid:101)M via may be included within f, κ, and η.
(cid:12) The role of η is to allow for the introduction of node-
f(cid:101)M(h u,r,h v) =
∂ζ(cid:101)(Φ⊤ ∂h zu,z;r)(cid:12)
(cid:12)
(cid:12)
wise constraints. For instance, if η(h) := I ∞[h <
(cid:12) 0], meaning a discontinuous indicator function with
z=Φ⊤hv
(cid:12) infinite weight applied element-wise to the entries of
f(cid:101)M(h v,r−1,h u) =
∂ζ(cid:101)(z,Φ ∂⊤ zh v;r)(cid:12)
(cid:12)
(cid:12)
(7) h less than zero, then we are effectively enforcing the
(cid:12) constraintthatnodeembeddingsmust benon-negative
z=Φ⊤hu
(Yang et al., 2021). To algorithmically handle this
for some Φ∈Rd×d(cid:101)and function ζ(cid:101):Rd(cid:101)×Rd(cid:101)→R with possibility, we also require the notion of a proximal
Lipschitz continuous gradients. operator (Parikh and Boyd, 2014), which is defined as
(cid:18) (cid:19)
This definition describes message functions that can be 1
prox (h):=argmin η(z)+ ∥z−h∥2 . (9)
expressed in terms of the gradient of an energy term, η,γ z 2γ
i.e., ζ or ζ(cid:101). Moreover, if a pair of bidirectional mes-
sage functions do not satisfy this criteria, then they Proximal operators prox η,γ : Rd → Rd of this form
cannot generally be obtained with a gradient-based are useful for deriving descent algorithms involving
A applied to any possible ℓ . To see this, consider constraints or nonsmooth terms such as η. We are
low
a graph with just two nodes and no node features nowpreparedtopresentourmainresultofthissection,
for simplicity. Moreover, assume that the aggregation with discussion and interpretations to follow:
function is identity (there is only one message in ei-
ther direction) and the update function is given by Proposition 2 For any canonical message-passing
f (h(l),µ(l) ) = h(l−1) −γµ(l) for some γ > 0. layer with constituent f(cid:101)U, f(cid:101)A, and f(cid:101)M abiding by Defi-
WU ethv enob(u s, er r,v v) ethatav nypossibl( eu ℓ,r,v)
canbeexpressed
nition 2 and f(cid:101)U satisfying Definition 3, there exists an
low algorithm A and functions f, κ, and η defining ℓ
w.l.o.g. as ℓ (H;W,G)=ζ(h ,h ;r) for some func- low
low 1 2 from (8) such that
tion ζ. Therefore, the only source for producing the
t aw ndo 2re →qui 1re id
s
tm akes insa gg ge rafu dn iec nt tio sn os f, tm heea fin ri sn tg af no dr s1 ec→ ond2 A[ℓ low, · ]=fˆ
U
◦f(cid:101)A◦f(cid:101)M, (10)
arguments of ζ.
where fˆ :Rd×Rd(cid:101)×Rd →Rd is defined as
U
3.3 Approximating the Full Canonical Form fˆ (h,a,x):=prox (h−γσ(a)[Φa+κ′(h;x)])
U η,γ
via Energy Optimization (11)
with σ(a) : Rd(cid:101) → R+ > 0 as an arbitrary positive
Toproceed, wefirstrequireanadequatelyflexibleform
of energyfunction with terms thatare, in a sense tobe function, Φ∈Rd×d(cid:101)as an arbitrary matrix, and κ′ as
formally quantified, sufficiently aligned with the canon- the first-order derivative of κ. Moreover, there exists a
ical form serving as our target. Intuitively, there is a γ′ >0 such that for any γ ∈(0,γ′],
trade-off here: If this energy is too complex, then itera-
ℓ (A[ℓ ,H];G) ≤ ℓ (H;G). (12)
tive optimization algorithms applied to it can produce low low low
update rules that deviate from Definition 2, e.g., the
update for any given node v could potentially involve Several salient points are worth highlighting as we
allothernodesinthegraph,asopposedtomerelythose unpack this result:
inN . Incontrast,iftheenergyistoonarrowlydefined,
v
we will be unable to match the expressive power of the • Once we have granted that f(cid:101)U adheres to Defini-
composite f(cid:101)U ◦f(cid:101)A◦f(cid:101)M. tion 3, all approximation error between the targetGraph Machine Learning through the Lens of Bilevel Optimization
composite message passing function f(cid:101)U ◦f(cid:101)A◦f(cid:101)M 4 On Broader Graph ML Regimes
and the proposed A[ℓ , · ] is confined to mis-
low In the previous section, our focus was on deriving flexi-
match in the respective update functions f(cid:101)U and
bleMP-GNNlayersobtainedbyapplyinganalgorithm
fˆ U,notthemessageandaggregationfunctionsf(cid:101)M
A to the minimization of a lower-level energy ℓ .
and f(cid:101)A.
Through Proposition 2, we demonstrated that
thel oow
p-
• While fˆ
U
cannot exactly represent all possible erator A[ℓ low, · ] is equivalent to a MP-GNN layer
f(cid:101)U, it maintains considerable degrees of freedom with unrestricted message function f(cid:101)M, aggregation
for replicating important special cases, including function f(cid:101)A, and approximate update function fˆ
U
as
many/most commonly adopted update functions. defined by (11). Analogous to any other MP-GNN
This flexibility comes from three primary sources: model, these components can be iterated for L steps
namely, η, σ, and κ can all be freely chosen to to produce final embeddings H(L) that can be inserted
widentheexpressivenessoffˆ ;wenotethatη and into the supervised node classification loss as in (1) to
U
κ are determined by ℓ , while σ is associated complete an end-to-end bilevel optimization pipeline.
low
with A (see Appendix E for further details).
We now expand our scope to encompass other inter-
• The function η, through its influence on prox , related graph ML domains, building on the general
can introduce a wide variety of nonlinear aη c, tγ i- optimization-based message-passing layers derived in
vations into the message passing layer. And in Section 3.3. Because our motivation stems from the
general, since η is arbitrary, prox can be any principlethatBiLevel Optimization Offers More Graph
arbitraryproximaloperator. Forexaη m,γ ple,prox Machine Learning, in the sequel we refer to our frame-
can implement any non-decreasing function of tη h,γ e work as BloomGML.
elementsofh(GribonvalandNikolova,2020),such
4.1 The General BloomGML Framework
as popular ReLU activations among other things.
WeformalizeBloomGMLviatheoptimizationproblem
• The function σ allows us to introduce softmax
self-attention in the message-passing layer. This (cid:16) (cid:17)
capabilityisexactlyanalogoustohowanoptimiza- minℓ up H(L)(W),Y′;Θ,G (13)
W,Θ
tion perspective can introduce self-attention into (cid:104) (cid:105)
typical Transformer layers (Yang et al., 2022a). s.t. H(l) =A ℓ low( · ;W,G),H(l−1) ,∀l=1,...,L,
• Meanwhile, the function κ provides for arbitrary where A and ℓ follow from Proposition 2 while ℓ
low up
interactions between node embeddings and input is a generic upper-level loss that can be specified on
node features. It also allows us to directly general- an application-specific basis, e.g., (1) is a special case
ize (4). More concretely, when we choose η(h)=0 of this form. Additionally, as before we assume that
(suchthatprox degeneratestoanidentitymap- rowsofY′ arelabels; however,formaximumgenerality
η,γ
ping), σ(a)=1, and κ′(h;x)=h−π(x;W), we we need not assume that these labels correspond with
exactly recover the message passing layer from (4). nodes. In this way, we can extend BloomGML to
generic GNN tasks besides node classification. More
• Increasingtheexpressivenessoffˆ
U
tomoretightly
importantlythough,wecanalsoformdeepconnections
match an arbitrary f(cid:101)U is challenging and possibly with non-GNN regimes equally as well. In this regard,
infeasible(atleastusingstandarddescentmethods
weconsiderthreesuchexamplesnext: knowledgegraph
for A). See Appendix C for further discussion.
embeddings(KGEs),labelpropagation(LP),andwhat
are often called graph-MLP models.
WhiletheproofofProposition2isdeferredtoAppendix
E, it is based on assigning A to be a form of proximal
4.2 Knowledge Graph Embedding Models
gradient descent with preconditioning. Although this
class of algorithm has attractive convergence guaran- Basics. Knowledge graphs are heterogeneous graphs,
tees, there remains potential to improve the conver- typically without node features or labels, where each
gence rate. Within the present context, this is tan- triplet/edgee=(u,r,v)containsrelationalinformation
tamount to reducing the actual number of message- between two entities, e.g., u = ‘Paris’, v = ‘France’,
passing layers needed to approach a minimum of ℓ low. and r = ‘capital of’ (Ji et al., 2021). The goal of
We conclude by noting that it is possible to retain knowledge graph completion (KGC) is to predict unob-
the expressiveness of (11) and the efficient message served/missing factual triplets given an existing knowl-
passingstructureofDefinition2whileacceleratingcon- edge graph of interest. A widely adopted approach to
vergence through the use of momentum (Polyak, 1964) this fundamental problem is based on learning knowl-
and related (Kingma and Ba, 2014); see Appendix F. edge graph embeddings (KGEs) as follows. First, allZheng, He, Qiu, Wang, Wipf
existingtripletsE aretreatedaspositivesamples,while learning, we choose Θ=W=E, Y′ ={∅} and define
a complementary set of negative samples are obtained
(cid:16) (cid:17) (cid:16) (cid:17)
by randomly choosing triplets e− =(u−,r−,v−)∈/ E, ℓ H(L)(W),Y′;Θ,G¯ ≡ ℓ H(L)(E),{∅};E,G¯
up up
withu−,v− ∈V andr− ∈R; wecanassemblethesein
(cid:16) (cid:17)
a negative graph G− ={V,R,E−}, where E− denotes := ℓ H(L)(E),E , (17)
kge
the set of false triplets.
We then train a KGE model by minimizing a binary where ℓ kge follows (14).
cross-entropy loss of the form Key Implications of this Reformulation. Per
ℓ (H,E) := (cid:88) logρ[s(h ,e ,h )] (14) Proposition 2 and the downstream BloomGML lens
kge u r v
from above, it follows that if we minimize (17) over
e∈E
(cid:88)
just E, we are effectively reproducing a minimizer of
+ log(1−ρ[s(h ,e ,h )]),
u− r− v− the original KGE loss from (14), with H computed
e′∈E− by L steps of A. This implies that learning KGEs
where ρ denotes a standard sigmoid function, H ∈ equates to training an L-layer MP-GNN on graph G¯
Rn×d are node embeddings as before, and E∈Rm×d′ with parameters E. Ofparticularrelevancehere, recent
are embeddings for each relation type r ∈ R. Fi- work has been devoted to empirically showing that
nally, s:Rd×Rd′ ×Rd →R is a so-called score func- when training powerful heterogeneous GNN models
tion, which evaluates the compatibility of the triplet using(14),contrarytoconventionalwisdom,completely
embeddings, with a higher score indicating a greater removing the GNN message-passing layers does not
likelihood of being a true triplet (Zheng et al., 2020). actually harm performance. However, this observation
Additionally, (14) is frequently used as a training loss is directly explained by the analysis herein: Explicit
for traditional heterogeneous GNN models applied to external message-passing is not necessary since we are
knowledge graphs (Schlichtkrull et al., 2017). Note alreadytraininganimplicitMP-GNNwhensolving(14)
that at inference time, a candidate triplet can be eval- over unconstrained embeddings. Beyond this finding,
uated by computing the score of the corresponding BloomGML offers an additional practical benefit. In
output-layer embeddings produced by the GNN. brief, because we are free to choose L, in inductive
settings we can pick a relatively small value such that
KGEs and BloomGML. We now demonstrate E is learned to adjust accordingly. Hence when new
how the above process emerges as a special case of nodes are introduced, their embeddings can be quickly
BloomGML, and the interpretable implications of this approximated via only L steps of A, rather than full
association, in particular relative to GNN modeling. model retraining. We test this capability in Section 5.
Let G¯:=G∪G− ={V,R¯,E¯}, where E¯=E ∪E−. We
also define R¯ =R∪R− where R− denotes a dummy 4.3 Other Graph ML Regimes
set of negative relations such that, for every r ∈ R
BloomGML elucidates other graph ML scenarios as
there exists a r− ∈ R− representing that an original
well, particularly LP and graph-regularized MLP mod-
relation r is now appearing in a negative triple. It
els. While full details are deferred to Appendix G,
follows that |R¯|=2m. Also, we assume that e =e
r r− we note here that LP variants can be recast within
for all r, i.e., the relation type embeddings are shared.
BloomGML when we treat observed labels as input
For BloomGML, following (8) we define ℓ w.r.t. G¯
low features and A as proximal gradient descent. We
as
ℓ
(H;W,G¯)=(cid:88)
f(h ,h ;r¯), (15)
also provide supplementary experiments that verify
low u¯ v¯ BloomGML predictions regarding the relationship be-
e¯∈E¯ tween LP and special cases of graph-regularized MLPs.
where we choose W=E and
(cid:26)
f(h ,h ;r¯)=
logρ[s(h u,e r,h v)], e¯∈E 5 Empirical Validation and Analysis
u¯ v¯ log(1−ρ[s(h ,e ,h )]), e¯∈E−
u− r− v−
(16) As BloomGML represents a conceptual framework
while implicitly assuming κ and η are zero. By con- that has independent value drawn from its explana-
structing ℓ in this way, the lower-level minimization tory/unifying nature, not just pushing SOTA per se,
low
problemprovidesoptimalnodeembeddingsconditioned the motivations for our experiments are two-fold:
on fixed relation embeddings E. Hence if we execute a
1. Showcase the versatility and interpretability of
descentalgorithmAforLsteps(e.g.,gradientdescent),
BloomGML across diverse testing regimes;
we can obtain some H(L) ≡H(L)(W)≡H(L)(E).
To complete the full BloomGML specification per (13), 2. Demonstrate the explanatory power of the bilevel
we must also select ℓ . In order to align with KGE optimization lens applied to graph ML tasks.
upGraph Machine Learning through the Lens of Bilevel Optimization
Table 1: Performance mitigating spurious input features.
Here the detect ratio is obtained by computing ∥h −
v
π(x ;W)∥ for all v ∈ V and then segmenting out the
v 2
percentage of corrupted nodes within the largest 20%.
Cora Citeseer Pubmed Arxiv
GCN 51.40 46.40 59.00 64.03
Accuracy
Basefrom(3) 54.97 38.98 45.67 47.63
BloomGML 65.83 49.33 72.70 69.11
DetectRatio BloomGML 94.27 87.82 96.98 100.00
Despite these quite general aims, with limited space Figure 1: ∥h(L) −h(L)∥ density for (u,v)∈E but with
u v 2
we only include a few representative use cases and co- different labels in the Roman dataset.
incident analyses. That being said, our experiments
still span quite diverse settings, where SOTA methods Comparison on Heterophily Graphs. For graphs
are different and often rely on domain-specific architec- exhibitingheterophily(Zhuetal.,2021a),nodessharing
turesand/orheuristics. Henceourstrategyissimplyto an edge are less likely to have the same label. Hence
pick strong, domain-specific baselines for comparisons an energy function that pushes the respective node
across each task and narrative purpose, noting that embeddings to a common value may be counterproduc-
these baselines generally outperform standard/generic tive, i.e., as in the edge-dependent term from (3). In-
alternatives(onatask-by-taskbasis)perresultsinprior stead, to address heterophily graphs using BloomGML,
work. Additionally, throughout this section, we use we form ℓ via f(h ,h ;r) = ∥h C−h ∥2, where
low v u u v 2
BloomGML to describe models derived from (13), with C ∈ Rd×d is a trainable weight and r is ignored for
differentiating descriptions accompanying each experi- the homogeneous case. Additionally, to reduce sen-
ment. Also, please see Appendix H for comprehensive sitivity to less reliable node features that could po-
details of experimental setups as well as reproduction tentially accompany heterophily graphs, we consider
of result tables with error bars. κ(h;x) = δ(h−π(x;W)) to contrast with κ(h;x) =
∥h−π(x;W)∥2. Finally, we select η(h)=I [h<0]
2 ∞
Mitigating Spurious Input Features. As moti- and σ(a)=1 and apply proximal gradient descent for
vated in Section 2.2, bilevel optimization with an ap- A. Table2displaysresultsusingtherecentheterophily
propriate ℓ should in principle be able to localize benchmarks Roman, Amazon, Minesweeper, Tolokers,
low
spurious input features, allowing the model to focus on and Questions from Platonov et al. (2023). We com-
more informative network effects when needed. How- pare BloomGML against recent GNN models explicitly
ever,toourknowledge,thiscapabilityhasnotasofyet designedforhandlingheterophily,namely,FAGCN(Bo
been exploited. To this end, we compare the original etal.,2021),FSGNN(Mauryaetal.,2022),GBK-GNN
base model formed via (3) and (4), with a more robust (Du et al., 2022), and JacobiConv (Wang and Zhang,
version drawn from the general BloomGML framework 2022). For reference, we also compare with the bilevel
of (13). In particular, since quadratic regularizers can baseline formed from (3); additional GNN baselines
be sensitive to outliers, we modify (3) by choosing from Platonov et al. (2023) are discussed in Appendix
κ(h;x) = δ(h−π(x;W)) within BloomGML, where H. On average, we observe that BloomGML achieves
δ : Rd → R+ represents a robust Huber loss penalty the best accuracy here. Even so, our focus is not on
(Huber, 1992) applied to each input dimension and solvingheterophilyproblemsperse,butrather,demon-
summed. We then conduct node classification experi- strating that interpretable modifications to ℓ can
low
mentsoverdatasetsCora,Citeseer,Pubmed,andogbn- induce predictable effects.
arxiv by randomly corrupting 20% of node features
Regarding the latter, we present one additional visual-
prior to training. Results are displayed in Table 1
ization afforded by the BloomGML paradigm. On the
where multiple conclusions are evident: The Huber
Romandataset,weincorporateδ withinthefunctionf,
loss improves the accuracy across all datasets, while
which allows greater freedom for embeddings sharing
correctly identifying the majority of outliers.
anedgebutwithdifferentlabelstodeviatefromonean-
Table 2: Node classification on heterophily graphs. Follow-
other. Thedistributionof∥h( uL)−h v(L)∥
2
for(u,v)∈E
ing convention, accuracy (%) is reported for Roman and but y ̸=y (i.e., different labels) is shown in Figure
v u
Amazon, while ROC-AUC (%) is used for Minesweeper, 1. Clearly, the Huber loss allows greater deviation in
Tolokers, and Questions. embeddings sharing an edge as expected. Interestingly,
Roman Amazon Minesweep Tolokers Questions Avg. theaccuracyusingHuberwithinf increasesto88.12%.
FAGCN 65.22 44.12 88.17 77.75 77.24 70.50
FSGNN 79.92 52.74 90.08 82.76 78.86 76.87
GBK-GNN 74.57 45.98 90.85 81.01 74.47 73.38
JacobiConv 71.14 43.55 89.66 68.66 73.88 69.38 Comparison on Heterogeneous Graphs. The
Basefrom(3) 76.63 52.37 88.97 80.91 76.72 75.12
BloomGML 84.45 52.92 91.83 84.84 77.62 78.33 base model formed via (3) has also recently been ex-
BloomGML(w/Hub.) 85.26 51.00 93.30 85.92 77.93 78.68Zheng, He, Qiu, Wang, Wipf
Table 3: Node classification accuracy involving heteroge- Table 4: Inductive KGC tasks. Reported results based on
neous graphs. BloomGML is modified from HALO to in- the Test Hits@10 metric. Sum and PNA are aggregators
clude robust regularization within κ(h;x). while T and D are TransE and DistMult score functions,
respectively.
AIFB MUTAG BGS AM
HALO 97.2 80.9 89.7 85.9 WN18RR_v1 FB15k-237_v1
PNA(T) Sum(T) PNA(D) Sum(D) PNA(T) Sum(T) PNA(D) Sum(D)
BloomGML 97.2 82.4 93.1 86.9 RefactorGNN / / / 0.885 / / / 0.787
BloomGML 0.952 0.937 0.960 0.946 0.744 0.747 0.836 0.792
tended to heterogeneous graphs via the HALO model
(Ahn et al., 2022). However, this extension remains
limited in its strict adherence to the quadratic edge-
and input feature-dependent penalty terms as in the
original (3). To this end, we modify HALO within the
confines of the BloomGML framework to explore non-
quadratic penalties (Log-Cosh and Huber) while main-
taining equivalence across all other aspects of model Figure 2: ℓ values versus propagation steps on Cora.
low
design. NodeclassificationresultsareinTable3,where
we observe that all else being equal, these changes lead mimics an NBFNet model layer; see Appendix I for
to an improvement in accuracy over HALO. derivations showing how this process aligns with an
MP-GNN layer when incorporated within BloomGML
Knowledge Graph Completion. As mentioned at under the right circumstances. We also set κ(h;x)=
the end of Section 4.2, BloomGML is particularly well- η(h) = 0, σ(a) = 1 for simplicity. Inductive KGC
resultsareshowninTable4onWordNet18RR_v1and
suited for inductive KGE tasks such as KGC. To this
FB15K237_v1 datasets (Teru et al., 2020). Across dif-
end, we analytically and empirically compare against
ferent score functions TransE and DistMult, and aggre-
the recent RefactorGNN model (Chen et al., 2022),
gators Sum and PNA (Corso et al., 2020), BloomGML
which includes related analysis showing how a par-
achieves strong performance relative to RefactorGNN.
ticular score function, DistMult (Yang et al., 2014),
Note that in Table 4 we have reported the best Refac-
when combined with a softmax training loss over posi-
torGNN model from Chen et al. (2022) as there is
tive samples, has some commonalities with GNN layer
presently no publicly-available code for reproducibiliy.
structure. From a practical standpoint, RefactorGNN
involves training a KGE model while resetting node
Incorporation of Momentum. We demonstrate
embeddings to initial values in regular intervals to fa-
how the incorporation of momentum within A, as pro-
cilitate the inductive setting (the edge embeddings are
posedattheendofSection3.3anddetailedinAppendix
trained as usual). While certainly a noteworthy contri-
F, can maintain MP-GNN structure while expediting
bution, there nonetheless remain three limitations of
convergence. ResultsareshowninFigure2andTable5
RefactorGNN relative to BloomGML.
usingasimplebasemodelakinto(3),whereweobserve
First,theRefactorGNNmodeldoesnotactuallyinduce that momentum can potentially improve BloomGML
an efficient/sparse MP-GNN, as there remains a global performance by reducing ℓ more quickly.
low
hub node to which all other nodes are connected and
receive messages. Secondly, the supporting analysis Table5: Nodeclassificationwithmomentum. Theoptimiza-
provided in Chen et al. (2022) only directly addresses tion algorithm for BloomGML is SGD with momentum.
the softmax/DistMult pairing while excluding consid- Cora Citeseer Pubmed Arxiv
eration of negative samples; in contrast, BloomGML SGD 80.1 73.2 78.0 72.0
BloomGML 83.4 74.0 80.7 72.6
serves as a general framework covering a wide variety
of both KGE and non-KGE models alike, and within
whichwehaveexplicitlyaccountedfornegativesamples Additional Results. Finally, please see Appendix J
through G¯. And lastly, the repeated reinitialization of for additional experiments including an ablation over
node embeddings during RefactorGNN training can be
learning λ, the trade-off parameter from (3), as well as
further demonstration of the versatility of BloomGML.
viewedasaheuristic,withnoguaranteeofconvergence;
BloomGML sidesteps this issue altogether within an
integrated bilevel optimization framework. 6 Conclusion
To conduct empirical comparisons with RefactorGNN, InthisworkwehaveintroducedtheBloomGMLframe-
we must first specify (8) for BloomGML. We take in- work, which provides a novel lens for understanding
spiration from NBFNet (Zhu et al., 2021c) and design various graph ML paradigms and introducing inter-
ℓ such that one step of proximal gradient descent pretable, targeted enhancements. Let graph ML bloom.
lowGraph Machine Learning through the Lens of Bilevel Optimization
References Hamilton, W., Ying, Z., and Leskovec, J. (2017). In-
ductive representation learning on large graphs. Ad-
Ahn, H., Yang, Y., Gan, Q., Moon, T., and Wipf,
vances in neural information processing systems, 30.
D. (2022). Descent steps of a relation-aware energy
produce heterogeneous graph neural networks. Hamilton, W.L.(2020). Graph representation learning.
Morgan & Claypool Publishers.
Ando, R. and Zhang, T. (2006). Learning on graph
with laplacian regularization. Advances in neural Hinton,G.,Srivastava,N.,andSwersky,K.(2012).Neu-
information processing systems, 19. ralnetworksformachinelearninglecture6aoverview
of mini-batch gradient descent. Cited on, 14(8):2.
Bo,D.,Wang,X.,Shi,C.,andShen,H.(2021). Beyond
low-frequency information in graph convolutional Hu, Y., You, H., Wang, Z., Wang, Z., Zhou, E., and
networks. In Proceedings of the AAAI Conference Gao, Y. (2021). Graph-mlp: Node classification
on Artificial Intelligence, volume 35. without message passing in graph. arXiv preprint
arXiv:2106.04051.
Bordes, A., Usunier, N., Garcia-Duran, A., Weston,
J., and Yakhnenko, O. (2013). Translating embed- Huber, P. J. (1992). Robust estimation of a location
dings for modeling multi-relational data. Advances parameter. Breakthroughs in statistics: Methodology
in neural information processing systems, 26. and distribution, pages 492–518.
Chen, J., Mueller, J., Ioannidis, V. N., Adeshina, S., Ji, S., Pan, S., Cambria, E., Marttinen, P., and Philip,
Wang, Y., Goldstein, T., and Wipf, D. (2021). Does S. Y. (2021). A survey on knowledge graphs: Rep-
your graph need a confidence boost? Convergent resentation, acquisition, and applications. IEEE
boosted smoothing on graphs with tabular node fea- Transactions on Neural Networks and Learning Sys-
tures. In International Conference on Learning Rep- tems.
resentations. Jiang, H., Liu, R., Yan, X., Cai, Z., Wang, M., and
Chen, Y., Mishra, P., Franceschi, L., Minervini, P., Wipf, D. (2023). MuseGNN: Interpretable and con-
Stenetorp, P., and Riedel, S. (2022). Refactor vergent graph neural network layers at scale. arXiv
gnns: Revisiting factorisation-based models from preprint arXiv:2310.12457.
a message-passing perspective. Kearnes, S., McCloskey, K., Berndl, M., Pande, V.,
Corso, G., Cavalleri, L., Beaini, D., Liò, P., and and Riley, P. (2016). Molecular graph convolutions:
Veličković, P. (2020). Principal neighbourhood ag- moving beyond fingerprints. Journal of computer-
gregation for graph nets. In Advances in Neural aided molecular design, 30:595–608.
Information Processing Systems, volume 33. Kingma, D. P. and Ba, J. (2014). Adam: A
Di Giovanni, F., Rowbottom, J., Chamberlain, B. P., method for stochastic optimization. arXiv preprint
Markovich, T., and Bronstein, M. M. (2023). Under-
arXiv:1412.6980.
standing convolution on graphs via energies. Trans- Kipf, T. N. and Welling, M. (2016). Semi-supervised
actions on Machine Learning Research. classification with graph convolutional networks.
Du, L., Shi, X., Fu, Q., Ma, X., Liu, H., Han, S., arXiv preprint arXiv:1609.02907.
and Zhang, D. (2022). Gbk-gnn: Gated bi-kernel Luo, Y., Huang, R., Chen, A., and Zeng, X. (2021).
graph neural networks for modeling both homophily Reslpa: Label propagation with residual learning. In
and heterophily. In Proceedings of the ACM Web Proceedings of the 2021 7th International Conference
Conference 2022, pages 1550–1558. on Computing and Artificial Intelligence, ICCAI ’21.
Duchi, J., Hazan, E., and Singer, Y. (2011). Adap- Ma, Y., Liu, X., Zhao, T., Liu, Y., Tang, J., and Shah,
tive subgradient methods for online learning and N. (2021). A unified view on graph neural networks
stochasticoptimization. Journal of machine learning as graph signal denoising. In Proceedings of the 30th
research, 12(7). ACM International Conference on Information &
Eswaran, D., Günnemann, S., Faloutsos, C., Makhija, Knowledge Management, pages 1202–1211.
D., and Kumar, M. (2017). Zoobp: Belief propaga- Maurya, S. K., Liu, X., and Murata, T. (2022). Simpli-
tion for heterogeneous networks. 10(5). fying approach to node classification in graph neu-
Gilmer, J., Schoenholz, S. S., Riley, P. F., Vinyals, O., ral networks. Journal of Computational Science,
62:101695.
and Dahl, G. E. (2017). Neural message passing for
quantum chemistry. In International Conference on Pan, X., Song, S., and Huang, G. (2020). A unified
Machine Learning, pages 1263–1272. PMLR. framework for convolution-based graph neural net-
works.
Gribonval, R. and Nikolova, M. (2020). A characteriza-
tionofproximityoperators. Journal of Mathematical Parikh, N. and Boyd, S. (2014). Proximal algorithms.
Imaging and Vision, 62(6):773–789. Found. Trends Optim., page 127–239.Zheng, He, Qiu, Wang, Wipf
Platonov, O., Kuznedelev, D., Babenko, A., and inference in knowledge bases. arXiv preprint
and Prokhorenkova, L. (2022). Characterizing arXiv:1412.6575.
graph datasets for node classification: Beyond
Yang, Y., Huang, Z., andWipf, D.(2022a). Transform-
homophily-heterophily dichotomy. arXiv preprint ers from an optimization perspective. In Advances
arXiv:2209.06177.
in Neural Information Processing Systems.
Platonov,O.,Kuznedelev,D.,Diskin,M.,Babenko,A.,
Yang, Y., Liu, T., Wang, Y., Huang, Z., and Wipf, D.
and Prokhorenkova, L. (2023). A critical look at the
(2022b). Implicit vs unfolded graph neural networks.
evaluationofGNNsunderheterophily: Arewereally
arXiv preprint arXiv:2111.06592.
making progress? In The Eleventh International
Yang,Y.,Liu,T.,Wang,Y.,Zhou,J.,Gan,Q.,Wei,Z.,
Conference on Learning Representations.
Zhang, Z., Huang, Z., and Wipf, D. (2021). Graph
Polyak, B. T. (1964). Some methods of speeding up neural networks inspired by classical iterative algo-
the convergence of iteration methods. Ussr com- rithms. In International Conference on Machine
putational mathematics and mathematical physics, Learning, pages 11773–11783. PMLR.
4(5):1–17.
Ying, Z., Bourgeois, D., You, J., Zitnik, M., and
Schlichtkrull, M., Kipf, T.N., Bloem, P., vandenBerg, Leskovec, J. (2019). Gnnexplainer: Generating ex-
R., Titov, I., and Welling, M. (2017). Modeling planations for graph neural networks. Advances in
relational data with graph convolutional networks. neural information processing systems, 32.
Sinha, A., Malo, P., and Deb, K. (2018). A review on Zaheer, M., Kottur, S., Ravanbakhsh, S., Poczos, B.,
bilevel optimization: From classical to evolutionary Salakhutdinov, R.R., andSmola, A.J.(2017). Deep
approaches and applications. IEEE Transactions on sets. Advances in neural information processing sys-
Evolutionary Computation, 22(2):276–295. tems, 30.
Teru, K., Denis, E., and Hamilton, W. (2020). In- Zhang, H., Wang, S., Ioannidis, V. N., Adeshina, S.,
ductive relation prediction by subgraph reasoning. Zhang,J.,Qin,X.,Faloutsos,C.,Zheng,D.,Karypis,
In International Conference on Machine Learning, G.,andYu,P.S.(2023). Orthoreg: Improvinggraph-
pages 9448–9457. PMLR. regularized mlps via orthogonality regularization.
Veličković, P., Cucurull, G., Casanova, A., Romero, Zhang, H., Yan, T., Xie, Z., Xia, Y., and Zhang, Y.
A., Lio, P., and Bengio, Y. (2017). Graph attention (2020). Revisiting graph convolutional network on
networks. arXiv preprint arXiv:1710.10903. semi-supervisednodeclassificationfromanoptimiza-
Wang, X. and Zhang, M. (2022). How powerful are tion perspective. arXiv preprint arXiv:2009.11469.
spectral graph neural networks. In International Zheng, D., Song, X., Ma, C., Tan, Z., Ye, Z., Dong,
ConferenceonMachineLearning,pages23341–23362. J., Xiong, H., Zhang, Z., and Karypis, G. (2020).
PMLR. Dgl-ke: Training knowledge graph embeddings at
scale. In Proceedings of the 43rd International ACM
Wang, Y., Jin, J., Zhang, W., Yang, Y., Chen, J.,
SIGIR Conference on Research and Development in
Gan, Q., Yu, Y., Zhang, Z., Huang, Z., and Wipf, D.
Information Retrieval, pages 739–748.
(2022). Why propagate alone? parallel use of labels
and features on graphs. In International Conference Zhou, D., Bousquet, O., Lal, T., Weston, J., and
on Learning Representations. Schölkopf, B. (2003). Learning with local and global
consistency. Advances in neural information process-
Wang, Z., Ling, Q., and Huang, T. (2016). Learning
ing systems, 16.
deep ℓ encoders. In AAAI Conference on Artificial
0
Intelligence, volume 30. Zhou, J., Cui, G., Hu, S., Zhang, Z., Yang, C., Liu, Z.,
Wang, L., Li, C., and Sun, M. (2020). Graph neural
Xu, K., Hu, W., Leskovec, J., and Jegelka, S. (2019).
networks: A review of methods and applications. AI
How powerful are graph neural networks?
open, 1:57–81.
Xue, R., Han, H., Torkamani, M., Pei, J., and Liu, X.
Zhou,K.,Huang,X.,Zha,D.,Chen,R.,Li,L.,Choi,S.-
(2023). LazyGNN:Large-scalegraphneuralnetworks
H., and Hu, X. (2021). Dirichlet energy constrained
via lazy propagation. In International Conference
learning for deep graph neural networks. Advances
on Machine Learning.
in Neural Information Processing Systems, 34:21834–
Yamaguchi,Y.,Faloutsos,C.,andKitagawa,H.(2016). 21846.
Camlp: Confidence-aware modulated label propaga-
Zhu, J., Rossi, R. A., Rao, A., Mai, T., Lipka, N.,
tion. pages 513–521.
Ahmed,N.K.,andKoutra,D.(2021a). Graphneural
Yang, B., Yih, W.-t., He, X., Gao, J., and Deng, L. networks with heterophily. In Proceedings of the
(2014). Embeddingentitiesandrelationsforlearning AAAI conference on artificial intelligence,volume35.Graph Machine Learning through the Lens of Bilevel Optimization
Zhu, M., Wang, X., Shi, C., Ji, H., andCui, P.(2021b).
Interpreting and unifying graph neural networks
with an optimization framework. arXiv preprint
arXiv:2101.11859.
Zhu, X. (2005). Semi-supervised learning with graphs.
PhD Thesis, Carnegie Mellon University.
Zhu, Z., Zhang, Z., Xhonneux, L.-P., and Tang, J.
(2021c). Neural bellman-ford networks: A general
graph neural network framework for link prediction.
Advances in Neural Information Processing Systems,
34:29476–29490.Zheng, He, Qiu, Wang, Wipf
A Appendix Overview
Appendix content more-or-less follows the order in which it was originally referenced in the main text; we
summarize as follows:
• In Section B, we derive the the message passing special case from (4).
• In Section C, we discuss why increasing the expressiveness of BloomGML in meaningful ways is challenging.
• In Sections D and E, we provide the proofs of Propositions 1 and 2, respectively.
• In Section F, we present a general framework for accelerating convergence and related special cases.
• In Section G, we fill in details relating BloomGML to label propagation and graph-regularized MLPs. We
also provide supporting empirical results.
• In Section H, we provide model implementation and experiment details related to Section 5.
• In Section I, we explore the connection between NBFNet and BloomGML.
• In Section J, we present additional empirical results/ablations.
B Derivation of Message Passing Special Case from (4)
The message passing updates from (4) in Section 2.2 are based on the energy function from (3), which we repeat
here for convenience as
ℓ (H;W,G)= (cid:88) 1∥h −π(x ;W)∥2+ λ (cid:88) ∥h −h ∥2. (18)
low 2 v v 2 2 u v 2
v∈V (u,v)∈E
Taking the derivative of this ℓ w.r.t. h we obtain
low v
(cid:88)
∇ ℓ (H;W,G)=h −π(x ;W)−λ (h −h ). (19)
hv low v v u v
u∈Nv
Let µ(l) := h(l−1) − h(l−1) and a(l) := (cid:80) µ(l) . There will then exist an update function
(u,r,v) u v v u∈Nv (u,r,v)
f (h(l−1),a(l),x ) such that we have
U v v v
h(l) =h(l−1)−γ∇ ℓ (H;W,G)=f (h(l−1),a(l),x ):=(1−γ)h(l−1)+γλa(l)+γπ(x ;W), (20)
v v hv low U v v v v v v
which reproduces (4).
C Possibilities for Generalizing BloomGML Expressiveness
As alluded to in Section 3.3, increasing the expressiveness of BloomGML in meaningful ways is challenging. At
a trivial level, we could always generalize (8) by allowing f → f to vary across each triplet; analogously
(u,r,v)
we could allow κ → κ and η → η or related. But for more interesting enhancements, we might consider
(v) (v)
flexible expressions of permutation-invariant functions given that the input graph is invariant to the order of
nodes and triplets. For example, results from Zaheer et al. (2017) show that, granted certain stipulations on the
input domain, permutation invariant functions operating on a set X can be expressed as ρ[(cid:80) ϕ(x)] when
x∈X
granted suitable transforms ρ and ϕ. From this type of result, it is natural to consider, for example, generalizing
the first term of (8) via
 
(cid:88) (cid:88)
f(h u,h v;r)→ρ f(h u,h v;r). (21)
(u,r,v)∈E (u,r,v)∈E
The problem here though is that, if we take the gradient of this revised term w.r.t. the node embeddings, by the
chain rule the result can depend on all triplets in the graph, which violates the message-passing schema from
either Definition 1 or 2.Graph Machine Learning through the Lens of Bilevel Optimization
D Proof of Proposition 1
The basic strategy here will be to push components of the original aggregation function f into the revised
A
message and update functions f(cid:101)M and f(cid:101)U, respectively. And the underlying goal is to do so without losing any
expressiveness in the resulting composite function, even while reducing the revised aggregation function f(cid:101)A to
simple, parameter-free addition.
Toproceed,werelyonasuitabledecompositionofpermutation-invariantsetfunctionsasrequiredbyf overinput
A
messages from N , where the domain of each message is Rd. In this regard, a number of permutation-invariant
v
function decompositions have been proposed in Zaheer et al. (2017) with conditions dependent on the domain of
the input sets. The strongest result they present provides a useful general form of exact decomposition; however,
unfortunately the underpinning guarantee only holds for input sets drawn from a countable universe, which Rd is
of course not.
To work around this limitation, we first assume that |N |=p<n for all v ∈V, where p>0 is a fixed constant.
v
We may then apply Theorem 9 from Zaheer et al. (2017) which stipulates that any continuous f defined on a
A
compact set within Rd can be approximated arbitrarily closely via a decomposition of the form
 
f A(cid:16) {µ( (l u)
,r,v)
:(u,r)∈N v}(cid:17) ≈ρ (cid:88) ϕ(µ( (l u) ,r,v)), (22)
(u,r)∈Nv
where ρ:Rd˜ →Rd and ϕ:Rd →Rd˜are suitable transforms.
To extend to the general case where |N | may vary across nodes (since graphs are generally not limited to a
v
fixed node degree), we add max |N |−|N | dummy neighbors (u′,r′) to N for every v ∈V, and form the
v∈V v v v
augmented neighbor sets N′, with the restriction that ϕ(µ ) = 0. Note that in the neighborhood of the
v u′,r′,v
associated dummy messages µ , we can no longer guarantee a close approximation to the true function;
(u′,r′,v)
however, wecanclustersuchdummymessagesinanareaofarbitrarilysmallmeasure, sotheirimpactisnegligible.
Given N′ so defined, f can be approximated arbitrarily closely (except for within a negligible, vanishingly small
v A
area of the input domain) via a decomposition of the form
   
f A({µ( (l u)
,r,v)
:(u,r)∈N v})≈ρ (cid:88) ϕ(µ( (l u) ,r,v))≡ρ (cid:88) ϕ(µ( (l u) ,r,v)). (23)
(u,r)∈N v′ (u,r)∈Nv
Now define f(cid:101)M := ϕ ◦ f M, µ (cid:101)( (l u)
,r,v)
:= f(cid:101)M(h( ul−1),r,h( vl−1)) ∈ Rd(cid:101), and f(cid:101)A({µ (cid:101)( (l u)
,r,v)
: (u,r) ∈ N v}) =
(cid:80) µ(l) . Then we have
(u,r)∈Nv (cid:101)(u,r,v)
ρ◦f(cid:101)A◦f(cid:101)M ≈f A◦f
M
(24)
Furthermore, if we define f(cid:101)U :=f
U
◦ρ, we may conclude that
f(cid:101)U ◦f(cid:101)A◦f(cid:101)M ≈ f
U
◦f A◦f
M
(25)
in the sense described above.
E Proof of Proposition 2
We assume that f(cid:101)M satisfies the gradient representation criteria from Definition 3. Now recall that the energy
function ℓ is given by
low
(cid:88) (cid:88)
ℓ (H;W,G):= f(h ,h ;r)+ [κ(h ;x )+η(h )]. (26)
low u v v v v
(u,r,v)∈E v∈V
First, we choose
f(h u,h v;r) := ζ(cid:101)(Φ⊤h u,Φ⊤h v;r), (27)Zheng, He, Qiu, Wang, Wipf
where Φ and ζ(cid:101)instantiate the gradient representation criteria for f(cid:101)M. From this it follows that
∂f(h ,h ;r)
∂u
h
v = Φf(cid:101)M(h u,r,h v)
v
∂f(h ,h ;r)
∂u
h
v = Φf(cid:101)M(h v,r−1,h u). (28)
u
We may then choose µ( (l u)
,r,v)
:=f(cid:101)M(h( ul−1),r,h( vl−1))∈Rd(cid:101), and by adding up all gradient terms depending on a
given node v, we can produce the aggregation function a( vl) :=f(cid:101)A({µ( (l u)
,r,v)
:(u,r)∈N v})=(cid:80) (u,r)∈Nvµ( (l u) ,r,v),
noting that in certain cases r =r¯−1 for some other relation r¯∈R. And by linearity of the aggregation, we can
later push Φ through to the update function (see below).
In this way, the gradient of ℓ w.r.t. h(l−1), excluding the (possibly) non-differentiable η term is given by
low v
Φa( vl)+κ′(h( vl−1);x v). Given the Lipschitz continuous gradients of ζ(cid:101), it follows that for a suitably small γ′ >0,
there will always exist an upper bound of the form
(cid:88) (cid:20)(cid:13) (cid:16) (cid:104) (cid:105)(cid:17)(cid:13)2 (cid:21)
ℓ (H;W,G)≤ 1 (cid:13)h − h(l−1)−γσ(a(l)) Φa(l)+κ′(h(l−1);x ) (cid:13) +η(h ) +C (29)
low 2γσ(a( vl)) (cid:13) v v v v v v (cid:13)
2
v
v∈V
for all γ ∈(0,γ′], where C is an irrelevant constant, σ is a preconditioner function, and we achieve equality iff
H=H(l−1). From this bound, we can construct an update function by taking a proximal step that is guaranteed
to reduce or leave unchanged ℓ . The form is given by
low
(cid:16) (cid:104) (cid:105)(cid:17)
fˆ(h(l−1),a(l),x ):=prox h(l−1)−γσ(a(l)) Φa(l)+κ′(h(l−1);x ) , (30)
v v v η,γσ(a( vl)) v v v v v
completing the proof.
F Accelerating BloomGML Convergence with Momentum/Hidden States
At the end of Section 3.3, we briefly alluded to retaining the expressiveness of (11) and the efficient message
passing structure of Definition 2 while accelerating convergence, meaning a smaller value of L will be adequate for
approximating a minimum of ℓ . We fill in further details here. At a high level, this goal can be accomplished
low
through the use of additional hidden states S={s } ∈Rn×d that instantiate momentum (Polyak, 1964). The
v v∈V
latter can mute the amplitude of oscillations in noisy gradients while traversing flatter regions of the loss surface
more quickly. A revised version of (11) festooned with momentum is given by
(cid:104) (cid:105)
s(l) =βs(l−1)+(1−β)σ(a(l)) Φa(l)+κ′(h(l−1);x)
(cid:16) (cid:17)
h(l) =prox h(l−1)−γs(l) , (31)
η,γ
where β is an additional trade-off parameter.
Beyondthevanillaversionofmomentumgivenby(31),forreferencewenowdefineamoregeneralformulationthat
encompasses many well-known acceleration methods including AdaGrad (Duchi et al., 2011), RMSProp (Hinton
et al., 2012), and Adam (Kingma and Ba, 2014) as special cases.
To begin, let ℓ be any differentiable energy function expressible as (8) with η =0 (we omit handling the more
low
general case for brevity, although it can be derived in a similar way) and g(l) := Φa(l) +κ′(h(l−1);x ) ∈ Rd.
v v v v
Given auxiliary variables S={s } ∈Rn×d and a coefficient β >0, the iterative mapping2
v v∈V
s(l) =βs(l−1)+(1−β)ξ(g(l−1))
(32)
h(l) =h(l−1)−γσ(s(l))φ(g(l−1))
provides a general framework for accelerating convergence. Here ξ : Rd → Rd, and φ : Rd → Rd are linear
node-wise functions defined on g. σ(·):Rd →R+ >0 is an arbitrary positive function. Note that when k >1
2We omit a subscript v in the following context for simplicity.Graph Machine Learning through the Lens of Bilevel Optimization
auxiliary variables are involved, φ is extended to Rkd →Rd. Such an iterative mapping preserves the properties
of message-passing functions. To clarify this, let f (h,a,s,x) denote the mapping defined in (32). For any
U′
v ∈ V, we have h(l) = f (h(l−1),a(l),s(l−1),x ), which means one update step only takes information from
v U′ v v v v
{v∪N }. Also, a =(cid:80) µ is a permutation-invariant function by construction, which means f is
also
pev
rmutation
v invaria( nu t,r o)∈ vN ervth( eu, sr e,v t)
s of 1-hop neighbors.
U′
Turning now to the remainder of this section, we verify that the vanilla momentum from above, as well as
AdaGrad, RMSProp, and Adam are all special cases of (32).
Momentum as Special Case. To show that vanilla momentum is a message-passing layer formed as a special
case of (32), we let ξ(g)=g, σ(s)=s, and φ=1, then the mapping is given by
s(l) =βs(l−1)+(1−β)g(l−1),
(33)
h(l) =h(l−1)−γs(l).
which reproduces (31).
AdaGrad. HereweshowAdaGrad(Duchietal.,2011)isalsoaspecialcaseof(32). Letβ =0,ξ(g)=diag(ggT),
φ(g)=g, and σ(s)=(cid:0) s(l)+ϵ(cid:1)− 21 where ϵ is a small constant to avoid division by zero. Then the updating rule
of AdaGrad is given by
s(l) =diag(g(l−1)(g(l−1))T),
(34)
(cid:16) (cid:17)−1
h(l) =h(l−1)−γ s(l)+ϵ 2 ⊙g(l−1).
Here the square root, inverse, and ⊙ are all element-wise operations.
RMSprop. RMSprop (Hinton et al., 2012) divides the gradient by a running average of its recent magnitude.
To show that it is also a special case of (32), let ξ(g)=g2, φ(g)=g, and σ(s)=(cid:0) s(l)+ϵ(cid:1)−1 2. Then the updating
rule of RMSprop is
s(l) =βs(l−1)+(1−β)(g(l−1))2,
(35)
(cid:16) (cid:17)−1
h(l) =h(l−1)− s(l)+ϵ 2 ⊙g(l−1).
Adam. Adam (Kingma and Ba, 2014) requires two auxiliary variables s and s . Let ξ (g)=g, ξ (g)=g2,
1 2 1 2
(cid:32)(cid:114) (cid:33)−1
φ(g)=1, and σ(s(l),s(l))= s( 1l) +ϵ · s 1(l) . The updating rule of Adam is
1 2 1−βl 1−βl
1 2
s(l) =β s(l−1)−(1−β )g(l−1),
1 1 1 1
s(l) =β s(l−1)+(1−β )(g(l−1))2,
2 2 2 2
(36)
(cid:115) −1
s(l) s(l)
h(l) =h(l−1)−γ 1 +ϵ · 1 .
1−βl 1−βl
1 2
G More on Label Propagation and Graph-Regularized MLPs
In this section we pick up where we left off in Section 4.3, filling in details at the nexus of label propagation,
graph-regularized MLPs, and BloomGML.
G.1 Label Propagation as a BloomGML Special Case
Consider the lower-level energy given by
ℓ low(H;G):=λtr(cid:0) H⊤LH(cid:1) +(cid:13) (cid:13)Y¯ −H(cid:13) (cid:13)2
F
+ (cid:88) I ∞[h
v
̸=y¯ v], (37)
v∈V′Zheng, He, Qiu, Wang, Wipf
where L is the graph Laplacian of G and Y¯ ∈ Rn×d is constructed with y¯ = y if v ∈ V′ and ∥y¯ ∥ = 0
v v v
otherwise. This form is a special case of BloomGML when we assume m = 1, f(h ,h ) = λ∥h −h ∥2 with
u v u v 2
λ > 0 a hyperparameter, and X = Y¯, κ(h ;x ) = ∥h −x ∥2 ≡ ∥h −y¯ ∥2. Furthermore, we generalize
v v v v 2 v v 2
η(h )→η(h ;x )≡η(h ;y¯ ) and choose
v v v v v
η(h ;y¯ ) = I[∥y¯ ∥≠ 0]I [h ̸=y¯′], (38)
v v v ∞ v v
where we are implicitly assuming that ∥y¯ ∥≠ 0 for any v ∈V′. The first two terms of (37) are smooth, while the
v
last enforces that embeddings for nodes with observable labels must equal those labels. We may minimize using
proximal gradient steps of the form
(cid:104) (cid:16) (cid:17)(cid:105)
H(l) = prox (1−γ)H(l−1)−γ λLH(l−1)−Y¯
η,γ
(cid:104) (cid:16) (cid:17)(cid:105)
= P Y¯ (1−γ)H(l−1)−γ λLH(l−1)−Y¯ , (39)
where P Y¯ :Rn×c →Rn×c is a projection operator that leaves rows v ∈/ V′ unchanged, while setting rows v ∈V′
as h →y¯ =y . Several points are worth noting regarding this result:
v v v
• For different choices of γ and λ, as well as the possible inclusion of a gradient preconditioner, the update
from (39) can exactly reproduce various forms of label propagation (LP) (Zhou et al., 2003; Zhu, 2005),
which represents a family of semi-supervised learning algorithms designed to predict unlabeled nodes by
propagating observed labels across edges of the graph.
• As a representative LP example, we include a gradient preconditioner D−1 within (39), where D is the
diagonaldegreematrixofG,andchooseλ→∞,γ →0suchthatλγ →1. Then(39)reducestothestandard,
interpretable form
(cid:104) (cid:105)
H(l) =P Y¯ D−1AH(l−1) , (40)
where A is the adjacency matrix of G.
• The input argument to P Y¯[ · ] in (39) exactly reduces to (4) across all v ∈V if we replace the π used in (4)
with the corresponding labels y¯ ; this follows from the derivations presented in Section B. Moreover, the
v
node-wise projection operator P Y¯ merely introduces a nonlinear activation function which can be merged
into an update function fˆ . In this way, executing the LP model propagation steps induces node embeddings
U
analogous to those from a message-passing GNN architecture.
• If we include the η-based indicator term in (37), then w.l.o.g. we can actually remove the κ-based term and
still reproduce some notable LP variants. However, we include the general form for two reasons: (i) It will
facilitate more direct comparisons with graph-regularized MLP models below, and (ii) If we instead remove
the η-based indicator term, then we can nonetheless recoup other flavors of LP without a projection step by
retaining the κ-based term.
• There exist more exotic LP models that are not directly covered by (37), such as ZooBP (Eswaran et al.,
2017) and CAMLP (Yamaguchi et al., 2016), which include compatibility matrices to address graphs beyond
vanilla homogeneous structure. These can be accommodated by generalizing (37); however, we do not pursue
this course further here.
We close by noting that, LP methods traditionally do not have parameters W to train, and the motivation
for including an upper-level loss ℓ analogous to BloomGML may be unclear. However, recently trainable
up
variants have been proposed (Wang et al., 2022) that incorporate randomized sampling to mitigate label leakage.
BloomGML can cover many such cases by supplementing (37) with parameters that can be subsequently trained
using, for example, a node classification loss for ℓ .
up
G.2 Graph-Regularized MLPs, BloomGML, and Connections with LP
Graph-regularized MLPs (GR-MLP) represent a class of models typically applied to node classification tasks
whereby no explicit message-passing occurs within actual layers of the architecture itself (Ando and Zhang,Graph Machine Learning through the Lens of Bilevel Optimization
2006; Hu et al., 2021; Zhang et al., 2023). Instead, a base model (like an MLP or related) is trained using a
typical supervised learning loss combined with an additional regularization factor designed to push together the
predictions of node labels sharing an edge. These models are often motivated by their efficiency at inference time,
where the graph structure is no longer utilized.
As a simple representative example,3 consider the loss
ℓ(W;G):= 1∥Y¯ −H∥2 + λtr(cid:2) H⊤LH(cid:3) , s.t. h =π(x ;W) ∀v ∈V. (41)
2 F 2 v v
Here we are assuming that Y¯ is defined analogously as in Section G.1. In this expression, the first term
represents quadratic supervision applied to node labels Y¯, while the second term introduces the eponymous
graph-regularization with graph Laplacian L. Within the constraint, π is some node-wise trainable model such as
an MLP. Hence model training amounts to learning a W such that the resulting node embeddings closely match
the given labels subject to smoothing across the graph. For this purpose, we take gradient steps
(cid:12)
W(l) =W(l−1)−γ
∂ℓ(W;G)(cid:12)
(cid:12) , l=1,...,L. (42)
∂W (cid:12)
W=W(l−1)
We now show how these steps induce implicit message-passing GNN-like structure in certain cases, and later,
connect to label propagation. To see this effect in transparent terms, assume that π(x;W)=xW. Then we have
∂ℓ(W;G) =X⊤(cid:0) H+λLH−Y¯(cid:1) . (43)
∂W
FromherewedefineH(l) :=XW(l) suchthattheabovegradientstepsw.r.t.Wproduceacorrespondingsequence
of embeddings given by
(cid:16) (cid:17)
H(l) =H(l−1)−γXX⊤ H(l−1)+λLH(l−1)−Y¯ . (44)
Provided that X is full column rank, we can assume w.l.o.g. that columns of X are orthonormal, i.e., X⊤X=I.
(This is possible because we can always convert non-orthonormal columns into orthonormal ones via an invertible
linear transformation that can be absorbed into W.) Consequently, we may directly conclude that XX⊤ =P ,
X
where P indicates an orthogonal projection to the range of X, denoted range[X]. We then provide the following
X
straightforward interpretations of this process:
• From the perspective of BloomGML, we can convert (41) to an equivalent ℓ given by
low
ℓ (H;W,G):= λ (cid:88) ∥h −h ∥2+(cid:88) 1∥h −y¯ ∥2+I {H∈/ range[X]}, (45)
low 2 u v 2 2 v v 2 ∞
(u,v)∈E v∈V
wherewearetreatingthelabelsfromY¯ asauxiliaryinputstotheκfunction,andwearetriviallygeneralizing
thenon-smoothη functiontoapplyacrossallofHwhileincludingadependencyontheoriginalnodefeatures
from X. Additionally, the proximal gradient update for minimizing (45) is given by
(cid:104) (cid:16) (cid:17)(cid:105)
H(l) =P (1−γ)H(l−1)−γ λLH(l−1)−Y¯ , (46)
X
which is equivalent to (44) provided H(0) ∈range[X]. Analogous to the LP derivations from Section G.1,
the input argument to P [ · ] in (46) exactly reduces to (4) across all v ∈ V if we replace the π used in
X
(4) with the corresponding labels y¯ ; this follows from the derivations presented in Section B. In this way,
v
training GR-MLP model weights induces node embeddings analogous to those from a message-passing GNN
architecture, the final projection operator notwithstanding.
• If d=n, then P =XX⊤ =I and we are operating in an overparameterized regime where H=W. In this
X
special case, the iterations from (46) exactly reproduce LP from Section G.1, excluding the projection step
as some LP variants do.
• In the more typical setting with d<n, the lingering difference between (46) and LP hinges on the projection
operatorbeinginvoked,P
X
versusP
Y¯
(whereP
Y¯
isdefinedinSectionG.1;itisnot equivalenttoaprojection
to range[Y¯]). Which operator is to be preferred depends heavily on the quality of input features. If range[X]
closely aligns with range[Y] (the range space of the true label matrix), then P will be preferable. In
X
contrast, when the relationship is closer to range[X]⊥range[Y], then P Y¯ will generally be superior.
3Admittedly, this example does not cover the full diversity of possible GR-MLP models in the literature. Nonetheless,
it remains a useful starting point for analysis purposes.Zheng, He, Qiu, Wang, Wipf
G.3 Empirical Illustration of LP/GR-MLP Insights
To illustrate some of the concepts from the previous section, we conduct a simple experiment involving Cora
and Citeseer data. We choose π(x;W)=xW and train GR-MLP models based on (46) using (i) original node
features, and (ii) overparameterized features whereby d=n. We also compare with an LP model, excluding the
projection step for a more direct evaluation.
NodeclassificationresultsfromtheseexperimentsareshowninTable6. Thefirsttworowsconfirmtheequivalence
between GR-MLP and LP for the reasons detailed in Section G.2. Furthermore, by examining the second and
third rows, we are able to observe how the quality of node features influences the efficacy of P . Specifically,
X
it has been previously demonstrated that the features and labels of Cora nodes have minimal positive (linear)
correlation (Luo et al., 2021). Therefore, from Table 6 we see that the GR-MLP model using the original features
performs much worse than the overparameterized version. In contrast, with Citeseer where the input features are
more aligned with the labels, the original feature model significantly outperforms the overparameterized one.
Table 6: Node classification accuracy (%) comparisons. Results are averaged over 5 trials; error bars (not shown)
are negligible.
Cora Citeseer
Label Propagation 70.2 50.2
GR-MLP Overparam. 70.2 50.2
GR-MLP Original 60.4 64.1
H Model Implementation and Section 5 Experiment Details
In this section we provide further details regarding the experiments from Section 5.
Details for Table 1. Our implementation is based on modifications of the public codebase from Yang et al.
(2021). We adopt the hyperparameters they reported for all datasets. For BloomGML, we apply a vectorized
version of a Huber penalty in the form
(cid:88)d (cid:26) 1u2, for |u |<1
δ(u):= 2 i i (47)
|u |− 1, otherwise,
i=1 i 2
where {u }d are the elements of the input vector u∈Rd. We then form the energy function
i i=1
ℓ (H;W,G)=
(cid:88)
f(h ,h
;r)+(cid:88)
κ(h ;x )=
λ (cid:88)
∥h −h
∥2+(cid:88)
δ(h −π(x ;W)), (48)
low u v v v 2 u v 2 v v
(u,v)∈E v∈V (u,v)∈E v∈V
where the dependency of f on r is irrelevant for homogeneous graphs. Note that because (48) is smooth and
differentiable (i.e., the Huber loss is differentiable and the ℓ norm defaults to merely a summation since its
1
argument is non-negative), no η term is needed. Hence, we choose A as gradient descent for producing the
BloomGML model A[ℓ , · ]. For ℓ we apply a standard node classification loss. We reproduce Table 1 from
low up
the main text with error bars obtained from averaging over 10 trials; these results are presented in Table 7.
Table 7: Full version of Table 1 including error bars from averaging over 10 runs. Performance mitigating spurious input
features. Here the detect ratio is obtained by computing ∥h −π(x ;W)∥ for all v ∈V and then segmenting out the
v v 2
percentage of corrupted nodes within the largest 20%.
Cora Citeseer Pubmed Arxiv
Base 54.97 ± 1.95 38.98 ± 1.96 45.67 ± 3.21 47.63 ± 0.23
Accuracy
BloomGML 65.83 ± 2.28 49.33 ± 1.95 72.70 ± 1.58 69.11 ± 0.23
Detect Ratio BloomGML 94.27 87.82 96.98 100.00Graph Machine Learning through the Lens of Bilevel Optimization
Details for Table 2. Building on the results from above and the description in Section 5 in the main text, we
consider the generalized form
ℓ (H;W,G)=
(cid:88)
f(h ,h
;r)+(cid:88)
κ(h ;x )=
λ (cid:88)
∥h C−h
∥2+(cid:88)
δ(h −π(x ;W)). (49)
low u v v v 2 u v 2 v v
(u,v)∈E v∈V (u,v)∈E v∈V
As before, this expression is differentiable so we can choose A as gradient descent with step-size parameter γ. All
of the hyperparameters were selected via Bayesian optimization using Wandb4 and we also provide the model
hyperparameters search space in Table 9. Note that we also found that it can be effective to simply learn λ, i.e.,
absorb it into W, which reduces the hyperparameter search space and can potentially even improve performance.
We show such supporting ablations in Section J.1.
In Table 8 we reproduce results from Table 2, including error bars from averaging over 10 runs and an additional
ablation. For the latter, we compare κ(h;x) = ∥h−π(x;W)∥2 labeled as BloomGML (w/o Huber) with the
2
κ(h;x)=δ(h −π(x ;W)) implied within (49). From Table 8 we observe that the Huber version works better
v v
on 4 of 5 datasets, the exception being Amazon where it is significantly worse. As one candidate explanation for
this exception, we note that the Amazon dataset has a significantly higher adjusted homophily ratio than the
other 4 datasets, where adjusted homophily is defined as in Platonov et al. (2022) (this metric is less sensitive to
the number of classes and their balance).
We also trained several standard GNN architectures, e.g., GCN and GAT, for reference purposes; however, we
found that their performance was considerably worse than the heterophily baseline architectures from Table 8
(and therefore Table 2 as well). This is in contrast to Platonov et al. (2023), which presents somewhat stronger
results associated with such popular architectures. However, this apparent contradiction can be resolved by
closer examination of the public implementations from Platonov et al. (2023). Here we observe that additional
modificationstotheoriginalmodelshavebeenintroducedtothecodebasethatcansignificantlyalterperformance.
While certainly valuable to consider, these types of enhancements can be widely applied and lie beyond the
scope of our bilevel optimization lens. Indeed the flexibility of BloomGML also readily accommodates further
architectural adjustments with the potential to analogously improve performance as well on specific heterophily
tasks.
Table8: FullversionofTable2includingadditionalablationsanderrorbarsfromaveragingover10runs. Nodeclassification
on heterophily graphs. Following convention, accuracy (%) is reported for Roman and Amazon, while ROC-AUC (%) is
used for Minesweeper, Tolokers, and Questions. The numbers in the upper block are from Platonov et al. (2023).
Roman Amazon Minesweep Tolokers Questions Avg.
FAGCN 65.22 ± 0.56 44.12 ± 0.30 88.17 ± 0.73 77.75 ± 1.05 77.24 ± 1.26 70.50
FSGNN 79.92 ± 0.56 52.74 ± 0.83 90.08 ± 0.70 82.76 ± 0.61 78.86 ± 0.92 76.87
GBK-GNN 74.57 ± 0.47 45.98 ± 0.71 90.85 ± 0.58 81.01 ± 0.67 74.47 ± 0.86 73.38
JacobiConv 71.14 ± 0.42 43.55 ± 0.48 89.66 ± 0.40 68.66 ± 0.65 73.88 ± 1.16 69.38
Base from (3) 76.63 ± 0.24 52.37 ± 0.20 88.97 ± 0.05 80.91 ± 0.24 76.72 ± 0.49 75.12
BloomGML (w/o Huber) 84.45 ± 0.31 52.92 ± 0.39 91.83 ± 0.28 84.84 ± 0.27 77.62 ± 0.33 78.33
BloomGML (w/ Huber) 85.26 ± 0.25 51.00 ± 0.47 93.30 ± 0.16 85.92 ± 0.14 77.93 ± 0.34 78.68
Table 9: Model hyperparameters selection search space for Tables 2 and 3. For BloomGML (w/o Huber), λ is a
learnable parameter that is not searched by BO; see Section J.1 for an ablation on learning λ.
Hyperparameters Range for Table 2 Range for Table 3
L [2,4,6] [4, 8, 16]
λ [0.1 , 1, 5, 10] [0.01, 0.1 , 1, 10]
γ [0 , 0.5, 1] [0 , 0.1, 0.5, 1]
MLP layers before prop [0, 1, 2] [0, 1, 2]
Hidden size [128,256,512] [16, 64, 128]
4https://wandb.ai/siteZheng, He, Qiu, Wang, Wipf
Details for Figure 1. To show the effect of BloomGML to have connected nodes with deviated embeddings,
we use a Huber penalty for the edge-dependent regularizer term leading to the energy function
ℓ (H;W,G)= (cid:88) f(h ,h ;r)+(cid:88) κ(h ;x )= λ (cid:88) δ(h −h )+ 1 (cid:88) ∥h −π(x ;W)∥2. (50)
low u v v v 2 u v 2 v v 2
(u,v)∈E v∈V (u,v)∈E v∈V
For the experiments, we fixed the hidden embedding size to 1024, which gives the model more capacity for
handling heterphility, and we run Bayesian optimization with the same range as in the left block of Table 9 for
the remaining hyperparameters.
Details for Table 3. Fortheseresultsinvolvingheterogeneousgraphs, wemodifythecodebasefromtheHALO
model (Ahn et al., 2022) and adopt κ(h;x)=δ(h −π(x ;W)) or κ(h;x)=ω(h −π(x ;W)), where ω is the
v v v v
Log-Cosh loss given by
d
(cid:88)
ω(u):= log(coshu ). (51)
i
i=1
The hyperparameters we used are also selected via Bayesian optimization with Wandb; the search range is
provided in the right block of Table 9, for both HALO and BloomGML. Note that we observe negligible variance
in the test accuracies across different seeds, so we omit error bars from multiple runs.
Details for Table 4. We provide specifics directly tied to creating Table 4 here, while a more comprehensive
treatment of how NBFNet relates to BloomGML is deferred to Section I. Our implementation is based on
modifications of the public codebase from Zhu et al. (2021c), which involves (among other things) two trainable
linear transformations. The first involves parameters W and b used to create the relation embeddings for each
r r
relation type via e =W q +b , where q is the query embedding. The other is Φ used to transform the output
r r q r q
of the aggregation function back to the embedding space. To satisfy the criteria of BloomGML, we modify the
code to share these weights across all layers. Performance results for RefactorGNN (Chen et al., 2022) were taken
from the original paper due to lack of publicly-available reproducible code for conducting our own comparisons.
Details for Table 5 and Figure 2. For Table 5, we modify the codebase from Yang et al. (2021) by using the
update step defined in (31). The components are chosen based on (4). The hyperparameters are chosen via grid
search over λ, γ and β. For Figure 2, we use the same set of hyperparameters for both SGD and BloomGML
(momentum), with the number of message-passing layers being 200.
I Exploring the Connection between NBFNet and BloomGML
Given a graph G =(V,R,E), let G denote a conditional version with the same node, relation, and edge set, but
cq
with node features X ={x } and labels Y ={y } that depend on a fixed source node c∈V (in
cq v,cq v∈V cq v,cq v∈V
this section c should not be confused with the class label dimension used elsewhere) and query relation q ∈R. By
conditioning in this way, KGC or link prediction queries specific to c and q can be converted to node classification,
e.g. y = 1 if (c,q,v) is a true edge in the original graph. We may then later expand to unrestricted link
v,cq
prediction tasks by replicating across all c∈V and q ∈R. This is the high-level strategy of NBFNet.
In this section, we first show that given a graph G described as above, an NBFNet-like architecture with
cq
parameters shared across layers can be induced using BloomGML iterations of the form A[ℓ , · ] for suitable
low
choice of energy ℓ and A. For this purpose, we define the lower-level energy function as
low
ℓ (H ;W,G ):= (52)
low cq cq
(cid:88) (cid:0) h⊤ v,cqΦh u,cq+h⊤ v,cqΦe r+h⊤ u,cqΦe r−1(cid:1) + 21 (cid:88) (cid:104) ||Ψ21h v,cq+Ψ− 21Φx v,cq||2 2+η(h v,cq)(cid:105) ,
(u,r,v)∈Ecq v∈Vcq
where H = {h } are node embeddings and e := W q+b are relation embeddings, q is a so-called
cq v,cq v∈V r r r
shared query relation embedding (distinct from the relation index of query q), and W and b are trainable
r r
parameters that are bundled within W along with Φ,Ψ∈Rd×d. Additionally, x :=I(c=v)q for all v ∈V .
v,cq cq
To ensure the gradient expressions are symmetric in form across relations and inverse relations, we require that Φ
and Ψ are symmetric matrices. However, it has been shown in Yang et al. (2022b) that in certain circumstances,Graph Machine Learning through the Lens of Bilevel Optimization
asymmetric weights can be reproduced by symmetric ones via an appropriate expansion of the hidden dimensions.
Let
f(h ,h ;r)=h⊤ Φh +h⊤ Φe +h⊤ Φe (53)
u,cq v,cq v,cq u,cq v,cq r u,cq r−1
Then we have
∂f(h ,h ;r)
u ∂,c hq v,cq =Φh u,cq+Φe
r
=:Φf(cid:101)M(h u,cq,r,h v,cq)
v,cq (54)
∂f(h ,h ;r)
u ∂,c hq v,cq =Φh v,cq+Φe
r−1
=:Φf(cid:101)M(h u,cq,r−1,h v,cq)
u,cq
Thus, the aggregated function is
(cid:88) (cid:88)
a
v,cq
= f(cid:101)M(h u,cq,r,h v,cq)= (h u,cq+e r) (55)
(u,r)∈Nv (u,r)∈Nv,cq
Next, let
κ(h v,cq;x v,cq)=||Ψ1 2h v,cq+Ψ− 21Φx v,cq||2
2
(56)
Then, by reintroducing η and adopting proximal gradient descent for A, we produce the message-passing function
µ(l) :=h(l−1)+e(l−1)
(u,r,v),cq u,cq r
a(l) := (cid:88) µ(l)
v,cq (u,r,v),cq (57)
(u,r)∈Nv,cq
(cid:16) (cid:104) (cid:105) (cid:17)
h(l) :=prox Φ a(l) +x +Ψh(l−1) .
v,cq 1,η v,cq v,cq v,cq
This expression closely resembles an NBFNet model layer with summation aggregation.
Proceeding further, we extend the scope of our discussion to an expanded graph G with node set V and edge
exp exp
set E , where G =∪G , ∀(c,q,·)∈E . Analogously, we define an extended energy function over G as
exp exp cq exp exp
(cid:88)
ℓ (H;W,G ):= ℓ (H ;W,G )
low exp low cq cq
(c,q,·)∈Eexp
(cid:32)
= (cid:88) (cid:88) (cid:0) h⊤ v,cqΦh u,cq+h⊤ v,cqΦe r+h⊤ u,cqΦe r−1(cid:1) (58)
(c,q,·)∈Eexp (u,r,v)∈Ecq
(cid:33)
+ 1
2
(cid:88) (cid:104) ||Ψ21h v,cq+Ψ− 21Φx v,cq||2 2+η(h v,cq)(cid:105)
v∈Vcq
For elements in Y , positive instances correspond with true edges in the original graph, while negative instances
cq
are obtained by sampling. With Y :=∪Y , we define an upper-level energy as
exp cq
(cid:88) (cid:88) (cid:104) (cid:16) (cid:17) (cid:105)
ℓ (W,Y ;Θ):= D g h(L)(W);Θ ,y , (59)
up exp v,cq v,cq
(c,q,·)∈Eexpv∈V c′
p
where V′ represents the node set of each G with observed labels. In this way, the resulting bilevel optimization
cq cq
process can be viewed as instantiating a BloomGML node classification task on the expanded graph G , with
exp
architecture mimicking a specialized NBFNet-like model designed to minimize ℓ .
low
J Additional Experiments
In this section we include additional ablations and further demonstration of the versatility of BloomGML.Zheng, He, Qiu, Wang, Wipf
J.1 Ablation on Learning λ
There are two potential advantages to learning differentiable hyperparameters within ℓ . First, relative to
low
hyperparametertuning,itmaybepossibletoboostperformance. Andsecondly,evenifperformanceimprovements
are stubborn, by learning such hyperparameters, we economize the sweep over any remaining hyperparameters as
the effective search space has compressed along one dimension. In this regard, Table 10 shows results on the
heterophily datasets (and the same setup from Tables 2 and 8) with and without learning λ. From these results
we observe that learning λ produces reliable performance, suggesting that it can be trained instead of tuned (as a
hyperparameter) without sacrificing accuracy.
Table 10: Ablation on learnable λ. Accuracy results are obtained by averaging over 10 trials.
Roman Amazon Minesweep Tolokers Questions Avg.
w/o Huber 84.28 ± 0.63 52.76 ± 0.27 91.89 ± 0.25 84.66 ± 0.33 77.46 ± 0.30 78.21
w/o Huber (learnable λ) 84.45 ± 0.31 52.92 ± 0.39 91.83 ± 0.28 84.84 ± 0.27 77.62 ± 0.33 78.33
w/ Huber 85.26 ± 0.25 51.00 ± 0.47 93.30 ± 0.16 85.92 ± 0.14 77.93 ± 0.34 78.68
w/ Huber (learnable λ) 85.36 ± 0.36 51.32 ± 0.43 92.70 ± 0.60 85.86 ± 0.18 78.01 ± 0.45 78.65
J.2 Efficiently Enforcing Layernorm Using η
If for some reason we prefer to have a model with layer-normalized node embeddings, one option is to introduce a
penalty of the form (∥h∥2−1)2, absorb this factor into κ(h;x), and pthen roceed to minimize ℓ using regular
2 low
gradient descent for A. However, this requires an additional trade-off hyperparameter, and if we want to be
arbitrarily close to the unit norm, convergence will become prohibitively slow. Alternatively, we can enforce
strict adherence to unit norm by choosing η(h)=I {∥h∥2 ≠ 1} and then adopt proximal gradient descent for
∞ 2
A to handle the resulting discontinuous loss surface. This involves simply taking gradient steps on all other
differentiable terms and then projecting to the unit sphere (i.e., the proximal operator). We compare these two
approaches in Figure 3, where proximal gradient descent converges far more rapidly. For the model without
proximal gradient descent, we add the penalty α(∥h∥2−1)2 to the energy from (3), where α is the trade-off
2
parameter; we choose α=1 in Figure 3. For the model with proximal gradient descent, we project each node
embedding to the surface of a ball satisfying ||h||2 =1.
2
Figure 3: ℓ value versus the number of propagation steps in Roman dataset.
low