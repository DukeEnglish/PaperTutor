iScore: Visual Analytics for Interpreting How Language Models
Automatically Score Summaries
AdamCoscia LangdonHolmes JoonSuhChoi
GeorgiaInstituteofTechnology WesleyMorris GeorgiaStateUniversity
Atlanta,Georgia,USA langdon.holmes@vanderbilt.edu Atlanta,Georgia,USA
acoscia6@gatech.edu wesley.g.morris@vanderbilt.edu jchoi92@gsu.edu
VanderbiltUniversity
Nashville,Tennessee,USA
ScottCrossley AlexEndert
VanderbiltUniversity GeorgiaInstituteofTechnology
Nashville,Tennessee,USA Atlanta,Georgia,USA
scott.crossley@vanderbilt.edu endert@gatech.edu
ABSTRACT KEYWORDS
Therecentexplosioninpopularityoflargelanguagemodels(LLMs) Datavisualization,visualanalytics,largelanguagemodels,explain-
hasinspiredlearningengineerstoincorporatethemintoadaptive ableAI,educationaltechnology
educationaltoolsthatautomaticallyscoresummarywriting.Un-
ACMReferenceFormat:
derstandingandevaluatingLLMsisvitalbeforedeployingthemin AdamCoscia,LangdonHolmes,WesleyMorris,JoonSuhChoi,ScottCross-
criticallearningenvironments,yettheirunprecedentedsizeand ley,andAlexEndert.2024.iScore:VisualAnalyticsforInterpretingHowLan-
expandingnumberofparametersinhibitstransparencyandim- guageModelsAutomaticallyScoreSummaries.In29thInternationalConfer-
pedestrustwhentheyunderperform.Throughacollaborativeuser- enceonIntelligentUserInterfaces(IUI’24),March18–21,2024,Greenville,SC,
centereddesignprocesswithseverallearningengineersbuilding USA.ACM,NewYork,NY,USA,16pages.https://doi.org/10.1145/3640543.
anddeployingsummaryscoringLLMs,wecharacterizedfundamen- 3645142
taldesignchallengesandgoalsaroundinterpretingtheirmodels,
1 INTRODUCTION
includingaggregatinglargetextinputs,trackingscoreprovenance,
andscalingLLMinterpretabilitymethods.Toaddresstheirconcerns, Theadventoflargelanguagemodels(LLMs)hascatalyzedstate-of-
wedevelopediScore,aninteractivevisualanalyticstoolforlearning the-artresearchinthelearninganalyticscommunityonadvancing
engineerstoupload,score,andcomparemultiplesummariessimul- thecapabilitiesofadaptiveeducationaltools,namelyautomated
taneously.Tightlyintegratedviewsallowuserstoiterativelyrevise scoringofsummarywriting[6,34].Forexample,LLMscanbeused
thelanguageinsummaries,trackchangesintheresultingLLM toautomaticallyscoreasummarywrittenonalargerbodyoftext
scores,andvisualizemodelweightsatmultiplelevelsofabstraction. (Fig.1)inavarietyoflearningenvironments.Fordatascientists
Tovalidateourapproach,wedeployediScorewiththreelearning inthelearninganalyticscommunity,henceforthcalledlearning
engineersoverthecourseofamonth.Wepresentacasestudy engineers,itisextremelyimportanttotestLLMsonmanydifferent
whereinteractingwithiScoreledalearningengineertoimprove summariesandunderstandhowtheLLMswork.However,using
theirLLM’sscoreaccuracybythreepercentagepoints.Finally,we deeplearningmodelsintroducesopaquenessintomodelevaluation
conductedqualitativeinterviewswiththelearningengineersthat [28,40],makingitdifficultforlearningengineerstoclosetheloop
revealed how iScore enabled them to understand, evaluate, and ofmodeldevelopment[58].InteractivelyexploringhowtheirLLMs
buildtrustintheirLLMsduringdeployment. scoredifferentsummariescanhelplearningengineersunderstand
thedecisionsonwhichtheLLMsbasetheirscores,discoverunin-
tendedbiases,updatetheLLMstoaddressthebiasesandmitigate
CCSCONCEPTS
thepotentialpedagogicalramificationsofprematurelydeploying
•Human-centeredcomputing→Visualanalytics;•Comput- untestedLLM-powerededucationaltechnologies[27].
ingmethodologies→Neuralnetworks;•Appliedcomputing UnderstandingandevaluatingLLMsisextremelychallenging
→Interactivelearningenvironments. duetotheirunwieldysizeandever-growingnumberofparameters,
makingitdifficulttoidentifythecausesofperformanceissuesand
addressthemasneeded[46].Transparencyiscriticalforbuilding
trustinusingLLMs[30],especiallyforlearningengineerswhoare
increasinglyusingLLMsas“blackboxes”fordownstreamtasks,as
wellasduringhuman-in-the-loopevaluationofLLMperformance
ThisworkislicensedunderaCreativeCommonsAttribution4.0InternationalLicense. wherequantitativebenchmarksoftenfallshort[3].Visualanalytics
IUI’24,March18–21,2024,Greenville,SC,USA toolsareincreasinglyusedforimprovingthetransparencyofLLMs
©2024Copyrightheldbytheowner/author(s).
andhelpingdevelopersinterpretLLMbehavior[24,31].Yetlittle
ACMISBN979-8-4007-0508-3/24/03.
https://doi.org/10.1145/3640543.3645142 researchatthenexusofmachinelearning(ML)andeducational
1
4202
raM
7
]CH.sc[
1v06740.3042:viXraIUI’24,March18–21,2024,Greenville,SC,USA Coscia,etal.
datahasleveragedvisualanalyticsforexplainingML-powereded- feedbackfromiScore,theengineerimprovedthescoringaccuracy
ucationaltechnologies[10,19,35,65]andnonehaveusedLLMs, oftheirLLMbythreepercentagepoints.Fromthesameexperts’
presentinganopportunitytocollaborativelydevelopdesignprinci- qualitativefeedback,wereportseveralfindings.iScoreimproved
plesinthisspacewithdomainexperts.Weseektobuildavisual understandingofhowspecificmodelswork;e.g.,removingthefirst
analyticssystemthatmakesevaluatingsummaryscoringLLMs sentenceofasummarycausedsomemodels’scorestodropbyupto
moretransparentforlearningengineers,helpingthemunderstand 90%.Themodelanalysisvisualizationshelpedengineers“see”what
andbuildtrustintheirLLMsbeforedeployment. theLLMspaidattentionto,whiletrackingchangesinscoresacross
Throughauser-centereddesignprocess[43]workingdirectly revisionsillustratedhowadversarialexamplescantrickLLMsinto
withlearningengineerswhotrain,test,anddeployautomaticsum- givingincorrectscores.Theperturbationvisualizationswereunani-
maryscoringLLMs(Sect.3),wesynthesizedseveralfundamental mouslyconsideredthemostusefulfeatureforinspiringtrustwhen
challenges around the aggregation, provenance, and scalability deployingLLMsincriticallearningenvironments.Reflectingon
ofvisualizingwritingandLLMscoringdatatogether.Forlearn- thecasestudyandfindings,wethendiscussimplicationsforthe
ingengineers,characterizingqualityinwritingsamplesinvolves designoffuturesystems,lessonslearnedonbuildingresponsible
comparingdifferencesinbothtextandLLMscoresacrossmulti- andethicalAIforeducation,generalizingourtechniquestoother
plescoringdimensionsandbetweenmultipledifferentsamples, LLMsandfinallylimitationsandfuturework(Sect.7).
revisionsofthesamesample,andexpert-scored“groundtruth” Insummary,ourpapercontributes:(1)designchallengesand
samples.Performingthesetasksenablesthemtocalibratetheir usertasksforhelpinglearningengineersevaluateautomatedsum-
trustintheLLMsummaryscores.IncreasingtransparencyinLLM maryscoringLLMs;(2)iScore,anopen-sourcevisualanalyticstool
scoresrequiresprobingmodelbehaviorexternallybyvaryinginput thataggregatesandcomparesLLMdataandLLM-scoredwriting
parametersandinternallybyexploringmodelweights,aswellas samples;(3)acasestudydetailinghowlearningengineersdeployed
scalingthesemethodstolargeamountsoftextwhilekeepinglearn- iScoretoimprovetheirLLMsandusagescenariosthatdemonstrate
ingengineersintheloopatdifferentlevelsofdetail.Toaddress thegeneralizabilityofiScore;and(4)aqualitativeevaluationwith
thesechallengesinasingleinterface,asuccessfulvisualization learningengineersdescribinghowiScorehelpedthemunderstand,
systemshouldhelplearningengineersscaffoldtheevaluationpro- evaluate,andbuildtrustintheirLLMsduringdeployment.
cessaroundcomparingrevisionsofmultiplewritingsamples,scale
multipleinterpretabilitymethodstoworkwithlargeinputsand 2 RELATEDWORK
visuallyaggregatetextatmultiplelevelsofabstraction.
2.1 AutomaticallyScoringSummaryWriting
WepresentiScore(Sect.4),anopen-source1,interactivevisual
Summarywritingisavaluablepedagogicaltooltohelplearners
analyticstoolforlearningengineerstoupload,score,andcompare
buildknowledgeaboutasubjectarea,aswellasforassessment
multiplesummariesofasourcetextsimultaneously.iScoreintro-
purposes[21,23,38].Ameta-analysisof56experimentsfoundthat
ducesanewworkflowforcomparingthelanguagefeaturesthat
textsummarizationimprovedlearningregardlessoftheknowledge
contributetodifferentLLMscoresbystructuringanalysisacross
domain[22].Thereasonfortheselearningincreasesmaybethat,
threecoordinatedviews.First,usersupload,scoreandcanmanually
whensummarizing,learnersareaskednotonlytoretrieveinfor-
reviseandre-scoremultiplesource/summarypairssimultaneously
mationfromthetextbutalsotoconstructandorganizetheirown
intheAssignmentsPanel.Then,userscanvisuallytrackhowscores
schemataofthecontentarea[17,36,45].Despitethebenefits,scal-
changeacrossrevisionsinthecontextofexpert-scoredLLMtrain-
ingsummarizationtasksinlearningenvironmentsischallenging;
ingdataintheScoresDashboard.Finally,userscancomparemodel
providingfeedbacktolearnersisbothdifficultandtime-consuming
weightsbetweenwordsacrossmodellayers,aswellasdifferences
[18]. In response to this challenge, researchers have developed
inscoresbetweenautomaticallyrevisedsummaryperturbations,
severalstrategiestoautomaticallyscoresummaries.Forexample,
usingtwomodelinterpretabilitymethodsintheModelAnalysis
Crossleyetal.usedindicesoflinguisticfeaturesinsummariesas
View.Together,theseviewsprovidelearningengineerswithac-
wellasWord2vecsimilarityscorestodevelopamodelthatexplained
cesstomultiplesummarycomparisonvisualizationsandseveral
53%ofscorevariance[12].Deeplearningmethodsareparticularly
well-knownLLMinterpretabilitymethodsincludingattentionattri-
well-suitedtothistask,asBotarleanuetal.demonstratedbyusing
bution,inputperturbation,andadversarialexamples.Combining
fine-tunedlargelanguagemodels(LLMs)toexplain55%ofthescore
thesevisualizationsandmethodsinasinglevisualinterfacebroadly
varianceintheirdataset[6].Morrisetal.improvedonthiswork
enablesdeeperanalysisofLLMbehaviorthatwaspreviouslytime-
withLLMsbyusingprincipalcomponentsratherthanrawscoresto
consuminganddifficulttoperform.
explain66−79%ofscorevariance[34].Whilethelanguagefeatures
Tovalidateourapproach,wedeployediScorewithourcollabo-
usedtoexplainsummaryscoringmappedontoexpectationsabout
rators,alearninganalyticsteam,overthecourseofamonth,and
whatmakesagoodsummary(i.e.,overlapwithsourcetext,text
conductedfollow-upinterviewswiththesameengineersfromthe
organization,andvocabulary),thelanguagefeaturesthatpower
teamwithwhomwecollaborativelydesignediScore(Sect.6).We
theLLMsummaryscoringmodelsareverydifficulttointerpret.
firstdescribeacasestudywhereoneofthelearningengineersused
iScoretoimprovetheaccuracyofLLMsusediniTELL,anintelli-
2.2 ModelingLanguageWithTransformers
genttextbookframeworkthatcanautomaticallyscoresummaries
oftextbooksectionswrittenbylearnerssuchasstudents.Using We aim to help learning engineers interpret large transformer-
basedlanguagemodelsthatautomaticallyassignscorestowritten
1iScoremodelsandcode:https://github.com/AdamCoscia/iScore summaries of source text. Language models learn to model the
2iScore:VisualAnalyticsforInterpretingHowLanguageModelsAutomaticallyScoreSummaries IUI’24,March18–21,2024,Greenville,SC,USA
probabilityofatokenoccurringinasequence(e.g.,awordina oflearnerdropoutinMOOCs.Garcia-Zanabriaetal.utilizedasim-
sentence).Transformer-basedlanguagemodelsworkbyencoding ilarapproachtovisuallyexplainpredictionsoflearnerdropout
allinputwordsinasentenceintonumericrepresentationsknown usingcounterfactualexplanationsintheirsystem,SDA-Vis[19].
asembeddings.Attentionmechanismsarethenusedtocombine Zhangetal.leveragedeeplearningmodels(CNNandLSTM)to
differentwordembeddingsinasentencetogether,creatingnew predictlearnerdropoutandvisualizecounterfactualexplanations
embeddingsthatarecontextuallyinformedbydifferentpartsof usingDropoutVis[65].Acommonthemethatcutsacrossthese
thesentence[52].Attentionweightsdeterminehowdifferentem- toolsisafocusonhuman-in-the-loopworkflowsthatenabledata
beddingsarecombined,andtheyhavebeenusedtoexplorehow scientistslikelearningengineerstoinjecthumanintuitionanddo-
transformerswork[11].Largetransformer-basedlanguagemodels mainexpertiseintotheiterativeprocessoftrainingandvalidating
suchasBERT[14]andGPT-3[8]havedemonstratedstate-of-the- modelperformance[58],whichiScoremakesheavyuseof.
artperformanceonavarietyoftasks[46]inpartduetopre-training Toincreasetransparencyforlearningengineers,weaimtovi-
viaself-supervisedlearningusinglarge-scaleunlabeleddocument sualizeLLMperformanceacrossmultipleinterpretabilitymethods.
corpora.Pre-trainingcapturesfoundationalknowledgeofsemantic Onemethodisdirectlyvisualizingamodel’sinternalarchitecture
andsyntacticrelationshipsinbaselinemodelsusefulforfine-tuning asaformofexplanation[48,60].Withtransformers,severaltools
onspecifictasksdownstreamwithfewerexamplesneeded. focusonvisualizingtheinternalpredictionprocessoftransform-
ThemodelsusedinourworkbuildonMorrisetal.[34]byadopt- ersaschangesinattentionweightsacrosseachlayerandheadof
ingtheLongformer[2]architecture.Longformersareanembedding amodel[13,25,54,59].However,thereisdebateastowhether
transformermodelbasedonRoBERTa[32]thattokenizeasequence attentionweightsintransformerscanbeusedasasourceofin-
ofwordsasinput,replaceeachtokenwitha768-dimensionalem- terpretationformodelperformance[1,11,26,61].Alternatively,
bedding in a semantic vector-space, sum the embeddings with structuringvisualcomparisonbetweenvariationsinmodelinputs
positionalembeddingstoencoderelativepositioninformation,and andresultingoutputspresentsamoreflexibleandmodel-agnostic
finallyfeedtheembeddingmatrixintoaneuralnetworkmodel. analysisapproach.Forexample,VizSeqisavisualanalysistoolkit
Themodelcomprises12attentionlayersinwhichtheinputembed- forinteractivelyevaluatinglanguagemodeltaskbenchmarks[57].
dingmatrixistransposedandmultipliedbyitselftoformsimilarity Othertoolspresentavisualanalyticsworkflowtoanalyzechanges
metricspairwisebetweenembeddings,orattentionweights.For inlanguagemodelweightsundervarioustask-specificscenarios
mostBERT-stylemodels,eachtokenateachattentionlayerisat- [20,50].Bycombininginternalandexternalinterpretabilitytech-
tendedtobyeachothertoken,resultinginhigherdemandsfor niquesiniScore,weseektogiveusersmorecontrolandflexibility
computationastheinputsequencelengthincreases.Asaresult, overhowtheyinteractwiththemodels,increasingunderstanding
thesemodelshavealimitedmaxsequencelengthtypicallyaround ofmodelperformanceusingalternativeperspectives[30].
512tokens.However,providingadditionalcontextbyincluding
sourcetextasinputduringLLMtrainingincreasedperformance 3 DESIGNPROCESS
comparedwithtrainingonsummariesalone[34].Tosolveinput
Our goal in this work is to build a visual analytics system that
sequencelengthlimitations,Longformerassignsglobalattention
enables data scientists in the learning analytics community, i.e.
tooneormoretokensandaslidingattentionwindowwhich
learningengineers,tointerprethowlargelanguagemodels(LLMs)
movesacrossthetext.Tokensinglobalattentionareattendedtoby
automaticallyscoresummarywriting,helpingthemcalibratetheir
everyothertokenwhiletokensintheslidingattentionwindoware
trustinthesemodelsbeforedeployingthem.Todothis,ween-
attendedtoonlybyothertokenswithinthewindow.Asaresult
gagedinuser-centereddesignmethodologiesincludingcontextual
ofthisdesign,Longformercanacceptmuchlongertexts,upto
inquiry,rapidprototyping,anddesigniteration.Ourteamcom-
4096tokens,whileremainingcomputationallyefficient.Toincrease
prised all authors and included visualization experts in human
modelinterpretability,iScoreaddressesnovelchallengesaround
centeredcomputingandinteractiondesignaswellaslearningen-
visualizingtheslidingwindow,globalattention,andtheincreased
gineerswithexpertiseinnaturallanguageprocessing(NLP)and
numberoftokensandattentionsatscale.
developinglearningtools.Overthecourseofseveralmonths,we
workedtogetherinmultiplevirtualformativesessionstodevelopa
2.3 InterpretingMLUsingVisualAnalytics sharedunderstandingofthepainpointsinmaintainingahuman-
in-the-loopworkflowformonitoringandimprovingLLMsusedfor
Visualanalyticsisanincreasinglypopularapproachforanalyz-
automaticsummaryscoring.
ingandinterpretingmachinelearning(ML)models[24,31].We
Wefirstdescribetheworkflowofourcollaboratorsandtheir
buildonarichhistoryofvisualanalyticstoolsforunderstanding
developmentofsummaryscoringLLMs(Sect.3.1)presentedas
educationaldata[16,53,64],drawinginspirationspecificallyfrom
examplesthroughoutthepaper.Wethensummarizethekeydesign
systemsthatseektovisuallyexplainML-powerededucationaltech-
challenges and user tasks (Sect. 3.2) for interpreting how these
nologies.Inthisspace,Mubaraketal.builtasystemforvisualizing
modelsareperformingthatourvisualanalyticssolutionaddresses.
patternsoflearnerinteractionswithvideosinMassiveOpenOnline
Courses(MOOCs)tohelpexplainpredictionsoflearners’weekly
3.1 Background:SummaryScoringLLMs
performancefromtheinteractiondatausingdeeplearningmodels
[35].Chenetal.engagedauser-centereddesignprocesswithin- Inthispaper,wedemonstrateiScoreusingsummary-scoringLLMs
structorsandMLresearcherstodesignanddevelopDropoutSeer thatourcollaboratorsspecificallydevelopedforiTELL.InSect.6.1,
[10],avisualizationsystemforvisuallyexplainingMLpredictions wepresentacasestudywithalearningengineerfromourteam
3IUI’24,March18–21,2024,Greenville,SC,USA Coscia,etal.
Textbook Source Learner Summaries Expert Scores
Learning engineers are developing LLMs that automatically score summaries of textbook sources Learners write summaries of the textbook source sections, to be automatically scored LLMs train to replicate z-
scores of 2 PCA features
from an expert-scored rubric
Sunbathing causes cancer
 1 This summary is plagiarised verbatim and thus both Content and Wording are scored low Content
By Alex McFadden, Journalist at slate.com

 1 2 3 4
Fisher's study shows that people who have used a tanning bed before the age of 35 have a 75% higher Main idea 2.5
According to a recent study, there is no safe way to sunbathe. Even small doses of ultraviolet radiation l ti hk ee l rih eo suo ld ts o sf h d oe wv e tl ho ap ti n hg a vs ik ni gn uc sa en dc e ar tt ah na nn i nth go bs ee d w eh vo e nh a ov ne c en o ret su us le tsd i na t aa hn in gi hn eg r b re isd k a ot f s su kc inh ca an n e ca er rl .

y age. In fact, CD oe hta ei sls ion 22 .. 00
from the sun and tanning beds may cause cancer. Ultraviolet (UV) radiation is one of the most Objective 3.5
frequent causes of cancer, but it can be avoided. So says Professor David E. Fisher in an interview According to a study conducted in Norway, “each year, approximately 250 people die in Norway from skin PCA: 5.86 Z-score: -1.049
with slate.com about his most recent study of the relationship between UV radiation and skin cancer.

 c ca an rec fe ur l p inr im apa pr li yly in b ge sc ua nu ss ce r o eef ne x ac ne ds s ti av ke i ns gu n bb rea ath ki sn ig n. T thh ee sr his ak d o ef . Tca hn ec ree r a i rn ec gre roa use ns d, ss i fn oc re s em ria on uy s a cr oe n n co et r ns u wffi hc ei ne ntly 1 2 3 4Wording
nearly 30% of adolescents report that they are “completely certain” that they will be sunburned during their Paraphrase1.0
Fisher’s study shows that people who have used a tanning bed before the age of 35 have a 75% higher holidays. Having sunburn increases the overall risk of skin cancer. Source text2.0
likelihood of developing skin cancer than those who have not used a tanning bed at such an early age. PCA: 2.06 Z-score: -1.116
In fact, the results show that having used a tanning bed even once results in a higher risk of skin 2 This summary shuffles plagiarised sentences around; Content improves but Wording is the same
cancer.


Fisher's study shows that people who have used a tanning bed before the age of 35 have a 75% higher
According to a study conducted in Norway, “each year, approximately 250 people die in Norway likelihood of developing skin cancer than those who have not used a tanning bed at such an early age. In fact, Content
from skin cancer primarily because of excessive sunbathing. The risk of cancer increases, since many t sh ue sp r ee cs tu l tt hs a s th yo ow u t hh aa vt e h ta ov oin lig tt u les e vd it aa m ta inn n Din , gg o b te od ye ov ue rn d o on cc te o rr e tosu hlt as v ein a a b h loig oh de r s ari msk p lo ef ts ak kin en c .a In f c ther e. I tf e y so tu s hows 1 2 3 4 Main idea 2.5
are not sufficiently careful in applying sunscreen and taking breaks in the shade. There are grounds that you have too little of this important vitamin, the solution ought to be cod-liver oil or a vitamin D Details 3.0
f to hr e ys e wri io llu s b ec o sn uc ne brn u rw neh de n d un re ia nr gly t3 h0 e% ir o hf o la id do ayle ss . c Hen at vs inre gp o surt n bth ua rt n t h ine cy r ea ar se e s“ c to hm e p ole vt ee rl ay l l c re ir st ka in o” f st kh ia nt s v imu itp pap m ol re i tnm a nDe t,n vgt i. o t F a tr moo im y no , a u th rm ed e sod oci ltc uoa tirl op to no i h on a ut v go e hf tav ti be olw o bo, e t d h c s ois a d mi -s l ipa vl eev r e t or ay ik l es onim r. aIp f vl e t ih t i aes ms tu e ine s . tI D f s y h so o uu w p ps s lu ets h mp ae etc nyt to .t u Fh ra h ot a my vo e au t m oh o ea v dlie itt c t aleo l o po ofl i itt t nhl tie s o f PCA: 7.40 Z-sC O co b oh jee rc es tii :o v n e -0.22 2 9. .05 4
cancer.

 v pi ee ow p, l eth si hs ois u a ld v se ur ny bs aim thp el e t ois osu be ta, ia nn ed n w oe u gh ha v ve it aa m c il nea Dr .a Wnd e u hn aa vm e b si ag fu eo wu as ya sn ts ow mer e t ao s uth ree pq eu oe ps lti eo 'sn l eo vf e w l oh fe t vh ite ar m in 1 2 3 4Wording
S t hhu ae sn p mb oa at p nh u yi ln a pg t ri oois bn a l e td o ma s sn u .

g ne bro atu hs e s to ou orc be t ao if n v si uta ffim ci in en D t . v F iti as mhe ir n p Do ,i n bt us t o hu et wh ao rw n ss to hm ate sr ue cse ha arc rh ee cr os m re mco enm dm ate in ond D h s Pao r, y o wa s fn e. Umd ssl u tw orc a re h v D h iU o aa V l vv e ie t dr a (s EUda .if V ae Fti ) i w sro haa n d ey io rs a n i ti nt eo o a nnt nr e i e s ie na d o tt s e nv reti vt o a io esm f wt tain h y w e D h i tme hd a o e sl st lfi ah tc ty fi er.e e .Tn cqh oc uiy ms e i nif as tn b n ce oe ac ui ue t th s s hes e ia sr s r v o my e f. orT cy sah ts ne m rcr ee ea c rri e,s t nb , t tunh stou tr is ut v dn ce yo ar n y or e fbh a tees h ao a el vtn h ro et y io ld, a" e tit Fr d oiy s . n h St so e o h r g is pu a ye ss s PCA: 2.06 Z-sSP coar oua r rp c eeh : r ta e -s x 1e t .121 1.. 00 6
between UV radiation and skin cancer.
“We are thus in a situation where people are recommended to use something that we know is
carcinogenic to obtain a vitamin. There are many far safer ways to obtain this vitamin. If you suspect 3 This summary is original and scores higher in both Content and Wording! 1 2 3 4Content
that you have too little vitamin D, go to your doctor to have a blood sample taken. If the test shows Main idea 2.5
that you have too little of this important vitamin, the solution ought to be cod-liver oil or a vitamin Sunbathing can cause cancer. It can be dangerous to use a tanning bed according to Professor David E. Details 3.5
D un asu mp bp il ge um oe un st a. nF sr wo em r ta o m the ed qic ua el sp tio oi nn t o o f f w v hi ee tw h, e t rh pis e ois p la e v sher oy u ls dim sup nle b i as ts hu ee , t oa n od b tw aie n h ea nv oe u a g hc l ve ia tr a man ind F N lai cos kh rw e or a f w y u sh e ixo nc' gs e r sse uss i nve sea c r s rc euh en nbh aa ats nh s dih n no g o w h t n a ts at kh le ia nat gd p ate o o b t rp h ele ae k u d in e nd a tte hhr e s3 o s5 hf w a2 dh 5 eo 0 . g Epo ve e ot npa n l gen e ei ttin ag c n hh g a y av e e sa u ra . n T7 bh5 ue% rns e o i nf r cig s rke estti a sinn ecg sr yesk oai usn e r c cda aun nec c e t eor r. In PCA: 8.83 Z-sC O co b oh jee rc es tii :o v n e 0.43 2 0. .5 5 7
D de. fi W cie e nh ca yv e i fs a nf ee c w esa sy as r yt .o T m he ea resu ir s e t hp ueo s p nl oe’ s r ele av soel n o tf o v tit ra ym toin g D ue, sa sn hd o w we m ha uv ce h s a Ufe V w ra ay ds i at to io t nre oat n v ei t na em edin s tD o r inis Vk. i tI at mis in no Dt r te oc to am kem ae n sud pe pd l eto m g ee nt t V oi rt a um sein c oD d f -r lio vm er l oay ili n ing s to eu at d i .n T t hh ie s wsu an y. yH oe u r ce ac no m bem se un rd e s t oif gy eo tu t a hr ee V d ie tafi mci ie nn Dt 1 2 3 4Wording
stay healthy. This is neither very smart, nor very healthy,” Fisher says. you need without risking your health. SP oar ua rp ceh r ta es xe t22 .. 55
PCA: 3.93 Z-score: 0.331
Figure1:Anexampleofatextbooksource,learnersummariesandexpertscoresusedtotraintheLLMsvisualizediniScore.
ContentandWordingLLMseachassignacontinuousscorethatrepresentscomponentsofananalyticrubric.Learningengineers
seektocharacterizehowchangesinscoresrelatetodifferencesinsummariesviacomparison.iScoreprovidesinputsformultiple
summariespersourceandvisualizestheirpredictedscoressimultaneouslyincontextofthe“groundtruth”trainingdata.
whousediScoretoimprovetheaccuracyofLLMsiniTELLbythree ThescoresaredefinedduringLLMtrainingbasedona1−4scaled
percentagepoints.Byscaffoldingevaluationaroundpairsofsource analyticrubricwithsixcriteria:(1)mainidea,ortowhatextent
andsummarytext,iScorepresentsanewworkflowforlearning thesummarycapturedthemainideaofthesource;(2)details;(3)
engineerstointeractwithsummaryscoringLLMs. cohesion,orhowwellthesummarywasrationallyandlogicallyor-
ganized;(4)objectivelanguage,orreflectingthepointofviewofthe
3.1.1 iTELL:TextbooksThatScoreSummaries. Computationalad- source;(5)paraphrasing,oravoidingplagiarismbyparaphrasing
vancesattheintersectionofartificialintelligence(AI)andnatural theoriginalmaterial;and(6)languagebeyondthesource,orhow
languageprocessing(NLP)havecatalyzedinterestindeveloping wellallrelevantdetailswereincludedinthesummary.Training
intelligenttextbooksthatpresentadaptive“smart”functionalities sampleswerefirstexpert-scoredaccordingtotherubric.Then,to
tolearnerssuchaspersonalizedfeedback[9].Tohelpinstructors reducethedimensionalityoftheLLMoutput,therubriccriteriafor
quicklyandsimplybuildintelligentdigitaltextbooksatscale,our eachtrainingsampleweredistilledusingaprincipalcomponent
collaboratorsdevelopediTELL(IntelligentTextsforEnhancedLife- analysis(PCA)intotwocomponents.Contentcomprisesmainidea,
longLearning)asaframeworkforconvertingstatictextintoa details,cohesionandobjectivelanguage,whileWordingcomprises
dynamicinteractivewebapplicationwithminimallaborandtech- paraphrasingandlanguagebeyondthesource.ThetwoPCAscores
nicalexpertiserequiredfromcontentcreators.Akeyintelligent forContentandWordingwerethenz-scorenormalized.Inthisway,
featurebuiltintoiTELLishavinglearnerssummarizethesection thescoreswhichtheLLMsaretrainedtoassignnowrepresentthe
thattheyhavereadattheendofeachtextbooksection.iTELLthen standarddeviationofthescoredsummaryfromthemeanscore
providesformativefeedbackontheContentandWordingofthese alongasingledimension,transformingtherubricintoamorein-
summariesusingtwoLLMs,oneforeachfacet. terpretableandusefulresult.Thesescoresarethenusedasatarget
labelinthetrainingdataforeachrespectivemodel.
3.1.2 Sources,SummariesandScores. Ourcollaboratorstrained
Themodelswerefine-tunedon4690differenttrainingsummaries
twoBERT-styleLongformer[2]modelsthatassignContentand
writtenon101differentsourcetexts,enablingthemtoadaptto
Wordingscorestosummariesoftextbooksourcecontent.After
source/summarycombinationsondifferenttopicswithminimal
thecasestudy,theytrainedtwomoremodels,Content(global)
to no adjustments needed. The training task was modeled as a
andWording(global).Weevaluateallfourmodelsinthispaper.
sentenceclassificationtask[56],whereeachmodeltriestopredict
Fig.1showshowscoresassignedbytheLLMscanvaryacross
acontinuousscore(i.e.theContentorWordingscore)asalabel.In
revisionsofthreedifferentsummariesofthesametextbooksource.
4iScore:VisualAnalyticsforInterpretingHowLanguageModelsAutomaticallyScoreSummaries IUI’24,March18–21,2024,Greenville,SC,USA
Table1:DesignChallenges(C)andUserTasks(T) Tofurtherenrichcomparison,engineersalsouseexpert-scoredwrit-
ingsamplesasareference(T3).InthecontextofourContentand
C1 Characterizinghowsummariesandscoresarerelated Wordingsummaryscoringmodels,theseexpert-scoredsamplesare
usedastrainingdata.Ourinterfaceshouldenableuserstocompare
T1 Stratifyscoringacrossseveralfacetsofwriting
multiplesummariesandscoresatonce,aswellasinreferenceto
T2 Comparemultiplesummariesandscoresatonce
the"groundtruth"expert-scoredtrainingdata.
T3 Useexpert-scoredwritingsamplesasareference
C2 Comparingdifferencesbetweenversionsofsummaries.
C2 Comparingdifferencesbetweenversionsofsummaries
Ifasummary’sscoresaretoolow,revisionsneedtobemadethat
T4 Highlighthowrevisionsaffectassignmentscores addressthefactorsthatthemodelsbasetheirassessmenton.It
T5 Investigatemultipledifferenttypesofrevisions iscriticaltogivelearningengineersmethodsthathighlighthow
T6 Tracktheprovenanceofboththewritingandscores revisionsaffectautomaticscoreassignments(T4).Engineersarealso
T7 Selectandviewpreviousversionsofsummaries interestedininvestigatingmultipletypesofrevisionsthatcaninclude
C3 Understandingmodelparametersandbehaviors spellingandgrammar,paraphrasing,andkeywords(T5).Together,
thesemethodscanrevealinsightsintohowthelanguagemodels
T8 Comparevariationsinmodelinputstomodeloutputs
areworkingandprovideengineerswithusefulcriteriaforfeedback
T9 Examinetheinternalmodelweights(i.e.attention)
tolearnersastheywritetheirsummaries.Asrevisionsaccumulate,
C4 Bridgingglobalandlocalmodelbehavior our system should track the provenance of both the writing and
scores (T6), and allow users to select and view previous versions
T10 Summarizeinteractionsbetweenalltokensatonce
ofsummaries(T7).Thiscomplimentstheneedforcomparisonof
T11 Comparetoken,layerandheadinteractions
multiplesummariesandscoressimultaneouslyinFig.1byadapting
T12 Drilldowntointeractionsatspecificlayersandheads
thecomparisontomultipleversionsofthesamesummary.
C3 Understandingmodelparametersandbehaviors.Usinga
deeplearningapproachforautomaticallyassigningscoresintro-
atypicalmulti-classclassificationtask,theoutputiscomposedof
ducesopaquenessintounderstandingmodelbehavior[30],making
multiplelabels,eachofwhichreceivealogitscore.Duringtraining,
itdifficulttosupportlearningengineersinclosingtheNLPloop
themodeloutputsarecomparedagainstone-hotencodedtarget
ofmodeldevelopment[58].Inthecontextofscoringwrittensum-
outputsandlossiscomputedusingcross-entropy.Duringinference,
maries,engineersgrapplewithtwodifficulttaskswheninterpreting
themaxlogitscoreischosenasthepredictedlabel.Whenpredicting
transformer-basedlanguagemodelsspecifically.Thefirstisunder-
acontinuousscore,however,onlyasinglelabelisusedandthe
standinghowvariationsinmodelinputscanleadtochangesinmodel
logitscoreofthatlabelisinterpretedasthepredictedscore.Instead
outputs(T8).Forexample,counterfactualexplanations[19,55,65]
ofcross-entropy,meansquarederror(MSE)waschosenastheloss
areoftenusedtoexploremodelbehaviorandrobustnessbysys-
functionbecauseMSEpenalizeslargererrorsmorethansmaller
tematically varying and comparing different types of revisions.
errors,thusencouragingcloseralignmentwiththepredictedlabels.
Thesecondisinterpretingtheinternalmodelweights(i.e.attention
Thismethodiscommonlyusedintasksthatrequiretheprediction
intransformers)(T9)tounderstandwhatthemodeliscapturing
ofacontinuouslabel[29,37,47].Themodelsweretrainedforsix
epochswithabatchsizeofeightandalearningrateof3𝑒-5. aboutsummaries.Thiscouldenableengineerstoidentifyemergent
representationsofsemanticknowledgeandlexicalstructurethat
contributetocertainmodelscores[42];e.g.,identifyingsalientin-
3.2 DesignChallengesandUserTasks
teractionsbetweenspecificcharacterspansacrossmodellayersand
Fromourformativesessions,wesynthesizedseveralkeydesign heads.Providingbothexternalandinternalmodelprobingmethods
challenges(C)andusertasks(T)summarizedinTable1andinte- canguideusersindiscoveringusefulinsightstohelpthemclose
gratedintoourdescriptionsofthesystem(Sect.4),usagescenarios theNLPloopofmodeldevelopment.
(Sect.5)andevaluation(Sect.6)throughouttherestofthepaper.
C4 Bridging global and local model behavior. In addition
C1 Characterizingtherelationshipbetweensummariesand toexistingchallengesinterpretingdeeplearningmodelbehavior,
scores.Summariesbalancemultiplefacetsofwritingquality(text learningengineersalsofaceascaleissue,asthemodelsusedin
length,cohesion,paraphrasing,details,etc.),makingitdifficultto iTELL can score textbook sources and summaries thousands of
automaticallyassignasinglegradedscoreencapsulatingallaspects words(tokens)inlength.Thismakessummarizingthesheerscale
of importance. To address this, learning engineers first seek to oftokensandattentionswhilehelpingusersmakesenseofthedata
modeleachofthesefacetsindividuallybystratifyingscoringalong difficult.Toaddressthis,engineersseekmethodstosummarizethe
several dimensions, using multiple scoring models (T1). This can interactionsbetweenalltokensinthesameinterface(T10).Bridging
helplanguagemodelsautomaticallyassignmoreobjectivescoresto thegapbetweenglobalandlocalmodelbehaviorcanbesupported
specificfacetssuchasplagiarismorgrammar.Withmultiplescores, bycomparinginteractionsbetweentokens,layersandheads(T11)
engineerscanthenbegintointerpretandimprovemodelperfor- whileenablinguserstodrilldowntospecifictokeninteractionsat
mancebycomparingmultiplesummariesandscoressimultaneously specificlayersandheads (T12).Byaggregatingdataatmultiple
(T2).Fig.1providesanexampleofhowdifferencesinsummaries levels,oursystemshouldrevealsubsetsofinterestinginteractions
(e.g.,syntacticandsemantic)canleadtodifferentlyassignedscores. thathelpengineersmakesenseofcomplexmodelbehaviors.
5IUI’24,March18–21,2024,Greenville,SC,USA Coscia,etal.
A Assignments Panel B Scores Dashboard
C Model Analysis View
Figure2:iScorevisualizesmultipleLLM-scoredwritingsamplestohelplearningengineersinterpretmodelperformance.Above,
alearningengineerinterpretshowtwoplagiarizedsummariesarescoredacrosstworuns(Sect.5).Userscanupload,scoreand
manuallyreviseandre-scoremultiplesource/summarypairssimultaneouslyintheAssignmentsPanel(A),visuallytrackhow
scoreschangeacrossrevisionsinthecontextofexpert-scoredLLMtrainingdataintheScoresDashboard(B),andcomparemodel
weightsbetweenwordsacrossmodellayers/heads,aswellasdifferencesinscoresbetweenautomaticallyrevisedsummary
perturbations,usingtwomodelinterpretabilitymethodsintheModelAnalysisView(C).
4 THEiScore SYSTEM 4.1 AssignmentsPanel
Basedonourdesignchallengesandusertasks,wedevelopediS- TheAssignmentsPanel(Fig.2A)providesmultiplesource/summary
core,aninteractivevisualanalyticstoolforlearningengineersto textinputsandmodelanalysisoptions.Thisstructureguidesusers
upload,evaluate,andvisualizetheresultsofautomaticallyscoring tocomparedifferentfacetsofsummarywritingintheScoresDash-
summarywritingusingLLMs.Oursystemhelpsuserscharacterize boardandModelAnalysisViewbychoosingbetweenmultiplemodel
qualityinwritingsamplesandenabletransparencyinLLMper- analysismethods.
formancebytightlyintegratingthreecoordinatedviews.Inthe Wescaffoldtheevaluationofsource/summarypairsinseveral
AssignmentsPanel(Sect.4.1),usersscaffoldtheevaluationprocess ways.Userscanchooseanynumberoflanguagemodelstheyhave
aroundcomparingmultiplesource/summarypairssimultaneously. trainedfromalisttocustomizehowtheyevaluateassignments
Afterevaluatingthepairs,theScoresDashboard (Sect.4.2)visu- andwhichmodelsarebeingcompared(T1).Wealsoprovidealist
alizestheprovenanceofscoresincontextofLLMtrainingdata, ofexamplesource/summarypairstohelpusersunderstandhow
whiletheModelAnalysisView (Sect.4.3)presentstwodifferent tostartinputtingsourcesandsummaries.Userscanthenprovide
LLMinterpretabilitymethodsatmultiplelevelsofabstraction. asinglesource,eitherthroughcopy/paste,upload,orbydirectly
typingandeditinginafreetextbox.Forthegivensource,users
canaddanynumberofsummarytextboxeswiththesameinput
capabilities(T2),withalistofmodelanalysisoptioncheckboxes
6iScore:VisualAnalyticsforInterpretingHowLanguageModelsAutomaticallyScoreSummaries IUI’24,March18–21,2024,Greenville,SC,USA
Scores Dashboard
discoverwhichsummariesareperformingbettercomparedwith
Rows represent runs (i.e., the othersacrossruns.Usersmayalsowanttounderstandhowsum-
user clicks the “Evaluate” button) maryscoresfromtheAssignmentsPanelcomparewiththeground
A Columns represent each of the truthmodeltrainingexamples(Sect.3.1.2),helpingthemgainad-
summary scoring LLMs chosen
in the Assignments Panel ditionalcontextintowhyaparticularsummaryreceivedacertain
score.Tofacilitatethis,wevisualizetrainingexamplescoresdi-
We group rows by summary
free text box, to track changes rectlyasascatterplotwithoverlappingbluedots(Fig.3B)(T3).We
in scores across revisions
thenplotthescoresofthecurrentsummariesintheAssignments
The light blue dots encode PCA Panelasyellowdotsoverlaidontopofthetrainingexamplescore
z-scores of training samples
B distributions(Fig.3C).Todigevendeeper,ourcollaboratorsre-
The yellow dots encode LLM questedtheabilitytore-scorethetrainingexamples,helpingthem
scores for each summary ID in
the Assignments Panel betterunderstandthedifferencesbetweentrainingandtestingon
As a summary ID accumulates thegroundtruthdataset.Inresponse,weallowuserstoclickon
runs, a dashed arrow is drawn in anydotinthescatterplottoloadthatsource/summaryexample
the direction of increasing run
intotheAssignmentsPanelandscoreit.Scoresaresavedandcan
number for that summary ID
beloadedontheflybyclickingonthecorrespondingdot(T7).
Users can hover on a dot to see
the summary, source, and scores Asusersexplorethehistoryanddistributionsofscores,wepro-
C Users can click on a dot to load videseveralinteractionsandusefuloverlays.Tohelpusersgain
the source/summary pair into additionalcontextandinsight,wevisualizetheprovenanceofsum-
the Assignments Panel and re-
score. Previous scores are saved maryscoresintheAssignmentsPanel byplottingeachrunofa
and can be retrieved this way summaryasadotnumberedbytherun(T6).Adashedarrowline
indicatesthedirectionthatthescoreismovingforthatsummary
Figure3:BreakdownoftheScoresDashboard.Thetableand betweenruns.Tosolveissueswithocclusion,wemadeeachtrain-
scatterplothelpuserscomparevariationsonsummariesby ingexampledotmostlytransparent,revealingpatternsintraining
trackinghowscoreschangeacrossmanualrevisions. examplescoredistributiondensitywheredotsoverlap.Wealso
explicitlyplotthedistributionsoftrainingexamplescoresasbar
charthistogramsalongthescatterplotaxes(T3).Toseedetails
foreachthatwillpopulatethevisualizationsintheModelAnalysis
on demand, users can hover on dots to reveal a snippet of the
Viewforeachsummarywithacheckboxselected(T8,T9).Because
source/summarypairtextforthatexampleaswellasthenumeric
eachmodelanalysisoptioncanbecomputationallyexpensive,users
scores.Theycanalsohoveronbarstoviewthebinsizeandrange.
cantogglewhichoptionstheywouldliketorunindependentlyfor
eachsummary.WeexploretheseoptionsindetailinSect.4.3. 4.3 ModelAnalysisView
Source/summarypairsarescoredinrealtimeusinganAPIthat
TheModelAnalysisView(Fig.2C)allowsuserstointerpretandgain
interfaceswithaPythonFlaskserverrunningPyTorchimplementa-
deeperinsightsintohowtheLLMsarescoringeachsource/summary
tionsofthelanguagemodels.WeusetheHuggingFaceTransform-
pairintheAssignmentsPanel.Wedothisbyleveragingtwobroadly
ers[62]APItoloadmodelsandperformsequenceclassification
effectivetechniquesforinterpretingdeeplearningmodelsatmul-
bycombiningeachsummarywiththesourceusingaseparator
tiple levels of text aggregation: (1) visualizing the results of in-
token.Userscanrapidlygeneratemultipletestcasesacrossseveral
putperturbationonmodeloutputsintheInputPerturbationplot
summaryvariationsforagivensourceandevaluatethemsimulta-
(Sect.4.3.1);and(2)visualizingattentionsoftransformer-basedlan-
neously,savingtimeandimprovingefficiency.
guagemodelsbetweentokensacrosseverymodellayerandhead
4.2 ScoresDashboard intheTokenAttentionplot(Sect.4.3.2).Wevisualizethesemethods
appliedtoasinglesource/summarypairatatime;userscanswitch
Afterscoringthesource/summarypairs,weprovideanoverview
betweenpairsusingadrop-downmenu.
ofmodelscoresforeachsummaryintheScoresDashboard(Fig.2B).
Wevisualizetheprovenanceofscoredatatohelpusersgaininsight 4.3.1 InputPerturbation. Perturbinginputparametersduringmodel
intohowchangesinsummariesaffectmodelscoresacrossrunsof inferencehasbeenwidelyusedtoexploretheinnerworkingsof
themodelatdifferentlevelsofaggregation. black-boxAImodels,particularlyinscenarioswherecomplexmodel
Eachtimetheuserscoresasetofsource/summarypairs,e.g., architecturesmakeitdifficulttotraceexplainabilitythroughthe
whenrevisingasummarytoseehowscoreschange,wevisual- entiremodelfrominputtooutput[33,41].Inthecaseofusing
ize the provenance of raw summary scores as a table (Fig. 3A) transformer-basedlanguagemodels,wecreatedfoursummarytext
(T6).Runsofthemodelsarenumbered.Rowsrepresentasingle perturbationoptionsintheAssignmentsPanel(T5).Thegrammar
source/summarypair,labeledbyrunandgroupedbysummary andwordsperturbationoptionsbothreplaceword-levelspansby
freetextbox.ColumnsdisplaythescoresforeachLLMchosenin fixingthespellingandusingsynonymsrespectively,whilethesen-
theAssignmentsPanel.Inthisway,wefacilitaterapidcomparison tencesandtokensperturbationoptionsmasksentence-leveland
acrossbothrunsforasinglesummary,aswellasacrosssummaries token-levelspansfromthesummaryrespectively.Afterautomati-
forthesamerun.Forexample,userscaneasilydetermineifscores callyperturbingeachsummary,weinferenceeachLLMrequested
are increasing/decreasing when a single summary is revised or intheAssignmentsPanelandvisuallycomparethenewsummaries’
7IUI’24,March18–21,2024,Greenville,SC,USA Coscia,etal.
Input Perturbation
spantotheunperturbedsummaryscore.Weuseasentencebound-
Token / word / sentence revisions are automatically applied to the summary arydetectiontokenizertolocateeachsentence-levelspaninthe
and re-scored. The revisions are underlined and colored by the difference in
summary,whileweuseeachLLM’stokenizerusedforinferenceto
score between the original summary and the revised summary. In the example
summary below, the original Content score is 0.682 extracttoken-levelspansthatcorrespondwiththetokensinthe
TokenAttentionplot(Sect.4.3.2).Aswiththewordsoption,we
Words - Replace words with synonyms using word tokenizer and WordNet
computeanewsummaryscoreforeachmaskedspan.
Finally,thegrammaroptionattemptstoautomaticallycorrect
Replacing people with mass
increases the score by 0.138 thespellingofeachsummary.Thisallowsuserstoquicklyexplore
Click on struck-out word to howthequalityofthegrammaraffectseachsummaryscore.We
reveal replacements on demand
Replacing Obtaining with receive firstperformacorrectionontheentiresummarythenqueryeach
decreases the score by -0.169
modelwiththecorrectedsummary,togetasinglescore.Weim-
True difference (diverging) plementedthreecorrectionvariationsforcomparison.Thefirst
Sentences - Remove sentences using sentence tokenizer
performssinglewordspellingcorrectionusinglookupsonindivid-
ualwordspansgeneratedfromatreebanktokenizer.Thisallowsus
Removing this sentence
decreases the score by -0.504 toaccuratelypreservethestructureofthesentence(i.e.thecasing
andpunctuation)butmaymisscontext-dependentspellingsugges-
tionsthatarelikelytooccur.Thesecondperformscompound-aware
Absolute difference (sequential)
Tokens - Mask tokens using LLM tokenizer multi-wordspellingcorrectionontheentiresummary.Incontrast
withsinglewordspellingcorrection,theaccuracyofrecognizing
andfixingmulti-worderrorsislikelytobehigheryetthestructure
ofthesentenceislostintheprocess.Thethirdattemptswordseg-
mentationtodividemulti-characterspansintotheirconstituent
True difference (diverging) words,preservingcasingbutignoringpunctuation.
Grammar - Correct spelling automatically using SymSpellPy
Original - Un-revised summary Visualizations. Tovisualizetheresultsofperturbation,weplot
theentiresummarytextinatextboxandunderlinetextspans
Single word correction - Preserves punctuation and casing, but does not understand multi-word errors
(token-,word-,orsentence-level)thatcorrespondwiththepertur-
Multi-word correction - Compound-aware, but removes punctuation and casing bationoptionschosenintheAssignmentsPanel(Fig.4).Wethen
encodethedifferencebetweeneachperturbedsummary’sscore
Word segmentation - Breaks apart words, keeps casing but loses punctuation
True difference (diverging) and the unperturbed summary’s score on a color ramp and ap-
plythecolortothetextunderlines(T4).Thisencodingscheme
Figure4:BreakdownoftheInputPerturbationvisualization.
allowsuserstoquicklyidentifyandcomparethelengthandeffect
Multipleperturbationmethodshelpuserstesthundredsof
sizeofeachspanintheperturbedsummarywiththeunperturbed
differentkindsofrevisionsatscalebyautomaticallyapply-
summaryscore.Userscanchoosebetweenared-to-bluediverging
ingandre-scoringsummariesforthem.
colorramprepresentingthetruenegative-to-positivedifferencein
scores,oratransparent-to-purplelinearcolorramprepresenting
theabsolutevalueofthedifferenceinscores.Wedisplayalegend
scoreswiththeoriginal,unperturbedsummaryscore(T4).These
fortheunderlinecolorsabovetheplot.Togetdetailsondemand,
perturbationoptionsmirrortypicalmethodsofevaluatingsum-
userscanhoveronanunderlinedspanatanyleveltogettheexact
marywritingsuggestedbyourdomainexpertcollaborators,and
differencevalues.Finally,weprovidedrop-downmenustoletusers
weredevelopedandtestedasaproofofconcept.
switchbetweenmodels,perturbationoptions,andcolorramps.
Data. Thewordsoptionreplacesallword-levelspansthatare Thesummarytextandunderlinesareplotteddifferentlybased
not English stop words or punctuation with synonyms in each ontheperturbationoption(Fig.4)(T5).Forthewordsoption,we
summary,givingusersanoverviewofwhichwordshaveastrong implementedawordreplacementviewcommoninmosttexteditors
effectoneachsummary’sscorewhenreplacedandifsemantically withmarkupcapabilities.Replacedwordshaveastrike-through
similarwordreplacementscouldimprovethescore.Weperform decorationandarefollowedbyellipses,denotingareplacement
replacementbyfirstidentifyingallword-levelspansusingatree- wasmadeforthatword.Thisgivesusersanoverviewataglance
banktokenizer,thenlookingupthesynonymsofeachwordand ofwhichwordshavehiddensynonymreplacementsthataffectthe
returningeachsynonym’slemma.Wordsarereplacedbytheirsyn- unperturbedsummaryscorethemost.Userscanclickonanyword
onyms’lemmasandanewsummaryscoreiscomputedforeach withreplacementstoshow/hidethatword’ssynonymsondemand.
lemma,resultinginalistofscoresforeachsynonymreplacement. Eachsynonymisshowninalistwithatextunderlinecoloredby
Boththesentencesandtokensoptionsapplymaskingtothe thedifferenceinsummaryscorewhenusingthatsynonyminplace
summaryatthesentence-levelandtoken-level,respectively.Mask- oftheoriginalword.Thereplacedwordisalsounderlined;the
ingspansattemptstoreplicatesaliencyinmodelinterpretability, underlineiscoloredbythemaximumsignedmagnitudeofscore
whichassignsascoretoeachinputbasedonthestrengthofits differences for all synonym replacements of that word. For the
contributiontothefinaloutput.IniScore,weusethedifferencein sentencesandtokensoptions,weunderlineeachsentence-level
scorewhenmaskingspansasaproxyfortheimportanceofthe andtoken-levelspanrespectivelywiththecolorrepresentingthe
8iScore:VisualAnalyticsforInterpretingHowLanguageModelsAutomaticallyScoreSummaries IUI’24,March18–21,2024,Greenville,SC,USA
Token Attention
tokens to4096tokens)forLongformers[2].Longformerassignsglobal
123... ...n
Transformers independently arrange heads in sequential layers. 1 attentiontooneormoretokensandaslidingattentionwindow
2
Heads compute weights called attention between all pairs of tokens 3
whichmovesacrossthetext.Tokensinglobalattentiongenerate
for the next layer. Longformers use a unique sliding attention window
to compute weights between tokens only inside the window. Global attentiontoeveryothertoken,whiletokensintheslidingattention
tokens are assigned attention between all other tokens, as usual. n windowonlygenerateattentiontoothertokenswithinthewindow.
Head 8 at each layer tokens Each head at Layer 1 tokens Thiscreatesissuesinvisualizingthesheernumberoftokensand
global global attentionsbothlocally(withinthecontextwindow)andglobally
window window (foralltokens),whilehelpingusersmakesenseofthedata.For
example,usinga256-tokenslidingcontextwindowaroundeach
token,asource/summarypairwith700tokens(≈500words,such
Pairwise attention at Layer 1 / Head 8 with the selected token at the slider position asa5-paragraphsourceand1-paragraphsummary)resultsina
global window
4-dimensionalmatrixof700×256×12×12≈26millionattentions
betweeneachtokenforamodelwith12layersand12headsin
Pairwise attention from selected token to underlined tokens
Weight (sequential) eachlayer.Ifweconsiderglobalattentionbetweeneverytoken,
thisnumberincreasesto700×700×12×12≈70millionattentions.
Attention weight from TextbooksourcesandsummariesiniTELLregularlyexceed4000
vitamin to people is 9.42e-5
tokens(> 150millionattentions).Becauseourmodelsinference
usingboththesourceandsummary,weconsidereddesignsthat
allowedustoscaleupto4096tokenattentionsvisualizedatonce.
No attention is computed
outside of the window
Visualizations. Tohelpusersdiscoverhigher-levelpatternsand
drilldownintospecificlayersandheads,weprovideanoverviewof
Attention weight from
vitamin to vitamin is 0.102 attentionfromtheselectedtokentoallothertokensusingheatmaps
(Fig.5)(T10).Weplottwodifferentheatmapsasuniformgridsof
allothertokensaroundtheselectedtokenarrangedvertically,and
eachlayer(i.e.theLayerheatmap)orhead(i.e.theHeadheatmap)
arrangedhorizontally.Cellsarecoloredbytheattentionweight
Figure5:BreakdownoftheTokenAttention visualization. fromtheselectedtokentotheothertoken(row)atthatlayerorhead
Thecombinationofheatmaps,rugplotandtextunderlin- (column).Ahorizontalsingle-endedrangesliderisalignedbelow
inghelpsusersmakesenseofcomplexmodelbehaviorsby eachheatmapandallowsuserstoselectaparticularlayer/head
keepingthemintheloopatmultiplelevelsofabstraction. combinationforviewinginthetextboxdescribedbelow.Asthe
rangesliderfortheLayerorHeadheatmapisdragged,theother
heatmapnotbeingdraggedwillupdate.Forexample,ifLayer2is
differenceinsummaryscorewhenthatspanwasmaskedduring selectedintheLayerheatmap,thentheHeadheatmapwillshow
inference.Thisallowsuserstoquicklyidentifyspanswithstrong anoverviewofattentionsacrossalltheheadsofLayer2.Thisallows
effectsonthefinalmodeloutput.Forthegrammaroption,we userstoquicklypanthroughlayersandheadstogaininsightsinto
plotandunderlinetheentiretyofeachcorrectionvariationofthe howattentionchangesinotherheadsandlayers,respectively.
unperturbedsummaryinasinglecolorrepresentingthedifference Belowtheheatmaps,weallowtheusertodrilldownandseeall
inthecorrectedsummary’sscore.Userscanthencomparechanges pairwiseattentionsbetweenaselectedtokenandallothertokensat
ingrammarandscoresside-by-sidebetweencorrections. thecurrentlayer/headselectedintheheatmaps(Fig.5)(T11).Todo
this,wepositionarugplotaboveahorizontalsingle-endedrange
4.3.2 TokenAttention. Complementingourapproachofevaluat-
sliderthatrepresentstheindexoftheselectedtoken,andvisualize
ingmodelsexternally,wealsovisualizetheinternalmulti-headed
theattentionweightfromtheselectedtokentoallothertokensat
attentionmechanismsunderlyingourtransformer-basedlanguage
eachindex.SimilartotheInputPerturbationplot,wethenplotboth
models.Byvisuallyconnectingattentiontothesemanticandsyn-
thesourceandsummarytextinatextboxandunderlineeachtoken
tactic structure of our source/summary pairs, we aim to reveal
(T12).Eachtokenunderlineiscoloredbytheattentionweightfrom
patternsandassociationsinthetextthatarestronglycontributing
theselectedtokentothattoken.Inthisway,weaddressissuesof
tothefinalmodeloutput.Thiscanhelpusersgaindeeperinsights
scaleasthenumberoftokensincreasesbyleveragingbasictext
intohowdifferenttrainingproceduresandtrainingdataleadto
formattingtohelpusersreadthetextnaturallywhilecomparing
prioritizingdifferentspansbetweenlayers/heads,andcomparehow
attentionweightsatthesametime.Userscanmanuallyclickon
attentionsandtheseprioritieschangebetweenmodels.
anytokeninthetextboxtoupdatetheheatmaps,rugplot,andtext
Data. Priorwork[54,59]hassuccessfullyexploredusinggraph underlineswithupdatedattentionweightsfromthenewlyselected
and matrix visualization techniques for visualizing pairwise at- token.Theycanalsousetherangeslidertoquicklypanthrough
tentionbetweentokensformodelswithsmallersequencelengths tokensandseepatternsatthedataset-level.
(< 256tokens).However,amajorchallengeinscalingattention Weprovideseveralinteractionsandvisualembellishments.A
visualizationforourmodelsisthesizeoftheslidingcontextwin- singlelineartransparent-to-purplecolorrampisusedtorepresent
dow(256or512tokens)comparedtotheinputsequencelength(up attentionweightfortheheatmaps,rugplot,andtextunderlines.
9
...
...IUI’24,March18–21,2024,Greenville,SC,USA Coscia,etal.
Whereattentioniszero(notmissing)betweentokens,wecolorcells, plottoloaditintheAssignmentsPanel(T3,T7).Theyapplythe
strips,andunderlinesinbrightredtomaketheseoutliersvisually samereorderingandrephrasing,andfinditproducesasimilarscore
distinct. We display a legend for the heat maps above the plot. pattern.Byunderstandingtheeffectsofreorderingversusrephras-
Becauseourmodelsuseaslidingcontextwindow,inallplotsthe ingonmodelperformanceusingiScore,thelearningengineercan
attentionweightsfortokensoutsideofthecontextwindowarenot augmenttrainingsamplesandre-trainnewmodelversions.Once
plotted,andattentionsfromtheselectedtokentoallglobaltokens theytrustthere-trainedmodelsarerobust,theycanconfidently
arealwaysshown.Inthetextbox,globaltokenshaveagolden warnlearnersthatthemodelsareawareofthesemethodsandthat
stroke applied and the selected token has a red stroke applied, summariesshouldbewrittenfromscratchelsetheywillfail.
tovisuallydistinguishthemfromothertokens.Togetdetailson Thelearningengineerwantstofollowupontheirintuitionthat
demand,userscanhoveroveranycell,strip,ortokenandgetthe certainwordsbeingrephrasedorsentencesbeingremovedleadsto
tokentextandexactattentionweightvalue.Finally,weprovide changesinscore.However,itwouldbetime-consumingandineffi-
drop-downmenustoletusersswitchbetweenmodels. cienttomanuallyreplaceeverywordandremoveeverysentence,
andcertainreplacementscouldbemissed.Usingthelatestrevised
4.4 Implementation versionsofthesummaries,theychoosethesentencesandwords
iScore isanopen-source2 webappbuiltusingD3.js[5].Weim- perturbationmodelanalysisoptionsintheAssignmentsPanelfor
plementedourinputperturbationmethodsinPythonusingsym-
eachsummary(T5,T8).ViewingthesentencesoptionintheInput
spellpy3forrevisinggrammarandNLTK’s[4]WordNetinterface Perturbation,theyaresurprisedtoseethatthefirstsentencetends
forwordsynonymreplacementandsentencemasking.iScoreuses
todropthescoredramaticallywhenremoved(T4).Ourcollabo-
HuggingFaceTransformersAPI[62]toloadLongformerpre-trained ratorsexpandonthepotentialreasonforthisresultintheexpert
modelsandprocessallsource/summarypairsinrealtime. feedbackinSect.6.2.Switchingtothewordsoption,theengineer
findsthatmorecomplexsynonymsgenerallyincreasetheContent
5 USAGESCENARIO score,indicatingapotentiallearnedassociationbetweenwordcom-
plexityandscore.Forthepurposesofthesummaryassignment,this
Alearningengineerismanuallyrevisingtwoplagiarizedsummaries
isconsideredapositivelearnedassociation.However,theythen
ofatextbooksourceiniTELLandcomparingtheirLLM-assigned
checktheWordingmodelandfindthesamepatterndoesnothold;
scoresusingiScore.Theyseekto:(1)seehowtheycantrickthe
synonymsnotfoundinthesourcetendtolowertheWordingscore.
modelsintogivingthesummarieshigherscoresbyreorderingand
Thisissurprising,sinceWordingconsistsofrewardingsummaries
rephrasingsentences;and(2)determineifmodifyingorremoving
thatgobeyondthesourcetext.Theengineermakestwodecisions:
certainphrasessuchaskeywordsandmainideaswillaffectmodel
(1)re-trainthemodelsbyexposingthemtomorecomplexsum-
scores.Theyhopetousetheirfindingstodevelopfeedbackfor
maries;and(2)givelearnersfeedbackthattheyshouldgenerally
learnerswritingsummariesoftextbooksectionsonhowtoimprove
useavarietyofvocabularythatgoesbeyondthesourcetext,but
scores,aswellasgenerateadditionaltrainingexamplesthatensure
thatdoesnotchangethemeaningoftheirsentences.
themodelsareresilienttolearnerstryingto“game”thesystem.
TheystartbyusingtheAssignmentsPaneltouploadasourcetext
6 EVALUATION
andtwosummaries(T2).Whiletheyexpectthatthemodelswill
scoreplagiarizedsummarieslowonbothContentandWording, OurgoalforevaluationwastovalidatetheeffectivenessofiScore
theyareunsurehowresilientthemodelsaretoreorderingand forhelpinglearningengineersunderstand,evaluate,andbuildtrust
rephrasing,andwhetherchangesinContentandWordingscores inhowtheirlargelanguagemodels(LLMs)automaticallyscore
will differ (T1). In the first run, they score the plagiarized text summarywriting.Todothis,wefirstdeployediScorewiththreeof
verbatim,resultinginnegativescoresthatindicatethesummaries thelearningengineers(domainexperts)withwhomwecollabora-
arebelowaverage.Inthesecondrun,theyreorderthesentences tivelydesignediScoreoverthecourseofamonth.Wefirstdescribe
withoutmodifyingwords.ThehistorytableintheScoresDashboard a case study detailing how iScore enabled one of the engineers
showsaslightincreaseinbothscoresbetweenruns,yetthescores onourteamtoimprovetheaccuracyofLLMsusediniTELLby
arestillnegative,agoodsign(T6).Theytryathirdrevision,by threepercentagepoints(Sect.6.1).Wehighlightwhichchallenges
paraphrasingeachsentencewithoutchangingtheorderofsen- andtasksraisedinSect.3.2andTable1theyachieved.Wethen
tences.Interestingly,thedashedarrowlinesinthedistributions conductedindividualsemi-structuredinterviews(1−2hourslong
scatter plot show a stronger increase in Content over Wording. each)withallthreeengineersonourteamandcollectedfeedback
Finally,theyreorderandparaphrasethesummariestotestthecom- [43]onthewaysiScoregenerallyimpactedhowtheyunderstood,
binedeffects.Thistime,ContentincreasesfurtherbutWording evaluated,andtrustedtheLLMsusediniTELL(Sect.6.2).
actuallydecreases.IfparaphrasingaloneimprovesbothContent
andWordingwhilereorderingandparaphrasingonlyimproves 6.1 CaseStudy
Content,thelearningengineerworriesthatlearnersmaybeableto
OneofthedomainexpertsonourteamthatwedeployediScore
paraphrasesummariesusingparaphrasingtoolslikeChatGPTthat
with and interviewed is a learning engineer who has expertise
couldpass.Toverifythispattern,theengineerclicksonatraining
in natural language processing (NLP). He is fairly new to deep
source/summarypairwithsimilarscoresinthedistributionsscatter
learningandwantstoimprovetheLLMsdeployediniTELLfor
2iScoremodelsandcode:https://github.com/AdamCoscia/iScore automaticallyscoringsummarywriting.Todothis,hetrainsseveral
3symspellpycode:https://github.com/mammothb/symspellpy LLMsusingvariouscombinationsofparametersandevaluatestheir
10iScore:VisualAnalyticsforInterpretingHowLanguageModelsAutomaticallyScoreSummaries IUI’24,March18–21,2024,Greenville,SC,USA
performanceonsamplewritingbycomparingscoresoneatatime. (1) Understanding LLMs – “How did iScore improve your
Heisinterestedin“openingtheblackbox”oftheLLMsandfeels understandingofLLMperformance?”
themostimportantuseofiScoreishelpingengineersconfirmthat (2) EvaluatingLLMs–“Howwerethevisualizationsandin-
theirmodelisdoingwhattheywantittodo:“Youdon’tseethe teractionsiniScoreusefulforinterpretingLLMs?”
modelworking.Youseteverythingupascarefullyasyoucanbutitis (3) TransparencyandTrustinLLMs–“HowdidiScoreenable
alldoneintheGPU.It’seasytomakeamistake,allowinformation transparencyandpromotetrustinusingLLMs?”
leaks,andthereisnothingto‘see’.Youdon’tknowwhereyoumight
havemessedup.” (1)UnderstandingLLMs. Wefirstinquiredaboutthemajorchal-
Duringdeployment,heusedtheTokenAttentionvisualization lengestheexpertsfaceinunderstandingLLMs.Wefoundseveral
toinvestigatetherelationshipbetweenthesummaryandsource recurring themes: understanding how decisions are made from
text.LookingattheContentmodel,henoticedthattheattention billionsofLLMparameters;investigatingwhetherLLMsusecon-
weightstendedtoconvergeinthefinalmodellayerstohighvalues textaroundwords;and“seeing”whatisgoingoninsidemodels
onlyinasmallradiusaroundaselectedtoken(i.e.,darkerpurple usingtraditionalprogrammingtools.Therewerealsoarchitecture-
underlinesfadinglinearlytolightpurplefortokensfartheraway) specificchallengesrelatedtoLongformers,asoneexpertexplained:
(T9,T10).ThisdidnotmatchhismentalmodelofwhattheLLM “SinceweareusingLongformerwiththeslidingattentionwindow,
shouldbedoing.Tokensinthesourceshouldpayattentiontothe therearedeveloperdegreesoffreedomthatcanbetweakedcompared
entiresummaryineachlayer,evenwhentheyarefarawayfromthe withothermodels.”Thesewereimportanttoaddressforseveralrea-
summary.HewonderedwhethertheLLMwasusingallavailable sons.Oneexpertwasconcernedwiththelong-termreproducibility
informationtodeterminethefinalscore.Hequicklycomparedthis oftheirfindings,feelingthat“thereisalwaysanxietyortension
qualitativepatternwithothersource/summarypairsaswellaswith aroundwhetheranoutcomeisjustafluke.” Anotherdescribedhow
theWordingmodelusingtheAssignmentsPanelandconfirmedthe addressingthesechallengescanalsoleadtonewresearchquestions:
sameinsight.Herealizedthat,“bydefault,onlytheclassification “Sometimeswecouldn’texplain[findings]withthevisualizations,but
tokenreceivesattention.Byusingthe[tokensliderinthe]attention itledustoasknewquestionsaboutourresearch,anditservedto
visualization,Iwatchedtheattentionwindowslideacrossthetext.It remindusthatwedon’talwaysknowhow[theLLMs]areworking.”
wasobviousthatonlyonetokenwasgettingglobalattention.” (T11, iScore enabled the experts to learn several new things about
T12)Theinsightgavehimanideatoimprovethemodel,byas- theirmodels.UsingtheTokenAttentionvisualization,oneexpert
signingglobalattentiontotheentiresummaryduringtrainingand discoveredthatpunctuationtendedtocontainalotofinformation
inference.Afterretraining,hefoundtheContentscoreimproved infinalmodellayers,asthemarkswerealwaysunderlinedindark
significantly,from𝑟2 =0.79to𝑟2 =0.82.Hereflectedonthisim-
purple.Consequently,removingpunctuationwouldcausescores
provement,saying“It’sasimplething,buttheattentionvisualization forbothContentandWordingmodelstogodowndrastically.They
confirmsthatthemodelnowdoeswhatthemodelwassupposedtodo! wereconcernedwiththeramificationsofthisfinding:“Wedidn’t
IgainedanimportantinsightusingiScorethatchangedthemodel.It expecttoseeattentionweightsonpunctuation...Thequestionis,does
wasactionableinformationthatledtoabettermodel.” thishaveanypedagogicalimpact?” Anotherexpertfoundthatre-
Overall,thelearningengineerovercamethechallengesoutlined movingthefirstsentencefromsummarieswoulddropscoresby
inSect.3.2byusingiScoretodrawarelationshipbetweensum- nearly90%.Theyhypothesizedthatthemodelswerefocusingon
maries and scores (C1), uncover discrepancies between models thebeginningofthesentenceandthepunctuationdenotingthe
anddifferentlyscoredsummaries(C2),deeplyinvestigateerrant endofthefirstsentence,astheclassificationtokenisatthestartof
modelparameters(C3),andfluidlyinteractwiththevisualizations theinput.Thismaycorroboratethefindingsfromourcasestudy,as
totransitionbetweenglobalandlocalmodelbehavior(C4). theretrainedmodelswithglobalattentionontheentiresummary
didnotexhibitthisbehaviorasstrongly.Theexpertreflectedon
thisbehavior,saying“While[topicsentences]couldbeimportantfor
6.2 ExpertFeedback
writingsummaries,itmaynotbethewaythemodelisoperating.”
Ourthreedomainexpertswereateamofgraduatestudentsbroadly
interestedincomputationalworkattheintersectionoflinguistics, (2)EvaluatingLLMs. Todiscovertheseinsights,theexpertsfelt
learningsciencesandNLP.Theseexpertswerethesamecollabora- theInputPerturbationandTokenAttentionvisualizationsallowed
torsinvolvedinthedesignprocessandcarriedoutthedeployment. themtoseehowthingsworkedatahigherlevel.Oneexpertsum-
SomeoftheexpertscontributedtothedevelopmentoftheiTELL marizedhowthishelpedthem,saying“wenowhavemethodstohelp
framework,andallhadexperienceworkingwithiTELL.Allstudied usshowthatthemodelsworkasweexpect,andalsowhenwedon’t
ortaughtlanguageandNLPinvariousways,includingworkinlin- expectresults.” Forexample,thesameexpertdescribedhowthey
guistics,languagelearning,development,andteaching,andusing usedtheAssignmentsPanelandInputPerturbationvisualizationsto
LLMstopredicttextreadability,readingcomprehension,personally createsummaryvariationsandanalyzetheirmodels:“Iwasusing
identifiableinformation,andbuildotherlearningtools.Allthree inputperturbationstoillustratehowadversarialattackscantrick
expertsself-identifiedasmale.Weorganizedtheinterviewques- models.Thisledmetonewresearchquestions.Theperturbationsare
tionsanddomainexperts’feedbackaroundthreebroadquestions onewordatatime.Whatifwedon-gramchanges?” Yettherewas
wedirectlyaskedeachcollaborator.Wethenconductedinductive roomforimprovement,asanotherexpertsometimeshadtrouble
thematicanalysis[7]ofthefeedbacktoidentifyemergentthemes knowingwhattolookatintheTokenAttentionvisualization:“There
thatwerediscussedamongstallauthors. seemstobeaconsistentnoiseintheattention.Whatissignificantover
11IUI’24,March18–21,2024,Greenville,SC,USA Coscia,etal.
andabovethenoise?” Inthefuture,iScorecouldincludestatistical 7 DISCUSSION
analysissuchashighlightingattentionsfaroutsideofthemean.
7.1 ImplicationsForDesign
Astheyexplainedtheirprocessfordiscoveringinsightswith
Bydistillingthefindingsfromourevaluation,wesynthesizedde-
iScore,twooftheexpertsdevelopednewideasforhowthevisual-
signimplicationsforfuturevisualanalyticssystemssituatedin
izationscouldhelpthemclosetheloopofmodeldevelopment.One
termsofourkeydesignchallenges(C)outlinedinSect.3.2aswell
usedtheModelAnalysisViewtoquicklyprofileanddecidebetween
aspriorliterature.
thebest-performingmodels: “Ioftentrainmultiplemodelsfora
taskandwanttocomparethem.WhatIliketodoisswitchbetween StructureLLMevaluationusingvisualhierarchies.Weob-
themodelsusingthedrop-downs.” Topromotereproducibilityin servedthatcommunicatingtheLLMevaluationprocessthrough
thefutureandcommunicatetheirfindingsdirectlyintheiScore visualhierarchiesenabledthoughtfulcomparisonbetweenLLM
interface,theyrequestedhypothesistestingcapabilitiessuchas inputsandoutputs,improvedunderstandingofhowmodelswork
achi-squaredtest.TheotherexpressedadesiretousetheScores andincreasedtrustthatmodelsareworkingcorrectly.Toscaffold
Dashboardfortrackingchangesacrossaspecifictask,suchaskey comparison, users should be able to easily upload and/or write
phraseanalysis:“Itmaybetooeasytojustthrowinthekeyphrases multipletextinputstotheirLLMsatonce(C1).Textinputsshould
andincreasemodelscoresandwedon’twantthat.” Thistypeofanal- bearrangedtovisuallycommunicatecomparison(e.g.,groupingby
ysiscouldgenerateadditionalexamplesforinjectingintotheLLM topic,version,author,etc.)andincludeoptionsforautomatically
trainingdata,tomakethemrobusttolearnerstryingto“game”the testingmultiplemodelssimultaneously(C1),savinguserstimeand
systembyspammingtheinputwithkeyphrases. aidingtheirsensemakingprocess.Further,wefoundthatvisualiz-
ingthedistributionofLLMoutputsacrossgroupsofmodels,inputs
andrevisionshelpeduserscommunicatevaluableinsightswhen
(3)TransparencyandTrustinLLMs. Theexpertsallreported reportingtheirfindings.Onewaytoenrichvisualcomparisonis
differentfactorsandrequirementsforbuildingtrust.Reproducibil- visualizingdistributionsincontextofthe"groundtruth"model
itywasarequirementforonelearningengineer,astheywantedto trainingdata(C1).Anotherwayistogiveusersoptionsforgroup-
avoidunexpectedsurprisesespeciallywhendeployingtheirLLMs: ingandvisualizingmultipletypesofautomatictextrevisions(C2).
“Youmightgetgreatresults,andsometimestheylooktoogoodandyou Finally,ouruserssuggestedvisuallyhighlightingandgrouping
havetogobackandcheckyourcode.” Addressingbiasesprimarily potentialrelationshipsbetweenalltokensusingstatisticaltests
inthetrainingdatawasmentionedbyanother,thoughtheexpert (C3,C4).WediscussfutureworkinthisareainSect.7.4.
alsonotedthedifficultendeavorofexplainingthevastnumberof
ScaleLLMinterpretabilitymethodstolargeinputs.Supporting
LLMparametersresponsibleforproducingascoreasabarrierto
analysisofmultiplelargetextinputsbecamebothacomputational
realizingtransparency.Anotherexpertbelievedinclearlydemon-
andmentalbottleneck,highlightingthechallengeofscalinginter-
strating model limitations and edge cases, especially given the
pretabilitymethodsasLLMsaredevelopedthatcanprocesslarger
abilityforLLMstohallucinate:“Ifwearedeadsetonfoolingmodels,
inputs[63].Leveragingfluidinteraction[15]canhelpusersidentify
wecancreateexamplestofoolthemodels.” Finally,thecontextof
andtracksetsoftokensthatamodelisfocusingonandcompare
deploymentwasconsideredimportantforallexperts,asoneexpert
thesesetsagainstothertokensatmultiplelevelsofaggregation.
explained:“Convincingstakeholdersbecomeseasierifyoucanshow
Forexample,usersshouldbeabletovisualizeandcomparediffer-
themexamplesofhowthemodelisworking.”
encesbetweentextinputsatmultiplelevelsofaggregation(e.g.,
Theexpertsunanimouslyagreedthatthemostusefulfeature
token,word,sentence)aswellasinteractivelyswitchbetweenlev-
ofiScoreforinspiringtrustwastheInputPerturbationvisualiza-
elsonthefly(C2,C3).Interactivelyspecifyingencodingsforhow
tion.Oneexpertfeltthat,“inaway,inputperturbationallowsyou
changestotextspansaffectLLMoutputsusinginlineannotations
toputinadversarialexamplestotestyourtrustinthemodel.” At
(e.g.,underliningtokens,words,sentences,etc.)allowsuserstoread
thesametime,anotherexpertcommentedthattheTokenAtten-
textnaturallyatscalewhilefluidlyidentifyingpatternsbetween
tionvisualizationdidnotachievecompletemodeltransparencyfor
individual and groups of annotations, even as inputs grow. We
them.Followingup,theystillexpressedoptimismthat“trustcan
foundthistobeusefulbothwhenexternallyprobingmodelssuch
beachievedevenwithoutfulltransparency”,especiallywhenusing
asinputperturbationandwhenanalyzinginternalmodelweights
theAssignmentsPanelandInputPerturbationvisualization:“The
(C3).Asinterpretabilitymethodscanbecomputationallyexpensive
aspectsofiScorethatcantestvariationsallowustodemonstratewhat
forlargenumbersoftokens,usersshouldalsobeabletosubset
happenswithchangesinsummaries.Thiscanbroadlyallowusto
andapplymethodsbothsimultaneouslyandindependentlytoany
improvetrustinthesesystems.” Thisbenefitextendedoutsideofthe
numberofinputs(C1).Resultsetsshouldthenbeeasytoswitch
developmentenvironment,allowingtheexpertstoshowtheirwork
betweenonthefly,allowinguserstoquicklyvisualizeandcompare
incontextofdeploymentwithstakeholders.Ontheoppositeendof
differenceswithoutslowingdowntheuserexperience(C1,C3).
thespectrum,fordevelopersdeepintheNLPloop,oneexpertfelt
iScoreprovidedalayerbetweendevelopmentandproductionwhere Potentialfutureapplications.Takentogether,thesedesignim-
fewtoolsexisttoprovidequalitativefeedbacktolearningengineers plicationsinspirefutureapplicationsofvisualanalyticsforinter-
attheintersectionofeducationaldataandmachinelearning.For pretingLLMs.Forexample,consideranengineertrainingLLMs
them,iScorepromotedtrustthroughreproducibilitybecausethepa- foraquestion-answermodule,oraNLPresearcherinvestigating
rametersofeachLLMcanbequicklyiterated,tested,andreported LLMstrainedtoassignareadabilityscore.Theycouldimporttheir
usingscreenshotsofthesystem. modelsintoiScoretointerpretwhatpartsofthetextthemodelsare
12iScore:VisualAnalyticsforInterpretingHowLanguageModelsAutomaticallyScoreSummaries IUI’24,March18–21,2024,Greenville,SC,USA
payingattentiontousingtheTokenAttentionvisualization,aswell RoBERTa,iScorecanbeusedtointerpretanytransformer-based
ashowthemodelsrespondtodifferentperturbationsoftheinput modelthatutilizesaseparatortokenforsequenceclassification,
textusingtheInputPerturbationvisualization.Developerscould suchasareferencetextandasummaryoraquestionandanan-
alsodeploytheperturbationfeatureswithuserstocollectfeedback swer,buildingonthesuccessesofothertransformer-basedLLM
forretrainingtheirmodels,byanalyzinghowinputsarerevised architectures.Thisincludesbothencoder-anddecoder-onlymodels
andhowscoreschangeasusersrespondtoseeingmodelscores. aswellasencoder-decodermodelssuchasGPT-3[8]andT5[39].
Toenablelargerinputtextsizes,StreamingLLM[63]isarecent
7.2 ResponsibleandEthicalAIforEducation frameworkthatcanreplicatethelargerinputtextsizecapabilities
ofLongformersusingattentionsinksformodelssuchasLLaMa
ThescalabilityandgeneralizabilityofLLMscouldenablesummary
[51].Finally,tomakeourmethodsaccessibletolearningengineers
writingassignmentstobeusedinawidervarietyofenvironments,
andNLPresearchersinthefuture,weusetransformersdownloaded
includingMassiveOpenOnlineCourses(MOOCs),learningplat-
fromtheHuggingFaceTransformers[62]modellibrarythatare
formsandintelligenttextbooks[18].Torealizethis,thereareseveral
open-sourceandlightweightenoughtorunonasingleGPU.
broadpracticalandethicalconsiderationsofusingmachinelearn-
ing(ML)andartificialintelligence(AI)thatiScoreaddresses,helping
7.4 LimitationsandFutureWork
learningengineersdeployresponsibleAIineducationaltoolslike
iTELLandmitigatethepedagogicalrisksofusingAI-poweredtools Severalanalyticallimitationswereexpressedduringtheinterviews.
incriticallearningenvironments[27]. Whileourcollaboratorswerepositiveaboutmovingtowardsusing
Itisnecessarytoremovepotentialbiasesandevaluatethefair- visualizationsthatshowmodelweights,theystillhadissueswith
nessofanyAImodel’spredictions.Ensuringfairnessmeansusing interpretingthem.Onefeltthattherewasaconstantnoiseinthe
balanceddatasetsduringthetrainingperiodaswellasoversight perturbationscoresandattentionweightsthatmadeitdifficult
andregularre-trainingbasedontheperformanceofAImodelsin toidentifyimportantpatternsandrelationships.Interactionsat
production.UsingiScore,learningengineerscanrapidlyexplore specificlayers/headswerealsodifficulttoexplain,asoneengineer
andidentifypotentialbiasesthatmanifesteitherinternallyinthe dislikedscanningacrosslayers/headstoperforman“axiomatic”
modelweightsorexternallyinthemodelscores.Forexample,they typeofanalysis.Whileattentionvisualizationsenabledourcollab-
couldtrackpatternsofbiasesinscoreassignmentsacrossrevisions oratorstoimprovemodelunderstandingandaccuracyinSect.6.1,
containingdifferentpersonalidentityphrasessuchaspronouns. thereison-goingdebatewhetherattentionweightsintransformers
AI-poweredtoolsforeducationshouldalsoaccommodatethetarget canbeusedasareliablesourceofinterpretationformodelper-
languageoflearnersbyprovidingbothequitableandinterpretable formance[1,11,26,61].Severalengineerssuggestedaugmenting
outputs.ThiscanbeachievedbyexpandingLLMtrainingacrossa iScore withstatisticalteststhatcouldimprovehowtointerpret
varietyofdatasetsinvariouslanguages,andusingtheinputand andcompareattentionsandperturbations.Thesecouldinclude
historytrackingfeaturesofiScoretoevaluateLLMperformanceat hypothesistestssuchasachi-squaredtesttocomparedistributions
scaleacrossmanydifferentsummaries.Finally,perceptionsofAI ofattentionweightsbetweenmodelsorascoreoutlierdetection
areanimportantfactorforeducatorsintheadoptionofAI-powered methodusinginterquartileranges.Further,ourcollaboratorsfound
educationtechnologies,withtrustbeingoneofthemostimportant theinputperturbationmethodstobethemostuseful,yetthiswas
determinantsofwhetheranAImodelisactuallyused[44].Oneof alsothemostcomputationallydemandingmethod,whichcanlimit
thegreatestbarrierstobuildingtrustisalackoftechnicalexpertise theresponsivenessandincreasedeploymentrequirementsforthis
amongeducatorsinunderstandingwhatAIiscapableof.Webe- feature.Onepromisingdirectiontoemulateperturbationisper-
lieveiScorecontributespracticalmethodsforenablingtransparency forminggradient-basedattributionofinputfeaturestothefinal
andhelpingnon-ML-expertsbuildtrustindeployingAImodelslike modelprediction[49],bydevelopingnewmethodsspecificallyfor
LLMsintoolslikeiTELL.Forexample,theperturbationfeaturesof theslidingattentionwindowinLongformermodels.Perturbation
iScorecouldbeadaptedandsimplifiedforpresentationtonon-ML- alsoonlyconsidersthemarginalcontributionofatoken/sentence
expertaudiences,suchasinstructorsandlearners,asahands-on amongtheremainingones,ignoringinteractionsamongfeatures
versionthatseekstopromotetrustinusingAI-powerededuca- (i.e.,token/sentence).UsingShapleyValues(e.g.,SHAP[33])may
tionaltechnology.Inthefieldsofeducation,machinelearningand beamoreaccurateapproach,albeitatthecostofincreasedcom-
HCIbroadly,itisimperativetodeveloptoolslikeiScorethatenable putationaltimeoverthealreadyexpensiveperturbations.Finally,
engineerstobuildethicalandresponsibleAItoolsandinterpret one collaborator requested a method for performing automatic
themwellenoughtobeginbuildingtheessentialtrustnecessaryto sentence-levelreplacement,toinvestigatehowthiscouldimprove
seethemimplementedsafelyinlearningenvironments. writingfeedbackinthecontextofautomaticscoring.Thiscould
beachievedbyusingadditionalLLMstogenerategrammatically
7.3 LLMGeneralizability and/orsemanticallysimilarsentencesforreplacement.
Inthispaper,wedemonstratedthecapabilitiesofiScoreusingLong-
8 CONCLUSION
former,apre-trainedLLMbasedonRoBERTa,asLongformerallows
forlongertextinputsthatcombinesummarywritingandsource Aseducationaltechnologiesincreasinglyutilize“blackbox”deep
textduringinference.RoBERTaitselfisafundamentallanguage learningmodels,itiscriticaltoempowermodeldeveloperstoun-
modelthatcontinuestoobtainstate-of-the-artresultsonclassi- derstand,evaluate,andbuildtrustintheirmodelsbeforedeploying
ficationandnaturallanguageunderstandingtasks[46].Beyond themincriticallearningenvironments.Inthiswork,weconducted
13IUI’24,March18–21,2024,Greenville,SC,USA Coscia,etal.
auser-centereddesignprocesswithateamoflearningengineers [12] ScottA.Crossley,MinkyungKim,LauraAllen,andDanielleMcNamara.2019.
whotrainanddeploylargelanguagemodels(LLMs)thatautomati- AutomatedSummarizationEvaluation(ASE)UsingNaturalLanguageProcessing
Tools.InArtificialIntelligenceinEducation,SeijiIsotani,EvaMillán,AmyOgan,
callyscorewrittensummariesofsourcetexts.WepresentediScore,
PeterHastings,BruceMcLaren,andRoseLuckin(Eds.).SpringerInternational
avisualanalyticssystemforlearningengineerstoupload,com- Publishing,Cham,84–95. https://doi.org/10.1007/978-3-030-23204-7_8
pareandvisualizemultipleLLM-scoredsummaries.Bystructuring [13] JosephF.DeRose,JiayaoWang,andMatthewBerger.2021. AttentionFlows:
AnalyzingandComparingAttentionMechanismsinLanguageModels. IEEE
evaluationofLLMperformancearoundcomparisonofmultiple TransactionsonVisualizationandComputerGraphics27,2(2021),1160–1170.
source/summary pairs, visualizing the provenance of summary https://doi.org/10.1109/TVCG.2020.3028976
[14] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.BERT:
scoresincontextoftheLLMtrainingdata,andpresentingdifferent
Pre-trainingofDeepBidirectionalTransformersforLanguageUnderstanding.In
LLMinterpretabilitymethodssimultaneouslyatmultiplescales, Proceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociation
iScoreincreasesLLMtransparencyandtrustthroughouthuman-in- forComputationalLinguistics:HumanLanguageTechnologies,Volume1(Longand
ShortPapers).AssociationforComputationalLinguistics,Minneapolis,Minnesota,
the-loopevaluationofLLMperformance.
4171–4186. https://doi.org/10.18653/v1/N19-1423
[15] NiklasElmqvist,AndrewVandeMoere,Hans-ChristianJetter,DanielCernea,
ACKNOWLEDGMENTS HaraldReiterer,andTJJankun-Kelly.2011. Fluidinteractionforinformation
visualization.InformationVisualization10,4(2011),327–340. https://doi.org/10.
ThismaterialisbaseduponworksupportedbytheNationalScience 1177/1473871611413180
[16] ScottR.Emmons,RobertP.Light,andKatyBörner.2017.MOOCvisualanalytics:
FoundationunderGrantNo.2247790andGrantNo.2112532.Any
Empoweringstudents,teachers,researchers,andplatformdevelopersofmas-
opinions,findings,andconclusionsorrecommendationsexpressed sivelyopenonlinecourses.JournaloftheAssociationforInformationScienceand
inthismaterialarethoseoftheauthor(s)anddonotnecessarily Technology68,10(2017),2350–2363. https://doi.org/10.1002/asi.23852
[17] DavidGalbraithandVeerleM.Baaijen.2018.TheWorkofWriting:Raidingthe
reflecttheviewsoftheNationalScienceFoundation.
Inarticulate.EducationalPsychologist53,4(2018),238–257. https://doi.org/10.
1080/00461520.2018.1505515
REFERENCES [18] DilrukshiGamage,ThomasStaubitz,andMarkWhiting.2021.Peerassessment
inMOOCs:Systematicliteraturereview.DistanceEducation42,2(2021),268–289.
[1] PepaAtanasova,JakobGrueSimonsen,ChristinaLioma,andIsabelleAugenstein. https://doi.org/10.1080/01587919.2021.1911626
2020.ADiagnosticStudyofExplainabilityTechniquesforTextClassification. [19] Germain Garcia-Zanabria, Daniel A. Gutierrez-Pachas, Guillermo Camara-
InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguage Chavez,JorgePoco,andErickGomez-Nieto.2022. SDA-Vis:AVisualization
Processing(EMNLP).AssociationforComputationalLinguistics,Online,3256– SystemforStudentDropoutAnalysisBasedonCounterfactualExploration.Ap-
3274. https://doi.org/10.18653/v1/2020.emnlp-main.263 pliedSciences12,12(2022). https://doi.org/10.3390/app12125785
[2] IzBeltagy,MatthewEPeters,andArmanCohan.2020.Longformer:Thelong- [20] MorGeva,AviCaciularu,GuyDar,PaulRoit,ShovalSadde,MicahShlain,Bar
documenttransformer.arXivpreprintarXiv:2004.05150(2020). Tamir,andYoavGoldberg.2022.LM-Debugger:AnInteractiveToolforInspection
[3] EmilyM.Bender,TimnitGebru,AngelinaMcMillan-Major,andShmargaret andInterventioninTransformer-BasedLanguageModels.InProceedingsofthe
Shmitchell.2021.OntheDangersofStochasticParrots:CanLanguageModelsBe 2022ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:System
TooBig?.InProceedingsofthe2021ACMConferenceonFairness,Accountability, Demonstrations.AssociationforComputationalLinguistics,AbuDhabi,UAE,
andTransparency(VirtualEvent,Canada)(FAccT’21).AssociationforComput- 12–21. https://doi.org/10.18653/v1/2022.emnlp-demos.2
ingMachinery,NewYork,NY,USA,610–623. https://doi.org/10.1145/3442188. [21] SteveGrahamandKarenR.Harris.2015. CommonCoreStateStandardsand
3445922 Writing:IntroductiontotheSpecialIssue.TheElementarySchoolJournal115,4
[4] StevenBird,EwanKlein,andEdwardLoper.2009.Naturallanguageprocessing (2015),457–463. https://doi.org/10.1086/681963
withPython:analyzingtextwiththenaturallanguagetoolkit."O’ReillyMedia, [22] SteveGraham,SharleneA.Kiuhara,andMeadeMacKay.2020. TheEffectsof
Inc.". WritingonLearninginScience,SocialStudies,andMathematics:AMeta-Analysis.
[5] MichaelBostock,VadimOgievetsky,andJeffreyHeer.2011. D3Data-Driven ReviewofEducationalResearch90,2(2020),179–226. https://doi.org/10.3102/
Documents. IEEETransactionsonVisualizationandComputerGraphics17,12 0034654320914744
(2011),2301–2309. https://doi.org/10.1109/TVCG.2011.185 [23] MarthaHHead,JohnEReadence,andRayRBuss.1989. Anexaminationof
[6] Robert-MihaiBotarleanu,MihaiDascalu,LauraK.Allen,ScottAndrewCrossley, summarywritingasameasureofreadingcomprehension.ReadingResearchand
andDanielleS.McNamara.2022.MultitaskSummaryScoringwithLongformers. Instruction28,4(1989),1–11. https://doi.org/10.1080/19388078909557982
InArtificialIntelligenceinEducation,MariaMercedesRodrigo,NoburuMat- [24] FredHohman,MinsukKahng,RobertPienta,andDuenHorngChau.2019.Visual
suda,AlexandraI.Cristea,andVaniaDimitrova(Eds.).SpringerInternational AnalyticsinDeepLearning:AnInterrogativeSurveyfortheNextFrontiers.IEEE
Publishing,Cham,756–761. https://doi.org/10.1007/978-3-031-11644-5_79 TransactionsonVisualizationandComputerGraphics25,8(2019),2674–2693.
[7] R.E.Boyatzis.1998.TransformingQualitativeInformation:ThematicAnalysisand https://doi.org/10.1109/TVCG.2018.2843369
CodeDevelopment.SAGEPublications. [25] BenjaminHoover,HendrikStrobelt,andSebastianGehrmann.2020.exBERT:A
[8] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan, VisualAnalysisTooltoExploreLearnedRepresentationsinTransformerModels.
PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda InProceedingsofthe58thAnnualMeetingoftheAssociationforComputational
Askell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan, Linguistics:SystemDemonstrations.AssociationforComputationalLinguistics,
RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter, Online,187–196. https://doi.org/10.18653/v1/2020.acl-demos.22
ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,Benjamin [26] SarthakJainandByronC.Wallace.2019.AttentionisnotExplanation.InPro-
Chess,JackClark,ChristopherBerner,SamMcCandlish,AlecRadford,Ilya ceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociation
Sutskever,andDarioAmodei.2020.LanguageModelsareFew-ShotLearners. forComputationalLinguistics:HumanLanguageTechnologies,Volume1(Longand
InAdvancesinNeuralInformationProcessingSystems,H.Larochelle,M.Ran- ShortPapers).AssociationforComputationalLinguistics,Minneapolis,Minnesota,
zato,R.Hadsell,M.F.Balcan,andH.Lin(Eds.),Vol.33.CurranAssociates, 3543–3556. https://doi.org/10.18653/v1/N19-1357
Inc.,1877–1901. https://proceedings.neurips.cc/paper_files/paper/2020/file/ [27] EnkelejdaKasneci,KathrinSeßler,StefanKüchemann,MariaBannert,Daryna
1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf Dementieva,FrankFischer,UrsGasser,GeorgGroh,StephanGünnemann,Eyke
[9] PeterBrusilovsky,SergeySosnovsky,andKhushbooThaker.2022.TheReturnof Hüllermeier,etal.2023.ChatGPTforgood?Onopportunitiesandchallenges
IntelligentTextbooks.AIMagazine43,3(Aug.2022),337–340. https://doi.org/ oflargelanguagemodelsforeducation.Learningandindividualdifferences103
10.1002/aaai.12061 (2023),102274.
[10] YuanzheChen,QingChen,MingqianZhao,SebastienBoyer,KalyanVeeramacha- [28] ToddKulesza,MargaretBurnett,Weng-KeenWong,andSimoneStumpf.2015.
neni,andHuaminQu.2016. DropoutSeer:Visualizinglearningpatternsin PrinciplesofExplanatoryDebuggingtoPersonalizeInteractiveMachineLearning.
MassiveOpenOnlineCoursesfordropoutreasoningandprediction.In2016 InProceedingsofthe20thInternationalConferenceonIntelligentUserInterfaces
IEEEConferenceonVisualAnalyticsScienceandTechnology(VAST).111–120. (Atlanta,Georgia,USA)(IUI’15).AssociationforComputingMachinery,New
https://doi.org/10.1109/VAST.2016.7883517 York,NY,USA,126–137. https://doi.org/10.1145/2678025.2701399
[11] KevinClark,UrvashiKhandelwal,OmerLevy,andChristopherD.Manning.2019. [29] SimoneLeonardi,DiegoMonti,GiuseppeRizzo,andMaurizioMorisio.2020.
WhatDoesBERTLookat?AnAnalysisofBERT’sAttention.InProceedingsof Multilingualtransformer-basedpersonalitytraitsestimation.Information11,4
the2019ACLWorkshopBlackboxNLP:AnalyzingandInterpretingNeuralNetworks (2020),179.
forNLP.AssociationforComputationalLinguistics,Florence,Italy,276–286. [30] QVeraLiaoandJenniferWortmanVaughan.2023.AITransparencyintheAge
https://doi.org/10.18653/v1/W19-4828 ofLLMs:AHuman-CenteredResearchRoadmap.arXivpreprintarXiv:2306.01941
14iScore:VisualAnalyticsforInterpretingHowLanguageModelsAutomaticallyScoreSummaries IUI’24,March18–21,2024,Greenville,SC,USA
(2023). Graphics25,1(2019),353–363. https://doi.org/10.1109/TVCG.2018.2865044
[31] ShixiaLiu,XitingWang,MengchenLiu,andJunZhu.2017. Towardsbetter [49] MukundSundararajan,AnkurTaly,andQiqiYan.2017.AxiomaticAttribution
analysisofmachinelearningmodels:Avisualanalyticsperspective. Visual forDeepNetworks.InProceedingsofthe34thInternationalConferenceonMachine
Informatics1,1(2017),48–56. https://doi.org/10.1016/j.visinf.2017.01.006 Learning(ProceedingsofMachineLearningResearch,Vol.70),DoinaPrecupand
[32] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,Omer YeeWhyeTeh(Eds.).PMLR,3319–3328. https://proceedings.mlr.press/v70/
Levy,MikeLewis,LukeZettlemoyer,andVeselinStoyanov.2019. Roberta:A sundararajan17a.html
robustlyoptimizedbertpretrainingapproach.arXivpreprintarXiv:1907.11692 [50] IanTenney,JamesWexler,JasmijnBastings,TolgaBolukbasi,AndyCoenen,
(2019). SebastianGehrmann,EllenJiang,MahimaPushkarna,CareyRadebaugh,Emily
[33] ScottMLundbergandSu-InLee.2017.AUnifiedApproachtoInterpretingModel Reif,andAnnYuan.2020. TheLanguageInterpretabilityTool:Extensible,In-
Predictions.InAdvancesinNeuralInformationProcessingSystems30,I.Guyon, teractiveVisualizationsandAnalysisforNLPModels.InProceedingsofthe
U.V.Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett 2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing:System
(Eds.).CurranAssociates,Inc.,4765–4774. http://papers.nips.cc/paper/7062-a- Demonstrations.AssociationforComputationalLinguistics,Online,107–118.
unified-approach-to-interpreting-model-predictions.pdf https://doi.org/10.18653/v1/2020.emnlp-demos.15
[34] WesleyMorris,ScottCrossley,LangdonHolmes,ChaohuaOu,DanielleMc- [51] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-Anne
Namara,andMihaiDascalu.2023. UsingLargeLanguageModelstoProvide Lachaux,TimothéeLacroix,BaptisteRozière,NamanGoyal,EricHambro,Faisal
FormativeFeedbackinIntelligentTextbooks.InArtificialIntelligenceinEducation. Azhar,etal.2023.Llama:Openandefficientfoundationlanguagemodels.arXiv
PostersandLateBreakingResults,WorkshopsandTutorials,IndustryandInnova- preprintarXiv:2302.13971(2023).
tionTracks,Practitioners,DoctoralConsortiumandBlueSky,NingWang,Genaro [52] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
Rebolledo-Mendez,VaniaDimitrova,NoboruMatsuda,andOlgaC.Santos(Eds.). AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.2017. AttentionisAll
SpringerNatureSwitzerland,Cham,484–489. https://doi.org/10.1007/978-3- youNeed.InAdvancesinNeuralInformationProcessingSystems,I.Guyon,U.Von
031-36336-8_75 Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett(Eds.),
[35] AhmedA.Mubarak,HanCao,WeizhenZhang,andWenliZhang.2021.Visual Vol.30.CurranAssociates,Inc. https://proceedings.neurips.cc/paper_files/paper/
analyticsofvideo-clickstreamdataandpredictionoflearners’performanceusing 2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
deeplearningmodelsinMOOCs’courses.ComputerApplicationsinEngineering [53] CamiloVieira,PaulParsons,andVetriaByrd.2018.Visuallearninganalyticsof
Education29,4(2021),710–732. https://doi.org/10.1002/cae.22328 educationaldata:Asystematicliteraturereviewandresearchagenda.Computers
[36] NancyNelsonandJamesR.King.2023.Discoursesynthesis:Textualtransforma- &Education122(2018),119–135. https://doi.org/10.1016/j.compedu.2018.03.018
tionsinwritingfromsources.ReadingandWriting36,4(01Apr2023),769–808. [54] JesseVig.2019.AMultiscaleVisualizationofAttentionintheTransformerModel.
https://doi.org/10.1007/s11145-021-10243-5 InProceedingsofthe57thAnnualMeetingoftheAssociationforComputational
[37] ChristopherMichaelOrmerod.2022.Mappingbetweenhiddenstatesandfeatures Linguistics:SystemDemonstrations.AssociationforComputationalLinguistics,
tovalidateautomatedessayscoringusingDeBERTamodels.PsychologicalTest Florence,Italy,37–42. https://doi.org/10.18653/v1/P19-3007
andAssessmentModeling64,4(2022),495–526. [55] EricWallace,PedroRodriguez,ShiFeng,IkuyaYamada,andJordanBoyd-Graber.
[38] EmilyPhillipsGallowayandPaolaUccelli.2019.Beyondreadingcomprehension: 2019.TrickMeIfYouCan:Human-in-the-LoopGenerationofAdversarialExam-
exploringtheadditionalcontributionofCoreAcademicLanguageSkillstoearly plesforQuestionAnswering.TransactionsoftheAssociationforComputational
adolescents’writtensummaries.ReadingandWriting32,3(01Mar2019),729–759. Linguistics7(2019),387–401. https://doi.org/10.1162/tacl_a_00279
https://doi.org/10.1007/s11145-018-9880-3 [56] AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuel
[39] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang, Bowman.2018. GLUE:AMulti-TaskBenchmarkandAnalysisPlatformfor
MichaelMatena,YanqiZhou,WeiLi,andPeterJ.Liu.2020. Exploringthe NaturalLanguageUnderstanding.InProceedingsofthe2018EMNLPWorkshop
LimitsofTransferLearningwithaUnifiedText-to-TextTransformer.Journalof BlackboxNLP:AnalyzingandInterpretingNeuralNetworksforNLP.Association
MachineLearningResearch21,140(2020),1–67. forComputationalLinguistics,Brussels,Belgium,353–355. https://doi.org/10.
[40] MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin.2016."WhyShouldI 18653/v1/W18-5446
TrustYou?":ExplainingthePredictionsofAnyClassifier.InProceedingsofthe [57] ChanghanWang,AnirudhJain,DanluChen,andJiataoGu.2019.VizSeq:avisual
22ndACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandData analysistoolkitfortextgenerationtasks.InProceedingsofthe2019Conferenceon
Mining(SanFrancisco,California,USA)(KDD’16).AssociationforComputing EmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJoint
Machinery,NewYork,NY,USA,1135–1144. https://doi.org/10.1145/2939672. ConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP):SystemDemonstra-
2939778 tions.AssociationforComputationalLinguistics,HongKong,China,253–258.
[41] MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin.2016."WhyShouldI https://doi.org/10.18653/v1/D19-3043
TrustYou?":ExplainingthePredictionsofAnyClassifier.InProceedingsofthe [58] ZijieJ.Wang,DongjinChoi,ShenyuXu,andDiyiYang.2021. PuttingHu-
22ndACMSIGKDDInternationalConferenceonKnowledgeDiscoveryandData mansintheNaturalLanguageProcessingLoop:ASurvey.InProceedingsof
Mining(SanFrancisco,California,USA)(KDD’16).AssociationforComputing theFirstWorkshoponBridgingHuman–ComputerInteractionandNaturalLan-
Machinery,NewYork,NY,USA,1135–1144. https://doi.org/10.1145/2939672. guageProcessing.AssociationforComputationalLinguistics,Online,47–52.
2939778 https://aclanthology.org/2021.hcinlp-1.8
[42] AnnaRogers,OlgaKovaleva,andAnnaRumshisky.2020.APrimerinBERTology: [59] ZijieJ.Wang,RobertTurko,andDuenHorngChau.2021. Dodrio:Exploring
WhatWeKnowAboutHowBERTWorks. TransactionsoftheAssociationfor TransformerModelswithInteractiveVisualization.InProceedingsoftheJointCon-
ComputationalLinguistics8(2020),842–866. https://doi.org/10.1162/tacl_a_00349 ferenceofthe59thAnnualMeetingoftheAssociationforComputationalLinguistics
[43] MichaelSedlmair,MiriahMeyer,andTamaraMunzner.2012. DesignStudy andthe11thInternationalJointConferenceonNaturalLanguageProcessing:Sys-
Methodology:ReflectionsfromtheTrenchesandtheStacks.IEEETransactions temDemonstrations.AssociationforComputationalLinguistics,Online,132–141.
onVisualizationandComputerGraphics18,12(2012),2431–2440. https://doi. https://zijie.wang/papers/dodrio/
org/10.1109/TVCG.2012.213 [60] ZijieJ.Wang,RobertTurko,OmarShaikh,HaekyuPark,NilakshDas,Fred
[44] YeonjuJangSeongyuneChoiandHyeoncheolKim.2023. InfluenceofPeda- Hohman,MinsukKahng,andDuenHorngPoloChau.2021. CNNExplainer:
gogicalBeliefsandPerceivedTrustonTeachers’AcceptanceofEducational LearningConvolutionalNeuralNetworkswithInteractiveVisualization.IEEE
ArtificialIntelligenceTools. InternationalJournalofHuman–ComputerInter- TransactionsonVisualizationandComputerGraphics27,2(2021),1396–1406.
action39,4(2023),910–922. https://doi.org/10.1080/10447318.2022.2049145 https://doi.org/10.1109/TVCG.2020.3030418
arXiv:https://doi.org/10.1080/10447318.2022.2049145 [61] SarahWiegreffeandYuvalPinter.2019. AttentionisnotnotExplanation.In
[45] AngélicaM.SilvaandRobertoLimongi.2019.Writingtolearnincreaseslong- Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguagePro-
termmemoryconsolidation:Amental-chronometryandcomputational-modeling cessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing
studyof“Epistemicwriting”.JournalofWritingResearch11,1(Jun.2019),211–243. (EMNLP-IJCNLP).AssociationforComputationalLinguistics,HongKong,China,
https://doi.org/10.17239/jowr-2019.11.01.07 11–20. https://doi.org/10.18653/v1/D19-1002
[46] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, [62] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,
AbubakarAbid,AdamFisch,AdamRBrown,AdamSantoro,AdityaGupta, AnthonyMoi,PierricCistac,TimRault,RemiLouf,MorganFuntowicz,Joe
AdriàGarriga-Alonso,etal.2022.Beyondtheimitationgame:Quantifyingand Davison,SamShleifer,PatrickvonPlaten,ClaraMa,YacineJernite,JulienPlu,
extrapolatingthecapabilitiesoflanguagemodels.arXivpreprintarXiv:2206.04615 CanwenXu,TevenLeScao,SylvainGugger,MariamaDrame,QuentinLhoest,
(2022). andAlexanderRush.2020. Transformers:State-of-the-ArtNaturalLanguage
[47] KennethSteimelandBrianRiordan.2020. Towardsinstance-basedcontent Processing.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNatural
scoringwithpre-trainedtransformermodels.InProceedingsoftheThirty-Fourth LanguageProcessing:SystemDemonstrations.AssociationforComputational
AAAIConferenceonArtificialIntelligence,Vol.34. Linguistics,Online,38–45. https://doi.org/10.18653/v1/2020.emnlp-demos.6
[48] HendrikStrobelt,SebastianGehrmann,MichaelBehrisch,AdamPerer,Hanspeter [63] GuangxuanXiao,YuandongTian,BeidiChen,SongHan,andMikeLewis.
Pfister,andAlexanderM.Rush.2019.Seq2seq-Vis:AVisualDebuggingToolfor 2023.Efficientstreaminglanguagemodelswithattentionsinks.arXivpreprint
Sequence-to-SequenceModels.IEEETransactionsonVisualizationandComputer arXiv:2309.17453(2023).
15IUI’24,March18–21,2024,Greenville,SC,USA Coscia,etal.
[64] GefeiZhang,ZihaoZhu,SujiaZhu,RonghuaLiang,andGuodaoSun.2022. [65] HuijieZhang,JialuDong,ChengLv,YimingLin,andJinghanBai.2023.Visual
Towardsabetterunderstandingoftheroleofvisualizationinonlinelearning: analyticsofpotentialdropoutbehaviorpatternsinonlinelearningbasedon
Areview.VisualInformatics6,4(2022),22–33. https://doi.org/10.1016/j.visinf. counterfactualexplanation.JournalofVisualization26,3(01Jun2023),723–741.
2022.09.002 https://doi.org/10.1007/s12650-022-00899-8
16