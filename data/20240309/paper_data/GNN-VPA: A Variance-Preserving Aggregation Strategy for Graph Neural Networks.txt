AcceptedatICLR2024(TinyPapersTrack)
GNN-VPA: A Variance-Preserving Aggregation
Strategy for Graph Neural Networks
LisaSchneckenreiter1∗ RichardFreinschlag1∗ FlorianSestak1
JohannesBrandstetter1,2 GünterKlambauer1 AndreasMayr1
1ELLISUnitLinzandLITAILab,InstituteforMachineLearning,
JohannesKeplerUniversity,Linz,Austria
2NXAIGmbH,Linz,Austria
last-name@ml.jku.at
Abstract
Graphneuralnetworks(GNNs),andespeciallymessage-passingneuralnetworks,
excel in various domains such as physics, drug discovery, and molecular mod-
eling. The expressivity of GNNs with respect to their ability to discriminate
non-isomorphicgraphscriticallydependsonthefunctionsemployedformessage
aggregationandgraph-levelreadout. Byapplyingsignalpropagationtheory,we
proposeavariance-preservingaggregationfunction(VPA)thatmaintainsexpressiv-
ity,butyieldsimprovedforwardandbackwarddynamics.Experimentsdemonstrate
thatVPAleadstoincreasedpredictiveperformanceforpopularGNNarchitectures
aswellasimprovedlearningdynamics. Ourresultscouldpavethewaytowards
normalizer-freeorself-normalizingGNNs.
1 Introductionandrelatedwork
Formanyreal-worldpredictiontasks,graphsnaturallyrepresenttheinputdata.Graphneuralnetworks
(GNNs)(Scarsellietal.,2009;Kipf&Welling,2017;Defferrardetal.,2016;Velicˇkovic´ etal.,2018)
arethereforeoflargeinterestastheyareabletonaturallyprocesssuchdata. Theyhavebeenusedfor
moleculepredictions(Duvenaudetal.,2015;Kearnesetal.,2016;Gilmeretal.,2017;Mayretal.,
2018;Satorrasetal.,2021),materialscience(Reiseretal.,2022;Merchantetal.,2023),modeling
physicalinteractionsorimprovingPDEsolversforphysicspredictions(Sanchez-Gonzalezetal.,
2020;Brandstetteretal.,2022;Mayretal.,2023), weatherprediction(Keisler,2022;Lametal.,
2022),predictionsaboutsocialnetworks(Hamiltonetal.,2017;Fanetal.,2019;Montietal.,2019),
generegulatorynetworksinsystemsbiology(Eetemadi&Tagkopoulos,2018;Wangetal.,2020),
combinatorialoptimization(Cappartetal.,2023;Sanokowskietal.,2023),andknowledgegraphs
(Schlichtkrulletal.,2018;Lietal.,2022)forreasoning.
Despite the huge successes of GNNs, there are some limitations. Morris et al. (2019) and Xu
etal.(2019)analyzedtheexpressivepowerofGNNsandfoundthattheyarenotmorepowerful
thantheWeisfeiler-Lemangraphisomorphismheuristic(1-WLtest)(Leman&Weisfeiler,1968)
at distinguishing non-isomorphic graphs. Moreover, Xu et al. (2019) constructed a GNN (GIN
architecture), which should attain the same expressive power as the 1-WL test. An important
conclusioninthedesignoftheGINarchitecturewasthatthechoiceofthemessageaggregationand
graph-levelreadoutfunctioniscrucialforenablingmaximumexpressivity. Morespecifically,SUM
aggregationallowstoattain1-WLexpressivepower,whileMEANorMAXaggregationeffectively
limitsexpressivity.
∗Equalcontribution
1
4202
raM
7
]GL.sc[
1v74740.3042:viXraAcceptedatICLR2024(TinyPapersTrack)
While the expressive power of GNNs has been investigated profoundly (Xu et al., 2019), signal
propagation(Neal,1995;Schoenholzetal.,2017;Klambaueretal.,2017)throughGNNsiscurrently
under-explored.Thereareplentyofworksonconventionalfully-connectedneuralnetworks(FCNNs),
which study signal propagation behavior (e.g., Schoenholz et al., 2017; Klambauer et al., 2017)
throughoutthenetworks. Typically,forFCNNsorconvolutionalneuralnetworks(CNNs),thereare
eitherweightinitializationschemes(e.g.,Glorot&Bengio,2010;Heetal.,2015)ornormalization
layers(e.g.,Ioffe&Szegedy,2015;Baetal.,2016),whichpreventthattheweightedsummedinputs
leadtoexplodingactivationsthroughoutthedepthofthenetwork.
ForGNNsandespeciallytheGINarchitecturewithSUMmessageaggregation,explodingactivations
areamainobstacleforefficienttrainingaswellandsignalpropagationbehaviorappearsproblematic.
Conventionalweightinitializationschemesattheaggregationstepcannotbeappliedinastraightfor-
wardmanner,sincethenumberofneighborsinanaggregationstepandthenumberofnodesina
grapharevariable. Moreover,thefactthatzerovarianceinmessagesmightbeacommoncasefor
graphclassificationalsolimitstheapplicabilityofnormalizationlayers.
Ouraiminthisworkistodevelopageneralaggregationapproach2,whichcanbeappliedtodif-
ferentGNNarchitectures,preservesmaximumexpressivity,andatthesametimeavoidsexploding
activations. Withsimplisticassumptions,wewillmotivatetheuseofavariance-preservingaggrega-
tionfunctionforGNNs(seeFig.1),whichimprovessignalpropagationandconsequentlylearning
dynamics.
N N N
1 (cid:88) N (cid:88) 1 (cid:88)
max √
N j=1 N
j=1 j=1 j=1
meanaggregation(MEAN) maxaggregation(MAX) sumaggregation(SUM) variance-preservingaggregation(VPA)
✘ expressivity ✘ expressivity ✔ expressivity ✔ expressivity
❍ signalpropagation ❍ signalpropagation ✘ signalpropagation ✔ signalpropagation
Figure1: Overviewofmainmessageaggregationfunctionsandtheirproperties.
2 GNNswithVariancePreservation
Notationalpreliminaries. WeassumeagraphG=(V,E)withnodesv ∈V,edgese ∈Eand
i ij
D-dimensionalnodefeaturesh ∈RD. WeuseN(i)toindicatethesetofneighboringnodestonode
i
v withinV. TobeconsistentwithFig.1,wedefineN alwaystobethenumberofneighboringnodes,
i
i.e. N :=|N(i)|,whereweassumethatiisclearfromthecontext. Forsimplicity,wedonotassume
anyedgefeatures.
Graphneuralnetworks(GNNs)exchangeinformation,i.e.,messages,throughtheapplicationofa
local,permutation-invariantfunctionacrossallneighborhoods. Thecorelayersiterativelyupdate
nodeembeddingsh atnodev viathreesubsteps1.-3.:
i i
(cid:16) (cid:17) (cid:16) (cid:17) (cid:77) (cid:16) (cid:16) (cid:17)(cid:17)
1. m =ϕ h ,h or m =ϕ h 2. m⊕ = m 3. h′ =ψ h ,θ m⊕
ij i j ij j i ij i i i
j∈N(i)
toanewembeddingh′,wheretheaggregation(cid:76)
atnodev isacrossallneighboringnodes,
i j∈N(i) i
i.e., those nodes v , that are connected to node v via an edge e . These nodes are renumbered
j i ij
accordingtoFig.1from1toN. DependingonthetypeofGNN,ϕ, ψ, andθ canberealizedas
learnablefunctions,usuallyMultilayerPerceptrons(MLPs). E.g.,forGraphConvolutionalNetworks
(GCNs)(Kipf&Welling,2017)onlyψislearnable,forgeneralMessagePassingNeuralNetworks
(Gilmeretal.,2017)ϕandψarelearnable,andforGraphIsomorphismNetworks(GINs)(Xuetal.,
2019)allthreearelearnable.
Signalpropagationtheoryallowstoanalyzethedistributionofquantitiesthroughrandomlyini-
tialized neural networks. From certain assumptions (for details see App. A.2) it follows that
2Wearenotinterestedinproposinganewpoolingmechanism,butinsuggestinganewaggregationfunction
thatcanoptionallybeappliedtograph-levelreadout.Forfurtherdetailsonthedifferencesbetweenaggregation
andpooling,seeApp.A.1.
2AcceptedatICLR2024(TinyPapersTrack)
m ∼ p (0,I). If m are further assumed to be independent of each other3, one obtains
ij N ij
m iSUM ∼ p N(0,NI)for SUM aggregation(i.e.,(cid:76) ≡ (cid:80)N i=1),andm iMEAN ∼ p N(0, N1I)for MEAN
aggregation(i.e.,(cid:76) ≡ 1 (cid:80)N )atinitialization.
N i=1
Avariancepreservingaggregation(VPA)function. Ourkeyideaistointroduceanewaggregation
function which preserves variance, i.e., m ∼ p (0,I). This is possible with the aggregation
i N
function(cid:76) ≡ √1 (cid:80)N . Wedenotethisaggregationfunctionasvariance-preservingaggregation
N i=1
(VPA)andshowthepreservationpropertybyapplyingLemma1element-wise. Foracompleteproof
seeApp. A.3.
Lemma1. Letz ,...,z beindependentcopiesofacenteredrandomvariablezwithfinitevariance.
1 N
Thentherandomvariabley = √1
N
(cid:80)N n=1z nhasthesamemeanandvarianceasz.
IncontrasttoSUMorMEANaggregationfunctions,VPAtheoreticallypreservesthevarianceacross
layers. Accordingtosignalpropagationtheory,suchbehaviorisadvantageousforlearning.
Expressive power of GNN-VPA. According to Xu et al. (2019) a prerequisite for maximum
expressivepowerw.r.t. discriminatingnon-isomorphicgraphsisaninjectiveaggregationfunction,
suchasSUMaggregation,whileMEANorMAXaggregationresultsinlimitedexpressivity. Amessage
passing algorithm with VPA has the same expressive power as SUM aggregation, which follows
analogouslytoXuetal.(2019)fromLemma2(seeApp.A.4foraproof).
Lemma2. AssumethemultisetX iscountableandthenumberofuniqueelementsinX isbounded
byanumberN. Thereexistsafunctionf :X →RN suchthath(X)= √1 (cid:80) f(x)isunique
|X| x∈X
foreachmultisetX ⊂X ofboundedsize,where|X|denotesthecardinalityofmultisetX (sumof
multiplicitiesofalluniqueelementsinthemultiset).
Extensionofvariancepreservationtoattention. Ourvariance-preservingaggregationstrategycan
beextendedtoattentionmechanisms. Weassume,thatrandomvariablesz ,...,z areaggregated
1 N
byanattentionmechanism. Therespectivecomputedattentionweightsareassumedtobegivenby
c ,...,c ,wherec ∈R+and(cid:80)N c =1holds. Further,weconsiderc tobeconstants4.
1 N i 0 i=1 i i
InordertofindausefulextensionofVPAtoattention,wefirstconsidertwoextremecasesonthe
distributionofattentionweights:
• Case1: Allattentionweightsareequal. Theninordertofulfill(cid:80)N c =1,allc = 1.
i=1 i i N
• Case2: Attentionfocusesonexactlyonevalue,whichmightbew.l.o.g. j. Thenc =1and
j
c =0 ∀i̸=j.
i
Wenotethatcase1isthesameasMEANaggregationandcase2correspondstoMAXaggregationif
max(z ,...,z )=z andz <z ∀i̸=j. Inbothcases,GNNshavemorelimitedexpressivity
1 N j i j
thanwithVPAorSUMaggregation.
(cid:113)
Toapplytheconceptofvariancepreservationtoattention,wedefineaconstantC := (cid:80)N c2and
i=1 i
usethefollowingattentionmechanism: y = 1 (cid:80)N c z . AsshowninLemma3thisresultsina
C i=1 i i
variance-preservingattentionmechanism. ForacompleteproofseeApp.A.5.
Lemma3. Letz ,...,z beindependentcopiesofacenteredrandomvariablezwithfinitevariance
1 N
and let c ,...,c be constants, where c ∈ R+ and (cid:80)N c = 1. Then the random variable
1 N i 0 i=1 i
(cid:113)
y = 1 (cid:80)N c z withC = (cid:80)N c2hasthesamemeanandvarianceasz.
C n=1 n n i=1 i
3Notethatthisassumptionistoostrong,sinceforafixedi,allm dependoneachotherbecausetheyare
ij
alldeterminedbytheinputh .
i
4Note, thatthismight beanover-simplisticassumption, especiallysince/whenkeysandvaluesarenot
independent.
3AcceptedatICLR2024(TinyPapersTrack)
3 Experiments
WetestedtheeffectivenessofourideaonarangeofestablishedGNNarchitectures5: GraphIsomor-
phismNetworks(GIN)(Xuetal.,2019),GraphConvolutionalNetwork(GCN)(Kipf&Welling,
2017),GraphAttentionNetworks(GAT)(Velicˇkovic´ etal.,2018)andSimpleGraphConvolution
Networks(SGC)(Wuetal.,2019). Toevaluatepredictionperformance, wecombined GIN and
GCNarchitectureswitheachoftheaggregationmethodsinFig.1bothformessageaggregationand
graph-levelreadout. NotethatweusedtheGCNformulationasreportedinMorrisetal.(2019)to
circumventtheinherentnormalizationintheGCNarchitecturebyKipf&Welling(2017).
ToincorporatetheideaofvariancepreservationintotheSGCarchitecture,wechangedtheupdateof
hfrom
N
h′ = 1 h +(cid:88) a ij h
i d i+1 i
j=1
(cid:112) (d i+1)(d
j
+1) j
to
N
h′ = √ 1 h +(cid:88) a ij h
i d i+1 i
j=1
(cid:112) 4 (d i+1)(d
j
+1) j
(wherea areentriesoftheadjacencymatrix,d andd arenodedegrees,and,h andh denotethe
ij i j i j
hiddenneuralrepresentationatsometimestepduringmessagepassing). Foravariance-preserving
versionofGAT,weadaptedattentionaccordingto Lemma3andnotethatinthepracticalimple-
mentation,wedonotbackpropagateerrorsthroughtheseconstantsduringtraining.
Benchmarking datasets and settings. We tested our methods on the same graph classification
benchmarksfromtheTUDatasetcollectionasXuetal.(2019), consistingoffivesocialnetwork
datasets(IMDB-BINARY,IMDB-MULTI,COLLAB,REDDIT-BINARY,andREDDIT-MULTI-5K)
andfourbioinformaticsdatasets(MUTAG,PROTEINS,PTCandNCI1). Sincethesocialnetwork
datasetsdonotcontainanynodefeatures,weintroducednodefeaturesintwodifferentways. Inthe
firstvariant,thegraphsareconsideredasgivenwithallnodefeaturessetto1. Intheothervariant,
theone-hotencodednodedegreeisusedasanadditionalnodefeature. Wereportresultsforthefirst
variantinTable1andresultsforthesecondvariantinTableB1. Thebioinformaticsdatasetswere
usedwiththeprovidednodefeatures. Formoredetailsontheuseddatasets,werefertoMorrisetal.
(2020)andXuetal.(2019).
Training,validation,andtestsplits. Ourexperimentswereevaluatedwith10-foldcross-validation.
Ineachiteration,weused1/10ofthedatafortesting,1/10forvalidationand8/10fortraining. The
validationsetwasonlyusedtoadjustthenumberoftrainingepochs,suchthatourtestaccuracieswere
computedfortheepochwiththehighestvalidationaccuracy. Formoredetailsonimplementation
andhyperparametersseeApp.B.1.
Results. TestaccuraciesforallfourGNNarchitecturescomparingVPAwiththestandardaggregation
methodsareshownin Table1. Inalmostallcases, VPA significantlyoutperformsthecompared
methods. Notably,theGINandGCNarchitecturesincombinationwithMEANorMAXaggregation
were unable to learn the social network tasks without additional node features, likely due to the
inherentinabilityoftheseaggregationfunctionstocaptureanode’sdegree. Thisemphasizesthe
increased expressivity of VPA compared to these methods. For additional results concerning the
trainingbehavior,seeApp.B.3.
4 Discussion
OurresultshintatapotentiallypowerfulnewaggregationfunctionwithequalexpressivityasSUM
aggregationandimprovedlearningdynamics.
Ingeneral, itneedstobeconsideredthatbetterpredictionperformanceofmorepowerfulGNNs
will only be observed when the underlying machine learning problem requires such a level of
expressiveness. Forbenchmarksfromreal-worlddata,itmight,however,notbeknownwhetherless
powerfulGNNscanalsoshowcompetitivepredictionperformance.
5Codeisavailableathttps://github.com/ml-jku/GNN-VPA.
4AcceptedatICLR2024(TinyPapersTrack)
IMDB-B IMDB-M RDT-B RDT-M5K COLLAB MUTAG PROTEINS PTC NCI1 AVG p
GIN+SUM 71.8±4.0 47.1±4.3 85.5±2.2 52.0±3.0 70.9±1.5 87.2±4.9 73.3±3.1 54.1±7.1 81.7±2.3 69.3 2.0e-5
GIN+MEAN 50.0±0.0 33.3±0.0 50.0±0.1 20.0±0.1 32.5±0.1 76.1±11.1 67.2±2.9 58.7±6.5 77.7±1.9 51.7 3.2e-15
GIN+MAX 50.0±0.0 33.3±0.0 49.7±0.5 20.2±0.4 52.0±0.0 77.0±8.2 71.8±3.6 59.0±9.7 80.5±2.8 54.8 3.9e-13
GIN+VPA 72.0±4.4 48.7±5.2 89.0±1.9 56.1±3.0 73.5±1.5 86.7±4.4 73.2±4.8 60.1±5.8 81.2±2.1 71.2 -
GCN+SUM 63.3±6.1 42.1±3.7 75.4±3.2 37.3±3.5 67.0±2.2 78.7±7.8 70.3±2.2 61.3±7.8 80.2±2.0 64.0 9.9e-9
GCN+MEAN 50.0±0.0 33.3±0.0 49.9±0.2 20.1±0.1 52.0±0.0 72.4±6.3 74.3±4.4 63.3±6.5 75.8±2.6 54.6 3.3e-12
GCN+MAX 50.5±0.0 33.3±0.0 50.0±0.0 20.0±0.1 52.0±0.0 67.6±4.3 43.9±7.3 58.7±6.6 55.1±2.6 47.8 1.4e-15
GCN+VPA 71.7±3.9 46.7±3.5 85.5±2.3 54.8±2.4 73.7±1.7 76.1±9.6 73.9±4.8 61.3±5.9 79.0±1.8 69.2 -
SGC 62.9±3.9 40.3±4.1 78.9±2.0 41.3±3.5 68.0±2.2 73.5±9.8 73.1±3.4 59.0±6.0 68.5±2.2 62.8 3.8e-12
SGC+VPA 70.4±4.1 47.5±4.4 84.2±2.2 53.4±2.7 71.7±1.7 73.9±6.2 75.4±4.2 63.1±8.0 76.4±2.8 68.4 -
GAT 51.0±4.4 37.4±3.6 74.5±3.8 33.1±1.9 56.2±0.6 77.7±11.5 75.4±2.9 60.5±5.5 77.7±2.2 60.4 7.6e-9
GAT+VPA 71.1±4.6 44.1±4.5 78.1±3.7 43.3±2.4 69.9±3.2 81.9±8.0 73.0±4.2 60.8±6.1 76.1±2.3 66.5 -
Table1: TestaccuracyontheTUDatasetswith10-foldcross-validation. Standarddeviationsare
indicated with ±. Column "AVG" shows the average test accuracy across data sets and column
"p"indicatesp-valuesofpairedone-sidedWilcoxontestsacrossalldatasetsandvalidationfolds
comparingeachmethodtothecorrespondingVPAvariant.
Furthermore,variancepreservationseemstobeanimportantpropertytoavoidexplodingorvanishing
activations. Thisisespeciallyrelevantforverydeepnetworks. Forthedatasetsused,allmethods
couldbetrainedwithoutdivergingduetoexplodingactivations. OnereasoncouldbethattheGNNs
arequiteshallowandthereforethereareonlyafewmessage-passingsteps. Nevertheless,resultsin
Table1andlearningcurvesinFig.B1showthatVPAhasadvantagesoverSUMaggregationinterms
ofconvergencespeed.
Onthesocialnetworkdatasets,VPAseemstoperformparticularlywellcomparedtoothermethods
when no additional node features are introduced, forcing the GNNs to learn from the network
structure instead (see experimental results in Table 1). However, including the node degree as
a feature improves the performance of less expressive GNNs (see Table B1). The advantage in
predictionperformanceofVPAoverothermethodsislesspronouncedinthissetting.
WhilewesuggestVPAasageneralaggregationscheme,whichiseasilyapplicabletomanyGNN
architectures,suchasGIN,itsapplicationmightnotbeobviousforothermodels. Forexample,SGC
inherentlycontainsanormalizationstrategyusingnodedegreesand GAT makesuseofattention
weightsduringaggregation. Inbothcases,signalpropagationisaffected. Takingthisintoaccount,
wesuggestvariantsof VPA for SGC and GAT.Variancepreservationfor GAT+VPA isshownin
Lemma3,however,wedidnotformallyproofvariancepreservationforSGC+VPA.
Itshouldfurtherbeconsidered,thatdistributionalassumptionstoformallyshowvariancepreservation
mightonlyholdatthetimeofinitialization. However,asdiscussedinApp.A.2eventhattimepointis
important. Furthermore,evenunderotherassumptionsonthedistributionofthemessages,arguments
abouttheincreaseanddecreaseofvariancewouldhold.
Acknowledgments
The ELLIS Unit Linz, the LIT AI Lab, the Institute for Machine Learning, are supported by the
FederalStateUpperAustria. WethanktheprojectsMedicalCognitiveComputingCenter(MC3),
INCONTROL-RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG-872172), DL for Gran-
ularFlow (FFG-871302), EPILEPSIA (FFG-892171), AIRI FG 9-N (FWF-36284, FWF-36235),
AI4GreenHeatingGrids(FFG-899943),INTEGRATE(FFG-892418),ELISE(H2020-ICT-2019-3
ID: 951847), Stars4Waters (HORIZON-CL6-2021-CLIMATE-01-01). We thank NXAI GmbH,
Audi.JKUDeepLearningCenter,TGWLOGISTICSGROUPGMBH,SiliconAustriaLabs(SAL),
FILLGesellschaftmbH,AnylineGmbH,Google,ZFFriedrichshafenAG,RobertBoschGmbH,
UCBBiopharmaSRL,MerckHealthcareKGaA,VerbundAG,GLS(Univ. Waterloo), Software
CompetenceCenterHagenbergGmbH,BorealisAG,TÜVAustria,FrauscherSensonic,TRUMPF
andtheNVIDIACorporation.
5AcceptedatICLR2024(TinyPapersTrack)
References
Ba,J.L.,Kiros,J.R.,andHinton,G.E. LayerNormalization. arXivpreprintarXiv:1607.06450,2016.
Brandstetter, J., Worrall, D. E., and Welling, M. Message Passing Neural PDE Solvers. In International
ConferenceonLearningRepresentations,2022.
Bronstein,M.M.,Bruna,J.,Cohen,T.,andVelicˇkovic´,P. GeometricDeepLearning:Grids,Groups,Graphs,
Geodesics,andGauges. arXivpreprintarXiv:2104.13478,2021.
Cappart,Q.,Chételat,D.,Khalil,E.B.,Lodi,A.,Morris,C.,andVelicˇkovic´,P. CombinatorialOptimizationand
ReasoningwithGraphNeuralNetworks. JournalofMachineLearningResearch,24(130):1–61,2023.
Chen,K.,Song,J.,Liu,S.,Yu,N.,Feng,Z.,Han,G.,andSong,M. DistributionKnowledgeEmbeddingfor
GraphPooling. IEEETransactionsonKnowledgeandDataEngineering,35(8):7898–7908,2023.
Corso,G.,Cavalleri,L.,Beaini,D.,Liò,P.,andVelicˇkovic´,P. PrincipalNeighbourhoodAggregationforGraph
Nets. InAdvancesinNeuralInformationProcessingSystems,2020.
Defferrard, M., Bresson, X., and Vandergheynst, P. Convolutional Neural Networks on Graphs with Fast
LocalizedSpectralFiltering. InAdvancesinNeuralInformationProcessingSystems,2016.
Duvenaud,D.K.,Maclaurin,D.,Iparraguirre,J.,Bombarell,R.,Hirzel,T.,Aspuru-Guzik,A.,andAdams,R.P.
ConvolutionalNetworksonGraphsforLearningMolecularFingerprints. InAdvancesinNeuralInformation
ProcessingSystems,2015.
Eetemadi,A.andTagkopoulos,I. GeneticNeuralNetworks: AnArtificialNeuralNetworkArchitecturefor
CapturingGeneExpressionRelationships. Bioinformatics,35(13):2226–2234,112018.
Fan,W.,Ma,Y.,Li,Q.,He,Y.,Zhao,E.,Tang,J.,andYin,D. GraphNeuralNetworksforSocialRecommenda-
tion. InTheWorldWideWebConference,2019.
Fey, M.andLenssen, J.E. FastGraphRepresentationLearningwithPyTorchGeometric. arXivpreprint
arXiv:1903.02428,2019.
Gilmer,J.,Schoenholz,S.S.,Riley,P.F.,Vinyals,O.,andDahl,G.E. NeuralMessagePassingforQuantum
Chemistry. InInternationalConferenceonMachineLearning,2017.
Glorot,X.andBengio,Y. UnderstandingtheDifficultyofTrainingDeepFeedforwardNeuralNetworks. In
InternationalConferenceonArtificialIntelligenceandStatistics,2010.
Hamilton,W.,Ying,Z.,andLeskovec,J. InductiveRepresentationLearningonLargeGraphs. InAdvancesin
NeuralInformationProcessingSystems,2017.
He,K.,Zhang,X.,Ren,S.,andSun,J. DelvingDeepintoRectifiers:SurpassingHuman-LevelPerformanceon
ImageNetClassification. InIEEEInternationalConferenceonComputerVision(ICCV),2015.
Hoedt,P.-J.andKlambauer,G. PrincipledWeightInitialisationforInput-ConvexNeuralNetworks. InAdvances
inNeuralInformationProcessingSystems,2023.
Ioffe,S.andSzegedy,C. BatchNormalization: AcceleratingDeepNetworkTrainingbyReducingInternal
CovariateShift. InInternationalConferenceonMachineLearning,2015.
Kearnes,S.,McCloskey,K.,Berndl,M.,Pande,V.,andRiley,P. MolecularGraphConvolutions: Moving
BeyondFingerprints. JournalofComputer-AidedMolecularDesign,30(8):595–608,2016.
Keisler,R. ForecastingGlobalWeatherwithGraphNeuralNetworks. arXivpreprintarXiv:2202.07575,2022.
Kipf,T.N.andWelling,M.Semi-SupervisedClassificationwithGraphConvolutionalNetworks.InInternational
ConferenceonLearningRepresentations,2017.
Klambauer,G.,Unterthiner,T.,Mayr,A.,andHochreiter,S. Self-NormalizingNeuralNetworks. InAdvancesin
NeuralInformationProcessingSystems,2017.
Lam,R.,Sanchez-Gonzalez,A.,Willson,M.,Wirnsberger,P.,Fortunato,M.,Pritzel,A.,Ravuri,S.,Ewalds,T.,
Alet,F.,Eaton-Rosen,Z.,etal. GraphCast:LearningSkillfulMedium-RangeGlobalWeatherForecasting.
arXivpreprintarXiv:2212.12794,2022.
LeCun,Y.A.,Bottou,L.,Orr,G.B.,andMüller,K.-R. EfficientBackProp. InMontavon,G.,Orr,G.B.,
andMüller,K.-R.(eds.),NeuralNetworks:TricksoftheTrade:SecondEdition,pp.9–48.SpringerBerlin
Heidelberg,2012.
6AcceptedatICLR2024(TinyPapersTrack)
Lee,J.,Sohl-Dickstein,J.,Pennington,J.,Novak,R.,Schoenholz,S.,andBahri,Y. DeepNeuralNetworksas
GaussianProcesses. InInternationalConferenceonLearningRepresentations,2018.
Lee,J.,Lee,I.,andKang,J. Self-AttentionGraphPooling. InInternationalConferenceonMachineLearning,
2019.
Leman,A.andWeisfeiler,B. Areductionofagraphtoacanonicalformandanalgebraarisingduringthis
reduction. Nauchno-TechnicheskayaInformatsiya,2(9):12–16,1968.
Li,Z.,Liu,H.,Zhang,Z.,Liu,T.,andXiong,N.N.LearningKnowledgeGraphEmbeddingWithHeterogeneous
RelationAttentionNetworks. IEEETransactionsonNeuralNetworksandLearningSystems,33(8):3961–
3973,2022.
Lu,Y.,Gould,S.,andAjanthan,T. BidirectionallySelf-NormalizingNeuralNetworks. NeuralNetworks,167:
283–291,2023.
Martens,J.,Ballard,A.,Desjardins,G.,Swirszcz,G.,Dalibard,V.,Sohl-Dickstein,J.,andSchoenholz,S.S.
RapidTrainingofDeepNeuralNetworkswithoutSkipConnectionsorNormalizationLayersusingDeep
KernelShaping. arXivpreprintarXiv:2110.01765,2021.
Mayr,A.,Klambauer,G.,Unterthiner,T.,Steijaert,M.,Wegner,J.K.,Ceulemans,H.,Clevert,D.-A.,and
Hochreiter, S. Large-Scale Comparison of Machine Learning Methods for Drug Target Prediction on
ChEMBL. ChemicalScience,9:5441–5451,2018.
Mayr,A.,Lehner,S.,Mayrhofer,A.,Kloss,C.,Hochreiter,S.,andBrandstetter,J. BoundaryGraphNeural
Networksfor3DSimulations. InAAAIConferenceonArtificialIntelligence,2023.
Merchant,A.,Batzner,S.,Schoenholz,S.S.,Aykol,M.,Cheon,G.,andCubuk,E.D. ScalingDeepLearning
forMaterialsDiscovery. Nature,624(7990):80–85,2023.
Monti,F.,Frasca,F.,Eynard,D.,Mannion,D.,andBronstein,M.M. FakeNewsDetectiononSocialMedia
usingGeometricDeepLearning. InInternationalConferenceonLearningRepresentations,2019.
Morris,C.,Ritzert,M.,Fey,M.,Hamilton,W.L.,Lenssen,J.E.,Rattan,G.,andGrohe,M. Weisfeilerand
LemanGoNeural:Higher-OrderGraphNeuralNetworks. InAAAIConferenceonArtificialIntelligenceand
InnovativeApplicationsofArtificialIntelligenceConferenceandAAAISymposiumonEducationalAdvances
inArtificialIntelligence,2019.
Morris,C.,Kriege,N.M.,Bause,F.,Kersting,K.,Mutzel,P.,andNeumann,M. TUDataset:ACollectionof
BenchmarkDatasetsforLearningwithGraphs. InICML2020WorkshoponGraphRepresentationLearning
andBeyond,2020.
Neal,R.M. BayesianLearningforNeuralNetworks. PhDthesis,UniversityofToronto,1995.
Reiser,P.,Neubert,M.,Eberhard,A.,Torresi,L.,Zhou,C.,Shao,C.,Metni,H.,vanHoesel,C.,Schopmans,H.,
Sommer,T.,andFriederich,P.GraphNeuralNetworksforMaterialsScienceandChemistry.Communications
Materials,3(1):93,2022.
Sanchez-Gonzalez,A.,Godwin,J.,Pfaff,T.,Ying,R.,Leskovec,J.,andBattaglia,P. LearningtoSimulate
ComplexPhysicswithGraphNetworks. InInternationalConferenceonMachineLearning,2020.
Sanokowski, S., Berghammer, W. F., Hochreiter, S., and Lehner, S. Variational Annealing on Graphs for
CombinatorialOptimization. InAdvancesinNeuralInformationProcessingSystems,2023.
Satorras,V.G.,Hoogeboom,E.,andWelling,M. E(n)EquivariantGraphNeuralNetworks. InInternational
ConferenceonMachineLearning,2021.
Scarselli,F.,Gori,M.,Tsoi,A.C.,Hagenbuchner,M.,andMonfardini,G. TheGraphNeuralNetworkModel.
IEEETransactionsonNeuralNetworks,20(1):61–80,2009.
Schlichtkrull,M.,Kipf,T.N.,Bloem,P.,vandenBerg,R.,Titov,I.,andWelling,M. ModelingRelationalData
withGraphConvolutionalNetworks. InTheSemanticWeb,2018.
Schoenholz,S.S.,Gilmer,J.,Ganguli,S.,andSohl-Dickstein,J.DeepInformationPropagation.InInternational
ConferenceonLearningRepresentations,2017.
Velicˇkovic´,P.,Cucurull,G.,Casanova,A.,Romero,A.,Liò,P.,andBengio,Y. GraphAttentionNetworks. In
InternationalConferenceonLearningRepresentations,2018.
7AcceptedatICLR2024(TinyPapersTrack)
Wang, J., Ma, A., Ma, Q., Xu, D., and Joshi, T. Inductive Inference of Gene Regulatory Network Using
SupervisedandSemi-SupervisedGraphNeuralNetworks. ComputationalandStructuralBiotechnology
Journal,18:3335–3343,2020.
Wu,F.,Souza,A.,Zhang,T.,Fifty,C.,Yu,T.,andWeinberger,K. SimplifyingGraphConvolutionalNetworks.
InInternationalConferenceonMachineLearning,2019.
Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., andYu, P.S. AComprehensiveSurveyonGraphNeural
Networks. IEEETransactionsonNeuralNetworksandLearningSystems,32(1):4–24,2021.
Xu,K.,Hu,W.,Leskovec,J.,andJegelka,S. HowPowerfulareGraphNeuralNetworks? InInternational
ConferenceonLearningRepresentations,2019.
8AcceptedatICLR2024(TinyPapersTrack)
Appendix
A TheoreticalDetails
A.1 Aggregationvs. Pooling
Themessageaggregationstepandthegraph-levelreadoutsteparecriticaloperationsinGNNs(Corsoetal.,2020).
Messagepassingongraphsinvolvesthepair-wiseexchangeofmessages,amessageaggregationmechanism,
whichcombinesmessagesfromallneighboringnodesintoonerepresentation,andsubsequentupdatesonnodes.
Thisprocesscanbelinkedtoconvolutionoperations(Wuetal.,2021;Kipf&Welling,2017;Bronsteinetal.,
2021).However,unliketraditionalconvolutions,wherethekernelsizeremainsfixed,themessageaggregationin
GNNsiscontingentonthenumberofneighboringnodesand,consequently,theincomingmessages (Wuetal.,
2021).
Forgraph-levelreadouts,thedistributedneuralrepresentationacrossthegraphneedstobefusedtoacommon
representationspace.ThisoperationisdenotedaspoolinginthecontextofGNNs.ForCNNspoolingoftenalso
referstotheaggregationstepitself.Wewillhoweverbemorestrictindistinguishingaggregationandpooling
hereandconsiderpoolingtobecausedbythestrideparameterofCNNs.Graph-levelreadoutpoolingoperations
canbegroupedintotopology-basedpooling,hierarchicalpooling,andglobalpooling(Leeetal.,2019).Global
poolingconsolidatesthegraphinformationintoasinglehiddenrepresentationbeforemakingfinalpredictions,
sosimilaroperationsasformessageaggregationcanbeusedhere.Advancedpoolingmechanismsconsiderthe
graphasadistribution,fromwhichnodesaresampled(Chenetal.,2023).
A.2 MLPSignalPropagation
In accordance with signal propagation literature (Schoenholz et al., 2017; Klambauer et al., 2017) we are
interestedinsignalpropagationofrandomlyinitializedneuralnetworks,i.e.,weassumedistributionsonweights
ofthesenetworks.Althoughitmightalsoseeminterestingtoknowaboutsignalpropagationbehavioratdifferent
timepointsduringtraining,thisismuchmoredifficulttostudy,sincethedistributionsofweightsmightthen
alsodependonthetrainingdata.However,anargumentforstudyingsignalpropagationatinitializationwould
bethatlearningmightnotworkatall(orstartwell)whensignalpropagationthroughoutthewholenetworkdoes
notevenwork(well)atinitialization.
Inordertoinvestigatetheforwarddynamicsofamessage-passingnetworkatinitializationtimewithsignal
propagationtheory,wetakethefollowingassumptions,assumingthecaseofϕtakingtwoarguments6. The
initialrepresentationofpairsofnoderepresentationshP = (h ,h )withi ̸= j followsadatadistribution
ij i j
hP ∼p withsomemeanE (cid:0) hP(cid:1) =µ andsomecovarianceCov (cid:0) hP(cid:1) =C .
ij data hP∼pdata ij hP hP∼pdata ij hP
We further assume a deep and broad MLP ϕ (.) with randomly sampled weights according to LeCun’s
w
initializationscheme(LeCunetal.,2012),w∼p (0,1/H),whereHisthefan-inofeachneuron,andwith
N
linearactivationinthelastlayer.SinceanMLPϕisameasurablefunction,m =ϕ (h ,h )isalsoarandom
ij w i j
variable.Then,centralresultsfromsignalpropagationtheory(Neal,1995;Schoenholzetal.,2017;Leeetal.,
2018;Hoedt&Klambauer,2023)implythatthedistributionofm atinitializationcanbeapproximatedbya
ij
standardnormaldistributionm ∼p (0,I)(Leeetal.,2018,Section2.2)andevenafixedpointatzero-mean
ij N
andunitvariancecanbeenforced(Klambaueretal.,2017;Luetal.,2023).Inpractice,batch-(Ioffe&Szegedy,
2015)orlayer-norm(Baetal.,2016)areoftenusedintheseMLPstopartlymaintainthesestatistics,i.e.zero
meanandunitvariance,alsoduringlearning.Weareawarethatthisapproximationonlyholdsatinitialization
andmightbeoverlysimplistic(Martensetal.,2021)(seeSection4).However,notethatweusethisassumption
onlytomakethepointofvariancepreservationoftheaggregationstep.Evenunderotherassumptionsonthe
distributionofm theargumentsaboutincreaseanddecreaseofvariancewouldhold.
ij
A.3 Proof Lemma1
Proof. Becausethevariablesz arecentered,wehave
n
(cid:34) N (cid:35) N
1 (cid:88) 1 (cid:88)
E[y]=E √ z = √ E[z ]=0=E[z]. (A1)
n n
N N
n=1 n=1
6Fortheone-argumentversionofϕ(wherethemessageiscomputedonlyfromthenoderepresentationh of
j
theneighboringnode)thelineofreasoningisalmostanalogous.
9AcceptedatICLR2024(TinyPapersTrack)
Furthermore,wehave
(cid:32) N (cid:33)2 (cid:34) N (cid:35)2
1 (cid:88) 1 (cid:88)
Var[y]=E √ z
n
−E √ z
n
= (A2)
N N
n=1 n=1
 (cid:32) N (cid:33)2  N N N 
=E N1 (cid:88) z
n
= N1 E(cid:88) z n2 +(cid:88) (cid:88) 2z nz m= (A3)
n=1 n=1 n=1m=1,m̸=n
1
= NE[z2]=Var[z ]=Var[z], (A4)
N n n
wherewehaveusedtheindependenceassumptionE[z z ]=E[z ]E[z ]=0andthatthez arecentered,
n m n m n
whichmeansthatE[z2]=Var[z ].
n n
A.4 Proof Lemma2
Proof. SincethenumberofuniqueelementsinX isboundedbyN,thereexistsabijectivemappingZ :X →
{1,...,N}assigninganaturalnumbertoeachx ∈ X. Thenanexampleofsuchafunctionf isaone-hot
encodingfunctionf(x)=e ,withe ∈RN beingastandardbasisvector,wherecomponentiofe ,
Z(x) Z(x) Z(x)
(cid:40)
0 for i̸=Z(x)
i.e.e [i]isdefinedase [i]:= .
Z(x) Z(x) 1 for i=Z(x)
Wedefineh(X)tobe:
1 (cid:88) 1 (cid:88)
h(X)= f(x)= e .
(cid:112) (cid:112) Z(x)
|X| |X|
x∈X x∈X
Summingupthecomponentsofh(X)yieldsthesquarerootofthecardinalityofX,i.e.theembeddingscontain
(cid:112)
informationonthecardinalityofX. Sinceweknow, |X|fromtheembedding,wecanjustmultiplythe
(cid:112)
embeddingh(X)with |X|toobtaintheoriginalmultiplicityofeachelementxinmultisetX. Thus,the
multisetX ⊂X canbeuniquelyreconstructedfromh(X),implyingthathisinjective.
Wenote,thatforMEANaggregation,i.e.,h˜(X)= 1 (cid:80) f(x),themultisetXcannotbereconstructedfrom
|X| x∈X
h˜(X),sinceinthatcasethecomponentsofh˜(X)sumupto1andtherefore,donotindicatethecardinalityofX
(e.g.,h({0,1})=(0.5,0.5)=h({0,0,1,1})).Incontrast,forh(X)= √1 (cid:80) f(x),theembeddings
|X| x∈X
containinformationonthecardinalityofX,whichislostforMEANaggregation.Multiplicationby|X|doesnot
workforMEANaggregationtoreconstructtheoriginalmultisetX,asnocardinalityinformationisstoredin
theembeddingh˜(X).Moregenerally,nofunctionf canbefoundsuchthath˜(X)isuniqueforeachmultiset
X ⊂X ofboundedsize(seeCorollary8inXuetal.(2019)).
A.5 Proof Lemma3
Proof. Becausethevariablesz arecentered,wehave
n
(cid:34) N (cid:35) N
1 (cid:88) 1 (cid:88)
E[y]=E c z = c E[z ]=0=E[z]. (A5)
C n n C n n
n=1 n=1
Furthermore,wehave
(cid:32) N (cid:33)2 (cid:34) N (cid:35)2
1 (cid:88) 1 (cid:88)
Var[y]=E
C
c nz
n
−E
C
c nz
n
= (A6)
n=1 n=1
 (cid:32) N (cid:33)2  N N N 
=E C1
2
(cid:88) c nz
n
= C1 E(cid:88) c2 nz n2 +(cid:88) (cid:88) 2c nc mz nz m= (A7)
n=1 n=1 n=1m=1,m̸=n
=
1 (cid:88)N
c2E[z2]=
1 (cid:16)(cid:88)N c2(cid:17)
E[z2]=Var[z ]=Var[z], (A8)
C2 n n (cid:80)N c2 i n n
n=1 i=1 i i=1
10AcceptedatICLR2024(TinyPapersTrack)
wherewehaveusedtheindependenceassumptionE[z z ]=E[z ]E[z ]=0andthatthez arecentered,
n m n m n
whichmeansthatE[z2]=Var[z ].
n n
Note,thatforcaseofuniformattentionweights,C
=(cid:113)
(cid:80)N c2
=(cid:114)
(cid:80)N
(cid:16) 1(cid:17)2
=
i=1 i i=1 N
(cid:113) √
N N1
2
= √1 N. Furthery = C1 (cid:80)N i=1c
i
z
i
= √1
1
(cid:80)N
i=1
N1 z
i
= N N1 (cid:80)N
i=1
z
i
= √1
N
(cid:80)N
i=1
z
i
is
N
obtained,whichisthesameasVPA.
Inthecasethatattentionfocusesonexactlyonevalue,i.e.,c = 1andc = 0 ∀i ̸= j givesC = 1,and
j i
y= 1 (cid:80)N c z =z .Cardinalityinformationislostinthiscase.However,theattentionmechanismmight
C i=1 i i j
belearnableandthereforenotconvergetothissolutioniflimitedexpressivityleadstolargerlossesduring
optimization.
11AcceptedatICLR2024(TinyPapersTrack)
B ExperimentalDetails&FurtherResults
B.1 ImplementationDetails
WeextendedourframeworkuponimplementationsasprovidedbyPyTorchGeometric(Fey&Lenssen(2019)).
Specifically,weusedthefollowingconvolutionallayers:GINConv(GIN),GraphConv(GCN),SGConv(SGC)
andGATConv(GAT).Weused5GNNlayersforGIN,GCNandGAT,respectively,andonelayerwithK =5
hopsforSGC.Thedimensionofthemessageswas64forallarchitectures.AnMLPwithonehiddenlayerwas
usedforclassificationwithahiddendimensionof64forGINand128forallothermodels.Furthermore,we
usedadropoutrateof0.5andthestandardAdamoptimizerwithalearningrateof0.001.
B.2 Extendedresults
TableB1showsresultsforthesocialdatasetsintheTUDatasetbenchmarkwiththenodedegreeencodedasnode
features.PleaserefertoSection3forfurtherdetailsandtoSection4foradiscussionoftheseresultscompared
tothoseinTable1.
IMDB-B IMDB-M RDT-B RDT-M5K COLLAB
GIN+SUM 72.5±4.5 50.8±4.1 81.5±1.7 47.5±2.4 82.2±1.7
GIN+MEAN 73.8±4.4 48.9±3.7 77.1±2.8 47.1±1.6 80.7±1.0
GIN+MAX 71.0±4.5 47.5±4.9 78.5±2.2 42.7±2.1 77.1±1.7
GIN+VPA 73.7±3.7 49.7±3.6 82.0±2.0 47.4±1.9 82.2±1.7
GCN+SUM 70.7±3.1 43.9±3.7 76.3±3.6 50.4±2.4 73.7±2.2
GCN+MEAN 71.9±5.2 51.3±3.4 71.0±2.5 46.3±2.3 80.6±1.0
GCN+MAX 62.9±3.5 43.1±4.2 63.4±5.0 30.6±2.6 74.8±1.6
GCN+VPA 73.6±5.5 50.5±2.7 80.6±3.4 47.9±2.3 81.3±1.5
SGC 72.9±3.9 50.6±3.5 81.0±2.4 49.0±1.9 81.3±1.8
SGC+VPA 72.6±3.7 49.4±3.6 81.5±2.3 47.8±2.8 80.5±1.1
GAT 73.9±3.4 50.2±4.0 78.3±3.0 47.0±2.7 81.2±1.4
GAT+VPA 71.7±4.9 49.6±6.1 79.1±2.3 47.5±1.7 79.5±1.5
TableB1: Resultsonthesocialdatasetsofthebenchmarksettingby(Xuetal.,2019). Inthisvariant
of the datasets, the number of neighbors of a node is encoded as a node feature. The compared
methodsareagainGINandGCNwithfourdifferentaggregationfunctionsandSGCandGATwith
theirtailor-madevariancepreservationmodifications.
12AcceptedatICLR2024(TinyPapersTrack)
B.3 LearningDynamics
Weinvestigatedthelearningdynamicsofthecomparedmethodsbasedonthetraininglosscurves(seeFigureB1).
ThelearningcurvesshowthatGINmodeltrainingconvergesfastwithMEAN,MAXandVPAandslowerwith
SUMaggregation,whichweattributetotheexplodingvarianceintheforwardpass.
IMDB-BINARY IMDB-MULTI REDDIT-BINARY
10 10
80
8 8
60
6 6
40
4 4
2 2 20
0 0 0
REDDIT-MULTI-5K COLLAB MUTAG
400
20 6
300 VPA 15 SUM
4 MEAN
200 10 MAX
100 5 2
0 0 0
PROTEINS PTC_FM NCI1
8 5
60
4
6
40 3
4
2
20 2 1
0 0 0
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
epoch epoch epoch
FigureB1: LearningCurvesofthe GIN architecturewithdifferentaggregationfunctionsonthe
TUDatasetbenchmarksusedbyXuetal.(2019)andwhichwereretrievedintheversionasprovided
byMorrisetal.(2020). NotethatthedefaulthyperparametersareadjustedtotheSUMaggregation
function. Nevertheless,thenetworktrainingconvergesfasterwithvariance-preservingaggregation
(VPA)comparedtoSUMaggregation. Atthesametime,VPAalsomaintainsexpressivity,whereas
MEANandMAXaggregationdecreasetheexpressivityofGNNs.
13
ssol
gniniart
ssol
gniniart
ssol
gniniart