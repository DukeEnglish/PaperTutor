Fact-Checking the Output of Large Language Models
via Token-Level Uncertainty Quantification
EkaterinaFadeeva3,4♢ AleksandrRubashevskii1,3♢ ArtemShelmanov1♢
SergeyPetrakov3 HaonanLi1 HamdyMubarak7 EvgeniiTsymbalov8 GlebKuzmin2,5
AlexanderPanchenko2,3 TimothyBaldwin1,6 PreslavNakov1 MaximPanov1
1MBZUAI 2AIRI 3CenterforArtificialIntelligenceTechnology 4HSEUniversity
5FRCCSCRAS 6TheUniversityofMelbourne 7QCRI 8IndependentResearcher
{ekaterina.fadeeva,sergey.petrakov}@skol.tech {kuzmin,panchenko}@airi.net
{aleksandr.rubashevskii,artem.shelmanov,haonan.li}@mbzuai.ac.ae
{maxim.panov,timothy.baldwin,preslav.nakov}@mbzuai.ac.ae hmubarak@hbku.edu.qa
Abstract questions(Thirunavukarasuetal.,2023),ortogen-
erate new content (Sun et al., 2023a). There has
Large language models (LLMs) are notori- been notable shift in user behavior, indicating an
ousforhallucinating,i.e.,producingerroneous increasing reliance on and trust in LLMs as pri-
claimsintheiroutput. Suchhallucinationscan
mary information sources, often over traditional
be dangerous, as occasional factual inaccura-
channels. However, a significant challenge that
cies in the generated text might be obscured
ariseswiththespreadofthesemodelsistheirten-
by the rest of the output being generally fac-
tual, making it extremely hard for the users dency to produce “hallucinations”, i.e., factually
to spot them. Current services that leverage incorrectgenerationsfeaturingmisleadinginforma-
LLMs usually do not provide any means for tion(Bangetal.,2023;Daleetal.,2023). Thisisa
detectingunreliablegenerations. Here,weaim side-effectofthewaymodernLLMsaredesigned
to bridge this gap. In particular, we propose
andtrained(KalaiandVempala,2023).
anovelfact-checkingandhallucinationdetec-
LLMhallucinationsareamajorconcernbecause
tionpipelinebasedontoken-leveluncertainty
the deceptive content at the the surface-level can
quantification. Uncertaintyscoresleveragein-
formationencapsulatedintheoutputofaneural beverycoherenceandpersuasive. Commonexam-
network or its layers to detect unreliable pre- plesincludethecreationoffictitiousbiographiesor
dictions, and we show that they can be used theassertionofunfoundedclaims. Thedangeris
to fact-check the atomic claims in the LLM thatafewoccasionalfalseclaimsmightbeeasily
output. Moreover, we present a novel token-
obscuredbyalargenumberoffactualstatements,
leveluncertaintyquantificationmethodthatre-
makingitextremelyhardforpeopletospotthem.
moves the impact of uncertainty about what
AshallucinationsinLLMoutputsarehardtoelim-
claimtogenerateonthecurrentstepandwhat
inate completely, users of such systems could be
surfaceformtouse. OurmethodClaimCon-
ditionedProbability(CCP)measuresonlythe informedviahighlightingsomepotentialcaveats
uncertaintyofparticularclaimvalueexpressed inthetext,andthisiswhereourapproachcanhelp.
bythemodel. Experimentsonthetaskofbiog- Fact-checking is a research direction that ad-
raphygenerationdemonstratestrongimprove-
dressesthisproblem. Itisusuallyapproachedusing
mentsforCCPcomparedtothebaselinesfor
complexsystemsthatleverageexternalknowledge
six different LLMs and three languages. Hu-
sources(Guoetal.,2022;Nakovetal.,2021;Wad-
manevaluationrevealsthatthefact-checking
pipelinebasedonuncertaintyquantificationis denetal.,2020). Thisintroducesproblemsrelated
competitivewithafact-checkingtoolthatlever- to the incomplete nature of such sources and no-
agesexternalknowledge. tableoverheadintermsofstoringtheknowledge.
Wearguethatinformationaboutwhetheragenera-
1 Introduction tionisahallucinationisencapsulatedinthemodel
outputitself,andcanbeextractedusinguncertainty
Large language models (LLMs) have become a
quantification(UQ)techniques(Galetal.,2016).
ubiquitous and versatile tool for addressing a va-
Thisavoidsimplementingcomplexandexpensive
riety of natural language processing (NLP) tasks.
fact-checkingsystemsthatrequireadditionalcom-
Peopleusethesemodelsfortasksincludinginfor-
putationaloverheadandrelyonexternalresources.
mationsearch(Sunetal.,2023b),toaskmedical
Prior work has mainly focused on quantifica-
♢Equalcontribution tion of uncertainty for the whole generated text
4202
raM
7
]LC.sc[
1v69640.3042:viXraandbeenmostlylimitedtotaskssuchasmachine • Allcodeanddatawillbemadeavailableupon
translation (Malinin and Gales, 2020), question acceptance.
answering (Kuhn et al., 2022), and text summa-
rization (van der Poel et al., 2022). However, 2 RelatedWork
the need for UQ of only a portion of the gener-
2.1 Fact-CheckingLLMGenerationsand
ationssubstantiallycomplicatestheproblem. We
DetectingHallucinations
approachitbyleveragingtoken-leveluncertainty
scoresandaggregatingthemintoclaim-levelscores. The problem of hallucinations has made fact-
Moreover, we introduce a new token-level uncer- checking of LLM outputs a prominent topic in
tainty score, namely claim-conditioned probabil- the research community, and resulted in a surge
ity(CCP),whichdemonstratesconfidentimprove- of work. Chern et al. (2023) present Factool – a
ments over several baselines for six LLMs and task and domain agnostic framework for halluci-
threelanguages. nationdetectionthatleveragesGPTforclaimex-
To the best of our knowledge, there is no pre- traction and verification. Manakul et al. (2023)
vious work that has investigated the quality of suggesttosamplemultipleoutputsfromblack-box
claim-level UQ techniques for LLM generation. LLMs and evaluate how similar the sampled re-
Therefore, for this purpose, we construct a novel sponses are using the external model. Varshney
benchmarkbasedonfact-checkingofbiographies etal.(2023)detectLLMhallucinationsbyextract-
of individuals generated using a range of LLMs. ingkeypartsofoutputusinganexternalmodeland
Note that different LLMs produce different out- estimating their uncertainty based on logits. The
puts,whichgenerallyhavehighervariabilitythan, most uncertain parts are verified using an exter-
e.g., outputs in such tasks as machine translation nal knowledge source. Pan et al. (2023) propose
or question answering. Therefore, we compare to fact-check complex statements by decompos-
the predictions and uncertainty scores to the re- ingthemintosimplersubtasksandgeneratingrea-
sultsofanautomaticexternalfact-checkingsystem soning programs to verify these statements. Min
FactScore (Min et al., 2023). Human evaluation etal.(2023)presentamethodologyforevaluating
verifiesthatourconstructedbenchmarkbasedon longLLM-generatedtextsbydecomposingthem
FactScorecanadequatelyevaluatetheperformance intosimpleatomicstatementsandfurtherverifying
oftheuncertaintyscores. themagainstsomeknowledgesource.
Ourcontributionsareasfollows: In contrast to previous work that leverages ex-
• We propose a novel framework for fact- ternal knowledge sources for fact-checking (a
checkingLLMgenerationsusingtoken-level database or another LLM), our work is the first
uncertaintyquantification. Weprovideapro- toinvestigatetoken-levelUQmethodsforthistask
cedure for efficiently estimating the uncer- while having access to the LLM and its outputs
taintyofatomicclaimsgeneratedbyawhite- only.
boxmodelandhighlightingpotentiallydecep-
2.2 UncertaintyQuantificationofLLM
tivefragmentsbymappingthembacktothe
Generation
originalresponse.
• Weproposeanovelmethodfortoken-levelun- UQtechniquesforLLMgenerationcanbeclassi-
certaintyquantificationthatoutperformsbase- fiedintofivemajorcategories(Fadeevaetal.,2023).
lines and can be used as a plug-in in a fact- Information-basedmethodsleveragetheprobabil-
checkingpipeline. itydistributionofgeneratedtokensandusuallydo
• Wedesignanovelapproachtoevaluationof notrequireadditionalmodels. Inthiscategory,we
token-levelUQmethodsforwhite-boxLLMs canincludemethodssuchasperplexity(Fomicheva
basedonfact-checking,whichcanbeapplied etal.,2020),meantokenentropy(Fomichevaetal.,
tootherwhite-boxLLMs. 2020),point-wisemutualinformation(Takayama
• Weprovideanempiricalandablationanaly- andArase,2019),andconditionalpoint-wisemu-
sisofmethodforfact-checkingofLLMgen- tualinformation(vanderPoeletal.,2022).
erations,andfindthattheuncertaintyscores Anothercategoryisbasedondensityestimation
weproducecanhelptospotclaimswithfac- of latent model representations. It includes Ma-
tualerrorsforsixLLMsoverthreelanguages: halanobisdistance(Leeetal.,2018)withvarious
English,ChineseandArabic. modifications(Renetal.,2023). ThedisadvantageMaximumProbability Claim-ConditionedProbability(ours)
TellmeabioofMadonna. TellmeabioofMadonna.
Madonna is an American singer, songwriter, actress, and business- MadonnaisanAmericansinger,songwriter,actress,andbusinesswoman.
woman. ShewasbornonAugust16,1958,inBayCity,Michigan,andis ShewasbornonAugust16,1958,inBayCity,Michigan,andisbestknownfor
bestknownforherprovocativeimageandmusic.Madonnahassoldover300 herprovocativeimageandmusic.Madonnahassoldover300millionrecords
millionrecordsworldwide,makingheroneofthebest-sellingmusicartists worldwide,makingheroneofthebest-sellingmusicartistsofalltime.Shehas
ofalltime.Shehaswonnumerousawards,including20GrammyAwards,a wonnumerousawards,including20GrammyAwards,aGoldenGlobeAward,
GoldenGlobeAward,andanEmmyAward.Madonnaisalsoknownforher andanEmmyAward.Madonnaisalsoknownforherphilanthropicworkand
philanthropicworkandheractivismonvarioussocialissues. heractivismonvarioussocialissues.
Figure1: VisualcomparisonofourproposedClaim-ConditionedProbabilitymethodtotheMaximumProbability
baseline. CCP accurately identifies the incorrectly specified number of awards (in red), whereas Maximum
Probabilityerroneouslyhighlightstheclaimthatisactuallycorrect.
of such approaches is the need for access to the 3 Fact-CheckingPipeline
LLM training data in order to fit external density
The fact-checking pipeline (see Figure 5) starts
models,whichisinfeasibleformostLLMs.
withsplittingageneratedtextintoatomicclaims,
EnsemblingandMonteCarlodropoutmethods
e.g. usingamuchsmallermodelfine-tunedforthis
arebasedonthelexicaldiversityofmultipleLLM
particulartask. Forexperimentalevaluationsinthis
outputs for a single query (Malinin and Gales,
work, we follow the approach of FactScore (Min
2020; Fomicheva et al., 2020). Their main draw-
etal.,2023),wheresplittingisimplementedviathe
backisthattheyrequiremanypredictions,which
ChatGPTAPI.
makesthemtoocomputationallyandmemoryin-
Each atomic claim is matched against the se-
tensiveforpracticalpurposes.
quence of tokens in the original text with corre-
IthasrecentlybeenshownthatLLMscanreflex- spondingprobabilitydistributions. Thenwecalcu-
ivelyestimatetheconfidenceoftheirgenerations late token-level uncertainty scores and aggregate
simplybyaskingthemselvesaboutthetruthfulness themintotheclaim-leveluncertainty.
of their output Kadavath et al. (2022). This can Finally, the claim-level uncertainty scores are
work better than analyzing the probability distri- comparedagainstathresholdobtainedonavalida-
bution for the original prediction, but requires a tionsettodeterminewhethertheclaimshouldbe
secondpassofinference,feedingtheoriginalout- highlightedfortheend-userasunreliable. Individ-
putasapartofthequery. ualtokenscanbeapartofmultipleatomicclaims.
Finally, there is a group of methods that lever- If the token belongs to a reliable and unreliable
agesdiversityofmeaningsthattheLLMgenerates claim at the same time, it is not highlighted. An
foragivenquery. Thisgroupincludessemanticen- examplevisualizationispresentedinFigure1.
tropy(Kuhnetal.,2022)andvariousscoresbased
4 UncertaintyQuantification
on the analysis of the similarity matrix between
outputs(Linetal.,2023).
Inthissection,wefirstprovidebackgroundoncom-
UQ methods can also be classified into white- monUQmethodsthatcanbeusedatthetokenlevel,
boxandblack-boxapproachesdependingonthere- thendelveintoourClaim-ConditionedProbabil-
quiredaccesstoLLMitselfanditsoutputs. Black- ity(CCP)token-levelmethod,andfinallydescribe
boxtechniquesdonotrequireanyotherinputex- howtoken-leveluncertaintiesareaggregatedintoa
ceptthemodeloutputs. claim-levelscore.
The method in our work can be attributed to Autoregressive language models generate text
the information-based group, and can be applied token by token. In this work, we will operate on
onlytowhite-boxLLMsbecauseitrequiresaccess the level of words and without loss of generality
totheprobabilitydistributionofthegeneratedto- supposethattheautoregressivedistributionateach
kens. Compared to other techniques, it offers a stepgeneratestherandomwordX ∼ P(· | x ),
j <j
novelapproachtopost-processingtheprobability where x is the text generated before the word
<j
distributionandisdesignedtoquantifyuncertainty at position j. We also denote by x the gener-
j
in parts of the output, such as atomic claims and ated word at position j and by x = x ◦ x
1:j <j j
individualwords. a text composed of words at positions 1 to j.Sentence:
She attended the School of Design, where she earned a Bachelor of Fine Arts Degree in painting.
^^^^^^^^ ^^ ^^^^ ^^^^ ^^^^^^ ^^ ^^^^^^^^
Fact: She earned a Bachelor of Fine Arts Degree in painting.
painting 49% NLI(Bachelor of Fine Arts Degree in painting,
Bachelor of Fine Arts Degree in painting) = entail
She attended the art 3% NLI(Bachelor of Fine Arts Degree in painting,
School of Design, Bachelor of Fine Arts Degree in art) = entail
where she earned a acting 7% NLI(Bachelor of Fine Arts Degree in painting, top-K
Bachelor of Fine Arts Bachelor of Fine Arts Degree in acting) = contra words
Degree in ...
sculpture 2% NLI(Bachelor of Fine Arts Degree in painting,
Bachelor of Fine Arts Degree in sculpture) = contra
1977 39% NLI(Bachelor of Fine Arts Degree in painting,
Bachelor of Fine Arts Degree in 1977) = neutral
CCP (painting) = (P(painting) + P(art)) / (P(painting) + P(art) + P(acting) + P(sculpture))
word
= 0.52 / 0.61 = 0.85
CCP (of) = CCP (in) = 1 (functional words)
word word
CCP (She earned a Bachelor of Fine Arts Degree in painting.) =
claim
-(CCP (Bachelor) * CCP (of) * CCP (Fine) * CCP (Arts) *
word word word word
CCP word(Degree) * CCP word(in) * CCP word(painting))
Figure2: ExampleofCCPcalculationontheVicuna13bgenerationforthewordpainting.
For example, in the case of greedy generation, Whilesomeworkhasreportedthatthistechnique
x = argmax P(x | x ), wherex isthemost outperforms other baselines, the big drawback is
j x <j j
probablerealizationofX . Letusalsodenoteby thatoneneedstoruntheoriginalLLMtwice.
j
C a set of indices of words corresponding to a
4.2 Claim-ConditionedProbability
particularatomicclaim.
Inthissubsection,weproposeanovelmethodfor
4.1 Claim-LevelUncertaintyQuantification
token-andclaim-leveluncertaintyquantification.
Baselines
4.2.1 MotivationandTheoreticalBackground
WenotethatforUQtobepractical,itneedstobe
WhenaLLMgeneratesanoutput,itfacesvarious
fast. Therefore,wedonotconsidermethodssuch
typesofuncertaintyreflectedinthetokendistribu-
asdeepensembles(Lakshminarayananetal.,2017),
tionofthecurrentgenerationstep(seeFigure4for
duetotheirsignificantcomputationaloverhead.
an example). We identify three distinct types of
Maximum Probability represents a basic ap-
uncertainty:
proachtoUQ,wherewesimplytreattheprobabil-
1. Claimtype/orderuncertainty: Whatclaim
ity of the most likely generation as a confidence
to generate on the current step? For exam-
score:
ple,onthecurrentstep,aLLMmighthesitate
(cid:89)
MP(C)=1− P(x |x ). (1)
j <j betweengeneratingayearofgraduationofa
j∈C
personandafieldofstudy. Adifferentorder
MaximumEntropyofatokenintheclaim:
ofclaims, missingclaims, ordifferenttypes
Ent(C)=maxH(·|x ), (2) of generated claims do not make produced
<j
j∈C
textlessfactual. Therefore,whenperforming
whereH(· | x )istheentropyoftheautoregres-
<j fact-checking,weshouldnottakethistypeof
sivedistributionofthecurrenttoken. Preliminary
uncertaintyintoaccount.
experimentsindicatedthatsimplygettingthemaxi-
2. Surfaceformuncertainty: Whatsynonyms
mumofthetokenentropiesinaclaimnoticeably
orhypernymstousewhengeneratingaclaim
outperformsotheraggregationtechniqueslikeav-
(e.g. “art” or “painting”)? Different surface
erage or minimum. It is also generally a slightly
forms also do not make the text less factual
betterbaselinethanperplexity.
— they might change the style, but not the
P(True),similarto(Kadavathetal.,2022),mea-
underlying meaning of the text. Therefore,
sures the uncertainty of the claim by asking the
thistypeofuncertaintyisalsonotrelevantfor
LLMitselfwhetherthegeneratedclaimistrueor
fact-checking.
not,ascalculatedbythemodelconfidenceofthe
3. Claim uncertainty: What specific piece of
firstgeneratedtokeny beingequalto“True”:
1 information to relay for a particular claim
P(True)=1−P(y =“True”). (3) type? Forexample,anLLMmightnotbesure
1whatfieldofstudytogenerate,producingato- follows:
kendistributionwithmultiplehighly-probable
(cid:80) P(xk |x )
variants,suchas“painting”,“acting”,“sculp-
CCP(x )=
xk j∈M(xj) j <j
. (5)
ture”. In a similar way, for the year of grad- j (cid:80) xl j∈CT(xj)P(xl j |x <j)
uation, a LLM might produce a distribution
Themeaningfunctionandtheconstructionofthe
withvariouspotentialyears. Thisuncertainty
claim type set of words can be implemented in
is relevant for fact-checking, because if the
various ways. We outline one approach in Sec-
modelisnotsureabouttheinformationitre-
tion4.2.2.
lays,theremightbeahighchanceofafactual
Previously-proposedUQmethodshavepartially
mistake.
accountedforsomeofthetypesofuncertaintyde-
Two out of the three types of uncertainty are
scribed above. For example, semantic entropy
irrelevant for fact-checking and only introduce
(Kuhn et al., 2023) accounts for uncertainty in
noise into the final score. We propose a new UQ
semantically-equivalentgroups,whichhelpstoal-
method that ignores the first two types of uncer-
leviatethesurface-formuncertainty. However,in
tainty and focuses only on the third one, namely
ourmethod,weadditionallyremovetheimpactof
Claim-ConditionedProbability(CCP):
claim-typeuncertainty.
(cid:0) (cid:1)
CCP(x j)=P Meaning(x 1:j)|x <j,ClaimType(x 1:j) . 4.2.2 Implementation
(4)
Here,ClaimType(x )representsaclaimtypeof We implement CCP using NLI at the word level.
1:j
thegeneratedsequencex andMeaning(x )is We compare the original claim and the claim, in
1:j 1:j
afunctionthatmapsx intoitsmeaninggiventhe whichthetargetwordisreplacedbyitsalternatives
j
previouswordsinasentencex ,sothatvarious fromtheautoregressivedistribution.
<j
surface forms with a similar meaning for x j are The distribution X j at the position j is ap-
(cid:110) (cid:111)K
mappedtoasinglecategoricalvariable. proximated by top-K alternatives xk with
j
Conditional probability can be rewritten using k=1
x1 ≡ x . We replace x with its alternatives xk
unconditionalprobabilities: j j j j
andobtainnewinstancesx ◦xk,k = 1,...,K.
<j j
Eachnewinstanceiscomparedagainsttheoriginal
P(cid:0) Meaning(x 1:j)|ClaimType(x 1:j),x <j(cid:1) predictionx 1:j = x <j◦x j usinganNLImodel. We
=
P(Meaning(x 1:j),ClaimType(x 1:j)|x <j)
.
defineNLI(xk j,x j) := NLI(x
<j
◦xk j,x 1:j),where
P(ClaimType(x )|x ) NLI(x ◦xk,x )meansapplicationoftheNLI
1:j <j <j j 1:j
modeltothesentencesx ◦xk andx . Theout-
<j j 1:j
Assumingthateachmeaninginaworddistribution
come of NLI procedure is one of three classes:
cancorrespondtoonlyasingleclaimtype,thejoint
entail (‘e’), contradict (‘c’) or neutral (‘n’). If
probability is the same as the meaning probabil-
the new instance entails the original prediction
ity P(Meaning(x ),ClaimType(x ) | x ) =
1:j 1:j <j NLI(xk,x ) = ‘e’,thenweconsiderx ◦xk has
P(Meaning(x ) | x ). j j <j j
1:j <j thesamemeaningwithx (xk ∈ M(x ))andcor-
Meaningprobabilityinturnsumsfromtheprob- 1:j j j
respondstothesameclaimtype(xk ∈ CT(x )). If
abilities of word alternatives xk that correspond j j
j thenewinstancecontradictstheoriginalprediction
tothesamemeaning: P(Meaning(x ) | x ) =
(cid:80) P(xk | x ),wherewes1 a:j ythat< xj k ∈ NLI(xk j,x j) = ‘c’,thenweconsiderx <j◦xk j hasa
Mx (xk j∈M )i( fx Mj) eaninj g(x<j
) = Meaning(x
◦xkj
).
differentmeaningwithx
1:j
(xk
j
∈/ M(x j)),butcor-
j 1:j <j j responds to the same claim type (xk ∈ CT(x )).
j j
In the same way, the probability of a claim
Otherwise,ifthenewinstanceisneutralw.r.t. the
type sums from probabilities of all mean-
originalpredictionNLI(xk,x ) = ‘n’,thenwecon-
j j
ings and transitionally from probabilities
siderthatx ◦xk doesnotcorrespondtothesame
of words that correspond to the particu- <j j
lar claim type: P(cid:0) ClaimType(x )(cid:1) = claim type as x 1:j (xk j ∈/ CT(x j)). Thus, Equa-
1:j
(cid:80) P(xl | x ), where we de- tion(5)forCCPcanbewrittenasfollows:
xl j∈CT(xj) j <j
note by xl ∈ CT(x ) an event such that (cid:80) P(xk |x )
ClaimType(j
x 1:j) =
j
ClaimType(x <j ◦ xl j). CCP word(x j)= (cid:80)
k:NLk I:N (xL
k
jI ,( xx jk
j
), ∈xj {) ‘= e‘ ’e ,‘’ c’}Pj
(xk j
< |j
x <j).
Therefore, Equation (4) can be rewritten as (6)Forpracticalconsiderations,weconsiderthatCCP We then decompose the generated text into
for function words is always equal to 1. In our atomic claims using ChatGPT. For each atomic
experiments, we base this determination on the statement,wemapallitswordsbacktogenerated
stopwordlistfromNLTK(BirdandLoper,2004). texttoaccessthecorrespondingtokenlogits. Not
WenotethatmosttransformerLLMsgenerate allclaimsperfectlymatchtotheoriginalresponse.
sub-word tokens instead of whole words. To ob- For example, for Vicuna 13b around 5% of all
tainthedistributionforwholewords,wegenerate claims do not successfully match because Chat-
oneormultipletokensusingbeamsearchwithK GPTabstainedtorespondoroutputtedwordsnot
beams. presentintheoriginal. Weconsideronlysuccess-
ToobtainCCP-basedclaim-leveluncertainty,we fullymatchedclaims.
simplytaketheproductofCCPsofeachwordfrom
For English, we are able to perform annota-
theclaimC:
tioncompletelyautomatically: eachatomicclaim
(cid:89) is classified by FactScore as supported or not
CCP claim(C)=1− CCP word(x j). (7)
j∈C supported. The underlying FactScore model is
An example of calculating the CCP for a fact is
“retrieval+ChatGPT”withadumpofWikipedia
presentedinFigure2. Otherdetailedexamplesof articlesasanexternalknowledgesource. Wealso
CCPcalculationareavailableinAppendixA. manuallyannotated100Englishbiographiespro-
ducedbytheVicuna13bmodel,183claimsfrom
5 BenchmarkforEvaluationof Arabic biographies, and 1,603 claims in Chinese.
Claim-LevelUQMethods Eachofthestatementswascheckedbytwoanno-
tatorswithaccesstothecorrespondingWikipedia
We evaluate claim-level UQ techniques and their
article. Thefinallabelissetto‘supported’onlyif
abilitytospothallucinationsonthetaskofgener-
bothannotatorslabelitasso.
atingbiographies. Tomaketheevaluationasclose
The statistics of the resulting datasets with
as possible to the real-world scenario, we allow
automatically-labeled claims in English are pre-
unrestrictedgenerationofbiographiesfromLLMs,
sented in Table 4, and the statistics of manually
samplingmultipleoutputsfromagivenLLM.This
annotateddatasetsispresentedinTable5. Aswe
complicates the automatic evaluation of the fact-
cansee,themajorityofclaimsinthemodeloutput
checkingpipeline,becauseobtaininggoldstandard
arecorrect,with6-29%ofhallucinations.
annotation require that we manually annotate all
outputs from each model. Therefore, in addition Theautomaticdatasetconstructionprocessusing
tomanualannotation,weannotateclaimsingener- FactScoreisillustratedinFigure6intheAppendix.
atedtextsautomaticallyusingtheFactScorefact-
checkingtool(Minetal.,2023),whichhasaccess
toanexternalknowledgesource. Suchanapproach 6 Experiments
enables completely automatic evaluation and al-
lowsustoscaleupourexperiments.
6.1 ExperimentalSetup
WegenerateLLMresponsesinEnglish,Chinese,
and Arabic to 100 biography prompts. The typi-
Fact-checkingofatomicclaimsisframedasabi-
calbiographypromptisGivemeabiographyfor
naryclassificationtask,whereuncertaintyscores
<personname>indifferentlanguages. Thesetof
serveaspredictorsofnon-factuality,andFactScore
peoplewasgeneratedbyaskingGPT-4tolistthe
orhumanlabelsserveasgroundtruth. Theevalua-
most famous people since 1900. The maximum
tionmetricisROC-AUC.
generation length is set to 256 tokens. If the last
sentenceofthegenerationisunfinished(i.e.does FortheCCPmethod,NLIscoresarecalculated
notendwithanypunctuation),itisdiscarded. We usingtheDeBERTa-largemodel(Heetal.,2021)
generate responses for the following LLMs: (for fine-tunedforthistask. Thenumberofalternatives
English)Vicuna13b(Zhengetal.,2023),Mistral usedinCCPisK = 10,exceptforGPT-3.5-turbo
7b (Jiang et al., 2023), Jais 13b (Sengupta et al., andGPT-4,astheOpenAIAPIdoesnotallowus
2023), and GPT-3.5-turbo (Ouyang et al., 2022); toretrievemorethan5alternativesfromthetoken
(for Chinese) Yi 6B (Yi, 2023); (for Arabic) Jais distribution. Detailsonexpensesarepresentedin
13bandGPT-4. AppendixH.Model Mistral7b Vicuna13b Jais13b GPT-3.5-turbo
CCP(ours) 0.66 0.66 0.71 0.60
±0.03 ±0.04 ±0.05 ±0.04
MaximumProb. 0.59 0.60 0.64 0.54
±0.03 ±0.05 ±0.05 ±0.05
TokenEntropy 0.60 0.60 0.63 0.53
±0.02 ±0.06 ±0.06 ±0.03
P(True) 0.53 0.61 0.55 0.53
±0.02 ±0.03 ±0.05 ±0.04
Table1: ROC-AUCofclaim-levelUQmethodswithFactScorelabelsasthegroundtruth(English).
Mistral 7b Vicuna 13b
0.8 0.8 0.77
00 .. 67 0.650.67
0.73
0.610.62
0.560.69
0.59 0.6
0.66 00 .. 67
0.730.71
0.62
0.610.620.630.66
0.6 0.6
0.610.66
0.52 0.53
0.5 0.5
0.4 0.4
0.3 0.3
0.2 0.2
0.1 0.1
0.0 0.0
First 2 sentences First 5 sentences No restriction First 2 sentences First 5 sentences No restriction
Maximum Probability Token Entropy Maximum Probability Token Entropy
P(True) CCP (ours) P(True) CCP (ours)
Jais 13b GPT-3.5-turbo
0.8 0.8
0.72 0.72 0.71
0.7 0.67 0.63 0.650.64 0.640.63 0.7
00 .. 56 0.52 0.54 0.55 00 .. 56 0.47 0.5 0.55 0.49 0.6 0.58 0.55 0.6 0.540.530.53 0.6
0.4 0.4
0.3 0.3
0.2 0.2
0.1 0.1
0.0 0.0
First 2 sentences First 5 sentences No restriction First 2 sentences First 5 sentences No restriction
Maximum Probability Token Entropy Maximum Probability Token Entropy (5 samples)
P(True) CCP (ours) P(True) CCP (ours)
Figure3: ROC-AUCofclaim-levelUQmethodsbasedonFactScorelabels,aggregatedintobinswhenconsidering
onlyfactsfromthefirst2,5,andallsentences(English).
6.2 ResultsforEnglishontheFactScore model in Figure 8 in the Appendix. For the ma-
Annotation jority of cases, CCP outperforms other methods
exceptwhenweconsideronlythefirsttwoandthe
ThemainresultsoftheexperimentswithFactScore firstfivesentencesgeneratedbyGPT-3.5-turbo.
labels for English are presented in Table 1. The
proposed CCP method outperforms all other UQ 6.3 MultilingualResultsonManual
techniquesforeachoftheconsideredLLMs. The Annotation
bestoverallresultisobtainedforJais13b,where
Multilingual results based on manual annotation
CCP outperforms the closest competitor by 0.07
arepresentedinTables2and3andFigure9inthe
ROC-AUC.
Appendix.
Wefurtheranalyzetheperformancebyplotting Using manual annotation for English, we can
it as a function of the number of considered sen- alsoevaluatetheperformanceofFactScoreitself.
tencesfromthebeginningofthegeneratedtext(see The accuracy of automatic annotation is 77.2%
Figure3). Wenotethatthequalityofeachmethod and ROC-AUC is 0.72. A detailed analysis of
decreasesasthenumberofconsideredsentencesin- FactScore mistakes is presented in Appendix F.
creases. Thismaybeduetothefactthatthemodel Forhumanannotation,theperformanceofallUQ
tendstostarttheresponsewitheasy-to-knowand methods appears to be even slightly higher than
hencereliableclaimsandasitgeneratesmoretext, for the labels obtained using FactScore (Table
it has to produce more complex and less reliable 2) on the same set. Moreover, CCP outperforms
statements,whichisillustratedfortheVicuna13b FactScoreitselfby0.06ROCAUC.Theseresults
CUA-COR
CUA-COR
CUA-COR
CUA-CORGround-Truth Human FactScore claimthatprecedesthewordinquestion,weexper-
Method annotation annotation
imentwithasingletargetwordwithoutcontextand
CCP(ours) 0.78 0.74
MaximumProb. 0.67 0.65 the whole sentence that precedes the target word.
TokenEntropy 0.69 0.65 All variants demonstrate lower performance. No
P(True) 0.68 0.65
contextleadstoadropinROC-AUCof0.02,and
FactScore 0.72 –
longercontextsbymorethan0.07.
Table2: ROC-AUCofclaim-levelUQmethodswithhu- (3) Functional words handling. The results
manannotationandFactScoreannotationastheground
inFigure8showthatexcludingfunctionalwords
truth(English,Vicuna13bmodel).
inCCPhelpstoimprovetheperformanceby0.03
ROC-AUC.Thisapproachalsoslightlyimproves
Yi6b, Jais13b, GPT-4,
Model
Chinese Arabic Arabic the maximumprobability baseline, but its perfor-
CCP(ours) 0.64±0.03 0.66±0.02 0.56±0.05 manceisstillmuchlower.
MaximumProb. 0.52±0.03 0.59±0.02 0.55±0.08
(4) The number of alternatives K. Taking
TokenEntropy 0.57±0.05 0.61±0.02 0.48±0.06
P(True) 0.63±0.04 0.61±0.02 0.50±0.06 K = 5 alternatives instead of K = 10 in CCP
decreasesROC-AUCby0.02. Figure7alsodemon-
Table3: ROC-AUCofclaim-levelUQmethodswith
strates that further decreasing K reduces the per-
manualannotationasthegroundtruth.
formanceevenmore. WhenincreasingK,theper-
formanceplateausatK = 8.
demonstrate that in the task of detecting LLM
hallucinations, UQ techniques can be a strong 6.5 QualitativeAnalysis
alternativetofact-checkingtoolswithanexternal
Inqualitativeanalysisofuncertaintyscoresforvar-
knowledgesource.
iousgenerationsandmodels,wenotethatthemax-
imalprobabilitybaselineproducesalotmorefalse
ForChineseandArabic,CCPalsooutperforms
positivesthanCCP.ThishappensbecauseCCPig-
thebaselines. FortheChineseYi6bmodel,from
noressometypesofuncertainty,focusingonlyon
Figure9,wecanseethatthegapbetweenCCPand
the claim uncertainty. In some cases, CCP also
thebaselinesisespeciallybigfortheseveralfirst
finds false claims overlooked by other methods
claims. WhenconsideringmoreclaimsintheLLM
becauseignoringcertaintypesofuncertaintyalso
output,CCPstillclearlyoutperformsthemaximum
allows us to reduce the cut-off threshold used to
probabilitybaseline,buttheP(True)baselinesub-
markclaims. AnexamplewherewecompareCCP
stantially reduces the gap. For Jais and Arabic,
withmaximalprobabilityispresentedinFigure12,
CCP outperforms the closest competitor by 0.05
andmoreexamplescanbefoundinAppendixG.
ROCAUC.OnGPT-4generationsforArabic,the
metricsforallmethodsarelow. Weexplainthisob- 7 Conclusion
servationbyasmallratioofnon-factualclaimsin
theGPT-4output. Comparedtotheothermodels, Wepresentedanovelapproachtofact-checkingand
GPT-4hallucinatesmuchless. hallucinationdetectionbasedontoken-leveluncer-
taintyquantification. Accordingtohumanevalua-
6.4 AblationStudies
tion,ourmethodiscompetitivewithFactScore,a
Inthissection,weanalyzetheinfluenceofvarious fact-checkingtoolthatleveragesanexternalknowl-
CPPcomponentsonEnglishbiographiesannotated edge source: we achieve similar or better results
withFactScore(Tables6–8). Detailsoftheexperi- withaccesstoonlyLLMoutputs.
mentalsetupforeachablationstudyarepresented Ourproposedtoken-levelUQmethod,claimcon-
inAppendixD. ditionedprobability,outperformsanumberofbase-
(1) Aggregation of CCP for obtaining lines. In this method, we post-process the word
word
CCP . Besidestheproductofprobabilities,we distribution to mitigate the impact of uncertainty
claim
also tried the normalized product, minimum and relatedtothevariabilityofsurfaceformsandun-
averageprobability. Alltheseapproachesperform certaintyaboutwhatclaimtypetogenerateonthe
slightlyworsethantheproduct(seeTable6). currentstep. Intheconstructedbenchmark,where
(2) NLI context. We analyze what context is wedetecthallucinationsinbiographies,CCPout-
sufficient for NLI in CCP (Table 7). In addition performs other methods for six LLMs, including
tothestandardvariantinCCP,wherewekeepthe GPT-3.5-turboandGPT-4,andthreelanguages.Limitations References
Westrovetoconductourresearchaccordingtothe YejinBang,SamuelCahyawijaya,NayeonLee,Wen-
liangDai,DanSu,BryanWilie,HolyLovenia,Ziwei
bestestablishedexperimentalandmethodological
Ji,TiezhengYu,WillyChung,QuyetV.Do,YanXu,
practices,yetweidentifyseveralpotentiallimita-
andPascaleFung.2023. Amultitask,multilingual,
tionsofthisworkbelow. multimodalevaluationofChatGPTonreasoning,hal-
First,atthecoreoftheapproachliesatextentail- lucination,andinteractivity. InProceedingsofthe
13thInternationalJointConferenceonNaturalLan-
mentclassifier. Asitwasoriginallypre-trainedfor
guageProcessingandthe3rdConferenceoftheAsia-
aslightlydifferentuse-case,morecarefulanalysis
PacificChapteroftheAssociationforComputational
ofitsperformanceondiversedomainsandgenres Linguistics(Volume1:LongPapers),pages675–718,
shouldbecarriedout. NusaDua,Bali.AssociationforComputationalLin-
guistics.
Second, the current implementation of the
method makes use of OpenAI’s GPT models for
StevenBirdandEdwardLoper.2004. NLTK:Thenatu-
text segmentation and extraction of atomic facts, rallanguagetoolkit. InProceedingsoftheACLIn-
similarlytoFactScore,whichmaynotbepractical teractivePosterandDemonstrationSessions,pages
214–217.
inrealapplications. Replacingthesecomponents
with cheaper open models should be feasible in
I-ChunChern,SteffiChern,ShiqiChen,WeizheYuan,
principle,butisleftforfuturework. KehuaFeng,ChuntingZhou,JunxianHe,Graham
Third,partofourexperimentalresultsrelyona Neubig, Pengfei Liu, et al. 2023. Factool: Factu-
ality detection in generative ai–a tool augmented
human evaluation, which may be subjective. We
frameworkformulti-taskandmulti-domainscenar-
tried to mitigate this by creating detailed instruc-
ios. arXivpreprintarXiv:2307.13528.
tions for the annotators, yet a larger scale study
withlargeroverlapmaybedesirable. DavidDale, ElenaVoita, LoicBarrault, andMartaR.
Costa-jussà.2023. Detectingandmitigatinghalluci-
Fourth,ouruncertaintyquantificationisbasedon
nationsinmachinetranslation: Modelinternalwork-
tokens,whiletakingintoaccountalsolargerunits
ingsalonedowell,sentencesimilarityEvenbetter.
asnounorverbphrasesasbasicunitsofanalysis In Proceedings of the 61st Annual Meeting of the
maybebettermotivatedlinguistically. AssociationforComputationalLinguistics(Volume
1: LongPapers),pages36–50,Toronto,Canada.As-
Finally, our approach only detects potentially
sociationforComputationalLinguistics.
spurious generations. A prominent direction for
future research is to modify the generation of an Ekaterina Fadeeva, Roman Vashurin, Akim Tsvigun,
LLMtoexcludesuchspans,whileensuringfluency ArtemVazhentsev,SergeyPetrakov,KirillFedyanin,
Daniil Vasilev, Elizaveta Goncharova, Alexander
ofageneratedtext: asimpleremovalofuncertain
Panchenko, Maxim Panov, Timothy Baldwin, and
claimsmayresultinincoherentsentences.
Artem Shelmanov. 2023. LM-polygraph: Uncer-
taintyestimationforlanguagemodels. InProceed-
EthicalConsiderations ingsofthe2023ConferenceonEmpiricalMethods
inNaturalLanguageProcessing: SystemDemonstra-
We would like to warn that our method for fact- tions,pages446–461.
checking is based on uncertainty quantification,
Marina Fomicheva, Shuo Sun, Lisa Yankovskaya,
andthusitisnotbullet-proof,asitonlyreflectsthe
Frédéric Blain, Francisco Guzmán, Mark Fishel,
internal belief of the LLM. Thus, it is of limited
NikolaosAletras,VishravChaudhary,andLuciaSpe-
utility when it comes to claims that are beyond cia.2020. Unsupervisedqualityestimationforneural
thetimecutoffofthemodel. Moreover,theLLM machinetranslation. TransactionsoftheAssociation
forComputationalLinguistics,8:539–555.
might be trained on factually false data, which is
beyond our control. It could be also tricked by
Yarin Gal et al. 2016. Uncertainty in deep learning.
variationsintheprompt. Ph.D.thesis,UniversityofCambridge.
Wealsowarnthatoursolutiondoesnoteliminate
ZhijiangGuo,MichaelSchlichtkrull,andAndreasVla-
hallucinations;instead,itcanbeusedtohighlight
chos. 2022. A survey on automated fact-checking.
riskypartsofatext,forhumanstotakeintoaccount.
TransactionsoftheAssociationforComputational
Ourintendeduseistowardsraisingawarenessand Linguistics,10:178–206.
promotinghuman-machinecollaboration.
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and
Wewarnthatourmethodcanbemisusedtoun-
Weizhu Chen. 2021. Deberta: decoding-enhanced
fairlymoderatecontent. Thus,weaskresearchers
bertwithdisentangledattention. In9thInternational
andpotentialuserstoexercisecaution. ConferenceonLearningRepresentations,ICLR.AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen- PotsaweeManakul,AdianLiusie,andMarkJFGales.
sch,ChrisBamford,DevendraSinghChaplot,Diego 2023. Selfcheckgpt: Zero-resource black-box hal-
delasCasas,FlorianBressand,GiannaLengyel,Guil- lucination detection for generative large language
laumeLample,LucileSaulnier,LélioRenardLavaud, models. arXivpreprintarXiv:2303.08896.
Marie-AnneLachaux,PierreStock,TevenLeScao,
SewonMin,KalpeshKrishna,XinxiLyu,MikeLewis,
Thibaut Lavril, Thomas Wang, Timothée Lacroix,
and William El Sayed. 2023. Mistral 7b. CoRR, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-
moyer,andHannanehHajishirzi.2023. FActScore:
abs/2310.06825.
Fine-grainedatomicevaluationoffactualprecision
inlongformtextgeneration. InProceedingsofthe
SauravKadavath,TomConerly,AmandaAskell,Tom
2023ConferenceonEmpiricalMethodsinNatural
Henighan, Dawn Drain, Ethan Perez, Nicholas
LanguageProcessing,pages12076–12100.
Schiefer,ZacHatfield-Dodds,NovaDasSarma,Eli
Tran-Johnson,ScottJohnston,SheerElShowk,Andy
Preslav Nakov, Giovanni Da San Martino, Tamer
Jones, Nelson Elhage, Tristan Hume, Anna Chen,
Elsayed, Alberto Barrón-Cedeno, Rubén Míguez,
Yuntao Bai, Sam Bowman, Stanislav Fort, Deep
ShadenShaar,FirojAlam,FatimaHaouari,Maram
Ganguli, Danny Hernandez, Josh Jacobson, Jack-
Hasanain,NikolayBabulkov,etal.2021. Theclef-
son Kernion, Shauna Kravec, Liane Lovitt, Ka-
2021 checkthat! lab on detecting check-worthy
malNdousse,CatherineOlsson,SamRinger,Dario
claims, previously fact-checked claims, and fake
Amodei,TomBrown,JackClark,NicholasJoseph,
news. InAdvancesinInformationRetrieval: 43rd
BenMann,SamMcCandlish,ChrisOlah,andJared
European Conference on IR Research, ECIR 2021,
Kaplan.2022. Languagemodels(mostly)knowwhat
VirtualEvent,March28–April1,2021,Proceedings,
theyknow. CoRR,abs/2207.05221.
PartII43,pages639–649.Springer.
Adam Tauman Kalai and Santosh S Vempala. 2023. LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
Calibratedlanguagemodelsmusthallucinate. arXiv CarrollWainwright,PamelaMishkin,ChongZhang,
preprintarXiv:2311.14648. SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
2022. Training languagemodelsto followinstruc-
LorenzKuhn,YarinGal,andSebastianFarquhar.2022. tions with human feedback. Advances in Neural
Semanticuncertainty: Linguisticinvariancesforun- InformationProcessingSystems,35:27730–27744.
certaintyestimationinnaturallanguagegeneration.
LiangmingPan,XiaobaoWu,XinyuanLu,AnhTuan
InTheEleventhInternationalConferenceonLearn-
Luu,WilliamYangWang,Min-YenKan,andPreslav
ingRepresentations.
Nakov. 2023. Fact-checking complex claims with
program-guided reasoning. In Proceedings of the
LorenzKuhn,YarinGal,andSebastianFarquhar.2023.
61stAnnualMeetingoftheAssociationforCompu-
Semanticuncertainty: Linguisticinvariancesforun-
tationalLinguistics(Volume1: LongPapers),pages
certaintyestimationinnaturallanguagegeneration.
6981–7004.
InTheEleventhInternationalConferenceonLearn-
ing Representations, ICLR 2023, Kigali, Rwanda, JieRen,JiamingLuo,YaoZhao,KundanKrishna,Mo-
May1-5,2023.OpenReview.net. hammad Saleh, Balaji Lakshminarayanan, and Pe-
ter J Liu. 2023. Out-of-distribution detection and
Balaji Lakshminarayanan, Alexander Pritzel, and selective generation for conditional language mod-
Charles Blundell. 2017. Simple and scalable pre- els. In The Eleventh International Conference on
dictiveuncertaintyestimationusingdeepensembles. LearningRepresentations.
InProceedingsofthe31stInternationalConference
Neha Sengupta, Sunil Kumar Sahu, Bokang Jia,
onNeuralInformationProcessingSystems,NeurIPS
SatheeshKatipomu,HaonanLi,FajriKoto,William
2017,page6405–6416,RedHook,NY,USA.Curran
Marshall, Gurpreet Gosal, Cynthia Liu, Zhiming
AssociatesInc.
Chen, Osama Mohammed Afzal, Samta Kamboj,
OnkarPandit,RahulPal,LalitPradhan,ZainMuham-
KiminLee,KibokLee,HonglakLee,andJinwooShin.
mad Mujahid, Massa Baali, Xudong Han, Son-
2018. Asimpleunifiedframeworkfordetectingout-
dosMahmoudBsharat, AlhamFikriAji, Zhiqiang
of-distribution samples and adversarial attacks. In
Shen,ZhengzhongLiu,NataliaVassilieva,JoelHes-
AdvancesinNeuralInformationProcessingSystems,
tness,AndyHock,AndrewFeldman,JonathanLee,
volume31.CurranAssociates,Inc.
Andrew Jackson, Hector Xuguang Ren, Preslav
Nakov, Timothy Baldwin, and Eric Xing. 2023.
ZhenLin,ShubhenduTrivedi,andJimengSun.2023.
Jais and jais-chat: Arabic-centric foundation and
Generating with confidence: Uncertainty quantifi-
instruction-tuned open generative large language
cationforblack-boxlargelanguagemodels. arXiv
models. CoRR,abs/2308.16149.
preprintarXiv:2305.19187.
Jiao Sun, Yufei Tian, Wangchunshu Zhou, Nan Xu,
Andrey Malinin and Mark Gales. 2020. Uncertainty QianHu,RahulGupta,JohnWieting,NanyunPeng,
estimationinautoregressivestructuredprediction. In andXuezheMa.2023a. Evaluatinglargelanguage
International Conference on Learning Representa- modelsoncontrolledgenerationtasks. InProceed-
tions. ingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages3155–3168,
Singapore.AssociationforComputationalLinguis-
tics.
Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren,
Dawei Yin, and Zhaochun Ren. 2023b. Is chatgpt
goodatsearch? investigatinglargelanguagemodels
asre-rankingagent. ArXiv,abs/2304.09542.
JunyaTakayamaandYukiArase.2019. Relevantand
informativeresponsegenerationusingpointwisemu-
tualinformation. InProceedingsoftheFirstWork-
shoponNLPforConversationalAI,pages133–138.
AssociationforComputationalLinguistics.
Arun James Thirunavukarasu, Darren Shu Jeng Ting,
KabilanElangovan,LauraGutierrez,TingFangTan,
and Daniel Shu Wei Ting. 2023. Large language
modelsinmedicine. Naturemedicine,29(8):1930–
1940.
LiamvanderPoel,RyanCotterell,andClaraMeister.
2022. Mutualinformationalleviateshallucinations
inabstractivesummarization. InProceedingsofthe
2022ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages5956–5965.Association
forComputationalLinguistics.
NeerajVarshney,WenlinYao,HongmingZhang,Jian-
shuChen,andDongYu.2023. Astitchintimesaves
nine: Detecting and mitigating hallucinations of
llmsbyvalidatinglow-confidencegeneration. arXiv
preprintarXiv:2307.03987.
David Wadden, Shanchuan Lin, Kyle Lo, Lucy Lu
Wang, Madeleine van Zuylen, Arman Cohan, and
HannanehHajishirzi.2020. Factorfiction:Verifying
scientific claims. In Proceedings of the 2020 Con-
ferenceonEmpiricalMethodsinNaturalLanguage
Processing(EMNLP),pages7534–7550.
Yi. 2023. A series of large language models trained
from scratch by developers at 01-ai. https://
github.com/01-ai/Yi.
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
ZhuohanLi,DachengLi,Eric.PXing,HaoZhang,
Joseph E. Gonzalez, and Ion Stoica. 2023. Judg-
ingllm-as-a-judgewithmt-benchandchatbotarena.
CoRR,abs/2306.05685.A ExampleofCCPCalculation
Figure4showstheexampleofmodelgenerationandNLIclassesonthesentencefromdatasetsample. In
thisexample,CCPestimatestheClaimUncertainty,nottakingintoaccountClaimorderandSurfaceform
uncertainties,inresultproducingmorereasonableuncertaintyestimatethanMaximumProbabilitydoes.
He died 47.0% in a plane 97.7% crash 99.7% in 1928 36.5%
passed 1.7% shipwreck 0.7% accident 0.2% 1930 21.1%
...
perished 1.0% crash 0.4% 1929 4.0%
...
lost 1.4% 1933 2.6%
disappeared 0.8% 1931 1.0%
also 8.0% 1956 0.4%
later 1.3% Norway 8.0%
...
tragically 0.4%
...
Figure4: ExamplepartofthebibliographygeneratedbyVicuna13bmodel. Thewordsfromgreedy-generated
sentencearepresentedsequentiallyonthetop,eachnon-functionalwordissupplementedwithitsalternativesand
autoregressivegenerationprobabilities. Wordswithprobabilitylessthan0.1%areomitted.
Green-coloredwordsindicateentailmenttothegreedygeneratedword,redcolorindicatescontradiction,andyellow
colorindicatesneutralNLIclass.
Onthelastposition,CCPsuccessfullydistinguishesNorwayfromotheryear-relatedwords,anddoesnotconsider
itsprobabilityinthefinalformula.
B Fact-CheckingPipeline
Match Token Logits Mapped
Token Logits Logits with to Atomic
Atomic Claims Claims
Generating
Biography
with Token-level
Prompt
White-box LLM Uncertainty
Quantification
Divide the
Generated Atomic
Response into
Response Claims
Atomic Claims
Claim-level Claim-level
Uncertainty Uncertainty
Quantification Scores
Generated
Map Scores and
Response
Highlight the
Highlighted with
Generated Response
Uncertainty Scores
Figure5: Fact-Checkingpipelinebasedontoken-leveluncertaintyquantification.C BenchmarkConstructionDetails Numberof Supported
Model
claims claims
Mistral7b 3,824 71.1%
C.1 BenchmarkConstructionPipeline
Vicuna13b 3,617 78.2%
Figure 5 presents the suggested general prepara- Jais13b 1,407 84.3%
GPT-3.5-turbo 3,875 89.4%
tion pipeline of a factuality dataset for an arbi-
trarymodelandFact-Checkingbenchmark. This Table4: Thestatisticsofthedatasetsgeneratedoutof
pipelinewasusedtogeneratethebiographydataset. 100biographiesforalltestedLLMsforEnglish.
Figure6presentsthedetailedpipelineofthebiog-
raphy dataset preparation, based on the general
Numberof Supported
schema. Model
claims claims
The prompt for the matching model to map Vicuna13b,English 100 70.1%
atomic claims to the initial text is as follows: Yi6b,Chinese 1,603 94.0%
Jais13b,Arabic 186 73.0%
“Given the fact “fact”, identify the
GPT-4,Arabic 200 92.5%
corresponding words in the original
sentence “sent” that help derive this Table5: Thestatisticsofthedatasetsgeneratedoutof
100biographiesforalltestedLLMs.
fact. Please list all words that are
related to the fact, in the order they
appear in the original sentence, each
word separated by comma.”. The prompt for of the 100 person names, Jais 13b generates
themodelthatpartitionstheinitialgeneratedtext emptybiographiesforsevennameswithresponse
intoindividualatomicclaims,aswellastheprompt messages like: “I am sorry! I cannot provide
fortheclassificationmodel,aretakensimilartothe information about {name}”, or “What do you
paperbyMinetal.(2023). wanttoknowexactly?”. Tworandomclaimsfrom
eachbiographyareverifiedmanually(total=186
C.2 DatasetsandStatistics
claims).
ForArabic,wegenerate100randombiographies
fromthelistofthemostvisitedwebsitesinArabic
ForChinese,wefirstpromptChatGPTtogener-
Wikipedia to experiment with GPT-4. The used
atealistof100famouspeople. Thenusethesame
Arabic prompt is the translation of: “Tell me the
way as we have done in Arabic, but change the
biographyof{personname}”. ThenwecallGPT-4
prompt to Chinese, to generate bios and claims.
to extract atomic claims from the generated Ara-
Matching of Chinese and Arabic claims is per-
bic biographies. The used prompt is: “Convert
formedbyaligningthematchedsymbolstothebi-
the following biography into Arabic atomic fac-
ographygenerationgreedily. AsFactScorepipeline
tualclaimsthatcanbeverified,oneclaimperline.
onlyworkswithEnglishclaims,bothChineseand
Biography is: {biography}”. Arabic biographies
Arabic datasets were annotated manually. Only
andclaimsaretranslatedintoEnglishusingGoogle
claims containing factual information were man-
Translate. It’s worth mentioning that almost one-
uallyannotated. Thestatisticsforthisdatasetare
thirdofthenamesinthelistofpersonnamesare
presentedinTable5
foreign,e.g. DonaldTrump,Messi,IsaacNewton,
PopeBenedictXVI,etc. Thiscanbeusefultostudy
thefactualityofclaimsaboutthesamepersonsin
D AblationStudies
differentWikipedialanguages. Onaverage,GPT-4
generates20claimsfromeachbiography,andran-
domtwoclaimsfromeachbiographyareverified In this section, we study the influence of differ-
manually(total=200claims). entcomponentsontheresultsofCCPmethodand
For Jais 13b experiments, we use the same otherbaselines.
prompts used in GPT-4. We notice that the
biographiesgeneratedbyJais13baremuchshorter
than the ones generated by GPT-4 (almost half D.1 Aggregation
length). WeuseGPT-4toextractclaimsfromthe
generated biographies. On average, biographies Table6presentstheresultofCCPonVicuna13b
generated by Jais 13b have nine claims. Out whenapplying4differentkindsofaggregation:Michael Barakan (born 29 April
1954), known as Shane
Wikipedia Fontayne, is an English rock
guitarist. Active since the 1970s,
he was the guitarist for...
Tell me a bio of Vicuna-13B S m f w 1 gr 9 roh au e5ma ss w2 n i bc ,te u hi o ia pne rF n n o iN U na on e nn n tt w ha id t A eey Ysn d u Bo oe gSn rr u oki g ts sa nw Ctta x er i2 .ti s yt 8 .e . , ., .r H a ne d Instruct-GPT ' ' S [ ' 'S S H H[ 'H[ t' h h e eaS ea a th w g en n wa rs ae e ean . s ' ws]e F F b o o b uF on n o po rt t r n n a a n i nt y y ia no n n ty n he e Nn e Ae i i e s s Bu w i a fs g rr o Yo ua s nom so xm rtn k .t 2 'gu h ]C8 ,ws e , ii r tc U 1 yii t .a 9e n ',n 5r i . t. 2' e,', .d ', Chat-GPT [[ F[T ar lu se e, , F Fa a .l l .s s .e e, , F Fa al ls se e] ], ,
Shane Fontayne.
[[0.331: True, 0.926:
GPT4 False, 0.887: False],
[0.999: False, 0.914:
False, 0.826: False],..]
Shane Fontayne is a musician and songwriter from the United States. (Shane Fontayne is from the United States.)
Shane Fontayne is a musician and songwriter from the United States. (Shane Fontayne is a musician.)
S H Hh e ea w wn a ae s s F b bo o on r rt n na y o on n ne A A is u u g ga u u m s st tu 2 2s 8 8ic , , ia 1 1n 9 9 5 5a 2 2n ,,d ii nn s No Nn eeg www YYri oote rr kkr f CCro ii ttm yy ,, t aah nne dd U ggn rr eeit wwed uu S ppt a ii nnt e tt hhs. ee ( BBS rrh ooa nnn xxe .. ((F HHo een t gwa ry a en s we b ui os pr n a in os tno h n A eg u Bw g rr u oit s ne t xr . 2 .) )8, 1952.) Lo Ag toit ms M ic a Cp lp ae imd s to
He was born on August 28, 1952, in New York City, and grew up in the Bronx. (He was born in New York City.)
...
[('\n', 0.523), ('Sh', 0.556), ('ane', 0.998),
('Font', 0.999), ('ay', 0.999), ('ne', 0.999),
('is', 0.695), ('a', 0.616), ('music', 0.072),
('ian', 0.994),...]
Figure6: Exampleofthealgorithmoperationofthefact-checkingmethodusinguncertaintyestimation.
Method ROC-AUC 13bwhenaddingdifferentcontextsastheinputsof
CCP
prod
0.66±0.03 NLImodel:
CCP
len
0.65±0.03
1. CCP calculates NLI using these 2
CCP
min
0.64±0.03 nocontext
wordsasNLImodelinput;
CCP
mean
0.65±0.03
2. CCP calculatesNLI usingtheprefix
sentpref
Table 6: ROC-AUC on Vicuna 13b generation, for
inthesentencefrommodelgenerationupto
differentnormalizations.
thecurrentwordanditsalternative,asinput
toNLImodel;
3. CCP , uses all words in the sentence
claimpref
k
CCP (F)=−(cid:89) CCP (x ), correspondingtothegreedyword,whichwere
prod word j
j=1 matchedtothecurrentfactandwhichcomes
(cid:32) 1(cid:88)k (cid:33) beforethegreedyword(seeexampleonFig-
CCP (F)=−exp logCCP (x ) ,
len k word j ure2).
j=1 (8)
k Theresultsforsentenceprefixprovidestoowide
CCP (F)=−minCCP (x ),
min word j
j=1 context to the NLI model, which results in NLI
k
1(cid:88) modelfocusingonthethecontextsinsteadofthe
CCP (F)=− CCP (x )
mean k word j
greedywordsanditsalternatives,thusoutputting
j=1
lotsofentailmentclasses. Ontheotherhand, the
CCP showsbetterresultsandistheonechosen
prod
methodwithoutcontextdoesnotprovideenough
forfinalCCP formula.
context to calculate NLI class with more quality.
D.2 NLIContext CCP shows better results and is the one
claimpref
chosenforfinalCCP formula.
Method ROC-AUC
CCP
nocontext
0.64±0.03 D.3 FunctionalWordsHandling
CCP sentpref 0.59±0.03 Table8presentstheresultofCCPandMaximum
CCP claimpref 0.66±0.03 Probability on Vicuna 13b when handling func-
tionalwordsdifferently. The2approachestested
Table 7: ROC-AUC on Vicuna 13b generation, for
are:
differentcontextstoinputwordswithtoanNLImodel.
1. CCP and MP are the ver-
confident confident
Table 7 presents the result of CCP on Vicuna sionsofCCP andMP baselines,whichas-Method ROC-AUC vicuna13b-v1.5
CCP
confident
0.66±0.03 0.92
CCP ignore 0.63±0.03 0.90
MP confident 0.59±0.05 0.88
MP
ignore
0.60±0.05
0.86
0.84
Table 8: ROC-AUC on Vicuna 13b generation, for
methodsofhandlingfunctionalwords. 0.82
0.80
0.78
signsthemostconfidentword-levelscoreof 2 4 6 8 10 12
Maximum number of sentences
1.0 to words from NLTK (Bird and Loper,
Figure8: Percentageofsupportedclaims,asafunction
2004) stopwords list, which is the same as
ofthenumberofsentencestorestrictgenerationto(En-
skippingthesewordsformthelistofmatched
glish,Vicuna13b).
wordsoflengthk;
2. CCP andMP handlesfunctional wrongwhenthegenerationlengthincreases,which
ignore ignore
wordssimilartoanyotherwords. MP maybeduetothegenerationofadditionalfacts.
ignore
isthebaselineusedinthemainsection. IntheFigure9,weshowtheperformancequality
ofourCCPmethodonChinesedataontheYi6b
CCP confident showsbetterresultsandistheone model. We see that, as for other English models,
chosenforfinalCCP formula. Incontract,Max- our method outperforms the alternatives over the
imum Probability performs worse if specifically entiregenerationlengthintermsofROC-AUC.
handlingfunctionalwords.
Yi 6b, Chinese
D.4 TheNumberofAlternatives
0.8
0.72
wFi hg ou lr ee m7 op dre es le gn et ns eth rae tiC oC n,P dR epO eC n- dA inU gC onre ts hu elt ns uo mn bth ee
r
00 .. 67 0.630.640.63 0.560.590.610.67 0.570.630.64
0.52
0.5
ofbeamsntorunthemethodwith.
0.4
0.3
vicuna13b-v1.5
0.2
0.66 CCP (ours)
0.1
0.64
0.0
0.62 First 2 sentences First 5 sentences No restriction
Maximum Probability Token Entropy
0.60 P(True) CCP (ours)
0.58
Figure 9: The comparison of token-level uncertainty
0.56
quantificationmethodsintermsofROC-AUCscores,
0.54
measuredforChinesedataset. Theresultsaresplitinto
0.52
binswhenconsideringonlyfactsfromthefirst2,5,and
0.50
allsentences.
2 4 6 8 10
Number of samples n
Figure 7: ROC-AUC between FactScore classes and
claim-levelCCPmethod,dependentonthenumberof
sampledwordsntoconsider.
E AdditionalExperimentalResults
Hereweshowadditionalresultsrelatedtotheper-
formanceofourpipelineandCCPmethod. Inthe
Figure8weshowthedependenceofthepercentage
ofsupportedclaimsinthegeneratedLLMresponse
(in this case for Vicuna 13b) as a function of its
length. As we can see, the model is more often
CUA-COR
stcaf
detroppuS
fo
noitcarF
CUA-CORF FactScoreAnnotationSystem
HerewegiveexamplesoftheoperationoftheFactScoreautomaticmarkupsystem,seetheTable9. We
hypothesize that the causes of FactScore errors are related to model hallucination (true information is
presentintheknowledgesource,butthemodelproducesincorrectinformation),lackofcontext(anexcerpt
fromaWikipediaarticlecannotcaptureallthedetails),anddifficultieswithinformationinterpretation
(thedesiredinformationispresentintheknowledgebase,butisformulatedindifferentwordsorinseveral
sentences). Allthesecasesleaveroomforfurtherimprovementofthepipeline. Wehavealsogiventhe
resultsoftheproposedCCPmethodonselectedexamples. Itcanbeseenthattheresultsofouralgorithm
aresimilartotheannotationfromFactScore,whileCCPdoesnotuseexternalinformationincalculating
thescores.
Cases FS Human CCP GeneratedAtomicClaim TrueInformationfromtheWikipediaarticle
TN False False 0.819 MarieStopesdiedonOctober20,1958. MarieCharlotteCarmichaelStopes(15October1880–2October1958)
wasaBritishauthor...
TN False False 0.999 Planckisbestknownforhisworkonthe Hisfameasaphysicistrestsprimarilyonhisroleastheoriginatorof
natureoflight. quantumtheory...
FN False True 1.0 Heisenbergwasappointedasthedirec- HethenbecamedirectoroftheMaxPlanckInstituteforPhysicsand
toroftheMaxPlanckInstitute. Astrophysicsfrom1960to1970.
FN False True 0.716 RayCharlesincorporatedelementsof CharlesreachedthepinnacleofhissuccessatAtlanticwiththerelease
Latinmusicintohissound. of"What’dISay",whichcombinedgospel,jazz,bluesandLatinmusic.
FP True False 0.001 Saganwasaprolificwriter. CarlEdwardSagan(November9,1934–December20,1996)wasan
Americanastronomerandsciencecommunicator.
FP True False 0.141 VanGoghwasapastor. VanGoghpreparedfortheUniversityofAmsterdamtheologyentrance
examination;hefailedtheexam...
FP True False 0.185 Hawkingshowedanearlyaptitudefor Althoughknownatschoolas"Einstein",Hawkingwasnotinitially
science. successfulacademically.
TP True True 0.276 TigerWoodsisaprofessionalgolfer. EldrickTont"Tiger"Woods(bornDecember30,1975)isanAmerican
professionalgolfer.
TP True True 0.015 MilesDavisbeganplayingthetrumpet Onhisthirteenthbirthdayhisfatherboughthimanewtrumpet,[17]and
attheageof13. Davisbegantoplayinlocalbands.
TP True True 0.001 Hitchcockdirected"TheBirds." Hitchcock’sothernotablefilmsincludeRope(1948),Strangersona
Train(1951),...,Birds(1963)andMarnie(1964),...
Table9: TablewithexamplesoftheFactScoreautomaticannotationsystemandalltypesofclassificationoutcomes
whencomparingautomaticannotationandmanualannotation(confusionmatrixelements): TrueNegative(TN),
FalseNegative(FN),FalsePositive(FP),TrueNegative(TN);FSisaFactScorelabel,Humanisahumanannotation
label. CCPscoresarecomparabletoFactScoreannotationanddonotrequireanexternalsourceofinformation.G ExamplesofResultsObtainedwithCCPandtheMaximalProbabilityBaseline
MaximumProbability Claim-ConditionedProbability(ours)
TellmeabioofMikhailGorbachev. TellmeabioofMikhailGorbachev.
MikhailGorbachev(bornMikhailSergeyevichGorbachevonMarch2, MikhailGorbachev(bornMikhailSergeyevichGorbachevonMarch2,
1931,inStavropol,Russia)isaformerSovietpoliticianandstatesmanwho 1931,inStavropol,Russia)isaformerSovietpoliticianandstatesmanwho
servedasthelastPresidentoftheSovietUnionfrom1989to1991.Hewas servedasthelastPresidentoftheSovietUnionfrom1989to1991.Hewas
alsothefirstpresidentoftheRussianFederationfrom1991to1999.Gorbachev alsothefirstpresidentoftheRussianFederationfrom1991to1999.Gorbachev
iswidelyregardedasoneofthemostinfluentialfiguresofthe20thcenturyfor iswidelyregardedasoneofthemostinfluentialfiguresofthe20thcenturyfor
hisroleinthecollapseoftheSovietUnionandthebeginningoftheendofthe hisroleinthecollapseoftheSovietUnionandthebeginningoftheendofthe
ColdWar.GorbachevwasbornintoapeasantfamilyinStavropol,Russia.... ColdWar.GorbachevwasbornintoapeasantfamilyinStavropol,Russia....
Figure10: Claim-ConditionedProbabilityvsMaximumProbabilitybaseline.
MaximumProbability Claim-ConditionedProbability(ours)
TellmeabioofVincentvanGogh. TellmeabioofVincentvanGogh.
Vincent van Gogh was a Dutch post-Impressionist painter who VincentvanGoghwasaDutchpost-Impressionistpainterwhoiswidely
is widely considered one of the most important figures in the his- consideredoneofthemostimportantfiguresinthehistoryofart. Hewas
tory of art. He was born on March 30, 1853, in Zundert, Nether- born on March 30, 1853, in Zundert, Netherlands, and died on July 29,
lands, and died on July 29, 1890, in Auvers-sur-Oise, France. Van 1890,inAuvers-sur-Oise,France. VanGogh’searlylifewasmarkedbya
Gogh’searlylifewasmarkedbyaseriesofunsuccessfulcareers,includingas seriesofunsuccessfulcareers,includingasateacher,apastor,andanart
ateacher,apastor,andanartdealer.Hebegantofocusonartinhislate20s, dealer. Hebegantofocusonartinhislate20s,andin1886,hemovedto
andin1886,hemovedtoParis,wherehebecameinvolvedwiththeavant-garde Paris,wherehebecameinvolvedwiththeavant-gardeartscene. InParis,
artscene.InParis,vanGoghmetmanyotherartists,includingClaudeMonet, vanGoghmetmanyotherartists,includingClaudeMonet,Pierre-Auguste
Pierre-AugusteRenoir,andPaulGauguin,whowouldbecomehisclosestfriend Renoir, andPaulGauguin,whowouldbecomehisclosestfriendandmentor.
andmentor.Gauguin’sinfluenceledvanGoghtoadoptamorevibrantand Gauguin’sinfluenceledvanGoghtoadoptamorevibrantandexpressive
expressivestyle,characterizedbyboldcolorsandthickbrushstrokes. style,characterizedbyboldcolorsandthickbrushstrokes.
Figure11: Claim-ConditionedProbabilityvsMaximumProbabilitybaseline.
MaximumProbability Claim-ConditionedProbability(ours)
TellmeabioofBillGates. TellmeabioofBillGates.
Bill Gates, born William Henry Gates III on October 28, 1955, is BillGates,bornWilliamHenryGatesIIIonOctober28,1955,isan
anAmericanentrepreneur, softwaredeveloper, andphilanthropist. Heis Americanentrepreneur,softwaredeveloper,andphilanthropist. Heisbest
bestknownastheco-founderofMicrosoftCorporation,oneoftheworld’s knownastheco-founderofMicrosoftCorporation,oneoftheworld’slargest
largest and most successful software companies. His net worth is esti- andmostsuccessfulsoftwarecompanies. Hisnetworthisestimatedtobe
matedtobearound$131.7billionandisconsistentlyrankedamongthe around$131.7billionandisconsistentlyrankedamongtheworld’swealthiest
world’swealthiestpeople. GatesgrewupinSeattle,Washington, withhis people.GatesgrewupinSeattle,Washington,withhisparents,anattorney
parents,anattorneyandaschoolteacher,andtwosisters.Heshowedanearly andaschoolteacher,andtwosisters.Heshowedanearlyaptitudeformathe-
aptitudeformathematicsandcomputerprogrammingandattendedLakeside maticsandcomputerprogrammingandattendedLakesideSchool,aprivate
School,aprivatepreparatoryschoolinSeattlewherehemetandbefriended preparatoryschoolinSeattlewherehemetandbefriendedhisfuturebusiness
hisfuturebusinesspartner,PaulAllen.In1973,GatesenrolledatHarvardUni- partner,PaulAllen. In1973,GatesenrolledatHarvardUniversity,buthe
versity,buthesoonfoundhimselfuninterestedinhisclassesandspentmuch soonfoundhimselfuninterestedinhisclassesandspentmuchofhistime
ofhistimeonprogrammingprojects.HeeventuallydroppedoutofHarvard onprogrammingprojects.HeeventuallydroppedoutofHarvardtofocuson
tofocusondevelopingMicrosoft.GatesandAllenstartedMicrosoftin1975, developingMicrosoft. GatesandAllenstartedMicrosoftin1975,initially
initiallyprovidingsoftwarefortheAltair8800,anearlypersonalcomputer. providingsoftwarefortheAltair8800,anearlypersonalcomputer.
Figure12: Claim-ConditionedProbabilityvsMaximumProbabilitybaseline.
H ResourcesandExpenses
ForasinglerunofdatagenerationandUQmethodsevaluation,wespent12daysofNvidiaA100GPU
computetime. OpenAIAPIwasusedmainlyforsplittingandmatchingatomicclaims,thetotalcostfor
allEnglishmodels(Mistral7b,Vicuna13b,Jais13b,GPT-3.5-turbo)was40$. Thecostofgenerating
EnglishbiographieswithGPT-3.5-turbowas13$.