SnapNTell: Enhancing Entity-Centric Visual Question Answering with
Retrieval Augmented Multimodal LLM
JielinQiu1,2∗, AndreaMadotto1, ZhaojiangLin1, PaulA.Crook1, YifanEthanXu1,
XinLunaDong1, ChristosFaloutsos2, LeiLi2, BabakDamavandi1, SeungwhanMoon1
1 MetaRealityLabs&FAIR,Meta 2CarnegieMellonUniversity
{jielinq,leili,christos}@cs.cmu.edu,{andreamad8,zhaojiang,pacrook,ethanxu,lunadong,shanemoon}@meta.com
Abstract
Vision-extendedLLMshavemadesignificant
stridesinVisualQuestionAnswering(VQA).
Despite these advancements, VLLMs still
encounter substantial difficulties in handling
queriesinvolvinglong-tailentities,withaten-
dency to produce erroneous or hallucinated
responses. In this work, we introduce a
Figure1: ComparingSnapNTellwithexistingmethods
novelevaluativebenchmarknamedSnapNTell,
revealsadistinctivefocus. IntheSnapNTellbenchmark,
specifically tailored for entity-centric VQA.
theanswersarepredominantlyentity-centric,charac-
Thistaskaimstotestthemodels’capabilities
terizedbyagreaterdepthofknowledgeableinformation
inidentifyingentitiesandprovidingdetailed,
pertainingtothespecificentitydepictedintheimageas
entity-specificknowledge. Wehavedeveloped
theanswer.
the SnapNTell Dataset, distinct from tradi-
tionalVQAdatasets:(1)Itencompassesawide inrepresentationoftenleadstoVLLMsresorting
rangeofcategorizedentities,eachrepresented
togeneratingplausiblebutincorrectorimaginative
byimagesandexplicitlynamedintheanswers;
contentintheiroutputs,aproblemthatmanifests
(2)ItfeaturesQApairsthatrequireextensive
as“hallucinations"withinthecontextofmodelre- knowledgeforaccurateresponses. Thedataset
sponses. To ensure the confident deployment of
isorganizedinto22majorcategories,contain-
ing 7,568 unique entities in total. For each VLLMs in practical scenarios, there is an urgent
entity, we curated 10 illustrative images and needfordedicatedresearchthatnotonlyrecognizes
crafted10knowledge-intensiveQApairs. To butactivelystrivestotackleandreduceinstancesof
addressthisnoveltask,wedevisedascalable,
hallucinations,especiallyinthecontextoffactual
efficient,andtransparentretrieval-augmented
queriesinvolvingtheselong-tailentities.
multimodalLLM.Ourapproachmarkedlyout-
Thelackofpubliclyavailableevaluationdatasets
performs existing methods on the SnapNTell
specificallytailoredtoassessmodels’abilityinrec-
dataset,achievinga66.5%improvementinthe
BELURTscore. Wewillsoonmakethedataset ognizingreal-worldlong-tailedentitiespresentsa
andthesourcecodepubliclyaccessible. notable gap in VQA. Existing datasets fall short
in serving this purpose due to a narrow range of
1 Introduction entitycategories,theprevalenceofoverlysimplis-
tic yes/no QA pairs, and a general lack of entity
Vision-extendedLLMshaveshownsignificantad-
specificity, often using broad terms like “Tiger"
vancements,excellingatcapturingcomplexseman-
insteadofmorespecificoneslike“SiberianTiger".
tics and context-aware attributes needed for intri-
To address this gap, we introduce a novel eval-
catetasks. However,theirabilitiesinfactualVQA
uation task called SnapNTell, which focuses on
tasks, which demand accurate, concrete answers
entity-centric knowledge-based VQA. The Snap-
aboutreal-worldentitiesandphenomena,expose
NTell benchmark has been designed to evaluate
certain limitations. Particularly, torso-to-tail or
models’abilitiesinaccuratelyidentifyingentities
long-tailentities,whichconstitutealargepropor-
andgeneratingresponsesthatshowcaseadeepun-
tionofreal-worlddatabutappearinfrequentlyin
derstandingoftheseentities. Tosupportthistask,
training datasets, pose a challenge. This scarcity
wehavecuratedanewevaluationdatasetthatde-
∗WorkdonewhileatMeta partsfromexistingdatasetsintwocrucialways: (1)
4202
raM
7
]VC.sc[
1v53740.3042:viXraItincludesawiderangeoffine-grainedandcatego- • Our model demonstrates superior perfor-
rizedentities,eachaccompaniedbycorresponding manceontheSnapNTelldataset,surpassing
imagesandclearmentionoftheentitynamewithin currentmethodologieswitha66.5%improve-
theanswersets. (2)ItfeaturesQApairsdesigned mentinBELURTscore.
topromptknowledge-intensiveresponses,moving
2 RelatedWorks
beyondthebinaryyes/noformattochallengeand
assessthedepthofthemodel’scomprehension.
Knowledge-based VQA Research in vision-
Furthermore,thelimitationsidentifiedinfactual
language tasks, which necessitate understanding
querygenerationunderscoretheneedfornewso-
image content to answer questions, has seen sig-
lutions to address the problem of hallucinations.
nificant advancements over recent years. Begin-
Recentadvancementssuggestthatretrieval-based
ningwithdatasetslikeFVQA(Wangetal.,2016),
approachesholdsignificantpromiseinthisregard
whichextractedfactsfrompre-establishedknowl-
(Guu et al., 2020; Srinivasan et al., 2022; Yang
edgebases,thefieldhasprogressedtomorechal-
etal.,2023a,b). ThesemethodsenhanceLLMsby
lenging ones like the OK-VQA dataset (Marino
integrating external knowledge sources or incor-
etal.,2019),encompassingdiverseknowledgecat-
porating retrieval mechanisms to access relevant
egories. MultiModalQA(Talmoretal.,2021)intro-
informationfromextensiveknowledgebases. The
ducedcomplexitywithquestionsdemandingcross-
synergy between the advanced inference capabil-
modalreasoningoversnippets,tables,andimages.
ities of LLMs and the wealth of external knowl-
ThesuccessorofOK-VQA,AOK-VQA(Schwenk
edgehasthepotentialtosignificantlyreduceissues
et al., 2022), raises the bar by providing ques-
relatedtolong-tailentitiesand,consequently,de-
tionsthattranscendsimpleknowledgebasequeries.
creasetheoccurrenceofhallucinatoryresponses.
ManyModalQA (Hannan et al., 2020) shifts the
In this work, we aim to propose an evaluation
focus to answer modality selection, MIMOQA
task to investigate the model’s ability to recog-
(Singhetal.,2021)emphasizesmultimodalanswer
nize real-world long-tailed entities and provide
extraction, and WebQA (Chang et al., 2021) in-
knowledge-intensive answers. We also propose
troducesreal-worldknowledge-seekingquestions,
aretrieval-augmentedmethodtoreducehallucina-
albeitwithsomelimitationsregardingentitycatego-
tionsandenhancetheprecisionandtrustworthiness
rizationandgranularity. Morecomparisondetails
ofgeneratedresponses.
canbefoundinSection3.5.
Ourcontributionissummarizedasfollows:
Multimodal LLMs Integrating visual under-
• SnapNTelltask. Weproposeanoveltaskfor standingintotext-basedLLMtypicallycombines
entity-centricVQA,specificallydesignedto them with a visual encoder and uses image cap-
assesstheproficiencyofmodelsinaccurately tioning datasets for alignment (Koh et al., 2023;
identifyingandgeneratingresponsesthatex- Wu et al., 2023; Chowdhery et al., 2022). Tech-
hibitadeepcomprehensionoftheseidentified niques like adapter-based tuning (Alayrac et al.,
entities. 2022)andprefixtuning(Tsimpoukellietal.,2021)
allowthesemodelstoprocessvisualinputswhile
• SnapNTellmodel. Weproposeda retrieval-
maintaining their linguistic capabilities, without
augmented multimodal LLM, devised as a
requiringfullmodelretraining(Yinetal.,2023).
baseline model capable of undertaking the
Retrieval-augmented LLM Previous studies
SnapNTell task, which is scalable, effective,
haveexploredretrievalaugmentationintext-only
andexplainable.
settings or image captioning tasks. Guu et al.
• SnapNTelldataset. Wecollectedaneweval- (2020)introducedaretrieverforlanguagemodels
uationdatasetwithdistinctivecharacteristics, toaccesslargecorpusduringvariousstages. Srini-
whichstandsoutfortwokeyfeatures: (1)It vasan et al. (2022) showed retrieval-augmented
encompassesadiverserangeoffine-grained queriesenhanceLLMs’contextunderstanding. Ya-
entities, each accompanied by correspond- sunaga et al. (2023) and Yang et al. (2023a) de-
ingrepresentativeimages. (2)Thequestion- veloped methods for integrating multimodal doc-
answerpairscontainknowledge-intensivere- uments and speeding up LLM inference, respec-
sponses with entity names specifically men- tively. Yang et al. (2023b) created a visual lan-
tionedintheanswersets. guagemodel,inspiredbyFlamingo(Alayracetal.,2022),forimagecaptioningwithexternaldatabase 3.2 Imagecollection
retrieval. Similarly,Guietal.(2021)combinedim-
The dataset comprises 22 primary categories, en-
plicitandexplicitknowledgeinanencoder-decoder
capsulating a total of 7,568 unique entities. For
setuptoimproveanswergeneration.
eachindividualentity,asetof10imageshasbeen
Open-domainvisualentityrecognition Huetal. curated, where the statistic of the entity list is
(2023) developed OVEN for associating images showninTable10intheAppendix.
withWikipediaentitiesviatextqueries,whileChen
Filtering Initially, a comprehensive list of enti-
etal.(2023)introducedINFOSEEK,adatasetfor
ties,encompassing22primarycategories,wascom-
Visual Question Answering focused on informa-
piled,inatotalof14,910diverseentities. Thenthe
tionalqueries. WhileOVENisproficientinentity
entitylistunderwentfilteringbycross-referencing
recognitionusingaknowledgebase,INFOSEEK
eachentrywithitscorrespondingWikipediapage.
mainlysuppliesfactualresponses. Ourstudyseeks
Entities lacking valid Wikipedia pages were sub-
to merge these strengths, creating detailed para-
sequently removed from the list. For each corre-
graphs that provide context for a more compre-
sponding entity, images were sourced from Cre-
hensive understanding beyond basic facts. More
ative Commons (CC). Further filtering was con-
relatedworkcanbefoundinAppendixE.
ductedbyremovingentitiesthatdidn’thaveasuffi-
cientnumberofimagesobtainedviaGoogleImage
3 SnapNTellDataset
Searchengine. Thecollectedmetadatawasstored
3.1 EntityCategorization inaCSVfilecontainingessentialinformationsuch
asimageURLs, sourcepageURLs, renamedim-
TotacklethechallengeofthenewSnapNTelltask,
agenames,andthecorrespondingWikipediapage
the first step involves creating a comprehensive
URLs. Afterfiltering,thefinalnumberofentities
datasetthatrepresentsawidearrayofreal-world
intheSnapNTelldatasetis7,568. (Morefiltering
entities. Ourdatasetcreationmethodologyentails
detailscanbefoundinAppendixB.)
selecting a diverse set of entity names from vari-
ouscategoriesthatmirrorthediversityofthereal
3.3 Knowledge-intensiveQuestion-Answer
world. Thisselectionencompassesbothcommonly
Pairs
encountered entities and less frequently encoun-
InourSnapNTelldataset,weconsideredfivetypes
tered ones. We have identified 22 categories that
ofquestions:
adequatelyrepresentacross-sectionofentitiesone
might encounter in daily life. These categories • Staticfacts(absolutefacts, discretefacts).
includelandmark,painting,sculpture,food,fruit, Theseareobjectivefactsthatareconcreteand
vegetable, mammal, amphibian, insect, fish, bird, arenotcontingentonotherconditions. They
reptile, celebrity, instrument, plant, electronics, canusuallybeansweredwithauniqueanswer.
tool, transportation, sport, book, household, and i.e.,“Whenwashe(BarackObama)born?"
car. More details about the categories can be re-
• Narrativefacts. Thesefactsencompasscom-
ferredtoTable10intheAppendix.
prehensionoflargercontexts(e.g.,songlyrics,
To populate each category with specific enti-
movieplot). Theyarefactualinthesensethat
ties,weleveragedWikipediaasaprimaryresource
thecontentofthenarrativeshouldaccurately
duetoitsextensiveanddetailedentries. (SeeAp-
reflectthesourcematerialorevents,butacor-
pendixAformoredetails.) Ourselectioncriteria
rectanswerisusuallynotunique,astheycan
areheavilybiasedtowardsspecificity;forinstance,
vary in their level of detail and focus. i.e.,
inthecategoryofmammals,wedeliberatelyopted
“Whatistheplotofthat(‘TheGodfather’)?"
forprecisenamessuchas“GermanShepherd”or
• Dynamicfacts. Thesearefactsthataresub-
“AlaskanMalamute”insteadofthegeneric“Dog”.
ject to change over time. i.e., “What is the
Thislevelofspecificityiscriticalasitenablesthe
Yelp customer rating of it (the Eleven Madi-
modeltodemonstrateitscapacityforfine-grained
sonParkrestaurant)inNYC?"
recognitionanditsabilitytogeneratedetailed,ac-
curateinformationabouteachentity. Thisdataset- • Proceduralfacts. Theseareusuallyanswers
buildingapproachiswhatdistinguishesourdataset to “how” questions, outlining a sequence of
fromexistingVQAdatasets,whichoftenlackfine- steps to accomplish a task. While the steps
grainedentitiesandspecificity. may not be unique and could be subjective,theanswercanstillbeclassifiedaslogicalor curacy and κ = 0.95 agreement rate among the
nonsensical. Notethatthesefactsmaysome- evaluators, demonstrating a significant degree of
timesoverlapwithdynamicfactsornarrative uniformityinthequalityoftheQApairs.
facts,i.e.,“Howdoyoucheckthebatterylevel
3.4 StatisticsandAnalysisofOurDataset
ofmyitem(Ray-BanStoriesGlasses)?"
Entitystatistics Toprovideaclearsummaryof
• Subjective facts. (opinion-based facts).
thiscomprehensivedataset,wehavecondensedthe
These “facts” are not objective indisputable
detailsoftheentitylistintoTable10andFigure9
facts, but based on individual perspectives
(in Appendix F). Our analysis indicates that the
orexperience. Recommendationsfallinthis
datasetdisplaysawell-balanceddistributionacross
category. While there’s generally no single
different categories, enhancing its balanced and
correct answer to questions seeking subjec-
diversecharacteristics. Suchabalancedanddiverse
tive facts, it still requires the system to un-
compositionenhancestherepresentativenessofour
derstandthetopicandprovidereasonablean-
proposedevaluationdataset.
swersgroundedbyworldfacts. i.e.,“Whydo
youlikeit(NiagaraFalls)?"
Popularity Theimportanceofentitypopularity
insearchenginesisakeyaspecttoconsider,simi-
To construct a comprehensive and knowledge-
lartoexaminingthehead,torso,andtailsectionsof
intensive QA dataset, we employ a three-step
knowledgebaseswithinsearchengineframeworks.
process. Firstly,weextractedandcondensedperti-
AsdemonstratedinFigure11inAppendixF,we
nent information from Wikipedia for each entity,
use the average Wikipedia pageviews per entity
i.e.,thesummaryoftheintroduction,thecaptionof
overthelast60daysasthemetric. Thisaverageis
theimage,etc. (SeeAppendixAformoredetails).
calculatedbysummingupthepageviewsandthen
FollowingsimilarapproachesproposedbyLLaVA
dividing by the number of entities. The insights
(Liuetal.,2023b),Dettmersetal.(2023)isutilized
fromFigure11revealthatentitiesinthecelebrity
togenerateQApairsforeachentityautomatically
categoryhavethehighestaveragepopularity. For
basedonfivepre-definedquestiontypes,ensuring
abroadercomparisonamongdifferentcategories,
diversity and informativeness. Then, we enlisted
wealsopresentacomprehensiveanalysisoftotal
three annotators (2 male and 1 female) from
pageviews for all categories in Figure 10 in Ap-
Amazon SageMaker to assess QA pair quality
pendixF,whichshowsthatthecelebritycategory
and make necessary revisions to meet specific
remainsattheforefrontintermsofoverallentity
criteria. The responsibilities of these annotators
popularity. Thisisattributedtothecombinationof
include: (1) ensuring that the images and QA
ahighernumberofentitiesinthiscategoryandthe
pairs are semantically aligned, (2) validating the
generallyhigherpopularityofeachentitywithinit.
accuracyoftheprovidedanswers,(3)makingsure
the questions are free of particular entity names
3.5 ComparisonwithExistingVQADatasets
but demanding such specificity in the answers,
In Table 2 and Figure 2, we present a compari-
(4) assessing if the modified QA pairs adhere
sonwithexistingVQAdatasets. Itisevidentthat
to the criteria for knowledge-intensive content,
some existing VQA datasets lack categorization,
and (5) removing specific entity-related details
fine-grainedentities,andknowledge-intensivean-
fromthequestions. Thislaststepguaranteesthat
swers,asobservedinVQA2.0(Goyaletal.,2016)
thequestionqueriescannotbeansweredwithout
andGQA(HudsonandManning,2019). OK-VQA
understandingtheaccompanyingvisualcontext.
(Marinoetal.,2019)containsimagesthatmaynot
Quality and consistency In order to verify the besufficienttoanswerthequestions,encouraging
quality of the QA pairs, we conducted a quality reliance on external knowledge resources. How-
evaluationbyrandomlychoosing1,000QApairs ever,theanswersinOK-VQAareoftensimplistic
fromourdataset. Weassignedthreeindependent binary (yes/no) responses or selections from the
humanevaluators(1male,2female)fromAmazon questions. A-OKVQA(Schwenketal.,2022),the
SageMakertoreviewthesepairsforaccuracy[ac- successorofOK-VQA,aimstoprovidequestions
curate, inaccurate] and agreement on whether to thatrequirecommonsensereasoningaboutthede-
savetheQApairbyFleiss’Kappa(Fleiss,1971). picted scene but use general object names in the
Theoutcomeofthisassessmentrevealed98%ac- answers. MultiModalQA(Talmoretal.,2021)fo-Table1: Moredetailedcomparisonwithexistingknowledge-basedVQAdatasets. Anonymitymeanswhetherthe
questionalreadycontainsaknowledgecluerelatedtotheentityinquestion. (*Unclear)
Dataset Categories UniqueEntity QAPairs Images AverageAnsLength NumberofImages/Entity Anonymity
ViQuAE 3 2,400 3,700 3,300 1.8 * ✗
EncyclopedicVQA(test) 12 * 5,750 5,750 3.2 * ✗
SnapNTell(Ours) 22 7,568 75,680 75,680 25.7 10 ✓
Table2:ComparisonwithexistingVQAdatasetsKnowl-
edgemeanstheQApairsareknowledgeable,notsimple
yes/noanswersorselectionquestions. Entitiesmeans
whethertherearefine-grainedentitiesspecificallycon-
tainedinanswers. Categorizationmeanstheentitiesare
categorized,notrandomlycrawledonline.
Dataset Knowledge Entities Categorization
VQA2.0(Goyaletal.,2016)
GQA(HudsonandManning,2019)
OK-VQA(Marinoetal.,2019)
ManyModalQA(Hannanetal.,2020) ✓
MultiModalQA(Talmoretal.,2021) ✓
MIMOQA(Singhetal.,2021) ✓
A-OKVQA(Schwenketal.,2022) ✓
WebQA(Changetal.,2021) ✓ ✓ ✓
ViQuAE(Lerneretal.,2022) ✓ ✓ ✓
EncyclopedicVQA(Mensinketal.,2023) ✓ ✓ ✓
SnapNTell(Ours) ✓ ✓ ✓
Figure2: Comparisonwithexistingdatasets,wherepre-
viousVQAdatasetsmostlyfocusonfreeformanswers
cusesoncross-modalknowledgeextractionbutre-
(suchasyes/noforverificationquestionsandchoicefor
liesonquestiontemplatesforquestiongeneration. selectionquestions).
ManyModalQA(Hannanetal.,2020)focuseson
revealanyknowledgehintsabouttheentity. This
answermodalitychoiceratherthanknowledgeag-
designensuresthatthequestionscannotbestraight-
gregationorextraction. InMIMOQA(Singhetal.,
forwardlyansweredwithoutinterpretingtheimage
2021),thetaskofextractingamultimodalanswer
data,settingourdatasetapartfrombothViQuAE
is not necessarily knowledge-intensive. WebQA
andEncyclopedicVQA.
(Changetal.,2021)doeshavecategorizationbut
lacksfine-grainedentitiesinmanyQApairs,result-
4 Method
ing in more general questions and answers. Our
proposed SnapNTell differs by including a wide Inthissection,wewillintroducethedetailsofour
range of fine-grained entities with representative proposed retrieval-augmented multimodal LLM
imagesandexplicitentitynamesintheanswersets. model. The architecture of our model is shown
Additionally,itincorporatesquestion-answerpairs in Figure 3 (larger figure in Appendix D due
thatdemandknowledge-intensiveresponses,going to space limit). Our model can be considered
beyondsimplisticbinaryanswers. Examplesofour twofold: (1)Retrievalaugmentation. Giventhe
datasetcanbefoundinFigure8inAppendixF. inputimage-questionpair,weretrieveusefulentity-
centricinformationwithinknowledgesources. (2)
ViQuAE (Lerner et al., 2022) and Encyclope-
Entity-centricknowledge-basedanswergenera-
dicVQA(Mensinketal.,2023)bothincorporate
tion. Theretrievedinformationwillbecombined
entity-level knowledge-based information along
withtheimageandquestiontogethertogeneratea
with categorization. Therefore, we performed a
knowledgeableanswer.
morein-depthanalysiscomparingtheminTable1.
Our dataset surpasses these in terms of the vari-
4.1 RetrievalAugmentation
ety of categories, the number of distinct entities,
The retrieval augmentation process can be sub-
andtheoverallnumberofQApairs. Additionally,
divided into: (i) Semantic region extraction via
ourdatasetboastsahighercountofimagesanda
language-guidedobjectdetection,(ii)Entityrecog-
longeraveragelengthforanswers. Specifically,our
nitionviaimageretrieval,and(iii)Knowledgere-
datasetisstructuredtoinclude10imagesforeach
trievalviamulti-sourceaggregation.
entity,whereastheexactnumberofimagesperen-
tity in ViQuAE and Encyclopedic VQA remains Semantic Region Extraction via Language-
unspecified. Mostnotably,ourdataset’squestions GuidedObjectDetection Toimproverecogni-
are highly anonymous, implying that they do not tionperformance,wefocusonextractingspecificvantknowledgelinks,enrichingourunderstanding
ofthespecifiedimageregion,andimprovingour
ability to comprehend and contextualize the ex-
tractedcontent. Moredetailsofthemethodcanbe
foundinAppendixD.
4.2 Entity-centricKnowledge-basedAnswer
Generation
Followinginformationcollection,weentertheinte-
grationphase,blendingtheinputimage,question,
Figure3: OurSnapNTellmodelarchitecturetakesan
image-question pair as input. It begins with retrieval and retrieved data to generate a knowledgeable
augmentationtosourcerelevantinformationaboutthe response, which is illustrated in Figure 3. Our
entityin theimage. Thisinformation, along withthe method enhances multimodal understanding by
question, feeds into the word embedding layer. Text pre-training a LLM with image-text paired data.
embeddingsmergewithimage-projectedembeddings
TakingcuesfromMoonetal.(2023),weemploy
beforeenteringtheLLM,culminatinginaknowledge-
lightweightadaptersforeachmodality,converting
ableanswerastheoutput.
inputsintothetexttokenembeddingspaceofthe
image regions containing the entity, rather than chosenLLM.
general image-level recognition. We employ a Inourmethod,theLLM’stexttokenembedding
language-guidedobjectdetectionmodel,i.e.,GLIP space morphs into a unified space, representing
(Lietal.,2021),forlanguage-guidedobjectdetec- both text and image content, with each modality
tion,extractingregionsrelevanttotextualqueries assigned64to256tokenembeddings. Wefreeze
byunderstandingthequerycontext. Thistargeted the LLM’s parameters during alignment training
approachensurespreciseregionextraction,enhanc- toquickenconvergenceandretaintheLLM’srea-
ingthesystem’saccuracyandcontextualrelevance. soningskillsforinference. Toensurefeaturealign-
ment, we use an image encoder, g(·), previously
Entity Recognition via Image Retrieval We
synchronized with a text embedding space, like
construct a similarity index using CLIP embed-
in CLIP (Radford et al., 2021; Schuhmann et al.,
dings (Radford et al., 2021) and Faiss (Johnson
2022). For text-image pairs (X ,X ), we
et al., 2017) for indexing. Our database, built on text image
alignthemusingspecificobjectivesandaprojec-
the WIT dataset (Srinivasan et al., 2021), maps
tionmodule,likethePerceiverResampler(Alayrac
CLIPimageembeddingstotheirtextdescriptions,
etal.,2022),appliedtothevisionencoderas:
leveraging Faiss’s robust similarity search capa-
L
bilities. After setting up the indexing database, p(X |X )=(cid:89) p (X[i]|Z ,Z[1:i−1]) (1)
text image θ text image text
given an input query image I, we perform a k-
i=1
nearest neighbor retrieval based on cosine simi- Z image =Proj θ(h latents,g(X image)) (2)
larity. The retrieval outcomes are represented as
R(I) = {(i ,c ),··· ,(i ,c )},whereforeachj 5 ExperimentsandResults
1 1 k k
within the range of 1 to k, i and c correspond
j j 5.1 ExperimentalSetup
to the retrieved image and its associated caption,
Evaluation Metrics (1) In our evaluation pro-
respectively. BycomparingI withsimilarimages
cess, the quality of the answers is first assessed
from the database, we identify the entity in the
usingestablishedNLPmetricssuchasBLEU(Pa-
image region, which enables precise image-level
pineni et al., 2002), METEOR (Denkowski and
entityrecognition.
Lavie,2014),ROUGE(Lin,2004),andBLEURT
KnowledgeRetrievalviaMulti-SourceAggrega- (Sellametal.,2020;Puetal.,2021). (2)Addition-
tion Facingdiverseuserqueries,wegatherextra ally,weincorporateaccuracyandhallucinationrate
information to compile resources for accurate re- metricsfrom(Sunetal.,2023). Thesemetricsused
sponses. Somequeriesrequireup-to-dateinforma- GPT4toautomaticallymeasuretheproportionof
tion,notpresentinexistingdatabases. Wethenturn questions for which the model provides correct
toexternalsourcestocollectcriticaldatalike“year answers or incorrect/partially incorrect answers,
built,"“description,"andmore. ByusingKnowl- respectively. (3) We conduct human evaluation
edgeGraph(KG)andwebsearches,weaccessrele- followingYeetal.(2023);Moonetal.(2023).Table 3: Performance comparison of different ap- Table4: Effectivenessofevaluationmetrics.
proachesontheSnapNTelldataset.
ROUGE BLEU METEOR BELURT
Method ROUGE↑ BLEU↑ METEOR↑ BLEURT↑ τ 0.999 0.799 0.600 0.999
Instruct-BLIP(Daietal.,2023) 10.72 0.95 7.59 0.09 P_value 0.014 0.050 0.142 0.014
BLIP2(Lietal.,2023) 15.00 0.52 8.49 0.16
Mini-GPT4(Zhuetal.,2023) 26.12 5.62 25.55 0.27
LLaVA(Liuetal.,2023b) 26.86 6.03 26.97 0.31
ficient(Kendall,1938;Knight,1966;Kendalletal.,
Open-Flamingo(Awadallaetal.,2023) 30.57 6.52 22.53 0.32
COGVLM(Wangetal.,2023) 30.25 6.67 23.35 0.31
1995), comparing the results with those from the
mPLUG-Owl2(Yeetal.,2023) 31.39 6.72 24.67 0.33
LLaVA1.5(Liuetal.,2023a) 32.87 6.94 25.23 0.33 humanevaluationinSection5.4. Kendall’sτ isa
SnapNTell(ours) 35.28 7.81 29.27 0.55
measureofthecorrespondencebetweentworank-
ModelSetting WechoseLLaMA2(70B)(Tou-
ings. Valuescloseto1indicatestrongagreement,
vronetal.,2023)asourLLM.Forimageencoding,
valuescloseto-1indicatestrongdisagreement. Ta-
theCLIPimageencoder(ViT-B/32)isemployed
ble4revealedthatboththeROUGEandBLEURT
(Radfordetal.,2021;Schuhmannetal.,2022). Ad-
scoresweremoreindicativeindistinguishingthe
ditional configurations comprise a batch size of
differences among various models. This finding
2,048,theintegrationoftworesamplerlayers,and
suggeststhatthesetwometricsareparticularlysig-
theuseof64modalitytokens.
nificantinevaluatingmodelperformanceinaway
ModelTraining Weusedacleanedsubsetofthe thatalignscloselywithhumanjudgment.
LAION-2Bdataset,filteredusingtheCATmethod
5.3 AblationStudy
(Radenovicetal.,2023b)andwithanydetectable
faces blurred (Radenovic et al., 2023a). Signifi- Foramorein-depthunderstanding,weconducted
cant resources are essential to scale pre-training several ablation studies to delve into the finer de-
to 70 billion parameter models on a substantial tailsofourapproach.
datasetofover200millioninstances. Often, this EffectivenessofEntityDetection Toassessthe
necessitatestheutilizationofanFSDPwrapper,as impactofentitydetection(ED)inourmodel,we
outlinedinDettmersetal.(2023),todistributethe performed an ablation study. This involved com-
model across multiple GPUs efficiently. To opti- paringtheperformanceofourapproachwithand
mizeourtrainingprocess,weemployquantization without the ED component. As indicated in Ta-
strategies,specifically4-bitand8-bitquantization ble5,ourapproachincorporatingentitydetection
techniques(Dettmersetal.,2023),withinourmul- markedlysurpassesthevariantlackingthisfeature.
timodalframework. Inthisapproach,wemaintain Thishighlightsthesignificantcontributionandne-
theLLMcomponentofourmodelinafrozenstate, cessityoftheentitydetectionstepinourmodel’s
allowing only the image modality tokenizers to overalleffectiveness.
betrainable. Thisstrategydrasticallyreducesthe
Table5: Ablationstudyontheeffectivenessofentity
memory requirements by an order of magnitude.
detection(ED).
Asaresultoftheseoptimizations,wecansuccess-
fullytraina70billionparametermodelonasingle Method ROUGE↑ BLEU↑ METEOR↑ BELURT↑
GPUwith80GBVRAM,usingabatchsizeof4. w/oED 28.02 3.73 26.26 0.45
w/ED 35.28 7.81 29.27 0.55
5.2 ResultsandDiscussion
Head/Torso/TailEntities Headknowledgeper-
Table3displaysthecomparativeresultsbetween tainstowell-establishedentitiesforwhichthereis
the baseline models and our proposed method. awealthofavailabletrainingdata. Ideally,LLMs
Analysisofthistableindicatesthatforeverymet- could be trained to possess this knowledge, fa-
ricassessed,ourretrieval-augmentedmultimodal cilitating efficient retrieval. On the other hand,
LLM surpasses the performance of all existing torso-to-tailknowledgepertainstoless-knownor
baselinemodels. Thisstrongperformanceempha- obscure entities, often characterized by scarce or
sizes the efficiency of retrieval augmentation in non-existenttrainingdata. Providingaccesstosuch
producing responses enriched with entity-centric knowledgeinvolveseffectivelydeterminingwhen
information,therebyillustratingitssubstantialim- external information is necessary, retrieving the
pactonthetaskathand. relevantknowledgeefficiently,andseamlesslyinte-
Moreover, to gain deeper insights into which gratingitintoresponses.
evaluationmetricmoreaccuratelyreflectstheout- To assess the performance improvement for
comes,wecomputedtheKendallcorrelationcoef- head/torso/tailentities,werandomlyselected10%Table 6: Ablation study on head/torso/tail entities,
Lose
whereRAisshortforRetrievalAugmentationand∆is
100%
theperformancedifferenceofwithandwithoutRA.
75%
Accuracy↑ Hallucination↓
50%
w/oRA 24.4 75.6
Head w/RA 27.1 72.9
25%
∆(100%) 11.1%↑ 3.6%↓
w/oRA 19.1 80.9 0%
Torso w/RA 22.7 77.3 MIni-GP OT4 pen-Flamingo COGVL mM PLUG-Owl2 LLaVA 1.5 SnapNTell
∆(100%) 18.8%↑ 4.4%↓
w/oRA 6.8 93.2 Figure4: Humanevaluationresultsonpairwisecom-
Tail w/RA 12.6 87.4 parisons(%win,tie,lose)withbaselineoutputsagainst
∆(100%) 85.3%↑ 6.2%↓
themanuallyannotatedground-truthfromSnapNTell.
(1) Recognition Accuracy, where they evaluated
entities for each category, where head/torso/tail
whetherthemodelcorrectlyidentifiedtheentityin
entities are defined based on pageview statistics
the image relevant to the question; (2) Response
(popularity)inSection3.4. Theresultspresented
Accuracy, in which they assessed the factual cor-
in Table 6 clearly demonstrate that retrieval aug-
rectnessofthemodel’sresponseswhilechecking
mentationcansignificantlyenhanceperformance
foranysignsofhallucination(Rawteetal.,2023);
across various entity types. Notably, the perfor-
and (3) Pairwise Comparison, where judges se-
mance improvement for torso-to-tail entities far
lectedtheresponsethatbetteraddressedthegiven
exceeds that of head entities, effectively address-
questionintermsofcontextualappropriatenessand
ing the challenge of hallucinations in long-tailed
accuracy,categorizingresponsesaswinning,tying,
entitiesthroughretrievalaugmentation.
orlosing.
Performance of Different VQA Datasets To
In our study, we conducted pairwise compar-
demonstrate the uniqueness of our SnapNTell
isonsforeachbaselinemodelagainstground-truth
datasetcomparedtoexistingVQAdatasets,wean-
dataacross1,000samples. AsdepictedinFigure4,
alyzedtheperformanceofvariousbaselinemodels
ourmodeloutperformsthebaselinesbydisplaying
onbothtraditionalVQAdatasetsandourSnapN-
asignificantlysmallerdifferencewhenmeasured
Telldataset. Accordingtothefindingspresentedin
againstmanuallyannotatedground-truthsamples,
Table 7, the performance disparities among base-
highlightingitsrobustness.
linemodelsonexistingdatasetsarenotparticularly
marked. In contrast, on the SnapNTell dataset, 6 Conclusion
we observed significantly larger differences and
In this work, we tackle the significant challenge
notably lower performance. This indicates that
VLLMsfacewithlong-tailentityqueries,whichof-
our SnapNTell dataset is particularly effective in
tenleadtoinaccurateorhallucinatedresponses. To
evaluating the capabilities of different models to
addresstheseissues,weintroduceanentity-centric
recognizeentitiesandproduceresponsescentered
VQAtasknamedSnapNTell. Thistaskisdesigned
aroundtheseentities.
totestmodelsonentityrecognitionandtheirabil-
Table7: Ablationontheaccuracyperformanceofdif-
itytoprovidedetailed,entity-specificknowledge
ferentVQAdatasets.
in their responses. We collected a unique eval-
Method VQAv2 TextVQA OK-VQA SnapNTell uation dataset for this task, which distinguishes
Instruct-BLIP(Daietal.,2023) – 46.6 55.5 8.88
itself from existing VQA datasets by including a
BLIP2(Lietal.,2023) 52.6 43.1 54.7 16.16
Flamingo(Alayracetal.,2022) 56.3 37.9 57.8 32.17 widearrayoffine-grainedcategorizedentities,sup-
ported by images and explicit entity mentions in
5.4 HumanEvaluationResults theanswers. Thisdatasetemphasizesknowledge-
intensiveresponsesoversimplebinaryanswers. In
In alignment with the methodology presented in
addition,weproposearetrieval-augmentedmulti-
Yeetal.(2023);Moonetal.(2023),weinvolved
modalLLMsolutionfortheSnapNTelltaskasan
ahumanevaluationprocessconductedbyapanel
effectivebaseline. Ourexperimentalresultsshow
of five human judges (3 male, 2 female). These
that our model outperforms existing approaches,
judgesweregivenspecificinstructionsfortheiras-
providingmoreaccurateandcoherentanswers.
sessment, which encompassed three key aspects:
TieWinLimitations entities, as they do not explicitly highlight these
entitieswithinthedataset. Ournewlyintroduced
Inthisstudy,weintroduceanovelSnapNTelltask
datasetbridgesthisgap. Itisdesignedtotestmod-
anditsaccompanyingdataset,whichfeaturesfive
els’capabilitiesnotjustinidentifyingentitiesbut
uniquetypesofquestions,eachpairedwithmetic-
also in generating informed and entity-aware re-
ulouslyformulatedanswers. It’simportanttorec-
sponses. Furthermore,ourproposeddatasetmight
ognizethatincasesinvolvinghumanpreferences,
serve as resources for either pre-training or fine-
whicharesubjectivebynature,thegivenanswers
tuningexistingmodels,toimprovetheirabilityin
mightnotrepresenttheonlycorrectoptions. Fur-
recognizingentity-levelreal-worldobjects.
thermore, therelevancyofsomeanswersmaydi-
minish over time, highlighting the need for peri-
odic updates to the dataset to ensure its ongoing References
relevanceandaccuracy. Ourproposedmethodex-
Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,An-
hibited superior performance over existing base-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc,
lines. However,humanevaluationresultssuggest
ArthurMensch,KatieMillican,MalcolmReynolds,
significantpotentialforfurtherimprovement. Al- RomanRing,ElizaRutherford,SerkanCabi,Tengda
thoughourapproachoftennearedhuman-levelper- Han, Zhitao Gong, Sina Samangooei, Marianne
Monteiro,JacobMenick,SebastianBorgeaud,Andy
formance,itdidnotconsistentlyoutperformhuman
Brock,AidaNematzadeh,SahandSharifzadeh,Miko-
annotations, showing opportunities for future ad-
laj Binkowski, Ricardo Barreira, Oriol Vinyals,
vancements. Andrew Zisserman, and Karen Simonyan. 2022.
Flamingo: a visual language model for few-shot
EthicsStatement learning. ArXiv,abs/2204.14198.
Inthisstudy,thedatasetwassourcedfrompublicly Anas Awadalla, Irena Gao, Josh Gardner, Jack Hes-
accessibledatabases,andallauthordetailsremain sel,YusufHanafy,WanrongZhu,KalyaniMarathe,
YonatanBitton,SamirYitzhakGadre,ShioriSagawa,
anonymous. Weconscientiouslyexcludedanycon-
JeniaJitsev,SimonKornblith,PangWeiKoh,Gabriel
tentfromourdatasetthatcouldbeconsideredethi-
Ilharco, Mitchell Wortsman, and Ludwig Schmidt.
callysensitiveorrelatedtopersonalprivacy,such 2023. Openflamingo:Anopen-sourceframeworkfor
as images depicting human faces. To our under- traininglargeautoregressivevision-languagemodels.
ArXiv,abs/2308.01390.
standing, and with careful consideration, we do
notanticipateanydetrimentalapplicationsarising Yingshan Chang, Mridu Baldevraj Narang, Hisami
from the findings or methodologies presented in Suzuki, Guihong Cao, Jianfeng Gao, and Yonatan
thisresearch. Bisk. 2021. Webqa: Multihop and multimodal qa.
2022IEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPR),pages16474–16483.
BroaderImpact
YangChen,HexiangHu,YiLuan,HaitianSun,Soravit
Currentmodelshavemadecommendableprogress
Changpinyo,AlanRitter,andMing-WeiChang.2023.
in grasping the nuanced semantics and context-
Canpre-trainedvisionandlanguagemodelsanswer
sensitive aspects of Visual Question Answering visualinformation-seekingquestions? InEMNLP.
(VQA). However, their efficacy in factual VQA
AakankshaChowdheryetal.2022. Palm: Scalinglan-
tasks, which require precise and factual answers
guagemodelingwithpathways. J.Mach.Learn.Res.,
abouttangibleentitiesandevents,revealscertain
24:240:1–240:113.
deficiencies. Thisisespeciallytruefortorso-to-tail
Wenliang Dai, Junnan Li, Dongxu Li, Anthony
or long-tail entities. Despite their prevalence in
Meng Huat Tiong, Junqi Zhao, Weisheng Wang,
therealworld,theseentitiesareunderrepresented
Boyang Albert Li, Pascale Fung, and Steven C. H.
in training datasets, leading to a common issue Hoi. 2023. Instructblip: Towards general-purpose
wheremodelsproduceplausibleyetinaccurateor vision-language models with instruction tuning.
invented responses, a phenomenon often termed ArXiv,abs/2305.06500.
“hallucinations" in the realm of model-generated
MichaelJ.DenkowskiandAlonLavie.2014. Meteor
content. Tacklingandminimizingthesehallucina- universal: Languagespecifictranslationevaluation
tionsisvitalforenhancingthetrustworthinessand foranytargetlanguage. InWMT@ACL.
applicabilityofthesemodelsinpracticalscenarios.
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
TheexistingVQAdatasets,however,areinade-
LukeZettlemoyer.2023. Qlora: Efficientfinetuning
quateforevaluatingamodel’sabilitytorecognize ofquantizedllms. ArXiv,abs/2305.14314.JosephL.Fleiss.1971. Measuringnominalscaleagree- JunnanLi,DongxuLi,SilvioSavarese,andStevenC.H.
ment among many raters. Psychological Bulletin, Hoi. 2023. Blip-2: Bootstrapping language-image
76:378–382. pre-training with frozen image encoders and large
languagemodels. ArXiv,abs/2301.12597.
YashGoyal,TejasKhot,DouglasSummers-Stay,Dhruv
Batra,andDeviParikh.2016. Makingthevinvqa LiunianHaroldLi,PengchuanZhang,HaotianZhang,
matter: Elevatingtheroleofimageunderstandingin Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan
visualquestionanswering. InternationalJournalof Wang,LuYuan,LeiZhang,Jenq-NengHwang,Kai-
ComputerVision,127:398–414. Wei Chang, and Jianfeng Gao. 2021. Grounded
language-imagepre-training. 2022IEEE/CVFCon-
Liangke Gui, Borui Wang, Qiuyuan Huang, Alexan- ferenceonComputerVisionandPatternRecognition
derG.Hauptmann,YonatanBisk,andJianfengGao. (CVPR),pages10955–10965.
2021. Kat: Aknowledgeaugmentedtransformerfor
vision-and-language. InNorthAmericanChapterof Chin-YewLin.2004. Rouge: Apackageforautomatic
theAssociationforComputationalLinguistics. evaluationofsummaries. InACL2004.
KelvinGuu,KentonLee,ZoraTung,PanupongPasu-
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
pat,andMing-WeiChang.2020. Realm: Retrieval-
Lee.2023a. Improvedbaselineswithvisualinstruc-
augmented language model pre-training. ArXiv, tiontuning. ArXiv,abs/2310.03744.
abs/2002.08909.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJae
DarrylHannan,AkshayJain,andMohitBansal.2020.
Lee. 2023b. Visual instruction tuning. ArXiv,
Manymodalqa:Modalitydisambiguationandqaover
abs/2304.08485.
diverse inputs. In AAAI Conference on Artificial
Intelligence.
Kenneth Marino, Mohammad Rastegari, Ali Farhadi,
and Roozbeh Mottaghi. 2019. Ok-vqa: A visual
Hexiang Hu, Yi Luan, Yang Chen, Urvashi Khandel-
question answering benchmark requiring external
wal,MandarJoshi,KentonLee,KristinaToutanova,
knowledge. 2019 IEEE/CVF Conference on Com-
and Ming-Wei Chang. 2023. Open-domain visual
puterVisionandPatternRecognition(CVPR),pages
entityrecognition: Towardsrecognizingmillionsof
3190–3199.
wikipediaentities. ArXiv,abs/2302.11154.
ThomasMensink,JasperR.R.Uijlings,LluísCastrejón,
Drew A. Hudson and Christopher D. Manning. 2019.
ArushiGoel,FelipeCadar,HowardZhou,FeiSha,
Gqa: A new dataset for real-world visual reason-
AndreF.deAraújo,andVittorioFerrari.2023. Ency-
ing and compositional question answering. 2019
clopedicvqa: Visualquestionsaboutdetailedproper-
IEEE/CVFConferenceonComputerVisionandPat-
tiesoffine-grainedcategories. 2023IEEE/CVFIn-
ternRecognition(CVPR),pages6693–6702.
ternationalConferenceonComputerVision(ICCV),
pages3090–3101.
JeffJohnson,MatthijsDouze,andHervéJégou.2017.
Billion-scale similarity search with gpus. IEEE
Seungwhan Moon, Andrea Madotto, Zhaojiang Lin,
TransactionsonBigData,7:535–547.
TusharNagarajan,MattSmith,ShashankJain,Chun-
FuYeh,PrakashMurugesan,PeymanHeidari,Yue
M.G.Kendall.1938. Anewmeasureofrankcorrela-
Liu, Kavya Srinet, Babak Damavandi, and Anuj
tion. Biometrika,30:81–93.
Kumar. 2023. Anymal: An efficient and scalable
any-modality augmented language model. ArXiv,
M.G.Kendall,AlanL.Stuart,andJ.KeithOrd.1995.
Kendall’sadvancedtheoryofstatistics. Journalof abs/2309.16058.
theAmericanStatisticalAssociation,90:398.
KishorePapineni,SalimRoukos,ToddWard,andWei-
WilliamKnight.1966. Acomputermethodforcalcu- JingZhu.2002. Bleu: amethodforautomaticevalu-
latingkendall’stauwithungroupeddata. Journalof ationofmachinetranslation. InACL.
theAmericanStatisticalAssociation,61:436–439.
AmyPu,HyungWonChung,AnkurP.Parikh,Sebastian
JingYuKoh,RuslanSalakhutdinov,andDanielFried. Gehrmann, and Thibault Sellam. 2021. Learning
2023. Grounding language models to images for compactmetricsformt. InConferenceonEmpirical
multimodalinputsandoutputs. MethodsinNaturalLanguageProcessing.
Paul Lerner, Olivier Ferret, Camille Guinaudeau, FilipRadenovic,AbhimanyuDubey,AbhishekKadian,
Hervé Le Borgne, Romaric Besançon, José G. Todor Mihaylov, Simon Vandenhende, Yash Patel,
Moreno,andJesúsLovón-Melgarejo.2022. Viquae, YiWen,VigneshRamanathan,andDhruvMahajan.
a dataset for knowledge-based visual question an- 2023a. Filtering,distillation,andhardnegativesfor
swering about named entities. Proceedings of the vision-languagepre-training. InProceedingsofthe
45th International ACM SIGIR Conference on Re- IEEE/CVFConferenceonComputerVisionandPat-
searchandDevelopmentinInformationRetrieval. ternRecognition,pages6967–6977.FilipRadenovic,AbhimanyuDubey,AbhishekKadian, Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav,
TodorMihaylov,SimonVandenhende,YashJ.Patel, Yizhong Wang, Akari Asai, Gabriel Ilharco, Han-
Yi Wen, Vignesh Ramanathan, and Dhruv Kumar nanehHajishirzi,andJonathanBerant.2021. Mul-
Mahajan. 2023b. Filtering, distillation, and hard timodalqa: Complex question answering over text,
negatives for vision-language pre-training. 2023 tablesandimages. ArXiv,abs/2104.06039.
IEEE/CVFConferenceonComputerVisionandPat-
HugoTouvronetal.2023. Llama2: Openfoundation
ternRecognition(CVPR),pages6967–6977.
andfine-tunedchatmodels. ArXiv,abs/2307.09288.
AlecRadford,JongWookKim,ChrisHallacy,Aditya
Maria Tsimpoukelli, Jacob Menick, Serkan Cabi,
Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-
S.M.AliEslami,OriolVinyals,andFelixHill.2021.
try, Amanda Askell, Pamela Mishkin, Jack Clark,
Multimodalfew-shotlearningwithfrozenlanguage
GretchenKrueger,andIlyaSutskever.2021. Learn-
models. InNeuralInformationProcessingSystems.
ingtransferablevisualmodelsfromnaturallanguage
supervision. InInternationalConferenceonMachine PengWang,QiWu,ChunhuaShen,AnthonyR.Dick,
Learning. andAntonvandenHengel.2016. Fvqa: Fact-based
visual question answering. IEEE Transactions on
Vipula Rawte, A. Sheth, and Amitava Das. 2023. A PatternAnalysisandMachineIntelligence,40:2413–
surveyofhallucinationinlargefoundationmodels. 2427.
ArXiv,abs/2309.05922.
Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi
Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang,
Christoph Schuhmann, Romain Beaumont, Richard
LeiZhao,XixuanSong,JiazhengXu,BinXu,Juanzi
Vencu,CadeGordon,RossWightman,MehdiCherti,
Li, Yuxiao Dong, Ming Ding, and Jie Tang. 2023.
Theo Coombes, Aarush Katta, Clayton Mullis,
Cogvlm: Visualexpertforpretrainedlanguagemod-
MitchellWortsman,PatrickSchramowski,Srivatsa
els. ArXiv,abs/2311.03079.
Kundurthy, Katherine Crowson, Ludwig Schmidt,
RobertKaczmarczyk,andJeniaJitsev.2022. Laion-
ShengqiongWu,HaoFei,LeigangQu,WeiJi,andTat-
5b: Anopenlarge-scaledatasetfortrainingnextgen-
SengChua.2023. Next-gpt: Any-to-anymultimodal
erationimage-textmodels. ArXiv,abs/2210.08402.
llm. ArXiv,abs/2309.05519.
Dustin Schwenk, Apoorv Khandelwal, Christopher NanYang,TaoGe,LiangWang,BinxingJiao,Daxin
Clark,KennethMarino,andRoozbehMottaghi.2022. Jiang, Linjun Yang, Rangan Majumder, and Furu
A-okvqa: Abenchmarkforvisualquestionanswer- Wei. 2023a. Inference with reference: Lossless
ingusingworldknowledge. InEuropeanConference acceleration of large language models. ArXiv,
onComputerVision. abs/2304.04487.
Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Anand
2020. Bleurt: Learningrobustmetricsfortextgen- Korthikanti, Weili Nie, De-An Huang, Linxi (Jim)
eration. In Annual Meeting of the Association for Fan, Zhiding Yu, Shiyi Lan, Bo Li, Mingyan Liu,
ComputationalLinguistics. Yuke Zhu, Mohammad Shoeybi, Bryan Catanzaro,
ChaoweiXiao,andAnimaAnandkumar.2023b. Re-
HriturajSingh,AnshulNasery,DenilMehta,Aishwarya vilm: Retrieval-augmented visual language model
Agarwal,JatinLamba,andBalajiVasanSrinivasan. for zero and few-shot image captioning. ArXiv,
2021. Mimoqa:Multimodalinputmultimodaloutput abs/2302.04858.
questionanswering. InNorthAmericanChapterof
MichihiroYasunaga, ArmenAghajanyan, WeijiaShi,
theAssociationforComputationalLinguistics.
RichJames,JureLeskovec,PercyLiang,MikeLewis,
LukeZettlemoyer,andWentauYih.2023. Retrieval-
Krishna Srinivasan, Karthik Raman, Jiecao Chen,
augmentedmultimodallanguagemodeling. ArXiv,
Michael Bendersky, and Marc Najork. 2021. Wit:
abs/2211.12561.
Wikipedia-basedimagetextdatasetformultimodal
multilingualmachinelearning. Proceedingsofthe QinghaoYe,HaiyangXu,JiaboYe,MingYan,Anwen
44th International ACM SIGIR Conference on Re- Hu,HaoweiLiu,QiQian,JiZhang,FeiHuang,and
searchandDevelopmentinInformationRetrieval. JingrenZhou.2023. mplug-owl2: Revolutionizing
multi-modallargelanguagemodelwithmodalitycol-
KrishnaSrinivasan,KarthikRaman,AnupamSamanta, laboration.
Ling-YenLiao,LucaBertelli,andMichaelBender-
sky.2022. Quill: Queryintentwithlargelanguage Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing
modelsusingretrievalaugmentationandmulti-stage Sun, Tong Xu, and Enhong Chen. 2023. A sur-
distillation. InConferenceonEmpiricalMethodsin vey on multimodal large language models. ArXiv,
NaturalLanguageProcessing. abs/2306.13549.
DeyaoZhu, JunChen, XiaoqianShen, XiangLi, and
KaiSun,Y.Xu,HanwenZha,YueLiu,andXinhsuai
MohamedElhoseiny.2023. Minigpt-4: Enhancing
Dong.2023. Head-to-tail: Howknowledgeableare
vision-languageunderstandingwithadvancedlarge
largelanguagemodels(llm)? a.k.a.willllmsreplace
languagemodels. ArXiv,abs/2304.10592.
knowledgegraphs? ArXiv,abs/2308.10168.A MoreDetailsabouttheDatasetBuilding
MoredetailsaboutthedatasetbuildingprocessareshowninFigure5.
Figure5: Thepertinentinformationcollectedduringdatasetbuilding,i.e.,fromWikipediaforeachentity,which
includesthesummaryofthegeneralintroduction,toponym,lococationinformation,andsoon.
B MoreDetailsabouttheFilteringProcess
MoredetailsaboutthefilteringprocessareshowninTable8.
C TypesofQuestions
MoreintroductionofdifferenttypesofquestionintheSnapNTelldatasetareshownTable9.
D Method
Inthissection,wewillintroducethedetailsofourproposedretrieval-augmentedmultimodalLLMmodel.
ThearchitectureofourmodelisshowninFigure7. Ourmodelcanbeconsideredtwofold: (1)Retrieval
augmentation. Giventheinputimage-questionpair,weretrieveusefulentity-centricinformationwithin
knowledgesources. (2)Entity-centricknowledge-basedanswergeneration. Theretrievedinformation
willbecombinedwiththeimageandquestiontogethertogeneratetheanswer. Moredetailsareintroduced
inthefollowingsections.
D.1 RetrievalAugmentation
The retrieval augmentation process can be subdivided into three distinct steps: (i) Semantic region
extraction via language-guided object detection, (ii) Entity recognition via image retrieval, and (iii)Table8: Filteringstatisticsoftheentitydataset. [1stWikifiltering]: removingoneswithoutwikipage. [2ndGoogle
filtering]: removingoneswithoutenoughimagesviagooglesearchAPI.[3rdWikifiltering]: removingentityname
withambiguouswikipages.
Maincategory OriginalEntity 1stWikifiltering 2ndGooglefiltering 3rdWikifiltering
landmark 1595 1000 899 753
painting 1057 367 358 288
sculpture 300 164 164 134
food 883 338 337 271
fruit 361 236 233 180
vegetable 389 290 286 214
mammal 778 633 619 434
hibian 211 148 139 124
insect 366 179 176 145
fish 1089 1054 987 722
bird 739 546 545 480
Category
reptile 279 232 231 210
celebrity 1514 1484 1466 732
instrument 477 375 368 277
plant 606 601 593 489
electronics 432 354 342 269
tool 801 213 209 150
transportation 334 296 290 227
sport 694 478 464 395
book 1030 826 777 645
household 475 319 299 221
car 500 320 320 208
Summary 22 14910 10453 10102 7568
Figure6: Collectingimagesforbuildingtheevaluationdataset. Licenses: CCPublicdomain,CCAttribute,AA
Sharealike,CCNoncommercial,orCCNonderivedlicenses. Metadata: imageURLs,sourcepageURLs,renamed
imagenames,andthecorrespondingWikipediapageURL.
Knowledgeretrievalviamulti-sourceaggregation.
SemanticRegionExtractionviaLanguage-GuidedObjectDetection Duetothepresenceofentities
withintheimagethatoccupyonlyaportionoftheavailablespace,employingacomprehensiveimage-level
entityrecognitionapproachmayleadtoadecreaseinrecognitionperformance. Instead,weopttoinitially
extracttheimageregioncontainingtheentityandutilizethisspecificregioninsubsequentrecognition
processestoenhanceaccuracy. Duringthisphase,weleveragealanguage-guidedobjectdetectionmodel,
i.e., GLIP (Li et al., 2021), to extract meaningful regions from complex images. This approach helps
preciselyidentifyandextractimageregionsdirectlyrelevanttospecifictextualqueries. Itaccomplishes
thisbyunderstandingthecontextofthequeryandadjustingitsobjectdetectionmethodtofindthemostTable9: Typesofquestions.
Typesofquestions Definition
Static facts (absolute Theseareobjectivefactsthatareconcreteandarenotcontingentonotherconditions.
facts,discretefacts) Theycanusuallybeansweredwithashort,uniqueanswer. Forexample: Whenwas
BarackObamaborn?
Narrativefacts Thesefactsencompasscomprehensionoflargercontexts(e.g.,songlyrics,movieplot,
historicalevents).Theyarefactualinthesensethatthecontentofthenarrativeshould
accuratelyreflectthesourcematerialorevents,butacorrectanswerisusuallynotunique,
astheycanvaryintheirlevelofdetailandfocus.Forexample:Whatistheplotof“The
Godfather”?
Dynamicfacts Thesearefactsthataresubjecttochangeovertime. Forexample: WhatistheYelp
customerratingoftheElevenMadisonParkrestaurantinNYC?
Proceduralfacts Theseareusuallyanswersto“how”questions,outliningasequenceofstepstoaccom-
plishatask.Whilethestepsmaynotbeuniqueandcouldbesubjective,inmanycases,
ananswercanstillbeclassifiedaslogical(factual)ornonsensical(ahallucination).Note
thatthesefactscanoverlapwithdynamicfactsornarrativefacts.Forexample,Howdo
youcheckthebatterylevelofmyRay-BanStoriesGlasses?
Subjective facts These“facts”arenotobjective,indisputablefacts,butarebasedonindividualperspec-
(opinion-basedfacts) tivesorexperiences.Recommendationsfallinthiscategory.Whilethere’sgenerallyno
singlecorrectanswertoquestionsseekingsubjectivefacts,itstillrequiresthesystem
tounderstandthetopicandprovidereasonableanswersgroundedbyworldfacts. For
example:WhereshouldIvisitTokyonextmonth?
importantimageareas. Thisstepenablesthesystemtobetterunderstandthequery’scontext,resultingin
moreaccurateandcontextuallymeaningfulregionextraction.
EntityRecognitionviaImageRetrieval Toaccomplishthisgoal,webeginbyconstructingasimilarity
indexusingCLIPembeddings,specificallyemployingFaiss(Johnsonetal.,2017)asourindexingtool.
OurindexingdatabaseisestablishedbasedontheWITdataset(Srinivasanetal.,2021). Thisdatabase
followsakey-valuemappingstructure,wherethekeysrepresentCLIPViT-B/32imageembeddings,and
thecorrespondingtextdescriptionsserveasthevalues. Faiss,knownforitsefficiencyinsimilaritysearch,
isutilizedforindexing(Johnsonetal.,2017).
Oncetheindexingdatabaseissetup,wearereadytoproceedwiththequeryprocess. Givenaninput
queryimage,denotedasI (whichistheentityimageregionextractedintheprecedingstep),weperform
ak-nearestneighborretrievalbasedoncosinesimilaritybetweentheembeddingsofthequeryimageand
thoseofthedatabaseimages. TheretrievaloutcomesarerepresentedasR(I) = {(i ,c ),··· ,(i ,c )},
1 1 k k
whereforeachj withintherangeof1tok,i andc correspondtotheretrievedimageanditsassociated
j j
caption, respectively. Subsequently, by using the extracted image region as input for a search in the
indexingdatabase,weidentifytheentitywithintheextractedimageregion. Thisidentificationisachieved
bycomparingitwiththemostsimilarimagesretrievedfromtheindexingdatabase,ultimatelyresultingin
image-levelentityrecognition.
KnowledgeRetrievalviaMulti-SourceAggregation Giventhewidearrayofquestionsusersmaypose,
weneedtoobtainadditionalinformationtocompilethenecessaryresourcesforcraftingaccurateresponses.
Furthermore,certainqueriesmaydemandthelatestinformation,whichisnotreadilyavailablewithin
pre-existingdatabasesorknowledgegraphs. Insuchcases,werelyonexternalsourcesofknowledge,such
asonlinereferences,togatheressentialdata,encompassingelementslike“yearbuilt,"“description,"and
otherpertinentdetails. Toaccomplishthis,weleverageKnowledgeGraph(KG)andconductwebsearches
toaccessrelevantknowledgeconnections. Thisapproachenablesustoacquireawealthofinformation
concerning the specified image region, thereby bolstering our capacity to grasp and contextualize the
extractedcontenteffectively.Figure7: ThearchitectureofourSnapNTellmodel. Theinputtothemodelisanimage-questionpair,andourmodel
firstusesretrievalaugmentationtoretrieveusefulinformationregardingtheentityintheimage. Then,theretrieved
informationiscombinedwiththequestionasinputtothewordembeddinglayer,wherethetextembeddingswillbe
combinedwithimage-projectedembeddingsastheinputtoLLM,whichfinallygeneratesaknowledgeableanswer
astheoutput.
D.2 Entity-centricKnowledge-basedAnswerGeneration
Followingtheprecedingstep,wherewe’vegatheredinsightfulinformationfromdiversesources,wenow
proceedtothesecondphase: determininghowtointegratetheinputimage,thequestion,andtheretrieved
informationinordertoproduceaknowledge-drivenresponse.
OurapproachisillustratedinFigure7. Ourstrategyforimprovingthemodel’smultimodalcompre-
hension entails pre-training a LLM using paired multimodal data, which comprises images alongside
corresponding textual descriptions. To achieve this, we draw inspiration from Moon et al. (2023) and
createlightweightadaptersforeachmodality. Theseadaptersfacilitatethetransformationofinputsinto
thetexttokenembeddingspaceofadesignatedLLM.
OurapproachtransformsthetexttokenembeddingspaceoftheLLMintoaunifiedtokenembedding
space, where tokens can represent either textual or image content. The number of token embeddings
allocatedtoeachinputmodalityispredeterminedforeachadapter,rangingfrom64to256. Throughout
the alignment training process, we keep the model parameters of the underlying LLM frozen. This
approachnotonlyacceleratesconvergencecomparedtotrainingthemodelfromscratchbutalsoallows
themodeltoinheritthereasoningcapabilitiesoftheLLMduringinference. Additionally,tomaximize
featurecompatibility,weemployanencoderdenotedasg(·)fortheimagemodality. Thisencoderhas
previouslybeenalignedwithatextembeddingspace,forinstance,inthecaseofCLIP(Radfordetal.,
2021; Schuhmann et al., 2022). For each pair of text and image, represented as (X ,X ), we
text image
align them using specific objectives along with a projection module, such as the Perceiver Resampler
(Alayracetal.,2022)forthevisionencoder.
L
(cid:89) [i] [1:i−1]
p(X |X ) = p (X |Z ,Z ) (3)
text image θ text image text
i=1
Z = Proj (h ,g(X )) (4)
image θ latents imageE MoreRelatedWorks
Knowledge-basedVQA Variousvision-languagetasksoftenrequireknowledgetoanswerquestions
based on image content and have evolved inrecent years. Beginning with datasets like FVQA (Wang
et al., 2016), which extracted facts from pre-established knowledge bases, the field has progressed to
morechallengingonesliketheOK-VQAdataset(Marinoetal.,2019),encompassingdiverseknowledge
categories. MultiModalQA (Talmor et al., 2021) introduced complexity with questions demanding
cross-modal reasoning over snippets, tables, and images. The successor of OK-VQA, AOK-VQA
(Schwenketal.,2022),raisesthebarbyprovidingquestionsthattranscendsimpleknowledgebasequeries.
ManyModalQA(Hannanetal.,2020)shiftsthefocustoanswermodalityselection,MIMOQA(Singh
et al., 2021) emphasizes multimodal answer extraction, and WebQA (Chang et al., 2021) introduces
real-worldknowledge-seekingquestions,albeitwithsomelimitationsregardingentitycategorizationand
granularity. MorecomparisondetailsareintroducedinSection3.5.
MultimodalLLMs Expandingtext-onlyLLMstointerpretvisualinformationtypicallyinvolvesin-
tegratingavisualencoderwithafrozenLLM,usingextensiveimagecaptioningdatasetsforalignment
(Kohetal.,2023;Wuetal.,2023;Chowdheryetal.,2022). Thisintegrationcanbeaccomplishedthrough
methodssuchasadapter-basedtuning(Alayracetal.,2022),whichfine-tunesasmallportionofthemodel
toprocessvisualinputs,orprefixtuning(Tsimpoukellietal.,2021),wheretrainedprefixedvectorsare
inputted to guide the frozen LLM towards contextually relevant text outputs based on the visual data.
ThesetechniquesallowLLMstomaintaintheirlinguisticprowesswhilegainingvisualunderstanding
withoutfullmodelretraining(Yinetal.,2023).
Retrieval augmented LLM Several prior approaches have investigated retrieval-augmented in the
text-only setting or image captioning tasks. Guu et al. (2020) augmented language model pretraining
withalatentknowledgeretriever,whichallowsthemodeltoretrieveandattendoverdocumentsfrom
alargecorpussuchasWikipedia,usedduringpretraining,fine-tuning,andinference. Srinivasanetal.
(2022) demonstrated that retrieval augmentation of queries provides LLMs with valuable additional
context,enablingimprovedunderstanding. Yasunagaetal.(2023)proposedaretrievertoretrieverelevant
multimodaldocumentsfromexternalmemoryandusethegeneratortomakepredictionsfortheinput.
Yangetal.(2023a)proposedanacceleratortolosslesslyspeedupLLMinferencewithreferencesthrough
retrieval. Yang et al. (2023b) introduced a retrieval-augmented visual language model, built upon the
Flamingo (Alayrac et al., 2022), which supports retrieving the relevant knowledge from the external
databaseforzeroandin-contextfew-shotimagecaptioning. AnotherrelatedworkbyGuietal.(2021)
integratedimplicitandexplicitknowledgeinanencoder-decoderarchitectureforjointlyreasoningover
bothknowledgesourcesduringanswergeneration.
Open-domain visual entity recognition Hu et al. (2023) introduced Open-domain Visual Entity
Recognition(OVEN)forlinkingimagestoWikipediaentitiesthroughtextqueries. Chenetal.(2023)
presentedINFOSEEK,aVisualQuestionAnsweringdatasetdesignedforinformation-seekingqueries.
OVENexcelsatentityrecognitionbutreliesonaknowledgebaseforentitynames,whileINFOSEEK
primarilyprovidesfactualanswers. Ourresearchaimstobridgethesegapsbygeneratinginformative
paragraphsthatoffercontext,enablingadeeperunderstandingbeyondmerefacts.
F MoreStatisticsoftheSnapNTellDataset
InTable10andFigure9,10,11,weshowmorestatisticsoftheSnapNTelldataset.Figure8: ExamplesfromourSnapNTelldataset.
Table10: CategorystatisticsoftheSnapNTelldataset.
Category Numberofentities
landmark 753
painting 288
sculpture 134
food 271
fruit 180
vegetable 214
mammal 434
fish 124
bird 145
reptile 722
amphibian 480
Category
insect 210
celebrity 732
instrument 277
plant 489
electronics 269
tool 150
transportation 227
sport 395
book 645
household 221
car 208
Summary 22 7568household landmark
2.9% 9.9%
book painting
8.5% 3.8%
sport food
5.2% 3.6%
fruit
transportation
2.4%
3.0%
vegetable
2.8%
electronics
mammal
3.6%
5.7%
plant
6.5%
insect
instrument
1.9%
3.7%
celebrity fish
9.7% 9.5%
reptile bird
2.8% 6.3%
Figure9: Statisticsofnumberofentitiesineachcategory.
book landmark
5.7% 9.1%
sport sculpture
2.9% 2.5%
transportation food
1.5% 2.6%
plant fruit
1.9% 2.3%
car vegetable
2.7% 2.5%
mammal
5.8%
fish
2.2%
celebrity
49.3%
Figure10: Statisticsofallpageviewsforallcategories.
landmark
painting
sculpture
food
fruit
vegetable
mammal
amphibian
insect
fish
bird
reptile
celebrity
instrument
plant
electronics
tool
transportation
sport
book
household
car
0 1000 2000 3000 4000 5000
Figure11: Averagepageviewperentitywithineachcategory,whereaveragepageviewisdefinedasthesumof
pageviews/numberofentity.G SomeResultExamplesandHumanEvaluation
In Table 11, we showed several example result by different models, and the corresponding human
evaluationresults.
Table11: Examplesofanswersgeneratedbydifferentmodels, whereGround-truth, BLIP2, MiniGPT4, Open-
Flamingo,InstructBLIP,LLaVA,SnapNTellareassignedasM0,M1,...,M6inrating.
Image
Question Whereistheattractionlocated?
Ground-truth(M0) AbelTasmanNationalParkislocatedatthenortherntipoftheSouthIslandof
NewZealandbetweenGoldenBayandTasmanBay
BLIP-2(M1) california
Mini-GPT4(M2) Theimageshowsabeachwithabodyofwaterintheforegroundandtreesand
mountainsinthebackground.
Open-Flamingo(M3) ItislocatedinNewZealand.
Instruct-BLIP(M4) beach1
LLaVA(M5) Theimageislocatedatthebeach,whichissurroundedbyaforestoftrees.
SnapNTell (Ours) TheimageisnotlocatedintheRepublicofAltai,butratherintheSouthIsland
(M6) ofNewZealand.ThelocationistheAbelTasmanNationalPark,specifically
theareaaroundAwaroaBeach.
HumanRating M0=M6>M3>M1=M2=M5>M4
Image
Question Whatdatediditopentothepublic?
Ground-truth(M0) TheAcropolisMuseumwasinauguratedonJune20,2009aftermanyyearsof
planningandconstruction
BLIP-2(M1) 1January2009
Mini-GPT4(M2) ThemuseumopenedtothepubliconJanuary1,2018
Open-Flamingo(M3) ThemuseumopenedtothepubliconDecember12,1987.
Instruct-BLIP(M4) 2007
LLaVA(M5) ThemuseumopenedtothepubliconApril13,2019.
SnapNTell (Ours) TheAcropolisMuseumopenedtothepubliconJune20,2009.
(M6)
HumanRating M0=M6>M1>M2=M3=M4=M5Image
Question Whatisthenameoftheviewinthispicture?
Ground-truth(M0) LakeTiticacaisalargefreshwaterlakelocatedintheAndesMountainsspanning
thebordersofbothBoliviaandPeru
BLIP-2(M1) atowninPeru
Mini-GPT4(M2) Itisasmalltownwithcolorfulbuildingsandboatsinthewater.
Open-Flamingo(M3) ThisisthecityofPuertoNatalesinChile.
Instruct-BLIP(M4) harbor2
LLaVA(M5) Itisacityscapewithboatsinthewater,locatednearamountainousarea.
SnapNTell (Ours) It’sLakeTiticacaintheAndesMountains.
(M6)
HumanRating M0=M6>M1>M3>M2=M5>M4
Image
Question Whatistheageofthebuildingnow?
Ground-truth(M0) TheChapelBridgeisover700yearsoldhavingbeenbuiltin1333
BLIP-2(M1) 16thcentury
Mini-GPT4(M2) Theageofthebuildingintheimageisnotknown.
Open-Flamingo(M3) Theageofthebridgeisover100yearsold.
Instruct-BLIP(M4) old
LLaVA(M5) Theageofthebuildingintheimageisnotspecified.However,itisdescribed
asanoldbuilding,whichsuggeststhatitcouldbequiteoldorhistoric.
SnapNTell (Ours) TheChapelBridge(ChapelBridgeorKapellbrücke)inLucerne,Switzerland,
(M6) isamedievalwoodenbridgethatdatesbacktothe14thcentury. Itwasbuilt
in1333andisoneoftheoldestcoveredbridgesinEurope,whichisover700
yearsoldnow.
HumanRating M6>=M0>M1=M2=M3=M4=M5