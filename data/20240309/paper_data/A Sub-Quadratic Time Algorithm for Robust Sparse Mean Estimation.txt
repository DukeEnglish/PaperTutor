A Sub-Quadratic Time Algorithm for
Robust Sparse Mean Estimation
Ankit Pensia
IBM Research
ankitp@ibm.com
March 8, 2024
Abstract
We study the algorithmic problem of sparse mean estimation in the presence of adversarial
outliers. Specifically, the algorithm observes a corrupted set of samples from N(µ,I ), where
d
the unknown mean µ∈Rd is constrained to be k-sparse. A series of prior works has developed
efficientalgorithmsforrobustsparsemeanestimationwithsamplecomplexitypoly(k,logd,1/ϵ)
and runtime d2poly(k,logd,1/ϵ), where ϵ is the fraction of contamination. In particular, the
fastest runtime of existing algorithms is quadratic (Ω(d2)), which can be prohibitive in high
dimensions. This quadratic barrier in the runtime stems from the reliance of these algorithms
on the sample covariance matrix, which is of size d2. Our main contribution is an algorithm for
robustsparsemeanestimationwhichrunsinsubquadratic timeusingpoly(k,logd,1/ϵ)samples.
We also provide analogous results for robust sparse PCA. Our results build on algorithmic
advances in detecting weak correlations, a generalized version of the light-bulb problem by
Valiant [Val15].
1 Introduction
Meanestimation,afundamentalunsupervisedinferencetaskstudiedinliterature,maybedescribed
as follows: Given a family of distributions P over Rd, the algorithm observes a set of i.i.d. points
from an unknown P ∈ P, with the goal of outputting µ such that, with high probability, ∥µ−µ∥ is
b b 2
small. Although this framework is well-studied in the literature, the data observed in practice may
deviate from the i.i.d. assumption and additionally may contain outliers. Crucially, these outliers
can easily break standard off-the-shelf estimators, for example, sample mean, geometric median,
and coordinate-wise median. To address this challenge, the field of robust statistics was initiated
in the 1960s, aiming to develop algorithms that are robust to outliers [Hub64; ABHHRT72; HR09].
Before proceeding further, we formally define the contamination model we study in this paper.
Definition 1.1 (Strong Contamination Model). Given a corruption parameter ϵ ∈ (0,1/2) and a
distribution P on uncorrupted samples, an algorithm obtains samples fromP with ϵ-contamination
as follows: (i) The algorithm specifies the number n of samples it requires. (ii) A set S of n i.i.d.
samplesfromP isdrawnbutnotyetshowntothealgorithm. (iii)Anarbitrarilypowerfuladversary
then inspects S, before deciding to replace any subset of ⌈ϵn⌉ samples with arbitrarily corrupted
points (“outliers”) to obtain the contaminated set T, which is then returned to the algorithm. We
say T is an ϵ-corrupted version of S and a set of ϵ-corrupted samples from P.
Our focus will be on high-dimensional distributions, i.e., when the distribution P is over Rd for
larged. Dealingwithoutliersbecomesharderinhighdimensionsbecauseclassical outlierscreening
1
4202
raM
7
]SD.sc[
1v62740.3042:viXraprocedures (which otherwise work well in low dimensions) rely on the norm of the data points and
are too coarse to distinguish outliers from inliers. Nevertheless, a long line of research, spurred
by advances in [DKKLMS16; LRV16], has developed a systematic theory of handling outliers in
high-dimensional robust statistics [DK23]. Notwithstanding this progress, major gaps persist in
our fine-grained understanding of fast robust algorithms for data with additional structure.
Structured high-dimensional data distributions are ubiquitous in practice, e.g., natural images
andsounds. Moreover, leveragingtheseunderlyingstructuresoftendramaticallyimprovesalgorith-
mic performance, e.g., in terms of error. A well-studied structure both in the theory and practice
of high-dimensional statistics is sparsity, see, for example, the textbooks [EK12; HTW15; van16].
Consequently, we concentrate our efforts on structured mean estimation, where we assume that the
underlying mean is sparse, i.e., an overwhelming majority of its coordinates are zero.
In light of the challenges posed by outliers above and the prevalence and importance of sparsity,
we study the problem of robust sparse mean estimation. We say a vector x ∈ Rd is k-sparse if x has
at most k non-zero entries. Our focus is on the practically relevant regime where k is much smaller
than d, say, poly-logarithmic in d. We formally define robust sparse mean estimation below.
Problem 1.2 (Gaussian Robust Sparse Mean Estimation). Let ϵ ∈ (0,1/2) be a sufficiently small
0
constant. Given ϵ ∈ (0,ϵ ), sparsity k ∈ N, and a set of ϵ-corrupted set of samples from N(µ,I )
0 d
withanunknownk-sparsemeanµ ∈ Rd,thegoalistooutputanestimateµ ∈ Rd suchthat∥µ−µ∥
b b 2
is small with high probability.
Robust sparse mean estimation algorithms, efficient both in runtime and samples, were first
developedin[BDLS17], withsamplecomplexityn = poly(k,logd,1/ϵ), runtimepoly(d,n,1/ϵ), and
near-optimal error ∥µ−µ∥ = O˜(ϵ).1 In particular, the sample complexity is only poly-logarithmic
b 2
in the ambient dimension d, thereby permitting statistical inference with far fewer samples than
the Ω(d) samples required by unstructured mean estimation. Therefore, for our algorithm for
Problem 1.2, we set as the first requirement this sample complexity of poly(k,logd,1/ϵ).
The focus of this work is to develop fast robust sparse mean estimation with the aforemen-
tioned sample complexity. Although the runtime of [BDLS17] is polynomial in dimension, their
algorithm uses the ellipsoid algorithm (which in turn solves a semidefinite program) and hence is
not practical in high dimensions. [DKKPS19] then developed a spectral algorithm with similar
error guarantees and sample complexity and an improved runtime of d2poly(k,logd,1/ϵ). Subse-
quent papers have proposed many algorithmic improvements and generalizations to a wider class
of distributions [ZJS22; CDKGGS22; DKKPP22; DKLP22]; see Section 1.3.
Despite this algorithmic progress, the fastest currently known algorithm for Problem 1.2 is
that of [DKKPS19] with runtime scaling as d2. This quadratic runtime of the algorithm can
be prohibitive in high dimensions—the very setting that benefits most from sparsity (because of
sample-efficiency). This quadratic dimension dependence is in stark contrast to the non-robust
setting (i.e., the outlier-free regime), where there exists a simple (folklore) algorithm2 with nearly-
linear runtime, which is also minimax optimal. This motivates the following fundamental question,
highlighted in [Dia19; Che21; Dia23]:
Question 1.3. Are there any nearly-linear time algorithms for robust sparse mean estimation?
1Inthepresenceofoutliers,vanishingerrorisusuallynotpossible. Inoursetting,thisisbecauseitisimpossibleto
distinguishtwoisotropicGaussiandistributionsthatareΩ(ϵ)-farapartinthepresenceofϵ-fractionofcontamination.
2Thealgorithmcomputesthesamplemeanandthresholdsentriestoensuresparsity,hencefailingifthereiseven
asingleoutlier. Moreover,naturalattemptstomakethisalgorithmrobust,suchascoordinate-wisemedian,incuran
√
highly suboptimal error of Ω(ϵ k).
2If we momentarily forgo sparsity (and the benefits that come along with it, e.g., the reduced
samplecomplexityandinterpretability)andfocusonrobustdense estimation,thenpositiveanswers
are known to Question 1.3, see, e.g., [CDG19; DHL19; DKKLT22; DKPP22]. However, the sample
complexities of the algorithms in these papers scale linearly with dimension,3 which considerably
exceeds our allowed budget of poly(k,logd,1/ϵ) samples.
In fact, as alluded to earlier, existing attempts at answering Question 1.3 do not even break the
quadratic runtime barrier. This is due to natural technical obstacles within current algorithms: to
robustly estimate the mean, they crucially rely on the sample covariance matrix to detect outliers;
but merely computing the sample covariance matrix costs Ω(d2) time! Sparsity also precludes com-
mon tricks such as the power iteration to bypass explicitly writing the covariance matrix. Indeed,
in certain parameter regimes, even detecting atypical values of the covariance matrix from samples
is conjectured to require Ω(d2) time [DS18]. This begs the question whether this quadratic gap is
inherent:
Question 1.4. Is there an algorithm for robust sparse mean estimation that runs in d2−Ω(1) time
and uses poly(k,logd,1/ϵ) samples?
ThemainresultofourworkisanaffirmativeanswertoQuestion1.4. Wehopeouranswerpaves
the path for progress towards answering Question 1.3, which was highlighted as an important open
problem in [Dia19; Che21; Dia23]. Our algorithm builds on advances in fast correlation detection
algorithms by Valiant [Val15].
1.1 Our Results
We establish the following result:
Theorem 1.5 (Robust Sparse Mean Estimation in Subquadratic Time). Let the contamination
rate be ϵ ∈ (0,ϵ ) for a small constant ϵ ∈ (0,1/2) and k ∈ N be the sparsity. Let T be an
0 0
ϵ-corrupted set of n samples from N(µ,I ) for an unknown k-sparse µ ∈ Rd. Then there is a
d
randomized algorithm A that takes as input the corrupted set T, contamination rate ϵ, sparsity
k ∈ N, and a parameter q ∈ N, and produces an estimate µ such that
b
(Sample Complexity and Error) If n ≳ (k2qlogd)/ϵ2q, then ∥µ − µ∥ ≲ ϵp log(1/ϵ) with
b 2
probability at least 0.9 over the randomness of the samples and the algorithm.4
(Runtime) The algorithm runs in time at most d1.62+3 qpoly(log(d),kq,1/ϵq).
p
Several remarks are in order. The error guarantee of Theorem 1.5, O(ϵ log(1/ϵ)), is nearly
optimal even given infinite data and runtime.5 The main contribution of Theorem 1.5 is the first
algorithm for robust sparse mean estimation with runtime d2−Ω(1)poly(k/ϵ) and sample complexity
poly(k,logd,1/ϵ) (by selecting q ∈ N to be a constant bigger than 9), thereby affirmatively answer-
ing Question 1.4. As q increases, the dependence of the runtime on the dimension approaches d1.62.
(cid:16) (cid:17)
In particular, for a constant contamination rate ϵ, we may set q as large as γ logd for a small
logk
γ ∈ (0,1), and the algorithm retains sublinear (in d) sample complexity dO(γ) and subquadratic
3Infact,theoverallruntimeofthesealgorithmsscalesasΘ˜(nd)=Θ˜(d2/ϵ2),whichisagainquadraticindimension.
4Thesuccessprobabilitycanbeboostedto1−δwithamultiplicativeincreaseoflog(1/δ)inthesamplecomplexity
and the runtime by repeating the procedure.
(cid:16) p (cid:17)
5The information-theoretic optimal error is Θ(ϵ). Moreover, it is computationally hard to beat Θ ϵ log(1/ϵ)
in the statistical query lower model [DKS17] and the low-degree polynomial tests [BBHLS21] under Definition 1.1.
3runtime d1.62+O(γ)kO(1/γ). Finally, the sample complexity of Theorem 1.5 is (polynomially) larger
than existing works; see Section 5 for further remarks.
We next focus on robust version of sparse principal component analysis (PCA). Sparse PCA
is a fundamental estimation task in high-dimensional statistics [dGJL07; HTW15], in which the
algorithm observes samples from N(0,I+ηvv⊤) for a k-sparse unit vector v ∈ Rd. Similarly to
sparse mean estimation, the standard algorithms for sparse PCA are not robust to outliers, and
hence we study robust sparse PCA. Again, existing robust sparse PCA algorithms from [BDLS17;
DKKPS19; CDKGGS22] require at least Ω(d2) time. In contrast, we establish the following result:
Theorem 1.6 (RobustSparsePCAinSubquadraticTime). Let T be an ϵ-corrupted set of samples
from N(0,I +ηvv⊤) for η ∈ (0,1) and a k-sparse unit vector. There is a randomized algorithm that
d
takes as input the corrupted set T, contamination rate ϵ, sparsity k ∈ N, spike η, and a parameter
q ∈ N, and produces an estimate v such that
b
(SampleComplexityandError)Ifn ≳ poly((kqlogd)/ϵq), then∥vv⊤−vv⊤∥ ≲ p ϵlog(1/ϵ)/η
bb Fr
with probability at least 1− 1 .
poly(d)
(Runtime) The algorithm runs in time at most d1.62+3 qpoly(n).
Thisresultgivesthefirstsubquadratictimealgorithmfordimension-independenterror,improv-
ing on the Ω(d2) runtime of [BDLS17; DKKPS19; CDKGGS22]. We note that the error guarantee
of Theorem 1.6 is sub-optimal by a polynomial factor of ϵ/η (like in [CDKGGS22]), since the
information-theoretic optimal error is ϵ/η. Despite this (polynomially) larger error, Theorem 1.6
is the first subquadratic time algorithm for robust sparse PCA with any non-trivial error, say, less
than 0.01. We defer detailed discussion to Section 4.
OurmaintechnicalingredientinprovingTheorems1.5and1.6isaresultondetectingcorrelated
coordinates of a high-dimensional distribution by [Val15]. We give an overview of Theorem 1.5 in
Section 1.2, with details in Section 3, and defer Theorem 1.6 to Section 4.
1.2 Overview of Techniques
We begin by presenting a brief overview of the landscape of current robust sparse mean estimation
algorithms, followed by challenges in using these approaches to obtain an o(d2) runtime, and then
conclude by presenting our algorithm.
(Dense) Robust Mean Estimation Let µ′ and Σ′ be the sample mean and the sample
covariance of the current (corrupted) dataset. The general guiding principle in robust dense mean
estimation is to use Σ′ to check for the presence of harmful outliers and iteratively remove them.
Recall that inliers are sampled from an isotropic covariance distribution N(µ,I ). Thus, if we take
d
Θ(d/ϵ2) samples, then the variance of the inliers in any direction is (1 ± O˜(ϵ)). Moreover, the
variance of any (1−ϵ) fraction of inliers is (1±O˜(ϵ)).
The following are the key insights in developing algorithms for robust dense mean estimation:
(i) Outliers cannot change the sample mean µ′ of the data in any direction v without significantly
increasing the covariance Σ′ in the direction v, (ii) Given a direction of large variance v of the data
(i.e., with variance larger than 1+Ω˜(ϵ)), one can reliably remove outliers by projecting the data
ontov andthresholdingappropriately, and(iii)Inthedensesetting, thedirectionsoflargevariance
correspond to leading eigenvectors of the covariance matrix Σ′, and further they can be computed
efficiently(innearly-lineartime)usingpoweriteration. Thus, onecaniterativelyremoveoutliersas
follows: compute(approximately)theleadingeigenvaluesandeigenvectorsofthesamplecovariance
matrix Σ′ and remove the samples that have large projections along the computed direction.
4Adapting to Sparsity and Smaller Sample Complexity Forrobustsparse meanestimation,
one can adapt the above strategy by focusing only on the sparse directions v. Indeed, (i) and (ii)
above are straight-forward and the resulting sample complexity of the algorithm is klog(d)/ϵ2
since we require concentration of the mean and the covariance only along k-sparse directions.
However, the problem of computing the direction of the leading sparse eigenvalues of a matrix,
max v⊤Σ′v, is computationally-hard in the worst case. Inspired by the literature on
v:∥v∥2=1,k-sparse
sparse PCA [dGJL07], [BDLS17] proposed the following convex relaxation6:
sup ⟨A,Σ′−I ⟩. (1)
d
{A:A⪰0,tr(A)=1,∥A∥1≤k}
Given such a feasible A with value larger than Ω˜(ϵ), one can remove outliers provided a larger
sample complexity of (k2logd)/ϵ2.7 Although the resulting algorithm is polynomial-time and the
desired sample complexity poly(k,logd,1/ϵ), the algorithm requires solving semidefinite programs
(SDPs), for which the current algorithms require time superquadratic in dimension [JKLPS20].
Spectral Algorithm of [DKKPS19] To avoid solving the SDPs from the preceding paragraph,
[DKKPS19] considers a different (and, in a sense, weaker) relaxation of sparse eigenvalues by
relying on the distributional properties of Gaussian distributions. Let B be the set of all
k2-sparse
(sparse) matrices B with Frobenius norm 1 and at most k2 non-zero entries. Importantly, B
k2-sparse
contains all vv⊤ for all k-sparse unit vectors v. Their starting point is the observation is that for
any B ∈ B , we have Var (x⊤Bx) = 2∥B∥2 = 2. Thus, for a fixed B ∈ B ,
k2-sparse x∼N(0,I d) Fr k2-sparse
the empirical mean of x⊤Bx over any 1−ϵ fraction of inliers should be tr(B)±O˜(ϵ). Moreover,
standard uniform concentration arguments imply that this holds uniformly over B ∈ B
k2-sparse
given (k2logd)/ϵ2 samples. Their key observation is that the resulting (non-convex) optimization
problem
max ⟨B,Σ′−I ⟩ (2)
d
B∈B k2-sparse
canbesolvedviastandardmatrixoperations(despitebeingnon-convex)withoutresortingtoSDPs
(asopposedto(1)); indeed, theoptimalBcorrespondstothetop-k2 valuesofΣ′−I inmagnitude,
d
computable in O˜(d2) time given Σ′.
Given such a feasible B ∈ B that achieves ⟨B,Σ′−I ⟩ ≥ Ω˜(ϵ), [DKKPS19] also propose
k2-sparse d
an efficient outlier-removal strategy. Overall, this yields an algorithm with runtime d2poly(k/ϵ)
and sample complexity (k2logd)/ϵ2. While a significant improvement over [BDLS17], this still
unfortunately falls short of our target runtime of o(d2).
The main challenge in extending [DKKPS19]’s algorithm to get an o(d2) runtime is that one
needstowritedownΣ′ explicitly,whichitselftakesΩ(d2)time. Moreover,thereisnoknownanalog
of power iteration for sparse settings with provable guarantees (recall that in the dense setting, the
power iteration can be implemented in nearly-linear runtime [SV14]). Our main technical insight
is to use advances in fast algorithms for correlation detection initiated by [Val15]; see Section 2.4
for precise statements. Next, we explain why [Val15] is potentially useful in our setting, explain
the challenges in a direction application of their result, and our proposed fix.
Fast Correlation Detection Algorithm To The Rescue Denote the correlation detection
algorithm in [Val15] by A . In our setting, this algorithm guarantees that given a ρ ∈ (0,1) and
corr
6For a matrix A∈Rd×d, ∥A∥ denotes the ℓ -norm of A when flattened as a d2-dimensional vector
1 1
7This larger sample complexity, k2 versus k, is due to the stronger concentration required by the relaxation.
5a large q ∈ N, it can find (off-diagonal) coordinate pairs (i,j) such that |Σ′ | ≥ ρ in subquadratic
i,j
time as long as there are at most O(d) off-diagonal coordinate pairs (i,′,j′) such that |Σ′ | ≥ ρq;
i′,j′
observe that ρq ≪ ρ.
We now illustrate why such a subroutine may be useful. Suppose that A returns many
corr
correlated coordinate pairs. Then the optimal value in (2) must be large (if we optimize over
B forsomek′ largeenough8), andwecanusethosecoordinatespairstoconstructak′-sparse
k′-sparse
B that can be used to remove outliers as before [DKKPS19]. If, on the other hand, A returns
corr
only a few coordinate pairs, then we know that only these small set of coordinates are (potentially)
corrupted, andwehavereducedourproblemtorobustdensemeanestimationonthesecoordinates;
recallthatinthissetting,thesamplemeanisagoodcandidatefortheremainingcoordinates. Thus,
a fast correlation detection algorithm leads to a subquadratic time algorithm to filter outliers (or
declare victory) provided that there are only O(d) coordinate pairs with correlation larger than ρq.
Challenges in Applying Fast Correlation Detection and A Proposed Fix A priori, it is
unclear why there must be only O(d) correlated coordinate pairs: indeed, the outliers are allowed
to be dense (similar to inliers — recall that only the population mean is sparse), and, in the worst
case, it is possible that they cause all coordinate pairs to be correlated (on the corrupted data).
Thus, we need an alternative procedure to (i) detect if there are many ρq-correlated pairs and (ii)
if so, find an alternative procedure to make progress.
Fortunately, it turns out that if there are Ω(d) correlated pairs, then a random pair has
Ω(d/d2) = Ω(1/d) probability to be correlated. Hence, we can sample a relatively large — but
subquadratic, say Θ(d1.5) — number of random pairs to estimate the true count of correlated pairs.
If random sampling does not find many such pairs, then the true count would anyway have been
small with high probability, and we may safely invoke [Val15]’s algorithm, solving the detection
problem (i) above. On the other hand, if we do observe many (i.e., scaling polynomially with d) ρq-
correlatedpairs, thenweknowthattheFrobeniusnormofthelargestk′ = poly(k)entriesofΣ′−I
√ d
must be large enough, Ω(ρq k′). In other words, we can find a relatively sparse B′ ∈ B
k′-sparse
such that ⟨Σ′ −I ,B′⟩ ≥ Ω˜(ϵ). Thus, we can iteratively remove outliers (or declare victory when
d
safe to do so) irrespective of the number of correlated pairs. Finally, the larger sample complexity
of algorithm comes from invoking the filter on k′-sparse matrices B for k′ ≫ k. We give a more
detailed overview in Section 3.
1.3 Related Work
Our work is situated within the field of algorithmic robust statistics, and we refer the reader to
[DK23] for an extensive exposition on the topic.
Robust Sparse Estimation Efficientalgorithmsforrobustsparsemeanestimationwerefirstde-
veloped in [BDLS17], giving an algorithm to compute µ with sample complexity poly(k,logd,1/ϵ),
b
runtime poly(d,1/ϵ), and near-optimal error ∥µ−µ∥ = O˜(ϵ). Their algorithm used the ellipsoid
b 2
algorithm with a separation oracle that requires solving an SDP. Invoking the ellipsoid algorithm
can be avoided by using [ZJS22] or through iterative filtering, but the resulting algorithm still
requires solving multiple SDPs, which, as noted earlier, is inherently slow. Bypassing the use of
SDPs, [DKKPS19] developed the first spectral algorithm for robust sparse estimation with runtime
d2poly(k,logd,1/ϵ). Another novel take on this problem was seen in [CDKGGS22], which gave
8Recallthatwecaninterpretthemaximumin(2)asEuclideannormofthelargestk2 entries,whichwouldbeat
√
least ρ k2 =ρk.
6an optimization-based algorithm showing that first-order stationary points of a natural non-convex
objective suffice. Although the resulting algorithm relies on simple matrix operations, the derived
runtime is super-quadratic in dimension. In a different direction, [DKKPP22] and [DKLP22] devel-
oped algorithms for robust sparse mean estimation for a wider class of distributions: heavy-tailed
distributions and light-tailed distributions with unknown covariance, respectively.
Robust sparse mean estimation is conjectured to have information-computation gaps [DKS17;
BB20; DKKPP22]. Particularly, while there exist inefficient (dk-time) algorithms for robust sparse
mean estimation using (klog(d))/ϵ2 samples, all polynomial time algorithms are conjectured to
require
Ω(cid:0) k2/ϵ2(cid:1)
samples [BB20].
Fast Algorithms for Robust Estimation Lookingbeyondpolynomialruntimeasthecriterion
of computational efficiency, a recent line of work has investigated faster algorithms for a variety of
robust estimation tasks: mean estimation [CDG19; DHL19; DL22; LLVZ20; DKKLT22; DKPP22;
DKPP23a], covariance estimation [CDGW19], principal component analysis [JLT20; DKPP23b],
list-decoding [CMY20; DKKLT22], and linear regression [CATJFB20; DKPP23a]. The overarching
goalinthislineofworkistodeveloprobustalgorithmsthathaveruntimesmatchingthecorrespond-
ing non-robust off-the-shelf algorithms, thus reducing the computational overhead of robustness.
However, none of these algorithms is tailored to sparsity and hence have sample complexity scaling
(at least) linearly with the dimension.9 Our main contribution is the first subquadratic runtime
algorithm for robust sparse mean estimation with sample complexity poly(klogd,1/ϵ).
Fast Correlation Detection Given n vectors in {±1}d and two thresholds 1 ≥ ρ > τ > 0,
the correlation detection problem asks to find all coordinate pairs that have correlation at least
ρ given that not too many pairs have correlation larger than τ. This problem is a generalization
of the light bulb problem [Val88]. The first subquadratic algorithm for both these problems was
given by [Val15], with further improvements and simplifications in [KKK18; KKKC20; Alm19].
Further algorithmic improvements for this task would likely also improve our runtime guarantees
in Theorems 1.5 and 1.6.
2 Preliminaries
Notation For a random variable X, E[X] denotes its expectation. For a finite set S and a
function g : S → Rd, we use E [g(X)] to denote (P g(X))/|S|. We use poly(···) to denote an
S x∈S
expression that is polynomial in its arguments. The notations ≲,≳,≍ hide absolute constants.
Foravectorx ∈ Rd, weuse∥x∥ and∥x∥ todenotethenumberofnon-zeroentriesofxandthe
0 2
ℓ -norm of x, respectively. For a vector x and k ∈ N, we define the ∥x∥ norm as the maximum
2 2,k
correlation between x and a unit k-sparse vector, i.e., ∥x∥ := sup ⟨v,x⟩. Estimation in
2,k v:∥v∥0≤k
∥·∥ immediately yields an estimate that is close in ℓ norm whenever µ is k-sparse:
2,k 2
Proposition 2.1 (Sparse estimation using ∥·∥ norm [CDKGGS22]). Let x ∈ Rd,y ∈ Rd, where
2,k
y is k-sparse. Let J ⊂ [d] be the top-k coordinates of x in magnitude, breaking ties arbitrarily.
Define x′ ∈ Rd to be x′ = x if i ∈ J and 0 otherwise. Then ∥x′−y∥ ≤ 6∥x−y∥ .
i i 2 2,k
Thus, in the sequel, we solve the harder problem of estimating an arbitrary mean µ ∈ Rd in the
∥·∥ norm.
2,k
We denote matrices by bold capital letters, e.g., A,Σ. We denote the d×d identity matrix by
I , omitting the subscript when clear. For a matrix A, we use ∥A∥ , and ∥A∥ , to denote the
d 0 Fr
9As a result, the dependence on the runtime again becomes quadratic because nd=Ω(d2).
7number of non-zero entries and the Frobenius norm, respectively. For matrices A and B of the
same dimensions, ⟨A,B⟩ to denotes the trace inner product tr(A⊤B).
For a subset H ⊂ [d], and a vector x ∈ Rd, define (x) to be |H|-dimensional vector that
H
corresponds to the coordinates in H. Similarly, for a matrix A, we define (A) to be the |H|×|H|
H
matrix corresponding to coordinates in H. For a square matrix A, we use diag(A) and offdiag(A)
to denote its diagonal and offdiagonal, respectively.
For a finite set T ⊂ Rd, we define µ and Σ to be the sample mean and the sample covariance
T T
ofT, respectively.10 WhenthesetT isclearfromcontext, foracoordinatepair(i,j) ∈ [d]×[d]with
(cid:12) q (cid:12)
i ̸= j, we denote the correlation between these coordinates on T as corr(i,j) := (cid:12)Σ′ / Σ′ Σ′ (cid:12)
(cid:12) i,j i,i j,j(cid:12)
for Σ′ = Σ . For a ρ ∈ (0,1), we say coordinates (i,j) are ρ-correlated if corr(i,j) ≥ ρ.
T
Robust sparse estimation requires checking whether the current covariance matrix Σ′ has small
quadraticforms,v⊤(Σ′−I)v,forsparseunitvectors. ForamatrixAandk ∈ N,wedefinethesparse
operator norm, ∥A∥ := sup |v⊤Av|. Since computing ∥·∥ is computationally
op,k v:∥v∥2=1,∥v∥0≤k op,k
hard, we look at the following relaxation from [DKKPS19]: For a matrix A, define ∥A∥ :=
Fr,k2
sup ⟨A,B⟩. It can be seen that ∥A∥ ≤ ∥A∥ since B could be all ±vv⊤
B:∥B∥Fr=1,∥B∥0≤k2 op,k Fr,k2
for k-sparse unit vectors v. Moreover, ∥A∥ is the Euclidean norm of the largest k2 entries (in
Fr,k2
magnitude) of A.
Since we will routinely look at the projections of the points on a subset of coordinates, we
formally define it below:
Definition 2.2 (Projection of Pairs of Coordinates). Let H ⊂ [d] × [d] be a set of pair of
pair
coordinates such that (i,i) ̸∈ H for any i ∈ [d]. For an even m ∈ [d2], we define the operator
pair
Proj that takes any such H and returns a set in [d] that has a cardinality at most m as follows:
m pair
If |H |≤m, return {i : (i,j) ∈ H or (j,i) ∈ H }.
pair 2 pair pair
Otherwise, let any m/2 distinct elements of H be (i ,i ), ...,(i ,i ). Return {i : j ∈
pair 1 2 m−1 m j
[m]}.
When the subscript m is omitted, we take m to be d2.
Informally, the operator returns a set H such that for any matrix A, for small m, ∥(A) ∥2 ≥
H Fr
P A2 , while for larger m, ∥(A) ∥2 ≥ mmin A2 .
(i,j)∈Hpair i,j H Fr (i,j)∈Hpair i,j
2.1 Deterministic Condition on Inliers
A recurring notion in developing robust algorithms is that of stability, which stipulates that the
first and second moment of the data not change much under removal of a small fraction of data
points.
Definition 2.3 (Stability). For an ϵ ∈ (0,1/2), δ ≥ ϵ, and sparsity k ∈ N, we say a set S ⊂ Rd
is (ϵ,δ,k)-stable with respect to µ ∈ Rd if the following holds for any subset S′ ⊆ S with |S′| ≥
(cid:13) (cid:13)
(1−ϵ)|S|: (i) ∥E [X −µ]∥ ≤ δ, and (ii) (cid:13)E [(X −µ)(X −µ)⊤]−I (cid:13) ≤δ2/ϵ.
S′ 2,k (cid:13) S′ d(cid:13) Fr,k2
The following result gives a nearly-tight bound on the sample complexity required to ensure
stability.
Lemma 2.4 (Stability Sample Complexity [CDKGGS22, Lemma 3.3]). Let S be a set of n i.i.d.
samples from a subgaussian distribution P over Rd such that P has (i) mean µ ∈ Rd, (ii) identity
10Not to be confused with (Σ) when H ⊂[d].
H
8covariance, and (iii) satisfies the Hanson-Wright inequality; in particular, N(µ,I ) satisfies all
d
three properties. Then if n ≳ (k2(logd)/ϵ2), then S is (ϵ,δ,k)-stable with high probability with
respect to µ for δ = Cϵp log(1/ϵ) where C is a large absolute constant.
WenotethatthedeterministicconditioninDefinition2.3isslightlystrongerthan[CDKGGS22]—
theconditionforthecovariance—buttheirproofcontinuestoworkwiththesamesamplecomplexity
for Definition 2.3.11
2.2 Randomized Filtering
We will use the following template of filtering algorithm from [DK23, Section 2.4.2] (after a slight
change in parameters). The following template for filtering has now become a standard in algorith-
mic robust statistics.
Algorithm 1 Randomized Filtering
1: Let T 1 ← T
2: i ← 1
3: while T i ̸= ∅ and T i does not satisfy the stopping condition S do
4: Get the scores f : T i → R + satisfying P x∈Ti∩Sf(x) ≤ P x∈Ti\Sf(x) and max x∈Tif(x) > 0
5: T i+1 ← T i
6: for each x ∈ T i do
f(x)
7: Remove the point x from T i+1 with probability maxx∈Tif(x)
8: i ← i+1
9: return T i
These filtering algorithms have become a standard template in algorithmic robust statistics.
Here, the stopping condition S can be a generic condition that can be evaluated in T time
stopping
using some algorithm A —it can be a randomized algorithm (using independent randomness from
s
Line 6) that succeeds with probability 1−τ. We also require that whenever the stopping condition
is not satisfied and the set T is an 10ϵ-corruption of S, then the scores f : T → R satisfying the
i i +
guarantees of Line 4 can be computed in time T .
score
Theorem 2.5 (GuaranteeofAlgorithm1; [DK23, Theorem2.17]). If the above stopping conditions
and filter conditions are met, then Algorithm 1 returns a set T′ ⊆ T such that, with probability at
least 8/9−τ|T|, the following statements hold.
Algorithm 1 runs in time O(|T|(T +T +|T|)).
stopping score
Each set T ⊆ T observed throughout the run of the algorithm (in particular, the output set
i
T′) is a 10ϵ-corruption of S.
T′ satisfies the stopping condition S.
2.3 Certificate Lemma and Frobenius Norm Filtering
The following standard certificate lemma guides the algorithmic design: if the sample covariance
matrix of the corrupted data is roughly isotropic, then the sample mean is a good estimate.
11Weremarkthat[CDKGGS22,Lemma3.3]claimstoestablishthedeterministicconditionforallisotropicsubgaus-
sian distributions. However, their proof crucially uses Hanson-Wright inequality, which does not hold for arbitrary
isotropic subgaussian distributions. In particular, the example in [Ver18, Exercise 6.3.6], a uniform mixture of two
spherical Gaussians, gives a counter example; see [Ver18, Section 6.3] for additional discussion.
9Lemma 2.6 (Sparse Certificate Lemma, see, e.g., [BDLS17]). Let T be an ϵ-corrupted version of
S, where S is (ϵ,δ,k)-stable with respect to µ (cf. Definition 2.3). Then,
q
∥µ −µ∥ ≲ δ+ ϵ∥Σ −I ∥ .
T 2,k T d op,k
We now state the guarantee of filtering procedures, where the goal is to filter outliers from a
corrupted set T. In dense mean estimation, the most common filters are based on scores of the
form (v⊤(x−µ ))2 for a direction of large variance v; this filter is guaranteed to succeed as long
T
as the covariance matrix Σ is far from the identity in operator norm. In our setting, we will
T
need a stronger filter guarantee that is guaranteed to succeed under the weaker condition that
the covariance (submatrix) matrix is far from the (submatrix) identity in the Frobenius norm;
Observe that the operator norm of (the corresponding submatrix of) Σ −I can be much smaller.
T d
The following lemma corresponds to the above situation and simplifies [DKKPS19, Steps 6-10 of
Algorithm 1]:
Lemma 2.7 (Sparse Filtering Lemma). Let ϵ ∈ (0,ϵ ) for a small absolute constant ϵ . Let T
0 0
be an ϵ-corrupted version of S, where S is (ϵ,δ,k)-stable with respect to µ. Let H ⊂ [d] be such
∥(Σ −I) ∥ = λ for λ ≳ δ2/ϵ and |H| ≤ k. There exists an algorithm A that takes T, H, ϵ, and
T H Fr
δ and returns scores f : T → R so that P f(x) ≤ P f(x), i.e., the sum of scores over
+ x∈S∩T x∈T\S
inliers is less than that of outliers, and max f(x) > 0. Moreover, the algorithm runs in time
x∈T
d·poly(|H||T|).
These scores can be used to filter points from T such that on expectation over the algorithm’s
randomness, more outliers are removed than inliers [DK23]. We give a proof of Lemma 2.7 in
Appendix A.
Thus, if Σ −I has large (sparse) Frobenius norm, then we can make progress by removing
T d
outliers. The contribution to this norm from the diagonal entries can be calculated efficiently in
O(dn) time, and if large, then can also be used to remove outliers. Thus, we will assume that the
corrupted set has already been pre-processed to satisfy the following:
Condition 2.8 (Preprocessing). Let T be an ϵ-corrupted version of S, where S is (ϵ,δ,k)-stable.
Suppose T satisfies ∥diag(Σ −I )∥ ≤
min(cid:0) O(δ2/ϵ),0.5(cid:1)
.
T d Fr,k2
For completeness, we give details in Appendix A.2. The next result, also proved in Ap-
pendixA.2, showsthatanyfurthersmallmodificationsofthepreprocessedsetsretainssmallsparse
operator norm.
Claim 2.9. Let C be a large enough constant C > 0. Let T′′ ⊂ T′ be two O(ϵ)-contamination of
S such that S is an (Cϵ,δ,k)-stable with respect to µ. Suppose that ∥diag(Σ −I )∥ ≲ δ2/ϵ.
T′ d op,k
Then ∥diag(Σ −I )∥ ≲ δ2/ϵ
T′ d op,k
2.4 Detecting Correlation in Subquadratic Time
We will use [Val15, Theorem 2.1] that can detect ρ-correlated coordinates in subquadratic time if
there are not too many τ-correlated coordinates for τ ≪ ρ, say ρ3.
Theorem 2.10 (Fast Correlation Detection [Val15]). Let ρ ∈ (0,1) be strong correlation threshold
and τ ∈ (0,1) be margin threshold with ρ > 12τ. Let T be a set of n vectors in Rd such that there
are at most s τ-correlated coordinate pairs. Then, there is an algorithm that takes ρ,τ,T as input,
and, with probability 1−o(1), will output all ρ-coordinate pairs. Additionally, the runtime of the
(cid:18) (cid:19)
algorithm is sd0.62+d1.62+2.4 ll oo gg (( 14 // 3ρ τ)
)
poly(n,logd,1/τ).
10The version above follows from [Val15, Theorem 2.1] for the binary vectors using standard
reductions, for example, [Val15, Lemma 4.1]. For completeness, we state the guarantees of [Val15,
Theorem 2.1] and show the reduction in Appendix A.
3 Robust Sparse Mean Estimation in Subquadratic Time
In this section, we explain our main technical contribution: a fast algorithm for robust sparse mean
estimation under the stability condition.
Theorem 3.1 (Robust Sparse Mean Estimation in Subquadratic Time). Let c be a small enough
absolute constant and C be a large enough absolute constant. Consider the corruption rate ϵ ∈
(0,ϵ ), where ϵ is a small enough absolute constant. Let k ∈ N be the sparsity parameter and
0 0
q ∈ N the correlation decay parameter with q ≥ 3. Let T be an ϵ-corrupted version of a set S, where
S satisfies (Cϵ,δ,k′)-stability with respect to µ for k′ := (Ck)q and δ2/ϵ ≤ c. Then there is a
(δ2/ϵ)q−1
randomized algorithm (Algorithm 5) that takes as inputs T, ϵ, δ, k, and q and produces an estimate
µ such that, with a probability at least 1−1/d2 over the randomness of the algorithm, we have the
b
following guarantees:
(Error) ∥µ−µ∥ ≲ δ.
b 2,k
(Runtime) The algorithm runs in time at most d1.62+3 qpoly(|T|,logd,kq,1/ϵq).
Organization InSection3.1, wehighlightthekeytechnicalchallengesofobtainingo(d2)runtime
algorithm and our proposed fix. We record the guarantees of the key subroutines in Section 3.2.
TheproofofTheorem3.1isgiveninSection3.3,andwefinallyshowinSection3.4howTheorem3.1
implies Theorem 1.5.
3.1 Algorithmic Blueprint of Theorem 3.1
To establish Theorem 3.1, we start with the following blueprint for robust sparse mean estimation,
with the aim of implementing it in o(d2) time.
Algorithm 2 Algorithmic Blueprint
1: Compute ∥Σ T −I d∥ Fr,k2 approximately.
2: while ∥Σ T −I d∥ Fr,k2 is large do
3: Let H be the corresponding coordinates with large Frobenius norm.
4: Filter T using H in Lemma 2.7.
5: Output the sample mean µ T.
The problem in implementing this blueprint naively is that Steps 1 and 3 in Algorithm 2 take
d2 time. However, these are the only two bottlenecks: Each filtering step takes only dpoly(k,n) =
dpoly(k/ϵ) time, and there are at most n = poly(k/ϵ) iterations. As we describe below, our goal in
this section is to use Theorem 2.10 to speed up the Steps 1 and 3.
Usefulness of Fast Correlation Detection We will run Theorem 2.10 to identify the off-
diagonal indices (i,j) ∈ [d]×[d] that are correlated. Recall that Theorem 2.10 takes two arguments
ρ (the threshold for strong correlation) and τ (the margin threshold). Suppose we fix ρ to be small,
roughly
δ2
. Then Theorem 2.10 reports back all ρ-correlated coordinate pairs. For the time to
ϵpoly(k)
11be subquadratic in d, we require the number of τ-correlated pairs to be small. Let H ⊂ [d]×[d]
pair
be the set of coordinate pairs in the output, and define H to be the set of all coordinates that
appear in H ; Proj(H ) from Definition 2.2 to be more formal. Then, one of the following two
pair pair
cases must be true:
|H| is small (poly(k/ϵ)): Since coordinates in H∁ have correlations at most ρ, then we know
that∥(Σ −I) ∥ ≤ kρ,whichcanbemadelessthanδ2/ϵforρsmallenough(ρ ≲ δ2/(kϵ)).
T H∁ Fr,k2
∁
Thus, the sample mean on H is a good estimate in ℓ norm (cf. Lemma 2.6). On H, we can
2,k
use a dense mean estimation algorithm, which would be fast as |H| = poly(k/ϵ).
|H|islarge: Sincetherearemanycoordinatepairswithcorrelationatleastρ, wecanfilterand
iterate as follows: If we take any H′ ⊂ H of size k′, then each row and column in (Σ −I)
√ T H′
has an entry larger than ρ (in absolute value), and thus ∥(Σ −I) ∥ ≥ ρ k′. By taking k′
T H′ Fr
large enough (larger than δ4/(ρ2ϵ2)), the resulting quantity will be bigger than δ2/ϵ, allowing
us to filter if the inliers satisfy stability with parameter k′ (cf. Lemma 2.7).
Thus, we can implement Steps 1 and 3 in Algorithm 2 fast using Theorem 2.10, so long as (i)
inliers satisfy (ϵ,δ,k′) stability, (ii) we choose ρ2 ≍ δ4 and k′ ≍ δ4 ≍ k2, and (iii) there are not
ϵ2k2 ϵ2ρ2
too many τ-correlated pairs.
Challenges in Using Fast Correlation Detection Suppose we set τ = ρq for some q ≥ 3.12
Looking at Theorem 2.10, we obtain a subquadratic time algorithm only if s, the number of τ-
correlated pairs for τ := ρq, is much smaller than d1.38; In fact, we will impose s to be less than d
so that it is not the dominant factor in the runtime. A priori, there is no reason for there to be at
most d coordinate pairs (out of d2 pairs) that are τ-correlated. Thus, we need a way to detect this
situation and find an alternative way to make progress.
Proposed Solutions: Efficient Detection and Filtering We begin with the detection pro-
cedure. If there are Ω(d) many τ-correlated pairs, then a pair sampled uniformly at random has
a probability of Ω(d−1) of being τ-correlated. Thus, if we check many random coordinate pairs,
superlinear but subquadratic, then we can accurately guess s.
In particular, let U be the number of τ-correlated pairs that were observed out of m random
pairs (sampled with replacement). Then, U is distributed roughly as Ber(m,s/d2). Binomial
concentrationimpliesthats ≲ (d2U/m)+(logd)(d2/m)withprobabilityatleast(1−1/d2). Taking
√ √
m to be d1.5, we see that s ≲ dU + dlogd. Thus, we obtain a fast (randomized) check to see if
√
s is less than d than runs in mn = d1.5poly(k/ϵ) time: simply check if U ≤ d. Thus, it remains
√
to ensure that we can make progress when U is large, in particular, Ω( d).
Let H′ be the τ-correlated coordinates that were observed in the above procedure; U =
pair √
|H′ |. Crucially, we are in the regime when |H′ | ≥ d. We want to use a small subset of
pair pair
the coordinates in H′ to filter outliers. Let H be k′′-sized set of coordinates that appear in
pair
H′ ; formally, H := Proj (H′ ). Thus, each row and column in (Σ −I) has an entry larger
pair k′′ pair √ T H
than τ in absolute value, implying that ∥(Σ −I) ∥ ≥ τ k′′. Therefore, we can use this H to
T H Fr √
filter using Lemma 2.7 as long as the original set is also (ϵ,δ,k′′)-stable and k′′τ ≫ δ2/ϵ, i.e.,
k′′ ≍ δ4 ≍ δ4 ≍ δ4 ≍ k2q .
ϵ2τ2 ϵ2ρ2q ϵ2(δ4/kϵ2)q (δ4/ϵ2)q−1
Observe that we require the inliers to satisfy (ϵ,δ,k′′)- stability for k′′ = poly(kq/ϵq). By
Lemma 2.4, a set of (k′′)2/ϵ2 many i.i.d. points will satisfy this stability condition, giving us the
sample complexity.
12We choose this parameterization because the runtime of Theorem 2.10 depends on log(1/ρ)/log(1/τ)=1/q.
123.2 Sparse Certificates and Filters in Subquadratic Time
We now give formal guarantees of the key procedures outlined above. First, consider the procedure
that randomly samples the coordinates to estimate the number of weakly-correlated coordinates.
Lemma 3.2. Let T ⊂ Rd be a multiset with covariance matrix Σ′. Let m ∈ N be the sampling
parameter. Let J ⊂ [d] × [d] be the off-diagonal coordinate pairs such that |Σ′ | ≥ τ. Then
∗ i,j
Algorithm 3 takes τ, m, and T as input and returns a set J ⊂ J in time O(m|T|) such that with
∗
probability 1−1/d2, |J| ≥ m|J∗| −16logd.
4d2
Algorithm 3 RandomlyCheckCoordinates
1: Let H pair ⊂ [d]×[d] of size m, with (i,j) ∈ H pair sampled i.i.d. from off-diagonal elements.
2: Let J ← {(i,j) ∈ H pair : |Σ′ i,j| ≥ τ} for Σ′ = Σ T
3: return J.
Proof. To show correctness, we shall use the following concentration inequality for Binomials: If
√ √ p
X ∼ Ber(n,p), then with probability 1−δ, | X − np| ≤ 2 log(1/δ). See, for example, [PW23,
√ √ √
Equations (15.21) and (15.22)]. In particular, with probability 1−1/d4, X ≥ np−4 logd,
which implies X ≥ 0.25np−16logd.13
The probability that a single pair in H has correlation of magnitude at least τ is exactly
pair
(|J |/d(d−1)), and thus |J| ∼ Ber(m,|J |/(d(d−1))). Therefore, applying the Binomial concen-
∗ ∗
tration, with probability 1−1/d2, it holds that |J| ≥ m|J∗| −16logd. The claim about the runtime
4d2
is immediate.
Combining Algorithm 3 with Theorem 2.10, Algorithm 4 either returns a small subset of coor-
dinates with large Frobenius norm or indicates when all tiny subsets have small Frobenius norm.
Proposition 3.3 (Subroutine to Identify Corrupted Coordinates Algorithm 4). Suppose we are
givenacorruptedsetT, Frobeniusthresholdκ ∈ R, correlationthresholdρ ∈ (0,1), marginthreshold
τ ∈ (0,ρ/12), sampling parameter m ∈ N, and correlation count s ∈ N. Suppose that each diagonal
(cid:16) (cid:17)
entry of Σ lies in [1/2,3/2] and c1d2 κ2 +logd ≤ s.
T m τ2
Then Algorithm 4 takes as input T,κ,ρ,τ,m,s as input and satisfies the following with proba-
bility 1−1/poly(d):
It either outputs a set H ⊂ [d] with |H| ≤ κ2 and ∥(offdiag(Σ )) ∥ ≥ κ/2.
τ2 T H Fr
Else, it outputs “⊥”. If it outputs “⊥”, then ∥offdiag(Σ )∥ is at most 2κ+2ρk.
T Fr,k2
Moreover, the algorithm runs in O(cid:16) m+sd0.62+d1.62+3 ll oo gg (( 14 // 3ρ τ) )(cid:17) poly(n,logd,1/τ) time.
Σ′
Proof. LetAbethematrixwithdiagonalentrieszeroandoff-diagonalentry(i,j)equalto i,j .
p
Σ′ Σ′
i,i j,j
Since diagonal entries of Σ lie in [0.5,1.5], the entries of A and offdiag(Σ ) = offdiag(Σ −I )
T T T d
havethesamesignsandhavethemagnitudeuptoafactorof2. Thus,atthecostofconstantfactor,
we will upper bound ∥A∥ and lower bound ∥(A) ∥ instead of dealing with offdiag(Σ −I ).
Fr,k2 H Fr T d
13We use that if a,b,c ∈ R then a ≥ b−c implies that a2 ≥ b2/4−c2. The proof is as follows: if b ≥ 2c then
+
a≥b/2 and thus a≥b2/4≥b2/4−c2; otherwise b2/4≤c2 and thus a2 ≥0≥b2/4−c2 holds trivially.
13Algorithm 4 Main Subroutine
Require: Frobenius threshold κ ∈ R , a finite set T ⊂ Rd such that diagonal entries of Σ lie
+ T
in [1/2,2], correlation threshold ρ ∈ (0,1), weak correlation threshold τ, sampling parameter
m ∈ N. We require the parameters to satisfy
!
c d2 κ2
1
+logd ≤ s. (3)
m τ2
Ensure: With high probability, output either (i) a set H ⊂ [d] with |H| ≤ k′′ and
∥(offdiag(Σ ) ∥ ≥ 0.5κ or (ii) “⊥”; If it outputs “⊥”, then ∥(offdiag(Σ ) ∥ ≤ 2κ+2kρ.
T H Fr T H Fr
1: Let H′ be the output of Random Subsampling Algorithm (Algorithm 3) with m and τ
pair
2: if |H′ | ≥ κ2/τ2 then
pair
3: Set H 1 ← Proj κ2/τ2(H proj)
4: return H 1
5: else
6: Let H pair ⊆ [d]×[d] be the output of Theorem 2.10 with τ,ρ,s on T
7: if |H pair| > κ2/ρ2 then
8: H 3 ← Proj κ2/ρ2(H proj)
9: return H 3
10: else
11: Let H 2 ← Proj(H pair)
12: if ∥(offdiag(Σ S)) H2∥ Fr ≥ κ then
13: return H 2
14: else
15: return “⊥”
We will use the notation from Algorithm 4. Let s be the number of off-diagonal coordinates of
∗
A that have absolute value larger than τ. Our algorithm will be correct on the event when both
∗
Theorem 2.10 and Lemma 3.2 succeed, which happens with high probability. For the rest of this
proof, we condition on both of these algorithms succeeding.
We first consider the case when s > s. Then, the lower bound on s in the statement (cf. (3)),
∗
coupled with Lemma 3.2, implies that
ms ms
|H′ | > ∗ −16logd > −16logd
pair 4d2 4d2
!!
m c d2 κ2 c κ2
1 1
> +logd −16logd > ,
4d2 m τ2 2τ2
where we use c is large enough, say c ≥ 100. By definition of H = Proj (H ), there are at
1 1 1 κ2/τ2 proj
least |κ2/τ2| entries in (A) with absolute value at least τ, and thus (A) has Frobenius norm
q
H1 H1
at least κ2/τ2τ = κ.
Considerthealternatecasewhens < s. If|H′ |happenstobelarge, thenthesameargument
∗ pair
as above implies the correctness and the runtime. Thus, in the rest of this proof, we consider the
case when we enter Line 5. Since s ≤ s, Theorem 2.10 runs in the desired time and finds all
∗
off-diagonal coordinate pairs of A, collected in H , with entries bigger than ρ in the promised
pair
time. If H has more than κ2/ρ2 entries, then by definition of H = Proj (H ), the same
pair 3 κ2/ρ2 pair
argument as above implies that the Frobenius norm of (A) is at least κ.
H3
14If |H | < κ2/ρ2, then H is defined to contain all coordinates that appear in H . If Line
pair 2 pair
12 succeeds, then the algorithm returns a subset of coordinates satisfying the desired conditions
(small size and Frobenius norm larger than κ). Suppose the if condition is not satisfied (and we
return “⊥”). We argue that ∥A∥ is small enough. Write A = B+B′, where B is non-zero
Fr,k2 i,j
only when i ∈ H ,j ∈ H with values equal to A and zero otherwise (B′ is then defined to be
2 2 i,j
A−B). By definition of H , we know each entry of B′ is at most ρ in absolute value. By triangle
2
inequality, ∥A∥ ≤ ∥B∥ +∥B′∥ = ∥(A) ∥ +∥B′∥ ≤ ∥(A) ∥ +kρ ≤ κ+kρ.
Fr,k2 Fr,k2 Fr,k2 H2 Fr,k2 Fr,k2 H2 Fr
The accuracy guarantee follows by noting that the entries of A and Σ−I are within a factor of 2.
Finally, the runtime guarantees are immediate by Theorem 2.10 and Lemma 3.2.
3.3 Proof of Theorem 3.1
The complete algorithm is given below:
Algorithm 5 Main Algorithm
Require: corruption rate ϵ ∈ (0,1), stability parameter δ ∈ (0,1), corrupted set T ⊂ Rd,
correlation-threshold ρ ∈ (0,1), correlation-decay q ∈ N, sparsity k ∈ N, sampling parame-
ter m ∈ N. We require T to be an ϵ-corrupted version of an (Cϵ,δ,k′) stable set with respect
to µ for k′ := (Ck)q and δ2/ϵ ≤ c for a small absolute constant c > 0.
(δ2/ϵ)q−1
Ensure: µ ∈ Rd such that, with high probability, ∥µ−µ∥ ≲ δ.
b b 2,k
1: T′ ← Filter T using Claim A.2
▷ Preprocessing along the diagonals to ensure Condition 2.8
2: i ← 1
3: T i ← T′.
4: H ← output of Proposition 3.3 with inputs: corrupted set T i, Frobenius threshold κ = C′δ2/ϵ
for a large constant C′, correlation threshold ρ = (δ2/ϵ)/k, margin threshold τ = (ρ/12)q,
sampling parameter m ≍ d(κ2/τ2+logd), and correlation count s = d
▷ Algorithm 4
5: while T i ̸= ∅ and H ̸= “⊥” do
6: Get the scores f : T i → R + from Lemma 2.7 with inputs T i, H, ϵ, and δ
7: T i+1 ← Filter T i using the scores f similar to Algorithm 1
8: i ← i+1
9: Update H as above
10: return the sample mean of T i
We now present the proof of its correctness:
Proof of Theorem 3.1. The first step of Algorithm 5 is the fast preprocessing step from Claim A.2,
which takes at most O˜(dk2|T|2) time and removes not too many inliers with high probability In
particular, the returned set T′ has diagonal values in [1/2,2]. In fact, Claim 2.9 implies that the
subsequent sets T ’s will satisfy this property, at least until we remove more than Ω(ϵ) fraction of
i
points, which is the regime of interest anyway (cf. Theorem 2.5). Thus, Proposition 3.3 will be
applicable.
WewillfollowthestandardprooftemplateofrandomizedfilteringalgorithmfromTheorem2.5,
withthestoppingconditionprovidedbyProposition3.3. ThechoiceofparametersinLine4aresuch
that whenever Proposition 3.3 does not return “⊥” (and T is 10ϵ-corruption of S), it returns a set
i
H satisfying the guarantees of Lemma 2.7. To see this, observe that whenever Algorithm 4 outputs
15a subset H ⊂ [d], then |H| ≤ κ2/τ2 and ∥(Σ −I ) ∥ > ∥(offdiag(Σ )) ∥ > κ/4 ≳ δ2/ϵ.
Ti d H Fr Ti H Fr
Moreover, whenever Proposition 3.3 returns “⊥”, then ∥(Σ −I ) ∥ ≲ κ+kρ ≲ δ2/ϵ since
Ti d H Fr,k2
κ ≍ δ2/ϵ and ρ ≍ δ2/ϵ. The choice of sparsity parameter in the stability of inliers is k′, which
is larger than κ2/τ2, because the choice of κ above and τ = (ρ/12)q imply that κ2/τ2 is of the
order (δ2/ϵ)/(c′ρq) ≍ (δ2/ϵ)/(((c′δ2/ϵ)/k)q) ≍ kq . Thus, the scores generated by H using
Cq(δ2/ϵ)q−1
Lemma 2.7 give more weights to outliers than inliers. Theorem 2.5 now implies that the final set
T′′, with probability at least 0.6, satisfies (i) T′′ is an O(ϵ)-contamination of S, (ii) T′′ is an O(ϵ)
contamination of T′, where T′ satisfies Condition 2.8, and (iii) ∥offdiag(Σ −I ) ∥ ≲ δ2/ϵ.
T′′ d H Fr,k2
We now argue that ∥Σ −I ∥ ≲ δ2/ϵ, and not just the off-diagonal terms. We follow the
T′′ d op,k
following inequalities using triangle inequality:
∥Σ −I ∥ ≤ ∥diag(Σ −I )∥ +∥offdiag(Σ −I )∥
T′′ d op,k T′′ d op,k T′′ d op,k
≲ δ2/ϵ+∥offdiag(Σ −I )∥
T′′ d Fr,k2
(using Claim 2.9 and T′ satisfies Condition 2.8)
≲ δ2/ϵ+δ2/ϵ ≲ δ2/ϵ.
Applying Lemma 2.6, the final output of the algorithm, sample mean of T′′ will be O(δ) close to
the true mean µ, implying the correctness of the procedure.
It remains to show the choice of the parameters m, s, and q lead to fast runtimes. We take
τ = (ρ/12)q and s = d. Finally, we take m ≍ (d2/s)·(κ2/τ2 +logd) ≤ (dlogd)(κ2/τ2), which
satisfies the parameter constraints in Proposition 3.3 (cf. (3)). Letting n = |T|, the resulting
runtime of a single application of Proposition 3.3 is thus at most
A =
(cid:16) m+sd0.62+d1.62+3 ll oo gg (( 14 // 3ρ τ) )(cid:17)
poly(n,logd,1/τ)
≤ (cid:16) d1.62+3 lol go (g (( ρ4 // 4ρ )) q)(cid:17) poly(n,logd,1/ρq)
≤ (cid:16) d1.62+3 q(cid:17) poly(n,logd,kq,1/ϵq).
Since there are at most n iterations, the claim on the total runtime follows. Finally, the success
probability of the algorithm can be boosted from 0.55 to 1 − δ by repeating the algorithm ℓ =
log(1/δ) times and outputting the estimate that is O(δ)-close (in the ∥·∥ norm) to the majority
2,k
of the ℓ estimates. It adds only a multiplicative factor of ℓ and an additive term of dℓ2 to the
runtime.
3.4 Proof of Theorem 1.5
We now explain how Theorem 3.1 implies Theorem 1.5.
Proof of Theorem 1.5. First, observe that by applying Proposition 2.1, the estimation guarantee
of Theorem 3.1 can be translated from ∥ · ∥ norm into ∥ · ∥ norm by hard-thresholding the
2,k 2
estimator. We now turn our attention to the sample complexity. Let S be a set of n i.i.d. samples
from N(µ,I). If n ≥ ((k′)2logd)/ϵ2, then Lemma 2.4 implies that S is (ϵ,δ,k′) stable with respect
to µ for δ ≲ ϵp log(1/ϵ). Plugging in the value of δ and k′ ≤ (O(k))q in Theorem 3.1 yields a sample
ϵq−1
complexity of at most n ≲ (k′)2logd ≲ (O(k/ϵ))2qlogd. The claim on the runtime in Theorem 1.5
ϵ2
follows from Theorem 3.1 along with the bound on the sample complexity.
164 Robust Sparse PCA
In this section, we show that the ideas from fast correlation detection can also be useful for robust
sparse PCA. Our main result in this section is the result below:
Theorem 1.6 (RobustSparsePCAinSubquadraticTime). Let T be an ϵ-corrupted set of samples
from N(0,I +ηvv⊤) for η ∈ (0,1) and a k-sparse unit vector. There is a randomized algorithm that
d
takes as input the corrupted set T, contamination rate ϵ, sparsity k ∈ N, spike η, and a parameter
q ∈ N, and produces an estimate v such that
b
(SampleComplexityandError)Ifn ≳ poly((kqlogd)/ϵq), then∥vv⊤−vv⊤∥ ≲ p ϵlog(1/ϵ)/η
bb Fr
with probability at least 1− 1 .
poly(d)
(Runtime) The algorithm runs in time at most d1.62+3 qpoly(n).
The result above provides the first subquadratic time algorithm for robust sparse PCA that
has error independent of d and k. Similar to the literature on robust sparse mean estimation,
existing algorithms for robust sparse PCA with similar dimension-independent error took Ω(d2)
time. However, the error guarantee of Theorem 1.6 is not optimal: the error guarantee above is
(ϵlog(1/ϵ))/η, same as [CDKGGS22], as opposed to the near-optimal error of (ϵ2polylog(1/ϵ))/η
in [BDLS17; DKKPS19].
We remark that Theorem 1.6 does not follow directly from Theorem 1.5. In particular, a
standard reduction relies on the fact that the mean of the random variable Y−I is exactly ηvv⊤
d
for Y = xx⊤, where x ∼ N(0,I+ηvv⊤), i.e., (robust) sparse PCA reduces to (robust) sparse mean
estimation. However, Y is a d2-dimensional object, and thus a naive application of Theorem 1.5
would yield a super-quadratic runtime, in fact, (d2)1.62 = Ω(d3). Moreover, the covariance of Y
is not isotropic and thus it is unclear if samples from Y would satisfy the stability condition from
Definition 2.3. In the rest of this section, we show that a more whitebox analysis of the main ideas
from Theorem 1.5, in particular, Proposition 3.3 can yield a subquadratic runtime.
Organization Thissectionisorganizedasfollows: Welistthedeterministicconditionsforrobust
PCA in Section 4.1. Section 4.2 contains the results pertaining to the certificate lemma, filtering,
anddenseestimationalgorithmforsparsePCA.Finally,thealgorithmanditsproofunderageneric
stability condition is given in Section 4.3. Finally, we prove Theorem 1.6 in Section 4.4.
4.1 Deterministic Conditions
For a set T, we use the notation Σ to denote the second moment matrix E [xx⊤]—not to be
T T
confused with the covariance matrix Σ .
T
Definition 4.1 (Stability Condition for PCA). For the contamination rate ϵ ∈ (0,1/2), error
parameter γ ≥ ϵ, spike strength η ∈ R , and sparsity k ∈ N, we say a set S ⊂ Rd is (ϵ,γ,k,η)-pca-
+
stable with respect to a k-sparse unit vector v ∈ Rd and spike strength η if
(cid:13) (cid:13)
1. For any subset S′ ⊆ S with |S′| ≥ (1−ϵ)|S|:(cid:13)Σ −(I +ηvv⊤)(cid:13) ≤γ
(cid:13) S′ d (cid:13)
Fr,k2
2. For all subsets H ⊂ [d] with |H| ≤ k, the set {(x) : x ∈ S} is (ϵ,γ)-covariance stable in the
H
sense of [DK23, Definition 4.5] (with respected to an appropriate flattening of I ).
|H|
The second stability condition allows us to perform covariance estimation in Frobenius norm
(and hence stronger than principal component analysis) but only when the support is known.
The following result lists the sample complexity of Definition 4.1.
17Lemma 4.2 (Sample Complexity). Let S be a set of n i.i.d. samples from N(0,I + ηvv⊤) for
η ∈ (0,1) and a k-sparse unit vector v. Then if n ≳ poly(k,log(d/δ),1/ϵ), then S is (ϵ,γ,k,η)-pca-
stable with respect to v and η for γ = O(ϵlog(1/ϵ)).
Proof. We sketch the argument here. The first condition is identical to [CDKGGS22, Definition
k2log(d/δ)
2.2] and [CDKGGS22, Lemma 4.2] establishes a sample complexity of .
ϵ2
For the second condition, we do an admittedly loose analysis. Fixing a subset H, it amounts
to showing a stability condition for a |H|-dimensional Gaussian distribution, for which [DK23,
Proposition 4.2] establishes a sample complexity of poly(klog(1/δ)/ϵ) for the set to be (ϵ,γ)-
covariance stable for γ = O(ϵlog(1/ϵ)) with probability 1−δ.14 Since there are at most dk possible
choicesofH,aunionboundshowsthatthesecondconditioninDefinition4.1holdswithprobability
1−δ with sample complexity poly(klog(dk/δ)/ϵ) = poly(klog(d/δ)/ϵ).
4.2 Filters, Certificates, and Dense Estimation
Dense Estimation Algorithm We shall use the following algorithm to estimate the spike when
we have identified the support of the spike.
Lemma 4.3 (Dense Covariance Estimation Algorithm; Implication of [DK23, Theorem 4.6]).
Let ϵ ∈ (0,ϵ ) for a small absolute constant ϵ . Let T be an ϵ-corrupted version of S, where S
0 0
is (ϵ,γ,k,η)-pca-stable with respect to v (cf. Definition 4.1) and η ∈ (0,1). Let H be a k-sparse
subset of [d]. There exists an algorithm A that takes as inputs corrupted set T, contamination
rate ϵ, error parameter γ, spike η, and the sparse support set H and outputs a k-sparse vector u,
(cid:13)(cid:16) (cid:17) (cid:16) (cid:17) (cid:13)
supported on H, such that (cid:13) I +ηuu⊤ − I +ηvv⊤ (cid:13) ≲ γ. Moreover, the algorithm runs in
(cid:13) d d (cid:13)
H H
time d·poly(k,|T|/ϵ).
Proof. Let T′ ⊂ Rk and S′ ⊂ Rk denote the projections of T and S, respectively, on H. Let
Σ′ = (I +ηvv⊤) = I +η′v′v′⊤ for v′ = (v) /∥(v) ∥ and η′ = η∥(v) ∥2.
d H |H| H H 2 H 2
[DK23, Theorem 4.6] gives an estimate Σb such that ∥Σb − Σ′∥
Fr
≲ γ∥Σ′∥
op
≲ γ, which can
be calculated in time dpoly(k|T|/ϵ); here ∥·∥ denotes the operator norm of a matrix. That is,
op
∥(Σb−I)−η′v′v′⊤∥
Fr
≲ γ. LettingAbethebestone-rankapproximationof(Σb−I)intheFrobenius
norm, we see that
∥A−η′v′v′⊤ ∥
Fr
≤ ∥A−(Σb −I)∥ Fr+∥(Σb −I)−η′v′v′⊤ ∥
Fr
≤ 2∥(Σb −I)−η′v′v′⊤ ∥
Fr
≲ γ,
where the second inequality follows from the fact that A is the best rank-one approximation.
Moreover, the symmetry of A implies that A must be of the form λ′u′u′⊤ for a unit vector u′ and
λ′ ∈ R. We thus obtain
∥λ′u′u′⊤ −η′v′v′⊤ ∥ ≲ γ.
Fr
Consequently, |λ′ − η′| ≲ γ by Weyl’s inequality. If λ′ ≤ 0, we set u = 0. Otherwise, we set
u = p λ′/ηu′. We now calculate the approximation error. If λ′ ≥ 0, then the resulting error in our
(cid:13)(cid:16) (cid:17) (cid:16) (cid:17) (cid:13) (cid:13) (cid:13)
estimate is (cid:13) I +ηuu⊤ − I +ηvv⊤ (cid:13) = (cid:13)λ′uu⊤−η′v′v′⊤(cid:13) ≲ γ by the approximation
(cid:13) d d (cid:13) (cid:13) (cid:13)
H H Fr Fr
guarantee. If λ′ < 0, then η′ must also be O(γ) because η′ ≤ λ′ +|λ′ −η′| ≲ γ. In this case, the
approximation error is η′, which is also O(γ) as required.
14Althoughtheproofof[DK23,Proposition4.2]doesnotexplicitlywritethedependenceonδ,butitisimmediate
from the proof.
18Certificate Lemma for Sparse PCA The following result shows that if the covariance matrix
∁
looks roughly isotropic on a subset of coordinates H , then the spike vector v places most of its
mass on the complement, H. The benefit of this stopping condition is that it does not depend on
the unknown spike vector v.
Lemma 4.4 (Sparse Certificate Lemma for PCA). Let T be an ϵ-corrupted version of S, where S
is (ϵ,γ,k,η)-pca-stable with respect to a k-sparse unit vector v ∈ Rd and spike strength η ∈ (0,1).
Let H ⊂ [d] be such that ∥(Σ −I) ∥ = O(γ), then ∥vv⊤−(v) (v)⊤∥2 = O(cid:0) (γ/η)+(γ/η)2(cid:1).
T H∁ op,k H H Fr
Proof. Let z be the unit vector along (v) , which is at most k-sparse because v is k-sparse.
H∁
(cid:12) (cid:16) (cid:17) (cid:12)
Since z is supported on H∁ , the assumption on Σ and H∁ implies that (cid:12)z⊤ Σ −I z(cid:12) ≲ γ,
T (cid:12) T (cid:12)
which further gives us that z⊤Σ z = 1±O(γ). Using the stability of S, we see z⊤Σ z ≥ (1−
T T
(cid:16) (cid:16) (cid:17) (cid:17)
ϵ)z⊤Σ z ≥ (1 − ϵ) z⊤ I+ηvv⊤ z−γ . Defining η′ := η(v⊤z)2 = η∥v ∥2, we have that
S∩T H∁ 2
z⊤Σ z ≥ (1−ϵ)(1+η′−γ). Combining this with the aforementioned upper bounds on z⊤Σ z, we
T T
obtain that (1−ϵ)(1+η′−γ) ≤ 1+O(γ). Thus, η′ = O(γ+ϵ) = O(γ) by using ϵ ≤ γ. Therefore,
∥(v) ∥2 = η′/η ≲ γ/η. Finally, the triangle inequality implies that
H∁ 2
(cid:18)q (cid:19)
∥vv⊤−(v) (v) ∥ ≤ ∥(v) ∥2+2∥(v) ∥ ≲ max (γ/η),(γ/η) .
H H Fr H∁ 2 H∁ 2
Filter for Sparse PCA We use the following result to filter outliers when we identify a small
set of coordinates with a large variance.
Lemma 4.5 (Sparse PCA Filter). Let ϵ ∈ (0,ϵ ) for a small absolute constant ϵ and let C be a
0 0
large enough absolute constant. Let T be an ϵ-corrupted version of S, where S is (ϵ,γ,k,η)-stable
(cid:13) (cid:13)
with respect to v (cf. Definition 4.1) and η ∈ (0,1). Let H ⊂ [d] be such (cid:13)(Σ −I−ηvv⊤) (cid:13) = λ
(cid:13) T H(cid:13)
Fr
for λ ≥ 8Cγ and |H| ≤ k.
Then there exists an algorithm A that takes T, H, ϵ, γ, and η and returns a score mapping
f : T → R such that the sum of inliers’ scores is less than outliers’: P f(x) ≤ P f(x)
+ x∈S∩T x∈T\S
and max f(x) > 0. Moreover, the algorithm runs in time dpoly(k,|T|).
x∈T
We give the proof of the result above in Appendix B.1.
Preprocessing Condition for Sparse PCA Similar to robust sparse mean estimation, our
algorithm tracks the contribution to the diagonal terms and off-diagonal terms separately. The
following conditions mirrors Condition 2.8.
Condition 4.6 (Idealistic Condition for PCA). Let T be an ϵ-corrupted version of S, where S is
(ϵ,γ,k,η)-stable with respect to a k-sparse unit vector v and spike strength η (cf. Definition 4.1).
We have an H ⊂ [d] and |H | ≲ k2 such that T satisfies ∥diag(Σ −I ) ∥ ≲ γ.
1 1 T d H∁ Fr,k2
1
The next result gives an efficient algorithm to ensure the above condition:
Claim 4.7. Let ϵ ∈ (0,ϵ ) and γ ∈ (0,γ ) for small constants ϵ ∈ (0,1/2), γ ∈ (0,1). Let sparsity
0 0 0 0
k ∈ N. Let C be a large enough constant and T be an ϵ-corrupted set S where S is (Cϵ,γ,k′,η)-
pca-stable with respect to an unknown k-sparse unit vector v ∈ Rd, η ∈ (0,1), γ ≥ ϵ, and k′ = C′k2
for a large enough constant C′ > 0. There is a randomized algorithm A that takes as input the
corrupted set T, contamination rate ϵ, sparsity k ∈ N, and a parameter q ∈ N, and returns a set
T′ ⊂ T and H ⊂ [d] in time O(dpoly(k,|T|)) such that with probability 0.9
1
191. T′ is an O(ϵ)-contamination of S.
2. Each diagonal entry of Σ ∈ [1/2,4].
T′
3. T′ and H satisfy Condition 4.6, i.e., |H | ≲ k2 and ∥diag(Σ −I ) ∥ ≲ γ.
1 1 T′ d H∁ Fr,k2
1
The proof of this simple claim is given in Appendix B.2.
Similar to Claim 2.9, the next result shows that all large subsets of a set satisfying Claim 4.7
are close to identity in the sparse operator norm.
Claim 4.8. Let C be a large enough constant C > 0 and k,k′ ∈ N. Let T′′ ⊂ T′ be two O(ϵ)-
contamination of S such that S is an (Cϵ,γ,k′,η)-pca-stable with respect to v and η ∈ (0,1). Let
H ⊂ [d] be a small subset |H| ≤ k such that ∥diag(Σ − I ) ∥ ≲ γ. Then ∥diag(Σ −
T′ d H∁ Fr,k2 T′′
1
I ) ∥ ≲ γ.
d H∁ op,k
1
4.3 Subquadratic Time Algorithm For Sparse PCA
We now establish the main technical result of this section:
Theorem 4.9 (Subquadratic Time Algorithm for Robust Sparse PCA under Stability). Let ϵ ∈
(0,ϵ ) and γ ∈ (0,γ ) for small constants ϵ ∈ (0,1/2), γ ∈ (0,1). Let C be a large enough constant
0 0 0 0
and k ∈ N be the sparsity parameter and q ∈ N the correlation decay parameter with q ≥ 3. Let
T be an ϵ-corrupted set S where S is (Cϵ,γ,Ck′,η)-pca-stable with respect to an unknown k-sparse
(cid:16) (cid:17)
unit vector v ∈ Rd, η ∈ (0,1), and γ ≥ ϵ for k′ = Cqk2q .
γ2q−2
There is a randomized algorithm A that takes as input the corrupted set T, contamination rate
ϵ, stability parameter γ, sparsity k ∈ N, and a parameter q ∈ N, and produces an estimate v such
b
that
q
(Error) We have ∥vv⊤−vv⊤∥ ≲ γ with high probability over the randomness of the samples
bb Fr η
and the algorithm.
(Runtime) The algorithm runs in time at most d1.62+3 qpoly(|T|logd,(k/ϵ)q).
The complete algorithm is given in Algorithm 6. We now present the proof of its correctness:
Proof of Theorem 4.9. We can assume that γ ≤ η, otherwise we can simply output any unit vector,
p
whose error will be at most O(1) = O( γ/η).
Wewillusethesametemplateoffiltering-basedalgorithmsfromTheorem2.5, withthefiltering
subroutines provided by Lemma 4.5.
The first step of Algorithm 6 is the preprocessing step from Claim 4.7, which takes at most
O˜(dpoly(k,|T|)) time and removes not too many inliers with high probability. In particular, the
diagonal entries of Σ lie in [1/2,4]; Moreover, all the subsequent sets T ’s will continue to satisfy
T′ i
this property by Claim 4.8 as long as we have removed at most O(ϵ) fraction of points (since γ is
small enough). We shall use the output H generated by Claim 4.7 in the end.
1
Wewilluseafine-grainedresultfromProposition3.3. ObservethatProposition3.3canbeused
not just for the covariance Σ but also for Σ , i.e., without centering, which is what we will use in
T T
thisproof. WeshallinvokeProposition3.3toidentifyallthecoordinatepairswithcorrelationlarger
than ρ = γ/k. We then set τ = (ρ/12)q and use the Frobenius threshold κ = C′γ for a constant
C > 100. Definingk′ := κ2/τ2 andk′′ := κ2/ρ2, wenotethatk′ ≥ k′′ = C′γ2/(γ/k)2 = C′k2 ≥ 2k2.
WewillshowthatthesetH allowsustofilterpointsusingLemma4.3. TheproofofProposition3.3
reveals that it returns H such that either
20Algorithm 6 Robust Sparse PCA Algorithm
Require: corruption rate ϵ ∈ (0,1), stability parameter γ ∈ (0,1), corrupted set T ⊂ Rd,
correlation-threshold ρ ∈ (0,1), correlation decay q ∈ N, sparsity k ∈ N, spike strength
η ∈ (0,1), sampling parameter m ∈ N. We require T to be an ϵ-corrupted version of an
(cid:16) (cid:17)
(Cϵ,δ,Ck′,η) stable set with respect to µ for k′ := Cqk2q and γ ≪ 1.
γ2q−2
Ensure: v ∈ Rd such that, with high probability, ∥vv−vv⊤∥ ≲ γ.
b bb Fr
1: H 1,T′ ← be the outputs of Claim 4.7 on T
▷ T′ is an O(ϵ)-contamination of S, 0.5I ⪯ diag(Σ) ⪯ 2I , |H | ≲ k2 and
d d 1
∥diag(Σ −I ) ∥ ≲ p γ/η
T d H∁ Fr,k2
1
2: i ← 1
3: T i ← T′.
4: H ← output of Proposition 3.3 with inputs: corrupted set T i, Frobenius threshold κ = Θ(γ),
correlation threshold ρ = γ/k, margin threshold τ = (ρ/12)q, sampling parameter m ≍
d(κ2/τ2+logd), and correlation count s = d ▷ Algorithm 4
5: while T i ̸= ∅ and H ̸= “⊥” and |H| ≤ κ2/ρ2 do
6: Get the scores f : T i → R + from Lemma 4.5 with inputs T i, H, ϵ, and δ
7: T i+1 ← Filter T i using the scores f.
8: i ← i+1
9: Update H as above
10: Let H end ← H ∪H 1
11: u ← output of Lemma 4.3 with inputs T i, ϵ, γ, η, and H end
(Case I) |H| = k′ = κ2/τ2 and for each coordinate in i ∈ H, there exists a j ∈ H such that (i,j)
is τ-correlated
Let T′′ be the current iterate of the corrupted data set. Since ηvv⊤ is a k2-sparse
(cid:16) (cid:17)
matrix and offdiag Σ −I has at least k′ entries with magnitude at least Θ(τ)15,
T d
their difference must have at least k′−k2 ≥ k′/2 = κ2/τ2 entries with magnitude Θ(τ).
Thus, the sparse Frobenius norm of their difference must be large, i.e.,
(cid:13)(cid:16) (cid:17) (cid:13)
(cid:13) Σ −I −ηvv⊤ (cid:13) ≳ κ ≳ γ.
(cid:13) T′′ d (cid:13)
H Fr
Given such an H, we filter outliers using Lemma 4.3.
(Case II) |H| ≥ κ2/ρ2 and for each coordinate in i ∈ H, there exists a j ∈ H such that (i,j) is
ρ-correlated
Bythesameargumentasabove,thematrixΣ −I −ηvv⊤ hasatleastk′′−k2 ≥ k′′/2 =
T′′ d
(cid:13)(cid:16) (cid:17) (cid:13)
κ2/(2ρ2)entrieswithmagnitudeΘ(ρ). Thus,(cid:13) Σ −I −ηvv⊤ (cid:13) ≳ κ ≳ γ. Again,
(cid:13) T′′ d (cid:13)
H Fr
we filter outliers using Lemma 4.3.
(Case III) |H| ≤ κ2/ρ2 and no coordinate pair in H∁ is ρ-correlated.
Let T′′ be the current iterate of the corrupted data set. Since H ⊂ H and the
end
∁ ∁
coordinates in H are at most ρ-correlated, it follows that the coordinates in H are
end
15Recall that each diagonal entry of Σ T′′ is Θ(1) and thus τ-correlation implies that the corresponding entry in
Σ T′′ is also Θ(τ).
21also at most ρ-correlated.
(cid:13) (cid:13) (cid:13) (cid:13)offdiag(cid:18)(cid:16) Σ T′′ −I d(cid:17) H e∁ nd(cid:19)(cid:13) (cid:13) (cid:13) (cid:13)
Fr,k2
≤ (cid:13) (cid:13) (cid:13)offdiag(cid:16)(cid:16) Σ T′′ −I d(cid:17) H∁(cid:17)(cid:13) (cid:13) (cid:13) Fr,k2 ≲ ρk ≲ γ. (4)
We now want to combine the above guarantee on the closeness along the offdiagonals
with the guarantee on the closeness along the diagonals from H . In particular, H
1 end
satisfies that
(cid:13) (cid:13) (cid:13) (cid:13)diag(cid:18)(cid:16) Σ T′′ −I d(cid:17)
H∁
(cid:19)(cid:13) (cid:13) (cid:13)
(cid:13)
≤ (cid:13) (cid:13) (cid:13) (cid:13)diag(cid:18)(cid:16) Σ T′′ −I d(cid:17) H∁(cid:19)(cid:13) (cid:13) (cid:13)
(cid:13)
≲ γ, (5)
end op,k 1 op,k
where T′′ has small sparse operator norm because it close to the preprocessed set T′
and thus Claim 4.8 is applicable. Combining (4) and (5), we obtain
(cid:13) (cid:13) (cid:13) (cid:13)(Σ T′′ −I d) H e∁ nd(cid:13) (cid:13) (cid:13) (cid:13) op,k ≤ (cid:13) (cid:13) (cid:13) (cid:13)diag(cid:18)(cid:16) Σ T′′ −I d(cid:17) H e∁ nd(cid:19)(cid:13) (cid:13) (cid:13) (cid:13) op,k +(cid:13) (cid:13) (cid:13) (cid:13)offdiag(cid:18)(cid:16) Σ T′′ −I d(cid:17) H e∁ nd(cid:19)(cid:13) (cid:13) (cid:13) (cid:13) op,k ≲ γ.
(6)
The size of H is at most O(k2) + k′ ≤ 2k′. Since S satisfies stability with 2k′,
end
Lemma 4.4 then implies that the spike v is mostly contained in H . By invoking the
end
dense PCA algorithm on H , Lemma 4.3 estimates the spike (v) (v)⊤ with uu⊤,
end Hend Hend
with u supported on H . Combining, we obtain:
end
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)vv⊤−uu⊤(cid:13) ≤ (cid:13)vv⊤−(v) (v)⊤ (cid:13) +(cid:13)uu⊤−(v) (v)⊤ (cid:13)
(cid:13) (cid:13)
Fr
(cid:13) Hend Hend(cid:13)
Fr
(cid:13) Hend Hend(cid:13)
Fr
q q
≲ γ/η+γ/η ≲ γ/η,
where the first term is bounded using Lemma 4.4 with (6) and the second term is
bounded using Lemma 4.3.
(Case IV) H = “⊥”.
The same argument as the previous case holds.
p
Thus, the output of the algorithm is O( γ/η) close as required. It remains to show the choice of
the parameters m, s, and q lead to the claimed runtime. We take τ = (ρ/12)q and s = d. Finally,
we take m ≍ (d2/s)·(κ2/τ2 +logd) ≤ (dlogd)(κ2/τ2), which satisfies the parameter constraints
in Proposition 3.3 (cf. (3)). Letting n = |T|, the resulting runtime of a single application of
Proposition 3.3 is thus at most
A =
(cid:16) m+sd0.62+d1.62+3 ll oo gg (( 14 // 3ρ τ) )(cid:17)
poly(n,logd,1/τ)
≤ (cid:16) d1.62+3 lol go (g (( ρ4 // 4ρ )) q)(cid:17) poly(n,logd,1/ρq)
≤ (cid:16) d1.62+3 q(cid:17) poly(n,logd,kq,1/ϵq).
As the iteration count is bounded by n, the total runtime is at most nA.
4.4 Proof of Theorem 1.6
Proof of Theorem 1.6 using Theorem 4.9. By Theorem 4.9, it suffices to establish that a set of n
i.i.d. samples from N(0,I+ηvv⊤) for a k-sparse unit vector v, with high probability, is (ϵ,γ,k′,η)-
pca-stable for γ ≲ ϵlog(1/ϵ) and k′ := (Cqk2q ), where C is a large absolute constant. Lemma 4.2
γ2q−2
gives a bound on the sample complexity, stating that it suffices to take n = poly(k′,log(d),1/ϵ)
many samples, thus establishing Theorem 1.6.
225 Discussion
Inthisarticle,wepresentedthefirstsubquadratictimealgorithmforrobustsparsemeanestimation.
We now discuss some related open problems and avenues for improvement. First, the sample com-
plexity of Theorem 1.5 is polynomially larger than k2(logd)/ϵ2—the sample complexity of existing
(quadratic runtime) algorithms.16 Bridging this gap is an important problem to improve sample
efficiency. Second, Theorem 1.5 is specific to isotropic distributions whose quadratic polynomials
haveboundedvariance(distributionsP withmeanµthatsatisfyVar ((x−µ)⊤A(x−µ)) ≲ ∥A∥2
x∼P Fr
for all symmetric matrices A). Indeed, both [DKKPS19; CDKGGS22] rely on the isotropy and the
aforementioned variance structure of quadratic polynomials to avoid solving SDPs that appear in
[BDLS17]. To the best of our knowledge, even obtaining an O(d2) runtime algorithm for unstruc-
tured distributions is still open. Third, because Theorem 1.5 relies on [Val15], which in turn relies
on fast matrix multiplication, the resulting algorithm may not offer practical benefits for moderate
dimensions; see the discussion in [Val15]. We believe overcoming these limitations is an important
practically-motivated question. Finally, Question 1.3 remains open.
Acknowledgements
We are grateful to Sushrut Karmalkar, Jasper Lee, Thanasis Pittas, and Kevin Tian for discussions
on robust sparse mean estimation. We also thank Ilias Diakonikolas and Daniel Kane for their
many insights on robustness.
References
[ABHHRT72] D. F. Andrews, P. J. Bickel, F. R. Hampel, P. J. Huber, W. H. Rogers, and J. W.
Tukey. Robust Estimates of Location: Survey and Advances. Princeton, NJ, USA:
Princeton University Press, 1972. [1]
[Alm19] J. Alman. “An Illuminating Algorithm for the Light Bulb Problem”. Proc. 2nd
Symposium on Simplicity in Algorithms (SOSA). 2019. [7]
[BB20] M.BrennanandG.Bresler.“ReducibilityandStatistical-ComputationalGapsfrom
SecretLeakage”.Proc. 33rd Annual Conference on Learning Theory (COLT).2020.
[7, 23]
[BBHLS21] M. Brennan, G. Bresler, S. B. Hopkins, J. Li, and T. Schramm. “Statistical query
algorithms and low-degree tests are almost equivalent”. Proc. 34th Annual Confer-
ence on Learning Theory (COLT). 2021. [3]
[BDLS17] S. Balakrishnan, S. S. Du, J. Li, and A. Singh. “Computationally Efficient Robust
SparseEstimationinHighDimensions”.Proc. 30th Annual Conference on Learning
Theory (COLT). 2017. [2, 4–6, 10, 17, 23]
[CATJFB20] Y. Cherapanamjeri, E. Aras, N. Tripuraneni, M. I. Jordan, N. Flammarion, and
P. L. Bartlett. “Optimal Robust Linear Regression in Nearly Linear Time”. arXiv
abs/2007.08137 (2020). [7]
16ThesamplecomplexityofΘ˜(k2/ϵ2)isalsoconjecturedtobenear-optimalamongcomputationally-efficientalgo-
rithms [BB20].
23[CDG19] Y.Cheng,I.Diakonikolas,andR.Ge.“High-DimensionalRobustMeanEstimation
in Nearly-Linear Time”. Proc. 30th Annual Symposium on Discrete Algorithms
(SODA). 2019. [3, 7]
[CDGW19] Y. Cheng, I. Diakonikolas, R. Ge, and D. P. Woodruff. “Faster Algorithms for
High-Dimensional Robust Covariance Estimation”. Proc. 32nd Annual Conference
on Learning Theory (COLT). 2019. [7]
[CDKGGS22] Y. Cheng, I. Diakonikolas, D. M. Kane, R. Ge, S. Gupta, and M. Soltanolkotabi.
“Outlier-Robust Sparse Estimation via Non-Convex Optimization”. Advances in
Neural Information Processing Systems 35 (NeurIPS). 2022. [2, 4, 6–9, 17, 18, 23]
[Che21] Y.Cheng.High-DimensionalRobustStatistics:FasterAlgorithmsandOptimization
Landscape. Robustness in High-dimensional Statistics and Machine Learning at
IDEAL Institute. See timestamp 22:00 in the talk. 2021. [2, 3]
[CMY20] Y. Cherapanamjeri, S. Mohanty, and M. Yau. “List decodable mean estimation
in nearly linear time”. Proc. 61st IEEE Symposium on Foundations of Computer
Science (FOCS). 2020. [7]
[dGJL07] A. d’Aspremont, L. E. Ghaoui, M. I. Jordan, and G. R. G. Lanckriet. “A Direct
FormulationforSparsePCAUsingSemidefiniteProgramming”.SIAM Review 49.3
(2007). [4, 5]
[DHL19] Y. Dong, S. B. Hopkins, and J. Li. “Quantum Entropy Scoring for Fast Robust
Mean Estimation and Improved Outlier Detection”. Advances in Neural Informa-
tion Processing Systems 32 (NeurIPS). 2019. [3, 7]
[Dia19] I. Diakonikolas. Computational-Statistical Tradeoffs and Open Problems. STOC
2019 Tutorial: Recent Advances in High-Dimensional Robust Statistics. See page
number36inhttp://www.iliasdiakonikolas.org/stoc19-tutorial/Tradeoffs-and-Open-
Problems.pdf. 2019. [2, 3]
[Dia23] I. Diakonikolas. Algorithmic Robust Statistics. Statistical thinking in the age of AI
: robustness, fairness and privacy (Meeting in Mathematical Statistics). Available
online at https://youtu.be/HKm0L2Cy69Y?t=3527. 2023. [2, 3]
[DK23] I. Diakonikolas and D. M. Kane. Algorithmic High-Dimensional Robust Statistics.
Cambridge University Press, 2023. [2, 6, 9, 10, 17, 18, 27]
[DKKLMS16] I.Diakonikolas,G.Kamath,D.M.Kane,J.Li,A.Moitra,andA.Stewart.“Robust
Estimators in High Dimensions without the Computational Intractability”. Proc.
57th IEEE Symposium on Foundations of Computer Science (FOCS). 2016. [2]
[DKKLT22] I. Diakonikolas, D. M. Kane, D. Kongsgaard, J. Li, and K. Tian. “Clustering Mix-
ture Models in Almost-Linear Time via List-Decodable Mean Estimation”. Proc.
54th Annual ACM Symposium on Theory of Computing (STOC). 2022. [3, 7]
[DKKPP22] I. Diakonikolas, D. M. Kane, S. Karmalkar, A. Pensia, and T. Pittas. “Robust
Sparse Mean Estimation via Sum of Squares”. Proc. 35th Annual Conference on
Learning Theory (COLT). 2022. [2, 7]
[DKKPS19] I. Diakonikolas, D. M. Kane, S. Karmalkar, E. Price, and A. Stewart. “Outlier-
Robust High-Dimensional Sparse Estimation via Iterative Filtering”. Advances in
Neural Information Processing Systems 32 (NeurIPS). 2019. [2, 4–6, 8, 10, 17, 23]
24[DKLP22] I. Diakonikolas, D. M. Kane, J. C. H. Lee, and A. Pensia. “Outlier-Robust Sparse
Mean Estimation for Heavy-Tailed Distributions”. Advances in Neural Information
Processing Systems 35 (NeurIPS). 2022. [2, 7]
[DKPP22] I. Diakonikolas, D. M. Kane, A. Pensia, and T. Pittas. “Streaming Algorithms
for High-Dimensional Robust Statistics”. Proc. 39th International Conference on
Machine Learning (ICML). 2022. [3, 7]
[DKPP23a] I. Diakonikolas, D. M. Kane, A. Pensia, and T. Pittas. “Near-Optimal Algorithms
for Gaussians with Huber Contamination: Mean Estimation and Linear Regres-
sion”. Advances in Neural Information Processing Systems 36 (NeurIPS). 2023.
[7]
[DKPP23b] I. Diakonikolas, D. M. Kane, A. Pensia, and T. Pittas. “Nearly-Linear Time and
Streaming Algorithms for Outlier-Robust PCA”. Proc. 40th International Confer-
ence on Machine Learning (ICML). 2023. [7]
[DKS17] I. Diakonikolas, D. M. Kane, and A. Stewart. “Statistical Query Lower Bounds for
Robust Estimation of High-Dimensional Gaussians and Gaussian Mixtures”. Proc.
58th IEEE Symposium on Foundations of Computer Science (FOCS). 2017. [3, 7]
[DL22] J. Depersin and G. Lecué. “Robust Subgaussian Estimation of a Mean Vector in
Nearly Linear Time”. The Annals of Statistics 50.1 (2022). [7]
[DS18] Y. Dagan and O. Shamir. “Detecting correlations with little memory and commu-
nication”. Proc. 31st Annual Conference on Learning Theory (COLT). 2018. [3]
[EK12] Y. C. Eldar and G. Kutyniok. Compressed sensing: theory and applications. Cam-
bridge University Press, 2012. [2]
[HR09] P. J. Huber and E. M. Ronchetti. Robust Statistics. John Wiley & Sons, 2009. [1]
[HTW15] T. Hastie, R. Tibshirani, and M. Wainwright. Statistical Learning with Sparsity:
The Lasso and Generalizations. 2015. [2, 4]
[Hub64] P. J. Huber. “Robust Estimation of a Location Parameter”. The Annals of Mathe-
matical Statistics 35.1 (Mar. 1964). [1]
[JKLPS20] H.Jiang,T.Kathuria,Y.T.Lee,S.Padmanabhan,andZ.Song.“AFasterInterior
Point Method for Semidefinite Programming”. Proc. 61st IEEE Symposium on
Foundations of Computer Science (FOCS). 2020. [5]
[JLT20] A. Jambulapati, J. Li, and K. Tian. “Robust sub-gaussian principal component
analysisandwidth-independentschattenpacking”.AdvancesinNeuralInformation
Processing Systems 33 (NeurIPS). 2020. [7]
[KKK18] M.Karppa,P.Kaski,andJ.Kohonen.“AFasterSubquadraticAlgorithmforFind-
ing Outlier Correlations”. ACM Trans. Algorithms 14.3 (2018). [7, 30]
[KKKC20] M. Karppa, P. Kaski, J. Kohonen, and P. Ó Catháin. “Explicit Correlation Ampli-
fiers for Finding Outlier Correlations in Deterministic Subquadratic Time”. Algo-
rithmica 82.11 (2020). [7]
[LLVZ20] Z. Lei, K. Luh, P. Venkat, and F. Zhang. “A Fast Spectral Algorithm for Mean
Estimation with Sub-Gaussian Rates”. Proc. 33rd Annual Conference on Learning
Theory (COLT). 2020. [7]
25[LRV16] K. A. Lai, A. B. Rao, and S. Vempala. “Agnostic Estimation of Mean and Covari-
ance”. Proc. 57th IEEE Symposium on Foundations of Computer Science (FOCS).
2016. [2]
[PW23] Y. Polyanskiy and Y. Wu. Information Theory: From Coding to Learning. Cam-
bridge University Press, 2023. [13]
[SV14] S. Sachdeva and N. K. Vishnoi. “Faster Algorithms via Approximation Theory”.
Foundations and Trends® in Theoretical Computer Science 9 (2014). [5]
[Val15] G. Valiant. “Finding Correlations in Subquadratic Time, with Applications to
Learning Parities and the Closest Pair Problem”. Journal of the ACM 62.2 (2015).
[1, 3–7, 10, 11, 23, 27, 30]
[Val88] L. G. Valiant. “Functionality in Neural Nets”. Proc. of the Seventh AAAI National
Conference on Artificial Intelligence. AAAI Press, 1988. [7]
[van16] S.vandeGeer.Estimation and Testing Under Sparsity.Écoled’ÉtédeProbabilités
de Saint-Flour. Springer, 2016. [2]
[Ver18] R. Vershynin. High-Dimensional Probability: An Introduction with Applications in
Data Science. Cambridge University Press, 2018. [9]
[ZJS22] B. Zhu, J. Jiao, and J. Steinhardt. “Robust Estimation via Generalized Quasi-
Gradients”. Information and Inference: A Journal of the IMA (2022). [2, 6]
26A Details Deferred from Section 2
In this section, we include details that were omitted from Section 2. Appendix A.1 provides the
proof of Lemma 2.7. Appendix A.2 ensures the preprocessing condition listed in Condition 2.8.
Finally, Appendix A.3 gives further details about the correlation detection algorithm from [Val15].
A.1 Defining Sparse Scores
In this section, we give the proof of Lemma 2.7.
Lemma 2.7 (Sparse Filtering Lemma). Let ϵ ∈ (0,ϵ ) for a small absolute constant ϵ . Let T
0 0
be an ϵ-corrupted version of S, where S is (ϵ,δ,k)-stable with respect to µ. Let H ⊂ [d] be such
∥(Σ −I) ∥ = λ for λ ≳ δ2/ϵ and |H| ≤ k. There exists an algorithm A that takes T, H, ϵ, and
T H Fr
δ and returns scores f : T → R so that P f(x) ≤ P f(x), i.e., the sum of scores over
+ x∈S∩T x∈T\S
inliers is less than that of outliers, and max f(x) > 0. Moreover, the algorithm runs in time
x∈T
d·poly(|H||T|).
Proof. Asafirststep, wesimplyprojectthedatapointsalongthecoordinatesinH, andbyabusing
the notation, call the projected set T. In the remainder of this proof, I refers to I . Computing
|H|
this projected set takes at most d|T||H| time.
Let λ = ∥Σ −I∥ . Define A = (Σ −I)/∥Σ −I∥ so that A maximizes the trace inner
T Fr T T1 Fr
productwithΣ −IoverunitFrobeniusnormmatrices. Definethefunctiong(x) := (x−µ )⊤A(x−
T T
µ )−tr(A). We first compute the average of g(x) over the T below:
T
hD Ei
E [g(x)] = E (x−µ )(x−µ )⊤−I,A = ⟨Σ −I,A⟩ = λ.
T T T T T
By abusing notation again, we use S to denote the projection of the inliers S along the coordinates
in H; Note that the projected set also inherits (ϵ,δ,k)-stability and the ∥ · ∥ reduces to the
Fr,k2
standard Frobenius norm. Thus, for any large subset of S′ ⊂ S with |S′| ≥ (1 − ϵ)|S|, we use
Lemma 2.6 to obtain the following:
(cid:12) hD Ei(cid:12)
|E [g(x)]| = (cid:12)E (x−µ )(x−µ )⊤−I,A (cid:12)
S′ (cid:12) S′ T T (cid:12)
(cid:12) (cid:12)
= (cid:12)⟨Σ −I,A⟩+2(µ−µ )⊤A(µ−µ )+(µ−µ )⊤A(µ−µ )(cid:12)
(cid:12) S′ T S′ T T (cid:12)
≤ ∥Σ −I∥ +2∥µ−µ ∥ ∥A∥ ∥µ−µ ∥ +∥µ−µ ∥2∥A∥2
S′ Fr T 2 Fr S′ 2 T 2 Fr
√ √
≲ δ2/ϵ+2(δ+ ϵλ)δ+(δ+ ϵλ)2 (using stability and Lemma 2.6)
√
≲ δ2/ϵ+3δ2+4δ ϵλ+ϵλ
≲ δ2/ϵ+7δ2+2ϵλ (using 2ab ≤ a2+b2)
≲ δ2/ϵ+ϵλ, (7)
where we used that ϵ ≤ 1. The following helper result, which is a slight generalization of [DK23,
Proposition 2.19] from non-negative h’s to real-valued h’s will be useful.
Claim A.1. Let h : S → R be a real-valued function on a finite set S. Further suppose that
|E [h(X)]| ≤ τ for all sets S′ ⊂ S with |S′| ≥ (1 − ϵ)|S|. For an ϵ ≤ 1/2, defining f′(x) =
S′
h(x)1 , we have that E [f′(x)] ≤ 3τ.
h(x)≥3τ/ϵ S
Proof. First, we show the following: for all subsets S′′ ⊂ S with |S′′| ≤ ϵ|S|,
E [max(h(X),0)] ≤ τ +2τ/ϵ. (8)
S′′
27To that end, for all sets S′′ with |S′′| = ϵ|S|, the triangle inequality implies
|E [h(X)]| =
(cid:12)
(cid:12)
(cid:12)E S[h(X)]−(1−ϵ)E S\S′′[h(X)](cid:12)
(cid:12)
(cid:12) ≤ 2τ/ϵ. (9)
S′′ (cid:12) ϵ (cid:12)
(cid:12) (cid:12)
Let S be the top ϵ|S| entries of S in the increasing order of h(·), not in the absolute value. Then
∗
establishing (8) is equivalent to establish an upper bound on 1 P max(h(X),0). Thus, if
|S∗| x∈S∗
all the entries of S are bigger than 0, then (8) follows by (9). We shall show that h(x) on S
∗ ∗
is lower bounded by −τ. Under this condition, we see that the desired result follows similarly
by (9): 1 P max(h(X),0) ≤ 1 P (h(X)+τ) ≤ τ + 2τ/ϵ. We now establish that
|S∗| x∈S∗ |S∗| x∈S∗
min h(x) ≥ −τ. If there exists an x ∈ S with h(X) ≤ −τ, then the average of S \S must be
x∈S∗ ∗ ∗
less than −τ, contradicting the assumption that |E [h(X)]| ≤ τ. Thus, we have established (8).
S\S∗
Given(8),weseethatthefractionofpointswithh(x) ≥ 3τ/ϵmustbelessthanϵ|S|. Otherwise,
the conditional average over those points would be at least than 3τ/ϵ ≥ 2τ/ϵ+τ, contradicting (8).
Therefore, the function f is non-zero only on at most ϵ-fraction of S. Therefore, the non-negativity
of f implies that
1 X 1 X 1 X
f(X) ≤ max f(X) ≤ max max(h(X),0) ≤ ϵ(τ +2τ/ϵ),
|S| |S| S′′⊂S:|S′′|≤ϵ|S| |S| S′′⊂S:|S′′|≤ϵ|S|
x∈S x∈S′′ x∈S′′
(10)
where we use non-negativity of f and (8).
Let R := C′(δ2 +ϵλ), for a large constant C′ > 0, be the bound from (7). Combining this with
ϵ
the claim above, we see that defining f(x) to be g(x)1 , the sum of scores over inliers is small:
x≥3R/ϵ
X f(x) ≤ X f(x) ≤ 3R|S| = 3C′(δ2/ϵ+ϵλ)|S| ≤ 0.25λ|S|, (11)
x∈S∩T x∈S
where we use that 3C′ϵ ≤ 1/8 and 3C′δ2/ϵ ≤ λ/8.
On the other hand, E [f(x)] must be large argued as argued below. We observe that
x∈T\S
f(x) ≥ g(x)−3R/ϵ, and thus applying (7) to T ∩S, which is of size at least (1−ϵ)|T| = (1−ϵ)|S|,
we obtain
! !!
X X X X
f(x) ≥ (g(x)−3R/ϵ) = g(x) − g(x) −(|T \S|3R/ϵ)
x∈T\S x∈T\S x∈T x∈T∩S
≥ (λ|T|)−((|T ∩S|)R)−ϵ|T|(3R/ϵ)
≥ λ|T|−4|T|R ≥ λ|T|/2, (12)
where the last inequality follows if we show that R ≤ λ/8. Indeed, this follows if C′δ2/ϵ ≤ λ/16
and C′ϵ ≤ 1/16. Combining (11) and (12), we get the desired result; the claim on max f(x)
x∈T
follows from (12). The complete algorithm is given below:
A.2 Preprocessing: Proofs of Claims 2.9 and A.2
In this section, we outline how to ensure Condition 2.8 quickly using Algorithm 1 and Lemma 2.7.
28Algorithm 7 Quadratic Scores
1: T ← {(x) H : x ∈ T} ▷ Projection onto H
2: Let A be the matrix (Σ T −I)/∥Σ T −I∥ Fr ▷ The matrix such that ⟨A,Σ T −I⟩ = ∥Σ T −I∥ Fr
3: Define g(x) := (x−µ T)⊤A(x−µ T)−tr(A)
(cid:16) (cid:17)
4: Define f(x) to be g(x) if g(x) ≥ 3C′ δ2 +λ otherwise 0
ϵ2
5: Return f
Condition 2.8 (Preprocessing). Let T be an ϵ-corrupted version of S, where S is (ϵ,δ,k)-stable.
Suppose T satisfies ∥diag(Σ −I )∥ ≤
min(cid:0) O(δ2/ϵ),0.5(cid:1)
.
T d Fr,k2
Giventhesparsefilteringlemma(Lemma2.7),wecansimplyfilteralongthediagonalstoensure
Condition 2.8 as shown below.
Claim A.2. Let ϵ ∈ (0,ϵ ) for a small absolute constant ϵ . Let c be a small enough absolute
0 0
constant and C be a large enough constant. Let T be an ϵ-corrupted version of S, where S is
(ϵ,δ,k2)-stable with respect to µ such that δ2/ϵ ≤ c. Then there is a randomized algorithm A that
takes as inputs ϵ, δ, and k such that it outputs a set T′ ⊆ T such that with probability at least 8/9:
(i) T′ is at most 10ϵ-corruption of S and (ii) ∥diag(Σ −I )∥ ≤ Cδ2/ϵ ≤ 0.1, and (iii) the
T′ d Fr,k2
algorithm runs in time O˜(|T|dk2+|T|2d).
Proof. We can simply invoke Algorithm 1 with the stopping condition on the set T set to
i
∥diag(Σ −I )∥ ≤ Cδ2/ϵ
Ti d Fr,k
foraconstantC largeenough. Toevaluatethisstoppingcondition, wecansimplycomputethema-
trix diag(Σ −I ) in O(d|T|) time. The associated ∥·∥ can be easily calculated by computing
T′ d Fr,k2
the Euclidean norm of its largest k2 entries, again computable in O˜(dk2) time.
If the stopping condition is not satisfied, Lemma 2.7 returns the required scores. Thus, we get
the desired guarantees on the set T from Theorem 2.5.
We now give the proof of Claim 2.9.
Claim 2.9. Let C be a large enough constant C > 0. Let T′′ ⊂ T′ be two O(ϵ)-contamination of
S such that S is an (Cϵ,δ,k)-stable with respect to µ. Suppose that ∥diag(Σ −I )∥ ≲ δ2/ϵ.
T′ d op,k
Then ∥diag(Σ −I )∥ ≲ δ2/ϵ
T′ d op,k
Proof. First we note that the lower bound on the sparse eigenvalues follow rather directly as shown
below. We make use of the equality Σ := 1 P (x−y)(x−y)⊤. For any sparse unit
T′′ |T′′|2 x,y∈T′′
vector v, we use the stability condition applied to S ∩T′′ to obtain the following:
v⊤Σ v = 1 X (v⊤(x−y))2 ≥ 1 X (v⊤(x−y))2 (using non-negativity)
T′′
|T′′|2 |T′′|2
x,y∈T′′ x,y∈S∩T′′
|S ∩T′′|2
= v⊤Σ v ≥ (1−O(ϵ))v⊤Σ v
|T′′|2
S∩T′′ S∩T′′
(cid:18) (cid:16) h i(cid:17) (cid:16) (cid:17)2(cid:19)
= (1−O(ϵ)) v⊤ E (X −µ)(X −µ)⊤ v− v⊤(µ−µ )
S∩T′′ S∩T′′
(cid:16) (cid:17)
≥ (1−O(ϵ)) 1−O(δ2/ϵ)−O(δ2) (using stability)
≥ 1−O(δ2/ϵ), (13)
29where the last inequality uses ϵ ≤ δ. For the upper bound, we observe that for any matrix A,
∥diag(A)∥ is attained by 1-sparse unit vectors v, i.e., ∥A∥ = ∥diag(A)∥ . Thus, for any
op,k op,1 op,k
T′′ ⊂ T′ and that |T′| ≤ |T′′|(1+O(ϵ)): for any 1-sparse unit vector v,
v⊤Σ v = 1 X (v⊤(x−y))2 ≤ |T′|22 1 X (v⊤(x−y))2 (using nonnegativity)
T′′
|T′′|2 |T′′|2 |T′|2
x,y∈T′′ x,y∈T′
|T′|22
= v⊤Σ v ≤ (1+O(ϵ))v⊤Σ v ≤ (1+O(ϵ))(1+O(δ2/ϵ)) = 1+O(δ2/ϵ), (14)
|T′′|2
T′ T′
whereweusethatfor1-sparseunitvectorsv, v⊤Σ v = v⊤diag(Σ )v. CombiningEquations(13)
T′ T′
and (14) for all 1-sparse unit vectors v, we obtain that ∥diag(Σ −I)∥ = O(δ2/ϵ).
T′′ op,k
A.3 Fast Correlation Detection
In this section, we show how to obtain Theorem 2.10 from [Val15, Theorem 2.1].
Theorem A.3 (Robust Correlation Detection For Boolean Vectors in Subquadratic Time [Val15,
Theorem 2.1]). Consider a set of n′ vectors in {−1,1}d′ and constants ρ,τ ∈ [0,1] with ρ > τ such
that for all but at most s pairs u,v of distinct vectors, |u⊤v|/∥u∥ ∥v∥ ≤ τ. There is an algorithm
2 2
that, with probability 1−o(1), will output all pairs of vectors whose normalized inner product is
least ρ. Additionally, the runtime of the algorithm is
(cid:18) (cid:19)
sd′n′0.62 +n′1.62+2.4 ll oo gg (( 11 // τρ)
) poly(logn,1/τ) .
An improved algorithm with better runtime was then provided in [KKK18, Corollary 1.8], but
we choose the version above for its simplicity. We provide the proof of Theorem 2.10, the version
we used in this work, from Theorem A.3 using standard arguments below:
Proof of Theorem 2.10 from [Val15, Theorem 2.1]. Let X ∈ Rd×n denote the matrix with the
columns of X denoting the centered vectors of T. That is, if T = {z ,...,z }, then the i-th
1 n
column of X is equal to z −µ . Let X denote the i-th row of the matrix X. For i,j ∈ [d]×[d],
i T i
(cid:12) (cid:12)
the correlation between the i-th and the j-th coordinate on T, corr(i,j), is equal to
(cid:12)
(cid:12)
X i⊤Xj (cid:12)
(cid:12).
(cid:12)∥Xi∥2∥Xj∥2(cid:12)
Thus, we would like to apply Theorem A.3 with the rows of X (thus n′ = d and d′ = n). However,
X′ is not a binary matrix.
A standard reduction allows us to compute a binary matrix that preserves the correlation
between the rows of X. Let G ∈ Rn×m be a matrix with independent M(0,1) entries. Let Y = XG
be in Rd×m and let Y′ = sgn(Y) ∈ Rd×m, where sgn is applied elementwise. Thus, Y′ is a boolean
matrix as required in Theorem A.3. The following arguments show that the correlation between
the rows of Y is preserved for m large enough. Let Y ,Y′ denote the i-th row of the matrices Y,Y′.
i i
Lemma A.4 ([Val15, Lemma 4.1]). If m ≥ 10log(n)/γ2, then with probability 1−o(1), we have
that for all i ̸= j ∈ [n], we have that
(cid:12)
(cid:12)
⟨Y′,Y′⟩
2 ⟨X ,X ⟩
!(cid:12)
(cid:12)
(cid:12) i j − arcsin i j (cid:12) ≤ γ.
(cid:12)∥Y′∥ ∥Y′∥ π ∥X ∥ ∥X ∥ (cid:12)
(cid:12) i 2 j 2 i 2 j 2 (cid:12)
30In particular, iftheoriginal correlation is lessthanτ in theabsolute value, then the correspond-
ingcorrelationinY′ intheabsolutevalueisatmost(2/π)arcsin(τ)+γ ≤ (4/π)τ+γ ≤ 2τ+γ,where
weuse|arcsin(x)| ≤ |2x|. Similarly,iftheoriginalcorrelationisatleastρinabsolutevalue,thenthe
new correlation is at least (2/π)arcsin(ρ)−γ ≥ (2/π)ρ−γ ≥ ρ/2−γ. Choosing γ = τ, the new ma-
trixY′ satisfiestheguaranteesofTheoremA.3withn′ = d,d′ = m = 10log(n)/τ2,τ′ = 3τ,ρ′ = ρ/4
(using τ ≤ ρ/4). The time taken to compute Y and Y′ is at most ndm. Thus, the total runtime is
at most
(cid:18) ndlogn +slogn d0.62+d1.62+2.4 ll oo gg (( 14 // 3ρ τ) )poly(logn,1/τ)(cid:19)
γ2 τ2
≤
(cid:18) sd0.62+d1.62+2.4 ll oo gg (( 14 // 3ρ τ) )(cid:19) ·poly(cid:16) n(logd)/γ2(cid:17)
.
The probability of success can be boosted using repetition, if needed.
B Details Deferred from Section 4
In this section, we give the proofs of Lemma 4.5 and Claims 4.7 and 4.8.
B.1 Proof of Lemma 4.5
Lemma 4.5 (Sparse PCA Filter). Let ϵ ∈ (0,ϵ ) for a small absolute constant ϵ and let C be a
0 0
large enough absolute constant. Let T be an ϵ-corrupted version of S, where S is (ϵ,γ,k,η)-stable
(cid:13) (cid:13)
with respect to v (cf. Definition 4.1) and η ∈ (0,1). Let H ⊂ [d] be such (cid:13)(Σ −I−ηvv⊤) (cid:13) = λ
(cid:13) T H(cid:13)
Fr
for λ ≥ 8Cγ and |H| ≤ k.
Then there exists an algorithm A that takes T, H, ϵ, γ, and η and returns a score mapping
f : T → R such that the sum of inliers’ scores is less than outliers’: P f(x) ≤ P f(x)
+ x∈S∩T x∈T\S
and max f(x) > 0. Moreover, the algorithm runs in time dpoly(k,|T|).
x∈T
Proof. We use the same ideas from the proof of Lemma 2.7 and similarly assume that T and S
already correspond to the projected coordinates. The challenge in applying the idea as is lie in the
uncertainty about v. We thus use Lemma 4.3 to first estimate v using the returned vector u which
satisfies that ∥ηuu⊤−ηvv⊤∥ ≲ γ.17 We give the algorithm below.
Fr
Algorithm 8 PCA Filter
1: T ← {(x) H : x ∈ T} ▷ Projection onto H
2: u ← be output of dense covariance estimation algorithm Lemma 4.3
3: Let A be the matrix (Σ T −I−ηuu⊤)/∥Σ T1 −I−ηuu⊤∥ Fr
4: Define g(x) := (x−µ T)⊤A(x−µ T)−tr(A)
5: Define f(x) to be g(x) if g(x) ≥ Ω(γ/ϵ) otherwise 0
6: Return f
(cid:13) (cid:13)
Let λ = (cid:13)(Σ −I−ηvv⊤) (cid:13) and define A = (Σ −I−ηuu⊤)/∥Σ −I−ηuu⊤∥ . Define
(cid:13) T H(cid:13)
Fr
T T1 Fr
the function g(x) := x⊤Ax−⟨I+ηuu⊤,A⟩. Computing the average of g(x) over the T, we obtain
hD Ei
E [g(x)] = E xx⊤−I−ηuu⊤,A = ⟨Σ −I−ηuu⊤,A⟩ = ∥Σ −I−ηuu⊤∥
T T T T Fr
17Observe that v here corresponds to (v) because of the projection to H. Hence v is no longer a unit vector.
H
31≥ ∥Σ −I−ηvv⊤∥ −∥ηuu⊤−ηvv⊤∥ = λ−O(γ) ≥ 3λ/4, (15)
T Fr Fr
where we use that λ ≳ γ. Using the stability of S (observe that on the projected set, the ∥·∥
Fr,k2
norm becomes the usual Frobenius norm), for any large subset of S′ ⊂ S with |S′| ≥ (1−ϵ)|S|, the
closeness between u and v implies the following:
(cid:12) hD Ei(cid:12) (cid:12)D E(cid:12)
|E [g(x)]| = (cid:12)E xx⊤−I−ηuu⊤,A (cid:12) = (cid:12) Σ −I−ηuu⊤,A (cid:12)
S′ (cid:12) S′ (cid:12) (cid:12) S′ (cid:12)
(cid:12)D E D E(cid:12) (cid:13) (cid:13) (cid:13) (cid:13)
= (cid:12) Σ −I−ηvv⊤,A + ηvv⊤−ηuu⊤,A (cid:12) ≤ (cid:13)Σ −I−ηvv⊤(cid:13) +η(cid:13)vv⊤−uu⊤(cid:13)
(cid:12) S′ (cid:12) (cid:13) S′ (cid:13) (cid:13) (cid:13)
Fr Fr
≤ γ +O(γ) ≤ Cγ (16)
for a large enough absolute constant C. Now define f(x) := g(x)1 , i.e., f(x) is equal to
g(x)≥3Cγ/ϵ
g(x) if g(x) ≥ 3Cγ/ϵ and 0 otherwise. By Claim A.1,
X X
f(x) ≤ f(x) ≤ 3Cγ|S| ≤ |T|λ/4, (17)
x∈S∩T x∈S
where we use that λ ≳ Cγ.
Ontheotherhand, E [f(x)]mustbelargearguedasarguedbelow. Applying(16)toT∩S,
x∈T\S
which is of size at least (1−ϵ)|T| = (1−ϵ)|S|, we obtain
! !!
X X X X
f(x) ≥ (g(x)−3Cγ/ϵ) = g(x) − g(x) −(|T \S|3Cγ/ϵ)
x∈T\S x∈T\S x∈T x∈T∩S
≥ (3λ|T|/4)−((|T ∩S|)(Cγ))−ϵ|T|(3Cγ/ϵ)
≥ (3λ|T|/4)−(4|T|Cγ)
≥ λ|T|/2, (18)
where we use (15) and λ ≳ γ. The desired conclusions follow from (17) and (18).
B.2 Proofs of Claims 4.7 and 4.8
Claim 4.7. Let ϵ ∈ (0,ϵ ) and γ ∈ (0,γ ) for small constants ϵ ∈ (0,1/2), γ ∈ (0,1). Let sparsity
0 0 0 0
k ∈ N. Let C be a large enough constant and T be an ϵ-corrupted set S where S is (Cϵ,γ,k′,η)-
pca-stable with respect to an unknown k-sparse unit vector v ∈ Rd, η ∈ (0,1), γ ≥ ϵ, and k′ = C′k2
for a large enough constant C′ > 0. There is a randomized algorithm A that takes as input the
corrupted set T, contamination rate ϵ, sparsity k ∈ N, and a parameter q ∈ N, and returns a set
T′ ⊂ T and H ⊂ [d] in time O(dpoly(k,|T|)) such that with probability 0.9
1
1. T′ is an O(ϵ)-contamination of S.
2. Each diagonal entry of Σ ∈ [1/2,4].
T′
3. T′ and H satisfy Condition 4.6, i.e., |H | ≲ k2 and ∥diag(Σ −I ) ∥ ≲ γ.
1 1 T′ d H∁ Fr,k2
1
Proof. WewillfilterthesetusingLemma4.5followingthetemplateofAlgorithm1untilthesecond
and the third conditions are met.
Starting with the second condition, the lower bound on the diagonal entries follows by the fact
that T contains an ϵ-fraction of S∩T and diagonal entries of Σ are at least 1−γ. Since ϵ and
S∩T
γ are small enough, (1−ϵ)(1−γ) ≥ 1/2. We now focus on establishing the upper bound, for each
coordinate i ∈ [d], the true variance is at most (1+η∥v∥2 ) ≤ 2, where ∥·∥ denotes the ℓ norm.
∞ ∞ ∞
32Thus, if for any coordinate i ∈ [d], the empirical variance of T is larger than 3, which is bigger than
1+η +λ, H := {i} satisfies the condition of Lemma 4.5. Thus, we can filter points until all the
empirical variances are less than 3 following the template of Theorem 2.5.
We now turn our attention to the third condition. For a vector x ∈ Rd, let the function
g : Rd → Rd be the coordinate-wise square of its input. Thus, for X ∼ N(0,I+ηvv⊤), we have
that µ := E[g(X)] = u+ηg(v), where u ∈ Rd denotes the all ones vector. Therefore, the required
(cid:16) (cid:17)
condition in Condition 4.6 can also be written equivalently as ∥ µ −u ∥ = O(γ). Let
f(T) H∁ 2,k2
1
f(T) and f(S) denote the sets transformed by f, i.e., f(T) := {f(x) : x ∈ T} and f(S) := {f(x) :
x ∈ S}. We first compute µ −u, which takes O(d|T|) time. Let J ⊂ [d] denote the coordinates
f(T)
i for which the i-th coordinate of µ −u is bigger than γ/k in absolute value. Let J denote the
f(T) ∗
support of the sparse spike vector v.
Consider the case when J is a large enough set: |J| > k′. Then let H ⊂ J be any subset of size
k′. Then |H \J | > |H|−|J | = k′−k ≥ k′/2, And thus we have that
∗ ∗
∥(Σ −I −ηvv⊤) ∥ ≥ ∥(Σ −I −ηvv⊤) ∥ ≥
T d H Fr,k′2 T d H\J∗ Fr,k′2
≥ ∥(Σ −I ) ∥
T d H\J∗ Fr,k′2
(cid:16) (cid:17)
≥ ∥diag (Σ −I ) ∥
T d H\J∗ Fr,k′2
q
≥ (γ/k) (k′/2) ≳ γ,
since k′ ≳ k2. Thus, we have obtained a set H that satisfies the guarantees of Lemma 4.5, which
allows us to filter using the template of Algorithm 1.
If, on the other hand, J happens to be small, then we return H = J, and since µ is
1 f(T)
∁
(γ/k)-close to u (that is, all-ones vector) on H , we obtain
1
(cid:16) (cid:17)
∥diag(Σ −I ) ∥ = ∥ µ −u ∥ ≤ kγ/k ≤ γ.
T d H 1∁ Fr,k2 f(T) H 1∁ 2,k2
Claim 4.8. Let C be a large enough constant C > 0 and k,k′ ∈ N. Let T′′ ⊂ T′ be two O(ϵ)-
contamination of S such that S is an (Cϵ,γ,k′,η)-pca-stable with respect to v and η ∈ (0,1). Let
H ⊂ [d] be a small subset |H| ≤ k such that ∥diag(Σ − I ) ∥ ≲ γ. Then ∥diag(Σ −
T′ d H∁ Fr,k2 T′′
1
I ) ∥ ≲ γ.
d H∁ op,k
1
Proof. Our proof strategy will be similar to that of Claim 2.9, and we refer the reader to the proof
of Claim 2.9 for more details. Starting with the lower bound on the sparse eigenvalues of Σ , we
T′′
note that for any k′-sparse unit vector u, the stability condition applied to S ∩T′′ implies
|S ∩T′′|
u⊤Σ u ≥ u⊤Σ u ≥ (1−O(ϵ))(u⊤(I +ηvv⊤)u−γ) ≥ (1−O(ϵ))(1−O(γ)) ≥ 1−O(γ)
T′′
|T′′|
S∩T′′
sinceϵ ≤ γ. Applyingthisinequalityfor1-sparseunitvectorsu,weobtainthat−u⊤diag(Σ −I)u ≤
T
Cγ for a large constant C > 0. Turning towards the upper bound, we proceed as follows: for any
∁
1-sparse unit vector u supported on H ,
1
u⊤(diag(Σ )) u = u⊤Σ u = 1 X (u⊤x)2 ≤ (1+O(ϵ))u⊤Σ u ≤ (1+O(ϵ))(1+O(γ)) = 1+O(γ).
T′′ H∁ T′′ |T′′| T′
1
x∈T′′
(cid:13) (cid:16) (cid:17)(cid:13)
Overall, we obtain the desired guarantee of (cid:13)diag (Σ −I) (cid:13) = O(γ).
(cid:13) T′′ H∁ (cid:13)
1 op,k
33