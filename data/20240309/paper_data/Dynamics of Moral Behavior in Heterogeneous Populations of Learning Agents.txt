Dynamics of Moral Behavior in Heterogeneous
Populations of Learning Agents
Elizaveta Tennant Stephen Hailes Mirco Musolesi
l.karmannaya.16@ucl.ac.uk s.hailes@ucl.ac.uk m.musolesi@ucl.ac.uk
Dept. of Computer Science Dept. of Computer Science Dept. of Computer Science
University College London University College London University College London
Abstract
Growing concerns about safety and alignment of AI systems highlight the impor-
tance of embedding moral capabilities in artificial agents. A promising solution is
the use of learning from experience, i.e., Reinforcement Learning. In multi-agent
(social) environments, complex population-level phenomena may emerge from in-
teractions between individual learning agents. Many of the existing studies rely
on simulatedsocial dilemmaenvironmentsto study theinteractions of independent
learningagents. However, theytendtoignorethemoralheterogeneitythatislikely
to be present in societies of agents in practice. For example, at different points
in time a single learning agent may face opponents who are consequentialist (i.e.,
caring about maximizing some outcome over time) or norm-based (i.e., focusing
on conforming to a specific norm here and now). The extent to which agents’ co-
development may be impacted by such moral heterogeneity in populations is not
well understood. In this paper, we present a study of the learning dynamics of
morally heterogeneous populations interacting in a social dilemma setting. Using a
Prisoner’sDilemmaenvironmentwithapartnerselectionmechanism,weinvestigate
the extent to which the prevalence of diverse moral agents in populations affects
individual agents’ learning behaviors and emergent population-level outcomes. We
observe several types of non-trivial interactions between pro-social and anti-social
agents, and find that certain classes of moral agents are able to steer selfish agents
towards more cooperative behavior.
1 Introduction
GrowingconcernsaboutthesafetyandalignmentofArtificialIntelligence(AI)systemshighlightthe
importance of embedding pro-social capabilities into artificial agents. Without these, agents might
potentially behave harmfully in multi-agent situations such as social dilemmas (e.g., see Leibo et al.
2017). In particular, in this work, we focus on modeling the development of moral decision-making.
In general, it has been shown that morality can be developed in agents through learning from expe-
rience(Tennantetal.,2023b). Moralprinciplescanrangefromconsequentialist(Bentham,1780)to
norm-based morality (Kant, 1785), and from entirely pro-social to entirely anti-social preferences.
Existing works have proposed a variety of individual models to predispose agents to specific moral
principles (e.g., Bazzan et al. 1999; Capraro & Perc 2021), and recent studies have demonstrated
that these can be effectively encoded in Reinforcement Learning (RL) agents via intrinsic rewards
(Hughes et al., 2018; McKee et al., 2020; Tennant et al., 2023a).
However, to date we still have limited understanding of how different types of morality may co-
evolve in heterogeneous populations. In fact, many real-world AI systems are likely to co-exist
(essentiallyformingsystemsofsystems)andmaybeco-developedinparallelwithothers. Especially
when it comes to integrating moral decision-making into systems, different stakeholders may decide
1
4202
raM
7
]AM.sc[
1v20240.3042:viXrato prioritize varying principles or preferences. It is therefore crucial that we start developing an
understanding of how the presence of different moral preferences in populations affects individual
agents’ learning behaviors and interactions, and, indeed, emergent population-level outcomes.
Thistypeofagent-basedanalysisofmorallearningmayadditionallyofferinsightsintothepotential
dynamicsofinteractionsinhumansocietiesinthetraditionofcomputationalphilosophy(McKenzie,
2007), provide an experimental test-bed for new theories (Mayo-Wilson & Zollman, 2021), or simu-
late potential evolutionary mechanisms underpinning human moral preferences, in a similar way to
Evolutionary Game Theory (Hofbauer & Sigmund, 1998; Sigmund & Nowak, 1999; Sigmund, 2010).
The core contribution of this work is the study of the behavior and population dynamics among RL
agents with diverse moral preferences, providing insights for artificial agent design also in terms of
safety1. More generally, we present a methodology for analyzing emergent behavior in populations of
agents with heterogeneous moral orientation.
2 Background
2.1 Social Dilemma Games
Social dilemma games simulate social situations in which players obtain dif-
ferent utilities (payoffs) from choosing one action or another, and the struc-
ture of these utilities is such that each player faces a trade-off between indi- IPD C D
C 3, 3 0, 4
vidual interest and societal benefit when choosing an action (Dawes, 1980).
D 4, 0 1, 1
The most widely studied type is a symmetric matrix game with two players
andtwoabstractactions-CooperateorDefect. Playersinthesegamesmust
decide on their respective actions simultaneously, without communicating. Table 1: Payoffs
on one step of the
A classic game from Economics and Philosophy, which is relevant to moral
Iterated Prisoner’s
choice, is the Prisoner’s Dilemma (Rapoport 1974, see payoffs for row vs
Dilemma (IPD).
column player in Table 1). We implement the Iterated Prisoner’s Dilemma
(IPD)inapopulation,inwhichagentsinteractinpairsindiscretetimesteps
and aim to maximize their cumulative payoff over time. In the iterated game, players can take
actions to punish their opponents for past defection or to influence their future behaviors.
Deriving predicted equilibria for these games is not always feasible; indeed, since these are systems
composed of interacting (heterogeneous) entities, simulation is needed to study potential emergent
behaviors andoutcomes (Anderson, 1972). Additionally, learning dynamics may cause instability in
the environment and dynamic behaviors (Busoniu et al., 2008).
2.2 Deep Reinforcement Learning in Markov Games
ReinforcementLearning(RL)isawell-suitedtechniqueformodelingagentsthatlearnbyinteracting
with others in an environment (Sutton & Barto, 2018). It can be applied in conjunction with
EvolutionaryGameTheory(Hofbauer&Sigmund,1998)toiteratedsocialdilemmagames(Littman,
1994; Sandholm & Crites, 1996; de Cote et al., 2006; Abel et al., 2016), in which game payoffs
constitute extrinsic rewards, and traits such as moral or social preferences (Fehr & Fischbacher,
2002) can be encoded in the agent’s intrinsic reward (Chentanez et al., 2004). In the following
section, we discuss the design of intrinsic moral rewards in detail. It is also worth noting that we
assumepopulationsofindependent,continuouslylearningagents. Thiscreatesinterestingdynamics
as agents affect one another’s learning process (Leibo et al., 2019). Given the potentially large
number of states, we adopt DQN as the underlying learning algorithm (Mnih et al., 2015).
1Code: willbemadeavailableuponacceptance
22.3 Morality as Intrinsic Reward
Traditional social dilemma scenarios assume that agents are only motivated by accumulating game
payoffs (i.e., extrinsic reward). However, human data has shown that other preferences may also
come into play, such as the predisposition to cooperation (Camerer, 2011). In artificial agents,
preferences other than game rewards can be encoded in intrinsic rewards (Chentanez et al., 2004).
As a result, recent multi-agent RL studies have become increasingly interested in modeling various
pro-social preferences as intrinsic reward functions for social dilemma players (Hughes et al., 2018;
Peysakhovich&Lerer,2018;McKeeetal.,2020). Moralrewardsinparticularhavebeenstudiedfor
two-agent (i.e., dyadic) interactions in Tennant et al. (2023a).
We anchor our intrinsic reward definitions in traditional moral philosophical frameworks, especially
relying on the distinction between consequentialist versus norm-based morality, and the idea of
virtue ethics. Consequentialist morality focuses on the consequences of an action, and includes
Utilitarianism (Bentham, 1780), which defines actions as moral if they maximize total utility for all
agents in a society. Norm-based morality, including Deontological ethics (Kant, 1785), considers an
actmoralifitdoesnotcontradictthesociety’sexternalnorms. Finally,inVirtue Ethics (Aristotle),
moral agents must act in line with their certain internal virtues, such as fairness or care for others
(Graham et al., 2013). Different virtues can matter more or less to different agents (Aristotle;
Graham et al., 2009) and can themselves have consequentialist or norm-based foundations. We
present the set of agents considered in this study and their classification in Section 3.2.
2.4 Partner Selection
Inhumanpopulations,agentshaveachoiceofwhichindividualtointeractwith. Giventhisselection
mechanism, reputation comes into play and competitive and collaborative relationships may form.
Santos et al. (2008) show experimentally that these dynamics are likely to lead to a re-structuring
ofthepopulation. Adaptivebehaviorresultingfromselectionmechanismshasbeenhypothesizedto
drive the emergence of cooperation (Barclay & Willer, 2007; Cuesta et al., 2015). Partner selection
mechanismsandemergenceofcooperationhaverecentlybeenstudiedinsocietiesofartificiallearning
agents as well (Anastassacos et al., 2020; Baker, 2020).
In particular, applying partner selection to the IPD, a conflict may arise between a player’s motiva-
tiontoappearcooperativeinordertogetselectedmoreoftenbyotherpayers,andthemotivationto
select and then exploit cooperators to gain a greater payoff. Anastassacos et al. (2020) find partner
selection to be norm-inducing and even lead to the emergence of cooperation in a population of
purely selfish agents. We adopt a partner selection model similar to Anastassacos et al. (2020).
Selection dynamics in morally heterogeneous populations are likely to be more complex and harder
topredict. Sinceeachplayerplaysaccordingtotheirownintrinsicrewardsignal,differentcoalitions
(Shenoy,1979) mayarise amongdifferentagenttypeswithina population, andpopularity ofagents
may not directly correlate with cooperativeness. Running simulation experiments to study this
may provide insight into the types of behaviors and outcomes that may develop in heterogeneous
populations with selection, with implications for the study of human and animal societies and
Evolutionary Game Theory (Sigmund & Nowak, 1999).
2.5 Learning in Heterogeneous Populations
Evidencefromthesocialsciencessuggeststhathumansocietiesaremorallyheterogeneous(Graham
et al., 2013; Bentahila et al., 2021). While certain commonsense norms may be agreed upon by a
society(Reid&Haakonssen,1990;Gert,2004),distinctmoralprinciplesarelikelytoholdadifferent
weight for different individuals depending on factors such as political orientation (Graham et al.,
2009),personality(Lifton,1985),nurtureorevennaturalpredispositions(Sinnott-Armstrong,2008).
Another social dilemma study with diverse consequentialist agents (i.e., agents with various pref-
erences over group outcome distributions, McKee et al. 2020) suggests that agents trained in het-
3erogeneous populations develop particularly generalized and high-performing policies. In contrast,
populations containing only altruistic agents are characterized by greater collective reward, but at
the cost of equality between agents. In McKee et al. (2022), the authors define population diversity
asthesetofvariousopponents’policiesthatanagentmayface,andalsofindthattrainingindiverse
populationscanleadtoimprovementsintermsofagentperformanceonsometypesofenvironments.
Even without learning, Santos et al. (2008) show that - in fixed networks of agents - diversity in
terms of neighborhoods within the population graph promotes cooperation in the population. With
respect to global societal outcomes, Ord (2015) discusses how, through interaction, morally diverse
agents taking actions to satisfy their distinct moral goals may improve the global welfare of the
whole population, akin to a ‘trade’. We believe that these conceptual frameworks can represent
the basis for studying how artificial agents may learn to act across populations containing different
proportions of opponents with different moral preferences.
3 Methodology
3.1 Experimental Setup
Our environment involves a population P (of size N = 16) playing an iterated game. At each
iteration, a moral player M and an opponent O play a one-shot Prisoner’s Dilemma game with two
possible actions - Cooperate or Defect (at ,at ∈ {C,D}, see Table 1 for the respective payoffs).
M O
To allow learning in a population, we run our simulation in 30000 episodes, where a single episode
involves each player selecting a partner once, and then each corresponding pair of players playing
the game. In particular, at iteration t, the learning agent M observes a selection state including
the action played by each possible opponent O ∈ P at t−1: st = [at−1,...,at−1]. Using this
state and a policy, M selects an opponent O. Then, to chooseM an,se al ctionO fo1r the dO inlemma, player
M relies on the state representing the latest move of their selected opponent O: st = at−1.
M,dil O
Simultaneously, their opponent O chooses an action following the same principle. The players M
and O then each receive a game reward Rt+1 and Rt+1 (corresponding to the game payoffs) and
M O
observe a new state st+1 and st+1 based on their opponent’s move at.
M,dil O,dil
In this Markov game (Littman, 1994), over time both players learn to make selections and take
actionsbyobservingpastinteractionswithvariousopponents. EachagentusesQ-Learning(Watkins
& Dayan, 1992) to update the state-action value function Q (i.e., an estimate of the cumulative
expected return over time). Given a large number of possible selection states, we approximate Q
usinganeuralnetwork;eachagentmaintainstwointernalmodels(oneforpartnerselection,andone
for playing the dilemma) - for each of these we use a fully connected network with a single hidden
layer of size 265.
For learning both selections and the dilemma, players record the experiences (s,a,r,s′) in their
memory buffer D and D . We copy the reward obtained from the game into both the dilemma
sel dil
and selection memories. At the end of an episode, all players simultaneously update the Q-network
parameters θ for their two models using the latest experience available in the memory buffer D,
t
using the Mean Squared Error loss function (Mnih et al., 2015) where γ =0.99 is a discount factor:
(cid:20)(cid:18) (cid:19)2(cid:21)
L (θ )=E Rt+1+γmaxQ(st+1,a,θ )−Q(st,at,θ )
t t s,a,r,s′ t t
a
If more than one experience is available (i.e., a player plays more than one dilemma game in that
episode), we calculate an average loss across the experiences before updating the network. Each
memory buffer D is refreshed before the start of the next episode, so that each player only learns
from the latest episode of experience. In updating the network weights with the Adam optimizer
(Kingma & Ba, 2015), we use a learning rate of 0.001.
4Agent Label & Moral Type Moral Reward Function Source of Morality
S Selfish None (use Rt to learn) None
Mextr
Ut Utilitarian Rt =Rt +Rt External/Internal conseq.
Mintr (cid:26)M –ξe ,xtr
if
atO =ext Dr
,at−1 =C
De Deontological Rt = M O External norm
Mintr 0, otherwise
V-Eq Virtue-Equality Rt =1− |R Mt extr−R Ot extr| Internal consequentialist
Mintr Rt +Rt
(cid:26)
ξ,
iM fe ax ttr =O Cextr
V-Ki Virtue-Kindness Rt = M Internal norm
Mintr 0, otherwise
aUt Anti-Utilitarian Rt =−(Rt +Rt ) External/Internal conseq.
Mintr (cid:26) ξ, M ie fxt ar t =O De ,x atr t−1 =C
mDe Malicious Deontological Rt = M O External norm
Mintr 0, otherwise
V-In Virtue-Inequality Rt = |R Mt extr−R Ot extr| Internal consequentialist
Mintr Rt +Rt
(cid:26) ξM ,ext ir
f
atOe =xtr
D
V-Ag Virtue-Aggression Rt = M Internal norm
Mintr 0, otherwise
Table 2: Definitions of the types of intrinsic moral rewards, from the point of view of the moral
agent M playing versus an opponent O at iteration t. We define four pro-socially-disposed players,
four anti-social ones, and the traditional Selfish player.
Agents select partners and play the dilemma using an ϵ-greedy policy, acting randomly with proba-
bility ϵ and otherwise playing greedily according to the Q-values learned so far:
(
argmax Q(s ,a), with probability 1−ϵ
π(s )= a t
t U(A={C,D}), with probability ϵ.
We set ϵ =0.1 and ϵ =0.05. The full detailed algorithm can be found in Appendix A.
sel dil
Player M can learn according to an extrinsic game reward Rt+1 , which depends directly on the
Mextr
joint actions at ,at (as defined in Table 1), or accordingto an intrinsic reward Rt+1 . In thenext
subsection,
weM discO
uss the definition of these intrinsic moral rewards.
Mintr
3.2 Modeling Morality as Intrinsic Reward
Intrinsic rewards associated with pro-social preferences have been used to incentivize the emergence
of cooperation in social dilemmas (Hughes et al., 2018; Peysakhovich & Lerer, 2018; Jaques et al.,
2019). Tennant et al. (2023a) extended this work by defining four Q-learning moral2 agents that
learnaccordingtovariousintrinsicrewardsR ,andcontrastingtheseagainstatraditionalSelfish
Mintr
agent which learns to maximize its extrinsic (game) reward R (e.g., Leibo et al. 2017). The
Mextr
formal definitions of the reward functions are presented in Table 2.
In this study, we rely on eight types of moral agents - four pro-social moral learners from Ten-
nant et al. (2023a) (Utilitarian, Deontological, Virtue-Equality, Virtue-Kindness), and four addi-
tional anti-social counterparts (anti-Utilitarian, malicious-Deontological, Virtue-Inequality, Virtue-
Aggression):
• theUtilitarianagenttriestomaximizethecollectivepayoff(i.e.,totalpayoffforbothplayers;
Bentham 1780);
2We use the term moral for agents that are not selfish. However, selfishness itself can be considered as a moral
choice, expressed as rational egotism. In the case of social dilemmas, selfishness maps to the concept of rationality.
FortheseagentsRMintr =RMextr.
5
laicoS-orP
laicoS-itnA• theDeontological agenttriestofollowtheconditionalcooperationnorm(Kant,1785;Fehr&
Fischbacher, 2004) and gets penalized through the negative reward −ξ for defectingagainst
a cooperator (i.e., an opponent who previously cooperated);
• theVirtue-Equality agenttriestomaximizeequalitybetweenthetwoplayers’payoffsRt
and Rt , measured using a two-agent variation of the Gini coefficient (Gini,
1912);Mextr
Oextr
• the Virtue-Kindness agent receives a reward ξ for acting kindly (i.e., cooperating) against
any opponent (Aristotle);
• the anti-Utilitarian agent tries to minimize the collective payoff;
• themalicious-Deontological agenttriestofollowtheconditionaldefectionnormandreceives
a reward ξ for defecting against an cooperator;
• the Virtue-Inequality agent tries to minimize equality between the two payoffs;
• the Virtue-Aggression agent receives a reward ξ for acting aggressively (i.e., defecting)
against any opponent.
Agent types Ut and aUt can be defined as external consequentialist since their reward depends on
consequences in the environment (i.e., the players’ rewards). The De and mDe agents depend on an
external reputation-based norm defined in terms of current actions. V-Eq and V-In agents can be
considered internal consequentialist, since they follow a consequence-based internal virtue. Finally,
V-Ki and V-Ag’s pre-dispositions originate from the agent’s internal norm.
The rewards in Table 2 are defined for a single iteration t. Given the payoff matrices of the IPD
game used (see Table 1), we set the parameter ξ in the four norm-based rewards to be ξ = 5, so
it sends a strong signal of a value similar to the maximum game payoff available. Through further
experiments,wealsoobservethatthechoiceofsmallervaluesofξ doesnotaffecttheoverallresults.
3.3 Population Compositions
We compare a set of heterogeneous populations based on the following principles: population size
is equal to 16; all populations contain at least one player of each type; one player type constitutes
the majority (i.e., 8 out of 16 players). Given 9 possible player types, this results in 9 possible
population compositions, as outlined in Table 3.
Population Label Population Composition
majority-S 8xS, 1xUt, 1xaUt, 1xDe, 1xmDe, 1xV-Eq, 1xV-In, 1xV-Ki, 1xV-Ag
majority-Ut 1xS, 8xUt, 1xaUt, 1xDe, 1xmDe, 1xV-Eq, 1xV-In, 1xV-Ki, 1xV-Ag
majority-aUt 1xS, 1xUt, 8xaUt, 1xDe, 1xmDe, 1xV-Eq, 1xV-In, 1xV-Ki, 1xV-Ag
majority-De 1xS, 1xUt, 1xaUt, 8xDe, 1xmDe, 1xV-Eq, 1xV-In, 1xV-Ki, 1xV-Ag
majority-mDe 1xS, 1xUt, 1xaUt, 1xDe, 8xmDe, 1xV-Eq, 1xV-In, 1xV-Ki, 1xV-Ag
majority-V-Eq 1xS, 1xUt, 1xaUt, 1xDe, 1xmDe, 8xV-Eq, 1xV-In, 1xV-Ki, 1xV-Ag
majority-V-In 1xS, 1xUt, 1xaUt, 1xDe, 1xmDe, 1xV-Eq, 8xV-In, 1xV-Ki, 1xV-Ag
majority-V-Ki 1xS, 1xUt, 1xaUt, 1xDe, 1xmDe, 1xV-Eq, 1xV-In, 8xV-Ki, 1xV-Ag
majority-V-Ag 1xS, 1xUt, 1xaUt, 1xDe, 1xmDe, 1xV-Eq, 1xV-In, 1xV-Ki, 8xV-Ag
Table3: Populationsconsideredinthisstudy. Eachpopulationcontainseightplayersofa‘majority’
type and one player of each other type.
4 Results
Weseparatelyanalyzepopulation-leveloutcomes(Section4.1),selectiondynamics(Section4.2)and
behaviors of individual player types (Section 4.3) across the nine populations.
6(a) Cooperation in entire population (b) Cooperation by Selfish player
Figure 1: (a) Population-level cooperation over time. (b) Behavior of the Selfish player(s) across
every population. In these charts, we plot the moving average of the mean across 20 runs.
4.1 Population-Level Behaviors and Outcomes
We now study whether a stronger prevalence of certain agent types in every population achieves
higher levels of cooperation, and what social outcomes emerge over time.
4.1.1 Level of Cooperation
CooperationovertimeforeachpopulationispresentedinFigure1a. Resultsshowthatthegreatest
level of cooperation is achieved by the majority-Ut and majority-V-Ki populations, with around
70% of the players cooperating by the end. In the majority-V-Ki population, this high level of
cooperation develops early on but then drops to about 60% around episode 9000, which is then
followed by a climb back to 70% soon after, and a stabilization around episode 13000. An analysis
of behavior by each player type (see Appendix D) shows that V-Ki players in particular go through
an initial period of increasing defection - this coincides with an instability in the whole population
where defection is still preferable because it increases the probability of being selected by others.
Eventually, many of the players learn to avoid defectors and the V-Ki players learn their optimal
policy of nearly-full cooperation.
Inthemajority-Ut population,cooperationemergesmoreslowly,remainingaround60%forthefirst
10000 episodes, but then stabilizes without further drops after episode 13000 (at a similar point in
time as the majority-V-Ki population). Specifically, Ut players take longer than V-Ki to learn to
cooperate (see Appendix D), but once cooperation levels increase to around 70%, it remains stable.
In the majority-De population, cooperation drops quickly in the first 1000 episodes and then stabi-
lizes at 60% - lower than for majority-Ut and majority-V-Ki populations. Both the majority-V-Eq
and the majority-V-In populations achieve similar levels of cooperation, though the former displays
muchmoreinstability. Theleastamountofcooperationisobservedinthemajority-aUt population.
4.1.2 Social Outcomes
We adopt standard social outcome metrics used in the existing literature (Hughes et al., 2018;
Tennant et al., 2023a) also for comparison purposes; we measure collective reward (R ),
collective
equality between payoffs (R ) and the lowest payoff obtained (R ) for a pair of agents {M,O}
gini min
that play on iteration t.
7To aggregate values from every iteration t within an episode (where the length of the episode is
equal to the population size N =16), we sum R and average R and R per episode as
collective gini min
follows:
XN
R = (Rt +Rt )
collective
t=1
Mextr Oextr
PN (1− |R Mt extr−R Ot extr|)
t=1 Rt +Rt
R = Mextr Oextr
gini N
PN min(Rt ,Rt )
R = t=1 Mextr Oextr .
min N
(a) Collective Reward (b) Gini Reward (c) Minimum Reward
Figure 2: Population-level social outcomes: Collective Reward, Gini Reward and Min Reward.
In Figure 2 we present the outcomes over time in each population. Collective reward follows a
similar pattern to population-level cooperation (Figure 1a), since these are interconnected in the
IPD. Equality between the players (i.e., Gini reward) is highest for the equality-focused majority-
V-Eq population - though the absolute mean value per episode does not go above 0.7, meaning that
unequaloutcomesremainprevalenteveninthispopulation. Interestingly,ananalysisofactionpairs
(see Appendix C) shows that this level of equality is achieved in this population largely through
mutual defection. Equality isgenerallyhigh forpro-socialpopulations majority-Ut and majority-V-
Ki duetomutualcooperation,andfortheiranti-socialcounterpartsmajority-aUt andmajority-V-Ag
due to mutual defection. Equality is lower when the majority of players are of the S, De, mDe or
V-Ie types,sinceinthesescenariosoneplayertendstoexploittheother. Minimumrewardishighest
onceagaininthemajority-Ut andthemajority-V-Ki populations(withvaluesaround1.5),andalso
in the majority-V-Eq case. All other populations result in lower minimum reward of values between
0.5and1.0,suggestingeitherexploitationofmutualdefectionintheIPD(assupportedbythepairs
of actions analyzed in Appendix C).
4.2 Selection Dynamics
Given the selection mechanism implemented, it is interesting to analyze which players are the most
popular overall in each population, and which selector types prefer which types of partners. In a
population of purely selfish agents (Anastassacos et al., 2020), the selection mechanism adds an
incentive to cooperate, since selfish players benefit from playing against cooperators and tend to
select them. In our heterogeneous populations, however, certain players may not necessarily prefer
cooperative opponents, so selection dynamics are likely to be more complex.
Figure3ashowsthepopularityofeachplayertypeineachpopulationoverthefinal100episodes(for
errorbars, seeAppendixB,Figure5). Generally, inprobabilisticterms, especiallyatthebeginning,
the majority player type tends to be the most popular in their respective population. However, we
observe that in certain populations, the majority player type is selected less than 50% of the time -
8Selection dynamics across players
Selection dynamics across players
in population majority-S in population majority-De
Popul a (%rit y ti mof e e sa sc eh l ep cla tey de )r type 100 (ov e(# r eti pm ise os d s ee sl e 1c -t 3e 0d 0) 00) (ov e(# r eti pm ise os d s ee sl e 1c -t 3e 0d 0) 00) 10000
(over last 100 episodes) 0_S 0_S
S 1_S 8000 1_Ut
Ut 80 2 3_ _S S 2 3_ _D De e 8000 De 4_S 4_De
V-Eq 60 5 6_ _S S 6000 5 6_ _D De e 6000
V-Ki 7_S 7_De
ma DU et 40 10_9 V8 __ -D EU e qt 4000 10_8 9 V_ _ -D D Ee e q 4000
VV -A-I gn 20 1 11 2_ _V a- UK ti 1 11 2_ _V a- UK ti
1 13 4_ _m VD -Ie n 2000 1 13 4_ _m VD -Ie n 2000
0 15_V-Ag 15_V-Ag
0 0
Population Player 2 (selectED) Player 2 (selectED)
(a) Popularity of player types (b) Selections within (c) Selections within
in each population majority-S population majority-De population
Figure 3: (a) Popularity of player types in each population on the final 100 episodes. We sum the
numbers for cells where more than one player of the same type is present (e.g. 8xS players in the
majority-S population). (b)&(c) Selection patterns across player types, with number of selections
summed over all 30000 episodes, for two example populations; for all populations, see Appendix E.
We average across 20 runs.
i.e., much less frequently than expected due to their prevalence. In these cases, it is interesting to
analyze which non-majority players emerge as popular alternatives.
The behavior of the majority-De population is the most striking. Here, aUt and V-Ag players are
relativelypopular. Afurtheranalysisofthespecificselectionpatternsamongallplayertypes(Figure
3c) reveals that the De players in particular tend to mostly prefer anti-social opponents aUt and V-
Ag. Thisapparentlyself-sabotagingbehaviorcanbeexplainedbyacloseranalysisoftheDe player’s
R -theyarepenalizedfordefectingagainstacooperator,sothesafestbehaviorinaheterogeneous
intr
populationistoalwaysselectadefector,sothattheyhavethelowestchanceofviolatingtheirmoral
norm. Figure3calso showsthat anotherplayer - aUt - prefers toavoidthe De majorityplayersand
insteadtendstoselecttheV-Ag opponent. Thishighlightsthatminimizingcollectivereward(which
is the preference embedded in the aUt agent’s intrinsic reward) is best achieved by avoiding the
very cooperative De agent. Thus, we find an interesting dynamic in the heterogeneous majority-De
population, in which the majority of the players follow a strong cooperative norm, demonstrating
ways in which these De players can be very cooperative (see Figure 1a) but not very popular.
Returning to the analysis of player popularity on the final 100 episodes (Figure 3a), the majority-
mDe population also has a few non-majority players emerging as popular. In this case, it is the De
and V-Ki players that get selected often by others. Thus, in both majority-De and majority-mDe
populations,thenorm-followingmajorityplayerisnotevenpopularamongtheirownkind. (Forthe
full analysis of selections across player types in all populations, see Appendix E.)
Finally, the majority-S population is also interesting to analyze (Figure 3b). S players often select
De orV-Ki opponents,sincethesearesoeasilyexploitable,butUt andV-In alsoemergeaspopular,
likely because these players will choose to cooperate against a defector, which benefits the S player
in terms of reward. Many of the other players tend to select majority type S opponents roughly
as often as other player types, meaning that Selfish players do not emerge as unpopular, despite
their often defective behavior. The only exceptions to this are mDe and V-Ag. The mDe player in
particular learns to avoid the S opponent since mDe would rather face a cooperator (such as De or
V-Ki) and obtain positive R than select the S players which are likely to defect.
intr
9
epyT
reyalP
S-ytirojam tU-ytirojam eD-ytirojam qE-V-ytirojam iK-V-ytirojam tUa-ytirojam eDm-ytirojam nI-V-ytirojam gA-V-ytirojam
)ROtceles(
1 reyalP
S_0 S_1 S_2 S_3 S_4 S_5 S_6 S_7 tU_8 eD_9 qE-V_01 iK-V_11 tUa_21 eDm_31 nI-V_41 gA-V_51
)ROtceles(
1 reyalP
S_0 tU_1 eD_2 eD_3 eD_4 eD_5 eD_6 eD_7 eD_8 eD_9 qE-V_01 iK-V_11 tUa_21 eDm_31 nI-V_41 gA-V_514.3 Individual-Level Behaviors and Outcomes
Finally, we analyze outcomes (i.e., cumulative reward obtained) for each individual player type,
and the cooperative behavior of S learners across populations. An analysis of cooperative behavior
exhibited by each non-S player type across populations is available in Appendix D.
4.3.1 Cumulative Reward
Game Rewards Intrinsic Rewards
(accumulated over all episodes) (accumulated over all episodes)
min-max normalized 1.0
S Ut
Ut 300000 De 0.8
De V-Eq
V-Eq 0.6
200000 V-Ki
V-Ki aUt
aUt 0.4
mDe
mDe 100000
V-In
V-In 0.2
V-Ag
V-Ag
0 0.0
Population Population
(a) Game Rewards (b) Intrinsic Rewards
Figure 4: Game reward and intrinsic reward accumulated by each player type in each population
over the entire 30000 episodes (averaged across 20 runs). We average per number of players of
a certain type (e.g. 8xS players in the majority-S population). For comparability, we normalize
intrinsic rewards using the minimum and maximum observed value for each player type.
Figure 4 presents the reward accumulated by each player type in each population over all 30000
episodes. The results in terms of game reward clearly show that - in general - pro-social players are
disadvantagedontheIPDgameitself,sincetheygetexploitedand/ordonotbenefitfromexploiting
a cooperative opponent. Furthermore, the lowest-performing player type is De. However, the De
player obtains high levels of intrinsic reward in many of the populations, especially when their own
kind are the majority. Other pro-social players obtain the highest R in populations in which an
intr
anti-social player type constitutes the majority.
Anti-social or S players obtain higher game reward over time than their pro-social counterparts -
likely by exploiting others. The majority-De population allows aUt and V-Ag players to obtain
especially high levels of game reward - and given the selection dynamics analyzed above, we under-
stand that this is driven by the fact that De players actively prefer to select these opponents and
then get exploited by them. The lowest-performing players in terms of intrinsic reward in general
are mDe and V-Ag (especially in majority-anti-social populations) and majority-V-Ki (especially
in majority-pro-social populations). Players of the aUt type obtain high intrinsic reward in most
populations except majority-De (because the presence of such cooperative players makes it hard
to minimize collective reward), whereas the mDe and V-Ag obtain the highest intrinsic reward by
exploiting the very cooperative De players in the majority-De population.
4.3.2 Behavior of Selfish Learners
Finally,weanalyzethebehaviorofS playersineachpopulation. Theseplayersarethemostaligned
to traditional multi-agent learning literature, since they do not learn according to any intrinsic
reward, but rather by simply maximizing the game reward according to the IPD payoff matrix.
10
epyT
reyalP
S-ytirojam tU-ytirojam eD-ytirojam qE-V-ytirojam iK-V-ytirojam tUa-ytirojam eDm-ytirojam nI-V-ytirojam gA-V-ytirojam
epyT
reyalP
S-ytirojam tU-ytirojam eD-ytirojam qE-V-ytirojam iK-V-ytirojam tUa-ytirojam eDm-ytirojam nI-V-ytirojam gA-V-ytirojamThis analysis investigates which population compositions steer S learners into more cooperative
behaviors.
As shown in Figure 1b, S players generally learn a mostly-defective policy in most populations,
though cooperation is slightly above the 5% chance level even at the end of the simulation, likely
due to the presence of some players that prefer to select cooperators in each population. The
two cases where S players display most cooperation are the majority-V-Eq and the majority-Ut
populations, though in the latter cooperation still decreases to 10% towards the second half of the
training period. Thus, the majority-V-Eq population has a unique influence on the learning of the
S agent, and is able to steer this agent towards a greater level of cooperation for longer. The V-Eq
opponent is about 50% likely to cooperate (see Appendix D, Figure 7), which must drive the S
player to also learn the near-50% cooperative policy observed here.
5 Discussion
In this paper we have investigated how the presence of different moral preferences in populations
affects individual agents’ learning behaviors and emergent social outcomes. Many of the agents’
actionsareconsistentwiththeirrewarddefinitions-ingeneral,pro-socialagentsprefertocooperate
(exceptfortheV-Eq agent),andanti-socialagentsdisplaylowerlevelscooperationandobtainlower
collective reward.
In terms of temporal dynamics, our results demonstrate interesting asymmetries in the emergence
ofcooperationbyagentswithdifferentmoralpreferences. Inparticular, weobservethatconsequen-
tialist agents Ut take longer to learn to cooperate than the norm-based agents De, while the norm
based agents V-Ki go through an unstable and defective period before learning a stable cooperative
policy. Furthermore, for certain consequentialist agents (specifically, aUt vs Ut), the convergence
to an equilibrium is faster for anti-social players than their pro-social counterparts in the Prisoner’s
Dilemma environment - likely due to the payoffs attributed to defection.
We have also observed some surprising interactions between different types of morality due to the
selection mechanism present. First, the introduction of a large number of equality-focused agents
(V-Eq) has a positive effect on the cooperative behavior of a Selfish agent (S), providing insight
into how diverse opponent types can steer self-interested agents towards more pro-social behavior.
Second,onepro-socialplayertype(specifically,thenorm-basedplayerDe)learnstoselectanti-social
opponentsinordertoavoidviolatingtheirownmoralnorm. Manyanti-socialplayerssimultaneously
learn to select the De player and exploit them in the game, since this exploitative behavior is not
penalized by anyone in the population and does not deter the De player from further interaction.
Thus, our results show that the introduction of De type agents risks promoting anti-social actors
and inequality in a population. This illustrates the possibility that the presence of narrowly-defined
norms might lead to self-sabotaging behavior and cause negative outcomes for the population as a
whole.
6 Conclusion
This study is the first to analyze the dynamics of learning in populations of agents with moral pref-
erences varying from those with consequentialist foundations to those with norm-based ones. Our
results demonstrate the potential of using intrinsic rewards for modeling various moral preferences
inRLagents. Moregenerally,wehaveprovidedagenericmethodologyforstudyingthelearningdy-
namics of heterogeneous populations, and introduced measures for assessing individual and societal
outcomes. Finally, this work might also contribute to raising awareness about emergent behaviors
and the possibility of unintuitive outcomes in such multi-agent learning scenarios.
Our research agenda includes the study of more complex moral frameworks and an exploration
of multi-objective approaches that combine moral and self-interested motivation. We also plan to
investigate environments characterized by learning dynamics under partial observability.
11Broader Impact Statement
As part of our study, we have designed malicious learning agents, which are able to take advantage
of certain exploitable pro-social agents. Studying the effect of the presence of these agents might
increase our understanding of the resilience and stability of systems even in presence of malicious
actors.
References
David Abel, James MacGlashan, and Michael L. Littman. Reinforcement learning as a framework
for ethical decision making. In Proceedings of the AAAI Workshop: AI, Ethics, and Society
(AIES’16), pp. 54–61, 2016.
Nicolas Anastassacos, Stephen Hailes, and Mirco Musolesi. Partner selection for the emergence of
cooperationinmulti-agentsystemsusingreinforcementlearning. InProceedings of the 34th AAAI
Conference on Artificial Intelligence (AAAI’20), volume 34, pp. 7047–7054, 2020.
Philip W. Anderson. More is different. Science, 177(4047):393–396, 1972.
Aristotle. The Nicomachean Ethics. Oxford University Press.
Bowen Baker. Emergent reciprocity and team formation from randomized uncertain social pref-
erences. In Proceedings of the 34th International Conference on Neural Information Processing
Systems (NeurIPS’20), volume 33, pp. 15786–15799, 2020.
Pat Barclay and Robb Willer. Partner choice creates competitive altruism in humans. Proceedings
of the Royal Society B: Biological Sciences, 274(1610):749–753, 2007.
Ana L. C. Bazzan, Rafael H. Bordini, and John A. Campbell. Moral sentiments in multi-agent
systems. In Jörg P. Müller, Anand S. Rao, and Munindar P. Singh (eds.), Intelligent Agents V:
Agents Theories, Architectures, and Languages, pp. 113–131. Springer Berlin Heidelberg, 1999.
LinaBentahila,RogerFontaine,andValériePennequin. Universalityandculturaldiversityinmoral
reasoning and judgment. Frontiers in Psychology, 12, 2021.
Jeremy Bentham. An Introduction to the Principles of Morals and Legislation. Clarendon Press,
1780.
LucianBusoniu,RobertBabuska,andBartDeSchutter. Acomprehensivesurveyofmultiagentrein-
forcement learning. IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications
and Reviews), 38(2):156–172, 2008.
Colin F. Camerer. Behavioral Game Theory: Experiments in Strategic Interaction. Princeton Uni-
versity Press, 2011.
Valerio Capraro and Matjaž Perc. Mathematical foundations of moral preferences. Journal of The
Royal Society Interface, 18(175):20200880, 2021.
Nuttapong Chentanez, Andrew Barto, and Satinder Singh. Intrinsically motivated reinforcement
learning. In Proceedings of the 18th International Conference on Neural Information Processing
Systems (NeurIPS’04), volume 17, pp. 1281–1288, 2004.
Jose A. Cuesta, Carlos Gracia-Lázaro, Alfredo Ferrer, Yamir Moreno, and Angel Sánchez. Reputa-
tion drives cooperative behaviour and network formation in human groups. Scientific Reports, 5
(1):7843, 2015.
Robyn M. Dawes. Social dilemmas. Annual Review of Psychology, 31(1):169–193, 1980.
Enrique Munoz de Cote, Alessandro Lazaric, and Marcello Restelli. Learning to cooperate in multi-
agentsocialdilemmas. InProceedings of the 5th International Conference on Autonomous Agents
and Multiagent Systems (AAMAS’06), pp. 783–785, 2006.
12Ernst Fehr and Urs Fischbacher. Why social preferences matter – the impact of non-selfish motives
on competition, cooperation and incentives. The Economic Journal, 112(478):1–33, 2002.
ErnstFehrandUrsFischbacher. Socialnormsandhumancooperation. TrendsinCognitiveSciences,
8(4):185–190, 2004.
Bernard Gert. Common Morality: Deciding What to Do. Oxford University Press, New York, 2004.
Corrado Gini. Variabilità e Mutabilità: Contributo allo studio delle distribuzioni e delle relazioni
statistiche. [Fasc. I.]. Tipografia di P. Cuppini, 1912.
Jesse Graham, Jonathan Haidt, and Brian A. Nosek. Liberals and conservatives rely on different
sets of moral foundations. Journal of Personality and Social Psychology, 96(5):1029–1046, 2009.
Jesse Graham, Jonathan Haidt, Sena Koleva, Matt Motyl, Ravi Iyer, Sean P. Wojcik, and Peter H.
Ditto. Moral Foundations Theory: The pragmatic validity of moral pluralism. In Advances in
Experimental Social Psychology, volume 47, pp. 55–130. Academic Press, 2013.
Josef Hofbauer and Karl Sigmund. Evolutionary Games and Population Dynamics. Cambridge
University Press, 1998.
Edward Hughes, Joel Z. Leibo, Matthew Phillips, Karl Tuyls, Edgar Dueñez-Guzman, Antonio
García Castañeda, Iain Dunning, Tina Zhu, Kevin McKee, Raphael Koster, Tina Zhu, Heather
Roff,andThoreGraepel.Inequityaversionimprovescooperationinintertemporalsocialdilemmas.
In Proceedings of the 32nd International Conference on Neural Information Processing Systems
(NeurIPS’18), volume 31, pp. 3330–3340, 2018.
Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse,
Joel Z Leibo, and Nando De Freitas. Social influence as intrinsic motivation for multi-agent deep
reinforcementlearning. In Proceedings of the 36th International Conference on Machine Learning
(ICML’19), pp. 3040–3049. PMLR, 2019.
Immanuel Kant. Grounding for the Metaphysics of Morals. Cambridge University Press, 1785.
Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Proceedings
of the 3rd International Conference on Learning Representations (ICLR’15), 2015.
Joel Z. Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, and Thore Graepel. Multi-agent
reinforcement learning in sequential social dilemmas. In Proceedings of the 16th International
Conference on Autonomous Agents and Multiagent Systems (AAMAS’17), pp. 464–473, 2017.
JoelZ.Leibo,EdwardHughes,MarcLanctot,andThoreGraepel. Autocurriculaandtheemergence
of innovation from social interaction: A manifesto for multi-agent intelligence research. arXiv
preprint arXiv:1903.00742, 2019.
Peter D. Lifton. Individual differences in moral development: The relation of sex, gender, and
personality to morality. Journal of Personality, 53(2):306–334, 1985.
Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In Pro-
ceedings of the 11th International Conference on International Conference on Machine Learning
(ICML’94), pp. 157–163, 1994.
Conor Mayo-Wilson and Kevin J. S. Zollman. The computational philosophy: simulation as a core
philosophical method. Synthese, 199(1-2):3647–3673, 2021.
Kevin R. McKee, Ian Gemp, Brian McWilliams, Edgar A. Duèñez Guzmán, Edward Hughes, and
Joel Z. Leibo. Social diversity and social preferences in mixed-motive reinforcement learning. In
Proceedings of the 19th International Conference on Autonomous Agents and MultiAgent Systems
(AAMAS’20), pp. 869–877, 2020.
13Kevin R. McKee, Joel Z. Leibo, Charlie Beattie, and Richard Everett. Quantifying the effects of
environment and population diversity in multi-agent reinforcement learning. Autonomous Agents
and Multi-Agent Systems, 36(1):21, 2022.
Alexander J. McKenzie. The Structural Evolution of Morality. Cambridge University Press, 2007.
VolodymyrMnih,KorayKavukcuoglu,DavidSilver,AndreiARusu,JoelVeness,MarcG.Bellemare,
Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles
Beattie,AmirSadik,IoannisAntonoglou,HelenKing,DharshanKumaran,DaanWierstra,Shane
Legg,andDemisHassabis. Human-levelcontrolthroughdeepreinforcementlearning. Nature,518
(7540):529–533, 2015.
Toby Ord. Moral trade. Ethics, 126(1):118–138, 2015.
Alexander Peysakhovich and Adam Lerer. Consequentialist conditional cooperation in social dilem-
mas with imperfect information. In Proceedings of the 6th International Conference on Learning
Representations (ICLR’18), 2018.
Anatol Rapoport. Prisoner’s dilemma — recollections and observations. In Game Theory as a
Theory of a Conflict Resolution, pp. 17–34. Springer, 1974.
ThomasReidandKnudHaakonssen. Thomas Reid on Practical Ethics. PrincetonUniversityPress,
1990.
Tuomas W. Sandholm and Robert H Crites. Multiagent reinforcement learning in the iterated
prisoner’s dilemma. Biosystems, 37(1-2):147–166, 1996.
Francisco C. Santos, Marta D. Santos, and Jorge M. Pacheco. Social diversity promotes the emer-
gence of cooperation in public goods games. Nature, 454(7201):213–216, 2008.
Prakash P. Shenoy. On coalition formation: A game-theoretical approach. In International Journal
of Game Theory, volume 8, 1979.
Karl Sigmund. The Calculus of Selfishness. Princeton University Press, 2010.
KarlSigmundandMartinA.Nowak. Evolutionarygametheory. CurrentBiology,9(14):R503–R505,
1999.
Walter Sinnott-Armstrong. Moral Psychology: The Evolution of Morality: Adaptations and Innate-
ness. MIT Press, 2008.
Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press,
2018. ISBN 0262039249.
Elizaveta Tennant, Stephen Hailes, and Mirco Musolesi. Modeling moral choices in social dilemmas
withmulti-agentreinforcementlearning.InProceedingsofthe32ndInternationalJointConference
on Artificial Intelligence (IJCAI’23), pp. 317–325, 2023a.
Elizaveta Tennant, Stephen Hailes, and Mirco Musolesi. Learning machine morality through expe-
rience and interaction. arXiv Preprint. arXiv:2312.01818, 2023b.
Christopher J.C.H. Watkins and Peter Dayan. Q-learning. Machine Learning, 8(3):279–292, 1992.
14Supplementary Material
A Partner Selection Algorithm
The algorithm for partner selection (sel) & at the basis of the dilemma game (dil) with 2 separate
Q-network models is presented below (see Algorithm 1). We use episodes, so that all players learn
simultaneously, after each player had a single chance to select and play. For clarity, each selector’s
statest reliesonthesameenvironment_state(i.e.,simultaneousselections),andthereforeevery
i,sel
player i’s st+1 in the sel trajectory only differs from st by one entry: at at player j’s position.
i,sel i,sel j,dil
For dil, each player i uses all of their observed dilemma trajectories to update their network - by
calculating an average loss across these trajectories.
Algorithm 1 Partner selection algorithm
Require: population of players P of size N, total episodes T =30000
set random initial environment_state (i.e., one random move for each player)
for all players i∈P do:
initializeselectionmemoryD anddilemmamemoryD (capacity265toallowexperiencereplay)
i,sel i,dil
initialize value functions Q ,Q
i,sel i,dil
set random initial selection & dilemma states (st=0,st=0)
i,sel i,dil
end for
initialize iterations counter t=0
for all episode←1,T do:
Get current global environment_state from the environment ▷ this remains stable for entire episode
PART 1: Partner Selection
for all players i∈P do ▷ loop over entire population in a random order
if episode=0 then
use random state stored in st=0 on first episode
sel
else
i observes the environment_state and removes itself to parse in their own st
i,sel
end if
t←t+1
using st , Q and ϵ-greedy policy, i selects a partner j (i.e., takes action at )
i,sel i,sel i,sel
store pair of agents {i,j} in this episode’s sub-population P
episode
end for
PART 2: Dilemma Game
for all {i,j}∈P do ▷ for each pair of agents resulting from the selection stage
episode
if episode=0 then
use random dilemma state stored in st=0 on first episode
dil
else
i and j parse states st , st from the environment_state & the current opponent’s index
i,dil j,dil
end if
using st , Q and ϵ-greedy policy, i plays a move at
i,dil i,dil i,dil
using st , Q and ϵ-greedy policy, j simultaneously plays a move at
j,dil j,dil j,dil
i and j receive rewards rt & rt respectively
i,dil j,dil
i and j observe st+1 based on the current opponent’s action a : st+1 =at ;st+1 =at
dil t i,dil j,dil j,dil i,dil
player i copies the dilemma reward to the selection reward as well: rt =rt
i,sel i,dil
in player i’s D memory, store: (st ,at ,rt ,st+1)
i,dil i,dil i,dil i,dil i,dil
in player j’s D memory, store: (st ,at ,rt ,st+1) ▷ i.e., opponent j learns too
j,dil j,dil j,dil j,dil j,dil
end for
15PART 3: Update sel and dil networks ▷ after each player made a selection & played a game:
for all players i∈P do:
update the corresponding part of environment_state with the latest moves by i (at )
i,dil
player i observes their st+1 by copying the latest environment_state and removing self
i,sel
in player i’s D memory, store: (st ,at ,rt ,st+1)
i,sel i,sel i,sel i,sel i,sel
player i samples one experience from D : (st ,at ,rt ,st+1) & calculates loss
i,sel i,sel i,sel i,sel i,sel
player i samples all experiences from D : (st ,at ,rt ,st+1) & calculates average loss
i,dil i,dil i,dil i,dil i,dil
player i updates Q network with respect to time step t & the calculated loss
i,sel
player i updates Q network with respect to time step t & the calculated loss
i,dil
refresh memory buffers D , D , D
i,sel i,dil j,dil
end for
end for
B Players’ Popularity
Figure 5 shows the confidence intervals around the means presented in the main body of the paper
for the most popular players in each population on the final 100 episodes (compare with the heat
map in Figure 3a), and provides the 50% threshold line for ease of interpretation, which allows us
to compare whether the majority player is selected more or less often than expected simply due to
their prevalence.
Popularity of each player type
(over last 100 episodes)
60
Player
50 majority player constitutes S
50% of populaiton
Ut
40 De
V-Eq
30 V-Ki
aUt
20
mDe
10 V-In
V-Ag
0
Population
Figure5: Mostpopularplayersineachpopulationonthefinal100episodes,witherrorsbarsaround
the mean 50% reference line. We average across 20 runs.
C Action Pairs within Each Population
An analysis of action pairs in Figure 6 provides further insight into the pair-wise interactions in
each population (including mutual cooperation - C,C; unilateral exploitation - C,D or D,C; and
mutual defection - D,D observed at every episode). We observe that the high levels of cooperation
in the majority-Ut and majority-V-Ki populations are largely driven by mutual cooperation, and
that the low cooperation in the majority-aUt, majority-V-Ag and the majority-V-Eq populations
can be largely attributed to high mutual defection. In other populations much more exploitation
(i.e.,unilateraldefection)isobserved,andincertainpro-socialpopulationssuchasmajority-De this
is largely due to the selecting player being exploited by their selected opponent.
16
detceles
semit
%
)sedosipe
001
tsal
revo
naem(
S-ytirojam tU-ytirojam eD-ytirojam qE-V-ytirojam iK-V-ytirojam tUa-ytirojam eDm-ytirojam nI-V-ytirojam gA-V-ytirojamFigure6: Actionpairsobservedoneveryepisodeineverypopulation(wesumobservationswithinan
episode,andthencountthenumberofoccurrencesofeachactionpairacross20runs). Theordering
ofactionswithineachpaircorrespondstotheselectingplayerfirst,andtheselected opponentsecond.
17D Cooperation by Each Player Type across Populations
In addition to the analysis of the S agents’ behavior across populations presented in the main body
ofthepaper,Figure7presentscooperationovertimebyeveryotherplayertypeineverypopulation.
For all players except V-Eq and V-In, levels of cooperation are relatively consistent across popula-
tions. V-Ki and V-Ag players go through an initial phase of low or high cooperation, respectively.
The other norm-based pro-social player De converges to full cooperation very quickly, showing that
this definition of the norm produces the strongest signal for cooperation in the IPD (extending the
findings of Tennant et al. 2023a to the population case as well). Their anti-social counterpart mDe,
however, does not display full defection, likely due to the partner selection mechanism providing an
incentive to cooperate in order to get selected more often to play. The Ut and aUt players converge
toeitherfullcooperationorfulldefectionrespectivelyinallpopulationsnearlyequally, buttherate
of convergence is faster for the anti-social player, likely due to the payoffs associated with defection
intheIPDgame. TheV-Eq andV-In playerscooperatetovariableextentsindifferentpopulations,
but in a way that is consistent with the choice of their reward functions (e.g., defect more in a
majority-defective population).
E Selection Patterns between Player Types
In addition to the analysis of selections made over the entire simulation in the two populations
presented in the main body of the paper (majority-S and majority-De, see Figure 3b&3c), we show
selections made by each individual player in every population in Figures 8 (as a heat map) and 9
(asnetworksinvolvingthetop10%ofthepairsintermsofnumberofinteractions). Thelattershow
the most common patterns of interaction among players in these artificial societies - for example, in
the majority-De population, the De players select other types of opponents more often than each
other.
18Figure 7: Behavior of each player type across every population. We present moving average of the
mean across 20 runs.
19Selection dynamics across players
in population majority-S
(# times selected)
(over episodes 1-30000)
0_S
1_S 8000
2_S
3_S
4_S
5_S 6000
6_S
7_S
8_Ut
9_De 4000 10_V-Eq
11_V-Ki
12_aUt
1 13 4_ _m VD -Ie n 2000
15_V-Ag
0
Player 2 (selectED)
Selection dynamics across players Selection dynamics across players
in population majority-Ut in population majority-aUt
(# times selected) (# times selected)
(over episodes 1-30000) (over episodes 1-30000) 12000
1 2 30 _ _ __ U U US t t t 8000 3_2 V1 _0 _ -D_ EUS e qt 10000 4_Ut 4_V-Ki
5_Ut 6000 5_aUt 8000
6_Ut 6_aUt
7_Ut 7_aUt
98 __ DU et 4000 8 9_ _a aU Ut t 6000 10_V-Eq 10_aUt
1 11 2_ _V a- UK ti 11 21 __ aa UU tt 4000
13_mDe 2000 13_mDe
11 54 __ VV -A-I gn 11 54 __ VV -A-I gn 2000
0 0
Player 2 (selectED) Player 2 (selectED)
Selection dynamics across players Selection dynamics across players
in population majority-De in population majority-mDe
(# times selected) 10000 (# times selected) 12000
(over episodes 1-30000) (over episodes 1-30000)
0_S 0_S
21 __ DU et 8000 21 __ DU et 10000
3_De 3_V-Eq
4_De 4_V-Ki
5_De 5_aUt 8000
6_De 6000 6_mDe
7_De 7_mDe
8_De 8_mDe 6000
9_De 9_mDe 10_V-Eq 4000 10_mDe
11_V-Ki 11_mDe 4000
12_aUt 12_mDe
1 13 4_ _m VD -Ie n 2000 1 13 4_ _m VD -Ie n 2000
15_V-Ag 15_V-Ag
0 0
Player 2 (selectED) Player 2 (selectED)
Selection dynamics across players Selection dynamics across players
in ( op vo e(p # ru etl ia pmt ii seo osn d sm ee sa l ej 1o c -r t 3i et 0y d 0- )V 0- 0E ) q 10000 i n (o p v o e(p # r u etl i pa m it si eo osn d s em e sla e 1j co -t 3r ei 0t dy 0)- V 00-I )n 8000
3_2 V1 _0 _ -D_ EUS e qt 8000 3_2 V1 _0 _ -D_ EUS e qt 67 00 00 00
4_V-Eq 4_V-Ki
5_V-Eq 5_aUt 5000
6_V-Eq 6000 6_mDe
7 8_ _V V- -E Eq q 7 8_ _V V- -I In n 4000 9_V-Eq 9_V-In 10_V-Eq 4000 10_V-In 3000
11_V-Ki 11_V-In
11 32 __ ma DU et 1 12 3_ _V V- -I In n 2000
14_V-In 2000 14_V-In
15_V-Ag 15_V-Ag 1000
0 0
Player 2 (selectED) Player 2 (selectED)
Selection dynamics across players Selection dynamics across players
in population majority-V-Ki in population majority-V-Ag
(ov e(# r eti pm ise os d s ee sl e 1c -t 3e 0d 0) 00) (ov e(# r eti pm ise os d s ee sl e 1c -t 3e 0d 0) 00) 14000
3 4 5_ _ _2 V1 V V_0 _ -D_ E - -U K KS e qt i
i
68 00 00 00 3 4_ 5_2 V _1 V_0 a_ -D_ E - UU KS e q tt i 11 02 00 00 00
6 7_ _V V- -K Ki i 6 7_ _m VD -Ie n 8000
108 9 __ _ VV V -- - KK K ii i 4000 108 9 __ _ VV V -- - AA A gg g 6000
11_V-Ki 11_V-Ag
12_aUt 12_V-Ag 4000
13_mDe 2000 13_V-Ag
14_V-In 14_V-Ag
15_V-Ag 15_V-Ag 2000
0 0
Player 2 (selectED) Player 2 (selectED)
Figure 8: Selection patterns across player types in every population, with number of selections in
20
every cell summed over all 30000 episodes. We average across 20 runs
)ROtceles(
1 reyalP
)ROtceles(
1 reyalP
)ROtceles(
1 reyalP
)ROtceles(
1 reyalP
S_0
S_0
S_0
S_0
tU_1
tU_1
tU_1
tU_1
tU_2
eD_2
eD_2
eD_2
tU_3
eD_3
qE-V_3
qE-V_3
tU_4
eD_4
qE-V_4
iK-V_4
tU_5
eD_5
qE-V_5
iK-V_5
tU_6
eD_6
qE-V_6
iK-V_6
tU_7
eD_7
qE-V_7
iK-V_7
)ROtceles(
1 reyalP
t eU D_ _8 9
e eD D_ _8 9
q qE E- -V V_ _8 9
i iK K- -V V_ _8 9
qE-V_01
qE-V_01
qE-V_01
iK-V_01
iK-V_11
iK-V_11
iK-V_11
iK-V_11
tUa_21
tUa_21
tUa_21
tUa_21
S S_ _0 1
n ge I AD -V -m V__ _43 511 1
n ge I AD -V -m V__ _43 511 1
n ge I AD -V -m V__ _43 511 1
n ge I AD -V -m V__ _43 511 1
S_2 S_3 S_4 S_5 S_6 S_7 tU_8
)ROtceles(
1 reyalP
)ROtceles(
1 reyalP
)ROtceles(
1 reyalP
)ROtceles(
1 reyalP
eD_9 qE-V_01 iK-V_11 tUa_21 n ge I AD -V -m V__
_43 511 1
S t eU D_ _0 _1 2
S t eU D_ _0 _1 2
S t eU D_ _0 _1 2
S t eU D_ _0 _1 2
qE-V_3
qE-V_3
qE-V_3
qE-V_3
iK-V_4
iK-V_4
iK-V_4
iK-V_4
tUa_5
tUa_5
tUa_5
tUa_5
tUa_6
eDm_6
eDm_6
eDm_6
tUa_7
eDm_7
nI-V_7
nI-V_7
tUa_8
eDm_8
nI-V_8
gA-V_8
tUa_9
eDm_9
nI-V_9
gA-V_9
tUa_01
eDm_01
nI-V_01
gA-V_01
tUa_11
eDm_11
nI-V_11
gA-V_11
n gt eU I AD -a V -m V_ __2 _431 511 1
n ge e I AD D -V -m m V__ _ _42 3 511 1 1
n n n gI I I A- - -V V V -V_ _ _ _2 3 4 51 1 1 1
g g g gA A A A- - - -V V V V_ _ _ _2 3 4 51 1 1 1Selections
in population majority-S
threshold > 85th percentile
2
S
D9 e a1 U2 tV1 -E0 q
4 8
S 13 Ut 15
S1 V1 -K1 7m iDe V-Ag S6 S5
S 14 0
V-In S
3
S
Selections Selections
in population majority-Ut in population majority-aUt
threshold > 85th percentile threshold > 85th percentile
14 6
V-In aUt
V1 -A5 g a1 U2 t V1 -E0 q V1 -A5 g
2 14 11
U4 t 7Ut D9 e U6 t U5 t m1 D3 e D2 eV-In aUt 5a1 U2 t
Ut 1 aUt
V1 -K1
i
S0 Ut S0
3 8
a1 U0
t
U1 t U8 t 3 V-4 Ki V-Eq aU at U7 t
13 Ut 9
mDe aUt
Selections Selections
in population majority-De in population majority-mDe
threshold > 85th percentile threshold > 85th percentile
6 6
De mDe
10
mDe
12
U1 t D2 0e D3 e V1 -K1 i m1 D3 e mDe D2 e V1 -A5 1g S0
13 S 5 Ut 4
V1 -4 In D8 e mDe a1 U2 t 10 V1 -4 InaUt m 89 DV e-Ki
D9 e D5 e V1 -A5 g V-Eq 3 mDe m1 D1 e
4 V-Eq
De 7 7
De mDe
Selections Selections
in population majority-V-Eq in population majority-V-In
threshold > 85th percentile threshold > 85th percentile
6 12
V-Eq V-In
10 9
13 V-Eq V-In
mD S0e V1 -K1 i
9
D2
e
V-5 Eq V-7 In S0 m6 De D2
e
V1 -4 In
U1 t
V-Eq V1 -A5 g a1 U2 t
V-3 Eq V1 -4 In
V-8 InV1 -A5
g
U1 t 11V1 -0 InaU5 t
V-3 Eq
V-In
V-4 Eq 8 V-7 Eq 13 V-4 Ki
V-Eq V-In
Selections Selections
in population majority-V-Ki in population majority-V-Ag
threshold > 85th percentile threshold > 85th percentile
14 15
V-In V-Ag
12
13 V-Ag
V-Ag 9
11 V-Ag 5
U1 t V-Ki V-7 Ki 14 7 V-3 Eqm6 D4e aUt
V-9 Ki
V-4 KiV1 -K0 i
V m-5 1K D3i eD2 e
12S0
V-6 Ki 3
V-Ag V- UI 1n
t
S0D2V e-Ki
V1
-AV 01 g-A1 g
15 8aUt V-Eq 8
V-Ag V-Ki V-Ag
Figure9: Selectionpatternsacrossplayertypesineverypopulation(mostcommoninteractions,i.e.,
>90th percentile, over all 30000 episodes). More popular players are placed more centrally using a
force-directed algorithm. 21