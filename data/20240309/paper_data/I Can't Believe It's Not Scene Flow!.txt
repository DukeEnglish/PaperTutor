I Can’t Believe It’s Not Scene Flow!
Ishan Khatri1∗, Kyle Vedder2∗, Neehar Peri3, Deva Ramanan3, James Hays4
1Stack AV, 2University of Pennsylvania, 3Carnegie Mellon, 4Georgia Institute of
Technology
∗
(a) GroundTruth (b) FastFlow3D[13] (c) DeFlow[52]
(d) NSFP[18] (e) ZeroFlowXL5x[41] (f) TrackFlow (Ours)
Fig.1: Wevisualizeacherrypickedexampleoftwomovingpedestrians(walkingfrom
left to right) with unusually high density lidar returns. We expect state-of-the-art
sceneflowmethodswouldworkwellonthiseasyinstance,butfindthattheyfallshort.
Notably, TrackFlow is the only method to estimate proper flow for these pedestrians.
Abstract. Current scene flow methods broadly fail to describe motion
on small objects, and current scene flow evaluation protocols hide this
failure by averaging over many points, with most drawn larger objects.
To fix this evaluation failure, we propose a new evaluation protocol,
BucketNormalizedEPE,whichisclass-awareandspeed-normalized,en-
ablingcontextualizederrorcomparisonsbetweenobjecttypesthatmove
at vastly different speeds. To highlight current method failures, we pro-
pose a frustratingly simple supervised scene flow baseline, TrackFlow,
built by bolting a high-quality pretrained detector (trained using many
classrebalancingtechniques)ontoasimpletracker,thatproducesstate-
of-the-art performance on current standard evaluations and large im-
provements over prior art on our new evaluation. Our results make it
clear that all scene flow evaluations must be class and speed aware, and
supervised scene flow methods must address point class imbalances. We
will release the evaluation code publicly upon publication.
Keywords: LiDAR Scene Flow, Autonomous Vehicles
∗Equal contributions
4202
raM
7
]VC.sc[
1v93740.3042:viXra2 I. Khatri et al.
1 Introduction
Sceneflowestimationisthetaskofdescribinga3Dmotionfieldbetweentempo-
rally successive point clouds [2,7,13,23,30,41,51]. In theory, high quality scene
flow estimators can provide a valuable signal about the dynamics of the real
world [13,41] for offline [30] and online [51] processing.
Do state-of-the-art scene flow methods actually work well in practice? Stan-
dard scene flow metrics suggest that existing methods can estimate motion to
centimeter-levelaccuracy.OnThreewayEPE[5],ametricdesignedtoseparately
measurestaticanddynamicperformancetobetterevaluatesceneflowmethods,
ZeroFlow XL 5x [41] boasts state-of-the-art performance on the Argoverse 2
benchmark [47], featuring an average Threeway EPE of only 4.9 centimeters
(1.9 inches); its dynamic component (objects moving faster than 0.5 m/s) has
anAverageEPEofof11.7centimeters(4.6inches).Onthescaleofcarsandpeo-
ple, these feel like tiny errors and seem to imply that current generation scene
flow methods are of high quality.
However,uponfurtherinvestigation,wefindthatthisisnotthecase.Wevi-
sualizetheflowresultsfromseveralstate-of-the-artsupervised(FastFlow3D[13],
DeFlow[52])andunsupervised(NSFP[18],ZeroFlow[41])sceneflowestimation
methods.Theresultsareshockinglybad—evenoninstancescherrypickedtobe
easy (Fig. 1), existing methods describe larger objects well, but consistently fail
todescribethemotionofsmallerobjectswithfewerlidarpoints(e.g.pedestrians
and bicyclists).
Thisisanalarmingdiscovery.Existingsceneflowmetricsarefailingtoartic-
ulate this bad performance on these important classes of objects because those
objectsareatinyfractionofthe(dynamic)pointsthatmakeupascene(Fig.2).
To address this limitation, we propose a new evaluation protocol, Bucket Nor-
malized EPE, that is class aware, allowing us to directly measure performance
disparities across classes of different sizes, and speed normalized, allowing us to
directly compare description quality across classes by evaluating the percentage
ofmotiondescribed,evenifobjectsaretravelingatdramaticallydifferentspeeds.
With this in mind, we observe that supervised 3D lidar detectors are also
evaluated on class-aware mean Average Precision, and thus are engineered to
perform well on smaller and rarer objects [54]. Based on this, we propose a
frustratingly simple scene flow baseline: run a state-of-the-art detector to pro-
pose object boxes [44] followed by a simple 3D Kalman Filter tracker [45] to
describe their motion. Despite its simplicity, this method, TrackFlow, achieves
state-of-the-art performance on the old metric of Threeway EPE, and massively
outperforms prior art on our Bucket Normalized EPE evaluation, capturing an
additional10%oftotalmotioningeneralandanadditional20%oftotalmotion
on pedestrians (a 1.5× improvement) compared to the next best method.
State-of-the-art performance from our simple baseline is an indictment of
existing supervised scene flow methods. These methods must start to evaluate
in a class and speed aware way, and utilize techniques to address the severe
class imbalance at the point level seen across real-world datasets. This also has
implications for the evaluation of unsupervised scene flow methods, which haveI Can’t Believe It’s Not Scene Flow! 3
broadly ignored the concept of object class and focused strictly on object geom-
etry; these too are biased against smaller objects and fail to properly capture
their flow.
We make three primary contributions in this work:
1. Raisingalarmbellsaboutthequalitativefailureofstate-of-the-artsceneflow
methods on important classes of objects.
2. Providing a new evaluation protocol, Bucket Normalized EPE, that enables
quantification of this problem.
3. Providing a frustratingly simple baseline, TrackFlow, that achieves state-of-
the-art performance on prior evaluations [5] and significantly outperforms
prior art on our class-aware Bucket Normalized EPE evaluation.
2 Related Work
2.1 Scene Flow Datasets and Ground Truth
Unlike next token prediction in language [36] or next frame prediction in vi-
sion [46], scene flow is not naïvely self-supervised: future observations do not
provide ground truth scene flow. Thus, to evaluate scene flow estimates, ground
truthmotiondescriptionsmustbeprovidedbyanoracle,typicallyhumananno-
tationofrealdata[4,27,28,38,47]orthegeneratorofsyntheticdatasets[26,53].
Forrealworld3Ddatasets(typicallyfromtheautonomousvehicledomain)these
human annotations are provided in the form of bounding boxes and tracks for
every object in the scene, and consequently the generated ground truth flow is
assumed to be rigid, even in the case of non-rigid motion like pedestrian gaits.
2.2 Scene Flow Estimation
Scene flow estimators, given point clouds P and P , predict Fˆ , a 3D
t t+1 t,t+1
vector per point in P that describes its motion from t to t+1 [6]. Performance
t
istypicallymeasuredusingAverageEndpointError(EPE)whichistheL norm
2
between the predicted (Fˆ ) and ground truth flow (F∗ ), as in Equation 1.
t,t+1 t,t+1
Average EPE(P )= 1 (cid:88) (cid:13) (cid:13)Fˆ (p)−F∗ (p)(cid:13) (cid:13) . (1)
t ∥P t∥ (cid:13) t,t+1 t,t+1 (cid:13) 2
p∈Pt
Current state-of-the-art methods for scene flow estimation broadly fall into
one of two categories: supervised and unsupervised.
SupervisedSceneFlow Supervisedsceneflowmethodstrainfeedforwardnet-
works to perform flow vector regression based on ground truth annotations [1,
3,11,13,15,17,20,23,35,39,43,48,52]. Many of these networks utilize custom
point operations such as point-based convolutions [11,15,20,23], making them
intractabletotrainonlargepointclouds.ThismotivatedFastFlow3D[13],which
uses a feedforward architecture based on PointPillars [16], an efficient lidar de-
tectorarchitecture,enablingefficienttrainingandinferenceofflowonreal-world4 I. Khatri et al.
point clouds. FastFlow3D’s speed and quality make it a popular base archite-
cuture, utilized by the state-of-the-art self-supervised ZeroFlow [41] and the
supervised DeFlow [52].
Unsupervised Scene Flow Unsupervised methods tend to use online op-
timization against surrogate objectives such as Chamfer distance [18], cycle-
consistency [29], distance transform [19], or some other hand-designed heuris-
tic [5,10,34]. Neural Scene Flow Prior (NSFP) [18] provides high quality scene
flow estimates by optimizing a small ReLU MLP at test time to minimize the
Chamfer distance and maintain cycle-consistency. Other unsupervised methods
indirectlyleverageonlineoptimization;ZeroFlow[41]introducesScene Flow via
Distillation, a framework that uses an optimization method to pseudolabel an
arbitrary amount of data to then train a feedforward network.
2.3 Scene Flow Evaluation Metrics
Inreal-worldscenes,mostpointsbelongtothestaticbackground.Consequently,
a simple Average EPE (Equation 1) over all points is dominated by describing
→−
thebackground:eitheregomotionorsimplyregressing 0.Inordertoseparately
measure non-ego dynamics, Chodosh et al. [5] introduce a new evaluation pro-
tocol, Threeway EPE, which computes a mean over the Average EPE for three
disjoint classes of points: Foreground Dynamic (points inside bounding box la-
bels moving greater than 0.5m/s), Foreground Static (points inside bounding
box labels moving less than 0.5m/s), and Background Static.
2.4 3D Object Detection and Tracking
Supevisedobjectdetectionhaslongstruggledwithmultiplekindsofclassimbal-
ances. To address the foreground / background class imbalance in single-stage
detector anchor boxes, methods typically employ Focal Loss [21]; to address the
classimbalancefoundintypicalobjecttaxonomies,3Dobjectdetectorsusetech-
niques such as Class Based Grouping and Sampling (CBGS) [54] and copy and
pasteaugmentations[49]toupsampleandthusrebalancerareclassestoimprove
performance.Additionally,3Dobjectdetectionmodelsareoftenabletotakead-
vantageofmulti-modaldatasuchascamerasinordertoproducedetections.This
data typically boosts performance on small, rare objects the most [25,31,42].
Theobjectdetectionliteraturefocusesheavilyonthetopicofclassimbalance
in large part because the metrics for object detection are class aware. By far
the most common evaluation metric is mean Average Precision, or mAP; the
“mean” in mAP is a simple mean over per-class average precision. Methods are
comparedbytheirmAPnumber,butmethodpapersoftenreporttheirper-class
AP to enable deeper analysis of method characteristics [8,22,40].I Can’t Believe It’s Not Scene Flow! 5
3 Bucket Normalized EPE: Revealing Performance on
Small Objects
As we show qualitatively in Fig. 1 (and further in Fig. 8), existing scene flow
methodsconsistentlystruggletodescribemotiononimportantsmallobjectslike
pedestrians. However, these failures are not captured by Threeway EPE because
these objects are small and thus have few points; Threeway EPE’s Foreground
Dynamic categoryisdominatedbylarge,commonobjectswithmanypointslike
cars and other vehicles. As we show in Fig. 2, almost 15% of all points are from
cars or other vehicles, while fewer than 1% of points are from pedestrians and
other Vulnerable Road Users (VRUs), resulting in an enormous class imbalance
and domination of vehicle performance in Foreground Dynamic’s Average EPE.
84.08%
109
9.39%
108
5.78%
107 0.66%
0.08%
106
B A C K G R O U N D OC TA HR E R V E HIC L E S P E D E S T RIA WN H E E L E D V R U
Fig.2: Number of points from each semantic meta-class for Argoverse 2’s val split.
AlthoughPEDESTRIANinstancesarecommon,theyarelessthan1%ofthetotalnumber
of points owing to their small size relative to CAR and OTHER VEHICLES. Number of
points (Y axis) shown on a log scale.
Additionally, Threeway EPE fails to account the large differences in speed
commonacrossobjects.Forexample,0.5m/sofestimationerroronacarmoving
75 kph is negligible (<2.5%), while 0.5m/s of estimation error on a pedestrian
moving0.5m/sisfailingtodescribe100%ofthepedestrian’smotion;yet,Three-
way EPE treats both estimation errors as equally bad.
With these shortcomings in mind, we propose a new evaluation protocol,
Bucket Normalized EPE, that has two important features:
ssalcatem
ni
stniop
radiL6 I. Khatri et al.
1. It’sclassaware.Thisallowsustocalloutperformanceacrossthedistribution
of geometry using a taxonomy that human labelers have deemed important,
as detectors have done by evaluating with mean Average Precision for over
a decade [22].
2. It’s speed normalized. This allows us to answer “what percentage of object
motion was described?” by contextualizing against the speed of the object,
and thus directly compare performance across object catagories.
We implement this by accumulating every point into a class-speed matrix
(e.g. Table 3) based on its ground truth speed and class, recording an Average
EPEaswellasaper-bucketaveragespeed.Tosummarizetheseresults,wereport
two numbers per class:
– Static EPE, taken directly from the Average EPE of the first speed bucket
for that class (i.e. the first column of Table 3)
– DynamicNormalizedEPE,computedfromameanovertheNormalizedEPE
(AverageEPE) of each non-empty speed bucket (i.e. an average across the
averagespeed
Normalized EPEs of the second column onwards in Table 3)
Dynamic Normalized EPE measures the fraction of motion not described by
theestimatedflowvectorsacrosstheentirespeedspectrum.Amethodthatonly
→−
predicts ego motion (e.g. 0 if ego motion is compensated for) will achieve 1.0
DynamicNormalizedEPE,andamethodthatperfectlydescribesallmotionwill
have0.0DynamicNormalizedEPE.Methodsmayachieveerrorsgreaterthan1.0
bypredictingerrorswithmagnitudegreaterthantheaveragespeed;forexample,
a method that describes the negative vector of true motion will get exactly
2.0 Dynamic Normalized EPE; every bucket’s Average EPE will be exactly 2×
the magnitude of the average speed. The range of Dynamic Normalized EPE is
between0(perfect)and∞(arbitrarilybad),andisundefinedforbucketswithout
anypoints.Astheyarenormalized,theDynamicNormalizedEPEnumberscan
be directly compared across classes.
An example of this resulting per-class performance table is shown in Table 1
for our method TrackFlow (Section 4). Results can be further summarized to a
singletupleofmean Static EPE andmean Dynamic Normalized EPE bytaking
a mean across classes(similar to mean Average Precision in the detection litera-
ture [22]) TrackFlow has a mean Static EPE of 0.076277 and a mean Dyanmic
Normalized EPE of 0.287368 (Table 1).
4 TrackFlow: Frustratingly Simple Scene Flow
To highlight the failure of current supervised scene flow methods on smaller
objects, we propose a very simple framework, Detect + Track: combine a high-
quality, pretrained 3D detector with a simple bounding box tracker, and use
boundingboxtrackmotiontogeneratesceneflow(Fig.3).WeinstantiateDetectI Can’t Believe It’s Not Scene Flow! 7
Class Static (Avg EPE) Dynamic (Norm EPE)
BACKGROUND 0.002402 -
CAR 0.018442 0.182092
OTHER VEHICLES 0.081475 0.312882
PEDESTRIAN 0.052842 0.396849
WHEELED VRU 0.062573 0.257647
Table 1: Bucket Normalized EPE results for our method, TrackFlow, from the Ar-
goverse 2’s test split, demonstrating performance across different meta-classes with a
focus on capturing motion of dynamic objects.
+ Track with LE3DE2E [44]† as the detector and a simple 3D Kalman Filter
based tracker, AB3DMOT [45] as the tracker to form TrackFlow. As we show in
Section 5, TrackFlow captures state-of-the-art on Threeway EPE and beats all
prior art by a large margin on Bucket Normalized EPE.
Detect
Track
Detect
Fig.3: Overview of TrackFlow’s framework, Detect + Track.
Post-hoc, Detect + Track is obvious; ground truth flows are generated from
human produced bounding box detections and tracks (Section 2.1), so a perfect
detector and tracker will achieve perfect flow. But the power of TrackFlow isn’t
just derived from its use of bounding boxes; it also greatly benefits from the
strength of modern 3D bounding box detectors across the class distribution. As
wediscussinSection2.4,moderndetectorsaretrainedwithalargebagoftricks
to achieve good Precision and Recall on every semantic class, including small
object classes. TrackFlow is arbitraging these advances into scene flow, allowing
it to perform far better than prior art on pedestrians and other small objects.
Interestingly,wefindthattheDetect+Trackframeworkperformsbestwhen
detections are used with a low confidence threshold. Typically, detectors have
their confidence threshold tuned for deployment by practitioners who select a
†LE3DE2E [44] is the winning method from the Argoverse 2 2023 3D Detection,
Tracking and Forecasting challenge [31–33].8 I. Khatri et al.
fairlystrictconfidencethreshold(0.7-0.9)thatdetectsmostobjects(highrecall)
withouttoomanyfalsepositives(highprecision).However,inourDetect+Track
framework, the tracker serves as a second stage outlier filter across frames, and
thus works best with lower precision, higher recall box predictions from the
detector created by setting the confidence threshold lower (0.4 for TrackFlow).
Inthismodeofoperation,Detect+Trackisoperatinglikeatwostagedetector:
thedetectorisoperatinglikeaboundingboxproposer(e.g.theRegionProposal
NetworkinFast/FasterRCNN[9,37])andthetrackerisactinglikearefinement
head.WeexploredetectorchoiceandconfidencethresholdsfurtherinSection5.3.
5 Experiments
Does TrackFlow live up to its hype as a high-quality baseline? Does Bucket
Normalized EPE make clear the performance differences between TrackFlow
and prior art? What makes a good detector for TrackFlow? To answer these
questions, we compare TrackFlow against a variety of representative supervised
andunsupervisedsceneflowmethods,FastFlow3D[13],DeFlow[52],NSFP[18],
and ZeroFlow [41], on the Argoverse 2 benchmark [47]‡.
TrackFlow (ours) 0.0479 TrackFlow (ours) 0.1050
ZeroFlow XL 5x 0.0494 DeFlow 0.1096
DeFlow 0.0502 NSFP 0.1158
ZeroFlow XL 3x 0.0504 ZeroFlow XL 5x 0.1177
ZeroFlow 5x 0.0510 ZeroFlow XL 3x 0.1191
ZeroFlow 3x 0.0572 ZeroFlow 5x 0.1239
NSFP 0.0606 ZeroFlow 3x 0.1423
FastFlow3D 0.0620 FastFlow3D 0.1564
ZeroFlow 1x 0.0781 ZeroFlow 1x 0.1994
0.00 0.02 0.04 0.06 0.08 0.00 0.05 0.10 0.15 0.20
Average EPE (m) Average EPE (m)
(a) ThreewayEPE (b) ThreewayEPE’sForegroundDynamic
Fig.4: Threeway EPE and Threeway EPE’s Foreground Dynamic component perfor-
mance of our method, TrackFlow, and a collection of state-of-the-art supervised and
unsupervised scene flow estimation methods on Argoverse 2’s test split. Supervised
methods shown with hatching. Lower is better. Method color is consistent between
plots.
‡Allevaluationsareperformedwithamaximumradiusof35mfromtheegovehicle
to maintain consistency with Chodosh et al. [5].I Can’t Believe It’s Not Scene Flow! 9
5.1 TrackFlow is state-of-the-art on Threeway EPE
TrackFlow is state-of-the-art on Threeway EPE (Fig. 4a) on the Argoverse 2
benchmark [47], achieving an overall reduction of 0.0015m (0.15cm, or 1.5mm)
over the next best method, ZeroFlow XL 5x, with this gain coming entirely
from an improvement to performance in Threeway EPE’s Dynamic Foreground
(Fig. 4b).
But is this performance difference meaningful? Based on our reduction of
1.5mm on Threeway EPE (about 4× the thickness of a human fingernail), it
would seem that TrackFlow, while state-of-the-art, is only an incremental im-
provementoverpriorart.However,qualitatively,TrackFlowsignificantlyoutper-
forms prior art on important small objects such as pedestrians (Fig. 1, Fig. 8).
As we show in the next section, under our proposed evaluation protocol Bucket
Normalized EPE, it becomes quantitatively clear that TrackFlow performs sig-
nificantly better on these important classes of objects.
5.2 Bucket Normalized EPE makes performance differences legible
TrackFlow (ours) 0.2874
DeFlow 0.3706
NSFP 0.4219
ZeroFlow XL 5x 0.4389
ZeroFlow XL 3x 0.4421
ZeroFlow 5x 0.4846
ZeroFlow 3x 0.5057
FastFlow3D 0.5323
ZeroFlow 1x 0.5941
0.0 0.2 0.4 0.6 0.8 1.0
mean Dynamic Normalized EPE
Fig.5: mean Dynamic Normalized EPE of our method, TrackFlow, and a collection
of state-of-the-art supervised and unsupervised scene flow estimation methods on Ar-
goverse 2’s test split. Supervised methods shown with hatching. Lower is better.
Theresultsofthesesamemethodsonourclass-aware,speed-normalizedeval-
uation, Bucket Normalized EPE, makes it clear that TrackFlow meaningfully
outperforms prior art (Fig. 5) — TrackFlow correctly describes almost 10%
additionaltotalmotionacrossmeta-classesthanthenextstrongestmethod,De-
Flow[52].Thisdifferenceindynamicperformancebecomesevenmoreclearwhen10 I. Khatri et al.
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
(a) CAR (b) OTHER VEHICLES
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
(c) PEDESTRIAN (d) WHEELED VRU
Fig.6:Permeta-classDynamicNormalizedEPEofourmethod,TrackFlow,andacol-
lection of state-of-the-art supervised and unsupervised scene flow estimation methods
onArgoverse2’stest split.Supervisedmethods shownwithhatching.Lowerisbetter.
Method color and position is consistent between plots.
brokendownbymeta-class:Fig.6showsthatTrackFlowistheonlymethodable
to describe more than 50% of pedestrian motion, beating the next best method,
DeFlow[52],bydescribingmorethananadditional20%oftotalmotion(Fig.6c),
a 1.5× improvement, while other state-of-the-art methods like NSFP [18] and
ZeroFlow XL 5x [41] describe less than 30% and 20% of pedestrian motion,
respectively.
Bucket Normalized EPE allows practitioners to quickly gain substantial in-
sightsintotheperformanceofothermethodsthatwerealmostindistinguishable
under Threeway EPE. For example, if you only care about flow performance
on cars, DeFlow out-performs all other methods including TrackFlow (Fig. 6a),
whileZeroFlowXL5xout-performsallothermethodsonlargervehicles(Fig.6b)
— none of this information was extractable from Threeway EPE differences on
the order of millimeters.
5.3 What makes a good detector for TrackFlow?
AswediscussinSection4,thedetectorinourDetect + Track frameworkhasits
confidencethresholdtunedtohavehighrecall,allowingittoserveasabounding
boxproposalsystemwiththetrackerbeingasecondstagerefinementstep.This
raises the question: what makes a good detector for TrackFlow?
To evaluate this, we propose a modified version of TrackFlow, TrackFlow-
BEVF, that replaces the LE3DE2E Detector [44] with another strong detector,
EPE
dezilamroN
cimanyD
EPE
dezilamroN
cimanyD
)sruo( wolFkcarT
wolFeD PFSN
x5 LX wolForeZ x3 LX wolForeZ x5 wolForeZ x3 wolForeZ D3wolFtsaF x1 wolForeZ
)sruo(
wolFkcarT wolFeD PFSN x5 LX wolForeZ x3 LX wolForeZ x5 wolForeZ x3 wolForeZ D3wolFtsaF
x1
wolForeZI Can’t Believe It’s Not Scene Flow! 11
Class Static (Avg EPE) Dynamic (Norm EPE)
BACKGROUND -0.000228 -
CAR +0.039049 +0.117944
OTHER VEHICLES +0.009013 +0.224830
PEDESTRIAN +0.007187 +0.224250
WHEELED VRU -0.025889 +0.151373
Table2:RelativeBucketNormalizedEPEperformanceofTrackFlowBEVFcompared
toTrackFlow,ontheArgoverse2’stest split.Increasesinerror(worse)areshownwith
a + in red, and decreases in error (better) are shown with a - in green. TrackFlow’s
absolute results are shown in Table 1.
BEVFusion [24]. BEVFusion only had 2% lower mean Average Precision (mAP)
than LE3DE2E on the same challenge§, making it seem like another strong de-
tector candidate; however, TrackFlowBEVF performs significantly worse than
TrackFlow. As shown in Table 2, TrackFlowBEVF performs significantly worse
acrosstheboard,with10%to22%dropsinperformanceonDynamicNormalized
EPE (we consider the Static EPE differences negligible).
This significant degradation is the result of BEVFusion’s poor recall at the
lowendoftheconfidencescale.AsshowninFig.7,atlowconfidence,LE3DE2E
has very high recall, producing many candidate boxes for the pedestrians in the
scene, while BEVFusion misses many of them. These misses are very costly to
→−
TrackFlowBEVF, as they result in 0 flow estimates that miss 100% of each of
the pedestrian’s motion.
Morebroadly,agooddetectorforDetect+Trackisn’tnecessarilyonewitha
highmAP;it’sisonewithveryhighrecallandaccurateheadingestimates,with
error characteristics that enable the tracker to reject false positives. We believe
this is interaction between the detector and tracker is an important but subtle
point — two detectors may have the same mAP, but one detector may have a
higher recall with false positives that are easy for the tracker to reject, resulting
in superior performance in the Detect + Track framework.
6 Conclusion
In this work, we present a startling observation: current scene flow methods
consistently fail to describe motion on pedestrians and other small objects. We
demonstratethatcurrentstandardevaluationmetricshidethisfact,andpresent
a new class-aware, speed normalized evaluation protocol (Bucket Normalized
EPE) to quantify this failure. To highlight these failures in current supervised
sceneflowmethods,wepresentafrustratinglysimplesupervisedsceneflowbase-
line (TrackFlow) that captures state-of-the-art on Threeway EPE and Bucket
§BEVFusion [24] was second on the Argoverse 2 2023 3D Detection, Tracking and
Forecasting challenge [31–33].12 I. Khatri et al.
BEVFusion
LE3DE2E
Fig.7:AqualitativecomparisonoftherecallofBEVFusionandLE3DE2E.LE3DE2E
has much higher recall, allowing it to pick out pedestrians BEVFusion missed (circled
inred),andbetterqualityboxheadingestimates.Bothdetectorsareusingaconfidence
threshold of 0.2.
NormalizedEPE.Wehopethisworkclearlytelegraphstwocrucialpointstothe
scene flow community:
Evaluate scene flow methods, supervised or unsupervised, using
Bucket Normalized EPE. Current evaluation protocols fail to reveal perfor-
mance across the distribution of objects we care about, and they fail to con-
textualize absolute errors in the context of the object’s speed. Class and speed
aware evaluation is important even if a method has zero human supervision; we
cannotexpectanymethodtomeaningfullygeneralizetothelongtailofunknown
objects if it cannot provide good quality motion descriptions on known a set of
objects.
Supervised scene flow needs to address class and point imbalances.
TrackFlow is very inelegant; it’s a detector repurposed as a bounding box pro-
poserthat’sboltedontoastateestimatorinventedover60yearsagorepurposed
as a tracker. Despite this, TrackFlow out-performs prior art by a wide margin
because the lidar detector literature has made great strides in addressing class
imbalanceissues,andTrackFlowisclumsilyarbitragingthoseadvancementsinto
scene flow. There are still enormous gains to be had from the supervised scene
flow community developing (or shamelessly stealing) techniques to properly ad-
dress class and point imbalances.
References
1. Battrawy,R.,Schuster,R.,Mahani,M.A.N.,Stricker,D.:RMS-FlowNet:Efficient
and Robust Multi-Scale Scene Flow Estimation for Large-Scale Point Clouds. In:I Can’t Believe It’s Not Scene Flow! 13
(a)GroundTruth(b) FastFlow3D (c) DeFlow (d) NSFP (e) ZeroFlow (f) TrackFlow
Fig.8: Visualizations of different methods on diverse scenes in Argoverse 2. Each
method is estimating flow from the blue to the green point cloud.
Row 1: Twopedestriansinleftforegroundwithcarsmovinginthebackground.Track-
Flow is the only method able to describe the pedestrian motion.
Row 2: Three pedestrians walking across an intersection in front of a stationary car.
DeFlowisabletocapturethefurthestpedestrian,butonlyTrackFlowisable
tocapturethemotionofallthree.TrackFlowalsofalselyestimatesmotionof
the moving box truck in the background.
Row 3: Top view of pedestrians walking down the sidewalk between a building and
severalcarsparkedinthestreet.TrackFlowistheonlymethodabletodescribe
the pedestrian motion.
Row 4: Pedestrians walking down the sidewalk next to a moving car. TrackFlow is
the only method able to describe the pedestrian motion.
Row 5: Twobicyclistsridingacrossanintersectionnexttodrivingcars.Mostmethods
areabletocapturethetrainingbicyclistsandthemovingcars,butonlyNSFP
and TrackFlow are able to capture the lead bicyclist.
Row 6: Twopedestrianswalkacrossanintersectionwhileacardrivesparalleltothem.
Allmethodscapturethecarmotion,butonlyDeFlow,NSFP,andTrackFlow
capture most of the pedestrian motion. TrackFlow also falsely estimates mo-
tion of one of the parked cars far down the street in the background.14 I. Khatri et al.
Int. Conf. Rob. Aut. pp. 883–889. IEEE (2022) 3
2. Baur,S.A.,Emmerichs,D.J.,Moosmann,F.,Pinggera,P.,Ommer,B.,Geiger,A.:
SLIM: Self-supervised LiDAR scene flow and motion segmentation. In: Int. Conf.
Comput. Vis. pp. 13126–13136 (2021) 2
3. Behl,A.,Paschalidou,D.,Donné,S.,Geiger,A.:Pointflownet:Learningrepresen-
tations for rigid motion estimation from point clouds. In: Int. Conf. Comput. Vis.
pp. 7962–7971 (2019) 3
4. Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A.,
Pan,Y.,Baldan,G.,Beijbom,O.:nuScenes:Amultimodaldatasetforautonomous
driving. In: IEEE Conf. Comput. Vis. Pattern Recog. pp. 11621–11631 (2020) 3
5. Chodosh, N., Ramanan, D., Lucey, S.: Re-Evaluating LiDAR Scene Flow for Au-
tonomous Driving. arXiv preprint (2023) 2, 3, 4, 8
6. Dewan, A., Caselitz, T., Tipaldi, G.D., Burgard, W.: Rigid scene flow for 3d lidar
scans. In: Int. Conf. Intel. Rob. Sys. pp. 1765–1770. IEEE (2016) 3
7. Erçelik, E., Yurtsever, E., Liu, M., Yang, Z., Zhang, H., Topçam, P., Listl, M.,
Çaylı, Y.K., Knoll, A.: 3D Object Detection with a Self-supervised Lidar Scene
FlowBackbone.In:Avidan,S.,Brostow,G.,Cissé,M.,Farinella,G.M.,Hassner,T.
(eds.)ComputerVision–ECCV2022.pp.247–265.SpringerNatureSwitzerland,
Cham (2022) 2
8. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? The
KITTI vision benchmark suite. In: IEEE Conf. Comput. Vis. Pattern Recog. pp.
3354–3361. IEEE (2012) 4
9. Girshick, R.: Fast R-CNN. In: 2015 IEEE International Conference on Computer
Vision (ICCV). pp. 1440–1448 (2015) 8, 17
10. Gojcic, Z., Litany, O., Wieser, A., Guibas, L.J., Birdal, T.: Weakly supervised
learning of rigid 3d scene flow. In: IEEE Conf. Comput. Vis. Pattern Recog. pp.
5692–5703 (2021) 4
11. Gu, X., Wang, Y., Wu, C., Lee, Y.J., Wang, P.: Hplflownet: Hierarchical permu-
tohedral lattice flownet for scene flow estimation on large-scale point clouds. In:
IEEE Conf. Comput. Vis. Pattern Recog. pp. 3254–3263 (2019) 3
12. Huang,X.,Wang,Y.,Guizilini,V.C.,Ambrus,R.A.,Gaidon,A.,Solomon,J.:Rep-
resentationLearningforObjectDetectionfromUnlabeledPointCloudSequences.
In: Liu, K., Kulic, D., Ichnowski, J. (eds.) Proceedings of The 6th Conference on
RobotLearning(CoRL).ProceedingsofMachineLearningResearch,vol.205,pp.
1277–1288 (2023) 17
13. Jund, P., Sweeney, C., Abdo, N., Chen, Z., Shlens, J.: Scalable Scene Flow From
PointCloudsintheRealWorld.IEEERoboticsandAutomationLetters(122021)
1, 2, 3, 8
14. Kim,D.,Lin,T.Y.,Angelova,A.,Kweon,I.S.,Kuo,W.:Learningopen-worldob-
jectproposalswithoutlearningtoclassify.IEEERoboticsandAutomationLetters
(RA-L) (2022) 17
15. Kittenplon, Y., Eldar, Y.C., Raviv, D.: Flowstep3d: Model unrolling for self-
supervised scene flow estimation. In: IEEE Conf. Comput. Vis. Pattern Recog.
pp. 4114–4123 (2021) 3
16. Lang,A.,Vora,S.,Caesar,H.,Zhou,L.,Yang,J.,Beijbom,O.:PointPillars:Fast
Encoders for Object Detection From Point Clouds. In: Proceedings of the 2019
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR).
pp. 12689–12697 (2019) 3
17. Li, R., Lin, G., He, T., Liu, F., Shen, C.: HCRF-Flow: Scene flow from point
clouds with continuous high-order CRFs and position-aware flow embedding. In:
IEEE Conf. Comput. Vis. Pattern Recog. pp. 364–373 (2021) 3I Can’t Believe It’s Not Scene Flow! 15
18. Li, X., Pontes, J.K., Lucey, S.: Neural Scene Flow Prior. Advances in Neural In-
formation Processing Systems 34 (2021) 1, 2, 4, 8, 10
19. Li, X., Zheng, J., Ferroni, F., Pontes, J.K., Lucey, S.: Fast neural scene flow.
In: Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV). pp. 9878–9890 (October 2023) 4
20. Li, Y., Bu, R., Sun, M., Wu, W., Di, X., Chen, B.: Pointcnn: Convolution on
x-transformed points. Adv. Neural Inform. Process. Syst. 31 (2018) 3
21. Lin, T., Goyal, P., Girshick, R.B., He, K., Dollár, P.: Focal Loss for Dense Object
Detection. In: ICCV 2017. pp. 2999–3007 (2017) 4
22. Lin,T.,Maire,M.,Belongie,S.J.,Bourdev,L.D.,Girshick,R.B.,Hays,J.,Perona,
P., Ramanan, D., Doll’a r, P., Zitnick, C.L.: Microsoft COCO: Common Objects
in Context. CoRR (2014) 4, 6
23. Liu, X., Qi, C.R., Guibas, L.J.: FlowNet3D: Learning Scene Flow in 3D Point
Clouds. Proceedings of the IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR) (2019) 2, 3
24. Liu, Z., Tang, H., Amini, A., Yang, X., Mao, H., Rus, D., Han, S.: Bevfusion:
Multi-taskmulti-sensorfusionwithunifiedbird’s-eyeviewrepresentation.In:IEEE
International Conference on Robotics and Automation (ICRA) (2023) 11
25. Ma,Y.,Peri,N.,Wei,S.,Hua,W.,Ramanan,D.,Li,Y.,Kong,S.:Long-tailed3d
detection via 2d late fusion. arXiv preprint arXiv:2312.10986 (2023) 4
26. Mayer, N., Ilg, E., Häusser, P., Fischer, P., Cremers, D., Dosovitskiy, A., Brox,
T.:ALargeDatasettoTrainConvolutionalNetworksforDisparity,OpticalFlow,
and Scene Flow Estimation. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) (2016) 3
27. Menze, M., Heipke, C., Geiger, A.: Joint 3D Estimation of Vehicles and Scene
Flow. In: ISPRS Workshop on Image Sequence Analysis (ISA) (2015) 3
28. Menze, M., Heipke, C., Geiger, A.: Object Scene Flow. ISPRS Journal of Pho-
togrammetry and Remote Sensing (JPRS) (2018) 3
29. Mittal, H., Okorn, B., Held, D.: Just Go With the Flow: Self-Supervised Scene
Flow Estimation. In: IEEE Conf. Comput. Vis. Pattern Recog. (June 2020) 4
30. Najibi,M.,Ji,J.,Zhou,Y.,Qi,C.R.,Yan,X.,Ettinger,S.,Anguelov,D.:Motion
Inspired Unsupervised Perception and Prediction in Autonomous Driving. Euro-
pean Conference on Computer Vision (ECCV) (2022) 2, 18
31. Peri, N., Dave, A., Ramanan, D., Kong, S.: Towards Long Tailed 3D Detection.
CoRL (2022) 4, 7, 11
32. Peri, N., Li, M., Wilson, B., Wang, Y.X., Hays, J., Ramanan, D.: An empirical
analysis of range for 3d object detection. arXiv preprint arXiv:2308.04054 (2023)
7, 11
33. Peri,N.,Luiten,J.,Li,M.,Ošep,A.,Leal-Taixé,L.,Ramanan,D.:Forecastingfrom
lidarviafutureobjectdetection.In:ProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition(CVPR).pp.17202–17211(June2022)
7, 11
34. Pontes, J.K., Hays, J., Lucey, S.: Scene flow from point clouds with or without
learning. In: Int. Conf. 3D Vis. pp. 261–270. IEEE (2020) 4
35. Puy,G.,Boulch,A.,Marlet,R.:Flot:Sceneflowonpointcloudsguidedbyoptimal
transport. In: Eur. Conf. Comput. Vis. pp. 527–544. Springer (2020) 3
36. Radford,A.,Narasimhan,K.,Salimans,T.,Sutskever,I.:Improvinglanguageun-
derstanding by generative pre-training (2018) 3
37. Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards Real-Time Object
DetectionwithRegionProposalNetworks.IEEETransactionsonPatternAnalysis
and Machine Intelligence 39(6), 1137–1149 (2017) 8, 1716 I. Khatri et al.
38. Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo,
J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H.,
Timofeev,A.,Ettinger,S.,Krivokon,M.,Gao,A.,Joshi,A.,Zhang,Y.,Shlens,J.,
Chen,Z.,Anguelov,D.:ScalabilityinPerceptionforAutonomousDriving:Waymo
OpenDataset.In:ProceedingsoftheIEEE/CVFConferenceonComputerVision
and Pattern Recognition (CVPR) (June 2020) 3
39. Tishchenko,I.,Lombardi,S.,Oswald,M.R.,Pollefeys,M.:Self-supervisedlearning
ofnon-rigidresidualflowandego-motion.In:Int.Conf.3DVis.pp.150–159.IEEE
(2020) 3
40. Vedder,K.,Eaton,E.:SparsePointPillars:MaintainingandExploitingInputSpar-
sity to Improve Runtime on Embedded Systems. In: Proceedings of the Interna-
tional Conference on Intelligent Robots and Systems (IROS) (2022) 4
41. Vedder, K., Peri, N., Chodosh, N., Khatri, I., Eaton, E., Jayaraman, D., Liu, Y.,
Ramanan,D.,Hays,J.:ZeroFlow:ScalableSceneFlowviaDistillation.In:Twelfth
InternationalConferenceonLearningRepresentations(ICLR)(2024) 1,2,4,8,10
42. Vora, S., Lang, A.H., Helou, B., Beijbom, O.: Pointpainting: Sequential fusion for
3d object detection. In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition. pp. 4604–4612 (2020) 4
43. Wang, J., Li, X., Sullivan, A., Abbott, L., Chen, S.: PointMotionNet: Point-
Wise Motion Learning for Large-Scale LiDAR Point Clouds Sequences. In: 2022
IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops
(CVPRW). pp. 4418–4427 (2022) 3
44. Wang, Z., Chen, F., Lertniphonphan, K., Chen, S., Bao, J., Zheng, P., Zhang, J.,
Huang,K.,Zhang,T.:Technicalreportforargoversechallengesonunifiedsensor-
based detection, tracking, and forecasting (2023) 2, 7, 10
45. Weng, X., Wang, J., Held, D., Kitani, K.: 3D Multi-Object Tracking: A Baseline
and New Evaluation Metrics. IROS (2020) 2, 7
46. Weng, X., Wang, J., Levine, S., Kitani, K., Rhinehart, N.: Inverting the pose
forecastingpipelinewithspf2:Sequentialpointcloudforecastingforsequentialpose
forecasting. In: Conference on robot learning. pp. 11–20. PMLR (2021) 3
47. Wilson, B., Qi, W., Agarwal, T., Lambert, J., Singh, J., Khandelwal, S., Pan, B.,
Kumar, R., Hartnett, A., Pontes, J.K., Ramanan, D., Carr, P., Hays, J.: Argov-
erse 2: Next Generation Datasets for Self-driving Perception and Forecasting. In:
ProceedingsoftheNeuralInformationProcessingSystemsTrackonDatasetsand
Benchmarks (NeurIPS Datasets and Benchmarks 2021) (2021) 2, 3, 8, 9
48. Wu, W., Wang, Z.Y., Li, Z., Liu, W., Fuxin, L.: Pointpwc-net: Cost volume on
point clouds for (self-) supervised scene flow estimation. In: Eur. Conf. Comput.
Vis. pp. 88–107. Springer (2020) 3
49. Yan,Y.,Mao,Y.,Li,B.:SECOND:SparselyEmbeddedConvolutionalDetection.
Sensors 18(10) (2018) 4
50. Yang,J.,Zeng,A.,Zhang,R.,Zhang,L.:UniPose:DetectionAnyKeypoints.arXiv
preprint arXiv:2310.08530 (2023) 17
51. Zhai,G.,Kong,X.,Cui,J.,Liu,Y.,Yang,Z.:FlowMOT:3DMulti-ObjectTracking
by Scene Flow Association. ArXiv abs/2012.07541 (2020) 2, 18
52. Zhang, Q., Yang, Y., Fang, H., Geng, R., Jensfelt, P.: DeFlow: Decoder of Scene
Flow Network in Autonomous Driving. ICRA (2024) 1, 2, 3, 4, 8, 9, 10
53. Zheng, Y., Harley, A.W., Shen, B., Wetzstein, G., Guibas, L.J.: PointOdyssey: A
Large-Scale Synthetic Dataset for Long-Term Point Tracking. In: ICCV (2023) 3
54. Zhu,B.,Jiang,Z.,Zhou,X.,Li,Z.,Yu,G.:Class-balancedGroupingandSampling
for Point Cloud 3D Object Detection. arXiv preprint arXiv:1908.09492 (2019) 2,
4I Can’t Believe It’s Not Scene Flow! 17
A Bucket Normalized EPE Structure
Class Speed Columns
0-0.4m/s0.4-0.8m/s0.8-1.2m/s...20-∞m/s
BACKGROUND - - - - -
CAR - - - - -
OTHER VEHICLES - - - - -
PEDESTRIAN - - - - -
WHEELED VRU - - - - -
Table 3: Example of Bucket Normalized EPE’s class-speed matrix.
B FAQ
B.1 TrackFlow uses bounding boxes and thus can only do rigid flow
— what does this paper have to say about non-rigid scene flow?
It’s true that TrackFlow operates on the level of bounding boxes, but as we
discussinSection 2.1, that’sbecausepublicreal-worlddatasetsarehavemotion
annotations from bounding box motion. If non-rigid labels were available, you
couldtrainadetectortoalsoregresskeypoints(oruseanoff-the-shelfpretrained
method [50]) and then track across those keypoints.
B.2 TrackFlow uses bounding boxes from a detector — does this
mean it cannot detect open-set objects?
but
TrackFlow uses a class-aware object detector as its bounding box proposer,
the Detect + Track framework does not require class annotations – nothing
prevents the use of a class agnostic open world bounding box proposer, either
trained like the Region Proposal Network from Fast/FasterRCNN [9,37] or Ob-
ject Localization Network [14], or via geometric priors [12].
B.3 Our metric is “just” Threeway EPE extended to multiple
classes and multiple speed buckets with normalization, and our
method is “just” gluing a detector onto a tracker. Where does
the novelty come in?
The ideas presented in this paper are simple and post-hoc obvious, but that’s
the point: they are simple, and, once said aloud, they are obvious, yet clearly18 I. Khatri et al.
they must be novel because the scene flow community broadly hasn’t yet picked
up on them.
Fundamentally, modern scene flow methods struggle to detect motion of ob-
jects that aren’t a large portion of the scene (Fig. 6 shows prior supervised
methods can’t even describe 50% of pedestrian motion!), but the downstream
applicationsthatactuallywanttousesceneflowasaprimitivearehopingitcan
go beyond the obvious and extract motion cues from beyond their known object
taxonomy [30,51]. But it’s not as though getting better than 50% of pedestrian
motion is some Herculean task — in fact, there’s so much low-hanging fruit to
improve scene flow methods that we were able to strap a Kalman filter to a
detector to get a state-of-the-art scene flow method — but this only becomes
obvious when the problem is measured properly.