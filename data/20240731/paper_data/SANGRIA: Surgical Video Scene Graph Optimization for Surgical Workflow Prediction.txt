SANGRIA: Surgical Video Scene Graph
Optimization for Surgical Workflow Prediction
Çağhan Köksal1,2∗, Ghazal Ghazaei1∗, Felix Holm1,2, Azade Farshad2,3, and
Nassir Navab2,3
1 Carl Zeiss AG, Germany
2 Technische Universität München, Germany
3 Munich Center for Machine Learning, Germany
Abstract. Graph-based holistic scene representations facilitate surgi-
cal workflow understanding and have recently demonstrated significant
success. However, this task is often hindered by the limited availabil-
ity of densely annotated surgical scene data. In this work, we introduce
an end-to-end framework for the generation and optimization of sur-
gical scene graphs on a downstream task. Our approach leverages the
flexibility of graph-based spectral clustering and the generalization ca-
pabilityoffoundationmodelstogenerateunsupervisedscenegraphswith
learnable properties. We reinforce the initial spatial graph with sparse
temporalconnectionsusinglocalmatchesbetweenconsecutiveframesto
predict temporally consistent clusters across a temporal neighborhood.
By jointly optimizing the spatiotemporal relations and node features of
the dynamic scene graph with the downstream task of phase segmenta-
tion,weaddressthecostlyandannotation-burdensometaskofsemantic
scenecomprehensionandscenegraphgenerationinsurgicalvideosusing
onlyweaksurgicalphaselabels.Further,byincorporatingeffectiveinter-
mediate scene representation disentanglement steps within the pipeline,
oursolutionoutperformstheSOTAontheCATARACTSdatasetby8%
accuracy and 10% F1 score in surgical workflow recognition.
Keywords: Surgical Phase Segmentation · Scene Graph Generation ·
Unsupervised Video Segmentation · Surgical Scene Understanding
1 Introduction
Surgical videos capture pivotal moments of surgery, providing a valuable source
of information that can facilitate better insights into the quality of surgery. Au-
tomated analysis of these videos can significantly enhance surgical procedures
viaonlineorofflinefeedback.Surgicalworkflowpredictionhasbeenafocalpoint
of numerous studies, highlighting its critical role in enhancing surgical precision
andefficiencythroughvideoanalysis[27,22,4,23,11,17]Recently,methodsbased
* Equal contribution. Corresponding author email: caghankoksal@gmail.com
** This work was conducted and fully financed by the Corporate Research and Tech-
nology department of Carl Zeiss AG.
4202
luJ
92
]VC.sc[
1v41202.7042:viXra2 C. Koksal, G. Ghazaei, F. Holm, A. Farshad, N. Navab.
Dynamic Scene Graph Generation Dynamic
Input Video Segmented frames Scene Graph Surgical Phases
t1 t1
t1 t1 PhacoemP u5 ls: ification
LLiigghhttGGlluuee
Xpool,Apool
t2 t2
TS ep me pct or ra al l Dyna Gm raic p S hc ene t4 Task Prediction t2 PhacoemP u5 ls: ification
Clustering Optimization
t3 t3 ...... ......
DDMMOONN t7 Graph Classification t3 P0: Idle
...... ......
Fig.1: Our end-to-end surgical scene graph generation and workflow prediction
pipeline, SANGRIA, comprising: 1) Spectral Temporal Clustering converts
input frames into a dynamic patchified graph leveraging graph partitioning and
local feature matching to produce an initial dynamic scene graph. 2) DSG Op-
timization optimizes the edge weights between clusters for the end task. 3)
Task prediction a GCN-based architecture to predict surgical phases.
on scene graph representations overperformed non-graph methods thanks to its
holisticsceneunderstandingcapabilities[11,24,16,9,17].Aprimarybarriertode-
veloping such technologies is the lack of dense annotations. Moreover, surgical
video annotation is not only inherently burdensome but requires specialized an-
notation platforms and expert annotators, which can be prohibitively expensive
for specialized surgeries. Given the inherent interdependence of surgical scene
understanding and workflow prediction during surgical procedures, it is imper-
ative to address these two tasks simultaneously. In this regard, we introduce
a novel approach of SurgicAl SceNe GRaph OptImizAtion (SANGRIA) that
tackles both problems using solely surgical phase labels.
Priorworksonunsupervisedsceneandvideosegmentationtakeadvantageof
optical flow [10] or similar to CutLER [28,29] use variations of NormalizedCut
[25] and self-training [30] to find salient objects in the scene. For surgical scene
understanding, similarly, optical flow or shape priors have been used [31,21].
Shapepriorslackadaptabilityandflexibilitytonewtoolsorsetupsandstruggle
when visually similar tools are present. While optical flow is more generaliz-
able,itintroducesnoisefromanatomicalmovementorfluidduringsurgery.The
main shortcoming with solutions such as [28,29] is the burdensome process of
mask generation followed by rounds of self-training, which would be needed
on every new dataset. The advent of foundation models [6,18,13] has opened
new opportunities for scene understanding with minimal annotations. In the
context of surgical scenes, this impact is still less pronounced, considering the
significant domain gap between common computer vision use cases and medi-
cal applications, requiring further fine-tuning. Previous research demonstrated
that (dynamic) scene graph representations provide a more holistic and inter-
pretable representation of surgical procedures for surgical workflow recognition
[11,17]. To mitigate the annotation scarcity in surgical videos and address both
... ... ... ...SANGRIA 3
scene understanding and surgical workflow recognition simultaneously, we pro-
poseanoveldata-agnosticpipelinethatintegratesthebenefitsofsemanticscene
graphs and graph-based unsupervised semantic scene segmentation techniques.
Our approach comprises two main components:
First, a task- and data-agnostic spectral and temporal clustering module
leveraging DMON layer [26] for unsupervised clustering of scene components
reinforced with LightGlue [15] for sparse and lightweight local feature matching
amongneighboringframes.Actingasazero-shotsemanticscenesegmenter,this
module ensures local and temporal consistency while optimizing scene represen-
tationfordownstreamtasks.Oursolutionbuildsuponinsightsfromrecentworks
[1,5], formulating semantic segmentation as a graph partitioning task. While
these models indicate promising performance on still images, they lack tempo-
ral consistency among neighboring frames when applied to video sequences. To
overcomethischallenge,weleverageLightGlue[15]intoourdynamicgraphcon-
struction, which predicts an assignment between points based on their pairwise
similarity and unary matchability. This leads to sparse dynamic links among
close frames and significantly reduced computations.
Second, a dynamic scene graph (DSG) generation module optimizing graph
relations based on the downstream task. The end-to-end pipeline is jointly opti-
mized for the specific downstream task, which is surgical phase segmentation in
thiswork.Asthepredictedsemanticsegmentationmapsandthegeneratedscene
graphs lack object class identities, we propose a simple yet efficient prototype
matchingstrategyviawhichonlyafewprototypesareexploitedtoretrieveclass
identities. In this work, we introduce SANGRIA, a novel graph-based pipeline
harnessing the holisticness and flexibility of graph-based learning to overcome
the challenge of annotation scarcity in surgical video analysis.
Our contributions are as follows: 1) Unsupervised semantic scene segmenta-
tionanddynamicscenegraphgenerationleveragingaspectralandtemporalclus-
tering module equipped with lightweight correspondence matching. 2) Demon-
stratingtheimportanceofunderstandingscenecomponentsandtheirspatiotem-
poralrelationsincontributingtothedownstreamtask.Bydisambiguatingvideo
representations in an end-to-end optimization setup, SANGRIA achieves state-
of-the-art performance on surgical phase segmentation on the CATARACTS [2]
dataset. 3) Offering a few-shot prototype matching mechanism enabling fur-
ther refinement of the predictions and demonstrating promising performance in
annotation-efficient scene graph generation.
2 Methodology
We tackle the problem of semantic scene understanding and scene graph gener-
ationwithanend-to-endgraph-basedpipeline.Formulatingsemanticsegmenta-
tion as a graph partitioning task, we patchify input images and establish sparse
temporal links with the neighboring frame patches via correspondence match-
ing, constructing a dynamic graph (the patch-based graph hereafter). We then
perform a spectral, temporal clustering of the patch-based graph to generate a4 C. Koksal, G. Ghazaei, F. Holm, A. Farshad, N. Navab.
dynamicsemanticscenegraph.ThisDSGisaugmentedwithadynamicrelation
predictionmoduletobefurtherrefinedforthedownstreamtaskofsurgicalphase
segmentation. Finally, a prototype matching mechanism is developed for a final
refinement, few-shot segmentation, and scene graph generation evaluations.
2.1 Dynamic Scene Graph Generation
ForaninputimageI,DINOkeyfeaturesf areobtainedbypartitioningI inton
patches and passing them to DINO. The adjacency matrix A is then generated
by patchwise dot product as follows:
(cid:40)
f ·f if f ·f >0
A = i j i j (1)
ij
0 otherwise
Next, we threshold the values in A>τ and connect highly similar nodes to-
gether, generating a static patch graph representation G = (V ,A ) for a given
t t t
frame t. An extension of graph clustering to sequences of frames without con-
sidering the temporal inter-dependencies leads to inconsistent clusters among
frames with close proximity. A naïve solution could be an expansion of adja-
cencymatrixcalculationacrossthethirddimensionoftimetofindspatiotempo-
ral similarities across patches. This leads to high computational costs O(wn2d)
specifically with the increasing length of the temporal window for n number of
patches, patch feature of length d, and w time steps. As temporal relations re-
quireacoarserlevelofattentioncomparedtospatialdependencies[7],wesuggest
asparsedynamiclinkingmechanismbetweenpatchesalongthetimedimension.
Inthiswork,weleveragecorrespondencematchingtofindprominentfeatures
within frames and match those efficiently. We incorporate LightGlue [15], a dis-
tilleddeepneuralnetworkpoweredwithself-andcross-attention,intoourpatch
graph construction setup. It is designed explicitly for low-latency problems and
sparse inputs by predicting matches from two sets of local features. Next, for
a clip with w frames, we construct a dynamic patch-based graph, G ti→−ti+w =
(V,E) with node set V, edge set E, node features X ∈ Rw×n×d. Spatial edges,
E are established using pairwise correlation similarity between those features
ti
(Equation1),whilefortemporaledgesE ti→−ti+1,LightGluecorrespondences be-
tween frames within a temporal sequence of w time stamps are exploited. Dy-
(cid:83)
namic graph edges can be represented as follows: E = 1≤t≤wE ti +E ti→−ti+1
We further reinforce the graph nodes with temporal and spatial encodings
to accentuate the dynamic relations between objects in the scene. Temporal
encodings capture the temporal order of object interactions and actions in a
video sequence, while spatial encodings capture objects’ relative positions and
orientations in a scene. For temporal encoding, we incorporate the location of
each frame along the temporal window by adding a temporal feature vector to
the node feature matrix X. For spatial encoding, we incorporate the position of
patcheswithintheframebyaddingaspatialfeaturevectortofeaturematrixX.
The graph clustering is performed by employing deep modularity networks
(DMON) [26] featuring a collapse regularization objective to improve unsuper-SANGRIA 5
w
LightGlue
n x n Xpool, Apool
n x 11n 22 33 ... message
11 4422 55 33 66 VViiTT passing
44 55 66
77 88 99 DMON
77 88 99
...
time time time
Patchified Input Patch-wise Affinity Matrices Graph Clustering Dynamic Scene Graph
Fig.2:DSGgeneration:ApatchifiedinputimageisfedintoDINOtoconstruct
apatch-wiseaffinitymatrix.Thestaticpatch-basedinputgraphsforneighboring
frames within a window of w are temporally linked via sparse matches provided
by LightGlue [15]. The dynamic patch-based graph is then clustered to predict
a DSG for the last frame of the window.
vised graph clustering in real-world scenarios. The DMON module comprises
multiple graph convolutional layers [12], MLP layers with softmax, as well as a
SeLUactivationfunction.TheinputgraphX isfedintotheDMONmoduletobe
clustered,optimizingthemodularityandcollapseregularizationobjectives.This
leadstoapooledscenegraphGp tio →−ol
ti+w
=(V tp io →−ol ti+w,E tp io →−ol ti+w)withpoolednode
features X tp io →−ol
ti+w
∈ RK×d and pooled adjacency matrix Ap tio →−ol
ti+w
∈ RK×K,
where K is the number of clusters. This DSG incorporates the summary of a
window, that is, the scene components, their relations, and motion along the
time. Figure 2 illustrates the DSG generation workflow.
DSG Optimization We establish a probabilistic estimation of edge weights
within the DSGs via equation Wpool = σ(MLP(Xpool ;Θ )), where
ti→ti+w ti→ti+w MLP
W tp io →−ol
ti+w
refers to the edge weights between the clusters of DSG, in which
w ∈ [0,1] indicating the strength of the relations between clusters i, j. Θ
ij MLP
represents the set of trainable parameters. The probabilistic setup allows for
flexibility in optimizing the edge weights while learning the downstream tasks
and equips the DSG generation to account for the inherent uncertainty and
variability present in the unsupervised graph clustering results, leading to more
robust and accurate inference.
2.2 End-to-end Pipeline
To tackle the task of phase segmentation, we propose a multi-layer GCN [12,11]
thattakestheDSGasinput.TheGCNconsistsofmultiplelayers,eachofwhich
enables learning increasingly complex representations of the scene graph. The
output of the GCN is fed to a global sum-pooling layer aggregating features
fromallnodesinthegraph.Afully-connectedlayerandasoftmaxfunctionpre-
dictprobabilitiesforeachphaseclass.Across-entropyobjectivefunctionL is
CE
employed to optimize the model parameters for the surgical phase segmentation
task.Thefinalobjectivefunctionoftheend-to-endpipelinecanthereforebefor-6 C. Koksal, G. Ghazaei, F. Holm, A. Farshad, N. Navab.
mulatedasL =L +L .ThefirsttermcorrespondstoDMONloss,while
joint u CE
the second term can be replaced with any other downstream task optimization
loss for each specific use case. This joint optimization provides a tailored repre-
sentationofthesceneforthedownstreamtaskandreducesthenoisesintroduced
during unsupervised steps by taking advantage of the classification labels.
Prototype MatchingToassignsemanticclassestoDSGnodes,wecreatepro-
totypesbyleveragingDINOpatchfeaturesandgroundtruth(GT)segmentation
annotations.Exploitingonly5annotationsperclass,patchfeaturescorrespond-
ing to GT segmentation masks are used as prototype patches. We use the mean
of the patch features as a prototype for that object class. To predict the se-
mantic categoryof anode (cluster), apairwise cosinesimilaritywith prototypes
is calculated. Since different clusters might represent the same region, such as
surgical tape, we used argmax to assign class labels to clusters.
3 Implementation Details
DatasetsWeexperimenton3datasets:CATARACTS[3,2]consistsof50cataract
surgery videos of 1920×1080 pixels at 30 fps. The dataset is split 25-5-20 for
training, validation, and testing, with videos annotated on 19 surgical phases.
CaDISdataset, asubset ofCATARACTS, consistsof 4670 pixel-wise annotated
images. We use Task II of CaDIS, which defines 17 classes of objects, includ-
ingsurgicaltools,anatomicalstructures,andmiscellaneous.Cataract101(C101)
comprises 101 videos of 720×540 pixels and 25 fps performed by surgeons with
various levels of expertise. The videos are annotated based on the 11 most com-
monphasesofcataractsurgeryandusedwith45-5-50train-validation-testsplits.
Training details Frames are resized to 224x224 to generate DINO-B embed-
dings. For graph generation, a frame similarity threshold of 0.9 is chosen. We
trainedphasesegmentationmodelsfor100epochsusinganAdamoptimizerwith
a learning rate of 0.0001 and a batch size of 32 on a single A40 GPU.
Metrics For phase segmentation, we compute the accuracy and F1 score. For
semantic segmentation, we measure the mean intersection over union (mIoU)
and pixel-wise accuracy (PAC).
I’m
4 Results & Discussion
Surgical Workflow PredictionTable1presentsanablationstudyonwindow
size and spatial and temporal embeddings as well as a comparative analysis of
ourproposedmethodagainstexistingtechniquesonphasesegmentationtasksfor
CATARACTS and Cataract101 datasets. We show that increasing window size
together with temporal embeddings improves phase segmentation performance,
while spatial embeddings have minimal impact. Our method demonstrates su-
periorperformanceintermsofaccuracyandF1scorebyindicating8%accuracy
and 10% F1 score improvement over previous graph-based phase segmentationSANGRIA 7
Table 1: Comparison of DMON-based Phase segmentation performance on
CATARACTS [2] and CAT101 [20] datasets. * indicates our implementation.
Method Graph SpatialTemp WS Accuracy F1
DINO-TCN++* [14]Non-graph full video 77.02 74.37
Holm et al.[11] Static 1 64.34 50.04
Holm et al.[11] Dynamic ✓ ✓ 30 75.15 68.56
SANGRIA Static 1 77.53 69.29
SANGRIA Dynamic 4 75.50 67.76
SANGRIA Dynamic 8 74.18 68.50
SANGRIA Dynamic ✓ 4 75.96 67.07
SANGRIA Dynamic ✓ 4 80.67 75.35
SANGRIA Dynamic ✓ ✓ 4 81.62 74.62
SANGRIA Dynamic ✓ 8 74.44 64.44
SANGRIA Dynamic ✓ 8 81.85 78.47
SANGRIA Dynamic ✓ ✓ 8 82.13 75.17
SANGRIA Dynamic ✓ 16 83.36 78.24
ViT [8] Non-graph 1 84.56 -
TimesFormer [4] Non-graph 8 90.76 -
GLSFormer [22] Non-graph 8 92.91 -
SANGRIA (Ours) Dynamic 8 91.26 85.02
SOTA [11]. As there are no other benchmarks on the CATARACTS dataset,
we add an additional non-graph-based solution representing the best practices
in the surgical workflow. It exploits DINO as a strong spatial feature extractor
followed by long-range temporal learning via TCN++[14]. Our dynamic end-to-
end setup outperforms the CNN baseline with 6% accuracy and 4% F1 score.
OurmethodconsistentlyyieldscloselycompetitiveresultsonCataract101.This
demonstratestherobustnessandadaptabilityofourapproach,providingasolid
foundation for further refinement and application in diverse contexts.
Semantic Segmentation Table 2 indicates that increasing the number of
temporalconnections(loweringtemporalsimilaritythreshold)infully-connected
graph increases segmentation performance on both anatomy and tools. Con-
versely, due to high computational costs with the increasing length of the tem-
poral window, fully-connected graphs have limited use for end tasks since sur-
gical phase recognition performance heavily depends on temporal information.
LightGlue correspondences effectively facilitate better temporal learning while
maintaining comparable performance and considerably reducing the amount of
computation. Table 3 indicates the prior performance of the unsupervised seg-
mentation performance of our proposed dynamic graph clustering algorithm to
SOTA object discovery method MaskCut [28]. Both tables emphasize the diffi-
culty of tool localization via mIoU while mIoU indicates higher segmen-
ins ana
tation performance for anatomical structures. In all setups, DMON consistently
outperforms other solutions. In Figure 3 A, we qualitatively evaluate MaskCut
[28] and variations of our segmentation solution. Our method leverages tempo-
ralconnectionsandpredictstemporallyconsistentsemanticsegmentationmaps.
STCARATAC
101C8 C. Koksal, G. Ghazaei, F. Holm, A. Farshad, N. Navab.
Fig.3: A) Comparison of various graph clustering setups (WS corresponds to
windowsize).B)End-to-endoptimizationimprovestheclassificationof’Primary
Knife’ since it plays a critical role in predicting the current phase.
UnlikeMaskCut,italsopredictsseparatemasksforanatomyandsurgicaltools.
DSG Generation Figure 3 B manifests improvement of joint optimization of
DSG generation with phase segmentation on surgical tool segmentation. The
jointlyoptimizedmodelclassifiesthe"PrimaryKnife"betterthantheperframe
optimized model. We hypothesize that guidance towards the downstream task
significantly improves attention to tools with high importance in the current
surgical phase. Our DSG also indicates better explainability by highlighting the
importance of graph edges in the end task.
Table2:Ablationstudyonthetempo- Table 3: Comparison of semantic seg-
ral connections. mentation on CaDIS sequences.
WSTemp.SimTemp.Conn PAC mIoUmIoUanamIoUins PoolMethodWSTemp.Conn PAC mIoUmIoUanamIoUins
1 - - 65.34 33.86 51.44 9.46 Maskcut* 1 - 60.56 24.75 33.34 6.23
3 - LightGlue 65.25 34.03 50.36 8.68 MincutPool 1 - 61.77 29.82 46.00 4.32
3 0.9 FC 65.21 33.79 51.27 9.51 DMON 1 - 65.3433.86 51.44 9.46
3 0.5 FC 65.3734.09 51.50 9.98 MincutPool 3 LightGlue 62.66 29.84 45.04 3.20
DMON 3 LightGlue 65.2534.03 50.36 8.68
5 Conclusion
We introduce SANGRIA, an end-to-end graph-based solution for concurrent
surgical workflow recognition, semantic scene segmentation, and dynamic scene
graph generation. Our jointly optimized setup featuring sparse temporal con-
nections and graph clustering, prioritizes the graph generation for the down-
stream task by disambiguating the graph and highlighting the most influential
componentsandtheirconnections.Byfocusingondownstreamtask-specificfea-
tures, we achieve state-of-the-art results in surgical phase segmentation on the
CATARACTS dataset while generating scene explanations with minimal anno-
tation.SANGRIA 9
Acknowledgments. This work was conducted and fully financed by the Corporate
Research and Technology department of Carl Zeiss AG.
Disclosure of Interests. The authors have no competing interests to declare that
are relevant to the content of this article.10 C. Koksal, G. Ghazaei, F. Holm, A. Farshad, N. Navab.
References
1. Aflalo, A., Bagon, S., Kashti, T., Eldar, Y.: Deepcut: Unsupervised segmentation
using graph neural networks clustering. In: Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision. pp. 32–41 (2023)
2. AlHajj,H.,Lamard,M.,Conze,P.H.,Roychowdhury,S.,Hu,X.,Maršalkaite˙,G.,
Zisimopoulos, O., Dedmari, M.A., Zhao, F., Prellberg, J., et al.: Cataracts: Chal-
lenge on automatic tool annotation for cataract surgery. Medical image analysis
52, 24–41 (2019)
3. ALHAJJ, H., Lamard, M., Conze, P.h., Cochener, B., Quellec, G.: Cataracts
(2021). https://doi.org/10.21227/ac97-8m18, https://dx.doi.org/10.21227/
ac97-8m18
4. Bertasius, G., Wang, H., Torresani, L.: Is space-time attention all you need for
video understanding? In: ICML. vol. 2, p. 4 (2021)
5. Bianchi, F.M., Grattarola, D., Alippi, C.: Spectral clustering with graph neural
networks for graph pooling. In: International conference on machine learning. pp.
874–883. PMLR (2020)
6. Caron,M.,Touvron,H.,Misra,I.,Jégou,H.,Mairal,J.,Bojanowski,P.,Joulin,A.:
Emerging properties in self-supervised vision transformers. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 9650–9660 (2021)
7. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the
kineticsdataset.In:proceedingsoftheIEEEConferenceonComputerVisionand
Pattern Recognition. pp. 6299–6308 (2017)
8. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth
16x16 words: Transformers for image recognition at scale. In: International Con-
ference on Learning Representations (2020)
9. Farshad, A., Yeganeh, Y., Chi, Y., Shen, C., Ommer, B., Navab, N.: Scenegenie:
Scene graph guided diffusion models for image synthesis. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 88–98 (2023)
10. Francesco,L.,Dirk,W.,Thomas,U.,Aravindh,M.,Georg,H.,Jakob,U.,Alexey,
D., Thomas, K.: Object-centric learning with slot attention. Adv. Neural Inform.
Process. Syst 33, 11525–11538 (2020)
11. Holm, F., Ghazaei, G., Czempiel, T., Özsoy, E., Saur, S., Navab, N.: Dynamic
scene graph representation for surgical video. In: Proceedings of the IEEE/CVF
International Conference on Computer Vision. pp. 81–87 (2023)
12. Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907 (2016)
13. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint
arXiv:2304.02643 (2023)
14. Li, S.J., AbuFarha, Y., Liu, Y., Cheng, M.M., Gall, J.: Ms-tcn++: Multi-stage
temporal convolutional network for action segmentation. IEEE transactions on
pattern analysis and machine intelligence (2020)
15. Lindenberger, P., Sarlin, P.E., Pollefeys, M.: Lightglue: Local feature matching at
light speed. arXiv preprint arXiv:2306.13643 (2023)
16. Murali, A., Alapatt, D., Mascagni, P., Vardazaryan, A., Garcia, A., Okamoto, N.,
Mutter, D., Padoy, N.: Latent graph representations for critical view of safety
assessment. arXiv preprint arXiv:2212.04155 (2022)SANGRIA 11
17. Murali, A., Alapatt, D., Mascagni, P., Vardazaryan, A., Garcia, A., Okamoto, N.,
Mutter, D., Padoy, N.: Encoding surgical videos as latent spatiotemporal graphs
for object and anatomy-driven reasoning. In: International Conference on Medi-
cal ImageComputingand Computer-Assisted Intervention. pp. 647–657. Springer
(2023)
18. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V.,
Fernandez,P.,Haziza,D.,Massa,F.,El-Nouby,A.,etal.:Dinov2:Learningrobust
visual features without supervision. arXiv preprint arXiv:2304.07193 (2023)
19. Pissas, T., Ravasio, C.S., Da Cruz, L., Bergeles, C.: Effective semantic segmenta-
tion in cataract surgery: What matters most? In: Medical Image Computing and
Computer Assisted Intervention – MICCAI 2021. pp. 509–518 (2021)
20. Schoeffmann, K., Taschwer, M., Sarny, S., Münzer, B., Primus, M.J., Putzgruber,
D.: Cataract-101: video dataset of 101 cataract surgeries. In: Proceedings of the
9th ACM multimedia systems conference. pp. 421–425 (2018)
21. Sestini,L.,Rosa,B.,DeMomi,E.,Ferrigno,G.,Padoy,N.:Fun-sis:Afullyunsu-
pervised approach for surgical instrument segmentation. Medical Image Analysis
85, 102751 (2023)
22. Shah, N.A., Sikder, S., Vedula, S.S., Patel, V.M.: Glsformer: Gated-long, short
sequencetransformerforsteprecognitioninsurgicalvideos.In:InternationalCon-
ference on Medical Image Computing and Computer-Assisted Intervention. pp.
386–396. Springer (2023)
23. Sharma,S.,Nwoye,C.I.,Mutter,D.,Padoy,N.:Rendezvousintime:anattention-
basedtemporalfusionapproachforsurgicaltripletrecognition.InternationalJour-
nal of Computer Assisted Radiology and Surgery pp. 1–7 (2023)
24. Sharma, S., Nwoye, C.I., Mutter, D., Padoy, N.: Surgical action triplet detection
by mixed supervised learning of instrument-tissue interactions. In: International
Conference on Medical Image Computing and Computer-Assisted Intervention.
pp. 505–514. Springer (2023)
25. Shi,J.,Malik,J.:Normalizedcutsandimagesegmentation.IEEETransactionson
pattern analysis and machine intelligence 22(8), 888–905 (2000)
26. Tsitsulin, A., Palowitch, J., Perozzi, B., Müller, E.: Graph clustering with graph
neural networks. Journal of Machine Learning Research 24(127), 1–21 (2023)
27. Twinanda, A.P., Shehata,S., Mutter,D.,Marescaux, J.,De Mathelin, M.,Padoy,
N.:Endonet:adeeparchitectureforrecognitiontasksonlaparoscopicvideos.IEEE
transactions on medical imaging 36(1), 86–97 (2016)
28. Wang, X., Girdhar, R., Yu, S.X., Misra, I.: Cut and learn for unsupervised object
detectionandinstancesegmentation.In:ProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition. pp. 3124–3134 (2023)
29. Wang, X., Misra, I., Zeng, Z., Girdhar, R., Darrell, T.: Videocutler: Surprisingly
simpleunsupervisedvideoinstancesegmentation.arXivpreprintarXiv:2308.14710
(2023)
30. Wang,Y.,Shen,X.,Yuan,Y.,Du,Y.,Li,M.,Hu,S.X.,Crowley,J.L.,Vaufreydaz,
D.:Tokencut:Segmentingobjectsinimagesandvideoswithself-supervisedtrans-
former and normalized cut. IEEE Transactions on Pattern Analysis and Machine
Intelligence (2023)
31. Zhao, Z., Jin, Y., Gao, X., Dou, Q., Heng, P.A.: Learning motion flows for semi-
supervisedinstrumentsegmentationfromroboticsurgicalvideo.In:MedicalImage
ComputingandComputerAssistedIntervention–MICCAI2020:23rdInternational
Conference,Lima,Peru,October4–8,2020,Proceedings,PartIII23.pp.679–689.
Springer (2020)SANGRIA 1
A SANGRIA: Supplementary Material
Table 1: Comparison of fully-supervised segmentation(OCRNET[19])-based [11]
and SANGRIA (Ours) on scene graph node classification on CaDIS sequences.
While the supervised model demonstrates superior performance, our weakly-
supervised approach offers promsing results on most influential objects within a
surgical scene and highlight the importance of their interactions.
Base[11] End-to-end
Class
PrecisionRecallF1-Score PrecisionRecallF1-Score Support
Pupil 1.00 1.00 1.00 1.00 0.98 0.99 1381
Tape 0.94 0.96 0.95 0.93 0.47 0.62 1101
Hand 0.64 0.32 0.43 0.00 0.00 0.00 218
Retractors 0.93 0.91 0.92 1.00 0.02 0.04 1030
Iris 1.00 1.00 1.00 1.00 0.78 0.88 1382
Skin 1.00 1.00 1.00 1.00 0.97 0.98 1381
Cornea 1.00 1.00 1.00 1.00 0.99 0.99 1382
Cannula 0.80 0.43 0.56 0.95 0.07 0.12 628
Cystotome 0.34 0.55 0.42 0.06 0.18 0.09 33
T. Forceps 0.77 0.43 0.55 1.00 0.00 0.02 227
Pr. Knife 0.79 0.32 0.46 0.66 0.14 0.24 228
Phaco 0.50 1.00 0.67 0.03 0.03 0.33 4
Lens Inj. 0.86 0.45 0.59 0.98 0.27 0.42 223
I/A 0.19 0.82 0.31 0.50 0.29 0.37 19
Sec. Knife 0.66 0.30 0.42 1.00 0.01 0.02 187
Micromanip. 0.14 0.45 0.22 0.00 0.00 0.00 13
Cap. Forceps 0.08 0.15 0.11 0.00 0.00 0.00 20
micro avg 0.94 0.87 0.91 0.96 0.61 0.75 9457
macro avg 0.68 0.65 0.62 0.65 0.32 0.34 9457
weighted avg 0.94 0.87 0.89 0.95 0.61 0.66 94572 C. Koksal, G. Ghazaei, F. Holm, A. Farshad, N. Navab.
Fig.1: Comparison of phase segmentation performance on CATARACTS [3,2]
test videos. A) Ground Truth phases. B) Predictions of the best dynamic model
of [11]. C) Best model of the SANGRIA(Ours). Our model predicts surgical
phases such as Irrigation/Aspiration, OVD Aspiration and Nucleus Breaking
moreconsistentlythankstoscenerepresentationandunderstandingcapabilities
of the SANGRIA.
Table2:ComparisonofperframeoptimizationofMincutPool[5]andDMON[26]
withourend2endsolutiononscenegraphnodeclassificationofCaDISsequences.
Theend-to-endmodelsignificantlyimprovesthenodeclassificationperformance
oftheLensInjector,Irrigation/Aspirationtools,whicharethemaincomponents
inOVDAspiration,Irrigation/Aspiration,andImplantInjectionphases.DMON
consistently demonstrates better scene graph node classification performance
than MincutPool.
MincutPool Dmon End-to-end
Class
PrecisionRecallF1-ScorePrecisionRecallF1-ScorePrecisionRecallF1-ScoreSupport
Pupil 1.00 0.93 0.97 1.00 0.96 0.98 1.00 0.98 0.99 1381
Tape 0.94 0.54 0.69 0.93 0.60 0.73 0.93 0.47 0.62 1101
Hand 0.91 0.29 0.44 0.95 0.38 0.54 0.00 0.00 0.00 218
Retractors 1.00 0.03 0.05 1.00 0.03 0.06 1.00 0.02 0.04 1030
Iris 1.00 0.71 0.83 1.00 0.81 0.90 1.00 0.78 0.88 1382
Skin 1.00 0.88 0.94 1.00 0.94 0.97 1.00 0.97 0.98 1381
Cornea 1.00 0.94 0.97 0.90 0.98 0.99 1.00 0.99 0.99 1382
Cannula 0.96 0.04 0.08 0.93 0.02 0.05 0.95 0.07 0.12 628
Cystotome 0.09 0.15 0.11 0.08 0.15 0.11 0.06 0.18 0.09 33
T.Forceps 0.98 0.18 0.30 1.00 0.30 0.47 1.00 0.00 0.02 227
Pr.Knife 0.55 0.10 0.17 0.54 0.11 0.19 0.66 0.14 0.24 228
Phaco 0.00 0.00 0.00 0.02 0.25 0.03 0.03 0.33 0.03 4
LensInj. 0.93 0.17 0.29 0.98 0.23 0.37 0.98 0.27 0.42 223
I/A 0.00 0.00 0.00 0.00 0.00 0.00 0.50 0.29 0.37 19
Sec.Knife 1.00 0.06 0.11 0.79 0.06 0.11 1.00 0.01 0.02 187
Micromanip. 0.03 0.15 0.05 0.04 0.18 0.06 0.00 0.00 0.00 13
Cap.Forceps 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 20
microavg 0.96 0.59 0.73 0.95 0.64 0.77 0.96 0.61 0.75 9457
macroavg 0.67 0.30 0.35 0.66 0.35 0.40 0.65 0.32 0.34 9457
weightedavg 0.97 0.59 0.66 0.96 0.64 0.70 0.95 0.61 0.66 9457