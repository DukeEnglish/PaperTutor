TechnicalReport
MindSearch
思·索
:
Mimicking Human Minds Elicits Deep AI Searcher
ZehuiChen1∗,KuikunLiu2∗,QiuchenWang1,JiangningLiu2,
WenweiZhang2,KaiChen2†,FengZhao1†
1UniversityofScienceandTechnologyofChina
2ShanghaiAILaboratory
Abstract
Informationseekingandintegrationisacomplexcognitivetaskthatconsumesenor-
moustimeandeffort. Searchenginesreshapethewayofseekinginformationbutoften
failtoalignwithcomplexhumanintentions. Inspiredbytheremarkableprogressof
LargeLanguageModels(LLMs),recentworksattempttosolvetheinformation-seeking
andintegrationtaskbycombiningLLMsandsearchengines. However,thesemethods
stillobtainunsatisfyingperformanceduetothreechallenges: (1)complexrequestsoften
cannot be accurately and completely retrieved by the search engine once; (2) corre-
spondinginformationtobeintegratedisspreadovermultiplewebpagesalongwith
massivenoise; and(3)alargenumberofwebpageswithlongcontentsmayquickly
exceedthemaximumcontextlengthofLLMs. Inspiredbythecognitiveprocesswhen
humanssolvetheseproblems,weintroduceMindSearch(思·索)tomimicthehuman
minds in web information seeking and integration, which can be instantiated by a
simple yet effective LLM-based multi-agent framework consisting of a WebPlanner
andWebSearcher. TheWebPlannermodelsthehumanmindofmulti-stepinformation
seekingasadynamicgraphconstructionprocess: itdecomposestheuserqueryinto
atomicsub-questionsasnodesinthegraphandprogressivelyextendsthegraphbased
onthesearchresultfromWebSearcher. Taskedwitheachsub-question,WebSearcher
performshierarchicalinformationretrievalwithsearchenginesandcollectsvaluable
informationforWebPlanner. Themulti-agentdesignofMindSearchenablesthewhole
framework to seek and integrate information parallelly from larger-scale (e.g., more
than300)webpagesin3minute,whichisworth3hoursofhumaneffort. Basedon
eitherGPT-4oorInternLM2.5-7Bmodels,MindSearchdemonstratessignificantimprove-
ment in the response quality in terms of depth and breadth, on both closed-set and
open-setQAproblems. Besides,responsesfromMindSearchbasedonInternLM2.5-7B
arepreferablebyhumanstoChatGPT-Web(byGPT-4o)andPerplexity.aiapplications,
whichimpliesthatMindSearchwithopen-sourcemodelscanalreadydeliveracompet-
itivesolutiontotheproprietaryAIsearchengine. Codeandmodelsareavailableat
https://github.com/InternLM/MindSearch.
1 Introduction
Informationseekingandintegrationisanecessarycognitiveprocessbeforeanalysisanddecision-
making in all walks of life, which usually consumes enormous human efforts and time. The
birthofsearchengines(Brin&Page,1998;Berkhin,2005)significantlyhasreshapedandeasedthe
information-seekingprocessofhumansociety,however,itstillsuffersinintegratingwebinformation
basedoncomplexhumanintentions. Recently,LargeLanguageModels(LLMs)haveshowcased
ProjectPage:https://mindsearch.netlify.app
∗EqualContribution.Alphabeticalorder †Correspondingauthor
1
4202
luJ
92
]LC.sc[
1v38102.7042:viXraTechnicalReport
Figure1: TheoverallframeworkofMindSearch. Itconsistsoftwomainingredients: WebPlanner
andWebSearcher. WebPlanneractsasahigh-levelplanner,orchestratingthereasoningstepsand
multipleWebSearchers. WebSearcherconductsfine-grainedwebsearchesandsummarizesvaluable
informationbacktotheplanner,formalizingasimpleyeteffectivemulti-agentframework.
remarkable progress in reasoning, language understanding, and information integration across
avarietyofdomains(Achiametal.,2023;Teametal.,2024;Touvronetal.,2023;Caietal.,2024),
whereastheystrugglingtodeliveraccurateknowledgeinresponses(Jietal.,2023;Guetal.,2024).
ThecomplementaryadvantagesofLLMsandsearchengineshighlightsacompellingopportunity
fortheircombination,wherethereasoningprowessofLLMscanbecomplementedbytheextensive
web information accessible via search engines, potentially revolutionizing the solution of web
informationseekingandintegration. Previousworks(Asaietal.,2023;Chanetal.,2024)simply
treattheinformationseekingandintegrationtaskasavanillaretrieve-augmentedgeneration(RAG)
task(Chenetal.,2017;Linetal.,2023). Suchaformulation,althoughstraightforward,oftenresults
in sub-optimal performance due to a superficial engagement with the depth and complexity of
web-basedinformationretrieval,facingthreemajorchallengesformorecomplexuserqueries:
(1)Real-worldproblemsoftenrequirein-depthanalysisandproperdecompositionofthequestion
beforeretrievingtherelatedinformation,whichcannotbedonebyretrievingwebpagesatonce.
(2)Theoverwhelmingvolumeofsearchedwebpagesandmassiveinformationnoiseposegreat
challengesforLLMsforefficientinformationintegration.
(3)Therapidproliferationofwebsearchcontentcanquicklyexceedthemaximumcontextlengthof
LLMs,whichfurtherdecreasestheinformationintegrationperformance.
Inspired by how human experts solve real-world problems, we propose MindSearch 思·索1, a
simpleyeteffectiveLLM-basedmulti-agentframework,whichconsistsofaWebPlanner(mimic
humanmindsforproblemreasoning)andmultipleWebSearcher(managetheinformationseeking
1TheChinesename’思·索‘meansthinkingashumanandexploringbysearching
2TechnicalReport
process). Given a user query, the WebPlanner first decomposes the query into multiple atomic
sub-questionsthatcanbeparallellysolvedanddispatchesthemtotherespectiveWebSearcher. To
furtherenhancethereasoningability,WebPlannermodelsthecomplexproblem-solvingprocess
asaniterativegraphconstruction: bypredefiningalistofstandardcodeinterfacesrelatedtothe
constructionofthetopologicalmindgraph,WebPlannerisabletoprogressivelydecomposethe
questionintosequential/parallelsub-problemsbyaddingnodes/edgesinthegraphviaPythoncode
generation. Meanwhile,theWebSearcher,taskedwitheachsub-problem,employsahierarchical
retrievalprocesstoextractvaluabledataforLLMs,whichsignificantlyimprovestheinformation
aggregationefficiencyfacingmassivesearchpages. Bydistributingdifferentaspectsofthereasoning
andretrievalprocesstospecializedagents,MindSearcheffectivelyreducestheloadoneachsingle
agent,facilitatingamorerobusthandlingoflongcontexts. Itseamlesslybridgesthegapbetween
therawdataretrievalcapabilitiesofsearchenginesandthecontext-understandingpowerofLLMs.
TovalidatetheeffectivenessofMindSearch,weconductedextensiveevaluationsonbothclosed-set
andopen-setquestion-answering(QA)problemsusingGPT-4oandInternLM2.5-7B-Chatmodels.
Theexperimentalresultsdemonstrateasubstantialimprovementinresponsequality,bothinthe
dimensions of depth and breadth. Moreover, comparative analysis shows that the responses of
MindSearch are more preferred by human evaluators over those from existing applications like
ChatGPT-Web(basedonGPT-4o)andPerplexityPro. ThesefindingssuggestthatMindSearchwith
open-sourceLLMscanofferahighlycompetitivesolutionforAI-drivensearchengines.
2 MindSearch
Toeffectivelysynergizethewebinformationretrievalcapabilitiesofsearchenginesandthereasoning
andinformationintegrationcapabilityofLLMs,MindSearchconsistsofaWebPlannerandagroup
ofWebSearchers(Fig.1). WebPlannerfirstdecomposestheuserquestionintosequentialorparallel
searchtasksviareasoningonthegraphanddeterminesthenextstepbasedonthesearchfeedback
(Sec.2.1). WebSearcheristaskedwiththequeryandperformshierarchicalinformationretrievalon
theInternettoanswersub-questions(Sec.2.2). Wealsodiscussthecontextmanagementwithinthe
scopeofthemulti-agentdesigninSec.2.3.
2.1 WebPlanner: PlanningviaGraphConstruction
TheWebPlannerfunctionsasahigh-levelplanner,orchestratingthereasoningstepsandcoordinating
other agents. However, we observed that merely prompting the LLM to plan the entire data
workflowarchitecturedoesnotyieldsatisfactoryperformance. Specifically,currentLLMsstruggle
withdecomposingcomplexquestionsandunderstandingtheirtopologicalrelationships,leading
tocoarse-grainedsearchqueries. ThisapproachunderutilizesthepotentialofLLMstoserveas
intermediariesbetweenhumansandsearchengines,transforminghumanintentionsintostep-by-
stepsearchtasksanddeliveringaccurateresponses.
ToenhancethecapabilityofLLMinaddressingcomplexquestions,wemodeltheproblem-solving
process as a directed acyclic graph (DAG). Given a user question Q, the solution trajectory is
representedasG(Q) = ⟨V,E⟩,whereV isasetofnodesv,eachrepresentinganindependentweb
search,includinganauxiliarySTARTnode(theinitialquestion)andanENDnode(thefinalanswer).
Erepresentsdirectededgesindicatingthereasoningtopologicalrelationshipsbetweennodes(search
contents). This DAG formalism captures the complexity of finding the optimal execution path,
providingamoreformalandintuitiverepresentationforLLMs.
LeveragingthesuperiorperformanceofcurrentLLMsoncodetasks(Guoetal.,2024;Roziereetal.,
2023),weexplicitlypromptthemodeltointeractwiththegraphthroughcodewriting. Toachieve
this,wepredefinedatomiccodefunctionstoaddnodesoredgestothegraph(Step1and2inFigure
2). Ateachturn,theLLMfirstreadstheentiredialogue,includingpreviouslygeneratedcodeand
websearchresults,thenoutputsthoughtsandnewcodeforreasoningonthemindgraph,which
is executed with a Python interpreter. During execution, once a node is added to the reasoning
graph,itinvokesaWebSearchertoexecutethesearchprocessandsummarizetheinformation. Since
3TechnicalReport
Figure2:AconcreteexampleofhowWebPlanneraddressesthequestionstepbystepviaplanning
ascoding. Duringeachturn,WebPlanneroutputsaseriesofthoughtsalongwiththegenerated
code. Thecodewillbeexecutedandyieldthesearchresultstotheplanner. Atthelastturn,the
WebPlannerdirectlyprovidesthefinalresponsewithoutanycodegeneration.
thenewlyaddednodesareonlydependentonnodesgeneratedinprevioussteps,wecanparallel
themtoachieveamuchfasterinformationaggregationspeed. Whenallinformationiscollected,the
plannerproducesthefinalresponsebyaddingtheendnode(Step3inFigure2).
ByintegratingwiththePythoninterpreter,WebPlannerinteractswiththegraphthroughunified
codeactions,dynamicallyconstructingthereasoningpath. This”codeasplanning”processenables
theLLMtofullyleverageitssuperiorcodegenerationability,benefitingcontrolanddataflowin
long-contextscenariosandleadingtobetterperformanceinsolvingcomplexproblems.
4TechnicalReport
2.2 WebSearcher: WebBrowsingwithHierarchicalRetrieval
WebSearcheractsasasophisticatedRAG(Retrieve-and-Generate)agentwithinternetaccess,sum-
marizingvaluableresponsesbasedonsearchresults(Figure3). Duetothemassivecontentavailable
ontheweb,itischallengingforLLMstoprocessallrelatedpageswithinalimitedcontextlength(e.g.
8Ktokens). Toaddressthis,weemployastraightforwardcoarse-to-fineselectionstrategy. Initially,
theLLMgeneratesseveralsimilarqueriesbasedontheassignedquestionsfromtheWebPlanner
tobroadenthesearchcontentandthusimprovetherecallofrelevantinformation. Thesequeries
arethenexecutedthroughvarioussearchAPIs,suchasGoogle,Bing,andDuckDuckGo,which
returnkeycontentsincludingwebURLs,titles,andsummaries. Thesearchresultsareautomatically
mergedbasedonthewebURLs,andtheLLMispromptedtoselectthemostvaluablepagesfor
detailed reading. The full content of the selected web URLs is then added to the input of LLM.
Afterreadingtheseresults,theLLMgeneratesaresponsetoanswertheoriginalquestionbased
on the search results. This hierarchical retrieval approach significantly reduces the difficulty of
navigatingmassivewebpagesandallowstoefficientlyextracthighlyrelevantinformationwith
in-depthdetails.
Figure3: AdetailedworkingpipelineofWebSearcher. Itcomprisesatmost4steps: queryrewrite,
searchcontentaggregation,detailedpageselection,andfinalsummarization.
2.3 LLMContextManagementinMindSearch
MindSearchprovidesasimplemulti-agentsolutiontocomplexinformationseekingandintegration
withsearchengines.Suchaparadigmalsonaturallyenableslong-contextmanagementamongdiffer-
entagents,whichimprovestheoverallefficiencyoftheframework,especiallyundercircumstances
thatrequirethemodeltoquicklyreadplentyofwebpages. SincetheWebPlannerdistributesthe
searchtasksintoseparatesearchagentsandonlyreliesonthesearchedresultsfromWebSearcher,
WebPlannercanpurelyfocusonthedecompositionandanalysisoftheuserquestionwithoutbeing
distractedbytheover-lengthwebsearchresults. Meanwhile,eachWebSearcheronlyneedstosearch
contentsforitstaskedsub-query,withoutdistractionfromothercontents. Thankstotheexplicit
roledistribution,MindSearchgreatlyreducescontextcomputationduringthewholeprocess,deliv-
eringanefficientcontextmanagementsolutiontolong-contexttasksforLLM.Suchamulti-agent
frameworkalsoprovidesastraightforwardandsimplelong-contexttaskconstructionpipelinefor
trainingsingleLLMs,whichisalsoobservedin(Team,2024). Eventually,MindSearchcollectsand
integratesrelatedinformationfrommorethan300pagesinlessthan3minute,whichcouldtake
humanexpertsabout3hourstofinishasimilarcognitiveworkload.
Duetotheexplicitcontextstatetransferacrossmultipleagents,weneedtocarefullyhandlethe
contextduringthewholeworkflow. Weempiricallyfindsimplyfocusingthedecomposedquery
fromthePlannermayloseusefulinformationduringtheinformationcollectionphaseduetothe
localreceptivefieldinsidethesearchagent. Howtoeffectivelyhandlethecontextbetweenmultiple
agentsisnon-trivial. Wefindthattheconstructedtopologicalrelationsthroughthedirectedgraph
edgeshelpuseasilyhandlethecontextacrossdifferentagents.Morespecifically,wesimplyprefixthe
responsefromitsfathernodeaswellastherootnodewhenexecutingeachsearchagent. Therefore,
eachWebSearchercaneffectivelyfocusonitssub-taskwithoutlosingthepreviousrelatedcontextas
wellasthefinalgoal.
5TechnicalReport
Figure 4: Subjective evaluation results judged by human experts on open-set QA questions.
MindSearchoutperformsChatGPT-WebandPerplexity.aiProbyalargemarginintermsofdepth,
breadth,andfacticity.
3 Experiments
WeevaluateMindSearchontwoprimarycategoriesofQuestionAnswering(QA)tasks: closed-set
QAandopen-setQA,whichreflectsboththesubjectiveandobjectivejudgmentofMindSearch. For
afaircomparison,allmodelsonlyhaveaccesstotheInternetthroughBINGsearchAPI,andno
extrareferencesourcesareconsidered.
3.1 Open-SetQA
3.1.1 ImplementationDetails
Tobettergaugetheutilityandsearchperformance,wecarefullycurate100real-worldhumanqueries
andcollectresponsesfromMindSearch(InternLM2.5-7b-chat(Caietal.,2024)),Perplexity.ai(its
Proversion),andChatGPTwithsearchpluginAchiametal.(2023). Weaskfivehumanexpertsto
manuallyselecttheirpreferredresponses,intermsofthefollowingthreeaspects:
• Depth: Depthreferstothethoroughnessandprofundityofananswer. Aresponsewith
depthprovidesdetailedinformationanddelvesintotheintricaciesofaquestion.
• Breadth: Breadthpertainstothescopeanddiversitycoveredbyananswer. Aresponse
withbreadthtouchesonvariousaspectsofthequestionormultiplerelatedfields,offering
differentperspectivesorsolutions.
• Factuality: Factualityisthedegreetowhichananswerisaccurateandfact-based. Itshould
begroundedinreliabledataandinformation,avoidingerrorsormisleadingcontent,and
ensuringthetruthfulnessandcredibilityoftheinformationprovided.
Thefinalresultsaredeterminedbasedonmajorvotes. Duringtheevaluation,thecorrespondence
betweentheresponseanditsmethodisinvisibletotheevaluatorstoguaranteefairness.
3.1.2 ResultsandAnalysis
TheevaluationresultsaredepictedinFigure4andwealsoprovidequantitativeresultsinFigure5.
FromFigure4,wecanobserveanabsoluteimprovementintermsofthedepthandbreadthofthe
modelresponse,whichvalidatesthesuperiorityofourproposedWebPlanner. Byintegratingcode
intotheDAGconstructionphase,LLMisabletoprogressivelydecomposethecomplexprobleminto
executablequerieswhilebalancingthetradeoffbetweentimeefficiencyandtheexplorationofthe
searchspace. Besides,MindSearchgoesthroughmorefine-grainedsearchtopicsaboutthequestion,
thereforeprovidingmorecompactanddetailedresponsescomparedtoothermodels. However,
MindSearchdoesnotyieldbetterperformanceintermsoffacticity. Wesuspectthatmoredetailed
searchresultsmaydistracttheconcentrationofthemodelontheinitialproblem,especiallywhen
LLMholdsincompletelong-contextcapability. Therefore,anaturalfutureworkofMindSearchisto
alleviatethehallucinationissuesduringthewebbrowsingprocess.
6TechnicalReport
Figure 5: Solution trajectory comparison between MindSearch and Perplexity.ai (Pro) on the
samequestion. MindSearchprovidesmoredetailedandproperresponsesthankstoitsfine-grained
searches.
Table1: Performancecomparisononvariousclosed-setQAtasks. Weselecttworepresentative
LLMs: GPT-4o(close-sourced)andInternLM2.5-7b-chat(open-sourced).
Musique HotpotQA AVG
Model Bamboogle
2-hop 3-hop 4-hop Easy Medium Hard
Closed-SoucedLLM(GPT-4o)
w/oSearchEngine 70.4 54.0 22.0 20.0 73.0 69.0 66.0 53.5
ReActSearch 75.2 48.0 25.0 13.3 81.0 73.0 70.0 55.1
MindSearch 76.8 60.0 35.0 14.6 80.0 74.0 78.0 59.8
Open-SourcedLLM(InternLM2.5-7b-chat)
w/oSearchEngine 34.0 28.0 10.0 17.3 47.0 26.0 40.0 28.9
ReActSearch 55.2 38.0 17.0 16.0 69.0 56.0 49.0 42.9
MindSearch 67.8 46.0 20.0 18.6 69.0 66.0 57.0 49.2
In addition to quantitative results, we also provide a qualitative response comparison between
Perplexity.ai (Pro) and MindSearch to deliver an intuitive understanding of their performance.
FromFigure5,wecanobservethatMindSearchyieldsmoreconcreteanddetailedresponses. We
empiricallyfindthatourbetterresponsescanbeattributedtotheproperplanningsearchpaths
comparedtoPerplexity.ai,whichalsoindicatesthathowtodecomposethehumanintentionisthe
keysteptothefinalproblem.
3.2 Closed-SetQA
3.2.1 ImplementationDetails
Weextensivelyevaluateourapproachonawiderangeofclosed-setQAtasks,includingBamboogle
(Press et al., 2022), Musique (Trivedi et al., 2022), and HotpotQA (Yang et al., 2018). To further
validatethegeneralizationofourapproach,weselectbothclosed-sourceLLM(GPT-4o)andopen-
sourceLLM(InternLM2.5-7b-chat)asourLLMbackend. Sinceourapproachadoptsazero-shot
experimentalsetting,weutilizeasubjectiveLLMevaluator(GPT4-o)togaugethecorrectnessof
HotpotQA.
7TechnicalReport
3.2.2 ResultsandAnalysis
InTable1,wecompareourapproachwithtwostraight-forwardbaselines: rawLLMwithoutsearch
engines(w/oSearchEngine),andsimplytreatingsearchenginesasanexternaltoolandadoptinga
ReAct-styleinteraction(ReActSearch). WecanconcludethatMindSearchsignificantlyoutperforms
itsvanillabaselinesbyalargemargin,validatingtheeffectivenessoftheproposedmethod. These
advantages are amplified when transferring from closed-sourced LLMs to open-sourced LLMs,
which further proves that MindSeach provides a simple approach to enhance weak LLMs with
broaderknowledgeandalleviatehallucinationissues.
4 RelatedWork
4.1 ToolUtilizationwithLLM
TheToolLearningframeworkempowersLLMstoseamlesslyintegratewithavarietyoftools(Qin
etal.,2023;Haoetal.,2024;Zhuangetal.,2024;Chenetal.,2023),suchassearchengines(Chan
et al., 2024), databases (Parisi et al., 2022), and APIs (Li et al., 2023; Patil et al., 2023), offering
dynamicsolutionstocomplexproblems. Thisintegrationisnotonlybeneficialforenhancingthe
interpretabilityandtrustworthinessofLLMsbutalsoforimprovingtheirrobustnessandadaptability
acrossdiversetasks,includingreducinghallucinations(Jietal.,2024),codegeneration(Gouetal.,
2023),andquestionanswering(Chenetal.,2024). Recentresearchhasfocusedonenhancingthetool
integrationcomponentofToolLearningsystems. Workssuchas(Huangetal.,2023;Shenetal.,2023;
Schicketal.,2024)haveconcentratedonimprovingtheretrievalmechanisms,ensuringthatLLMs
canaccessthemostpertinenttoolsforagiventask. Otherstudies,like(Qianetal.,2023;Yuanetal.,
2023),aimatrefiningtheLLMs’abilitytoeffectivelyutilizetheretrievedinformation,optimizing
thereadingandcomprehensionprocesseswithintheframework.
4.2 RAGwithLLM
RAGdemonstratessignificantadvantagesinaddressingknowledge-intensiveproblems,especially
inopen-domainscenarioswiththeintegrationofsearchengines(Chenetal.,2017;Lietal.,2017).
RAGallowsLLMstointegratewiththeretriever,providingtimelyinformationandofferingeffective
solutions. Moreover,RAGisalsoappliedinvarioustaskssuchasreducinghallucinations(Shuster
etal.,2021;Guetal.,2024),codegeneration(Zhouetal.,2022),andquestionanswering(Lewisetal.,
2020). Recently,somework(Karpukhinetal.,2020;Xiongetal.,2020;Quetal.,2020)focuseson
enhancingtheretrievalcomponentofRAGsystems,whileothers(Izacard&Grave,2020;Borgeaud
et al., 2022; Yu et al., 2021; Lei et al., 2017) enhances the language model’s ability as a reader to
optimizetheframework.
WiththeadvancementofLLMcapabilities,someresearchershavebeguntoreoptimizeframeworks
and redesign methodologies for model training. SAIL (Luo et al., 2023) trains LLM to be more
focusedoncredibleandinformativesearchresults. Self-RAG(Asaietal.,2023)enablesLMMsto
independentlyfetch,introspect,andaugmenttheirtextgenerationcapabilities. RQ-RAG(Chan
etal.,2024)enhancesqueryformulationbylearningtorefinequeriesthroughaniterativeprocess.
OurworkintegrateswebsearchcapabilitiesintoLLMs,enhancingresponsequalitybyretrieving
valuableinformationfromtheInternet.
4.3 WebAgents
Web automation agents have evolved from question-answering tools to sophisticated systems
capableofcomplexwebinteractions. EarlymodelslikeWebGPT(Nakanoetal.,2021)andWebGLM
(Liuetal.,2023)primarilyaddressedQAtasks,whilerecentadvancementshaveshiftedtowards
more dynamic operations (Yao et al., 2022; He et al., 2024). MindAct (Deng et al., 2024) and
WebAgent(Guretal.,2023)representthisprogression,withthelattershowingexceptionalweb
navigationdespitedeploymentchallengesduetoitssize. AutoWebGLM(Laietal.,2024)offersa
8TechnicalReport
practicalalternativewithrobustcapabilitiesandamorecompactmodelsize. Theincorporation
of reinforcement learning (Bai et al., 2024) and behavior cloning techniques (Zheng et al., 2024;
Pateletal.,2024)pavesthewayforevenmoreautonomousandefficientwebautomation,moving
thefieldtowardsscalableandversatilesolutionsforreal-worldapplications. Thispapermainly
focusesmoreonthewebinformation-seekingandintegrationtaskwithsearchenginesinsteadof
webbrowsing,andsolvesthemainchallengeswithamulti-agentframework.
5 Conclusion
ThispaperintroducesMindSearch,anovelLLM-basedmulti-agentframeworkforcomplexweb
information-seekingandintegrationtasks,bymorecomprehensivelyleveragingthestrengthsof
both search engines and LLMs. MindSearch conducts effective and sufficient decomposition of
complexqueriesfollowedbyhierarchicalinformationretrievaltoimprovetheprecisionandrecall
oftheretrievedrelevantwebinformation,bymodelingtheproblem-solvingprocessasaniterative
graphconstruction. Themulti-agentdesigndistributesthecognitiveloadamongspecializedagents,
facilitatingrobusthandlingofcomplexandlengthycontexts. Extensiveevaluationsonclosed-set
andopen-setQAproblemsusingGPT-4oandInternLM2.5-7Bmodelsdemonstratedsignificant
advantagesintheresponsequalityofMindSearch. Theresultsthathumanevaluatorspreferredthe
responsesfromMindSearchoverthosefromChatGPT-WebandPerplexity.aiindicateitscompetitive
edgeinAI-drivensearchsolutions. Wewishthisworkpavethewayforfutureresearchonmulti-
agentframeworkforsolvinghuman-levelcomplexcognitivetasks.
6 Acknowledgement
WewouldliketoexpressoursinceregratitudetoJiayeGeforheroutstandingtechnicalinsights,
productdesignskills,andcoordinationabilities. WealsothankZhongyingTu,YingZhao,Fang
Fang,andYitingWangfortheirefficientexecutionindevelopingtheprojectdemo. Ourgratitude
extendstoXingyuanLiu,ShuaikeLi,ZikePan,WeijiaSong,andYuzheGufortheireffortsinproject
suggestion.
References
JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport.
arXivpreprintarXiv:2303.08774,2023.
AkariAsai,ZeqiuWu,YizhongWang,AvirupSil,andHannanehHajishirzi. Self-rag: Learningto
retrieve,generate,andcritiquethroughself-reflection. arXivpreprintarXiv:2310.11511,2023.
HaoBai,YifeiZhou,MertCemri,JiayiPan,AlaneSuhr,SergeyLevine,andAviralKumar. Digirl:
Trainingin-the-wilddevice-controlagentswithautonomousreinforcementlearning.arXivpreprint
arXiv:2406.11896,2024.
PavelBerkhin. Asurveyonpagerankcomputing. Internetmathematics,2(1):73–120,2005.
SebastianBorgeaud,ArthurMensch,JordanHoffmann,TrevorCai,ElizaRutherford,KatieMillican,
GeorgeBmVanDenDriessche,Jean-BaptisteLespiau,BogdanDamoc,AidanClark,etal. Improv-
inglanguagemodelsbyretrievingfromtrillionsoftokens. InInternationalconferenceonmachine
learning,pp.2206–2240.PMLR,2022.
Sergey Brin and Lawrence Page. The anatomy of a large-scale hypertextual web search engine.
ComputernetworksandISDNsystems,30(1-7):107–117,1998.
ZhengCai,MaosongCao,HaojiongChen,KaiChen,KeyuChen,XinChen,XunChen,ZehuiChen,
ZhiChen,PeiChu,etal. Internlm2technicalreport. arXivpreprintarXiv:2403.17297,2024.
9TechnicalReport
Chi-MinChan,ChunpuXu,RuibinYuan,HongyinLuo,WeiXue,YikeGuo,andJieFu. Rq-rag:
Learningtorefinequeriesforretrievalaugmentedgeneration. arXivpreprintarXiv:2404.00610,
2024.
DanqiChen,AdamFisch,JasonWeston,andAntoineBordes. Readingwikipediatoansweropen-
domainquestions. arXivpreprintarXiv:1704.00051,2017.
ZehuiChen,WeihuaDu,WenweiZhang,KuikunLiu,JiangningLiu,MiaoZheng,JingmingZhuo,
SongyangZhang,DahuaLin,KaiChen,etal. T-eval: Evaluatingthetoolutilizationcapability
stepbystep. arXivpreprintarXiv:2312.14033,2023.
ZehuiChen,KuikunLiu,QiuchenWang,WenweiZhang,JiangningLiu,DahuaLin,KaiChen,and
FengZhao. Agent-flan: Designingdataandmethodsofeffectiveagenttuningforlargelanguage
models. arXivpreprintarXiv:2403.12881,2024.
XiangDeng,YuGu,BoyuanZheng,ShijieChen,SamStevens,BoshiWang,HuanSun,andYuSu.
Mind2web: Towards a generalist agent for the web. Advances in Neural Information Processing
Systems,36,2024.
Zhibin Gou, Zhihong Shao, Yeyun Gong, Yujiu Yang, Minlie Huang, Nan Duan, Weizhu Chen,
etal. Tora: Atool-integratedreasoningagentformathematicalproblemsolving. arXivpreprint
arXiv:2309.17452,2023.
YuzheGu, ZiweiJi, WenweiZhang, ChengqiLyu, DahuaLin, andKaiChen. Anah-v2: Scaling
analytical hallucination annotation of large language models. arXiv preprint arXiv:2407.04693,
2024.
DayaGuo,QihaoZhu,DejianYang,ZhendaXie,KaiDong,WentaoZhang,GuantingChen,XiaoBi,
YuWu,YKLi,etal. Deepseek-coder: Whenthelargelanguagemodelmeetsprogramming–the
riseofcodeintelligence. arXivpreprintarXiv:2401.14196,2024.
IzzeddinGur,HirokiFuruta,AustinHuang,MustafaSafdari,YutakaMatsuo,DouglasEck,and
Aleksandra Faust. A real-world webagent with planning, long context understanding, and
programsynthesis. arXivpreprintarXiv:2307.12856,2023.
ShiboHao,TianyangLiu,ZhenWang,andZhitingHu. Toolkengpt: Augmentingfrozenlanguage
modelswithmassivetoolsviatoolembeddings. Advancesinneuralinformationprocessingsystems,
36,2024.
HongliangHe,WenlinYao,KaixinMa,WenhaoYu,YongDai,HongmingZhang,ZhenzhongLan,
andDongYu. Webvoyager: Buildinganend-to-endwebagentwithlargemultimodalmodels.
arXivpreprintarXiv:2401.13919,2024.
YueHuang,JiawenShi,YuanLi,ChenruiFan,SiyuanWu,QihuiZhang,YixinLiu,PanZhou,Yao
Wan, Neil Zhenqiang Gong, et al. Metatool benchmark for large language models: Deciding
whethertousetoolsandwhichtouse. arXivpreprintarXiv:2310.03128,2023.
GautierIzacardandEdouardGrave. Leveragingpassageretrievalwithgenerativemodelsforopen
domainquestionanswering. arXivpreprintarXiv:2007.01282,2020.
ZiweiJi,NayeonLee,RitaFrieske,TiezhengYu,DanSu,YanXu,EtsukoIshii,YeJinBang,Andrea
Madotto, and Pascale Fung. Survey of hallucination in natural language generation. ACM
ComputingSurveys,55(12):1–38,2023.
ZiweiJi, YuzheGu, WenweiZhang, ChengqiLyu, Dahua Lin, and KaiChen. Anah: Analytical
annotationofhallucinationsinlargelanguagemodels. arXivpreprintarXiv:2405.20315,2024.
VladimirKarpukhin,BarlasOg˘uz,SewonMin,PatrickLewis,LedellWu,SergeyEdunov,Danqi
Chen, andWen-tauYih. Densepassageretrievalforopen-domainquestionanswering. arXiv
preprintarXiv:2004.04906,2020.
10TechnicalReport
HanyuLai,XiaoLiu,IatLongIong,ShuntianYao,YuxuanChen,PengboShen,HaoYu,Hanchen
Zhang,XiaohanZhang,YuxiaoDong,etal.Autowebglm:Bootstrapandreinforcealargelanguage
model-basedwebnavigatingagent. arXivpreprintarXiv:2404.03648,2024.
XiangyuLei,GuilinZhang,ShuaijunLi,HuihuanQian,andYangshengXu. Dual-springagvshock
absorptionsystemdesign:Dynamicanalysisandsimulations.In2017IEEEInternationalConference
onRoboticsandBiomimetics(ROBIO),pp.1068–1074.IEEE,2017.
PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,
HeinrichKu¨ttler,MikeLewis,Wen-tauYih,TimRockta¨schel,etal. Retrieval-augmentedgener-
ationforknowledge-intensivenlptasks. AdvancesinNeuralInformationProcessingSystems,33:
9459–9474,2020.
MinghaoLi,YingxiuZhao,BowenYu,FeifanSong,HangyuLi,HaiyangYu,ZhoujunLi,FeiHuang,
andYongbinLi. Api-bank: Acomprehensivebenchmarkfortool-augmentedllms. arXivpreprint
arXiv:2304.08244,2023.
ShuaijunLi,GuilinZhang,XiangyuLei,XiaoYu,HuihuanQian,andYangshengXu. Trajectory
trackingcontrolofaunicycle-typemobilerobotwithanewplanningalgorithm. In2017IEEE
InternationalConferenceonRoboticsandBiomimetics(ROBIO),pp.780–786.IEEE,2017.
XiVictoriaLin,XilunChen,MingdaChen,WeijiaShi,MariaLomeli,RichJames,PedroRodriguez,
JacobKahn,GergelySzilvasy,MikeLewis,etal. Ra-dit: Retrieval-augmenteddualinstruction
tuning. arXivpreprintarXiv:2310.01352,2023.
XiaoLiu,HanyuLai,HaoYu,YifanXu,AohanZeng,ZhengxiaoDu,PengZhang,YuxiaoDong,and
JieTang. Webglm: Towardsanefficientweb-enhancedquestionansweringsystemwithhuman
preferences. InProceedingsofthe29thACMSIGKDDConferenceonKnowledgeDiscoveryandData
Mining,pp.4549–4560,2023.
HongyinLuo,Yung-SungChuang,YuanGong,TianhuaZhang,YoonKim,XixinWu,DannyFox,
Helen Meng, and James Glass. Sail: Search-augmented instruction learning. arXiv preprint
arXiv:2305.15225,2023.
ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu,LongOuyang,ChristinaKim,Christopher
Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, et al. Webgpt: Browser-assisted
question-answeringwithhumanfeedback. arXivpreprintarXiv:2112.09332,2021.
AaronParisi,YaoZhao,andNoahFiedel. Talm: Toolaugmentedlanguagemodels. arXivpreprint
arXiv:2205.12255,2022.
Ajay Patel, Markus Hofmarcher, Claudiu Leoveanu-Condrei, Marius-Constantin Dinu, Chris
Callison-Burch,andSeppHochreiter. Largelanguagemodelscanself-improveatwebagenttasks.
arXivpreprintarXiv:2405.20309,2024.
ShishirGPatil,TianjunZhang,XinWang,andJosephEGonzalez. Gorilla: Largelanguagemodel
connectedwithmassiveapis. arXivpreprintarXiv:2305.15334,2023.
OfirPress,MuruZhang,SewonMin,LudwigSchmidt,NoahASmith,andMikeLewis. Measuring
andnarrowingthecompositionalitygapinlanguagemodels. arXivpreprintarXiv:2210.03350,
2022.
Cheng Qian, Chi Han, Yi R Fung, Yujia Qin, Zhiyuan Liu, and Heng Ji. Creator: Tool creation
for disentangling abstract and concrete reasoning of large language models. arXiv preprint
arXiv:2305.14318,2023.
YujiaQin,ShihaoLiang,YiningYe,KunlunZhu,LanYan,YaxiLu,YankaiLin,XinCong,Xiangru
Tang,BillQian,etal. Toolllm: Facilitatinglargelanguagemodelstomaster16000+real-world
apis. arXivpreprintarXiv:2307.16789,2023.
11TechnicalReport
YingqiQu,YuchenDing,JingLiu,KaiLiu,RuiyangRen,WayneXinZhao,DaxiangDong,Hua
Wu,andHaifengWang. Rocketqa: Anoptimizedtrainingapproachtodensepassageretrievalfor
open-domainquestionanswering. arXivpreprintarXiv:2010.08191,2020.
BaptisteRoziere,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,Yossi
Adi,JingyuLiu,TalRemez,Je´re´myRapin,etal. Codellama: Openfoundationmodelsforcode.
arXivpreprintarXiv:2308.12950,2023.
TimoSchick,JaneDwivedi-Yu,RobertoDess`ı,RobertaRaileanu,MariaLomeli,EricHambro,Luke
Zettlemoyer,NicolaCancedda,andThomasScialom. Toolformer: Languagemodelscanteach
themselvestousetools. AdvancesinNeuralInformationProcessingSystems,36,2024.
YongliangShen,KaitaoSong,XuTan,WenqiZhang,KanRen,SiyuYuan,WeimingLu,Dongsheng
Li,andYuetingZhuang. Taskbench: Benchmarkinglargelanguagemodelsfortaskautomation.
arXivpreprintarXiv:2311.18760,2023.
KurtShuster,SpencerPoff,MoyaChen,DouweKiela,andJasonWeston. Retrievalaugmentation
reduceshallucinationinconversation. arXivpreprintarXiv:2104.07567,2021.
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya
Pathak,LaurentSifre,MorganeRivie`re,MihirSanjayKale,JulietteLove,etal. Gemma: Open
modelsbasedongeminiresearchandtechnology. arXivpreprintarXiv:2403.08295,2024.
Qwen Team. Generalizing an llm from 8k to 1m context using qwen-agent, May 2024. URL
https://qwenlm.github.io/blog/qwen-agent-2405/.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundationand
fine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
HarshTrivedi,NiranjanBalasubramanian,TusharKhot,andAshishSabharwal. Musique: Multihop
questionsviasingle-hopquestioncomposition. TransactionsoftheAssociationforComputational
Linguistics,10:539–554,2022.
LeeXiong,ChenyanXiong,YeLi,Kwok-FungTang,JialinLiu,PaulBennett,JunaidAhmed,and
ArnoldOverwijk. Approximatenearestneighbornegativecontrastivelearningfordensetext
retrieval. arXivpreprintarXiv:2007.00808,2020.
ZhilinYang,PengQi,SaizhengZhang,YoshuaBengio,WilliamWCohen,RuslanSalakhutdinov,
andChristopherDManning. Hotpotqa: Adatasetfordiverse,explainablemulti-hopquestion
answering. arXivpreprintarXiv:1809.09600,2018.
ShunyuYao,HowardChen,JohnYang,andKarthikNarasimhan. Webshop: Towardsscalablereal-
worldwebinteractionwithgroundedlanguageagents. AdvancesinNeuralInformationProcessing
Systems,35:20744–20757,2022.
DonghanYu,ChenguangZhu,YuweiFang,WenhaoYu,ShuohangWang,YichongXu,XiangRen,
Yiming Yang, and Michael Zeng. Kg-fid: Infusing knowledge graph in fusion-in-decoder for
open-domainquestionanswering. arXivpreprintarXiv:2110.04330,2021.
LifanYuan,YangyiChen,XingyaoWang,YiRFung,HaoPeng,andHengJi. Craft: Customizing
llmsbycreatingandretrievingfromspecializedtoolsets. arXivpreprintarXiv:2309.17428,2023.
BoyuanZheng,BoyuGou,JihyungKil,HuanSun,andYuSu. Gpt-4v(ision)isageneralistweb
agent,ifgrounded. arXivpreprintarXiv:2401.01614,2024.
Shuyan Zhou, Uri Alon, Frank F Xu, Zhiruo Wang, Zhengbao Jiang, and Graham Neubig.
Docprompting: Generatingcodebyretrievingthedocs. arXivpreprintarXiv:2207.05987,2022.
YuchenZhuang,YueYu,KuanWang,HaotianSun,andChaoZhang. Toolqa: Adatasetforllm
questionansweringwithexternaltools. AdvancesinNeuralInformationProcessingSystems,36,2024.
12