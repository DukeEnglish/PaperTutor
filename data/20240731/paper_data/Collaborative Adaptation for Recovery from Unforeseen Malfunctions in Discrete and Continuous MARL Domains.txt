Collaborative Adaptation for Recovery from Unforeseen Malfunctions
in Discrete and Continuous MARL Domains
Yasin Findik, Hunter Hasenfus, and Reza Azadeh
Abstract—Cooperative multi-agent learning plays a crucial and global exploration [10]. However, despite its effective-
role for developing effective strategies to achieve individual or ness in collaborative tasks, CTDE methods struggle with
shared objectives in multi-agent teams. In real-world settings,
rapid adaptation to unforeseen agent malfunction(s) or fail-
agents may face unexpected failures, such as a robot’s leg
ure. This challenge might be due to two primary reasons:
malfunctioning or a teammate’s battery running out. These
malfunctionsdecreasetheteam’sabilitytoaccomplishassigned the lack of built-in mechanisms to manage unforeseen robot
task(s), especially if they occur after the learning algorithms failures, or the absence of design elements to steer agents’
have already converged onto a collaborative strategy. Current collaboration strategies. Consequently, these algorithms face
leading approaches in Multi-Agent Reinforcement Learning
delays in adapting, as they need to independently discover
(MARL) often recover slowly – if at all – from such malfunc-
and converge on new effective collaboration strategies with-
tions.Toovercomethislimitation,wepresenttheCollaborative
Adaptation(CA)framework,highlightingitsuniquecapability out any guidance. To assist agents in searching for these
to operate in both continuous and discrete domains. Our strategies, we associate the discovery of such collaborative
framework enhances the adaptability of agents to unexpected behaviors through inter-agent relationships, thereby acceler-
failuresbyintegratinginter-agentrelationshipsintotheirlearn-
atingandimprovingtheadaptabilityoftheagentsasateam.
ing processes, thereby accelerating the recovery from malfunc-
Inthispaper,weintroduceanovelMARLframeworkthat
tions. We evaluated our framework’s performance through
experiments in both discrete and continuous environments. integrates a relational network with any multi-agent learning
Empirical results reveal that in scenarios involving unforeseen algorithm following the CTDE paradigm. This integration
malfunction,althoughstate-of-the-artalgorithmsoftenconverge highlightstherelativeimportanceamongagents,therebyen-
onsub-optimalsolutions,theproposedCAframeworkmitigates
hancing their collaboration towards a specific strategy. This
and recovers more effectively.
innovation facilitates faster and more effective adaptation to
I. INTRODUCTION unexpected robot failures. We conducted experiments in two
The popularity of multi-agent learning methods is rising settings: a multi-agent discrete environment with multiple
due to their ability to manage the complexities of robotic robots and a continuous environment [11] simulating an ant
systems and their importance in tasks that require detailed with multiple agents (an agent per leg). We demonstrate the
coordination [1]. These methods are valuable across various effectiveness of our approach by comparing it with Inde-
fields, such as robotics for search & rescue operations [2] pendent Deep Q-Networks (IDQN) [12] and Value Decom-
and autonomous driving [3], whether involving multiple or position Networks (VDN) [13] in the discrete environment,
single robot systems. In single-robot applications, multi- and with Independent Q-Functionals (IQF) [14] and Multi-
agent learning facilitates coordination among different sys- Agent Deep Deterministic Policy Gradient (MADDPG) [15]
tem modules, exemplified by treating each joint of a robot inthecontinuousenvironment.Empiricalresultsindicatethat
manipulator as an individual agent within a collaborative our method improves teamwork among agents, whether in
team. In both multi-robot and single-robot systems, the configurationslikeasinglerobotwithmultiplelegs/jointsor
collaborative behavior of the agents is particularly crucial multiple robots. More importantly, it enhances the ability to
for quickly and autonomously recovering from unexpected adapt to unexpected failure by utilizing relational networks.
malfunctions, such as joint failures or battery failure that
preventstherobot(s)frommoving.Inthesesituations,agents
II. BACKGROUNDANDRELATEDWORK
must improve their collaboration further and modify their
Reinforcement learning (RL) aims at enabling an agent
joint strategies to successfully tackle these challenges.
to maximize its cumulative future rewards through learn-
In the field of Multi-Agent Reinforcement Learning
ing from its interactions within an environment. This
(MARL)forcooperativetasks,theCentralizedTrainingwith
process is often represented as a Decentralized Markov
Decentralized Execution (CTDE) [4] paradigm has emerged
Decision Process (DEC-MDP), characterized by a tuple
as a prominent approach, notably through recent advance- ⟨S,A,R,T,γ,N⟩. S, A and R indicate the true state of
ments in deep reinforcement learning [5], [6], [7]. It effec-
the environment, the joint set of agents’ action and reward,
tivelyaddressesarangeofcooperativechallengesinMARL, respectively.T(s,A,s′): S×A×S (cid:55)→[0,1]isthedynamics
includingthecurseofdimensionality[8],non-stationarity[9] function, defining the transition probability, where s and s′
are the current and next state, respectively. γ ∈[0,1) is the
Authors are with the Persistent Autonomy and Robot Learn-
ing (PeARL) Lab, Richard Miner School of Computer and In- discount factor, and N represents the number of agents.
formation Sciences, University of Massachusetts Lowell, MA, USA
yasin findik@student.uml.edu, reza@cs.uml.edu Q-Learning&Q-Functionals.Q-Learning[16]andDeep
4202
luJ
72
]AM.sc[
1v44191.7042:viXraQ-Networks (DQN) [7] are two prominent value-based ap- Value Function Factorization. In the context of joint
proaches in single-agent RL with discrete action spaces. Q- action-value function learning, where complexity escalates
Learning employs an action-value table, Qπ, for a policy π, exponentially with the number of agents, value function
defined as Qπ(s,a) = E[G|st = s,at = a], where G and factorization methods emerge as a promising solution. Fol-
t denote the return and time-step, respectively. This can be lowing the centralized training with decentralized learning
re-formulated recursively as: (CTDE) [4] paradigm, these methods facilitate autonomous
action execution by individual agents, while centralizing the
Qπ(s,a)=E [R(s,a)+γE [Qπ(s′,a′)]],
s′ a′∼π integration of their strategies. Consequently, these methods
where R represents the reward function. However, DQN resolve the non-stationarity issue inherent in independent
learnsanaction-valuefunction,denotedasQˆ,corresponding learning through centralized training and tackle the scal-
to the optimal policy by minimizing this loss: ability challenges associated with centralized learning by
enabling decentralized execution.
L(w PN)=E s,a,r,s′[(r+γm aa ′x(Qˆ(s′,a′;w TN))−Qˆ(s,a;w PN))2]. (1)
VDN [13] and QMIX [20] stand out as two significant
The parameters for the prediction network are denoted by learning algorithms in action-value function factorization.
w , while w represents the parameters for the target These algorithms are characterized by using separate action-
PN TN
network. w are periodically updated with w to enhance valuefunction,denotedasQˆ ,foreachagenti∈{1,...,N}.
TN PN i
the stability of the learning process. DQN also utilizes an The primary distinction between these methods is their
experience replay memory that hold tuples ⟨s,a,r,s′⟩, to strategy for calculating the total action value, Q . VDN
tot
enhance the learning stability further. achieves Q by summing the Q s, as expressed in:
tot i
In these approaches, state-action pairs are directly associ-
N
ated with R, as denoted by: Qˆ =(cid:88) Qˆ (s,a ), (4)
tot i i
Qˆ(s,a):(S×A)(cid:55)→R. (2) i=1
whereasQMIXemploysastate-dependentcontinuousmono-
This direct mapping restricts their utility to scenarios within
tonic function to integrate these values, denoted by:
discrete environments (i.e., a finite set of actions) because
it is unfeasible to calculate the action-value for every state- Qˆ :=f (Qˆ (s,a ),...,Qˆ (s,a )), (5)
tot s 1 1 n N
actionpairincontinuousdomains.Toaddressthislimitation,
Q-Functionals (QF) [17], restructure the mapping by decou- with the condition of ∂fs ≥ 0,∀i ∈ {1,...,N}. Often,
∂Qˆ
i
pling the state and action elements, illustrated as: these factorization algorithms use DQN for approximating
the action-value function and aim to minimize the loss (1)
QˆF(s,a):S (cid:55)→(A(cid:55)→R). (3)
in a centralized way, as follows:
Here, the state is initially mapped to a function that subse-
quently associates actions with R. Essentially, each state is L(wPN)=E s,u,rteam,s′[(rteam+γm ua ′x(Qˆ tot(s′,u′;wTN))−Qˆ tot(s,u;wPN))2]. (6)
depicted as a function formed through the learning of basis where r is obtained by uniformly summing the agents
team
function coefficients within the action domain. These state- individual rewards, and u is the joint action of the agents.
representativefunctionsenablethequickassessmentofmany The current value function factorization methods have
actions via matrix operations between the action representa- demonstrated proficiency in creating stable collaborative
tions and the learned coefficients. Thus, QF adeptly manage within discrete environments. However, even with their ad-
continuous domains in single-agent contexts. vancediterations[21],[22],theseapproachesencounterchal-
Within multi-agent systems, the basic application of these
lenges in formulating effective policies within continuous
value-based approaches involves each agent i independently
environments.Insuchsettings,theactionspaceisdelineated
optimizing its respective action-value table or function,
by continuous vectors, thereby encompassing an unbounded
such as Independent Q-Learning (IQL) [18], Independent
set of potential actions [23].
Deep Q-Networks (IDQN) [7], Independent Q-Functionals
PolicyGradient(PG).Insingle-agentRLwithcontinuous
(IQF) [14]. These methods, however, encounter difficul-
action spaces Policy Gradient (PG) approaches [24] are
ties because the independent policy modifications by each
essential. These methods aim to directly optimize the policy
agent make the environment appear non-stationary from
parameters, symbolized as θ, to maximize the expected
their respective perspectives. This non-stationarity violates
return. The principal strategy is to modify θ in the direction
the stationary requirements of the Markov property, crucial
of the policy’s gradient, formulated as:
for the stable convergence of these algorithms. As a simple
alternative to independent learning, fully centralized learn- ∇ J(θ)=E [∇ logπ (a|s)Qπ(s,a)], (7)
ing [19] can be adopted, in which a singular controller is
θ s∼pπ,a∼πθ θ θ
utilized across all agents, enabling the joint learning of a where pπ denotes the distribution of states under the policy
value function. Yet, this centralized strategy is characterized π. Generally, PG approaches differ from each other in their
by computational demand and scalability problems, as the methods for estimating Qπ.
observation and action spaces grow exponentially with the Moreover, Deterministic Policy Gradient (DPG) [25]
number of agents, possibly resulting in intractability. adapts the PG theorem and represents policy as µ :S (cid:55)→A
θwith a vector of n parameters θ ∈ Rn. The formulation of Algorithm 1: Collaborative Adaptation
policy gradient becomes as: input : P-NN, Qˆprediction; T-NN, Qˆtarget; relational
∇ J(µ )=E [∇ µ (s)∇ Qµ(s,a)| ]. (8) network, G; batch size, b; number of
θ θ s∼pµ θ θ a a=µθ(s)
iterations for updates, m
Also, Deep Deterministic Policy Gradient (DDPG) [26] 1 foreach episode do
approach extends DPG by employing deep neural networks 2 Initialize s
to approximate both the policy µ and critic Qˆµ. 3 foreach step of episode do
Multi-Agent DDPG (MADDPG). MADDPG [15] ex- 4 Choose a from s using policy derived from
Qˆprediction (with ε-greedy)
tends DDPG by adhering CTDE paradigm, facilitating its
application in multi-agent settings. The policy of agents 5 Take action a, observe r, s′
are represented as π = [π ,...,π ] with parameters θ = 6 Store s, a, r, s′ in memory
1 N
[θ ,...,θ ]inMADDPG.Theoptimizationofthesepolicies 7
s←s′
1 n
is conducted through the gradient of the expected return, 8 for i=1,...,m do
which for each agent i∈{1,...,N} is formulated as: 9 S, A, R, S′ ← sample chunk, size of b, from
memory
∇ θiJ(θ i)=E x∼pµ,ai∼πi[∇ θilogπ i(a i|o i)Qˆπ
i
(o,a)],
10
Qp vare lud eic stion ←Qˆprediction(S)
where Qˆπ(o,a) denotes the centralized action-value func- 11 Qp vare lud eic stion ← action A of Qp vare lud eic stion of every
i agent in every sample
tion, incorporating both the states o = [o ,...,o ] and ac-
Ttio hn es fa ram= e[ wa o1, rk.. i. s,a fuN rt] ho ef ra al dl aa pg te en dts foto
r
c do em t1 erp mut ie nit sh tN e icQ p- ov la iclu iee s. 11 32 QQ t vp t ao ar rte lga ud l e ei tc sti ←on ← Qˆtau rgs ee t(S(1 ′0 )) with G and Qprediction
µ=[µ θ1,...,µ θN], modifying the gradient expression as: 14 Qtarget ← max of Qt va ar lg ue et s of every agent in
every sample
∇ θiJ(µ θi)=E o∼pµ,a∼µ[∇ θiµ θi(a i|o i)∇ aiQˆµ i(o,a)| ai=µθi(oi)]. 15 Qt ta or tg alet ← use (10) with G and Qtarget
16 Rteam ← use (9) with G and R
MADDPG,similartotheDQNalgorithm,incorporatestarget
networks and experience replay to enhance the stability of 17 L← use (6) with Rteam, Qt ta or tg alet, Qp tor te ad liction
the policy networks. The loss function for updating the 18 Backpropagate L to the weights of Qˆprediction
centralized action-value function Qµ is defined as: 19 Update the parameters of Qˆtarget with the
i
parameters of Qˆprediction
L(θ)=E [(r +γQµ′ (o′,a′)| −Qµ(o,a))2],
i o,a,r,o′∼D i i a′=µ′(o′) i
where D denotes the replay memory storing tuples
⟨o,a,r,o′⟩ and µ′ = [µ ,...,µ ] represents the peri- To effectively overcome these challenges, we propose a
θ′ θ′
odically updated target poli1 cies. DesN pite MADDPG’s state- novel framework known as Collaborative Adaptation (CA).
of-the-artperformanceinmaximizingagents’return,itfaces Thisframeworkistailoredtoconsiderthedynamicsbetween
challenges like converging to local optima due to inefficient agents, guiding them toward adopting cooperative strategies
sampling and limited exploration in complex environments to improve team performance and accelerate adaptation. In
withextensivestateandactionspaces,specificallyrecovering the current field of cooperative MARL, such a mechanism
from unforeseen malfunction(s) or failure. is notably absent, leading to increased difficulty and time
required for adaptation in the face of unforeseen malfunc-
III. PROPOSEDMETHOD tions.OurCAframeworkaimstofillthisgapbyencouraging
agents, through inter-agent relationships, to either assist
In cooperative MARL, both value-based and policy-based
malfunctioning one directly or completing its task on behalf
techniques are utilized to maximize collective rewards, aim-
of it, thus improving the team’s overall performance.
ing to converge toward an optimal solution, potentially the
The proposed framework integrates a relational network
global optimum. The convergence process is influenced by
into the agents’ learning process, structured as a directed
the stochastic nature of agents’ exploration, particularly in
graph G =(V,E,W), whichdefines theinter-agent relation-
scenarios where multiple cooperation strategies offer the
ships to enhance cooperative strategies.
same maximum total reward. Thus, the team’s collaboration
strategy, and inherently its structural dynamics, are signifi- w =0.7
cantly influenced by the randomness in agents’ exploration w 11 =0.3 v 1 12 v 2 w 22 =1
process.However,inreal-worldapplications,theimportance
of the team’s structure becomes pronounced when robots Each agent i∈{1,2,...,N} is depicted as a vertex v in V,
i
encounter mechanical issues (e.g., battery or joint failures), with E representing the set of directed edges e extending
ij
necessitating a strategy that does not solely depend on ran- from v to v , and W containing the edge weights w ∈
i j ij
domness.Inotherwords,thecurrentstate-of-the-artmethods [0,1]. These weights, w , quantify the level of influence or
ij
demonstrate limited effectiveness in adapting team behavior interest agent i has in the outcomes of agent j.
to scenarios that involve unforeseen failures. Inthecontextoftheassignedtaskstoagents,therelation-shipsmaybeutilizedeitherforthecomputationoftheteam’s
action value, Q , through the agents’ individual action-
tot
values, Q , or for the calculation of the team reward, r ,
i team
via the individual rewards of the agents, r . For instance, in
i
scenarios where agents receive rewards based on individual
actions while engaging in a collective task, the aggregate
team reward is calculated for use in (6), as follows:
(a) (b) (c)
(cid:88)(cid:88)
r = w r . (9)
team ij j Fig. 1: (a) multi-agent grid-world environment with four agents.
i∈Vj∈Ei (b-c) Relational networks employed in CA-VDN.
In cases where agents receive a uniform team reward from
the environment and lack access to individual rewards, the
team’s action-value, Q , is re-defined to incorporate the agent. However, the agent incurs an individual penalty of
tot
inter-agent relationships for utilization in (6), as: −1foreachtime-stepperunconsumedresource,exceptwhen
occupyingaresourcelocation,whichactsasasafespot.The
(cid:88)(cid:88)
Q = w Q , (10) episode ends when either all resources have been consumed
tot ij j
i∈Vj∈Ei or the maximum number of time-steps has been achieved.
The design of this environment is intended to be solvable
where Q denotes the action-value of agent j.
j
by VDN, while also highlighting the challenges that unfore-
The modifications introduced in our framework allow
seen malfunction(s) can introduce, even in simple scenarios.
for the direction or guidance of the agents’ collaborative
Amalfunctionissimulatedbyimmobilizingthegreenagent.
approach,therebyimprovingtheiradaptabilitytounexpected
Thissetupillustratestheimpactofintegratingagentrelation-
robot failure and accelerating the convergence to optimal
ships into the learning to address these challenges.
solutions even without any malfunction, as evidenced by
our empirical findings. During our experimental investiga- B. Multi-agent MuJoCo
tions,VDN[20]andMixedQ-Functionals(MQF)[14]were
Multi-agentMuJoCo(MAMuJoCo)[27]isanovelbench-
employed as the learning algorithms for the agents within
mark forcontinuous cooperative multi-agentrobotic control.
the CA framework, tailored for discrete and continuous
Basically, it is an extension of MuJoCo [28], by creating
settings, denoted as CA-VDN and CA-MQF, respectively.
a wide variety of novel scenarios in which multiple agents
The pseudo-code corresponding to the CA framework is
withinasinglerobothavetosolveataskcooperatively.More
presented in Algorithm 1.
specifically,weusedtheMaMuJoCo-Antenvironmentwhich
IV. ENVIRONMENTS is a robotic simulation environment that emulates a 3D ant,
as depicted in Fig.2(b). It consists of a freely rotating torso
To evaluate how effectively our framework influences
linked to four legs, with each leg having two joints. The
agent behaviors and improves their ability to adapt to un-
primary goal of this environment is to achieve coordinated
expected failures of their teammate(s) in both discrete and
movement in the forward direction (toward the positive
continuous tasks, we conducted experiments in two distinct
direction on the x-axis). This is achieved by strategically
environments. In the context of discrete tasks, we employed
applyingtorquestotheeightjoints,thuscontrollingtheant’s
a multi-agent grid-world environment and compared our
movements.
framework’s performance against value-based methods, par-
In our experiments, we employed a variant of the ant
ticularly IDQN and VDN. For continuous tasks, we used
environment that assigns an individual agent to each leg, as
a multi-agent MuJoCo environment and conducted com-
parisons with policy-based methods, specifically variants of
DDPG designed for multi-agent scenarios.
A. Multi-agent Grid-world Environment
In this environment, depicted in Fig 1(a), the goal of each AGENT 1 AGENT 2 AGENT 1 AGENT 2
episode is for agents to consume all resources by visiting
their locations. Agents can perform five actions: move up, (a)
down, left, right, or stay idle. Moreover, they have access
AGENT 3 AGENT 4 AGENT 3 AGENT 4
to a special action called push, which allows them to push
adjacent agents. This action is possible when the pushing
agent executes a non-idle action towards an idle agent to be
(b) (c) (d)
pushed. Following a push, the agent initiating the push stays
stationary, whereas the pushed agent is moved one space Fig. 2: (a) Representation of an ant featuring four agents, each
in the direction of the push. When an agent successfully distinguished by a different color and (b) The MaMuJoCo-Ant
consumes a resource, it receives a reward of +10. Each simulation environment. (c-d) Relational networks used in CA-
resource is single-use and can be consumed by only one MQF.depicted in Fig2(a). Consequently, the ant is controlled by memoryof500ktime-steps.Targetnetworksreceiveupdates
four distinct agents, each in charge of a leg comprising two ateachtime-stepthroughasoft updateprocess,integratinga
joints.Inthisconfiguration,eachagenthastwoactionvalues small factor (τ =0.01) with the weights from the prediction
that range from [−1,+1]. During an episode, agents receive networks. In both discrete and continuous environments, the
a team reward at each step, calculated as r = r + Adamoptimizerisutilizedfornetworkoptimization,andthe
team stable
r −r ,wherer isafixedrewardgivenatevery squaredTemporalDifference(TD)errorisusedasthemetric
forward ctrlcost stable
time-stepformaintainingastableposture(notupsidedown), for loss evaluation.
r is the reward for moving forward, measured as ∆x
forward dt
(with dt representing the time between actions and ∆x the VI. EXPERIMENTALRESULTS
change in the x direction), and r ctrlcost is the penalty for In this section, we present our experimental results on
executing excessively large actions. Additionally, agents are MaMuJoCo-Ant and our cooperative multi-agent grid-world
penalized with a −100 reward if the ant becomes upside environment. We have simulated a malfunction after agents
down. For our experiments, we set r stable at +0.01 and the convergedtooneoftheseveralpossiblesolutions(potentially
maximumnumberofstepsperepisodeat100.Itisimportant the global optimal solution). Then, we compared the adap-
to notice that agents have no access to their individual tation abilities of our collaborative adaptation framework
rewards, but only team reward, r team. with IDQN and VDN for discrete tasks, and with IQF and
The integration of multiple agents within a single robotic MADDPG for continuous action domains.
ant showcases the effectiveness of learning algorithms in
In case of a unforeseen failure, our study assumes that
enhancing robustness against single-point failures, making
a detection mechanism is in place, capable of determining
it well-suited for our experimental framework. In particular,
the malfunction’s timing (possibly by monitoring the team’s
for our experiments, one leg is immobilized to represent
collective reward) and identifying the specific agent that is
a malfunction. The experiments are designed to evaluate
malfunctioning.Theidentificationprocessmightinvolveana-
the agents’ ability to effectively synchronize actions and
lyzingtheagents’previousmovementstoidentifydeviations
adapttounforeseenmalfunctionthattheirteammate(s)might
from the behavioral patterns established during training.
encounter.
The CA framework demonstrates superior adaptability
V. MODELSANDHYPERPARAMETERS tounexpectedagentfailurecomparedtoIDQNandVDN
Inthisstudy,theneuralnetworkarchitectureemployedfor in our multi-agent grid-world environment involving
agent modelling in all algorithms tested is the Multi-Layer discrete actions. Despite the initial similarity in perfor-
Perceptron (MLP) for both environments. For the discrete mance between the algorithms, IDQN and VDN encounter
environment, the MLP configuration consists of two hidden challenges in recovering post-malfunction, where the green
layers, each with 128 neurons, using the ReLU activation agent,asdepictedinFigure1(a),isimmobilized.Toimprove
function. The prediction networks are trained at the end of thealgorithms’abilitytoidentifynewstrategiesorbehaviors,
each episode through 10 iterations, utilizing batches of 32 the epsilon (ε) value is reset after the malfunction incident.
instances (time-steps) that are randomly selected from the Fig. 3 displays the experimental outcomes in this setup,
replay memory, capable of storing up to 50k time-steps. showcasing the average training (indicated by shaded areas)
Every200episodes,theweightsofthetargetneuralnetworks and test rewards (represented by solid lines) across ten runs.
are re-assigned with those of prediction networks. In the The test rewards are calculated by assessing the agents’
continuous environment, the MLPs feature three hidden lay- individualrewardsusingagreedyapproach,withthetraining
ers, each with 256 neurons, and employ the TanH activation process being paused after every 50 episodes.
function (while critic networks in MADDPG still uses the IDQN-trained agents, after malfunction, operate au-
ReLU). The prediction networks’ weight are updated every tonomously without a system to foster mutual cooperation,
10stepswithbatchesof512,randomlychosenfromareplay leading to inconsistent behaviors across multiple runs. Fol-
Malfunction Malfunction Malfunction
(a) (b) (c)
Fig. 3: Multi-agentGrid-worldresults:Averageindividualrewardsofagentsbeforeandafterthegreenagent’smalfunctionatthe5000th
episode for (a) IDQN, (b) VDN and (c) CA-VDN.lowing the consumption of three resources, agents typically TABLE I: Average metrics with 95% confidence intervals for ten
runs upon training completion.
engage in one of three behaviors: (i) the episode continues
due to an unconsumed resource, resulting in random actions BeforeMalfunction AfterMalfunction
and penalties for the agents; (ii) an agent may learn to IDQN VDN CA-VDN IDQN VDN CA-VDN
BlueAgent 6.00±0.00 5.80±0.25 5.50±0.64 6.00±2.37 -74.20±18.14 6.90±0.19
consume the remaining resource, thereby concluding the RedAgent 6.00±0.00 5.50±0.50 5.70±0.40 -2.40±12.36 -63.60±20.26 9.90±0.19
episode; (iii) an agent might discover how to push the mal- OrangeAgent 6.00±0.00 5.20±0.67 5.40±0.79 8.10±7.39 -66.50±21.47 9.70±0.40
GreenAgent 6.00±0.00 5.70±0.40 5.60±0.74 -12.70±6.45 -35.70±28.60 10.50±0.93
functioning agent, aiming to secure the reward for pushing
Sum 24.00±0.00 22.20±1.64 22.20±2.38 -1.00±22.59 -240.00±72.87 37.00±1.49
while another goes to consume the remaining resource to
endtheepisode.However,thereisnoevidenceofintentional
collaborative efforts among the agents, such as assisting the note that, although the environment positively rewards such
malfunctioning agent in consuming the nearest resource by movement, it concurrently imposes penalties for the execu-
pushing. tion of large actions (refer to r in IV-B), potentially
ctrlcost
In VDN, despite increased exploration following this limiting energy consumption per action.
malfunction, challenges in recovery persist in some runs, In this setting, the CA framework employs QF as a learn-
leadingtounstableagents(refertoFig.3(b)).Specifically,in ing algorithm, referred to as CA-MQF. Due to the agents’
40% of the runs, agents learn to push malfunctioning agents lack of access to individual rewards, relational networks
toward a resource and then proceed to the nearest resource. are applied to their state-action values. As demonstrated in
However, in 60% of the runs, agents either become stuck TableII,bothCA-MQF,withtherelationalnetworkdepicted
pushing each other or consume resources without assisting inFig.2(b),andMADDPGexhibitcomparableperformance,
the malfunctioning agent. and they surpass IQF in achieving distance from the origin
Ontheotherhand,theCAframework,employingVDNas in the +x direction. However, CA-MQF outperforms both
its learning algorithm (denoted as CA-VDN), incorporates a MADDPGandIQFintermsofrewards,asMADDPGtends
relationalnetworkasillustratedinFigure1(c).Thisnetwork to generate excessively large actions.
enables all agents to prioritize the green agent, identified as In the 30000th episode, a simulated malfunction restricts
the malfunctioning agent, thereby expediting the adaptation the movement of one leg, leading to a significant decrease
process. In this setting, the relational network is directly in the average rewards of the algorithms, as depicted in
integrated into the rewards, given that the agents have Fig. 4(a). This incident necessitates modification of the re-
accesstotheirindividualrewards.Theobservedcollaborative lational network from Fig. 2(b) to Fig. 2(c). The importance
behavior, aimed at optimizing team performance, is charac- placed by non-malfunctioning agents to the malfunctioning
terizedbythefollowingmovements:(i)theredagentpushes one drops to zero (refer to Fig. 2(c)), as its state-action
the green agent downward by one cell, (ii) the orange agent values becomes unreliable because of the malfunction. The
movesthegreenagentonecellright.Thiscooperationallows trajectories in Fig. 4(b-c) illustrate that CA-MQF, with its
thegreenagenttoaccessthenearestresourcelocationdespite adjusted inter-agent relationships, is effective in recovering
its malfunction. The framework’s effectiveness is validated from such malfunctions. While MADDPG outperforms IQF,
byacomparativeanalysisoftheindividualandtotalrewards it does not reach the level of adaptability demonstrated by
of each agent before and after the malfunction, with the CA-MQF. The accompanying video shows the animation of
results presented in Table I. this experiment.1
Upon deeper analysis, a noticeable difference is observed
The CA framework outperforms both IQF and MAD-
between the average team reward depicted in Figure 4(a)
DPG in the MaMuJoCo-Ant environment for continuous
and Table II. This difference arises because, to reduce the
action domains. Once the ant is fully capable of navigating
trainingtime,thetestingaverageteamrewardwascomputed
the map (up to the 30000th episode), all tested algorithms
with only 100 episodes, pausing training every 1000 steps,
enable four agents (representing each leg) to collaboratively
learn to move the ant in the +x direction. It is crucial to 1Accompanyingvideo:https://youtu.be/-0Qd5jyRGIY
CA-MQF CA-MQF
IQF IQF
Malfunction MADDPG MADDPG
(a) (b) (c)
Fig. 4: MaMuJoCo-Ant results: (a) Average team rewards before and after malfunction occurred at the 30000th episode. (b-c) Robot
trajectoriesinx-yplane:(b)beforeand(c)aftermalfunction,uponcompleting30kand60ktrainingepisodes,respectively.Itcanbeseen
that the robot can cover more distance using CA-MQF with higher rewards.TABLE II: Average metrics with 95% confidence intervals for
[7] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
three runs upon training completion.
Bellemare,A.Graves,M.Riedmiller,A.K.Fidjeland,G.Ostrovski,
et al., “Human-level control through deep reinforcement learning,”
BeforeMalfunction AfterMalfunction
nature,vol.518,no.7540,pp.529–533,2015.
TeamReward OriginDistance TeamReward OriginDistance [8] Y.Shoham,R.Powers,andT.Grenager,“Ifmulti-agentlearningisthe
IQF 568.86±16.06 10.37±0.33 369.95±204.02 8.81±1.65 answer,whatisthequestion?”Artificialintelligence,vol.171,no.7,
MADDPG 296.24±15.64 13.81±0.25 340.23±20.02 11.37±0.40 pp.365–377,2007.
CA-MQF 724.38±19.16 13.20±0.43 768.02±14.55 12.80±0.14 [9] L.Busoniu,R.Babuska,andB.DeSchutter,“Acomprehensivesurvey
ofmultiagentreinforcementlearning,”IEEETransactionsonSystems,
Man, and Cybernetics, Part C (Applications and Reviews), vol. 38,
whereas the data in Table II were obtained from 10000 test no.2,pp.156–172,2008.
[10] L. Matignon, G. J. Laurent, and N. Le Fort-Piat, “Independent rein-
episodes upon training completion, leading to more reliable
forcement learners in cooperative markov games: a survey regarding
results. This may also indicate that the IQF and MADDPG coordinationproblems,”TheKnowledgeEngineeringReview,vol.27,
models are not stable, as their rewards decreased when the no.1,pp.1–31,2012.
[11] R. de Lazcano, K. Andreas, J. J. Tai, S. R. Lee, and J. Terry,
number of test episodes increased.
“Gymnasium robotics,” 2023. [Online]. Available: http://github.com/
Overall, the results from experiments in both the discrete Farama-Foundation/Gymnasium-Robotics
and continuous environments highlight that agents trained [12] A.Tampuu,T.Matiisen,D.Kodelja,I.Kuzovkin,K.Korjus,J.Aru,
J.Aru,andR.Vicente,“Multiagentcooperationandcompetitionwith
using the Collaborative Framework exhibit enhanced co-
deepreinforcementlearning,”PloSone,vol.12,no.4,2017.
operative behaviors. Furthermore, these agents display an [13] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi,
improvedcapacityforadaptingtheirpoliciestorecoverfrom M.Jaderberg,M.Lanctot,N.Sonnerat,J.Z.Leibo,K.Tuyls,etal.,
“Value-decompositionnetworksforcooperativemulti-agentlearning,”
unexpected malfunctions, capitalizing on the dynamics of
arXivpreprintarXiv:1706.05296,2017.
inter-agent relationships. [14] Y. Findik and S. R. Ahmadzadeh, “Mixed q-functionals: Advancing
value-based methods in cooperative marl with continuous action
VII. CONCLUSIONANDFUTUREWORK domains,”2024,p.9.
[15] R.Lowe,Y.I.Wu,A.Tamar,J.Harb,O.PieterAbbeel,andI.Mor-
Thisstudypresentsanovelframeworkthatintegratesinter-
datch, “Multi-agent actor-critic for mixed cooperative-competitive
agent relationships into the learning process, specifically environments,” Advances in neural information processing systems,
addressing unexpected malfunction scenarios. The efficacy vol.30,2017.
[16] C. J. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8,
of our method in improving cooperative behaviors among
pp.279–292,1992.
agents and facilitating effective adaptation to unforeseen [17] S. Lobel, S. Rammohan, B. He, S. Yu, and G. Konidaris, “Q-
robotic malfunctions has been demonstrated in both contin- functionalsforvalue-basedcontinuouscontrol,”inProceedingsofthe
AAAIConferenceonArtificialIntelligence,vol.37,no.7,2023.
uous and discrete environments. The experimental findings
[18] M.Tan,“Multi-agentreinforcementlearning:Independentvs.cooper-
indicatethatthetestedalgorithmsoftenstuckatsub-optimal ativeagents,”inProceedingsofthetenthinternationalconferenceon
solutions, thus decreasing team rewards, whereas our pro- machinelearning,1993,pp.330–337.
[19] C.ClausandC.Boutilier,“Thedynamicsofreinforcementlearningin
posed algorithm successfully mitigates and recovers from
cooperativemultiagentsystems,”AAAI/IAAI,vol.1998,no.746-752,
these malfunctions. Future research will aim to expand the p.2,1998.
experiments to more complex situations involving multiple [20] T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster,
and S. Whiteson, “Monotonic value function factorisation for deep
agents encountering various unexpected malfunctions (for
multi-agentreinforcementlearning,”TheJournalofMachineLearning
instance, noisy or adversarial leg/agent) and undertake a Research,vol.21,no.1,pp.7234–7284,2020.
comparative analysis with other leading methodologies to [21] Y.Findik,P.Robinette,K.Jerath,andS.R.Ahmadzadeh,“Impactof
relational networks in multi-agent learning: A value-based factoriza-
further evaluate the robustness and efficiency of our frame-
tionview,”in202362ndIEEEConferenceonDecisionandControl
work in these settings. (CDC). IEEE,2023,pp.4447–4454.
[22] Y.Findik,H.Osooli,P.Robinette,K.Jerath,andS.R.Ahmadzadeh,
VIII. ACKNOWLEDGMENTS “Influence of team interactions on multi-robot cooperation: A re-
lational network perspective,” in 2023 International Symposium on
This work is supported in part by NSF (IIS-2112633) and
Multi-RobotandMulti-AgentSystems. IEEE,2023,pp.50–56.
the Army Research Lab (W911NF20-2-0089). [23] S. Lim, A. Joseph, L. Le, Y. Pan, and M. White, “Actor-expert: A
framework for using q-learning in continuous action spaces,” arXiv
REFERENCES preprintarXiv:1810.09103,vol.9,2018.
[24] R.S.Sutton,D.McAllester,S.Singh,andY.Mansour,“Policygradi-
[1] A. Dorri, S. S. Kanhere, and R. Jurdak, “Multi-agent systems: A entmethodsforreinforcementlearningwithfunctionapproximation,”
survey,”IeeeAccess,vol.6,pp.28573–28593,2018. Advancesinneuralinformationprocessingsystems,vol.12,1999.
[2] A.Kleiner,J.Prediger,andB.Nebel,“Rfidtechnology-basedexplo- [25] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Ried-
rationandslamforsearchandrescue,”in2006IEEE/RSJInternational miller, “Deterministic policy gradient algorithms,” in International
ConferenceonIntelligentRobotsandSystems. IEEE,2006,p.4054. conferenceonmachinelearning. Pmlr,2014,pp.387–395.
[3] S. D. Pendleton, H. Andersen, X. Du, X. Shen, M. Meghjani, Y. H. [26] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
Eng, D. Rus, and M. H. Ang Jr, “Perception, planning, control, and D.Silver,andD.Wierstra,“Continuouscontrolwithdeepreinforce-
coordinationforautonomousvehicles,”Machines,vol.5,no.1,2017. mentlearning,”arXivpreprintarXiv:1509.02971,2015.
[4] F.A.Oliehoek,M.T.Spaan,andN.Vlassis,“Optimalandapproxi- [27] B. Peng, T. Rashid, C. Schroeder de Witt, P.-A. Kamienny, P. Torr,
mateq-valuefunctionsfordecentralizedpomdps,”JournalofArtificial
W. Bo¨hmer, and S. Whiteson, “Facmac: Factored multi-agent cen-
IntelligenceResearch,vol.32,pp.289–353,2008. tralisedpolicygradients,”AdvancesinNeuralInformationProcessing
[5] M.Hu¨ttenrauch,S.Adrian,G.Neumann,etal.,“Deepreinforcement Systems,vol.34,pp.12208–12221,2021.
learningforswarmsystems,”JournalofMachineLearningResearch,
[28] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for
vol.20,no.54,pp.1–31,2019. model-basedcontrol,”in2012IEEE/RSJinternationalconferenceon
[6] S. Levine, C. Finn, T. Darrell, and P. Abbeel, “End-to-end training intelligentrobotsandsystems. IEEE,2012,pp.5026–5033.
of deep visuomotor policies,” The Journal of Machine Learning
Research,vol.17,no.1,pp.1334–1373,2016.