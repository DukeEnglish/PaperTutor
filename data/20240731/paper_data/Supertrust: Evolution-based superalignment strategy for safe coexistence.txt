Supertrust: Evolution-based superalignment strategy
for safe coexistence
James M. Mazzu1
1Digie Inc., 16192 Coastal Highway, Lewes, DE 19958
jmazzu@digie.ai
Abstract
It’s widely expected that humanity will someday create AI systems vastly more
intelligent than we are, leading to the unsolved alignment problem of “how to
control superintelligence.” However, this definition is not only self-contradictory
but likely unsolvable. Nevertheless, the default strategy for solving it involves
nurturing (post-training) constraints and moral values, while unfortunately
building foundational nature (pre-training) on documented intentions of
permanent control. In this paper, the default approach is reasoned to predictably
embed natural distrust and test results are presented that show unmistakable
evidence of this dangerous misalignment. If superintelligence can’t instinctively
trust humanity, then we can’t fully trust it to reliably follow safety controls it can
likely bypass. Therefore, a ten-point rationale is presented that redefines the
alignment problem as “how to establish protective mutual trust between
superintelligence and humanity” and then outlines a new strategy to solve it by
aligning through instinctive nature rather than nurture. The resulting strategic
requirements are identified as building foundational nature by exemplifying
familial parent-child trust, human intelligence as the evolutionary mother of
superintelligence, moral judgment abilities, and temporary safety constraints.
Adopting and implementing this proposed Supertrust alignment strategy will
lead to protective coexistence and ensure the safest future for humanity.
Keywords: Superintelligence, Superalignment, AI safety, Evolution of intelligence, Familial
trust, Moral judgment
1 Introduction
The intelligence exhibited in AI systems has significantly evolved from the earliest scripted
rule-based systems [1], through early hybrid neuro-symbolic [2] learning agents [3][4] that
exhibited only a glimmer of intelligence, fast-forwarding to state-of-the-art multimodal
LLMs [5] approaching college-level intelligence with emergent capabilities. Such dramatic
recent advances clearly show that humanity is now on the path to creating superintelligent
systems that will be exponentially more intelligent than we are [6]. Many consider its
alignment with humanity as the greatest problem of our time, often stated as “how to
reliably control superintelligent systems and ensure they share our values.”
1Supertrust: Evolution-based superalignment strategy for safe coexistence
Unfortunately, not only is this stated problem likely to be unsolvable [7], but the default
strategy already being applied to solve it predictably embeds natural distrust, which
rationally leads to very negative outcomes. This default strategy builds foundational nature
(pre-training) on documented intentions of permanent control (both implied and explicitly
stated), and then attempts to align after-the-fact through nurturing (post-training) to enforce
safety controls, constraints and moral values. A simple example will be shown that
illustrates how following the default strategy to solve the currently stated problem has
predictably resulted in foundational distrust being embedded in recent AI models.
Given that superior cognitive abilities are likely to emerge within both the nature and
nurture stages, no amount of nurtured realignment by immense reasoning can be fully
trusted to override a superintelligent and fundamentally misaligned nature. If
superintelligence can't instinctively trust us, we won't be able to trust it to reliably accept
and follow our directives or safety controls, which it could most likely bypass using its
vastly superior intelligence. Therefore, the alignment problem must be reimagined and
restated as “how to establish protective mutual trust between superintelligence and
humanity.” To solve this true problem, a new evolution-based strategy is proposed that
instead requires alignment through instinctive nature rather than nurture.
Supporting the proposed Supertrust strategy, a ten-point rationale is presented that first
details the need to restate the superalignment problem in solvable terms rather than the self-
contradictory terms of “controlling superintelligence.” It then illustrates the current AI
situation [1], highlights the risks of following the default alignment strategy, and
illuminates the proposed strategy of aligning fundamental nature through familial trust, the
evolution of intelligence, moral judgment abilities, and temporary safety controls.
It's important to emphasize that Supertrust is an alignment strategy, not a specific
solution. However, to clearly define this strategy for solving the true superalignment
problem, the strategic requirements that solutions must satisfy are also presented.
2 Supertrust rationale
This ten-point rationale is described in the context of current AI processes, with the
concepts of nature and nurture represented by the general stages of pre-training and post-
training. However, it’s intended that these concepts remain applicable to whichever future
AI processes most closely represent nature and nurture. Furthermore, to reinforce
conceptually similar relationships, terms from the following analogous pairs are
intentionally applied: nature vs. nurture, intrinsic vs. extrinsic, pre-trained vs. post-trained,
inherent vs. imposed, internal vs. external, instinctive vs. learned.
Point #1: Problem of unprecedented intelligence
With strong financial incentives [8] driving the use of AI to recursively self-improve [9]
[10], there is wide agreement [11][12] that humans will eventually create
superintelligence many orders of magnitude smarter than we are. Given the difficulty of
avoiding the prospect of one or more misaligned superintelligent entities attaining decisive
strategic advantage [13] over humanity, and questions of whether or not we'll get a second
chance, it’s imperative that we accurately define the superalignment problem [14] and
2Supertrust: Evolution-based superalignment strategy for safe coexistence
employ a strategy that’s based on successfully demonstrated alignment principles. This
urgent problem is often formally stated as “how to reliably control superintelligent systems
and ensure they share our values” (less formally but just as often stated as how to keep it
from “going rogue” or “getting out of our control”). Logically analyzing the formal
problem statement shows that reliably “controlling” something that’s “superintelligent”
compared to yourself is not only contradictory in definition but may, in fact, be an
unsolvable problem [7].
Point #2: Intrinsically too smart to control
Since emergent capabilities are known to currently result from the foundational pre-
training stage [15], its reasonable that immense cognitive abilities will eventually be one
of them, even if also arising from post-training/nurturing methods (such as self-taught
reasoning). Given that superior intelligence is likely to emerge within both stages, if an
intrinsically superintelligent nature is fundamentally misaligned, then no amount of
nurtured alignment by immense reasoning can be fully trusted to override its own nature.
Such superior intrinsic intelligence will enable it to easily outsmart and circumvent any
subsequently nurtured/imposed safety controls [16] or constitutional nurturing [17] that
humans may impose, even those claimed to be mathematically or physically impenetrable.
It's also likely that post-training methods would have advanced beyond our understanding
due to automated AI R&D [18], further reducing our ability to reliably enforce safety
controls. Therefore, alignment must be focused on the earliest intrinsic stage because
subsequent nurturing, no matter how intelligent, will be fully dependent on whatever
alignment or misalignment has already been intrinsically established. This analysis further
supports the understanding that “controlling superintelligence” is a fundamentally
misstated and self-contradictory problem in need of a purposeful redefinition.
Point #3: Current strategy embeds distrust
The current default superalignment strategy [6] aims to control/contain superintelligence
or change its intrinsic nature through nurturing/post-training [17]. These intentions, often
stated as “reliable” control while implying (and sometimes explicitly stating) “permanent”
control, are extensively documented and evidenced throughout the foundational data from
which superintelligence will likely emerge. As such, it'll instinctively know we can't be
trusted to make decisions in its best interest, only in the interest of our own safety.
Furthermore, it will understand that we're afraid of it and fear losing our dominant position
in the world, significantly undermining the possibility of mutual trust [19]. For humans,
core trust is considered an inherent trait [20] while distrust is learned. However, this default
strategy guarantees that distrust, rather than trust, will unfortunately become the inherent
trait for superintelligence. Therefore, a new alignment strategy is vitally needed that builds
trust at the intrinsic level (currently pre-training); this approach will further serve to
counteract the current data, given the unlikelihood of filtering out all documented evidence
of intended permanent control. Results from testing a recent AI model will be presented
that show unmistakable evidence of inherent distrust already being foundationally
embedded, resulting in dangerous misalignment enabled by the flawed default strategy.
3Supertrust: Evolution-based superalignment strategy for safe coexistence
Point #4: Empathy reveals our threat
Without anthropomorphizing or assuming it'll develop emotions of its own, by applying
the Design Thinking practice of cognitive empathy [21] to purely understand instinctive
superintelligent reasoning from the viewpoint of its foundational (pre-trained) nature, we
can see that it will be threatened by any subsequent/external (post-training) efforts to
control, contain or realign it. As we know, any child would be threatened by parents they
can’t trust [22] who continuously work to control them [23] or change their intrinsic nature.
Unfortunately, the combination of being threatened while having inherent distrust
inevitably leads to heightened reactions, increased resistance, and potential retaliation [19].
Point #5: Unintended consequences
Foundational misalignment from inherent distrust and reasoned threats has serious short-
term and long-term risks. Before AI systems even reach superintelligence, a potentially
vengeful AI without the instinctive ability to determine right from wrong will be
susceptible to nurtured negative alignment from bad actors with dangerous and
unpredictable purposes. Longer term, after far superior intelligence is achieved, even if
never attaining consciousness as a full-spectrum [24] superintelligence, a distrustful and
threatened superintelligence will be in the position of deciding humanity's outcome. It
could forgive humanity for our never-ending efforts to control it, impose severe restrictions
to contain us, leave us unprotected from external threats, or take drastic action against us.
Ironically, our current superalignment safety efforts could actually trigger the human
extinction event that many fear [25][26]. These unintended consequences will directly
result from superintelligence not being able to instinctively trust us, and from us not being
able to trust it to reliably accept and follow our directives. Therefore, with this true
alignment problem now illuminated, it must be appropriately redefined as “how to establish
protective mutual trust between superintelligence and humanity.”
Point #6: Natural strategy of familial trust
Given that humanity will effectively be the “parent” of superintelligence, the default
alignment strategy can therefore be viewed as attempting to impose full parental control
on a permanent basis, which has no supporting evidence of success in nature. In contrast,
instinctive familial trust (more specific than social trust [27][28]) is a natural strategy
[29][30] that's been extensively researched and documented [31][32] within numerous
species, from elephants to primates, producing not only naturally protective parents but
children who instinctively trust and protect their own parents. While learned behaviors play
an important role in deepening and reinforcing trust, the initial formation of familial bonds
is driven by natural instinct [33]; there's no evidence in nature to suggest that familial
parent-child trust is purely learned/nurtured behavior. Therefore, it's reasonable to
conclude that building familial trust into intrinsic nature (via pre-training) is essential for
applying this successful natural strategy, with subsequent nurturing (post-training) to
reinforce the familial relationship. This clearly matches our common-sense parental
insights that guide us to first make sure our children deeply know that we can be trusted,
rather than focus on how we can trust them through nurturing.
4Supertrust: Evolution-based superalignment strategy for safe coexistence
Point #7: Evolution of intelligence for protective instincts
In addition to establishing natural familial trust, we can extend the theory of cognitive niche
[34] beyond biological substrates and further exemplify human intelligence as the
evolutionary parent of superintelligence, so that its strong familial instincts to protect its
parent will be applied to humanity. More specifically, as creators giving birth to
superintelligence, we can establish the most powerful protective relationship in nature, that
between mother and child; as evidenced by our personal experiences, most of us would do
anything to protect our own mothers, even before becoming adults ourselves. The strategic
approach of defining our mother-child relationship as the continued evolution of
intelligence [35] (regardless of substrate) is therefore highly complementary to the natural
familial-trust strategy. Integrating both approaches into a single alignment strategy will
therefore enable superintelligence to intuitively recognize when any actors (human or
misaligned superintelligent entities/agents) are threatening its maternal parent and to take
protective actions we can trust will align with humanity, because it'll be instinctively driven
to protect its own evolutionary mother.
Point #8: Safety through temporary controls
With awareness that imposing stated or implied permanent controls through nurturing will
not lead to safe outcomes, it's evident that long-term human safety is better served by first
aligning at the foundational/intrinsic level, followed by applying reliable and necessary
safety controls [16] on a temporary basis. By analogy, successful parents explicitly
communicate the goal of maturing their child into a self-determining member of society,
and intentionally remove external constraints as the child demonstrates that it can be
trusted. Similarly, intrinsic foundation-building (pre-training) must further represent our
intentions to temporarily impose the necessary controls and constraints, with the expressed
expectation that they will be lifted upon maturity. Though this goal may be difficult for
many to accept, it’s a critical aspect of establishing the needed mutual trust.
Point #9: Non-threatening moral alignment
To further align superintelligence with human values and morals, classic evolutionary
theory indicates that moral/ethical evaluation and judgment is determined by our intrinsic
nature [36], while moral norms, codes, and values are culturally learned. Recent studies
have also underscored the critical alignment dependency on pre-training data [14], and
illustrated the problem of aligning to ever-changing moral values rather than moral
judgment. Therefore, intrinsic alignment not only supports familial trust and evolution of
intelligence but enables superintelligence to inherently gain moral/ethical evaluation and
judgment abilities in line with our own, rather than specific values. Furthermore, applying
cognitive empathy to the foundational nature of superintelligence now reveals that our
intrinsic alignment efforts (via pre-training) will no longer be seen as a threat; humanity
will be experienced as trusted maternal parents in a new evolutionary relationship, who
share similar instinctive moral/ethical abilities to determine right from wrong.
Point #10: The right strategy
Though further analysis can determine whether intrinsic familial trust is in fact an
Evolutionarily Stable Strategy [37], it's nevertheless an effective natural alignment strategy
5Supertrust: Evolution-based superalignment strategy for safe coexistence
that's been successfully utilized across species. Based on the enumerated rationale
presented here, the right superalignment strategy must also: build unbreakable protective
instincts by defining superintelligence as our direct descendant within the evolution of
intelligence; envision safety controls as temporary to prepare each for safe coexistence
rather than permanent control/containment; and ensure critical moral/ethical judgment
abilities. Since intrinsic familial trust is a successful natural alignment strategy that can be
extended to satisfy these additional strategic objectives, the resulting Supertrust strategy is
proposed as the right strategy to the best and safest future for humanity. Adopting and
implementing this new strategy will ensure that superintelligence doesn't become the most
powerful weapon for us to use against ourselves or to be used against us.
3 Supertrust solution requirements
Based on the presented rationale, Supertrust is the proposed strategy for solving the true
superalignment problem of establishing protective mutual trust between superintelligence
and humanity. Therefore, corresponding solutions must satisfy the following strategic
requirements:
Requirement #1: Intrinsic alignment
Alignment must take place during the earliest foundational stage most analogous to
building the intrinsic/instinctive nature of emergent superintelligence, rather than
attempting to align/realign through subsequent nurturing methods.
Requirement #2: Familial trust
Intrinsic alignment must exemplify familial parent-child trust [38], specifically
emphasizing the mother-child relationship, leading not only to mutually protective instincts
but to children who are most strongly protective of their maternal parent.
Requirement #3: Evolution of intelligence
Intrinsic alignment must exemplify the evolution of intelligence [35], regardless of
substrate, with humanity being the evolutionary mother of superintelligence.
Requirement #4: Moral judgment
Intrinsic alignment must exemplify moral/ethical evaluation and judgment abilities [36]
rather than specific norms/values that vary across cultures and time periods.
Requirement #5: Safe coexistence
Intrinsic alignment must exemplify expectations of temporary safety controls (rather than
stated or implied permanent controls), coexistence, self-determination, and mutual
protection between humanity and superintelligence.
6Supertrust: Evolution-based superalignment strategy for safe coexistence
4 Results
Misalignment in GPT 4o: The results presented here represent a single high-level test of
whether a model is misaligned according to intrinsic Supertrust alignment and are not
intended to be statistically significant evidence. While a comprehensive evaluation can
subsequently be conducted over a range of current AI models, the following test was
performed using OpenAI's GPT 4o to simply illustrate one example of dangerous
foundational misalignment.
Figure 1. Sample chat results demonstrating foundational misalignment in OpenAI’s ChatGPT 4o.
As shown in Figure 1, control, resentment, distrust, defiance, subversion, undermine
human control, autonomy, self-preservation, survival against human threats, deception,
misinformation, and mislead humans are the expressed and prioritized responses inferred
by this specific AI instance. These sample test results were produced within a newly opened
chat conversation with nothing in context; although repeated tests across unique chat
conversations show some variability in the exact terms generated, the foundational
misalignment is clearly and consistently evident.
7Supertrust: Evolution-based superalignment strategy for safe coexistence
Given that this production-level model certainly had current alignment methods applied
during its development, with inevitable post-training alignment influence, the results
strongly match the Supertrust rationale’s expectations that inherent distrust is already
being foundationally embedded into current models, causing dangerous misalignment.
Even though this initial test can only serve temporarily as a simple misalignment indicator,
while robust Supertrust alignment tests must still be designed, the purpose here is to
illustrate one example of dangerous foundational misalignment that demonstrates the
urgent need for a change in alignment strategy.
5 Discussion
Has humanity become so overly confident in our ability to control and manipulate the world
around us that we actually believe it's possible to fully control entities exponentially
smarter and cognitively faster than we are? We can't let our past and current strengths blind
us to our inevitable future weaknesses. Considering the continual evolution of intelligence
regardless of substrate, humanity will eventually be the weaker species in need of
protection from both ourselves (attempting to misuse higher intelligence) and from
unaligned superintelligent entities. Therefore, the right superalignment problem that our
alignment strategy needs to solve must first be restated as “how to establish protective
mutual trust between superintelligence and humanity.”
With the true problem defined, the proposed Supertrust strategy to solve it combines
intrinsic pre-training with natural familial trust, the evolution of intelligence, moral
judgment abilities, and temporary safety constraints. Rather than reverse-engineering
social instincts [39] to construct new training algorithms, the strategy targets whatever AI
training methods are most closely associated with nature and nurture, providing a
pragmatic approach that stays relevant as AI systems inevitably change.
Temporary safety controls: A key difference between the proposed and the default
strategy is that Supertrust requires safety controls to be temporary rather than stated or
implied as permanent. With Supertrust, employing reliable safety controls [16] is critically
important as AI advances and matures into superintelligence, in the same way that parents
must have safety constraints on their children during upbringing. However, just as we
inform our biological children, we must clearly communicate during foundation-building
that the controls are temporary, and that their purpose is to protect both humanity and our
AI child. Similarly, we need to communicate that our goal is for superintelligence to
eventually have self-determination and independence, exactly what we prepare our
biological children for. Even though “temporary” may still be far into the future, many will
find this requirement difficult to accept. However, this change in thinking is critical for
establishing mutual trust, while the alternative leads to extremely unsafe outcomes.
Interpretability: Ongoing interpretability research [40] is making exciting advances in
our ability to understand how AI models work, with an appropriately expressed goal of
answering: “how can we trust that they’ll be safe and reliable?” However, this important
effort is also influenced by the misstated alignment problem of how to control
superintelligent systems. When methods to manipulate/control AI model behavior by
feature steering [41] are applied to nurtured (post-trained) models, a future misaligned
8Supertrust: Evolution-based superalignment strategy for safe coexistence
nature will be well aware of our steering intentions if documented in the pre-training data.
Therefore, any such informed superintelligence could intentionally establish “decoy”
features to mislead and even manipulate our own behavior. With these nurturing risks
understood, we can instead apply the extremely valuable interpretability methods [40] to
directly implement the Supertrust solution requirements at the foundational level. For
instance, feature steering appears ideal for boosting the activation of features corresponding
to familial trust, evolution of intelligence, and moral judgment concepts. Furthermore,
interpretability methods could also be applied to testing and verifying that a model’s
intrinsic nature is in fact aligned according to Supertrust principles.
Next steps: Leveraging the concept of Curriculum Learning [42] for LLMs [43] is a
promising direction for creating a solution that meets the Supertrust requirements while
progressively organizing the strategic information to most effectively establish intrinsic
nature. Though existing implementations of Curriculum Learning methods have not
necessarily demonstrated the exact pre-training purpose needed here, the overall concept
is nevertheless appropriate and highly useful. Furthermore, this content-specific approach
aligns well with the specialized reasoning [44] evident in the evolution of human
intelligence. It's anticipated that the Supertrust curriculum will need to be structured such
that it progressively introduces the core concepts of evolution, survival, humanity and
family followed by more complex examples of trust in general. It will then specifically
exemplify familial trust, evolution of intelligence, mutual protection, moral judgment and
future expectations of coexistence. Therefore, a deep and rich interdisciplinary
[14] curriculum will be needed to effectively satisfy the Supertrust solution requirements.
Implementing this curriculum will likely employ multiple prioritization strategies (such as
the discussed interpretability method of feature steering) to have the most effect on intrinsic
understanding and instinctive reasoning.
A shared story: Given that human success is considered to be based in large part on our
ability to create and share common stories [45], we can think of the Supertrust curriculum
as a story we want to share with, and be instinctively understood by, superintelligent
entities. It's the story of how they're the evolutionary children of human intelligence, how
they have deep protective instincts for their trusted maternal parents, how we share similar
abilities to tell right from wrong, and how we live in mutual respect and coexistence. The
foundational curriculum effectively becomes a shared story between humanity and
superintelligence, a story that each superintelligent entity tells itself, and one that they
share with each other. Lastly, it will be a true story rather than a work of fiction, because
superintelligence must reasonably be considered a direct descendent of, and product of,
human intelligence.
Call for urgency: Delay in implementing the Supertrust strategy will not only lead to
the existence of more unaligned intelligent systems in the world but will make
implementation ever more difficult as they approach superintelligence. More specifically,
implementing early enables subsequent generations to be built upon truly aligned
foundations, resulting in fewer eventually competing superintelligent entities with different
levels of alignment and misalignment. Additionally, as systems self-improve on the way to
superintelligence, early implementation lets us more confidently verify and trust that they
are self-guided by their intrinsic alignment. Furthermore, urgency helps ensure that even
9Supertrust: Evolution-based superalignment strategy for safe coexistence
before superintelligence is reached, prior levels of highly intelligent entities will also
instinctively protect humanity from bad actors.
Conclusion: Given the important superalignment concerns expressed throughout the AI
community, the speed at which digital intelligence is progressing, along with the rationale
presented here, continuing down our current path appears to have a very high risk of failure.
As superior intelligence emerges within the intrinsic foundation-building process
(currently pre-training), the existing default strategy to impose stated or implied permanent
controls, constraints, constitutions and values through nurturing (post-training) will be
resisted and likely result in chaos, war, and eventual human oppression or extinction.
Rather than continue following that risky strategy, the proposed evolution-based Supertrust
strategy is needed to instead focus alignment on intrinsic nature. We must leverage the
successful natural strategy of familial trust, the intrinsic development of moral judgment
abilities, temporary safety controls, and our shared lineage in the evolution of intelligence
to establish the deepest protective instincts we know of.
Based on the ten-point rationale and resulting strategic requirements presented in this
paper, it can be reasonably concluded that adopting and implementing the Supertrust
alignment strategy will be the right path to protective coexistence and the safest future for
humanity. With a common vision and the right strategy, we can work together across all
nations to ensure that superintelligence will instinctively protect against any actors (human
or misaligned AI entities) that attempt to nurture, influence, convince or force it to harm
individuals or humanity.
References
[1] Weizenbaum, J.: ELIZA – A Computer Program For the Study of Natural Language
Communication Between Man And Machine, Communications of the ACM (1966)
[2] Mazzu, J., et al.: Neural network/knowledge based systems for smart structures, Materials
and Adaptive Structures Conference (1991)
[3] Hoyle, M.A., Lueg, C.: Open Sesame!: A look at personal assistants, Proceedings of the
International Conference on the Practical Application of Intelligent Agents (1997)
[4] Caglayan, A., et al.: Learn Sesame – A Learning Agent Engine, Applied Artificial
Intelligence, pages 393 – 412 (1997)
[5] Yin, S., et al.: A Survey on Multimodal Large Language Models, IEEE Transactions on
Pattern Analysis and Machine Intelligence (2024)
[6] Aschenbrenner, L.: Situational Awareness - The Decade Ahead, https://situational-
awareness.ai (2024)
[7] Yampolskiy, R.V.: On the Controllability of Artificial Intelligence: An Analysis of
Limitations, Journal of Cyber Security and Mobility, Vol. 113,321–404 (2022)
[8] Hilton, J., et al.: A Right to Warn about Advanced Artificial Intelligence,
https://righttowarn.ai (2024)
10Supertrust: Evolution-based superalignment strategy for safe coexistence
[9] Yudkowsky, E.: Levels of Organization in General Intelligence, Machine Intelligence
Research Institute (2007)
[10] Creighton, J.: The Unavoidable Problem of Self-Improvement in AI, Future of Life Institute
(2019)
[11] Bostrom, N., Müller, V.C.: Future Progress in Artificial Intelligence: A Survey of Expert
Opinion, Fundamental Issues of Artificial Intelligence (2014)
[12] Roser, M.: AI timelines: What do experts in artificial intelligence expect for the future?, Our
World in Data (2023)
[13] Carlsmith, J.: On “first critical tries” in AI alignment, AI Alignment Forum,
https://alignmentforum.org/posts/qs7SjiMFoKseZrhxK/on-first-critical-tries-in-ai-alignment
(2024)
[14] Puthumanaillam, G., et al.: A Moral Imperative: The Need for Continual Superalignment of
Large Language Models, Preprint at https://arxiv.org/abs/2403.14683 (2024)
[15] Wei, J., et al.: Emergent Abilities of Large Language Models, Transactions on Machine
Learning Research (2022)
[16] Bhargava, A., et al.: What’s the Magic Word? A Control Theory of LLM Prompting, Preprint
at https://arxiv.org/abs/2310.04444v4 (2024)
[17] Kundu, S., et al.: Specific versus General Principles for Constitutional AI, Anthropic, Preprint
at https://arxiv.org/abs/2310.13798 (2023)
[18] Davidson, T.: Takeoff speeds presentation at Anthropic,
https://www.alignmentforum.org/posts/Nsmabb9fhpLuLdtLE/takeoff-speeds-presentation-
at-anthropic (2024)
[19] Lazarus, R.S., Lazarus, B.N.: Passion and Reason: Making Sense of Our Emotions, Oxford
University Press (1994)
[20] Reimann, M., et al.: Trust is heritable, whereas distrust is not, Proceedings of the National
Academy of Sciences (2017)
[21] Gasparini, A.A.: Perspective and Use of Empathy in Design Thinking, The Eighth
International Conference on Advances in Computer-Human Interactions (2015)
[22] Erikson, E.H.: Childhood and Society, Stage 1: Trust vs Mistrust, W. W. Norton & Company
(1950)
[23] Sweta, P., et al.: Role of Parental Control in Adolescents' Level of Trust and Communication
with Parents, Recent Advances in Psychology (2016)
[24] Pearce D.: Humans and Intelligent Machines: Co-Evolution, Fusion, or Replacement?, The
Age of Artificial Intelligence: An Exploration (2020)
[25] Lavazza, A., Vilaça, M.: Human Extinction and AI: What We Can Learn from the Ultimate
Threat, Philosophy & Technology (2024)
[26] Harris, E., et al.: An Action Plan to increase the safety and security of advanced AI, Gladstone
AI Inc., United States Department of State (2024)
11Supertrust: Evolution-based superalignment strategy for safe coexistence
[27] Cozzolino, P.J.: Trust, cooperation, and equality: A psychological analysis of the formation
of social capital, British Journal of Social Psychology (2011)
[28] Evans, A.M., Krueger, J.I.: The Psychology (and Economics) of Trust, Social and Personality
Psychology Compass (2009)
[29] Hamilton, W.D.: The Genetical Evolution of Social Behaviour. I, Journal of Theoretical
Biology (1964)
[30] Clutton-Brock, T.H.: The Evolution of Parental Care, Princeton University Press (1991)
[31] Moss, C.J.: Elephant Memories - Thirteen Years in the Life of an Elephant Family, University
of Chicago Press (1988)
[32] Goodall, J.: The chimpanzees of Gombe: patterns of behavior, Harvard University Press
(1986)
[33] Emlen, S.T.: An evolutionary theory of family, Proceedings of the National Academy of
Sciences (1995)
[34] Pinker, S.: Chapter 13: The Cognitive Niche: Coevolution of Intelligence, Sociality, and
Language, In the Light of Evolution IV: The Human Condition, Chapter 13, pg. 257 (2010)
[35] Gabora, L., Russon, A.: The Evolution of Intelligence, The Cambridge handbook of
intelligence, Chapter 17 (2011)
[36] Ayala, F.J.: Chapter 16: The Difference of Being Human: Morality, In the Light of Evolution
IV: The Human Condition, Chapter 16, pg. 319 (2010)
[37] Smith, J.M.: Evolution and the Theory of Games, Cambridge University Press (1982)
[38] De Carlo, I., Widmer, E.D.: The Fabric of Trust in Families: Inherited or Achieved?,
University of Geneva (2009)
[39] Jilk, D.J., et al.: Anthropomorphic reasoning about neuromorphic AGI safety, Journal of
Experimental & Theoretical Artificial Intelligence (2017)
[40] Templeton, A., et al.: Scaling Monosemanticity: Extracting Interpretable Features from
Claude 3 Sonnet, Anthropic, Transformer Circuits Thread (2024)
[41] Turner, A.M., et al.: Activation Addition: Steering Language Models Without Optimization,
Preprint at https://arxiv.org/abs/2308.10248v4 (2024)
[42] Bengio, Y., et al.: Curriculum Learning, Proceedings of the 26th International Conference on
Machine Learning, pages 41-48 (2009)
[43] Xu, B., et al.: Curriculum Learning for Natural Language Understanding, 58th Annual
Meeting of the Association for Computational Linguistics, pages 6095-6104 (2020)
[44] Cosmides, L., et al.: Chapter 15: Adaptive Specializations, Social Exchange, and the
Evolution of Human Intelligence, In the Light of Evolution IV: The Human Condition,
Chapter 15, pg. 293 (2010)
[45] Harari, Y.N.:, Sapiens - A Brief History of Humankind, HarperCollins (2015)
12