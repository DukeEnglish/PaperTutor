Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing
EkaterinaIakovleva1* FabioPizzati2∗ PhilipTorr2 Ste´phaneLathuilie`re1
1LTCI,Te´le´com-Paris,InstitutPolytechniquedeParis
2UniversityofOxford
Abstract
Ambiguous instruction
Make the dog cool!
Text-based editing diffusion models exhibit limited per-
formance when the user’s input instruction is ambiguous.
Tosolvethisproblem,weproposeSpecifyANdEdit(SANE),
a zero-shot inference pipeline for diffusion-based editing
systems. We use a large language model (LLM) to de- Input User Editing model Output
composetheinputinstructionintospecificinstructions,i.e.
well-definedinterventionstoapplytotheinputimagetosat-
i is nf sy trt uh ce tiou nse sr a’s lor ne gqu te hs et. orW ige inb ae ln oe nfi et ,fr tho am nkth se toLL aM n- od ve er liv de ed
-
MA am kb ei g tu hou es din ostr gu c cti oon
ol! No
training! AdSp dec sif uic nin gst lr au sct sio en
s!
noisingguidancestrategyspecificallydesignedforthetask.
Our experiments with three baselines and on two datasets
demonstrate the benefits of SANE in all setups. Moreover,
ourpipelineimprovestheinterpretabilityofeditingmodels,
and boosts the output diversity. We also demonstrate that Input User Editing model Output
ourapproachcanbeappliedtoanyedit, whetherambigu- + SANE
ous or not. Our code is public at https://github.
com/fabvio/SANE. Figure1.Problemdefinition.Abstractuserinstructionsmaylead
existing editing diffusion models to failure (top). SANE solves
thisproblembydecomposinginputinstructionsintospecificones,
satisfying the user’s request by integrating detailed edits in the
1.Introduction
editingprocess(bottom). SANEiscompletelyzero-shot,withno
trainingrequired.
Recent advances in text-to-image generation have at-
tracted a lot of attention in the research community and
beyond it. This success is primarily due to development
instruction “Make the dog cool.” What does it mean for a
of text-conditioned diffusion models [10,40,45,46,48]
dog to look cool? Is there only one way for a dog to ap-
whichallowtotranslatetextualconcepts,describedinnat-
pearcool? Toanswerthesequestions,theeditingmodelre-
ural language in the form of text prompts, into semanti-
quiresanuancedcontextualunderstanding. Thesamecool
callycoherentvisualizations.Besidesimagesynthesis,text-
adjective would suggest entirely different modifications if
conditioneddiffusionmodelshavedemonstratedstrongper-
the subject were an inanimate object, like furniture, or a
formance on the image editing task [6,21,27,31,49,56],
landscape. Moreover, there are multiple ways to make the
whereusersdescribeinplaintexttheeditinginstructionsto
doglookcool(e.g.,addingglasses,makingitsquint,chang-
applytoinputimages.
ing the surroundings), all of which are equally valid. To
Despitetheremarkablesuccessoftext-conditionededit-
address the multimodal nature of this task, editing models
ing methods, in this work, we start from the observation
needreasoningandabstractioncapabilitiesthatcurrentedit-
thattheseapproachesusuallyfailtosuccesfullyeditimages
ingdiffusionmodelslack[16].
when the edit prompt provided by the user is ambiguous.
To illustrate this, consider the example in Figure 1, which Toaddressambiguousinputinstructions,weproposeour
presentsaneditingtaskforapictureofadogwiththeuser
methodSpecifyANdEdit(SANE),whichleveragestherea-
soningcapacitiesandthegeneralknowledgeofLargeLan-
*Equalcontribution.Orderdecidedrandomly. guage Models (LLMs) [3,5,7,36,37]. More precisely,
1
4202
luJ
92
]VC.sc[
1v23202.7042:viXra
tra
eht
fo
etatS
sruOSANE breaks down the input edit into a series of specific Large Language Models. LLMs [3,5,7,36,37] are not
instructions that, when applied together, transform the in- only capable of human-like text completion, but are also
put into well-defined editing tasks. Through this process, successfullysolvingvariousreasoningtasks[1,3,4,7,9,47].
knownasspecification, wereversetheabstractionsassoci- Thisisachievedbyapplyingvariousreasoningandprompt-
atedwithinputinstructions. Byincorporatingvariousspe- ing techniques, e.g. In-Context Learning (ICL) [7], where
cificdetails,weeffectivelyreducetheoverallambiguityof the model is given a few task examples in the form of
theinputinstructions.Afterthat,weconditionapre-trained demonstration [13]. Another important direction of re-
editing diffusion model using both the specific and origi- search is Visual-Language Models (VLMs) [2,25,32,33,
nal ambiguous instructions. More precisely, we start with 43,44]whichaimtoconnectvisualandlanguagespaces.In
inferring individual noise estimations for each specific in- thiswork,weusemultimodalGPT-4o[41]tocaptionorig-
struction using the chosen diffusion model. These noise inalimages,andtodecomposeambiguousinstructionsinto
estimations are then combined using a novel strategy de- setsofspecificinstructionsusingICLandcaptions. There
scribed in Section 3.3. Finally, the combined noise, along areseveraleditingsystemsrelevanttoSANE,whichjointly
with the noise predicted from conditioning on the initial finetune diffusion models and VLMs to enhance input in-
ambiguousinstruction, isusedinamodifiedclassifier-free structions [16,26]. While addressing a similar problem of
guidance [23]. This allows to preserve the fidelity to the commonsense reasoning for image editing, these models
originaluserindication,whileguidingtheprocesswiththe rely on implicit concept interpretations learned by VLMs.
specificinterventions.Amongbenefitsonperformance,this In contrast, SANE is zero-shot and relies on explicit con-
allows us to provide the specific instructions to the user at ceptdecompositionperformedbyGPT-4o.
inferencetime,potentiallyraisingtheinterpretabilityofthe
Multi-instructionEditing Thereareseveralworksthatin-
editing instruction. Notably, SANE can be applied to an
vestigatemulti-instructionscenariosfortext-to-imagegen-
arbitrarypre-trainedinstruction-baseddiffusionmodelina
erationandediting[14,16,19,26,28,34,55]. Inimageedit-
zero-shotmanner.
ing,thediffusionmodelisgivenapre-definedsetofinstruc-
In short, our contributions are: (i) We propose the first tionstofollow.FoI[19]extractsaregionofinterestforeach
editingmethoddesignedspecificallytoaddressambiguous of them, and restricts InstructPix2Pix [6] to remain within
instructions. (ii) We introduce an LLM-based instruction the union of these regions. Instead of editing an image in
decompositionpipeline,andaconditioningmechanismfor one step, EMILIE [28] applies InstructPix2Pix iteratively,
editingdiffusionmodelscombiningambiguousandspecific oneinstructionatatime.Toavoidartifacts,theauthorspro-
instructions, specifically designed for the task. Our ap- pose to remain in the latent space of the diffusion model,
proachrequiresnotraining. (iii)Weperformextensiveex- andtodecodeonlythelasteditedlatentvariable. CCA[20]
perimentsontwodatasets,withthreestate-of-the-artmeth- buildsamulti-agentsystemthattakesacompositeinstruc-
ods,outperformingall.Weprovideadditionalresultsonthe tionasinput,thensplitsitintoelementaryinstructionsand
propertiesofSANE,andablations. iterativelyappliesthemusinganLLM,aVLM,andseveral
editing diffusion models. In this work, we also consider
2.Relatedwork sets of instructions, however, SANE is the first to address
instructiondecompositionforambiguousinstructions.
Diffusion-based Image Editing Diffusion mod-
els [22, 46, 50, 51] have achieved remarkable results 3.Method
in generative image modeling by representing the gener-
This section introduces the SANE pipeline. First, we
ative process as a series of denoising steps. Conditioning
presentnotationsandpreliminariesfordiffusion-basedim-
thesemodelsontexthasenabledcontrollabletext-to-image
age editing in Section 3.1. After that, we introduce our
synthesis[40,45,46,48],aswellasdevelopmentofdifferent
LLM-based instruction decomposition in Section 3.2, and
diffusion-basededitingsystems[8,11,31,35,40,42]. Some
ournovelinstructioncombinationstrategyinSection3.3.
systems rely on image inversion technique [12,51] to pro-
videeditedversionsofaninputscene[8,29,38,39,52,54].
3.1.BackgroundonDiffusion-basedImageEditing
Althougheffective,thesetechniquesarenormallycompute-
intensive due to the inversion process. The seminal We consider an instruction-based diffusion model such
InstructPix2Pix [6] solved this probelm by finetuning a as InstructPix2Pix [6]. The purpose of such models is to
diffusion model on instruction prompts, benefiting from edit an input image x while following a user-defined edit-
syntheticpairsofimagesfortraining. Althoughmanybuilt inginstructionc,inordertogenerateaneditedimagex˜.The
on this result [18,27,49,55,56], the impact of ambiguous editedimagerespectstheinputinstructionwhilepreserving
instructionsoninstruction-basededitingmodelsisstillnot the appearance of the original image. Following existing
explored,motivatingtheproposedSANE. work[6,27,56]weusealatentdiffusionmodel[46],where
2Instruction Specification (Sec. 3.2) Repeat times
Noise at step Noise at step
Ambiguous instruction
Noise update
Make the cat look funny
Output image
Input image
U-Net CFG
Ambiguous
estimated noise
LLM
U-Net
Specific instructions Combined
Add a hat to the cat Specific estimated noise
Add a bow tie to the cat estimated noises
Add a colored background Denoising loop Instructions Combination (Sec. 3.3) : frozen
Figure2.SANEinferencepipeline.WepromptanLLMtomapanambiguousinputinstructionctoasetofspecificinstructionsS(left).
Weprovideadescriptionofxascontext,inadditiontoc.OnceSinstructionsareextracted,weusetheminadditiontocinthedenoising
loopofaneditingdiffusionmodel(right). Foreachiteration,weestimatethenoiseinz byconditioningthediffusionU-Netf onall
t θ
instructions.Wethemcombineallspecificinstructionsinasinglenoiseestimation,thatwelateruseinclassifier-freeguidance(CFG).We
updatethenoisez toz followingstandardapproaches.AfterT iterations,weobtaintheoutputimagex˜=D(z ).
t t−1 0
the denoising operation is performed in the latent space of 3.2.InstructionSpecification
anautoencoderwithencoderE anddecoderD. Thesemod-
We noticed that directly applying ambiguous input
els typically include a denoising U-Net f . At inference
θ instructions as c may lead to limited editing perfor-
time,x˜isproducedbyiterativlydenoisingasampledGaus-
mance. This is due to the ambiguity of c which
siannoisez withf overT iterations. Inotherwords, at
T θ can be represented by multiple editing interventions.
stept,f isusedtoestimatethenoiseϵ inacorresponding
θ t Let us assume that x represents a cat, while c =
noisy latent z . The latent z is then updated to z , by
t t t−1 “make the cat look funny” is a user instruction,
removing the estimated noise ϵ and reintroducing Gaus-
t as in Figure 2 (left). As mentioned in Section 1, we aim
sian noise with lower intensity, following strategies from
tomapcwithanLLMtoasetofspecificandinterpretable
literature [22,30,51]. This is repeated for each t ∈ [1,T],
instructionsthatwouldsatisfytheuser’srequest,e.g. mapc
resultinginthefinalimagex˜=D(z ).
0 to“add a hat to the cat”.
In instruction-based models, noise estimation is con-
Formally, we want to extract from c a set of N editing
ditioned on the instruction c, which describes the de-
instructions S = {s }N , where each s is a specific in-
sired changes, and on the input x, to enforce consis- i i=1 i
struction describing one modification consistent with c, to
tency with the original image. This noise is denoted as
apply to the input scene. As shown in Figure 2 (left), we
ϵ = f (z ,E(x),c). In practice, instruction-based diffu-
t θ t prompt an LLM to map c to N specific instructions, pro-
sion models benefit from classifier-free guidance [23] to
vidingarichcaptionofxascontext. Ourpromptcontains
boostconsistencytowardstheinstructionandtheinputim-
general descriptions of ambiguous and specific modifica-
age[6]. Thismeansthatforeacht,z isdenoisedusingϵ˜,
t t tions,andtwoin-contextlearningexamples[7]ofambigu-
rather than ϵ , with ϵ˜ being a combination of three terms:
t t ous input instructions associated with desired specific in-
unconditioned, conditioned on the image, and conditioned
structions. Furthermore, we set additional restrictions on
ontheinstructionc:
the content and formatting style of the output instructions,
ϵ˜ =ϵU+ϵI +ϵC, (1) topreserveimageconsistencyandtosimplifyparsing. Due
t t t t
tolimitedspace,wereportthefullpromptintheappendix.
where
Notably,obtainedspecificinstructionsSareavailabletothe
ϵU
t
=f θ(z t,∅,∅), userduringmodelinference,providinginsightsonhowthe
ϵI =wI·(f (z ,E(x),∅)−f (z ,∅,∅)), (2) input instruction c is respected. This also allows SANE to
t θ t θ t
explicitlyshowhowcimpactstheinputimagex,whichin-
ϵC =wC·(ϵ −f (z ,E(x),∅)).
t t θ t creasesinterpretabilityoftheimageeditingprocess.
In Equation (2), ∅ indicates that the conditioning element
3.3.InstructionsCombination
isreplacedwithzeros,andwIandwCareguidancestrength
parameters, controlling the conditioning strength on x and AfterextractingdecompositionS ={s }N ,wecanuse
i i=1
c,respectively. ittoguidetheimageeditingprocess. Ourideaistoinclude
3
sharedS in the denoising procedure of f along the original am- 4.Experiments
θ
biguous instruction c. Intuitively, this constrains the state
4.1.ExperimentalSetup
of solutions satisfying the required editing c, allowing the
modeltofocusontheselectedspecificinterventionsS. On Baselines WetestSANEbyadaptingtheeditingdiffusion
theotherhand,includingcinthedenoisingprocessallows models trained in InstructPix2Pix [6] (IP2P), MB [56]
thediffusionmodeltosynthesizecomplementaryelements (MB)andHQEdit[27].Weusethedefaultinferencehyper-
necessary for satisfying the user request, but not included parameters for all methods: wC = 7,wI = 1.5. We select
in S. Hence, we propose a combination strategy that ag- with a grid search wS = 7 for IP2P, wS = 5 for MB, and
gregate c and each s i ∈ S, balancing the influence of the wS = 9forHQEdit. WetestwithN = {1,2,3}, limiting
specificinstructionswithoutlosingconsistencywithc. the number of instructions due to increased computational
We start by conditioning the denoising process on each time associated with higher N values. We incrementally
specificinstruction,toisolatetheireffects. Hence,foreach buildS forincreasingN values,insuchawaythatS with
s i ∈S weextracttheestimatednoiseϵs i,tattimestept: lower N are subsets of those with higher N. We use 30
ϵs =f (z ,E(x),s ), ∀i∈[1,N]. (3) diffusionstepsforimagegeneration.Imagesare512×512.
i,t θ t i
Datasets WetestSANEontwodatasets. Wefirstconsider
Thisresultsinthesetofestimatednoises{ϵs }N ,onefor theglobalsplitoftheEMU-Editdataset,including220real
i,t i=1
each s i ∈ S. Next, we aim to combine this set of noise images with ambiguous instructions satisfying our defini-
estimations into a single noise estimate that aggregates all tion [49]. We also introduce a subset of 370 images and
specificinstructions. Simpleaveragingwoulddiminishthe editing instructions from the IP2P dataset [6], following
impactofspecificinstructionsthataffectparticularregions related works [19]. We call this set IP2P data. To focus
byblendingthemwithothernoiseestimates. Therefore,we our evaluation on ambiguous instructions, we manually
propose an alternative aggregation scheme, assuming that classify the 370 samples. We process instructions in both
each image region is predominantly affected by a single datasetswithGPT-4otoextractS.
specificinstruction. Toidentifythespatiallocationswhere Metrics We evaluate the quality of edited images with
the instruction s i most significantly impacts the diffusion three CLIP-based [44] metrics: input image preservation,
process, we calculate the absolute difference between the editing strength and adherence to the edit. First, we use
estimatednoisesinthesetandthenoiseobtainedwithcon- CLIP to measure input image preservation as in [19],
i
ditioningonlyontheinputimage. Formally,itcanbewrit- which is the CLIP space cosine similarity between x
tenas: and x˜. This captures how similar is x˜ to the original x.
∆ϵs
i,t
=|ϵs i,t−f θ(z t,E(x),∅)|. (4) Then,wemeasuretheeditingstrehgthCLIP
d
asthecosine
WethenaggregatetheseinamaskM ,capturingtheindex distance between the CLIP image embedding of x˜ and
t
ofthemostsignificantelementacrossi: the text embedding of the final caption, following [44].
Thisassessesthefidelityoftheeditedimage x˜ tothefinal
M =argmax∆ϵs (5)
t i,t caption. Finally, we use the directional similarity of [17],
i
referredtoasCLIP ,tomeasureadherencetotheedit. For
Finally,toaggregatethenoisesϵs basedonthemaskM , ∆
i,t t that,wefirstprocesseachpair(x,c)withGPT-4otoobtain
we use M to select the most significant estimated noise
t
a short initial caption, describing the input scene, and a
for each spatial location. The aggregated noise ϵ¯s can be
t finalcaption,describingthedesiredsceneaftertheediting.
computedasfollows:
For instance, if the initial caption is “a woman by the
ϵ¯s =(cid:88) I(M =i)·ϵs , (6) pool” and c is “make her a robot”, a final caption
t t i,t maybe“a robot by the pool”.CLIP isevaluated
∆
i
asthecosinesimilaritybetweenthedifferenceoftheCLIP
where I(M = i) is an indicator function that is 1 if M
t t imageembeddingsofxandx˜,andthedifferenceofthetext
equals i, and 0 otherwise. Similarly to existing litera-
embeddingsoftheinitialandfinalcaptions. Thiscompares
ture[34],weuseclassifier-freeguidance[23]forinstruction
the change in the image to the change in the caption. In
combination,redefiningEquation(1)as:
addition to these three metrics, we also use TIFA [24] to
ϵ˜ =ϵU+ϵI +ϵC+ϵS, (7) evaluatetheeffectivenessofs ∈ S,aswellasLPIPS[57]
t t t t t i
andDreamSim[15]toevaluateimagediversity. Finally,we
where the classifier-free guidance term for the specific in-
evaluatepairwiseimagepreferenceswithGPT-4o.
structionsisgivenby:
ϵS =ws(ϵ¯s−f (z ,E(x),∅)). (8) 4.2.EditingPerformance
t t θ t
TheprocessisshowninFigure2,center. Weapplythisfor Here, we compare against the state of the art. For each
everyiterationt∈[1,T]. of the three editing diffusion models, we evaluate their
4EMU-Edit IP2Pdata
Emu-edit IP2Pdata
Method N CLIP ↑ CLIP ↑ CLIP ↑ CLIP ↑ CLIP ↑ CLIP ↑
d i ∆ d i ∆
24.1 24.6
Baseline - 0.2923 0.8810 0.1203 0.2903 0.9027 0.1724
1 0.2961 0.7779 0.1705 0.2886 0.8196 0.1999 75.9 75.4
SANE 2 0.2962 0.7654 0.1785 0.2910 0.8107 0.2063
3 0.2968 0.7531 0.1858 0.2935 0.8101 0.2057
39.1 34.0
Baseline - 0.2888 0.7858 0.1618 0.2855 0.7934 0.2028
1 0.2948 0.7994 0.1661 0.2848 0.7970 0.1992 60.9 66.0
SANE 2 0.2952 0.8134 0.1669 0.2870 0.8068 0.1996
3 0.3006 0.8209 0.1655 0.2878 0.8056 0.1998
32.7 27.3
Baseline - 0.2675 0.6501 0.1417 0.2785 0.7035 0.1848
67.3 72.7
1 0.2725 0.6546 0.1458 0.2734 0.6997 0.1759
SANE 2 0.2789 0.6802 0.1433 0.2782 0.7057 0.1842
3 0.2823 0.6870 0.1474 0.2818 0.7136 0.1855 Baseline Ours
(a)Editingqualityresults (b)GPTevaluation
Table1. Quantitativecomparison. In(a), wecompareagainstbaselinesbyselectingdifferenteditingmodelsandapplyingSANEon
topofthem. WeshowresultsforN = {1,2,3},evaluatingCLIP ,CLIP andCLIP onall. SANEconsistentlyimproveimagequality
d i ∆
performanceacrossdatasetsandmodels.In(b),weuseGPT-4otoevaluatethequalityofeditedimages,alwaysoutperformingbaselines.
performance with and without SANE applied on top of performance in Table 1a. For each image-instruction pair
them. We evaluate models on EMU-Edit and IP2P data (x,c) on each dataset, we prompt GPT to choose the best
bothquantitatively(Table1)andqualitatively(Figure3). editedimagex˜betweenoneofthebaselinesandSANE.We
asktotakeintoaccountthefidelitytotheeditinginstruction,
CLIPMetrics WepresentresultsofCLIP-basedmetrics
thequalityandrealismofthegeneratedimage,andthecon-
inTable1a. Overall,weobserveperformanceimprovement
tentpreservationfromtheoriginalpicturewhilemakingthe
acrossallmetrics, modelsanddatasets, advocatingthead-
decision. The original image x is also provided for refer-
vantagesofSANE.Notably,performanceincreaseswithN,
ence. Thepromptisshownintheappendix. Wereportthe
withmodelsusingN = 3specificinstructionsperforming
averageofGPTchoicesforallmodelpairsinTable1b. As
best. In particular, we report the biggest improvements in
before,wesignificantlyoutperformallbaselines. Inpartic-
IP2P, where we report for SANE/Baseline 0.1858/0.1203
ular,webeattheMBbaselineonIP2Pdata(SANE/Baseline
on EMU-Edit for N = 3. This suggests that our instruc-
is 66.0%/34.0%). This is especially interesting, consid-
tion decomposition helps to follow the ambiguous instruc-
ering that it was the only setup where SANE was not
tionc. Moreover, weobserveanincreaseinimageconsis-
outperforming the baseline CLIP in Table 1a, reporting
∆
tency in MB and HQEdit, where we report improvements
0.1998/0.2028. Thisresultprovesthatwecanbenefitfrom
in CLIP metric. We attribute this behavior to our decom-
i the improved quality of the generated images, even when
positionstrategy. Indeed,whilegeneralinstructionscalone
the fidelity to c is slightly penalized. We attribute this ob-
mayleadtoambiguouseditsimpactingtheentirescene,in-
servationtotheimprovedprecisionofoureditingprocess.
jecting s ∈ S for inference guides the editing process on
i
spatially-constrained edits. These still convey the desired
Qualitative Results We show qualitative results in
modifications, as proved by the improvements in CLIP
∆
Figure 3. We use IP2P as a baseline model, and we add
andCLIP . InIP2P,theBaselinereportsthehighestCLIP.
d i
SANE on top of it with N = {1,2,3} instructions. We
While it might seem as if Baseline outperforms all other
use the same hyperparameters and random seed for the
methods,inrealityweobservethatIP2Pmayfailtoperform
baselineandSANE.SANEgraduallyaddsdetailswiththe
any change to x when the input instruction is too ambigu-
specific instructions to respect the ambiguous instruction.
ous,thusartificiallyinflatingtheCLIP metric. Thisisalso
i
Interestingly, adding more specific instructions (higher N)
confirmedbythelowerperformanceinCLIP andCLIP .
d ∆
bringsadvantagesofremovingeditingartifacts. Thisises-
GPT Evaluation We use GPT-4o to measure pairwise peciallyevidentinthethirdrow,wherethelasttwospecific
preferenceforourSANEagainstthechosenbaselines. We instructions help to remove the background artifacts gen-
choosetheconfigurationwithN =3,sinceityieldsthebest erated by IP2P. We believe such artifacts are generated by
5
P2PI
BM
tidEQH
P2PI
hsurBcigaM
tidEQHSANE
Baseline N =1 N =2 N =3
Originalimage
Change the scene into Add snowy mountain
Instructions→ the alps in winter Cover the ground with peaks in the Add snow-covered pine
snow trees along the track
time background
Make the photo seem Replace the plate with Add a garnish of fresh
Instructions→ like it was taken in a a gourmet presentation herbs to the bacon and Add elegant silverware
next to the plate
fancy restaurant plate eggs
Replace the road with Add yellow taxis
Instructions→ Change this to a New a busy New York city Add tall buildings in surrounding the green
York city street the background
street car
Figure 3. Qualitative results. Using SANEon top ofIP2P helpsto respect theambiguous instruction(underlined in grey)by adding
specificelementsintotheinputscene. Weshowhowincreasingthenumberofspecificinstructions(orange,cyan,purple)addsimportant
detailstothescene,ignoredbythebaseline. Examplesofsuchdetailsarethesnowontheground(firstrow),theherbsgarnish(second
row),andthetaxis(thirdrow).Coloredboxedintheheaderindicatetheinstructionusedforeachcolumn.
theambiguityoftheinputinstruction. Specificinstructions Effects of Specific Instructions To prove that SANE is
help to constrain the solution space for the editing task, workingcorrectly,weneedtoevaluatewhetherspecificin-
improvingtherobustnesstosuchundesiredvisualeffects. structions are applied. As mentioned in Section 3.2, this
wouldalsoenableinterpretabilityoftheeditingprocess: at
4.3.SANEProperties
inference time, we can provide specific instructions S =
In this section, we discuss additional properties of SANE, {s i}N
i=1
to the user, to explain how the editing instruction
namelytheimpactofinstructionsinS (Table2)anditsef- chasbeenapplied. Forthisreasonwequantifyhowmuch
fects on the variability of outputs (Figure 4). We use real the edited image x˜ respects each instruction in a reference
imagesfromEMU-Editforallevaluations. set. Weaverage, foreach x˜ andfor N = {1,2,3}, CLIP d
6EMU-Edit
IP2P MagicBrush HQEdit
Method N CLIP ↑CLIP ↑ IP2P 1 2 3
d ∆ 1.0 0.5
Baseline - 0.2860 0.0861 1 2 3 1 2 3
0.5
1 0.2854 0.1304
0.0
SANE 2 0.2851 0.1436 Base Ours 0.0
Base Ours Base Ours Base Ours
3 0.2878 0.1518
MagicBrush
1.0
Baseline - 0.2725 0.1108 0.5 1 2 3
1 0.2831 0.1240 0.5 1 2 3 1 2 3
SANE 2 0.2876 0.1282 0.0
3 0.2926 0.1284 Base Ours
0.0
HQEdit
Baseline - 0.2522 0.1090 1.0 Base Ours Base Ours Base Ours
(a)Quantitativeevaluation
1 0.2854 0.1238 0.5
Image Baseline SANE
SANE 2 0.2889 0.1306
3 0.2907 0.1326 0.0
Base Ours
(a)CLIPmetrics (b)TIFAscore
Table 2. Specific instructions effect. We evaluate how much
SANE and baselines respect the instructions in S. With CLIP ‘‘Make the photo seem like it was
taken during a snow storm’’
metrics(a)andTIFA(b),weverifythatweimprovefidelitytoS. (b)Pixeldifferencevisualization
Figure4. Variability. Wequantifyvariabilityofgeneratedsam-
andCLIP foreachs inthereferenceset. Wetakeasref- pleswithLPIPSandDreamSim,evaluatingtheaveragedistanceto
∆ i
erence set the S obtained with N = 3, to fairly compare theinputimage(a).Forboth,highervaluesimplyhighervariabil-
all method and baselines. Results in Table 2a show that ity. AboveOursbars,wereportN. In(b),weshowtheaverage
pixeldifferenceoforiginalandsynthesizedimagesforthereported
weconsistentlyandconsiderablyoutperformallbaselines,
instruction.UsingSANEallowstomodifymorepixels.
demonstratingabilityofSANEtoeffectivelyapplyspecific
instructions. Asexpected,performanceincreasesforhigher
N,withN =3setupsconsistentlyoutperforminginallsce- set[56]. WedisplayinFigure4btheaveragepixelwisedif-
narios.ThismeansthatSANEisexploitingeachinstruction ferencebetweenxandx˜forIP2PandIP2P+SANE.SANE
inSforediting. Additionally,weuseTIFA[24]togenerate produceseditsthatarebetterdistributedspatially,including
questionsrelatedtoeachs iwithanLLM.Wethenevaluate regionsignoredbyothermodels(e.g.,thedogfur).
ifs areappliedansweringthesetofquestionwithvisual-
i
4.4.AblationStudies
questionanswering. Formoredetails,wereferto[24]. Due
tohighevaluationcosts,wereportresultsonlyforthesetup
We focus our ablations on instruction combination strate-
with N = 3. Our evaluations in 2b confirm that SANE
gies(Table3),alsoprovidinginsightsonourdesignchoices
correctlyexploitstheS instructions.
(Table 4) and on the effectiveness of SANE on different
typesofinputinstructions(Table5).
Image Variability To evaluate variability of edited im-
ages,werandomlyselect20inputimagesfromEMU-Edit, Instruction Combination We combine specific instruc-
and produce 10 edited samples for each of them using tions as presented in Section 3.3. Here, we study the ef-
SANEandbaselines.WeevaluatethemeanLPIPS[57]and fectivenessofalternativesolutionsforinstructioncombina-
DreamSim[15]betweeninputimagesxandeditedimages tion. WefirstproposeanaivePromptConcatenationbase-
x˜. Higher metrics imply higher variability of the output. line,whereweconcatenatethetextoftheinstructioncwith
LPIPSisparticularlysensitivetolow-leveldifferences[57], all s ∈ S, using commas as separator. We then perform
i
whileDreamSimcapturessemanticvariability[15].Results theeditingusingtheobtainedconcatenatedinstruction. We
in Figure 4a show that we outperform the baseline (Base) alsocombinetheeffectsofcandalls ∈ S withCompos-
i
for IP2P and HQEdit. The best performing setups are ableDiffusion[34]. Werefertotheoriginalpaper[34]for
N = 1 with HQedit and N = 2 with IP2P. We speculate details. Forafaircomparison,wesetwCandwSasweights
thathigherN increasestheprobabilityoftwosampleshav- forcandeachs , respectively. WetestwithN =3. From
i
ing at least one overlap in generated specific instructions. resultsinTable3,weinferthatourstrategyforinstruction
We observe that MB [56] baseline outputs highly variable combination outperforms other strategies. We explain this
outputs, which we attribute to the quality of its training withthehierarchicalnatureofinstructions: wepreservethe
7
P2PI
BM
tidEQH
AFIT
AFIT
AFIT
SPIPL
miSmaerD
P2PIEMU-Edit EMU-Edit
Method CLIPd↑ CLIPi↑ CLIP∆↑ Method CLIP d↑ CLIP i↑ CLIP∆↑
Promptconcat 0.2933 0.8188 0.1439 Baseline 0.2923 0.8810 0.1203
Comp.Diffusion 0.2944 0.7299 0.1847 SANEw/oc 0.2883 0.7796 0.1552
Ours 0.2968 0.7531 0.1858 SANEw/avg 0.2974 0.8190 0.1587
Promptconcat 0.2890 0.7802 0.1639 SANE 0.2968 0.7531 0.1858
Comp.Diffusion 0.2990 0.8210 0.1649 Baseline 0.2888 0.7858 0.1618
Ours 0.3006 0.8209 0.1655 SANEw/oc 0.2954 0.8795 0.1375
Promptconcat 0.2720 0.6086 0.1512 SANEw/avg 0.2978 0.8087 0.1681
Comp.Diffusion 0.2753 0.6547 0.1398 SANE 0.3006 0.8209 0.1655
Ours 0.2823 0.6870 0.1474
Baseline 0.2675 0.6501 0.1417
SANEw/oc 0.2795 0.6983 0.1321
Table3.Instructioncombinationbaselines.Weoutperformtwo SANEw/avg 0.2807 0.6852 0.1442
baselinesforinstructioncombination,asshownbyCLIPmetrics. SANE 0.2823 0.6870 0.1474
Onlyourapproachallowstobenefitfromthedistinctionsbetween
ambiguousandspecificinstructions. Table4. Designchoices. Removingtheambiguousinstructionc
(SAGEw/oc)resultsindegradedperformance,provingthatedit-
ingmodelsbenefitfrombothspecificandambiguousinstructions.
fidelitytotheoriginalcbydesign,aggregatingtheeffectsof Also,replacingthemaskingoperationwithaveraging(SAGEw/
differentspecificinstructionsinasinglenoiseestimation. avg)resultsinworseediting.Wehighlightfirstandsecondbest.
ImpactofDesignChoices Here,weanalyzetheeffectof Prompts Method CLIPd↑ CLIPi↑ CLIP∆↑
severaldesignchoices.First,westudytheimpactoftheini- Baseline 0.2946 0.8992 0.1504
Amb.
tialinstructioncontheperformance. Onemayarguethat, SANE 0.2998(+1.76%) 0.8225(-8.53%) 0.1767(+17.5%)
sinceeachs isrelatedtoc,itmaybesufficienttoapplyS = Baseline 0.3044 0.9000 0.2012
i Spec.
{s }N without c to achieve good editing. We test SANE SANE 0.3044(+0.00%) 0.8302(-7.75%) 0.2228(+10.8%)
i i=1
onEMU-EditwithN = 3andsettingwT = 0,i.e.remov-
Table 5. Evaluation on non-ambiguous prompts. We report
ing c guidance. Results in Table 4 (“SANE w/o c”) prove
beneficial effects of SANE on both ambiguous (Amb.) and
thatalthoughSANEw/occanoutperformcertainbaselines specificinstructions(Spec.)
(e.g. CLIP = 0.1552 on IP2P), using c for denoising is
∆
important to achieve the best performance. Then, we re-
placethemaskingoperationdescribedinEquation(6)with effective as a general-purpose editing method, i.e. applied
a naive averaging of the estimated noises ϵs . In Table 4, to an arbitrary input instruction, and that using LLM can
i,t
we show that this alternative strategy, reported as “SANE boosttheperformance.
w/avg”,yieldslowerperformanceonCLIPmetrics. Inaddition,theimprovementforambiguousinstructions
is higher (e.g. +17.5% in CLIP ) compared to improve-
∆
SANE with Non-ambiguous Instructions SANE is mentforspecificones(+10.8%),supportingthemotivation
based on the assumption that the input prompt is ambigu- forourwork.Notably,theabsoluteperformanceonspecific
ous. We now propose a preliminary evaluation on non- instructions is higher, confirming our initial observation
ambiguousinstructionstoshowthat:1)SANEcanalsoper- thateditingmodelshavedifficultieswithambiguousinputs.
formwellonnon-ambiguousprompts;2)Thegainbrought
bySANEishigheronambiguousinstructions,thusvalidat- 5.Conclusions
ingthemotivationbehindthedesignofSANE.
To this aim, we propose the following experiment. We We have introduced Specify ANd Edit (SANE), a
considerthefirstsetof1199imagesfromtheIP2Pdataset zero-shotinferencepipelinethatimprovestheperformance
whichcanbeeitherambiguousornot. WethenuseGPT-4o of diffusion-based text-to-image editing methods with
to identify ambiguous instructions leading to 696 ambigu- ambiguous instructions. By using an LLM to decompose
ous and 503 specific (x,c) pairs. The prompt is reported instructions into specific interventions, SANE enhances
intheappendix. Forbothtypesofpairs(subsetsAmb. and both interpretability and editing quality. Our experiments
Spec.),weprocessthesamplesusingIP2Pwithandwithout show consistent performance improvements and increased
SANEontopofit,leadingtotheresultsinTable5.Wealso output diversity. Moreover, SANE is versatile, and it can
reporttherelativechangewithrespecttothebaseline. benefit both ambiguous and clear editing tasks. In the
Experiment on non-ambiguous instructions show that future, we plan to address the limitations of SANE, such
SANE achieves higher adherence to the input edit (higher as the difficulty in handling a high number of specific
CLIP ) while achieving similar preservation of the input instructions and the lack of guarantee that each specific
∆
image (similar CLIP ). It demonstrates that SANE can be instructionisactuallyapplied(seeappendix).
d
8
P2PI
BM
tidEQH
P2PI
P2PI
BM
tidEQHAcknowledgements [13] QingxiuDong,LeiLi,DamaiDai,CeZheng,ZhiyongWu,
Baobao Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A
EI and SL are supported by the French National
surveyonin-contextlearning. arXiv,2022. 2
Research Agency (ANR) with the ANR-20-CE23-0027
[14] YutongFeng,BiaoGong,DiChen,YujunShen,YuLiu,and
project. FP is funded by KAUST (Grant DFR07910).
Jingren Zhou. Ranni: Taming text-to-image diffusion for
PT is supported by UKRI grant: Turing AI Fellowship accurateinstructionfollowing. InCVPR,2024. 2
EP/W002981/1,andbytheRoyalAcademyofEngineering
[15] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy
undertheResearchChairandSeniorResearchFellowships Chai,RichardZhang,TaliDekel,andPhillipIsola. Dream-
scheme. sim: Learning new dimensions of human visual similarity
usingsyntheticdata. InNeurIPS,2023. 4,7
References [16] Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang,
Yinfei Yang, and Zhe Gan. Guiding instruction-based im-
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah- ageeditingviamultimodallargelanguagemodels. InICLR,
mad,IlgeAkkaya,FlorenciaLeoniAleman,DiogoAlmeida,
2024. 1,2
JankoAltenschmidt, SamAltman, ShyamalAnadkat, etal.
[17] RinonGal,OrPatashnik,HaggaiMaron,AmitHBermano,
Gpt-4technicalreport. arXiv,2023. 2
Gal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-
[2] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, Antoine
guideddomainadaptationofimagegenerators. ACMTOG,
Miech,IainBarr,YanaHasson,KarelLenc,ArthurMensch,
2022. 4
KatherineMillican, MalcolmReynolds, etal. Flamingo: a
[18] ZigangGeng,BinxinYang,TiankaiHang,ChenLi,Shuyang
visuallanguagemodelforfew-shotlearning.NeurIPS,2022.
Gu,TingZhang,JianminBao,ZhengZhang,HouqiangLi,
2
HanHu,etal. Instructdiffusion: Ageneralistmodelingin-
[3] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-
terfaceforvisiontasks. InCVPR,2024. 2
BaptisteAlayrac,JiahuiYu,RaduSoricut,JohanSchalkwyk,
[19] Qin Guo and Tianwei Lin. Focus on your instruction:
AndrewMDai,AnjaHauth,KatieMillican,etal. Gemini:
Fine-grained and multi-instruction image editing by atten-
Afamilyofhighlycapablemultimodalmodels.arXiv,2023.
tionmodulation. InCVPR,2024. 2,4
1,2
[20] Tiankai Hang, Shuyang Gu, Dong Chen, Xin Geng, and
[4] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-
BainingGuo. Cca:Collaborativecompetitiveagentsforim-
son, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,
ageediting. arXiv,2024. 2
EmanuelTaropa,PaigeBailey,ZhifengChen,etal. Palm2
technicalreport. arXiv,2023. 2 [21] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,
YaelPritch,andDanielCohen-Or. Prompt-to-promptimage
[5] Anthropic. Introducingthenextgenerationofclaude,2024.
editingwithcrossattentioncontrol. arXiv,2022. 1
Accessed:2024-07-13. 1,2
[22] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu-
[6] TimBrooks,AleksanderHolynski,andAlexeiAEfros. In-
sionprobabilisticmodels. InNeurIPS,2020. 2,3
structpix2pix:Learningtofollowimageeditinginstructions.
InCVPR,2023. 1,2,3,4,13 [23] Jonathan Ho and Tim Salimans. Classifier-free diffusion
[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- guidance. InNeurIPSWorkshops,2021. 2,3,4
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan- [24] YushiHu,BenlinLiu,JungoKasai,YizhongWang,MariOs-
tan,PranavShyam,GirishSastry,AmandaAskell,etal.Lan- tendorf,RanjayKrishna,andNoahASmith. Tifa:Accurate
guagemodelsarefew-shotlearners. NeurIPS,2020. 1,2,3 and interpretable text-to-image faithfulness evaluation with
[8] MingdengCao,XintaoWang,ZhongangQi,YingShan,Xi- questionanswering. InICCV,2023. 4,7
aohuQie,andYinqiangZheng. Masactrl: Tuning-freemu- [25] Xinyu Huang, Youcai Zhang, Jinyu Ma, Weiwei Tian, Rui
tualself-attentioncontrolforconsistentimagesynthesisand Feng, Yuejie Zhang, Yaqian Li, Yandong Guo, and Lei
editing. InICCV,2023. 2 Zhang. Tag2text:Guidingvision-languagemodelviaimage
[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, tagging. ICLR,2024. 2
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul [26] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan,
Barham, Hyung Won Chung, Charles Sutton, Sebastian Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui
Gehrmann, et al. Palm: Scaling language modeling with Huang, Ruimao Zhang, et al. Smartedit: Exploring com-
pathways. JMLR,2023. 2 plexinstruction-basedimageeditingwithmultimodallarge
[10] Guillaume Couairon, Marlene Careil, Matthieu Cord, languagemodels. InCVPR,2024. 2
Ste´phaneLathuiliere,andJakobVerbeek. Zero-shotspatial [27] MudeHui, SiweiYang, BingchenZhao, YichunShi, Heng
layout conditioning for text-to-image diffusion models. In Wang, PengWang, YuyinZhou, andCihangXie. Hq-edit:
ICCV,2023. 1 A high-quality dataset for instruction-based image editing.
[11] GuillaumeCouairon,JakobVerbeek,HolgerSchwenk,and arXiv,2024. 1,2,4
Matthieu Cord. Diffedit: Diffusion-based semantic image [28] KJJoseph,PratekshaUdhayanan,TriptiShukla,Aishwarya
editingwithmaskguidance. arXiv,2022. 2 Agarwal,SrikrishnaKaranam,KoustavaGoswami,andBal-
[12] PrafullaDhariwalandAlexanderNichol. Diffusionmodels ajiVasanSrinivasan. Iterativemulti-granularimageediting
beatgansonimagesynthesis. InNeurIPS,2021. 2 usingdiffusionmodels. InWACV,2024. 2
9[29] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and [47] Bernardino Romera-Paredes, Mohammadamin Barekatain,
QiangXu. Directinversion: Boostingdiffusion-basededit- AlexanderNovikov,MatejBalog,MPawanKumar,Emilien
ingwith3linesofcode. arXiv,2023. 2 Dupont, FranciscoJRRuiz, JordanSEllenberg, Pengming
[30] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Wang, Omar Fawzi, et al. Mathematical discoveries from
Elucidating the design space of diffusion-based generative programsearchwithlargelanguagemodels. Nature,2024.
models. NeurIPS,2022. 3 2
[31] BahjatKawar,ShiranZada,OranLang,OmerTov,Huiwen [48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Chang,TaliDekel,InbarMosseri,andMichalIrani. Imagic: Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Text-based real image editing with diffusion models. In RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,
CVPR,2023. 1,2 etal.Photorealistictext-to-imagediffusionmodelswithdeep
[32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. languageunderstanding. NeurIPS,2022. 1,2
Blip-2: Bootstrapping language-image pre-training with [49] ShellySheynin,AdamPolyak,UrielSinger,YuvalKirstain,
frozenimageencodersandlargelanguagemodels.InICML, AmitZohar,OronAshual,DeviParikh,andYanivTaigman.
2023. 2 Emuedit:Preciseimageeditingviarecognitionandgenera-
[33] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. tiontasks. InCVPR,2024. 1,2,4,13
Visualinstructiontuning. NeurIPS,2024. 2 [50] JaschaSohl-Dickstein, EricWeiss, NiruMaheswaranathan,
[34] Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and and Surya Ganguli. Deep unsupervised learning using
JoshuaBTenenbaum. Compositionalvisualgenerationwith nonequilibriumthermodynamics. InICML,2015. 2
composablediffusionmodels. InECCV,2022. 2,4,7 [51] JiamingSong,ChenlinMeng,andStefanoErmon. Denois-
[35] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jia- ingdiffusionimplicitmodels. InICLR,2021. 2,3
junWu,Jun-YanZhu,andStefanoErmon. Sdedit: Guided
[52] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali
imagesynthesisandeditingwithstochasticdifferentialequa-
Dekel. Plug-and-play diffusion features for text-driven
tions. arXiv,2021. 2
image-to-imagetranslation. InCVPR,2023. 2
[36] MetaAI.Introducingmetallama3:Themostcapableopenly
[53] LaurensVanderMaatenandGeoffreyHinton. Visualizing
availablellmtodate,2024. Accessed:2024-07-06. 1,2,12
datausingt-sne. JMLR,2008. 14
[37] MistralAITeam. Mistral-7b-instruct-v0.3,2024. Accessed:
[54] BramWallace,AkashGokul,andNikhilNaik. Edict:Exact
2024-07-06. 1,2,12
diffusion inversion via coupled transformations. In CVPR,
[38] Daiki Miyake, Akihiro Iohara, Yu Saito, and Toshiyuki
2023. 2
Tanaka. Negative-prompt inversion: Fast image inversion
[55] QianWang,BiaoZhang,MichaelBirsak,andPeterWonka.
foreditingwithtext-guideddiffusionmodels. arXiv,2023.
Instructedit:Improvingautomaticmasksfordiffusion-based
2
imageeditingwithuserinstructions. arXiv,2023. 2
[39] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and
[56] KaiZhang,LingboMo,WenhuChen,HuanSun,andYuSu.
DanielCohen-Or.Null-textinversionforeditingrealimages
Magicbrush: A manually annotated dataset for instruction-
usingguideddiffusionmodels. InCVPR,2023. 2
guidedimageediting. NeurIPS,2024. 1,2,4,7
[40] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
[57] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,
Shyam,PamelaMishkin,BobMcGrew,IlyaSutskever,and
and Oliver Wang. The unreasonable effectiveness of deep
MarkChen. Glide:Towardsphotorealisticimagegeneration
featuresasaperceptualmetric. InCVPR,2018. 4,7
andeditingwithtext-guideddiffusionmodels. arXiv,2021.
1,2
[41] OpenAI. Hellogpt-4o,2024. 2
[42] GauravParmar,KrishnaKumarSingh,RichardZhang,Yijun
Li,JingwanLu,andJun-YanZhu.Zero-shotimage-to-image
translation. InSIGGRAPH,2023. 2
[43] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,Shaohan
Huang,ShumingMa,andFuruWei. Kosmos-2: Grounding
multimodallargelanguagemodelstotheworld.arXiv,2023.
2
[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
AmandaAskell,PamelaMishkin,JackClark,etal. Learn-
ingtransferablevisualmodelsfromnaturallanguagesuper-
vision. InICML,2021. 2,4
[45] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,
andMarkChen. Hierarchicaltext-conditionalimagegener-
ationwithcliplatents. arXiv,2022. 1,2
[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
PatrickEsser,andBjo¨rnOmmer.High-resolutionimagesyn-
thesiswithlatentdiffusionmodels. InCVPR,2022. 1,2
10Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing
Appendix
In the main paper, we present SANE, a novel zero- Decompositionprompt
shot pipeline for instruction-based diffusion models, used
toboostperformanceonambiguousinstructions.Inthisap- You are a helpful assistant for image editing. I
pendix, we propose additional information to complement will provide you with a caption that describes an
our paper. We discuss limitations in Section A. Then, in image, and an editing instruction that represents
Section B we report the LLM prompts used for each task. an ambiguous modification of the scene. Your
Finally,inSectionCweprovideadditionalresults, includ- task is to propose specific modifications. You
ingnewexperimentsandqualitativeevaluations. can ask to add or replace elements in the scene,
proposing consistent modification that agree with
theambiguousinstruction.
A.Limitations
In this section, we discuss the limitations of SANE. Be concise and output your instructions without
First,despitetheobservationthatspecificinstructionsS = further considerations or reasoning, one local
{s }N improvemodelperformanceontheinitialinstruc- modificationperline. Donotoutputanyothertext
i i=1
tionc,thereisnoguaranteethatallofthesespecificinstruc- thanthesuggestedoutputs,donotwrite“suggested
tions will always be respected. In our experience, the hire output:”.
is N, the more the editing process tends to avoid follow-
ing one or more specific instructions s . This is expected, Iamgoingtoprovidesomeexamplesnow.
i
sinceincreasingthenumberofinstructionsmakestheedit- Caption: aphotoofaurbanscenario,withcars.
ingtaskmorechallengingforthediffusionmodel. Topro- ambiguousinstruction: makethescenevintage.
videatransparent evaluation, weincludeanadditional ex- Suggestedoutput: replacethecarswitholdcars
perimentquantifyinghowasingleeditinginstructionisre- Caption: aphotoofadogrunningonthegrass.
spected in Section C. Moreover, SANE brings additional ambiguousinstruction: makeitlookfunny.
computational costs due to multiple denoising operations. Suggestedoutput: addahattothedog.
WequantifytheseeffectsaswellinSectionC.
The main subject of the scene must stay the same.
For instance, if the photo is describing a cat as
B.Prompts
the main subject, you cannot replace the cat with
Here, we report all the prompts used in the paper. another animal. You should NEVER remove
To make it easier to understand our prompt engineering elements. Only propose instructions targeting ele-
choices, we color differently the parts of the prompts de- mentsthatappearinthecaption,withoutimagining
signed for encouraging specific behaviour in the LLM. In anythingelse.
particular, we use blue for indicating the task-related in-
struction to the model, orange for parsing-related instruc- Now, provide <N> outputs for the following
tions, green for in-context learning, and red for additional caption and subjective instruction. Caption:
task-specificinstructionstomitigatethepresenceoferrors, <caption>ambiguousinstruction<c>
establishedviatrial-and-error.
Captioningprompt ToevaluateCLIP (Section4.1),we
∆
Instruction Decomposition Our instruction decomposi- generatesimultaneouslythecaptionsoftheinitialandthefi-
tion prompt used in Section 3.2 is reported below. The nal scenes with GPT-4o. Please note that we use only the
<caption> is obtained by using the captioning prompt, captionoftheoriginalimageinourinferencepipeline(Sec-
showninthenextparagraph. tion3). However,weobservethatgeneratingbothcaptions
simultaneously improves their consistency, allowing us to
11better evaluate CLIP in Section 4. Here, we report the Ambiguous instruction selection Here, we present the
∆
promptusedforthecaptioningtask. Weinputtheambigu- prompt used to automatically select samples with abstract
ousinstructionc,alongwiththeinputimagex. Theprompt instructionsforTable5.
is:
Instructionselectionprompt
Captioningprompt
Youareahelpfulassistantforimageediting. Iwill
Iamgoingtoprovideaninputimageandanediting provideyouwithaneditinginstructionthatrequires
instruction. You should propose 1) a caption that certainmodificationofthesceneintheimage. Your
describes accurately the input image, max 10 task is to decide whether this instruction repre-
words,focusingonlyonvisualcontent2)acaption sentsanabstractinstructionoraspecificinstruction.
that encompasses how the image should look like
after applying the instruction. The instruction is: Herearesomeexamples:
<c>. Theimageis<x>. ambiguous instruction: ‘Change the image so it
apearsoldandmusty.’
Try to keep these captions as compact as possible. ambiguousinstruction: ‘Makeitasnowyday.’
The captions should be as similar as possible to ambiguous instruction: ‘Change the image so the
eachother. playerslooklikezombies.’
Specific instruction: ’Add the word ‘tray’ in white
Youshouldreplyfollowingtheformat: tothebottomoftheimage.’
1. “caption1” Specificinstruction: ‘Changethesheepintoacalf.’
2. “caption2” Specific instruction: ‘Draw this in an oil painting
Just reply with the captions without reasoning or style.’
considerations.
Now,tellmeifthefollowinginstructionisambigu-
ousorspecific. Theinstructionis: <c>.
GPT evaluation Here, we report the prompt used for
Before answering, motivate your decision by
evaluatingimagequalitywithGPT-4o(Section4.2). Wein-
reasoningaboutthepropertiesofthisinstruction.
dicatewithx˜ andx˜ theeditingresultsofbaselines
baseline ours
andSANE,respectively.
Your response should start with either ‘Response:
ambiguous.’ or‘Response: specific.
GPTevaluationprompt
I’mgoingtoprovidethreepicturesandoneediting
textual instruction. The first is an original picture. C.AdditionalResults
The second and the third pictures are edited pic-
In this section we provide complementary results and
tures, where image editing methods are applying
analysisforvariousaspectsoftheproposedpipeline. First,
transformations to the original picture by follow-
we provide additional evaluation of the model fidelity
ing the instruction. The image editing methods
to specific instructions S. Then, we share insights on
identifiers are A and B. You should tell me what
the embedding distribution for ambiguous and specific
is the editing method that produces the best edited
instructions. Finally, we provide additional qualitative
image. For your evaluation, you should balance
resultsforallmethodsdiscussedinSection4.
howmuchtheeditedimagerespectstheinstruction,
thequalityandrealismofthegeneratedimage,and
thecontentpreservationfromtheoriginalpicture.
ImpactoftheLanguageModel WereplaceGPT-4owith
LLaMA3-instruct [36] and Mistral v0.3 [37], and evaluate
Reply with A or B only without further text.
SANE with N = 3. Results in Table 6 report only slight
The images are ordered in this way: original im-
decrease in performance. Interestingly, decomposing in-
age,theimageofmethodA,theimageofmethodB.
structions wtih LLaMA3-instruct seems to promote image
consistency, outperforming GPT-4o on CLIP using IP2P
Now,provideyouranswerfortheinputimagesand i
(0.7597)andMB(0.8299).Moreimportant,thisprovesthat
the instruction: <c> images: <x>, <x˜ >,
baseline
SANEcanbeusedinconjunctionwithopensourcemodels,
<x˜ >.
ours
promotingaccessibilityandreproducibilityofourresults.
12EMU-Edit EMU-Edit
Method LLM CLIPd↑ CLIPi↑ CLIP∆↑
Method N CLIP ↑ CLIP ↑
d ∆
Baseline - 0.2923 0.8810 0.1203
1 0.2990 0.1679
GPT-4o 0.2968 0.7531 0.1858
SANE 2 0.2907 0.156
SANE LLaMA3-instruct 0.2903 0.7597 0.1657
Mistralv0.3 0.2955 0.7520 0.1771 3 0.2878 0.1518
Baseline - 0.2888 0.7858 0.1618 1 0.2930 0.1551
GPT-4o 0.3006 0.8209 0.1655 SANE 2 0.2924 0.1408
SANE LLaMA3-instruct 0.3001 0.8299 0.1605
3 0.2926 0.1284
Mistralv0.3 0.2943 0.7977 0.1688
Baseline - 0.2675 0.6501 0.1417 1 0.2970 0.1593
SANE 2 0.2944 0.1455
GPT-4o 0.2823 0.6870 0.1474
SANE LLaMA3-instruct 0.2808 0.6793 0.1336 3 0.2907 0.1326
Mistralv0.3 0.2772 0.6521 0.1394
Table7.ImpactofalltheappliedspecificinstructionsWeeval-
Table6. PerformancewithotherLLMs. Opensourcealterna- uatehowmuchtheappliedspecificinstructionsinfluencetheper-
tives to GPT-4o such as LLaMA3-instruct and Mistral v0.3 per- formance of SANE. As expected, with an increasing number of
formcompetitivelyontheinstructiondecompositiontask. instructions,thefidelitytoalltheinstructionsetSdecreases.
IP2Pdata→EMU-Edit EMU-Edit→IP2Pdata
How much each specific instruction is respected? In
Method CLIP d↑ CLIP i↑ CLIP∆↑ CLIP d↑ CLIP i↑ CLIP∆↑
Section4.3ofthemainpaper,wehighlightthatthefidelity
Baseline 0.2906 0.8789 0.1076 0.2937 0.9084 0.1745
to specific instructions is an important property of SANE. SANE 0.2939 0.7828 0.1468 0.2985 0.837 0.2005
Indeed,ifwerespectspecificinstructionsduringediting,we
Baseline 0.292 0.8163 0.142 0.2872 0.8067 0.186
can provide them to the user, improving interpretability of SANE 0.2998 0.8301 0.1457 0.296 0.8297 0.1816
theeditingprocess.Inthemainpaper(Table2),weconsider Baseline 0.2662 0.6765 0.1184 0.2747 0.7226 0.1754
the reference sets S of specific instructions with N = 3, SANE 0.2853 0.6904 0.1288 0.2895 0.7435 0.1735
and compare the average fidelity of the baselines and the
proposed SANE with N = {1,2,3} to each of those pre- Table 8. Evaluation on cross-datasets. For each dataset, we
select a new cross-dataset of ambiguous instructions using the
defined3instructions. Thisallowsustoconcludethateach
nearestneighborclassifierfittedontheotherdataset. Theresults
specificinstructionhasanimpactonthefinalimagetrans-
demonstratetherobustnessofSANEacrossdifferentdatasets
formationrepresentedbythatreferencesetS withN = 3.
However,wealsoneedtoevaluateforeachN thefidelityof
SANEtothesetofspecificinstructionsactuallybeingused
inthedenoisingprocess. InasimilarveintoTable2,inTa-
ble7wereporttheaverageoverN CLIP-basedmetricswith
respecttoeachspecificinstructions ,butconsideringonly
i
thecorrespondingsetofN instructions.Thismeansthatwe
evaluatefidelitywithrespecttoasinglespecificinstruction
if N = 1, to two if N = 2, and to three if N = 3. Con-
sidering that the baselines do not use specific instructions
forinference,itisimpossibletocomparewiththeminthis
setup,thusmotivatingthesetupchosenforTable2. Asvis- Classes
add_EMU-Edit
iblefromtheresults,thefidelitytoeachspecificinstruction ambiguous_IP2P
background_EMU-Edit
decreaseswithhigherN. Thisisexpected,sincewithmore color_EMU-Edit
global_EMU-Edit
instructionstofollow,theeditingtaskismorechallenging, local_EMU-Edit
remove_EMU-Edit
and the editing effects might overlap. However, we high- style_EMU-Edit
text_EMU-Edit
light that the N = 3 setup still performs best in Table 1,
resultinginatrade-offforthechoiceofN. Weempirically Figure 5. t-SNE visualisation of instruction embeddings. We
findN =3tobeagoodvalue. compute embeddings for all instructions from the EMU-Edit
datasetandforambiguousinstructionsfromtheIP2Pdataset,and
we visualise them using t-SNE. We show that embeddings form
Analysis of instruction embeddings In Section 4 we
clusterscorrespondingtothetasktypes(splits)inEMU-Edit.
extensively evaluate our SANE on two datasets: EMU-
Edit[49]andIP2Pdata[6].Toanalyzetheinstructionspace
ofthesedatasets,weembedallinstructionsintothevectors
13
P2PI
BM
tidEQH
P2PI
BM
tidEQH
P2PI
BM
tidEQH11 the schoice to limit the number of specifc instructions to
N ={1,2,3}inourwork.
10
9
Additional qualitative results We provide additional
8
qualitativeresultsforSANEappliedontopofallthreebase-
7
lines. In particular, in Figure 7 we show more editing ex-
6
amples using InstructPix2Pix as a baseline, complement-
5
ing qualitative results in the main paper (Figure 3), while
4
in Figure 8 and Figure 9 we report results on MagicBrush
1 2 3 5 10 andHQEditbaselines,respectively,whicharenotincluded
N
inthemainmanuscriptduetolimitedspace.
Figure 6. Computational costs for different N. We measure
theaveragetimeforeditingoneimagewithmultipleN configura-
tions.N andprocessingtimesaredirectlyproportional.
of length 3072 using text-embedding-3-large by
OpenAI,andvisualisetheseembeddingsusingt-SNE[53].
In Figure 5, we show all splits from EMU-Edit against
ambiguous instructions from IP2P data. Interestingly, in-
struction embeddings from EMU-Edit form distinct clus-
ters corresponding to the types of the task (splits). This
shows that the instruction space has a complex structure
which depends on the semantics encoded by instructions.
Moreover, the set of ambiguous instructions from EMU-
Edit (global EMU-Edit) is completely included in the set
of ambiguous instructions from IP2P (ambiguous IP2P),
showingtheaffinityofthesesets. Toevaluatecross-dataset
robustnessofourSANE,wetakeadvantageofthisaffinity,
andperformcross-datasetambiguousinstructionclassifica-
tionbyusingambiguousinstructionsfromonedatasettofit
anearestneighborclassifier,andbyselectingwiththisclas-
sifiertheambiguousinstructionsfromtheotherdataset. We
denote the selected instructions as “cross-datasets” IP2P
data→EMU-Edit(forEMU-EditclassifiedwithIP2Pdata)
andEMU-Edit→IP2Pdata(forIP2PclassifiedwithEMU-
Edit). We evaluate our SANE on these cross-datasets for
N = 3 and compare it against the same baselines as in
Section4. TheresultsinTable8notonlydemonstratethe
robustness of our model across different datasets, but also
show that such cross-dataset classification can be used as
analternativeambiguousinstructionselectionstrategythat
doesnotrequireanyprompting.
Computational times SANE implies an increased com-
putational load due to multiple denoising operations re-
quiredforprocessingspecificinstructions. Wemeasurethe
time required to produce an edited image x˜ depending on
the number of specific instructions N. For each point in
Figure 6, we average the processing time over 10 images,
using InstructPix2Pix as baseline on NVIDIA 4090 GPU.
As seen, increasing N brings a considerable computation
overload. This, in addition to results in Table 7, justify
14
)s(
emiTSANE
Baseline N =1 N =2 N =3
Originalimage
Instructions→ Change the image so it Apply a sepia filter Add visible rust to Add cracks and stains
apears old and musty. to the entire image the refrigerator to the stove
Change the image so it Add futuristic
Instructions→ appears to be set in buildings in the Replace the road with Add holographic signs
a modern hover road around the police box
the distant future background
Have the image look
Instructions→ like it was taken Dim the lighting in Replace the background Add a candle on the
the scene with darkness table
during a power outage
Replace the bright sky
Instructions→ Change the time of the with a starry night Add a moon in the Add streetlights along
day to night background the dirt road
sky
Figure7.AdditionalqualitativeresultsforInstructPix2Pix.
15SANE
Baseline N =1 N =2 N =3
Originalimage
Change the image to Replace the grassy
Instructions→ appear like a fire in field with a burning Add flames around the Add smoke in the sky
Ducati motorcycle above the forest
a forest forest background
Add snow to the roofs
Instructions→ Make it a snowy day Add snow on the ground and ledges of the Add falling snowflakes
around the motorcycles throughout the scene
museum building
Add rocky textures to Adjust the lighting to
Instructions→ Put this inside of a Replace the background the monitor, keyboard, darker, more cave-like
cave with a cave interior
mouse, and speakers conditions
Replace the kitchen
Add multiple
table with an
Instructions→ Change this to a industrial Add commercial-grade professional kitchen
restaurant kitchen ovens and stoves knives and utensils on
stainless-steel
the countertop
worktable
Figure8.QualitativeresultsforMagicBrush.
16SANE
Baseline N =1 N =2 N =3
Originalimage
Make the scene seem Add a basket with
Instructions→ like it is an outdoor Add a picnic blanket Add trees in the fruits beside the
under the sandwich background
picnic sandwich
Replace the clear sky
Instructions→ Change this to a Add falling snow to Add snow accumulation with a cloudy,
snowstorm the scene to the cart and cow
snow-filled sky
Change this to night
Instructions→ time with lots of Change the sky to a Adjust the lighting to Add a glowing moon in
dark, star-filled sky create moonlit shadows the background
stars
Change the man’s red
Make the image appear
Instructions→ to be in the Sahara Replace the snowy hill jacket to a lighter, Add a blazing sun in
with sand dunes desert-appropriate the sky
desert
color
Figure9.QualitativeresultsforHQEdit.
17