MambaGesture: Enhancing Co-Speech Gesture Generation with
Mamba and Disentangled Multi-Modality Fusion
ChencanFu∗ YabiaoWang∗ JiangningZhang† XiaofengMao
ZhejiangUniversity ZhejiangUniversity ZhengkaiJiang FudanUniversity
Hangzhou,China Hangzhou,China TencentYoutuLab Shanghai,China
chencan.fu@zju.edu.cn TencentYoutuLab Shanghai,China xfmao23@m.fudan.edu.cn
Shanghai,China vtzhang@tencent.com
caseywang@tencent.com zhengkjiang@tencent.com
JiafuWu ChengjieWang YanhaoGe YongLiu†
WeijianCao ShanghaiJiaoTong VIVO ZhejiangUniversity
TencentYoutuLab University Shanghai,China Shanghai,China
Shanghai,China TencentYoutuLab halege@vivo.com yongliu@iipc.zju.edu.cn
jiafwu@tencent.com Shanghai,China
weijiancao@tencent.com jasoncjwang@tencent.com
Abstract Keywords
Co-speechgesturegenerationiscrucialforproducingsynchronized GestureGeneration,MotionProcessing,Data-DrivenAnimation
andrealistichumangesturesthataccompanyspeech,enhancingthe
ACMReferenceFormat:
animationoflifelikeavatarsinvirtualenvironments.Whilediffu-
ChencanFu,YabiaoWang,JiangningZhang,ZhengkaiJiang,XiaofengMao,
sionmodelshaveshownimpressivecapabilities,currentapproaches JiafuWu,WeijianCao,ChengjieWang,YanhaoGe,andYongLiu.2024.
oftenoverlookawiderangeofmodalitiesandtheirinteractions,re- MambaGesture:EnhancingCo-SpeechGestureGenerationwithMamba
sultinginlessdynamicandcontextuallyvariedgestures.Toaddress andDisentangledMulti-ModalityFusion.InProceedingsofthe32ndACM
thesechallenges,wepresentMambaGesture,anovelframework InternationalConferenceonMultimedia(MM’24),October28–November
integratingaMamba-basedattentionblock,MambaAttn,with 1,2024,Melbourne,VIC,Australia.ACM,NewYork,NY,USA,10pages.
amulti-modalityfeaturefusionmodule,SEAD.TheMambaAttn https://doi.org/10.1145/3664647.3680625
block combines the sequential data processing strengths of the
1 Introduction
Mambamodelwiththecontextualrichnessofattentionmecha-
nisms,enhancingthetemporalcoherenceofgeneratedgestures. Co-speechgesturegeneration,thetaskofproducinghumanges-
SEADadeptlyfusesaudio,text,style,andemotionmodalities,em- turessynchronizedwithaudioandothermodalities,iscrucialfor
ployingdisentanglementtodeepenthefusionprocessandyieldges- enhancingavatarrealisminanimation,film,andinteractivegaming.
tureswithgreaterrealismanddiversity.Ourapproach,rigorously Craftinggesturesthatarebothrealisticanddiverseisasignificant
evaluatedonthemulti-modalBEATdataset,demonstratessignifi- challengeandafocalpointincontemporaryresearch.
cantimprovementsinFréchetGestureDistance(FGD),diversity Extensiveresearchhasbeenconductedinthisarea,leadingto
scores,andbeatalignment,achievingstate-of-the-artperformance thedevelopmentofnumerousinnovativeapproaches.Gesturegen-
inco-speechgesturegeneration. erationtechniquesaregenerallydividedintorule-basedanddata-
drivenmethods,withthelatterfurthercategorizedintostatistical
CCSConcepts andlearning-basedapproaches.Thispaperfocusesonlearning-
•Human-centeredcomputing→Humancomputerinterac- basedco-speechgenerationmethods,whichcangenerallybedi-
tion(HCI);•Computingmethodologies→Motionprocessing. videdintotwocategories.1)Autoencoder-basedmethods,which
employautoencoders(AEs)[27]orvariationalautoencoders(VAEs)
[26,48]totranslategesturegenerationintoareconstructiontask,
∗Bothauthorscontributedequallytothisresearch.
asshowninFigure1.Despitetheircomputationalefficiency,these
†Correspondingauthors.
methodsarelimitedbytheirarchitecture,resultinginrestricted
gesturediversity.2)Diffusion-basedmethods,recognizedforpro-
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
ducingdiversegestures,suchasDiffuseStyleGesture+[46],which
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation combinesdiffusionmodelswithTransformerencodersandinte-
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe gratesmultiplemodalitiestoenhancerealismanddiversity.How-
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
ever,thesemethodsoftenfailtofullyexploittherichinteractions
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. amongmulti-modaldata,leadingtolessexpressivegestures.
MM’24,October28-November1,2024,Melbourne,Australia. DrawinginspirationfromthestatespacemodelMamba,effective
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
insynthetictasks,languagemodeling,andaudiogeneration[10],
ACMISBN979-8-4007-0686-8/24/10
https://doi.org/10.1145/3664647.3680625 werecognizeitspotentialforco-speechgesturegeneration.Mamba,
4202
luJ
92
]CH.sc[
1v67991.7042:viXraMM’24,October28-November1,2024,Melbourne,Australia. ChencanFuetal.
Diversity Realism ourgesturediffusionmodelenablestheproductionofgesturesthat
Audio
arebothrealisticanddiverse.Additionally,wepresentthecross- Text
Style Gesture attentionenhancedStyleandEmotionAwareDisentangled(SEAD)
Emotion AE-based featurefusionmodule.Thismoduleintroducesanoveltechnique
fordisentanglingaudiotoextractpersonalstyleandemotion.The
Audio combinationofMambaAttn’sadvancedsequencemodelingwith
Text Gesture theSEADmoduleempowersourframeworktogenerateco-speech
Style
gesturesthatarebothdiverseandrealistic.
Quadratic Scaling
Ourprimarycontributionsareasfollows:
Diffusion with attention Diffusion-based
(1)WearethefirsttointroducetheMambamodeltothefield
Audio ofdiffusion-basedco-speechgesturegeneration.Mamba’ssupe-
Text riorsequencemodelingcapabilitiesmakeitwell-suitedfortasks
Gesture
Style
requiringtemporalcoherenceanddynamicgesturerepresentation.
Emotion
Linear Scaling (2)WeintroducetheMambaAttnblock,whichenhancessequen-
Diffusion with SSM Ours tialmodeling,andtheSEADmodule,anovelaudiodisentanglement
approachthatfusesmulti-modaldata.TheSEADmoduleeffectively
Figure 1: Comparison of our approach with mainstream capturespersonalstyleandemotionfromspeechaudio,enriching
co-speechgenerationmethods.(a)Autoencoder(AE)-based themulti-modalinformationusedforgesturegenerationandlead-
methods[26,27]synthesizegesturesbyfusingmulti-modal ingtomorerealisticanddiversegestures.
databutinherentlysufferfromlimiteddiversityduetoar- (3)Ourextensiveexperimentalevaluationonthelargemulti-
chitecturalconstraints.(b)Diffusion-basedmethods[44,46] modalBEATdatasetconfirmsthatMambaGestureachievesstate-of-
employdiffusionmodelswithTransformerstogeneratedi- the-artperformanceinco-speechgesturegeneration.Ourmethod
versegesturesbutarehinderedbythequadraticcomplexity outperformsexistingmodelsonseveralkeymetrics,highlighting
ofTransformerandoftenoverlookintricatemulti-modalcor- theeffectivenessofourcontributions.
relations.(c)OurMambaGestureleveragesthelinearscaling
andsequentialdataprocessingadvantagesoftheStateSpace
2 RelatedWork
Modeltoenhancegesturediversityandeffectivelyharness
multi-modaldatawithdisentangledfeaturefusion,ensuring 2.1 Co-speechGestureGeneration
abroaderspectrumandhigherrealismingesturegeneration. Co-speech gesture generation involves producing gestures syn-
chronizedwithspeechaudio,achallengingtaskduetothelackof
explicitmappingsbetweenspeechandgestures.
anevolutionofRNNs,overcomestheirlimitationsinparallelcom- Gesturegenerationmethodologiescanbebroadlycategorized
putationthroughaparallelscanalgorithmandboastslinearscaling, intorule-basedanddata-drivenapproaches.Thelatterisfurther
contrastingwiththequadraticcomplexityoftraditionaltransform- subdividedintostatisticalandlearning-basedmethods[32].Rule-
ers.OurexplorationintoMamba’sapplicationinco-speechgesture based methods [4, 19, 20, 29, 35, 40] are known for generating
generationrevealsitscapacitytoproducegesturesthatarebothre- high-qualitymotionsbutlacktheflexibilityanddiversityofdata-
alisticanddiverse.Experimentally,wefindthatcombiningMamba’s drivensystems.Statisticalsystems[3,8,17,22,31,47]modelges-
sequentialmodelingstrengthswiththecontextualawarenessofat- ture distribution by analyzing the statistics rather than relying
tentionmechanismsyieldsthebestresults.Weproposeintegrating onexpert-encodedrules,typicallyinvolvingpre-computingcondi-
multiplemodalities(audio,text,style,andemotion)tosignificantly tionalprobabilitiesorassigningpriorprobabilitydistributions.
elevatetherealismanddiversityofgeneratedgestures.Whileau- Recently,learning-basedmethodsusingCNNs,RNNs,andtrans-
diolaysthefoundationforgesturecues,addingstyleandemotion formershavegainedtraction.Liuetal.[28]proposeahierarchical
enrichesthegesturaloutput,capturingindividualexpressionsand approachforgesturegeneration,consideringthehierarchicalna-
emotionalsubtleties.Previousresearchhasoftenfocusedonaudio tureofspeechsemanticsandhumangestures.Yoonetal.[49]treat
astheprimaryinput,neglectingthedepthofinformationfrom gesturegenerationasatranslationproblem,employingarecurrent
othermodalities.Wearguethataudiocontainsrichdetailsthat, neuralnetworkthatutilizesmulti-modalcontextsofspeechtext,
wheneffectivelydisentangledandcombinedwithtext,style,and audio,andspeakeridentity,incorporatinganadversarialscheme
emotion,cangreatlyrefinegesturesynthesis.Ourapproachaimsto toenhancerealism.Liuetal.[27]introducetheBEATdataset,fea-
fullyexploitthisrichmulti-modaldata,withaparticularemphasis turinggestures,facialexpressions,audio,text,emotions,speaker
onaudiodisentanglement,toachievemorenuancedandcontextu- identityandsemantics.TheirCascadedMotionNetwork(CaMN)
alizedco-speechgesturegeneration.Theconceptofourapproach synthesizes body and hand gestures using adversarial training
isillustratedinFigure1. acrossmultiplemodalities.DisCO[25]disentanglesmotioninto
Inthiswork,weintroduceMambaGesture,anovelframework implicitcontentandrhythmfeatures,feedingtheprocessedfea-
thatintegratesaMamba-basedattentionblock,MambaAttn,with turesintoamotiondecodertosynthesizegestures.EMAGE[26]
amulti-modalityfeaturefusionmodule,SEAD.TheMambaAttn employsMaskedGestureReconstruction(MG2G)toencodebody
block,depictedinFigure2,leveragestherobustsequencemodeling hintsandAudio-ConditionedGestureGeneration(A2G)todecode
capabilitiesofthestatespacemodelMamba.Thisintegrationwithin pre-trainedfaceandbodylatentfeatures,generatingfacialand
erutaeF
delgnatnesiD
noisuF
noisuF
erutaeF noisuF redcoeD
redcoeD
redcoeDMambaGesture MM’24,October28-November1,2024,Melbourne,Australia.
localbodymotionsusingapre-trainedVQ-Decoder.Yietal.[48] employstatevariablestomodeldynamicsystems,makingthem
introduceTalkSHOW,whichutilizesanautoencoderforfacialgen- foundationalinfieldslikecontroltheoryandrobotics.
erationandaVQ-VAEforbodyandhandmotiongeneration,with Thecorestatespacemodelisrepresentedbytheequations:
anautoregressivemodelpredictingthemultinomialdistributionof 𝑥′(𝑡)=A𝑥(𝑡)+B𝑢(𝑡) (1)
futuremotionduringinference.
𝑦(𝑡)=C𝑥(𝑡)+D𝑢(𝑡), (2)
Thesemethodsoftentreatco-speechgesturegenerationasa
reconstructiontask,facingchallengesinestablishingmappingsbe- where𝑥(𝑡)isanN-dimensionallatentstate,𝑦(𝑡)istheoutput,𝑢(𝑡)
tweenspeechandgestures,resultinginlimiteddiversity.However, istheinput,andA,B,C,Daresystemparameters,withD𝑢often
recentadvancementsindiffusion-basedmethodsofferapromising actingasaskipconnection.
alternative,producinggestureswithhighrealismanddiversity. TheStructuredStateSpaceSequencemodel(S4),introducedby
Guetal.[11],buildsonSSMstoachievegenerativemodelingatscale
andfastautoregressivegeneration.S4usestheHiPPOmatrixto
2.2 Diffusion-basedGestureGeneration
constructparameter𝐴,mitigatingthechallengeofgradientscaling
Diffusionmodels,knownforcomplexdatadistributionmodeling withsequencelength.Despitetheiradvantages,SSMs’constant
andmany-to-manymappings,aregainingpopularityforgesture parameterscanlimittheiradaptabilityandcontentawareness.
synthesis.Severalworkshaveusedtextasaconditionfordiffusion Mamba[10]revolutionizessequencemodelingbymaintaining
models to generate human motion, such as MotionDiffuse [50], linearcomplexity,akintostatespacemodels(SSMs),whilerivaling
FLAME[16],andMDM[38].Recentresearchfocusesongenerat- Transformers’capabilities.Itachievesthisthroughaninnovative
ingco-speechgesturesusingdiffusionmodels.Alexanderson[1] input-dependentselectionmechanismthatdynamicallyadjustspa-
adaptstheDiffWavearchitecture[18],replacingdilatedconvolu- rametersbasedontheinput,significantlyenhancingthemodel’s
tionswithTransformersorConformerstoenhanceperformance. contentsensitivity. Additionally,Mambaincorporatescomputa-
Zhuetal.[53]introduceDiffGesture,whichconcatenatesnoisyges- tionalstrategiessuchaskernelfusion,parallelscan,andrecom-
turesequenceswithcontextualinformationinthefeaturechannel putation,whichcollectivelystreamlinethecomputationalprocess.
fortemporalmodelingusingtransformers.DiffuseStyleGesture+ ThesefeatureshavespurredthecreationofMamba-basedapplica-
[46] conditions on audio, text, style, and seed gesture, employ- tions,includingMambaTalk[42],whichreplacestraditionalatten-
inganattention-basedarchitecturefordenoising.UnifiedGesture tionmechanismswithMambablocksforefficientandhigh-quality
[43]utilizesaretargetingnetworktostandardizeprimalskeletons gesturegeneration,andMotionMamba[51],whichleveragesa
from various datasets, expanding the data pool and employing U-Netarchitecturewithhierarchicaltemporalandbidirectional
VQ-VAEandreinforcementlearningforgesturegenerationrefine- spatialMambablocksforadvanceddenoisingcapabilities,further
ment. LivelySpeaker [52] emphasizes the importance of seman- augmentedbyaCLIPtextencoderforinputconditioning.Thede-
ticsingestureunderstandingandadoptsatwo-stagestrategyfor velopmentoftheseapplicationsunderscoresMamba’sadaptability
semantic-awareandrhythm-awaregesturegeneration.AMUSE[7] anditsburgeoningroleinenhancingmotiongenerationtasks.
disentanglesspeechintocontent,emotion,andpersonalstylela-
tentrepresentations,usingamotionVAEtransformerarchitecture 3 Preliminary
fortheconditionaldenoisingprocessinlatentspace.FreeTalker
3.1 HumanGestureDataFormat
[45] generates spontaneous co-speech gestures from audio and
performstext-guidednon-spontaneousgesturegeneration.Ges- Human gestures are predominantly represented using rotation-
tureDiffuCLIP[2]processestext,motion,andvideopromptswith basedformats,withjointrotationstypicallyexpressedinSO(3).
differentCLIPmodelstoachievestylecontrol.DiffSHEG[5]con- Thesecanbeparameterizedthroughvariousmethods,including
sidersexpressionsascuesforgestures,achievingreal-timejoint Eulerangles,axisangles,andquaternions[54].
generationofexpressionsandco-speechgestures. In this paper, we utilize the BEAT dataset [27], noted for its
However,mostexistingapproachesdonotconsiderfullmodali- extensive duration and diverse modalities. The motion capture
ties,nordotheyofferacomprehensiveanalysisoftheinteractions dataintheBEATdatasetisstoredinBVHfileformat,withmotion
betweenthesemodalities,potentiallyleadingtolessdiverseand
representedviaEulerangles:75×3rotations+1×3roottranslation.
realisticgeneratedgestures.OurproposedMambaGestureintro- Thisdatasetincludes27bodyjointsand48handjoints.Consistent
ducesanovelcross-attentionenhancedStyleandEmotionAware withthemethodologiesemployedbyDiffuseStyleGesture+[44],
Disentangled(SEAD)featurefusionmodule,whichcleverlydisen- wepreferrotationmatricesoverEuleranglesforjointrotations
tanglesstyleandemotionfromaudioinputsandintegratesrich toenhancetherobustnessandaccuracyofourgesturegeneration
multi-modalconditions(audio,text,style,andemotion)tofacilitate model.Consequently,wetreattheroottranslationasasinglejoint,
thegenerationofgesturesthatarebothrealisticanddiverse. resultinginatotalof76joints.Weuseall76jointsforwhole-body
gesturegenerationandselect14upper-bodyjoints,alongwiththe
48handjoints,forupper-bodygestures.
2.3 StateSpaceModels
3.2 DenoisingDiffusionProbabilisticModel
StateSpaceModels(SSMs)[9,11,12,15,33,37]haveregainedpop-
ularityinsequencemodelingtasksduetotheirlinearornear-linear Denoisingdiffusionprobabilisticmodels(DDPMs)aregenerative
scalingwithsequencelength,outperformingattentionmechanisms modelsdesignedtoapproximatereal-worlddatadistributions,de-
withquadraticscaling.OriginatingfromRNNsandCNNs,SSMs noted as𝑞(x0). Introduced by Ho et al. [14], DDPMs employ aMM’24,October28-November1,2024,Melbourne,Australia. ChencanFuetal.
forwarddiffusionprocessthatincrementallyaddsGaussiannoise yield 𝑓 𝑡𝑒𝑥𝑡 and 𝑓 𝑠,respectively.Emotionfeaturesareencodedto
todata,transitioningittowardsanoisedistribution,andareverse produce𝑓 𝑒,withbothstyleandemotionfeaturessubjecttorandom
processthatreconstructstheoriginaldatafromthenoise. maskingduringtraining.
TheforwarddiffusionisaMarkovchaindescribedas: Thepreliminaryfeatureprocessingofthemulti-modalitydata
𝑇 iscompletedbeforetraining.Thesepreprocessedfeaturesarethen
(cid:214)
𝑞(𝒙1:𝑇|𝒙0)= 𝑞(𝒙𝑡|𝒙𝑡−1), (3) utilizedinthesubsequentfusionmodule.
𝑡=1 Previousworkshaveexploredmulti-modalityfusionforinput
𝑞(𝒙𝑡|𝒙𝑡−1)=N(𝒙𝑡;√︁ 1−𝛽 𝑡𝒙𝑡−1,𝛽 𝑡𝑰), (4) conditions[27,44].Theyperformfusioninacascadedwayorlever-
agethepowerfulmodelingcapabilitiesofattentionmechanism[39].
where𝛽 𝑡 increasesovertime,makingthedataresembleGaussian However,incurrentdiffusion-basedgesturegenerationmethods,
noiseN(0,𝑰).
theprocessofmulti-modalityfeaturefusionisoftencomplexand
Thereverseprocessreconstructsthedatadistribution𝑝 𝜃(𝒙0:𝑇):
lacksclarity.WefirstintroducetheStyleAware(SA)featurefu-
𝑇 sionmodule,whichsimplyusesaudiofeature𝑓 𝑎,textfeature𝑓 𝑡𝑒𝑥𝑡
(cid:214)
𝑝 𝜃(𝒙0:𝑇)=𝑝 𝜃(𝒙𝑇) 𝑝 𝜃(𝒙𝑡−1|𝒙𝑡), (5) andstylefeature𝑓 𝑠 asconditions,andconcatenatesthemwiththe
𝑡=1 noisygesturefeature𝑓 𝑔andtimefeature𝑓 𝑡.Afterconcatenatingall
𝑝 𝜃(𝒙𝑡−1|𝒙𝑡)=N(𝒙𝑡−1;𝜇 𝜃(𝒙𝑡,𝑡),Σ 𝜃(𝒙𝑡,𝑡)), (6) modalityfeatures,weemployacross-localattention[36]moduleto
capturelocalinformationwithintheconcatenatedfeature,resulting
withΣ 𝜃(𝒙𝑡,𝑡) asatime-dependentconstant.Themodelapprox-
inthefusedmulti-modalityfeature𝑓 𝑓𝑢𝑠𝑒.
imatesthemeanoftheGaussiandistributionduringthereverse
Moreover,manystudiesoverlooktheroleofemotion.However,
process.
emotioncansignificantlyinfluenceourgestureswhenwespeak.
Ourapproachdivergesfrompredictingnoiseateachstep𝑡.In-
Toaddressthis,weproposetheStyleandEmotionAware(SEA)
stead,wepredictthecleandatasample𝒙0directly,followingrecent
featurefusionmodule,buildinguponSA,whichintroducesemotion
methodologies[34,38,44],toenhancethegenerativemodel’seffi-
asagenerationcondition.Theemotionfeature𝑓 𝑒 isfusedthrough
ciencyandaccuracy.
concatenationinthesameway.Byutilizingaudio,text,styleand
emotion,thecomprehensiveconditionsprovideamorespecific
4 Method
descriptionandcommand,whichbenefitsourgesturegeneration.
Ourapproachisstructuredaroundtwopivotalcomponents:the SEADFusion.Existingworksonmultimodalfusionoftenfail
cross-attentionenhancedStyleandEmotionAwareDisentangled tofullyexploittheinherentrelationshipsbetweenmodalities.Our
(SEAD)featurefusionmoduleandtheMambaAttn-baseddenois- StyleandEmotionAwareDisentangled(SEAD-basic)module,an
ingnetwork.Atthecoreofourmotiongenerationliesthediffusion extensionoftheSEAmodule,disentanglesstyleandemotionfrom
modelframework,whichemploysaniterativeprocessofdiffusion audioinputusingself-supervisedlearning.Itavoidsthecomplexity
(addingnoise)anddenoisingtoreconstructoriginalgesturesfrom oftwo-stagedisentanglementmethods.Thismoduleconsistsof
anoisedistribution,conditionedonaudioandadditionalmulti- threeindependentunitsthatextractstyleandemotionfeatures
modaldatainputs.TheSEADmoduleistaskedwiththeintricate fromtheaudiofeature𝑓 𝑎asfollows:
f inu gsio nn eto wf om rku il sti- dm edo id ca al ti et dy td oa tt ha, ew ach cil ue rath tee pM rea dm icb ta ioA ntt on f- gba ess te ud red se .n To his e- 𝑓 𝑎𝑠 =Linear(𝑓 𝑎)
overviewofourproposedMambaGestureisillustratedinFigure2. 𝑓 𝑎𝑒 =Linear(𝑓 𝑎) (7)
𝑔
𝑓
𝑎
=Linear(𝑓 𝑎),
4.1 DisentangledMulti-ModalFusion
wherelinearlayersareusedtoextractstyleandemotionfeatures
Ourmethodologyintroducesaprogressiveseriesofmulti-modality from𝑓 𝑎.Theextracted𝑓 𝑎𝑠 and𝑓 𝑎𝑒 arealignedwiththecorrespond-
featurefusiontechniques.Thesemethodsincrementallyintegrate
ingstyleandemotionfeaturesusingstyleloss L𝑠 andemotion
featuresfromvariousmodalities,evolvingfromsimpleconcatena-
lossL𝑒 tofacilitatetheseparationofpersonalstyleandemotion
tiontosophisticated,disentangledfusion.
informationfromtheaudio.
SAandSEAFusion.Adoptingthestate-of-the-artDiffuseS- Afterdisentanglingtheaudiostylefeature𝑓 𝑎𝑠 andaudioemo-
tyleGesture+(DSG+)[44,46]asourbaseline,werefinethegesture tionfeature𝑓 𝑎𝑒 ,wefusethemwiththeoriginalstyleandemotion
g tie mn ee sr ta et pio 𝑡n ,p nr oo isc yes gs eb sy tuc ro en 𝑥d 𝑡i ,ti ao nn din cg onon dia tiose nt so 𝑐f .m Ino cd oa nli tt rie as s, ti tn ocl Du Sd Gin +g
,
featurestoobtainenhancedrepresentations𝑓 𝑠ℎ and𝑓 𝑒ℎ :
whichemploysaudio𝑎,text𝑡𝑒𝑥𝑡,style𝑠,andseedgesture𝑑,our 𝑓 𝑠ℎ =Linear(Cat(𝑓 𝑎𝑠,𝑓 𝑠)) (8)
approachintroducesemotion𝑒 asaconditionwhiledispensing 𝑓 𝑒ℎ =Linear(Cat(𝑓 𝑎𝑒,𝑓 𝑒)), (9)
withtheseedgesture.Thisdecisionisinformedbytherecognition
thatemotionplaysacriticalroleinthenaturalvariationofgestures. whereLineardenotesalinearlayer,andCatdenotesconcatenation.
Weprocesseachmodalitythroughaseriesofstepstopreparethe Bycombiningthemwiththeoriginalfeatures,wedecouplestyle
featuresforfusion.Thetimestepisencodedthroughpositionencod- andemotionfromtheoriginalaudioandenhancethem.There-
𝑔
ingandanMLPtoproducethetimefeature𝑓 𝑡.Thenoisygesture mainingfeature𝑓 𝑎 isdirectlyrelatedtothegesture.Theremaining
𝑥 𝑡 isencodedbyalinearlayertoobtainthenoisygesturefeature𝑓 𝑔. processofSEAD-basicisthesameasSEA,whereweconcatenate
Audiofeaturesareextractedandenrichedwithpretrainedmodels 𝑓 𝑎𝑔 , 𝑓 𝑠ℎ , 𝑓 𝑒ℎ , 𝑓 𝑡𝑒𝑥𝑡 with 𝑓 𝑡 and 𝑓 𝑔,andusecross-localattentionto
toform𝑓 𝑎,whiletextandstylefeaturesaresimilarlyprocessedto obtainthefusedfeature𝑓 𝑓𝑢𝑠𝑒.MambaGesture MM’24,October28-November1,2024,Melbourne,Australia.
LayerNorm
Linear Denoising
Self-Attention
Diffusion
Linear C Linear Linear Linear
Conv
Linear q Cross + Denoising
Attention
k, v SSM
Linear C Linear Diffusion x
... Linear
Mamba
Denoising
Linear
LayerNorm
Style and Emotion Aware Disentangled Feature Fusion Module
Sampling MambaAttn Block
Figure2:OverviewofourproposedMambaGesture.Weintroduceanovelfeaturefusionstrategy:thecross-attentionenhanced
StyleandEmotionAwareDisentangled(SEAD)featurefusionmodule.Thismoduleemploysstyle𝒔,audio𝒂,emotion𝒆,and
text𝒕𝒆𝒙𝒕 asconditionstoprovidecomprehensiveinformationandeffectivelydisentanglestyleandemotionfromtheaudio.
The𝒇′ isobtainedbyconcatenating𝒇′ and𝒇′,andprojectedtooriginaldimensionbylinearlayer.Besides,wepresenta
𝒔𝒆 𝒔 𝒆
Mamba-basedcomponenttermedtheMambaAttnblock,whichmergesMambawithitssequencemodelingproficiencyand
employsanattentionmechanismtolearnglobalinformation.Ourdenoisingarchitecture,MambaAttndenoiser,iscomposed
ofastackofMambaAttnblocksandalinearlayer.Duringthesamplingphase,wepredictthegesture𝒙ˆ0byapplyingthefused
conditionswithinacyclicaldenoisinganddiffusionprocedure.
Building on the SEAD-basic module, the cross-attention en- MambaAttnBlock.TheMambaAttnblockisanovelarchi-
hancedStyleandEmotionAwareDisentangled(SEAD)feature tecturalcomponentthatcombinesthesequentialdataprocessing
𝑔
fusionmodulefurtherenhancestheaudiofeature𝑓 𝑎 withthefused strengthsoftheMambamodelwiththecontextualawarenessof
styleandemotionfeature𝑓 𝑠′ 𝑒 usingacross-attentionmechanism: theattentionmechanism.Thisfusioncreatesapowerfultoolfor
capturingtheintricaciesofco-speechgesturegeneration.
𝑄𝐾𝑇
𝑎𝑡𝑡𝑛=Attention(𝑄,𝐾,𝑉)=softmax( √ )𝑉, (10) ThestructureoftheMambaAttnblock,asshowninFigure2,
𝑑 includesaself-attentionlayer,aMambablock,andtwoinstances
𝑔 oflayernormalization.Theprocessbeginswiththefusedfeature
w styh le ere an𝑄 dere mp or te is oe nn ft es at th ue refe 𝑓 𝑠a ′ 𝑒t ,u ar ne d𝑓 𝑎 𝑑, is𝐾 tha end di𝑉 mer ne sp ir oe nse on ft tht ehe fef au ts ue rd e. a𝑓 𝑓 tt𝑢 e𝑠 n𝑒 tu ion nd mer og do uin leg :layernormalization,whichthenenterstheself-
Thefusedstyleandemotionfeature𝑓 𝑠′ 𝑒 isobtainedasfollows:
𝑓 𝑠′ 𝑒 =Linear(Cat(𝑓 𝑠′,𝑓 𝑒′)). (11) 𝐴𝑡𝑡𝑛=Attention(𝑄,𝐾,𝑉)) (12)
TheSEADmodulerepres 𝑔entsthepinnacleofourfusionapproach, where𝑄,𝐾,𝑉 = LN(𝑓 𝑓𝑢𝑠𝑒),andLNdenoteslayernormalization.
wheretheaudiofeature𝑓 𝑎 isfurtherrefinedthroughcross-attention Theself-attentionmoduledistillsglobalcontextualinformation,
withfusedstyleandemotionfeature 𝑓 𝑠′ 𝑒.Thismechanismeffec- whichisthenrefinedbytheMambablocktoenhancesequence
tivelyintegratestheaudiowiththestyleandemotion,enhancing modeling.Theoutputissubsequentlyprocessedasfollows:
therepresentationalcapabilityoftheaudiofeatureandresultingin
acomprehensivefusedmulti-modalityfeature𝑓 𝑓𝑢𝑠𝑒. 𝑥ˆ0=LN(Mamba(𝐴𝑡𝑡𝑛)). (13)
4.2 MambaAttnDenoiser
ThefinaloutputoftheMambaAttnblocksisthenprojectedbackto
Afterintegratingthemulti-modalconditionsintoacomprehensive theoriginaldimensionofthegesturedatathroughthelinearlayer,
featurerepresentation 𝑓 𝑓𝑢𝑠𝑒,wemovetothegestureprediction yieldingthepredictedgesture𝑥ˆ0.
phase.Here,weemployourinnovativeMambaAttn-basednetwork, TrainingandSampling.Totrainournetworks,weusethe
MambaAttnDenoiser,topredictgestures𝑥ˆ0fromthenoisygesture HuberlossfunctionastheprimarymetricforgesturelossL𝑔:
input𝑥 𝑡.TheMambaAttnDenoisercomprises8MambaAttnblocks
andalinearprojectionlayer. L𝑔 =𝐸 𝑥 0∈𝑞(𝑥 0|𝑐),𝑡∼[1,𝑇][HuberLoss(𝑥 0−𝑥ˆ0)]. (14)
etanetacnoC
noitnettAMM’24,October28-November1,2024,Melbourne,Australia. ChencanFuetal.
Forthestyleandemotionlosses,L𝑠 andL𝑒,weusetheL1Loss, whicharethenprocessedthroughalinearlayertoobtainthetext
formulatedas: feature𝑓 𝑡𝑒𝑥𝑡.Personalstyle𝑠andemotion𝑒areencodedasone-hot
L𝑠 =|𝑓 𝑎𝑠 −𝑓 𝑠| vectorsandtransformedintocorrespondingfeatures𝑓 𝑠 and𝑓 𝑒 via
L𝑒 =|𝑓 𝑎𝑒 −𝑓 𝑒|. (15) linearlayers.Thediffusionprocessissetto1000steps.Wetraineach
modelfor40,000stepswithabatchsizeof400,usingtheAdamW
The final loss function is a composite of the gesture, style, and
optimizerwithalearningrateof3×10−5.Allexperimentsare
emotionlosses,weightedandsummedasfollows:
conductedonasingleNVIDIAH800GPU,ensuringreproducibility
L=L𝑔+𝛼L𝑠 +𝛽L𝑒, (16) onstandardhardwareconfigurations.
whereweset𝛼 =1and𝛽 =1forsimplicity. Wecompareourproposedmodelwithstate-of-the-artgesture
generationmethods,includingDiffuseStyleGesture+(DSG+)[44],
FollowingtheDDPMdenoisingparadigm,weiterativelypredict
thegesture𝑥ˆ0ateachtimestep𝑡,asillustratedinFigure2,refining CaMN[27],andMDM[38].Weretrainthesemethodsonourpar-
titioneddatasettoensureafaircomparison.CaMNisretrained
ourmodel’sabilitytogenerateaccurateandlifelikegestures.
withaudio,text,emotion,andspeakeridentityasconditions,while
5 Experiments DSG+employsseedgestures,audio,andspeakeridentity.MDMis
conditionedsolelyonaudiofeatures.Ourevaluationincludesboth
5.1 ExperimentSettings
whole-bodyandupper-bodygesturegeneration.
Dataset.WeevaluateourmethodusingtheBEATdataset[27],
whichincludeshumanmotionscapturedat120Hzviaamotion 5.2 QuantitativeResults.
capturesystem.Thisdatasetfeaturesextendedconversationaudios
Theresults,summarizedinTable1,demonstratethatourMam-
(approximately10minuteseach)andbriefself-talkaudios(around
baGestureframeworkachievesthelowestFGDScoreforbothupper
1minuteeach)from30diversespeakers.Itismulti-modal,offering
bodyandwholebodygesturegeneration,indicatingahighsimilar-
motion,audio,text,style(identity),emotion,andfacialexpression
itybetweenthegeneratedgesturesandtherealdatadistribution.
annotations.Thedatasetcovers8emotions:neutral,anger,happi-
Specifically,ourwholebodygesturegenerationrecordsanFGD
ness,fear,disgust,sadness,contempt,andsurprise.Following[41],
Scoreof22.11,asignificantimprovementoverMDM’s106.56and
weselectonehourofaudioperspeaker,splittingthedatainto70%
DSG+’s103.15.Additionally,ourmethodexcelsinDiversityScore
fortraining,10%forvalidation,and10%fortesting.Wegenerate
andL1DiversityScore,withupper-bodygesturesscoring374.08
bothupperbodyandwholebodygesturesforcomparisonwith
and875.06,respectively,andwhole-bodygesturesscoring434.94
state-of-the-artmethods.Inablationstudies,wefocusonwhole
and1128.79,respectively.Thesescoresunderscoretheenhanced
bodygesturegeneration.
diversityofourgeneratedgestures.Notably,diffusion-basedmeth-
Evaluation Metrics. We use multiple metrics to rigorously
odssuchasMDMandDSG+exhibitbetterdiversitythanCaMN,
assess the quality and diversity of the generated gestures. The
whichreliesonautoencodersforgesturegeneration.Althoughour
FréchetGestureDistance(FGD)[49]measurestheFréchetdistance
methoddoesnotachievethehighestSRGRScores,itremainscom-
betweenthefeaturedistributionsofrealandsynthesizedgestures,
petitivewithCaMN.OurMambaGestureexcelsinBeatAlignfor
similartotheFID[13]usedinimagegeneration.Thegesturefeature
bothupperandwhole-bodygestures,reflectingamoresynchro-
extractor,trainedunsupervisedlywithareconstructionlossusing
nizedaudio-gesturealignment.
L1loss,servesasthebasisforthiscomparison.
Toquantifygesturediversity,weusetheDiversityScore[21]and
5.3 QualitativeResults.
L1DiversityScore[23].TheDiversityScoremeasurestheaverage
featuredistancebyrandomlyselecting500featuresandcomputing UserStudy.Weconductauserstudytosubjectivelyevaluateour
theaverageL1distancebetweenthem. proposedmethodagainststate-of-the-artmethods.Fifteenpartici-
WealsoincorporateSemantic-RelevantGestureRecall(SRGR) pantswereaskedtoevaluate30gesturesamples,eachcontaining
[27]andBeatAlign[24]toevaluatethesemanticrelevanceand gesturesgeneratedbyfourdifferentmethodsusingthesamecor-
synchronyofthegeneratedgestures.SRGR,anevolutionofthe respondingaudio.Theywerethenaskedtoselectthebestgesture
ProbabilityofCorrectKeypoint(PCK),measuresthesemanticrele- foreachofthefollowingcriteria:motionnaturalness,smoothness,
vanceofgesturestotheaccompanyingspeech.BeatAlignassesses diversity, and semantic preservation. The preferred method for
thetemporalalignmentbetweenaudioandgesturebeatsusing eachcriterionwasdeterminedbasedonthenumberofselections
ChamferDistance,providinginsightintotherhythmicharmonyof received,andtheresultsarepresentedaspercentagesinTable2.
thegeneratedgestureswiththespokencontent. VisualizationResults.Figure3visualizestheexperimental
ImplementationDetails.Wedownsamplemotiondatafrom resultsforthespeechtranscript"...whenyouhavetoworkMonday
120Hzto30Hzandsegmentitinto300-frameclips(10seconds throughFridaythewholeweek,youareverytired...",asentence
each).Audiodataisdownsampledto16kHzfromahighersampling thatshouldelicitrichbodymovements.Visualcomparisonshows
rate.Wecomputeacomprehensivesetofaudiofeatures,including thatgesturesgeneratedbyCaMNandDSG+exhibitlimitedmo-
MFCCs, Mel spectrogram features, prosodic features, and pitch tion,whileourapproachdemonstratesrichmotion,highlighting
onsetpoints(onsets).Thesefeaturesareconcatenatedwiththose itseffectiveness.
extractedbythepretrainedWavLMLargemodel[6]toformarich Bothnumericalandvisualresultscorroboratethatourmethod
audiofeatureset𝑓 𝑎.Fortextdata,weemploythepretrainedfastText generatesrealisticanddiverseco-speechgestures,advancingthe
model[30]toextractwordvectorsfromthespeechtranscripts, state-of-the-artinthefield.MambaGesture MM’24,October28-November1,2024,Melbourne,Australia.
Table1:QuantitativecomparisonofourproposedmethodwithcurrentleadingapproachesontheBEATdataset.Boldindicates
thetop-performingmethodandunderlinesignifiesthesecond-bestperformanceacrossvariousevaluationcriteria.
Method FGDScore↓ DiversityScore↑ L1DivScore↑ SRGRScore↑ BeatAlign↑
GT - 403.27 754.75 - 0.894
CaMN[27] 60.67 295.62 519.53 0.216 0.823
UpperBody MDM[38] 54.13 327.82 821.18 0.208 0.823
DSG+[46] 60.50 358.62 748.42 0.213 0.850
Ours 32.45 374.08 875.06 0.213 0.863
GT - 395.20 850.51 - 0.893
CaMN[27] 65.74 277.06 587.12 0.241 0.819
WholeBody MDM[38] 106.56 331.53 1001.52 0.229 0.810
DSG+[46] 103.15 352.31 789.83 0.238 0.841
Ours 22.11 434.94 1128.79 0.237 0.853
Table2:UserStudyResults gesturegeneration.TheintegrationofourSEAmodule,whichintro-
ducesemotionasanovelcondition,leadstoanadditionalreduction
Method Natural Smooth Diversity Semantic intheFGDScore,withothermetricsshowingslightvariations.
CaMN[27] 9.78% 8.00% 3.11% 5.78% Theculminationofourfusionapproach,theSEADmodule,which
MDM[38] 7.78% 9.11% 2.67% 5.56% disentanglesstyleandemotionfromaudiofeaturesandenhances
DSG+[46] 21.33% 22.22% 38.89% 21.56% themthroughcross-attention,furtherdecreasestheFGDScoreand
Ours 61.11% 60.67% 55.33% 67.11%
notablyincreasestheDiversityScoreandL1DiversityScore.These
resultsrobustlyvalidatetheefficacyofourproposedmethod.
OptimalNumberofLayersinMambaAttnDenoiser.An
additionalablationstudyexaminestheoptimalnumberoflayersin
theMambaAttndenoiser.Testingconfigurationsof1,2,4,8,and
12layers,wefindthatan8-layerMambaAttndenoiseryieldsthe
bestperformanceacrosskeyevaluationmetrics,includingtheFGD
Score,DiversityScore,andL1DiversityScore.Notably,increasing
thenumberoflayersto12doesnotconferadditionalbenefitsand
insteadleadstodecreasedperformance.Thus,weselectan8-layer
configurationfortheMambaAttndenoiserinourfinalmodel,as
demonstratedbytheupperpartofTable4.Thisfindingunderscores
theimportanceofbalancingmodelcomplexitywithperformance,
asoverlycomplexmodelsmaysufferfromdiminishingreturnsor
Figure 3: Visualization results comparing state-of-the-art evenperformancedegradation.
methods.Speechtranscript:"...whenyouhavetoworkMonday DesignChoicesforMambaAttnBlock.Furtherexperimen-
throughFridaythewholeweek,youareverytired..." tationisconductedtorefinethearchitecturaldesignoftheMam-
baAttnblock.Initially,weconsiderincorporatingaconvolutional
layertocapturelocalinformation,withself-attentiontocapture
5.4 AblationStudies globalinformation,whilerelyingonMambaforsequentialmodel-
Torigorouslyevaluatethecontributionsofourproposedmethod’s ing.Ourexperimentswithvariouscombinationsofthesemodules,
components,weconductaseriesofablationstudies,systematically asshowninthelowerpartofTable4,indicatethattheinclusion
isolatingandanalyzingtheimpactofeachmodule. ofaconvolutionallayerdoesnotenhanceperformance.Removing
EffectivenessofProposedComponents.WeestablishDiffus- eithertheself-attentionorMambacomponentsfromtheblockre-
eStyleGesture+(DSG+)asourbaselinemodelandincrementally sultedinasignificantdecreaseintheFGDScore,withtheremoval
integrateournovelcomponents:SEA,SEADandtheMambaAttnde- ofMambaleadingtoamorepronounceddropinDiversityScore
noiser.Theresults,detailedinTable3,showthatreplacingDSG+’s andL1DiversityScore.Especiallywhenweremoveself-attention,
transformerencoderwithourMambaAttndenoisersignificantly withonlyMambaandlayernormsinourblocks,itcanbeseen
reducestheFréchetGestureDistance(FGD)Score,indicatinga thattheMamba-onlyarchitecturealsoworkswellforgenerating
closermatchtothegroundtruthgestures.Thisisaccompaniedby realisticanddiversegestures.ThissuggeststhatMamba’ssequen-
substantialimprovementsinboththeDiversityScoreandtheL1 tialmodelingabilityiscrucialforgeneratingdiversegestures.The
DiversityScore.AlthoughtheSemantic-RelevantGestureRecall experimentsfurtherconfirmthisobservation.
(SRGR)Scoreseesamarginaldecrease,theBeatAlignScoreim- FeatureFusionModuleDesigns.Lastly,weevaluatevarious
proves,underscoringtheMambaAttnblock’srobustcapabilityfor designsforthefeaturefusionmodule.TheresultsarepresentedMM’24,October28-November1,2024,Melbourne,Australia. ChencanFuetal.
Table3:Ablationstudyassessingthecontributionofeachinnovativecomponentwithinourgesturegenerationframework.
ThisstudysystematicallyaltersindividualelementstoevaluatetheirimpactontheoverallperformanceontheBEATdataset.
No. Name FGDScore↓ Diversity↑ L1DivScore↑ SRGR↑ BeatAlign↑
1 BasicDSG+ 103.15 352.31 789.83 0.238 0.841
2 +MambaAttn 64.84 389.77 1081.14 0.233 0.855
3 +SEA 29.95 387.29 955.75 0.237 0.840
4 +SEAD 22.11 434.94 1128.79 0.237 0.853
Table4:AblationstudyexaminingtheinfluenceofthenumberoflayersintheMambaAttndenoiserandthearchitectural
designoftheMambaAttnblock.ThisexperimentexplorestheoptimalconfigurationforourmodelontheBEATdataset.
No Name FGDScore↓ DiversityScore↑ L1DivScore↑ SRGRScore↑ BeatAlign↑
A1 MambaAttn-1 62.54 333.29 743.76 0.240 0.831
A2 MambaAttn-2 41.17 329.03 749.73 0.240 0.851
A3 MambaAttn-4 36.47 343.49 895.69 0.238 0.849
A4 MambaAttn-8 22.11 434.94 1128.79 0.237 0.853
A5 MambaAttn-12 38.36 424.39 1110.82 0.235 0.865
B1 MambaAttn 22.11 434.94 1128.79 0.237 0.853
B2 w/Conv 29.88 388.81 862.08 0.238 0.861
B3 w/oAttn 46.58 358.49 864.19 0.238 0.867
B4 w/oMamba 44.21 329.12 771.29 0.240 0.838
B5 w/Conv,w/oAttn 93.25 418.94 895.71 0.237 0.845
B6 w/Conv,w/oMamba 34.10 363.65 813.65 0.239 0.858
B7 w/Conv,w/oAttn&Mamba 75.37 324.37 760.02 0.240 0.846
Table5:Ablationstudyexploringtheeffectivenessofdifferentfeaturefusionstrategiesinourgesturegenerationmodel.The
studycomparesvariousapproachestointegratingmulti-modaldata,includingaudio,text,style,andemotion,todetermine
theirimpactonthequalityofgeneratedgesturesontheBEATdataset.
No Name FGDScore↓ DiversityScore↑ L1DivScore↑ SRGRScore↑ BeatAlign↑
1 OriginDSG+Input 64.84 389.77 1081.14 0.233 0.855
2 Simplify 32.08 387.72 968.36 0.237 0.838
3 Simplify+Emo(SEA) 29.95 389.52 955.75 0.237 0.848
4 SEA+Disentanglement 26.61 395.51 986.89 0.237 0.850
5 SEAD 22.11 434.94 1128.79 0.237 0.853
inTable5.Startingwithasimplifiedfusionapproachthatdirectly 6 Conclusion
concatenatesaudio,text,andstylefeatures(ourSAfusionmodule), ThispaperintroducesMambaGesture,anovelframeworkforco-
weobserveasubstantialdecreaseintheFGDScore,albeitwitha speech gesture generation that leverages the state space model
declineindiversitymetrics,asthismethodforgoesthebaseline’s Mambaandadisentangledmulti-modalityfusiontechnique.Ourap-
featurefusionstrategy.Byaddingtheemotionmodality,wecre- proachintegratestheMambaAttnblock,whichcombinesMamba’s
ateourSEAmodule,whichfurtherimprovestheFGDScore.The strengths in sequential data processing with the contextual un-
subsequentdisentanglementinourSEAD-basicmoduleresultsin derstandingprovidedbyattentionmechanisms.Additionally,the
improvementsacrosstheFGDScore,DiversityScore,andL1Diver- SEADmoduleeffectivelydisentanglesstyleandemotionfromaudio
sityScore.Thefinalenhancementwiththefusedstyleandemotion features,enablingthegenerationofmorerealisticandexpressive
feature 𝑓 𝑠′ 𝑒 inourSEADmodulesignificantlyincreasesboththe gestures.ComprehensiveexperimentsontheBEATdatasetdemon-
DiversityScoreandL1DiversityScore. stratethatMambaGestureoutperformsstate-of-the-artmethods
Thedetailedresultsoftheseablationstudiesdemonstratethe acrossmultiplemetrics,validatingtheeffectivenessoftheSEAD
incrementalbenefitsofeachproposedcomponentinourgesture moduleandMambaAttnblock.Futureworkwilladdresscurrent
generationframework,providingclearevidenceoftheirindividual limitations, such as the slow synthesis speed, and may explore
andcollectiveimpactonthemodel’sperformance. integratingfacialexpressionstosynthesizeacompleteavatar.
rebmunreyal
ngisedkcolbMambaGesture MM’24,October28-November1,2024,Melbourne,Australia.
References
[25] HaiyangLiu,NaoyaIwamoto,ZihaoZhu,ZhengqingLi,YouZhou,ElifBozkurt,
[1] SimonAlexanderson,RajmundNagy,JonasBeskow,andGustavEjeHenter.2023. andBoZheng.2022.DisCo:DisentangledImplicitContentandRhythmLearn-
Listen,denoise,action!audio-drivenmotionsynthesiswithdiffusionmodels. ingforDiverseCo-SpeechGesturesSynthesis.InProceedingsofthe30thACM
ACMTransactionsonGraphics(TOG)(2023). InternationalConferenceonMultimedia.
[2] TenglongAo,ZeyiZhang,andLibinLiu.2023.Gesturediffuclip:Gesturediffusion [26] HaiyangLiu,ZihaoZhu,GiorgioBecherini,YichenPeng,MingyangSu,YouZhou,
modelwithcliplatents.ACMTransactionsonGraphics(TOG)(2023). NaoyaIwamoto,BoZheng,andMichaelJBlack.2023.EMAGE:TowardsUnified
[3] KirstenBergmannandStefanKopp.2009.Increasingtheexpressivenessofvirtual HolisticCo-SpeechGestureGenerationviaMaskedAudioGestureModeling.
agents:autonomousgenerationofspeechandgestureforspatialdescription arXivpreprintarXiv:2401.00374(2023).
tasks..InAAMAS(1). [27] HaiyangLiu,ZihaoZhu,NaoyaIwamoto,YichenPeng,ZhengqingLi,YouZhou,
[4] JustineCassell,CatherinePelachaud,NormanBadler,MarkSteedman,Brett ElifBozkurt,andBoZheng.2022.Beat:Alarge-scalesemanticandemotional
Achorn,TrippBecket,BrettDouville,ScottPrevost,andMatthewStone.1994. multi-modaldatasetforconversationalgesturessynthesis.InEuropeanconference
Animatedconversation:rule-basedgenerationoffacialexpression,gesture& oncomputervision.
spokenintonationformultipleconversationalagents.InProceedingsofthe21st [28] XianLiu,QianyiWu,HangZhou,YinghaoXu,RuiQian,XinyiLin,XiaoweiZhou,
annualconferenceonComputergraphicsandinteractivetechniques. WayneWu,BoDai,andBoleiZhou.2022.LearningHierarchicalCross-Modal
[5] JunmingChen,YunfeiLiu,JiananWang,AilingZeng,YuLi,andQifengChen. AssociationforCo-SpeechGestureGeneration.InProceedingsoftheIEEE/CVF
2024.Diffsheg:Adiffusion-basedapproachforreal-timespeech-drivenholistic ConferenceonComputerVisionandPatternRecognition.
3dexpressionandgesturegeneration.(2024). [29] StacyMarsella,YuyuXu,MargauxLhommet,AndrewFeng,StefanScherer,and
[6] SanyuanChen,ChengyiWang,ZhengyangChen,YuWu,ShujieLiu,Zhuo AriShapiro.2013.Virtualcharacterperformancefromspeech.InProceedingsof
Chen,JinyuLi,NaoyukiKanda,TakuyaYoshioka,XiongXiao,JianWu,Long the12thACMSIGGRAPH/Eurographicssymposiumoncomputeranimation.
Zhou,ShuoRen,YanminQian,YaoQian,JianWu,MichaelZeng,andFuruWei. [30] TomasMikolov,EdouardGrave,PiotrBojanowski,ChristianPuhrsch,andAr-
2021.WavLM:Large-ScaleSelf-SupervisedPre-trainingforFullStackSpeech mandJoulin.2018.AdvancesinPre-TrainingDistributedWordRepresentations.
Processing.arXivpreprintarXiv:2110.13900(2021). InProceedingsoftheInternationalConferenceonLanguageResourcesandEvalua-
[7] KiranChhatre,RadekDaněček,NikosAthanasiou,GiorgioBecherini,Christopher tion(LREC2018).
Peters,MichaelJ.Black,andTimoBolkart.2024. AMUSE:EmotionalSpeech- [31] MichaelNeff,MichaelKipp,IreneAlbrecht,andHans-PeterSeidel.2008.Gesture
driven3DBodyAnimationviaDisentangledLatentDiffusion.InProceedings modelingandanimationbasedonaprobabilisticre-creationofspeakerstyle.
IEEEConferenceonComputerVisionandPatternRecognition(CVPR). ACMTransactionsOnGraphics(TOG)(2008).
[8] Chung-ChengChiuandStacyMarsella.2011.Howtotrainyouravatar:Adata [32] SimbarasheNyatsanga,TarasKucherenko,ChaitanyaAhuja,GustavEjeHenter,
drivenapproachtogesturegeneration.InInternationalWorkshoponIntelligent andMichaelNeff.2023.AComprehensiveReviewofData-DrivenCo-Speech
VirtualAgents. GestureGeneration.InComputerGraphicsForum.
[9] DanielYFu,TriDao,KhaledKSaab,ArminWThomas,AtriRudra,andChristo- [33] MichaelPoli,StefanoMassaroli,EricNguyen,DanielYFu,TriDao,StephenBac-
pherRé.2022.Hungryhungryhippos:Towardslanguagemodelingwithstate cus,YoshuaBengio,StefanoErmon,andChristopherRé.2023.Hyenahierarchy:
spacemodels.arXivpreprintarXiv:2212.14052(2022). Towardslargerconvolutionallanguagemodels.InInternationalConferenceon
[10] AlbertGuandTriDao.2023. Mamba:Linear-timesequencemodelingwith MachineLearning.
selectivestatespaces.arXivpreprintarXiv:2312.00752(2023). [34] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen.
[11] AlbertGu,KaranGoel,andChristopherRé.2022.EfficientlyModelingLongSe- 2022. Hierarchicaltext-conditionalimagegenerationwithcliplatents. arXiv
quenceswithStructuredStateSpaces.InTheInternationalConferenceonLearning preprintarXiv:2204.06125(2022).
Representations(ICLR). [35] BrianRavenet,CatherinePelachaud,ChloéClavel,andStacyMarsella.2018.
[12] AlbertGu,IsysJohnson,KaranGoel,KhaledSaab,TriDao,AtriRudra,and Automatingtheproductionofcommunicativegesturesinembodiedcharacters.
ChristopherRé.2021.Combiningrecurrent,convolutional,andcontinuous-time Frontiersinpsychology(2018).
modelswithlinearstatespacelayers.Advancesinneuralinformationprocessing [36] AurkoRoy,MohammadSaffar,AshishVaswani,andDavidGrangier.2021.Effi-
systems(2021). cientcontent-basedsparseattentionwithroutingtransformers.Transactionsof
[13] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,and theAssociationforComputationalLinguistics(2021).
SeppHochreiter.2017.Ganstrainedbyatwotime-scaleupdateruleconverge [37] YutaoSun,LiDong,ShaohanHuang,ShumingMa,YuqingXia,JilongXue,Jiany-
toalocalnashequilibrium.Advancesinneuralinformationprocessingsystems ongWang,andFuruWei.2023.Retentivenetwork:Asuccessortotransformer
(2017). forlargelanguagemodels.arXivpreprintarXiv:2307.08621(2023).
[14] JonathanHo,AjayJain,andPieterAbbeel.2020.Denoisingdiffusionprobabilistic [38] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and
models.Advancesinneuralinformationprocessingsystems(2020). AmitHaimBermano.2022. HumanMotionDiffusionModel.InTheEleventh
[15] AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,andFrançoisFleuret. InternationalConferenceonLearningRepresentations.
2020.Transformersarernns:Fastautoregressivetransformerswithlinearatten- [39] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,
tion.InInternationalconferenceonmachinelearning. AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.2017. Attentionisall
[16] JihoonKim,JiseobKim,andSungjoonChoi.2023.Flame:Free-formlanguage- youneed.Advancesinneuralinformationprocessingsystems(2017).
basedmotionsynthesis&editing.InProceedingsoftheAAAIConferenceon [40] HannesVilhjálmsson,NathanCantelmo,JustineCassell,NicolasE.Chafai,
ArtificialIntelligence. MichaelKipp,StefanKopp,MaurizioMancini,StacyMarsella,AndrewNMar-
[17] MichaelKipp.2005. Gesturegenerationbyimitation:Fromhumanbehaviorto shall,CatherinePelachaud,etal.2007.Thebehaviormarkuplanguage:Recent
computercharacteranimation. developmentsandchallenges.InIntelligentVirtualAgents:7thInternationalCon-
[18] ZhifengKong,WeiPing,JiajiHuang,KexinZhao,andBryanCatanzaro.2020. ference,IVA2007Paris,France,September17-19,2007Proceedings7.
DiffWave:AVersatileDiffusionModelforAudioSynthesis.InInternational [41] SenWang,JiangningZhang,WeijianCao,XiaobinHu,MoranLi,Xiaozhong
ConferenceonLearningRepresentations. Ji,XinTan,MengtianLi,ZhifengXie,ChengjieWang,etal.2024.MMoFusion:
[19] StefanKopp,BrigitteKrenn,StacyMarsella,AndrewNMarshall,Catherine Multi-modalCo-SpeechMotionGenerationwithDiffusionModel.arXivpreprint
Pelachaud,HannesPirker,KristinnRThórisson,andHannesVilhjálmsson.2006. arXiv:2403.02905(2024).
Towardsacommonframeworkformultimodalgeneration:Thebehaviormarkup [42] ZunnanXu,YukangLin,HaonanHan,SichengYang,RonghuiLi,YachaoZhang,
language.InIntelligentVirtualAgents:6thInternationalConference,IVA2006, andXiuLi.2024.MambaTalk:EfficientHolisticGestureSynthesiswithSelective
MarinaDelRey,CA,USA,August21-23,2006.Proceedings6. StateSpaceModels.arXivpreprintarXiv:2403.09471(2024).
[20] StefanKoppandIpkeWachsmuth.2002.Model-basedanimationofco-verbal [43] SichengYang,ZilinWang,ZhiyongWu,MingleiLi,ZhensongZhang,Qiaochu
gesture.InProceedingsofComputerAnimation2002(CA2002). Huang,LeiHao,SongcenXu,XiaofeiWu,ChangpengYang,etal.2023.Unifiedges-
[21] Hsin-YingLee,XiaodongYang,Ming-YuLiu,Ting-ChunWang,Yu-DingLu, ture:Aunifiedgesturesynthesismodelformultipleskeletons.InProceedingsof
Ming-HsuanYang,andJanKautz.2019.Dancingtomusic.Advancesinneural the31stACMInternationalConferenceonMultimedia.
informationprocessingsystems(2019). [44] SichengYang,ZhiyongWu,MingleiLi,ZhensongZhang,LeiHao,Weihong
[22] SergeyLevine,ChristianTheobalt,andVladlenKoltun.2009.Real-timeprosody- Bao,MingCheng,andLongXiao.2023. DiffuseStyleGesture:stylizedaudio-
drivensynthesisofbodylanguage.InACMSIGGRAPHAsia2009papers. drivenco-speechgesturegenerationwithdiffusionmodels.InProceedingsofthe
[23] JingLi,DiKang,WenjiePei,XuefeiZhe,YingZhang,ZhenyuHe,andLinchao Thirty-SecondInternationalJointConferenceonArtificialIntelligence.
Bao.2021.Audio2gestures:Generatingdiversegesturesfromspeechaudiowith [45] SichengYang,ZunnanXu,HaiweiXue,YongkangCheng,ShaoliHuang,Ming-
conditionalvariationalautoencoders.InProceedingsoftheIEEE/CVFInternational mingGong,andZhiyongWu.2024.Freetalker:ControllableSpeechandText-
ConferenceonComputerVision. DrivenGestureGenerationBasedonDiffusionModelsforEnhancedSpeaker
[24] RuilongLi,ShanYang,DavidARoss,andAngjooKanazawa.2021.Aichoreog- Naturalness.InICASSP2024-2024IEEEInternationalConferenceonAcoustics,
rapher:Musicconditioned3ddancegenerationwithaist++.InProceedingsofthe SpeechandSignalProcessing(ICASSP).
IEEE/CVFInternationalConferenceonComputerVision. [46] SichengYang,HaiweiXue,ZhensongZhang,MingleiLi,ZhiyongWu,Xiaofei
Wu,SongcenXu,andZonghongDai.2023.TheDiffuseStyleGesture+entryto
theGENEAChallenge2023.InProceedingsofthe25thInternationalConferenceMM’24,October28-November1,2024,Melbourne,Australia. ChencanFuetal.
onMultimodalInteraction. [51] ZeyuZhang,AkideLiu,IanReid,RichardHartley,BohanZhuang,andHaoTang.
[47] YanzheYang,JimeiYang,andJessicaHodgins.2020. Statistics-basedmotion 2024. MotionMamba:EfficientandLongSequenceMotionGenerationwith
synthesisforsocialconversations.InComputerGraphicsForum. HierarchicalandBidirectionalSelectiveSSM. arXivpreprintarXiv:2403.07487
[48] HongweiYi,HualinLiang,YifeiLiu,QiongCao,YandongWen,TimoBolkart, (2024).
DachengTao,andMichaelJBlack.2023.Generatingholistic3dhumanmotion [52] YihaoZhi,XiaodongCun,XuelinChen,XiShen,WenGuo,ShaoliHuang,and
fromspeech.InProceedingsoftheIEEE/CVFConferenceonComputerVisionand ShenghuaGao.2023.Livelyspeaker:Towardssemantic-awareco-speechgesture
PatternRecognition. generation.InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
[49] YoungwooYoon,BokCha,Joo-HaengLee,MinsuJang,JaeyeonLee,JaehongKim, Vision.
andGeehyukLee.2020.Speechgesturegenerationfromthetrimodalcontextof [53] LingtingZhu,XianLiu,XuanyuLiu,RuiQian,ZiweiLiu,andLequanYu.2023.
text,audio,andspeakeridentity.ACMTransactionsonGraphics(TOG)(2020). Tamingdiffusionmodelsforaudio-drivenco-speechgesturegeneration.InPro-
[50] MingyuanZhang,ZhongangCai,LiangPan,FangzhouHong,XinyingGuo,Lei ceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
Yang,andZiweiLiu.2024.Motiondiffuse:Text-drivenhumanmotiongenera- [54] WentaoZhu,XiaoxuanMa,DongwooRo,HaiCi,JinluZhang,JiaxinShi,Feng
tionwithdiffusionmodel.IEEETransactionsonPatternAnalysisandMachine Gao,QiTian,andYizhouWang.2023.Humanmotiongeneration:Asurvey.IEEE
Intelligence(2024). TransactionsonPatternAnalysisandMachineIntelligence(2023).