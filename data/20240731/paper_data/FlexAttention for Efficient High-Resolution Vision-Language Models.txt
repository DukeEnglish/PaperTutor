FlexAttention for Efficient High-Resolution
Vision-Language Models
Junyan Li1 , Delin Chen1 , Tianle Cai2 , Peihao Chen3 , Yining Hong4 ,
Zhenfang Chen5 , Yikang Shen5 , and Chuang Gan1,5
1 UMass Amherst
2 Princeton University
3 South China University of Technology
4 University of California, Los Angeles
5 MIT-IBM Watson AI Lab
{junyanli,delinchen}@umass.edu
tianle.cai@princeton.edu
{phchencs,yninghong,chenzhenfang2013,yikang.shn,ganchuang1990}@gmail.com
Abstract. Current high-resolution vision-language models encode im-
ages as high-resolution image tokens and exhaustively take all these to-
kens to compute attention, which significantly increases the computa-
tional cost. To address this problem, we propose FlexAttention, a
flexibleattentionmechanismforefficienthigh-resolutionvision-language
models. Specifically, a high-resolution image is encoded both as high-
resolutiontokensandlow-resolutiontokens,whereonlythelow-resolution
tokensandafewselectedhigh-resolutiontokensareutilizedtocalculate
the attention map, which greatly shrinks the computational cost. The
high-resolution tokens are selected via a high-resolution selection mod-
ulewhichcouldretrievetokensofrelevantregionsbasedonaninputat-
tention map. The selected high-resolution tokens are then concatenated
to the low-resolution tokens and text tokens, and input to a hierarchi-
cal self-attention layer which produces an attention map that could be
used for the next-step high-resolution token selection. The hierarchical
self-attentionprocessandhigh-resolutiontokenselectionprocessareper-
formed iteratively for each attention layer. Experiments on multimodal
benchmarksprovethatourFlexAttentionoutperformsexistinghigh-
resolutionVLMs(e.g.,relatively∼9%inV*Bench,∼7%inTextVQA),
whilealsosignificantlyreducingthecomputationalcostbynearly40%.1
Keywords: High-resolutionImage·Vision-languageModel·Attention
Mechanism
1 Introduction
Large vision-language models (VLMs), such as those described in [28,30], ex-
hibitremarkablecapabilitiesacrossarangeofmultimodaltasksincludingimage
1 Project page: https://vis-www.cs.umass.edu/flexattention
4202
luJ
92
]VC.sc[
1v82202.7042:viXra2 J. Li et al.
Question：What street was this picture taken on? Answer：Darlinghurst Rd.
Detail
Blurring
VLM Downsample
VLM
Low-resolution FlexAttn
Image Computation
(a) Cost H.R. Selection
… Module
High-resolution
Image
VLM
…
High-resolution L.R. Image Tokens H.R. Image Tokens Text Tokens Output Tokens
Image
(b) (c)
Fig.1:AnoverviewofVLMsprocessinghigh-resolutionimagesfortheVQA
task.(a)low-resolutionVLMwillfirstdownsamplethehigh-resolutionimagetomeet
its vision encoder requirement. The detail in the low-resolution image is missing, thus
itishardforittocorrectlyanswerthequestion.(b)high-resolutionVLMcantakethe
high-resolution image as input, at the cost of a large amount of high-resolution image
tokens,leadingtoexcessivecomputationalcost.(c)EquippedwithourFlexAttention,
the model encodes the whole high-resolution image and dynamically selects a small
portionofthehigh-resolutionfeaturethatthemodelispayingattentiontoduringthe
generation, thus avoiding the high computational cost.
captioning,visualquestionanswering,image-textmatching,andsoon.However,
thesemodelstypicallyprocessimagesatrelativelylowresolutions(e.g.,224×224
or336×336),thusstrugglinginscenarioswheredetailedscrutinyofsmallregions
(e.g., minor texts or small objects) is required. This limitation becomes evident,
for instance, in Fig. 1 (a), where these models fail to discern the words on the
printed sign due to the constraints of low-resolution inputs.
To address this problem, several high-resolution VLMs (e.g., LLaVA-1.5-HD
[35] and CogAgent [20]) have been proposed, which could take high-resolution
images as inputs and encode them as high-resolution tokens. Although such
models provide a more detailed examination of small regions, they exhaustively
process all high-resolution tokens to compute attention, which places a heavy
burden upon computational resources. These models deviate from the way hu-
man beings perform visual reasoning. Instead of memorizing all pixel-perfect
details, we tend to maintain a coarse representation at first, and attend to rele-
vant regions for retrieval of more details only when instilled with external stim-
uli [5,41,41,42]. It’s essential that high-resolution VLMs could also flexibly and
dynamically attend to the regions of interest based on low-resolution features
for high-resolution detail retrieval.
To this end, we present FlexAttention, a novel attention mechanism that
could be seamlessly plugged into most vision-language models to empower their
abilitiestoperceiveimageswithhigherresolutionsinanefficientmanner.Specif-
ically, as is shown in Fig. 1 (c), FlexAttention takes a high-resolution im-
age as input, and encodes the image both as high-resolution image tokens and
low-resolution image tokens. For computational efficiency, we only feed the low-
resolution tokens and text tokens to the first few layers to roughly understandFlexAttention for Efficient High-Resolution Vision-Language Models 3
the entire image. For subsequent layers, only the low-resolution tokens and a
very small portion of high-resolution tokens are utilized to calculate the atten-
tion, which significantly shrinks the computational cost. At each decoder layer
withFlexAttention,wehaveahigh-resolutionfeatureselectionmoduleanda
hierarchical self-attention module. The high-resolution feature selection module
retrieves high-resolution tokens of relevant regions based on the input attention
map. The selected high-resolution tokens are concatenated to the low-resolution
tokens along with text tokens, and input to the hierarchical self-attention mod-
ule. The hierarchical self-attention module produces an attention map, which
could be used for the high-resolution token selection that selects high-resolution
tokensthatareinputtothenext-layerhierarchicalself-attention.Thetwomod-
ulesareiterativelyprocesseduntilthelastlayer,whichproducesthefinalanswer
through a projector.
We evaluate our FlexAttention on several high-resolution multimodal
benchmarks, including general benchmarks such as V* Bench [53] and Mag-
nifierbench [27], as well as domain-specific benchmarks such as TextVQA [45]
for text understanding and RSVQA [38] for remote sensing. We show a better
performancethanotherhigh-resolutionmethodswithnearly40%computational
cost reduction, proving the efficiency of our method. What’s more, we achieve a
higherscoreinV*BenchcomparedtocommercialchatbotssuchasGPT-4V[1].
2 Related Works
VisionLanguageModels.Ourworkiscloselyrelatedtotheresearchthattried
totrainlargemultimodalmodels[33,39,46,47,60,61]forvariousvisionlanguage
tasks like visual question answering [22,45], referring expression comprehen-
sion [23,57] and text-based image retrieval [32,56]. Traditional methods [19,29]
usually collected large vision-language datasets and learned joint representation
betweenvisionandlanguagefromscratchtohandledifferenttasks.Suchmodels
usuallyworkedwellinin-domaindatabutperformedinferiorinthebenchmarks
that require common sense understanding and outside world knowledge [27,40].
Later, large language models (LLMs) [6,49,50] showed impressive power in
natural language understanding and reasoning, which brought new possibilities
and capabilities to the research of vision and language. A series of large vision-
language models have been proposed, which typically connect a pre-trained vi-
sionencoder[13,44]withapre-trainedlargelanguagemodel[9,50].Flamingo[2]
first used the cross-attention mechanisms to encode the visual context into the
LLMs.BLIP2[31]proposedtheQFormer,whichusesabertmodel[24]totrans-
form the visual features into a set of learned tokens for LLMs. Fuyu [4] directly
projected the image patches into inputs for LLMs to get rid of pre-trained vi-
sionencoders.Whilethesemodelshaveimpressiveperformanceoncommonsense
understanding and perform incredibly well on traditional vision-language tasks,
they often fail to handle tasks that require high-resolution inputs [27,53] due to
two reasons: 1) most VLMs utilize CLIP [44] as their vision encoder, and this
limitstheinputimagesizeoftheseVLMstothefixedandrelativelysmallresolu-4 J. Li et al.
tionthatCLIPistrainedon(e.g.,224x224),and2)theylackmodelmechanisms
toefficientlyhandlelongimagepatchsequences,whichwillleadtotheexcessive
computational cost when the number of image patches increased quadratically
with the image resolution increased.
High-Resolution VLMs. To improve VLMs’ capability to handle inputs with
high resolutions, several VLMs have been proposed [14,20,36]. DocPedia [14]
transformed the image into a frequency domain to maintain better semantics of
the high-resolution images. LLaVA 1.6 [36] designed inputs of various scales to
meet the needs of different tasks and balance efficiency and performance. While
these models relieved the problem of dense computation, they are orthogonal
to our method and have not designed any new attention mechanisms to han-
dle the quadratic computational cost increase challenge introduced by the self-
attentionmechanism.Recently,CogAgent[20]designedanewvisionencoderfor
high-resolution image input. Different from us, it requires calculating the dense
correspondence between the hidden states and the whole high-resolution image
featurethroughcross-attentionateverylayerofthelargelanguagemodel,mak-
ingitlessefficient.Also,thedatafortrainingthemodelisnotpubliclyavailable
whileweareplanningtoreleasealldata,code,andmodelsforthewholeresearch
community.
Efficient Mechanisms for Sequence Modeling.Ourworkrelatestothede-
velopment of efficient mechanisms for sequence modeling. One approach tackles
thequadraticcomplexityofstandardattentionmechanismsconcerningsequence
length. This is done by using structured approximations [10,25,43,48,52,59] or
linearattention[7].Anotherapproachreplacesattentionentirelywithrecurrent-
style models, such as Recurrent Neural Networks (RNNs) and state-space mod-
els [17,18,21,54]. Of particular relevance is the work by Yang et al. [55], who
introduced a hierarchical attention network for document classification. Their
model uses a hierarchical structure and two-level attention mechanisms to im-
provedocumentrepresentation.Ourworkdivergesbyfocusingonefficientmech-
anisms specifically designed for high-resolution image inputs, ensuring seamless
cooperation with the computations of large language models.
3 Preliminary
Notation. We define some terms that will be used throughout the paper. For a
high-resolution vision-language model, we define its high-resolution image input
asI andthetextinputasT.Furthermore,wedefinethelow-resolutionimage
HR
tokens as f , the high-resolution image tokens as f , and the text tokens as
LR HR
f . The hidden state for the VLM is denoted as H ∈ RN×D, with a sequence
T
length of N and a hidden state size of D. The hidden state H comprises N
i
low-resolution image tokens followed by N text tokens. We define f as the
t SHR
selected subset of M high-resolution image tokens f .
HR
Autoregressive Large Language Models. Autoregressive large language
models (LLMs) such as LLaMA [50] play a crucial role in most vision-language
models as they are responsible for taking both image and text tokens as inputFlexAttention for Efficient High-Resolution Vision-Language Models 5
and generating the answer sequence. An autoregressive LLM is constituted by
several stacked decoder layers. Each decoder layer has two sub-layers. The first
is a self-attention module, and the second is a feed-forward (FFN) layer. A skip
connectionisemployedaroundeachofthetwosub-layers,followedbylayernor-
malization (LN). In short, the output of each sub-layer is LN(x+SubLayer(x)).
For simplicity, layer normalization will be omitted in the subsequent discussion.
Self-attention and Attention Map. Self-attention [51] is the basic module
for a decoder layer. For the self-attention, given input hidden state H ∈RN×D,
itwillfirstutilizealinearprojectionlayertoprojectH intoQ,K,andV,namely
the query, key, and value matrix, and performs the following calculation:
(cid:18) QKT(cid:19)
Self-attention(H)=softmax √ V, (1)
d
k
where Q = HW , K = HW , V = HW and W /W /W ∈ RD×d is the
Q K V Q K V
learnable linear projection matrix. Specifically, the attention map Map is ob-
tained after the softmax operation:
(cid:18) QKT(cid:19)
Map=softmax √ . (2)
d
k
The attention map Map is an NxN matrix that measures the importance be-
tween tokens: the (i, j) attention value in the attention map indicates the im-
portance of the j-th token to the i-th token, and a higher value means that the
j-th token is more important to the i-th token.
Limitation of Self-attention. The computational cost of the self-attention
mechanism is characterized by a quadratic increase relative to the sequence
length N of the hidden state H. This computational complexity is further am-
plified when integrating high-resolution images, as it substantially increases the
number of image tokens, consequently extending the length of the hidden state.
Asaresult,thecomputationalrequirementsoftheself-attentionmechanismun-
dergo a significant escalation, making the processing of high-resolution image
inputs impractical due to the prohibitive computational overhead.
4 Vision-language Model with FlexAttention
4.1 Overall Architecture
To solve the limitations of self-attention when dealing with high-resolution im-
ages, we introduce FlexAttention, which efficiently analyzes high-resolution
imagesbydynamicallyattendingtoimportantregionsofhigh-resolutionimages.
The FlexAttention can be plugged into most vision-language models by re-
placingtheirself-attentionmodulewithourproposedFlexAttentionmodule.
AsshowninFig.2,themodifiedvision-languagemodelconsistsofN +N
SA FA
decoder layers, where the first N layers are with the vanilla self-attention
SA6 J. Li et al.
[Output]
Projector
FFN
xNFA
FFN
FlexAttention H.R. Selection
elpm Module
asnw
Attention
oD
[Question]
FFN
xNSA
Map
What street
was this SelfAttention Hierarchical Self-Attention
picture
taken on?
Text Tokens Low-Resolution Image Tokens High-Resolution Image Tokens
Fig.2: An Overview of FlexAttention. Within each FlexAttention layer, the en-
codedhigh-resolutionimagefeaturesareselectedaccordingtotheinputattentionmap.
Theseselectedfeaturesaretheninputtedintothehierarchicalself-attentionmechanism
alongsideinputhiddenstates,whichencompassbothlow-resolutionimagetokensand
text tokens, for computation.
module and the last N layers are with our proposed FlexAttention mod-
FA
ule. Given a high-resolution image, we first downsample it to a low-resolution
one and feed both images into an image encoder to get high-resolution and low-
resolution image tokens, respectively. For computational efficiency, we only feed
thelow-resolutionimagetokensandtexttokenstothefirstN layerstoroughly
SA
understandthewholeimage.ForthesubsequentN decoderlayerswithFlex-
FA
Attention, to efficiently perceive more image details, we additionally feed it
with selected high-resolution image tokens. Specifically, FlexAttention con-
sistsoftwomodules:ahigh-resolutionfeatureselectionmoduleandahierarchical
self-attention module. Instead of feeding forward all high-resolution tokens, the
high-resolutionfeatureselectionmoduleflexiblyselectsimportanttokensforthe
next layer according to an attention map. The hierarchical self-attention mod-
ule is designed to fuse the selected high-resolution information into the original
hidden state. Finally, we use a projector linear layer to produce textual output.
4.2 High-resolution Feature Selection Module
For an autoregressive LLM, the next token is predicted by the last hidden state
of the last token. By inspecting the attention values of all other tokens cor-
responding to the last token in the attention map in Eq. 2, we can find out
which tokens the model is paying attention to when generating the next pre-
dicted token. When it comes to the vision-language model, this also applies to
imagetokensf .Thoseimagetokensthatpossessahighattentionvaluecanbe
LR
treated as relevant to important image regions when generating the next token.
Although the details contained in the low-resolution image tokens are limited,FlexAttention for Efficient High-Resolution Vision-Language Models 7
K Attention Map
High-resolution Features Selected
High-resolution
Features𝑓!"#
Reshape
Reshape
Threshold ＞T
H.R. Image Features
L.R. Image Features
Q Text Features
FeatureSelectionModule 0 1
Last Text Features
Fig.3: Illustration of high-resolution feature selection module.
wecouldretrievethehigh-resolutiondetailsofthesameimageregionsthathave
been attended to. Therefore, instead of feeding all high-resolution tokens to the
attention module which will lead to excessive computational cost, we dynam-
ically select a very small portion (approximately 10%) of the high-resolution
tokens, namely f , and only forward this portion to the attention module.
SHR
As is shown in Fig. 3, we take the first N values from the last column of
i
the attention map, which corresponds to the importance of the low-resolution
image tokens to the last text token, and reshape this 1-D vector to a 2-D map,
denotedastheattentionmask.Eachvalueinthismaskislinkedwithapatchin
the low-resolution image I , indicating that patch’s importance. The mask is
LR
normalized, binarized, and then resized to the same size as the high-resolution
feature patch tokens to form the high-resolution selection mask, which serves as
the selection decision on whether to select the token of a patch or not. Finally,
we apply this mask to the high-resolution image tokens to get the selected high-
resolution feature f .
SHR
4.3 Hierarchical Self-attention Module
The hierarchical self-attention is the core mechanism to fuse the information
from the selected high-resolution tokens f into the hidden state H which
SHR
consists of both low-resolution tokens and the text tokens. It takes the selected
high-resolution tokens f ∈ RM×D and the hidden state H ∈ RN×D as
SHR
inputs, and outputs the attention map Map′ and the updated hidden state H′.
The calculation of the hierarchical self-attention is summarized as
Q=HW , (3)
Q
K =Concat(HW ,f W′ ), (4)
all K SHR K
V =Concat(HW ,f W′ ), (5)
all V SHR V
(cid:18) QKT (cid:19)
Hierarchical Self-attention(H,f )=softmax √ all V , (6)
SHR all
d
k8 J. Li et al.
Algorithm 1 Inference Algorithm of VLM with FlexAttention.
1: Input: High-resolution Image I , Text T,
HR
2: Sub-modules:ImageEncoderE (·),TextTokenizerE (·),Self-AttentionA(·),Hi-
i t
erarchical Self-Attention HA(·), Feed-Forward Network FFN(·), Prediction Head
Head(·)
3: Parameters: #Self-Attention layer N , #FlexAttention layer N
SA FA
4: Downsample I to low-resolution image I .
HR LR
5: Generate image and text tokens f =E (I ), f0 =E (I ), f0 =E (T)
HR i HR LR i LR T t
6: H0 =Concat(f0 ,f0)
LR T
7: # Decoder layers with self-attention
8: for i = 1 ...N do
SA
9: Mapi,Hi =A(Hi−1) # Self-attention
10: Hi =Hi+Hi−1 # Skip connection
11: Hi =FFN(Hi)+Hi # FFN + skip connection
12: end for
13: # Decoder layers with FlexAttention
14: for i = N +1 ...N +N do
SA SA FA
15: fi−1 =R(f ,Mapi−1) # Select attended high-resolution feature
SHR HR
16: Mapi,Hi =HA(Hi−1,fi−1 ) # Hierarchical attention
SHR
17: Hi =Hi+Hi−1 # Skip connection
18: Hi =FFN(Hi)+Hi # FFN + skip connection
19: end for
20: Generate output tokens from Head(HNSA+NFA)
where W /W /W /W′ /W′ ∈RD×d is the learnable linear projection matrix.
Q K V K V
K ∈R(N+M)×d and V ∈R(N+M)×d are the key and value matrix that fuses
all all
the information from high-resolution features. Similar to self-attention, we can
obtain an attention map after the softmax operation:
(cid:18) QKT (cid:19)
Map′ =softmax √ all . (7)
d
k
Different from self-attention, this attention map Map′ has a shape of N ×
(N+M)asitadditionallycontainstheattentionvaluesofhigh-resolutiontokens
corresponding to other tokens. We only keep the first N ×N attention values
of the matrix shown in Eq. 7 to be the attention map Map used to select the
high-resolution feature that will be used in the next layer. A pseudo algorithm
forhowthevision-languagemodelwithourFlexAttentionworksisdescribed
in Alg. 1.
4.4 Complexity Analysis
FlexAttention offers the advantage of executing computations akin to tradi-
tional self-attention, thereby minimizing alterations to the model’s architecture
while facilitating an efficient fusion of multi-grained features. Let the length of
theselectedhigh-resolutionfeaturebeM,thelengthoftheoriginalhiddenstateFlexAttention for Efficient High-Resolution Vision-Language Models 9
be N, and the hidden state size be D. The computational complexity of our
hierarchical self-attention is
T=O((M +N)ND). (8)
Ifnotusingourhierarchicalself-attentionanddirectlyaddingthehigh-resolution
image along with the low-resolution one, the computational complexity will be
T =O((M +N)2D). (9)
original
For vanilla self-attention, the addition of an extra high-resolution feature will
lead to a quadratic increase in computation time due to the need to process a
significantly larger matrix, as every additional element in the sequence adds to
the computational load on a per-element basis. However, the hierarchical self-
attentionmechanismemployedbyFlexAttentioncleverlymitigatesthisissue
by maintaining a linear relationship in terms of the addition of high-resolution
features, thereby considerably reducing the computational burden.
5 Experiments
We evaluate FlexAttention on both high-resolution multimodal benchmarks
[27,38,45,53] and general multimodal benchmarks [3,15,22,34,37,57,58], com-
paringourmethodwiththelow-resolutionlargevision-languagemodels[8,11,28]
as well as other high-resolution methods [20,35].
5.1 Implementation
To assess the performance and efficiency of our proposed FlexAttention, we
integrated it into LLaVA-1.5-7b [35], resulting in a variant we call LLaVA-
FlexAttn. The input resolution is set to be 1008x1008, which is three times
the original input image resolution. We then compared this variant with the
original LLaVA-1.5-7b model to demonstrate the advantages of utilizing high-
resolution image inputs. We also compare FlexAttention with the methods
used in LLaVA-1.5-HD [35] and CogAgent [20] that enables the input of high-
resolutionimageinthosemodels,toshowtheefficiencyofourproposedmethod.
LLaVA-1.5-HD [35] In this model, the high-resolution image tokens act like
normaltokens.Theyareconcatenatedwiththelow-resolutionimagetokensand
are fed into the large language model together. Since this model has not been
publicly released yet, we re-implement it on top of the codebase for LLaVA-1.5.
We use the LLaVA-1.5-7b model as the base model. The input resolution of the
high-resolution image is set to 448x448 following the setting in [35]. We refer to
this baseline as LLaVA-HD.10 J. Li et al.
CogAgent [20] In this model, the high-resolution feature is perceived using a
cross-attention module. In the cross-attention module, the high-resolution fea-
tures serve as the key and value, while the hidden states, comprising both low-
resolution image tokens and text tokens, act as the query. Since CogAgent is
trained on document and GUI style data, and the data processing and training
code has not been released, for fair comparison on the effectiveness of the high-
resolution operator used in CogAgent, we transfer the cross-attention module
in CogAgent’s inference codebase to LLaVA-1.5 and re-implement the training
code. We use the LLaVA-1.5-7b model as the base model. The input resolution
of the high-resolution image is set to 1008x1008 to keep it the same as ours. We
refer to this baseline as LLaVA-XAttn.
5.2 Training Settings
For a fair comparison, both high-resolution baselines (LLaVA-HD and LLaVA-
XAttn) and our LLaVA-FlexAttn load the pre-trained weight for LLaVA-1.5-
7b as initialization, and are then finetuned on the LLaVA-1.5-7b’s finetuning
dataset for one epoch. We use a batch size of 1152 and a learning rate of 2e-5,
withacosinelearningratescheduler.Allevaluationsareperformedinazero-shot
manner.
5.3 Evaluation on High-resolution Multimodal Benchmarks
Datasets. We conduct experiments on four high-resolution benchmarks: V*
Bench [53], MagnifierBench [27], TextVQA [45] and RSVQA-HRBEN [38]. The
firsttwobenchmarksfocusonevaluatingthemodel’scapabilityongeneralhigh-
resolution VQA, while the last two benchmarks focus on evaluating the model’s
performance on domain-specific high-resolution VQA such as TextVQA for text
understanding and RSVQA-HRBEN for remote sensing.
Baselines. We conduct a comparative analysis between LLaVA-FlexAttn and
twocategoriesofVision-LanguageModels(VLMs):low-resolutionVLMs,specif-
icallyInstructBLIP[11],Otter[28],MiniGPT-4[62],MiniGPTv2[8]andLLaVA
[35],aswellashigh-resolutionVLMsthatwerere-implementedforthisresearch.
Additionally, comparisons are made with commercial chatbots such as GPT-
4V [1], and specialist VLM such as GeoChat [26], to evaluate the significance of
high-resolution image input capabilities.
Results. Table 1 shows the evaluation results on the two high-resolution gen-
eral VQA benchmarks. In general, all three high-resolution VLMs are better
than low-resolution VLMs, while our model is consistently better than other
high-resolution VLMs, with an overall accuracy of 54.5% for V* Bench and an
accuracy of 35.0% for MagnifierBench. Compared to the base model LLaVA-
1.5-7b, the overall accuracy gain for V* Bench is 6.9% and the accuracy gain
for MagnifierBench is 8.2%. Compared to other high-resolution methods, our
methodachievescomparableandevenhigheraccuracyatthecostofmuchlower
TFLOPs than other high-resolution methods, nearly 30% lower TFLOPs than
LLaVA-HD(from24.9to17.1)andover37%lowerTFLOPsthanLLaVA-XAttnFlexAttention for Efficient High-Resolution Vision-Language Models 11
V* Bench
Resolution MagnifierBench
Attribute Spatial Overall
Commercial Chatbots
Bard [16] - 31.3 46.1 37.2 -
Gemini Pro [12] - 40.9 59.2 48.2 -
GPT-4V [1] - 51.3 60.5 55.0 -
Low-resolution VLMs
InstructBLIP [11] 2242 25.2 47.4 34.0 5.6
Otter [28] 2242 27.0 56.6 38.7 25.7
MiniGPT-4 [62] 2242 30.4 50.0 38.2 22.6
LLaVA-1.5-7b [35] 3362 41.7 56.6 47.6 26.8
High-resolution VLMs
LLaVA-HD [35] 4482 45.2 61.8 51.8 35.0
LLaVA-XAttn [20] 10082 42.6 56.6 48.2 32.2
LLaVA-FlexAttn 10082 47.8 64.5 54.5 35.0
Table 1: General high-resolution VQA benchmark results comparison.
(from27.1to17.1).DetaileddiscussionontheTFLOPsandinferencetimecanbe
found in Sec. 5.6. Thanks to the high-resolution feature selection and hierarchi-
cal self-attention, our method can enable the input image resolution to increase
three times compared to the original resolution, with the cost of a sub-linear
computational cost increasing, achieving a better trade-off between computa-
tional cost and accuracy. Compared with GPT-4V on V* Bench, our method
shows competitive performance, achieving even higher accuracy on spatial cate-
gory than GPT-4V, and a comparable overall performance with GPT-4V.
Table 2 presents the results on two high-resolution domain-specific VQA
benchmarks. Our LLaVA-FlexAttn is consistently superior to the base model
and other high-resolution methods on both RSVQA-HRBEN and TextVQA.
Furthermore, our approach surpasses GeoChat [26] in terms of overall accuracy
on the RSVQA-HRBEN benchmark, a model explicitly crafted and fine-tuned
for remote sensing Visual Question Answering benchmarks. This outcome un-
derscores the efficacy of incorporating high-resolution image inputs, suggesting
that the increased detail and clarity provided by high-resolution inputs can sig-
nificantly improve the model’s understanding and processing of intricate visual
patterns in specialized VQA tasks.
5.4 Evaluation on General Multimodal Benchmarks
DatasetsandBaseline. Weevaluatethegeneralvision-languagemodelperfor-
mance on several multimodal tasks including GQA [22], VQAv2 [3], POPE [34],
RefCOCO [57], MM-Bench [37], MME [15], and MM-Vet [58]. This collection12 J. Li et al.
RSVQA-HRBEN
TextVQA
PresenceComparisonOverall
Low-resolution VLMs
GeoChat [26] 58.5 83.2 72.3 -
MiniGPTv2 [8] 40.8 50.9 46.5 27.5
LLaVA-1.5-7b [35] 69.8 67.3 68.4 46.0
High-resolution VLMs
LLaVA-HD [35] 69.0 67.6 68.4 45.6
LLaVA-XAttn [20] 71.4 70.9 71.1 45.5
LLaVA-FlexAttn 72.2 73.1 72.7 48.9
Table 2: Domain-specific high-resolution VQA benchmark results comparison.
RefCOCOPOPEGQAVQAv2MM-BenchMMEMM-Vet
LLaVA-1.5-7b [35] 75.8 85.9 62.0 78.5 64.3 1511 31.1
LLaVA-FlexAttn 79.3 85.9 62.2 78.7 65.7 1479 29.4
Table 3: Comparison of the multimodal capability between the base model and our
model on a broad range of multimodal benchmarks.
of benchmarks assesses the model’s overall capabilities, including spatial un-
derstanding, localization, ability to avoid hallucinations, and performance in
academic-oriented tasks. We compare our method to the base model LLaVA-
1.5-7b to analyze the change in the model’s general ability.
Results. In Table 3 we show that with our FlexAttention, the performance
onRefCOCOisimproved.RefCOCOrequiresthelocalizationofanobjectbased
onareferringexpression.Thus,incorporatingahigh-resolutionfeaturecouldre-
duce the challenge of identifying a small object and enhance the precision of
its location prediction. We achieve a similar rate of hallucination on POPE and
maintain similar performance on large-scale VQA benchmarks. This indicates
that incorporating FlexAttention does not impact the model’s overall capa-
bility.
5.5 Ablation Study
H.R.FeatureSelectionStrategy.Wefirstconductanablationstudytoverify
the effectiveness of the key design of our method, which is the strategy to select
high-resolutionfeaturesusingtheattentionmap.Wecompareourattentionmap
selection strategy with two naive baseline strategies: 1) random selection, which
means randomly selecting a few patches of the high-resolution features, and 2)
center selection, which means selecting the center region of the high-resolution
features. The selection ratio is kept to approximately the same as our attention
mapselectionstrategywhichisabout10%.WefinetunethemrespectivelyusingFlexAttention for Efficient High-Resolution Vision-Language Models 13
47.5
45.0
MagnifierbenchTextVQA 42.5 Magnifierbench
40.0 TextVQA
Random 31.4 44.5 37.5
Center 30.7 45.9 35.0
Attn. Map 35.0 48.9 3 s2 i.5 ze=672, size=1008, size=1344,
tflops=12.1 tflops=17.1 tflops=17.8
Fig.4: Ablation studies of selection strategies (left) and image sizes (right).
RefCOCO Val Acc. Large Small Overall
LLaVA 75.9 41.3 75.4
LLaVA-FlexAttn 78.8 (+2.9)51.3 (+10.0)78.4 (+3.0)
Table 4: Analysis on accuracy across different object sizes.
thesamefinetuningdatasetandfollowingthesametrainingsettingandevaluate
their performance on Magnifierbench and TextVQA.
The experiment results in Fig. 4 (left) shows that our attention map selec-
tion strategy is better than the other two baseline strategies. For the baseline
strategies,sincethemodelcannotdynamicallypaymoreattentiontotheregion
thatneedstobefocused,thebenefitofthehigh-resolutionimageislimited,and
no consistent improvement is observed especially for TextVQA which requires
the model to focus on a specific region to give the correct answer.
Impact of Resolution. We alsoexplore the effect of the high-resolution image
size.Thedefaultsettingis1008x1008,triplingtheresolutionoftheoriginallow-
resolution image. Additionally, we introduce two other settings: 672x672 and
1344x1344, doubling and quadrupling the original resolution, respectively. We
finetune them respectively using the same finetuning dataset and following the
same training setting. We measure their average TFLOPs on Magnifierbench
benchmarks and evaluate their performance on Magnifierbench and TextVQA.
Fig.4(right)showstheexperimentresults.Wecanseethatastheresolution
increases, the performance on Magnifierbench also increases. Performance on
TextVQA significantly enhances when the resolution is increased from 672 to
1008 but sees no further improvement from 1008 to 1344. Since the average
resolution of images in TextVQA is 950×811, further increasing the resolution
beyond its original resolution is unbeneficial. This pattern is aligned with what
we observe for general VQA benchmarks.
ImpactofObjectSize.ForgeneralbenchmarksevaluatedinSection5.4,most
questions do not focus on small details, and thus cannot reveal the capability of
our model for handling high-resolution image. To better evaluate our model on
general benchmarks, we divide the benchmark into two subsets according to the
size of question-relevant objects, categorizing those larger than 5% of the image
ecnamrofreP14 J. Li et al.
as large objects and the rest as small. We conduct experiments on RefCOCO
val set as it provides object sizes.
Table4showsthattheaccuracyimprovementonsmallobjectsismuchhigher
than large objects. It indicates that even for non high-resolution benchmarks
suchasRefCOCO,ourmethodcanstillimprovetheaccuracywhenthequestions
involve small objects or detailed information.
5.6 Inference Time on Hardware
WemeasuretheinferencetimeonhardwaretoassesstheefficiencyofourFlex-
Attention.ModelsareimplementedinPyTorchandtheinferencetimeismea-
sured on a single NVIDIA V100 32G GPU. We measure the average TFLOPs
andtotalinferencetimeontwobenchmarks:Magnifierbench,inwhichthemodel
answer is a single letter, and TextVQA, in which the model answer is a short
phrase. Warm-up before inference and CUDA synchronization are employed to
ensure the accuracy of the measurement results.
The measurement results are presented in Table 5. In Magnifierbench, the
inferencetimereductionislinearlyproportionaltothetheoreticalcomputational
costreductionmeasureinTFLOPs,andourmethodisnearly30%and40%faster
than thetwobaselines respectively.In TextVQA, thespeed superiorslightly de-
clined,butstillabout15%and25%fasterthanbaselines.Notethattheaverage
outputlengthforTextVQAislongerthanMagnifierbench,sotheinferencetime
will be affected more by the generation phase, which is memory-bound instead
of computation-bound. A discussion is provided in the Supplementary.
Magnifierbench TextVQA
TFLOPsTime(s)TFLOPsTime(s)
LLaVA-HD [35] 24.9 154 24.5 3273
LLaVA-XAttn [20] 27.1 178 26.7 3741
LLaVA-FlexAttn 17.1 112 17.1 2839
Table 5: AverageTFLOPsandtotalinferencetimemeasuredonNVIDIAV100GPU.
6 Conclusion
Inthispaper,weproposeFlexAttention,amethoddesignedtoenhancelarge
vision-languagemodelsbyallowingthemtoefficientlyprocessandderiveadvan-
tages from high-resolution image inputs. By leveraging dynamic high-resolution
feature selection and hierarchical self-attention mechanism, FlexAttention
surpasses existing high-resolution methods in terms of performance as well as
efficiency. The idea behind FlexAttention can be extended to other long se-
quencemodalitiessuchasvideooraudio,whichcanbeacrucialfuturedirection.FlexAttention for Efficient High-Resolution Vision-Language Models 15
Acknowledgments
We are grateful to the anonymous reviewers for their valuable feedback. We
also extend our thanks to AiMOS for supplying the computational resources
necessary for this project.
References
1. Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I.,Aleman,F.L.,Almeida,
D.,Altenschmidt,J.,Altman,S.,Anadkat,S.,etal.:Gpt-4technicalreport.arXiv
preprint arXiv:2303.08774 (2023)
2. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K.,
Mensch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model
for few-shot learning (2022)
3. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.:
Vqa:Visualquestionanswering.In:ProceedingsoftheIEEEinternationalconfer-
ence on computer vision. pp. 2425–2433 (2015)
4. Bavishi, R., Elsen, E., Hawthorne, C., Nye, M., Odena, A., Somani, A., Taşırlar,
S.: Fuyu-8b: A multimodal architecture for ai agents (2024)
5. Broadbent,D.E.:PerceptionandCommunication.PergamonPress(1958).https:
//doi.org/10.1037/10037-000
6. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,etal.:Languagemodelsarefew-shot
learners (2020)
7. Cai, H., Li, J., Hu, M., Gan, C., Han, S.: Efficientvit: Lightweight multi-scale
attention for high-resolution dense prediction. In: ICCV (2023)
8. Chen,J.,Zhu,D.,Shen,X.,Li,X.,Liu,Z.,Zhang,P.,Krishnamoorthi,R.,Chan-
dra, V., Xiong, Y., Elhoseiny, M.: Minigpt-v2: large language model as a unified
interface for vision-language multi-task learning. arXiv preprint arXiv:2310.09478
(2023)
9. Chiang,W.L.,Li,Z.,Lin,Z.,Sheng,Y.,Wu,Z.,Zhang,H.,Zheng,L.,Zhuang,S.,
Zhuang,Y.,Gonzalez,J.E.,Stoica,I.,Xing,E.P.:Vicuna:Anopen-sourcechatbot
impressing gpt-4 with 90%* chatgpt quality (2023), https://lmsys.org/blog/
2023-03-30-vicuna/
10. Child, R., Gray, S., Radford, A., Sutskever, I.: Generating long sequences with
sparse transformers. arXiv preprint arXiv:1904.10509 (2019)
11. Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B., Fung, P., Hoi,
S.:Instructblip:Towardsgeneral-purposevision-languagemodelswithinstruction
tuning (2023)
12. DeepMind, G.: Gemini (December 2023), https://deepmind.google/
technologies/gemini
13. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth
16x16 words: Transformers for image recognition at scale. arXiv (2020)
14. Feng,H.,Liu,Q.,Liu,H.,Zhou,W.,Li,H.,Huang,C.:Docpedia:Unleashingthe
power of large multimodal model in the frequency domain for versatile document
understanding. arXiv (2023)
15. Fu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang, J., Zheng, X., Li,
K., Sun, X., et al.: Mme: A comprehensive evaluation benchmark for multimodal
large language models. arXiv preprint arXiv:2306.13394 (2023)16 J. Li et al.
16. Google: Bard (Febraury 2023), https://bard.google.com
17. Gu, A., Dao, T.: Mamba: Linear-time sequence modeling with selective state
spaces. arXiv (2023)
18. Gu,A.,Goel,K.,Ré,C.:Efficientlymodelinglongsequenceswithstructuredstate
spaces. ICLR (2022)
19. Hao, W., Li, C., Li, X., Carin, L., Gao, J.: Towards learning a generic agent for
vision-and-language navigation via pre-training. In: CVPR (2020)
20. Hong, W., Wang, W., Lv, Q., Xu, J., Yu, W., Ji, J., Wang, Y., Wang, Z., Zhang,
Y.,Li,J.,Xu,B.,Dong,Y.,Ding,M.,Tang,J.:Cogagent:Avisuallanguagemodel
for gui agents (2023)
21. Hou,H.,Yu,F.R.:Rwkv-ts:Beyondtraditionalrecurrentneuralnetworkfortime
series tasks. arXiv preprint arXiv:2401.09093 (2024)
22. Hudson,D.A.,Manning,C.D.:Gqa:Anewdatasetforreal-worldvisualreasoning
and compositional question answering. In: CVPR (2019)
23. Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.: Referitgame: Referring to
objects in photographs of natural scenes. In: EMNLP (2014)
24. Kenton, J.D.M.W.C., Toutanova, L.K.: Bert: Pre-training of deep bidirectional
transformers for language understanding. In: NAACL (2019)
25. Kitaev,N.,Kaiser,L.,Levskaya,A.:Reformer:Theefficienttransformer.iclr(2020)
26. Kuckreja, K., Danish, M.S., Naseer, M., Das, A., Khan, S., Khan, F.S.:
Geochat:Groundedlargevision-languagemodelforremotesensing.arXivpreprint
arXiv:2311.15826 (2023)
27. Li, B., Zhang, P., Yang, J., Zhang, Y., Pu, F., Liu, Z.: Otterhd: A high-resolution
multi-modality model. arXiv (2023)
28. Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: A multi-modal
model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 (2023)
29. Li,G.,Duan,N.,Fang,Y.,Gong,M.,Jiang,D.:Unicoder-vl:Auniversalencoder
for vision and language by cross-modal pre-training. In: AAAI (2020)
30. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models (2023)
31. Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training
for unified vision-language understanding and generation. In: ICML (2022)
32. Li,W.,Duan,L.,Xu,D.,Tsang,I.W.H.:Text-basedimageretrievalusingprogres-
sive multi-instance learning. In: ICCV. IEEE (2011)
33. Li, X., Yin, X., Li, C., Hu, X., Zhang, P., Zhang, L., Wang, L., Hu, H., Dong,
L., Wei, F., Choi, Y., Gao, J.: Oscar: Object-semantics aligned pre-training for
vision-language tasks. ECCV 2020 (2020)
34. Li,Y.,Du,Y.,Zhou,K.,Wang,J.,Zhao,W.X.,Wen,J.R.:Evaluatingobjecthal-
lucinationinlargevision-languagemodels.arXivpreprintarXiv:2305.10355(2023)
35. Liu,H.,Li,C.,Li,Y.,Lee,Y.J.:Improvedbaselineswithvisualinstructiontuning
(2023)
36. Liu,H.,Li,C.,Li,Y.,Li,B.,Zhang,Y.,Shen,S.,Lee,Y.J.:Llava-next:Improved
reasoning,ocr,andworldknowledge(2024),https://llava-vl.github.io/blog/
2024-01-30-llava-next/
37. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J.,
He,C.,Liu,Z.,etal.:Mmbench:Isyourmulti-modalmodelanall-aroundplayer?
arXiv preprint arXiv:2307.06281 (2023)
38. Lobry, S., Marcos, D., Murray, J., Tuia, D.: Rsvqa: Visual question answering
for remote sensing data. IEEE Transactions on Geoscience and Remote Sensing
58(12), 8555–8566 (2020)FlexAttention for Efficient High-Resolution Vision-Language Models 17
39. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolin-
guistic representations for vision-and-language tasks. NeurIPS (2019)
40. Marino, K., Rastegari, M., Farhadi, A., Mottaghi, R.: Ok-vqa: A visual question
answering benchmark requiring external knowledge. In: CVPR (2019)
41. Oishausen, B.A., Anderson, C.H., Van Essen, D.C.: A neurobiological model of
visual attention and invariant pattern recognition based on dynamic routing of
information. The Journal of Neuroscience 13(11), 4700–4719 (November 1993)
42. Palmer, S.E.: The psychology of perceptual organization: a transformational ap-
proach. In: Beck, J., Hope, B., Rosenfeld, A. (eds.) Human and machine vision.
Academic Press, New York (1983), in: Beck, J., Hope, B. & Rosenfeld, A. (Eds.)
43. Peng,H.,Pappas,N.,Yogatama,D.,Schwartz,R.,Smith,N.A.,Kong,L.:Random
feature attention. In: 9th International Conference on Learning Representations,
ICLR2021,VirtualEvent,Austria,May3-7,2021.OpenReview.net(2021),https:
//openreview.net/forum?id=QtTKTdVrFBB
44. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML. PMLR (2021)
45. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh,
D., Rohrbach, M.: Towards vqa models that can read. In: Proceedings of the
IEEE/CVFconferenceoncomputervisionandpatternrecognition.pp.8317–8326
(2019)
46. Sun, C., Myers, A., Vondrick, C., Murphy, K., Schmid, C.: Videobert: A joint
model for video and language representation learning. In: ICCV (2019)
47. Tan,H.,Bansal,M.:Lxmert:Learningcross-modalityencoderrepresentationsfrom
transformers. arXiv (2019)
48. Tay, Y., Bahri, D., Metzler, D., Juan, D.C., Zhao, Z., Zheng, C.: Synthesizer:
Rethinkingself-attentionintransformermodels.arXivpreprintarXiv:2005.00743
(2020)
49. Team,G.,Anil,R.,Borgeaud,S.,Wu,Y.,Alayrac,J.B.,Yu,J.,Soricut,R.,Schalk-
wyk,J.,Dai,A.M.,Hauth,A.,etal.:Gemini:afamilyofhighlycapablemultimodal
models. arXiv (2023)
50. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
lykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)
51. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
Ł., Polosukhin, I.: Attention is all you need. Advances in neural information pro-
cessing systems 30 (2017)
52. Wang, S., Li, B.Z., Khabsa, M., Fang, H., Ma, H.: Linformer: Self-attention with
linear complexity. arXiv preprint arXiv: Arxiv-2006.04768 (2020)
53. Wu,P.,Xie,S.:V*:Guidedvisualsearchasacoremechanisminmultimodalllms.
arXiv preprint arXiv:2312.14135 17 (2023)
54. Yang, S., Wang, B., Shen, Y., Panda, R., Kim, Y.: Gated linear attention trans-
formers with hardware-efficient training. arXiv preprint arXiv: 2312.06635 (2023)
55. Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., Hovy, E.: Hierarchical attention
networks for document classification. In: NAACL (2016)
56. Young,P.,Lai,A.,Hodosh,M.,Hockenmaier,J.:Fromimagedescriptionstovisual
denotations:Newsimilaritymetricsforsemanticinferenceovereventdescriptions.
TACL (2014)
57. Yu,L.,Poirson,P.,Yang,S.,Berg,A.C.,Berg,T.L.:Modelingcontextinreferring
expressions. In: ECCV (2016)18 J. Li et al.
58. Yu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang, X., Wang, L.: Mm-
vet:Evaluatinglargemultimodalmodelsforintegratedcapabilities.arXivpreprint
arXiv:2308.02490 (2023)
59. Zaheer, M., Guruganesh, G., Dubey, K.A., Ainslie, J., Alberti, C., Ontañón, S.,
Pham,P.,Ravula,A.,Wang,Q.,Yang,L.,Ahmed,A.:Bigbird:Transformersfor
longer sequences. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., Lin,
H. (eds.) Advances in Neural Information Processing Systems 33: Annual Confer-
ence on Neural Information Processing Systems 2020, NeurIPS 2020, December
6-12, 2020, virtual (2020), https://proceedings.neurips.cc/paper/2020/hash/
c8512d142a2d849725f31a9a7a361ab9-Abstract.html
60. Zhang, P., Li, X., Hu, X., Yang, J., Zhang, L., Wang, L., Choi, Y., Gao, J.:
Vinvl: Making visual representations matter in vision-language models. CVPR
2021 (2021)
61. Zhou,L.,Palangi,H.,Zhang,L.,Hu,H.,Corso,J.,Gao,J.:Unifiedvision-language
pre-training for image captioning and vqa. In: AAAI (2020)
62. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-
language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592 (2023)