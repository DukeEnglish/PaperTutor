SAPG: Split and Aggregate Policy Gradients
JayeshSingla*1 AnanyeAgarwal*1 DeepakPathak1
CarnegieMellonUniversity
Regular On-policy RL
Regular On-policy RL
Parallel
EnPavrsallel
Envs
SiSnignleg lleea lrenaerrner
SplSitp alint adn Ad gAggrgereggaattee PPoolliiccyy G Grardaideinetns t(Ss A(SPGA)PG)
Followers
Followers
Leader
Leader
Figure1.Weintroduceanewclassofon-policyRLalgorithmsthatcanscaletotensofthousandsofparallelenvironments.Incontrastto
regularon-policyRL,suchasPPO,whichlearnsasinglepolicyacrossenvironmentsleadingtowastedenvironmentcapacity,ourmethod
learnsdiversefollowersandcombinesdatafromthemtolearnamoreoptimalleaderinacontinuousonlinemanner.
Abstract 1.Introduction
Broadly, there are two main categories in reinforcement
learning(RL):off-policyRL,e.g.,Q-learning[32],andon-
Despiteextremesampleinefficiency,on-policyre-
policyRL,e.g.,policygradients[29]. On-policymethods
inforcementlearning,akapolicygradients,hasbe-
are relatively more sample inefficient than off-policy but
comeafundamentaltoolindecision-makingprob-
oftentendtoconvergetohigherasymptoticperformance.
lems. With the recent advances in GPU-driven
Duetothisreason,on-policyRLmethods,especiallyPPO
simulation, the ability to collect large amounts
[27],areusuallythepreferredRLparadigmforalmostall
ofdataforRLtraininghasscaledexponentially.
sim2real robotic applications [19, 1, 2] to games such as
However,weshowthatcurrentRLmethods,e.g.
StarCraft[30],whereonecouldsimulateyearsofreal-world
PPO,failtoingestthebenefitofparallelizedenvi-
experienceinminutestohours.
ronmentsbeyondacertainpointandtheirperfor-
mance saturates. To address this, we propose RLisfundamentallyatrial-n-error-basedframeworkand
a new on-policy RL algorithm that can effec- henceissampleinefficientinnature. Duetothis,oneneeds
tivelyleveragelarge-scaleenvironmentsbysplit- tohavelargebatchsizesforeachpolicyupdate,especially
ting them into chunks and fusing them back to- inthecaseofon-policymethodsbecausetheycanonlyuse
getherviaimportancesampling. Ouralgorithm, datafromcurrentexperience. Fortunately,inrecentyears,
termedSAPG,showssignificantlyhigherperfor- theabilitytosimulatealargenumberofenvironmentsin
mance across a variety of challenging environ- parallelhasbecomeexponentiallylargerduetoGPU-driven
mentswherevanillaPPOandotherstrongbase- physicsengines,suchasIsaacGym[17],PhysX,Mujoco-
linesfailtoachievehighperformance. Webpage 3.0,etc. ThismeansthateachRLupdatecaneasilyscale
athttps://sapg-rl.github.io. tobatchesofsizehundredsofthousandstomillions,which
areovertwoordersofmagnitudehigherthanwhatmostRL
1
4202
luJ
92
]GL.sc[
1v03202.7042:viXraSAPG:SplitandAggregatePolicyGradients
Shadow Hand Allegro Kuka Throw by employing a baseline to decrease the variance of the
estimator while not compromising on the bias. [26, 27]
12000
20 incorporateconservativepolicyupdatesintopolicygradients
10000
toincreasetherobustness.
8000 15
6000 10
4000 Distributed reinforcement learning Reinforcement
5
2000 learning algorithms are highly sample inefficient, which
0 0 callsforsomeformofparallelizationtoincreasethetrain-
0 250005000075000100000 0 250005000075000100000 ingspeed.Whentraininginsimulation,thisspeed-upcanbe
Batch size Batch size
achievedbydistributingexperiencecollectionordifferent
Figure2.PerformancevsbatchsizeplotforPPOruns(bluecurve)
parts of training across multiple processes.[22, 21, 6, 12]
acrosstwoenvironments.ThecurveshowshowPPOtrainingruns
cannottakebenefitoflargebatchsizeresultingfrommassively However,throughtheintroductionofGPU-basedsimula-
parallelizedenvironmentsandtheirasymptoticperformancesatu- torssuchasIsaacGym[17],thecapacityofsimulationhas
ratesafteracertainpoint.Thedashedredlineistheperformance increasedbytwotothreeordersofmagnitude. Duetothis,
of our method, SAPG, with more details in the results section. insteadoffocusingonhowtoparallelizepartsofthealgo-
Itservesasevidencethathigherperformanceisachievablewith rithm, the focus has shifted to finding ways to efficiently
largerbatchsizes. utilizethelargeamountofsimulationdata. Previousworks
suchas[1,2,8,25,10]usedatafromGPU-basedsimulation
benchmarkstypicallyhave. tolearnpoliciesincomplexmanipulationandlocomotion
settings. However,mostoftheseworksstillusereinforce-
Inthispaper,wehighlightanissuewithtypicalon-policy
ment learning algorithms to learn a single policy, while
RLmethods,e.g. PPO,thattheyarenotabletoingestthe
augmentingtrainingwithtechniquesliketeacher-student-
benefitswithincreasinglylargersamplesizesforeachup-
based training and game-based curriculum. We find that
date. InFigure2,weshowthatPPOperformancesaturates
usingtheincreasedsimulationcapacitytonaivelyincrease
after a certain batch size despite the ceiling being higher.
thebatchsizeisnotthebestwaytoutilizemassivelyparallel
Thisisduetotheissueindatasamplingmechanisms. In
simulation.
particular,ateachtimestepactionsaresampledfromaGaus-
sianwithsomemeanandvariance. Thisimpliesthatmost [24] develop a population-based training framework that
sampledactionsarenearthemeanandwithlargenumberof dividesthelargenumberofenvironmentsbetweenmultiple
environments,manyenvironmentsareexecutingthesame policies and using hyperparameter mutation to find a set
actions leading to duplicated data. This implies that the ofhyperparametersthatperformswell. However,eventhis
performanceofPPOsaturatesatsomepointasweincrease doesnotutilizeallthedatacompletelyaseachpolicylearns
thenumberofenvironments. independently. Weproposeawaytoensuremostofthedata
fromtheenvironmentscontributestolearningbyusingall
Weproposeasimplefixtothisproblem. Insteadofrunning
collectedtransitionsforeachupdate.
asinglePPOpolicyforallenvironments,wedivideenviron-
mentsintoblocks. Eachblockoptimizesaseparatepolicy,
allowing for more data diversity than just i.i.d. sampling Off-policyPolicyGradients Unlikeon-policyalgorithms,
fromthesameGaussian. Next,wedoanoff-policyupdate off-policy algorithms can reuse all collected data or data
tocombinedatafromallthesepoliciestokeeptheupdate collected by any policy for their update. Most off-policy
consistentwiththeobjectiveofon-policyRL.Thisallows algorithms[20,16,9]trytolearnavaluefunctionwhich
ustousethePPO’sclippedsurrogateobjective,maintaining is then implicitly/explicitly used to learn a policy. [15]
thestabilitybenefitsofPPOwhilelatchingontohighreward developed a variant of Deep Deterministic Policy Gradi-
trajectorieseventhoughtheyareoff-policy. Aschematic ent (DDPG) called PQL which splits data collection and
ofourapproach,termedSAPG,isshowninFigure1. We learningintomultipleprocessesandshowsimpressiveper-
evaluateSAPGacrossavarietyofenvironmentsandshow formanceonmanybenchmarktasks. WeusePQLasone
significantlyhighasymptoticperformanceinenvironments ofourbaselinestocompareourmethodtooff-policyRLin
wherevanillaPPOevenfailstogetanypositivesuccess. complextasks. Althoughoff-policyalgorithmsaremuch
moredata-efficient,theyusuallygetlowerasymptoticper-
2.RelatedWork formancethanon-policypolicygradients. Thishasinspired
works to develop techniques to use off-policy data in on-
Policygradients REINFORCE[33],oneoftheearliest policymethods. [11]hasbeenoneofthemajortechniques
policygradientalgorithmsusesanestimatoroftheobjective used to realize this. Previous works [5, 31, 6, 7] develop
usingsimpleMonteCarloreturnvalues. Workssuchas[14] techniquestouseoff-policydatainon-policyalgorithmsus-
and[28]improvethestabilityofpolicygradientalgorithms ingimportancesampling-basedupdatesalongwithfeatures
2
ecnamrofrep
citotpmysA
ecnamrofrep
citotpmysASAPG:SplitandAggregatePolicyGradients
suchasbiascorrection. 4.SplitandAggregatePolicyGradients
Policygradientmethodsarehighlysensitivetothevariance
3.Preliminaries
in the estimate of gradient. Since CPU-based simulators
typicallyrunonly100sofenvironmentsinparallel,conven-
Inthispaper,weproposeamodificationtoon-policyRLto
tional wisdom is to simply sample on-policy data from a
achievehigherperformanceinthepresenceoflargebatch
Gaussianpolicyinalltheenvironmentssinceasthenumber
sizes. WebuilduponPPO,althoughourproposedideasare
ofdatapointsincreases,theMonteCarloestimatebecomes
generallyapplicabletoanyon-policyRLmethod.
moreaccurate. However,thisintuitionnolongerholdsin
theextremelylarge-scaledatasettingwherewehavehun-
On-policy RL Let (S,A,P,r,ρ,γ) be an MDP where
dreds of thousands of environments on GPU-accelerated
S is the set of states, A the set of actions, P are transi-
simulatorslikeIsaacGym. IIDsamplingfromaGaussian
tion probabilities, r the reward function, ρ the initial dis-
policywillleadtomostactionslyingnearthemean, and
tribution of states and γ the discount factor. The objec-
mostenvironmentswillexecutesimilaractions,leadingto
tive in reinforcement learning is to find a policy π(a|s)
wasteddata(fig.2).
whichmaximisesthelongtermdiscountedrewardJ(π)=
(cid:104) (cid:105)
E (cid:80)T−1γtr(s ,a ) . WeproposetoefficientlyuselargenumbersofN environ-
s0∼ρ,at∼π(·|st) t=0 t t
ments using a divide-and-conquer setup. Our algorithm
Policy-gradient algorithms [33, 14, 26, 21] optimize the trainsavarietyofM policiesπ ,...,π insteadofhaving
1 M
policyusinggradientdescentwithMonteCarloestimates justonepolicy. However,simplytrainingmultiplepolicies
ofthegradient bydividingenvironmentsbetweenthemisalsoinefficient.
(cid:104) (cid:105) This is equivalent to training an algorithm with different
∇ θJ(π θ)= E ∇ θlog(π θ(a))Aˆπθ(s,a) (1)
seedsandchoosingthebestseed. Oneapproachistoadd
s∼ρd,a∼π(·|s)
hyperparametermutation[24]tothepoliciesandchoosing
where Aˆπθ(s,a) is an advantage function that estimates thehyperparametersthatperformthebestamongallofthem.
However,eveninthiscase,allofthedatafromthe“worse”
the contribution of the transition to the gradient. A com-
mon choice is Aˆπθ(s,a) = Qˆπθ(s,a) − Vˆπθ(s), where policiesgoestowaste,andtheonlyinformationgainedis
Qˆπθ(s,a), Vˆπθ(s) are estimated Q and value functions. thatsomecombinationsofhyperparametersarebad,even
thoughthepoliciesthemselvesmayhavediscoveredhigh
Thisformofupdateistermedasanactor-criticupdate[14].
reward trajectories. We need to somehow aggregate data
Sincewewantthegradientoftheerrorwithrespecttothe
frommultiplepoliciesintoasingleupdate. Weproposeto
currentpolicy,onlydatafromthecurrentpolicy(on-policy)
dothisviaoff-policyupdates.
datacanbeutilized.
PPO Actorcriticupdatescanbequiteunstablebecause 4.1.Aggregatingdatausingoff-policyupdates
gradientestimatesarehighvarianceandthelosslandscapeis
Oneofthemajordrawbacksofon-policyRLisitsinability
complex. Anupdatestepthatistoolargecandestroypolicy
tousedatafrompastversionsofthepolicy. Onesolutionis
performance. ProximalPolicyOptimization(PPO)modifies
touseimportancesampling[5,18]toweightupdatesusing
Eq.1torestrictupdatestoremainwithinanapproximate
data from different policies. In practice, this is not used
“trustregion”wherethereisguaranteedimprovement[26,
sincegivenlimitedcomputeitisbeneficialtosampleon-
13].
policyexperiencethatismoredirectlyrelevant. However,
thisisnolongertrueinthelargebatchsettingwhereenough
on-policydataisavailable. Inthiscase,itbecomesadvanta-
L (π )= E [min(r (π ),
on θ πold t θ (2) geoustohavemultiplepoliciesπ 1,...,π M andusethemto
clip(r (π ),1−ϵ,1+ϵ))Aπold] samplediversedata,evenifitisoff-policy. Inparticular,to
t θ t
updatepolicyπ usingdatafrompolicyπ ,j ∈X weuse
i j
[18]
Here,r (π ) = πθ(at|st) ,ϵisaclippinghyperparameter
and π t isθ the pπ oo ll id c( yat| cs ot) llecting the on-policy data. The 1 (cid:88)
old L (π ;X)= E [min(r (s,a),
clipping operation ensures that the updated π stays close off i |X| j∈X(s,a)∼πj πi (3)
toπ . Empirically,givenlargenumbersofsamples,PPO
achio el vd
es high performance, is stable and robust to hyper-
clip(r πi(s,a),µ(1−ϵ),µ(1+ϵ)))Aπi,old(s,a)]
parameters. However,itwasdevelopedforrelativelysmall
wherer (s,a)= πi(s,a) andµisanoff-policycorrection
batchsizes(≈100parallelenvs). Wefindthatinthelarge- πi πj(s,a)
scale setting (>10k envs), it is suboptimal because many termµ= πi π,o jl (d s( ,s a, )a).Notethatwheni=j,thenπ j =π i,old
parallelenvsaresamplingnearlyidenticalon-policydata. andthisreducestotheon-policyupdateasexpected. This
3SAPG:SplitandAggregatePolicyGradients
Massively parallel simulation
Importance
Sampling
Loss
Figure3.WeillustrateoneparticularvariantofSAPGwhichperformswell.ThereisoneleaderandM −1followers(M =3infigure).
EachpolicyhasthesamebackbonewithsharedparametersB butisconditionedonlocallearnedparametersϕ . Eachpolicygetsa
θ i
blockof N environmentstorun.Theleaderisupdatedwithitson-policydataaswellasimportance-sampledoff-policydatafromthe
M
followers.Eachofthefollowersonlyusestheirowndataforon-policyupdates.
isthenscaledandcombinedwiththeon-policyterm(eq.2) gradients from off-policy data are typically noisier than
gradientsfromon-policydata,wechooseλ = 1,butsub-
L(π i)=L on(π i)+λ·L off(π i;X) (4) sampletheoff-policydatasuchthatweuseequalamounts
ofon-policyandoff-policydata.
The update target for the critic is calculated using n-step
returns(heren=3).
4.3.Leader-followeraggregation
t+2
Vtarget(s )=(cid:88) γk−tr +γ3V (s ) (5) While the above choice prevents data wastage, since all
on,πj t k πj,old t+3
the policies are updated with the same data, it can lead
k=t
topoliciesconverginginbehavior,reducingdatadiversity
However,thisisnotpossibleforoff-policydata.Instead,we anddefeatingthepurposeofhavingseparatepolicies. To
assumethatanoff-policytransitioncanbeusedtoapprox- resolvethis,webreaksymmetrybydesignatinga“leader”
imatea1-stepreturn. Thetargetequationsareasfollows policyi = 1whichgetsdatafromallotherpoliciesX =
- {2,3,...,M}whiletherestare“followers”andonlyuse
V ot fa fr ,g πe jt(s′ t)=r t+γV πj,old(s′ t+1) (6) their own on-policy data for updates X = ϕ. As before,
we choose λ = 1, but subsample the off-policy data for
Thecriticlossisthen
theleadersuchthatweuseequalamountsofon-policyand
Lcritic(π )= E (cid:2) (V (s)−Vtarget(s))2(cid:3) (7) off-policydatainamini-batchupdate.
on i
(s,a)∼πi
πi on,πi
1 (cid:88) (cid:104) (cid:105) 4.4.Encouragingdiversityvialatentconditioning
Lcritic(π ;X)= E (V (s)−Vtarget(s))2
off i |X|
j∈X(s,a)∼πj
πi off,πi
Whatistherightparameterizationforthissetofpolicies?
(8) One simple choice is to have a disjoint set of parameters
Lcritic(π )=Lcritic(π )+λ·Lcritic(π ) (9) foreachwithnosharingatall. However,thisimpliesthat
i on i off i
each follower policy has no knowledge of any other pol-
Giventhisupdatescheme,wemustnowchooseasuitable
icywhatsoeverandmaygetstuckinabadlocaloptimum.
X ⊆{1,...,M}andthesetofistoupdate,alongwiththe
WemitigatethisbyhavingasharedbackboneB foreach
correctratioλ. Weexploreseveralvariantsbelow. θ
policyconditionedonhangingparametersϕ localtoeach
j
policy. Similarly,thecriticconsistsofasharedbackbone
4.2.Symmetricaggregation
C conditioned on parameters ϕ . The parameters ψ,θ
ψ j
Asimplechoiceistoupdatealli′swiththedatafromall aresharedacrosstheleaderandallfollowersandupdated
policies. In this case, we choose to update each policy with gradients from each objective, while the parameters
i ∈ {1,2,...,M}andforeachiuseoff-policydatafrom ϕ j are only updated with the objective for that particular
all other policies X = {1,2,i−1,i+1,...,M}. Since policy. We choose ϕ j ∈ R32 for complex environments
4
redaeL
srewolloFSAPG:SplitandAggregatePolicyGradients
whileϕ ∈R16fortherelativelysimplerones. Algorithm1SAPG
j
Initializesharedparametersθ,ψ
4.5.Enforcingdiversitythroughentropyregularization Fori∈{1,...,M}initializeparametersϕ
i
InitializeN environmentsE ,...,E .
Tofurtherencouragediversitybetweendifferentpolicies, 1 N
InitializedatabuffersforeachpolicyD ,...,D .
inadditiontothePPOupdatelossL weaddanentropy 1 M
on
fori=1,2,...,do
losstoeachofthefollowerswithdifferentcoefficients. In
forj =1,2,...,M do
particular, the entropy loss is H(π(a | s)). The overall (cid:16) (cid:17)
lossforthepolicyi(orthe(i−1)th)followerisL(π i)= D j ←CollectData E j MN:(j+1) MN,θ,ψ j
L (π )+λ (i−1)·H(π(a|s)).Theleaderdoesn’thave endfor
on i ent
anyentropyloss. Differentscalesofcoefficientsproduce L←0
policieswithdifferentexplore-exploittradeoffs. Followers Sample|D |transitionsfrom∪M D togetD′.
1 j=2 i 1
withlargeentropylossestendtoexploremoreactionseven L←L+OffPolicyLoss(D′)
1
iftheyaresuboptimal,whilethosewithsmallcoefficients L←L+OnPolicyLoss(D )
1
stayclosetooptimaltrajectoriesandrefinethem. Thisleads forj =2,...,M do
toalargedatacoveragewithagoodmixofoptimalaswell L←L+OnPolicyLoss(D )
j
asdiversetrajectories. Wetreatλ asahyperparameter. endfor
ent
Updateθ ←θ−η∇ L
θ
Updateψ ←ψ−η∇ L
ψ
endfor
Fortesting,wechooseasuiteofmanipulationenvironments
thatarechallengingandrequirelarge-scaledatatolearnef-
fectivepolicies[24].Inparticular,theseconsistofdexterous
handsmountedonarmsleadingtohighnumbersofdegrees
Leader-follower Symmetric
offreedom(upto23). Thisischallengingbecausesample
complexityscalesexponentiallywithdegreesoffreedom.
Figure4.Twodataaggregationschemesweconsiderinthispaper.
Theyareunder-actuatedandinvolvemanipulatingfreeob-
(Left)onepolicyisaleaderandusesdatafromeachofthefollow-
jectsincertainwayswhileundertheinfluenceofgravity.
ers(Right)asymmetricschemewhereeachpolicyusesdatafrom
allothers.Ineachcase,thepolicyalsousesitsownon-policydata. Thisleadstocomplex,non-linearinteractionsbetweenthe
agentandtheenvironmentsuchascontactsbetweenrobot
4.6.Algorithm: SAPG andobject,objectandtable,androbotandtable. Overall,
this implies that to learn effective policies an agent must
We roll out M different policies and collect data
collectalargeamountofrelevantexperienceandalsouseit
D ,...,D foreach. Followerpolicies2,...,M areup-
1 M efficientlyforlearning.
datedusingtheusualPPOobjectivewithminibatchgradient
descentontheirrespectivedatasets. However,weaugment
5.1.Tasks
the dataset of the leader D with data from D ,...,D ,
1 2 M
weighed by the importance weight µ. The leader is then Weconsideratotalof6tasksgroupedintotwoparts: Four
updatedbyminibatchgradientdescentaswell. hard tasks and two easy tasks. Hard and easy is defined
bythesuccessrewardachievedbyoff-policy(inparticular,
5.ExperimentalSetup PQL)methodsintheseenvironments. Ineasyenvironments,
evenQ-learning-basedoff-policymethodscanobtainnon-
Weconductexperimentson5manipulationtasks(3hardand zeroperformancebutnotinhardtasks. Seeappendixsec.A.
2easy)andcomparethemagainstSOTAmethodsforthe
large-scaleparallelizedsetting. WeuseaGPU-accelerated
HardDifficultyTasks Allfourhardtasksarebasedon
simulator,IsaacGym[17]whichallowssimulatingtensof
the Allegro-Kuka environments [24]. These consist
thousands of environments in parallel on a single GPU.
ofanAllegroHand(16DoF)mountedonaKukaarm(7
Inourexperiments,wefocusonthelarge-scalesettingand
dof). Theperformanceoftheagentintheabovethreetasks
simulate24576parallelenvironmentsunlessotherwisespec-
is measured by the successes metric which is defined as
ified. Notethatthisistwoordersofmagnitudelargerthan
the number of successes in a single episode. Three tasks
thenumberofenvironmentsPPO[27]wasdevelopedon,
include:
andweindeedfindthatvanillaPPOdoesnotscaletothis
setting. • Regrasping: The object must be lifted from the table
5SAPG:SplitandAggregatePolicyGradients
andheldnearagoalpositiong ∈R3 forK =30steps. with the weights of best-performing policies and their
t
Thisiscalleda“success”. Thetargetpositionandobject hyperparametersaremutatedrandomly.
positionareresettoarandomlocationaftereverysuccess.
Duetothecomplexityofthesetasks,experimentstakeabout
• Throw: The object must be lifted from the table and
48-60hoursonasingleGPU,collecting≈2e10transitions.
thrownintoabucketatg ∈ R3 placedoutofreachof
t Sincewerunexperimentsondifferentmachines,thewall
the arm. The bucket and the object position are reset
clocktimeisnotdirectlycomparableandwecompareruns
randomlyaftereverysuccessfulattempt.
againstthenumberofsamplescollected. Werun5seedsfor
• Reorientation: Pickuptheobjectandreorientittoapar-
eachexperimentandreportthemeanandstandarderrorin
ticulartargetposeg t ∈R7(position+orientation). The theplots. Ineachplot,thesolidlineisy(t) = 1 (cid:80) y (t)
targetposeisresetoncetheagentsucceeds. Thismeans n i i
whilethewidthoftheshadedregionisdeterminedbystan-
t ph oa st esth ie na sug ce cn ets ssn ioe ne ,d ws hto ichre mor aie yn st ot mhe etio mb eje sc et ni tn aid li pf lf ae cr ie nn gt darderror √2 n(cid:80) i(y(t)−y i(t))2.
theobjectsonthetableandliftingitupinadifferentway. For each task, we use M = 6 policies for our method
• TwoArmsReorientation: Similartothereorientation and DexPBT in a total of N = 24576 environments for
taskabove,pickuptheobjectandreorientittoaparticular eachmethod. Weusethedimensionoflearnedparameter
targetpose. However,therearetwoarmsinthesystem, ϕ ∈R32fortheAllegroKukataskswhileweuseϕ ∈R16
j j
adding the additional complexity of having to transfer fortheShadowHandandAllegroHandtaskssincetheyare
objectsbetweenarmstoreachposesindifferentregions relativelysimpler. WeusearecurrentpolicyfortheAlle-
ofspace. groKuka tasks and an MLP policy for the Shadow Hand
and Allegro Hand tasks and use PPO to train them. We
collect16stepsofexperienceperinstanceoftheenviron-
EasyDifficultyTasks: Inaddition,wetestonthefollow- ment before every PPO update step. For SAPG, we tune
ingdexteroushandtasks. Asbefore,theobservationspace theentropycoefficientσbychoosingthebestfromasmall
consistsofthejointanglesandvelocitiesq t,q˙ t,objectpose set {0,0.003,0.005} for each environment. We find that
x tandvelocitiesv t,ω t. σ = 0 works best for all AllegroHand, Regrasping, and
Throwwhileσ =0.005worksbetterforShadowHandand
• ShadowHand: Wetestin-handreorientationtaskofa
Reorientation.
cubeusingthe24-DoFShadowHand.
• AllegroHand: Thisisthesameasthepreviousin-hand
reorientationtaskbutwiththe16-DoFAllegroHand. 6.ResultsandAnalysis
Inthelarge-scaledatasetting,weareprimarilyconcerned
5.2.Baselines
withoptimalitywhilesample-efficiencyandwall-clocktime
We test against state-of-the-art RL methods designed for are secondary concerns. This is because data is readily
theGPU-acceleratedlarge-scalesettingweconsiderinthis available—one only needs to spin up more GPUs, what
paper. We compare against both on-policy [24] and off- is really important is how well our agent performs in the
policy[15]variantsaswellasvanillaPPO[27]. downstreamtasks.Indeed,thisalignswithhowpractitioners
useRLalgorithmsinpractice[1,3,23],whereagentsare
• PPO (Proximal Policy Optimization) [27]: In our set-
trained with lots of domain randomization in large simu-
ting, we just increase the data throughput for PPO by
lationsandtheprimaryconcernishowwelltheagentcan
increasingthebatchsizeproportionatelytothenumber
adapt and learn in these environments since this directly
ofenvironments. Inparticular,weseeovertwoordersof
translatestoreal-worldperformance.
magnitudeincreaseinthenumberofenvironments(from
128to24576).
• Parallel Q-Learning [15] A parallelized version of 6.1.AllegroKukatasks
DDPGwithdifferentmixedexplorationi.e.varyingexplo-
TheAllegroKukatasks(Throw,Regrasping,Reorientation,
rationnoiseacrossenvironmentstofurtheraidexploration.
TwoArmsReorientation)arehardduetolargedegreesof
Weusethisbaselinetocompareifoff-policymethodscan
freedom. The environment also offers the possibility of
outperformon-policymethodswhenthedatacollection
manyemergentstrategiessuchasusingthetabletoreorient
capacityishigh.
thecube,orusinggravitytoreorientthecube. Therefore,
• DexPBT [24] A framework that combines population-
a large amount of data is required to attain good perfor-
based training with PPO. N Environments are divided
manceonthesetasks. FollowingPetrenkoetal.[24]weuse
into M groups, each containing N environments. M
M thenumberofsuccessesasaperformancemetriconthese
separatepoliciesaretrainedusingPPOineachgroupof
tasks. Note that the DexPBT baseline directly optimizes
environments with different hyperparameters. At regu-
forsuccessbymutatingtherewardscalestoachievehigher
larintervals,theworst-performingpoliciesarereplaced
6SAPG:SplitandAggregatePolicyGradients
Allegro Kuka Regrasping Allegro Kuka Throw Allegro Kuka Reorientation
40
25
30 20 30
20 15 20
10
10 10
5
0 0 0
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Number of envsteps 1e10 Number of envsteps 1e10 Number of envsteps 1e10
Allegro Kuka Two Arms Reorientation Allegro Hand Shadow Hand
30
12500 12500
10000 10000
20
7500 7500
10 5000 5000
2500 2500
0 0 0
1 0 1 2 3 4 5 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Number of envsteps 1e10 Number of envsteps 1e10 Number of envsteps 1e10
SAPG (Ours) PPO PBT PQL
Figure5.PerformancecurvesofSAPGwithrespecttoPPO,PBTandPQLbaselines.OnAllegroKukatasks,PPOandPQLbarelymake
progressandSAPGbeatsPBT.OnShadowHandandAllegroKukaReorientatioandTwoArmsReorientation,SAPGperformsbestwith
anentropycoefficientof0.005whilethecoefficientis0forotherenvironments.OnShadowHandandAllegroHand,whilePQLisinitially
moresampleefficient,SAPGismoreperformantinthelongerrun.AllegroKukaenvironmentsusesuccessesasaperformancemetric
whileAllegroHandandShadowHanduseepisoderewards.
TASK PPO[27] PBT[24] PQL[15] SAPG SAPG
(λ =0) (λ =0.005)
ENT ENT
ALLEGROHAND 1.01e4±6.31e2 7.28e3±1.24e3 1.01e4±5.28e2 1.23e4±3.29e2 9.14e3±8.38e2
SHADOWHAND 1.07e4±4.90e2 1.01e4±1.80e2 1.28e4±1.25e2 1.17e4±2.64e2 1.28e4±2.80e2
REGRASPING 1.25±1.15 31.9±2.26 2.73±0.02 35.7±1.46 33.4±2.25
THROW 16.8±0.48 19.2±1.07 2.62±0.08 23.7±0.74 18.7±0.43
REORIENTATION 2.85±0.05 23.2±4.86 1.66±0.11 33.2±4.20 38.6±0.63
TWOARMSREORIENTATION 1.73±0.51 14.46±2.91 - - 28.58±1.55
Table1.Performanceafter2e10samplesfordifferentmethodswithstandarderror.ThisismeasuredbysuccessesfortheAllegroKuka
tasksandbyepisoderewardsforin-handreorientationtasks.Acrossenvironments,wefindthatourmethodperformsbetterthanbaselines.
successrate,whereasourmethodcanonlyoptimizeafixed progress. In particular, we find that PQL is very sample-
rewardfunction. Despitethis,weseethatSAPGachieves efficientbecauseitisoff-policyandutilizespastdataforup-
a12−66%highersuccessratethanDexPBTonregrasp- dates. However,wefindthatSAPGachieveshigherasymp-
ing, throwandreorientation. SAPGperforms66%better toticperformance. Thisisbecauseon-policymethodsare
thanPBTonthechallengingreorientationtask. SAPGfairs betteratlatchingontohighrewardtrajectoriesanddonot
even better on the two-arm reorientation, obtaining more have to wait several iterations for the Bellman backup to
thantwicethenumberofsuccessesonaveragecompared propagate back to initial states. As discussed previously,
toPBT.NotethatvanillaPPOandPQLareunabletolearn inlarge-scalesettingsinsimulation,weareprimarilycon-
anyusefulbehaviorsonthesehardtasks. cernedwithasymptoticperformancesincewewanttomaxi-
mizethedownstreamperformanceofouragents(withina
6.2.In-handreorientation reasonabletrainingtimebudget). WeseethatonAllegro-
Hand, SAPG beats PQL by a 21% margin, while on the
TheAllegroHandandShadowHandreorientationtasksfrom
ShadowHandtaskitachievescomparableperformance. On
Lietal.[15]arecomparativelyeasiersincetheyhavelower
these tasks, both PBT and PPO generally perform worse.
degrees of freedom and the object doesn’t move around
ThisisbecausePPOisnotabletoefficientlyleveragethe
much and remains inside the hand. On these tasks, we
largebatchsize. PBTlosesthebenefitofitshyperparameter
observe that PQL and PPO are able to make significant
7
sesseccus
edosipE
sesseccus
edosipE
sdrawer
edosipE
sesseccus
edosipE
sdrawer
edosipE
sesseccus
edosipESAPG:SplitandAggregatePolicyGradients
Allegro Kuka Regrasping Allegro Kuka Throw Allegro Kuka Reorientation
40
25 40
30 20
30
20 15
20
10
10 5 10
0 0 0
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Number of envsteps 1e10 Number of envsteps 1e10 Number of envsteps 1e10
Allegro Hand Shadow Hand
15000
12500
10000
10000
7500
5000 5000
2500
0 0
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Number of envsteps 1e10 Number of envsteps 1e10
Ours Ours (with entropy coef = 0.003) Ours (w/o off-policy)
Ours (with entropy coef = 0.005) Ours (high off policy ratio) Ours (symmetric off-policy)
Figure6.Performancecurvesforablationsofourmethod.Thevariantsofourmethodwithasymmetricaggregationschemeorwithout
anoff-policycombinationperformsignificantlyworse.Entropyregularizationaffectsperformanceacrossenvironments,givingabenefit
inreorientation.Usingahighoff-policyratiowithoutsubsamplingdataleadstoworseperformanceonShadowHandandAllegroHand.
mutationbecausetheenvironmentissimplerandthedefault on-policy loss. A natural alternative is where there are
hyperparametersworkwell,soitroughlyreducestosimple no privileged policies and each policy is updated with
PPOin N environments. off-policydatafromallothersasdiscussedinsec.4.2.
M
6.3.Ablations WeobservethatSAPGoutperformsorachievescomparable
performancetotheentropy-regularizedvariantexceptinthe
ThecoreideabehindSAPGistocombinedatafromdiffer-
Reorientation environment where the variant with coeffi-
entpoliciesinsteadofoptimizingasinglepolicywithan
cient5e−3performsupto16.5%better. Reorientationis
extremelylargebatch. Inthissection,wewillanalyzeour
oneofthehardertasksoutofthefourAllegroKukatasks
specificdesignchoicesforhowwecombinedata(choice
andhasalotofscopeforlearningemergentstrategiessuch
ofiandX andλ)andforhowweenforcediversityamong
asusingthetabletomovetheobjectaround,etc. Explicit
thedatacollectedbythepolicies. Inparticular,wehavethe
explorationmightbeusefulindiscoveringthesebehaviors.
followingvariants
Thevariantofourswhichusesalltheoff-policydataissig-
nificantlyworseontheAllegroHandandShadowHandtasks
• SAPG(withentropycoef)Asdiscussedinsec.5.2,here
and marginally worse on Regrasping and Throw environ-
weaddanentropylosstothefollowerstoencouragedata
ments. ItismoresampleefficientthanSAPGonReorienta-
diversity. We explore different choices for the scaling
tionbutachieveslowerasymptoticperformance. Thiscould
coefficientofthislossσ ∈{0,0.005,0.003}.
bebecauseinthesimpleenvironments,additionaldatahas
• SAPG(highoff-policyratio)InSAPG,whenupdating
marginalutility. IntheharderAllegroKukaenvironments,it
the leader, we subsample the off-policy data from the
isbeneficialtouseallthedatainitiallysinceitmaycontain
followers such that the off-policy dataset size matches
optimaltrajectoriesthatwouldotherwisebemissed. How-
theon-policydata. Thisisdonebecauseoff-policydata
ever,onceanappreciablelevelofperformanceisachieved,
istypicallynoisierandwedonotwanttodrownoutthe
itbecomesbettertosubsampletopreventthenoiseinthe
gradientfromon-policydata. InSAPGwithahighoff-
off-policyupdatefromdrowningouttheon-policygradient.
policyratio,weremovethesubsamplingstepandinstead
see the impact of computing the gradient on the entire Finally,thesymmetricvariantofourmethodperformssig-
combinedoff-policy+on-policydataset. nificantlyworseacrosstheboard. Thisispossiblybecause
• Ours(symmetric)InSAPG,wechoosei=1tobethe usingallthedatatoupdateeachpolicyleadstothemcon-
“leader”andtherestare“followers”. Onlytheleaderre- verginginbehavior. Ifallthepoliciesstartexecutingthe
ceivesoff-policydatawhilethefollowersusethestandard sameactions,thebenefitofdatadiversityislostandSAPG
8
sesseccus
edosipE
sdrawer
edosipE
sesseccus
edosipE
sdrawer
edosipE
sesseccus
edosipESAPG:SplitandAggregatePolicyGradients
Figure7.Curvescomparingreconstructionerrorforstatesvisitedduringtrainingusingtop-kPCAcomponentsforSAPG(Ours),PPO
andarandomlyinitializedpolicy
Figure8.CurvescomparingreconstructionerrorforstatesvisitedduringtrainingusingMLPswithvaryinghiddenlayerdimensionsfor
SAPG(Ours),PPOandarandomlyinitializedpolicy
reducestovanillaPPO.Ofcourse,thereisarichspaceof andPPOduringtraining. Theideabehindthisisthat
possiblealgorithmsdependingonparticularchoicesofhow ifabatchofstateshasamorediversedatadistribution
dataisaggregatedanddiversityisencouragedofwhichwe thenitshouldbehardertoreconstructthedistribution
haveexploredasmallfraction. usingsmallhiddenlayersbecausehighdiversityim-
pliesthatthedistributionislesscompressible. Thus,
6.4.Diversityinexploration hightrainingerroronabatchofstatesisastrongin-
dicatorofdiversityinthebatch. Ascanbeobserved
To analyze why our method outperforms the baseline
fromtheplotsinFigure-8,wefindthattrainingerroris
method,weconductexperimentscomparingthediversityof
consistentlyhigherforourmethodcomparedtoPPO
statesvisitedbyeachalgorithmduringtraining. Wedevise
acrossdifferenthiddenlayersizes.
twometricstomeasurethediversityofthestatespaceand
findthatourmethodbeatsPPOinbothmetrics.
7.Conclusion
• PCA-Wecomputethereconstructionerrorofabatch
Inthiswork,wepresentamethodtoscalereinforcement
ofstatesusingkmostsignificantcomponentsofPCA
learningtoutilizelargesimulationcapacity. Weshowhow
andplotthiserrorasafunctionofk. Ingeneral,aset
currentalgorithmsobtaindiminishingreturnsifweperform
thathasvariationalongfewerdimensionsofspacecan
naive scaling by batch size and do not use the increased
becompressedwithfewerprincipalvectorsandwill
volume ofdata efficiently. Our methodachieves state-of-
havelowerreconstructionerror. Thismetrictherefore
the-artperformanceonhardsimulationbenchmarks.
measurestheextenttowhichthepolicyexploresdif-
ferentdimensionsofstatespace. Figure-7containsthe
Acknowledgements
plotsforthismetric. Wefindthattherateofdecrease
inreconstructionerrorwithanincreaseincomponents
WethankAlexLiandRussellMendoncaforfruitfuldiscus-
istheslowestforourmethod.
sions regarding the method and insightful feedback. We
• MLP - We train feedforward networks with small would also like to thank Mihir Prabhudesai and Kevin
hidden layers on the task of input reconstruction on Gmelinforproofreadinganearlierdraft. Thisprojectwas
batchesofenvironmentstatesvisitedbyouralgorithm supportedinpartbyONRN00014-22-1-2096andNSFNRI
9SAPG:SplitandAggregatePolicyGradients
IIS-2024594. [10] Ankur Handa, Arthur Allshire, Viktor Makoviy-
chuk, Aleksei Petrenko, Ritvik Singh, Jingzhou
Liu,DenysMakoviichuk,KarlVanWyk,Alexander
References
Zhurkevich, Balakumar Sundaralingam, Yashraj S.
[1] Ananye Agarwal, Ashish Kumar, Jitendra Ma- Narang, Jean-Francois Lafleche, Dieter Fox, and
lik, and Deepak Pathak. Legged locomo- Gavriel State. Dextreme: Transfer of agile
tion in challenging terrains using egocentric vi- in-hand manipulation from simulation to reality.
sion. In Conference on Robot Learning, 2022. 2023 IEEE International Conference on Robotics
URLhttps://api.semanticscholar.org/ and Automation (ICRA), pages 5977–5984, 2022.
CorpusID:252733339. URLhttps://api.semanticscholar.org/
CorpusID:253107794.
[2] TaoChen,JieXu,andPulkitAgrawal. Asystemfor
generalin-handobjectre-orientation,2021. [11] W.K.Hastings. MonteCarlosamplingmethodsusing
Markovchainsandtheirapplications. Biometrika,57
[3] Xuxin Cheng, Kexin Shi, Ananye Agarwal, and
(1):97–109,041970. ISSN0006-3444.
Deepak Pathak. Extreme parkour with legged
robots. ArXiv, abs/2309.14341, 2023. URL [12] Dan Horgan, John Quan, David Budden, Gabriel
https://api.semanticscholar.org/ Barth-Maron,MatteoHessel,HadovanHasselt,and
CorpusID:262826068. David Silver. Distributed prioritized experience re-
play. CoRR, abs/1803.00933, 2018. URL http:
[4] Djork-Arne´ Clevert, Thomas Unterthiner, and Sepp
//arxiv.org/abs/1803.00933.
Hochreiter. Fastandaccuratedeepnetworklearning
byexponentiallinearunits(elus),2016. [13] Sham Kakade and John Langford. Approximately
optimalapproximatereinforcementlearning. InPro-
[5] ThomasDegris,MarthaWhite,andRichardS.Sutton.
ceedingsoftheNineteenthInternationalConference
Off-policyactor-critic. CoRR,abs/1205.4839,2012.
onMachineLearning,pages267–274,2002.
URLhttp://arxiv.org/abs/1205.4839.
[14] Vijay Konda and John Tsitsiklis. Actor-critic
[6] Lasse Espeholt, Hubert Soyer, Re´mi Munos, Karen
algorithms. In S. Solla, T. Leen, and K. Mu¨ller,
Simonyan, Volodymyr Mnih, Tom Ward, Yotam
editors, Advances in Neural Information Pro-
Doron,VladFiroiu,TimHarley,IainDunning,Shane
cessing Systems, volume 12. MIT Press, 1999.
Legg, and Koray Kavukcuoglu. IMPALA: scalable
URL https://proceedings.neurips.
distributed deep-rl with importance weighted actor-
cc/paper_files/paper/1999/file/
learner architectures. CoRR, abs/1802.01561, 2018.
6449f44a102fde848669bdd9eb6b76fa-Paper.
URLhttp://arxiv.org/abs/1802.01561.
pdf.
[7] Rasool Fakoor, Pratik Chaudhari, and Alexander J.
[15] ZechuLi,TaoChen,Zhang-WeiHong,AnuragAjay,
Smola. P3o: Policy-on policy-off policy optimiza-
andPulkitAgrawal. Parallelq-learning: Scalingoff-
tion. InRyanP.AdamsandVibhavGogate,editors,
policyreinforcementlearningundermassivelyparallel
ProceedingsofThe35thUncertaintyinArtificialIntel-
simulation,2023.
ligenceConference,volume115ofProceedingsofMa-
chineLearningResearch,pages1017–1027.PMLR, [16] Timothy P. Lillicrap, Jonathan J. Hunt, Alexander
22–25 Jul 2020. URL https://proceedings. Pritzel,NicolasHeess,TomErez,YuvalTassa,David
mlr.press/v115/fakoor20a.html. Silver,andDaanWierstra. Continuouscontrolwith
deepreinforcementlearning,2019.
[8] ZipengFu,XuxinCheng,andDeepakPathak. Deep
whole-bodycontrol: Learningaunifiedpolicyforma- [17] ViktorMakoviychuk,LukaszWawrzyniak,Yunrong
nipulation and locomotion. ArXiv, abs/2210.10044, Guo,MichelleLu,KierStorey,MilesMacklin,David
2022. URLhttps://api.semanticscholar. Hoeller,NikitaRudin,ArthurAllshire,AnkurHanda,
org/CorpusID:252968218. et al. Isaac gym: High performance gpu-based
physicssimulationforrobotlearning. arXivpreprint
[9] Tuomas Haarnoja, Aurick Zhou, P. Abbeel, and
arXiv:2108.10470,2021.
Sergey Levine. Soft actor-critic: Off-policy max-
imum entropy deep reinforcement learning with [18] WenjiaMeng,QianZheng,GangPan,andYilongYin.
a stochastic actor. ArXiv, abs/1801.01290, 2018. Off-policyproximalpolicyoptimization. Proceedings
URLhttps://api.semanticscholar.org/ oftheAAAIConferenceonArtificialIntelligence,37
CorpusID:28202810. (8):9162–9170, June 2023. ISSN 2159-5399. doi:
10SAPG:SplitandAggregatePolicyGradients
10.1609/aaai.v37i8.26099. URLhttp://dx.doi. David Blei, editors, Proceedings of the 32nd In-
org/10.1609/aaai.v37i8.26099. ternational Conference on Machine Learning, vol-
ume 37 of Proceedings of Machine Learning Re-
[19] TakahiroMiki,JoonhoLee,JeminHwangbo,Lorenz
search, pages 1889–1897, Lille, France, 07–09 Jul
Wellhausen,VladlenKoltun,andMarcoHutter. Learn-
2015. PMLR. URL https://proceedings.
ing robust perceptive locomotion for quadrupedal
mlr.press/v37/schulman15.html.
robotsinthewild. ScienceRobotics,7(62):eabk2822,
2022. [27] JohnSchulman,FilipWolski,PrafullaDhariwal,Alec
Radford, and Oleg Klimov. Proximal policy opti-
[20] VolodymyrMnih,KorayKavukcuoglu,DavidSilver,
mization algorithms. ArXiv, abs/1707.06347, 2017.
AlexGraves,IoannisAntonoglou,DaanWierstra,and
URLhttps://api.semanticscholar.org/
Martin A. Riedmiller. Playing atari with deep re-
CorpusID:28695052.
inforcement learning. ArXiv, abs/1312.5602, 2013.
URLhttps://api.semanticscholar.org/ [28] John Schulman, Philipp Moritz, Sergey Levine,
CorpusID:15238391. MichaelJordan,andPieterAbbeel. High-dimensional
continuouscontrolusinggeneralizedadvantageesti-
[21] VolodymyrMnih,Adria`Puigdome`nechBadia,Mehdi
mation,2018.
Mirza,AlexGraves,TimothyP.Lillicrap,TimHarley,
DavidSilver,andKorayKavukcuoglu. Asynchronous [29] Richard S Sutton, David McAllester, Satinder
methods for deep reinforcement learning. CoRR, Singh, and Yishay Mansour. Policy gradient
abs/1602.01783, 2016. URL http://arxiv. methods for reinforcement learning with func-
org/abs/1602.01783. tion approximation. In S. Solla, T. Leen, and
K. Mu¨ller, editors, Advances in Neural Informa-
[22] Arun Nair, Praveen Srinivasan, Sam Blackwell,
tion Processing Systems, volume 12. MIT Press,
Cagdas Alcicek, Rory Fearon, Alessandro De
1999. URLhttps://proceedings.neurips.
Maria, Vedavyas Panneershelvam, Mustafa Suley-
cc/paper_files/paper/1999/file/
man, Charles Beattie, Stig Petersen, Shane Legg,
464d828b85b0bed98e80ade0a5c43b0f-Paper.
VolodymyrMnih,KorayKavukcuoglu,andDavidSil-
pdf.
ver. Massively parallel methods for deep reinforce-
mentlearning. CoRR,abs/1507.04296, 2015. URL [30] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czar-
http://arxiv.org/abs/1507.04296. necki,Michae¨lMathieu,AndrewDudzik,Junyoung
Chung,DavidH.Choi,RichardPowell,TimoEwalds,
[23] OpenAI, Marcin Andrychowicz, Bowen Baker, Ma-
Petko Georgiev, Junhyuk Oh, Dan Horgan, Manuel
ciekChociej,RafalJo´zefowicz,BobMcGrew,Jakub
Kroiss, Ivo Danihelka, Aja Huang, L. Sifre, Trevor
Pachocki, Arthur Petron, Matthias Plappert, Glenn
Cai,JohnP.Agapiou,MaxJaderberg,AlexanderSasha
Powell, Alex Ray, Jonas Schneider, Szymon Sidor,
Vezhnevets,Re´miLeblond,TobiasPohlen,Valentin
Josh Tobin, Peter Welinder, Lilian Weng, and Wo-
Dalibard, David Budden, Yury Sulsky, James Mol-
jciech Zaremba. Learning dexterous in-hand ma-
loy,TomLePaine,CaglarGulcehre,ZiyunWang,To-
nipulation. CoRR, abs/1808.00177, 2018. URL
biasPfaff,YuhuaiWu,RomanRing,DaniYogatama,
http://arxiv.org/abs/1808.00177.
DarioWu¨nsch,KatrinaMcKinney,OliverSmith,Tom
[24] Aleksei Petrenko, Arthur Allshire, Gavriel State, Schaul, Timothy P. Lillicrap, Koray Kavukcuoglu,
AnkurHanda,andViktorMakoviychuk.Dexpbt:Scal- DemisHassabis,ChrisApps,andDavidSilver. Grand-
ingupdexterousmanipulationforhand-armsystems master level in starcraft ii using multi-agent rein-
withpopulationbasedtraining. InKostasE.Bekris, forcement learning. Nature, 575:350 – 354, 2019.
Kris Hauser, Sylvia L. Herbert, and Jingjin Yu, ed- URLhttps://api.semanticscholar.org/
itors, Robotics: Science and Systems XIX, Daegu, CorpusID:204972004.
Republic of Korea, July 10-14, 2023, 2023. doi:
[31] ZiyuWang,VictorBapst,NicolasHeess,Volodymyr
10.15607/RSS.2023.XIX.037. URLhttps://doi.
Mnih,Re´miMunos,KorayKavukcuoglu,andNando
org/10.15607/RSS.2023.XIX.037.
de Freitas. Sample efficient actor-critic with expe-
[25] NikitaRudin,DavidHoeller,PhilippReist,andMarco rience replay. CoRR, abs/1611.01224, 2016. URL
Hutter. Learningtowalkinminutesusingmassively http://arxiv.org/abs/1611.01224.
paralleldeepreinforcementlearning,2022.
[32] Christopher Watkins and Peter Dayan. Q-
[26] John Schulman, Sergey Levine, Pieter Abbeel, learning. Machine Learning, 8:279–292, 1992.
Michael Jordan, and Philipp Moritz. Trust re- URLhttps://api.semanticscholar.org/
gion policy optimization. In Francis Bach and CorpusID:208910339.
11SAPG:SplitandAggregatePolicyGradients
[33] Ronald J. Williams. Simple statistical gradient-
followingalgorithmsforconnectionistreinforcement
learning. Machine Learning, 8:229–256, 2004.
URLhttps://api.semanticscholar.org/
CorpusID:19115634.
12SAPG:SplitandAggregatePolicyGradients
A.TaskandEnvironmentDetails
HardDifficultyTasks AllfourhardtasksarebasedontheAllegro-Kukaenvironments[24]. Theseconsistofan
Allegro Hand (16 dof) mounted on a Kuka arm (7 dof). In each case, the robot must manipulate a cuboidal kept on a
fixedtable. Theobservationspaceiso = [q,q˙,x ,v ,ω ,g ,z ],whereq,q˙ ∈ R23 arethejointanglesandvelocities
t t t t t t
respectivelyofeachjointoftherobot,x ∈R7istheposeoftheobject,v isitslinearvelocityandω isitsangularvelocity,
t t t
g isatask-dependentgoalobservationandz isauxiliaryinformationpertinenttosolvingthetasksuchasiftheobjecthas
t t
beenlifted. Thesetasksconsistofacomplexenvironmentbutasimplerewardfunctionallowingopportunitiesforemergent
strategiestobelearntsuchasin-handreorientationundertheinfluenceofgravity,reorientationagainstthetable,different
typesofthrowsandgraspsandsoon. Theperformanceoftheagentintheabovethreetasksismeasuredbythesuccesses
metricwhichisdefinedasthenumberofsuccessesinasingleepisode. Threetasksinclude:
• Regrasping-Theobjectmustbeliftedfromthetableandheldnearagoalpositiong ∈R3forK =30steps. This
t
iscalleda“success”. Thetargetpositionandobjectpositionareresettoarandomlocationaftereverysuccess. The
successtoleranceδdefinesthemaximumerrorbetweenobjectposeandgoalposeforasuccess∥g −(x ) ∥≤δ.
t t 0:3
Thistoleranceisdecreasedinacurriculumfrom7.5cmto1cm,decrementedby10%eachtimethetheaveragenumber
ofsuccessesinanepisodecrosses3. Therewardfunctionisaweightedcombinationofrewardsencouragingthehand
toreachtheobjectr ,abonusr ,rewardsencouragingthehandtomovetogoallocationafterliftingr
reach lift target
andasuccessbonusr .
success
• Throw-Theobjectmustbeliftedfromthetableandthrownintoabucketatg ∈R3placedoutofreachofthearm.
t
Thebucketandtheobjectpositionareresetrandomlyaftereverysuccessfulattempt. Therewardfunctionissimilarto
regraspingwiththedifferencebeingthatthetargetisnowabucketinsteadofapoint.
• Reorientation-Thistaskinvolvespickinguptheobjectandreorientingittoaparticulartargetposeg ∈R7(position
t
+orientation). Similartotheregraspingtask,thereisasuccesstoleranceδwhichisvariedinacurriculum. Thetarget
poseisresetoncetheagentsucceeds. Thismeanstheagentsneedstotheobjectindifferentposesinsuccession,which
maysometimesentailplacingtheobxfjectonthetableandliftingitupinadifferentway. Heretoo,therewardfunction
issimilartoregrasping,withthegoalnowbeingaposeinR7insteadofR3.
• TwoArmsReorientation: TheobjectiveisthesametotheAllegroKukaReorientationtask. Thedifferenceisthatthe
setuphasanotherarmnow. Inthistask,thetargetposemaybewithinthereachofonearmbutnottheother,which
meansthatthearmsmayneedtotransfertheobjectbetweenthemselves,forwhichitneedstolearncomplexthrowing
andcatchingbehaviours.
Easy Difficulty Tasks: In addition, we test on the following dexterous hand tasks. As before, the observation space
consistsofthejointanglesandvelocitiesq ,q˙ ,objectposex andvelocitiesv ,ω . Followingpreviousworks[15],we
t t t t t
usethenetepisoderewardasaperformancemetricfortheShadowHandandAllegroHandtasks.
• ShadowHand: Wetestonin-handreorientationtaskofacubeusingthe24-DoFShadowHand([23]). Thetaskisto
attainaspecifiedgoalorientation(specifiedasaquaternion)forthecubeg ∈R4. Therewardisacombinationofthe
t
orientationerrorandasuccessbonus.
• AllegroHand: Thisisthesameasthepreviousin-handreorientationtaskbutwiththe16-DoFAllegroHandinstead.
B.Traininghyperparameters
WeusetwodifferentsetsofdefaulthyperparaetersforPPOinAllegroKukaandShadowHandtaskswhicharedescibed
below.
B.1.AllegroKukatasks
WeuseaGaussianpolicywherethemeannetworkisanLSTMwith1layercontaining768hiddenunits. Theobservationis
alsopassedthroughanMLPofwithhiddenlayerdimensions768×512×256andanELUactivation[4]beforebeing
inputtotheLSTM.ThesigmafortheGaussianisafixedlearnablevectorindependentofinputobservation.
13SAPG:SplitandAggregatePolicyGradients
Hyperparameter Value
Discountfactor,γ 0.99
τ 0.95
Learningrate 1e-4
KLthresholdforLRupdate 0.016
Gradnorm 1.0
Entropycoefficient 0
Clippingfactorϵ 0.1
Mini-batchsize num envs·4
Criticcoefficientλ′ 4.0
Horizonlength 16
LSTMSequencelength 16
Boundslosscoefficient 0.0001
Miniepochs 2
Table2. TraininghyperparametersforAllegroKukatasks
B.2.ShadowHand
WeuseaGaussianpolicywherethemeannetworkisanMLPwithhiddenlayersdimensions512×512×256×128and
anELUactivation[4]
Hyperparameter Value
Discountfactor,γ 0.99
τ 0.95
Learningrate 5e-4
KLthresholdforLRupdate 0.016
Gradnorm 1.0
Entropycoefficient 0
Clippingfactorϵ 0.1
Mini-batchsize num envs·4
Criticcoefficientλ′ 4.0
Horizonlength 8
Boundslosscoefficient 0.0001
Miniepochs 5
Table3. TraininghyperparametersforShadowHand
B.3.AllegroHand
WeuseaGaussianpolicywherethemeannetworkisanMLPwithhiddenlayersdimensions512×256×128andanELU
activation.
14SAPG:SplitandAggregatePolicyGradients
Hyperparameter Value
Discountfactor,γ 0.99
τ 0.95
Learningrate 5e-4
KLthresholdforLRupdate 0.016
Gradnorm 1.0
Entropycoefficient 0
Clippingfactorϵ 0.2
Mini-batchsize num envs·4
Criticcoefficientλ′ 4.0
Horizonlength 8
Boundslosscoefficient 0.0001
Miniepochs 5
Table4. TraininghyperparametersforShadowHand
Note: Incaseofexperimentswithentropybasedexploration,eachblockofenvironmentshasit’sownlearnablevector
sigmawhichenablepoliciesfordifferentblockstohavedifferententropies.
15