On the Effects of Irrelevant Variables in Treatment Effect
Estimation with Deep Disentanglement
AhmadSaeedKhana,ErikSchaffernichta andJohannesAndreasStorka
aÖrebroUniversity,Örebro,Sweden
Abstract. Estimatingtreatmenteffectsfromobservationaldatais
paramountinhealthcare,education,andeconomics,butcurrentdeep
disentanglement-basedmethodstoaddressselectionbiasareinsuffi-
cientlyhandlingirrelevantvariables.Wedemonstrateinexperiments
thatthisleadstopredictionerrors.Wedisentanglepre-treatmentvari-
ableswithadeepembeddingmethodandexplicitlyidentifyandrep-
resentirrelevantvariables,additionallytoinstrumental,confounding
andadjustmentlatentfactors.Tothisend,weintroduceareconstruc-
tionobjectiveandcreateanembeddingspaceforirrelevantvariables
using an attached autoencoder. Instead of relying on serendipitous
suppressionofirrelevantvariablesasinpreviousdeepdisentangle-
ment approaches, we explicitly force irrelevant variables into this
Figure1. AveragePEHEerroronIHDPdatasetagainstnumberofirrele-
embeddingspaceandemployorthogonalizationtopreventirrelevant
vantvariabledimensions(smallerthebetter).PEHEgenerallydegradeswith
informationfromleakingintothelatentspacerepresentationsofthe
mthoeroeriertrieclaevlarnetsfualcttsoresmbupthoausrizminetghohdairsmlefsuslacffoencsteedq.uences of unprinci-
otherfactors.Ourexperimentswithsyntheticandreal-worldbench-
pledcovariateinclusion[23].
mark datasets show that we can better identify irrelevant variables
Inthispaper,weaddresstheissueofunidentifiedirrelevantpre-
and more precisely predict treatment effects than previous meth-
treatmentvariableswithanoveldeepdisentanglementapproachfor
ods,whilepredictionqualitydegradeslesswhenadditionalirrelevant
estimatingtreatmenteffectswhichexplicitlyidentifiesandrepresents
variablesareintroduced.
irrelevantfactors,additionallytoinstrumental,confoundingandad-
justment factors. We achieve disentanglement of irrelevant factors
1 Introduction byintroducinganadditionalembeddingspaceforirrelevantfactors
usingcovariatereconstructionandorthogonalityobjectives.
Treatment effect estimation from observational data is challenging Weempiricallyevaluateourapproachandcompareittostate-of-
becausetheuncontrolledmodeofdatacollectioncanleadtoselec- the-artdeepdisentanglementbaselinesusingtheinfanthealthandde-
tion bias. Selection bias causes a distributional difference between velopmentprogram(IHDP),jobsandasyntheticdatasetswithvary-
observedpre-treatmentvariablesfordifferenttreatmentgroups,lead- ingnumberofirrelevantvariables.Wefindthatourmodelisbetter
ing to biased counterfactual predictions. Managing this imbalance thanbaselinesatidentifyinganddisentanglingthelatentfactors,in-
betweentreatmentgroupsisthereforeanimportantobjectiveforim- cludingirrelevantfactors,accordingtoperturbationimportanceanal-
provingtreatmenteffectestimation[4,15]. ysis[1,7,31]andanalysisofweightsoftherepresentationnetworks
Deepdisentanglementapproaches[11,3]userepresentationlearn- [32].WealsoobservebetterperformanceonPEHEandpolicyrisk
ingtoidentifytheunderlyingfactorsasinstrumental,confounding, evaluation criteria with increased number of irrelevant variables as
oradjustment.Thisallowsthemtobalancefactorsindividuallyfor comparedtobaselines.Ourapproachispracticableandinprinciple
improving treatment effect estimation [11]. However, this assumes compatiblewithpreviousdeepdisentanglement-basedworksasthe
thatallpre-treatmentvariablesarepre-screenedforrelevance,which additionalchannelandreconstructionobjectiveleavetheotherrep-
isimpracticalinincreasinglyprevalentdata-drivenandbigdataset- resentationnetworksandobjectivesunaltered.
tings. Ourcorecontributionsare:
Ourempiricalanalysisshowsthatignoringthepresenceofirrele-
vantvariablesinthedatacriticallydegradespredictionswithasig- • Investigating the impact of irrelevant variables on estimation of
nificantdropintheprecisioninestimationofheterogeneouseffects treatmenteffectsforstate-of-the-artdisentangledrepresentational
(PEHE)forestablishedbenchmarkdatasets(seeFig.1).Relyingon learningmethods.
serendipitous suppression of irrelevant variables as state-of-the-art • Theproposalofanautoencoder-basedapproachtodisentangleir-
deep disentanglement approaches do, is insufficient as it does not relevantfactorsexplicitly.
reliably prevent irrelevant information from leaking into other fac- • A thorough evaluation of our approach, showing that it outper-
tors. Instead, it is necessary to actively disentangle irrelevant vari- formsthebaselinemethodsinestimatingindividualtreatmentef-
ablesfromothercovariatesusedforprediction.Thisissupportedby fects,especiallyinthepresenceofirrelevantvariables.
4202
luJ
92
]GL.sc[
1v30002.7042:viXra2 RelatedWork
Selectionbiasinobservationaldataisawellknownproblemintreat-
ment effect estimation, which is classically countered by balanc-
ingconfounderswithmatching,stratification,andre-weightingap-
proaches[25,24,21].However,assumingunfoundednessorrelying
onpriorknowledgeofthecausalstructureisunsuitableinreal-world,
high-dimensionalsettings,leadingtounderperformance[18].
Deep representation learning has been proposed for balancing
variables in higher-dimensional settings [15, 34, 27, 12]. The idea
behind these approaches is to make the embedded data look like
a Randomized Controlled Trial (RCT) by minimizing the discrep-
ancybetweentreatmentgroups.Maximummeandiscrepancy[8]and
Wassersteindistance[29]areintegralprobabilitymeasuresthatare
both used as discrepancy losses in these methods. We also employ
thelattertobalanceourembeddingspaces.Theappealingsimplicity
ofbalancingallcovariatesinasharedsingleembeddingspace,how- Figure2. (a):IllustratestheriseinPEHEasthenumberofirrelevantvari-
ever,overlooksthefactthatusuallynotallcovariatescontributeto ablesgrowsbasedonabaselineapproach(TVAE).(b):visualizestheindivid-
both,treatmentandeffect. ualcontributionsofvariablestowardslearningtheencoderformiscellaneous
Disentanglementapproaches,incontrast,accountfortheunderly- factors.Notably,thecontributionofirrelevantvariablesmirrorsthatofrele-
ing causal structure by creating separate representations for instru- vantones,underscoringthelimitationsofTVAEindisentanglingirrelevant
mental,confoundingandadjustmentfactors[11,17,19,32,3].Dis- variables.(c)showsaveragecontributionofeachvariableinPEHEincrease
entangledrepresentationsgiveinsightsandcanbeusedtoreduce,as byusingpermutationofvariables.Irrelevantvariablesaresignificantlypar-
wellasaccountfor,thenegativeimpactofselectionbias[11].Ama- tIitciepmatpinlgoyinsPaEnHeEncinocdreearsteo. manage miscellaneous factors [30], but a
jorchallengefortheseapproachesisimperfectdecompositionwith crucial distinction arises. While TVAE aims to disentangle irrele-
pre-treatment variables information leaking into unrelated factors, vancewhenit’sintertwinedwithrelevantvariables,itfallsshortin
whichcandegradeperformanceonthedownstreampredictiontask. identifying and disentangling irrelevant variables existing in sepa-
Toensurebetterseparation,severaltechniqueshavebeenproposed ratedimensionsinpre-treatmentvariables.Ourfindings,depictedin
suchasdifferentorthogonalization[17,19,32]andmutualinforma- Figure2,revealanotableincreaseinPEHEerrorasirrelevantvari-
tion[3]objectives.WhileKuangetal.[17,19]onlyconsiderlinear ables increase based on TVAE. Furthermore, the encoder designed
embeddings,Wuetal.[32]presentadeeporthogonalregularizerfor for miscellaneous factors demonstrates an inability to disentangle
deeprepresentationnetworks,whichwealsouseinthiswork.Zhang these variables within the data. Additionally, as depicted in Figure
etal.[35]proposetousevariationalautoencoders(VAE)forsepa- 2,itbecomesevidentthatTVAEfaceschallengesinmitigatingthe
ratingthethreefactors(TEDVAE).VAEshavebeenusedpreviously influenceofirrelevantvariables,whichcontributetotheincreasein
inLouizosetal.[22]toestimatetheconfoundingfactorsonly,with- PEHE.Theseobservationsarebasedonthesamesyntheticdatauti-
outtheuseofanydiscrepancyloss.WuandFukumizu[33]present lizedintheoriginalstudybyVowelsetal.[30].
prognosticscorebasedVAEapproachtoestimatecausaleffectsfor
datawithlimitedoverlapbetweentreatmentgroups.
3 FormalizationandAssumptions
In real-world observational studies unidentified, irrelevant vari-
ables are inevitable and our empirical results show that irrelevant Inthissection,wefirstgivethenotationsandassumptionsfortreat-
variables can degrade prediction results. Removing irrelevant vari- menteffectestimationinobservationaldata.Moreover,wealsode-
ableshasbeenstudiedinfeatureselectionfield[10]andmanyrep- fineunderlyinglatentfactorsofpre-treatmentvariables.
resentationlearningapproachesareimplicitlyconsideredtoremove Formally, observational studies have a dataset: D =
irrelevant information. Yet there are indications [9, 16] that this is {x ,t ,y }N , where the ith instance has some contextual in-
i i i i=1
notsufficient,especiallyfortabulardata.However,applyingclassic formationx ∈X ⊆RK (oftencalledpre-treatmentvariables:e.g.,
i
featureselectioninadisentanglementtaskfortreatmenteffectesti- genderandage),t istheobservedtreatmentfromthesetoftreat-
i
mationisnotstraightforwardduetoproxyvariables. mentsT (e.g.,0:medication,1:surgery)andy ∈ Y (e.g.,survival
i
Someofthedisentanglement-basedworksdiscussedbeforecon- time;Y ⊆ R+)istherespectiveoutcomeastheresultofparticular
siderirrelevantvariables:HassanpourandGreiner[11]includesone treatment t . In data D, we only observe one outcome against the
i
singleGaussiannoisefactorintheirsyntheticdatasetforevaluations; usedtreatment(knownasfactualoutcomeyt)butalternativeoutput
i
Wu et al. [32] claim that orthogonal regularization reduces the in- (counterfactualoutcomey¬t)isneverobservedinthedata.Insuch
i
fluence of irrelevant variables on the prediction; and Kuang et al. datasets, X influences treatment assignment policy which causes
[17] mention that they eliminate irrelevant variables in their linear selection bias in the data, where the condition P(T|X) = P(T)
embeddingwithL penalties.Whilethelatteranalyzeseparationof doesnotholdanditlacksRCTproperties[14,11].
1
confoundersandadjustmentfactors,theydonotreportontheiden- HassanpourandGreiner[11]assume,withoutlossofgenerality,
tification of irrelevant factors. In contrast to our work, none of the thatX isgeneratedbyunknownjointdistributionP(X | Γ,∆,Υ),
approaches above explicitly represents irrelevant factors to achieve whereΓ,∆,Υarelatentfactors;wearekeepingthepreviouslyes-
disentanglement.Alloftheapproachesaboverelyonserendipitous tablishednotationsforthethesefactors.Γ(instrumentalfactors)only
suppressionofirrelevantfactorswhichisinsomecasesencouraged influence treatment selection, ∆ (confounding factors) affect both
byregularization. treatment selection and outcome, while Υ (adjustment factors) im-
TargetedVAE(TVAE)isthemostsimilarworktoourapproach. pactoutcomeonly.Weassumethatthereisanotherunderlyingirrel-Figure3. UnderlyingfactorsofX.ObservethatΩhasnoassociateddown-
se tv rea an mtl ta at se kn wtf ia thct ao nr y( oΩ b) seb re vh edin vd art ih ae blg e.enerationofX,depictedinFigure Finally, esFtiimguartein4g. ΩHidgihrelecvtleylairschditieffictcuureltosfiDncReI-ItTheEr.e is no asso-
3.Moreover,wealsoassumethatlatentfactorsareassociatedwith ciated downstream task as shown in Fig.3. Instead we employ a
separatedimensionsofX asstatedinKuangetal.[17].Thelearn- decoder that reconstructs X. The core idea is that reconstructing
ingofΩfactorhelpstomatchtruedatagenerationprocesswithout the input data X in this autoencoder fashion requires to capture
harmingtheidentifiabilityofcausaleffects[30]. alllatentfactorsincludingthosenotrelevanttotheITEestimation.
The objective of this paper is to estimate Individual Treatment Intuitively, this allows us to use orthogonality objectives to sepa-
Effect (ITE) for each x i: ite i = y i1 −y i0, by learning a function rate the ITE irrelevant factors into their own embedding space as
f: X ×T → Y. However, it is not straightforward to learn such Ω=X \{Γ,∆,Υ}.
functionf becauseD containsselectionbiasandirrelevantfactors Fromacomputationalpointofview,DRI-ITEismoderatelymore
(Ω).ItisessentialtodisentangleΩfromotherlatentfactorstoeffi- expensivethancomparableapproachesduetotheextraembedding
cientlymitigateselectionbiasandtohavereliableestimateofITEby spacetolearntheirrelevantfactors.
avoiding the overfitting of regression function f [17]. Empirically, Theformalalgorithmisprovidedin1andwewilldiscussdetails
we have observed a decline in the performance of recent disentan- ofeachlossfunctioninthefollowing.
gledrepresentationlearningmethodsfortheITEestimationwiththe
increasingpresenceofΩ,asshownintheFigure1. Algorithm1DisentangledRepresentationwithIrrelevantFactorfor
Ourwork,likeothermethodsinthisdomain,alsoreliesonthree IndividualTreatmentEffectEstimation(DRI-ITE)
assumptionsaspresentedinRubin[26]. Input:D={x ,t ,y },...,{x ,t ,y }
1 1 1 N N N
Output:yˆ1,yˆ0
1. StableUnitTreatmentValue:Thetreatmentassignmenttoone
Lossfunction:L
unitdoesnotaffectthedistributionofpotentialoutcomesofthe main
Components: Four representation networks
otherunit.
{Γ(.),∆(.),Υ(.),Ω(.)}, two regression networks { h0(.),h1(.)},
2. Unconfoundedness:meansthereisnounmeasuredconfounding. y y
one decoder h (.) and one classification network
AllconfoundingeffectonY andT hasbeenmeasured,formally, recon
h
Y ⊥⊥T |X. c
1: fori=1toN
3. Overlap:assumptionstatesthattheprobabilityofassigningany
treatmenttoxishigherthanzero.Formally,P(t | x) > 0∀t ∈ 2: {x i,t i,y i}N i=1 →{Γ(x i),∆(x i),Υ(x i),Ω(x i)}
T,∀x∈X. 3: h c(Γ(x i),∆(x i))→tˆ i
4: h0 y(∆(x i),Υ(x i)),h1 y(∆(x i),Υ(x i))→yˆ1,yˆ0
The assumptions of unconfoundedness and overlap are jointly 5: h recon(Γ(x i),∆(x i),Υ(x i),Ω(x i))→x i
knownasstrongignorability. 6: w←Adam{L main}
7: endfor
4 Methods 8: returnyˆ1,yˆ0
ConsideringthelikelihoodofΩbeingpresentinobservationaldata,
Themainobjectivefunctiontobeminimizedisasfollows:
itbecomesimperativetodeviseanapproachthatdisentanglesΩand
estimatesITErobustly. L =L +α·L +β·L
main reg class disc
Thereto, we propose DRI-ITE, which learns disentangled repre-
sentationwithfourlatentfactors(Γ,∆,Υ,Ω),accountsforselection +γ·L recons+λ·L orth (1)
biasandsimultaneouslylearnstopredictcounterfactualoutcomefor +µ·Reg(h1 y,h0 y,h c,h recon).
thefinalestimateoftreatmenteffect.Weachievedisentanglementof
Reg is a regularization term for the respective functions and
Ωbyintroducinganadditionalembeddingspaceforirrelevantfac-
α,β,γ,λ,µareweightingparameters.
torsusingX reconstructionandorthogonalityobjectives.
WedefineL as:
Figure4showstheDRI-ITEarchitecture,whichcontainsfourrep- reg
resentationalnetworks(encoders).Eachnetworklearnsonespecific L =L[y ,hti(∆(x ),Υ(x ))]. (2)
reg i y i i
latentfactor.Tworegressionnetworks(oneforeachtreatmentgroup)
learn to predict factual and counterfactual outcomes, and help two L istheregressionloss.Wetraintworegressionnetworksasused
reg
representationalnetworkstodisentangle∆andΥusingL .One inShalitetal.[27]andHassanpourandGreiner[11]topredictob-
reg
classificationnetworklearnstopredictthetreatmentandhelpsindis- servedoutcomebasedonrespectivetreatment.Itisnoteworthythat
entanglingΓand∆usingL . these regressors are learning on the concatenation of the ∆ and Υ
classfactors.MinimizingL ensuresthatinformationregardingtheout- 5.1.1 SyntheticDataset
reg
come y is retained in these two latent factors and both representa-
The dataset comprises a sample size of N, with dimen-
tionalnetworkslearnitsrespectivefactors.
sions [mΓ,m∆,mΥ], along with mean and covariance matrices
WedefineL classas:
(µ
,(cid:80)
)foreachlatentfactorL ∈ [Γ,∆,Υ].Amultivariatenor-
L L
L class =L[t i,h c(Γ(x i),∆(x i))]. (3) maldistributionisemployedfordatageneration,andthecovariates
matrix is constructed as N × (m + m + m ). The synthetic
L is the classification loss. Classifier h learns to predict the Γ ∆ Υ
class c dataset is generated using the same settings and approach as pre-
treatmentusingtheconcatenationof∆andΓ.
sented by Hassanpour and Greiner [11]. To generate Ω, we follow
WedefineL asfollows:
disc thefeatureselectioncommunitybyaddingartificialcontrasts.Each
L =disc[Υ(x )t ,Υ(x )t ]. (4) irrelevant variable is a permutation of a randomly selected feature
disc i i=0 i i=1
generatedfortheotherfactorsassimplyusingGaussianoruniform
ByminimizingL ,weensurethatΥcontainsnoinfluencefrom
disc distributionsmaynotbesufficient[28].
Γ.Inotherwords,L helpstomitigateselectionbiascausedby
disc
Γtohaveunbiasedpredictionsforthedownstreamtask.Weusethe
WassersteindistanceasdiscrepancylossasproposedbyChengetal. 5.1.2 InfantHealthandDevelopmentProgram(IHDP)
[3].
This is a binary treatment dataset based on IHDP conducted by
ThedefinitionofL isasfollows:
recons Brooks-Gunnetal.[2].Hill[13]introducedselectionbiasinoriginal
L =L[x ,h (Γ(x ),∆(x ),Υ(x ),Ω(x ))]. (5) RCTdatatomakeitanobservationaldataset.Itcontains25covari-
recons i recon i i i i
atesthatdescribedifferentaspectsofthechildandmother,suchas
L isthereconstructionlossusedbytheautoencodertorecon-
recons birth weight, neonatal health index, mother’s age, drug status, etc.
structX basedonallfourembeddingspaces.WeuseMSEforthe
Datahas747instancesintotal,139belongtothetreatedgroupand
reconstruction.
608belongtothecontrolgroup.Thepurposeofthestudy/datawas
L isdeeporthogonalregularizertoensuredistinctionamong
orth tochecktheeffectoftreatment(specialisthomevisits)onthecog-
latentfactors.ItsideaisoriginallyinspiredbyKuangetal.[17].We
nitivehealthofchildren.IHDPdoesnotcontainirrelevantvariables,
usedthelossL inthesamewayasitwasusedbyWuetal.[32].
orth therefore we augment it with artificial contrasts for the evaluation
However,insteadof constraining orthogonalityonpairsofaverage
purpose.
weightvectorsofjustthreerepresentationalnetworks,weconstrain
orthogonality for the three more pairs to keep Ω separate from all
5.1.3 Jobs
otherthreebasicfactors.WedefineL asfollows:
orth
L =W¯T ·W¯ +W¯T ·W¯ +W¯T ·W¯ ItisanobservationaldatasetcollectedundertheLalondeexperiment.
orth Γ ∆ ∆ Υ Υ Γ
(6) Itcontainseightpre-treatmentcovariates;age,educ,black,hisp,mar-
+W¯T ·W¯ +W¯T ·W¯ +W¯T ·W¯ ,
Ω Γ Ω ∆ Ω Υ ried,nodegr,re74,re75.Jobsdatasetisbinarytreatmentdata;treat-
whereW ⊆ Rd×d istheproductofweightmatricesacrossalllay- ment shows whether a person received job training or not. At the
ers within a representational network, W¯ ⊆ Rd×1 represents the sametime,theoutcomevariableindicatestheearningsofaperson
in 1978. Data has 614 instances in total, 185 belong to the treated
row-wiseaveragevectoroftheabsolutevaluesofW foreachnet-
work. The vectorW¯ provides insight intothe average contribution group,and429belongtothecontrolgroup[13,20,5].Weuseartifi-
cialcontrastsfortheevaluationpurpose.
ofeachfeaturewithinthatspecificrepresentationalnetwork.When
L isminimized,thedotproductsbetweentheweightvectorsbe-
orth
comesmallorclosetozeroindicatingorthogonality.Orthogonality 5.2 Evaluationcriteria
betweenrepresentationsencourageseachrepresentationalnetworkto
PrecisioninEstimationofHeterogeneousEffect(PEHE):
focusoncapturinguniquepatternsandfeaturesrelevanttoitsspecific
task.Itpreventsthenetworksfromredundantlylearningsimilarin- (cid:118)
(cid:117) N
formation.Alternatively,conceptsfrominformationtheoryi.e.total (cid:117) 1 (cid:88)
PEHE =(cid:116) (eˆ −e )2 (7)
correlationormutualinformationcanalsobeutilizedtoseparatein- N i i
i=1
formationbetweentherepresentationalnetworks,butgiventhecom-
putationalconstraintsofthesemethodsitiscommontoemploydeep whereeˆ =yˆ1−yˆ0ande =y1−y0arepredictandtrueeffects
i i i i i i
orthogonalregularizers. respectively.
Policyrisk(R ):
pol
5 Experiments
Beforediscussingtheresultsoftheproposedmethod,wewillbriefly R pol(π f)=1−(E[Y1|π f(x)=1]·p(π f =1)
(8)
discusstheuseddatasets,evaluationcriteriaandexperimentdetails. +E[Y0|π (x)=0]·p(π =0))
f f
The policy risk is a measure of the average loss in value when
5.1 Datasets
followingaspecifictreatmentpolicy.Thetreatmentpolicy(π (x))
f
Weusebothsyntheticandreal-worlddatasetstoevaluatetheperfor- isasetofrulesbasedonthepredictionsofamodelf.Specifically,
manceoftheproposedmethod.Asyntheticdatasetallowstocontrol ifthedifferenceinthemodel’spredictionsfortreatment(1)andno
all the latent factor that make up X. To this purpose, we are aug- treatment (0) is greater than a threshold (λ), then treat (π (x) =
f
menting the existing dataset proposed by Hassanpour and Greiner 1), otherwise do not treat (π (x) = 0). This formula involves the
f
[11]withadditionalirrelevantvariables.Additionally,performance expected outcomes when following the treatment policy, weighted
onthecommonlyusedIHDPandjobsdatasetwillbeanalyzed. bytheprobabilitiesofapplyingthepolicy[27].Figure5. Thevisualizationoffeaturecontributionsoneachlatentfactorrepresentationalnetworkisconductedforthedatasetwithdimensions8,8,8,15
(Γ,∆,Υ,Ω)utilizingtheW¯ criterionbasedonDRI-ITE(ours).Thetoprowvisualizesallindividualfeatures,wherehighvaluesareexpectedforthefeatures
betweendottedlines,thebottomrowrepresentstheaverageoverallfeaturesthataresupposedtoberepresentedbythatparticularnetworkcomparedtothe
averageweightofwronglyrepresentedfeatures.
Figure6. TheaveragecontributionofeachfeatureinincreasingBCE,MSEandPEHElossisassessedbypermutingfeaturesbasedonDRI-ITE(ours).Figure
(a)showstheaveragecontributionofeachfeatureinincreasingBCE(contributionofΓand∆featuresshouldhavehigherincreasesinBCEascomparedto
restofthefeatures,ifΓand∆factorsareidentifiedcorrectly).Figure(b)showstheaveragecontributionofeachfeatureinincreasingMSE(contributionof∆
andΥfeaturesshouldhavehigherincreasesinMSE,if∆andΥfactorsareidentifiedcorrectly).Figure(c)showstheaverageincreaseinPEHE(0.0017)by
irrelevantvariablesusingDRI-ITE(ours).AlowerimpactofirrelevantvariablesinincreasingPEHEindicatesaccuratedisentanglementofΩandreliableITE
estimation.
Figure 7. Radar charts visualizing the PEHE (mean values) results on the synthetic dataset. Each vertex represents the dimensions of latent factors
(Γ_∆_Υ_Ω).PEHEvaluesclosertothecenterarebetter.Dashedredlinesshowourproposedmethod(DRI-ITE).Table1. Resultsofablationstudyoflossfunction(boldnumbersindicatesmallest/bestresults).
Loss 8_8_8_5 8_8_8_10 8_8_8_15 8_8_8_20 8_8_8_25
L +L +L 0.21 0.25 0.26 0.28 0.32
reg class disc (0.009) (0.01) (0.01) (0.01) (0.006)
L +L +L +L 0.21 0.25 0.25 0.28 0.31
reg class disc orth (0.01) (0.02) (0.01) (0.01) (0.005)
L +L +L +L +L 0.22 0.20 0.20 0.21 0.21
reg class disc orth recons (0.01) (0.01) (0.02) (0.01) (0.01)
5.3 Experimentdetails precise disentanglement of latent factors achieved by our repre-
sentationalnetworks.Theunderlyingprincipleisstraightforward:
ifshufflingafeatureleadstoanincreaseinmodelerroraftertrain-
Table2. Hyper-parametersandRanges.
Hyper-parameter Range ing,thefeatureisdeemedimportant;otherwise,itcanbeconsid-
eredunimportant.
Latentdimensions {5,10,15,100}
Hiddendimensions {50,100,200} InFigure5,thetophalfshowstheW¯ barplotsfortheΓ,∆,Υ,and
Layers {2,3,4}
Ωrepresentationalnetworksusingsyntheticdata.Notably,onlyrel-
Batchsize {32,64,128,256}
evantfeaturesexhibithighweightscomparedtotheremainingones.
Learningrate {1e−2,1e−3,1e−4,1e−5}
Thefigureconfirmsthateachnetworkaccuratelyidentifiesitscorre-
α,β,γ,λ,µ {0.01,0.1,1,5,10,100}
spondinglatentfactorswhileeffectivelyavoidinginformationleak-
ageamongthem.ThebottomhalfofFigure5presentstheaverage
Weemployedthreelayersfortherepresentationalnetworkofeach weights of the respective features (between vertical lines) and the
latent factor (Γ, ∆, Υ, Ω). The hidden and output layer for Γ, ∆, remaining features for each representational network. This visual-
Υ,Ω consisted of 10,15 neurons across multiple experiments. We izationemphasizesthatourapproachselectivelyfocusesonrelevant
utilized Adam as the optimizer, and ELU served as the activation information for each network, leading to accurate identification of
function. The batch size was set to 256, the number of epochs to latentfactors.
5000 maximum, and the learning rate to 1e−5. Following the ap- Figure6illustratestheidentificationoflatentfactorsbasedonsec-
proachoutlinedin[27],weemployedPEHE nn onthevalidation ondcriterionofpermutationfeatureimportancetheory[6].Specifi-
settosavethebestmodel.Thedatasplitbetweentrainingandtesting cally,Figure6(a)vividlydemonstratesthatonlyΓand∆features
mirroredthatusedin[27,11],with20%ofthetrainingdatareserved
activelycontributetoincreasingBinaryCross-Entropy(BCE)loss.
for the validation set. We used the same settings for jobs and syn- Thisobservationsupportstheconclusionthatourmethodaccurately
theticdatasetsbutonlyemployed100dimensionalrepresentational identifiesΓand∆factorsfromthedata.Likewise,Figure6(b)con-
networkstoassignenoughcapacityforfaircomparisons. firms the successful identification of ∆ and Υ factors indicated by
Toselecthyper-parameters,weemployedgridsearchacrossdif- theincreasedMeanSquaredError(MSE).
ferent ranges (see Table 2). These parameter ranges were inspired The illustration in Figure 6 (c) shows again that DRI-ITE accu-
byvariousbaselinemethods.Codeisavailableathttps://github.com/ ratelydisentanglesandidentifiesΩ,aspermutingirrelevantfeatures
askhanatgithub/DRI_ITE. does not increase the PEHE, while permuting any relevant feature
does.Weconjecturethatthebaselinemethodsfailtocapturethefea-
5.4 Results tureimportancecompletely.Iftrue,thisshouldresultinoveralllower
ITEestimationerrors.
Weevaluateourmethodontwoevaluationcriteria:howaccuratelyit Investigating the synthetic dataset, it is evident from Figure 7
identifiesalldisentangledlatentfactors(Subsection5.4.1)andsec- that DRI-ITE consistently outperforms the baseline methods on
ondlyhoweffectivelyitestimatesthetreatmenteffectusingPEHE PEHEevaluation.AsthedimensionsofΩincrease,baselinemeth-
andpolicyrisk(R )criterion(Subsection5.4.2). ods experience a much stronger decline in performance. DRI-ITE
pol
demonstratesbetterperformance,particularlyinscenarioswithhigh-
dimensionalΩ.
5.4.1 IdentificationofDisentangledLatentFactors
In Table 1, we perform an ablation study to analyze the impact
of the components L and L on the PEHE compared to
Toquantify,howpreciselyourproposedmethodidentifiesalllatent orth recons
the basic loss (i.e., L +L +L ). The results show that
factorsinthedisentangledembeddingspacesweusethefollowing reg class disc
the addition of the orthogonal loss results in a minor improvement
metrics:
in performance, while the addition of the reconstruction loss leads
• Calculationofaverageweightvector:WecomputeW,defined to a significant decrease in the PEHE when ten or more irrelevant
astheproductofweightmatricesacrossalllayerswithinarepre- variablesareintroducedtotheoriginalvariableset.
sentationalnetwork,andW¯,whichrepresentstherow-wiseaver-
agevectoroftheabsolutevaluesofW foreachrepresentational 5.4.2 EvaluationonEstimationofTreatmentEffect
network.ThevectorW¯ providesinsightintotheaveragecontribu-
SuccessfullydisentanglinglatentfactorsincludingΩinitselfisnot
tionofeachfeaturewithinthatspecificrepresentationalnetwork.
enough, but ultimately we aim to have improved estimates of the
Inthecaseofsyntheticdata,wheretheassignmentoffeaturesto
latentfactorsisknown,wegeneratedpost-trainingplotsofW¯ for ITE.WeassessedtheperformanceofDRI-ITEusingPEHEonthe
IHDPbenchmarkdataset;andusingpolicyrisk(R )criterionon
each network. The rationale behind this analysis [32] lies in the pol
Jobsdataset.
expectationthattheaverageweightscorrespondingtofeaturesas-
WearecomparingourresultswithfourSOTAbaselinedisentan-
sociatedwithaparticularlatentfactorshouldexhibithighervalues
glementapproaches.
comparedtootherfeatures.
• Permutation feature importance analysis: Secondly, we em- • DisentangledRepresentationsforCounterfactualRegression:DR-
ployedpermutationfeatureimportancetheory[6]tovalidatethe CFR[11].Table3. PEHE(mean(std))onIHDPwithdifferentdimensionsofΩandvariedlatentdimensionsofrepresentationalnetworks(boldnumbersindicate
smallest/bestresults).
Latentdim=10 Latentdim=15
Data_Ω DR-CFR RLO-DRCFR TEDVAE TVAE DRI-ITE(Ours) DR-CFR RLO-DRCFR TEDVAE TVAE DRI-ITE(Ours)
IHDP_5 1.30(0.78) 1.33(0.81) 0.95(0.62) 1.25(0.38) 1.12(0.62) 1.19(0.62) 1.26(0.71) 0.93(0.62) 1.28(0.56) 1.06(0.60)
IHDP_10 1.48(0.94) 1.36(0.76) 1.18(0.80) 1.29(0.43) 1.12(0.65) 1.25(0.73) 1.34(0.72) 1.15(0.82) 1.31(0.46) 1.20(0.66)
IHDP_15 1.51(0.98) 1.37(0.78) 1.33(0.83) 1.43(0.58) 1.21(0.69) 1.29(0.73) 1.36(0.77) 1.35(0.86) 1.29(0.51) 1.23(0.65)
IHDP_20 1.52(1.01) 1.49(0.91) 1.42(0.94) 1.23(0.48) 1.23(0.65) 1.30(0.74) 1.30(0.70) 1.41(0.89) 1.23(0.48) 1.15(0.61)
• Learning Disentangled Representations for Counterfactual Re- servational data. While deep disentanglement-based methods have
gression via Mutual Information Minimization: RLO-DRCFR been widely employed, they face limitations in handling irrelevant
[3]. factors, leading to prediction errors. In the era of data-driven and
• TreatmentEffectwithDisentangledAutoencoder:TEDVAE[35]. big data approaches, where pre-screening for relevance is imprac-
• TargetedVAE:VariationalandTargetedLearningforCausalIn- tical, our work seeks to provide a robust solution to the inevitable
ference:TVAE[30] presenceofirrelevantfactorsinobservationalstudies.Wepresenta
novel approach that goes beyond traditional deep disentanglement
Table4. Policyrisk(mean(std))onJobswithdifferentdimensionsofΩ methodsbyexplicitlyidentifyingandrepresentingirrelevantfactors,
(boldnumbersindicatesmallest/bestresults). inadditiontoinstrumental,confounding,andadjustmentfactors.Our
Data_Ω DR-CFR RLO-DRCFR TEDVAE TVAE DRI-ITE(Ours)
methodleveragesadeepembeddingtechnique,introducingarecon-
Jobs_5 0.13(0.03) 0.13(0.03) 0.20(0.03) 0.14(0.01) 0.11(0.02) structionobjectivetocreateadedicatedembeddingspaceforirrele-
Jobs_15 0.12(0.03) 0.12(0.03) 0.21(0.04) 0.15(0.01) 0.12(0.04)
vantfactorsthroughanautoencoder.Ourempiricalexperiments,con-
Jobs_20 0.14(0.04) 0.13(0.04) 0.19(0.03) 0.22(0.08) 0.11(0.02)
ductedonsyntheticandreal-worldbenchmarkdatasets,demonstrate
theefficacyofourmethod.Weshowcaseanimprovedabilitytoiden-
We evaluated DRI-ITE on IHDP. Table 3 presents PEHE values
tifyirrelevantfactorsandachievemoreprecisepredictionsoftreat-
on the widely used IHDP benchmark dataset. The PEHE values
menteffectscomparedtopreviousapproaches.Whileourapproach
(mean ) are calculated from the first 30 realizations of IHDP, in-
(std) primarilyaddressesthescenariowithtwotreatmentgroups,infuture
corporatingdifferentdimensionsofΩandvaryinglatentdimensions
weplantoworkwithmultipleandcontinuoustreatments.
ofrepresentationalnetworks.
Again, the performance of SOTA methods tends to degrade
Acknowledgements
strongly with increasing dimensions of Ω. As depicted in Table 3,
DRI-ITEeffectivelymaintainsalowPEHEincomparisontobase- ThisworkhasbeensupportedbytheIndustrialGraduateSchoolCol-
linemethodsaftertheintroductionofΩ.Particularlynoteworthyis laborativeAI&RoboticsfundedbytheSwedishKnowledgeFoun-
thestruggleofbaselinemethods,inscenarioswithlow-dimensional dationDnr:20190128,andtheKnutandAliceWallenbergFounda-
representational networks, which supposedly suppress Ω through tionthroughWallenbergAI,AutonomousSystemsandSoftwarePro-
regularization. This results in the assimilation of information from gram(WASP).
Ω into other relevant factors, consequently leading to poor perfor-
mance.Incontrast,ourmethodadeptlydisentanglesΩandconsis-
References
tentlyoutperformsbaselinemethods.
However,astherepresentationalnetworksincreaseindimension- [1] L.Breiman.Randomforests.MachineLearning,45:5–32,2001.
[2] J. Brooks-Gunn, F. ruey Liaw, and P. K. Klebanov. Effects of early
ality, the performance of baseline methods also improves. We ob-
interventiononcognitivefunctionoflowbirthweightpreterminfants.
servedthatforthebaselines,regularizationbecomesmoreeffective TheJournalofPediatrics,120(3):350–359,1992.
insuppressingΩinhigh-dimensionalscenarioscomparedtolowdi- [3] M. Cheng, X. Liao, Q. Liu, B. Ma, J. Xu, and B. Zheng. Learning
disentangled representations for counterfactual regression via mutual
mensionalnetworks.Despitethis,ourapproachcontinuestoprovide
informationminimization.InACMSIGIRConferenceonResearchand
betterresults.However,TEDVAEshowsgoodperformanceagainst DevelopmentinInformationRetrieval,page1802–1806,2022.
smallnumberofΩbutitfailstoignoreΩwithhigherdimensions. [4] W.G.CochranandD.B.Rubin.Controllingbiasinobservationalstud-
Table 4 presents a comparison between baseline methods and ies:Areview.Sankhya¯:TheIndianJournalofStatistics,SeriesA,pages
417–446,1973.
DRI-ITEregardingpolicyrisk(R )criteria.Theseresultsareesti-
pol [5] R.H.DehejiaandS.Wahba. Causaleffectsinnonexperimentalstud-
mates(mean(std))derivedfromtheinitial30realizationsofthejobs ies:Reevaluatingtheevaluationoftrainingprograms. Journalofthe
dataset. Notably, the table illustrates that the performance of DRI- AmericanStatisticalAssociation,94(448):1053–1062,1999.
ITEremainsconsistentlybetterandunaffectedbytheinclusionofΩ. [6] A.Fisher,C.Rudin,andF.Dominici. Allmodelsarewrong,butmany
areuseful:Learningavariable’simportancebystudyinganentireclass
Incontrast,baselinemethodsexperienceadeclineinperformanceas
ofpredictionmodelssimultaneously.TheJournalofMachineLearning
thedimensionalityofΩincreases. Research,20,2018.
These results substantiate our assertion that SOTA methods lack [7] A.Fisher,C.Rudin,andF.Dominici. Allmodelsarewrong,butmany
areuseful:Learningavariable’simportancebystudyinganentireclass
anexplicitandreliablemechanismtodisentangleorignoreΩ.Con-
ofpredictionmodelssimultaneously.TheJournalofMachineLearning
versely,ourapproachconsistentlydisentanglesΩfactorsandreliably Research,20(177):1–81,2019.
estimatesITEacrossallscenariosincomparisontoSOTAmethods. [8] A. Gretton, A. Smola, J. Huang, M. Schmittfull, K. Borgwardt,
Moreover,theseresultsarestatisticallysignificantbasedonthet-test B.Schölkopf,etal. Covariateshiftbykernelmeanmatching. Dataset
ShiftinMachineLearning,3(4):5,2009.
withα=0.05.
[9] L.Grinsztajn,E.Oyallon,andG.Varoquaux. Whydotree-basedmod-
elsstilloutperformdeeplearningontypicaltabulardata? InAdvances
inNeuralInformationProcessingSystems,volume14,pages507–520,
6 Conclusion
2022.
[10] I.GuyonandA.Elisseeff.Anintroductiontovariableandfeatureselec-
Inthispaper,weaddresstheproblemoflearningdisentangledrepre-
tion. TheJournalofMachineLearningResearch,3(Mar):1157–1182,
sentationforIndividualTreatmentEffect(ITE)estimationwithob- 2003.[11] N.HassanpourandR.Greiner. Learningdisentangledrepresentations
forcounterfactualregression.InInternationalConferenceonLearning
Representations,2019.
[12] N.HassanpourandR.Greiner. Counterfactualregressionwithimpor-
tancesamplingweights. InternationalJointConferenceonArtificial
Intelligence,7:5880–5887,2019.
[13] J.L.Hill.Bayesiannonparametricmodelingforcausalinference.Jour-
nalofComputationalandGraphicalStatistics,20(1):217–240,2011.
[14] G.W.ImbensandD.B.Rubin. CausalInferenceforStatistics,So-
cial,andBiomedicalSciences:AnIntroduction. CambridgeUniversity
Press,2015.
[15] F.D.Johansson,U.Shalit,andD.Sontag. Learningrepresentations
forcounterfactualinference. InInternationalConferenceonMachine
Learning,2016.
[16] B.Kim,H.Kim,K.Kim,S.Kim,andJ.Kim. Learningnottolearn:
Trainingdeepneuralnetworkswithbiaseddata. InInProceedingsof
theIEEE/CVFconferenceoncomputervisionandpatternrecognition,
pages9012–9020,2019.
[17] K.Kuang,P.Cui,B.Li,M.Jiang,S.Yang,andF.Wang. Treatment
effectestimationwithdata-drivenvariabledecomposition. AAAICon-
ferenceonArtificialIntelligence,31,2017.
[18] K. Kuang, P. Cui, B. Li, M. Jiang, Y. Wang, F. Wang, and S. Yang.
Treatmenteffectestimationviadifferentiatedconfounderbalancingand
regression. ACM Transactions on Knowledge Discovery from Data
(TKDD),31:1–25,2019.
[19] K.Kuang,P.Cui,H.Zou,B.Li,J.Tao,F.Wu,andS.Yang. Data-
driven variable decomposition for treatment effect estimation. IEEE
TransactionsonKnowledgeandDataEngineering,34(5):2120–2134,
2020.
[20] R.J.LaLonde. Evaluatingtheeconometricevaluationsoftrainingpro-
gramswithexperimentaldata. TheAmericanEconomicReview,76(4):
604–620,1986.
[21] S.Li,N.Vlassis,J.Kawale,andY.Fu. Matchingviadimen-sionality
reductionforestimationoftreatmenteffectsindigitalmarketingcam-
paigns. In International Joint Conference on Artificial Intelligence,
page3768–3774,2016.
[22] C.Louizos,U.Shalit,J.Mooij,D.Sontag,R.Zemel,andM.Welling.
Causaleffectinferencewithdeeplatent-variablemodels. Advancesin
neuralinformationprocessingsystems,30,2017.
[23] J.Pearl. Onaclassofbias-amplifyingcovariatesthatendangereffect
estimates. In Proceedings of the 26th Conference on Uncertainty in
ArtificialIntelligence,2010,pages417–424,2010.
[24] P.R.Rosenbaum.Model-baseddirectadjustment.JournaloftheAmer-
icanStatisticalAssociation,page387–394,1987.
[25] P. R. Rosenbaum and D. B. Rubin. The central role of the propen-
sityscoreinobservationalstudiesforcausaleffects. Biometrika,page
41–55,1983.
[26] D.B.Rubin. Causalinferenceusingpotentialoutcomes:Design,mod-
eling,decisions. JournaloftheAmericanStatisticalAssociation,100
(469):322–331,2005.
[27] U.Shalit,F.D.Johansson,andD.Sontag. Estimatingindividualtreat-
ment effect: generalization bounds and algorithms. In International
ConferenceonMachineLearning,2017.
[28] E.Tuv,A.Borisov,G.Runger,andK.Torkkola.Featureselectionwith
ensembles,artificialvariables,andredundancyelimination.TheJournal
ofMachineLearningResearch,10:1341–1366,2009.
[29] C.Villanietal.Optimaltransport:oldandnew,volume338.Springer,
2009.
[30] M. J. Vowels, N. C. Camgoz, and R. Bowden. Targeted vae: Varia-
tionalandtargetedlearningforcausalinference. In2021IEEEInter-
nationalConferenceonSmartDataServices(SMDS),pages132–141.
IEEE,2021.
[31] P.Wei,Z.Lu,andJ.Song. Variableimportanceanalysis:Acompre-
hensivereview.ReliabilityEngineering&SystemSafety,142:399–432,
2015.
[32] A.Wu,J.Yuan,K.Kuang,B.Li,R.Wu,Q.Zhu,Y.Zhuang,andF.Wu.
Learningdecomposedrepresentationsfortreatmenteffectestimation.
IEEETransactionsonKnowledgeandDataEngineering,35(5):4989–
5001,2023.
[33] P.WuandK.Fukumizu. Betaintactvae:Identifyingandestimating
causaleffectsunderlimitedoverlap. arXivpreprintarXiv:2110.05225,
2021.
[34] L.Yao,S.Li,Y.Li,M.Huai,andJ.G.A.Zhang.Representationlearn-
ingfortreatmenteffectestimationfromobservationaldata. Advances
inneuralinformationprocessingsystems,page2633–2643,2018.
[35] W.Zhang,L.Liu,andJ.Li.Treatmenteffectestimationwithdisentan-
gledlatentfactors. ProceedingsoftheAAAIConferenceonArtificial
Intelligence,35,2021.