EXPLORING LARGE LANGUAGE MODELS TO GENERATE EASY TO
READ CONTENT
PalomaMartínez LourdesMoreno
ComputerScienceandEngineeringDepartment ComputerScienceandEngineeringDepartment
UniversidadCarlosIIIdeMadrid UniversidadCarlosIIIdeMadrid
Leganés,Madrid,Spain Leganés,Madrid,Spain
pmf@inf.uc3m.es lmoreno@inf.uc3m.es
AlbertoRamos
ComputerScienceandEngineeringDepartment
UniversidadCarlosIIIdeMadrid
Leganés,Madrid,Spain
albramos@pa.uc3m.es
July30,2024
ABSTRACT
Ensuringtextaccessibilityandunderstandabilityareessentialgoals,particularlyforindividualswith
cognitiveimpairmentsandintellectualdisabilities,whoencounterchallengesinaccessinginformation
acrossvariousmediumssuchaswebpages,newspapers,administrativetasks,orhealthdocuments.
InitiativeslikeEasytoReadandPlainLanguageguidelinesaimtosimplifycomplextexts;however,
standardizing these guidelines remains challenging and often involves manual processes. This
workpresentsanexploratoryinvestigationintoleveragingArtificialIntelligence(AI)andNatural
LanguageProcessing(NLP)approachestosystematicallysimplifySpanishtextsintoEasytoRead
formats,withafocusonutilizingLargeLanguageModels(LLMs)forsimplifyingtexts,especially
ingeneratingEasytoReadcontent. ThestudycontributesaparallelcorpusofSpanishadaptedfor
EasyToReadformat,whichservesasavaluableresourcefortrainingandtestingtextsimplification
systems. Additionally,severaltextsimplificationexperimentsusingLLMsandthecollectedcorpus
areconducted,involvingfine-tuningandtestingaLlama2modeltogenerateEasytoReadcontent. A
qualitativeevaluation,guidedbyanexpertintextadaptationforEasytoReadcontent,iscarriedout
toassesstheautomaticallysimplifiedtexts. Thisresearchcontributestoadvancingtextaccessibility
forindividualswithcognitiveimpairments,highlightingpromisingstrategiesforleveragingLLMs
whileresponsiblymanagingenergyusage.
Keywords Largelanguagemodel·textsimplification·plainlanguage·EasytoRead·digitalaccessibility·Natural
LanguageProcessing
1 Introduction
In a society increasingly saturated with information, the ability to understand digital content has become a real
challengeformanypeople. DespitethewidespreadaccesstoinformationfacilitatedbyInformationandCommunication
Technologies(ICT),aconsiderablenumberofpeopleencounterdifficultiesinunderstandingtextualcontentbecause
noteveryonecanreadfluently,andtheinformationwrittencanexcludemanypeople.
Thechallengeofunderstandingtextscontaininglongsentences,unusualwords,andcomplexlinguisticstructurescan
posesignificantaccessibilitybarriers. Thegroupsofusersdirectlyaffectedincludepeoplewithintellectualdisabilities
4202
luJ
92
]LC.sc[
1v64002.7042:viXraExploringlargelanguagemodelstogenerateEasytoReadcontent
andindividualswithcognitiveimpairments,butitalsoimpactsthosewithliteracyorcomprehensionproblems,the
elderly,theilliterate,andimmigrantswhosenativelanguageisdifferent.
In Spain alone, more than 285,684 individuals with intellectual disabilities face challenges in understanding texts
nottailoredtotheirneedsImserso[2022]. Additionally,over435,400individualswithacquiredbraininjury(ABI)
encounterobstaclesinreadingcomprehension,oftenasaresultofstrokesortraumaticbraininjuriesInstitutoNacional
deEstadística(INE)[2022]. Theagingpopulationaddsanotherlayertothisissue. Withnearly20%oftheSpanish
populationbeingover65yearsoldandaglobaltrendtowardanagingpopulationWorldHealthOrganization[n.d.],the
needtoadapttextualcontenttothecognitiveneedsofolderindividualsisclear. Theseindividualsmayexperience
a natural decline in reading comprehension abilities. By 2050, the global population over 60 years is expected to
almostdouble,emphasizingtheurgencytoaddresscognitiveaccessibilitybarriers. Also,accordingtoWorldHealth
Organization[2019],withapproximately50millionpeopleaffectedbydementiaworldwide,andanewcaseevery
threeseconds,thenumberofpeoplewithdementiaisexpectedtotripleby2050. Furthermore,individualswithlow
educationallevelsfacemoresignificantchallenges. Despiteover86%oftheworld’spopulationbeingabletoreadand
write,disparitiesinreadingcomprehensionremainprofound. ThisisevidentinthePIAACresults,whichrevealthatthe
Spanishpopulationaged16to65scoresbelowtheOECDandEUaverageinreadingcomprehension. Thesestatistics
underscorethesignificantimpactofcognitiveaccessibilitybarriersonvariousreadergroupsandtheneedtoprovide
simplifiedadaptedtextswithrelevantinformationtocitizens.
Toprovideuniversalaccesstoinformationandmaketextsmoreaccessible,thereareinitiativesliketheEasytoReadand
PlainLanguageguidelines. However,standardizingtheseguidelinesischallengingduetotheabsenceoftoolsdesigned
tosystematicallysupportthesimplificationprocesses. Websitesthatoffersimplerversionsoftextscurrentlyrelyon
manualprocesses. Asasolution,therearemethodsofArtificialIntelligence(AI)andNaturalLanguageProcessing
(NLP)usinglanguageresourcesandmodels.
This work presents an exploratory study on how approaches from these disciplines can be utilized to support the
systematicfulfillmentofsimplifyingthecomplexityofSpanishtexts,withkeypremisesbeingtheuseofeasyreading
guidelinesandresources,andensuringtheparticipationofindividualswithdisabilitiesorexpertsinthefield. inlight
ofthehighenergyconsumptionoflargegenerativeAImodels,whichisnotenvironmentallysustainable,andtheir
associatedcoststhatcouldincreasesocialinequalities,thisworkaimstoexploremorecost-effectiveandenergy-efficient
solutions.
Thearticleisorganizedasfollows: Sections1and2explainthemotivationbehindthisresearch,aswellasstate-of-the-
artNLPtechniquesappliedtotextsimplification,withaspecialfocusoncreatingEasytoReadcontent. Sections3
and4covertheproposedmethodology,includingthedatasetsusedintrainingandtesting,thetechnicalarchitecture
deployedtogenerateEasytoReadcontent,theLLMsexplored,andtheexperimentalconfigurations. Sections5and6
provideapreliminaryanalysis,aswellassomeinsights.
2 Background
This section encompasses a discussion on strategies aimed at making text understanding and accessible, detailing
theirspecificimplementationintheSpanishlanguageandalignmenttostandardswithinSpain,alongsideareviewof
researchintextsimplification,especiallywithintherealmsofNLPandAI.
2.1 UnderstandingStrategies: EasytoReadandPlainLanguage
Historically,InclusionEuropecraftedstandardsforthosewithintellectualdisabilities,emphasizingsimplicityand
the importance of feedback to ensure clarity, a cornerstone of the Easy to Read method for making information
understandabletothisgroupFreyhoffetal.[1998]. Incontrast,theInternationalFederationofLibraryAssociations
andInstitutions(IFLA1)promotedbroader,moreadaptableguidelines,reflectinganunderstandingofaccessibility
fordiversereadinglevels. Thisapproachbroadenedthescopeofinformationaccessibility. Astimepassed,theEasy
toReadmethodologyevolvedNomuraetal.[2010],continuallyrefiningitsstrategiestoservethespecificneedsof
individualswithintellectualdisabilities.
Another approach is Plain Language, which historically predates Easy to Read and has been used in some fields,
suchaslegalscopeWydick[1979],buthasexperiencedaresurgenceinrecenttimes. TherevivalofPlainLanguage
underscoresitsgrowingsignificanceinaccessiblecommunication,reflectinganexpandedrecognitionoftheneedfor
clear,understandableinformationNomuraetal.[2010],EuropeanCommission. Directorate-GeneralforTranslation
[2011]. Emphasizing direct, concise communication, Plain Language aims to be accessible to a wide audience,
1https://www.ifla.org/
2ExploringlargelanguagemodelstogenerateEasytoReadcontent
including those with cognitive impairments and the general public, like individuals with limited reading skills or
non-nativespeakers. The goalofPlainLanguage isdrivenbyanincreasedunderstanding ofitsrolein promoting
transparency. Varioussectorssuchasgovernment,healthcare,legal,andbusinessesareadoptingPlainLanguageto
improvecommunicationwiththeirstakeholders,highlightingitsuniversalimportanceincreatinganinclusivesociety
whereinformationisaccessibletoeveryone,irrespectiveoftheirreadinglevelorbackground.
ThecomparisonbetweenEasytoReadandPlainLanguagecanbeunderstoodthroughseveralkeyaspects,eachcatering
to different needs and audiences. Easy to Read is primarily designed for individuals with cognitive or intellectual
disabilities, including those facing learning difficulties. This approach places a high emphasis on structural and
linguisticsimplicity,aimingtoreducecognitiveloadbyusingshortandsimplesentences,clearanddirectlanguage,and
incorporatingspecificimagesandsymbolstoaidcomprehension. AdistinctivefeatureofEasytoReadisitspractice
ofinvolvingthetargetaudienceintestingandreviewingtextstoensureclarityandeaseofunderstanding,directly
obtainingfeedbacktorefinethecontent. Incontrast,PlainLanguageisdirectedtowardabroaderaudience,including
thegeneralpopulation,peoplewithreadinglimitations,andnon-nativespeakers,makingitapplicableacrossdiverse
groups. Itsfocusisonclarity,conciseness,andlogicalorganization,employingtechniquessuchasreader-centered
organization,activevoice,andtheuseofcommonandeverydaywordstoeliminateambiguity. Thisapproachaimsto
makeinformationaccessibletoasmanyreadersaspossible,withlessemphasisonvisualaidsandastrongfocuson
avoidingjargontopromotegeneralaccessibility.
Theadvantagesofeachapproacharenotable. EasytoRead’stailoredsupportforindividualswithspecificneedsensures
thatcontentisaccessibletothosewhofacesignificantchallengesinreadingandunderstandingstandardtexts. Theuse
ofvisualsfurtherenhancescomprehension,makingitahighlyeffectivemethodforitstargetaudience. Ontheother
hand,PlainLanguage’sbroadapplicabilityensuresthatawiderangeofreaders,regardlessoftheirreadingabilitiesor
linguisticbackgrounds,canaccessandunderstandinformationeasily. Itsprinciplesofclarityenhanceunderstanding
forall,makingitatoolforpromotingclearcommunication.
The application domains also differ, with Easy to Read commonly used in educational materials, and legal and
government documents designed for people with intellectual disabilities, while Plain Language is widely used in
government communications, legal, health, educational documents, business documents, and websites aimed at a
generalaudience.
Bothapproachesstrivetomakeinformationaccessibletoall,highlightingtheneedforclearcommunicationtofoster
a more inclusive society. However, it’s important to note that simply applying some of their guidelines does not
automaticallyqualifyatextasEasytoReadorPlainLanguage. Atextmustadheretoalltheirrespectiveguidelinesto
betrulyconsideredassuch.
2.2 EasytoReadinSpain
InSpain,thesignificantadvancementinaccessiblecommunicationwashighlightedbytheapprovalandpublicationof
theworld’sfirstEasyReadstandard,UNE[2018]. Basedonthisstandard,theEasytoReadmethodologyisbuiltupon
threekeyprinciples: treatingEasytoReadasamethodforpublishingdocumentsthataddressbothwritinganddesign,
involvingthetargetgroupinthecreationprocesstoensurethefinalproductmeetstheirneeds,anddefiningEasyto
Readasatoolforpeoplewithreadingdifficultiesratherthanforthegeneralpopulace.
A literature review on text simplification in Spain shows limited research aimed specifically at producing Easy to
Read texts following these principles. Relevant initiatives that utilize AI and NLP methods, such as the Simplext
project Saggion et al. [2015], which aimed to develop an automated Easy Language translator, and the Flexible
InteractiveReadingSupportTool(FIRST)Barbuetal.[2015],focusedontextsimplificationforimprovedaccessibility,
demonstrate some progress in this area. However, despite the exploration of Easy to Read best practices, the full
implementation of writing guidelines for complete adaptation to Spanish standards is rare. Additionally, much of
theresearchontextsimplificationtendstooverlooktheparticipationofindividualswithdisabilitiesAlarconetal.
[2023]. Theseeffortsmainlyconcentrateonwritingguidelines—spelling,grammar,vocabulary,andstyle—andless
ondocumentdesignandlayout. Itisworthmentioningpreviousworkbytheauthorsonalexicalsimplificationtool
thatdidaddressenhancingsimplificationwithvisualelementslikepictogramsandprovidedsimpledefinitionsand
synonymsinaglossaryformat(EASIER)Alarconetal.[2021]Morenoetal.[2023]Alarconetal.[2022]. Otherworks
focusonsupportingEasytoReadexpertsintheirdailytaskoftextadaptationSuárez-Figueroaetal.[2022].
ThisresearchaimstobridgethesegapsbyfollowingtheprinciplesoutlinedintheUNE15310:20181standard,explicitly
focusingonrigorouslyinvolvingpeoplewithintellectualdisabilitiesandindividualswithcognitiveimpairments,in
additiontoaddressingthewriting-relatedguidelines. Ourgoalistoprovidecomprehensivesupportforthepractical
applicationofEasytoReadinSpain,coveringboththesimplificationprocessesandtheprofessionalneedsofEasyto
Readexperts.
3ExploringlargelanguagemodelstogenerateEasytoReadcontent
2.3 RelatedWork
Inthelastyears,thecommunityofNLPandAIresearchersaddressedsolutionstoautomaticallytranslatetextsinto
simpleronesbuttherearenoapproachestoautomaticallygenerateEasytoReadcontentfromtexts. Generationofthis
typeofcontentisacomplextaskbecausetherequirementsofthetargetaudience(peoplewithintellectualdisabilities)
shouldbetakenintoaccountshouldbetakenintoaccountasintroduced.
Simplificationcanbeapproachedasanall-in-oneprocessorasamodularapproach. Inamodularapproach,there
is a pipeline composed of separated processes, for instance, replacing complex words with simpler ones, dividing
coordinatedorrelativesentencesintosimplesentences,replacingpassivesentenceswithactiveones,etc. Inanall-in-
oneapproachsimplificationisdoneinasinglestep,suchasthosebasedongenerativemodels. Firstpreviousworks
werefocusedonrule-basedsimplificationapproachestoreducesyntacticalcomplexityinsubordinateorcoordinated
sentences once a morpho-syntactic analysis is done using a set of hand-crafted rules, such as Siddharthan [2006].
Additionally,otherapproachesusedmachinelearningapproachestodiscoverpatternsinparallelcorporafollowing
statisticalmachinetranslationsuchasSpecia[2010]andCosterandKauchak[2011]. Seeforadetailedsurveyontext
simplificationfollowingtheseapproaches. Morerecently,neuraltextsimplificationmodelssuchastransformed-based
approacheshaveemerged. Researchfocusonencoder-basedmodels,likeBERT,thatispretrainedtopredictaword
givenleftandrightcontextusingahighvolumeoftextdata. Otheradditionaltrainingmethodscanbedone,forinstance,
nextsentenceprediction. Thesepretrainedtaskscanbeleveragedtosimplifywordsinatextorsimplifycomplex
sentences,Qiangetal.[2020],Martinetal.[2019].
Traditionalmachinelearningapproachesorfine-tuningofLargeLanguageModels(LLM)requiredatasetsoflabeled
texts, and easy dictionaries among others used in training and testing systems. Manual annotation is costly and
time-consuming. Inaddressingtheneedforcomprehensiveresources,weconductedasurveyofcorpusresourcesinthe
Spanishlanguage. ThisefforthighlightsdifferentinitiativesinthefieldoflexicalsimplificationinSpanish. Parallel
corpora,whichincludebothoriginaltextsandtheirsimplifiedversions,areextremelyvaluabletoolsfortrainingtext
simplificationalgorithms,especiallyinlanguageswithlimitedresources,suchasSpanish. Themostcommoncorpora
arethosecomposedofasetoforiginalsentencesandtheiralignedsimplifiedversions. Asaresultofthisliterature
review, Table 1 compiles some corpora and datasets for text simplification in Spanish describing the source, type,
annotators,andsize.
Focusing on lexical simplification, three works on the Spanish language are highlighted: (1) in Bott et al. [2012],
an unsupervised system for lexical simplification in Spanish, which utilizes an online dictionary and the web as a
corpusisintroduced. Threefeatures(wordvectormodel,wordfrequency,andwordlength)arecalculatedtoidentify
themostsuitablecandidatesforthesubstitutionofcomplexwords. Thecombinationofasetofrulesanddictionary
lookupallowsforfindinganoptimalsubstituteterm. (2)Ferrésetal.[2017]describestheTUNERCandidateRanking
System,anadaptationoftheTUNERLexicalSimplificationarchitecturedesignedtoworkwithSpanish,Portuguese,
andEnglish. Thissystemsimplifieswordsincontext,omittingthecompletesimplificationofsentences. Thesystem
proposes four phases: sentence analysis, disambiguation of word senses (WSD), synonym classification based on
wordformfrequencyusingcountsfromWikipediaintherespectivelanguage,andmorphologicalgeneration. (3)Our
previousworkinAlarconetal.[2021],aneuralnetwork-basedsystemforlexicalsimplificationinSpanishthatuses
pre-trainedwordembeddingvectorsandBERTmodelsisdescribed. Thesesystemswereevaluatedinthreetasks:
complexwordidentification(CWI),SubstituteGeneration(SG),andSubstituteSelection(SS).InthecaseoftheCWI
task, the shared tasks dataset for CWI 2018 Yimam et al. [2017] in Spanish was used. For SG and SS tasks, the
evaluationwascarriedoutusingtheEASIER-500corpusAlarconetal.[2023]. Thefourthtask,substituteclassification
(SR),wasnotevaluatedduetotheabsenceofSpanishdatasetsforlexicalsimplificationthatcouldbeusedforthat
purpose.
AfteranalyzingEasytoReadguidelinesandasaresultofthisanalysis,wepresentinTable3aproposedsetofEasyto
ReadguidelinesbasedontheSpanishstandardandEuropeanGuidelinesinSpanishILSMH[1998]. Thisproposed
setofguidelinesdrivestheautomatedadaptationprocess,influencingthedesignofpromptinputsforthemodelsand
alsoservingasabenchmarkforthevalidationoftheexperimentation. Thisworkaimstofine-tunedecoder-based
modelssuchasLlama2Touvronetal.[2023]usingdatasetscomposedofEasytoReadSpanishdocumentsasaproof
ofconceptingeneratingEasytoReadcontent.
3 Method
Inthissection,themethodologyofthisresearchisdescribed,includingthedatasetusedtotrainandtestthegenerative
LLMstosimplifytextsinEasytoReadformats,themethodusedtoaligntheoriginalsentenceswiththecorresponding
simplification(ifexists),theLLMarchitecturechosenandfinallytheevaluationprocedure. Theuseofanopen-source
generative model is proposed, highlighting its capacity to address complex language-related tasks. This approach
4ExploringlargelanguagemodelstogenerateEasytoReadcontent
Table1: SurveyStudyofCorporaforTextSimplificationinSpanish
Corpus Typeandsource Annotators Size
Simplext,Štajner News stories from Trainedhumaneditor 200shortnewsarticles
etal.[2015] Servimedia
RANLP2017,Yi- Wiki news and 54 turkers (Native and 14,280sentenceswithtargetcom-
mametal.[2017] Wikipediaarticles non-nativespeakers) plexwords
PPDB-S/M Šta- TextsfromEuroparl, Built by filtering and or- 5,709unigramsforSmall(S)DB
jneretal.[2019] Wikipedia and sim- dering paraphrases pairs sizeand15,524unigramsforM
pleWikipedia fromPPDB (LargerDB)size
CASSA Štajner OpenThesaurusand All unique 5-grams pairs 5,640,6945-grams
etal.[2019] EuroWordnet fromCASSA
CWI 2018 Yi- WikiNews and 10annotators,amixofna- 17605annotatedwords
mametal.[2017] Wikipedia tiveandnon-nativespeak-
ers
ALexS 2020 Transcriptions of 430students 55 texts with 9,175 words, 723
Ortiz-Zambranoa academicvideos annotatedascomplexwords
and Montejo-
Ráezb[2020]
EASIERAlarcon Newsarticles 3 native Spanish annota- 3977 sentences annotated with
etal.[2023] tors(peoplewithintellec- 8155 complex words and 3396
tualdisabilities,expertlin- sentences annotated with 7892
guists and specialists in suggested synonyms (121,064
EasytoRead) words)
NewselaXuetal. Newsarticles Manually produced by Parallelsimple-complexarticles
[2015] professionaleditors with 11-grade levels (1,130 arti-
cles)
FinTOC2022El- Financial docu- Manualprocessingbyan- 90 financial documents with an
Hajetal.[2022] ments(FinT-esp) notators averageof148tagsperdocument
(250,000words)
TSAR Štajner WikiNews and Prolific2annotators 368instanceswiththesentences
etal.[2022] Wikipedia (CWI target complex words, and gold
2018) annotations(31,021words)
ALEXSIS Ferrés CWI Shared Task Prolificannotators 381sentenceswithatargetcom-
and Saggion 2018 Spanish plexword,and25candidatesub-
[2022] dataset(Newstexts) stitutions
CLARA-MeD Drug leaflets, sum- Manually annotated by A collection of 24,298 pairs of
Campillos- maries of products, pairsofexpertannotators professionalandsimplifiedtexts
Llanos et al. abstracts, cancer- (96Mtokens)
[2022] related summaries,
andclinicaltrials
involvesfine-tuningthemodelusingacorpuscontainingtextsadaptedforeasyreading,toachievespecificadaptation
to readability requirements, primarily targeting individuals with intellectual disabilities and difficulties in reading
comprehension. The contributions of this work are highlighted in this section, including the creation of a parallel
corpusresultingfromanalignmentprocess,whichservesasaresourceofimmensevalueforfutureresearchintext
simplificationinEasytoReadformats. Theuseofanopen-sourcegenerativemodelisemphasized,whichisadapted
throughafine-tuningprocesstogenerateEasytoReadcontent.
3.1 Dataset
TheAmasFácilService3,anentitythatoffersaspecificserviceoftextadaptationforeasyreadinginadditiontoother
cognitiveaccessibilityservicesforindividualswithintellectualdisabilities,hasprovidedvarioussetsofgeneraldomain
documents. Thesedocumentsfeaturetheoriginaltextanditscorrespondingadaptationforeasyreading,tailoredfor
userswithintellectualdisabilitieswhoencounterchallengesinreadingcomprehension. Thisdatasetemergesasa
significant source of data in the simplification. These texts are adapted for users with intellectual disabilities who
3https://amasfacil.org/
5ExploringlargelanguagemodelstogenerateEasytoReadcontent
experiencereadingcomprehensiondifficulties. Fromthesetexts,asubsetof13documentsrelatedtosportsguides,
literature,competitiveexaminations,andexhibitionshasbeenselected.
Thecorpushasatotalof1941sentences,56,212wordsoftheoriginaltextand40,372wordsofadaptedtexts,withan
averageof27wordsinthesourcesentenceand20wordsinanadaptedsentence. Theoriginaltextscontaintermsand
expressionsthatimpedecomprehensionforindividualswithintellectualdisabilities.
Theoriginalsourcetextsandtheirrespectiveadaptationsfollowadifferentlanguageandstyledependingonthetopicof
thetext. Forinstance,thelanguageusedinliteraturetextsisentirelydifferentfromthatemployedinasportsguide. We
haveemployedtextsfromdifferentthemesbecausethereisalimitationregardingthenumberoftextsadaptedforeasy
readinginSpanish. Conversely,moreresourcesareavailablefortextswithlexicalsimplificationandadaptedtoplain
language,butthisdeviatesfromthefocusonaccessibilityandinclusionforindividualswithintellectualdisabilities.
3.2 Textselection
Inthemethodologyemployedforsentenceselection,initially,anysentencesfromtheoriginaltextsthatdidnothave
aneasyreadingadaptationwereexcluded. Thisexclusionensuresthatonlytextswithboththeoriginalandadapted
versionsareconsideredforanalysis. Inourapproach,eachoriginalsentenceisuniquelyidentifiedandpairedwith
itspotentialadaptations. Thesepairsarethenevaluatedbasedonthecosinesimilaritybetweentheoriginalsentence
anditsadaptation. Aspartofthecosinesimilarityevaluationprocess,anyoriginalsentenceslackingacorresponding
candidateadaptationareremoved.
Subsequently, fromthepoolofcandidateadaptationsforeachoriginalsentence, weselecttheadaptationwiththe
highestcosinesimilarityscore. Thisprocessguaranteesthatforeachoriginalsentence,weidentifythemostclosely
alignedadaptedversionforeasyreading.Thisselectionstrategyresultsinacuratedsetofsentencepairs,eachconsisting
ofanoriginalsentenceanditsoptimaleasyreadingadaptation. Table2providesanoverviewofthedistributionoftexts
andsentencesacrossvarioustopics,illustratingthebreadthofcontentconsideredinouranalysis.
Adaptingtextstoenhancereadabilityofteninvolvessignificantsimplification,includingtheomissionofwords,entire
sentences,orpartsthereof. Thislevelofmodificationintroduceschallengesinaccuratelyidentifyingcorresponding
sentencesbetweentheoriginalandadaptedversions. Toovercomethischallengeandfacilitatethecollectionofparallel
data,wehavedevelopedaspecializedsentencealignerwhichispresentedinthefollowingsection.
Table2: Distributionoftextsbytheme
Theme Totaltext Totalsentences
Sport 3 480
Literature 2 1118
Exhibitions 2 72
Competitiveexaminations 5 271
3.3 SentenceAlignmentProcess
Theadaptationoftextstoenhancereadabilityofteninvolvessignificantsimplification,includingtheremovalofwords,
entiresentences, orpartsthereof. Thislevelofmodificationintroduceschallengesinaccuratelyidentifyingwhich
sentencescorrespondbetweentheoriginaltextanditsadaptedversion. Toaddressthisissueandfacilitatethecollection
ofparalleldata,wehavedevelopedasentencealigner.
Themainfunctionofthisaligneristoprocesssentencesfromthetexts,aligningthembetweentheoriginaltextand
itsadaptedversionforeasyreading. Initially,thetextsaresegmentedintoindividualsentencestogeneratesentence
embeddings. Theseembeddingsarevectorrepresentationsthatencapsulatesemanticinformationaboutthesentences,
enablingthecomparisonandmeasurementofsemanticsimilaritybetweensentences. Forgeneratingtheseembeddings,
weutilizetheSentenceTransformersframeworkReimersandGurevych[2019],whichoffersasuiteofpre-trained
modelsspecificallyoptimizedforthetaskofsentencealignment.
Upongeneratingthesentenceembeddings,wedeterminethesemanticsimilaritybetweenthembycomputingthecosine
similarity. TheLaBSEmodelFengetal.[2020],aBERTDevlinetal.[2018]modification,isemployedforthispurpose.
Optimizedtoproduceanalogousrepresentationsforpairsofsentences,thismodelassessesthecosinesimilarityacross
allsentencepairs,selectingthemostsimilarsentenceamongpotentialcandidatesasthecorrectalignment.
It’simportanttohighlightthatthedevelopmentofthissentencealignerandthesubsequentcreationofaparallelcorpus
representsignificantcontributionstothisresearch. Theparallelcorpus,whichisintheprocessofbeingpublished,will
offeravaluableresourcetothescientificcommunity,aimingtoenhancetextsimplificationefforts.
6ExploringlargelanguagemodelstogenerateEasytoReadcontent
3.4 GeneratingEasytoReadtexts
InlinewithourinitialcommitmenttoaddresstheenvironmentalandeconomicimpactsoflargegenerativeAImodels,
thissectiondemonstratesapracticalapplicationthatalignswithourgoalsofsustainabilityandinclusivity. Thereliance
onenergy-intensivemodelsposessignificantenvironmentalchallengesandexacerbatessocialinequalitiesthroughtheir
substantialmonetarycosts. Tomitigatetheseissues,wehaveselectedLlama2,anopen-sourceTouvronetal.[2023]
modelprovidedbyHuggingFace(Llama-2-7b-hf),foritsefficiencyandadaptabilityingeneratingEasytoReadcontent.
Also,ithasshownconsiderablepromiseinexecutingcomplexreasoningtasksacrossabroadspectrumofdomains,
fromgeneraltohighlyspecializedfields,includingtextgenerationbasedonspecificinstructionsandcommands.
The auto-regressive Llama2 transformer is initially trained on an extensive corpus of self-generated data and then
fine-tunedtohumanpreferencesusingReinforcementLearningwithHumanFeedback(RLHF)Touvronetal.[2023].
Whilethetrainingmethodologyissimple,thehighcomputationalrequirementshaveconstrainedthedevelopmentof
LLMs. Llama2istrainedon2billiontokensoftextdatafromvarioussourcesandhasmodelsrangingfrom7Bto70B
parameters. Additionally,theyhaveincreasedthesizeofthepretrainingcorpusby40%,doubledthemodelcontext
lengthto4096tokens,andadoptedclusteredqueryattentiontoenhancethescalabilityofinferenceinthelarger70B
model.
Since Llama2 is open-source and has a commercial license, it can be used for a multitude of tasks such as lexical
simplification. However,toachievebetterresultsandperformanceinthetask,itisnecessarytofine-tuneitwiththe
specificdataandadaptittothereadabilityneedsofindividualswithintellectualdisabilities. Therefore,theweightsand
parametersofthepre-trainedmodelwillbeadjusted,resultinginincreasedprecisionintaskoutcomes,aswellasa
reductioninthelikelihoodofinappropriatecontent.
AkeycontributionofthisworkisthedemonstrationoftheadaptabilityofLlama2forthegenerationofEasytoRead
texts,markingasignificantsteptowardscustomizingadvancedlanguagemodelsforenhancedaccessibility.
3.5 Evaluationprocedure
Invalidatingourmodel,wedivergefromusingtraditionalperformancemetricscommonintextsimplificationtasks,
suchasBLEUPapinenietal.[2002],SARIXuetal.[2016],andtheFlesch-KinkaidreadabilityindexKincaidetal.
[1975],amongothers. Instead,weprioritizeaqualitativehumanevaluationconductedbyprofessionaleasyreading
adaptersandindividualswithintellectualdisabilities. Whiletraditionalmetricscangaugeamodel’seffectivenessto
someextent,theydonotadequatelycapturethenuancesoflanguageunderstanding,simplicity,andthepreservationof
meaninginpracticalcontexts.
Engagingprofessionaleasyreadingadaptersinourevaluationprocessiscriticaltoensuringtheresultsaregenuinely
accessibleandbeneficialforpeoplewithintellectualdisabilities. Thishuman-centricapproachisvitalinthecontextof
easyreading,whereadherencetospecificguidelinesisnecessarytoenhancereadabilityandunderstanding.
Fortheevaluation,wehavechosendocumentsrelatedtoindoorsoccerregulationsandsportsguidelines,specifically
focusingonthecompetitionregulationsforindoorsoccerandthesportsguiderelatedtotheOrganicLawforCompre-
hensiveProtectionofChildrenandAdolescentsagainstViolence(LOPIVI)withinthesportsdomain. Thesedocuments
provideasuitablebasisforassessingifthemodel’soutputsalignwithEasytoReadstandards.
Thequalitativeevaluationconductedbyexpertsisbasedontheirexpertiseinadaptingtextsineasyreadingandthe
standardsoutlinedinTable3,whichsummarizestheguidelinesforadaptingtextsEasytoReadasintroducedinILSMH
[1998]anddeNormalización[2018]. Thistableistheresultofacomprehensiveanalysisofeasyreadingguidelines,
drawnfromstandardsandbestpractices.
4 Experimentalsetup
Buildingonthemethodologyintroducedintheprevioussection,anarchitectureisdepictedinFigure1.Thisarchitecture
outlinesfourkeyprocesses: (1)sentencealignment,(2)low-rankadaptation,(3)LLMfine-tuningforthesmaller7B
parameterLlama2model,and(4)testingutilizingbothLlama27BandLlama270Bmodels.
SentencealignmentistheprocessrequiredtomatchsentencesfromsourcetextstothecorrespondingsentencesofEasy
toReadadaptationpreservingthemeaningacrossthetextsused. Sentenceswithoutcorrespondencearedeleted,and
amongthosewithmultiplecandidates,theonewiththehighestcosinesimilaritytotheoriginalsentenceischosen.
Toachievethisgoal,weemploytheLanguage-agnosticBERTSentenceEmbedding(LaBSE)encodermodelFeng
etal.[2020],pre-trainedtosupport109languages. ThemodelhasbeentrainedwithbothMaskedLanguageModeling
(MLM)andTranslationLanguageModeling(TLM),allowingitnottodependonparalleldatasetsfortrainingheavily.
7ExploringlargelanguagemodelstogenerateEasytoReadcontent
Table3: EasytoReadguidelines
ID Guideline IDUNE IDUE
G1 Oneshouldnotwritewordsorphraseswithalltheirlettersinuppercase, 6.1.1 -
exceptwhentheyareacronyms
G2 Linkedideasshouldbeseparatedbyaperiodinsteadofacomma 6.1.4 5.13
G3 Thesemicolon(;)shouldnotbeused 6.1.7 5.13
G4 Usesimpleandcommonlyusedlanguage 6.2.1 5.1
G5 Vocabularyshouldbeappropriatefortheenduserofthedocument. 6.2.2 5.4
G6 Avoidusingabstract,technical,orcomplexterms 6.2.4 5.2
G7 Avoidsuperlatives 6.2.8 -
G8 Avoidusingwordsinotherlanguagesunlesstheyarewidelyknownand 6.2.10 5.17
properlyexplained
G9 Avoidabbreviations 6.2.11 5.20
G10 Deter from using expressions or metaphors that all readers may not 6.2.15 5.15
understandunlesstheyarecommonineverydaylanguage
G11 Usethesamewordthroughoutthetexttorefertothesameobjector 6.2.17 5.12
referent
G12 Usesimplesentencesandavoidcomplexsentences 6.3.1 5.7
G13 Usethepresentindicativewheneverpossible 6.3.2 -
G14 Oneshouldavoidcompoundoruncommonverbtenses,aswellasthe 6.3.3 5.14
useofconditionalsandsubjunctives
G15 Avoidusingthepassivevoice 6.3.4 5.10
G16 Usetheimperativeonlyinclearcontextstoavoidconfusionwiththe 6.3.6 -
thirdpersonsingularofthepresentindicative
G17 Oneshouldavoidtheuseofimpersonalsentences 6.3.7 5.6
G18 One should avoid using two or more verbs in a row, except for pe- 6.3.9 5.1
riphraseswithmodalverbslike"should,""want,""know,"and"can."
G19 Preferably use affirmative sentences, except in cases such as simple 6.3.10 5.9
prohibitions,wherenegativeformsmaybeclearerandmoredirect
G20 Avoidnegativeformsanddoublenegations 6.3.11 5.9
G21 Includeonlyonemainideaineachsentence 6.3.15 5.8
IthasanarchitecturesimilartoBERTDevlinetal.[2018]. Themodeltransformssentencesintovectorrepresentations,
whichareusedtocalculatethesimilaritybetweensentences.
Oncethealignedcorpusisobtained,theLLama2modelisusedtoperformthetaskoftextsimplificationintoEasy
toReadtext. TherearesomeavailableSpanishLLMsattheBarcelonaSupercomputingCenterBSC[2005]butthe
availableSpanishdecoder-basedGPT-2Gutiérrez-Fandiñoetal.[2021]isolderthanthenewLLama2|Touvronetal.
[2023]thathasemergedshowingbetterresultsacrossavarietyoftasks.
Tofine-tunetheLlama2modelwith70Bparameters,significantcomputingresourcesarerequired. Wehaveutilized
the smaller Llama2 model with 7B parameters. Yet, it still requires 30 GB of GPU memory, so we employed the
QLoRa(QuantitativeLow-RankAdaptation)Dettmersetal.[2023]techniquetoreducethemodelsizeandexpeditethe
process,achievinggreaterefficiency. QLoRAtechniqueisemployedusingthePEFTlibraryMangrulkaretal.[2022]
tofine-tuneLLMswithhighermemoryrequirementscombiningquantizationandLoRA,wherequantizationisthe
processofreducingtheprecisionof32-bitfloating-pointnumbersto4bits. Precisionreductionisappliedtothemodel’s
parameters,layerweights,andlayeractivationvalues. Thisresultsinagilityincalculationsandimprovementinthe
model’sstoragesizeinGPUmemory. Whileachievinggreaterefficiency,thereisaslightlossofprecisioninthemodel.
TheLoRatechniqueincludesaparametermatrixinthemodelthathelpsitlearnspecificinformationmoreefficiently,
leadingtofasterconvergence.
TheLlama27Bmodelisfinetunedwiththeentirecorpusdataset,atotalof1941sentences. WeimplementitinPyTorch
usingtheTransformersandQLoRalibraries. TheentireimplementationiscompletedusingJupyterNotebook,andan
NVIDIAGeForceRTX306012GBisusedtotrainthetextsimplificationmodel. TheGPUfacilitatesasynchronous
dataloadingandmultiprocessing.
TheLlama2modelispre-trainedwith32layersand7Bparameters. Thepre-trainedmodelshowssignificantimprove-
mentsoverthepreviousversionLlama1. Ithasbeentrainedwith40%moretokensandacontextlengthof4096tokens.
Wetrainitforfourepochswithamaximumlengthof512tokensforinputtothetransformermodel. Table4shows
8ExploringlargelanguagemodelstogenerateEasytoReadcontent
Figure1: Diagramofthesystemarchitecture
thehyperparametersofthemodel. Afterfine-tuning,asetofinstructiontemplatesisusedtoguidethesystemonthe
desiredoutput. Thepromptusedis: Transformthesentencetomakeiteasiertounderstandforpeoplewithintellectual
disabilitiesanddifficultiesinreadingcomprehension. Useverysimple,short,directsentencesintheactivevoice,and
avoidcomplicatedwords.
Table4: HyperparametersoftheLLMtested
Hyperparameter Llama2
bnb4bitcomputedtype float16
bnb4bitquanttype nf4
cache False
loraalpha 16
loradropout 0.1
lorar 64
batchsize 6
optim pagedadamw32bit
learningrate 2×10−5
maxgradnorm 0.3
warmupratio 0.03
maxseqlength 512
Tocomplementourmainfindings,additionalexperimentswereconductedusingvariousapproachestotextsimplification.
Specifically, the ollama library was employed to test the Llama2 model with 70B parameters without fine-tuning,
contrastingitwithourprimarymethodthatinvolvesfine-tuningasmallerLlama2model. Thisapproachfacilitates
localmodeltesting,leveraging4-bitquantizationandpackagingmodelweights,configurations,anddatasetsintoa
unifiedModelfile. Table5outlinestheseexperiments,providinginsightsintotheefficacyofdifferentconfigurations
andpromptsingeneratingaccessibletexts.
Aparticularfocusoftheseexperimentswasthetranslationapproach,wheretheoriginaltextistranslatedintoEnglish,
simplified,andthenthesimplifiedtextistranslatedbackintoSpanish. Thismethodevaluatestheimpactoftranslations
onthecoherenceandaccessibilityofthefinalsimplifiedtext. Thisaspectiscrucialbecauseitdirectlyaddressesthe
challengesposedbythelimitedpresenceoftheSpanishlanguageintheresourcesofexistingLLMs.
9ExploringlargelanguagemodelstogenerateEasytoReadcontent
ThesecomparativeanalysesaimtoidentifyoptimalstrategiesforsimplifyingtextsintoEasytoReadformats,taking
intoaccountavailableresourcesandtheunderrepresentationoftheSpanishlanguageincurrentLLMsresources.
Table5: ExperimentsperformedusingLlama270BandLlama27B,differentpromptsandwith/withoutpromptand
outputtranslations
ID LLM Fine- Translation Prompt
Tuning approach
E1 Llama2 Yes No Shortandstraightforwardprompt,indicatingthemainidea,thetarget
7B user, andhowthesimplificationshouldbe(output: Useverysimple,
short,anddirectsentencesinactivevoice,avoidingcomplicatedwords)
E2 Llama2 Yes Yes SameE1prompt,buttranslatedintoEnglish. Inthisprocess,themodel
7B mustfirsttranslatetheinputintoEnglish,thensimplifythesentence,
andfinallytranslatethesimplificationintoSpanish
E3 Llama2 No No Shortandstraightforwardprompt,indicatingthemainidea,thetarget
70B user, andhowthesimplificationshouldbe(output: Useverysimple,
short,anddirectsentencesinactivevoice,avoidingcomplicatedwords)
E4 Llama2 No Yes SameE3prompt,buttranslatedintoEnglish. Inthisprocess,themodel
70B mustfirsttranslatetheinputintoEnglish,thensimplifytheinputtext,
andfinallytranslatethesimplificationintoSpanish
E5 Llama2 No No Thepromptbeginsbypresentingthemainideaandspecifyingtheuser
70B towhomthesimplificationisdirected. Finally,theguidelinesthatthe
modelmustfollowtocarryoutthesimplificationaredetailed,which
arefoundinTable3
5 Resultsanddiscussion
This section presents an exploratory evaluation of the Easy to Read simplification system, results from specific
experiments,anexpertreviewofsimplificationquality,errorclassification,andacomparativeanalysis. Itconcludes
withimplicationsforfutureresearch.
5.1 Overview
Wepresentanexploratoryevaluationofoursimplificationsystemusingdocumentsonsportsregulationsandguidelines.
Table6displaysexamplesofsentencesinputintothesystemandtheircorrespondingsimplificationsaccordingtoEasy
toReadguidelinesperformedbyanexperthumanadapterinEasytoRead.
TheexpertadapterconductsathoroughevaluationoftheadaptationbybothLlama2models. Inaninitialreview,a
significantachievementinlexicalsimplificationwasevident. Afterward,EasytoReadguidelineshavebeenanalysedin
detailtoidentifythosethathavebeenimplementedandthosethathavenot.
5.2 LimitationsoftheExperimentalStudy
Inthisexperimentation,certainguidelineswillnotbeevaluatedduetotheirinapplicabilitytothetextsunderconsidera-
tion. Specifically,guidelinesG1,G3,G7,G9,andG16cannotbeassessed. G1isirrelevantbecausethetextsdonot
containwordsorphrasesentirelyinuppercaseletters,exceptforacronyms. G3isnotapplicableasnoneoftheinput
sentencesincludesemicolons. G7,concerningtheavoidanceofsuperlatives,doesnotapplybecausetheoriginaltexts
donotusesuperlativeforms. G9,whichadvisesagainstabbreviations,isnotrelevanthereasthetextsdonotcontain
anyabbreviations. Lastly,G16,pertainingtotheuseoftheimperativemood,isnotapplicablebecausetheimperativeis
notusedinanyoftheinputsentences,makingitimpossibletoassesstheguideline’sadherenceinthiscontext. Ofthe
21guidelinesprovided,16willbeapplicableintheexperimentalstudy
5.3 ExperimentalResults
Table7showstheresultoftheexperimentsdetailedinTable5alongwiththecorrespondinginputsentenceprovidedin
Table6.
10ExploringlargelanguagemodelstogenerateEasytoReadcontent
Table6: Examplesofsentencesusedasinputmodelsandcorrespondinghuman-adaptedversions
ID Inputsentence HumanadaptationtoEasytoReadguidelines
S1 Paraladisputadelosencuentros,podránconvocarse Elequipopuedellamarajugarhasta14jugadores
unmáximode14jugadores. Dadalalimitacióndel para cada partido. En el acta en el que el árbitro
acta,losjugadoresdemásseañadiránenelreverso inscribe a los jugadores solo caben 14. Cuando el
deestayseráreflejadoporelárbitro(Amaximumof equipollamaajugaramásde14jugadores,elárbitro
14playersmaybecalledupforthematches. Dueto escribesusnombresenlapartedeatrásdelacta(The
thelimitationofthescoresheet,theextraplayerswill teamcancallupto14playerstoplayeachmatch. In
beaddedonthebackofitandwillberecordedbythe thescoresheetwheretherefereeregisterstheplayers,
referee) only14canfit. Whentheteamcallsupmorethan14
playerstoplay,therefereewritestheirnamesonthe
backofthescoresheet)
S2 Losequipospodráninscribirunmínimode8deportis- Losequipospuedeninscribirentre8y16deportistas
tasyunmáximode16deportistasporequipo(Teams por equipo (Teams can register between 8 and 16
mayregisteraminimumof8athletesandamaximum athletesperteam)
of16athletesperteam)
S3 Paralainscripcióndeequiposserealizaráatravés Los equipos y los deportistas deben in-
delaplataformadeGestióndeLicenciasdeFemaddi scribirse en la plataforma de Gestión de
(TeamregistrationwillbedonethroughtheFemaddi Licencias de FEMADDI en la página web:
LicenseManagementplatform) https://femaddi.playoffinformatica.com/(Teams
and athletes must register on the FEMADDI
License Management platform on the website:
https://femaddi.playoffinformatica.com/)
Theexperimentsexploreddifferentapproachestosimplification,withafocusonassessingtheeffectivenessoftext
simplification for easy reading in specific knowledge areas where the model has been fine-tuned, the impact of
translationsonsimplificationquality,andtheperformanceofsimplificationwithoutdomain-specificfine-tuning.
5.4 Discussion
Theresultsoftheexperimentswithdifferentapproachesarepresentedbelow,anddetailedinTable5.
• TheobjectiveofExperimentE1istoconfirmtheeffectivenessoftextsimplificationforeasyreadinginspecific
knowledgeareaswherethemodelhasbeenfine-tuned. Inthiscase,thefocuswasonthesportsdomain,using
sports regulations and guidelines as references. It was observed that the performance of simplification is
notablybetterwhenthemodelisfamiliarwiththedomain.
• InexperimentE2,thedefaultLlama27BparametermodelachievesaccuratetranslationsfromSpanishto
English. However,afterfine-tuningforeasyreadingsimplification,themodeldoesnotaccuratelytranslate. In
mostcases,theoutputsremaininSpanish,andsomeshowsimplifications,eventhoughtheoriginalpromptonly
requeststranslationintoEnglish. Thisdiscrepancymaystemfromthelayerselectionduringthefine-tuning
process, where weight modifications enable the model to adapt to the new task. In this specific process,
alllayersweremodifiedsinceitisrecommendedtousethecompletesetoflayersformorecomplexand
demandingtasksinnaturallanguageprocessing. However,itispossiblethatusingfine-tuningwith8orfewer
layerscouldachievecorrecttranslationintoEnglish. So,theoptionofemployingthemethodoftranslating
intoEnglish,simplifyingthetext,andthentranslatingitbackintoSpanishforthefine-tunedmodelisrejected.
AsshowninTable7,thisapproachyieldsthemostdeficientresults.
• TheresultsofexperimentE3arelesssatisfactorycomparedtomodel7Bwithfine-tuning. Thisisbecauseitis
limitedsolelytoreplacingtermswithsimplersynonyms,withoutachievingshorterormoredirectsentences.
Thereasonisthatthemodellacksfamiliaritywithexamplesofeasyreading,hasnotbeenpreviouslytrained
inthisarea,anddoesnotpossessthenecessaryguidelinestocarryoutpropersimplificationineasyreading.
• InexperimentE4,theresultsaresuperiortothoseobtainedinexperimentE3. Themodeleffectivelysimplifies
byusingshortanddirectsentences,withaclearmainideaandavoidingusingcomplexwords. Althoughnot
specifiedintherequest,themodeloftenseparatessentencesusingperiodsinsteadofcommas. TheLlama27B
modelwithfine-tuningyieldsbetterresultsindomainswhereitisfamiliarcomparedtotheresultsobtained
usingthisapproach.
• Finally, in experiment E5, the results can be considered superior to the 7B model with fine-tuning. This
modelfollowstheinstructionsandguidelinesofTable3toperformEasytoReadsimplification. Additionally,
11ExploringlargelanguagemodelstogenerateEasytoReadcontent
Table7: Outputoftheexperiments
Experiment Sentence Output
E1 S1 Los equipos pueden reunir 14 jugadores en cada partido. El árbitro añadirá
jugadoresenlapartedeatrásdelactacuandoelequipotengamásde14jugadores
(Theteamscanfield14playersineachmatch. Therefereewilladdplayerstothe
backofthescoresheetiftheteamhasmorethan14players)
E2 S1 Paralospartidos,sepuedenconvocarunmáximode14jugadores. Dadoquehay
14jugadoresenelacta,losjugadoresqueseanmásseañadiránenelreversodel
actaporelárbitro(Forthematches,amaximumof14playerscanbecalledup.
Sincethereare14playersonthescoresheet,anyadditionalplayerswillbeadded
tothebackofthescoresheetbythereferee)
E3 S1 Losequipospuedenllamara14jugadoresajugarelpartido. Sihaymásjugadores,
elarbitrolosañadiráenlapartedeatrásdelacta(Theteamscancallup14players
toplaythematch. Iftherearemoreplayers,therefereewilladdthemtothebackof
thescoresheet)
E4 S1 Se puede llamar a un máximo de 14 jugadores para el partido. Los jugadores
adicionalesseañadiránenlaparteposteriordelalistayseinformaráalárbitro
(Amaximumof14playerscanbecalledforthematch. Additionalplayerswillbe
addedtothebackofthelist,andtherefereewillbeinformed)
E5 S1 Enunpartidopuedenjugarhasta14jugadores. Elárbitroañadirájugadoresenla
partedeatrásdelalistacuandoelequipotengamásde14jugadores(Upto14
playerscanplayinamatch. Therefereeaddsplayerstothebackoftherosterwhen
theteamhasmorethan14players.)
E1 S2 Elequipopuedeinscribirentre8y16deportistas(Theteamcanregisterbetween8
and16athletes)
E2 S2 Los equipos pueden inscribir un mínimo de 8 deportistas y un máximo de 16
deportistas (Teams can register a minimum of 8 athletes and a maximum of 16
athletes)
E3 S2 Los equipos pueden inscribir entre 8 y 16 deportistas (The teams can register
between8and16athletes)
E4 S2 Unequipopuedetenerentre8y16deportistas(Ateamcanhavebetween8and16
athletes)
E5 S2 Losequipospuedeninscribirentre8y16jugadores(Teamscanregisterbetween8
and16players)
E1 S3 LosequipodebeninscribirseenlaplataformadeFemaddiparapoderparticipar
(TheteamsmustregisterontheFemaddiplatforminordertoparticipate)
E2 S3 Para inscribirse en la competición, hay que usar la plataforma de licencias de
Femaddi (To register for the competition, you must use the Femaddi licensing
platform)
E3 S3 ParainscribirequiposusaremoslaplataformadeFemaddi(Toregisterteams,we
willusetheFemaddiplatform)
E4 S3 La plataforma de Gestión de Licencias de Femaddi se utiliza para inscribir los
equipos(TheFemaddiLicensingManagementPlatformisusedtoregisterteams)
E5 S3 LosequiposdebeninscribirseenlaPlataformadeLicenciasFemaddi(Teamsmust
registerontheFemaddiLicensingPlatform)
the70Bmodelwiththispromptdoesnotneedtobefamiliarizedwiththedomainlikethe7Bmodelwith
fine-tuningtoachievegoodresults.
Thetable8showcasestheclassificationoferrorsidentifiedduringtheevaluationofresultsbytheEasytoReadadapter.
ItdetailsthetypesoferrorsalongwiththeircorrespondingdescriptionsandtheiralignmentwiththeEasytoRead
guidelines.OnlyerrorsfoundintheLlama27Bmodelwithfine-tuningandtheLlama270Bmodelusingthepromptthat
includeseasyreadingguidelinesarepresented. Theseapproacheshavebeendemonstratedtoyieldthemostcompetitive
results.
Oneoftheerrorsisthatthemodelsusethepluralarticlewithsingularnouns, andviceversa. Forexample, inthe
sentenceLosequipodenuevacreaciónquequieranaccederalasligasdeberándisputarunpartidoamistoso(The
12ExploringlargelanguagemodelstogenerateEasytoReadcontent
Table8: Classificationoferrorsdetectedbythehumanevaluator
Type Error
Numberandgenderagreement It’suncommon,butsometimestherearenogenderandnumber
agreementsbecausemultilingualLLMsaretrainedwithfewtexts
inSpanishcomparedtoEnglish
Usethesameterm In some cases, the same word is not used throughout the text
torefertothesameobjectorreferent. Itdoesnotcomplywith
guidelineG11.
Explanationofterms Sometechnicaltermsarenotexplained. Itdoesnotcomplywith
guidelineG6.
newlyformedteamsthatwishtoaccesstheleaguesmustplayafriendlymatch.),thedeterminantisinpluralandthe
nouninsingular. ThelanguageusedinLlama2ismostlyEnglishandnotsomuchSpanish.
AcommonmistakerelatedtoEasytoReadgenerationistousedifferenttermstorefertothesamething. Forexample,
thefutsalregulationsdocumentsometimesreferstoteammembersas"players"andothertimesas"athletes".
Theeasy-readingadapterhighlightedtheimportanceofprovidingexplanationsorincludingcertaintermsinaglossary
whenworkinginafieldwithtechnicalterminology. Forexample,thewordacta(scoresheet)isdefinedintheglossary
oftheGoldStandarddocument,whilethemodeloutputpresentsitwithoutexplanationorreplacesitwiththesynonym
lista(roster).
TheadaptationsaimedatreaderswithdifficultieshaveimprovedtheLlama2modelthroughfine-tuning,tosimplify
languageforpeoplewithintellectualdisabilities. Ithasbeenshownthatgoodresultsareachievedbyusingatraining
datasetfocusedonthedomainofsimplification. Creatingadatasetthatencompassesdifferentdomainsadaptedfor
easyreadingcouldrepresentavaluableresourcetoachieveevenmorecompetitiveresultsinotherareas. Thistask
can also be applied to English, a simpler language than Spanish, with a more complex morphology and extensive
verbalinflection. ThetextsinSpanisharecharacterizedbythepresenceofnumeroussubordinateclausesandextensive
phrases,whichcanexhibitawidevariationinwordorder.
5.5 ImplicationsandFutureDirections
Theconductedstudyopensnewpossibilitiesforfutureresearchinlexicalsimplificationandadaptationtoeasyreading
inSpanish,highlightingtheversatilityoftheemployedmethodology,whichcanbeappliedacrossvariousfieldsand
domains. ItwasobservedthattheLlama27Bmodelprovidesadequateresultsinaninitialevaluation. However,a
greaternumberoftextsadaptedforeasyreadingcouldsignificantlyenhancetheobtainedresults. Ontheotherhand,
moreadvancedmodelssuchastheLlama270Bofferevenmorecompetitiveoutcomes. Tooptimizeaccuracy,itis
suggestedtoutilizetheLlama270Bmodelwithoutquantizationandfine-tuningusingabroadsetofadaptationsforeasy
reading,potentiallyleadingtoexceptionalperformancewithhigh-qualityresults. It’sworthnotingthatthisapproach
wouldrequireconsiderablyhighcomputationalresourcesforimplementation.
6 Conclusion
TheresearchdescribedinthisarticlerepresentsthefirststudyonsimplifyingtoSpanishEasytoReadlanguagefor
people with intellectual disabilities, through the use of decoder-based LLMs. We have outlined the procedure for
creatingaparallelcorpusofEasytoReadtextscomposedofacollectionof1941sentencesthatisavaluableresource
totrainmachinelearningapproachestosimplifycontent. Inthenearfuture,itwouldbebeneficialtoexploreother
strategiestogrouppiecesofinformation(sentences,paragraphs,orevenentiretexts)toobtainabroadercontextinthis
typeofcorpus. IncreasingthesizeoftheseresourcesisanopenchallengebecauseadaptingLLMtonewtasksrequires
largercorpora. Sometestshavebeenconductedwithtextsfromdomainsthatthemodelisunfamiliarwith,andthe
resultsarenotasgoodaswhenthemodelisfamiliarwiththedomaininquestion.
We suggest that future research consider incorporating training with a domain terminology dictionary in which
researchers are currently working. This is because some documents contain complex or uncommon terminology
dependingonthedomainbeingworkedon. Thisway,whenanunfamiliartermappears,itcanbeaccuratelydescribed,
facilitatingreadercomprehension.
13ExploringlargelanguagemodelstogenerateEasytoReadcontent
The experiments presented illustrate a use case and show that the corpus has allowed the evaluation of lexical
simplificationapproachesbasedonlanguagemodels. Themethodologyusedcouldbeappliedtootherlanguages,such
asEnglish,forwhichlinguisticmodelswithmoretrainingdataareavailable.
In summary, this first approach significantly improves the accessibility of documents in Spanish for people with
intellectualdisabilities. Inaddition,thedevelopmentofcorporaplaysacrucialroleinthedevelopmentofsimplification
systems for people with reading comprehension difficulties. However, it is essential to emphasize that documents
simplifiedthroughanautomatedsystemmustalwaysbereviewedandvalidatedbyaprofessional.
ConflictofInterestStatement
Theauthorsdeclarethattheresearchwasconductedintheabsenceofanycommercialorfinancialrelationshipsthat
couldbeconstruedasapotentialconflictofinterest.
AuthorContributions
PM:conceptualization,investigation,methodology,writing-originaldraft,writing-review,andediting.
AR:datacuration,software,validation,writing-originaldraft.
LM:conceptualization,resources,fundingacquisition,writing-originaldraft.
Funding
This work was supported by ACCESS2MEET project (PID2020-116527RB-I0) supported by MCIN
AEI/10.13039/501100011033/.
Acknowledgments
WewouldliketothanktheadapteroftheAMASfoundationforhisassistancewiththeresultevaluation.
References
Imserso. Baseestataldedatosdepersonasconvaloracióndelgradodediscapacidad,2022. URLhttps://imserso.
es/documents/20123/146998/bdepcd_2022.pdf/390b54fe-e541-3f22-ba1a-8991c5efc88f.
Instituto Nacional de Estadística (INE). Encuesta de discapacidad, autonomía personal y situaciones de depen-
dencia, 2022. URL https://www.ine.es/dyngs/INEbase/es/operacion.htm?c=Estadistica_C&cid=
1254736176782&menu=resultados&idp=1254735573175. Retrievedfrom.
World Health Organization. Ageing and health. https://www.who.int/news-room/fact-sheets/detail/
ageing-and-health,n.d. Retrievedfrom.
WorldHealthOrganization. RiskReductionofCognitiveDeclineandDementia: WHOGuidelines. https://iris.
who.int/handle/10665/312180,2019. WHOIRIS.
GeertFreyhoff,GerhardHess,LindaKerr,BrorTronbacke,andKathyVanDerVeken. Makeitsimple,June1998.
Followthisandadditionalworksat: https://digitalcommons.ilr.cornell.edu/gladnetcollect.
M. Nomura et al. Guidelines for easy-to-read materials, 2010. URL https://www.ifla.org/wp-content/
uploads/2019/05/assets/hq/publications/professional-report/120.pdf. Retrievedfrom.
RichardC.Wydick. PlainEnglishforLawyers. CarolinaAcademicPress,1979.
EuropeanCommission.Directorate-GeneralforTranslation.Howtowriteclearly,2011.URLhttps://data.europa.
eu/doi/10.2782/29211.
UNE153101:2018: Easy-to-ReadStandards,2018. Retrievedfromhttps://www.une.org/.
HoracioSaggion,SanjaŠtajner,StefanBott,SimonMille,LuzRello,andBiljanaDrndarevic. Makingitsimplext:
Implementation and evaluation of a text simplification system for spanish. ACM Transactions on Accessible
Computing(TACCESS),6(4):1–36,2015.
14ExploringlargelanguagemodelstogenerateEasytoReadcontent
EduardBarbu,MTeresaMartín-Valdivia,EugenioMartinez-Camara,andLAlfonsoUrena-López. Languagetech-
nologiesappliedtodocumentsimplificationforhelpingautisticpeople. ExpertSystemswithApplications,42(12):
5076–5086,2015.
RodrigoAlarcon,LourdesMoreno,andPalomaMartínez. Easiercorpus: Alexicalsimplificationresourceforpeople
withcognitiveimpairments. Plosone,18(4):e0283622,2023.
RodrigoAlarcon,LourdesMoreno,andPalomaMartínez. Lexicalsimplificationsystemtoimprovewebaccessibility.
IEEEAccess,9:58755–58767,2021.
L.Moreno,H.Petrie,P.Martínez,andR.Alarcon. Designinguserinterfacesforcontentsimplificationaimedatpeople
withcognitiveimpairments. UniversalAccessintheInformationSociety,2023. doi:10.1007/s10209-023-00986-z.
URLhttps://doi.org/10.1007/s10209-023-00986-z.
R.Alarcon,L.Moreno,P.Martínez,andJ.A.Macías.Easiersystem.evaluatingaspanishlexicalsimplificationproposal
withpeoplewithcognitiveimpairments. InternationalJournalofHuman–ComputerInteraction,pages1–15,2022.
doi:10.1080/10447318.2022.2134074. URLhttps://doi.org/10.1080/10447318.2022.2134074.
M.C.Suárez-Figueroa,I.Diab,E.Ruckhaus,etal. Firststepsinthedevelopmentofasupportapplicationforeasy-
to-readadaptation. UniversalAccessintheInformationSociety,2022. doi:10.1007/s10209-022-00946-z. URL
https://doi.org/10.1007/s10209-022-00946-z.
AdvaithSiddharthan. Syntacticsimplificationandtextcohesion. ResearchonLanguageandComputation,4:77–109,
2006.
Lucia Specia. Translating from complex to simplified sentences. In Computational Processing of the Portuguese
Language: 9thInternationalConference,PROPOR2010,PortoAlegre,RS,Brazil,April27-30,2010.Proceedings9,
pages30–39.Springer,2010.
William Coster and David Kauchak. Simple english wikipedia: a new text simplification task. In Proceedings of
the49thAnnualMeetingoftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,pages
665–669,2011.
JipengQiang,YunLi,YiZhu,YunhaoYuan,andXindongWu. Lexicalsimplificationwithpretrainedencoders. In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume34,pages8649–8656,2020.
LouisMartin,BenoîtSagot,EricdelaClergerie,andAntoineBordes. Controllablesentencesimplification. arXiv
preprintarXiv:1910.02677,2019.
StefanBott,LuzRello,BiljanaDrndarevic´,andHoracioSaggion.Canspanishbesimpler?lexsis:Lexicalsimplification
forspanish. InProceedingsofCOLING2012,pages357–374,2012.
Daniel Ferrés, Ahmed Ghassan Tawfiq AbuRa’ed, and Horacio Saggion. Spanish morphological generation with
wide-coveragelexiconsanddecisiontrees. ProcesamientodelLenguajeNatural.2017;58: 109-116,2017.
SeidMuhieYimam,SanjaStajner,MartinRiedl,andChrisBiemann. Multilingualandcross-lingualcomplexword
identification. InRANLP,pages813–822,2017.
SanjaŠtajner,IacerCalixto,andHoracioSaggion. Automatictextsimplificationforspanish: Comparativeevaluationof
varioussimplificationstrategies. InProceedingsoftheinternationalconferencerecentadvancesinnaturallanguage
processing,pages618–626,2015.
SanjaŠtajner,HoracioSaggion,andSimonePaoloPonzetto. Improvinglexicalcoverageoftextsimplificationsystems
forspanish. ExpertSystemswithApplications,118:80–91,2019.
JennyAOrtiz-ZambranoaandArturoMontejo-Ráezb. Overviewofalexs2020: Firstworkshoponlexicalanalysisat
sepln. InProceedingsoftheIberianLanguagesEvaluationForum(IberLEF2020),volume2664,pages1–6,2020.
WeiXu,ChrisCallison-Burch,andCourtneyNapoles. Problemsincurrenttextsimplificationresearch: Newdatacan
help. TransactionsoftheAssociationforComputationalLinguistics,3:283–297,2015.
MahmoudEl-Haj,JuyeonKang,AbderrahimAitAzzi,SandraBellato,IsmailElMaarouf,MeiGan,AnaGisbert,
andAntonioSandoval. Thefinancialdocumentstructureextractionsharedtask(fintoc2022). InThe4thFinancial
NarrativeProcessingWorkshop,pages92–97,2022.
SanjaŠtajner,DanielFerrés,MatthewShardlow,KaiNorth,MarcosZampieri,andHoracioSaggion. Lexicalsimplifi-
cationbenchmarksforenglish,portuguese,andspanish. FrontiersinArtificialIntelligence,5:991242,2022.
DanielFerrésandHoracioSaggion. Alexsis: adatasetforlexicalsimplificationinspanish. InProceedingsofthe
ThirteenthLanguageResourcesandEvaluationConference,pages3582–3594,2022.
Leonardo Campillos-Llanos, Ana Rosa Terroba Reinares, Sofía Zakhir Puig, Ana Valverde Mateos, and Adrián
CapllonchCarrión. Clara-medcorpus. 2022.
15ExploringlargelanguagemodelstogenerateEasytoReadcontent
AsociaciónEuropeaILSMH. Elcaminomásfácil: Directriceseuropeasparagenerarinformacióndefácillectura.
Portugal: GrupoILSMH.Recuperadodehttps://sid.usal.es/idocsF,1998.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundationandfine-tunedchatmodels.
arXivpreprintarXiv:2307.09288,2023.
NilsReimersandIrynaGurevych. Sentence-bert: Sentenceembeddingsusingsiamesebert-networks. InProceedings
ofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,page3982–3992.Associationfor
ComputationalLinguistics,112019. URLhttps://arxiv.org/abs/1908.10084.
FangxiaoyuFeng,YinfeiYang,DanielCer,NaveenArivazhagan,andWeiWang. Language-agnosticbertsentence
embedding. arXivpreprintarXiv:2007.01852,2020.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu. Bleu: amethodforautomaticevaluationofmachine
translation. InProceedingsofthe40thannualmeetingoftheAssociationforComputationalLinguistics, pages
311–318,2002.
WeiXu,CourtneyNapoles,ElliePavlick,QuanzeChen,andChrisCallison-Burch. Optimizingstatisticalmachine
translationfortextsimplification. TransactionsoftheAssociationforComputationalLinguistics,4:401–415,2016.
doi:10.1162/tacl_a_00107. URLhttps://aclanthology.org/Q16-1029.
JPeterKincaid,RobertPFishburneJr,RichardLRogers,andBradSChissom. Derivationofnewreadabilityformulas
(automatedreadabilityindex,fogcountandfleschreadingeaseformula)fornavyenlistedpersonnel. 1975.
AsociaciónEspañoladeNormalización. Normaespañolaexperimentalune153101ex.lecturafácil: Pautasyrecomen-
dacionesparalaelaboracióndedocumentos. 2018.
BSC. BSC-CNS. BSC-CNS,2005. URLhttps://www.bsc.es/.
Asier Gutiérrez-Fandiño, Jordi Armengol-Estapé, Marc Pàmies, Joan Llop-Palao, Joaquín Silveira-Ocampo,
Casimiro Pio Carrino, Aitor Gonzalez-Agirre, Carme Armentano-Oller, Carlos Rodriguez-Penagos, and Marta
Villegas. Spanishlanguagemodels. 2021.
TimDettmers,ArtidoroPagnoni,AriHoltzman,andLukeZettlemoyer. Qlora: Efficientfinetuningofquantizedllms.
arXivpreprintarXiv:2305.14314,2023.
Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft:
State-of-the-artparameter-efficientfine-tuningmethods. 2022. URLhttps://github.com/huggingface/peft.
16