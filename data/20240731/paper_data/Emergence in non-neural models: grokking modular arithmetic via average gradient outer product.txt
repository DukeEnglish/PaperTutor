Emergence in non-neural models:
grokking modular arithmetic via average gradient outer product
Neil Mallinar1 Daniel Beaglehole1 Libin Zhu1
Adityanarayanan Radhakrishnan2 Parthe Pandit3 Mikhail Belkin1
1UC San Diego 2The Broad Institute of MIT and Harvard 3IIT Bombay
{nmallina,dbeaglehole,libinzhu,mbelkin}@ucsd.edu
aradha@mit.edu ; pandit@iitb.ac.in
Abstract
Neural networks trained to solve modular arithmetic tasks exhibit grokking, a phenomenon where
the test accuracy starts improving long after the model achieves 100% training accuracy in the training
process. It is often taken as an example of “emergence”, where model ability manifests sharply through
a phase transition. In this work, we show that the phenomenon of grokking is not specific to neural
networksnortogradientdescent-basedoptimization. Specifically,weshowthatthisphenomenonoccurs
whenlearningmodulararithmeticwithRecursiveFeatureMachines(RFM),aniterativealgorithmthat
uses the Average Gradient Outer Product (AGOP) to enable task-specific feature learning with general
machine learning models. When used in conjunction with kernel machines, iterating RFM results in a
fast transition from random, near zero, test accuracy to perfect test accuracy. This transition cannot
be predicted from the training loss, which is identically zero, nor from the test loss, which remains
constant in initial iterations. Instead, as we show, the transition is completely determined by feature
learning: RFM gradually learns block-circulant features to solve modular arithmetic. Paralleling the
results for RFM, we show that neural networks that solve modular arithmetic also learn block-circulant
features. Furthermore, we present theoretical evidence that RFM uses such block-circulant features to
implement the Fourier Multiplication Algorithm, which prior work posited as the generalizing solution
neural networks learn on these tasks. Our results demonstrate that emergence can result purely from
learning task-relevant features and is not specific to neural architectures nor gradient descent-based
optimization methods. Furthermore, our work provides more evidence for AGOP as a key mechanism
for feature learning in neural networks.1
1 Introduction
Inrecentyearstheideaof“emergence”hasbecomeanimportantnarrativeinmachinelearning. Whilethere
isnobroadagreementonthedefinition[37],itisoftenarguedthat“skills”emergeduringthetrainingprocess
once certain data size, compute, or model size thresholds are achieved [2, 43]. Furthermore, these skills are
believed to appear rapidly, exhibiting sharp and seemingly unpredictable improvements in performance at
thesethresholds. Oneofthesimplestandmoststrikingexamplessupportingthisideais“grokking”modular
arithmetic [31, 33]. A neural network trained to predict modular addition or another arithmetic operation
on a fixed data set rapidly transitions from near-zero to perfect (100%) test accuracy at a certain point in
the optimization process. Surprisingly, this transition point occurs long after perfect training accuracy is
achieved. Not only is this contradictory to the traditional wisdom regarding overfitting but, as we will see
below,someaspectsofgrokkingdonotfitneatlywithourmodernunderstandingof“benignoverfitting”[4,8].
1Codeisavailableathttps://github.com/nmallinar/rfm-grokking.
1
4202
luJ
92
]LM.tats[
1v99102.7042:viXraRFM Iterations
Learned Feature (AGOP) Matrices
Figure 1: Recursive Feature Machines grok the modular arithmetic task f∗(x,y)=(x+y)mod59.
Despitealargeamountofrecentworkonemergenceand, specifically, grokking, (see, e.g., [13,22,25,31,33,
39]), the nature or even existence of the emergent phenomena remains contested. For example, the recent
paper [38] suggests that the rapid emergence of skills may be a “mirage” due to the mismatch between the
discontinuous metrics used for evaluation, such as accuracy, and the continuous loss used in training. The
authorsarguethat,incontrasttoaccuracy,thetest(orvalidation)lossorsomeothersuitablychosenmetric
maydecreasegraduallythroughouttrainingandthusprovideausefulmeasureofprogress. Anotherpossible
progress measure is the training loss. As SGD-type optimization algorithms generally result in a gradual
decrease of the training loss, one may posit that skills appear once the training loss falls below a certain
threshold in the optimization process. Indeed, such a conjecture is in the spirit of classical generalization
theory, which considers the training loss to be a useful proxy for the test performance [27].
In this work, we show that sharp emergence in modular arithmetic arises entirely from feature learning,
independently of other aspects of modeling and training, and is not predicted by the standard measures
of progress. We then clarify the nature of feature learning leading to the emergence of skills in modular
arithmetic. We discuss these contributions in further detail below.
Summary of the contributions. We demonstrate empirically that grokking modular arithmetic
• is not specific to neural networks;
• is not tied to gradient-based optimization methods;
• is not predicted by training or test loss2, let alone accuracy.3
Specifically,weshowgrokkingforRecursiveFeatureMachines(RFM)[35],analgorithmthatiterativelyuses
the Average Gradient Outer Product (AGOP) to enable task-specific feature learning in general machine
2We note that for neural networks trained by SGD, emergence cannot be decoupled from training loss, as non-zero loss is
requiredfortrainingtooccuratall.
3Notethatzerotest/trainlossimpliesperfecttest/trainaccuracy.
2
ycaruccA
ssoL
erauqSlearning models. In this work, we use RFM to enable feature learning in kernel machines, which are a class
of predictors with no native mechanism for feature learning. In this setting, RFM iterates between three
steps: (1) training a kernel machine, f, to fit training data; (2) computing the AGOP matrix of f, M, over
the training data to extract task-relevant features; and (3) transforming input data, x, using the learned
features via the map x→Ms/2x for a matrix power s>0 (see Section 2 for details).
InFig.1wegivearepresentativeexampleofRFMgrokkingmodularaddition,despitenotusinganygradient-
based optimization methods and achieving perfect (numerically zero) training loss at every iteration. We
see that during the first few iterations both the test loss and and test accuracy remain at the constant
(random) level. However, around iteration 10, the test loss starts improving, and a few iterations later, test
accuracy quickly transitions to 100%. We also observe that even early in the iteration, structure emerges
in AGOP feature matrices (see Fig. 1). The gradual appearance of structure in these feature matrices is
particularly striking given that the training loss is identically zero at every iteration and that the test loss
doesnotsignificantlychangeuntilatleastiterationeight. Thestripedpatternsobservedinfeaturematrices
correspond to matrices whose sub-blocks are circulant with entries that are constant along the diagonals.4
Such circulant feature matrices are key to learning modular arithmetic. In Section 3 we demonstrate that
standardkernelmachinesusingrandomcirculantfeatureseasilylearnmodularoperations. Astheserandom
circulantmatricesaregeneric,wearguethatnoadditionalstructureisrequiredtosolvemodulararithmetic.
To demonstrate that the feature matrices evolve toward this structure (including for multiplication and
division under an appropriate re-ordering of the input coordinates), we introduce two “hidden progress
measures” (cf. [3]):
1. Circulant deviation, which measures how far the diagonals of a given matrix are from being constant.
2. AGOP alignment, which measures similarity between the feature matrix at iteration t and the AGOP
of a fully trained model.
As we demonstrate, both of these measures show gradual (initially nearly linear) progress toward a model
that generalizes.
Wefurtherarguethatemergenceinfullyconnectedneuralnetworkstrainedonmodulararithmeticidentified
in prior work [15, 21] is analogous to that for RFM and is exhibited through the AGOP (see Section 4). In-
deed, by visualizing covariances of network weights, we observe that these models also learn block-circulant
features to grok modular arithmetic. We demonstrate that these features are highly correlated with the
AGOPofneuralnetworks, corroboratingpriorobservationsfrom[35]. Furthermore, parallelingourobserva-
tionsforRFM,ourtwoproposedprogressmeasuresindicategradualprogresstowardageneralizingsolution
during neural network training. Finally, just as for RFM, we demonstrate that training neural networks on
data transformed by random block-circulant matrices dramatically decreases training time needed to learn
modular arithmetic.
Why are these learned block circulant features effective for modular arithmetic? We provide supporting
theoretical evidence that such circulant features result in kernel machines implementing the Fourier Multi-
plication Algorithm (FMA) for modular arithmetic (see Section 5). For the case of neural networks, several
prior works have argued empirically and theoretically that neural networks learn to implement the FMA to
solve modular arithmetic [29, 31, 42]. Thus, while kernel-RFM and neural networks utilize different classes
of predictive models, our results suggest that these models discover similar algorithms for implementing
modular arithmetic. In particular, these results imply that the two methods have the same out-of-domain
generalization properties when the entries of input vectors contain real numbers instead of 0’s or 1’s in the
training data.
4More precisely the entries of circulants are constant along the “long” diagonals which wrap around the matrix. Feature
matricesmayalsobeblockHankelmatriceswhichareconstantonanti-diagonals. Unlessthedistinctionisimportant,wewill
generallyrefertoallsuchmatricesascirculant.
3Overall,bydecouplingfeaturelearningfrompredictortraining,ourresultsprovideevidencefortheemergent
properties of machine learning models arising purely as a consequence of their ability to learn features. We
hope that our work will help isolate the underlying mechanisms of emergence and shed light on the key
practical concern of how, when, and why these seemingly unpredictable transitions occur.
Paper outline. Section 2 contains the necessary concepts and definitions. In Section 3, we demonstrate
emergence with RFM and show AGOP features consist of circulant blocks. In Section 4, we show that the
neural network features are circulant and are captured by the AGOP. In Section 5, we prove that kernel
machines learn the FMA with circulant features. We provide a broad discussion and conclude in Section 6.
2 Preliminaries
Learning modular arithmetic. Let Z = Z/pZ denote the field of integers modulo a prime p and let
p
Z∗ =Z \{0}. We learn modular functions f∗(a,b)=g(a,b)modp where f∗ :Z ×Z →Z , a,b∈Z , and
p p p p p p
g : Z×Z → Z is an arithmetic operation on a and b, e.g. g(a,b) = a+b. Note that there are p2 discrete
input pairs (a,b) for all modular operations except for f∗(a,b)=(a÷b)modp, which has p(p−1) inputs as
the denominator cannot be 0.
To train models on modular arithmetic tasks, we construct input-label pairs by one-hot encoding the input
andlabelintegers. Specifically,foreverypaira,b∈Z ,wewritetheinputase ⊕e ∈R2p andtheoutputas
p a b
e ∈Rp, where e ∈Rp is the i-th standard basis vector in p dimensions and ⊕ denotes concatenation.
f∗(a,b) i
For example, for addition modulo p = 3, the equality 1+2 = 0mod3 is encoded as an input/label pair of
vectors in R6 and R3, respectively:
Input: (0 1 0 0 0 1) ; Label: (1 0 0) .
(cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
a=1 b=2 a+bmod3=0
The training dataset consists of a random subset of n = r ×N input/label pairs, where r is termed the
training fraction and N =p2 or p(p−1) is the number of possible discrete inputs.
Complex inner product and Discrete Fourier Transform (DFT). In our theoretical analysis in
Section 5, we will utilize the following notions of complex inner product and DFT. The complex inner
product ⟨·,·⟩C is a map from Cd×Cd →C of the form
⟨u,v⟩C =u⊤v¯ , (1)
√
where v¯ is the complex conjugate of v . Let i = −1 and let ω = exp(−2πi). The DFT is the map
j j d
F : Cd → Cd of the form F(u) = Fu, where F ∈ Cd×d is a unitary matrix with F
ij
= √1 ωij. In matrix
d
form, F is given as
 
1 1 1 ··· 1
1 ω ω2 ··· ωd−1 
F = √1  1 ω2 ω4 ··· ω2(d−1)   . (2)
d  . .
.
. .
.
. .
.
... . .
.
 

1 ωd−1 ω2(d−1) ··· ω(d−1)(d−1)
Circulant matrices. Aswewillshow,thefeaturesthatRFMsandneuralnetworkslearninordertosolve
modular arithmetic contain blocks of circulant matrices, which are defined as follows. Let σ : Rp → Rp be
the cyclic permutation which acts on a vector u∈Rp by shifting its coordinates by one cell to the right:
[σ(u)] =u , (3)
j j−1modp
4Algorithm 1 Recursive Feature Machine (RFM) [35]
Require: X,y,k,T,L ▷ Train data: (X,y), base kernel: k, iters.: T, matrix power: s, and bandwidth: L
M =I
0 d
for t=0,...,T −1 do
Solve α←k(X,X;M )−1y ▷ f(t)(x)=k(x,X;M )α
t t
M ←[G(f(t))]s
t+1
end for
return α,M ▷ Solution to kernel regression: α, and feature matrix: M
T−1 T−1
for j ∈ [p]. We write the ℓ-fold composition of this map σℓ(u) ∈ Rp with entries [σℓ(u)] = u . A
j j−ℓmodp
circulant matrix C ∈Rp×p is determined by a vector c=[c ,...,c ]∈Rp, and has form:
0 p−1
 
c
 σ(c) 
C = . . .  .
 . . . 
 . . . 
σp−1(c)
Namely,circulantmatricesarethoseinwhicheachrowisshiftedoneelementtotherightofitsprecedingrow.
Hankelmatricesarealsodeterminedbyavectorc,buttherowsarec,σ−1(c),...,σ−(p−1)(c). Whilecirculant
matrices have constant diagonals, Hankel matrices have constant anti-diagonals. To ease terminology, we
will use the word circulant to refer to Hankel or circulant matrices as previously defined.
Average Gradient Outer Product (AGOP). The AGOP matrix, which will be central to our discus-
sion, is defined as follows.
Definition 2.1 (AGOP). Given a predictor f : Rd → Rc with c outputs, f(x) ≡ [f (x),...,f (x)], let
0 c−1
∂f(x′) ∈Rd×c be the Jacobian of f evaluated at some point x′ ∈Rd with entries [∂f(x′)] = ∂fℓ(x′).5 Then,
∂x ∂x ℓ,s ∂xs
forf trainedonasetofdatapoints{x(j)}n , withx(j) ∈Rd, theAverageGradientOuterProduct(AGOP),
j=1
G, is defined as,
1
(cid:88)n ∂f(x(j))∂f(x(j))⊤
G(f;{x(j)}n )= ∈Rd×d. (4)
j=1 n ∂x ∂x
j=1
For simplicity, we omit the dependence on the dataset in the notation. Top eigenvectors of AGOP can be
viewed as the “most relevant” input features, those input directions that influence the output of a general
predictor (for example, a kernel machines or a neural network) the most. As a consequence, the AGOP can
beviewedasatask-specifictransformationthatcanbeusedtoamplifyrelevantfeaturesandimprovesample
efficiency of machine learning models.
Indeed, a line of prior works [11, 18, 40, 44] have used the AGOP to improve the sample efficiency of
predictorstrainedonmulti-indexmodels, aclassofpredictivetasksinwhichthetargetfunctiondependson
a low-rank subspace of the data. The AGOP was recently shown to be a key mechanism for understanding
feature learning in neural networks and enabling feature learning in general machine learning models [35].
Though the study of AGOP has been motivated by these multi-index examples, we will see that the AGOP
can be used to recover useful features for modular arithmetic that are, in fact, not low-rank.
AGOPandfeaturelearninginneuralnetworks. Priorwork[35]positedthatAGOPwasamechanism
through which neural networks learn features. In particular, the authors posited the Neural Feature Ansatz
5WhiletheJacobianistypicallydefinedasalinearmapfromRd→Rc,wewilluseitstransposeinthiswork. Whenc=1,
thisissimplythegradientoff.
5
|
|
|
|
|
|A
Accuracy & Loss
RFM
Circ:
frob
RFM Iterations RFM Iterations RFM Iterations RFM Iterations
B Progress Measures
RFM Iterations RFM Iterations RFM Iterations RFM Iterations
Add Sub Mul Div
Figure 2: RFM with the quadratic kernel on modular arithmetic with modulus p = 61 trained for 30
iterations. (A) Test accuracy, test loss (mean squared error) over all output coordinates, and test loss of
the correct class output coordinate do not change in the first 8 iterations and then, sharply transition.
(B) Circulant deviation and AGOP alignment show gradual progress towards generalizing solutions despite
accuracy and lossmetrics not changing in theinitial iterations. Formultiplication (Mul) and division (Div),
circulant deviation is measured with respect to the feature sub-matrices after reordering by the discrete
logarithm.
(NFA) stating that for any layer ℓ of a trained neural network with weights W , the Neural Feature Matrix
ℓ
(NFM),WTW ,arehighlycorrelatedtotheAGOPofthemodelcomputedwithrespecttotheinputoflayer
ℓ ℓ
ℓ. The NFA suggests that neural networks learn features at each layer by utilizing the AGOP. For more
details on the NFA, see Appendix A.
Recursive Feature Machine (RFM). Importantly, AGOP can be computed for any differentiable pre-
dictor,includingthosesuchaskernelmachinesthathavenonativefeaturelearningmechanism. Assuch,the
authorsof[35]developedanalgorithmknownasRFM,whichiterativelyusestheAGOPtoextractfeatures.
Below, wepresenttheRFMalgorithmusedinconjunctionwithkernelmachines. Supposewearegivendata
samples (X,y) ∈ Rn×d ×Rn where X contains n samples denoted {x(j)}n . Given an initial symmetric
j=1
positive-definite matrix M ∈ Rd×d, and Mahalanobis kernel k(·,·;M) : Rd ×Rd → R, RFM iterates the
0
following steps for t∈[T]:
Step 1 (Predictor training): f(t)(x)=k(x,X;M )α with α=k(X,X;M )−1y ; (5)
t t
Step 2 (AGOP update): M =[G(f(t))]s ; (6)
t+1
where s>0 is a matrix power and k(X,X;M)∈Rn×n denotes the matrix with entries [k(X,X;M)] =
j1j2
k(x(j1),x(j2);M) for j 1,j
2
∈ [n]. In this work, we select s = 21 for all experiments (see Algorithm 1). We
use the following two Mahalanobis kernels: (1) the quadratic kernel, k(x,x′;M) = (cid:0) x⊤Mx′(cid:1)2 ; and (2) the
6
fo
ssoL
tseT
ycaruccA
tseT
tnalucriC
POGA
tcerroC
)%(
ssoL
noitaiveD
tnemngilA
ssalC
tuptuOGaussian kernel k(x,x′;M)=exp(cid:16) −∥x−x′∥2 M(cid:17) , where for z ∈Rd, ∥z∥2 =z⊤Mz, and L is the bandwidth.
L M
3 Emergence with Recursive Feature Machines
In this section, we will demonstrate that RFM (Al-
gorithm1)exhibitssharptransitionsinperformance
Learned Feature Matrices (AGOP)
onfourmodulararithmetictasks(addition,subtrac-
tion, multiplication, and division) due to the emer-
A B
gence of features, which are block-circulant matri-
ces.
We will use a modulus of p = 61 and train RFM
with quadratic and Gaussian kernel machines (ex-
perimental details are provided in Appendix B). As
we solve kernel ridgeless regression exactly, all iter- Add Mul Div
ations of RFM result in zero training loss and 100% C (original) (original)
trainingaccuracy. ThetoptworowsofFig.2Ashow
that the first several iterations of RFM result in
near-zerotestaccuracyandapproximatelyconstant,
largetestloss. Despitethesestandardprogressmea-
sures initially not changing, continuing to iterate
RFM leads to a dramatic, sharp increase to 100%
Sub Mul Div
test accuracy and a corresponding decrease in the (reordered) (reordered)
test loss later in the iteration process.
Sharp transition in loss of correct output co- Figure 3: RFM with the quadratic kernel for modular
ordinate. It is important to note that our total arithmetictaskswithmodulusp=61. (A)Thesquare
loss function is the square loss averaged over p=61 rootofthekernelAGOPsforaddition(Add),subtrac-
classes. It is thus plausible that, due to averaging, tion (Sub) visualized without their diagonals to em-
the near-constancy of the total square loss over the phasize circulant structure in off-diagonal blocks. (B)
first few iterations conceals steady improvements in Square root of the kernel AGOPs for multiplication
the predictions of the correct class. However, it is (Mul), division (Div). (C) For Mul and Div, rows and
not the case. In Fig. 2A (third row) we show that columns of the individual sub-matrices of the AGOP
the test loss for the output coordinate (logit) corre- arere-orderedbythediscretelogarithmbase2,reveal-
spondingtothecorrectclasscloselytracksthetotal ing the block-circulant structure in the features.
test loss.
Emergence of block-circulant features in RFM. In order to understand how RFM generalizes, we
visualize the 2p×2p matrix given by the square root of the AGOP of the final iteration of RFM. We refer
to these matrices as feature matrices. We first visualize the feature matrices for RFM trained on modular
addition/subtraction in Fig. 3A. Their visually-evident striped structure suggests the following more precise
characterization.
Observation 1 (Block-circulant features). Feature matrix M∗ ∈ R2p×2p at the final iteration of RFM on
modular addition/subtraction is of the form
(cid:18)
A
C⊤(cid:19)
M∗ = , (7)
C A
where A,C ∈ Rp×p and C is an asymmetric circulant matrix (i.e. C is not a degenerate circulant such as
C =I or C =11⊤). Furthermore, we note that A is of the form c I+c 11⊤ for constants c ,c .
1 2 1 2
Similarly to addition and subtraction, RFM successfully learns multiplication and division. Yet, in contrast
to addition and subtraction, the structure of feature matrices for these tasks, shown in Fig. 3B, is not at all
7obvious. Nevertheless, re-ordering the rows and columns of the feature matrices for these tasks brings out
their hidden circulant structure of the form stated in Eq. (7). We show the effect of re-ordering in Fig. 3C
(see also Appendix Fig. 1 for the evolution of re-ordered and original features during training). We briefly
discuss the reordering procedure below and provide further details in Appendix C.
Our reordering procedure uses the fact of group theory that the multiplicative group Z∗ is a cyclic group of
p
order p−1 (e.g., [19]). By definition of the cyclic group, there exists at least one element g ∈Z∗, known as
p
a generator, such that Z∗ = {gi ; i ∈ {1,...,p−1}}. Using zero-indexing for our coordinates, we reorder
p
rows and columns of the off-diagonal blocks of the feature matrices by moving the matrix entry in position
(r,c) with r,c∈Z∗ to position (i,j) where r =gi and c=gj. The entries in row zero and column zero are
p
not reordered as they are identically zero (multiplying any integer by zero results in zero). In the setting of
modular multiplication/division, the map taking gi to i is known as the discrete logarithm base g [19, Ch.3].
It is natural to expect block-circulant feature matrices to arise in modular multiplication/division after re-
orderingbythediscretelogarithmas(1)thediscretelogarithmconvertsmodularmultiplication/divisioninto
modular addition/subtraction and (2) we already observed block-circulant feature matrices in addition/sub-
traction in Fig. 3A. We note the recent work [12] also used the discrete logarithm to reorder coordinates in
the context of constructing a solution for solving modular multiplication with neural networks.
Progress measures. We will propose and examine two measures of feature learning, circulant deviation
and AGOP alignment.
Circulant deviation. The fact that the final feature matrices contain circulant sub-blocks suggests a natural
progress measure for learning modular arithmetic with RFM: namely, how far AGOP feature matrices are
from a block-circulant matrix.
For a feature matrix M, let A denote the bottom-left sub-block of M. We define circulant deviation as the
totalvarianceofthe(wrapped)diagonalsofAnormalizedbythenorm∥A∥2. Inparticular,letS ∈Rp×p →
F
Rp×p denote the shift operator, which shifts the ℓ-th row of the matrix by ℓ positions to the right. Also let
p−1
(cid:88)
Var(v)= (v −Ev)2
j
j=0
be the variance of a vector v. If A[j] denotes the j-th column of A, we define circulant deviation D as
p−1
1 (cid:88)
D(A)= Var(S(A)[j]). (Circulant deviation)
∥A∥2
F j=0
Circulant deviation is a natural measure of how far a matrix is from a circulant, since circulant matrices are
constant along their (wrapped) diagonals and, therefore, have a circulant deviation of 0.
We see in Fig. 2B (top row) that circulant deviation exhibits gradual improvement through the course
of training with RFM. We find that for the first 10 iterations, while the training loss is numerically zero
and the test loss does not improve, circulant deviation exhibits gradual, nearly linear, improvement. The
improvements in circulant deviation reflect visual improvements in features, as was also shown in Fig. 1.
These curves also provide further support for Observation 1, as the circulant deviation is close to 0 at the
end of training.
Note that circulant deviation is specific to modular arithmetic and depends crucially on the observation
that the feature matrices contained circulant blocks (upon reordering for multiplication/division). For more
generaltasks,wemaynotbeabletoidentifysuchstructure,andthus,wewouldneedamoregeneralprogress
measure that does not require a precise description of structures in generalizing features. To this end, we
propose a second progress measure, AGOP alignment, that applies beyond modular arithmetic.
8AGOP alignment. Given two matrices A,B ∈ Rd×d, let ρ(A,B) denote the standard cosine similarity
between these two matrices when vectorized. Specifically, let A˜,B˜ ∈Rd2 denote the vectorization of A and
B respectively, then
⟨A˜,B˜⟩
ρ(A,B)= . (8)
∥A˜∥∥B˜∥
If M denotes the AGOP at iteration t of RFM (or epoch t of a neural network) and M∗ denotes the final
t
AGOP of the trained RFM (or neural network), then AGOP alignment at iteration t is given by ρ(M ,M∗).
t
The same measure of alignment was used in [46], except that alignment in [46] was computed with respect
to the AGOP of the ground truth model. Note that as modular operations are discrete, in our setting there
is no unique ground truth model for which AGOP can be computed.
Like circulant deviation, AGOP alignment exhibits gradual improvement in the regime that test loss is
constant and large (see Fig. 2B, bottom row). Moreover, AGOP alignment is a more general progress
measuresinceitdoesnotrequireassumptionsonthestructureoftheAGOP.Forinstance,AGOPalignment
can be measured without reordering for modular multiplication/division. While AGOP alignment does not
require a specific form of the final features, it is still an a posteriori measurement of progress as it requires
access to the features of a fully trained model.
Random circulant features allow standard kernels
to generalize. We conclude this section by providing
Add Mul
further evidence that the form of feature matrices given
inObservation1iskeytoenablinggeneralizationinkernel
machinestrainedtosolvemodulararithmetictasks. Note
thatinObservation1,weonlystatedthattheoff-diagonal
matrixC wasacirculant. Aswenowshow,atransforma-
tionwithagenericblock-circulantmatrixenableskernels
machinestolearnmodulararithmetic. Namely,wegener-
atearandomcirculantmatrixC byfirstsamplingentries
of the first column i.i.d. from the uniform distribution
on [0,1] ⊂ R and then shifting the column to generate
the remaining columns of C. We proceed to construct
the matrix M∗ in Observation 1 with c = 1,c = −1.
1 2 p
For modular addition, we transform the input data by Training fraction (%) Training fraction (%)
mapping x =e ⊕e to
ab a b
x˜ ab =(M∗)1 4x ab , (9) Figure 4: Random circulant features general-
ize with standard kernels for modular arithmetic
and then train on the new data pairs (x˜ ,e ) for
ab a+bmodp
a subset of all possible pairs (a,b)∈Z2. Note that trans- tasks. RFM with the Gaussian kernel on modu-
p
lar addition (Add) and multiplication (Mul) for
forming data with (M∗)1 4 is akin to using s = 1 in the
2 modulusp=61iscomparedtoastandardGaus-
RFM algorithm. We do the same for modular multipli-
sian kernel machine trained on random circulant
cation after reordering the random circulant by the dis-
features(forMul,thesub-blocksarecirculantaf-
crete logarithm as described above. The experiments in
terre-orderingbythediscretelogarithmbase2).
Fig.4demonstratethatstandardkernelmachinestrained
onfeaturematriceswithrandomcirculantblocks6 outperformkernel-RFMsthatlearnsuchfeaturesthrough
AGOP.
Additionally, we also find that directly enforcing circulant structure in the sub-matrices of M throughout
t
RFM iterations accelerates grokking and improves test loss (see Appendix D, Appendix Fig. 2).
These experiments provide strong evidence that the structure in Observation 1 is key for generalization on
modular arithmetic and, furthermore, no additional structure beyond a generic circulant is required.
6WehavefoundthatrandomHankelfeaturesalsoenablegeneralizationwithstandardkernelmachines,similarlytorandom
circulantfeatures.
9
)%(
ycaruccA
tseT
ssoL
tseT
)%(
ycaruccA
tseT
ssoL
tseTA
Accuracy & Loss
NN
Circ:
frob
Epochs Epochs Epochs Epoch
B Progress Measures
Epochs Epochs Epochs Epochs
Add Sub Mul Div
Figure 5: One hidden layer fully-connected networks with quadratic activations trained on modular arith-
metic with modulus p = 61 trained for 50 epochs with the square loss. (A) Test accuracy, test loss over
all outputs, and test loss of the correct class output do not change in the initial iterations. (B) Progress
measures for circulant deviation and AGOP alignment. Circulant deviation for Mul and Div are computed
after reordering by the discrete logarithm base 2.
Remark: multiple skills. Throughout this section, we focused on modular arithmetic settings for a
single task. In more general domains such as language, one may expect there to be many “skills” that need
to be learned. In such settings, it is possible that these skills are grokked at different rates. While a full
discussionisbeyondthescopeofthiswork,toillustratethisbehavior,weperformedadditionalexperimentsin
AppendixEandAppendixFig.3wherewetrainRFMonapairofmodulararithmetictaskssimultaneously
and demonstrate that different tasks are indeed grokked at different points throughout training.
4 Emergence in neural networks through AGOP
We now show that grokking in two-layer neural networks relies on the same principles as grokking by
RFM. Specifically we demonstrate that (1) block-circulant features are key to neural networks grokking
modular arithmetic; and (2) our measures (circulant deviation and AGOP alignment) indicate gradual
progress towards generalization, while standard measures of generalization exhibit sharp transitions. All
experimental details are provided in Appendix B.
Grokking with neural networks. We first reproduce grokking with modular arithmetic using fully-
connected networks as identified in prior works (Fig. 5A) [15]. In particular, we train one hidden layer fully
connected networks f :R2p →Rp of the form
f(x)=W σ(W x) (10)
2 1
10
fo
ssoL
ycaruccA
ssoL
tcerroC
tnalucriC
POGA
)%(
noitaiveD
tnemngilA
ssalC
tuptuOA
NFM
B
NN
AGOP
Add Sub Mul (reordered) Div (reordered)
Figure 6: Feature matrices from one hidden layer neural networks with quadratic activations trained on
addition, subtraction, multiplication, and division modulo 61. The Pearson correlations between the NFM
andsquarerootoftheAGOPforeachtaskare0.955(Add), 0.942(Sub), 0.924(Mul), 0.929(Div). Muland
Div are shown after reordering by the discrete logarithm base 2.
withquadraticactivationσ(z)=z2 onmodulusp=61datawithatrainingfraction50%. Wetrainnetworks
using AdamW [23] with a batch size of 32.
Consistent with prior work [15] and analogously to RFMs, neural networks exhibit an initial training period
where the train accuracy reaches 100%, while test accuracy is at 0% and test loss does not improve (see
Fig. 5A).7 After this point, we see that the accuracy rapidly improves to achieve perfect generalization. As
we did for RFM, we verify that the sharp transition in test loss is not an artifact of averaging the loss over
alloutputcoordinates. InthethirdrowofFig.5Aweshowthatthetestlossoftheindividualcorrectoutput
coordinate closely tracks the total loss, exhibiting the same transition.
Emergence of block-circulant features in neural networks. In order to understand the features
learnedbyneuralnetworksformodulararithmetic, wevisualizethefirstlayerNeuralFeatureMatrix, which
is defined as follows.
Definition 4.1. Given a fully connected network of the form in Eq. (10), the first layer Neural Feature
Matrix (NFM) is the matrix W⊤W ∈R2p×2p.
1 1
We note the NFM is also the un-centered covariance of network weights and has been used in prior work
in order to understand the features learned by various neural network architectures at any layer [35, 41].
Fig. 6A displays the NFM for one hidden layer neural networks with quadratic activations trained on mod-
ular arithmetic tasks. For addition/subtraction, we find that the NFM exhibits block circulant structure,
akin to the feature matrix for RFM. As described in Section 3 and Appendix C, we reorder the NFM for
networks trained on multiplication/division with respect to a generator for Z∗ in order to observe block-
p
circulantstructure(seeAppendixFig.4Aforacomparisonofmultiplication/divisionNFMsbeforeandafter
reordering). The block-circulant structure in both the NFM and the feature matrix of RFM suggests that
the two models are learning similar sets of features.
Note that neural networks automatically learn features, as is demonstrated by visualizing the NFMs. The
work [35] posited that AGOP is the mechanism through which these models learn features. Indeed, the
authors stated their claim in the form of the Neural Feature Ansatz (NFA), which states that NFMs are
7Ofcourse,unlikeRFM,thetraininglossdecreasesgraduallythroughouttheoptimizationprocess.
11Add Mul
NN Random Circulant + NN NN Random Circulant + NN
Epochs Epochs Epochs Epochs
Figure 7: Random circulant features speed up generalization in neural networks for modular arithmetic
tasks. Wecompareonehiddenlayerfully-connectednetworkswithquadraticactivationstrainedonmodular
addition and multiplication for p = 61 using standard one-hot encodings or using one-hot encodings trans-
formed by random circulant matrices (re-ordered by the discrete logarithm in the case of multiplication).
17.5% - ¼ M matrix
proportional to a matrix power of AGOP through training (see Eq. (15) for a restatement of the NFA).
As such, we alternatively compute the square root of the AGOP to examine the features learned by neural
networks trained on modular arithmetic tasks. We visualize the square root of the AGOPs of these trained
models in Fig. 6B. Corroborating the findings from [35], we find that the square root of the AGOP and the
NFM are highly correlated (greater than 0.92), where Pearson correlation is equal to cosine similarity after
centeringtheinputstobemean0. Moreover,wefindthatthesquarerootofAGOPofneuralnetworksagain
exhibits the same structure as stated in Eq. (7) of Observation 1 (see Appendix Fig. 4B for a comparison of
multiplication/division AGOPs before and after reordering).
Random circulant maps improve generalization of neural networks. To further establish the
importanceofblock-circulantfeaturesforsolvingmodulararithmetictaskswithneuralnetworks,wedemon-
stratethattrainingnetworksoninputstransformedwitharandomblock-circulantmatrixgreatlyaccelerates
learning. In Fig. 7, we compare the performance of neural networks trained on one-hot encoded modulo p
integers and the same integers transformed using a random circulant matrix generated using the procedure
in Eq. (9). At a training fraction of 17.5%, we find that networks trained on transformed integers achieved
100% test accuracy within several hundred epochs and exhibit little delayed generalization while networks
trained on non-transformed integers do not achieve 100% test accuracy even within 3000 epochs.
Progress measures. GiventhatthesquarerootoftheAGOPofneuralnetworksexhibitsblock-circulant
structure, we can use circulant deviation and AGOP alignment to measure gradual progress of neural net-
works toward a generalizing solution. As before, we measure circulant deviation in the case of multiplica-
tion/division after reordering the feature submatrix by a generator of Z∗. In Fig. 5B, we observe that our
p
measures indicate gradual progress in contrast to sharp transitions in the standard measures of progress
shown in Fig. 5A. Indeed, there is a period of 5-10 epochs where circulant deviation and AGOP alignment
improve while test loss is large (and test accuracy is small) and does not improve. Therefore, as was the
case of RFM, these metrics reveal gradual progress of neural networks toward generalizing solutions.
AGOP regularization and weight decay. It has been argued in prior work that weight decay (ℓ
2
regularization on network weights) is necessary for grokking to occur when training neural networks for
modular arithmetic tasks [10, 31, 42]. Under the NFA (Eq. (15)), which states that W⊤W is proportional
1 1
to a matrix power of G(f), we expect that performing weight decay on the first layer, i.e., penalizing the
loss by ∥W ∥2 =tr(W⊤W ), should behave similarly to penalizing the trace of the AGOP, tr(G(f)), during
1 F 1 1
12
)%(
ycaruccA
ssoL
erauqS
)%(
ycaruccA
ssoL
erauqS
)%(
ycaruccA
ssoL
erauqS
)%(
ycaruccA
ssoL
erauqStraining.8 To this end, we compare the impact of using (1) no regularization; (2) weight decay; and (3)
AGOP regularization when training neural networks on modular arithmetic tasks. In Appendix Fig. 5, we
findthat,akintoweightdecay,AGOPregularizationleadstogrokkingincaseswhereusingnoregularization
results in no grokking and poor generalization. These results provide further evidence that neural networks
solve modular arithmetic by using the AGOP to learn features.
5 Fourier multiplication algorithm from circulant features
Wehaveseensofarthatfeaturescontainingcirculantsub-blocksenablegeneralizationforRFMsandneural
networks across modular arithmetic tasks. We now provide theoretical support that shows how kernel
machines equipped with such circulant features learn generalizing solutions. In particular, we show that
there exist block-circulant feature matrices, as in Observation 1, such that kernel machines equipped with
these features and trained on all available data for a given modulus p solve modular arithmetic through
the Fourier Multiplication Algorithm (FMA). Notably, the FMA has been argued both empirically and
theoreticallyinpriorworkstobethesolutionfoundbyneuralnetworkstosolvemodulararithmetic[31,45].
For completeness, we state the FMA for modular addition/subtraction from [31] below. While these prior
works write this algorithm in terms of cosines and sines, our presentation simplifies the statement by using
the DFT.
Fourier Multiplication Algorithm for modular addition/subtraction. Considerthemodularaddi-
tion task with f∗(a,b)=(a+b)modp. For a given input x=x ⊕x ∈R2p, the FMA generates a value
[1] [2]
for output class ℓ, y (x;ℓ), through the following computation:
add
1. Compute the Discrete Fourier Transform (DFT) for each digit vector x and x , which we denote
[1] [2]
x =Fx and x =Fx where the matrix F is defined in Eq. (2).
(cid:98)[1] [1] (cid:98)[2] [2]
2. Compute the element-wise product x ⊙x .
(cid:98)[1] (cid:98)[2]
√
3. Return p · ⟨x
(cid:98)[1]
⊙x (cid:98)[2],Fe ℓ⟩C where e
ℓ
denotes ℓ-th standard basis vector and ⟨·,·⟩C denotes the
complex inner product (see Eq. (1)).
This algorithmic process can be written concisely in the following equation:
√ (cid:10) (cid:11)
y (x;ℓ)= p· Fx ⊙Fx ,Fe . (11)
add [1] [2] ℓ C
Note that for x=e ⊕e , the second step of the FMA reduces to
a b
1
Fe ⊙Fe = √ Fe . (12)
a b p (a+b)modp
Using the fact that F is a unitary matrix, the output of the FMA is given by
(cid:28) (cid:29)
√ 1
p· √ Fe ,Fe =e⊤ F⊤F¯e =e⊤ e =1 . (13)
p (a+b)modp ℓ (a+b)modp ℓ (a+b)modp ℓ {(a+b)modp=ℓ}
C
Thus, the output of the FMA is a vector e , which is equivalent to modular addition. We provide
(a+b)modp
an example of this algorithm for p=3 in Appendix F.
Remarks. WenotethatourdescriptionoftheFMAusesallentriesoftheDFT,referredtoasfrequencies,
while the algorithm as proposed in prior works allows for utilizing a subset of frequencies. Also note that
the FMA for subtraction, written y , is similar and given by
sub
√ (cid:10) (cid:11)
y (x;ℓ)= p· Fx ⊙Fe ,Fx . (14)
sub [1] p−ℓ−1 [2] C
Having described the FMA, we now state our theorem.
8WenotethisregularizerbeenusedpriorworkwhereAGOPiscalledtheGrammatrixoftheinput-outputJacobian[17].
13(cid:8)(cid:0) (cid:1)(cid:9)p−1
Theorem 5.1. Given all of the discrete data e ⊕e ,e , for each output class ℓ ∈
a b (a−b)modp a,b=0
{0,··· ,p−1}, suppose we train a separate kernel predictor f (x) = k(x,X;M )α(ℓ) where k(·;·;M ) is a
ℓ ℓ ℓ
(cid:18)
0
Cℓ(cid:19)
quadratic kernel with M = and C ∈Rp×p is a circulant matrix with first row e . When α(ℓ)
ℓ (Cℓ)⊤ 0 1
is the solution to kernel ridgeless regression for each ℓ, the kernel predictor f = [f ,...,f ] is equivalent
0 p−1
to Fourier Multiplication Algorithm for modular subtraction (Eq. (14)).
As C is circulant, Cℓ is also circulant. Hence, each M has the structure described in Observation 1, where
ℓ
A=0. NoteourconstructiondiffersfromRFMinthatweuseadifferentfeaturematrixM foreachoutput
ℓ
coordinate, rather than a single feature matrix across all output coordinates. Nevertheless, Theorem 5.1
provides support for the fact that block-circulant feature matrices can be used to solve modular arithmetic.
WeprovidetheproofforTheorem5.1forinAppendixG.TheargumentfortheFMAforaddition(Eq.(11))
is identical provided we replace Cℓ with CℓR and (Cℓ)⊤ with (CℓR)⊤ in each M , where R is the Hankel
ℓ
matrixthatreversestheroworder (i.e. onesalong themainanti-diagonal, zero’selsewhere), whosefirstrow
is e . An analogous result follows for multiplication and division under re-ordering by a group element,
p−1
as described in Section 3.
Our proof uses the well-known fact that circulant matrices can be diagonalized using the DFT matrix [14]
(see Lemma G.2 for a restatement of this fact). This fundamental relation intuitively connects circulant
features and the FMA. By using kernels with block-circulant Mahalanobis matrices, we effectively represent
theone-hotencodeddataintermsoftheirFouriertransforms. Weconjecturethatthisimplicitrepresentation
is what enables RFM to learn modular arithmetic with more general circulant matrices when training on
just a fraction of the discrete data.
Not only do neural networks and RFM learn similar features, we now have established a setting where
kernel methods equipped with block-circulant feature matrices learn the same out-of-domain solution as
neural networks on modular arithmetic tasks. This result is interesting, in part, as the only constraint
for generalization on these tasks is to obtain perfect accuracy on inputs that are standard basis vectors.
However, assuchfunctionscanbeextendedarbitrarilyoverallofR2d, thereareinfinitelymanygeneralizing
solutions. Therefore, the particular out-of-domain solution found by training is determined by the specifics
of the learning algorithm. It is intriguing that kernel-RFMs and neural networks, which are clearly quite
differentalgorithms,arebothimplicitlybiasedtowardsolutionsthatinvolveblock-circulantfeaturematrices.
6 Discussion and Conclusions
In recent years our understanding of generalization in machine learning has undergone dramatic changes.
Most classical analyses of generalization relied on the training loss serving as a proxy for the test loss and
thus a useful measure of generalization. Empirical results of deep learning have upended this long-standing
belief. In many settings, predictors that interpolate the data (fit the data exactly) can still generalize, thus
invalidatingtraininglossasapossiblepredictoroftestperformance. Thishasledtotherecentdevelopments
in understanding benign overfitting, not just in neural networks but even in classical kernel and linear
models[4,8]. Sincethetraininglossmaynotbeareliablepredictorforgeneralization,thecommonsuggestion
hasbeentousethevalidationlosscomputedonaseparatevalidationdataset,onethatisnotusedintraining
andisideallyindistinguishablefromthetestset. Thisprocedureisstandardpractice: neuralnetworktraining
is typically stopped once the validation set loss stops improving.
Emergent phenomena, such as grokking, show that we cannot rely even on validation performance at inter-
mediate training steps to predict generalization at the end of training. Indeed, validation loss at a certain
iteration may not be indicative of the validation loss itself only a few iterations later. Furthermore, it is
clear that, contrary to [38], these phase transitions in performance are not generally “a mirage” since, as we
demonstrate in this work, they are not predicted by standard a priori measures of performance, continuous
or discontinuous. Instead, at least in our setting, emergence is fully determined by feature learning, which
14is difficult to observe without having access to a fully trained generalizable model. Indeed, the progress
measures discussed in this work, as well as those suggested before in, e.g., [3, 12, 31] can be termed a
posteriori progress indicators. They all require either non-trivial understanding of the algorithm imple-
mented by a fully generalizable trained model (such as our circulant deviation, the Fourier gap considered
in [3], or the Inverse Participation Ratio in [12]) or access to such a model (such as AGOP alignment).
To sharpen this last point, consider generalizable features for modular
multiplication shown in Fig. 8 on the right. Aside from the block struc-
Mul (original) Mul (reordered)
ture, the original features shown in the left panel of Fig. 8 do not have a
visually identifiable pattern. In contrast, re-ordered features in the right
panel are clearly striped and are thus immediately suggestive of block-
circulants. Note that, as discussed in Section 3, reordering of features
requires understanding that the multiplicative group Z∗ is cyclic of order
p
p−1. Whileitisawell-knownresultingrouptheory,itisfarfromobvious
a priori. It is thus plausible that in other settings hidden feature struc-
Figure 8: Square root of AGOP
tures may be hard to identify due to a lack of comparable mathematical
for quadratic RFM trained on
insights.
modularmultiplicationwithp=
61 before reordering (left) and
Why is learning modular arithmetic surprising? The task of
after reordering (right).
learning modular operations appears to be fundamentally different from
manyotherstatisticalmachinelearningtasks. IncontinuousMLsettings,
we typically posit that the “ground truth” target function is smooth in
an appropriate sense. Hence any general purpose algorithm capable of learning smooth functions (such as,
for example, k-nearest neighbors) should be able to learn the target function given enough data. Primary
differences between learning algorithms are thus in sample and computational efficiency. In contrast, it is
not clear what principle leads to learning modular arithmetic from partial observations. There are many
ways to fill in the missing data and we do not know a simple inductive bias, such as smoothness, to guide us
towardasolution. Severalrecentworksarguedthatmarginmaximizationwithrespecttocertainnormscan
account for learning modular arithmetic [24, 26, 29]. While the direction is promising, general underlying
principles have not yet been elucidated.
Low rank learning. The problem of learning modular arithmetic can be viewed as a type of matrix
completion – completing the p×p matrix (so-called Cayley table) representing modular operations, from
partial observations. The best studied matrix completion problem is low rank matrix completion, where the
goal is to fill in missing entries of a low rank matrix from observing a subset of the entries [28, Ch.8]. While
many specialized algorithms exist, it has been observed that neural networks can recover low rank matrix
structures [16]. Notably, in a development paralleling the results of this paper, low-rank matrix completion
can provably be performed by linear RFMs using the same AGOP mechanism [36].
It is thus tempting to posit that grokking modular operations in neural networks or RFM can be explained
as a low rank prediction problem. Indeed modular operations can be implemented by an index 4 model,
i.e., a function of the form f = g(Ax), where x ∈ R2p and A is a rank 4 matrix (see Appendix H for the
construction). It is a plausible conjecture as there is strong evidence, empirical and theoretical, that neural
networks are capable of learning such multi-index models [9, 30] as well as low-rank matrix completion.
Furthermore, a phenomenon similar to grokking was discussed in [34, Fig. 5, 6] in the context of low rank
feature learning for both neural networks and RFM. However, despite the existence of generalizeable low
rank models, the actual circulant features learned by both Neural Networks and RFM are not low rank.
Interestingly, this observation mirrors the problem of learning parity functions through neural network in-
spiredminimumnorminterpolation,whichwasanalyzedin[1]. Whilesingle-directional(indexone)solutions
exist in that setting, the authors show that the minimum norm solutions are all multi-dimensional.
Emergence with respect to compute, training data size and model size. In this paper we have
primarily dealt with emergence at some point in the course of the training process. In that setting (e.g.,
Fig. 1) we can consider the x axis as measurement of the amount of compute, analogous to the number
15of epochs in training neural networks. The dependence of model quality on compute is a question of key
practical importance as every training process has to eventually stop. What if just one more iteration
endowed the model with new amazing skills? As we have shown, there is no simply identifiable progress
measurewithoutaccesstothetrainedmodelorsomeinsightintothealgorithmitimplements. Theevidence
is more mixed for emergence with respect to the training data. As we show in Appendix Fig. 6, while
the test accuracy improves fairly sharply at about 25% training fraction threshold, the test loss exhibits
gradual improvement as a function of the training data with no obvious phase transitions. Thus, unlike the
emergence with respect to compute, emergence with respect to the training data size may be a “mirage” in
the sense of [38]. As far as the model size is concerned, we note that kernel machines can be thought of as
neural networks of infinite width. We are thus not able to analyze emergence with respect to the model size
in these experiments.
Analyses of grokking. Recent works [20, 24, 26] argue that grokking occurs in neural networks through
a two phase mechanism that transitions from a “lazy” regime, with no feature learning, to a “rich” feature
learning regime. Our experiments clearly show that grokking in RFM does not undergo such a transition.
For RFM on modular arithmetic tasks, our progress measures indicate that the features evolve gradually
toward the final circulant matrices, even as test performance initially remains constant (Fig. 2). Grokking
inthesesettingsisentirelyduetothegradualfeaturequalityimprovementandtwo-phasegrokkingdoesnot
occur. Therefore, two-phase cannot serve as a general explanation of grokking with respect to the compute.
Additionally, we have not observed significant evidence of “lazy” to “rich” transition as a mechanism for
grokkinginourexperimentswithneuralnetworks,asmostofourmeasuresoffeaturelearningstartimproving
early on in the training process (improvement in circulant deviation measure is delayed for addition and
subtraction, but not for multiplication and division, while AGOP feature alignment is initially nearly linear
for all tasks), see Fig. 5. These observations for neural networks are in line with the results in [12, 31],
where their proposed progress measures, Inverse Participation Ratio and Gini coefficients of the weights
in the Fourier domain, are shown to increase prior to improvements in test loss and accuracy for modular
multiplication and addition.
Furthermore,asgrokkingmodulararithmeticoccursinakernelmodelequippedwithalinearfeaturelearning
mechanism, a general explanation for grokking cannot depend on mechanisms that are specific to neural
networks. Therefore, explanations for grokking that depend on the magnitude of the weights or neural
circuitefficiency(e.g.,[31,42])orotherattributesofneuralnetworks,suchasspecificoptimizationmethods,
cannot account for the phenomena described in our work.
Conclusions. In this paper, we showed that grokking modular arithmetic happens in feature learning
kernel machines in a manner very similar to what has been observed in neural networks. Perhaps the most
unexpected aspect of our findings is that feature learning can happen independently of improvements in
both training and test loss. Note that this is hard to observe in the context of neural networks as non-zero
training loss is required for the training process. Not only does this finding reinforce the narrative of rapid
emergence of skills in neural networks, it is also not easily explicable within the framework of the existing
generalization theory.
Finally,thisworkaddstothegrowingbodyofevidencethattheAGOP-basedmechanismsoffeaturelearning
canaccountforsomeofthemostinterestingphenomenaindeeplearning. Theseincludegeneralizationwith
multi-indexmodels[32],deepneuralcollapse[7],andtheabilitytoperformlow-rankmatrixcompletion[36].
Thus, RFM provides a framework that is both practically powerful and serves as a theoretically tractable
model of deep learning.
Acknowledgements
We acknowledge support from the National Science Foundation (NSF) and the Simons Foundation for the
CollaborationontheTheoreticalFoundationsofDeepLearningthroughawardsDMS-2031883and#814639
as well as the TILOS institute (NSF CCF-2112665). This work used the programs (1) XSEDE (Extreme
16scienceandengineeringdiscoveryenvironment)whichissupportedbyNSFgrantnumbersACI-1548562,and
(2)ACCESS(Advancedcyberinfrastructurecoordinationecosystem: services&support)whichissupported
byNSFgrantsnumbers#2138259,#2138286,#2138307,#2137603,and#2138296. Specifically,weusedthe
resourcesfromSDSCExpanseGPUcomputenodes,andNCSADeltasystem,viaallocationsTG-CIS220009.
NMandARgratefullyacknowledgefundingandsupportforthisresearchfromtheEricandWendySchmidt
Center at the Broad Institute of MIT and Harvard. The authors would like to thank Jonathan Xue for
assistanceintheinitialphaseoftheproject, DanielHsuforinsightfulcommentsandreferences, andDarshil
Doshi and Tianyu He for useful discussion on related literature. NM would additionally like to thank Sarah
Heller for editing suggestions.
References
[1] N.Ardeshir, D.J.Hsu, andC.H.Sanford. Intrinsicdimensionalityandgeneralizationpropertiesofthe
r-norm inductive bias. In G. Neu and L. Rosasco, editors, Proceedings of Thirty Sixth Conference on
Learning Theory, volume 195 of Proceedings of Machine Learning Research, pages 3264–3303. PMLR,
12–15 Jul 2023.
[2] S. Arora and A. Goyal. A theory for emergence of complex skills in language models. arXiv preprint
arXiv:2307.15936, 2023.
[3] B.Barak,B.Edelman,S.Goel,S.Kakade,E.Malach,andC.Zhang. Hiddenprogressindeeplearning:
Sgd learns parities near the computational limit. Advances in Neural Information Processing Systems,
35:21750–21764, 2022.
[4] P. L. Bartlett, A. Montanari, and A. Rakhlin. Deep learning: a statistical viewpoint. Acta numerica,
30:87–201, 2021.
[5] D. Beaglehole, I. Mitliagkas, and A. Agarwala. Feature learning as alignment: a structural property of
gradient descent in non-linear neural networks. arXiv preprint arXiv:2402.05271, 2024.
[6] D. Beaglehole, A. Radhakrishnan, P. Pandit, and M. Belkin. Mechanism of feature learning in convo-
lutional neural networks. arXiv preprint arXiv:2309.00570, 2023.
[7] D.Beaglehole,P.Su´ken´ık,M.Mondelli,andM.Belkin. Averagegradientouterproductasamechanism
for deep neural collapse. arXiv preprint arXiv:2402.13728, 2024.
[8] M. Belkin. Fit without fear: remarkable mathematical phenomena of deep learning through the prism
of interpolation. Acta Numerica, 30:203–248, 2021.
[9] A. Damian, J. Lee, and M. Soltanolkotabi. Neural networks can learn representations with gradient
descent. In Conference on Learning Theory, pages 5413–5452. PMLR, 2022.
[10] X. Davies, L. Langosco, and D. Krueger. Unifying grokking and double descent. ML Safety Workshop,
36th Conference on Neural Information Processing Systems (NeurIPS 2022), 2023.
[11] I.Diakonikolas, D.M.Kane,V.Kontonis,C.Tzamos,andN.Zarifis. Agnosticallylearningmulti-index
models with queries. arXiv preprint arXiv:2312.16616, 2023.
[12] D.Doshi, T.He, A.Das, andA.Gromov. Grokkingmodularpolynomials. International Conference on
Learning Representations (ICLR): BGPT Workshop, 2024.
[13] H. Furuta, G. Minegishi, Y. Iwasawa, and Y. Matsuo. Interpreting grokked transformers in complex
modular arithmetic. arXiv preprint arXiv:2402.16726, 2024.
[14] R. M. Gray et al. Toeplitz and circulant matrices: A review. Foundations and Trends® in Communi-
cations and Information Theory, 2(3):155–239, 2006.
[15] A. Gromov. Grokking modular arithmetic. arXiv preprint arXiv:2301.02679, 2023.
17[16] S. Gunasekar, B. E. Woodworth, S. Bhojanapalli, B. Neyshabur, and N. Srebro. Implicit regularization
in matrix factorization. Advances in neural information processing systems, 30, 2017.
[17] J. Hoffman, D. A. Roberts, and S. Yaida. Robust learning with jacobian regularization. arXiv preprint
arXiv:1908.02729, 5(6):7, 2019.
[18] M. Hristache, A. Juditsky, J. Polzehl, and V. Spokoiny. Structure adaptive approach for dimension
reduction. Annals of Statistics, pages 1537–1566, 2001.
[19] N. Koblitz. A course in number theory and cryptography, volume 114. Springer Science & Business
Media, 1994.
[20] T. Kumar, B. Bordelon, S. J. Gershman, and C. Pehlevan. Grokking as the transition from lazy to rich
training dynamics. International Conference on Learning Representations (ICLR), 2024.
[21] Z. Liu, O. Kitouni, N. S. Nolte, E. Michaud, M. Tegmark, and M. Williams. Towards understanding
grokking: An effective theory of representation learning. Advances in Neural Information Processing
Systems, 35:34651–34663, 2022.
[22] Z. Liu, E. J. Michaud, and M. Tegmark. Omnigrok: Grokking beyond algorithmic data. International
Conference on Learning Representations (ICLR), 2023.
[23] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101,
2017.
[24] K.Lyu, J.Jin, Z.Li, S.S.Du, J.D.Lee, andW.Hu. Dichotomyof earlyandlatephaseimplicitbiases
can provably induce grokking. In The Twelfth International Conference on Learning Representations
(ICLR), 2023.
[25] J. Miller, C. O’Neill, and T. Bui. Grokking beyond neural networks: An empirical exploration with
model complexity. Transactions on Machine Learning Research (TMLR), 2024.
[26] M. A. Mohamadi, Z. Li, L. Wu, and D. J. Sutherland. Why do you grok? a theoretical analysis on
grokkingmodularaddition. InForty-firstInternationalConferenceonMachineLearning(ICML),2024.
[27] M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of machine learning. MIT Press, 2018.
[28] A. Moitra. Algorithmic aspects of machine learning. Cambridge University Press, 2018.
[29] D. Morwani, B. L. Edelman, C.-A. Oncescu, R. Zhao, and S. Kakade. Feature emergence via margin
maximization: case studies in algebraic tasks. International Conference on Learning Representations
(ICLR), 2024.
[30] A.Mousavi-Hosseini,S.Park,M.Girotti,I.Mitliagkas,andM.A.Erdogdu. Neuralnetworksefficiently
learn low-dimensional representations with sgd. arXiv preprint arXiv:2209.14863, 2022.
[31] N. Nanda, L. Chan, T. Lieberum, J. Smith, and J. Steinhardt. Progress measures for grokking via
mechanistic interpretability. International Conference on Learning Representations (ICLR), 2023.
[32] S. Parkinson, G. Ongie, and R. Willett. Relu neural networks with linear layers are biased towards
single- and multi-index models. arXiv preprint arXiv:2305.15598, 2023.
[33] A. Power, Y. Burda, H. Edwards, I. Babuschkin, and V. Misra. Grokking: Generalization beyond
overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177, 2022.
[34] A. Radhakrishnan, D. Beaglehole, P. Pandit, and M. Belkin. Mechanism of feature learning in
deep fully connected networks and kernel machines that recursively learn features. arXiv preprint
arXiv:2212.13881, 2022.
[35] A. Radhakrishnan, D. Beaglehole, P. Pandit, and M. Belkin. Mechanism for feature learning in neural
networks and backpropagation-free machine learning models. Science, 383(6690):1461–1467, 2024.
18[36] A.Radhakrishnan, M.Belkin, andD.Drusvyatskiy. Linearrecursivefeaturemachinesprovablyrecover
low-rank matrices. arXiv preprint arXiv:2401.04553, 2024.
[37] A. Rogers and S. Luccioni. Position: Key claims in llm research have a long tail of footnotes. In
Forty-first International Conference on Machine Learning, 2023.
[38] R. Schaeffer, B. Miranda, and S. Koyejo. Are emergent abilities of large language models a mirage? In
Thirty-seventh Conference on Neural Information Processing Systems, 2023.
[39] V. Thilak, E. Littwin, S. Zhai, O. Saremi, R. Paiss, and J. Susskind. The slingshot mechanism: An
empiricalstudyofadaptiveoptimizersandthegrokkingphenomenon. arXivpreprintarXiv:2206.04817,
2022.
[40] S.Trivedi,J.Wang,S.Kpotufe,andG.Shakhnarovich. Aconsistentestimatoroftheexpectedgradient
outerproduct. In UAI, pages 819–828, 2014.
[41] A. Trockman, D. Willmott, and J. Z. Kolter. Understanding the covariance structure of convolutional
filters. arXiv preprint arXiv:2210.03651, 2022.
[42] V.Varma,R.Shah,Z.Kenton,J.Kram´ar,andR.Kumar.Explaininggrokkingthroughcircuitefficiency.
International Conference on Learning Representations (ICLR), 2023.
[43] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou,
D. Metzler, E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, and W. Fedus. Emergent abilities
of large language models. Transactions on Machine Learning Research (TMLR), 2022.
[44] G.Yuan,M.Xu,S.Kpotufe,andD.Hsu.Efficientestimationofthecentralmeansubspaceviasmoothed
gradient outer products. arXiv preprint arXiv:2312.15469, 2023.
[45] Z. Zhong, Z. Liu, M. Tegmark, and J. Andreas. The clock and the pizza: Two stories in mechanistic
explanation of neural networks. Advances in Neural Information Processing Systems, 36, 2024.
[46] L. Zhu, C. Liu, A. Radhakrishnan, and M. Belkin. Catapults in sgd: spikes in the training loss and
theirimpactongeneralizationthroughfeaturelearning. International Conference on Machine Learning
(ICML), 235, 2024.
19A Neural Feature Ansatz
While the NFA has been observed generally across depths and architecture types [5, 6, 35], we restate this
observation for fully-connected networks with one hidden-layer of the form in Eq. (10).
Ansatz 1 (Neural Feature Ansatz for one hidden layer). For a one hidden-layer neural network fNN and a
matrix power α∈(0,1], the following holds:
W⊤W ∝G(fNN)s . (15)
1 1
Note that this statement implies that W⊤W and G(fNN)s have a cosine similarity of ±1.
1 1
In this work, we choose α = 1, following the main results in [35]. While the absolute value of the cosine
2
similarityiswritteninEq.(15)tobe1,itistypicallyahighvaluelessthan1,wheretheexactvaluedepends
on choices of initialization, architecture, dataset, and training procedure. For more understanding of these
conditions, see [5].
B Model and training details
Gaussiankernel: ThroughoutthisworkwetakebandwidthL=2.5whenusingtheMahalanobisGaussian
kernel. We solve ridgeless kernel regression using NumPy on a standard CPU.
Neural networks: Unless otherwise specified, we train one hidden layer neural networks with quadratic
activation functions and no biases in PyTorch on a single A100 GPU. Models are trained using AdamW
with hidden width 1024, batch size 32, learning rate of 10−3, weight decay 1.0, and standard PyTorch
initialization. All models are trained using the Mean Squared Error loss function (square loss).
FortheexperimentsinAppendixFig.5,wetrainonehiddenlayerneuralnetworkswithquadraticactivation
and no biases on modular addition modulo p=61. We use 40% training fraction, PyTorch standard initial-
ization, hidden width of 512, weight decay 10−5, and AGOP regularizer weight 10−3. Models are trained
with vanilla SGD, batch size 128, and learning rate 1.0.
C Reordering feature matrices by group generators
Our reordering procedure uses the standard fact of group theory that the multiplicative group Z∗ is a cyclic
p
group of order p−1 [19]. By definition of the cyclic group, there exists at least one element g ∈Z∗, known
p
as a generator, such that Z∗ ={gi ; i∈{1,...,p−1}}.
p
Given a generator g ∈ Z∗, we reorder features according to the map, ϕ : Z∗ → Z∗, where if h = gi,
p g p p
then ϕ (h) = i. In particular, given a matrix B ∈ Rp×p, we reorder the bottom right (p−1)×(p−1)
g
sub-block of B as follows: we move the entry in coordinate (r,c) with r,c∈Z∗ to coordinate (ϕ (r),ϕ (c)).
p g g
For example if g = 2 in Z∗, then (2,3) entry of the sub-block would be moved to coordinate (1,3) since
5
21 = 2 and 23mod5 = 3. In the setting of modular multiplication/division, the map ϕ defined above is
g
known as the discrete logarithm base g [19, Ch.3]. The discrete logarithm is analogous to the logarithm
defined for positive real numbers in the sense that it converts modular multiplication/division into modular
addition/subtraction. Lastly, in this setting, we note that we only reorder the bottom (p−1)×(p−1)
sub-block of B as the first row and column are 0 (as multiplication by 0 results in 0).
Upon re-ordering the p×p off-diagonal sub-blocks of the feature matrix by the map ϕ , the feature matrix
g
of RFM for multiplication/division tasks contains circulant blocks as shown in Fig. 3C. Thus, the reordered
feature matrices for these tasks also exhibit the structure in Observation 1. As a remark, we note that
there can exist several generators for a cyclic group, and thus far, we have not specified the generator g we
20use for re-ordering. For example, 2 and 3 are both generators of Z∗ since {2,22,(23mod5),(24mod5)} =
5
{3,(32mod5),(33mod5),(34mod5)}=Z∗. LemmaG.1impliesthatthechoiceofgeneratordoesnotmatter
5
for observing circulant structure. As a convention, we simply reorder by the smallest generator.
D Enforcing circulant structure in RFM
We see that the structure in Observation 1 gives generalizing features on modular arithmetic when the
circulantC isconstructedfromtheRFMmatrix. Weobservethatenforcingthisstructureateveryiteration,
and comparing to the standard RFM model at that iteration, improves test loss and accelerates grokking
on e.g. addition (Appendix Fig. 2). The exact procedure to enforce this structure is as follows. We first
perform standard RFM to generate feature matrices M ,...,M . Then for each iteration of the standard
1 T
RFM, we construct a new M(cid:102)t on which we solve ridgeless kernel regression for a new α and evaluate on the
test set. To construct M(cid:102), we take D = diag(M t) and first let M(cid:102) = D−1/2MD−1/2, to ensure the rows
and columns have equal scale. We then reset the top left and bottom right sub-matrices of M(cid:102)as I− 111T,
p
and replace the bottom-left and top-right blocks with C and C⊤, where C is an exactly circulant matrix
constructed from M . Specifically, where c is the first column of the bottom-left sub-matrix of M , column
t t
ℓ of C is equal to σℓ(M ).
t
E Grokking multiple tasks
We train RFM to simultaneously solve the following two modular polynomial tasks: (1) x+ymodp ; (2)
x2+y2modp for modulus p = 61. We train RFM with the Mahalanobis Gaussian kernel using bandwidth
parameter L = 2.5. Training data for both tasks is constructed from the same 80% training fraction. In
addition to concatenating the one-hot encodings for x,y, we also append an extra bit indicating which task
to solve (0 indicating task (1) and 1 indicating task (2)). The classification head is shared for both tasks
(e.g. output dimension is still Rp).
In Appendix Fig. 3, we observe that there are two sharp transitions in the test loss and test accuracy. By
decomposing the loss into the loss per task, we observe that RFM groks task (1) prior to grokking task (2).
Overall, these results illustrate that grokking of different tasks can occur at different training iterations.
F FMA example for p = 3
We now provide an example of the FMA for p = 3. Let x = e ⊕e . In this case, we expect the FMA to
1 2
output the vector e since (1+2)mod3=0. Following the first step of the FMA, we compute
0
1 1
x =Fe = √ [1,ω,ω2]⊤ ; x =Fe = √ [1,ω2,ω4]⊤ , (16)
(cid:98)[1] 1 (cid:98)[2] 2
3 3
which are the first and second columns of F, respectively. Then their element-wise product is given by
1 1 1
Fe ⊙Fe = [1,ω3,ω6]⊤ = [1,1,1]⊤ = √ Fe , (17)
1 2 3 3 3 0
√ (cid:68) (cid:69)
whichis √1 timesthefirstcolumnoftheDFTmatrix. Finally, wecomputetheoutputs 3 √1 Fe 0,Fe
ℓ
3 3 C
for each ℓ ∈ {0,1,2}. As F is unitary, y (e ⊕e ;ℓ) = 1 , so that coordinate 0 of the output
add 1 2 {1+2=ℓmod3}
will have value 1, and all other coordinates have value 0.
G Additional results and proofs
Lemma G.1. Let C ∈ Rp×p with its first row and column entries all equal to 0. Let the (p−1)×(p−1)
sub-block starting at the second row and column be C×. Then, C× is either circulant after re-ordering by
any generator q of Z∗, or C× is not circulant under re-ordering by any such generator.
p
21Proof of Lemma G.1. We prove the lemma by showing that for any two generators q ,q of Z∗, if C× is
1 2 p
circulant re-ordering with q , then it is also circulant when re-ordering by q .
1 2
SupposeC× iscirculantre-orderingwithq . Leti,j ∈{1,...,p−1}. Notethatbythecirculantassumption,
1
for all s∈Z,
C =C , (18)
qi,qj qi+s,qi+s
1 1 1 1
where we take each index modulo p.
As q is a generator for Z∗, we can access all entries of C× by indexing with powers of q . Further, as q is
2 p 2 1
a generator, we can write q =qk, for some power k. Let a∈Z. Then,
2 1
C =C
qi,qj qki,qkj
2 2 1 1
=C
qki+ka,qkj+ka
1 1
=C
qk(i+a),qk(j+a)
1 1
=C .
qi+a,qj+a
2 2
Therefore, C is constant on the diagonals under re-ordering by q , concluding the proof.
2
We next state Lemma G.2, which is used in the proof of Theorem 5.1.
Lemma G.2 (See, e.g., [14]). Circulant matrices U can be written (diagonalized) as:
U =FDF¯⊤ ,
where F is the DFT matrix, F¯⊤ is the element-wise complex conjugate of F⊤ (i.e. the Hermitian of F),
√
and D is a diagonal matrix with diagonal p·Fu, where u is the first row of U.
We now present the proof of Theorem 5.1, restating the theorem below for the reader’s convenience.
(cid:8)(cid:0) (cid:1)(cid:9)p−1
Theorem. Given all of the discrete data e ⊕e ,e , for each output class ℓ∈{0,··· ,p−
a b (a−b)modp a,b=0
1}, suppose we train a separate kernel predictor f (x) = k(x,X;M )α(ℓ) where k(·;·;M ) is a quadratic
ℓ ℓ ℓ
(cid:18)
0
Cℓ(cid:19)
kernel with M = and C ∈ Rp×p is a circulant matrix with first row e . When α(ℓ) is
ℓ (Cℓ)⊤ 0 1
the solution to kernel ridgeless regression for each ℓ, the kernel predictor f = [f ,...,f ] is equivalent to
0 p−1
Fourier Multiplication Algorithm for modular subtraction (Eq. (14)).
Proof of Theorem 5.1. We present the proof for modular subtraction as the proof for addition follows anal-
ogously. We write the standard kernel predictor for class ℓ on input x=x ⊕x ∈R2p as,
[1] [2]
p−1
f (x)= (cid:88) α(ℓ)k(x,e ⊕e ;M ) ,
ℓ a,b a b ℓ
a,b=0
wherewehavere-writtentheindexintokernelcoefficientsforclassℓ,α(ℓ) ∈Rp×p,sothatthecoefficientsare
multi-indexed by the first and second digit. Specifically, now α(ℓ) is the kernel coefficient corresponding to
a,b
therepresenterk(·,x)forinputpointx=e ⊕e . Recallweuseaquadratickernel,k(x,z;M )=(x⊤M z)2.
a b ℓ ℓ
In this case, the kernel predictor simplifies to,
p−1
f (x)=
(cid:88) α(ℓ)(cid:16)
x⊤Cℓe +e⊤Cℓx
(cid:17)2
.
ℓ a,b [1] b a [2]
a,b=0
22Then, the labels for each pair of input digits, written as a matrix Y(ℓ) ∈ Rp×p for the ℓ-th class where the
row and column index the first and second digit respectively, are Y(ℓ) =C−ℓ.
For x=e ⊕e , i.e. x in the discrete dataset, we have,
a′ b′
p−1
f (x)=
(cid:88) α(ℓ)(cid:0)
δ +δ +2δ δ
(cid:1)
ℓ a,b (a,b′−ℓ) (a′,b−ℓ) (a,b′−ℓ) (a′,b−ℓ)
a,b=0
=e⊤ α(ℓ)1+1⊤α(ℓ)e +2e⊤ α(ℓ)e
b′−ℓ a′+ℓ b′−ℓ a′+ℓ
=e⊤C−ℓα(ℓ)1+1⊤α(ℓ)C−ℓe +2e⊤C−ℓα(ℓ)C−ℓe
b′ a′ b′ a′
=e⊤(cid:0) C−ℓα11⊤+11⊤αC−ℓ+2C−ℓαC−ℓ(cid:1)
e ,
b′ a′
where δ = 1 . Let f (X) ∈ Rp×p be the matrix of function values of f , where [f (X)] =
(u,v) {u=v} ℓ ℓ ℓ a,b
f (e ⊕e ), and, therefore, f (e ⊕e ) = e⊤f (X)e . Then, to solve for α(ℓ), we need to solve the system
ℓ a b ℓ a b a ℓ b
of equations for α,
f (X)=(cid:0) C−ℓα11⊤+11⊤αC−ℓ+2C−ℓαC−ℓ(cid:1)⊤ =C−ℓ
ℓ
⇐⇒ C−ℓα11⊤+11⊤αC−ℓ+2C−ℓαC−ℓ =Cℓ
Note, by left-multiplying both sides by C−ℓ, we see this equation holds iff,
C−2ℓα11⊤+11⊤αC−ℓ+2C−2ℓαC−ℓ =I .
Notethesolutionisuniqueasthekernelmatrixisfullrank. Wepositthesolutionα suchthatC−2ℓαC−ℓ =
1I+λ11⊤, which is α= 1C3ℓ+λ11⊤. Then, solving for λ, we require,
2 2
11⊤+2pλ11⊤+2λ11⊤ =0 ,
whichimpliesλ=− 2 . Substitutingthisvalueofλandsimplifying,weseefinallythatf (x)=x⊤C−ℓx .
2p+2 √ ℓ [1] [2]
Therefore, using that circulant matrices are diagonalized by C = pFDF¯⊤ (Lemma G.2) and F¯⊤F = I,
where D =diag(Fe ), we derive,
1
√
f (x)= p·x⊤FD−ℓF¯⊤x
ℓ [1] [2]
√
= p·x⊤Fdiag(Fe )F¯⊤x
[1] p−ℓ−1 [2]
√ (cid:10) (cid:11)
= p· Fx ⊙Fe ,Fx
[1] p−ℓ−1 [2] C
which is the output of the FMA on modular subtraction.
H Low rank solution to modular arithmetic
Addition WepresentasolutiontothemodularadditiontaskwhoseAGOPislowrank, incontrasttothe
full rank AGOP recovered by RFM and neural networks.
We define the “encoding” map Φ:Rp →C as follows. For a vector a=[a ,...,a ],
0 p−1
p−1 (cid:18) (cid:19)
(cid:88) k2πi
Φ(a)= a exp .
k p
k=0
(cid:16) (cid:17)
Notice that Φ is a linear map such that Φ(e )=exp k2πi . Notice also that Φ is partially invertible with
k p
the “decoding” map Ψ:C→Rp.
(cid:18)(cid:28) (cid:18) (cid:19)(cid:29) (cid:28) (cid:18) (cid:19)(cid:29)(cid:19)
0·2πi (p−1)·2πi
Ψ(z)=max z,exp ,... z,exp .
(cid:103) p p
23Above max is a function that makes all entries zero except for the largest one and the inner product is the
(cid:103)
usual inner product in C considered as R2. Thus
(cid:18) (cid:18) (cid:19)(cid:19)
k·2πi
Ψ exp =e . (19)
p k
ΨisanonlinearmapC→Rp. Whileitisdiscontinuousbutcaneasilybemodifiedtomakeitdifferentiable.
By slight abuse of notation, we will define Φ:Rp×Rp →C2 on pairs:
Φ(e ,e )=(Φ(e ),Φ(e )) .
j k j k
This is still a linear map but now to C2.
Consider now a quadratic map M on C2 →C given by complex multiplication:
M(z ,z )=z z .
1 2 1 2
It is clear that the composition ΨMΦ implements modular addition
ΨMΦ(e ,e )=e
j k (j+k)modp
Furthermore, since Φ is a liner map to a four-dimensional space, the AGOP of the composition ΨMΦ is of
rank 4.
Multiplication The construction is for multiplication is very similar with modifications which we sketch
below. We first re-order the non-zero coordinates by the discrete logarithm with base equal to a generator
of the multiplicative group e (see Appendix C), while keeping the order of index 0. Then, we modify Φ to
g
remove index a from the sum for inputs a. Thus for multiplication,
0
p−1 (cid:18) (cid:19)
(cid:88) k·2πi
Φ(a)= a exp ,
k p−1
k=1
(cid:16) (cid:17) (cid:16) (cid:17)
Hence that Φ(e )=0, Φ(e )=exp 2πi and Φ(e )=exp k·2πi . We extend Φ to Rp×Rp as in Eq. 19
0 g p−1 gk p−1
above. Note that Φ and the re-ordering together are still a linear map of rank 4.
Then, the “decoding” map, Ψ(z), will be modified to return 0, when z =0, and otherwise,
Ψ(z)=gm(cid:103)ax(⟨z,exp(0 p· −2π 1i)⟩,...⟨z,exp((p− p2 −)· 12πi)⟩)
.
M is still defined as above. It is easy to check that the composition of ΨMΦ with reordering implements
modular multiplication modulo p and furthermore, the AGOP will also be of rank 4.
24Iter 1 Iter 5 Iter 10 Iter 15
Appendix Figure 1: AGOP evolution for quadratic RFM trained on modular multiplication with p = 61
before reordering (top row) and after reordering by the logarithm base 2 (bottom row).
RFM Iterations RFM Iterations
AppendixFigure2: WetrainaGaussiankernel-RFMonx+ymod97andplottestlossandaccuracyversus
RFMiterations. WealsoevaluatetheperformanceofthesamemodeluponmodifyingtheM matrixtohave
exact block-circulant structure stated in Observation 1.
25
luM
)deredroer(
luM
)%(
ycaruccA
tseT
ssoL
tseTBoth Tasks: Task 1: Task 2:
x + y and x2 + y2 x2 + y2 x + y
RFM Iterations RFM Iterations RFM Iterations
Appendix Figure 3: RFM with the Gaussian kernel trained on two modular arithmetic tasks with modulus
p=61. Task 1 is to learn x2+y2modp and task 2 is to learn x+ymodp.
A
NFM
B
NN
AGOP
Mul Mul (reordered) Div Div (reordered)
AppendixFigure4: (A)Wevisualizetheneuralfeaturematrix(NFM)fromaonehiddenlayerneuralnetwork
with quadratic activations trained on modular multiplication and division, before and after reordering by
the discrete logarithm. (B) We visualize the square root of the AGOP of the neural network in (A) before
and after reordering.
26
tseT
tseT
ssoL
ycaruccANo Regularization Weight Decay AGOP Regularization
Epochs Epochs Epochs
AppendixFigure5: Onehiddenlayerfullyconne0c.4te tdrnaientiwnog rfkrsawctiitohnquadraticactivationstrainedonmodular
addition with p = 61 with vanilla SGD. Without any regularization the test accuracy does not go to 100%
whereas using weight decay or regularizing using the trace of the AGOP result in 100% test accuracy and
grokking.
Training Fraction (%) Training Fraction (%) Training Fraction (%)
Appendix Figure 6: We train kernel-RFMs for 30 iterations using the Mahalanobis Gaussian kernel for
x+ymod97. We plot test accuracy, test loss, and AGOP alignment versus percentage of training data
used (denoted training fraction). All models reach convergence (i.e., both the test loss and test accuracy no
longerchange)after30iterations. Weobserveasharptransitionintestaccuracywithrespecttothetraining
fraction, but we observe gradual change in test loss and AGOP alignment with respect to the training data
fraction.
27
ycaruccA
ssoL
erauqS
ycaruccA
tseT
)%(
ycaruccA
ssoL
erauqS
ssoL
erauqS
tseT
ycaruccA
ssoL
erauqS
POGA
tnemngilA