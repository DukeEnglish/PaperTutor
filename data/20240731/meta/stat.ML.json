[
    {
        "title": "Emergence in non-neural models: grokking modular arithmetic via average gradient outer product",
        "authors": "Neil MallinarDaniel BeagleholeLibin ZhuAdityanarayanan RadhakrishnanParthe PanditMikhail Belkin",
        "links": "http://arxiv.org/abs/2407.20199v1",
        "entry_id": "http://arxiv.org/abs/2407.20199v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20199v1",
        "summary": "Neural networks trained to solve modular arithmetic tasks exhibit grokking, a\nphenomenon where the test accuracy starts improving long after the model\nachieves 100% training accuracy in the training process. It is often taken as\nan example of \"emergence\", where model ability manifests sharply through a\nphase transition. In this work, we show that the phenomenon of grokking is not\nspecific to neural networks nor to gradient descent-based optimization.\nSpecifically, we show that this phenomenon occurs when learning modular\narithmetic with Recursive Feature Machines (RFM), an iterative algorithm that\nuses the Average Gradient Outer Product (AGOP) to enable task-specific feature\nlearning with general machine learning models. When used in conjunction with\nkernel machines, iterating RFM results in a fast transition from random, near\nzero, test accuracy to perfect test accuracy. This transition cannot be\npredicted from the training loss, which is identically zero, nor from the test\nloss, which remains constant in initial iterations. Instead, as we show, the\ntransition is completely determined by feature learning: RFM gradually learns\nblock-circulant features to solve modular arithmetic. Paralleling the results\nfor RFM, we show that neural networks that solve modular arithmetic also learn\nblock-circulant features. Furthermore, we present theoretical evidence that RFM\nuses such block-circulant features to implement the Fourier Multiplication\nAlgorithm, which prior work posited as the generalizing solution neural\nnetworks learn on these tasks. Our results demonstrate that emergence can\nresult purely from learning task-relevant features and is not specific to\nneural architectures nor gradient descent-based optimization methods.\nFurthermore, our work provides more evidence for AGOP as a key mechanism for\nfeature learning in neural networks.",
        "updated": "2024-07-29 17:28:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20199v1"
    },
    {
        "title": "The generator gradient estimator is an adjoint state method for stochastic differential equations",
        "authors": "Quentin BadolleAnkit GuptaMustafa Khammash",
        "links": "http://arxiv.org/abs/2407.20196v1",
        "entry_id": "http://arxiv.org/abs/2407.20196v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20196v1",
        "summary": "Motivated by the increasing popularity of overparameterized Stochastic\nDifferential Equations (SDEs) like Neural SDEs, Wang, Blanchet and Glynn\nrecently introduced the generator gradient estimator, a novel unbiased\nstochastic gradient estimator for SDEs whose computation time remains stable in\nthe number of parameters. In this note, we demonstrate that this estimator is\nin fact an adjoint state method, an approach which is known to scale with the\nnumber of states and not the number of parameters in the case of Ordinary\nDifferential Equations (ODEs). In addition, we show that the generator gradient\nestimator is a close analogue to the exact Integral Path Algorithm (eIPA)\nestimator which was introduced by Gupta, Rathinam and Khammash for a class of\nContinuous-Time Markov Chains (CTMCs) known as stochastic chemical reactions\nnetworks (CRNs).",
        "updated": "2024-07-29 17:21:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20196v1"
    },
    {
        "title": "AutoScale: Automatic Prediction of Compute-optimal Data Composition for Training LLMs",
        "authors": "Feiyang KangYifan SunBingbing WenSi ChenDawn SongRafid MahmoodRuoxi Jia",
        "links": "http://arxiv.org/abs/2407.20177v1",
        "entry_id": "http://arxiv.org/abs/2407.20177v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20177v1",
        "summary": "To ensure performance on a diverse set of downstream tasks, LLMs are\npretrained via data mixtures over different domains. In this work, we\ndemonstrate that the optimal data composition for a fixed compute budget varies\ndepending on the scale of the training data, suggesting that the common\npractice of empirically determining an optimal composition using small-scale\nexperiments will not yield the optimal data mixtures when scaling up to the\nfinal model. To address this challenge, we propose *AutoScale*, an automated\ntool that finds a compute-optimal data composition for training at any desired\ntarget scale. AutoScale first determines the optimal composition at a small\nscale using a novel bilevel optimization framework, Direct Data Optimization\n(*DDO*), and then fits a predictor to estimate the optimal composition at\nlarger scales. The predictor's design is inspired by our theoretical analysis\nof scaling laws related to data composition, which could be of independent\ninterest. In empirical studies with pre-training 774M Decoder-only LMs (GPT-2\nLarge) on RedPajama dataset, AutoScale decreases validation perplexity at least\n25% faster than any baseline with up to 38% speed up compared to without\nreweighting, achieving the best overall performance across downstream tasks. On\npre-training Encoder-only LMs (BERT) with masked language modeling, DDO is\nshown to decrease loss on all domains while visibly improving average task\nperformance on GLUE benchmark by 8.7% and on large-scale QA dataset (SQuAD) by\n5.9% compared with without reweighting. AutoScale speeds up training by up to\n28%. Our codes are open-sourced.",
        "updated": "2024-07-29 17:06:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20177v1"
    },
    {
        "title": "Finite-Sample Guarantees for Best-Response Learning Dynamics in Zero-Sum Matrix Games",
        "authors": "Fathima Zarin FaizalAsuman OzdaglarMartin J. Wainwright",
        "links": "http://arxiv.org/abs/2407.20128v1",
        "entry_id": "http://arxiv.org/abs/2407.20128v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20128v1",
        "summary": "We study best-response type learning dynamics for two player zero-sum matrix\ngames. We consider two settings that are distinguished by the type of\ninformation that each player has about the game and their opponent's strategy.\nThe first setting is the full information case, in which each player knows\ntheir own and the opponent's payoff matrices and observes the opponent's mixed\nstrategy. The second setting is the minimal information case, where players do\nnot observe the opponent's strategy and are not aware of either of the payoff\nmatrices (instead they only observe their realized payoffs). For this setting,\nalso known as the radically uncoupled case in the learning in games literature,\nwe study a two-timescale learning dynamics that combine smoothed best-response\ntype updates for strategy estimates with a TD-learning update to estimate a\nlocal payoff function. For these dynamics, without additional exploration, we\nprovide polynomial-time finite-sample guarantees for convergence to an\n$\\epsilon$-Nash equilibrium.",
        "updated": "2024-07-29 15:56:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20128v1"
    },
    {
        "title": "On the Effects of Irrelevant Variables in Treatment Effect Estimation with Deep Disentanglement",
        "authors": "Ahmad Saeed KhanErik SchaffernichtJohannes Andreas Stork",
        "links": "http://arxiv.org/abs/2407.20003v1",
        "entry_id": "http://arxiv.org/abs/2407.20003v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20003v1",
        "summary": "Estimating treatment effects from observational data is paramount in\nhealthcare, education, and economics, but current deep disentanglement-based\nmethods to address selection bias are insufficiently handling irrelevant\nvariables. We demonstrate in experiments that this leads to prediction errors.\nWe disentangle pre-treatment variables with a deep embedding method and\nexplicitly identify and represent irrelevant variables, additionally to\ninstrumental, confounding and adjustment latent factors. To this end, we\nintroduce a reconstruction objective and create an embedding space for\nirrelevant variables using an attached autoencoder. Instead of relying on\nserendipitous suppression of irrelevant variables as in previous deep\ndisentanglement approaches, we explicitly force irrelevant variables into this\nembedding space and employ orthogonalization to prevent irrelevant information\nfrom leaking into the latent space representations of the other factors. Our\nexperiments with synthetic and real-world benchmark datasets show that we can\nbetter identify irrelevant variables and more precisely predict treatment\neffects than previous methods, while prediction quality degrades less when\nadditional irrelevant variables are introduced.",
        "updated": "2024-07-29 13:34:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20003v1"
    }
]