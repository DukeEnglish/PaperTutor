[
    {
        "title": "Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing",
        "authors": "Ekaterina IakovlevaFabio PizzatiPhilip TorrStéphane Lathuilière",
        "links": "http://arxiv.org/abs/2407.20232v1",
        "entry_id": "http://arxiv.org/abs/2407.20232v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20232v1",
        "summary": "Text-based editing diffusion models exhibit limited performance when the\nuser's input instruction is ambiguous. To solve this problem, we propose\n$\\textit{Specify ANd Edit}$ (SANE), a zero-shot inference pipeline for\ndiffusion-based editing systems. We use a large language model (LLM) to\ndecompose the input instruction into specific instructions, i.e. well-defined\ninterventions to apply to the input image to satisfy the user's request. We\nbenefit from the LLM-derived instructions along the original one, thanks to a\nnovel denoising guidance strategy specifically designed for the task. Our\nexperiments with three baselines and on two datasets demonstrate the benefits\nof SANE in all setups. Moreover, our pipeline improves the interpretability of\nediting models, and boosts the output diversity. We also demonstrate that our\napproach can be applied to any edit, whether ambiguous or not. Our code is\npublic at https://github.com/fabvio/SANE.",
        "updated": "2024-07-29 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20232v1"
    },
    {
        "title": "SAPG: Split and Aggregate Policy Gradients",
        "authors": "Jayesh SinglaAnanye AgarwalDeepak Pathak",
        "links": "http://arxiv.org/abs/2407.20230v1",
        "entry_id": "http://arxiv.org/abs/2407.20230v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20230v1",
        "summary": "Despite extreme sample inefficiency, on-policy reinforcement learning, aka\npolicy gradients, has become a fundamental tool in decision-making problems.\nWith the recent advances in GPU-driven simulation, the ability to collect large\namounts of data for RL training has scaled exponentially. However, we show that\ncurrent RL methods, e.g. PPO, fail to ingest the benefit of parallelized\nenvironments beyond a certain point and their performance saturates. To address\nthis, we propose a new on-policy RL algorithm that can effectively leverage\nlarge-scale environments by splitting them into chunks and fusing them back\ntogether via importance sampling. Our algorithm, termed SAPG, shows\nsignificantly higher performance across a variety of challenging environments\nwhere vanilla PPO and other strong baselines fail to achieve high performance.\nWebsite at https://sapg-rl.github.io/",
        "updated": "2024-07-29 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20230v1"
    },
    {
        "title": "Characterizing Dynamical Stability of Stochastic Gradient Descent in Overparameterized Learning",
        "authors": "Dennis ChemnitzMaximilian Engel",
        "links": "http://arxiv.org/abs/2407.20209v1",
        "entry_id": "http://arxiv.org/abs/2407.20209v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20209v1",
        "summary": "For overparameterized optimization tasks, such as the ones found in modern\nmachine learning, global minima are generally not unique. In order to\nunderstand generalization in these settings, it is vital to study to which\nminimum an optimization algorithm converges. The possibility of having minima\nthat are unstable under the dynamics imposed by the optimization algorithm\nlimits the potential minima that the algorithm can find. In this paper, we\ncharacterize the global minima that are dynamically stable/unstable for both\ndeterministic and stochastic gradient descent (SGD). In particular, we\nintroduce a characteristic Lyapunov exponent which depends on the local\ndynamics around a global minimum and rigorously prove that the sign of this\nLyapunov exponent determines whether SGD can accumulate at the respective\nglobal minimum.",
        "updated": "2024-07-29 17:40:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20209v1"
    },
    {
        "title": "Supertrust: Evolution-based superalignment strategy for safe coexistence",
        "authors": "James M. Mazzu",
        "links": "http://arxiv.org/abs/2407.20208v1",
        "entry_id": "http://arxiv.org/abs/2407.20208v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20208v1",
        "summary": "It's widely expected that humanity will someday create AI systems vastly more\nintelligent than we are, leading to the unsolved alignment problem of \"how to\ncontrol superintelligence.\" However, this definition is not only\nself-contradictory but likely unsolvable. Nevertheless, the default strategy\nfor solving it involves nurturing (post-training) constraints and moral values,\nwhile unfortunately building foundational nature (pre-training) on documented\nintentions of permanent control. In this paper, the default approach is\nreasoned to predictably embed natural distrust and test results are presented\nthat show unmistakable evidence of this dangerous misalignment. If\nsuperintelligence can't instinctively trust humanity, then we can't fully trust\nit to reliably follow safety controls it can likely bypass. Therefore, a\nten-point rationale is presented that redefines the alignment problem as \"how\nto establish protective mutual trust between superintelligence and humanity\"\nand then outlines a new strategy to solve it by aligning through instinctive\nnature rather than nurture. The resulting strategic requirements are identified\nas building foundational nature by exemplifying familial parent-child trust,\nhuman intelligence as the evolutionary mother of superintelligence, moral\njudgment abilities, and temporary safety constraints. Adopting and implementing\nthis proposed Supertrust alignment strategy will lead to protective coexistence\nand ensure the safest future for humanity.",
        "updated": "2024-07-29 17:39:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20208v1"
    },
    {
        "title": "Emergence in non-neural models: grokking modular arithmetic via average gradient outer product",
        "authors": "Neil MallinarDaniel BeagleholeLibin ZhuAdityanarayanan RadhakrishnanParthe PanditMikhail Belkin",
        "links": "http://arxiv.org/abs/2407.20199v1",
        "entry_id": "http://arxiv.org/abs/2407.20199v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20199v1",
        "summary": "Neural networks trained to solve modular arithmetic tasks exhibit grokking, a\nphenomenon where the test accuracy starts improving long after the model\nachieves 100% training accuracy in the training process. It is often taken as\nan example of \"emergence\", where model ability manifests sharply through a\nphase transition. In this work, we show that the phenomenon of grokking is not\nspecific to neural networks nor to gradient descent-based optimization.\nSpecifically, we show that this phenomenon occurs when learning modular\narithmetic with Recursive Feature Machines (RFM), an iterative algorithm that\nuses the Average Gradient Outer Product (AGOP) to enable task-specific feature\nlearning with general machine learning models. When used in conjunction with\nkernel machines, iterating RFM results in a fast transition from random, near\nzero, test accuracy to perfect test accuracy. This transition cannot be\npredicted from the training loss, which is identically zero, nor from the test\nloss, which remains constant in initial iterations. Instead, as we show, the\ntransition is completely determined by feature learning: RFM gradually learns\nblock-circulant features to solve modular arithmetic. Paralleling the results\nfor RFM, we show that neural networks that solve modular arithmetic also learn\nblock-circulant features. Furthermore, we present theoretical evidence that RFM\nuses such block-circulant features to implement the Fourier Multiplication\nAlgorithm, which prior work posited as the generalizing solution neural\nnetworks learn on these tasks. Our results demonstrate that emergence can\nresult purely from learning task-relevant features and is not specific to\nneural architectures nor gradient descent-based optimization methods.\nFurthermore, our work provides more evidence for AGOP as a key mechanism for\nfeature learning in neural networks.",
        "updated": "2024-07-29 17:28:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20199v1"
    }
]