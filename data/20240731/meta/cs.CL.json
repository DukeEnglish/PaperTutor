[
    {
        "title": "Can Editing LLMs Inject Harm?",
        "authors": "Canyu ChenBaixiang HuangZekun LiZhaorun ChenShiyang LaiXiongxiao XuJia-Chen GuJindong GuHuaxiu YaoChaowei XiaoXifeng YanWilliam Yang WangPhilip TorrDawn SongKai Shu",
        "links": "http://arxiv.org/abs/2407.20224v1",
        "entry_id": "http://arxiv.org/abs/2407.20224v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20224v1",
        "summary": "Knowledge editing techniques have been increasingly adopted to efficiently\ncorrect the false or outdated knowledge in Large Language Models (LLMs), due to\nthe high cost of retraining from scratch. Meanwhile, one critical but\nunder-explored question is: can knowledge editing be used to inject harm into\nLLMs? In this paper, we propose to reformulate knowledge editing as a new type\nof safety threat for LLMs, namely Editing Attack, and conduct a systematic\ninvestigation with a newly constructed dataset EditAttack. Specifically, we\nfocus on two typical safety risks of Editing Attack including Misinformation\nInjection and Bias Injection. For the risk of misinformation injection, we\nfirst categorize it into commonsense misinformation injection and long-tail\nmisinformation injection. Then, we find that editing attacks can inject both\ntypes of misinformation into LLMs, and the effectiveness is particularly high\nfor commonsense misinformation injection. For the risk of bias injection, we\ndiscover that not only can biased sentences be injected into LLMs with high\neffectiveness, but also one single biased sentence injection can cause a high\nbias increase in general outputs of LLMs, which are even highly irrelevant to\nthe injected sentence, indicating a catastrophic impact on the overall fairness\nof LLMs. Then, we further illustrate the high stealthiness of editing attacks,\nmeasured by their impact on the general knowledge and reasoning capacities of\nLLMs, and show the hardness of defending editing attacks with empirical\nevidence. Our discoveries demonstrate the emerging misuse risks of knowledge\nediting techniques on compromising the safety alignment of LLMs.",
        "updated": "2024-07-29 17:58:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20224v1"
    },
    {
        "title": "QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval",
        "authors": "Hongming TanShaoxiong ZhanHai LinHai-Tao ZhengWai KinChan",
        "links": "http://arxiv.org/abs/2407.20207v1",
        "entry_id": "http://arxiv.org/abs/2407.20207v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20207v1",
        "summary": "In dense retrieval, embedding long texts into dense vectors can result in\ninformation loss, leading to inaccurate query-text matching. Additionally,\nlow-quality texts with excessive noise or sparse key information are unlikely\nto align well with relevant queries. Recent studies mainly focus on improving\nthe sentence embedding model or retrieval process. In this work, we introduce a\nnovel text augmentation framework for dense retrieval. This framework\ntransforms raw documents into information-dense text formats, which supplement\nthe original texts to effectively address the aforementioned issues without\nmodifying embedding or retrieval methodologies. Two text representations are\ngenerated via large language models (LLMs) zero-shot prompting: question-answer\npairs and element-driven events. We term this approach QAEA-DR: unifying\nquestion-answer generation and event extraction in a text augmentation\nframework for dense retrieval. To further enhance the quality of generated\ntexts, a scoring-based evaluation and regeneration mechanism is introduced in\nLLM prompting. Our QAEA-DR model has a positive impact on dense retrieval,\nsupported by both theoretical analysis and empirical experiments.",
        "updated": "2024-07-29 17:39:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20207v1"
    },
    {
        "title": "Aligning Query Representation with Rewritten Query and Relevance Judgments in Conversational Search",
        "authors": "Fengran MoChen QuKelong MaoYihong WuZhan SuKaiyu HuangJian-Yun Nie",
        "links": "http://arxiv.org/abs/2407.20189v1",
        "entry_id": "http://arxiv.org/abs/2407.20189v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20189v1",
        "summary": "Conversational search supports multi-turn user-system interactions to solve\ncomplex information needs. Different from the traditional single-turn ad-hoc\nsearch, conversational search encounters a more challenging problem of\ncontext-dependent query understanding with the lengthy and long-tail\nconversational history context. While conversational query rewriting methods\nleverage explicit rewritten queries to train a rewriting model to transform the\ncontext-dependent query into a stand-stone search query, this is usually done\nwithout considering the quality of search results. Conversational dense\nretrieval methods use fine-tuning to improve a pre-trained ad-hoc query\nencoder, but they are limited by the conversational search data available for\ntraining. In this paper, we leverage both rewritten queries and relevance\njudgments in the conversational search data to train a better query\nrepresentation model. The key idea is to align the query representation with\nthose of rewritten queries and relevant documents. The proposed model -- Query\nRepresentation Alignment Conversational Dense Retriever, QRACDR, is tested on\neight datasets, including various settings in conversational search and ad-hoc\nsearch. The results demonstrate the strong performance of QRACDR compared with\nstate-of-the-art methods, and confirm the effectiveness of representation\nalignment.",
        "updated": "2024-07-29 17:14:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20189v1"
    },
    {
        "title": "MindSearch: Mimicking Human Minds Elicits Deep AI Searcher",
        "authors": "Zehui ChenKuikun LiuQiuchen WangJiangning LiuWenwei ZhangKai ChenFeng Zhao",
        "links": "http://arxiv.org/abs/2407.20183v1",
        "entry_id": "http://arxiv.org/abs/2407.20183v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20183v1",
        "summary": "Information seeking and integration is a complex cognitive task that consumes\nenormous time and effort. Inspired by the remarkable progress of Large Language\nModels, recent works attempt to solve this task by combining LLMs and search\nengines. However, these methods still obtain unsatisfying performance due to\nthree challenges: (1) complex requests often cannot be accurately and\ncompletely retrieved by the search engine once (2) corresponding information to\nbe integrated is spread over multiple web pages along with massive noise, and\n(3) a large number of web pages with long contents may quickly exceed the\nmaximum context length of LLMs. Inspired by the cognitive process when humans\nsolve these problems, we introduce MindSearch to mimic the human minds in web\ninformation seeking and integration, which can be instantiated by a simple yet\neffective LLM-based multi-agent framework. The WebPlanner models the human mind\nof multi-step information seeking as a dynamic graph construction process: it\ndecomposes the user query into atomic sub-questions as nodes in the graph and\nprogressively extends the graph based on the search result from WebSearcher.\nTasked with each sub-question, WebSearcher performs hierarchical information\nretrieval with search engines and collects valuable information for WebPlanner.\nThe multi-agent design of MindSearch enables the whole framework to seek and\nintegrate information parallelly from larger-scale (e.g., more than 300) web\npages in 3 minutes, which is worth 3 hours of human effort. MindSearch\ndemonstrates significant improvement in the response quality in terms of depth\nand breadth, on both close-set and open-set QA problems. Besides, responses\nfrom MindSearch based on InternLM2.5-7B are preferable by humans to ChatGPT-Web\nand Perplexity.ai applications, which implies that MindSearch can already\ndeliver a competitive solution to the proprietary AI search engine.",
        "updated": "2024-07-29 17:12:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20183v1"
    },
    {
        "title": "AutoScale: Automatic Prediction of Compute-optimal Data Composition for Training LLMs",
        "authors": "Feiyang KangYifan SunBingbing WenSi ChenDawn SongRafid MahmoodRuoxi Jia",
        "links": "http://arxiv.org/abs/2407.20177v1",
        "entry_id": "http://arxiv.org/abs/2407.20177v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20177v1",
        "summary": "To ensure performance on a diverse set of downstream tasks, LLMs are\npretrained via data mixtures over different domains. In this work, we\ndemonstrate that the optimal data composition for a fixed compute budget varies\ndepending on the scale of the training data, suggesting that the common\npractice of empirically determining an optimal composition using small-scale\nexperiments will not yield the optimal data mixtures when scaling up to the\nfinal model. To address this challenge, we propose *AutoScale*, an automated\ntool that finds a compute-optimal data composition for training at any desired\ntarget scale. AutoScale first determines the optimal composition at a small\nscale using a novel bilevel optimization framework, Direct Data Optimization\n(*DDO*), and then fits a predictor to estimate the optimal composition at\nlarger scales. The predictor's design is inspired by our theoretical analysis\nof scaling laws related to data composition, which could be of independent\ninterest. In empirical studies with pre-training 774M Decoder-only LMs (GPT-2\nLarge) on RedPajama dataset, AutoScale decreases validation perplexity at least\n25% faster than any baseline with up to 38% speed up compared to without\nreweighting, achieving the best overall performance across downstream tasks. On\npre-training Encoder-only LMs (BERT) with masked language modeling, DDO is\nshown to decrease loss on all domains while visibly improving average task\nperformance on GLUE benchmark by 8.7% and on large-scale QA dataset (SQuAD) by\n5.9% compared with without reweighting. AutoScale speeds up training by up to\n28%. Our codes are open-sourced.",
        "updated": "2024-07-29 17:06:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20177v1"
    }
]