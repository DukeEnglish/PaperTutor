[
    {
        "title": "Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing",
        "authors": "Ekaterina IakovlevaFabio PizzatiPhilip TorrStéphane Lathuilière",
        "links": "http://arxiv.org/abs/2407.20232v1",
        "entry_id": "http://arxiv.org/abs/2407.20232v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20232v1",
        "summary": "Text-based editing diffusion models exhibit limited performance when the\nuser's input instruction is ambiguous. To solve this problem, we propose\n$\\textit{Specify ANd Edit}$ (SANE), a zero-shot inference pipeline for\ndiffusion-based editing systems. We use a large language model (LLM) to\ndecompose the input instruction into specific instructions, i.e. well-defined\ninterventions to apply to the input image to satisfy the user's request. We\nbenefit from the LLM-derived instructions along the original one, thanks to a\nnovel denoising guidance strategy specifically designed for the task. Our\nexperiments with three baselines and on two datasets demonstrate the benefits\nof SANE in all setups. Moreover, our pipeline improves the interpretability of\nediting models, and boosts the output diversity. We also demonstrate that our\napproach can be applied to any edit, whether ambiguous or not. Our code is\npublic at https://github.com/fabvio/SANE.",
        "updated": "2024-07-29 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20232v1"
    },
    {
        "title": "SAPG: Split and Aggregate Policy Gradients",
        "authors": "Jayesh SinglaAnanye AgarwalDeepak Pathak",
        "links": "http://arxiv.org/abs/2407.20230v1",
        "entry_id": "http://arxiv.org/abs/2407.20230v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20230v1",
        "summary": "Despite extreme sample inefficiency, on-policy reinforcement learning, aka\npolicy gradients, has become a fundamental tool in decision-making problems.\nWith the recent advances in GPU-driven simulation, the ability to collect large\namounts of data for RL training has scaled exponentially. However, we show that\ncurrent RL methods, e.g. PPO, fail to ingest the benefit of parallelized\nenvironments beyond a certain point and their performance saturates. To address\nthis, we propose a new on-policy RL algorithm that can effectively leverage\nlarge-scale environments by splitting them into chunks and fusing them back\ntogether via importance sampling. Our algorithm, termed SAPG, shows\nsignificantly higher performance across a variety of challenging environments\nwhere vanilla PPO and other strong baselines fail to achieve high performance.\nWebsite at https://sapg-rl.github.io/",
        "updated": "2024-07-29 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20230v1"
    },
    {
        "title": "Improving 2D Feature Representations by 3D-Aware Fine-Tuning",
        "authors": "Yuanwen YueAnurag DasFrancis EngelmannSiyu TangJan Eric Lenssen",
        "links": "http://arxiv.org/abs/2407.20229v1",
        "entry_id": "http://arxiv.org/abs/2407.20229v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20229v1",
        "summary": "Current visual foundation models are trained purely on unstructured 2D data,\nlimiting their understanding of 3D structure of objects and scenes. In this\nwork, we show that fine-tuning on 3D-aware data improves the quality of\nemerging semantic features. We design a method to lift semantic 2D features\ninto an efficient 3D Gaussian representation, which allows us to re-render them\nfor arbitrary views. Using the rendered 3D-aware features, we design a\nfine-tuning strategy to transfer such 3D awareness into a 2D foundation model.\nWe demonstrate that models fine-tuned in that way produce features that readily\nimprove downstream task performance in semantic segmentation and depth\nestimation through simple linear probing. Notably, though fined-tuned on a\nsingle indoor dataset, the improvement is transferable to a variety of indoor\ndatasets and out-of-domain datasets. We hope our study encourages the community\nto consider injecting 3D awareness when training 2D foundation models. Project\npage: https://ywyue.github.io/FiT3D.",
        "updated": "2024-07-29 17:59:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20229v1"
    },
    {
        "title": "FlexAttention for Efficient High-Resolution Vision-Language Models",
        "authors": "Junyan LiDelin ChenTianle CaiPeihao ChenYining HongZhenfang ChenYikang ShenChuang Gan",
        "links": "http://arxiv.org/abs/2407.20228v1",
        "entry_id": "http://arxiv.org/abs/2407.20228v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20228v1",
        "summary": "Current high-resolution vision-language models encode images as\nhigh-resolution image tokens and exhaustively take all these tokens to compute\nattention, which significantly increases the computational cost. To address\nthis problem, we propose FlexAttention, a flexible attention mechanism for\nefficient high-resolution vision-language models. Specifically, a\nhigh-resolution image is encoded both as high-resolution tokens and\nlow-resolution tokens, where only the low-resolution tokens and a few selected\nhigh-resolution tokens are utilized to calculate the attention map, which\ngreatly shrinks the computational cost. The high-resolution tokens are selected\nvia a high-resolution selection module which could retrieve tokens of relevant\nregions based on an input attention map. The selected high-resolution tokens\nare then concatenated to the low-resolution tokens and text tokens, and input\nto a hierarchical self-attention layer which produces an attention map that\ncould be used for the next-step high-resolution token selection. The\nhierarchical self-attention process and high-resolution token selection process\nare performed iteratively for each attention layer. Experiments on multimodal\nbenchmarks prove that our FlexAttention outperforms existing high-resolution\nVLMs (e.g., relatively ~9% in V* Bench, ~7% in TextVQA), while also\nsignificantly reducing the computational cost by nearly 40%.",
        "updated": "2024-07-29 17:59:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20228v1"
    },
    {
        "title": "Correspondence-Free SE(3) Point Cloud Registration in RKHS via Unsupervised Equivariant Learning",
        "authors": "Ray ZhangZheming ZhouMin SunOmid GhasemalizadehCheng-Hao KuoRyan EusticeMaani GhaffariArnie Sen",
        "links": "http://arxiv.org/abs/2407.20223v1",
        "entry_id": "http://arxiv.org/abs/2407.20223v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20223v1",
        "summary": "This paper introduces a robust unsupervised SE(3) point cloud registration\nmethod that operates without requiring point correspondences. The method frames\npoint clouds as functions in a reproducing kernel Hilbert space (RKHS),\nleveraging SE(3)-equivariant features for direct feature space registration. A\nnovel RKHS distance metric is proposed, offering reliable performance amidst\nnoise, outliers, and asymmetrical data. An unsupervised training approach is\nintroduced to effectively handle limited ground truth data, facilitating\nadaptation to real datasets. The proposed method outperforms classical and\nsupervised methods in terms of registration accuracy on both synthetic\n(ModelNet40) and real-world (ETH3D) noisy, outlier-rich datasets. To our best\nknowledge, this marks the first instance of successful real RGB-D odometry data\nregistration using an equivariant method. The code is available at\n{https://sites.google.com/view/eccv24-equivalign}",
        "updated": "2024-07-29 17:57:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20223v1"
    }
]