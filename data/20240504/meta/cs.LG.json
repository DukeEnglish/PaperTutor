[
    {
        "title": "Multi-Space Alignments Towards Universal LiDAR Segmentation",
        "authors": "Youquan LiuLingdong KongXiaoyang WuRunnan ChenXin LiLiang PanZiwei LiuYuexin Ma",
        "links": "http://arxiv.org/abs/2405.01538v1",
        "entry_id": "http://arxiv.org/abs/2405.01538v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01538v1",
        "summary": "A unified and versatile LiDAR segmentation model with strong robustness and\ngeneralizability is desirable for safe autonomous driving perception. This work\npresents M3Net, a one-of-a-kind framework for fulfilling multi-task,\nmulti-dataset, multi-modality LiDAR segmentation in a universal manner using\njust a single set of parameters. To better exploit data volume and diversity,\nwe first combine large-scale driving datasets acquired by different types of\nsensors from diverse scenes and then conduct alignments in three spaces, namely\ndata, feature, and label spaces, during the training. As a result, M3Net is\ncapable of taming heterogeneous data for training state-of-the-art LiDAR\nsegmentation models. Extensive experiments on twelve LiDAR segmentation\ndatasets verify our effectiveness. Notably, using a shared set of parameters,\nM3Net achieves 75.1%, 83.1%, and 72.4% mIoU scores, respectively, on the\nofficial benchmarks of SemanticKITTI, nuScenes, and Waymo Open.",
        "updated": "2024-05-02 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01538v1"
    },
    {
        "title": "Customizing Text-to-Image Models with a Single Image Pair",
        "authors": "Maxwell JonesSheng-Yu WangNupur KumariDavid BauJun-Yan Zhu",
        "links": "http://arxiv.org/abs/2405.01536v1",
        "entry_id": "http://arxiv.org/abs/2405.01536v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01536v1",
        "summary": "Art reinterpretation is the practice of creating a variation of a reference\nwork, making a paired artwork that exhibits a distinct artistic style. We ask\nif such an image pair can be used to customize a generative model to capture\nthe demonstrated stylistic difference. We propose Pair Customization, a new\ncustomization method that learns stylistic difference from a single image pair\nand then applies the acquired style to the generation process. Unlike existing\nmethods that learn to mimic a single concept from a collection of images, our\nmethod captures the stylistic difference between paired images. This allows us\nto apply a stylistic change without overfitting to the specific image content\nin the examples. To address this new task, we employ a joint optimization\nmethod that explicitly separates the style and content into distinct LoRA\nweight spaces. We optimize these style and content weights to reproduce the\nstyle and content images while encouraging their orthogonality. During\ninference, we modify the diffusion process via a new style guidance based on\nour learned weights. Both qualitative and quantitative experiments show that\nour method can effectively learn style while avoiding overfitting to image\ncontent, highlighting the potential of modeling such stylistic differences from\na single image pair.",
        "updated": "2024-05-02 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01536v1"
    },
    {
        "title": "Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks",
        "authors": "Murtaza DalalTarun ChiruvoluDevendra ChaplotRuslan Salakhutdinov",
        "links": "http://arxiv.org/abs/2405.01534v1",
        "entry_id": "http://arxiv.org/abs/2405.01534v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01534v1",
        "summary": "Large Language Models (LLMs) have been shown to be capable of performing\nhigh-level planning for long-horizon robotics tasks, yet existing methods\nrequire access to a pre-defined skill library (e.g. picking, placing, pulling,\npushing, navigating). However, LLM planning does not address how to design or\nlearn those behaviors, which remains challenging particularly in long-horizon\nsettings. Furthermore, for many tasks of interest, the robot needs to be able\nto adjust its behavior in a fine-grained manner, requiring the agent to be\ncapable of modifying low-level control actions. Can we instead use the\ninternet-scale knowledge from LLMs for high-level policies, guiding\nreinforcement learning (RL) policies to efficiently solve robotic control tasks\nonline without requiring a pre-determined set of skills? In this paper, we\npropose Plan-Seq-Learn (PSL): a modular approach that uses motion planning to\nbridge the gap between abstract language and learned low-level control for\nsolving long-horizon robotics tasks from scratch. We demonstrate that PSL\nachieves state-of-the-art results on over 25 challenging robotics tasks with up\nto 10 stages. PSL solves long-horizon tasks from raw visual input spanning four\nbenchmarks at success rates of over 85%, out-performing language-based,\nclassical, and end-to-end approaches. Video results and code at\nhttps://mihdalal.github.io/planseqlearn/",
        "updated": "2024-05-02 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01534v1"
    },
    {
        "title": "Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models",
        "authors": "Nishad SinghiJae Myung KimKarsten RothZeynep Akata",
        "links": "http://arxiv.org/abs/2405.01531v1",
        "entry_id": "http://arxiv.org/abs/2405.01531v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01531v1",
        "summary": "Concept Bottleneck Models (CBMs) ground image classification on\nhuman-understandable concepts to allow for interpretable model decisions.\nCrucially, the CBM design inherently allows for human interventions, in which\nexpert users are given the ability to modify potentially misaligned concept\nchoices to influence the decision behavior of the model in an interpretable\nfashion. However, existing approaches often require numerous human\ninterventions per image to achieve strong performances, posing practical\nchallenges in scenarios where obtaining human feedback is expensive. In this\npaper, we find that this is noticeably driven by an independent treatment of\nconcepts during intervention, wherein a change of one concept does not\ninfluence the use of other ones in the model's final decision. To address this\nissue, we introduce a trainable concept intervention realignment module, which\nleverages concept relations to realign concept assignments post-intervention.\nAcross standard, real-world benchmarks, we find that concept realignment can\nsignificantly improve intervention efficacy; significantly reducing the number\nof interventions needed to reach a target classification performance or concept\nprediction accuracy. In addition, it easily integrates into existing\nconcept-based architectures without requiring changes to the models themselves.\nThis reduced cost of human-model collaboration is crucial to enhancing the\nfeasibility of CBMs in resource-constrained environments.",
        "updated": "2024-05-02 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01531v1"
    },
    {
        "title": "A separability-based approach to quantifying generalization: which layer is best?",
        "authors": "Luciano DyballaEvan GerritzSteven W. Zucker",
        "links": "http://arxiv.org/abs/2405.01524v1",
        "entry_id": "http://arxiv.org/abs/2405.01524v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01524v1",
        "summary": "Generalization to unseen data remains poorly understood for deep learning\nclassification and foundation models. How can one assess the ability of\nnetworks to adapt to new or extended versions of their input space in the\nspirit of few-shot learning, out-of-distribution generalization, and domain\nadaptation? Which layers of a network are likely to generalize best? We provide\na new method for evaluating the capacity of networks to represent a sampled\ndomain, regardless of whether the network has been trained on all classes in\nthe domain. Our approach is the following: after fine-tuning state-of-the-art\npre-trained models for visual classification on a particular domain, we assess\ntheir performance on data from related but distinct variations in that domain.\nGeneralization power is quantified as a function of the latent embeddings of\nunseen data from intermediate layers for both unsupervised and supervised\nsettings. Working throughout all stages of the network, we find that (i) high\nclassification accuracy does not imply high generalizability; and (ii) deeper\nlayers in a model do not always generalize the best, which has implications for\npruning. Since the trends observed across datasets are largely consistent, we\nconclude that our approach reveals (a function of) the intrinsic capacity of\nthe different layers of a model to generalize.",
        "updated": "2024-05-02 17:54:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01524v1"
    }
]