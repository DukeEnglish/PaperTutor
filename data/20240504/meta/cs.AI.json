[
    {
        "title": "Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks",
        "authors": "Murtaza DalalTarun ChiruvoluDevendra ChaplotRuslan Salakhutdinov",
        "links": "http://arxiv.org/abs/2405.01534v1",
        "entry_id": "http://arxiv.org/abs/2405.01534v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01534v1",
        "summary": "Large Language Models (LLMs) have been shown to be capable of performing\nhigh-level planning for long-horizon robotics tasks, yet existing methods\nrequire access to a pre-defined skill library (e.g. picking, placing, pulling,\npushing, navigating). However, LLM planning does not address how to design or\nlearn those behaviors, which remains challenging particularly in long-horizon\nsettings. Furthermore, for many tasks of interest, the robot needs to be able\nto adjust its behavior in a fine-grained manner, requiring the agent to be\ncapable of modifying low-level control actions. Can we instead use the\ninternet-scale knowledge from LLMs for high-level policies, guiding\nreinforcement learning (RL) policies to efficiently solve robotic control tasks\nonline without requiring a pre-determined set of skills? In this paper, we\npropose Plan-Seq-Learn (PSL): a modular approach that uses motion planning to\nbridge the gap between abstract language and learned low-level control for\nsolving long-horizon robotics tasks from scratch. We demonstrate that PSL\nachieves state-of-the-art results on over 25 challenging robotics tasks with up\nto 10 stages. PSL solves long-horizon tasks from raw visual input spanning four\nbenchmarks at success rates of over 85%, out-performing language-based,\nclassical, and end-to-end approaches. Video results and code at\nhttps://mihdalal.github.io/planseqlearn/",
        "updated": "2024-05-02 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01534v1"
    },
    {
        "title": "Improving Intervention Efficacy via Concept Realignment in Concept Bottleneck Models",
        "authors": "Nishad SinghiJae Myung KimKarsten RothZeynep Akata",
        "links": "http://arxiv.org/abs/2405.01531v1",
        "entry_id": "http://arxiv.org/abs/2405.01531v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01531v1",
        "summary": "Concept Bottleneck Models (CBMs) ground image classification on\nhuman-understandable concepts to allow for interpretable model decisions.\nCrucially, the CBM design inherently allows for human interventions, in which\nexpert users are given the ability to modify potentially misaligned concept\nchoices to influence the decision behavior of the model in an interpretable\nfashion. However, existing approaches often require numerous human\ninterventions per image to achieve strong performances, posing practical\nchallenges in scenarios where obtaining human feedback is expensive. In this\npaper, we find that this is noticeably driven by an independent treatment of\nconcepts during intervention, wherein a change of one concept does not\ninfluence the use of other ones in the model's final decision. To address this\nissue, we introduce a trainable concept intervention realignment module, which\nleverages concept relations to realign concept assignments post-intervention.\nAcross standard, real-world benchmarks, we find that concept realignment can\nsignificantly improve intervention efficacy; significantly reducing the number\nof interventions needed to reach a target classification performance or concept\nprediction accuracy. In addition, it easily integrates into existing\nconcept-based architectures without requiring changes to the models themselves.\nThis reduced cost of human-model collaboration is crucial to enhancing the\nfeasibility of CBMs in resource-constrained environments.",
        "updated": "2024-05-02 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01531v1"
    },
    {
        "title": "FLAME: Factuality-Aware Alignment for Large Language Models",
        "authors": "Sheng-Chieh LinLuyu GaoBarlas OguzWenhan XiongJimmy LinWen-tau YihXilun Chen",
        "links": "http://arxiv.org/abs/2405.01525v1",
        "entry_id": "http://arxiv.org/abs/2405.01525v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01525v1",
        "summary": "Alignment is a standard procedure to fine-tune pre-trained large language\nmodels (LLMs) to follow natural language instructions and serve as helpful AI\nassistants. We have observed, however, that the conventional alignment process\nfails to enhance the factual accuracy of LLMs, and often leads to the\ngeneration of more false facts (i.e. hallucination). In this paper, we study\nhow to make the LLM alignment process more factual, by first identifying\nfactors that lead to hallucination in both alignment steps:\\ supervised\nfine-tuning (SFT) and reinforcement learning (RL). In particular, we find that\ntraining the LLM on new knowledge or unfamiliar texts can encourage\nhallucination. This makes SFT less factual as it trains on human labeled data\nthat may be novel to the LLM. Furthermore, reward functions used in standard RL\ncan also encourage hallucination, because it guides the LLM to provide more\nhelpful responses on a diverse set of instructions, often preferring longer and\nmore detailed responses. Based on these observations, we propose\nfactuality-aware alignment, comprised of factuality-aware SFT and\nfactuality-aware RL through direct preference optimization. Experiments show\nthat our proposed factuality-aware alignment guides LLMs to output more factual\nresponses while maintaining instruction-following capability.",
        "updated": "2024-05-02 17:54:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01525v1"
    },
    {
        "title": "A separability-based approach to quantifying generalization: which layer is best?",
        "authors": "Luciano DyballaEvan GerritzSteven W. Zucker",
        "links": "http://arxiv.org/abs/2405.01524v1",
        "entry_id": "http://arxiv.org/abs/2405.01524v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01524v1",
        "summary": "Generalization to unseen data remains poorly understood for deep learning\nclassification and foundation models. How can one assess the ability of\nnetworks to adapt to new or extended versions of their input space in the\nspirit of few-shot learning, out-of-distribution generalization, and domain\nadaptation? Which layers of a network are likely to generalize best? We provide\na new method for evaluating the capacity of networks to represent a sampled\ndomain, regardless of whether the network has been trained on all classes in\nthe domain. Our approach is the following: after fine-tuning state-of-the-art\npre-trained models for visual classification on a particular domain, we assess\ntheir performance on data from related but distinct variations in that domain.\nGeneralization power is quantified as a function of the latent embeddings of\nunseen data from intermediate layers for both unsupervised and supervised\nsettings. Working throughout all stages of the network, we find that (i) high\nclassification accuracy does not imply high generalizability; and (ii) deeper\nlayers in a model do not always generalize the best, which has implications for\npruning. Since the trends observed across datasets are largely consistent, we\nconclude that our approach reveals (a function of) the intrinsic capacity of\nthe different layers of a model to generalize.",
        "updated": "2024-05-02 17:54:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01524v1"
    },
    {
        "title": "Analyzing the Role of Semantic Representations in the Era of Large Language Models",
        "authors": "Zhijing JinYuen ChenFernando GonzalezJiarui LiuJiayi ZhangJulian MichaelBernhard SchölkopfMona Diab",
        "links": "http://arxiv.org/abs/2405.01502v1",
        "entry_id": "http://arxiv.org/abs/2405.01502v1",
        "pdf_url": "http://arxiv.org/pdf/2405.01502v1",
        "summary": "Traditionally, natural language processing (NLP) models often use a rich set\nof features created by linguistic expertise, such as semantic representations.\nHowever, in the era of large language models (LLMs), more and more tasks are\nturned into generic, end-to-end sequence generation problems. In this paper, we\ninvestigate the question: what is the role of semantic representations in the\nera of LLMs? Specifically, we investigate the effect of Abstract Meaning\nRepresentation (AMR) across five diverse NLP tasks. We propose an AMR-driven\nchain-of-thought prompting method, which we call AMRCoT, and find that it\ngenerally hurts performance more than it helps. To investigate what AMR may\nhave to offer on these tasks, we conduct a series of analysis experiments. We\nfind that it is difficult to predict which input examples AMR may help or hurt\non, but errors tend to arise with multi-word expressions, named entities, and\nin the final inference step where the LLM must connect its reasoning over the\nAMR to its prediction. We recommend focusing on these areas for future work in\nsemantic representations for LLMs. Our code:\nhttps://github.com/causalNLP/amr_llm.",
        "updated": "2024-05-02 17:32:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.01502v1"
    }
]