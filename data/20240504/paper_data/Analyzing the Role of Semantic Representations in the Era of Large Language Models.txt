Analyzing the Role of Semantic Representations
in the Era of Large Language Models
ZhijingJin∗ YuenChen∗ FernandoGonzalez∗ JiaruiLiu
MPI&ETH UIUC ETH CMU
jinzhi@ethz.ch yuenc2@illinois.edu fer.adauto@gmail.com jiarui@cmu.edu
JiayiZhang JulianMichael BernhardSchölkopf MonaDiab
UniversityofMichigan NYU MPI CMU
jiayizzz@umich.edu julianjm@nyu.edu bs@tue.mpg.de mdiab@cs.cmu.edu
Abstract Pursuit of Representation Power
In math: Input Output
Traditionally, natural language processing
Three hundred eighty-eight plus twelve Four hundred
(NLP)modelsoftenusearichsetoffeatures
Better representation
createdbylinguisticexpertise,suchasseman-
Easier computation
ticrepresentations. However,intheeraoflarge 388+12 400
languagemodels(LLMs),moreandmoretasks
In language:
are turned into generic, end-to-end sequence Input Output
generationproblems. Inthispaper,weinvesti- Are the two sentences paraphrases of each other?
gatethequestion: whatistheroleofsemantic 1 Gary of Rochester played against Paul of Philadelphia. Not paraphrases.
2 Paul of Rochester played against Gary of Philadelphia.
representations in the era of LLMs? Specif-
ically, we investigate the effect of Abstract
MeaningRepresentation(AMR)acrossfivedi- AMR AMR Difference
verseNLPtasks. WeproposeanAMR-driven 1 Play 2 Play :ARG0 :ARG1 :ARG0 :ARG1 "Gary" is associated with
chain-of-thoughtpromptingmethod,whichwe Paul Gary Gary Paul "Philadelphia" in AMR1, but
:loc :loc :loc :loc with "Rochester" in AMR2.
callAMRCOT,andfindthatitgenerallyhurts Philadelphia Rochester Philadelphia Rochester [...]
performance more than it helps. To investi-
To test the representation power:
gate what AMR may have to offer on these
tasks, we conduct a series of analysis exper- Traditional Test Setup Practical Setup in the Era of LLMs
iments. We find that it is difficult to predict Text Trainable Model Output Text Output
Pretrained
whichinputexamplesAMRmayhelporhurt Trainable Model LLMs
AMR Output AMR Output
on, but errors tend to arise with multi-word
expressions,namedentities,andinthefinalin-
In the era of LLMs, which one have better
ferencestepwheretheLLMmustconnectits representation power: text or AMR?
reasoningovertheAMRtoitsprediction. We
Figure1:Theroleofrepresentationpowerindifferentfields.
recommendfocusingontheseareasforfuture
AnalogoustoArabicnumbersformath,AMRisdesignedto
workinsemanticrepresentationsforLLMs.1
efficientlyandexplicitlyrepresentthesemanticfeaturesof
text. ExistingworkusingAMRisconcernedwithtrainable
1 Introduction models,whereasweinvestigatetheuseofAMRinthemodern
practicalsetupofpre-trainedLLMs.
Formalrepresentationsoflinguisticstructureand
meaning have long held an important role in the
construction and evaluation of NLP systems. Se-
codegeneration(YinandNeubig,2017),andoth-
manticrepresentationssuchasAbstractMeaning
ers(DohareandKarnick,2017;Jangraetal.,2022;
Representation(AMR;Banarescuetal.,2013)are
Wolfsonetal.,2020;Kapanipathietal.,2021). By
designedtodistillthesemanticinformationoftext
explicitlyrepresentingthepropositionalstructure
to a graph consisting of the entities, events, and
ofsentences,AMRremovesmuchoftheinforma-
statesmentionedinatextandtherelationsbetween
tion from text that is irrelevant to semantic tasks,
them. Existingstudieshaveshownthebenefitsof
while surfacing the most important information
representationslikeAMRinavarietyofNLPtasks,
(entities,relations,etc.),renderingthemeasierto
such as paraphrase detection (Issa et al., 2018),
operateon. Intheory,thisimpliesthatusingAMR
machinetranslation(Songetal.,2019), eventex-
as an intermediate representation should make it
traction (Garg et al., 2015; Huang et al., 2018),
easier for a model to learn to perform such tasks,
∗Equalcontribution. inthesamewaythatarepresentationlikeArabic
1Ourcode:https://github.com/causalNLP/amr_llm. numeralsaidswitharithmetic(seeFigure1).
4202
yaM
2
]LC.sc[
1v20510.5042:viXraHowever, learning to produce and operate over find that AMR is helpful for a subset of samples.
representationslikeAMRisnontrivial,especially We also find that the next step for using AMR to
since AMR data is limited. In contrast, mod- improveperformanceislikelynotimprovingAMR
ernNLPsystemsbasedonlargelanguagemodels parserperformance,butimprovingtheLLM’sabil-
(LLMs) learn to directly manipulate text very ef- ity to map AMR representations into the output
fectively (Ignat et al., 2024), not only achieving space.
high performance on a variety of tasks without
Insummary, thecontributionsofourworkareas
usingintermediateformalrepresentations(Brown
follows:
etal.,2020),butalsoachievinggainsbydirectlyus-
inginformaltextualintermediatesinmethodssuch
1. We are the first to investigate how semantic
as chain-of-thought (CoT) prompting (Wei et al.,
representationssuchasAMRcanbeleveraged
2022). Due to economic concerns (Zhao et al.,
tohelpLLMperformanceinthepracticalsit-
2022; Samsi et al., 2023; Patterson et al., 2021;
uationwherenotrainingisinvolved;
Shariretal.,2020),thereisagrowingtrendtouti-
2. We propose a formalization of representa-
lizereadilyavailablepre-trainedLLMsinvarious
tion power for intermediate representations
applicationscenarios,withoutallocatingadditional
of language, and comprehensive experimen-
resources for training or fine-tuning the models.
tal studies investigating whether, when, and
Thesetrendsraisethequestion:
whyAMRcanhelpwhenperformingseman-
tictaskswithLLMs;
Whatistheroleofsemanticrepresenta-
tionsintheeraofLLMs,whennotrain- 3. Wepresentthoroughanalysisexperimentsand
ingorfinetuningisinvolved? resultsreflectingonthecontributionoftradi-
tional linguistic structures such as semantic
Motivatedbythisquestion,weproposeatheoreti- representationsinthecurrentwaveofLLMs,
calformulationofrepresentationpower,andwhatit andpointoutpotentialareasforimprovement.
meanstohaveanidealrepresentationfortext,using
ideasfromKolmogorovcomplexity(Solomonoff, 2 FormalizingRepresentationPower
1964;Kolmogorov,1965). Ourkeyobservationis
In this section, we propose a framework to for-
thatmakinguseofevenaverystrongintermediate
mulaterepresentationpowerinboththepre-LLM
representationrequiresoptimizingthemodelwith
era,wherewedonotoutsourcethetrainingofthe
regard to that representation; however, when us-
models, and the LLM era, where a lot of practi-
ing(outofthebox)pretrainedLLMs,theoptimal
calsettingsaretooptimizetherepresentationwith
representationwillbetheonewhichtheLLMcan
regardtogivenfixedLLMs.
mosteffectivelyuseonthebasisofitspretraining,
whichmightshiftawayfromtheoptimalrepresen-
2.1 Notation
tationforalearnablemappingtotheoutputspace.
SupposewehaveadatasetD := {(x ,y )}N con-
Inshort,theaprioriidealrepresentationforatask i i i=1
sisting of N pairs of input x and corresponding
is not necessarily the ideal representation for an i
outputy . Giventhetasktolearnthex (cid:55)→ y map-
LLMtouse. i
ping, wecanconsideritasatwo-stagemodeling
Giventhis,weempiricallystudyhowgoodAMRis process: thefirststepistoconverttherawinputx
asanintermediaterepresentationforLLMs. Specif- intoagoodrepresentationr bytherepresentation
ically,weanswerthefollowingthreequestions: (1) model g : x (cid:55)→ r, and the second step is to per-
Does AMR help LLM performance? (2) When formthecomputationthattakestherepresentation
does AMR help, and when doesn’t it? (3) Why randpredictstheoutputybyacomputationmodel
doesithelpornothelpinthesecases? h : r (cid:55)→ y. Inthisway,wedecomposetheresulting
overallx (cid:55)→ y modelingprocessinto
OnadiversesetoffiveNLPtasks,ourexperiments
show that the contribution of AMR in the era of p(y|x) = p (r|x)p (y|r), (1)
g h
LLMsisnotasgreatasthatinthetraditionalsetup
wherewecanoptimizethemodelfortherepresen- where the overall probabilistic model p(y|x) is
tation. AMRcausesaslightfluctuationofperfor- turnedintofirsttherepresentationstepp (r|x)to
g
manceby-3to+1percentagepoints. However,we generatetherepresentationrgiventheinputx,andthenthecomputationstepp (y|r)toperformoper- optimal representation function g∗ ∈ G from the
h
ationsontheintermediaterepresentationrtoderive setG ofpossiblefunctionsshouldsatisfy
theoutputy.
g∗ = argminminK(h), (2)
2.2 ProblemFormulation g∈G h∈H
whereh : g(x) (cid:55)→ y , (3)
Letusdrawsomeintuitionfromthemathexample
inFigure1. Torepresentnumbers,thefirstrepre-
andtheoptimalrepresentationr∗ is
sentationchoicer istheEnglishexpression,such
1
as“Threehundredeighty-eightplustwelve,”and r∗ = g∗(x). (4)
thesecond,intuitivelystrongerrepresentationr is
2
thesamecalculationinArabicnumbers,“388+12”. Here,notethatgiveneachrepresentationfunction,
ForourresearchquestionofwhetherAMRdemon- we optimize over all possible computation mod-
stratesstrongerrepresentationpowerthanrawtext, els h from thehypothesis space H to achievethe
weformulatethequestionasfollows: minimalKolmogorovcomplexityK(h).
• Representation choices: R = {text,amr} If the representation enables the computation
(whichisaninstanceoftextanditssemantic model h to have a low Kolmogorov complexity,
representation,acommonquestionofinterest it usually results in several good properties, such
inlinguistics); asthatlearninghhassmallergeneralizationrisks,
requiresfewerdatasamples,hassmallerempirical
• Representationpower: somepropertiesofthe
risks,andresultsinmorerobustness,asintroduced
functionh : r (cid:55)→ y.
in Jin et al. (2021). Various studies explore the
Thenextquestionbecomestheorizingwhatprop- theoretical foundations for the above claims by
ertiesofthecomputationmodelh : r (cid:55)→ y weare connecting Kolmogorov complexity with statisti-
optimizing for, for which we will introduce two cal learning theory. For example, Vapnik (1995,
formulations,oneinthepre-LLMeraandtheother §4.6.1)showsthatanupperboundofKolmogorov
intheLLMera. complexity,called“compressioncoefficient,”can
bound the generalization error in classification
2.3 IdealRepresentationsinthePre-LLMEra
problems;Shalev-ShwartzandBen-David(2014,
Continuing on with the Arabic number example: Eq.3)andGoldblumetal.(2023)showthatgener-
Ourclaimisthattherepresentationr 2(i.e.,theAra- alizationerrorisupperboundedbytrainingerror
bicnumber)isbetterthanr 1 (i.e.,theEnglishex- plusaterminvolvingtheKolmogorovcomplexity
pression)becausethecomputationforh 2 : r 2 (cid:55)→ y ofthehypothesisspace.
is simpler than h : r (cid:55)→ y, as measured by
1 1
This is, we argue, the implicit framework behind
Kolmogorov complexity, or algorithmic entropy
many previous studies showing AMR as a bet-
(Solomonoff,1964;Kolmogorov,1965),whichis
ter representation than the raw text sequence by
atheoreticalconstructofthecomplexityofanal-
demonstratingitsbetterperformance(Turianetal.,
gorithminbits.2 Intuitively,theshortestprogram
2010), data efficiency (Liu et al., 2021), and ro-
specifyinganalgorithmtotakeEnglishexpressions
bustnessanddomaintransferability(Lietal.,2016;
like “Three hundred eighty-eight plus twelve” as
Jin et al., 2021). A crucial element of these stud-
inputandproduce“Fourhundred”asoutputshould
iesisthattheytrainmodelscustomizedexplicitly
belongerthanfortheonetaking“388+12”asin-
fortheAMRrepresentation,optimizinghoverthe
putand“400”asoutput,sincetheformerrequires
hypothesisspaceH.
morecomplicatedstringmanipulationtoachieve
thesameeffect.
2.4 RepresentationPowerintheLLMEra
We also use this notion of Kolmogorov complex- Asmentionedpreviously,intheeraofLLMs,we
ity to quantify the power of representations for aremovingtowardstheparadigmwherethemodel
language. Theintuitionisthatpowerfulrepresenta- training is usually outsourced, and during the in-
tionsarethosethatsignificantlysimplifythecom- ference stage, i.e., for most use cases, the model
plexity of the computation model h. Hence, the weightsarefixed. Formally,thismeanstwodiffer-
encesfromtheprevioussetting: (1)thehypothesis
2Formally, Kolmogorovcomplexityisthelengthofthe
shortestprogramwhichproducesastring. space H is collapsed to a size of one, containingonlythefixedfunctionh ,(2)theoptimization Dataset Task TestSize
LLM
constraintinEq.(3)thathcanmaptherepresen- PAWS ParaphraseDetection 8,000
WMT16 Translation 5,999
tationtothegroundtruthy isnotnecessarilyguar-
Logic LogicalFallacyDetection 2,449
anteed,namelythathcouldleadtoyˆ,withcertain Pubmed45 EventExtraction 5,000
SPIDER Text2SQLCodeGeneration 8,034
estimationerror.
Table1:Tasksanddatasetsused.
Therefore,thekeymeasureofrepresentationpower
intheLLMeranaturallyshiftsfromsimplicityof
thecomputationmodelh—whichaidsoptimization fromAMRin thepre-LLMera(Issaet al.,2018;
towards low estimation error—to low estimation Songetal.,2019;Gargetal.,2015;YinandNeu-
error itself, i.e., E[δ(yˆ,y)], where δ is the error big,2017).
function. This change results in a shift from the
For each dataset, we first take the entire original
doubleoptimizationoverbothr andhtotheopti-
test set, and if it has fewer than 5,000 examples,
mizationonlyofr withregardtothefixedh :
LLM
we also include the development or training set.
g∗ = argminE[δ(yˆ,y)], (5) DatastatisticsareinTable1anddetailsontestset
LLM
g∈G constructionareinAppendixA.1.
= argminE[δ(h (g(x)),y)], (6)
LLM
g∈G 3.2 AMRCOT PromptDesign
wheretheoptimalrepresentationr∗ becomes To test the utility of AMR with LLMs, we draw
LLM
inspirationfromtheCoTpromptdesign(Weietal.,
r∗ = g∗ (x). (7)
LLM LLM 2022;Nyeetal.,2021),togetherwithCoTvariants
on causal (Jin et al., 2023) and moral reasoning
This framework can also be used to explain the
tasks(Jinetal.,2022b),whichenablesmodelsto
success,forexample,ofCoTprompting(Weietal.,
answeraninitiallydifficultquestionwiththehelp
2022; Nye et al., 2021) in terms of how the in-
of assistive intermediate steps to render the task
termediaterepresentationgeneratedbyCoTbetter
easier.
unlocksthepowerofLLMs.
ComparingEqs.(2)to(4)withEqs.(5)to(7),we
WeproposeAMRCOT,inwhichwesupplementthe
canseethattheidealbestrepresentationr∗ isnot input text with an automatically-generated AMR
necessarily equal to the representation r∗ that andconditiontheLLMontheinputtextandAMR
LLM
whengeneratingtheanswer. IfAMRhasastronger
workswellwithLLMs,sothereremainsaneedfor
representationpowerthantherawtext,thenprovid-
experimentstofillinthisknowledgegap.
ingAMRasanassistiveintermediatestepshould
It is also worth noting that for any learned repre-
improvetheperformanceofLLMs.
sentation function g, errors in p (r|x) relative to
g
p (r|x) may cascade into the computation step We compare AMRCOT to directly querying the
g∗
p(y|r), harming the final output. We investigate LLMs,denotedBASE. Anexamplepromptpairis
thisconcerninSection6.1. showninTable2,andallpromptsforalldatasets
3 Designingthe AMRCOT Experiments
BASE PleasetranslatethefollowingtextfromEnglishto
German.
WeintroduceanAMR-drivenpromptingmethod
Text:{sentence1}
whichwecallAMRCOT,andinvestigateitsperfor-
Translation:
manceonfivedatasetswithfiveLLMs. AMRCOT Youaregivenatextanditsabstractmeaningrepre-
sentation(AMR).
3.1 DatasetSetup Text:{sentence1}
AMR:
Wetest AMRCOTonparaphrasedetection(Zhang {amr1}
et al., 2019), machine translation (Bojar et al., Please translate the text from English to German.
YoucanrefertotheprovidedAMRifithelpsyouin
2016),logicalfallacydetection(Jinetal.,2022a),
creatingthetranslation.
event extraction (Garg et al., 2015), and text-to- Translation:
SQLgeneration(Yuetal.,2018). Weselectthese
Table2:ExampleBASEandAMRCOTprompt(forthetrans-
tasksastheyhingeoncomplexsentencestructures lation task). We serialize AMRs with the commonly used
and most of them are reported to have benefited Penmannotation(Patten,1993).areinAppendixA.3. Dataset %Helped %Hurt %Unchanged
PAWS 16.48 20.16 63.36
3.3 LanguageModels WMT 16.45 21.17 62.38
Logic 1.96 2.45 95.59
Sinceourexperimentsrequiremodelsthatcanrea- Pubmed45 4.84 11.66 83.5
SPIDER 4.94 4.33 90.72
sonably understand and reason over the symbols
inAMRs, wefindthatonlytheinstruction-tuned Table4:Percentageoftestsamplesthatarehelped(%Helped),
GPTmodels,fromtext-davinci-001toGPT-4are hurt(%Hurt),orunchanged(%Unchanged)whenwechange
fromBASEtoAMRCOTusingGPT-4.
capableofprocessingit,butnottheopen-sourced
modelssuchasLLaMaandAlpaca,atthetimewe
conductedourresearch. Forreproducibility,weset thecase,wecalculatethepercentageofexamples
the text generation temperature to 0 for all mod- whicharehelpedandhurtbyAMRCOT,shownin
els,andweusethemodelcheckpointsfromJune Table 4. We count a sample as helped by AMR
13,2023forGPT-3.5andGPT-4,namelygpt-3.5- ifitspredictionimproves(i.e.,theoutputchanges
turbo-0613andgpt-4-0613. fromincorrecttocorrectinclassificationtasks,or
its score increases in text generation tasks), and
3.4 AddressingResearchQuestions
hurtbyAMRifitspredictiondegrades;therestof
4 Q1: DoesAMRHelpLLMs? theexamplesareconsideredunchanged.
First,weareinterestedintheutilityofAMRasan
intermediaterepresentationforLLMs. Specifically, AsshowninTable4,AMRcanchangeasignificant
we answer the following subquestions: what is proportion of answers, with 36.64% changed on
the overall effect of AMR as a representation on PAWS,and37.62%changedonWMT.Onitsface,
LLMs’performance(Section4.1)? Doestheeffect the lack of overall improvement from AMR sup-
vary case by case (Section 4.2)? And how does ports the current concern in the NLP community
the effect change with using various LLMs with thattraditionallinguisticsmighthavelittleroleto
differentlevelsofcapabilities(Section4.3)? playinimprovingtheperformanceofNLPsystems
intheeraofLLMs(Ignatetal.,2024). However,as
4.1 OverallEffectofAMR thereisasubstantialsubsetofthedatawhereAMR
helps, if these improvements come from certain
We first evaluate the overall effect of AMR as a
systematicallyidentifiablesubsetsofthedata,then
representationtoassistLLMs. Followingthesetup
this could provide clues for how structures such
in Section 3, Table 3 shows performance on our
asAMRmaypotentiallybeleveragedtoimprove
fiveNLPtasks. Comparing AMRCOTtothe BASE
overallperformance. Weinvestigatethisquestion
methodwhichdirectlyqueriesLLMs,AMRdoes
furtherinSections5and6.
nothaveanoverallpositiveimpactonperformance.
Theperformancefluctuatesbetweenaslightdrop
(-1to-3inmosttasks)andaslightincrease(+0.61
4.3 AMR’sEffectonModelswithDifferent
inthecaseofText-to-SQLcodegeneration).
Capabilities
Dataset Task BASE ∆AMRCOT
Figure 2showsthe resultsof our experiments on
PAWS ParaphraseDetection 78.25 -3.04
models of varying capability, from text-davinci-
WMT Translation 27.52 -0.83
Logic FallacyDetection 55.61 -0.49 001, -002, -003, to GPT-3.5 and GPT-4. Over-
Pubmed45 EventExtraction 39.65 -3.87
all, AMRCOT hurts performance for most tasks
SPIDER Text2SQL 43.78 +0.61
andmodels,againwithText-to-SQLbeingtheex-
Table 3: Across the five tasks, we report the baseline per- ception, at least for text-davinci-003 and GPT-4.
formance (BASE), and the additional impact of AMRCOT
Insomecases,lesscapablemodelsdegrademore
(∆AMRCOT),usingGPT-4.Seestatisticalsignificancetests
inAppendixD.1. when using AMR, which might be due to their
limited ability to comprehend AMR and reason
4.2 HelpfulnessofAMRinSomeCases
over its special symbols. This is consistent with
UsingAMRhardlychangesoverallperformance, ourpreliminaryobservationsthatnoneofthenon–
butthiscouldbeeitherbecauseitdoesnotchange instruction-tuned earlier GPT models, or the less
model predictions or because it helps in roughly capablemodelssuchasLLaMaandAlpaca,com-
as many cases as it hurts. To explore which is prehendAMRorreasonoverthemwell.PAWS WMT Logic Pubmed45 SPIDER
40 60 60 60
80
30
60 40 40 40
40 20
20 20 20
20 10
0 0 0 0 0
text-d. t-
e0
x0
t-1
d. t-
e0
x0
t-2 d.-003 gpt-3.5 gpt-4
text-d. t-
e0
x0
t-1
d. t-
e0
x0
t-2 d.-003 gpt-3.5 gpt-4
text-d. t-
e0
x0
t-1
d. t-
e0
x0
t-2 d.-003 gpt-3.5 gpt-4
text-d. t-
e0
x0
t-1
d. t-
e0
x0
t-2 d.-003 gpt-3.5 gpt-4
text-d. t-
e0
x0
t-1
d. t-
e0
x0
t-2 d.-003 gpt-3.5 gpt-4
Figure2:PerformanceofBASE(inpurple)andAMRCOT(inred)on5datasetsacross5modelversions:text-davinci-001|-002|-
003,GPT-3.5andGPT-4.
5 Q2: WhenDoesAMRHelp/Hurt? OriginalSentencewithMWE
Herswansongdisappointedherfans.
(z0/disappoint-01
The previous section shows that AMR is helpful
:ARG0(z1/song
orharmfulfordifferentsamples. Nowweinvesti- :mod(z2/swan)
gatetheconditionsunderwhichithelpsorharms :poss(z3/she))
:ARG1(z4/fan
performance,inparticularwhetherthiscanbepre-
:possz3))
dicted from features of the input text. We first
illustrateacasestudyinSection5.1,whereAMR’s ParaphraseCandidate1 ParaphraseCandidate2
(✗Notaparaphrase.) (✓Aparaphrase.)
lackofabilitytocapturethesemanticequivalence
ofmulti-wordexpressions(MWEs)hinderspara- Herbirdsongdisappointed Herfinalperformance
herfans. disappointedherfans.
phrasedetection. Then,weperformtwosystematic
(z0/disappoint-01 (z0/disappoint-01
interpretability studies: First, we treat linguistic
:ARG0(z1/song :ARG0(z1/perform-02
features as our hypotheses, and extract features :mod(z2/bird) :ARG0(z2/she)
:poss(z3/she)) :mod(z3/final))
withhighcorrelationwithAMRhelpfulness(Sec-
:ARG1(z4/fan :ARG1(z4/fan
tion 5.2); second, we directly train classifiers to :possz3)) :possz2))
learnAMRhelpfulness(Section5.3).
Figure3:AnexampleshowingthefailureofAMRforpara-
phrasedetectionwhentheoriginalsentenceinvolvesaMWE.
5.1 CaseStudy: AMR’sShortcomingson
ThisexampleisfromourGoldSlang-ComposedAMRdataset.
MWEs
and GoldAMR-ComposedSlang. For GoldSlang-
AMR has its unique advantages and limitations,
ComposedAMR, we use the curated slang para-
fromwhichwecaninterpretwhatcasesitcanhelp,
phrase pairs by Tayyar Madabushi et al. (2021)
andwhatcasesnot. OnesuchlimitationofAMR
and generate their AMRs with an off-the-shelf
is its lack of ability to capture MWEs such as id-
parser (Drozdov et al., 2022). For GoldAMR-
iomaticexpressions,whichmakesitoverlookcer-
ComposedSlang,weusegoldAMRsfromtheLDC
tain semantic equivalences for paraphrase detec-
AMR3.0corpus(Banarescuetal.,2013),andcom-
tion. ConsidertheexampleinFigure3. Here,the
poseslangparaphrasesusingacombinationofman-
properparaphrasefortheMWEswansongisnot
ual annotation and assistance from GPT-4. The
“birdsong,”but“finalperformance.” However,the
data curation steps and data statistics are in Ap-
AMRsforthethreesentencesdonotreflectthis;the
pendixB.1.
AMRforthe“swansong”sentenceisstructurally
andlexicallymoresimilartothe“birdsong”AMR
Table5showsevaluationresults,where AMRCOT
thantheoneforthe“finalperformance”variant.
causes a large drop in performance compared to
Giventhisintuition,wequalitativelystudywhether
BASE,moresubstantialthantheslightfluctuation
of-3to+1percentagepointsshownpreviouslyin
AMR systematically fails on texts that contain
more MWEs. We run AMRCOT on a self-
composed dataset of paraphrase detection in- Dataset BASE ∆AMRCOT
volving slang, assuming slang has more MWEs. GoldSlang-ComposedAMR 86.83 -6.63
Since our experiments need annotations for both GoldAMR-ComposedSlang 77.69 -8.78
slang paraphrase pairs and AMRs, we com- Table5: AMRCOTresultsinalargedropinperformanceon
pose two datasets, GoldSlang-ComposedAMR, slang-comprisingparaphrasedetectiondata.
1F
UELB
1F 1F
hctaMtcaxETop5PositiveFeatures PearsonCorrelation Top5NegativeFeatures PearsonCorrelation
AdjPOSTagFrequency 0.0393 #NamedEntities -0.0630
Avg.WordComplexity 0.0343 %ofTokensContainingDigits -0.0281
#Adjuncts 0.0337 #ProperNouns -0.0258
MaxWordComplexity 0.0316 #ThirdPersonSingularPronouns -0.0236
Avg.WordFrequency 0.0271 #QuantifierPhraseModifiers -0.0222
Table6: Thetopfivefeatureswiththehighestpositivecor- Table7:Thetopfivefeatureswiththehighestnegativecorre-
relationcoefficientstoAMRhelpfulness: thefrequencyof lationcoefficientstoAMRhelpfulness:thenumberofnamed
adjectives among all the words (Adj POS Tag Frequency), entities, percentage of tokens containing digits, number of
averagewordcomplexitylevelbytheageofacquisition(Ku- propernouns(e.g.,London),numberofthirdpersonsingular
perman et al., 2012), number of adjuncts, maximum word pronouns(e.g.,he),andnumberofquantifierphrasemodifiers.
complexitylevelbytheageofacquisition,andaverageword SeedetailedexplanationsoffeaturesinAppendixC.
frequency.
tences: AMR is most helpful for samples with
moreadjectives,complexwords,andadjuncts. In
Table3. Itisverylikelythat,duetotheshortcom-
Table 7, the top negative feature, the number of
ingsofAMRonMWEs,AMRCOTmostlydistracts
namedentities,echoesthefindinginourprevious
themodel,yieldingworseperformance.
MWEcasestudyinSection5.1,andwesystemati-
5.2 Large-ScaleTextFeatureAnalysis callyshowthatAMRismostharmfulonsamples
withmanynamedentities,tokenscontainingdigits,
The case study above provides a precise insight
andpropernouns.
intoaspecialcasewhenAMRdoesnotwork. To
systematically explore a larger set of hypotheses, 5.3 AMRHelpfulnessPredictionasa
weperformafeatureanalysisovertheinputtexts. LearningTask
WeformulatethecontributionofAMRastheAMR
Now we analyze the upper-bound predictability
helpfulnessscore,whichistheper-exampleperfor-
of AMR helpfulness from the input, both on the
mance difference between AMRCOT and BASE,
basis of our linguistic features and text input it-
rangingbetween-100%and100%,whereanega-
self. Specifically,wetrainmodelstopredictAMR
tivevaluemeansthatAMRhurtsperformanceon
helpfulnessasabinaryclassificationtaskwherethe
theexample,andapositivevaluemeansthatAMR
positiveclassisthecasewhereAMRhelps,andthe
improvesperformance.
negativeclassistherest. Mergingallfivedatasets
For each input, we compute a comprehensive set together,wehaveabinaryclassificationdatasetof
oflinguisticfeatures,including139featuresonthe 19,405trainingsamples,4,267developmentsam-
textrepresentation,and4featuresderivedfromthe ples, and 5,766 test samples, with positive labels
AMR.Specifically,weobtain55featuresusingthe composing10.38%ofthedataset.
TextCharacterizationToolkit(TCT)(Simigetal.,
AsshowninTable8,classifiersbasedonlinguis-
2022),whichisspecificallydesignedtofacilitate
tic features achieve an F1 score of up to 32.67%.
the analysis of text dataset properties, 17 differ-
BERT-baseddeeplearningmodelsimprovebyup
entpart-of-speech(POS)tags,44dependencytags,
to1.16F1scores,withsubstantialincreasesinre-
and61otherhand-craftedfeatures,whichcharac-
call. For interpretability, we run Shapley feature
terizethesemanticandsyntacticcomplexityofthe
attributionmethod(Fryeretal.,2021)andfindthat
inputtext,suchasthenumberofargumentsvs.ad-
juncts(Haspelmath,2014).
Model F1 Acc P R
Tables 6 and 7 show the Pearson correlation be- RandomBaseline 16.14 49.95 9.65 49.16
tween each linguistic feature and the AMR help- UsingLinguisticFeatures
RandomForest 32.67 81.93 25.72 44.75
fulnessscore. Overall,thecorrelationofeachindi-
XGBoost 30.08 78.47 22.06 47.27
vidualfeaturetotheAMRhelpfulnessscoreisnot Ensemble 30.42 77.59 21.85 50.00
strong,eitherbecausethesefeaturesdonotexplain UsingtheFree-FormTextInput
BERT 33.83 79.70 25.00 52.28
much about AMR helpfulness, or because it re-
RoBERTa 33.29 80.36 25.11 49.38
quiresacombinationofmultiplefeatures. Though
Table 8: Classification performance of various models on
the correlations are weak, the top correlated fea-
AMRhelpfulness.WereporttheF1,precision(P),andrecall
turesinTable6alignwithourintuitionthatAMR (R)ofthepositiveclass,aswellastheaccuracy(Acc). See
should be helpful for semantically complex sen- theimplementationdetailsofthemodelsinAppendixA.5.words that signal the existence of clauses tend to parser-generatedAMRonNER,showninTable9.
have high importance for the classifier, such as Both lead to similar results, with a difference of
“what,” “how,” “said,” and “says.” These results lessthantwopercentagepoints(whichisnotsta-
donotprovideaclearexplanationofwhenAMR tisticallysignificant,withp=0.627byt-test). The
canhelp,butgiveastartingpoint,andwewelcome test set is unfortunately too small to reliably de-
futureresearchtocontinueexploringthepotential tect an effect of reasonable size, due to the lack
benefitsofAMR.ThefactthatAMRhelpfulness of available data with both gold AMR and NLP
is challenging to predict even for BERT models taskannotations;thisresultisalsospecifictoNER,
mayindicateeitherthatweneedmoredatatolearn which may not have all of the relevant features
the features that predict this, or that a substantial for understanding the effect of gold versus auto-
portionofthechangesthatAMRmakestomodel maticallyproducedAMRs. However,thefactthat
predictionscorrespondtonoise(i.e., helporhurt the observed effect size is very small constitutes
inunpredictableways). someevidencethatimprovingthepredictedAMRs
would likely not play a huge role in increasing
6 Q3: WhyDoesAMRHelp/Hurt? downstreamperformancewithcurrentmodels.
To understand why AMR helps or hurts when it
6.2 AblatingtheAMR/TextRepresentation
does,welookintothefollowingsubquestions: (1)
AsdiscussedinSection2,AMRandtextrepresen-
howdoesparser-generatedAMRworkcompared
tationsaretwodifferentsurfaceformsforexpress-
with gold AMR (Section 6.1)? (2) what is the
ingsentencesemantics,butonerepresentationmay
representationpowerofAMRversustextwhenthe
bemoreusefultotheLLMthantheother. Totest
otherisablated(Section6.2)? And(3)howdoes
this,weconductanablationstudyremovingeither
AMR help in each step of the reasoning process
theoriginaltextortheAMRandmeasuringperfor-
(Section6.3)?
mance(seeAppendixA.6). Toavoidthepotential
6.1 Goldvs. Parser-GeneratedAMR forcascadingerrorsfromtheparsingprocess,we
usetheAMR-NERdatasetwiththegoldAMR.
First, we investigate whether there are cascading
errors before the CoT process, due to mistakes
70
in the parser-generated AMR. For example, the
reportedperformance ofDrozdovet al.(2022) is 60
83% on AMR 3.0 (Banarescu et al., 2013). To
50
assess this, we compare AMRCOT performance
40 %AMRAblated
whenusingpredictedversusgoldAMRs. Testing %TextAblated
30
this requires data with gold AMR annotations as
0 20 40 60 80 100
wellasgoldlabelsforsomedownstreamNLPtask
AMR/TextTokens(%)AblatedinthePrompt
wecanevaluatethemodelson. Tothisend,wetake
theintersectionoftheAMR3.0dataset(Banarescu Figure4:AblationstudiesofAMRandtextrepresentations
inthepromptontheAMR-NERdatasetusingGPT-4.Start-
et al., 2013) with Ontonotes 5.0 (Pradhan et al.,
ingfromthe AMRCOT promptwiththecompletetextand
2011),whichcontains131sentencesthathaveboth AMR,werandomlydropoutacertainportionoftokensinthe
text/AMR,andseetheeffectonthetaskperformance.
gold AMR and named entity recognition (NER)
annotations. WelisttheintuitionofwhyAMRcan
behelpfulforNERinAppendixB.2.
Results Inadditiontopreviousresultscontrast-
Using this AMR-NER dataset, we compare the
ing AMRCOT, which provides both the text and
performanceofAMRCOTwithgoldAMRversus
AMRintheprompt,andBASEwiththetext-only
input,weshowtheresultsofamoregranularanal-
ysisinFigure4,wherewerandomlydropouttext
Dataset BASE AMR ∆AMRCOT
and AMR tokens and measure the effect on task
Gold +0.03
AMR-NER 60.51 performance. Similar to the above, we find that
Parser +1.91
droppingAMRmarginallydecreasesperformance,
Table9:ModelperformanceontheAMR-NERdatausingthe
anddroppingtextmuchmoredrasticallydegrades
goldAMR(Gold)andparser-generatedAMR(Parser). We
reporttheBASEperformance,andthechangeofperformance LLMperformance, showingthegreaterutilityof
byAMRCOT(∆AMRCOT)intermsofF1scores. textasarepresentationforLLMs. Wealsoconduct
)%(ecnamrofrePksaTthesameablationstudyon1,000randomsamples assemanticparsing(KuhnandDeMori,1995),ma-
from the WMT dataset using predicted AMRs in chinetranslation(WuandFung,2009;Wongand
AppendixD.3,wheretheobservationsaresimilar. Mooney,2006),andtextsummarization(Liuetal.,
2018). Recent research also looks into whether
6.3 CheckingtheStep-By-StepReasoning
LLMs already incorporate a good understanding
TobetterunderstandhowLLMsuseAMR,wedi- of semantic representations (e.g., Staliu¯naite˙ and
rectlyexaminethestep-by-stepreasoningprocess Iacobacci,2020;Blevinsetal.,2023).
producedby AMRCOT withGPT-4. Werandomly
Chain-of-Thought Prompting Rapid advance-
select50samplesfromthePAWSdatasetandman-
ment in LLMs has led to a new paradigm of per-
ually annotate the correctness of each step in the
formingNLPtasksbyelicitingmodelbehaviorvia
reasoningprocess. ForparaphrasingonPAWS,the
instructionsandexamplesusingprompting(Brown
steps(andourevaluations)areasfollows:
etal.,2020;Raffeletal.,2020). Chain-of-thought
1. ProducetheAMRfortheinputsentencesus- (CoT) prompting (Wei et al., 2022; Nye et al.,
ingDrozdovetal.(2022)’sstructuredBART 2021), which pairs input examples with step-by-
model. Instead of manually annotating cor- stepexplanationsofhowtoproducetheirrespec-
rectnessoftheseAMRs,wedefertotheirre- tive outputs, has been shown to improve LLMs’
portedperformanceof82.6SMATCHscores performanceatvariousreasoningtasks(Yuetal.,
ontheAMR3.0dataset. 2023),anditsvariantshavealsoshownsuccessin
2. ProvidetheAMRstoGPT-4intheparaphras- variousscenarios,suchas CAUSALCOTforcausal
ing task prompt using AMRCOT, and then reasoning(Jinetal.,2023),and MORALCOT for
instruct it to list all the commonalities and moralreasoning(Jinetal.,2022b).
differencesoftheAMRs. Ourmanualcheck
Ourworkproposesawaytobridgelinguisticrepre-
finds that GPT-4 achieves a 97% F1 score
sentations with text by AMRCOT, providing an
(with 95% precision, 98% recall) at listing
AMR as an intermediate representation for the
these.
LLMtoreasonover. Ourresultsaremixed,demon-
3. GPT-4 then outputs a final decision on strating the relative advantage that unstructured,
whether the sentences are paraphrases. We free-textrepresentationshaveforlanguagemodels
evaluatethatitsjudgmentinthisstepiscon- pretrained on large amounts of natural language
sistentwiththereasoninginthepriorstep80% data.
ofthetime.
8 Conclusion
EventhoughGPT-4wasabletocorrectlyenumer-
atetherelevantfeaturesoftheAMRs, itstillhad Inthiswork,weanalyzetheroleofsemanticrep-
troublesynthesizingthisinformationintoacorrect resentations in the era of LLMs. In response to
paraphrasingjudgment. Thesemistakesaswellas theongoingparadigmshiftintheNLPcommunity,
thepotentialforcascadingerrorsmayexplainwhy we show that AMR in general is not yet a rep-
AMRCOT achieves a performance of 75.21% on resentationimmediatelyfitforpre-trainedLLMs.
PAWS,whichisaslightdropfromthe BASE per- However, our study shows that AMR still helps
formanceof78.25%. Overall,thisprovidesfurther onsomesamples. Wealsosuggestthatapotential
evidenceoftheadvantagesthatfree-formtextitself directiontoenhanceAMR’scontributiontoLLMs
hasasarepresentationforLLMstooperateon. istoimprovetheunderstandingofLLMsoverthe
schemesandsymbolsofAMR,andmapittothe
7 RelatedWork reasoning of the respective NLP task. This work
presents an effort to bridge the traditionally rich
Semantic Representations Traditionally, NLP
linguisticstructureswiththestrengthofLLMs.
modelsoftenrepresenttextbyfeaturesdeveloped
onthebasisoflinguisticexpertise,amongwhich
LimitationsandFutureWork
semanticrepresentationssuchasAMR(Banarescu
etal.,2013)areusedtoabstractawaythesurface This work explores one form of linguistic repre-
formofthetextanddistillthemostimportantele- sentationoftext. Inthefuture,wewelcomemore
mentsofitsmeaning. Inthepast,suchrepresenta- exploration on various other linguistic represen-
tionshavehelpedwithavarietyofNLPtasks,such tations using the methodology presented in thiswork. Moreover,weexploreoneintuitivewayof mainexpertise. WealsoappreciateWendongLiang
promptingthemodel. Futureworkiswelcometo forinsightfuldiscussionsonKomolgorovcomplex-
explore different ways of prompting to make the ity,whichisafoundationofthetheoreticalframe-
AMR information more accessible and useful to workinthiswork. WethankNilsHeilforextract-
themodel. ing the SQL schemes of the SPIDER dataset so
thatwecanincorporatethemintheprompttoim-
Inaddition,someofouranalysesarelimitedbya
proveourperformance. Thismaterialisbasedin
lackofannotatedresources,sowewereonlyable
part upon works supported by the German Fed-
toshowexperimentalresultsonhundredsofexam-
eralMinistryofEducationandResearch(BMBF):
ples in some cases where gold AMR annotation
Tübingen AI Center, FKZ: 01IS18039B; and by
is needed. This is a commonly known issue for
theMachineLearningClusterofExcellence,EXC
AMR,whichisexpensiveandrequiresahighlevel
number2064/1–Projectnumber390727645;Zhi-
oflinguisticexpertisetoannotate. Thislimitation
jingJinissupportedbyPhDfellowshipsfromthe
makestheresultslessstatisticallysignificantthan
FutureofLifeInstituteandOpenPhilanthropy.
what we could have if there are more annotated
AMRsavailable. Inthiswork,wehopetostrikea
balancetostillshowsomemeaningfultrendswhile
AuthorContributions
tryingtogetthelargestsizeofannotateddatawe
can.
MonaDiabinitiatedtheprojectideabasedonher
Moreover,ifanyfutureworkhastheresourcesto strong expertise in traditional linguistics, and an
train an LLM specifically optimized for AMR as intuitionthatthesemanticrepresentationsshould
arepresentation,thiswouldbetheidealsettingto helpmodelefficiency,robustness,andinterpretabil-
checkouttheupperboundofthepowerofAMR ity. During the course of exploration by Zhijing
intheeraofLLMs. Jin and Mona Diab together for over a year, they
findthattheAMRrepresentationsdoesnotalways
As for the limitations for specific parts of the pa-
helpLLMsovermultipleexperimentalsetupsand
per, for example for the notion of gold AMRs in
modelimplementations.
Section 6, although we use the AMR annotated
by humans in the official Banarescu et al. (2013)
dataset,itshouldbenotedthatsuchAMRsarenot Zhijingfurtherexploresthetheoreticalformulation
necessarily“perfect”,ashumansmightalsohavea of representation power to provide the explana-
non-perfectinter-annotatoragreementoversome tions behind the observed performance, together
AMRs. AndwhileSMATCHscorescanbepredic- withtheexpertiseofBernhardSchoelkopfincausal
tive, they may not perfectly reflect the quality of representation learning. Julian Michael provided
parser-generatedAMRs(OpitzandFrank,2022). valuable insights and overview of the field of se-
These are both open research questions, and we manticrepresentations,whichbringsthedepthof
usetheAMRsreleasedbytheofficialsource(Ba- the project to another level. Julian also provided
narescu et al., 2013) as a proxy for ground-truth constructivesuggestionsforimprovingtheexperi-
AMR. mentsandstructuringthepaper,andsubstantially
improvedthewriting.
EthicalConsiderations
Thedatasetsusedinthispaperareexistingpublic YuenChenandFernandoGonzalezcontributedsub-
datasets on general NLP tasks without any user- stantiallytoscalingupalltheexperimentsacross
sensitiveinformation. Wearenotawareofspecific multipledatasetsandmultiplemodelversions,and
ethical concerns with the analysis in this study, analyzing the results. Jiarui Liu and Jiayi Zhang
whichisaneutralinvestigationtounderstandthe helped with the training the BERT-based classi-
roleoftraditionallinguisticstructuressuchasse- fiers, and analyzing the Shapley values. Jiarui
manticrepresentationsintheeraofLLMs. Liu conducted several important experiments for
thecamera-readyversionofthepaper,especially
Acknowledgments
oncheckingtheceilingperformanceof AMRCOT
WethankJuriOpitzforhisinsightfulsuggestions withvariouspromptimprovementsanddatasetups.
on our AMR experiments based on profound do-References transition-basedAMRparsing. InProceedingsofthe
2022ConferenceoftheNorthAmericanChapterofthe
Laura Banarescu, Claire Bonial, Shu Cai, Madalina AssociationforComputationalLinguistics:HumanLan-
Georgescu,KiraGriffitt,UlfHermjakob,KevinKnight, guageTechnologies,pages1086–1098,Seattle,United
PhilippKoehn,MarthaPalmer,andNathanSchneider. States.AssociationforComputationalLinguistics. 6,8,
2013. Abstract meaning representation for sembank- 9,16
ing. In Proceedings of the 7th Linguistic Annotation
Workshop and Interoperability with Discourse, LAW- DanielVidaliFryer,IngaStrümke,andHienD.Nguyen.
ID@ACL2013,August8-9,2013,Sofia,Bulgaria,pages 2021. Shapleyvaluesforfeatureselection: Thegood,
178–186.TheAssociationforComputerLinguistics. 1, thebad,andtheaxioms. CoRR,abs/2102.10936. 7
6,8,9,10,16
SahilGarg,A.G.Galstyan,UlfHermjakob,andDaniel
TerraBlevins,HilaGonen,andLukeZettlemoyer.2023. Marcu. 2015. Extracting biomolecular interactions
Promptinglanguagemodelsforlinguisticstructure. In using semantic parsing of biomedical text. ArXiv,
Proceedingsofthe61stAnnualMeetingoftheAssoci- abs/1512.01587. 1,4,16
ationforComputationalLinguistics(Volume1: Long
MicahGoldblum,MarcFinzi,KeeferRowan,andAn-
Papers),pages6649–6663,Toronto,Canada.Associa-
drewGordonWilson.2023. Thenofreelunchtheorem,
tionforComputationalLinguistics. 9
Kolmogorovcomplexity,andtheroleofinductivebiases
OndrejBojar,RajenChatterjee,ChristianFedermann, inmachinelearning. CoRR,abs/2304.05366. 3
YvetteGraham,BarryHaddow,MatthiasHuck,Anto-
NamanGoyal,CynthiaGao,VishravChaudhary,Peng-
nioJimenoYepes,PhilippKoehn,VarvaraLogacheva,
Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krish-
Christof Monz, Matteo Negri, Aurelie Neveol, Mari-
nan, Marc’Aurelio Ranzato, Francisco Guzmán, and
anaNeves,MartinPopel,MattPost,RaphaelRubino,
Angela Fan. 2022. The Flores-101 evaluation bench-
CarolinaScarton, LuciaSpecia, MarcoTurchi, Karin
markforlow-resourceandmultilingualmachinetransla-
Verspoor,andMarcosZampieri.2016. Findingsofthe
tion. TransactionsoftheAssociationforComputational
2016conferenceonmachinetranslation. InProceed-
Linguistics,10:522–538. 17
ings of the First Conference on Machine Translation,
pages131–198,Berlin,Germany.AssociationforCom- MartinHaspelmath.2014. Argumentsandadjunctsas
putationalLinguistics. 4 language-particularsyntacticcategoriesandascompar-
ativeconcepts. LinguisticDiscovery,12:3–11. 7
TomB.Brown,BenjaminMann,NickRyder,Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind LifuHuang,HengJi,KyunghyunCho,IdoDagan,Se-
Neelakantan, Pranav Shyam, Girish Sastry, Amanda bastianRiedel,andClareVoss.2018. Zero-shottransfer
Askell,SandhiniAgarwal,ArielHerbert-Voss,Gretchen learningforeventextraction. InProceedingsofthe56th
Krueger,TomHenighan,RewonChild,AdityaRamesh, AnnualMeetingoftheAssociationforComputational
DanielM.Ziegler,JeffreyWu,ClemensWinter,Christo- Linguistics(Volume1: LongPapers),pages2160–2170,
pherHesse, MarkChen, EricSigler, MateuszLitwin, Melbourne,Australia.AssociationforComputational
ScottGray, BenjaminChess, JackClark, Christopher Linguistics. 1,16
Berner,SamMcCandlish,AlecRadford,IlyaSutskever,
OanaIgnat,ZhijingJin,ArtemAbzaliev,LauraBiester,
and Dario Amodei. 2020. Language models are few-
SantiagoCastro,NaihaoDeng,XinyiGao,AylinGunal,
shotlearners. InAdvancesinNeuralInformationPro-
JackyHe,AshkanKazemi,MuhammadKhalifa,Namho
cessingSystems33: AnnualConferenceonNeuralIn-
Koh, Andrew Lee, Siyang Liu, Do June Min, Shinka
formationProcessingSystems2020,NeurIPS2020,De-
Mori, Joan Nwatu, Verónica Pérez-Rosas, Siqi Shen,
cember6-12,2020,virtual. 2,9
ZekunWang,WinstonWu,andRadaMihalcea.2024.
Tianqi Chen and Carlos Guestrin. 2016. XGBoost: Has it all been solved? Open nlp research questions
A scalable tree boosting system. In Proceedings of notsolvedbylargelanguagemodels. InProceedings
the22ndACMSIGKDDInternationalConferenceon ofthe2024JointInternationalConferenceonCompu-
KnowledgeDiscoveryandDataMining,SanFrancisco, tationalLinguistics,LanguageResourcesandEvalua-
CA,USA,August13-17,2016,pages785–794.ACM. tion(LREC-COLING2024).InternationalCommittee
14 onComputationalLinguistics. 2,5
Marie-CatherinedeMarneffeandChristopherD.Man- Fuad Issa, Marco Damonte, Shay B. Cohen, Xiaohui
ning. 2008. The Stanford typed dependencies repre- Yan,andYiChang.2018. Abstractmeaningrepresenta-
sentation. In Coling 2008: Proceedings of the work- tionforparaphrasedetection. InNorthAmericanChap-
shoponCross-FrameworkandCross-DomainParser teroftheAssociationforComputationalLinguistics. 1,
Evaluation,pages1–8,Manchester,UK.Coling2008 4
OrganizingCommittee. 17
AnubhavJangra,PrekshaNema,andAravindanRaghu-
ShibhanshDohareandHarishKarnick.2017. Textsum- veer.2022. T-STAR:TruthfulstyletransferusingAMR
marizationusingabstractmeaningrepresentation. 1 graphasintermediaterepresentation. InProceedingsof
the2022ConferenceonEmpiricalMethodsinNatural
AndrewDrozdov,JiaweiZhou,RaduFlorian,Andrew Language Processing, pages 8805–8825, Abu Dhabi,
McCallum, Tahira Naseem, Yoon Kim, and Ramón UnitedArabEmirates.AssociationforComputational
Astudillo. 2022. Inducing and using alignments for Linguistics. 1Zhijing Jin, Yuen Chen, Felix Leeb, Luigi Gresele, Sadeh,andNoahA.Smith.2018. Towardabstractive
OjasvKamal,ZhihengLyu,KevinBlin,FernandoGon- summarizationusingsemanticrepresentations. CoRR,
zalez, Max Kleiman-Weiner, Mrinmaya Sachan, and abs/1805.10399. 9
BernhardSchölkopf.2023. CLadder: Assessingcausal
reasoninginlanguagemodels. InNeurIPS. 4,9 ZhiyuanLiu,YankaiLin,andMaosongSun.2021. Rep-
resentation learning for natural language processing.
ZhijingJin,AbhinavLalwani,TejasVaidhya,Xiaoyu CoRR,abs/2102.03732. 3
Shen, Yiwen Ding, Zhiheng Lyu, Mrinmaya Sachan,
RadaMihalcea,andBernhardSchölkopf.2022a. Logi- MaxwellI.Nye,AndersJohanAndreassen,GuyGur-
calfallacydetection. InFindingsoftheAssociationfor Ari,HenrykMichalewski,JacobAustin,DavidBieber,
ComputationalLinguistics: EMNLP2022,pages7180– David Dohan, Aitor Lewkowycz, Maarten Bosma,
7198,AbuDhabi,UnitedArabEmirates.Association DavidLuan,CharlesSutton,andAugustusOdena.2021.
forComputationalLinguistics. 4 Showyourwork: Scratchpadsforintermediatecompu-
tationwithlanguagemodels. CoRR,abs/2112.00114.
ZhijingJin,SydneyLevine,FernandoGonzalez,Ojasv 4,9
Kamal,MaartenSap,MrinmayaSachan,RadaMihal-
cea,JoshTenenbaum,andBernhardSchölkopf.2022b. JuriOpitzandAnetteFrank.2022. BetterSmatch=bet-
Whentomakeexceptions: Exploringlanguagemodels terparser? AMRevaluationisnotsosimpleanymore.
asaccountsofhumanmoraljudgment. InNeurIPS. 4, InProceedingsofthe3rdWorkshoponEvaluationand
9 ComparisonofNLPSystems,pages32–43,Online.As-
sociationforComputationalLinguistics. 10
ZhijingJin,JuliusvonKügelgen,JingweiNi,TejasVaid-
hya,AyushKaushal,MrinmayaSachan,andBernhard KishorePapineni,SalimRoukos,ToddWard,andWei-
Schölkopf. 2021. Causal direction of data collection JingZhu.2002. Bleu: amethodforautomaticevalua-
matters: Implicationsofcausalandanticausallearning tionofmachinetranslation. InProceedingsofthe40th
for NLP. In Proceedings of the 2021 Conference on AnnualMeetingoftheAssociationforComputational
Empirical Methods in Natural Language Processing, Linguistics,pages311–318,Philadelphia,Pennsylvania,
pages9499–9513,OnlineandPuntaCana,Dominican USA.AssociationforComputationalLinguistics. 14,
Republic.AssociationforComputationalLinguistics. 3 17
PavanKapanipathi,IbrahimAbdelaziz,SrinivasRavis- Terry Patten. 1993. Book reviews: Text generation
hankar,SalimRoukos,AlexanderGray,RamónFernan- andsystemic-functionallinguistics: Experiencesfrom
dezAstudillo,MariaChang,CristinaCornelio,Saswati EnglishandJapanese. ComputationalLinguistics,19(1).
Dana, Achille Fokoue, Dinesh Garg, Alfio Gliozzo, 4
SairamGurajada, HimaKaranam, NaweedKhan, Di-
David Patterson, Joseph Gonzalez, Quoc Le, Chen
nesh Khandelwal, Young-Suk Lee, Yunyao Li, Fran-
Liang,Lluis-MiquelMunguia,DanielRothchild,David
coisLuus,NdivhuwoMakondo,NandanaMihindukula-
So, MaudTexier, andJeffDean.2021. Carbonemis-
sooriya,TahiraNaseem,SumitNeelam,LucianPopa,
sionsandlargeneuralnetworktraining. 2
RevanthGangiReddy,RyanRiegel,GaetanoRossiello,
UditSharma,GPShrivatsaBhargav,andMoYu.2021.
Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,
LeveragingAbstractMeaningRepresentationforknowl-
MarthaPalmer,RalphWeischedel,andNianwenXue.
edgebasequestionanswering. InFindingsoftheAs-
2011. CoNLL-2011sharedtask: Modelingunrestricted
sociationforComputationalLinguistics: ACL-IJCNLP
coreference in OntoNotes. In Proceedings of the Fif-
2021,pages3884–3894,Online.AssociationforCom-
teenthConferenceonComputationalNaturalLanguage
putationalLinguistics. 1
Learning: SharedTask,pages1–27,Portland,Oregon,
AndreiNKolmogorov.1965. Threeapproachestothe USA.AssociationforComputationalLinguistics. 8
quantitativedefinitionofinformation. Problemsofin-
ColinRaffel,NoamShazeer,AdamRoberts,Katherine
formationtransmission,1(1):1–7. 2,3
Lee,SharanNarang,MichaelMatena,YanqiZhou,Wei
R. Kuhn and R. De Mori. 1995. The application of Li, and Peter J. Liu. 2020. Exploring the limits of
semanticclassificationtreestonaturallanguageunder- transferlearningwithaunifiedtext-to-texttransformer.
standing. IEEETransactionsonPatternAnalysisand JournalofMachineLearningResearch,21(140):1–67.
MachineIntelligence,17(5):449–460. 9 9
Victor Kuperman, Hans Stadthagen-González, and SiddharthSamsi,DanZhao,JosephMcDonald,Baolin
MarcBrysbaert.2012. Age-of-acquisitionratingsfor Li,AdamMichaleas,MichaelJones,WilliamBergeron,
30,000 english words. Behavior Research Methods, Jeremy Kepner, Devesh Tiwari, and Vijay Gadepally.
44:978–990. 7,17 2023. FromWordstoWatts: BenchmarkingtheEnergy
CostsofLargeLanguageModelInference. 2
YitongLi,TrevorCohn,andTimothyBaldwin.2016.
Learningrobustrepresentationsoftext. InProceedings ShaiShalev-ShwartzandShaiBen-David.2014. Un-
ofthe2016ConferenceonEmpiricalMethodsinNat- derstandingMachineLearning-FromTheorytoAlgo-
uralLanguageProcessing,pages1979–1985,Austin, rithms. CambridgeUniversityPress. 3
Texas.AssociationforComputationalLinguistics. 3
OrSharir,BarakPeleg,andYoavShoham.2020. The
FeiLiu, JeffreyFlanigan,SamThomson, NormanM. costoftrainingnlpmodels: Aconciseoverview. 2DanielSimig,TianluWang,VernaDankers,PeterHen- Technologies: ConferenceoftheNorthAmericanChap-
derson,KhuyagbaatarBatsuren,DieuwkeHupkes,and teroftheAssociationofComputationalLinguistics,Pro-
MonaDiab.2022. Textcharacterizationtoolkit(TCT). ceedings, May 31 - June 5, 2009, Boulder, Colorado,
InProceedingsofthe2ndConferenceoftheAsia-Pacific USA,ShortPapers,pages13–16.TheAssociationfor
ChapteroftheAssociationforComputationalLinguis- ComputationalLinguistics. 9
tics and the 12th International Joint Conference on
PengchengYinandGrahamNeubig.2017. Asyntactic
NaturalLanguageProcessing: SystemDemonstrations,
neuralmodelforgeneral-purposecodegeneration. In
pages72–87,Taipei,Taiwan.AssociationforComputa-
Proceedingsofthe55thAnnualMeetingoftheAssoci-
tionalLinguistics. 7
ationforComputationalLinguistics(Volume1: Long
RayJSolomonoff.1964. Aformaltheoryofinductive Papers),pages440–450,Vancouver,Canada.Associa-
inference.partii. Informationandcontrol, 7(2):224– tionforComputationalLinguistics. 1,4
254. 2,3
Ping Yu, Tianlu Wang, Olga Golovneva, Badr
LinfengSong,DanielGildea,YueZhang,ZhiguoWang, AlKhamissi,SiddharthVerma,ZhijingJin,GargiGhosh,
andJinsongSu.2019. Semanticneuralmachinetrans- MonaDiab,andAsliCelikyilmaz.2023. ALERT:adapt-
lationusingAMR. TransactionsoftheAssociationfor ing language models to reasoning tasks. In Proceed-
ComputationalLinguistics,7:19–31. 1,4 ingsofthe61stAnnualMeetingoftheAssociationfor
Computational Linguistics (Volume 1: Long Papers),
IevaStaliu¯naite˙ andIgnacioIacobacci.2020. Compo-
Toronto, Canada.AssociationforComputationalLin-
sitionalandlexicalsemanticsinRoBERTa,BERTand
guistics. 9
DistilBERT:AcasestudyonCoQA. InProceedings
ofthe2020ConferenceonEmpiricalMethodsinNatu- Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga,
ralLanguageProcessing(EMNLP),pages7046–7056, DongxuWang,ZifanLi,JamesMa,IreneLi,Qingning
Online.AssociationforComputationalLinguistics. 9 Yao,ShanelleRoman,ZilinZhang,andDragomirRadev.
2018. Spider: Alarge-scalehuman-labeleddatasetfor
Student. 1908. The probable error of a mean.
complexandcross-domainsemanticparsingandtext-
Biometrika,6(1):1–25. 17
to-SQLtask. InProceedingsofthe2018Conference
onEmpiricalMethodsinNaturalLanguageProcessing,
Harish Tayyar Madabushi, Edward Gow-Smith, Car-
pages3911–3921,Brussels,Belgium.Associationfor
olinaScarton,andAlineVillavicencio.2021. AStitchIn-
ComputationalLinguistics. 4
LanguageModels: Datasetandmethodsfortheexplo-
rationofidiomaticityinpre-trainedlanguagemodels.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
InFindingsoftheAssociationforComputationalLin-
Weinberger, and Yoav Artzi. 2020. Bertscore: Eval-
guistics: EMNLP2021,pages3464–3477,PuntaCana,
uatingtextgenerationwithBERT. In8thInternational
Dominican Republic. Association for Computational
ConferenceonLearningRepresentations,ICLR2020,
Linguistics. 6,16
Addis Ababa, Ethiopia, April 26-30, 2020. OpenRe-
view.net. 17
JosephTurian,Lev-ArieRatinov,andYoshuaBengio.
2010. Word representations: A simple and general
Yuan Zhang, Jason Baldridge, and Luheng He. 2019.
methodforsemi-supervisedlearning. InProceedingsof
Paws: Paraphraseadversariesfromwordscrambling. 4,
the48thAnnualMeetingoftheAssociationforCompu-
16
tationalLinguistics,pages384–394,Uppsala,Sweden.
AssociationforComputationalLinguistics. 3 DanZhao,NathanC.Frey,JosephMcDonald,Matthew
Hubbell,DavidBestor,MichaelJones,AndrewProut,
VladimirNaumovichVapnik.1995. TheNatureofSta-
Vijay Gadepally, and Siddharth Samsi. 2022. A
tisticalLearningTheory. Springer. 3
green(er) world for a.i. In 2022 IEEE International
ParallelandDistributedProcessingSymposiumWork-
JasonWei,XuezhiWang,DaleSchuurmans,Maarten
shops(IPDPSW),pages742–750. 2
Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,
and Denny Zhou. 2022. Chain of thought prompting
elicitsreasoninginlargelanguagemodels. InAdvances
inNeuralInformationProcessingSystems. 2,4,9
TomerWolfson,MorGeva,AnkitGupta,MattGardner,
Yoav Goldberg, Daniel Deutch, and Jonathan Berant.
2020. Breakitdown: Aquestionunderstandingbench-
mark. CoRR,abs/2001.11770. 1
YukWahWongandRaymondMooney.2006. Learning
for semantic parsing with statistical machine transla-
tion. In Proceedings of the Human Language Tech-
nology Conference of the NAACL, Main Conference,
pages439–446,NewYorkCity,USA.Associationfor
ComputationalLinguistics. 9
DekaiWuandPascaleFung.2009. Semanticrolesfor
SMT:Ahybridtwo-passmodel. InHumanLanguageA ExperimentalDetails ParaphraseDetection(PAWS)
BASE ParaphraseDetection: Determineifthefollowingtwosentencesare
exactparaphrases(rewrittenversionswiththesamemeaning)ofeach
other.
A.1 DataSplitDetails Sentence1:{sentence1}
Sentence2:{sentence2}
Answer[Yes/No]andthenprovideabriefexplanationofwhyyouthink
For thefivedatasetsthat weusein Section5, we thesentencesareparaphrasesornot.
Paraphrase:
compose the test sets in the following way. To AMRCOT ParaphraseDetection: Youaregiventwosentencesandtheabstract
meaningrepresentation(AMR)ofeach.
maketheexperimentalresultsinourpaperrepre- Sentence1:{sentence1}
AMR1:
sentative,weaimatcomposingalargetestsetfor {amr1}
Sentence2:{sentence2}
each task, ideally more than a size of 5,000 test AMR2:
{amr2}
samples. We sequentially check whether the test Explainwhatarethecommonalitiesanddifferencesbetweenthetwo
AMRs. Thendetermineifthetwosentencesareexactparaphrases
setislargeenough,andifnot,thenweincludethe (rewrittenversionswiththesamemeaning)ofeachotherandprovidea
briefexplanationofwhyyouthinkthesentencesareparaphrasesornot.
development set and the training set sequentially. Usethefollowingformat:Answer:[Yes/No]
Translation(WMT16)
Notethatsinceourexperimentsarezero-shot,i.e.,
BASE PleasetranslatethefollowingtextfromEnglishtoGerman.
Text:{sentence1}
wedonottrainourmodelsatall,anyoftheoriginal
Translation:
test, development, or training sets can be used to AMRCOT Youaregivenatextanditsabstractmeaningrepresentation(AMR).
Text:{sentence1}
reporttheperformanceon. AMR:
{amr1}
PleasetranslatethetextfromEnglishtoGerman.Youcanrefertothe
providedAMRifithelpsyouincreatingthetranslation.
Translation:
Asaresult,forthePAWSdataset,weuseitsentire
LogicalFallacyDetection(Logic)
test set, which is large enough with 8,000 sam- BASE Text:{sentence1}.
Youmustansweroneoptionfromthelistedcategorieswithoutadditional
ples. ForWMT16,sinceitstestsethasonly2,999 text.
AMRCOT Text:{sentence1}
samples,wealsoincludeitsdevelopmentsetwith AMR:
{amr1}
3,000samples,totaling5,999samplesforourtest. Youmustansweroneoptionfromthelistedcategorieswithoutadditional
text.
For the LOGIC dataset, as it is a relatively small
EventExtraction(Pubmed45)
dataset, we add up all its original test, develop-
BASE Thisquestionaimstoassessyourproficiencyinvalidatingrelationships
betweendifferententitiesinbiomedicaltext.Youwillbepresentedwith
ment,ortrainingsetstoobtain2,449samples. For asentencefromanarticleandaskedtodeterminewhethertheinteraction
betweentheentitiesmentionedinthesentenceisvalidornot.Youshould
Pubmed45,whichcontains25,360unsplitsamples, respondwithasingledigit,either"0"iftheinteractionisinvalid,"1"
ifitisvalid,or"2"ifswappingthepositionsofanytwoentitieswould
werandomlyselect5,000datapointsforouranaly- maketheinteractionvalid.Pleasenotethatyouarerequiredtoprovide
onlyoneofthesethreeresponses.
sis. ForSPIDER,asitstestsetisnotreleased,and Text:{sentence1}
Interaction:{interaction}
developmentsethasonly1,034samples,wealso AMRCOT Thisquestionaimstoassessyourproficiencyinvalidatingrelationships
betweendifferententitiesinbiomedicaltext.Youwillbepresentedwith
include itstraining set of7,000 samples, totaling asentencefromanarticleanditsabstractmeaningrepresentation(AMR)
andaskedtodeterminewhethertheinteractionbetweentheentities
8,034samplesforourtest. mentionedinthesentenceisvalidornot.Youshouldrespondwitha
singledigit,either"0"iftheinteractionisinvalid,"1"ifitisvalid,or"2"
ifswappingthepositionsofanytwoentitieswouldmaketheinteraction
valid.Pleasenotethatyouarerequiredtoprovideonlyoneofthesethree
responses.
A.2 EvaluationMetrics Text:{sentence1}
AMR:
{amr1}
For evaluation, we report the performance of Interaction:{interaction}
PAWS, Logic, and Pubmed45 by F1 scores, Table10:PromptsforPAWS,WMT16,Logic,andPubmed45.
the performance of machine translation on
the WMT16 dataset by BLEU scores (Pa-
A.4 ExampleDataSamples
pineni et al., 2002), and the performance
of text-to-SQL generation using the official Togetabettersenseofhowthedatasampleslook,
evaluation setup at https://github.com/ we provide some example (text, AMR) pairs in
taoyds/test-suite-sql-eval. To eval- AppendixD.4.
uate the generation quality of parser-produced
A.5 ImplementationDetails
AMRs, we report the SMATCH scores using the
evaluation codes at https://github.com/ Asfortheexperimentaldetails,fortheBERTand
snowblink14/smatch. RoBERTamodels, weuse theweightedcrossen-
tropy loss, with a batch size of 16, learning rate
of 1e-5, and dropout of 0.1, and train for five
A.3 Prompts
epochsuntilconvergence. FortheXGBoostclassi-
Welistthepromptsfor BASE and AMRCOT ofall fier(ChenandGuestrin,2016),weusethedefault
datasetsinTables10and11,aswellasthesystem hyperparameters, and set the random seed to 0,
promptsinTable12. and the class weight proportional to the class ra-Text2SQL(SPIDER) PAWS You are an NLP assistant whose purpose is to
BASE WriteanSQLquerythatretrievestherequestedinformationbasedonthe perform Paraphrase Identification. The goal of
givennaturallanguagequestion.RemembertouseproperSQLsyntax
andconsideranynecessarytablejoinsorconditions. ParaphraseIdentificationistodeterminewhether
Question:{sentence1} apairofsentenceshavethesamemeaning.
Query:
AMRCOT WriteanSQLquerythatretrievestherequestedinformationbasedonthe WMT16 YouareanNLPassistantexpertinmachinetrans-
givennaturallanguagequestionanditsabstractmeaningrepresentation lationfromEnglishtoGerman.
(AMR).RemembertouseproperSQLsyntaxandconsideranynecessary
tablejoinsorconditions. Logic You are an expert in logic whose purpose is to
Question:{sentence1} determinethetypeoflogicalfallacypresentedina
AMR:
textorcompletethetextwithoneofthefollowing
{amr1}
Query: logicalfallacies.1)FaultyGeneralization
NamedEntityRecognition(AMR-NER) 2)FalseCausality
BASE Thefollowingisanamedentityrecognitiontask. Pleaseextractall
3)CircularClaim
thenamedentitiesofthefollowingtypesfromthegivensentence.
TYPE="CARDINAL":Numeralsthatdonotfallunderanothertype, 4)AdPopulum
e.g.,“one”,“ten”TYPE="DATE":Absoluteorrelativedatesorperiods. 5)AdHominem
E.g.,“thesummerof2005”,“recentyears”TYPE="EVENT":Named
hurricanes,battles,wars,sportsevents,etc. E.g.,“Olympiadgames” 6)DeductiveFallacy
TYPE="FAC":Buildings,airports,highways,bridges,etc.E.g.,“Dis- 7)AppealtoEmotion
ney”,“theNorthPole”TYPE="GPE":Countries,cities,states.E.g.,
“HongKong”,“Putian”TYPE="LAW":Nameddocumentsmadeinto 8)FalseDilemma
laws.E.g.,“Chapter11ofthefederalBankruptcyCode”TYPE="LOC": 9)Equivocation
Non-GPElocations,mountainranges,bodiesofwater. E.g.,“Mai
10)FallacyofExtension
PoMarshes”,“Asia”TYPE="MONEY":Monetaryvalues,including
unit.E.g.,“$1.3million”,“morethan$500million”TYPE="NORP": 11)FallacyofRelevance
Nationalitiesorreligiousorpoliticalgroups. E.g.,“Chinese”,“Bud- 12)FallacyofCredibility
dhism”TYPE="ORDINAL":E.g.,"first","second",etc.TYPE="ORG":
Companies,agencies,institutions,etc. E.g.,“EighthRouteArmy”, 13)IntentionalFallacy.
“theChineseCommunistParty”TYPE="PERCENT":Percentage,in- Pubmed45 Youareamedicalprofessionalexpert.
cluding"%".E.g.,“25%”TYPE="PERSON":People,includingfic-
tional.E.g.,“ZhuDe”,“SaddamHussein”TYPE="PRODUCT":Ob- SPIDER Youarealanguagemodeldesignedtogenerate
jects,vehicles,foods,etc.(Notservices.)E.g.,“iPhone”,“CokeCola” SQLqueriesbasedonnaturallanguagequestions.
TYPE="QUANTITY":Measurements,asofweightordistance.E.g.,
Givenaquestion,youneedtogeneratethecorre-
“23sq.km”TYPE="TIME":Timessmallerthanaday.E.g.,“homecom-
ingnight”Sentence:{sentence1} spondingSQLquerythatretrievestherequested
Usejsonformatfortheresponsewhereeachkeyisanentitytype. informationfromadatabase.
AMRCOT Thefollowingisanamedentityrecognitiontask.Pleaseextractallthe
namedentitiesofthefollowingtypesfromthegivensentenceanditsab- AMR-NER You are an NLP assistant whose purpose is to
stractmeaningrepresentation(AMR).TYPE="CARDINAL":Numerals performnamedentityrecognition(NER).
thatdonotfallunderanothertype,e.g.,“one”,“ten”TYPE="DATE":
Absoluteorrelativedatesorperiods. E.g.,“thesummerof2005”,
“recentyears”TYPE="EVENT": Namedhurricanes,battles,wars, Table12:Systempromptsforalldatasets.
sportsevents,etc. E.g.,“Olympiadgames”TYPE="FAC": Build-
ings,airports,highways,bridges,etc.E.g.,“Disney”,“theNorthPole”
TYPE="GPE":Countries,cities,states.E.g.,“HongKong”,“Putian”
TYPE="LAW": Nameddocumentsmadeintolaws. E.g.,“Chapter
11ofthefederalBankruptcyCode”TYPE="LOC": Non-GPEloca-
tions,mountainranges,bodiesofwater. E.g.,“MaiPoMarshes”,
“Asia”TYPE="MONEY":Monetaryvalues,includingunit. E.g.,“$ PleasetranslatethetextfromEnglishtoGerman.
1.3million”,“morethan$500million”TYPE="NORP": National-
itiesorreligiousorpoliticalgroups. E.g.,“Chinese”,“Buddhism” YoucanrefertotheprovidedAMRifithelpsyou
TYPE="ORDINAL":E.g.,"first","second",etc.TYPE="ORG":Com-
panies,agencies,institutions,etc. E.g.,“EighthRouteArmy”,“the increatingthetranslation. Translation:”
ChineseCommunistParty”TYPE="PERCENT":Percentage,including
"%". E.g.,“25%”TYPE="PERSON": People,includingfictional.
E.g.,“ZhuDe”,“SaddamHussein”TYPE="PRODUCT": Objects,
vehicles,foods,etc. (Notservices.) E.g.,“iPhone”,“CokeCola”
TYPE="QUANTITY":Measurements,asofweightordistance.E.g., We also provide an example of ablating 100% of
“23sq.km”TYPE="TIME":Timessmallerthanaday.E.g.,“homecom-
ingnight”Sentence:{sentence1} thetext:
AMR:
{amr1}
Usejsonformatfortheresponsewhereeachkeyisanentitytype.
Table11:PromptsforSPIDERandAMR-NER.
“Youaregivenatextanditsabstractmeaningrepre-
sentation(AMR).
Text:
tio, namely setting the positive weight to be the
AMR:
inverse of the number of samples in the positive
(f/friendly-019
classdividedbythatofthenegativeclass.
:ARG1(r/relation-031
A.6 DetailsofAblationStudy :ARG0(p/person3
:name(n/name3
For text/AMR ablation experiments, we use AM-
:op1"Obama"3))
RCOT prompt with portions of text/AMR string
:ARG2(p2/person5
ablated. Anexampleofablating100%oftheAMR
:name(n2/name5
isasfollows:
:op1"Netanyahu"5)))
“Youaregivenatextanditsabstractmeaningrepre- :mod(e/exact8)
sentation(AMR). :polarity-7)
Text: The relationship between Obama and Ne- PleasetranslatethetextfromEnglishtoGerman.
tanyahuisnotexactlyfriendly. YoucanrefertotheprovidedAMRifithelpsyou
AMR: increatingthetranslation. Translation:”B DataCollection Pleaseevaluatethefollowingsentenceforthepres-
ence of slang expressions. A slang expression is
B.1 ComposingtheSlang-Involved
aphraseorexpressionthatisintheonlineslang
ParaphraseDetectionDataset
dictionariesandhasameaningthatisverydiffer-
Since our experiments need annotations for both entfromitsliteralform. Forinstance,‘rainingcats
slang paraphrase pairs and AMRs, we com- and dogs’ is slang, while ‘middle school’ is not.
pose two datasets, GoldSlang-ComposedAMR, Although‘middleschool’isacompoundphrase,it
and GoldAMR-ComposedSlang. For GoldSlang- doesnotcarryameaningbeyonditsliteralinter-
ComposedAMR, we use the curated slang para- pretation. Hereisthe sentenceforyouranalysis:
phrase pairs by Tayyar Madabushi et al. (2021), premise. Pleaseformatyourresponseasfollows:
and generate their AMRs with an off-the-shelf ‘YesorNo,slangs.’
parser(Drozdovetal.,2022). Fortheotherdataset, Ifthere’snoslangused,justanswer‘No’. Ifthere
GoldAMR-ComposedSlang, we use gold AMRs are multiple slang expressions, please separate
fromtheLDCAMR3.0corpus(Banarescuetal., themwithasemicolon(‘;’). Remember,theidioms
2013),andcomposeslangparaphrasesusingacom- weareinterestedinarethosethat,whentakenlit-
binationofhumaneffortsandassistancefromGPT- erally,wouldhaveacompletelydifferentsemantic
4. meaning.
Then we mannually check whether the extracted
Composing the GoldSlang-ComposedAMR
expressionsareslangandareappropriate. Consis-
Dataset WeadaptasubsetoftheASILM(Tay-
tentwiththespiritof(Zhangetal.,2019),weuse
yar Madabushi et al., 2021), an idiomatic MWE
thefollowingprompttoquerygpt-3.5-turbo-0613
dataset, into a paraphrase detection task. Each
togenerateoneparaphraseandonenonparaphrase
sentenceinthesubsetcontainingidiomaticexpres-
ofeachsentence.
sionsispairedwithaparaphrase(wheretheidiom
isreplacedwithitsliteralsemanticequivalent)and Rewrite the following sentence in two ways Sen-
anon-paraphrase(wheretheidiomisreplacedwith tence: sentence1. Replacing“slang”withitsin-
aphraseofsimilarsuperficialmeaningbutdiffer- tended meaning. 2. Replacing “slang” with its
ingsemanticmeaning). Thisresultsinabalanced literal meaning, such that the sentence loses its
paraphrasedetectiondatasetwithrespecttoground originalmeaning. Donotchangeanythingelse
truthlabels.
Lastly, for each pair of (origi-
nal_sentence, (non)paraphrase_sentence),
Composing the GoldAMR-ComposedSlang
we give (original_sentence, original_amr,
Dataset Apossibleerrorin AMRCOTliesinthe
(non)paraphrase_sentence)togpt-3.5-turbo-0613,
imperfection of parser-generated AMRs. To dis-
and ask it to generate (non)paraphrase_amr by
entangletheharmcausedby(1)incorrectAMRs
minimally modifying the original_amr. The
produced by the parsers and (2) poor represen-
promptisasfollows:
tation of slang expressions by AMRs, we hand-
craftedtheGoldAMR-Slang-Paradataset. Wefirst
“TheAMRofthesentence‘og_sentence’is
extract a subset from LDC-AMR3.0 (Banarescu
og_amr
etal.,2013)thatinvolveslangexpressions. Then,
WhatistheAMRofthesentence‘paraphrase’?
foreachsentence,wereplacetheslangexpression
Modified the given AMR to fit the sentence ‘hy-
with an alternative expression of the same mean-
pothesis’ and words not present in the sentence
ing,andasemanticallydifferentexpressionwhich
’hypothesis’shouldnotappearinyourAMR.
seemsliterallysimilar,thuscreatingaparaphrase
Startyouresponsewith‘(’.”
andnon-paraphrasesentence,respectively. Thecor-
respondingAMRscanbederivedfromtheoriginal B.2 IntuitionofWhyAMRMightBeHelpful
LDC-AMR3.0AMRswithminimalmodifications. forNER
Specifically,weoperationalizetheprocessasfol- For some intuition of why we choose the NER
lows. We first use gpt-3.5-turbo-0613 to identify task out of the OntoNotes 5.0 dataset, it has al-
500 samples of slang usage from LDC-AMR3.0 readybeenshowninexistingworkthatAMRcan
withthefollowingprompt: help event extraction (Garg et al., 2015; Huangetal.,2018),whichisalsoatypeofnamedentities. Dataset BASE ∆AMRCOT Sig.(p)
Specifically,thegraphicalstructureandtypedtags PAWS 78.25 -3.04 ✓(5.209e-8)
WMT 27.52 -0.83 ✗(0.0716)
of AMR make it easy to identify named entities. Logic 55.61 -0.49 ✗(0.7300)
Forinstance,inthesentence“thetopmoneyfunds Pubmed45 39.65 -3.87 ✓(0.0309)
SPIDER 43.78 +0.61 ✗(0.4362)
arecurrentlyyieldingwellover9%”intheAMR-
GoldSlang-ComposedAMR 86.83 -6.63 ✓(0.0014)
NER dataset, we discover the AMR substructure GoldAMR-ComposedSlang 77.69 -8.78 ✗(0.1309)
“(p/percentage-entity10:value9),”whichmakes AMR-NER(GoldAMR) 60.51 +0.03 ✗(0.9935)
AMR-NER(ParserAMR) 60.51 +1.91 ✗(0.6227)
iteasytoidentify“9%”asanamedentityoftype
percent. Table13:ForalltheexperimentscomparingBASEandAMR-
COTusingGPT-4mentionedinthemaintext,wecalculate
whetherthedifference∆AMRCOTisstatisticallysignificant
C ExplanationofLinguisticFeatures
(Sig.)usingt-test(Student,1908)bythethresholdp=0.05,
andreporttheactualpvalues.
In our analysis, we delve into specific linguistic
featuresthatexhibitstrongcorrelationswithAMR BERTScore spBLEU
Model
helpfulness,asdetailedinTables6and7. BASE ∆AMRCOT BASE ∆AMRCOT
text-d.-001 90.48 -1.09 30.29 -4.19
text-d.-002 91.00 -0.30 33.14 -1.67
text-d.-003 91.37 -0.25 34.75 -1.55
Number of Adjuncts This feature involves
GPT-3.5 91.70 -0.06 37.09 -0.43
counting words that serve as modifiers to nouns, GPT-4 91.79 -0.08 37.71 -0.72
pronouns, verbs, and other parts of speech. Ad-
Table 14: Performance on WMT by additional metrics,
junctstypicallyprovideadditionalcontextorem- BERTScoreandspBLEU.
phasisbutcanbeomittedwithoutalteringthecore
meaning of the sentence. For example, in “John
reallylikesapples,”theword“really”isanadjunct, D.2 AdditionalEvaluationResultsfor
modifyingtheverb“likes.” MachineTranslation
In the main paper, we mainly report the perfor-
WordComplexity Weassesswordcomplexity mance of machine translation using the standard
usingtheageofacquisitionmetric,followingthe evaluation metric BLEU (Papineni et al., 2002).
methodologyofKupermanetal.(2012). Recent studies has proposed new metrics to eval-
uate the quality of machine translation, such as
BERTScore (Zhang et al., 2020) and spBLEU
NumberofQuantifierPhraseModifiers This
(Goyal et al., 2022), so we also report the model
feature quantifies the modifiers within quantifier
performanceaccordingtothesetwoadditionalmet-
phrases that adjust the head, or primary element,
rics in Table 14. We use the version of spBLEU
of the phrase. An illustration of this can be seen built from the Flores-200 dataset. 3 The perfor-
in the sentence “About 5000 people attended the
mance trend is consistent with Section 4, where
conference,” where “about” modifies the quanti-
AMR has a marginal effect on the baseline LLM
fier “5000.” This concept is further explained by
performance.
deMarneffeandManning(2008).
D.3 Larger-ScaleAblationStudyUsingWMT
D AdditionalExperiments
WeunderstandthattheablationstudyinSection6.2
D.1 StatisticalSignificanceTests
isonasmallscale(inordertousethegoldanno-
Weconductstatisticalsignificancetestsfortheex- tated data). As an alternative tradeoff to regress
perimentsinthemainpapercomparing BASE and abitonthedataquality,butaimatalargerscale,
AMRCOT,includingTables3,5and9. Usingthe wealsoconductthesameablationstudyon1,000
Student’st-test(Student,1908),wereportthesig- randomsamplesfromtheWMTdatasetusingpre-
nificancetestresultsinTable13. Theresultsecho dictedAMRs. OurresultsinFigure5alsoconfirm
our earlier observation that the changes by AM- thattexthasamoreinstrumentalroleforLLMs.
RCOT is mostly small-scale fluctuations, as the
differences are statistically insignificant in most
3https://github.com/facebookresearch/
cases. flores/tree/main/flores20040
30
20
ParaphraseDetection(PAWS)
10 Text Text Sentence1:Thedefendantthenbrokeintothehouseandtriedunsuc-
AMR cessfullytoremoveandopenthevault.
Sentence2:Thedefendantthenbrokeintothehouseandtriedunsuc-
0 cessfullytoopenthesafeandthentoremovethem.
0 20 40 60 80 100
AMR AMR1:(a/and7:op1(b/break-023:ARG0(d/defendant1):ARG1
(h/house6)):op2(t2/try-018:ARG0d:ARG1(a2/and12:op1(r
AMR/TextTokens(%)KeptinthePrompt
/remove-0111:ARG0d:ARG1(v/vault15)):op2(o/open-0113
:ARG0d:ARG1v)):ARG1-of(s/succeed-019:ARG0d:polarity-9))
:time(t/then2))
Figure5:AblationstudiesofAMRandtextrepresentations
AMR2:(a/and7:op1(b/break-023:ARG0(d/defendant1):ARG1
intheprompton1,000randomsamplesoftheWMTdataset (h/house6)):op2(t2/try-018:ARG0d:ARG0-of(s2/succeed-019
usingGPT-4. StartingfromtheAMRCOTpromptwiththe :polarity-11):ARG1(a2/and14:op1(o/open-0111:ARG0d:ARG1
(s/safe13)):op2(r/remove-0117:ARG0d:ARG1s))):time(t/
complete text and AMR, we randomly drop out a certain then2))
portionoftokensinthetext/AMR,andseetheeffectonthe Translation(WMT16)
taskperformance. Text ObamareceivesNetanyahu
AMR (r/receive-011:ARG0(p/person0:name(n/name0:op1"Obama"0))
:ARG1(p2/person2:name(n2/name2)):rel(e/Netanyahu2))
LogicalFallacyDetection(Logic)
Text Onmywalktoworkthismorning,awomanonherbikenearlyranme
D.4 Few-ShotExperiments
offthesidewalk.Ihadn’trealizedthatcyclistsweresoaggressiveand
rude!
In addition to the main results of the overall ef- AMR (m2/multi-sentence19:snt1(n/near-0213:ARG2(r3/run-1014
:ARG0(b/bike12:poss(w2/woman9)):ARG1(i/i15):ARG2(s
fectofAMRinSection4usingzero-shotprompt- /sidewalk18):time(w/walk-012:ARG0i:ARG2(w3/work-014
:ARG0i):time(d/date-entity6:dayperiod(m/morning6):mod(t/
ing, we also check whether adding few-shot ex- today5))))):snt2(r/realize-0123:ARG0(i2/i20):ARG1(h/have-
degree-9127:ARG1(p/person25:ARG0-of(c/cycle-0125)):ARG2
amples to the prompt will help. Since the ex- (a2/and29:op1(a/aggressive28):op2(r2/rude-0130:ARG1p))
:ARG3(s2/so27)):polarity-22))
periments are very costly, we conduct a small-
EventExtraction(Pubmed45)
scale preliminary check running few-shot AMR- Text E anx dam S7i 2n 9at mio un tao tf ioa nc sti hv aa dte nd op eh ffo es cp th oo n- MM EE KK al ce tv ie vl as tir oe nv ie na dle ud cet dha bt yt th he eF hB igm
h-
COT on 200 random samples from PAWS using activityV600EB-Rafprotein;however,theFBmandS729Amutations
increasedanddecreased,respectively,theabilitiesoftheintermediate
gpt-3.5-turbo-0613. AsAMRsarelengthy, G466Aandkinase-impairedD594GB-RafproteinstoactivateMEK
(Fig.4B),indicatingacorrelationbetweenthetransformationpotential
andPAWSisabinaryclassificationtask,weselect oftheseproteinsandtheirabilitytoactivateERKcascadesignalingin
vivo.
one random example with the positive label, and AMR (c3/contrast-0135:ARG1(r/reveal-017:ARG0(e4/examine-010
:ARG1(l/level6:ARG1-of(a/activate-012):mod(e/enzyme5:name
another with the negative label, totaling an aver- (n/name5:op1"MEK"5):ARG1-of(p/phosphorylate-013)))):ARG1
(a6/affect-0117:ARG0(m/mutate-0114):ARG1(a2/activate-0120
age prompt length of 371 tokens. The resulting :ARG1(p3/protein19:name(n5/name19:op1"MEK"19)):ARG1-of
(i4/induce-0121:ARG0(p4/protein33:name(n2/name5:op1
F1 score is 63.67, close to BASE performance of "FBm"10:op1"V"27:op260028:op3"B-Raf"30):ARG0-of(a5/
activity-0626:ARG1-of(h/high-0224))))):polarity-16):ARG1-of
63.92,servingasapreliminaryobservationthatthe (d2/describe-0176:ARG0(f/figure73:ARG0-of(i3/indicate-0178
:ARG1(c4/correlate-0180:ARG1(p2/potential84:domain(t2/
few-shotsettingmightnotchangeourobservations transform-0183:ARG0(p7/protein87:mod(t/this86)))):ARG2
(c2/capable-0190:ARG1p7:ARG2(a4/activate-0192:ARG0p7
much. :ARG1(s/signal-0795:ARG0(e3/enzyme93:name(n7/name93
:op1"ERK"93:op2"cascade"94))):manner(v/vivo97))))):mod
4B75))):ARG2(a7/and45:op1(i2/increase-0144:ARG0(m2/
mutate-0143:ARG1(e2/enzyme19:namen2)):ARG1(c/capable-
0151:ARG1(a8/and58:op1m2:op1(p5/protein68:namen2):op2
(m3/mutate-0143:ARG1p3):op2(p6/protein68:name(n6/name68
:op1"G"55:op246656:op3"A"57):ARG2-of(i/impair-0161:ARG1
(k/kinase59)))):ARG2(a3/activate-0170:ARG0a8:ARG1e2)))
:op2(d/decrease-0146:ARG0m3:ARG1c)):rel(n3/name5:op1
"D"62:op2"594"63):rel(n4/name12:op1"S"12:op2"729"13):rel
(f2/figure73):rel(i5/intermediate54))
Text2SQL(SPIDER)
Text Listthenumberofallmatcheswhoplayedinyearsof2013or2016.
AMR (l/list-010:ARG0(y/you0):ARG1(n/number2:quant-of(m/
match-035:ARG0-of(p/play-017:time(o/or12:op1(d/date-
entity11:year201311):op2(d2/date-entity13:year201613))):mod
(a/all4))):modeimperative0)
NamedEntityRecognition(AMR-NER)
Text Then,intheguests’honor,thespeedwayhauledoutfourdrivers,crews
andeventheofficialIndianapolis500announcerfora10-lapexhibition
race.
AMR (h/haul-0110:purpose(h2/honor-016:ARG1(g/guest4)):purpose
(r/race-0229:ARG1-of(e3/exhibit-0128):ARG3(l/lap27:quant
1025)):ARG0(s/speedway9):ARG1(a/and14:op1(p/person13
:quant412:ARG0-of(d/drive-0113)):op2(c/crew15):op3(p2/
person15:ARG0-of(a2/announce-0122:ARG1(e2/event21:name
(n/name20:op1"Indianapolis"20:op250021))):mod(e/even17)
:mod(o/official19))):direction(o2/out11):time(t/then0))
Table15:Exampletext-AMRpairsforeachtask.
)UELB(ecnamrofrePksaT