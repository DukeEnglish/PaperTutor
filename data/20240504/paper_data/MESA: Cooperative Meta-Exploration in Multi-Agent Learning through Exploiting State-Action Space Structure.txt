MESA: Cooperative Meta-Exploration in Multi-Agent Learning
through Exploiting State-Action Space Structure
ZhichengZhang∗ YanchengLiang∗
CarnegieMellonUniversity UniversityofWashington
Pittsburgh,Pennsylvania,UnitedStates Seattle,Washington,UnitedStates
zhichen3@cs.cmu.edu yancheng@cs.washington.edu
Yi Wu FeiFang
TsinghuaUniversity CarnegieMellonUniversity
Beijing,China Pittsburgh,Pennsylvania,UnitedStates
jxwuyi@gmail.com feif@cs.cmu.edu
ABSTRACT
Multi-agentreinforcementlearning(MARL)algorithmsoftenstrug-
gletofindstrategiesclosetoParetooptimalNashEquilibrium,ow-
inglargelytothelackofefficientexploration.Theproblemisex-
acerbatedinsparse-rewardsettings,causedbythelargervariance
exhibitedinpolicylearning.ThispaperintroducesMESA,anovel
meta-explorationmethodforcooperativemulti-agentlearning.It
learns toexplore by first identifying the agents’ high-rewarding
jointstate-actionsubspacefromtrainingtasksandthenlearning
asetofdiverseexplorationpoliciesto“cover”thesubspace.These Figure1:Illustrationofstructuredexplorationandunstruc-
trainedexplorationpoliciescanbeintegratedwithanyoff-policy tured exploration behavior in the 2-player climb game.
MARL algorithm for test-time tasks. We first showcase MESA’s The rows and columns indicate the players’ action space.
advantageinamulti-stepmatrixgame.Furthermore,experiments While unstructured exploration aims to visit novel states,
showthatwithlearnedexplorationpolicies,MESAachievessignif- structuredexplorationexploitsstructuresinthejointstate-
icantlybetterperformanceinsparse-rewardtasksinseveralmulti- action space, helping agents coordinatedly and more effi-
agentparticleenvironmentsandmulti-agentMuJoCoenvironments, cientlyexplorethepotentialhigh-rewardsubspace.
andexhibitstheabilitytogeneralizetomorechallengingtasksat
testtime.
KEYWORDS
Theexplorationchallengehasbeenstudiedextensivelyandex-
Multi-AgentReinforcementLearning;Meta-Learning;Exploration istingworkscanbecategorizedmainlyintotwostreams.Onecore
Strategy ideawithgreatsuccessistoincentivizetheagenttovisittheunder-
exploredstatesmorefrequentlybyaddinganintrinsicrewardbased
ACMReferenceFormat:
onavisitationmeasure[3,25,28,37]orsomeotherheuristics[17,
ZhichengZhang,YanchengLiang,YiWu,andFeiFang.2024.MESA:Co-
operative Meta-ExplorationinMulti-AgentLearningthroughExploiting 39].
State-ActionSpaceStructure.InProc.ofthe23rdInternationalConference However,inmulti-agentsettings,duetotheexponentialgrowth
onAutonomousAgentsandMultiagentSystems(AAMAS2024),Auckland, ofthe joint state-actionspace, simply visiting more novel states
NewZealand,May6–10,2024,IFAAMAS,15pages. canbeincreasinglyineffective.Explorationpoliciesneedtobetter
capture the low-dimensional structureof the tasks and leverage
1 INTRODUCTION
thestructuralknowledgeforhigherexplorationefficiency.
Reinforcement learning (RL) algorithms often adopt a trial-and- Anotherlineofworkspecificallylearnsexplorationstrategies.
errorlearningparadigmandoptimizethepolicybasedonthere- However, these works do not explicitly consider the underlying
ward signals given by the environment. The effectiveness of RL taskstructure.Forexample,Mahajanetal.conditionsthepolicyon
reliesonefficientexploration,especiallyinsparserewardsettings, asharedlatentvariable[24]learnedviamutualinformationmaxi-
asitiscriticaltogetsufficientexperienceswithhighrewardsto mization.Liuetal.adoptsagoal-conditionedexplorationstrategy
guidethetraining. bysettingstatefeaturesasgoals[21].Otherworksinthesingle-
agentsettings[6,26,35]learnexplorationpoliciesthroughapre-
∗Equalcontribution.
definedintrinsicreward.Alltheseworkstraintheexplorationpol-
Proc.ofthe23rdInternationalConferenceonAutonomousAgentsandMultiagentSys- icyusingtask-agnosticexploration-specificrewards.InSection4,
tems(AAMAS2024),N.Alechina,V.Dignum,M.Dastani,J.S.Sichman(eds.),May6– wewillpresentasimplematrixgametoshowthatpopularexplo-
10,2024,Auckland,NewZealand.©2024InternationalFoundationforAutonomous
rationmethodscanhavedifficultiesfindingtheoptimalsolution
AgentsandMultiagentSystems(www.ifaamas.org).Thisworkislicencedunderthe
CreativeCommonsAttribution4.0International(CC-BY4.0)licence. duetotherewardstructureofthegame.
4202
yaM
1
]GL.sc[
1v20900.5042:viXraHow canwe enable theagents to moreeffectively exploreby The key idea is to meta-learn a separate exploration policythat
leveragingtheintrinsicstructureoftheenvironment?Weadopta canbeusedinthetestingtask.Mostcloselyrelatedtoourworkis
meta-explorationframework(i.e.,learningtoexplore)forMARL: [26],whereanexplorationpolicyispretrainedonasetoftraining
wefirsttrainmultiplestructuredexplorationpoliciesfromasetof tasks.However,theirmethodisdesignedforthesingle-agentset-
trainingtasks(referredtoasthemeta-trainingstage),andthenuse tingandlearnstheexplorationpolicybyusingatask-agnosticin-
these exploration policies to facilitate agents’ learning in a test- trinsicrewardtoincentivizevisitationofinterestingstates,while
timetask,whichistypicallyanewtasksampledfromthetaskdis- we directly utilize the task reward to learn the structure of the
tribution(referred toasmeta-testingstage). Wedevelop amulti- environments.Otherexistingworksinmeta-explorationpropose
agent meta-explorationmethod,CooperativeMeta-Exploration in to learn a latent-space exploration policythat is conditioned on
Multi-AgentLearningthroughExploitingState-ActionSpaceStruc- ataskvariable,whichcanbeaccomplishedbymeta-policygradi-
ture (MESA) for fully cooperative settings. MESA leverages the ent[14,20,40],variationalinference[32]orinformationmaximiza-
taskstructuresbyexplicitlyidentifyingtheagents’high-rewarding tion[42]overthetrainingtasks.Therefore,attesttime,posterior
joint state-action subspace in the training tasks. It then trains a inferencecanbeperformedforthelatentvariabletowardsfastex-
setofdiverseexplorationpoliciestocoverthisidentifiedsubspace. plorationstrategyadaption.Ourapproachfollowsasimilarmeta-
Theexplorationpoliciesaretrainedwitharewardschemeinduced explorationparadigmbylearningadditionalexplorationpolicies.
bythedistancetothehigh-rewardingsubspace.Themeta-learned However,existingmeta-explorationmethodsfocusonthesingle-
explorationpoliciescanbecombinedwithanyoff-policyMARLal- agentsettingwhileweconsidermuchmorechallengingmulti-agent
gorithmduringthemeta-testingstagebyrandomlyselectinglearned gameswithadistributionofsimilarly-structuredtasks,forexam-
explorationpoliciestocollectvaluableexperiences.Suchstructured ple,theMPEenvironment[23]withadistributionoftargetland-
exploration can help the agents to learn good joint policies effi- marks that theagents need toreach. In addition, we meta-learn
ciently(Figure1).WeempiricallyshowthesuccessofMESAonthe adiscretesetofexplorationpoliciesthroughaniterativeprocess,
matrixclimbgameanditshardermulti-stagevariant.Inaddition, whichresultsinamuchsimpler meta-testing phase withoutthe
weevaluateMESAintwocontinuouscontroltasks,i.e.,theMPE need for posterior sampling or gradient updates on exploration
environment [23] and the multi-agent MuJoCo benchmark [29]. policies.Besides,someothermethodspretrainexplorationpolicies
WedemonstratethesuperiorperformanceofMESAcomparedto fromanofflinedataset[7,31,36],whichisbeyondthescopeofthis
existingmulti-agentlearningandexplorationalgorithms.Further- paper.
more, we show that MESA is capable of generalizing to unseen Finally,ourapproachlargelydiffersfromthesetting ofmulti-
test-timetasksthataremorechallengingthananyofthetraining tasklearning[1,2,11,16,27],whicharecommonlyevaluatedin
tasks. environments withheterogeneous tasksorscenarios. Ourexplo-
rationpoliciesarenottrainedtoachievehighreturnsinthetrain-
ingtasks.Instead,theyaretrainedtoreachasmanyhigh-reward
2 RELATEDWORK
state-actionpairsaspossiblecollectedinadiversesetoftasks.There-
Explorationhasbeenalong-standingchallengeinRLwithremark- fore,thestate-actionpairscoveredbyasingleexplorationpolicy
ableprogressachievedinthesingle-agentsetting[3,5,10,25,28, areverylikelytobedistributedacrossdifferenttrainingtasks.
34,37].Mostoftheseworksmaintain pseudo-countsover states
and construct intrinsic rewards to encourage the agents to visit
3 PRELIMINARIES
rarelyvisited statesmorefrequently[3,25,28,37].Thesecount-
basedmethodshavebeenextendedtothemulti-agentsettingbyin- Dec-POMDP. We consider fully-cooperative Markov games de-
centivizingintra-agentinteractionsorsocialinfluence[17–19,39]. scribed by a decentralized partially observable Markov decision
However,inthemulti-agentsetting,asimplecount-basedmethod process(Dec-POMDP),whichisdefinedbyhS,A,𝑃,𝑅,Ω,O,𝑛,𝛾i.
canbelesseffectiveduetothepartialobservabilityofeachagent, Sisthestatespace.A≡A1×...×A𝑛isthejointactionspace.The
anexponentiallylargejointstate-actionspace,andtheexistenceof dynamicsisdefinedbythetransitionfunction𝑃(𝑠′ |𝑠,𝒂).Agents
multiplenon-Pareto-optimalNE.Therefore,recentworksfocuson sharearewardfunction𝑅(𝑠,𝒂),and𝛾 ∈ (0,1)isthediscountfactor.
discovering thestructuresofpossiblemulti-agent behaviors.For Ω ≡ Ω 1×..×Ω 𝑛 isthejointobservationspace,whereΩ 𝑖 isthe
example,[24]adoptsvariationalinference tolearningstructured observationspaceforagent𝑖.Ateachtimestep,eachagent𝑖 only
latent-space-policies;[15]generatessimilartaskswithsimplerre- hasaccesstoitsownobservation𝑜𝑖 ∈ Ω 𝑖 definedbythefunction
wardfunctionstopromotecooperation;[21]learnstoselectasub- O:S×A↦→Ω.ThegoalofagentsinDec-POMDPistomaximize
setofstatedimensionsforefficientexploration.Wefollowameta- thecommonexpecteddiscountedreturnunderthejointpolicy𝝅:
learningframeworkandlearnstructuredexplorationstrategiesby J(𝝅)=E 𝝅 𝑡𝛾𝑡𝑅(𝑠𝑡,𝒂 𝑡) .
exploitinghigh-rewardingsubspaceinthejointstate-actionspace. LearningtoExplore.Meta-RLassumesataskdistribution𝑝(T)
(cid:2)Í (cid:3)
Our methodalsoleverages acount-basedtechniqueasasubrou- overtasks,andanagent aimstolearntoquicklyadapttoatest-
tineduringthemeta-training phasetoprevent over-exploitation timetaskTtestdrawnfrom𝑝(T)aftertraininginabatchoftrain-
andmodecollapse. ing tasks {T𝑖 | T𝑖 ∼ 𝑝(T)} 𝑖𝐵 =1. Inspired by the explicit explo-
Meta reinforcement learning (meta-RL) is a popular RL para- ration methods [6, 42], we adopt a meta-exploration framework
digmthatfocusesontrainingapolicythatcanquicklyadapton for MARL: we learn joint exploration policies 𝝅 𝑒 from training
an unseen task at test time [9, 12, 14, 20, 32, 40, 42, 44]. Such a tasks {T𝑖 | T𝑖 ∼ 𝑝(T)} 𝑖𝐵 =1 and use𝝅 𝑒 tocollectexperiences for
paradigmhasbeenextendedtothesettingoflearningtoexplore. thetrainingoftheagents’policyprofile𝝅intaskTtest,denotedas𝝅(𝝅 𝑒,Ttest).Formally,theobjectiveofmeta-explorationis thecommonly-usedmulti-layerperceptron,andtherearealsothe-
oreticalresults[8]analyzingneuralnetworkswithquadraticacti-
m 𝝅a 𝑒xE Ttest∼𝑝(T) "E 𝝅(𝝅𝑒,Ttest)
" 𝑡
𝛾𝑡𝑅𝑖(𝑠𝑡,𝒂 𝑡) ##. (1) v coat ei ffion c. ieF no tr st mh ae kc eli tm heb jog ia nm t𝑄e, fi ut nis cte ia os ny suto ffiv ce ieri nfy tlyth ea xt prth ese siq vu ea td or pat ei rc
-
Õ
fectlyfittherewardfunctionbysettingWtobetherewardmatrix.
NashEquilibriumandParetoOptimality.Ajointpolicy𝝅
Therefore,thelearningprocessof𝑄ismainlyaffectedbyhowthe
isanNEifeachagent’spolicy𝜋𝑖 isabestresponsetotheother
explorationpolicysamplesthedata.
𝜋ag 𝑖′e ,n wt es’ hp ao vl eic 𝑄ie 𝑖s (𝝅𝝅 )−𝑖 ≥.T 𝑄h 𝑖a (𝜋t 𝑖′i ,s, 𝝅f −o 𝑖r ),a wny hea rg ee 𝑄nt 𝑖𝑖 i’ ss ta hl ete vr an la ut eiv fe unp co tl ii oc ny Consideranexplorationpolic (y 𝑡)𝑝 𝑒(𝑡) thatselectsjointaction𝒂=
foragent𝑖.Ajointpolicy𝝅isParetooptimaliftheredoesnotexist (𝑖,𝑗)atstep𝑡withprobability𝑝 𝑒 (𝑖,𝑗).Theefficiencyofanexplo-
analternativejointpolicy𝝅′ suchthat∀𝑖, 𝑄𝑖(𝝅′) ≥ 𝑄𝑖(𝝅) and rationpolicycanbemeasuredbytherequirednumberofstepsfor
∃𝑖, 𝑄𝑖(𝝅′) >𝑄𝑖(𝝅). learninganequivalentlyoptimal𝑄 functionusingthemaximum
(𝑡)
likelihoodestimatoroverthedatasampledfrom𝑝 .Thelearning
𝑒
4 AMOTIVATINGEXAMPLE:CLIMBGAME objectiveincludesboththeprior𝑝(W)andthelikelihoodofpredic-
WeanalyzeafullycooperativematrixgameknownasClimbGame.
tionerror𝑝(𝐸𝑖𝑗),wherethepredictionerror𝐸𝑖𝑗 =𝑞(e𝑖,e𝑗;·)−𝑅𝑖𝑗.
IfthepredictionerrorisassumedtobedepictedbyaGaussiandis-
In Section 4.1, we show how popular exploration strategies, in-
cludingunstructuredstrategieslikeuniformexplorationandtask-
tribution𝑝(𝐸𝑖𝑗) =N(𝐸𝑖𝑗;0,𝜎 𝑒2)foreveryvisitedjointaction(𝑖,𝑗),
thenthelearningobjectiveforthe𝑄 functioncanbeformulated
specificstrategieslike𝜖−greedy,failtoefficientlyexploretheclimb
as:
game.Bycontrast,weshowinSection4.2thatasimplestructured
explorationstrategycansubstantiallyimprovetheexplorationef-
J(𝑇)(W,b,c,𝑑)
ficiency. 𝑇
A𝑖A =c {li 0m ,.b ..g ,a 𝑈m −e 1𝐺 }𝑓 fo( r𝑛 a,𝑢 n, y𝑈 p) lai ys ea r𝑛 𝑖.- Tp hla ey re er wg aa rm de ofw ait joh inac tt ai co tn ios np 𝒂ac ∈e =E {(𝑖(𝑡),𝑗(𝑡))∼𝑝𝑒(𝑡)}𝑇 𝑡=1log 𝑝(W)
𝑡
Ö′=1𝑝(𝐸 𝑖(𝑡)𝑗(𝑡))
!
A isdeterminedbythenumberofplayersperformingaspecific 𝑇
action𝑢(denotedas#𝑢),whichis = E (𝑖,𝑗)∼𝑝𝑒(𝑡) logN(𝑞(e𝑖,e𝑗;W,b,c,𝑑)−𝑅𝑖𝑗;0,𝜎 𝑒2)
𝑡=1
1, if#𝑢 =𝑛, +Õ logN(W;0,𝜎2(cid:2)
𝐼)+Const.
(cid:3)
(4)
𝑅(𝒂) = 1−𝛿 (0<𝛿 <1), if#𝑢 =0, . (2) 𝑤
0, otherwise. thaW tme au xse im𝑞 iJ ze(𝑇 s) J(W (𝑇, )b, ac t, s𝑑 t) epto 𝑇d .e 𝑞n Jo (t 𝑇e )t (h We ,l bea ,cr ,n 𝑑e )d ij so din et te𝑄 rmfu inn ec dtio bn
y
4.1 ExplorationChallenge theexplorationpolicy𝑝(𝑡) andtheexplorationsteps𝑇.Thenwe
 𝑒
havethefollowingtheoremfortheuniformexplorationstrategy.
Aclimbgame𝐺 (𝑛,𝑢,𝑈)hasthreegroupsofNE:theParetoopti-
𝑓
malNE(𝑢,𝑢,...,𝑢),thesub-optimalNEs{(𝑎1,𝑎2,...,𝑎𝑛) |∀𝑖,𝑎𝑖 ≠ Theorem4.2(uniformexploration). Assume𝛿 ≤ 1 6,𝑈 ≥3.Using
𝑢},andthezero-rewardNEs{(𝑎1,𝑎2,...,𝑎𝑛) | 1 < #𝑢 < 𝑛}.The auniformexplorationpolicyintheclimbgame𝐺 𝑓(2,0,𝑈),itcanbe
sheer difference in the size ofthethree subsets ofNEsmakes it provedthat𝑞 J(𝑇)(W,b,c,𝑑)willbecomeequivalentlyoptimalonly
particularlychallengingforRLagentstolearntheoptimalpolicy after𝑇 = Ω(|A|𝛿−1) steps.When𝛿 = 1,𝑇 =𝑂(1) stepssufficeto
profilewithoutsufficientexploration,asevidencedbythetheoret- learntheequivalentlyoptimaljointQfunction,suggestingtheinef-
icalanalysisbelowandempiricalevaluationinSection6. ficiencyofuniformexplorationisduetoalargesetofsub-optimal
Considera2-agentclimbgame𝐺 𝑓(2,0,𝑈).Ajointaction𝒂can NEs.
berepresentedbyapairofone-hotvectors[e𝑖,e𝑗] ∈{0,1}2𝑈.Let
TheintuitionbehindTheorem4.2isthatthehardnessofexplo-
𝑞(x,y;𝜃)beajointQfunctionparameterizedby𝜃thattakesinput
rationinclimbgameslargelycomesfromthesparsityofsolutions:
x,y ∈ {0,1}𝑈 and is learned to approximate the reward of the
asetofsub-optimalNEsexistbutthereisonlyasingleParetoop-
game.WehopethejointQfunctionhasthesameoptimalpolicy
timalNE.Learningthejoint𝑄 functioncanbeinfluencedbythe
profile.
sub-optimalNEs.Andiftheexplorationattemptsarenotwellco-
Definition4.1. Wecallajoint𝑄 function𝑞(x,y;𝜃) equivalently ordinated,alotofzeroreward wouldbeencountered,making it
optimalwhen𝑞(e0,e0;𝜃) = max0≤𝑖,𝑗<𝑈𝑞(e𝑖,e𝑗;𝜃).Whenajoint hardtofindtheParetooptimalNE.Wealsoremarkthatuniform
𝑄 functionisequivalentlyoptimal,onecanuseittofindtheopti- explorationcanbeparticularlyinefficientsincetheterm|A| can
malpolicy. beexponentiallylargeinamulti-agentsystem.Thisindicatesthat
moreefficientexplorationcanpotentiallybeachievedbyreducing
Sinceneuralnetworksaredifficulttoanalyzeingeneral[4],we thesearchspaceandidentifyingasmaller“critical”subspace.
parameterizethejoint𝑄 functioninaquadraticform: ToformallyproveTheorem4.2,wedefine 𝑓1,𝑓2,𝑓3 asthestep-
𝑞(x,y;W,b,c,𝑑)=x⊤Wy+b⊤x+c⊤y+𝑑 (3) averagedprobabilityoftakingthejointactioninoptimalNE,sub-
optimalNEandzero-reward,respectively.Weshowthattomake
AGaussianprior𝑝(W) = N(W;0,𝜎2𝐼) isintroducedunderthe thejoint𝑄functionequivalentlyoptimal,thereisanecessarycon-
𝑤
assumptionthatanon-linearWisharderandslowertolearn.Qua- ditionthat𝑓1,𝑓2,𝑓3shouldfollow.When𝑇 isnotlargeenough,this
draticfunctionshavebeenusedinRL[13,38]asareplacementfor conditioncannotbesatisfied.DetailedproofisinAppendixA.2.Figure2:MESA’smeta-learningframework.Inthemeta-trainingstage,MESAlearnsexplorationpoliciestocoverthehigh-
rewardingsubspace.Inthemeta-testingstage,MESAusesthelearnedexplorationpoliciestoassistthelearninginanunseen
task.Eachcolorcorrespondstoadifferenttask,andthecoloredpointsrepresentthehigh-rewardingjointstate-actionpairs
collectedinthattask.
Next,weconsiderthecaseofanotherpopularexplorationpar- Theorem4.4showstheefficiencyofexplorationcanbegreatly
adigm,𝜖-greedyexploration. improvediftheexplorationstrategycapturesaproperstructureof
theproblem,i.e.,allagentstakingthesameaction.Wefurtherre-
Theorem4.3(𝜖-greedyexploration). Assume𝛿 ≤ 1,𝑈 ≥4,𝑈 ≥ markthatbyconsideringasetofsimilarclimbgamesG,whereG=
32
𝜎𝑤𝜎 𝑒−1.Intheclimbgame𝐺 𝑓(2,0,𝑈),under𝜖-greedyexploration {𝐺 𝑓(2,𝑢,𝑈)} 𝑢𝑈 =− 01,thestructuredexplorationstrategy𝑝 𝑒(𝑡) (𝑖,𝑗) =
withfixed𝜖 ≤ 1 2,𝑞 J(𝑇)(W,b,c,𝑑)willbecomeequivalentlyoptimal 𝑈−1 1 𝑖=𝑗 canbeinterpretedasauniformdistributionoverthe
onlyafter𝑇 = Ω(|A|𝛿−1𝜖−1) steps.If𝜖(𝑡) = 1/𝑡,itrequires𝑇 = optimalpoliciesofthisgamesetG.Thisinterestingfactsuggests
exp Ω |A|𝛿−1 explorationstepstobeequivalentlyoptimal. thatw(cid:2) ecan(cid:3) firstcollectasetofsimilarlystructuredgamesandthen
derive effective exploration strategies from these similar games.
(cid:0) (cid:0) (cid:1)(cid:1)
TheproofissimilartothatofTheorem4.2(detailedinAppendix Onceasetofstructuredexplorationstrategiesarecollected,wecan
A.3). By comparing 4.2 and 4.3,𝜖-greedy results in even poorer furtheradoptthemforfastlearninginanovelgamewithasimi-
explorationefficiencythanuniformexploration.Notethe𝜖-greedy larproblemstructure.Wetaketheinspirationhereanddevelopa
strategy is training policy specific, i.e., the exploration behavior generalmeta-explorationalgorithminthenextsection.
varies asthe training policychanges. Theorem4.3 suggests that
whenthepolicyissub-optimal,theinduced𝜖-greedyexploration 5 METHOD
strategycanbeevenworsethanuniformexploration.Hence,itcan
WedetailourmethodCooperativeMeta-ExplorationinMulti-Agent
bebeneficialtoadoptaseparateexplorationindependentfromthe
Learning through Exploiting State-Action Space Structure (MESA)
trainingpolicy.
forcooperativemulti-agentlearning.AsshowninFigure2,MESA
Theaboveanalysisshowsthatcommonexplorationstrategies
consistsofameta-trainingstage(Algo.1)andameta-testingstage
likeuniformexplorationor𝜖-greedyexplorationareinefficientfor
(Algo.2).Inthemeta-trainingstage,MESAlearnsexplorationpoli-
suchasimplegameandthemainreasonisthatitrequirescoordina-
cies by training in a batch of training tasks that share intrinsic
tionbetweendifferentagentstoreachhigh-rewardingstates,but
structuresinthestate-actionspace.Inthemeta-testingstage,MESA
naiveexplorationstrategieslacksuchcooperation.
utilizesthemeta-learnedexplorationpoliciestoassistlearningin
anunseentasksampledfromthedistributionofthetrainingtasks.
4.2 StructuredExploration
Wewillshowthatitispossibletodesignabetterexplorationstrat- 5.1 Meta-Training
egywithsomepriorknowledgeoftheclimbgamestructure.Con-
sideraspecificstructuredexplorationstrategy𝑝 𝑒(𝑡) (𝑖,𝑗) =𝑈−1 1 𝑖=𝑗 , T reh we am rde it na g-t sr ta ain tei -n ag cts it oa nge suc bo sn pt aa ci en ,s at nw do 2s )t te rp as in:1 a) sid eten ot fif ey xpth loe rah ti ig oh n-
where both agents always choose the same action. With such a
(cid:2) (cid:3) policiesusingthesubspace-inducedrewards.
strategy, we can quickly find the optimal solution to the game.
Moreformally,wehavethefollowingtheorem. 5.1.1 IdentifyingHigh-RewardingJointState-ActionSubspace. For
eachtrainingtaskT𝑖,wecollectexperiencesD𝑖 ={(𝑠𝑡,𝒂 𝑡,𝑟𝑡,𝑠𝑡+1)}.
Theorem4.4(structuredexploration). Intheclimbgame𝐺 (2,0,𝑈), ★
𝑓 Ifthereward𝑟𝑡 is higher thanathreshold𝑅 ,we callthisjoint
understructuredexploration𝑝 𝑒(𝑡) (𝑖,𝑗) =𝑈−1 1 𝑖=𝑗 ,𝑞 J(𝑇)(W,b,c,𝑑) state-action pair (𝑠𝑡,𝒂 𝑡) valuable and store it into a dataset M∗.
isequivalentlyoptimalatstep𝑇 =𝑂(1).
(cid:2) (cid:3)
Forgoal-orientedtaskswhere𝑟 = 1 𝑠=𝑔𝑜𝑎𝑙,thethreshold canbeAlgorithm1MESA:Meta-Training Algorithm2MESA:Meta-Testing
Input:Meta-trainingtasks{T𝑖} 𝑖𝐵 =1∼𝑝(T),off-policyMARL Input:TesttaskTˆ,meta-trainedexplorationpolicies{𝝅𝑖 𝑒} 𝑖𝐸 =1,
algorithm𝑓,distancemetrick·k F off-policyMARLalgorithm𝑓
★
Parameter:#policies𝐸,threshold𝑅 ,horizonℎ Parameter:horizonℎ
Output:Explorationpolicies{𝝅𝑖}𝐸 Output:Policy𝝅 fortaskTˆ
𝑒 𝑖=1 𝜃
1: M∗←∅,globalpseudo-count𝑁ˆ ←0 1: Initializepolicy𝝅 𝜃,D =∅,annealing𝜖
2: fori=1toBdo 2: whilenotconvergeddo
3: Initializepolicy𝝅 𝜃 3: Determine𝑝𝑒 underannealingprobabilityschedule𝜖
4: Train𝝅 𝜃 with𝑓 andcollectdataset𝐷𝑖 ={(𝒔 𝑡,𝒂 𝑡,𝑟𝑡,𝒔 𝑡+1)} 4: Choosepolicytoperformrolloutsby
★
5 6 7: :
:
e fon rM d if∗ =o← r 1toM E∗ d∪ o{𝜏 |𝑅(𝜏) ≥𝑅 ,𝜏 ∈𝐷𝑖} 𝝅 𝑑 = (𝝅 𝝅𝑒 𝜃,∼U({𝝅𝑖 𝑒} 𝑖𝐸 =1), ow t. hp e. r𝑝 w𝑒
ise.
18
9
0:
:
:
I wn hi It niia
l ie
tli iz a𝝅e
li𝑖 𝑒
ze
’
esxp 𝑁trlo
a
air
n
sa it 𝑁ni ˆo
g
,n Dnp oo ←tl cic
o
∅y nv𝝅 e𝑖 𝑒
rgeddo
765 ::: for OExt
be
s=
c
eu0 rvtt
e
eo
𝒂
th
r𝑡
a-1
∼
nsd
𝝅
io
ti𝑑 o( n𝑠𝑡 () 𝑠.
𝑡,𝒂 𝑡,𝑟𝑡,𝑠𝑡+1).
1 11 2:
:
for Ext e= cu0 tt eo 𝒂h 𝑡-1 ∼d 𝝅o
𝑖 𝑒(𝑠𝑡),andobserve(𝑠𝑡,𝒂𝒕 ,𝑟𝑡,𝑠𝑡+1)
98 :: enD df← orD∪(𝑠𝑡,𝒂 𝑡,𝑟𝑡,𝑠𝑡+1)
11 43 :: SC ta ol rc eul (a 𝑠t 𝑡e ,𝒂𝑟ˆ 𝑡𝑡 ,b 𝑟ˆ𝑡a ,s 𝑠e 𝑡d +1o )n inE tq o. D5or6 1 10 1:
:
enO dp wti hm ii lz ee𝝅 𝜃 withalgorithm𝑓 onreplaybufferD
15: 𝑁(𝜙(𝑠𝑡,𝒂 𝑡)) ←𝑁(𝜙(𝑠𝑡,𝒂 𝑡))+1
12:
return𝝅
𝜃
16: endfor
17: Optimizepolicy𝝅𝑖 𝑒 withalgorithm𝑓
18: endwhile
19: Update𝑁ˆ usingD
pairsareclose.Thenifavisitedjointstate-actionpair(𝑠,𝒂)isclose
20: endfor
21: return{𝝅𝑖 𝑒} 𝑖𝐸 =1 𝜖en ,io tu wg oh ut lo dth beei ad se sn igt nifi ee dd asu db es rip va ec de pM os∗ i, ti i. ve e., rm ewin a𝑑 r∈ dM 𝑟ˆ.∗ Ik n( c𝑠 r, e𝒂 a) s, i𝑑 nk gF th<
e
valueof𝐵 inthecollectionstepwouldgenerallyresultinamore
accuratedistancemeasurement. However, thiscomesatthecost
setas𝑅★=1.Forothertasks,thethresholdcanbesetasahyper- ofmakingtheminimizationcalculationmorecomputationallyex-
pensive.
parameter,forexample,acertainpercentileofallcollectedrewards.
★ Toencourageabroadercoverageofthesubspaceandtoavoid
Asmaller𝑅 resultsinalargeridentifiedsubspacebutalesseffi-
modecollapse,therewardassignmentschemeensuresthatrepeated
cientexplorationpolicy.
visitstosimilarjointstate-actionpairswithinonetrajectorywould
ThedatastoredinM∗ishighlydiversifiedsinceitcomesfrom
result in a decreasing reward for each visit. Similar to [37], we
allthe𝐵 training tasks,whichareexpectedtoshareanintrinsic
adoptapseudo-countfunction𝑁 withahashfunction𝜙(𝒔,𝒂) to
structure. Weexpectthatwiththisintrinsic structure,thehigh-
generalizebetweensimilarjointstate-actionpairs.Wethenapplya
rewardingjointstate-actionpairsfallintosomelow-dimensional
decreasingfunction𝑓 :N ↦→[0,1]onthetrajectory-levelpseudo-
subspace.Inthesimplestcase,theymayformseveraldenseclus- 𝑑
count𝑁(𝜙((𝑠,𝒂)).Theresultedrewardassignmentschemeisde-
ters,ormanyofthemlieinahyperplane.Evenifthesubspaceis
finedasfollows:
noteasilyinterpretabletohumans,itmaystillbeeffectively“cov-
ered”byasetofexplorationpolicies(tobefoundinthesubsequent 𝑟˜𝑡 =𝑟ˆ𝑡𝑓 𝑑(𝑁(𝜙((𝑠𝑡,𝒂 𝑡))) 1 min𝑑∈M∗k(𝑠𝑡,𝒂𝑡),𝑑kF<𝜖 (6)
step).
h i
Wealsoexplicitlydealwiththerewardsparsityproblembyas- After one exploration policy is trained with this reward, we
signing apositivereward toajoint state-actionpair (𝑠𝑡,𝒂 𝑡) if it willtrainanewpolicytocoverthepartoftheidentifiedsubspace
haszerorewardbutleadstoavaluablestate-actionpair (𝑠𝑡′,𝒂 𝑡′) thathasnotyetbeencovered.Thisisachievedbyhavingaglobal
laterinthesametrajectory.Wealsoputtheserelabeledpairsinto pseudo-count𝑁ˆ whichisupdatedaftertrainingeachexploration
thedatasetM∗.Let𝑡′ = argmin𝑡′>𝑡[𝑟𝑡′ > 0],wethereforehave policyusingitsvisitationcountsandismaintainedthroughoutthe
thefollowingdensifiedrewardfunction trainingofallexplorationpolicies.Thisiterativeprocesscontinues
untilthesubspaceiswell-coveredbythesetoftrainedexploration
𝑟ˆ𝑡 = (𝛾 𝑟𝑡𝑡 ,′−𝑡 ·𝑟𝑡′, 𝑟 𝑟𝑡𝑡 = >0 0,
.
(5) policies.
5.2 Meta-Testing
5.1.2 LearningExplorationPolicies. Inthisstep,weaimtolearn
a diverse set ofexplorationpolicies to cover the identified high- Duringmeta-testing,MESAusesthemeta-learnedexplorationpoli-
rewarding joint state-action subspace. We use a distance metric cies{𝝅𝑖 𝑒} 𝑖𝐸 =1toassistthetrainingofanygenericoff-policyMARL
k · k F (e.g., 𝑙2 distance) to determine whether two state-action algorithmonatest-timetaskTˆ.Specifically,foreachrolloutepisode,wechoosewithprobability𝜖toexecuteoneuniformlysampledex-
plorationpolicy𝝅 𝑒 ∼ U({𝝅𝑖 𝑒} 𝑖𝐸 =1).Forthebestempiricalperfor-
mance,wealsoadoptanannealingschedule𝜖 :𝑇 ↦→[0,1]sothat
theexplorationpoliciesprovidemorerolloutsattheinitialstage
ofthetrainingandaregraduallyturnedofflater.
Herewefurtherprovidesomeanalysisofdeployingthemeta-
learnedexplorationpolicyonunseentestingtasks.
Theorem5.1 (Explorationduring Meta-Testing). Consider goal-
oriented tasks with goal space G ⊆ S. Assume the training and
testinggoalsaresampledfromthedistribution𝑝(𝑥) on G,andthe
datasethas𝑁 i.i.d.goalssampledfromadistribution𝑞(𝑥) onS.If Figure3:Learningcurveofthetwoclimbgamevariantsw.r.t
theexplorationpolicygeneralizestoexplore𝜖nearbygoalsforevery numberofenvironmentsteps.Thereturnisaveragedover
trainingsample,wehave thatthetestinggoalisnotexploredwith timestepsforthemulti-stagegames.Thedottedlinesindi-
probabilityatmost catethe suboptimalreturnof 0.5 (purple) and the optimal
𝐾𝐿(𝑝||𝑞)+H(𝑝) return1(blue)foreachagent.
𝑃 ≈ 𝑝(𝑥)(1−𝜖𝑞(𝑥))𝑁𝑑𝑥 ≤𝑂 . (7)
fail log(𝜖𝑁)
∫ (cid:18) (cid:19)
Theorem5.1showsthatthegoodperformanceofmeta-learned
explorationpolicyrelieson1)asmalldifferencebetweenthetrain-
ingandtestingdistribution;and2)astructured,e.g.,low-dimensional, meta-learningbaselines,includingonewithanunconditionedshared
high-rewardingsubspaceGtoreduceH(𝑝).Andwhenuniformly policy, which is trained over all training tasks, and one with a
samplingthetrainingdata,𝐾𝐿(𝑝||𝑞)isboundedbylogΩ G inour goal-conditionedpolicy,whichtakesthetargetlandmarksasparts
method.Thisterm,however,canbeuptologΩ S withanuncoor- of the input. We also adapt the single-agent meta-RL algorithm
dinatedexplorationonthejointstatespaceS,where Ω S canbe MAESN[14]tothemulti-agentsetting.Finally,weadaptthesingle-
exponentiallylargerthanΩ G. agent C-BET [26]tomulti-agent settingsbased onMAPPO.The
trainingandtestingtasksareasdefinedinSection6.1.Pleaserefer
5.3 ImplementationDetailofMESA totheAppendixformorevisualizationandexperimentalresults.
Environments.WeexperimentontheClimbGame,Multi-agent
WechooseMADDPG,followingthecentralizedtrainingwithde-
ParticleEnvironment(MPE)[23],andmulti-agentMuJoCo[29],on
centralized execution (CTDE) paradigm, as the off-policy MARL
whichgeneratingadistributionofmeta-trainingtasks𝑝(T)isfea-
algorithmforMESAsince it canbeappliedtobothdiscrete and
sible.
continuousactionspace,asshowninitsoriginalpaper[23].Weuse
aclusteringmapping𝑓𝑐 asthehashfunction𝜙 sothatthedataset
M∗isclusteredinto𝐶clustersdefinedbytheclusteringfunction 6.2 ClimbGameVariants
𝑓𝑐 :S×A ↦→ [𝐶].Theclustermappingisimplementedwiththe First,weconsidertaskspacesconsistingofvariantsoftheafore-
KMeansclusteringalgorithm[22].Thenumberofexplorationpoli- mentioned climb games. We extend previous climb game to (1)
ciestolearnisviewedasahyperparameter.SeetheAppendixfor one-stepclimbgame𝐺(𝑛,𝑘,𝑢,𝑈),whichisa𝑛-playergamewith
detailedhyperparametersettings. 𝑈 actionsforeachplayer,andthejointrewardis1if#𝑢=𝑘,1−𝛿
if #𝑢 = 0, and 0 otherwise. The task space Tone consists of all
6 EXPERIMENTS 𝑈
one-stepclimbgamesthatcontaintwoplayersand𝑈 actions;(2)
Our experimental evaluationaims toanswer thefollowingques- multi-stageclimbgame,whichisan𝑆-stagegamewhereeach
tions: (1) Are the meta-learned exploration policies capable of stage is a one-stage climb game withthe same number of avail-
achievingmoreefficientexplorationduringmeta-testingonnewly ableactions.Eachstage𝑡 hasitsownconfiguration(𝑘𝑡,𝑢𝑡)ofthe
sampledtasksinmatrixclimbgamevariants(Section6.2)andhigh- one-stage climb game𝐺(2,𝑘𝑡,𝑢𝑡,𝑈). Agents observe thehistory
dimensional domains (Section 6.3 and 6.4)? (2) Can these meta- ofjointactionsandthecurrentstage𝑡.ThetaskspaceTmulticon-
𝑆,𝑈
learnedexplorationpoliciessuccessfullygeneralizetounseentest- sistsofall multi-stageclimb games with𝑆 stages and𝑈 actions.
time tasksfromamorechallenging (e.g., withmoreagents) test Inourexperiments,weuseToneandTmultiasthetaskspacefor
10 5,10
taskdistributionwhich isdifferent thetraining taskdistribution theone-stepandmulti-stageClimbGames.Wechooseuniformly
(Section6.5)? atrandomtentrainingtasksandthreedifferenttesttasksfromthe
taskspaceT,andwekeep𝛿 = 1 asintheclassicclimbgames.
6.1 EvaluationSetup 2
ResultsonClimbGameVariants.Forthematrixgames,we
ComparedMethods.Wecompareto3multi-agentreinforcement additionallycomparewithMA-MAESN,whichisouradaptation
learningalgorithms:MADDPG[23],MAPPO[41],andQMIX[33], oftheoriginalsingle-agentmeta-learningalgorithmMAESN[14]
tomeasuretheeffectiveness ofourexplorationpolicies.Wealso tothemulti-agentscenarioInthesingle-stepmatrixgame,MESA
compareto3multi-agentexplorationalgorithm:MAVEN[24],MAPPO exhibitsbetterperformance,beingabletofindtheoptimalreward
withRNDexploration[5],andEMC[43].Tocomparewithbase- insomehardertaskswhen𝑘 =2,whileotherbaselinesarestuck
lines that adoptasimilar meta-training stage, we addtwo naive atthesub-optimalrewardforalmostalltasks.Figure4:LearningcurvesofMESAandthecomparedbaselinesw.r.tthenumberofenvironmentinteractionsduringthemeta-
testingstageintheMPEdomainandthemulti-agentMuJoCoenvironmentSwimmer.Thetwodottedlinesindicatetheideal
optimal(purple)andsub-optimal(blue)returnsummedovertimesteps.Areturnabovethebluelinewouldtypicallyindicate
thattheagentsareabletolearntheoptimalstrategy.
Inthemorechallenging10-actionmulti-stagegamewheretask
spaceisexponentiallylarger,MESAoutperformsallcomparedal-
gorithmsbyalargemargin.Withthehelpoftheexplorationpoli-
ciesthathavelearnedthehigh-rewardingjointactionpairs,MESA
quicklylearnstheoptimaljointactionforeachstageandavoidsbe-
ingstuckatthesub-optimal.
Figure6:Visualizationofstructuredexplorationbehaviors
discoveredbythemeta-trainedexplorationpolicyinMESA.
onthe2-agenttasks(TMPE andTMPE)and3-agenttasks(TMPE
2,5 2,6 3,5
andTMPE)whilefixing𝑘 = 2.Eachsampledtrainingandtesting
3,6
taskhasadifferentconfigurationoflandmarkpositions.
AdaptationPerformanceinMPE.WeshowinFigure4the
learning curve of ourapproach MESA compared with theafore-
mentioned baseline methods. MESA outperforms the compared
baselinesbyalargemargin,beingabletocoordinatelyreachthe
tasklandmarkquickly,asevidenced bythenear-optimal reward.
EvenwhencombinedwithRND-basedexploration,MAPPOeasily
sticks to the sub-optimal equilibrium. Value-based methods like
QMIXandMAVENareunabletolearnthecorrect𝑄-functionbe-
Figure5:Visualizationsofa2-player3-landmarkMPEclimb
cause the reward is quite sparse before agents can consistently
game.
movethemselvestoalandmark.EMCsometimesjumpsoutofthe
suboptimalequilibriumwithcuriosity-drivenexploration,butthe
performanceisnotrobust.Furthermore,asthemeta-learningbase-
6.3 MPEDomain
lines only learn the sub-optimal behavior during meta-training,
WeextendthematrixclimbgamestoMPE[23],whichhasacon- theyfailtolearntheoptimalequilibriumduringtesttimeandquickly
tinuoushigh-dimensional state space. Agents mustfirst learn to convergetothesuboptimalequilibrium.
reachthelandmarksundersparserewardsandthenlearntoplay Visualizationof Exploration Policies. To answer question
theclimbgamesoptimally. (2), we visualize the learned exploration policies in a 2-agent 3-
InaMPEClimbGame𝐺¯(𝑛,𝑘,𝑢,𝑈,{𝐿𝑗}𝑈 0−1)(Figure5),thereare landmarkMPE taskin Figure 6.We can see that the learned ex-
𝑈non-overlappinglandmarkswithpositions{𝐿𝑗}𝑈 𝑗=− 01.Thereward plorationpolicyconsecutivelyvisited the3landmarkswithin20
isnon-zeroonlywheneveryagentisonsomelandmark.Agents timestepsinonetrajectory.
will be given a reward of 1 if there are exactly𝑘 agents located
6.4 Multi-agentMuJoCoEnvironments
onthe𝑢-thlandmark(targetlandmark),andasuboptimalreward
of1−𝛿 willbegivenwhennoneoftheagentsarelocatedonthe Wealsoextendthematrixclimbgamestomulti-agentMuJoCoen-
targetlandmark.Otherwise,therewardwillbezero.Asbefore,𝑢 vironments[29].Weconsiderspecificallythe2-agentSwimmeren-
and𝑘 arenotpresentintheobservationandcanonlybeinferred vironmentwhereeachagentisahingeontheswimmer’sbody,and
fromthereceivedreward.AtaskspaceTMPEconsistsofallMPE eachagent’sactionistheamountoftorqueappliedtohingerotors.
𝑛,𝑈
climbgameswith𝑛playersand𝑈 landmarks.WeevaluateMESA Theextensionconsiderstheanglesbetweenthetwohingesandthebodysegments.Eachtaskinthetaskspaceisatargetanglesuch
that areward of 1 will be given only if the two angles are both
closetothetargetangles,a0.5suboptimalrewardisgivenifnone
oftwoanglesareclosetothetarget,andarewardof0ifonlyone
ofthetwoanglesareclose.
Thismulti-agentenvironment isextremelyhardasagentsare
verylikelytoconvergetothesuboptimalrewardof0.5,whichis
confirmed bytheresultsthat none ofthebaselines were ableto
findtheoptimalequilibriuminFigure4.Therefore,MESAvastly
outperformsallthecomparedbaselinesbylearningafinalpolicy
thatfrequentlyreachesthetargetangle.
Figure7:GeneralizationresultsofMESAonthehard3-agent
MPE Climb game. Left: Zero-shot generalizability of the
meta-explorationpolicies,measuredbythenumberofvis-
itationsonhigh-rewardtransitionsperepisodeonthetest
6.5 GeneralizationPerformanceofMESA tasks.Thepurpledottedlinecorrespondstotherandomex-
In this section, our goal is to evaluate the generalization perfor- plorationpolicy.Theplotshowstheconcatenatedtraining
manceofthemeta-trainedexplorationpolicyinscenarioswhere curvesforallexplorationpolicies.Right:Learningcurvesof
themeta-trainingandmeta-testingtaskdistributionsaredifferent. MESAunder differentsettingsusing the meta-exploration
Inparticular,wefocusonthesettingwherethetest-timetasksare policiestrainedonthetwodifferenttraining-taskdistribu-
morechallengingthanthetraining-timetasksandexaminehowan tions.
explorationpolicylearnedfromsimplertaskscanboosttraining
performancesonhardertasks.
Thetesttaskhereisuniformonthe3-agenthigh-difficultyMPE 7 CONCLUSIONS
Climbgames.Thetaskdifficultyisdefinedbytheaveragepairwise
Thispaperintroducesameta-explorationmethod,MESA,formulti-
distancesbetweenthelandmarkpositionsandtheinitialpositions
agent learning. The key idea is to learn a diverse set of explo-
oftheagents.Weconsidertwosimplertrainingtaskdistributions,
rationpoliciestocoverthehigh-rewardingstate-actionsubspace
including(1)a2-agentsettingwiththesamedifficulty,and(2)a
and achieve efficient exploration in an unseen task. MESA can
3-agentsettingwithalowerdifficulty.Inbothsettings,themeta-
workwithanyoff-policyMARLalgorithm,andempiricalresults
trainingtasksarelesschallengingthanthetest-timetasks.Foreval-
confirmtheeffectivenessofMESAinclimbgames,MPEenviron-
uation,themeta-trainedexplorationpolicyfromeachsettingwill
ments, and multi-agent MuJoCo environments and its generaliz-
bedirectlyappliedtoassistthetrainingonthemorechallenging
abilitytomorecomplextest-timetasks.
test-timetasks,withoutanyfine-tuning.
Wemodifiedtheneuralnetworkarchitecturebyadoptinganat-
ACKNOWLEDGMENTS
tentionlayerinbothactorandcritictoensuretheyarecompatible
withavaryingnumberofagents.Theattentionmechanismactsas ThisresearchissupportedinpartbyNSFIIS-2046640(CAREER)
anaggregationfunctionbetweentherelativepositionsoftheother andSloanResearchFellowship.WethankNVIDIAforproviding
agents and its own relative positionto the landmarks to handle computingresources.ZhichengZhangissupportedinpartbySCS
the varying observation dimensions. Additionally, we employed Dean’sFellowship.Thefundershavenoroleinstudydesign,data
behaviorcloning(BC)[30]ontherolloutsoftheexplorationpoli- collectionandanalysis,decisiontopublish,orpreparationofthe
ciesasawarm-uptoacceleratelearningofthefinalpolicy. manuscript.
InFigure7,wepresentthegeneralizationresultsfromourstudy.
Weevaluatethezero-shotgeneralizationabilityofthemeta-exploration REFERENCES
policy by measuring the average number of high-reward transi- [1] JacobAndreas,DanKlein,andSergeyLevine.2017.Modularmultitaskreinforce-
tionshitinatesttaskrandomlysampledfromthetesttaskdistri- mentlearningwithpolicysketches.InICML.PMLR,166–175.
[2] MarcinAndrychowicz,FilipWolski,AlexRay,JonasSchneider,RachelFong,Pe-
bution.AsshownontheleftofFigure7,themeta-explorationpoli- terWelinder,BobMcGrew,JoshTobin,PieterAbbeel,andWojciechZaremba.
ciesareabletoexplorethetest-timetasksmuchmoreefficiently 2017.Hindsightexperiencereplay.arXivpreprintarXiv:1707.01495(2017).
[3] MarcBellemare,SriramSrinivasan,GeorgOstrovski,TomSchaul,DavidSaxton,
thanarandomexplorationpolicy,evenontest-timetasksthatare
andRemiMunos.2016.Unifyingcount-basedexplorationandintrinsicmotiva-
drawnfromahardertaskdistribution.Notably,thegeneralization tion.NeurIPS29(2016),1471–1479.
abilityincreaseswiththenumberofexplorationpolicies(𝐵).Using [4] AvrimBlumandRonaldRivest.1988. Traininga3-nodeneuralnetworkisNP-
complete.NeurIPS1(1988).
themeta-explorationpoliciestrainedonthesimplertasks,MESA
[5] YuriBurda,HarrisonEdwards,AmosStorkey,andOlegKlimov.2018. Explo-
isabletoconsistentlyreachthehigh-rewardregionintheunseen rationbyrandomnetworkdistillation.InICLR.
hard3-agenttasks,asopposedtothevanillaMADDPGalgorithm [6] CédricColas,OlivierSigaud,andPierre-YvesOudeyer.2017. GEP-PG:Decou-
plingExplorationandExploitationinDeepReinforcementLearningAlgorithms.
thatonlylearnsthesub-optimalequilibrium.Wealsoseethatwith InICML.
anincreasingnumberofmeta-explorationpolicies,theperformance [7] RonDorfman,IdanShenfeld,andAvivTamar.2020. OfflineMetaLearningof
Exploration.arXivpreprintarXiv:2008.02598(2020).
ofMESAincreases,buttheimprovementbecomesmarginal,while
[8] SimonDuandJasonLee.2018.Onthepowerofover-parametrizationinneural
themeta-trainingtimeincreaseslinearlywithE. networkswithquadraticactivation.InICML.PMLR,1329–1338.[9] YanDuan,JohnSchulman,XiChen,PeterLBartlett,IlyaSutskever,andPieter 20516–20530.
Abbeel.2016.Rl2:Fastreinforcementlearningviaslowreinforcementlearning. [27] EmilioParisotto,LeiJimmyBa,andRuslanSalakhutdinov.2016. Actor-Mimic:
arXivpreprintarXiv:1611.02779(2016). DeepMultitaskandTransferReinforcementLearning.InICLR(Poster).
[10] AdrienEcoffet,JoostHuizinga,JoelLehman,KennethOStanley,andJeffClune. [28] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. 2017.
2021.Firstreturn,thenexplore.Nature590,7847(2021),580–586. Curiosity-driven exploration by self-supervised prediction. In ICML. PMLR,
[11] LasseEspeholt,HubertSoyer,RemiMunos,KarenSimonyan,VladMnih,Tom 2778–2787.
Ward,YotamDoron,VladFiroiu,TimHarley,IainDunning,etal.2018.Impala: [29] BeiPeng,TabishRashid,ChristianSchroederdeWitt,Pierre-AlexandreKami-
Scalabledistributeddeep-rlwithimportanceweightedactor-learnerarchitec- enny,PhilipTorr,WendelinBöhmer,andShimonWhiteson.2021.Facmac:Fac-
tures.InICML.PMLR,1407–1416. toredmulti-agentcentralisedpolicygradients.NeurIPS34(2021).
[12] ChelseaFinn,PieterAbbeel,andSergeyLevine.2017. Model-AgnosticMeta- [30] DeanAPomerleau.1991.Efficienttrainingofartificialneuralnetworksforau-
LearningforFastAdaptationofDeepNetworks.InProceedingsofthe34thICML tonomousnavigation.Neuralcomputation3,1(1991),88–97.
(PMLR, Vol. 70), Doina Precupand Yee Whye Teh(Eds.). PMLR, 1126–1135. [31] VitchyrHPong,AshvinNair,LauraSmith,CatherineHuang,andSergeyLevine.
https://proceedings.mlr.press/v70/finn17a.html 2021.OfflineMeta-ReinforcementLearningwithOnlineSelf-Supervision.arXiv
[13] ShixiangGu,TimothyLillicrap,IlyaSutskever,andSergeyLevine.2016. Con- preprintarXiv:2107.03974(2021).
tinuousdeepq-learningwithmodel-basedacceleration.InICML.PMLR,2829– [32] KateRakelly,AurickZhou,ChelseaFinn,SergeyLevine,andDeirdreQuillen.
2838. 2019. Efficientoff-policymeta-reinforcementlearningviaprobabilisticcontext
[14] Abhishek Gupta, RussellMendonca, Yuxuan Liu,Pieter Abbeel, and Sergey variables.InICML.PMLR,5331–5340.
Levine.2018. Meta-ReinforcementLearningofStructuredExplorationStrate- [33] Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar,
gies.NIPS2018(2018),5302–5311. JakobFoerster,andShimonWhiteson.2018. Qmix:Monotonicvaluefunction
[15] TarunGupta,AnujMahajan,BeiPeng,WendelinBöhmer,andShimonWhite- factorisationfordeepmulti-agentreinforcementlearning.InICML.PMLR,4295–
son.2021. Uneven:Universalvalueexplorationformulti-agentreinforcement 4304.
learning.InICML.PMLR,3930–3941. [34] MartinRiedmiller,RolandHafner,ThomasLampe,MichaelNeunert,JonasDe-
[16] Matteo Hessel, Hubert Soyer, Lasse Espeholt, Wojciech Czarnecki, Simon grave,TomWiele,VladMnih,NicolasHeess,andJostTobiasSpringenberg.2018.
Schmitt,andHadovanHasselt.2019. Multi-taskdeepreinforcementlearn- Learningbyplayingsolvingsparserewardtasksfromscratch.InICML.PMLR,
ingwithpopart.InProceedingsoftheAAAIConferenceonArtificialIntelligence, 4344–4353.
Vol.33.3796–3803. [35] LukasSchäfer,FilipposChristianos,JosiahPHanna,andStefanoVAlbrecht.
[17] EdwardHughes, Joel ZLeibo,Matthew Phillips, KarlTuyls, Edgar Duenez- 2022. DecoupledReinforcementLearningtoStabiliseIntrinsically-Motivated
Guzman,Antonio GarciaCastaneda,IainDunning,TinaZhu,KevinMcKee, Exploration.InProceedingsofthe21stAAMAS.1146–1154.
RaphaelKoster,etal.2018.Inequityaversionimprovescooperationinintertem- [36] AviSingh,HuihanLiu,GaoyueZhou,AlbertYu,NicholasRhinehart,andSergey
poralsocialdilemmas.NIPS201831(2018). Levine.2020.Parrot:Data-DrivenBehavioralPriorsforReinforcementLearning.
[18] NatashaJaques,AngelikiLazaridou,EdwardHughes,CaglarGulcehre,Pedro InICLR.
Ortega,DJStrouse,JoelZLeibo,andNandoDeFreitas.2019. Socialinfluence [37] HaoranTang,ReinHouthooft,DavisFoote,AdamStooke,XiChen,YanDuan,
asintrinsicmotivationformulti-agentdeepreinforcementlearning.InICML. JohnSchulman,FilipDeTurck,andPieterAbbeel.2017.#exploration:Astudy
PMLR,3040–3049. ofcount-basedexplorationfordeepreinforcementlearning.In31stNIPS,Vol.30.
[19] NatashaJaques,AngelikiLazaridou,EdwardHughes,CaglarGulcehre,PedroA 1–18.
Ortega,DJStrouse,JoelZLeibo,andNandodeFreitas.2018. Intrinsicsocial [38] Pin Wang, Hanhan Li, and Ching-Yao Chan. 2019. Quadratic Q-network
motivationviacausalinfluenceinmulti-agentRL.(2018). for learning continuous control for autonomous vehicles. arXiv preprint
[20] LinLan,ZhenguoLi,XiaohongGuan,andPinghuiWang.2019. Metarein- arXiv:1912.00074(2019).
forcementlearningwithtaskembeddingandsharedpolicy. arXivpreprint [39] TonghanWang,JianhaoWang,YiWu,andChongjieZhang.2019. Influence-
arXiv:1905.06527(2019). BasedMulti-AgentExploration.InICLR.
[21] Iou-JenLiu,UnnatJain,RaymondAYeh,andAlexanderSchwing.2021.Cooper- [40] TianbingXu,QiangLiu,LiangZhao,andJianPeng.2018. Learningtoexplore
ativeexplorationformulti-agentdeepreinforcementlearning.InICML.PMLR, withmeta-policygradient.arXivpreprintarXiv:1803.05044(2018).
6826–6836. [41] ChaoYu,AkashVelu,EugeneVinitsky,YuWang,AlexandreBayen,andYiWu.
[22] StuartLloyd.1982. LeastsquaresquantizationinPCM. IEEEtransactionson 2021. TheSurprisingEffectivenessofPPOinCooperative,Multi-AgentGames.
informationtheory28,2(1982),129–137. arXiv:2103.01955[cs.LG]
[23] RyanLowe,YiWu,AvivTamar,JeanHarb,PieterAbbeel,andIgorMordatch. [42] JinZhang,JianhaoWang,HaoHu,TongChen,YingfengChen,ChangjieFan,
2017. Multi-AgentActor-CriticforMixedCooperative-CompetitiveEnviron- and Chongjie Zhang. 2021. Metacure: Meta reinforcement learning with
ments.InNIPS. empowerment-drivenexploration.InICML.PMLR,12600–12610.
[24] AnujMahajan,TabishRashid,MikayelSamvelyan,andShimonWhiteson.2019. [43] LuluZheng,JiaruiChen,JianhaoWang,JiaminHe,YujingHu,YingfengChen,
MAVEN:Multi-AgentVariationalExploration.InNeurIPS,Vol.32.7613–7624. ChangjieFan,YangGao,andChongjieZhang.2021.EpisodicMulti-agentRein-
[25] GeorgOstrovski,MarcGBellemare,AäronOord,andRémiMunos.2017.Count- forcementLearningwithCuriosity-drivenExploration.NeurIPS34(2021).
basedexplorationwithneuraldensitymodels.InICML.PMLR,2721–2730. [44] LuisaZintgraf,KyriacosShiarlis,MaximilianIgl,SebastianSchulze,YarinGal,
[26] SimoneParisi,VictoriaDean,DeepakPathak,andAbhinavGupta.2021.Interest- KatjaHofmann,andShimonWhiteson.2020. VariBAD:AVeryGoodMethod
ingobject,curiousagent:Learningtask-agnosticexploration.NeurIPS34(2021), forBayes-AdaptiveDeepRLviaMeta-Learning.InICLR.Appendix
𝑇
A PROOFS
J =−
2𝜎2
𝑓0(𝑊0+2𝐵+𝐷−𝑟)2
𝑒
(cid:16)
A.1 ProofofLemmaonEquivalentOptimality +𝑓1(𝑊1+𝐵+𝐶+𝐷)2
L spe am cem |Aa |A =.1 𝑈. aIn ndth re ew2- aa rg de mnt aC trl ii xmbGamewithsingle-agentaction + 𝑓2(𝑊2+2𝐶+𝐷−𝑟(1−𝛿))2
1 (cid:17)
𝑟 0 ··· 0 − 𝑊2+2𝑚𝑊2+𝑚2𝑊2 (10)
2𝜎2 0 1 2
0 𝑟(1−𝛿) ··· 𝑟(1−𝛿) 𝑤
(cid:16) (cid:17)
𝑅= © 0. . . 𝑟(1. . . −𝛿) ·. ·. ·. 𝑟(1. . . −𝛿)ª ® ®, Further,let 𝐾0=2𝐵+𝐷−𝑟
(𝑡)
®
®
𝐾1=𝐵+𝐶+𝐷
f to ryr ia nn gy ace tx iop nlor (a 𝑖,ti 𝑗o )n
a«
tp to il mic ey s𝑝 te𝑒 pw 𝑡,h ge ir ve e𝑝 n𝑒 the(𝑖 o, b𝑗 j) ecis tivt ¬h ee fup nr co tb ioa nbility of 𝐾2=2𝐶+𝐷−𝑟(1−𝛿)
andimmediately
J(𝑇)(W,b,c,𝑑)
𝐾0+𝐾2=2𝐾1−𝑟(2−𝛿) (11)
𝑇
= E (𝑖,𝑗)∼𝑝𝑒(𝑡) logN(𝑞(e𝑖,e𝑗;W,b,c,𝑑)−𝑅𝑖𝑗;0,𝜎 𝑒2) Followingequation(10),
+Õ l𝑡 o= g1
N(𝑊;0,𝜎
𝑊2(cid:2)
𝐼)+Constant
(cid:3)
(8)
(cid:18)2 𝜕𝑊𝜕
0
+ 𝜕𝑊𝜕
1
− 𝜕𝜕
𝐵
(cid:19)J =0⇒𝑊0=−𝑚𝑊1
maximizedbyparametersW∗,b∗,c∗,𝑑∗,thejointQfunction 𝜕 𝜕 𝜕
𝑞(e𝑖,e𝑗;W∗,b∗,c∗,𝑑∗)isequivalentlyoptimalifthefollowingcrite-
(cid:18)2
𝜕𝑊2
+
𝜕𝑊1
−
𝜕𝐶
(cid:19)J =0⇒𝑊1=−𝑚𝑊2
rionh 𝑟o 𝛿ld ≥s
𝑓2
−1
𝑚2 𝑟(2−𝛿)
. (9)
𝜕𝑊𝜕
0J =0⇒𝑊0=−
𝑓0𝜆𝑓0𝜆
+1𝐾0
(cid:18)𝑓0 (cid:19) 𝑓2𝜆+𝑚2 1+ 2𝑓 𝑓2 1( (𝑓 𝑓1 2𝜆 𝜆+ +2 𝑚𝑚 2) )𝑚 + 𝑓 𝑓2 0( (𝑓 𝑓0 2𝜆 𝜆+ +1 𝑚)𝑚 2)2 𝜕𝑊𝜕 1J =0⇒𝑊1=− 𝑓1𝜆𝑓1 +𝜆 2𝑚𝐾1
Hereweuse
𝑓0= 𝑇1 𝑇 𝑝 𝑒(𝑡) (0,0)
𝜕𝑊𝜕
2J =0⇒𝑊2=−
𝑓2𝜆𝑓2 +𝜆
𝑚2𝐾2
Õ𝑡=1 𝜕
J
=0⇒𝑊0+𝐾0
=−
𝑓1
𝑓1= 𝑇1 Õ𝑡𝑇 =1𝑈 Õ𝑖=− 11 (cid:16)𝑝 𝑒(𝑡) (0,𝑖)+𝑝 𝑒(𝑡) (𝑖,0)
(cid:17)
𝜕𝜕 𝜕 𝐶𝐵
J
=0⇒𝑊 𝑊𝑊1 21+ ++𝐾 𝐾𝐾1 21 =−2 2 𝑓𝑓 𝑓 10 2
𝑇 𝑈−1𝑈−1
𝑚𝑓2 == 𝑈𝑇1
−Õ𝑡= 11 Õ𝑖=1
Õ𝑗=1𝑝 𝑒(𝑡) (𝑖,𝑗) andtogetherw 𝐾i 2th =eq 1u +at 2io 𝑓2n (𝑓( 11 𝜆1 +)
𝑟
2, 𝑚(w
2
)𝑚e −o
𝛿
+b )ta 𝑓2i (n
𝑓0𝜆+1)𝑚2
. (12)
𝑓1(𝑓2𝜆+𝑚2) 𝑓0(𝑓2𝜆+𝑚2)
𝑇𝜎2
𝜆= 𝑤 Finallywededucethecriterion
𝜎2
𝑒 𝑊0+2𝐵+𝐷 ≥𝑊2+2𝐶+𝐷
foraclearerdemonstrationofthecriterion.
𝑓2
Proof. Fromthesymmetryoftheparametersandtheconcavity ⇔𝑟𝛿 ≥ (cid:18)1− 𝑓0(cid:19)(𝑊2+𝐾2)
oftheobjectivefunction,∃𝑊0,𝑊1,𝑊2,𝐵,𝐶,𝐷suchthat 𝑓2 𝑚2 𝑟(2−𝛿)
𝑊 𝑊0 1= =W W∗ 0
∗
0𝑖0
=W 𝑖∗ 0, ∀𝑖≠0
⇔𝑟𝛿 ≥ (cid:18)𝑓0 −1 (cid:19) 𝑓2𝜆+𝑚2 1+ 2𝑓 𝑓2 1( (𝑓 𝑓1 2𝜆 𝜆+ +2 𝑚𝑚 2) )𝑚 + 𝑓 𝑓2 0( (𝑓 𝑓0 2𝜆 𝜆+ +1 𝑚)𝑚 2)2 .
(cid:3)
𝑊2=W 𝑖∗ 𝑗, ∀𝑖,𝑗 ≠0
𝐵=b∗=c∗ A.2 ProofforTheorem4.2(uniform
0 0
𝐶=b∗=c∗, ∀𝑖≠0 exploration)
𝑖 𝑖
𝐷 =𝑑 Theorem4.2. Assume𝛿 ≤ 61,𝑈 ≥3.IntheClimbGame𝐺 𝑓(2,0,𝑈),
given the quadratic joint Q function form 𝑞(x,y;W,b,c,𝑑) and a
Rewritetheobjectivefunction(8)weobtain Gaussianprior𝑝(W) =N(W;0,𝜎2𝐼),usingauniformexploration
𝑤
policy,𝑞 J(𝑇)(W,b,c,𝑑)willbecomeequivalentlyoptimalonlyafter
𝑇 = Ω(|A|𝛿−1) steps.When𝛿 = 1,𝑇 =𝑂(1)stepssufficetolearn
theequivalentlyoptimaljointQfunction,meaningtheinefficiency
ofuniformexplorationisduetoalargesetofsuboptimalNEs.Proof. Underuniformexploration,
1 2𝑚 𝑚2 𝑓2 𝑚2 𝑟(2−𝛿)
Criterion(9)canbe𝑓0 re= for𝑈 m2 u, l𝑓 a1 te= d𝑈 to2,𝑓2= 𝑈2. 𝑟𝛿 ≥ (cid:18)𝑓0 −1 (cid:19) 𝑓2𝜆+𝑚2 1+ 2𝑓 𝑓2 1( (𝑓 𝑓1 2𝜆 𝜆+ +2 𝑚𝑚 2) )𝑚 + 𝑓 𝑓2 0( (𝑓 𝑓0 2𝜆 𝜆+ +1 𝑚)𝑚 2)2
(𝑚2−1)(2−𝛿) =
𝑓2
−1
𝑚2 𝑟(2−𝛿)
𝛿 ≥
1+ 𝜆 (𝑚+1)2
(13) (cid:18)𝑓0 (cid:19) 𝑓2𝜆+𝑚2 1+ 𝑓2(𝑓0 𝑓0𝜆 (+ 𝑓1 2𝜆)( +𝑚 𝑚2 2+ )2𝑚)
andthuswith𝜆= 𝑇 𝜎𝜎 𝑒2𝑤2 ,𝑚(cid:16) ≥2,𝛿𝑈2 ≤(cid:17) 61, = (cid:18)𝑓 𝑓2
0
−1
(cid:19)
𝑓2𝜆𝑚 +2 𝑚2 (𝑟 𝑚(2 +− 1𝛿 )) 2 (14)
𝑇
≥𝑈2𝜎 𝑒2 (𝑚2−1)(2−𝛿)
−1
≥(𝑚2−1) 𝜆𝑚 +𝑚2
2
(𝑟 𝑚(2 +− 1𝛿 ))
2
𝜎2 (𝑚+1)2
𝑤 (cid:18) (cid:19)
Similartoinequality(13),thisyieldsto
𝑈2𝜎2 3 6
≥ 𝑒 −
𝜎 𝑤2 (cid:18)𝛿 𝛿 (cid:19) 𝑚2
=𝑈2𝜎 𝑒2 𝜆 ≥
6𝛿
(15)
6𝜎 𝑤2𝛿 Followinginequality(14),wefurtherget
Ontheotherhand,innon-penaltyClimbGamewhere𝛿 =1,if 𝑓2 𝑚2 𝑟(2−𝛿)
w
pat
ae
ra
i
agn mhy
s
et tai em
c rt
se
io
rs
n
et le a(p
𝑎
te𝑖∃ d,𝑎( t𝑖
𝑗
o, )𝑗 ()
m
𝑖,≠
o
𝑗)re( w0 t,
ih
t0 ha)
n
tw
ht
ohh seer rae
ec
lt
t
ah
i to
ee
n
djo
( t𝑎
oin
0
(t
,
0𝑎Q
,0 0)
)f ,u ajn
u
nc
s
dt ti to
s
hwn ea𝑞
op
bJ jt( eh𝑇 ce)
-
𝑟𝛿 ≥
≥(cid:18)
𝑓2𝑓0 − 𝑚1
2(cid:19)
𝑓2 𝑟𝜆+𝑚2 (𝑚+1)2
tivefunctionJ(𝑇)willbeincreased,whichmakesacontradiction. 2𝑓0𝜆+𝜆4𝑚2
𝑟
Hence,𝑇 =1sufficesforthenon-penaltyClimbGame. ≥ ,
(cid:3) 16𝑓0𝜆
whichis
A.3 ProofforTheorem4.3(𝜖-greedy
𝛿−1
𝜆 ≥ (16)
exploration) 16𝑓0
Forfixed𝜖,
Theorem4.3 Assume𝛿 ≤ 31 2,𝑈 ≥ max(4,𝜎𝑤𝜎 𝑒−1).IntheClimb
𝜖 1 𝜖
Game𝐺 𝑓(2,0,𝑈),giventhequadraticjointQfunctionform 𝑓0 ≤
𝑈2
+
𝑇𝑈2
≤
𝑈2
+𝜆−1,
𝑞(x,y;W,b,c,𝑑) andaGaussianprior𝑝(W) = N(W;0,𝜎2𝐼),un-
𝑤 andfurther
der𝜖-greedy exploration with fixed 𝜖 ≤ 1 2,𝑞 J(𝑇)(W,b,c,𝑑) will
𝛿−1
becomeequivalentlyoptimalonlyafter𝑇 =Ω(|A|𝛿−1𝜖−1)steps.If 𝜆 ≥
𝜖(𝑡) =1/𝑡,itrequires𝑇 =exp Ω |A|𝛿−1 explorationstepstobe
16( 𝑈𝜖
2
+𝜆−1)
equivalentlyoptimal. 𝑈2 𝛿−1 𝑈2𝛿−1
(cid:0) (cid:0) (cid:1)(cid:1) ⇒𝜆 ≥ −1 ≥
𝜖 16 32𝜖
Proof. Underthecircumstanceshere,afterthefirststepofuni- (cid:18) (cid:19)
formexploration,thesub-optimalpolicywillbeusedfor𝜖-greedy whichshowsthat
exploration.Thenforbothfixed𝜖 orlinearlydecaying𝜖,thefol- 𝑇 =Θ(𝜆) =Ω(𝑈2𝛿−1𝜖−1).
lowingalwaysholds: When𝜖 = 1,
𝑇
𝑓𝑓 01 =2𝑚 𝑓0 ≤ 𝑈1
2
Í𝑇 𝑡 𝑇=1𝑡1 ≤ 2l 𝑈og 2𝑇(𝑇)
𝑓2
≥max(2,𝑚2)
andfurther
𝑓0
𝜆 ≥
𝛿−1
𝑓2 ≥min(1−𝜖,𝑚2𝑈−2) ≥ 1 . 162l 𝑈og 2𝑇(𝑇)
2
𝑈2𝜎2
Thenitcanbederivedfromcriterion9that ⇒log(𝑇) ≥ 𝑒
32𝜎2𝛿
𝑤
whichshowsthat
𝑇 =exp(Ω(𝑈2𝛿−1)).
(cid:3)A.4 ProofforTheorem4.4(Structured (cid:3)
exploration)
Youmayrefertothefollowinglemmawhichisusedintheabove
Theorem4.4 IntheClimbGame𝐺 𝑓(2,0,𝑈),giventhequadratic proof.
jointQfunctionform𝑞(x,y;W,b,c,𝑑)andaGaussianprior𝑝(W)=
𝑞N J( (W 𝑇); (0 W,𝜎 ,𝑤2 b,𝐼 c) ,, 𝑑u )n id se er qs utr iu vc at lu enre td lye ox pp tlo imra at lio an t𝑝 st𝑒( e𝑡 p) ( 𝑇𝑖, =𝑗) 𝑂= (𝑈 1)− .1 (cid:2)1 𝑖=𝑗 (cid:3), LemmaA.3. ∀𝑘 𝑒> −1 𝑘6 𝑥,𝑥 ≤> lo0,
g 𝑥1
+
𝑥 +𝑥
(18)
1log𝑘 1log𝑘 𝑘
Proof. ItiseasytoverifythatW=c=0,b= (1,0,....,0)⊤,𝑑 = 2 2
0isthelearnedparameterthatmaximizesboththepriorofWand Proof. Let𝑥0 = 2lo 𝑘g𝑘 .Wecanproveinequality18byproving
thelikelihoodofpredictionerroratanystep𝑇.Thisparametercon- thefollowingthreeconditions.
figurationdirectlygives thejoint𝑄 functionthatisequivalently
𝑥
optimal. 1.∀𝑥 ≥𝑥0, ≥𝑒−𝑘𝑥 (19)
(cid:3) 𝑘
log1
A.5 ProofforTheorem5.1(Explorationduring 2.∀0≤𝑥 ≤𝑥0, 1log𝑥
𝑘
≥𝑒−𝑘𝑥 (20)
Meta-Testing) 2
1
DefinitionA.2(𝜖Generalization). Supposetherearetrainingand 3.∀𝑥 >0,𝑥+log ≥0. (21)
𝑥
testing data from the same space S. Let𝑔(𝑥,𝑦) ∈ {0,1} denote
Toprove(19),itsufficestoshow
whetheraexplorationpolicytrainedontrainingsample𝑥canlearn
toexplore𝑦duringtestingtime.Andwealwaysassume𝑔(𝑥,𝑦) =
𝑥0
=
2log𝑘
≥
1
=𝑒−𝑘𝑥0,
𝑔(𝑦,𝑥). Then we say a exploration policygeneralizes to explore
𝑘 𝑘2 𝑘2
𝜖 nearbygoalsifforeverytrainingsample𝑥,∃aneighbourhood as 𝑥 ismonotoneincreasingand𝑒−𝑘𝑥 ismonotonedecreasing.
𝑘
|Ω(𝑥)| ≥𝜖 of𝑥 s.t.∀𝑦 ∈ Ω(𝑥),𝑔(𝑥,𝑦) =1.Intuitively,thatmeans Nowweprove(20).Let
theexplorationpolicieslearnstoexplore𝜖nearbyregionofevery log1
trainingsample. 𝑓(𝑥) = 𝑥 −𝑒−𝑘𝑥.
1log𝑘
2
Theorem5.1[ExplorationduringMeta-Testing] Considergoal-
Since
orientedtaskswithgoalspaceG ⊆S.Assumethetrainingandtest- 2
𝑥𝑓′(𝑥)=𝑘𝑥𝑒−𝑘𝑥 −
inggoalsaresampledfromthedistribution𝑝(𝑥) on G,thedataset log𝑘
has𝑁 i.i.d.goalssampledfromadistribution𝑞(𝑥)onS.Iftheexplo- isincreasing for𝑥 ∈ (0,1/𝑘) and decreasing for𝑥 ∈ (1/𝑘,+∞),
rationpolicygeneralizestoexplore𝜖nearbygoalsforeverytraining ∃0<𝑥1 <1/𝑘 <𝑥2s.t.𝑓′(𝑥) >0⇔𝑥1 ≤𝑥 ≤𝑥2.Here𝑥1,𝑥2are
sample,wehavethatthetestinggoalisnotexploredwithprobability twosolutionsof𝑘𝑥𝑒−𝑘𝑥 −2/log𝑘 =0.
atmost
Therefore,toprove(20),itsufficestoshow𝑓(𝑥1) ≥0and𝑓(𝑥0) ≥
𝑃 ≈ 𝑝(𝑥)(1−𝜖𝑞(𝑥))𝑁𝑑𝑥 ≤𝑂
𝐾𝐿(𝑝||𝑞)+H(𝑝)
. (17) 0,andthelateronecanbeverifiedas
H tiner
uef oa uwil
se
.m∫
aketheassumptionthat𝜖
iss(cid:18) mallalo ng d( 𝑔𝜖𝑁 is) Lipsch(cid:19)
itzcon-
𝑓(𝑥0)=lo
1
2g l2 ol go𝑘 g 𝑘𝑘
−.
𝑘1
2
log2 1
Proof. Foranytestinggoal𝑥,everytrainingsample𝑡 ∈ Ω(𝑥) ≥ −
1log𝑘 𝑘2
enablestheexplorationpolicytoexplore𝑦duringtestingtime.As 2
Ω(𝑥)isaneighborhoodof𝑥and𝑔isLipschitzcontinuous,wecan ≥0
select𝜖 trainingsamples𝑡 fromΩ(𝑥) thatisclosestto𝑥,andwe Since𝑥1 <1/𝑘,wehave
thinkthosesamples𝑡 hasasimilarsamplingdensityfunction,i.e.,
𝑔(𝑡) ≈ 𝑔(𝑥). Thus, with 𝑁 i.i.d samples, there is approximately
2
=𝑘𝑥1𝑒−𝑘𝑥1 >
𝑘𝑥1
⇒𝑥1 <
1
.
log𝑘 𝑒 2
(1−𝜖𝑞(𝑥))𝑁 probabilitythattestinggoals𝑥 willnotbeexplored
Thus,
duringthetestingtime.Thenwehave
𝑃 fail ≈ 𝑝(𝑥)(1−𝜖𝑞(𝑥))𝑁𝑑𝑥
lo2
g𝑘 =𝑘𝑥1𝑒−𝑘𝑥1 >𝑘𝑥1(1−𝑘𝑥1) >
𝑘 2𝑥1
⇒𝑥1 <
𝑘lo4
g𝑘
∫
≤ 𝑝(𝑥)𝑒−𝑁𝜖𝑞(𝑥)𝑑𝑥
∫
log 1 +2𝑞(𝑥)+16
𝑞(𝑥)
≤ 𝑝(𝑥) 𝑑𝑥
1log(𝜖𝑁)
∫ 2
𝐾𝐿(𝑝||𝑞)+H(𝑝)
=𝑂
log(𝜖𝑁)
(cid:18) (cid:19)and theobservationandcanonlybeinferredfromthereceivedreward.
log 1 AtaskspaceTMPE ={𝐺¯(𝑛,𝑘,𝑢,𝑈,𝐿) |𝑘 =𝑛,0≤0<𝑈,𝐿∼Ψ𝑈}
𝑓(𝑥1)= 1log𝑥1
𝑘
−𝑒−𝑘𝑥1 consistsofall𝑛 M,𝑈
PEclimbgameswith𝑛playersand𝑈 landmarks,
2 andisfullycooperativebysetting𝑘 =𝑛.
log𝑘log𝑘 WeevaluateMESAonthe2-agenttasks(TMPEandTMPE)and
≥ 4 −1 2,5 2,6
1 2log𝑘 3-agenttasks(T 3,M 5PEandT 3,M 6PE)whilefixing𝑘 =2.Thetaskdistri-
log𝑘+loglog𝑘−log4
bution𝑝(T)isdefinedbytheprobabilitydensityfunction𝑝(𝐺¯(𝑛,𝑘,𝑢,𝑈,𝐿)) =
=
1log𝑘
−1 𝑈−1Ψ𝑈(𝐿).
2 Weset𝛿 = 1.Theenvironmentsaredifferentinlandmarkposi-
≥0. 2
tions,andthetasksaredifferentintargetlandmarks.
Finally,(21)isdirectfromthefact𝑒𝑥 ≥𝑥. (cid:3)
B.1.3 Multi-agent MuJoCo Domain. In the multi-agent MuJoCo
Swimmer environment, we similarly define a climb game 𝐺(𝛼)
B ADDITIONALEXPERIMENTDETAIL
wherethereare2agentsandtheangularstatesofthetwojoints
B.1 EnvironmentSettings
determinethecurrentactionofthetwoagents.Specifically,ifstate
B.1.1 Climb Game Variants. (i) One-step climb game. A one- 𝑠correspondstothetwojointsforminganglesof𝛼1and𝛼2,then
timeclimbgame𝐺(𝑛,𝑘,𝑢,𝑈)isa𝑛-playermatrixgamewhereev- therewardcanbedefinedas:
eryplayerhas𝑈 actionstochoosefrom.Therewardisdetermined 1, if|𝛼1−𝛼| <𝜖,|𝛼2−𝛼| <𝜖
bythenumberofplayerswhochooseaction𝑢,whichcanbede- 𝑅(𝑠,𝒂)= 1−𝛿, if|𝛼1−𝛼| >𝜖,|𝛼2−𝛼| >𝜖
finedas 0,
otherwise,
1, if#𝑢=𝑘,
𝑅(𝒂)= 1 0,−𝛿, i of t# h𝑢 er= wi0 s,
e.
w 𝛼anh <ge lr
e
3e
s
0◦𝜖
b }e
.i ts
w
Wa
e
eev ane lsr
−
oy
3
s0s

em
d
tea 𝛿gll =rea en 1sg .l ae n. dT 3h 0e dt ea gs rk ees sp ,a ic .ee .,c Ton =sis {t 𝛼s o |f −t 3a 0r ◦ge <t
2
wh Ter he e𝛿 ta∈ sk( s0 p,1 a) c. eTonecon
tainsall2-playerone-stepclimbgames
B.2 HyperparameterandComputationSettings
𝑈
with𝑈 actionsforeachplayer,i.e.,Tone={𝐺(2,𝑘,𝑢,𝑈) |1≤𝑘 ≤ The hyperparameters are detailed in Table 1. All tasks are sam-
𝑈
𝑛,0≤𝑢 <𝑈}. pleduniformlyatrandomfromthetaskspacedetailedinSection
(ii)Multi-stageclimbgame.Amulti-stageclimbgame 6andthendividedintothetrainingandtestingtasks.Weusedif-
𝐺ˆ(𝑆,𝑛,[(𝑘𝑡,𝑢𝑡)]𝑆 𝑡=1,𝑈)isan𝑆-stagegame,whereeachstage𝑡itself ferenttasksforthemeta-trainingstage,whichincludesthehigh-
isaone-stepclimbgame𝐺(𝑛,𝑘𝑡,𝑢𝑡,𝑈).Atstage𝑡,agent𝑖isgiven rewarddatasetcollectionandthetrainingoftheexplorationpoli-
theobservation O 𝑖𝑡 = [𝑡,ℎ𝑡−1],whereℎ𝑡−1 isthehistoryofthe cies. Weevaluate themeta-trained explorationpoliciesonnovel
jointactions. meta-testingtasksover3runswithdifferent seeds,eachconsist-
ThetaskspaceTmulticonsistsofall2-playermulti-stageclimb ingofadifferent set ofmeta-testing tasks.Computationisdone
𝑆,𝑈
gamewith𝑆stagesand𝑈actions,i.e.,T 𝑆m ,𝑈ulti=𝐺ˆ(𝑆,2,[(𝑘𝑡,𝑢𝑡)]𝑆 𝑡=1,𝑈) | ona32-coreCPUwith256GBRAMandanNVIDIAGeForceRTX
3090.
∀𝑡 1 ≤ 𝑘𝑡 ≤ 𝑛,0 ≤𝑢𝑡 <𝑈}.WechooseT 5,m 10ulti (10-Multi)forthe
experiments. B.3 BaselineMethods
Inallexperiments𝛿 issetto 1.WeuseToneandTmultiinour
2 10 5,10 ForMAPPO[41]andRandomNetworkDistillation[5],weusethe
experiments.The taskdistribution𝑝(T) isuniformover thetask
releasedcodebase1.
space. Ten training tasks aresampled fromthetaskdistribution,
ForQMIX[33]andMAVEN[24],weusethereleasedcodebase2.
and threetesting tasksthat are different fromthe training tasks
InMA-MuJoCoenvironments,wediscretizetheactioninto11even
arechosentoevaluatetheperformance.
pointsforQMIX.
B.1.2 MPEDomain. InaMPEClimbGame𝐺¯(𝑛,𝑘,𝑢,𝑈,𝐿),there ForMAESN[14],wemodifythesingle-agentversiontoamulti-
are𝑈non-overlappinglandmarksonthemapwithpositions{𝐿𝑗}𝑈 𝑗=− 01. agentversionbytreatingagentsasindependentlyoptimizingtheir
Weassumeadistribution𝐿 ∼ Ψ𝑈 fromwhichthelandmarkposi- ownreturnswithoutconsiderationoftheotheragents.
tions𝐿.Therewardisdeterminedbythenumberofagentslocating
ForEMC[29],weusethereleasedcodebase3.
onthe𝑢-thlandmark.Moreformally,suppose𝑓𝑗(𝑠)isthenumber Formeta-training-based methods,wepretrainapolicy(condi-
ofagentslocatingonthe𝑗-thlandmark,therewardcanbedefined tioned or unconditioned) with the same configuration of MESA
as (numberoftasks,numberoftrainingsteps)andthendeployiton
1, if𝑓𝑢(𝑠)=𝑘and 𝑈 𝑗=− 01𝑓𝑗(𝑠)=𝑛, t mh ae tm ioe nta o- fte thst ein tag st kas gk o. aT lh (ke eg yo aa cl- tc ioo nnd foit rio cn lie md bp go ali mcy e,ta kk ee ys lt ah ne di mnf ao rr k-
𝑅(𝑠,𝒂) = 01 ,−𝛿, oif th𝑓𝑢 e( r𝑠 w) i= se0
.
and ÍÍ𝑈 𝑗=− 01𝑓𝑗(𝑠)=𝑛,
1 2h ht tt tp ps s: :/ // /g gi it th hu ub b. .c co om m/ /m Ana url jb Men ahch am janar Ok x/o f/n M-p Ao Vli Ec Ny
3https://github.com/kikojay/EMC
The observationof agent 𝑖 contains the relative positions of all
landmarksandotheragents.Asbefore,𝑢and𝑘willnotbegiveninHyperparameter Value
off-policyMARLAlgorithm𝑓 MADDPG
50kinOne-stepClimbGame
100kinMulti-stageClimbGame
Meta-trainingsteps
3MinMPE
3MinMA-MuJoCo
30KinClimbGame
High-rewarddatacollectionsteps 500kinMPE
1MinMA-MuJoCo
10inClimbGame
Meta-trainingtasksize
30inMPEandMA-MuJoCoSwimmer
50kinOne-stepClimbGame
100kinMulti-stageClimbGame
Meta-testingsteps
3M/6MinMPE
2.5MinMA-MuJoCoSwimmer
5inClimbGame
Meta-testingtasksize 15,18in5-agentMPEand6-agentMPE
6inMA-MuJoCoSwimmer
3000stepsinClimbGame
Randomexploration
50KstepsinMPEandMA-MuJoCo
RecurrentNeuralNetwork
Networkarchitecture
(oneGRUlayerwith64hiddenunits)
★
Threshold𝑅 1(sparse-rewardtasks)
Relabel𝛾 0.05
Decreasingfunction𝑓 1/𝑥5
𝑑
DistanceMetrick·k L2norm
F
4inClimbGame
NumberofExplorationPolicies 2/4inMPE
2inMA-MuJoCo
Learningrate 5e-3/1e-4(Adamoptimizer)
32trajectoriesinClimbGame
Batchsize 300trajectoriesinMPE
8trajectoriesinMA-MuJoCo
Table1:HyperparametersusedinMESA
B.4 ResultsOnAllEnvironments
Table2 gives the final performance foreach algorithmin all en-
vironments.WeobservethatourproposedMESAoutperformsall
otherbaselinemethodsacrossallenvironments.
C VISUALIZATIONOFLEARNED
EXPLORATION POLICIES
We visualize two exploration policiesin the 2-agent 3-landmark
Figure8:Visualizationoftwolearnedexplorationpoliciesin MPEClimbGametasks.BothexplorationpoliciesareshowninFig-
2-agent3-landmarkMPEtasks.Thefigureshowsthecritical ure8.Inaddition,thelearnedpolicyvisitedthethreelandmarks
stepsinthetrajectorywhereagentscoordinatelyreachhigh- within20timesteps,lessthanathirdofthelengthofthetrajectory,
rewardingstatesthatwerepreviouslycollected. whichshowcasesitsabilitytoquicklycoverthecollectedpromis-
ingsubspace.Bothpoliciessuccessfullyvisitedallthreelandmarks
consecutivelyandwithinonly1/3oftheepisodelength.
idforMPEDomain,keyanglesforMA-MuJoCo)asadditionalob-
servation.WeadaptC-BET[26]tomulti-agentbasedonMAPPO D ABLATION STUDIES
[41].
Tofigureouttheextenttowhichthehigh-rewardingstate-action
datasetM∗ andthetrainedexplorationpoliciescontributetothe
overallbetterperformance,weperformanablationstudyonthe
MPEdomain.One-step Multi-stage MA-MuJoCo
2A5LMPE 2A6LMPE 3A5LMPE 3A6LMPE
ClimbGame ClimbGame Swimmer
MESA 0.83±0.20 0.81±0.06 61.32±8.24 58.73±10.16 51.83±13.37 44.71±15.92 599.32±35.93
MADDPG 0.74±0.25 0.50±0.00 21.09±23.07 19.44±18.41 5.45±6.49 3.16±5.73 499.33±0.52
MAPPO 0.50±0.00 0.54±0.05 24.42±2.99 27.57±4.86 10.12±5.83 13.52±2.91 496.88±1.98
MAPPO-RND 0.50±0.00 0.53±0.04 24.05±2.45 27.10±5.67 17.18±3.52 7.30±2.76 496.36±1.76
QMIX 0.50±0.00 0.50±0.00 0.21±0.04 0.48±0.12 0.06±0.02 0.04±0.03 499.97±0.03
MAVEN 0.50±0.00 0.50±0.00 0.06±0.03 0.13±0.10 0.06±0.05 0.07±0.00 495.41±2.63
EMC 0.50±0.00 0.50±0.00 50.49±17.26 50.61±17.41 36.63±0.11 36.47±0.28 499.66±0.20
Pretrain 0.55±0.00 0.55±0.00 23.26±2.19 29.81±4.97 17.91±2.63 17.72±5.28 496.49±1.33
CBET 0.55±0.00 0.55±0.00 23.69±1.01 28.37±3.59 19.85±5.05 19.80±4.36 497.54±1.49
Goal 0.55±0.00 0.55±0.00 31.61±0.90 32.14±0.10 30.60±1.03 30.44±0.60 498.13±0.45
Table2:Summaryoffinalperformanceforallthealgorithmsevaluatedonalltheenvironments.Numbersinboldindicate
thebestperforminginaparticularenvironment.Forsimplicity,2A5LMPEstandsfor2-agentMPEwith5landmarksandthe
sameforotherMPEresults.
WeobserveinFigure9thatbyinitializingthebufferwithM∗,
the training process is accelerated (from 1M steps to 2M steps).
However,eventhoughM∗containsthecollectedhigh-rewardstates,
directlylearningtheintrinsicstructureandgeneralizingtounseen
meta-testingtasksisnontrivial.Hence,MADDPGwithM∗initial-
izationstillfailstolearntheoptimalNE.
Butwhenassistedwiththetrainedexplorationpolicies,thealgo-
rithmisabletofindtheglobaloptimumwhilealsotrainingfaster
than the vanilla MADDPG. The greater variance of the ablated
methodalsosuggeststhatM∗containsusefulbutsubtleinforma-
tionforlearning,andtheexplorationpolicieshelpwithextracting
thatinformation.
Figure 9: Ablation study.Comparing our approach with1)
vanilla MADDPG without using the high-rewarding state-
action dataset M∗ or the trained exploration policies and
2) initializingthereplaybufferwiththeM∗ but notusing
theexplorationpolicies,weshowthatbothcomponentscon-
tributetooverallperformance.