Multi-Space Alignments Towards Universal LiDAR Segmentation
YouquanLiu∗,1 LingdongKong∗,2,3 XiaoyangWu4 RunnanChen4 XinLi5
LiangPan2 ZiweiLiu6 YuexinMa1
1ShanghaiTechUniversity,2ShanghaiAILaboratory,3NationalUniversityofSingapore
4UniversityofHongKong,5EastChinaNormalUniversity,6S-Lab,NanyangTechnologicalUniversity
https://github.com/youquanl/M3Net
Abstract
AunifiedandversatileLiDARsegmentationmodelwith
strongrobustnessandgeneralizabilityisdesirableforsafe
autonomousdrivingperception.ThisworkpresentsM3Net,
a one-of-a-kind framework for fulfilling multi-task, multi-
dataset, multi-modality LiDAR segmentation in a univer-
salmannerusingjustasinglesetofparameters. Tobetter
exploit data volume and diversity, we first combine large-
scaledrivingdatasetsacquiredbydifferenttypesofsensors
from diverse scenes and then conduct alignments in three
spaces, namely data, feature, and label spaces, during the
training. As a result, M3Net is capable of taming hetero-
geneousdatafortrainingstate-of-the-artLiDARsegmenta-
tion models. Extensive experiments on twelve LiDAR seg-
mentationdatasetsverifyoureffectiveness. Notably, using
asharedsetofparameters,M3Netachieves75.1%,83.1%,
and72.4%mIoUscores,respectively,ontheofficialbench-
marksofSemanticKITTI,nuScenes,andWaymoOpen. Figure 1. Performance comparisons among M3Net [•], Single-
DatasetTraining[•],andNa¨ıveJointTraining[•]acrosstwelve
LiDARsegmentationdatasets. Forbettercomparisons,theradius
isnormalizedbasedonM3Net’sscores.Thelargertheareacover-
1.Introduction
age,thehighertheoverallperformance.Bestviewedincolors.
Denseandstructural3Dsurroundingsceneunderstand-
ingprovidescrucialinformationforautonomousvehiclesto
sensortypesandweatherconditions,diverseclassdistribu-
makeproperdecisions[72]. Withtherecentadvancements
tions arising from varying capture scenarios, and distinct
insensingtechnologies,especiallytheLightDetectionand
labelspacesshapedbyspecificannotationprotocols. These
Ranging(LiDAR)sensor,aholisticsceneperceptioncanbe
factors collectively pose a formidable challenge in harmo-
achievedbysegmentingtheacquiredsensordata[29,85].
nizing disparate LiDAR point clouds and jointly optimiz-
MostexistingLiDARsegmentationmodels[1,38,113,
ing model parameters to effectively address multiple tasks
123, 130] are trained and tested in a single-task, single-
across a range of sensor modalities [91, 120]. Empirical
dataset, single-modality manner. Despite achieving com-
evidence in Fig. 3 further reveals that na¨ıvely combining
mendable results in the single domain, there is a signifi-
heterogeneousdatatotrainaLiDARsegmentationmodel–
cant performance drop when transitioning to new domains
withoutstrategicalignments–oftenleadstosub-optresults.
[42,50]. Thelimitedgeneralizationcapabilityhinderstheir
Recent works [7, 41, 50, 81, 89, 96, 108] resort to un-
facilitation of real-world applications [49, 52, 89]. In re-
supervised domain adaptation (UDA) for utilizing training
ality, LiDAR datasets are marred by significant variances,
data from both source and target domains to optimize one
encompassing variations in data patterns due to different
parameter set. Nevertheless, they either focus on only the
∗Thefirsttwoauthorscontributedequallytothiswork. sharing mapping between two domains (by ignoring dis-
1
4202
yaM
2
]VC.sc[
1v83510.5042:viXrajoint classes) or directly merge source domain labels to mentations [77, 86, 107], etc. Most recently, researchers
align with the target domain [42, 115]. The overlook of started to explore data efficiency [51, 58], annotation ef-
the performance degradation on the source dataset and the ficiency [59, 65, 67, 88, 99], annotation-free learning [11,
destructionoforiginallabelmappingsinevitablyconstrains 12, 124], zero-shot learning [13, 71], domain adaptation
such a learning paradigm. Furthermore, there have been [7,41,50,56,75,81,108], androbustness[49]inLiDAR
efforts [87, 98, 106, 120] to employ multi-dataset learning segmentation, shedding lights for practitioners. Existing
strategies to bolster the generalization prowess of 3D per- pursues, however, learn separate parameter sets for each
ception models. However, they either necessitate dataset- dataset, impeding the scalability. This motivates us to ex-
specificfine-tuning, deviatingfromatrulyuniversallearn- plore LiDAR segmentation in a multi-task, multi-dataset,
ingapproach, orconvergelabelspacestoacoarserset, re- multi-modalitymannerwithjustasinglesetofparameters.
sultinginthedilutionoffine-grainedsegmentationcapabil-
Multi-TaskLearning. Aproperpipelinedesigncoulden-
itiesacrossdiversesemanticcategories.
ablethemodeltogeneratesuitablepredictionstofulfillmul-
Inthiswork,wedefineanovelparadigmtowardslever-
tiple tasks simultaneously [17, 33]. The current research
aging LiDAR point clouds from different datasets to tame
endeavors mainly focus on building image or video seg-
asinglesetofparametersformulti-taskLiDARsegmenta-
mentationmodelstohandlesemantic,instance,andpanop-
tion. Sibling to image segmentation communities [45, 55,
tic segmentation tasks [40, 61, 100, 102, 121, 122, 132].
126], we call this paradigm universal LiDAR segmenta-
Recently,severalattemptshavebeenmadetoenablemulti-
tion.Theultimategoalofsuchasynergisticwayoflearning
task segmentation on LiDAR point clouds. MaskRange
is to build a powerful segmentation model that can absorb
[32] and MaskPLS [73] extend the mask classification
richcross-domainknowledgeand,inreturn,achievestrong
paradigm[16]forjointsemanticandpanopticLiDARseg-
resilience and generalizability for practical usage. Given
mentation. LidarMultiNet [119] uses global context pool-
thesubstantialdifferences amongdatasetsintermsof data
ing and task-specific heads to handle LiDAR-based detec-
characteristics, feature distributions, and labeling conven-
tion and segmentation. P3Former [110] proposed a spe-
tions,weintroduceacomprehensivemulti-spacealignment
cialized positional embedding to handle the geometry am-
approach that encompasses data-, feature-, and label-level
biguity in panoptic LiDAR segmentation. Our framework
alignments,toeffectivelypavethepathforefficientanduni-
also supports multi-task learning. Different from existing
versallyapplicableLiDARsegmentation. Inparticular,the
approaches, the proposed M3Net stands out by combining
multi-modal data, including images and texts, is fully ex-
knowledge from different sensor data across multiple data
ploited to assist the alignment process with the guidance
sources,whichachievessuperiorperformanceoneachtask.
ofmoregeneralknowledge. Throughaforementionedpro-
Multi-Dataset Learning. Leveraging data samples from
cesses, we propose M3Net to learn common knowledge
different sources for training has been proven effective in
acrossdatasets,modalities,andtasks,therebysignificantly
enhancingrobustnessandgeneralizability[74]. Variousap-
enhancingitsapplicabilityinpracticalscenarios.
proaches have been proposed to merge image datasets for
To substantiate the efficacy of M3Net and the utility of
object detection [15, 60, 62, 101, 127, 128], image seg-
eachmoduledeveloped,wehavecarriedoutaseriesofthor-
mentation [31, 44, 45, 55, 126], depth estimation [14, 84],
ough comparative and ablation studies across an extensive
etc. Due to large domain gaps, the image-based methods
array of driving datasets, as shown in Fig. 1. Notably, our
are often hard to be transferred to 3D. To combine multi-
best model achieves state-of-the-art LiDAR segmentation
ple LiDAR datasets for 3D object detection, MDT3D [91]
performance with 75.1%, 83.1%, 72.4% mIoU scores on
definesacoarselabelsettohandlethelabelspaceconflicts
SemanticKITTI [3], nuScenes [28], Waymo Open [92], re-
indifferentpointclouddatasets. MS3D++[97,98]ensem-
spectively,usingasharedsetofparameters. Moreover,our
blespre-traineddetectorsfromdifferentsourcedatasetsfor
approach also performs well for direct knowledge transfer
multi-domain adaptation. Uni3D [120] resorts to dataset-
andout-of-distributionadaptations,furtherunderscoringits
specificdetectionheadsandfeaturere-couplingfortraining
robustcapabilityforeffectiveknowledgetransfer.
aunified3Dobjectdetector. Recently,PPT[106]proposed
topre-trainapointcloudsegmentationnetworkusingdata
2.RelatedWork
from multiple datasets. However, the pre-trained weights
LiDARSegmentation. Aholisticperceptionof3Dscenes are then fine-tuned on each specific dataset, which breaks
is crucial for safe autonomous driving [4, 8, 35, 53, 63]. the universal learning manner. The closest work to us is
Various LiDAR segmentation models have been proposed, COLA [87], which trains a single model across multiple
with distinct focuses on aspects include LiDAR repre- sources by converting dataset-specific labels to a common
sentations [21, 76, 93–95, 105, 123, 130], model archi- coarseset. Suchaconversion,however,leadstothelossof
tectures [1, 18, 25, 37, 48, 54, 82, 114], sensor fusion fine-grainedsegmentationacrossthevarioussemanticcate-
[19,64,66,115,131],post-processing[113,125],dataaug- gories. Differently, our M3Net is tailored to tame a single
2Figure2.StatisticalanalysisofsixsharingsemanticclassesinthenuScenes[•],SemanticKITTI[•],andWaymoOpen[•]datasets.Each
violinplotshowstheclassdistributionacrossLiDARscenesspanning50meters,centeredaroundtheego-vehicle.Bestviewedincolors.
parametersettofulfillmulti-taskpredictionacrossmultiple datasetsforbettergeneralizability. However,asdepictedin
datasetswhilestillmaintainingtheoriginallabelmappings. Fig. 2, it is often non-trivial to na¨ıvely combine heteroge-
Multi-ModalityLearning. Recenttrendfavorssynergistic neousdatawithlargedatadistributiongapstotrainauniver-
learning from data of different modalities, such as vision, salLiDARsegmentationmodelwithoutproperalignments.
language,andspeech[2,9,20,27,78,83,103].ForLiDAR To testify this, we conducted a pilot study using the prior
segmentation,severalworks[10,41,42,69,117]explored artMinkUNet[21]forbothstandaloneandjointtrainingon
thedistillationofimagefeaturestopointclouds. Recently, threelarge-scaledatasets[3,28,92]. AsshowninFig.3(a)
OpenScene[80]andCLIP2Scene[12]proposedtoleverage and(d),abrutalcombinationunderminesthesegmentation
point clouds along with multi-view images and language performance.Duetolargediscrepanciesinaspectslikesen-
foropen-vocabularylearning. PPKT[68],SLidR[88],and sor configurations, data acquisitions, label mappings, and
Seal[67]formcross-sensorcontrastivelearningobjectives domainshifts,thejointlytrainedrepresentationstendtobe
to pre-train the LiDAR segmentation models. The advan- disruptiveinsteadofbeingmoregeneral.
tagesofsensorfusionhavebeenconsistentlyproven.Inthis LiDARSensorDiscrepancy. Tounderstandtherootcause
work,topursueuniversalLiDARsegmentation,wepropose of performance degradation, we conducted another study
toalignmulti-spacepointcloudsviaimagesandtexts. thatcontrolspointclouddensitydiscrepancieswhenmerg-
ingdatasets.AsshowninFig.3(b)and(c),jointtrainingon
3.Approach datacollectedbysensorswithdifferentbeamnumberstends
to suffer more severely than merging less density variant
Ourstudyservesasanearlyattemptatcombiningmulti-
data. Wehypothesizethatthisismainlycausedbythedata
task,multi-dataset,multi-modalityknowledgeintoasingle
statisticalvariations. Inlightoftheseobservations,wepro-
setofparameterstofulfilluniversalLiDARsegmentation.
pose a bag of suitable operations in the following sections
Westartwithapilotstudytounveilthedifficultiesinmerg-
to alleviate the large domain gaps among different LiDAR
ing heterogeneous LiDAR point clouds (cf. Sec. 3.1). We
segmentationdatasets[3,4,8,28,92].
then present M3Net, a versatile LiDAR segmentation net-
worktailoredtopursuei)statisticalconsistencyinthedata
3.2.Data-SpaceAlignment
space (cf. Sec. 3.2), ii) cross-modality-assisted alignment
inthefeaturespace(cf.Sec.3.3),andiii)language-guided GivenatotalofS datasetsDs ={(xs,ys)|1≤s≤S},
unificationinthelabelspace(cf.Sec.3.4). where (xs,ys) denotes the data-label pairs constituting a
dataset. For the LiDAR segmentation task, xs often en-
3.1.PilotStudy
compasses the LiDAR point cloud Ps = {p ,p ,p }s ∈
x y z
The current de facto of training a LiDAR segmenta- RN×3 and synchronized multi-view camera images Vs =
tion network adopts a task-by-task and dataset-by-dataset {I 1,...,I l}|l = 1,...,L}, where I t ∈ RH×W×3, N is the
pipeline. Despitethesuperiorperformanceachievedunder numberofpoints,Ldenotesthenumberofcamerasensors,
such standalone settings, the trained parameter sets cannot H and W are the height and width of the image, respec-
besharedtosatisfyout-of-domainrequirementsand,there- tively. ys ∈ RN denotes point cloud labels in the label
fore,limitstheirusecasesforpracticalapplications.
spaceYs,weunifythelabelspaceasYu =Y1∪Y2...∪YS.
Na¨ıveJointTraining. Anaturalalternativetobreakingthe Cross-ModalityDataAlignment. Asamulti-sensingsys-
aboveconstraintistojointlytrainanetworkacrossmultiple tem, the information encoded in Ps and Vs are intuitively
i i
3Standalone Training Joint Training (Same # Beam) Decoupled BN. Another challenge in training across mul-
tipledatasetsisthepresenceofdomaingaps,whichcanre-
nuScenes sultinsignificantstatisticalshiftsoffeaturelearningamong
datasets. Such shifts can hinder the convergence and af-
KITTI -4.2
fect the model’s ability to generalize well across diverse
Waymo -3.5 datasets. Weadoptadecoupledbatchnorm(BN)forpoint
cloud features in each dataset. Instead of using the tradi-
mIoU (%) 52 59 66 73 mIoU (%) 52 59 66 73
a b tional BN, which calculates mean and variance across all
c d samples in a mini-batch, the decoupled BN tends to adapt
Joint Training (32 & 64 Beam) Joint Training (All Together) eachdataset’sspecificcharacteristicsindependently.
3.3.Feature-SpaceAlignment
-8.7 -13.5
-7.7 -9.6 We aim to acquire a generalized feature representation
for downstream tasks. Compared to point clouds, images
-5.9 contribute stronger visual, textural, and semantic informa-
mIoU (%) 52 59 66 73 mIoU (%) 52 59 66 73 tion. Thus, the collaboration between pixels and points
could enrich the overall representation. Previous research
Figure3. Apilotstudyofna¨ıvelymergingdifferentdatasetsfor
[12,62,66]hasconsistentlydemonstratedthatsuchacom-
trainingtheMinkUNet[21]model. Comparedtothestandalone
binationresultsoftenleadstoimprovedperformance.
trainingin(a),eitherjointlytrainingwith(b)thesame,(c)differ-
ent,or(d)allsensor-acquireddatawillcauseseveredegradation. Cross-Modality Assisted Alignment. In the context of
multi-dataset joint training, our objective is to establish a
unifiedfeaturespacebyleveragingimagefeaturestoassist
complementary to each other [3, 8, 92]. To leverage such
point cloud features. Acknowledging that images used in
an advantage, we resort to the correspondences embed-
training lack ground truth labels [3, 28], we utilize image
ded in camera calibration matrices to bridge the LiDAR
featuresfromapre-trainedmodelasanalternative,facilitat-
pointsandcameraimagepixels. Specifically,foreachpoint
p = (px,py,pz)inPs, thecorrespondingpixel(u,v)can ing a more universally applicable representation. We feed
camera images Vs into a pre-trained DeepLab [100] and
befoundbythefollowingtransformations:
a vision-language model (VLM) and visualize the output
1
[u,v,1]T = ·T ·T ·[px,py,1]T, (1) imagefeaturesbyt-SNE[26]. AsshowninFig.4, weob-
pz s serve that image features from DeepLab appear disorderly
whereT ∈R4×4isthecameraextrinsicmatrixthatconsists andlacksemantics. Incontrast,featuresfromVLMsharea
ofarotationmatrixandatranslationmatrix,andT ∈R3×4 moreunifiedfeaturespace. Motivatedbythis, wepropose
s
across-modalityassistedalignmentthatusesVLMtohelp
isthecameraintrinsicmatrix. Aswewillshowinthefol-
alignthefeaturespace. Specifically,thecameraimagesVs
lowingsections, suchacross-sensordataalignmentserves
arefedtothefrozenimageencoderfromVLMtoobtainim-
asthefoundationforalignmentsinotherspaces.
agefeaturesF ={F1,F2,...,Fs},whereFs ∈Rc×h×w.
Cross-Sensor Statistical Alignment. To mitigate the dis- v v v v v
TheLiDARpointcloudsPs, ontheotherhand, arefedto
crepancies in sensor installations across different datasets,
the point encoder followed by a projection layer to gen-
we incorporate a point coordinate alignment operation.
erate the point features F = {F1,F2,...,Fs}, where
Specifically, drawing upon insights from prior domain p p p p
Fs ∈ Rm×c; m denotes the number of non-empty grids.
adaptationapproaches[104,118],weadjustthecoordinate p
originsofpointcloudsfromdifferentdatasetsbyintroduc- We then leverage the paired image features Fˆ v ∈ Rmp×c
inganoffsetσ ∈R1×3tothegroundplane. Wefindempir- andpointfeatureFˆ p ∈ Rmp×c foralignment,wherem p is
icallythatsuchanalignmentcanlargelyreducethedegra- thenumberofpoint-pixelpairs. AfterobtainingFˆ andFˆ ,
p v
dationcausedbythevariationsindifferentsensorsetups. thecross-modalityalignmentisexpressedasfollows:
Dataset-Specific Rasterization. It is conventional to ras-
terize LiDAR point clouds Ps using unified rasterization L (Fˆ ,Fˆ )=1− Fˆ v·Fˆ p . (2)
parameters, e.g., voxel size [93, 130] or horizontal range cma v p ∥Fˆ ∥·∥Fˆ ∥
v p
view resolution [76, 113]. However, the point clouds ac-
quired in different LiDAR datasets naturally differ in den- Domain-Aware Cross-Modality Alignment. With cross-
sity, range, intensity, etc., which tends to favor different modality alignment, we transfer the knowledge of VLM
rasterization parameters. To meet such a requirement, we to the point encoder, enabling the point features to gain
select dataset-specific parameters for rasterization on each a more comprehensive representation. However, during
datasetthroughempiricalexperimentsandanalyses. the execution of the above alignment, we have narrowed
4modality assisted alignment. The sets of features from all
datasets are concatenated along the channel dimension to
form F(cid:101)v ∈ Rcv×h×w. Subsequently, we sequentially feed
F(cid:101)v throughabranchthatconsistsofaglobalaveragepool-
ing and an MLP. Simultaneously, F(cid:101)v is fed to an auxiliary
branchthatundergoesthesameprocessingflowandgener-
atesanoutputafterthesoftmaxfunctionG(·). Theoutputs
a b
frombothbranchesaremultipliedtoobtainF
m
∈Rcv×1×1.
Theoverallprocesscanbedescribedasfollows:
c d
F
m
=MLP(Pool(F(cid:101)v))·G(MLP(Pool(F(cid:101)v))). (3)
Next,weforwardF toasigmoidactivationfunctionH(·)
m
andmultiplyitwithinputimagefeaturesF(cid:101)v. Theresulting
output is added to F(cid:101)v and passed through the MLP layers
toobtainthefinalimagefeaturesF ∈ Rc×h×w. Thefor-
vf
wardprocessofthisoperationisdepictedasfollows:
Figure 4. The t-SNE plots of learned features before and af-
terthefeature-spacealignmentinmergingthenuScenes[•], Se-
F
vf
=MLP((H(F m)·F(cid:101)v)+F(cid:101)v). (4)
manticKITTI [•],andWaymoOpen[•]datasets. Weshowimage
Finally, we leverage the cross-modality data alignment to
f ce loat uu dre fs eaf tr uo rm es( (a c) )bst ea fn od real ao nn de (n de )t aw fo ter rk ts h; e(b fe) aS tuA reM -sp[ a4 c6 e], aa lin gd nmpo ei nn tt
.
acquire paired image features Fˆ
vf
∈ Rmp×c and paired
pointfeatureFˆ . Theoverallobjectivefunctionis:
p
Fˆ ·Fˆ
L (Fˆ ,Fˆ )=1− vf p . (5)
v2p vf p ∥Fˆ ∥·∥Fˆ ∥
vf p
3.4.Label-SpaceAlignment
LabelConflict. Inmulti-datasetjointtrainingsettings, la-
bel conflicts emerge as a significant challenge. This often
refersto theinconsistenciesin classlabelsacross different
datasets involved in the training process. The discrepancy
canariseduetovariationsinannotationconventions,label-
ing errors, or even differences in the underlying semantics
of classes between datasets. In our baseline, we unionize
thedifferentlabelspacesacrossdatasetsintoYu,whereall
datasets share a single LiDAR segmentation head. How-
ever,thismayintroduceseveralpotentialdrawbacks:
• Loss of granularity: Unified label spaces could lose se-
Figure5. Feature-spacealignmentinM3Net. Weleverageboth
manticgranularity,particularlywhendealingwithsubtle
imagefeaturesF andLiDARpointcloudfeaturesF extracted
v p
fromimageencoderE andpointencoderE toemploythe categorydifferencesinbetweendifferentdatasets.
img pcd
regularizationviaV2Plossandachievefeature-spacealignment. • Information loss: During label space consolidation, de-
tails unique to each dataset may be obscured or lost, es-
peciallyforthoserelatedtodomain-specificcategories.
it exclusively to image and point features from the same • Increasedcomplexity:Handlingaunifiedlabelspacemay
dataset. In this mode, point features solely learn from necessitatemorecomplexmodelarchitecturesortraining
matching image features, restricting their knowledge ac- strategies,therebyincreasingoverallcomplexity.
quisition. Ideally, we aim to ensure that image features Toaddresstheseissues,weintroducealanguage-guided
encompass not only scenes identical to those represented label-space alignment to facilitate a more holistic seman-
in point clouds but also scenes from other datasets. To ticcorrelationacrossdatasets. Giventhenaturalcorrespon-
address this, we propose a domain-aware cross-modality dencebetweenimagesandtextsandthestrongcorrelation
guided alignment, as illustrated in Fig. 5. Specifically, we between images and point clouds, we aim to strategically
firstextract,foreachdataset,F andF fromthesameim- utilizetheimagemodalityasabridgetoestablishlanguage-
v p
ageencoderE andpointencoderE duringthecross- guidedalignments. Suchaprocessconsistsofatext-driven
img pcd
5Cross-Modality-Assisted Label Alignment. After text-
drivenalignments,thesubsequentcrucialstepentailsalign-
ing the point and image modalities within the label space.
We first obtain image logits F = {F1,F2,...,Fs}
vl vl vl vl
andpointlogitsF = {F1,F2,...,Fs}fromtext-driven
pl pl pl pl
alignments, where Fs ∈ RQ×H×W, Fs ∈ RN×Q. Sub-
vl pl
sequently, we conduct cross-modality alignment to obtain
paired image logits Fˆ
vl
∈ Rmp×Q and paired point log-
its Fˆ
pl
∈ Rmp×Q. Formally, the cross-modality-assisted
alignmentinthelabelspaceisformulatedasfollows:
Fˆ ·Fˆ
L (Fˆ ,Fˆ )=1− vl pl . (8)
i2p vl pl ∥Fˆ ∥·∥Fˆ ∥
vl pl
Finally, the complete objective function for the language-
Figure6. Label-spacealignmentinM3Net. Weleverageimage
guidedlabel-spacealignmentisexpressedasfollows:
featuresF , pointcloudfeaturesF , andtextembeddingF ex-
v p t
tractedfromE ,E ,andE ,respectively,forregularization
img pcd txt
L =L +L +L . (9)
viatheI2P,P2T,andV2Tlossesinthelabel-spacealignment. label p2t i2p v2t
3.5.UniversalLiDARSegmentation
pointalignment,atext-drivenimagealignment,andacross-
modality-assistedlabelalignment. We enhance the versatility of M3Net via multi-tasking
Text-DrivenAlignments.AsdepictedinFig.6,imagesVs learning. Thisintegrationinvolvesaninstanceextractorto
are fed into the frozen image encoder E to extract the enablejointsemanticandpanopticLiDARsegmentation.
img
image features F . Concurrently, the LiDAR point clouds PanopticLiDARSegmentation.MotivatedbyDSNet[34,
v
Ps areprocessedbythepointencoderE togeneratethe 35], ourinstanceextractorcomprisesaninstanceheadand
pcd
pointfeaturesF .Additionally,giventhetextinputTs,text a clustering step. The instance head encompasses several
p
embeddingfeaturesF ∈ RQ×c areobtainedfromafrozen MLPsdesignedtopredicttheoffsetsbetweeninstancecen-
t
textencoderE ,whereQrepresentsthenumberofcate- ters. Theclusteringstepusessemanticpredictionstofilter
txt
goriesacrossdatasets. Thetextiscomposedofclassnames outstuff points,therebyretainingonlythoseassociatedwith
from unified label space Y placed into pre-defined tem- thing points. The remaining points undergo a mean-shift
u
plates, and the text embedding captures semantic informa- clustering[22],utilizingfeaturesfromtheinstanceheadto
tionofthecorrespondingclasses. Subsequently, pixel-text discern distinct instances. Lastly, we employ the L1 loss
pairs{v k,t k}M
k=1
andpoint-textpairs{p k,t k}M
k=1
aregen- L l1tooptimizethethingpointregressionprocess.
erated, whereM representsthenumberofpairs. Leverag- OverallObjectives. Puttingeverythingtogether,theover-
ingthesemanticinformationcontainedinthetext,weselec- allobjectiveofM3Netistominimizethefollowinglosses:
tivelychoosepositiveandnegativesamplesforbothimages
and points for contrastive learning. It is noteworthy that L=L v2p+L label+L ce+L lovasz+L l1, (10)
negative samples are confined to the specific dataset cate-
where L and L denote the cross-entropy loss and
ce lovasz
gory space. The overall objective of the text-driven point
theLovasz-softmax[5]loss,respectively.
alignmentfunctionisshownasfollows:
L
=−(cid:88)Q
log(
(cid:80) tk∈q,pkexp(<t k,p
k
>/τ)
),
4.Experiments
p2t (cid:80) exp(<t ,p >/τ) 4.1.ExperimentalSetups
q=1 tk∈q,tk∈/q,pj k k
(6) Datasets. OurM3Netframeworkandbaselinesaretrained
where t k ∈ q indicates that t k is generated by the q-th on a combination of nuScenes [28], SemanticKITTI [3],
classesname,andQisthenumberofclasses. Symbol<,> and Waymo Open [92]. Meanwhile, we resort to another
denotesthescalarproductoperationandτ isatemperature fiveLiDAR-basedperceptiondatasets[43,47,79,108,109]
term (τ > 0). Similarly, the objective of the text-driven andtwo3Drobustnessevaluationdatasets[49]toverifythe
imagealignmentfunctionisillustratedasfollows: strong generalizability of M3Net. Due to space limits, ad-
L
=−(cid:88)Q
log(
(cid:80) tk∈q,vkexp(<t k,v
k
>/τ)
).
d Imiti po ln ea mld enet ta ai tl is or neg Dar ed ti an ig ls.the Mda 3t Nas ee tts isar ie mi pn leth me eA ntp ep den bd ai sx e.
d
v2t (cid:80) exp(<t ,v >/τ)
q=1 tk∈q,tk∈/q,vj k k on Pointcept [24] and MMDetection3D [23]. We use two
(7) backbones in our experiments, i.e., MinkUNet [21] and
6Table1. AblationstudyontheM3NetalignmentshappenintheData, Feature, andLabelspaces, respectively, whencombiningthe
SemanticKITTI[3],nuScenes[28],andWaymoOpen[92]datasets.ThemAccandmIoUscoresareinpercentage.Bestscoresareinbold.
MinkUNet[21] PTv2+[105]
- Configurations SemKITTI nuScenes Waymo SemKITTI nuScenes Waymo
mAcc mIoU mAcc mIoU mAcc mIoU mAcc mIoU mAcc mIoU mAcc mIoU
Baseline Na¨ıveJointTraining 62.43 54.03 65.05 59.84 73.76 65.39 67.96 61.59 76.53 69.65 75.68 67.00
Data Feature Label - - - - - -
✓ 73.82 69.01 83.66 76.89 77.88 69.37 78.55 69.95 86.22 79.13 80.96 72.15
M3Net ✓ ✓ 74.36 69.64 85.17 78.88 78.31 69.70 79.43 70.87 87.10 80.26 80.74 72.33
(Ours) ✓ ✓ 73.85 69.34 85.20 78.90 78.04 69.55 80.30 71.13 87.44 80.45 80.69 72.30
✓ ✓ ✓ 74.40 69.85 85.30 79.00 78.66 70.15 80.00 72.00 87.91 80.90 81.11 72.40
Table2. PanopticLiDARsegmentationresultsonthevalsetsofthePanoptic-SemanticKITTI[3]andPanoptic-nuScenes[28]datasets.
Allscoresaregiveninpercentage.Thebestandsecond-bestscoresarehighlightedinboldandunderline,respectively.
Panoptic-SemanticKITTI Panoptic-nuScenes
Method Configurations
PQ PQ† RQ SQ mIoU PQ PQ† RQ SQ mIoU
Panoptic-TrackNet[39] 40.0 - 48.3 73.0 53.8 51.4 56.2 63.3 80.2 58.0
Panoptic-PolarNet[129] 59.1 64.1 70.2 78.3 64.5 63.4 67.2 75.3 83.9 66.9
EfficientLPS[90] Single-DatasetTraining 59.2 65.1 69.8 75.0 64.9 59.2 62.8 82.9 70.7 69.4
DSNet[34] 61.4 65.2 72.7 79.0 69.6 64.7 67.6 76.1 83.5 76.3
Panoptic-PHNet[57] 61.7 - - - 65.7 74.7 77.7 84.2 88.2 79.7
Baseline Na¨ıveJointTraining 56.03 59.64 65.78 73.72 61.59 56.67 60.61 66.75 83.49 69.65
Data Feature Label - -
✓ 62.34 65.17 72.60 74.67 69.95 68.49 71.11 79.13 85.49 79.13
M3Net ✓ ✓ 62.91 65.73 73.32 75.47 70.87 71.47 73.86 81.53 86.71 80.26
(Ours) ✓ ✓ 63.23 67.89 73.61 81.66 71.13 71.53 73.91 81.80 86.92 80.45
✓ ✓ ✓ 63.87 68.66 73.10 82.35 72.00 71.70 74.01 82.20 86.47 80.90
PTv2+ [105]. We trained M3Net on four A100 GPUs for ment,thecombinationsofmulti-viewimagesatthefeature
50epochswithabatchsizeof6foreachGPU.Theinitial spaceandthelanguage-guidedknowledgeatthelabelspace
learning rate is set to 0.002. We adopt the AdamW opti- furtherenhancethelearnedfeaturerepresentations. There-
mizer [70] with a weight decay of 0.005 and cosine decay sultsshowthattheyworksynergisticallyinmergingknowl-
learning rate scheduler. For the dataset-specific rasteriza- edgefromheterogeneousdomainsduringjointtraining.
tion,wesetvoxelsizesto0.05m,0.1m,and0.05mforSe- PanopticLiDARSegmentation. InTab.2,wepresentan-
manticKITTI[3],nuScenes[28],andWaymoOpen[92],re- otherablationstudyfocusingonpanopticLiDARsegmen-
spectively. For the data augmentation, we employ random tation. AllthreealignmentsincorporatedinM3Netdemon-
flipping,jittering,scaling,rotation,andMix3D[77].Dueto strate significant improvements over the baselines. This
spacelimits,kindlyrefertoAppendixforadditionaldetails. highlightsthepronouncedefficacyofourmulti-spacealign-
Evaluation Metrics. We adopt conventional reportings of ments. Moreover, our approach outperforms the single-
mAcc and mIoU for LiDAR semantic segmentation, PQ, dataset state-of-the-art method Panaptic-PHNet [57] by a
PQ†,SQ,andRQforpanopticsegmentation,andmCEand notable 2.17% PQ on Panoptic-SemanticKITTI [3] and
mRRfor3Drobustnessevaluation. Duetothespacelimit, achievescompellingresultsonPanoptic-nuScenes[28].
kindlyrefertoourAppendixformoredetaileddefinitions. VisualFeatureAlignments.Weconductaqualitativeanal-
ysis of the learned visual feature distributions in the form
4.2.AblationStudy
of t-SNE [26]. Fig. 4 (a) and (b) represent the distribu-
Multi-Space Alignments. The effectiveness of three pro- tions of learned visual features among three datasets from
posedalignmentsoverthejointtrainingbaselinesisshown DeepLab and VLM backbones, respectively. The features
in Tab. 1. We observe that the data-space alignment plays obtained by the latter exhibit more distinct semantics in
themostcrucialroleinimprovingtheuniversalLiDARseg- feature space. The concentrated distribution space is ad-
mentation performance. Without proper data alignments, vantageousforachievingfeaturealignmentsacrossmultiple
joint training with either MinkUNet [21] or the stronger datasets. Additionally,Fig.4(c)and(d)illustratethedistri-
PTv2+ [105] will suffer severe degradation, especially on butionofpointcloudfeaturesbeforeandafterfeature-space
sparser point clouds [28]. On top of the data-space align- alignment.Ascanbeseen,thefeaturedistributiondistances
7Table3. KnowledgetransferandgeneralizationanalysesacrossfiveLiDARsegmentationdatasetsandtwo3Drobustnessevaluation
datasets.Allscoresaregiveninpercentage.Thebestandsecond-bestscoresarehighlightedinboldandunderline,respectively.
RELLIS-3D SemanticPOSS SemanticSTF SynLiDAR DAPS-3D SemKITTI-C nuScenes-C
Method
1% 10% Half Full Half Full 1% 10% Half Full mCE mRR mCE mRR
PPKT[68] 49.71 54.33 50.18 56.00 50.92 54.69 37.57 46.48 78.90 84.00 - - 105.64 76.06
SLidR[88] 49.75 54.57 51.56 55.36 52.01 54.35 42.05 47.84 81.00 85.40 - - 106.08 75.99
Seal[67] 51.09 55.03 53.26 56.89 53.46 55.36 43.58 49.26 81.88 85.90 - - 92.63 83.08
Na¨ıveJoint 37.77 50.23 42.19 52.31 46.70 48.00 18.56 42.37 73.91 77.89 113.65 84.73 128.97 81.45
Single-Dataset 40.17 54.25 47.69 55.00 50.33 51.19 23.17 45.08 75.10 80.87 95.11 84.95 99.63 79.06
M3Net(Ours) 51.27 55.05 53.60 57.17 53.78 55.42 44.10 49.93 82.08 86.00 86.43 85.77 91.03 79.15
Table 4. LiDAR semantic segmentation results on the val and knowledgetransfercapabilityofM3Net,weconductexten-
testsetsofSemanticKITTI [3]andnuScenes[28],andthevalset siveexperimentsonfivedifferentLiDAR-basedperception
ofWaymoOpen[92]. Allscoresareinpercentage. Thebestand
datasets[43,47,79,108,109]. Thesedatasetshaveunique
second-bestscoresarehighlightedinboldandunderline.
data collection protocols and data distributions. As shown
inFig.1andthefirsttencolumnsinTab.3,ourframework
SemKITTI nuScenes Waymo
Method
Val Test Val Test mIoU mAcc constantly outperforms the prior arts, the na¨ıve joint train-
RangeNet++[76] - 52.2 - 65.5 - - ing,andthesingle-datasetbaselinesacrossallfivedatasets.
PolarNet[123] 57.2 54.3 71.0 69.8 - - Thisconcretelysupportsthestrongknowledgetransfereffi-
SalsaNext[25] - 59.5 - 72.2 - -
cacybroughtbymulti-spacealignmentsinM3Net.
RangeViT[1] 60.7 64.0 75.2 - - -
MinkUNet[21] 63.8 63.7 73.3 - 65.9 76.6 Out-of-Distribution Generalization. Evaluating the gen-
SPVNAS[93] 64.7 66.4 - 77.4 67.4 - eralizationabilityofmodelsonout-of-training-distribution
AMVNet[64] 65.2 65.3 77.2 77.3 - -
RPVNet[114] 65.5 70.3 77.6 - - - data is crucial, particularly in safety-critical fields like au-
(AF)2-S3Net[19] - 69.7 - 78.3 - - tonomousdriving[52,111,112]. Inthiscontext,weresort
Cylinder3D[130] 65.9 67.8 76.1 77.9 66.0 -
tothetwocorruptiondatasetsfromtheRobo3D[49]bench-
PVKD[36] 66.4 71.2 76.0 - - -
WaffleIron[82] 66.8 70.8 79.1 - - - mark, i.e., SemanticKITTI-C and nuScenes-C, to conduct
RangeFormer[48] 67.6 73.3 78.1 80.1 - - our assessment. From the last four columns of Tab. 3, we
SphereFormer[54] 67.8 74.8 78.4 81.9 69.9 -
observe that M3Net achieves better results than the na¨ıve
FRNet[116] 68.7 73.3 79.0 82.5 - -
PTv2+[105] 70.3 70.6 80.2 82.6 70.6 80.2 joint training and other single-dataset approaches, proving
LidarMultiNet[119] - - - 81.4 73.8 -
thestronggeneralizabilityofthelearnedrepresentations.
M3Net(Ours) 72.0 75.1 80.9 83.1 72.4 81.1
5.Conclusion
betweenthethreedatasetshavebeenlargelyreduced, pro- In this work, we presented M3Net, a universal frame-
vidingevidenceofthealignmenteffectiveness. work capable of fulfilling multi-task, multi-dataset, multi-
modalityLiDARsegmentationusingasinglesetofparam-
4.3.ComparativeStudy
eters. Through extensive analyses, we validated the effec-
tiveness of applying data-, feature-, and label-space align-
ComparisonstoStateoftheArts. InTab.4,wecompare
ments to handle such a challenging task. In addition, our
M3Netwithcurrentbest-performingmodelsonthebench-
comprehensiveanalysisanddiscoursehavedelvedintothe
marks of SemanticKITTI [3], nuScenes [28], and Waymo
fundamentalchallengesofacquiringthegeneralknowledge
Open [92]. Remarkably, M3Net consistently outperforms
for scalable 3D perception, which holds substantial poten-
existing approaches across all three datasets. Specifically,
tial to propel further research in this domain. Our future
on SemanticKITTI [3], M3Net achieves a 72.0% mIoU on
strides focus on combining more data resources to further
the validation set, surpassing the closest method by a no-
enhancethealignmentsandadaptationsinourframework.
table margin of 1.7% mIoU. Similarly, on nuScenes [28],
M3Netachieves80.9%mIoUand83.1%mIoUontheval- Acknowledgements.ThisworkwaspartiallysupportedbyNSFC
idationandtestsets,demonstratingitsrobustnessandgen- (No.62206173) and MoE Key Laboratory of Intelligent Percep-
eralization capabilities. Additionally, the performance of tion and Human-Machine Collaboration (ShanghaiTech Univer-
M3NetonWaymoOpen[92]iscompetitivewithpriorarts. sity).ThisworkwasalsosupportedbytheMinistryofEducation,
WeachieveamIoUof72.4%andamAccof81.1%. These Singapore,underitsMOEAcRFTier2(MOET2EP20221-0012),
resultshighlightagainthesuperiorityofM3Netinhandling NTU NAP, and under the RIE2020 Industry Alignment Fund –
complexdiverseLiDARsegmentationtasks. Industry Collaboration Projects (IAF-ICP) Funding Initiative, as
DirectKnowledgeTransfer. Tofurthervalidatethestrong wellascashandin-kindcontributionfromtheindustrypartner(s).
8Appendix is shown in Tab. 5. For multi-dataset training and evalua-
tions,weusetheLiDARandcameradatafromthenuScenes
[8,28],SemanticKITTI[3],andWaymoOpen[92]datasets.
A.Multi-DatasetConfiguration 9
• nuScenes is a large-scale public dataset for autonomous
A.1.Overview . . . . . . . . . . . . . . . . . . . 9
driving, created by Motional (formerly nuTonomy). It
A.2.StatisticalAnalysis . . . . . . . . . . . . . . 11
is widely used in the research and development of au-
A.2.1 nuScenes . . . . . . . . . . . . . . 11
tonomousvehiclesandrelatedtechnologies. Thedataset
A.2.2 SemanticKITTI . . . . . . . . . . . 11
includesacomprehensiverangeofsensordatacrucialfor
A.2.3 WaymoOpen . . . . . . . . . . . . 12
autonomousdriving. Ittypicallycontainsdatafrommul-
B.Multi-TaskConfiguration 15 tiple cameras, LiDAR, RADAR, GPS, IMU, and other
B.1.Overview . . . . . . . . . . . . . . . . . . . 15 sensors. This multimodal data collection is essential for
developingandtestingalgorithmsforperception,predic-
B.2.MeanShift . . . . . . . . . . . . . . . . . . 15
tion, and motion planning in autonomous vehicles. One
C.AdditionalImplementationDetails 15 of the strengths of the nuScenes dataset is its diversity.
C.1.Datasets . . . . . . . . . . . . . . . . . . . . 15 Thedataencompassesvariousdrivingconditions,includ-
C.2.TextPrompts . . . . . . . . . . . . . . . . . 15 ing different times of day, weather conditions, and ur-
C.3.Backbones . . . . . . . . . . . . . . . . . . 15 ban environments. This diversity is crucial for train-
C.3.1 MinkUNet . . . . . . . . . . . . . 15 ing robust algorithms that can handle real-world driving
C.3.2 PTv2+. . . . . . . . . . . . . . . . 16 scenarios. In this work, we use the LiDAR semantic
and panoptic segmentation data from the lidarseg1 sub-
C.4.TrainingConfiguration . . . . . . . . . . . . 16
set in the nuScenes dataset, which includes segmenta-
C.5.EvaluationConfiguration . . . . . . . . . . . 17
tion labels for the entire nuScenes dataset, encompass-
D.AdditionalExperimentalResults 19 ing thousands of scenes, each a 20-second clip captured
D.1.PilotStudy . . . . . . . . . . . . . . . . . . 19 from a driving vehicle in various urban settings. 32
D.2.AblationStudy . . . . . . . . . . . . . . . . 19 classes are manually labeled, covering a wide range of
D.3.LiDARPanopticSegmentation. . . . . . . . 20 objects and elements in urban scenes, where 16 of them
D.3.1 Panoptic-SemanticKITTI . . . . . . 20 aretypicallyadoptedinevaluatingthesegmentationper-
formance. More details of this dataset can be found at
D.3.2 Panoptic-nuScenes . . . . . . . . . 20
https://www.nuscenes.org/nuscenes.
D.4.Out-of-DistributionGeneralization . . . . . 20
• SemanticKITTI is a well-known dataset in the field of
E.QualitativeAssessment 20 autonomousdrivingandrobotics,specificallytailoredfor
E.1.VisualComparisons . . . . . . . . . . . . . 20 thetaskofsemanticandpanopticsegmentationusingLi-
DAR point clouds. It is an extension of the original
F.BroaderImpact 22 KITTI Vision Benchmark Suite2 [30], with annotations
F.1.PositiveSocietalInfluence . . . . . . . . . . 22 forover20sequencesofdrivingscenarios,eachcontain-
F.2.PotentialLimitation . . . . . . . . . . . . . 22 ingtensofthousandsofLiDARscans.Thedatasetcovers
a variety of urban and rural scenes. This includes city
G.PublicResourcesUsed 22 streets, residential areas, highways, and country roads,
G.1.PublicDatasetsUsed . . . . . . . . . . . . . 22 providing a diverse set of environments for testing al-
G.2.PublicModelsUsed . . . . . . . . . . . . . 22 gorithms. The dataset provides labels for 28 different
G.3.PublicCodebasesUsed . . . . . . . . . . . . 22 semantic classes, including cars, pedestrians, bicycles,
various types of vegetation, buildings, roads, and so on.
19 classes are typically adopted for evaluation. In total,
A.Multi-DatasetConfiguration
around 4549 million points are annotated, and such ex-
In this section, we elaborate on the details of combin- tensive labeling provides a dense coverage for each Li-
ing multiple heterogeneous LiDAR segmentation datasets DAR scan. More details of this dataset can be found at
totrainauniversalLiDARsegmentationmodel. http://semantic-kitti.org.
• WaymoOpenisalargedatasetforautonomousdriving,
A.1.Overview providedbyWaymoLLC,acompanythatspecializesin
thedevelopmentofself-drivingtechnology. Thisdataset
Inthiswork,weresorttotendrivingdatasetsforachiev-
is particularly notable for its comprehensive coverage
ingi)multi-datasettrainingandevaluations,ii)knowledge
transferandgeneralization,andiii)out-of-distributiongen- 1https://www.nuscenes.org/lidar-segmentation.
eralization. A summary of the datasets used in this work 2https://www.cvlibs.net/datasets/kitti.
9Table5.Summaryofthedatasetsusedinthiswork.Wesplitdifferentdatasetsintothreecategories:i)ThenuScenes[28],SemanticKITTI
[3], and Waymo Open [92] datasets are used for multi-dataset training and evaluations. ii) The RELLIS-3D [43], SemanticPOSS [79],
SemanticSTF[109],SynLiDAR[108],andDAPS-3D[47]datasetsareusedforknowledgetransferandgeneralization(w/ fine-tuning).
iii)TheSemanticKITTI-C[49]andnuScenes-C[49]datasetsareusedforout-of-distributiongeneralization(w/ofine-tuning).
DatasetSummary
nuScenes[28] SemanticKITTI[3] WaymoOpen[92] RELLIS-3D[43] SemanticPOSS[79]
[Link] [Link] [Link] [Link] [Link]
SemanticSTF[109] SynLiDAR[108] DAPS-3D[47] SemanticKITTI-C[49] nuScenes-C[49]
[Link] [Link] [Link] [Link] [Link]
ofvariousscenariosencounteredinautonomousdriving. ments for autonomous navigation and perception, de-
The data is collected using Waymo’s self-driving vehi- veloped by Texas A&M University. It contains mul-
cles,whichareequippedwithanarrayofsensors,includ- timodal sensor data, including high-resolution LiDAR,
ing high-resolution LiDARs, cameras, and radars. This RGB imagery, and GPS/IMU data, providing a compre-
multimodaldatacollectionallowsforcomprehensiveper- hensive set for developing and evaluating algorithms for
ception modeling. The dataset includes a wide range of off-road autonomous driving. The dataset features di-
drivingenvironmentsandconditions,suchascitystreets, verseterraintypes,suchasgrasslands,forests,andtrails,
highways,andsuburbanareas,capturedatdifferenttimes offering unique challenges compared to urban scenar-
of day and in various weather conditions. This variety ios. RELLIS-3D includes annotations for 13 semantic
iscrucialfordevelopingrobustautonomousdrivingsys- classes, including natural elements and man-made ob-
tems. In this work, we use its 3D Semantic Segmenta- jects, crucial for navigation in off-road settings. More
tion subset, which specifically provides point-level an- details of this dataset can be found at http://www.
notations for 3D point clouds generated by LiDAR sen- unmannedlab.org/research/RELLIS-3D.
sors. 22semanticclassesareusedduringevaluation,en- • SemanticPOSS focuses on panoramic LiDAR scans,
compassingawiderangeofobjectclasses,suchasvehi- which include urban scenes, highways, and rural areas.
cles,pedestrians,andcyclists,aswellasstaticobjectslike Thedatasetcontainsannotationsfor14semanticclasses,
roadsigns,buildings,andvegetation. Moredetailsofthis coveringvehicles,pedestrians,cyclists,andvariousroad
datasetcanbefoundathttps://waymo.com/open. elements. Itspanoramicviewprovidesa360-degreeun-
derstanding of the vehicle’s surroundings, which is ben-
To validate that the learned features from our multi-
eficial for comprehensive scene analysis. More details
dataset training setup are superior to that of the singe-
ofthisdatasetcanbefoundathttps://www.poss.
dataset training in knowledge transfer and generalization,
pku.edu.cn/semanticposs.
we conduct fine-tuning experiments on the following five
• SemanticSTF studies the 3D semantic segmentation of
datasets: RELLIS-3D [43], SemanticPOSS [79], Semantic-
LiDAR point clouds under adverse weather conditions,
STF[109],SynLiDAR[108],andDAPS-3D[47].
including snow, rain, and fog. It is built from the real-
• RELLIS-3D is a dataset focusing on off-road environ-
10world STF [6] dataset with point-wise annotations of 21 C, which follow the original dense annotations in Se-
semantic categories. The original LiDAR data in STF manticKITTI. More details of this dataset can be found
was captured by a Velodyne HDL64 S3D LiDAR sen- athttps://github.com/ldkong1205/Robo3D.
sor. Intotal,SemanticSTFselected2076scansfordense • nuScenes-Csharesthesamecorruptionandseveritylevel
annotations, including694snowy, 637dense-foggy, 631 definitions with SemanticKITTI-C and is built upon the
light-foggy, and 114 rainy scans. More details of this validationsetofthenuScenes[28]dataset. Intotal,there
dataset can be found at https://github.com/ are 144456 LiDAR scans in nuScenes-C, which follow
xiaoaoran/SemanticSTF. the original dense annotations in nuScenes. More de-
• SynLiDAR is a synthetic dataset for LiDAR-based se- tailsofthisdatasetcanbefoundathttps://github.
mantic segmentation. It is generated using advanced com/ldkong1205/Robo3D.
simulationtechniquestocreaterealisticurban,suburban,
A.2.StatisticalAnalysis
and rural environments. SynLiDAR offers an extensive
range of annotations for a variety of classes, including
In this section, we conduct a comprehensive statistical
dynamic objects like vehicles and pedestrians, as well
analysis of the nuScenes [8, 28], SemanticKITTI [3], and
as static objects like buildings and trees. This dataset
Waymo Open [92] datasets to showcase the difficulties in
is useful for algorithm development and testing in sim-
mergingheterogeneousLiDARandcameradata.
ulated environments where real-world data collection is
challenging. Moredetailsofthisdatasetcanbefoundat
https://github.com/xiaoaoran/SynLiDAR. A.2.1 nuScenes
• DAPS-3D is a dataset focusing on dynamic and static
TheLiDARpointcloudsinthenuScenes[8,28]dataset
pointcloudsegmentation. ItincludesLiDARscansfrom
areacquiredbyaVelodyneHDL32Ewith32beams,1080
diverse urban environments, providing detailed annota-
(+/−10) points per ring, 20Hz capture frequency, 360-
tions for dynamic objects such as vehicles, pedestrians,
degree Horizontal FOV, +10-degree to −30-degree Verti-
andcyclists,aswellasstaticobjectslikebuildings,roads,
cal FOV, uniform azimuth angles, a 80m to 100m range,
and vegetation. DAPS-3D is designed to advance re-
anduptoaround1.39millionpointspersecond. Thereare
search in dynamic scene understanding and prediction
atotalof16semanticclassesinthisdataset. Thedistribu-
in autonomous driving, addressing the challenges posed
tions of these classes across a 50 meters range are shown
by moving objects in complex urban settings. More de-
inTab.6. Ascanbeseen,mostsemanticclassesdistribute
tailsofthisdatasetcanbefoundathttps://github.
within the 20 meters range. The dynamic classes, such as
com/subake/DAPS3D.
bicycle,motorcycle,bus,car,andpedestrian,
Meanwhile, we leverage the SemanticKITTI-C and show a high possibility of occurrence at round 5 meters to
nuScenes-C datasets in the Robo3D benchmark [49] to 10meters. Thestaticclasses,onthecontrary,areoftendis-
probetheout-of-training-distributionrobustnessof tributed across a wider range. Typically examples include
• SemanticKITTI-C is built upon the validation set of terrain and manmade. Different semantic classes ex-
the SemanticKITTI [3] dataset. It is designed to cover hibituniquedistributionpatternsaroundtheego-vehicle.
out-of-distribution corruptions that tend to occur in the
real world. A total of eight corruption types are bench-
A.2.2 SemanticKITTI
marked, including fog, wet ground, snow, motion blur,
beam missing, crosstalk, incomplete echo, and cross- The LiDAR point clouds in the SemanticKITTI [3]
sensor cases. For each corruption, three subsets that dataset are acquired by a Velodyne HDL-64E with 64
cover different levels of corruption severity are created, beams,providinghigh-resolutiondata.TheVelodyneHDL-
i.e. easy, moderate, and hard. The LiDAR segmentation 64Efeaturesa360-degreeHorizontalFieldofView(FOV),
modelsareexpectedtobetrainedonthecleansetswhile aVerticalFOVrangingfrom+2to−24.33degrees,andan
tested on these eight corruption sets. The performance angularresolutionofapproximately0.08−0.4degrees(ver-
degradation under corruption scenarios is used to mea- tically)and0.08−0.35degrees(horizontally). Thesensor
sure the model’s robustness. Two metrics are designed operatesata10Hzcapturefrequencyandcandetectobjects
for such measurements, namely mean Corruption Error withinarangeofupto120m, deliveringdenselysampled,
(mCE)andmeanResilienceRate(mRR).mCEcalculates detailedpointcloudswithapproximately1.3millionpoints
therelativerobustnessofacandidatemodelcomparedto persecond. Thereareatotalof19semanticclassesinthis
the baseline model, while mRR computes the absolute dataset. The distributions of these classes across a 50 me-
performance degradation of a candidate model when it ters range are shown in Tab. 7. It can be seen from these
is tested on clean and corruption sets, respectively. In statistical plots that the distributions are distinctly differ-
total, there are 97704 LiDAR scans in SemanticKITTI- ent from each other; points belonging to the road class
11Table6. Thestatisticalanalysisofthe16semanticclassesinthenuScenes[28]dataset. Statisticsarecalculatedfromthetrainingsplit
ofthedataset.EachviolinplotshowstheLiDARpointclouddensitydistributionina50metersrange.Bestviewedincolors.
nuScenes(16classes)
barrier bicycle bus car
construction-vehicle motorcycle pedestrian traffic-cone
trailer truck driveable-surface other-flat
sidewalk terrain manmade vegetation
are intensively distributed in between 5 meters to 10 me- mid-rangeLiDARhasanon-uniforminclinationbeaman-
ters around the ego-vehicle, while those dynamic classes gle pattern. The range of the mid-range LiDAR is trun-
like bicyclist, motorcyclist, other-vehicle cated to a maximum of 75 meters. The range of the
and truck, tend to appear in a wider range. Similar to short-range LiDARs is truncated to a maximum of 20 me-
thenuScenesdataset,the19classesinSemanticKITTI also ters. The strongest two intensity returns are provided for
exhibitdistinctpatternsofoccurrenceinthedrivingscenes. all five LiDARs. An extrinsic calibration matrix trans-
forms the LiDAR frame to the vehicle frame. The point
clouds of each LiDAR in Waymo Open are encoded as a
A.2.3 WaymoOpen
range image. Two range images are provided for each Li-
DAR, one for each of the two strongest returns. There are
The 3D semantic segmentation subset of the Waymo
four channels in the range image, including range, inten-
Open [92] dataset features LiDAR point clouds obtained
sity, elongation, and occupancy. The distributions of these
usingWaymo’sproprietaryLiDARsensors, whichinclude
classes across a 50 meters range are shown in Tab. 8. As
mid-rangeandshort-rangeLiDARs.TherearefiveLiDARs
can be seen. the class distributions of Waymo Open are
intotal-onemid-rangeLiDAR(top)andfourshort-range
morediversethanthosefromnuScenesandSemanticKITTI.
LiDARs (front, side left, side right, and rear), where the
12Table7. Thestatisticalanalysisofthe19semanticclassesintheSemanticKITTI[3]dataset. Statisticsarecalculatedfromthetraining
splitofthedataset.EachviolinplotshowstheLiDARpointclouddensitydistributionina50metersrange.Bestviewedincolors.
SemanticKITTI(19classes)
car bicycle motorcycle truck
other-vehicle person bicyclist motorcyclist
road parking sidewalk other-ground
building fence vegetation trunk
terrain pole traffic-sign
Some semantic classes, including motorcyclist,
pedestrian, construction-cone, vegetation,
andtree-trunk,aredistributedacrossalmosttheentire
drivingscenescapturedbythefiveLiDARsensors.
13Table8. Thestatisticalanalysisofthe22semanticclassesintheWaymoOpen[92]dataset. Statisticsarecalculatedfromthetraining
splitofthedataset.EachviolinplotshowstheLiDARpointclouddensitydistributionina50metersrange.Bestviewedincolors.
WaymoOpen(22classes)
car truck bus other-vehicle
motorcyclist bicyclist pedestrian traffic-sign
traffic-light pole construction-cone bicycle
motorcycle building vegetation tree-trunk
curb road lane-marker other-ground
walkable sidewalk
14B.Multi-TaskConfiguration The SemanticKITTI, nuScenes, and Waymo Open datasets
contain 19130, 174780, and 118455 camera images in the
In this section, we supplement more details of our de-
train set, respectively, where SemanticKITTI has single-
sign and implementation toward multi-task (semantic and
camera (front-view) data, nuScenes is with a six-camera
panoptic)LiDARsegmentation.
(threefront-viewandthreeback-view)systems,andWaymo
B.1.Overview Openhasfivecameraviewsintotal.
For multi-task experiments on SemanticKITTI [3] and
Aproperpipelinedesigncouldenablethemodeltogen-
Panoptic-nuScenes [28], we follow the official data prepa-
eratesuitablepredictionstofulfillmultipletaskssimultane-
ration procedures to set up the training and evaluations.
ously. In the context of LiDAR segmentation, we are es-
Specifically, these two datasets share the same amount of
pecially interested in unifying semantic and panoptic seg-
data with their semantic segmentation subsets, i.e., 19130
mentation of LiDAR point clouds. Such a holistic way of
and29130trainingLiDARscans,and4071and6019vali-
3Dsceneunderstandingiscrucialforthesafeperceptionin
dationLiDARscans,respectively. EachLiDARscanisas-
autonomousvehicles.
sociatedwithapanopticsegmentationmapwhichindicates
theinstanceIDs. Foradditionaldetails, kindlyrefertothe
B.2.MeanShift
originalpapers.
In this work, we enhance the versatility of our frame- For the knowledge transfer fine-tuning experiments on
work in an end-to-end fashion through the integration of a the RELLIS-3D [43], SemanticPOSS [79], SemanticSTF
multi-taskingapproach. Thisadaptationinvolvesthemod- [109],SynLiDAR[108]andDAPS-3D[47]datasets,wefol-
ification of the instance extractor on top of the semantic lowthesameprocedureasSeal[67]topreparethetraining
predictions, which enables a dual output for both LiDAR and validation sets. Kindly refer to the original paper for
semantic and panoptic segmentation. Specifically, draw- moredetailsonthisaspect.
ing inspiration from DS-Net [34, 35], our instance extrac- Fortheout-of-training-distributiongeneralizationexper-
torcomprisesaninstancehead, succeededbyapointclus- imentsonSemanticKITTI-CandnuScenes-C,wefollowthe
tering step. The instance head encompasses a sequence of same data preparation procedure in Robo3D [49]. There
multi-layer perceptrons designed to predict the offsets be- areeightdifferentcorruptiontypesineachdataset,includ-
tween instance centers. This point clustering step strategi- ing fog, wet ground, snow, motion blur, beam missing,
callyemployssemanticpredictionstofilteroutstuff points, crosstalk, incomplete echo, and cross-sensor cases, where
therebyretainingonlythoseassociatedwiththinginstances, each corruption type contains corrupted data from three
such as pedestrian, car, and bicyclist. Subse- severity levels. In total, there are 97704 LiDAR scans in
quently, the remaining points undergo mean-shift cluster- SemanticKITTI-C and 144456 LiDAR scans in nuScenes-
ing [22], utilizing features from the instance head to dis- C.Foradditionaldetails,kindlyrefertotheoriginalpaper.
cern distinct instances. This meticulous process enhances
the framework’s capacity for accurate instance segmenta- C.2.TextPrompts
tion. The bandwidth for mean-shift in the SemanticKITTI
Inthiswork,weadoptthestandardtemplatesalongwith
and Panoptic-nuScenes datasets is set to 1.2 and 2.5, re-
specified class text prompts to generate the CLIP text em-
spectively.
bedding for the three datasets used in our multi-dataset
training pipeline. Specifically, the text prompts associ-
C.AdditionalImplementationDetails
ated with the semantic classes in the nuScenes [28], Se-
Inthissection,weprovideadditionaldetailstoassistthe manticKITTI[3],andWaymoOpen[92]datasetsareshown
implementationandreproductionoftheapproachproposed inTab.9,Tab.10,andTab.11,respectively.
inthemainbodyofthispaper.
C.3.Backbones
C.1.Datasets
Inthiswork,weadopttwomodelstoserveastheback-
In our multi-dataset training pipeline, we train our bone ofour proposed M3Net, i.e., theclassical MinkUNet
M3Net framework on the three most popular large-scale [21]andthemorerecentPTv2+[105].
drivingdatasets,i.e.,theSemanticKITTI[3],nuScenes[28],
and Waymo Open [92] datasets. These three datasets con-
C.3.1 MinkUNet
sist of 19130, 29130, and 23691 training LiDAR scans,
and4071,6019,and5976validationLiDARscans,respec- The primary contribution of MinkUNet [21] is the in-
tively. Besides, we leverage the synchronized camera im- troductionofaneuralnetworkarchitecturecapableofpro-
ages from the corresponding datasets as our 2D inputs in cessing4Dspatiotemporaldata(3Dspace+time). Thisis
theM3Nettrainingpipelineforcross-modalityalignments. particularly relevant for applications that involve dynamic
15Table9.TextpromptsdefinedforthenuScenes[28]dataset(16classes)inourproposedM3Netframework.
nuScenes(16classes)
# class textprompt
1 barrier ‘barrier’,‘barricade’
2 bicycle ‘bicycle’
3 bus ‘bus’
4 car ‘car’
5 construction-vehicle ‘bulldozer’,‘excavator’,‘concretemixer’,‘crane’,‘dumptruck’
6 motorcycle ‘motorcycle’
7 pedestrian ‘pedestrian’,‘person’
8 traffic-cone ‘traffic-cone’
9 trailer ‘trailer’,‘semi-trailer’,‘cargocontainer’,‘shippingcontainer’,‘freightcontainer’
10 truck ‘truck’
11 driveable-surface ‘road’
12 other-flat ‘curb’,‘trafficisland’,‘trafficmedian’
13 sidewalk ‘sidewalk’
14 terrain ‘grass’,‘grassland’,‘lawn’,‘meadow’,‘turf’,‘sod’
15 manmade ‘building’,‘wall’,‘pole’,‘awning’
16 vegetation ‘tree’,‘trunk’,‘treetrunk’,‘bush’,‘shrub’,‘plant’,‘flower’,‘woods’
environments,likeautonomousdriving,whereunderstand- additionalpositionencodingmultiplierstrengthensthepo-
ing the temporal evolution of the scene is crucial. A key sition information for attention, allowing for more accu-
feature of the Minkowski convolution, and by extension rate and detailed data processing. Extensive experiments
MinkUNet,isitsabilitytoperformconvolutionaloperations demonstrate that PTv2+ achieves state-of-the-art perfor-
onsparsedata. Thisisachievedthroughtheuseofagener- manceonseveralchallenging3Dpointcloudunderstanding
alizedsparseconvolutionoperationthatcanhandledatain benchmarks. In this work, we resort to the Pointcept [24]
high-dimensional spaces while maintaining computational implementation of PTv2+ implementation of MinkUNet
efficiency. TheimplementationofMinkowskiconvolutions and adopt the base version as our backbone network in
is facilitated by the Minkowski Engine, a framework for M3Net. Moredetailsofthisusedbackbonecanbefoundat
high-dimensionalsparsetensoroperations. Thisengineen- https://github.com/Pointcept/Pointcept.
ables the efficient implementation of the MinkUNet and
C.4.TrainingConfiguration
other similar architectures. In this work, we resort to the
Pointcept[24]implementationofMinkUNetandadoptthe Inthiswork,weimplementtheproposedM3Netframe-
base version as our backbone network in M3Net. More work based on Pointcept [24] and MMDetection3D [23].
details of this used backbone can be found at https: We trained our baselines and M3Net on four A100 GPUs
//github.com/Pointcept/Pointcept. eachwith80GBmemory. WeadopttheAdamWoptimizer
[70] with a weight decay of 0.005 and a learning rate of
0.002. Thelearningrateschedulerutilizediscosinedecay
C.3.2 PTv2+
andthebatchsizeissetto6foreachGPU.
PTv2+ [105] introduces an effective grouped vector at- Inthedata-specificrasterizationprocess,werasterizethe
tention (GVA) mechanism. GVA facilitates efficient infor- point clouds with voxel sizes tailored to the dataset char-
mation exchange both within and among attention groups, acteristics. Specifically, we set the voxel sizes to [0.05m,
significantlyenhancingthemodel’sabilitytoprocesscom- 0.05m, 0.05m], [0.1m, 0.1m, 0.1m], and [0.05m, 0.05m,
plexpointclouddata. PTv2+alsointroducesanimproved 0.05m] for the SemanticKITTI [3], nuScenes [28], and
position encoding scheme. This enhancement allows for WaymoOpen[92]datasets,respectively.
better utilization of point cloud coordinates, thereby bol- For data augmentation, we leverage several techniques,
steringthespatialreasoningcapabilitiesofthemodel. The includingrandomflipsalongtheX,Y,andXY axes,and
16Table10.TextpromptsdefinedfortheSemanticKITTI[3]dataset(19classes)inourproposedM3Netframework.
SemanticKITTI(19classes)
# class textprompt
1 car ‘car’
2 bicycle ‘bicycle’
3 motorcycle ‘motorcycle’
4 truck ‘truck’
5 other-vehicle ‘othervehicle’, ‘bulldozer’, ‘excavator’, ‘concretemixer’, ‘crane’, ‘dumptruck’, ‘bus’,
‘trailer’,‘semi-trailer’,‘cargocontainer’,‘shippingcontainer’,‘freightcontainer’
6 person ‘person’
7 bicyclist ‘bicyclist’
8 motorcyclist ‘motorcyclist’
9 road ‘road’
10 parking ‘parking’
11 sidewalk ‘sidewalk’
12 other-ground ‘otherground’,‘curb’,‘trafficisland’,‘trafficmedian’
13 building ‘building’
14 fence ‘fence’
15 vegetation ‘tree’
16 trunk ‘treetrunk’,‘trunk’
17 terrain ‘grass’,‘grassland’,‘lawn’,‘meadow’,‘turf’,‘sod’
18 pole ‘pole’
19 traffic sign ‘trafficsign’
randomjitteringwithintherangeof[-0.02m,0.02m].Addi- C.5.EvaluationConfiguration
tionally,weincorporateglobalscalingandrotation,choos-
In this work, we follow the conventional reporting
ing scaling factors and rotation angles randomly from the
and employ the Intersection-over-Union (IoU) for individ-
intervals [0.9, 1.1] and [0, 2π], respectively. Furthermore,
ual classes and the mean Intersection-over-Union (mIoU)
we integrate Mix3D [77] into our augmentation strategy
across all classes as our evaluation metrics for LiDAR se-
during the training. There also exists some other augmen-
manticsegmentation.Specifically,theIoUscoreforseman-
tation techniques, such as LaserMix [51], PolarMix [107],
ticclassciscomputedasfollows:
RangeMix[48,50],andFrustumMix[116].
For the network backbones, we have opted for TP
MinkUNet[21]andPTv2+[105].InthecaseofMinkUNet, IoU c = TP +FPc +FN . (11)
c c c
theencoderchannelsaresetas{32,64,128,256}, andthe
decoderchannelsare{256,128,64,64},eachwithakernel Here,TP ,FP ,andFN representthetruepositive,false
c c c
size of 3. Meanwhile, for the PTv2+, the encoder chan- positive, and false negative of class c, respectively. The
nels are {32,64,128,256,512}, and the decoder channels mIoU score on each dataset is calculated by averaging the
are {64,64,128,256}. For additional details, kindly refer IoUscoresacrosseverysemanticclass. Notably,following
totheoriginalpapers. recent works [105, 117], we report mIoU with Test Time
For the loss function, we incorporate the conventional Augmentation(TTA).Foradditionaldetails,kindlyreferto
cross-entropy loss and the Lovasz-softmax [5] loss to pro- theoriginalpapers.
vide optimization for the LiDAR semantic and panoptic For panoptic LiDAR segmentation, we follow conven-
segmentationtask. Additionally,weemploytheL1lossto tionalreportingandutilizethePanopticQuality(PQ)asour
optimizetheinstancehead,aidingintheregressionofpre- primarymetric.ThedefinitionandcalculationofthePanop-
ciseinstanceoffsets. ticQuality(PQ),SegmentationQuality(SQ),andRecogni-
17Table11.TextpromptsdefinedfortheWaymoOpen[92]dataset(22classes)inourproposedM3Netframework.
WaymoOpen(22classes)
# class textprompt
1 car ‘car’
2 truck ‘truck’
3 bus ‘bus’
4 other-vehicle ‘other vehicle’, ‘pedicab’, ‘construction vehicle’, ‘recreational vehicle’, ‘limo’,
‘tram’, ‘trailer’, ‘semi-trailer’, ‘cargo container’, ‘shipping container’, ‘freight
container’,‘bulldozer’,‘excavator’,‘concretemixer’,‘crane’,‘dumptruck’
5 motorcyclist ‘motorcyclist’
6 bicyclist ‘bicyclist’
7 pedestrian ‘person’,‘pedestrian’
8 traffic-sign ‘traffic sign’, ‘parking sign’, ‘direction sign’, ‘traffic sign without pole’, ‘traffic
lightbox’
9 traffic-light ‘trafficlight’
10 pole ‘lamppost’,‘trafficsignpole’
11 construction-cone ‘constructioncone’
12 bicycle ‘bicycle’
13 motorcycle ‘motorcycle’
14 building ‘building’
15 vegetation ‘bushes’, ‘tree branches’, ‘tall grasses’, ‘flowers’, ‘grass’, ‘grassland’, ‘lawn’,
‘meadow’,‘turf’,‘sod’
16 tree-trunk ‘treetrunk’,‘trunk’
17 curb ‘curb’
18 road ‘road’
19 lane-marker ‘lanemarker’
20 other-ground ‘otherground’,‘bumps’,‘cateyes’,‘railtracks’
21 walkable ‘walkable’,‘grassyhill’,‘pedestrianwalkwaystairs’
22 sidewalk ‘sidewalk’
tionQuality(RQ)scoresaregivenasfollows: wefollowRobo3D[49]andadoptthecorruptionerror(CE)
(cid:80) andresiliencerate(RR),aswellasthemeancorruptioner-
IoU(i,j) |TP|
PQ= (i,j)∈TP × . ror (mCE) and mean resilience rate (mRR) as the evalua-
|TP| |TP|+ 1|FP|+ 1|FN|
2 2 tionmetricsincomparingtherobustness. Tonormalizethe
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
SQ RQ severity effects, we chose MinkUNet [21] as the baseline
(12) model. TheCEandmCEscoresarecalculatedasfollows:
The three aforementioned metrics are also calculated in-
dividually for things and stuff classes, resulting in PQth,
SQth, RQth, andPQst, SQst, RQst. Additionally, wealso CE =
(cid:80)3
l=1(1−Acc k,l) , mCE= 1
(cid:88)N
CE ,
report the PQ† score as widely used in many prior works k (cid:80)3 (1−Accbaseline) N k
l=1 k,l k=1
[34,57,90,129]. Thismetricisdefinedbyexchangingthe (13)
PQofeachstuff classwithitsIoUandthenaveragingacross whereAcc denotesmIoUscoresoncorruptiontypek at
k,l
all semantic classes. For additional details, kindly refer to severity level l. N = 8 is the total number of corruption
theoriginalpapers. types. ThemRRservesastherelativerobustnessindicator
To further assess the capability of a LiDAR segmenta- formeasuringhowmuchaccuracyamodelcanretainwhen
tion model for out-of-training-distribution generalization, evaluatedonthecorruptionsets. TheRRandmRRscores
18Standalone Training Joint Training (Same # Beam) Standalone Training Joint Training (Same # Beam)
nuScenes nuScenes
KITTI -4.2 KITTI -2.0
Waymo -3.5 Waymo -7.0
mIoU (%) 52 59 66 73 mIoU (%) 52 59 66 73 mIoU (%) 52 60 68 76 mIoU (%) 52 60 68 76
a b a b
c d c d
Joint Training (32 & 64 Beam) Joint Training (All Together) Joint Training (32 & 64 Beam) Joint Training (All Together)
-8.7 -13.5 -8.1 -8.4
-7.7 -9.6 -7.2 -3.5
-5.9 -2.5
mIoU (%) 52 59 66 73 mIoU (%) 52 59 66 73 mIoU (%) 52 60 68 76 mIoU (%) 52 59 66 73 80
(a)M3Net(w/MinkUNet[21]backbone) (b)M3Net(w/PTv2+[105]backbone)
Figure7. Apilotstudyofna¨ıvelymergingdifferentdatasetsfortrainingtheMinkUNet[21]model. Comparedtothestandalonetraining
in(a),eitherjointlytrainingwith(b)thesame,(c)different,or(d)allsensor-acquireddatawillcauseseveredegradation. Subfigure(a):
M3Netw/ aMinkUNet[21]backbone.Subfigure(b):M3Netw/ aPTv2+[105]backbone.
arecalculatedasfollows: disruptiveinsteadofbeingmoregeneral. Suchdegradation
is particularly overt using na¨ıvely combining LiDAR data
RR =
(cid:80)3
l=1Acc
k,l , mRR=
1
(cid:88)N
RR , (14) acquiredbydifferentsensorsetups,suchasthedirectmerge
k 3×Acc
clean
N k
ofnuScenes[28](VelodyneHDL32Ewith32laserbeams)
k=1
and SemanticKITTI [3] (Velodyne HDL-64E with 64 laser
whereAcc denotesthemIoUscoreonthecleanvalida-
clean beams).
tion set of each dataset. Kindly refer to the original paper
Meanwhile, we also supplement the complete compari-
foradditionaldetails.
sonresultsamongtheSingle-DatasetTraining,Na¨ıveJoint
D.AdditionalExperimentalResults Training, andourproposedM3Netpipelinesandshowthe
results in Fig. 8. As can be seen, compared to the Single-
Inthissection,wepresentthecompleteexperimentalre- Dataset Training baselines, a na¨ıve merging of heteroge-
sultsasasupplementtothefindingsandconclusionsdrawn neousLiDARdatawillcausesevereperformancedegrada-
inthemainbodyofthispaper. tion. This observation holds true for both the MinkUNet
D.1.PilotStudy [21]backboneasinFig.8aandthePTv2+[105]backbone
asinFig.8b,whichhighlightsagaintheimportanceofcon-
Inthemainbodyofthispaper,weconductapilotstudy ductingalignmentswhenmergingmultipledrivingdatasets
to showcase the potential problems in the Single-Dataset for training. Notably, after proper data, feature, and label
Training and Na¨ıve Joint Training pipelines. Specifically, space alignments, we are able to combine the advantage
weobservethatitisnon-trivialtona¨ıvelycombinehetero- ofleveragingthediversetrainingdatasourcesandachieve
geneousdatafromdifferentdrivingdatasetswithlargedata better performance than the Single-Dataset Training base-
distributionandsensorconfigurationgapstotrainauniver- lines.Suchimprovementsareholistic,asshownintheradar
salLiDARsegmentationmodel. charts,ourproposedM3Netachievessuperiorperformance
We show in Fig. 7 our pilot study with the MinkUNet gainsoverthebaselinesunderallthetestedscenariosacross
[21] backbone in subfigure (a) and the PTv2+ [105] back- alltwelveLiDARsegmentationdatasets.
bone in subfigure (b), for both standalone and joint train-
ing setups. As can be seen, using either the classical
D.2.AblationStudy
MinkUNet or the most recent PTv2+ as the backbone, the
brutalcombinationwillunderminethesegmentationperfor- In this section, we supplement more fine-grained abla-
mance. Due to large discrepancies in aspects like sensor tionanalysisinthethirdcolumnofFig.9andFig.10onthe
configurations, data acquisitions, label mappings, and do- SemanticKITTI [3], nuScenes [28], and Waymo Open [92]
main shifts, the jointly trained representations tend to be datasets. Theresultsverifytheeffectivenessofeachofthe
19(a)M3Net(w/MinkUNet[21]backbone) (b)M3Net(w/PTv2+[105]backbone)
Figure8. PerformancecomparisonsamongM3Net[•],Single-DatasetTraining[•],andNa¨ıveJointTraining[•]acrosstwelveLiDAR
segmentationdatasets. Subfigure(a): M3Netw/ aMinkUNet[21]backbone. Subfigure(b): M3Netw/ aPTv2+[105]backbone. For
bettercomparisons,theradiusisnormalizedbasedonM3Net’sscores.Thelargertheareacoverage,thehighertheoverallperformance.
threealignmentsproposedinM3Net. allthefine-grainedmetrics. Theresultsverifytheeffective-
nessoftheproposedM3Netcomparedtothesinge-dataset
D.3.LiDARPanopticSegmentation trainingandna¨ıvejointtrainingbaselines.
In this section, we supplement the PQ, RQ, and SQ
D.4.Out-of-DistributionGeneralization
scores, as well as their fine-grained scores regarding the
things and stuff classes for our panoptic LiDAR segmen- In this section, we supplement the class-wise CE and
tationexperimentsontheSemanticKITTI[3]andPanoptic- RR scores of the out-of-training-distribution generaliza-
nuScenes[28]datasets. tion experiments on the SemanticKITTI-C and nuScenes-
C datasets in the Robo3D [49] benchmark. Specifically,
Tab. 13 and Tab. 14 show the per-corruption IoU scores
D.3.1 Panoptic-SemanticKITTI
of prior works, our baselines, and the proposed M3Net
ForthedetailedPQ,RQ,andSQscoresofourcompara- on the SemanticKITTI-C and nuScenes-C datasets, respec-
tivestudyontheSemanticKITTI[3]dataset,wesupplement tively.WeobservethatM3Netsetsupclearsuperiorityover
Tab.12tofacilitatedetailedcomparisonswithstate-of-the- prior arts across almost all eight corruption types. Such
art LiDAR segmentation approaches on the validation set. robust feature learning is crucial to the safe operation of
WeobservethattheproposedM3Netiscapableofachiev- autonomousvehiclesunderout-of-training-distributionsce-
ing new arts on the validation set, especially for the more narios,especiallyinsafety-criticalareas[49,111,112].
fine-grainedmetricslikeRQandSQ.Theresultsverifythe
effectivenessoftheproposedM3Netcomparedtothesinge- E.QualitativeAssessment
datasettrainingandna¨ıvejointtrainingbaselines.
In this section, we provide a comprehensive qualitative
assessmenttovalidatefurthertheeffectivenessandsuperi-
D.3.2 Panoptic-nuScenes orityoftheproposedM3Netframework.
For the detailed PQ, RQ, and SQ scores of our com-
E.1.VisualComparisons
parative study on the Panoptic-nuScenes [28] dataset, we
supplement Tab. 12 to facilitate detailed comparisons with We supplement several qualitative comparisons of our
state-of-the-artLiDARsegmentationapproachesontheval- proposed M3Net over the single-dataset training base-
idationset. WeobservethattheproposedM3Netiscapable line. Specifically, the visual comparisons across the Se-
of achieving new arts on the validation set, across almost manticKITTI [3], nuScenes [28], and Waymo Open [92]
20nuScenes SemanticKITTI WaymoOpen
NaiveJointTraining(MinkUNet) 59.84 54.03 65.39
CenterAlignment 60.12 54.45 65.41
Dataset-SpecificRasterization 62.29 54.12 65.47
DecoupledBatchNorm 75.11 68.59 68.64
[All]DataSpace 76.89 69.01 69.37
[+17.05mIoU] [+14.98mIoU] [+3.98mIoU]
Cross-ModalityAssistAlign 78.24 69.11 69.52
Domain-GuidedAlign(2Sets) 78.65 69.42 69.66
[All]Data&FeatureSpaces 78.88 69.64 69.70
[+19.04mIoU] [+15.61mIoU] [+4.31mIoU]
Text-DrivenAlign
78.33 69.13 69.15
[All]Data&LabelSpaces 78.90 69.34 69.55
[+19.06mIoU] [+15.31mIoU] [+4.16mIoU]
[M3Net]ThreeSpaceAlignments 79.00 69.85 70.15
53 60 67 74 81 46 53 60 67 74 62 65 68 71
Figure9.Ablationstudyofthedata,feature,andlabelspacealignmentsintheproposedM3Net(w/ MinkUNet[21]backbone).
nuScenes SemanticKITTI WaymoOpen
NaiveJointTraining(PTv2+) 69.65 61.59 67.00
CenterAlignment 70.00 61.91 67.05
Dataset-SpecificRasterization 70.97 62.03 67.22
DecoupledBatchNorm 78.88 69.39 71.61
[All]DataSpace 79.13 69.95 72.15
[+9.48mIoU] [+8.36mIoU] [+5.15mIoU]
Cross-ModalityAssistAlign 79.68 70.31 72.02
Domain-GuidedAlign(2Sets) 80.00 70.53 72.21
[All]Data&FeatureSpaces 80.26 70.87 72.33
[+10.61mIoU] [+9.28mIoU] [+5.33mIoU]
Text-DrivenAlign
79.81 70.47 72.18
[All]Data&LabelSpaces 80.45 71.13 72.30
[+10.80mIoU] [+9.54mIoU] [+5.30mIoU]
[M3Net]ThreeSpaceAlignments 80.90 72.00 72.40
65 69 73 77 81 57 61 65 69 73 64 67 70 73
Figure10.Ablationstudyofthedata,feature,andlabelspacealignmentsintheproposedM3Net(w/ PTv2+[105]backbone).
datasetsareshowninFig.11,Fig.12,and Fig.13,respec- posed M3Net in enhancing performance in the multi-task,
tively. Aswecansee,theproposedM3Netshowssuperior multi-dataset,multi-modalitytrainingsetting.Additionally,
performance than the baseline under different driving sce- wepresentqualitativeresultsinFig.14toshowcasetheca-
narios. Such results highlight the effectiveness of the pro- pabilityofM3NetintacklingboththeLiDARsemanticseg-
21mentationandpanopticsegmentationtasks. Aswecansee, • nuScenes-devkit4 .................ApacheLicense2.0
theproposedM3Netdemonstrateseffectivenessinmaking • SemanticKITTI5 ..................CCBY-NC-SA4.0
accurate predictions among the complex object and back- • SemanticKITTI-API6 ...................MITLicense
groundclassesinthedrivingscenes,underscoringitseffec- • WaymoOpenDataset7 ........WaymoDatasetLicense
tivenessinhandlingmulti-taskLiDARsegmentation. • RELLIS-3D8 .....................CCBY-NC-SA3.0
• SemanticPOSS9 ...........................Unknown
F.BroaderImpact • SemanticSTF10 ...................CCBY-NC-SA4.0
• SynLiDAR11 ...........................MITLicense
In this section, we elaborate on the positive societal in-
• DAPS-3D12 ............................MITLicense
fluence and potential limitations of our multi-task, multi-
• Robo3D13 ........................CCBY-NC-SA4.0
dataset,multi-modalityLiDARsegmentationframework.
G.2.PublicModelsUsed
F.1.PositiveSocietalInfluence
Weacknowledgetheuseofthefollowingpublicimple-
Inthiswork,wepresentaversatileLiDARsegmentation
mentations,duringthecourseofthiswork:
frameworkdubbedM3Netforconductingmulti-task,multi-
• MinkowskiEngine14 .....................MITLicense
dataset, multi-modalityLiDARsegmentationinaunifying
• PointTransformerV215 .....................Unknown
pipeline. LiDAR segmentation is crucial for the develop-
• spvnas16 ...............................MITLicense
ment of safe and reliable autonomous vehicles. By accu-
• Cylinder3D17.....................ApacheLicense2.0
rately interpreting the vehicle surroundings, LiDAR helps
• SLidR18 .........................ApacheLicense2.0
in obstacle detection, pedestrian safety, and navigation,
• OpenSeeD19 .....................ApacheLicense2.0
therebyreducingthelikelihoodofaccidentsandenhancing
• segment-anything20 ...............ApacheLicense2.0
road safety. LiDAR segmentation contributes significantly
• Segment-Any-Point-Cloud21 .......CCBY-NC-SA4.0
tosocietalwelfarethroughitsapplicationsinvariousfields.
• Mix3D22 ..................................Unknown
Its ability to provide accurate, detailed 3D representations
• LaserMix23 .......................CCBY-NC-SA4.0
ofphysicalenvironmentsenablesmoreinformeddecision-
making,enhancessafety,andpromotessustainability. G.3.PublicCodebasesUsed
F.2.PotentialLimitation We acknowledge the use of the following public code-
bases,duringthecourseofthiswork:
Although our proposed M3Net is capable of leveraging
• mmdetection3d24 .................ApacheLicense2.0
multiple heterogeneous driving datasets to train a versatile
• Pointcept25 .............................MITLicense
LiDAR segmentation network and achieve promising uni-
• OpenPCSeg26 ....................ApacheLicense2.0
versal LiDAR segmentation results, there still exists room
for improvement. Firstly, our framework leverages cali-
brated and synchronized camera data to assist the align-
4https://github.com/nutonomy/nuscenes-devkit.
ments. Sucharequirementmightnotbemetinsomeolder
5http://semantic-kitti.org.
LiDAR segmentation datasets. Secondly, we do not han- 6https://github.com/PRBonn/semantic-kitti-api.
dletheminorityclassesduringmulti-datasetlearning,espe- 7https://waymo.com/open.
cially for some dynamic classes that are uniquely defined
8http://www.unmannedlab.org/research/RELLIS-3D.
9http://www.poss.pku.edu.cn/semanticposs.html.
by a certain dataset. Thirdly, we do not consider the com-
10https://github.com/xiaoaoran/SemanticSTF.
bination of simulation data with real-world LiDAR point 11https://github.com/xiaoaoran/SynLiDAR.
clouds. We believe these aspects are promising for fu- 12https://github.com/subake/DAPS3D.
ture work to further improve our multi-task, multi-dataset, 13https://github.com/ldkong1205/Robo3D.
14https://github.com/NVIDIA/MinkowskiEngine.
multi-modalityLiDARsegmentationframework.
15https://github.com/Gofinge/PointTransformerV2.
16https://github.com/mit-han-lab/spvnas.
G.PublicResourcesUsed 17https://github.com/xinge008/Cylinder3D.
18https://github.com/valeoai/SLidR.
Inthissection,weacknowledgetheuseofdatasets,mod- 19https://github.com/IDEA-Research/OpenSeeD.
els,andcodebases,duringthecourseofthiswork. 20https://github.com/facebookresearch/segment-
anything.
G.1.PublicDatasetsUsed 21https://github.com/youquanl/Segment-Any-Point-
Cloud
Weacknowledgetheuseofthefollowingpublicdatasets, 22https://github.com/kumuji/mix3d.
duringthecourseofthiswork: 23https://github.com/ldkong1205/LaserMix.
• nuScenes3 ........................CCBY-NC-SA4.0 24https://github.com/open-mmlab/mmdetection3d.
25https://github.com/Pointcept/Pointcept.
3https://www.nuscenes.org/nuscenes. 26https://github.com/PJLab-ADG/OpenPCSeg.
22Table12. Theclass-wisepanopticsegmentationscoresonthevalsetsofthePanoptic-SemanticKITTI [3]andPanoptic-nuScenes[28]
datasets.Allscoresaregiveninpercentage(%).Foreachevaluatedmetric:bold-bestincolumn;underline-secondbestincolumn.
Panoptic-SemanticKITTI Panoptic-nuScenes
Method
PQ PQ† RQ SQ mIoU PQ PQ† RQ SQ mIoU
Panoptic-TrackNet[39] 40.0 - 48.3 73.0 53.8 51.4 56.2 63.3 80.2 58.0
Panoptic-PolarNet[129] 59.1 64.1 70.2 78.3 64.5 63.4 67.2 75.3 83.9 66.9
EfficientLPS[90] 59.2 65.1 69.8 75.0 64.9 59.2 62.8 82.9 70.7 69.4
DSNet[34] 61.4 65.2 72.7 79.0 69.6 64.7 67.6 76.1 83.5 76.3
Panoptic-PHNet[57] 61.7 - - - 65.7 74.7 77.7 84.2 88.2 79.7
Na¨ıveJoint(MinkUNet) 47.8 54.1 56.9 71.6 54.0 45.0 50.3 55.3 79.4 59.8
Single-Dataset(MinkUNet) 60.7 65.6 70.6 83.2 64.4 58.4 62.7 82.9 69.3 73.3
M3Net(MinkUNet) 63.9 68.5 73.2 82.3 69.9 67.9 71.1 78.1 85.9 79.0
Na¨ıveJoint(PTv2+) 56.0 59.6 65.8 73.7 61.6 56.7 60.6 66.8 83.5 69.7
Single-Dataset(PTv2+) 59.5 63.6 69.5 75.3 65.1 67.0 69.8 77.8 85.0 78.1
M3Net(PTv2+) 63.9 68.7 73.1 82.4 72.0 71.7 74.0 82.2 86.5 80.9
Table13.Theclass-wiserobustnessevaluationscoresontheSemanticKITTI-CdatasetfromtheRobo3Dbenchmark[49].Allscoresare
giveninpercentage(%).Foreachevaluatedmetric:bold-bestincolumn;underline-secondbestincolumn.
Method mCE↓ mRR↑
Na¨ıveJoint(MinkUNet) 113.7 84.7 48.5 54.0 39.8 41.1 49.1 39.8 47.7 46.1
Single-Dataset(MinkUNet) 95.1 85.0 50.6 52.3 51.4 54.5 59.3 56.9 56.2 56.6
M3Net(MinkUNet) 86.4 85.8 56.7 63.8 55.1 63.3 64.5 50.6 60.7 58.1
Na¨ıveJoint(PTv2+) 109.8 77.5 49.7 53.1 43.6 45.3 51.6 39.7 50.2 48.8
Single-Dataset(PTv2+) 92.7 85.9 52.3 53.7 51.8 55.8 60.2 56.4 59.3 57.6
M3Net(PTv2+) 83.3 84.0 60.4 66.1 52.7 63.9 65.1 55.1 62.6 57.9
Table14.Theclass-wiserobustnessevaluationscoresonthenuScenes-CdatasetfromtheRobo3Dbenchmark[49].Allscoresaregiven
inpercentage(%).Foreachevaluatedmetric:bold-bestincolumn;underline-secondbestincolumn.
Method mCE↓ mRR↑
PPKT[68] 105.6 76.1 64.0 72.2 59.1 57.2 63.9 36.3 60.6 39.6
SLidR[88] 106.1 76.0 65.4 72.3 56.0 56.1 62.9 41.9 61.2 38.9
Seal[67] 92.6 83.1 72.7 74.3 66.2 66.1 66.0 57.4 59.9 39.9
Na¨ıveJoint(MinkUNet) 129.0 81.5 54.0 57.3 50.9 57.5 47.3 42.3 49.4 30.9
Single-Dataset(MinkUNet) 99.6 79.1 60.7 74.6 50.8 65.0 67.1 32.4 63.2 50.0
M3Net(MinkUNet) 91.0 79.2 62.5 76.2 49.7 75.4 66.2 43.3 64.7 52.5
Na¨ıveJoint(PTv2+) 122.2 73.4 55.2 60.0 51.4 58.7 52.7 43.3 52.9 34.7
Single-Dataset(PTv2+) 89.6 79.1 63.1 76.4 51.6 75.2 67.9 41.4 65.4 53.5
M3Net(PTv2+) 85.9 78.2 54.4 78.0 51.2 76.8 68.0 44.3 66.7 55.9
23
gof
gof
dnuorg-tew
dnuorg-tew
wons
wons
rulb-noitom
rulb-noitom
gnissim-maeb
gnissim-maeb
klatssorc
klatssorc
ohce-etelpmocni
ohce-etelpmocni
rosnes-ssorc
rosnes-ssorcGround-Truth Singe-DatasetTraining M3Net(Ours)
Figure11.QualitativecomparisonsbetweentheSingle-DatasetTrainingandtheproposedM3NetforLiDARsemanticsegmentationon
theSemanticKITTIdataset[3].Tohighlightthedifferences,thecorrect/incorrectpredictionsarepaintedingray/red,respectively.
24Ground-Truth Singe-DatasetTraining M3Net(Ours)
Figure12.QualitativecomparisonsbetweentheSingle-DatasetTrainingandtheproposedM3NetforLiDARsemanticsegmentationon
thenuScenesdataset[28].Tohighlightthedifferences,thecorrect/incorrectpredictionsarepaintedingray/red,respectively.
25Ground-Truth Singe-DatasetTraining M3Net(Ours)
Figure13.QualitativecomparisonsbetweentheSingle-DatasetTrainingandtheproposedM3NetforLiDARsemanticsegmentationon
theWaymoOpendataset[92].Tohighlightthedifferences,thecorrect/incorrectpredictionsarepaintedingray/red,respectively.
26Ground-Truth M3Net(SemanticSegmentation) M3Net(PanopticSegmentation)
Figure14. QualitativecomparisonsbetweentheGround-TruthandtheproposedM3NetforLiDARpanopticsegmentationontheSe-
manticKITTIdataset[3]. Tohighlightthepanopticsegmentationeffect,thesemanticpredictionsinthethirdcolumnarepaintedingray.
Forpanopticsegmentationpredictions,eachcolor-codedclusterrepresentsadistinctinstance.
27References [11] Runnan Chen, Youquan Liu, Lingdong Kong, Nenglun
Chen, Xinge Zhu, Yuexin Ma, Tongliang Liu, and Wen-
[1] AngelikaAndo,SpyrosGidaris,AndreiBursuc,GillesPuy,
pingWang. Towardslabel-freesceneunderstandingbyvi-
AlexandreBoulch,andRenaudMarlet. Rangevit:Towards
sionfoundationmodels.InAdvancesinNeuralInformation
vision transformers for 3d semantic segmentation in au-
ProcessingSystems,2023. 2
tonomousdriving. InIEEE/CVFConferenceonComputer
[12] RunnanChen, YouquanLiu, LingdongKong, XingeZhu,
VisionandPatternRecognition,pages5240–5250,2023.1,
YuexinMa,YikangLi,YuenanHou,YuQiao,andWenping
2,8
Wang. Clip2scene:Towardslabel-efficient3dsceneunder-
[2] AlexeiBaevski,Wei-NingHsu,QiantongXu,ArunBabu,
standing by clip. In IEEE/CVF Conference on Computer
JiataoGu,andMichaelAuli. Data2vec: Ageneralframe-
Vision and Pattern Recognition, pages 7020–7030, 2023.
workforself-supervisedlearninginspeech,visionandlan-
2,3,4
guage. InInternationalConferenceonMachineLearning,
[13] RunnanChen,XingeZhu,NenglunChen,WeiLi,Yuexin
pages1298–1312,2022. 3
Ma,RuigangYang,andWenpingWang.Bridginglanguage
[3] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-
andgeometricprimitivesforzero-shotpointcloudsegmen-
zel,SvenBehnke,CyrillStachniss,andJuergenGall. Se- tation. In ACM International Conference on Multimedia,
mantickitti: Adatasetforsemanticsceneunderstandingof
pages5380–5388,2023. 2
lidarsequences.InIEEE/CVFInternationalConferenceon
[14] Tian Chen, Shijie An, Yuan Zhang, Chongyang Ma,
ComputerVision,pages9297–9307,2019. 2,3,4,6,7,8,
HuayanWang,XiaoyanGuo,andWenZheng. Improving
9,10,11,13,15,16,17,19,20,23,24,27
monoculardepthestimationbyleveragingstructuralaware-
[4] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen- nessandcomplementarydatasets.InEuropeanConference
zel, Sven Behnke, Ju¨rgen Gall, and Cyrill Stachniss. To- onComputerVision,pages90–108,2020. 2
wards 3d lidar-based semantic scene understanding of 3d
[15] YanbeiChen,ManchenWang,AbhayMittal,ZhenlinXu,
pointcloudsequences: Thesemantickittidataset. Interna-
PaoloFavaro,JosephTighe,andDavideModolo.Scaledet:
tionalJournalofRoboticsResearch,40:959–96,2021. 2, Ascalablemulti-datasetobjectdetector.InIEEE/CVFCon-
3 ferenceonComputerVisionandPatternRecognition,pages
[5] Maxim Berman, Amal Rannen Triki, and Matthew B 7288–7297,2023. 2
Blaschko. The lova´sz-softmax loss: a tractable surrogate [16] BowenCheng,AlexSchwing,andAlexanderKirillov.Per-
fortheoptimizationoftheintersection-over-unionmeasure pixelclassificationisnotallyouneedforsemanticsegmen-
inneuralnetworks.InIEEE/CVFConferenceonComputer tation. InAdvancesinNeuralInformationProcessingSys-
VisionandPatternRecognition,pages4413–4421,2018.6, tems,pages17864–17875,2021. 2
17
[17] Bowen Cheng, Ishan Misra, Alexander G. Schwing,
[6] Mario Bijelic, Tobias Gruber, Fahim Mannan, Florian Alexander Kirillov, and Rohit Girdhar. Masked-attention
Kraus, Werner Ritter, Klaus Dietmayer, and Felix Heide. mask transformer for universal image segmentation. In
Seeing through fog without seeing fog: Deep multimodal IEEE/CVF Conference on Computer Vision and Pattern
sensor fusion in unseen adverse weather. In IEEE/CVF Recognition,pages1290–1299,2022. 2
Conference on Computer Vision and Pattern Recognition, [18] HuixianCheng,XianfengHan,andGuoqiangXiao.Cenet:
pages11682–11692,2020. 11 Toward concise and efficient lidar semantic segmentation
[7] AlexandreBoulch,CorentinSautier,Bjo¨rnMichele,Gilles forautonomousdriving. InIEEEInternationalConference
Puy, and Renaud Marlet. Also: Automotive lidar self- onMultimediaandExpo,pages1–6,2022. 2
supervisionbyoccupancyestimation. InIEEE/CVFCon- [19] Ran Cheng, Ryan Razani, Ehsan Taghavi, Enxu Li, and
ferenceonComputerVisionandPatternRecognition,pages Bingbing Liu. Af2-s3net: Attentive feature fusion with
13455–13465,2023. 1,2 adaptivefeatureselectionforsparsesemanticsegmentation
[8] HolgerCaesar,VarunBankiti,AlexHLang,SourabhVora, network.InIEEE/CVFConferenceonComputerVisionand
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, PatternRecognition,pages12547–12556,2021. 2,8
GiancarloBaldan,andOscarBeijbom. nuscenes: Amulti- [20] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
modaldatasetforautonomousdriving. InIEEE/CVFCon- Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
ferenceonComputerVisionandPatternRecognition,pages Barham, Hyung Won Chung, Charles Sutton, Sebas-
11621–11631,2020. 2,3,4,9,11 tian Gehrmann, Parker Schuh, Kensen Shi, Sasha
[9] MathildeCaron,HugoTouvron,IshanMisra,Herve´Je´gou, Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Barnes,YiTay,NoamShazeer,VinodkumarPrabhakaran,
Emergingpropertiesinself-supervisedvisiontransformers. EmilyReif,NanDu,BenHutchinson,ReinerPope,James
In IEEE/CVF International Conference on Computer Vi- Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari,
sion,pages9650–9660,2021. 3 PengchengYin,TojuDuke,AnselmLevskaya,SanjayGhe-
[10] JunCen,ShiweiZhang,YixuanPei,KunLi,HangZheng, mawat, Sunipa Dev, Henryk Michalewski, Xavier Gar-
Maochun Luo, Yingya Zhang, and Qifeng Chen. Cmd- cia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny
fusion: Bidirectional fusion network with cross-modality Zhou,DaphneIppolito,DavidLuan,HyeontaekLim,Bar-
knowledge distillation for lidar semantic segmentation. retZoph,AlexanderSpiridonov,RyanSepassi,DavidDo-
arXivpreprintarXiv:2307.04091,2023. 3 han, Shivani Agrawal, Mark Omernick, Andrew M. Dai,
28ThanumalayanSankaranarayanaPillai,MariePellat,Aitor [33] KaimingHe,GeorgiaGkioxari,PiotrDolla´r,andRossGir-
Lewkowycz,EricaMoreira,RewonChild,OleksandrPolo- shick. Maskr-cnn. InIEEE/CVFInternationalConference
zov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren- onComputerVision,pages2961–2969,2017. 2
nanSaeta,MarkDiaz,OrhanFirat,MicheleCatasta,Jason [34] FangzhouHong,HuiZhou,XingeZhu,HongshengLi,and
Wei,KathyMeier-Hellstern,DouglasEck,JeffDean,Slav ZiweiLiu.Lidar-basedpanopticsegmentationviadynamic
Petrov,andNoahFiedel.Palm:Scalinglanguagemodeling shifting network. In IEEE/CVF Conference on Computer
withpathways. arXivpreprintarXiv:2204.02311,2022. 3 VisionandPatternRecognition,pages13090–13099,2021.
[21] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 6,7,15,18,23
4d spatio-temporal convnets: Minkowski convolutional [35] Fangzhou Hong, Lingdong Kong, Hui Zhou, Xinge Zhu,
neural networks. In IEEE/CVF Conference on Computer HongshengLi,andZiweiLiu. Unified3dand4dpanoptic
Vision and Pattern Recognition, pages 3075–3084, 2019. segmentationviadynamicshiftingnetworks. IEEETrans-
2,3,4,6,7,8,15,17,18,19,20,21 actions on Pattern Analysis and Machine Intelligence, 46
[22] Dorin Comaniciu and Peter Meer. Mean shift: A robust (5):3480–3495,2024. 2,6,15
approachtowardfeaturespaceanalysis.IEEETransactions
[36] Yuenan Hou, Xinge Zhu, Yuexin Ma, Chen Change Loy,
onPatternAnalysisandMachineIntelligence, 24(5):603–
and Yikang Li. Point-to-voxel knowledge distillation for
619,2005. 6,15 lidar semantic segmentation. In IEEE/CVF Conference
[23] MMDetection3D Contributors. MMDetection3D: Open- onComputerVisionandPatternRecognition,pages8479–
MMLab next-generation platform for general 3D object 8488,2022. 8
detection. https://github.com/open-mmlab/
[37] QingyongHu, BoYang, LinhaiXie, StefanoRosa, Yulan
mmdetection3d,2020. 6,16
Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham.
[24] Pointcept Contributors. Pointcept: A codebase for point
Randla-net: Efficientsemanticsegmentationoflarge-scale
cloud perception research. https://github.com/
pointclouds.InIEEE/CVFConferenceonComputerVision
Pointcept/Pointcept,2023. 6,16
andPatternRecognition,pages11108–11117,2020. 2
[25] Tiago Cortinhal, George Tzelepis, and Eren Erdal Aksoy.
[38] Qingyong Hu, Bo Yang, Sheikh Khalid, Wen Xiao, Niki
Salsanext: Fast, uncertainty-aware semantic segmentation
Trigoni, and Andrew Markham. Towards semantic seg-
oflidarpointclouds.InInternationalSymposiumonVisual
mentationofurban-scale3dpointclouds:Adataset,bench-
Computing,pages207–222,2020. 2,8
marksandchallenges. InIEEE/CVFConferenceonCom-
[26] LaurensVanderMaatenandGeoffreyHinton. Visualizing
puter Vision and Pattern Recognition, pages 4977–4987,
datausingt-sne. JournalofMachineLearningResearch,9
2021. 1
(11),2008. 4,7
[39] Juana Valeria Hurtado, Rohit Mohan, Wolfram Burgard,
[27] Christoph Feichtenhofer, Yanghao Li, and Kaiming He.
and Abhinav Valada. Mopt: Multi-object panoptic track-
Masked autoencoders as spatiotemporal learners. In Ad-
ing. arXivpreprintarXiv:2004.08189,2020. 7,23
vancesinNeuralInformationProcessingSystems,2022. 3
[40] JiteshJain,JiachenLi,MangTikChiu,AliHassani,Nikita
[28] WhyeKitFong,RohitMohan,JuanaValeriaHurtado,Lub-
Orlov,andHumphreyShi. Oneformer: Onetransformerto
ingZhou,HolgerCaesar,OscarBeijbom,andAbhinavVal-
ruleuniversalimagesegmentation. InIEEE/CVFConfer-
ada. Panopticnuscenes: Alarge-scalebenchmarkforlidar
ence on Computer Vision and Pattern Recognition, pages
panoptic segmentation and tracking. IEEE Robotics and
2989–2998,2023. 2
AutomationLetters,7:3795–3802,2022. 2,3,4,6,7,8,9,
[41] MaximilianJaritz,Tuan-HungVu,RaouldeCharette,Em-
10,11,12,15,16,19,20,23,25
ilieWirbel,andPatrickPe´rez. xmuda: Cross-modalunsu-
[29] Biao Gao, Yancheng Pan, Chengkun Li, Sibo Geng, and
perviseddomainadaptationfor3dsemanticsegmentation.
HuijingZhao.Arewehungryfor3dlidardataforsemantic
InIEEE/CVFConferenceonComputerVisionandPattern
segmentation? a survey of datasets and methods. IEEE
Recognition,pages12605–12614,2020. 1,2,3
Transactions on Intelligent Transportation Systems, 2021.
1 [42] Maximilian Jaritz, Tuan-Hung Vu, Raoul De Charette,
E´milie Wirbel, and Patrick Pe´rez. Cross-modal learning
[30] AndreasGeiger,PhilipLenz,andRaquelUrtasun. Arewe
fordomainadaptationin3dsemanticsegmentation. IEEE
readyforautonomousdriving? thekittivisionbenchmark
suite. In IEEE/CVF Conference on Computer Vision and
TransactionsonPatternAnalysisandMachineIntelligence,
PatternRecognition,pages3354–3361,2012. 9 45(2):1533–1544,2023. 1,2,3
[31] Xiuye Gu, Yin Cui, Jonathan Huang, Abdullah Rashwan, [43] PengJiang, PhilipOsteen, MaggieWigness, andSrikanth
XuanYang, XingyiZhou, GolnazGhiasi, WeichengKuo, Saripallig. Rellis-3ddataset: Data, benchmarksandanal-
Huizhong Chen, Liang-Chieh Chen, and David A. Ross. ysis. In IEEE International Conference on Robotics and
Dataseg: Tamingauniversalmulti-datasetmulti-taskseg- Automation,pages1110–1116,2021. 6,8,10,15
mentationmodel. arXivpreprintarXiv:2306.01736,2023. [44] TarunKalluri, GirishVarma, ManmohanChandraker, and
2 C. V. Jawahar. Universal semi-supervised semantic seg-
[32] Yi Gu, Yuming Huang, Chengzhong Xu, and Hui mentation. In IEEE/CVF International Conference on
Kong. Maskrange: A mask-classification model for ComputerVision,pages5259–5270,2019. 2
range-view based lidar segmentation. arXiv preprint [45] DongwanKim,Yi-HsuanTsai,YuminSuh,MasoudFaraki,
arXiv:2206.12073,2022. 2 Sparsh Garg, Manmohan Chandraker, and Bohyung Han.
29Learning semantic segmentation from multiple datasets [57] JinkeLi,XiaoHe,YangWen,YuanGao,XiaoqiangCheng,
with label shifts. In European Conference on Computer and Dan Zhang. Panoptic-phnet: Towards real-time and
Vision,pages20–36,2022. 2 high-precision lidar panoptic segmentation via clustering
[46] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi pseudo heatmap. In IEEE/CVF Conference on Computer
Mao,ChloeRolland,LauraGustafson,TeteXiao,Spencer VisionandPatternRecognition,pages11809–11818,2022.
Whitehead,AlexanderC.Berg,Wan-YenLo,PiotrDolla´r, 7,18,23
and Ross Girshick. Segment anything. In IEEE/CVF In- [58] LiLi,HubertPHShum,andTobyP.Breckon.Lessismore:
ternational Conference on Computer Vision, pages 4015– Reducingtaskandmodelcomplexityfor3dpointcloudse-
4026,2023. 5 mantic segmentation. In IEEE/CVF Conference on Com-
[47] Alexey Klokov, Di Un Pak, Aleksandr Khorin, Dmitry puter Vision and Pattern Recognition, pages 9361–9371,
Yudin, Leon Kochiev, Vladimir Luchinskiy, and Vitaly 2023. 2
Bezuglyj. Daps3d: Domain adaptive projective segmen- [59] Rong Li, Raoul de Charette, and C. A. O. Anh-Quan.
tation of 3d lidar point clouds. IEEE Access, 11:79341– Coarse3d: Class-prototypes for contrastive learning in
79356,2023. 6,8,10,15 weakly-supervised3dpointcloudsegmentation. InBritish
[48] LingdongKong,YouquanLiu,RunnanChen,YuexinMa, MachineVisionConference,2022. 2
Xinge Zhu, Yikang Li, Yuenan Hou, Yu Qiao, and Zi- [60] Xin Li, Botian Shi, Yuenan Hou, Xingjiao Wu, Tianlong
wei Liu. Rethinking range view representation for lidar Ma,YikangLi,andLiangHe. Homogeneousmulti-modal
segmentation. In IEEE/CVF International Conference on feature fusion and interaction for 3d object detection. In
ComputerVision,pages228–240,2023. 2,8,17 EuropeanConferenceonComputerVision,pages691–707.
[49] LingdongKong,YouquanLiu,XinLi,RunnanChen,Wen- Springer,2022. 2
wei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and Ziwei [61] Xiangtai Li, Wenwei Zhang, Jiangmiao Pang, Kai Chen,
Liu. Robo3d: Towards robust and reliable 3d perception Guangliang Cheng, Yunhai Tong, and Chen Change Loy.
against corruptions. In IEEE/CVF International Confer- Video k-net: A simple, strong, and unified baseline for
ence on Computer Vision, pages 19994–20006, 2023. 1, video segmentation. In IEEE/CVF Conference on Com-
2,6,8,10,11,15,18,20,23 puterVisionandPatternRecognition,pages18847–18857,
[50] Lingdong Kong, Niamul Quader, and Venice Erin Liong. 2022. 2
Conda: Unsuperviseddomainadaptationforlidarsegmen- [62] Xin Li, Tao Ma, Yuenan Hou, Botian Shi, Yuchen Yang,
tation via regularized domain concatenation. In IEEE In- YouquanLiu,XingjiaoWu,QinChen,YikangLi,YuQiao,
ternationalConferenceonRoboticsandAutomation,pages etal. Logonet: Towardsaccurate3dobjectdetectionwith
9338–9345,2023. 1,2,17 local-to-globalcross-modalfusion. InIEEE/CVFConfer-
[51] Lingdong Kong, Jiawei Ren, Liang Pan, and Ziwei Liu. ence on Computer Vision and Pattern Recognition, pages
Lasermixforsemi-supervisedlidarsemanticsegmentation. 17524–17534,2023. 2,4
InIEEE/CVFConferenceonComputerVisionandPattern [63] Ye Li, Lingdong Kong, Hanjiang Hu, Xiaohao Xu, and
Recognition,pages21705–21715,2023. 2,17 Xiaonan Huang. Optimizing lidar placements for robust
[52] Lingdong Kong, Shaoyuan Xie, Hanjiang Hu, Lai Xing driving perception in adverse conditions. arXiv preprint
Ng,BenoitR.Cottereau,andWeiTsangOoi. Robodepth: arXiv:2403.17009,2024. 2
Robust out-of-distribution depth estimation under corrup- [64] Venice Erin Liong, Thi Ngoc Tho Nguyen, Sergi Wid-
tions. InAdvancesinNeuralInformationProcessingSys- jaja,DhananjaiSharma,andZhuangJieChong. Amvnet:
tems,2023. 1,8 Assertion-basedmulti-viewfusionnetworkforlidarseman-
[53] LingdongKong,XiangXu,JunCen,WenweiZhang,Liang ticsegmentation. arXivpreprintarXiv:2012.04934,2020.
Pan,KaiChen,andZiweiLiu. Calib3d:Calibratingmodel 2,8
preferences for reliable 3d scene understanding. arXiv [65] MinghuaLiu,YinZhou,CharlesR.Qi,BoqingGong,Hao
preprintarXiv:2403.17010,2024. 2 Su,andDragomirAnguelov. Less: Label-efficientseman-
[54] XinLai, YukangChen, FanbinLu, JianhuiLiu, andJiaya ticsegmentationforlidarpointclouds. InEuropeanCon-
Jia. Spherical transformer for lidar-based 3d recognition. ferenceonComputerVision,pages70–89,2022. 2
InIEEE/CVFConferenceonComputerVisionandPattern [66] Youquan Liu, Runnan Chen, Xin Li, Lingdong Kong,
Recognition,pages17545–17555,2023. 2,8 YuchenYang,ZhaoyangXia,YeqiBai,XingeZhu,Yuexin
[55] JohnLambert, ZhuangLiu, OzanSener, JamesHays, and Ma,YikangLi,YuQiao,andYuenanHou. Uniseg: Auni-
Vladlen Koltun. Mseg: A composite dataset for multi- fiedmulti-modallidarsegmentationnetworkandtheopen-
domainsemanticsegmentation. InIEEE/CVFConference pcsegcodebase.InIEEE/CVFInternationalConferenceon
onComputerVisionandPatternRecognition,pages2879– ComputerVision,pages21662–21673,2023. 2,4
2888,2020. 2 [67] Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen,
[56] Guangrui Li, Guoliang Kang, Xiaohan Wang, Yunchao WenweiZhang,LiangPan,KaiChen,andZiweiLiu. Seg-
Wei, and Yi Yang. Adversarially masking synthetic to mentanypointcloudsequencesbydistillingvisionfounda-
mimic real: Adaptive noise injection for point cloud seg- tionmodels.InAdvancesinNeuralInformationProcessing
mentationadaptation. InIEEE/CVFConferenceonCom- Systems,2023. 2,3,8,15,23
puterVisionandPatternRecognition,pages20464–20474, [68] Yueh-Cheng Liu, Yu-Kai Huang, Hung-Yueh Chiang,
2023. 2 Hung-Ting Su, Zhe-Yu Liu, Chin-Tang Chen, Ching-Yu
30Tseng,andWinstonH.Hsu.Learningfrom2d:Contrastive Openscene: 3dsceneunderstandingwithopenvocabular-
pixel-to-pointknowledgetransferfor3dpretraining. arXiv ies.InIEEE/CVFConferenceonComputerVisionandPat-
preprintarXiv:2104.0468,2021. 3,8,23 ternRecognition,pages815–824,2023. 3
[69] ZhijianLiu,AlexanderAminiHaotianTang,XinyuYang, [81] XidongPeng, RunnanChen, FengQiao, LingdongKong,
Huizi Mao, Daniela L. Rus, and Song Han. Bevfu- YouquanLiu,TaiWang,XingeZhu,andYuexinMa. Sam-
sion:Multi-taskmulti-sensorfusionwithunifiedbird’s-eye guided unsupervised domain adaptation for 3d segmenta-
viewrepresentation. InIEEEInternationalConferenceon tion. arXivpreprintarXiv:2310.08820,2023. 1,2
RoboticsandAutomation,pages2774–2781,2023. 3 [82] GillesPuy,AlexandreBoulch,andRenaudMarlet.Usinga
[70] Ilya Loshchilov and Frank Hutter. Decoupled weight de- waffleironforautomotivepointcloudsemanticsegmenta-
cayregularization. InInternationalConferenceonLearn- tion. InIEEE/CVFInternationalConferenceonComputer
ingRepresentations,2019. 7,16 Vision,pages3379–3389,2023. 2,8
[71] Yuhang Lu, Qi Jiang, Runnan Chen, Yuenan Hou, Xinge [83] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Zhu,andYuexinMa. Seemoreandknowmore:Zero-shot Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
point cloud segmentation via multi-modal visual data. In Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
IEEE/CVF International Conference on Computer Vision, Krueger, and Ilya Sutskever. Learning transferable visual
pages21674–21684,2023. 2 modelsfromnaturallanguagesupervision.InInternational
[72] JiagengMao, ShaoshuaiShi, XiaogangWang, andHong- ConferenceonMachineLearning,pages8748–8763,2021.
sheng Li. 3d object detection for autonomous driving: A 3
comprehensivesurvey. InternationalJournalofComputer
[84] Rene´ Ranftl, Katrin Lasinger, David Hafner, Konrad
Vision,2023. 1
Schindler, and Vladlen Koltun. Towards robust monocu-
[73] Rodrigo Marcuzzi, Lucas Nunes, Louis Wiesmann, Jens
lar depth estimation: Mixing datasets for zero-shot cross-
Behley, and Cyrill Stachniss. Mask-based panoptic lidar
datasettransfer.IEEETransactionsonPatternAnalysisand
segmentationforautonomousdriving. IEEERoboticsand
MachineIntelligence,44(3):1623–1637,2020. 2
AutomationLetters,8(2):1141–1148,2023. 2
[85] Giulia Rizzoli, Francesco Barbato, and Pietro Zanuttigh.
[74] PanagiotisMeletisandGijsDubbelman. Trainingofcon-
Multimodalsemanticsegmentationinautonomousdriving:
volutionalnetworksonmultipleheterogeneousdatasetsfor
A review of current approaches and future perspectives.
streetscenesemanticsegmentation.InIEEEIntelligentVe-
Technologies,10(4),2022. 1
hiclesSymposium,pages1045–1050,2018. 2
[86] Cristiano Saltori, Fabio Galasso, Giuseppe Fiameni, Nicu
[75] Bjo¨rnMichele,AlexandreBoulch,GillesPuy,Tuan-Hung
Sebe, Elisa Ricci, and Fabio Poiesi. Cosmix: Composi-
Vu,RenaudMarlet,andNicolasCourty. Saluda: Surface-
tionalsemanticmixfordomainadaptationin3dlidarseg-
based automotive lidar unsupervised domain adaptation.
mentation. In European Conference on Computer Vision,
arXivpreprintarXiv:2304.03251,2023. 2
pages586–602,2022. 2
[76] Andres Milioto, Ignacio Vizzo, Jens Behley, and Cyrill
[87] Jules Sanchez, Jean-Emmanuel Deschaud, and Franc¸ois
Stachniss. Rangenet++: Fast and accurate lidar semantic
Goulette. Cola: Coarse-label multi-source lidar seman-
segmentation. In IEEE/RSJ International Conference on
tic segmentation for autonomous driving. arXiv preprint
IntelligentRobotsandSystems,pages4213–4220,2019. 2,
arXiv:2311.03017,2023. 2
4,8
[88] Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre
[77] AlexeyNekrasov, JonasSchult, OrLitany, BastianLeibe,
Boulch,AndreiBursuc,andRenaudMarlet.Image-to-lidar
andFrancisEngelmann. Mix3d: Out-of-contextdataaug-
self-superviseddistillationforautonomousdrivingdata. In
mentationfor3dscenes.InInternationalConferenceon3D
IEEE/CVF Conference on Computer Vision and Pattern
Vision,pages116–125,2021. 2,7,17
Recognition,pages9891–9901,2022. 2,3,8,23
[78] MaximeOquab,Timothe´eDarcet,The´oMoutakanni,Huy
[89] Alvari Seppa¨nen, Risto Ojala, and Kari Tammi. 4de-
Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,
noisenet: Adverse weather denoising from adjacent point
Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,
clouds.IEEERoboticsandAutomationLetters,8:456–463,
MahmoudAssran,NicolasBallas,WojciechGaluba,Rus-
2022. 1
sell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra,
Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu [90] KshitijSirohi,RohitMohan,DanielBu¨scher,WolframBur-
Xu, Herve´ Jegou, Julien Mairal, Patrick Labatut, Armand gard, and Abhinav Valada. Efficientlps: Efficient lidar
Joulin, and Piotr Bojanowski. Dinov2: Learning ro- panoptic segmentation. IEEE Transactions on Robotics,
bust visual features without supervision. arXiv preprint 2021. 7,18,23
arXiv:2304.07193,2023. 3 [91] Louis Soum-Fontez, Jean-Emmanuel Deschaud, and
[79] YanchengPan,BiaoGao,JilinMei,SiboGeng,Chengkun Franc¸ois Goulette. Mdt3d: Multi-dataset training for li-
Li,andHuijingZhao. Semanticposs:Apointclouddataset dar 3d object detection generalization. arXiv preprint
withlargequantityofdynamicinstances. InIEEEIntelli- arXiv:2308.01000,2023. 1,2
gentVehiclesSymposium,pages687–693,2020. 6,8,10, [92] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aure-
15 lienChouard,VijaysaiPatnaik,PaulTsui,JamesGuo,Yin
[80] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan,
Tagliasacchi, Marc Pollefeys, and Thomas Funkhouser. Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev,
31ScottEttinger, MaximKrivokon, AmyGao, AdityaJoshi, domaingapfor3dobjectdetection. InEuropeanConfer-
YuZhang, JonathonShlens, ZhifengChen, andDragomir enceonComputerVision,pages179–195.Springer,2022.
Anguelov. Scalabilityinperceptionforautonomousdriv- 4
ing: Waymo open dataset. In IEEE/CVF Conference on [105] XiaoyangWu,YixingLao,LiJiang,XihuiLiu,andHeng-
Computer Vision and Pattern Recognition, pages 2446– shuangZhao. Pointtransformerv2: Groupedvectoratten-
2454, 2020. 2, 3, 4, 6, 7, 8, 9, 10, 11, 12, 14, 15, 16, tion and partition-based pooling. In Advances in Neural
18,19,20,26 InformationProcessingSystems,2022. 2,7,8,15,16,17,
[93] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji 19,20,21
Lin, HanruiWang, andSongHan. Searchingefficient3d [106] XiaoyangWu,ZhuotaoTian,XinWen,BohaoPeng,Xihui
architectures with sparse point-voxel convolution. In Eu- Liu,KaichengYu,andHengshuangZhao. Towardslarge-
ropean Conference on Computer Vision, pages 685–702, scale 3d representation learning with multi-dataset point
2020. 2,4,8 prompt training. arXiv preprint arXiv:2308.09718, 2023.
[94] HuguesThomas,CharlesRQi,Jean-EmmanuelDeschaud, 2
Beatriz Marcotegui, Franc¸ois Goulette, and Leonidas J [107] AoranXiao,JiaxingHuang,DayanGuan,KaiwenCui,Shi-
Guibas. Kpconv: Flexibleanddeformableconvolutionfor jianLu,andLingShao. Polarmix:Ageneraldataaugmen-
point clouds. In IEEE/CVF International Conference on tationtechniqueforlidarpointclouds.InAdvancesinNeu-
ComputerVision,pages6411–6420,2019. ral Information Processing Systems, pages 11035–11048,
[95] LarissaTTriess,DavidPeter,ChristophBRist,andJMar- 2022. 2,17
ius Zo¨llner. Scan-based semantic segmentation of lidar [108] AoranXiao,JiaxingHuang,DayanGuan,FangnengZhan,
point clouds: An experimental study. In IEEE Intelligent and Shijian Lu. Transfer learning from synthetic to real
VehiclesSymposium,pages1116–1121,2020. 2 lidarpointcloudforsemanticsegmentation. InAAAICon-
[96] LarissaT.Triess,MariellaDreissig,ChristophB.Rist,and ferenceonArtificialIntelligence,pages2795–2803,2022.
J.MariusZo¨llner. Asurveyondeepdomainadaptationfor 1,2,6,8,10,15
lidar perception. In IEEE Intelligent Vehicles Symposium [109] Aoran Xiao, Jiaxing Huang, Weihao Xuan, Ruijie Ren,
Workshop,pages350–357,2021. 1 Kangcheng Liu, Dayan Guan, Abdulmotaleb El Saddik,
[97] Darren Tsai, Julie Stephany Berrio, Mao Shan, Eduardo ShijianLu,andEricXing.3dsemanticsegmentationinthe
Nebot, and Stewart Worrall. Ms3d: Leveraging multiple wild: Learning generalized models for adverse-condition
detectorsforunsuperviseddomainadaptationin3dobject point clouds. In IEEE/CVF Conference on Computer Vi-
detection. arXivpreprintarXiv:2304.02431,2023. 2 sionandPatternRecognition,pages9382–9392,2023. 6,
[98] Darren Tsai, Julie Stephany Berrio, Mao Shan, Eduardo 8,10,15
Nebot,andStewartWorrall. Ms3d++:Ensembleofexperts [110] Zeqi Xiao, Wenwei Zhang, Tai Wang, Chen Change Loy,
formulti-sourceunsuperviseddomainadaptionin3dobject Dahua Lin, and Jiangmiao Pang. Position-guided point
detection. arXivpreprintarXiv:2308.05988,2023. 2 cloud panoptic segmentation transformer. arXiv preprint
[99] Ozan Unal, Dengxin Dai, and Luc Van Gool. Scribble- arXiv:2303.13509,2023. 2
supervisedlidarsemanticsegmentation.InIEEE/CVFCon- [111] Shaoyuan Xie, Lingdong Kong, Wenwei Zhang, Jiawei
ferenceonComputerVisionandPatternRecognition,pages Ren,LiangPan,KaiChen,andZiweiLiu. Robobev: To-
2697–2707,2022. 2 wardsrobustbird’seyeviewperceptionundercorruptions.
[100] HuiyuWang,YukunZhu,HartwigAdam,AlanYuille,and arXivpreprintarXiv:2304.06719,2023. 8,20
Liang-Chieh Chen. Max-deeplab: End-to-end panoptic [112] Shaoyuan Xie, Lingdong Kong, Wenwei Zhang, Jiawei
segmentationwithmasktransformers. InIEEE/CVFCon- Ren, Liang Pan, Kai Chen, and Ziwei Liu. Benchmark-
ferenceonComputerVisionandPatternRecognition,pages ingandanalyzingbird’seyeviewperceptionrobustnessto
5463–5474,2021. 2,4 corruptions. Preprint,2023. 8,20
[101] XudongWang,ZhaoweiCai,DashanGao,andNunoVas- [113] ChenfengXu, BichenWu, ZiningWang, WeiZhan, Peter
concelos.Towardsuniversalobjectdetectionbydomainat- Vajda, KurtKeutzer, andMasayoshiTomizuka. Squeeze-
tention. InIEEE/CVFConferenceonComputerVisionand segv3: Spatially-adaptive convolution for efficient point-
PatternRecognition,pages7289–7298,2019. 2 cloudsegmentation. InEuropeanConferenceonComputer
[102] Xudong Wang, Shufan Li, Konstantinos Kallidromitis, Vision,pages1–19,2020. 1,2,4
Yusuke Kato, Kazuki Kozuka, and Trevor Darrell. Hier- [114] JianyunXu,RuixiangZhang,JianDou,YushiZhu,JieSun,
archicalopen-vocabularyuniversalimagesegmentation. In andShiliangPu. Rpvnet:Adeepandefficientrange-point-
AdvancesinNeuralInformationProcessingSystems,2023. voxelfusionnetworkforlidarpointcloudsegmentation.In
2 IEEE/CVF International Conference on Computer Vision,
[103] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, pages16024–16033,2021. 2,8
Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting [115] JingyiXu, WeidongYang, LingdongKong, YouquanLiu,
everything in context. arXiv preprint arXiv:2304.03284, Rui Zhang, Qingyuan Zhou, and Ben Fei. Visual foun-
2023. 3 dation models boost cross-modal unsupervised domain
[104] YiWei,ZibuWei,YongmingRao,JiaxinLi,JieZhou,and adaptation for 3d semantic segmentation. arXiv preprint
Jiwen Lu. Lidar distillation: Bridging the beam-induced arXiv:2403.10001,2024. 2
32[116] XiangXu,LingdongKong,HuiShuai,andQingshanLiu. [129] ZixiangZhou,YangZhang,andHassanForoosh.Panoptic-
Frnet: Frustum-rangenetworksforscalablelidarsegmen- polarnet: Proposal-freelidarpointcloudpanopticsegmen-
tation. arXivpreprintarXiv:2312.04484,2023. 8,17 tation. InIEEE/CVFConferenceonComputerVisionand
[117] XuYan,JiantaoGao,ChaodaZheng,ChaoZheng,Ruimao PatternRecognition,pages13194–13203,2021. 7,18,23
Zhang, Shuguang Cui, and Zhen Li. 2dpass: 2d priors [130] XingeZhu,HuiZhou,TaiWang,FangzhouHong,Yuexin
assisted semantic segmentation on lidar point clouds. In Ma,WeiLi,HongshengLi,andDahuaLin.Cylindricaland
EuropeanConferenceonComputerVision,pages677–695, asymmetrical3dconvolutionnetworksforlidarsegmenta-
2022. 3,17 tion. In IEEE/CVF Conference on Computer Vision and
[118] JihanYang,ShaoshuaiShi,ZheWang,HongshengLi,and PatternRecognition,pages9939–9948,2021. 1,2,4,8
XiaojuanQi. St3d: Self-trainingforunsuperviseddomain [131] Zhuangwei Zhuang, Rong Li, Kui Jia, Qicheng Wang,
adaptation on 3d object detection. In IEEE/CVF Confer- Yuanqing Li, and Mingkui Tan. Perception-aware multi-
ence on Computer Vision and Pattern Recognition, pages sensor fusion for 3d lidar semantic segmentation. In
10368–10378,2021. 4 IEEE/CVF International Conference on Computer Vision,
[119] DongqiangziYe,ZixiangZhou,WeijiaChen,YufeiXie,Yu pages16280–16290,2021. 2
Wang, Panqu Wang, and Hassan Foroosh. Lidarmultinet: [132] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie
Towardsaunifiedmulti-tasknetworkforlidarperception. Li, JianfengGao, andYongJaeLee. Segmenteverything
InAAAIConferenceonArtificialIntelligence,pages3231– everywhereallatonce. InAdvancesinNeuralInformation
3240,2023. 2,8 ProcessingSystems,2023. 2
[120] BoZhang,JiakangYuan,BotianShi,TaoChen,YikangLi,
andYuQiao. Uni3d: Aunifiedbaselineformulti-dataset
3dobjectdetection.InIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages9253–9262,2023.1,
2
[121] HaoZhang,FengLi,XueyanZou,ShilongLiu,Chunyuan
Li, Jianfeng Gao, Jianwei Yang, and Lei Zhang. A sim-
ple framework for open-vocabulary segmentation and de-
tection. arXivpreprintarXiv:2303.08131,2023. 2
[122] Wenwei Zhang, Jiangmiao Pang, Kai Chen, and
Chen Change Loy. K-net: Towards unified image seg-
mentation. InAdvancesinNeuralInformationProcessing
Systems,pages10326–10338,2021. 2
[123] YangZhang,ZixiangZhou,PhilipDavid,XiangyuYue,Ze-
rongXi,BoqingGong,andHassanForoosh. Polarnet: An
improved grid representation for online lidar point clouds
semanticsegmentation. InIEEE/CVFConferenceonCom-
puter Vision and Pattern Recognition, pages 9601–9610,
2020. 1,2,8
[124] Zihui Zhang, Bo Yang, Bing Wang, and Bo Li. Growsp:
Unsupervised semantic segmentation of 3d point clouds.
InIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages17619–17629,2023. 2
[125] YimingZhao,LinBai,andXinmingHuang. Fidnet: Lidar
pointcloudsemanticsegmentationwithfullyinterpolation
decoding. InIEEE/RSJInternationalConferenceonIntel-
ligentRobotsandSystems,pages4453–4458,2021. 2
[126] QiangZhou,YuangLiu,ChaohuiYu,JingliangLi,Zhibin
Wang, and Fan Wang. Lmseg: Language-guided multi-
dataset segmentation. In International Conference on
LearningRepresentations,2022. 2
[127] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp
Kra¨henbu¨hl, andIshanMisra. Detectingtwenty-thousand
classes using image-level supervision. In European Con-
ferenceonComputerVision,pages350–368,2022. 2
[128] Xingyi Zhou, Vladlen Koltun, and Philipp Kra¨henbu¨hl.
Simplemulti-datasetdetection. InIEEE/CVFConference
onComputerVisionandPatternRecognition,pages7571–
7580,2022. 2
33