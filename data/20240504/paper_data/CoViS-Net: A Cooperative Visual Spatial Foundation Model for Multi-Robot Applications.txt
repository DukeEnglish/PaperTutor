CoViS-Net: A Cooperative Visual Spatial
Foundation Model for Multi-Robot Applications
Jan Blumenkamp, Steven Morad, Jennifer Gielis, Amanda Prorok
University of Cambridge, Department of Computer Science and Technology
{jb2270, sm2558, jag233, asp45}@cam.ac.uk
Abstract—Spatial understanding from vision is crucial for
robotsoperatinginunstructuredenvironments.Intherealworld,
spatial understanding is often an ill-posed problem. There are
a number of powerful classical methods that accurately regress
relative pose, however, these approaches often lack the ability
to leverage data-derived priors to resolve ambiguities. In multi-
robot systems, these challenges are exacerbated by the need for
accurate and frequent position estimates of cooperating agents.
To this end, we propose CoViS-Net, a cooperative, multi-robot,
visual spatial foundation model that learns spatial priors from
data. Unlike prior work evaluated primarily on offline datasets, Fig. 1. Our model can be used to control the relative pose (yellow and
we design our model specifically for online evaluation and real- purplecircle)ofmultiplefollowerrobots(yellowandpurplecone)toaleader
worlddeploymentoncooperativerobots.Ourmodeliscompletely robot(withbluecone)followingareferencetrajectoryusingvisualcuesinthe
environmentonly.Evenifthereisnodirectvisualoverlap(coneintersection),
decentralized, platform agnostic, executable in real-time using
themodelisabletocorrectlyestimatetherelativepose.
onboard compute, and does not require existing network infras-
tructure. In this work, we focus on relative pose estimation and
local Bird’s Eye View (BEV) prediction tasks. Unlike classical
learn human priors to solve complex spatial tasks [27, 37,
approaches, we show that our model can accurately predict
relative poses without requiring camera overlap, and predict 63, 64, 6, 3, 40]. Unfortunately, such methods have yet to
BEVs of regions not visible to the ego-agent. We demonstrate be broadly integrated and demonstrated on robots in real-
our model on a multi-robot formation control task outside the world scenarios – particularly when operating under hardware
confines of the laboratory.
or computational constraints. When considering multi-robot
systems, prior assumptions on data availability and latency
I. INTRODUCTION
are broken, and otherwise promising work fails to deliver.
Spatial understanding is a cornerstone of robotic operation Furthermore, uncertainty metrics are essential to reason about
inunstructuredenvironments,relyingoneffectiveposeestima- the reliability of predictions.
tionandenvironmentalperception[48,46,8,58,49,7,28,29]. Acknowledging these limitations, our work introduces
While traditional methods leveraging GNSS, LiDAR, Time CoViS-Net, a decentralized, platform-agnostic visual spatial
of Flight (ToF), and Ultra-Wide Band (UWB) sensors have foundationmodeldesignedforreal-worldmulti-robotapplica-
been instrumental in advancing robotics, they often come tions. This model operates in real-time on onboard computers
with constraints (e.g., indoor, outdoor, sunny, IR-reflective withouttheneedforpre-existingstaticnetworkinfrastructure,
surfaces, etc.). Unlike these methods, color cameras offer a thus addressing critical scalability, deployment flexibility, and
low-cost, energy-efficient, and rich data source suitable for operational robustness challenges.
many environments, aligned with the vision-centric design We design a deep distributed stateless architecture to focus
principles of real-world, human-designed environments. on the challenges specific to multi-robot systems, including
Classical vision-based techniques such as Visual Odome- the need for relative pose estimation [39, 50, 56, 26, 67]
try [48], Visual SLAM [7], and Structure-from-Motion [49], and the prediction of BEVs of regions without local camera
struggle with the inherent ill-posed nature of spatial tasks, coverage [21, 43, 62, 22]. We demonstrate the efficacy of our
lacking the capability to integrate human-like semantic priors approach through real-world applications, showcasing its use
to resolve ambiguous situations. These challenges are exac- on a multi-robot control task outside laboratory settings.
erbated in multi-robot applications, requiring not only envi- In summary, our contributions are threefold:
ronmental spatial understanding but also rapid and accurate 1) A novel architecture for platform-agnostic, decentral-
relative pose estimates to other robots, as seen in scenarios ized,real-time,multi-robotposeestimationfrommonoc-
such as flocking [39], path planning [50], and collabora- ular images, incorporating uncertainty awareness with-
tive perception [69]. Explicit detection of other agents is a out the need for a predefined map.
viable alternative [58, 8, 46, 28, 29], but requires line-of- 2) Extension of this architecture to predict uncertainty-
sight measurements and is not platform agnostic. On the aware BEVs, enhancing spatial understanding in multi-
other hand, deep pose predictors demonstrate the ability to robot systems, especially in occluded or unobserved
4202
yaM
2
]OR.sc[
1v70110.5042:viXraareas. timation. Our approach diverges from this prior body of work
3) Real-world validation of our model’s applicability to byprovidingreliablerelativeposeswithuncertaintyestimates,
multi-robot control and BEV estimation, even for ill- without requiring visual overlap, and using unmodified pre-
posed situations where images share no overlap. trained foundation models, enabling transfer learning [60].
BEV Prediction: In autonomous vehicle applications, de-
II. RELATEDWORK
tailedenvironmentalrepresentationiscrucial.TheBEV,merg-
Multi-Robot Systems: Multi-robot systems necessitate the ing data from diverse sensors in an agent-centric frame, is
coordinationofmultiple,oftenfast-movingrobotswithinclose particularly effective. Traditional BEV generation involves
proximity in real-world settings. This coordination requires multiple steps, but deep-learning approaches have shown
not only a spatial understanding of the immediate environ- promise.PioneerslikeRoddickandCipolla[41]projectedim-
ment but also accurate and quick pose estimates of cooper- agefeaturesintoaBEVrepresentationforprediction.TheLift,
ating robots. Key applications include flocking, where agents Splat, Shoot methodology by Philion and Fidler [36] adopted
maintain proximity using rules of separation, alignment, and a novel multi-camera rig approach. BEVFormer [65], PETR
cohesion [39]; formation control, where agents maintain or [24], and BEVSegformer [34] contributed new dimensions,
adjustformationstonavigateobstacles[31];andtrajectoryde- such as spatiotemporal information and multi-task learning.
confliction to prevent agent collisions (RVO) [56]. Additional Extensions to multi-agent scenarios [62, 43, 21, 22, 59, 16]
scenarios involve multi-agent object manipulation [26, 2] and predominantly relied on GNSS for global pose knowledge.
area coverage [67, 51]. Recent advances employ Graph Neu- InspiredbyDuttaetal.[11],ourworkemploysatransformer-
ral Networks (GNNs) for enhanced multi-agent control and based BEV predictor, mapping transformer output sequences
cooperative perception, facilitating inter-robot communication directly to a BEV grid. We focus on a 6m x 6m indoor
through latent messages to address partial knowledge chal- area, unlike the larger scales typical in autonomous driving.
lenges [55, 14, 15, 5, 4, 59, 69, 21, 43, 62, 22]. A significant Beyond[11],weintegrateposeembeddingsfromourestimator
gapinthisresearchareaistheacquisitionandrelianceonpose intomulti-nodefeatureaggregation,showinghowtheinfusion
knowledge, typically relative pose, which we address in this of explicit pose information provides benefits over simple
paper. For learning-based approaches, our model can be used aggregation.
as a plug-in model to fine-tune for more specific use cases.
Map-FreeRelativePoseRegression:Map-BasedPoseRe-
gression,asin[18,45],predictsacameraimage’sposewithin
III. PROBLEMFORMULATION
a learned model of a scene. Conversely, Map-Free Relative
Given a multi-robot system, our goal is (i) for each robot
Pose Regression, exemplified by [3], estimates the relative
to predict its pose with respect to other robots based only on
pose between two camera images without needing scene-
non-line-of-sight visual correspondences in the environment,
specific training. Feature matching between images retrieves
and (ii) to use this information for downstream tasks, such
orientation and a scale-less transformation vector. Traditional
as for estimating a local occupancy grid map or for real-time
non-learning methods like SIFT [25] handle images with
multi-robot control in transfer learning [60].
significant overlap and minimal differences effectively. How-
ever, learning-based approaches for feature matching, such as Consideramulti-robotsystemrepresentedbyasetofnodes
Superglue [44], LOFTR [53], Gluestick [33], and Lightglue V. Each node v i ∈ V has a position p w,i and an orientation
[23], have gained popularity, even though they still require R w,i in the world coordinate frame F w. The set of edges
considerable visual overlap. Integrating scale is possible by E ⊆V×V, defined by the communication topology, connects
pairing these methods with depth maps or directly through nodes v i to v j and defines the graph G = (V,E). For each
Siamese network architectures that use dual CNNs for image edge (v i,v j)∈E, the relative position of v j in the coordinate
encoding and pose estimation via a fully connected layer frame of v i (denoted as F i) is p i,ij = R −w,1 i·(p w,j −p w,i),
[27, 20, 12, 1, 63, 37, 64]. These models, generally assum- and the relative rotation is R i,ij =R −w,1 i·R w,j.
ing visual overlap and using mean-squared-error loss, lack Each node v is equipped with a camera C , located
i i
uncertainty estimates, limiting their applicability to robotics. and oriented identically to the robot. The camera output at
Robotics applications of similar methods [63, 37, 64] have node v is represented as an image I . From the images,
i i
not been tested in real deployments. To the best of our we must estimate relative poses and associated uncertainties
knowledge, Winkelbauer et al. [61] is the only method to (pˆ Rˆ , σ2)∀v ∈N(v ) with respect to v in its co-
i,ij,, i,ij ij j i i
employ uncertainty estimation in robot localization through ordinate frame F . Notably, we do not assume that there is
i
MonteCarlodropout,whichisanepistemicuncertainty.Inthis overlapbetweenimages,norline-of-sightpositioningbetween
work,weintroduceamethodtopredictaleatoricuncertainties. robots. Relying exclusively on common image features makes
Extending beyond field-of-view pose prediction, [6] and [3] this approach platform-agnostic and only requires a single
proposed novel rotation and position estimation methods in monocular camera. Utilizing these estimated poses, the robots
challenging scenarios. Learning-based limitations are high- aretaskedtocooperativelygenerateaBEVBEˆV aroundeach
i
lightedby[68],advocatingforessentialmatrixestimation,and v . The pose estimates and BEV can be used as input for a
i
[40], using vision transformers for direct essential matrix es- wide range of downstream multi-robot applications.v0 v1 v2 v3 v4
I I I
k i j 2
f f f
0 enc enc enc
−2 E k pˆ ,Rˆ ,σ2 pˆ ,Rˆ ,σ2 E j
i,ik i,ik ik i,ij i,ij ij
2
E
0 f i f
pose pose
2
− E E
ik ij
2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5
− − − − −
|| 0 || ||
Fig.2. WevisualizeasamplefromthesimulationvalidationdatasetDSim F F F
Val
for five nodes, corresponding to the five columns. Each node is associated F F F
withthecoloredborderinthefirstrow.Thetoprowshowsthecameraimage ik ii ij
I i fromtheperspectiveofeachnode.Themiddlerowshowsthefiveground
truthlabelsforeachrespectivenodeinthecoordinatesystemF i.Eachrobot f
agg
F
i
is centered in its coordinate frame, facing upwards at position (0, 0). The
groundtruthposesp i,ij oftheotherrobotsareindicated,togetherwiththeir
field of view. The background shows the BEV map BEV i for each robot. Fig.3. Overviewofthemodelarchitecture:Weillustratethedecentralized
Thebottomrowshowsthecorrespondingpredictionsforeachrobot.Weonly evaluation with respect to v i, which receives embeddings E i,E j generated
showposepredictionspˆ i,ij,Rˆ i,ij withasufficientlylowuncertaintyσ i2ij. by the encoder fenc from the neighbors v j,v k ∈N(v i). For each received
Inthebackground,thepredictedcorrespondingBEVBEˆV i isshown. embedding, we employ fpose to compute the pose. This pose can then be
utilizedforposecontroloranyotherrelevanttask.Weconcatenatethepose
embedding E ij with the image embedding E i and subsequently aggregate
thisintoanodefeatureF i.
IV. MODELTRAINING
Inthissection,weexplainthewholetrainingpipelineofour
The ground truth BEV BEV is a cropped 6 m × 6 m
i
foundationmodel.Wegenerateasimulationdatasetanddefine
viewofthefloor’sBEVaroundv ,generatedthroughHabitat’s
i
our model architecture before we introduce our uncertainty
navigation mesh. Each agent v is centered and facing north.
i
estimation strategy and training protocol.
B. Model
A. Dataset
We employ three models: f , f , and f . The encoder
enc pose agg
We utilize the Habitat simulator [47, 54] along with the f generatesacommonembeddingofimages,f performs
enc pose
training set of the HM3D dataset [38], which comprises 800 apairwiseestimateofposes,andf spatiallyaggregatesnode
agg
scenes of 3D-scanned real-world multi-floor buildings. We features based on the output from f . An overview of the
pose
selectHabitatandHM3Dfortheirphotorealismandtheavail- model architecture is provided in Fig. 3.
ability of ground-truth pose information, as well as detailed Transformers: Our methodology utilizes the Transformer
navigation mesh information that can be converted to a BEV architecture [57], distinguished by its self-attention mecha-
of the scene. We instantiate a single agent equipped with a nism. This mechanism computes the output as a weighted
calibratedcamerawithafieldofviewof120 .Foreachscene, sum of the values (V), with the weights determined by the
◦
we randomly sample several uniform agent positions based compatibility of queries (Q) and keys (K) as per the formula:
√
on the total area of the scene. For each sample, the camera Attention(Q,K,V)=softmax((QKT)/ d )V, where d is
k k
orientation is randomized across all three axes. Additionally, thedimensionalityofthekeys.Typically,Q,K,andVareset
for each scene, we extract the processed navigation mesh, tothesameinputfeatures.Therefore,werefertooneattention
which indicates areas that are navigable by a mobile robot, block as f and to an instance of a multi-layer transformer,
attn
or not navigable due to obstacles or overhangs. In total, we including both the attention mechanism and a position-wise
extract 3,816,288 images from 1,411 floors in 800 scenes. feed-forward network, as finst.
trans
WedividethisdataintotrainingDSim (80%),testingDSim TheVisionTransformer(ViT)[10]extendsthisarchitecture
Train Test
(19%), and validation DSim (1%) sets. From these sets, we to images by dividing an image into patches treated as a
Val
sample pairs of N =5 images and poses by (1) uniformly sequence.
max
sampling 25% of individual images from each dataset, and Image encoder: We use DinoV2 [32] as a pre-trained
then(2)uniformlysamplingN −1imageswithinadistance encoder. DinoV2 is a vision foundation model based on ViT
max
of d = 2 m around each sample from (1). This process andpre-trainedinaself-supervisedmannerontheLVD-142M
max
yields763,257trainingand276,680testtuples,eachofwhich dataset, which is a cumulation of various filtered datasets,
are at most 2d apart and pointing in random directions, including various versions of ImageNet [9] and Google Land-
max
thus providing realistically close and distant data samples for marks.WespecificallychoseDinoV2duetoitspre-trainingon
estimatingbothposesanduncertaintiesincaseofnooverlaps. a wide variety of images and demonstrated powerful feature
iI
hturTdnuorG
noitciderPrepresentationaswellasdemonstrateddepthunderstandingof and then extract the first element of the sequence dimension
scenes. Weuse the smallestdistilled version ofDinoV2 based (Eq. 3), and eventually use this edge embedding to estimate
onViT-S/14with22.1Mparametersforperformancereasons. poses and orientations (Eq. 4) and uncertainties (Eq. 5 and
We refer to this model as f . Eq. 6).
dino
For explicit communication, agents utilize a one-hop com- Multi-node aggregation: Lastly, we perform the position-
munication network, detailed further in Sec. V-C. Each node aware feature aggregation, utilizing the node embeddings E
j
v processestheimageI throughtheencoderfunctionf to generated in f for each corresponding edge. The feature
i i enc pose
generateanodeembeddingE ∈RS F,whichconstitutesthe aggregation function f integrates information from the set
i × agg
soleinformationcommunicatedtoandutilizedbyneighboring of nodes v ∈N(v )∪{v }. The resulting output is a feature
j i i
nodes. Here, S represents the sequence length (which can be vector F for each node v , encapsulating the aggregated
i i
reducedtoadjustmodelperformance,asexplainedinSec.V), information from its local vicinity, which can be used for any
andF denotesthesizeofeachfeaturevectorinthesequence. downstream task in a transfer-learning context [60], requiring
The weights in f are frozen, and an instance of fenc , geometric information in a robotic swarm, for example, to
dino trans
which is trained, is concatenated, resulting in f (Eq. 2). predict a BEV of the environment around the robot that
enc
captures areas that might not necessarily be visible by the
robot itself.
f =f ·fenc (1)
enc dino trans We introduce an instance of two more transformers fagg
trans
f enc(I i)=E i (2) and fpost, an MLP f , a learnable transformer positional
trans aggpos
Freezing the weights of f
dino
facilitates the ability to use embeddingS agg,aswellasaBEVdecoderf BEV.Weperform
therawoutputusedindependentlyofourproposedfoundation the node aggregation as
model, meaning that a wide variety of open-source fine-tuned
imagedetectorscanbeleveragedwithoutrunninganadditional
X =((E || E )+S ) (7)
ij i S j agg
forward pass.
F =(X || f (E )) (8)
Pairwise pose encoder: Our goal is to develop a pose ij ij F aggpos ij
predictionfunction,f pose.Ateachrootnodev i,wegeneratean F ii =(X ii|| F f aggpos(0)) (9)
imageencodingE usingf .ThisencodingE istransmitted  
i enc i
(cid:88)
to neighboring nodes. Given the root and neighbor encodings F i =f tp rao nst s f ta rg ang sr(F ij) (10)
E and E , f estimates the pose of each neighbor relative
toi
the
rooj
t
nopo ds ee
. Specifically, for a given root node v and
vj∈N(vi) ∪{vi}
S1
i BEˆV =f (F ). (11)
each neighbor v , f calculates the set of estimated relative i BEV i
j pose
poses and uncertainties (pˆ :Rˆ ,:σ2):∀:v ∈N(v ), We concatenate the node features along the sequence di-
i,ij,, i,ij ij j i
with respect to v in the coordinate frame F , as well as edge mension and add the positional embedding (Eq. 7). We then
i i
embeddings E :∀:v ∈N(v ). 1 concatenate a nonlinear transformation of the edge features
ij j i
We define the concatenation operation of two tensors A along the feature dimension to the output of the previous
and B along axis C as (A|| B) and the indexing operation operation (Eq. 8). The previous operations are performed for
C
on tensor A by the first element along axis B as A . We all v ∈N(v ), but it is essential to include self-loops to the
B1 j i
introduce an instance of another transformer fpose, and four graph topology, as a full forward pass of the neural network
trans
MLPs fµ, fσ, fµ, fσ. Furthermore, we introduce a learnable would otherwise not be possible if N(v ) were empty. We
p p R R i
transformer positional embedding for the sequence dimension therefore set the edge embedding to the zero tensor 0 in that
S .ForeachincomingE ∀v ∈N(v ),weperformapose case (Eq. 8). We then aggregate over the set of neighbors and
pose j j i
estimation as itself to produce the output embedding (Eq. 10) and finally
use this to estimate any other downstream task, for example,
E =fpose((E || E )+S ) (3) estimating a BEV (Eq. 11).
ij trans i S j pose S1
pˆ , Rˆ =fµ(E ), fµ(E ) (4) C. Training
i,ij i,ij p ij R ij
σ2 , σ2 =fσ(E ), fσ(E ) (5) For training our models, we resort to supervised learning.
p,ij R,ij p ij R ij
This section describes the losses as well as our approach to
σ2 =(σ2 , σ2 ) (6)
ij p,ij R,ij estimating uncertainty.
f pose(E i,E j)=(pˆ i,ij,Rˆ i,ij,σ i2 j). Uncertainty estimation: In general, two kinds of uncer-
tainties can be modeled. While epistemic uncertainty captures
We concatenate the node features along the sequence di-
uncertaintyinthemodel,i.e.uncertaintythatcanbeimproved
mension, add the positional embedding, run the transformer
by training the model on more data, aleatoric uncertainty
1Althoughimageoverlapisnotastrictassumption,ouruncertaintymeasure captures noise inherent to a specific observation [17]. We
σ i2 j enablesustoidentifyimagepairsthatareunsuitableforjointestimation argue that for estimating poses and applications to pose
duetolackoffeaturecorrespondence,thusallowingtheapplicationtoreact
control in a robotic system, aleatoric uncertainty is much
accordingly,e.g.,prioritizerotationalalignmentoverpositionalalignmentto
regaincertainty. moreimportantthanepistemicuncertaintysinceadownstreamapplication can act on the predicted aleatoric uncertainty to
decrease it. Approaches such as Monte Carlo Dropout [13]  
or Model Ensembles [19] have previously used in similar (cid:88) (cid:88)
L= L i,BEV+ L i,ij,Pose. (18)
applications [61], but these approaches model the epistemic
uncertainty. Instead, we propose to use the Gaussian Negative
vi∈V vj∈N(vi)
Log Likelihood Loss, as introduced by Nix and Weigend [30] We find that the uncertainty terms in the loss are necessary
and revisited by Kendall and Gal [17]. As a generalization of fortraining,asmanysampleswithinthedatasetdonotcontain
the mean squared error (MSE) loss, it allows the model to overlaps, and are sometimes in two different (yet adjacent)
not only predict a mean µˆ, but also a variance σˆ2, which is rooms.Otherwise,theerrorfromthesedifficultsampleswould
learned from data points µ. The loss is defined as dwarf the error from other samples, hampering learning.
We use Pytorch Lightning to train our model for 15 epochs
ontwoNVidiaA100GPUs.Thistrainingtakesapproximately
(cid:32) (cid:33)
L (µ,µˆ,σˆ2)= 1 log(cid:0) σˆ2(cid:1) + (µˆ−µ)2 , (12) 24 hours. We optimize the weights using the AdamW op-
GNLL 2 σˆ2 timizer and use the 1cycle learning rate annealing schedule
throughout the training and configured weight decay. We
where intuitively, the left component penalizes predictions choose the best model based on the performance on the
of high variance, while the right component scales the MSE validation set. We provide further details about the training
loss by the predicted variance to penalize errors more when procedure in the supplementary materials.
the predicted uncertainty is low. After training the model on DSim , we freeze the weights
Train
Estimating Rotations: Estimating rotation is inherently and perform no further sim-to-real finetuning, thus zero-shot
difficult due to discontinuities that occur in many orientation transferring from simulation to real-world evaluation.
representations.Inmachinelearning,quaternionsaretypically
chosen to represent rotations due to their numerical efficiency V. EXPERIMENTSANDRESULTS
and lack of singularities. Peretroukhin et al. [35] propose to We demonstrate the performance of our trained models on
represent rotations through a symmetric matrix that defines a simulated testset DSim and new real-world test sets DReal,
Test Test
a Bingham distribution over unit quaternions, which allows introducedinthissection.First,weoutlinethemetricsweuse.
them to predict epistemic uncertainty. To obtain an aleatoric Then,wediscussoursimulationexperimentsandresults.Next,
uncertainty for the rotation, we combine Eq. 14 with Eq. 12 we collect real-world images using our multirobot system,
andletthemodelpredictonevariancefortherotationestimate, and analyze the model performance. Finally, we compile and
as shown in Eq. 15. deploy our model on a real-time trajectory tracking task, and
report our findings.
Metrics: We compute the absolute Euclidean distance be-
d (qˆ,q)=min(∥q−qˆ∥ ,∥q+qˆ∥ ) (13)
quat 2 2 tween the predicted and ground truth poses and the geodesic
L2 chord(qˆ,q)=2d2 quat(qˆ,q)(cid:0) 4−d2 quat(qˆ,q)(cid:1) (14) distance between rotations as
LGNLL(cid:0) q,qˆ,σˆ2(cid:1)
=
1(cid:18)
log(cid:0) σˆ2(cid:1)
+
L2 chord(qˆ,q)(cid:19)
(15) D (p ,pˆ )=∥p −pˆ ∥
chord 2 σˆ2 pos i,ij i,ij i,ij i,ij
(cid:18) 1 (cid:16) (cid:17)(cid:19)
D (R ,Rˆ )=4·arcsin d Rˆ ,R
BEV prediction: For the BEV prediction, we use a combi- rot i,ij i,ij 2 quat i,ij i,ij
nation of the Dice loss [52] L and Binary Cross Entropy
Dice for all edges and report median errors.
(BCE)LossL toachievesharpandclearpredictions.The
BEC For simulation results, we also report the Dice as well as
BEV loss is defined as
the Intersection over Union (IoU) metric.
(cid:16) (cid:17) A. Simulation
L =α·L BEV ,BEˆV
i,BEV Dice i i
Weconductedaseriesofexperimentsacrossvariousmodels
(cid:16) (cid:17)
+(1−α)·L BEV ,BEˆV , (16) trained with different parameters to showcase our model’s
BCE i i
efficiency. The quantitative results are presented in Tab. I and
where 0 ≤ α ≤ 1 balances between the Dice and BCE Tab. II, with qualitative findings in Fig. 2.
objectives. Tab. I details the performance across seven different exper-
Training: We define the pose loss as iments on the simulation test set DSim, as outlined in section
Test
Sec. IV-A. We evaluated the impact of image embedding
sizes E on communication efficiency, reporting sizes based
L =L (p ,pˆ ,σ2 ) i
i,ij,Pose GNLL i,ij i,ij p,ij on 16-bit floats. The Intersection over Union (IoU) and Dice
(cid:16) (cid:17)
+βLGNLL R ,Rˆ ,σ2 , (17) metrics for the BEV representation of the local environment,
chord i,ij i,ij R,ij
alongsidemedianposeestimatesforallsamplesinthedataset,
where 0 ≤ β ≤ 1 balances between the position and are provided. It’s important to note the limitation in reporting
orientation loss, and, lastly, the total loss over the graph as estimation feasibility between nodes due to Habitat’s lack ofTABLEI
experiment, we use the model configuration with F = 48
ABLATIONSTUDYOVERTHENUMBEROFPATCHESANDSIZEOF
FEATURESFOREACHPATCH.WEREPORTTHEBEVPERFORMANCEAS and S = 128. (3) A BEV model trained with ground truth
WELLAS ST IH ZE EM OFED THIA EN EE MR BR EO DR DF INO GR EPO iS IE SS RO EN POT RH TE ED DA IT NA BS YET TED ST .Si em st.THE p mo os de ei ln ’sfo prm era foti ro mn af nr co em isth ce os nim sidu ela reti don antra ui pn pin eg
r
s lie mtD it,TSi arm a si sn u. mT ih ni gs
perfect pose estimation. Evaluation on the simulation test set
Model BEV MedianPoseErr.
DSim indicatesourmodelperformsbetweenthetwobaselines.
S F Mem Dice IoU Test
Itsignificantlyoutperformsthepose-freemodelwithan8.75%
256 48 24kB 69.1 57.1 36cm 8.3°
128 96 24kB 68.8 56.8 40cm 8.4° improvement in accuracy, while the model using ground truth
128 48 12kB 67.9 56.1 38cm 7.7° posesshowsan18.31%performanceincreasecomparedtothe
128 24 6kB 66.7 54.6 50cm 9.5°
baseline without pose information. These results highlight the
64 48 6kB 61.0 43.5 51cm 9.6°
1 3072 6kB 47.0 1.4 144cm 89.9° substantial benefit of incorporating pose predictions into BEV
1 348 696B 47.1 1.4 84cm 11.7° estimation, positioning our approach as a notably effective
middle ground between the two extremes.
TABLEII
ABLATIONOVERDIFFERENTMODESFORTHEBEVPREDICTIONONTHE B. Real-World
SIMULATIONTESTSETDSim.
Test
Inthissection,wedetailourrobotsetup,customreal-world
test set, and model evaluation on this dataset. We run a series
Experiment Dice IoU MedianPoseErr.
of experiments with quantitative results in Tab. III, Tab. VI,
None 0.628 0.495 N/A
Tab. IV, Tab. V, and Fig. 5 and qualitative findings in Fig. 4,
Predicted 0.683 0.561 31cm,5.0
◦
Groundtruth 0.743 0.632 0cm,0.0 Fig. 7, and Fig. 6.
◦
Robot Platform: Our setup includes four DJI RoboMaster
S1 robots, each equipped with an NVidia Jetson Orin NX
specific information (e.g., samples facing the same direction 16GB and a forward-facing calibrated Raspberry Pi HQ cam-
might be in adjacent rooms, unseen by the dataset). era. The camera’s raw field of view is approximately 170 ,
◦
Training one model with F = 48 and S = 128 across rectified to 120 using the OpenCV Fisheye module, with an
◦
N = 5 different seeds revealed a very low average standard image resolution of 224×224 px.
deviation of 0.011374, leading us to report results for only Real-WorldTestsetCollection:Ourcustomreal-worldtest
N =1 seeds in the table for clarity. set DReal was collected in eight office-like indoor scenes,
Test
The experiments demonstrate that the sequence length S, tailored to our robot platform’s requirements. Unlike existing
acting as the context size for each image, significantly in- datasets like Seven Scenes, which have a constrained FOV
fluences BEV prediction and pose estimate performance more of 53.8 , our dataset is designed for our model’s application,
◦
thanfeaturesizeF.WesetS to1fortwoexperiments:(1)we reflecting dynamic environmental changes through multiple
increase F to match the size of the embeddings for the other agents. It represents one of the first multi-agent datasets
experiments,and(2)wedecreaseF,whicheffectivelyreduces collected with locally connected devices.
the transformer to a CNN-only architecture by removing the Data collection involved a base station with dual cameras
attention mechanism. This resulted in a marked decrease in and a 2D Lidar, plus a distinctively shaped marker on each
performanceforbothBEVandposeestimations,underscoring robot. We manually navigated robots around the base station
thecriticalroleofattentioninourmodel.Thebest-performing tocaptureavarietyofrelativeposes,thenusedacustomtoolto
model with S = 256 achieved an BEV IoU of 0.571, a Dice label Lidar data for pose extraction. In total, we collect 5692
score of 0.691, and median pose accuracy of 36 cm and 8.3 . labeled samples. To balance out the distribution of relative
◦
Conversely,modelswithS =1showedsignificantdegradation positions in the dataset, we sample image pairs to ensure a
tonearunusableBEVpredictionsandposeerrorsonthescale uniform distribution of relative positions in the range of 0 m
ofmeters.ModelswithF =24andS =64exhibitedslightly to 2 m. We sample a similar number of pairs from all scenes,
inferior performance compared to those with larger F or S. resulting in a total of 14008 sets of samples each with three
Lastly, we assess the effectiveness of BEV predictions, images, resulting in a total of 14008 · 2 · 3 = 84048 pose
focusing on the contribution of pose predictions to enhanc- edges.32%ofthesegroundtruthedgeshavenovisualoverlap.
ing BEV accuracy, in an ablation study detailed in Tab. II. We show a sample of each scene from the dataset in Fig. 4.
We conduct three experiments: (1) A model trained without Our dataset covers a diverse and challenging range of indoor
incorporating any pose prediction into the BEV aggregator. scenes,aswellasoneoutdoorscene.Weincludemoredetails
This model can estimate local information and, to a degree, about the testset in the supplementary material.
information aggregated through neighbors. However, it must Metrics: In Tab. III, Tab. VI, Tab. IV and Tab. V, we
implicitly learn to predict the relative poses between obser- categorize the median pose error into the categories Invisible,
vations to successfully merge the BEV representations from Visible and Invisible Filtered. Visible and Invisible edges are
multiple agents. (2) A model trained as described in this determined through the FOV overlap (an edge is considered
work, integrating pose estimates with BEV predictions and invisibleiftheangleD (R ,0)>FOV).InvisibleFiltered
rot i,ij
aggregating this information from neighbor nodes. For this samples use the predicted position uncertainty fσij of our
p(a)CorridorA (b)CorridorB (c)OfficeA (d)OfficeB (e)StudyA (f)StudyB (g)Outdoor (h)Sunny
Fig.4. Wecollectthereal-worlddatasetDReal fromfiveuniquescenesinanindoorofficebuilding.Thescenesshowdifferentchallenges,fromcluttered
Test
environmentsoversceneswithhighceilingstosunnyfloors.
TABLEIII
ABLATIONSTUDYOVERTHENUMBEROFPATCHESANDSIZEOF
FEATURESFOREACHPATCH.WEREPORTTHEBEVPERFORMANCEAS
180
WELLASTHEMEDIANERRORFORPOSESONTHEDATASETD TR ee sa tl.THE
135
SIZEOFTHEEMBEDDINGE iISREPORTEDINBYTES.
90
45
S F Mem InvisibleFiltered Invisible Visible 0 0.00.51.01.52.0 0.0 0.5 1.0 1.5 2.0 0 22.5 45 67.5 90 0 22.5 45 67.5 90
Pos.Err Pos.Var Rot.Err Rot.Var(deg)
256 48 24kB 61cm 6.8° 97cm 7.9° 33cm 5.8°
128 96 24kB 55cm 7.7° 97cm 7.4° 32cm 5.6° Fig. 5. We show qualitative distributions for position and rotation errors
128 48 12kB 67cm 9.9° 113cm 9.6° 29cm 5.7° andvariancesovertherelativeanglebetweentworobotsonalledgesofthe
128 24 6kB 83cm 11.2° 112cm 9.7° 31cm 5.7° real-worldtestsetDReal,containing84048uniqueedges.Twoagentsfacing
Test
64 48 6kB 81cm 9.4° 119cm 10.8° 36cm 6.3° thesamedirectionwouldhaveanangledifferenceof0 ,whereastwoagents
◦
facing the opposite direction would have an angle difference of 180 . We
1 3072 6kB 123cm 25.7° 122cm 25.8° 93cm 138.2° ◦
showtheFOVasdashedline,whereanyresultsbelowthelineindicatethat
1 348 696B 150cm 164.1° 135cm 37.9° 80cm 11.1°
theedgehassomeimageoverlapandabovenoimageoverlap.Thehorizontal
marginals show the distribution of each individual plot while the vertical
marginalshowsthedistributionofangledifferencesoverthetestset.Notethat
theangledistributionisnotuniformduetothereal-worlddatasetcollection
model to reject pose estimations with high uncertainty based
whereatleasttwocamerasalwaysfacetheoppositedirection.
on a threshold computed with the Youden’s index [66].
Results:Evaluationonthereal-worlddatasetDReal focuses
Test
onpositionandrotationaccuracy,excludingquantitativeBEV without (above the line). The analysis reveals that position
performance due to unavailable ground truth data. errors remain consistent up to the FOV threshold, beyond
We first report results similar to Tab. I on DReal in Tab. III. which they escalate, as reflected in the predicted position
Test
TheresultsareinlinewiththeresultsreportedforDSim,with variance.Conversely,rotationerrorsstayuniformacrossangle
Test
a distinct correlation between the number of patches S and differences, with only a marginal increase for higher angles,
features F and the performance of pose estimation. Notably, though the predicted rotation variance notably rises.
the performance in estimating rotations does not significantly These findings demonstrate our model’s capability to re-
vary between visible and invisible samples, demonstrating liably estimate poses, even with minimal overlap, and to
our model’s capability to robustly estimate poses without accurately predict higher variances under such conditions. We
necessitating FOV overlap. The model exhibiting the highest attributethisperformancetotherobustrepresentationalability
accuracy achieves a median localization error of 97 cm and oftheDinoV2encoderandvisiontransformermodelsatlarge.
7.9 for invisible samples, and 33 cm and 5.8 for visible The model appears to go beyond mere feature matching,
◦ ◦
samples. Note that the samples in the dataset DReal are at instead achieving a sophisticated scene understanding that
Test
most 2 m apart, but the pose prediction is three-dimensional, allowsitto“imagine”unseensceneportionsbasedonlearned
resulting in a worst-case error of up to 4 m. data priors, thereby facilitating pose estimation.
In all further evaluations, we detail the outcomes for the In Tab. IV, we investigate the performance of our model
model configuration with S = 128 and F = 48. This model across the different scenes in the testing dataset DReal. We
Test
wasselectedforitsbalancedtrade-offbetweenBEVandpose provide a detailed breakdown of the median pose error for
estimation accuracy and manageable embedding size. Invisible, Invisible Filtered and Visible cases for each of the
We investigate the efficiency of the uncertainty estimation eight scenes. While the pose error for visible samples is
as well as the distributions of pose errors more closely. Fig. 5 consistently between 22 cm/5.2°and 48 cm/7.5°, there are
illustrates the distributions of position and rotation errors, more outliers for invisible samples. This is due to some
along with the predicted variances over the angle difference scenes having more or less symmetry, thus making it more
between robot pairs in the dataset DSim. An angle difference challengingforthemodeltoaccuratelypredictposes.Thepose
Test
of0 signifiesrobotsfacingthesamedirection,potentiallyre- predictionfortheoutdoorsceneareleastaccurate,whichisto
◦
sulting in significant image overlap, while an angle difference beexpectedsincethemodelwastrainedonindoorscenes.The
of 180 indicates opposite directions, with no overlap. The Sunnyscenehassevereshadowsanddirectsunlight,whilestill
◦
Field Of View (FOV) overlap threshold is highlighted in red, providing low pose errors. The pose predictions are useful in
delineating errors with overlap (below the line) from those all scenarios, demonstrating impressive generalization ability
)ged(.ffidelgnATABLEIV v v v
0 1 2
THEMEDIANPOSEESTIMATIONERROROFOURMODELONEACHOFTHE
EIGHTDRealSCENES.
Test
Scenario Invisible InvisibleFiltered Visible
CorridorA 138cm 129.6° 90cm 11.4° 22cm 5.2°
CorridorB 112cm 5.9° 261cm 8.2° 31cm 5.2°
OfficeA 86cm 14.4° 38cm 9.6° 33cm 7.5°
OfficeB 146cm 10.4° 248cm 4.6° 20cm 4.7°
Outdoor 134cm 9.5° 193cm 28.7° 49cm 6.9°
StudyA 132cm 6.7° 40cm 5.1° 28cm 5.3°
2
StudyB 115cm 8.9° 56cm 9.6° 34cm 5.5°
Sunny 77cm 9.3° 49cm 8.7° 22cm 6.0°
0
TABLEV
WESHOWMEDIANPOSEPREDICTIONERRORSFORDIFFERENTDISTANCE 2
THRESHOLDSdONTHEDATASETDReal. −
Test
d Invisible InvisibleFiltered Visible
2
<0.4m 76cm 7.6° 34cm 6.7° 12cm 4.9°
<0.8m 80cm 8.8° 48cm 8.6° 21cm 5.2°
0
<1.2m 95cm 9.5° 56cm 8.5° 31cm 5.8°
<1.6m 123cm 10.6° 97cm 12.9° 45cm 6.0°
<2.0m 153cm 10.5° 134cm 16.2° 65cm 6.8° 2
−
2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5
− − −
across a wide range of scenes.
Fig.6. Byleveragingpriors,suchasthewayhumanslayouttheirrooms,
Next, we evaluate the magnitude of pose prediction errors ourmodelcanstillprovideroughrelativepositionevenwithoutoverlapping
fieldsofview.Thisisgenerallyimpossibletodousingclassicalapproaches.
over different thresholds of distances between image pairs in
We visualize a sample from our custom real-world testset DReal for three
Tab. V. We consider five bands of distances separated in 0.4 nodes,similartothesimulationsampleinFig.2.WedonotshoT wes tt heground-
m chunks, ranging up to 2m. Note that the worst-case pose truthBEVsegmentationsincewedonothavelabelsforthis.Additionallyto
error is 2·d due to the three-dimensional pose predictions. theposepredictionspˆ i,ij,Rˆ i,ij,wealsovisualizethepredicteduncertainties
σ2ij.
While the error increases with an increasing distance between i
samples, the rotational error in particular stays low, and the TABLEVI
position errors stay useful, ranging from 12 cm to 65 cm for WEREPORTTHEAUCMETRICAT20,45AND90°ONAFIRST-PRINCIPLES
visiblesamples,34cmto134cmforinvisiblefilteredsamples
OPENCVANDANNN-BASEDBASELINE.WEREPORTTHEAUCMETRIC
AT20,45AND90DEGREE.
(samplesthatarenotvisiblebutforwhichhemodelpredictsa
highconfidence)and76cmto153cmforallinvisiblesamples.
Invisible Visible
We provide a qualitative sample in Fig. 6, where addition- AUC@ 20 45 90 20 45 90
ally to the simulation sample in Fig. 2, we also visualize
ORB/OpenCV[42] 0.16 1.60 7.43 9.41 21.92 38.03
the prediction uncertainty. More samples can be found in the LightGlue[23] 0.02 0.09 0.31 23.94 39.82 55.14
supplementary material. Ours 9.22 24.40 45.74 32.74 58.53 76.01
Finally, we conducted a baseline comparison with two
feature-matching-based approaches, as summarized in Table
VI. Both baseline methods extract and match features across such overlap, highlighting its robustness and applicability in a
two images to estimate the essential matrix, from which a broader range of scenarios.
rotation and a scale-less translation vector are derived. The
C. Real-World Pose Control
first baseline employs OpenCV for ORB feature extraction
followed by a brute-force matching technique. The second In this section, we describe our real-world (online) multi-
baseline, LightGlue, utilizes a neural network-based approach robot control experiment setup, including model deployment,
for feature extraction and matching. Given that both baselines wireless communication strategies, controllers, and results
estimatescale-lesstransformations,weadoptedthesamemet- reported in Fig. 7, Fig. 8, Tab. VIII and Tab. VII.
ric as used in LightGlue [23]. This metric computes the AUC Model deployment: We deploy a GNN-based model, se-
atthresholdsof20,45,and90degreesbasedonthemaximum lecting F = 24 and S = 128 for its balance between
error in rotation and translation. Our findings indicate that embeddingsizeandperformance,asreportedinTab.III.After
our model surpasses both baselines in terms of performance trainingcentralizedinPyTorch,wecompilef ,f ,andf
enc pose agg
across image pairs with and without visual overlap. It is for decentralized deployment. Using TensorRT, a proprietary
important to note that while both baselines necessitate visual NVidiatoolforahead-of-timecompilationofneuralnetworks,
overlap to function effectively, our approach does not require we achieve sub-50 ms processing times. We benchmark the
iI
hturT
dnuorG
noitciderP0s 1s 2s 3s 4s 5s
Fig.7. Weshowsnapshotsofreal-worlddeploymentsindifferentsceneswithuptofourrobots.Eachrowcontainssixframesfromavideorecording,each
spaced1sintime.Weindicatethepositionsoftherobotsinthefirstframe,wheretheleaderiscircledyellowandthefollowersblue.Thefirstthreesamples
showindoorscenes,andthebottomsampleisanoutdoordeployment.
TABLEVII TABLEVIII
WEREPORTTHEMEANANDABSOLUTETRACKINGERRORFORBOTH MODELRUNTIMEEVALUATIONFORS=128ANDF =24.
LEADERSFORALLTHREETRAJECTORIES,ASWELLASAVERAGE
VELOCITIES.THEER RR EO PR OS RA TR EE DC INON TS AI BS .T IE IN I.TWITHTHERESULTS Type Mode Dev. fenc fpose
FP16 TRT GPU 19.22ms ±0.92ms 4.15ms ±0.49ms
FP16 JIT GPU 36.79ms ±1.33ms 8.04ms ±0.73ms
Trajectory Robot MeanAbs. Median Vel
FP32 JIT GPU 99.32ms ±1.56ms 13.57ms ±1.10ms
Fig.8dyn. A 38cm,5.6 38cm,4.8 0.59m/s FP32 JIT CPU 952.04ms ±116.72ms 248.41ms ±45.98ms
◦ ◦
B 28cm,5.1 25cm,4.7 0.58m/s
◦ ◦
Fig.8stat. A 51cm,5.2 48cm,5.3 0.60m/s
◦ ◦
B 28cm,5.6 26cm,5.4 0.61m/s
◦ ◦ cast communications topology, which theoretically can permit
Rect.dyn. A 41cm,4.4 42cm,4.4 0.32m/s
◦ ◦ operation at scale. Broadcast messaging has a significant
B 29cm,5.1 27cm,5.1 0.81m/s
◦ ◦
drawback, however, in that modern wireless data networking
standards (IEEE 802.11/WiFi) fallback to very low bit rates
to ensure maximum reception probability.
runtime performance of the model on the Jetson Orin NX 802.11 uses low bit rates for broadcast due to a lack of
in Tab. VIII by averaging 100 sequential forward passes. We acknowledgment messages, which are the typical feedback
compare16-bitand32-bitfloatevaluatedontheGPUorCPU mechanism for data rate control and retransmissions. The
andcompilationwithTensorRT.Themodelf enc,generatingthe probability of a frame being lost due to the underlying
communicatedembeddingE i,hasatotalof30Mparameters, CSMA/CA frame scheduler is non-trivial (3%+); a condition
and takes ca. 20 ms for one forward pass as FP16 optimized which worsens with increasing participating network nodes.
with Torch TensorRT on the GPU. This is 50× faster than A higher level system, such as ROS2 communications mid-
running the same model on the CPU. We also note that dleware package CycloneDDS, can manage retransmissions,
changing from 32-bit floats to 16-bit floats results in a 2.75× however currently available systems are not well suited to the
speedup. The model f pose, processing a pair of embeddings requirements of a decentralized robotic network. In particular,
into a relative pose, has a total of 6 M parameters and takes they cannot detect network overload conditions, which can
ca. 4 ms for one forward pass. The model is run in a custom result in retransmissions that further load the network; finally
C++environmentintegratedwithROS2Humbleandanad-hoc resultinginarapidincreaseinpacketlossrateswhichrenders
WiFi network for inter-node communication to communicate the network as a whole unusable. While it is possible to tune
image embeddings E i, achieving a 15 Hz processing rate for such communications packages manually, the best parameter
the whole pipeline, including image pre-processing, model values are sensitive to highly variable deployment configu-
evaluation, communication and pose prediction. ration parameters including ambient wireless network traffic.
Ad-hoc WiFi: Image embeddings are sent between nodes Manualtuningofnetworkhardware,suchasforcingincreased
at a rate of 15hz, with each being just over 6KiB. The fully broadcast bit rates, can alleviate this problem but is generally
distributed nature of our system matches well with a broad- not as resilient at scale.Figure 8 static (1) Figure 8 dynamic (2) Rectangle dynamic (3)
3 3 3
2 2 2
1 1 1
0 0 0
1 1 1
− Follower A − −
Follower B
2 2 2
− Leader − −
3 3 3
− − −
2 0 2 2 0 2 2 0 2
− − −
x [m] x [m] x [m]
1.50 1.50 1.50
80 FollowerA 80 FollowerA 80 FollowerA
1.25 1.25 1.25
FollowerB FollowerB FollowerB
1.00 60 1.00 60 1.00 60
0.75 0.75 0.75
40 40 40
0.50 0.50 0.50
20 20 20
0.25 0.25 0.25
0.00 0 0.00 0 0.00 0
0 20 40 0 20 40 0 20 40 0 20 40 0 20 40 0 20 40
0.6 5 0.6 5 0.6 5
0.5 4 0.5 4 0.5 4
0.4 0.4 0.4
3 3 3
0.3 0.3 0.3
2 2 2
0.2 0.2 0.2
0.1 1 0.1 1 0.1 1
0.0 0 0.0 0 0.0 0
0 20 40 0 20 40 0 20 40 0 20 40 0 20 40 0 20 40
Time[s] Time[s] Time[s] Time[s] Time[s] Time[s]
Fig.8. Weevaluatethetrackingperformanceofourmodelanduncertainty-awarecontrolleronthreereferencetrajectories,withtwofollowerrobots(inblue
andorange),positionedleftandrightoftheleaderrobot(ingreen).Foreachtrajectory,wereportthetrackingperformanceovertimeforpositionandrotation,
as well as the predicted uncertainties. The left trajectory (1) is a Figure Eight with the leader always facing in the same direction. The middle trajectory
(2) issimilar to thefirst, but the leaderis always facingthe direction itis moving. Inthis trajectory, FollowerA starts facingthe opposite directionof the
leader,resultinginhighpositionvarianceandlowrotationvariance,whichdecreasesastherobotrotates,facingthesamedirectionastheleader.Aspurious
erroneousrotationpredictionisaccompaniedbyahighuncertainty.Notethatastheleadermovesthroughthetrajectory,thereisalwaysoneinsideandone
outsidefollowerrobot.Thisrolechangesdependingonthesectionofthefigure,resultinginthesefollowertrajectories.Therighttrajectory(3)isarounded
rectangle.Weshowthetrajectoryfor120sandthetrackingerrorforthefirst60s.Thetrackingperformanceisconsistentacrossmultipleruns.
Our custom messaging protocol includes the ability to are programmed to maintain a fixed distance of 0.6 m from a
dynamically backoff messaging load based upon detected leader robot executing predefined trajectories. For the quanti-
network conditions. Our system is also uses a shared slot tative experiments, we assess the system’s tracking accuracy
TDMA message scheduling system which works with the by comparing the ground truth and estimated relative poses,
802.11 frame scheduler to maximize frame reception prob- focusing on position and rotation errors and the predicted
ability without compromising overall data rates. uncertainties for each dimension, using the same metrics
Uncertainty-aware relative pose control: We utilize a PD introduced in Sec. V. Our model computes one uncertainty
controller for relative pose control of follower robots, imple- per 3D dimension, which we report as mean.
menting independent controllers for position and orientation. The three trajectories are two Figure Eight trajectory, one
The controllers adjust based on the predicted uncertainty, de- for which we keep the orientation of the leader constant, and
activating if uncertainty exceeds certain thresholds to prevent one for which the leader is always facing in the direction it
inaccurate operations. This ensures that even with unreliable is moving. The third trajectory is a rounded rectangle, again
pose estimates, the robot can maintain its orientation toward with the leader facing the direction it is moving.
the leader until a reliable estimate becomes available. Results: Fig. 7 shows snapshots from four different real-
Experiment: We conduct qualitative experiments in a wide world deployments, and Fig. 8 shows the trajectories of all
range of different environments, including outdoor, and quan- robots for all quantitative experiments, as well as a visualiza-
titative experiments in a large room equipped with a motion tion of the position and rotation error over time. We provide
capturesystemforaccurateposetracking.Twofollowerrobots additional snapshots from more real-world experiments in the
]m[
y
]m[rrEsoP
]m[.ved.dtSsoP
]ged[rrEtoR
]ged[.ved.dtStoR
]m[
y
]m[rrEsoP
]m[.ved.dtSsoP
]ged[rrEtoR
]ged[.ved.dtStoR
]m[
y
]m[rrEsoP
]m[.ved.dtSsoP
]ged[rrEtoR
]ged[.ved.dtStoRappendix. ACKNOWLEDGMENTS
The qualitative experiments show a robust system that gen-
This work was supported in part by European Research
eralizestoawiderangeofscenarios,includingoutdoors,albeit
Council (ERC) Project 949949 (gAIa). J. Blumenkamp ac-
the scale prediction is less reliable in outdoor settings. Our
knowledges the support of the ‘Studienstiftung des deutschen
model was trained exclusively on indoor data and generalizes
Volkes’andanEPSRCtuitionfeegrant.Wealsoacknowledge
to outdoor data due to the wide range of training data used
a gift from Arm.
on the DinoV2 encoder. The performance can be significantly
improved by training on outdoor samples.
REFERENCES
The tracking error in the quantitative experiments oscillates
throughout the trajectories as the followers reactively adjust [1] Yehya Abouelnaga, Mai Bui, and Slobodan Ilic. Distill-
their position to the movements of the leader. Additionally, Pose: Lightweight Camera Localization Using Auxiliary
Figure 8 dynamic starts with Follower A facing the opposite
Learning.In2021IEEE/RSJInternationalConferenceon
direction of the Leader, resulting in higher position error and
IntelligentRobotsandSystems(IROS),pages7919–7924,
uncertainty while the rotation error is low. As the Follower 2021. doi: 10.1109/IROS51168.2021.9635870. URL
rotates,thepositionuncertaintydecreases.Duringthisprocess, https://ieeexplore.ieee.org/document/9635870.
the model generates an erroneous rotation prediction that is [2] Javier Alonso-Mora, Ross Knepper, Roland Siegwart,
accompanied with a spike in the uncertainty prediction. We and Daniela Rus. Local motion planning for collabo-
note that the tracking error for Figure 8 static differs between rative multi-robot manipulation of deformable objects.
both agents. This is due to the lack of structure in the lab In 2015 IEEE International Conference on Robotics
environment and it affects both robots differently, resulting and Automation (ICRA), pages 5495–5502, 2015. doi:
in better tracking performance for Follower A. The tracking 10.1109/ICRA.2015.7139967.
error is consistent for the other trajectories. Tab. VII reports [3] Eduardo Arnold, Jamie Wynn, Sara Vicente, Guillermo
quantitative evaluations for the same results, which are in line Garcia-Hernando, A´ron Monszpart, Victor Adrian
with the results reported in Tab. III. Note that the velocities Prisacariu, Daniyar Turmukhambetov, and Eric Brach-
reported are average velocities of the follower robots. The mann. Map-free Visual Relocalization: Metric Pose
velocities are consistent for both Figure 8 trajectories, but Relative to a Single Image. In ECCV, 2022. URL
differ for the rectangle trajectory, as the inner robot moves https://dl.acm.org/doi/10.1007/978-3-031-19769-7 40.
more slowly than the outer robot. [4] Matteo Bettini, Ajay Shankar, and Amanda Prorok. Het-
erogeneous Multi-Robot Reinforcement Learning. In
Proceedings of the 2023 International Conference on
VI. CONCLUSION Autonomous Agents and Multiagent Systems, AAMAS
’23, page 1485–1494, Richland, SC, 2023. International
In this work, we introduced a novel visual spatial founda- FoundationforAutonomousAgentsandMultiagentSys-
tion model for multi-robot control applications, based on an tems. ISBN 9781450394321. URL https://dl.acm.org/
existing computer vision foundation model. From monocular doi/abs/10.5555/3545946.3598801.
camera images, our model outputs accurate relative pose esti- [5] Jan Blumenkamp, Steven Morad, Jennifer Gielis, Qing-
matesandBEVsinthelocalframebyaggregatinginformation biao Li, and Amanda Prorok. A Framework for
from other robots. We validated our model on a custom Real-WorldMulti-RobotSystemsRunningDecentralized
real-world testset collected in various locations of an indoor GNN-Based Policies. In 2022 International Conference
office building. Then, we deployed our model onto robots, on Robotics and Automation (ICRA), pages 8772–8778,
demonstrating real-time multi-robot control. We find that our 2022. doi: 10.1109/ICRA46639.2022.9811744. URL
model can accurately predict poses, even when there are no https://ieeexplore.ieee.org/document/9811744.
pixel-level correspondences, and correctly predict BEVs for [6] RuojinCai,BharathHariharan,NoahSnavely,andHadar
obscured regions. In the real-world, we demonstrate accurate Averbuch-Elor. Extreme Rotation Estimation using
trajectory tracking, paving the way for more complex vision- DenseCorrelationVolumes.InIEEE/CVFConferenceon
only robotics tasks. Our model is able to estimate relative ComputerVisionandPatternRecognition(CVPR),2021.
poses with a median pose error of up to 33 cm and 5.8 ◦ URL https://openaccess.thecvf.com/content/CVPR2021/
for visible edges and 97 cm and 5.9 ◦ for invisible edges on html/Cai Extreme Rotation Estimation Using Dense
our real-world dataset containing 84048 relative poses. This Correlation Volumes CVPR 2021 paper.html.
is roughly equivalent to one body length of our robot. Using [7] Carlos Campos, Richard Elvira, Juan J. Go´mez
theseestimatedposes,ourmodelincreasestheBEVprediction Rodr´ıguez, Jose´ M. M. Montiel, and Juan D. Tardo´s.
scoreby8.75%comparedtoamulti-nodeaggregationscheme ORB-SLAM3: An Accurate Open-Source Library for
without poses. Visual, Visual–Inertial, and Multimap SLAM. IEEE
In future work, we plan to investigate how to apply this Transactions on Robotics, 37(6):1874–1890, 2021. doi:
model for other downstream tasks, and in combination with 10.1109/TRO.2021.3075644. URL https://ieeexplore.
learning-based multi-agent control policies. ieee.org/abstract/document/9440682.[8] Oscar De Silva, George K. I. Mann, and Raymond G. [16] Yushan Han, Hui Zhang, Huifang Li, Yi Jin, Congyan
Gosine. An Ultrasonic and Vision-Based Relative Posi- Lang, and Yidong Li. Collaborative Perception in Au-
tioningSensorforMultirobotLocalization.IEEESensors tonomous Driving: Methods, Datasets, and Challenges.
Journal, 15(3):1716–1726, 2015. doi: 10.1109/JSEN. IEEE Intelligent Transportation Systems Magazine, 15
2014.2364684. URL https://ieeexplore.ieee.org/abstract/ (6):131–151, 2023. doi: 10.1109/MITS.2023.3298534.
document/6934978. URL https://ieeexplore.ieee.org/document/10248946.
[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai [17] Alex Kendall and Yarin Gal. What Uncertainties Do
Li, and Li Fei-Fei. ImageNet: A large-scale hierar- We Need in Bayesian Deep Learning for Computer
chical image database. In 2009 IEEE Conference on Vision? In I. Guyon, U. Von Luxburg, S. Bengio,
Computer Vision and Pattern Recognition, pages 248– H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-
255, 2009. doi: 10.1109/CVPR.2009.5206848. URL nett, editors, Advances in Neural Information Process-
https://ieeexplore.ieee.org/document/5206848. ing Systems, volume 30. Curran Associates, Inc., 2017.
[10] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov, URL https://papers.nips.cc/paper files/paper/2017/hash/
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, 2650d6089a6d640c5e85b2b88265dc2b-Abstract.html.
Mostafa Dehghani, Matthias Minderer, Georg Heigold, [18] Alex Kendall, Matthew Grimes, and Roberto Cipolla.
Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An PoseNet: A Convolutional Network for Real-Time
Image is Worth 16x16 Words: Transformers for Im- 6-DOF Camera Relocalization. In Proceedings of the
age Recognition at Scale. In International Confer- IEEE International Conference on Computer Vision
ence on Learning Representations, 2021. URL https: (ICCV), December 2015. URL https://openaccess.
//openreview.net/forum?id=YicbFdNTTy. thecvf.com/content iccv 2015/html/Kendall PoseNet
[11] Pramit Dutta, Ganesh Sistu, Senthil Yogamani, Edgar A Convolutional ICCV 2015 paper.html.
Galva´n, and John McDonald. ViT-BEVSeg: A Hi- [19] BalajiLakshminarayanan,AlexanderPritzel,andCharles
erarchical Transformer Network for Monocular Birds- Blundell. Simple and Scalable Predictive Un-
Eye-View Segmentation. In 2022 International Joint certainty Estimation using Deep Ensembles. In
Conference on Neural Networks (IJCNN), pages 1–7, I. Guyon, U. Von Luxburg, S. Bengio, H. Wal-
2022. doi: 10.1109/IJCNN55064.2022.9891987. URL lach, R. Fergus, S. Vishwanathan, and R. Garnett,
https://ieeexplore.ieee.org/document/9891987. editors, Advances in Neural Information Processing
[12] Sovann En, Alexis Lechervy, and Fre´de´ric Jurie. RP- Systems, volume 30. Curran Associates, Inc., 2017.
Net: an End-to-End Network for Relative Camera URL https://papers.nips.cc/paper files/paper/2017/hash/
Pose Estimation. In Proceedings of the European 9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html.
Conference on Computer Vision (ECCV) Workshops, [20] Zakaria Laskar, Iaroslav Melekhov, Surya Kalia, and
2018. URL https://link.springer.com/chapter/10.1007/ Juho Kannala. Camera Relocalization by Computing
978-3-030-11009-3 46. Pairwise Relative Poses Using Convolutional Neural
[13] Yarin Gal and Zoubin Ghahramani. Dropout as a Network. In The IEEE International Conference on
Bayesian Approximation: Representing Model Uncer- Computer Vision (ICCV), Oct 2017. URL https://github.
tainty in Deep Learning. In Maria Florina Balcan and com/AaltoVision/camera-relocalisation.
Kilian Q. Weinberger, editors, Proceedings of The 33rd [21] Yiming Li, Dekun Ma, Ziyan An, Zixun Wang, Yiqi
International Conference on Machine Learning, vol- Zhong, Siheng Chen, and Chen Feng. V2X-Sim: Multi-
ume 48 of Proceedings of Machine Learning Research, Agent Collaborative Perception Dataset and Benchmark
pages 1050–1059, New York, New York, USA, 20–22 for Autonomous Driving. IEEE Robotics and Automa-
Jun 2016. PMLR. URL https://proceedings.mlr.press/ tion Letters, 7(4):10914–10921, 2022. doi: 10.1109/
v48/gal16.html. LRA.2022.3192802. URL https://ieeexplore.ieee.org/
[14] Fernando Gama, Ekaterina Tolstaya, and Alejandro document/9835036.
Ribeiro. Graph Neural Networks for Decentralized [22] Yiming Li, Juexiao Zhang, Dekun Ma, Yue Wang, and
Controllers. In ICASSP 2021 - 2021 IEEE International Chen Feng. Multi-Robot Scene Completion: Towards
Conference on Acoustics, Speech and Signal Process- Task-Agnostic Collaborative Perception. In Karen Liu,
ing (ICASSP), pages 5260–5264, 2021. doi: 10.1109/ Dana Kulic, and Jeff Ichnowski, editors, Proceedings of
ICASSP39728.2021.9414563. URL https://ieeexplore. The 6th Conference on Robot Learning, volume 205 of
ieee.org/document/9414563. ProceedingsofMachineLearningResearch,pages2062–
[15] Fernando Gama, Qingbiao Li, Ekaterina Tolstaya, 2072.PMLR,14–18Dec2023. URLhttps://proceedings.
Amanda Prorok, and Alejandro Ribeiro. Synthesiz- mlr.press/v205/li23e.html.
ing Decentralized Controllers With Graph Neural Net- [23] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc
works and Imitation Learning. IEEE Transactions Pollefeys. LightGlue: Local Feature Matching at
on Signal Processing, 70:1932–1946, 2022. doi: 10. Light Speed. In Proceedings of the IEEE/CVF
1109/TSP.2022.3166401. URL https://ieeexplore.ieee. International Conference on Computer Vision
org/document/9755021. (ICCV), pages 17627–17638, October 2023. URLhttps://openaccess.thecvf.com/content/ICCV2023/html/ Robust Visual Features without Supervision, 2023. URL
Lindenberger LightGlue Local Feature Matching at https://github.com/facebookresearch/dinov2/tree/main.
Light Speed ICCV 2023 paper.html. [33] Re´mi Pautrat, Iago Sua´rez, Yifan Yu, Marc Pollefeys,
[24] Yingfei Liu, Junjie Yan, Fan Jia, Shuailin Li, Aqi and Viktor Larsson. GlueStick: Robust Image Matching
Gao, Tiancai Wang, and Xiangyu Zhang. PETRv2: by Sticking Points and Lines Together. In Proceedings
A Unified Framework for 3D Perception from Multi- of the IEEE/CVF International Conference on Computer
Camera Images. In Proceedings of the IEEE/CVF Vision (ICCV), pages 9706–9716, October 2023.
International Conference on Computer Vision (ICCV), URL https://openaccess.thecvf.com/content/ICCV2023/
pages 3262–3272, October 2023. URL https: html/Pautrat GlueStick Robust Image Matching by
//openaccess.thecvf.com/content/ICCV2023/html/Liu Sticking Points and Lines Together ICCV 2023
PETRv2 A Unified Framework for 3D Perception paper.html.
from Multi-Camera Images ICCV 2023 paper.html. [34] LangPeng,ZhirongChen,ZhangjieFu,PengpengLiang,
[25] David G Lowe. Distinctive image features from scale- and Erkang Cheng. BEVSegFormer: Bird’s Eye View
invariant keypoints. International journal of computer Semantic Segmentation From Arbitrary Camera Rigs.
vision, 60:91–110, 2004. URL https://link.springer.com/ In Proceedings of the IEEE/CVF Winter Conference on
article/10.1023/B:VISI.0000029664.99615.94. Applications of Computer Vision (WACV), pages 5935–
[26] Alcherio Martinoli, Kjerstin Easton, and William Agas- 5943,January2023. URLhttps://openaccess.thecvf.com/
sounon. Modeling swarm robotic systems: A case study content/WACV2023/html/Peng BEVSegFormer Birds
in collaborative distributed manipulation. The Interna- Eye View Semantic Segmentation From Arbitrary
tional Journal of Robotics Research, 23(4-5):415–436, Camera Rigs WACV 2023 paper.html.
2004. [35] Valentin Peretroukhin, Matthew Giamou, W. Nicholas
[27] Iaroslav Melekhov, Juha Ylioinas, Juho Kannala, and Greene,DavidRosen,JonathanKelly,andNicholasRoy.
Esa Rahtu. Relative Camera Pose Estimation Using A Smooth Representation of Belief over SO(3) for Deep
Convolutional Neural Networks. In Jacques Blanc- Rotation Learning with Uncertainty. In Proceedings of
Talon, Rudi Penne, Wilfried Philips, Dan Popescu, and Robotics: Science and Systems, Corvalis, Oregon, USA,
Paul Scheunders, editors, Advanced Concepts for In- July 2020. doi: 10.15607/RSS.2020.XVI.007. URL
telligent Vision Systems, pages 675–687, Cham, 2017. https://www.roboticsproceedings.org/rss16/p007.html.
Springer International Publishing. ISBN 978-3-319- [36] JonahPhilionandSanjaFidler.Lift,Splat,Shoot:Encod-
70353-4. URLhttps://link.springer.com/chapter/10.1007/ ing Images From Arbitrary Camera Rigs by Implicitly
978-3-319-70353-4 57. Unprojecting to 3D. In Proceedings of the European
[28] Akmaral Moldagalieva and Wolfgang Ho¨nig. Virtual Conference on Computer Vision, 2020. URL https:
Omnidirectional Perception for Downwash Prediction //dl.acm.org/doi/abs/10.1007/978-3-030-58568-6 12.
within a Team of Nano Multirotors Flying in Close [37] Praveen Kumar Rajendran, Sumit Mishra, Luiz Felipe
Proximity, 2023. URL https://arxiv.org/abs/2303.03898. Vecchietti, and Dongsoo Har. RelMobNet: End-to-
[29] Paola Torrico Moro´n, Sahar Salimpour, Lei Fu, Xianjia End Relative Camera Pose Estimation Using a Ro-
Yu, Jorge Pen˜a Queralta, and Tomi Westerlund. Bench- bust Two-Stage Training. In Leonid Karlinsky, Tomer
markingUWB-BasedInfrastructure-FreePositioningand Michaeli, and Ko Nishino, editors, Computer Vision
Multi-Robot Relative Localization: Dataset and Charac- – ECCV 2022 Workshops, pages 238–252, Cham,
terization, 2023. URL https://arxiv.org/abs/2305.08532. 2023. Springer Nature Switzerland. ISBN 978-3-031-
[30] D.A. Nix and A.S. Weigend. Estimating the mean 25075-0. URLhttps://link.springer.com/chapter/10.1007/
and variance of the target probability distribution. In 978-3-031-25075-0 18.
Proceedings of 1994 IEEE International Conference on [38] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik
Neural Networks (ICNN’94), volume 1, pages 55–60 Wijmans, Oleksandr Maksymets, Alexander Clegg,
vol.1, 1994. doi: 10.1109/ICNN.1994.374138. URL John M Turner, Eric Undersander, Wojciech Galuba,
https://ieeexplore.ieee.org/document/374138. Andrew Westbury, Angel X Chang, Manolis Savva, Yili
[31] Kwang-KyoOh,Myoung-ChulPark,andHyo-SungAhn. Zhao, and Dhruv Batra. Habitat-Matterport 3D Dataset
A survey of multi-agent formation control. Automatica, (HM3D):1000Large-scale3DEnvironmentsforEmbod-
53:424–440, 2015. ied AI. In Thirty-fifth Conference on Neural Information
[32] Maxime Oquab, Timothe´e Darcet, Theo Moutakanni, Processing Systems Datasets and Benchmarks Track,
Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre 2021. URL https://aihabitat.org/datasets/hm3d.
Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin [39] Craig W Reynolds. Flocks, herds and schools: A dis-
El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu tributed behavioral model. In Proceedings of the 14th
Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, annualconferenceonComputergraphicsandinteractive
Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan techniques, pages 25–34, 1987.
Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Ar- [40] Chris Rockwell, Justin Johnson, and David F. Fouhey.
mand Joulin, and Piotr Bojanowski. DINOv2: Learning The 8-Point Algorithm as an Inductive Bias for RelativePose Prediction by ViTs. In 2022 International Con- https://openaccess.thecvf.com/content ICCV 2019/
ference on 3D Vision (3DV), pages 1–11, 2022. doi: html/Savva Habitat A Platform for Embodied AI
10.1109/3DV57658.2022.00028. URLhttps://ieeexplore. Research ICCV 2019 paper.html.
ieee.org/abstract/document/10044394. [48] Davide Scaramuzza and Friedrich Fraundorfer. Vi-
[41] Thomas Roddick and Roberto Cipolla. Predicting sual Odometry [Tutorial]. IEEE Robotics & Automa-
Semantic Map Representations From Images Using tion Magazine, 18(4):80–92, 2011. doi: 10.1109/MRA.
Pyramid Occupancy Networks. In Proceedings 2011.943233. URL https://ieeexplore.ieee.org/abstract/
of the IEEE/CVF Conference on Computer Vision document/6096039.
and Pattern Recognition (CVPR), June 2020. [49] Johannes L. Scho¨nberger and Jan-Michael Frahm.
URL https://openaccess.thecvf.com/content CVPR Structure-from-Motion Revisited. In 2016 IEEE Con-
2020/html/Roddick Predicting Semantic Map ference on Computer Vision and Pattern Recogni-
Representations From Images Using Pyramid tion (CVPR), pages 4104–4113, 2016. doi: 10.
Occupancy Networks CVPR 2020 paper.html. 1109/CVPR.2016.445. URL https://ieeexplore.ieee.org/
[42] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and abstract/document/7780814.
Gary Bradski. Orb: An efficient alternative to sift or [50] Guni Sharon, Roni Stern, Ariel Felner, and Nathan R
surf. In 2011 International Conference on Computer Sturtevant. Conflict-basedsearchforoptimalmulti-agent
Vision, pages 2564–2571, 2011. doi: 10.1109/ICCV. pathfinding. Artificial intelligence, 219:40–66, 2015.
2011.6126544. [51] Stephen L. Smith, Mac Schwager, and Daniela Rus.
[43] Xu Runsheng, Xiang Hao, Tu Zhengzhong, Xia Xin, Persistent robotic tasks: Monitoring and sweeping in
Yang Ming-Hsuan, and Ma Jiaqi. V2X-ViT: Vehicle- changing environments. IEEE Transactions on Robotics,
to-EverythingCooperativePerceptionwithVisionTrans- 28(2):410–426, 2012. doi: 10.1109/TRO.2011.2174493.
former. In Proceedings of the European Conference on [52] Carole H Sudre, Wenqi Li, Tom Vercauteren, Sebastien
ComputerVision(ECCV),2022. URLhttps://dl.acm.org/ Ourselin,andMJorgeCardoso. Generaliseddiceoverlap
doi/abs/10.1007/978-3-031-19842-7 7. as a deep learning loss function for highly unbalanced
[44] Paul-Edouard Sarlin, Daniel DeTone, Tomasz segmentations. In Deep Learning in Medical Image
Malisiewicz, and Andrew Rabinovich. SuperGlue: Analysis and Multimodal Learning for Clinical Decision
Learning Feature Matching With Graph Neural Support: Third International Workshop, DLMIA 2017,
Networks. In Proceedings of the IEEE/CVF and 7th International Workshop, ML-CDS 2017, Held
Conference on Computer Vision and Pattern in Conjunction with MICCAI 2017, Que´bec City, QC,
Recognition (CVPR), June 2020. URL https: Canada, September 14, Proceedings 3, pages 240–248.
//openaccess.thecvf.com/content CVPR 2020/html/ Springer, 2017. URL https://link.springer.com/chapter/
Sarlin SuperGlue Learning Feature Matching With 10.1007/978-3-319-67558-9 28.
Graph Neural Networks CVPR 2020 paper.html. [53] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao,
[45] Paul-Edouard Sarlin, Ajaykumar Unagar, Mans Larsson, andXiaoweiZhou. LoFTR:Detector-FreeLocalFeature
Hugo Germain, Carl Toft, Viktor Larsson, Marc Matching With Transformers. In Proceedings of the
Pollefeys, Vincent Lepetit, Lars Hammarstrand, IEEE/CVF Conference on Computer Vision and Pattern
Fredrik Kahl, and Torsten Sattler. Back to the Recognition(CVPR),pages8922–8931,June2021. URL
Feature: Learning Robust Camera Localization https://openaccess.thecvf.com/content/CVPR2021/html/
From Pixels To Pose. In Proceedings of the Sun LoFTR Detector-Free Local Feature Matching
IEEE/CVF Conference on Computer Vision and Pattern With Transformers CVPR 2021 paper.html.
Recognition(CVPR),pages3247–3257,June2021. URL [54] Andrew Szot, Alex Clegg, Eric Undersander, Erik
https://openaccess.thecvf.com/content/CVPR2021/html/ Wijmans,YiliZhao,JohnTurner,NoahMaestre,Mustafa
Sarlin Back to the Feature Learning Robust Camera Mukadam, Devendra Chaplot, Oleksandr Maksymets,
Localization From Pixels CVPR 2021 paper.html. Aaron Gokaslan, Vladimir Vondrus, Sameer Dharur,
[46] Martin Saska, Jan Vakula, and Libor Pˇreuc´il. Swarms of Franziska Meier, Wojciech Galuba, Angel Chang, Zsolt
micro aerial vehicles stabilized under a visual relative Kira, Vladlen Koltun, Jitendra Malik, Manolis Savva,
localization. In 2014 IEEE International Conference and Dhruv Batra. Habitat 2.0: Training Home Assistants
on Robotics and Automation (ICRA), pages 3570–3575, to Rearrange their Habitat. In Advances in Neural
2014. doi: 10.1109/ICRA.2014.6907374. URL https: Information Processing Systems (NeurIPS), 2021.
//ieeexplore.ieee.org/abstract/document/6907374. URL https://proceedings.neurips.cc/paper/2021/hash/
[47] ManolisSavva,AbhishekKadian,OleksandrMaksymets, 021bbc7ee20b71134d53e20206bd6feb-Abstract.html.
Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, [55] Ekaterina Tolstaya, Fernando Gama, James Paulos,
Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, George Pappas, Vijay Kumar, and Alejandro Ribeiro.
and Dhruv Batra. Habitat: A Platform for Embodied AI Learning Decentralized Controllers for Robot Swarms
Research. InProceedingsoftheIEEE/CVFInternational with Graph Neural Networks. In Leslie Pack Kaelbling,
Conference on Computer Vision (ICCV), 2019. URL Danica Kragic, and Komei Sugiura, editors, Proceed-ings of the Conference on Robot Learning, volume 100 [64] Chenhao Yang, Yuyi Liu, and Andreas Zell. Learning-
of Proceedings of Machine Learning Research, pages based Camera Relocalization with Domain Adaptation
671–682. PMLR, 30 Oct–01 Nov 2020. URL https: via Image-to-Image Translation. In 2021 International
//proceedings.mlr.press/v100/tolstaya20a.html. Conference on Unmanned Aircraft Systems (ICUAS),
[56] Jur Van den Berg, Ming Lin, and Dinesh Manocha. pages 1047–1054, 2021. doi: 10.1109/ICUAS51884.
Reciprocal velocity obstacles for real-time multi-agent 2021.9476673. URL https://ieeexplore.ieee.org/abstract/
navigation. In 2008 IEEE international conference on document/9476673.
robotics and automation, pages 1928–1935. Ieee, 2008. [65] Chenyu Yang, Yuntao Chen, Hao Tian, Chenxin
[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Tao, Xizhou Zhu, Zhaoxiang Zhang, Gao Huang,
Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Hongyang Li, Yu Qiao, Lewei Lu, Jie Zhou, and
Kaiser, and Illia Polosukhin. Attention is All you Jifeng Dai. BEVFormer v2: Adapting Modern
Need. In I. Guyon, U. Von Luxburg, S. Bengio, Image Backbones to Bird’s-Eye-View Recognition
H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar- via Perspective Supervision. In Proceedings of the
nett, editors, Advances in Neural Information Process- IEEE/CVF Conference on Computer Vision and Pattern
ing Systems, volume 30. Curran Associates, Inc., 2017. Recognition (CVPR), pages 17830–17839, June 2023.
URL https://papers.nips.cc/paper files/paper/2017/hash/ URL https://openaccess.thecvf.com/content/CVPR2023/
3f5ee243547dee91fbd053c1c4a845aa-Abstract.html. html/Yang BEVFormer v2 Adapting Modern Image
[58] Viktor Walter, Martin Saska, and Antonio Franchi. Fast Backbones to Birds-Eye-View Recognition via
Mutual Relative Localization of UAVs using Ultravio- CVPR 2023 paper.html.
let LED Markers. In 2018 International Conference [66] William J Youden. Index for rating diagnostic tests.
on Unmanned Aircraft Systems (ICUAS), pages 1217– Cancer, 3(1):32–35, 1950.
1226, 2018. doi: 10.1109/ICUAS.2018.8453331. URL [67] Alexander Zelinsky, Ray A Jarvis, JC Byrne, Shinichi
https://ieeexplore.ieee.org/abstract/document/8453331. Yuta, et al. Planning paths of complete coverage of
[59] Tsun-Hsuan Wang, Sivabalan Manivasagam, Ming an unstructured environment by a mobile robot. In
Liang, Bin Yang, Wenyuan Zeng, and Raquel Urtasun. Proceedings of international conference on advanced
V2VNet: Vehicle-to-Vehicle Communication for Joint robotics, volume 13, pages 533–538. Citeseer, 1993.
Perception and Prediction. In Andrea Vedaldi, Horst [68] Qunjie Zhou, Torsten Sattler, Marc Pollefeys, and Laura
Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Leal-Taixe´. To Learn or Not to Learn: Visual Local-
Computer Vision – ECCV 2020, pages 605–621, Cham, ization from Essential Matrices. In 2020 IEEE Interna-
2020. Springer International Publishing. ISBN 978-3- tional Conference on Robotics and Automation (ICRA),
030-58536-5. URL https://link.springer.com/chapter/10. pages 3319–3326, 2020. doi: 10.1109/ICRA40945.
1007/978-3-030-58536-5 36. 2020.9196607. URL https://ieeexplore.ieee.org/abstract/
[60] KarlWeiss,TaghiMKhoshgoftaar,andDingDingWang. document/9196607.
A survey of transfer learning. Journal of Big data, 3:1– [69] Yang Zhou, Jiuhong Xiao, Yue Zhou, and Giuseppe
40, 2016. Loianno. Multi-Robot Collaborative Perception With
[61] Dominik Winkelbauer, Maximilian Denninger, and Graph Neural Networks. IEEE Robotics and Automa-
Rudolph Triebel. Learning to Localize in New En- tion Letters, 7(2):2289–2296, 2022. doi: 10.1109/LRA.
vironments from Synthetic Training Data. In 2021 2022.3141661. URL https://ieeexplore.ieee.org/abstract/
IEEEInternationalConferenceonRoboticsandAutoma- document/9676458.
tion (ICRA), pages 5840–5846, 2021. doi: 10.1109/
ICRA48506.2021.9560872. URL https://ieeexplore.ieee.
org/document/9560872.
[62] Runsheng Xu, Zhengzhong Tu, Hao Xiang, Wei Shao,
Bolei Zhou, and Jiaqi Ma. CoBEVT: Cooperative Bird’s
Eye View Semantic Segmentation with Sparse Trans-
formers. In Karen Liu, Dana Kulic, and Jeff Ichnowski,
editors, Proceedings of The 6th Conference on Robot
Learning,volume205ofProceedingsofMachineLearn-
ingResearch,pages989–1000.PMLR,14–18Dec2023.
URL https://proceedings.mlr.press/v205/xu23a.html.
[63] Chenhao Yang, Yuyi Liu, and Andreas Zell. RCPNet:
Deep-Learning based Relative Camera Pose Estimation
for UAVs. In 2020 International Conference on Un-
manned Aircraft Systems (ICUAS), pages 1085–1092,
2020. doi: 10.1109/ICUAS48674.2020.9214000. URL
https://ieeexplore.ieee.org/abstract/document/9214000.APPENDIXA
TRAINING 1
A. Dataset val
0 train
The samples of the datasets DSim DSim and DSim are
Train Test Val
randomly sampled to be within orientation range [−π, π]
32 32 1
for roll and pitch, and within a height of 0.1 to 0.3 meters, −
representing the camera mounted on the robot.
0 2 4 6 8 10 12 14
Epoch
B. Model architecture
This section introduces all hyperparameters and implemen- Fig. 9. We show the training and validation loss for a model trained with
tation details of the model architecture. F = 48 and S = 128 over N = 5 different training seeds. We report a
lowaveragestandarddeviationof0.011374overallepochsonthevalidation
All transformer blocks have 12 attention heads. The MLP
loss.
used in the transformer has a hidden size of 4F. All blocks
utilize a dropout of 0.2.
The encoder f consists of the pre-trained DinoV2 vi-
enc
sion transformer and two more transformer layers previously
described as fenc . The input feature size is 224 × 224, of
trans
which DinoV2 generates an embedding of size S = 256 and
F =384.WeaddalinearlayerthattransformsF downtothe
parameter described in this paper (e.g. 24) while maintaining
S. The output of this is then cropped to the S described in
this paper (e.g. 128) by cropping the sequence length at that
position.
Fig.10. Tocollectthereal-worldtestset,weconstructastaticbasestation
The pairwise pose encoder f pose consists of transformer equippedwithtwocamerasfacinginoppositedirectionsandalidar.Weadd
fpose, which is assembled from five transformer blocks, and amarkerontherobotsothatitsposecanbetrackedwiththelidarandthen
fotr uan rs MLPs fµ, fσ, fµ, fσ. We first project the embedding manuallymovetherobotaroundthebasestation.
p p R R
E to a larger feature size F = 192 using a linear layer.
i
Each transformer layer has F =192 in and out features. The
APPENDIXB
learnabletransformerpositionalembeddingS isinitialized
pose REAL-WORLDDATASET
to match F and S, with normal distribution initialization of
We visualize the five different scenes we collected the
σ = 0.02. The MLPs fµ, fσ, fµ, fσ are represented by
p p R R dataset from as well as the data distribution from each re-
one linear layer mapping the output of the transformer from
spective scene in Fig. 12. We explain the setup of our data
F = 192 to F = 17 (position: 3, position uncertainty: 3,
collection unit in Fig. 10.
orientation: 10, orientation uncertainty: 1).
The feature aggregation function f agg is built from f ta rg ang s, APPENDIXC
whichconsistsoffivetransformerlayers,andf tp rao nst s,whichcon- RESULTS
sistsofonetransformerlayer,allofwhichhaveanembedding
Weperformafurtherevaluationonthedistributionsofpose
sizeofF =192.TheMLPf consistsofthreelayerswith
aggpos errorsandvariationsonthesimulationtestsetDSim inFig.13.
17, 48 and 48 neurons each, the positional embedding S Test
agg The results are in line with the results performed on DReal in
is initialized similar to the one in f , and the BEV decoder Test
pose the main manuscript.
f takes the first sequence element of fpost as input and
BEV trans
scales it up through seven alternations of convolutions and A. Simulation
upsampling. We visualize four additional samples from the simulation
dataset DSim in Fig. 14, Fig. 15, Fig. 16, Fig. 17, Fig. 18 and
C. Loss function Test
Fig. 19.
TheparametersαbalancesbetweenDiceandBCEloss,and
we set it to α = 0.5, and the parameter β balances between B. Real-World
position and orientation prediction accuracy, and we set it to We visualize two more samples from the real-world dataset
1, given equal weight to both. DReal in Fig. 20, Fig. 21, Fig. 22, Fig. 23.
Test
D. Training
We set the initial learning rate to 1 5 and configure a
−
learning rate schedule to increase this over two epochs up
to 1 3, followed by a sinusoidal decline to 0 over 20 epochs.
−
We show the train and validation loss for N = 5 training
runs with different seeds in Fig. 9.
ssoL0s 1s 2s 3s 4s 5s
Fig. 11. We show additional snapshots of real-world deployments in different scenes with up to four robots. Each row contains six frames from a video
recording,eachspaced1sintime.2 2 2 2 2 2 2 2
1 1 1 1 1 1 1 1
0 0 0 0 0 0 0 0
−1 −1 −1 −1 −1 −1 −1 −1
−2 −2 −2 −2 −2 −2 −2 −2
246
−2 −1 Po0sx 1 2
10 2468
−2 −1 Po0sx 1 2
10 2468
−2 −1 Po0sx 1 2
2468
−2 −1 Po0sx 1 2
10 2468
−2 −1 Po0sx 1 2
2468
−2 −1 Po0sx 1 2
11 02 257 ..... 50505
−2 −1 Po0sx 1 2
2468
−2 −1 Po0sx 1 2
0-180-135 -90 -45yaw0gt45 90 135 180 0-180-135 -90 -45yaw0gt45 90 135 180 0-180-135 -90 -45yaw0gt45 90 135 180 0-180-135 -90 -45yaw0gt45 90 135 180 0-180-135 -90 -45yaw0gt45 90 135 180 0-180-135 -90 -45yaw0gt45 90 135 180 0.0-180-135 -90 -45yaw0gt45 90 135 180 0-180-135 -90 -45yaw0gt45 90 135 180
Fig.12. Wecollectthereal-worlddatasetDReal fromfiveuniquescenesinanindoorofficebuilding.Thescenesshowdifferentchallenges,fromcluttered
Test
environmentsoversceneswithhighceilingstosunnyfloors.Weshowthedistributionofpositionsforeachscene(secondrow)andthedistributionofrelative
anglesbetweennodes(thirdrow).
180
135
90
45
0
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0 22.5 45 67.5 90 0 22.5 45 67.5 90
Pos. Err Pos. Var Rot. Err Rot. Var (deg)
Fig. 13. We show qualitative distributions for position and rotation errors and variances over the relative angle between two robots on all edges of the
simulationtestsetDSim.Twoagentsfacingthesamedirectionwouldhaveanangledifferenceof0 ,whereastwoagentsfacingtheoppositedirectionwould
Test ◦
have an angle difference of 180 . We show the FOV as dashed line, where any results below the line indicate that the edge has some image overlap and
◦
above no image overlap. The horizontal marginals show the distribution of each individual plot while the vertical marginal shows the distribution of angle
differences over the dataset. While the position error and variance increase over the threshold, the majority of the predictions are still useful. The rotation
errorisconsistentacrossangledifferences,whiletheuncertaintyincreases.
ysoP
tnecreP
)ged(
.ffid
elgnA
ysoP
tnecreP
ysoP
tnecreP
ysoP
tnecreP
ysoP
tnecreP
ysoP
tnecreP
ysoP
tnecreP
ysoP
tnecrePv v v v v
0 1 2 3 4
2
0
2
−
2
0
2
−
2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5
− − − − −
Fig.14. SampleAfromDSim.
Test
v v v v v
0 1 2 3 4
2
0
2
−
2
0
2
−
2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5
− − − − −
Fig.15. SampleBfromDSim.
Test
I
hturT
dnuorG
noitciderP
I
hturT
dnuorG
noitciderP
i
iv v v v v
0 1 2 3 4
2
0
2
−
2
0
2
−
2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5
− − − − −
Fig.16. SampleCfromDSim.
Test
v v v v v
0 1 2 3 4
2
0
2
−
2
0
2
−
2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5
− − − − −
Fig.17. SampleDfromDSim.
Test
I
hturT
dnuorG
noitciderP
I
hturT
dnuorG
noitciderP
i
iv v v v v
0 1 2 3 4
2
0
2
−
2
0
2
−
2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5
− − − − −
Fig.18. SampleEfromDSim.
Test
v v v v v
0 1 2 3 4
2
0
2
−
2
0
2
−
2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5
− − − − −
Fig.19. SampleFfromDSim.
Test
I
hturT
dnuorG
noitciderP
I
hturT
dnuorG
noitciderP
i
iv v v v v v
0 1 2 0 1 2
2 2
0 0
2 2
− −
2 2
0 0
2 2
− −
2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5
− − − − − −
Fig.20. SampleAfromDReal. Fig.22. SampleCfromDReal.
Test Test
v v v v v v
0 1 2 0 1 2
2 2
0 0
2 2
− −
2 2
0 0
2 2
− −
2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5 2.5 0.0 2.5
− − − − − −
Fig.21. SampleBfromDReal. Fig.23. SampleDfromDReal.
Test Test
iI
hturT
dnuorG
noitciderP
iI
hturT
dnuorG
noitciderP
iI
hturT
dnuorG
noitciderP
iI
hturT
dnuorG
noitciderP