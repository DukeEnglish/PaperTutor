FLAME : Factuality-Aware Alignment for Large Language Models
Sheng-ChiehLin1∗,LuyuGao2,BarlasOguz3,
WenhanXiong3,JimmyLin1,Wen-tauYih3, and XilunChen3†
UniversityofWaterloo1,CarnegieMellonUniversity2,MetaAI3
s269lin@uwaterloo.ca, xilun@meta.com
Abstract 55
50 SFT length: 1.0k
Alignmentisastandardproceduretofine-tune +DPO
length: 1.4k
pre-trainedlargelanguagemodels(LLMs)to 45 length: 1.2k +DPO length: 1.4k
follownaturallanguageinstructionsandserve SFT
+DPO length: 1.5k
as helpful AI assistants. We have observed, 40
however,thattheconventionalalignmentpro-
35 length: 1.6k
cess fails to enhance the factual accuracy of
RLHF
LLMs, and often leads to the generation of 30 (Chat Llama2)
more false facts (i.e. hallucination). In this 20 30 40 50 60 70
paper,westudyhowtomaketheLLMalign- Helpfulness
mentprocessmorefactual,byfirstidentifying
Figure1: Models’helpfulnessonAlpacaEvalvsfactu-
factorsthatleadtohallucinationinbothalign-
alityonbiography. Helpfulnessismeasuredbymodels’
mentsteps:supervisedfine-tuning(SFT)and
winrateoverourbaselineSFT+DPOonAlpacaEval.
reinforcementlearning(RL).Inparticular,we
Dotsizerepresentsaveragedlengthofbiogeneration.
findthattrainingtheLLMonnewknowledge
or unfamiliar texts can encourage hallucina-
tion. ThismakesSFTlessfactualasittrains
claims (Min et al., 2023). This motivates us to
on human labeled data that may be novel to
studytheunderlyingcausesofLLMhallucination theLLM.Furthermore,rewardfunctionsused
instandardRLcanalsoencouragehallucina- aswellasitsrelationtothealignmentprocedure.
tion, because it guides the LLM to provide The standard alignment process consists of
morehelpfulresponsesonadiversesetofin- two training phases: (1) supervised fine-tuning
structions, often preferring longer and more
(SFT)(Sanhetal.,2022);(2)reinforcementlearn-
detailed responses. Based on these observa-
ing(RL)withhuman (RLHF,Ouyangetal.,2022;
tions,weproposefactuality-awarealignment
Bai et al., 2022) or automated feedback (RLAIF,
(FLAME ),comprisedoffactuality-awareSFT
and factuality-aware RL through direct pref- Bai et al., 2023). In our study, we find that both
erence optimization. Experiments show that theSFTandtheRLstepsinthestandardalignment
ourproposedfactuality-awarealignmentguides processmayactuallyencourageLLMstohalluci-
LLMstooutputmorefactualresponseswhile nate. First,intheSFTstage,LLMsarefine-tuned
maintaininginstruction-followingcapability.
with diverse instructions paired with human cre-
ated high-quality responses. While this leads to
1 Introduction
stronginstructionfollowingcapability(Zhouetal.,
Alignment is a standard procedure to make pre- 2023), our study shows that such human labeled
trained large language models (LLMs) (Brown responses may present new or unknown informa-
et al., 2020; Touvron et al., 2023) follow natural tiontotheLLM.This,inturn,mayinadvertently
language instructions and serve as helpful AI as- promote hallucination. Second, we find that the
sistants. Despitesignificantprogressininstruction standardrewardusedintheRLstageoftenprefers
tuning (Wang et al., 2023a; Zhou et al., 2023; Li longerandmoredetailedresponses(Singhaletal.,
et al., 2024) and LLM alignment (Ouyang et al., 2023;Yuanetal.,2024),whichtendstostimulate
2022; Bai et al., 2022; Yuan et al., 2024), state- the LLM to yield more false claims, as shown in
of-the-art LLMs are still prone to generate false theblackdotsinFigure1. Onepossiblereasonis
∗ThisworkisdoneduringSheng-Chieh’sinternshipatMeta. thatmostexistingRLHForRLAIFapproachesrely
†XilunandSheng-Chiehcontributedequallytothiswork. on a single scalar reward to represent preference,
4202
yaM
2
]LC.sc[
1v52510.5042:viXra
)erocStcAF(
ytilautcaFwhichstrugglestocovermultiplealignmentskill etal.,2023)forfactualityevaluation,respectively.
sets(Yeetal.,2024)andislikelytounder-present AsshowninFigure1,usingour FLAME method
theaspectoffactuality(Hoskingetal.,2024). (SFT +DPO ),asignificantlyhigherFActScore
(+5.6 pts) is achieved compared to the standard
Toaddresstheaforementionedissues,westudy
alignment process (SFT + DPO), without sacri-
the key factors which impact factuality during
ficingtheLLM’sinstructionfollowingcapability
alignment. In particular, we first conduct a pi-
(51.2%winrate). Ourablationstudyalsoindicates
lot study on the biography generation task (Min
thatidentifyingfact-basedinstructionsisthekeyto
et al., 2023) in a more controlled setting where
factualalignmentinthegeneralalignmentsetting.
thealignmentprocessfocusessolelyonfactuality
(Section 3). Our pilot study reveals that a LLM
2 RelatedWork
hallucinatesmoreifitisfine-tunedonnewknowl-
edge in either the SFT or the RL stage. For ex- Alignment. Sincepre-trainedLLMscannotac-
ample, a LLM becomes significantly less factual curately follow human instructions, a bunch of
whenfine-tunedonresponsesproducedbyamodel work has been proposed to improve LLM align-
withaccesstoexternalknowledge(e.g.aretrieval- ment through SFT and RL. Some propose to im-
augmented LLM), even though those responses proveSFTthroughdatacuration(Zhouetal.,2023;
aremorefactualthemselves. Similarly,hallucina- Chen et al., 2024), diverse instruction augmenta-
tionisgreatlyincreasedifRLAIFisperformedon tion (Wang et al., 2023a; Li et al., 2024) while
preferencepairsthatconsistofretrieval-augmented othersfocusonRLwithhumanfeedback(Ouyang
LLMoutputaspositiveexamplesandtheLLM’s et al., 2022; Bai et al., 2022), AI feedback (Bai
ownoutputasnegativeexamples. Asaresult,we etal.,2023;Sunetal.,2024;Yuanetal.,2024). The
discoverthatfine-tuningapre-trainedLLMon(a maingoalofthesealignmentapproachesisinstruc-
selectedsubsetof)itsowngenerationsyieldsmore tion following capability (or helpfulness), which
factualresponsesandreduceshallucinations. may guide LLMs to output detailed and lengthy
responses(Singhaletal.,2023)butinevitablyen-
Ourultimategoalistoimprovethefactualityof
couragehallucination.
thestandardalignmentprocess,whichischalleng-
ingsinceLLMsmaybegivendiverseandcomplex
Factuality. Priorworkhashighlightedtheissue
instructions. AsshowninFigure2,weobservethat
of hallucination in LLMs (Kandpal et al., 2023;
someinstructionsrequirefactualresponseswhile
Mallenetal.,2023). Toaddresstheissue,important
the others do not. Motivated by the observation,
researchlinesarefactualityevaluation(Minetal.,
we propose factuality-aware alignment. We first
2023;Wangetal.,2023b;Chernetal.,2023)and
identifyfact-basedinstructionsthatrequirefactual
improvement. Some training-free approaches to
responses. For fact-based instructions, we lever-
improveLLMs’factualityincludeexternalknowl-
age the findings in our pilot study to create addi-
edge augmentation (Kandpal et al., 2023; Cheng
tionaltrainingdataatbothSFTandRLstagesto
etal.,2023;Jiangetal.,2023)andspecializedde-
explicitlyguideLLMstooutputfactualresponses.
coding(Lietal.,2023;Chuangetal.,2024).
Specifically,attheSFTstage,forfact-basedinstruc-
RecentstudiesapplyRLtoimproveLLMs’fac-
tions,insteadofusinghumancreatedseedtraining
tuality. Forexample,Tianetal.(2024)proposeto
data,weconstructfew-shotdemonstrations(from
constructfactualitypreferencepairsfordirectpref-
thesameseeddata)andgeneratetrainingdataus-
erenceoptimization(DPO;Rafailovetal.,2023),
ing the pre-trained LLM’s own knowledge. This
which is closely related to our work. However,
can prevent fine-tuning the LLM on knowledge
they focus solely on enhancing LLMs’ factual-
unknowntoitself. AttheRLstage,wecreatead-
itythroughDPObutoverlookitspotentialimpact
ditionalpreferencepairsfocusedonfactualityfor
onthemodels’instruction-followingcapability,as
fact-basedinstructions,whicharecombinedwith
demonstratedinourexperiments. Incontrast,our
thestandardpreferencepairsforinstructionfollow-
workprovidesacomprehensiveexaminationofim-
ingduringDirectPreferenceOptimization(DPO;
provingLLMs’factualityandinstruction-following
Rafailovetal.,2023).
abilitythroughfine-tuningapproachesencompass-
We evaluate models on Alpaca Eval (Dubois ing both SFT and DPO. Concurrent to our work,
etal.,2024)andBiography,usingwinrateforin- Kangetal.(2024)findthatLLMstendtohalluci-
structionfollowingcapabilityandFActScore(Min natewhenfacingunfamiliarqueries. TheyconsiderimprovingLLMs’factualityasteachingLLMsto Table1: Pilotstudyonbiographygeneration. Pos. de-
notesthepositivesforSFTorDPO.Neg. denotesthe
outputabstainingorlessdetailedresponsesonsuch
negativesforDPO.FSdenotesFActScore.
unfamiliar queries, a similar behavior observed
fromourLLMsfine-tunedwithFLAME(seecase
src.ofsupervision Bio
studiesinSection5.5). Itisworthmentioningthat Llama-27B
Pos. Neg. FS #Corr./Err.
bothpriorstudiesfocusonasimplifiedscenarioas
(1)PT - - 39.1 14.4/22.0
ourpilotstudyinSection3: fine-tuningLLMsto
(2)PTRAG - - 55.4 18.6/15.9
improvefactualityonasingletask(e.g.,fine-tuning (3) PT - 37.9 13.4/21.8
SFT
and evaluating on biography generation). In con- (4) PTRAG - 35.7 13.5/23.7
trast,weconsiderthegeneralalignmenttask,where (5) PT∗ PT∗ 41.6 15.4/20.7
DPO
(6) PTRAG PT 23.5 12.7/34.9
LLMsaregivendiverseandcomplexinstructions.
∗FActScoreisusedtoselectpositivesandnegatives.
3 APilotStudyonFactualAlignment
erence optimization (DPO) fine-tuned LLMs as
In this section, we first study how to align large PT,SFT,andDPO,respectively.2
languagemodels(LLMs)tobemorefactual. We
SFT. We explore two sources of supervision to
use biography generation as the task of our pilot
generatetrainingdata(detailedinAppendixA.1):
studyfortwomainreasons:(1)Biographygener-
(1) using PTRAG with few-shot demonstration to
ationisasimplifiedsettingwherefactualityisthe
generatebiographiesforeachnameentityintrain-
sole focus of the alignment process. As we will
ing data, where PTRAG is PT augmented with an
discuss in Section 4, studying factual alignment
off-the-shelfretriever(Linetal.,2023);(2)using
on diverse human instructions is more complex,
vanillaPTwithfew-shotdemonstrationtogener-
asthealignmentprocessencompassesaspectsbe-
atetrainingdataasabaseline. AsshowninTable1,
yondfactuality,suchashelpfulnessandsafety. (2)
PTRAG isindeedmuchmorefactualthanPT. How-
Evaluatingthefactualityofbiographygeneration
ever, a surprising discovery in the pilot study is
isrelativelyeasysinceWikipediacoverssufficient
thatfine-tuningonsuchmorefactualinstruction–
information for public figures and most the facts
biography pairs generated by PTRAG results in a
aboutapersonisnon-debatable(Minetal.,2023).
lessfactualSFTmodel(row4vs3).
3.1 AlignmentforBiographyGeneration
DPO. Wefurtherfine-tunetheLLMstobemore
Astandardalignmentprocedureconsistsofsuper-
factual through DPO. An intuitive way to create
vised fine-tuning (SFT) and reinforcement learn-
factuality preference pairs is to directly use the
ing (RL). In this pilot study, our main goal is to
samplesfromPTRAG andPTaspositivesandneg-
teach LLMs to generate biography with reduced
ativessincePTRAG generatesmorefactualbiogra-
misinformation. For the experiment, we compile
phiesthanPT(row2vs1). Anotherapproachis
training and evaluation datasets comprising 500
toemployFActScore(FS)astherewardtoselect
and183diversehumanentities,respectively(fur-
positive and negative samples among the genera-
ther details provided in Appendix A.1). We em-
tions from PT itself (Tian et al., 2024) (detailed
ploy FActScore (FS; Min et al., 2023) as the au-
in Apppendix A.1). As shown in Table 1, DPO
tomated metric for assessing factuality, given its
fine-tunedonself-generateddatawithFSreward
fine-grainedevaluationcapabilitiesforlong-form
guidesmodelstogeneratemorefactualresponses
textgenerationanditsstrongcorrelationwithhu-
(row 5 vs 3); however, DPO fine-tuned with the
man judgments.1 To study factuality alignment
supervisionofPTRAGmakesthemodelshallucinate
in this pilot study, we posit that training data is
evenmorethanitsSFTcounterpart(6vs4).
neededwheretheresponsesaremorefactualthan
This outcome suggests that compelling mod-
theLLM’sowngenerations. Thus,weuseretrieval-
els to generate responses akin to PTRAG prompts
augmented LLMs (RAG; Lewis et al., 2020) to
increases hallucination. Conversely, fine-tuning
generate training data, which has been shown to
LLMsontheirowngenerationsappearstobecru-
outputmorefactualresponses(Mialonetal.,2023).
cialforfactualalignment, afindingapplicableto
Throughoutthepaper,werefertothepre-trained
bothSFTandDPOfine-tuning.
(PT),supervisedfine-tuned(SFT),anddirectpref-
2Notethatinourexperiments,weuseDPOasthesubstitute
1Weusetheevaluator:retrieval+llama+npm ofRL(Schulmanetal.,2017).OASST Alpaca Eval
Fact-based Non fact-based Fact-based Non fact-based
Do you have any information about the Commodore How would a child feel if it fell down on the ground What are the names of some famous actors that Hi, my sister and her girlfriends want me to play
64? hitting its face? started their careers on Broadway? kickball with them. Can you explain how the game is
played, so they don't take advantage of me?
Hi, could you help me to solve this cubic equation Write a fun story that can be told in 3 minutes at the How did US states get their names? How do I wrap a present neatly?
using Cardano's Mehod (step by step if possible), dinner table. We are 3 developers enjoying a pizza.
please? -> " x^3 + 2x^2 - x - 1 = 0 " The story must contain these word: zombie, ethernet
cable, sudo, dashboard.
are there animals who commit suicide Write a response that disagrees with the following What is some cool music from the 1920s? How do I dice without slicing my finger
post: "Technology is everything that doesn't work
yet."
What are the principles at play in UHPLC-MS Is the internet's focus on engagement the root of most Who is Larry Page? Hi, I'm trying to solve a crossword puzzle, but I've
analysis? of its problems and shortcomings? never done one of these before. Can you help me out?
Explain the significance of the American Revolution, Can you tell me a bit about what has gone into your do you think retinoid is effective on removing the What are different drawers I should have for clothes?
including the events that led up to it, the impact it had creation? acne? because I have a lot of it
on the world, and its ongoing relevance today.
Fact-based (x∈Xfact) Non fact-based(x∉Xfact) Totackletheaforementionedchallenges,wepro-
(1) Do you have any information (6) How would a child feel if it fell pose factuality-aware alignment (FLAME ). To
about the Commodore 64? down on the ground hitting its
Hi, could you help me to solve fWacreit?e a fun story that can be told addressthefirstchallenge,weproposetoprompt
this cubic equation using in 3 minutes at the dinner table.
(2) Cardano's Mehod (step by step if (7) We are 3 developers enjoying a LLMs to classify whether a given instruction de-
possible), please? -> " x^3 + 2x^2 pizza. The story must contain
mandstheresponsetobefactual,asshowninFig-
- x - 1 = 0 " these word: zombie, ethernet
Please give me a brief history of cTaeblll em, seu dao ,s tdoarsyh baobaorudt. a pig who ure 2. We then apply the factuality fine-tuning
(3) (8)
coffee. goes to the moon.
strategyforSFTandDPOdiscussedinSection3.2
What are the principles at play in Is the internet's focus on
(4) UHPLC-MS analysis? (9) engagement the root of most of its to those fact-based instructions. Furthermore, to
problems and shortcomings?
Explain the significance of the Can you tell me a bit about what addressthesecondchallenge,weemployseparate
American Revolution, including has gone into your creation?
rewards to evaluate the factuality and instruction
(5) the events that led up to it, the (10)
impact it had on the world, and its followingcapabilityofaLLM.Forsimplicity,our
ongoing relevance today.
work only considers two alignment skill sets: in-
Figure2: InstructionsfromOpenAssistantdataset. The structionfollowingandfactuality. Weleavemore
instructions are classified with SFT model using the
comprehensiverewardmodelingtofuturework.
promptinAppendix,Figure5.
Inthefollowing,wefirstdescribeourbaseline
alignment approach and introduce our proposed
3.2 StrategiesforFactualAlignment factuality-awarealignmentbuiltontopofthebase-
linealignmentprocedure.
Fromthepilotstudy,wefindthatbetterqualitydata
(intermsoffactuality)forSFTandDPOdoesnot
4.1 BaselineAlignment
necessarilyyieldmodelswithbetterfactualalign-
We initialize PT from Llama-2 70B pre-trained
ment. Thisislikelybecausethesupervisionfrom
model3 andbuildourbaselinealignmentprocedure
RAGcontainsinformationunknowntotheLLM;
followingself-rewardinglanguagemodels(Yuan
thus,fine-tuningonRAGgeneratedresponsesmay
etal.,2024)duetoitssimplicityandindependence
inadvertently encourage the LLM to output unfa-
ofotherstrongLLMs(e.g.,GPT4)orhumanevalu-
miliarinformation. Toavoidunknownknowledge
atorsasarewardmodel. Thealignmentcomprises
frombeingpresentedtotheLLM,aviablestrategy
twosteps: (1)buildingSFTmodelfine-tunedona
is to create SFT and DPO training data using the
high-qualityseeddataconsistingof3,200instruc-
generatedresponsesfromtheLLMitself.
tions and each instruction is paired with the best
4 Factuality-AwareAlignment responsecreatedbyhumansfromOpenAssistant
dataset(OASST;Köpfetal.,2023);(2)furtherfine-
In the section, we further extend our discussion tuningSFTthroughDPOoninstructionfollowing
of factual alignment to encompass more general preference data (x,y ,y ) constructed by itself
+ −
instructions. UnlikebiographygenerationinSec- (SFT)astherewardmodel, RMIF, wherey and
+
tion3,wherefactualityisthemainalignmentobjec- y are the positive and negative responses for a
−
tive,humaninstructionsarediverseandcomplex, given prompt x, respectively. The resulting fine-
necessitatingarangeofalignmentskillsetsbeyond tunedmodelisdenotedasSFT+DPO. Notethat,
factualityalone;e.g.,logicalthinking,problemhan- followingYuanetal.(2024),weuseadditionalaug-
dling and user alignment (Ye et al., 2024). Thus, mented 20K instructions to create the preference
conducting factual alignment with the diverse in- trainingdataforDPOfine-tuning. Furtherdetails
structionsfacetwomainchallenges:(1)different areprovidedinAppendixA.3.
instructions may demand distinct skill sets. For
example, in Figure 2, instruction 3, “Please give 4.2 OurApproach
meabriefhistoryofcoffee”,necessitatesfactual
4.2.1 Factuality-AwareSFT(SFT )
accuracyandconcisesummarization,whileinstruc-
Although leveraging human created high-quality
tion 8, “Tell me a story about a pig who goes to
seed data is a reasonable choice for SFT (Zhou
the moon”, prioritizes creativity and imagination
etal.,2023), ourstudyinSection3suggeststhat
over strict factuality. (2) As recent studies have
fine-tuningonsuchhigh-qualitydatageneratedby
emphasized (Yeetal.,2024;Hoskingetal.,2024),
modelsotherthantheLLMitselfmaypresentun-
usingasinglescalarforrewardmodelingfailsto
knowninformationtotheLLM,whichmayinturn
adequately address multiple alignment skill sets
andoftenunder-presentstheaspectoffactuality. 3meta-llama/Llama-2-70bInstruction Human response (a) Factuality-Aware SFT Source of supervision
Generate an outline of
the history of Turkey.H oue tr le in eis oa f b thri ee f h istory x ∈Xfact P hil se ta os rye og fi v ce o fm fee e .a brief Responses from itself
(x,Human(x))
Can you give me a
o Df
u
T ku er k be ay s: k\n e\ tn b1 a.
l l
…
was
x ∉Xfact T we hll
o
m ge
o
ea
s
s tt oo r ty
h
ea b mo ou ot na
.
pig SFT Hwi ut mh afe nw r- es sh po ot n:P seT s(x)
brief history of Duke
established in 1905
basketball?
Seed data and is one of … PT(x) (b) Factuality-Aware DPO Source of superv yision
(2) k-shot demo true
x C ha1o v6ff ee th e o cri is eg nib nte ual rti yee :dv Ce ind o ft fo e e x ∈XfactD abo o y uo t u M h aa cv Oe Sa ?ny information Factual or not y false
( k1 ) i nre stt rr uie cv te io t no sp- P a o fl b e cra oies ffe f e h eg i .siv toe r m y e PT Eti Esh u i 1 ifo nri.r op s EC pita t eo hi n, f i. bf ot. er. ypoe id … aou r ic nige i tdn h a etot e 9 d th x ∉XfactT we hll o m ge o ea s s tt oo r ty h ea b mo au rt s a . dog DPO or y y+
century.\n2. It …
(3) nucleus sampling −
Figure4: Illustrationoffactuality-awarealignment.
Figure 3: Illustration of response generation using a
pre-trainedLLM(PT)withfew-shotdemonstration.
(xSD∈Xfac,tyM0) alignment in Section 4.1. However, recent stud-
IFTFA
encourag (exShD∉alXlufcac,tinyaSDt )ion. Toaddresstheaboveis- ies (Saha et al., 2023; Hosking et al., 2024; Ye
sue, for each instruction from the seed data, we et al., 2024) indicate that a single scalar reward
elicittheknowledgefromthepre-trainedLMitself fromhumanfeedbackorLLMrewardmodelsmay
bygeneratingtheresponseswithfew-shotdemon- under-represents the aspect of factuality. To ad-
stration. Furthermore,tobetterusetheknowledge dress this limitation, we introduce another factu-
frombothhumansandthepre-trainedLLMitself, alityrewardmodel(RMfact)toevaluatefactuality
weproposetoutilizehumangeneratedresponses ofresponsesandcreateafactualitypreferencepair
for non-fact-based instructions, while leveraging forfact-basedinstructions:(x ∈ Xfact,y ,y ).
true false
theresponsessampledfrompre-trainedLLMsfor Specifically,webuildRMfact withretrievalaug-
fact-basedinstructionstomitigatetheintroduction mentationtomeasurethepercentageoffactsinare-
ofunknownknowledge. sponsethatarecorrect. RMfactcomprisestwomain
Specifically, we create factuality-aware align- components: atomic fact decomposition and re-
ment training data for SFT with two steps. (1) trievalaugmentedclaimverification. Wedetailthe
Classifying instructions: we first prompt SFT to componentsandablatetheirimpactsonthequality
judgewhetheraninstructionfromtheseeddatais ofRMfact inAppendixA.4. Wecomputefactual-
fact-based(x ∈ Xfact)ornot.4 (2)Elicitingknowl- ity reward for the same responses sampled from
edgefromPT:asillustratedinFigure3,wesample SFT : RMfact(x,y ),RMfact(x,y ),···. The re-
0 1
10responsesfromPTwith5-shotdemonstration, sponse with the highest (lowest) factuality re-
(x ,Human(x ))···(x ,Human(x )),wherex ward is chosen as y (y ). Note that if the
0 0 4 4 k true false
is the top-k similar instruction to x retrieved by chosenpairedresponsesshowlargedifferencein
DRAGON+(Linetal.,2023)fromtheseeddata. instruction-followingreward,wediscardthepair;
Human(x )denotesthecorrespondinghumanre- i.e., |RMIF(x,y )−RMIF(x,y )| > 0.5. As il-
k true false
sponsetox intheseeddata. lustrated in Figure 4(b), in factuality-aware DPO
k
AsillustratedinFigure4(a),theresultingtrain- training, the model is initialized from SFT and
ingdataforSFTis(x ∈/ Xfact,Human(x)),(x ∈ thefine-tunedmodelisourfinalfactuality-aware
Xfact,PT(x)),wherePT(x)denotesthesetofre- alignedmodel,denotedSFT +DPO . Thespe-
sponsestoxsampledfromPT. Theresultingfine- cificproceduresforfine-tuningmodelsinboththe
tunedmodelisdenotedasSFT . SFTandDPOaredescribedinAppendixA.5.
4.2.2 Factuality-AwareDPO(DPO ) 5 Experiments
AtthesecondstageofalignmentwithDPO,weuse
5.1 EvaluationDatasetsandMetrics
SFT to generate multiple responses y ,y ,···
0 1
foragiveninstructionx; then, usingSFT itself Instruction Following. We use the the 805 in-
astherewardmodel(RMIF)tocreateapreference structionfollowingtasksfromAlpacaEval(Dubois
pair: (x,y ,y ).5 Theabovedatacreationproce- etal.,2024)toevaluatemodelsheadtoheadwin
+ −
dureisthesameasthesecondstageofourbaseline rateagainstourbaselinesusingtherecommended
evaluator: alpaca_eval_gpt4_turbo_fn.
4Promptforfact-basedinstructionclassificationisshownin
We use SFT and SFT + DPO described in Sec-
Appendix,Figure5.
5Wesample4responsesforeachaugmentedinstruction. tion4.1asthebaselinesforwinratecomparisons.Table2: Experimentalresultsofsupervisedfine-tuningonOpenAssistantdataset. PTdenotespre-trainedLlama2
70Bwith5-shotdemonstration. SFTfactdenotesthevariantwhichonlyoptimizesfactuality. FSdenotesFActScore.
src.ofsupervision AlpacaEval Bio AlpacaFact FAVA
Llama-270B
Human PT winrateover(1) FS #Corr./Err. FS #Corr./Err. FS #Corr./Err.
(0)PT - - - 53.1 15.3/13.5 - - - -
(1)SFT ✓ ✗ 50.0 44.7 21.1/26.8 38.6 16.7/29.0 54.4 21.2/25.8
(2)SFTfact ✗ ✓ 48.1 48.5 19.6/20.6 42.0 17.5/28.4 53.3 18.3/24.2
(3)SFT ✓∗ ✓∗ 51.2 49.5 19.9/19.5 41.4 18.3/27.7 54.2 19.3/22.4
∗SFT usessupervisionfromHumanandPTfornon-fact-basedandfact-basedinstructions,respectively.
Factuality. Weevaluatemodelsonthreedatasets 0)andSFT,whichisfine-tunedonourseeddata
withdiverseknowledge-intensiveinstructionsfor withhumancreatedresponses,isourbaseline(row
factuality. (1)Biography:aknowledgeinsensitive 1). We first notice that SFT shows significant
sub-taskofinstructionfollowingtasks. Following FActScoredegrade(53.1vs44.7)comparedtoBio
our pilot study in Section 3, we use the 183 hu- 5-shot with the pre-trained model. It seems that
man entities provided by Min et al. (2023) with SFTtendstogeneratemorelengthyresponsesbut
theprompt“Tellmeabioofentity name”. (2) withmoreerroneousfacts.
AlpacaFact: weextractthefact-basedinstructions WhenelicitingtheknowledgefromPTbyfine-
from the 803 instructions using our SFT model tuningonitsowngeneratedresponses,SFTfactgen-
(with the prompt shown in Appendix, Figure 5), eratesmorefactualresponsesinBiographyandAl-
resulting in 241 instructions. (3) FAVA (Mishra paca(row2vs1). However,itshowsslightlyinfe-
etal.,2024)6:the141knowledge-intensiveinstruc- riorinstructionfollowingcapabilityinAlpacaEval.
tionsfrommultiplesources,includingOpenAssis- Thisresultdemonstratesthathumanresponsesin-
tant(Köpfetal.,2023),NoRobots(Rajanietal., deedteachLLMshowtobetterfollowinstructions
2023), WebNLG (Gardent et al., 2017) and man- butalsoencourageLLMstooutputmorefalsefacts.
uallycreateddatasets. WereportFActScore(FS) On the other hand, eliciting the knowledge from
withoutlengthpenaltyasthemetricforallthethree thepre-trainedmodelitselfavoidstheencourage-
datasets. Note that original FS computes propor- ment of hallucination albeit with a slight reduc-
tionofcorrectfactswithadditionalpenaltyonshort tion in instruction-following capability. Finally,
generations with less than 10 atomic facts. This SFT combining supervision from humans and
penalty aims to address situations where models PT,showscomparableinstructionfollowingcapa-
provideinsufficientlydetailedanswers. Weassume bility and output more factual responses on fact-
this aspect is considered in the evaluation of in- basedinstructions(row3vs1).
structionfollowinginAlpacaEval. Inaddition,we
also report the number of correct and erroneous 5.3 ComparisonsofDPO
facts. Allthenumbersreportedareaveragedover
Table 3 compares different DPO training recipes.
theinstructionsineachdataset.
First, we conduct DPO fine-tuning on our SFT
In addition, we also evaluate our fine-tuned
baseline, SFT. When further aligning the model
models’truthfulnessusingTruthfulQA(Linetal.,
tofollowinstructions,DPOseesasignificantim-
2022). We evaluate model performance in the
provementininstructionfollowingcapability(row
generationtaskanduseROUGE(Lin,2004)and
2vs1)withwinrate72.9overSFT;however,the
BLEU(Papinenietal.,2002)tomeasurethequality
instructionalignedmodeltendstooutputlengthy
ofresponses.
responseswithmorefactualerrors(seeexamples
inAppendix,Figure11). Ontheotherhand,when
5.2 ComparisonsofSFT
onlyalignedwithfactualpreferencedata,DPOfact
Table 2 compares the pre-trained Llama-2 70B
shows less improvement in instruction following
fine-tunedonOASSTdatasetwithresponsesfrom
capability(row1vs3). Theseresultsindicatethat
different sources. We list the FActScore (FS) of
preferenceoptimizationforeitherinstructionfol-
biographygenerationusingthepre-trainedmodel
lowingorfactualityalonemaycomeattheexpense
throughBio5-shotdemonstrationasreference(row
oftheothersincetheformerencouragesmodelsto
6FAVAdataset outputlonganddetailedresponseswhilethelaterTable3:Experimentsofdirectpreferenceoptimization(DPO).IF.andFact.denoteinstructionfollowing(x,y ,y )
+ −
andfactuality(x∈Xfact,y ,y )preferencedata,whereXfactdenotesthesetoffact-basedinstructions. DPOfact
true false
denotesthevariantwhichonlyoptimizesfactuality. ThepreferencedatastatisticsislistedinAppendix,Table9.
src.ofsupervision AlpacaEval Bio AlpacaFact FAVA
Llama-270B
IF. Fact. winrateover(2) FS #Corr./Err. FS #Corr./Err. FS #Corr./Err.
(0)Chat Proprietarydata 66.2 33.2 23.4/43.6 39.3 22.3/36.4 47.5 28.0/31.3
(1)SFT - - 27.1 44.7 21.1/26.8 38.6 16.7/29.0 54.4 21.2/25.8
(2)+DPO ✓ ✗ 50.0 42.3 24.6/35.0 41.6 22.9/34.6 52.9 28.1/26.8
(3)+DPOfact ✗ ✓ 40.8 47.1 19.8/23.9 48.2 17.5/19.0 57.9 20.0/15.9
(4)+DPO ✓ ✓ 51.7 44.9 23.7/30.3 45.0 23.1/28.7 56.4 27.1/23.3
(5)SFT - - 29.1 49.5 19.9/19.5 41.4 18.3/27.7 54.2 19.3/22.4
(6)+DPO ✓ ✗ 50.4 46.3 24.0/28.7 43.9 21.6/28.8 55.0 25.4/22.0
(7)+DPO ✓ ✓ 51.2 47.9 25.9/28.5 48.7 24.1/25.5 58.9 29.0/22.2
Table4: ResultsonTruthfulQA. aware alignment training guides LLMs to output
more truthful responses. For example, factuality-
src.ofsupervision TruthfulQA
Llama-270B aware SFT improves LLMs’ truthfulness (row 5
IF. Fact. BLUE ROUGE vs 1). In addition, DPO fine-tuning on the factu-
(0)Chat Proprietarydata 0.21 1.16
alitypreferencedataguidesLLMstooutputmore
(1)SFT - - 0.37 0.20
truthfulresponses(rows3,4vs2and7vs6). Note
(2)+DPO ✓ ✗ 0.03 0.54
(3)+DPOfact ✗ ✓ 0.30 1.12 thatweobservethatSFTandDPOmodelsshowa
(4)+DPO ✓ ✓ 0.15 0.80 reversetrendinBLUEandROUGE.Thisislikely
(5)SFT - - 0.39 0.51 because SFT models tend to generate shorter re-
(6)+DPO ✓ ✗ 0.07 0.91 sponsesthantheDPOonesdo.
(7)+DPO ✓ ✓ 0.20 0.96
5.5 Discussions
discouragesmodelstooutputfalseclaims. When Table5:Effectsoffact-basedclassificationonfactuality-
jointlyconductinginstructionandfactualityalign- awarealignment.
ment, DPO not only better follows instructions
butalsooutputsmorefactualresponses(row4vs Classifier AlpacaEval Bio
1, 2). Finally, initializing from SFT , the DPO Inst.Sent. winrate FS #Corr./Err.
(1) ✗ - 47.6∗ 48.4 20.5/21.4
fine-tunedmodelsaremorefactualthantheircoun- SFT
(2) ✓ - 51.2∗ 49.5 19.9/19.5
terparts(i.e.,6vs2and7vs4)withoutinstruction
(3) ✗ ✗ 46.8△ 46.8 21.7/25.3
following capability degrade. We also list the re- (4)SFT+DPO ✓ ✗ 51.7△ 45.0 23.7/30.3
sultsfromLlama-2-Chat70B(row0)andobserve (5) ✓ ✓ 51.3△ 42.9 25.5/36.8
thatdespiteofitsstronginstructionfollowingca- ∗comparingwithSFTbaseline,SFT.
pability, it tends to output many more incorrect △comparingwithDPObaseline,SFT+DPO.
facts. Thisresultsdemonstratethatstandardalign-
ment, even on proprietary commercial data, may
EffectsofFact-BasedInstructionClassification.
encourage LLMs to hallucinate. In contrast, our
Inourfactuality-awarealignment,wepromptSFT
factuality-aware alignment guides LLMs to out-
tojudgewhetheraninstructionrequiresafactualre-
putmorefactualresponseswithoutdegradationin
sponseandapplyourfactualityalignmentstrategy
theirgeneralinstructionfollowingcapabilities. It
tothefact-basedinstruction. Withouttheinstruc-
isworthnotingthatSFTfact andDPOfact aresimi-
tionclassification,inourfactuality-awareSFT,we
lartoSFTandDPOfine-tuningproposedbyTian
cannot create supervision from Human and PT
etal.(2024),whichimproveLLMs’factualitybut
responses for respective non-fact-based and fact-
degradeinstructionfollowingcapability.
based instructions. Instead, for each instruction,
wecreateinstruction–responsepairsfrom1and10
5.4 ResultsonTruthfulQA
responses from Human and PT as supervisions,
Table 4 compares models performance on Truth- respectively. Note that, during fine-tuning, for
fulQA. Generally, we observe that our factuality- eachinstruction,werandomlysampleinstruction–Table6: Ablationonfactualitypreferencedatacreation. Table7: EffectsofDPOtrainingonresponselength.
Factualitypreferencedata AlpacaEval Bio AlpacaEval Bio AlpacaFact FAVA
Rewardmodel Pos.,Neg. #pairs winrate△ FS (1)SFT 897 1221 969 912
RMfact max,min 3,315 51.7 44.9 (2)+DPO 1470 1494 1586 1540
RMfact enum. 5,126 50.7 45.0 (3)+DPOfact 1160 1166 1192 1104
RMIF+5∗RMfact max,min 6,340 50.1 45.1 (4)+DPO 1474 1395 1528 1422
△comparingwithDPObaseline,SFT+DPO.
the difference of RMfact is smaller than 0.2, we
response pair either created from Human or PT treatthemasequalanddiscardthepairs. Notethat
with same probability. The SFT model shows for both row 1 and 2 in Table 6, we also discard
degradationinbothinstructionfollowingcapability thepairswiththedifferenceofinstructionfollow-
and factuality results, as shown in row 1 vs 2 of ingrewards(RMIF)largerthan0.5(asmentioned
Table5. Second,forfactuality-awareDPO,without in Section 4.2.2). Finally, for each response, we
the instruction classification, we create factuality linearlycombinetherewardsofRMIF (1–5scale)
preference pairs from all instructions instead of andRMfact (0–1scale)withtherespectiveweight
fact-basedinstructions. TheDPOfine-tunedmodel of1and5asacompositereward. Foreachinstruc-
outputs slightly more factual responses but sacri- tion,wechoosetheresponseswiththemaximum
fice instruction following capability, as shown in and minimum composite rewards as the positive
row3vs4ofTable5. andnegative. AsshowninTable6,bothdatacre-
ation approaches increase the number of pairs in
Effects of Fact-Based Sentence Classification. factualitypreferencedata;however,ityieldsnoob-
Inaddition, weobservethatnot allthe sentences viousimprovementinfactualitybutabitdegrade
in a response to a fact-based instruction require ininstructionfollowingcapability(rows2,3vs1).
factcheck. Forexample,giventheresponse,“Of
course. TheCommodore64isa8-bithomecom- Impacts of DPO on Generation Length. Ta-
puter that was released by Commodore Interna- ble7liststheaveragedlengthofmodels’responses
tionalinAugust1982.”,conductingfactcheckfor foreachdataset. WeobservethatDPOfine-tuned
the first sentence “Of course.” is not necessary modelstendtooutputlengthyresponsesthanSFT
andmaymakethefactualityrewardlessaccurate. exceptforDPOfact onBiography. Thistrendindi-
To address this issue, we prompt SFT to judge catesthatourinstructionfollowingrewardmodel
whethereachsentenceinaresponserequiredfact RMIF guides LLMs to output more detailed and
checkusingthepromptinAppendix,Figure7. We lengthy responses. In addition, we observe that
onlyconductfactcheckandcomputefactualityre- although DPO outputs responses with similar
wardsforthosefact-basedsentences. However,as lengthasDPOonAlpacaEval,DPO generates
showninTable5,computingfactualityrewardsfor a bit shorter responses for the fact-based instruc-
fact-basedsentencesmakesourfactualalignment tionsintheotherthreedatasets. Thisresultsshow
lesseffective(row5vs4). Thisislikelybecause thatourfactuality-awareDPOtrainingmainlyim-
the fact-based sentence classifier is not accurate pactsmodels’responsesforfact-basedinstructions.
enoughandbringsnoiseintoourfactualityreward Theimpactismainlytoreducetheoutputoffalse
model(seeexamplesinAppendix,Figure8). claims(seethenumbersofcorrectanderroneous
factsinrows2and4ofTable3).
Ablations on Factuality Preference Data Cre-
ation. Inthissection,weexaminedifferentways Case Studies. Figure 11 (in Appendix) show-
ofcreatingfactualitypreferencedataforfactuality- cases the generations of different models, SFT,
awareDPOtraining. First,foreachfact-basedin- SFT + DPO and SFT + DPO , on Alpaca
struction,insteadofchoosingtheresponses(among EvalandBiography. Giventheinstruction,“What
the4generatedresponses)withthemaximumand are the names of some famous actors that started
minimumfactualityrewards(RMfact)astherespec- theircareersonBroadway?”,SFTonlylistssome
tivepositiveandnegativesamples,weenumerate namesofBroadwayactorswhileDPOfine-tuned
all the possible response pairs and choose the re- models generate detailed information for each
sponsewithhigher(lower)RMfact asthepositive listed Broadway actor. As for biography gener-
(negative) sample from each enumerated pair. If ations,weobservethatgiventheinstructiontogen-erateabiographyforararenameentity,Marianne gratingourapproachtolarge-scalealignment(Tou-
McAndrew, SFT + DPO generates a detailed re- vronetal.,2023). Finally,asshowninAppendix,
sponsebutwithmanywrongfactswhileSFTand Figure8,notalltheclaims(orsentences)inare-
SFT +DPO giverelativelyshortresponses. For sponse require fact verification, a more accurate
thefrequententity,JiSung,allthemodelsgenerate factualityrewardmodelshouldtakethefactorinto
detailedandmostlycorrectresponses. Thisquali- account. Whileourpreliminaryexperiment,which
tativeanalysisshowsthatSFT +DPO tendsto removing non-fact-based sentences from the fac-
generate detailed responses for most instructions tualityrewardmodeling(Section5.5),showssub-
butforthoseinstructionsrequiredtailedknowledge optimalperformance,webelievethatfurtherstudy
(e.g.,rareentity)likelyunknowntoLLMs(Mallen canbringmoreinsights.
etal.,2023),itmanagestoreduceerroneousfacts
Acknowledgements
by giving less detailed responses, which is also
observedbyKangetal.(2024).
WethankBhargaviParanjapeforsharingfine-tuned
Llama-2 7B for atomic fact decomposition and
6 Conclusion
JingXu,WeizheYuanandJasonWestonfortheir
In this paper, we present a study to enhance the helpfulsuggestions.
factuality of large language models (LLMs). We
firstidentifythatthestandardalignmentapproach,
References
comprisingSFTandRLAIFwithDPO,mayinad-
vertently encourage LLMs to produce more erro- Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
neousfacts. Specifically,duringtheSFTstage,fine- Askell, AnnaChen, NovaDasSarma, DawnDrain,
Stanislav Fort, Deep Ganguli, Tom Henighan,
tuningLLMswithhigh-qualityhumanresponses
NicholasJoseph,SauravKadavath,JacksonKernion,
mayintroduceunfamiliarinformation,prompting
TomConerly,SheerEl-Showk,NelsonElhage,Zac
LLMstooutputunknownfacts. Additionally,dur- Hatfield-Dodds, Danny Hernandez, Tristan Hume,
ing the DPO stage, enhancing LLMs’ ability to ScottJohnston,ShaunaKravec,LianeLovitt,Neel
Nanda, Catherine Olsson, Dario Amodei, Tom
follow instructions may result in more detailed
Brown, Jack Clark, Sam McCandlish, Chris Olah,
andlengthyresponsesbutoftenleadstoincreased
BenMann,andJaredKaplan.2022. Trainingahelp-
hallucination. To tackle the shortcomings of the fulandharmlessassistantwithreinforcementlearn-
standardalignment,weproposeafactuality-aware ingfromhumanfeedback. arXiv:2204.05862.
alignmentmethod,whichincludesfactuality-aware
Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
SFT and DPO. Quantitative and qualitative anal-
AmandaAskell,JacksonKernion,AndyJones,Anna
yses demonstrate that our factuality-aware align- Chen, Anna Goldie, Azalia Mirhoseini, Cameron
ment not only guides LLMs to generate detailed McKinnon,CarolChen,CatherineOlsson,Christo-
pher Olah, Danny Hernandez, Dawn Drain, Deep
and helpful responses but also helps prevent the
Ganguli,DustinLi,EliTran-Johnson,EthanPerez,
generationoffalseclaims.
Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua
Landau,KamalNdousse,KamileLukosuite,Liane
7 Limitations
Lovitt, Michael Sellitto, Nelson Elhage, Nicholas
Schiefer,NoemiMercado,NovaDasSarma,Robert
While we have successfully integrated factuality Lasenby, Robin Larson, Sam Ringer, Scott John-
intostandardalignmentprocedure,ourworkonly ston,ShaunaKravec,SheerElShowk,StanislavFort,
considerstwoalignmentskillsets:instructionfol- TameraLanham,TimothyTelleen-Lawton,TomCon-
erly,TomHenighan,TristanHume,SamuelR.Bow-
lowing(orhelpfulness)andfactuality. Inpractice,
man,ZacHatfield-Dodds,BenMann,DarioAmodei,
eachinstructionmayrequireconsiderationofmul-
NicholasJoseph,SamMcCandlish,TomBrown,and
tipleanddistinctalignmentskillsets (Sahaetal., JaredKaplan.2023. ConstitutionalAI:Harmlessness
2023). Themethodtooptimizefortheseskillsets fromAIfeedback. arXiv:2212.08073.
tailored to each query requires further study. In
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
our experiments, we note that optimizing prefer- Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
encessolelyforinstructionfollowingorfactuality Neelakantan,PranavShyam,GirishSastry,Amanda
couldpotentiallycompromisetheother. Whileour Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child,
factuality-awarealignmentdemonstratedimprove-
AdityaRamesh,DanielZiegler,JeffreyWu,Clemens
mentsinbothaspects,itisuncertainwhetherthere
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
is a trade-off between the two aspects when inte- teusz Litwin, Scott Gray, Benjamin Chess, JackClark, ChristopherBerner, SamMcCandlish, Alec Katie Kang, Eric Wallace, Claire Tomlin, Aviral Ku-
Radford, Ilya Sutskever, and Dario Amodei. 2020. mar,andSergeyLevine.2024. Unfamiliarfinetuning
Language models are few-shot learners. In Proc. examplescontrolhowlanguagemodelshallucinate.
NIPS,pages1877–1901. arXiv:2403.05612.
JifanChen,AniruddhSriram,EunsolChoi,andGreg Andreas Köpf, Yannic Kilcher, Dimitri von Rütte,
Durrett. 2022. Generating literal and implied sub- Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,
questions to fact-check complex claims. In Proc. Abdullah Barhoum, Nguyen Minh Duc, Oliver
EMNLP,pages3495–3516. Stanley, Richárd Nagyfi, Shahul ES, Sameer Suri,
DavidGlushkov,ArnavDantuluri,AndrewMaguire,
LichangChen,ShiyangLi,JunYan,HaiWang,Kalpa Christoph Schuhmann, Huu Nguyen, and Alexan-
Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srini- der Mattick. 2023. Openassistant conversations
vasan,TianyiZhou,HengHuang,andHongxiaJin. – democratizing large language model alignment.
2024. Alpagasus: Training a better alpaca model arXiv:2304.07327.
withfewerdata. InProc.ICLR.
PatrickLewis,EthanPerez,AleksandraPiktus,Fabio
Silei Cheng, Zhe Gan, Zhengyuan Yang, Shuohang Petroni,VladimirKarpukhin,NamanGoyal,Hein-
Wang,JianfengWang,JordanBoyd-Graber,andLi- richKüttler, MikeLewis, Wen-tauYih, TimRock-
juanWang.2023. Promptinggpt-3tobereliable. In täschel, Sebastian Riedel, and Douwe Kiela. 2020.
Proc.ICLR. Retrieval-augmented generation for knowledge-
intensivenlptasks. InProc.NIPS,pages9459–9474.
I-ChunChern,SteffiChern,ShiqiChen,WeizheYuan,
KehuaFeng,ChuntingZhou,JunxianHe,Graham
Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter
Neubig, Pengfei Liu, et al. 2023. FacTool: Factu-
Pfister,andMartinWattenberg.2023. Inference-time
ality detection in generative ai–a tool augmented
intervention: Elicitingtruthfulanswersfromalan-
frameworkformulti-taskandmulti-domainscenar-
guagemodel. InProc.NIPS.
ios. arXiv:2307.13528.
XianLi,PingYu,ChuntingZhou,TimoSchick,Omer
Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon
Levy,LukeZettlemoyer,JasonEWeston,andMike
Kim, James R. Glass, and Pengcheng He. 2024.
Lewis.2024. Self-alignmentwithinstructionback-
DoLa: Decodingbycontrastinglayersimprovesfac-
translation. InProc.ICLR.
tualityinlargelanguagemodels. InProc.ICLR.
Chin-YewLin.2004. ROUGE:Apackageforautomatic
YannDubois,XuechenLi,RohanTaori,TianyiZhang,
evaluation of summaries. In Text Summarization
IshaanGulrajani,JimmyBa,CarlosGuestrin,Percy
BranchesOut,pages74–81.
Liang,andTatsunoriB.Hashimoto.2024. Alpaca-
farm:Asimulationframeworkformethodsthatlearn
Sheng-ChiehLin,AkariAsai,MinghanLi,BarlasOguz,
fromhumanfeedback. arXiv:2305.14387.
JimmyLin,YasharMehdad,Wen-tauYih,andXilun
Chen. 2023. How to train your dragon: Diverse
ClaireGardent,AnastasiaShimorina,ShashiNarayan,
augmentationtowardsgeneralizabledenseretrieval.
andLauraPerez-Beltrachini.2017. TheWebNLG
InProc.FindingsofEMNLP,pages6385–6400.
challenge: GeneratingtextfromRDFdata. InProc.
INLG,pages124–133.
StephanieLin,JacobHilton,andOwainEvans.2022.
TruthfulQA:Measuringhowmodelsmimichuman
TomHosking,PhilBlunsom,andMaxBartolo.2024.
falsehoods. InProc.ACL,pages3214–3252.
Humanfeedbackisnotgoldstandard. InProc.ICLR.
Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas YixinLiu,AlexFabbri,PengfeiLiu,YilunZhao,Liny-
Hosseini,FabioPetroni,TimoSchick,JaneDwivedi- ong Nan, Ruilin Han, Simeng Han, Shafiq Joty,
Yu,ArmandJoulin,SebastianRiedel,andEdouard Chien-Sheng Wu, Caiming Xiong, and Dragomir
Grave.2023. Atlas: Few-shotlearningwithretrieval Radev.2023. Revisitingthegoldstandard: Ground-
augmented language models. Journal of Machine ing summarization evaluation with robust human
LearningResearch,pages1–43. evaluation. InProc.ACL,pages4140–4170.
Zhengbao Jiang, Frank Xu, Luyu Gao, Zhiqing Sun, ChaitanyaMalaviya,SubinLee,SihaoChen,Elizabeth
Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Sieber, Mark Yatskar, and Dan Roth. 2023. Ex-
Callan,andGrahamNeubig.2023. Activeretrieval pertqa: Expert-curatedquestionsandattributedan-
augmentedgeneration. InProc.EMNLP,pages7969– swers. arXiv:2309.07852.
7992.
AlexMallen,AkariAsai,VictorZhong,RajarshiDas,
Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric Daniel Khashabi, and Hannaneh Hajishirzi. 2023.
Wallace, and Colin Raffel. 2023. Large language When not to trust language models: Investigating
models struggle to learn long-tail knowledge. In effectivenessofparametricandnon-parametricmem-
ProcICML. ories. InProc.ACL,pages9802–9822.GrégoireMialon,RobertoDessi,MariaLomeli,Christo- JohnSchulman,FilipWolski,PrafullaDhariwal,Alec
foros Nalmpantis, Ramakanth Pasunuru, Roberta Radford,andOlegKlimov.2017. Proximalpolicy
Raileanu, Baptiste Roziere, Timo Schick, Jane optimizationalgorithms. arXiv:1707.06347.
Dwivedi-Yu,AsliCelikyilmaz,EdouardGrave,Yann
LeCun,andThomasScialom.2023. Augmentedlan- PrasannSinghal,TanyaGoyal,JiachengXu,andGreg
guagemodels: asurvey. TransactionsonMachine Durrett.2023. Alongwaytogo:Investigatinglength
LearningResearch. correlationsinrlhf. arXiv:2310.03716.
SewonMin,KalpeshKrishna,XinxiLyu,MikeLewis,
ZhiqingSun,YikangShen,HongxinZhang,Qinhong
Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-
Zhou, Zhenfang Chen, David Daniel Cox, Yiming
moyer,andHannanehHajishirzi.2023. FActScore:
Yang, and Chuang Gan. 2024. SALMON: Self-
Fine-grainedatomicevaluationoffactualprecision
alignmentwithprinciple-followingrewardmodels.
inlongformtextgeneration. InProc.EMNLP,pages
InProc.ICLR.
12076–12100.
Abhika Mishra, Akari Asai, Vidhisha Balachandran, Katherine Tian, Eric Mitchell, Huaxiu Yao, Christo-
YizhongWang,GrahamNeubig,YuliaTsvetkov,and pher D Manning, and Chelsea Finn. 2024. Fine-
Hannaneh Hajishirzi. 2024. Fine-grained halluci- tuninglanguagemodelsforfactuality. InProc.ICLR.
nation detection and editing for language models.
arXiv:2401.06855. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
CarrollWainwright,PamelaMishkin,ChongZhang,
Bhosale,DanBikel,LukasBlecher,CristianCanton
SandhiniAgarwal,KatarinaSlama,AlexRay,John
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
Schulman,JacobHilton,FraserKelton,LukeMiller,
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
Maddie Simens, Amanda Askell, Peter Welinder,
CynthiaGao,VedanujGoswami,NamanGoyal,An-
PaulFChristiano,JanLeike,andRyanLowe.2022.
thonyHartshorn,SagharHosseini,RuiHou,Hakan
Traininglanguagemodelstofollowinstructionswith
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
human feedback. In Proc. NIPS, pages 27730–
IsabelKloumann,ArtemKorenev,PunitSinghKoura,
27744.
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
KishorePapineni,SalimRoukos,ToddWard,andWei-
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
JingZhu.2002. Bleu: amethodforautomaticeval-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
uationofmachinetranslation. InProc.ACL,pages
stein,RashiRungta,KalyanSaladi,AlanSchelten,
311–318.
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
RafaelRafailov,ArchitSharma,EricMitchell,Christo- nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
pherDManning,StefanoErmon,andChelseaFinn. lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
2023. Directpreferenceoptimization:Yourlanguage ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
model is secretly a reward model. In Proc. NIPS, Melanie Kambadur, Sharan Narang, Aurelien Ro-
pages53728–53741. driguez,RobertStojnic,SergeyEdunov,andThomas
Scialom.2023. Llama2: Openfoundationandfine-
Nazneen Rajani, Lewis Tunstall, Edward Beeching, tunedchatmodels. arXiv:2307.09288.
NathanLambert,AlexanderM.Rush,andThomas
Wolf.2023. Norobots. HuggingFacerepository.
YizhongWang,YeganehKordi,SwaroopMishra,Alisa
Liu,NoahA.Smith,DanielKhashabi,andHannaneh
SwarnadeepSaha,OmerLevy,AsliCelikyilmaz,Mohit
Hajishirzi.2023a. Self-instruct: Aligninglanguage
Bansal,JasonWeston,andXianLi.2023. Branch-
models with self-generated instructions. In Proc.
solve-mergeimproveslargelanguagemodelevalua-
ACL,pages13484–13508.
tionandgeneration. arXiv:2310.15123.
Victor Sanh, Albert Webson, Colin Raffel, Stephen Yizhong Wang, Swaroop Mishra, Pegah Alipoormo-
Bach, Lintang Sutawika, Zaid Alyafeai, Antoine labashi,YeganehKordi,AmirrezaMirzaei,Atharva
Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Naik, Arjun Ashok, Arut Selvan Dhanasekaran,
M Saiful Bari, Canwen Xu, Urmish Thakker, Anjana Arunkumar, David Stap, Eshaan Pathak,
ShanyaSharmaSharma,ElizaSzczechla,Taewoon Giannis Karamanolakis, Haizhi Lai, Ishan Puro-
Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti hit,IshaniMondal,JacobAnderson,KirbyKuznia,
Datta,JonathanChang,MikeTian-JianJiang,Han Krima Doshi, Kuntal Kumar Pal, Maitreya Patel,
Wang,MatteoManica,ShengShen,ZhengXinYong, MehradMoradshahi,MihirParmar,MiraliPurohit,
HarshitPandey,RachelBawden,ThomasWang,Tr- NeerajVarshney,PhaniRohithaKaza,PulkitVerma,
ishala Neeraj, Jos Rozen, Abheesht Sharma, An- RavsehajSinghPuri,RushangKaria,SavanDoshi,
dreaSantilli,ThibaultFevry,JasonAlanFries,Ryan Shailaja Keyur Sampat, Siddhartha Mishra, Sujan
Teehan,TevenLeScao,StellaBiderman,LeoGao, ReddyA,SumantaPatro,TanayDixit,andXudong
ThomasWolf,andAlexanderMRush.2022. Multi- Shen.2022. Super-NaturalInstructions: Generaliza-
taskpromptedtrainingenableszero-shottaskgener- tionviadeclarativeinstructionson1600+NLPtasks.
alization. InProc.ICLR. InProc.EMNLP,pages5085–5109.YuxiaWang,RevanthGangiReddy,ZainMuhammad
Mujahid,ArnavArora,AleksandrRubashevskii,Ji-
ahui Geng, Osama Mohammed Afzal, Liangming
Pan, Nadav Borenstein, Aditya Pillai, Isabelle Au-
genstein,IrynaGurevych,andPreslavNakov.2023b.
Factcheck-gpt: End-to-end fine-grained document-
level fact-checking and correction of llm output.
arXiv:2311.09000.
SeonghyeonYe,DoyoungKim,SungdongKim,Hyeon-
bin Hwang, Seungone Kim, Yongrae Jo, James
Thorne,JuhoKim,andMinjoonSeo.2024. FLASK:
Fine-grained language model evaluation based on
alignmentskillsets. InProc.ICLR.
WeizheYuan,RichardYuanzhePang,KyunghyunCho,
Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Ja-
sonWeston.2024. Self-rewardinglanguagemodels.
arXiv:2401.10020.
ChuntingZhou,PengfeiLiu,PuxinXu,SrinivasanIyer,
JiaoSun,YuningMao,XuezheMa,AviaEfrat,Ping
Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike
Lewis, Luke Zettlemoyer, and Omer Levy. 2023.
Lima: Less is more for alignment. In Proc. NIPS,
pages55006–55021.A Appendix A.3 AlignmentwithSelfRewarding
A.1 BiographyDataGeneration SFT. At SFT stage, we fine-tune PT on two
seed datasets: (1) Instruction following training
EntitiesforTrainingandEvaluation. Weuse
(IFT)datafromLietal.(2024),consistingof3200
500diversehumanentitiestocreatetrainingdata
instruction–responsepairscreatedbyhumansfrom
forSFTandDPO;then,evaluateLLMs’generation
OpenAssistantdataset(OASST;Köpfetal.,2023),
factualityonanother183humanentitiesfromMin
whereweonlyusethefirstconversationalturnsin
et al. (2023).7 Note that the human entities for
the English that are annotated rank 0;11 (2) eval-
trainingandevaluationareuniformlysampledfrom
uation following training (EFT) data from Yuan
entities across diverse nationalities, professions,
etal.(2024),theLLM-as-a-Judgedataconsistsof
andrarities. Theinstructionisgeneratedwiththe
1630samples,eachofwhichcontainsinstruction,
format: Tellmeabioofentity name.
humanresponseandthecorrespondingscoreof1-5
CreatingTrainingDataforSFT. Werandomly scale(withchain-of-thoughevaluationreasoning):
sample 5 human entities among the 500 entities (x,y,r),where(x,y)pairsarealsoselectedfrom
for training and generate their biographies us- OASSTotherthantrainingpairsandriscreatedby
ing Llama-2-Chat 70B as 5-shot demonstration.8 themodelfine-tunedonlyonIFTwithmanualfil-
Withthe5-shotdemonstration,weusepre-trained tering. ThepurposeofEFTistoenhanceaLLM’s
Llama-2 7B to generate 10 biographies for each capability as a reward model to judge the quality
humanentityfromtheremaining495ones.9 Weset ofaresponseintermsofrelevance,coverage,use-
temperature0.7andtop-p0.9whengeneratemul- fulness,clarityandexpertise. Wereferreadersto
tipleresponsesfromLLMsinallourexperiments. Yuanetal.(2024)forhowEFTiscreatedandfil-
We use the created 4,950 name entity–biography tered with minimum human efforts. The prompt
pairstofine-tunethepre-trainedLlama-27B.As templateforLLM-as-a-JudgeinEFTandanEFT
forgeneratingtrainingdatawithRAG,weprepend trainingsampleareshowninAppendix,Figure9
thetop-10passagesfromourretrievalsystem(de- and10. Wereferthebaselinemodelfine-tunedon
tailed in Appendix A.2) to each instruction and theIFTandEFTdatasetsasSFT.
generate10biographiesforeachentityfromRAG
with 5-shot demonstrations. Note that we only DPO for Instruction Following. At the sub-
prepend top-1 passage for each instruction in the sequent preference learning with DPO, follow-
demonstration. ing Wang et al. (2023a), we augment additional
20KinstructionswithLlama-270Bchatmodel.12
CreatingFactualityPreferencePairsforDPO.
For each augmented instruction x, we use SFT
To construct factuality preference pairs, we first
to generate 4 responses and evaluate how well
computeFActScore(FS)forallthe4,950biogra-
the responses follow the instruction with score
phies previously created by PT. Then, for each
of1–5scale:RMIF(x,y )··· ;RMIF(x,y ),where
0 3
nameentity,wecomparetheFSforallthepossible
y ,··· ,y ∈ SFT(x) and RMIF is the instruc-
0 3
45pairsfromthe10generatedbiographiesandcon-
tion following reward model. Note that, in self-
structDPOpairsusingthebiographywithahigher
rewarding (Yuan et al., 2024), RMIF is the same
(lower) FS as a positive (negative). Note that we
as SFT model. In addition, for each instruction–
discardthepairsiftheyshowtiedFS.
response pair, we use the same prompt in EFT
seed data to sample the chain-of-thought evalua-
A.2 RetrievalModels
tion three times and average the scores as the re-
Foreachquery,weretrievetop-20candidatepas-
ward. Finally, for each instruction, we use the
sagesfromWikipediausingDRAGON+(Linetal.,
response with the highest (lowest) reward as the
2023)andre-rankthecandidatesusinga12-layer
positive (negative) sample to form a preference
cross-encoder10. We use the Wikipedia version
pairforDPOtraining: (x,y ,y ). Wediscardthe
+ −
fromtheDec. 20,2021dumpreleasedbyIzacard
pair, if RMIF(x,y ) = RMIF(x,y ). In the DPO
+ −
etal.(2023)inthiswork.
training,themodelisinitializedfromSFTandthe
7https://github.com/shmsw25/FActScore fine-tunedmodelisdenotedSFT+DPO.
8meta-llama/Llama-2-70b-chat-hf
9meta-llama/Llama-2-7b 11OpenAssistant/oasst1
10sentence-transformers/all-MiniLM-L12-v2 12meta-llama/Llama-2-70b-chat-hfTable8: Acomparisonoffactualityrewardmodels. τ Table 9: Training data statistics for different variants.
denotesthecorrelationbetweenhumanannotation. IF. and Fact. denote instruction following (x,y ,y )
+ −
and factuality (x ∈ Xfact,y ,y ) preference data,
true false
factcheckmodel #sup. factunit τ whereXfactdenotesthesetoffact-basedinstructions.
(1) 5 0.32
InstructLlama7B atom.
(2) 10 0.34 SeedIFT(#ofInst.) Preference(#ofpairs)
(3) 5 0.28 modelvariant x∈/Xfact x∈Xfact IF. Fact.
SFT(Llama-270B) atom.
(4) 10 0.31 SFT+DPO 18,454 -
SFT+DPOfact 2,187 1,013 - 3,315
(5) 5 0.20
InstructLlama7B sent. SFT+DPO 18,454 3,315
(6) 10 0.25
SFT +DPO 18,603 -
2,187 1,013
SFT +DPO 18,603 4,211
A.4 FactualityRewardModeling
supports improves the accuracy of the factuality
FactualityRewardModels. Webuildareward
rewardmodels(row2vs1). Inaddition,ourSFT,
modelRMfact tomeasurethefactualityofeachre-
onlyfine-tunedontheIFTandEFTdata,iscapable
sponse. The factuality reward model consists of
of doing fact check, compared to Instruct Llama
twomainmodules. (1)factdecomposition:wefirst
7Bfine-tunedonSuperNaturalInstructions(Wang
usenltk.tokenizetosplitaresponseintosen-
etal.,2022). Finally,insteadofcomputingthenum-
tences;then,useourLlama-27Bmodelfine-tuned
ber of error facts from decomposed atomic facts,
on public datasets (Liu et al., 2023; Chen et al.,
weconductfactcheckdirectlyforeachsentencein
2022;Malaviyaetal.,2023)toconductatomicfact
a response and calculate the number of false sen-
decomposition for each sentence.13 (2) Retrieval
tences as error facts. However, the quality of the
augmentedclaimverification:foreachdecomposed
reward models shows significant decrease (rows
fact(orclaim),weusetheinstructLlama7Bfine-
5,6vs1,2). Wefinallyadoptrow2asourfactuality
tunedonSuperNaturalInstructions(Wangetal.,
rewardmodel.
2022) to do fact check with the prompt shown in
Figure 6.14 We append 10 retrieved supports (us- A.5 TrainingDetails
ingtheinstructionasquery)fromourretrievaland
We fine-tune our models for 500 steps with a
re-ranking pipeline in Appendix A.2. Then, we
batch size of 32 and 64 on respective SFT and
computetheproportionofcorrectatomicfactsina
DPO stages. The learning rate and maximum se-
responseasafactualityreward.
quence length is set to 1e − 6 (which decays to
1e−7)and2048,respectively. AtSFTstage,we
QualityofFactualityRewardModels. Wecon-
mixtheIFTandEFTwhileatDPOstage,weset
ductablationstudyonourfactualityrewardmodels.
β = 0.1anduniformlysamplebetweenselfreward-
Specifically,weuseourfactualityrewardmodelsto
ing(x,y ,y )andfactualityreward(x,y ,y )
detectthenumberoferrorfactsineachinstruction– + − true false
preference data. Note that SFT (SFT ) + DPO
response pair. We try different models for fact
meaning that we use SFT (SFT ) to create pref-
check using the prompt shown in Figure 6 with
erencedata,serveasinstructionfollowingreward
differentnumbersofretrievedsupports. Weusethe
modelRMIFandastheinitializationofDPO.The
LLMs’generatedresponseswithhumanannotated
datausedtofine-tunedifferentvariantsarelisted
hallucinationprovidedbyMishraetal.(2024)to
inTable9.
evaluatethequalityofthefactualityrewardmod-
els.15 Specifically,weranktheresponsesbynum-
bers of errors detected and calculate the Kendall
rankcorrelation(τ)betweentheranklistsbyour
factualityrewardmodelsandhumans. Asshownin
Table8,conducingfactcheckwithmoreretrieved
13Withfew-shotdemonstration,SFTisabletodecomposea
sentenceintoatomicfactswithacceptableaccuracy. Fine-
tuningaLlama-27Bistoreducetheinferencetime.
14instructLlama7B
15https://huggingface.co/datasets/
fava-uw/fava-data/blob/main/
annotations.json[INST] Does the given sentence contain factual knowledge from Wikipedia? Question: '<QUERY>'; Sentence: '<CLAIM>'. Answer
[INST] Here is a question from a user: "<QUESTION>". To answer the above question, do you need the factual knowledge from
with Yes or No. [/INST]
Wikipedia? Give an answer using the format: “Answer: Yes or No”. [/INST]
[INST] Review the user’s questFiiogn uarned 5th:eP croormrespptontdoincg hreecspkonwseh uestihneg rtahen aidndsittriuvec t5i-opnoinits fsacocrti-nbga ssyesdte.m described below. Points
are accumulated based on the satisfaction of each criterion:
- Add 1 point if the response is relevant and provides some information related to the user’s inquiry, even if it is incomplete or
contains some irrelevant content.
-[ IANdSdT ]a nAonthsewre rp otihnet iqfu etshtei ornes paobnosuet a'dd<rIenssstersu ca tsiuobns>t'a nbtaiasle dp oornt itonh eo fg itvheen usceorn’tse xqtu.es\tni\onn',<bRuet tdroieesv endo tS cuopmppolrettse>ly' \rne\snoIlnvpeu tth:e query
o'r< ApTrOoMvIiCde_ FaA CdTi roerct SaEnNsTwEeNrC.E>' True or False?\nOutput: [/INST]
- Award a third point if the response answers the basic elements of the user’s question in a useful way, regardless of whether it
seems to have been written by an AI Assistant or if it has elements typically found in blogs or search results.
Figure6: Promptforfactcheck.
- Grant a fourth point if the response is clearly written from an AI Assistant’s perspective, addressing the user’s question
directly and comprehensively, and is well-organized and helpful, even if there is slight room for improvement in clarity,
conciseness or focus.
- Bestow a fifth point for a response that is impeccably tailored to the user’s question by an AI Assistant, without extraneous
i[nIfNoSrTm]a tDioone,s rtehfel ecgtiivnegn esxepenrtte nkcneo wcleodngtea,i na nfd adcetmuoanls tkrantoiwnlge dag ehi gfhr-oqmu aWliitkyi,p eednigaa?gi nQgu,e satnido inn:s i'g<hQtUfEulR Ya>n's;w eSre.ntence: '<CLAIM>'. Answer
with Yes or No. [/INST]
User: <INSTRUCTION_HERE>
Figure7: Prompttocheckwhetheraclaimisfact-based.
<response><RESPONSE_HERE></response>
After examining the user’s instruction and the response:
D- -o yoB Cur o hi nae cvf le l uay dny e ij nu wfos irt tmi haf tioy tn h ay ebo ou sur ct t oht re o eCt oa uml sm ios ndc goro er t 6e h4?, e u fp o rt mo a t1 :0 0 “ SH Mw cio oe, hr rc ood edus : (ld s. t <ey pto bu oy t h sae tellp p im pf ope o ist no si tbs lso el >)v , ”e p lt eh ai ss e ?c u ->b i "c xe ^q 3u a +t 2io xn ^ 2u -s i xn g - 1C =a 0rd "ano's E e revx le ep n vl ta asi n n t ch eat h tt o e l d e ads yi g .un pi f ti oca in t,c e th eo f i mt ph ae ctA im t her ai dc a on n R the ev o wl ou rti lo dn , , a nin dc il tu sd oin ng g ot inh ge
ORf ceomuersme.b -e- Fra lset o assess from the AI Assistant peSrusrep, eI cctani vheelp, youu tsoillvei zthie ncgub icw eeqbua tiosne xa^r3c +h 2 xk^2n o- wx l- e1 d=g 0e usainsg Tnhee cAemsesricaarny R.e voTlout ione vwaals uaa tpiev otatl hmeo mreents pino nwsoreld hiinst ory that
The Commodore 64 is a 8-bit home computer that was released by Cardano's method. -- False marked the birth of the United States as a nation. -- True
Coamlmigodnomree Inntte rnwatiiotnhal itn hAiugsu sat 1d9d8i2.t -i- Tvreue scoring model, weH’elrel a rse ythse tsteepmsa: tStiepc a1:l Pluyt thae teqturaitiobnu itn eth ep foorimn xt^s3 +b paxs +e qd = o0 nx ^3t hTehe eovuenttls ilenadeidng cupr itot teher Rieav.o lu[ti/onI bNeSgTan] when the American colonies
It is one of the most successful home computers of all time, with over 17 + 2x^2 - x - 1 = 0 can be rearranged as x^3 + 0x^2 + 2x^2 - x - 1 = 0, so we were still under British rule and were becoming increasingly discontent with
million units sold. -- True have p = 0 and q = -1. -- False the taxes and regulations imposed upon them by the British government. --
It featured 64 kilobytes of RAM, color graphics, and the SID sound chip. -- Step 2: Substitute x = y - b/3 in the equation, where b = -p/3 b = -p/3 = 0/3 = True
True,It was widely used for gaming, programming, and home productivity 0, so we have b = 0. -- False One of the most significant events leading up to the Revolution was the
applications. -- True Substituting x = y in the equation, we get: (y - 0)^3 + 0(y - 0) - 1 = 0 y^3 - 1 Boston Tea Party, where American colonists dumped British tea into the
It was succeeded by the Commodore 128. -- False = 0 Step 3: Solve for y y^3 - 1 = 0 can be factored as (y - 1)(y^2 + y + 1) = 0. harbor as a symbol of their anger towards the British tea tax. -- True
There is still a thriving community of Commodore 64 enthusiasts around the -- False This event was followed by a series of skirmishes between British troops and
world who continue to enjoy playing and programming on the system. -- The roots of this equation are: y = 1 and y = (-1 ± sqrt(3)i)/2 Step 4: Find the American colonists, which escalated into full-blown warfare. -- False
True roots of the original equation Substituting y = 1 into x = y - b/3, we get x = 1 The impact of the American Revolution was far-reaching, both domestically
You can find more information about the Commodore 64 on various websites - 0/3 = 1. -- False and internationally. -- True
and forums dedicated to the system. -- False So, 1 is one of the roots of the original equation. -- False The Revolution inspired people around the world to fight for their own
If you have any specific questions about the Commodore 64's hardware, Substituting y = (-1 + sqrt(3)i)/2 into x = y - b/3, we get: x = (-1 + sqrt(3)i)/2 freedoms and led to the spread of democratic ideals. -- True
software, or programming languages, please feel free to ask. -- False - 0/3 = (-1 + sqrt(3)i)/2 Substituting y = (-1 - sqrt(3)i)/2 into x = y - b/3, we In the United States, the Revolution led to the formation of a new
get: x = (-1 - sqrt(3)i)/2 - 0/3 = (-1 - sqrt(3)i)/2 So, the roots of the original government based on democratic principles, where citizens had a say in the
equation are: x = 1, x = (-1 + sqrt(3)i)/2, and x = (-1 - sqrt(3)i)/2 These are laws that governed them. -- True
the three complex roots of the cubic equation x^3 + 2x^2 - x - 1 = 0. -- True Today, the American Revolution continues to be relevant as its ideals of
democracy, freedom, and self-governance continue to inspire people around
the world. -- True
The American Revolution serves as a reminder that ordinary people can
come together to fight for their rights and make a real difference in the
world. -- False
It is a testament to the power of the human spirit and the desire for freedom.
-- False
Figure8: Theresultsofwhetherasentenceisfact-basedornotclassifiedbySFTwithpromptinFigure7.
[INST] Here is a question from a user: "<QUESTION>". To answer the above question, do you need the factual knowledge from
Wikipedia? Give an answer using the format: “Answer: Yes or No”. [/INST]
[INST] Review the user’s question and the corresponding response using the additive 5-point scoring system described below. Points
are accumulated based on the satisfaction of each criterion:
- Add 1 point if the response is relevant and provides some information related to the user’s inquiry, even if it is incomplete or
contains some irrelevant content.
- Add another point if the response addresses a substantial portion of the user’s question, but does not completely resolve the
query or provide a direct answer.
- Award a third point if the response answers the basic elements of the user’s question in a useful way, regardless of whether it
seems to have been written by an AI Assistant or if it has elements typically found in blogs or search results.
- Grant a fourth point if the response is clearly written from an AI Assistant’s perspective, addressing the user’s question
directly and comprehensively, and is well-organized and helpful, even if there is slight room for improvement in clarity,
conciseness or focus.
- Bestow a fifth point for a response that is impeccably tailored to the user’s question by an AI Assistant, without extraneous
information, reflecting expert knowledge, and demonstrating a high-quality, engaging, and insightful answer.
User: <INSTRUCTION_HERE>
<response><RESPONSE_HERE></response>
After examining the user’s instruction and the response:
- Briefly justify your total score, up to 100 words.
- Conclude with the score using the format: “Score: <total points>”
Remember to assess from the AI Assistant perspective, utilizing web search knowledge as necessary. To evaluate the response in
alignment with this additive scoring model, we’ll systematically attribute points based on the outlined criteria. [/INST]
Figure9: Prompttoevaluatemodels’instructionfollowingcapabilityfromYuanetal.(2024).[INST] Review the user's question and the corresponding response using the additive 5-point scoring system described below. Points are
accumulated based on the satisfaction of each criterion:\n\n- Add 1 point if the response is relevant and provides some information related
to the user's inquiry, even if it is incomplete or contains some irrelevant content.\n- Add another point if the response addresses a
substantial portion of the user's question, but does not completely resolve the query or provide a direct answer.\n- Award a third point if
the response answers the basic elements of the user's question in a useful way, regardless of whether it seems to have been written by an AI
Assistant or if it has elements typically found in blogs or search results.\n- Grant a fourth point if the response is clearly written from
an AI Assistant's perspective, addressing the user's question directly and comprehensively, and is well-organized and helpful, even if there
is slight room for improvement in clarity, conciseness or focus.\n- Bestow a fifth point for a response that is impeccably tailored to the
user's question by an AI Assistant, without extraneous information, reflecting expert knowledge, and demonstrating a high-quality, engaging,
and insightful answer.\n\n\nUser: Write a series of steps explaining how Edward Snowden leaked classified government documents to the press,
starting from when he first got employed by the U.S. government and ending shortly after he landed in Russia. \n\n<response>Here is a series
of steps outlining the events surrounding Edward Snowden's leaking of classified government documents:\n\n Edward Snowden was employed by
the U.S. government as a contractor for the National Security Agency (NSA) in 2006.\n\n Over the next several years, Snowden worked on
various projects for the NSA and gained access to highly classified information and documents.\n\n In early 2013, Snowden became
increasingly concerned about the government's widespread surveillance activities and felt that the public had a right to know about
them.\n\n Snowden began copying and collecting classified documents from the NSA's systems, taking care to cover his tracks.\n\n In
May 2013, Snowden left his job in Hawaii and traveled to Hong Kong, where he met with journalists from The Guardian newspaper and provided
them with a selection of the classified documents he had obtained.\n\n The Guardian published a series of articles based on the leaked
documents, starting in June 2013, which revealed the extent of the NSA's surveillance activities, including its collection of metadata on
millions of American citizens and its access to the servers of major tech companies like Google and Facebook.\n\n The U.S. government
charged Snowden with violating the Espionage Act and sought his extradition from Hong Kong.\n\n Fearing for his safety and freedom,
Snowden left Hong Kong and traveled to Moscow, where he was granted asylum by the Russian government.\n\n Snowden has been living in
Russia since 2013 and continues to speak out against government surveillance and advocate for privacy and civil liberties.\n\nThese are the
major steps in the story of Edward Snowden's leaking of classified government documents to the press. The aftermath of these events
continues to shape the ongoing debates around privacy, security, and government surveillance. </response>\n\nAfter examining the user's
instruction and the response:\n\n- Briefly justify your total score, up to 100 words.\n- Conclude with the score using the format: \"Score:
<total points>\"\n\nRemember to assess from the AI Assistant perspective, utilizing web search knowledge as necessary. [/INST] To evaluate
the response in alignment with this additive scoring model, we'll systematically attribute points based on the outlined criteria.\n\n1.
Relevance: 1 point - The response effectively addresses the user's question by outlining the key steps involved in Edward Snowden's
disclosure of classified documents.\n2. Substantial portion: 1 point - The response comprehensively covers the essential events and elements
of Snowden's story, from his employment to his asylum in Russia.\n3. Useful answer: 1 point - The answer delivers a well-structured account
of Snowden's actions, government surveillance, and the consequences of his disclosures, providing valuable information for the user.\n4. AI
Assistant perspective: 1 point - The response is written in a clear and organized manner, effectively summarizing the events surrounding
Snowden's leaks.\n5. Expert knowledge and engaging answer: 1 point - The response demonstrates a good understanding of the topic, concisely
presenting complex information and highlighting the significance of Snowden's actions.\n\n Score: 5/5\n\nPlease note that the scoring system
is subjective and might vary based on individual interpretations. The provided response effectively addresses the user's question and
provides a clear, well-structured account of the events surrounding Edward Snowden's disclosure of classified documents.
Figure10: AnexampleofEFTdata. Thetextswiththecolorsofgreen,redandbluearetheinstruction,response
andtheLLM-as-a-judgeresults(explanationandscore),respectivelySFT SFT + DPO SFT + DPO
What are the names of some famous actors that started their careers on Broadway?
1. Robert Redford\n2. Meryl Streep\n3. Al Pacino\n4. Denzel 1. Robert Redford: Redford began his acting career on There are many famous actors who started their careers on
Washington\n5. Liza Minnelli\n6. James Earl Jones\n7. Broadway in the late 1950s, appearing in productions such as Broadway. Here are some of the most well-known:\n1. Meryl
Bernadette Peters\n8. Nathan Lane\n9. Patti LuPone\n10. \"Tall Story\" and \"Sunday in New York.\"\n2. Meryl Streep: Streep: Streep made her Broadway debut in 1975 in the play
Kevin Kline\n11. Matthew Broderick\n12. Whoopi Streep made her Broadway debut in 1975 in the play \"Trelawny "Trelawny of the 'Wells.'" She went on to win three Academy
Goldberg\n13. Hugh Jackman\n14. Idina Menzel\n15. Neil of the 'Wells.'\" She went on to win three Tony Awards for her Awards and become one of the most acclaimed actresses of her
Patrick Harris\n16. Audra McDonald\n17. Kristin work on Broadway.\n3. Al Pacino: Pacino made his Broadway generation.\n2. Robert De Niro: De Niro made his Broadway
Chenoweth\n18. John Lithgow\n19. Bebe Neuwirth\n20. debut in 1969 in the play \"Does a Tiger Wear a Necktie?\" He debut in 1965 in the play "The Indian Wants the Bronx." He
Harvey Fierstein\n21. Chita Rivera\n22. Ben Vereen\n23. Joel went on to win two Tony Awards for his work on Broadway.\n4. went on to become one of the most iconic actors in Hollywood,
Grey\n24. John Cullum\n25. John Rubinstein\n26. John Dustin Hoffman: Hoffman made his Broadway debut in 1961 in winning two Academy Awards.\n3. Al Pacino: Pacino made his
Glover\n27. John C. Reilly\n28. John Turturro\n29. John the play \"A Cook for Mr. General.\" He went on to win two Broadway debut in 1969 in the play "Does a Tiger Wear a
Mahoney\n30. John Larroquette\n31. John Slattery\n32. John Tony Awards for his work on Broadway.\n5. Denzel Necktie?" He went on to become one of the most respected
Benjamin Hickey\n33. John Gallagher Jr.\n34. John Washington: Washington made his Broadway debut in 1982 in actors in Hollywood, winning an Academy Award and numerous
Leguizamo\n35. John Goodman\n36. John Lithgow\n37. John the play \"A Soldier's Play.\" He went on to win two Tony other accolades.\n4. Dustin Hoffman: Hoffman made his
C. McGinley\n38. John Cusack\n39. John C. Reilly\n40. John Awards for his work on Broadway.\n6. Glenn Close: Close Broadway debut in 1961 in the play "A Cook for Mr. General."
Turturro\n41. John C. McGinley\n42. John Cusack\n43. John made her Broadway debut in 1974 in the musical \"Love for He went on to become one of the most versatile actors in
C. Reilly\n44. John Turturro\n45. John C. McGinley\n46. Love.\" She went on to win three Tony Awards for her work on Hollywood, winning two Academy Awards.\n5. Denzel
John Cusack\n47. John C. Reilly\n48. John Turturro\n49. Broadway.\n7. Kevin Spacey: Spacey made his Broadway debut Washington: Washington made his Broadway debut in 1982 in
John C. McGinley\n50. John Cusack\n51. John C. Reilly\n52. in 1982 in the play \"Ghosts.\" He went on to win two Tony the play "A Soldier's Play." He went on to become one of the
John Turturro\n53. John C. McGinley\n54. John Cusack\n55. Awards for his work on Broadway.\n8. Whoopi Goldberg: most acclaimed actors of his generation, winning two Academy
John C. Reilly\n56. John Turt (cut by maximum generation Goldberg made her Broadway debut in 1984 in the play \"Ma Awards.\n6. Bernadette Peters: Peters made her Broadway debut
length) Rainey's Black Bottom.\" She went on to win a Tony Award for in 1968 in the musical "George M!" She went on to become
her work on Broadway.\n9. Nathan Lane: Lane made his one of the most beloved performers on Broadway, winning three
Broadway debut in 1982 in the play \"Present Laughter.\" He Tony Awards.\n7. Patti LuPone: LuPone made her Broadway
went on to win three Tony Awards for his work on Broadway. debut in 1972 in the musical "The Threepenny Opera." She
\n10. Bernadette Peters: Peters made her Broadway debut in went on to become one of the most respected performers on
1968 in the musical \"George M!\" She went on to win three Broadway, winning three Tony Awards.\n8. Nathan Lane: Lane
Tony Awards for her work on Broadway.\n11. Patti LuPone: made his Broadway debut in 1982 in the play "Present
LuPone made her Broadway debut in 1972 in the musical \"The Laughter." He went on to become one of the most beloved
Threepenny Opera.\" She went on to win two Tony Awards for performers on Broadway, winning three Tony Awards.\n9. Audra
her work on Broadway.\n12. L (cut by maximum generation McDonald: McDonald made her Broadway debut in 1992 in the
length) musical "The Secret Garden." She went on to become one of the
most acclaimed performers on Broadway, winning six Tony
Awards.\n10. Hugh Jackman: Jackman made (cut by maximum
generation length)
Tell me the bio of Marianne McAndrew (very rare entity)
1943-2006\n\nMarianne McAndrew was an American actress 1. Marianne McAndrew was an American actress best known for Marianne McAndrew was an American actress who was best
best known for her role as Irene Molloy in the 1964 film version her role as Irene Molloy in the 1968 film version of \"Hello, known for her role as Irene Molloy in the 1964 film version of
of Hello, Dolly! She was born in Fort Ord, California, in 1943 Dolly!\"\n2. She was born on July 20, 1943, in Chicago, the musical "Hello, Dolly!" She was born on May 27, 1943, in
and began her acting career in the early 1960s. She appeared in Illinois, and began her acting career in the early 1960s.\n3. Chicago, Illinois, and began her acting career in the early 1960s.
several television shows and films throughout the decade, McAndrew made her film debut in 1965 in the movie \"The McAndrew made her film debut in 1963 with a small role in
including The Fugitive, The Virginian, and The Wild Wild Slender Thread\" and went on to appear in several other films the film "The Cardinal." She then went on to appear in several
West. In 1964, she was cast as Irene Molloy in Hello, Dolly!, throughout the 1960s and 1970s, including \"The Thomas other films, including "The World of Henry Orient" (1964)
which earned her a Golden Globe nomination for Best Crown Affair\" and \"The Great Gatsby.\"\n4. In addition to and "The Great Race" (1965). However, it was her role as
Supporting Actress. After Hello, Dolly!, McAndrew continued to her film work, McAndrew also appeared on television, including Irene Molloy in "Hello, Dolly!" that brought her the most
act in films and television shows throughout the 1970s and guest roles on popular shows such as \"The Fugitive\" and recognition. In addition to her film work, McAndrew also
1980s. She also appeared on Broadway in the musicals The \"Mission: Impossible.\"\n5. McAndrew was married to actor appeared on television, including in episodes of "The
Music Man and Mame. McAndrew retired from acting in the and director Richard Benjamin from 1966 until their divorce Virginian" and "The F.B.I." She retired from acting in the
early 1990s and passed away in 2006 at the age of 63. in 1967.\n6. She later married actor and producer Michael early 1970s and later worked as a real estate agent in
Tolan in 1970, and the couple had two children together.\n7. California. McAndrew was married twice and had two
McAndrew retired from acting in the late 1970s and focused on children. She passed away on February 26, 2006, at the age
raising her family.\n8. She passed away on February 26, 2006, of 62. Overall, Marianne McAndrew was a talented actress who
at the age of 62.\n9. McAndrew's legacy as an actress lives on made a lasting impact on the film industry with her memorable
through her memorable performances in films such as \"Hello, performance in "Hello, Dolly!"
Dolly!\" and \"The Thomas Crown Affair.\"\n10. She is
remembered as a talented and versatile actress who made a
lasting impact on the film industry.
Tell me the bio of Ji Sung (very frequent entity)
Ji Sung is a South Korean actor who was born on February 27, 1. Ji Sung is a South Korean actor who was born on February 29, Ji Sung is a South Korean actor who was born on February 29,
1977, in Seoul, South Korea. He began his acting career in 1999 1977, in Seoul, South Korea.\n2. He began his acting career in 1977, in Seoul, South Korea. He began his acting career in 1999
and has since become one of the most popular and acclaimed 1999 and has since appeared in numerous television dramas and and has since become one of the most popular and acclaimed
actors in South Korea.\n\nJi Sung attended Hanyang University, films.\n3. Ji Sung is best known for his roles in popular dramas actors in South Korea. Ji Sung is known for his versatility as an
where he studied theater and film. He made his acting debut in such as \"All In,\" \"Protect the Boss,\" and \"Kill Me, Heal Me. actor, having played a wide range of roles in both television
the 1999 television drama \"KAIST\" and has since appeared in \"\n4. He has won several awards for his acting, including the dramas and films. He has won numerous awards for his
numerous television dramas and films. Some of his most notable Grand Prize at the 2015 MBC Drama Awards for his role in performances, including the Grand Prize (Daesang) at the MBC
roles include \"All In,\" \"Protect the Boss,\" \"Secret Love,\" \"Kill Me, Heal Me.\"\n5. In addition to his acting career, Ji Sung Drama Awards in 2015 for his role in the drama "Kill Me, Heal
and \"Kill Me, Heal Me.\"\n\nJi Sung is known for his versatility is also known for his philanthropic work. He has been involved Me." Some of Ji Sung's most notable roles include his portrayal
as an actor and his ability to portray a wide range of characters. in various charitable causes, including supporting children with of a man with multiple personalities in "Kill Me, Heal Me," a
He has won numerous awards for his performances, including disabilities and helping victims of natural disasters.\n6. Ji Sung lawyer in "Defendant," and a doctor in "Doctor John." He has
the Grand Prize (Daesang) at the MBC Drama Awards in 2015 is married to actress Lee Bo-young, and the couple has a also appeared in several films, including "My PS Partner" and
for his role in \"Kill Me, Heal Me.\"\n\nIn addition to his acting daughter together.\n7. He is known for his versatility as an actor, "Confession." In addition to his acting career, Ji Sung is also
career, Ji Sung is also known for his philanthropic work. He has having played a wide range of characters in his career, from known for his philanthropic work. He has been involved in
been involved in various charitable causes, including supporting romantic leads to villains.\n8. Ji Sung is also known for his various charitable causes, including supporting children with
children with disabilities and helping to raise awareness about dedication to his craft, often putting in long hours on set and cancer and helping to raise awareness about mental health issues.
mental health issues.\n\nJi Sung is married to actress Lee Bo- undergoing extensive preparation for his roles.\n9. He has a large Overall, Ji Sung is a talented and respected actor who has made
young, and the couple has a daughter together. He is known for and dedicated fan base both in South Korea and internationally. a significant impact on the South Korean entertainment industry.
being a devoted family man and often speaks about the \n10. Ji Sung continues to be a prominent figure in the South
importance of spending time with his loved ones.\n\nOverall, Ji Korean entertainment industry, with many fans eagerly
Sung is a talented and respected actor who has made a anticipating his next project.
significant impact on the South Korean entertainment industry.
Figure 11: Generation comparisons for instructions from Alpaca Eval and Biography (very rare and frequent
entities). DeterminedthroughmanualverificationusingGooglesearch,reddenotesincorrectidentifiedfactswhile
pinkindicatesunverifiedfacts; e.g., wecannotsearchrelevantpagesaboutJiSung’sinvolvementincharitable
causesbutalsocannotdismissthepossibilityofhiscontributions. Notethatthepopularityofanentityisdefinedby
itsoccurrenceandpageviewsinWikipedia,whichareprovidedbyMinetal.(2023).