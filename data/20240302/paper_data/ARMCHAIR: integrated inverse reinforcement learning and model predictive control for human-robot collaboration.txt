ARMCHAIR: integrated inverse reinforcement learning and
model predictive control for human-robot collaboration
Angelo Caregnato-Neto1*, Luciano Cavalcante Siebert3, Arkady Zgonnikov4,
Marcos R. O. A. Maximo2, Rubens J. M. Afonso1
1*Electronic Engineering Division, Instituto Tecnol´ogico de Aeron´autica, S˜ao Jos´e dos
Campos, S˜ao Paulo, Brazil.
2Autonomous Computational Systems Lab (LAB-SCA), Computer Science Division,
Instituto Tecnol´ogico de Aeron´autica, S˜ao Jos´e dos Campos, S˜ao Paulo, Brazil.
3Intelligent Systems Department, Interactive Intelligence Group, Delft University of
Technology, Delft, The Netherlands.
4Department of Cognitive Robotics, Delft University of Technology, Delft, The
Netherlands.
*Corresponding author(s). E-mail(s): caregnato.neto@ieee.org;
Contributing authors: l.cavalcantesiebert@tudelft.nl ; a.zgonnikov@tudelft.nl ;
mmaximo@ita.br; rubensjm@ita.br;
Abstract
Oneofthekeyissuesinhuman-robotcollaborationisthedevelopmentofcomputationalmodelsthat
allowrobotstopredictandadapttohumanbehavior.Muchprogresshasbeenachievedindeveloping
suchmodels,aswellascontroltechniquesthataddresstheautonomyproblemsofmotionplanningand
decision-makinginrobotics.However,theintegrationofcomputationalmodelsofhumanbehaviorwith
suchcontroltechniquesstillposesamajorchallenge,resultinginabottleneckforefficientcollaborative
human-robot teams. In this context, we present a novel architecture for human-robot collaboration:
Adaptive Robot Motion for Collaboration with Humans using Adversarial Inverse Reinforcement
learning (ARMCHAIR).Our solution leverages adversarial inverse reinforcementlearning and model
predictive control to compute optimal trajectories and decisions for a mobile multi-robot system
that collaborates with a human in an exploration task. During the mission, ARMCHAIR operates
withouthumanintervention,autonomouslyidentifyingthenecessitytosupportandactingaccordingly.
Our approach also explicitly addresses the network connectivity requirement of the human-robot
team.Extensivesimulation-basedevaluationsdemonstratethatARMCHAIRallowsagroupofrobots
to safely support a simulated human in an exploration scenario, preventing collisions and network
disconnections, and improving the overall performance of the task.
Keywords:Cyber-physical-humansystems,human-robotcollaboration,inversereinforcementlearning,
trajectoryplanning,taskallocation,modelpredictivecontrol
1
4202
beF
92
]OR.sc[
1v82191.2042:viXra1 Introduction
team. Often, proper operation of MRS requires
the maintenance of network connectivity [5].
A cyber-physical-human system (CPHS) is an
This property holds particular importance when
emerging concept of control schemes that empha-
humans are integrated into the team since typical
sizes the interaction between machines and
interactions, in the form of direct human com-
humans [1]. An exciting application of CPHS is
mands, negotiations, or adaptations, also entail
the design of collaborative multi-robot systems
some form of communication. By properly pre-
(MRS) devised to support humans in a variety of
dictinghumanmotionwithintheMPCframework
tasks, from construction [2] to manufacturing [3]
through machine-learned behavior models, hard
and transport [1]. One of the core challenges in
constraintsonconnectivitycanbeenforced,allow-
theconceptionofsuchMRSisthedevelopmentof
ing the robots to receive commands, perform
their motion planning and decision-making algo-
negotiations, or adapt to human actions by con-
rithms, which must enable the group of robots
tinually monitoring them. The maintenance of
to harmoniously operate with humans towards an
communication has also been demonstrated as an
aligned objective [4].
important factor in improving human-robot trust
Aparticularlyattractivetechniqueusedtothis
[15].
endismodelpredictivecontrol(MPC),areceding
horizon control scheme that has been extensively
1.1 Background
demonstrated for multi-robot real-time motion
planning and decision-making [5, 6, 7]. As a Originally developed for process control in chem-
model-based approach, MPC typically requires ical plants [16], model predictive control (MPC)
accurate prediction models of the dynamic ele- evolved over the last decades into a flexible tool
ments in the environment [8]. This requirement thatcanbefine-tunedforawiderangeofapplica-
is particularly relevant in the context of human- tions. In the field of robotics, it has been already
robot teaming, where appropriate group coordi- demonstrated for motion planning of complex
nation depends on the capability of predicting platforms such as autonomous cars [6] and MRS
and adapting to human decisions and motion in [17]. The inherent robustness associated with the
real-time [9]. closed-loop formulation alongside its capability
The human behavior prediction models are to perform constrained optimization is ideal in
not only a fundamental component of most pre- addressing major issues in robot motion planning
dictive control schemes designed for human-robot [18], such as active collision avoidance in environ-
collaboration but also outlined as a key ele- mentswithmovingobstacles[7],safeoperationin
ment in the development of general CPHS [1]. unknown environments [19], and coordinated ren-
Traditionally, fields such as psychology rely on dezvousmaneuvering[20].TheuseofMPCalong-
descriptivepredictionsofhumanbehaviorthatare side mixed-integer programming (MIP) further
based on first principles [1, 10, 11]. Alternatively, enhancesthetechnique,allowingtheoptimization
recent advances in machine learning techniques of integer and continuous variables. This exten-
have enabled an increasing number of sophisti- sion allows MPC-MIP optimization models to
cated data-driven models for human motion and encode a variety of additional requirements such
decisions [12, 13]. As a quantitative methodol- as minimum-time maneuvering, task allocation,
ogy, machine learning offers the benefit of pro- and connectivity maintenance [21, 22, 5].
viding mathematical or algorithmic models that To devise human prediction models, this work
are naturally encoded into machines [14], facil- leverages inverse reinforcement learning (IRL)
itating their integration with powerful control [23], a machine learning method that has been
techniques. As a result, machine-learned human extensively employed to model human behavior
behavior models and MPC have been successfully inthecontextofsequentialdecision-making,from
integratedtodesignefficientalgorithmsforCPHS aircraftpilots[24]tocardrivers[25].Unlikeother
[7, 6]. popular machine learning methods used for this
Another concrete motivation for this inte- purpose,suchasimitationlearning[26],IRLalgo-
gration is the possibility of enforcing connectiv- rithms aim to recover a reward function given a
ity between the elements of the human-robot dataset of demonstrations. This distinction offers
2substantialbenefitssincetheensuingrewardfunc- predicted human actions. An MPC-MIP formula-
tions can be used to infer human intentions and tion is employed to safely and efficiently steer the
also provide enhanced flexibility, being used for vehicle in the lane-changing maneuver. A strat-
optimizations considering scenarios that deviate egy based on Interaction-Aware Model Predictive
from the ones present in the original data [23]. Path Integral (IA-MPPI) is proposed in [38] for
These traits have been leveraged for many appli- trajectory planning of a team of Unmanned Sur-
cations, such as collaborative autonomous driving face Vessels (USV) in urban canals. The system
[25],selectingtrajectoriesforamanipulatorrobot is able to predict trajectories and goals using
thatcloselyresembleobservedhumanmotion[27], pedestrianmodelsandsocialvariationalrecurrent
and inferring objectives of rogue drones [28]. neural networks.
Adversarial Inverse Reinforcement Learning In addition to MPC and machine learning
(AIRL) [29] is an alternate rendition of this methods, another popular methodology for the
method that leverages the principle of maximum- development of human-robot collaboration algo-
entropy IRL [30] and adversarial deep networks. rithmsformobileplatformsarehierarchicalarchi-
This approach has been shown to outperform tecturesthatleveragemultipledistincttechniques
established deep imitation learning algorithms in thatindividuallyaddresstheproblemsofdecision-
terms of generalization [29, 31], demonstration- making, motion planning, and connectivity. For
efficiency [31],andprecision [32].Similarlyto tra- example, [39] proposes an architecture with three
ditional IRL, AIRL has found use in a multitude layers: a global planner leverages the fast march-
ofapplications,fromthedesignofagentsthatcan ingsquarealgorithmtoprovidepropertrajectories
remain under human meaningful control [33, 34] to the goal, while a local planner addresses col-
to improving lane-changing [35] and lane-keeping lision avoidance. A Social Force Model (SFM)
maneuvers [36] of autonomous vehicles. with parameters estimated by a neural network
is employed for human prediction. A Lloyd-based
1.2 Related work controller handles the multi-agent coordination
problem and provides further safety guarantees.
Westartthepresentationoftheselectedliterature
A similar approach is proposed in [40], where
with works that employ MPC and machine learn-
the task of path planning is divided into global
ing to design algorithms for mobile human-robot
and local planners, while collision avoidance is
teams. In [6], an MPC-based formation controller
handled by a low-level controller. Human demon-
wasdesignedforhuman-leadingtruckplatooning.
strations are employed by an IRL algorithm to
A model of the driver’s behavior was computed
learn proper social navigation behaviors, which
using IRL and then integrated into decentralized
are then used to compute human-like trajectories
MPC controllers in each autonomous truck. The
for the robot. A B-spline path generation algo-
inherentuncertaintyinthebehaviorofthedrivers
rithmisproposedin[41]toaddresstheproblemof
was handled using chance constraints.
humanoid robot navigation alongside humans. A
Theproblemofcoordinatingagroupofrobots,
combination of SFM and Bayesian human motion
humans, and moving obstacles without communi-
models is used to predict human trajectories. The
cation networks is addressed in [7]. A Recurrent
integration between adaptive control strategies
Neural Network (RNN) is trained using a dataset
andsocialproximitypotentialfieldshasalsobeen
of robot demonstrations to develop prediction
proposedasasolutionforhuman-robotnavigation
models. The proposed MPC trajectory planners
under specific social conventions [42].
are able to steer independent robots in swap
In [43] a semi-autonomous approach is pro-
maneuvers without a priori exchange of their
posed where the human performs a search-and-
intended paths.
rescue operation with the support of Unmanned
A lane-changing problem in mixed-traffic
Aerial Vehicles (UAVs). The tasks of the team
scenarios is considered in [37]. The proposed
areassignedbytheoperatorwhilecollisionavoid-
approach relies on the recognition of six driver
ance is handled by a trajectory planner based on
actions that are modeled using hidden Markov
replanningusinglinearspatialseparations.Differ-
models. Probability models are then trained to
ent levels of autonomy in human-robot teaming
predict the acceleration of the vehicle given the
3are studied in [44] considering the task of robot- Human demonstrations
assisted docking of a ship conducted by a human.
The problem of human-robot collaborative AIRL
search has been successfully demonstrated in [45]
Policy
through a series of thorough experiments. The
Human
proposal leverages the concept of Social Reward Prediction
Sources(SRS)tomodeltheenvironmentandmis- Model
sion,whileanarchitectureusingMonteCarloTree Trajectory
Search (MCTS) and Rapidly-exploring Random
MPC-MIP
Trees (RRT) is used to collaboratively navigate a Robot
group of robots. The interactions occur through Commands
messages with the humans transmitting their
intended goals and positions using a smartphone.
Target
1.3 Contributions
Obstacle
We propose Adaptive Robot Motion for Col-
laboration with Humans using Adversarial Target
Target
Inverse Reinforcement learning (ARMCHAIR), a
novel architecture that leverages the integration Fig. 1 ARMCHAIR control architecture. The AIRL
offlinelayerprovidesahumanpredictionmodelthatisused
between IRL and MPC-MIP for human-robot
by the MPC-MIP algorithm to compute proper trajecto-
collaboration. Our work differentiates itself by: riesanddecisionsfortheMRSinaclosedloop.
1. providing both movement coordination and
collaboration in task allocation for an MRS
them to certified global optimality in finite-time
that supports a human in an exploration
[46].Inaddition,usingtheAIRL-basedprediction
mission;
model, ARMCHAIR is able to operate without
2. not requiring human intervention, with the
human intervention by inferring their intentions
robot team identifying the necessity of sup-
and adapting to them. This is made possible,
port and acting as needed;
in part, by the active enforcement of network
3. handling the joint decision-making and
connectivity through the constraints in the MPC-
motion planning optimization problems in a
MIP problem. The optimization is performed in a
unified MPC-MIP framework;
closed-loop fashion using the current feedback of
4. actively enforcing network connectivity;
the human and robot states, which are sensored
Unlike most of the proposals found in the lit-
andcommunicatedtotheplannerthroughthenet-
erature, ARMCHAIR addresses: a) the typical
work. The planner provides periodical updates on
movement coordination problems associated with
the trajectories and decisions of the MRS in a
collision avoidance and connectivity maintenance
receding horizon fashion.
thatarerequiredforpropercoexistenceandcoop-
eration between humans and robots collaborating
1.4 Notation and definitions
on the same task; and b) it provides harmo-
nious task allocation with the machines updating The prediction of a variable “◦” for the time
their objectives to conform with human deci- step k + ℓ made at time step k is written as
sions, avoiding conflicts and effectively allowing ˆ◦(ℓ|k).Boldlettersrepresentmatricesandvectors,
thegrouptocollaborate towardsanalignedglobal whereasscalarsarewritteninplaintext.Thesets
goal in spite of the uncertainty in the human’s of all integers in the interval [m,n] are written as
decisions. In ={m,m+1,...,n}. Sets are denoted by cal-
m
ARMCHAIR’s architecture is presented in ligraphic letters as in B. All polytopes mentioned
Figure 1. The trajectories are computed with the in this work are convex. The Minkowski sum and
proposed MPC-MIP scheme, which handles both Pontryagin difference operators are denoted by ⊕
the motion planning and task allocation issues in and ∼, respectively [47].
thesameframework,beingabletojointlyoptimize
4
setats
namuH
setats
toboR
enifflO
enilnO
noitcelloc
atad
gnidecerP2 Problem Description
Human
We consider a group of n ∈ N robots and one
a
human in a task of visiting targets that represent
regions of interest in a planar environment. This
setup represents a general exploration mission,
which is relevant in multiple real-world contexts Robot 1
such as search-and-rescue and surveillance tasks
[45].Thediscrete-timedynamicsoftherobotscon-
sidering a sampling period of T >0 are described
by the following equations
Robot 2
x (k+1)=A x (k)+B u (k) (1)
i i i i i
Fig.2 IllustrationofenvironmentwithatargetoftypeB
y i(k)=Cx i(k), ∀i∈I 1na (2) in T1 and two targets of type A represented by the poly-
topes T2 and T3; three obstacles O1, O2, and O3; and a
where x
i
∈ Rnx, u
i
∈ Rnu, and y
i
∈ R2 are the terminal region F. The polytopes R1 and R2 are outer
approximationsoftherobots’bodies.Connectivityregions
state, input, and position of robot i, respectively;
aredepictedascircles.
A
i
∈ Rnx×nx, B
i
∈ Rnx×nu, and C ∈ R2×nx
are the corresponding matrices of the state-space
regions are further divided into two types: A and
representation. States and inputs are bounded
B. From the perspective of the robots both types
by the polytopic sets X ⊂ Rnx and U ⊂ Rnu,
are equally valuable, whereas the human may be
respectively.
biased toward a particular type. In the design of
Thegroupperformsthetaskwithinthebound-
the proposed algorithms, the preferences of the
aries of a region represented by the polytope A⊂
human are not explicitly known and can only be
R2 which contains n ∈ N polytopic obstacles
o inferred from data. The number of targets of type
O ⊂ R2, ∀g ∈ Ino. The space occupied by the
g 1 A and B is denoted by n ≥ 0 and n ≥ 0,
body of the robots is defined as A B
respectively, such that n +n = n . The mis-
A B t
sion finishes when the human reaches a terminal
R (k)≜y (k)⊕R , ∀i∈Ina (3)
i i i 1 region F ⊂ R2. Figure 2 provides an example of
the envisioned scenario.
whereR ⊂R2isapolytopecenteredattheorigin
i In our setup, the group of robots must also
representing the shape of robot i. Similarly, the
maintain connectivity at each time step. We con-
space occupied by the human is defined as
sider proximity-based links, where two agents are
connected if they are close enough to each other,
Rh(k)≜yh(k)⊕Rh (4)
i.e., if their relative position is within a communi-
cationregionrepresentedbythepolytopeC ⊂R2.
withyh ∈R2 beingthepositionofthehumanand
Under this condition for individual connections,
Rh ⊂ R2 representing the human body. Define
the whole group can communicate if the underly-
O¯ ≜ (cid:83)no O . Then, the free operation region of
g=1 g ing network, represented by an undirected graph,
the i-th robot at time step k is written as, ∀i ∈
is connected [22].
Ina,
1 In order to cooperate with the human, the
   robots make use of a behavior model of that
A¯ i(k)= A\ O¯∪Rh(k)∪
(cid:91)na
R j(k)  ∼R
i
h alu tm hoa un g. hT th his eb foe lh loa wvi io nr gm aso sd ue mli ps tii on nit sia al rly eu mn ak dn eo :wn,
  
A1) a set of demonstrations D containing human
j=1
j̸=i trajectories in similar tasks is available;
(5)
A2) the human operates trying to maximize a
compromise between visiting the maximum
Thetargetsintheenvironmentarerepresented
number of targets and finishing the mission
by the polytopes T ⊂ R2, ∀e ∈ Int, where
e 1 as soon as possible.
n ∈ N is the total number of targets. These
t
5A3) the ensuing behavior is not necessarily opti- the environment. The time-demanding learning
mal nor deterministic; process of AIRL is carried offline and only the
A4) the human moves with constant velocity. resulting policy is used, alongside the feedback
The set of human demonstrations assumed of current human and robot states, in the online
in A1 can be obtained by monitoring and col- closed-loop scheme. The MPC-MIP motion plan-
lecting data directly during their activities. In ning and decision-making algorithm performs an
thisproof-of-conceptwork,wegeneratedsynthetic optimization considering the current state of the
human behavior data rather than collecting it group and employs the policy computed by the
from real humans (see Appendix A for details). AIRL method to predict future motion and deci-
This approach allows for preliminary evaluation sions of the human given their current state. The
of algorithms for human-robot collaboration [33, inherent uncertainty in human actions is handled
48, 31, 24, 38] since it can generate human-like by a straightforward robustness approach using
data in a scalable way, although it is limited in safety regions.
that it does not exactly represent actual human Each element of the ARMCHAIR’s architec-
behavior. Assumptions A2 and A3 imply that the ture is thoroughly discussed in the subsequent
human is aware of their task and has some degree sections.
ofcompetencyincompletingit.However,optimal
performance is typically not achieved, as humans 3.1 Adversarial Inverse
are susceptible to bounded rationality and uncer- Reinforcement Learning
tainty over objectives [49, 50, 48]. We employed
reinforcement learning to train a synthetic agent Typically, MPC requires a model to predict the
to emulate these characteristics (Appendix A). dynamic elements of the environment. Consider-
The resulting policy is optimized to maximize a ing the problem description discussed in Section
compromise between collecting rewards (by visit- 2, ARMCHAIR learns a prediction model for the
ing targets) and finishing the task quickly but is human given only the set of demonstrations D
also stochastic and not necessarily globally opti- under the aforementioned assumptions. To this
mal. Thus, both A2 and A3 are satisfied by the end, we leverage AIRL [29] to recover a reward
proposed synthetic agent. For the sake of sim- function so that ARMCHAIR can train a pol-
plicity, Assumption A4 constrains the problem to icy that imitates the behavior observed in the
humans moving with constant velocity. However, dataset.Thispolicywillprovidepredictionsofthe
the methods herein proposed can be expanded to human’s high-level decisions, i.e., the targets that
accommodate a varying velocity. In this context, arelikelytobevisited,andthecorrespondingpath
the main problem addressed by ARMCHAIR is throughtheenvironment.Henceforth,wefollowed
Problem 1. Design a motion planning and task the AIRL approaches from [29, 33].
allocationalgorithmthatallowtherobotstocollab- In particular, we follow the scheme presented
orate with the human, complementing their efforts in [33] and divide the environment presented in
inthetaskwhennecessarywithoutthenecessityof Section 2 into n c ∈ N cells of a grid. The ensu-
human intervention. ing scenario is represented by a Markov Decision
Process (MDP) ⟨S,N,p,r,γ⟩ with
3 Adaptive Robot Motion for
s=[F ,F ,F ,F ,F ] (6)
Collaboration with Humans pos A B obs ter
using Adversarial Inverse being a state belonging to the set of states S
Reinforcement learning with the feature maps F pos,F A,F B,F obs,F ter ∈
{0,1}nc×nc representing the positions of the
(ARMCHAIR)
human, type A targets, type B targets, obstacles,
andterminalregion,respectively.Examplesoffea-
The ARMCHAIR scheme, presented in Figure
ture maps are presented in Figure 3. The set of
1, addresses Problem 1 by leveraging AIRL to n ∈ N actions is denoted as N = {a ,...,a },
model human behavior through a dataset of their
a 1 na
whereas p(s′|s,a) is the transition distribution
demonstrations while performing the mission in
6T chance constraints [6], contingency [51], or con-
straint tightening. We opt for a straightforward
approachwherethepredictionsarethemostlikely
sequence of states given by the policy, with the
uncertainty in the human’s actions being handled
bytherobustformulationpresentedinSection3.4.
The sequence of states is computed through the
propagation of the (8) and (9) until the terminal
state is reached:
aˆ(n)=argmaxπˆ(a′|sˆ(n)) (8)
Fig. 3 Human position, target of type A, and terminal
regionTinagridencodedasthefeaturemapsFpos,FA, a′∈N
Fter,respectively. ˆs(n+1)=argmaxp(s′|ˆs(n),aˆ(n)) (9)
s′∈S
associated with the dynamics of the environ-
withaˆ andˆsrepresentingactionandstatepredic-
ment. The reward function and discount factor
tions, respectively.
are denoted by r : S × N → R and γ ∈ [0,1],
Algorithm 1 summarizes the process of com-
respectively.
puting the human predictions. It starts by asso-
ThetwomaincomponentsoftheAIRLframe-
ciating the first predicted state ˆs(0) with the
work are the reward and policy networks repre-
measured state of the human at time step k,
sented by f and π , which are parameterized by
θ ϕ smeas(k).Thecorrespondingwaypointwˆ(n)∈R2,
θ and ϕ, respectively. The discriminator
representing the predicted Cartesian position of
the human, is then extracted from the feature
exp(f (s,a,s′))
D (s,a,s′)= θ (7) map F (n) using the function getWaypoints.
θ exp(f (s,a,s′))+π (a|s) pos
θ ϕ Considering the current state, the procedures in
lines 5 and 6 compute the most likely action and
previouslyproposedin[29]outputstheconfidence
next state, respectively. This process is repeated
thataparticularstate-actionpair(s,a)belongsto
in a loop until the predictions reach the termi-
the demonstration dataset rather than being gen-
nal state. Then, assuming that the human moves
erated from π . The discriminator is trained to
ϕ with a constant linear velocity v > 0, the pre-
improvethisdataclassificationusingbinarylogis-
dictedtrajectoriesattimestepk arecomputedby
tic regression, further refining the reward approx-
the function getTraj yielding a sequence of pre-
imator f θ at each iteration. The policy network dicted human positions, yˆh(ℓ|k) ∈ R2, ∀ℓ ∈ IN¯
is updated using PPO [33], to generate trajecto- 0
with N¯ being the maximum prediction horizon
ries that better match the demonstrations, this
considered.
way “confusing” the classification process of the
Algorithm 1 runs periodically, employing
discriminator [29]. The policy recovered by AIRL
updatedmeasurementsofthehuman’sstate,with
is denoted by πˆ : S → ∆ and is a probability
N a frequency dictated by the sampling period T.
distributionoverallactionsconditionedbyastate.
3.3 MPC-MIP Formulation
3.2 Human Prediction Model
The MPC-MIP formulation designed for the
Once πˆ is properly learned, we can determine
human-robot motion planning and decision-
sequences of states representing the potential
making problem is similar to [5], with the distinc-
paths and decisions of the human given an ini-
tion of accounting for an independent agent, i.e.
tialstate.However,duetothestochasticityofthe
the human, over which the group of robots has
policy, many distinct sequences can be computed.
no command. Within the multi-agent team, the
This uncertainty can be addressed by robust
humanisalwaysassignedtothelastindexn +1,
MPCplannerswithamultitudeoftechniquese.g., a
where n is the number of robots, without loss of
a
7Algorithm 1 Human prediction model ∀i∈Ina+1, ∀e∈Int, i<j ≤n +1, ∀ℓ∈IN¯
1 1 a 0
Input: smeas(k) ▷ Human state at time step k btar(ℓ|k) =⇒ y (ℓ|k)∈T (10i)
i,e i e
Input: πˆ ▷ Policy recovered with AIRL
1: n←0
(cid:88)N¯ n (cid:88)a+1
btar(ℓ|k)≤1 (10j)
2:
ˆs(0)←smeas(k) i,e
k=0 h=1
3: while terminal region not reached do bcon(ℓ|k) =⇒ y (ℓ|k)−y (ℓ|k)∈C (10k)
4: wˆ(n)← getWaypoints(ˆs(n)) i,j i j
65 :: ˆsaˆ (( nn) +← 1)a ←rgm ara gx ma′ a∈ xN s′πˆ ∈S(a p′| (ˆs s( ′n |ˆs) ()
n),aˆ(n))
deg i(ℓ|k)=(cid:88)i−1 bc ϵ,o in(ℓ|k)+ (cid:88)na bc i,o ϵn(ℓ|k) (10l)
7: n←n+1 ϵ=1 ϵ=i+1
8 9: : e (cid:8)n yˆd h(ℓw |kh )i (cid:9)le N ℓ¯
=0
← getTraj(cid:16) {wˆ(j)}n j=0(cid:17) d (cid:88)ne ag i(ℓ|k)+deg j(ℓ|k)≥n a, j ̸=n a+1 (10m)
Output:
(cid:8) yˆh(ℓ|k)(cid:9)N¯ bc i,o nn a+1(ℓ|k)≥1 (10n)
ℓ=0 i=1
bhor(ℓ|k) =⇒ y (ℓ|k)∈F (10o)
na+1
generality.Implementationdetailsareavailablein
the provided source code1. The cost (10a) is a compromise between the
Let the tuple λ =
(cid:10)
x ,u
,bhor,btar,bcon(cid:11)
rep-
control effort of the robots (weighted by γ
u
> 0)
i i i,e i,j
and the collection of rewards through target vis-
resent all variables to be optimized. Then, the
itation, with the rewards being represented by
human-robot teaming problem can be formalized
γ > 0. Notice that the activation of the target
asthefollowingmixed-integerlinearprogramming t
binaries btar ∈{0,1}, which implies that an agent
(MILP) model. i,e
visited a target due to constraint (10i), results in
a discount that contributes to the minimization
of (10a). Thus, the cost drives the group towards
target visitation while also considering the total
Human-robot teaming optimization model
controleffort(representedbytheinputsu )ofthe
(cid:88)N¯ (cid:32) (cid:88)na (cid:88)nt n (cid:88)a+1 (cid:33) maneuvers. i
min γ ∥u (k)∥ −γ btar(k)
u i 1 t i,e Constraint (10b) performs the assignment of
λ
k=0 i=1 e=1 i=1 thepredictionscomingfromthehumanprediction
(10a)
model (Algorithm 1) to the appropriate variables
s.t., in the optimization model, y .
na+1
Human Constraints (10c) - (10g) are related to the
∀ℓ∈IN¯
,
robots. Constraint (10c) establishes the reced-
0 ing horizon scheme of the MPC technique. The
y na+1(ℓ|k)=yˆh(ℓ|k) (10b) dynamicsareencodedby(10d)and(10e),whereas
Robots (10f) and (10g) enforce the robot’s state and
inputbounds,respectively.Avoidanceofcollisions
∀i∈Ina, ∀ℓ∈IN¯
1 0 between elements of the group and with obstacles
x i(0|k)=x i(k), (10c) is enforced by (10h).
x (ℓ+1|k)=A x (ℓ|k)+B u (ℓ|k) (10d) Finally, collaboration between the robots in
i i i i i
the group is achieved through constraints (10i) -
y (ℓ|k)=C x (ℓ|k) (10e)
i i i
(10o). The relationship between target binaries,
x (ℓ+1|k)∈X (10f)
i i the positions of the agents, and the target poly-
u i(ℓ|k)∈U i (10g) topesisdescribedbytheimplicationinconstraint
y (ℓ+1|k)∈A¯(ℓ+1|k) (10h) (10i).Constraint(10j)establishesthatthereward
i i
associated with each target can be collected only
Collaboration
once. Constraint (10k) conditions the activation
of the connectivity binary bcon ∈ {0,1} of agents
i,j
i and j to their relative position being within the
1https://gitlab.com/caregnatonetoopen/ARMCHAIR
8as, ∀i∈Ina, ∀k ∈IN¯,
1 0
A¯(ℓ+1|k)=(A\P (ℓ+1|k))∼R (12)
i i i
A similar approach is employed to prevent
network disconnections. A shrunken connectivity
region is computed considering the safety region
C¯=C ∼R¯h (13)
Fig. 4 Safetyregionaroundthehumantobeavoidedby
theMRS. and constraint (10k) is rewritten to employ the
shrunk region when considering connectivity with
connectivity region polytope C. The degree of the the human, ∀i∈I 1na, ∀ℓ∈I 0N¯−1, j =n a+1,
verticesthatrepresenttheagentsinthecommuni-
cation network is encoded by (10l). The condition bc i,o jn(ℓ+1|k) =⇒ y i(ℓ|k)−y j(ℓ|k)∈C¯ (14)
for connectivity between robots is enforced by
constraint (10m), whereas (10n) guarantees that 4 Results
thereisalwaysatleastonerobotconnectedtothe
human,forminganetworkwithallelementsofthe We evaluated the proposed scheme considering
team. two environment configurations with four obsta-
The mission is finished when the horizon cles, one terminal region, and a maximum of four
binary bhor ∈ {0,1} is active. Constraint (10o) targets. Environment 1 was designed to investi-
conditions the activation of this binary to the gate the functionality of ARMCHAIR in a situ-
human predicted position reaching the terminal ation where the targets are sparsely distributed,
region F. whereas environment 2 contains closely grouped
targets that represent a more challenging prob-
3.4 Robust formulation lem in terms of human behavior prediction and
appropriate robot response.
As discussed in 3.2, the predictions employed by
Inbothscenarios,thesimulatedhumanissup-
the motion planning algorithm take into account
ported by a group of n =2 robots with discrete-
a
only the most likely actions of the human. We
time double integrator dynamics described by (1)
address the ensuing potential prediction errors
and (2) considering a sampling period of T = 1
by making the method more robust through an s. The states are x = [r ,v ,r ,v ]⊤, ∀i ∈
i x,i x,i y,i y,i
approach based on safety regions. Ina, where r and r are the positions of robot
1 x,i y,i
To prevent potential collisions, we construct a
i in the x and y axes, respectively, and v and
safety region R¯h ⊂ R2, that must be avoided by x,i
v are the corresponding velocities; the input
y,i
therobots,byenlargingthepolytoperepresenting u = [a ,a ]⊤, ∀i ∈ Ina, is comprised of
i x,i y,i 1
thebodyofthehumantoaccountforanypossible
the corresponding accelerations. Although simpli-
directionofmovementatthenextstep(Figure4).
fied, this choice of dynamics model for motion
Define, ∀i∈Ina, ∀ℓ∈IN¯,
1 0 planning has been successfully demonstrated in
experiments with real robots using MPC-MIP
P (k+1)≜O¯∪R¯h(k)∪
(cid:91)na
R (k+1) (11) setups [5]. The velocity and acceleration of the
i j
robots are bounded to the intervals [0,2.5] m/s
j=1
j̸=i and [−5,5] m/s2, respectively. A circular connec-
tivity region with radius 4 m is approximated by
as the union of the sets representing the obsta- a regular octagonal polytope inscribed in the cir-
cles, the body of the human, and the bodies of cle C ⊂ R2, following [5]. The width and length
robotsfori̸=j,i.e.,theregionsthatrobotiisnot of the robots, human, and operation region are
allowed to occupy over time. Then, we adapt the 0.5×0.5m,0.41×0.29m,and7×7m,respectively.
free operation region (5) used in constraint (10h) The weights of (10a) were selected as γ = 0.01
u
9and γ t = 5, ∀t ∈ I 14, i.e., all targets are equally Target A Target B Robot 1
valuable for the robot team.
Obstacles Visited target Robot 2
AsstatedinSection2,weemployedasynthetic
Terminal Redundant visit Human
agent to represent the human, which operates
independentlyfromtherobots.Itsuseallowsfora 2 2
deeper evaluation of the proposed scheme as data
T T
can be generated to study the method in a multi-
tudeofcircumstances.Theactionsofthesynthetic
agent are determined by a policy π∗ : S → ∆ .
N
The dataset D used in the AIRL training process
1 1
discussed in Section 3.1 is generated using π∗.
4 4
The effectiveness of ARMCHAIR was evalu-
ated by comparing it to two baselines: 3 3
a) open-loop MIP: An open-loop version of our a) 0 s b) 21 s
algorithm, where the optimization is solved Fig. 5 Example simulation of the open-loop MIP base-
once at the start of the mission considering line in Environment 1 (Sparse target distribution): initial
a)andfinalb)timestepsforsimulation2outof1000.Solid
themostlikelytrajectoryofthehumangiven
and dashed lines represent the actual motion and predic-
by πˆ; tions of each agent, respectively. The human prediction is
b) No robot support: The human performs the computedusingonlytheinitialconditions(open-loop)and
task without robot support. πˆ.ThepredictionsuggeststhatonlytargetsoftypeBwill
bevisitedbythehumanandtherobotsaredispatchedto
A comparison to the baseline without any
the remaining ones. In b) we observe redundant visits to
robotsupportallowedustoevaluatetheinfluence targets 3 and 4 (Type A) since the human deviates from
ofproperrobotsupportonthetask.Furthermore, theprediction.
by comparing ARMCHAIR with an open-loop
4.1 Environment 1: Sparse target
MIPapproach,weaimedtoinvestigatetheimpact
oftheclosed-loopstrategyemployedbytheformer distribution
method,wheretrajectoriesanddecisionsareperi-
We found that in the sparse environment, open-
odically updated, as opposed to the latter, where
loop MIP produces frequent redundant visits to
a single optimization is performed at the start of
targetsincaseswhenthehumandeviatedfromthe
the mission.
initiallymostlikelycourseofaction.Forinstance,
Since the actions of the simulated human are
insimulation2outof1000attheinitialtimestep
stochastic, we performed a Monte Carlo evalu-
(Figure 5a) the learned policy πˆ predicted a tra-
ation considering 1000 simulations in each envi-
jectory where the human visits only the preferred
ronment. To illustrate the performance of ARM-
targets 1 and 2, ignoring the remaining ones. In
CHAIRcomparedtothebaselinemethodsineach
response, the open-loop MIP dispatched robot 2
environment,wefirsthighlightseveralrepresenta-
to visit targets 3 and 4, while robot 1 maintained
tive simulations in detail (recordings of all 1000
the connectivity between the human and robot 2.
simulations are available in the supplementary
However,after21s(Figure5b)thehumandrifted
material2). We then present statistical analysis
from the initial prediction by visiting all targets,
across all simulations considering the following
resulting in redundant visits of targets 3 and 4
metrics: average number of collisions, network
whichwerereachedbothbythehumanandrobot
disconnections,targetsvisited,andredundanttar-
2.
get visitations, where a target is visited twice in
Conversely, we observed that the redundant
the same simulation due to a conflict in target
visits were completely prevented when the agents
allocation.
operated using the ARMCHAIR algorithm, as
illustrated by simulations 41 and 111 out of 1000
inFigures6ato6h.Simulation41,showsasignif-
icant detour (Figure 6b and 6c) where the human
2https://gitlab.com/caregnatonetoopen/ARMCHAIR
10Target A Target B Robot 1
Obstacles Visited target Robot 2
Terminal Redundant visit Human
Environment 1 - Simulation 41 out of 1000
2 2 2 2
T T T T
1 1 1 1
4 4 4 4
3 3 3 3
a) 0 s b) 8 s c) 9 s d) 22 s
Environment 1 - Simulation 101 out of 1000
2 2 2 2
T T T T
1 1 1 1
4 4 4 4
3 3 3 3
e) 0 s f) 12 s g) 13 s h) 25 s
Fig. 6 ExamplesimulationofARMCHAIRinEnvironment1(Sparsetargetdistribution):Fourtimestepsofsimulations
41 and 111 out of 1000. Solid and dashed lines represent the motion and predictions of each agent, respectively. The
ARMCHAIRalgorithmallowsthereplanningoftherobot’strajectoriesandtargetassignments,preventinganyredundant
visitswhenthehumandeviatesfromtheinitialprediction.
Table 1 PerformanceofARMCHAIRcomparedtothebaselinemethodsinEnvironment1(Sparsetargetdistribution).
Eachmetricisrepresentedbymeanand95%bootstrapconfidenceintervals(CI)for1000simulations.
Collisions Disconnections Targets Redundant visits
Mean 0.001 0.0 4.0 0.0
ARMCHAIR
CI (0.0,0.003) (0.0,0.0) (4.0,4.0) (0.0,0.0)
Mean 0.58 0.02 4.0 0.43
Open-loop MIP
CI (0.53,0.63) (0.01,0.03) (4.0,4.0) (0.40,0.47)
Mean 2.7
No robot support N\A N\A N\A
CI (2.6,2.7)
demonstratesitsintentiontovisittarget3bymov- visits target 3 but also decides to capture target
ing downwards instead of upwards. ARMCHAIR 4 (Figures 6f and 6g). As a result, ARMCHAIR
promptlyupdatesthetrajectoriesofrobot1,stop- updatesthetrajectoryofrobot2,steeringitaway
pingitforafewtimestepsallowingthehumanto from the bottom right-hand side region, clearing
move away from the region around target 4, and thepathforthehumantofinishthemissionwhile
then dispatching robot 2 to visit it. Simulation robot 1 preserves the connectivity of the group.
101 depicts a similar situation, where the human
11Confirming the insights based on represen- Target A Target B Robot 1
tative simulations, ARMCHAIR demonstrated Obstacles Visited target Robot 2
superior performance compared to both baselines
Terminal Redundant visit Human
across all 1000 simulations (Table 1). In partic-
ular, ARMCHAIR provided proper responses to
the uncertainty in human behavior, as no net-
4 T 4 T
work disconnections or redundant visits occurred,
2 2
whereas such issues were present in the open-
loop MIP case. The average number of redundant 3 3
visits was particularly high (0.43) for open-loop
1 1
MIP, due to the incapability of the open-loop
planner to update trajectories and allocation of
tasksinreal-time.ARMCHAIRalsoproducedfew
collisions (on average 0.001 collisions per simula- a) 0 s b) 12 s
tion) compared to the open-loop MIP (0.58 colli- Fig.7 Examplesimulationoftheopen-loopMIPbaseline
inEnvironment2(Groupedtargetdistribution):initiala)
sions per simulation) which was unable to adapt
and final b) time steps for simulation 1 out of 1000. Solid
to the human deviating from the initially esti-
and dashed lines represent the actual motion and predic-
matedtrajectory.Intermsofefficiency,alltargets tions of each agent, respectively. The human prediction is
werevisitedwhentheopen-loopandARMCHAIR computedusingonlytheinitialconditions(open-loop)and
πˆ.Thepredictionsuggeststhatalltargetswillbevisitedby
planners were used, as opposed to the case where
thehumanandtherobotsaredispatchedonlytomaintain
the human operated alone (2.7). However, only
connectivity. In b) we observe that the human decides to
ARMCHAIR prevented redundant visits. ignorethetargetsoftypeAdeviatingfromtheinitialpre-
diction.Asaresult,thehuman-robotteamprovidesinferior
performanceinthemission.
4.2 Environment 2: Grouped target
distribution
decidesnottocapturetarget2eventhoughitwas
In the environment with grouped targets, the ini- properly positioned to do so. By moving upwards
tialpredictionfromthelearnedpolicy(Figure7a) at time step 15 (Figure 8c) the human indirectly
indicates that the human is likely to visit all tar- declaresitsintentiontofinishthemissionwithout
getsintheenvironmentsincetheyareclosetoeach visiting the remaining targets, and the predic-
other. Consequently, the robots are dispatched tions are readily updated with the human going
only to maintain connectivity. After 12 seconds directly towards the terminal cell. Even with this
(Figure 7b) we observed that the human deviates unexpected behavior of the human and the small
from the predictions, choosing to visit only tar- window of time remaining in the mission, ARM-
gets 1 and 2 and finish the mission by reaching CHAIR was able to update trajectories for robots
the terminal region. This result illustrates a dis- 1 and 2 such that targets 4 and 2 were visited
tinct issue with the open-loop MIP approach: if (Figure 8d).
in the initial prediction all targets are assigned to In simulation 184 out of 1000, the human
be likely visited by the human and a deviation presented a more conventional behavior, visiting
occurs,therobotteamisunabletoupdateitsdeci- the preferred targets of type B (Figure 8f) but
sions and motion to compensate for unexpected againdeviatingfromtheinitialprediction.Figure
human behavior, yielding worse performance in 8g shows that after visiting target 2, the human
the mission. moves towards the terminal cell and the predic-
The simulation results in Figure 8 show how tionsareupdatedaccordingly.ARMCHAIRreacts
ARMCHAIR is able to enhance the performance by planning a trajectory for robot 2 in which it
in the mission even when the human behavior visits the remaining targets 3 and 4, while robot
predictions are imperfect. In simulation 25 out of 1 maintains connectivity.
1000, the prediction that the human will visit all These observations are supported by the sta-
targets is maintained until time step 14 (Figure tisticalresultspresentedinTable2.Asinthefirst
8b), and the robots operate very similarly to the environment, ARMCHAIR was able to maintain
open-loop MIP case. Unexpectedly, the human the network connected in all simulations, whereas
12Target A Target B Robot 1
Obstacles Visited target Robot 2
Terminal Redundant visit Human
Environment 2 - Simulation 25 out of 1000
4 T 4 T 4 T 4 T
2 2 2 2
3 3 3 3
1 1 1 1
a) 0 s
a) 0 s b) 14 s c) 15 s d) 17 s
Environment 2 - Simulation 184 out of 1000
4 T 4 T 4 T 4 T
2 2 2 2
3 3 3 3
1 1 1 1
e) 0 s f) 12 s g) 13 s h) 17 s
Fig.8 ExamplesimulationofARMCHAIRinEnvironment2(Groupedtargetdistribution):Fourtimestepsofsimulations
25and184outof1000.Solidanddashedlinesrepresentthemotionandpredictionsofeachagent,respectively.Inthiscase,
thereplanningcapabilitiesprovidedbytheARMCHAIRalgorithmallowtherobotstoquicklyadapttocompensateforan
unexpected change in the behavior of the human, who decides to ignore targets that are easily accessible. As a result, all
targetsarevisitedattheendofthemission.
Table 2 PerformanceofARMCHAIRcomparedtothebaselinemethodsinEnvironment2(Groupedtarget
distribution).Eachmetricisrepresentedbymeanand95%bootstrapconfidenceintervals(CI)for1000simulations.
Collisions Disconnections Targets Redundant visits
Mean 0.013 0.0 3.444 0.0
ARMCHAIR
CI (0.007,0.02) (0.0,0.0) (3.398,3.489) (0.0,0.0)
Mean 0.004 0.181 2.384 0.0
Open-loop MIP
CI (0.001,0.008) (0.157,0.205) (2.354,2.415) (0.0,0.0)
Mean 2.393
No robot support N\A N\A N\A
CI (2.362,2.424)
an average of 0.181 disconnections occurred when than the open-loop algorithm (2.384). Better per-
the open-loop MIP scheme was used. Although formance in terms of average reward collection is
no redundant visits were registered with either achieved with the robot team is used as opposed
approach, ARMCHAIR provides a higher aver- tothecasewithoutsupport,forwhichtheaverage
age number of target reward collections (3.444) was (2.393).
13WealsoobservedthatARMCHAIRperformed planning and decision-making has been success-
worse in terms of collision avoidance in environ- fully addressed in [43] and [45], with the latter
ment 2. This result stems from the increased work thoroughly demonstrating its effectiveness
complexity of the motion planning problem in throughexperimentswithhumans.However,both
this scenario due to the tight placement of tar- of these solutions are still dependent on frequent
gets, which requires the group to operate in close human inputs. In [45], for example, the necessity
proximity. Furthermore, the results indicate that of the human operator to divide their attention
the open-loop MIP scheme outperformed ARM- between sending messages to the robot team and
CHAIR in terms of collision avoidance in this working on the mission was reported as an open
particular environment. This occurs because the issue. ARMCHAIR addresses this gap by actively
robot team is not dispatched to visit any tar- predicting and adapting to the operator based
gets when the open-loop approach is employed. solelyontheobservedhumanbehavior.Thus,our
As a result, the robots typically remain far away approach has an extra degree of autonomy that
from the human and collisions are less likely. allows the operator to concentrate on perform-
Conversely,ARMCHAIRconsistentlyupdatesthe ingthemission,withoutthenecessityofexplicitly
trajectoriesanddecisionsoftherobotstoimprove directing the actions of the robot team.
performance. Consequently, they are occasion- Duetothenoveltyoftheproposedintegration,
ally steered towards targets closer to the human, ARMCHAIR is still bound by a few limitations.
increasing the likelihood of collisions. For example, its receding horizon formulation
lacks a formal demonstration of recursive feasibil-
5 Discussion and Conclusion ity and has no active method to handle unfea-
sibility, simply commanding the robots to stop
This paper presented ARMCHAIR, a novel moving until the problem becomes feasible again.
architecture for human-robot collaboration that Inaddition,asacentralizedscheme,ourapproach
leverages the integration of AIRL and MPC. may have scalability issues when larger teams are
Through extensive Monte-Carlo simulation trials, considered.Thesimulation-basedevaluationspre-
wedemonstratedthattheschemeprovidesproper sentedillustratethattheintegrationofMPC-MIP
adaptive trajectories and decision-making, in the and AIRL for human-robot teaming algorithms
form of target allocations, for a group of robots is promising. However, further investigation with
supporting a human in an exploration mission. thorough experimental demonstrations involving
ARMCHAIR was successful in both: a) coor- real humans, as performed in [45] considering
dinatingthemotionoftherobotteam,preventing similar exploration missions, is still required.
most collisions and network disconnections; and Infutureworks,theinvestigationofconditions
b)autonomouslyidentifyingwhentoprovidesup- for formal demonstrations of theoretical prop-
port based on the observed human behavior and erties, in particular recursive feasibility, should
current mission circumstances. This was made be addressed. Reactive strategies, such as inner
possiblebytheadaptivedecision-makingprovided potential field control laws, could be devised to
bytheMPC-MIPscheme,whichimprovedperfor- handle the occasional collisions that occur due to
mance by preventing overlaps in the allocation of infeasibleoptimizationproblems.Thehumanpre-
targets to robots and the human. dictionmodelcanberefinedtoaddressmorecom-
The achieved results illustrate that ARM- plex concepts, such as intentions and visual per-
CHAIR provides distinct capabilities compared ceptions;themodelingofthehumanmotioncould
to most of the literature on human-robot col- alsobeimproved,allowingforsmoothermovement
laboration with mobile robots. Excellent solu- predictionsthatwouldreducetheconservatismin
tions for social navigation (coordination) between the MRS trajectories. Alternative methodologies
robots and humans are found in [6, 7, 37, 42]. for robustness could also be explored, reducing
However, the scenarios considered in this paper conservatism by tolerating risk. Distributed MIP
require the extra capability of harmonious collab- schemescouldbeinvestigatedtoaddressthescala-
oration towards the same global objective. The bilityissues.Theuseofdemonstrationswithdirect
ensuing joint problem of human-robot trajectory
14interaction between robots and humans can pro- Table A1 Ground-truthrewardsand
penalizationsforthesynthetichumanagent.
vide enhanced capabilities to the human predic-
tionmodel.Experimentaltrialsusingrealhumans
RA RB RC RM RF
and robots can further validate the effectiveness
Value 0.5 1.0 -1.0 -0.1 -20
of the proposal.
Overall, this work demonstrates that the inte-
gration of AIRL and MPC-MIP in ARMCHAIR Appendix A Synthetic
yields a platform that already provides notable
human agent
capabilities for human-robot collaboration and
can be further refined to become an enabling
As a surrogate for the human, we employ a rein-
technology for safe, harmonious, and efficient
forcement learning agent trained using Proximal
human-robot teaming in the future.
PolicyApproximation(PPO)onthegroundtruth
Supplementary information. All the algo- rewardsdetailedinTableA1.Itsactionsrepresent
rithmsandresultingdatarelatedtothispaperare movementinthenorth,south,west,andeast direc-
available at the repository3. tions, as well as a collection that allows the agent
tocapturetargetrewards.Therelativevalueofthe
Author Contributions. ACN devised the
targetstothehumanisrepresentedbytherewards
researchquestion,performedtheliteraturereview,
R >0 and R >0, with R >R . Thus, it has
prepared all figures and tables, wrote the new A B B A
a preference for the type B. The rewards can only
code required, devised the simulation environ-
be collected at most once. There are three types
ments, evaluated results, and wrote all sections.
ofpenalization:collisionswithwallsandobstacles,
LCS,AZ,MROAM,andRJMAprovided support
movement, and failure in reaching the terminal
indeterminingtheresearchquestionandperform-
region in the required number of steps, they are
ing the literature review, suggested the addition
represented by the quantities R < 0, R < 0,
of figures, helped in the determination of simu- C M
and R <0, respectively.
lation environments and evaluation methodology, F
We employ the same PPO setup as in [33]
and contributed to the writing and proofreading
with γ = 0.999. The training employs new ran-
of all sections.
domly generated environments similar to the one
Funding. This study was financed in part by depicted in Figure 6. For each environment, the
the Coordena¸c˜ao de Aperfei¸coamento de Pessoal maximum number of targets is n ∼ U(0,4). The
t
de N´ıvel Superior - Brasil (CAPES) - Finance number of target of type B and A is computed
Code 001. Marcos Maximo is partially funded as n ∼U(0,n ) and n =n −n , respectively.
B t A t B
by CNPq – National Research Council of Brazil The initial position is also randomized for each
through the grant 307525/2022-8. The participa- scenario,beinguniformlydistributedoverallcells
tion of Luciano Cavalcante Siebert and Arkady that are not targets or obstacles. The position of
Zgonnokov in this research was partially sup- the obstacles and terminal region remains fixed.
ported by TAILOR, a project funded by EU The human does not consider the robot’s support
Horizon 2020 research and innovation programme explicitly but rather reacts indirectly, through
under GA No 952215. their impact on the environment, e.g., capture of
targets.
Declarations Afterrunningthealgorithmfor16×105 steps,
the synthetic agent’s policy π∗ :S →∆ is com-
N
Conflict of interest. The authors declare that puted,where∆ isaprobabilitydistributionover
A
they have no conflict of interest or compet- all actions. Finally, π∗ is used to generate a set
ing financial interests or personal relationships of 8000 trajectories, D, representing the human
that could have appeared to influence the work navigation in different environments.
reported in this paper.
3https://gitlab.com/caregnatonetoopen/ARMCHAIR
15Table B2 AIRL
hyperparameters. Table B3 Comparisonbetweenrecoveredandsurrogate
humanpolicies.
Hyperparameter Value
Learn.ratediscr. 4e−4 Synthetic human agent
Learn.rategen. 4e−4 Average ConfidenceInterval
Batchsizediscr. 256 Trajectorylength 12.7 (12.3,13.0)
Batchsizegen. 8 Tar.Acollection 0.68 (0.63,0.73)
Environmentsteps 3.5e6 Tar.Bcollection 1.09 (1.02,1.16)
ϵ-clip 0.1 Collisions 0.0 (0.00,0.00)
γ 0.999 Return 0.05 (0.01,0.09)
PPOepochs 5
AIRL recovered
Trajectorylength 12.3 (11.9,12.7)
Tar.Acollection 0.61 (0.56,0.66)
Appendix B AIRL training
Tar.Bcollection 0.81 (0.75,0.87)
Collisions 0.0 (0.00,0.00)
TableB2presentsthehyperparametersusedinthe Return -0.85 −(1.01,0.70)
AIRL training. We evaluate the performance of
therecoveredpolicybycomparingthetrajectories
generated by πˆ and π∗ in 1000 new environments
[3] Gervasi, R., Barravecchia, F., Mastrogia-
built randomly using the same procedure dis-
como, L. & Franceschini, F. Applications
cussed in Appendix A. Table B3 summarizes the
of affective computing in human-robot inter-
resultsintermsofaveragelengthofthetrajectory,
action: State-of-art and challenges for man-
number of targets of type A and B visited, colli-
ufacturing. Proceedings of the Institution
sions, and return over the ground truth rewards
of Mechanical Engineers, Part B: Journal
considering the trajectories achieved using the
of Engineering Manufacture 237, 815–832
synthetichumanagentandtherecoveredpolicies.
(2023).
The results show that the policy recovered by
[4] Mavrogiannis, C. et al. Core challenges of
theAIRLalgorithmgeneratesusefulpredictionsof
social robot navigation: A survey. J. Hum.-
thehumanagent.Theresultingaveragetrajectory
Robot Interact. 12 (2023).
lengths are very similar, suggesting that the pre-
[5] Caregnato-Neto, A., Maximo, M. R. O. A.
diction model accurately captures the “urgency”
& Afonso, R. J. M. Real-time motion plan-
ofthehumaninfinishingthemission.Theobstacle
ning and decision-making for a group of
avoidance capability is also appropriately emu-
differential drive robots under connectivity
lated with no collisions occurring in both cases.
constraints using robust MPC and mixed-
However, there are mismatches in the target col-
integerprogramming. Advanced Robotics 37,
lection averages, in particular targets of type B.
356–379 (2023).
Therecoveredmodelshowsalessgreedybehavior,
[6] Ozkan,M.F.&Ma,Y.Distributedstochastic
with smaller target collection averages.
model predictive control for human-leading
heavy-duty truck platoon. IEEE Transac-
References tions on Intelligent Transportation Systems
23, 16059–16071 (2022).
[1] Annaswamy, A. M., Johansson, K. H. & [7] Zhu, H., Claramunt, F. M., Brito, B. &
Pappas, G. J. Control for Societal-scale Alonso-Mora, J. Learning interaction-aware
Challenges: Road Map 2030 (IEEE Control trajectorypredictionsfordecentralizedmulti-
Systems Society Publication, 2023). robot motion planning in dynamic environ-
[2] Liang, C.-J., Wang, X., Kamat, V. R. & ments. IEEE Robotics and Automation Let-
Menassa, C. C. Human–robot collaboration ters 6, 2256–2263 (2021).
in construction: Classification and research [8] Camacho,E.F.,Bordons,C.,Camacho,E.F.
trends. Journal of Construction Engineering & Bordons, C. Model predictive controllers
and Management 147, 03121006 (2021). (Springer, 2007).
[9] Liu, Y., Liu, K., Wang, G., Sun, Z. & Jin,
16L. Noise-tolerantzeroingneurodynamicalgo- [19] Nascimento, I. B. P., Ferramosca, A.,
rithm for upper limb motion intention-based Pimenta, L. C. A. & Raffo, G. V. NMPC
human–robotinteractioncontrolinnon-ideal strategy for a quadrotor UAV in a 3D
conditions. Expert Systems with Applications unknown environment, 19th International
213, 118891 (2023). Conference on Advanced Robotics (ICAR),
[10] Zheng, H., , Smereka, M. J. & Mikulski, 179–184 (2019).
D. Bayesian optimization based trust model [20] Ramalho, G. M., Carvalho, S. R., Finardi,
for human multi-robot collaborative motion E. C. & Moreno, U. F. Trajectory optimiza-
tasks in offroad environments. International tion using sequential convex programming
Journal of Social Robotics 1181–1201 (2023). with collision avoidance. Journal of Con-
[11] Schumann, J. F., Srinivasan, A. R., Kober, trol, Automation and Electrical Systems 29,
J., Markkula, G. & Zgonnikov, A. Using 318–327 (2018).
models based on cognitive theory to predict [21] Richards, A. & How, J. P. Aircraft trajectory
humanbehaviorintraffic:Acasestudy,IEEE planningwithcollisionavoidanceusingmixed
26th International Conference on Intelligent integerlinearprogramming,Vol.3ofProceed-
Transportation Systems (ITSC), 5870–5875 ings of the 2002 American Control Confer-
(2023). ence, 1936–1941 (Anchorage, USA, 2002).
[12] Liu, W., Liang, X. & Zheng, M. Task- [22] Afonso, R. J. M., Maximo, M. R. O. A. &
constrained motion planning considering Galv˜ao, R. K. H. Task allocation and trajec-
uncertainty-informed human motion predic- toryplanningformultipleagentsinthepres-
tionforhuman–robotcollaborativedisassem- ence of obstacle and connectivity constraint
bly. IEEE/ASME Transactions on Mecha- with mixed-integer linear programming. Int.
tronics 1–8 (2023). J. Robust Nonlinear Control 30, 5464–5491
[13] Jin, Z., Liu, A., Zhang, W.-A., Yu, L. & Su, (2020).
C.-Y. A learning based hierarchical control [23] Ng, A. & Russell, S. Algorithms for inverse
framework for human–robot collaboration. reinforcement learning, Vol. 1 of ICML, 2
IEEE Transactions on Automation Science (2000).
and Engineering 20, 506–517 (2023). [24] Town, J., Morrison, Z. & Kamalapurkar,
[14] Fuchs,A.,Passarella,A.&Conti,M. Model- R. Pilot performance modeling via observer-
ing,replicating,andpredictinghumanbehav- based inverse reinforcement learning (2023).
ior: A survey. ACM Trans. Auton. Adapt. 2307.13150.
Syst. 18 (2023). [25] Sadigh,D.,Landolfi,N.,Sastry,S.S.,Seshia,
[15] Ullman, D. & Malle, B. F. Human-robot S.A.&Dragan,A.D. Planningforcarsthat
trust: Just a button press away, Proceedings coordinate with people: leveraging effects on
of the Companion of the 2017 ACM/IEEE human actions for planning and active infor-
International Conference on Human-Robot mation gathering over human internal state.
Interaction, 309–310 (2017). Autonomous Robots 42, 1405–1426 (2018).
[16] Cutler, C. R. & Ramaker, B. L. Dynamic [26] Fuchs,A.,Passarella,A.&Conti,M. Model-
matrixcontrol:Acomputercontrolalgorithm, ing,replicating,andpredictinghumanbehav-
no. 17 in Joint Automatic Control Confer- ior: A survey. ACM Trans. Auton. Adapt.
ence, 72 (1980). Syst. 18 (2023).
[17] Ferranti, L., Lyons, L., Negenborn, R. R., [27] Avaei, A., van der Spaa, L., Peternel, L. &
Keviczky, T. & Alonso-Mora, J. Distributed Kober, J. An incremental inverse reinforce-
nonlinear trajectory optimization for multi- ment learning approach for motion planning
robot motion planning. IEEE Transactions withseparatedpathandvelocitypreferences.
on Control Systems Technology 31, 809–824 Robotics 12 (2023).
(2023). [28] Perrusqu´ıa, A. & Guo, W. Drone’s objective
[18] Ioan, D., Prodan, I., Olaru, S., Stoican, F. & inference using policy error inverse reinforce-
Niculescu, S. Mixed-integer programming in mentlearning. IEEE Transactions on Neural
motionplanning. Annu.Rev.Control (2020). Networks and Learning Systems 1–12(2023).
17[29] Fu, J., Luo, K. & Levine, S. Learning robust sampling-based MPC with learned local goal
rewards with adversarial inverse reinforce- predictions (2023). 2309.14931.
ment learning, International Conference on [39] Boldrer, M., Antonucci, A., Bevilacqua,
Learning Representations (2018). P., Palopoli, L. & Fontanelli, D. Multi-
[30] Ziebart, B. D., Maas, A. L., Bagnell, J. A., agent navigation in human-shared environ-
Dey, A. K. et al. Maximum entropy inverse ments: A safe and socially-aware approach.
reinforcementlearning.,Vol.8ofProc.AAAI Robotics and Autonomous Systems 149,
Conf. Artif. Intell., 1433–1438 (Chicago, IL, 103979 (2022).
USA, 2008). [40] Kim, B. & Pineau, J. Socially adaptive
[31] Sestini, A., Kuhnle, A. & Bagdanov, path planning in human environments using
A. D. Demonstration-efficient inverse rein- inverse reinforcement learning. Int J of Soc
forcement learning in procedurally generated Robotics 8, 51–66 (2016).
environments, 2021 IEEE Conference on [41] Galvan, M., Repiso, E. & Sanfeliu, A.
Games (CoG), 1–8 (IEEE, 2021). Robot navigation to approach people using b-
[32] Yu, L., Song, J. & Ermon, S. Chaudhuri, K. splinepathplanningandextendedsocialforce
&Salakhutdinov,R.(eds)Multi-agentadver- model, Robot 2019: Fourth Iberian Robotics
sarial inverse reinforcement learning. (eds Conference, 15–27 (2020).
Chaudhuri, K. & Salakhutdinov, R.) Pro- [42] Wang, C., Li, Y., Ge, S. S. & Lee, T. H.
ceedings of the 36th International Conference Adaptive control for robot navigation in
on Machine Learning, Vol. 97 of Proceedings human environments based on social force
of Machine Learning Research, 7194–7201 model, 2016 IEEE International Conference
(PMLR, 2019). on Robotics and Automation (ICRA), 5690–
[33] Peschl, M., Zgonnikov, A., Oliehoek, F. A. 5695 (2016).
& Siebert, L. C. MORAL: Aligning ai with [43] Hooi Chan, T. et al. A robotic system of sys-
human norms through multi-objective rein- tems for human-robot collaboration in search
forced active learning, Proc. of the 21st Int. and rescue operations, 2023 IEEE/ASME
Conf.onAutonomousAgentsandMultiagent International Conference on Advanced Intel-
Systems, 1038–1046 (2022). ligent Mechatronics (AIM), 878–885 (2023).
[34] Cavalcante Siebert, L. et al. Meaningful [44] Wampler,J.,Li,B.,Mosciki,T.&vonEllen-
human control: actionable properties for ai rieder, K. D. Towards adjustable autonomy
system development. AI and Ethics 3, 241– for human-robot interaction in marine sys-
255 (2023). tems,OCEANS2017-Aberdeen,1–7(2017).
[35] Wang, P., Liu, D., Chen, J., Li, H. & Chan, [45] Dalmasso, M. et al. Shared task represen-
C.-Y. Decision making for autonomous driv- tationforhuman–robotcollaborativenaviga-
ing via augmented adversarial inverse rein- tion: The collaborative search case. Interna-
forcement learning, 2021 IEEE International tional Journal of Social Robotics (2023).
Conference on Robotics and Automation [46] Richards, A. & How, J. Mixed-integer pro-
(ICRA), 1036–1042 (IEEE, 2021). gramming for control, Proceedings of the
[36] Lee, K., Isele, D., Theodorou, E. A. & Bae, 2005 American Control Conference, 2676–
S. Spatiotemporal costmap inference for 2683 vol. 4 (2005).
MPC via deep inverse reinforcement learn- [47] Baotic, M. Polytopic computations in con-
ing. IEEE Robotics and Automation Letters strained optimal control. Automatika 50,
7, 3194–3201 (2022). 119–134 (2009).
[37] Lu, J., Hossain, S., Sheng, W. & Bai, [48] You, Y., Thomas, V., Colas, F., Alami, R. &
H. Cooperative driving in mixed traffic of Buffet,O. Robustrobotplanningforhuman-
manned and unmanned vehicles based on robot collaboration (2023). 2302.13916.
human driving behavior understanding, 2023 [49] Kahneman, D. Maps of bounded rational-
IEEE International Conference on Robotics ity:Psychologyforbehavioraleconomics.The
and Automation (ICRA), 3532–3538 (2023). American Economic Review 93, 1449–1475
[38] Jansma, W., Trevisan, E., A´lvaro Serra- (2003).
G´omez & Alonso-Mora, J. Interaction-aware
18[50] Shah, R., Gundotra, N., Abbeel, P. & Dra-
gan, A. On the feasibility of learning,
rather than assuming, human biases for
rewardinference,InternationalConferenceon
MachineLearning,5670–5679(PMLR,2019).
[51] Schweidel, K. S., Koehler, S. M., Desaraju,
V. R. & Bari´c, M. Driver-in-the-loop contin-
gency MPC with invariant sets, 2022 Euro-
pean Control Conference (ECC), 808–813
(2022).
19