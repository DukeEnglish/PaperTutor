[
    {
        "title": "Understanding Iterative Combinatorial Auction Designs via Multi-Agent Reinforcement Learning",
        "authors": "Greg d'EonNeil NewmanKevin Leyton-Brown",
        "links": "http://arxiv.org/abs/2402.19420v1",
        "entry_id": "http://arxiv.org/abs/2402.19420v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19420v1",
        "summary": "Iterative combinatorial auctions are widely used in high stakes settings such\nas spectrum auctions. Such auctions can be hard to understand analytically,\nmaking it difficult for bidders to determine how to behave and for designers to\noptimize auction rules to ensure desirable outcomes such as high revenue or\nwelfare. In this paper, we investigate whether multi-agent reinforcement\nlearning (MARL) algorithms can be used to understand iterative combinatorial\nauctions, given that these algorithms have recently shown empirical success in\nseveral other domains. We find that MARL can indeed benefit auction analysis,\nbut that deploying it effectively is nontrivial. We begin by describing\nmodelling decisions that keep the resulting game tractable without sacrificing\nimportant features such as imperfect information or asymmetry between bidders.\nWe also discuss how to navigate pitfalls of various MARL algorithms, how to\novercome challenges in verifying convergence, and how to generate and interpret\nmultiple equilibria. We illustrate the promise of our resulting approach by\nusing it to evaluate a specific rule change to a clock auction, finding\nsubstantially different auction outcomes due to complex changes in bidders'\nbehavior.",
        "updated": "2024-02-29 18:16:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19420v1"
    },
    {
        "title": "ARMCHAIR: integrated inverse reinforcement learning and model predictive control for human-robot collaboration",
        "authors": "Angelo Caregnato-NetoLuciano Cavalcante SiebertArkady ZgonnikovMarcos Ricardo Omena de Albuquerque MaximoRubens Junqueira Magalhães Afonso",
        "links": "http://arxiv.org/abs/2402.19128v1",
        "entry_id": "http://arxiv.org/abs/2402.19128v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19128v1",
        "summary": "One of the key issues in human-robot collaboration is the development of\ncomputational models that allow robots to predict and adapt to human behavior.\nMuch progress has been achieved in developing such models, as well as control\ntechniques that address the autonomy problems of motion planning and\ndecision-making in robotics. However, the integration of computational models\nof human behavior with such control techniques still poses a major challenge,\nresulting in a bottleneck for efficient collaborative human-robot teams. In\nthis context, we present a novel architecture for human-robot collaboration:\nAdaptive Robot Motion for Collaboration with Humans using Adversarial Inverse\nReinforcement learning (ARMCHAIR). Our solution leverages adversarial inverse\nreinforcement learning and model predictive control to compute optimal\ntrajectories and decisions for a mobile multi-robot system that collaborates\nwith a human in an exploration task. During the mission, ARMCHAIR operates\nwithout human intervention, autonomously identifying the necessity to support\nand acting accordingly. Our approach also explicitly addresses the network\nconnectivity requirement of the human-robot team. Extensive simulation-based\nevaluations demonstrate that ARMCHAIR allows a group of robots to safely\nsupport a simulated human in an exploration scenario, preventing collisions and\nnetwork disconnections, and improving the overall performance of the task.",
        "updated": "2024-02-29 13:06:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19128v1"
    },
    {
        "title": "Facility Location Games with Scaling Effects",
        "authors": "Yu HeAlexander LamMinming Li",
        "links": "http://arxiv.org/abs/2402.18908v1",
        "entry_id": "http://arxiv.org/abs/2402.18908v1",
        "pdf_url": "http://arxiv.org/pdf/2402.18908v1",
        "summary": "We take the classic facility location problem and consider a variation, in\nwhich each agent's individual cost function is equal to their distance from the\nfacility multiplied by a scaling factor which is determined by the facility\nplacement. In addition to the general class of continuous scaling functions, we\nalso provide results for piecewise linear scaling functions which can\neffectively approximate or model the scaling of many real world scenarios. We\nfocus on the objectives of total and maximum cost, describing the computation\nof the optimal solution. We then move to the approximate mechanism design\nsetting, observing that the agents' preferences may no longer be single-peaked.\nConsequently, we characterize the conditions on scaling functions which ensure\nthat agents have single-peaked preferences. Under these conditions, we find\nresults on the total and maximum cost approximation ratios achievable by\nstrategyproof and anonymous mechanisms.",
        "updated": "2024-02-29 07:08:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18908v1"
    },
    {
        "title": "ELA: Exploited Level Augmentation for Offline Learning in Zero-Sum Games",
        "authors": "Shiqi LeiKanghoon LeeLinjing LiJinkyoo ParkJiachen Li",
        "links": "http://arxiv.org/abs/2402.18617v1",
        "entry_id": "http://arxiv.org/abs/2402.18617v1",
        "pdf_url": "http://arxiv.org/pdf/2402.18617v1",
        "summary": "Offline learning has become widely used due to its ability to derive\neffective policies from offline datasets gathered by expert demonstrators\nwithout interacting with the environment directly. Recent research has explored\nvarious ways to enhance offline learning efficiency by considering the\ncharacteristics (e.g., expertise level or multiple demonstrators) of the\ndataset. However, a different approach is necessary in the context of zero-sum\ngames, where outcomes vary significantly based on the strategy of the opponent.\nIn this study, we introduce a novel approach that uses unsupervised learning\ntechniques to estimate the exploited level of each trajectory from the offline\ndataset of zero-sum games made by diverse demonstrators. Subsequently, we\nincorporate the estimated exploited level into the offline learning to maximize\nthe influence of the dominant strategy. Our method enables interpretable\nexploited level estimation in multiple zero-sum games and effectively\nidentifies dominant strategy data. Also, our exploited level augmented offline\nlearning significantly enhances the original offline learning algorithms\nincluding imitation learning and offline reinforcement learning for zero-sum\ngames.",
        "updated": "2024-02-28 17:44:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18617v1"
    },
    {
        "title": "Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent Reinforcement Learning",
        "authors": "Zeyang LiuLipeng WanXinrui YangZhuoran ChenXingyu ChenXuguang Lan",
        "links": "http://arxiv.org/abs/2402.17978v1",
        "entry_id": "http://arxiv.org/abs/2402.17978v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17978v1",
        "summary": "Effective exploration is crucial to discovering optimal strategies for\nmulti-agent reinforcement learning (MARL) in complex coordination tasks.\nExisting methods mainly utilize intrinsic rewards to enable committed\nexploration or use role-based learning for decomposing joint action spaces\ninstead of directly conducting a collective search in the entire\naction-observation space. However, they often face challenges obtaining\nspecific joint action sequences to reach successful states in long-horizon\ntasks. To address this limitation, we propose Imagine, Initialize, and Explore\n(IIE), a novel method that offers a promising solution for efficient\nmulti-agent exploration in complex scenarios. IIE employs a transformer model\nto imagine how the agents reach a critical state that can influence each\nother's transition functions. Then, we initialize the environment at this state\nusing a simulator before the exploration phase. We formulate the imagination as\na sequence modeling problem, where the states, observations, prompts, actions,\nand rewards are predicted autoregressively. The prompt consists of\ntimestep-to-go, return-to-go, influence value, and one-shot demonstration,\nspecifying the desired state and trajectory as well as guiding the action\ngeneration. By initializing agents at the critical states, IIE significantly\nincreases the likelihood of discovering potentially important under-explored\nregions. Despite its simplicity, empirical results demonstrate that our method\noutperforms multi-agent exploration baselines on the StarCraft Multi-Agent\nChallenge (SMAC) and SMACv2 environments. Particularly, IIE shows improved\nperformance in the sparse-reward SMAC tasks and produces more effective\ncurricula over the initialized states than other generative methods, such as\nCVAE-GAN and diffusion models.",
        "updated": "2024-02-28 01:45:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17978v1"
    }
]