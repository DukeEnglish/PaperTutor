[
    {
        "title": "Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for Specialized Tasks",
        "authors": "Bálint MucsányiMichael KirchhofSeong Joon Oh",
        "links": "http://arxiv.org/abs/2402.19460v1",
        "entry_id": "http://arxiv.org/abs/2402.19460v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19460v1",
        "summary": "Uncertainty quantification, once a singular task, has evolved into a spectrum\nof tasks, including abstained prediction, out-of-distribution detection, and\naleatoric uncertainty quantification. The latest goal is disentanglement: the\nconstruction of multiple estimators that are each tailored to one and only one\ntask. Hence, there is a plethora of recent advances with different intentions -\nthat often entirely deviate from practical behavior. This paper conducts a\ncomprehensive evaluation of numerous uncertainty estimators across diverse\ntasks on ImageNet. We find that, despite promising theoretical endeavors,\ndisentanglement is not yet achieved in practice. Additionally, we reveal which\nuncertainty estimators excel at which specific tasks, providing insights for\npractitioners and guiding future research toward task-centric and disentangled\nuncertainty estimation methods. Our code is available at\nhttps://github.com/bmucsanyi/bud.",
        "updated": "2024-02-29 18:52:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19460v1"
    },
    {
        "title": "Listening to the Noise: Blind Denoising with Gibbs Diffusion",
        "authors": "David Heurtel-DepeigesCharles C. MargossianRuben OhanaBruno Régaldo-Saint Blancard",
        "links": "http://arxiv.org/abs/2402.19455v1",
        "entry_id": "http://arxiv.org/abs/2402.19455v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19455v1",
        "summary": "In recent years, denoising problems have become intertwined with the\ndevelopment of deep generative models. In particular, diffusion models are\ntrained like denoisers, and the distribution they model coincide with denoising\npriors in the Bayesian picture. However, denoising through diffusion-based\nposterior sampling requires the noise level and covariance to be known,\npreventing blind denoising. We overcome this limitation by introducing Gibbs\nDiffusion (GDiff), a general methodology addressing posterior sampling of both\nthe signal and the noise parameters. Assuming arbitrary parametric Gaussian\nnoise, we develop a Gibbs algorithm that alternates sampling steps from a\nconditional diffusion model trained to map the signal prior to the family of\nnoise distributions, and a Monte Carlo sampler to infer the noise parameters.\nOur theoretical analysis highlights potential pitfalls, guides diagnostic\nusage, and quantifies errors in the Gibbs stationary distribution caused by the\ndiffusion model. We showcase our method for 1) blind denoising of natural\nimages involving colored noises with unknown amplitude and spectral index, and\n2) a cosmology problem, namely the analysis of cosmic microwave background\ndata, where Bayesian inference of \"noise\" parameters means constraining models\nof the evolution of the Universe.",
        "updated": "2024-02-29 18:50:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19455v1"
    },
    {
        "title": "Heavy-Tailed Class Imbalance and Why Adam Outperforms Gradient Descent on Language Models",
        "authors": "Frederik KunstnerRobin YadavAlan MilliganMark SchmidtAlberto Bietti",
        "links": "http://arxiv.org/abs/2402.19449v1",
        "entry_id": "http://arxiv.org/abs/2402.19449v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19449v1",
        "summary": "Adam has been shown to outperform gradient descent in optimizing large\nlanguage transformers empirically, and by a larger margin than on other tasks,\nbut it is unclear why this happens. We show that the heavy-tailed class\nimbalance found in language modeling tasks leads to difficulties in the\noptimization dynamics. When training with gradient descent, the loss associated\nwith infrequent words decreases slower than the loss associated with frequent\nones. As most samples come from relatively infrequent words, the average loss\ndecreases slowly with gradient descent. On the other hand, Adam and sign-based\nmethods do not suffer from this problem and improve predictions on all classes.\nTo establish that this behavior is indeed caused by class imbalance, we show\nempirically that it persist through different architectures and data types, on\nlanguage transformers, vision CNNs, and linear models. We further study this\nphenomenon on a linear classification with cross-entropy loss, showing that\nheavy-tailed class imbalance leads to ill-conditioning, and that the\nnormalization used by Adam can counteract it.",
        "updated": "2024-02-29 18:47:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19449v1"
    },
    {
        "title": "Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality",
        "authors": "Siyu ChenHeejune SheenTianhao WangZhuoran Yang",
        "links": "http://arxiv.org/abs/2402.19442v1",
        "entry_id": "http://arxiv.org/abs/2402.19442v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19442v1",
        "summary": "We study the dynamics of gradient flow for training a multi-head softmax\nattention model for in-context learning of multi-task linear regression. We\nestablish the global convergence of gradient flow under suitable choices of\ninitialization. In addition, we prove that an interesting \"task allocation\"\nphenomenon emerges during the gradient flow dynamics, where each attention head\nfocuses on solving a single task of the multi-task model. Specifically, we\nprove that the gradient flow dynamics can be split into three phases -- a\nwarm-up phase where the loss decreases rather slowly and the attention heads\ngradually build up their inclination towards individual tasks, an emergence\nphase where each head selects a single task and the loss rapidly decreases, and\na convergence phase where the attention parameters converge to a limit.\nFurthermore, we prove the optimality of gradient flow in the sense that the\nlimiting model learned by gradient flow is on par with the best possible\nmulti-head softmax attention model up to a constant factor. Our analysis also\ndelineates a strict separation in terms of the prediction accuracy of ICL\nbetween single-head and multi-head attention models. The key technique for our\nconvergence analysis is to map the gradient flow dynamics in the parameter\nspace to a set of ordinary differential equations in the spectral domain, where\nthe relative magnitudes of the semi-singular values of the attention weights\ndetermines task allocation. To our best knowledge, our work provides the first\nconvergence result for the multi-head softmax attention model.",
        "updated": "2024-02-29 18:43:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19442v1"
    },
    {
        "title": "Negative-Binomial Randomized Gamma Markov Processes for Heterogeneous Overdispersed Count Time Series",
        "authors": "Rui HuangSikun YangHeinz Koeppl",
        "links": "http://arxiv.org/abs/2402.18995v1",
        "entry_id": "http://arxiv.org/abs/2402.18995v1",
        "pdf_url": "http://arxiv.org/pdf/2402.18995v1",
        "summary": "Modeling count-valued time series has been receiving increasing attention\nsince count time series naturally arise in physical and social domains. Poisson\ngamma dynamical systems (PGDSs) are newly-developed methods, which can well\ncapture the expressive latent transition structure and bursty dynamics behind\ncount sequences. In particular, PGDSs demonstrate superior performance in terms\nof data imputation and prediction, compared with canonical linear dynamical\nsystem (LDS) based methods. Despite these advantages, PGDS cannot capture the\nheterogeneous overdispersed behaviours of the underlying dynamic processes. To\nmitigate this defect, we propose a negative-binomial-randomized gamma Markov\nprocess, which not only significantly improves the predictive performance of\nthe proposed dynamical system, but also facilitates the fast convergence of the\ninference algorithm. Moreover, we develop methods to estimate both\nfactor-structured and graph-structured transition dynamics, which enable us to\ninfer more explainable latent structure, compared with PGDSs. Finally, we\ndemonstrate the explainable latent structure learned by the proposed method,\nand show its superior performance in imputing missing data and forecasting\nfuture observations, compared with the related models.",
        "updated": "2024-02-29 09:46:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.18995v1"
    }
]