[
    {
        "title": "Context-Guided Diffusion for Out-of-Distribution Molecular and Protein Design",
        "authors": "Leo KlarnerTim G. J. RudnerGarrett M. MorrisCharlotte M. DeaneYee Whye Teh",
        "links": "http://arxiv.org/abs/2407.11942v1",
        "entry_id": "http://arxiv.org/abs/2407.11942v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11942v1",
        "summary": "Generative models have the potential to accelerate key steps in the discovery\nof novel molecular therapeutics and materials. Diffusion models have recently\nemerged as a powerful approach, excelling at unconditional sample generation\nand, with data-driven guidance, conditional generation within their training\ndomain. Reliably sampling from high-value regions beyond the training data,\nhowever, remains an open challenge -- with current methods predominantly\nfocusing on modifying the diffusion process itself. In this paper, we develop\ncontext-guided diffusion (CGD), a simple plug-and-play method that leverages\nunlabeled data and smoothness constraints to improve the out-of-distribution\ngeneralization of guided diffusion models. We demonstrate that this approach\nleads to substantial performance gains across various settings, including\ncontinuous, discrete, and graph-structured diffusion processes with\napplications across drug discovery, materials science, and protein design.",
        "updated": "2024-07-16 17:34:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11942v1"
    },
    {
        "title": "Impossibility of latent inner product recovery via rate distortion",
        "authors": "Cheng MaoShenduo Zhang",
        "links": "http://arxiv.org/abs/2407.11932v1",
        "entry_id": "http://arxiv.org/abs/2407.11932v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11932v1",
        "summary": "In this largely expository note, we present an impossibility result for inner\nproduct recovery in a random geometric graph or latent space model using the\nrate-distortion theory. More precisely, suppose that we observe a graph $A$ on\n$n$ vertices with average edge density $p$ generated from Gaussian or spherical\nlatent locations $z_1, \\dots, z_n \\in \\mathbb{R}^d$ associated with the $n$\nvertices. It is of interest to estimate the inner products $\\langle z_i, z_j\n\\rangle$ which represent the geometry of the latent points. We prove that it is\nimpossible to recover the inner products if $d \\gtrsim n h(p)$ where $h(p)$ is\nthe binary entropy function. This matches the condition required for positive\nresults on inner product recovery in the literature. The proof follows the\nwell-established rate-distortion theory with the main technical ingredient\nbeing a lower bound on the rate-distortion function of the Wishart distribution\nwhich is interesting in its own right.",
        "updated": "2024-07-16 17:23:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11932v1"
    },
    {
        "title": "Bayesian Causal Forests for Longitudinal Data: Assessing the Impact of Part-Time Work on Growth in High School Mathematics Achievement",
        "authors": "Nathan McJamesAnn O'SheaAndrew Parnell",
        "links": "http://arxiv.org/abs/2407.11927v1",
        "entry_id": "http://arxiv.org/abs/2407.11927v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11927v1",
        "summary": "Modelling growth in student achievement is a significant challenge in the\nfield of education. Understanding how interventions or experiences such as\npart-time work can influence this growth is also important. Traditional methods\nlike difference-in-differences are effective for estimating causal effects from\nlongitudinal data. Meanwhile, Bayesian non-parametric methods have recently\nbecome popular for estimating causal effects from single time point\nobservational studies. However, there remains a scarcity of methods capable of\ncombining the strengths of these two approaches to flexibly estimate\nheterogeneous causal effects from longitudinal data. Motivated by two waves of\ndata from the High School Longitudinal Study, the NCES' most recent\nlongitudinal study which tracks a representative sample of over 20,000 students\nin the US, our study introduces a longitudinal extension of Bayesian Causal\nForests. This model allows for the flexible identification of both individual\ngrowth in mathematical ability and the effects of participation in part-time\nwork. Simulation studies demonstrate the predictive performance and reliable\nuncertainty quantification of the proposed model. Results reveal the negative\nimpact of part time work for most students, but hint at potential benefits for\nthose students with an initially low sense of school belonging. Clear signs of\na widening achievement gap between students with high and low academic\nachievement are also identified. Potential policy implications are discussed,\nalong with promising areas for future research.",
        "updated": "2024-07-16 17:18:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11927v1"
    },
    {
        "title": "Global Optimisation of Black-Box Functions with Generative Models in the Wasserstein Space",
        "authors": "Tigran RamazyanMikhail HushchynDenis Derkach",
        "links": "http://arxiv.org/abs/2407.11917v1",
        "entry_id": "http://arxiv.org/abs/2407.11917v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11917v1",
        "summary": "We propose a new uncertainty estimator for gradient-free optimisation of\nblack-box simulators using deep generative surrogate models. Optimisation of\nthese simulators is especially challenging for stochastic simulators and higher\ndimensions. To address these issues, we utilise a deep generative surrogate\napproach to model the black box response for the entire parameter space. We\nthen leverage this knowledge to estimate the proposed uncertainty based on the\nWasserstein distance - the Wasserstein uncertainty. This approach is employed\nin a posterior agnostic gradient-free optimisation algorithm that minimises\nregret over the entire parameter space. A series of tests were conducted to\ndemonstrate that our method is more robust to the shape of both the black box\nfunction and the stochastic response of the black box than state-of-the-art\nmethods, such as efficient global optimisation with a deep Gaussian process\nsurrogate.",
        "updated": "2024-07-16 17:09:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11917v1"
    },
    {
        "title": "Combining Wasserstein-1 and Wasserstein-2 proximals: robust manifold learning via well-posed generative flows",
        "authors": "Hyemin GuMarkos A. KatsoulakisLuc Rey-BelletBenjamin J. Zhang",
        "links": "http://arxiv.org/abs/2407.11901v1",
        "entry_id": "http://arxiv.org/abs/2407.11901v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11901v1",
        "summary": "We formulate well-posed continuous-time generative flows for learning\ndistributions that are supported on low-dimensional manifolds through\nWasserstein proximal regularizations of $f$-divergences. Wasserstein-1 proximal\noperators regularize $f$-divergences so that singular distributions can be\ncompared. Meanwhile, Wasserstein-2 proximal operators regularize the paths of\nthe generative flows by adding an optimal transport cost, i.e., a kinetic\nenergy penalization. Via mean-field game theory, we show that the combination\nof the two proximals is critical for formulating well-posed generative flows.\nGenerative flows can be analyzed through optimality conditions of a mean-field\ngame (MFG), a system of a backward Hamilton-Jacobi (HJ) and a forward\ncontinuity partial differential equations (PDEs) whose solution characterizes\nthe optimal generative flow. For learning distributions that are supported on\nlow-dimensional manifolds, the MFG theory shows that the Wasserstein-1\nproximal, which addresses the HJ terminal condition, and the Wasserstein-2\nproximal, which addresses the HJ dynamics, are both necessary for the\ncorresponding backward-forward PDE system to be well-defined and have a unique\nsolution with provably linear flow trajectories. This implies that the\ncorresponding generative flow is also unique and can therefore be learned in a\nrobust manner even for learning high-dimensional distributions supported on\nlow-dimensional manifolds. The generative flows are learned through adversarial\ntraining of continuous-time flows, which bypasses the need for reverse\nsimulation. We demonstrate the efficacy of our approach for generating\nhigh-dimensional images without the need to resort to autoencoders or\nspecialized architectures.",
        "updated": "2024-07-16 16:34:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11901v1"
    }
]