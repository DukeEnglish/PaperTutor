[
    {
        "title": "Efficient Training with Denoised Neural Weights",
        "authors": "Yifan GongZheng ZhanYanyu LiYerlan IdelbayevAndrey ZharkovKfir AbermanSergey TulyakovYanzhi WangJian Ren",
        "links": "http://arxiv.org/abs/2407.11966v1",
        "entry_id": "http://arxiv.org/abs/2407.11966v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11966v1",
        "summary": "Good weight initialization serves as an effective measure to reduce the\ntraining cost of a deep neural network (DNN) model. The choice of how to\ninitialize parameters is challenging and may require manual tuning, which can\nbe time-consuming and prone to human error. To overcome such limitations, this\nwork takes a novel step towards building a weight generator to synthesize the\nneural weights for initialization. We use the image-to-image translation task\nwith generative adversarial networks (GANs) as an example due to the ease of\ncollecting model weights spanning a wide range. Specifically, we first collect\na dataset with various image editing concepts and their corresponding trained\nweights, which are later used for the training of the weight generator. To\naddress the different characteristics among layers and the substantial number\nof weights to be predicted, we divide the weights into equal-sized blocks and\nassign each block an index. Subsequently, a diffusion model is trained with\nsuch a dataset using both text conditions of the concept and the block indexes.\nBy initializing the image translation model with the denoised weights predicted\nby our diffusion model, the training requires only 43.3 seconds. Compared to\ntraining from scratch (i.e., Pix2pix), we achieve a 15x training time\nacceleration for a new concept while obtaining even better image generation\nquality.",
        "updated": "2024-07-16 17:59:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11966v1"
    },
    {
        "title": "UrbanWorld: An Urban World Model for 3D City Generation",
        "authors": "Yu ShangJiansheng ChenHangyu FanJingtao DingJie FengYong Li",
        "links": "http://arxiv.org/abs/2407.11965v1",
        "entry_id": "http://arxiv.org/abs/2407.11965v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11965v1",
        "summary": "Cities, as the most fundamental environment of human life, encompass diverse\nphysical elements such as buildings, roads and vegetation with complex\ninterconnection. Crafting realistic, interactive 3D urban environments plays a\ncrucial role in constructing AI agents capable of perceiving, decision-making,\nand acting like humans in real-world environments. However, creating\nhigh-fidelity 3D urban environments usually entails extensive manual labor from\ndesigners, involving intricate detailing and accurate representation of complex\nurban features. Therefore, how to accomplish this in an automatical way remains\na longstanding challenge. Toward this problem, we propose UrbanWorld, the first\ngenerative urban world model that can automatically create a customized,\nrealistic and interactive 3D urban world with flexible control conditions.\nUrbanWorld incorporates four key stages in the automatical crafting pipeline:\n3D layout generation from openly accessible OSM data, urban scene planning and\ndesigning with a powerful urban multimodal large language model (Urban MLLM),\ncontrollable urban asset rendering with advanced 3D diffusion techniques, and\nfinally the MLLM-assisted scene refinement. The crafted high-fidelity 3D urban\nenvironments enable realistic feedback and interactions for general AI and\nmachine perceptual systems in simulations. We are working on contributing\nUrbanWorld as an open-source and versatile platform for evaluating and\nimproving AI abilities in perception, decision-making, and interaction in\nrealistic urban environments.",
        "updated": "2024-07-16 17:59:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11965v1"
    },
    {
        "title": "Motion-Oriented Compositional Neural Radiance Fields for Monocular Dynamic Human Modeling",
        "authors": "Jaehyeok KimDongyoon WeeDan Xu",
        "links": "http://arxiv.org/abs/2407.11962v1",
        "entry_id": "http://arxiv.org/abs/2407.11962v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11962v1",
        "summary": "This paper introduces Motion-oriented Compositional Neural Radiance Fields\n(MoCo-NeRF), a framework designed to perform free-viewpoint rendering of\nmonocular human videos via novel non-rigid motion modeling approach. In the\ncontext of dynamic clothed humans, complex cloth dynamics generate non-rigid\nmotions that are intrinsically distinct from skeletal articulations and\ncritically important for the rendering quality. The conventional approach\nmodels non-rigid motions as spatial (3D) deviations in addition to skeletal\ntransformations. However, it is either time-consuming or challenging to achieve\noptimal quality due to its high learning complexity without a direct\nsupervision. To target this problem, we propose a novel approach of modeling\nnon-rigid motions as radiance residual fields to benefit from more direct color\nsupervision in the rendering and utilize the rigid radiance fields as a prior\nto reduce the complexity of the learning process. Our approach utilizes a\nsingle multiresolution hash encoding (MHE) to concurrently learn the canonical\nT-pose representation from rigid skeletal motions and the radiance residual\nfield for non-rigid motions. Additionally, to further improve both training\nefficiency and usability, we extend MoCo-NeRF to support simultaneous training\nof multiple subjects within a single framework, thanks to our effective design\nfor modeling non-rigid motions. This scalability is achieved through the\nintegration of a global MHE and learnable identity codes in addition to\nmultiple local MHEs. We present extensive results on ZJU-MoCap and MonoCap,\nclearly demonstrating state-of-the-art performance in both single- and\nmulti-subject settings. The code and model will be made publicly available at\nthe project page: https://stevejaehyeok.github.io/publications/moco-nerf.",
        "updated": "2024-07-16 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11962v1"
    },
    {
        "title": "Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation",
        "authors": "Olga ZatsarynnaEmad BahramiYazan Abu FarhaGianpiero FrancescaJuergen Gall",
        "links": "http://arxiv.org/abs/2407.11954v1",
        "entry_id": "http://arxiv.org/abs/2407.11954v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11954v1",
        "summary": "Long-term action anticipation has become an important task for many\napplications such as autonomous driving and human-robot interaction. Unlike\nshort-term anticipation, predicting more actions into the future imposes a real\nchallenge with the increasing uncertainty in longer horizons. While there has\nbeen a significant progress in predicting more actions into the future, most of\nthe proposed methods address the task in a deterministic setup and ignore the\nunderlying uncertainty. In this paper, we propose a novel Gated Temporal\nDiffusion (GTD) network that models the uncertainty of both the observation and\nthe future predictions. As generator, we introduce a Gated Anticipation Network\n(GTAN) to model both observed and unobserved frames of a video in a mutual\nrepresentation. On the one hand, using a mutual representation for past and\nfuture allows us to jointly model ambiguities in the observation and future,\nwhile on the other hand GTAN can by design treat the observed and unobserved\nparts differently and steer the information flow between them. Our model\nachieves state-of-the-art results on the Breakfast, Assembly101 and 50Salads\ndatasets in both stochastic and deterministic settings. Code:\nhttps://github.com/olga-zats/GTDA .",
        "updated": "2024-07-16 17:48:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11954v1"
    },
    {
        "title": "Temporally Consistent Stereo Matching",
        "authors": "Jiaxi ZengChengtang YaoYuwei WuYunde Jia",
        "links": "http://arxiv.org/abs/2407.11950v1",
        "entry_id": "http://arxiv.org/abs/2407.11950v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11950v1",
        "summary": "Stereo matching provides depth estimation from binocular images for\ndownstream applications. These applications mostly take video streams as input\nand require temporally consistent depth maps. However, existing methods mainly\nfocus on the estimation at the single-frame level. This commonly leads to\ntemporally inconsistent results, especially in ill-posed regions. In this\npaper, we aim to leverage temporal information to improve the temporal\nconsistency, accuracy, and efficiency of stereo matching. To achieve this, we\nformulate video stereo matching as a process of temporal disparity completion\nfollowed by continuous iterative refinements. Specifically, we first project\nthe disparity of the previous timestamp to the current viewpoint, obtaining a\nsemi-dense disparity map. Then, we complete this map through a disparity\ncompletion module to obtain a well-initialized disparity map. The state\nfeatures from the current completion module and from the past refinement are\nfused together, providing a temporally coherent state for subsequent\nrefinement. Based on this coherent state, we introduce a dual-space refinement\nmodule to iteratively refine the initialized result in both disparity and\ndisparity gradient spaces, improving estimations in ill-posed regions.\nExtensive experiments demonstrate that our method effectively alleviates\ntemporal inconsistency while enhancing both accuracy and efficiency.",
        "updated": "2024-07-16 17:44:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11950v1"
    }
]