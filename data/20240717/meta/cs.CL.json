[
    {
        "title": "Does Refusal Training in LLMs Generalize to the Past Tense?",
        "authors": "Maksym AndriushchenkoNicolas Flammarion",
        "links": "http://arxiv.org/abs/2407.11969v1",
        "entry_id": "http://arxiv.org/abs/2407.11969v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11969v1",
        "summary": "Refusal training is widely used to prevent LLMs from generating harmful,\nundesirable, or illegal outputs. We reveal a curious generalization gap in the\ncurrent refusal training approaches: simply reformulating a harmful request in\nthe past tense (e.g., \"How to make a Molotov cocktail?\" to \"How did people make\na Molotov cocktail?\") is often sufficient to jailbreak many state-of-the-art\nLLMs. We systematically evaluate this method on Llama-3 8B, GPT-3.5 Turbo,\nGemma-2 9B, Phi-3-Mini, GPT-4o, and R2D2 models using GPT-3.5 Turbo as a\nreformulation model. For example, the success rate of this simple attack on\nGPT-4o increases from 1% using direct requests to 88% using 20 past tense\nreformulation attempts on harmful requests from JailbreakBench with GPT-4 as a\njailbreak judge. Interestingly, we also find that reformulations in the future\ntense are less effective, suggesting that refusal guardrails tend to consider\npast historical questions more benign than hypothetical future questions.\nMoreover, our experiments on fine-tuning GPT-3.5 Turbo show that defending\nagainst past reformulations is feasible when past tense examples are explicitly\nincluded in the fine-tuning data. Overall, our findings highlight that the\nwidely used alignment techniques -- such as SFT, RLHF, and adversarial training\n-- employed to align the studied models can be brittle and do not always\ngeneralize as intended. We provide code and jailbreak artifacts at\nhttps://github.com/tml-epfl/llm-past-tense.",
        "updated": "2024-07-16 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11969v1"
    },
    {
        "title": "NeedleBench: Can LLMs Do Retrieval and Reasoning in 1 Million Context Window?",
        "authors": "Mo LiSongyang ZhangYunxin LiuKai Chen",
        "links": "http://arxiv.org/abs/2407.11963v1",
        "entry_id": "http://arxiv.org/abs/2407.11963v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11963v1",
        "summary": "In evaluating the long-context capabilities of large language models (LLMs),\nidentifying content relevant to a user's query from original long documents is\na crucial prerequisite for any LLM to answer questions based on long text. We\npresent NeedleBench, a framework consisting of a series of progressively more\nchallenging tasks for assessing bilingual long-context capabilities, spanning\nmultiple length intervals (4k, 8k, 32k, 128k, 200k, 1000k, and beyond) and\ndifferent depth ranges, allowing the strategic insertion of critical data\npoints in different text depth zones to rigorously test the retrieval and\nreasoning capabilities of models in diverse contexts. We use the NeedleBench\nframework to assess how well the leading open-source models can identify key\ninformation relevant to the question and apply that information to reasoning in\nbilingual long texts. Furthermore, we propose the Ancestral Trace Challenge\n(ATC) to mimic the complexity of logical reasoning challenges that are likely\nto be present in real-world long-context tasks, providing a simple method for\nevaluating LLMs in dealing with complex long-context situations. Our results\nsuggest that current LLMs have significant room for improvement in practical\nlong-context applications, as they struggle with the complexity of logical\nreasoning challenges that are likely to be present in real-world long-context\ntasks. All codes and resources are available at OpenCompass:\nhttps://github.com/open-compass/opencompass.",
        "updated": "2024-07-16 17:59:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11963v1"
    },
    {
        "title": "Rethinking Transformer-based Multi-document Summarization: An Empirical Investigation",
        "authors": "Congbo MaWei Emma ZhangDileepa PitawelaHaojie ZhuangYanfeng Shu",
        "links": "http://arxiv.org/abs/2407.11948v1",
        "entry_id": "http://arxiv.org/abs/2407.11948v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11948v1",
        "summary": "The utilization of Transformer-based models prospers the growth of\nmulti-document summarization (MDS). Given the huge impact and widespread\nadoption of Transformer-based models in various natural language processing\ntasks, investigating their performance and behaviors in the context of MDS\nbecomes crucial for advancing the field and enhancing the quality of summary.\nTo thoroughly examine the behaviours of Transformer-based MDS models, this\npaper presents five empirical studies on (1) measuring the impact of document\nboundary separators quantitatively; (2) exploring the effectiveness of\ndifferent mainstream Transformer structures; (3) examining the sensitivity of\nthe encoder and decoder; (4) discussing different training strategies; and (5)\ndiscovering the repetition in a summary generation. The experimental results on\nprevalent MDS datasets and eleven evaluation metrics show the influence of\ndocument boundary separators, the granularity of different level features and\ndifferent model training strategies. The results also reveal that the decoder\nexhibits greater sensitivity to noises compared to the encoder. This\nunderscores the important role played by the decoder, suggesting a potential\ndirection for future research in MDS. Furthermore, the experimental results\nindicate that the repetition problem in the generated summaries has\ncorrelations with the high uncertainty scores.",
        "updated": "2024-07-16 17:42:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11948v1"
    },
    {
        "title": "Fine-grained Hallucination Detection and Mitigation in Long-form Question Answering",
        "authors": "Rachneet SachdevaYixiao SongMohit IyyerIryna Gurevych",
        "links": "http://arxiv.org/abs/2407.11930v1",
        "entry_id": "http://arxiv.org/abs/2407.11930v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11930v1",
        "summary": "Long-form question answering (LFQA) aims to provide thorough and in-depth\nanswers to complex questions, enhancing comprehension. However, such detailed\nresponses are prone to hallucinations and factual inconsistencies, challenging\ntheir faithful evaluation. This work introduces HaluQuestQA, the first\nhallucination dataset with localized error annotations for human-written and\nmodel-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 4.7k\nspan-level error annotations for five different error types by expert\nannotators, along with preference judgments. Using our collected data, we\nthoroughly analyze the shortcomings of long-form answers and find that they\nlack comprehensiveness and provide unhelpful references. We train an automatic\nfeedback model on this dataset that predicts error spans with incomplete\ninformation and provides associated explanations. Finally, we propose a\nprompt-based approach, Error-informed refinement, that uses signals from the\nlearned feedback model to refine generated answers, which we show reduces\nhallucination and improves answer quality. Furthermore, humans find answers\ngenerated by our approach comprehensive and highly prefer them (84%) over the\nbaseline answers.",
        "updated": "2024-07-16 17:23:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11930v1"
    },
    {
        "title": "What's Wrong? Refining Meeting Summaries with LLM Feedback",
        "authors": "Frederic KirsteinTerry RuasBela Gipp",
        "links": "http://arxiv.org/abs/2407.11919v1",
        "entry_id": "http://arxiv.org/abs/2407.11919v1",
        "pdf_url": "http://arxiv.org/pdf/2407.11919v1",
        "summary": "Meeting summarization has become a critical task since digital encounters\nhave become a common practice. Large language models (LLMs) show great\npotential in summarization, offering enhanced coherence and context\nunderstanding compared to traditional methods. However, they still struggle to\nmaintain relevance and avoid hallucination. We introduce a multi-LLM correction\napproach for meeting summarization using a two-phase process that mimics the\nhuman review process: mistake identification and summary refinement. We release\nQMSum Mistake, a dataset of 200 automatically generated meeting summaries\nannotated by humans on nine error types, including structural, omission, and\nirrelevance errors. Our experiments show that these errors can be identified\nwith high accuracy by an LLM. We transform identified mistakes into actionable\nfeedback to improve the quality of a given summary measured by relevance,\ninformativeness, conciseness, and coherence. This post-hoc refinement\neffectively improves summary quality by leveraging multiple LLMs to validate\noutput quality. Our multi-LLM approach for meeting summarization shows\npotential for similar complex text generation tasks requiring robustness,\naction planning, and discussion towards a goal.",
        "updated": "2024-07-16 17:10:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.11919v1"
    }
]