Preprint
URBANWORLD: AN URBAN WORLD MODEL FOR 3D
CITY GENERATION
YuShang1,JianshengChen2,HangyuFan3,JingtaoDing1,JieFeng1,YongLi1∗
1TsinghuaUniversity 2UniversityofScienceandTechnologyBeijing 3TsingrocInc.
ABSTRACT
Cities, as the most fundamental environment of human life, encompass diverse
physicalelementssuchasbuildings,roadsandvegetationwithcomplexintercon-
nection. Craftingrealistic,interactive3Durbanenvironmentsplaysacrucialrole
inconstructingAIagentscapableofperceiving,decision-making,andactinglike
humans in real-world environments. However, creating high-fidelity 3D urban
environmentsusuallyentailsextensivemanuallaborfromdesigners,involvingin-
tricatedetailingandaccuraterepresentationofcomplexurbanfeatures.Therefore,
howtoaccomplishthisinanautomaticalwayremainsalongstandingchallenge.
Toward this problem, we propose UrbanWorld, the first generative urban world
modelthatcanautomaticallycreateacustomized,realisticandinteractive3Dur-
ban world with flexible control conditions. UrbanWorld incorporates four key
stagesintheautomaticalcraftingpipeline: 3Dlayoutgenerationfromopenlyac-
cessible OSM data, urban scene planning and designing with a powerful urban
multimodal large language model (Urban MLLM), controllable urban asset ren-
dering with advanced 3D diffusion techniques, and finally the MLLM-assisted
scene refinement. The crafted high-fidelity 3D urban environments enable real-
istic feedback and interactions for general AI and machine perceptual systems
in simulations. We are working on contributing UrbanWorld as an open-source
and versatile platform for evaluating and improving AI abilities in perception,
decision-making,andinteractioninrealisticurbanenvironments.
1 INTRODUCTION
Citiesarethemostcomplexhuman-centricenvironments,characterizedbytheirintricatestructures,
diverse elements, and dynamic interactions. Creating near-realistic 3D urban world environments
is a fundamental and pivotal technique for broad research and real applications across various do-
mains such as AI agents (Yang et al., 2024), urban planning (Schrotter & Hu¨rzeler, 2020), urban
simulation (Xu et al., 2023) and metaverse (Allam et al., 2022). Traditionally, achieving this in-
volvesexpensivelaborcostsforhumandesignersondetailedassetmodeling,texturemapping,and
scene composition. With the advancement of generative AI, there have emerged more automatic
approachesfor3Dscenegenerationbasedonvolumetricrendering(Linetal.,2023;Xieetal.,2024)
anddiffusionmodels(Dengetal.,2023;Luetal.,2024). Theseapproacheshaverevolutionizedthe
paradigmof3Dscenegeneration,alleviatingthehighcostsofmanualdesign. However,thecrafted
3Dscenesareonlyvisuallyappealingvideos,whicharesignificantlydifferentfromtherealembod-
ied physical world. Regarding this issue, a recent series of methods known as world models have
emerged,preliminarilyfocusingonautonomousdrivingscenes(Huetal.,2023;Wangetal.,2023).
Thesemodelsareshowntopossessthecapabilityofunderstandingthescenedynamicsandpredict-
ing future states, uplifting the interactivity of 3D scene generation. However, there is still a large
gulfbetweenthecreatedurbanenvironmentsandtherealurbanworldinwhichhumanslive.Tosum
up,thereisstillalongwayfromtheactual“urbanworldmodels”,whichwedefineasmodelsable
tocreateurbanenvironmentsthatare(1)realisticandinteractive(2)customizableandcontrollable
(3)capableofsupportingembodiedagentlearning.
∗Correspondingauthor,correspondencetoliyong07@tsinghua.edu.cn
1
4202
luJ
61
]VC.sc[
1v56911.7042:viXraPreprint
Urban world models are of great significance in developing embodied intelligence and Artificial
GeneralIntelligence(AGI).Firstly,itispromisingtobridgethegapbetweenvirtualenvironments
andthe realworld, enablingembodiedagents tointeractwith andlearnfromrichly detailed, real-
isticurbanenvironments. Secondly, bycraftingsynthetic3Durbanenvironments, researcherscan
gaincompletecontroloverdatageneration,withfullaccesstoallgenerativeparameters. Machine
perceptualsystemscanthusbetrainedontasksthatarenotwellsuitedtoconductintherealworld
or require various environments. Finally, a sophisticated urban world model can simulate a wide
varietyofenvironments,frombustlingcitycenterstoquietresidentialneighborhoods,withrealistic
visualappearancesofphysicalinfrastructuressuchasbuildings, roads, andnaturalspaces. Thisis
crucialtoavoidoverfittingandcreatingagentswithhighgeneralizationindiverseanddynamicen-
vironments.Somepreliminaryexplorationshavebeenwitnessedinsomecommercialplatformslike
Omniverse1, anopenplatformforcreatingandsimulatingdetailed3Dworldenvironments. How-
ever, there are no specialized urban world models for crafting interactive 3D urban environments,
hinderingthedevelopmentofagentsintherealisticurbanenvironments.
Toward this issue, we propose UrbanWorld, a generative urban world model that can automati-
callycreaterealistic,controllableandembodied3DurbanenvironmentsjustfromatextandOpen-
StreetMap2 (OSM)prompt. Indetail,therearefourkeymodulesintheframeworkofUrbanWorld.
Firstly,UrbanWorldautomaticallygenerates3Durbanlayoutsandconductsdetailedassetprocess-
ingbasedonopen-accessibleOSMdataviaBlender3.Then,UrbanWorldadoptsafine-tunedurban-
specific multimodal large language model (called Urban MLLM) to effectively plan and design
urbanscenesfollowinguserinstructions,generatingdetailedtextualdescriptionsofurbanelements.
Next, UrbanWorld integrates a 3D asset renderer based on texture diffusion and refinement, flexi-
blycontrolledbytextualandvisualconditions. Finally, tofurtheroptimizethevisualappearance,
UrbanWorld utilizes Urban MLLM to scrutinize the crafted 3D urban scenes, generating detailed
suggestions for refinement and activate an additional iteration of rendering. This framework fully
exploits the controllable generation capabilities of diffusion models and the reasoning and plan-
ningabilitiesofMLLMs,contributingtothehigh-fidelityurbanenvironmentrenderingandsuperior
generationflexibility.
The created urban environment features detailed and realistic visual representations of urban in-
frastructures,includingbuildings,roads,andnaturalareas,greatlybenefitingthedeploymentofAI
systems. Notably, it can support near-realistic physical interactions between urban environments
andagents. Itistailoredtoenhancearangeofembodiedabilitiesincludingsensorygrounding,per-
ceptionanddecision-making. Forexample,embodiedagentscanbedeployedandtrainedforobject
recognition,routeplanningandautomaticnavigationviainteractionwiththerealisticurbanenviron-
ment. Besides,UrbanWorldcanserveasanopenplatformtosupportthecreationandmanipulation
ofmoreadvanced3Durbanenvironments,facilitatingtheadvancementofbroadAIcommunities.
Thecontributionsofthisworkcanbesummarizedasfollows:
• We present UrbanWorld, the first urban world model for automatically creating realistic, cus-
tomizedandinteractiveembodied3Durbanenvironmentswithflexiblecontrols.
• UrbanWorld demonstrates its superior generative ability to craft high-fidelity 3D urban environ-
ments,greatlyenhancingtheauthenticityofinteractionsintheenvironment.
• We provide UrbanWorld as an open-source tool and contribute a 3D asset dataset with various
urbanenvironments,whichfacilitatestheadvancementofbroadAI-relatedresearch,suchasem-
bodiedintelligenceandAIagents,layingthegroundworkforadvancingAGI.
2 RELATED WORKS
2.1 3DURBANSCENEGENERATION
3Durbanscenegenerationaimstocreaterealistic3Durbanenvironmentswithsophisticatedurban
planning and visual element design, usually requiring high human efforts such as complex asset
1https://www.nvidia.com/en-us/omniverse/
2https://www.openstreetmap.org/
3https://www.blender.org/
2Preprint
Table1: Comparisonbetweenexistingworksfor3DurbanscenegenerationandUrbanWorldfrom
fouraspects:text-controllable,image-controllable,creative(i.e.,whethernewassetscanbecreated)
andembodied/interactive(whetherthecreatedurbanenvironmentisphysicallyinteractive).
Method Text-controllable Image-controllable Creative Embodied/Interactive
SceneDreamer(Chenetal.,2023) × ✓ ✓ ×
PersistentNature(Chaietal.,2023) × ✓ ✓ ×
InfinicityLinetal.(2023) × ✓ ✓ ×
CityDreamer(Xieetal.,2024) × × ✓ ×
CityGen(Dengetal.,2023) × × ✓ ×
SceneCraft(Huetal.,2024) ✓ × × ✓
CityCraftDengetal.(2024) ✓ × × ✓
UrbanWorld ✓ ✓ ✓ ✓
modeling, texture mapping, and scene composition. With the advancement of deep learning tech-
niques,recentlytherearethreelinesofworktryingtoachievethisinanautomatedway,including
NeRF-based methods (Lin et al., 2023; Xie et al., 2024; Chen et al., 2023), diffusion-based meth-
ods (Deng et al., 2023; Inoue et al., 2023; Wu et al., 2024) and professional software script-based
methods (Zhou et al., 2024; Hu et al., 2024). An overview of the method comparison is shown
in Table 1. NeRF-based methods implicitly represent the urban scene and perform the volumetric
renderingfortheneuralfields. Forexample,CityDreamer(Xieetal.,2024)firstseparatesthescene
intobuildingsandbackgroundsthenintroducesdifferenttypesofneuralfieldsforassetrendering.
Thesemethodscanproduceahigh-qualityvisualappearancewhilepotentiallylosinggeometricfi-
delity. Diffusion-based methods utilize diffusion models to generate city layouts or urban scenes.
CityGen(Dengetal.,2023)providesanend-to-endpipelinetocreatediverse3Dcitylayoutswith
Stable Diffusion. These methods are creative in generating scene images or videos, but hard to
obtainembodied3Denvironments,limitingthepracticalusages. Recently,someprofessionalsoft-
ware script-based methods have been proposed, trying to develop an automatic agentic workflow
using LLMs to control the professional software for scene creation. A representative example is
Scenecraft(Huetal.,2024),whichestablishesanLLM-basedagenttotranslatetextualdescriptions
intoPythonscriptsforscenecreationinBlender. AmorerecentworkCityCraft(Dengetal.,2024)
adoptsLLMsindesigningandorganizing3Durbanenvironmentsfromoff-the-shelfassetlibraries.
Suchapproachesareeffectivebutonlycreateurbanenvironmentsbyretrievingandorganizingex-
istingassets,unabletoflexiblycreatenewassetswhennecessary. Differently,ourapproachadopts
and integrates both diffusion-based and professional software script-based methods, providing an
effective,controllableandcreativewayfor3Durbanworldgeneration.
2.2 3DWORLDSIMULATOR
ApersistentobjectiveinAIresearchhasbeentodevelopmachineagentscapableofengagingwith
variousenvironmentsin3Dspacelikehumans. Towardthisgoal,researchershavebeendevotedto
buildingvariousinteractiveworldsimulatorsintheformatofvideos(Bruceetal.,2024)orembodied
environment(Shenetal.,2022). Existingworldsimulationenvironmentsandplatformsaremostly
forindoorscenes(Puigetal.,2018;Xiaetal.,2018;Kolveetal.,2017;Xiaetal.,2020).Differently,
Threedworld (Gan et al., 2020) pays attention to creating outdoor environments by retrieving and
compositing objects from an existing asset library. When it comes to urban scenes which is the
mostimportantopenenvironment, existingworksmostlyfocusonthegenerativeworldmodelfor
autonomous driving capable of learning scene dynamics and understanding the geometry of the
physicalworld(Huetal.,2023;Wangetal.,2023;2024). However,thesemodelscanonlygenerate
newscenesintheformatofvideos,hardtoprovideanembodiedandinteractiveurbanenvironment
forrealuse. UGI(Xuetal.,2023)takesaforwardsteptowardurbanworldsimulation,proposingto
developanembodiedurbanenvironmentforagentdevelopment. Althoughitconceptualizedsome
relevantideas,thereisalackofspecificframeworksforimplementation. Toaddressthischallenge,
weproposeUrbanWorld,whichisexpectedtofacilitatetheconstructionofdiverseembodiedurban
environments with controllable and refined visual appearance, supporting agent development or
simulationinvariousurbanscenes.
3Preprint
C. Diffusion-based D. MLLM-assisted
A. OSM-guided Urban Layout Generation
Urban Asset Scene Refinement
Texture Rendering
Blender 3D Diffusion
Transformation Renderer
Untextured 3D coim nda ig tie on Rendered 3D
urban environment urban environment
textual
condition
prompting asset separation Urban MLLM
OSM Reference images
B. MLLM-empowered Urban “A modern library with “The concrete path
SceneDesign glass windows peppered with shades of
dominating the façade …” gray and brown…”
Buildings Roads & Paths
“A teaching area
in modern university” “An unfurling tapestry of “A shimmering blue pond
olive green, with varieties with clear surface reflecting
of plants…” the azure sky…”
User prompts Urban MLLM Forest & Vegetation Water
Detailed urban asset descriptions
Generation process Refinement process
Figure1: IllustrationofthewholeframeworkofUrbanWorld,includingfourkeycomponents: (A)
OSM-guided urban layout generation; (B) MLLM-empowered urban scene design; (C) Diffusion-
basedurbanassettexturerendering;(D)MLLM-assistedscenerefinement.
3 METHODOLOGY
Therearethreemainchallengestosolveforbuildingan“urbanworldmodel”: efficientembodied
environment construction, professional urban scene planning and high-quality texture generation.
Towards these objectives, UrbanWorld introduces four key components: (1) OSM-guided urban
layoutgeneration,anautomatic2D-to-3Dtransformationmodulebasedongloballyopen-accessible
OSMdata,whichcanaddressthefirstchallenge. (2)MLLM-empoweredurbanscenedesign,which
exploitsthesuperiorurbansceneunderstandingabilityofthetrainedurbanMLLMtodraftreason-
able urban scenes like human designers towards the second challenge. (3) Controllable diffusion-
basedurbanassettexturerenderer,aflexibleurbanassetrenderingbasedon3Ddiffusionfollowing
customizedprompts.(4)MLLM-assistedurbanscenerefinement,afinalreflectionmoduletofurther
improvethescenedesign,inspiredbytheiterativerevisioninthestandardoperationprocessofhu-
mandesigners.Thelasttwocomponentscontributetohigh-fidelitytexturesof3Dassets,effectively
tacklingthethirdchallenge. TheoverviewofUrbanWorldisillustratedinFigure1.
3.1 OSM-GUIDEDURBANLAYOUTGENERATION
ConsideringtheeasyaccessibilityandglobalcoverageofOSMdata,UrbanWorldismainlydevel-
oped based on OSM to generate 3D urban layouts. OSM data contains a wealth of information,
mainly including the geographic locations and attributes of roads, buildings, vegetation, forests,
water,andotherinfrastructureelements. Thecontainedurbanassetssuchasbuildings,forest,veg-
etation, water and roads are then separated as independent objects for subsequent element-wise
rendering. Inthisstep,UrbanWorldalsorecordstheobjectcenterlocationforfurtherreorganization
ofassets,makingitmatchtherealurbanlayout.
3.2 MLLM-EMPOWEREDURBANSCENEDESIGN
Aiming to effectively craft customized urban environments, UrbanWorld integrates an advanced
urban MLLM trained on extensive urban street-view imagery data. Specifically, we first collected
urban street-view images globally and labeled the corresponding textual descriptions with GPT-4
and then conducted a manual check and filtered the low-quality data. Then we fine-tune an open-
source MLLM, LLaVA-1.5 (Liu et al., 2024) on around 100K image-text pairs from the collected
dataset. We have validated that the obtained urban MLLM possesses outstanding performance on
4Preprint
“A contemporary white and grey
Design
modern CBD skyscraper with
large glass windows.”
Condition
Urban MLLM Textual prompts Reference image
UV Unwarping
Depth-aware
Diffusion
Untextured Urban Asset Multi-view Depth Map Multi-view Snapshots
UV Warping
Mapping
Position-aware
Texture Completion
RenderedUrban Asset Refined Texture Map Texture Mask
Condition Initial Texture Map
Position
Encoder
UV Position Map
Figure 2: Illustration of the urban asset rendering method in UrbanWorld, mainly including two
stages: Depth-aware UV texture generation with flexible control under textual and visual prompts
andUVposition-awaretexturerefinement.
urbansceneunderstandingtaskssuchasimagecaptioningandsceneclassificationthuscanbenefit
the urban scene analysis and design. In UrbanWorld, the urban MLLM is introduced to act as a
human-likedesigner,whichautomaticallydraftshigh-qualityanddetailedurbanscenedescriptions,
ensuring the urban scenes are visually coherent. Specifically, taking a simple textual instruction
(e.g., a teaching area in the university) from users and the selected OSM layout image as input,
UrbanWorld calls the urban MLLM with carefully designed prompts and returns diverse detailed
descriptionsaboutvisualappearanceandmaterialsforeachasset. Theproducedassetdescriptions
willbeusedastheconditioncontrollingthelaterrenderingprocess.
3.3 CONTROLLABLEDIFFUSION-BASEDURBANASSETTEXTURERENDERING
Renderingalarge-scaleurbansceneischallengingduetotheexistenceofcomplexelementsandre-
lations,wherebythescene-levelrenderingwillinevitablyresultinmismatchingandlow-resolution
texture. Therefore,wefollowtheelement-wiserenderingprincipletoensuretherenderingquality.
Simultaneouslytospeeduptherenderingprocess,wemergesomeurbanelementtypesandfinally
definefourmaincategories: buildings,roadsandpaths,forestandvegetation,andwater. Weimple-
menttherenderingwithacontrollablediffusion-basedmethodconsistingoftwostages: UVtexture
generationandtexturerefinementdetailedasshowninFigure2.
Following the previous procedures, we have obtained the untextured 3D mesh of the whole urban
sceneSandeachassetS .Foreachtypeofelement,wenowhavecorrespondingtextualdescriptions
i
t fromtheMLLM.herethemodelalsosupportstakingreferenceimager aspromptstocontrolthe
i i
generation. TheUVmapofthei-thassetS isdenotedasU .
i i
We first set a series of camera views v = {v }N to capture the multi-view appearance of the
i k k=1
object. Next, weutilizethedepth-awareControlNet(Zhangetal.,2023)tocontrola2Ddiffusion
model F to generate an image I showing the visual appearance on different views, together with
i
theconditionc ∈{t ,r ,(t ,r )}:
i i i i i
I =F(c ;d ;z), (1)
i i i
wherez isthelatentembeddingforthediffusionprocess,thedepthmapfromdifferentviewsd is
i
obtained from the rendering process d = P(S ,v ). Then we conduct a reverse process P−1 of
i i i
renderingtoback-projectI intotheUVtexturespacefromeachview:
i
U =P−1(v ;I ;S ), (2)
i i i i
5Preprint
Untextured Urban Scene Initial Textured Urban Scene Refined Urban Scene
Figure3:Illustrationoftheevolutionofcreatedurbanenvironments,includingtheuntexturedurban
scene,initialtexturedurbansceneandrefinedurbanscene.
ThenwecropU intotexturemapsfromeachview{Uk}N ,eachcontaininguniquetextualinfor-
i i k=1
mationfromdifferentviews. Subsequently,wemergethesetexturemapsintoasingletexturemap
U :
i
n
(cid:88)
U = Mk⊙Uk, (3)
i i i
k=1
whereMk denotesthecorrespondingmaskintheUVspacefromtheviewv .
i i
Up to now, we have obtained the preliminary texture map for the asset S , while practically we
i
have found that there are still some untextured areas on the object due to the discrete sampling of
views, especially for buildings with many faces. Inspired by the inpainting capability of diffusion
models,weintroduceanadditionalUVtextureinpaintingprocesstogetcompleteandnaturaltex-
tures.However,suchinpaintingcannotbedirectlyachievedwithgeneraldiffusion-basedinpainting
consideringtherearestrictpositionmappingrequirementsforUVtexturemaps. Therefore,wefol-
lowtheUVinpaintingprocessofgeneral3Dobjects (Zengetal.,2024),andintroduceaposition
map-guidedbuildingUVinpaintingprocess.
Tobespecific,weaddaUVpositionmapencoderE toencodethepositionmapV ∈ RH×W×3,
V i
indicating the adjacency relation of the UV texture fragments, where E shares the same archi-
V
tecture of the image encoder in the diffusion model. Then we curate a set of paired UV position
maps and UV texture maps for urban assets with complex surfaces and train the position encoder
following the pipeline of ControlNet (Zhang et al., 2023). With the control of UV position maps,
it’sexpectedtoachieveaccurateandreasonableinpaintingforUVtexturemaps. TheUVinpainting
processisformulatedasfollows:
Urefine =F(c ;U ;E (V )). (4)
i i i V i
With the above texture generation and completion steps, UrbanWorld can produce coherent and
high-fidelitytexturesforvariousurbanelements.
For a better visual aesthetic of the rendering, we conduct image upscaling with ControlNet-tile to
furtherenhancethestructuresharpnessandrealismofthetexturemap,producingmoredetailedand
realisticappearancesforurbanassets.
3.4 MLLM-ASSISTEDURBANSCENEREFINEMENT
Afterurbanassetrendering,UrbanWorldautomaticallyreorganizestheassetsguidedbythelocation
information extracted from the real OSM data, effectively recovering the original urban layout.
Inspiredbythestandardoperationprocessofhumandesigning,whereexpertswilltakeanoverview
ofthesceneandmakeminoradjustments.Tomimicsucheffort,UrbanWorldactivatesurbanMLLM
againtoscrutinizethecrafted3Durbanscenesandtexturedetails.WeprompttheMLLMtoidentify
inconsistencies between the scene imagery and previous design prompts and examine whether the
texture is as realistic as in the real world. Finally, the urban MLLM will provide sophisticated
suggestions for further refinement, including elements to be modified and refined design prompts.
Thentherenderingmodulewillbeactivatedandtheinvolvedelementswillberenderedunderthe
refined text prompts and updated in the scene. With such a refinement process, UrbanWorld can
furtheralignthegeneratedurbanenvironmenttotherealcities. Weprovideavisualizationexample
6Preprint
Infinicity
CityGen
CityDreamer
UrbanWorld
Figure 4: Qualitative comparisons of generated 3D urban environments from Infinicity, CityGen
(resultsfromtheoriginalpapersincethecodeisnotopen-source), CityDreamerandUrbanWorld.
By comparison, our method can craft more diverse 3D urban scenes with high-fidelity textures
followinguserinstructions.
showingtheevolutionofthecreatedurbanenvironmentsinFigure3.ItcanbeseenthatUrbanWorld
worksinaniterativerefinementmannertocreatehigh-fidelityurbanenvironments,wherethelow-
qualitytextureswillbeautomaticallyidentifiedandrefinedwiththepowerfulurbanMLLM.
4 EXPERIMENTS
In this section, we first introduce the experimental setup and implementation details (see Section
4.1),andthenwepresentthegenerationresultsofUrbanWorldforqualitativeestimation(seeSec-
tion 4.2). Finally, we provide some quantitative evaluations of the created urban environments to
demonstratethesuperiorityofUrbanWorld(seeSection4.3).
4.1 IMPLEMENTATIONDETAILS
UrbanWorldincorporatesthreekeytechniques: Blenderastheprofessional3Dmodelingsoftware,
diffusion-basedrenderingandMLLM-empoweredscenedesignandrefinement.Specifically,weuse
Blender-3.2.2forLinuxsystemsandthecompatibleBlosmaddontohandletheOSMtransforma-
tion. Intermsofdiffusion-basedrendering,weutilizeStableDiffusion-1.5(Rombachetal.,2022)
asthefundamentaldiffusionbackbone,togetherwithControlNet-Depth(Zhangetal.,2023)inthe
generationofmulti-viewimages. WealsointroduceIP-Adapter(Yeetal.,2023)tosupporttaking
referenceimagesastheadditionalgenerationcondition.WeuseControlNet-inpainting(Zhangetal.,
2023)asthediffusioncontrollerintheUVtexturerefinementandContrlNet-tile(Zhangetal.,2023)
intherealnessenhancementstage. Forthehyper-parametersettingsinthediffusion-basedrender-
ing,wesetthenumberofcameraviewsN = 4,whichcanbasicallysatisfytherenderingneedsof
mosturbanassets. Thenumberofinferencestepsinalldiffusionprocessesissetas30bydefault.
TheUVmapsof3Dassetsareunwrappedinthe“smartprojection”modeoperatedinBlender. As
fortheurbanMLLM,wefine-tuneLLaVA-1.5onacurateddatasetwith 100Kstreetviewimage-text
datapairs. AllexperimentsareconductedonasingleNVIDIATeslaA100GPU.
4.2 QUALITATIVERESULTS
WepresentsomegenerationresultsofUrbanWorldinFigure4,includingvariousrepresentativeur-
ban scenes, including educational places, commercial blocks and residential areas. For intuitive
7Preprint
Table2: Quantitativeevaluationofexistingworksfor3DurbanscenegenerationandUrbanWorld
ondeptherror,homogeneityindexandrealisticscore.
Method DepthError(↓) HomogeneityIndex(↓) RealisticScore(↑)
SceneDreamer(Chenetal.,2023) 0.152 0.746 5.6
CityDreamer(Xieetal.,2024) 0.147 0.745 6.0
UrbanWorld 0.089 0.683 6.8
comparison, we also provide some generation samples from Infinicity (Lin et al., 2023), City-
Gen(Dengetal.,2023)andCityDreamer(Xieetal.,2024).TheresultsofInfinicityandCityGenare
takenfromtheoriginalpaperbecausethecodesarenotopen-source. Itcanbeseenthatscenesfrom
Infinicityareshortofcleartexturesandwell-maintainedbuildingstructures. ScenesfromCityGen
feature homogeneous styles without clear characteristics of urban functions. Similarly, the visual
appearance of urban elements (especially buildings) in the environments from CityDreamer lacks
diversityandishardtodistinguish.Besides,therearealsocleargeometricdistortionsofthebuilding
boundariesinCityDreamer.Theseissueswillposegreatchallengesfortherealinteractionsbetween
subjectsandurbanenvironments. Forexample,embodiedagentsarehardtobetrainedtoconduct
urbannavigationbecausethesurroundingelementsaretoosimilartorecognize.Bycomparison,the
urban elements created by UrbanWorld possess distinct functional characteristics, benefiting from
the high controllability with text and reference image prompts. Besides, the overall scene is more
authenticandvisuallyharmonious,demonstratingtheeffectivenessofintelligentsceneplanningand
designoftheMLLM.
4.3 QUANTITATIVEEVALUATIONS
TobetterdemonstratethesuperiorgenerationperformanceofUrbanWorld,inthissection,wepro-
videquantitativeresultsfromthefollowingthreeaspects:
Depth error. Depth error (DE) is utilized to evaluate the 3D geometry accuracy, following the
implementationofEG3D(Chanetal.,2022)andCityDreamer(Xieetal.,2024). Specifically, we
usethepre-traineddepthestimationmodel(Ranftletal.,2020)toobtainthe“groundtruth”ofdepth
maps via density accumulation. DE is then calculated as the L2 distance between the normalized
predicted depth maps and the “ground truth”. The final result is averaged on the result from 100
capturedframesofgeneratedurbanscenes.
Homogeneityindex. Realisticcitiesarefeaturedbycomplexelementswithdiversevisualappear-
ances,indicatingvariousfunctionalusesofdifferenturbanareas. Inordertocapturethiskeychar-
acter,weproposetoevaluatethehomogeneityofgeneratedscenes,mainlymeasuringthevariance
ofdifferenturbanscenes. Tobespecific,wefirstextractthevisualfeatureofeachgeneratedscene
image with ResNet (He et al., 2016). The homogeneity index is then calculated as the averaged
cosinesimilarityofeachpairofscenesinthefeaturespace. Thesmallervalueofthehomogeneity
indexmeansahigherdiversityofgeneratedurbanenvironments.
Realistic score. Another important aspect to evaluate is the realness of generated urban environ-
ments,whichischallengingevenforhumandesigners. Hereweintroduceanoff-the-shelfMLLM
(GPT-4o)astheevaluatortoscoretherealisticlevelandqualityofgeneratedurbanelements(rang-
ingfrom1to10). Ahigherrealisticscoreindicatesbetterrealnessandfidelityofgeneratedtextures
ofurbanassets.
We compare our model with some existing methods for urban scene generation, mainly including
SceneDreamer(Dengetal.,2023)andCitydreamerXieetal.(2024),representingthemostadvanced
performance of automatic 3D urban environment generation. We don’t provide results from other
methods such as CityGen (Deng et al., 2023), CityCraft (Deng et al., 2024) and SceneCraft (Hu
etal.,2024)becausethesourcecodesarenotopenwhenthisworkwasdone.
From the results presented in Table 2, it can be seen that UrbanWorld outperforms on each quan-
titative metric compared with baselines. Statistically, UrbanWorld achieves 39.5% on depth error
comparedwiththemostcompetitivebaseline,demonstratingthegeometry-preservingabilityofUr-
banWorld.Bycomparison,renderingmethodssuchasSceneDreamerandCityDreamercanproduce
8Preprint
Table3: StudyoftheeffectivenessofthreekeydesignsinUrbanWorld: MLLM-empoweredurban
scenedesign,textureenhancementandMLLM-assistedscenerefinement.
Method DepthError(↓) HomogeneityIndex(↓) RealisticScore(↑)
UrbanWorld 0.089 0.683 6.8
w/oscenedesign 0.104 0.701 6.1
w/otextureenhancement 0.125 0.687 6.5
w/oscenerefinement 0.096 0.690 6.4
visuallyappealingurbanscenes, butcommonlylosegeometryconsistency. Intermsofthehomo-
geneity index, UrbanWorld has 8.3% improvement compared with baselines. Consistent with the
observation on the qualitative results in Section 4.2, the generated scenes from existing methods
exhibit high homogeneity, limited to the style of training data. By comparison, UrbanWorld can
producemorediverseurbanenvironmentsaccordingtouserinstruction, effectivelyachievingcus-
tomizedcreation. Thismakesitrealforcraftinganytypeof3Durbanenvironmentadaptingtothe
needsofdifferentscenes. Lastly,thecreatedurbanenvironmentsfromUrbanWorldshowoutstand-
ingrealnessandhightexturefidelity,increasing11.8%realisticscorecomparedwithbaselines. The
overall impression of scenes crafted by SceneDreamer and CityDreamer is harmonious, however,
the texture quality with a closer look is unsatisfying. Differently, UrbanWorld conducts element-
wisetexturerenderingbasedonthespecific3Dmesh,ensuringthegeometricmatchingandfidelity
ofgeneratedtextures.
4.4 ABLATIONSTUDY
To further validate the effectiveness of key designs in UrbanWorld, we conduct ablation studies
and show the results in Table 3. We explore the influence of three designs on the performance,
includingMLLM-empoweredurbanscenedesign,textureenhancementandMLLM-assistedscene
refinement. The results indicate that all these techniques contribute to the final generation perfor-
mance. Specifically,thescenedesignfromtheMLLMcontributesmosttotherealnessofgenerated
urban environments, benefiting from the powerful knowledge understanding and reasoning ability
ofMLLMs. Thetexturecompletionandenhancementoperationhasthemostnotableeffectonthe
estimated depth error because better texture fidelity can help with geometric perception. Besides,
the final scene refinement process leads to a gain in all evaluation metrics, further promoting the
wholeperformance.
5 CONCLUSION
WeproposeUrbanWorld,thefirstgenerativeurbanworldmodeltocreaterealistic,customizedand
interactive 3D urban environments with flexible control conditions in a fully automatic manner.
Integrating the powerful urban scene understanding ability of urban MLLM and the controllable
generation ability of diffusion models, UrbanWorld can effectively craft high-fidelity and diverse
urbanenvironmentsoutperformingexistingurbanscenegenerationmethods. Moreimportantly,the
created urban environments can provide realistic and high-fidelity information feedback, ensuring
embodied agents interact with and learn from the richly detailed, realistic urban world. We are
contributing UrbanWorld as an open-source tool to benefit broad research communities, including
butnotlimitedtoAIagentsandembodiedintelligence. Webelievethisworkcanpaveanewwayto
efficientlyestablishthe3Dvirtualurbanenvironment,acceleratingthedevelopmentofAGI.
Forfuturework,wewillcontinuetoenhancethecurrentversionofUrbanWorldandtherearemainly
threedirections. Firstly, wewillsupplementmoreelementsintourbanenvironmentcreation, such
aspersonsandvehicles,furtherimprovingtherichnessandrealnessofcraftedenvironments. Sec-
ondly, we will conduct experiments on interactive tasks such as visual recognition and navigation
for embodied agents to validate the real usability of created 3D environments. Lastly, we are or-
ganizingthecodesandthegenerated3Durbanenvironmentassetsforrelease. Weexpecttomake
UrbanWorldanopen-sourcetoolkitforconvenientuseinvariousrealapplications.
9Preprint
REFERENCES
Zaheer Allam, Ayyoob Sharifi, Simon Elias Bibri, David Sydney Jones, and John Krogstie. The
metaverseasavirtualformofsmartcities: Opportunitiesandchallengesforenvironmental,eco-
nomic,andsocialsustainabilityinurbanfutures. SmartCities,5(3):771–801,2022.
Jake Bruce, Michael D Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes,
MatthewLai,AditiMavalankar,RichieSteigerwald,ChrisApps,etal. Genie: Generativeinter-
activeenvironments. InForty-firstInternationalConferenceonMachineLearning,2024.
LucyChai,RichardTucker,ZhengqiLi,PhillipIsola,andNoahSnavely. Persistentnature: Agen-
erativemodelofunbounded3dworlds. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pp.20863–20874,2023.
EricRChan,ConnorZLin,MatthewAChan,KokiNagano,BoxiaoPan,ShaliniDeMello,Orazio
Gallo,LeonidasJGuibas,JonathanTremblay,SamehKhamis,etal. Efficientgeometry-aware3d
generativeadversarialnetworks. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pp.16123–16133,2022.
Zhaoxi Chen, Guangcong Wang, and Ziwei Liu. Scenedreamer: Unbounded 3d scene generation
from2dimagecollections.IEEEtransactionsonpatternanalysisandmachineintelligence,2023.
Jie Deng, Wenhao Chai, Jianshu Guo, Qixuan Huang, Wenhao Hu, Jenq-Neng Hwang, and
Gaoang Wang. Citygen: Infinite and controllable 3d city layout generation. arXiv preprint
arXiv:2312.01508,2023.
JieDeng,WenhaoChai,JunshengHuang,ZhonghanZhao,QixuanHuang,MingyanGao,Jianshu
Guo, Shengyu Hao, Wenhao Hu, Jenq-Neng Hwang, et al. Citycraft: A real crafter for 3d city
generation. arXivpreprintarXiv:2406.04983,2024.
ChuangGan,JeremySchwartz,SethAlter,DamianMrowca,MartinSchrimpf,JamesTraer,Julian
DeFreitas,JonasKubilius,AbhishekBhandwaldar,NickHaber,etal. Threedworld: Aplatform
forinteractivemulti-modalphysicalsimulation. arXivpreprintarXiv:2007.04954,2020.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecog-
nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.
770–778,2016.
AnthonyHu,LloydRussell,HudsonYeo,ZakMurez,GeorgeFedoseev,AlexKendall,JamieShot-
ton, and Gianluca Corrado. Gaia-1: A generative world model for autonomous driving. arXiv
preprintarXiv:2309.17080,2023.
ZiniuHu,AhmetIscen,AashiJain,ThomasKipf,YisongYue,DavidARoss,CordeliaSchmid,and
AlirezaFathi. Scenecraft: Anllmagentforsynthesizing3dscenesasblendercode. InForty-first
InternationalConferenceonMachineLearning,2024.
Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yamaguchi. Layoutdm:
Discrete diffusion model for controllable layout generation. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.10167–10176,2023.
Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt
Deitke,KianaEhsani,DanielGordon,YukeZhu,etal. Ai2-thor: Aninteractive3denvironment
forvisualai. arXivpreprintarXiv:1712.05474,2017.
Chieh Hubert Lin, Hsin-Ying Lee, Willi Menapace, Menglei Chai, Aliaksandr Siarohin, Ming-
Hsuan Yang, and Sergey Tulyakov. Infinicity: Infinite-scale city synthesis. In Proceedings of
theIEEE/CVFInternationalConferenceonComputerVision,pp.22808–22818,2023.
HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee.Improvedbaselineswithvisualinstruction
tuning. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion,pp.26296–26306,2024.
FanLu,Kwan-YeeLin,YanXu,HongshengLi,GuangChen,andChangjunJiang. Urbanarchitect:
Steerable3durbanscenegenerationwithlayoutprior. arXivpreprintarXiv:2404.06780,2024.
10Preprint
Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Tor-
ralba. Virtualhome: Simulating household activities via programs. In Proceedings of the IEEE
conferenceoncomputervisionandpatternrecognition,pp.8494–8502,2018.
Rene´Ranftl,KatrinLasinger,DavidHafner,KonradSchindler,andVladlenKoltun. Towardsrobust
monoculardepthestimation: Mixingdatasetsforzero-shotcross-datasettransfer. IEEEtransac-
tionsonpatternanalysisandmachineintelligence,44(3):1623–1637,2020.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition,pp.10684–10695,2022.
GerhardSchrotterandChristianHu¨rzeler. Thedigitaltwinofthecityofzurichforurbanplanning.
PFG–Journal of Photogrammetry, Remote Sensing and Geoinformation Science, 88(1):99–112,
2020.
Yuan Shen, Wei-Chiu Ma, and Shenlong Wang. Sgam: Building a virtual 3d world through si-
multaneous generation and mapping. Advances in Neural Information Processing Systems, 35:
22090–22102,2022.
Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, and Jiwen Lu. Drivedreamer: Towards
real-world-drivenworldmodelsforautonomousdriving. arXivpreprintarXiv:2309.09777,2023.
Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and Zhaoxiang Zhang. Driving into
thefuture: Multiviewvisualforecastingandplanningwithworldmodelforautonomousdriving.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
14749–14759,2024.
ZhennanWu,YangLi,HanYan,TaizhangShang,WeixuanSun,SenboWang,RuikaiCui,Weizhe
Liu, Hiroyuki Sato, Hongdong Li, et al. Blockfusion: Expandable 3d scene generation using
latenttri-planeextrapolation. arXivpreprintarXiv:2401.17053,2024.
FeiXia, AmirRZamir, ZhiyangHe, AlexanderSax, JitendraMalik, andSilvioSavarese. Gibson
env: Real-world perception for embodied agents. In Proceedings of the IEEE conference on
computervisionandpatternrecognition,pp.9068–9079,2018.
Fei Xia, William B Shen, Chengshu Li, Priya Kasimbeg, Micael Edmond Tchapmi, Alexander
Toshev,RobertoMart´ın-Mart´ın,andSilvioSavarese.Interactivegibsonbenchmark:Abenchmark
forinteractivenavigationinclutteredenvironments.IEEERoboticsandAutomationLetters,5(2):
713–720,2020.
HaozheXie,ZhaoxiChen,FangzhouHong,andZiweiLiu. Citydreamer:Compositionalgenerative
modelofunbounded3dcities. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pp.9666–9675,2024.
Fengli Xu, Jun Zhang, Chen Gao, Jie Feng, and Yong Li. Urban generative intelligence (ugi): A
foundationalplatformforagentsinembodiedcityenvironment.arXivpreprintarXiv:2312.11813,
2023.
Jihan Yang, Runyu Ding, Ellis Brown, Xiaojuan Qi, and Saining Xie. V-irl: Grounding virtual
intelligenceinreallife. arXivpreprintarXiv:2402.03310,2024.
HuYe,JunZhang,SiboLiu,XiaoHan,andWeiYang. Ip-adapter: Textcompatibleimageprompt
adapterfortext-to-imagediffusionmodels. arXivpreprintarXiv:2308.06721,2023.
XianfangZeng,XinChen,ZhongqiQi,WenLiu,ZiboZhao,ZhibinWang,BinFu,YongLiu,and
GangYu. Paint3d: Paintanything3dwithlighting-lesstexturediffusionmodels. InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.4252–4262,2024.
Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image
diffusionmodels.InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pp.3836–3847,2023.
MengqiZhou,JunHou,ChuanchenLuo,YuxiWang,ZhaoxiangZhang,andJunranPeng. Scenex:
Procedural controllable large-scale scene generation via large-language models. arXiv preprint
arXiv:2403.15698,2024.
11