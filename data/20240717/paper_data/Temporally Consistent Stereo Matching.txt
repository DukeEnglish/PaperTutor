Temporally Consistent Stereo Matching
Jiaxi Zeng1,2 , Chengtang Yao1,3, Yuwei Wu1,2∗, and Yunde Jia2,1
1 Beijing Key Laboratory of Intelligent Information Technology,
School of Computer Science & Technology, Beijing Institute of Technology, China
2 Guangdong Laboratory of Machine Perception and Intelligent Computing,
Shenzhen MSU-BIT University, China
3 Horizon Robotics
{jiaxi,yao.c.t,wuyuwei,jiayunde}@bit.edu.cn
Abstract. Stereo matching provides depth estimation from binocular
imagesfordownstreamapplications.Theseapplicationsmostlytakevideo
streamsasinputandrequiretemporallyconsistentdepthmaps.However,
existingmethodsmainlyfocusontheestimationatthesingle-framelevel.
Thiscommonlyleadstotemporallyinconsistentresults,especiallyinill-
posed regions. In this paper, we aim to leverage temporal information
to improve the temporal consistency, accuracy, and efficiency of stereo
matching.Toachievethis,weformulatevideostereomatchingasapro-
cess of temporal disparity completion followed by continuous iterative
refinements. Specifically, we first project the disparity of the previous
timestamp to the current viewpoint, obtaining a semi-dense disparity
map. Then, we complete this map through a disparity completion mod-
uletoobtainawell-initializeddisparitymap.Thestatefeaturesfromthe
current completion module and from the past refinement are fused to-
gether,providingatemporallycoherentstateforsubsequentrefinement.
Basedonthiscoherentstate,weintroduceadual-spacerefinementmod-
ule to iteratively refine the initialized result in both disparity and dis-
paritygradientspaces,improvingestimationsinill-posedregions.Exten-
siveexperimentsdemonstratethatourmethodeffectivelyalleviatestem-
poral inconsistency while enhancing both accuracy and efficiency. Cur-
rently, our method ranks second on the KITTI 2015 benchmark, while
achievingsuperiorefficiencycomparedtootherstate-of-the-artmethods.
The code is available at https://github.com/jiaxiZeng/Temporally-
Consistent-Stereo-Matching.
Keywords: Stereo Matching · Temporal Consistency · Disparity Com-
pletion · Iterative Refinement
1 Introduction
Stereo matching estimates the disparity by finding the horizontal correspon-
dences between the rectified left and right images. It plays a significant role in
various fields such as autonomous driving [28,47], robotics [39], SLAM [3,29],
∗Corresponding author
4202
luJ
61
]VC.sc[
1v05911.7042:viXra2 J. Zeng et al.
T T+1 T+2
(b)
Iterations
(a) (c)
Fig.1: (a) Visualization of disparity sequences from RAFT-Stereo [22] and our TC-
Stereo. The failure cases of RAFT-Stereo lie in the reflective areas on the ground. A
smallmotioncancauseseverejittersindisparitypredictions,whileourmethodachieves
temporally stable outputs. (b) The jitter |∆d| between aligned successive disparity
mapsinallandoccludedareasonTartanAirdataset[43].Ourmethodachievesbetter
temporal consistency than RAFT-Stereo. (c) The update step size changed with the
iterations of RAFT-Stereo and our TC-Stereo on TartanAir. Compared to RAFT-
Stereo, our method performs a disparity search within a local range.
AR/VR [8,40]. In these applications, stereo videos are typically taken as in-
put and the output disparity sequences are employed for downstream tasks.
However, the majority of existing methods [19,22,32,44,45] predict disparities
independently for each stereo pair, disregarding the coherence between consec-
utive frames. This typically results in temporally inconsistent results, as shown
in the second row of Fig. 1 (a). The inconsistent depth information has been
demonstrated to substantially influence the accuracy and consistency of down-
stream tasks [10,41,51]. Although some techniques, such as Extended Kalman
Filtering (EKF) [14] or Bundle Adjustment (BA) [38], mitigate the impact of
inconsistency, they also have respective limitations. For instance, EKF suffers
from non-linear errors, while BA brings intensive computation.
Inthiswork,ourgoalistoimprovethetemporalconsistencyofstereomatch-
ing. We analyze the source of temporal inconsistency in stereo matching from
twoperspectives.Ontheonehand,mostoftheexistingmethods[4,22,32]inde-
pendently infer the disparity map from scratch for each frame. This leads to a
globaldisparitysearchingrange,whichintroducesgreatervariabilityindisparity
computation (e.g., larger update step size), increasing the likelihood of incon-
sistency between consecutive frames. As shown in Fig. 1 (c), our method has a
smaller update step size, indicating that it searches for the ground truth within
a local disparity range, while RAFT-Stereo regresses the disparity from scratch
inaglobaldisparityrange.Ontheotherhand,cameraorobjectmotionleadsto
appearancechangesinoccludedareas,reflectivesurfaces,orlow-textureregions.
segamI
tfeL
oeretS-TFAR
sruO
)xp(
eziS
petS
etadpU
)xp(|

∆|Temporally Consistent Stereo Matching 3
The inherent ambiguity in these ill-posed regions causes the model to output
unstable resultswhenprocessing thetemporally changing imagesequences.Fig.
1 (b) demonstrates that the temporal inconsistency in ill-posed regions, like oc-
clusions, is more serious than that in general regions.
Based on the above analyses, we propose to leverage the temporal informa-
tion to mitigate temporal inconsistency. To avoid large update steps during the
refinement process, we use the semi-dense disparity map from the previous time
stepfordisparitycompletion,providingawell-initializeddisparityforsubsequent
iterative refinement. This allows the refinement module to focus on local-range
disparity updates. Additionally, we conduct a straightforward temporal state
fusion to fuse the hidden state from the previous refinement with the current
statefeaturesfromthecompletionmodule,thusprovidingatemporallycoherent
initial hidden state for further refinement.
Moreover,wefindthat,forill-posedareaslikeocclusionsorreflectiveregions,
itiseasiertoestimatethedisparitygradientthanthedisparityitself.Thisispri-
marily because, in the real world, the depth of most regions tends to be smooth
andcontinuous,producingconstantorgraduallychanginggradientsinthesear-
eas.Basedonthisobservation,weproposeadual-spacerefinementmodule.This
moduletakesthetemporallyinitializeddisparityandfusedstatesasinputs,and
refinestheresultsinbothdisparityspaceanddisparitygradientspace.Through
iterative refinement, the local smoothness constraints in the disparity gradient
space are progressively extended to more global areas, thereby improving the
smoothness of ill-posed regions and resulting in stable outputs.
Extensiveexperimentsshowthatourmethodachievesstate-of-the-arttempo-
ral consistency and accuracy. The contributions of our work can be summarized
as follows: (1) We analyze the causes of temporal inconsistency and propose
a temporally consistent stereo matching method. (2) We propose a temporal
disparity completion and a temporal state fusion module to exploit temporal
information, providing a well-initialized disparity and a coherent hidden state
for refinement. (3) We propose an iterative dual-space refinement module to re-
fine and stabilize the results in ill-posed regions. (4) Our method improves the
temporalconsistencyofstereomatchingandachievesSOTAresultsonsynthetic
and real-world datasets.
2 Related Work
Deep Stereo Matching Existing deep stereo matching methods primarily re-
volve around cost volume to design networks or representations, which is com-
monly categorized into regression-based methods [4,11–13,16,17,32,44,46] and
iterative-basedmethods[19,22,45,49].Regression-basedmethodsregressaprob-
ability volume to compute the disparity map, which can be classified into 3D
volumemethods[21,27,37,46]and4Dvolumemethods[4,11,16,26,32,44].They
eitherdirectlyregressthedisparityintheglobaldisparityrange[4,16,44]orper-
formaglobal-to-localregression[11,26,32].However,themotionbetweenframes
maycausevariationintheglobaldisparityprobabilitydistribution,leadingtoin-4 J. Zeng et al.
consistenciesbetweensequentialdisparitymaps.Iterative-basedmethodsregard
stereomatchingasacontinuousoptimizationprocessinthedisparityspace,iter-
ativelyretrievingthecostvolumetorefinethedisparitymap.RAFT-Stereo[22]
leveragestheopticalflowmethodRAFT[36]forstereomatchingandintroduces
a multi-level GRU to enlarge the receptive field. CREStereo [19] introduces a
cascadenetworkandadaptivegroupcorrelationlayertoaddresschallengessuch
as thin structures and imperfect rectification. DLNR [52] employs LSTM to
alleviate the data coupling problem in the iteration of GRU. PCVNet [49] pro-
posedaparameterizedcostvolumetoacceleratetheconvergenceofiterationsby
predicting larger update steps. IGEV-Stereo [45] regresses a geometry encoding
volume to provide an accurate initial disparity for the GRU module, acceler-
ating the convergence of disparity optimization. Although these methods have
achieved excellent results, independently inferring the disparities for each frame
leads to poor temporal consistency. In contrast, our method integrates the dis-
parity and state information of the previous frame to ensure coherence between
the consecutive outputs.
VideoStereoMatchingRecently,therehasbeenincreasingattentiononvideo
stereomatching.Dynamic-Stereo[15]andCODD[20]focusonconsistentstereo
matching in dynamic scenes. Dynamic-Stereo jointly processes multiple frames,
improving the temporal consistency of the results. However, the latency and
computational overhead introduced by multi-frame inference limit its applica-
tion in online scenarios. CODD predicts SE(3) transformations for each pixel
to align successive disparity maps and fuses them together. Different from their
post-fusion strategy, we integrate temporal information before our refinement,
providing a robust local disparity searching range. TemporalStereo [50] miti-
gates the adverse effects of occlusions and reflections by augmenting current
cost volume with the costs from the previous frame. Nevertheless, it still relies
on coarse-to-fine regression in the global disparity range. XR-Stereo [8] extends
RAFT-Stereo[22]overtime,reducingiterationsbyreusingresultsfromthepre-
viousframe,achievingover100FPSinalow-resolutionmode.However,itsimply
takes the previous disparity and hidden state as the initialization of the current
frame, disregarding the initial disparity and hidden state are semi-dense due to
non-overlapping regions between frames. For these regions, it is still necessary
to regress the disparity from scratch. Instead, we suggest completing the dis-
parity map and incorporating the current state information to obtain a robust
initializationforlocalrefinement.Theimportanceofthesedesignsforimproving
temporal consistency was demonstrated in our ablation experiments.
DepthCompletionDepthcompletionreliesonasparsedepthmaptogenerate
a dense depth map [5,6,25,34]. Some methods [1,9] formulate depth completion
as a stereo matching task, while some stereo matching methods [2,42] utilizing
sparse depth from LiDAR to guide the matching process. Different from these
works,ourdisparitycompletionmoduledoesnotrelyonadditionalLiDARdata
or complicated architectures. It utilizes a simple encoder-decoder to complete a
semi-dense disparity map projected from the previous time stamp and provide
the state features of the completed disparity map for further refinement.Temporally Consistent Stereo Matching 5
Semi-dense Disp Initial Disp Refined Disp 0 Refined Disp 1
T=0 Disparity flow
… Feature flow
Pose-based
Cost          Projection
Volume Initial 0  
Stereo Frame Final Disparity TDC …
Module
Encoder
Semi-dense Disp Initial Disp Refined Disp 0 Refined Disp 1
T=1
…
C  o s  t
Volume  
Stereo Frame Final Disparity TDC …
Module
Encoder
Next Frame Next Frame
Fig.2: Pipeline of TC-Stereo. We first use an encoder to extract left and right
features for the current stereo frame. These features are then used to construct a
cost volume. A semi-dense disparity map, derived from the cost volume (for the first
frame) or projected from the previous timestamp (for subsequent frames), is fed into
theTemporalDisparityCompletion(TDC)moduletoobtainaninitialdensedisparity
map. The output state of the TDC module is fused with the state from the past
to provide an initial hidden state for refinement. The dual-space refinement module
iteratively retrieves the cost volume and alternately refines the disparity map in the
disparity and disparity gradient spaces. The final disparity map and hidden state are
projected into the viewpoint of the next frame, serving as the temporal information
for continuous disparity estimation.
Normal Guided Depth EstimationSubstantialresearch[18,23,24,30,35,48]
demonstratesthatutilizingsurfacenormalcontributestodepthestimation.Most
methods [18,23,24,48] design loss functions to leverage normal for constraining
the depth map. Geo-Net [30] utilizes constraints between the normal and depth
during inference, but this constraint is limited to local areas. HITNet [35] up-
samples the disparity map through predicted disparity gradients to constrain
the local surfaces. In this work, we iteratively refine results in the disparity and
disparity gradient spaces, propagating the surface constraint globally.
3 Method
Our method processes stereo video sequences in an online manner and outputs
the temporally consistent disparity maps. The pipeline of TC-Stereo is depicted
in Fig. 2. In the following subsections, we will provide a detailed introduction
to three key components of our TC-Stereo: temporal disparity completion (Sec.
3.1), temporal state fusion (Sec. 3.2), and iterative dual-space refinement (Sec.
3.3). Finally, we will present the loss functions involved (Sec. 3.4).
,     
,     
laropmeT
laropmeT
noisuF
etatS
noisuF
etatS
ecaps-lauD
ecaps-lauD
tnemenifeR
tnemenifeR
ecaps-lauD
ecaps-lauD
tnemenifeR
tnemenifeR
ecaps-lauD
ecaps-lauD
tnemenifeR
tnemenifeR6 J. Zeng et al.
3.1 Temporal Disparity Completion
Theaimoftemporaldisparitycompletionistoprovideawell-initializeddisparity
map by leveraging the result of the previous frame. However, for the initial
frame, temporal information is unattainable. As a result, there is no disparity
point to use as the hint for the completion module, causing it to degrade into a
monocular module and affecting the reliability of the initialization. To address
this, we propose leveraging a semi-dense disparity map derived from the cost
volume to compensate for the lack of temporal information in the initial frame.
Semi-densedisparitymapfromthecostvolumeWedefinethecostvolume
C ∈[0,1]H×W×D as the cosine similarity between the left and right features for
each disparity hypothesis d∈D={0,1,...,D−1}. This is expressed as:
⟨F (v,u),F (v,u−d)⟩
C(v,u,d)= l r , (1)
∥F (v,u)∥ ×∥F (v,u−d)∥
l 2 r 2
whereF (v,u)andF (v,u−d)representthefeaturevectorsatpixelcoordinates
l r
(v,u) in the left image and (v,u − d) in the right image, respectively. Here,
⟨·,·⟩ denotes the inner product and ∥·∥ is Euclidean norm of a feature vector.
2
We first compute the disparity from the cost volume by the winner-take-all
strategy. Then, a threshold is applied to filter out outliers and obtain a semi-
dense disparity map. This process can be summarized by the following formula:
d =argmax C(d),
1 d∈D
d =argmax C(d),
2 d∈D\{d1−1,d1,d1+1}
(2)
(cid:40)
d , if C(d )−C(d )>θ
d = 1 1 2 ,
s 0, otherwise
whered isthedisparitywiththehighestsimilarity,andd isthesecond-highest
1 2
disparity,excludingtheimmediateneighborsofd .Thesemi-densedisparitymap
1
d retains d only if the C(d ) exceeds the C(d ) by a threshold θ; otherwise,
s 1 1 2
it is set to 0, indicating an invalid point. This process filters out disparities
with high uncertainty, yielding a more reliable semi-dense disparity map for the
completion.Moreover,toeffectivelylearnthesemi-densedisparitymapfromthe
cost volume, we apply a contrastive loss [35] to the cost volume. We discuss the
loss in detail in Section 3.4.
Semi-dense disparity map from the previous timestamp For subsequent
frames,weprojectthepreviousdisparitymapintothecurrentimagecoordinates
basedontheintrinsicparametersandposes.Thisprocesscanberepresentedas:
pt−1→t =Π (Tt−1→t·Π−1(pt−1,zt−1)),
c c
zt−1→t =(Tt−1→t·Π−1(pt−1,zt−1)) ,
c z
(3)
bf
dt−1→t = ,
zt−1→t
dt =warp(dt−1→t,pt−1→t).
sTemporally Consistent Stereo Matching 7
Here, pt−1 and zt−1 denote the coordinates of pixels and depths at time t−1,
respectively. Π represents the image projection process, which maps 3D points
c
onto the 2D image. Tt−1→t is the relative transformation from time t−1 to t.
b is the baseline of stereo camera, and f is the pixel-presented focal length. We
forwardwarpdt−1→tbythepixelmappingpt−1→ttogetthesemi-densedisparity
map dt at time t. The flexible sources of the semi-dense disparity map enable
s
our model to perform inference in both single-frame and multi-frame modes.
Disparity completionOurtemporaldisparitycompletionmoduleutilizescon-
textsfromthefeatureencoder,thesemi-densedisparity,andabinarymaskthat
indicates the sparsity as inputs. It employs a lightweight encoder-decoder net-
work to regress a dense disparity map and outputs state features of the map
for temporal state fusion. The detailed architecture of the completion module is
provided in the supplementary materials.
3.2 Temporal State Fusion
Different from XR-Stereo [8], which directly uses the warped previous hidden
statestoinitializetherefinementmodule,wearguethatthepreviousstatesmay
not be the best choice for initializing the current states. On the one hand, the
past states only encode the state information along the previous line of sight,
whichmayfailinthecurrentviewpoint.Ontheotherhand,fornon-overlapping
regionsbetweentwoframes,therearenopasthiddenstatesavailable.Therefore,
we leverage a lightweight GRU-like module to fuse the current state features ct
from the TDC module with the past hidden states ht−1 (N denotes the num-
N
ber of refinement iterations) to provide initial hidden states ht for the current
0
refinement module:
zt =σ(W ·[ct,ht−1 ]),
z N−1
rt =σ(W ·[ct,ht−1 ]),
r N−1 (4)
qt =tanh(W ·[rt⊙ct,ht−1 ]),
q N−1
ht =zt⊙ct+(1−zt)⊙qt.
0
Here, W , W , and W are the parameters of the network, σ(·) denotes the
z r q
sigmoid function, and ⊙ is the element-wise dot. This simple module plays a
crucial role in enhancing stability and accuracy, which we will demonstrate in
the experimental section.
3.3 Dual-space Refinement
We observed that for some challenging regions for the disparity space, such
as reflective areas, the gradient values can be easily estimated in the disparity
gradient space. Based on this observation, we design a dual-space refinement
module to refine the results in both the disparity space and the gradient space.
As shown in Fig. 3, the pipeline of the dual-space module consists of three
steps:disparityspacerefinement,gradientspacerefinement,andgradient-guided
disparity propagation.8 J. Zeng et al.
Dual-space Refinement Module
Disparity Space Refinement Gradient Space Refinement Gradient-guided Disparity Propagation
Cost volume
L
′  +    + 
Hidden states GRUs Sampled gradients Refined gradient Disparity candidates
P
Disparity map  (  ,  )  Refined disparity
∆  Updated disparity Encoder-decoder
Σ
Contexts
Fig.3: Architecture of the dual-space refinement module. The disparity map
corresponds to a scene consisting of a wall, a floor, and a transparent glass door. The
encircledLdenotesthelookupoperationtothecostvolume,Prepresentsthegradient-
guided disparity propagation, and Σ means weighted summation.
Disparity space refinementFollowRAFT-Stereo[22],weleveragethemulti-
level GRUs to refine the disparity map. It takes the hidden state h (i denotes
i
the iteration), contextual features, the looked-up costs, and the disparity map
as inputs. It outputs an intermediate hidden state h′ and a step size ∆d to
i+1
update the disparity map.
Gradient space refinement We convert the updated disparity map to its
disparitygradientspace.Forapoint(u ,v ,d )inthedisparityspace,wesample
0 0 0
two neighbor points (u ,v ,d ) and (u ,v ,d ), which yields two vectors, x =
1 1 1 2 2 2 1
(∆u ,∆v ,∆d ) = (u − u ,v − v ,d − d ) and x = (∆u ,∆v ,∆d ) =
1 1 1 1 0 1 0 1 0 2 2 2 2
(u −u ,v −v ,d −d ). We assume that x and x are non-collinear, and the
2 0 2 0 2 0 1 2
rotation from x to x in the u-v coordinate is clockwise. The formula of the
1 2
disparity gradient can be represented as:
   
∂d ∆v1∆d2−∆v2∆d1
∂u =∆u2∆v1−∆u1∆v2. (5)
∂d ∆u2∆d1−∆u1∆d2
∂v ∆u2∆v1−∆u1∆v2
By sampling different points within the neighborhood, we can obtain a series of
gradientmaps.Thesampledgradientsandcontextualfeaturesareinputintoan
encoder-decoder network to regress a refined gradient map.
Gradient-guided disparity propagation We utilize the refined gradients to
guidethepropagationofrefineddisparityinthedisparityspace.Specifically,for
eachpixel(u,v)withdisparityd,wepropagatethedisparitytoitsneighborhood
(u ,v ) based on the local planar hypothesis:
n n
∂d ∂d
dˆ =d+(u −u) +(v −v) , (6)
n n ∂u n ∂v
where dˆ is a propagated disparity candidate. We concatenated the h′ , con-
n i+1
texts, and the propagated disparity candidates and fed them into several convo-
lutionallayerswithasoftmaxfunctiontoregressasetofweights,denotedasw.
The refined disparity is regressed through the weighted summation of the dis-
parity candidates. Finally, a lightweight GRU network with 1×1 convolutional
Convs
GRUTemporally Consistent Stereo Matching 9
layers updates the intermediate hidden state using the refined disparity. The
updated hidden state h and the disparity are then used as inputs for the
i+1
next iteration. By iteratively refining the results in both disparity and gradi-
ent space, the surface constraints are propagated globally, improving results in
ill-posed areas.
3.4 Loss functions
Our loss function consists of three components: the cost volume loss L , the
cv
disparitylossL ,andthedisparitygradientlossL .FortheL ,weadopt
disp grad cv
the contrastive loss proposed by HITNet [35] to supervise the cost volume:
L =1−ψ(dgt)+max(η+ψ(dnm)−ψ (dgt),0),
cv detach
dnm =argmax C(d), (7)
d∈D\[dgt−1.5,dgt+1.5]
ψ(d)=(d−⌊d⌋)C(⌊d⌋+1)+(⌊d⌋+1−d)C(⌊d⌋).
ψ(d) is the cost (strictly speaking, similarity) for a sub-pixel disparity d. It is
obtained through linear interpolation between C(⌊d⌋+1) and C(⌊d⌋), where ⌊·⌋
is the floor function. The aim of L is to maximize ψ(dgt), while penalizing
cv
ψ(dnm) to ensure it remains at least a threshold η lower than ψ(dgt).
AsforthedisparitylossL ,itcomprisesthreesubparts:disparitycompletion
disp
loss L , disparity space refinement loss L , and gradient-guided disparity
dc dsr
propagation loss L , which can be expressed as:
gdp
N
(cid:88)
L =λ L + γN−i(Li +λ Li ). (8)
disp dc dc dsr gdp gdp
i=1
N represents the number of iterations, γ denotes the decay coefficient, and λ
dc
and λ represent the balancing scalars. All these loss functions utilize the L1
gdp
loss between the dgt and the disparity outputs at the corresponding stage.
The disparity gradient losses L can be formulated as:
grad
L =∥ggsr−ggt∥ +∥ggdp−ggt∥ ,
grad 1 1
ggt =∇ dgt, (9)
u,v
ggdp =∇ dgdp.
u,v
The ggsr represents the refined gradient map in the gradient space refinement.
Theggt andggdp arederivedbytakingthegradientsofdgt anddgdp withrespect
to u and v. The final loss is the combination of L , L and L :
cv disp grad
L=L +L +L . (10)
cv disp grad
4 Experiments
4.1 Datasets
TartanAir [43] is a synthetic dataset for visual SLAM, covering various chal-
lenging indoor and outdoor scenarios. It encompasses over a thousand stereo10 J. Zeng et al.
videos,totalingapproximately306Kstereopairs,makingitwell-suitedfortrain-
ing our temporal model. In this study, we employ this dataset for pre-training
and conduct ablation experiments on it.
Sceneflow [27] is a synthetic dataset containing three sub-datasets covering
indoorandoutdoorscenes.Thisdatasetincludesshortstereovideosequences(10
frames),withlargemotionsbetweenframes.Weutilizethedatasetfortemporal
training and compare our method with others.
KITTI[28]isareal-worlddatasetforautonomousdriving.Itprovidesaleader-
board to evaluate the methods on a test set consisting of 200 images. Our work
utilizes video sequences provided by KITTI for inference.
ETH3D SLAM [31] is a real-world dataset for SLAM in indoor environments.
It provides stereo video sequences, as well as depth and pose ground truth. We
evaluated the generalization performance of our method on this dataset.
4.2 Implementation Details
Our method is based on the implementation of RAFT-Stereo [22]. For both
training and testing, we set the number of refinement iterations N to 5. Dur-
ing the training, we utilize a slice of stereo sequences as input and output the
corresponding disparities frame by frame. We compute the loss for each frame’s
output and accumulate the gradients across all frames before updating the pa-
rameters. In our experiment, we set the sequence length to 2 or 4 frames for
training. During the inference process, our method takes a video stream of ar-
bitrary length as input and outputs disparity predictions in an online manner.
For the hyperparameters, we set θ =0.3, η =0.5, γ =0.9, and λ and λ to
dc gdp
0.1and1.2,respectively.WeusetheAdamWoptimizerandaone-cyclelearning
rate schedule where the maximum learning rate is set to 0.0002. The batch size
issetto8foralltheexperiments.FortheTartanAirdataset,weusealldata(in-
cluding Easy and Hard)for the ablation study,and the detailed training-testing
split can be found in the supplementary material. We train for 100k steps on
this dataset. To make a fair comparison with TemporalStereo [50], we retrain
our model with a sequence length of 2 and 200k steps according to their train-
test split. For the SceneFlow dataset, we train with the provided sequence data
for 200k steps, with a sequence length of 2. For the KITTI dataset, due to the
lack of temporally annotated data, we follow the setting of TemporalStereo [50]
and train our model using pseudo-label data exported by the SOTA method
LEAStereo [7] on the KITTI raw data. We train for 50k steps with a sequence
length of 4 and a max learning rate of 0.0001, based on the model pre-trained
on TartanAir. All pose data are either provided by the dataset or derived from
SLAM algorithms. All experiments are conducted on two NVIDIA A40 GPUs.
4.3 Ablation Study and Analysis
Ablation Study We demonstrate the effectiveness of our designs through ab-
lation experiments conducted on the TartanAir dataset. As shown in Table 1,
weinvestigatetheimpactsofsequencelengthfortraining,temporalinformationTemporally Consistent Stereo Matching 11
ALL OCC
Seq Pastdisp State TDC Dual-space
Setting N >1px>3pxEPE|∆d|Relu(∆e) >1px >3px EPE|∆d|Relu(∆e)
length &state fusionmodulerefinement
(%) (%) (px) (px) (px) (%) (%) (px) (px) (px)
(A) 2 5 8.04 2.98 0.53 0.35 0.16 34.91 16.60 1.63 1.15 0.47
(B) 2 32 6.02 2.26 0.41 0.28 0.13 25.89 12.39 1.28 0.93 0.39
(C) 2 5 ✓ 8.77 3.27 0.60 0.29 0.12 35.04 16.01 1.67 0.94 0.34
(D) 2 5 ✓ ✓ 6.98 2.55 0.45 0.25 0.10 29.89 13.33 1.32 0.86 0.31
(E) 2 5 ✓ ✓ ✓ 6.10 2.33 0.42 0.23 0.10 25.97 12.40 1.21 0.78 0.28
(F) 2 5 ✓ ✓ ✓ 6.28 2.42 0.42 0.21 0.09 25.89 11.86 1.18 0.74 0.28
(G) 2 5 ✓ ✓ ✓ ✓ 5.98 2.19 0.40 0.21 0.09 24.67 11.28 1.14 0.71 0.27
(H) 4 5 ✓ ✓ ✓ ✓ 5.84 2.05 0.390.20 0.08 23.8510.501.060.68 0.26
(I) 4 5 ✓ ✓ ✓ 6.50 2.29 0.43 0.28 0.13 26.80 12.48 1.26 0.90 0.38
Table 1: Ablation study on the TartanAir dataset. ALL and OCC respectively
represent metrics for all regions and occluded regions.
(i.e. Past disp & state), temporal state fusion, temporal disparity completion
(TDC)module,anddual-spacerefinementmoduleontheaccuracyinallregions
andoccludedregions.Initialsettings(A)and(B)serveasbaselines,representing
RAFT-Stereo [22] with 5 and 32 iterations respectively. For a fair comparison,
wesetthesequencelengthduringtrainingto2forthem,eventhoughtheydon’t
use temporal information. Similar to XR-Stereo [8], setting (C) utilizes the past
disparity and hidden states as initialization, but direct iterations on this initial-
ization reduce performance, as analyzed in Section 3.2. As shown by the results
of setting (D), the performance is improved in both all and occluded regions
compared with (C), demonstrating the necessity of temporal state fusion. The
accuracy of (E) shows further improvement, particularly on the 1-px error rate,
which benefits from the well-initialized disparity provided by the TDC module.
This enables the refinement module to be devoted to improving the fine-grained
matching. (F) demonstrates that the dual-space refinement effectively improves
the resultsin occluded regions.By combining (E) and(F), (G)achieves aneven
more precise stereo matching capability, exemplifying the synergy between the
modules. Furthermore, we extend the training sequence length from 2 to 4 and
get a better disparity output, suggesting that the performance of our model
scales with a more extensive temporal context. Setting (I) corresponds to the
model of (H) in the single-frame mode. Although there is a slight decrease in
accuracy compared to setting (H), the results are still superior to (A). This
demonstrates the versatility of our method in the single-frame and multi-frame
mode, while also emphasizing the importance of temporal information. To sum
up,ourTC-Stereo(G)and(H),withonly5iterations,significantlyimprovethe
results of RAFT-Stereo (A) and even surpass the performance of RAFT-Stereo
with 32 iterations (B) on all metrics, greatly enhancing efficiency.
Temporal Consistency We design two types of metrics to evaluate the tem-
poral consistency. For the first one, We convert the predicted disparity map
dt+1 to the image coordinate at time t through poses and ground truth op-
tical flow, denoting as d(cid:101)t+1. We use the absolute difference between d(cid:101)t+1 and
dt, denoted as |∆d| = |d(cid:101)t+1 −dt|, to evaluate temporal consistency. However,
evaluating temporal absolute differences alone is one-sided. If a disparity dt is
incorrect, the model should be allowed to make correct predictions at the next
time step t+1, even if this results in temporary inconsistencies. To evaluate12 J. Zeng et al.
T T+1 T+2 Left image RAFT-Stereo Ours
(a) (b)
Fig.4: Visualizations on KITTI 2015. (a) Comparison of temporal disparity se-
quences from RAFT-Stereo [22], IGEV [45] and our method. (b) Comparison of dis-
parities in ill-posed regions between RAFT-Stereo [22] and our method.
temporal consistency comprehensively, we design a more lenient metric to al-
low the temporal change towards dgt. Concretely, we calculate the error maps
e = |d−dgt| for timestamp t and t+1, and then compute the change in error,
∆e = et+1 −et. We use Relu(∆e) to evaluate the extent of error divergence
(cid:101)
over time (i.e., error increased from the previous frame for the same 3D point).
Table 1 proves that our final models, (G) and (H), achieve the best temporal
consistency and convergence compared to other settings. Settings (D), (E), and
(F) respectively demonstrate the effectiveness of the state fusion, TDC module,
and Dual-space refinement module in improving temporal consistency. Notably,
(G) is comparable in accuracy to the baseline with 32 iterations (B), yet (G)
significantly outperforms (B) in temporal metrics, especially in occlusion areas,
furtherdemonstratingtheeffectivenessofourTC-Stereoinmitigatingtemporal
inconsistencies.Additionally,although(I)surpasses(D)inaccuracymetrics,itis
inferiorto(D)intemporalmetrics,furtherrevealingthecrucialroleoftemporal
information in improving temporal consistency. We also provide the results of
the 3-pixel error of consistency metrics in the supplementary materials.
We present visualizations of disparity sequences to illustrate the improve-
ment in temporal consistency achieved by our method. As shown in Fig. 4a,
our method produces more consistent disparity maps in the highlighted white
box areas compared to other SOTA methods. More visualizations of temporal
consistency are given in the appendix and our project website.
Local Searching Range and Fast ConvergenceFig.5illustratesthedispar-
ity maps and update step sizes of our method and RAFT-Stereo, step by step.
It can be seen that our method iterates based on a well-initialized disparity,
providing a local disparity searching range. This leads to smaller update step
sizes and faster convergence to the ground truth.
Improvement on Ill-posed Areas Fig. 4b shows the results of occlusion,
transparent areas, and out-of-view areas, respectively. Thanks to the tempo-
Method RAFT-Stereo[22]LEAStereo[7]DLNR[52]DynamicStereo[15]TemporalStereo[50]CODD[20]Ours
EPE(px) 0.65 0.78 0.47 0.71 0.53 0.60 0.50
>3px(%) 3.13 - - 3.38 2.75 2.80 2.39
>1px(%) 6.67 7.82 5.38 7.57 6.38 - 4.93
Table 2: Benchmark results on Sceneflow dataset.
TFAR
VEGI
sruOTemporally Consistent Stereo Matching 13
Step 1 Step 2 Step 3 Step 4 Step 5
Noc All
Time
Method D1-bgD1-fgD1-allD1-bgD1-fgD1-all
(s)
RAFT-Stereo’s (%) (%) (%) (%) (%) (%)
initial disparity RAFT-Stereo[22] 1.45 2.94 1.69 1.58 3.05 1.82 0.38
CREStereo[19] 1.33 2.6 1.54 1.45 2.86 1.69 0.41
LEAStereo[7] 1.29 2.65 1.51 1.4 2.91 1.65 0.3
DLNR[52] 1.45 2.39 1.61 1.6 2.59 1.76 0.3
IGEV-Stereo[45] 1.27 2.62 1.49 1.38 2.67 1.59 0.18
PCW-Net[33] 1.26 2.93 1.53 1.37 3.16 1.67 0.44
initialO du isr parity HITNet[35] 1.54 2.72 1.74 1.74 3.2 1.98 0.02 TemporalStereo[50] 1.52 2.58 1.70 1.61 2.78 1.81 0.04
TC-Stereo(ours) 1.21 2.24 1.38 1.29 2.33 1.46 0.09
Fig.5:Visualizationsofthedisparitymap Table3:BenchmarkresultsofKITTI2015.
andtheupdatestepsizeateachiteration.
Method
OCC ALL
Method
>1px >3pxEPE
>1px >3pxEPE>1px>3pxEPE (%) (%) (px)
TemporalStereo*[50] 39.78 18.46 1.6011.18 3.81 0.66 IGEV-Stereo[45](N=32) 37.72 24.82 6.41
RAFT-Stereo[22](N=32)34.2016.67 1.45 8.25 3.33 0.53 RAFT-Stereo[22](N=32)38.19 24.95 6.36
Ours(N=5) 34.6816.361.36 8.51 3.310.52 Ours(N=5) 37.0023.296.27
Table 4: Left: Results of Tartanair dataset based on the train-test splits of Tempo-
ralStereo[50].*representsusingtheweightsprovidedbytheauthor.Right:Zero-shot
results on ETH3D SLAM dataset.
ral information and the design of dual-space refinement, our method achieved
smoother disparity output and clearer boundaries in these ill-posed regions.
Robustness Analysis Eq. 3 is based on the static scene assumption, resulting
in unreliable initial disparities in dynamic areas. However, our method remains
robust in ordinary dynamic scenes since we only use the temporal information
for initialization. Disparities in dynamic regions will be corrected by iterative
refinements. The moving car in Fig. 4a is a good example of this robustness.
Additionally, we discuss the robustness of our method on incorrect poses and
large motions in the supplementary materials.
Zero-shot Generalization We evaluated the zero-shot generalization perfor-
mance on the ETH3D SLAM dataset, as shown in Table 4 (right). All models
areonlytrainedonSceneFlow.ThemetricsexcluderegionswithEPE>50dueto
significant noise of depth ground truth around object boundaries. Our method
surpassesothermethodsingeneralizationwithfeweriterations.Wealsoprovide
the qualitative results of our method on ETH3D SLAM, as shown in Fig. 6.
4.4 Benchmark Results
Sceneflow We conducted evaluations on the SceneFlow dataset, as shown in
Table 2. Overall, our method outperforms the other methods except DLNR
on the EPE metric, which may be attributed to their use of a more powerful
backboneandmoreiterations.Ourmethodexhibitsaclearadvantageoverothers
in the >1px and >3px error rate metrics.
TartanAir We compared our method with TemporalStereo [50] and RAFT-
Stereo[22]ontheTartanAirdatasetusingthetrain-testsplitofTemporalStereo.
As shown in Table 4 (left), our model surpassed RAFT-Stereo with only 5 iter-
oeretS-TFAR
ezis
petS
sruO
ezis petS14 J. Zeng et al.
Fig.6: Zero-shot qualitative results of our method on ETH3D SLAM dataset.
ations. Furthermore, it demonstrates significant advantages over the other two
methodsintermsoftheEPEmetricinoccludedregions.Thisindicatesthatour
design can enhance performance in ill-posed areas.
KITTI2015WepresentacomprehensivecomparisonofourmethodwithSOTA
methodsontheKITTI2015dataset.AsshowninTable3,ourproposedmethod,
TC-Stereo, outperforms the other methods in most key metrics. Specifically, it
achieves the lowest error rates in both non-occluded (Noc) and all regions, with
a D1-all error rate of 1.46%, which is an 8.18% improvement over the best-
performingIGEV-Stereo[45].Amongallthesemethods,ourmethodexhibitsthe
smallestdifferencebetweentheNocandAllmetrics,demonstratingoursuperior
performance in occluded areas. Additionally, TC-Stereo showcases exceptional
efficiency, with a processing time of only 0.09 seconds per frame, which is sig-
nificantly faster than other methods with high performance. This performance
shows that TC-Stereo excels in both accuracy and speed, making it well-suited
for online applications.
5 Limitation
Despite the robustness of our model for ordinary dynamic regions and pose
noise, it remains sensitive to large motions of dynamic objects, large errors in
camera pose, and little overlap between adjacent frames, as we discuss in the
supplementary materials. In these challenging cases, our method degrades to
searching for the ground truth in a global disparity range like RAFT-Stereo.
6 Conclusion
Inthispaper,weproposedatemporallyconsistentstereomatchingmethodthat
exploits temporal information through our temporal disparity completion mod-
ule and temporal state fusion module, providing a well-initialized disparity map
and hidden states. We further proposed a dual-space refinement module that
iteratively refines the temporally initialized results in both disparity and dis-
parity gradient spaces, improving estimations in ill-posed regions. Experiments
demonstrated that our method effectively mitigates temporal inconsistency in
video stereo matching and achieves SOTA results on challenging datasets.Temporally Consistent Stereo Matching 15
Acknowledgements
This work was supported by the Natural Science Foundation of Shenzhen un-
der Grant No. JCYJ20230807142703006, Natural Science Foundation of China
(NSFC) under Grants No. 62172041 and No. 62176021, and Key Research Plat-
formsandProjectsoftheGuangdongProvincialDepartmentofEducationunder
Grant No.2023ZDZX1034.
References
1. Bartolomei, L., Poggi, M., Conti, A., Tosi, F., Mattoccia, S.: Revisiting depth
completion from a stereo matching perspective for cross-domain generalization.
arXiv preprint arXiv:2312.09254 (2023)
2. Bartolomei,L.,Poggi,M.,Tosi,F.,Conti,A.,Mattoccia,S.:Activestereowithout
pattern projector. In: Proceedings of the IEEE/CVF International Conference on
Computer Vision. pp. 18470–18482 (2023)
3. Campos,C.,Elvira,R.,Rodríguez,J.J.G.,Montiel,J.M.,Tardós,J.D.:Orb-slam3:
An accurate open-source library for visual, visual–inertial, and multimap slam.
IEEE Transactions on Robotics 37(6), 1874–1890 (2021)
4. Chang, J.R., Chen, Y.S.: Pyramid stereo matching network. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp.
5410–5418 (2018)
5. Cheng,X.,Wang,P.,Guan,C.,Yang,R.:Cspn++:Learningcontextandresource
aware convolutional spatial propagation networks for depth completion. In: Pro-
ceedingsoftheAAAIConferenceonArtificialIntelligence.vol.34,pp.10615–10622
(2020)
6. Cheng, X., Wang, P., Yang, R.: Depth estimation via affinity learned with convo-
lutional spatial propagation network. In: Proceedings of the European conference
on computer vision (ECCV). pp. 103–119 (2018)
7. Cheng, X., Zhong, Y., Harandi, M., Dai, Y., Chang, X., Li, H., Drummond, T.,
Ge,Z.:Hierarchicalneuralarchitecturesearchfordeepstereomatching.Advances
in Neural Information Processing Systems (NeurIPS) 33, 22158–22169 (2020)
8. Cheng,Z.,Yang,J.,Li,H.:Stereomatchingintime:100+fpsvideostereomatch-
ing for extended reality. In: Proceedings of the IEEE/CVF Winter Conference on
Applications of Computer Vision. pp. 8719–8728. IEEE (2024)
9. Choi, K., Jeong, S., Kim, Y., Sohn, K.: Stereo-augmented depth completion from
a single rgb-lidar image.In:2021 IEEE International Conference on Robotics and
Automation (ICRA). pp. 13641–13647. IEEE (2021)
10. Deng,K.,Liu,A.,Zhu,J.Y.,Ramanan,D.:Depth-supervisednerf:Fewerviewsand
fastertrainingforfree.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition. pp. 12882–12891 (2022)
11. Duggal, S., Wang, S., Ma, W.C., Hu, R., Urtasun, R.: Deeppruner: Learning effi-
cient stereo matching via differentiable patchmatch. In: Proceedings of the IEEE
International Conference on Computer Vision (ICCV). pp. 4384–4393 (2019)
12. Gu, X., Fan, Z., Zhu, S., Dai, Z., Tan, F., Tan, P.: Cascade cost volume for high-
resolution multi-view stereo and stereo matching. In: Proceedings of the IEEE
ConferenceonComputerVisionandPatternRecognition(CVPR).pp.2495–2504
(2020)16 J. Zeng et al.
13. Guo, X., Yang, K., Yang, W., Wang, X., Li, H.: Group-wise correlation stereo
network.In:ProceedingsoftheIEEEConferenceonComputerVisionandPattern
Recognition (CVPR). pp. 3273–3282 (2019)
14. Kalman, R.E.: A new approach to linear filtering and prediction problems (1960)
15. Karaev,N.,Rocco,I.,Graham,B.,Neverova,N.,Vedaldi,A.,Rupprecht,C.:Dy-
namicstereo: Consistent dynamic depth from stereo videos. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.13229–
13239 (2023)
16. Kendall,A.,Martirosyan,H.,Dasgupta,S.,Henry,P.,Kennedy,R.,Bachrach,A.,
Bry,A.:End-to-endlearningofgeometryandcontextfordeepstereoregression.In:
Proceedings of the IEEE International Conference on Computer Vision (ICCV).
pp. 66–75 (2017)
17. Khamis,S.,Fanello,S.,Rhemann,C.,Kowdle,A.,Valentin,J.,Izadi,S.:Stereonet:
Guidedhierarchicalrefinementforreal-timeedge-awaredepthprediction.In:Pro-
ceedings of the European Conference on Computer Vision (ECCV). pp. 573–590
(2018)
18. Kusupati,U.,Cheng,S.,Chen,R.,Su,H.:Normalassistedstereodepthestimation.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 2189–2199 (2020)
19. Li, J., Wang, P., Xiong, P., Cai, T., Yan, Z., Yang, L., Liu, J., Fan, H., Liu, S.:
Practicalstereomatchingviacascadedrecurrentnetworkwithadaptivecorrelation.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 16263–16272 (2022)
20. Li,Z.,Ye,W.,Wang,D.,Creighton,F.X.,Taylor,R.H.,Venkatesh,G.,Unberath,
M.:Temporallyconsistentonlinedepthestimationindynamicscenes.In:Proceed-
ings of the IEEE/CVF winter conference on applications of computer vision. pp.
3018–3027 (2023)
21. Liang, Z., Feng, Y., Guo, Y., Liu, H., Qiao, L., Chen, W., Zhou, L., Zhang, J.:
Learningdeepcorrespondencethroughpriorandposteriorfeatureconstancy.arXiv
preprint arXiv:1712.01039 (2017)
22. Lipson, L., Teed, Z., Deng, J.: Raft-stereo: Multilevel recurrent field transforms
for stereo matching. In: 2021 International Conference on 3D Vision (3DV). pp.
218–227. IEEE (2021)
23. Long, X., Lin, C., Liu, L., Li, W., Theobalt, C., Yang, R., Wang, W.: Adaptive
surfacenormalconstraintfordepthestimation.In:ProceedingsoftheIEEE/CVF
international conference on computer vision. pp. 12849–12858 (2021)
24. Long,X.,Liu,L.,Theobalt,C.,Wang,W.:Occlusion-awaredepthestimationwith
adaptive normal constraints. In: Computer Vision–ECCV 2020: 16th European
Conference,Glasgow,UK,August23–28,2020,Proceedings,PartIX16.pp.640–
657. Springer (2020)
25. Ma, F., Karaman, S.: Sparse-to-dense: Depth prediction from sparse depth sam-
ples and a single image. In: 2018 IEEE international conference on robotics and
automation (ICRA). pp. 4796–4803. IEEE (2018)
26. Mao,Y.,Liu,Z.,Li,W.,Dai,Y.,Wang,Q.,Kim,Y.T.,Lee,H.S.:Uasnet:Uncer-
taintyadaptivesamplingnetworkfordeepstereomatching.In:Proceedingsofthe
IEEE/CVF International Conference on Computer Vision. pp. 6311–6319 (2021)
27. Mayer, N., Ilg, E., Hausser, P., Fischer, P., Cremers, D., Dosovitskiy, A., Brox,
T.: A large datasetto train convolutional networks for disparity,optical flow, and
sceneflowestimation.In:ProceedingsoftheIEEEConferenceonComputerVision
and Pattern Recognition (CVPR). pp. 4040–4048 (2016)Temporally Consistent Stereo Matching 17
28. Menze,M.,Geiger,A.:Objectsceneflowforautonomousvehicles.In:Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).
pp. 3061–3070 (2015)
29. Mur-Artal, R., Tardós, J.D.: ORB-SLAM2: an open-source SLAM system for
monocular, stereo and RGB-D cameras. IEEE Transactions on Robotics 33(5),
1255–1262 (2017). https://doi.org/10.1109/TRO.2017.2705103
30. Qi,X.,Liao,R.,Liu,Z.,Urtasun,R.,Jia,J.:Geonet:Geometricneuralnetworkfor
jointdepthandsurfacenormalestimation.In:ProceedingsoftheIEEEConference
on Computer Vision and Pattern Recognition. pp. 283–291 (2018)
31. Schops,T.,Sattler,T.,Pollefeys,M.:Badslam:Bundleadjusteddirectrgb-dslam.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 134–144 (2019)
32. Shen,Z.,Dai,Y.,Rao,Z.:Cfnet:Cascadeandfusedcostvolumeforrobuststereo
matching.In:ProceedingsoftheIEEEConferenceonComputerVisionandPattern
Recognition (CVPR). pp. 13906–13915 (2021)
33. Shen,Z.,Dai,Y.,Song,X.,Rao,Z.,Zhou,D.,Zhang,L.:Pcw-net:Pyramidcombi-
nationandwarpingcostvolumeforstereomatching.In:ComputerVision–ECCV
2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceed-
ings, Part XXXII. pp. 280–297. Springer (2022)
34. Tang, J., Tian, F.P., Feng, W., Li, J., Tan, P.: Learning guided convolutional
network for depth completion. IEEE Transactions on Image Processing 30, 1116–
1129 (2020)
35. Tankovich, V., Hane, C., Zhang, Y., Kowdle, A., Fanello, S., Bouaziz, S.: Hit-
net: Hierarchical iterative tile refinement network for real-time stereo matching.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 14362–14372 (2021)
36. Teed, Z., Deng, J.: Raft: Recurrent all-pairs field transforms for optical flow. In:
Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August
23–28, 2020, Proceedings, Part II 16. pp. 402–419. Springer (2020)
37. Tonioni, A., Tosi, F., Poggi, M., Mattoccia, S., Stefano, L.D.: Real-time self-
adaptivedeepstereo.In:ProceedingsoftheIEEEConferenceonComputerVision
and Pattern Recognition (CVPR). pp. 195–204 (2019)
38. Triggs, B., McLauchlan, P.F., Hartley, R.I., Fitzgibbon, A.W.: Bundle adjust-
ment—a modern synthesis. In: Vision Algorithms: Theory and Practice: Inter-
national Workshop on Vision Algorithms Corfu, Greece, September 21–22, 1999
Proceedings. pp. 298–372. Springer (2000)
39. Usenko, V., Engel, J., Stückler, J., Cremers, D.: Direct visual-inertial odometry
with stereo cameras. In: 2016 IEEE International Conference on Robotics and
Automation (ICRA). pp. 1885–1892. IEEE (2016)
40. Wang, J., Scharstein, D., Bapat, A., Blackburn-Matzen, K., Yu, M., Lehman, J.,
Alsisan,S.,Wang,Y.,Tsai,S.,Frahm,J.M.,etal.:Apracticalstereodepthsystem
for smart glasses. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 21498–21507 (2023)
41. Wang, J., Wang, P., Long, X., Theobalt, C., Komura, T., Liu, L., Wang, W.:
Neuris: Neural reconstruction of indoor scenes using normal priors. In: European
Conference on Computer Vision. pp. 139–155. Springer (2022)
42. Wang, T.H., Hu, H.N., Lin, C.H., Tsai, Y.H., Chiu, W.C., Sun, M.: 3d lidar and
stereofusionusingstereomatchingnetworkwithconditionalcostvolumenormal-
ization. In: 2019 IEEE/RSJ International Conference on Intelligent Robots and
Systems (IROS). pp. 5895–5902. IEEE (2019)18 J. Zeng et al.
43. Wang, W., Zhu, D., Wang, X., Hu, Y., Qiu, Y., Wang, C., Hu, Y., Kapoor,
A., Scherer, S.: Tartanair: A dataset to push the limits of visual slam. In: 2020
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).
pp. 4909–4916. IEEE (2020)
44. Xu,G.,Cheng,J.,Guo,P.,Yang,X.:Acvnet:Attentionconcatenationvolumefor
accurate and efficient stereo matching. arXiv preprint arXiv:2203.02146 (2022)
45. Xu, G., Wang, X., Ding, X., Yang, X.: Iterative geometry encoding volume for
stereo matching. In: Proceedings of the IEEE/CVF Conference on Computer Vi-
sion and Pattern Recognition. pp. 21919–21928 (2023)
46. Xu,H.,Zhang,J.:Aanet:Adaptiveaggregationnetworkforefficientstereomatch-
ing. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). pp. 1959–1968 (2020)
47. Yang,G.,Song,X.,Huang,C.,Deng,Z.,Shi,J.,Zhou,B.:Drivingstereo:Alarge-
scale dataset for stereo matching in autonomous driving scenarios. In: IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR) (2019)
48. Yin, W., Liu, Y., Shen, C., Yan, Y.: Enforcing geometric constraints of virtual
normalfordepthprediction.In:ProceedingsoftheIEEE/CVFInternationalCon-
ference on Computer Vision. pp. 5684–5693 (2019)
49. Zeng, J., Yao, C., Yu, L., Wu, Y., Jia, Y.: Parameterized cost volume for stereo
matching. In: Proceedings of the IEEE/CVF International Conference on Com-
puter Vision. pp. 18347–18357 (2023)
50. Zhang, Y., Poggi, M., Mattoccia, S.: Temporalstereo: Efficient spatial-temporal
stereo matching network. In: 2023 IEEE/RSJ International Conference on Intelli-
gent Robots and Systems (IROS). pp. 9528–9535. IEEE (2023)
51. Zhang,Y.,Tosi,F.,Mattoccia,S.,Poggi,M.:Go-slam:Globaloptimizationforcon-
sistent 3d instant reconstruction. In: Proceedings of the IEEE/CVF International
Conference on Computer Vision. pp. 3727–3737 (2023)
52. Zhao,H.,Zhou,H.,Zhang,Y.,Chen,J.,Yang,Y.,Zhao,Y.:High-frequencystereo
matching network. In: Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition. pp. 1327–1336 (2023)