Combining Wasserstein-1 and Wasserstein-2
proximals: robust manifold learning via well-posed
generative flows
HyeminGu MarkosA.Katouslakis LucRey-Bellet BenjaminJ.Zhang
DepartmentofMathematicsandStatistics
UniversityofMassachusettsAmherst
{hgu,markos,luc,bjzhang}@umass.edu
Abstract
We formulate well-posed continuous-time generative flows for learning distri-
butions that are supported on low-dimensional manifolds through Wasserstein
proximalregularizationsoff-divergences. Wasserstein-1proximaloperatorsregu-
larizef-divergencessothatsingulardistributionscanbecompared. Meanwhile,
Wasserstein-2proximaloperatorsregularizethepathsofthegenerativeflowsby
adding an optimal transport cost, i.e., a kinetic energy penalization. Via mean-
fieldgametheory,weshowthatthecombinationofthetwoproximalsiscritical
forformulatingwell-posedgenerativeflows. Generativeflowscanbeanalyzed
throughoptimalityconditionsofamean-fieldgame(MFG),asystemofaback-
wardHamilton-Jacobi(HJ)andaforwardcontinuitypartialdifferentialequations
(PDEs) whose solution characterizes the optimal generative flow. For learning
distributionsthataresupportedonlow-dimensionalmanifolds,theMFGtheory
showsthattheWasserstein-1proximal,whichaddressestheHJterminalcondition,
andtheWasserstein-2proximal,whichaddressestheHJdynamics,arebothnec-
essaryforthecorrespondingbackward-forwardPDEsystemtobewell-defined
andhaveauniquesolutionwithprovablylinear flowtrajectories. Thisimplies
thatthecorrespondinggenerativeflowisalsouniqueandcanthereforebelearned
inarobustmannerevenforlearninghigh-dimensionaldistributionssupportedon
low-dimensionalmanifolds. Thegenerativeflowsarelearnedthroughadversarial
trainingofcontinuous-timeflows,whichbypassestheneedforreversesimulation.
We demonstrate the efficacy of our approach for generating high-dimensional
imageswithouttheneedtoresorttoautoencodersorspecializedarchitectures.
1 Introduction
Continuous normalizing flows (CNF) are a class of generative models [1, 2] based on learning
deterministicODEdynamicsthattransportasimplereferencedistributiontoatargetdistribution.
However,twochallengesarisewhenapplyingCNFsforlearninghigh-dimensionaldistributions: (a)
theflowsareheavilydiscretizationorimplementationdependentduetolackofauniqueoptimizer,
and(b)theyarenotsuitableforlearningdistributionssupportedonmanifolds,asistypicallythe
caseinmanyhigh-dimensionaldatasets,astheyneedtobeinvertibleinpractice. Forexample,[3,4]
havenotedthatwithoutadditionalregularizers,normalizingflowscanproducemultiplevalidflows,
many of which are difficult to integrate. [5] has noted that the test accuracy of neural ODEs are
discretizationdependent. Moreover,asnormalizingflowsneedtobeinvertible,latentrepresentations
orspecializedarchitecturesarefrequentlyneededtosuccessfullygeneratefromdistributionsthatare
supportedonmanifoldsforevenmoderatelyhigh-dimensionaldatasets[1,2,3,4,6]. Thechallenges
Preprint.Underreview.
4202
luJ
61
]LM.tats[
1v10911.7042:viXraofCNFsarefundamentallyrootedinthemathematicalfactthattheirassociatedoptimizationproblems
areill-defined,especiallyfordistributionssupportedonlow-dimensionalmanifolds.
Inthispaper,throughrigoroustheoryandnumericalexperiments,wedemonstratethatthecomposi-
tionoftheWasserstein-1andWasserstein-2proximalregularizationsisessentialforformulating
well-posed optimization problems in the training of deterministic flow-based generative models.
Recentwork[3,6,4]hasshownthattraininggenerativeflowsviaaKullback-Leibler(KL)divergence
regularized with a Wasserstein-2 proximal (W ) through the Benamou-Brenier formulation of
2
optimal transport yields sample paths that are more regular. In fact, we prove via the theory of
Hamilton-Jacobiequationsandmean-fieldgamesthatregularizingtheKLdivergencewithaW
2
proximalproducesoptimaltrajectorieswithconstantvelocity,i.e.,straightlines. Theproofisbased
onconnectionsofHamilton-JacobiequationstothecompressibleEulerequationsinfluidmechanics,
whichleadtofurtherinterestinginterpretationsofgenerativeflows. Suchflowscanbediscretized
withfewertimesteps,savingoncomputationaloverheadandmaylearnthetargetdistributionfaster.
Learningdistributionssupportedonmanifolds,however,isstillachallengewiththeWasserstein-2
proximalaloneastheKL(or,moregenerally,f-)divergenceisunabletocomparedistributionsthat
arenotabsolutelycontinuouswithrespecttoeachotherastheyarefiniteonlyfordistributionsthat
sharethesamesupport. WeproposeregularizingtheKL(orf-)divergencewithaWasserstein-1
(W )proximalaswell[7,8]. ApplyingaW proximalregularizationoftheKLallowsustocompare
1 1
mutuallysingulardistributions. ThevariationalderivativeofaW regularizedKLdivergenceis
1
well-definedforanyperturbationinthespaceofprobabilitydistributions,includingmutuallysingular
ones, meaning that the resulting training procedure (i.e., gradient descent) is robust for learning
distributionssupportedonmanifolds.
ThiscompositionofW andW proximalsforimprovingnormalizingflowsisgroundedinthetheory
1 2
ofmean-fieldgames. Generativeflowscanberoughlyviewedasoptimalcontrolproblemswherea
velocityfield,i.e.,acontrol,islearnedsothatacollectionofparticleswillevolvefromareference
distributiontoatargetdistributionbyoptimizinganobjectivefunctionalinvolvinginteractionsof
generatedandrealsamplesthroughvariousmean-fieldmechanismsthatdependontheclassofmodels,
e.g. CNFs,score-basedmodelsorgradientflows[9]. TheoptimalityconditionsoftheseMFGsare
apairofcoupledbackward-forwardpartialdifferentialequations(PDEs): abackwardHamilton-
Jacobiequationwhosesolutiondescribestheoptimalvelocityfield,andaforwardcontinuityequation
describingthegenerationprocess.Thebackward-forwardstructuremirrorsthatofforwardandinverse
flowsingenerativemodeling[9]. Theterminalconditionisassociatedwiththetargetdistributionand
choiceofprobabilitydivergence. WeproposeatrainingobjectivewiththecompositionofW and
1
W proximalswhosewell-posednessisguaranteedbythewell-posednessoftheMFGoptimality
2
conditions.TheW proximalyieldsawell-definedHamilton-Jacobiequation,whiletheW proximal
2 1
off-divergencesprovidesawell-definedterminalconditionfordistributionssupportedonmanifolds.
WeshowthattheMFGiswell-posedviaauniquenesstheoremwhereweshowthatifthesolutionof
theMFGPDEsystemissmooth,thenthesolutionisunique. Thisuniquenessandwell-posedness
propertyofthegenerativeflowproducesmorerobustlearningalgorithmsthatarelessdependenton
implementationdetails.
Ouralgorithmisbasedonadversarialtraining,wherethe
proximalstructureprovidesclearinterpretationsforthe
discriminatorsandgenerators.IncontrasttopreviousCNF
approaches [3, 4, 2], the adversarial formulation avoids
needingtoinvertthegenerativeflowduringtraining. Our
resultingW ⊕W proximalgenerativeflowsrobustly
1 2
learnsdistributionssupportedonmanifoldswithoutspe-
cializedarchitecturesorautoencoders. Wepreviewour
numericalresultsinFigure1,whichshowtheimpactof
W 1 and W 2 proximals for training generative flows of Figure 1: Optimality indicator (44)
distributionssupportedonmanifolds. Noticethatwithout blows up without W proximal (left).
1
a W 1 proximal, the terminal KL cost diverges over the ThecompositionofW 1 andW 2 proxi-
courseofthetraining,whilewithoutaW 2proximal,the malsminimizeskineticenergyandkeeps
kineticenergyoscillatesduringthecourseofthetraining, thevaluelessoscillatorywhiletraining
implyingalackofconvergencetoauniqueflow. (right). SeeSection6fordetails.
21.1 Contributions
• WerigorouslydemonstratethatusingthecompositionofWasserstein-1andWasserstein-2
proximalsoff-divergencestotraingenerativeflowsisindispensableforrobustlylearning
distributionssupportedonlow-dimensionalmanifolds. TheW proximalguaranteestra-
2
jectoriesarelinearandtheW proximalrobustlylearnsdistributionsonmanifolds. These
1
propertiesareduetothewell-posednessoftheoptimizationproblem,whichisjustifiedvia
thetheoryofmean-fieldgamesandthepropertiesofW andW proximals.
1 2
• Weproduceanadversarialtrainingalgorithmfortrainingnormalizingflowsbasedonthe
dualformulationoftheWasserstein-1proximaloff-divergences. Thiseliminatestheneed
forforward-backwardsimulationsoftrajectoriesthatareneededtotrainotherCNFs. The
callourresultinggenerativemodelW ⊕W proximalgenerativeflows.
1 2
• WefindthatW ⊕W generativeflowsrobustlylearnhigh-dimensionaldatasupportedon
1 2
low-dimensionalmanifoldswithoutpretrainingautoencodersortuningspecializedarchitec-
tures.
1.2 Relatedwork
TheuseofWassersteinproximalsingenerativemodelinghasalreadyfrequentlyappearedinprevious
studies. UsingW regularizedKLdivergencesfortrainingCNFswerefirstintroducedin[3]and
2
[4]. Thequalityoftheirimagegenerationmodels,however,werestillimplementationdependent. In
[6],theflowsareregularizedwiththeHamilton-JacobiequationcorrespondingtoaW proximal,
2
althoughtheirobjectivefunctiondoesnotincludethekineticenergypenalization.Inalltheseprevious
works,theHJterminalconditionsareill-definedwhenthetargetdistributionsaresupportedonlow-
dimensional manifolds, and therefore depend on using some sort of autoencoder. W proximals
2
of linear and nonlinear functions were generally studied in [10]. Score-based generative models
wereshowntobeWasserstein-2proximalsofcross-entropyin[11]usingideasfrom[10]and[9].
Usingadversarialtrainingfornormalizingflowswasconsideredin[12],butwithoutakineticenergy
penalization (i.e., a Wasserstein-2 proximal) a Jacobian normalization is employed to regularize
training. Thekeyconnectionbetweenmean-fieldgamesandgenerativeflows[9]isacrucialtoolthat
isusedtoproveourproposedW ⊕W aretrainedviawell-posedoptimizationproblems.
1 2
TheconstructionofWassersteingradientflowbasedtheW proximaloff-divergencesforgenerative
1
modelingwereconsideredin[8]. There,astableparticletransportalgorithmwasconstructedthat
is able to flow between two mutually singular empirical distributions. Additional results about
Wasserstein-1regularizedf-andRényidivergencescanbefoundin[7,13]. Theirusefordesigning
generativeadversarialnetswasstudiedin[14].
2 Wassersteinproximals
LetP (Rd)betheWasserstein-pspace,thespaceofprobabilitydistributionsonRdwithfinitep-th
p
momentswiththeWasserstein-pdistance. Wasserstein-1andWasserstein-2proximalshavebeen
been used intentionally or inadvertently for studying and designing generative models [7, 13, 3,
8,10,4,6,11]. Wasserstein-pproximalsforp = 1,2regularizecostfunctions(e.g.,divergences)
F : P (Rd) → R by an infimal convolution between the function F(R) and the p-th power of
p
Wasserstein-pdistanceWp(P,R)fromtheinputmeasureP
p
inf
(cid:8) F(R)+θ·Wp(P,R)(cid:9)
(1)
p
R∈Pp(Rd)
where θ > 0 is a weighting parameter. We define the Wasserstein proximal operator proxWp :
θ,F
P (Rd)→P (Rd)tobethemappingfrominputmeasureP totheoptimizerofthe(1),R∗. Thatis,
p p
R∗ =proxWp(P):=arg inf (cid:8) F(R)+θ·Wp(P,R)(cid:9) . (2)
θ,F R∈Pp(Rd) p
TheWassersteinproximaloperatorsforP (Rd),aretheanaloguesofproximaloperatorsthatarisein
p
convexoptimizationonRd[15]1. Infinitedimensionaloptimization,proximaloperatorsareuseful
1Let f : Rd → R be a semicontinuous convex function. The proximal operator of f is a general-
ization of a single gradient flow step for f using a backward Euler scheme with stepsize 1/λ. For Rd,
3fordevelopingoptimizationmethodsfornonsmoothordiscontinuousfunctions. Analogously,the
WassersteinproximaloperatorsareusedtooptimizenonsmoothfunctionalsoverWasserstein-pspace.
WefirstreviewhowtheWasserstein-2andWasserstein-1proximalsaredefinedandimplementedin
existinggenerativemodels,andhowthecompositionoftheproximalsisdefined.
2.1 Wasserstein-2proximalstabilizestrainingofgenerativeflows
Incanonicalcontinuousnormalizingflows,wefindavelocityfieldvsuchthatthedifferentialequation
dx =v(x(t),t)flowsareferencemeasureρ(·,0)=ρ toatargetmeasureπ(·)givenonlysamples
dt 0
ofthelatter. Thevelocityfieldislearnedbyoptimizingadivergence,suchastheKLdivergenceso
thattheterminalvelocityfieldρ =ρ(·,T)well-approximatesπ. Inthiscase,thecostfunctionis
T
(cid:104) (cid:105)
definedF(ρ )=D (ρ ∥π)=E logdρT . TheWasserstein-2proximalofF isthen
T KL T ρT dπ
(cid:26) (cid:27)
λ
inf F(ρ )+ W2(ρ ,ρ ) . (3)
ρT T 2T 2 0 T
In relation to (2), here θ = λ/(2T). By the Benamou-Brenier dynamic formulation of optimal
transport,thecontinuoustimeoptimizationproblemof(3)is
(cid:40) (cid:41)
(cid:90) T (cid:90) 1
inf F(ρ )+λ |v(x,t)|2ρ(x,t)dxdt:∂ ρ+∇·(vρ)=0, ρ(x,0)=ρ (x) . (4)
ρT,v T 0 Rd 2 t 0
Thisispreciselytheoptimizationproblemconsideredin[4]and[3]forregularizingthetrainingof
continuousnormalizingflows. Theoptimaltransportcostcanbeinterpretedasakineticenergywhich
favorsstraighterpathsandmakestheoptimizationmorewell-posedasiteliminatesmanyvalidflows
thatmaybedifficulttointegrate. Infact,inTheorem4.1weshowthattheoptimaltrajectoriesare
exactlylinear.
Previouscomputationalstudies[3,4]showedthatoptimaltransportregularizationsstabilizedtraining
of CNFs and significantly reduced the training time. Learning distributions supported on low-
dimensionalmanifolds,however,isnotstraightforward,andpreviousworkimplementedspecialized
architecturesorpreprocessedthedatawithautoencoders.
2.2 Wasserstein-1proximalenablesmanifoldlearning
(cid:104) (cid:16) (cid:17)(cid:105)
Thef-divergencebetweentwodistributionsρandπ isdefinedD (ρ∥π) = E f dρ ,where
f π dπ
dρ is the Radon-Nikodym derivative. A key weakness of f-divergences is that they are only
dπ
finite when the likelihood ratio dρ/dπ is finite, meaning that they are unable to effectively learn
discrepancies when ρ and π are not supported on the same manifold. To address this deficiency,
[7] studied a broad class of regularized f-divergences called the (f,Γ)-divergence, denoted DΓ,
f
definedviatheinfimalconvolutionofthef-divergencewithanintegralprobabilitymetric(IPM)
overfunctionspaceΓ. DefineW asageneralintegralprobabilitymetric(IPM)overfunctionspace
Γ
Γ,W (ρ,ν)=sup {E [ϕ]−E [ϕ]}.ThemainbenefitofIPMsisthattheyareabletocompare
Γ ϕ∈Γ ρ ν
distributionswithdifferentsupports. TheDΓdivergencewithweightingparameterL>0is
f
DΓ(ρ∥π)= inf {D (ν∥π)+L·W (ρ,ν)}. (5)
f f Γ
ν∈P1(Rd)
Viathedualformulation,theWasserstein-1distanceisanIPMwithΓbeingthespaceof1-Lipschitz
functions. Wasserstein-1proximalsofD areaspecial(f,Γ)-divergencecalledLipschitz-regularized
f
f-divergences,whoseprimalanddualformulationsare
DΓL(ρ∥π)= inf {D (ν∥π)+L·W (ρ,ν)} (6)
f ν∈P1(Rd) f 1
= sup {E [ϕ]−E [f⋆(ϕ)]}, (7)
ρ π
ϕ∈ΓL
where Γ is the space of L-Lipschitz functions and f⋆ is the convex conjugate of f. Lipschitz-
L
regularizedf-divergencesarealsorelatedtotheso-calledMoreau-Yosidaf-divergences[16].
prox (v)=argmin
(cid:2)
f(x)+
λ∥x−v∥2(cid:3)
.Proximaloperatorsareusefulforoptimizingdiscontinuous
λ,f x∈Rd 2 2
ornon-differentiablefunctionsf.
4Manifolddetectingproperties. Bydefinitionofitsprimalformulation(6),thenewdivergence
satisfiesthefollowingfundamentalinequality
0≤DΓL(ρ∥π)≤min{D (ρ∥π),L·W (ρ,π)} . (8)
f f 1
Theinequalityimpliesthat,unlikef-divergences,thenewdivergenceDΓL(ρ∥π)isabletocompare
f
measuresρ,πthataremutuallysingular. Thisisespeciallyimportantwhenweapproximateatarget
distributionπbythegenerativedistributionρ. Whenπissupportedonanunknownmanifold,which
istypicalinhigh-dimensionaldatasetssuchasimages,thegenerativedistributionρcaneasilybe
singularwithrespecttoπ.
Lipschitz-regularized f-divergences are smooth. Another key property of DΓL(ρ∥π) is the
f
existence of a variational derivative with respect to ρ, for any ρ,π ∈ P , without any absolute
1
continuityordensityassumptionsoneitherρorπ [8]. Inaddition, theoptimizerϕ∗ of(7)isthe
variationalderivativeoftheLipschitzregularizedf-divergence
δDΓL(ρ∥π)
f =ϕ∗ =argmax{E [ϕ]−E [f⋆(ϕ)]} , (9)
δρ ϕ∈ΓL ρ π
Apriorithemaximizeronther.h.sof(9)isdefineduniquely,uptoanadditiveconstant,inthesupport
ofρandπbutthevarationalderivativeshouldbeunderstoodastheL-Lipschitzregularizationofthe
optimizerwhichisthendefinedandL-LipschitzcontinuouseverywhereinRd,seeLemma2.3and
Remark2.4in[8]. Inthecontextofadversarialtraining,ϕ∗canbeinterpretedasthediscriminator
[8, 14]. This fact was first proved in [8] (Theorem 2.1), and was used to construct data-driven
WassersteingradientflowsofLipschitz-regularizedf-divergences.
Theexistenceofauniquewell-definedvariationalderivativeforanyperturbationsofallmeasures
in P implies robust distribution learning even for distributions supported on a low-dimensional
1
manifold. Gradient-basedoptimizationmethodsimplicitlyapproximatethevariationalderivative
ofthecostfunctional. IPMs,suchasW ,donothavevariationalderivatives,whilef-divergences
1
cannothandledistributionswithdifferingsupports. TheWasserstein-1proximalregularizationof
f-divergencesinheritsthebeneficialqualitiesoftheW distanceandf-divergences.
1
3 CombiningWasserstein-1andWasserstein-2proximals
ThemaincontributionofthispaperistheuseofbothW andW proximalsoff-divergencesto
1 2
trainnormalizingflowsthatcanlearndistributionssupportedonmanifolds. TheW proximaloff-
1
divergencesproducesLipschitz-regularizedf-divergencesDΓL thatenablescomparingdistributions
f
with differing supports. The W proximal of DΓL stabilizes the training of the generative flow
2 f
by making the optimization problem well-posed. Let ρ be the reference distribution, π be the
0
targetdistributionandρ betheapproximatingdistributiontoπ. ThecombinedWasserstein-1and
T
Wasserstein-2proximaloff-divergencesisdefinedas
(cid:26) (cid:27)
λ
inf DΓL(ρ ∥π)+ ·W2(ρ ,ρ ) (10)
ρT f T 2T 2 0 T
(cid:40) (cid:41)
λ
=inf inf{D (σ∥π)+L·W (ρ ,σ)}+ ·W2(ρ ,ρ ) . (11)
ρT σ f 1 T 2T 2 0 T
(cid:124) (cid:123)(cid:122) (cid:125)
D fΓL=W1proximalofDf
(cid:124) (cid:123)(cid:122) (cid:125)
W2proximalofD fΓL
Asis,itisnotclearhowonemaypracticallycomputetheobjectivefunctional(11). Wedescribea
computableformofthisoptimizationproblemifweconsideroptimizingforacontinuoustimeflow.
TheW distanceisreplacedwiththeBenamou-Brenierformulation(4). Furthermore,tocompute
2
DΓ,weuseitsdualformulation(7)asitisimplementedin[7,8]. Theresultingoptimizationproblem
f
5is
(cid:40) DualformulationofD fΓL (cid:122)Benamou-Brenierdy (cid:125)n (cid:124)amicOTformulation (cid:123)(cid:41)
inf (cid:122) sup (cid:8)E [ϕ(cid:125)(cid:124) ]−E [f⋆(ϕ)](cid:9)(cid:123) +λ(cid:90) T (cid:90) 1 |v(x,t)|2ρ(x,t)dxdt (12)
v,ρ ϕ∈ΓL ρ(·,T) π 0 Rd 2
dx
=v(x(t),t), x(0)∼ρ , t∈[0,T].
dt 0
Again,notethatΓ isthespaceofL-Lipschitzcontinuousfunctionsandf⋆istheconvexconjugate
L
off. Roughlyspeaking,givenaproposedvelocityfieldandsamplesfromπ,theobjectivefunctional
(19)iscomputedbyrunninganensembleoftrajectorieswithinitialconditionsstartingfromreference
distributionρ . Theoptimaltransportcostisapproximatedusingtrajectories. Thefinallocationof
0
theensembledeterminesρ(·,T),whichiscomparedwithsamplesfromπbyoptimizationforaϕ
givenbythedualformulationofDΓ. InSection5,wewilloutlinethealgorithmforcomputingthe
f
objectivefunction(19)indetail. Wecalltheresultingflowstrainedonthisoptimizationproblem
W ⊕W generativeflows.
1 2
Throughagenerativeadversarialnetworkperspective, wecalltheoptimalϕ∗, definedin(9), the
discriminatorthatevaluatesthesimilaritybetweentheflow-generateddistributionρ(·,T)andthe
targetdistributionπ. Thediscriminatorexistsuniquelyuptoanadditiveconstant,iswell-definedand
L-LipschitzonallofRd.
Wedescribefurtherpropertiesofthediscriminatorϕ∗inRemark3.1,andwereferthereaderto[7,8]
forfurthertechnicaldetails. ToconnectwiththenotionofproximaloperatorsinWassersteinspace
(2)[15,10],wenotethatthereferencedistributionρ isrelatedtothegenerateddistributionρ by
0 T
(cid:26) (cid:27)
λ
ρ =proxW2 (ρ )=arg min DΓL(ν∥π)+ ·W2(ρ ,ν) . (13)
T 2λ T,D fΓL(·∥π) 0 ν∈P2(Rd) f 2T 2 0
Remark3.1(CompositionofWassersteinproximalsanditsinterpretation). Onemaynoticethat
whileweusetwoWassersteinproximalstoconstructtheW ⊕W generativeflow,thereisonlya
1 2
single(W )proximaloperatorthatrelatesρ toρ in(13). ThereisanimplicitsecondW proximal
2 T 0 1
operatorbeingcomputedwhencalculatingtheDΓdivergence. Inadditiontoρ ,ρ ,andπ,thereis
f 0 T
afourthdistributionσ∗thatisproducedwhencomparingρ withπ. Wehave
T
σ∗ =proxW1 (ρ )=arg min {D (σ∥π)+L·W (ρ ,σ)}, (14)
L,Df(·∥π) T σ∈P1(Rd) f 1 T
whereσ∗canbeinterpretedasanintermediatemeasurethatisclosetoρ intheW distancewhile
T 1
sharingthesamesupportasπsothattheycanbecomparedviathef-divergence. Moreover,by(13),
weseethatσ∗isequaltoacompositionoftheW andW proximaloperators
1 2
(cid:18) (cid:19)
σ∗ = proxW1 ◦proxW2 (ρ ). (15)
L,Df(·∥π) λ ,DΓL(·∥π) 0
2T f
Themeasureσ∗isnotexplicitlycomputedinouralgorithmasweopttoworkwiththedualformulation
oftheW proximalofD . However,σ∗isrelatedtotheoptimaldiscriminatorϕ∗bytherelation
1 f
dσ∗
(x)=(f⋆)′(ϕ∗(x)−c) (16)
dπ
forsomeconstantc. Thatis,thelikelihoodratioofσ∗ withrespecttothetargetdistributionπ is
equaltothederivativeoff⋆evaluatedataconstantshiftoftheoptimaldiscriminatorϕ∗. See[7],
Theorem25forfurtherdetails.
4 W ⊕W generativeflowsasmean-fieldgames
1 2
Inthissection,westudyournewW ⊕W generativeflowasamean-fieldgame(MFG)anddiscuss
1 2
theconditionsforthecombinedproximaltrainingobjectivetobewell-posed. Wewillseethatthe
well-posednessofouroptimizationcanbeunderstoodthroughMFGtheory. Afterreviewingthe
basics of MFG theory for generative flows, we prove that the W ⊕W generative flows have a
1 2
well-definedPDEsystemevenforhigh-dimensionaldatasupportedonmanifolds. ThePDEsolutions
6areuniqueassumingthatthesolutionissmooth,whichimpliestheoptimizationproblemiswell-
posed, and yields a unique optimal generative flow. This uniqueness property of the generative
flowisexpectedtorendercorrespondingalgorithmsmorestableandnotsubjecttoimplementation-
dependentoutcomes. Moreover,weshowthattheoptimaltrajectoriesarelinearthroughconnections
toPDEtheoryandfluidmechanics.
4.1 Backgroundoncontinuous-timegenerativeflowsasmean-fieldgames
Generative flows were shown to be trained by solving mean-field game problems in [9]. In a
generalsense, themean-fieldgamefordeterministicgenerativeflowsisgivenbyaterminalcost
functionalF :P(Rd)→R,suchastheKLdivergencewithrespecttotargetdistributionπ,which
measuresthediscrepancybetweenπandapproximatingdistributionρ(·,T),andaconvexrunning
costL(x,v):Rd×Rd →R,whichmeasuresthecostparticlemovingataparticularvelocityvat
positionx. Themean-fieldgameis
(cid:90) T (cid:90)
min F(ρ(·,T))+ L(x,v(x,t))ρ(x,t)dxdt
v,ρ 0 Rd (17)
s.t. ∂ ρ+∇·(vρ)=0 ρ(x,0)=ρ (x)
t 0
ViaaLagrangianinterpretation,thecontinuityequationcanbereplacedbyparticletrajectoriesthat
the differential equation dx/dt = v(x(t),t), where x(t) ∼ ρ(·,t). The advantage of the MFG
perspectiveisthatitsassociatedoptimalityconditions,whichareintheformofasystemofbackward-
forwardPDEs,describethemathematicalstructureoftheoptimalvelocityfieldanddensityevolution
aprioritocomputation. Moreover,thelackofoptimalityconditionsimpliesill-posednessofthe
optimizationproblem.
Define the Hamiltonian of the MFG to be the convex conjugate of the running cost function L,
H(x,p) = sup
(cid:2) −p⊤v−L(x,v)(cid:3)
. The optimizers of (17) are characterized by the optimality
v
conditions

∂U δF(ρ(·,T))
−
∂t
+H(x,∇U)=0, U(x,T)=
δρ(·,T)
(x),
(18)
∂ ∂ρ
t
−∇·(∇ pH(x,∇U)ρ)=0, ρ(x,0)=ρ 0(x).
ThefirstequationisabackwardHamilton-Jacobi(HJ)equation,whilethesecondequationisthe
forwardcontinuityequationwiththeoptimalvelocityfieldisdeterminedbythesolutiontotheHJ
equationv∗ =−∇ H(x,∇U). Thewell-posednessoftheoptimizationproblemfortrainingCNFs
p
(17)canthenbestudiedviaPDEtheoryoftheoptimalityconditions(18).
Empirically, the optimization problem for learning canonical CNFs, i.e., when F(ρ) is the KL
divergenceandL(x,v) = 0,isknowntoexhibitinstabilitiesduringtrainingastherearemultiple
flowsthatcanminimizetheobjectivefunctional[5,3,4]. Thisfactcanbeimmediatelyseenthrough
MFGssincewithL = 0,thereisnowell-definedHamiltonian,meaningthereisnoHJequation,
andthereforenooptimalityconditions. Itisalsoeasytoseethatthereareinfinitelymanyoptimal
flowsinthissetting. See[9]forfurtherdetails. Ontheotherhand,withanoptimaltransportcost
L(x,v)= 1|v|2,[4,3]foundthattrainingstabilized,and[4]derivedtheassociatedHamilton-Jacobi
2
equationandusedittofurtheracceleratethetrainingprocess.
4.2 MFGforW ⊕W generativeflowshavewell-definedoptimalityconditionsandimply
1 2
linearoptimaltrajectories
Wasserstein-2proximalsstabilizethetrainingofgenerativeflows,whileWasserstein-1proximals
enablesmanifoldlearning. Inthissection,webackuptheseclaimsbystudyingthePDEsystem
that arises from their optimality conditions. In particular, we will see the W proximal yields a
2
well-defined Hamilton-Jacobi equation for a potential function U(x,t), while the W proximal
1
produceswell-definedterminalconditionU(x,T).
Theorem4.1(MFGforW ⊕W proximalgenerativeflow). Letπbeanunknownarbitrarytarget
1 2
measure,ρ(x,0)=ρ beagivenreferencemeasure,andv :Rd×R→Rdbeavectorfield. Fixa
0
7terminaltimeT >0andλ>0. Theoptimizationproblem
(cid:40) (cid:41)
inf sup (cid:8)E [ϕ]−E [f⋆(ϕ)](cid:9) +λ(cid:90) T (cid:90) 1 |v(x,t)|2ρ(x,t)dxdt (19)
v,ρ ϕ∈ΓL ρ(·,T) π 0 Rd 2
whereρ(x,t)satisfiesthecontinuityequation∂ ρ+∇·(ρv)=0,ρ(x,0)=ρ (x)hasthefollowing
t 0
optimalityconditions:
1. Wasserstein-2 proximal yields a well-posed Hamilton-Jacobi equation. The optimal
velocity v∗ is given by v∗ = −1∇U where U and ρ satisfy a system consisting of a
λ
backwardHamilton-Jacobiequationandaforwardcontinuityequationfort∈[0,T]
 1
−∂ tU + 2λ|∇U|2 =0
(cid:18) (cid:19) (20)
∇U
∂ tρ−∇· ρ
λ
=0 where ρ(x,0)=ρ 0(x).
2. Wasserstein-1proximalprovidesawell-definedterminalcondition. Thedualformulation
andthevariationalderivativeoftheWasserstein-1proximalDΓL(ρ(·,T)∥π),(7)and(9)
f
respectively,providetheterminalconditionoftheHamilton-Jacobiequationgivenas
δDΓL(ρ(·,T)∥π)
U(x,T)= f (x)=argmax(cid:8)E [ϕ]−E [f⋆(ϕ)](cid:9) =ϕ∗(x). (21)
δρ(·,T) ϕ∈ΓL ρ(·,T) π
3. Optimaltrajectoriesarelinear. Theoptimalvelocityfieldisv(x,t)=−1∇U(x,t). Foran
λ
initialconditionx(0)∼ρ(·,0),theoptimaltrajectorieshavethefollowingrepresentations
1 (cid:90) t t
x(t)=x(0)− ∇U(x(s),s)ds=x(0)− ∇U(x(0),0)
λ λ
0
1 (cid:90) t T −t
=x(T)− ∇U(x(s),s)ds=x(T)+ ∇U(x(T),T) (22)
λ λ
T
T −t
=x(T)+ ∇ϕ∗(x(T))
λ
meaningthattheyareexactlylinear.
Proof. TheproofisadirectapplicationofthegeneralMFGoptimalityconditionsin(18)[9].Observe
thattheHamiltonianis
(cid:26) (cid:27)
λ 1
H(x,p)= sup −p⊤v− |v|2 = |p|2. (23)
2 2λ
v∈Rd
Akeyelementofthisproofistheusethevariationalderivative(9)oftheWasserstein-1proximal
DΓL(ρ(·,T)∥π),whichexistsforanyprobabilitymeasuresinP withoutanyabsolutecontinuityor
f 1
densityassumptions,andisequaltothediscriminatorϕ∗,asdiscussedinSection2.2and[8]. This
terminalconditionisL-LipschitzcontinuouseverywhereinRd.
To see that the optimal trajectories are linear, consider the (system of) PDEs associated with the
optimalvelocityfieldv(x,t)=−1∇U(x,t),whichisderivedfromtheHamilton-Jacobiequation
λ
1 1
− ∂ ∇U + ∇U ·∇(∇U)=0 =⇒ ∂ v+(v·∇)v =0. (24)
λ t λ2 t
WeimplicitlyassumethatU issufficientlydifferentiable. SeeRemark4.1forfurthercomments
onregularity. Notethattogetherwiththecontinuityequationin(20),thissystemofPDEsarises
ininviscidcompressiblefluidmechanicsandiscalledtheEulerequations[17]. Next,considerthe
characteristicsforthePDEsofv,i.e.,thelevelcurvesonwhichvisconstant[18]. Lety(t)bethe
characteristicswhichsatisfy
d
v(y(t),t)=∂ v(y(t),t)+∂ y(t)·∇v(y(t),t)=0. (25)
dt t t
8However,from(24),weknowthaty(t)isacharacteristiconlyif
1
∂ y(t)=v(y(t),t)=− ∇U(y(t),t) (26)
t λ
whichisexactlytheevolutionoftheoptimaltrajectoriesx(t)in(22). Thismeansthattheoptimal
trajectoriesarepreciselythecharacteristicsofthevelocityfieldPDEs,implyingthatthevelocityis
constantalongtheoptimaltrajectories. Therefore,v(x(t),t)=v(x(0),0)=−1∇U(x(0),0)for
λ
allt∈[0,T],andso
(cid:90) t t
x(t)=x(0)+ v(x(s),s)ds=x(0)− ∇U(x(0),0). (27)
λ
0
Theoptimaltrajectoriesmayinsteadbecharacterizedbywiththeirterminalendpoints. Alongoptimal
trajectories, v(x(t),t) = v(x(T),T) = −1∇U(x(T),T). Theterminalconditionisequaltothe
λ
discriminator,i.e.,U(x,T)=ϕ∗(x). Therefore,
1 (cid:90) t T −t
x(t)=x(T)− ∇U(x(s),s)ds=x(T)+ ∇U(x(T),T) (28)
λ λ
T
T −t
=x(T)+ ϕ∗(x(T)). (29)
λ
4.3 Wasserstein-1proximalprovidesrobustlearningofdistributionsonmanifolds
TheselectionoftheWasserstein-1proximaldivergence(6)and(7)asterminalconditionin(19)relied
ontwoimportantfeaturesofthisnewdivergenceDΓL(P∥Q): (a)itcancomparemutuallysingular
f
measures—apropertyinheritedbyWasserstein-1—ascanbereadilyseenin(8);(b)itissmooth,i.e.
hasavariationalderivative(9),apropertyinheritedbytheKLdivergenceorotherf-divergence. The
latterisimportantinthewell-posednessofthegenerativeflowascapturedbytheMFGoptimality
conditions(20)and(21)ofTheorem4.1.
f-divergencesfailtolearnmanifolds. Whenanf-divergenceisusedastheterminalcosttotrain
generativeflows,evenwithaW proximal,themean-fieldgame(19)willnotbewell-posed. The
2
terminalcondition(21)isthevariationalderivativeofF(ρ)=D (ρ∥π)forafixedπ, δDf(ρ∥π) =
f δρ
(cid:16) (cid:17)
(f⋆)′ dρ . Inparticular,fortheKLdivergence
dπ
δD (ρ∥π) dρ
KL (x)=1+log (x). (30)
δρ dπ
IfρisdefinedasthepushforwardofastandardnormaldistributioninRdunderinvertiblemapping,
andthesupportofπisonalowerdimensionalmanifold,thentheterminalconditionwillblowupfor
x ∈/ supp(π). Inessence,theKLdivergencebecomesuninformativeforlearning,whenthetarget
distributionissupportedonadifferentmanifoldthanρ.
W proximalofD providesrobustmanifoldlearning First,(8)impliesthattheWasserstein-1
1 f
proximalregularizedf-divergenceball,{ρ : DΓL(ρ∥π) ≤ 1}containsboththef-divergenceball
f
{ρ:DΓL(ρ∥π)≤1}andtheWasserstein-1ball{ρ:L·W (ρ,π)≤1}. Thisisanimportantpoint
f 1
inourcontextbecausealgorithmicallyaKLoranf-divergenceballaroundatargetdistributionπ
thatissupportedonalow-dimensionalmanifold,asistypicalinmanyhigh-dimensionaldatasets
suchasimages,willonlyincludeotherdistributionswithsupportsonthesame(unknown)manifold.
OntheotherhandaballinDΓL(·∥π)doesnothavethisconstraintandhenceitisexpectedtobe
f
mathematicallyandcomputationallymorerobustforgenerativetasks. Inparticular,whenlearninga
targetdistributionπsupportedonanunknownmanifold,generatedmodelsρ caneasilybesingular
T
with respect to π unless great care is employed using carefully constructed autoencoders and/or
neuralarchitectures. OuruseoftheWasserstein-1proximalDΓL(·∥π)doesawaywithsuchneeds,as
f
Theorem4.1suggestsandaswedemonstratecomputationallyinSection6. Thisdiscussionalsohas
connectiontodistributionallyrobustoptimization(DRO)[19].
9Well-definedvariationalderivativeimpliesstabletraining Thevariationalderivativeϕ∗in(9)
fortheterminalconditionexistsandisalwayswell-definedduetoourchoiceoftheterminalcost
F as the Wasserstein-1 proximal of an f-divergence DΓL(·∥π). Here the use of a proximal is
f
necessary: (i)inthecaseofapuref-divergencethevariationalderivativewillblowupformutually
singularmeasures,while(ii)inthecaseofapureWasserstein-1distance,thereisnouniquelydefined
variationalderivative. TheMFGwell-posednessanalysisofTheorem4.1allowsustomakerigorous
the following heuristics. In the optimization process over possible velocities v = v(x,t) in our
learningobjectiveinTheorem4.1,namely
(cid:40) (cid:41)
(cid:90) T (cid:90) 1
inf DΓL(ρ ∥π)+λ |v(x,t)|2ρ(x,t)dxdt , (31)
v,ρ f T 0 Rd 2
theexistenceofthevariationalderivative(9)oftheterminalcostDΓL(ρ ∥π)foranyperturbations
f T
ofmeasuresincludingsingularones,suggeststhatthisminimizationproblemissmoothinvandthe
optimalgenerativeflowcanbetheoreticallydiscoveredbygradientoptimization. Wediscussand
implementsuchalgorithmsinspiredbyTheorem4.1inSection5.
4.4 UniquenessofW ⊕W generativeflowsimplieswell-posednessofoptimizationproblem
1 2
InTheorem4.1wehaveshownthattheoptimizationproblem(19)andthecorrespondingbackward-
forwardMFGsystem(20)arewell-definedinthesensethattheHamiltonianHexistsandtheterminal
condition(21)isalwayswell-defined. Nextweshowthat(20)and(21)haveauniquesmoothsolution
foraboundeddomainwithperiodicboundaryconditions,i.e.,atorus. Thisisakeyresultofthis
paperbecauseitimpliesthatthecorrespondinggenerativeflowwithvelocityfieldv∗(x,t)isalso
uniqueandthereforecanbelearnedinastablemanner. Wewillalsodemonstratethatlatterpointin
ourexperimentsinSection6.
Theorem 4.2 (Uniqueness of Wasserstein-1/Wasserstein-2 proximal generative flows). If the
backward-forward PDE system (20) with terminal condition (21) has smooth solutions (U,ρ∗)
onthetorusΩ,thentheyareuniqueandthesolutiontotheoptimizationproblem(19)isalsounique.
WefollowthestrategyoutlinedinTheorem2.5of[20]. Themainideaoftheproofistoconsiderthe
weakformofthecontinuityequationwhereweusethesolutionsoftheHJequationastestfunctions.
Wefocusontheperiodicdomain(torus)caseforthesakeofsimplicity,althoughotherboundary
conditionscanbeconsidered,see[20]. Theproofrequirestwoproperties: (a)theconvexityofthe
HamiltonianH(x,p)= 1 |p|2;and(b)theterminalconditions(21)isnecessarilyincreasinginsome
2λ
suitablesenseinρ. ConvexityoftheHamiltonianisclear. Asforthemononicityrequirement,we
firstprovethefollowingLemma4.1. TheresultisbasedonthepropertiesoftheW -proximal.
1
Lemma 4.1 (Monotonicity of (21)). If ϕ∗ is the optimizer (21) of the dual problem (7) for
i
D (ρ (·,T)∥π)fori=1,2,then
f i
(cid:90)
(ϕ∗(x)−ϕ∗(x))d(ρ (·,T)−ρ (·,T))(x)≥0 (32)
1 2 1 2
Ω
forallρ (·,T),ρ (·,T)∈P(Ω).
1 2
Proof. Forthesimplicitywewriteµ = ρ (·,T)andµ = ρ (·,T). Werewrite(32)andusethe
1 1 2 2
optimalityofϕ∗forthedualformulation(7)ofDΓL(µ ∥π)withi=1,2toget
i f i
E [ϕ∗]−E [ϕ∗]−E [ϕ∗]+E [ϕ∗]
µ1 1 µ2 1 µ1 2 µ2 2
={D fΓL(µ 1∥π)+E π[f⋆(ϕ∗ 1)]}−E µ2[ϕ∗ 1]−E µ1[ϕ∗ 2]+{E π[f⋆(ϕ∗ 2)]+D fΓL(µ 2∥π)} (33)
≥DΓL(µ ∥π)−DΓL(µ ∥π)−DΓL(µ ∥π)+DΓL(µ ∥π)≥0.
f 1 f 2 f 1 f 2
WenowproveTheorem4.2.
10Proof. Let(U ,ρ )and(U ,ρ )betwosmoothsolutionsof(20)and(21). DefineU = U −U
1 1 2 2 1 2
andρ=ρ −ρ . WeproveU =0andρ=0. BasedonLemma4.1,wehavethemonotonicityof
1 2
theterminalconditionU(·,T). Furthermore,H(x,p) = 1 |p|2,isconvexwithrespecttothelast
2λ
variable. Withoutlossofgenerality,letλ=1. Observethat∇2H(x,p)=I . Nowobservethat
p d
d (cid:90) (cid:90)
Uρdx= [(∂ U)ρ+U(∂ ρ)]dx
dt t t
Ω Ω
(cid:90)
= (H(x,∇U )−H(x,∇U ))ρdx (34)
1 2
Ω
(cid:90)
+ U(∇·(ρ ∇ H(x,∇U ))−∇·(ρ ∇ H(x,∇U )))dx.
1 p 1 2 p 2
Ω
Viaintegrationbyparts,with⟨·,·⟩denotingthestandardinnerproductbetweenvectorsinRd,weget
(cid:90)
(34)= [(H(x,∇U )−H(x,∇U ))ρ−⟨∇U,ρ ∇ H(x,∇U )−ρ ∇ H(x,∇U )⟩]dx
1 2 1 p 1 2 p 2
Ω
(cid:90)
=− ρ [H(x,∇U )−H(x,∇U ))−⟨∇U −∇U ,∇ H(x,∇U )⟩]dx (35)
1 2 1 2 1 p 1
Ω
(cid:90)
− ρ [H(x,∇U )−H(x,∇U ))−⟨∇U −∇U ,∇ H(x,∇U )⟩]dx.
2 1 2 1 2 p 2
Ω
DuetotheconvexityofH,aTaylorseriesexpansionyields
d (cid:90) (cid:90) ρ +ρ
(35)= Uρdx≤− 1 2|∇U −∇U |2dx≤0. (36)
dt 2 1 2
Ω Ω
Byintegratingthisinequalityonthetimeinterval[0,T]weobtain
(cid:90) (cid:90) (cid:90) T (cid:90) ρ +ρ
U(x,T)ρ(x,T)dx≤ U(x,0)ρ(x,0)dx− 1 2|∇U −∇U |2dxdt (37)
2 1 2
Ω Ω 0 Ω
Usingthatρ(x,0)=0andthemonotonicity(32),wehavetheLHSof(37)isnonnegative. Itimplies
(cid:82)
that U(x,T)ρ(x,T)dx=0resultingin
Ω
∇U =∇U in{ρ >0}∪{ρ >0}. (38)
1 2 1 2
This proves that ρ solves the same equation as ρ , with the same drift ∇ H(x,∇U ) =
1 2 p 1
∇ H(x,∇U )andthereforeρ =ρ . CorrespondinglyU andU solvethesameHJequationand
p 2 1 2 1 2
thereforeU =U ,astheysharethesameterminalcondition(21).
1 2
From a computational perspective, this theorem provides confidence that any numerical imple-
mentation to the optimization problem (12) is approximating a single unique limiting solution.
Algorithmically,thismeansthatthetrainingwillnotoscillatebetweenmultiplemodesrepresenting
differentpossiblelimitingflows.
Remark4.1. InTheorem4.2weassumedtheexistenceofclassicalsolutionstoproveuniqueness.
HoweverfirstorderHJdonotalwayshaveclassicalsolutions,hencefurtherinvestigationisneeded
in future work. For instance, we intend to consider stochastic normalizing flows for which the
correspondingMFGhasclassicalsolutions[20],andauniquenessresultinthespiritofTheorem4.2
canholdwithoutadditionalregularityassumptions.
5 Proximalregularizationproducesadversarialtrainingalgorithm
Inthissection,weprovidemoredetailsastohowtheobjectivefunctionfortrainingW ⊕W proximal
1 2
generativeflowsarecomputed. Themean-fieldgameformulationin(19)enablesimplementation
of the new flow through adversarial training algorithms. This is due to the dual formulation for
computingtheDΓdivergence,andhasbeensuccessfullyappliedingenerativemodelingtasksthrough
f
DΓ-basedgenerativeadversarialnetworks[7,14,13],andWasssersteingradientflowsofDΓ[8]. A
f f
generativeadversarialflowhaspreviouslybeenproposedin[12]. Ourformulation,however,resolves
thepersistentill-posednessissues.
115.1 AnadversarialtrainingalgorithmforW ⊕W generativeflows
1 2
Wecomputetheobjectivefunctionin(19),seealso(12),andwewillseethatincontrasttonormalizing
flowsanditsW proximalregularizations[2,3,4],weonlyneedtosimulateforwardtrajectories
2
oftheflowduringtraining. Supposewearegivenadataset{X(n)}N ∼π. Letthediscriminator
n=1
ϕ(x;θ )andpotentialfunctionU(x,t;θ )beparametrizedvianeuralnetworks,withparameters
ϕ U
θ andθ ,respectively. RecallthatϕneedstobeL-Lipschitzcontinuous. Fixaninitialreference
ϕ U
distribution ρ (e.g., a normal distribution), and simulate M independent trajectories with initial
0
condition{Y(m)}M ∼ρ andvelocityfieldv(y,t)=−1∇ U(y(t),t). Thenumberofsimulated
0 m=1 0 λ x
trajectoriesM donotneedtomatchthenumberofsamplesN fromπ. Wefollowadiscretize-then-
optimizeapproachfortrainingthemodelparameters,asdescribedin[4]. Eachtrajectoryissimulated
usingaforwardEulerscheme. Discretizethetimeinterval[0,T]intoK equalsegmentsoflengthh
sothatt =T =Kh. Thenforthemthtrajectoryattimet =kh,wehavetherecurrencerelation
K k
h
Y(m) =Y(m)− ∇U(Y(m),kh)∼ρ(·,(k+1)h), fork =0,...,K−1. (39)
k+1 k λ k
WefirstestimatethedivergenceDΓ(ρ(·,T)∥π)byfindingtheoptimalϕ∈Γ
f L
(cid:40) M N (cid:41)
maxJ (θ )=max 1 (cid:88) ϕ(cid:16) Y(m);θ (cid:17) − 1 (cid:88) f⋆(cid:16) ϕ(X(n);θ )(cid:17) +L (θ ) (40)
θϕ ϕ ϕ θϕ M
m=1
K ϕ N
n=1
ϕ ϕ ϕ
where L (θ
)=−min (cid:88)(M,N) max(cid:18)(cid:12)
(cid:12)∇ϕ(c Y(n)+(1−c )X(n);θ
)(cid:12) (cid:12)2 −L2,0(cid:19)
. (41)
ϕ ϕ (cid:12) n K n ϕ (cid:12)
n=1
Here, L (θ ) penalizes discriminators that have Lipschitz constant greater than L for random
ϕ ϕ
convex combinations of Y(n) and X(n) [7, 21]. Here, c is randomly sampled from a uniform
K n
distributionon[0,1]. Discrminatorparametersθ areupdatedaccordingtoagradientascentmethod
ϕ
θ(l+1) =θ(l)+η∇ J (θ(l))forsomelearningrateη. Letϕ∗(x)=ϕ(x;θ∗)theresultingoptimal
ϕ ϕ θϕ ϕ ϕ ϕ
discriminator. Thereafter,theobjectivefunctionJ(θ )forlearningtheparametersofU isestimated
U
by
J (θ )=
1 (cid:88)M ϕ∗(cid:16) Y(m)(cid:17)
−
1 (cid:88)N f⋆(cid:16) ϕ∗(X(n))(cid:17)
+
λh (cid:88)M K (cid:88)−1(cid:12) (cid:12) (cid:12)∇U(Y k(m),kh;θ U)(cid:12) (cid:12) (cid:12)2
.
U U M K N 2M (cid:12) λ (cid:12)
(cid:12) (cid:12)
m=1 n=1 m=1 k=0
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
EstimatesD fΓL(ρ(·,T)∥π) Estimatesλ(cid:82) 0T(cid:82) Rd 1 2|v(x,t)|2ρ(x,t)dxdt
(42)
Modelparametersθ aresimilarlyupdatedviagradientdescent: θ(l+1) = θ(l) −η∇ J (θ(l)).
U U U θU U U
Fromhere,newsampletrajectoriesaresimulatedusingthepotentialatstepl+1,U(l+1)(x,t) =
U(x,t;θ(l+1))andtheparametersareupdatedagain. LikeGANs,thereisaminimaxstructureinthe
U
optimizationproblem,wheretheinneroptimizationproblemissolvedforeachupdateoftheouter
problem. Ourproposedgenerativemodelcanbeviewedasanadversarialgenerativeflowgivenin
termsofaminimaxproblemforadiscriminatorϕ∗andapotentialfunctionU∗. Thepseudocodefor
learningU∗andϕ∗aredescribedinAlgorithms1and2,respectively.
5.2 Hamilton-Jacobioptimalityindicators
Theoptimalityconditions(20)and(21)ofTheorem4.1provideaposteriorierrorestimatorsthat
mayprovideinsightintothequalityofthelearnedgenerativeflow. RecallthattheHamilton-Jacobi
equation(43)anditsterminalcondition(44)characterizetheoptimalvelocityfieldofthegenerative
flow,whichmeansattheendoftraining,thelearnedflowshouldsatisfytheHJequation.
WedefinetheHamilton-JacobiresidualandtheHamilton-Jacobiterminalconditionerrorasfollows:
HJresidual:R HJ(θ U)= Mh (cid:88)M K (cid:88)−1(cid:12) (cid:12) (cid:12) (cid:12)∂ tU(Y k(m),hk;θ U)+ 21 λ(cid:12) (cid:12) (cid:12)∇U(Y k(m),hk;θ U)(cid:12) (cid:12) (cid:12)2(cid:12) (cid:12) (cid:12)
(cid:12)
(43)
m=1 k=0
HJterminalconditionerror: R (θ )=
1 (cid:88)M (cid:12)
(cid:12)∇U(Y(m),T;θ
)−∇ϕ∗(Y(m))(cid:12)
(cid:12). (44)
T U M (cid:12) K U K (cid:12)
m=1
12Algorithm1TrainingW ⊕W proximalgenerativeflows
1 2
1: Input: Dataset{X(n)}N ∼π
n=1
2: Initialize: Discriminatorϕ(x;θ ϕ)andpotentialfunctionU(x,t;θ U)withparametersθ ϕandθ U,
LipschitzconstantL,initialreferencedistributionρ ,numberoftrajectoriesM,timeinterval
0
[0,T],withstepsizeh,T =Kh,learningrateη,Nϕ ,NU
iter iter
3: forl←1toNU do
iter
4: SampleM initialconditions{Y 0(m)}M m=1 ∼ρ 0
5: form=1toM do ▷Simulatetrajectories
6: fork =0toK−1do
7: Y k( +m 1) =Y k(m)− λh∇U(Y k(m),kh;θ U)
8: endfor
9: endfor
10: ϕ∗(x)←LEARNDISCRIMINATOR({X(n)},{Y(m)},L,η,Nϕ )
K iter
11: D U(θ U)← M1 (cid:80)M m=1ϕ∗(Y K(m))− N1 (cid:80)N n=1f⋆(ϕ∗(X(n))) ▷EstimateD fΓL(ρ T∥π)
(cid:12) (cid:12)2
12: J U(θ U)←D U(θ U)+ 2λ Mh (cid:80)M m=1(cid:80) kK =− 01(cid:12) (cid:12) (cid:12)∇U(Y k(m λ),kh;θU)(cid:12) (cid:12)
(cid:12)
▷Estimateobjective
13: θ U ←θ U −η∇ θUJ U(θ U) ▷Updateθ U viagradientdescent
14: R( Hl)
J
← Mh (cid:80)M m=1(cid:80)K k=− 01(cid:12) (cid:12) (cid:12) (cid:12)∂ tU(Y k(m),hk;θ U)+ 21 λ(cid:12) (cid:12) (cid:12)∇U(Y k(m),hk;θ U)(cid:12) (cid:12) (cid:12)2(cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12) (cid:12)
15: R( Tl) ← M1 (cid:80)M m=1(cid:12) (cid:12)∇U(Y K(m),T;θ U)−∇ϕ∗(Y K(m))(cid:12)
(cid:12)
▷Optimalityindicators(43),(44)
16: endfor
Algorithm2Learningthediscriminatorϕ∗(x)
1: functionLEARNDISCRIMINATOR({X(n)}N ,{Y(m)}M ,L,η,Nϕ )
n=1 K m=1 iter
2: Initialize: Discriminatorϕ(x;θ ϕ)withparametersθ ϕ
3: forl←1toNϕ do
iter
4: c n ∼U[0,1]forn=1,...,min(M,N)
5: Z(n) ←c nY K(n)+(1−c n)X(n)forn=1,...,min(M,N)
6: L ϕ(θ ϕ)←−(cid:80)m n=in 1(M,N)max(cid:16)(cid:12) (cid:12)∇ϕ(Z(n);θ ϕ)(cid:12) (cid:12)2 −L2,0(cid:17) ▷Lipschitzregularization
7: D ϕ(θ ϕ)← M1 (cid:80)M m=1ϕ(Y K(m);θ ϕ)− N1 (cid:80)N n=1f⋆(ϕ(X(n);θ ϕ))
8: J ϕ(θ ϕ)←D ϕ(θ ϕ)+L ϕ(θ ϕ)
9: θ ϕ ←θ ϕ+η∇ θϕJ ϕ(θ ϕ) ▷Updateθ ϕviagradientascent
10: endfor
11: returnϕ∗(x)=ϕ(x;θ ϕ)
12: endfunction
13These error functions are not used for training, but rather are used as a posteriori indicators of
optimality of the learned generative flow. This is in constrast to [4], where the Hamilton-Jacobi
equationisusedasanadditionalregularizerduringtraining. DuetoTheorem4.2,thereisaunique
(smooth)optimalgenerativeflowandisexpectedtosatisfytheoptimalityconditions(20)and(21).
HenceanycomputedgenerativeflowbasedonAlgorithm1canbetestedagainstthecorresponding
HJresiduals(43)and(44). Inthissensetheycanalsobeusedasanoptimalityindicatorforreal-
timeterminationofoursimulationwhentheHJresidualsaresufficientlysmall. InSection6,we
successfullydemonstratethealgorithmonaMNISTdatasetthatavoidstheuseofautoencoders. We
showthattheoptimalityconditionareeventuallysatisfiedinthecourseofourtraining. Additional
computationalcostsforcalculating(43)and(44)takeO(KM)andO(Md)gradientevaluations,
respectively,whereM,K aredefinedin(43)and(44).
5.3 Adversarialtrainingeliminatestheneedforforward-backwardsimulation
Continuous Normalizing Flow (CNF) models, such as OT flow [4] requires both forward and
backwardsimulationofinvertibleflowsduringtraining. Thesemodelslearntheflowsintheforward
normalizingdirection
(cid:90) T (cid:90) 1
infD (ρ(·,T),N )+λ |v(x,t)|2ρ(x,t)dxdt (45)
v,ρ KL d 0 Rd 2
whereN referstothed-dimensionalGaussiandistribution,andgeneratesamplesusingthebackward
d
flowsγ −∇·(γv)=0forγ(·,t)=ρ(·,T−t). Amainreasonfortheforward-backwardsimulation
t
isbecausethelikelihoodneedstobecomputedbyintegratingoverthepathtoestimatethevalue
oftheKLdivergence. However,inhigh-dimensionalexamples,thedataπ = ρ oftenlieonlow
0
dimensionalmanifoldsandimmediatelytwochallengesarise.
First,duetothelackofabsolutecontinuity,aKL-ballin(45)failstocontainanoptimizer,leadingto
unstabletrainingandmathematicallytoanundefinedterminalconditionfortheHJequationU(·,T)
ifweuseD astheterminalconditioninsteadoftheWassersten-1proximalinTheorem4.1. A
KL
secondmathematicalandalgorithmicreasonforthefailureofCNFsinthecasewherethetargetis
supportedonalowerdimensionalmanifold: thereisnoinvertiblemappingbetweentheGaussianand
atargetdistributionπsupportedonalower-dimensionaldistribution,leadingtonon-existenceofan
inversetransportmapfromN toρ =π. Forthisreason,autoencodersarenecessaryforOTflow
d 0
[4]andpotentialflowgenerators[6]toembedtheflowsintotheappropriatespacesbeforeapplying
thegenerativemodels.
UnlikeCNF-basedmodelssuchas(45),ourmodelinAlgorithm1,basedonTheorem4.1,bypasses
bothsuchchallengesthrough: (a)adversarialtraining,whichdoesnotrequireinversionoftheflow,
and(b)Wasserstein-1proximalregularizedf-divergences,whichdonotrequireabsolutecontinuity
betweenmeasures. Thereforeourmodelisabletogenerateimageswithouttheaidofautoencoders,
aswedemonstratenext.
6 Numericalresults
Wedemonstratethatourproposedgenerativeflows(19)formulatedcombiningWasserstein-1and
Wasserstein-2proximalregularizationsfacilitatelearningdistributionsthataresupportedonlow-
dimensionalmanifolds. WelearntheMNISTdatasetonΩ = [0,1]784 usingtheadversarialflow
modeldescribedinAlgorithm1,withouttheuseofanyautoencoders. Todemonstratetheimpactof
Wasserstein-1andWasserstein-2proximalregularizations,weconsiderthefollowingfourtestcases:
1. Unregularized flow: no running cost penalty for t ∈ (0,T) and the terminal cost is the
standarddualformulationoff-divergences
(cid:40) (cid:41)
inf sup (cid:8)E [ϕ]−E [f⋆(ϕ)](cid:9) : dx =v(x(t),t), x(0)∼ρ(·,0) (46)
v ϕ∈C(Rd) ρ(·,T) π dt
2. Wasserstein-2proximaloff-divergenceflow: optimaltransportcostfort∈(0,T)andthe
terminalcostisthestandarddualformulationoff-divergences
14(cid:40) (cid:41)
inf sup (cid:8)E [ϕ]−E [f⋆(ϕ)](cid:9) +λ(cid:90) T (cid:90) 1 |v(x,t)|2ρ(x,t)dxdt (47)
v ϕ∈C(Rd) ρ(·,T) π 0 Rd 2
dx
s.t. =v(x(t),t), x(0)∼ρ(·,0)
dt
3. Wasserstein-1proximalflowoff-divergenceflow: norunningcostpenaltyfort∈(0,T)
andtheterminalcostistheregularizeddualformulationoff-divergences
(cid:40) (cid:41)
inf sup (cid:8)E [ϕ]−E [f⋆(ϕ)](cid:9) : dx =v(x(t),t), x(0)∼ρ(·,0) (48)
v ϕ∈ΓL ρ(·,T) π dt
4. ComposedWasserstein-1/Wasserstein-2proximalflow(W1/W2proximalflow): optimal
transportcostfort∈(0,T)andtheterminalcostistheregularizedf-divergence
(cid:40) (cid:41)
inf sup (cid:8)E [ϕ]−E [f⋆(ϕ)](cid:9) +λ(cid:90) T (cid:90) 1 |v(x,t)|2ρ(x,t)dxdt (49)
v ϕ∈ΓL ρ(·,T) π 0 Rd 2
dx
s.t. =v(x(t),t), x(0)∼ρ(·,0)
dt
For our numerical experiments, we choose the Lipschitz constant to be L = 1, the weighting
parameterstobeλ=0.05,theterminaltimeT =5,timestepsizeh=1.0,andf(x)=−log(x)
forthereverseKLdivergence. Inaddition,thediscriminatorϕandthepotentialU shareacommon
neuralnetworkarchitecture;seeAppendixA.3.
Figure2demonstratestheindispensabilityofusingtheWasserstein-1proximalregularizationfor
stabilizingthetrainingprocedure. Observethattheflowsthatarenotregularizedoronlyrelyon
Wasserstein-2 regularization fail in the middle of their training procedures as their terminal cost
blowsupsoonafterinitialization(seeredandgreencurvesinFigure2a). Thisisbecausetheterminal
costcannoteffectivelycomparemeasuresρ(·,T)(whichisabsolutelycontinuouswithrespectto
ρ(·,0)andπ. TheWasserstein-2proximaldoesnotavoidthisissueasitalonecannotmakeupforan
ill-definedterminalcondition.
Moreover, we see in Figure 2b that for the flow trained on only the Wasserstein-1 proximal, the
resultingflowhassubstantiallyhigherkineticenergythanthetheflowthatusesbothproximals. This
impliesthatthetrajectoriesarenotasregularorpotentiallystronglynonlinear. Moreover,itislikely
thatthelearnedflowisstronglydependentontheparticularparametrizationofthevelocityfields.
Ourcontinuous-timeflowmodelstandsoutforitsabilitytolearnhigh-dimensionalproblemsdirectly
intheoriginalspace,withoutspecializedarchitectureortheuseofautoencodersasinnormalizng
flows. OT-flow[4]andthepotential-flowgenerator[6]appliesautoencoderstogetlatentrepresenta-
tionsofthetargetinlowerdimensionalspacesandlearnnormalizingflowsinthelowerdimensional
space. Additionally,ourflowcanbelearnedusingadiscretize-then-optimize(DTO)approachwith
justafewtimestepsK = 5. Weareabletousesuchacrudestepsizeastheoptimaltrajectories
areknowntobeprovablylinear(Theorem4.1). Thisdemonstratesthatourproblemformulation
(19)enablesmoreefficienttrainingcomparedtomethodswithhighlyengineeredarchitecturessuch
as the 10-layer-stacked CNF setting used for RNODE [3]. Our approach is similar to the GAN
implementationofthePotentialflowgenerator[6]. Butouralgorithmdiffersfromitinthesense
thatwesolvetheWasserstein-2regularizedproblemwithaWasserstein-1regularizedterminalcost
whosecombinationprovideswell-definedoptimalityconditions(20),(21)withoutimposingthemas
regularizationtermsintheobjectivefunction.
InFigure3,wecanseethatwhenthetwoproximalsarecombined,theresultinggenerativeflows
arediscretization-invariantasthegeneratedimagesdonotchangewiththestepsize. Thisfurther
supportingourclaimthatawell-posedgenerativedoesnothaveparametrization-dependentperfor-
mance. ThisbehaviorisduetoTheorem4.1,whichshowthattheoptimaltrajectoriesarelinear,so
thereisnoincreaseddiscretizationerrorswithlargerstepsizes. Ontheotherhand,whenonlytheW
1
proximalisapplied,weseethatsomeimagesmaychangewithdifferentstepsizes,meaningthatthe
trajectoriesareincurringextradiscretizationerrorwithlargerstepsizesastheyarenotlinear.
15(a)TerminalcostDΓL(ρ(·,T)∥π)(7) (b)Kineticenergy
f
(c)Optimalityindicator(44) (d)Optimalityindicator(43)
Figure 2: Evaluation of learning objectives (a - b) and optimality indicators (c - d) from MFG
optimalityconditionsoverthecourseoftraining.WelearntheMNISTdatasetusingthefourtestcases
withT =5.0,h=1.0. Wechooseλ=0.05andL=1fortheweightparamtersofWasserstein-2
andWasserstein-1proximals,respectively. InFigure(a),observethattheterminalcostF(ρ(·,T))=
DΓL(ρ(·,T)∥π)diverges(green,red)withouttheWasserstein-1proximalregularization. (b)The
f
Wasserstein-2proximaladditionallyregularizestheflowtohavelowerkineticenergyandtobeless
oscillatorytrainingobjectives. LessoscillationisalsorelatedtotheuniquenessoftheMFGsolution
inTheorem4.2whichinturnisexpectedtorenderthealgorithmsmorerobust,i.e. inourcontextnot
susceptibletoimplementation-dependentchoices. (c)Asinferredfrommean-fieldgame,thelearning
problemwithoutWasserstein-1proximalregularizationlacksawell-definedterminalcondition. The
explodingoptimalityindicator(44)exemplifiesthisbehavior. (d)W ⊕W proximalgenerative
1 2
flowresultsinlowervaluesoftheoptimalityindicator(43)comparedtothosefromWasserstein-1
proximalregularizedflow. (c-d)showthattheoptimalityindicatorscaninformwhenanoptimal
generativeflowhasbeendiscovered.
7 Discussionandconclusion
Inthispaper,weintroducedW ⊕W proximalgenerativeflows,whichcombinesWasserstein-1
1 2
and Wasserstein-2 proximal regularizations for stabilizing the training of continuous-time deter-
ministic normalizing flows, especially for learning high-dimensional distributions supported on
low-dimensionalmanifolds. TheW proximalregularizationenforceflowsthatminimizekinetic
2
energy, resulting in straight line paths that are simpler to learn. We showed in Theorem 4.1 that
theoptimaltrajectoriesareexactlylinear,andweempiricallydemonstratedthisfactinFigure3by
showingthetrainedflowisdiscretizationinvariant.
For learning distributions on manifolds, we choose the terminal cost for the generative flow to
beWasserstein-1proximalregularizationoff-divergences,whichcancomparemutuallysingular
measures. Typically targets are supported on an unknown data manifold, so generated models
can easily miss the manifold and be singular with respect to the target distribution. In addition,
Wasserstein-1proximalregularizationsoff-divergencesinheritfromf-divergencestheexistence
ofvariationalderivativesforanyperturbationsofmeasures,includingsingularones,whereasboth
16(a)GeneratedsamplesfromW ⊕W proximalgenerativeflow(49)withdifferenttimestepsizesh=20(left),
1 2
h=2−3(center),h=2−6(right)
(b)GeneratedsamplesfromW proximalflow(48)withdifferenttimestepsizesh = 20 (left), h = 2−3
1
(center),h=2−6(right)
Figure3: Wasserstein-2proximalregularizationimpliesdiscretizationinvarianceingenerativeflows.
After learning the MNIST dataset W ⊕W and W proximal generative flows with T = 5.0,
1 2 1
h=1.0,wegeneratedsamplesbyintegratingthelearnedvectorfieldx˙(t)=−1∇U(x(t),t)over
λ
time with different time step sizes h. In (a), we see that Wasserstein-2 proximal regularization
providesalmoststraightflowtrajectorieswhichleadstogeneratedsampleswhicharealmostinvariant
totimediscretization. Thisrobustnessofthecontinuoustimeflowgeneratorensureshighfidelityof
generatedsamplesregardlessoftimediscretization. Thisempiricalobservationisjustifiedbythe
theoreticalresultin(22)ofTheorem4.1. Ontheotherhand,weseein(b)thatwithoutWasserstein-2
regularization,theresultingvectorfieldismoresensitivetovaryingstepsizesascertaindigitsmay
fliptootherones.
f-divergences and the Wasserstein-1 distance fail in that respect. This differentiability property
suggeststhatcorrespondingminimizationproblemsaresmoothandtheoptimalgenerativeflowcan
berobustlydiscoveredbygradientoptimization. Inthenumericalexample,wesawthatwithouta
W proximal,thetrainingobjectivewoulddivergequickly.
1
OurmathematicalanalysisofW ⊕W flowsandtheircorrespondingalgorithms,reliesonthetheory
1 2
ofmean-fieldgameand,morespecifically,onthewell-posednessoftheMFGoptimalityconditions,
consistingofabackwardsHamilton-JacobiequationcoupledwithaforwardcontinuityPDE.The
potentialfunctionfortheoptimalgenerativeflowwillsolvetheHJequation,whilethevariational
derivativeoftheWasserstein-1proximal(9)isexactlytheterminalconditionoftheHJequation,
whichiswell-definedforanydistributions. Wewereabletoexpresstheoptimalityconditionsof
W ⊕W generative flows in Theorem 4.1, meaning that the optimality conditions are at least
1 2
well-defined. WethenprovedinTheorem4.2,auniquenessresultfortheMFGofourWasserstein-1
andWasserstein-2proximalflow. Thisshowedthattheoptimalgenerativeflowisandunique,and
thereforetheoptimizationproblemiswell-posed,implyingthatitshouldberobusttodifferentvalid
discretizationsandapproximations.
WederivedanadversarialtrainingalgorithmforlearningW ⊕W proximalgenerativeflowsusing
1 2
thedualformulationoftheWasserstein-1proximal. Duetothisadversarialstructure,noforward-
backward simulation of the flow is required, which is in contrast to likelihood-based training of
CNFs. TheresultinggenerativeflowismorerobustforlearningdatasetssuchastheMNISTdataset
withouttheuseofanypre-trainedautoencodersortuningarchitecturesasistypicallyrequiredin
CNFs[1,2,3,4,6].
Finally,wealsodemonstratedtheuseoftheHJoptimalityconditionsasanewreal-timecriterionfor
optimalityofsolutionsandterminationofthealgorithm. Wefoundthemtobemoreinformativefor
17determiningwhentostoptrainingtheflowratherthanimposingthemasadditionalregularizersinthe
learningobjectivesasinsomerelatedwork[4,6].
AnappealingattributeofW ⊕W generativeflowsisthefacttheycanbetrainedwithonlyforward
1 2
simulationsofthesampletrajectories. Forfuturework,wemayexplorenewapproachesfortraining
stochasticnormalizingflows,whichhasbeenachallengeasbackwardSDEsaredifficulttosimulate
efficiently[22,23]. Moreover,wemayinvestigateW ⊕W generativeflowsanditsconnections
1 2
andapplicationstodistributionallyrobustoptimization(DRO)[19]. AsW ⊕W generativeflows
1 2
areanotherexampleofanMFG-basedgenerativemodel,wemayrigorouslystudytheirrobustnessto
implementationerrorsusingPDEtheory,followingthestyleandapproachof[24].
8 Acknowledgement
TheauthorsarepartiallyfundedbyAFOSRgrantFA9550-21-1-0354. H.G.,M.K.andL.R.-B.are
partiallyfundedbyNSFDMS-2307115. H.G.andM.K.arepartiallysupportedbyNSFTRIPODS
CISE-1934846.
References
[1] RickyTQChen,YuliaRubanova,JesseBettencourt,andDavidKDuvenaud. Neuralordinary
differentialequations. Advancesinneuralinformationprocessingsystems,31,2018.
[2] Will Grathwohl, Ricky T. Q. Chen, Jesse Bettencourt, and David Duvenaud. Scalable re-
versiblegenerativemodelswithfree-formcontinuousdynamics. InInternationalConference
onLearningRepresentations,2019.
[3] ChrisFinlay,Joern-HenrikJacobsen,LevonNurbekyan,andAdamOberman. Howtotrain
yourneuralODE:theworldofJacobianandkineticregularization. InHalDauméIIIandAarti
Singh,editors,Proceedingsofthe37thInternationalConferenceonMachineLearning,volume
119ofProceedingsofMachineLearningResearch,pages3154–3164.PMLR,13–18Jul2020.
[4] DerekOnken, SamyWuFung, XingjianLi, andLarsRuthotto. Ot-flow: Fastandaccurate
continuousnormalizingflowsviaoptimaltransport. InProceedingsoftheAAAIConferenceon
ArtificialIntelligence,volume35,pages9223–9232,2021.
[5] KatharinaOtt,PrateekKatiyar,PhilippHennig,andMichaelTiemann. ResNetafterall: Neural
ODEsandTheirNumericalSolution. InInternationalConferenceonLearningRepresentations,
2021.
[6] Liu Yang and George Em Karniadakis. Potential flow generator with L2 optimal transport
regularityforgenerativemodels. IEEETransactionsonNeuralNetworksandLearningSystems,
33:528–538,2019.
[7] JeremiahBirrell,PaulDupuis,MarkosAKatsoulakis,YannisPantazis,andLucRey-Bellet.
(f,Γ)-Divergences: Interpolating between f-Divergences and Integral Probability Metrics.
JournalofMachineLearningResearch,23(39):1–70,2022.
[8] HyeminGu,PanagiotaBirmpa,YannisPantazis,LucRey-Bellet,andMarkosA.Katsoulakis.
Lipschitz-RegularizedGradientFlowsandGenerativeParticleAlgorithmsforHigh-Dimensional
ScarceData. SIAMJ.DataScience,toappear,2024.
[9] BenjaminJZhangandMarkosAKatsoulakis. Amean-fieldgameslaboratoryforgenerative
modeling. arXivpreprintarXiv:2304.13534,2023.
[10] Wuchen Li, Siting Liu, and Stanley Osher. A kernel formula for regularized wasserstein
proximaloperators. ResearchintheMathematicalSciences,10(4):43,2023.
[11] BenjaminJZhang,SitingLiu,WuchenLi,MarkosAKatsoulakis,andStanleyJOsher. Wasser-
steinproximaloperatorsdescribescore-basedgenerativemodelsandresolvememorization.
arXivpreprintarXiv:2402.06162,2024.
[12] AdityaGrover,ManikDhar,andStefanoErmon. Flow-gan: Combiningmaximumlikelihood
andadversariallearningingenerativemodels. InProceedingsoftheThirty-SecondAAAICon-
ferenceonArtificialIntelligenceandThirtiethInnovativeApplicationsofArtificialIntelligence
ConferenceandEighthAAAISymposiumonEducationalAdvancesinArtificialIntelligence,
AAAI’18/IAAI’18/EAAI’18.AAAIPress,2018.
18[13] Jeremiah Birrell, Yannis Pantazis, Paul Dupuis, Luc Rey-Bellet, and Markos Katsoulakis.
Function-spaceregularizedRényidivergences. InTheEleventhInternationalConferenceon
LearningRepresentations,2023.
[14] JeremiahBirrell,MarkosA.Katsoulakis,LucRey-Bellet,andWeiZhu. Structure-preserving
GANs. InInternationalConferenceonMachineLearning,2022.
[15] NealParikhandStephenBoyd. Proximalalgorithms. Found.TrendsOptim.,1(3):127–239,Jan
2014.
[16] DávidTerjék. Moreau-yosidaf-divergences. InMarinaMeilaandTongZhang,editors,Pro-
ceedingsofthe38thInternationalConferenceonMachineLearning,volume139ofProceedings
ofMachineLearningResearch,pages10214–10224.PMLR,18–24Jul2021.
[17] WilfridGangboandMichaelWestdickenberg. Optimaltransportforthesystemofisentropic
eulerequations. CommunicationsinPartialDifferentialEquations,34(9):1041–1073,2009.
[18] LawrenceCEvans. Partialdifferentialequations,volume19. AmericanMathematicalSociety,
2022.
[19] Hamed Rahimian and Sanjay Mehrotra. Frameworks and results in distributionally robust
optimization. OpenJournalofMathematicalOptimization,3:1–85,2022.
[20] Jean-MichelLasryandPierre-LouisLions. Meanfieldgames. Japanesejournalofmathematics,
2(1):229–260,2007.
[21] IshaanGulrajani,FarukAhmed,MartinArjovsky,VincentDumoulin,andAaronCCourville.
ImprovedtrainingofWassersteinGANs. Advancesinneuralinformationprocessingsystems,
30,2017.
[22] Belinda Tzen and Maxim Raginsky. Neural stochastic differential equations: Deep latent
gaussianmodelsinthediffusionlimit. arXivpreprintarXiv:1905.09883,2019.
[23] LiamHodgkinson,ChrisvanderHeide,FredRoosta,andMichaelWMahoney. Stochastic
normalizingflows. arXivpreprintarXiv:2002.09547,2020.
[24] NikiforosMimikos-Stamatopoulos,BenjaminJZhang,andMarkosAKatsoulakis. Score-based
generativemodelsareprovablyrobust: anuncertaintyquantificationperspective. arXivpreprint
arXiv:2405.15754,2024.
19A Experimentaldetails
A.1 Computeresources
Ourexperimentiscomputedusingpersonalcomputerintheenvironment: Apple M2 8 coresand
Apple M2 24 GB - Metal 3.
A.2 Dataset
MNISTisastandardmachinelearningexamplewherethedatasetcanberetrievedfromsomemachine
libraries. Theoriginaldatasetconsistsoftotal60000samplesof28×28gray-scaleimageswhose
pixelvaluesrangefromintegersbetween0and255. Wenormalizethepixelvaluesandtherefore
locatesamplesin[0,1]28×28. Eachsamplerepresentsoneofhandwrittendigitsfrom0to9. Weuse
randomlychosen6000samples(10%oftheentiredataset)astrainingdata.
A.3 Neuralnetworkarchitecture
CNNDiscriminator CNNPotential
7×7Conv,1×1stride(1→8) 7×7Conv,1×1stride(1→8)
ReLU ReLU
2×2maxpool,2×2stride 2×2maxpool,2×2stride
7×7Conv,1×1stride(8→8) 7×7Conv,1×1stride(8→8)
ReLU ReLU
2×2maxpool,2×2stride 2×2maxpool,2×2stride
Flattenwithdimensionℓ Spatialflattenwithdimensionℓ
3 3
W4 ∈Rℓ3×256,b4 ∈R256 W4 ∈R(ℓ3+1)×512,b4 ∈R512
ReLU ReLU
W5 ∈R256×256,b5 ∈R256 W5 ∈R512×512,b5 ∈R512
ReLU ReLU
W6 ∈R256×256,b6 ∈R256 W6 ∈R512×512,b6 ∈R512
ReLU ReLU
W7 ∈R256×1,b7 ∈R W7 ∈R512×1,b7 ∈R
Linear Linear
Table1: Neuralnetworkarchitecturesofdiscriminatorϕ:Rd →Randpotentialϕ:Rd×R→R.
WeadaptedtheneuralnetworkarchitecturefromthecodeforthepotentialflowgeneratorGAN[6]
toaccommodatehigh-dimensionalimageexamples.
20