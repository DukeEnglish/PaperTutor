Rethinking Transformer-based Multi-document Summarization: An
Empirical Investigation
CongboMa1,WeiEmmaZhang2,DileepaPitawela2,HaojieZhuang2,YanfengShu3
1MacquarieUniversity,Sydney,Australia,2TheUniversityofAdelaide,Adelaide,Australia,
3CSIRO,Australia. congbo.ma@mq.edu.au,yanfeng.shu@data61.csiro.au
{wei.e.zhang, dileepa.pitawela, haojie.zhuang}@adelaide.edu.au,
Abstract model design pipeline: (1) Document input per-
spective: weconductexperimentstoquantitatively
The utilization of Transformer-based models
assesstheimpactofdocumentboundaryseparators
prospers the growth of multi-document sum-
from a standpoint of document input; (2) Trans-
marization (MDS). Given the huge impact
andwidespreadadoptionofTransformer-based former structure perspective: we explore the ef-
modelsinvariousnaturallanguageprocessing fectiveness of different mainstream Transformer
tasks,investigatingtheirperformanceandbe- structures;(3)Thesignificanceofencoderandde-
haviorsinthecontextofMDSbecomescrucial coderperspective: wedesignempiricalstudiesby
foradvancingthefieldandenhancingthequal-
addingnoisesontopoftheencoderanddecoder;
ity of summary. To thoroughly examine the
(4) Training strategy perspective: we restructure
behaviours of Transformer-based MDS mod-
thesourcedocumentsandincludeself-supervised
els,thispaperpresentsfiveempiricalstudieson
(1)measuringtheimpactofdocumentbound- learning;(5)Summarygenerationperspective,we
aryseparatorsquantitatively;(2)exploringthe exploretheuncertaintieswhenrepetitionproblems
effectiveness of different mainstream Trans- occurinthesummarygenerationprocess.
formerstructures; (3)examiningthesensitiv- TheprimarydistinctionbetweenSDSandMDS
ityoftheencoderanddecoder;(4)discussing
lies in the variance of source document numbers.
differenttrainingstrategies; and(5)discover-
OnestraightforwardwaythatconvertMDStoSDS
ing the repetition in a summary generation.
is concatenating text spans and processing them
The experimental results on prevalent MDS
as a flat sequence (Liu et al., 2018; Chu and Liu,
datasets and eleven evaluation metrics show
theinfluenceofdocumentboundaryseparators, 2019; Brazinskas et al., 2020; Mao et al., 2020;
thegranularityofdifferentlevelfeaturesand Zhao et al., 2022). One way to aid the models
differentmodeltrainingstrategies. Theresults indetectingandmodelingdocument-to-document
alsorevealthatthedecoderexhibitsgreatersen- relationshipsinoneflatsequenceistoutilizedoc-
sitivitytonoisescomparedtotheencoder. This
ument boundary separators (Fabbri et al., 2019;
underscores the important role played by the
Xiao et al., 2022). However, there is a notable
decoder, suggesting a potential direction for
gap in the current literature regarding a qualita-
futureresearchinMDS.Furthermore,theex-
perimental results indicate that the repetition tiveandquantitativeexaminationoftheinfluence
probleminthegeneratedsummarieshascorre- of document boundary separators. This absence
lationswiththehighuncertaintyscores. of exploration serves as the driving force behind
our initiative to investigate whether these separa-
1 Introduction
torscontributetoenhancedmodelperformanceand
The innovation and contemporary developments foster awareness of document boundaries within
ofTransformerarchitecture(Vaswanietal.,2017) the feature space of MDS models. Through ex-
thrivesmulti-documentsummarization(MDS)(Ma perimentsconductedonthreedistinctTransformer
et al., 2022a). This motivates us to study the be- structures, we discerned that the impact of docu-
haviors of the Transformer structure MDS mod- ment boundary separators varies among models
els. Throughtheseanalyses,weaimtoprovidea withdifferinghierarchies. Uncertaintyanalysisis
thoroughunderstandingofMDSanditsintricacies a pivotal approach employed in the examination
withintheMDSmodelframework. Weundertake and assessment of generation systems (Xu et al.,
a comprehensive investigation from five distinct 2020) which can serve as an important indicator
perspectivescoveringtheTransformer-basedMDS toshowhowthemodelperformsduringthesum-
4202
luJ
61
]LC.sc[
1v84911.7042:viXramarygeneration. Wetheninvestigatethevariation greatersensitivitycomparedtotheencoderinMDS
of summary prediction uncertainty by exploring scenarios. This finding underscores the need for
therelationsbetweenseparatorsandthepredictive increasedattentiontodecoderenhancementsinfu-
uncertaintyofthestructures. Certainly,measuring tureresearchwithintheMDScommunity.
uncertainty in the context of summarization can Based on the analysis of Transformer-based
provide insights into how the presence of docu- MDS models, we also pay attention to exploring
mentboundaryseparatorsaffectsthebehaviorof differenttrainingstrategiesforfurtherenhancing
Transformer-based models and their summariza- theperformanceofMDSmodels. Differenttrain-
tionoutcomes. Byquantifyinguncertaintythrough ing strategies offer unique approaches to utilize
the entropy calculations, we gain a deeper under- available data and optimize model performance.
standingofthelevelofconfidenceorambiguitythe Byinvestigatingdiversetrainingstrategies,weaim
modelhasinitsgeneratedsummaries. to identify the most effective methods for train-
Insteadofsimplyconcatenatingalltheinputdoc- ing MDS models, leveraging the characteristics
umentsintoaflatsequenceandapplyingSDSmod- ofthedatasetandthesummarizationtaskathand.
els,thehierarchicalTransformerstructure(Liuand Thesestrategiesinvolveusingpseudodatasets,fine-
Lapata,2019;Pasunuruetal.,2021;Lietal.,2020) tuningonoriginaldatasets,oracombiningofboth.
hasbeenproposedtospecificallysolveMDStasks. Togeneratepseudodata,wetreatindividualdocu-
This structure has been used for encoding multi- mentsinadocumentsetaspseudo-summariesand
pledocumentsinahierarchicalmanner,enabling createmultiplesetsofpseudo-document-summary
the capture of cross-document relations through pairs. Weevaluatethreetrainingapproaches: train-
the utilization of an attention mechanism. The ingexclusivelyonthepseudodataset,mixingthe
hierarchicalTransformerstructurecontainsalow- pseudodatasetwiththeoriginaldataset,andatwo-
levelTransformerthatencodestokensandahigh- stepprocessoftrainingonthepseudodatasetfol-
level Transformer that is used to encode coarser- lowedbyfine-tuningontheoriginaldataset. The
grainedtextualunits. Thismotivatesustofurther experimentalresultsdemonstratethatthepretrain-
explore the influence of different hierarchies on finetune strategy consistently outperformed the
MDSperformances. Weexploretheeffectofdif- othertrainingstrategies,leadingtoimprovedsum-
ferentgranularityofhigh-levelTransformeronthe marization quality. The analysis of feature distri-
performanceofMDSmodels. Inthispaper,wecon- butionsfurthersupportedthisfinding,highlighting
sider sentence-level and document-level features thealignmentbetweenthefinetunedmodelandthe
asdifferentgranularities. Basedontheempirical baselinemodel. Theseresultsprovidevaluablein-
studies, our findings indicate that for MDS tasks sightsintotheeffectivenessofthepretrain-finetune
involving relatively short documents, flat Trans- approachinenhancingsummarizationperformance.
former models are a suitable choice. Also, the Thefindingsofthisstudycanguidefutureresearch
hierarchicalstructureprefershighergranularityin anddevelopmentinthefieldofabstractivesumma-
high-levelTransformerstructures. rization, emphasizing the importance of training
Inadditiontoexploringthehierarchicalstructure strategiesforachievinghigher-qualitysummaries.
ofTransformer-basedMDSmodels,weexplorethe Moreover,whilethedifferentTransformerstruc-
Transformer’sinternalstructure. Basedontheex- tures and training strategies demonstrated varia-
istingTransformer-basedMDSmethods,wefind tionsinperformances, anobservationisthepres-
that many of the MDS models focus on modify- ence of repetitive patterns in the generated sum-
ing the components of encoder (Liu and Lapata, maries, indicating a potential issue that needs to
2019; Pasunuru et al., 2021; Liu et al., 2021; Ma beaddressedinabstractivesummarizationsystems.
et al., 2022b) and fewer works pay attention to Liuetal. (Liuetal.,2023)gavetwopossiblerea-
amelioratingthedecoder(JinandWan,2020;Liu sons behind the repetition problem in abstractive
et al., 2022) to cater the requirements for MDS summarization: (1)attendingtothesamelocation
tasks. Thismotivatesustoexploretherobustness inthesourceand(2)attendingtosimilarbutdiffer-
of the encoder and decoder towards interference ent sentences in the source. In this paper, we ex-
under the same noise conditions. Therefore, we plorethecauseofrepetitiveproblemsinabstractive
addGaussiannoisesattheparameterspaceofthe summarizationbyexaminingpredictiveuncertainty.
encoderordecodertofulfillthispurpose. Theex- Wequantifyuncertaintyscoresateachtimeslotdur-
perimentalresultsindicatethatthedecoderexhibits ingthesummarygenerationprocess. Theanalysisaimstoobservehowtheuncertaintyscorechanges et al., 2020), we sort the prediction distribution
whenrepetitionphenomenaoccur,allowingusto X in descending order and get a minimal set of
i
identify positions where uncertainty is localized tokenswherethesumpredictionvaluesarelarger
inrepetitivebehavior. Theanalysisrevealsthatas than0.95,andthennormalizethedistribution. We
themodelgeneratesrepetitivesentencesorwords, calculate the entropy value based on the new dis-
theuncertaintyscorerises,pointingoutdecreased tribution P′(x ). The utilization of entropy as
ij
confidenceandincreaseduncertaintyregardingthe a measure allows us to gauge the distribution of
appropriatenessandrelevanceofrepeatedelements probabilitiesacrossdifferenttokenswithinthepre-
inthesummary. Understandingthisrelationshipal- dictivepositionsofthesummaries. Higherentropy
lowsustodevelopstrategiestomitigaterepetition valuesindicateawiderspreadofprobabilities,sug-
andimprovethequalityofgeneratedsummaries. gestingthatthemodelislesscertainaboutthemost
appropriate token to choose. Conversely, lower
2 Methodology entropyvaluessuggestthatthemodelismorecon-
fidentinitstokenpredictions. Thequantification
WeintroducehowtodesigntheMDSexperiments
ofuncertaintythroughentropymeasurementsand
fromthefollowingangles: inputdata,Transformer
its qualitative analysis enables us to assess how
structures,trainingstrategiesandsummarygener-
theintroductionofdocumentboundaryseparators
ation. Therefore, we design five experiments to
influencestheperformanceofthesummariesgen-
evaluatethebehaviorsofTransformer-basedMDS
eratedbyTransformer-basedmodels. Thisholistic
models: (1) the measurable impact of document
approachhelpsusunravelthenuancedimpactof
boundary separators; (2) the effectiveness of dif-
documentboundaryseparatorsontheMDSprocess
ferent Transformer structures; (3) the sensitivity
andgainvaluableinsightsintothebehaviorofthese
oftheencoderanddecoder;(4)differenttraining
modelsinhandlingmultipledocumentinputs.
strategies;(5)repetitionindocumentgeneration.
2.2 TheEffectivenessofDifferent
2.1 TheMeasurableImpactofDocument
TransformerStructures
Separators
Transformer structures have become an essential
We modify the source documents instead of the
component of many state-of-the-art natural lan-
summarization models to the format of: D =
guage processing models. However, the design
{d1,s,d2,s,...,s,dN}, where N is the number
oftheTransformerarchitecturecanvarydramati-
ofdocumentsinadocumentsetD,thesuperscript
cally,anddifferentstructuresmayimpacttheper-
dn representsthen-thdocumentintheset, ands
formance of the model on different tasks. In this
denotes the special tokens. We investigate differ-
study,weaimtoevaluatetheeffectivenessofdiffer-
entTransformermodelsontwoMDSdatasetsand
entTransformerstructuresforMDStasks. Specif-
elevenevaluationmetricstoexploretheimpactof
ically, we focus on two types of structures: flat
thedocumentboundaryseparatorsqualitativelyand
TransformerandhierarchicalTransformer.
quantitatively. Weanalyzeandcomparethepredic-
The flat Transformer consists of a single layer
tionuncertaintyfromdifferentdatasetsanddiffer-
ofself-attentionandfeed-forwardneuralnetwork
entformatsofsourcedocumentsbyinspectingen-
layersthatprocesstheinputtokenssequentially. In
tropyvaluesduringsummarygeneration. Weaim
contrast,thehierarchicalTransformerhasamore
tounderstandhowdecisionsbyaddingdocument
complexstructure,wheretheinputtokensarefirst
boundary separators are reflected in the model’s
groupedintosentencesordocuments,andthenpro-
uncertainty. Inthegenerationprocess,eachpredic-
cessedbylocalandglobalTransformerlayers. To
tivepositionX hasanoutcomeprobabilisticdis-
i explorethehierarchicalTransformerstructure,we
tributionx ,...,x ,misthenumberofacorpus
i1 im investigatetwodifferentgranularitiesofhigh-level
pool. We use entropy as an uncertainty measure-
Transformer: sentence-level and document-level.
mentwhichcanbecalculatedasfollows:
BuildingontheworkofLiu(LiuandLapata,2019),
we make modifications to the local Transformer
m
(cid:88)
H(X i)=− P(x ij)logP(x ij) (1) layerstoencodeindividualdocuments. Theglobal
j=1 Transformerlayersarethenabletoexchangeinfor-
Becausethesizeofthecorpuspoolislargeandthe mationatthesentenceordocumentlevel.
prediction distribution is usually long-tailed (Xu Ouranalysisismotivatedbytheneedtobetterunderstand how different Transformer structures original MDS dataset is denoted as the original
canimpacttheperformanceofMDSmodels. By datasetinthesubsequentanalysis.
comparingtheperformanceoftheflatTransformer To evaluate the effectiveness of different train-
and hierarchical Transformer structures, we aim ingstrategies,wedesignthreedistinctapproaches.
to identify which structure is more effective for Firstly, we train the MDS model exclusively on
multipledocumentsummarizationdata. thepseudodataset. Secondly,wemixthepseudo
datasetwiththeoriginaldataset,creatingacompre-
2.3 TheSensitivityofEncoderandDecoder
hensivemegadataset,onwhichtheMDSmodelis
Insummarizationtasks,theencoderplaysacrucial trained. Lastly,weemployatwo-stepprocess,ini-
role in extracting representations from the input tiallytrainingthemodelonthepseudodatasetand
text,whilethedecoderisresponsibleforgenerat- subsequentlyfine-tuningitontheoriginaldataset.
ingtheoutputsummary,whichrequiresproducing
coherentandmeaningfullanguage. Giventheintri- 2.5 RepetitioninDocumentGeneration
catenatureofsummarygeneration,thedecoder’s
ForabstractiveMDS,apersistentchallengearises
role demands fine-grained control and precision,
from the inclination of models to produce repeti-
making it potentially more sensitive than the en-
tivesentencesorwordsduringthesummarization
coder. To explore the sensitivity of the encoder-
process. Thistendencycreatesaloopthatisdiffi-
decoderinTransformer-basedsummarizationmod-
culttobreak,hamperingthegenerationofaccurate
els,weaddGaussiannoiseattheparameterspace
summaries. Toanalyzewhatmaycauserepetitive
ofthe encoderordecoder. Wedevise thisexperi-
problems,wedelveintoananalysisofprediction
mentbasedontheintuitionthatamodule(whether
uncertainty,examininguncertaintyscoresthrough-
it’s the encoder or decoder) exhibits varying sen-
out the generation process and localizing uncer-
sitivity to noise, thereby signifying the differing
taintytocertainpositionsinarepetitionbehavior.
degreesofimportanceeachmoduleholdsforover-
To quantify uncertainty, we employ Equation
allperformance. Formally,wehave:
1,whichcalculatestheuncertaintyscoreforeach
z=f(x;Θ+αn),n∼N(µ,δ) (2) timeslotduringthesummarizationgeneration. By
applying this equation, we obtain a measure of
where f(·) is the component in Transformer; Θ
uncertaintythatcorrespondstothelevelofdoubt
is the parameters in f(·); n represents Gaussian
orambiguityassociatedwiththegeneratedoutput.
noise;µ,δ aremeanandvarianceintheGaussian
Theanalysisfocusesonobservinghowtheuncer-
noise,αistheweightedfactor.
taintyscoreevolvesinresponsetotheoccurrence
2.4 DifferentTrainingStrategies ofrepetitionphenomena.
In this study, we aim to investigate the impact of
3 EmpiricalStudiesandAnalyses
differenttrainingstrategiesonTransformermodels
forabstractivesummarization. Whilewehavepre-
3.1 SettingsforEmpiricalStudies
viouslyexaminedthecomponentsofTransformer
models,thespecificinfluenceoftrainingstrategies WeevaluatetheperformanceofthreeTransformer
remainsunexplored. Ourobjectiveistoidentifythe models: VanillaTransformer(VT)(Vaswanietal.,
mosteffectivetrainingstrategiesbyleveragingthe 2017),VanillaTransformerwithcopymechanism
inherentcharacteristicsofMDSdatasets,without (VTC), and modified Hierarchical Transformer
theneedforexternaldatasources. Tocreatepseudo (HT) (Liu and Lapata, 2019). These models are
datautilizingthecharacteristicMDS,weadopta assessedontwowidelyusedMDSdatasets: Multi-
straightforwardapproach. Wetreatonedocument XScience(Luetal.,2020)andMulti-News(Fabbri
fromagivendocumentsetasapseudo-summary et al., 2019). To comprehensively analyze their
whileconsideringtheremainingdocumentsasin- performance,weemployelevenevaluationmetrics:
putdocuments. Thisprocessisiterated,systemati- ROUGE (Lin, 2004) including ROUGE-1 (R-1),
callyselectingeachdocumentinthesetasapseudo- ROUGE-2(R-2), ROUGE-L (R-L), ROUGE-SU
summary, until all input documents have served (R-SU), ROUGE-WE (R-WE) (Ng and Abrecht,
aspseudo-summaries. Consequently,wegenerate 2015),BLEU(Papinenietal.,2002),S3 (Peyrard
multiplesetsofpseudo-document-summarypairs, et al., 2017) including pyramid (pyr) and respon-
which we refer to as pseudo-MDS dataset. The siveness(resp)scores,BertScore(BS)(Zhangetal.,2020), Relevance (Rel) (Peyrard, 2019), Redun-
dancy(Red)(Peyrard,2019).
3.2 ImpactofDocumentSeparators
We investigate the VT, VTC, and HT models on
bothdatasetsandreporttheelevenevaluationmet-
ricstoexploretheimpactofthedocumentbound-
ary separators. From Table 1, interestingly, we
find that adding separators reduces models’ per-
formance in half of the cases (3 out of 6). For Figure2: Performancevariationwithdocument-level
example,modelVTwithseparatorsperformsrela- (greenline)andsentence-level(orangeline)HTmod-
els on Multi-XScience (left) and Multi-News (right)
tivelyworseonMulti-News(theresultsof8evalu-
datasets. BLEU,RedundancyandRelevancearescaled
ationmetricsareworseamong11evaluationmet-
(0to0.6)tomakeallpointintheplotboundary.
rics); model VTC performs relatively worse on
both Multi-XScience (the results of 9 evaluation
metrics are worse among 11 evaluation metrics) boundaryseparatorsandtokenuncertaintyscores.
and Multi-News (the results of 8 evaluation met- Figure1showstheuncertaintyscoresofgenerated
ricsareworseamong11evaluationmetrics)when tokens of VTC models on both datasets. Surpris-
withseparators. Theseresultsindicateinputdocu- ingly, the figure reflects that separators are asso-
mentswithseparatorsarenotveryhelpfulforflat ciated with high uncertainty score actions which
Transformer models. However, we can perceive meanstheseparatorsincreasethepredictiveuncer-
thattheHTmodelachievesbetterperformanceon taintyofmodels. Possiblebecausetheseparators
bothdatasetswithdocumentboundaryseparators. have no semantic relations with the source docu-
Another interesting finding is the most com- mentsandseparatorsmayberegardedasnoiseto
monly used ROUGE, in a few cases, shows the increasethepredictiveuncertainty. Themedianun-
oppositeresultfromotherevaluationmetrics. For certaintyscoreoftheMulti-Newsislargerthanthe
instance, on the Multi-XScience dataset, the VT Multi-XSciencealigningwiththesizeofdatasets.
(withdocumentboundaryseparators)showsbetter
3.3 QuantitativePerformanceonDifferent
ROUGEresultsthanVT(withoutdocumentbound-
TransformerStructures
ary separators) but contradicts the results on “R-
WE",“BLEU”,“S3”,”BertScore",“Redundancy” We investigate (1) the effectiveness of differ-
and “Relevance". It indicates that the ROUGE- ent Transformer architectures: flat Transformer
centricevaluationsystemneedstobeupdatedand (VT,VTC)andhierarchicalTransformer(HT);(2)
the measurement of summarization can not rely theinfluencesofdifferentgranularitieswithinhier-
solelyonROUGE. archicalTransformerstructure. Theresultsarealso
foundinTable1. Inmostevaluationmetrics, the
e10000 w separator HTmodelcannotachieveasgoodresultsastwo
cn
e 8000 w/o separator flatTransformermodelsonbothdatasets. Thetwo
ic
itluS MX - 6 40 00 00
0
p mo ote dn et li ia sl lr oe na gso erns tha ar ne: th( e1) flath te Trp ai np se fl oin re mo ef rmth oe dH elT
s
2000
which makes the HT model hard to train. (2) the
0
Multi-XScience and Multi-New datasets are not
12000
sw10000 longdocumentsummarizationdatasets. Theaver-
itlue N - 68 00 00 00 agedocumentlengthofMulti-XScienceandMulti-
M Neware778.08and2103.49. Fromtheexperimen-
4000
2000 talresults,wecanconcludethattheHTmodelis
0 0 1 2 3 4 5 6 moresuitableforlengthydocuments,implyingthat
Uncertainty Score
flatTransformermodelsareagoodchoicefortasks
Figure1:TheuncertaintyscoresofVTConMulti-News
withshorterdocuments.
andMulti-XScience.Thex-axisandy-axisarethevalue
ofuncertaintyscoresandthenumberoftokens. As mentioned in Section 2.2, to evaluate the
influencesofdifferentgranularitieswithinthehi-
Wealsodiscovertherelationsbetweendocument erarchical Transformer structure, we modify theDatasets Models R-1↑ R-2↑ R-L↑ R-SU↑ R-WE↑ BLEU↑ S3(pyr/resp)↑ BS↑ Red↓ Rel↑
VT 0.2714 0.0490 0.1030 0.0784 0.1523 2.9773 0.2103/0.3609 0.5330 -4.0712 -5.8352
VTw/oS 0.2670 0.0480 0.1553 0.0767 0.1580 3.3623 0.2202/0.3663 0.5405 -6.1908 -4.8609
Multi VTC 0.2635 0.0483 0.1499 0.0734 0.1659 4.6037 0.2561/0.3885 0.5590 -7.0585 -4.5802
-XScience VTCw/oS 0.2713 0.0468 0.1502 0.0780 0.1702 4.7615 0.2554/0.3861 0.5621 -7.8402 -4.2908
HT 0.2571 0.0483 0.1615 0.0692 0.1407 7.1501 0.1769/0.3473 0.5303 -4.6987 -8.0379
HTw/oS 0.2216 0.0376 0.1446 0.0521 0.1100 5.2862 0.1428/0.3295 0.5108 -4.0142 -11.6068
VT 0.2445 0.0523 0.1301 0.0603 0.1480 2.0054 0.1380/0.3212 0.4622 -5.7674 -7.4220
VTw/oS 0.2555 0.0550 0.1347 0.0651 0.1491 2.0193 0.1384/0.3214 0.4605 -5.2098 -8.0488
Multi VTC 0.4233 0.1471 0.2059 0.1625 0.2860 11.3861 0.3778/0.4871 0.5955 -6.0966 3.9027
-News VTCw/oS 0.4363 0.1555 0.2053 0.1698 0.2885 13.015 0.3967/0.5017 0.5916 -6.2869 3.8355
HT 0.2349 0.0371 0.1352 0.0598 0.1154 3.5434 0.1097/0.3074 0.4987 -5.0249 -17.1520
HTw/oS 0.2304 0.0384 0.1430 0.0580 0.1193 3.0499 0.1023/0.3031 0.4966 -4.9433 -16.8205
Table 1: Evaluation results on Multi-XScience and Multi-News datasets, both with and without the document
boundaryseparators. “S"indicatesdocumentseparators.
localTransformerlayerstoencodeindividualdoc- andevenslightperturbationsintheencodedinput
uments. Figure 2 shows the performances of canimpacttheattentionweightsandsubsequently
document-levelandsentence-levelHTmodels. All influence the decoding process. Consequently, it
themetricsareshowingbetterperformanceswith underscoresthecrucialroleplayedbythedecoder
thedocument-levelHTcomparedtothesentence- insummarizationtasks. Thesefindingsshedlight
levelHTasthegreenlineexceedstheboundaryof onthehighimportanceofthedecoder’scontribu-
theorangelineineverydimension(redundancyis tiontotheoverallsummarizationprocess.
the lower the better). The apparent trend implies
3.5 QuantitativePerformanceofDifferent
thatahigherlevelofgranularityismorefavorable
TrainingStrategies
forthehierarchicalTransformerstructure.
3.4 QuantitativePerformanceonthe
x VTC x VTC
SensitivityofEncoderandDecoder o VTC (self-supervised) o VTC (self-supervised)
^ VTC (finetune) ^ VTC (finetune)
Toinvestigatethehypothesisinsection2.3,wese-
lecttheVTCmodelasthefoundationforevaluating
theeffectivenessoftheencoder-decoderstructure
on the Multi-XScience and Multi-News datasets.
ByexaminingTable2,weobservelargedifferences Multi-News Multi-XScience
inperformancewhenintroducingnoisetotheen-
Figure3: ThefeaturevisualizationofVTC,VTCwith
coderanddecoderinhighlynoisyscenarios(with self-supervisedtrainingandVTCwithfinetuningafter
α = 1e-1 and α = 1e-2). Specifically, in noisy self-supervisedtrainingwithPCA.
conditions,wefindthataddingnoisetothedecoder
hasamoresubstantialimpactonperformancecom- The experimental results presented in Table 3
paredtoaddingnoisetotheencoder. However,as provideanoverviewoftheperformanceoftheVTC
the noise levels decreased, the performance gaps modeltrainedusingdifferentpretrainingstrategies
between the two approaches narrowed. This ob- on the Multi-XScience and Multi-News datasets.
servation supports our initial hypothesis that the Inthetable,theVTCistrainedontheoriginaldoc-
decoderismoresensitivethantheencoder. Thepo- ument set and golden summary pairs. The “fine-
tentialreasonsare: (1)errorsorinaccuraciesinthe tune" strategy refers to the training of the model
decodercanhaveacascadingeffectonsubsequent onthepseudodataset(introducedinSection2.4)
tokensgeneratedduringdecoding. Thiserrorprop- first and then fine-tuning on the original dataset.
agationphenomenoncanmakethedecodermore The“self-supervised"strategydenotestrainingthe
sensitivetosmallperturbations,asanymistakesor VTCmodelexclusivelyonthepseudodataset. The
noiseintroducedduringdecodingcanamplifyand “mix"strategyillustratestrainingthemodelusinga
affecttheoverallqualityofthegeneratedsummary; combinationofthepseudodatasetandtheoriginal
(2)Transformer-basedmodelsoftenemployanat- dataset. By comparing the results obtained from
tentionmechanismthatallowsthedecodertofocus thesedifferenttrainingstrategies,weaimtoiden-
ondifferentpartsoftheencodedinputduringthe tifythemosteffectiveapproachforeachdataset.
decodingprocess. Thedecoder’ssensitivityiscru- For the Multi-XScience, the results show that
cialineffectivelyattendingtorelevantinformation, theVTC(pretrain-finetune)strategyoutperformsDatasets Models R-1↑ R-2↑ R-L↑ R-SU↑ R-WE↑ BLEU↑ S3(pyr/resp)↑ BS↑ Red↓ Rel↑
En(α=1e-3) 0.2656 0.0477 0.1507 0.0739 0.1660 4.6288 0.2560/0.3881 0.5593 -5.2615 2.5252
De(α=1e-3) 0.2637 0.0483 0.1499 0.0735 0.1676 4.8116 0.2573/0.3890 0.5608 -5.2806 2.5377
Multi En(α=1e-2) 0.2433 0.0412 0.1386 0.0650 0.1523 4.0228 0.2276/0.3713 0.5506 -5.2222 2.4878
-XScience De(α=1e-2) 0.2130 0.0362 0.1277 0.0512 0.1333 2.6732 0.1933/0.3535 0.5189 -4.5961 2.4406
En(α=1e-1) 0.0305 0.0019 0.0232 0.0035 0.0057 0.0979 -0.0786/0.2085 0.3631 -1.8267 0.1420
De(α=1e-1) 0.0282 0.0039 0.0259 0.0019 0.0012 0.0422 -0.0350/0.2347 0.3533 -0.9935 1.2109
En(α=1e-3) 0.4178 0.1439 0.2063 0.1598 0.2817 10.5326 0.3345/0.4623 0.5943 -5.8867 3.8567
De(α=1e-3) 0.4172 0.1427 0.2053 0.1589 0.2802 10.6737 0.3348/0.4625 0.5941 -5.8923 3.8533
Multi En(α=1e-2) 0.2899 0.0689 0.1405 0.0888 0.2095 5.5596 0.2260/0.3778 0.5335 -5.2695 3.7854
-News De(α=1e-2) 0.2248 0.0602 0.1134 0.0706 0.1842 4.0850 0.2288/0.3793 0.4972 -4.7247 3.3888
En(α=1e-1) 0.0938 0.0049 0.0724 0.0101 0.0266 0.0549 -0.0499/0.2223 0.3330 -1.2151 1.7586
De(α=1e-1) 0.0458 0.0018 0.0330 0.0041 0.0186 0.0476 -0.0537/0.2207 0.3410 -2.3011 1.3539
Table2: EvaluationresultsonMulti-XScienceandMulti-Newsdatasetsabouttheencoder-decoderstructure.
theVTCtrainedontheoriginaldatasetacrossmost distributionsincebothmodelspossessbetterrepre-
metrics,indicatingtheeffectivenessofthepretrain- sentationsforthefinalprediction. Conversely,for
finetunestrategyinimprovingsummarizationqual- theMulti-News,thefinetunedmodelexhibitsonly
ity. Incontrast,theVTC(self-supervised)exhibits marginalimprovementsovertheVTC.Thisobser-
lowerperformancecomparedtotheVTC(pretrain- vationalsoexplainstheoverlapbetweenfeatures
finetune),suggestingthatjustself-supervisedtrain- fromthefinetunedmodelandtheself-supervised
ingislesseffectiveforthisdataset. model, as finetuning adjusts the feature distribu-
Similarly,fortheMulti-Newsdataset,theresults tiontowardsthe‘genuine’distribution,albeittoa
implytheVTCmodelachievesgoodperformance limitedextent.
acrossallmetrics,withhigherscoresontheVTC
Summary #1
(pretrain-finetune)strategy,showcasingimproved
summarizationquality. Conversely,theVTC(self-
supervised)andVTC(mix)strategyyieldslower
performancecomparedtotheotherstrategies.
Thecomparisonofthesedifferenttrainingstrate-
Tokens
giesrevealsthatthepretrain-finetuneapproachcon- Summary #2
sistentlyleadstobettersummarizationperformance
compared to the baseline VTC model and other
Start repetition
training strategy, highlighting its effectiveness in
improvingsummarizationquality.
To find the potential reason why the finetune Tokens
strategy works well, we visualize the feature dis- Figure4: Therelationshipbetweenuncertaintyscores
tributionsofthreetrainingstrategies: VTC,VTC andtokenrepetitionsondifferentsummaries.
(self-supervised) VTC (finetune) using Principal
ComponentAnalysis(PCA)asillustratedinFigure
3.6 TheRelationBetweenRepetitionand
3. FortheMulti-News,thefeaturescomefromthe
Uncertainty
encoderoftheVTC(self-supervised)andtheVTC
(finetuning)exhibitsoverlapping,whilemaintain- Weexaminethecorrelationbetweenrepetitionand
ing distance from the plain VTC. In contrast, for uncertaintyintheprocessofgeneratingsummaries.
the Multi-XScience, the VTC (finetune) is more Toassessuncertainty,wecomputeascoreforeach
similartotheplainVTCbutstillnoticeablydistinct token generated. Two summaries are presented:
fromtheVTC(self-supervised). Thisobservation one featuring repetition and the other as a stan-
isconsistentwiththeperformanceresultspresented dard summary without repetition. The outcomes
inTable3. InthecaseoftheMulti-XScience,fine- are depicted in Figure 4. The X-axis represents
tuningthemodelafterself-supervisedtrainingsig- token indexes, while the Y-axis illustrates uncer-
nificantlyimprovesthemodel’sperformancecom- taintyscoresforeachtoken. Insummary#1,where
paredtotheVTC.However,whenthemodelisonly norepetitionsoccur,theuncertaintiesoftokensre-
pretrained using self-supervised learning, it per- mainwithina“normal"range. Thissuggeststhat
formsworsethantheVTC.Thisdiscrepancycan the model successfully avoids repetitive patterns,
beattributedtothefactthatthefeaturesofthefine- resulting in lower uncertainty scores throughout
tuned model closely align with the VTC model’s the summary generation process. Conversely, inDatasets Models R-1↑ R-2↑ R-L↑ R-SU↑ R-WE↑ BLEU↑ S3(pyr/resp)↑ BS↑ Red↓ Rel↑
VTC 0.2635 0.0483 0.1499 0.0734 0.1659 4.6037 0.2561/0.3885 0.5590 -7.0585 -4.5802
Multi- VTC(finetune) 0.2955 0.0558 0.1671 0.0879 0.1770 3.9727 0.2569/0.3886 0.5511 -5.0020 2.5824
XScience VTC(self-supervised) 0.2585 0.0368 0.1471 0.0678 0.1325 1.2885 0.1694/0.3343 0.5173 -5.3546 2.2064
VTC(mix) 0.2547 0.0350 0.1468 0.0653 0.1324 1.2922 0.1526/0.3246 0.5176 -5.3285 2.1945
VTC 0.4233 0.1471 0.2059 0.1625 0.2860 11.3861 0.3778/0.4871 0.5955 -6.0966 3.9027
Multi- VTC(finetune) 0.4271 0.1509 0.2084 0.1643 0.2886 11.5514 0.3893/0.4960 0.6004 -6.2075 3.9135
News VTC(self-supervised) 0.2724 0.0484 0.1349 0.0738 0.1399 2.8583 0.1281/0.3159 0.4737 -5.5046 2.4027
VTC(mix) 0.3046 0.0673 0.1485 0.0938 0.1728 5.2611 0.1909/0.3595 0.4979 -5.8684 2.7281
Table3: DifferenttrainingstrategiesonMulti-NewsandMulti-XSciencedatasets.
summaries #2, we observe a distinct pattern. As simple structure, flat Transformer, has been able
therepetitionoftokensorphrasesbegins,theun- toshowbetterperformanceontheMulti-XScience
certaintyscoresescalaterapidly. andMulti-Newsdatasetsthanthecomplicatedhi-
By comparinguncertainty scoresacross differ- erarchical Transformer structure. The flat Trans-
enttimeslots,wegaininsightsintotherelationship former models are sufficient for MDS tasks with
between repetition and uncertainty in abstractive relativelyshortlengthofdocuments.
summarization. When a repetition phenomenon Furthermore,addingnoisetothedecoderaffects
occurs, we observe notable changes in the uncer- performancemorethanaddingnoisetotheencoder.
tainty score, indicating a correlation between the This sensitivity is likely due to error propagation
two factors. Specifically, as the model generates during decoding and the attention mechanism’s
repetitivesentencesorwords,theuncertaintyscore dependence on accurate encoding. These results
tendstoincrease. Thisincreaseinuncertaintysug- emphasizethedecoder’scrucialroleinproducing
gests that the model becomes less confident and high-qualitysummariesanditssignificantimpact
more uncertainabout theappropriateness orrele- onthesummarizationprocess.
vanceoftherepeatedelementswithinthesummary. The pretrain-finetune strategy that trains the
Byunderstandingthisrelationship,wecandevise model on the pseudo labels first and then fine-
strategiestomitigaterepetitionandsubsequently tuningitontheoriginaldatasetconsistentlyleadsto
enhance the quality of generated summaries. By improvedsummarizationperformancewhencom-
reducinguncertaintythroughtheminimizationof paredtoothertrainingstrategies. Thisfindinghigh-
repetition,wepavethewayformoreaccurateand lightstheeffectivenessofthepretrain-finetunestrat-
reliableabstractivesummarization. egyinenhancingMDSmodelperformance.
Moreover,theanalysisoftherelationsbetween
4 ConclusionandDiscussion repetition and uncertainty provides valuable in-
sightsintoimprovingthequalityofgeneratedsum-
Thisstudyattemptstoempiricallyexaminethein- maries. Thefindingssuggestthatasrepetitionoc-
fluencesonTransformerbehaviorsfromfiveimpor- cursinthesummaries,thereisanoticeableincrease
tantperspectives: documentboundaryseparators, inuncertaintyscores. Byrecognizingthisrelation-
Transformerstructures,thesensitivityofencoder- ship,strategiescanbedevelopedtomitigaterepeti-
decoder architecture, training strategies, and the tionandreduceuncertainty,ultimatelyenhancing
relationshipbetweenrepetitionanduncertaintyin theoverallqualityofabstractivesummaries. These
generatedsummaries. Wefirstexploretheimpact insightscontributetotheadvancementofabstrac-
ofseparatorsontwoflatTransformerandonehier- tive summarization techniques and open avenues
archicalTransformerstructure. forfurtherresearchinimprovingthereliabilityand
Experiments indicate that adding separators effectivenessofsummarygeneration.
makes hierarchical Transformers aware of docu- We also point out the possible exploring direc-
ment boundaries, unlike flat Transformers. This tionforfutureMDSwork: (1)evaluatethegener-
suggests that for models handling complex struc- atedsummariesfrommultipleevaluations;(2)add
tures,separatorscanenhanceperformance. Thene- thehigherlevelofgranularityinformationintothe
cessityofadoptingseparatorsshouldbeconsidered models;(3)investigatetheMDSmethodforpartic-
dependingontheTransformerstructureapplied. ularlylonginputdocuments;(4)paymoreattention
The Transformer structure exploring experi- to the decoder when designing the Transformer-
ments demonstrate that a higher level of granu- basedsummarizationmodels;(5)trytoreducethe
larityisfavorableforthehierarchicalTransformer Suddensharpincreaseandhighuncertaintyscore
structure. The experiments also demonstrate the duringthesummarygenerationprocess.Limitations ShuaiqiLiu,JiannongCao,RuosongYang,andZhiyuan
Wen. 2021. Highlight-Transformer: Leveraging
TheoriginalHierarchicalTransformer(HT)model Key Phrase Aware Attention to Improve Abstrac-
istrainedonfourGPUs(NVIDIATITANXp)for tive Multi-Document Summarization. In Findings
of the Association for Computational Linguistics
500,000steps,butwithanunspecifiedbatch-size.
(ACL/IJCNLP2021),pages5021–5027,Online.
In order to keep a fair comparison and consider
thelimitationofourcomputationresource,allthe YangLiuandMirellaLapata.2019. HierarchicalTrans-
models reported in the paper are trained on the formersforMulti-DocumentSummarization. InPro-
ceedingsofthe57thConferenceoftheAssociation
sameoneGPU,whichinturninfluencesthesetting
for Computational Linguistics (ACL 2019), pages
ofbatch-size. ItmayeffecttheperformanceofHT
5070–5081,Florence,Italy.
model.
YixinLiu,PengfeiLiu,DragomirR.Radev,andGra-
ham Neubig. 2022. BRIO: Bringing Order to Ab-
References stractiveSummarization. InProceedingsofthe60th
AnnualMeetingoftheAssociationforComputational
ArthurBrazinskas,MirellaLapata,andIvanTitov.2020. Linguistics(ACL2022),pages2890–2903,Dublin,
Few-shot learning for opinion summarization. In Ireland.
Proceedings of the 2020 Conference on Empirical
MethodsinNaturalLanguageProcessing(EMNLP YizhuLiu,XinyueChen,XushengLuo,andKennyQ.
2020),pages4119–4135,Online. Zhu.2023. Reducingrepetitioninconvolutionalab-
stractivesummarization. volume29,pages81–109.
EricChuandPeterJ.Liu.2019. Meansum: Aneural
modelforunsupervisedmulti-documentabstractive YaoLu,YueDong,andLaurentCharlin.2020. Multi-
summarization. In Proceedings of the 36th Inter- XScience: ALarge-scaleDatasetforExtremeMulti-
national Conference on Machine Learning (ICML
documentSummarizationofScientificArticles. In
2019),pages1223–1232,LongBeach,UnitedStates. Proceedings of the 2020 Conference on Empirical
MethodsinNaturalLanguageProcessing(EMNLP
Alexander R. Fabbri, Irene Li, Tianwei She, Suyi Li,
2020),pages8068–8074,Online.
andDragomirR.Radev.2019. Multi-News:ALarge-
ScaleMulti-DocumentSummarizationDatasetand
CongboMa,WeiEmmaZhang,MingyuGuo,HuWang,
AbstractiveHierarchicalModel. InProceedingsof
andQUANZ.Sheng.2022a. Multi-DocumentSum-
the57thConferenceoftheAssociationforCompu-
marizationviaDeepLearningTechniques: ASurvey.
tationalLinguistics(ACL2019),pages1074–1084,
ACMComputingSurveys.
Florence,Italy.
Congbo Ma, Wei Emma Zhang, Pitawelayalage Da-
HanqiJinandXiaojunWan.2020. AbstractiveMulti-
sun Dileepa Pitawela, Yutong Qu, Haojie Zhuang,
Document Summarization via Joint Learning with
andHuWang.2022b. Document-awarepositional
Single-DocumentSummarization. InFindingsofthe
encodingandlinguistic-guidedencodingforabstrac-
AssociationforComputationalLinguistics(EMNLP
tivemulti-documentsummarization. InIEEEWorld
2020),pages2545–2554,online.
Congress On Computational Intelligence (WCCI
2022),Padua,Italy.
Wei Li, Xinyan Xiao, Jiachen Liu, Hua Wu, Haifeng
Wang,andJunpingDu.2020. LeveragingGraphto
Yuning Mao, Yanru Qu, Yiqing Xie, Xiang Ren, and
Improve Abstractive Multi-Document Summariza-
JiaweiHan.2020. Multi-documentsummarization
tion. InProceedingsofthe58thAnnualMeetingof
withmaximalmarginalrelevance-guidedreinforce-
theAssociationforComputationalLinguistics(ACL
mentlearning. InProceedingsofthe2020Confer-
2020),pages6232–6243,Online.
ence on Empirical Methods in Natural Language
Chin-Yew Lin. 2004. ROUGE: A Package for Auto- Processing(EMNLP2020),pages1737–1751,On-
maticEvaluationofSummaries. InProceedingsof line.
theWorkshopofTextSummarizationBranchesOut,
pages74–81,Barcelona,Spain. Jun-PingNgandViktoriaAbrecht.2015. BetterSum-
marization Evaluation with Word Embeddings for
TianyangLin,YuxinWang,XiangyangLiu,andXipeng ROUGE. InProceedingsofthe2015Conferenceon
Qiu. 2021. A Survey of Transformers. volume EmpiricalMethodsinNaturalLanguageProcessing
abs/2106.04554. (EMNLP2015),pages1925–1930,Lisbon,Portugal.
Peter J. Liu, Mohammad Saleh, Etienne Pot, Ben KishorePapineni,SalimRoukos,ToddWard,andWei-
Goodrich,RyanSepassi,LukaszKaiser,andNoam Jing Zhu. 2002. BLEU: A Method for Automatic
Shazeer. 2018. Generating Wikipedia by Summa- EvaluationofMachineTranslation. InProceedings
rizing Long Sequences. In Proceedings of the 6th of the 40th Annual Meeting of the Association for
International Conference on Learning Representa- ComputationalLinguistics(ACL2002),pages311–
tions(ICLR2018),Vancouver,Canada. 318,Philadelphia,UnitedStates.Ramakanth Pasunuru, Mengwen Liu, Mohit Bansal, the first 8,000 steps, followed by a subsequent
Sujith Ravi, and Markus Dreyer. 2021. Efficiently multi-steplearningratereduction. Duringthetrain-
Summarizing Text and Graph Encodings of Multi-
ing process, a batch size of 4,096 is utilized, and
Document Clusters. In Proceedings of the 2021
theoptimizationisperformedfor20,000stepsus-
Conference of the North American Chapter of the
AssociationforComputationalLinguistics: Human ing the Adam optimizer. A dropout rate of 0.2 is
LanguageTechnologies(NAACL-HLT2021),pages employedtoenhancemodelrobustness. Allexper-
4768–4779,Online.
iments are conducted on a single NVIDIA 3090
GPUwithoneInteli9-10900XCPU.Theoperating
MaximePeyrard.2019. ASimpleTheoreticalModel
ofImportanceforSummarization. InProceedingsof environmentisprovidedbyUbuntu22.04.3LTS.
the57thConferenceoftheAssociationforCompu-
tationalLinguistics(ACL2019),pages1059–1073,
A.2 SummarizationModels
Florence,Italy.
MaximePeyrard,TeresaBotschen,andIrynaGurevych. VanillaTransformer(VT)(Vaswanietal.,2017)
2017. LearningtoScoreSystemSummariesforBet- isasequence-to-sequencemodelthatisproposed
terContentSelectionEvaluation. InProceedingsof
formachinetranslationtask. Itissubsequentlygen-
the Workshop on New Frontiers in Summarization
(NFiS@EMNLP2017), pages74–84, Copenhagen, eralizedinvarioustasksofNLPduetoitsstrong
Denmark. performance(Linetal.,2021).
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Vanilla Transformer with Copy Mechanism
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz (VTC)1. This variant has a mechanism to copy
Kaiser,andIlliaPolosukhin.2017. AttentionIsAll theattentiondistributionthatoneoftherandomly
YouNeed. InProceedingsofthe31stAnnualCon-
chosenattentionheadsfromtheencodersideinto
ferenceonNeuralInformationProcessingSystems
(NIPS2017),pages5998–6008,LongBeach,United thedecodersothatthegeneratedtextbecomesless
States. repetitiveandlessfactuallyinaccurate.
WenXiao,IzBeltagy,GiuseppeCarenini,andArman HierarchicalTransformer(HT)(LiuandLapata,
Cohan. 2022. PRIMERA: Pyramid-based Masked 2019) proposed a hierarchical attention structure
SentencePre-trainingforMulti-documentSumma-
to attend long sequences effectively and capture
rization. In Proceedings of the 60th Annual Meet-
cross-paragraphcontextualrelationships. Thelocal
ingoftheAssociationforComputationalLinguistics
(ACL2022),pages5245–5263,Dublin,Ireland. Transformerlayersencodeindividualparagraphs
andglobalTransformerlayersexchangeparagraph-
JiachengXu,ShreyDesai,andGregDurrett.2020. Un-
level information from local layers across para-
derstandingneuralabstractivesummarizationmodels
graphs.
via uncertainty. In Proceedings of the 2020 Con-
ferenceonEmpiricalMethodsinNaturalLanguage
Processing(EMNLP2020),pages6275–6281,On-
A.3 Datasets
line.
The empirical studies are based on two widely
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
used MDS datasets: Multi-XScience (Lu et al.,
Weinberger, and Yoav Artzi. 2020. BERTScore:
Evaluating Text Generation with BERT. In Pro- 2020)andMulti-News(Fabbrietal.,2019). Multi-
ceedings of the 8th International Conference on XSciencecontainsdatafromscientificarticles. The
LearningRepresentations(ICLR2020),AddisAbaba,
taskofthisdatasetistogeneratetherelatedwork
Ethiopia.
sectionofatargetpaperbasedonitsabstractand
Chao Zhao, Tenghao Huang, Somnath Basu Roy theabstractsofthearticlesitrefersto. Multi-News
Chowdhury, Muthu Kumar Chandrasekaran, Kath- collectsnewsarticlesfromthesite"newser.com."
leen R. McKeown, and Snigdha Chaturvedi. 2022.
Eachsetofsourcedocumentshasaprofessionally
Readtopnewsfirst:Adocumentreorderingapproach
written summary and the task is to generate that
for multi-document news summarization. In Find-
ingsoftheAssociationforComputationalLinguistics summarybasedonthesources. Table4describes
(ACL2022findings),pages613–621,Dublin,Ireland. the statistics of these two datasets, including the
sizeofthetrain,test,andvalidationset,theaverage
A Appendix
documentlength,andtheaveragesummarylength.
A.1 ImplementationDetails
1We implement the VT and VTC based
The training of all models begins with an initial
on https://github.com/Alex-Fabbri/Multi-
learningrateof2. Aninitialwarm-upphasespans News/tree/master/code/OpenNMT-py-baselines.AverageDocument AverageSummary
Datasets Train/Test/Validation
Length Length
Multi-XScience 30,369/5,093/5,066 778.08 116.44
Multi-News 44,972/5,622/5,622 2,103.49 263.66
Table4: Descriptionoftwousedmulti-documentsummarizationdatasets: Multi-NewsandMulti-XScience.
A.4 DataProcessing BertScore (BS)3 (Zhang et al., 2020) measures
the soft overlap of the token BERT embeddings
ForMulti-XScienceandMulti-Newsdatasets,the
fromthemachine-generatedsummariesandgolden
sourcedocumentsareseparatedbyaspecialtoken
summaries.
named“story_separator_special_tag”. Thelength
oftheinputdocumentsisrestrictedto1024tokens. Relevance(Rel)(Peyrard,2019)calculatescross-
In each document set, the number of tokens for entropyoverindividuallyconstructedprobability
one document is 1024, where N is the number of distributionsforasummarySandasourceDusing
N
documents in a document set. For some shorter their own semantic units ω: Relevance(S,D) =
(cid:80)
documents, the documents repeat themselves to P S(ω i).log(P D(ω i)),whereprobabilitydistri-
fill the 1024 token quota. In the Multi-XScience
ωi
butionsofsummaryandsourcedocumentaregiven
dataset,thecitationsinthesourcesandtargetsare
byP andP respectively.
S D
replacedbyacommontoken‘@cite’.
Redundancy(Red)(Peyrard,2019)evaluatesthe
A.5 EvaluationMetrics quality of the accumulation of information in
the candidate summaries: Redundancy(S) =
ROUGE2 Recall-OrientedUnderstudyforGisting (cid:80)
P (ω ).log(P (ω )).
S i S i
Evaluation (Lin, 2004) is a set of evaluation met- ωi
rics for comparing the overlapping textual units
A.6 VisualizationontheImpactofDocument
between generated summaries and golden sum-
Separators
maries,includingROUGE-1(R-1),ROUGE-2(R-
Wecompareandanalyzetheembeddingspaceof
2),ROUGE-L(R-L),ROUGE-SU(R-SU).R-1and
the tokens after they feed into the encoder with
R-2measuretheoverlappingunigramsandbigrams
and without document separators by t-SNE visu-
respectively while R-L identifies the longest co-
alization (Figure 5). After token representations
occurringsequenceofn-grams. R-SUiscalculated
feedintothehierarchicalTransformerencoder,the
asastatistictomeasuretheco-occurrenceofuni-
cluster boundaries of documents with separators
gramandskip-bigram.
areeasiertobeidentifiedintheembeddingspace.
ROUGE-WE(R-WE)(NgandAbrecht,2015)is
DifferentfromthehierarchicalTransformermodel,
avariantoftheROUGEmetricwhichreplacesthe
thesetwoflatTransformermodelshavedifficulties
hard lexical matching in ROUGE-N with a soft
todistinguishthedocumentclusterboundariesin
matching based on the cosine similarity of word
the embedding space when the token representa-
embeddings. The soft matching in ROUGE-WE
tions after feed into Transformer encoder. Poten-
providesamoreforgivingevaluationbynotstrictly
tially, the hierarchical Transformer prefers more
requiringexactlexicalmatches,thusallowingfor
structural information of documents to compose
variationsinwordorderandphrasing.
the final summaries, while the flat Transformer
BLEUBiLingualEvaluationUnderstudy(Papineni doesnot.
etal.,2002)introducesabrevitypenaltytermand
computes the geometric average of the modified
n-gramprecision.
S3 (Peyrard et al., 2017) is a model-based met-
ric that considers the features from other evalua-
tion metrics, including R-N, R-L, R-WE and JS-
divergence,toproducepyramid(pyr)andrespon-
siveness(resp)scores.
2TheparametersofROUGEare-c95-2-1-U-r1000-n
4-w1.2-a-m. 3ThemodeltypeofBertScoreisbert-base-uncased.VT encoder input VT encoder output VTC encoder input VTC encoder output
(with separators) (with separators) (with separators) (with separators)
VT encoder input VT encoder output VTC encoder input VTC encoder output
(without separators) (without separators) (without separators) (without separators)
(VT) (VTC)
HT encoder input HT encoder output
(with separators) (with separators)
HT encoder input HT encoder output
(without separators) (without separators)
(HT)
Figure5: t-SNEvisualizationoftwoembeddingspacesonMulti-NewsdatasetwithVT,VTCandHTmodels: (1)
tokenrepresentationsbeforefeedingintotheTransformerencoder;(2)tokenrepresentationsafterfeedingintothe
Transformerencoder. Thefiguresinthe1strowarethevisualizationwithdocumentseparatorsandinthe2stroware
thevisualizationwithoutdocumentseparators.