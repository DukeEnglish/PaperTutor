A Comparative Analysis of Interactive
Reinforcement Learning Algorithms in Warehouse
Robot Grid Based Environment
Arunabh Bora
MS Robotics & Autonomous Systems
University of Lincoln
Student ID - 27647565
27647565@students.lincoln.ac.uk
Abstract‚Äî Warehouse robot is a highly demanding field at reinforcement learning (IRL) environment loop impacts the
present. Major technology and logistics companies are investing performance of different value-based reinforcement learning
heavily in developing these advanced systems. Training robots algorithms.
to operate in such a complex environment is very challenging.
In most of the cases, robot requires human supervision to adapt To train the robot agent, I have employed two
and learn from the environment situations. Interactive Reinforcement Learning algorithms to utilize the human
reinforcement learning (IRL) is one of the training computer interaction though IRL, first one is Q-learning and
methodologies use in the field of human-computer interaction. second one is SARSA. With the same person feedback reward
This paper approaches a comparative study between two system, I have compared the reinforcement learning
interactive reinforcement learning algorithms: Q-learning and algorithms behaviours, their success rate, average reward,
SARSA. The algorithms are trained in a virtual grid-simulation- steps taken to reach the goal state etc. To get a fix reward
based warehouse environment. To avoid varying feedback system, I have assigned negative and positive reward
reward biases, same-person feedback has been taken in the methodology. With the help of PyGame visualization, I have
study. created an interactive environment for the robot agent and
human.
Keywords‚Äî Interactive Reinforcement Learning, Q-
Learning, SARSA, Human Computer Interaction (HCI) The initial part of the paper covers the introduction and
motivation of the study. Second part delves into the related
I. INTRODUCTION works previously did by researchers, third part describes the
Autonomous Robot for Warehouse is a highly trending theoretical background of algorithms, fourth part delineates
name now a days. Company like Amazon has already the methodological works and processes of this study. Finally,
deployed over 750,000 robots to work collaboratively in the the fifth part describes the results and analysis and sixth part
their warehouses and workstations [1]. There are multiple is about the conclusion of the study.
reasons why some of the logistic companies are willing to
deploy warehouse robots. Major reasons are they are very
II. LITERATURE REVIEW
cost-effective, highly efficient for some important tasks of In the past decade, the progress of Human Computer
warehouse like packing, picking and transporting goods. Interaction (HCI) was incredible. Many researchers had
However optimizing the autonomous warehouse robots is a already developed several methodologies. The paper [2]
very challenging also rewarding task. Most of the advanced provides an extensive overview of interactive reinforcement
robots uses the computer vision methods or advanced level learning (IRL) methodologies. It offers valuable insights
reinforcement learning methodology. about different IRL algorithms that utilize human feedback.
These include model-based methods, model-free methods and
This paper introduces a simple grid-world simulation that
the hybrid methods. Also, the paper explores different human
serves as an environment for a virtual robot agent. This robot
feedback methodologies such as unimodal feedback
can move only to the define setup of grid world. The primary
(hardware delivered & natural interaction), and multi-model
task is to demonstrate to go the goal step, and avoid other
feedback system (speech-gesture, hardware-facial feedback
states. To demonstrate the robot agent, I have employed the
system and many more).
Interactive Reinforcement Learning (IRL) algorithms [2].
Fundamentally, I consider the grid world to be a warehouse,
and the goal state represents the robot's designated task
position. Upon reaching this point from any starting location
within the warehouse, the robot can then proceed with its other
tasks.
There are multiple methodologies for Human Computer
Fig(1) - Interactive Reinforcement Learning Framework [2]
Interactions. Primarily, Learning from Demonstration (LfD),
Interactive Machine Learning (IML) and Interactive Human computer feedback system varies in different
Reinforcement Learning (IRL). In IRL specifically, the scenario [2], the paper [3] examines four feedback adviser
human supervisor has a clear means of influencing learning - approaches ‚Äì probabilistic early, importance, and mistake
reward. Reward can be positive (reinforcing desired correcting advising within a domestic robot scenario. The
behaviour) or negative (discouraging undesired behaviour). paper also tells the importance of choosing appropriate
advising strategy. It shows the mistake correcting advising
My research question focuses on how the incorporation of
approach achieves the better results than others.
human feedback in a warehouse robot, within an interactive
1The paper [4] presents an interactive Q-learning approach Interactive Reinforcement Learning Algorithm, I have
to enhance the robot autonomy. The paper also suggests an modified the action & reward system of the algorithm. Q-
adaptive way that adjusts behaviour to match individual learning is an off-policy algorithm and SARSA is an on-
preferences and needs. It discusses the state-of-the-art in robot policy algorithm. The primary difference between these two
learning and experimental setups. reinforcement learning algorithms is how they update their
The paper [5] introduces a novel approach utilizing a Deep values, Q-learning picks the best possible action for the next
Q-Network (DQN) model to address the dispatching and step whereas SARSA chooses the actual action taken. The
routing problems in warehouse environment for robot. DQN below figures [Fig(2) & Fig(3)] illustrate my two algorithms
is a value-based reinforcement learning. The authors of workflow pseudo code.
paper[5] compares their model with Shortest Travel
A. Interactive Q-Learning
Distance[STD] rule, and the DQN outperforms the STD. So,
DQN can be good option for Interactive reinforcement This algorithm [Fig(2)] is a modified version of the Q-
learning algorithm in warehouse simulation. learning algorithm. The Q-values are updated based on the
human acceptance received and the maximum expected
The paper [6] shows an innovative approach in human-
future reward.
computer interaction through Deep Reinforcement Learning
for autonomous drones, which can be use for autonomous 1. Initialize Q-table, Œ±, Œ≥
2. while not end of interaction do:
robots also. The paper discusses total 3 DRL algorithms
a. a_t = Œµ-greedy action selection for state s_t
comparison for the task, the authors find that PPO outperforms
b. if Human accepts a_t: -
other two algorithm DDPG and SAC. So, the authors decided i. Execute a_t, observe reward r_t from
to enhance the DDPG algorithm through a Human-in-the- environment, next state s_(t+1)
loop-feedback. And the authors are able to achieve a great ii. Update Q-value (based on agent's action
result as expectation. and environment reward): Q(s_t, a_t) =
Q(s_t, a_t) + Œ± (r_t + Œ≥ * max_a'
Q(s_(t+1), a') - Q(s_t, a_t) )
III. THEORITICAL BACKGROUND
c. else:
Reinforcement Learning [2] is a framework in which i. Execute a_t, observe human-provided
agents learn to solve sequential decision-making problems by r eward r_human, next state s_(t+1)
ii. U pdate Q-value (based on agent's action
interacting with an environment to maximize cumulative
and environment reward): Q(s_t, a_t) =
rewards. RL algorithms are usually divided into three Q(s_t, a_t) + Œ± (r_t + Œ≥ * max_a'
categories: policy search methods, value function methods Q(s_(t+1), a') - Q(s_t, a_t) )
and actor-critic methods [2]. d. s_t = s_(t+1)
3. end while
In the reinforcement learning the Markov Decision
Fig (2) ‚Äì Interactive Q-Learning for learning from the Human
Process (MDP) is utilize to formulate the concept. In the
MDP, with states S, actions A, transition reward T, reward B. Interactive SARSA
function R and a discount factor Œ≥, the goal is to basically find
This algorithm [Fig(3)] is a modified version of SARSA. This
the optimal policy œÄ‚àó , which can maximize the expected
approach maintain the core SARSA update while
future discounted reward.
incorporating the Human feedback in a way that can
The value function VœÄ represents the expected future
influence the agent‚Äôs learning without directly modifying the
discounted reward from each state under the policy œÄ. And
Q-value based on the chosen action. Because in standard
the action-value function Q œÄ represents the expected future
SARSA the reward comes from the environment based on the
discounted reward when taking a specific action in a state and
executed action.
then following policy œÄ.
So mathematically, this can be shown by using Bellman 1.Initialize Q-table, Œ±, Œ≥
2.while not end of interaction do :
Equation [7] ‚Äì
a. a_t = Œµ-greedy action selection for state s_t
ùëâ(cid:3095)(ùë†)= (cid:3533)ùúã(ùë†,ùëé)ùëÑ(cid:3095)(ùë†,ùëé) b. if Human accepts a_t: -
i. Execute a_t, observe reward r_t from
(cid:3028) environment, next state s_(t+1)
ùëÑ(cid:3095)(ùë†,ùëé)= ‚àë (cid:3046)(cid:4593)ùëá(ùë†(cid:4593)|ùë†,ùëé)[ùëÖ(ùë†,ùëé,ùë†(cid:4593))+ùõæùëâ(cid:3095)(ùë†‚Ä≤)]
ii. a_(t+1) = Œµ-greedy action selection for state
s_(t+1)
Interactive Reinforcement Learning [Fig(1)] is a iii. Update Q-value (based on agent's action):
Q(s_t, a_t) = Q(s_t, a_t) + Œ± (r_t + Œ≥ *
framework where humans provides the feedback or guidance
Q(s_(t+1), a_(t+1)) - Q(s_t, a_t) )
during agent learning process. c. else: -
i. Execute a_t, observe human-provided
IV. METHODOLOGY reward r_human, next state s_(t+1)
ii. a_(t+1) = Œµ-greedy action selection for state
s_(t+1)
For this research study, I have employed two Reinforcement iii. Update Q-value (based on agent's action):
Learning Algorithms ‚Äì Q-learning and SARSA. Q-learning Q(s_t, a_t) = Q(s_t, a_t) + Œ± (use
and SARSA (State-Action-Reward-State-Action) are both environment reward+ Œ≥ * Q(s_(t+1),
a_(t+1)) - Q(s_t, a_t) )
value-based reinforcement learning algorithms. They learn
d. s_t = s_(t+1)
the value function (Q-values) associated with state-action e. human reward
pairs but not from the policy directly. Instead, they derive 3. end while
policy from the learned value function by selecting actions
with the highest Q-values. To make these algorithms an Fig (3) ‚Äì Interactive SARSA for learning from the Human
2I have designed a grid-based simulation using the Pygame representation of the number of Episodes vs the number of
library to visualize agent performance in real-time. This steps required for a typical convergence.
function essentially constructs a warehouse environment
simulation on a grid. The robot is represented by a blue circle,
the goal state is a green square, and the lose states are in black
color squares. Figure 4 illustrates the complete visualization
process.
Fig(5) ‚Äì Episode Vs Number of Steps Required In Training
Fig (4) ‚Äì PyGame Visualisation for Agent Real-time Performance As the algorithm is designed primarily for training
Monitoring purposes, it provides real-time rewards per episode during the
learning phase. The reward system works as follows:
The following table [Table(1)] represents the
Hyperparameters and Parameters for both learning ÔÇ∑ +10: The robot achieves the winning state.
algorithms.
ÔÇ∑ -10: The agent enters a losing state.
Table (1) - Hyperparameters and Parameters Of Both
Algorithms ÔÇ∑ 0: The robot doesn't reach any state (win or lose)
within 120 steps.
Hyperparameters Interactive Interactive
The below figure [Fig(6)] shows the average to total rewards
and Parameters Q-Learning SARSA per episode for both Interactive Q-learning and Interactive
Grid Size 4 4 SARSA.
Cell Size 100 100
Number of 100 100
Training Episodes
Maximum Steps 120 120
Learning Rate 0.001 0.001
Discount Factor 0.89 0.89
Epsilon 0.97 0.99
Epsilon_decay 0.99 0.98
Actions UP,DOWN, UP,DOWN,
LEFT,RIGHT LEFT,RIGHT
Wining Position (2,2) (2,2)
Lose Position 1 (1,2) (1,2)
Lose Position 2 (3,2) (3,2) Fig (6) ‚Äì Average of Total Reward per Episode
Wining Reward 10 10
The above figure [Fig(6)] shows that the average of the total
Losing Reward -10 -10 reward graph looks similar for both algorithms after 50 to 60
training episodes. This signifies that as the learning process
increases the agent is able to learn the trajectory of the
The algorithms are designed for primarily training purposes.
environment. Also, from the figure [Fig(5)] it‚Äôs clearly visible
But, the code saved the Q-table as a pickle file, which can be
that the number of steps gets reduced as the number of
used for testing purposes.
learning episodes increases.
V. RESULT
After completing the training phase, the algorithm
Both of the learning algorithms performed well overall. In the
shows the Q-table for the agent. The below figure [Fig(7)]
training phase, Interactive SARSA takes more time and steps
shows a breif idea of the updated Q-table for both algorithms.
than Interactive Q-Learning. The following figure is the
To draw the graph of average Q-values for each action, I have
calculated the mean of the Q table elements.
3VI. CONCLUSION
Emerging fields like Robotics and AI in current days are
dominating all industries. Researchers and Scientists are
trying to apply these advanced approaches in all possible
domains. Logistics companies are endeavouring to integrate
advanced technological fields into the demanding tasks within
warehouses.
In this paper, I have introduced two human-in-loop-
feedback methodology compatible interactive reinforcement
learning algorithms, which are responsible for training the
robot agent to go to a specific position within the grid
simulation-based warehouse environment. I have modified the
two primary basic value-based reinforcement learning
algorithms ‚Äì Q-learning and SARSA. My algorithms have the
capability to train the robotic agent by utilizing real-time
human-provided rewards for each action undertaken.
Fig (7) ‚Äì Average Q-values for each action As a result, the interaction between human-provided
rewards and the agent actions, both of the algorithms have
The above figure [Fig(7)] demonstrates that the most
achieved an overall good performance. Interactive Q-learning
demanding actions taken are RIGHT and DOWN.
has shown a success rate of 75% and the Interactive SARSA
The below table [Table (2)] is derived from the training has shown a 60% success rate in achieving the goal position.
history data.
While both algorithms have performed well, they still have
Table (2) ‚Äì Performance Metrics of Both Algorithms limitations. The primary limitation is the training time. A
higher learning rate can address this, but achieving a deeper
Performance Interactive Interactive
understanding of the environment requires a more balanced
Metrics
Q-Learning SARSA approach. Simply providing additional human rewards may
not be the most effective solution. Alternatively, other deep
Average of Total 7.5 3.8
reinforcement learning algorithms could perform well ,
Reward per
algorithms such as DQN, PPO , DDPG are some better
Episode
alternative approaches for Human Computer Interaction.
Success Rate 75% 60%
Average Number 12 16.13
of Steps per
Episode REFERENCES
Exploration Rate 30% 25% [1] Amazon. "The New Robotic Solutions, Sequoia and Digit, Will
Support Workplace Safety and Help Amazon Deliver to Customers
Faster."October2023.https://www.aboutamazon.com/news/operations/
amazon-introduces-new-robotics-solutions.
[2] Lin, Jinying & Ma, Zhen & Gomez (2020). A Review on Interactive
Reinforcement Learning From Human Social Feedback. IEEE Access.
PP. 1-1. 10.1109/ACCESS.2020.3006254.
[3] F. Cruz, P. W√ºppen, S. Magg, A. Fazrie and S. Wermter, "Agent-
advising approaches in an interactive reinforcement learning scenario,"
2017, pp. 209-214, doi: 10.1109/DEVLRN.2017.8329809.
[4] G. Magyar, M. Virc√≠kov√° and P. Sinc√°k, "Interactive Q-learning for
social robots that learn from the Wizard: A pilot study," 2016 IEEE
(SMC), pp. 003683-003687, doi: 10.1109/SMC.2016.7844806.
[5] M. P. Li, P. Sankaran, M. E. Kuhl, R. Ptucha, A. Ganguly and A.
Kwasinski, "Task Selection by Autonomous Mobile Robots in A
Warehouse Using Deep Reinforcement Learning," 2019 pp. 680-689,
doi: 10.1109/WSC40007.2019.9004792.
[6] Lee, H., & Park, S. (2024). Sensing-Aware Deep Reinforcement
Learning With HCI-Based Human-in-the-Loop Feedback for
Autonomous Nonlinear Drone Mobility Control. DOI
10.1109/ACCESS.2023.3346917.
[7] MacGlashan, J., Ho, M.K., Loftin, R., Peng, B., Wang, G., Roberts,
D.L., Taylor, M.E. & Littman, M.L. (2017). Interactive Learning from
Policy-Dependent Human Feedback. 70:2285-2294. Available from
https://proceedings.mlr.press/v70/macglashan17a.html.
[8] A. Balachandran, A. Lal S "Autonomous Navigation of an AMR using
Deep Reinforcement Learning in a Warehouse Environment," 2022doi:
Fig (8) ‚Äì Comparison Metrics of The Learning Algorithms
10.1109/MysuruCon55714.2022.9971804.
From the above table [Table (2)] and [Fig(8)], it is clearly [9] L. Vordemann, ‚ÄòSafe Reinforcement Learning for Human-Robot
Collaboration : Shielding of a Robotic Local Planner in an Autonomous
visible that Interactive Q-learning performs better than
Warehouse Scenario‚Äô, Dissertation, 2022.
Interactive SARSA.
4