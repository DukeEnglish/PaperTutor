Learning to Imitate Spatial Organization in Multi-robot Systems
Ayomide O. Agunloye1, Sarvapali D. Ramchurn1, and Mohammad D. Soorati1
Abstract—Understanding collective behavior and how it observedbehavior.Recently,data-drivenapproachesleverage
evolves is important to ensure that robot swarms can be imitation learning (IL) algorithms such as inverse reinforce-
trusted in a shared environment. One way to understand
ment learning (IRL) and generative adversarial imitation
the behavior of the swarm is through collective behavior
learning (GAIL) for improved reconstruction accuracy in
reconstructionusingpriordemonstrations.Existingapproaches
often require access to the swarm controller which may not be swarm scenarios modeled as multi-agent systems [10], [11],
available.Wereconstructcollectivebehaviorsindistinctswarm [12], [13], [14], [15]. Genetic programming and graphical
scenarios involving shared environments without using swarm neuralnetworkshavealsobeenusedfordata-drivenbehavior
controller information. We achieve this by transforming prior
reconstruction with swarm controllers extracted from video
demonstrations into features that sufficiently describe multi-
demonstrations[16]andswarmbehaviorprediction[17].For
agent interactions before behavior reconstruction with multi-
agentgenerativeadversarialimitationlearning(MA-GAIL).We recognition of collective behaviour, binary classification of
show that our approach outperforms existing algorithms in all observedbehaviorasdefinedorundefinedcollectivebehavior
investigated swarm scenarios, and can be used to observe and is a common approach [18], [19], [20]. This approach
reconstructaswarm’sbehaviorforfurtheranalysisandtesting, does not scale, as each swarm scenario requires a unique
whichmightbeimpracticalorundesirableontheoriginalrobot
classifier. Multinomial classification addresses this and has
swarm.
been achieved in closely related swarming scenarios using
I. INTRODUCTION predefined and learned multi-agent interaction quantifiers
[21], [22], [23].
Swarmroboticsanditsapplicationsaretransitioningfrom
Existing data-driven behavior reconstruction approaches
the laboratory to the real world [1], [2], and it is expected
extract multi-agent interactions from expert demonstrations
to lead to the large-scale deployment of multiple robots
without capturing swarm-environment interactions. As a re-
in environments occupied by humans and other types of
sult,therecoveredmulti-agentinteractionscannotaccurately
robots. For the robot-robot and human-robot interactions to
reconstruct or predict expert behavior. While [17] address
beseamless, robotswarmsmust besafeand trustworthy[3].
thisintheirwork,theirapproachreliedonlearnedextraction
Ensuring the safe and trustworthy behavior of the swarms
of multi-agent interactions and it is difficult to explain how
in shared environments requires a precise and continuous
the robots interact with the environment.
knowledge of their collective behavior and how it evolves.
Inthiswork,weinvestigatethereconstructionofcollective
In scenarios where the swarm controllers are directly ac-
behavior in practical swarm scenarios involving shared en-
cessible, collective behavior can be easily modeled using
vironments without using swarm controller information. We
the controllers [4]. However, in practical swarm scenarios,
consider three common swarm robotic scenarios: aggrega-
swarm controllers may not be accessible for various reasons
tion, homing, and obstacle avoidance. We model these sce-
(e.g., extracting controllers from natural swarms). In robot
narios as single-objective swarm scenarios where swarming
swarms,accesstocontrollersmayberestrictedorimpractical
agents interact with each other and the environment while
because some robots are remotely operated or the controller
completing the swarm objective. We generate informed and
informationisintentionallyencryptedorhiddenforstrategic
explainablemulti-agentinteractionsthroughfeaturetransfor-
or privacy reasons [5], [6]. To this end, understanding the
mation of expert demonstrations and, achieve near-optimal
collective behavior of a swarm and modeling its dynamics
behavior reconstruction using multi-agent GAIL. We show
without using swarm controller information requires thor-
thatevenwhenlearnerrobotsareinitializedfromunforeseen
ough research.
states, they perform similarly to the expert robots in all
Collective behavior reconstruction and recognition are
investigated scenarios.
two established methods for modeling swarm dynamics
Themaincontributionsofthispaperareasfollows.(1)We
or explaining collective behavior [7]. Collective behavior
present an approach for reconstructing collective behaviors
reconstruction can be model-based or data-driven. In model-
in shared environments without accessing robot controllers;
based approaches, behavior reconstruction is achieved us-
and(2)Wedemonstratetheuseofinformedandexplainable
ing a mathematical or regression model [8], [9]. In data-
multi-agentinteractionsforimprovedlearningrepresentation
driven approaches, multi-agent interactions are statistically
in data-driven behavior reconstruction.
extracted or learned from prior demonstrations to reproduce
II. RELATEDWORKS
1Authors are with the School of Electronics and Computer Science,
IRL has been extensively used in literature for collective
University of Southampton, Southampton, SO17 1TR, United Kingdom.
{a.o.agunloye, sdr1, m.soorati} @soton.ac.uk behaviorreconstructionbecauseitcanrecovertheunderlying
4202
luJ
61
]OR.sc[
1v29511.7042:viXrareward functions while reproducing expert behavior. Sˇosˇic´ S×A→R gives each agent an individual reward r ∈R.
i
et al. [11] reconstructed the behavior of a homogeneous γ ∈[0,1) denotes the reward discount factor.
swarm by assuming that all agents are interchangeable and GAIL achieves imitation learning by matching the occu-
shareacentralrewardfunction,therebyreducingtheproblem pancy measures ρ of the expert policy π in the learner
πE E
to a single-agent IRL. Another study [12] reconstructed domain through generative adversarial training [26]. The
the collective behavior observed in a flock of pigeons by occupancy measure is the unnormalized distribution of an
recoveringindividualrewardfunctionsforeachpigeon.This agent’s trajectory as it navigates the environment using a
individualisticapproachexposedthemulti-agentinteractions policy π [27]. In GAIL, the generator is a policy network
intheflock andallowedtheresearchersto modeltheleader- π that produces trajectories from a similar environment as
follower hierarchy. In [24], a similar individualistic reward the expert. The discriminator network D compares gener-
function approach was used to evolve the robot controller ated trajectories with expert demonstrations and attempts to
using IRL by manually specifying the desired goal location distinguish them through binary classification.
or the path. Other studies [13], [10] extended the IRL The GAIL objective function can be written in terms of
framework to multi-agent IRL to simultaneously recover the occupancymeasuresandexpectationsoverexpertandlearner
reward functions of several agents. Liu et al. [14], however, policies as [27]:
integrated GAIL with population-based training for collec-
tive behavior reconstruction. Aside IRL and GAIL, other ψ⋆ (ρ −ρ )= max E [log(D(s,a))]+
machine learning methods have been used to reconstruct
GA π πE
D∈(0,1)S×A
π
collective behavior. In [16], genetic programming was used E πE[log(1−D(s,a))] (1)
to extract explainable controllers from video demonstrations
where ψ⋆ is the convex regularization imposed on the
of collective behavior with 8 predefined swarm interaction GA
generator by D, D(s,a) is the discriminator output, and
metrics defining the fitness measure. Zhou et al. [17] used
logD(s,a) is the learning signal for the generator.
graphical neural networks to imitate the behavior of expert
GAIL optimizes Equation 1 by finding its saddle point
robots and predict trajectories. Their approach considers
(π,D). At this point, D is unable to differentiate between
swarm-environment interactions and modeled robots and
trajectories from π and π . When π and D are represented
environmental entities as graph nodes, but does not provide E
byfunctionapproximators,GAILfitsaparameterizedpolicy
explainable interactions or controllers. Most of these works
π andadiscriminatornetworkD withweightsw.Thedis-
demonstrate behavior reconstruction in a single scenario or θ w
criminatorfeedbackservesarewardfunctionthatencourages
multiple similar scenarios (e.g. swarming and schooling).
the generator to minimize the dissimilarity between ρ and
However, Yu et al. [15] leveraged Adversarial Imitation π
ρ .
Learning with parameter sharing (PS-AIRL) for behavior πE
In multi-agent systems, individual agents optimize sep-
reconstructionindistinctswarmingscenarios.Theirapproach
arate reward functions that describe their behavior. As a
focused on homogeneous biological swarms and did not
result, multiple reward functions exist and optimality is only
consider swarm-environment interactions. Furthermore, they
guaranteedthroughasetofstationarypoliciesthatprovidea
require access to the original swarm controllers which is
Nash equilibrium solution. Multi-agent GAIL addresses this
rarely available in practical swarm scenarios. In contrast,
by jointly optimizing the Nash equilibrium constraints with
our approach considers swarm-environment interactions and
the objective function during occupancy measures matching
reconstructs expert behavior without accessing robot con-
[28].
trollers. We also generate informed multi-agent interactions
that can be used to explain swarm behavior.
IV. METHOD
III. BACKGROUND In this section, we describe our approach to accurate
We consider decentralized Partially Observable Markov collective behavior reconstruction. We formulate the prob-
Decision Processes (Dec-POMDP) [25] in which agents lem as a collective behavior reconstruction problem in a
receive individual rewards for their actions. A Dec- shared Dec-POMDP environment. Expert demonstrations D
POMDP is defined as an MDP comprising of a tuple contain the absolute position of all observable entities M
⟨N,S,A,T,R,O,Ω,γ⟩. N represents the set of agents in the environment. We transform D into informed multi-
in the Dec-POMDP, S is the global state space of the agent interactions before recovering policies that accurately
environment,Acontainsthesharedactionspaceofallagents reproduce expert behaviors using multi-agent GAIL (MA-
in N and O represents the joint observation space of all GAIL) [28].
agentsintheenvironment.Ateachepisodictimestept,each
A. Expert Demonstrations Transformation
agent i∈N ≡{1,...,n} takes an action a ∈A to form the
i
joint action a ∈ A ≡ An based on its partial observation We transform each expert trajectory in D to a set of state
of the environment o ∈ Ω as provided by the observation representative features f describing the interaction between
i s
function O(s,a) using parameterized policy π (a |o ). The theexpertandallotherobservableentitiesintheenvironment
i i i
state transition function T(s′|s,a) : S ×A → S provides given a state s ∈ D. We achieve the transformation by
thenextglobalstate,andthesharedrewardfunctionr(s,a): computing the cohesion between agent i and every otherentityinM.Thus,thestaterepresentativefeaturesforagent and obstacle avoidance. We model these scenarios as practi-
i in state s is: calcooperativeandsingle-objectiveinasharedenvironment.
We consider a swarm size of 3 and represent our swarming
fi =[−dist(i,j)|j ∈M,j ̸=i] (2)
s agentsasunmannedaerialvehicles(UAVs)withinaccessible
where dist(i,j) denotes the euclidean distance between controllers. To improve learning representation, we reduce
agent i and entity j. thecomplexityofthesharedenvironmentandseparateitinto
motion and control layers. The control layer is a discretized
B. Policy Recovery with Multi-Agent GAIL
grid world representation of the shared environment with
To recover stationary policies in the DEC-POMDP, we reducedstateandactionspace.Wecomparetheperformance
use MA-GAIL with n individual discriminators D = oftheproposedapproachwithbehaviorcloning(BC),where
{D 1,D 2,...,D n} and match occupancy measures on trans- adirectmappingbetweenexpertstatesandactionsislearned
formed expert demonstrations. For the generator network π, [30], and PS-AIRL [15].
weuseMulti-AgentProximalPolicyOptimization(MAPPO)
with parameter sharing in which all learners use a single A. Swarm Scenarios
policy network. This applies to our environment since our
In aggregation, the objective of swarming UAVs is to
agentshaveidenticalobservationandactionspaces(homoge-
maximize the intra-swarm cohesion. They achieve this by
neous agents) [29]. Using individual discriminators ensures
safely forming a cluster at any suitable zone in a shared
that each learner strictly matches the occupancy measures
environment. The shared environment includes two active
of a particular expert. For homogeneous agents, however,
UAVs hovering at fixed positions. Swarming UAVs can
thispreventsgeneralizationaslearnersreceivepoorfeedback
observe fixed-position UAVs if they are within perception
if they behave like any other experts. We address this
range in both layers but can only interact with them in the
through expert demonstration sharing and allow individual
motion layer. We model the individual reward at each time
discriminators to compare trajectories from their learners
step in the control layer r as:
n
with all expert demonstrations available. This ensures that
(cid:40)
learners are positively rewarded for demonstrating any valid n ×c if n >1
r = agents agents (3)
expert behavior instead of the particular behavior from one n −c otherwise
expert.
The policy recovery algorithm is summarized in Algo- where n =|{c >t∀n∈N}|, and t is an environment
agents n
rithm 1. Given expert demonstrations D, learners interact specific aggregation threshold.
with the environment and generate rollout trajectories T . The objective in the Homing scenario differs from aggre-
k
The discriminators are trained using feature transformed gation in that the clustering zone—home position—is fixed
D and T . At each time step, learners receive individual and cannot be dynamically chosen by swarming UAVs. In
k
reward feedback r with which the shared policy is this scenario, the UAVs must explore the environment and
π,Dn
improved. Compared to PS-AIRL [15], our algorithm uses locate the home positions before the episode ends. Once a
n discriminators instead of one and transforms all input UAV finds a home position, it must remain there until all
into state representative features before passing them to the other UAVs have homed. We model r as the maximum
n
discriminators. It also allows the discriminators to share the cohesion between UAV n and any home position at a given
features for improved learning representation. time step.
In a new behavior that we refer to as obstacle avoid-
Algorithm 1 Policy recovery with MA-GAIL ance, UAVs must navigate the shared environment without
Input: expert demonstrations D interacting with fixed-location inactive UAVs in the shared
Randomly initialize generator π & discriminators D environment. This scenario differs from existing obstacle
for k =1,2,... do avoidancescenariosinthattheUAVscanaccessthepositions
Rollout learner T ={T ,T ,...,T } using π already occupied by the inactive UAVs in the control layers.
1 2 k
for n=1,2,...,|N| do However, they receive a large negative reward for doing
Train D to classify fn∀s∈D from fn∀s∈T this. The motivation for this behavior is that it is crucial
n s s k
end for to maintain a safe distance from unknown entities in a
Generate r for each generator policy practicalsharedenvironment,eveniftheyseeminactive.The
π,Dn
UAVs also receive a small negative reward for insufficient
r ←[log(D (fn))]+[log(1−D (fn)]
π,Dn n s n s exploration. We model r
n
as a large constant −c when the
Update π using r with PS-MAPPO cohesion between the UAV n and any fixed position UAV is
π,Dn
end for 0 and 0 otherwise.
B. Simulation Environment
V. EXPERIMENT
Weimplementthemotionlayerforthesharedenvironment
We evaluate the performance of our proposed approach in inWebots[31]usingsimulatedCrazyflies2.0drones[32]as
threeclassicalswarmroboticscenarios:aggregation,homing, theswarmingUAVs.Thesimulationboundaryisa3mby3mThis update frequency ensures that the discriminators are
properly initialized but do not change too quickly, thus
allowing learner UAVs to understand reward patterns.
3) Network Training: All models were trained and evalu-
ated on a single cluster node with a 64 cores 2.2 GHz Intel
CPU and 256 GB of RAM. Expert policies training took
about 7 hours per scenario, while learner policies training
onlytookanhourperscenario.PS-MAPPOimplementations
Fig. 1. Snapshot of the motion layer showing the obstacle avoidance for expert policy and MA-GAIL generator network used the
behavior and position trace for Experts, Learners, and Random swarming
default hyperparameters provided in the original paper [29].
UAVs between t=0 and t=300s. X represents UAVs positions at t=0. Red
boxesareinactiveUAVslocations. The MA-GAIL discriminators were simple 2 layer multi-
layer perceptron network (MLP) with 128 hidden units and
rectified linear unit (relu) activations. These discriminators
continuous rectangular world. UAVs can move in all direc- were trained in parallel using a learning rate of 1×10−5 so
tions and can detect obstacles using onboard range sensors. that their training does not influence the training time. PS-
Figure1showsthepositiontraceoftheexperts,learners,and AIRL was implemented using the algorithm provided in the
random swarming UAVs in the obstacle avoidance scenario paperwhileBCwasachievedusingindividual3layerMLPs
between t = 0 and t = 300s with small red crosses. The with 128 hidden units and relu activation.
three larger crosses on each setup mark the initial positions.
The control layer is a 10×10 grid world environment.
Here, the action space in the is limited to a ∈ R5 cor-
responding only to the high-level control of the UAVs
{stop,right,left,forward,andbackward}.Low-levelmotion
controlssuchaslift-off,turning,translation,andhoveringare
implementeddeterministicallyinthemotionlayer.Agentsin
the grid world can observe the positions of other entities up
to6gridpositionsinalldirections.Giventhatonlytwofixed-
position entities are in the aggregation scenario, the agent’s
observationspace is o∈R10 inthis scenarioand o∈R12 in
others.Allepisodesrunforafixeddurationof50timesteps Fig. 2. Boxplots of true episode rewards obtained in 200 evaluation
episodesbytheproposedapproach(MA-GAIL),BC,andPS-AIRLtrained
in the control layer. This corresponds to an episode duration
with400expertdemonstrationsinallscenarios.
of about 300s in the motion layer.
C. Implementation Details VI. RESULTS
1) ExpertDemonstrations: Togenerateexpertdemonstra- Wedemonstratetheadvantageofourbehaviorreconstruc-
tions of collective behavior in each scenario, we solve the tion algorithm in two different ways. In the first setup, we
corresponding Dec-POMDP grid world using PS-MAPPO initialize learner UAVs from starting positions present in D
for100,000trainingepisodes.Aftertraining,wegeneratean foreveryevaluationepisode.Figure2showstheperformance
expert demonstration data pool of 1,000 trajectories in each comparison between the proposed approach (MA-GAIL),
scenario. We also generate noisy expert demonstrations by PS-AIRL, and BC trained using 400 expert demonstrations
varying expert optimality ϵ∈[0,1], where 0 implies optimal over 200 evaluation episodes with unnormalized reward
experts and 1 implies experts sampling actions at random. It values in all scenarios. As the figure demonstrates, our
shouldbenotedthattrajectoriesintheexpertdemonstrations approach closely reproduces expert behavior in all scenarios
data pool are randomly generated and may contain similar compared to BC and PS-AIRL. We attribute this impressive
expert UAV behavior. performance across distinct swarm scenarios to the trans-
2) Learners UAVs: Learner UAVs interact with the envi- formed expert demonstrations that sufficiently describe the
ronment for 10,000 training episodes using expert demon- multi-agentinteractionsinthesharedenvironmentandexpert
strations between 200 and 500. This expert demonstration demonstrationsharingwhichincreasesthesetofvalidexpert
range was chosen as it agrees with expert dataset sizes in behaviors. PS-AIRL outperformed BC in aggregation and
existing works [17], [15]. Learner UAVs receive individual homing scenariosbut failedto maintainits superiorityin the
rewardsfromtheirdiscriminatorforeachepisodictimestep. obstacle avoidance scenario. We attribute this to the sparsity
The rewards and corresponding trajectories are stored in a of the reward function in the obstacle avoidance scenario,
sharedbufferfortrainingthePS-MAPPOpolicyattheendof which forces the experts to conservatively explore a small
each episode. After 50 training episodes, the discriminators area in the shared environment and avoid the large negative
are first initialized and trained using available learner and rewards. The abundance of sequential data from this region
experttrajectories.Theyarethenupdatedevery50thepisode makesiteasyforBCtocloneexpertactionswheninitialized
for 1,000 training episodes and then every 500th episode. from starting positions close to the area and outperformPS-AIRL. Conversely, the continuous reward function in
aggregation and homing scenarios provides a normal distri-
bution of state-action pairs in expert demonstrations making
it difficult for BC to clone expert actions. It should be noted
that evaluation results are from learners trained using 400
expertdemonstrationsastheyrepresentthebest-performance
region for PS-AIRL and BC.
Fig. 4. Visualization of swarming UAVs positions for 10 evaluation
episodesinallscenarios.Areacoverageofexpert(top)andlearner(bottom).
ActiveUAVlocationsaremarkedbyXandinactivelocationsareshownas
O.
starting positions. Variations in expert and learner UAVs’
absolute positions in Figure 4 result from learner UAVs
matching the occupancy measures of features describing
Fig.3. BoxplotsofnormalizedrewardvaluesforExpert,Random,andMA- expert behaviors in the control layer and not their absolute
GAIL-400 over 200 evaluation episodes initialized from random starting
positions.ThisisintuitivesincetheGAILconvexregularizer
statesinallscenarios.
only penalizes the generator heavily when it maximizes dis-
Inthesecondevaluationsetup,learnerUAVsareinitialized similarity between expert and learner occupancy measures,
randomly from unforeseen starting states at the beginning of and expert demonstration transformation and sharing reduce
each evaluation episode. Figure 3 presents the normalized how often this happens based on absolute positions.
reward values for the experts, random (suboptimal experts Transforming expert demonstration to generate informed
withϵ=1),andlearnerstrainedwith400expertdemonstra- and explainable multi-agent interactions improves learning
tions(MA-GAIL-400)over200differentevaluationepisodes representation and facilitates accurate behavior reconstruc-
in all scenarios. As the figure demonstrates, learners do not tion. Furthermore, using n individual discriminators while
perfectly reproduce expert behaviors in all scenarios due allowing them to share the transformed demonstrations re-
to their non-familiarity with the initial states. This effect duces the complexity of each discriminator network and
is, however, pronounced in the obstacle avoidance scenario guarantees learners will be intuitively rewarded for all valid
where expert agents can safely navigate the fixed-position expert behavior. Nevertheless, these improvements introduce
entitieswithoutinteractingwiththem,eventhoughitisrisky certain limitations. First, cohesion may not sufficiently de-
due to the sparsity of the reward function. Learner UAVs, scribe multi-agent interactions in complex swarm scenarios
on the other hand, do not consistently reproduce this risky involving multiple collective behaviors, and the search for
behavior when initialized from random starting states. This a suitable interaction quantifier in such scenarios might not
shows that while imitating the controllers of multi-robot be trivial. A straightforward approach to address this is to
systems generated with a sparse reward or cost function transform expert demonstrations using several interaction
may be easy, accurately predicting how they will perform metrics as in [16]. However, this may introduce redundancy
in unforeseen states still requires further research. It should and increase the computation budget as the swarm size
benotedthatmodelingtheobstacleavoidancescenariousing increases.Second,usingndiscriminatorsandsharingexpert
acontinuouscohesion-basedrewardfunctiondidnotproduce demonstrationsbetweenthemcanintroducescalabilityissues
optimal expert policies in the control layer. asnbecomesverylarge.Whileweaddressthisthroughdis-
Figure 4 shows the area coverage of optimal experts and tributed discriminator training in this work, several discrim-
MA-GAIL-400learnerUAVsin10evaluationepisodesinall inators (e.g. n > 100) may be computationally expensive
scenarios. The fixed-position active UAVs are marked with to train even in parallel. Finally, the challenge of collecting
‘X’ in aggregation, while inactive UAVs are represented as an adequate number of expert demonstrations remains, and
‘O’ in obstacle avoidance. We observe that learner UAVs do we have not optimized our model to use minimal demon-
not directly reproduce particular expert behavior but unravel strations. We intend to focus on these limitations in future
patterns in the demonstrations that allow them to maximize works.
discriminator reward and mimic any expert. This is evident
VII. CONCLUSION
in the aggregation and obstacle avoidance scenarios where
learnerUAVsdonotexplorethesharedenvironmentasmuch In this work, we reconstructed collective behavior in
astheexperts,eventhoughtheyareinitializedfromthesame shared environments without having access to the swarmcontroller. We achieve this by transforming expert demon- [13] T.Costa,A.Laan,F.J.H.Heras,andG.G.dePolavieja,“Automated
strations into state features that sufficiently describe the Discovery of Local Rules for Desired Collective-Level Behavior
ThroughReinforcementLearning,”Front.Phys.,vol.8,2020.
multi-agent interactions between entities in the shared en-
[14] S. Liu, X. Peng, and T. Wang, “PBT-GAIL: An Imitation Learning
vironments. We investigate three distinct classical swarm FrameworkinSwarmSystems,”inProc.Int.Conf.Auton.Unmanned
robotics modeled as practical scenarios using UAVs. To Syst.(ICAUS),ser.Lect.NotesElectr.Eng.,M.Wu,Y.Niu,M.Gu,
andJ.Cheng,Eds. Singapore:Springer,2022,pp.1884–1894.
improve learning representation, we separate the shared
[15] X.Yu,W.Wu,P.Feng,andY.Tian,“SwarmInverseReinforcement
environments into motion and control layers and model the LearningforBiologicalSystems,”inIEEEInt.Conf.Bioinf.Biomed.
control layers as DEC-POMDPs grid world environments. (BIBM),Dec.2021,pp.274–279.
[16] K. Alharthi, Z. S. Abdallah, and S. Hauert, “Automatic Extraction
Ourresultsinthecontrollayerdemonstratethecapabilityof
of Understandable Controllers from Video Observations of Swarm
our MA-GAIL approach to accurately reconstruct observed Behaviors,”inSwarmIntell.,ser.Lect.NotesComput.Sci.,M.Dorigo,
collective behaviors in practical scenarios compared to ex- H.Hamann,M.Lo´pez-Iba´n˜ez,J.Garc´ıa-Nieto,A.Engelbrecht,C.Pin-
ciroli,V.Strobel,andC.Camacho-Villalo´n,Eds. Cham:SpringerInt.
isting reconstruction algorithms. We show that transforming
Publ.,2022,pp.41–53.
expert demonstrations into state features that sufficiently [17] S. Zhou, M. J. Phielipp, J. A. Sefair, S. I. Walker, and H. B. Amor,
describemulti-agentinteractionsandallowinglearnerrobots “CloneSwarms:LearningtoPredictandControlMulti-RobotSystems
by Imitation,” in IEEE/RSJ Int. Conf. Intell. Robots Syst. (IROS).
to behave like any expert can facilitate swarm behavior pre-
Macau,China:IEEE,Nov.2019,pp.4092–4099.
dictionindistinctorunrelatedswarmscenarios.Inthefuture, [18] M. M. Khan, K. Kasmarik, and M. Barlow, “Autonomous detection
we will investigate behavior reconstruction and recognition of collective behaviours in swarms,” Swarm Evol. Comput., vol. 57,
p.100715,Sep.2020.
in complex practical scenarios involving multiple collective
[19] N.Khattab,S.Abpeikar,K.Kasmarik,andM.Garratt,“Autonomous
behaviors. As cohesion may not sufficiently describe the RecognitionofCollectiveMotionBehavioursinRoboticSwarmsfrom
multi-agent interactions in these scenarios, we will inves- VideousingaDeepNeuralNetwork,”inInt.JointConf.NeuralNetw.
(IJCNN),Jun.2023,pp.1–8,iSSN:2161-4407.
tigate the automated discovery of explainable interaction
[20] W. Li, M. Gauci, and R. Groß, “Turing learning: a metric-free
quantifiers to achieve this. Future work will also consider approachtoinferringbehavioranditsapplicationtoswarms,”Swarm
thereal-worldconstraintsinexperimentswithphysicalmulti- Intell.,vol.10,no.3,pp.211–243,Sep.2016.
[21] M.CenekandS.Dahl,“TowardsEmergentDesign:Analysis,Fitness
robot systems.
andHeterogeneityofAgentBasedModelsUsingGeometryofBehav-
ioralSpacesFramework.”inProc.Artif.LifeConf. Cancun,Mexico:
REFERENCES MITPress,2016,pp.46–53.
[22] D.S.BrownandM.A.Goodrich,“Limitedbandwidthrecognitionof
[1] M. Dorigo, G. Theraulaz, and V. Trianni, “Swarm Robotics: Past,
collectivebehaviorsinbio-inspiredswarms.”inProc.Int.Conf.Auton.
Present, and Future [Point of View],” Proc. IEEE, vol. 109, no. 7,
AgentsMulti-agentSyst.(AAMAS),2014,pp.405–412.
pp.1152–1165,Jul.2021.
[23] S. Abpeikar, K. Kasmarik, and M. Garratt, “Automatic Multi-Class
[2] A.R.Cheraghi,S.Shahzad,andK.Graffi,“Past,Present,andFuture
CollectiveMotionRecognitionUsingaDecisionForestExtractedfrom
ofSwarmRobotics,”inIntell.Syst.Appl.,ser.Lect.NotesNetw.Syst.,
NeuralNetworks,”inIEEEReg.10Symp.(TENSYMP),Sep.2023,pp.
K.Arai,Ed. Cham:SpringerInt.Publ.,2022,pp.190–233.
1–6,iSSN:2642-6102.
[3] M. D. Soorati, M. Naiseh, W. Hunt, K. Parnell, J. Clark, and
[24] I.Gharbi,J.Kuckling,D.G.Ramos,andM.Birattari,“ShowmeWhat
S.D.Ramchurn,“Enablingtrustworthinessinhuman-swarmsystems
you want: Inverse Reinforcement Learning to Automatically Design
throughadigitaltwin,”inPuttingAIintheCrit.Loop,P.Dasgupta,
RobotSwarmsbyDemonstration,”inIEEEInt.Conf.Robot.Autom.
J.Llinas,T.Gillespie,S.Fouse,W.Lawless,R.Mittu,andD.Sofge,
(ICRA). London,UnitedKingdom:IEEE,May2023,pp.5063–5070.
Eds. AcademicPress,2024,pp.93–125.
[25] F.A.OliehoekandC.Amato,AConciseIntroductiontoDecentralized
[4] A. Ligot and M. Birattari, “On Using Simulation to Predict the
POMDPs,ser.SpringerBriefsIntell.Syst. Cham:SpringerInt.Publ.,
PerformanceofRobotSwarms,”Sci.Data,vol.9,no.1,p.788,Dec.
2016.
2022,number:1Publisher:NaturePublishingGroup.
[26] J.Gui,Z.Sun,Y.Wen,D.Tao,andJ.Ye,“AReviewonGenerative
[5] W.Hunt,J.Ryan,A.O.Abioye,S.D.Ramchurn,andM.D.Soorati,
Adversarial Networks: Algorithms, Theory, and Applications,” IEEE
“Demonstrating Performance Benefits of Human-Swarm Teaming,”
Trans. Knowl. Data Eng., vol. 35, no. 4, pp. 3313–3332, Apr. 2023,
Mar.2023,arXiv:2303.12390[cs].
conferenceName:IEEETrans.Knowl.DataEng.
[6] L. Chen, S. Fu, L. Lin, Y. Luo, and W. Zhao, “Privacy-Preserving
[27] J.HoandS.Ermon,“GenerativeAdversarialImitationLearning,”in
SwarmLearningBasedonHomomorphicEncryption,”inAlgorithms
Adv.NeuralInf.Process.Syst.,vol.29. CurranAssociates,Inc.,2016.
Archit. Parallel Process., ser. Lect. Notes Comput. Sci., Y. Lai,
[28] J.Song,H.Ren,D.Sadigh,andS.Ermon,“Multi-AgentGenerative
T.Wang,M.Jiang,G.Xu,W.Liang,andA.Castiglione,Eds. Cham:
Adversarial Imitation Learning,” Jul. 2018, arXiv:1807.09936 [cs,
SpringerInt.Publ.,2022,pp.509–523.
stat].
[7] M. Naiseh, M. D. Soorati, and S. Ramchurn, “Outlining the design
[29] C.Yu,A.Velu,E.Vinitsky,J.Gao,Y.Wang,A.Bayen,andY.Wu,
spaceofexplainableswarm(xswarm):Experts’perspective,”inDis-
“The Surprising Effectiveness of PPO in Cooperative, Multi-Agent
trib.Auton.Robot.Syst. Cham:SpringerNatureSwitzerland,2024,
Games,”Nov.2022,arXiv:2103.01955[cs].
pp.28–41.
[30] B. Zheng, S. Verma, J. Zhou, I. Tsang, and F. Chen, “Imita-
[8] C.W.Reynolds,“Flocks,herdsandschools:Adistributedbehavioral
tion Learning: Progress, Taxonomies and Challenges,” Oct. 2022,
model,”ACMSIGGRAPHComput.Graph.,vol.21,no.4,pp.25–34,
arXiv:2106.12177[cs].
Aug.1987.
[31] Webots,“Cyberbotics:RoboticssimulationwithWebots.”
[9] M.Sinhuber,K.VanDerVaart,Y.Feng,A.M.Reynolds,andN.T.
[32] W. Giernacki, M. Skwierczyn´ski, W. Witwicki, P. Wron´ski, and
Ouellette,“Anequationofstateforinsectswarms,”Sci.Rep.,vol.11,
P.Kozierski,“Crazyflie2.0quadrotorasaplatformforresearchand
no.1,p.3773,Feb.2021. educationinroboticsandcontrolengineering,”inInt.Conf.Methods
[10] D.Waelchli,P.Weber,andP.Koumoutsakos,“DiscoveringIndividual ModelsAutom.Robot.(MMAR),Aug.2017,pp.37–42.
Rewards in Collective Behavior through Inverse Multi-Agent Rein-
forcementLearning,”May2023,arXiv:2305.10548[cs].
[11] A. Sˇosˇic´, W. R. KhudaBukhsh, A. M. Zoubir, and H. Koeppl,
“Inverse Reinforcement Learning in Swarm Systems,” Mar. 2017,
arXiv:1602.05450[cs,stat].
[12] R.Pinsler,M.Maag,O.Arenz,andG.Neumann,“InverseReinforce-
ment Learning of Bird Flocking Behavior,” IEEE Int. Conf. Robot.
Autom.(ICRA)Swarms:Biol.Robot.BackWorkshop,2018.