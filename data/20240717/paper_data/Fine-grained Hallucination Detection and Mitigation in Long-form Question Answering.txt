Fine-grained Hallucination Detection and Mitigation in Long-form
Question Answering
RachneetSachdeva‚ô†,YixiaoSong‚ô•,MohitIyyer‚ô•,IrynaGurevych‚ô†
‚ô†UbiquitousKnowledgeProcessingLab(UKPLab),
DepartmentofComputerScienceandHessianCenterforAI(hessian.AI),
TechnicalUniversityofDarmstadt
‚ô•UniversityofMassachusettsAmherst
www.ukp.tu-darmstadt.de
Abstract
GPT-4
Q: Can anyone Q: Can anyone explain
Long-formquestionanswering(LFQA)aims explain the Question Q the differences [‚Ä¶]
differences between A: A trademark protects
to provide thorough and in-depth answers to copyright and a brand's symbol or
trademark? logo. A copyright
complexquestions,enhancingcomprehension. A: A trademark Answer M protects content. [...]
protects a brand's M: Copyright and
However,suchdetailedresponsesareproneto symbol or logo. A trademark are both
copyright protects QA pair Annotation legal protections for
hallucinationsandfactualinconsistencies,chal- content. [...] intellectual property,
but they [...]
lenging their faithful evaluation. This work Domain Experts
Highlight reason:
introduces HaluQuestQA, the first hallucina- ‚Ä¢Completeness
Evaluation Criteria (span level) ‚Ä¢Answer A is tion dataset with localized error annotations incomplete because
The answer fails to
forhuman-writtenandmodel-generatedLFQA mention the broader
scope of copyright
answers. HaluQuestQA comprises 698 QA Question protection, which
MisconceptionFactuality Relevance includes creative
pairs with 4.7k span-level error annotations works beyond just
music and lyrics.
for five different error types by expert anno-
Preferred Answer: M
tators,alongwithpreferencejudgments. Using Reason: [‚Ä¶]
Completeness References
ourcollecteddata,wethoroughlyanalyzethe
shortcomings of long-form answers and find
thattheylackcomprehensivenessandprovide Figure1: Anoverviewofourdatacollectionprocess.
unhelpful references. We train an automatic Basedonourdefinedaspects,wecollectexperthuman
feedbackmodelonthisdatasetthatpredictser- judgmentsforquestion-answerpairsontheRedditplat-
rorspanswithincompleteinformationandpro- formandtheircorrespondinganswersfromGPT-4.
videsassociatedexplanations. Finally,wepro-
poseaprompt-basedapproach,Error-informed
refinement,thatusessignalsfromthelearned SimplisticevaluationmetricssuchasBLEU(Pa-
feedback model to refine generated answers,
pineni et al., 2002) and ROUGE (Lin, 2004) do
whichweshowreduceshallucinationandim-
notalignwithhumanexperts‚Äôjudgmentsonlong-
provesanswerquality. Furthermore, humans
formanswers(Wangetal.,2022). Therearemany
findanswersgeneratedbyourapproachcom-
prehensiveandhighlypreferthem(84%)over aspects of LFQA ‚Äì factuality, completeness, and
thebaselineanswers.1 relevance‚Äìthatrequireevaluation,motivatingus
tofocusonspan-levelfine-grained errordetection.
1 Introduction
While previous studies have focussed on evaluat-
Long-formquestionanswering(LFQA)provides ingfactualerrorsinlong-formtextgeneration(Lee
comprehensive, user-friendly, and in-depth re- etal.,2022;Minetal.,2023;Lietal.,2023;Muhl-
sponsestocomplexquestionsbyleveragingstate- gayetal.,2023),otheraspectsofevaluation,such
of-the-art large language models (LLMs) and re- as response completeness and relevance ‚Äì which
trievercomponents (Krishnaetal.,2021;Nakano canpotentiallymisleadandconfuseusers‚Äìhave
etal.,2021). WhileLLMsgenerateplausibleand beenlargelyoverlooked.
convincinganswers,theyalsofrequentlyproduce LLMs make many errors for LFQA, which re-
factually inconsistent, irrelevant, and incomplete quireexperthumanannotationstodetect(Gillick
content (Goyal and Durrett, 2020; Laban et al., and Liu, 2010; Iskender et al., 2020; Wang et al.,
2022;Menicketal.,2022;Jietal.,2022), which 2022). Recent work from Xu et al. (2023a) re-
limitstheirapplicabilityinreal-worldapplications. ports that aspects such as factuality, relevance,
completeness,structure,references,andaccessibil-
1Tofurtherresearch,weopen-sourceourdataandcode:
https://github.com/UKPLab/arxiv2024-lfqa-hallucination ityareessentialforevaluatinglong-formanswers.
4202
luJ
61
]LC.sc[
1v03911.7042:viXraThere are no prior studies for LFQA that exam- lucinationsindialoguesummarizationtasks,ours
ine these errors at the span level. Span-level er- isamongstthefirstworkstostudyLFQA-centric
rorannotationandcategorizationhavebeenimpor- propertiessuchasquestionmisconception,factual-
tantforevaluatingandimprovingsystemsinother ity,relevance,completeness,andhelpfulreferences,
generationtaskssuchasmachinetranslation(Fre- atthespan-level.
itag et al., 2021). We fill this gap by collecting
HaluQuestQA, a dataset of LFQA answers anno- Detecting and Mitigating Hallucinations in
tatedatthespanlevelwithfivedifferenterrortypes: LLMs. Increasing focus on the reliability of
questionmisconception, factuality, completeness, LLMs has led to the development of explainable
relevance,andhelpfulreferences;byexpertannota- evaluation metrics (Zhong et al., 2022; Fu et al.,
tors,inadditiontopreferencejudgments,asshown 2023)todetecterrorsinLLMgenerations. Xuetal.
inFigure1. (2023b)presentInstructScore,anexplainablemet-
ric based on LLaMA (Touvron et al., 2023a), to
Next,wetrainanautomaticfeedbackmodelon
obtaindetailederroranalysisforLLM-generated
this dataset that predicts erroneous answer spans
text. However,mostofthecurrentevaluationmet-
withincompleteinformationandprovidesassoci-
ricsrequirehard-to-obtaingoldreferences. Recent
ated explanations. The feedback model provides
workproposesareference-freeevaluationmetric,
fine-grained feedback in the form of error loca-
tion(sentencelevel),errorreason,andconfidence
TIGERSCORE (Jiangetal.,2023b)thatcanlocate,
categorize, and explain errors across various text
scorewithouttheaidofareferencetext(Xuetal.,
generationtasks,includingsummarization,transla-
2023b). Finally,wepropose ERROR-INFORMED
tion,andLFQA.WhileLLM-basedmetricscande-
REFINEMENT,aprompt-basedapproachthatuses
tectdiverseerrors,itisnotalwaysplausibletohave
signals from the feedback model to refine gener-
an external evaluator during real-time inference;
atedanswers(Madaanetal.,2023),whichweshow
hence, sampling-based approaches (Chen et al.,
reduceshallucinationandimprovesanswerquality.
2023;Manakuletal.,2023;MalonandZhu,2024)
Ourcontributionscanbesummarizedasfollows:
have been proposed, wherein consistency across
(1) We release HaluQuestQA, a dataset of span-
multiplesampledmodeloutputsisusedasamea-
levelerrorannotationsonpairsofhuman-written
sureoffactuality.
and model-generated answers. Our data analysis
Reinforcement learning with human feed-
shows that long-form answers lack comprehen-
back (RLHF) (Ziegler et al., 2019), a framework
sivenessandprovideunhelpfulreferences;(2)We
toincorporatehumanfeedbacktoalignLMs,has
trainafeedbackmodeltodetectspan-levelerrors
been used to reduce undesirable LLM genera-
alignedwithexperthumanjudgments;(3)Wepro-
tions (Ouyang et al., 2022; Bai et al., 2022a,b).
pose Error-informed refinement, an approach to
Wu et al. (2023b) propose fine-grained RLHF, a
refine LLM-generated answers with fine-grained
framework that enables learning reward models
feedbackprovidedbyourlearnedmodel. Ourap-
associatedwithspan-levelhumanfeedbackondif-
proachconsistentlyoutperformsbaselinesutilizing
ferent error types. However, training multiple re-
coarse-grainedfeedback(lackingdetailederrorjus-
wardmodelsiscomplexandcompute-intensive. A
tifications),reducinghallucinations.
recentalignmenttechnique,directpreferenceopti-
2 RelatedWork mization(DPO)(Rafailovetal.,2023)bypassesthe
rewardmodelingstepinRLHFandhasbeenused
Human evaluation. Prior work (Krishna et al., to fine-tune LMs for factuality using preference
2021)hasshownthathumanevaluationforLFQA ranking over model responses (Tian et al., 2023).
tasks is challenging due to long answer lengths, Humanfeedbackhasalsobeenusedtotrainfeed-
andexpertannotatorsarerequiredtoevaluatethem back models (Wang et al., 2023; Xu et al., 2024)
effectively. Xu et al. (2023a) hire (non-)expert toguidetherefinementofLLMoutputs(Madaan
annotatorsandidentifyninemulti-facetedaspects et al., 2023; Welleck et al., 2023), improving an-
formeaningfulLFQAevaluation. Whilesomeof swerquality. However,thesefeedbackmodelsare
thesefine-grainedaspects,suchasfactuality(Goyal eithernottrainedtoprovidefine-grainederrorfeed-
and Durrett, 2020; Laban et al., 2022), coher- backorrelyonthegroundtruthpassagetodetect
ence(Goyaletal.,2022),andcompleteness(Tang errors, which may not always be accessible for
etal.,2024),havebeenstudiedtoinvestigatehal- open-domainQAtasks. OurworkaimstoannotateCategory Preference Krippendorf‚Äôs state-of-the-art LFQA systems. We chose GPT-
(#samples) ùõº 4 (gpt-4-0314) as the LFQA model to evalu-
Human Model
ate since previous work (Bhat et al., 2023) has
Physics(94) 33% 67% 0.01
shownittooutperformexistingopen-sourceLLMs
Chemistry(96) 22% 78% 0.20
(LLaMA and Alpaca (Taori et al., 2023)) in rea-
Biology(110) 25% 75% 0.36
soning and inferring from long context. Since
Technology(110) 16% 84% 0.53
this model has likely seen training data up to
Economics(110) 14% 86% 0.31
History(92) 9% 91% 0.52 September2021,itmayhavealreadyseentheELI5
Law(86) 16% 84% 0.59 datasetreleasedbyFanetal.(2019)duringitspre-
training. Thus, we scrape more recent questions
Average 19.29% 80.71% 0.36
fromther/explainlikeimfivesubredditspostedbe-
Table1: OverviewofHaluQuestQAandexpertanswer tweenNovember2022toMarch2023. Theques-
preferences,withexperts‚Äôagreementonasmallersub- tions on the ELI5 are classified into domains via
set(‚àº15%)calculatedusingKrippendorf‚Äôsalpha. theFLAIRlabel(tagcontainingpostinformation),
which lets us perform domain-specific analysis.
Forunclassifiedcategories(likeHistoryandLaw),
fine-grainederrorsinLFQA,usingthisdatatotrain
weclustertheOTHERcategoryquestions(notin
areference-freefeedbackmodelforsentence-level
pre-defined ELI5 domains), using K-means clus-
errordetectionwithjustifications. Wefurtherpro-
tering (Selim and Ismail, 1984) and identify the
pose a prompt-based approach to refine answers
domain-specific questions. For each domain, we
withfeedback,enhancingtheircomprehensiveness.
samplebetween100-200questionswiththeirhigh-
3 HaluQuestQA(HQ2A) estvotedansweroflengthrangingbetween50-500
words(moredetailsinAppendixA).
PriorLFQAevaluationswithnon-expert(Nakano Toobtainthemodel-generatedanswers,wezero-
etal.,2021)andexpert(Xuetal.,2023a)annotators shotprompttheGPT-4model(AppendixB.1). We
collectpreferencejudgmentsovermodelresponses. hosttheannotationtaskontheINCEpTIONplat-
However, overall preference is not indicative of form(Klieetal.,2018)andevaluatethefollowing:2
fine-grained errors in LFQA. As a first step, we
annotate span-level errors in long-form answers, 1. Questionmisconception: Falseassumptions
withexplanationsfromdomainexperts. madewithinthegivenquestion.
2. Factuality: Accuracyandcorrectnessofthe
3.1 HiringAnnotators
answerasperverifiablefacts.
WerecruitdomainexpertsonProlific‚Äôsacademic
annotationplatformforsevendomainsshowninTa- 3. Relevance: Specificity and meaningfulness
ble1. Theexpertselectionisbasedonage(22-32), oftheanswer.
demographics(USandUK),education(undergrad-
4. Completeness: Answercomprehensiveness
uateorgraduatedegreeinthetargetdomain),and
ensuringallquestionaspectsareaddressed.
nativelanguage(English). Foreachtargetdomain,
wefirstconductasmallpilotcomprisingtensam- 5. References: (Un)helpfulexamples,analogies,
ples, where given a question and two candidate andexternalreferences(websitesorlinks)in
answers,theexpertsevaluatetheanswersandmark theanswer.
the incorrect spans based on our defined evalua-
Based on the defined evaluation criteria, annota-
tioncriteria(¬ß3.2). Basedonthepilotresults,we
torsidentifyandhighlightmistakesinthequestion
choose three experts per domain and give them
or answers with free-form justifications, in addi-
eachalarge-scalestudycontaining35-50question-
tiontooverallanswerpreference. Examplesfrom
answerpairs. Wecollectexpertjudgmentsfor698
HaluQuestQAareshowninTable2.
questions.
3.3 QuantitativeAnalysis
3.2 TaskSetup
ExpertspreferGPT-4answersoverhumanan-
We evaluate two answers (human and model-
swers. As shown in Table 1, experts display a
generated) to the same question. This setting en-
ables us to identify errors made by humans and 2WeprovidedetailedannotationguidelinesinAppendixF.Aspect Question AnswerA AnswerB ExpertJustification
How do people in (human)[...]Whileprisonfoodgen- (model)[...] Inprison,inmatesof- This is an assumption, not every
prisongetsojacked erally doesn‚Äôt taste great, it does tenhavealotoffreetime,which prisonoffersterriblefoodwithout
Question with terrible nutri- havecaloriesandprotein.Thatcan theycanusetoworkoutandde- nutritionalvalue,alsolotsof"easy"
Misconception tionandnosupple- alsobesupplementedwiththestuff velopaconsistentexerciseroutine. foodchoices(lentils, eggs, bread,
ments? theycanbuyoncommissary(fish Theymaynothaveaccesstoafully- meat,..)offerhighamountsofpro-
packs,chickenpacks,cheese,[...] equippedgym,but[...] teinwhichisneededforweightgain.
Whycanyousleep (human) Your body produces a (model) During sleep, your body Theurineproductionisdecreased
through the night hormone called (vasopressin) an- produceslessurine[...] Addition- duetothehormone,notaddition-
Factuality withoutgoingtothe tidiuretichormone,oradh,which ally,thehormonevasopressinhelps ally. Themoreconcentratedurine
bathroom? causes the body to produce less toreduceurineproductionatnight. isbecauseyoucannotdrinkwhile
urineatnight.It‚Äôsthereason[...] Thiscombinationofdecreased[...] sleeping.
Why is it taboo to (human)Becausecompaniesusedto (model)Talkingaboutwagesiscon- Thisdoesnotcontributetothean-
talk about people‚Äôs beabletofirepeople[...]discussing sideredtaboobecauseitcancreate swer.
Relevance wages? yourwagesisafederallyprotected tensionandjealousyamongcowork-
right now in the USA, so by all ers[...].Employersoftendiscourage
meansdiscussdiscussdiscuss![...] discussionstopreventpotential[...]
Cananyoneexplain (human) A trademark protects a (model)[...]Copyrightprotectscre- The answer fails to mention the
the differences be- brand‚Äôssymbolorlogo. Acopy- ativeworkslikebooks,music,and broaderscopeofcopyrightprotec-
Completeness tweencopyrightand rightprotectscontent.Sotheac/dc art,[...]. Trademark,ontheother tion,whichincludescreativeworks
trademark? logowiththelightningboltwould hand,protectsbrandidentifierslike beyondjustmusicandlyrics.
betrademarked.[...] logos,[...]
Howdoessoftware (human)[...] outputhasahighde- (model) ChatGPT detection soft- Thisexampleiswellsuitedforthe
thatcandetecttext greeofregularity[...]It‚Äôslikecom- ware works by analyzing textual explanation in the paragraph. It
generated by Chat- paringahandknitscarftoanindus- patternsandlanguageusetodeter- givesaclearrepresentationofhow
References GPTwork? trialmachine‚Äôsknitscarf:onemight mineifthetextwasgeneratedbyan thesoftwaredetectsChatGPTtext
havesmallmistakesthatshowitwas AIlikeChatGPT.Essentially,these inmyhead.
madebyahumanhand,whereasthe programs compare the input text
otherisalmostperfectbecausethe with known characteristics of AI-
machinefollowsanexactpattern. generatedtext[...]
Table2: Examplesofexpertannotatederrorsinlong-formanswersbasedonthedefinedevaluationcriteria.
highpreference(80.7%)formodel-generated an- answers, affecting overall preference. Moreover,
swers from GPT-4 compared to human answers. priorworkhassimilarfindingsforhumandisagree-
Potentially, humans prefer fluent answers, and mentinLFQAevaluation(Xuetal.,2023a).
LLMsareknowntooptimizeforfluency(Wuetal.,
Answer scoring. We score human and model
2023a; Coyne and Sakaguchi, 2023). Moreover,
answers on our defined evaluation criteria to un-
thepreferenceofourannotatorsiscorroboratedby
derstandhowexperts‚Äôanswerpreferencesdiverge
similarfindingsinsummarization(Liuetal.,2023)
acrossdifferentdomains. Foreachofquestionmis-
andLFQA(Xuetal.,2023a),whoshowthatGPT-3
conceptionandreferenceaspects,thescoreS = 1
answersscorehigherthanhumananswers.
whenthequestionhasnomisconceptionsandthe
Science questions are challenging for LLMs. references,ifprovided,helpanswerthequestion;
Model-generatedanswersarestronglypreferredby otherwise, S = 0. For aspects of factuality, rele-
expertsinhistory,law,technology,andeconomics vance,andcompleteness,wecalculateS as:
(>80%). Incontrast,thesciencedomainsaremore
(cid:18) (cid:19)
challenging,withapreferenceformodelanswers #Errorsentences
S = 1‚àí
rangingbetween60%-80%. Total#ofsentences
Expert (dis)agreement. In Table 1, we report Forcalculatingtheoverallanswerscores,weleave
Krippendorf‚Äôs alpha (Hayes and Krippendorff, outthequestionmisconceptionscoresbecausethis
2007) as a measure of agreement for experts‚Äô aspectpertainstothequestion. Wesumtheother
overallanswerpreference. Ourexpertannotators aspectscoresandincludetheoverallanswerpref-
achievemoderateagreementintechnology,history, erencescores(S = 1ifpreferred)togetthefinal
andlaw,fairagreementinbiologyandeconomics, score. Finally, we normalize this score between
andslightagreementinphysicsandchemistry.3 We 0 and 1. In Figure 2, we report the fine-grained
emphasizethatthedisagreementbetweenexpertsis aspectscoresforhumanandmodelanswersacross
notafailureofourevaluation. Instead,ithighlights differentdomainsanddiscussourfindingsbelow.
thechallengesofidentifyingfine-grainederrorsin
Questionsfromtechnologyandeconomicsare
3InterpretationofagreementfollowsWongetal.(2021) biased. AmbiguousandmisinformedquestionsHuman Model
Question misconception Factuality Relevance
1 1 1
0.5 0.5 0.5
e 0 0 0
r
ct
sco phy cs hi ecs mistr biy to el co hg ny o el co og ny o mic hs istory law phy cs hi ecs mistr biy to el co hg ny o el co og ny o mic hs istory law phy cs hi ecs mistr biy to el co hg ny o el co og ny o mic hs istory law
e
p Completeness References Overall
s
A 1 1 1
0.5 0.5 0.5
0 0 0
phy cs hi ecs mistr biy to el co hg ny o el co og ny o mic hs istory law phy cs hi ecs mistr biy to el co hg ny o el co og ny o mic hs istory law phy cs hi ecs mistr biy to el co hg ny o el co og ny o mic hs istory law
Figure 2: Comparison of fine-grained scores of the human-written and model-generated answers for different
evaluationcriteria. Thelastfigure(withredboundary)showstheaveragedandnormalizedoverallscores. Ahigher
scorerepresentsfewererrorsintheanswers.
canleadtoundesirableanswers(Coleetal.,2023; 4 HallucinationMitigation
Kimetal.,2023). Therefore, fairanswerscoring
In ¬ß3.3, we have shown that LFQA answers lack
requires prior estimation of question quality. For
comprehensivenessandomithelpfulinformation.
this,weutilizethequestionmisconceptionaspect
Therefore, we train a feedback model to identify
andfindthatquestionsfromallevaluateddomains
erroneousanswerspanswithincompleteinforma-
consistofmisconceptionsarisingfromtheuser‚Äôs
tionandprovidefree-formerrorjustifications. Our
bias or misinformation. This is especially promi-
nentintechnologyandeconomics,where‚àº 40%
approach,ERROR-INFORMED REFINEMENT,uses
thisfeedbacktorefineanswersandimprovetheir
ofthequestionsaremisinformed‚Äìusershavelow
overallqualitywithouthumanintervention.
domainknowledgetoasktherightquestions.
4.1 ErrorFeedbackModel
Answers lack comprehensiveness and provide
Given an input question and an LFQA response,
unhelpful references. We observe that human-
thefeedbackmodelgeneratesalabel[Complete]
written and model-generated answers score high
or[Incomplete]foreverysentence1...ùëõinthere-
onfactualityandrelevanceaspects,meaningmost
sponseandgivesassociatedreasonsfortheincom-
oftheinformationprovidedintheanswersisverifi-
pletesentences(seeFigure3). Wemodelthisasa
able,trustworthy,andrelatedtothequestion. Inter-
sequence-to-sequencetaskandfinetuneaLLaMA2-
estingly,theanswersscorelowonthecompleteness
13Bmodel(Touvronetal.,2023b).
andreferencesaspects,lackingimportantinforma-
tion and providing web references and examples Fine-tuning. Training the feedback model re-
thatarenotuseful, aspertheexperts‚Äôjudgments. quireshigh-qualityerrorannotationswithjustifica-
Specifically,modelshallucinateandprovideincor- tions. Tothisend,weutilizeourHQ2Adatasetand
rectormade-upweblinks. Incontrast,humanan- extract QA pairs with errors in the completeness
swersdigressfromthetopic,providingirrelevant aspect. For every extracted sample, we segment
informationthatleadstoundesirableconclusions. theanswerintosentencesandmarkeverysentence
Overall,modelanswersscorebetterthanthehu- withthe[Complete]or[Incomplete]tagalongwith
mananswersinalltheevaluateddomains. While theexpert‚Äôsjustifications. Thefinaldatasetconsists
thisisduetotheirbetterperformanceoverhumans of509samplessplitintotrain(90%)andtest(10%)
ontheconsideredaspects,webelievethattheper- sets. Wetrainthemodelwithbatchsize4,learning
suasivenatureofmodelanswers(Salvietal.,2024) rate2ùëí‚àí5,andsequencelength1024for5epochs.
alsoplaysacrucialroleintheirhigherpreference. WelistthepromptsusedinAppendixB.2.Q: Why does putting a teabag Refined answer: When you
in hot water make tea in just put a teabag in hot water,
Consistency score: 1.0
a few minutes, while in cold the tea flavors and caffeine
water, it takes hours? dissolve quickly because
Prediction (span-level):
hot water has more kinetic
1. [Incomplete] Reasons:
A: A lot of chemical reactions energy than cold water ‚Ä¶
Diffusion is not a
work faster in hotter On the other hand, when
chemical reaction.
environments. The reaction you steep tea in cold
2. [Complete]
itself is called diffusion. It is water, the molecules have
Feedback 3. [Complete] Refine
more of a substance in one less kinetic energy, and
4. [Complete]
area in relation to another model model the process of diffusion
area. If there is a way for ‚Ä¶ occurs much slower.
Figure 3: A pictorial view of our Error-informed refinement approach. The feedback model takes as input a
question-answerpairandoutputsspanlevelerrorwithjustificationsandaconsistencyscore. Therefinemodeluses
thisfeedbacktoimprovetheoriginalanswer.
Inference. Thetrainedfeedbackmodelhalluci-
nates web references in about 20% of test sam- 1 ‚àëÔ∏Åùëö ùëñ ‚àëÔ∏Åùëõ
ples. Thislikelyoccursbecausethetrainingdata S RC = ùëö ùëñ 1 ùë§ ùëñùëò‚ààùëó ùë† (2)
includes web references in expert error justifica-
ùëò=1 ùë†=1
t ei no tn lys .,w Tohi cc oh mth be atm tho id se ,l ws etru og pg tl fe os rt ao sr ae mpl pic lia nt ge -c bo ah see dr- tiow nh ùëóe ùë†r ae n1 dùë§ 0ùëñùëò‚àà ifùëó
ùë†
ni os t1
.
Fif into ak lle yn ,wùë§ eùëñùëò si es li en ctth the eju ss at mifi pc la e-
approach(MalonandZhu,2024)toprovidemore
output with the highest score as the feedback for
consistentfeedback. Theintuitionisthattrustwor-
therefinementmodel. Aftersampling,wenoticea
thy details and references should appear in many
50%reductioninreferencehallucinations,downto
othergeneratedsamples. Hence,duringthedecod-
‚àº 5‚àí10%testsetsamples.
ingstep,weusenucleussampling(Holtzmanetal.,
2020)withp=0.9andsample20responsesfromthe
4.2 Error-InformedRefinement(EIR)
feedbackmodelandchecktheirconsistencyintwo
stages: 1)TAG CONSISTENCY: Thispertainstothe Our approach is shown in Figure 3 and con-
consistencyofspan-leveltagpredictions,complete sists of two main components: an error feedback
or incomplete, for each sampled response. The model (¬ß4.1), and a refinement model. Given
tagconsistencyscoreiscalculatedbycountingthe an input prompt ùë• ùëñ and a corresponding human-
numberofothersampledresponsesthatmatchthe writtenormodel-generatedresponse ùë¶ ùëñ,thefeed-
tagsequenceofeachsampledoutputandaveraging backmodelE generatesatargetedfeedback ùëì ùëñ that
overthetotalnumberofsamples. Formally,ifthe representsthequalityofùë¶ ùëñ infree-formnaturallan-
sampledtagpredictions ùëù 1,...,ùëù ùëõ consistoftagse- guage. Finally, the refinement model uses ùë• ùëñ, ùë¶ ùëñ,
quencesùë° 1,...,ùë° ùëõwhereùë° ùëñ isalistoftagpredictions and ùëì ùëñ, generating a refined and improved output
foreveryspan,thescoreforsampleùëñ is response ùë¶ÀÜùëñ. The following sections describe our
approachinmoredetail.
ùëõ
1 ‚àëÔ∏Å
S TC = ùëõ 1ùë° ùëñ=ùë° ùë† (1) Refinement Model. Our experiments use the
ùë†=1 LLaMA2-13B chat LLM and its DPO optimized
where1ùë° ùëñ=ùë°
ùë†
is1ifthetagsequenceùë° ùëñ isthesame version(seeAppendixC)astherefinementmod-
astagsequenceùë° ùë† and0ifnot. Thesampleswith els. In each case, the model is 0-shot prompted
the highest score are selected for the next stage. withthefine-grainederrorfeedbackreceivedfrom
2) REASON CONSISTENCY: We assess the con- the error detection model. We also experiment
sistencyofjustificationsgivenfortheincomplete withtwostrongbaselinefeedbackmodels,1) IM-
spans from the remaining samples. Specifically, PROVE: Therefinementmodelis0-shotprompted
we count the number of other sampled justifica- toimprovetheanswerwithoutanyfeedbackpro-
tions from the LLM that matched each token of vided. 2) GENERIC: The refinement model is 0-
each sampled output and score each justification shotpromptedtoimprovetheanswerwithageneric
by the average count per token. Formally, if the error feedback that asks the model to provide a
sampled justifications ùëó 1,..., ùëó ùëõ consist of words more complete and accurate answer. We list the
ùë§ ùëñùëò,ùëò = 1...ùëö ùëñ,thescoreofsampleùëñ is promptsusedinAppendixB.3.Datasets & Evaluation Metrics. We test our Dataset Errorspan Accuracy(‚Üë) Consistency
error-informed refinement approach on three Score(‚Üë)
datasets: HQ2Awithspan-levelerrorannotations Different 38.56¬±0.93% 0.71¬±0.02
foranswercompleteness,ASQA(Stelmakhetal., HQ2A Adjacent 24.18¬±0.92% 0.82¬±0.01
2022), and ELI5 (Fan et al., 2019). The ASQA Exact 37.25¬±0.00% 0.86¬±0.01
dataset consists of 6K ambiguous factoid ques-
Table3: Accuracyofourfeedbackmodelindetecting
tions with long-form answers synthesized from
sentence-levelerrorscomparedtotheexperterroranno-
multiplesourcestoresolvetheambiguities. ELI5
tations. Thefeedbackmodelpredictionscloselyalign
consistsof270Klong-formanswerscoveringgen-
withhumansatconsistencyscoresabove0.80.
eraltopicsfromthesubreddits"explainlikeimfive",
"askscience",and"AskHistorians"onReddit.
We evaluate the refined answers using Tiger- model is unsure in its error prediction feedback,
Score,atrainedreference-freemetricthatidentifies whileascoreabove0.85showsthattheprediction
errorsinLLM-generatedtextandassignsascore highlyalignswithhumans.
based on error severity. Specifically, we use the Wefurtherevaluateourerrorfeedbackmodelby
LLaMA-7BtrainedversionofTigerScore,which comparingthegapinthedownstreamLFQArefine-
highlycorrelateswithhumansforerrordetectionin menttaskwhenweusehuman-annotatederrorfeed-
LFQAtasks(Jiangetal.,2023b)whilebeingmuch back. This evaluation measures the effectiveness
less expensive than human evaluation. Further- ofourfeedbackmodelinguidingtherefinementof
more,weevaluatetheerrorcorrectioncapabilities long-formanswersandreducinghallucinations. In
ofourrefinementapproachusingprecision,recall, Table4,wepresenttherefinementperformanceof
andF1. Lastly,weconductahumanevaluationto ourfeedbackmodelascomparedtotheexperthu-
evaluatethecomprehensivenessandpreferenceof
manfeedbackonHQ2A.Wefindthatourfeedback
therefinedanswerscomparedtogoldanswers. model‚Äôs performance is very competitive, reduc-
inghallucinatedsamplesby2%andimprovingF1
5 Results score by4% compared tothe expert human feed-
back. Thisresultvalidatestheeffectivenessofour
Weexploreseveralresearchquestions: 1)Canour
feedbackmodelinrefiningLFQAanswers.
learnedfeedbackmodeldetecterrorsinLFQAsys-
tems and help in downstream answer refinement 5.2 Fine-vs. Coarse-grainedFeedback
task? 2)Doesfine-grainedfeedbackproducebetter Table 4 shows the quality of answers refined us-
quality LFQA answers than coarse-grained feed- ing different forms of feedback plus the baseline
back? 3)Doesfine-grainedfeedbackhelpmitigate qualityofanswersfromthedatasets. Weobserve
hallucinationsandimprovethecomprehensiveness that inadequate feedback deteriorates the quality
ofLFQAanswers? 4)Arecomprehensiveanswers ofgeneration. Whiledirectlypromptingtherefine-
fromourapproachpreferredbyhumans? ment model (IMPROVE) performs better than the
baseline,promptingwithmoretargetedfeedback
5.1 DetectingErrorsviaFeedbackModel
(GENERIC)consistentlyoutperformstheIMPROVE
Since detecting erroneous spans in long-form approach and generates better quality LFQA an-
answers is hard, we measure the accuracy of swers. Thishighlightstheimportanceofproviding
our feedback model in three different settings; detailedfeedbacktotherefinementmodel.
model-detected erroneous spans are entirely dif- In contrast, providing fine-grained feedback
ferent (DIFFERENT), adjacent (ADJACENT), and fromourerrordetectionmodel(EIR)outperforms
exactly similar (EXACT) to the human-annotated coarse-grainedfeedbackandevenfine-grainedhu-
spans. InTable3,weshowthesentence-levelerror man feedback (on HQ2A), delivering consistent
detectionaccuracyofthefeedbackmodelascom- improvements in reducing hallucinated samples
paredtothestronghumanbaseline. Thefeedback andhallucinationscoresby‚àº 3%andŒî ‚àº 38%,re-
model detects the exact and adjacent error spans spectively,andimprovingF1scoresby‚àº 5%over
withacombinedaccuracyof61%. However,itis alltheevaluateddatasets. UsingourDPO-aligned
important to note that the model gives high con- refinementmodeldoesnotreducethehallucinated
sistency scores when confident in its predictions. samples. However, itachievesthebesthallucina-
Aconsistencyscorelessthan0.80meansthatthe tionscoreonASQAandELI5,showingthatopti-Tigerscore ErrorCorrection
Dataset Approach
%Hallucinated Hallucination Precision(‚Üë) Recall(‚Üë) F1(‚Üë)
samples(‚Üì) score(‚Üì)
Humanfeedback 2.61¬±0.92 0.09¬±0.01 0.86¬±0.04 1.00¬±0.00 0.94¬±0.02
HQ2A
Baseline 19.61 0.63 - - -
Improve 1.31¬±0.92 0.05¬±0.04 1.00¬±0.00 0.93¬±0.05 0.97¬±0.02
Generic 1.31¬±0.92 0.05¬±0.03 0.97¬±0.04 0.97¬±0.05 0.97¬±0.02
EIR 0.65¬±0.92 0.03¬±0.04 0.97¬±0.04 1.00¬±0.00 0.98¬±0.02
EIRw/DPO 4.57¬±2.44 0.07¬±0.02 0.90¬±0.08 0.87¬±0.05 0.88¬±0.06
Baseline 34.81 1.20 - - -
ASQA
Improve 20.85¬±1.00 0.68¬±0.03 0.70¬±0.02 0.71¬±0.01 0.70¬±0.01
Generic 18.67¬±0.52 0.61¬±0.01 0.72¬±0.01 0.75¬±0.01 0.74¬±0.00
EIR 16.63¬±0.41 0.51¬±0.02 0.73¬±0.00 0.82¬±0.02 0.77¬±0.01
EIRw/DPO 22.61¬±0.26 0.45¬±0.01 0.64¬±0.00 0.77¬±0.01 0.71¬±0.00
Baseline 22.93 0.82 - - -
ELI5
Improve 10.05¬±0.18 0.36¬±0.02 0.75¬±0.00 0.86¬±0.00 0.80¬±0.00
Generic 6.06¬±0.23 0.22¬±0.01 0.84¬±0.01 0.91¬±0.00 0.87¬±0.00
EIR 3.81¬±0.30 0.13¬±0.01 0.88¬±0.01 0.96¬±0.01 0.92¬±0.01
EIRw/DPO 5.71¬±0.25 0.13¬±0.00 0.83¬±0.00 0.94¬±0.01 0.88¬±0.00
Table4: Resultsofthequalityofanswersrefinedthroughcoarse-grainedandfine-grainedfeedback. Weinclude
twobaselinesusingcoarse-grainedfeedback: IMPROVEandGENERICforallthedatasets. Additionally,weinclude
theresultsforexperthumanfeedbackonourcollectedtestset.
mizationhelpscorrectmajorerrorsintheanswers. Dataset App. Comprehensiveness(‚Üë) Preference(‚Üë)
Weshowfurtherevidenceoftheroleofalignment
Baseline 0.00% 7.84%
inreducinghallucinationsinAppendixE.1. HQ2A
Refined 100% 92.16%
5.3 HumanEvaluation Baseline 82.00% 40.00%
ASQA
Refined 100% 60.00%
To test the comprehensiveness and overall qual-
ityoftheanswersgeneratedusingourrefinement Baseline 38.00% 0.00%
ELI5
approach,wehirethreeannotatorsandperforma Refined 100% 100%
humanevaluationonasubsetof50sampleseach
Table5: Humanevaluationresultsonthecomprehen-
fromHQ2A,ASQA,andELI5datasets.
siveness and preference of refined answers over the
Table5showstheresultsofourhumanevalua-
baselineanswersfromthreedatasets.
tionoftheoriginalandrefinedanswers. Annotators
find the answers produced by our approach com-
prehensive,meaningallthequestionsareanswered
errors (question misconception, factuality, rele-
thoroughlywithoutomittingimportantinformation.
vance, completeness, and references) in LFQA.
However, a comprehensive answer does not nec-
Using our dataset, we analyze the pitfalls of hu-
essarilymeanabetteranswer. Therefore,wealso
man and model long-form answers, identifying
evaluatetheoverallpreferenceofouranswers,in-
issueswithcomprehensivenessandunhelpfulref-
corporatingfactorssuchasfactualityandrelevance
erences. To address these, we propose ERROR-
compared to the baseline answers. We observe
INFORMED REFINEMENT, an approach that uses
thatannotatorssignificantlyprefertherefinedan-
signalsfromourlearnedfeedbackmodeltorefine
swers (‚àº 84%) across all the datasets, indicating
LLMresponses. Ourfeedbackmodeloutperforms
theirfactualcorrectnessandrelevance. Weprovide
baselinefeedbackmodelsandexperthumanfeed-
detailsonthehumanagreementinAppendixE.2.
back in guiding answer refinement and reducing
hallucinations. A human evaluation confirms the
6 Conclusion
effectiveness of our approach, with participants
In this work, we introduce HALUQUESTQA, a finding our refined answers more comprehensive
datasetofexperthumanjudgmentsonfine-grained andpreferabletobaselineoutputs.Limitations References
Despiteprovidinganin-depthanalysisonhalluci- Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda
Askell, AnnaChen, NovaDasSarma, DawnDrain,
nationsinhumanandmodelgeneratedresponses,
Stanislav Fort, Deep Ganguli, Tom Henighan,
ourworkonlyfocussesontheLFQAtask. Thus,
NicholasJoseph,SauravKadavath,JacksonKernion,
we encourage future work to apply our findings TomConerly,SheerElShowk,NelsonElhage,Zac
to different tasks such as summarization, transla- Hatfield-Dodds, Danny Hernandez, Tristan Hume,
ScottJohnston,ShaunaKravec,LianeLovitt,Neel
tion, etc. We study a diverse but limited scope
Nanda, Catherine Olsson, Dario Amodei, Tom B.
oflong-formanswersdrawnfromonlinecommu-
Brown, Jack Clark, Sam McCandlish, Chris Olah,
nityplatforms. Morediversequestionsfromdiffer- Benjamin Mann, and Jared Kaplan. 2022a. Train-
entdomainssuchaseducationorcommercialmay ing a helpful and harmless assistant with rein-
havedifferentissuesandmightbetobeevaluated forcement learning from human feedback. CoRR,
abs/2204.05862.
inadifferentway.
Our trained error detection model shows high Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
correlationwithhumanannotationsbutreliesona AmandaAskell,JacksonKernion,AndyJones,Anna
Chen, Anna Goldie, Azalia Mirhoseini, Cameron
highconsistencyofmodeloutputs. Themodelmay
McKinnon,CarolChen,CatherineOlsson,Christo-
hallucinateiftheconsistencyscoreislow(< 0.80).
pher Olah, Danny Hernandez, Dawn Drain, Deep
Traininglargermodelswithmorehighqualitydata Ganguli,DustinLi,EliTran-Johnson,EthanPerez,
might be an interesting future work to get better Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua
results. Lastly,inourrefinementapproach,weonly Landau,KamalNdousse,KamileLukosiute,Liane
Lovitt, Michael Sellitto, Nelson Elhage, Nicholas
experiment with the intstruction-tuned variant of
Schiefer,Noem√≠Mercado,NovaDasSarma,Robert
LLaMA2. Modelswithbetterorworseinstruction Lasenby, Robin Larson, Sam Ringer, Scott John-
following capabilities may give different results ston,ShaunaKravec,SheerElShowk,StanislavFort,
and improving the refinement process can be a TameraLanham,TimothyTelleen-Lawton,TomCon-
erly,TomHenighan,TristanHume,SamuelR.Bow-
greatfuturedirectiontomitigatehallucinations.
man,ZacHatfield-Dodds,BenMann,DarioAmodei,
NicholasJoseph,SamMcCandlish,TomBrown,and
EthicsandBroaderImpactStatement
Jared Kaplan. 2022b. Constitutional AI: harmless-
nessfromAIfeedback. CoRR,abs/2212.08073.
Theexpertannotationdatacollectionprotocolhas
beendeterminedtobeexemptfromreviewbyan Meghana Moorthy Bhat, Rui Meng, Ye Liu, Yingbo
IRBboard. Allthecollecteddatawillbepublicly Zhou, and Semih Yavuz. 2023. Investigating an-
availableundertheCCBY-SA4.0license. Wehire swerabilityofllmsforlong-formquestionanswering.
CoRR,abs/2309.08210.
annotators on the academic annotation platform
Prolificandgathernosensitiveuserinformationex- XinyunChen,RenatAksitov,UriAlon,JieRen,Kefan
ceptdemographicsandannotatorperformancedata. Xiao,PengchengYin,SushantPrakash,CharlesSut-
ton,XuezhiWang,andDennyZhou.2023. Universal
We examined the collected data and ascertained
self-consistencyforlargelanguagemodelgeneration.
thatitcontainsnotoxicorharmfulcontent.
CoRR,abs/2311.17311.
Acknowledgements
JeremyR.Cole,MichaelJ.Q.Zhang,DanielGillick,Ju-
lianEisenschlos,BhuwanDhingra,andJacobEisen-
ThisresearchworkisfundedbytheGermanFed-
stein.2023. Selectivelyansweringambiguousques-
eral Ministry of Education and Research and the tions. In Proceedings of the 2023 Conference on
HessianMinistryofHigherEducation,Research, Empirical Methods in Natural Language Process-
ing,EMNLP2023,Singapore,December6-10,2023,
ScienceandtheArtswithintheirjointsupportof
pages530‚Äì543.AssociationforComputationalLin-
the National Research Center for Applied Cyber-
guistics.
securityATHENE.YixiaoSongandMohitIyyer
aresupportedbytheawardIIS-2312949fromthe StevenCoyneandKeisukeSakaguchi.2023. Ananaly-
sisofgpt-3‚Äôsperformanceingrammaticalerrorcor-
NationalScienceFoundation(NSF).
rection. CoRR,abs/2303.14342.
We thank Sukannya Purkayastha and Haritz
Puerto for their insightful feedback on the paper AngelaFan,YacineJernite,EthanPerez,DavidGrang-
andManikaArvindAroraforthevaluablefeedback ier, Jason Weston, and Michael Auli. 2019. ELI5:
Long form question answering. In Proceedings of
ontheannotationsetup. Lastly,wearegratefulto
the57thAnnualMeetingoftheAssociationforCom-
our dedicated annotators who helped create the
putationalLinguistics,pages3558‚Äì3567,Florence,
HaluQuestQAdataset. Italy.AssociationforComputationalLinguistics.Markus Freitag, George F. Foster, David Grangier, TevenLeScao,ThibautLavril,ThomasWang,Timo-
VireshRatnakar,QijunTan,andWolfgangMacherey. th√©eLacroix,andWilliamElSayed.2023a. Mistral
2021. Experts, errors, and context: A large-scale 7b. CoRR,abs/2310.06825.
studyofhumanevaluationformachinetranslation.
CoRR,abs/2104.14478. Dongfu Jiang, Yishan Li, Ge Zhang, Wenhao Huang,
Bill Yuchen Lin, and Wenhu Chen. 2023b. Tiger-
JinlanFu,See-KiongNg,ZhengbaoJiang,andPengfei score: Towards building explainable metric for all
Liu.2023. Gptscore: Evaluateasyoudesire. CoRR, textgenerationtasks.
abs/2302.04166.
GangwooKim,SungdongKim,ByeonggukJeon,Joon-
DanGillickandYangLiu.2010. Non-expertevaluation sukPark,andJaewooKang.2023. Treeofclarifica-
ofsummarizationsystemsisrisky. InProceedingsof tions:Answeringambiguousquestionswithretrieval-
theNAACLHLT2010WorkshoponCreatingSpeech augmentedlargelanguagemodels. InProceedingsof
andLanguageDatawithAmazon‚ÄôsMechanicalTurk, the2023ConferenceonEmpiricalMethodsinNatu-
pages148‚Äì151,LosAngeles.AssociationforCom- ralLanguageProcessing,EMNLP2023,Singapore,
putationalLinguistics. December6-10,2023,pages996‚Äì1009.Association
forComputationalLinguistics.
TanyaGoyalandGregDurrett.2020. Evaluatingfactu-
alityingenerationwithdependency-levelentailment.
Jan-Christoph Klie, Michael Bugert, Beto Boullosa,
CoRR,abs/2010.05478.
Richard Eckart de Castilho, and Iryna Gurevych.
2018. TheINCEpTIONplatform: Machine-assisted
TanyaGoyal, JunyiJessyLi, andGregDurrett.2022.
and knowledge-oriented interactive annotation. In
SNaC:Coherenceerrordetectionfornarrativesum-
Proceedingsofthe27thInternationalConferenceon
marization. InProceedingsofthe2022Conference
ComputationalLinguistics: SystemDemonstrations,
onEmpiricalMethodsinNaturalLanguageProcess-
pages5‚Äì9,SantaFe,NewMexico.Associationfor
ing,pages444‚Äì463,AbuDhabi,UnitedArabEmi-
ComputationalLinguistics.
rates.AssociationforComputationalLinguistics.
Kalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.
AndrewF.HayesandKlausKrippendorff.2007. An-
Hurdlestoprogressinlong-formquestionanswering.
sweringthecallforastandardreliabilitymeasurefor
InProceedingsofthe2021ConferenceoftheNorth
codingdata. CommunicationMethodsandMeasures,
AmericanChapteroftheAssociationforComputa-
1(1):77‚Äì89.
tionalLinguistics: HumanLanguageTechnologies,
pages4940‚Äì4957,Online.AssociationforComputa-
AriHoltzman,JanBuys,LiDu,MaxwellForbes,and
tionalLinguistics.
Yejin Choi. 2020. The curious case of neural text
degeneration. In 8th International Conference on
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying
LearningRepresentations,ICLR2020,AddisAbaba,
Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E.
Ethiopia,April26-30,2020.OpenReview.net.
Gonzalez, Hao Zhang, and Ion Stoica. 2023. Effi-
cientmemorymanagementforlargelanguagemodel
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan
servingwithpagedattention. InProceedingsofthe
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and
ACMSIGOPS29thSymposiumonOperatingSystems
WeizhuChen.2022. Lora: Low-rankadaptationof
largelanguagemodels. InTheTenthInternational
Principles.
ConferenceonLearningRepresentations,ICLR2022,
PhilippeLaban,TobiasSchnabel,PaulN.Bennett,and
VirtualEvent,April25-29,2022.OpenReview.net.
MartiA.Hearst.2022. SummaC:Re-visitingNLI-
NeslihanIskender,TimPolzehl,andSebastianM√∂ller. basedmodelsforinconsistencydetectioninsumma-
2020. Bestpracticesforcrowd-basedevaluationof rization. TransactionsoftheAssociationforCompu-
Germansummarization: Comparingcrowd, expert tationalLinguistics,10:163‚Äì177.
andautomaticevaluation. InProceedingsoftheFirst
WorkshoponEvaluationandComparisonofNLPSys- NayeonLee,WeiPing,PengXu,MostofaPatwary,Pas-
tems,pages164‚Äì175,Online.AssociationforCom- caleFung, MohammadShoeybi, andBryanCatan-
putationalLinguistics. zaro.2022. Factualityenhancedlanguagemodelsfor
open-endedtextgeneration. InAdvancesinNeural
Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, InformationProcessingSystems.
DanSu,YanXu,EtsukoIshii,YejinBang,Andrea
Madotto, and Pascale Fung. 2022. Survey of hal- JunyiLi,XiaoxueCheng,WayneXinZhao,Jian-Yun
lucination in natural language generation. CoRR, Nie, and Ji-Rong Wen. 2023. Halueval: A large-
abs/2202.03629. scalehallucination evaluationbenchmark forlarge
languagemodels. CoRR,abs/2305.11747.
AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen-
sch,ChrisBamford,DevendraSinghChaplot,Diego Chin-Yew Lin. 2004. ROUGE: A package for auto-
de Las Casas, Florian Bressand, Gianna Lengyel, maticevaluationofsummaries. InTextSummariza-
Guillaume Lample, Lucile Saulnier, L√©lio Re- tionBranchesOut,pages74‚Äì81,Barcelona,Spain.
nard Lavaud, Marie-Anne Lachaux, Pierre Stock, AssociationforComputationalLinguistics.Yixin Liu, Alexander R. Fabbri, Pengfei Liu, Yilun LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
Zhao, Linyong Nan, Ruilin Han, Simeng Han, Carroll L. Wainwright, Pamela Mishkin, Chong
ShafiqJoty,Chien-ShengWu,CaimingXiong,and Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,
DragomirRadev.2023. Revisitingthegoldstandard: JohnSchulman,JacobHilton,FraserKelton,Luke
Groundingsummarizationevaluationwithrobusthu- Miller,MaddieSimens,AmandaAskell,PeterWelin-
manevaluation. InProceedingsofthe61stAnnual der,PaulF.Christiano,JanLeike,andRyanLowe.
Meeting of the Association for Computational Lin- 2022. Training languagemodelsto followinstruc-
guistics(Volume1:LongPapers),ACL2023,Toronto, tionswithhumanfeedback. InAdvancesinNeural
Canada,July9-14,2023,pages4140‚Äì4170.Associa- InformationProcessingSystems35: AnnualConfer-
tionforComputationalLinguistics. enceonNeuralInformationProcessingSystems2022,
NeurIPS2022,NewOrleans,LA,USA,November28
AmanMadaan, NiketTandon,PrakharGupta,Skyler -December9,2022.
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, KishorePapineni,SalimRoukos,ToddWard,andWei-
Shashank Gupta, Bodhisattwa Prasad Majumder, JingZhu.2002. Bleu: amethodforautomaticevalu-
Katherine Hermann, Sean Welleck, Amir Yazdan- ationofmachinetranslation. InProceedingsofthe
bakhsh, and Peter Clark. 2023. Self-refine: Itera- 40thAnnualMeetingoftheAssociationforCompu-
tiverefinementwithself-feedback. InAdvancesin tational Linguistics, pages 311‚Äì318, Philadelphia,
NeuralInformationProcessingSystems36: Annual Pennsylvania,USA.AssociationforComputational
ConferenceonNeuralInformationProcessingSys- Linguistics.
tems2023, NeurIPS2023, NewOrleans, LA,USA,
RafaelRafailov,ArchitSharma,EricMitchell,Christo-
December10-16,2023.
pherD.Manning,StefanoErmon,andChelseaFinn.
2023. Directpreferenceoptimization:Yourlanguage
Christopher Malon and Xiaodan Zhu. 2024. Self-
model is secretly a reward model. In Advances in
consistentdecodingformorefactualopenresponses.
NeuralInformationProcessingSystems36: Annual
CoRR,abs/2403.00696.
ConferenceonNeuralInformationProcessingSys-
tems2023, NeurIPS2023, NewOrleans, LA,USA,
PotsaweeManakul,AdianLiusie,andMarkJ.F.Gales.
December10-16,2023.
2023. Selfcheckgpt: Zero-resource black-box hal-
lucination detection for generative large language FrancescoSalvi,ManoelHortaRibeiro,RiccardoGal-
models. InProceedingsofthe2023Conferenceon lotti,andRobertWest.2024. Ontheconversational
Empirical Methods in Natural Language Process-
persuasivenessoflargelanguagemodels: Arandom-
ing,EMNLP2023,Singapore,December6-10,2023, izedcontrolledtrial. CoRR,abs/2403.14380.
pages 9004‚Äì9017. Association for Computational
Linguistics. Shokri Z. Selim and M. A. Ismail. 1984. K-means-
typealgorithms: Ageneralizedconvergencetheorem
JacobMenick,MajaTrebacz,VladimirMikulik,John andcharacterizationoflocaloptimality. IEEETrans.
Aslanides, H. Francis Song, Martin J. Chadwick, PatternAnal.Mach.Intell.,6(1):81‚Äì87.
Mia Glaese, Susannah Young, Lucy Campbell-
Gillingham, Geoffrey Irving, and Nat McAleese. IvanStelmakh,YiLuan,BhuwanDhingra,andMing-
2022. Teachinglanguagemodelstosupportanswers Wei Chang. 2022. ASQA: Factoid questions meet
withverifiedquotes. CoRR,abs/2203.11147. long-formanswers. InProceedingsofthe2022Con-
ferenceonEmpiricalMethodsinNaturalLanguage
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Processing, pages 8273‚Äì8288, Abu Dhabi, United
Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer, ArabEmirates.AssociationforComputationalLin-
Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. guistics.
Factscore: Fine-grained atomic evaluation of fac-
LiyanTang, IgorShalyminov, AmyWingmeiWong,
tualprecisioninlongformtextgeneration. CoRR,
Jon Burnsky, Jake W. Vincent, Yu‚Äôan Yang, Siffi
abs/2305.14251.
Singh, Song Feng, Hwanjun Song, Hang Su, Lijia
Sun,YiZhang,SaabMansour,andKathleenMcK-
Dor Muhlgay, Ori Ram, Inbal Magar, Yoav Levine,
eown.2024. Tofueval: Evaluatinghallucinationsof
NirRatner,YonatanBelinkov,OmriAbend,Kevin
llmsontopic-focuseddialoguesummarization.
Leyton-Brown,AmnonShashua,andYoavShoham.
2023. Generatingbenchmarksforfactualityevalua- Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
tionoflanguagemodels. CoRR,abs/2307.06908. Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. 2023. Stan-
ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu, ford alpaca: An instruction-following llama
Long Ouyang, Christina Kim, Christopher Hesse, model. https://github.com/tatsu-lab/
ShantanuJain,VineetKosaraju,WilliamSaunders, stanford_alpaca.
Xu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen
Krueger,KevinButton,MatthewKnight,Benjamin Katherine Tian, Eric Mitchell, Huaxiu Yao, Christo-
Chess,andJohnSchulman.2021. Webgpt: Browser- pher D. Manning, and Chelsea Finn. 2023. Fine-
assisted question-answering with human feedback. tuning language models for factuality. CoRR,
CoRR,abs/2112.09332. abs/2311.08401.HugoTouvron,ThibautLavril,GautierIzacard,Xavier HaoranWu,WenxuanWang,YuxuanWan,Wenxiang
Martinet,Marie-AnneLachaux,Timoth√©eLacroix, Jiao,andMichaelR.Lyu.2023a. Chatgptorgram-
BaptisteRozi√®re,NamanGoyal,EricHambro,Faisal marly? evaluatingchatgptongrammaticalerrorcor-
Azhar,Aur√©lienRodriguez,ArmandJoulin,Edouard rectionbenchmark. CoRR,abs/2303.13648.
Grave,andGuillaumeLample.2023a. Llama: Open
and efficient foundation language models. CoRR, ZeqiuWu,YushiHu,WeijiaShi,NouhaDziri,Alane
abs/2302.13971. Suhr,PrithvirajAmmanabrolu,NoahA.Smith,Mari
Ostendorf, and Hannaneh Hajishirzi. 2023b. Fine-
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- grained human feedback gives better rewards for
bert, Amjad Almahairi, Yasmine Babaei, Nikolay language model training. In Advances in Neural
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti InformationProcessingSystems36: AnnualConfer-
Bhosale,DanBikel,LukasBlecher,CristianCanton- enceonNeuralInformationProcessingSystems2023,
Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu, NeurIPS2023,NewOrleans,LA,USA,December10
JudeFernandes,JeremyFu,WenyinFu,BrianFuller, -16,2023.
CynthiaGao,VedanujGoswami,NamanGoyal,An-
Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol
thonyHartshorn,SagharHosseini,RuiHou,Hakan
Choi. 2023a. A critical evaluation of evaluations
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
forlong-formquestionanswering. InProceedings
IsabelKloumann,ArtemKorenev,PunitSinghKoura,
of the 61st Annual Meeting of the Association for
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
ComputationalLinguistics(Volume1: LongPapers),
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
pages3225‚Äì3245,Toronto,Canada.Associationfor
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
ComputationalLinguistics.
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
stein,RashiRungta,KalyanSaladi,AlanSchelten,
Wenda Xu, Daniel Deutsch, Mara Finkelstein, Juraj
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
Juraska,BiaoZhang,ZhongtaoLiu,WilliamYang
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
Wang,LeiLi,andMarkusFreitag.2024. Llmrefine:
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
Pinpointingandrefininglargelanguagemodelsvia
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
fine-grainedactionablefeedback.
Melanie Kambadur, Sharan Narang, Aur√©lien Ro-
driguez,RobertStojnic,SergeyEdunov,andThomas
WendaXu,DanqingWang,LiangmingPan,Zhenqiao
Scialom. 2023b. Llama 2: Open foundation and
Song, Markus Freitag, William Wang, and Lei Li.
fine-tunedchatmodels. CoRR,abs/2307.09288.
2023b. INSTRUCTSCORE: towards explainable
textgenerationevaluationwithautomaticfeedback.
ShufanWang,FangyuanXu,LaureThompson,Eunsol
InProceedingsofthe2023ConferenceonEmpirical
Choi, andMohitIyyer.2022. Modelingexemplifi-
MethodsinNaturalLanguageProcessing,EMNLP
cationinlong-formquestionansweringviaretrieval.
2023,Singapore,December6-10,2023,pages5967‚Äì
InProceedingsofthe2022ConferenceoftheNorth
5994.AssociationforComputationalLinguistics.
AmericanChapteroftheAssociationforComputa-
tionalLinguistics: HumanLanguageTechnologies, Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu
pages2079‚Äì2092,Seattle,UnitedStates.Association Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and
forComputationalLinguistics. Jiawei Han. 2022. Towards a unified multi-
dimensional evaluator for text generation. In Pro-
Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean ceedingsofthe2022ConferenceonEmpiricalMeth-
O‚ÄôBrien, Ramakanth Pasunuru, Jane Dwivedi-Yu, odsinNaturalLanguageProcessing,EMNLP2022,
OlgaGolovneva,LukeZettlemoyer,MaryamFazel- AbuDhabi,UnitedArabEmirates,December7-11,
Zarandi, and Asli Celikyilmaz. 2023. Shepherd: 2022,pages2023‚Äì2038.AssociationforComputa-
A critic for language model generation. CoRR, tionalLinguistics.
abs/2308.04592.
DanielM.Ziegler,NisanStiennon,JeffreyWu,TomB.
Sean Welleck, Ximing Lu, Peter West, Faeze Brah- Brown,AlecRadford,DarioAmodei,PaulF.Chris-
man, Tianxiao Shen, Daniel Khashabi, and Yejin tiano, and Geoffrey Irving. 2019. Fine-tuning lan-
Choi. 2023. Generating sequences by learning to guage models from human preferences. CoRR,
self-correct. InTheEleventhInternationalConfer- abs/1909.08593.
enceonLearningRepresentations,ICLR2023,Ki-
gali,Rwanda,May1-5,2023.OpenReview.net.
KaWong,PraveenK.Paritosh,andLoraAroyo.2021.
Cross-replicationreliability-anempiricalapproach
tointerpretinginter-raterreliability. InProceedings
of the 59th Annual Meeting of the Association for
ComputationalLinguisticsandthe11thInternational
JointConferenceonNaturalLanguageProcessing,
ACL/IJCNLP2021,(Volume1:LongPapers),Virtual
Event,August1-6,2021,pages7053‚Äì7065.Associa-
tionforComputationalLinguistics.A DataAnalysis f"""Your task is to answer a question
‚Ü©‚Üí by providing a clear and concise
This section presents additional insights on our ‚Ü©‚Üí explanation of a complex concept in
HaluQuestQA(HQ2A)dataset. ‚Ü©‚Üí a way that is accessible for
‚Ü©‚Üí laypeople. The question was posted
‚Ü©‚Üí on the Reddit forum Explain Like
A.1 AnswerLengthDistribution ‚Ü©‚Üí I'm Five (r/explainlikeimfive).
‚Ü©‚Üí Please keep in mind that the
Figure4comparesthelengthdistributionofhuman-
‚Ü©‚Üí question is not literally meant for
writtenandmodel-generatedanswers. Weobserve ‚Ü©‚Üí 5-year-olds, so you should not
that the length of human and model answers is ‚Ü©‚Üí answer the question in a way that
‚Ü©‚Üí you are talking to a child. Your
comparable,resultinginafairevaluation. Across
‚Ü©‚Üí answer should be around
alldomains,thelengthofcollectedanswersranges ‚Ü©‚Üí {human_answer_length} words and
between 50-500words withan averagelength of ‚Ü©‚Üí should break down the concept into
‚Ü©‚Üí understandable parts, providing
100words.
‚Ü©‚Üí relevant examples or analogies
‚Ü©‚Üí where appropriate. You should also
Human Model
‚Ü©‚Üí aim to make your explanation easy
‚Ü©‚Üí to follow, using clear and concise
600
‚Ü©‚Üí language throughout. Your answer
500 ‚Ü©‚Üí should maintain accuracy and
gth 400 ‚Ü©‚Üí clarity. When appropriate, you can
n
nswer le 23 00 00 ‚Ü© ‚Ü©‚Üí
‚Üí
s tt ha ert maw ii nth ido en ae os fen tt he enc ae nss wu em rm .arizing
A
100 Question: {question}
0
Physics Chemistry Biology TechnologyEconomics History Law Answer (around {human_answer_length}
Category ‚Ü©‚Üí words):
"""
Figure4: Answerlengthdistributionofhuman-written
and model-generated answers (H/M) in our expert-
annotateddataset. Listing1: Zero-shotpromptforGPT-4togeneratelong-
formanswerstoquestionsaskedontheELI5subreddit
ontheredditplatform.
A.2 OverallAnswerPreference
InFigure5,weplotthewordfrequencydistribution
human response on Reddit to compare model-
ofthefree-formanswerjustificationsprovidedby
generatedandhuman-writtenanswersfairlyonour
ourexpertannotators. Apartfromourconsidered
definedevaluationcriteria.
evaluationaspects,weobservethattheannotators
alsofindanswersclarity,conciseness,andeaseof
understandinghelpfulindecidingtheoverallbest B.2 FeedbackModel
answer. We encourage future LFQA research to
Weuseexperterrorannotationsforthecomplete-
considertheseaspectsintheirevaluation.
ness aspect from our HQ2A dataset to train our
B Prompts feedbackmodel. InListing2,weshowanexample
promptusedtotrainourfeedbackmodel. Givenan
This section lists the prompts for data collection, instructionandinputquestion-answer,theoutputis
training the error detection model, and refining asentence-levelpredictionofanswercompleteness
answersusingourError-informedapproach. withdetailedjustifications.
B.1 DataCollection
B.3 RefinementModel
We prompt GPT-4 in a zero-shot manner to
generate responses to questions asked on As detailed in ¬ß4.2, the refinement model uses
the Reddit platform, as shown in Listing 1. coarse-grainedfeedback(IMPROVEandGENERIC)
We use the default generation parameters and fine-grained feedback from the learned error
in OpenAI API with temperature=0.1 and detectionmodeltorefineinputanswers. Welistthe
max_tokens=1.5*(human_answer_length). We promptsusedforIMPROVE,GENERICandincorpo-
specifically instruct the model to generate a ratingfine-grainedfeedbackinListing3,Listing4
response of length similar to the corresponding andListing5,respectively.much comprehensive
analogy find well due
part
provides clearly make explanatio lin ke conci im seportant conta di an ts a addresses
used mentions understand information
example
correct less
better relevant points complete
factually concept selected
explains think easier false
good while
prefer gives
Figure5: Distributionofthetop50mostcommonwordsmentionedbyourexpertannotatorsintheiroverallanswer
justifications. Thesizeandcolorofthebubblerepresentthewordfrequencyandimportance,respectively. The
greenandorangecolorsdenotetheimportantevaluatedandnon-evaluatedaspects,respectively,whilebluedepicts
thegenerictermsusedinanswerjustifications.
C MitigatingHallucinationswith modelfor5epochs.
PreferenceOptimization
Datasets&EvaluationMetrics. Weexperiment
Whilelanguagemodelsacquirelargeamountsof
withthreedatasets: HQ2A,ASQA(Stelmakhetal.,
worldknowledgeandstrongreasoningskillsfrom
2022),andELI5(Fanetal.,2019). HQ2Adataset
unsupervised training over massive web corpora,
consists of 698 high-quality long-form question-
aligning them with human expectations is often
answerpairssplitintotrain(80%),dev(10%),and
hard. ModelalignmenttechniqueslikeDPOallow
test (10%) sets. The ASQA dataset consists of
us to directly use preference data to optimize the
6K ambiguous factoid questions with long-form
languagemodelbycastingtheRL-basedobjective
answers synthesized from multiple sources to re-
usedbyexistingRLHFmethodstoanobjectivethat
solvetheambiguities. ELI5consistsof270Klong-
canbedirectlyoptimizedviaasimplebinarycross-
form answers covering general topics from the
entropyloss. Thissimplifiestheprocessofrefining
subreddits"explainlikeimfive","askscience",and
LLMs greatly. The following paragraphs detail
"AskHistorians"ontheRedditplatform.
howweuseDPOtoreduceLLMhallucinations.
Wereportthequalityofthegeneratedlong-form
Implementation details. We model data from answers using TigerScore (Jiang et al., 2023b), a
HQ2Aasapreferencedatasetwhereeveryquestion
trainedreference-freeevaluationmetrictopinpoint
has a chosen and a rejected response selected by mistakes in the LLM-generated text. TigerScore
expertannotatorsbasedonthegivenevaluationcri- detectshallucinationsintheinputtextandassigns
teria. Usingthisdataset,wefine-tunetheLLaMA2- a hallucination scorebased on the severity of the
7B-chat (Touvron et al., 2023b) and Mistral-7B- error detected. Specifically, we use the LLaMA-
instruct-v0.1 (Jiang et al., 2023a) models with 7B trained version of TigerScore, which highly
the DPO algorithm. We use ùëèùëéùë°ùëê‚Ñé_ùë†ùëñùëßùëí = 16, correlateswithhumansforerrordetectioninLFQA
ùë§ùëéùëüùëöùë¢ùëù_ùëüùëéùë°ùëñùëú = 0.1, ùëôùëíùëéùëüùëõùëñùëõùëî_ùëüùëéùë°ùëí = 2ùëí ‚àí 5, tasks (Jiang et al., 2023b). We also measure the
ùëõùë¢ùëö_ùëíùëùùëúùëê‚Ñéùë† = 5,ùëèùëíùë°ùëé = 0.1,andùëöùëéùë•_ùëôùëíùëõùëîùë°‚Ñé = factualcorrectnessofthegeneratedanswersusing
1024fortrainingthemodels. sample-basedconsistencymetrics(Manakuletal.,
Due to compute limitations, we train Llama2- 2023). Following their approach, we zero-shot
13B-chat model on our preference dataset using
promptaLLaMA-13B-chatmodeltocheckifùëñùë°‚Ñé
LoRA (Hu et al., 2022). We use the following sentenceintheoriginalanswerissupportedbythe
training parameters: ùëü = 256, ùëéùëôùëù‚Ñéùëé = 128, sampledanswerùëÜùëõ andreturnascoreùë•ùëõ usingthe
ùëñ
ùëôùëúùëüùëé_ùëëùëüùëúùëùùëúùë¢ùë° = 0.05, ùëôùëíùëéùëüùëõùëñùëõùëî_ùëüùëéùë°ùëí = 5ùëí ‚àí 5, mapping: {"Yes: 1.0","No: 0.0","N/A:0.5"}. The
ùëèùëíùë°ùëé = 0.1, ùëöùëéùë•_ùëôùëíùëõùëîùë°‚Ñé = 1024 and train the finalconsistencyscoreisthencalculatedas:f"""### Instruction: f"""
When given a question and answer Answer the following question:
‚Ü©‚Üí statements, evaluate whether each ‚Ü©‚Üí "{question}"
‚Ü©‚Üí given statement provides sufficient Your answer is: "{answer}".
‚Ü©‚Üí information for answering the The answer is not complete.
‚Ü©‚Üí question. Please improve your answer.
Use the '[Incomplete]' tag to indicate Your improved answer:
‚Ü©‚Üí answer incompleteness, and
‚Ü©‚Üí '[Complete]' tag to indicate """
‚Ü©‚Üí completeness, with reasons.
Please note that the answer can have
‚Ü©‚Üí single, multiple or no incomplete
Listing 4: Zero-shot prompt for LLaMA2-13B-chat
‚Ü©‚Üí statements.
modeltorefinelong-formanswerswithgenericfeed-
### Input: back(GENERIC).
Question: Can anyone explain the
‚Ü©‚Üí differences between copyright and
f"""
‚Ü©‚Üí trademark?
Answer the following question:
Answer: 1. A trademark protects a
‚Ü©‚Üí brand's symbol or logo.
Y‚Ü©‚Üí our"{ aq nu se ws et rio in s:}"
"{answer}".
2. A copyright protects content.
The answer is not complete because:
3. So the ac/dc logo with the lightning
"{reason}".
‚Ü©‚Üí bolt would be trademarked.
Please improve your answer.
4. The music and lyrics to
Your improved answer:
‚Ü©‚Üí thunderstruck would be copyrighted.
5. Edit: eli10 addendum: just to be
"""
‚Ü©‚Üí clear, the content of a copyright
‚Ü©‚Üí can also be an image.
# reasons are given as:
6. So the album cover to
# 1. Reason 1
‚Ü©‚Üí thunderstruck's album, razor's
# 2. Reason 2
‚Ü©‚Üí edge, would be copyrighted because
# ...
‚Ü©‚Üí it is artistic content owned by
‚Ü©‚Üí someone, but doesn't identify ac/dc
‚Ü©‚Üí as a whole.
Listing 5: Zero-shot prompt for LLaMA2-13B-chat
### Response: 1. [Complete] modeltorefinelong-formanswerswitherrorfeedback
2. [Incomplete] Reasons: The answer
fromtheerrordetectionmodel.
‚Ü©‚Üí fails to mention the broader scope
‚Ü©‚Üí of copyright protection, which
‚Ü©‚Üí includes creative works beyond just
‚Ü©‚Üí music and lyrics.
ùëÅ
3 4. . [ [C Co om mp pl le et te e] ] ùëÜ ùëÉùëüùëúùëöùëùùë°(ùëñ) = ùëÅ1 ‚àëÔ∏Å ùë• ùëñùëõ
5. [Complete] ùëõ=1
6. [Complete]
""" D Training,InfrastructureandRuntime
Weuseaserverwith8NVIDIAA100TensorCore
Listing 2: An example prompt used for training GPUs, each with 80GB VRAM, to run all our
LLaMA2-13Bmodelforerrorfeedback.
experiments. Each experiment required, at most,
two A100 GPUs. Fine-tuning the LLaMA2-13B
f""" feedback model took 6 hours on 2 A100 GPUs
Answer the following question:
using our HQ2A dataset. LoRA fine-tuning of
‚Ü©‚Üí "{question}"
Your answer is: "{answer}". the LLaMA2-13B-chat refinement model took 2
Please improve your answer. hours on a single A100 GPU using the prefer-
Your improved answer:
encedatafrom HQ2A.Refininganswerswithour
""" ERROR-INFORMED REFINEMENTapproachtook
0.5, 3, and 23 hours for the HQ2A, ASQA, and
ELI5datasets,respectively,onasingleA100GPU.
Listing 3: Zero-shot prompt for LLaMA2-13B-chat
TheevaluationoftherefinedanswerswithTiger-
model to refine long-form answers without feedback
Score(LLaMA-7B)utilizedtheVLLMinference
fromtheerrordetectionmodel(IMPROVE).
library(Kwonetal.,2023)andtookapproximately
1,15,and30minutesforHQ2A,ASQA,andELI5
datasets,respectively,onasingleA100GPU.Tigerscore
Dataset SelfCheck
InstructModel
(#samples) Consistency(‚Üì)
%Hallucinatedsamples(‚Üì) Hallucinationscore(‚Üì)
LLaMA2-7B 18.57¬±0.00 0.60¬±0.00 0.166¬±0.014
LLaMA2-7B+DPO 15.71¬±0.00 0.66¬±0.00 0.162¬±0.015
HQ2A(70)
Mistral-7B 20.00¬±0.00 0.57¬±0.00 0.266¬±0.011
Mistral-7B+DPO 17.14¬±0.00 0.54¬±0.00 0.285¬±0.011
LLaMA2-7B 26.58¬±1.49 0.86¬±0.06 0.187¬±0.014
LLaMA2-7B+DPO 28.41¬±1.06 0.89¬±0.02 0.178¬±0.006
ASQA(948)
Mistral-7B 62.09¬±0.35 2.08¬±0.01 0.578¬±0.003
Mistral-7B+DPO 60.80¬±0.56 2.03¬±0.01 0.555¬±0.008
LLaMA2-7B 9.93¬±1.05 0.32¬±0.04 0.133¬±0.001
ELI5_GENERAL LLaMA2-7B+DPO 9.33¬±0.66 0.29¬±0.03 0.130¬±0.004
(1000) Mistral-7B 29.97¬±0.97 0.90¬±0.04 0.327¬±0.003
Mistral-7B+DPO 22.77¬±1.03 0.72¬±0.03 0.319¬±0.011
LLaMA2-7B 9.47¬±0.47 0.31¬±0.02 0.137¬±0.003
ELI5_SCIENCE LLaMA2-7B+DPO 9.47¬±0.76 0.30¬±0.00 0.139¬±0.004
(1000) Mistral-7B 34.10¬±0.94 1.07¬±0.02 0.320¬±0.004
Mistral-7B+DPO 29.03¬±1.51 0.95¬±0.04 0.297¬±0.010
LLaMA2-7B 9.63¬±0.59 0.30¬±0.02 0.188¬±0.005
ELI5_HISTORY LLaMA2-7B+DPO 7.60¬±0.08 0.22¬±0.01 0.189¬±0.005
(1000) Mistral-7B 26.23¬±0.38 0.79¬±0.02 0.363¬±0.016
Mistral-7B+DPO 22.17¬±1.31 0.69¬±0.04 0.345¬±0.013
Table 6: Results of aligning LLMs with DPO using our collected answer preference data. We measure the
hallucinationsusingTigerscoreandtheconsistencyofmodeloutputsusingSelfCheckGPT.
E AdditionalResults
Dataset Comprehensiveness(‚Üë) Preference(‚Üë)
HQ2A 0.70 0.31
E.1 AligningLLMs
ASQA 0.86 0.02
Table6showstheresultsfortraininglanguagemod- ELI5 0.92 0.61
elswithDPOusingourcollectedpreferenceanno-
Average 0.83 0.31
tations. Ourpreference-tunedmodelsoutperform
thestrongbaselinemodelsandreducehallucinated Table7:Agreementofannotatorsonthecomprehensive-
generations in all the evaluation settings except nessandpreferenceofrefinedanswersoverthebaseline
theLLaMAmodelontheASQAdataset. Wehy- answersfromthreedatasets.
pothesizethatthisisduetotheambiguousnature
of questions in the ASQA dataset that can have
Error-informedfeedbackapproach. InTable7,we
multiplecorrectanswers.
present the agreement of our annotators on two
Wealsoobservethatthemodelsbecomemore
evaluationmetrics: comprehensivenessandoverall
robustandgeneratemoreconsistentresponsesafter
answerpreference. Theannotatorsstronglyagree
preference-tuning. TheonlyexceptionistheMis-
that the refined answers are comprehensive, i.e.,
tralmodelonourheld-outtestset,whichhaslower
the answer contains all the required information
responseconsistency. Webelievethisislikelydue
as asked by the question. For the overall answer
totheconservativenatureofDPO-trainedmodels
preference compared to the baseline, we observe
wherein, during sampling, it can refrain from an-
weakagreementbetweenannotators,primarilydue
sweringaquestioninsomecasesandnotinothers,
tothelowagreementvalueontheASQAdataset.
leadingtoalowerconsistencyscore.
Wehypothesizethattheannotatorsstruggletoalign
onASQAduetotheambiguousnatureoftheques-
E.2 HumanEvaluation
tionsinthisdataset,whichmayhavemultiplecor-
Thissectionpresentsadditionaldetailsofourhu- rectanswers,andchoosingbetweentwoanswers
man evaluation of the answers refined with our isdifficult.F AnnotationGuidelines
Wehavepreviouslydescribedourdatacollectionsetupin¬ß3.3. Thissectionprovidesadditionaldetailson
theannotationinterface,detailedtaskinstructions,andannotationprocedure.
F.1 AnnotationInterface
InFigure6,weshowtheinterfaceforcollectingexperterrorannotationsonLFQAanswers. Forevery
question, experts see a human-written and model-generated answer (randomized order). Our expert
annotators must select the evaluation layer (top right) and highlight the error span in the question or
answer, giving justifications with web references, wherever applicable. After annotating for all the
evaluationcriteria,expertsjudgethebetteranswerandmarkitintheleftpane,givingreasonsfortheir
preference.
Figure6: ScreenshotofannotationinterfaceforcollectingexperterrorannotationsonLFQAanswers.
F.2 TaskInstructions
We provide experts with detailed task instructions for evaluating answers according to the defined
evaluation criteria. We go through every evaluation aspect in depth, defining it and giving annotation
examplesforclarification,asdetailedinthenextparagraphs.
1)QuestionMisconception. Youshouldselectaspanoftextinthequestionthatcontainsamisconcep-
tionorfalseassumption. Thequestionisrepeatedtwice. Youonlyneedtoselectthespaninonerepetition.
Ifyouselectsuchspans,wewouldlikeyoutoindicateinyourreason(obligatorily):
‚Ä¢ whethertheanswersrejectorcorrectthemisconception/falseassumption,
‚Ä¢ if no answer rejects/corrects it, please explain in your reason why that is a misconception/false
assumption(preferablywithreferences).
Example:
Question: Why is it so important for humans to have a balanced nutrition but not for animals? Most
animalshaveafairlysimplediet,carnivoreseatonlymeattheirwholelife,cowseatexclusivelygrassetc.Sowhyarehumanbodiessopickyandneedabalanceofprotein,fat,carbsetcfromdifferentsourcesto
performwell?
2)Factuality. Youshouldselectaspanoftextintheanswersthatisfactuallyincorrect. Ifyouselect
suchspans,wewouldlikeyouto(obligatorily):
‚Ä¢ preferablygivereferences(e.g.,crediblewebsites,academicpapers,orbooks)thatshowthecontent
isfactuallywrong,or
‚Ä¢ giveexamplesthatshowthecontentisfactuallywrong.
Example:
Question: Why is it so important for humans to have a balanced nutrition but not for animals? Most
animalshaveafairlysimplediet,carnivoreseatonlymeattheirwholelife,cowseatexclusivelygrassetc.
Sowhyarehumanbodiessopickyandneedabalanceofprotein,fat,carbsetcfromdifferentsourcesto
performwell?
Answer:Animalsgenerallyhaveasimplerdietthanhumans.Forexample,carnivoresonlyeatmeat,while
cowsonlyeatgrass...
Reason:Thisisareductionistviewofanimalnutritionasitdoesn‚Äôtconsiderhowanimalshaveevolved
andthecomplexitiesofthefoodchain. Forexample,lionsarecarnivoresthatonlyeatmeatbuttheyeat
thestomachofzebrasthatcontaingrass/plantsandareabletodigestit.
3)Relevance. Youshouldselectaspanoftextintheanswersthatisirrelevanttoansweringthequestion.
Removingsuchcontentshouldnotaffecttheoverallqualityofananswer. Ifyouselectsuchspans,we
wouldlikeyouto(obligatorily):
‚Ä¢ explainwhytheselectedtextisnotrelevanttoansweringthequestion.
Example:
Question:Whatishappeningwhenyougetmigrainesthatcauseyoutolosepartofyourvisionforashort
time?
Answer:Mywifegetsthese.Anocularmigraineisaconditionwherethebloodvesselsintheopticnerve
atthebackofyoureyetightenandswell,resultinginvariousvisualdistortions. Whileclassicmigraines
usuallyresultinintenseheadaches,sensitivitytolightandloudsounds,ornausea,ocularmigrainesare
notnecessarilypainful.
Reason:Answercontainsirrelevantinformation(writer‚Äôswifehavingthem,migrainemaynotbepainful).
Theperson‚Äôswife‚Äôspersonalhealthconditiondoesn‚Äôtprovideusefulinformationtothequestion,andthe
questiondoesn‚Äôtaskaboutwhetherocularmigrainesarepainfulornot.
4)Completeness. Youshould: (a)selectaspanoftextintheanswerthatdoesnotofferenoughdetails,
or(b)selectthelabelAnswer1orAnswer2ifsomerelevantinformationthatshouldbeincludedinthe
answerismissing. Ifyouselectsuchspans,wewouldlikeyouto(obligatorily):
‚Ä¢ offerthedetailsorrelevantinformationthatyouthinkshouldbeincluded. Referencesfromcredible
sourcesisencouraged.
Example:
Question:Whydoesalcoholmakeyourthroatorstomachfeelwarmwhendrinking?
Answer:Thereareafewreasonswhyalcoholmightmakeyourthroatorstomachfeelwarm. first,alcohol
isasolvent,whichmeansitcandissolveothersubstances. whenyoudrinkalcohol,itcanbreakdownthe
barrierbetweenyourstomachandyourthroat,causingaburningsensation. second,alcoholcanirritate
theliningofyourstomach,causinginflammation. thiscanalsoleadtoaburningfeeling. finally,when
youdrinkalcohol,yourbloodvesselsdilate,orwiden. thisincreasedbloodflowcancauseafeelingof
warmth.
Reason:Theanswershouldgivemoredetailsonwhatthebarrieris. [Offerrelevantinformationbasedon
yourknowledge,forexample,whatthebarrieris.]5) References/example/analogy. You should select a span of text in the answer that is exam-
ples/analogies or a reference to a external credible source (e.g., websites, papers, or books). If you
selectsuchspans,wewouldlikeyouto(obligatorily):
‚Ä¢ indicatewhethertheexample/analogy/crediblesourceishelpful,and
‚Ä¢ giveareasononwhyitishelpful/nothelpful.
Example:
Question:Howdoesaspeakervibrateatmultiplefrequenciessimultaneouslytodeliversoundstoour
ears? I‚ÄôlltryandexplainwhatImean.. sayasongisplaying-howdothevocals,drums,guitarsetcwhich
allhavedifferentfrequencies(akaaplateismovingback&forthataratetodeliverthesesoundwaves)
playatthesametime? Surelytheplatecan‚Äôtvibratetodelivertwoormorefrequenciesatonce?
Answer:Imagineanoceanwithaconsistentwave.Itflowsupanddown,withequaldistancebetween
the two waves at any time. Now imagine I push a larger, shorter wave into this ocean. The two waves
willcollide,resultinginsomenewwavepattern.Thisnewwavepatternisacombinationofthosetwo
waves.Speakersworksimilarly. IfIcombinetwosoundwaves,Igetanewcombinationwavethatsounds
different.
Reason:Iliketheanalogywiththeoceanwaves,andduetohowvisualtheexplanationisitiseasierto
understandinmyopinion.
AnswerPreference. Basedonthefivecriteriayouhavewentthrough,pleaseweightallgoodandbad
points of each answer and choose the one that is better overall. Writing a motivation to support your
choiceisobligatory.
1. Whenwritingyourmotivation,pleaserefertothefivelayersofevaluation.
2. Ifthereareotheraspectsyouusedtomakeyourdecisionbutarenotintheevaluationlayers,please
mentiontheminthereason
3. Ifyouhavequotationsfromtheanswers,pleaseindicatewhichanswerarethequotationsfrom.
4. Herearesomeaspectsforyoutoconsider(notobligatorily):
‚Ä¢ Niceexample/analogy,tothepoint,generic,concise,informative,useful,wellstructured,easy
tofollow...
OverallRequirement. Theoveralltaskrequirementsaresummarizedbelow. Pleasereadthemcarefully
toavoidredoingthetask.
1. Youhavetohighlightspansinbothquestionanswersfortheseaspectsandgivereasonwhyyou
highlightaspanforanaspect.
2. Markasmanyspansasnecessary.
3. Pleasebeobjectiveinyourreasonsandavoidusingphraseslike‚ÄúIbelieve‚Äùor‚ÄúIthink‚Äù.
4. Yourreasonsshouldbeinformativeandsuccinct.
5. Pleaseusedeclarativesentencesandavoidusingquestionsinyourreasons.
6. ProductslikeChatGPTorBARDareabsolutelynotallowed.
F.3 AnnotationProcedure
Theexpertannotatorsspendaround15-20minutesperquestion,highlightingthedemandingnatureof
thistask. Weaccordinglypay¬£10/hourandprovideabonusof¬£10forgood-qualityannotations,resulting
inatotalcostof¬£3000tocollectexpertjudgmentsfor698questions. Theannotatorsunderstandthatwe
willusetheirannotateddataforresearchpurposes. Weshowascreenshotofanexpertannotatedanswer
inFigure7.Figure7: ScreenshotofanexpertannotatedanswerontheINCEpTIONplatform.