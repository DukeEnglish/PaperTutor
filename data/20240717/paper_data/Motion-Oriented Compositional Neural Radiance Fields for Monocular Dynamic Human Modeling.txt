Motion-Oriented Compositional Neural Radiance
Fields for Monocular Dynamic Human Modeling
Jaehyeok Kim1, Dongyoon Wee2, and Dan Xu1(cid:66)
1 HKUST, Hong Kong 2 NAVER Cloud Corp., South Korea
jkimbf@cse.ust.hk, dongyoon.wee@navercorp.com, danxu@cse.ust.hk
Abstract. ThispaperintroducesMotion-orientedCompositionalNeu-
ral Radiance Fields (MoCo-NeRF), a framework designed to perform
free-viewpointrenderingofmonocularhumanvideosvianovelnon-rigid
motion modeling approach. In the context of dynamic clothed humans,
complexclothdynamicsgeneratenon-rigidmotionsthatareintrinsically
distinct from skeletal articulations and critically important for the ren-
dering quality. The conventional approach models non-rigid motions as
spatial(3D)deviationsinadditiontoskeletaltransformations.However,
itiseithertime-consumingorchallengingtoachieveoptimalqualitydue
to its high learning complexity without a direct supervision. To target
this problem, we propose a novel approach of modeling non-rigid mo-
tions as radiance residual fields to benefit from more direct color super-
vision in the rendering and utilize the rigid radiance fields as a prior
to reduce the complexity of the learning process. Our approach utilizes
a single multiresolution hash encoding (MHE) to concurrently learn the
canonicalT-poserepresentationfromrigidskeletalmotionsandtheradi-
anceresidualfieldfornon-rigidmotions.Additionally,tofurtherimprove
bothtrainingefficiencyandusability,weextendMoCo-NeRFtosupport
simultaneous training of multiple subjects within a single framework,
thankstooureffectivedesignformodelingnon-rigidmotions.Thisscala-
bilityisachievedthroughtheintegrationofaglobalMHEandlearnable
identity codes in addition to multiple local MHEs. We present exten-
sive results on ZJU-MoCap and MonoCap, clearly demonstrating state-
of-the-art performance in both single- and multi-subject settings. The
code and model will be made publicly available at the project page:
https://stevejaehyeok.github.io/publications/moco-nerf.
Keywords: Dynamichumanmodelingfrommonocularvideos·Neural
Radiance Fields · Novel view synthesis of humans
1 Introduction
Novel view synthesis of monocular human video with photo-realistic quality is a
challengingproblem,asitinvolvescomplexarticulatedbodymotionsandneces-
sitatessophisticatedmodelingofnon-skeletalornon-rigidmotions(e.g.,wrinkles
(cid:66) Corresponding author.
4202
luJ
61
]VC.sc[
1v26911.7042:viXra2 J. Kim et al.
a) Motion-Oriented Compositional NeRF
Skeletal
Transformations Neural Renderings Radiance Residual
from Monocular via Rigid Branch Compositions
Video
Identical Pose-adaptive
mean motions non-rigid motions
Intermediate Novel View Final Novel View
Canonical T-pose Rendering Renderings Ground Truth
b) Comparisons 6 mins 1 hour2 hours 1.5 mins1 hour 2 hours 6 mins1 hour 2 hours 6 mins 2 hours39 hours
Input Frame Ground Truth MoCo-NeRF (Ours) GauHuman Instant-NVR HumanNeRF
Fig.1:a)Weintroduceamotion-orientedcompositionalNeRFforphoto-realisticmod-
eling of dynamic humans from monocular videos. Our approach innovatively employs
radiancecompositionstocapturepose-adaptivenon-rigidmotions,overcomingthelim-
itationsofskeletaltransformationsthattypicallyyieldanaverageofobserveddeforma-
tions(meanmotions).b)TheproposedMoCo-NeRFachievesstate-of-the-artrendering
qualityandnoteworthyefficiencyinnovelviewsynthesis,comparedtoleadingcompeti-
tors [14,21,73]. The bold denotes the proposed training duration of each comparison
model. MoCo-NeRF uniquely captures coherent non-rigid motions, like T-shirt wrin-
kles relative to body pose, from entirely new viewpoints. Moreover, MoCo-NeRF sig-
nificantlysurpassestheefficiencyofanotherphoto-realisticmethod,HumanNeRF[73].
of clothes) from unseen viewpoints. In particular, non-rigid motion modeling is
essentialforachievingphoto-realismandcoherenceinfree-viewpointrenderings.
Aprevalentsolutionadoptedbyrecentmethods[14,21,73]modelsthenon-rigid
motions as pose-dependent 3D spatial offsets of canonical point samples. How-
ever,theoffset-basedstrategiesleadtoeitherextensivetrainingtimeorirrelevant
modeling of non-rigid motions due to its high learning complexity, incurred by
the following reasons. First, a single spatial offset spans an unbounded range
andisuniqueforevery3Dpointacrossallinputframesandbodyposes,thereby
escalating computational complexity. Second, the learning objective of spatial
offsetsisnotdirectlyalignedwiththeconventionalRGB-basedsupervision,fur-
ther complicating the modeling process.
To target the issues above-discussed, in this paper, we propose Motion-
orientedCompositionalNeuralRadianceFields(MoCo-NeRF),anewframework
that effectively models non-rigid motions at a radiance-level through a unique
design of residual radiance field learning forfree-viewpointrenderingsofmonoc-
ular human videos. Specifically, our proposed framework decomposes a singleMoCo-NeRF 3
MHE [44] into two branches: a rigid branch that learns a canonical T-pose radi-
ance field solely via rigid skeletal transformations, and a non-rigid branch that
optimizes pose-driven radiance residuals for capturing appearance discrepancies
between renderings of the rigid branch and the ground-truth images, which are
typically caused by non-rigid motions. The proposed approach circumvents the
complexities associated with learning unbounded spatial offsets by reducing the
prediction range to the radiance space with the rigid radiance field as a prior.
It further establishes a more direct supervision from the ground-truth pixel col-
ors for optimizing the rigid and non-rigid radiance fields by embedding them
together in the rendering process, which can further enhance the learning effi-
ciency.Besides,wepresentacross-attentionmoduletoqueryaposeconditioned
implicit feature from a learnable base code. This process allows our framework
to extract more discriminative implicit features tailored to each particular pose,
helping our non-rigid branch to learn coherent pose-driven radiance residuals.
In addition, thanks to our effective design for modeling non-rigid motions,
the proposed MoCo-NeRF architecture can be flexibly extended to support uni-
fied training across multiple subjects while maintaining photo-realistic quality,
without a significant increase in training time. For this architectural extension,
weintroducemultiplelocalMHEsandaglobalMHE,andalsoextendtheafore-
mentioned base code to a set of identity (ID) codes as a learnable codebook.
The multiple local MHEs are designed for modeling multiple subjects and the
global MHE targets the learning of generic and universal body representations.
Furthermore, subject-specific learnable ID codes that are pivotal for generating
pose-embedded implicit features enable effective sharing of the MLP decoder in
therigidbranchbyallsubjects,enhancingthemodel’sefficiencyandscalability.
MoCo-NeRFachievesstate-of-the-artperformanceonfree-viewpointrender-
ingofmonocular humanvideosinbothsingle-andmulti-subjectsettings,show-
ing clear improvements compared to the prior works. We present our extensive
results on ZJU-MoCap [52] and MonoCap [17,18,50] to demonstrate the effec-
tiveness of the proposed MoCo-NeRF in learning photo-realistic representation
and modeling complex non-rigid motions from monocular videos.
In summary, the contribution of our work is threefold:
– We propose a novel motion-oriented compositional NeRF approach, MoCo-
NeRF, which concurrently learns canonical rigid radiance fields and pose-
dependent radiance residual fields, effectively handling both rigid and non-
rigidmotions.Wealsointroduceapose-embeddedimplicitfeature,computed
by cross-attention between a body pose and a designed learnable base code,
to implicitly enhance pose-dependent radiance residual learning.
– The proposed MoCo-NeRF is the first work that can be readily extended to
enable unified training of multiple subjects without significantly increasing
the GPU memory demands. We achieve this through our local MHEs, a
global MHE, and the extended identity codebook, which enable subject-
discriminative representation learning and effective module sharing.
– TheproposedMoCo-NeRFachievesstate-of-the-artperformanceunderboth
single- and multi-subject training settings with a significant efficiency im-4 J. Kim et al.
provement compared to the best-performing methods. MoCo-NeRF jointly
learns all 6 subjects of ZJU-MoCap on one RTX 3090 GPU within 2 hours.
2 Related Work
We review closely related work from three perspectives: deformable neural ren-
dering,neuralhumanrendering,andmonocularphoto-realistichumanrendering.
Deformable neural rendering.NeRF[42]leveragesanMLPtolearnastatic
3Drepresentationofascenefromadensesetofimagesfromdiverseviewpoints.
Foritsapplicationinreal-lifescenarios,priorworkshaveenhancedNeRFinvari-
ousdirectionssuchasimprovingefficiencies[4,13,19,36,44,54,59,82]orrelieving
strongrelianceonimpracticalsettingslikedenseinputviews[5,24,38,70,83].In
addition, there are more branches of work such as large-scale modeling [55,66,
68,77,79], improving rendering quality [2,3], reconstruction quality [46,69,81]
andmore.However,theyarerestrictedtostaticsceneswhilethemajorityofthe
real-world objects are dynamic.
Recent works including [11,12,35,47,48,53,67,76] have broadened the mod-
eling capabilities of NeRF to dynamic scenes with small and simple move-
ments/deformations.However,theyoftenfailinhandlingmoredynamicorcom-
plex motions like human body articulations. In contrast, our approach enables
learning of human-articulated motions from monocular videos, and performing
free-viewpoint rendering of a human performer at any time frame of the videos.
Neuralrenderingofhumans.Manyearly-stagepriorworksforneuralhuman
renderingsutilizetraditionaltechniqueslikemulti-viewstereo[16,60,61,75]orto
buildsophisticatedsystemsforcapturinghumanmotions[8,10,40,65].However,
constructing a dense set of training views and building those capturing systems
areexpensive,makingthemunsuitableforreal-lifesituations.[1,6,57,58]address
3D human reconstruction by utilizing ground truth human scans, relieving the
aforementioned requirements. [9,20,45] further improves it by enabling human
reconstruction from 2D image collections. In recent days, there have been many
workslearninganimplicithumanrepresentationfornovelposesynthesis[22,23,
25,27,34,50,56,64,71,87–89] or for a novel-view synthesis [27,28,31,37,51,52,
64,73,74,78,85,86]. There also have been several generalizable works [41,43,62]
for a novel human subject with pre-training on multi-view dataset and a few-
shot fine-tuning. [7,80] focus on improving rendering efficiency to achieve real-
time free-view rendering of dynamic humans. Furthermore, [14,26,28,49] adopt
explicit hash encoding [44] in order to boost the training efficiency for human
representation modeling. In contrast to our problem setup, most of those works
require multi-view videos or do not model the non-rigid motions.
Monocular photo-realistic human rendering. HumanNeRF [73] presents
a state-of-the-art performance by optimizing motion fields and canonical T-
pose representation of a human performer. However, its implicit modeling of
pose-dependent non-rigid motions requires tens of hours to converge. Instant-
NVR [14] achieves efficient training with part-based representation and 2D mo-
tionparameterizationforlearningnon-rigidmotionsas3Doffsets.NotethattheMoCo-NeRF 5
Observation Space Canonical Space
Rigid Radiance Fields
Rigid RGB
Skeletal
Transform MLP
3D Body
Pose C : Concatenation
Non-rigid Radiance Residuals
Final RGB
Multiresolution
Learnable IP mos pe l- iE cim t Fb ee ad td ue rd e Hash Encoding C MLP Base Code Generation
Fig.2: OverviewoftheproposedframeworkMoCo-NeRFforfree-viewhumanrender-
ingfromamonocularvideo.Withoutestimatingageometricaloffsetofeachcontinuous
canonical point for each body pose, our framework is able to handle all deformations
via its radiance-compositional approach with a single MHE [44] and achieve state-of-
the-artperformance.Thepose-embeddedimplicitfeaturefurtherenhancesthelearning
of non-rigid radiance residuals by enabling pose-distinctive representation learning.
human parametric model (SMPL) [39], an extra requirement for training, plays
a significant role in their framework. In addition, a concurrent work GauHu-
man [21], which also employs spatial offset approach with Gaussian Splatting
(GS)[29],furtherenhancestheefficiencyandperformanceupon[14]byleverag-
ing SMPL for the density control of GS. Nevertheless, [14,21] struggle to learn
complex non-rigid motions, even with an extended training duration as shown
in Fig. 1. In this paper, instead of directly learning spatial offsets like these
existing works, we propose a novel approach that effectively models non-rigid
body motions by learning motion-oriented compositional radiance fields condi-
tionedbypose-embeddedimplicitfeatures.Ourapproachclearlyoutperformsthe
best-performing competitors HumanNeRF [73], Instant-NVR [14], and GauHu-
man [21] and can further extend to support joint training of multiple subjects.
3 The Proposed MoCo-NeRF Method
TheproposedMoCo-NeRFaimstoeffectivelylearnhigh-fidelityrepresentations
forfree-viewpointrenderingofdynamichumansubjectsinmonocularvideos.We
introduce our novel radiance compositional NeRF approach based on different
motiontypes:rigidandnon-rigidmotions,asillustratedinFig.2.Thecomposi-
tionofcanonicalradianceandtheper-samplepose-dependentradianceresiduals
(Sec. 3.2) facilitates photo-realistic quality of rendering with fine-grained mod-
eling of non-rigid motions, rather than leveraging the conventional spatial 3D
offset learning approach. We further improve the quality of non-rigid radiance
residuals by introducing a pose-embedded implicit features (Sec. 3.2) that is
jointly optimized with the non-rigid radiance branch. In Sec. 3.3, thanks to our
effectivedesignofnon-rigidmotionmodeling,itenablesMoCo-NeRFtobeflex-
ibly extended for unified training of multiple subjects with a high efficiency.
Linear
+ ReLU6 J. Kim et al.
3.1 Preliminary: Skeletal Transformations
The skeletal motion learns an inverse formulation of the linear blend skin-
ning [32], which transforms canonical points to different body poses by skinning
weights. Weng et al. [73] reformulate the linear blend skinning to transform a
point x in the observation space to a canonical point x :
c
x
=(cid:88)K
wo(R x+t ),wo =
w kc(R kx+t k)
, (1)
c k k k k (cid:80)K wc(R x+t )
k=1 j=1 j j j
where wo and wc respectively represent observation and canonical skinning
k k
weights of bone k, and R and t are the rotation and translation of bone k,
k k
respectively.GivenEq.1,following[73],weoptimizeanexplicitvolumedecoder
to optimize {wc}K where K is the total number of joints.
k k=1
3.2 Motion-Oriented Compositional Neural Radiance Fields
Rigid neural branch for learning canonical radiance field. The rigid
branch of MoCo-NeRF is specifically designed to learn the canonical T-pose
representation of the target subject using only rigid body movements. Our ap-
proach is different from but simpler than the prevalent spatial offset learning
approach, which incorporates complex modeling of non-rigid motions as ∆x .
c
Our rigid branch instead directly predicts the color and density of the canonical
point x transformed by inverse linear blend skinning (Sec. 3.1), as depicted in
c
Fig. 2. First, we query multiresolution volumetric features f of the canonical
xc
point x from the MHE ψ as follows:
c hash
f =ψ (x ). (2)
xc hash c
Then, we slice the first half (H dimensions) of f at each level of resolution to
xc
compute f:H and predicts the radiance c and density σ of the rigid canonical
xc
point x with an MLP decoder M as follows:
c rigid
c, σ =M (f:H). (3)
rigid xc
Non-rigid neural branch for learning radiance residual field. While the
canonicalrepresentationdoesnotexplicitlymodelpose-dependentnon-rigidmo-
tions,itstillcapturesanaverageofsuchmotions(meanmotions)aftertraining.
However, the mean motions cannot effectively reflect the complex and realistic
non-rigid motions under different body poses. It is only capable of rendering
identical deformations across different poses, as can be observed in Fig. 1. To
address this effectively, the non-rigid branch of MoCo-NeRF is designed to con-
currently optimize for a radiance residual field. This models the appearance
discrepancies between the renderings of the rigid branch and the ground-truth
colors, typically caused by the complex non-rigid motions or deformations. We
additionally condition the radiance residual predictions with the canonical radi-
ance representation of the rigid branch as prior information. This enables moreMoCo-NeRF 7
C
Indexing with
subject ID Global MHE
Softmax
C
Local MHE
Pose-Embedded
Implicit Feature
L Be aa sr en Cab ol de e Generation
Fig.3: Illustration of the proposed Fig.4: Illustration of extended architecture of
pose-embeddedimplicitfeaturegener- MoCo-NeRFforthemulti-subjectunifiedtrain-
ation. We employ cross-attention to ing(Sec.3.3).Majorcomponentsconsistofthe
modulate the single learnable base globalMHE,setoflocalMHEs,andthedictio-
codetopose-adaptivefeatures. naryoflearnablebasecodesasIDcodes.
coherentmodelingofnon-rigidmotions,asthenon-rigidandrigidradiancefields
are highly correlated.
Specifically, unlike the rigid branch, the queried feature f is not sliced, en-
xc
abling the non-rigid branch to learn the radiance residuals with reference to the
canonical representation. Instead, we freeze the first half of f that represents
xc
the canonical radiance field, and only optimize the other half to learn the ra-
diance residuals for non-rigid motion modeling. We denote this partially frozen
feature as f∗ . We observe that providing frozen f:H to the non-rigid radiance
xc xc
branchenablesmoreeffectiveandcoherentresiduallearning.Inaddition,wealso
conditiontheradianceresidualfieldpredictionwithourpose-embeddedimplicit
feature P , which is achieved by concatenating P to the first intermediate
nr nr
outputoftheMLPdecoderM ,asillustratedinFig.2.ThegenerationofP
nr nr
is explained in the last subsection of Sec. 3.2. Eventually, the color and density
residualsarepredictedbyM forthefinalcompositionalradiancecomputation:
nr
∆c, ∆σ =M (f∗ ;P ), (4)
nr xc nr
Compositional volume rendering. Following [42], we adopt stratified sam-
pling and volume rendering to predict the RGB color of each ray. For final
rendering,wequeryM samplesperrayrandperformapproximatedintegration
to obtain final RGB C from (c+∆c,σ+∆σ) as follows:
final
C (r)=(cid:80)M T (1−exp(−(σ +∆σ )δ ))(c +∆c ). (5)
final m=1 m m m m m m
whereδ istheadjacentdistancefromthem-thtothem+1-thsampleandT =
m m
exp(−(cid:80)m−1σ
δ ) as the transmittance. In addition, we compute C in the
n=1 n n rigid
same way from (c,σ) for an additional loss term that encourages a decomposed
learning of the canonical radiance field and the radiance residuals (Sec. 3.4).
Learnable Pose-Embedded Implicit Feature.Weintroducehigh-frequency
modulatedimplicitfeaturescomputedfromalearnablebasecodeanda3Dbody
Linear
+ ReLU8 J. Kim et al.
pose.AlearnablebasecodeS∈R64israndomlyinitializedandjointlyoptimized
for a single subject with our motion-oriented compositional radiance fields. To
makeSdistinguishableforeachpose,weproposetoemploythe3Dbodyposeas
aguidanceforcomputingimplicitfeaturesthatencouragehigh-fidelityandpose-
dependent radiance residual learning. A cross-attention mechanism is designed
to perform interactions between the learnable base code and 23 joint positions
(i.e. J ={j }23 ). A detailed overview of the mechanism is illustrated in Fig. 3.
i i=1
To allow high-frequency implicit feature learning with 3D body joints, we con-
duct positional encoding [42] for the input joint coordinates. We first project
each joint position into a higher dimension with a sinusoidal positional encod-
ing, γ(j ) = (cid:0) j ,sin(20πj ),cos(20πj ),...,sin(2L−1πj ),cos(2L−1πj )(cid:1), where L = 10
i i i i i i
is the number of frequency bands. We then project the encoded points (i.e.,
γ(J)∈R23×63) to a raw pose-embedded code P by a projection matrix W :
raw p
P =W ·γ(J). (6)
raw p
Then, we generate a query signal P from S, and key and value signals from
q
P via linear projections as follows:
raw
P =W ·S, P =W ·P , P =W ·P . (7)
q Q k K raw v V raw
Finally, we generate the learnable pose-embedded implicit feature P for the
nr
radiance residual estimation as follows:
P =softmax(P ·(P )⊤)·P . (8)
nr q k v
As explained earlier, we employ P to condition M , the MLP decoder of
nr nr
the non-rigid radiance branch. M optimizes the implicit feature P and its
nr nr
generation module for estimating pose-coherent radiance residuals instead of
solely relying on explicit volumetric features.
3.3 Multi-subject Unified Training
Our effective radiance compositional approach for non-rigid motion modeling
furtherenablesaflexibleextensionofoursingle-subjectMoCo-NeRFforunified
trainingwithmultiplesubjects,whichisverybeneficialforpracticalapplications.
Each monocular video exhibits distinct non-rigid motions, thereby requiring in-
dividualizedmodeling.However,itisimpossibleforthetraditionalspatialoffset
learning approach [73] that typically requires a heavy module for photo-realistic
quality. In contrast, our radiance compositional approach can achieve it by em-
ploying a separate, lightweight MHE for each subject, referred to as local MHE.
Building on this foundation, we propose three architectural modifications
as illustrated in Fig. 4. To benefit from unified training of multiple subjects,
we introduce a coarse global MHE ψG shared across all subjects to learn
hash
subject-generic representations (e.g., body shapes). We also ensure learning of
subject-specificdetailswithN localMHEs{ψi }Ns ,whereN isthenumber
s hash i=1 s
of subjects. For each canonical sample x , we query global features from the
cMoCo-NeRF 9
globalMHEandconcatenateittotheslicedfeaturefromthecorrespondinglocal
MHEforradiancepredictions.Weadditionallyproposetoleveragethelearnable
base codes of all subjects {S }Ns as ID codes, for sharing MLP decoders while
i i=1
ensuring subject-distinctive canonical representations. The modified radiance
estimations of the rigid branch for the multi-subject training are as follows:
c, σ =M (ψG (x )⊕(ψi (x )):H ⊕S ) (9)
rigid hash c hash c i
3.4 Model Optimization
We optimize MoCo-NeRF with all N image frames {I }Ni from the monoc-
i j j=1
ular video via patch-based ray sampling. Given an image frame I at each
j
iteration, we randomly sample N patches denoted as {P }Np to construct
p k k=1
the LPIPS [84] loss. The LPIPS loss term can measure the perceptual dis-
tance between two image patches, and thus we also adopt it for more perceptu-
ally coherent renderings. Our framework first renders {Pˆr,Pˆf}Np respectively
k k k=1
from C and C , and compute two LPIPS loss terms Lr and Lf
rigid final LPIPS LPIPS
againstthegroundtruthpatchPGT byusingthepre-trainedVGGNet[63]toas
k
L = LPIPS(VGG(Pˆ ), VGG(PGT)). We respectively compute the MSE
LPIPS k k
loss terms Lr ,Lf from each of the estimated patches {Pˆr,Pˆf}Np . More-
MSE MSE k k k=1
over,weadditionallycomputeanappearancematchinglosswithSSIMLf [15]
SSIM
only for the final RGB {Pˆf}Np , to improve the rendering quality. In summary,
k k=1
there are 5 loss terms in total: Lr ,Lr for the canonical radiance field of
LPIPS MSE
the rigid branch, and Lf ,Lf ,Lf for the final motion-oriented com-
LPIPS MSE SSIM
positional renderings. To enforce the rigid branch to learn the main canonical
radiancefieldswhilethenon-rigidbranchtolearnradianceresiduals,wecompute
the final overall loss function Lo with a hyperparameter λ=0.2 as:
Lo =λLr+(1−λ)Lf (10)
Lr =Lr +Lr , Lf =Lf +Lf +Lf (11)
LPIPS MSE LPIPS MSE SSIM
4 Experiments
WeconductextensiveexperimentsonZJU-MoCap[52]andMonoCap[17,18,50]
to verify the effectiveness of our proposed approach on both single- and multi-
subjectsettings.Themulti-subjectsettingindicatesmultiplesubjectsarejointly
optimized.OurfindingsdemonstratethatMoCo-NeRFcanlearnphoto-realistic
dynamichumanrepresentationsbyeffectivelyhandlingnon-rigidmotions.More-
over, it learns discriminative representations of each performer in the multi-
subject setting, while prior works struggle.
4.1 Datasets
We leverage ZJU-MoCap and MonoCap datasets for both training and evalua-
tion, following [14,21], and provide the details in this section. For ZJU-MoCap,10 J. Kim et al.
Table1:Quantitativeresultsofsingle-subjectmethodsonZJU-MoCapandMonoCap
datasetswhereLPIPS∗ =LPIPS×103 withthebestandsecond-bestperformanceno-
tations. MoCo-NeRF outperforms the baselines on most of the evaluation metrics in
both settings. Despite pixel-wise metrics (PSNR, SSIM) tend to favor smooth render-
ings generated from [14,21], we achieve state-of-the-art performance while effectively
modeling the photo-realistic non-rigid motions as illustrated in Fig. 5.
Single-subject ZJU-MoCap [52] MonoCap [50]
methods PSNR↑ SSIM↑ LPIPS∗↓ PSNR↑ SSIM↑ LPIPS∗↓
HumanNeRF [73] 30.18 0.9709 30.13 32.92 0.9864 12.93
Instant-NVR [14] 30.73 0.9697 38.87 31.89 0.9870 16.07
GauHuman [21] 30.98 0.9620 30.16 32.87 0.9849 13.54
MoCo-NeRF (Ours) 31.06 0.9734 28.83 33.01 0.9874 13.02
HumanNeRF Instant-NVR GauHuman Ours Ground Truth HumanNeRF Instant-NVR GauHuman Ours Ground Truth
Fig.5: Qualitative comparison of novel view synthesis from single-subject training.
MoCo-NeRF achieves photo-realistic rendering qualities with high-fidelity non-rigid
motion modeling. Although HumanNeRF [73] presents comparable quality, however,
MoCo-NeRF achieves much less training time as shown in Tab. 3.
weevaluateourMoCo-NeRFandotherbaselinesonall6subjects(i.e.,377,386,
387,392,393,394).Outof23cameraviews,weuseoneofthemfortrainingand
the other 22 views for evaluation. For MonoCap that comprises DynaCap [17]
andDeepCap[18],weutilizeall4subjects(i.e.,lan,marc,olek,vlad)andusethe
same set of cameras as [14]. For multi-subject evaluations presented in Tab. 2,
we evaluate on subjects 377, 386, and 392 of ZJU-MoCap.
4.2 Implementation Details
We optimize our model with Adam optimizer [30] with betas (0.9,0.99), and
learning rates of 5e−5 for the skeletal weights volume decoder and 5e−3 for theMoCo-NeRF 11
Table2:Quantitativeresultsofmulti-subjectmethodsonsubjects#377,#386,#392
of ZJU-MoCap [52]. Despite the same evaluation settings, MoCo-NeRF significantly
outperforms baseline methods under monocular settings with a shorter training time.
Multi-subject methods PSNR↑ SSIM↑ LPIPS∗↓ Training videos Train time
NHP [31] 26.64 0.9185 40.47 multi-view 86 hours
∼
GHuNeRF [33] 27.28 0.9291 40.82 monocular ∼123 hours
MoCo-NeRF (Ours) 31.97 0.9781 24.54 monocular ∼2 hours
NHP GHuNeRF Ours GT NHP GHuNeRF Ours GT NHP GHuNeRF Ours GT
Fig.6: Qualitative comparison of novel view synthesis from multi-subject training.
Although all subjects are seen during training, only our multi-subject MoCo-NeRF
achieves photo-realistic quality comparable to the ones of single-subject models.
others. Each MHE has 16 levels with feature dimensions H = 2, and 4 levels
with H = 2 for the global MHE. For each iteration, six 32×32 patches are
used with 256 samples per ray. We train all models including the baselines on 1
NvidiaRTX3090,wherebothbaselinesaretrainedfortheirsuggestedduration.
Our model is trained for 1 ∼ 2 hours depending on each subject’s complexity
for the single-subject training, and 2 hours for the multi-subject training.
4.3 State-of-the-art Comparison
Single-subject methods. Tab. 1 presents PSNR, SSIM [72], and LPIPS∗ of
MoCo-NeRF and our baselines [14,21,73], for novel-view synthesis under the
single-subject setting. MoCo-NeRF achieves state-of-the-art performance on all
metrics except one comparable LPIPS∗ for the MonoCap dataset. As shown in
Fig. 5, Instant-NVR [14] shows moderate performance, however, it often gener-
ates smoothed-out results without proper non-rigid motions due to high depen-
dencyonaccurateSMPLparameters.GauHuman[21]achievesbetterqualitative
performance than [14] but still struggles to model sophisticated non-rigid mo-
tions.Fig.1presentslongeroptimizationsaresurprisinglyineffectiveandharm-
ful respectively for Instant-NVR [14] and GauHuman [21] in terms of non-rigid
motionmodeling.Thisimpliesthattheirapproachescannotachieveoptimalnon-
rigid motion modeling. Moreover, we observe that PSNR and SSIM often favor
those results without fine-grained non-rigid motion modeling since predicting
pose-dependent deformations from novel viewpoints may not be pixel-wise ac-
curate. Instead, theperception metric (LPIPS∗) evaluates betterin this case, as12 J. Kim et al.
Table 3: Training efficiency comparison for methods with photo-realistic non-rigid
motionmodelingonasingleRTX3090.Inthetable,single-sub.andmulti-sub.respec-
tively represent training for 1 and 6 subjects. MoCo-NeRF significantly outperforms
HumanNeRF in terms of both GPU memory usage and training time. HumanNeRF
requires sequential training to achieve comparable non-rigid motion modeling for all
subjects, thus resulting in linearly-increased training time.
GPU Mem. (GB) Train time (hours)
single-sub. multi-sub. ∆ single-sub. multi-sub. ∆
HumanNeRF [73] 23.7 23.7 (+0) 38.9 233.3 (+194.5)
MoCo-NeRF (Ours) 16.2 20.3 (+4.1) 1.9 2.1 (+0.2)
HumanNeRF [73] has a better LPIPS∗ score and also shows qualitatively better
results. Thus, we consider GauHuman’s comparable PSNR might be favored by
asuperiorrepresentation(3DGaussians[29])insteadofbetternon-rigidmotion
modeling. A more detailed analysis is in supplementary materials. Despite this,
westillachievestate-of-the-artperformancewithsignificantlyimprovedtraining
efficiency compared to another photo-realistic model [73] (Tab. 3).
Multi-subject methods. We evaluate our MoCo-NeRF against NHP [31] and
GHuNeRF[33],bothofwhicharedesignedtoachievegeneralizationcapabilities.
This is necessitated by the absence of prior works that directly target the joint
learning task for multiple subjects from monocular videos. Additionally, we ob-
servethatthesingle-subjectbaselinemodelsfailtolearndistinctrepresentations
when trained in a unified manner. This observation underscores our decision to
compare against generalization models that are specifically tailored to leverage
multi-subjectdatafortraining.Forfairness,weusetheirofficialcheckpointsand
ensure all evaluated subjects were included for training.
As demonstrated in Tab. 2, MoCo-NeRF remarkably outperforms all the
baseline methods on all metrics while also excelling in training efficiency up to
61 times faster. Even though MoCo-NeRF only utilizes a monocular video of
each subject, Fig. 6 illustrates our method succeeds in learning both distinct
representations and pose-dependent radiance residuals of each subject, enabling
photo-realistic free-viewpoint renderings.
4.4 Model Analysis
In this section, we present analyses of our contributions respectively for two
major achievements: novel non-rigid motion modeling (NR modeling) and
multi-subject unified training (multi-subject modeling). For both analyses,
weprogressivelyremoveeachcomponentfromafullmodeltoavanillaapproach.
NR Modeling: Analysis of radiance compositional approach. Tab. 4
demonstrates our radiance compositional design perceptually improves the ren-
dering results by closing the radiance discrepancies with predicted residuals.
Fig. 7 shows that the representation learned solely by the rigid radiance branch
(c) suffers from noisy renderings caused by incorrect correspondences due toMoCo-NeRF 13
Table 4: Ablation study in the single- Table 5: Ablation study of multi-subject
subject task on ZJU-MoCap for b) pose- MoCo-NeRF on ZJU-MoCap for our pro-
embeddedimplicitfeaturesandc)motion- posed architectural modifications.
oriented compositional NeRF approach.
Multi-subject PSNR SSIM LPIPS∗
Single-subject PSNR SSIM LPIPS∗
(i) MoCo-NeRF(Ours) 30.91 0.9746 27.69
(a) MoCo-NeRF(Ours) 31.06 0.9734 28.83 (ii) (–)GlobalMHE 30.83 0.9746 28.49
(b)(–)Posefeatures 30.64 0.9702 29.65 (iii)(–)Ns localMHEs 30.70 0.9742 29.03
(c) (–)ResidualNeRF 30.62 0.9702 29.93 (iv)(–)Identitycodes 28.63 0.9666 38.29
Ground Truth (a) (b) (c) Ground Truth (i) (ii) (iii) (iv)
Fig.7: Qualitative results of the ablation Fig.8: Qualitative results of the ablation
study on single-subject MoCo-NeRF pre- studyinTab.5wherewecanobservegrad-
sented in Tab. 4. ual improvements from (iv) to (i).
non-rigid motions. With our radiance residual modeling (b), those issues are
mitigated by jointly learning per-sample radiance residuals, resulting in more
complete rendering and better non-rigid motion modeling as clearly shown in
Fig.7.Inaddition,Fig.9illustratessuccessfuldecomposedlearningoftwotypes
of representations and validates the effectiveness of our radiance compositional
approach in non-rigid motion modeling.
NR Modeling: Analysis of pose-embedded implicit feature. Tab. 4 fur-
ther illustrates that the final version with the proposed pose-embedded implicit
features (Tab. 4 (a)) significantly improves the quantitative performance on all
metrics. It enables the non-rigid branch to implicitly utilize body pose informa-
tion in addition to the explicit features, yielding smoother and more coherent
non-rigid motion modeling as shown in Fig. 7. We can observe remarkable qual-
ityenhancementsthatdemonstratetheeffectivenessofourattention-basedcode
generation module. We also present an ablation study on the choice of cross-
attention module for the feature generation in the supplementary material.
Multi-subject Modeling: Analysis of using learnable base codes as ID
codes. In Fig. 8, the results of MoCo-NeRF without any modifications (iv) ex-
hibit mixed-up appearances on all target subjects, similar to the ones of [73]
that are presented in the supplementary material. Utilizing a set of learnable
basecodesasIDcodes(Tab.5(iii))enablesasingleMLPdecoderM todif-
rigid
ferentiate each subject by optimizing the ID codes as plug-in parameters. Thus,
distinct representations of all subjects are successfully learned as illustrated in
(iii) of Fig. 8, yielding significant quantitative improvements as well (Tab. 5).14 J. Kim et al.
Fig.9: Qualitativeillustrationsofnon-rigidradianceresidualsonnovel-viewsynthesis
where δ → c′ = c+δ∆c,σ′ = σ +δ∆σ. The first row progressively demonstrates
the capability of the non-rigid radiance branch to learn the non-rigid motions on the
subject’s face in addition to the cloth folds. The last two rows clearly illustrate the
effects of radiance residuals on cloth folds and hems corresponding to the body pose.
Multi-subjectModeling:AnalysisofN localMHEs.Fig.8showsthatthe
s
model(iii)lacksfine-graineddetailsduetothesharingofoneMHE.Byutilizing
separate N local MHEs (Tab. 5 (ii)), the model learns subject-discriminative
s
non-rigid motions as they now optimize separate explicit features for each sub-
ject’s radiance residuals. Fig. 8 shows clear improvement of the model (ii) in
non-rigid motion modeling where its quantitative gains are reflected in Tab. 5.
Multi-subject Modeling: Analysis of global MHE.Theadditionofglobal
MHE (i) further empowers MoCo-NeRF to learn better representations with
more coherent non-rigid motions as illustrated in Fig. 8. Although the global
MHE is not used for learning non-rigid radiance residuals, it encourages the
rigid part of features from local MHE to concentrate on subject-specific repre-
sentations. As its frozen copy is used for the non-rigid radiance branch, it facili-
tatesbetterlearningofradianceresidualsformodelingnon-rigidmotions.Tab.5
clearly shows that it can help to learn perceptually better representations.
5 Conclusion
Inthispaper,wepresentedanovelframeworkMoCo-NeRFforfree-viewpointdy-
namichumanrenderingthatiscapableofbothsingle-andmulti-subjecttraining
frommonocularvideos.Bydecomposingtheneuralrepresentationofadynamic
human by motions and utilizing proposed implicit pose codes, MoCo-NeRF ef-
fectively learns radiance discrepancies induced by non-rigid motions. The pro-
posed multi-subject MoCo-NeRF efficiently achieves comparable performance
comparedtothesingle-subjectmodel.Ithasfullydemonstrateditseffectiveness
and established new state-of-the-art performance on both problems.MoCo-NeRF 15
Acknowledgments. This research is supported in part by the Hong Kong
PhDFellowshipScheme(HKPFS),EarlyCareerSchemeoftheResearchGrants
Council (RGC) of the Hong Kong SAR under grant No. 26202321, SAIL Re-
search Project, HKUST-Zeekr Collaborative Research Fund, HKUST-WeBank
Joint Lab Project, and Tencent Rhino-Bird Focused Research Program.
References
1. Alldieck,T.,Zanfir,M.,Sminchisescu,C.:Photorealisticmonocular3dreconstruc-
tion of humans wearing clothing. In: CVPR (2022)
2. Barron, J.T., Mildenhall, B., Tancik, M., Hedman, P., Martin-Brualla, R., Srini-
vasan, P.P.: Mip-NeRF: A multiscale representation for anti-aliasing neural radi-
ance fields. In: ICCV (2021)
3. Barron,J.T.,Mildenhall,B.,Verbin,D.,Srinivasan,P.P.,Hedman,P.:Mip-NeRF
360: Unbounded anti-aliased neural radiance fields. In: CVPR (2022)
4. Chen, A., Xu, Z., Geiger, A., Yu, J., Su, H.: TensoRF: Tensorial radiance fields.
In: ECCV (2022)
5. Chen,A.,Xu, Z.,Zhao,F.,Zhang,X.,Xiang,F., Yu,J., Su,H.:MVSNeRF: Fast
generalizableradiancefieldreconstructionfrommulti-viewstereo.In:ICCV(2021)
6. Chen,X.,Jiang,T.,Song,J.,Yang,J.,Black,M.J.,Geiger,A.,Hilliges,O.:gDNA:
Towards generative detailed neural avatars. In: CVPR (2022)
7. Chen, Y., Wang, X., Chen, X., Zhang, Q., Li, X., Guo, Y., Wang, J., Wang, F.:
Uv volumes for real-time rendering of editable free-view human performance. In:
CVPR (2023)
8. Collet,A.,Chuang,M.,Sweeney,P.,Gillett,D.,Evseev,D.,Calabrese,D.,Hoppe,
H., Kirk, A., Sullivan, S.: High-quality streamable free-viewpoint video. In: SIG-
GRAPH (2015)
9. Dong,Z.,Chen,X.,Yang,J.,J.Black,M.,Hilliges,O.,Geiger,A.:AG3D:Learning
to generate 3D avatars from 2D image collections. In: ICCV (2023)
10. Dou, M., Khamis, S., Degtyarev, Y., Davidson, P., Fanello, S.R., Kowdle, A., Es-
colano,S.O.,Rhemann,C.,Kim,D.,Taylor,J.,Kohli,P.,Tankovich,V.,Izadi,S.:
Fusion4D: Real-time performance capture of challenging scenes. In: SIGGRAPH
(2016)
11. Du, Y., Zhang, Y., Yu, H.X., Tenenbaum, J.B., Wu, J.: Neural radiance flow for
4d view synthesis and video processing. In: ICCV (2021)
12. Gao, C., Saraf, A., Kopf, J., Huang, J.B.: Dynamic view synthesis from dynamic
monocular video. In: ICCV (2021)
13. Garbin,S.J.,Kowalski,M.,Johnson,M.,Shotton,J.,Valentin,J.:FastNeRF:High-
fidelity neural rendering at 200fps. In: ICCV (2021)
14. Geng, C., Peng, S., Xu, Z., Bao, H., Zhou, X.: Learning neural volumetric repre-
sentations of dynamic humans in minutes. In: CVPR (2023)
15. Godard, C., Mac Aodha, O., Brostow, G.J.: Unsupervised monocular depth esti-
mation with left-right consistency. In: CVPR (2017)
16. Guo, K., Lincoln, P., Davidson, P., Busch, J., Yu, X., Whalen, M., Harvey, G.,
Orts-Escolano, S., Pandey, R., Dourgarian, J., Tang, D., Tkach, A., Kowdle, A.,
Cooper, E., Dou, M., Fanello, S., Fyffe, G., Rhemann, C., Taylor, J., Debevec,
P., Izadi, S.: The relightables: Volumetric performance capture of humans with
realistic relighting. In: SIGGRAPH Asia (2019)16 J. Kim et al.
17. Habermann, M., Liu, L., Xu, W., Zollhoefer, M., Pons-Moll, G., Theobalt, C.:
Real-time deep dynamic characters. In: SIGGRAPH (2021)
18. Habermann, M., Xu, W., Zollhoefer, M., Pons-Moll, G., Theobalt, C.: DeepCap:
Monocularhumanperformancecaptureusingweaksupervision.In:CVPR(2020)
19. Hedman, P., Srinivasan, P.P., Mildenhall, B., Barron, J.T., Debevec, P.: Baking
neural radiance fields for real-time view synthesis. In: ICCV (2021)
20. Hong, F., Chen, Z., Lan, Y., Pan, L., Liu, Z.: EVA3D: Compositional 3d human
generation from 2d image collections. In: ICLR (2023)
21. Hu,S.,Liu,Z.:GauHuman:Articulatedgaussiansplattingfrommonocularhuman
videos. In: CVPR (2024)
22. Hu, T., Yu, T., Zheng, Z., Zhang, H., Liu, Y., Zwicker, M.: HVTR: Hybrid
volumetric-textural rendering for human avatars. In: 3DV (2022)
23. Huang, Y., Yi, H., Liu, W., Wang, H., Wu, B., Wang, W., Lin, B., Zhang, D.,
Cai, D.: One-shot implicit animatable avatars with model-based priors. In: ICCV
(2023)
24. Jain, A., Tancik, M., Abbeel, P.: Putting nerf on a diet: Semantically consistent
few-shot view synthesis. In: ICCV (2021)
25. Jiang,B.,Hong,Y.,Bao,H.,Zhang,J.:SelfRecon:Selfreconstructionyourdigital
avatar from monocular video. In: CVPR (2022)
26. Jiang, T., Chen, X., Song, J., Hilliges, O.: Instantavatar: Learning avatars from
monocular video in 60 seconds. In: CVPR (2023)
27. Jiang, W., Yi, K.M., Samei, G., Tuzel, O., Ranjan, A.: NeuMan: Neural human
radiance field from a single video. In: ECCV (2022)
28. Jiang, Y., Yao, K., Su, Z., Shen, Z., Luo, H., Xu, L.: Instant-NVR: Instant neural
volumetric rendering for human-object interactions from monocular rgbd stream.
In: CVPR (2023)
29. Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3d gaussian splatting for
real-time radiance field rendering. ACM TOG (July 2023)
30. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR
(2015)
31. Kwon, Y., Kim, D., Ceylan, D., Fuchs, H.: Neural human performer: Learning
generalizableradiancefieldsforhumanperformancerendering.In:NeurIPS(2021)
32. Lewis,J.P.,Cordner,M.,Fong,N.:Posespacedeformation:Aunifiedapproachto
shape interpolation and skeleton-driven deformation. In: SIGGRAPH (2000)
33. Li, C., Lin, J., Lee, G.H.: Generalizable human nerf from a monocular video. In:
3DV (2024)
34. Li,R.,Tanke,J.,Vo,M.,Zollhofer,M.,Gall,J.,Kanazawa,A.,Lassner,C.:TAVA:
Template-free animatable volumetric actors. In: ECCV (2022)
35. Li, Z., Niklaus, S., Snavely, N., Wang, O.: Neural scene flow fields for space-time
view synthesis of dynamic scenes. In: CVPR (2021)
36. Liu,L.,Gu,J.,Lin,K.Z.,Chua,T.S.,Theobalt,C.:Neuralsparsevoxelfields.In:
NeurIPS (2020)
37. Liu,L.,Habermann,M.,Rudnev,V.,Sarkar,K.,Gu,J.,Theobalt,C.:Neuralactor:
Neuralfree-viewsynthesisofhumanactorswithposecontrol.In:SIGGRAPHAsia
(2021)
38. Liu,Y.,Peng,S.,Liu,L.,Wang,Q.,Wang,P.,Christian,T.,Zhou,X.,Wang,W.:
Neural rays for occlusion-aware image-based rendering. In: CVPR (2022)
39. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: A
skinned multi-person linear model. In: SIGGRAPH Asia (2015)MoCo-NeRF 17
40. Martin-Brualla, R., Pandey, R., Yang, S., Pidlypenskyi, P., Taylor, J., Valentin,
J., Khamis, S., Davidson, P., Tkach, A., Lincoln, P., Kowdle, A., Rhemann, C.,
Goldman,D.B.,Keskin,C.,Seitz,S.,Izadi,S.,Fanello,S.:LookinGood:Enhancing
performance capture with real-time neural re-rendering. ACM TOG (2018)
41. Mihajlovic, M., Bansal, A., Zollhoefer, M., Tang, S., Saito, S.: KeypointNeRF:
Generalizingimage-basedvolumetricavatarsusingrelativespatialencodingofkey-
points. In: ECCV (2022)
42. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.: NeRF: Representing scenes as neural radiance fields for view synthesis. In:
ECCV (2020)
43. Mu, J., Sang, S., Vasconcelos, N., Wang, X.: ActorsNeRF: Animatable few-shot
human rendering with generalizable nerfs. In: ICCV (2023)
44. Müller, T., Evans, A., Schied, C., Keller, A.: Instant neural graphics primitives
with a multiresolution hash encoding. In: SIGGRAPH (2022)
45. Noguchi, A., Sun, X., Lin, S., Harada, T.: Unsupervised learning of efficient
geometry-aware neural articulated representations. In: ECCV (2022)
46. Oechsle,M.,Peng,S.,Geiger,A.:UNISURF:Unifyingneuralimplicitsurfacesand
radiance fields for multi-view reconstruction. In: ICCV (2021)
47. Park,K.,Sinha,U.,Barron,J.T.,Bouaziz,S.,Goldman,D.B.,Seitz,S.M.,Martin-
Brualla, R.: Nerfies: Deformable neural radiance fields. In: ICCV (2021)
48. Park,K.,Sinha,U.,Hedman,P.,Barron,J.T.,Bouaziz,S.,Goldman,D.B.,Martin-
Brualla, R., Seitz, S.M.: HyperNeRF: A higher-dimensional representation for
topologically varying neural radiance fields. In: SIGGRAPH Asia (2021)
49. Peng,B.,Hu,J.,Zhou,J.,Zhang,J.:SelfNeRF:Fasttrainingnerfforhumanfrom
monocular self-rotating video. arXiv preprint arXiv:2210.01651 (2022)
50. Peng,S.,Dong,J.,Wang,Q.,Zhang,S.,Shuai,Q.,Zhou,X.,Bao,H.:Animatable
neural radiance fields for modeling dynamic human bodies. In: ICCV (2021)
51. Peng,S.,Xu,Z.,Dong,J.,Wang,Q.,Zhang,S.,Shuai,Q.,Bao,H.,Zhou,X.:An-
imatable implicit neural representations for creating realistic avatars from videos.
TPAMI (2024)
52. Peng, S., Zhang, Y., Xu, Y., Wang, Q., Shuai, Q., Bao, H., Zhou, X.: Neural
Body: Implicit neural representations with structured latent codes for novel view
synthesis of dynamic humans. In: CVPR (2021)
53. Pumarola, A., Corona, E., Pons-Moll, G., Moreno-Noguer, F.: D-NeRF: Neural
Radiance Fields for Dynamic Scenes. In: CVPR (2020)
54. Reiser, C.,Peng,S.,Liao,Y., Geiger,A.:KiloNeRF:Speeding upneuralradiance
fields with thousands of tiny mlps. In: ICCV (2021)
55. Rematas,K.,Liu,A.,Srinivasan,P.P.,Barron,J.T.,Tagliasacchi,A.,Funkhouser,
T., Ferrari, V.: Urban radiance fields. In: CVPR (2022)
56. Remelli,E.,Bagautdinov,T.,Saito,S.,Wu,C.,Simon,T.,Wei,S.E.,Guo,K.,Cao,
Z., Prada, F., Saragih, J., et al.: Drivable volumetric avatars using texel-aligned
features. In: SIGGRAPH (2022)
57. Saito, S., , Huang, Z., Natsume, R., Morishima, S., Kanazawa, A., Li, H.: PIFu:
Pixel-aligned implicit function for high-resolution clothed human digitization. In:
ICCV (2019)
58. Saito,S.,Simon,T.,Saragih,J.,Joo,H.:PIFuHD:Multi-levelpixel-alignedimplicit
function for high-resolution 3d human digitization. In: CVPR (2020)
59. SaraFridovich-KeilandAlexYu,Tancik,M.,Chen,Q.,Recht,B.,Kanazawa,A.:
Plenoxels: Radiance fields without neural networks. In: CVPR (2022)
60. Schönberger, J.L., Zheng, E., Frahm, J.M., Pollefeys, M.: Pixelwise view selection
for unstructured multi-view stereo. In: ECCV (2016)18 J. Kim et al.
61. Schönberger,J.L.,Frahm,J.M.:Structure-from-motionrevisited.In:CVPR(2016)
62. Shao, R., Zhang, H., Zhang, H., Chen, M., Cao, Y., Yu, T., Liu, Y.: DoubleField:
Bridgingtheneuralsurfaceandradiancefieldsforhigh-fidelityhumanreconstruc-
tion and rendering. In: CVPR (2022)
63. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
image recognition. In: ICLR (2015)
64. Su, S.Y., Yu, F., Zollhöfer, M., Rhodin, H.: A-NeRF: Articulated neural radiance
fields for learning human shape, appearance, and pose. In: NeurIPS (2021)
65. Su, Z., Xu, L., Zheng, Z., Yu, T., Liu, Y., Fang, L.: RobustFusion: Human volu-
metriccapturewithdata-drivenvisualcuesusingargbdcamera.In:ECCV(2020)
66. Tancik,M.,Casser,V.,Yan,X.,Pradhan,S.,Mildenhall,B.,Srinivasan,P.,Barron,
J.T.,Kretzschmar,H.:Block-NeRF:Scalablelargesceneneuralviewsynthesis.In:
CVPR (2022)
67. Tretschk, E., Tewari, A., Golyanik, V., Zollhöfer, M., Lassner, C., Theobalt, C.:
Non-rigid neural radiance fields: Reconstruction and novel view synthesis of a dy-
namic scene from monocular video. In: ICCV (2021)
68. Turki,H.,Ramanan,D.,Satyanarayanan,M.:Mega-NERF:Scalableconstruction
of large-scale nerfs for virtual fly-throughs. In: CVPR (2022)
69. Wang, P., Liu, L., Liu, Y., Theobalt, C., Komura, T., Wang, W.: NeuS: Learning
neural implicit surfaces by volume rendering for multi-view reconstruction. In:
NeurIPS (2021)
70. Wang, Q., Wang, Z., Genova, K., Srinivasan, P., Zhou, H., Barron, J.T., Martin-
Brualla,R.,Snavely,N.,Funkhouser,T.:IBRNet:Learningmulti-viewimage-based
rendering. In: CVPR (2021)
71. Wang,S.,Schwarz,K.,Geiger,A.,Tang,S.:ARAH:Animatablevolumerendering
of articulated human sdfs. In: ECCV (2022)
72. Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image quality assessment: from
error visibility to structural similarity. TIP (2004)
73. Weng,C.Y.,Curless,B.,Srinivasan,P.P.,Barron,J.T.,Kemelmacher-Shlizerman,
I.:HumanNeRF:Free-viewpointrenderingofmovingpeoplefrommonocularvideo.
In: CVPR (2022)
74. Weng, C.Y., Srinivasan, P.P., Curless, B., Kemelmacher-Shlizerman, I.: Person-
NeRF: Personalized reconstruction from photo collections. In: CVPR (2023)
75. Wu,M.,Wang,Y.,Hu,Q.,Yu,J.:Multi-viewneuralhumanrendering.In:CVPR
(2020)
76. Xian, W., Huang, J.B., Kopf, J., Kim, C.: Space-time neural irradiance fields for
free-viewpoint video. In: CVPR (2021)
77. Xiangli, Y., Xu, L., Pan, X., Zhao, N., Rao, A., Theobalt, C., Dai, B., Lin, D.:
Bungeenerf:Progressiveneuralradiancefieldforextrememulti-scalescenerender-
ing. In: ECCV (2022)
78. Xu,H.,Alldieck,T.,Sminchisescu,C.:H-NeRF:Neuralradiancefieldsforrender-
ing and temporal reconstruction of humans in motion. In: NeurIPS (2021)
79. Xu, L., Xiangli, Y., Peng, S., Pan, X., Zhao, N., Theobalt, C., Dai, B., Lin, D.:
Grid-guided neural radiance fields for large urban scenes. In: CVPR (2023)
80. Xu, Z., Peng, S., Lin, H., He, G., Sun, J., Shen, Y., Bao, H., Zhou, X.: 4K4D:
Real-time 4d view synthesis at 4k resolution. In: CVPR (2024)
81. Yariv, L., Gu, J., Kasten, Y., Lipman, Y.: Volume rendering of neural implicit
surfaces. In: NeurIPS (2021)
82. Yu,A.,Li,R.,Tancik,M.,Li,H.,Ng,R.,Kanazawa,A.:PlenOctreesforreal-time
rendering of neural radiance fields. In: ICCV (2021)MoCo-NeRF 19
83. Yu,A.,Ye,V.,Tancik,M.,Kanazawa,A.:pixelNeRF:Neuralradiancefieldsfrom
one or few images. In: CVPR (2021)
84. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: CVPR (2018)
85. Zhang,R.,Chen,J.:NDF:Neuraldeformablefieldsfordynamichumanmodelling.
In: ECCV (2022)
86. Zhao, F., Yang, W., Zhang, J., Lin, P., Zhang, Y., Yu, J., Xu, L.: HumanNeRF:
Efficiently generated human radiance field from sparse inputs. In: CVPR (2022)
87. Zheng,Z.,Huang,H.,Yu,T.,Zhang,H.,Guo,Y.,Liu,Y.:Structuredlocalradiance
fields for human avatar modeling. In: CVPR (2022)
88. Zheng, Z., Zhao, X., Zhang, H., Liu, B., Liu, Y.: AvatarRex: Real-time expressive
full-body avatars. In: SIGGRAPH (2023)
89. Zhi, Y., Qian, S., Yan, X., Gao, S.: Dual-space nerf: Learning animatable avatars
and scene lighting in separate spaces. In: 3DV (2022)
A. Comparisons with GauHuman (Effectiveness of
Pixel-Wise Metrics)
In the main paper, we mention that the pixel-wise metrics (PSNR, SSIM) favor
smoother renderings generated by GauHuman [21], which is a concurrent work
published one week prior to our submission. As results, our proposed MoCo-
NeRF and GauHuman show comparable PSNR on both datasets and we would
liketoprovideananalysisonthisphenomenon.Asmentionedinthemainpaper,
GauHuman cannot capture pose-dependent cloth wrinkles as effectively as our
model. Pixel-wise metrics (PSNR) are less sensitive to these details, but better
reflected by perceptual metric LPIPS∗ as shown in Fig. A. Nevertheless, Tab. 1
in the paper shows our significant improvements in LPIPS∗and SSIM. Fig. 5
in the paper and Fig. A show GauHuman lacks detailed cloth wrinkles for
most of subjects, even with longer training. This limitation may be due to its
reliance on accurate parametric model (SMPL) like Instant-NVR, while ours is
free from the dependency. Overall, our work achieves clear state-of-the-art in
single-subject task as well.
Fig.A: Failures of pixel-wise metrics and longer training of GauHuman [21].
B. Integration of MHE into HumanNeRF
In this section, we show that replacing MLPs of HumanNeRF [73] with MHEs
is non-trivial. First, if we replace canonical MLP of HumanNeRF with MHE20 J. Kim et al.
Fig.B: Illustrations of HumanNeRF with MHEs.
and train for an hour, it results in wiggly novel-view appearances as shown in
Fig.B(a).Itisduetothenon-rigidMLP,whichconvergesin39hours,predicting
inconsistent spatial offsets ∆x for each canonical point x . As a result, multiple
c
hash entries are created for the same x .
c
Fig. B (b) shows using both non-rigid and canonical MHEs worsens the
results as explicit MHE fails to replicate the implicit MLP’s non-rigid motion
modelingdespitethesametrainingconfigurations.Incontrast,ournovelmotion
modeling enables not only successful modeling of non-rigid motions but also
optimal utilization of MHE, yielding faster and better performance.
C. Additional Ablation Study on Multi-Subject
MoCo-NeRF
Ourablationstudiescumulativelyremovingeachcomponentmightmisleadthat
the introduction of ID codes contribute the most to the performance, but this
is not true. As shown in Tab. A, adding N local MHEs to the vanilla single-
s
subject MoCo-NeRF (Tab. 5 (iv) in main paper) also significantly boosts the
performance, paralleling the impact of ID codes. Therefore, other components
beyond ID codes also greatly contribute to state-of-the-art performance.
Table A: Ablation studies on N local MHEs for multi-subject training.
PSNR ↑ SSIM ↑ LPIPS∗ ↓
Vanilla Single-subject MoCo-NeRF 28.63 0.9666 38.29
(+) N local MHEs 30.86 0.9741 28.80
D. Reason for Splitting MHE Features
Leveraging two separate MHEs for rigid and non-rigid branches of MoCo-NeRF
slows down its training and rendering by ×1.14 due to an overhead of querying
hash tables twice. Our proposed novel feature-splitting approach successfully
avoids the overhead by querying the hash tables only once while optimizing
independent hash features for two different purposes.MoCo-NeRF 21
E. Novel Pose Synthesis
Although MoCo-NeRF does not specifically target novel-pose animations, we
stillcantransferunseenposestothelearnedrepresentationsasshowninFig.C.
Fig.C: Illustrations of novel-pose synthesis of MoCo-NeRF.
F. Additional Qualitative Comparisons
In the demo video demo.mp4, we additionally illustrate the effectiveness of
MoCo-NeRF in handling non-rigid motions and modeling multiple subjects at
the same time. In two sections of the video, we compare against single-subject
methods [14,21,73] on both single- and multi-subject training settings, respec-
tively.
Inthesingle-subjectsetting,MoCo-NeRFachievesphoto-realisticnovel-view
synthesisviamotion-basedcompositionalNeRFwhileGauHuman[21]andInstant-
NVR[14]failtomodelcoherentpose-dependentnon-rigidmotions.DespiteHu-
manNeRF’s comparable quality, its training efficiency is much worse than our
proposedMoCo-NeRFasdiscussedinthemainpaper.Forthemulti-subjectset-
ting,allsingle-subjectmethods[14,21,73]cannotmodeldistinctrepresentations
for each subject. On the other hand, MoCo-NeRF readily extends to handle
multiple distinct representations benefiting from the radiance compositional ap-
proach as also discussed in the main paper.
Inthelastsectionofthedemovideo,wecomparemulti-subjectMoCo-NeRF
against GHuNeRF [33] and NHP [31] that are trained with videos of multiple
subjects for generalization capability. As mentioned in the main paper, we com-
pare against these models targeting different task since there is no prior work
learning distinct representations for multiple subjects from monocular videos.
ThedemovideoclearlyillustratesthatonlyMoCo-NeRFachievesphoto-realistic
quality of renderings with pose-dependent non-rigid motions modeling as well.
NHP [31] also presents a slight modeling of non-rigid motions, however, it re-
quires multi-view videos for its training and rendering.
G. Additional Model Analysis
Ablationstudyofcross-attentionforimplicitfeatures:Inthemainpaper,
we introduce to adopt cross-attention mechanism on a learnable base code S22 J. Kim et al.
and input body poses J to memory-efficiently generate pose-adaptive implicit
features for the radiance residual field. To analyze its effectiveness, we compare
the performance when directly using S and J as the conditions instead of P
nr
in Tab. B. It demonstrates that using cross-attention to generate pose-coherent
conditions can further improves the performance.
Ablation study of feature sharing methods for our compositional radi-
ance fields: For our radiance residual predictions, we propose to utilize frozen
features of the canonical radiance field as representational priors to encourage
lesscomplexlearningandmorecoherentnon-rigidmotionmodeling.Toanalyze
the effectiveness of this specific design, we present results when features are not
shared (i) and features are shared without freezing (ii) in Tab. C. Interestingly,
allowingsharedfeaturestoremainunfrozenresultsinperformancedegradation,
as the learning process for radiance residual fields interferes with that of the
canonical radiance field. Overall, it shows that frozen feature yields the best
performance, implying that our motivation to provide representational priors
for learning radiance residuals is satisfied.
Table B: Analysis of cross-attention for pose-embedded implicit feature generation.
Single-subject PSNR↑ SSIM↑ LPIPS∗↓
(a) Ours (w/o cross-attention) 30.88 0.9706 30.66
(b) Ours (full model) 31.06 0.9734 28.83
Table C: Analysis of feature sharing methods for our proposed compositional NeRF
architecture.
Single-subject PSNR↑ SSIM↑ LPIPS∗↓
(i) Ours (w/ no feature sharing) 30.88 0.9717 30.17
(ii) Ours (w/ unfrozen feature sharing) 30.76 0.9710 30.71
(iii) Ours (full model) 31.06 0.9734 28.83