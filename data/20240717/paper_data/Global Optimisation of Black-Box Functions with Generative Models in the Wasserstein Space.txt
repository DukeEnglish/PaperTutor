Global Optimisation of Black-Box Functions with
Generative Models in the Wasserstein Space
TigranRamazyana,*,MikhailHushchyna andDenisDerkacha
aHSEUniversity,Moscow,Russia
ORCID(TigranRamazyan): https://orcid.org/0000-0002-8770-0637,ORCID(MikhailHushchyn):
https://orcid.org/0000-0002-8894-6292,ORCID(DenisDerkach): https://orcid.org/0000-0001-5871-0628
Abstract. representedbyaMonteCarlosimulatorwhichprovidesdatafrom
We propose a new uncertainty estimator for gradient-free opti- anintractablelikelihood[34].Inadditiontothat,simulationsamples
misation of black-box simulators using deep generative surrogate usedasinputtotrainmachinelearningmodels,usuallyconsistof
models.Optimisationofthesesimulatorsisespeciallychallenging severalsamplesrepresentingdisconnectedregionsoftheparameter
for stochasticsimulators andhigher dimensions. To addressthese space.Inthiscase,surrogatemodellingandoptimisationdirection
issues,weutiliseadeepgenerativesurrogateapproachtomodelthe estimationareparticularlychallengingduetohighuncertainty,thus
blackboxresponsefortheentireparameterspace.Wethenleverage makinggradient-freeoptimisationmuchmorepreferablethangradient
this knowledge to estimate the proposed uncertainty based on the methods.
Wassersteindistance-theWassersteinuncertainty.Thisapproachis Commongradient-freeoptimisationmethods,e.g.,Bayesianopti-
employedinaposterioragnosticgradient-freeoptimisationalgorithm misation,relyonuncertaintyestimationovertheparameterspaceto
thatminimisesregretovertheentireparameterspace.Aseriesoftests predictthepotentialminimum.Weneedtoleveragetheknowledge
wereconductedtodemonstratethatourmethodismorerobusttothe ofthestochasticsimulatorresponse,henceanuncertaintyestimator
shapeofboththeblackboxfunctionandthestochasticresponseof thatwouldaccountforthecomplexityofboththeresponseshape
theblackboxthanstate-of-the-artmethods,suchasefficientglobal andthegeometryoftheobjectivefunctionisrequired.Wepropose
optimisationwithadeepGaussianprocesssurrogate. tominimiseregretasinthelowerconfidenceboundapproach,but
thevarianceofthepredictiveposteriorismovedfrom(Rd,L )met-
2
ricspaceto(P ,W )metricspace.Thatisdonebymodellingthe
1 Introduction 2 2
stochasticsimulatorresponsewithadeepgenerativemodel(DGM)
Simulationofreal-worldexperimentsiskeytoscientificdiscoveries asthesurrogate[50,47].
andengineeringsolutions.Suchtechniquesuseparametersdescribing Awell-trainedDGMsurrogatecanproperlyreplicatethesimulator
configurationsandarchitecturesofthesystem.Acommonchallenge response.TheuseofDGMsassurrogatesprovidestheopportunityto
istofindtheoptimalconfigurationforsomeobjective,e.g.,suchthat workinagnosticpredictiveposteriorsettings [25].Also,asinglerun
itmaximisesefficiencyorminimisescostsandoverhead[20]. ofaDGMsurrogateisordersofmagnitudefasterthanasimulator
Togeneralisetheproblem,weexaminethecaseofstochasticsim- call,whichisexactlytheadvantageofneuralnetworksurrogatesover
ulators. A single call of such simulators is often computationally simulators.
expensive, which is the case for many domains, especially when We review related works in Section 2. Section 3 describes the
the simulator is Monte Carlo-based. For instance simulators mod- proposedapproach.InSection4weexamineourapproachonaset
ellingparticlescatterings[56],moleculardynamics[1],andradiation ofexperimentsandcompareourresultswithselectedbaselines.The
transport[5].Theproblemisalsooftenreferredtoasdesignoptimisa- resultsoftheresearcharediscussedinSection5andconcludedin
tion[51]. Section6.
Inthiscase,thesimulatoristreatedasablackboxexperiment.This
meansthatobservationscomefromanunknownandlikelydifficult-to-
2 RelatedWork
estimatefunction.Usingasurrogatemodelforblack-boxoptimisation
(BBO)isanestablishedtechnique[14],asBBOhasarichhistoryof
Blackboxoptimisationencompassesnumerousmethods.Common
bothgradientandgradient-freemethods [4],mostofwhichcome
methods include conjugate gradient [54], Quasi-Newton methods,
fromtacklingproblemsthatariseinPhysics,Chemistry,andEngi-
suchasBFGS[49],trust-regionmethods[12,13],andmulti-armed
neering.
banditapproaches[24].Inparticular,Ref.[55,9,33,38]exploredeep
Although gradient-based optimisation has proven to work well
generativemodelsforblack-boxoptimisation.[9,33]focusonpos-
fordifferentiableobjectivefunctions,e.g., [11,18,16],inpractice
teriorparametersinferenceusinggivenobservations,and[55]uses
non-differentiableobjectivefunctionsappearofteninapplications[2].
directgradientoptimisationoftheobjectivefunctionusingalocal
Forinstance,whenoneaddressesparameteroptimisationofasystem
generativesurrogatemodel.Also,[30]and[38]exploitthearchitec-
∗CorrespondingAuthor.Email:tramazyan@hse.ru. tureofgenerativemodelsusedforoptimisation-[30]optimisesina
Github:https://github.com/ramazyant/wu-go latentspacethataDGMlearns,and[38]employtheinvertibilityof
4202
luJ
61
]GL.sc[
1v71911.7042:viXradiffusionmodelstosolvetheinverseoptimisationproblem.Further, ofµisreasonable.Itisdenotedasµ ∈ P ,whereP isthesetof
2 2
[26]aimstofindoptimalparametersofaDGMthatapproximatea probabilitymeasureswithfinitesecondmoment.
fixedblackboxmetricthebest. Asmentionedearlier,inblackboxoptimisationsimulatedsam-
Inpractice,non-differentiableobjectivefunctionsareoftenneeded. plestypicallyrepresentdisjointregionsoftheparametersspace.This
InthiscaseBayesianoptimisation(BO)[57,53,58],geneticalgo- meansthatthereislittletonoinformationaboutmostofthesearch
rithms[51,7],ornumericaldifferentiation[61]arepreffered.BO space.Inaddition,theprobabilitymeasureµisunknown.Thus,fol-
is typically equipped with a Gaussian process (GP), which is de- lowing[42],weconsidertheproblemofoptimisationunderuncer-
finedbyameanfunctionandakernelfunctiondescribingcovariance tainty-aproblemtackledviadistributionallyrobustoptimisation.The
shape.BOrequiresGPcovariancematrixinversionwithO(n3)cost problemofuncertaintyestimationamountstoevaluatingtheworst-
intermsofthenumberofobservations,whichischallengingfora caseriskoftheobjectivefunctionoveranambiguitysetP ⊂P (Rd):
2
largenumberofresponsesandinhigherdimensions.TomakeBO
scalable[17,19,28,65]consideralow-dimensionallinearsubspace supEµ[f(θ,x)]. (1)
anddecomposeitintosubsetsofdimensions,i.e.,somestructural µ∈P
assumptionsarerequiredthatmightnothold.Inaddition,GPrequires Generally,ambiguitysetsareusedtoensurestatisticalperformance
aproperchoiceofkerneliscrucial.BOmayneedtheconstructionof andcomputationalfeasibility[22].Inthiswork,weusetheconstruc-
newkernels[31].[21]proposesagreedyapproachforautomatically tionofconsideredambiguitysets,Wassersteinballstobeprecise,and
buildingakernelbycombiningbasickernelssuchaslinear,exponen- thetopologytheyinducetoquantifyuncertainty.TheWasserstein
tial,periodic,etc.,throughkernelsummationandmultiplication. metric was studied for distributionally robust optimisation in Ref.
BOisnotboundtouseGP.Forexample,deepGaussianprocesses [27,8,39].
(DGP)[15,35,29]attempttoresolvetheissueoffindingthebest Thus,wefindtheoptimalconfigurationbysolvingthefollowing
kernelforaGP.ThatisdonebystackingGPsinahierarchicalstruc- optimisationproblem:
ture as Perceptrons in a multilayer perceptron, but the number of
variationalparameterstobelearnedbyDGPsincreaseslinearlywith inf supEµ[f(θ,x)]. (2)
thenumberofdatapoints,whichisinfeasibleforstochasticblack-box θ∈Θµ∈P
optimisation,andtheyhavethesamematrixinvertingissueasGPs, Theoptimaldesignisfoundbyselectingapointofpotentialmini-
whichlimitstheirscalability. mumamongasetofexploredcandidatesandcallingtheblackboxfor
OthersurrogatemodelsthatarereasonabletoconsiderforBOare saidpoint.Consequentlyfillingthesearchspacewithgroundtruths
Bayesianneuralnetworks(BNN)[37,59]andensemblemodels[63]. andexploitingtheobtainedinformationtoincreasetheconfidenceof
BNNdirectlyquantifyuncertaintyintheirpredictionsandgeneralise thepredictedoptimalconfiguration.
betterthanotherneuralnetworks.However,theirconvergencemay
besloweraseachweightistakenfromadistribution.Thisalsomeans
3.2 DeepGenerativeModelsforSurrogateModelling
that BNN might require much more data than GPs to yield good
predictions. Havingageneratormodelthatwouldmapasimpledistribution,Z,
Ontheotherhand,anintuitiveapproachforuncertaintyquantifi- to a complex and unknown distribution, p(µ), is desired in many
cation is using an ensemble surrogate model, e.g., an adversarial settingsasitallowsforthegenerationofsamplesfromtheintractable
deepensemble(DE)[41,63].Singlepredictorsoftheensembleare data space. For our purposes, conditional deep generative models
expectedtoagreeontheirpredictionsoverobservedregionsofthe (DGMs)areusedtomodelthestochasticresponseofthesimulator.
featurespace,i.e.,wheredataisgivenandsouncertaintyislow,and Thisisparticularlyadvantageousinexploringunknownregionsof
viceversa.Thefurtherthesesinglepredictorsgetfromknownregions theparameterspace.Byparametrisingtheresponsespaceweexploit
ofthefeaturespace,thegreatershouldbethediscrepancyintheir deepgenerativemodelsinshowingthattheycanbeusedforboth
predictions. approximatingthe objectivefunction andmodelling thesimulator
Thechoiceoftheabove-mentionedsurrogatesisnotrandom.Inthis forthewholefeaturespaceandhencealsocomputingtheproposed
work,wefocusonapproachesthattakeadvantageoftheuncertainty uncertainty.
incorporatedinthepredictiveposteriorsofthesurrogate.GP,DGP, ThegoalofDGMistoobtainageneratorG:Rs →Rdthatmaps
andBNNhaveadistributionastheiroutputs,andDEcomprisesone agivendistribution,e.g.,aGaussian,Z supportedinRs toadata
throughitssinglepredictors. distributionp(µ)supportedinRd.
Accordingtotheproblemstatement,eachsampleisassociatedwith
aspecificconfiguration,i.e.,avectorofparametersθ,soweconsider
3 Method
conditionalDGMs.ThenthegoalistoobtainG : Rs×Θ → Rd
3.1 ProblemStatement suchthatdistributionsG(Z;θ)andp(µ;θ)matchforeachθ ∈ Θ.
SinceZ andp(µ)areindependent,wegetG(Z;θ) ∼ p(µ;θ).For
Indesignoptimisation,oneaimstofindaconfigurationofthesystem simplicityofnotationwedenoteG(Z;θ)asG(θ).
θinthesearchspaceΘ⊆Rm,whichminimisestheexpectedcost Thus,weexplorethesearchspacebytrainingadeepgenerative
Eµ[f(θ,x)]oftheblackbox.f isareal-valuedfunctionf : Θ× surrogateGonthesetofgroundtruthsM.Foreachvalueθ ∈ Θ,
Rd → R, where x ∼ µ ∈ P(Rd), and P(Rd) is the space of wecangeneratethecorrespondingresponseofthesimulatorν(θ)=
BorelprobabilitymeasuresonRd.Thismeansthatf ispointwise G(θ),i.e.,νisthepredictivedistributionatθ.
observable, defined over a set of parameters Θ. In this work, we
takecontinuousΘ.Weassumethatf isboundedfrombelowand
3.3 WassersteinUncertainty
Lipschitzcontinuous.ThesetΘisexplicitandnon-relaxable,i.e.,
itdoesnotrelyonestimatesandf cannotbecomputedoutsidethe IntheusualBayesianoptimisationformulation,theepistemicuncer-
feasibleregion.Theassumptionoffinitefirstandsecondmoments taintyestimatoristhevarianceofthepredictiveposteriordistribution.Sincethesimulatorisstochastic,weaimtohaveanuncertaintyesti- Lemma2. Forapredictedrandomvariableνandasetofground
matorthatwouldaccountfortheintricaciesofbothsampledensity truthsM
andthegeometryoftheobjectivefunction.Hence,weproposethe σW(θ)=0⇔∃µ∈M:µ=ν. (7)
WassersteinUncertainty-aWassersteindistance-baseduncertaintyes-
timatorthatencapsulatesthedifferencebetweenknownandpredicted CalculatingthetrueWassersteindistancewouldaddcomputational
samplesinbothdistributionandtheL space. coststhatwouldnotmarginallybenefitthealgorithm’sperformance.It
2
istypicallyapproximatedwiththecorrespondingL norm.Inallour
p
Definition1. 2-Wassersteindistance,denotedbyW,istheoptimal experimentsweconsideronlyunivariatereal-valuedrandomvariables.
transportcostbetweentwoprobabilitydistributionsµ,ν ∈P 2with AccordingtoRef.[62],L 2andEnergydistancesareequivalentinthis
L2 2costfunction: case.
(cid:18) (cid:90) (cid:19)1
W(µ,ν)= inf |x−y|2dπ(x,y) 2 . (3)
π∈Π(µ,ν) D2(µ,ν)=2E∥X−Y∥−E∥X−X′∥−E∥Y −Y′∥, (8)
Π(µ,ν)isthesetofcouplingsofµandν.TheWassersteindistance
whereX,X′ ∼ F ,Y,Y′ ∼ F ,andF ,F aretheCDFsofµ
definesasymmetricmetriconP . µ ν µ ν
2
andν respectively.WeestimatetheWassersteinuncertainty,Eq.6,
Optimisationoveranambiguitysetallowsustodescribetheregions forapredictiveposteriorG(θ)viaEnergydistanceasfollows:
oftheparameterspacewithcertainlevelsofuncertaintywithrespect
togivengroundtruths.Ambiguitysetsareusuallydefinedtoensure σˆW(θ)= minD(µ,G(θ)). (9)
µ∈M
statisticalperformanceguaranteesandcomputationaltractability[48,
44].ApopularchoiceofambiguitysetsareWassersteinballs[64,52].
3.4 Optimisation
Definition2. Wassersteinballofradiusεandcentredatν,denoted
byB (ν),istheclosurewithrespecttothetopologyinducedbythe Bayesianoptimisation(BO)[46]isperhapsthemostcommonap-
ε
Wassersteindistanceasfollows: proachforglobaloptimisation.Itisbasedonacquisitionfunctions.In
BOtheacquisitionfunctionisrepeatedlyapplieduntilconvergence.
B (ν):={µ∈P (Rd):W(µ,ν)≤ε}. (4) Authorsoftheefficientglobaloptimisation(EGO)algorithm[36]
ε 2
proposedtheexpectedimprovement(EI)heuristictofullyexploit
Thissignificantlyeasesthesearchforapotentialoptimum,asfora
thepredictiveuncertainty.Potentialforoptimisationislinkedviaa
candidateνitsuncertaintyamountsto:
measureofimprovement-arandomvariabledefinedforaninput
θ∈Θas
F(µ)= sup Eµ[f]. (5)
µ∈Bε(ν)
I(θ)=max{0,f −ν(θ)}, (10)
min
The uncertainty of ν is associated with the closest known re-
sponse, i.e., for any prediction ν we consider µ˜ ∈ M such that wheref min isthebestobjectivevalueobtainedsofar,andν(θ)
µ˜=arginf W(µ,ν).Fromtheuncertaintyestimationperspec- isthepredictivedistributionofthefittedmodelatθ.Ifν(θ)hasa
µ∈M
tive,wecanbeascertainaboutν asabouttheclosesttoitµ˜ ∈M. nonzeroprobabilityoftakinganyvalueontherealline,thenI(θ)has
This also agrees with [39], where a decision under uncertainty is anonzeroprobabilityofbeingpositiveatanyθ.
modelledwithareal-valuedlossfunction,andtheriskoftheoptimal To target potentials for large improvements more precisely, the
decisionistheleastriskyadmissibleloss. EGOalgorithmaimstomaximiseexpectedimprovementEI(θ)=
Consider B (ν) a Wasserstein ball centered at ν with radius E[I(θ)]. In predictive posterior agnostic settings, EI is calculated
ε
ε = W(ν,µ˜), i.e., such that µ˜ is located at the boundary of the throughMCapproximation
ambiguityset.ConsideringanyotherradiusofB (ν)wouldyield
ε M
eitheroverestimationorunderestimationoftheuncertaintyofν. 1 (cid:88)
EI(θ)≈ max{0,f −x }, (11)
Infact,foranapproximationoff oftheformfˆ=⟨w,x⟩,e.g.,a M min j
j=1
neuralapproximator,thesupremumisattainedattheboundaryofthe
Wassersteinball. where{x j}M j=1,x j ∼ ν(θ).AsM → ∞theapproximationbe-
comesexact.Ifν(θ)isGaussianwithmeanµ(θ)andvarianceσ2(θ),
Lemma1. (Existenceofaminimumattheboundary,[42])There e.g.,asinGaussianProcesses,thenEIacceptsthefollowingclosed
existsaworst-caseprobabilitymeasureµ∗attainingthesupremum
form.
(5)suchthatW(ν,µ∗)=ε.
Ingeneral,findingµ∗isataskofitsown,whichwouldaddcompu- EI(θ)=σ(θ)[z(θ)·Φ(z(θ))+ϕ(z(θ))], (12)
tationalcoststotheoptimisationproblem.Andνwouldbeequidistant wherez(θ)= fmin−µ(θ) andΦandϕareGaussianCDFandPDF
σ(θ)
totheclosestknownresponse,µ˜,andtheworst-caseriskoftheobjec- respectively.
tivefunction,µ∗.Thus,forapredictedνwequantifyitsuncertainty
Another common BO approach is lower confidence bound
as: (LCB)[40].Itsacquisitionfunctionisregretdefinedasfollows:
σW(θ)= inf W(µ,ν). (6) R(θ)=µ(θ)−κ·σ(θ). (13)
µ∈M
ForσW tobeaviableuncertaintyestimator,itmustbezerofor Itisalinearcombinationofexploitation,µ(θ),andexploration,
knownresponsesandviceversa.Lemma2followsfromthefactthat σ(θ).Thetrade-offbetweenthetwoiscontrolledviathehyperpa-
Wisadistance.TheproofisprovidedinAppendixA. rameterκ.Smallerκyieldsmoreexploitation,andlargervaluesofκyieldmoreexplorationofhigh-varianceresponses,whereuncertainty Algorithm1WassersteinUncertaintyGlobalOptimisation(WU-GO)
ishigher,i.e.,wheretheblackboxismoreunknown.
Input:GroundtruthsM,generatorG,candidatesΘ˜,parameterκ
Weomitanyassumptionsonthefunctionalformoftheposterior
Output:Optimalconfigurationθˆ∗
distributionandthecomputationalandsamplingchallengesofMonte
Carloestimates.LCBcanadjustexplorationandexploitation,and
whilestoppingcriteriaarenotmetdo
EGO considers the predictive posterior distribution explicitly. We
FitGonM
proposetocombinebothapproachesandaccountforthestochasticity
Approximatef:fˆ(θ)=E[G(θ)]≈ 1 (cid:80)n x ,x ∼G(θ)
notintheobjectiveexplicitlyasinEGO,butimplicitlyasinLCB. n j=1 j j
Thus,usingtheproposedinEq.6Wassersteinuncertainty,wesuggest
EstimateσW:σˆW(θ)=min µ∈MD(µ,G(θ))
thefollowingformulationofregret:
Predictθˆ=argmin θ∈Θ˜{fˆ(θ)−κ·σˆW(θ)}}
Callsimulatorforθˆ:µˆ
RW(θ)=µ(θ)−κ·σW(θ) (14) M=M∪{µˆ}
endwhile
Thisalsoagreeswiththeoptimalsolutionofsupremum(5)fora
neuralapproximator(seeProposition4.4inRef.[42]).
Toselectthevalueofκ,weconductanablationstudy.Thevalue
OptimisationofacquisitionfunctionssuchasEIcanbedonenu-
κ = 2isthebestamongtheconsideredones(seeAppendixCfor
merically.Theproposedacquisitionfunction,Eq.14,isnon-convex
moredetails).
andnon-differentiable,whichcomplicatesitsoptimisation.Wedo
itviadirectsearchoveralargesetofcandidatepoints.Itsimplifies
thesearchforthenextbestsetofparametersbutlimitstheprecision 3.7 SurrogateModelandBaselines
oftheoptimisationalgorithm.Onecoulditerativelyupdatetheset
WU-GO is not bound to any specific generative surrogate model.
ofcandidatepoints,andthusimproveoptimisationresults.However,
Inthiswork,wetakeaconditionalWGAN-GP[32].UsingGANs
suchastrategymayyieldanunfeasiblylargenumberofcandidates.
is advantageous for faster sampling, i.e., faster exploration of the
Forconsistencyofcomparison,weusethedirectsearchapproachfor
searchspace.Gradientpenaltyprovidesabetterobjectivefunction
allmethods.Thisissuewillbeaddressedinourfutureworks.
approximation, and the Wasserstein uncertainty could be directly
estimatedfromthesurrogatemodel,whichwouldbehighlyefficient,
3.5 Convergence
especiallyformultidimensionaloutputs.However,[43]and[60]argue
thatforthetrueWassersteindistance,oneshouldalterthetrainingof
Undersometheoreticalassumptionsconvergenceratescanbeguar-
theWGAN.
anteed.Authorsof[10]provideconvergenceratesforexpectedim-
Asbaselines,wetakeEGOandLCBwithGaussianposterior.As
provementalgorithms.However,sinceinpracticepriorsareestimated
surrogatesforthesealgorithms,wetakeGaussianprocessregression,
fromdata,standardestimatorsmaynotholduptotheseconvergence
DeepGaussianProcesses,Bayesianneuralnetworks,andadversarial
rates,andthustheyneedtobealtered.
deepensembles.Aneffortwasmadetomakeallmodelsassimilar
Onemaysearchforamoreefficientmethodwithimprovedaverage-
aspossibletoachievemoreconsistentresults.AppendixBcontains
caseperformancethatstillensuresreliableconvergence.Thechal-
modelparametersandexperimentsettingsforreproducibility.
lengeindevelopingsuchamethodliesinbalancingexplorationand
exploitation,aswithLCBorWU-GO.Exploitingthedataspecifically
inregionsofthesearchspacewheretheblack-boxfunctionisknown 4 Experiments
tobesuboptimalmayhelpusfindtheoptimumquickly.However,
withoutexploringallregionsofthesearchspace,wemightneverfind WeevaluateWU-GOperformanceusingeightexperiments.Areal
theoptimaldesignnorbeconfidentintheoptimalityoftheproposed workingexampleofahighenergyphysicsdetectorandsevenexperi-
solution[45].Thisisfurtherdiscussedintheablationstudyreported mentsbasedoncommonlyusedoptimisationobjectivefunctions.In
inAppendixC. eachsynthetictest,wechangethesettingsoftheexperimenttoexam-
In general, to converge to the exact solution, as with any other inethemodelsinvariousscenarios.Ashortdescriptionisprovided
gradient-freeoptimisationapproach,itmaytakeaninfiniteamount belowandforafulldescriptionseeAppendixD.
ofsimulatorcalls.Onemightdeduceaworst-casescenario,butit
mayrequireanunfeasiblylargenumberofsimulatorcalls.Sincesuch Table1. Experiments.
scenariosareunlikelyinpracticeandthenumberofsimulationsis BLACKBOXFUNCTION N n Θ
limited,itmakessensetorelaxtheguaranteestoaddresspractical
THREEHUMPCAMEL 4 102 [−5,5]2
constraintswhilestillfeasiblesolutions.
ACKLEY 4 102 [−5,5]2
LÉVI 4 102 [−4,6]2
3.6 Algorithm HIMMELBLAU 4 101 [−5,5]2
ROSENBROCK 121 102 [−2,2]8
The proposed algorithm is summarised in Algorithm 1. Our al- ROSENBROCK 25 102 [−2,2]20
gorithmrequiresagenerativesurrogatemodelandchoosesthenext STYBLINKI-TANG 25 102 [−5,5]20
pointminimisingRWfromEq.14.
AsstatedinAlgorithm1,theblack-boxfunctionisevaluatedover
asetofcandidatevaluesrepresentedviaagridΘ˜ ⊂Θrepresenting Similarlytoexperimentsof [6],allinstancesarestochastic.Each
thesearchspace.Thisisanumericallimitation,astheWasserstein experiment is defined by the black-box response shape. Namely,
uncertaintyisnotdifferentiable.Ontheotherhand,thisallowsusto N(f(θ),10−2),wheref(θ)istheblack-boxfunction.Theonlyex-
easilyconsiderconfigurationsandsubspacesofinterest,anddefine ception is the Lévi experiment, where the variance response is a
stoppingcriteria,i.e.,anε-solutioncriteria. bimodalGaussianchangingtheshapewiththefunction.Figure1. Experimentresults-comparisonofEGOandWU-GO.SeeSection4.1formoredetails.
Table1providestheparametersofeachexperiment.Foreachtest resultsforeachexperimentarehighlightedingreen,yellow,andred
function,weconduct10runsforN differentstartingconfigurations respectively.
chosenviaLatinhypercubesampling.Eachresponseisrepresented InthemainbodyofthispaperwecompareWU-GOtoEGOwith
byasampleofsizen.Theoptimisationisconductedoverthesearch baseline surrogates, as described in Section 3.7. The discrepancy
spaceΘamongasetofcandidatepointsΘ˜ comprisedof1012points. between WU-GO and EGO results is less evident and should be
discussed.BaselineswiththeLCBapproachperformedsignificantly
worse.WeplaceLCBresultsinAppendixE.
4.1 Results
AfasterdecreaseofmeanvaluesinFig.1impliesfasterconver-
genceofthealgorithm.Smallervaluesofstandarddeviationmean
Weevaluateallmodelsintermsofdistancetooptimumandspeedof
higherconfidenceinpredictions,i.e.,consistencyofthemeanvalue.
convergence.Aseachiterationofanoptimisationalgorithmwould
Acombinationofthetwoimpliestheefficiencyofthemodel-not
requireacallofthesimulator,wemeasurethespeedofconvergence
onlydoesthemodelconvergebutitisalsoconfidentinitspredictions.
asthenumberofsimulatorcalls.InFig.1wereportthemeanandthe
Inexperimentswithtwo-dimensionalsearchspaces,WU-GOat-
standarddeviationofthedistancetominimumw.r.t.thenumberof
tainsminimamoreefficientlythanallselectedbaselines.Thatisnot
simulatorcalls.
thecasefor,e.g.,DEintheLéviexperimentorBNNintheHimmel-
Table2andTable3provideinsightsonresultsfromFig.1ina
blauexperiment,inwhichstandarddeviationspreadsoveralarge
cross-sectionalmanner.Table2showcasestheprobabilityofamodel
portionofthepresenteddomain.Amongbaselines,DGP’sperfor-
toyieldanε-solutionwithinthefirst100iterations.Thatisdoneby
manceistheclosesttothatofWU-GO.
consideringeachmodelresultasaBernoullirandomvariablewiththe
TheHimmelblautestshowsthatdespiteresponsesbeingunder-
probabilityofsuccessbeingtheproportionofrunswhenthemodel
represented-only10observationsweresimulatedforeachsample,
metthestoppingcriteria.Table3isanumericalrepresentationof
WU-GOmanagestooutperformallbaselines.Itislikely,thatitwould
Fig.1-itshowsthedistancetooptimaafter50simulatorcalls.Top-3Table2. Probabilitytoyieldanε-solutionwithinthefirst100iterations.
BLACKBOXFUNCTION WU-GO BNN DE GP DGP
THREEHUMPCAMEL 1.0±0.000 0.9±0.095 0.1±0.095 0.2±0.126 1.0±0.000
ACKLEY 1.0±0.000 0.2±0.126 0.1±0.095 0.4±0.155 0.9±0.095
LÉVI 1.0±0.000 0.5±0.158 0.0±0.000 0.2±0.126 0.9±0.095
HIMMELBLAU 1.0±0.000 0.4±0.155 0.0±0.000 0.2±0.126 0.7±0.145
ROSENBROCK(8DIM.) 0.6±0.155 0.0±0.000 0.1±0.095 0.9±0.095 0.5±0.158
ROSENBROCK(20DIM.) 0.3±0.145 0.0±0.000 0.8±0.126 0.0±0.000 0.0±0.000
STYBLINKI-TANG 0.8±0.126 0.0±0.000 0.0±0.000 1.00±0.000 0.3±0.145
Table3. Distancetooptimumafter50simulatorcalls.
BLACKBOXFUNCTION WU-GO BNN DE GP DGP
THREEHUMPCAMEL 0.071±0.000 0.089±0.035 1.286±0.796 0.326±0.240 0.131±0.151
ACKLEY 0.089±0.035 0.272±0.198 1.109±0.779 0.283±0.209 0.146±0.091
LÉVI 0.089±0.035 0.157±0.105 0.996±0.978 0.329±0.138 0.126±0.074
HIMMELBLAU 0.058±0.034 0.220±0.172 0.760±0.307 0.397±0.180 0.103±0.063
ROSENBROCK(8DIM.) 1.226±0.188 1.977±0.317 2.048±0.363 1.452±0.624 1.270±0.270
ROSENBROCK(20DIM.) 3.934±1.460 4.205±0.623 2.210±0.398 4.483±0.827 4.667±0.837
STYBLINKI-TANG 5.993±4.452 10.490±1.479 11.233±1.924 2.067±0.028 7.651±2.334
not be able to reproduce the original densities precisely. Still, the parameters:length,widthsatbothends,heights,andair-gapwidths
black-boxfunctionapproximationisgoodenoughfortheoptimisa- betweenthefieldandreturnfield.Overallweconsider22parameters
tiontask. describingthearchitectureandthegeometryofthedetector.
ThisisallevidencethatWU-GOcarriesoutoptimisationefficiently
ofcomplexfunctions,e.g.,AckleyandLévi,aswellasitdoeson
simplertestssuchastheThreeHumpCamel.ThissignalsthatWU-
GOshouldbemorerobustingeneralthanEGO.Itisworthnoting
thatresultsacrossallfourtwo-dimensionalexperimentsaresimilar.
Thisshowstheconsistencyofallmodels.
Asthedimensionalityofthesearchspaceincreasesconvergence
is expected to decrease. We see this effect on WU-GO. However,
the only time DE converge and shows a stable performance is in
the 20-dimensional Rosenbrock experiment. GP behave similarly.
Although GPconvergesin the 8-dimensionalRosenbrock and the
20-dimensional Styblicky-Tang experiment, they fail with the 20-
dimensionalRosenbrock.EGOmaymarginallyoutperformWU-GO,
buttheperformancevariessignificantlydependingonthesurrogate
model.Thisshouldmakeonequestionthestabilityandtheconsistency Figure2. SHiPMuonShieldoptimisationresults.
ofEGO.
Theobjectivefunctionofthemuonshieldoptimisationisacombi-
AlthoughWU-GO’sperformanceworsens,itconvergesinoverhalf
nationofshieldingefficiencyandamasscostfunction.Wecompare
oftherunsandistheonlyothermodeltoshowsuchresultsexcept
theresultofWU-GOwithapreviouslyobtainedresultbyBayesian
thepreviouslymentionedDEandGPonspecifictests.WU-GO’s
optimisationwithaGaussianprocesssurrogate.Thedesignsuggested
performancecanbeimprovedbytuningκ.Itdoesrequireadditional
viaBayesianoptimisationwasapprovedfortheconstructionofthe
computations,buttheresultingperformancemaybeworthitasinthe
experiment.
two-dimensionalexperiments(seeablationstudyinAppendixCfor
Withinthefirst300iterationsWU-GOsuggestedaconfiguration
moredetails).
oftheSHiPexperimentwithalmostidenticalperformanceasthat
suggestedbyBayesianoptimisation.IncontrastwithBayesianop-
4.2 Physicsexperiment timisation,WU-GOalsosuggestedsomelightweightandcompact
designs.Althoughtheshieldingefficiencyissuboptimal,thesmaller
ThemuonshieldisanessentialelementoftheSearchforHidden dimensionsofthemuonshieldmayallowustocombinethemuon
Particles(SHiP)experiment,deflectingtheabundantmuonfluxgener- shieldwiththerestoftheexperimentinamoreefficientmanner,i.e.,
atedwithinthetargetawayfromthedetector,otherwiseasubstantial minimisethebackgroundnoise.Thiswillbethesubjectofafuture
backgroundforparticlesearches.Muondeflectionoutofthespec- workofours.
trometer’sacceptancebyamagneticfieldisstraightforward;however, ThestartingconfigurationisdescribedinRef.[3]andallabove-
thechallengeliesinthewidedistributionofmuonsinphasespace. mentioned muon shield configurations can be found in Appendix
Toaddressthis,SHiPusesbothmagneticandpassiveshielding,as F.
detailedinRef.[3],tosafeguardtheemulsiontargetfrommuons. ItisworthnotingthattheBayesianoptimisationstopsafter2500
To refine the muon shielding and optimize its configuration, a iterations because it runs out of memory. WU-GO is constant in
GEANT4[23]simulationwasdevelopedtotrackmuons’trajectories memory.Hencewearenotaslimitedinthenumberofoptimisation
throughthemagnets.Eachmagnet’sspecificationsencompassseven loopiterationsandwemaygatherasmuchstatisticsaswewouldlike.Infieldssuchashighenergyphysics,thisfeatureissignificant. [3] A. Akmete et al. The active muon shield in the ship experiment.
BayesianoptimisationwithaGPsurrogatemodelmaybethemost JournalofInstrumentation,12(05):P05011,may2017. doi:10.1088/
1748-0221/12/05/P05011.URLhttps://dx.doi.org/10.1088/1748-0221/
common approach to a design optimisation problem. With a GP
12/05/P05011.
surrogate,thefollowingtwoissuesmayalsoarise. [4] S.Alarie,C.Audet,A.E.Gheribi,M.Kokkolaras,andS.LeDigabel.
Thefirstchallengeoccurswhengroundtruthobservationscoverthe Twodecadesofblackboxoptimizationapplications.EUROJournalon
ComputationalOptimization,9:100011,2021.
searchspacewithlargegapsinbetween.Aspreviouslymentioned,this
[5] G. Amadio, J. Apostolakis, M. Bandieramonte, S. Behera, R. Brun,
isageneraloccurrenceinblack-boxoptimisation.Especially,when P.Canal,F.Carminati,G.Cosmo,L.Duhem,D.Elvira,etal.Stochastic
thesearchspaceisofhigherdimensionality,e.g.,the22-dimensional optimization of geantv code by use of genetic algorithms. In Jour-
searchspaceofSHiPexperimentparameters.Inthiscase,GPtends nalofPhysics:ConferenceSeries,volume898(4),page042026.IOP
Publishing,2017.
tounderestimatetheuncertaintywithinthegapsofthesearchspace.
[6] C.Audet,J.Bigeon,R.Couderc,andM.Kokkolaras.Sequentialstochas-
Thesecondissueariseswhenthevarianceofgroundtruthobserva- ticblackboxoptimizationwithzeroth-ordergradientestimators,2023.
tionsdependsonthederivativeoftheblackboxfunction.Thenthe [7] W.Banzhaf,P.Nordin,R.E.Keller,andF.D.Francone. Geneticpro-
gramming:anintroduction:ontheautomaticevolutionofcomputer
GPvarianceestimatestendtothevarianceofallgroundtruthsamples,
programsanditsapplications.MorganKaufmannPublishersInc.,1998.
i.e.,someaveragevalue.Thusthevarianceisgenerallymispredicted.
[8] J.BlanchetandK.R.A.Murthy.Quantifyingdistributionalmodelrisk
BothareillustratedontoyexamplesinAppendixG.TheGPisfitted viaoptimaltransport,2017.
wellinthesense,thattheblack-boxfunctionapproximationisclose. [9] D.Brookes,H.Park,andJ.Listgarten.Conditioningbyadaptivesam-
plingforrobustdesign.InInternationalconferenceonmachinelearning,
However,theaforementionedissuesofmispredictedvariancemay
pages773–782.PMLR,2019.
yieldmispredictionsofbothEGOandLCBoptimisationalgorithms. [10] A.D.Bull.Convergenceratesofefficientglobaloptimizationalgorithms.
JournalofMachineLearningResearch,12(88):2879–2904,2011.URL
http://jmlr.org/papers/v12/bull11a.html.
5 Conclusion [11] M.B.Chang,T.D.Ullman,A.Torralba,andJ.B.Tenenbaum. A
compositional object-based approach to learning physical dynamics.
Weproposeanovelapproachforgradient-freeoptimisationofstochas- CoRR,abs/1612.00341,2016.URLhttp://arxiv.org/abs/1612.00341.
ticnon-differentiablesimulators.Thealgorithmisbasedonthecon- [12] R.Chen,M.Menickelly,andK.Scheinberg. Stochasticoptimization
ceptofWassersteinballsasambiguitysetsforuncertaintyquantifi-
usingatrust-regionmethodandrandommodels.MathematicalProgram-
ming,169:447–487,2018.
cation.Itusesadeepgenerativesurrogatemodel,whichallowsusto
[13] A.R.Conn,N.I.Gould,andP.L.Toint.Trustregionmethods.SIAM,
modelandoptimiseentitiesfromsimpleGaussianrandomvariables 2000.
tomorecomplicatedcases.Weperformexperimentsonarealhigh [14] A. Cozad, N. V. Sahinidis, and D. C. Miller. Learning surrogate
models for simulation-based optimization. AIChE Journal, 60(6):
energyphysicsdetectorsimulatorandaseriesoftoyproblemscov-
2211–2227,2014. doi:https://doi.org/10.1002/aic.14418. URLhttps:
eringawiderangeofpossiblerealexperimentsettings.Ourmethod, //aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.14418.
WU-GO,iscomparedwithefficientglobaloptimisationandlower [15] A. Damianou and N. D. Lawrence. Deep Gaussian processes. In
confidenceboundwithBayesianneuralnetwork,adversarialdeep C. M. Carvalho and P. Ravikumar, editors, Proceedings of the Six-
teenthInternationalConferenceonArtificialIntelligenceandStatistics,
ensemble,Gaussianprocess,anddeepGaussianprocessassurrogate
volume31ofProceedingsofMachineLearningResearch,pages207–
models.WU-GOattainsminimamoreefficientlyincomparisonwith 215,Scottsdale,Arizona,USA,29Apr–01May2013.PMLR. URL
allselectedbaselines,withDGPresultsbeingtheclosest-typically https://proceedings.mlr.press/v31/damianou13a.html.
[16] F. de Avila Belbute-Peres, K. Smith, K. Allen, J. Tenenbaum, and
worsebyamarginofonestandarddeviation.Experimentsalsosug-
J. Z. Kolter. End-to-end differentiable physics for learning and
gestthatourapproachisrobusttotheshapeofboththeresponse control. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
and the objective function. WU-GO is easily extendible to higher N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural In-
dimensionsoftheparametersspacebutmightrequiretuningofthe formation Processing Systems, volume 31. Curran Associates, Inc.,
2018.URLhttps://proceedings.neurips.cc/paper_files/paper/2018/file/
explorationv.exploitationhyperparameterκ.Ourmethodmaynot
842424a1d0595b76ec4fa03c46e8d755-Paper.pdf.
justmitigateissuesofcommonlyusedapproachesbutresolvethem-a [17] N.deFreitasandZ.Wang.Bayesianoptimizationinhighdimensionsvia
deepgenerativesurrogatemodelcanhandlehigh-dimensionalinputs, randomembeddings.InProceedingsoftheTwenty-ThirdInternational
JointConferenceonArtificialIntelligence,2013.
yieldhighlyaccuratereconstructionforvariousrequiredconfigura-
[18] J.Degrave,M.Hermans,J.Dambre,andF.Wyffels. Adifferentiable
tions,andgeneratelargesampleswithlowcomputingtime.WU-GO physicsenginefordeeplearninginrobotics. CoRR,abs/1611.01652,
maybeauniversalapproachforoptimisationproblems,especially 2016.URLhttp://arxiv.org/abs/1611.01652.
fornon-differentiableobjectivesandwhenreconstructionplaysakey [19] J.Djolonga,A.Krause,andV.Cevher. High-dimensionalgaussian
processbandits.Advancesinneuralinformationprocessingsystems,26,
roleintheoptimisationpipeline.
2013.
[20] T.Dorigoetal.Towardtheend-to-endoptimizationofparticlephysics
instrumentswithdifferentiableprogramming. ReviewsinPhysics,10:
Acknowledgements
100085,2023. ISSN2405-4283. doi:https://doi.org/10.1016/j.revip.
2023.100085.URLhttps://www.sciencedirect.com/science/article/pii/
Theresearchleadingtotheseresultshasreceivedfundingfromthe
S2405428323000047.
BasicResearchProgramattheNationalResearchUniversityHigher [21] D.Duvenaud.AutomaticmodelconstructionwithGaussianprocesses.
SchoolofEconomics.Thisresearchwassupportedinpartthrough PhDthesis,UniversityofCambridge,2014.
[22] P.M.EsfahaniandD.Kuhn. Data-drivendistributionallyrobustop-
computationalresourcesofHPCfacilitiesatHSEUniversity.
timizationusingthewassersteinmetric:Performanceguaranteesand
tractablereformulations,2017.
References [23] S.A.etal. Geant4—asimulationtoolkit. NuclearInstrumentsand
MethodsinPhysicsResearchSectionA:Accelerators,Spectrometers,
[1] M.J.AbrahamandJ.E.Gready.Optimizationofparametersformolec- DetectorsandAssociatedEquipment,506(3):250–303,2003.ISSN0168-
ulardynamicssimulationusingsmoothparticle-meshewaldingromacs 9002.doi:https://doi.org/10.1016/S0168-9002(03)01368-8.URLhttps:
4.5.Journalofcomputationalchemistry,32(9):2031–2040,2011. //www.sciencedirect.com/science/article/pii/S0168900203013688.
[2] M.Aehleetal. Explorationofdifferentiabilityinaprotoncomputed [24] A.D.Flaxman,A.T.Kalai,andH.B.McMahan. Onlineconvexop-
tomographysimulationframework.PhysicsinMedicine&Biology,68 timizationinthebanditsetting:gradientdescentwithoutagradient,
(24):244002,dec2023. doi:10.1088/1361-6560/ad0bdd. URLhttps: 2004.
//dx.doi.org/10.1088/1361-6560/ad0bdd. [25] P.I.Frazier.Atutorialonbayesianoptimization,2018.[26] S.-W.Fu,C.-F.Liao,Y.Tsao,andS.-D.Lin. Metricgan:Generative tionwithmomentambiguitysets.JournalofScientificComputing,94
adversarialnetworksbasedblack-boxmetricscoresoptimizationfor (1):12,Dec.2022.ISSN1573-7691.doi:10.1007/s10915-022-02063-8.
speechenhancement.InInternationalConferenceonMachineLearning, [49] J.NocedalandS.J.Wright.Numericaloptimization.Springer,1999.
pages2031–2041.PMLR,2019. [50] M.Paganini,L.deOliveira,andB.Nachman.Acceleratingsciencewith
[27] R.GaoandA.J.Kleywegt.Distributionallyrobuststochasticoptimiza- generativeadversarialnetworks:Anapplicationto3dparticleshowers
tionwithwassersteindistance,2022. in multilayer calorimeters. Phys. Rev. Lett., 120:042003, Jan 2018.
[28] R.Garnett,M.A.Osborne,andP.Hennig. Activelearningoflinear doi:10.1103/PhysRevLett.120.042003.URLhttps://link.aps.org/doi/10.
embeddingsforgaussianprocesses. arXivpreprintarXiv:1310.6740, 1103/PhysRevLett.120.042003.
2013. [51] P.Y.PapalambrosandD.J.Wilde.Principlesofoptimaldesign:model-
[29] R. Gnanasambandam, B. Shen, A. C. C. Law, C. Dou, and ingandcomputation.Cambridgeuniversitypress,2000.
Z. Kong. Deep Gaussian Process for Enhanced Bayesian [52] S. M. Pesenti and S. Jaimungal. Portfolio optimization within a
Optimization and its Application in Additive Manufactur- wassersteinball.SIAMJournalonFinancialMathematics,14(4):1175–
ing. 6 2023. doi: 10.36227/techrxiv.23548143.v1. URL 1214,2023.doi:10.1137/22M1496803.URLhttps://doi.org/10.1137/
https://www.techrxiv.org/articles/preprint/Deep_Gaussian_Process_ 22M1496803.
for_Enhanced_Bayesian_Optimization_and_its_Application_in_ [53] B.Shahriari,K.Swersky,Z.Wang,R.P.Adams,andN.deFreitas.
Additive_Manufacturing/23548143. Takingthehumanoutoftheloop:Areviewofbayesianoptimization.
[30] R.Gómez-Bombarelli,J.N.Wei,D.Duvenaud,J.M.Hernández-Lobato, ProceedingsoftheIEEE,104(1):148–175,2016.doi:10.1109/JPROC.
B.Sánchez-Lengeling,D.Sheberla,J.Aguilera-Iparraguirre,T.D.Hirzel, 2015.2494218.
R.P.Adams,andA.Aspuru-Guzik.Automaticchemicaldesignusinga [54] J.R.Shewchuk. Anintroductiontotheconjugategradientmethod
data-drivencontinuousrepresentationofmolecules.ACScentralscience, withouttheagonizingpain.Technicalreport,USA,1994.
4(2):268–276,2018. [55] S. Shirobokov, V. Belavin, M. Kagan, A. Ustyuzhanin, and A. G.
[31] N.S.Gorbach,A.A.Bian,B.Fischer,S.Bauer,andJ.M.Buhmann. Baydin. Black-box optimization with local generative surro-
Modelselectionforgaussianprocessregression.InPatternRecognition: gates. In H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan,
39thGermanConference,GCPR2017,Basel,Switzerland,September and H. Lin, editors, Advances in Neural Information Processing
12–15,2017,Proceedings39,pages306–318.Springer,2017. Systems, volume 33, pages 14650–14662. Curran Associates, Inc.,
[32] I.Gulrajani,F.Ahmed,M.Arjovsky,V.Dumoulin,andA.C.Courville. 2020.URLhttps://proceedings.neurips.cc/paper_files/paper/2020/file/
Improvedtrainingofwassersteingans.Advancesinneuralinformation a878dbebc902328b41dbf02aa87abb58-Paper.pdf.
processingsystems,30,2017. [56] T.Sjöstrand,S.Mrenna,andP.Skands.Abriefintroductiontopythia
[33] A.GuptaandJ.Zou.Feedbackganfordnaoptimizesproteinfunctions. 8.1.ComputerPhysicsCommunications,178(11):852–867,2008.
NatureMachineIntelligence,1(2):105–111,2019. [57] J. Snoek, H. Larochelle, and R. P. Adams. Practical bayesian opti-
[34] M.U.Gutmann,J.Cor,ander.Bayesianoptimizationforlikelihood-free mization of machine learning algorithms. In F. Pereira, C. Burges,
inferenceofsimulator-basedstatisticalmodels. JournalofMachine L. Bottou, and K. Weinberger, editors, Advances in Neural Infor-
LearningResearch,17(125):1–47,2016. URLhttp://jmlr.org/papers/ mation Processing Systems, volume 25. Curran Associates, Inc.,
v17/15-017.html. 2012.URLhttps://proceedings.neurips.cc/paper_files/paper/2012/file/
[35] A.Hebbal,L.Brevault,M.Balesdent,T.El-Ghazali,andN.Melab. 05311655a15b75fab86956663e1819cd-Paper.pdf.
Bayesian optimization using deep Gaussian processes with appli- [58] J.Snoek,O.Rippel,K.Swersky,R.Kiros,N.Satish,N.Sundaram,
cations to aerospace system design - Optimization and Engineer- M.Patwary,M.Prabhat,andR.Adams.Scalablebayesianoptimization
ing — link.springer.com. https://link.springer.com/article/10.1007/ usingdeepneuralnetworks. InInternationalconferenceonmachine
s11081-020-09517-8#citeas,2020.[Accessed01-12-2023]. learning,pages2171–2180.PMLR,2015.
[36] D.R.Jones,M.Schonlau,andW.J.Welch.Efficientglobaloptimization [59] J.T.Springenberg,A.Klein,S.Falkner,andF.Hutter. Bayesianopti-
ofexpensiveblack-boxfunctions.JournalofGlobaloptimization,13: mizationwithrobustbayesianneuralnetworks.InD.Lee,M.Sugiyama,
455–492,1998. U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural
[37] I.Kononenko.Bayesianneuralnetworks.BiologicalCybernetics,61(5): InformationProcessingSystems,volume29.CurranAssociates,Inc.,
361–370,1989. 2016.URLhttps://proceedings.neurips.cc/paper_files/paper/2016/file/
[38] S.Krishnamoorthy,S.M.Mashkaria,andA.Grover.Diffusionmodels a96d3afec184766bfeca7a9f989fc7e7-Paper.pdf.
forblack-boxoptimization,2023. [60] J.Stanczuk,C.Etmann,L.M.Kreusser,andC.-B.Schönlieb.Wasser-
[39] D.Kuhn,P.M.Esfahani,V.A.Nguyen,andS.Shafieezadeh-Abadeh. steingansworkbecausetheyfail(toapproximatethewassersteindis-
Wassersteindistributionallyrobustoptimization:Theoryandapplications tance).arXivpreprintarXiv:2103.01678,2021.
inmachinelearning.InOperationsresearch&managementsciencein [61] K.Svanberg. Themethodofmovingasymptotes—anewmethodfor
theageofanalytics,pages130–166.Informs,2019. structuraloptimization. InternationalJournalforNumericalMethods
[40] T.LaiandH.Robbins. Asymptoticallyefficientadaptiveallocation inEngineering,24(2):359–373,1987.doi:https://doi.org/10.1002/nme.
rules.AdvancesinAppliedMathematics,6(1):4–22,1985.ISSN0196- 1620240207.URLhttps://onlinelibrary.wiley.com/doi/abs/10.1002/nme.
8858.doi:https://doi.org/10.1016/0196-8858(85)90002-8.URLhttps: 1620240207.
//www.sciencedirect.com/science/article/pii/0196885885900028. [62] G.Szekely.E-statistics:Theenergyofstatisticalsamples.Skandinavisk
[41] B.Lakshminarayanan,A.Pritzel,andC.Blundell.Simpleandscalable Aktuarietidskrift,012003.
predictiveuncertaintyestimationusingdeepensembles. Advancesin [63] J.YinandN.Li.Ensemblelearningmodelswithabayesianoptimization
neuralinformationprocessingsystems,30,2017. algorithmformineralprospectivitymapping.OreGeologyReviews,145:
[42] N.Lanzetti,S.Bolognani,andF.Dörfler. First-orderconditionsfor 104916,2022.ISSN0169-1368.doi:https://doi.org/10.1016/j.oregeorev.
optimizationinthewassersteinspace,2022. 2022.104916.URLhttps://www.sciencedirect.com/science/article/pii/
[43] H.Liu,X.GU,andD.Samaras. Atwo-stepcomputationoftheexact S0169136822002244.
GANWassersteindistance.InJ.DyandA.Krause,editors,Proceedings [64] M.-C.Yue,D.Kuhn,andW.Wiesemann.Onlinearoptimizationover
ofthe35thInternationalConferenceonMachineLearning,volume80of wassersteinballs,2021.
ProceedingsofMachineLearningResearch,pages3159–3168.PMLR, [65] M.Zhang,H.Li,andS.Su. Highdimensionalbayesianoptimization
10–15Jul2018.URLhttps://proceedings.mlr.press/v80/liu18d.html. viasuperviseddimensionreduction.arXivpreprintarXiv:1907.08953,
[44] F.LuoandS.Mehrotra.Distributionallyrobustoptimizationwithdeci- 2019.
siondependentambiguitysets.OptimizationLetters,14(8):2565–2594,
Nov.2020.ISSN1862-4480.doi:10.1007/s11590-020-01574-3.
[45] W. Macready and D. Wolpert. Bandit problems and the explo-
ration/exploitationtradeoff.IEEETransactionsonEvolutionaryCompu-
tation,2(1):2–22,1998.doi:10.1109/4235.728210.
[46] J. Mockus. Bayesian approach to global optimization: Theory and
applications.KluwerAcademicPublishers,1989.
[47] M.Mustafa,D.Bard,W.Bhimji,Z.Lukic´,R.Al-Rfou,andJ.M.Kra-
tochvil. Cosmogan:creatinghigh-fidelityweaklensingconvergence
maps using generative adversarial networks. Computational Astro-
physics and Cosmology, 6(1):1, May 2019. ISSN 2197-7909. doi:
10.1186/s40668-019-0029-9.
[48] J.Nie,L.Yang,S.Zhong,andG.Zhou.Distributionallyrobustoptimiza-AppendixA.Proofs
Lemma2.ForageneratedrandomvariableνandasetofgroundtruthsM
σW(θ)=0⇔∃µ∈M:µ=ν. (15)
Proof. Let’sstartbyprovingtheequivalencefromlefttoright.
σW(θ)=0⇔ inf W(µ,ν)=0⇒∃µ∈M:W(µ,ν)=0. (16)
µ∈M
AsW isadistance,fromthestatementabovewehavethatϕ=ψ.
2
Now,let’sprovetheequivalencefromrighttoleft.
Ifµ=ν,thenW(µ,ν)=0.AsWisnon-negative,inf µ∈MW(µ,ν)=0⇔σW(θ)=0.Hence∃µ∈MsuchthatσW(θ)=0.AppendixB.ModelsandReproducibility
• WU-GO:
– GANwithbothGeneratorandDiscriminatorarchitecture:Linear-Tanh-Linear;
∗ latentdimension:10;
∗ hiddensize:64;
∗ gradientpenalty:λ =1;
GP
∗ numberofdiscriminatoriterationspergeneratoriteration:5;
∗ Adamoptimiser,learningrate:10−3;
∗ numberofepochs:100.
– Schedulerstartsattheseconditerationoftheoptimisationalgorithm:
∗ scheduler:StepLR;
∗ startinglearningrate:10−1;
∗ multiplicativefactoroflearningratedecay:γ =10−1;
∗ periodoflearningratedecay:30epochs;
– Pythonlibraryofimplementation:PyTorch.
• BNN:
– architecture:BayesLinear-Tanh-BayesLinear;
– meanofpriornormaldistributionofBayesLinear:µ=0;
– sigmaofpriornormaldistributionofBayesLinear:σ=10−1;
– hiddensize:64;
– KLweight:10−2;
– numberofpredictionstoaverageover:10;
– Pythonlibraryofimplementation:TorchBNN.
• DE:
– Singleregressor:
∗ architecture:Linear-Tanh-Linear;
∗ hiddensize:16;
– numberofsingleregressors:10;
– Adamoptimiser;
– scheduler:StepLR;
– startinglearningrate:10−1;
– multiplicativefactoroflearningratedecay:γ =10−1;
– periodoflearningratedecay:6epochs;
– Pythonlibraryofimplementation:EnsemblePyTorch.
• GP:
– RBFkernel;
– Pythonlibraryofimplementation:GPy.
• DGP:
– architecture:RBFkernel-RBFkernel;
– hiddensize:10;
– Pythonlibraryofimplementation:PyDeepGP.
AsmentionedinSection3.7,wetrytokeepallmodelsassimilartoeachotheraspossibleaslongasitdoesnothurtmodelperformance.AppendixC.Ablationstudy
Figure3. Ablationstudyforoptimalvaluesofκ.
IncontrastwiththeEGOalgorithm,WU-GO,similarlytotheLCBapproach,hasahyperparametercontrollingtheexploration-exploitation
tradeoffofthealgorithm-κ.Thehyperparameterissimilartothelearningrateinagradient-basedoptimisation.ConvergenceofWU-GOhighly
dependsonκ.TheoptimalityofaspecificκiscontingentonthenumberofstartingpointsandthedimensionalityofthesearchspaceΘ.
WestudytheperformanceofWU-GOfordifferentvaluesofκ-weruntheThreeHumpCamel(2-dimensionalsearchspace)andthe
Rosenbrock(20-dimensionalsearchspace)experimentsforarangeofvaluesofκ.Fig 3showstheresultsoftheablationstudy.
IntheThreeHumpCamelexperimentweseethatκ=2.0issignificantlybetterthananyothertestedvalue.AsWU-GOwithotherκ’sdoes
notevenconverge.Thusforalltheabove-describedexperiments,wehadfixedthevaluesofκ=2.0.
Onewouldassumethatwithanincreaseinthesearchspacedimensionality,oneoughttoconsiderlargervaluesofκ.However,theablation
studyonthe20-dimensionalRosenbrockshowstheopposite.Withthesmallerκ=1.0,WU-GOconvergesbetterthanwiththedefaultκ=2.0.
Ontheotherhand,largervalues,e.g.,κ=16.0andκ=64.0,alsoimprovethepreviouslyobtainedresultstartingfromthefortiethiterationof
thealgorithm.
AsFig.1andFig.3suggest,randomlyselectingtheκcoefficientmayresultinsuboptimalpredictionsatbest.Ideally,itshouldbetunedfor
eachconfigurationofanexperiment.Thisdoescomewiththecostofadditionalteststorun.But,asdescribedinresultsobtainedfromnumerical
experiments,itmaybeworthitasWU-GOoutperformedEGOinmostexperiments.
Theoptimalityoftheκhyperparameterdependsnotonlyonthenumberofpointsinitiallygiventothemodelandthedimensionalityofthe
searchspacebutalsoontheirrelativeposition.Thisquestionshouldandwillbeinvestigatedinourfutureworks.AppendixD.Experimentsfulldescription
ThreeHumpCamelisabaseexperimentwithapolynomialblack-boxfunctionandthefollowingresponse:
µ∼N(cid:0) µ;f(θ),10−2(cid:1) s.t.f(θ)=2θ2−1.05θ4+ 1 θ6+θ θ +θ2 (17)
1 1 6 1 1 2 2
inthesearchspaceΘ=[−5,5]2withglobaloptimumθ∗ =(0,0),f(θ∗)=0.
Ackleyaddressesthecaseofacomplexblack-boxfunctionwithmultiplelocaloptima:
µ∼N(cid:0) µ;f(θ),10−2(cid:1)
s.t.
(cid:34) (cid:114) (cid:35) (cid:20) (cid:21)
1 1 1
f(θ)=−20exp − (θ2+θ2) −exp (cos(2πθ )+cos(2πθ )) +e+20 (18)
5 2 1 2 2 1 2
inthesearchspaceΘ=[−5,5]2withglobaloptimumθ∗ =(0,0),f(θ∗)=0.
Léviaddressesthecaseofresponsechangingshapecorrespondinglytotheblackboxfunction:
µ∼N(cid:0) µ;f(θ),σ2(cid:1)
s.t.
β
σ2 = 1 (β[4−3sin2(3πθ )]+(1−β)[0.1+3sin2(3πθ )]),β ∼Bern(n,0.5)and
β 100 2 2
f(θ)=sin2(3πθ )+(θ −1)2(1+sin2(3πθ ))+(θ −1)2(1+sin2(2πθ )) (19)
1 1 2 2 2
inthesearchspaceΘ=[−4,6]2withglobaloptimumθ∗ =(1,1),f(θ∗)=0.
Himmelblauconsidersanunderrepresentedresponse,i.e.,thesamplesizeisdecreasedton=10:
µ∼N(cid:0) µ;f(θ),10−2(cid:1) s.t.f(θ)=(θ2+θ −11)2+(θ +θ2−7)2 (20)
1 2 1 2

(−3.0,−2.0)
(−2.8005118,3.131312)
inthesearchspaceΘ=[−5,5]2withglobaloptimaθ∗ = ,f(θ∗)=0.
( (3− .53 8.7 47 49 23 81 ,0 −,− 1.3 84.2 88 13 21 68 )6)
Rosenbrockisahigherdimensionalexperiment-testedfordimensionsm=8andm=20andsignificantlydenserstartingconfigurations
forthe8-dimensionaltest:
q−1
µ∼N(cid:0) µ;f(θ),10−2(cid:1) s.t.f(θ)=(cid:88)(cid:2) 100(θ −θ2)+(1−θ )2(cid:3) (21)
i+1 i i
i=1
inthesearchspaceΘ=[−2,2]qwithglobaloptimumθ∗ =1 ,f(θ∗)=0.
q
Styblinski-Tangisahigher-dimensionalexperiment(m=20)withapolynomialblack-boxfunction:
q
µ∼N(cid:0) µ;f(θ),10−2(cid:1) s.t.f(θ)= 1(cid:88) θ4−16θ2+5θ (22)
2 i i i
i=1
inthesearchspaceΘ=[−5,5]q,
withglobaloptimumθ∗ =(−2.903534,···−2.903534),−39.16617·q<f(θ∗)<−39.16616·q.
(cid:124) (cid:123)(cid:122) (cid:125)
qtimesAppendixE.LowerConfidenceBoundResults
AsmentionedinSection4.1lowerconfidencebound(LCB)baselinesyieldresultsthatmaylooksimilartoEGOones.However,theperformance
ofLCBbaselinesissignificantlyworse.ResultspresentedinthisappendixarereportedexactlyasinSection4.1onlywithbaselinemodelsusing
theLCBalgorithmforoptimisation.BothFig.5andTable5agreewiththeaforementionedthatefficientglobaloptimisationandLCBresults
looksimilar.However,Table4revealsthatexceptforBayesianneuralnetworksanddeepensemblesinsomerunsandGaussianprocesses
converginginallrunsinthe8-dimensionalRosenbrockandtheStyblinki-Tangexperiments,allbaselinesurrogatemodelswithLCBdonot
converge.
Figure4. Experimentresults-comparisonofLCBandWU-GO.
Table4. Probabilitytoyieldanε-solutionwithinthefirst100iterations.
BLACKBOXFUNCTION WU-GO BNN DE GP DGP
THREEHUMPCAMEL 1.0±0.000 0.0±0.000 0.0±0.000 0.0±0.000 0.0±0.000
ACKLEY 1.0±0.000 0.0±0.000 0.0±0.000 0.0±0.000 0.0±0.000
LÉVI 1.0±0.000 0.0±0.000 0.0±0.000 0.0±0.000 0.0±0.000
HIMMELBLAU 1.0±0.000 0.0±0.000 0.0±0.000 0.0±0.000 0.0±0.000
ROSENBROCK(8DIM.) 0.6±0.155 0.1±0.095 0.0±0.000 1.0±0.000 0.0±0.000
ROSENBROCK(20DIM.) 0.3±0.145 0.0±0.000 0.0±0.000 0.0±0.000 0.0±0.000
STYBLINKI-TANG 0.8±0.126 0.0±0.000 0.2±0.126 1.00±0.000 0.0±0.000
Table5. Distancetooptimumafter50simulatorcalls.
BLACKBOXFUNCTION WU-GO BNN DE GP DGP
THREEHUMPCAMEL 0.071±0.000 0.436±0.473 1.560±0.860 0.295±0.173 0.277±0.189
ACKLEY 0.089±0.035 0.194±0.047 1.560±0.860 0.339±0.129 0.232±0.091
LÉVI 0.089±0.035 0.636±0.884 1.560±0.860 0.619±0.744 0.185±0.040
HIMMELBLAU 0.058±0.034 0.397±0.319 0.604±0.178 0.214±0.063 0.242±0.154
ROSENBROCK(8DIM.) 1.226±0.188 1.991±0.386 2.218±0.041 0.943±0.000 2.051±0.232
ROSENBROCK(20DIM.) 3.934±1.460 3.640±0.780 2.945±0.200 4.133±0.839 3.829±1.127
STYBLINKI-TANG 5.993±4.452 10.291±1.607 9.190±0.085 5.628±3.562 10.007±1.394AppendixF.SHiPmuonshieldconfigurations
Figure5. StartingSHiPmuonshieldconfiguration.
Figure6. SHiPmuonshieldconfigurationsuggestedbyBayesianoptimisation.
Figure7. SHiPmuonshieldconfigurationsuggestedbyWU-GO.
Figure8. LightweightandcompactSHiPmuonshieldconfigurationsuggestedbyWU-GO.AppendixG.Gaussianprocesssurrogatevariancemispredictioncases
Figure9. ExamplesofGPsurrogatesmispredictingvariance.