Gated Temporal Diffusion for Stochastic
Long-Term Dense Anticipation
Olga Zatsarynna1*, Emad Bahrami1*, Yazan Abu Farha2, Gianpiero
Francesca3, and Juergen Gall1,4
1 University of Bonn, Germany
2 Birzeit University, Palestine
3 Toyota Motor Europe, Belgium
4 Lamarr Institute for Machine Learning and Artificial Intelligence, Germany
*Equal contribution
Abstract. Long-termactionanticipationhasbecomeanimportanttask
for many applications such as autonomous driving and human-robot in-
teraction. Unlike short-term anticipation, predicting more actions into
the future imposes a real challenge with the increasing uncertainty in
longerhorizons.Whiletherehasbeenasignificantprogressinpredicting
more actions into the future, most of the proposed methods address the
task in a deterministic setup and ignore the underlying uncertainty. In
this paper, we propose a novel Gated Temporal Diffusion (GTD) net-
workthatmodelstheuncertaintyofboththeobservationandthefuture
predictions. As generator, we introduce a Gated Anticipation Network
(GTAN) to model both observed and unobserved frames of a video in a
mutual representation. On the one hand, using a mutual representation
for past and future allows us to jointly model ambiguities in the obser-
vation and future, while on the other hand GTAN can by design treat
theobservedandunobservedpartsdifferentlyandsteertheinformation
flow between them. Our model achieves state-of-the-art results on the
Breakfast, Assembly101 and 50Salads datasets in both stochastic and
deterministic settings.
1 Introduction
In this work, we address the task of long-term dense action anticipation. Given
a video as observation, the goal is to predict future actions and their durations
where the forecasting horizons can span from several seconds to several minutes
into the future, which makes it a challenging problem. Yet, solving it is crucial
for many real-world applications, such as autonomous driving or human-robot
interaction. Over the last few years, the task has gained increased attention and
there has been a steady progress [1,2,26,32,47,53], but most works address this
taskdeterministically,whichmeansthatonlyonepredictionismadeforasingle
observation. The task of forecasting future actions, however, is highly uncertain
by nature, especially for longer anticipation horizons, since the same observa-
tion can have multiple plausible continuations. Despite its importance, dealing
2 Corresponding author: zatsarynna@iai.uni-bonn.de
3 Code: https://github.com/olga-zats/GTDA
4202
luJ
61
]VC.sc[
1v45911.7042:viXra2 O. Zatsarynna et al.
Noise
Predicted Samples
GTD
Observation Anticipation
Fig.1: The proposed Gated Temporal Diffusion (GTD) model generates multiple fu-
turelong-termpredictionsofactionsfromasinglepartiallyobservedvideo.Incontrast
topreviousworks,itmodelstheuncertaintyofboththeobservationandthefuture.In
thisexample,thelightconditionsmakeitdifficulttodistinguishifabunoranorange
is cut. This ambiguity is reflected in the predicted samples where the uncertainty of
the past impacts the predicted future.
withuncertaintyforlong-termdenseactionanticipationhassofarreceivedlittle
attention. Farha et al. [23] proposed to address this task in a stochastic man-
ner. The approach generates multiple predictions in an autoregressive way by
predicting the probabilities of the next action and its duration, and then sam-
pling from the predicted probabilities. Alternatively, a GAN-based probabilistic
encoder-decodernetworkhasbeenproposedtogeneratemultiplepredictions[73].
Both approaches, however, assume that the action labels of the observed frames
are already given, either pre-estimated [23] or taken from the ground-truth ac-
tions [23,73]. In this way, the uncertainty of the observation is not taken into
account. The observation, however, can also be ambiguous due to occlusions or
difficult light conditions as shown in Fig. 1. We, therefore, argue that stochastic
action anticipation needs to consider not only the ambiguity of the future but
also of the observation.
Inthiswork,weproposeadiffusion-basedmodel[14,28,50,54,56,57]-Gated
TemporalDiffusion(GTD)-thatmodelstheuncertaintyofboththeobservation
andthefuturepredictions.Inparticular,wemakeuseofdiffusionsinceitinher-
ently models uncertainty by generating multiple predictions. The key aspect of
our GTD, however, is that we explicitly model the uncertainty of observed and
future actions jointly by predicting them simultaneously with a single diffusion
process.Tothisend,weuseajointsequencerepresentationillustratedinFig.1,
whichweconstructbyextendingtheobservedvideoframeswithzeropaddingin
placeoffutureframes.Wethencastthissequenceastheper-frameconditioning
ofthepastandfutureactionvariables,whoseconditionaljointprobabilitieswill
be learned during training. While grouping both observed and unobserved ac-
tionvariablesallowsustomodelthemusingasinglediffusionprocess,thesetwo
parts are nevertheless intrinsically distinct. To take this difference into account,
we propose a novel generator - GaTed Anticipation Network (GTAN) - that
can differentiate between past and future frames in a distinct manner and steer
the information flow between them in a data-driven way.
We evaluate our proposed model on the Breakfast [36], Assembly101 [52]
and50Salads[58] datasets.Weshowthatour approachachievesstate-of-the-art
results for stochastic long-term dense action anticipation. Additionally, despite
our focus on stochastic anticipation, we also demonstrate that our proposedGated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 3
GTAN network achieves state-of-the-art results in the deterministic setting. In
summary, we make the following contributions:
– Weproposethefirstdiffusionmodel-GatedTemporalDiffusion-forstochas-
tic long-term action anticipation that jointly models observed and future
actions.
– We propose a novel generator backbone - Gated Anticipation Network -
which steers the information flow between observed and future frames.
– We show that our model achieves state-of-the-art results in stochastic and
deterministic settings on three datasets: Breakfast, Assembly101 and 50Sal-
ads.
2 Related Work
Action Anticipation in Videos. The task of action anticipation in videos is
to forecast future actions given video observations from the current and past
moments of time. Following the introduction of anticipation benchmarks on the
recent [18,27,52] and less recent [36,37] datasets, this task has been gaining in-
creasingattention[75].Ingeneral,existingliteraturedistinguishesbetweenshort-
termandlong-termanticipationapproaches,basedonthelengthoftheanticipa-
tion horizon. Short-term anticipation methods [24,25,41,44,53,64,69–71,74,76]
forecastonlyasingleactionthattakesplaceinthenearfuture.Long-termaction
anticipation methods, which are the focus of our work, have a longer forecast-
ing horizon and predict multiple future actions several minutes into the future.
Among long-term anticipation approaches, one can further pin down several es-
tablishedtaskformulations.Inthefirstlineofwork[2],Farhaet al.proposedto
anticipate action classes densely for a subset of frames from the predefined fu-
turetimeinterval.Thisformulation,addressedin[1,2,26,32,47,53]andreferred
to here as deterministic long-term dense anticipation, implies the necessity to
predict not only classes of future actions but also their duration. In their work,
Farhaet al.[2]introducedtwomodelswithdifferentmodesofpredictionsbased
on CNN and RNN networks. Ke et al. [32] proposed a TCN-based model with
time conditioning to enable direct action prediction at a predefined moment of
time. In the later works, several encoder-decoder architectures were proposed
by Farha et al. [1], Sener et al. [53], Gong et al. [26] and Nawhal et al. [47]
based on GRUs [6], Non-Local blocks [63] and Transformer layers [61]. In the
second line of work [23], Farha et al. extended the previous formulation into
the probabilistic domain, so that the uncertainty of future anticipation could be
taken into account. In this formulation, called here stochastic long-term dense
anticipation, actions still need to be anticipated densely, however, several future
sequencesareallowedtobepredictedforasingleobservation.Intheirwork[23],
the authors proposed a probabilistic RNN network that made autoregressive
predictionsbasedonthesamplesdrawnfromthepredictedactionclassdistribu-
tions.Zhaoetal.[73]laterproposedaGAN-basedprobabilisticencoder-decoder
network. Lastly, in the anticipation frameworks from [27,46], which we refer to
astranscriptlong-termanticipation,theestimationofdurationforfutureactions
is discarded. This setting has been addressed in [3,19,27,43,46,47].4 O. Zatsarynna et al.
Diffusion. Diffusion models [28,50,54,56,57] are a class of deep probabilis-
tic generative models that recover the data sample from Gaussian noise via
a gradual denoising process. In a following work, denoising diffusion implicit
models(DDIMs)[55]wereintroducedtospeedupthediffusionmodelsampling.
Later,[12,17]extendedthecontinuousdiffusionmodelstogeneratediscretedata.
Diffusionmodelshaveshownoutstandingresultsinvariousgenerationtaskssuch
asimagesynthesis[21,51],videogeneration[10,29],speechprocessing[49,59,66],
natural language processing [5,30] and motion generation [60,72]. Furthermore,
diffusionmodelshaveachievedgreatsuccessincomputervisionperceptiontasks
suchassegmentation[8,11,16],objectdetection[15],temporalactionsegmenta-
tion [39] and detection [45]. The stochastic nature of diffusion models has been
leveraged for motion anticipation [9,60,65] and procedure planning [62].
3 Stochastic Long-Term Dense Anticipation
Previouslyproposedstochasticlong-termanticipationapproaches[23,73]assume
that the observed video segments share the same format as the future predic-
tions, namely action labels. This assumption, however, overlooks the ambiguity
inherent in certain video observations, such as insufficient context due to lim-
ited observation duration, challenging lighting conditions, occlusions, and other
factors(seeFig.1andFig.4).Insuchcases,theestimatedorpre-definedaction
labels fail to convey the uncertainty associated with observed frames.
Toovercomethislimitation,weaimtojointlymodeltheuncertaintyofboth
futureandobservedactions.Tothisend,weproposeanapproachbasedondiffu-
sion, known for its suitability for modelling uncertainty [14]. Standard diffusion
models,however,cannotbedirectlyappliedtothetaskoflong-termactionantic-
ipation. Thus, we introduce a novel model for stochastic long-term anticipation,
termedGatedTemporalDiffusionforAnticipation(GTD),whichwedescribein
detail in Sec. 4. Although we focus on stochastic anticipation, we also show how
our method can be used in the deterministic case in Sec. 4.4. Before discussing
our proposed approach, we formally define the task of stochastic long-term an-
ticipation.
Following the popular protocol introduced by [2], the observed part and the
durationofthefuturepredictionaredefinedbypercentagesαandβ oftheentire
video.Moreprecisely,givenα|V|observedframesofavideowith|V|frames,the
goalistopredictactionlabelsforthefutureβ|V|frames.Accordingly,N =α|V|
o
is the number of observed frames, N = β|V| is the number of frames whose
a
action classes we want to anticipate, and N = N +N is the total number of
o a
frames that are considered. Since β can go up to 0.5, the prediction duration
can be very long.
Whiledeterministicapproachesmakeonlyasinglemostlikelyfuturepredic-
tion,stochasticapproachesconsidertheuncertaintyofthefuturemodellingand
generate multiple (M >1) future samples.
Formally, the task of stochastic long-term anticipation can be formulated
as learning to draw samples from the underlying joint probability of per-frameGated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 5
future actions conditioned on the observed video frames:
YˆNo+1:N ∼p (YNo+1,...,YN|F) (1)
θ
F =(ϕ(x1),...,ϕ(xNo))∈RNo×D, (2)
wherexk isthekth inputframe,ϕisthefeatureextractornetwork,Yi istheac-
tionvariablecorrespondingtoith frameandYi:j denotesasequenceofvariables
for frame i to j.
Sinceinourwork,weaimtoadditionallymodeltheuncertaintypresentinthe
video observations, we instead learn to sample Yˆ = Yˆ1:N from the conditional
joint distribution of both future and observed actions.
4 Gated Temporal Diffusion for Anticipation
To model uncertainty and perform stochastic action anticipation, we formulate
our network as a diffusion model as illustrated in Fig. 1. While the standard
diffusion model, which we describe in Sec. 4.1, serves as a foundation, it cannot
be directly applied to the task of long-term action anticipation. Hence, we in-
troduce a novel model, called Gated Temporal Diffusion (GTD), which jointly
modelstheuncertaintyofbothobservedandunobservedeventswhilepreserving
the inherent difference between the two. We discuss our proposed approach in
Sec. 4.2.
4.1 Diffusion Model
Diffusion models learn to map noise samples Y ∼N(0,I) to the samples from
T
the data distribution Y ∼ q(Y) in an iterative manner using a reverse Markov
0
chain process p (Y ) with learnable transition parameters θ:
θ 0:T
T
(cid:89)
p (Y )=p (Y ) p (Y |Y ), p (Y |Y )=N(Y ;µ (Y ,t),Σ(t)). (3)
θ 0:T θ T θ t−1 t θ t−1 t t−1 θ t
t=1
To learn these parameters, a forward Markov process is defined. It specifies
the transitions in the inverse direction by adding Gaussian noise to the data
according to a fixed variance schedule β ,...,β :
1 T
T
(cid:89) (cid:112)
q(Y |Y )= q(Y |Y ), q(Y |Y )=N(Y ; 1−β Y ,β I). (4)
1:T 0 t t−1 t t−1 t t t−1 t
t=1
Training. Optimization of diffusion models is performed using the variational
lowerboundonthenegativelog-likelihoodofthedatasampleswheresomeprop-
erties of the forward process are harnessed. Given the Gaussian nature of the
forward transition probabilities, one can use the reparametrization trick [33] to
draw samples directly from Y by corrupting it following the schedule γ :
0 t
√ (cid:112)
Y = γ Y + (1−γ )ϵ , (5)
t t 0 t t
where ϵ ∼ N(0,I), γ =
(cid:81)t
(1 − β ). For training, Y is obtained using
t t k=1 k t
the forward process (5) at step t ∼ U(1,T) and a denoising generator network6 O. Zatsarynna et al.
T T-1
GTAN GTAN
Fig.2: We formulate stochastic action anticipation as a diffusion process where the
initial input consists of Gaussian noise, Y , and zero-padded features, F˜. Given the
T
inputs,theGTANgeneratorpredictsthedenoisedactionlabels,Yˆ .FromstepT-1to
0,T
0, the GTAN generator uses self-conditioning by taking the previous denoised action
labels as additional input. The noise ϵˆ and mean µˆ terms for steps T-1 to 0 are
t t
computed using equations (7) and (8). ⊕ indicates channel-wise concatenation.
G (Y ,t) = Yˆ is trained to reverse the noise and predict the reconstruction
θ t 0,t
Yˆ of Y by minimizing the l reconstruction error between them:
0,t 0 2
L =E ∥G (Y ,t)−Y ∥2. (6)
diff t∼U(1,T),ϵt∼N(0,I) θ t 0
The key contribution of a diffusion model is the design of the generator network
G (Y ,t).
θ t
Inference. Once trained, sampling from the diffusion model requires following
a sequence of denoising steps. In the DDPM [28] sampling procedure, inference
followsT denoisingsteps.Startingatstept=T,arandomsampleisdrawnfrom
Y ∼N(0,I) and fed to the generator G that predicts Yˆ . Assuming that we
T θ 0,T
have already sampled Y and reconstructed Yˆ at step t, we can generate the
t 0,t
next sample Y by first approximating ϵ by:
t−1 t
1 √
ϵˆ = √ (Y − γ Yˆ ). (7)
t t t 0,t
1−γ
t
Since the reverse transition probabilities q(Y |Y ,Y ) become tractable when
t−1 t 0
conditioned on Y and can be expressed as Gaussians N(Y ;µ˜ ,β˜I), we can
0 t−1 t t
estimate the parameters of this Gaussian by:
√ (cid:113) 1−γ
µˆ = γ Yˆ + 1−γ −β˜ϵˆ, β˜ = t−1β (8)
t t−1 0,t t−1 t t t 1−γ t
t
and sample Y ∼N(Y ;µˆ ,β˜I). The steps continue until t=1, after which
t−1 t−1 t t
Yˆ is taken as the final generated sample. For an alternative DDIM [55] sam-
0,1
pling, variances of the transition probabilities are set to zero during inference,
i.e., β˜ = 0, making the denoising process deterministic for a particular noise
t
sample Y . This way, Y is equal to the mean value Y = µˆ . The DDIM
T t−1 t−1 t
sampling scheme performs better than DDPM if less denoising steps are used
during inference compared to training [55].
4.2 Gated Temporal Diffusion for Anticipation
In contrast to previous works, we aim to model uncertainty both in the past
observation and the future. We thus extend the formulation described in Sec. 3Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 7
and generate multiple samples not only for the future, but also multiple inter-
pretations of the past as shown in Fig. 4. Since the uncertainty of the future
depends on the uncertainty in the observation, we treat them as a unified se-
quence Yˆ =Yˆ1:N and model them with a shared diffusion model.
WhilethediffusionmodeldescribedinSec.4.1,generatesmultipledatasam-
ples by sampling Y ∼ N(0,I) repeatedly, it is not directly applicable to our
T
problem for several reasons. Specifically, it operates in the continuous domain
and it does not incorporate any conditioning information during the generation
process. Moreover, the generator network G treats all variables uniformly, dis-
θ
regarding the distinction between observed and future action variables in the
inputsequence.Toaddresstheselimitations,weproposeaGatedTemporalDif-
fusion model (GTD). As our key contribution, we introduce a GTAN generator
network, described in Sec. 4.3. This network employs gated temporal convolu-
tions and, on one hand, models observed and unobserved data jointly, while, on
theotherhand,steerstheinformationflowbetweenpastandfutureentrieswith
the learnable gates, thereby controlling their fusion. Apart from that, we also
propose discretization and conditioning schemes, elaborated below.
To model discrete action categories using continuous state diffusion, we rep-
resent action labels as one-hot encoded vectors Y ∈ RN×C and regard them
0
as “analog bits” [17] that can be directly modelled by continuous state diffusion
models. In this way, training remains unchanged and the inference process also
remains the same, except that we map generated samples back to the discrete
domain by applying the argmax operation over the class dimension.
To condition the generation on the observed frames, we make use of frame-
wise feature vectors F (2) and adapt them to act as the per-frame condition for
both observed and future action variables. More specifically, we expand it by
incorporating zero-padding to compensate for the absent features of the future
unobserved frames:
Yˆ ∼p (Y|F˜), F˜ =(ϕ(x1),...,ϕ(xNo),0,...,0)∈RN×D. (9)
θ
In this way, the resulting vector F˜ can be used to condition the per-frame dif-
fusion generation process. To this end, we channel-wise concatenate the padded
observed frame features F˜ ∈ RN×D to the current sample Y ∈ RN×C at each
t
step t. Furthermore, we employ self-conditioning [17] by using the previous esti-
mate Yˆ ∈RN×C as additional input for the generator G :
0,t+1 θ
G (Y ,Yˆ ,F˜,t)=Yˆ ∈RN×C, (10)
θ t 0,t+1 0,t
whereduringtrainingYˆ israndomlysettozerowithprobabilityp,whichis
0,t+1
then equivalent to training G without self-conditioning. While Y , Yˆ , and
θ t 0,t+1
F˜ are concatenated and used as input to the generator G , shown in Fig. 2, at
θ
eachdiffusionstept,weencodethesteptassinusoidalpositionalembedding.In
the next section, we describe our proposed GTAN generator network in detail.
4.3 Gated Anticipation Generator
To model temporal dependencies in the input, we use temporal convolutional
layers for the generator network G . As the input sequence to our network con-
θ8 O. Zatsarynna et al.
GTAN
Loss Loss Loss
Stage 1 Stage S
x L x L
GTA Block GTA Block
Feature Conv
Dropout 1x1 Conv ReLU
Gate Conv
Fig.3: The GTAN takes as input a joint representation sequence for observed and
future frames. Each stage consists of GTA blocks. The dilated gated convolutions de-
activate features at certain frames.
sists of two distinct parts (observed and unobserved), directly applying the net-
work with classical temporal convolution layers as in other tasks [7,22,67] is
sub-optimal. This is because the distinction between observed and future parts
would not be possible, leading to all values being treated equally. While gated
convolutions [4,20,48,68] have been proposed for different purposes, we adopt
this concept such that the generator network adaptively decides how much mix-
ing between the past and future occurs at different levels of the network.
Motivated by this, we propose a Gated Temporal Anticipation Network
(GTAN). It comprises S stages that produce output vectors of action proba-
bilitiesoverC classes.EachstageconsistsofLresidualGTAblocksasshownin
Fig. 3. Each GTA block includes two dilated temporal convolutional layers: one
for feature processing (feature convolution) and another for gating the features
(gate convolution). Formally, given a feature vector as input, both layers are
applied to it separately with the same dilation rate, kernel size, and number of
channels. The layers are then combined by the element-wise product between
the output of the feature convolution and the sigmoid-activated output of the
gate convolution. Given the input feature H˜ , the output H¯ of the gated
s,l−1 s,l
convolution at layer l and stage s is computed as follows:
H¯ =σ(W ∗H˜ +b )⊙(W ∗H˜ +b ), (11)
s,l g s,l−1 g f s,l−1 f
where σ is the sigmoid function, ∗ denotes the convolution operator, and ⊙
denotes the element-wise product. W and W are weights of the convolutional
g f
filters and b and b are biases for gate and feature convolutions, respectively.
g f
The output H¯ is then passed through a dropout layer and then to a 1×1
s,l
convolutional layer followed by the ReLU non-linearity. Finally, the output of
the layer l is computed by applying a residual connection after the ReLU. At
each layer l, the dilation factor for the gate and feature convolution is set to 2l,
i.e., it increases by a factor of two with each layer.
In our ablations studies, we show the benefit of gating and demonstrate the
importanceoflearningtemporalgatesinadata-drivenway,asopposedtousing
manual masks [40] or using channel-wise gating [31], i.e. without the temporal
component. We also show that our proposed gated generator is superior to the
previously proposed gated temporal convolutional network [4].
ObsGated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 9
4.4 Training and Inference
For training our proposed Gated Temporal Diffusion model, we sample step
t ∼ U(1,T) and Y (5) for a given observation F˜ (9) with ground-truth Y . We
t 0
then apply our proposed GTAN generator G (Y ,Yˆ ,F˜,t) that produces a
θ t S,0,t+1
set of stage-wise predictions {Yˆ }S . The self-conditioning with prediction
s,0,t s=1
Yˆ at step t+1 is only included if t<T. We train our network with the l
S,0,t+1 2
reconstruction loss accumulated over all S stages:
S
L =E (cid:88) ∥Yˆ −Y ∥2. (12)
stoch t∼U(1,T),ϵt∼N(0,I) s,0,t 0
s=1
During inference, we apply the DDIM [55] sampling scheme and use a subset
of D denoising steps to get our final predictions. As the final output of our
model, we take the reconstruction made at the denoising step t = 1 of the
last GTAN stage S, i.e., Yˆ = Yˆ . To generate multiple action sequences
S,0,1
for a given observation, we run the denoising process starting from M different
noisesamples{Y }M ,whicharethenreconstructedintodistinctpredictions
T,m m=1
{Yˆ m}M m=1, where {Yˆ mNo+1:N}M
m=1
are the future action predictions.
Deterministic Anticipation. Although we focus on stochastic long-term an-
ticipation, we also report results for the deterministic anticipation. In this case,
we use the proposed GTAN without the diffusion process. Given F˜ as the input
sequence, our model produces intermediate {Yˆ }S−1 and final Yˆ = Yˆ predic-
s s=1 S
tions. Using these predictions, we train our network using the standard cross-
entropy loss applied to all stages and frames:
S N
L =−(cid:88)(cid:88) YnlogYˆn, (13)
determ s
s=1n=1
where Yn ∈RC is the one-hot encoded ground-truth action at frame n. During
inference, we only consider predictions for the future frames.
5 Experiments
5.1 Datasets and Evaluation Metrics
We evaluate our proposed network on three challenging datasets: Breakfast,
Assembly101 and 50Salads.
Breakfast[36]containsbreakfastpreparationvideos.Itcontains1712videos
of actors preparing 10 breakfast-related dishes. Each video is densely annotated
withactionsfrom48classes.Onaverage,videosare2.3minuteslongandcontain
6 action segments. The longest video is 10.8 minutes long, so the duration of
anticipation is up to 5.4 minutes. For evaluation, we use the standard 4 splits
for cross-validation and report the average result.
Assembly101[52]isalarge-scaledatasetoftoyvehicleassemblyanddisas-
sembly. It contains 4321 videos, which are densely annotated with 100K coarse
action segments from 202 coarse action classes. The duration of anticipation is10 O. Zatsarynna et al.
Table 1: Comparison to the state of the art for stochastic anticipation on Breakfast,
Assembly101 and 50Salads. * Indicates that we trained UAAA on Assembly101.
β(α=0.2) β(α=0.3)
Dataset Metric Method
0.1 0.2 0.3 0.5 0.1 0.2 0.3 0.5
Tri-gram[23] 15.4 13.7 12.9 11.9 19.3 16.6 15.8 13.9
MeanMoC
UAAA[23] 15.7 14.0 13.3 13.0 19.1 17.2 17.4 15.0
Breakfast
Ours 24.0 22.0 21.4 20.6 29.1 26.8 25.3 24.2
UAAA[23] 28.9 28.4 27.6 28.0 32.4 31.6 32.8 30.8
Top-1MoC
Ours 51.2 47.3 45.6 45.0 54.0 50.4 49.6 47.8
Tri-gram 2.8 2.2 1.9 1.5 3.5 2.7 2.3 1.8
MeanMoC UAAA*[23] 2.7 2.1 1.9 1.7 2.4 2.1 1.9 1.7
Assembly101 Ours 6.4 4.4 3.5 2.8 5.9 4.2 3.5 2.9
UAAA*[23] 6.9 5.9 5.6 5.1 5.9 5.5 5.2 4.9
Top-1MoC
Ours 18.0 12.8 9.9 7.7 16.0 11.9 10.2 7.7
Tri-gram[23] 21.4 16.4 13.3 9.4 24.6 15.6 11.7 8.6
MeanMoC UAAA[23] 23.6 19.5 18.0 12.8 28.0 18.0 14.8 12.1
50Salads Ours 28.3 22.1 17.8 11.7 29.9 18.5 14.2 10.6
UAAA[23] 53.5 43.0 40.5 33.7 56.4 42.8 35.8 30.2
Top-1MoC
Ours 69.6 55.8 45.2 28.1 66.2 44.9 39.2 31.0
upto12.5minutesinvideoslasting25minutes.Thedatasetisdividedintotrain,
validation, and test splits. Since the test split is not publicly available, we train
our model on the train split and report our results on the validation set.
50Salads [58] contains videos of salad preparations. It comprises 50 videos
annotatedwithdensesegmentsfrom17fine-grainedactions.Themeanlengthof
videosis6.4minutes,whilethelongestvideois10.1minuteslong,sotheduration
ofanticipationisupto5.1minutes.Forevaluation,weusethepredefined5splits
for cross-validation and report the average result.
Forafaircomparisonwithexistingmethodsonthesedatasets,weusedprevi-
ously extracted visual features. On the Breakfast and 50Salads datasets, we use
theI3D[13]featuresprovidedby[2],whileforAssembly101weutilizedTSM[38]
features provided by [52]. Further implementation details of our model for these
datasets are provided in the supp. material.
Evaluation Protocol and Metrics. We evaluate our method using the α
and β from the protocol defined in [2]. Specifically, we test our network for α∈
{0.2,0.3} and β ∈ {0.1,0.2,0.3,0.5}, where α and β denote the percentages of
framesofavideothatareusedasobservationandfutureprediction,respectively.
We evaluate our approach in the stochastic and deterministic settings. For the
deterministicsetting,wereportmeanoverclasses(MoC)accuracyasin[2,26,32].
Inthestochasticsetting,wegeneratemultiplepredictionsforthesameobserved
frames. As proposed in [23], we report two metrics: mean and top-1 MoC across
M =25 predictions, called Mean MoC and Top-1 MoC, respectively.
5.2 Stochastic Anticipation
Comparison with State of the Art. We first compare our proposed stochas-
tic approach with the state of the art for stochastic anticipation on the Break-
fast, Assembly101 and 50Salads datasets. In Sec. 5.4, we compare our approach
to deterministic approaches. The comparison of our diffusion-based model with
UAAA [23] and tri-gram baseline [23] is presented in Tab. 1. UAAA [23] andGated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 11
the tri-gram baseline are the only available probabilistic models with a compa-
rable evaluation protocol since [73] only uses ground-truth action labels as their
observations. [23] is a two-step approach that first predicts action labels for the
observed frames and then forecasts the future actions. To compare our method
to [23] on the Assembly101 dataset, we used MS-TCN [22] for the first step. We
trained a MS-TCN network for each value of α using full supervision, but only
theobservedframesastrainingdata,i.e.,thefirst20%or30%,respectively.We
then trained UAAA [23]. Note that two-step approaches [23,73] do not model
any uncertainty in the observation. As shown in Tab. 1, our method outper-
forms[23]withalargemarginonBreakfastandAssembly101atbothMeanand
Top-1 MoC accuracy with improvements across all observation and anticipation
ratios. On the 50Salads dataset, our approach outperforms [23] at Top-1 MoC,
while the Mean MoC accuracy is on par.
InFig.4,wepresentpredictedsamplesfromourmodel.Inthefirstexample,
the action sequence involves the high-level activity ‘Scrambled Eggs’. However,
the observed part of the sequence contains only the action ‘Butter Pan’, shared
with another high-level class, ‘Fry Eggs’. Thus, based solely on this segment,
distinguishing the underlying activity is challenging. Our network predicts se-
quences belonging to either ‘Scrambled Eggs’ or ‘Fry Eggs’, demonstrating am-
biguity in order, length, and presence/absence of actions within categories. In
the second example, the ground-truth activity is ‘Cereals’. Poor lighting condi-
tionsmakerecognizingactionsintheobservedsegmentdifficultandambiguous.
Consequently, our model classifies observed actions differently across samples,
leading to consistent yet different predictions. By addressing uncertainty in ob-
servations, our model produces correct predictions despite the poor quality of
theobservedsegment.Weprovidemorequalitativeresultsinthesupp.material.
5.3 Ablation Study
Number of Inference Steps and Stages. In Tab. 2, we explored the impact
ofvaryingthenumberofdenoisingstepsD inour diffusionmodelwhile keeping
the number of GTAN stages fixed. Increasing the number of steps from 10 to 50
ledtoimprovedaccuracy,butfurtherincreasingitto100didnotyieldadditional
benefits. Fig. 6 illustrates how predictions evolve with different denoising steps:
noise decreases significantly after 10 steps, with further refinement observed
Table 2: Ablation on the num. of
diffusion inference steps and stages in Table 3: Ablation on GTAN architecture
GTAN on Breakfast. Numbers show onBreakfast.NumbersshowMeanMoCac-
Mean MoC accuracy. curacy.
Num. Num. β(α=0.2) β(α=0.3)
β(α=0.2) β(α=0.3)
stages steps 0.1 0.2 0.3 0.5 0.1 0.2 0.3 0.5 Method
0.1 0.2 0.3 0.5 0.1 0.2 0.3 0.5
5 1 19.6 17.7 17.3 15.9 24.9 22.6 22.5 20.0
5 10 23.0 20.7 20.3 19.5 28.0 25.7 24.7 23.8 Ours 24.0 22.0 21.4 20.6 29.1 26.8 25.3 24.2
5 50 24.0 22.0 21.4 20.6 29.1 26.8 25.3 24.2 Oursw/oGC 23.0 21.1 20.6 19.8 27.6 25.7 24.3 23.5
5 100 24.2 21.8 21.3 20.5 29.1 26.7 25.0 24.0 Oursw/oDil.GC 22.9 21.1 20.6 20.2 28.6 26.0 24.7 23.9
1 50 23.7 21.9 21.1 20.2 29.2 26.7 25.3 23.9 Aslanetal.[4] 18.8 17.3 16.7 15.7 20.7 18.9 18.8 16.7
3 50 23.4 21.6 21.0 20.4 29.6 26.8 25.2 24.4 Part.Conv.[40] 23.2 21.5 20.9 20.1 27.9 25.6 24.4 23.6
5 50 24.0 22.0 21.4 20.6 29.1 26.8 25.5 24.2 SE[31] 22.4 21.0 20.4 19.9 28.0 25.7 24.5 23.6
5 50 24.0 22.0 21.4 20.6 29.1 26.8 25.3 24.2
1 250 23.9 21.8 21.0 20.0 29.6 27.0 25.5 23.912 O. Zatsarynna et al.
Fig.4:QualitativeresultsofourproposedGTD Fig.5: MeanMoCofGTDforse-
for stochastic long-term action anticipation on quences sorted by MFSS diversity
Breakfast. Best viewed zoomed in. for the obs. part on Breakfast.
with 50 steps. Next, with D fixed at 50, we evaluated the influence of different
numbers of stages. Smaller networks (1 and 3 stages) exhibited lower accuracy
with50denoisingsteps,butwith250steps,thesingle-stageGTAN’sperformance
approached that of 5 stages. For all other experiments, we use 5 stages and 50
denoising steps.
Gated Convolution. In Tab. 3, we assess the impact of gated convolution
on our GTAN generator. Removing the gate convolution branch (see Fig. 3)
from the GTA block and leaving only the feature convolution (Ours w/o GC)
leads to lower performance, highlighting the necessity of the gating mechanism.
Similarly, removing the dilation factor from the gate convolution branch (Ours
w/o Dil. GC) also decreases performance, as expected. Substituting our GTAN
architecturewiththegatedtemporalconvolutionalnetworkproposedbyAslanet
al. [4] results in a significant decline in performance, indicating its unsuitability
for dense anticipation.
We also explore alternatives to gated convolutions, including manual mask
adaptation and channel-wise feature reweighing, by replacing gated convolu-
tions with partial convolution [40] and squeeze-and-excitation [31] blocks, re-
spectively. Although manual mask adaptation outperforms the model without
gating, it falls short compared to our proposed approach, highlighting the ad-
vantage of learning to perform gating in a data-driven manner. Employing the
squeeze-and-excitation block instead of gated convolutions yields better perfor-
mancecomparedtopreviousalternatives,butitremainsinferiortoourproposed
method. This emphasizes the importance of the temporal component of gated
convolutions.WeprovidefurtherablationsoftheGTANdesignandaqualitative
example of learned gates in the supp. material.
Modelling Observation Uncertainty. We conduct an experiment to in-
vestigate the impact of modelling observation ambiguity on the performance of
Fig.6: Qualitative results of our proposed GTD with different numbers of inference
diffusion steps. Best viewed zoomed in.Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 13
Table 4: Ablation on modelling observa- Table5:Ablationonthediffusionloss
tion uncertainty (o.u.) on Breakfast. type on Breakfast.
β(α=0.2) β(α=0.3) β(α=0.2) β(α=0.3)
Metric Method 0.1 0.2 0.3 0.5 0.1 0.2 0.3 0.5 Metric Loss 0.1 0.2 0.3 0.5 0.1 0.2 0.3 0.5
MeanMoC O Ou ur rs sw/oo.u. 2 24 3. .0 7 2 22 1. .0 4 2 21 0. .4 5 2 10 9. .6 9 22 99 .. 81 22 76 .. 18 2 25 5. .3 3 2 24 4. .2 1 MeanMoC M CESE 22 64 .. 20 22 32 .. 30 22 31 .. 94 22 20 .. 66 32 29 .. 11 22 96 .. 28 22 75 .. 53 22 64 .. 32
Top-1MoC O Ou ur rs sw/oo.u. 5 41 2. .2 8 4 47 0. .3 7 4 35 8. .6 8 4 35 8. .0 4 5 44 8. .0 4 5 40 4. .4 2 4 49 4. .6 9 4 47 3. .8 5 Top-1MoC M CESE 5 41 6. .2 9 4 47 2. .3 2 4 45 1. .6 7 4 45 0. .0 7 5 44 8. .0 8 5 40 6. .4 5 4 49 5. .6 3 4 47 4. .8 0
MFSS O Ou ur rs sw/oo.u. 4 31 1. .5 2 4 34 4. .3 7 4 35 6. .6 3 4 38 9. .4 4 3 23 6. .7 1 3 26 9. .6 4 3 38 0. .0 8 4 31 4. .6 3 MFSS M CESE 4 21 9. .5 0 4 34 1. .3 8 4 35 3. .6 5 4 38 5. .4 0 3 23 4. .7 0 3 26 6. .6 9 3 28 8. .0 2 4 31 0. .6 0
our model. To this end, we utilize an MS-TCN [22] to predict labels of the ob-
served part, which serves as the condition vector for GTD instead of the frame
featuresF.Also,wecomputethelossfunctionL onlyforthefutureframes.
stoch
The results of this experiment are shown in Tab. 4. While the Mean MoC of
both methods remains comparable, the Top-1 MoC of the model without un-
certainty modelling drops significantly, indicating its limited ability to generate
diversepredictions.Todirectlymeasurethepredictiondiversity,weadditionally
introduce a new metric - Mean Framewise Sample Similarity (MFSS). MFSS
calculates diversity as the mean normalized pairwise sample distance averaged
over the videos in the split:
Z  (cid:32) N (cid:33)
MFSS = Z1 (cid:88)  M(M2 −1) (cid:88) 100 1− N1 (cid:88) 1(Yˆ zn ,i =Yˆ zn ,j) , (14)
z=1 1≤i<j≤M n=1
where Z is the total number of videos. As evident from the results, omitting the
observationuncertaintymodellingleadstoareductioninthediversityofpredic-
tions.InFig.5,wedemonstratethecorrelationbetweenMeanMoCanticipation
accuracyanduncertaintyoftheGTDmodelfortheobservation.Tothisend,we
sorted the videos into four groups based on the MFSS of the predictions on the
observedpartsofthevideosandcalculatedtheaverageMeanMoCforanticipa-
tionforeachgroup.ForsequenceswithhighMFSS,GTDisuncertainaboutthe
observed actions. The plot reveals an inverse correlation between future Mean
MoC and observed MFSS since future actions are harder to predict for videos
with higher observation uncertainty. This analysis underscores the significance
of modelling ambiguity in observed frames for future action prediction.
Diffusion Loss Type. We employ Mean Squared Error (MSE) as the loss
function for our diffusion approach, following prior work [17,28]. Additionally,
we evaluate the impact of using the Cross Entropy (CE) loss and present the
resultsinTab.5.Notably,themodeltrainedwithMSElossachievesthehighest
Top-1 MoC accuracy and MFSS, while the model trained with CE loss attains
the best Mean MoC. These results of the CE-trained network can be attributed
toadditionalSoftmaxnormalizationrequiredbythisloss.TheSoftmaxrestricts
the handling of uncertainty in early diffusion steps, emphasizing the most likely
action class prematurely. Directly measuring diversity using the MFSS metric
confirms these observations, with the MSE-trained model exhibiting higher di-
versity than the CE-trained model. We used the MSE loss in our experiments.14 O. Zatsarynna et al.
Table 6: Comparison with state-of-the-art methods for deterministic anticipation on
the Breakfast dataset. * Indicates retrained results.
β(α=0.2) β(α=0.3)
Dataset Method
0.1 0.2 0.3 0.5 0.1 0.2 0.3 0.5
RNN[2] 18.1 17.2 15.9 15.8 21.6 20.0 19.7 19.2
CNN[2] 17.9 16.3 15.4 14.5 22.4 20.1 19.7 18.8
UAAA(mode)[23] 16.7 15.4 14.5 14.2 20.7 18.3 18.4 16.9
TimeCond.[32] 18.4 17.2 16.4 15.8 22.8 20.4 19.6 19.8
Breakfast
TempAgg[53] 24.2 21.1 20.0 18.1 30.4 26.3 23.8 21.2
CycleCons.[1] 25.9 23.4 22.4 21.5 29.7 27.4 25.6 25.2
FUTR[26] 27.7 24.6 22.8 22.1 32.3 29.9 27.5 25.9
Ours 28.8 26.3 25.8 26.0 35.5 32.9 30.5 29.6
UAAA(mode)*[23] 2.7 2.1 1.8 1.6 2.4 2.1 1.9 1.7
Assembly101 FUTR*[26] 7.5 5.5 4.7 4.1 7.8 6.0 5.2 4.0
Ours 9.0 6.8 6.6 5.5 8.4 6.8 6.0 5.0
5.4 Deterministic Anticipation
Comparison with State of the Art. The majority of dense long-term antic-
ipation methods [1,2,26,32,53] has been trained and evaluated in the determin-
istic setting and we compare with these methods in Tab. 6. We do not compare
with Anticipatr [47] since it uses a different protocol [75]. We also compare with
UAAA [23] in the deterministic setting (mode). On Assembly 101, we compare
withUAAA(mode)andFUTR[26]usingthepubliclyavailablecodefortraining
andevaluation.Weprovideresultsforthe50Saladsdataset,qualitativecompar-
isons and ablation studies for the deterministic GTAN in the supp. material.
On both Breakfast and Assembly101, our method outperforms all methods that
use the same evaluation protocols. Compared to the previously best-performing
method FUTR [26], our method shows a substantial improvement in particular
onBreakfastforlong-termprediction(β =0.5)whereMoCisincreasedby+3.9
and +3.7 for α=0.2 and α=0.3, respectively.
6 Conclusion
We have proposed a Gated Temporal Diffusion network to address the task of
stochasticlong-termdenseactionanticipation.Asthebackboneforourdiffusion
model, we introduced a Gated Anticipation Network (GTAN) that allows for
mutual modelling of the actions in the observed and future frames. In this way,
the uncertainty is not only modelled for the future but also for the observed
part. We demonstrated that the approach generatesdifferentpredictionsfor the
observed frames in case of ambiguities due to poor light conditions and that
these ambiguities impact the future predictions. Furthermore, we demonstrated
that GTAN can be applied to deterministic long-term dense action anticipation
as well. In our experiments, we showed that our model achieves state-of-the-art
results on three datasets in deterministic and stochastic settings. A limitation
of our proposed model is its current efficiency. For example, the average time of
generating a single prediction on the Breakfast dataset, which has an average
anticipation horizon of 1.15 minutes, is 3.8 seconds. While this is sufficient for
mid-term action planning, i.e. range of minutes, a further reduction of inference
time is needed. This can be achieved by using techniques to accelerate inference
of diffusion models like distillation [35] or DeepCache [42].Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 15
Acknowledgements
The work has been supported by the Deutsche Forschungsgemeinschaft (DFG,
German Research Foundation) GA 1927/4-2 (FOR 2535 Anticipating Human
Behavior), by the Federal Ministry of Education and Research (BMBF) under
grant no. 01IS22094E WEST-AI, the project iBehave (receiving funding from
the programme “Netzwerke 2021”, an initiative of the Ministry of Culture and
ScienceoftheStateofNorthrhineWestphalia),andtheERCConsolidatorGrant
FORHUE (101044724). The authors gratefully acknowledge the Gauss Centre
for Supercomputing e.V. (www.gauss-centre.eu) for funding this project by pro-
vidingcomputingtimethroughtheJohnvonNeumannInstituteforComputing
(NIC) on the GCS Supercomputer JUWELS at Jülich Supercomputing Centre
(JSC). The sole responsibility for the content of this publication lies with the
authors.
References
1. Abu Farha, Y., Ke, Q., Schiele, B., Gall, J.: Long-term anticipation of activities
with cycle consistency. In: DAGM German Conference on Pattern Recognition
(GCPR) (2020)
2. AbuFarha,Y.,Richard,A.,Gall,J.:Whenwillyoudowhat?-Anticipatingtempo-
raloccurrencesofactivities.In:IEEEConferenceonComputerVisionandPattern
Recognition (CVPR) (2018)
3. Ashutosh, K., Girdhar, R., Torresani, L., Grauman, K.: Hiervl: Learning hierar-
chical video-language embeddings. In: IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2023)
4. Aslan, F., Kozat, S.: Handling irregularly sampled signals with gated temporal
convolutional networks. Signal, Image and Video Processing (2022)
5. Austin,J.,Johnson,D.D.,Ho,J.,Tarlow,D.,vandenBerg,R.:Structureddenois-
ing diffusion models in discrete state-spaces. In: Advances in Neural Information
Processing Systems (NeurIPS) (2021)
6. Bahdanau,D.,Cho,K.,Bengio,Y.:Neuralmachinetranslationbyjointlylearning
to align and translate. ArXiv (2014)
7. Bahrami, E., Francesca, G., Gall, J.: How much temporal long-term context is
needed for action segmentation? In: IEEE International Conference on Computer
Vision (ICCV) (2023)
8. Baranchuk, D., Voynov, A., Rubachev, I., Khrulkov, V., Babenko, A.: Label-
efficientsemanticsegmentationwithdiffusionmodels.In:InternationalConference
on Learning Representations (ICLR) (2022)
9. Barquero, G., Escalera, S., Palmero, C.: Belfusion: Latent diffusion for behavior-
drivenhumanmotionprediction.In:IEEEInternationalConferenceonComputer
Vision (ICCV) (2023)
10. Blattmann,A.,Rombach,R.,Ling,H.,Dockhorn,T.,Kim,S.W.,Fidler,S.,Kreis,
K.:Alignyourlatents:High-resolutionvideosynthesiswithlatentdiffusionmodels.
In:IEEEConferenceonComputerVisionandPatternRecognition(CVPR)(2023)
11. Brempong,E.A.,Kornblith,S.,Chen,T.,Parmar,N.,Minderer,M.,Norouzi,M.:
Denoising pretraining for semantic segmentation. In: IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) (2022)16 O. Zatsarynna et al.
12. Campbell, A., Benton, J., Bortoli, V.D., Rainforth, T., Deligiannidis, G., Doucet,
A.: A continuous time framework for discrete denoising models. In: Advances in
Neural Information Processing Systems (NeurIPS) (2022)
13. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the
kinetics dataset. IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) (2017)
14. Chan, M.A., Molina, M.J., Metzler, C.A.: Hyper-diffusion: Estimating epistemic
and aleatoric uncertainty with a single model. arXiv preprint arXiv:2402.03478
(2024)
15. Chen,S.,Sun,P.,Song,Y.,Luo,P.:Diffusiondet:Diffusionmodelforobjectdetec-
tion.In:IEEE/CVFInternationalConferenceonComputerVision(ICCV)(2023)
16. Chen, T., Li, L., Saxena, S., Hinton, G., Fleet, D.J.: A generalist framework for
panopticsegmentationofimagesandvideos.In:IEEE/CVFInternationalConfer-
ence on Computer Vision (ICCV) (2023)
17. Chen,T.,Zhang,R.,Hinton,G.:Analogbits:Generatingdiscretedatausingdiffu-
sion models with self-conditioning. In: International Conference on Learning Rep-
resentations (ICLR) (2023)
18. Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E.,
Moltisanti, D., Munro, J., Perrett, T., Price, W., Wray, M.: The epic-kitchens
dataset:Collection,challengesandbaselines.IEEETransactionsonPatternAnal-
ysis and Machine Intelligence (TPAMI) (2021)
19. Das,S.,Ryoo,M.S.:Video+clipbaselineforego4dlong-termactionanticipation.
ArXiv (2022)
20. Dauphin,Y.,Fan,F.,Auli,M.,Grangier,D.:Languagemodelingwithgatedcon-
volutional networks. In: International Conference on Machine Learning (ICML)
(2016)
21. Dhariwal,P.,Nichol,A.:DiffusionmodelsbeatGANsonimagesynthesis.Advances
in Neural Information Processing Systems (NeurIPS) (2021)
22. Farha,Y.,Gall,J.:Ms-tcn:Multi-stagetemporalconvolutionalnetworkforaction
segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) (2019)
23. Farha,Y.,Gall,J.:Uncertainty-awareanticipationofactivities.In:IEEEInterna-
tional Conference on Computer Vision Workshop (ICCVW) (2019)
24. Furnari, A., Farinella, G.M.: Rolling-unrolling lstms for action anticipation from
first-person video. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence (TPAMI) (2020)
25. Girdhar,R.,Grauman,K.:AnticipativeVideoTransformer.In:IEEEInternational
Conference on Computer Vision (ICCV) (2021)
26. Gong, D., Lee, J., Kim, M., Ha, S., Cho, M.: Future transformer for long-term
actionanticipation.In:IEEEConferenceonComputerVisionandPatternRecog-
nition (CVPR) (2022)
27. Grauman, K., Westbury, A., et al.: Ego4d: Around the world in 3,000 hours of
egocentric video. IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) (2022)
28. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in
Neural Information Processing Systems (NeurIPS) (2020)
29. Ho,J.,Salimans,T.,Gritsenko,A.,Chan,W.,Norouzi,M.,Fleet,D.J.:Videodif-
fusion models. In: Advances in Neural Information Processing Systems (NeurIPS)
(2022)Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 17
30. Hoogeboom, E., Nielsen, D., Jaini, P., Forré, P., Welling, M.: Argmax flows and
multinomial diffusion: Learning categorical distributions. In: Advances in Neural
Information Processing Systems (NeurIPS) (2021)
31. Hu, J., Shen, L., Albanie, S., Sun, G., Wu, E.: Squeeze-and-excitation networks.
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018)
32. Ke,Q.,Fritz,M.,Schiele,B.:Time-conditionedactionanticipationinoneshot.In:
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)
33. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. International Con-
ference on Learning Representations (ICLR) (2014)
34. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Interna-
tional Conference on Learning Representations (ICLR) (2015)
35. Kohler, J., Pumarola, A., Schönfeld, E., Sanakoyeu, A., Sumbaly, R., Vajda, P.,
Thabet, A.: Imagine flash: Accelerating emu diffusion models with backward dis-
tillation. arXiv preprint arXiv:2405.05224 (2024)
36. Kuehne,H.,Arslan,A.,Serre,T.:Thelanguageofactions:Recoveringthesyntax
andsemanticsofgoal-directedhumanactivities.In:IEEEConferenceonComputer
Vision and Pattern Recognition (CVPR) (2014)
37. Li, Y., Liu, M., Rehg, J.M.: In the eye of the beholder: Gaze and actions in first
person video. IEEE Transactions on Pattern Analysis and Machine Intelligence
(TPAMI) (2020)
38. Lin, J., Gan, C., Han, S.: Tsm: Temporal shift module for efficient video under-
standing. In: IEEE International Conference on Computer Vision (ICCV) (2019)
39. Liu, D., Li, Q., Dinh, A.D., Jiang, T., Shah, M., Xu, C.: Diffusion action seg-
mentation. In: IEEE/CVF International Conference on Computer Vision (ICCV)
(2023)
40. Liu, G., Reda, F.A., Shih, K.J., Wang, T.C., Tao, A., Catanzaro, B.: Image in-
paintingforirregularholesusingpartialconvolutions.In:EuropeanConferenceon
Computer Vision (ECCV) (2018)
41. Liu, M., Tang, S., Li, Y., Rehg, J.M.: Forecasting human-object interaction: joint
predictionofmotorattentionandactionsinfirstpersonvideo.In:EuropeanCon-
ference on Computer Vision (ECCV) (2020)
42. Ma,X.,Fang,G.,Wang,X.:Deepcache:Acceleratingdiffusionmodelsforfree.In:
IEEE/CVF Conference on Computer Vision and Pattern Recognition (2024)
43. Mascaró, E., Ahn, H., Lee, D.: Intention-conditioned long-term human egocentric
action anticipation. In: IEEE Winter Conference on Applications of Computer
Vision (WACV) (2023)
44. Miech, A., Laptev, I., Sivic, J., Wang, H., Torresani, L., Tran, D.: Leveraging the
presenttoanticipatethefutureinvideos.In:IEEEConferenceonComputerVision
and Pattern Recognition Workshop (CVPRW) (2019)
45. Nag,S.,Zhu,X.,Deng,J.,Song,Y.Z.,Xiang,T.:Difftad:Temporalactiondetec-
tionwithproposaldenoisingdiffusion.In:IEEE/CVFInternationalConferenceon
Computer Vision (ICCV) (2023)
46. Nagarajan, T., Li, Y., Feichtenhofer, C., Grauman, K.: Ego-topo: Environment
affordances from egocentric video. In: IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2020)
47. Nawhal,M.,Jyothi,A.A.,Mori,G.:Rethinkinglearningapproachesforlong-term
actionanticipation.In:EuropeanConferenceonComputerVision(ECCV)(2022)
48. Van den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals, O., Graves, A., et al.:
Conditional image generation with pixelcnn decoders. Advances in Neural Infor-
mation Processing Systems (NeurIPS) (2016)18 O. Zatsarynna et al.
49. Popov,V.,Vovk,I.,Gogoryan,V.,Sadekova,T.,Kudinov,M.:Grad-tts:Adiffusion
probabilistic model for text-to-speech. In: International Conference on Machine
Learning (ICML) (2021)
50. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents. ArXiv preprint (2022)
51. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) (2022)
52. Sener,F.,Chatterjee,D.,Shelepov,D.,He,K.,Singhania,D.,Wang,R.,Yao,A.:
Assembly101:Alarge-scalemulti-viewvideodatasetforunderstandingprocedural
activities.IEEEConferenceonComputerVisionandPatternRecognition(CVPR)
(2022)
53. Sener, F., Singhania, D., Yao, A.: Temporal aggregate representations for long-
rangevideounderstanding.In:EuropeanConferenceonComputerVision(ECCV)
(2020)
54. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsuper-
visedlearningusingnonequilibriumthermodynamics.In:InternationalConference
on Machine Learning (ICML) (2015)
55. Song,J.,Meng,C.,Ermon,S.:Denoisingdiffusionimplicitmodels.In:International
Conference on Learning Representations (ICLR) (2021)
56. Song, Y., Ermon, S.: Generative modeling by estimating gradients of the data
distribution.AdvancesinNeuralInformationProcessingSystems(NeurIPS)(2019)
57. Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Ermon,S.,Poole,B.:Score-
based generative modeling through stochastic differential equations. In: Interna-
tional Conference on Learning Representations (ICLR) (2021)
58. Stein, S., McKenna, S.J.: Combining embedded accelerometers with computer vi-
sionforrecognizingfoodpreparationactivities.ACMinternationaljointconference
on Pervasive and ubiquitous computing (2013)
59. Tae, J., Kim, H., Kim, T.: EdiTTS: Score-based Editing for Controllable Text-to-
Speech. In: Proc. Interspeech 2022 (2022)
60. Tanke, J., Zhang, L., Zhao, A., Tang, C., Cai, Y., Wang, L., Wu, P.C., Gall, J.,
Keskin, C.: Social diffusion: Long-term multiple human motion anticipation. In:
IEEE/CVF International Conference on Computer Vision (ICCV) (2023)
61. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
L., Polosukhin, I.: Attention is all you need. In: Advances in Neural Information
Processing Systems (NeurIPS) (2017)
62. Wang, H., Wu, Y., Guo, S., Wang, L.: Pdpp: Projected diffusion for procedure
planning in instructional videos. In: IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2023)
63. Wang, X., Girshick, R.B., Gupta, A.K., He, K.: Non-local neural networks. IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (2018)
64. Wu, Y., Zhu, L., Wang, X., Yang, Y., Wu, F.: Learning to anticipate egocentric
actions by imagination. In: IEEE Transactions on Image Processing (TIP) (2020)
65. Xu,S.,Wang,Y.X.,Gui,L.Y.:Stochasticmulti-person3dmotionforecasting.In:
International Conference on Learning Representations (ICLR) (2023)
66. Yang, D., Yu, J., Wang, H., Wang, W., Weng, C., Zou, Y., Yu, D.: Diffsound:
Discrete diffusion model for text-to-sound generation. IEEE/ACM Transactions
on Audio, Speech, and Language Processing (2023)
67. Yi,F.,Wen,H.,Jiang,T.:Asformer:Transformerforactionsegmentation.In:The
British Machine Vision Conference (BMVC) (2021)Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 19
68. Yu,J.,Lin,Z.L.,Yang,J.,Shen,X.,Lu,X.,Huang,T.S.:Free-formimageinpaint-
ing with gated convolution. IEEE/CVF International Conference on Computer
Vision (ICCV) (2018)
69. Zatsarynna, O., Farha, Y., Gall, J.: Multi-modal temporal convolutional network
for anticipating actions in egocentric videos. In: IEEE Conference on Computer
Vision and Pattern Recognition Workshop (CVPRW) (2021)
70. Zatsarynna, O., Gall, J.: Action anticipation with goal consistency. In: IEEE In-
ternational Conference on Image Processing (ICIP) (2023)
71. Zatsarynna, O., Farha, Y.A., Gall, J.: Self-supervised learning for unintentional
actionprediction.In:DAGMGermanConferenceonPatternRecognition(GCPR)
(2022)
72. Zhang, M., Cai, Z., Pan, L., Hong, F., Guo, X., Yang, L., Liu, Z.: Motiondif-
fuse: Text-driven human motion generation with diffusion model. arXiv preprint
arXiv:2208.15001 (2022)
73. Zhao, H., Wildes, R.P.: On diverse asynchronous activity anticipation. In: Euro-
pean Conference on Computer Vision (ECCV) (2020)
74. Zhao,Y.,Krähenbühl,P.:Real-timeonlinevideodetectionwithtemporalsmooth-
ing transformers. In: European Conference on Computer Vision (ECCV) (2022)
75. Zhong, Z., Martin, M., Voit, M., Gall, J., Beyerer, J.: A survey on deep learning
techniques for action anticipation. In: Arxiv (2023)
76. Zhong, Z., Schneider, D., Voit, M., Stiefelhagen, R., Beyerer, J.: Anticipative fea-
turefusiontransformerformulti-modalactionanticipation.In:IEEEWinterCon-
ference on Applications of Computer Vision (WACV) (2023)Supplementary Material: Gated Temporal
Diffusion for Stochastic Long-Term Dense
Anticipation
Olga Zatsarynna1*, Emad Bahrami1*, Yazan Abu Farha2, Gianpiero
Francesca3, and Juergen Gall1,4
1 University of Bonn, Germany
2 Birzeit University, Palestine
3 Toyota Motor Europe, Belgium
4 Lamarr Institute for Machine Learning and Artificial Intelligence, Germany
*Equal contribution
InSec.1,weprovideimplementationdetails.InSec.2,weprovideadditional
ablation studies and qualitative results for stochastic long-term anticipation. In
Sec. 3, we present additional results for deterministic long-term anticipation.
1 Implementation Details
We implemented our models in Pytorch. For both stochastic and deterministic
settings, we used 5 stages and 9 layers for the GTAN model. For the filters of
thegatedconvolutions,wesetthekernelsizeto3,thenumberofchannelsto64,
and the dilation factor to 2l, i.e., the dilation factor is increased by a factor of
two with each layer l in a stage. We trained our network with a batch size of 16
using the Adam [34] optimizer with the learning rate 5·10−4 on the Breakfast
and Assembly101 datasets. For the 50salads dataset, we used the batch size of
8, set the learning rate to 10−3.
Forstochasticanticipation,wetrainedourGTDmodelwithT =1000denois-
ingsteps.Duringinference,weappliedDDIM[55]samplingprocedureand used
a subset of D = 50 denoising steps for generating predictions on the Breakfast
and Assembly101 datasets and D =10 steps on the 50Salads dataset.
2 Stochastic Long-Term Anticipation
2.1 Ablation Study
Gated Convolution. InFig.1,weillustratetwoexamplesofgatevaluestaken
from two distinct layers of the second stage of our proposed network. In both
cases, there is a clear difference between observed and future frames visible, not
just in the early layers of the network. For instance, at layer 8, there are some
channelswheremainlytheframesfromtheobservation(channel9)ormainlythe
future frames (channel 45) are considered. Additionally, the amount of mixing
between observed and future frames changes for different layers. Starting from
almost no mixing in the early layers, more and more mixing between unmasked
and masked parts occurs as the network progresses.Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 21
Fig.1:VisualizationofgatesfromtheGTAN.Bothoutputsaretakenfromthesecond
stage from layers l = 2 (left) and l = 8 (right). The vertical green line marks the
boundary between the observed and future frames.
ModellingObservationUncertainty. InFig.5ofthemainpaper,weshowed
the relationship between the future Mean MoC accuracy and the MFSS for the
observed frames. Both metrics are computed over 25 predictions per observed
video as shown in Figs. 9-12. Additionally, in Fig. 2, we further visualize the re-
lationbetweentheMFSSofthepredictedpastandfutureactions.Thegrouping
based on quartiles is the same as in Fig. 5 of the main paper. The first bar (first
quartile) includes the videos with the lowest MFSS for the predicted actions of
the observed part and the last bar (fourth quartile) includes the videos with the
highest MFSS. For the videos of each quartile, we calculated the average MFSS
forthefuturepredictions,whereasFig.5ofthemainpaperplotstheMeanMoC
accuracyforthefuturepredictionsforeachquartile.Theplotdemonstratesadi-
rectcorrelationbetweentheMFSSoftheobservedandfutureparts,asexpected.
When there is more uncertainty about the actions in the observed frames, the
future predictions become more diverse.
Fig.2: MFSS of GTD future predictions for sequences sorted by MFSS diversity for
the observation part on Breakfast.
Effect of the number of samples. InTab.1andTab.2wedemonstratehow
varying the number of samples affects the Mean MoC and Top-1 MoC accuracy22 O. Zatsarynna et al.
Fig.3: Qualitative comparison of the GTD model trained with three different loss
terms. The example is from the Breakfast dataset. Best viewed zoomed in.
ofourproposedGTDmodel.Ontheonehand,theTop-1MoCincreasesconsis-
tently,showingthatourmodelgeneratesdiversepredictions.Ontheotherhand,
weobserveaslightreductioninMeanMoCaccuracywiththeincreasednumber
of samples. This is expected due to the average value taken over increasingly
diverse predictions.
Table 1: Effectofthenumberofsamples Table 2: Effectofthenumberofsamples
on Mean MoC accuracy on Breakfast. on Top-1 MoC accuracy on Breakfast.
Num. β(α=0.2) β(α=0.3) Num. β(α=0.2) β(α=0.3)
samples 0.1 0.2 0.3 0.5 0.1 0.2 0.3 0.5 samples 0.1 0.2 0.3 0.5 0.1 0.2 0.3 0.5
5 24.2 21.9 21.7 20.9 29.1 26.9 25.4 24.7 5 38.8 36.2 35.4 34.2 41.9 39.2 37.7 36.5
10 23.9 21.8 21.5 20.9 29.1 26.8 25.5 24.4 10 44.0 40.7 39.8 38.9 47.1 44.2 43.5 42.2
20 24.0 21.9 21.4 20.7 29.2 26.7 25.3 24.2 20 49.2 45.5 44.2 43.7 52.3 49.0 47.9 46.3
25 24.0 22.0 21.4 20.6 29.1 26.8 25.3 24.2 25 51.2 47.3 45.6 45.0 54.0 50.4 49.6 47.8
Diffusion Loss Type. In Tab. 5 of the main paper, we compared the mean
squared error (MSE) with the cross-entropy (CE) loss. In Tab. 3, we include
resultsforthebinarycross-entropyloss(BCE)aswell.TheBCElossisbetween
MSE and CE in terms of all metrics. The sigmoid of BCE results in a higher
diversity compared to the softmax used by CE, but the diversity is still lower
than for MSE. This is also visible in the example shown in Fig. 3.
Self conditioning. In Tab. 4, we analyze the effect of self-conditioning. Ex-
cluding self-conditioning (w/o Self Cond.) decreases the performance. Without
self-conditioning, the approach generates many unrealistic predictions. For in-
stance,sample5ofFig.4containsmanyveryshortactionsegmentsandneither
the order nor the duration of the actions are plausible.
Loss for Observations vs. Anticipation. In our model, we apply the same
loss for the observed frames as well as the future frames. Since the GTD model
is only evaluated for anticipation, one might expect that giving a lower weight
for the observed frames than for the future frames is beneficial. To this end, weGated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 23
Table 3: Ablation on the diffusion loss type on Breakfast.
β(α=0.2) β(α=0.3)
Metric Loss 0.1 0.2 0.3 0.5 0.1 0.2 0.3 0.5
MSE 24.0 22.0 21.4 20.6 29.1 26.8 25.3 24.2
MeanMoC BCE 25.6 22.8 22.4 21.9 31.7 28.9 27.2 26.1
CE 26.2 23.3 23.9 22.6 32.1 29.2 27.5 26.3
MSE 51.2 47.3 45.6 45.0 54.0 50.4 49.6 47.8
Top-1MoC BCE 49.4 46.0 44.6 42.6 52.4 48.8 47.2 44.9
CE 46.9 42.2 41.7 40.7 48.8 46.5 45.3 44.0
MSE 41.5 44.3 45.6 48.4 33.7 36.6 38.0 41.6
MFSS BCE 35.1 37.6 38.7 39.3 28.3 30.9 31.6 33.1
CE 29.0 31.8 33.5 35.0 24.0 26.9 28.2 30.0
Table 4: Ablation on self-conditioning for GTD on Breakfast. Numbers show Mean
MoC accuracy.
β (α=0.2) β (α=0.3)
Method
0.1 0.2 0.3 0.5 0.1 0.2 0.3 0.5
Ours 24.022.021.420.629.126.825.324.2
Oursw/oSelfCond. 23.1 21.1 20.5 19.7 28.6 25.9 24.9 24.0
Fig.4: Qualitative comparison of the GTD model trained with and without self-
conditioning(w/oselfcond.).TheexampleisfromtheBreakfastdataset.Bestviewed
zoomed in.
averagethelossfortheobservedframesandfutureframesandweightheaverage
over the observed frames by a factor of less than 1. The results are shown in
Tab. 5. The best performance for our method is achieved when the weights for
both terms are equal (1.0) and weighing down the loss applied to the observed
frames consistently decreases the performance.
2.2 Qualitative Results
We present additional qualitative results of GTD in Figs. 5 and 6 on the Break-
fast dataset, in Fig. 7 on Assembly101 and in Fig. 8 on the 50Salads dataset.
Additionally, in Figs. 9-12, we present all M = 25 samples produced by our
model on the Breakfast dataset. For each of these examples, we indicate to
which observation diversity group they fall, following the grouping from Fig. 2.
Iftheobservedframescanbeclearlyrecognized,asinFigs.5and9,theGTD
model predicts sequences from the correct high-level activity with variations in
the length and order of individual actions. In some cases, however, there are24 O. Zatsarynna et al.
Table 5: Impact of weighting the loss on the observed frames lower than the loss on
thefutureframesfortheGTDonBreakfast.Incaseof1.0,observedandfutureframes
are equally weighted. Numbers show Mean MoC accuracy.
β (α=0.2) β (α=0.3)
ObsCoef 0.1 0.2 0.3 0.5 0.1 0.2 0.3 0.5
0.1 23.7 21.8 21.1 20.4 28.7 26.3 24.9 23.9
0.5 23.8 21.8 21.220.7 28.9 26.5 25.124.3
1.0 24.022.021.4 20.629.126.825.3 24.2
ambiguitiesintheobservedframesduetopoorlightconditionsorocclusions,e.g.
inFigs.6,10,11and12.Inthiscase,themodelgeneratesdifferentinterpretations
oftheobservedframesandanticipatesactionsconsistentwiththeinterpretation
oftheobservation.Thisleadstomorediversepredictionsforthefuture,reflecting
the uncertainty of the observed frames.
For example, in Figs. 6 and 11, the object being cut by the person is barely
visible. Consequently, the network generates samples with different interpreta-
tions, i.e. a bun or an orange as the object being cut. In Fig. 10, when the
person quickly takes a bowl from a cupboard, it becomes partially occluded by
a thermos jug. As a result, the object becomes ambiguous, leading the model
to predict actions such as ‘take bowl’ or ‘take cup’ depending on the sample.
Similarly,thenextobject(cereals)remainsunclearduetoocclusionsandmotion
blur. Finally, in Fig. 12, the main action takes place in the corner of the frame
due to unfavourable camera positioning. Meanwhile, the foreground features a
static cup of coffee, unrelated to the depicted activity of making ‘juice’. This
scenario confuses the model, resulting in predictions ranging from the correct
high-level activity to unrelated ones such as ‘coffee’, ‘tea’, or ‘chocolate milk’.
Additionally, the limited visibility of the orange leads to the model interpreting
it as an egg for some samples and predicting the sequence from the high-level
activity of the ‘scrambled egg’.
These examples clearly demonstrate that it is essential to model ambiguities
oftheobservationandthefuturetogether.TheproposedGTDmodelisthefirst
approach to do so.Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 25
Fig.5: QualitativeresultsofourproposedGTDforstochasticlong-termactionantic-
ipation on the Breakfast [36] dataset.
Fig.6: QualitativeresultsofourproposedGTDforstochasticlong-termactionantic-
ipation on the Breakfast [36] dataset.26 O. Zatsarynna et al.
Fig.7: QualitativeresultsofourproposedGTDforstochasticlong-termactionantic-
ipation on the Assembly101 [52] dataset.
Fig.8: QualitativeresultsofourproposedGTDforstochasticlong-termactionantic-
ipation on the 50Salads [58] dataset.Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 27
Fig.9: QualitativeresultsofourproposedGTDforstochasticlong-termactionantic-
ipationforavideofromthefirstquartile(MFSSforobservation)oftheBreakfast[36]
dataset. Best viewed zoomed in.
Fig.10:QualitativeresultsofourproposedGTDforstochasticlong-termactionantic-
ipationforavideofromthesecondquartile(MFSSforobservation)oftheBreakfast[36]
dataset. Best viewed zoomed in.28 O. Zatsarynna et al.
Fig.11:QualitativeresultsofourproposedGTDforstochasticlong-termactionantic-
ipationforavideofromthethirdquartile(MFSSforobservation)oftheBreakfast[36]
dataset. Best viewed zoomed in.
Fig.12:QualitativeresultsofourproposedGTDforstochasticlong-termactionantic-
ipationforavideofromthefourthquartile(MFSSforobservation)oftheBreakfast[36]
dataset. Best viewed zoomed in.Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 29
3 Deterministic Long-Term Anticipation
3.1 Comparison to State of The Art
WepresenttheresultsofourproposedGTANmodelfordeterministiclong-term
actionanticipationonthe50SaladsdatasetinTab.6.Onthisdataset,thereisno
clear leading approach. While [32] performs best in most cases for α =0.3, our
approach performs best for most cases for α=0.2. Overall, our model performs
on par with the state of the art on this dataset.
Table 6: Comparison with state-of-the-art methods for deterministic anticipation on
the 50Salads dataset. ** Indicates results from the FUTR github repository.
β(α=0.2) β(α=0.3)
Dataset Method
0.1 0.2 0.3 0.5 0.1 0.2 0.3 0.5
RNN[2] 30.1 25.4 18.7 13.5 30.8 17.2 14.8 9.8
CNN[2] 21.2 19.0 15.9 9.9 29.1 20.1 17.5 10.9
UAAA(mode)[23] 24.9 22.4 19.9 12.8 29.1 20.5 15.3 12.3
TimeCond.[32] 32.5 27.6 21.3 16.0 35.1 27.1 22.1 15.6
50Salads
TempAgg[53] 25.5 19.9 18.2 15.1 30.6 22.5 19.1 11.2
CycleCons.[1] 34.8 28.4 21.8 15.3 34.4 23.7 19.0 15.9
FUTR**[26] 37.0 27.8 22.5 16.8 33.3 23.2 22.1 15.5
Ours 36.7 27.7 23.8 17.4 32.2 24.9 17.4 14.9
3.2 Ablations
In this section, we provide a set of ablation experiments for our proposed model
in the deterministic setting.
Number of Stages and Layers. We analyze the impact of the number of
stages and layers in the GTAN model. The results reported in Tab. 7 show
that the best performance is achieved by the model with S = 5 stages and
L = 9 layers. Additionally, we analyze if gated convolutions are required at
all stages of the network. We did so by using them only in the first S′ stages
while using only feature convolutions for the remaining stages. The respective
results with different S′ are shown in Tab. 8. The results show that using gated
convolutionsforallstagesperformsbest.Asmentionedbefore,thedifferentiation
between the observed and anticipated parts is preserved even in the later stages
of the proposed GTAN model, which hints at the usefulness of such separation.
Therefore, we speculate that the deterioration of the results for S′ < S was
caused by the inability of the network’s latest stages to distinguish between the
observed and future frames caused by the absence of the gating mechanism.
Gated Temporal Convolution. We analyze the effect of gated convolutions
on our deterministic GTAN model. To this end, we performed the same series
of experiments as for GTD, shown in Tab. 9. Removing the gate convolution30 O. Zatsarynna et al.
Table 7: Ablation on the number of layers and stages for GTAN on Breakfast.
Num. Num. β (α=0.2) β (α=0.3)
LayersStages 0.1 0.2 0.3 0.5 0.1 0.2 0.3 0.5
9 4 27.5 24.8 24.1 24.0 32.1 29.6 28.3 28.2
9 5 28.826.325.8 26.035.532.930.529.6
9 6 29.3 25.9 25.0 25.1 33.7 31.0 29.5 29.0
8 5 27.7 24.6 24.1 23.9 33.1 30.1 28.5 28.3
9 5 28.826.325.8 26.035.532.930.529.6
10 5 27.8 25.3 24.4 24.1 32.0 29.3 27.5 27.2
Table 8: Ablation on the number of stages with gated convolutions on Breakfast.
Stages β(α=0.2) β(α=0.3)
w.gated 0.1 0.2 0.3 0.5 0.1 0.2 0.3 0.5
1/5 28.7 25.6 24.6 24.1 33.7 31.3 29.2 27.9
3/5 28.826.4 25.7 25.2 34.3 32.1 29.5 28.6
5/5 28.8 26.3 25.826.035.532.930.529.7
branchandleavingonlythefeaturebranchwithstandardtemporalconvolutions
(Oursw/oGC)ledtoconsiderablylowerperformancecomparedtotheproposed
approach.Alsoremovingthedilationfromthegatedconvolution(Oursw/oDil.
GC) resulted in the performance drop. This demonstrates the necessity of using
the gating mechanism. We also tested three alternative gating formulations -
Aslan et al. [4], SE [31] and Partial Convolution [40]. The performance of these
methods is inferior to the suggested approach. The results are consistent with
the results in Tab. 3 of the main paper.
Table 9: Ablation on the gated convolution for GTAN on Breakfast.
β (α=0.2) β (α=0.3)
0.1 0.2 0.3 0.5 0.1 0.2 0.3 0.5
Ours 28.826.325.826.035.532.930.529.6
Oursw/oGC 28.1 24.6 24.0 23.9 32.7 30.3 28.3 27.3
Oursw/oDil.GC 28.9 25.8 24.9 25.4 33.3 30.5 29.2 28.5
Aslanetal.[4] 21.9 20.7 19.7 19.0 25.8 23.1 22.4 20.0
Part.Conv[40] 28.8 25.3 24.6 24.1 32.9 30.5 28.2 27.6
SE[31] 27.2 24.7 24.6 23.3 31.7 28.9 27.4 27.9
Loss Type. As for the GTD model, we have analyzed the effect of the
training loss on our proposed GTAN model in the deterministic setting. Instead
ofusingthecross-entropy(CE)loss,wehavetrainedournetworkwiththemean-
square-error (MSE) loss. The results reported in Tab. 10 show that using the
MSE loss in the deterministic setting leads to a considerable decrease in the
performance.Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 31
Table 10: Impact of using the MSE loss instead of the cross entropy (CE) loss in the
deterministic setting.
β (α=0.2) β (α=0.3)
Loss
0.1 0.2 0.3 0.5 0.1 0.2 0.3 0.5
MSE 23.8 21.8 21.1 19.3 27.4 25.5 24.2 22.2
CE 28.826.325.826.035.532.930.529.6
3.3 Qualitative Results
We present additional qualitative results of our approach in the deterministic
setting in Figs. 13 and 14 on the Breakfast dataset, in Fig. 15 on Assembly101
and in Fig. 16 on 50Salads dataset. In the first example of Fig. 13 and the first
and third examples of Fig. 14, FUTR fails to recognize the correct high-level
actions,whichleadstotheoverallwrongpredictionsoftheupcomingactions.In
other cases, the high-level action is recognized correctly, however, the duration
or presence of some actions is anticipated inaccurately.32 O. Zatsarynna et al.
Fig.13: QualitativecomparisonsofourapproachwithFUTR[26]inthedeterministic
setting on the Breakfast [36] dataset.
Fig.14: QualitativecomparisonsofourapproachwithFUTR[26]inthedeterministic
setting on the Breakfast [36] dataset.Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 33
Fig.15: QualitativecomparisonsofourapproachwithFUTR[26]inthedeterministic
setting on the Assembly101 [52] dataset.
Fig.16: QualitativecomparisonsofourapproachwithFUTR[26]inthedeterministic
setting on the 50Salads [58] dataset.34 O. Zatsarynna et al.
References
1. Abu Farha, Y., Ke, Q., Schiele, B., Gall, J.: Long-term anticipation of activities
with cycle consistency. In: DAGM German Conference on Pattern Recognition
(GCPR) (2020)
2. AbuFarha,Y.,Richard,A.,Gall,J.:Whenwillyoudowhat?-Anticipatingtempo-
raloccurrencesofactivities.In:IEEEConferenceonComputerVisionandPattern
Recognition (CVPR) (2018)
3. Ashutosh, K., Girdhar, R., Torresani, L., Grauman, K.: Hiervl: Learning hierar-
chical video-language embeddings. In: IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2023)
4. Aslan, F., Kozat, S.: Handling irregularly sampled signals with gated temporal
convolutional networks. Signal, Image and Video Processing (2022)
5. Austin,J.,Johnson,D.D.,Ho,J.,Tarlow,D.,vandenBerg,R.:Structureddenois-
ing diffusion models in discrete state-spaces. In: Advances in Neural Information
Processing Systems (NeurIPS) (2021)
6. Bahdanau,D.,Cho,K.,Bengio,Y.:Neuralmachinetranslationbyjointlylearning
to align and translate. ArXiv (2014)
7. Bahrami, E., Francesca, G., Gall, J.: How much temporal long-term context is
needed for action segmentation? In: IEEE International Conference on Computer
Vision (ICCV) (2023)
8. Baranchuk, D., Voynov, A., Rubachev, I., Khrulkov, V., Babenko, A.: Label-
efficientsemanticsegmentationwithdiffusionmodels.In:InternationalConference
on Learning Representations (ICLR) (2022)
9. Barquero, G., Escalera, S., Palmero, C.: Belfusion: Latent diffusion for behavior-
drivenhumanmotionprediction.In:IEEEInternationalConferenceonComputer
Vision (ICCV) (2023)
10. Blattmann,A.,Rombach,R.,Ling,H.,Dockhorn,T.,Kim,S.W.,Fidler,S.,Kreis,
K.:Alignyourlatents:High-resolutionvideosynthesiswithlatentdiffusionmodels.
In:IEEEConferenceonComputerVisionandPatternRecognition(CVPR)(2023)
11. Brempong,E.A.,Kornblith,S.,Chen,T.,Parmar,N.,Minderer,M.,Norouzi,M.:
Denoising pretraining for semantic segmentation. In: IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR) (2022)
12. Campbell, A., Benton, J., Bortoli, V.D., Rainforth, T., Deligiannidis, G., Doucet,
A.: A continuous time framework for discrete denoising models. In: Advances in
Neural Information Processing Systems (NeurIPS) (2022)
13. Carreira, J., Zisserman, A.: Quo vadis, action recognition? a new model and the
kinetics dataset. IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) (2017)
14. Chan, M.A., Molina, M.J., Metzler, C.A.: Hyper-diffusion: Estimating epistemic
and aleatoric uncertainty with a single model. arXiv preprint arXiv:2402.03478
(2024)
15. Chen,S.,Sun,P.,Song,Y.,Luo,P.:Diffusiondet:Diffusionmodelforobjectdetec-
tion.In:IEEE/CVFInternationalConferenceonComputerVision(ICCV)(2023)
16. Chen, T., Li, L., Saxena, S., Hinton, G., Fleet, D.J.: A generalist framework for
panopticsegmentationofimagesandvideos.In:IEEE/CVFInternationalConfer-
ence on Computer Vision (ICCV) (2023)
17. Chen,T.,Zhang,R.,Hinton,G.:Analogbits:Generatingdiscretedatausingdiffu-
sion models with self-conditioning. In: International Conference on Learning Rep-
resentations (ICLR) (2023)Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 35
18. Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E.,
Moltisanti, D., Munro, J., Perrett, T., Price, W., Wray, M.: The epic-kitchens
dataset:Collection,challengesandbaselines.IEEETransactionsonPatternAnal-
ysis and Machine Intelligence (TPAMI) (2021)
19. Das,S.,Ryoo,M.S.:Video+clipbaselineforego4dlong-termactionanticipation.
ArXiv (2022)
20. Dauphin,Y.,Fan,F.,Auli,M.,Grangier,D.:Languagemodelingwithgatedcon-
volutional networks. In: International Conference on Machine Learning (ICML)
(2016)
21. Dhariwal,P.,Nichol,A.:DiffusionmodelsbeatGANsonimagesynthesis.Advances
in Neural Information Processing Systems (NeurIPS) (2021)
22. Farha,Y.,Gall,J.:Ms-tcn:Multi-stagetemporalconvolutionalnetworkforaction
segmentation. In: IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) (2019)
23. Farha,Y.,Gall,J.:Uncertainty-awareanticipationofactivities.In:IEEEInterna-
tional Conference on Computer Vision Workshop (ICCVW) (2019)
24. Furnari, A., Farinella, G.M.: Rolling-unrolling lstms for action anticipation from
first-person video. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence (TPAMI) (2020)
25. Girdhar,R.,Grauman,K.:AnticipativeVideoTransformer.In:IEEEInternational
Conference on Computer Vision (ICCV) (2021)
26. Gong, D., Lee, J., Kim, M., Ha, S., Cho, M.: Future transformer for long-term
actionanticipation.In:IEEEConferenceonComputerVisionandPatternRecog-
nition (CVPR) (2022)
27. Grauman, K., Westbury, A., et al.: Ego4d: Around the world in 3,000 hours of
egocentric video. IEEE Conference on Computer Vision and Pattern Recognition
(CVPR) (2022)
28. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in
Neural Information Processing Systems (NeurIPS) (2020)
29. Ho,J.,Salimans,T.,Gritsenko,A.,Chan,W.,Norouzi,M.,Fleet,D.J.:Videodif-
fusion models. In: Advances in Neural Information Processing Systems (NeurIPS)
(2022)
30. Hoogeboom, E., Nielsen, D., Jaini, P., Forré, P., Welling, M.: Argmax flows and
multinomial diffusion: Learning categorical distributions. In: Advances in Neural
Information Processing Systems (NeurIPS) (2021)
31. Hu, J., Shen, L., Albanie, S., Sun, G., Wu, E.: Squeeze-and-excitation networks.
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018)
32. Ke,Q.,Fritz,M.,Schiele,B.:Time-conditionedactionanticipationinoneshot.In:
IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)
33. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. International Con-
ference on Learning Representations (ICLR) (2014)
34. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Interna-
tional Conference on Learning Representations (ICLR) (2015)
35. Kohler, J., Pumarola, A., Schönfeld, E., Sanakoyeu, A., Sumbaly, R., Vajda, P.,
Thabet, A.: Imagine flash: Accelerating emu diffusion models with backward dis-
tillation. arXiv preprint arXiv:2405.05224 (2024)
36. Kuehne,H.,Arslan,A.,Serre,T.:Thelanguageofactions:Recoveringthesyntax
andsemanticsofgoal-directedhumanactivities.In:IEEEConferenceonComputer
Vision and Pattern Recognition (CVPR) (2014)36 O. Zatsarynna et al.
37. Li, Y., Liu, M., Rehg, J.M.: In the eye of the beholder: Gaze and actions in first
person video. IEEE Transactions on Pattern Analysis and Machine Intelligence
(TPAMI) (2020)
38. Lin, J., Gan, C., Han, S.: Tsm: Temporal shift module for efficient video under-
standing. In: IEEE International Conference on Computer Vision (ICCV) (2019)
39. Liu, D., Li, Q., Dinh, A.D., Jiang, T., Shah, M., Xu, C.: Diffusion action seg-
mentation. In: IEEE/CVF International Conference on Computer Vision (ICCV)
(2023)
40. Liu, G., Reda, F.A., Shih, K.J., Wang, T.C., Tao, A., Catanzaro, B.: Image in-
paintingforirregularholesusingpartialconvolutions.In:EuropeanConferenceon
Computer Vision (ECCV) (2018)
41. Liu, M., Tang, S., Li, Y., Rehg, J.M.: Forecasting human-object interaction: joint
predictionofmotorattentionandactionsinfirstpersonvideo.In:EuropeanCon-
ference on Computer Vision (ECCV) (2020)
42. Ma,X.,Fang,G.,Wang,X.:Deepcache:Acceleratingdiffusionmodelsforfree.In:
IEEE/CVF Conference on Computer Vision and Pattern Recognition (2024)
43. Mascaró, E., Ahn, H., Lee, D.: Intention-conditioned long-term human egocentric
action anticipation. In: IEEE Winter Conference on Applications of Computer
Vision (WACV) (2023)
44. Miech, A., Laptev, I., Sivic, J., Wang, H., Torresani, L., Tran, D.: Leveraging the
presenttoanticipatethefutureinvideos.In:IEEEConferenceonComputerVision
and Pattern Recognition Workshop (CVPRW) (2019)
45. Nag,S.,Zhu,X.,Deng,J.,Song,Y.Z.,Xiang,T.:Difftad:Temporalactiondetec-
tionwithproposaldenoisingdiffusion.In:IEEE/CVFInternationalConferenceon
Computer Vision (ICCV) (2023)
46. Nagarajan, T., Li, Y., Feichtenhofer, C., Grauman, K.: Ego-topo: Environment
affordances from egocentric video. In: IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2020)
47. Nawhal,M.,Jyothi,A.A.,Mori,G.:Rethinkinglearningapproachesforlong-term
actionanticipation.In:EuropeanConferenceonComputerVision(ECCV)(2022)
48. Van den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals, O., Graves, A., et al.:
Conditional image generation with pixelcnn decoders. Advances in Neural Infor-
mation Processing Systems (NeurIPS) (2016)
49. Popov,V.,Vovk,I.,Gogoryan,V.,Sadekova,T.,Kudinov,M.:Grad-tts:Adiffusion
probabilistic model for text-to-speech. In: International Conference on Machine
Learning (ICML) (2021)
50. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-
conditional image generation with clip latents. ArXiv preprint (2022)
51. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) (2022)
52. Sener,F.,Chatterjee,D.,Shelepov,D.,He,K.,Singhania,D.,Wang,R.,Yao,A.:
Assembly101:Alarge-scalemulti-viewvideodatasetforunderstandingprocedural
activities.IEEEConferenceonComputerVisionandPatternRecognition(CVPR)
(2022)
53. Sener, F., Singhania, D., Yao, A.: Temporal aggregate representations for long-
rangevideounderstanding.In:EuropeanConferenceonComputerVision(ECCV)
(2020)
54. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsuper-
visedlearningusingnonequilibriumthermodynamics.In:InternationalConference
on Machine Learning (ICML) (2015)Gated Temporal Diffusion for Stochastic Long-Term Dense Anticipation 37
55. Song,J.,Meng,C.,Ermon,S.:Denoisingdiffusionimplicitmodels.In:International
Conference on Learning Representations (ICLR) (2021)
56. Song, Y., Ermon, S.: Generative modeling by estimating gradients of the data
distribution.AdvancesinNeuralInformationProcessingSystems(NeurIPS)(2019)
57. Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Ermon,S.,Poole,B.:Score-
based generative modeling through stochastic differential equations. In: Interna-
tional Conference on Learning Representations (ICLR) (2021)
58. Stein, S., McKenna, S.J.: Combining embedded accelerometers with computer vi-
sionforrecognizingfoodpreparationactivities.ACMinternationaljointconference
on Pervasive and ubiquitous computing (2013)
59. Tae, J., Kim, H., Kim, T.: EdiTTS: Score-based Editing for Controllable Text-to-
Speech. In: Proc. Interspeech 2022 (2022)
60. Tanke, J., Zhang, L., Zhao, A., Tang, C., Cai, Y., Wang, L., Wu, P.C., Gall, J.,
Keskin, C.: Social diffusion: Long-term multiple human motion anticipation. In:
IEEE/CVF International Conference on Computer Vision (ICCV) (2023)
61. Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,
L., Polosukhin, I.: Attention is all you need. In: Advances in Neural Information
Processing Systems (NeurIPS) (2017)
62. Wang, H., Wu, Y., Guo, S., Wang, L.: Pdpp: Projected diffusion for procedure
planning in instructional videos. In: IEEE Conference on Computer Vision and
Pattern Recognition (CVPR) (2023)
63. Wang, X., Girshick, R.B., Gupta, A.K., He, K.: Non-local neural networks. IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (2018)
64. Wu, Y., Zhu, L., Wang, X., Yang, Y., Wu, F.: Learning to anticipate egocentric
actions by imagination. In: IEEE Transactions on Image Processing (TIP) (2020)
65. Xu,S.,Wang,Y.X.,Gui,L.Y.:Stochasticmulti-person3dmotionforecasting.In:
International Conference on Learning Representations (ICLR) (2023)
66. Yang, D., Yu, J., Wang, H., Wang, W., Weng, C., Zou, Y., Yu, D.: Diffsound:
Discrete diffusion model for text-to-sound generation. IEEE/ACM Transactions
on Audio, Speech, and Language Processing (2023)
67. Yi,F.,Wen,H.,Jiang,T.:Asformer:Transformerforactionsegmentation.In:The
British Machine Vision Conference (BMVC) (2021)
68. Yu,J.,Lin,Z.L.,Yang,J.,Shen,X.,Lu,X.,Huang,T.S.:Free-formimageinpaint-
ing with gated convolution. IEEE/CVF International Conference on Computer
Vision (ICCV) (2018)
69. Zatsarynna, O., Farha, Y., Gall, J.: Multi-modal temporal convolutional network
for anticipating actions in egocentric videos. In: IEEE Conference on Computer
Vision and Pattern Recognition Workshop (CVPRW) (2021)
70. Zatsarynna, O., Gall, J.: Action anticipation with goal consistency. In: IEEE In-
ternational Conference on Image Processing (ICIP) (2023)
71. Zatsarynna, O., Farha, Y.A., Gall, J.: Self-supervised learning for unintentional
actionprediction.In:DAGMGermanConferenceonPatternRecognition(GCPR)
(2022)
72. Zhang, M., Cai, Z., Pan, L., Hong, F., Guo, X., Yang, L., Liu, Z.: Motiondif-
fuse: Text-driven human motion generation with diffusion model. arXiv preprint
arXiv:2208.15001 (2022)
73. Zhao, H., Wildes, R.P.: On diverse asynchronous activity anticipation. In: Euro-
pean Conference on Computer Vision (ECCV) (2020)
74. Zhao,Y.,Krähenbühl,P.:Real-timeonlinevideodetectionwithtemporalsmooth-
ing transformers. In: European Conference on Computer Vision (ECCV) (2022)38 O. Zatsarynna et al.
75. Zhong, Z., Martin, M., Voit, M., Gall, J., Beyerer, J.: A survey on deep learning
techniques for action anticipation. In: Arxiv (2023)
76. Zhong, Z., Schneider, D., Voit, M., Stiefelhagen, R., Beyerer, J.: Anticipative fea-
turefusiontransformerformulti-modalactionanticipation.In:IEEEWinterCon-
ference on Applications of Computer Vision (WACV) (2023)