Impossibility of latent inner product recovery via rate distortion
Cheng Mao and Shenduo Zhang
School of Mathematics, Georgia Institute of Technology
July 17, 2024
Abstract
In this largelyexpository note, we presentan impossibility result for inner productrecovery
in a random geometric graph or latent space model using the rate-distortion theory. More
precisely,supposethatweobserveagraphAonnverticeswithaverageedgedensitypgenerated
from Gaussian or spherical latent locations z1,...,z
n
Rd associated with the n vertices. It
∈
is of interest to estimate the inner products z ,z which represent the geometry of the latent
i j
h i
points. We prove that it is impossible to recover the inner products if d&nh(p) where h(p) is
the binary entropy function. This matches the condition required for positive results on inner
product recoveryin the literature. The proof follows the well-established rate-distortiontheory
with the main technical ingredient being a lower bound on the rate-distortion function of the
Wishart distribution which is interesting in its own right.
1 Introduction
Random graphs with latent geometric structures comprise an important class of network models
usedacrossabroadrangeoffields[Pen03,HRH02,Bar11]. Inatypicalformulationofsuchamodel,
each vertex of a graph on n vertices is assumed to be associated with a latent location z Rd
i
∈
where i =1,...,n. With A 0,1 n n denoting the adjacency matrix of the graph, each edge A
× ij
∈ { }
follows the Bernoulli distribution with probability parameter κ(z ,z ), where κ : Rd Rd [0,1]
i j
× →
is a kernel function. In other words, the edges of the graph are formed according to the geometric
locations ofthevertices inalatent space. Given thegraphA, thecentral questionis thentorecover
the latent geometry, formulated as estimating the inner products z ,z 1.
i j
h i
In the study of this class of random graphs, a Gaussian or spherical prior is often imposed on
the latent locations z ,...,z , including in the early work on latent space models [HRH02, HRT07,
1 n
Hof07,KHRH09]andinthemorerecentworkonrandomgeometricgraphs[AVY19,EMP22,LS23].
Inparticular,theisotropicsphericalorGaussianpriorallowsthelatterlineofworktousethetheory
of spherical harmonics to analyze spectral methods for estimating the latent inner products. For a
class of kernels including the step function κ(z ,z ) = z ,z τ for a threshold τ, it is known
i j i j
{h i ≥ }
(see Theorem 1.4 of [LS23]) that the inner products can be estimated consistently if d nh(p)
≪
1Onecan alsoformulate theproblemasasestimating thepairwisedistances {kzi−zjk 2}n
i,j=1
whichisessentially
equivalent to inner product estimation. The problem is not formulated as estimating the latent locations {zi}n
i=1
themselves, because the kernel function κ is typically invariant under an orthogonal transformation of z 1,...,zn,
making them non-identifiable.
1
4202
luJ
61
]TS.htam[
1v23911.7042:viXra
1where p is the average edge density of the graph and h(p) is the binary entropy function. However,
a matching negative result was not established (as remarked in Section 1.3 of [LS23]).
Inthislargelyexpositorynote,weclosethisgapbyprovinginCorollary2.3thatitisinformation-
theoretically impossible to recover the inner products in a random geometric graph model if
d & nh(p), thereby showing that d nh(p) is indeed the recovery threshold2. In fact, it is not diffi-
≍
cult to predict this negative result from entropy counting: It is impossible to recover the geometry
of n vectors in dimension d from n binary observations with average bias p if nd& n h(p) since
2 2
there is not sufficient entropy. And this argument does not rely on the specific model (such as the
(cid:0) (cid:1) (cid:0) (cid:1)
kernel function κ) for generating the random graph A.
To formalize the entropy counting argument, the rate-distortion theory [Sha59] provides a stan-
dard approach (see also [Cov99, PW24] for a modern introduction). The key step in this approach
is a lower bound on the rate-distortion function of the estimand, i.e., X Rn n with X := z ,z
× ij i j
∈ h i
in our case. If z ,...,z are isotropic Gaussian vectors, then X follows the Wishart distribution.
1 n
Therefore, our main technical work lies in estimating the rate-distortion function for the Wishart
distribution (and its variant when z ,...,z are on a sphere), which has not been done explicitly
1 n
in the literature to the best of our knowledge. See Theorem 2.2.
The technical problem in this note is closely related to a work [LWB17] on low-rank matrix
estimation. To be more precise, Theorem VIII.17 of [LWB17] proves a lower bound on the rate-
distortion function of a rank-d matrix X = ZZ where Z Rn d. Our proof partly follows
⊤ ×
∈
the proof of this result but differs from it in two ways: First, the result of [LWB17] assumes
that Z is uniformly distributed on the Stiefel manifold, i.e., the columns of Z are orthonormal,
while we assume that Z has i.i.d. Gaussian or spherical rows. Without the simplification from
the orthonormality assumption, our proof requires different linear algebraic technicalities. Second,
the result of [LWB17] focuses on d n, while we also consider the case d > n which requires a
≤
completely different proof.
Finally, as a byproduct of the lower bound on the rate-distortion function of X, we present in
Corollary2.4animpossibilityresultforone-bitmatrixcompletion. Whileone-bitmatrixcompletion
has been studied extensively in the literature [DPVDBW14, CZ13, BJ15], less is known for the
Bayesian model where a prior is assumed on the matrix X to be estimated [CA18, Mai24]. Similar
to inner product estimation from a random geometric graph, the goal of one-bit matrix completion
is to estimate a (typically low-rank) matrix X from a set of binary observations. It is therefore
plausible that many techniques for random graphs can be used for one-bit matrix completion, and
vice versa. This note provides such an example.
2 Main results
In this section, we study the rate-distortion function for the Wishart distribution and its spherical
variant. Let I(X;Y) denote the mutual information between random variables X and Y. The
rate-distortion function is defined as follows (see Part V of [PW24]).
Definition 2.1 (Rate-distortion function). Let X be a random variable taking values in Rℓ, and
let P be a conditional distribution on Rℓ given X. Let L be a distortion measure (or a loss
Y X
|
2Another related statistical problem is testing a random geometric graph model against an Erd˝os–R´enyi graph
model with the same average edge density [BDER16]. This testing threshold, or detection threshold, is conjectured
to bed≍(nh(p))3, and thelower bound is still largely open. See [BDER16, BBN20, LMSY22].
2function), i.e., a bivariate function L : Rℓ Rℓ R . For D > 0, the rate-distortion function of
0
× → ≥
X with respect to L is defined as
RL(D) := inf I(X;Y).
X
PY|X:EL(X,Y) ≤D
Themaintechnicalresultofthisnoteisthefollowinglowerboundontherate-distortionfunction
of a Wishart matrix.
Theorem 2.2 (Rate-distortion function of a Wishart matrix). For positive integers n and d, let
Z := [z ...z ] Rn d where the i.i.d. rows z ,...,z follow either the Gaussian distribution
1 n ⊤ × 1 n
∈
(0, 1I ) or the uniform distribution on the unit sphere d 1 Rd. Let X := ZZ . Define a loss
N d d S − ⊂ ⊤
function3
d
L(X,Xˆ) := X Xˆ 2. (1)
n(n+1)k − kF
Let n d := min n,d . There is an absolute constant c> 0 such that for any D (0,c), we have
∧ { } ∈
1
RL(D) cn(n d)log .
X ≥ ∧ D
For d < n, the n n matrix X is rank-deficient and is a function of Z Rn d, so we expect
×
× ∈
the order nd for the rate-distortion function; for d n, we expect the order n2 considering the
≥
size of X. The matching upper bound on the rate-distortion function can be obtained using a
similar argument as that in Section 3.1 for small d and through a comparison with the Gaussian
distribution for large d (see Theorem 26.3 of [PW24]). Since it is in principle easier to obtain the
upper bound and only the lower bound will be used in the downstream statistical applications, we
do not state it here. Moreover, at the end of this section, we discuss the best possible constant c in
the above lower bound. The bulk of the paper, Section 3, will be devoted to proving Theorem 2.2.
With this theorem in hand, we first establish corollaries for two statistical models via entropy
counting.
Corollary 2.3 (Random geometric graph or latent space model). Fix positive integers n,d and a
parameter p (0,1). Suppose that we observe a random graph on n vertices with adjacency matrix
∈
A with average edge density p, i.e., E[A ]= n p. Suppose that A is generated according
(i,j) ([n]) ij 2
∈ 2
to an arbitrary model from the latent vectors z ,...,z given in Theorem 2.2, and the goal is to
P 1 n(cid:0) (cid:1)
estimate the inner products X := z ,z in the norm L defined in (1). If d cnh(p) where c > 0
ij i j
h i ≥
is any absolute constant and h(p) := plogp (1 p)log(1 p) is the binary entropy function,
− − − −
then for any estimator Xˆ measurable with respect to A, we have EL(X,Xˆ) D for a constant
≥
D = D(c) > 0.
Proof. TheestimandX,theobservationA,andtheestimatorXˆ formaMarkovchainX A Xˆ.
→ →
By the data processing inequality, we have
I(X;Xˆ) I(A;Xˆ) H(A),
≤ ≤
3ThenormalizationinthedefinitionofLischosensothatthetrivialestimatorEX =InofXhasriskEL(X,Xˆ)=1
in thecase of Gaussian zi, since E[X i2 j]=E[hzi,zji2]=1/d for i6=j and E[(Xii−1)2]=E[(hzi,zii−1)2]=2/d.
3where H(A) denotes the entropy of A. Since E[A ] = n p, by the maximum entropy
(i,j) ([n]) ij 2
∈ 2
under the Hamming weight constraint (see Exercise I.7 of [PW24]), we get
P (cid:0) (cid:1)
n
H(A) h(p).
≤ 2
(cid:18) (cid:19)
If EL(X,Xˆ) D, then combining the above inequalities with Theorem 2.2 gives
≤
1 n
cn(n d)log RL(D) I(X,Xˆ) h(p).
∧ D ≤ X ≤ ≤ 2
(cid:18) (cid:19)
Taking D > 0 to be a sufficiently small constant, we then get n d < cnh(p), i.e., d< cnh(p).
∧
As a second application of Theorem 2.2, we consider one-bit matrix completion with a Wishart
prior.
Corollary 2.4 (One-bit matrix completion). Fix positive integers n,d and a parameter p (0,1).
∈
Suppose that X Rn n is a rank-d matrix to be estimated. Assume the prior distribution of X
×
∈
as given in Theorem 2.2. For each entry (i,j) [n]2, suppose that with probability p , we have a
ij
∈
one-bit observation A 0,1 according to an arbitrary model, and with probability 1 p , we do
ij ij
∈ { } −
not have an observation, denoted as A = . Let p be the average probability of observations, i.e.,
ij
n p = n2p. Let L be the loss function∗ defined in (1). If d cn(h(p)+p) where c > 0 is any
i,j=1 ij ≥
absolute constant and h(p) := plogp (1 p)log(1 p), then for any estimator Xˆ measurable
P − − − −
with respect to A, we have EL(X,Xˆ) D for a constant D = D(c) > 0.
≥
Proof. The argument is the same as the proof of Corollary 2.3, except the bound on the entropy
of A. Let Z 0,1 n n have Bernoulli(p ) entries such that Z = A = . Then we have
× ij ij ij
∈ { } { 6 ∗}
the conditional entropy H(Z A) = 0. Conditional on any value of Z, the entropy of A is at most
|
log2 Z 1. As a result,
k k
H(A Z) E log2 Z 1 = n2plog2.
Z k k
| ≤
We therefore obtain
H(A) = H(A Z)+I(Z;A) =H(A Z)+H(Z) n2(h(p)+plog2).
| | ≤
The rest of the proof is the same as that for the random geometric graph model.
Open problems. Several interesting problems are left open.
• Sharp constant: Recall that the lower bound on the rate-distortion function of the Wishart
distribution in Theorem 2.2. While the order n(n d)log 1 is believed to be optimal, we
∧ D
did not attempt to obtain the sharp constant factor. In the case d n, the rate-distortion
≥
function can be bounded from above by that of a Gaussian Wigner matrix, and the best
leading constant is 1/4 (see Theorems 26.2 and 26.3 of [PW24]). Indeed, the end result of
Section 3.2 indeed shows a lower bound with the constant 1/4 in the leading term if D 0.
→
In the case d/n 0, Lemma 3.3 suggests that the best constant may be 1/2, but we did not
→
make the effort to obtain it as the end result. Themost difficult situation appears to bewhen
d < n = O(d), in which case our techniques fail to obtain any meaningful constant factor.
4
1• Optimal rate: Combined with the work [LS23], Corollary 2.3 gives the recovery threshold
d nh(p)for randomgeometric graphs withGaussian or sphericallatent locations. However,
≍
it remains open to obtain an optimal lower bound on EL(X,Xˆ) as a function of d,n,p in the
regime d nh(p). We believe the simple approach of entropy counting is not sufficient for
≪
obtaining the optimal rate and new tools need to be developed.
• General latent distribution: Existing positive and negative results for estimation in random
geometric graph models are mostly limited to isotropic distributions of latent locations, such
as Gaussian or spherical in [AVY19, EMP22, LS23] and this work. It is interesting to extend
these results to more general distributions and metric spaces; see [BB23a, BB23b] for recent
work. Even for random geometric graphs with anisotropic Gaussian latent points, while there
has been progress on the detection problem [EM20, BBH24], extending the recovery results
to the anisotropic case remains largely open.
3 Proof of Theorem 2.2
Let c (0,1) be some absolute constant to be determined later. We first consider the Gaussian
∗
∈
model where z (0, 1I ). The proof is split into three cases d c n, d n, and c n < d < n,
i ∼ N d d ≤ ∗ ≥ ∗
provedinSections3.1,3.2,and3.3respectively. WethenconsiderthesphericalmodelinSection3.4.
3.1 Case d c n
∗
≤
To study the rate-distortion function of X = ZZ , we connect it to the rate-distortion function
⊤
of Z in the distortion measure to be defined in (2). The strategy is inspired by [LWB17], but the
key lemma connecting the distortion of X to that of Z is different. For Z,Zˆ Rd d, define a loss
×
∈
function for recovering Z up to an orthogonal transformation
1
ℓ(Z,Zˆ):= inf Z ZˆO 2, (2)
nO (d)k − kF
∈O
where (d) denotes the orthogonal group in dimension d. The normalization is chosen so that
O
Eℓ(Z,EZ) =Eℓ(Z,0) = 1. We start with a basic linear algebra result.
Lemma 3.1. Let A,B Rn d. For the loss functions L and ℓ defined by (1) and (2) respectively,
×
∈
we have
n+1
ℓ(A,B) L(AA ,BB ).
≤ n ⊤ ⊤
r
Proof. Consider the polar decompositions A = (AA )1/2U and B = (BB )1/2V where U,V
⊤ ⊤
∈
(d). Then we have
O
1
ℓ(A,B) = inf A BO 2
nO (d)k − kF
∈O
1
(AA )1/2U (BB )1/2V(V U) 2
≤ nk ⊤ − ⊤ ⊤ kF
1
= (AA )1/2 (BB )1/2 2.
nk ⊤ − ⊤ kF
5The Powers–Størmer inequality [PS70] gives
(AA )1/2 (BB )1/2 2 AA BB ,
k ⊤ − ⊤ kF≤ k ⊤ − ⊤ k∗
where denotes the nuclear norm. In addition, AA and BB are at most rank d, so
⊤ ⊤
k·k∗
1 √d n+1
ℓ(A,B) AA BB AA BB = L(AA ,BB ).
≤ nk ⊤ − ⊤ k∗≤ n k ⊤ − ⊤ kF
r
n ⊤ ⊤
Next, we relate the rate-distortion function of X = ZZ in the loss L to the rate-distortion
⊤
function of Z in the loss ℓ.
Lemma 3.2. Let Z and X be defined as in Theorem 2.2, and let L and ℓ be defined by (1) and
(2) respectively. Recall the notation of the rate-distortion function in Definition 2.1. For D > 0,
we have
RL(D) Rℓ (√8D).
X ≥ Z
Proof. Fix a conditional distribution P such that EL(X,Y) D. Define
Y X
| ≤
Z˜ = argmin Y WW ,
⊤ F
W
Rn×dk − k
∈
where the non-unique minimizer Z˜ is chosen arbitrarily. Then we have
ZZ Z˜Z˜ ZZ Y + Y Z˜Z˜ 2 ZZ Y .
⊤ ⊤ F ⊤ F F ⊤ F
k − k ≤ k − k k − k ≤ k − k
In other words,
L(ZZ ,Z˜Z˜ ) 4L(X,Y).
⊤ ⊤
≤
By Lemma 3.1,
ℓ(Z,Z˜) 2L(ZZ ,Z˜Z˜ ) 8L(X,Y).
⊤ ⊤
≤ ≤
q
Jensen’s inequality then yields p
Eℓ(Z,Z˜) E 8L(X,Y) 8EL(X,Y) √8D.
≤ ≤ ≤
p p
Let O be a uniform random orthogonal matrix over (d), independentfrom everything else. In
O
view of the definition of ℓ, we have
Eℓ(ZO,Z˜)= Eℓ(Z,Z˜) √8D.
≤
Therefore, by the definition of the rate-distortion function Rℓ (see Definition 2.1),
Z
I(ZO;Z˜) Rℓ (√8D) = Rℓ (√8D),
≥ ZO Z
where the equality follows from the orthogonal invariance of the distribution of Z.
Next, we note that
I(ZO;Z˜) I(ZZ ;Z˜).
⊤
≤
6(In fact, equality holds because the reverse inequality is trivial by data processing.) To see this,
given ZZ , take any A Rn d such that ZZ = AA , and let Q be a uniform random orthogonal
⊤ × ⊤ ⊤
∈
matrix over (d) independent from everything else. Since A = ZP for some P (d), we
O ∈ O
have (AQ,Z˜) = (ZPQ,Z˜) =d (ZO,Z˜), where =d denotes equality in distribution. Hence, the data
processing inequality gives I(ZZ ;Z˜) I(AQ;Z˜) = I(ZO;Z˜).
⊤
≥
Combining the above two displays and recalling that Z˜ is defined from Y, we apply the data
processing inequality again to obtain
I(X;Y) I(ZZ ;Z˜Z˜ ) Rℓ (√8D).
≥ ⊤ ⊤ ≥ Z
Minimizing P subject to the constrain EL(X,Y) D yields the the rate-distortion function
Y X
RL(D) on the l| eft-hand side, completing the proof. ≤
X
Lemma 3.3. Let Z be defined as in Theorem 2.2, let ℓ be defined by (2), and let Rℓ be given by
Z
Definition 2.1. There is an absolute constant C > 0 such that for any D (0,1/4), we have
∈
nd 1 d2 C
Rℓ (D) log log .
Z ≥ 2 4D − 2 D
Proof. Fix a conditional distribution P such that Eℓ(Z,Zˆ) D. Let O = O(Z,Zˆ) (d) be
ZˆZ
≤ ∈ O
such that 1 ZˆO Z 2= ℓ(Z,Zˆ). Then| we have E ZˆO Z 2 nD. Let N( (d),ǫ) be an ǫ-net
nk − kF k − kF≤ O
of (d) with respect to the Frobenius norm, where ǫ2 = nD d. For O = O(Z,Zˆ), choose
O E Z 2 ∧
k k2
Oˆ = Oˆ(Z,Zˆ) N( (d),ǫ) such that Oˆ O 2 ǫ2. Define W := ZˆOˆ. We have
∈ O k − kF≤
E W Z 2 = E ZˆOˆ Z 2=E Zˆ ZOˆ 1 2
k − kF k − kF k − − kF
2E Zˆ ZO 1 2+2E ZO 1 ZOˆ 1 2
≤ k − − kF k − − − kF
2E ZˆO Z 2+2E Z 2 O 1 Oˆ 1 2
≤ k − kF k k k − − − kF
2nD+2ǫ2E Z 2=4nD,
≤ k k
where denotes the spectral norm.
k·k
By Theorem 26.2 of [PW24] (with d replaced by ndand σ2 replaced by 1/d), the rate-distortion
function of Z with respect to the Frobenius norm L (Z,W) := Z W 2 is
0 k − kF
nd n
RL0(D) = log . (3)
Z 2 D
Since E W Z 2 4nD, we obtain
k − kF≤
nd 1
I(Z;W) RL0(4nD)= log .
≥ Z 2 4D
Moreover, we have
I(Z;W) I(Z;Zˆ,Oˆ) = I(Z;Zˆ)+I(Z;Oˆ Zˆ) I(Z;Zˆ)+H(Oˆ),
≤ | ≤
where the three steps follow respectively from the data processing inequality, the definition of
conditional mutual information I(Z;Oˆ Zˆ), and a simple bound on the mutual information by the
|
entropy. The above two inequalities combined imply
nd 1
I(Z;Zˆ) I(Z;W) H(Oˆ) log H(Oˆ).
≥ − ≥ 2 4D −
7Since Oˆ N( (d),ǫ), the entropy H(Oˆ) can be bounded by the metric entropy of (ǫ). By
∈ O O
Theorem 8 of [Sza97], there is an absolute constant C > 1 such that the covering number of (d)
0
O
d2
with respect to the Frobenius norm is at most √C0d for any ǫ (0,√d). We have
ǫ ∈
(cid:16) (cid:17)
nD
ǫ = √d c √dD
sE Z 2 ∧ ≥ 1
k k
for an absolute constant c > 0, where the bound follows from the concentration of Z at order
1
k k
O(√n+√d)
(see, e.g., Corollary 5.35 of [Ver10]) and that d n. Therefore,
√d ≤
d2 C d d2 C
H(Oˆ) log N( (d) log 0 log 0 .
≤ | O |≤ 2 ǫ2 ≤ 2 c2D
1
Putting it together, we obtain
nd 1 d2 C
I(Z;Zˆ) log log 0 ,
≥ 2 4D − 2 c2D
1
finishing the proof in view of the definition of RL(D).
Z
Combining Lemmas 3.2 and 3.3, we conclude that
nd 1 d2 C nd 1
RL(D) log log log
X ≥ 2 4√8D − 2 √8D ≥ 8 D
provided that D (0,c ) and d c n for a sufficiently small constant c > 0.
∗ ∗ ∗
∈ ≤
3.2 Case d n
≥
In the case d n, the Wishart distribution of X = ZZ has a density on the set of symmetric
⊤
≥
matrices Rn(n+1)/2, and we can apply the Shannon lower bound [Sha59] on the rate-distortion
function. See Equation (26.5) and Exercise V.22 of the book [PW24] (with the norm taken to be
the Euclidean norm and r = 2) for the following result.
Lemma 3.4 (Shannon lower bound [Sha59]). Let Y be a continuous random vector with a density
on RN. For D > 0, let RL0(D) be the rate-distortion function of Y with respect to the Euclidean
Y
norm L (Y,Yˆ):= Y Yˆ 2. Let h(Y) denote the differential entropy of Y. Then we have
0 k − k2
N 2πeD
RL0(D) h(Y) log .
Y ≥ − 2 N
As a result, for the loss L defined by (1) and the random matrix X distributed over Rn(n+1)/2, we
have
n(n+1) 4πeD
RL(D) h(X) log .
X ≥ − 4 d
The differential entropy h(X) of the Wishart matrix X is known.
8Lemma 3.5 (Differential entropy of a Wishart matrix [LR78]). For X defined in Theorem 2.2, we
have
n(n+1) 2 d d n 1 d nd
h(X) = log +logΓ − − ψ + ,
n n
2 d 2 − 2 2 2
(cid:18) (cid:19) (cid:18) (cid:19)
where Γ is the multivariate gamma function and ψ is the multivariate digamma function.
n n
The above two results combined give the lower bound
n(n+1) 2 d d n 1 d nd n(n+1) 4πeD
RL(D) log +logΓ − − ψ + log
X ≥ 2 d n 2 − 2 n 2 2 − 4 d
(cid:18) (cid:19) (cid:18) (cid:19)
nd n(n+1) 1 d d n 1 d
= + log +logΓ − − ψ . (4)
n n
2 4 πeDd 2 − 2 2
(cid:18) (cid:19) (cid:18) (cid:19)
We now analyze the functions Γ and γ . By Stirling’s approximation for the gamma function
n n
log(2π)
(see Equation 6.1.40 of [AS48]), we have logΓ(x + 1/2) xlog(x +1/2) x 1/2 + for
≥ − − 2
x 0. Together with the definition of the multivariate gamma function Γ , this gives
n
≥
n
d n(n 1) d+1 i
logΓ = − logπ+ logΓ −
n
2 4 2
(cid:18) (cid:19) i=1 (cid:18) (cid:19)
X
n
n(n 1) d i d+1 i d+1 i log(2π/e)
− logπ+ − log − − +
≥ 4 2 2 − 2 2
i=1(cid:18) (cid:19)
X
n2 nd n d i d+1 i
log(πe) + − log − O(n).
≥ 4 − 2 2 2 −
i=1(cid:18) (cid:19)
X
Moreover, by Equation (2.2) of [Alz97], the digamma function satisfies logx 1 < ψ(x) < logx for
−x
x > 0. Combining this with the definition of the multivariate digamma function ψ , we obtain
n
n
d n 1 d d n 1 d+1 i
− − ψ = − − ψ −
n
2 2 2 2
(cid:18) (cid:19) i=1 (cid:18) (cid:19)
X
n
d n 1 d+1 i
− − log − +O(n),
≤ 2 2
i=1
X
where we note that the O(n) term is only necessary in the case that d= n and d n 1 is negative.
−2−
Plugging the above two estimates into (4), we see that
n
n(n+1) 1 n+1 i d+1 i
RL(D) log + − log − O(n). (5)
X ≥ 4 Dd 2 2 −
i=1(cid:18) (cid:19)
X
If d 2n, then
≥
n
n(n+1) 1 d+1 n n+1 i
RL(D) log + log − − O(n)
X ≥ 4 Dd 2 2 −
(cid:18) (cid:19)i=1
X
n(n+1) 1 n(n+1) d+1 n
= log + log − O(n)
4 D 4 2d −
n(n+1) 1
log O(n2).
≥ 4 D −
9For n d < 2n, we first note that the term n+1 i log d+1 i with i = n can be dropped from the
≤
2− 2−
sumin(5),because n+1 n log d+1 n < 0onlyifd= n,inwhichcasethenegative quantity 1log 1 is
2− 2− 2 2
subsumed by the O(n) term. Furthermore, since the function x n+1 xlog d+1 x is decreasing
− 7→
2− 2−
on [1,n], we have
n −1 n+1 i d+1 i n n+1 x d+1 x
− log − − log − dx
2 2 ≥ 2 2
i=1(cid:18) (cid:19) Z1
X
2dn d2 d n2 1
= − log + − log(d+1 n)+O(n2),
4 d+1 n 4 −
−
where the integral can be evaluated explicitly but we suppress O(n2) terms for brevity. Plugging
this back into (5), we obtain
n(n+1) 1 2dn d2 n2+1 d
RL(D) log + − − log O(n2).
X ≥ 4 D 4 d+1 n −
−
Since 2dn d2 n2 0 and log d n 1 n 1, it holds that
− − ≤ d+1 n ≤ d+−1 n ≤ d−n
− − −
2dn d2 n2+1 d 2dn d2 n2 n 1 1
− − log − − − = (d n)(n 1).
4 d+1 n ≥ 4 · d n −4 − −
− −
(While the above argument relied on d > n due to the presence of d n in the denominator, the
−
conclusion clearly holds for d= n.) Consequently, we again have
n(n+1) 1
RL(D) log O(n2).
X ≥ 4 D −
This readily implies the desired lower bound.
3.3 Case c n < d < n
∗
This case can be easily reduced to the case d n. Fix a conditional distribution P such that
Y X
EL(X,Y) D. Let X be the top left d d p≥ rincipal minor of X and define Y sim| ilarly. Then
d d
≤ ×
X clearly has the Wishart distribution as X in Theorem 2.2 with n replaced by d. Let L be the
d d
loss L in (1) with n replaced by d. Then we have
d d 1
L (X ,Y ) = X Y 2 X Y 2= L(X,Y),
d d d d(d+1)k d − d kF≤ (c )2n(n+1)k − kF (c )2
∗ ∗
so EL (X ,Y ) D/(c )2. Applying the result for the case d= n, we get
d d d ∗
≤
d(d+1) (c )2 c nd (c )2
I(X ;Y ) log ∗ O(d2) ∗ log ∗ O(nd).
d d
≥ 4 D − ≥ 4 D −
Since I(X;Y) I(X ;Y ), to complete the proof, it remains to take D c for a sufficiently small
d d
≥ ≤
constant c> 0 depending only on c and the hidden constant in O(nd).
∗
103.4 Spherical case
We now consider the case Z = [z ...z ] and X = ZZ wherez ,...,z are i.i.d. uniform random
1 n ⊤ ⊤ 1 n
vectors over the unit sphere d 1 Rd. The proof is via a reduction from the Gaussian case. Let
−
S ⊂
w ,...,w be i.i.d. (0, 1I ) vectors and let β := w , so that z = w /β and w = β z . Let
1 n N d d i k i k2 i i i i i i
B Rn n be the diagonal matrix with β ,...,β on its diagonal. Let Y = BXB. Then Y has the
× 1 n
∈
distribution of X in the case where z ,...,z are Gaussian vectors, so the result of the Gaussian
1 n
case gives
1
RL(D) cn(n d)log . (6)
Y ≥ ∧ D
Fix a conditional distribution P such that EL(X,Xˆ) D. Let g ,...,g be i.i.d. (0,δ2)
Xˆ X ≤ 1 n N
randomvariablesindependentfromev| erythingelse,whereδ > 0istobechosen. Defineβˆ := β +g ,
i i i
and let Bˆ Rn n bethe diagonal matrix with βˆ ,...,βˆ on its diagonal. Define Yˆ := BˆXˆBˆ. Since
× 1 n
∈
z is independent from β , we see that (X,Xˆ) is independent from (B,Bˆ). Hence,
i i
I(Y;Yˆ) I(X,B;Xˆ,Bˆ) = I(X;Xˆ)+I(B;Bˆ).
≤
For the term I(B;Bˆ), the independence across the pairs (β ,βˆ) for i= 1,...,n implies
i i
n
I(B;Bˆ) = I(β ;βˆ) = nI(β ;βˆ ).
i i 1 1
i=1
X
We have Var(β ) = Var( w ) = 1(d 2Γ((d+1)/2)2 ) 1/(2d) using the variance of the χ dis-
1 k i k2 d − Γ(d/2)2 ≤ d
tribution and basic properties of the gamma function. Let g (0,1/(2d)). Then the Gaussian
′
∼ N
saddle point theorem (see Theorem 5.11 of [PW24]) gives
1 1
I(β ;βˆ ) I(g ;g +g )= log 1+ .
1 1 ≤ ′ ′ 1 2 2dδ2
(cid:18) (cid:19)
The above three displays combined yield
n 1
I(X;Xˆ) I(Y;Yˆ) log 1+ . (7)
≥ − 2 2dδ2
(cid:18) (cid:19)
It remains to bound I(Y;Yˆ) from below. To this end, note that
Yˆ Y 2 = BˆXˆBˆ BXB 2
k − kF k − kF
2 BˆXˆBˆ BˆXBˆ 2+2 BˆXBˆ BXB 2
≤ k − kF k − kF
n n
= 2 βˆ2βˆ2(Xˆ X )2+2 X2(βˆβˆ β β )2.
i j ij − ij ij i j − i j
i,j=1 i,j=1
X X
Sinceβˆ = β +g ,wehaveE[βˆ2]= E[β2]+E[g2]= 1+δ2. Moreover, wehaveE[X2]= E[(z z )2] = 1
i i i i i i ii i⊤ i
and E[X2] = E[(z z )2] = 1/d for i = j. Finally,
ij i⊤ j 6
E[(βˆβˆ β β )2]= E[(β g +β g +g g )2]= 2δ2 +E[g2g2]+2E[β β ]E[g g ]
i j − i j i j j i i j i j i j i j
11so E[(βˆ2 β2)2] = 4δ2 + 3δ4 and E[(βˆβˆ β β )2] = 2δ2 + δ4 for i = j. Since βˆ ,...,βˆ are
i − i i j − i j 6 1 n
independent and B,Bˆ,X are mutually independent, we conclude that
n(n 1)
E Yˆ Y 2 2(1+δ2)2E Xˆ X 2+2n(4δ2 +3δ4)+2 − (2δ2 +δ4)
k − kF ≤ k − kF d
n(n+1) n n(n 1)
8 D+14 D+6 − D,
≤ d d d2
where we used that EL(X,Xˆ) D for the loss L defined in (1) and chose δ2 = D/d < 1. Hence,
we have EL(Y,Yˆ) 28D. This≤ together with (6) implies that
≤
1
I(Y;Yˆ) cn(n d)log .
≥ ∧ 28D
Plugging this bound into (7), we obtain
1 n 1
I(X;Xˆ) cn(n d)log log 1+ .
≥ ∧ 28D − 2 2D
(cid:18) (cid:19)
The above bound completes the proof if d C for some constant C > 0 depending only on
≥
c. For the case d C (in fact, for the entire case d c n), it suffices to note that the proof in
∗
≤ ≤
Section3.1alsoworksforthesphericalmodel. Tobemoreprecise, thereareonlythreeplaceswhere
the Gaussianity assumption is used. First, the proof of Lemma 3.2 uses the orthogonal invariance
of the distribution of the rows of Z, which is also true for the spherical model where z is uniform
i
over d 1. Second, (3) uses the rate-distortion function of the entrywise Gaussian matrix Z. In
−
S
the case where Z have i.i.d. rows distributed uniformly over d 1, it suffices to replace this formula
−
S
by a lower bound: By Theorems 27.17 and 24.8 of [PW24], we have
n(d 1) 1
RL0(D) − log nC
Z ≥ 2 D − 2
for an absolute constant C > 0, which is sufficient for the rest of the proof. Third, the proof of
2
Lemma 3.3 also uses that E Z 2 is of order n+d, which is obviously true if d is of constant size and
k k d
the rows of Z are on the unit sphere.
Acknowledgments
This work was supported in part by NSF grants DMS-2053333, DMS-2210734, and DMS-2338062.
We thank Shuangping Li, Eric Ma, and Tselil Schramm for generously sharing their different
approachtoasimilarresultonrandomgeometricgraphs;thetwoworksweredevelopedconcurrently
and independently. We thank Yihong Wu and Jiaming Xu for helpful discussions on the rate-
distortion theory.
References
[Alz97] Horst Alzer. On some inequalities for the gamma and psi functions. Mathematics
of computation, 66(217):373–389, 1997.
12[AS48] Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions with
formulas, graphs, and mathematical tables, volume 55. US Government printing
office, 1948.
[AVY19] Ernesto Araya Valdivia and De Castro Yohann. Latent distance estimation for
random geometric graphs. Advances in Neural Information Processing Systems, 32,
2019.
[Bar11] Marc Barth´elemy. Spatial networks. Physics reports, 499(1-3):1–101, 2011.
[BB23a] Kiril Bangachev and Guy Bresler. Detection of L geometry in random geo-
∞
metric graphs: Suboptimality of triangles and cluster expansion. arXiv preprint
arXiv:2310.14501, 2023.
[BB23b] Kiril Bangachev and Guy Bresler. Random algebraic graphs and their convergence
to erdos-renyi. arXiv preprint arXiv:2305.04802, 2023.
[BBH24] Matthew Brennan, Guy Bresler, and Brice Huang. Threshold for detecting high
dimensional geometry in anisotropic random geometric graphs. Random Structures
& Algorithms, 64(1):125–137, 2024.
[BBN20] Matthew Brennan, Guy Bresler, and Dheeraj Nagaraj. Phase transitions for de-
tecting latent geometry in random graphs. Probability Theory and Related Fields,
178(3-4):1215–1289, 2020.
[BDER16] S´ebastien Bubeck, Jian Ding, Ronen Eldan, and Miklo´s Z Ra´cz. Testing for
high-dimensional geometry in random graphs. Random Structures & Algorithms,
49(3):503–532, 2016.
[BJ15] Sonia A Bhaskar and Adel Javanmard. 1-bit matrix completion under exact low-
rank constraint. In 2015 49th Annual Conference on Information Sciences and
Systems (CISS), pages 1–6. IEEE, 2015.
[CA18] Vincent Cottet and Pierre Alquier. 1-bit matrix completion: Pac-bayesian analysis
of a variational approximation. Machine Learning, 107(3):579–603, 2018.
[Cov99] Thomas M Cover. Elements of information theory. John Wiley & Sons, 1999.
[CZ13] TonyCaiandWen-XinZhou. Amax-normconstrainedminimizationapproachto1-
bit matrix completion. Journal of Machine Learning Research, 14(114):3619–3647,
2013.
[DPVDBW14] Mark A Davenport, Yaniv Plan, Ewout Van Den Berg, and Mary Wootters. 1-bit
matrixcompletion. Information andInference: AJournalofthe IMA,3(3):189–223,
2014.
[EM20] Ronen Eldan and Dan Mikulincer. Information and dimensionality of anisotropic
random geometric graphs. In Geometric Aspects of Functional Analysis: Israel
Seminar (GAFA) 2017-2019 Volume I, pages 273–324. Springer, 2020.
13[EMP22] Ronen Eldan, Dan Mikulincer, and Hester Pieters. Community detection and per-
colation ofinformationinageometric setting. Combinatorics, Probability and Com-
puting, 31(6):1048–1069, 2022.
[Hof07] Peter Hoff. Modeling homophily and stochastic equivalence in symmetric relational
data. Advances in neural information processing systems, 20, 2007.
[HRH02] Peter D Hoff, Adrian E Raftery, and Mark S Handcock. Latent space ap-
proaches to social network analysis. Journal of the american Statistical association,
97(460):1090–1098, 2002.
[HRT07] MarkSHandcock, AdrianERaftery, andJeremyMTantrum. Model-basedcluster-
ing for social networks. Journal of the Royal Statistical Society Series A: Statistics
in Society, 170(2):301–354, 2007.
[KHRH09] Pavel N Krivitsky, Mark S Handcock, Adrian E Raftery, and Peter D Hoff. Rep-
resenting degree distributions, clustering, and homophily in social networks with
latent cluster random effects models. Social networks, 31(3):204–213, 2009.
[LMSY22] Siqi Liu, Sidhanth Mohanty, Tselil Schramm, and Elizabeth Yang. Testing thresh-
olds for high-dimensional sparse random geometric graphs. In Proceedings of the
54th Annual ACM SIGACT Symposium on Theory of Computing, pages 672–677,
2022.
[LR78] A.V. Lazo and P. Rathie. On the entropy of continuous probability distributions
(Corresp.). IEEE Transactions on Information Theory, 24(1):120–122, January
1978.
[LS23] Shuangping Li and Tselil Schramm. Spectral clustering in the gaussian mixture
block model. arXiv preprint arXiv:2305.00979, 2023.
[LWB17] KiryungLee,YihongWu,andYoramBresler. Near-optimal compressedsensingofa
class of sparselow-rank matrices via sparsepower factorization. IEEE Transactions
on Information Theory, 64(3):1666–1698, 2017.
[Mai24] The Tien Mai. Concentration properties of fractional posterior in 1-bit matrix
completion. arXiv preprint arXiv:2404.08969, 2024.
[Pen03] Mathew Penrose. Random geometric graphs, volume 5. OUP Oxford, 2003.
[PS70] RobertT Powers and Erling Størmer. Free states of the canonical anticommutation
relations. Communications in Mathematical Physics, 16(1):1–33, 1970.
[PW24] Yury Polyanskiy and Yihong Wu. Information Theory: From Coding to Learning.
Cambridge University Press, 2024.
[Sha59] Claude E Shannon. Coding theorems for a discrete source with a fidelity criterion.
IRE Nat. Conv. Rec, 4(142-163):1, 1959.
[Sza97] Stanislaw J. Szarek. Metric Entropy of Homogeneous Spaces, January 1997.
14[Ver10] RomanVershynin. Introductiontothenon-asymptoticanalysisofrandommatrices,
November 2010.
15