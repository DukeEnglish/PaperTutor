Tackling Oversmoothing in GNN via Graph
Sparsification: A Truss-based Approach
Tanvir Hossain∗, Khaled Mohammed Saifuddin∗, Muhammad Ifte Khairul Islam∗, Farhan Tanvir∗, Esra Akbas∗
∗Department of Computer Science, Georgia State University, Atlanta, GA 30302, USA
{thossain5, ksaifuddin1, mislam29}@student.gsu.edu, {ftanvir, eakbas1}@gsu.edu
Abstract—Graph Neural Network (GNN) achieves great suc- varies for complex connections in different graph areas, and
cessfornode-levelandgraph-leveltasksviaencodingmeaningful an individual node with high degrees converges to stationary
topological structures of networks in various domains, ranging
states earlier than lower-degree nodes. Hence, the networks’
fromsocialtobiologicalnetworks.However,repeatedaggregation
regional structures affect the phenomenon because repeated
operations lead to excessive mixing of node representations,
particularlyindenseregionswithmultipleGNNlayers,resulting messagepassingoccurswithinthedenseneighborhoodregions
in nearly indistinguishable embeddings. This phenomenon leads of the nodes. Therefore, we observe the impact of congested
to the oversmoothing problem that hampers downstream graph graph regions on oversmoothing.
analytics tasks. To overcome this issue, we propose a novel
We conduct a small experiment to demonstrate the early
and flexible truss-based graph sparsification model that prunes
oversmoothing at highly connected regions on a toy graph
edgesfromdenseregionsofthegraph.Pruningredundantedges
in dense regions helps to prevent the aggregation of excessive (Figure1(a)).Tocalculatethedensityonthegraph,weutilize
neighborhood information during hierarchical message passing the k-truss [1], one of the widely used cohesive subgraph
and pooling in GNN models. We then utilize our sparsification extraction models based on the number of triangles each edge
model in the state-of-the-art baseline GNNs and pooling models,
contains. To show the smoothness of the node features, we
such as GIN, SAGPool, GMT, DiffPool, MinCutPool, HGP-SL,
utilizetheaveragenoderepresentationdistance(ANRD)[11].
DMonPool, and AdamGNN. Extensive experiments on different
real-world datasets show that our model significantly improves WemeasuretheANRDofdifferentk-trussregionsandpresent
the performance of the baseline GNN models in the graph how it changes through the increasing number of layers in
classification task. GNN.WepresentthetoygraphandANRDvalueswithrespect
Index Terms—GNN, Oversmoothing, Graph Sparsification, k-
to the number of layers in Figure 1(b). While the toy graph in
truss Subgraphs, Graph Classification.
thefigureisa4-trussgraph,ithas6,7,and8-trusssubgraphs.
Nodes and edges are colored based on their trussness. As
I. INTRODUCTION
known, k-truss subgraphs have hierarchical relations, e.g., 7
In recent years, graph neural networks (GNN) have given and 8-truss subgraphs are included in the 6-truss subgraph.
promising performance in numerous applications over differ- Even at layer 2, we observe the ANRD of 7 and 8-truss
entdomains,suchasgeneexpressionanalysis[19],trafficflow subgraphs substantially degrades compared to the lower truss
forecasting [28], fraud detection [22], and recommendation (k =4,6) subgraphs.
system[7].GNNeffectivelylearnstherepresentationofnodes While oversmoothing is observed at the node level, it may
and graphs via encoding topological graph structures into also result in losing crucial information for the graphs’ repre-
low-dimensional space through message passing and aggrega- sentation to distinguish them. Furthermore, to learn the graph
tion mechanisms. To learn the higher-order relations between representation, GNNs employ various hierarchical pooling
nodes, especially for large graphs, we need to increase the approaches, including hierarchical coarsening and message-
numberoflayers.However,creatinganexpressiveGNNmodel
by adding more convolution layers increases redundant recep-
tive fields for computational nodes and results in oversmooth-
ing as node representations become nearly indistinguishable.
Severalresearchworksillustratethatduetooversmoothing,
nodes lose their unique characteristics [5], [9], adversely
affecting GNNs’ performance on downstream tasks, including
nodeandgraphclassification.Differentmodelshavebeenpro-
posed to overcome the problem, such as skip connection [6],
dropedge[17],GraphCON[18].Whilemanyofthesemethods
focus on node classification, they often overlook the impact
(a) Experiment Graph (b) ANRD vs Layers
of oversmoothing on the entire network’s representation. Ad-
Fig. 1: Early Oversmoothing: Average node representation
ditionally, only a limited number of studies have investigated
distance(ANRD) of different k-truss subgraphs concerning
the influence of specific regions causing oversmoothing [6],
GNN layers (2-10)
[9]inGNNs.ThesestudiesshowthatthesmoothnessinGNN
4202
luJ
61
]GL.sc[
1v82911.7042:viXrapassing, resulting in oversmoothing via losing unique node Therestofthispaperisorganizedasfollows.SectionIIdis-
features [24], [33]. Consequently, dense regions’ identical cusses the related work that informs our research. Section III
node information affects the graph’s representation learning. introduces the model’s preliminaries, whereas Section IV
We extend the preliminary investigation on the toy graph describes the model itself. Next, Section V represents our
giveninFigure1(a).WefirstapplytheSAGPoolmodel.After model’s experiment results and an analysis of its performance
each pooling layer’s operation, we measure the coarsened on different datasets. Finally, we conclude the paper with a
graph’s nodes’ embedding space matrix (ESM) with l norm, discussion of future research directions.
2
then present the results for the first 2 pooling layers in
II. RELATEDWORKS
Figure 2(a) and 2(b). We observe that embedding distances
are getting smaller for nodes within the dense regions, sig- Graph Classification. Early GNN models leverage simple
nificantly reducing the final graph’s representation variability. readout functions to embed the entire graph. GIN [26] intro-
These node and graph representation characteristics through duces their lack of expressivity and employs deep multiset
GNN models inspire us to work at different levels of dense sums to represent graphs. In recent years, graph pooling
regions in the network to mitigate oversmoothing. methods have acquired excellent traction for graph represen-
tation. They consider essential nodes’ features instead of all
Our Work. To tackle the challenge, we develop a truss-
nodes. Flat pooling methods utilize the nodes’ representation
based graph sparsification (TGS) model. Earlier sparsification
without considering their hierarchical structure. Among them,
models apply supervised techniques [31] and randomly drop
GMT[3]proposesamultisettransformerforcapturingnodes’
edges [9], [17] which may result in losing meaningful con-
hierarchicalinteractions.Anotherapproach,SOPool[25],cap-
nections. However, our model selects the initial extraneous
italizes vertices second-order statistics for pooling graphs.
information propagating edges by utilizing edge trussness. It
There are two main types of hierarchical pooling methods:
operates on the candidate edge’s nodes’ neighborhood and
clustering-based and selection-based. Clustering-based meth-
measurestheconnectivitystrengthofnodes.Thisconnectivity
ods assign nodes to different clusters: computing a cluster
strength assists in understanding the edge’s local and global
assignment matrix [27], utilizing modularity [21] or spectral
impact on GNN’s propagation steps. Based on their specific
clustering [4] from the node’s features and adjacency. On the
node strength limits, we decide which edges to prune and
other hand, selection-based models compute nodes’ impor-
which to keep. Removing selected redundant edges from
tance scores up to different hop neighbors and select essential
dense regions reduces noisy message passing. That decreases
nodes from them. Two notable methods are SAGPool [12],
oversmoothing and facilitates the GNN’s consistency in per-
which employs a self-attention mechanism to compute the
formance during training and inference time. As we see in
node importance, and HGP-SL [30], which uses a sparse-
Figure 2(c) and 2(d), the sparsified graph exhibits greater
max function to pool graphs. KPLEXPOOL [2] hierarchically
diversity in node distances than the original, enhancing better
leverages k-plex and graph covers to capture essential graph
representation learning. In a nutshell, the contributions of our
structures and facilitates the diffusion between contexts of
model are listed as follows.
distance nodes. Some approaches combine both hierarchical
• Weobservethepriorstationaryrepresentationlearningof pooling types to represent graphs. One model, ASAP [16],
nodesemerginginthenetwork’shigh-trussregion,which adapted a new self-attention mechanism for node sectioning,
denotes a new perspective on explaining oversmoothing. a convolution variant for cluster assignment. Another model,
We develop a unique truss-based graph sparsification AdamGNN [32], employs multi-grained semantics for adapt-
technique to resolve this issue. ing selection and clustering for pooling graphs.
• In the edge pruning step, we measure the two nodes’ Oversmoothing: While increasing number of layers for a
average neighborhood trussness to detect the regional regular neural network may results better learning, it may
interconnectedness strength of the nodes. During the cause an oversmoothing problem in which nodes get sim-
message passing steps in GNN, as we trim down the ilar representations during graph learning because of the
dense connections within subgraphs, nodes in less dense information propagation in GNN. To tackle this, researchers
areasatvaryinghopdistancesacquirediversehierarchical propose different approaches: DROPEDGE [17] randomly
neighbor information. Conversely, nodes in highly dense prunes edges like a data augmentor that reduces the mes-
regions receive reduced redundant information. This pro- sage passing speed, DEGNN [14], [23] applies connectivity
videssmoothnesstothenoderepresentationaswellasto aware decompositions that balance information propagation
the graph representation. flow and overfitting issue, MADGap [5] measures the average
• We provide a simple but effective model by pruning distance ratio between intra-class and inter-class nodes which
noisy edges from graphs based on their nodes’ average lower value ensures over-smoothing. However, these methods
neighborhood trussness. The effectiveness of our model overlook networks’ regional impact on oversmoothing. The
hasbeenevaluatedincomparisonwithstandardGNNand k-truss [10] algorithm primarily applies to community-based
graphpoolingmodels.Extensiveexperimentsondifferent network operations to identify and extract various dense re-
real-world graphs show that our approach outperforms gions. It has been employed in different domains, such as
most of those baselines in graph classification tasks. high-performance computing [8] and graph compression [1].(a) Org. pool 1 (b) Org. pool 2 (c) Spars. pool 1 (d) Spars. pool 2
Fig. 2: ESM of the toy graph and sparsified graph for δ =7
Our TGS model functions as a technique equipped with layers, one or more pooling function(s) operate on them.
foundation graph pooling methods. It leverages the k-truss These pooling layers are pivotal in enhancing the network’s
algorithm and edges’ minimum node strength to provide ability to generalize from graph data through effective graph
networks’ structural interconnectedness. Pruning highly dense summarization. In general, pooling operations are categorized
connections helps to restrict excessive message passing paths into two types: Flat pooling and Hierarchical pooling.
to reduce oversmoothing in GNN models. We empirically Flat pooling [13] is a straightforward graph readout opera-
justified it by experimenting with different graph topologies tion.Itsimplifiestheencodingbyprovidingauniformmethod
in section V. to represent graphs of different sizes in a fixed size.
III. PRELIMINARIES h =READOUT({h(k)|v ∈V}) (3)
G v
This section discusses the fundamental concepts for GNN
Hierarchical pooling [13] iteratively coarsens the graph
and pooling, and also formulate the oversmoothing problem
and encodes comprehensive information in each iteration,
including the essential components for our solution to this
reducing the nodes and edges of the graph and preserving
problem.Webeginwithdiscussinggraphneuralnetworksand
the encoding. It enables the graph’s representations to achieve
graph pooling techniques. Then, define the issue, including
short and long-sighted structural details. In contrast to Flat
the task. Finally, we delve into the foundation concept of
Pooling, it gives deeper insights into inherent graph patterns
our model (k-truss), which plays a crucial role in solving the
and relationships.
problem.
Between the two types of hierarchical graph pooling meth-
A. GNN and Graph Pooling ods,theselection-based methodsemphasizeprioritizingnodes
Graph Neural Network (GNN) [20] is an information byassigningthemascore,aimingtoretainthemostsignificant
nodes in the graph. They employ a particular attention
processing framework that defines deep neural networks on
functionforeachnodetocomputethenodeimportance.Based
graph data. Unlike traditional neural network architectures
onthecalculatedscores,topknodesareselectedtoconstructa
that excel in processing Euclidean data, GNNs are experts
pooledgraph.Thefollowingequationgivesageneraloverview
in handling non-Euclidean graph structure data. The principal
of the top k selection graph pooling method:
purpose of GNN is to encode node, subgraph, and graph into
low-dimension space that relies upon the graph’s structure. In S =score(G,X); idx=topK(S,[α×N])
GNN, for each layer, K in the range 1,2,...k, the computa- (4)
tional node aggregates (1) messages m(k) from its K-hop A(l+1) =A idx,idx
N(v)
neighbors and updates (2) its representation h(k+1) with the where S ∈ RN×1 is the scores of nodes, α is the pooling
v
help of the AGGREGATE function. ratio, and N is the number of nodes. Conversely, clustering-
based pooling methods form supernodes by grouping original
m(k) =AGGREGATE(k)({h(k),∀u∈N(v)}) (1)
N(v) u graph nodes that summarize the original nodes’ features. A
h(k+1) =UPDATE(k)(h(k),m(k) ) (2) cluster assignment matrix S ∈ RN×K using graph structure
v v N(v)
and/or node features are learned by the models. Then, nodes
In the context of graph classification, GNNs must focus on are merged into super nodes by S ∈ RN×K to construct the
aggregating and summarizing information across the entire pooled graph at (l+1)th layer as follows
graph. Hence, the pooling methods come into play.
Graph Pooling. [13] Graph pooling performs a crucial A(l+1) =S(l)T A(l)S(l)
(5)
operation in encoding the entire graph into a compressed H(l+1) =S(l)T H(l)
representation. This process is vital for graph classification
tasks as it facilitates capturing the complex network structure where A ∈ RN×N is the adjacency matrix and H ∈ RN×d
into a meaningful form in low-dimensional vector space. is the feature matrix with d dimensional node feature and
During the nodes’ representation learning process at different N is the number of nodes. Note that, the AGGREGATE,UPDATE and READOUT operations are different oper- D. K-truss
ational functions, commonly including min, max, average,
Identifying and extracting cohesive subgraphs is a pivotal
and concat. task in the study of complex networks. The k-truss subgraph
extraction algorithm is instrumental as it isolates subgraphs
B. Oversmoothing
based on a specific connectivity criterion. The root of the
According to [29], continual neighborhood aggregation of criterion is the term support, which refers to the enumeration
nodes’featuresgivesanalmostsimilarrepresentationtonodes of triangles in which an edge participates. The support
foranincreasingnumberoflayersK.simply,withoutconsid- serves as the cornerstone to measure the cohesiveness of a
ering the non-linear activation and transformation functions, subgraph. The following two definitions explain the criterion
the features converge as - for extracting specific tightly interconnected subgraphs from
a complex network.
h∞ =Aˆ∞X, Aˆ = (d i+1)r(d j +1)1−r (6) Definition 1: Support: In graph G = (V,E) , the support
i,j 2m+n of an edge e=(u,v)∈E is denoted as sup (e) the number
G
of triangles where e involves, i.e sup (e) = |{ ∆ :
where, v and v are source and target nodes, d and d are G uvw
i j i j w ∈V }| .
their degrees respectively, Aˆ is the final smoothed adjacency
Definition 2: k-truss subgraph: A subgraph S = (V ,E )
matrix and r ∈ [0,1] is the convolution coefficient. The S s
where, S ⊆ G, V ⊆ V and E ⊆ E is a k-truss subgraph
equation (6) shows for an infinite number of propagations, s S
where every edge e ∈ E has at least k−2 support, where
the final features are blended and only rely upon the degrees S
k ≥2.
of target and source nodes. Furthermore, through spectral and
Notably, the concept of k-truss is inherently dependent on
empiricalanalysis [6]shows:nodeswithhigher-dreearemore
the count of triangles within the graph, establishing that any
likely to suffer from oversmoothing.
graph can be considered a 2-truss subgraph. The hierarchical
structure of k-truss subgraphs implies that a 3-truss subgraph
hk(j)=(cid:112) d
j
+1((cid:88)n (cid:112) 2md j ++ n1 x i± (cid:80)n i=1 (cid:112)x di(1 +− 1λ 22 G)k ) i as sa Gs 3ub ⊆se Gto 2f .t Sh ie m2 il- atr ru lys ,s Gsu 4b ⊆gra Gph 3·( ·th ·e Go kri ⊆gin Ga kl −g 1ra .ph)denoted
i=1 j Definition3:EdgeTrussness:ForagivengraphG,fork >
(7)
2, an edge, E(u,v), can exist in multiple k-truss subgraphs.
In the equation (7), λ is the spectral gap, m is the number
G The trussness of the edge, denoted as T (u,v), is quantified
of edges, and n is the number of nodes. It represents the r
fromthehighestk valueforwhichtheedgeisincludedinthat
features convergence relied upon the spectral gap λ and
summation
(cid:80)n
of feature entries. When the
numG
ber of
subgraph. That is, T r(u,v)=k and (u,v)∈/ G k+1.
i=1
layers K goes to infinity, the second term disappears (after IV. TRUSSBASEDGRAPHSPARSIFICATION
±). Hence, all vertices’ features converge to steady-state for
In this section, we explain the proposed truss-based graph
oversmoothing, which mainly depends on the nodes’ degrees. sparsification model (TGS) to overcome the oversmoothing
problemforgraphclassification.Ingraphanalytics,classifying
C. Problem Formulation
graphsischallengingduetotheirlargesizeandcomplexstruc-
Thisresearchaimstoalleviateoversmoothingbyeffectively ture.Graphsparsification–atechniquethatreducesthenumber
simplifying graphs to balance global and local connections, ofgraphconnectionsbypreservingcrucialgraphstructures,is
resulting in better graph classification results. Formally, A an emerging technique to address these challenges. We aim to
Graph is denoted as G = (V,E,X), where V is the set getaneffectivesimplifiedgraphthatkeepsessentialshort-and
of nodes and E is the set of edges. Symbol X ∈ RN×d long-distance graph connections through graph sparsification,
represents the graph’s feature matrix of dimension d, where which produces the optimal graph classification result. The
N = |V| is the number of nodes in G and x ∈ Rd,x ∈ overall architecture of the proposed model is presented in
v v
X and v ∈ V is a d dimensional feature of a particular Figure 3. The model consists of 2 parts: truss-based graph
node in the graph. The neighborhood of a node u is denoted sparsification and graph learning on the sparsified graph.
as N(u), and its degree is represented as d(u) = |N(u)|. We observe four phases to develop the truss-based graph
For a dataset D = (G,Y) consisting of a set of graphs sparsification framework.
G = {G ,G ···G }, label pair Y = {Y ,Y ,....Y }, Phase 1:Computeedgetrussness:Atfirst,weapplythek-truss
1 2 N 1 2 N
our truss-based sparsification algorithm introduces a set of decomposition algorithm on an unweighted graph to compute
sparsifiedgraphsasG ={G ,G ....G }.Thealgorithm its edges’ trussness as weight. Next, we split all edges into
S S1 S2 SN
is designed to remove redundant graph connections and retain groups based on their truss values: high-truss edges and low-
thegraph’sessentialstructuralinformation.Subsequently,This truss values for a given threshold η.
sparsified graphs set is analyzed using GNN models to learn Phase 2: Measure node strenght: TGS focuses on high-truss
a function f :G →Y leveraging the reduced complexity of edges for sparsification. As high-truss edges have higher
S
the graphs. The principal objective is to enhance the accuracy degrees, they massively contribute to the oversmoothing phe-
(Acc) of GNN models in graph classification tasks. nomenon (Section III-B). Thus, strategic pruning of thosePhase 1: Edge Truss Phase 2: Node Strength Phase 3: Prune and Update Phase 4: Learning
Label
MLP
GNN
Fig. 3: Architecture of the TGS (η =3,δ =2.5).
edges helps to reduce oversmoothing. However, at the same Algorithm 1: Computing Edge Trussness
time, important structural connections need to be maintained. Input: A Graph G=(V,E)
To do so, we measure the minimum node strength of its two Output: A graph G , where edge trussness T (e) as
T r
end nodes for each candidate high-truss edge, indicating the weight for each e∈E
edge’s surroundings’ density status. 1 Compute sup G(e), ∀e∈E
Phase 3: Prune and update: When that minimum value ex- 2 Sort(e∈E, item=sup G(e)) // in non-decreasing
ceeds the standard density assuring threshold, we prune the order
edgefromthegraphandupdatealltheedge’strussnessvalues. 3 k ←2 ; G T ←G.copy()
Due to the cascading effect of the network, pruning affects 4 while ∃e∈E,sup G(e)≤(k−2) do
other edges’ trussness. Therefore, edge trussness needs to be 5 e∗(u,v)←argmin e∈E sup G(e) // assume
updated after each pruning operation. The process continues w.o.l.g, d(u)≤d(v)
the pruning step until all high-truss edges are examined. 6 foreach w ∈N(u)∩N(v) and e∗ =(u,v)∈E
Phase 4: Learning: At the end of the sparsification, we first do
feed the processed graph to GNN models for graph learning. 7 sup G(u,w)←sup G(u,w)−1
Finally, we experiment the entire graph’s representation with 8 sup G(v,w)←sup G(v,w)−1
a multi-layer perceptron (MLP) network. 9 Reorder(u,w) and (v,w)
Note that our model follows some strategies during the 10 end
graph sparsification steps: (a) sorts the high-truss edges in 11 G T[u][v][W]←T r(e∗)←k;
descending order to prune more dense regions’ edges earlier, 12 remove e∗ from E
and (b) examines each edge only once. Removing an edge 13 end
from the graph might affect other edges; then, in further 14 if ∃e∈E then
exploration,oneedgemightsatisfythepruningcondition.The 15 k ←k+1
phenomenon negligibly happens as TGS starts to prune from 16 goto the while-loop (line 4)
more dense edges. Hence, the technique avoids recursion. 17 end
A. Dense Region Identification 18 return G T
To learn the structure of the graph, GNN applies message
passing between nodes through edges. Through repeated mes-
sagepassing,nodesinthedenseregionsgetsimilarneighbors’ discover all k-trusses from G. At first, it takes an unweighted
feature information, which causes oversmoothing. As a result, graph as input and computes the supports of all edges. Then
thefeaturesofthoseregions’nodesbecomeindistinguishable. initialize the value of k as 2 and select the edge e∗ with the
Many different density measures exist, including k-truss, k- lowest support (line 5). Next, the value k is assigned as edge
core, and k-edge. This paper uses k-truss, defined based on weight W, and the edge is removed (line 12). Removing an
triangle connectivity, to identify the dense regions. edgedecreasesotheredgessupports.Hence,wereorderedges
Ourapproachemploysatruss-decomposition[1]algorithm, according to their new support values (line 9). The process
as detailed in Algorithm 1, to compute edge trussness and continues until the edges that have support no greater than(k − 2) are removed from the graph. Next, the algorithm Algorithm 2: Truss-based Graph Sparsification (TGS)
checks whether any edge exists to access or not. If one or Input: A Graph (G), threshold δ, cutoff η
more exist(s), it increments k by one and goes to the line 4 Output: A Sparsified Graph G ⊂G
S
againtomeasuretheirtrusseness(line 14-17).Edgetrussness 19 G T =Computing Edge Trussness(G)
facilitatesunderstandingthehighestdenseregionwithinwhich 20 E H ={∀(u,v)∈E,T r(u,v)≥η}
theedgeexists.Aftercalculatingtheedgetrussness,toidentify 21 E H ←Sort(E H,reverse=True)
highly dense areas, TGS separates the edges in G T into 22 n←length(E H)
two sets: High-Truss Edges and Low-Truss Edges. Following 23 G S ←G T
condition (1), it compares all edges’ trussness with the given 24 for r ←1 to n do
threshold value, η, and determines the High Truss Edges E H. 25 foreach E(u,v)∈E H do
For example, in Figure 3, given η =3, the blue (T r(E)=4) 26 Compute T N¯ (E) from Equations (8) and (9)
and golden (T r(E) = 3) colored edges are high-truss edges. 27 if T N¯ (E) ≥δ then
Pruning {E ∈E H} reduces the load of high-degree nodes in 28 G S ←G T\E(u,v) // Edge Cut
dense regions, which assists in mitigating oversmoothing in 29 E H ←E H\E(u,v) // Reduce High Truss
GNN.
Edge List
Condition 1: High Truss Edges E : In any graph for a
H 30 G S ←UpdateTr(G S)
specificvariableη,ifanedge’strussnessvalueisgreaterthan
31 end
or equal to η then the edge is considered as a high truss edge
32 end
and their set is denoted as E .
H 33 end
B. Pruning Redundant Edges 34 return G S
Ascertaining the high-truss edges is crucial for under-
standing the density level in different parts of the graph.
However, directly pruning these edges may break up essential or exceeds the value of δ, we assume the edge is part of a
connectivity between nodes. For example, low-degree nodes highly dense region, and there is a high chance of excessive
could be connected with a dense region node, and pruning an messagespassingbetweenthatregion’snodes.Thatmaycause
incidenthigh-trussedgemaynotprovideadequateinformation them to blend their representations, leading to oversmoothing
to that low-degree node. To balance the connectivity between during graph learning through GNN (section III-B). From the
nodes, we determine the nodes’ strength of edge high-truss condition, the model understands which edges contribute to
edges and then proceed to the next step. To measure nodes’ undesirabledensitylevelsthatfosteroversmoothinginGNNs.
(n ∈ E, E ∈ E ) strength, TGS calculates the average
H
trussness T ¯ ,n∈V. This score ensures the density depth T ¯ ←minimum(T ¯ ,T ¯ ) (9)
N(n) N(E) N(u) N(v)
of a node and implies its important connectivity.
Definition 4: The strength of a node is measured as the Condition 2: An Edge e = (u,v) is eligible for pruning
summationofallofitsincidentedgeweights.However,Inthis when the minimum average neighborhood edge weight be-
research, node strength is applied as the average of nodes’ tween u and v equals or exceeds the threshold δ.
incident edges’ trussness. T ¯ ≥δ (10)
N(E)
T ¯ ← 1 (cid:88) T (n,u)
N(n) |N(n)| r (8) For example, at the lower-left in phase 3 (in Figure 3), the
u∈N(n) edge (v , v ), where the degrees of v and v are 5 and
2 10 2 10
For a candidate edge E = (u,v), after measuring the 2, respectively. The node strengths, T ¯ is {(3 × 3) +
node strength of u and v (8) , their minimum value (9) has (2×2)}/5 = 2.6, and T ¯ is {(2N +(v 22 )) }/2 = 2. Given
been taken. Notably, a node may be included in different k- δ = 2.5, and the
minimN u( mv10)
node strength, T ¯ =
truss subgraphs. Hence, its neighborhood’s trussness provides T ¯ = min (2.6, 2) = 2. Hence, the pruninN g(v c2 o,v n1 d0) ition
N(E)
more connectivity information. The minimum node strength is unsatisfactory, and the edge will stand in the graph. If
of an edge’s two endpoints signifies the least density of its TGS pruned the edge, the neighborhood of v would be
10
surroundings. As we aim to reduce the density of highly sparser than before and miss its crucial global information.
connected regions to combat oversmoothing, TGS follows a On the other hand, at the upper-right in phase 3, in context of
technique to decide to prune edges. For this purpose, the E =(v , v ), T ¯ =3 and T ¯ =3. Hence, comparing
1 3 N(v1) N(v3)
minimum node strength of the edge E is compared to a to the value of δ they are already in the dense region and
threshold δ. In condition (2), This comparison ensures the T ¯ = 3 ≥ 2.5. In this case, the pruning will help
N(v1,v3)
edge’s presence in a prunable dense region. The condition to prevent blended node representation in GNN, especially
indicates that if any end of the candidate edge is sparse between highly interconnected subgraphs. According to our
T ¯ < δ, TGS avoids cutting it because that connection model, it considers the nodes will still stay in enough dense
N(E)
serves as an essential message-passing medium in the GNNs regions to receive meaningful local and global neighborhood
aggregation step. In contrast, when the minimum score equals information after pruning.TABLE I: Datasets’ Statistics
B. Experimental Settings
Datasets #Graphs Avg#|V| Avg#|E| #Classes To compare fairly, we executed the existing standard im-
PROTEINS 1113 39.06 72.82 2
plementations of the baselines and incorporated them with
NCI1 4110 29.87 32.30 2
our model. For evaluation, we split the datasets into 80%
NCI109 4127 29.68 32.13 2
PTC 344 25.56 25.96 2 for training, 10% for validation, and 10% testing. Mostly,
DD 1178 284.32 715.66 2 we stopped learning early for 50 consecutive same validation
IMDB-B 1000 19.77 96.53 2 results in training. We measured the performance using the
IMDB-M 1500 13.00 65.94 3
accuracymetricbyruiningeachmodel10timesfor10random
REDDIT-B 2000 429.63 497.75 2
seedsandreportedtheirmean.Thebatchsizewas128formost
ofthemodels.Theeffectivenessofourpruningmethodmostly
The Algorithm 2 represents the TGS model. Lines (19- depends on two crucial parameters: the cutoff parameter η
and the edge pruning threshold δ. For all experiments, we
21) identify the dense regions and ensure high-truss edges of
set η = 3, which means any edge with a trussness score
the network while lines (24-33) demonstrate the pruning of
below 3 cannot be pruned from the graph. On the other
noisy high-truss edges in the network. Algorithm UpdateTr
hand, we experimented with various δ values across the
in line 30 (similar to section 4.2, [10]), updates all edges’
datasets. Specifically, we used δ values of {3,4,5,6,7} for
trussness after each pruning step.
IMDB-BINARY,IMDB-MULTI,and{3,3.5,4}forREDDIT-
BINARY datasets. For PROTEINS and DD datasets δ was set
C. Algorithm Complexity
as{3,3.25,3.5,3.75, and4}andforNCI1,NCI109,weused
(cid:112)
ThecomplexityofmeasuringedgetrussnessisO(E (E)) δ values of { 2.5, and 3} while for PTC only 2.5.
and the updateTr algorithms complexity is O(E). As we
C. Result Analysis
explore all high truss edges in the algorithm, the exploration
TableIIreportstheexperimentresults,providingacompara-
complexity is O(E) in the worst case. Hence, the Algo-
rithms complexity is O(E(O(E))+O(E(cid:112) (E)) =O(E2)+ tiveanalysisbetweenourestablishedmodelandoriginalbase-
O(E(cid:112) (E)) = O(E2) in the worst case. Although it seems lines across various datasets. TGS integrated with backbone
graph pooling models consistently outperforms the baselines,
highly complex the real-world datasets are not hardly dense.
and demonstrates its robustness in graph classification tasks.
In addition, due to updating the edges trussness score many
On selection-based models, with the incorporation of the
high-truss edges are removed before examination.
SAGPool model, TGS achieves a 1.5-5.5% gain(G) (11) over
the original models. Notably, on DD and IMDB-BINARY
V. EXPERIMENTDESIGNANDANALYSIS
datasets, the gains are 3.34% and 5.17%, respectively. In the
This section validates our technique on different real-world
experimentwiththeHGP-SLmodel,TGSattainsasustainable
datasets by applying standard graph pooling models. First, we improvement of nearly 4.5% on the IMDB-BINARY dataset
provideanoverviewofthedatasets.Then,webrieflydescribe and on the PTC dataset, which is over 2.5%. Adapting
the parameters of various methods. Finally, we compare the TGS along with the flat pooling model GMT acquires a
performance of our TGS algorithm’s enhancement with the significant gain (nearly 7%) over the NCI109 dataset and
original baselines in the graph classification tasks including maintains consistent performance on other datasets.
analysis of parameters, deeper networks, and ablation study. In experiments with cluster-based modes, TGS equipped to
Diffpool model achieves a magnificent accuracy gain on the
PTCdataset,whichisnearly19%.Italsodemonstratesstrong
A. Datasets and Baselines
performance with the DMonPool model over all datasets.
We experiment with our model on eight different TU Notably, it achieves the highest accuracy on the REDDIT-
Dortmund [15] datasets: Five of them are biomedical do- BINARY dataset, which is 85.75%
main: PROTEINS, NCI1, NCI109, PTC, and DD and
TGS−Original
three of them from social network domain: IMDB-BINARY, Gain(G)= ×100% (11)
Original
IMDB-MULTI, and REDDIT-BINARY. We extend the
TGS algorithm with seven state-of-the-art backbone graph Extended Experiment: In addition to the pooling method,
pooling models. Among them, three are node clustering- we incorporate the TGS model with the fundamental GNN
based pooling methods: DiffPool [27], DMonPool [21] and models: two versions of graph isomorphic networks (GIN-
MinCutPool [4]. Two models, SAGPool [12] and HGP- 0 and GIN-ϵ) and the simple graph convolution network
SL [30], utilize a node selection approach for pooling the for graph classification. During the experiment with GIN
graphs. Of the remaining two, one learns graph representation networks, we follow 10-fold cross-validation to evaluate the
through flat-pooling (GMT [3]), and another one utilizes an validity of our model. On the other hand, we assess the
adaptive pooling approach by applying both node selection GCN as other pooling models (section V-B). In most con-
and clustering for the pooling procedure: AdamGNN [32]. texts (in Table II), our technique outperforms these models
We report the statistics of the datasets in the Table I. on every dataset. Especially on the PTC and IMDB-BINARYTABLE II: Result Table (in %). Results that achieve at least 0.5% gain over the counterpart model we mark in bold. We show
a model-by-model comparison.
BiomedicalDataset SocialNetworkDataset
Backbones PROTEINS NCI1 NCI109 PTC DD IMDB-B IMDB-M REDDIT-B
Original 72.68 70.10 67.37 57.14 77.31 71.60 44.87 77.55
SAGPool
TGS 73.84 70.72 67.78 58.00 80.67 75.30 44.67 79.05
Original 70.63 60.29 48.86 50.86 65.88 71.10 47.40 70.95
GMT
TGS 71.25 61.56 52.32 51.14 69.50 74.50 47.93 71.25
Original 71.10 66.96 67.60 46.29 75.31 63.90 44.80 80.92
DiffPool
TGS 69.82 68.30 67.20 55.14 78.75 64.80 42.67 82.46
Original 75.45 74.20 72.61 53.71 79.32 74.00 49.53 84.95
DMon
TGS 75.80 74.74 73.77 56.00 80.42 74.90 49.60 85.85
Original 73.75 72.77 73.28 56.28 76.63 71.40 51.06 76.85
MinCut
TGS 73.84 73.58 73.04 58.57 77.31 72.80 51.20 77.05
Original 78.12 47.36 65.16 60.00 70.63 72.48 49.53 OOM
AdamGNN
TGS 81.77 47.36 67.81 62.86 74.25 77.60 50.57 OOM
Original 74.64 73.33 72.87 56.00 72.35 72.90 49.47 OOM
HGP-SL
TGS 74.28 73.38 74.22 57.43 73.19 76.10 49.80 OOM
Original 73.42 81.70 74.99 68.86 74.58 73.00 47.60 73.60
GIN-0
TGS 73.84 82.50 75.08 69.43 75.93 78.10 52.53 74.35
Original 73.12 81.85 75.93 68.28 76.44 73.80 49.33 73.55
GIN-e
TGS 73.39 82.50 76.97 68.29 74.75 73.90 53.40 74.55
Original 68.29 71.41 69.61 52.00 64.87 75.20 50.00 82.30
GCN
TGS 70.45 72.24 69.69 52.57 73.10 75.50 50.60 84.30
datasets, TGS(GIN-0) achieves the highest accuracy scores SAGPool, MinCutPool, DMonPool, and HGP-SL, showing
of 69.43% and 78.10%, respectively. Additionally, TGS with near-consistent performance. However, when increasing the δ
the backbone GCN model, attains the overall second-highest value from 3 to 3.25, TGS(AdamGNN)’s accuracy decreases
accuracy on the REDDIT-BINARY data, with 84.30%. and shows an almost stable performance. Assembled with
GMT and Diffpool, the accuracy of TGS shows some vari-
D. Analysis in Deeper Network
ations from δ = (3−3.5) and then remains near the same
WeexaminetheimpactoftheTGSindeeperlayerswiththe score for other values.
backbonemodels.InthisanalysisalongwiththeSAGPool,we Regarding the change of cutoff variable η,
choosethreeGNNmodels:GCN,GIN-ϵ,andGIN-0.Besides, figures 5(b) and 5(d) show the performance changes on
we select two datasets from the biomedical domain (DD and the same datasets. Similar to δ, the number of pruned
PROTEINS)andonefromthesocialnetworkdomain(IMDB- edges in the graph decreases for increasing the value η,
BINARY). Figure 4 shows the best two ranked (Table IV and TGS(SAGPool)’s accuracy increases. In contrast, for the
and V in the section VII) TGS variants for the threshold same reason, the performance of our skill with backbone
δ compared to the original model performance. Columns 1 AdamGNN and HGP-SL degrades. Other models display
(TGS(GCN))and4(TGS(SAGPool))revealthatforincreasing minor fluctuations in accuracy with the change of η value.
the number of layers, in most cases the TGS outperforms the
F. Ablation Study
original models on all three datasets in multiple layers. Fig-
ure 4(j) and 4(k) illustrate the similar trends in the context of This section observes the strategical and functional signif-
TGS(GIN-ϵ)andTGS(GIN-0)ontheIMDB-BINARYdataset. icance of TGS with the backbone graph pooling models. We
However, both of these models show fluctuations in accuracy chose four datasets and six pooling methods. In Table III,
on the DD and PROTEINS datasets. One interesting fact is for each dataset at the first two rows, we change the edge’s
that the accuracy trend TGS(GIN-0) has dis-proportionally connectivity strength measuring equations (8), and (9). In the
increased in deeper networks on DD. A possible reason could firstrow,theequation(8)remainsthesamebutatequation(9)
be the dense nature of the networks in the dataset. insteadofminimumtheaverageoftwoendnodes’strength
has been taken. On the other hand, in the second row, node
E. Sensitivity Analysis
strength measuring equation (8) is modified whereas the other
In Figure 5, we demonstrate our model’s performance for equation remains unchanged. In the last two rows, we change
variations of hyperparameters’ values on the IMDB-BINARY thepruningprocedure,examiningtoprunetwo(prune2∗)and
and PROTEINS datasets. Notably, in most cases, the pruning three (prune 3∗) edges without updating the edge trussness.
rate decreases as much as the delta value increases. Fig- Due to the change of equations, the model’s performance on
ure 5(a) shows at δ = 3 value, our equipped TGS models thePROTEINSdatasetincreaseswithitsextensiontoMinCut-
perform well. We observe that for lower δ value, the accuracy PoolandDiffPoolmethods.However,ontheNCI1dataset,the
of TGS with AdamGNN increases, whereas degrades for performance of AdamGNN dramatically decreases (47.36%
TGS(SAGPool). On the other hand, in the PROTEINS dataset and61.22%).Regardingexamining2and3edgesforpruning,
(Figure 5(c)), for changing the δ value, TGS decorated with sometimesmorethanoneedgeisprunedwithoutupdatingthe(a) DD(GCN) (b) DD(GIN-0) (c) DD(GIN-ϵ) (d) DD(SAGPool)
(e) PROT..(GCN) (f) PROT..(GIN-0) (g) PROT..(GIN-ϵ) (h) PROT..(SAGPool)
(i) IMDB-B(GCN) (j) IMDB-B(GIN-0) (k) IMDB-B(GIN-ϵ) (l) IMDB-B(SAGPool)
Fig. 4: Models’ Performance in Deeper Networks
(a) IMDB-B δ (b) IMDB-B η (c) PROTEINS δ (d) PROTEINS η
Fig. 5: Change of parameters δ and η on IMDB-BINARY and PROTEINS.
otheredges’trussness.Hence,significantinformationprocess- model includes more variability to the input graph for alle-
ing connections could prune in the system. Compared to the viating oversmoothing. Comprehensive experiments on eight
result in Table II, our model’s performance severely degrades renowneddatasetsverifythatTGSisconsistentinperformance
on some datasets with the components change. Nonetheless, over popular graph pooling and readout-based GNN models.
the modified pruning strategy with MinCutPool and DiffPool We expect our research to show some interesting directions:
achieve better results over TGS are 74.37% and 70.27% Learning the edge pruning threshold during training, applying
respectively on the PROTEINS dataset. parallelization during pruning edges at different k-truss sub-
graphs, and joint learning to measure edge importance during
VI. CONCLUSION graph sparsification.
In this paper, we have proposed an effective k-truss-based REFERENCES
graph sparsification model to facilitate graph learning of the
[1] E. Akbas and P. Zhao. Truss-based community search: a truss-
graph neural networks (GNN). Through the sparsification of
equivalence based indexing approach. Proceedings of the VLDB En-
dense graph regions’ overflowed message passing edges, our dowment,10(11):1298–1309,2017.TABLE III: Ablation Study Table. Boldly marked entries perform better than TGS.
Changes SAGPool GMT DMonPool MnCutPool DiffPool AdamGNN
avg(avg,avg) 75.00 74.59 74.70 73.09 63.00 75.43
min(min,min) 74.00 48.06 72.70 73.13 65.10 79.77
IMDB-BINARY
Prune2* 70.99 48.69 51.80 48.50 61.10 53.73
Prune3* 68.70 47.90 54.89 49.59 62.79 59.80
avg(avg,avg) 44.67 47.40 48.86 50.59 41.46 50.04
min(min,min) 46.67 48.06 49.13 50.73 38.86 46.92
IMDB-MULTI
Prune2* 43.99 47.93 49.73 49.09 39.47 50.04
Prune3* 50.59 47.46 49.40 50.99 41.06 47.96
avg(avg,avg) 71.14 70.62 75.53 74.01 69.99 81.51
min(min,min) 69.64 67.41 73.83 74.10 71.01 74.10
PROTEINS
Prune2* 72.40 66.78 73.84 74.37 70.27 69.79
Prune3* 72.58 66.60 59.46 62.05 69.54 71.35
avg(avg,avg) 70.07 61.55 68.8 69.97 68.10 47.36
min(min,min) 73.47 61.19 69.05 73.52 68.17 61.22
NCI1
Prune2* 67.73 60.97 69.63 70.34 68.17 58.16
Prune3* 68.71 59.17 68.83 70.07 67.15 60.02
[2] D. Bacciu, A. Conte, R. Grossi, F. Landolfi, and A. Marino. K-plex [18] T.K.Rusch,B.Chamberlain,J.Rowbottom,S.Mishra,andM.Bron-
coverpoolingforgraphneuralnetworks. DataMiningandKnowledge stein. Graph-coupled oscillator networks. In International Conference
Discovery,35(5):2200–2220,2021. onMachineLearning,pages18888–18909.PMLR,2022.
[3] J.Baek,M.Kang,andS.J.Hwang.Accuratelearningofgraphrepresen- [19] K. M. Saifuddin, B. Bumgardner, F. Tanvir, and E. Akbas. Hygnn:
tations with graph multiset pooling. arXiv preprint arXiv:2102.11533, Drug-druginteractionpredictionviahypergraphneuralnetwork.In2023
2021. IEEE39thInternationalConferenceonDataEngineering(ICDE),pages
[4] F. M. Bianchi, D. Grattarola, and C. Alippi. Spectral clustering with 1503–1516.IEEE,2023.
graph neural networks for graph pooling. In International conference [20] F.Scarselli,M.Gori,A.C.Tsoi,M.Hagenbuchner,andG.Monfardini.
onmachinelearning,pages874–883.PMLR,2020. Thegraphneuralnetworkmodel.IEEEtransactionsonneuralnetworks,
[5] D. Chen, Y. Lin, W. Li, P. Li, J. Zhou, and X. Sun. Measuring and 20(1):61–80,2008.
relieving the over-smoothing problem for graph neural networks from [21] A.Tsitsulin,J.Palowitch,B.Perozzi,andE.Mu¨ller. Graphclustering
thetopologicalview.InProceedingsoftheAAAIconferenceonartificial with graph neural networks. Journal of Machine Learning Research,
intelligence,volume34,pages3438–3445,2020. 24(127):1–21,2023.
[6] M. Chen, Z. Wei, Z. Huang, B. Ding, and Y. Li. Simple and deep [22] R.VanBelle,C.VanDamme,H.Tytgat,andJ.DeWeerdt. Inductive
graphconvolutionalnetworks. InInternationalconferenceonmachine graph representation learning for fraud detection. Expert Systems with
learning,pages1725–1735.PMLR,2020. Applications,193:116463,2022.
[7] Y.Deng.Recommendersystemsbasedongraphembeddingtechniques: [23] Y. Wang and T. Derr. Tree decomposed graph neural network. In
Areview. IEEEAccess,10:51587–51633,2022. Proceedingsofthe30thACMInternationalConferenceonInformation
[8] S. Diab, M. G. Olabi, and I. El Hajj. Ktrussexplorer: Exploring the &KnowledgeManagement,pages2040–2049,2021.
designspaceofk-trussdecompositionoptimizationsongpus. In2020
[24] Y. Wang, H. Wang, H. Jin, X. Huang, and X. Wang. Exploring graph
IEEEHighPerformanceExtremeComputingConference(HPEC),pages
capsualnetworkforgraphclassification.InformationSciences,581:932–
1–8.IEEE,2020.
950,2021.
[9] R. Huang and P. Li. Hub-hub connections matter: Improving edge
[25] Z. Wang and S. Ji. Second-order pooling for graph neural networks.
dropouttorelieveover-smoothingingraphneuralnetworks.Knowledge-
IEEETransactionsonPatternAnalysisandMachineIntelligence,2020.
BasedSystems,270:110556,2023.
[26] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph
[10] X.Huang,H.Cheng,L.Qin,W.Tian,andJ.X.Yu. Queryingk-truss
neuralnetworks? arXivpreprintarXiv:1810.00826,2018.
community in large and dynamic graphs. In Proceedings of the 2014
ACMSIGMODinternationalconferenceonManagementofdata,pages [27] Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec.
1311–1322,2014. Hierarchical graph representation learning with differentiable pooling.
Advancesinneuralinformationprocessingsystems,31,2018.
[11] T. N. Kipf and M. Welling. Semi-supervised classification with graph
convolutionalnetworks. arXivpreprintarXiv:1609.02907,2016. [28] S. Zhang, Y. Guo, P. Zhao, C. Zheng, and X. Chen. A graph-based
[12] J.Lee,I.Lee,andJ.Kang.Self-attentiongraphpooling.InInternational temporal attention framework for multi-sensor traffic flow forecasting.
conferenceonmachinelearning,pages3734–3743.PMLR,2019. IEEE Transactions on Intelligent Transportation Systems, 23(7):7743–
7758,2021.
[13] C. Liu, Y. Zhan, J. Wu, C. Li, B. Du, W. Hu, T. Liu, and D. Tao.
Graph pooling for graph neural networks: Progress, challenges, and [29] W.Zhang,M.Yang,Z.Sheng,Y.Li,W.Ouyang,Y.Tao,Z.Yang,and
opportunities. arXivpreprintarXiv:2204.07321,2022. B. Cui. Node dependent local smoothing for scalable graph learning.
AdvancesinNeuralInformationProcessingSystems,34:20321–20332,
[14] X. Miao, N. M. Gu¨rel, W. Zhang, Z. Han, B. Li, W. Min, S. X. Rao,
2021.
H.Ren,Y.Shan,Y.Shao,etal.Degnn:Improvinggraphneuralnetworks
with graph decomposition. In Proceedings of the 27th ACM SIGKDD [30] Z. Zhang, J. Bu, M. Ester, J. Zhang, C. Yao, Z. Yu, and C. Wang.
ConferenceonKnowledgeDiscovery&DataMining,pages1223–1233, Hierarchical graph pooling with structure learning. arXiv preprint
2021. arXiv:1911.05954,2019.
[15] C. Morris, N. M. Kriege, F. Bause, K. Kersting, P. Mutzel, and [31] C. Zheng, B. Zong, W. Cheng, D. Song, J. Ni, W. Yu, H. Chen, and
M.Neumann.Tudataset:Acollectionofbenchmarkdatasetsforlearning W.Wang.Robustgraphrepresentationlearningvianeuralsparsification.
withgraphs. arXivpreprintarXiv:2007.08663,2020. InInternationalConferenceonMachineLearning,pages11458–11468.
[16] E.Ranjan,S.Sanyal,andP.Talukdar. Asap:Adaptivestructureaware PMLR,2020.
poolingforlearninghierarchicalgraphrepresentations. InProceedings [32] Z. Zhong, C.-T. Li, and J. Pang. Multi-grained semantics-aware
of the AAAI Conference on Artificial Intelligence, volume 34, pages graph neural networks. IEEE Transactions on Knowledge and Data
5470–5477,2020. Engineering,2022.
[17] Y. Rong, W. Huang, T. Xu, and J. Huang. Dropedge: Towards deep [33] X. Zuo, H. Yuan, B. Yang, H. Wang, and Y. Wang. Exploring graph
graph convolutional networks on node classification. arXiv preprint capsual network and graphormer for graph classification. Information
arXiv:1907.10903,2019. Sciences,640:119045,2023.TABLE V: DD, PROTEINS, NCI1 and NCI109 results
VII. SUPPLEMENT
DD
δ
A. Result Details
Original 3 3.25 3.5 3.75 4
SAGPool 77.31 79.83 78.15 80.67 80.67 79.00
ThissectionreportstheexperimentdetailsoftheTGSmodel GMT 65.88 69.5 67.39 68.49 64.62 63.78
pipelined with the backbone graph pooling and GNN models. DiffPool 75.31 78.13 78.75 78.13 77.81 73.75
DMon 79.32 80.42 79.49 79.57 79.66 80.08
In all the tables, the social networks and biomedical domains’
MinCut 76.63 77.31 77.22 76.97 76.55 76.72
datasets’ results are shown with the accuracy (%) metric. AdamGNN 64.63 71.65 74.25 71.12 67.87 71.00
We measure the average accuracy for each dataset and rank HGP-SL 72.35 71.68 72.44 71.43 72.86 73.19
GIN-ϵ 76.44 73.73 71.36 74.58 73.9 74.75
them for different variations of the edge pruning threshold δ
GIN-0 74.58 73.22 74.75 75.08 74.49 75.93
compared to the original backbone model’s scores. Table IV GCN 58.82 73.95 66.38 67.23 70.58 69.75
and V report all the results of different TGS-variants for Mean 72.13 74.94 74.02 74.33 73.90 73.80
separate δ values. Due to limited space, we represent the Rank 6 1 3 2 4 5
(REDDIT-BINARY & PTC) and (NCI1 & NCI109) datasets’ PROTEINS
results together in sub-tables. δ
Original 3 3.25 3.5 3.75 4
SAGPool 72.68 73.04 73.84 73.57 71.60 71.79
TABLE IV: IMDB-B, IMDB-M, REDDIT-B and PTC results GMT 70.63 71.07 69.46 71.25 70.36 70.27
DiffPool 71.10 67.71 68.07 69.82 69.45 68.72
IMDB-BINARY DMon 75.45 75.09 75.8 75.36 75.27 75.09
δ MinCut 73.75 72.86 73.57 73.84 73.75 73.48
Original 3 4 5 6 7 AdamGNN 78.12 81.77 79.43 79.17 78.91 78.39
SAGPool 72.80 73.10 73.10 73.20 72.10 75.30 HGP-SL 74.64 72.68 73.66 73.30 74.28 72.14
GMT 71.10 74.50 73.70 72.20 72.20 73.60 GIN-ϵ 73.12 71.88 71.88 71.07 73.39 70.57
DiffPool 63.90 65.20 63.60 62.30 64.80 62.80 GIN-0 73.42 73.42 73.84 73.12 72.78 72.50
DMon 74.00 73.40 73.50 74.90 73.50 73.20 GCN 65.18 66.07 63.39 66.07 64.28 63.39
MinCut 71.40 72.80 71.80 71.60 71.20 71.70
Mean 72.81 72.56 72.29 72.66 72.41 71.63
AdamGNN 72.48 77.60 74.83 73.09 75.26 71.70
Rank 1 3 5 2 4 6
HGP-SL 72.90 74.10 73.40 75.40 76.10 74.70
GIN-ϵ 73.80 73.80 73.90 73.00 73.40 72.50 NCI1 NCI109
GIN-0 73.00 78.10 69.50 71.60 73.20 73.30 δ δ
GCN 74.00 75.00 79.00 78.99 74.00 77.99 Org. 2.5 3 Org. 2.5 3
SAGPool 70.10 68.49 70.72 67.37 66.84 67.07
Mean 71.94 73.76 72.63 72.62 72.58 72.68
GMT 60.29 57.42 58.44 48.86 51.01 51.13
Rank 6 1 3 4 5 2
DiffPool 66.96 67.64 68.18 67.60 65.97 67.2
IMDB-MULTI DMon 74.20 74.23 74.01 72.61 73.50 73.38
δ MinCut 81.85 82.46 82.50 73.28 73.04 72.56
Original 3 4 5 6 7 AdamGNN 81.70 81.21 81.87 65.16 61.79 67.81
SAGPool 44.86 42.67 44.67 41.47 42.73 43.00 HGP-SL 71.53 73.48 70.07 72.87 72.58 72.56
GMT 47.40 47.60 45.53 46.27 47.93 47.13 GIN-ϵ 81.85 82.46 82.50 75.93 75.98 76.61
DiffPool 44.80 41.53 40.93 40.67 40.67 42.67 GIN-0 81.70 81.21 81.87 74.99 74.53 74.94
DMon 49.53 49.30 48.4 48.46 49.60 48.33 GCN 71.53 73.48 70.07 75.60 76.81 72.70
MinCut 51.06 50.06 51.20 50.73 50.20 50.93
Mean 74.17 74.21 74.02 69.43 69.21 69.60
AdamGNN 49.53 46.92 46.40 50.57 50.05 46.40
Rank 2 1 3 2 3 1
HGP-SL 49.46 49.80 48.80 48.87 47.33 49.00
GIN-ϵ 49.33 43.80 49.00 47.93 50.80 53.40
TABLE VI: Training Parameters in Models (Part 1)
GIN-0 47.60 41.47 46.13 48.73 51.00 52.53
GCN 41.33 44.67 40.67 42.67 46.67 40.67
model lr.rate #epochs #layers hid.size
Mean 47.49 45.78 46.17 46.64 47.70 47.41 SAGPool 0.005 1,000,000 3 128
Rank 2 6 5 4 1 3 GMT 0.0005 500 3 32
REDDIT-BINARY PTC DiffPool 0.001 500 3 64
δ δ DMon 0.001 500 3 32
Org. 3 3.5 4 Org. 2.5 MinCut 0.0005 15,000 3 32
SAGPool 77.55 76.15 79.05 77.80 57.14 59.14 AdamGNN 0.01 200 1or2 64
GMT 70.95 70.95 71.05 71.25 50.86 51.14 HGP-SL 0.001 1,000 3 128
DiffPool 80.72 81.45 82.32 82.03 46.29 55.14 GIN-(ϵ&0) 0.01 350 5 16
DMon 84.95 85.75 84.70 85.85 53.71 56.00 GCN 0.005 350 4 128
MinCut 76.85 77.05 76.19 76.45 56.28 58.57
AdamGNN OOM OOM OOM OOM 60.00 62.86
HGP-SL OOM OOM OOM OOM 56.00 57.43 B. Parameter Details
GIN-ϵ 73.55 73.75 74.00 74.55 68.28 66.57
GIN-0 73.60 73.65 74.10 73.30 68.86 65.43 Table VI and VII represent all the baseline models’ pa-
GCN 88.99 89.50 88.50 73.50 45.71 45.71 rameters’ details. A notable observation is that the number
Mean 78.40 78.53 78.74 76.84 56.31 57.80 of maximum epochs for the SAGPool and MinCutPool seems
Rank 3 2 1 4 2 1
endless. However, due to the patience variable, models take a
much smaller number of epochs during the experiment. TheTABLE VII: Training Parameters in Models (Part 2)
model weightdec. patience batchSize dropout
SAGPool 0.0001 100 128 50%
GMT 0.0001 50 128 50%
DiffPool Default 50 128 No
DMon Default 50 128 No
MinCut 0.0001 50 128 No
AdamGNN Default 50 64 50%
HGP-SL 0.001 50 512 No
GIN-(ϵ&0) 0.5 No 128 50%
GCN 0.0001 100 128 50%
AdamGNN model determines the number of layers for the
experiment by analyzing the graphs’ structural properties. All
the models are developed in the PyTorch library and utilize
the Adam optimizer where the default weight decay is set
to 0 in most cases. Except for AdamGNN (64) and HGP-SL
(512),thebatchsizeis128foralloftheothermodels.Allthe
baselinemodelsemploydifferentlearningratesforevaluation.
Six of the models employ the dropout parameter with a rate
of 50%, while the other four models abstain from utilizing it.