Context-Guided Diffusion for
Out-of-Distribution Molecular and Protein Design
LeoKlarner1 TimG.J.Rudner2 GarrettM.Morris1 CharlotteM.Deane1 YeeWhyeTeh1
Abstract
Generativemodelshavethepotentialtoaccelerate
keystepsinthediscoveryofnovelmolecularther-
apeuticsandmaterials. Diffusionmodelshavere-
centlyemergedasapowerfulapproach,excelling
atunconditionalsamplegenerationand,withdata-
driven guidance, conditional generation within
their training domain. Reliably sampling from
high-valueregionsbeyondthetrainingdata,how-
ever, remains an open challenge—with current
Figure1: Guidancemodelsthatgeneralizepoorlyunderdis-
methods predominantly focusing on modifying
tributionshiftscanbeamajorperformancebottleneckfor
thediffusionprocessitself. Inthispaper,wede-
property-guideddiffusionmodels. Weintroduceaguidance
velopcontext-guideddiffusion(CGD),asimple
modelregularizerthatimprovesgeneralizationunderdistri-
plug-and-play method that leverages unlabeled
butionshiftsandenablescontext-guideddiffusion(CGD).
dataandsmoothnessconstraintstoimprovethe
WeshowthatCGDleadstoconditionalsamplingprocesses
out-of-distributiongeneralizationofguideddiffu-
thatconsistentlygeneratenovel,high-valuemolecules(red).
sionmodels. Wedemonstratethatthisapproach
leadstosubstantialperformancegainsacrossvar-
ioussettings,includingcontinuous,discrete,and Deepgenerativemodelshavethepotentialtoacceleratethis
graph-structureddiffusionprocesseswithappli- processbycapturingandabstractingkeystructuralproper-
cationsacrossdrugdiscovery,materialsscience, tiesoftheirinputdomain(Sanchez-Lengeling&Aspuru-
andproteindesign. Guzik, 2018; Bilodeau et al., 2022). In recent years, de-
noisingdiffusionmodels(Sohl-Dicksteinetal.,2015;Ho
1.Introduction et al., 2020; Song et al., 2020; Karras et al., 2022) have
emergedasthemethodofchoice,displayingimpressiveper-
Thecentralgoalofmoleculardiscoveryistoidentifynovel formanceinconditionalandunconditionalimagegeneration
compounds with desirable functional properties. The im- tasks(Dhariwal&Nichol,2021;Sahariaetal.,2022;Rom-
mensesizeoftheunderlyingsearchspaces—upto1060for bachetal.,2022;Zhangetal.,2023),aswellasapplications
drug-likesmallmolecules(Bohaceketal.,1996)and20N acrosschemistry(Jingetal.,2022;Corsoetal.,2023)and
forN-lengthproteinsequences(MaynardSmith,1970)— biology(Watsonetal.,2023;Abramsonetal.,2024).
rendersthisachallengingcombinatorialoptimizationprob-
Whilediffusionmodelsexcelatcapturingcomplex,multi-
lem. Consideringthesubstantialcostofsynthesizingand
modaldensities(Kadkhodaieetal.,2024),accuratelymod-
validatingcandidatecompoundsexperimentally,theques-
elingthetrainingdistributionisonlythefirststep. Tofully
tionofhowtoefficientlynavigatethesesearchspacestolo-
leveragediffusionmodelsforappliedmolecularoptimiza-
catehigh-valuesubsetsliesatthecoreofmodernmolecular
tion,werequirecarefullydesignedguidancefunctionsthat
design(Go´mez-Bombarellietal.,2018;Sanchez-Lengeling
steerthesamplegenerationprocesstowardcompoundswith
&Aspuru-Guzik,2018;Bilodeauetal.,2022).
desirableproperties(Weissetal.,2023;Gruveretal.,2023;
1University of Oxford, UK 2New York Univer- Leeetal.,2023). However,whenlabeleddatafortraining
sity, New York, USA. Correspondence to: Leo Klarner theseguidancefunctionsisscarceandonlyavailablefora
<leo.klarner@stats.ox.ac.uk>.
biasedandunrepresentativesubsetoftheinputdomain,as
isoftenthecaseinpractice,overconfidentguidancesignals
Proceedings of the 41st International Conference on Machine
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by risk steering the generative process toward false-positive
theauthor(s). regionsofchemicalorproteinsequencespace(Figure1).
1
4202
luJ
61
]MB.oib-q[
1v24911.7042:viXraContext-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
Figure2: Context-guideddiffusionleveragesunlabeledcontextdatatocombinesignalsfromlabeledtrainingdatawith
structural information of the broader input domain (left). Specifically, we construct a data- and noise scale-dependent
guidancemodelregularizerthatencouragessmoothgradients,meanreversion,andhighpredictiveuncertaintyonout-of-
distribution(OOD)inputs,allowingtheconditionaldenoisingprocessunderacontext-guideddiffusionmodeltofocuson
promisingnear-OODsubsetsofchemicalandproteinsequencespace(right).
Inthispaper,weproposecontext-guideddiffusion(CGD)— 2.GuidedDiffusionModels
a simple plug-and-play method that leverages unlabeled
Westartbyprovidingabriefoverviewofstandarduncondi-
data and smoothness constraints to improve the out-of-
tionaldiffusionmodels,followingSongetal.(2020),and
distribution generalization of property-guided diffusion
then discuss conditional sampling schemes, focusing on
models.Specifically,weconstructaguidancemodelregular-
classifier guidance as introduced in Sohl-Dickstein et al.
izerthatleveragesunlabeledcontextdatatotrainguidance
(2015),Songetal.(2020)andDhariwal&Nichol(2021).
modelsthatboth(i)fitthetrainingdatawelland(ii)exhibit
highuncertaintyandsmoothgradientsinout-of-distribution
(OOD)regionsoftheinputdomain(Figure2). Weshow Unconditional Diffusion Models. Denoising diffusion
that using the resulting context-aware guidance model to modelsprogressivelyaddnoisetoadatadistributionp 0un-
performcontext-guideddiffusionleadstoconditionalsam- tilitapproachesatractablereferencedistributionp T. This
plingprocessesthatconsistentlygeneratenovel,near-OOD forwardnoisingprocessisgovernedbyastochasticdiffer-
moleculeswithdesirableproperties. entialequation(SDE)(Songetal.,2020),definedas
CGDisstraightforwardtoimplement,requiresnochanges dX =f(X ,t)+g(t)dB , (1)
t t t
tothearchitectureoftheguidancemodel,andaddsnocom-
putationaloverheadatsamplingtime. Crucially,itisagnos- whereX ∼ p issampledfromthedatadistributionand
0 0
tictotheunderlyinggenerativeprocessandthuscompatible B representsthed-dimensionalBrownianmotion. Acom-
t
withlatent(Rombachetal.,2022;Xuetal.,2023),equiv- mon choice is the VP-SDE (Sohl-Dickstein et al., 2015;
ariant(Hoogeboometal.,2022),Riemannian(DeBortoli Hoetal.,2020),withdriftcoefficientf(X ,t)=−1β X
√ t 2 t t
etal.,2022;Huangetal.,2022), andconstrained(Lou& anddiffusioncoefficientg(t)= β ,wherethenoisescale
t
Ermon,2023;Fishmanetal.,2023a;b)diffusionmodels. β >0 determines the amount of corruption. Over time
t
stepst∈[0,T],thisSDEgraduallynoisesX ∼p untilit
Wedemonstratetheversatilityofourapproachbyapplying t t
convergestothereferencedistributionp =N(0,I).
ittothedesignofsmallmoleculeswithgraph-structureddif- T
fusionprocesses(Leeetal.,2023),thegenerationofnovel Togeneratenewsamplesfromp ,diffusionmodelsleverage
0
materials with equivariant diffusion models (Weiss et al., thefactthattheforwardnoisingprocessdefinedbyEqua-
2023),andtheoptimizationofdiscreteproteinsequences tion(1)admitsatime-reversal(Anderson,1982;Haussmann
withcategoricaldiffusion(Gruveretal.,2023). Wefindthat &Pardoux,1986). FortheVP-SDE,thisprocessisgivenby
CGDconsistentlyoutperformsstandardguidancemodels,as
(cid:26) (cid:27)
wellasmoresophisticatedpre-traininganddomainadapta- 1 (cid:112)
dX =−β X +∇logp (X ) dt+ β dB , (2)
tiontechniques,andthatitenablesmorereliablegeneration t t 2 t t t t t
ofnovelsamplesfromhigh-valuesubsetsofchemicaland
proteinsequencespace. whichflowsbackwardintime. X T ∼p T issampledfrom
thereferencedistributionandincrementallydenoiseduntil
Thecodeforourexperimentscanbeaccessedat: it reflects p . We have used the shorthand ∇logp (X )
0 t t
https://github.com/leojklarner/
todenote∇ logp (x)| andwillcontinueusingthis
context-guided-diffusion x t x=Xt
shorthandintheremainderofthetext.
2Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
Thegradientofthelog-density∇logp (x )inEquation(2) 3.Context-GuidedDiffusionModels
t t
isestimatedbyatime-dependentscorenetworks (x ,t)
ψ t
Inthissection,wepresentcontext-guideddiffusion(CGD)—
usingtechniquesfromscorematching(Hyva¨rinen&Dayan,
a method that produces more robust conditional gradient
2005). Oncetrained,s canbeusedtogeneratesamples
ψ
estimatesthatconsistentlygeneratenovelsampleswithim-
fromthedatadistributionp bysimulatingthereverse-time
0
proved properties. We first introduce the prevailing ap-
SDEwitharangeofnumericalsolvers(Songetal.,2020).
proachtotrainingguidancemodelsandhighlightitslimi-
tations. Wethenpresentadomain-informedregularization
GuidedDiffusionModels. Inmanyapplications,wemay
termdesignedtobiasguidancemodeltrainingtowardfunc-
wishtoconditionthegenerativeprocessofdiffusionmod-
tions that both (i) fit the training data well and (ii) revert
elstoproducesampleswithspecificpropertiesy,suchas
todomain-appropriatebehaviorinout-of-distributionset-
imagesofacertainclassormoleculeswithadesirablephar-
tings.Finally,wedescribehowtoincorporatecontext-aware
macologicaleffect.Thatis,wewanttomodifythedenoising
guidance models into guided diffusion to generate novel
processinEquation(2)toincorporatetheconditioningin-
moleculesandproteinsequenceswithdesirableproperties.
formationy,andsamplefromtheconditionaldistribution
p (x |y) rather than the unconditional data distribution
0 0
3.1.StandardGuidanceModelTraining
p (x ). Thecorrespondingreverse-timeSDEisgivenby
0 0
(cid:26) 1 (cid:27) (cid:112) We consider a supervised learning setting with diffu-
dX =−β X +∇logp (X | y) dt+ β dB ,
t t 2 t t t t t sion timesteps t=0,...,T and N i.i.d. data realizations
D ={(x(n),y¯(n))}N of the noised inputs x ∈X and
differingfromEquation(2)onlyintheneedforaconditional t t n=1 t
targets y¯ ∈Y ⊆Rk. Furthermore, we define the guid-
scorefunction∇logp (X | y),whichcanbeexpressedas
t t ancemodelfj(·;θ)=˙ gj(h (·;θ );θj)withj =1,2and
t t h L
∇logp (X |
y)=∇logp t(y | X t)p t(X t) θ=˙ {θ h,θ L1,θ L2}. Here, h t(·;θ h):X →Rd is an em-
t t Z (3) bedding model, and g1(h (·;θ );θ1)=˙ h (·;θ )θ1 and
t h L t h L
=∇logp (y | X )+∇logp (X ), g2(h (·;θ );θ2)=˙ exp(h (·;θ )θ2) are k-dimensional
t t t t t h L t h L
meanandvarianceheads,respectively.
sinceZ isanormalizationconstantindependentofX .
t
Conventional supervised learning techniques for training
To approximate ∇logp (y | X ), a time-dependent dis-
t t
guidancemodelsdefinethediffusiontimestep-dependent
criminativeguidancemodelf (x ;θ)istrainedonalabeled
t t
datasetD =∪T {(x(n),y¯(n))}N withinputscorrupted negativelog-likelihoodobjective
t=1 t n=1 N
bythesameforwardnoisingprocessasthediffusionmodel. L(θ,D ,T)=˙ −(cid:88) logp (y¯(n)| x(n);fj(·;θ)) (4)
Oncetrained,theguidancemodelcanbeincorporatedintoa T T T T
n=1
guidancefunctionp(y | X ;f(·;θ))thatisusedtocompute
t where T ∼p (t) is a randomly sampled timestep. The
the conditional scores ∇logp (X | y;f(·;θ)), which in T
t t guidancemodelparametersθareoptimizedbyminimizing
turnsteerthereverse-timeSDEtowardsampleswithahigh
theL -regularizedobjectivefunction
predictedlikelihoodofobservedlabelyvia 2
1
(cid:26) (cid:27) L¯(θ,D )=˙ E [L(θ,D ,T)]+ ||θ||2, (5)
1 (cid:112) T pT T 2λ 2
dX =−β X +∇logp (X | y;f (·;θ)) dt+ β dB .
t t 2 t t t t t t whereλ∈R ischosenasahyperparameter.
+
Alternativeapproachessuchasclassifier-freeguidance(Ho Unfortunately, this approach is only able to operate over
&Salimans,2022)estimatetheconditionalscoresbypass- subsetsoftheinputdomainX forwhichlabeleddataisavail-
ingtheconditioninginformationtothescorenetwork able.Whenlabelsarescarce,asisthecaseinmanypractical
settings,modelstrainedwithstandardsupervisedlearning
∇logp (X | y)≈s (x ,y,t),
t t ψ t objectivesarepronetogeneratingincorrectandoverconfi-
eitherintheformofclasslabels(Ho&Salimans,2022)or dentpredictionswhenexposedtodistributionshifts(Ovadia
pre-trainedembeddings(Nicholetal.,2021;Rameshetal., etal.,2019;Liuetal.,2020;VanAmersfoortetal.,2020;
2022; Saharia et al., 2022). However, the requirement to Kohetal.,2021;Bandetal.,2021;Rudneretal.,2021;Tran
providey asanexplicitinputtothescorenetworklimits etal.,2022;Rudneretal.,2022;Klarneretal.,2023). This
theapplicabilityofthesemethodstoregression-basedopti- issue is exacerbated by the fact that guidance models are
mizationproblemsfrequentlyencounteredinmolecularand trained on noised data points: Since neural networks are
proteindesign(Leeetal.,2023;Weissetal.,2023;Gruver bothhighlysensitivetonoiseperturbations(Szegedyetal.,
etal.,2023),wheretheobjectiveistomaximizeorminimize 2013;Goodfellowetal.,2014;Ho&Salimans,2022)and
a property for which the optimal value is often unknown abletooverfittofullycorruptedinputs(Zhangetal.,2021),
andlikelytoliewelloutsidethetrainingdistributionofthe theerrorsofsystematicallyoverconfidentgradientsignals
conditionalscorenetwork. mayaccumulateoverhundredsofdenoisingsteps.
3Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
Algorithm 1 One guidance model training
epoch with the context-guided optimization
objectivefromEquation(9).
for(x ,y¯)∈D do
0 0
t∼U(0,T)
x ←NoisingProcess(x ,t) (1)
t 0
L←−logp (y¯|x ;fj(x ;θ)) (4)
t t t t
L¯←L+ 21 λ||θ||2 2 (5) (b)PredictionsandsamplesfromanL 2-regularizedguidancemodel.
xˆ ∼p
0 Xˆ
0
s∼U(0,T)
xˆ ←NoisingProcess(xˆ ,s) (1)
s 0
R←(cid:80) D (fj(xˆ ;θ),mj,K )2 (6)
M s s t t
j
θ ←Update(θ,L¯+R)
end
(a) An algorithmic overview of the proposed
context-guidedregularizationscheme.
(c)Predictionsandsamplesfromacontext-awareguidancemodel.
Figure3: Panel(a)presentsanalgorithmicperspectiveonhowtheproposedregularizationschemefitsintothetrainingloop
ofastandardguidancemodel. Panels(b)and(c)illustratehowtheregularizationschemeaffectsthefunctionsaguidance
modellearns. Tworegressionmodelswithdifferentregularizersaretrainedonalow-labelsubsetofanillustrativedataset
(color-codedcrosses).However,withoutadditionalinformationabouttheinputdomain,thepredictionsoftheL -regularized
2
model(b)exhibitpoorgeneralizationandmiscalibrateduncertaintyestimateswhenevaluatedinnew,unseenregions. In
contrast, the context-aware model (c) is able to leverage unlabeled data sampled uniformly from [−2.5,2.5]2 ⊂ R2 to
generatemoreaccurateandbetter-calibratedpredictions. Thecorrespondingcontext-guideddenoisingprocessgenerates
samplesthatrecoveraheld-out,high-labeltestset(blackcrosses). FullexperimentaldetailsarepresentedinAppendixB.1.
3.2.Context-AwareGuidanceModels Morespecifically,weconsiderregularizersoftheform
To improve the performance and reliability of guidance R(θ,f ,t,p )
models in this setting, we propose a family of diffusion
t Xˆ
t
 
timestep-dependentregularizersdesignedtofavorparam- (cid:88)2 (6)
eters θ that encode desirable model behaviors on out-of- =˙ E p Xˆt  D M(f tj(xˆ t;θ),mj t(xˆ t),K t(xˆ t))2 ,
distributiondata. Whileidealmodelbehaviorsunderdistri- j=1
butionshiftsareoftenapplication-dependent,wedesigna
where p is a uniform distribution over noised context
general-purposeregularizer—applicableacrossdomains— Xˆ t
batchesxˆ ofsizeM (withM ≪N ),and
thatencouragesuncertainpredictionsinareasoftheinput t C
domainwithlittleornosignalfromthelabeleddata. This D (fj(xˆ ;θ),mj(xˆ ),K (xˆ ))2 (7)
M t t t t t t
regularizer maximizes sensitivity at the expense of speci-
=˙ (fj(xˆ ;θ)−mj(xˆ ))⊤K (xˆ )−1(fj(xˆ ;θ)−mj(xˆ ))
ficityandenablestheconditionaldenoisingprocesstofocus t t t t t t t t t t
onregionsindataspacethatarenearthetrainingdataand
isthesquaredMahalanobisdistancebetweennoisedmodel
have the highest likelihood of containing molecules with
predictions fj(xˆ ;θ) and a data- and timestep-dependent
improvedproperties. t t
distribution over random vectors with mean mj(xˆ ) and
t t
Toencodethisbehavior,weconstructatimestep-dependent covarianceK (xˆ ). Here,mj(xˆ )specifiesthedesiredout-
t t t t
distributionoverguidancefunctionoutputsthatisexplicitly of-distributionbehaviorforeachguidancemodelhead,and
designedtoexhibithighuncertaintyonaproblem-specific, K (xˆ )∈RM×M allowsustoencodeadditionalinforma-
t t
out-of-distributioncontextsetxˆ C=˙ {x( Ci)}N i=C 1. tionaboutstructuralpropertiesoftheinputdomain.
4Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
Specifically, K (xˆ ) ̸= I controls how strongly the guid- Implementation. The proposed regularizer R does not
t t
ancemodelpredictionsfj(xˆ ;θ)canvarybetweensimilar requireanychangestotheguidancemodelitselfandcan
t t
context points, allowing us to enforce smoother gradient beeasilyintegratedintostandardguidancemodeltraining
estimates and more stable guidance signals in regions of pipelines: At each iteration, we sample a mini-batch xˆ
t
dataspacewithlimitedsignalfromthelabeledtrainingdata. ofsizeM fromthecontextsetxˆ andperturbitwiththe
C
samenoisingprocessastheunconditionaldiffusionmodel
Thiscovariancefunctioncanbedefinedintermsofanyvalid,
inEquation(1). Wethenperformagradient-freeforward
domain-appropriatesimilaritymetric. Inthefollowing,we
pass of the embedding model to compute h (xˆ ,ϕ) and
presentageneralapproachtoconstructingK frommodel t t
t constructthecovariancematrixK (xˆ ). Finally,weusethe
embeddings. Specifically, we consider a separate set of t t
predictionsoftheguidancemodelf (xˆ ;θ)onthecontext
fixed parameters ϕ to generate the network embeddings t t
batchtocomputethefullregularizationterminEquation(6).
h (xˆ ,ϕ)∈RM×dofallmoleculesinthecontextbatchxˆ .
t t t AnalgorithmicoutlineofthisprocessisshowninFigure3.
Wethenusetheirinnerproducttoconstruct
As the risk of miscalibrated gradient estimates is partic-
K t(xˆ t)=˙ σ th t(xˆ t,ϕ)h t(xˆ t,ϕ)T +τ tI, (8) ularly high in the earlier stages of the sample generation
process—wherehighlydiffusedinputpointsareassociated
whereσ tisacovariancescaleparameterandτ tisadiagonal withthelabelsofthecorrespondingun-noisedpoints—it
offsetparameter. Thesehyperparametersallowustocali- is important to ensure that our regularizer is able to take
bratethebehaviorofourregularizer,withτ t determining thenoiselevelβ tintoaccount. Specifically,weincreaseτ t
howcloselythepredictionshavetomatchthemeanfunction with the same schedule as the noise levels β to penalize
t
mj t(xˆ t)andσ tdeterminingthestrengthofthesmoothness overconfident predictions as context points approach the
constraintsplacedonf tj(xˆ t;θ)forbothj =1,2. referencedistributionp
T
=N(0,I). Similarly,asthedis-
tinctionbetweenin-distributionandout-of-distributiondata
Thisapproachisagnostictohowtheembeddingsh (xˆ ,ϕ)
t t
pointsbecomeslessmeaningfulatlargernoiselevels,we
arederived,solongastheyencoderelevantstructuralprop-
decreaseσ withaninvertedschedule.
ertiesoftheinputdomain. Wefindthatarelativelysimple t
approach is already sufficient to produce substantial per-
3.3.GenerationviaContext-GuidedDiffusion
formance gains in the settings we consider in Section 5.
Specifically,weuseafixedsetofrandomlyinitializedpa- Oncethecontext-awareguidancemodelfj(·;θ)hasbeen
t
rametersϕtoobtainh (xˆ ,ϕ)fromthemodel’sembedding
t t trained,wecanuseittosteerthedenoisingprocesstowards
trunk. Thisisapplicableacrossdifferentnetworktypesand
out-of-distributionsamplesthataremostlikelytoexhibit
domains,andmotivatedbytheobservationthatneuralnet-
desiredproperties. Todoso,weusethecontext-awareguid-
workswithsuitableinductivebiases,suchaspermutation-
ancemodeltospecifyacontext-awareguidancefunction
orroto-translationalequivarianceinSections5.1and5.2,
p(y | x ;f (·;θ)). Thisfunctiondefinesadistribution,con-
t t
canalreadyproducerobustandinformativerepresentations
ditionedonboththetrainingdataandthecontextset,that
inarandomlyinitializedstate(Kipf&Welling,2016).
reflectstheprobabilityofobtainingthedesiredproperties
HavingconstructedthecovariancematrixK t(xˆ t),wenow givenx t. Thisallowsustointegratesignalsfromlabeled
specify the mean function mj(xˆ ) for j = 1,2. In par- data points with structural knowledge about the broader
t t
ticular,lettingf (·;θ)beasdefinedabove,withalearned inputdomainintothesamplingprocess. Detailsaboutguid-
t
mean function f1(xˆ ;θ) and a learned variance function ancefunctionspecificationareprovidedinAppendixA,and
t t
f2(xˆ ;θ),wewishforthemeanpredictionstoreverttothe anillustrativeexamplethatshowcasestheeffectofthepro-
t t
meanofthetraininglabelsandforthevariancetobehigh posedregularizerontheguidancemodel(andtheresulting
when the model is evaluated on points that are meaning- guideddiffusionprocess)isshowninFigure3.
fullydifferentfromthetrainingdata. Toencodethisdesired
Finally,thecontext-awareguidancefunctioncanbeseam-
behavior into the regularizer defined in Equation (6), we
lesslyintegratedintoexistingguideddiffusionmodelframe-
definem1(xˆ )=˙ 1 (cid:80)N y¯(n)forthetrainingdatalabels
t t N n=1 worksbyreplacingstandardguidancefunctionsatsampling
y¯(n),m2(xˆ )=˙ σ2,foratargetvarianceparameterσ2.
t t 0 0 time. Thismodular, plug-and-playapproachensuresthat
Addingthisregularizertothesupervisedlearninglossfrom ourmethodisindependentofthespecificdiffusionmodel
Equation(5),wearriveatthemodifiedtrainingobjective architectureandgenerativeprocessandrequiresminimal
implementationaloverhead. Importantly,likeconventional
L∗(θ,D )=E [L(θ,D ,T)+R(θ,f ,T,p )], classifier-guideddiffusion,ourmethoddoesnotnecessitate
T pT T T Xˆ
T (9) any(re-)trainingorfine-tuningofthediffusionmodelitself,
makingitaconvenientout-of-the-boxtoolforconditioning
wherebothexpectations(overp andp )canbeestimated
T Xˆ
t
thegenerationprocessofapre-trainedmodel.
viasimpleMonteCarloestimation.
5Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
4.RelatedWork 5.EmpiricalEvaluation
Guided Diffusion for Molecular Design. Guiding the Wecompareourmethodtobothstandardguidancemodels
denoisingprocessofdiffusionmodelswithgradientsfrom andmoresophisticatedpre-traininganddomainadaptation
aclassifierorconditionalscorenetworkisaneffectiveap- techniquesacrossarangeofexperimentalsettings. Specifi-
proachthathasbeenappliedtoabroadrangeofscientific cally,wedemonstratetheversatilityofcontext-guideddif-
problems,suchasthegenerationofdesirableproteinfolds fusionbyapplyingittothedesignofsmallmoleculeswith
(Watson et al., 2023; Ingraham et al., 2023) and molecu- graph-structured diffusion processes (Section 5.1), to the
lar conformations (Hoogeboom et al., 2022; Peng et al., generation of novel materials with equivariant diffusion
2023; Guan et al., 2023). Similarly to our approach, re- models (Section 5.2), and to the optimization of discrete
centworkhasexploredproperty-guideddiffusionmodels proteinsequenceswithcategoricaldiffusionmodels(Sec-
formolecularoptimizationwithaparticularfocusonout- tion5.3). Weobservethatcontext-guideddiffusionleadsto
of-distributionsamplegeneration. Forinstance,Leeetal. improvedconditionalsamplingprocessesthatconsistently
(2023) proposed a modification of the unconditional de- andsubstantiallyoutperformexistingmethodsacrossdiffer-
noisingprocessthat, incombinationwithstandardsuper- entdiffusionmodeltypesandapplicationdomains.
vised guidance, generated more diverse small molecules
with improved properties. Likewise, Weiss et al. (2023) 5.1.Graph-StructuredDiffusionforSmallMolecules
used a guided diffusion model to optimize the electronic
Toevaluateourmethodinthecontextofgraph-structured
propertiesofpolycyclicaromaticsystems,demonstrating
diffusionprocessesforsmallmoleculegeneration,wefol-
generalizationtounseenheterocycles. Furthermore,Gruver
lowtheframeworkofLeeetal.(2023)andconsideradis-
etal.(2023)exploredproperty-guideddiscretediffusionfor
cretediffusionprocessoverthespaceofmoleculargraphs.
multi-objective protein sequence optimization, balancing
Definingagraphg asatupleofnodefeatureandadjacency
naturalnesswithimprovedpropertiesthroughagridofsam- t
matricesg = (x ,a ),theforwardandreverseprocesses
plinghyperparameters. However, alloftheseapproaches t t t
aregivenbyajointsystemof SDEs(Joetal.,2022). To
primarilyfocusonmodifyingtheunconditionalsampling
encouragetheexplorationofnovelchemicalspace,Leeetal.
processitselfandrelyonstandardsupervisedlearningtech-
(2023)conditionthedenoisingprocessonahyperparameter
niquesforguidancemodeltraining. Ourmethodisorthogo-
γ ∈[0,1)thatshrinkstheunconditionalgradients
naltothesecontributionsandaimstofurtherimprovethe
performance of property-guided diffusion models across
∇logp (g |γ)=∇logp (g )+∇logp (γ|g )
applicationdomainsandgenerativeprocesses. t t √t t t t (10)
=(1− γ)∇logp (g ).
t t
RegularizationwithUnlabeledData. Usingunlabeled
Inaddition,thedenoisingprocessisconditionedondesir-
datatoimprovetheperformanceofpredictivemodelsisa
ablemolecularpropertieswithastandardsupervisedguid-
well-establishedparadigmthatunderpinsmanysuccessful
ancemodelasoutlinedinSections2and3.1. Thismodel
semi-supervised(Kingmaetal.,2014;Laine&Aila,2016;
istrainedtopredictthelabelsy∈[0,1],definedasacom-
Berthelot et al., 2019) and self-supervised (Brown et al.,
positeofthesyntheticaccessibility(SA;Ertl&Schuffen-
2020;Bommasanietal.,2021)algorithms. Thisincludesa
hauer,2009),drug-likeness(QED;Bickertonetal.,2012),
rangeofunsuperviseddomainadaptationandgeneralization
andQuickVinadockingscores(Alhossaryetal.,2015),de-
techniquesthatuseunlabeleddatatomitigatetheadverseef-
scribed in further detail in Appendix B.2. In this setting,
fectofdistributionshifts(Tzengetal.,2015;Sun&Saenko,
largerlabelsdenotemoredesirableproperties.
2016;Ganinetal.,2016;Kangetal.,2019;Lietal.,2020).
However,thesemethodspredominantlyfocusonaligning As the docking scores are protein pocket-dependent, we
modelembeddingsandare,contrarytoourapproach,un- repeatallexperimentsacrossthefivedifferenttargetschosen
abletodirectlyregularizemodelpredictions. Themethod byLeeetal.(2023),namelyPARP1,FA7,5HT1B,BRAF
mostcloselyrelatedtoourapproachistheregularization and JAK2. We closely follow their experimental setup
schemeproposedinRudneretal.(2023). However,incon- and train guidance models on a subset of 250000 small
trasttoRudneretal.(2023),whoonlyconsidersupervised molecules from the ZINC database (Irwin et al., 2012).
imageclassificationtasks,ourapproachisspecificallyde- Specifically,wepre-computethelabelsyforeverytarget
signedfortheconditionalgenerationofmolecularstructures. andusethemtosplitthedataintoalow-propertytraining
Keymethodologicaldevelopmentsinthisregardincludethe andahigh-propertyvalidationset. Thislabelsplitallowsus
introductionofafactorizedregularizationtermthatissuit- toselectregularizationhyperparametersthatmaximizethe
ableforusewithregressionmodels,aswellasanexplicit abilityoftheguidancemodelstogeneralizewelltonovel,
timestep-dependencethataccountsforthenoisingprocess high-valueregionsofchemicalspace,servingasaproxyfor
oftheunderlyingdiffusionmodel. thedesiredbehavioratsamplingtime.
6Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
Regularization Type
ours
ours (linear)
ours (constant)
deep ensemble
self-supervised
L regularization
2
weight decay
Figure4: Comparisonofthesmallmoleculesgeneratedwithdifferentguideddiffusionmodelsacrossfivedistinctprotein
targets. Objectivevalues(↑)arenormalizedwithrespecttothehighestscoreoftheheld-outhigh-propertyvalidationsetand
averagedacrossfiveindependenttrainingandsamplingrunswithdifferentrandomseeds.
Toconstructanappropriatecontextsetforusewithourreg- Toputtheseabsolutevaluesintoperspective,wenormalize
ularizer,wesampleanadditional500000unlabeledsmall themsothat1correspondstothehighestdockingscorein
moleculesfromtheZINCdatabase. Following Section3.2, the high-property validation set. The results are reported
wespecifyapredictivedistributionthatrevertstothetrain- inFigure4anddemonstratethatourmethodconsistentlyim-
ing set mean and high uncertainty on out-of-distribution provestheperformanceofproperty-guideddiffusionmodels
context points. We compare this approach to a range of acrossdifferentproteintargets. Thefulldistributionsofall
alternativeregularizationschemes,includingweightdecay componentsoftheobjectivefunction, theresultsofmod-
(usedinLeeetal.(2023)),explicitL regularization,and elstrainedwithlabelnoise,aswellasversionsnormalized
2
anensembleofindependentlytrainednetworks(Lakshmi- byheavyatomcount,arepresentedinAppendixB.2and
narayananetal.,2017). Tofacilitateafaircomparisonof displayidenticaltrends. Whileensemblingandpre-training
thesemethods,weensurethatalltrainingparameters,except techniques perform best on in-distribution tasks (see Ap-
regularization type and strength, are kept constant across pendixB.2),wefindthattheyalonearenotsufficienttoover-
experiments. Afterselectingtheoptimalhyperparameters come the limitations of standard supervised training/fine-
usingthehigh-propertyvalidationset,allmodelsareinde- tuningtechniques. However,wenotethatcontextguidance
pendentlyretrainedwithfivedifferentrandomseeds. We isanorthogonalapproachthatisstraightforwardtousein
refertoAppendixB.2forfullimplementationaldetails,hy- combinationwiththesemethods.
perparameterranges,additionalresults,andsanitychecks.
Toinvestigatehowimportantthetimestep-dependenceof
We additionally evaluate the performance of other ap- ourmethodistoitsempiricalperformance,wecarryouta
proachesthatareabletomakeuseofunlabeleddata. Specif- seriesofablationsinwhichtheregularizationhyperparame-
ically,wepre-trainaguidancemodelonallmoleculesfrom tersσ andτ either(i)mimictheβ scheduleofthenoising
t t t
ourcontextsetwithaself-superviseddenoisingobjective process,(ii)changelinearlyovertime,or(iii)stayconstant
(Zaidietal.,2023)andthenfine-tuneitonthelabeleddata, acrossallnoisescales.Weobserveastrictperformancedrop
usingweightdecay. Furthermore,weconsiderarangeof acrossthesesettings,indicatingthatadaptingtheregularizer
unsupervised domain adaptation and generalization tech- tothenoisingprocessishighlybeneficial(Figure4).
niques,namelyDeepCoral(Sun&Saenko,2016),domain-
Furthermore, we investigate the impact that the size and
adversarialneuralnetworks(Ganinetal.,2016),anddomain
composition of our context set have on the performance
confusion(Tzengetal.,2015),andpresenttheresultsfor
ofourmethodbycomparingmodelstrainedwiththefull
thesemethodsinAppendixB.2.
contextsettomodelstrainedwithrandomlysampledsubsets
Adhering to the evaluation protocol of Lee et al. (2023), ofsize10%and1%,respectively. Asexpected,weobserve
we use the retrained guidance models to generate 3000 astrongdecreaseinperformance,withtheresultsofmodels
moleculeseachandderivethecorrespondinglabelswiththe trainedwiththesmallestcontextset(1%)revertingtothatof
provideddockingandevaluationpipeline. AsinLeeetal. standardweightdecay. Additionally,weselectthe10%of
(2023),weonlyretaincompoundsthatpasssyntheticacces- compoundsintheoriginalcontextsetthatareeithermostor
sibilityanddrug-likenessthresholdsanddisplayamaximum leastsimilartothelabeledtrainingdataandfindthatusing
ECFP4-basedTanimotosimilarity(Rogers&Hahn,2010) amuchsmallersetofmoresimilar,near-OODmoleculesis
oflessthan0.4toanymoleculeinthetrainingset,before abletomatchtheperformanceofthefullcontextset. Full
reportingthetop5%oftheremainingdockingscores. detailsandexperimentalresultsarepresentedinTable6.
7Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
Regularization Type
ours (full context set)
ours (reduced context set)
L regularization
2
weight decay
Figure5: Comparisonofpolycyclicaromaticsystemsgeneratedwithdifferentguidancemodelsacrosstenindependent
trainingandsamplingruns. Left: Fulldistributionofgeneratedobjectivevalues(↓)ablatedoverdifferentcontextsetsand
guidancescales. Right: UMAPplot(McInnesetal.,2018)oftraining(upperleft)andtestset(lowerright),aswellas
samplesfromguideddiffusionmodels. ValidityandnoveltyareanalyzedinAppendixB.3andshowsimilartrends.
5.2.EquivariantDiffusionForMaterials 5.3.DiscreteDiffusionforProteinSequences
Weadditionallyevaluateourmethodbyapplyingittoequiv- Furthermore,weexplorehowwellourmethodextendsto
ariantdiffusionmodelsformaterialsdesign. Specifically, categorical diffusion models in the context of protein se-
wefollowtheexperimentalsetupofWeissetal.(2023)and quence optimization. In particular, we follow the experi-
trainanE(3)-equivariantdiffusionmodel(Hoogeboometal., mentalframeworkofGruveretal.(2023)toapplycontext-
2022)onallpolycyclicaromaticsystemsconsistingofup guided diffusion to the property-conditioned infilling of
to 11 cata-condensed benzene rings (Wahab et al., 2022). antibodycomplementarity-determiningregions. Incontrast
AsinWeissetal.(2023), weguidethesamplingprocess to the applications in Sections 5.1 and 5.2, the guidance
towardsmoleculeswithdesirableelectronicpropertiesby modelofGruveretal.(2023)isnotaseparatenetwork,but
leveragingthegradientsofanE(3)-equivariantgraphneural ratheraregressionheadofthescoremodel. Specifically,a
network (Satorras et al., 2021) to minimize a composite maskedlanguagemodel(Austinetal.,2021;Bhargavaetal.,
objectiveofadiabaticionizationpotential,electronaffinity, 2021)isusedtolearnsequenceembeddingsfromwhicha
andHOMO-LUMOgap. linearoutputheadandafeed-forwardnetworkestimatethe
scoresandproperties,respectively.
IncontrasttotheexperimentalsettinginSection5.1,this
application considers an exhaustively enumerated search Atsamplingtime,Gruveretal.(2023)generatesequences
space,allowingustoconstructclustereddatasplitsinwhich withagridofdenoisinghyperparametersandexaminethe
maximalgeneralizationisrequiredtoreachheld-outregions resultingParetofrontofobjectivevaluesversus“naturalness”
with optimal objective values. Using this setup, we train i.e. howlikelyasequenceistobesynthesizable,estimated
a diffusion model and different guidance networks on a by its likelihood under a protein language model (Ferruz
low-valuetrainingsetandexaminehowwelltheyareable et al., 2022). Following this approach, we train different
torecovermoleculesfromheld-outregionswithdesirable guidance models and use them to generate samples with
electronicproperties. Wepresentacomparisonofdifferent thesamehyperparametergridasGruveretal.(2023). The
methodsinFigure5,confirmingthatourapproachperforms correspondingParetofrontsarepresentedinFigure6.While
significantlybetterthanstandardregularizationtechniques standardmethodsfarewellinin-distributionsettings, we
acrossguidancescales. Weadditionallyablatetheinforma- observethatmodelstrainedwithourregularizeroutperform
tioncontentofthecontextsetbyremovingthemostrelevant themassamplesprogressintoanout-of-distributionregime,
entries, observingthattheseareakeydeterminantofour consistentlyproducingbetterpropertiesforagivenlevelof
method’ssuccess. Fullexperimentaldetailsandadditional naturalness. Fullexperimentaldetailsandadditionalresults
resultsareprovidedinAppendixB.3. areprovidedinAppendixB.4.
8Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
Table1: Ablationoverthecontextbatchsizeusedbyour
regularizer. Larger context batches generally yield better
resultsbutincurhighercomputationalcosts.Wefindthatthe
contextbatchsizecanbereducedsubstantiallywithonlya
marginaldeteriorationofmodelperformance. Performance
metrics and runtimes are averaged across 5 independent
trainingandsamplingrunswithdifferentrandomseeds.
Graph-StructuredDiffusionforSmallMolecules(Section5.1)
Regularization Type
BatchSize BatchTime(s) PARP1(↑) FA7(↑) 5HT1B(↑)
ours
L regularization 128 0.09±0.01 0.66±0.01 0.48±0.01 0.57±0.01
w2
eight decay
64 0.08±0.00 0.66±0.01 0.48±0.01 0.57±0.01
32 0.07±0.00 0.66±0.01 0.48±0.01 0.57±0.02
16 0.07±0.00 0.65±0.02 0.47±0.02 0.56±0.03
8 0.07±0.00 0.62±0.03 0.45±0.03 0.53±0.03
EquivariantDiffusionForMaterials(Section5.2)
Figure6: Paretofrontsofsamplesgeneratedwithdifferent
BatchSize BatchTime(s) scale=0(↓) scale=2(↓) scale=4(↓)
regularizationschemes,highlightingthetrade-offbetween
objectivevalue(↑)andnaturalness(↑). Assamplesmove
128 0.28±0.02 0.45±0.00 0.39±0.01 0.34±0.01
64 0.25±0.03 0.45±0.00 0.40±0.01 0.34±0.02
awayfromthetrainingdataandenteranout-of-distribution 32 0.26±0.01 0.45±0.00 0.42±0.01 0.36±0.01
regime,ourmethodconsistentlygeneratessequenceswith 16 0.24±0.02 0.45±0.00 0.41±0.01 0.38±0.03
betterpropertiesforanygivenlevelofnaturalness. 8 0.23±0.01 0.45±0.00 0.42±0.01 0.39±0.02
6.DiscussionandLimitations Beyondchoosingthecontextbatchsize,ourapproachre-
quirestheoptimizationoftwoadditionalhyperparameters:
Weconductedacomprehensiveempiricalevaluationofour thecovariancescaleσ andthediagonaloffsetparameterτ .
t t
methodacrossthreedistinctmolecularandproteindesign Additionally,wefindthatthechoiceofthecontextsetsig-
tasks, observing consistent and significant improvements nificantlyimpactsmodelperformance,requiringsufficient
overexistingstate-of-the-artguideddiffusionmodels. By domainexpertisetoensuretheinclusionofinformativeand
leveragingunlabeleddatatotrainguidancefunctionsthat relevantunlabeleddatapoints.
exhibithighuncertaintyandwell-behavedgradientsonout-
of-distributioncontextpoints,ourcontext-guidedmodelsen- 7.Conclusions
abledthemorereliablegenerationofnovelmoleculeswith
improvedproperties. Thesegainsweremostpronounced Inthispaper,weintroducedcontext-guideddiffusion,asim-
whengeneralizationfromscarceandbiasedtrainingdata pleplug-and-playmethodthatleveragesunlabeleddataand
was required to generate compounds from unseen, high- smoothnessconstraintstoimprovetheout-of-distribution
valuesubsetsofchemicalandproteinsequencespace. Our generalizationofguideddiffusionmodels.Wedemonstrated
resultshighlightthatevenrelativelysimpleregularization theversatilityofthisapproachbyapplyingittothedesignof
techniques can yield substantial empirical benefits when smallmolecules,materials,andproteins,achievingsubstan-
combinedwithrelevantcontextsets. tialperformancegainsineachdomain. Severalpromising
directionsforfutureworkexist,suchasencodingmorecom-
Theproposedmethodisbestsuitedfordomainsinwhich
plexout-of-distributionbehaviorintotheguidancemodel,
unlabeleddataisabundantorcanbeeasilygenerated.While
for instance by reverting to the outputs of physics-based
itdoesnotincuranycomputationaloverheadatsampling
methodsratherthanhighpredictiveuncertainty. Another
time,wherehundredsofmodelevaluationsarerequiredto
promisingavenueistheconstructionofmaximallyinforma-
generateasinglemolecule,itdoescomewithanincreased
tivecontextsets,potentiallythroughactivelearningstrate-
costduringtraining. Specifically,computingtheregulariza-
giesthatiterativelyselectunlabeleddatapointsbasedon
tiontermrequirestwoadditionalforwardpassespertraining
their relevance and potential to improve guidance model
iterationtogeneratethecontextsetembeddingsandpredic-
performance. Exploringtheintegrationofcontext-guided
tions. Fortunately,wefoundthisaddedcosttobemoderate
diffusion with techniques such as multi-task learning or
inabsoluteterms(seeAppendixB.5),asguidancemodels
meta-learningcouldalsoenablethedevelopmentofmore
tendtoberelativelylightweight. Additionally,weprovidea
versatileguidancefunctionsthatleverageknowledgefrom
sensitivitystudyovercontextbatchsizesinTable1,which
relateddomainsortasks. Webelievethatexploringthisline
suggeststhatthecomputationaloverheadcanbefurtherre-
ofresearchhasthepotentialtoleadtomoreadaptableand
ducedwhilemaintainingfavorableempiricalperformance.
robustdiffusionmodelsthatareabletosolveawiderrange
ofchallengingreal-worldproblems.
9Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
ImpactStatement Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N.,
Oliver,A.,andRaffel,C.A. Mixmatch: Aholisticap-
Thisworkintroducesamethodthatisdesignedtoimprove
proachtosemi-supervisedlearning. AdvancesinNeural
the performance of guided diffusion models for inverse
InformationProcessingSystems,32,2019.
molecularandproteindesign. Ourmethodaimstoacceler-
atethediscoveryanddevelopmentofnewmedicinesand
Bhargava,P.,Drozd,A.,andRogers,A. Generalizationin
moreperformantandsustainablematerialsthatcouldbenefit
nli: Ways (not) to go beyond simple heuristics. arXiv
societyinavarietyofways(Wangetal.,2023). However,
preprintarXiv:2110.01518,2021.
thereisalsotheriskofdual-useormisuse,forexamplefor
thegenerationofhazardousagents(Urbinaetal.,2022).
Bickerton,G.R.,Paolini,G.V.,Besnard,J.,Muresan,S.,
andHopkins,A.L. Quantifyingthechemicalbeautyof
Acknowledgements drugs. NatureChemistry,4(2):90–98,2012.
Wethanktheanonymousreviewersfortheirusefulfeedback. Bilodeau,C.,Jin,W.,Jaakkola,T.,Barzilay,R.,andJensen,
LKacknowledgesfinancialsupportfromtheUniversityof K.F. Generativemodelsformoleculardiscovery: Recent
Oxford’sClarendonFundandF.Hoffmann-LaRocheAG. advancesandchallenges.WileyInterdisciplinaryReviews:
WegratefullyacknowledgetheOxfordAdvancedResearch ComputationalMolecularScience,12(5):e1608,2022.
Computingserviceforprovidingcomputingresourcesand
infrastructure. Bohacek,R.S.,McMartin,C.,andGuida,W.C. Theart
andpracticeofstructure-baseddrugdesign: amolecular
References modelingperspective. MedicinalResearchReviews,16
(1):3–50,1996.
Abramson, J., Adler, J., Dunger, J., Evans, R., Green, T.,
Pritzel,A.,Ronneberger,O.,Willmore,L.,Ballard,A.J., Bommasani, R., Hudson, D. A., Adeli, E., Altman, R.,
Bambrick, J., et al. Accurate structure prediction of Arora,S.,vonArx,S.,Bernstein,M.S.,Bohg,J.,Bosse-
biomolecularinteractionswithalphafold3. Nature,pp. lut,A.,Brunskill,E.,etal. Ontheopportunitiesandrisks
1–3,2024. offoundationmodels. arXivpreprintarXiv:2108.07258,
2021.
Alhossary, A., Handoko, S. D., Mu, Y., and Kwoh, C.-
K. Fast,accurate,andreliablemoleculardockingwith Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,
QuickVina2. Bioinformatics,31(13):2214–2216,2015. Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell,A.,etal. Languagemodelsarefew-shotlearners.
Anderson,B.D. Reverse-timediffusionequationmodels. AdvancesinNeuralInformationProcessingSystems,33:
StochasticProcessesandtheirApplications,12(3):313– 1877–1901,2020.
326,1982.
Clevert, D.-A., Unterthiner, T., and Hochreiter, S. Fast
Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van andaccuratedeepnetworklearningbyexponentiallinear
DenBerg,R. Structureddenoisingdiffusionmodelsin units(elus). arXivpreprintarXiv:1511.07289,2015.
discrete state-spaces. Advances in Neural Information
ProcessingSystems,34:17981–17993,2021. Cock,P.J.,Antao,T.,Chang,J.T.,Chapman,B.A.,Cox,
C. J., Dalke, A., Friedberg, I., Hamelryck, T., Kauff,
Baek,J.,Kang,M.,andHwang,S.J. Accuratelearningof F., Wilczynski, B., et al. Biopython: freely available
graphrepresentationswithgraphmultisetpooling. arXiv python tools for computational molecular biology and
preprintarXiv:2102.11533,2021. bioinformatics. Bioinformatics,25(11):1422,2009.
Bajusz, D., Ra´cz, A., andHe´berger, K. Whyistanimoto Corso, G., Sta¨rk, H., Jing, B., Barzilay, R., andJaakkola,
indexanappropriatechoiceforfingerprint-basedsimilar- T. S. Diffdock: Diffusion steps, twists, and turns for
itycalculations? JournalofCheminformatics,7(1):1–13, moleculardocking. InTheEleventhInternationalConfer-
2015. enceonLearningRepresentations,2023. URLhttps:
//openreview.net/forum?id=kKF8 K-mBbS.
Band, N., Rudner, T. G. J., Feng, Q., Filos, A., Nado,
Z., Dusenberry, M. W., Jerfel, G., Tran, D., and Gal, DeBortoli,V.,Mathieu,E.,Hutchinson,M.,Thornton,J.,
Y. Benchmarking Bayesian deep learning on diabetic Teh, Y. W., and Doucet, A. Riemannian score-based
retinopathydetectiontasks. InAdvancesinNeuralInfor- generativemodelling. AdvancesinNeuralInformation
mationProcessingSystems34,2021. ProcessingSystems,35:2406–2422,2022.
10Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K. Bert: Go´mez-Bombarelli, R., Wei, J. N., Duvenaud, D.,
Pre-training of deep bidirectional transformers for lan- Herna´ndez-Lobato,J.M.,Sa´nchez-Lengeling,B.,She-
guageunderstanding. arXivpreprintarXiv:1810.04805, berla,D.,Aguilera-Iparraguirre,J.,Hirzel,T.D.,Adams,
2018. R. P., and Aspuru-Guzik, A. Automatic chemical de-
sign using a data-driven continuous representation of
Dhariwal, P. and Nichol, A. Diffusion models beat gans molecules. ACSCentralScience,4(2):268–276,2018.
on image synthesis. Advances in Neural Information
ProcessingSystems,34:8780–8794,2021. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explain-
ingandharnessingadversarialexamples. arXivpreprint
Dunbar, J. and Deane, C. M. Anarci: antigen receptor arXiv:1412.6572,2014.
numbering and receptor classification. Bioinformatics,
Gruver,N.,Stanton,S.,Frey,N.C.,Rudner,T.G.,Hotzel,
32(2):298–300,2016.
I.,Lafrance-Vanasse,J.,Rajpal,A.,Cho,K.,andWilson,
Dunbar,J.,Krawczyk,K.,Leem,J.,Baker,T.,Fuchs,A., A.G.Proteindesignwithguideddiscretediffusion.arXiv
Georges, G., Shi, J., and Deane, C. M. Sabdab: the preprintarXiv:2305.20009,2023.
structuralantibodydatabase. NucleicAcidsResearch,42
Guan, J., Zhou, X., Yang, Y., Bao, Y., Peng, J., Ma, J.,
(D1):D1140–D1146,2014.
Liu,Q.,Wang,L.,andGu,Q. DecompDiff: Diffusion
Ertl,P. Cheminformaticsanalysisoforganicsubstituents: modelswithdecomposedpriorsforstructure-baseddrug
identificationofthemostcommonsubstituents,calcula- design. In Krause, A., Brunskill, E., Cho, K., Engel-
tionofsubstituentproperties,andautomaticidentification hardt, B., Sabato, S., and Scarlett, J. (eds.), Proceed-
ofdrug-likebioisostericgroups. JournalofChemicalIn- ings of the 40th International Conference on Machine
formationandComputerSciences,43(2):374–380,2003. Learning,volume202ofProceedingsofMachineLearn-
ingResearch,pp.11827–11846.PMLR,23–29Jul2023.
Ertl,P.andSchuffenhauer,A. Estimationofsyntheticacces- URLhttps://proceedings.mlr.press/v202/
sibilityscoreofdrug-likemoleculesbasedonmolecular guan23a.html.
complexityandfragmentcontributions. JournalofChem-
Harris, C. R., Millman, K. J., van der Walt, S. J., Gom-
informatics,1:1–11,2009.
mers,R.,Virtanen,P.,Cournapeau,D.,Wieser,E.,Taylor,
Ferruz,N.,Schmidt,S.,andHo¨cker,B. Protgpt2isadeep J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer,
unsupervisedlanguagemodelforproteindesign. Nature S., van Kerkwijk, M. H., Brett, M., Haldane, A., del
Communications,13(1):4348,2022. R´ıo, J. F., Wiebe, M., Peterson, P., Ge´rard-Marchant,
P.,Sheppard,K.,Reddy,T.,Weckesser,W.,Abbasi,H.,
Fishman, N., Klarner, L., Bortoli, V. D., Mathieu, E., Gohlke, C., and Oliphant, T. E. Array programming
and Hutchinson, M. J. Diffusion models for con- with NumPy. Nature, 585(7825):357–362, September
strained domains. Transactions on Machine Learning 2020. doi: 10.1038/s41586-020-2649-2. URLhttps:
Research, 2023a. ISSN 2835-8856. URL https:// //doi.org/10.1038/s41586-020-2649-2.
openreview.net/forum?id=xuWTFQ4VGO. Ex-
Haussmann,U.G.andPardoux,E. Timereversalofdiffu-
pertCertification.
sions. TheAnnalsofProbability,pp.1188–1205,1986.
Fishman,N.,Klarner,L.,Mathieu,E.,Hutchinson,M.J.,
Ho,J.andSalimans,T. Classifier-freediffusionguidance.
andBortoli,V.D. Metropolissamplingforconstrained
arXivpreprintarXiv:2207.12598,2022.
diffusionmodels.InThirty-seventhConferenceonNeural
InformationProcessingSystems,2023b. URLhttps:
Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusionproba-
//openreview.net/forum?id=jzseUq55eP.
bilisticmodels. AdvancesinNeuralInformationProcess-
ingSystems,33:6840–6851,2020.
Ganin,Y.,Ustinova,E.,Ajakan,H.,Germain,P.,Larochelle,
H.,Laviolette,F.,March,M.,andLempitsky,V. Domain- Hoogeboom,E.,Satorras,V.G.,Vignac,C.,andWelling,
adversarialtrainingofneuralnetworks. JournalofMa- M. Equivariantdiffusionformoleculegenerationin3d.
chineLearningResearch,17(59):1–35,2016. In International Conference on Machine Learning, pp.
8867–8887.PMLR,2022.
Gaulton,A.,Hersey,A.,Nowotka,M.,Bento,A.P.,Cham-
bers, J., Mendez, D., Mutowo, P., Atkinson, F., Bellis, Huang, C.-W., Aghajohari, M., Bose, J., Panangaden, P.,
L. J., Cibria´n-Uhalte, E., et al. The chembl database andCourville,A.C. Riemanniandiffusionmodels. Ad-
in2017. NucleicAcidsResearch,45(D1):D945–D954, vances in Neural Information Processing Systems, 35:
2017. 2750–2761,2022.
11Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
Hunter,J.D. Matplotlib: A2dgraphicsenvironment. Com- Kingma,D.P.andBa,J. Adam: Amethodforstochastic
putinginScience&Engineering,9(3):90–95,2007. doi: optimization. arXivpreprintarXiv:1412.6980,2014.
10.1109/MCSE.2007.55.
Kingma, D. P., Mohamed, S., Jimenez Rezende, D., and
Hyva¨rinen,A.andDayan,P. Estimationofnon-normalized Welling,M. Semi-supervisedlearningwithdeepgenera-
statisticalmodelsbyscorematching. JournalofMachine tivemodels. AdvancesinNeuralInformationProcessing
LearningResearch,6(4),2005. Systems,27,2014.
Ingraham,J.B.,Baranov,M.,Costello,Z.,Barber,K.W.,
Kipf, T. N. and Welling, M. Semi-supervised classifica-
Wang, W., Ismail, A., Frappier, V., Lord, D. M., Ng-
tionwithgraphconvolutionalnetworks. arXivpreprint
Thow-Hing, C., Van Vlack, E. R., et al. Illuminating
arXiv:1609.02907,2016.
protein space with a programmable generative model.
Nature,pp.1–9,2023.
Klarner, L., Rudner, T. G., Reutlinger, M., Schindler, T.,
Morris,G.M.,Deane,C.,andTeh,Y.W. Drugdiscovery
Irwin, J.J., Sterling, T., Mysinger, M.M., Bolstad, E.S.,
undercovariateshiftwithdomain-informedpriordistri-
andColeman,R.G. ZINC:afreetooltodiscoverchem-
butionsoverfunctions. InInternationalConferenceon
istryforbiology. JournalofChemicalInformationand
MachineLearning,pp.17176–17197.PMLR,2023.
Modeling,52(7):1757–1768,2012.
Isert, C., Atz, K., Jime´nez-Luna, J., and Schneider, G. Koh,P.W.,Sagawa,S.,Marklund,H.,Xie,S.M.,Zhang,
Qmugs, quantum mechanical properties of drug-like M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips,
molecules. ScientificData,9(1):273,2022. R. L., Gao, I., et al. Wilds: A benchmark of in-the-
wilddistributionshifts. InInternationalConferenceon
Ji,Y.,Zhang,L.,Wu,J.,Wu,B.,Li,L.,Huang,L.-K.,Xu, MachineLearning,pp.5637–5664.PMLR,2021.
T., Rong, Y., Ren, J., Xue, D., etal. Drugood: Out-of-
distributiondatasetcuratorandbenchmarkforai-aided Laine, S. and Aila, T. Temporal ensembling for semi-
drugdiscovery–afocusonaffinitypredictionproblems supervisedlearning. arXivpreprintarXiv:1610.02242,
withnoiseannotations. InProceedingsoftheAAAICon- 2016.
ferenceonArtificialIntelligence,volume37,pp.8023–
8031,2023. Lakshminarayanan,B.,Pritzel,A.,andBlundell,C. Simple
andscalablepredictiveuncertaintyestimationusingdeep
Jing,B.,Corso,G.,Chang,J.,Barzilay,R.,andJaakkola,
ensembles. AdvancesinNeuralInformationProcessing
T.S. Torsionaldiffusionformolecularconformergenera-
Systems,30,2017.
tion. InOh,A.H.,Agarwal,A.,Belgrave,D.,andCho,
K. (eds.), Advances in Neural Information Processing Landrum,G.etal.Rdkit:Asoftwaresuiteforcheminformat-
Systems, 2022. URL https://openreview.net/ ics, computational chemistry, and predictive modeling.
forum?id=w6fj2r62r H. GregLandrum,8:31,2013.
Jo, J., Lee, S., and Hwang, S. J. Score-based generative
Lee,S.,Jo,J.,andHwang,S.J. Exploringchemicalspace
modelingofgraphsviathesystemofstochasticdifferen-
withscore-basedout-of-distributiongeneration. InInter-
tialequations. InInternationalConferenceonMachine
nationalConferenceonMachineLearning,pp.18872–
Learning,pp.10362–10383.PMLR,2022.
18892.PMLR,2023.
Kadkhodaie,Z.,Guth,F.,Simoncelli,E.P.,andMallat,S.
Li,R.,Jiao,Q.,Cao,W.,Wong,H.-S.,andWu,S. Model
Generalizationindiffusionmodelsarisesfromgeometry-
adaptation: Unsupervised domain adaptation without
adaptiveharmonicrepresentations. InTheTwelfthInter-
source data. In Proceedings of the IEEE/CVF Confer-
nationalConferenceonLearningRepresentations,2024.
ence on Computer Vision and Pattern Recognition, pp.
Kang, G., Jiang, L., Yang, Y., and Hauptmann, A. G. 9641–9650,2020.
Contrastiveadaptationnetworkforunsuperviseddomain
adaptation. In Proceedings of the IEEE/CVF Confer- Liu,J.,Lin,Z.,Padhy,S.,Tran,D.,BedraxWeiss,T.,and
ence on Computer Vision and Pattern Recognition, pp. Lakshminarayanan,B. Simpleandprincipleduncertainty
4893–4902,2019. estimationwithdeterministicdeeplearningviadistance
awareness. AdvancesinNeuralInformationProcessing
Karras,T.,Aittala,M.,Aila,T.,andLaine,S. Elucidating Systems,33:7498–7512,2020.
the design space of diffusion-based generative models.
AdvancesinNeuralInformationProcessingSystems,35: Loshchilov,I.andHutter,F. Decoupledweightdecayregu-
26565–26577,2022. larization. arXivpreprintarXiv:1711.05101,2017.
12Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
Lou, A. and Ermon, S. Reflected diffusion models. In Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V.,
Krause,A.,Brunskill,E.,Cho,K.,Engelhardt,B.,Sabato, Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P.,
S., and Scarlett, J. (eds.), Proceedings of the 40th In- Weiss,R.,Dubourg,V.,Vanderplas,J.,Passos,A.,Cour-
ternational Conference on Machine Learning, volume napeau,D.,Brucher,M.,Perrot,M.,andDuchesnay,E.
202ofProceedingsofMachineLearningResearch,pp. Scikit-learn: Machine learning in Python. Journal of
22675–22701.PMLR,23–29Jul2023. URLhttps:// MachineLearningResearch,12:2825–2830,2011.
proceedings.mlr.press/v202/lou23a.html.
Peng,X.,Guan,J.,Liu,Q.,andMa,J. MolDiff:Addressing
Marsland,S. Machinelearning: analgorithmicperspective. the atom-bond inconsistency problem in 3D molecule
ChapmanandHall/CRC,2011. diffusiongeneration. InKrause,A.,Brunskill,E.,Cho,
K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.),
MaynardSmith,J. Naturalselectionandtheconceptofa
Proceedingsofthe40thInternationalConferenceonMa-
proteinspace. Nature,225(5232):563–564,1970.
chineLearning,volume202ofProceedingsofMachine
LearningResearch,pp.27611–27629.PMLR,23–29Jul
McInnes,L.,Healy,J.,andMelville,J. UMAP:Uniform
2023. URL https://proceedings.mlr.press/
manifold approximation and projection for dimension
v202/peng23b.html.
reduction. arXivpreprintarXiv:1802.03426,2018.
Ramesh,A.,Dhariwal,P.,Nichol,A.,Chu,C.,andChen,M.
Mysinger, M.M., Carchia, M., Irwin, J.J., andShoichet,
Hierarchicaltext-conditionalimagegenerationwithclip
B.K. Directoryofusefuldecoys,enhanced(dud-e): bet-
terligandsanddecoysforbetterbenchmarking. Journal latents. arXivpreprintarXiv:2204.06125,1(2):3,2022.
ofMedicinalChemistry,55(14):6582–6594,2012.
Rogers, D. and Hahn, M. Extended-connectivity finger-
Nair,V.andHinton,G.E. Rectifiedlinearunitsimprove prints. JournalofChemicalInformationandModeling,
restrictedboltzmannmachines.InProceedingsofthe27th 50(5):742–754,2010.
InternationalConferenceonMachineLearning(ICML-
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
10),pp.807–814,2010.
Ommer,B. High-resolutionimagesynthesiswithlatent
Nichol,A.,Dhariwal,P.,Ramesh,A.,Shyam,P.,Mishkin, diffusionmodels. InProceedingsoftheIEEE/CVFcon-
P., McGrew, B., Sutskever, I., and Chen, M. Glide: ferenceoncomputervisionandpatternrecognition,pp.
Towards photorealistic image generation and editing 10684–10695,2022.
with text-guided diffusion models. arXiv preprint
Rudner,T.G.,Chen,Z.,Teh,Y.W.,andGal,Y. Tractable
arXiv:2112.10741,2021.
function-spacevariationalinferenceinBayesianneural
Nichol,A.Q.andDhariwal,P.Improveddenoisingdiffusion networks. AdvancesinNeuralInformationProcessing
probabilistic models. In International Conference on Systems,35:22686–22698,2022.
MachineLearning,pp.8162–8171.PMLR,2021.
Rudner, T. G., Kapoor, S., Qiu, S., and Wilson, A. G.
Olsen, T.H., Boyles, F., andDeane, C.M. Observedan- Function-spaceregularizationinneuralnetworks:Aprob-
tibodyspace: Adiversedatabaseofcleaned,annotated, abilisticperspective. InInternationalConferenceonMa-
andtranslatedunpairedandpairedantibodysequences. chineLearning,pp.29275–29290.PMLR,2023.
ProteinScience,31(1):141–146,2022.
Rudner,T.G.J.,Lu,C.,Osborne,M.A.,Gal,Y.,andTeh,
Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D., Y.W. OnpathologiesinKL-regularizedreinforcement
Nowozin, S., Dillon, J., Lakshminarayanan, B., and learning from expert demonstrations. In Advances in
Snoek,J. Canyoutrustyourmodel’suncertainty? evalu- NeuralInformationProcessingSystems34,2021.
atingpredictiveuncertaintyunderdatasetshift. Advances
Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,
inNeuralInformationProcessingSystems,32,2019.
E.L.,Ghasemipour,K.,GontijoLopes,R.,KaragolAyan,
pandasdevelopmentteam,T. pandas-dev/pandas: Pandas, B.,Salimans,T.,etal. Photorealistictext-to-imagedif-
February2020. URLhttps://doi.org/10.5281/ fusion models with deep language understanding. Ad-
zenodo.3509134. vances in Neural Information Processing Systems, 35:
36479–36494,2022.
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
Chanan,G.,Killeen,T.,Lin,Z.,Gimelshein,N.,Antiga, Sanchez-Lengeling, B. and Aspuru-Guzik, A. Inverse
L.,etal. Pytorch: Animperativestyle,high-performance molecular design using machine learning: Generative
deeplearninglibrary. AdvancesinNeuralInformation modelsformatterengineering. Science,361(6400):360–
ProcessingSystems,32,2019. 365,2018.
13Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
Satorras, V. G., Hoogeboom, E., and Welling, M. E (n) Urbina,F.,Lentzos,F.,Invernizzi,C.,andEkins,S.Dualuse
equivariantgraphneuralnetworks. InInternationalCon- ofartificial-intelligence-powereddrugdiscovery. Nature
ference on Machine Learning, pp. 9323–9332. PMLR, MachineIntelligence,4(3):189–191,2022.
2021.
VanAmersfoort,J.,Smith,L.,Teh,Y.W.,andGal,Y. Un-
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and certainty estimation using a single deep deterministic
Ganguli,S. Deepunsupervisedlearningusingnonequi- neuralnetwork. InInternationalConferenceonMachine
libriumthermodynamics. InInternationalConferenceon Learning,pp.9690–9700.PMLR,2020.
MachineLearning,pp.2256–2265.PMLR,2015.
Van Rossum, G. and Drake, F. L. Python 3 Reference
Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Er- Manual. CreateSpace, ScottsValley, CA,2009. ISBN
mon,S.,andPoole,B. Score-basedgenerativemodeling 1441412697.
throughstochasticdifferentialequations. arXivpreprint
Virtanen,P.,Gommers,R.,Oliphant,T.E.,Haberland,M.,
arXiv:2011.13456,2020.
Reddy, T., Cournapeau, D., Burovski, E., Peterson, P.,
Weckesser,W.,Bright,J.,vanderWalt,S.J.,Brett,M.,
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,
Wilson,J.,Millman,K.J.,Mayorov,N.,Nelson,A.R.J.,
andSalakhutdinov,R. Dropout: asimplewaytoprevent
Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, ˙I.,
neuralnetworksfromoverfitting.TheJournalofMachine
Feng, Y., Moore, E. W., VanderPlas, J., Laxalde, D.,
LearningResearch,15(1):1929–1958,2014.
Perktold,J.,Cimrman,R.,Henriksen,I.,Quintero,E.A.,
Stiefl, N., Watson, I. A., Baumann, K., and Zaliani, A. Harris,C.R.,Archibald,A.M.,Ribeiro,A.H.,Pedregosa,
Erg:2dpharmacophoredescriptionsforscaffoldhopping. F.,vanMulbregt,P.,andSciPy1.0Contributors. SciPy
JournalofChemicalInformationandModeling, 46(1): 1.0: FundamentalAlgorithmsforScientificComputing
208–220,2006. in Python. Nature Methods, 17:261–272, 2020. doi:
10.1038/s41592-019-0686-2.
Sun,B.andSaenko,K. Deepcoral: Correlationalignment
fordeepdomainadaptation. InComputerVision–ECCV Wahab,A.,Pfuderer,L.,Paenurk,E.,andGershoni-Poranne,
2016Workshops: Amsterdam,TheNetherlands,October R. The compas project: A computational database of
8-10and15-16,2016,Proceedings,PartIII14,pp.443– polycyclic aromatic systems. phase 1: cata-condensed
polybenzenoidhydrocarbons. JournalofChemicalInfor-
450.Springer,2016.
mationandModeling,62(16):3704–3713,2022.
Szegedy,C.,Zaremba,W.,Sutskever,I.,Bruna,J.,Erhan,
Wang, H., Fu, T., Du, Y., Gao, W., Huang, K., Liu, Z.,
D.,Goodfellow,I.,andFergus,R.Intriguingpropertiesof
Chandak, P., Liu, S., Van Katwyk, P., Deac, A., et al.
neuralnetworks. arXivpreprintarXiv:1312.6199,2013.
Scientific discovery in the age of artificial intelligence.
Tingle, B. I., Tang, K. G., Castanon, M., Gutierrez, J. J., Nature,620(7972):47–60,2023.
Khurelbaatar,M.,Dandarchuluun,C.,Moroz,Y.S.,and
Waskom, M. L. seaborn: statistical data visualization.
Irwin,J.J. ZINC-22-afreemulti-billion-scaledatabase
Journal of Open Source Software, 6(60):3021, 2021.
oftangiblecompoundsforliganddiscovery. Journalof
doi: 10.21105/joss.03021. URLhttps://doi.org/
ChemicalInformationandModeling,63(4):1166–1176,
10.21105/joss.03021.
2023.
Watson,J.L.,Juergens,D.,Bennett,N.R.,Trippe,B.L.,
Tran,D.,Liu,J.,Dusenberry,M.W.,Phan,D.,Collier,M.,
Yim,J.,Eisenach,H.E.,Ahern,W.,Borst,A.J.,Ragotte,
Ren,J.,Han,K.,Wang,Z.,Mariet,Z.,Hu,H.,Band,N.,
R.J.,Milles,L.F.,etal. Denovodesignofproteinstruc-
Rudner,T.G.J.,Singhal,K.,Nado,Z.,vanAmersfoort,J.,
ture and function with rfdiffusion. Nature, 620(7976):
Kirsch,A.,Jenatton,R.,Thain,N.,Yuan,H.,Buchanan,
1089–1100,2023.
K., Murphy, K., Sculley, D., Gal, Y., Ghahramani, Z.,
Snoek, J., and Lakshminarayanan, B. Plex: Towards Weiss, T., Mayo Yanes, E., Chakraborty, S., Cosmo, L.,
reliability using pretrained large model extensions. In Bronstein, A. M., and Gershoni-Poranne, R. Guided
ICMLWorkshoponPre-training: Perspectives,Pitfalls, diffusionforinversemoleculardesign. NatureComputa-
andPathsForward,2022. tionalScience,3(10):873–882,2023.
Tzeng, E., Hoffman, J., Darrell, T., and Saenko, K. Si- Xu,M.,Powers,A.S.,Dror,R.O.,Ermon,S.,andLeskovec,
multaneousdeeptransferacrossdomainsandtasks. In J. Geometriclatentdiffusionmodelsfor3dmoleculegen-
Proceedings of the IEEE International Conference on eration. InInternationalConferenceonMachineLearn-
ComputerVision,pp.4068–4076,2015. ing,pp.38592–38610.PMLR,2023.
14Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
Zaidi, S., Schaarschmidt, M., Martens, J., Kim, H., Teh,
Y.W.,Sanchez-Gonzalez,A.,Battaglia,P.,Pascanu,R.,
and Godwin, J. Pre-training via denoising for molecu-
lar property prediction. In The Eleventh International
ConferenceonLearningRepresentations,2023.
Zhang,C.,Bengio,S.,Hardt,M.,Recht,B.,andVinyals,O.
Understanding deep learning (still) requires rethinking
generalization. CommunicationsoftheACM,64(3):107–
115,2021.
Zhang,L.,Rao,A.,andAgrawala,M. Addingconditional
controltotext-to-imagediffusionmodels. InProceedings
oftheIEEE/CVFInternationalConferenceonComputer
Vision,pp.3836–3847,2023.
15Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
Appendix
TableofContents
A GuidanceFunctionParameterization 17
A.1 ProbabilisticInterpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B ExperimentalDetails 18
B.1 IllustratingtheBehaviourofGuidedDiffusionModelsontheSwissRollDataset . . . . . . . . . . . . . 18
B.2 Graph-StructuredDiffusionforSmallMolecules . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.3 EquivariantDiffusionforMaterials. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
B.4 DiscreteDiffusionforProteinSequences. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
B.5 RuntimeComparison. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
B.6 SimilarityoftheGeneratedSamplestotheTrainingandValidationSets. . . . . . . . . . . . . . . . . . . 38
Reproducibility
Alldataandcodeneededtoreproduceourexperimentscanbeaccessedat:
https://github.com/leojklarner/context-guided-diffusion
16Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
A.GuidanceFunctionParameterization
Toguidesamplinginadiffusionmodeltowardcertaindesiredproperties,aguidancemodelf (·;θ)istrainedonadatasetD,
t
aparametricguidancefunctionp(y | x ;f (·;θ))isdefinedusingthelearnedguidancemodel,andthentheconditional
t t
gradient/scorefunction
∇logp(y | x ;f (·;θ))+∇logp(x ), (A.1)
t t t
isusedtoguidegenerationtowardshigh-densityregionsp(y | x ;f1(·;θ)). Toprovideageneralframeworkthataccommo-
t t
datesbothclassificationandregressionsettings,weletYdenotean“optimality”randomvariableindicatingtheoptimality
(undersomedomain-specificoptimalityfunction)ofagiveninputx .
t
Fordiscretedesirableproperties,theguidancemodelf (·;θ)(denotingthelogitsforagivennumberofclasses)istrainedon
t
aclassificationtask,andtheguidancefunctionp(y | x ;f (·;θ))foraspecificclassyisdefinedasacategoricallikelihood
t t
functionevaluatedaty,making−logp(y | x ;f (·;θ))across-entropyloss. Thescorefunctionthenbecomestheclass
t t
probabilityofthedesiredclassunderthelearnedmodel:
∇logp(y | x ;f1(·;θ))=∇logsoftmax(f1(x |θ)) . (A.2)
t t t c
Forcontinuousdesirableproperties,theguidancemodel(withameanandavariancehead)istrainedonaregressiontask,
andtheguidancefunctionp(y | x ;f (·;θ))isdefinedasaBernoullilikelihoodfunctionwith
t t
p(y=1|x ,C;f (·;θ))=exp(f1(x ;θ)−C), (A.3)
t t t t
givingtheprobabilityofoptimalityasafunctionofthemean-headoutputf1(x ;θ),andC representingadomain-specific
t t
“goalvalue”. Importantly,thisformulationrequiresthatf1(x ;θ) < C sothatincreasesinf1(x ;θ))indicatedesirable
t t t t
properties.
A.1.ProbabilisticInterpretation
Lettingybean“optimality”randomvariableindicatingwhetherpredictionsf (xˆ ;θ)areoptimalforachievingadesired
t t
behavior (e.g., high predictive uncertainty on data points far away from the training data), we can define a Bernoulli
observationmodelwhere
p(y=1|θ,t;f ,p )=exp(−R(θ,f ,t,p )), (A.4)
t Xˆ
t
t Xˆ
t
denotestheprobabilityofoptimalityasafunctionoftheregularizer. Thatis,thesmallerR(θ,f ,t,p ),thehigherthe
t Xˆ
t
probabilitythatapredictionf (xˆ ;θ)isoptimal.
t t
FollowingtheresultinRudneretal.(2023),wenotethatspecifyingadata-dependentregularizeroftheformshownin
Equation(6)correspondstospecifyingadata-drivenpriordistributionoverneuralnetworkparameters
p(y=1|θ,t;f ,p )p(θ)
p(θ|t,y=1;f ,p )=
t Xˆ
t .
t Xˆ t p(y=1|t;f t,p Xˆ t)
MinimizingtheobjectivefunctioninEquation(9)correspondstofindingavariationalapproximationtotheposteriorimplied
bythisdata-drivenpriorandtheobserveddata. Thatis,minimizingtheobjectivefunctioninEquation(9)(approximately)
correspondstofindingavariationaldistributionq =N(θ;θ′,σ2)(withfixedandverysmallσ2)thatsolvesthevariational
Θ
problem
minE (cid:2)D (q ||p )(cid:3) , (A.5)
qΘ
pT KL Θ Θ|DT,y,T
allowingustointerpretthelearnedguidancemodelthroughthelensofBayesianinference.
17Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
B.ExperimentalDetails
Thissectionprovidesadditionalinformationandexperimentalresultscomplementingthemaintext. AppendixB.1details
theregressionexperimentsfromSection3andcontrastsourapproachwithstandardL regularization,illustratedinFigure3.
2
AppendixB.2presentsourextensionofLeeetal.(2023)’sworkongraph-structureddiffusionmodelsforsmallmolecule
generation(Section5.1). AppendixB.3presentsourextensionofWeissetal.(2023)’sexperimentsonequivariantdiffusion
modelsforgeneratingnovelpolycyclicaromaticsystems(Section5.2). Finally,AppendixB.4presentsourextensionof
Gruveretal.(2023)’sapproachtooptimizingdiscreteproteinsequencesusingcategoricaldiffusionmodels(Section5.3).
Allcodewaswrittenin(VanRossum&Drake,2009)andcanbeaccessedathttps://github.com/leojklarner/context-guided-
diffusion. Arangeofcorescientificcomputinglibrarieswereusedfordatapreparationandanalysis,includingNUMPY
(Harrisetal.,2020), SCIPY (Virtanenetal.,2020), PANDAS (pandasdevelopmentteam,2020), MATPLOTLIB (Hunter,
2007),SEABORN(Waskom,2021),SCIKIT-LEARN(Pedregosaetal.,2011)andRDKIT(Landrumetal.,2013). Alldeep
learningmodelswereimplementedinPYTORCH(Paszkeetal.,2019).
B.1.IllustratingtheBehaviourofGuidedDiffusionModelsontheSwissRollDataset
Dataset. Inordertoillustratetheimpactofourregularizeronthelearningdynamicsofaguidancemodel,particularly
whenonlytrainedonaconstrainedandlessdiversesubsetoftheinputdomain,weconductacomparisonwithstandard
parameter-spaceregularizationschemes,namelyexplicitL regularization. ThiscomparisonisperformedontheSwiss
2
rolldataset,asdescribedbyMarsland(2011)andimplementedintheSCIKIT-LEARNlibrary(Pedregosaetal.,2011). The
inputdataX ∈ R2 andregressionlabelsy ∈ RaredepictedinFigure7. Athresholdofy = 1isusedtosplitthedata
t
intotrainingandvalidationsets. Thetrainingsetencompassesdatapointswithlowerlabelvalues,whilethevalidationset
consistsofdatapointswithhigherlabels. Thissplitisdesignedtoevaluatethecapabilityofaguideddiffusionmodeltrained
onasubsetoflower-valuedatatoextrapolatetoandgeneratedatapointsfromhigher-value,out-of-distributionregions.
2.0
1.5
1.5
1.0
1.0
0.5
0.5
0.0 0.0
0.5
0.5
1.0
1.0
1.5
1.5
2.0
2.0 1.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0
x1
Figure7: AvisualizationoftheSwissrolldatasetusedtotraindifferentguidancemodels. Itconsistsof500datapoints
generatedwiththecorrespondingfunctionintheSCIKIT-LEARNlibrary(Pedregosaetal.,2011)usinganoiseparameterof
0.3. BothcovariatesX ∈R2 andlabelsy∈Rhavebeennormalizedtoameanofzeroandastandarddeviationofone.
t
Thecolor-codedregressionlabelsarederivedfromthedatapoints’sequentialorderingalongthemanifold’sarclength. A
labelthresholdofy=1,denotedbyadashedline,showsthetraining-validationsplit: thetrainingdata(circles)contains
datapointswithlowerlabelvalues,whilethevalidationdata(crosses)consistsofdatapointswithhigherlabels. Thissplitis
designedtoevaluatetheabilityofaguideddiffusionmodeltrainedonasubsetoflower-valuedatatoextrapolatetoand
generatedatapointsfromhigher-value,out-of-distributionregions.
18
2xContext-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
DiffusionModelTraining. Totrainunconditionaldiffusionmodels,weadaptanexistingimplementationofthedenoising
diffusionprobabilisticmodelsfromHoetal.(2020)totraindiffusionmodelsondatasetsinR2(https://github.com/albarji/toy-
diffusion). Specifically,wegenerate100000datapointsfromthesamedata-generatingprocessasourlabeledtrainingset,
i.e. datapointsfromtheSwissrollgeneratorwithstandardizedlabelsy<1. Usingastandardcosinebetaschedule(Nichol
&Dhariwal,2021)
α¯ f(t)
(cid:18)
t/T +ϵ
π(cid:19)2
β =1− t , with α¯ = and f(t)=cos · (B.6)
t α¯ t f(0) 1+ϵ 2
t−1
whereϵ=0.008andT =40,weensurethatthenoisingprocessconvergestothetargetdistributionN(0,I). Thescore
networks :R2×[1,...,T]→R2 isconstructedasamulti-layerperceptronwith5hiddenlayersofdimension64and
θ
ReLUactivationfunctions(Nair&Hinton,2010). ItistrainedwiththeL noisepredictionobjectivefromHoetal.
simple
(2020)over100epochsofstochasticgradientdescent. Specifically,weusethe ADAM optimizer(Kingma&Ba,2014)
withabatchsizeof2048andalearningrateof1×10−3thatdropsoffto1×10−5followingalinearlearningratedecay
schedule. SamplesfromthisunconditionalmodelareshowninFigure8.
Guidance Function Training. For the training of guidance models fj(·;θ) : R2 ×[1,...,T] → R, we use smaller
t
multi-layerperceptronswith3hiddenlayersofdimension32,applyingsinactivationfunctionsanddropout(Srivastava
et al., 2014) with p = 0.2 after each layer. These networks are trained to output both the most likely regression label
µ (x ;θ) = f1(x ;θ), as well as log-variances logσ2(x ;θ) = f2(x ;θ) that serve as an estimator of their predictive
t t t t t t t t
uncertainty. Given the regression labels y, the models are optimized with respect to a negative log-likelihood loss
L
t
=−logN(y;µ t,σ t2I)over100epochsofstochasticgradientdescent,usingtheADAMoptimizerwithabatchsizeof
128andalearningrateof1×10−2. ThecorrespondingobjectiveLwithanexplicitL regularizationtermisgivenby
2
L(θ,D )=˙ E (cid:2) −logN(y;µ ,σ2I)(cid:3) + 1 ||θ||2, (B.7)
T pT T T 2λ 2
wheretheregularizationstrengthλisoptimizedasahyperparameter. Similarly,wetraincontext-guidedmodelsbyadding
theadditionalregularizationtermR(θ,f ,t,p )asintroducedinSection3andgivenby
t Xˆ
t
(cid:20) (cid:88)2 (cid:16) (cid:17)⊤ (cid:16) (cid:17)(cid:21)
R(θ,f ,t,p )=E fj(xˆ ;θ)−mj(xˆ ) K (xˆ )−1 fj(xˆ ;θ)−mj(xˆ ) , (B.8)
t Xˆ t p Xˆt j=1 t t t t t t t t t t
Here,xˆ ∼p (xˆ )representsacontextbatchsampledfromanout-of-distributioncontextsetcomprising10000points
t Xˆ
t
t
distributeduniformlyover[−2.5,2.5]2 ⊂R2. AsdetailedinSection3,weusearandomlyinitializedparametersetϕto
generate32-dimensionalcontextsetembeddingsh (xˆ ,ϕ)∈RM×32andusethemtoconstructthecovariancematrix
t t
K (xˆ )=σ h (xˆ ,ϕ)h (xˆ ,ϕ)T +τ I.
t t t t t t t t
Asthemodelreturnsmeanandlog-varianceestimatesf1(xˆ ;θ)andf2(xˆ ;θ),thetargetfunctionvaluesmj(xˆ )aregiven
t t t t t t
bythetrainingsetmeanm1(xˆ )=−0.38andalog-variancehyperparameterm2(xˆ )=σ2,respectively. Thelatterisset
t t t t 0
toσ =0.7,chosentoinducehighpredictiveuncertaintyestimatesofexp(σ2)≈2inout-of-distributionregionsofthe
0
inputdomain. Thevaluesofthecovariancescaleσ andthediagonaloffsetτ areoptimizedashyperparameters. While
t t
empiricalperformancetendstoimprovewhenσ andτ aremadetime-dependent,wefoundthatinthissettingwecan
t t
alreadyachievegoodperformancewhenkeepingthemconstant. Theresultingaugmentedtrainingobjectiveisgivenas
(cid:104) (cid:105)
L∗(θ,D)=E L(θ,D ,T)+R(θ,f ,T,p ) . (B.9)
pT T t Xˆ
T
Theoptimalregularizationhyperparameterswereselectedbyperforminggridsearchoverthehyperparametergridpresented
inTable2withrespecttothesupervisedlossLevaluatedontheheld-outvalidationset.
Table2: HyperparametersearchspaceforregularizationschemesofregressionmodelsdetailedinAppendixB.1.
Hyperparameter Description SearchSpace
λ L regularizationstrength 10−4,10−3,10−2,10−1,1,101,102,103,104
2
σ covariancescale 10−5,10−3,10−1,1,101,103,105
t
τ diagonaloffset 10−5,10−3,10−1,1,101,103,105
t
19Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
Results AcomparativeanalysisofmodelperformancesisprovidedinFigure8. WeobservethatbothL -regularized
2
andcontext-guidedmodelsfitthetrainingdatawell. However, withoutadditionalinformationabouttheinputdomain,
the predictions of the L -regularized model exhibit poor generalization and miscalibrated confidence estimates when
2
evaluatedinnew,unseenregions. Incontrast,thecontext-gudedregularizerisabletousethisadditionalinformationtosteer
guidancemodeltrainingtowardfunctionsthatseemtocapturetheunderlyingstructureoftheinputdomainmoreaccurately.
Moreimportantly,italsogeneratessignificantlybetter-calibrateduncertaintyestimates. Thisimprovementtranslatesinto
higher-qualitysampleswhenusinggradientsfromthecontext-guidedmodelforconditionalsampling.
Predicted Mean Predicted Log-Variance
0.9 0.9
Guidance Type Guidance Type
2 guided 0.6 2 guided 0.0
unguided 0.3 unguided
1 1 0.9
0.0
0.3 1.8
0 0
0.6 2.7
1 0.9 1
3.6
1.2
2 2 4.5
1.5
2 1 0 1 2 2 1 0 1 2
x x
1 1
(a) Predictionsandsamplesfromacontext-guidedmodel
Predicted Mean Predicted Log-Variance
1.2
Guidance Type Guidance Type
2 0.3 2 0.8
guided guided
unguided unguided 0.4
0.0
1 1
0.0
0.3
0.4
0 0
0.6 0.8
1.2
1 0.9 1
1.6
1.2
2 2 2.0
1.5 2.4
2 1 0 1 2 2 1 0 1 2
x x
1 1
(b)Predictionsandsamplesfromacontext-guidedguidancemodel.
Figure8: Comparativeanalysisofpredictions,uncertaintyestimates,andsamplesgeneratedbyguidancemodelstrained
withL regularizationandcontext-guidanceschemes. Thetrainingandvalidationsets, (seeFigure7), aredepictedby
2
color-coded and black crosses, respectively. We display 50 samples generated by both unconditional and conditional
models,depictedasgreenandbluecircles,respectively. Thesamplesaregeneratedwiththesamerandomseedandno
cherry-picking. Theleftpanelshowsthemodel’smeanpredictionsacrossabroaderinputspace[−2.5,2.5]2 ⊂R2,with
warmercolors(red)indicatinghigherpredictedvalues. Therightpanelshowsthemodel’slog-varianceestimatesacrossthe
sameinterval,servingasameasureofuncertainty,withdarkerregionsindicatingmoreuncertainpredictons. Weobserve
thatbothL -regularizedandcontext-guidedmodelsfitthetrainingdatawell. However,withoutadditionalinformationabout
2
theinputdomain,thepredictionsoftheL -regularizedmodelexhibitpoorgeneralizationandmiscalibratedconfidence
2
estimateswhenevaluatedinnew,unseenregions. Incontrast,ourcontext-guidedapproachisabletousethisadditional
informationtosteerguidancemodeltrainingtowardfunctionsthatseemtocapturetheunderlyingstructureoftheinput
domainmoreaccurately. Moreimportantly, italsogeneratessignificantlybetter-calibrateduncertaintyestimates. This
improvementtranslatesintohigher-qualitysampleswhenusinggradientsfromthecontext-guidedmodelforconditional
sampling.
20
x
x
2
2
x
x
2
2Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
B.2.Graph-StructuredDiffusionforSmallMolecules
Dataset. WeevaluateallmethodsonthesamedatasetasLeeetal.(2023), consistingof250000moleculessampled
uniformlyfromtheZINCdatabaseofcommerciallyavailablecompounds(Irwinetal.,2012). Eachmoleculeisannotated
withground-truthlabelsy,definedintermsof
• theclippedandnormalizednegativeQuickVinadockingscorey (Alhossaryetal.,2015),
vina
• thenormalizedsyntheticaccessibility(SA)scorey (Ertl,2003),and
SA
• thequantitativeestimatefordrug-likeness(QED)scorey (Bickertonetal.,2012).
QED
Theresultingcompositeobjective(forwhichlargervaluesindicatebetterperformance)isgivenby
clip(−y ,20,0) 10−y
y= vina · SA ·y ,with y∈[0,1]. (B.10)
20 9 QED
AstheQuickVinadockingscorey isafunctionofthespecificproteinpocketthatamoleculeisdockedinto,alldata
vina
preparationandmodeltrainingstepsdescribedinthissectionarerepeatedacrossfivedifferentproteinsfromtheDUD-E
dataset (Mysinger et al., 2012) selected by Lee et al. (2023), namely PARP1 (Poly [ADP-ribose] polymerase-1), FA7
(CoagulationfactorVII),5HT1B(5-hydroxytryptaminereceptor1B),BRAF(Serine/threonine-proteinkinaseB-raf),and
JAK2(Tyrosine-proteinkinaseJAK2). Allcompoundsarefeaturizedastopologicalmoleculargraphsg 0,definedastuples
g =(x ,a )ofnodefeaturesx ∈RM×F andadjacencymatricesa ∈RM×M,whereM =38isthemaximumnumber
0 0 0 0 0
of heavy atoms in the training set and F = 10 is the dimensionality of node features, denoting a one-hot encoding of
theelementtypesC,N,O,F,P,S,Cl,Br,I,andNONE. Differentbondtypesarelearnedbydiscretizingthecontinuous
adjacencymatrica
0
intobinscorrespondingtocertainbonds,namelya
i,j
< 0.5 → NONE,0.5 < a
i,j
< 1.5 → single,
1.5<a <2.5→double,a >2.5→triple. FollowingLeeetal.(2023),wethenorderthemoleculesbytheirlabelsy
i,j i,j
andsplitthemevenlyintoalow-valuetrainingsetandahigh-valuetestsetof125000datapointseach,repeatedforeach
ofthefiveproteintargets. Thisexperimentalsetupallowsustoevaluatetheabilityofguideddiffusionmodelstrainedon
moleculeswithlow-valuepropertiestoextrapolatetoandgeneratedatapointsfromhigher-value,out-of-distributionregions.
Togenerateacontextsetxˆ forourcontext-guideddiffusionmethod,anadditional500000unlabeledcompoundswere
C
sampleduniformlyfromtheZINCdatabaseandprocessedinthesamefashionasbefore. Acomparisonofthetrainingand
contextsetintermsofdifferentmolecularpropertiesderivedwithRDKIT(Landrumetal.,2013)ispresentedinFigure9.
labeled dataset
unlabeled context points
0.05 0.030
0.025
0.150
0.04 0.025
0.020 0.125
0.020
0.015 0.03 0.100
0.015
0.075
0.010 0.02
0.050 0.010
0.005 0.01 0.025 0.005
0.000 0.00 0.000 0.000
200 400 600 10 20 30 40 0 5 10 15 5 0 5 10
molecular weight number of atoms number of H-bond acceptors lipophilicity
0.30 0.035
0.12
0.25 0.20 0.030
0.10
0.20 0.025
0.08 0.15
0.020
0.15
0.06
0.10 0.015
0.10 0.04 0.010
0.05
0.05 0.02 0.005
0.00 0.00 0.00 0.000
0 5 10 0 10 20 0.0 2.5 5.0 7.5 0 100 200
number of H-bond donors number of rotable bonds number of aromatic rings polarizable surface area
Figure9: Comparisonofthemolecularpropertiesofthelabeledtrainingandunlabeledcontextsets,verifyingthatthelatter
consistsofmeaningfullyrelatedyetdistinctcompoundsandissuitableforusewithourregularizationscheme.
21
ytilibaborP
ytilibaborP
ytilibaborP
ytilibaborP
ytilibaborP
ytilibaborP
ytilibaborP
ytilibaborPContext-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
DiffusionModelsoverGraphs. TheMolecularOut-Of-distributionDiffusion(MOOD)frameworkintroducedinLee
etal.(2023)leveragesdiffusionmodelsdefinedoverthespaceofmoleculargraphs. Specifically,theyemploytheGraph
DiffusionviatheSystemofStochasticdifferentialequations(GDSS)formalismofJoetal.(2022),whichwerestatein
thefollowing. Inthisframework,amoleculargraphg isdefinedasatupleg =(x ,a )ofnodefeaturesx ∈RM×F
0 0 0 0 0
andadjacencymatricesa ∈RM×M,whereM isthemaximumnumberofheavyatomsinthetrainingsetandF isthe
0
dimensionality of the node features. Following the formalism of Song et al. (2020) outlined in Section 2, the forward
diffusionprocess{G =(X ,A )}T isgivenby
t t t t=0
dG =f(G ,t)dt+β dB (B.11)
t t t t
anddecomposesintoasystemofstochasticdifferentialequationsthatcanbemodeledjointly
dX =f (X ,t)dt+β dB
t X t X,t t
(B.12)
dA =f (A ,t)dt+β dB ,
t A t A,t t
wheref(G ,t)=[f (X ,t),f (A ,t)]andβ =[β ,β ]. Thecorrespondingtimereversal{G¯ =(X¯ ,A¯ )} =
t X t A t t X,t A,t t t t t≥0
{G =(X ,A )} isgivenby
T−t T−t T−t t∈[0,T]
dG¯ =(cid:8) −f(G¯ ,T −t)+β2 ∇ logp (cid:0) G¯ (cid:1)(cid:9) dt+β dB , (B.13)
t t T−t G T−t t T−t t
flowingbackwardintimeanddecomposingintoacorrespondingjointsystemofSDEsgivenby
dX¯ =(cid:8) −f (X¯ ,T −t)+β2 ∇ logp (cid:0) X¯ ,A¯ (cid:1)(cid:9) dt+β dB
t X t X,T−t X T−t t t X,T−t t
(B.14)
dA¯ =(cid:8) −f (A¯ ,T −t)+β2 ∇ logp (cid:0) X¯ ,A¯ (cid:1)(cid:9) dt+β dB ,
t A t A,T−t A T−t t t A,T−t t
where∇ logp (·)and∇ logp (·)thatareapproximatedbyseparatescorenetworkss (X ,A ,t)ands (X ,A ,t).
X t A t θX t t θA t t
Toencouragetheexplorationofnovelchemicalspace,Leeetal.(2023)conditionthedenoisingprocessonahyperparameter
γ ∈[0,1]thatshrinkstheunconditionalgradientstoexplorenearout-of-distributionregionsaroundthetrainingdistribution
∇ logp (G |γ)=∇ logp (G )+∇ logp (γ |G )
G t t G t t G t t
(cid:16) √ (cid:17)
=∇ logp (G )+∇ log p (G )− γ (B.15)
G t t G t t
√
=(1− γ)∇ logp (G ).
G t t
√
Inallofourexperiments,wefollowLeeetal.(2023)andset γ =0.2. Toadditionallyconditionthedenoisingprocesson
desirablemolecularproperties,Leeetal.(2023)makeuseofstandardguideddiffusionasoutlinedinSection2,specifiedas
∇ logp (G |γ,y)=∇ logp (G )+∇ logp (γ |G )+∇ logp (y|G )
G t t G t t G t t G t t
√ (B.16)
=(1− γ)∇ logp (G )+∇ logp (y|G ).
G t t G t t
Here,aregressionmodelfj(g ,θ)isusedtoapproximatetheconditionaldistributionp (y|g )≈exp(α f1(g ,θ))/Z ,
t t t t t t t t
whereα isanadaptiveguidancescalebasedonahyperparameterr andgivenby
t G,0
∥s (G ,t)∥
α =r θ t 2 with r =0.1tr (B.17)
G,t G,t∇ log∥β (G ,t)∥ G,t G,0
G θ t 2
Combining the decomposition into a system of joint SDEs from Equation (B.14), the shrinking of the unconditional
gradientsfromEquation(B.15)andtheguidancemodelformulationfromEquation(B.17),thefullconditionalsample
generationprocessusedbyLeeetal.(2023)isgivenby
(cid:26) (cid:27)
dX¯ =β2 1 X¯ +(1−√ γ)s (cid:0) X¯ ,A¯ ,T −t(cid:1) +∇ logexp(cid:0) α f (cid:0) X¯ ,A¯ ;θ(cid:1)(cid:1) dt+β dB
t X,T−t 2 t θX t t X X,T−t T−t t t X,T−t t
dA¯ =β2 (cid:8) (1−√ γ)s (cid:0) X¯ ,A¯ ,T −t(cid:1) +∇ logexp(cid:0) α f (cid:0) X¯ ,A¯ ;θ(cid:1)(cid:1)(cid:9) dt+β dB
t A,T−t θA t t A A,T−t T−t t t A,T−t t
(B.18)
wherethedriftanddiffusioncoefficientsofdX¯ anddA¯ aredeterminedbythevariance-preserving(VP)andvariance-
t t
exploding(VE)SDEfromSongetal.(2020),respectively.
22Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
DiffusionModelTraining. AsinLeeetal.(2023),weadoptthepre-trainednodefeatureandadjacencymatrixscore
networkss (x ,a ,t)ands (x ,a ,t)fromJoetal.(2022)todefineourunconditionaldiffusionmodel. Thesenetworks
θX t t θA t t
arebuiltongraphmulti-headattention(GMH)architectures(Baeketal.,2021)toensurepermutation-equivariance. Each
network consists of two graph attention layers with four attention heads of hidden dimension 16 and tanh activation
functions,followedbyamulti-layerperceptronoutputheadwiththreelayers,eachofdimension82andeLUactivation
functions(Clevertetal.,2015). ThenodefeatureSDEisbasedonthevariance-preservingSDEfromSongetal.(2020),
usingalinearβ-schedule(β =0.1,β =1). Incontrast,theadjacencymatrixSDEusesthevariance-explodingSDE
X,0 X,T
alsofromSongetal.(2020),withanexponentialβ-schedule(β = 0.2,β = 1). BothmodelsemployT = 1000
A,0 A,T
discretizationsteps. It’simportanttonotethattheunconditionalscorenetworksfrom(Joetal.,2022)weretrainedonani.i.d
85%-15%training-validationsplitofourdataset,meaningthattheymayhavebeenoptimizedusingpartofthehigh-property
validationsetalsousedfortrainingotherguidancemodels. However,thisisnotanissueas(a)theunconditionaldiffusion
modelwastrainedwithoutaccesstolabelinfomration,(b)weonlyusethevalidationsetforhyperparameteroptimization,
and (c) the vast search space ensures that most generated compounds are neither in the training nor the validation set
(seeAppendixB.6foradetailedanalysis).
GuidanceModelTraining. Fortrainingtheguidancemodelsfj(·;θ),weusethegraphconvolutionalneuralnetwork
t
(GNN)architecturefromLeeetal.(2023),consistingof3GCNlayerswith16hiddenunitsandtanhactivations(Kipf
&Welling,2016). Theembeddingsfromeachlayerareconcatenatedandfedintotwofullyconnectedheadswithtanh
andsigmoidactivations,respectively,whoseoutputsofdimension16arethenmultipliedandfedintoanothertwo-layer
MLP with the same dimension and RELU activation functions (Nair & Hinton, 2010). We train separate networks
foreachproteintarget, predictingboththemostlikelyregressionlabelµ (g ;θ) = f1(g ;θ), aswellaslog-variances
t t t t
logσ2(g ;θ) = f2(g ;θ) that serve as an estimator of their predictive uncertainty. Given the regression labels y, the
t t t t
modelsareoptimizedwithrespecttoanegativelog-likelihoodlossL =−logN(y;µ ,σ2I)Followingtheprotocolof
t t t
Leeetal.(2023),allmodelsaretrainedfor10epochsofstochasticgradientdescent,usingtheADAMoptimizerwitha
batchsizeof1024andalearningrateof1×10−3(Kingma&Ba,2014). ThecorrespondingobjectiveLwithanexplicit
L regularizationtermisgivenby
2
L(θ,D )=˙ E (cid:2) −logN(y;µ ,σ2I)(cid:3) + 1 ||θ||2, (B.19)
T pT T T 2λ 2
wheretheregularizationstrengthλisoptimizedasahyperparameter. Forweightdecayregularization,weinsteadswitchto
theADAMWoptimizer(Loshchilov&Hutter,2017)andoptimizethecorrespondinghyperparameter. Similarly,wetrain
context-guidedmodelsbyaddingtheadditionalregularizationtermR(θ,f ,t,p )asintroducedinSection3:
t Xˆ
t
(cid:20) (cid:88)2 (cid:16) (cid:17)⊤ (cid:16) (cid:17)(cid:21)
R(θ,f ,t,p )=E fj(gˆ ;θ)−mj(gˆ ) K (gˆ )−1 fj(gˆ ;θ)−mj(gˆ ) , (B.20)
t Xˆ t p Xˆt j=1 t t t t t t t t t t
Here, gˆ is a context batch sampled from an out-of-distribution context set of 500000 unlabeled molecules that were
t
randomlysampledfromZINCandprocessedidenticallytothetrainingdata. AsdescribedinSection3,weusearandomly
initializedsetofparametersϕtogenerate16-dimensionalcontextsetembeddingsh (gˆ ,ϕ) ∈ RM×16 andusethemto
t t
constructthecovariancematrix
K (gˆ )=σ h (gˆ ,ϕ)h (gˆ ,ϕ)T +τ I.
t t t t t t t t
Sincethemodelreturnsmeanandlog-varianceestimatesf1(gˆ ;θ)andf2(gˆ ;θ),thetargetfunctionvaluesmj(gˆ )are
t t t t t t
givenbythetrainingsetmeanm1(gˆ ) = mean(y)andalog-variancehyperparameterm2(gˆ ) = σ2,respectively. The
t t t t 0
latter is set to σ = 0.7, chosen to induce high predictive uncertainty estimates of exp(σ2) ≈ 2 in out-of-distribution
0
regionsoftheinputdomain. Theresultingtrainingobjectiveis
(cid:104) (cid:105)
L∗(θ,D )=E L(θ,D ,T)+R(θ,f ,T,p ) . (B.21)
T pT T T Xˆ
T
Thecovariancescaleσ anddiagonaloffsetτ areoptimizedashyperparameters. Toavoidoverconfidentpredictionsas
t t
contextpointsapproachtheinvariantdistributionN(0,I),weincreaseτ withthesamescheduleasthenoisescalesβ ,
t t
startingfromβ =τ andendingatβ =10τ . Similarly,asthedistinctionbetweenin-andout-of-distributionbecomes
0 t T t
meaninglessatlargernoiselevels,wedecreaseσ withaninvertedschedulestartingfromβ =σ andendingatβ =0.1σ .
t 0 t T t
23Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
Thedeepensembles(Lakshminarayananetal.,2017)comprise5independentmodelsregularizedwithweightdecay,which
isoptimizedasahyperparameter. Thepre-trainedmodelwasoptimizedwithaself-superviseddenoisingobjective,which
hasbeenshowntobeaneffectivepre-trainingmethodZaidietal.(2023). Itwastrainedonall500,000unlabeledmolecules
fromthecontextsetfor50epochswiththeAdamWoptimizer(Loshchilov&Hutter,2017),usingabatchsizeof1024,a
learningrateof10−4,andaweightdecayof10−4. Afterpre-training,thismodelwasfine-tunedonwithweightdecay.
AllregularizationhyperparameterswereoptimizedwithrespecttothesupervisedlossLontheheld-out,high-labeltestset
byperformingagridsearchoverthehyperparameterspacepresentedinTable3. Thebesthyperparametercombinationwas
thenselectedandusedtore-trainfiveindependentmodelsusingdifferentrandomseeds.
Table3: HyperparametersearchspaceforregularizationschemesofregressionmodelsdetailedinAppendixB.2.
Description Hyperparameter SearchSpace
L regularization λ 10−4,10−3,10−2,10−1,1,101,102,103,104
2
Singlemodelwithweightdecay λ 10−8,10−7,10−6,10−5,10−4,10−3,10−2,10−1,0.5
Fine-tuningwithweightdecay λ 10−8,10−7,10−6,10−5,10−4,10−3,10−2,10−1,0.5
Ensemblewithweightdecay λ 10−8,10−7,10−6,10−5,10−4,10−3,10−2,10−1,0.5
covariancescaleσ 10−5,10−3,10−1,1,101,103
t
ContextGuidance diagonaloffsetτ 10−5,10−3,10−1,1,101,103
t
numberofcontextpoints 256,512
Afterretrainingfiveindependentguidancemodelswiththebestregularizationhyperparametersforeachofthefivedifferent
proteins,weusethegradientsfromthesemodelstoconditionthedenoisingprocess. FollowingLeeetal.(2023),weset
theguidancestrengthhyperparameterstor =0.5andr =0,respectively,andsample3000moleculeseach. The
X,0 G,0
SA(Ertl,2003)andQED(Bickertonetal.,2012)scoresforthesenewmoleculesarederivedwithRDKIT(Landrumetal.,
2013),whilethedockingscoreisderivedwithaQUICKVINAscript(Alhossaryetal.,2015)preparedbyLeeetal.(2023).
AdditionalExperimentalResults. WerepeattheheaslineresultsfromSection5.1inFigure10;following(Leeetal.,
2023),wepresentthetop5%ofnormalizeddockingscoresofcompoundsthataredrug-like(y <5andy >0.5)
SA QED
andnovel(Tanimotosimilarityof<0.4tothetrainingset(Bajuszetal.,2015)). Wealsoshowthepercentageofdifferent
compoundsthatwouldbeclassifiedashitswiththethresholdsdefinedinLeeetal.(2023)inFigure11. Inadditiontothe
mainmetricsfromLeeetal.(2023),wealsopresentthefulldistributionsof:
• Theobjectivey= clip(−y vina,20,0) · 10−y SA ·y inFigure12.
20 9 QED
• TheclippedandnormalizednegativeQUICKVINAscore clip(−y vina,20,0) inFigure13.
20
• Thenormalizedsyntheticaccessibility(SA)score 10−y SA inFigure14.
9
• Thequantitativeestimateofdrug-likeness(QED)scorey inFigure15.
QED
Furthermore,wenormalizethescoresofthegeneratedcompoundsbytheirmolecularweightandpresentthecorresponding
resultsinFigure16,ensuringtheperformancedifferencesarenotcausedbythebiasofthedockingscoretowardlarger
molecules. Finally,weshowthesamemodelstrainedonnoisylabelsy¯ =y·ϵwhereϵ∼N(0,I)inFigure17. Finally,the
fulldistributionofmodelstrainedonan85%-15%i.i.d. splitispresentedinFigure18.
24Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
Regularization Type
ours
ours (linear)
ours (constant)
deep ensemble
self-supervised
L regularization
2
weight decay
Figure10: Comparisonofthesamplesgeneratedwithdifferentguideddiffusionmodelsacrossfivedistinctproteintargets
usingthenormalizedtop-5%dockingscoreofnovelanddrug-likecompoundsasinLeeetal.(2023).
parp1 fa7 5ht1b braf jak2
1.4 0.40 16 3.0 6
1.2 0.35 14 2.5 Regularizer
5 0.30 12 ours 1.0 0.25 10 2.0 4 ours (linear)
0.8 ours (no drop-off)
0.20 8 1.5 3 ensemble
0.6
0.15 6 1.0 2 pre-trained
0.4 0.10 4 L2 regularisation
0.2 0.05 2 0.5 1 weight decay
0.0 0.00 0 0.0 0
Regularization Type Regularization Type Regularization Type Regularization Type Regularization Type
Figure11: ThepercentageofsmallmoleculesdenotedashitsusingthethresholdsfromLeeetal.(2023), givenbyan
unnormalizednegativedockingscoreof10forPARP1,8.5forFA7,8.7845for5HT1B,9.1forJAK2,and10.3forBRAF.
protein = parp1 protein = fa7 protein = 5ht1b
0.4
0.4
0.3
0.3
0.3
0.2
0.2
0.2
0.1 0.1 0.1
0.0 0.0 0.0
0.4
protein = braf protein = jak2
ours L2
weight
decay ensemble pre-trained
0.3
0.3 Regularization Type
0.2 ours
0.2
L2
0.1 0.1 weight decay
ensemble
0.0 0.0 pre-trained
ours L2
weight
decay ensemble pre-trained ours L2
weight
decay ensemble pre-trained
Regularization Type Regularization Type
Figure 12: Full distributions of the objective values y of all generated compounds for each protein target across five
independenttrainingandsamplingrunswithdifferentrandomseeds. TheseresultsmirrorthoseinFigures4and10.
25
) ( egatnecrep
tih
)
(
eulaV
evitcejbOContext-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
protein = parp1 protein = fa7 protein = 5ht1b
0.5
0.6
0.6
0.4
0.4 0.3 0.4
0.2
0.2 0.2
0.1
0.0 0.0 0.0
protein = braf protein = jak2
ours L2
weight
decay ensemble pre-trained
0.6
0.6
0.5 Regularization Type
0.4
0.4 ours
0.3
L2
0.2 0.2 weight decay
0.1 ensemble
0.0 0.0 pre-trained
ours L2
weight
decay ensemble pre-trained ours L2
weight
decay ensemble pre-trained
Regularization Type Regularization Type
Figure13: Fulldistributionsoftheclippedandnormalizednegative QUICKVINA dockingscorey
vina
(Alhossaryetal.,
2015)ofallgeneratedcompoundsforeachproteintargetacrossfiveindependenttrainingandsamplingrunswithdifferent
randomseeds. TheseresultsmirrorthoseinFigures4and10.
protein = parp1 protein = fa7 protein = 5ht1b
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
1.0
protein = braf
1.0
protein = jak2
ours L2
weight
decay ensemble pre-trained
0.8 0.8 Regularization Type
ours
0.6 0.6
L2
0.4 0.4 weight decay
0.2 0.2 ensemble
pre-trained
0.0
ours L2
weight
decay ensemble pre-trained ours L2
weight
decay ensemble pre-trained
Regularization Type Regularization Type
Figure14:Fulldistributionsofthenormalizedsyntheticaccessibility(SA)scorey (Ertl,2003)ofallgeneratedcompounds
SA
foreachproteintargetacrossfiveindependenttrainingandsamplingrunswithdifferentrandomseeds. Theseresultsmirror
thoseinFigures4and10.
26
)
(
eulaV
evitcejbO
)
(
eulaV
evitcejbOContext-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
protein = parp1 protein = fa7 protein = 5ht1b
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
1.0
protein = braf
1.0
protein = jak2
ours L2
weight
decay ensemble pre-trained
0.8 0.8
Regularization Type
0.6 0.6 ours
0.4 0.4 L2
weight decay
0.2 0.2
ensemble
0.0 0.0 pre-trained
ours L2
weight
decay ensemble pre-trained ours L2
weight
decay ensemble pre-trained
Regularization Type Regularization Type
Figure15: Fulldistributionsofthequantitativeestimatefordrug-likeness(QED)scorey ofallgeneratedcompounds
QED
foreachproteintargetacrossfiveindependenttrainingandsamplingrunswithdifferentrandomseeds. Theseresultsmirror
thoseinFigures4and10.
protein = parp1 protein = fa7 protein = 5ht1b
0.00175 0.00150
0.00150 0.00125 0.0015
0.00125
0.00100
0.00100 0.0010 0.00075
0.00075
0.00050
0.00050 0.0005
0.00025 0.00025
0.00000 0.00000 0.0000
protein = braf
0.00150
protein = jak2
ours L2
weight
decay ensemble pre-trained
0.0015 0.00125
Regularization Type
0.00100
0.0010 0.00075 ours
L2
0.00050
0.0005 weight decay
0.00025
ensemble
0.0000 0.00000 pre-trained
ours L2
weight
decay ensemble pre-trained ours L2
weight
decay ensemble pre-trained
Regularization Type Regularization Type
Figure16: Fulldistributionsoftheobjectivevaluesyofallgeneratedcompoundsnormalizedbytheirmolecularweightfor
eachproteintargetacrossfiveindependenttrainingandsamplingrunswithdifferentrandomseeds. Theseresultsmirror
thoseinFigures4and10.
27
)
(
eulaV
evitcejbO
)
(
thgieW
raluceloM
/
erocS
gnikcoDContext-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
protein = parp1 protein = fa7 protein = 5ht1b
0.00175 0.00150
0.00150
0.00150 0.00125
0.00125
0.00125 0.00100
0.00100
0.00100
0.00075
0.00075 0.00075
0.00050 0.00050 0.00050
0.00025 0.00025 0.00025
0.00000 0.00000 0.00000
protein = braf
0.00150
protein = jak2
ours L2
weight
decay ensemble pre-trained
0.0015 0.00125
Regularization Type
0.00100
0.0010 0.00075 reg_type
ours
0.0005 0.00050 L2
weight decay
0.00025
ensemble
0.0000 0.00000 pre-trained
ours L2
weight
decay ensemble pre-trained ours L2
weight
decay ensemble pre-trained
Regularization Type Regularization Type
Figure17: Fulldistributionsoftheobjectivevaluesyofallgeneratedcompoundswhenmodelsaretrainedonnoisedlabels
y¯=y·ϵwithaddednoiseϵ∼N(0,I),acrossfiveindependenttrainingandsamplingrunswithdifferentrandomseeds.
TheseresultsmirrorthoseinFigures4and10.
protein = parp1 protein = fa7 protein = 5ht1b
0.4 0.4
0.3
0.3 0.3
0.2
0.2 0.2
0.1 0.1 0.1
0.0 0.0 0.0
0.4
protein = braf
0.4
protein = jak2
ours L2
weight
decay ensemble pre-trained
0.3 0.3 Regularization Type
ours
0.2 0.2 L2
0.1 0.1 weight decay
ensemble
0.0 0.0 pre-trained
ours L2
weight
decay ensemble pre-trained ours L2
weight
decay ensemble pre-trained
Regularization Type Regularization Type
Figure18: Fulldistributionsoftheobjectivevaluesyofallgeneratedcompoundswhenguidancemodelsthataretrained
onan85%-15%i.i.d. randomsplit,acrossfiveindependenttrainingandsamplingrunswithdifferentrandomseeds. As
expected,thedeepensemblesandpre-trainedmethodsperformparticularlywellinthissetting.Interestingly,theperformance
ofourdomain-awareguidancemethodstaysapproximatelythesamebetweentherandomandlabel-splitevaluationsettings.
28
)
(
eulaV
evitcejbO
)
(
eulaV
evitcejbOContext-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
ComparisontoUnsupervisedDomainAdaptationTechniques. Inadditiontothemodelspresentedabove,wealso
compareourmethodtoguidancefunctionsthatareregularizedwiththreerobustandwell-establishedunsuperviseddomain
adaptationtechniques,namely:
• DEEPCORAL: amethodthatalignsthefeaturedistributionsofthelast-layerguidancemodelembeddingsacrossthe
labeleddataandcontextsetthroughanauxiliarylossterm(Sun&Saenko,2016).
• Domain-AdversarialNeuralNetworks(DANN):agradient-reversalapproachthatalignsthelabeleddataandcontext
setembeddingsbysubtractingthegradientsofalineardomainclassifierfromthoseoftheguidancemodel(Ganin
etal.,2016).
• DOMAIN CONFUSION: amethodthattriestolearndomain-invariantrepresentationsbymaximizingthepredictive
entropyofanadversariallytraineddomainclassifier(Tzengetal.,2015).
WeadaptedtheDEEPCORALandDANNimplementationsfromtheDrugOODlibrary(Jietal.,2023)andimplemented
theDOMAINCONFUSIONmodelsfromscratch,closelyfollowingthedescriptioninTzengetal.(2015). Allmodelswere
evaluatedwiththesametraining,hyperparameterselection,andsamplegenerationprotocolsasinAppendixB.2,usingthe
domainadaptationhyperparametersearchspacespresentedinTable4.
Table4: Hyperparametersearchspacefortheunsuperviseddomainadaptationalgorithms.
Method Hyperparameter SearchSpace
DEEPCORAL lossweightλ 10−2,5×10−2,10−1,5×10−1,1,5,101,5×101,102
DANN reversalfactorα 10−2,5×10−3,10−3,5×10−4,10−4,5×10−5
DOMAINCONFUSION lossweightλ 10−2,5×10−2,10−1,5×10−1,1,5,101,5×101,102
TheresultingperformancemetricsforthefirstthreeproteintargetsfromLeeetal.(2023)(PARP1,FA7,and5HT1B)are
presentedinTable5. Whilethesemethodsconsistentlyoutperformtheguidancemodelstrainedwithstandardapproaches
such as L regularization and weight decay, they still perform considerably worse than our approach. One possible
2
explanationforthisperformancegapisthattheDEEPCORAL,DANN,andDOMAINCONFUSIONmethodsonlyactonthe
embeddingspaceofaguidancemodelwithoutconsideringthebehaviorofitspredictions. Incontrast,ourcontext-guided
approachdirectlyregularizesthemodel’spredictions,encouragingittolearnfunctionsthatreverttoanuninformativeprior
inregionsthatarefarfromthetrainingdata. Thisallowsourmethodtobettercapturetheunderlyingstructureofthedata
andimprovesitsrobustnessinout-of-distributionsettings.
Table5: Performancecomparisonofourmethodagainstguidancemodelsregularizedwithunsuperviseddomainadaptation
techniques(DEEPCORAL,DANN,andDOMAINCONFUSION)andstandardregularizationapproaches(WEIGHTDECAY)
onthefirstthreeproteintargetsfromLeeetal.(2023)(PARP1,FA7,and5HT1B). Ourcontext-guidedregularizationscheme
outperformsalldomainadaptationtechniqueswhilemaintainingcomparablecomputationalefficiency. Resultsarereported
asthemeanandstandarddeviationacrossfiveindependenttrainingandsamplingrunswithdifferentrandomseeds. The
bestresultsforeachtargetareshowninbold.
Regularizer Averagebatchtime(s) PARP1(↑) FA7(↑) 5HT1B(↑)
Ours 0.08±0.00 0.64±0.01 0.47±0.00 0.60±0.03
DEEPCORAL 0.08±0.00 0.36±0.01 0.32±0.00 0.35±0.04
DANN 0.07±0.00 0.37±0.07 0.27±0.03 0.34±0.07
DOMAINCONFUSION 0.08±0.00 0.40±0.01 0.30±0.01 0.38±0.01
WEIGHTDECAY 0.02±0.00 0.31±0.00 0.26±0.01 0.31±0.01
29Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
ContextSetSensitivityStudies. Asthecontextsetprovidesourmethodwithstructuredinformationaboutabroader
subsetoftheinputdomain,wefoundthatitssizeandcompositionarekeyfactorsindeterminingmodelperformance. To
investigatethisrelationship,wecarriedoutaseriesofsensitivitystudiesforthefistthreeproteintargetsfromLeeetal.
(2023)(PARP1, FA7,and 5HT1B). Allexperimentswereperformedwiththesameexperimentalsetup,hyperparameter
searchspace,andevaluationprotocolasdescribedinAppendixB.2,differingonlyinthechoiceofcontextset. Thefull
resultsarepresentedinTable6
First,wecomparetheperformanceofamodeltrainedwiththefullcontextset(100%)tomodelstrainedwithrandomly
sampledsubsetsofsize10%and1%,respectively. MirroringtheresultsfromSection5.2,weobserveastrongdeterioration
inperformanceacrossthesesettings,withtheperformanceofthemodelstrainedwiththesmallestcontextset(1%)reverting
tothatofmodelstrainedwithstandardweightdecay.
Toexplorewhetheranevenlargerandmorediversecontextsetcouldleadtofurtherperformancegains, wetrainedan
additionalsetofmodelsusingthe665kbiologicallyandpharmaceuticallyrelevantmoleculesfromtheQMUGSdataset(Isert
etal.,2022),originallycuratedfromtheCHEMBLdatabase(Gaultonetal.,2017). Whileweobservesomeperformance
improvements,theyarerelativelymarginal,indicatingthatouroriginalcontextsetwaslikelyalreadyatorpastthepointof
diminishingreturnsregardingitssize.
Finally,weperformedanadditionalsensitivitystudyforwhichweselectedthe10%ofmoleculesintheoriginalcontextset
thatwereeithermostorleastsimilartothelabeledtrainingdata(asmeasuredbytheirmaximumECFP4-basedTanimoto
similarity,calculatedwithrdkit(Landrumetal.,2013)). WenotethattheaveragemaximumTanimotosimilaritybetween
thesecontextsetsandthetrainingdataisstillrelativelylow,at0.42±0.06and0.31±0.04,respectively. Weobservethat
usingamuchsmallercontextsetwithnear-OODmolecules(mostsimilar10%)isabletorecovertheperformanceofusing
thefullcontextset(100%),whilerelyingonlessrelevantmolecules(leastsimilar10%)leadstoworseperformancethana
randomsubsample(10%).
Table6: Impactofthesizeandcompositionofthecontextsetontheperformanceofourproposedregularizerforthesmall
moleculesapplicationpresentedinSection5.1. Wecomparethefullcontextset(100%)torandomlysampledsubsets(10%
and1%),aswellassubsetsoftheoriginalcontextsetcontainingthe10%ofmoleculesthatareeithermostorleastsimilar
tothelabeledtrainingdata. Weadditionallytrainmodelswithanevenlargerandmorediversecontextsetderivedfrom
QMUGS(Isertetal.,2022). Resultsarereportedasthemeanandstandarddeviationacrossfiveindependenttrainingand
samplingrunswithdifferentrandomseeds.
Regularizer parp1(↑) fa7(↑) 5ht1b(↑)
ours(100%) 0.64±0.01 0.47±0.00 0.60±0.03
ours(10%) 0.54±0.01 0.45±0.00 0.44±0.04
ours(1%) 0.37±0.07 0.27±0.03 0.30±0.07
ours(QMUGS) 0.69±0.02 0.47±0.00 0.62±0.02
ours(mostsimilar10%) 0.66±0.01 0.45±0.01 0.59±0.01
ours(leastsimilar10%) 0.51±0.02 0.32±0.04 0.42±0.01
weightdecay 0.31±0.00 0.26±0.01 0.31±0.01
30Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
B.3.EquivariantDiffusionforMaterials.
Dataset. Webuild onthe experimental setup from Weiss et al. (2023) and usea dataset of all 34071cata-condensed
polybenzenoid hydrocarbons (cc-PBHs) with up to 11 benzene rings (Wahab et al., 2022). In contrast to the setup
inSection5.1andAppendixB.2,whichusesasubsetofthebillionsofcompoundsfromtheZINCdatabase(Tingleetal.,
2023)torepresentthebroaderchemicalspaceofupto1×1060drug-likesmallmolecules(Bohaceketal.,1996;Ertl,2003),
thisapplicationoperatesinaclosedandfullyenumeratedinputspace. Thisallowsustoconstructdatasplitsthatprecisely
controlhowmuchgeneralizationisrequiredtoreachheld-outregionswithoptimalobjectivevalues.
StartingfromtheGFN2-xTBoptimizedground-statecc-PBHstructures,weadoptthegraph-of-ringsrepresentationfrom
Weissetal.(2023),representingacompoundwithRringsasapointcloudofringcentroidsx ∈RR×3. Whilecomputing
t
electronicpropertiesrequires3Drepresentations,muchofthecombinatorialstructureofthisdatapersistsin2Dtopological
graphs. Wethusgenerateextendedreducedgraphfingerprints(Stiefletal.,2006)inRDKIT(usingafuzzinessincrement
of0andamaximumpathlengthof50),whichcorrespondtoa2Dequivalentofthe3Dgraph-of-ringsframework. These
fingerprintsarebinarizedandcomparedviatheJaccard-Tanimotokernel(Bajuszetal.,2015)tomeasurepairwisesimilarities.
Thisenablesustoclusterthedatasetandderivemaximallydistincttraining,validation,test,andcontextsets(Figure19).
SincetheframeworkofWeissetal.(2023)onlygeneratescompoundswithapredeterminednumberofringsRand75%of
thedatasetconsistsofmoleculeswithR=11,wesplitthedataasfollows: AllcompoundswithR<11areusedaspartof
anout-of-distributioncontextset. Theremainingcompoundsaresplitinto3groupsusingspectralclustering(scikit-learn)
(Pedregosaetal.,2011). Thelargestclusterof16973compoundsisusedasthetrainingset. Thesecondlargestclusterof
4538compoundsisusedastheheld-outtestset. Thesmallestclusterof3883compoundsisusedasthevalidationsetfor
hyperparameterselectionandearlystopping. Whentrainingmodelswithourcontext-guidedregularizationscheme,weuse
eitherallmoleculeswithR<11asareducedcontextsetorallcompoundsnotinthetrainingsetasafullcontextset
Figure19: UMAPdimensionalityreductionplots(McInnesetal.,2018)ofthecc-PBHdatasetusingextendedreducedgraph
fingerprints(Stiefletal.,2006)andtheJaccard-Tanimotodistance. Left: Pointsarecolor-codedaccordingtowhetherthey
belongtotheout-of-distributioncontextsetwithfewerthan11ringsorthetraining,validation,andtestset.Right:Thesame
points,color-codedaccordingtothecompositeobjectiveusedbyWeissetal.(2023)anddefinedasy=3y +y −y .
HLG IP EA
Lowervaluesindicatemoredesirablepropertiesandare,forR=11,concentratedintheclusterusedasaheld-outtestset.
EachmoleculeinthedatasetusedbyWeissetal.(2023)isannotatedwithfiveelectronicproperties:LUMOenergy(y ),
LUMO
HOMO-LUMOgap(y ),relativeenergy(y ),adiabaticionizationpotential(y ),andadiabaticelectronaffinity(y )
HLG REL IP EA
(Wahabetal.,2022). ThefullpropertydistributionsforeachdatasplitareshowninFigure20. Conveniently,theheld-out
testsetclustercontainscompoundswithsignificantlybettery ,y ,andy valuesthanthetrainingset,whicharethe
HLG IP EA
propertiesusedtodefinethecompositeobjectiveinWeissetal.(2023),asisalsoapparentinFigure19.
31Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
variable = LUMO_eV variable = GAP_eV variable = Erel_eV
0.08
0.10
0.08
0.08 0.06
0.06
0.06
0.04
0.04
0.04
0.02
0.02 0.02
0.00 0.00 0.00
8.0 7.5 7.0 1 2 3 0.0 0.5 1.0 1.5
variable = aIP_eV variable = aEA_eV
Label Value
0.07 0.07
0.06 0.06
0.05 0.05
< 11 rings
0.04 0.04
training set
0.03 0.03
validation set
0.02 0.02
test_set
0.01 0.01
0.00 0.00
11 12 13 7 6 5
Label Value Label Value
Figure20: Thefullelectronicpropertydistributionsforeachdatasplit.
DiffusionModelTraining. FollowingWeissetal.(2023),wetrainanE(3)-equivariantdiffusionmodel(EDM;Hooge-
boometal.(2022))onthegraph-of-rings-derivedpointcloudsx ∈RR×3. Boththeforwardnoisingandreversedenoising
t
processoutlinedinSection2aremadeequivarianttopermutations,translations,rotations,andreflectionsbyusinganE(3)-
equivariantgraphneuralnetwork(E(3)-GNN)(Satorrasetal.,2021)asthescoremodelandredefiningatranslation-invariant
(cid:80)
noisingprocessinthelinearsubspaceof x =0(Hoogeboometal.,2022).
i i
We optimized the resulting equivariant diffusion model on the 16973 compounds in the training set using the same
hyperparametersettingsasinWeissetal.(2023). Specifically,thescorenetworks (x ,t)isgivenbya9-layerE(3)-GNN
θ t
with192hiddenunitsandtanhactivations. Itistrainedfor1000epochswiththeADAMoptimizer(Kingma&Ba,2014)
usingalearningrateof1×10−3,abatchsizeof256,andasmallweightdecayhyperparameterof10−12. Thediffusion
processisgivenbyacosineβ schedule(Nichol&Dhariwal,2021)withT =1000time-steps,andβ =0andβ =1.
t 0 T
Duetothereducedproblemspaceofthegraph-of-ringsrepresentation,wefindthatthevastmajorityofgeneratedpoint
clouds(≥99%,asinWeissetal.(2023))correspondtovalidmoleculesandcanbeparsedbyRDKIT(Landrumetal.,2013).
GuidanceModelTraining. Similartothescorenetwork,theguidancemodelusesa12-layerE(3)-GNNarchitecture
with192hiddenunitsandtanhactivations,matchingthemodelhyperparametersfromWeissetal.(2023). Thesemodels
aretrainedusingtheADAMoptimizer(Kingma&Ba,2014)forupto1000epochswithalearningrateof6×10−4and
batchsizeof256,stoppingifthevalidationsetperformancedeterioratesformorethan10consecutiveepochs.
AsinWeissetal.(2023),wetrainthesemodelsasmulti-propertypredictionnetworks,outputtingboththeregressionmean
µ (xˆ ;θ)=f1(xˆ ;θ)andlog-variancelogσ2(xˆ ;θ)=f2(xˆ ;θ)forallfivepropertiesy ,y ,y ,y ,andy .
t t t t t t t t HLG IP LUMO REL EA
Giventhetrueregressionlabelsy,themodelsaretrainedbyminimizingtoanegativelog-likelihoodlossacrossthesefive
propertiesL =−logN(y;µ ,σ2I).
t t t
32
ytisneD
ytisneDContext-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
ThecorrespondingobjectiveLwithanexplicitL -regularizationtermisgivenby
2
L(θ,D )=˙ E (cid:2) −logN(y;µ ,σ2I)(cid:3) + 1 ||θ||2, (B.22)
T pT T T 2λ 2
wheretheregularizationstrengthλisoptimizedasahyperparameter. Similarly,modelsareregularizedwithweightdecay
byswitchingtotheADAMWoptimizer(Loshchilov&Hutter,2017)andoptimizingthecorrespondinghyperparameter.
Context-guidedmodelsaretrainedwiththeregularizationtermR(θ,f ,t,p )asintroducedinSection3
t Xˆ
t
(cid:20) (cid:88)2 (cid:16) (cid:17)⊤ (cid:16) (cid:17)(cid:21)
R(θ,f ,t,p )=E fj(xˆ ;θ)−mj(xˆ ) K (xˆ )−1 fj(xˆ ;θ)−mj(xˆ ) , (B.23)
t Xˆ t p Xˆt j=1 t t t t t t t t t t
wherexˆ ∼p (xˆ )isacontextbatchsampledfromeitherthefullcontextsetcoveringallcompoundsnotinthetrainingset,
t Xˆ
t
t
orthereducedcontextsetcontainingonlycompoundswithfewerthan11rings(seeFigure19). AsdescribedinSection3,
weusearandomlyinitializedparametersetϕtogenerate196-dimensionalcontextsetembeddingsh (xˆ ,ϕ)∈RM×196
t t
fromtheE(3)-GNN’slastlayerandusethemtoconstructthecovariancematrix
K (xˆ )=σ h (xˆ ,ϕ)h (xˆ ,ϕ)T +τ I.
t t t t t t t t
Sincethemodelpredictsmeansandlog-variancesf1(xˆ ;θ)andf2(xˆ ;θ),thetargetfunctionvaluesmj(xˆ )aregivenby
t t t t t t
thetrainingsetmeanandalog-variancehyperparameterm2(xˆ )=σ2,respectively. Thelatterissettoσ =1,chosen
t t 0 0
toinducehighpredictiveuncertaintyestimatesofexp(σ2) ≈ einout-of-distributionregionsoftheinputdomain. The
resultingtrainingobjectiveis
(cid:104) (cid:105)
L∗(θ,D)=E L(θ,D ,T)+R(θ,f ,T,p ) . (B.24)
pT T T Xˆ
T
Thecovariancescaleσ anddiagonaloffsetτ areoptimizedashyperparameters. Toavoidoverconfidentpredictionsas
t t
contextpointsapproachtheinvariantdistributionN(0,I),weincreaseτ withthesamescheduleasthenoisescalesβ ,
t t
startingfromβ =τ andendingatβ =10τ . Similarly,asthedistinctionbetweenin-andout-of-distributionbecomes
0 t T t
meaninglessatlargernoiselevels,wedecreaseσ withaninvertedschedulestartingfromβ =σ andendingatβ =0.1σ .
t 0 t T t
Theregularizationhyperparametersareoptimizedviagridsearchontheheld-outvalidationset,usingthesupervisedlossL
astheperformancemetric. Thegridsearchwasoverthehyperparameterspaceshownin Table7. Thebesthyperparameter
combinationwasselectedandusedtore-traintenindependentmodelswithdifferentrandomseeds.
Table7: HyperparametersearchspaceforregularizationschemesofregressionmodelsdetailedinAppendixB.3.
Description Hyperparameter SearchSpace
L regularization λ 10−3,10−2,10−1,1,101,102,10−3
2
Weightdecay λ 10−8,10−7,10−6,10−5,10−4,10−3,10−2,10−1,0.5
covariancescaleσ 10−2,10−1,1,101,10−2
t
ContextGuidance diagonaloffsetτ 10−2,10−1,1,101,10−2
t
numberofcontextpoints 16,64,256
Afterretrainingtenindependentguidancemodelsforeachregularizationtype,theirpredictionsfortheHOMO-LUMOgap
(y ),adiabaticionizationpotential(y ),andadiabaticelectronaffinity(y )propertieswereusedtodefinethecomposite
HLG IP EA
objectivey =3y +y −y . Theresultinggradientswerethenusedtoguidethegenerationof512sampleseach,
HLG IP EA
usinganadditionalguidancescales
∇ logp (x |y)=∇ logp (x )+∇ logp (y|x )s =s (x ,t)+s·∇ logf1(x ;θ)
X t t X t t X t t θ t X t t
AdditionalExperimentalResults. InadditiontotheobjectivevaluesinFigure5,wealsocalculatetheproportionof
generatedsamplesthatare(1)validandnovel(i.e. notinthetrainingset)inFigure21and(2)validandunique(i.e. only
generatedonce)inFigure22. TogetherwithFigure5,theresultssuggestthatasguidancestrengthincreases,ourmethod
generatesmorenovelmoleculeswithbetterobjectivevalues,yetlessdiversity. Thismaybeexplainedbystrongerguidance
signalscausingthedenoisingprocesstoconvergetothesamehigh-valuesubsetsoftheinputdomain.
33Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
0.8
0.6
Regularization Type
ours (full context set)
0.4
ours (reduced context set)
L2 regularization
weight decay
0.2
0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
Guidance Scale
Figure 21: Performance comparison of guidance models with different regularization schemes across ten independent
training and sampling runs. The proportion of compounds that are both valid and novel (i.e. not in the training set) is
computedforeachrunandaggregatedacrossrandomseeds.
1.0
0.9
0.8
Regularization Type
ours (full context set)
ours (reduced context set)
0.7
L2 regularization
weight decay
0.6
0.5
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
Guidance Scale
Figure 22: Performance comparison of guidance models with different regularization schemes across ten independent
training and sampling runs. The proportion of compounds that are both valid and unique (i.e. only generated once) is
computedforeachrunandaggregatedacrossrandomseeds.
34
)
(
levoN
dna
dilaV
)
(
euqinU
dna
dilaVContext-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
B.4.DiscreteDiffusionforProteinSequences.
Dataset. WeusethesameantibodysequencedatasetasGruveretal.(2023),consistingofapproximately100000paired
heavyandlightchainsequencesfromtheOASandSABDABdatabases(Olsenetal.,2022;Dunbaretal.,2014)aligned
withANARCI(Dunbar&Deane,2016)andpaddedtolength300. Wefocusonthesynthetic-labelapplicationspresented
byGruveretal.(2023),namelythesingle-objectiveoptimizationofaprotein’ssolvent-accessiblesurfacearea(SASA)
andpercentageofbetasheets. Inthefollowingexperimentsweoptimizethelatter,usingcodefromGruveretal.(2023)
andtheBIOPYTHONlibrary(Cocketal.,2009)toderivethelabelsy. SimilartoAppendicesB.2andB.3,wesplitthe
dataintoalow-propertytrainingsetandahigh-propertyvalidationsettooptimizeforhyperparametersthatperformwellin
label-shifted,out-of-distributionsettings. Additionally,arandomsubsampleofathirdofthedatasetissplitoffasaheld-out
unlabeledcontextsetforourcontext-guidedmethod,orderingtheremainingtwo-thirdsbytheirlabelsyandsplittingthem
inhalftogeneratethelow-valuetrainingandhigh-valuevalidationsets.
DiffusionandGuidanceModelTraining. Incontrastto Sections5.1and5.2,theguidancemodelofGruveretal.(2023)
isnotastandalonemodel, butratheraregressionheadofthediffusionmodel’sscorenetwork, trainedjointlywiththe
denoisingobjective. Specifically,amaskedlanguagemodel(Austinetal.,2021)learnssequenceembeddings,fromwhich
twoMLPsestimatethescoreandproperties. FollowingGruveretal.(2023),weuseaBERT-basedtransformerbackbone
(Bhargavaetal.,2021;Devlinetal.,2018)with4layersand512hiddenunits. Thescoreheadisasinglelinearlayer,while
thepropertypredictionheadisafeedforwardnetworkwithone512-unithiddenlayerandtanhactivations. Thismodelis
trainedfor500epochswithADAMW(Loshchilov&Hutter,2017),usingabatchsizeof128andalearningrateof5×10−4
witha10-steplinearwarmupschedule. ThenoisingprocessisgivenbyadiscreteformulationfromGruveretal.(2023),
usingcosineβ schedule(Nichol&Dhariwal,2021)withT =1000time-steps,andβ =0andβ =1.
t 0 T
Wetraintheregressionheadtoestimateboththeregressionmeanµ (xˆ ;θ)=f1(xˆ ;θ)andlog-variancelogσ2(xˆ ;θ)=
t t t t t t
f2(xˆ ;θ). Giventhetrueregressionlabelsy,thepropertypredictionmodelisoptimizedwithrespecttothecorresponding
t t
negativelog-likelihoodloss:
L=−logN(y;µ ,σ2 I)
T T
Following(Gruveretal.,2023),wetakeonegradientstepwithrespecttoLforevery5gradientstepswithrespecttothe
scorematchingobjective,loggingthevalidationlossevery5epochsandloadingthebestcheckpointforsamplegeneration.
WetrainL -regularizedmodelsbyaugmentingthesupervisedobjectiveLwithanexplicitL penalty
2 2
L(θ,D )=˙ E (cid:2) −logN(y;µ ,σ2I)(cid:3) + 1 ||θ||2, (B.25)
T pT T T 2λ 2
wheretheregularizationstrengthλisoptimizedasahyperparameter. Similarly,modelsareregularizedwithweightdecay
byswitchingtotheADAMWoptimizer(Loshchilov&Hutter,2017)andoptimizingthecorrespondinghyperparameter.
Context-guidedmodelsaretrainedbyaddingtheadditionalregularizationtermR(θ,f ,t,p )givenby
t Xˆ
t
(cid:20) (cid:88)2 (cid:16) (cid:17)⊤ (cid:16) (cid:17)(cid:21)
R(θ,f ,t,p )=E fj(xˆ ;θ)−mj(xˆ ) K (xˆ )−1 fj(xˆ ;θ)−mj(xˆ ) , (B.26)
t Xˆ t p Xˆt j=1 t t t t t t t t t t
wherexˆ ∼p (xˆ )isacontextbatchsampledfromtheunlabeledcontextsetsplitoffasdetailedbefore. Asdescribed
t Xˆ
t
t
inSection3,weusearandomlyinitializedparametersetϕtogenerate512-dimensionalcontextsetembeddingsh (xˆ ,ϕ)∈
t t
RM×512fromthelastlayeroftheregressionheadandusethemtoconstructthecovariancematrix
K (xˆ )=σ h (xˆ ,ϕ)h (xˆ ,ϕ)T +τ I.
t t t t t t t t
Sincethemodelpredictsmeanandlog-varianceestimatesf1(xˆ ;θ)andf2(xˆ ;θ),thetargetfunctionvaluesmj(xˆ )are
t t t t t t
givenbythetrainingsetmeanandalog-variancehyperparameterm2(xˆ )=σ2,respectively. Thelatterissettoσ =1,
t t 0 0
chosentoinducehighpredictiveuncertaintyestimatesofexp(σ2)≈einout-of-distributionregionsoftheinputdomain.
Theresultingtrainingobjectiveis
(cid:104) (cid:105)
L∗(θ,D)=E L(θ,D ,T)+R(θ,f ,T,p ) . (B.27)
pT T T Xˆ
T
Thecovariancescaleσ anddiagonaloffsetτ areoptimizedashyperparameters. Toavoidoverconfidentpredictionsas
t t
contextpointsapproachtheinvariantdistributionN(0,I),weincreaseτ withthesamescheduleasthenoisescalesβ ,
t t
35Context-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
startingfromβ =τ andendingatβ =10τ . Similarly,asthedistinctionbetweenin-andout-of-distributionbecomes
0 t T t
meaninglessatlargernoiselevels,wedecreaseσ withaninvertedschedulestartingfromβ =σ andendingatβ =0.1σ .
t 0 t T t
Atsamplingtime, Gruveretal.(2023)generatesequencesusingagridofdenoisinghyperparametersandexaminethe
resultingParetofrontofobjectivevaluesversus“naturalness”i.e. howlikelyasequenceistobesynthesizable,estimatedby
itslikelihoodunderaproteinlanguagemodel(Ferruzetal.,2022). Weusesamehyperparametergrid(presentedinTable8),
generatingsampleswithfiveindependentlytrainedmodelswithdifferentrandomseeds.
Table8: HyperparametersearchspaceforregularizationschemesofregressionmodelsdetailedinAppendixB.4.
Hyperparameter Description SamplingRange
λ strengthofnaturalnessregularization 101,1,10−1,10−2,10−3
η stepsizeofLangevindynamicsupdate 1.0
K numberofstepsusedtoupdateembedding 10
guidancelayer whichembeddingtouseforupdate first
returnbest returnsampleswithbestguidancemodelpredictions true
AdditionalExperimentalResults. InadditiontotheParetofrontofthegeneratedsamplesshowninFigure6,wefollow
Gruveretal.(2023)andalsovisualizethefulldistributionofgeneratedsamplesbyplottingtheirkerneldensityestimates.
TheresultsarepresentedinFigure23,mirroringtheconclusionsdrawnfromtheanalysisoftheParetofront.
0.475
Regularizer
ours
0.450 L2 regularization
weight decay
0.425
0.400
0.375
0.350
0.325
0.300
650 600 550 500 450 400
Naturalness ( )
Figure23: Kerneldensityestimates(KDE)ofallsamplesgeneratedwithdifferentregularizationschemesandsampling
hyperparameters,usingthestandardKDEparametersoftheSEABORNplottinglibrary(Waskom,2021)andfivedensity
isocontourseach. SimilartotheParetofront, thishighlightsthetrade-offbetweenobjectivevalueandnaturalness. As
samples move away from the training data and enter an out-of-distribution regime, our method consistently generates
sequenceswithbetterpropertiesatthesamelevelofnaturalness.
36
)
(
eulaV
evitcejbOContext-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
B.5.RuntimeComparison.
We quantify computational cost as the average execution time per mini-batch iteration, including data loading, model
forwardpasses,backpropagation,andparameterupdates. Figure24comparesregularizationschemesfortheapplication
inSection5.1across5modeltrainingruns. Figure25showsthesamecomparisonfortheexperimentsinSection5.3. All
modelsweretrainedwithanidenticalsetupandonthesameNVIDIAA100GPUs,differingonlyintheregularizer.
Whentrainingastandardguidancemodel,asinLeeetal.(2023),weobservethattheaddedcomputationalcostattraining
timeisroughlyequivalenttotrainingadeepensemblewithM =5models(Figure24). However,wenotethatourmethod
doesnotinduceanyadditionalcostatinference,i.e.,samplingtime. Whentrainingthescoreandguidancemodelsjointly
andonlyupdatingthelatterevery5steps,asinGruveretal.(2023),weobservethattheincreaseincomputationalcost
becomesnegligible(Figure25).
Regularization
ours
ours (linear)
0.05 ours (no drop-off)
ensemble
pre-trained
L2 regularisation
0.04 weight decay
0.03
0.02
ours ours (linear) ours (no drop-off) ensemble pre-trained L2 regularisation weight decay
Regularization Type
Figure24: AveragebatchtimesofmodelsfromSection5.1trainedwithanidenticalsetupandonthesameGPUs.
Regularization
ours
3.075 L2 regularization
weight decay
3.050
3.025
3.000
2.975
2.950
2.925
2.900
ours L2 regularization weight decay
Regularization Type
Figure25: AveragebatchtimesofmodelsfromSection5.3trainedwithanidenticalsetupandonthesameGPUs.
37
)
(
emiT
hctaB
egarevA
)
(
emiT
hctaB
egarevAContext-GuidedDiffusionforOut-of-DistributionMolecularandProteinDesign
B.6.SimilarityoftheGeneratedSamplestotheTrainingandValidationSets.
Asourexperimentalsetupsselectregularizationhyperparametersbasedonanout-of-distributionvalidationsetwithmore
desirablepropertiesthanthetrainingdata,itisimportanttoverifythatthemodelsdonotsimplygeneratemoleculesfromthe
high-valuevalidationset. Theresultsarepresentedbelowanddemonstratethatguidancemodeltrainedwithourregularizer
do not simple generate molecules from the validation set, but are instead learning to sample molecules with improved
propertiesfromnovelsubsetsoftheinputdomain.
B.6.1.GRAPH-STRUCTUREDDIFFUSIONFORSMALLMOLECULES
Toinvestigatethis,wecomputedthemaximumECFP4-based(Rogers&Hahn,2010)Jaccard-Tanimotosimilarityofthe
moleculesgeneratedbyourmodelinSection5.1toanycompoundinthetrainingandvalidationsets,respectively. The
resultsareprovidedinTable9andshowthatthesimilaritiesareconsistentlylowandcomparableacrossthetrainingand
validationsets(athresholdof0.7isoftenusedtoindicatemoleculesascloselyrelated).
Table9: Maximumextended-connectivityfingerprint(ECFP4;Rogers&Hahn(2010))-basedTanimotosimilarity(Bajusz
etal.,2015)ofthesmallmoleculesgeneratedinSection5.1tocompoundsinthetrainingandvalidationsets. Meansand
standarddeviationsarereportedacrossfiveindependenttrainingandsamplingrunswithdifferentrandomseeds.
Set PARP1 FA7 5HT1B BRAF JAK2
TrainingSet 0.33±0.09 0.35±0.08 0.35±0.08 0.37±0.07 0.36±0.08
ValidationSet 0.33±0.09 0.36±0.09 0.37±0.09 0.39±0.10 0.38±0.09
B.6.2.EQUIVARIANTDIFFUSIONFORMATERIALS
Similarly,weinvestigatedthedistributionofgeneratedmoleculesacrossthetraining,validation,andtestsetsofthematerials
designexperimentsfromSection5.2. Asthisexperimentalsetupoperatesinanexhaustivelyenumeratedsearchspace,we
comparetheproportionsofgeneratedsamplesacrossallsetswithbothunguidedandweightdecay-regularizedbaselines.
Weuseaguidancescaleof4,asthisisthesettinginwhichbothguideddiffusionmodelsperformbest(seeFigure5). We
observethatourmodelgeneratesasimilarproportionofmoleculesfromthemedium-valuevalidationsetastheweight
decay-regularizedbaseline,whileproducingsubstantiallymorecompoundsfromthecompletelyheld-outhigh-propertytest
set.
Table 10: Distribution of generated molecules across the training, validation, and test sets for the materials science
experimentsinSection5.2. Meansandstandarddeviationsarereportedacrosstenindependenttrainingandsamplingruns
withdifferentrandomseeds.
Model ratiointrainingset ratioinvalidationset ratiointestset
Unguided 0.91±0.01 0.05±0.00 0.04±0.00
WeightDecay 0.49±0.07 0.32±0.05 0.19±0.03
Ours 0.19±0.03 0.39±0.05 0.46±0.06
38