[
    {
        "title": "mDPO: Conditional Preference Optimization for Multimodal Large Language Models",
        "authors": "Fei WangWenxuan ZhouJames Y. HuangNan XuSheng ZhangHoifung PoonMuhao Chen",
        "links": "http://arxiv.org/abs/2406.11839v1",
        "entry_id": "http://arxiv.org/abs/2406.11839v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11839v1",
        "summary": "Direct preference optimization (DPO) has shown to be an effective method for\nlarge language model (LLM) alignment. Recent works have attempted to apply DPO\nto multimodal scenarios but have found it challenging to achieve consistent\nimprovement. Through a comparative experiment, we identify the unconditional\npreference problem in multimodal preference optimization, where the model\noverlooks the image condition. To address this problem, we propose mDPO, a\nmultimodal DPO objective that prevents the over-prioritization of language-only\npreferences by also optimizing image preference. Moreover, we introduce a\nreward anchor that forces the reward to be positive for chosen responses,\nthereby avoiding the decrease in their likelihood -- an intrinsic problem of\nrelative preference optimization. Experiments on two multimodal LLMs of\ndifferent sizes and three widely used benchmarks demonstrate that mDPO\neffectively addresses the unconditional preference problem in multimodal\npreference optimization and significantly improves model performance,\nparticularly in reducing hallucination.",
        "updated": "2024-06-17 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11839v1"
    },
    {
        "title": "MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs",
        "authors": "Ziyu LiuTao ChuYuhang ZangXilin WeiXiaoyi DongPan ZhangZijian LiangYuanjun XiongYu QiaoDahua LinJiaqi Wang",
        "links": "http://arxiv.org/abs/2406.11833v1",
        "entry_id": "http://arxiv.org/abs/2406.11833v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11833v1",
        "summary": "Generating natural and meaningful responses to communicate with multi-modal\nhuman inputs is a fundamental capability of Large Vision-Language\nModels(LVLMs). While current open-source LVLMs demonstrate promising\nperformance in simplified scenarios such as single-turn single-image input,\nthey fall short in real-world conversation scenarios such as following\ninstructions in a long context history with multi-turn and multi-images.\nExisting LVLM benchmarks primarily focus on single-choice questions or\nshort-form responses, which do not adequately assess the capabilities of LVLMs\nin real-world human-AI interaction applications. Therefore, we introduce MMDU,\na comprehensive benchmark, and MMDU-45k, a large-scale instruction tuning\ndataset, designed to evaluate and improve LVLMs' abilities in multi-turn and\nmulti-image conversations. We employ the clustering algorithm to ffnd the\nrelevant images and textual descriptions from the open-source Wikipedia and\nconstruct the question-answer pairs by human annotators with the assistance of\nthe GPT-4o model. MMDU has a maximum of 18k image+text tokens, 20 images, and\n27 turns, which is at least 5x longer than previous benchmarks and poses\nchallenges to current LVLMs. Our in-depth analysis of 15 representative LVLMs\nusing MMDU reveals that open-source LVLMs lag behind closed-source counterparts\ndue to limited conversational instruction tuning data. We demonstrate that\nffne-tuning open-source LVLMs on MMDU-45k signiffcantly address this gap,\ngenerating longer and more accurate conversations, and improving scores on MMDU\nand existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA:+1.2%). Our\ncontributions pave the way for bridging the gap between current LVLM models and\nreal-world application demands. This project is available at\nhttps://github.com/Liuziyu77/MMDU.",
        "updated": "2024-06-17 17:59:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11833v1"
    },
    {
        "title": "Language Modeling with Editable External Knowledge",
        "authors": "Belinda Z. LiEmmy LiuAlexis RossAbbas ZeitounGraham NeubigJacob Andreas",
        "links": "http://arxiv.org/abs/2406.11830v1",
        "entry_id": "http://arxiv.org/abs/2406.11830v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11830v1",
        "summary": "When the world changes, so does the text that humans write about it. How do\nwe build language models that can be easily updated to reflect these changes?\nOne popular approach is retrieval-augmented generation, in which new documents\nare inserted into a knowledge base and retrieved during prediction for\ndownstream tasks. Most prior work on these systems have focused on improving\nbehavior during prediction through better retrieval or reasoning. This paper\nintroduces ERASE, which instead improves model behavior when new documents are\nacquired, by incrementally deleting or rewriting other entries in the knowledge\nbase each time a document is added. In two new benchmark datasets evaluating\nmodels' ability to answer questions about a stream of news articles or\nconversations, ERASE improves accuracy relative to conventional\nretrieval-augmented generation by 7-13% (Mixtral-8x7B) and 6-10% (Llama-3-8B)\nabsolute. Code and data are available at https://github.com/belindal/ERASE",
        "updated": "2024-06-17 17:59:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11830v1"
    },
    {
        "title": "WPO: Enhancing RLHF with Weighted Preference Optimization",
        "authors": "Wenxuan ZhouRavi AgrawalShujian ZhangSathish Reddy IndurthiSanqiang ZhaoKaiqiang SongSilei XuChenguang Zhu",
        "links": "http://arxiv.org/abs/2406.11827v1",
        "entry_id": "http://arxiv.org/abs/2406.11827v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11827v1",
        "summary": "Reinforcement learning from human feedback (RLHF) is a promising solution to\nalign large language models (LLMs) more closely with human values. Off-policy\npreference optimization, where the preference data is obtained from other\nmodels, is widely adopted due to its cost efficiency and scalability. However,\noff-policy preference optimization often suffers from a distributional gap\nbetween the policy used for data collection and the target policy, leading to\nsuboptimal optimization. In this paper, we propose a novel strategy to mitigate\nthis problem by simulating on-policy learning with off-policy preference data.\nOur Weighted Preference Optimization (WPO) method adapts off-policy data to\nresemble on-policy data more closely by reweighting preference pairs according\nto their probability under the current policy. This method not only addresses\nthe distributional gap problem but also enhances the optimization process\nwithout incurring additional costs. We validate our method on instruction\nfollowing benchmarks including Alpaca Eval 2 and MT-bench. WPO not only\noutperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2\nbut also establishes a remarkable length-controlled winning rate against\nGPT-4-turbo of 48.6% based on Llama-3-8B-Instruct, making it the strongest 8B\nmodel on the leaderboard. We will release the code and models at\nhttps://github.com/wzhouad/WPO.",
        "updated": "2024-06-17 17:59:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11827v1"
    },
    {
        "title": "Embodied Instruction Following in Unknown Environments",
        "authors": "Zhenyu WuZiwei WangXiuwei XuJiwen LuHaibin Yan",
        "links": "http://arxiv.org/abs/2406.11818v1",
        "entry_id": "http://arxiv.org/abs/2406.11818v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11818v1",
        "summary": "Enabling embodied agents to complete complex human instructions from natural\nlanguage is crucial to autonomous systems in household services. Conventional\nmethods can only accomplish human instructions in the known environment where\nall interactive objects are provided to the embodied agent, and directly\ndeploying the existing approaches for the unknown environment usually generates\ninfeasible plans that manipulate non-existing objects. On the contrary, we\npropose an embodied instruction following (EIF) method for complex tasks in the\nunknown environment, where the agent efficiently explores the unknown\nenvironment to generate feasible plans with existing objects to accomplish\nabstract instructions. Specifically, we build a hierarchical embodied\ninstruction following framework including the high-level task planner and the\nlow-level exploration controller with multimodal large language models. We then\nconstruct a semantic representation map of the scene with dynamic region\nattention to demonstrate the known visual clues, where the goal of task\nplanning and scene exploration is aligned for human instruction. For the task\nplanner, we generate the feasible step-by-step plans for human goal\naccomplishment according to the task completion process and the known visual\nclues. For the exploration controller, the optimal navigation or object\ninteraction policy is predicted based on the generated step-wise plans and the\nknown visual clues. The experimental results demonstrate that our method can\nachieve 45.09% success rate in 204 complex human instructions such as making\nbreakfast and tidying rooms in large house-level scenes.",
        "updated": "2024-06-17 17:55:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11818v1"
    }
]