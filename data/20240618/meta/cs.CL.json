[
    {
        "title": "mDPO: Conditional Preference Optimization for Multimodal Large Language Models",
        "authors": "Fei WangWenxuan ZhouJames Y. HuangNan XuSheng ZhangHoifung PoonMuhao Chen",
        "links": "http://arxiv.org/abs/2406.11839v1",
        "entry_id": "http://arxiv.org/abs/2406.11839v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11839v1",
        "summary": "Direct preference optimization (DPO) has shown to be an effective method for\nlarge language model (LLM) alignment. Recent works have attempted to apply DPO\nto multimodal scenarios but have found it challenging to achieve consistent\nimprovement. Through a comparative experiment, we identify the unconditional\npreference problem in multimodal preference optimization, where the model\noverlooks the image condition. To address this problem, we propose mDPO, a\nmultimodal DPO objective that prevents the over-prioritization of language-only\npreferences by also optimizing image preference. Moreover, we introduce a\nreward anchor that forces the reward to be positive for chosen responses,\nthereby avoiding the decrease in their likelihood -- an intrinsic problem of\nrelative preference optimization. Experiments on two multimodal LLMs of\ndifferent sizes and three widely used benchmarks demonstrate that mDPO\neffectively addresses the unconditional preference problem in multimodal\npreference optimization and significantly improves model performance,\nparticularly in reducing hallucination.",
        "updated": "2024-06-17 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11839v1"
    },
    {
        "title": "Language Modeling with Editable External Knowledge",
        "authors": "Belinda Z. LiEmmy LiuAlexis RossAbbas ZeitounGraham NeubigJacob Andreas",
        "links": "http://arxiv.org/abs/2406.11830v1",
        "entry_id": "http://arxiv.org/abs/2406.11830v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11830v1",
        "summary": "When the world changes, so does the text that humans write about it. How do\nwe build language models that can be easily updated to reflect these changes?\nOne popular approach is retrieval-augmented generation, in which new documents\nare inserted into a knowledge base and retrieved during prediction for\ndownstream tasks. Most prior work on these systems have focused on improving\nbehavior during prediction through better retrieval or reasoning. This paper\nintroduces ERASE, which instead improves model behavior when new documents are\nacquired, by incrementally deleting or rewriting other entries in the knowledge\nbase each time a document is added. In two new benchmark datasets evaluating\nmodels' ability to answer questions about a stream of news articles or\nconversations, ERASE improves accuracy relative to conventional\nretrieval-augmented generation by 7-13% (Mixtral-8x7B) and 6-10% (Llama-3-8B)\nabsolute. Code and data are available at https://github.com/belindal/ERASE",
        "updated": "2024-06-17 17:59:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11830v1"
    },
    {
        "title": "WPO: Enhancing RLHF with Weighted Preference Optimization",
        "authors": "Wenxuan ZhouRavi AgrawalShujian ZhangSathish Reddy IndurthiSanqiang ZhaoKaiqiang SongSilei XuChenguang Zhu",
        "links": "http://arxiv.org/abs/2406.11827v1",
        "entry_id": "http://arxiv.org/abs/2406.11827v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11827v1",
        "summary": "Reinforcement learning from human feedback (RLHF) is a promising solution to\nalign large language models (LLMs) more closely with human values. Off-policy\npreference optimization, where the preference data is obtained from other\nmodels, is widely adopted due to its cost efficiency and scalability. However,\noff-policy preference optimization often suffers from a distributional gap\nbetween the policy used for data collection and the target policy, leading to\nsuboptimal optimization. In this paper, we propose a novel strategy to mitigate\nthis problem by simulating on-policy learning with off-policy preference data.\nOur Weighted Preference Optimization (WPO) method adapts off-policy data to\nresemble on-policy data more closely by reweighting preference pairs according\nto their probability under the current policy. This method not only addresses\nthe distributional gap problem but also enhances the optimization process\nwithout incurring additional costs. We validate our method on instruction\nfollowing benchmarks including Alpaca Eval 2 and MT-bench. WPO not only\noutperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2\nbut also establishes a remarkable length-controlled winning rate against\nGPT-4-turbo of 48.6% based on Llama-3-8B-Instruct, making it the strongest 8B\nmodel on the leaderboard. We will release the code and models at\nhttps://github.com/wzhouad/WPO.",
        "updated": "2024-06-17 17:59:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11827v1"
    },
    {
        "title": "On Efficient Language and Vision Assistants for Visually-Situated Natural Language Understanding: What Matters in Reading and Reasoning",
        "authors": "Geewook KimMinjoon Seo",
        "links": "http://arxiv.org/abs/2406.11823v1",
        "entry_id": "http://arxiv.org/abs/2406.11823v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11823v1",
        "summary": "Recent advancements in language and vision assistants have showcased\nimpressive capabilities but suffer from a lack of transparency, limiting\nbroader research and reproducibility. While open-source models handle general\nimage tasks effectively, they face challenges with the high computational\ndemands of complex visually-situated text understanding. Such tasks often\nrequire increased token inputs and large vision modules to harness\nhigh-resolution information. Striking a balance between model size and data\nimportance remains an open question. This study aims to redefine the design of\nvision-language models by identifying key components and creating efficient\nmodels with constrained inference costs. By strategically formulating datasets,\noptimizing vision modules, and enhancing supervision techniques, we achieve\nsignificant improvements in inference throughput while maintaining high\nperformance. Extensive experiments across models ranging from 160M to 13B\nparameters offer insights into model optimization. We will fully open-source\nour codebase, models, and datasets at https://github.com/naver-ai/elva .",
        "updated": "2024-06-17 17:57:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11823v1"
    },
    {
        "title": "Iterative Length-Regularized Direct Preference Optimization: A Case Study on Improving 7B Language Models to GPT-4 Level",
        "authors": "Jie LiuZhanhui ZhouJiaheng LiuXingyuan BuChao YangHan-Sen ZhongWanli Ouyang",
        "links": "http://arxiv.org/abs/2406.11817v1",
        "entry_id": "http://arxiv.org/abs/2406.11817v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11817v1",
        "summary": "Direct Preference Optimization (DPO), a standard method for aligning language\nmodels with human preferences, is traditionally applied to offline preferences.\nRecent studies show that DPO benefits from iterative training with online\npreferences labeled by a trained reward model. In this work, we identify a\npitfall of vanilla iterative DPO - improved response quality can lead to\nincreased verbosity. To address this, we introduce iterative length-regularized\nDPO (iLR-DPO) to penalize response length. Our empirical results show that\niLR-DPO can enhance a 7B model to perform on par with GPT-4 without increasing\nverbosity. Specifically, our 7B model achieves a $50.5\\%$ length-controlled win\nrate against $\\texttt{GPT-4 Preview}$ on AlpacaEval 2.0, and excels across\nstandard benchmarks including MT-Bench, Arena-Hard and OpenLLM Leaderboard.\nThese results demonstrate the effectiveness of iterative DPO in aligning\nlanguage models with human feedback.",
        "updated": "2024-06-17 17:55:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11817v1"
    }
]