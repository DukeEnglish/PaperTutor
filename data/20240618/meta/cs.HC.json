[
    {
        "title": "Folk-ontological stances toward robots and psychological human likeness",
        "authors": "Edoardo Datteri",
        "links": "http://arxiv.org/abs/2406.11759v1",
        "entry_id": "http://arxiv.org/abs/2406.11759v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11759v1",
        "summary": "It has often been argued that people can attribute mental states to robots\nwithout making any ontological commitments to the reality of those states. But\nwhat does it mean to 'attribute' a mental state to a robot, and what is an\n'ontological commitment'? It will be argued that, on a plausible interpretation\nof these two notions, it is not clear how mental state attribution can occur\nwithout any ontological commitment. Taking inspiration from the philosophical\ndebate on scientific realism, a provisional taxonomy of folk-ontological\nstances towards robots will also be identified, corresponding to different ways\nof understanding robotic minds. They include realism, non-realism,\neliminativism, reductionism, fictionalism and agnosticism. Instrumentalism will\nalso be discussed and presented as a folk-epistemological stance. In the last\npart of the article it will be argued that people's folk-ontological stances\ntowards robots and humans can influence their perception of the human-likeness\nof robots. The analysis carried out here can be seen as encouraging a\n'folk-ontological turn' in human-robot interaction research, aimed at\nexplicitly determining what beliefs people have about the reality of robot\nminds.",
        "updated": "2024-06-17 17:23:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11759v1"
    },
    {
        "title": "STAR: SocioTechnical Approach to Red Teaming Language Models",
        "authors": "Laura WeidingerJohn MellorBernat Guillen PeguerolesNahema MarchalRavin KumarKristian LumCanfer AkbulutMark DiazStevie BergmanMikel RodriguezVerena RieserWilliam Isaac",
        "links": "http://arxiv.org/abs/2406.11757v1",
        "entry_id": "http://arxiv.org/abs/2406.11757v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11757v1",
        "summary": "This research introduces STAR, a sociotechnical framework that improves on\ncurrent best practices for red teaming safety of large language models. STAR\nmakes two key contributions: it enhances steerability by generating\nparameterised instructions for human red teamers, leading to improved coverage\nof the risk surface. Parameterised instructions also provide more detailed\ninsights into model failures at no increased cost. Second, STAR improves signal\nquality by matching demographics to assess harms for specific groups, resulting\nin more sensitive annotations. STAR further employs a novel step of arbitration\nto leverage diverse viewpoints and improve label reliability, treating\ndisagreement not as noise but as a valuable contribution to signal quality.",
        "updated": "2024-06-17 17:16:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11757v1"
    },
    {
        "title": "SeamPose: Repurposing Seams as Capacitive Sensors in a Shirt for Upper-Body Pose Tracking",
        "authors": "Tianhong Catherine YuManruZhangPeter HeChi-Jung LeeCassidy CheesmanSaif MahmudRuidong ZhangFrançois GuimbretièreCheng Zhang",
        "links": "http://arxiv.org/abs/2406.11645v1",
        "entry_id": "http://arxiv.org/abs/2406.11645v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11645v1",
        "summary": "Seams are areas of overlapping fabric formed by stitching two or more pieces\nof fabric together in the cut-and-sew apparel manufacturing process. In\nSeamPose, we repurposed seams as capacitive sensors in a shirt for continuous\nupper-body pose estimation. Compared to previous all-textile motion-capturing\ngarments that place the electrodes on the surface of clothing, our solution\nleverages existing seams inside of a shirt by machine-sewing insulated\nconductive threads over the seams. The unique invisibilities and placements of\nthe seams afford the sensing shirt to look and wear the same as a conventional\nshirt while providing exciting pose-tracking capabilities. To validate this\napproach, we implemented a proof-of-concept untethered shirt. With eight\ncapacitive sensing seams, our customized deep-learning pipeline accurately\nestimates the upper-body 3D joint positions relative to the pelvis. With a\n12-participant user study, we demonstrated promising cross-user and\ncross-session tracking performance. SeamPose represents a step towards\nunobtrusive integration of smart clothing for everyday pose estimation.",
        "updated": "2024-06-17 15:28:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11645v1"
    },
    {
        "title": "PyGWalker: On-the-fly Assistant for Exploratory Visual Data Analysis",
        "authors": "Yue YuLeixian ShenFei LongHuamin QuHao Chen",
        "links": "http://arxiv.org/abs/2406.11637v1",
        "entry_id": "http://arxiv.org/abs/2406.11637v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11637v1",
        "summary": "Exploratory visual data analysis tools empower data analysts to efficiently\nand intuitively explore data insights throughout the entire analysis cycle.\nHowever, the gap between common programmatic analysis (e.g., within\ncomputational notebooks) and exploratory visual analysis leads to a disjointed\nand inefficient data analysis experience. To bridge this gap, we developed\nPyGWalker, a Python library that offers on-the-fly assistance for exploratory\nvisual data analysis. It features a lightweight and intuitive GUI with a shelf\nbuilder modality. Its loosely coupled architecture supports multiple\ncomputational environments to accommodate varying data sizes. Since its release\nin February 2023, PyGWalker has gained much attention, with 612k downloads on\nPyPI and over 10.5k stars on GitHub as of June 2024. This demonstrates its\nvalue to the data science and visualization community, with researchers and\ndevelopers integrating it into their own applications and studies.",
        "updated": "2024-06-17 15:16:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11637v1"
    },
    {
        "title": "ESI-GAL: EEG Source Imaging-based Kinemaics Parameter Estimation for Grasp and Lift Task",
        "authors": "Anant JainLalan Kumar",
        "links": "http://arxiv.org/abs/2406.11500v1",
        "entry_id": "http://arxiv.org/abs/2406.11500v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11500v1",
        "summary": "Objective: Electroencephalogram (EEG) signals-based motor kinematics\nprediction (MKP) has been an active area of research to develop brain-computer\ninterface (BCI) systems such as exosuits, prostheses, and rehabilitation\ndevices. However, EEG source imaging (ESI) based kinematics prediction is\nsparsely explored in the literature. Approach: In this study, pre-movement EEG\nfeatures are utilized to predict three-dimensional (3D) hand kinematics for the\ngrasp-and-lift motor task. A public dataset, WAY-EEG-GAL, is utilized for MKP\nanalysis. In particular, sensor-domain (EEG data) and source-domain (ESI data)\nbased features from the frontoparietal region are explored for MKP. Deep\nlearning-based models are explored to achieve efficient kinematics decoding.\nVarious time-lagged and window sizes are analyzed for hand kinematics\nprediction. Subsequently, intra-subject and inter-subject MKP analysis is\nperformed to investigate the subject-specific and subject-independent\nmotor-learning capabilities of the neural decoders. The Pearson correlation\ncoefficient (PCC) is used as the performance metric for kinematics trajectory\ndecoding. Main results: The rEEGNet neural decoder achieved the best\nperformance with sensor-domain and source-domain features with the time lag and\nwindow size of 100 ms and 450 ms, respectively. The highest mean PCC values of\n0.790, 0.795, and 0.637 are achieved using sensor-domain features, while 0.769,\n0.777, and 0.647 are achieved using source-domain features in x, y, and\nz-directions, respectively. Significance: This study explores the feasibility\nof trajectory prediction using EEG sensor-domain and source-domain EEG features\nfor the grasp-and-lift task. Furthermore, inter-subject trajectory estimation\nis performed using the proposed deep learning decoder with EEG source domain\nfeatures.",
        "updated": "2024-06-17 13:02:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11500v1"
    }
]