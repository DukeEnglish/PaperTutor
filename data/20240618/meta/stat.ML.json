[
    {
        "title": "Learning sum of diverse features: computational hardness and efficient gradient-based training for ridge combinations",
        "authors": "Kazusato OkoYujin SongTaiji SuzukiDenny Wu",
        "links": "http://arxiv.org/abs/2406.11828v1",
        "entry_id": "http://arxiv.org/abs/2406.11828v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11828v1",
        "summary": "We study the computational and sample complexity of learning a target\nfunction $f_*:\\mathbb{R}^d\\to\\mathbb{R}$ with additive structure, that is,\n$f_*(x) = \\frac{1}{\\sqrt{M}}\\sum_{m=1}^M f_m(\\langle x, v_m\\rangle)$, where\n$f_1,f_2,...,f_M:\\mathbb{R}\\to\\mathbb{R}$ are nonlinear link functions of\nsingle-index models (ridge functions) with diverse and near-orthogonal index\nfeatures $\\{v_m\\}_{m=1}^M$, and the number of additive tasks $M$ grows with the\ndimensionality $M\\asymp d^\\gamma$ for $\\gamma\\ge 0$. This problem setting is\nmotivated by the classical additive model literature, the recent representation\nlearning theory of two-layer neural network, and large-scale pretraining where\nthe model simultaneously acquires a large number of \"skills\" that are often\nlocalized in distinct parts of the trained network. We prove that a large\nsubset of polynomial $f_*$ can be efficiently learned by gradient descent\ntraining of a two-layer neural network, with a polynomial statistical and\ncomputational complexity that depends on the number of tasks $M$ and the\ninformation exponent of $f_m$, despite the unknown link function and $M$\ngrowing with the dimensionality. We complement this learnability guarantee with\ncomputational hardness result by establishing statistical query (SQ) lower\nbounds for both the correlational SQ and full SQ algorithms.",
        "updated": "2024-06-17 17:59:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11828v1"
    },
    {
        "title": "Stochastic Neural Network Symmetrisation in Markov Categories",
        "authors": "Rob Cornish",
        "links": "http://arxiv.org/abs/2406.11814v1",
        "entry_id": "http://arxiv.org/abs/2406.11814v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11814v1",
        "summary": "We consider the problem of symmetrising a neural network along a group\nhomomorphism: given a homomorphism $\\varphi : H \\to G$, we would like a\nprocedure that converts $H$-equivariant neural networks into $G$-equivariant\nones. We formulate this in terms of Markov categories, which allows us to\nconsider neural networks whose outputs may be stochastic, but with\nmeasure-theoretic details abstracted away. We obtain a flexible, compositional,\nand generic framework for symmetrisation that relies on minimal assumptions\nabout the structure of the group and the underlying neural network\narchitecture. Our approach recovers existing methods for deterministic\nsymmetrisation as special cases, and extends directly to provide a novel\nmethodology for stochastic symmetrisation also. Beyond this, we believe our\nfindings also demonstrate the utility of Markov categories for addressing\nproblems in machine learning in a conceptual yet mathematically rigorous way.",
        "updated": "2024-06-17 17:54:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11814v1"
    },
    {
        "title": "Efficient Discovery of Significant Patterns with Few-Shot Resampling",
        "authors": "Leonardo PellegrinaFabio Vandin",
        "links": "http://arxiv.org/abs/2406.11803v1",
        "entry_id": "http://arxiv.org/abs/2406.11803v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11803v1",
        "summary": "Significant pattern mining is a fundamental task in mining transactional\ndata, requiring to identify patterns significantly associated with the value of\na given feature, the target. In several applications, such as biomedicine,\nbasket market analysis, and social networks, the goal is to discover patterns\nwhose association with the target is defined with respect to an underlying\npopulation, or process, of which the dataset represents only a collection of\nobservations, or samples. A natural way to capture the association of a pattern\nwith the target is to consider its statistical significance, assessing its\ndeviation from the (null) hypothesis of independence between the pattern and\nthe target. While several algorithms have been proposed to find statistically\nsignificant patterns, it remains a computationally demanding task, and for\ncomplex patterns such as subgroups, no efficient solution exists.\n  We present FSR, an efficient algorithm to identify statistically significant\npatterns with rigorous guarantees on the probability of false discoveries. FSR\nbuilds on a novel general framework for mining significant patterns that\ncaptures some of the most commonly considered patterns, including itemsets,\nsequential patterns, and subgroups. FSR uses a small number of resampled\ndatasets, obtained by assigning i.i.d. labels to each transaction, to\nrigorously bound the supremum deviation of a quality statistic measuring the\nsignificance of patterns. FSR builds on novel tight bounds on the supremum\ndeviation that require to mine a small number of resampled datasets, while\nproviding a high effectiveness in discovering significant patterns. As a test\ncase, we consider significant subgroup mining, and our evaluation on several\nreal datasets shows that FSR is effective in discovering significant subgroups,\nwhile requiring a small number of resampled datasets.",
        "updated": "2024-06-17 17:49:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11803v1"
    },
    {
        "title": "Joint Linked Component Analysis for Multiview Data",
        "authors": "Lin XiaoLuo Xiao",
        "links": "http://arxiv.org/abs/2406.11761v1",
        "entry_id": "http://arxiv.org/abs/2406.11761v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11761v1",
        "summary": "In this work, we propose the joint linked component analysis (joint\\_LCA) for\nmultiview data. Unlike classic methods which extract the shared components in a\nsequential manner, the objective of joint\\_LCA is to identify the view-specific\nloading matrices and the rank of the common latent subspace simultaneously. We\nformulate a matrix decomposition model where a joint structure and an\nindividual structure are present in each data view, which enables us to arrive\nat a clean svd representation for the cross covariance between any pair of data\nviews. An objective function with a novel penalty term is then proposed to\nachieve simultaneous estimation and rank selection. In addition, a refitting\nprocedure is employed as a remedy to reduce the shrinkage bias caused by the\npenalization.",
        "updated": "2024-06-17 17:25:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11761v1"
    },
    {
        "title": "A Clipped Trip: the Dynamics of SGD with Gradient Clipping in High-Dimensions",
        "authors": "Noah MarshallKe Liang XiaoAtish AgarwalaElliot Paquette",
        "links": "http://arxiv.org/abs/2406.11733v1",
        "entry_id": "http://arxiv.org/abs/2406.11733v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11733v1",
        "summary": "The success of modern machine learning is due in part to the adaptive\noptimization methods that have been developed to deal with the difficulties of\ntraining large models over complex datasets. One such method is gradient\nclipping: a practical procedure with limited theoretical underpinnings. In this\nwork, we study clipping in a least squares problem under streaming SGD. We\ndevelop a theoretical analysis of the learning dynamics in the limit of large\nintrinsic dimension-a model and dataset dependent notion of dimensionality. In\nthis limit we find a deterministic equation that describes the evolution of the\nloss. We show that with Gaussian noise clipping cannot improve SGD performance.\nYet, in other noisy settings, clipping can provide benefits with tuning of the\nclipping threshold. In these cases, clipping biases updates in a way beneficial\nto training which cannot be recovered by SGD under any schedule. We conclude\nwith a discussion about the links between high-dimensional clipping and neural\nnetwork training.",
        "updated": "2024-06-17 16:50:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11733v1"
    }
]