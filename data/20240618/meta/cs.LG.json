[
    {
        "title": "mDPO: Conditional Preference Optimization for Multimodal Large Language Models",
        "authors": "Fei WangWenxuan ZhouJames Y. HuangNan XuSheng ZhangHoifung PoonMuhao Chen",
        "links": "http://arxiv.org/abs/2406.11839v1",
        "entry_id": "http://arxiv.org/abs/2406.11839v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11839v1",
        "summary": "Direct preference optimization (DPO) has shown to be an effective method for\nlarge language model (LLM) alignment. Recent works have attempted to apply DPO\nto multimodal scenarios but have found it challenging to achieve consistent\nimprovement. Through a comparative experiment, we identify the unconditional\npreference problem in multimodal preference optimization, where the model\noverlooks the image condition. To address this problem, we propose mDPO, a\nmultimodal DPO objective that prevents the over-prioritization of language-only\npreferences by also optimizing image preference. Moreover, we introduce a\nreward anchor that forces the reward to be positive for chosen responses,\nthereby avoiding the decrease in their likelihood -- an intrinsic problem of\nrelative preference optimization. Experiments on two multimodal LLMs of\ndifferent sizes and three widely used benchmarks demonstrate that mDPO\neffectively addresses the unconditional preference problem in multimodal\npreference optimization and significantly improves model performance,\nparticularly in reducing hallucination.",
        "updated": "2024-06-17 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11839v1"
    },
    {
        "title": "MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs",
        "authors": "Ziyu LiuTao ChuYuhang ZangXilin WeiXiaoyi DongPan ZhangZijian LiangYuanjun XiongYu QiaoDahua LinJiaqi Wang",
        "links": "http://arxiv.org/abs/2406.11833v1",
        "entry_id": "http://arxiv.org/abs/2406.11833v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11833v1",
        "summary": "Generating natural and meaningful responses to communicate with multi-modal\nhuman inputs is a fundamental capability of Large Vision-Language\nModels(LVLMs). While current open-source LVLMs demonstrate promising\nperformance in simplified scenarios such as single-turn single-image input,\nthey fall short in real-world conversation scenarios such as following\ninstructions in a long context history with multi-turn and multi-images.\nExisting LVLM benchmarks primarily focus on single-choice questions or\nshort-form responses, which do not adequately assess the capabilities of LVLMs\nin real-world human-AI interaction applications. Therefore, we introduce MMDU,\na comprehensive benchmark, and MMDU-45k, a large-scale instruction tuning\ndataset, designed to evaluate and improve LVLMs' abilities in multi-turn and\nmulti-image conversations. We employ the clustering algorithm to ffnd the\nrelevant images and textual descriptions from the open-source Wikipedia and\nconstruct the question-answer pairs by human annotators with the assistance of\nthe GPT-4o model. MMDU has a maximum of 18k image+text tokens, 20 images, and\n27 turns, which is at least 5x longer than previous benchmarks and poses\nchallenges to current LVLMs. Our in-depth analysis of 15 representative LVLMs\nusing MMDU reveals that open-source LVLMs lag behind closed-source counterparts\ndue to limited conversational instruction tuning data. We demonstrate that\nffne-tuning open-source LVLMs on MMDU-45k signiffcantly address this gap,\ngenerating longer and more accurate conversations, and improving scores on MMDU\nand existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA:+1.2%). Our\ncontributions pave the way for bridging the gap between current LVLM models and\nreal-world application demands. This project is available at\nhttps://github.com/Liuziyu77/MMDU.",
        "updated": "2024-06-17 17:59:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11833v1"
    },
    {
        "title": "Learning sum of diverse features: computational hardness and efficient gradient-based training for ridge combinations",
        "authors": "Kazusato OkoYujin SongTaiji SuzukiDenny Wu",
        "links": "http://arxiv.org/abs/2406.11828v1",
        "entry_id": "http://arxiv.org/abs/2406.11828v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11828v1",
        "summary": "We study the computational and sample complexity of learning a target\nfunction $f_*:\\mathbb{R}^d\\to\\mathbb{R}$ with additive structure, that is,\n$f_*(x) = \\frac{1}{\\sqrt{M}}\\sum_{m=1}^M f_m(\\langle x, v_m\\rangle)$, where\n$f_1,f_2,...,f_M:\\mathbb{R}\\to\\mathbb{R}$ are nonlinear link functions of\nsingle-index models (ridge functions) with diverse and near-orthogonal index\nfeatures $\\{v_m\\}_{m=1}^M$, and the number of additive tasks $M$ grows with the\ndimensionality $M\\asymp d^\\gamma$ for $\\gamma\\ge 0$. This problem setting is\nmotivated by the classical additive model literature, the recent representation\nlearning theory of two-layer neural network, and large-scale pretraining where\nthe model simultaneously acquires a large number of \"skills\" that are often\nlocalized in distinct parts of the trained network. We prove that a large\nsubset of polynomial $f_*$ can be efficiently learned by gradient descent\ntraining of a two-layer neural network, with a polynomial statistical and\ncomputational complexity that depends on the number of tasks $M$ and the\ninformation exponent of $f_m$, despite the unknown link function and $M$\ngrowing with the dimensionality. We complement this learnability guarantee with\ncomputational hardness result by establishing statistical query (SQ) lower\nbounds for both the correlational SQ and full SQ algorithms.",
        "updated": "2024-06-17 17:59:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11828v1"
    },
    {
        "title": "WPO: Enhancing RLHF with Weighted Preference Optimization",
        "authors": "Wenxuan ZhouRavi AgrawalShujian ZhangSathish Reddy IndurthiSanqiang ZhaoKaiqiang SongSilei XuChenguang Zhu",
        "links": "http://arxiv.org/abs/2406.11827v1",
        "entry_id": "http://arxiv.org/abs/2406.11827v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11827v1",
        "summary": "Reinforcement learning from human feedback (RLHF) is a promising solution to\nalign large language models (LLMs) more closely with human values. Off-policy\npreference optimization, where the preference data is obtained from other\nmodels, is widely adopted due to its cost efficiency and scalability. However,\noff-policy preference optimization often suffers from a distributional gap\nbetween the policy used for data collection and the target policy, leading to\nsuboptimal optimization. In this paper, we propose a novel strategy to mitigate\nthis problem by simulating on-policy learning with off-policy preference data.\nOur Weighted Preference Optimization (WPO) method adapts off-policy data to\nresemble on-policy data more closely by reweighting preference pairs according\nto their probability under the current policy. This method not only addresses\nthe distributional gap problem but also enhances the optimization process\nwithout incurring additional costs. We validate our method on instruction\nfollowing benchmarks including Alpaca Eval 2 and MT-bench. WPO not only\noutperforms Direct Preference Optimization (DPO) by up to 5.6% on Alpaca Eval 2\nbut also establishes a remarkable length-controlled winning rate against\nGPT-4-turbo of 48.6% based on Llama-3-8B-Instruct, making it the strongest 8B\nmodel on the leaderboard. We will release the code and models at\nhttps://github.com/wzhouad/WPO.",
        "updated": "2024-06-17 17:59:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11827v1"
    },
    {
        "title": "Spectral Introspection Identifies Group Training Dynamics in Deep Neural Networks for Neuroimaging",
        "authors": "Bradley T. BakerVince D. CalhounSergey M. Plis",
        "links": "http://arxiv.org/abs/2406.11825v1",
        "entry_id": "http://arxiv.org/abs/2406.11825v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11825v1",
        "summary": "Neural networks, whice have had a profound effect on how researchers study\ncomplex phenomena, do so through a complex, nonlinear mathematical structure\nwhich can be difficult for human researchers to interpret. This obstacle can be\nespecially salient when researchers want to better understand the emergence of\nparticular model behaviors such as bias, overfitting, overparametrization, and\nmore. In Neuroimaging, the understanding of how such phenomena emerge is\nfundamental to preventing and informing users of the potential risks involved\nin practice. In this work, we present a novel introspection framework for Deep\nLearning on Neuroimaging data, which exploits the natural structure of gradient\ncomputations via the singular value decomposition of gradient components during\nreverse-mode auto-differentiation. Unlike post-hoc introspection techniques,\nwhich require fully-trained models for evaluation, our method allows for the\nstudy of training dynamics on the fly, and even more interestingly, allow for\nthe decomposition of gradients based on which samples belong to particular\ngroups of interest. We demonstrate how the gradient spectra for several common\ndeep learning models differ between schizophrenia and control participants from\nthe COBRE study, and illustrate how these trajectories may reveal specific\ntraining dynamics helpful for further analysis.",
        "updated": "2024-06-17 17:58:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11825v1"
    }
]