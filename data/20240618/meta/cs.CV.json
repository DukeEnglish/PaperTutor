[
    {
        "title": "LLaNA: Large Language and NeRF Assistant",
        "authors": "Andrea AmaduzziPierluigi Zama RamirezGiuseppe LisantiSamuele SaltiLuigi Di Stefano",
        "links": "http://arxiv.org/abs/2406.11840v1",
        "entry_id": "http://arxiv.org/abs/2406.11840v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11840v1",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated an excellent\nunderstanding of images and 3D data. However, both modalities have shortcomings\nin holistically capturing the appearance and geometry of objects. Meanwhile,\nNeural Radiance Fields (NeRFs), which encode information within the weights of\na simple Multi-Layer Perceptron (MLP), have emerged as an increasingly\nwidespread modality that simultaneously encodes the geometry and photorealistic\nappearance of objects. This paper investigates the feasibility and\neffectiveness of ingesting NeRF into MLLM. We create LLaNA, the first\ngeneral-purpose NeRF-language assistant capable of performing new tasks such as\nNeRF captioning and Q\\&A. Notably, our method directly processes the weights of\nthe NeRF's MLP to extract information about the represented objects without the\nneed to render images or materialize 3D data structures. Moreover, we build a\ndataset of NeRFs with text annotations for various NeRF-language tasks with no\nhuman intervention. Based on this dataset, we develop a benchmark to evaluate\nthe NeRF understanding capability of our method. Results show that processing\nNeRF weights performs favourably against extracting 2D or 3D representations\nfrom NeRFs.",
        "updated": "2024-06-17 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11840v1"
    },
    {
        "title": "Autoregressive Image Generation without Vector Quantization",
        "authors": "Tianhong LiYonglong TianHe LiMingyang DengKaiming He",
        "links": "http://arxiv.org/abs/2406.11838v1",
        "entry_id": "http://arxiv.org/abs/2406.11838v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11838v1",
        "summary": "Conventional wisdom holds that autoregressive models for image generation are\ntypically accompanied by vector-quantized tokens. We observe that while a\ndiscrete-valued space can facilitate representing a categorical distribution,\nit is not a necessity for autoregressive modeling. In this work, we propose to\nmodel the per-token probability distribution using a diffusion procedure, which\nallows us to apply autoregressive models in a continuous-valued space. Rather\nthan using categorical cross-entropy loss, we define a Diffusion Loss function\nto model the per-token probability. This approach eliminates the need for\ndiscrete-valued tokenizers. We evaluate its effectiveness across a wide range\nof cases, including standard autoregressive models and generalized masked\nautoregressive (MAR) variants. By removing vector quantization, our image\ngenerator achieves strong results while enjoying the speed advantage of\nsequence modeling. We hope this work will motivate the use of autoregressive\ngeneration in other continuous-valued domains and applications.",
        "updated": "2024-06-17 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11838v1"
    },
    {
        "title": "mDPO: Conditional Preference Optimization for Multimodal Large Language Models",
        "authors": "Fei WangWenxuan ZhouJames Y. HuangNan XuSheng ZhangHoifung PoonMuhao Chen",
        "links": "http://arxiv.org/abs/2406.11839v1",
        "entry_id": "http://arxiv.org/abs/2406.11839v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11839v1",
        "summary": "Direct preference optimization (DPO) has shown to be an effective method for\nlarge language model (LLM) alignment. Recent works have attempted to apply DPO\nto multimodal scenarios but have found it challenging to achieve consistent\nimprovement. Through a comparative experiment, we identify the unconditional\npreference problem in multimodal preference optimization, where the model\noverlooks the image condition. To address this problem, we propose mDPO, a\nmultimodal DPO objective that prevents the over-prioritization of language-only\npreferences by also optimizing image preference. Moreover, we introduce a\nreward anchor that forces the reward to be positive for chosen responses,\nthereby avoiding the decrease in their likelihood -- an intrinsic problem of\nrelative preference optimization. Experiments on two multimodal LLMs of\ndifferent sizes and three widely used benchmarks demonstrate that mDPO\neffectively addresses the unconditional preference problem in multimodal\npreference optimization and significantly improves model performance,\nparticularly in reducing hallucination.",
        "updated": "2024-06-17 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11839v1"
    },
    {
        "title": "Scaling the Codebook Size of VQGAN to 100,000 with a Utilization Rate of 99%",
        "authors": "Lei ZhuFangyun WeiYanye LuDong Chen",
        "links": "http://arxiv.org/abs/2406.11837v1",
        "entry_id": "http://arxiv.org/abs/2406.11837v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11837v1",
        "summary": "In the realm of image quantization exemplified by VQGAN, the process encodes\nimages into discrete tokens drawn from a codebook with a predefined size.\nRecent advancements, particularly with LLAMA 3, reveal that enlarging the\ncodebook significantly enhances model performance. However, VQGAN and its\nderivatives, such as VQGAN-FC (Factorized Codes) and VQGAN-EMA, continue to\ngrapple with challenges related to expanding the codebook size and enhancing\ncodebook utilization. For instance, VQGAN-FC is restricted to learning a\ncodebook with a maximum size of 16,384, maintaining a typically low utilization\nrate of less than 12% on ImageNet. In this work, we propose a novel image\nquantization model named VQGAN-LC (Large Codebook), which extends the codebook\nsize to 100,000, achieving an utilization rate exceeding 99%. Unlike previous\nmethods that optimize each codebook entry, our approach begins with a codebook\ninitialized with 100,000 features extracted by a pre-trained vision encoder.\nOptimization then focuses on training a projector that aligns the entire\ncodebook with the feature distributions of the encoder in VQGAN-LC. We\ndemonstrate the superior performance of our model over its counterparts across\na variety of tasks, including image reconstruction, image classification,\nauto-regressive image generation using GPT, and image creation with diffusion-\nand flow-based generative models. Code and models are available at\nhttps://github.com/zh460045050/VQGAN-LC.",
        "updated": "2024-06-17 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11837v1"
    },
    {
        "title": "OoDIS: Anomaly Instance Segmentation Benchmark",
        "authors": "Alexey NekrasovRui ZhouMiriam AckermannAlexander HermansBastian LeibeMatthias Rottmann",
        "links": "http://arxiv.org/abs/2406.11835v1",
        "entry_id": "http://arxiv.org/abs/2406.11835v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11835v1",
        "summary": "Autonomous vehicles require a precise understanding of their environment to\nnavigate safely. Reliable identification of unknown objects, especially those\nthat are absent during training, such as wild animals, is critical due to their\npotential to cause serious accidents. Significant progress in semantic\nsegmentation of anomalies has been driven by the availability of\nout-of-distribution (OOD) benchmarks. However, a comprehensive understanding of\nscene dynamics requires the segmentation of individual objects, and thus the\nsegmentation of instances is essential. Development in this area has been\nlagging, largely due to the lack of dedicated benchmarks. To address this gap,\nwe have extended the most commonly used anomaly segmentation benchmarks to\ninclude the instance segmentation task. Our evaluation of anomaly instance\nsegmentation methods shows that this challenge remains an unsolved problem. The\nbenchmark website and the competition page can be found at:\nhttps://vision.rwth-aachen.de/oodis .",
        "updated": "2024-06-17 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11835v1"
    }
]