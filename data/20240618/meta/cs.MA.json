[
    {
        "title": "Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical Questioning for Socratic Code Debugging",
        "authors": "Priyanka KarguptaIshika AgarwalDilek Hakkani-TurJiawei Han",
        "links": "http://arxiv.org/abs/2406.11709v1",
        "entry_id": "http://arxiv.org/abs/2406.11709v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11709v1",
        "summary": "Socratic questioning is an effective teaching strategy, encouraging critical\nthinking and problem-solving. The conversational capabilities of large language\nmodels (LLMs) show great potential for providing scalable, real-time student\nguidance. However, current LLMs often give away solutions directly, making them\nineffective instructors. We tackle this issue in the code debugging domain with\nTreeInstruct, an Instructor agent guided by a novel state space-based planning\nalgorithm. TreeInstruct asks probing questions to help students independently\nidentify and resolve errors. It estimates a student's conceptual and\nsyntactical knowledge to dynamically construct a question tree based on their\nresponses and current knowledge state, effectively addressing both independent\nand dependent mistakes concurrently in a multi-turn interaction setting. In\naddition to using an existing single-bug debugging benchmark, we construct a\nmore challenging multi-bug dataset of 150 coding problems, incorrect solutions,\nand bug fixes -- all carefully constructed and annotated by experts. Extensive\nevaluation shows TreeInstruct's state-of-the-art performance on both datasets,\nproving it to be a more effective instructor than baselines. Furthermore, a\nreal-world case study with five students of varying skill levels further\ndemonstrates TreeInstruct's ability to guide students to debug their code\nefficiently with minimal turns and highly Socratic questioning.",
        "updated": "2024-06-17 16:28:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11709v1"
    },
    {
        "title": "Decentralized Collaborative Pricing and Shunting for Multiple EV Charging Stations Based on Multi-Agent Reinforcement Learning",
        "authors": "Tianhao BuHang LiGuojie Li",
        "links": "http://arxiv.org/abs/2406.11496v1",
        "entry_id": "http://arxiv.org/abs/2406.11496v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11496v1",
        "summary": "The extraordinary electric vehicle (EV) popularization in the recent years\nhas facilitated research studies in alleviating EV energy charging demand.\nPrevious studies primarily focused on the optimizations over charging stations\n(CS) profit and EV users cost savings through charge/discharge scheduling\nevents. In this work, the random behaviors of EVs are considered, with EV users\npreferences over multi-CS characteristics modelled to imitate the potential CS\nselection disequilibrium. A price scheduling strategy under decentralized\ncollaborative framework is proposed to achieve EV shunting in a multi-CS\nenvironment, while minimizing the charging cost through multi agent\nreinforcement learning. The proposed problem is formulated as a Markov Decision\nProcess (MDP) with uncertain transition probability.",
        "updated": "2024-06-17 12:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11496v1"
    },
    {
        "title": "KAOS: Large Model Multi-Agent Operating System",
        "authors": "Zhao ZhuoRongzhen LiKai LiuHuhai ZouKaiMao LiJie YuTianhao SunQingbo Wu",
        "links": "http://arxiv.org/abs/2406.11342v1",
        "entry_id": "http://arxiv.org/abs/2406.11342v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11342v1",
        "summary": "The intelligent interaction model based on large models reduces the\ndifferences in user experience across various system platforms but faces\nchallenges in multi-agent collaboration and resource sharing. To demonstrate a\nuniform user experience across different foundational software platforms and\naddress resource coordination management challenges, this paper proposes a\nmulti-agent operating system based on the open-source Kylin. The research\nmethod involves empowering agents with large models to serve applications.\nFirst, by introducing management role agents and vertical multi-agent\ncollaboration to construct or replace typical application software. Second, by\nstudying system-level shared resource scheduling strategies to enhance user\nexperience and optimize resource utilization. And finally, by validating the\nefficiency and superiority of the large model multi-agent operating system\nthrough real applications and scoring intelligence. The feasibility of this\nsystem is demonstrated, providing a new perspective for the development of\nmulti-agent operating systems. Experimental results show significant advantages\nof multi-agent collaboration in various application scenarios.",
        "updated": "2024-06-17 08:59:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11342v1"
    },
    {
        "title": "Reconfigurable Intelligent Surface Assisted VEC Based on Multi-Agent Reinforcement Learning",
        "authors": "Kangwei QiQiong WuPingyi FanNan ChengQiang FanJiangzhou Wang",
        "links": "http://arxiv.org/abs/2406.11318v1",
        "entry_id": "http://arxiv.org/abs/2406.11318v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11318v1",
        "summary": "Vehicular edge computing (VEC) is an emerging technology that enables\nvehicles to perform high-intensity tasks by executing tasks locally or\noffloading them to nearby edge devices. However, obstacles such as buildings\nmay degrade the communications and incur communication interruptions, and thus\nthe vehicle may not meet the requirement for task offloading. Reconfigurable\nintelligent surfaces (RIS) is introduced to support vehicle communication and\nprovide an alternative communication path. The system performance can be\nimproved by flexibly adjusting the phase-shift of the RIS. For RIS-assisted VEC\nsystem where tasks arrive randomly, we design a control scheme that considers\noffloading power, local power allocation and phase-shift optimization. To solve\nthis non-convex problem, we propose a new deep reinforcement learning (DRL)\nframework that employs modified multi-agent deep deterministic policy gradient\n(MADDPG) approach to optimize the power allocation for vehicle users (VUs) and\nblock coordinate descent (BCD) algorithm to optimize the phase-shift of the\nRIS. Simulation results show that our proposed scheme outperforms the\ncentralized deep deterministic policy gradient (DDPG) scheme and random scheme.",
        "updated": "2024-06-17 08:35:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11318v1"
    },
    {
        "title": "The Benefits of Power Regularization in Cooperative Reinforcement Learning",
        "authors": "Michelle LiMichael Dennis",
        "links": "http://arxiv.org/abs/2406.11240v1",
        "entry_id": "http://arxiv.org/abs/2406.11240v1",
        "pdf_url": "http://arxiv.org/pdf/2406.11240v1",
        "summary": "Cooperative Multi-Agent Reinforcement Learning (MARL) algorithms, trained\nonly to optimize task reward, can lead to a concentration of power where the\nfailure or adversarial intent of a single agent could decimate the reward of\nevery agent in the system. In the context of teams of people, it is often\nuseful to explicitly consider how power is distributed to ensure no person\nbecomes a single point of failure. Here, we argue that explicitly regularizing\nthe concentration of power in cooperative RL systems can result in systems\nwhich are more robust to single agent failure, adversarial attacks, and\nincentive changes of co-players. To this end, we define a practical pairwise\nmeasure of power that captures the ability of any co-player to influence the\nego agent's reward, and then propose a power-regularized objective which\nbalances task reward and power concentration. Given this new objective, we show\nthat there always exists an equilibrium where every agent is playing a\npower-regularized best-response balancing power and task reward. Moreover, we\npresent two algorithms for training agents towards this power-regularized\nobjective: Sample Based Power Regularization (SBPR), which injects adversarial\ndata during training; and Power Regularization via Intrinsic Motivation (PRIM),\nwhich adds an intrinsic motivation to regulate power to the training objective.\nOur experiments demonstrate that both algorithms successfully balance task\nreward and power, leading to lower power behavior than the baseline of\ntask-only reward and avoid catastrophic events in case an agent in the system\ngoes off-policy.",
        "updated": "2024-06-17 06:10:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.11240v1"
    }
]