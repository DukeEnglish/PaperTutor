1
ESI-GAL: EEG Source Imaging-based Kinemaics
Parameter Estimation for Grasp and Lift Task
Anant Jain and Lalan Kumar
Abstract— in computational power and machine learning algorithms
Objective. Electroencephalogram (EEG) signals-based motor have empowered BCI systems to decode neural signals for
kinematicsprediction(MKP)hasbeenanactiveareaofresearch
assisting, augmenting, or restoring motor functionality [11].
to develop brain-computer interface (BCI) systems such as
BCI systems encompass a series of sequential procedures
exosuits, prostheses, and rehabilitation devices. However, EEG
source imaging (ESI) based kinematics prediction is sparsely comprising the acquisition and processing of neural signals,
explored in the literature. relevant feature extraction, intention detection, and user
Approach.Inthisstudy,pre-movementEEGfeaturesareutilized feedback signals generation. Invasive and non-invasive
topredictthree-dimensional(3D)handkinematicsforthegrasp-
recording techniques are utilized for capturing neural activity.
and-liftmotortask.Apublicdataset,WAY-EEG-GAL,isutilized While invasive approaches offer greater accuracy in brain
for MKP analysis. In particular, sensor-domain (EEG data) and
source-domain(ESIdata)basedfeaturesfromthefrontoparietal activity recognition, they necessitate surgical implantation of
region are explored for MKP. Deep learning-based models are sensors beneath the scalp [12], [13]. Conversely, non-invasive
explored to achieve efficient kinematics decoding. Various time- BCIs capture neural signals by pacing sensors on the scalp.
lagged and window sizes are analyzed for hand kinematics
Various non-invasive techniques include functional near-
prediction. Subsequently, intra-subject and inter-subject MKP
infrared spectroscopy (fNIRS) [14], magnetoencephalography
analysis is performed to investigate the subject-specific and
subject-independent motor-learning capabilities of the neural (MEG) [15], functional magnetic resonance imaging (fMRI)
decoders. The Pearson correlation coefficient (PCC) is used as [16] and electroencephalogram (EEG) [17].
the performance metric for kinematics trajectory decoding. EEG-based BCI systems have gained popularity among
Main results. The rEEGNet neural decoder achieved the best
these methods due to their high temporal resolution, porta-
performance with sensor-domain and source-domain features
bility, and cost-effectiveness. They have been utilized across
with the time lag and window size of 100 ms and 450 ms,
respectively. The highest mean PCC values of 0.790, 0.795, and various applications, such as fatigue and drowsiness detection
0.637 are achieved using sensor-domain features, while 0.769, [18], emotion recognition [19], wearable exoskeletons [20],
0.777,and0.647areachievedusingsource-domainfeaturesinx, [21], and robotic control [22]–[24]. Additionally, EEG-based
y, and z-directions, respectively.
BCIs have been employed for classifying motor imagery
Significance. This study explores the feasibility of trajectory
or motor execution tasks [25]–[27]. Although classification-
prediction using EEG sensor-domain and source-domain EEG
features for the grasp-and-lift task. Furthermore, inter-subject based approaches have been extensively explored, continuous
trajectory estimation is performed using the proposed deep kinematic estimation-based approaches offer enhanced perfor-
learning decoder with EEG source domain features. mance and efficient control of external devices such as neural
Index Terms—Brain computer interface (BCI), electroen- prostheses, exosuits, or exoskeletons.
cephalography, Deep learning, Motor kinematics prediction
(MKP),inter-subjectdecoding,EEG,sourceimaging,sLORETA
B. Related work
EEG-basedkinematicsdecodinghasbeenperformedinboth
I. INTRODUCTION motor execution task [28]–[30] and motor imagery task [31].
EEG-based kinematics decoding is performed for 2D center-
A. Background
out reaching task in [32] using multiple linear regression
Brain−computer interface (BCI), also known as (mLR)decoder.EEGslowcorticalpotentialsbasedupperlimb
brain−machine interfaces (BMIs), represent a cutting- trajectories(hand,elbow,andshoulder)decodingisreportedin
edge technology that facilitates control over external devices [33] using mLR decoding model. Hand kinematics decoding
through brain activity, bypassing the need for peripheral forunimanualtarget-reachingmovementisinvestigatedin[34]
nerves and muscles [1]. This technology holds significant with Kalman filter (KF) and mLR decoders using scalp EEG
promise for enhancing the life quality of individuals with signals. Recent studies have utilized deep learning-based de-
motor disabilities [2]–[7] and enables interaction with coders[35]–[40]formotorkinematicsprediction(MKP)using
healthy individuals [8]–[10]. Over time, advancements scalp EEG signals. Knee joint angle trajectory is estimated
using EEG signals with the NARX neural network in [35].
Anant Jain is with the Department of Electrical Engineering, Indian
EEG-based MKP is investigated in [36] using a convolutional
Institute of Technology Delhi, New Delhi 110016, India (e-mail: anant-
jain@ee.iitd.ac.in). neuralnetwork-bidirectionallongshort-termmemory(CNN-
Lalan Kumar is with the Department of Electrical Engineering, Bharti biLSTM) decoding model during unimanual target-reaching
School of Telecommunication, and Yardi School of Artificial Intelligence,
movementforcontrollingaroboticarm.MKPisperformedfor
Indian Institute of Technology Delhi, New Delhi 110016, India (e-mail:
lkumar@ee.iitd.ac.in). the grasp-and-lift task using a wavelet packet decomposition
4202
nuJ
71
]PS.ssee[
1v00511.6042:viXra2
(WPD) based CNN-LSTM decoder in [38]. Low-frequency (Section II-C), deep-learning models (Section II-D), and ex-
EEG signals are utilized in [37] for MKP with CNN-LSTM perimental details (Section II-E). Performance evaluation for
decoding architecture. The inter-subject decoding analysis MKP and an extensive discussion of the results are detailed
is investigated in [39] using EEG signals along with deep in Section III and Section IV provides the conclusions about
learning-based decoding models. Elbow joint angle trajectory the research work.
decoding is performed in [40] that utilized an attention-based
CNN-LSTM decoder with EEG signals during the biceps-curl II. MATERIALSANDMETHODS
task.
This Section includes a detailed description of the experi-
Inthefieldofbrain-computerinterface(BCI)research,non-
mentalparadigm,dataset,datapre-processingpipeline,feature
invasive investigations have traditionally focused on utilizing
extraction, EEG source imagining, and neural decoders for
EEGsignalsdirectly,operatingwithinthesensorspace.How-
trajectory estimation. An overview of the proposed trajectory
ever, advancements in EEG source imaging (ESI) techniques
estimation framework is shown in Fig. 1.
enable the inference of cortical sources from non-invasive
brain signals. This approach has prompted studies exploring
decoding brain activity in the source space via ESI. The A. Dataset Description
application of source-space decoding has predominantly been
WAY-EEG-GAL (Wearable interfaces for hAnd function
investigated within the classification domain, encompassing
recovery EEG grasp and lift) dataset [49] is used for MKP.
tasks such as motor imagery classification [41]–[43], arm
Synchronous EEG and hand kinematics data were collected
directionclassification[44]andgesture(alphabets)recognition
during the grasp-and-lift task. EEG data was recorded using
[45]. Across these studies, source-space classification has
32-channel active EEG electrodes (actiCAP, Brain Products
frequently demonstrated enhanced performance compared to
GmbH, Germany) in conjunction with a biosignal amplifier
traditional sensor space methods. ESI-based MKP has been
(BrainAmp, Brain Products GmbH, Germany) at a 500 Hz
exploredforreach-to-targettask[46],snaketrajectorytracking
sampling rate. The hand kinematics data was acquired using
[47] and grasp-and-lift task [48]. In [46], the mLR decoder is
a 3D position sensor (FASTRAK, Polhemus Inc., USA) with
deployed to estimate hand, elbow, and shoulder trajectories
a 500 Hz sampling frequency. In particular, the neural and
during a target-reaching experiment utilizing EEG current
kinematics data of twelve participants while performing the
source dipoles. The study reported an average correlation
right-hand grasp-and-lift task is included in the dataset.
of 0.36 ± 0.13 across all trajectories for actual movement
Ineachtrial,participantswereinstructedtoperformagrasp-
execution.Source-spaceEEG-basedhandtrajectoryestimation
and-lift task on an object. The task involved reaching for a
is conducted by employing a combination of the partial least
small object, gripping it between the index finger and thumb,
squares(PLS)regressionmodelandthesquare-rootunscented
elevatingitafewcentimetersintotheair,maintainingastable
Kalman filter (SR-UKF) in [47]. The study reported the
hold briefly,and replacing and releasingit to itsoriginal posi-
highest average correlation of 0.31 ± 0.09 and 0.35 ± 0.09
tion. The initiation of the reaching motion and the controlled
for hand position and velocity, respectively. In [48], hand
lowering of the object were prompted by a visual cue in
trajectory estimation is performed for the grasp-and-lift task
the form of an LED signal. The participants had autonomy
usingEEGsource-domainsignalswitharesidualCNN-LSTM
over the overall pace of the task, allowing for a naturalistic
neural decoder. The mean Pearson correlation coefficient of
execution. Throughout the experimental sessions, the object’s
0.59, 0.61, and 0.56 is reported for the window size of 300
properties underwentsystematic variations inphysical proper-
ms in the x, y, and z directions, respectively.
ties. These variations encompassed unpredictable changes in
weight(165,330,or660g)andthecontactsurface(sandpaper,
C. Objectives and Contribution
suede, or silk).
In this study, MKP is performed using sensor-domain and
source-domain EEG features for the grasp-and-lift task. Pre-
B. Data Preprocessing
movement EEG segments are utilized for kinematics estima-
tion along with convolutional neural network (CNN) based Neural data preprocessing is performed using the open-
deep learning models for external device (exosuit, prosthesis) source plugin EEGLAB [50] in the MATLAB software pack-
control-based BCI applications. The open-source WAY-EEG- age. Initially, the raw EEG data is band-pass filtered using
GAL[49]databaseisusedfortheproposedmethodology.The a zero-phase Hamming windowed sync FIR filter ranging
sensor-domain and source-domain features are explored for between 0.1 and 40 Hz to eliminate artifacts in the low band
motorkinematicspredictionduringthegrasp-and-lifttask.The (e.g., baseline drift and motion artifacts) and high band (e.g.,
frontoparietal regions are utilized as regions of interest (ROI) electromyogram (EMG)). Subsequently, the filtered EEG data
for both sensor-domain and source-domain-based feature se- is re-referenced using common average referencing (CAR),
lection. Additionally, inter-subject decoding performance is followed by Independent Component Analysis (ICA) to elim-
evaluated to demonstrate the subject-adaptability of the pro- inate artifacts related to electrooculography (EOG) and EMG.
posed kinematics decoding framework. Thenumberofremovedindependentcomponentsrangedfrom
The organization of the article is as follows: Section II in- three to six across subjects, with the removed components
cludes the dataset description (Section II-A), data preprocess- primarily projecting over frontal cortical areas following the
ing(SectionII-B),EEGsourceImagingandfeatureextraction inverse ICA transform. Further, downsampling of denoised3
Fig.1. FlowchartofproposedkinematicsdecodingframeworkforGrasp-and-Lifttask.
EEG data to 100 Hz sampling rate is performed for reduction scalp and the current activations originating from the cortical
of the computational cost. Eighteen EEG sensors around the surface sources. Subsequently, the solution to the inverse
motor-cortex region are selected for MKP. problem aims to identify the cortical current distribution that
Two key steps were involved in kinematic data prepro- alignsthescalppotentialwithinapredefinedsetofconstraints.
cessing to eliminate measurement noise and signal scaling. It can be noted that scalp EEG data undergoes mapping onto
First, a low-pass filter with a cutoff frequency of 2 Hz is a higher-dimensional cortical source grid with typical EEG
applied to remove measurement noise from raw kinematics sensors of around 30 and an estimated number of source
data.Forscaling,min-maxnormalizationisemployedtoscale dipoles of around 15,000. Due to the significant difference
the filtered data onto a range of [0, 1] as follows: in the number of sensors and sources, ESI becomes a highly
underdetermined problem. The two stages involved in the ESI
are further detailed below:
k[t]−k
K[t]= min (1)
k −k 1) Forward Problem: The computation of the lead field
max min
matrix is carried out in the forward modeling stage. The lead
where,K[t]andk[t]arethenormalizedandmeasuredposition
fieldmatrixdepictsthetransmissionofcorticalsourcecurrents
coordinate values, respectively. k and k are maximum
max min through various conductivity layers to the scalp electrodes.
and minimum hand position coordinates, respectively. For
This procedure integrates the principles of EEG generation,
scaling the kinematics data in the range of [0,1], min-max
incorporatingconsiderationsofNeumannandDirichletbound-
normalizationisperformedforthex,y,andzpositioncoordi-
ary conditions to establish the relationship between voltages
nates.Further,thenormalizedkinematicsdataisdownsampled
recorded at the scalp and cortical current densities. The
from 500 Hz to 100 Hz sampling rate.
mathematical relationship can be expressed as:
(cid:2) (cid:3) (cid:2) (cid:3) (cid:2) (cid:3) (cid:2) (cid:3)
C. EEG Source Imaging E = A S + η (1)
I×Ns I×K K×Ns I×Ns
As the neural electrical signals transmit from the cortical
surfacetothescalp,theyundergodistortionanddispersiondue where, E represents pre-processed scalp EEG data, I is the
to volume-conduction effects. EEG source imaging (ESI) has totalnumberofEEGsensors(=32),Aisthelead-fieldmatrix,
thepotentialtomitigatecross-electrodecorrelationthatresults S denotes source activity, K is the total number of source
due to volume conduction effects. The fundamental principle dipoles, and η is the measured noise.
behind ESI involves the estimation of primary cortical current Theboundaryelementmethod(BEM)isutilizedtocompute
activations based on the recorded scalp EEG signals. Two the head model by employing the ICBM152 MRI template
primarystagesareinvolvedinESI:thesolutionoftheforward [51] as the anatomy and subsequently co-registering with
problemandtheinverseproblem.Thesolutionfortheforward the positions of the EEG electrodes. The forward modeling
problem employs an anatomical head model to define the is performed using the OpenMEEG [52] with parameters
mappingbetweenthevoltagesdetectedbyEEGsensorsonthe σ =1,σ =0.0125,andσ =1.Inparticular,the
scalp skull brain4
Brainstorm toolbox [53] is utilized for EEG source imaging a comprehensive description is provided in the subsequent
(ESI). subsections:
2) Inverse Problem: The objective for solving the ESI 1) rEEGNet: It is a CNN-based compact architecture em-
inverse problem is to estimate the cortical source current ployed across various BCI tasks. The compact nature of
signal [S] using the lead field matrix [A], derived from the the decoder offers the advantage of efficient training with
head model computation. A noise perturbation matrix [η] is limited data. This study uses a modified version of EEGNet
introduced to account for errors resulting from solving the ill- architecture for trajectory estimation. The model architecture
posed and ill-conditioned inverse problem. In this study, the can be segmented into two stages. In the first stage, a 2D
inverse problem is solved using the standard low-resolution convolutional (Conv) layer comprising 32 filters with kernel
electrical tomography (sLORETA) [54] method. sLORETA is size (1, 32) is applied to the input signals. Afterward, a
utilizedwithnormaltocortexdipoleorientationandminimum depthwise Conv layer is employed. The kernel size of the
norm imaging. depthwise Conv layer is (18, 1) and (15, 1) for the sensor
As a result of the transformation into source space, the domain and source domain input features, respectively, due to
number of signals increases to 15,000 source signals from 32 thevariationininputchannels.Batchnormalizationisapplied
scalp EEG signals. The primary aim of regions of interest following both Conv layers. The linear activation function is
(ROI) scouting is to sub-group the number of signals within utilized with the 2D CNN layer, while the Exponential Linear
the source space based on different brain regions. The ROIs Unit(ELU)activationfunctionisusedforthedepthwiseCNN
ofthebraincorticalareaaredefinedbasedontheMindboggle layer.TheweightsofthedepthwiseCNNlayerareregularized
atlas [55] for the ICBM152 MRI template. There are a total with a maximum norm weight constraint of 1. A 2D average
of 62 ROIs defined for the Mindboggle altas. However, even pooling layer with the size of (1, 2) is employed at the output
defined ROIs consist of signals in the order of hundreds. stagealongwithadropoutlayerwitharateof0.5toregularize
Hence, the mean of cortical source signals across each ROI the decoder. The second stage of the decoder included a
for each timestamp is utilized. Further, ROIs are selected separable Conv layer with 96 filters, each of size (1, 16), a
in the frontoparietal region of the brain. The selected ROIs batchnormalizationlayer,andanactivationfunctionlayerwith
are caudalmiddlefrontal (CMF), lateralorbitofrontal (LOF), ELU activation function. Subsequently, a 2D average pooling
medialorbitofrontal (MOFL), superiorfrontal (SF), paracentral layer is applied with a size of (1, 4), and a feature vector is
(PCL), postcentral (PoCL), precentral (PreCL), superiorpari- obtained by flattening the output of the average pooling layer.
etal (SP) and inferiorparietal (IP). In particular, a total of 15 Further, the feature vector is fed to the output dense layer
ROIs were selected for the trajectory decoding, as shown in consisting of three neurons and the linear activation function.
Fig. 2. The incorporation of depthwise and separable Conv layers
resultsinasignificantreductioninthemodelparametercount,
rendering a more compact model. Table I shows the model
architecture details for sensor domain input features.
2) rDeepConvNet: DeepConvNet model includes a total of
four convolution blocks. The first block of the model consists
oftwo2DConvlayersfollowedbyabatchnormalizationlayer
and an activation layer with the ELU activation function. The
first Conv layer has the size of (1, 5); however, the kernel
size of the second Conv layer is (18, 1) and (15, 1) for sensor
domain and source domain input features, respectively. The
remainingthreeblockshaveidenticalconfigurations,witheach
block consisting of a 2D Conv layer of kernel size (1, 5),
a batch normalization layer, and an activation layer with an
ELUactivationlayer.Thelastblockisfollowedbya2DMax-
poolinglayerofkernelsizeandstride(1,2).EachConvlayer
Fig.2. Selected15RegionofInterest(ROI)usingMindboggleAtlas
consists of 25 filters in the first block; the number of filters
in subsequent blocks is doubled, as shown in Table II. The
fourth block output is flattened and fed to the output dense
D. Decoding Models
layer with three neurons and linear activation.
Convolutional neural network (CNN) based Deep Learning 3) rShallowConvNet: The ShallowConvNet model consists
architectures have achieved high accuracy for various EEG- oftwo2DConvlayerswith40filtersandthelinearactivation
based BCI applications. In this study, we have utilized mod- function. The first 2D Conv layer has a kernel size of (1,
ified versions of three popular architectures, EEGNet [56], 13), while the kernel size of the second Conv layer is (18,
DeepConvNet [57], and ShallowConvNet [57], for continuous 1) and (15, 1) for sensor and source domain input features,
trajectoryestimation.Themodifiedversionsofthemodelsare respectively. The output of the Conv layers undergoes batch
depictedasrEEGNET,rDeepConvNet,andrShallowConvNet, normalization, followed by the application of the square ac-
where ’r’ is the abbreviation for ’regression’ here. The model tivation function. Subsequently, a 2D average-pooling layer
architectures are summarized in tables I to III; however, of kernel size and stride (1, 5) and (1, 2), respectively, is5
TABLEI denotes the mean and standard deviation of the vn signal,
MODELARCHITECTUREOFREEGNETDECODERFOREEG respectively.
SENSOR-DOMAINFEATURESINPUT.
Normalized sensor and source domain time series are
Layer KernelSize #offilters LayerParameters utilized as input to neural decoders. For each trial, hand
Conv2D (1,32) 32 Stride=(1,1),Activation=Linear kinematics data is utilized from the movement initiation until
BatchNorm - - -
DepthwiseConv2D (18,1) - Depthmultiplier=3 theparticipantreinstatedthehandtotheinitialrestingposition.
BatchNorm - - -
Time-laggedEEGsensorsandsourcetimeseriessegmentsthat
Activation - - Activation=ELU
AveragePooling2D (1,2) - Stride=(1,2) included the pre-motor neural information for hand trajectory
Dropout - - DropoutRate=0.5
SeperableConv2D (1,16) 96 ZeroPadding decoding are utilized. This study considered various window
BatchNorm - - - sizes (250 ms to 450 ms) and lags (50 ms to 200 ms) with
Activation - - Activation=ELU
AveragePooling2D (1,4) - Stride=(1,4) EEG data up to 650 ms prior to movement onset for the
Dropout - - DropoutRate=0.5
Flatten - - - decoding analysis. The input to the neural decoders was a
Dense - - Neurons=3,Activation=Linear 2D matrix of dimension C ×W, where C is the number of
selected channels and W is the window size. The number of
TABLEII
selected channels (C) for the sensor and source domain time
MODELARCHITECTUREOFRDEEPCONVNETDECODERFOREEG
SENSOR-DOMAINFEATURESINPUT. series are 18 and 15, respectively.
The dataset is divided into three data subsets for the
Layer KernelSize #offilters LayerParameters
performance evaluation of the neural decoders: training, val-
Conv2D (1,5) 25 Stride=(1,1),Activation=Linear
Conv2D (18,1) 25 Stride=(1,1),Activation=Linear idation, and test datasets. Model training is performed using
BatchNorm - - -
the training dataset, while the validation dataset is utilized
Activation - - Activation=ELU
Dropout - - DropoutRate=0.5 for tuning model hyper-parameters and avoiding over-fitting.
Conv2D (1,5) 50 Stride=(1,1),Activation=Linear
BatchNorm - - - The performance of the trained model was evaluated on the
Activation - - Activation=ELU
test dataset. Data corresponding to 294 trials is available for
Dropout - - DropoutRate=0.5
Conv2D (1,5) 100 Stride=(1,1),Activation=Linear each participant in the WAY-EEG-GAL dataset. For intra-
BatchNorm - - -
Activation - - Activation=ELU subjectdecodinganalysis,thedataforeachparticipantissub-
Dropout - - DropoutRate=0.5 divided into training, validation, and test data corresponding
Conv2D (1,5) 200 Stride=(1,1),Activation=Linear
BatchNorm - - - to 234 trials, 30 trials, and 30 trials, respectively. For inter-
Activation - - Activation=ELU
subject decoding analysis, a 4-fold cross-validation strategy
MaxPooling2D (1,2) - Stride=(1,2)
Dropout - - DropoutRate=0.5 is adopted. Training and validation are carried out using
Flatten - - -
Dense - - Neurons=3,Activation=Linear data from nine subjects, and the trained model is evaluated
using data corresponding to the remaining three subjects.
TABLEIII The model training is performed using a mini-batch training
MODELARCHITECTUREOFRSHALLOWCONVNETDECODERFOREEG processwithabatchsizeof64.Themean-squarederror(mse)
SENSOR-DOMAINFEATURESINPUT.
loss is minimized using the Adam optimization algorithm to
Layer KernelSize #offilters LayerParameters define model parameters. The model undergoes training for
Conv2D (1,13) 40 Stride=(1,1),Activation=Linear
a maximum of 400 epochs, and to prevent overfitting, early
Conv2D (18,1) 40 Stride=(1,1),Activation=Linear
BatchNorm - - - stopping is implemented based on the validation set mse loss,
Activation - - Activation=Square
AveragePooling2D (1,5) - Stride=(1,2) with a patience of 5 epochs.
Activation - - Activation=log
Dropout - - DropoutRate=0.5
Flatten - - - III. RESULTSANDDISCUSSION
Dense - - Neurons=3,Activation=Linear
A. Performance Metric
The Pearson correlation coefficient (PCC) is employed as
utilized along with a logarithmic activation function. Further,
a performance metric for evaluating the MKP efficacy of
the output of the average pooling layer is flattened to a vector
the decoding models. This coefficient is calculated across
and fed to the output-dense layer with three neurons with the
measured and predicted kinematics trajectory, representing
linear activation function. A dropout layer with a drop rate of
linear correlation with output values in the range [−1,1]. A
0.5 is implemented to prevent overfitting.
PCCvalueof-1and+1indicatesastrongnegativeandpositive
correlation,respectively,whileaPCCvalueof0representsno
E. Experimental Details
correlation. The mathematical expression that represents the
ThedatanormalizationisperformedontheEEGsensorand
Pearsoncorrelationcoefficientbetweenthemeasured(C )and
x
source time series using the z-score normalization technique,
estimated (C ) is depicted as follows:
y
depicted as
Vn[t]=
vn[t] ρ−Π
vn (2) Π(C x,C y)=
T
−1 1(cid:88)T (cid:18) C xi β− CxαCx(cid:19)(cid:32) C yi β− CyαCy(cid:33)
(3)
vn i=1
where, at time instant t, vn[t] and Vn[t] are the nth channel where,α andβ arethemeanandstandarddeviationofm,
m m
denoised and normalized signal, respectively. Π and ρ respectively, with m∈{C ,C }.
vn vn x y6
TABLEIV
MEANPCCVALUESFORINTRA-SUBJECTTRAJECTORYDECODINGINTHEX,Y,ANDZDIRECTIONSUSINGEEGSENSOR-DOMAINAND
SOURCE-DOMAINTIMESERIESINPUT.THEEFFECTONPCCVALUESUSINGDIFFERENTEEGLAGANDWINDOWSIZESWITHDIFFERENTDECODING
MODELSISALSODEPICTED.
SensorDomain SourceDomain
Direction Decoders EEGLag EEGWindow EEGWindow
250 300 350 400 450 250 300 350 400 450
50 0.340 0.351 0.369 0.374 0.385 0.410 0.420 0.429 0.429 0.433
100 0.333 0.353 0.361 0.374 0.395 0.402 0.414 0.417 0.423 0.451
mLR
150 0.335 0.346 0.361 0.383 0.384 0.396 0.402 0.411 0.441 0.442
200 0.329 0.346 0.369 0.372 0.382 0.384 0.396 0.429 0.431 0.431
50 0.731 0.734 0.727 0.723 0.714 0.706 0.707 0.710 0.698 0.692
100 0.736 0.734 0.730 0.717 0.723 0.709 0.710 0.706 0.704 0.700
rSCNet
150 0.744 0.735 0.726 0.725 0.717 0.713 0.706 0.701 0.705 0.699
200 0.738 0.726 0.727 0.722 0.715 0.708 0.696 0.702 0.692 0.683
x
50 0.741 0.752 0.745 0.715 0.722 0.717 0.732 0.731 0.718 0.715
100 0.746 0.752 0.741 0.730 0.724 0.722 0.723 0.722 0.707 0.707
rDCNet
150 0.752 0.741 0.744 0.739 0.702 0.722 0.730 0.717 0.715 0.704
200 0.734 0.743 0.743 0.733 0.715 0.716 0.718 0.730 0.708 0.703
50 0.781 0.783 0.787 0.785 0.785 0.753 0.766 0.769 0.767 0.753
100 0.785 0.781 0.785 0.781 0.790 0.761 0.768 0.758 0.759 0.769
rEEGNet
150 0.781 0.781 0.780 0.788 0.781 0.758 0.758 0.755 0.767 0.766
200 0.783 0.777 0.787 0.779 0.779 0.760 0.751 0.765 0.759 0.760
50 0.349 0.358 0.377 0.379 0.392 0.416 0.427 0.436 0.434 0.438
100 0.340 0.361 0.366 0.382 0.401 0.407 0.420 0.422 0.429 0.458
mLR
150 0.342 0.351 0.369 0.390 0.388 0.401 0.407 0.417 0.448 0.446
200 0.333 0.354 0.376 0.377 0.387 0.388 0.403 0.436 0.436 0.435
50 0.738 0.742 0.736 0.735 0.727 0.711 0.715 0.719 0.711 0.708
100 0.743 0.741 0.738 0.728 0.733 0.717 0.720 0.714 0.711 0.709
rSCNet
150 0.749 0.740 0.735 0.736 0.729 0.721 0.712 0.710 0.715 0.706
200 0.743 0.732 0.736 0.728 0.725 0.714 0.704 0.711 0.705 0.697
y
50 0.753 0.763 0.762 0.751 0.754 0.727 0.741 0.742 0.734 0.733
100 0.754 0.761 0.760 0.754 0.756 0.733 0.735 0.731 0.730 0.736
rDCNet
150 0.760 0.751 0.757 0.755 0.744 0.731 0.738 0.729 0.736 0.734
200 0.743 0.752 0.755 0.753 0.750 0.725 0.725 0.739 0.730 0.730
50 0.787 0.788 0.792 0.787 0.789 0.759 0.774 0.775 0.773 0.761
100 0.789 0.788 0.788 0.786 0.795 0.768 0.774 0.763 0.768 0.777
rEEGNet
150 0.786 0.783 0.782 0.793 0.784 0.764 0.764 0.762 0.776 0.773
200 0.787 0.780 0.791 0.785 0.782 0.766 0.756 0.771 0.767 0.767
50 0.156 0.158 0.164 0.169 0.173 0.289 0.296 0.296 0.301 0.304
100 0.153 0.159 0.164 0.167 0.173 0.287 0.289 0.293 0.297 0.298
mLR
150 0.154 0.160 0.162 0.167 0.173 0.281 0.285 0.289 0.290 0.290
200 0.155 0.157 0.163 0.168 0.170 0.277 0.281 0.283 0.282 0.287
50 0.562 0.565 0.564 0.560 0.545 0.568 0.578 0.580 0.582 0.576
100 0.559 0.562 0.555 0.555 0.549 0.576 0.579 0.578 0.574 0.583
rSCNet
150 0.565 0.558 0.550 0.559 0.551 0.584 0.576 0.569 0.581 0.577
200 0.553 0.549 0.553 0.543 0.530 0.575 0.571 0.581 0.572 0.565
z
50 0.540 0.564 0.559 0.537 0.549 0.570 0.601 0.610 0.603 0.596
100 0.547 0.571 0.561 0.557 0.542 0.572 0.593 0.601 0.588 0.597
rDCNet
150 0.548 0.557 0.566 0.575 0.520 0.586 0.599 0.591 0.606 0.587
200 0.533 0.558 0.569 0.552 0.529 0.576 0.590 0.609 0.590 0.592
50 0.601 0.608 0.625 0.624 0.624 0.620 0.633 0.639 0.640 0.635
100 0.597 0.607 0.617 0.616 0.637 0.627 0.632 0.633 0.634 0.647
rEEGNet
150 0.604 0.598 0.609 0.626 0.624 0.616 0.626 0.621 0.645 0.640
200 0.599 0.601 0.617 0.611 0.616 0.620 0.613 0.639 0.637 0.639
Note:theboldentriesrepresentthehighestPCCvalueobtainedinthex,y,andzdirectionsfortheEEGsensor-domainandsource-domaininputfeatures.
rDCNet-rDeepConvNet,rSCNet-rShallowConvNet
B. Results coding is also explored. The mean correlation values across
twelve subjects using the mLR model and three deep learning
In the first set of experiments, the performance of motor
models are presented in Table V. The decoding analysis is
kinematics prediction is explored for intra-subject settings
performed for 50 ms time lag and window size up to 450
using sensor-domain and source-domain EEG features. Table
ms for the MKP of hand position in x, y, and z-directions.
IV presents the mean correlation values for twelve subjects
In particular, sensor-domain and source-domain EEG features
using different decoding models. The decoding analysis is
are utilized for comparing MKP performance.
performed for hand kinematics trajectory in the x, y, and z-
directions. In addition, the effect of EEG lag and window
size is also presented in Table IV. EEG lag up to 200 ms
C. Discussion
is analyzed with various EEG window sizes in the range of
250−450 ms. 1) mLR Performance analysis: In this work, the mLR
Furthermore, the performance analysis for inter-subject de- model is utilized for MKP using EEG sensor-domain and7
TABLEV
MEANPCCVALUESFORINTER-SUBJECTTRAJECTORYDECODINGINTHEX,Y,ANDZDIRECTIONSUSINGEEGSENSOR-DOMAINANDSOURCE-DOMAIN
TIMESERIESINPUT.THEEFFECTONPCCVALUESUSING50MSLAGANDDIFFERENTWINDOWSIZESWITHDIFFERENTDECODINGMODELSISALSO
DEPICTED.
SensorDomain SourceDomain
Direction Decoders Windowsize(ms) Windowsize(ms)
250 300 350 400 450 250 300 350 400 450
mLR 0.143 0.151 0.168 0.186 0.203 0.150 0.150 0.167 0.182 0.201
rSCNet 0.685 0.692 0.691 0.694 0.692 0.625 0.625 0.627 0.621 0.633
x
rDCNet 0.730 0.731 0.731 0.731 0.715 0.664 0.658 0.658 0.652 0.643
rEEGNet 0.745 0.744 0.749 0.753 0.748 0.683 0.682 0.684 0.683 0.683
mLR 0.145 0.156 0.172 0.188 0.203 0.158 0.160 0.176 0.188 0.204
rSCNet 0.688 0.694 0.697 0.703 0.695 0.627 0.629 0.628 0.621 0.636
y
rDCNet 0.729 0.731 0.731 0.733 0.714 0.668 0.664 0.660 0.656 0.643
rEEGNet 0.745 0.743 0.750 0.756 0.744 0.685 0.686 0.687 0.687 0.680
mLR 0.051 0.051 0.048 0.047 0.047 0.146 0.144 0.142 0.147 0.149
rSCNet 0.412 0.407 0.391 0.402 0.419 0.431 0.442 0.448 0.445 0.453
z
rDCNet 0.442 0.445 0.455 0.468 0.465 0.468 0.473 0.491 0.497 0.493
rEEGNet 0.465 0.454 0.461 0.478 0.490 0.484 0.495 0.515 0.506 0.509
Note:theboldentriesrepresentthehighestPCCvalueobtainedinthex,y,andzdirectionsfortheEEGsensor-domainandsource-domaininputfeatures.
rDCNet-rDeepConvNet,rSCNet-rShallowConvNet
source-domain features for hand trajectory in x, y, and z- 3) EEG Lag Analysis: In this analysis, hand trajectory is
directions. The decoding performance is evaluated for intra- decoded using pre-movement EEG data with distinct window
subject and inter-subject decoding settings as shown in Table sizes and lags. For intra-subject decoding analysis, EEG
IV and V, respectively. In intra-subject decoding analysis, the segment up to 200 ms prior to movement-onset and size
mLR with source-domain EEG features has better decoding in the 250 − 450 ms range is utilized as shown in Table
performance in x, y, and z-directions in comparison with IV. The best correlation values are obtained for an EEG lag
sensor-domain-basedMKP.Thesamecanbedepictedfromp- of 100 ms and window size of 450 ms while using the
valuesshowninTableVI.However,thedecodingperformance rEEGNet decoding model. For sensor-domain EEG features,
degrades in the case of inter-subject decoding. Also, the the mean correlation values obtained are 0.790, 0.795, and
decoding performance is statistically similar using the EEG 0.637 for x, y, and z-directions, respectively, while the mean
sensor-domainandsource-domainfeaturesforMKP,asshown correlation values of 0.769, 0.777, and 0.647 are observed
in Table VI. Furthermore, the deep learning-based decoders with EEG source-domain features. In the case of inter-subject
outperformedthemLRmodelwithsensor-domainandsource- decoding analysis, EEG segments with varying sizes and 50
domain features in intra-subject and inter-subject settings, as ms pre-movement EEG data are taken as depicted in Table
depicted from the p-values shown in Table VII-VI. V. The rEEGNet model has the best decoding performance
with sensor-domain as well as source-domain input features.
2) Deep Learning Model Analysis: Three deep learning With sensor-domain input features, the best mean correlation
models,namely,rShallowConvNet,rDeepConvNet,andrEEG- values of 0.753 and 0.756 are observed with 400 ms window
Net, are utilized to access the MKP performance during the size in the x and y directions, respectively, while 0.490 mean
grasp-and-lift task. As evident from the results in Tables IV correlation value is obtained with 450 ms window size in the
- V, the rEEGNet decoding model outperforms the models z-direction. The decoding performance with source-domain
for MKP in the x, y, and z-directions. The model architecture features is observed with 350 ms window size and mean
consists of depthwise and separable convolution layers that correlation values of 0.684, 0.687, and 0.515 in the x, y, and
aredesignedforfeatureextractionfromEEGdatawithinend- z-directions, respectively.
to-end model training. Further, rEEGNet has the minimum 4) Sensor-domain and Source-domain Feature Analysis:
parameters to train among the deep learning models utilized EEGsensor-domainandsource-domainfeaturesareemployed
forMKPanalysis.Subsequently,adetailedt-testisperformed as inputs to the decoding models to access the suitable
tocomparethetrajectorydecodingperformanceofthevarious features for MKD during the grasp-and-lift task. In intra-
decoding models. The statistical analysis is performed to subject decoding analysis, the highest correlation values are
compare the decoding performance of models with sensor- obtained by deep learning models while using EEG sensor-
domainfeaturesandsource-domaininputfeatures.Theresults domain features in x and y-direction decoding. However,
of the t-test are shown in Table VII for the intra-subject in z-direction, the decoding performance of the models is
decoding case. It can be noted that the deep learning-based better with EEG source-domain features. In particular, the
decoders performed significantly better than the traditionally rEEGNet neural decoder performs best with both sensor-
used mLR model. In particular, the rEEGNet model decoding domain and source-domain features as input. In inter-subject
performance is statistically better in comparison to other decoding analysis, it is observed that the best correlation
decoding models. For the inter-subject case, all p-values are values are obtained using sensor-domain EEG features with
<0.05 in the t-test, which signifies the superior performance the rEEGNet decoder. However, the rEEGNet model with
of the rEEGNet model over the other decoding models. source-domain features has better decoding performance in8
TABLEVI
pVALUESOFONE-TAILEDt-TESTBETWEENSENSOR-DOMAINANDSOURCE-DOMAINFEATURES-BASEDTRAJECTORYDECODERSFORINTRA-SUBJECT
ANDINTER-SUBJECTSETTINGSINTHEX,Y,ANDZDIRECTIONS.
Intra-Subject Inter-Subject
SourceDomain SourceDomain
x-direction x-direction
mLR rSCNet rDCNet rEEGNet mLR rSCNet rDCNet rEEGNet
mLR 7.40×10−5 2.93×10−6 1.58×10−6 6.45×10−7 mLR 4.32×10−1 7.54×10−7 2.32×10−6 5.82×10−7
rSCNet 9.35×10−7 1.14×10−4 2.09×10−1 4.51×10−4 rSCNet 2.64×10−7 5.80×10−6 7.59×10−4 2.48×10−3
rDCNet 3.03×10−6 9.98×10−4 3.09×10−2 1.08×10−2 rDCNet 6.96×10−7 1.55×10−5 1.75×10−6 6.57×10−5
rEEGNet 1.76×10−8 1.18×10−5 3.37×10−5 1.08×10−3 rEEGNet 1.57×10−7 1.01×10−6 1.75×10−5 6.85×10−7
SourceDomain SourceDomain
y-direction y-direction
mLR rSCNet rDCNet rEEGNet mLR rSCNet rDCNet rEEGNet
mLR 6.87×10−5 1.11×10−6 6.08×10−7 5.56×10−7 mLR 6.22×10−2 7.11×10−7 2.35×10−6 6.30×10−7
rSCNet 3.81×10−7 1.47×10−4 4.94×10−1 2.19×10−4 rSCNet 1.32×10−7 3.88×10−5 1.48×10−3 5.66×10−3
rDCNet 1.25×10−7 4.71×10−6 7.12×10−5 7.36×10−3 rDCNet 5.22×10−7 2.82×10−5 5.41×10−6 1.97×10−5
rEEGNet 3.04×10−8 5.20×10−7 1.29×10−5 1.69×10−3 rEEGNet 1.33×10−7 6.51×10−6 2.94×10−5 2.92×10−6
SourceDomain SourceDomain
z-direction z-direction
mLR rSCNet rDCNet rEEGNet mLR rSCNet rDCNet rEEGNet
mLR 1.36×10−8 5.27×10−9 1.07×10−7 3.12×10−9 mLR 3.64×10−7 4.96×10−8 1.79×10−7 1.22×10−7
rSCNet 6.54×10−7 6.59×10−3 4.19×10−3 7.17×10−5 rSCNet 1.40×10−7 1.96×10−3 3.88×10−4 2.14×10−4
rDCNet 8.18×10−7 3.18×10−3 8.72×10−4 5.79×10−5 rDCNet 1.75×10−7 1.35×10−2 3.59×10−5 1.46×10−4
rEEGNet 2.93×10−8 1.39×10−4 4.68×10−3 8.93×10−4 rEEGNet 2.31×10−7 4.56×10−3 2.25×10−2 4.56×10−3
rDCNet-rDeepConvNet,rSCNet-rShallowConvNet
TABLEVII
pVALUESOFONE-TAILEDt-TESTBETWEENTRAJECTORYDECODINGMODELSWITHSENSOR-DOMAINANDSOURCE-DOMAINFEATURESFOR
INTRA-SUBJECTSETTINGINTHEX,Y,ANDZDIRECTIONS.
Sensor-domain Source-domain
x-direction x-direction
mLR rSCNet rDCNet rEEGNet mLR rSCNet rDCNet rEEGNet
mLR - 2.90×10−6 6.00×10−6 2.66×10−7 mLR - 8.69×10−7 3.38×10−7 9.79×10−8
rSCNet 2.90×10−6 - 5.91×10−2 7.46×10−5 rSCNet 8.69×10−7 - 6.43×10−4 3.99×10−5
rDCNet 6.00×10−6 5.91×10−2 - 1.56×10−3 rDCNet 3.38×10−7 6.43×10−4 - 5.94×10−5
rEEGNet 2.66×10−7 7.46×10−5 1.56×10−3 - rEEGNet 9.79×10−8 3.99×10−5 5.94×10−5 -
y-direction y-direction
mLR rSCNet rDCNet rEEGNet mLR rSCNet rDCNet rEEGNet
mLR - 1.55×10−6 7.72×10−7 2.98×10−7 mLR - 1.73×10−7 6.20×10−8 7.57×10−8
rSCNet 1.55×10−6 - 5.43×10−4 2.04×10−5 rSCNet 1.73×10−7 - 1.04×10−4 8.73×10−6
rDCNet 7.72×10−7 5.43×10−4 - 6.94×10−5 rDCNet 6.20×10−8 1.04×10−4 - 3.03×10−5
rEEGNet 2.98×10−7 2.04×10−5 6.94×10−5 - rEEGNet 7.57×10−8 8.73×10−6 3.03×10−5 -
z-direction z-direction
mLR rSCNet rDCNet rEEGNet mLR rSCNet rDCNet rEEGNet
mLR - 1.88×10−7 2.55×10−7 3.13×10−9 mLR - 9.95×10−9 3.98×10−7 8.13×10−9
rSCNet 1.88×10−7 - 7.89×10−2 6.33×10−4 rSCNet 9.95×10−9 - 7.53×10−3 7.97×10−7
rDCNet 2.55×10−7 7.89×10−2 - 3.55×10−4 rDCNet 3.98×10−7 7.53×10−3 - 2.52×10−4
rEEGNet 3.13×10−9 6.33×10−4 3.55×10−4 - rEEGNet 8.13×10−9 7.97×10−7 2.52×10−4 -
rDCNet-rDeepConvNet,rSCNet-rShallowConvNet
the z-direction. A detailed t-test is performed to compare the across three axes are obtained, with 100 ms EEG lag and
decodingperformancebetweenthedecodingmodelswithEEG 450 ms window size for sensor-domain and source-domain
sensor-domain and source-domain input features. The results features as decoder input, respectively. We can conclude
ofthe t-testareshown inTableVIinthe formof p-valuesfor that the rEEGNet model can learn the subject-specific motor
intra-subject and inter-subject settings. information from the EEG sensor-domain and source-domain
features for MKP.
5) Intra-subject and Inter-subject Decoding Analysis: The
objective of the intra-subject and inter-subject decoding anal-
ysis is to explore the subject-specific and subject-independent In inter-subject decoding analysis, subject-independent fea-
feature-learning capabilities of the decoding model for the turesarelearnedbythedecodingmodels,andtheperformance
kinematics trajectory prediction during the grasp-and-lift task. evaluation is done on the subject data that is excluded from
Itcanbenotedthatthedecodingmodel,inparticularrEEGNet, the training phase. The rEEGNet model with sensor-domain
is able to learn subject-specific features for MKP with sensor- input features has the best decoding performance in the x and
domainaswellassource-domainEEGfeaturesasinputduring y directions. However, in the z-direction, source-domain input
the grasp-and-lift task, as can be depicted from the results in featuresbasedrEEGNethavebetterdecodingperformance,as
Table IV. The mean correlation values of 0.741 and 0.731 shown in Table VI.
rosneS
rosneS
rosneS
niamoD
niamoD
niamoD
rosneS
rosneS
rosneS
niamoD
niamoD
niamoD9
IV. CONCLUSION [10] S. Moreno-Caldero´n, V. Mart´ınez-Cagigal, E. Santamar´ıa-Va´zquez,
S.Pe´rez-Velasco,D.Marcos-Mart´ınez,andR.Hornero,“Assessingthe
This study explores the EEG source imaging-based kine-
Potential of Brain-Computer Interface Multiplayer Video Games using
matics decoding using a deep learning-based decoding model. c-VEPs:APilotStudy,”in202345thAnnualInternationalConference
In particular, the frontoparietal regions are selected for the oftheIEEEEngineeringinMedicine&BiologySociety(EMBC). IEEE,
2023,pp.1–4.
MKP during the grasp-and-lift task. The motor-related in-
[11] N.Robinson,R.Mane,T.Chouhan,andC.Guan,“Emergingtrendsin
formation encoded in the pre-movement brain activation is BCI-roboticsfor motor controland rehabilitation,”Current Opinionin
utilized for hand trajectory decoding using the time-lagged BiomedicalEngineering,vol.20,p.100354,2021.
[12] J.d.R.Milla´n,“Invasiveornoninvasive:understandingbrain-machine
EEGfeaturesinsensorandsourcedomains.TheMKPanalysis
interfacetechnology,”IEEEEngineeringinMedicineandBiologyMag-
is performed using the convolutional neural network-based azine,vol.29,no.1,pp.16–22,2010.
decoding models. Further, inter-subject decoding analysis has [13] K. J. Miller, D. Hermes, and N. P. Staff, “The current state of
electrocorticography-based brain–computer interfaces,” Neurosurgical
been performed to evaluate the subject-independent feature-
focus,vol.49,no.1,p.E2,2020.
learning capabilities of the decoding models. The proposed [14] N.NaseerandK.-S.Hong,“fNIRS-basedbrain-computerinterfaces:a
rEEGNet yielded the overall best kinematics decoding perfor- review,”Frontiersinhumanneuroscience,vol.9,p.3,2015.
[15] X. Wang, Y. Zheng, F. Wang, H. Ding, J. Meng, and Y. Zhuo,
mance using the EEG sensor-domain features. The Pearson
“Unilateralmovementdecodingofupperandlowerlimbsusingmagne-
correlation coefficient (PCC) is used as a performance metric toencephalography,”BiomedicalSignalProcessingandControl,vol.93,
forMKP.Thedecodinganalysisshowstheviabilityofcontin- p.106215,2024.
[16] H. I. Baqapuri, L. D. Roes, M. Zvyagintsev, S. Ramadan, M. Keller,
uous trajectory decoding using EEG source domain features.
E. Roecher, J. Zweerings, M. Klasen, R. C. Gur, and K. Mathiak, “A
However,thedecodingaccuracyusingthesensor-domainEEG novel brain–computer interface virtual environment for neurofeedback
featuresisbetterthanthesource-domaincounterpartusingthe duringfunctionalMRI,”FrontiersinNeuroscience,vol.14,p.593854,
2021.
proposed decoding model.
[17] X.Gu,Z.Cao,A.Jolfaei,P.Xu,D.Wu,T.-P.Jung,andC.-T.Lin,“EEG-
based brain-computer interfaces (BCIs): A survey of recent studies on
ACKNOWLEDGMENT signal sensing technologies and computational intelligence approaches
andtheirapplications,”IEEE/ACMtransactionsoncomputationalbiol-
The authors would like to thank Prof. Sitikantha Roy and
ogyandbioinformatics,vol.18,no.5,pp.1645–1666,2021.
Prof. Shubhendu Bhasin from the Indian Institute of Technol- [18] A.Othmani,A.Q.M.Sabri,S.Aslan,F.Chaieb,H.Rameh,R.Alfred,
ogy Delhi and Dr. Suriya Prakash from All India Institute of andD.Cohen,“EEG-basedneuralnetworksapproachesforfatigueand
drowsinessdetection:Asurvey,”Neurocomputing,p.126709,2023.
MedicalSciencesDelhifortheirconstructivecommentsduring
[19] X.Li,Y.Zhang,P.Tiwari,D.Song,B.Hu,M.Yang,Z.Zhao,N.Kumar,
the preparation of the manuscript. and P. Marttinen, “EEG based emotion recognition: A tutorial and
review,”ACMComputingSurveys,vol.55,no.4,pp.1–57,2022.
REFERENCES [20] Z. Tang, H. Wang, Z. Cui, X. Jin, L. Zhang, Y. Peng, and B. Xing,
“An upper-limb rehabilitation exoskeleton system controlled by MI
[1] J. R. Wolpaw, J. D. R. Millan, and N. F. Ramsey, “Brain-computer recognition model with deep emphasized informative features in a
interfaces:Definitionsandprinciples,”Handbookofclinicalneurology, VR scene,” IEEE Transactions on Neural Systems and Rehabilitation
vol.168,pp.15–23,2020. Engineering,2023.
[2] R.Mane,T.Chouhan,andC.Guan,“BCIforstrokerehabilitation:motor [21] W. Li, Y. Ma, K. Shao, Z. Yi, W. Cao, M. Yin, T. Xu, and X. Wu,
andbeyond,”Journalofneuralengineering,vol.17,no.4,p.041001, “The Human–Machine Interface Design Based on sEMG and Motor
2020. Imagery EEG for Lower Limb Exoskeleton Assistance System,” IEEE
[3] A.Miladinovic´,M.Ajcˇevic´,P.Busan,J.Jarmolowska,G.Silveri,M.De- Transactions on Instrumentation and Measurement, vol. 73, pp. 1–14,
odato,S.Mezzarobba,P.P.Battaglini,andA.Accardo,“Evaluationof 2024.
MotorImagery-BasedBCImethodsinneurorehabilitationofParkinson’s [22] J. Ai, J. Meng, X. Mai, and X. Zhu, “BCI control of a robotic arm
Diseasepatients,”in202042ndAnnualInternationalConferenceofthe basedonSSVEPwithmovingstimuliforreachandgrasptasks,”IEEE
IEEE Engineering in Medicine & Biology Society (EMBC). IEEE, JournalofBiomedicalandHealthInformatics,2023.
2020,pp.3058–3061. [23] Y. Zhou, T. Yu, W. Gao, W. Huang, Z. Lu, Q. Huang, and Y. Li,
[4] R.Na,C.Hu,Y.Sun,S.Wang,S.Zhang,M.Han,W.Yin,J.Zhang, “Shared three-dimensional robotic arm control based on asynchronous
X.Chen,andD.Zheng,“AnembeddedlightweightSSVEP-BCIelectric BCI and computer vision,” IEEE Transactions on Neural Systems and
wheelchairwithhybridstimulator,”DigitalSignalProcessing,vol.116, RehabilitationEngineering,2023.
p.103101,2021. [24] R. Fu, X. Feng, S. Wang, Y. Shi, C. Jia, and J. Zhao, “Control of the
[5] L.Cao,W.Wang,C.Huang,Z.Xu,H.Wang,J.Jia,S.Chen,Y.Dong, roboticarmsystemwithanSSVEP-basedBCI,”MeasurementScience
C.Fan,andV.H.C.deAlbuquerque,“Aneffectivefusingapproachby andTechnology,2024.
combiningconnectivitynetworkpatternandtemporal-spatialanalysisfor [25] L. Gu, Z. Yu, T. Ma, H. Wang, Z. Li, and H. Fan, “EEG-based
EEG-based BCI rehabilitation,” IEEE Transactions on Neural Systems classificationoflowerlimbmotorimagerywithbrainnetworkanalysis,”
andRehabilitationEngineering,vol.30,pp.2264–2274,2022. Neuroscience,vol.436,pp.93–109,2020.
[6] J. M. Catala´n, E. Trigili, M. Nann, A. Blanco-Ivorra, C. Lauretti, [26] H.Altaheri,G.Muhammad,M.Alsulaiman,S.U.Amin,G.A.Altuwai-
F. Cordella, E. Ivorra, E. Armstrong, S. Crea, M. Alcan˜iz et al., jri,W.Abdul,M.A.Bencherif,andM.Faisal,“Deeplearningtechniques
“Hybrid brain/neural interface and autonomous vision-guided whole- for classification of electroencephalogram (EEG) motor imagery (MI)
arm exoskeleton control to perform activities of daily living (ADLs),” signals:Areview,”NeuralComputingandApplications,vol.35,no.20,
JournalofNeuroEngineeringandRehabilitation,vol.20,no.1,p.61, pp.14681–14722,2023.
2023. [27] J.Wang,L.Bi,andW.Fei,“EEG-BasedMotorBCIsforUpperLimb
[7] P. Demarest, N. Rustamov, J. Swift, T. Xie, M. Adamek, H. Cho, Movement:CurrentTechniquesandFutureInsights,”IEEETransactions
E. Wilson, Z. Han, A. Belsten, N. Luczak et al., “A novel theta- onNeuralSystemsandRehabilitationEngineering,vol.31,pp.4413–
controlled vibrotactile brain–computer interface to treat chronic pain: 4427,2023.
apilotstudy,”ScientificReports,vol.14,no.1,p.3433,2024. [28] T.J.Bradberry,R.J.Gentili,andJ.L.Contreras-Vidal,“Reconstructing
[8] A. Hekmatmanesh, P. H. Nardelli, and H. Handroos, “Review of the three-dimensionalhandmovementsfromnoninvasiveelectroencephalo-
state-of-the-art of brain-controlled vehicles,” IEEE Access, vol. 9, pp. graphicsignals,”Journalofneuroscience,vol.30,no.9,pp.3432–3437,
110173–110193,2021. 2010.
[9] A. Nijholt, J. L. Contreras-Vidal, C. Jeunet, and A. Va¨ljama¨e, “Brain- [29] A. Presacco, R. Goodman, L. Forrester, and J. L. Contreras-Vidal,
computer interfaces for non-clinical (home, sports, art, entertainment, “Neural decoding of treadmill walking from noninvasive electroen-
education, well-being) applications,” Frontiers in Computer Science, cephalographic signals,” Journal of neurophysiology, vol. 106, no. 4,
vol.4,p.860619,2022. pp.1875–1887,2011.10
[30] P.OfnerandG.R.Mu¨ller-Putz,“Decodingofvelocitiesandpositionsof analysis,” Journal of neuroscience methods, vol. 134, no. 1, pp. 9–21,
3DarmmovementfromEEG,”in2012annualinternationalconference 2004.
oftheIEEEengineeringinmedicineandbiologysociety. IEEE,2012, [51] V. Fonov, A. C. Evans, K. Botteron, C. R. Almli, R. C. McKinstry,
pp.6406–6409. D.L.Collins,B.D.C.Groupetal.,“Unbiasedaverageage-appropriate
[31] A. Korik, R. Sosnik, N. Siddique, and D. Coyle, “Decoding imagined atlasesforpediatricstudies,”Neuroimage,vol.54,no.1,pp.313–327,
3DhandmovementtrajectoriesfromEEG:evidencetosupporttheuse 2011.
of mu, beta, and low gamma oscillations,” Frontiers in neuroscience, [52] A. Gramfort, T. Papadopoulo, E. Olivi, and M. Clerc, “OpenMEEG:
vol.12,p.323240,2018. opensource software for quasistatic bioelectromagnetics,” Biomedical
[32] Y. Sun, H. Zeng, A. Song, B. Xu, H. Li, J. Liu, and P. Wen, “Inves- engineeringonline,vol.9,no.1,pp.1–20,2010.
tigation of the phase feature of low-frequency electroencephalography [53] F. Tadel, S. Baillet, J. C. Mosher, D. Pantazis, and R. M. Leahy,
signalsfordecodinghandmovementparameters,”in2017IEEEInter- “Brainstorm:auser-friendlyapplicationforMEG/EEGanalysis,”Com-
national Conference on Systems, Man, and Cybernetics (SMC), 2017, putationalintelligenceandneuroscience,vol.2011,2011.
pp.2312–2316. [54] R. D. Pascual-Marqui et al., “Standardized low-resolution brain elec-
[33] R.SosnikandO.B.Zur,“Reconstructionofhand,elbowandshoulder tromagnetic tomography (sLORETA): technical details,” Methods Find
actual and imagined trajectories in 3D space using EEG slow cortical ExpClinPharmacol,vol.24,no.SupplD,pp.5–12,2002.
potentials,” Journal of neural engineering, vol. 17, no. 1, p. 016065, [55] A.Klein,B.Mensh,S.Ghosh,J.Tourville,andJ.Hirsch,“Mindboggle:
2020. automatedbrainlabelingwithmultipleatlases,”BMCmedicalimaging,
[34] N. Robinson, T. W. J. Chester, and S. KG, “Use of Mobile EEG in vol.5,no.1,pp.1–14,2005.
DecodingHandMovementSpeedandPosition,”IEEETransactionson [56] V.J.Lawhern,A.J.Solon,N.R.Waytowich,S.M.Gordon,C.P.Hung,
Human-MachineSystems,vol.51,no.2,pp.120–129,2021. andB.J.Lance,“EEGNet:acompactconvolutionalneuralnetworkfor
[35] F.Shakibaee,E.Mottaghi,H.R.Kobravi,andM.Ghoshuni,“Decoding EEG-based brain-computer interfaces,” Journal of neural engineering,
knee angle trajectory from electroencephalogram signal using NARX vol.15,no.5,p.056013,2018.
neural network and a new channel selection algorithm,” Biomedical [57] R.T.Schirrmeister,J.T.Springenberg,L.D.J.Fiederer,M.Glasstetter,
Physics&EngineeringExpress,vol.5,no.2,p.025024,2019. K. Eggensperger, M. Tangermann, F. Hutter, W. Burgard, and T. Ball,
[36] J.-H. Jeong, K.-H. Shim, D.-J. Kim, and S.-W. Lee, “Brain-controlled “Deep learning with convolutional neural networks for EEG decoding
robotic arm system based on multi-directional CNN-BiLSTM network and visualization,” Human brain mapping, vol. 38, no. 11, pp. 5391–
using EEG signals,” IEEE Transactions on Neural Systems and Reha- 5420,2017.
bilitationEngineering,vol.28,no.5,pp.1226–1238,2020.
[37] A. Jain and L. Kumar, “PreMovNet: Premovement EEG-Based Hand
KinematicsEstimationforGrasp-and-LiftTask,”IEEESensorsLetters,
vol.6,no.7,pp.1–4,2022.
[38] S. Pancholi, A. Giri, A. Jain, L. Kumar, and S. Roy, “Source Aware
Deep Learning Framework for Hand Kinematic Reconstruction Using
EEG Signal,” IEEE Transactions on Cybernetics, vol. 53, no. 7, pp.
4094–4106,2023.
[39] A.JainandL.Kumar,“Subject-independenttrajectorypredictionusing
pre-movement EEG during grasp and lift task,” Biomedical Signal
ProcessingandControl,vol.86,p.105160,2023.
[40] M.Saini,A.Jain,S.P.Muthukrishnan,S.Bhasin,S.Roy,andL.Ku-
mar,“BiCurNet:PremovementEEG-BasedNeuralDecoderforBiceps
CurlTrajectoryEstimation,”IEEETransactionsonInstrumentationand
Measurement,vol.73,pp.1–11,2024.
[41] B. J. Edelman, B. Baxter, and B. He, “EEG source imaging enhances
thedecodingofcomplexright-handmotorimagerytasks,”IEEETrans-
actionsonBiomedicalEngineering,vol.63,no.1,pp.4–14,2015.
[42] M.-A.Li,Y.-F.Wang,S.-M.Jia,Y.-J.Sun,andJ.-F.Yang,“Decodingof
motorimageryEEGbasedonbrainsourceestimation,”Neurocomputing,
vol.339,pp.182–193,2019.
[43] Y. Hou, L. Zhou, S. Jia, and X. Lun, “A novel approach of decoding
EEGfour-classmotorimagerytasksviascoutESIandCNN,”Journal
ofneuralengineering,vol.17,no.1,p.016048,2020.
[44] V.S.Handiru,A.Vinod,andC.Guan,“EEGsourcespaceanalysisof
the supervised factor analytic approach for the classification of multi-
directionalarmmovement,”Journalofneuralengineering,vol.14,no.4,
p.046008,2017.
[45] A.Tripathi,A.Gupta,A.Prathosh,S.P.Muthukrishnan,andL.Kumar,
“NeuroAiR:DeepLearningFrameworkforAirwritingRecognitionfrom
Scalp-recordedNeuralSignals,”IEEETransactionsonInstrumentation
andMeasurement,pp.1–1,2024.
[46] R.SosnikandL.Zheng,“Reconstructionofhand,elbowandshoulder
actualandimaginedtrajectoriesin3DspaceusingEEGcurrentsource
dipoles,”Journalofneuralengineering,vol.18,no.5,p.056011,2021.
[47] N. Srisrisawang and G. R. Mu¨ller-Putz, “Applying dimensionality re-
ductiontechniquesinsource-spaceelectroencephalographyviatemplate
and magnetic resonance imaging-derived head models to continuously
decodehandtrajectories,”Frontiersinhumanneuroscience,vol.16,p.
830221,2022.
[48] A. Jain and L. Kumar, “EEG Cortical Source Feature based Hand
KinematicsDecodingusingResidualCNN-LSTMNeuralNetwork,”in
202345thAnnualInternationalConferenceoftheIEEEEngineeringin
Medicine&BiologySociety(EMBC),2023,pp.1–4.
[49] M.D.Luciw,E.Jarocka,andB.B.Edin,“Multi-channelEEGrecord-
ingsduring3,936graspandlifttrialswithvaryingweightandfriction,”
Scientificdata,vol.1,no.1,pp.1–11,2014.
[50] A. Delorme and S. Makeig, “EEGLAB: an open source toolbox for
analysisofsingle-trialEEGdynamicsincludingindependentcomponent