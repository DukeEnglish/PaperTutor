Linked Component Analysis for Multiview Data
Lin Xiao1 and Luo Xiao1
1,2North Carolina State University
1 Introduction
Recent technological advances have led to increased availability of multiple sources of high-
content data. In particular, multiview data refers to different types of variables collected
from the same set of individuals. One typical example is the Roadmap Epigenomics Project
(Kundaje et al., 2015) which integrates information about histone marks, DNA methylation,
DNA accessibility and RNA expression to infer high-resolution maps of regulatory elements
annotated jointly across a total of 127 reference epigenomes spanning diverse cell and
tissue types. Another example is the data used in NCI-DREAM drug sensitivity prediction
challenge(Costelloetal.(2014))whichcontainsgeneexpression(GE),RNA,DNAmethylation
(MET), copy number variation (CNV), protein abundance (RPPA) and exome sequence
(EX) measurements for 53 human breast cancer cell lines. The prevalence of multiview
data has motivated research on uncovering associations between different data views. For
instance, many studies in neuroscience have been focusing on identifying joint relationship
between nonimaging features including genetic, demographics, behavioral, clinical information
and imaging derived features represented by brain structural measurements, functional
connectivity measurements and electrophysiological recordings (Zhuang et al., 2020). This
type of association analysis can hopefully provide insights on the disease-related links between
neurobiological activities and phenotypic features.
In application, canonical correlation analysis (CCA) is one of the most commonly used
methods for extracting common variation shared among two data views. CCA aims at seeking
for a pair of canonical coefficients such that the correlation between two transformed data
views is maximized. From the perspective of a latent factor model, CCA assumes that all
data views are generated from a common latent subspace of which the rank is lower than that
of any data view and the aim is to identify the common latent subspace. A number of CCA
1
4202
nuJ
71
]LM.tats[
1v16711.6042:viXravariants have been proposed to adapt to more complex scenarios. In order to capture the
potentially nonlinear correlation between different data views, kernel CCA (Akaho (2006);
Melzer et al. (2001)) and deep CCA (Andrew et al., 2013) are proposed, where kernel CCA
projects each data view onto a new feature space through a nonlinear mapping embedded in
a fixed kernel function while deep CCA learns this nonlinear representation via deep neural
networks in a more flexible manner. In a high dimensional setting where the number of
features might exceed the number of observations, sparse CCA (Witten et al. (2009); Wang
et al. (2019)) induces sparsity on the canonical coefficients for the purpose of providing more
reliable and interpretable estimation. Classic CCA-based methods are usually developed
for two data views, though, extension to the situation where multiple views are available
is quite straightforward. There are a variety of formulations for multiset CCA and they
differ in the proposed optimization objective functions. Among these formulations, some
propose to maximize the pairwise correlations or pairwise covariances (SUMCOR CCA,
SSQCOR CCA, SABSCOR CCA) and the others try to directly identify a common subspace
such that the Euclidean distance between the common subspace and the projected data
view is maximized (MAXVAR CCA, LS-CCA). We refer to Kettenring (1971) for a more
comprehensive summary of possible multiset CCA formulations. Another related work (Min
and Long, 2020) introduces the multiple co-inertia analysis (mCIA) method, which aims to
find a set of co-inertia loadings and a synthetic center such that the weighted sum of squared
covariances between each transformed data view and the synthetic center is maximized.
While the aforementioned methods primarily focus on common relationships across
different data views, many existing approaches take a richer set of association structures
into account. For instance, JIVE (Lock et al., 2013), COBE (Zhou et al., 2015), and AJIVE
(Feng et al., 2018) explore joint and individual structures simultaneously. Going further, Jia
et al. (2010), Van Deun et al. (2011) and SLIDE (Gaynanova and Li, 2019) take partially
shared structures into consideration. The majority of these methods take advantage of matrix
decomposition models. Specifically, each data view is decomposed into the summation of a
signal matrix and an error matrix, and the signal part is further decomposed into several
low-rank terms with each term representing a specific association structure. In particular,
the joint structure here is typically defined as the intersection of the column spaces of the
signal matrices associated with each data view.
Among the aformentioned approaches, CCA-based and CIA-based methods typically
extract the shared subspace in a successive way. A natural question that would arise for these
sequential methods is how to determine the number of shared components, or equivalently,
how to estimate the rank of the common latent subspace shared by all data views. In a
situation where two data views are available, rank selection can be performed based on the
2cumulative correlation plot or the scree plot for the sample cross covariance matrix (Kim
et al., 2021). Specifically, this sequential procedure can be terminated at the point where
the increment in the cumulative correlation or the decline in the singular values becomes
insignificant, or where the pre-specified percentage of cumulative canonical correlation or
cumulative variation has been met. Despite the handiness of these empirical rules, one
obvious drawback is that they require manual inspection and hence the resulting decision
is more or less subjective. As for the matrix-decomposition based approaches, the majority
of works focus on estimation of low rank components associated with each data view and a
direct approach for rank identification is missing except JIVE and SLIDE. JIVE provides two
rank selection schemes when the underlying rank is unknown: the rank of the joint signal
is determined either through a permutation test procedure or via a Bayesian Information
Criterion (BIC) selection algorithm. In addition, SLIDE provides estimation of the loading
matrices and score matrices associated with each type of structure and the rank for each
structure including the joint structure can be directly identified by counting the number of
the columns of the corresponding loading matrix.
In this work, we propose the joint linked component analysis (joint_LCA) for multiview
data. Unlike classic methods which extract the shared components in a sequential manner,
the objective of joint_LCA is to identify the view-specific loading matrices and the rank of
the common latent subspace simultaneously. We formulate a matrix decomposition model
where a joint structure and an individual structure are present in each data view, which
enables us to arrive at a clean svd representation for the cross covariance between any pair
of data views. An objective function with a novel penalty term is then proposed to achieve
simultaneous estimation and rank selection. In addition, a refitting procedure is employed as
a remedy to reduce the shrinkage bias caused by the penalization.
The paper is organized as follows. In section 2, we introduce the matrix decomposition
model and formulate the objective function for joint_LCA. In section 3, we elaborate on the
algorithm used for solving the optimization problem, along with the initialization schema
and the refitting procedure. In section 4, we compare the performance of joint_LCA with
other competitors in terms of estimation error and rank selection accuracy through extensive
simulations. In section 5, we apply joint_LCA for integrative analysis of real multiview data.
We conclude with discussion in section 6.
2 Proposed Method
Suppose that we have I data views from n subjects, i.e, X ∈ Rn×pi for i = 1,··· ,I and the
i
sample cross covariance matrix between data view X and data view X is denoted as Σˆ .
i j ij
3The data generative model is:
⊺ ⊺
X = U(V D ) +U (V D ) +W , i = 1,··· ,I
i i i i0 i0 i0 i
where the U(V D )⊺ is the submatrix of the joint structure that is associated with X , and
i i i
U (V D )⊺ represents the individual structure unique to X . The score matrix U ∈ Rn×p0
i0 i0 i0 i
is shared across all data views, and score matrices U is specific to X . D ∈ Rr0×r0 is a
i i i
diagonal matrix with singular values corresponding to the joint structure associated with X .
i
r denotes the rank of the common structure among all data views, namely, the number of
0
common components shared across X s. W is the error matrix. We assume that the columns
i i
of score matrices U and U s are independent random vectors with mean zero and variance 1.
i0
Hence, if we calculate the cross covariance matrix between any two data matrices X and X :
i j
cov(X ,X ) = E[X⊺ X ]
i j i j
= V D
E[U⊺ U]D⊺ V⊺
+V D
E[U⊺
U
]D⊺ V⊺
i i j j i i j0 j0 j0 (1)
+V D
E[U⊺ U]D⊺ V⊺
+V D
E[U⊺
U
]D⊺ V⊺
i0 i0 i0 j j i0 i0 i0 j0 j0 j0
⊺ ⊺
= V D D V
i i j j
The last equality is due to the fact that E[U⊺U] = I , E[U⊺ U] = 0, E[U⊺U ] = 0, and
n i0 j0
E[U⊺ U ] = 0.
i0 j0
Unlike classical canonical correlation methods with sequential formation, the goal of our
method is to jointly estimate the number of common factors r and the canonical loading
0
matrices V ,i = 1,··· ,I. Based on our data generative model and the cross covariance (1),
i
we propose the following optimization problem:
p0 (cid:115)
min (cid:88) w ∥Sˆ −V D D⊺ V⊺ ∥2 +λ(cid:88) (cid:88) w σ2
ij ij i i j j F ij ijk
Ui,Di (2)
1≤<i<j≤I k=1 1≤i<j≤I
⊺
s.t. V V = I ∀1 ≤ i ≤ I
i i
The data fidelity term in our objective function is a weighted sum of the squared loss
∥Sˆ −V D D V⊺ ∥2. In order to perform rank selection simultaneously, we add a penalty
ij i i j j F
term as follows:
p0 (cid:115)
(cid:88) (cid:88)
σ2
ijk
k=1 1≤i<j≤I
where σ = d d and d is the kth diagonal entry of D . This penalty enforces all
ijk ik jk ik i
D ,i = 1,··· ,I to have the same rank, which is exactly the number of common factors. w
i ij
4is a positive weight associated with the canonical correlation between data matrix X and
i
X . In specific, w can be constructed as 1/∥Sˆ ∥2, the inverse of the squared Frobenius
j ij ij F
norm of the sample cross covariance matrix Sˆ . And we choose p = min p , which is
ij 0 1≤i≤I i
the maximal possible number of common components. (The rank of solutions is penalized by
adding a group penalty on each diagonal position across all views)
3 Estimation
3.1 Algorithm
In this section, we describe how to solve optimization problem (2). The minimization of
the objective function with respect to parameters {(V ,D )} is approached by iteratively
i j i=1
updating each of the parameters (V or D ) in turn, keeping others fixed.
i i
First, for finding a solution of V , we fix D ,i = 1,··· ,I and V ,i = 2,··· ,I. Then
1 i i
problem (2) becomes
min (cid:88) w ∥Sˆ −V D D⊺ V⊺ ∥2
1j 1j 1 1 j j F
V1
(3)
1<j≤I
s.t. V⊺ V = I
1 1
Noting that the objective function in (7) can be rewritten as
∥Sˆ −V M ∥2
1,−1 1 1,−1 F
where Sˆ is the concatenation of all pairwise cross covariance matrices between data
1,−1
(cid:104)√ √ (cid:105)
view X and any other data view, that is, Sˆ = w Sˆ ,··· , w Sˆ . And M =
1 1,−1 12 12 1I 1I 1,−1
(cid:2)√ w D D V⊺ ,··· ,√ w D D V⊺(cid:3) is the concatenated matrix of D D V⊺ for all j ̸= 1.
12 1 2 2 1I 1 I I 1 j j
Hence, the problem (7) can be transformed to the well-known orthogonal procrustes problem
below:
min∥Sˆ −V M ∥2
1,−1 1 1,−1 F
V1
s.t. V⊺ V = I
1 1
which has a closed form solution V = RQ⊺, where R and Q are the left and right singular
1
vectors of Sˆ M⊺ , and can be obtained by performing SVD on Sˆ M⊺ . Rest of the
1,−1 1,−1 1,−1 1,−1
loading matrices V ,i = 2,··· ,I can be estimated by applying the same procedure as V .
i 1
Next, we consider finding a solution to D ,i = 1,··· ,I. We first simplify the data fidelity
i
5term in equation (2). Taking into account the fact that V ,i = 1,··· ,I are orthonormal
i
matrices, we have
(cid:88) w ∥Sˆ −V D D⊺ V⊺ ∥2
ij ij i i j j F
1≤i<j≤I
= (cid:88) w ∥V⊺ Sˆ V −D D⊺ ∥2
ij i ij j i j F
1≤i<j≤I
= (cid:88) w ∥S˜ −D D⊺ ∥2
ij ij i j F
1≤i<j≤I
where V⊺ Sˆ V is denoted as S˜ . Since D ,i = 1,··· ,I is diagonal, the above term can be
i ij j ij i
further simplified as follows:
(cid:88) w ∥S˜ −D D⊺ ∥2
ij ij i j F
1≤i<j≤I
(cid:88)
(cid:88)p0
(cid:16) (cid:17)2
˜
= w (S ) −d d +C
ij ij kk ik jk
1≤i<j≤I k=1
(cid:88)
(cid:88)p0
(cid:16) (cid:17)2
˜
= w (S ) −σ +C
ij ij kk ijk
1≤i<j≤I k=1
where (S˜ ) denotes the kth diagonal entry of S˜ and C is a constant value that does
ij kk ij
not depend on D ,i = 1,··· ,I, which can be removed from the objective function when
i
estimating D ,i = 1,··· ,I.
i
Now we combine the simplified data fidelity term and the penalty term and obtain the
following objective function:
(cid:88)p0
(cid:88) (cid:16) (cid:17)2
(cid:88)p0 (cid:115)
(cid:88)
min w (S˜ ) −σ +λ w σ2
ij ij kk ijk ij ijk
Di,i=1,···,I
k=11≤i<j≤I k=1 1≤i<j≤I
Apparently, the above objective function is decomposable with respect to index k =
1,··· ,p . Hence, we can obtain solutions for σ = (σ ) by solving the optimization
0 k ijk 1≤i<j≤I
problem below for each fixed k:
(cid:115)
(cid:88) (cid:16)√ √ (cid:17)2 (cid:88)
min w (S˜ ) − w σ +λ w σ2 (4)
ij ij kk ij ijk ij ijk
σ
ijk
1≤i<j≤I 1≤i<j≤I
(4) can be viewed as a problem with the format ’squared loss’+’group-lasso’ penalty with
6√
respect to σ˜ = ( w σ ) , and hence the solution can be explicitly expressed as
k ij ijk 1≤i<j≤I
(cid:18) (cid:19)
λ
σ˜ = 1− Y
k k
Y
k +
(cid:16)√ (cid:17)
where Y = w (S˜ ) . Then σ can be derived immediately from σ˜ .
k ij ij kk k k
1≤i<j≤I
Based on solutions for σ , denoted as σˆ , we propose to estimate the diagonal entries
k k
d ,k = 1,··· ,p of D ,i = 1,··· ,I by solving the following least-square problem:
ik 0 i
p0
(cid:88) (cid:88)
min (d d −σˆ )2
ik jk ijk
d (5)
ik
k=11≤i<j≤I
s.t. d ≥ 0,∀i = 1,··· ,I,k = 1,··· ,p
ik 0
Similarly, this problem is equivalent to solve
(cid:88)
min (d d −σˆ )2
ik jk ijk
d ,i=1,···,I
ik (6)
1≤i<j≤I
s.t. d ≥ 0,∀i = 1,··· ,I
ik
separately/simultaneously for each k, which can also be computed by an iterative process.
Suppose that d ,j = 2,··· ,I are fixed, and we want to estimate d by solving
jk 1k
(cid:88)
min (d d −σˆ )2
1k jk 1jk
d
1k (7)
1<j≤I
s.t. d ≥ 0
1k
It is straightforward to derive the solution for d :
1k
 (cid:80) 1<j≤Id jkσˆ 1jk, (cid:80) d σˆ > 0 and (cid:80) d2 ̸= 0
d = (cid:80) 1<j≤Id2 jk 1<j≤I jk 1jk 1<j≤I jk (8)
1k
0 (cid:80) d σˆ < 0 or (cid:80) d2 = 0
1<j≤I jk 1jk 1<j≤I jk
Other variables d ,j = 2,··· ,I can be updated iteratively in a similar manner.
jk
3.2 Initialization
As problem (2) is not convex, convergence to the global optimum is not guaranteed, and the
(cid:110) (cid:111)I
solution would depend on starting values V(0),D(0) . As initial values in optimization,
i i
i=1
we use an approximation of the solution under the assumptions that all components are
globally joint. Specifically, we initialize V(0) with the first p left singular vectors of S ,
i 0 i,−i
7which is the concatenation of all pairwise cross covariance matrices between data view
X and other matrices. In terms of initializing D ,i = 1,··· ,I, We propose to use the
i i
solutions to the optimization problem below as the starting values for the diagonal entries
d ,i = 1,··· ,I,k = 1,··· ,p :
ik 0
(cid:88)p0
(cid:88) (cid:104) (cid:16)(cid:104) (cid:105) (cid:17)(cid:105)2
min d d −max (V(0))⊺ Sˆ V(0) ,0
ik jk i ij j
d ik,i=1,···,I,k=1,···,p0 kk
k=11≤i<j≤I
Assuming that for a fixed k, all d ,i = 1,··· ,I are equal, then the solution to the above
ik
problem would be
(cid:118)
(cid:117) (cid:117)(cid:80) max(cid:16)(cid:104) (V(0))⊺Sˆ V(0)(cid:105) ,0(cid:17)
d = ··· = d = (cid:116) 1≤i<j≤I i ij j kk ,∀k = 1,··· ,p
1k Ik 0
I(I −1)/2
3.3 Model Refitting Given a Pre-specified Rank
By fitting (2), the number of nonzero diagonal entries of D ,i = 1,··· ,I, denoted as r, is
i
our estimate of the rank of common components. However, the penalty term would cause
shrinkage on the nonzero diagonal entries at the same time. Hence, we consider refitting the
model without the penalty term for this given rank r:
min (cid:88) w ∥Sˆ −V D D⊺ V⊺ ∥2 (9)
ij ij i i j j F
Ui,Di
1≤i<j≤I
s.t. V⊺ V = I ∀1 ≤ i ≤ I (10)
i i
Similarly, an iterative procedure can be applied. The subproblem for updating V would be
i
the same as (7). In terms of updating D ,i = 1,··· ,I, the subproblem becomes
i
r
(cid:88) (cid:88) (cid:16) (cid:17)2
˜
min w (S ) −d d
ij ij kk ik jk
d ,i=1,···,I,k=1,···,r
ik
k=11≤i<j≤I
which is similar to problem (5) and hence we can follow the same procedure (5)-(8) to obtain
the solution for d as follows:
1k
(cid:88) (cid:16) (cid:17)2
˜
min w (S ) −d d
1j 1j kk 1k jk
d 1k (11)
1<j≤I
s.t. d ≥ 0
1k
8 (cid:80) 1<j≤Iw1jd jk(S˜ 1j) kk, (cid:80) w d (S˜ ) > 0 and (cid:80) w d2 ̸= 0
d = (cid:80) 1<j≤Iw1jd2 jk 1<j≤I 1j jk 1j kk 1<j≤I 1j jk (12)
1k
0 (cid:80) w d (S˜ ) < 0 or (cid:80) w d2 = 0
1<j≤I 1j jk 1j kk 1<j≤I 1j jk
d ,j = 2,··· ,k = 1,··· ,I can be obtained in the same way.
jk
4 Simulation Study
In this section, we conduct simulation studies to evaluate the performance of joint_LCA. We
also apply JIVE with two options for rank selection, denoted as JIVE_BIC and JIVE_perm
respectively. In addition, mCIA, Min and Long (2020) and multiple canonical correlation
analysis (mCCA, Witten and Tibshirani (2009)) which is a direct extension of standard CCA
to multiple data views, are implemented for comparison.
For all settings, we generate I data views X ∈ Rn×pi as follows:
i
X = Z +W ,
i i i (13)
⊺ ⊺
Z = U(V D ) +U (V D ) ,
i i i i0 i0 i0
where two types of structures are encoded into the the true signal Z : a joint component
i
(U(V D )⊺) and an individual component (U (V D )⊺). More specifically, U corresponds to
i i i0 i0 i0
the common score matrix shared by I data views, and V is the view-specific loading matrix
i
associated with the joint structure. All score matrices and loading matrices are generated
using standard normal distribution with subsequent centering and orthonormalization. Each
entry in the error matrix W is generated from N(0,σ2), where the noise level is set such that
i
the overall signal to noise ratio equals to one:
(cid:80)I ∥Z ∥2
i=1 i F = 1.
n((cid:80)I
p )σ2
i=1 i
We denote the rank of the joint component and the rank of the individual structure associated
with the ith data view as r and r respectively. In all simulation settings, the rank of all
0 i
individual structures is set as r = 1,∀i.
i
In the simulation studies, we hope to investigate how different methods perform as the
underlying rank of the joint structure r varies. Specifically, we consider: 1) r = 2: the rank
0 0
of the joint structure is comparable to the rank of the individual structure; 2) r = 5: the
0
rank of the joint structure is relatively larger than that of the individual structure. Another
9aim is to study how the strength of the joint signal might affect the estimation results. To
that end, we generate the diagonal matrices D ,D in two ways. In case I, the diagonal
i i0
entries for all diagonal matrices are generated from the the standard uniform distribution
and hence the joint signal and the individual signal has comparable strength. In case II, the
diagonal entries of the diagonal matrices associated with the joint structures (D ,1 ≤ i ≤ I)
i
and those of the diagonal matrices associated with the individual structures (D ,1 ≤ i ≤ I)
i0
are generated from different uniform distributions:
√ √
D ∼ U[0.5 5, 5],D ∼ U[0.5,1],1 ≤ i ≤ I,
i i0
where the percentage of variation explained by the joint structure is relatively larger than
that explained by the individual structure. The sample size n is another varying factor and
we choose n over {100,200}.
To access how well joint_LCA and JIVE perform in terms of determining the rank of joint
component, we look into the rank selection accuracy, namely the proportion of replications
in which the true rank is identified. In addition, we are interested in its performance in
recovering the view-specific loading matrix V in comparison to mCIA and mCCA. Specifically,
i
we evaluate the scaled squared Frobenius norm error between the subspace spanned by true
loadings V and the subspace spanned by its estimate Vˆ, defined as
i i
(cid:88)I ∥V V⊺ −Vˆ Vˆ⊺ ∥2
i i i i F. (14)
⊺
I∥V V ∥2
i=1 i i F
JIVE gives signal matrix estimation associated with the joint structure for each data view
instead of view-specific loading matrices. We perform SVD on the estimated joint signal
matrix for each view and the first three right singular vectors are together used as an estimate
for V . As for sequential methods mCIA and mCCA which estimates one single loading at
i
one time, for fair comparison, we extract r components sequentially for each method, which
0
are then concatenated horizontally to form the corresponding Vˆ.
i
When implementing joint_LCA, the weight associated with each cross covariance error
term is set as:
1
w =
ij ∥Σˆ ∥2
ij F
where Σˆ corresponds to the estimate of cross covariance between X and X . Specifically for
ij i j
the tuning parameter selection, five-fold cross validation approach is employed to choose the
best tuning parameter λ. In each simulation, we split the simulated multiview data into five
folds. Each time the kth fold is held out as the testing set, and we obtain the estimate Vˆ, Dˆ
i i
10from the remaining samples, which are then used to construct the pairwise cross covariance
estimate for the training data:
Σˆ(−k)
=
Vˆ Dˆ Dˆ⊺ Vˆ⊺
.
ij i i j j
The weighted sum of the squared Frobenius norm error between pairwise cross covariance
estimate Σˆ(−k) from the training data and the standard cross covariance estimate Σˆ(k) from
ij ij
the testing data
(cid:88)
w
∥Σˆ(−k) −Σˆ(k)∥2
ij ij ij F
1≤i<j≤I
is used as the criteria for selecting optimal λ. In practice, we found that the tuning parameter
derived this way usually leads to an overestimation of joint rank r . In order to alleviate
0
the overfitting issue, we apply one standard error rule to the cross validated estimate. More
precisely, we select the largest tuning parameter within one standard error of the optimal
tuning parameter yielded by the cross validation criteria.
4.1 Three Data Views
First, we consider I = 3 data views. Apart from three design factors mentioned in the
last section (r , n and the way that D and D are generated), we also consider two
0 i i0
experimental designs when setting up the dimension for each data view: 1) balanced design:
p = p = p = 100; 2) unbalanced design: p = 100,p = 200,p = 300.
1 2 3 1 2 3
Results of rank selection accuracy for joint_LCA, JIVE_perm and JIVE_BIC are shown
in table 1 and 2. It can been seen that the proposed joint_LCA gives consistently good and
robust rank estimation across all scenarios. In particular, joint_LCA correctly identifies
the rank of the joint signal in at least 90 percent replications in the setting where r = 2.
0
One observation that might be a little surprising is that the increase in sample size does not
necessarily lead to improvement in rank selection accuracy for joint_LCA. There might be
two reasons underlying this phenomenon. First, an increase in the sample size will add more
information to both the joint signal and the individual signal, which does not necessarily
result in a larger separation between these two types of signals. Second, we adopt one
standard error rule in the rank selection procedure to reduce overfitting and the standard
error might get smaller as the sample size increases, making it harder to move the original
optimal estimate selected by the cross validation criteria. JIVE_perm and JIVE_BIC also
give comparably decent results in terms of rank selection in most scenarios, however, there are
a few cases where either JIVE_perm or JIVE_BIC yields significantly worse rank estimation.
JIVE_perm tends to overestimate the rank of the joint structure in case I where the strength
of the joint signal is comparable to that of the individual signal and an increase in the sample
11size n or the total dimension of data views fails to improve its performance in the rank
selection. While JIVE_BIC performs very well when n = 200, scenarios with a smaller
sample size (n = 100) and a larger r seems to be more challenging to JIVE_BIC, where it
0
underestimates r in the majority of replications.
0
Boxplots of the estimation error (14) are displayed in figure 1 and figure 2. On the first
look, r = 5 seems to be a more challenging case than r = 2 for all methods with regard to
0 0
the estimation of view-specific loading matrices, which is up to our expectation, since the
number of parameters in the view-specific loading matrices would increase in the number of
common components r . The estimates given by joint_LCA yield consistently small errors
0
with low variances under all combinations of n, p and r in case I and case II. Some outliers
0
which are located outside the whiskers of the boxplot can be observed for joint_LCA. The
error outliers mostly come from those replications in which the rank of the joint signal is
wrongly estimated, implying that the additional noisy columns incorrectly included in Vˆ
i
might be responsible for these relatively larger estimation errors. The estimation given by
JIVE_BIC and JIVE_perm is overall good in most settings. JIVE_perm yields inaccurate
estimates with larger variance in case I and JIVE_BIC significantly underperforms all other
methods when r = 5 and n = 100, which is as expected considering their unsatisfactory
0
performance in terms of rank selection in these scenarios. As for mCCA and mCIA, even
if the corresponding Vˆ is constructed given the true rank, there still exist a few outliers
i
which are most likely attributed to the inefficiency in estimating the signal, suggesting that
joint_LCA is relatively more robust in the signal estimation compared to these two sequential
methods. mCIA performs relatively worse than joint_LCA and mCCA, especially in case I.
The difference in the performance between mCIA and the other two methods might stem
from the fact that joint_LCA and mCCA are cross covariance based methods while the
formulation of mCIA starts from individual data matrices. And the results suggest that the
cross covariance based methods seem more preferable in terms of loading matrices estimation
under our generative model 13.
12Table 1: Results of rank selection accuracy for joint_LCA, JIVE_perm and
JIVE_BIC over 100 replications in the scenarios where there are I = 3
data views and the rank of joint structure r = 2. Experimental factors in-
0
clude 1) sample size: n ∈ {100,200}; 2) feature dimensions: (p ,p ,p ) ∈
1 2 3
{Balanced (100,100,100),Unbalanced (100,200,300)}; 3) the way that the diag-
onal matrices D and D are generated: case I and case II.
i i0
Case I Case II
Method Sample size Balanced Unbalanced Balanced Unbalanced
n = 100 0.94 0.92 0.96 0.94
joint_LCA
n = 200 0.92 0.90 0.90 0.90
n = 100 0.58 0.56 1.00 1.00
JIVE_perm
n = 200 0.50 0.42 1.00 1.00
n = 100 1.00 0.96 1.00 1.00
JIVE_BIC
n = 200 0.98 1.00 1.00 1.00
Table 2: Results of rank selection accuracy for joint_LCA, JIVE_perm and
JIVE_BIC over 100 replications in the scenarios where there are I = 3
data views and the rank of joint structure r = 5. Experimental factors in-
0
clude 1) sample size: n ∈ {100,200}; 2) feature dimensions: (p ,p ,p ) ∈
1 2 3
{Balanced (100,100,100),Unbalanced (100,200,300)}; 3) the way that the diag-
onal matrices D and D are generated: case I and case II.
i i0
Case I Case II
Method Sample size Balanced Unbalanced Balanced Unbalanced
n = 100 0.68 0.80 0.82 0.78
joint_LCA
n = 200 0.86 0.84 0.88 0.94
n = 100 0.62 0.62 0.92 0.98
JIVE_perm
n = 200 0.62 0.58 1.00 1.00
n = 100 0.00 0.00 0.14 0.00
JIVE_BIC
n = 200 0.98 1.00 1.00 1.00
13Figure 1: Displayed are boxplots of estimation error (cid:80)3 ∥Vˆ Vˆ⊺ − V V⊺ ∥2/3∥V V⊺ ∥2 for
i=1 i i i i 2 i i 2
joint_LCA, JIVE_perm, JIVE_BIC, mCCA and mCIA where there are I = 3 data views
and the rank of the joint structure is r = 2. Panel (a) and (b) show results for case I where
0
D and D are all generated from the standard uniform distribution; panel (c) and (d) show
i i0
results for case II where D and D are generated from the uniform distribution based on
√ √ i i0
[0.5 5, 5] and [0.5,1] respectively.
(a): Case I, r =2, p =p =p =100
0 1 2 3
n=100 n=200
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
(b): Case I, r =2, p =100, p =200, p =300
0 1 2 3
n=100 n=200
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
(c): Case II, r =2, p =p =p =100
0 1 2 3
n=100 n=200
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
(d): Case II, r =2, p =100, p =200, p =300
0 1 2 3
n=100 n=200
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
Method JIVE_BIC JIVE_PERM joint_LCA mCCA mCIA
14
rorre
noitamitsE
rorre
noitamitsE
rorre
noitamitsE
rorre
noitamitsEFigure 2: Displayed are boxplots of estimation error (cid:80)3 ∥Vˆ Vˆ⊺ − V V⊺ ∥2/3∥V V⊺ ∥2 for
i=1 i i i i 2 i i 2
joint_LCA, JIVE_perm, JIVE_BIC, mCCA and mCIA where there are I = 3 data views
and the rank of the joint structure is r = 5. Panel (a) and (b) show results for case I where
0
D and D are all generated from the standard uniform distribution; panel (c) and (d) show
i i0
results for case II where D and D are generated from the uniform distribution based on
√ √ i i0
[0.5 5, 5] and [0.5,1] respectively.
(a): Case I, r =5, p =p =p =100
0 1 2 3
n=100 n=200
1.25 1.25
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
(b): Case I, r =5, p =100, p =200, p =300
0 1 2 3
n=100 n=200
1.25 1.25
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
(c): Case II, r =5, p =p =p =100
0 1 2 3
n=100 n=200
1.25 1.25
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
(d): Case II, r =5, p =100, p =200, p =300
0 1 2 3
n=100 n=200
1.25 1.25
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
Method JIVE_BIC JIVE_PERM joint_LCA mCCA mCIA
15
rorre
noitamitsE
rorre
noitamitsE
rorre
noitamitsE
rorre
noitamitsE4.2 Four Data Views
We then look into a setting where the multiview data consists of I = 4 sources. Similar to
the scenario where we have three data views, we consider both balanced and unbalanced
experiment design. Specifically, the dimensions for different data views are either set as equal
(p = p = p = p = 100) or unequal (p = 100,p = 200,p = 300,p = 400).
1 2 3 4 1 2 3 4
Results in table 3 and table 4 show that the joint_LCA correctly identifies the rank
for the joint structure in over 90% replications across all scenarios. Moreover, a boost in
the estimation performance can also be observed in figure (3) and figure (4): additional
informationfromanotherdataviewhelpyieldmoreaccurate(smallermedianestimationerror)
and more stable (lower estimation variance) estimation of the view-specific loading matrices
for all methods. Specifically for joint_LCA, there exist fewer and less extreme outliers mainly
due to an enhancement in the rank selection accuracy. These observations illustrate the power
of integrating more related data sources into analysis. As for JIVE_PERM and JIVE_BIC,
while there are slight improvements in the scenarios where they fail to do well when only
three data views are in use, their performance is still much inferior to that of joint_LCA in
terms of both rank identification and signal estimation.
Table 3: Results of rank selection accuracy for joint_LCA, JIVE_perm and
JIVE_BIC over 100 replications in the scenarios where there are I = 4
data views and the rank of joint structure r = 2. Experimental factors in-
0
clude 1) sample size: n ∈ {100,200}; 2) feature dimensions: (p ,p ,p ,p ) ∈
1 2 3 4
{Balanced (100,100,100,100),Unbalanced (100,200,300,400)}; 3) the way that
the diagonal matrices D and D are generated: case I and case II.
i i0
Case I Case II
Method Sample size Balanced Unbalanced Balanced Unbalanced
n = 100 0.98 1.00 0.98 1.00
joint_LCA
n = 200 1.00 1.00 0.98 0.98
n = 100 0.72 0.58 1.00 1.00
JIVE_perm
n = 200 0.68 0.62 1.00 1.00
n = 100 1.00 1.00 1.00 1.00
JIVE_BIC
n = 200 1.00 1.00 1.00 1.00
16Table 4: Results of rank selection accuracy for joint_LCA, JIVE_perm and
JIVE_BIC over 100 replications in the scenarios where there are I = 4
data views and the rank of joint structure r = 5. Experimental factors in-
0
clude 1) sample size: n ∈ {100,200}; 2) feature dimensions: (p ,p ,p ,p ) ∈
1 2 3 4
{Balanced (100,100,100,100),Unbalanced (100,200,300,400)}; 3) the way that
the diagonal matrices D and D are generated: case I and case II.
i i0
Case I Case II
Method Sample size Balanced Unbalanced Balanced Unbalanced
n = 100 0.92 0.98 0.98 1.00
joint_LCA
n = 200 1.00 1.00 1.00 1.00
n = 100 0.80 0.70 1.00 1.00
JIVE_perm
n = 200 0.60 0.64 1.00 1.00
n = 100 0.00 0.00 0.32 0.02
JIVE_BIC
n = 200 0.98 1.00 1.00 1.00
17Figure 3: Displayed are boxplots of estimation error (cid:80)3 ∥Vˆ Vˆ⊺ − V V⊺ ∥2/4∥V V⊺ ∥2 for
i=1 i i i i 2 i i 2
joint_LCA, JIVE_perm, JIVE_BIC, mCCA and mCIA where there are I = 4 data views
and the rank of the joint structure is r = 2. Panel (a) and (b) show results for case I where
0
D and D are all generated from the standard uniform distribution; panel (c) and (d) show
i i0
results for case II where D and D are generated from the uniform distribution based on
√ √ i i0
[0.5 5, 5] and [0.5,1] respectively.
(a): Case I, r =2, p =p =p =p =100
0 1 2 3 4
n=100 n=200
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
(b): Case I, r =2, p =100, p =200, p =300, p =400
0 1 2 3 4
n=100 n=200
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
(c): Case II, r =2, p =p =p =p =100
0 1 2 3 4
n=100 n=200
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
(d): Case II, r =2, p =100, p =200, p =300, p =400
0 1 2 3 4
n=100 n=200
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
Method JIVE_BIC JIVE_PERM joint_LCA mCCA mCIA
18
rorre
noitamitsE
rorre
noitamitsE
rorre
noitamitsE
rorre
noitamitsEFigure 4: Displayed are boxplots of estimation error (cid:80)3 ∥Vˆ Vˆ⊺ − V V⊺ ∥2/4∥V V⊺ ∥2 for
i=1 i i i i 2 i i 2
joint_LCA, JIVE_perm, JIVE_BIC, mCCA and mCIA where there are I = 3 data views
and the rank of the joint structure is r = 5. Panel (a) and (b) show results for case I where
0
D and D are all generated from the standard uniform distribution; panel (c) and (d) show
i i0
results for case II where D and D are generated from the uniform distribution based on
√ √ i i0
[0.5 5, 5] and [0.5,1] respectively.
(a): Case I, r =5, p =p =p =p =100
0 1 2 3 4
n=100 n=200
1.25 1.25
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
(b): Case I, r =5, p =100, p =200, p =300, p =400
0 1 2 3 4
n=100 n=200
1.25 1.25
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
(c): Case II, r =5, p =p =p =p =100
0 1 2 3 4
n=100 n=200
1.25 1.25
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
(d): Case II, r =5, p =100, p =200, p =300, p =400
0 1 2 3 4
n=100 n=200
1.25 1.25
1.00 1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
Method JIVE_BIC JIVE_PERM joint_LCA mCCA mCIA
19
rorre
noitamitsE
rorre
noitamitsE
rorre
noitamitsE
rorre
noitamitsE5 Real Data Application
5.1 Nutrimouse Data
The nutrimouse data set comes from a nutrition study in mouse (Martin et al. (2007)). In this
study, two sets of variables were acquired from forty mice. In specific, one feature set contains
expressions of 120 cells measured in liver cells, and the other set concerns measurements
of concentrations of 21 hepatic fatty acids (FA). In addition, biological units (mice) are
cross-classified according to two factors:
• Genotype: wide-type (WT) mice and PPARα deficient (PPARα) mice
• Diet: reference diet (REF), saturated FA diet (COC), ω6 FA-rich diet (SUN), ω3
FA-rich diet (LIN), and the FISH diet.
The Nutrimouse data view is available within package CCA. We applied our method to the
Figure 5: Nutrimouse
Nutrimouse data, and the rank of the common space is estimated to be two. We project the
each data matrix onto these two dimensions and we also plot the correlation between the
projected score and the original features to see the contribution from each feature to each
common component, which is shown in figure (5). The left panel shows a clear separation of
genotypes along the second dimension, though, there are a few overlaps. This agrees with
the finding in Martin et al. (2007) that PPARα mice have high concentrations of linoleic acid
20(C18.2n.6), which is also consistent with the observation in the right panel that C18.2n.6 has
high positive coordinates on the second dimension.
5.2 Boston Housing Data
Boston Housing data set (http://math.furman.edu/~dcs/courses/math47/R/library/
mlbench/html/BostonHousing.html) which contains N = 506 14-dimensional instances.
The predictors can be divided into 3 different data sets with dimensions n = 4,n = 6,n = 3:
1 2 3
• X (AN, AGE, TAX, RM): Variables directly related with the housing market.
1
• X (CRIM, INDUS, NOX, PRTATIO, B, LSTAT): Variables indirectly related with
2
the housing the housing market.
• X (CHAS, DIS, RAD): Geographical variables.
3
In addition, the target variable is MEDV, the median value of owner-occupied homes in USD.
A detailed description of variables listed above can be found at http://math.furman.edu/
~dcs/courses/math47/R/library/mlbench/html/BostonHousing.html
This data view has been widely adopted to illustrate how to build a model to predict
the housing values using the given features by different techniques. We propose to construct
a new feature set based on the score matrix from canonical correlation analysis and then
use regression algorithms to predict the housing prices. Specifically, our method can be
applied to the Boston Housing data and obtain the canonical loading matrices {V ,V ,V }
1 2 3
for {X ,X ,X }. The score matrix is calculated as U = X1V1D 1−1+X2V2D 2−1+X3V3D 3−1, which is
1 2 3 3
then fed into regression algorithms like support vector regression (SVR) and regression tree
(CART). For the purpose of comparison, we also consider performing principle component
analysis on the original feature sets first and using thee obtained principle components as
the features. According to the scree plot (6), we use either the first two/three principal
components as the new feature set.
We compared the performance/effectiveness between different feature sets by the squared
root of the mean squared error obtained from 10-fold cross validation. For the canonical
variate features, the rMSE are 6.88/6.65 for the SVR model and CART respectively. For
the principle component features, the rMSE corresponding to the SVR are 6.81 and 6.39
for the CART. The principal component features perform slightly better since it contains
comprehensive information for all kinds of structures while canonical variate features are
constructed based only on the common structure.
21Figure 6: Scree Plot
5.3 Russett Data
The Russett data view contains three data views for 47 countries. This data view was collected
in order to study the relationship between Agricultural Inequality, Industrial Development,
and Political Instability.
• X (Gini, Farm, Rent): Variables related to industrial development
1
• X (Gnpr, Labo): Variables that measure industrial development
2
• X (Inst, Ecks, Deat, Demo): Variables that describe political instability. In particular,
3
Demo is a categorical variable that describes the political regime: stable democracy,
unstable democracy and dictatorship.
Twopairsofcanonicalvariatesareidentifiedbyourmethod. Theleftgraphicalplotisobtained
by plotting the first pair of canonical variates (X V , X V ), labeled by their political regime
1 1 2 2
in 1960. Clearly, the first component exhibits a separation among regimes. For instance,
the countries with dictatorships are concentrated in the upper right quadrant. It is worth
noting that the factors labor and gini have a large positive contribution to the first dimension
from the right plot. High percentage of labor force employed in agriculture indicates below
average industrial development and high gini index is related to unequal land distribution. It
is difficult for a country to escape dictatorship when its industrial development is low while
its agricultural inequality is high. Our result also agrees with the finding in some countries
22with unstable democracy labelled by green, (Greece, Brazil, Chile, Argentina ) locate in the
first quadrant, which became dictatorships for a period of time after 1960.
Score plot Correlation plot
India Bolivia 1.0
2 Libia
SouthVietnamHoEngdyuprtas
Taiwan
PGeurauI tr ea mk
ala
1 Yugoslavia Philippine NDicoamraingiucaSanaBRlvrEaeacspduiuolabrdliocr 0.5 labo
Japan
PanamaGreS Cepc oae li oCn mosbtiaaR Cic ha
ile
Poland Cuba gnpr
dictator
0 Irland Finland AuUstrruiaIgtauAlarygye Vn eti nn ea zuela 0.0 inst
ecks
−1 Denmark FranceNorway demostab
gini
Luxembur Wg estGermany farm
TheNetherlands −0.5 death
NewZealand
Sweden Australia
Belgium
−2 SwitzCearnlaandda
UKUSA
−1.0
rent
−3 −2 −1 0 1 2 −1.0 −0.5 0.0 0.5 1.0
comp1 dim1
a demostab a demoinst a dictator a X1 a X2 a X3
Figure 7: Russett Data
5.4 Multiview Single Cell Data
Next, we consider a collection of four single cell datasets from Polioudakis et al. (2019). In
this study, developing brain tissue samples were obtained from four patient donors (donor
372, donor 371, donor 370, donor 368). Dissection were first performed on fetal brain tissues
and single cells were isolated for analysis. In this process, sixteen types of cells at different
stages of neuronal differentiation and maturation were identified. Drop-seq (Macosko et al.,
2015) were then run on single cells to obtain high quality gene expression profiles for around
40, 000 cells. This study provides us with the ability to deepen our understanding in human
neurogenesis, cortical evolution and the cellular basis of neuropsychiatric disease (Polioudakis
et al., 2019). Here, we only include ten major cell types for analysis. As a result, we have
four data views:
X ∈ R35543×8530,X ∈ R35543×9082,X ∈ R35543×7066,X ∈ R35543×5454.
1 2 3 4
To illustrate the distribution of cell types, t-distributed stochastic neighbor embedding (tSNE)
is applied to perform dimension reduction for each data view. Then we project each data
view onto the two-dimensional subspace found by tSNE where each cell is colored by the
23
2pmoc 2midcorresponding cell type. As can be seen from Figure 8, the cell types can be very well spatially
segregated in this way.
Figure 8: Scatterplot visualization of cells colored by cell types based on the first two
directions obtained by tSNE.
We apply joint_LCA to this multiview single cell data. The estimated number of common
components is rˆ = 8 and we obtain view-specific estimation Vˆ and Dˆ . We look into each
0 i i
column in the loading matrix multiplied by the diagonal matrix, i.e., Vˆ Dˆ , which is referred
i i
to as a common component. Based on the 2D scatterplot in Figure 8, each cell point in Figure
9 is colored by the corresponding value in the common component. With respect to each
row, the color distribution is quite similar across different data views, suggesting that the
common components found capture shared information across different donors. In addition,
we identify some important genes by looking at the correlation between the gene expression
data X and the common components Vˆ Dˆ , including genes HES1, MEF2C, MKI67. For
i i i
instance, gene MKI67 is most positively correlated with the 7th component. These three
genes identified have been shown to be informative in the neurogenesis process (Polioudakis
et al. (2019); Mathews et al. (2017)). Additionally, we observe some clusters of cells with
either the highest or the lowest values for each component in Figure 8, which turn out to be
connected with the identified genes. Specifically, for each common component, the identified
gene tends to have high expression level in the clusters of cells which have the highest or
lowest values in the component.
24Figure 9: Scatterplot of cells colored by the corresponding value in the common component
(a specific column in Vˆ Dˆ ,1 ≤ i ≤ 4). Results for component 2, 5 and 7 are shown below.
i i
Table 5: Mean expression level of genes HES1, MEF2C, MKI67 in each type of cells.
HES1 vRG oRG PgS PgG2M IP ExN ExM ExM-U ExDp1 ExDp2
7.32 7.71 3.35 2.79 0.79 0.16 0.19 0.13 0.13 0.00
MEF2C vRG oRG PgS PgG2M IP ExN ExM ExM-U ExDp1 ExDp2
0.41 0.35 0.29 0.29 0.29 0.48 5.42 9.24 10.27 10.10
MKI67 vRG oRG PgS PgG2M IP ExN ExM ExM-U ExDp1 ExDp2
1.68 2.38 8.54 12.21 1.47 0.36 0.15 0.23 0.07 0.17
256 Discussion
There has been growing interests and demands in studying heterogeneous data from multiple
sources in a collective way. It is of particular interest to capture variation common to all
data views, which is helpful to understand the scientific association between different data
views. Existing CCA-based methods focus on projecting each data view onto a common
latent subspace. Specifically, they take advantage of the pairwise cross covariance matrices,
and the view-specific orthonormal loadings corresponding to the joint structure are extracted
in a successive way. One drawback associated with these sequential techniques is lack of
an objective way to determine the number of shared components, namely, the rank of the
common latent subspace. We propose the joint linked component analysis (joint_LCA)
for multiview data, which seeks for a set of canonical variates for each data view while
simultaneously determining the rank of the common latent subspace. A novel nuclear-norm
based penalty is designed to perform rank selection and a simple refitting procedure is adopted
to correct the shrinkage bias in our estimate. We investigate the empirical performance
of joint_LCA in various simulation settings including small and large number of common
components, low and high strength of joint signal and different number of data views. In
comparison to sequential CCA-based methods (mCCA, mCIA), or JIVE with two schemes for
rank selection, joint_LCA yields consistently and reasonably well results across all scenarios
in terms of rank selection and loading matrix estimation, whereas its competitors perform
poorly in some of these settings. We also apply joint_LCA to several real multiview data
views to illustrate the power of joint_LCA as a useful tool for exploring common relationships
among multiple data views.
The proposed method can be generalized to high dimensional data sets where sparsity is
desired. One straightforward way is to impose structural penalty on loading matrices V in
i
the objective function (2). However, we expect this to be a very challenging problem from
a computational perspective, due to the complex interaction between the sparsity penalty
and the orthogonality constraint. Kallus et al. (2019) deals with a similar problem. They
address this challenge by representing each orthonormal matrix V as a multiplication of
i
Givens rotation matrices (Shepard et al., 2015) and the optimization of the new objective
function is performed with Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm. While
the reparametrization can successfully reduce the number of parameters and remove the
orthogonal constraints, the computational cost of this strategy is still too high to be scaled to
large data sets. Hence, future work is needed to develop a computationally efficient method
that extends (2) to accommodate high dimensional data.
267 Appendix
In this section, we want to establish the connection between our proposed method and the
generalized canonical correlation methods in the literature. Our finding is that, in the case
I = 2, r = 1, the objective function below:
0
min ∥Sˆ −V D D⊺ V⊺ ∥2
12 1 1 2 2 F
V1,V2,D1,D2
s.t. V⊺ V = I,i = 1,2
i i
is equivalent to the formulation of diagonalized CCA proposed in Witten et al. (2009)
(cid:88) ⊺ ⊺
max V X X V
i i j j
V1,···,VI
(15)
i<j
s.t ∥V ∥2 ≤ 1
i
If I = 3 and r = 1, the optimization problem
0
min (cid:88) ∥Sˆ −V D D⊺ V⊺ ∥2
ij i i j j F
Vi,Di,i=1,2,3
1≤i<j≤3
s.t. V⊺ V = I,i = 1,2,3
i i
is equivalent to the SSQCOV-1 problem in Hanafi and Kiers (2006), which is defined as
(cid:88)
max cov2(X V ,X V )
i i j j
V1,V2,V3
1≤i<j≤3
s.t. ∥V ∥ = 1,i = 1,2,3
i 2
Denote the objective function as f = Suppose that the true rank is r = 1,then D = d ,
1 1
D = d then minimizing the objective function is equivalent to minimizing
2 2
f(V ,V ,D ,D ) = −2d d ⟨Sˆ ,V V⊺ ⟩+d2d2
1 2 1 2 1 2 12 1 2 1 2 (16)
s.t. −d ≤ 0,V⊺ V = I,i = 1,2
i i i
Keeping V ,i = 1,2 fixed, the optimization problem with respect to d and d is
i 1 2
f(D ,D ) = −2d d ⟨Sˆ ,V V⊺ ⟩+d2d2
1 2 1 2 12 1 2 1 2
s.t. −d ≤ 0,i = 1,2
i
27The Langrangian multiplier associated with this problem is,
L(D ,D ,v ,v ) = −2d d ⟨Sˆ ,V V⊺ ⟩+d2d2 −v d −v d
1 2 1 2 1 2 12 1 2 1 2 1 1 2 2
The corresponding KKT conditions are:
∂L
= −2d ⟨Sˆ ,V V⊺ ⟩+2d d2 −v = 0
∂d 2 12 1 2 1 2 1
1
∂L
= −2d ⟨Sˆ ,V V⊺ ⟩+2d2d −v = 0
∂d 1 12 1 2 1 2 2
2
−v d = 0,i = 1,2
i i
−d ≤ 0,i = 1,2
i
∂L
d = −2d d ⟨Sˆ ,V V⊺ ⟩+2d2d2 −d v
1 ∂d 1 2 12 1 2 1 2 1 1
1
= −2d d ⟨Sˆ ,V V⊺ ⟩+2d2d2
1 2 12 1 2 1 2
ˆ ⊺
= 2d d [d d −⟨S ,V V ⟩]
1 2 1 2 12 1 2
= 0
Since d > 0,d > 0, we have
1 2
d d = ⟨Sˆ ,V V⊺ ⟩ > 0 (17)
1 2 12 1 2
Substituting this into (16),
f(V ,V ) = −(⟨Sˆ ,V V⊺ ⟩)2
1 2 12 1 2
Due to (17), this is equivalent to minimizing
ˆ ⊺ ⊺ˆ
−⟨S ,V V ⟩ = −V S V
12 1 2 1 12 2
Now suppose that we have three data views, the objective function becomes
f(V ,V ,V ,D ,D ,D ) = −2d d ⟨Sˆ ,V V⊺ ⟩+d2d2
1 2 3 1 2 3 1 2 12 1 2 1 2
−2d d ⟨Sˆ ,V V⊺ ⟩+d2d2 −2d d ⟨Sˆ ,V V⊺ ⟩+d2d2
1 3 13 1 3 1 3 2 3 23 2 3 2 3
s.t. −d ≤ 0,V⊺ V = I,i = 1,2,3
i i i
28Similarly, we obtain that
∂f
= −2d ⟨Sˆ ,V V⊺ ⟩+2d d2 −2d ⟨Sˆ ,V V⊺ ⟩+2d d2 +v = 0
∂d 2 12 1 2 1 2 3 13 1 3 1 3 1
1
∂f
= −2d ⟨Sˆ ,V V⊺ ⟩+2d2d −2d ⟨Sˆ ,V V⊺ ⟩+2d d2 +v = 0
∂d 1 12 1 2 1 2 3 23 2 3 2 3 2
2
∂f
= −2d ⟨Sˆ ,V V⊺ ⟩+2d2d −2d ⟨Sˆ ,V V⊺ ⟩+2d2d +v = 0
∂d 1 13 1 3 1 3 2 23 2 3 2 3 3
3
∂f
d = −2d d ⟨Sˆ ,V V⊺ ⟩+2d2d2 −2d d ⟨Sˆ ,V V⊺ ⟩+2d2d2 +d v = A +A = 0
1 ∂d 1 2 12 1 2 1 2 1 3 13 1 3 1 3 1 1 1 2
1
∂f
d = −2d d ⟨Sˆ ,V V⊺ ⟩+2d2d2 −2d d ⟨Sˆ ,V V⊺ ⟩+2d2d2 +d v = A +A = 0
2 ∂d 1 2 12 1 2 1 2 2 3 23 2 3 2 3 2 2 1 3
2
∂f
d = −2d d ⟨Sˆ ,V V⊺ ⟩+2d2d2 −2d d ⟨Sˆ ,V V⊺ ⟩+2d2d2 +d v = A +A = 0
3 ∂d 1 3 13 1 3 1 3 2 3 23 2 3 2 3 3 3 2 3
3
where
A = −2d d ⟨Sˆ ,V V⊺ ⟩+2d2d2
1 1 2 12 1 2 1 2
A = −2d d ⟨Sˆ ,V V⊺ ⟩+2d2d2
2 1 3 13 1 3 1 3
A = −2d d ⟨Sˆ ,V V⊺ ⟩+2d2d2
3 2 3 23 2 3 2 3
It is straightforward to derive that A = A = A = 0, suggesting that
1 2 3
ˆ ⊺
d d = ⟨S ,V V ⟩
1 2 12 1 2
ˆ ⊺
d d = ⟨S ,V V ⟩
1 3 13 1 3
ˆ ⊺
d d = ⟨S ,V V ⟩
2 3 23 2 3
f(V ,V ,V ) = −(⟨Sˆ ,V V⊺ ⟩)2 −(⟨Sˆ ,V V⊺ ⟩)2 −(⟨Sˆ ,V V⊺ ⟩)2
1 2 3 12 1 2 13 1 3 23 2 3
29References
Shotaro Akaho. Akernelmethod for canonicalcorrelation analysis. arXiv preprint cs/0609071,
2006.
Galen Andrew, Raman Arora, Jeff Bilmes, and Karen Livescu. Deep canonical correlation
analysis. In International conference on machine learning, pages 1247–1255. PMLR, 2013.
James C Costello, Laura M Heiser, Elisabeth Georgii, Mehmet Gönen, Michael P Menden,
Nicholas J Wang, Mukesh Bansal, Petteri Hintsanen, Suleiman A Khan, John-Patrick
Mpindi, et al. A community effort to assess and improve drug sensitivity prediction
algorithms. Nature biotechnology, 32(12):1202, 2014.
Qing Feng, Meilei Jiang, Jan Hannig, and JS Marron. Angle-based joint and individual
variation explained. Journal of multivariate analysis, 166:241–265, 2018.
Irina Gaynanova and Gen Li. Structural learning and integrative decomposition of multi-view
data. Biometrics, 75(4):1121–1132, 2019.
Mohamed Hanafi and Henk AL Kiers. Analysis of k sets of data, with differential emphasis
on agreement between and within sets. Computational Statistics & Data Analysis, 51(3):
1491–1508, 2006.
Yangqing Jia, Mathieu Salzmann, and Trevor Darrell. Factorized latent spaces with structured
sparsity. Advances in neural information processing systems, 23, 2010.
JonatanKallus, PatrikJohansson, SvenNelander, andRebeckaJörnsten. Mm-pca: integrative
analysis of multi-group and multi-view data. arXiv preprint arXiv:1911.04927, 2019.
Jon R Kettenring. Canonical analysis of several sets of variables. Biometrika, 58(3):433–451,
1971.
Boyoung Kim, Yunju Im, and Keun Yoo Jae. Seedcca: An integrated r-package for canonical
correlation analysis and partial least squares. R J., 13(1):7, 2021.
Anshul Kundaje, Wouter Meuleman, Jason Ernst, Misha Bilenky, Angela Yen, Alireza Heravi-
Moussavi, Pouya Kheradpour, Zhizhuo Zhang, Jianrong Wang, Michael J Ziller, et al.
Integrative analysis of 111 reference human epigenomes. Nature, 518(7539):317–330, 2015.
Eric F Lock, Katherine A Hoadley, James Stephen Marron, and Andrew B Nobel. Joint and
individual variation explained (jive) for integrated analysis of multiple data types. The
annals of applied statistics, 7(1):523, 2013.
30Evan Z Macosko, Anindita Basu, Rahul Satija, James Nemesh, Karthik Shekhar, Melissa
Goldman, Itay Tirosh, Allison R Bialas, Nolan Kamitaki, Emily M Martersteck, et al.
Highly parallel genome-wide expression profiling of individual cells using nanoliter droplets.
Cell, 161(5):1202–1214, 2015.
Pascal GP Martin, Hervé Guillou, Frédéric Lasserre, Sébastien Déjean, Annaig Lan, Jean-
Marc Pascussi, Magali SanCristobal, Philippe Legrand, Philippe Besse, and Thierry Pineau.
Novel aspects of pparα-mediated regulation of lipid and xenobiotic metabolism revealed
through a nutrigenomic study. Hepatology, 45(3):767–777, 2007.
Kathryn J Mathews, Katherine M Allen, Danny Boerrigter, Helen Ball, Cynthia Shan-
non Weickert, and Kay L Double. Evidence for reduced neurogenesis in the aging human
hippocampus despite stable stem cell markers. Aging cell, 16(5):1195–1199, 2017.
Thomas Melzer, Michael Reiter, and Horst Bischof. Nonlinear feature extraction using
generalized canonical correlation analysis. In International Conference on Artificial Neural
Networks, pages 353–360. Springer, 2001.
EunJeongMinandQiLong. Sparsemultipleco-inertiaanalysiswithapplicationtointegrative
analysis of multi-omics data. BMC bioinformatics, 21:1–12, 2020.
Damon Polioudakis, Luis de la Torre-Ubieta, Justin Langerman, Andrew G Elkins, Xu Shi,
Jason L Stein, Celine K Vuong, Susanne Nichterwitz, Melinda Gevorgian, Carli K Opland,
et al. A single-cell transcriptomic atlas of human neocortical development during mid-
gestation. Neuron, 103(5):785–801, 2019.
Ron Shepard, Scott R Brozell, and Gergely Gidofalvi. The representation and parametrization
of orthogonal matrices. The Journal of Physical Chemistry A, 119(28):7924–7939, 2015.
Katrijn Van Deun, Tom F Wilderjans, Robert A Van Den Berg, Anestis Antoniadis, and
Iven Van Mechelen. A flexible framework for sparse simultaneous component based data
integration. BMC bioinformatics, 12(1):1–17, 2011.
Meiling Wang, Wei Shao, Xiaoke Hao, Li Shen, and Daoqiang Zhang. Identify consistent cross-
modality imaging genetic patterns via discriminant sparse canonical correlation analysis.
IEEE/ACM transactions on computational biology and bioinformatics, 18(4):1549–1561,
2019.
DanielaMWittenandRobertJTibshirani. Extensionsofsparsecanonicalcorrelationanalysis
with applications to genomic data. Statistical applications in genetics and molecular biology,
8(1), 2009.
31Daniela M Witten, Robert Tibshirani, and Trevor Hastie. A penalized matrix decomposi-
tion, with applications to sparse principal components and canonical correlation analysis.
Biostatistics, 10(3):515–534, 2009.
Guoxu Zhou, Andrzej Cichocki, Yu Zhang, and Danilo P Mandic. Group component analysis
for multiblock data: Common and individual feature extraction. IEEE transactions on
neural networks and learning systems, 27(11):2426–2439, 2015.
Xiaowei Zhuang, Zhengshi Yang, and Dietmar Cordes. A technical review of canonical
correlation analysis for neuroscience applications. Human Brain Mapping, 41(13):3807–
3833, 2020.
32