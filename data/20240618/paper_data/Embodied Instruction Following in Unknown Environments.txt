Embodied Instruction Following in Unknown
Environments
ZhenyuWu1,ZiweiWang2,XiuweiXu3,JiwenLu3,HaibinYan1∗
1BeijingUniversityofPostsandTelecommunications
2CarnegieMellonUniversity,3TsinghuaUniversity
{wuzhenyu, eyanhaibin}@bupt.edu.cn, ziweiwa2@andrew.cmu.edu
xxw21@mails.tsinghua.edu.cn, lujiwen@tsinghua.edu.cn
https://gary3410.github.io/eif_unknown/
Abstract
Enablingembodiedagentstocompletecomplexhumaninstructionsfromnatural
languageiscrucialtoautonomoussystemsinhouseholdservices. Conventional
methodscanonlyaccomplishhumaninstructionsintheknownenvironmentwhere
allinteractiveobjectsareprovidedtotheembodiedagent,anddirectlydeployingthe
existingapproachesfortheunknownenvironmentusuallygeneratesinfeasibleplans
thatmanipulatenon-existingobjects. Onthecontrary,weproposeanembodied
instructionfollowing(EIF)methodforcomplextasksintheunknownenvironment,
wheretheagentefficientlyexplorestheunknownenvironmenttogeneratefeasible
planswithexistingobjectstoaccomplishabstractinstructions. Specifically,we
buildahierarchicalembodiedinstructionfollowingframeworkincludingthehigh-
leveltaskplannerandthelow-levelexplorationcontrollerwithmultimodallarge
languagemodels. Wethenconstructasemanticrepresentationmapofthescene
withdynamicregionattentiontodemonstratetheknownvisualclues,wherethe
goal of task planning and scene exploration is aligned for human instruction.
Forthetaskplanner,wegeneratethefeasiblestep-by-stepplansforhumangoal
accomplishmentaccordingtothetaskcompletionprocessandtheknownvisual
clues. Fortheexplorationcontroller,theoptimalnavigationorobjectinteraction
policyispredictedbasedonthegeneratedstep-wiseplansandtheknownvisual
clues. Theexperimentalresultsdemonstratethatourmethodcanachieve45.09%
success rate in 204 complex human instructions such as making breakfast and
tidyingroomsinlargehouse-levelscenes.
1 Introduction
Buildingintelligentautonomoussystems[10,21,5,1]tocompletehouseholdtaskssuchasmaking
breakfastandtidyingroomsishighlydemandedtoreducethelaborercostinourdailylife. Theagent
isrequiredtounderstandthevisualcluesofthesurroundingsceneandthelanguageinstructions,and
feasibleactionplansarethengeneratedforobjectinteractionwiththegoalofhighsuccessrateand
lowactioncosttoaccomplishhumandemands.
Toachievethis,end-to-endmethods[24,37,32]directlygeneratethelow-levelactionsfromraw
imageinputandnaturallanguagewiththesupervisionofexperttrajectories. Toreducethelearning
difficultiesinthecomplextask,modularmethods[7,12,22,16],sequentiallylearntheinstruction
comprehension,stateperception,spatialmemoryconstruction,high-levelplanningandlow-level
controltocompletehumangoals. Sinceembodiedagentsareexpectedtocompletemorediverseand
complexinstructions,largelanguagemodels(LLMs)arewidelyemployedinEIF[17,34,8,20,27]
∗Correspondingauthor.
Preprint.Underreview.
4202
nuJ
71
]OR.sc[
1v81811.6042:viXraInstruction: I need to drink water
T=2 T=5 T=6 T=2 T=4 T=5
Move to find bottle Move to find bottle Move to find bottle Navigate to the frontier Pick up the mug Put the mug to get drink
(a) Existing approaches (b) Our approach
Instruction: I need to drink water
AA BB
T=2 T=5 T=6 T=2 T=3 T=5
Move to find bottle Move to find bottle Move to find bottle Navigate to frontier A Pick up the mug Put the mug to get drink
(a) Existing approaches (b) Our approach
Instruction: I need to drink water
AA BB
T=2 T=5 T=6 T=2 T=3 T=5
Move to find bottle Move to find bottle Move to find bottle Navigate to frontier A Pick up the mug Put the mug to get drink
(a) Existing approaches (b) Our approach
Figure1:ComparisonbetweenconventionalEIFmethodsandourapproachinunknownenvironments.
Existingmethodsfailtocompletetheinstructionevenwithlongexplorationcost,whileourmethod
efficientlyachievesthegoalwithefficientnavigationandobjectinteraction.
duetotheirstrongreasoningpowerandhighgeneralizationability. However,existingmethodscan
onlygenerateplansinknownenvironmentswherecategoriesofallinteractableobjectsinthescene
aregiventoLLMs. Sincetheagentdoesnotknowtheobjectsintheunknownenvironment, the
generatedplansareusuallyinfeasiblebecauseofinteractingwithnon-existingobjects. Figure1(a)
demonstratesanexampleofexistingmethods,wheretheagentisunawarethatnobottlesexistinthe
unknownenvironment. Interactingwiththenon-existentbottlesbasedontheinfeasibleplanfailsto
accomplishthehumangoalsofwaterserving.
Inrealisticdeploymentscenarios,householdagentsusuallyworkinunknownenvironmentswithout
storedscenemaps. Buildingscenemapsinadvancecannotaccuratelyrepresentthescene,where
objectpropertiessuchaslocationandexistencechangefrequentlyduetohumanactivityindailylife.
Forexample,themugmaybeonthediningtableandthecoffeetablerespectivelywhenhumans
arehavingdinnerandwatchingTV.Meanwhile,potatoesmighthavebeenconsumedandtomatoes
arethenpurchasedforthenextbreakfast. Therefore,failingtogeneratefeasibleplansinunknown
environmentsstrictlylimitsthepracticalityoftheembodiedagents. Theagentworkinginrealistic
deploymentscenariosisrequiredtobuildreal-timescenemaps,wherefeasibleplansaregenerated
withminimalexplorationcost.
Inthispaper,weproposeanEIFmethodforcomplextasksintheunknownenvironment. Different
from conventional methods that assumes knowing interactable objects in advance, our method
navigatestheunknownenvironmenttoefficientlydiscoverobjectsthatarerelevanttothecomplex
humanrequirements. Therefore, theembodiedagentcangeneratefeasibletaskplansinrealistic
indoorsceneswherethelocationsandexistenceofobjectsarefrequentlychanging. Figure1(b)
alsodemonstratesthesameexampleofwaterservingimplementedbyourmethod,andouragent
efficientlydiscoversthemugandusesitasthereceptacleofwaterbecausenobottlesexistinthe
scene. WefirstconstructahierarchicalEIFframeworkincludingthehigh-leveltaskplannerand
thelow-levelexplorationcontrollerwithmulti-modalLLMs,whicharefinetunedbythelarge-scale
generatedtrajectoriesofthecomplexEIFtasks. Wethendesignascene-levelsemanticrepresentation
maptodepictthevisualcluesintheknownarea,throughwhichthegoalsofthetaskplannerandthe
explorationcontrollercanbealignedtofeasiblycompletehumaninstructions.
Morespecifically,thegoalofthetaskplanneristogeneratefeasibleplansforhumaninstruction
includingnavigationandmanipulationinnaturallanguage. Thetaskplannerpredictsthenextstep
basedonthesemanticrepresentationmapandthetaskcompletionprocess. Theexplorationcontroller
aimsatdiscoveringtask-relatedobjectswithlowactioncost,whichselectstheoptimalnavigation
policyfromallnavigablebordersorobjectinteractionpolicyaccordingtothesemanticrepresentation
mapandthegeneratedstep-wiseplans. Forthescene-levelsemanticfeaturemap,weprojectthe
CLIPfeaturesofcollectedRGBimagesduringexplorationtothetop-downmapwithdynamicregion
attention,whichpreservesthetask-relevantvisualinformationinthemapwithoutredundancy. The
experimentalresultsinProcTHORandAI2THORsimulationenvironmentsshowthatourmethod
canachieve45.09%successratein204complexhumaninstructionsinlargehouse-levelscenes.
22 RelatedWorks
EmbodiedInstructionFollowing: TheEIFtaskrequirestherobottofollowhumaninstructions
represented by natural language in the interactive environment. Recent research can be divided
intotwocategories: end-to-endapproachesandmodularmethods. Fortheformer, agentsutilize
RGB-Dimagesequencestorepresentthesceneandgeneratelow-levelinteractiveactionsdirectly
throughimagesequencesandlanguageembedding. Singhetal. [30]mitigatedsemanticconfusion
byprovidingglobalandpixel-levelsemanticcuesforactionpredictionandobjectlocalization,and
Nguyenetal. [23]exploitedtherelationshipbetweeninstructionsandvisualclues. However,the
end-to-endapproachesrequirelarge-scaleimagesandannotatedlow-levelactiondata,resultingin
lowperformanceonunseenscenes. Themodularapproachescontainhigh-levelplanning,low-level
control and spatial memory construction modules with enhanced unreliability. Min et al. [19]
significantlyimprovedtaskexecutionsuccessviaemployingexplicitspatialmemoryandsemantic
searchstrategiesforagentguidance, andKimetal. [15]enhancedtheefficiencyoftheagentby
focusingontask-relevantobjectswiththeusageofcontextualplanningandmemoryframeworks.
However,existingEIFmethodsaredesignedforknownenvironmentswherevisualcluesofthewhole
scenecanbeeasilyacquiredbylookingaround,andfailtodiscoverrequiredvisualcluesinunknown
environmentsforfeasibleactiongeneration.
SceneRepresentationforVisual-languageNavigation: Visual-languagenavigationrequiresagents
toexploreunknownenvironmentstolocatetargetobjectsandfollownaturallanguageinstructions.
The primary challenge lies in efficiently representing expansive unknown scenes for generating
navigationpolicies. Existingscenerepresentationsconsistofthreecategories: 2Dsemanticmaps,
3D geometric maps and scene graphs. Early works [3, 2] constructed the 2D semantic maps by
projectingvisualcluesinthetop-downview,whichareleveragedfornavigationfrontierselectionfor
targetfinding. PONI[25]proposedascoringnetworkforallpotentialfrontiersofunseenregions
2Dsemanticmaps,andL3MVN[35]determinedthesemanticrelevanceoftheobjectsaroundeach
frontiertothetargetbyBERT[6]. Toembedthegeometricinformation,3Dgeometricmapsare
investigatedbyfusingthestructureandsemanticinformation. LERF[14]andConceptFusion[13]
integratedfine-grainedalignmentofsemanticfeatureswith3DmapsinSLAM,multi-viewfusion,
andNeRF[18]formultipledownstreamtasks. Toreducethestorageoverhead,scenegraphs[9,11]
areproposedtorepresentobjectsorconceptsasnodesandspatialrelationsasedgestorepresent
the scene topology efficiently. SayPlan [26] enabled agents to focus on task-relevant nodes by
integratingsubgraphfoldingandreplanningmechanisms. Inspiredbytheaboveapproaches, we
constructsemanticfeaturemapstoempowerembodiedagentstoexploreunknownenvironments,
wheretask-relevantinformationcanbeacquiredforactiongenerationwithlowexplorationcost.
3 ProblemStatement
Given the human instruction I in natural language, the robot should generate a sequence of ac-
tionprimitivesincluding(PickUp, Place, Open, Close, ToggleOn, ToggleOff, Slice)to
completetheinstruction. Theagentcanonlyacquirethesceneinformationforinstructionfollowing
via an RGB-D camera mounted on the agent, through which the agents build a semantic map S
togeneratethefeasibleinteraction. Inrealisticdeployment, theembodiedagentusuallyworkin
unknown environments, where the location and existence of objects in the house-level scene are
notknown. Therefore,weaddanadditionalactionprimitive(Navigation)toenabletheagentto
explorethesceneforvisualinformationcollection.
Theagentconsistsofahigh-levelplannerthatreasonsstep-by-stepplansP ={p }T fromhuman
i i=1
instructionsandalow-levelcontrollerthatpredictsthespecificactionsA={ai}τi foreachstep
j j=1
forscenenavigationorobjectinteraction. T meansthenumberofstepstoachievethehumangoal,
andτ isthenumberofspecialactionstoachievethei stepinthehigh-levelplan. Thehigh-level
i th
plannerisrepresentedbynaturallanguage(e.g. Step2. Heatthepotato)giventhehumaninstruction
(e.g. Canyoumakebreakfastforme?),andthelow-levelcontrollertransfersthestep-by-stepplans
intoexecutableactionswithactionprimitives,locationandtargetobjects(e.g. Place,potato,(10,8)
orNavigate,frontier,(2,3)). Finally,theagentonlymanipulatestheexistingrelevantobjectsto
achievehumangoals.
3Organize small items into the Box for tidying up the room
Observed Scene
Perception Module Scene Representation
N Ina tAv eci rg t aia o ct tni io on
n
noisuF
tpecnoC
noitcejorP dirG
lexoV
gnilooPxaM 1024
Primitive process
RGB-D Observation Semantic Feature Map Frontiers Mask
Weight
High-Level Planning Task relevant image Low-Level Controller
Current Action: descriptions LongCLIP
Organize small items into Step 2. Move to the Pencil Interaction
the Bo D Sx
t
o efo n pr e
1
t Ai .d c My tii oon vng
e
:r o too m
Box
A V aB L7 L- P S S … Pt tl la ae e …n np pn
n
3 4i in n. . g P G
g
i oA Tc k ac t rt ou gi o p t en h tts :eh: e B oP xencil A B ….. …[[ FF 33 rr o58o n21n t,t ,
i
i ee 43 rr 4 6 FC 25 eh a]]o tuic re es
s
gniddebm E VT ise ux at lT To ok ke en
n
A V aB L7 L-
The
P aO Ei nPc xp sk u pe wU t ln oO ep O r rbO a b j itb e sj ie j c o Be c t nc .tt
User
…… [Box, Pencil, RemoteControl] PLM I wil tl hG eo Pto eL no cc ila .tion
Instruction Step-by-step Planning Executable Actions
Observed Scene
Scene Representation
coI nn te ae id
n
et
r
o tf oin pd
a
a
c k B
the objects
n o A
isu
F
[4.25, 4.5] C
Exploration
Action: Navigation update Voxel Feature
Location: [4.25, 4.5] Observation Semantic Feature Map Frontiers Mask
High-Level Planning Texture Feature Semantic Embedding Low-Level Controller
Semantic Embedding
Organize small items for A Visual Token Interaction
tidying room LLaVA-7B B C B 7 LocaA tic ot nio : n [4: .O 2p 5e , n 5.25]
A- Target: Box
InstU ruse cr tionD Sto en pe 1 A . T c T et r xi yo t n t Po: r of mind p tthe box C S P S …t tlu ae e …r np pr ne 2 3n in. .t O g P A ip A cc ke ct i n t uo i o pn th n : te s h: eb o px encil pp ee y tt b Ss --gn in n alP A B C...
F
[[[ r633
oT 258e n121x
t,,,
it eu
343
r7r
46
e
D325 ]
e]]F se ca rt iu pr te
ion
Text Token
V
a L L EL xoA ecc a cti tE uo ix o tnp n a:l : bNo [r l3a ea .vt 2 i Ai g 5o a c,n t 2 ti i.o o2n 5 n]
s
Observed Scene
Scene Representation
coI nn te ae id
n
et
r
o tf oin pd
a
a
c k B
the objects
n o A
isu
F
[4.25, 4.5] C
Exploration
Action: Navigation update Voxel Feature
Location: [4.25, 4.5] Observation Semantic Feature Map Frontiers Mask
High-Level Planning Texture Feature Semantic Embedding Low-Level Controller
Semantic Embedding
Organize small items for A Visual Token A I cn tt ioe nra : c Oti po en n
tidying room LLaVA-7B B B Location: [4.25, 5.25]
C 7 Target: Box
A-
InstU ruse cr tionD Sto en pe 1 A . c Tt r Pi yo rT on t eo m: x tf pi n td the box C S P S …t tlu ae e …r np pr ne 2 3n in. .t O g P A ip A cc ke ct i n t uo i o pn th n : te s h: eb o px encil pp ee y tt b Ss --gn in n alP FroTe nA B Cx t...
i
t e[[[u 633 rr 258e D121 eF ,,, se 343 ca 746 rt i325u p]]]r te
ion
Text Token V a L L EL xoA ecc a ct tiE uo ix o tnp n a:l : bNo [r l3a ea .vt 2 i Ai g 5o a c,n t 2 ti i.o o2n 5 n]
s
Figure2:Overviewofourapproach. Thescenefeaturemapisconstructedbasedonreal-timeRGB-D
images,whichisleveragedasvisualcluesforthehigh-levelplannerandthelow-levelcontroller.
Theplannergeneratesthestep-wiseplans,whichareleveragedtopredictthespecificactionsinthe
controller. Theoptimalborderbetweenunknownandknownregionsisselectedforsceneexploration,
andthescenefeaturemapisupdatedwiththevisualcluesseeninduringtheexploration.
4 Approach
In this section, we first introduce the overall pipeline of our EIF method designed for unknown
environments,andthenwedescribethedetailsofthehigh-levelplannerandthelow-levelcontroller.
Moreover,weelaboratetheconstructionoftheonlinesemanticfeaturemapsthatgroundtheplanner
andthecontrollertothephysicalscene. Finally,wedemonstratethemodeltrainingandtheinference
ofourframeworkinpracticaldeployment.
4.1 OverallPipeline
Inrealisticdeploymentscenariosofhouseholdrobots,thephysicalworldisusuallyunknownfor
theagentbecausetheexistenceandlocationsfrequentlychangeduetohumanactivity. Therefore,
theagentsarerequiredtoconstructtheonlinescenefeaturemapaccordingtothereal-timevisual
perceptionduringtherobotnavigation,throughwhichtheagentgeneratesfeasiblestep-by-stepplans
toachievethehumangoalandtheefficientexplorationtrajectoriesfortheunknownsceneincluding
navigationandobjectinteractiontocompleteeachstepintheplan. Figure2demonstratestheoverall
pipelineofouragent. Thescenefeaturemaprepresentsthevisualcluesofthesceneinthetop-down
viewbasedonthecollectedRGB-Dimagesduringexploration,wherethepre-trainedfeaturesof
regionswithhigherrelevancetotheinstructionareassignedwithhigherimportanceforfeaturemap
construction. Thehigh-levelplannergeneratestheplansforthenextstepwithnaturallanguagebased
onthetaskcompletionprocessandthesemanticfeaturemap,andthelow-levelcontrollerpredicts
thetemplatedactionprimitives,locationandtargetobjectsforexecutablenavigationormanipulation
basedonthescenefeaturemapandtheplanforthenextstep.
4.2 HierarchicalEmbodiedAgentsforEIFinUnknownEnvironments
WedecomposeEIFinunknownenvironmentsintotwosub-tasksincludingthehigh-levelplanning
andthelow-levelexploration. Thegeneratedhigh-levelplansareleveragedasguidancefortheagent
toselectthemostrelevantregionsforexploration,andthepredictedlow-levelactionsupdatethe
semanticfeaturemapstoprovidevisualcluesforfeasibleplangeneration. Boththeplannerandthe
explorerareimplementedbyafinetunedLLaVAmodel.
High-levelplanner: Theplannergeneratestheplanforthenextstepinnaturallanguage,which
considersthetextualinformationincludingthehumaninstructionandthecompletedstepsandthe
visualcluesrepresentedbythesemanticfeaturemaps. Theforwardpassofthehigh-levelplanner
4HP canberepresentedasfollows:
p =HP(I,{p }i−1;S ) (1)
i k k=1 i−1
whereS meansthesemanticfeaturemapsupdatedinthei stepandweleverageaLLaVAmodel
i th
whosevisualencoderistheViT-L/14architectureforthehigh-levelplanner.
Low-levelcontroller: Thelow-levelcontrollerpredictsthespecificactionsincludingtheaction
primitives,locations,andtargetobjectsaccordingtothegeneratedhigh-levelplansandthesemantic
featuremaps,whichexplorestheunknownsceneandcompletesthestep-wiseplan. Theforwardpass
ofthelow-levelcontrollerLC canberepresentedasfollows:
{ai,li,oi}=LC(p ,{fi } ;{si } ) (2)
j j j i m m m m
whereli andoi arethepredictedlocationandtargetobjectsforthej actionsinthei stepofthe
j j th th
high-levelplan.Meanwhile,fi meansthetextualfeaturesofthem segmentofthefrontierbetween
m th
known and unknown regions for S , where m represents the number of frontier segments in the
i
entireS . Thetextualfeaturesaredemonstratedbythecoordinateofthemiddlepointforthefrontier
i
segment. si denotesthesemanticfeaturesofthem frontiersegments,whichisdemonstratedby
m th
thesemanticfeaturemappatchescontainingthecorrespondingfrontiers. Thelow-levelcontroller
notonlyexplorestheunknownscenewithnavigationandobjectinteractionbutalsocompletesthe
step-wiseplansbymanipulatingthetargetobject(e.g. pickupthetomato). Foractionprimitives
exceptfornavigate,thepredictedactionsareimplementedonthetargetobjects. Fornavigate,
therobotjustmovestothepredictedlocationswithoutobjectinteraction.
4.3 OnlineSemanticFeatureMaps
The high-level planner and the low-level controller should be aligned so that they can generate
feasibleplansandexploratoryactionstoachievehumaninstructionsintheunknownenvironment.
Thesemanticfeaturemapscanbeleveragedforalignmentsincetheyprovidevisualcluesofthe
sceneforboththehigh-levelplannerandthelow-levelcontroller. Inrealisticdeploymentscenarios
ofhouseholdrobots,theexistenceandlocationsfrequentlychangeduetohumanactivity. Therefore,
weproposeanonlinesemanticfeaturemapthatisdynamicallyupdatedduringtheexplorationofthe
unknownsceneforeachhumaninstruction.
Semanticfeaturemapsmeanthevisualrepresentationforeachpixelinthetop-downviewofthe
entirehouse. Comparedwithconventionalsemanticmaps,oursemanticfeaturemapscanrepresent
implicitrelationshipsbetweenobjectsinthesceneinsteadofonlycategories,whichprovidescrucial
informationforeffectiveexplorationpolicygeneration. ForEIFinunknownenvironments,thevisual
informationcollectedinthei timestepcontainstheRGBimageC andthedepthimageD . To
th i i
enablethesemanticfeaturemapstoacquirehighgeneralizationabilityindiversehumaninstructions,
weleverageCLIPtoextractthepixel-wisevisualfeaturesfi attimeiforthepixelinx rowand
xy th
y columnofC viafollowingConceptFusion[13],andfusethefeatureoftheentireimageandthat
th i
oftheinstancemaskforthepixelasthepixel-wisevisualfeaturesfi inthesameway. Thevisual
xy
featurescontributetotheprojectedlocationinthescenefeaturemapinthetop-downview,whichcan
bedepictedasfollows:
(cid:88)
Fi = fi ·I(P (x,y)∈S(u,v)) (3)
uv xy Di
x,y
whereFi meansthecontributiontotheelementintheu rowandv columnofthesemantic
uv th th
featuremapfromthevisualinformationcollectedintimei,andP (x,y)demonstratestheprojected
Di
coordinatesinthetop-downviewforofthepixel(x,y)basedonthedepthimageD . S(u,v)means
i
thepixelintheu rowandv columninthesemanticfeaturemap,andTheindicatorfunctionI(·)
th th
equalsonefortrueandzerootherwise.
Thesemanticfeaturemapisupdatedateachtimestepduringtheexplorationprocess,wheretheagent
seesnewvisualinformationforrecording. Sincethehouseforembodiedinstructionfollowingin
realisticworldisusuallyverylarge,regardingallimageswithequalimportanceinsemanticfeature
map construction leads to significant information redundancy. Meanwhile, different visual clues
usuallymakevariouscontributiontothegivenhumaninstruction. Therefore,weshouldassignlarge
importancetorelevantvisualclueswhenupdatingthesemanticfeaturemaps,sothatsufficientvisual
informationcanberepresentedwithoutredundancyforhigh-levelplanningandlow-levelexploration.
5Thetaskrelevancecanbeacquiredasfollows. Thehigh-levelplannerisalsorequiredtogenerate
thedemandedobjects{O } forthepredictedcorrespondingstep-wiseplan,whichareleveraged
k k
toconstructthreepromptsincluding(a)theimagecontains{O } ,(b)theimagedoesnotcontain
k k
{O } and(c)theimagecontainsnothing. Wethenleverageapre-trainedLongCLIP[36]topredict
k k
thesimilarityscorebetweentheimageandallprompts. Finally,theonlinesemanticfeaturemapis
updatedwithdynamicregionattention:
Si =(1−w )Si−1+w Fi (4)
uv i uv i uv
whereSi meansthefeaturesinthei rowandj columnofthesemanticfeaturemapsattime
uv th th
i. Thenormalizedweightw
=wˆ/1(cid:80)i
wˆ representstheimportanceofthecurrentsemantic
i i i k=1 k
featurescomparedwithknownvisualclues,wherew istheoriginalsimilarityscorebetweenthe
k
imageandthepromptinthek timestep. Theonlinesemanticfeaturemapscontainrichvisual
th
information,andthemostrelevantregionscanbeexploredvianavigatingtheoptimalborderand
interactingwithrelatedobjectstoachievehumangoalswithminimizedactioncost.
4.4 TrainingandInference
Training: The training samples for the high-level planner consist of human instruction, current
completedplans,currentsemanticfeaturemapsandthegroundtruthplanforthenextstep,andthose
forthelow-levelcontrollerincludeplanforthenextstep,textualandsemanticfeaturesforcurrent
bordersegmentsandthegroundtruthactionsequencesrepresentingprimitives,locationandtargets.
ThedetailsofinputandoutputareprovidedinAppendixE.WeleverageGPT-4andtheProcTHOR
simulatortogeneratethelarge-scaledatasettotraintheLLaVA-basedhigh-levelplanner.Weannotate
severalseedinstructionsandleverageGPT-4togeneratemoreinstructionsandcorrespondingplans
basedontheobjectlistforeachsceneintheProcTHOR,wheresampleswithlogicalerrorsarefiltered
withPDDLparameters[28]. WethenimplementthegeneratedplansinProcTHORandcollectthe
navigationtrajectories,RGB-Dimages,objectlocationsandrobotposesasthetrainingdata. Finally,
thegeneratedsamplesareparsedintohigh-levelplanningsamplesandlow-levelactiondata. We
follow the supervised fine-tuning paradigm in LLM for training the LLaVA model in high-level
plannerandlow-levelcontroller,wherewemaskoutp and{ai}τi inthei step. TheCEloss
i j j=1 th
leveragedinthetrainingprocessisrepresentedby:
M
(cid:104) (cid:88) (cid:105)
L =−E logp (R |R ,X ,X ) (5)
(XT,R)∼D θ m <m V T
m=1
whereX denotesscenefeaturemapsandX meansinputtextprompttokens. R represents
V T <m
theoutputtexttokensbeforethem tokenR andM arenumberofoutputtokens. Inthisway,the
th m
pre-trainedmultimodalLLMscanbegroundedtohigh-levelplanningandlow-levelcontroltasksin
realisticscenes,whereexecutableplansandactionsaregeneratedbasedonthescenerepresentation.
Inference: The high-level planner generates the planning for the next step based on the current
RGB-Dimageandsceneinformationrepresentedbythesemanticmap,andthelow-levelcontroller
predictstheactionprimitives,targetobjectandinteractionpositionbasedonthegeneratedstep-wise
plan. Thesemanticfeaturemapsareupdatedwhenimplementingthelow-levelactionsequences.
Thehigh-levelplannerwillgeneratetheplansforthenextsteponlywhenthecurrentlow-levelaction
issuccessfullyachieved. ThedetailedprocessisillustratedinAppendixB.
5 Experiments
5.1 ImplementationDetails
Trainingconfigurations: WeemployedtheLLaVA-7BarchitecturewiththeVincua-1.3-7Bpre-
trainingweightsforthehigh-levelplannerandthelow-levelcontroller,whichisfinetunedwithour
generateddatabytheLoRAstrategy. Forthevisualencoder,wesampled32visualembeddingsfrom
eachfrontierinthesemanticfeaturemapsupto256tokensassceneinformationrepresentation. We
generated2kinstructionswiththreesubparts(1386target-specificshort,333target-specificlong
and332abstractinstructions)for2509scenesinProcTHOR,whichresultsin30kgroundtruthplans
fortrainingthehigh-levelplanner. WeimplementedtheplansinProcTHORwithA∗algorithmto
collecttheexperttrajectoryasthegroundtruthfortraininglow-levelcontroller. Target-specificshort
6Table 1: Comparison with different EIF methods across different instructions in the ProcTHOR
simulator,whereLLM-P∗representstheLLM-Pwithoutperformingre-planning.
Small-scale Large-scale
Method
SR PLWSR GC PLWGC Path SR PLWSR GC PLWGC Path
Target-specificShort
LLM-P∗ 27.86 23.49 41.50 35.35 25.27 17.16 11.70 33.25 22.87 65.75
LLM-P 28.36 23.62 42.33 35.57 27.47 18.63 12.64 35.21 24.63 63.47
FILM 5.97 5.97 11.17 11.17 16.55 0.49 0.49 4.84 4.84 33.68
Ours 45.77 40.75 57.88 51.14 23.29 45.09 34.41 58.21 43.13 59.11
Target-specificLong
LLM-P∗ 5.97 5.14 18.91 17.26 60.56 1.52 0.82 15.28 13.05 78.03
LLM-P 5.97 4.80 19.65 17.30 64.89 1.52 1.01 16.04 14.17 64.14
FILM 0.00 0.00 4.14 4.14 79.17 0.00 0.00 6.26 6.26 70.14
Ours 13.43 12.44 27.11 24.67 62.21 19.70 17.34 35.61 31.08 78.99
Abstract
LLM-P∗ 1.32 0.92 15.68 12.57 38.69 6.16 2.83 16.92 11.21 70.92
LLM-P 3.95 2.33 16.78 12.45 36.27 6.16 3.58 18.15 12.42 67.20
FILM 0.00 0.00 4.87 4.87 33.23 0.00 0.00 8.02 8.02 49.45
Ours 10.53 8.09 24.23 19.68 35.90 9.59 5.74 21.30 15.01 61.54
Table 2: Comparison with different methods on the ALFRED benchmark with the AI2THOR
simulator. Weconducttheexperimentswithdifferentnumbersofsamplesformodeladaptation.
Test Validation
Method ShotNum. Seen Unseen Seen Unseen
SR GC SR GC SR GC SR GC
HLSM 100 0.82 6.88 0.61 3.72 0.13 2.82 0.00 1.86
FILM 100 0.20 6.71 0.00 4.23 0.00 13.19 0.00 9.65
LLM-P∗ 100 13.05 20.58 11.58 18.47 11.82 23.54 11.10 22.44
LLM-P 100 15.33 24.57 13.41 22.89 13.53 28.28 12.92 25.35
Ours 0 16.11 28.51 14.32 27.83 13.17 27.54 13.40 31.39
andlonginstructionsmeanthosecontainingobjectstobeinteracted(e.g. Placetheegginthebowl)
fortaskachievements,whosenumberofstepplanisrespectivelylowerthan15andnot. Abstract
instructionsdonotcontaintheinteractedobjectsintheinstructions(e.g. Makeasimplelunchfor
me). Wealsogenerate201,67and152dataforeachsubpartasthetestset. Weutilized8NVIDIA
3090GPUstofinetunethehigh-levelplannerandthelow-levelcontrollerforanhourinthetraining
stage. MoredetailsareprovidedinAppendixA.
Metrics: FollowingtheALFREDbenchmark[28],we
usesuccessrates(SR),goalconditionsuccess(GC),path
lengthandtheirpath-length-weighted(PLW)counterparts
for evaluation. SR means the ratio of the cases where Knife
theagentcompletelyachievethehumaninstructions,and
GC measures the ratio of objects in the state of goal Tomato
achievements. PLWSRandPLWGCcalculateSRandGC
weighted by the expert trajectory planning step number
dividedbytheactualexecutionstepnumber,whichmea-
suresthetrade-offbetweenperformanceandefficiency.
Simulated environments: We perform extensive ex- Sink
periments in the ProcTHOR and AI2THOR simulators,
wherethestepsizeoftranslationandrotationfortheagent Figure3: Examplevisualizationofdy-
is0.25mand90◦ respectively. ProcTHORcontains10k namicregionattentionweights.
house-levelsceneswithobjectsfrom93categories,where
the agent receives 600×600 RGB-D images in the egocentric view. We divide the scenes into
small-scale([0,10])andlarge-scale([10,16])onesbasedonthesidelengthoftheroom. AI2THOR
has120room-levelsceneswith105categoriesofobjects,andwefollowtheALFREDsettingfora
7Table3: Effectivenessofourgeneratedplansandexplo-
rationactions.
Too close to targets GT NormalSpecific
Method
31 Plan. Exp. SR PLWSR GC PLWGC Path(m)
✓ ✓ 64.18 62.51 72.76 69.54 18.23
Object in closed receptacle
✓ - 49.75 47.20 60.07 56.38 21.64
91 25 Ours - ✓ 55.72 53.20 66.67 62.71 14.70
Total failure - - 45.77 40.75 57.88 51.14 23.29
Target not found
Navigation failure 23
Table4: Ablationstudyofdifferentscenefeaturemaps.
12 Perception error
NormalSpecific
18 Planning failure 10 Interaction error Method SR PLWSR GC PLWGC Path(m)
6
NoMap 41.29 35.03 54.25 46.54 27.59
Wrong targets or action
Action missing NoAttention 44.78 39.02 156.63 49.40 24.54
RandomAttention 44.27 38.24 56.72 47.96 25.90
Figure4: AllfailurecasesonProcTHOR Ours 45.77 40.75 57.88 51.14 23.29
simulator.
faircomparisonwithothermethods[19,31,4]. Theagentreceives300×300RGBegocentricviews
andemploysmodelsprovidedbyFILMfordepthestimation.
5.2 ComparisonwithBaselines
Table1demonstratestheresultsonProcTHORforLLM-Planner[31],FILM[19]andourmethod,
whereourapproachsignificantlyoutperformsthestate-of-the-art-methodLLM-Planner. Although
LLM-PlannerutilizestherichcommonsenseembeddedinLLMstogenerateplansfortheagent,
itfailstoalignthepre-trainedLLMwiththesceneinformation. Thegeneratedplansareusually
infeasibleduetothenon-existenceoftheobjectsforinteraction,andthere-planningmodulesuffers
from low success rate and low efficiency. On the contrary, our method construct the semantic
featuremapswhichgroundsthepre-trainedmultimodalLLMstotherealisticphysicalscene,andthe
unknownenvironmentcanbeefficientlyexploredbyunderstandingthevisualcluesforexecutable
plangeneration. Ourapproachremainsleadinginperformanceinmorechallengingabstracttask
settingsandloseslessthan1%ofperformanceonlarge-scalescenescomparedtosmallones. The
semanticfeaturemapallowsourapproachtoefficientlyfindthesuitabletargettosatisfytheinstruction
during active exploration. Meanwhile, the leading PLWSR and PLWGC metrics verify that our
low-levelcontrollercanfindthetargetobjectatalowernavigationcost. Moreover,thesuccessrate
ofconventionalmethodsinthelarge-scalescenesisnearzero,whileourapproachcanachieve9.59%
successrate. Sincetheservicerobotisusuallydeployedinhouse-levelscenes,ourmethodisproven
tobemorepractical.
In order to evaluate the generalization ability of our method, we compare our method with the
state-of-the-artEIFapproachesontheALFREDbenchmarkwithfewornosamplesforfinetuning
asillustratedinTable2,whereourmethodisevaluatedinzero-shotsettingswithoutfinetuningthe
pre-trainedagent. Despiteleveragingthecommonsenseinthepre-trainedmultimodalLLMs,our
methodprovidesrichvisualinformationofthescenetogroundthemultimodalLLMforfeasible
plan generation. As a result, we achieve higher success rate with shorter path length than LLM-
Planner,eventhoughwedonotleverageanysamplesonALFREDforfinetuning. Theperformance
improvementislessthanthatonProcTHORbecauseALFREDonlycontainsroom-levelscenes,and
simplylookingaroundcancollectinformationofmostobjectswithouttherequirementofsemantic
mapconstructionandsceneexploration.
WedemonstratethequantitativeresultsinFigure5,whereweshowthestep-wiseplan,theexploration
processandtherobotimplementationduringawholesequenceforEIF.Inthebeginning,theagentis
initializedinthebedroomareaandselectsthenavigationbordersoutsidetheroomforexploration,
astheinstruction’makingbreakfast’isirrelevanttobedrooms. Duringthenavigation, theagent
graduallyknowstoexplorethekitchenareabyobservingthediningtableandthecounter,andit
isevenawarethatopeningthefridgemayfindfoodforbreakfastduetotherichcommonsensein
ourfinetunedlow-levelcontroller. Asaresult,abstractinstructionisachievedbyservingdiverse
8frontier A
unknown
environment
Unknown
environment
Fetch an apple for breakfast
MMoovvee ttoo tthhee ffrriiddggee OOppeenn tthhee ffrriiddggee aanndd TTrryy ttoo ffiinndd ssiinnkk ffoorr PPuutt tthhee aappppllee iinn tthhee TTrryy ttoo ffiinndd kknniiffee ffoorr PPiicckk uupp kknniiffee ffoorr
ttoo ffiinndd aann aappppllee ppiicckk uupp tthhee aappppllee cclleeaanniinngg aappppllee ssiinnkk ttoo cclleeaann iitt sslliicciinngg aappppllee sslliicciinngg aappppllee
NNaavviiggaattee OOppeenn PPllaaccee PPiicckkUUpp
NNaavviiggaattee NNaavviiggaattee
ffrriiddggee FFrriiddggee SSiinnkk KKnniiffee
[[1133..2255,, 99..7755]] [[1111..2255,, 1144..7755]]
[[1100..2255,, 1122..5500]] [[1100..2255,, 1122..5500]] [[1133..2255,, 1100..2255]] [[1122..2255,, 1144..7755]]
Figure5: AnexampleofEIFinunknownenvironments. Theagentonlynavigatesthetask-related
regionsforvisualcluecollectionwithhighefficiency,andgeneratesfeasibleplanstocompletethe
abstractinstructions.
foodforbreakfast,whereonlyrelatedregionsarenavigatedwithhighexplorationefficiencyinthe
unknownenvironment. Figure4illustratesthestatisticsoffailurecasescausedbydifferentreasons.
Thefailuremostlycomesfromunsuccessfulnavigationbecauseofthelargehouse-levelscene,and
thetopreasonsincluding’tooclosetotargets’and’failtoseeclosedspace’indicatethatnavigation
algorithmsshouldbedesignedwithhighcompatibilityofthesubsequentmanipulation.
5.3 AblationStudies
Effectivenessofthehigh-levelplannerandthelow-levelcontroller: Weevaluatedthevariants
ofourmethodwheretheplannerandthecontrollerarerespectivelyreplacedwiththegroundtruth
step-wise plans and groundtruth action sequences. Table 3 demonstrates the results where the
performanceofourmethodsisclosetothatofthegroundtruth,whichindicatestheeffectivenessof
ourLLaVA-basedplannerandcontroller. Moreover,theperformanceofplannersmainlyinfluences
thesuccessratebecauseitisimportantfortheagenttointeractwiththerightobjectsinthecorrect
orders,whilethecontrollersignificantlyimpactsthepathlengthsincedirectlyexploringtherelated
regionsenablestheagenttoaccomplishtheinstructionfaster.
Effectiveness of the online semantic feature map: The semantic feature map provides visual
information of explored regions for the planner and the controller to generate feasible plans and
efficientactions,andwereporttheperformanceofdifferentsemanticmapstovalidatetheeffectiveness
of our method. Table 4 demonstrates the results for the settings of no semantic maps, semantic
maps with only category information, semantic feature maps without dynamic attention and our
semanticfeaturemaps. Theresultsdemonstratethattheimplicitrichsemanticfeaturesarenecessary
foreffectiveexplorationofunknownenvironments, andthedynamicattentionalsoenhancesthe
performanceofthesemanticfeaturemapasitremovestheinformationredundancyforthelarge
house-levelscenes.Wealsovisualizethedynamicregionattentionwhentheagentbuildsthesemantic
feature map in the unknown environment as illustrated in Figure 3. For the instruction Slice the
tomatoforsalad,thefeaturesofthekitchenareaespeciallythetomatoandthesinkareconsidered
withhighattention(Thegreencolorrepresentsgreaterweight), whichindicatesthatthedynamic
regionattentionlearnsrelevantvisualcluesforfeasibleactiongeneration.
6 Conclusion
Inthispaper, wehaveproposedanEIFapproachforunknownenvironments, wheretheagentis
requiredtoexploretheenvironmentefficientlytogeneratefeasibleactionplanswithexistingobjectsto
achievehumaninstructions.WefirstbuildahierarchicalEIFframeworkincludingahigh-levelplanner
andalow-levelcontroller,andthenbuildasemanticfeaturemapwithdynamicregionattentionto
9providevisualinformationfortheplannerandthecontroller. Extensiveexperimentsdemonstratethe
effectivenessandefficiencyofourframeworkinthehouse-levelunknownenvironment.
Limitations: Thisworklacksrealmanipulationimplementationandthedesignednavigationpolicy
ignores the compatibility with manipulation. We will design mobile manipulation strategies for
generaltasksandimplementtheclosed-loopsystemonrealrobotsinthefuture.
References
[1] MichaelAhn,AnthonyBrohan,NoahBrown,YevgenChebotar,OmarCortes,ByronDavid,ChelseaFinn,
ChuyuanFu,KeerthanaGopalakrishnan,KarolHausman,etal. Doasican,notasisay: Grounding
languageinroboticaffordances. arXivpreprintarXiv:2204.01691,2022.
[2] PeterAnderson,QiWu,DamienTeney,JakeBruce,MarkJohnson,NikoSünderhauf,IanReid,Stephen
Gould, andAntonVanDenHengel. Vision-and-languagenavigation: Interpretingvisually-grounded
navigationinstructionsinrealenvironments. InProceedingsoftheIEEEconferenceoncomputervision
andpatternrecognition,pages3674–3683,2018.
[3] DhruvBatra,AaronGokaslan,AniruddhaKembhavi,OleksandrMaksymets,RoozbehMottaghi,Manolis
Savva,AlexanderToshev,andErikWijmans. Objectnavrevisited: Onevaluationofembodiedagents
navigatingtoobjects. arXivpreprintarXiv:2006.13171,2020.
[4] ValtsBlukis,ChrisPaxton,DieterFox,AnimeshGarg,andYoavArtzi. Apersistentspatialsemantic
representationforhigh-levelnaturallanguageinstructionexecution. InCoRL,pages706–717.PMLR,
2022.
[5] AnthonyBrohan,NoahBrown,JusticeCarbajal,YevgenChebotar,JosephDabis,ChelseaFinn,Keerthana
Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for
real-worldcontrolatscale. arXivpreprintarXiv:2212.06817,2022.
[6] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert:Pre-trainingofdeepbidirec-
tionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
[7] MingyuDing,YanXu,ZhenfangChen,DavidDanielCox,PingLuo,JoshuaBTenenbaum,andChuang
Gan. Embodiedconceptlearner:Self-supervisedlearningofconceptsandmappingthroughinstruction
following. InCoRL,pages1743–1754.PMLR,2023.
[8] DanielGordon,AniruddhaKembhavi,MohammadRastegari,JosephRedmon,DieterFox,andAliFarhadi.
Iqa: Visualquestionansweringininteractiveenvironments. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pages4089–4098,2018.
[9] QiaoGu,AlihuseinKuwajerwala,SachaMorin,KrishnaMurthyJatavallabhula,BipashaSen,Aditya
Agarwal, Corban Rivera, William Paul, Kirsty Ellis, Rama Chellappa, et al. Conceptgraphs: Open-
vocabulary3dscenegraphsforperceptionandplanning. arXivpreprintarXiv:2309.16650,2023.
[10] WenlongHuang,ChenWang,RuohanZhang,YunzhuLi,JiajunWu,andLiFei-Fei.Voxposer:Composable
3dvaluemapsforroboticmanipulationwithlanguagemodels. InCoRL,pages540–562.PMLR,2023.
[11] NathanHughes,YunChang,andLucaCarlone. Hydra:Areal-timespatialperceptionsystemfor3dscene
graphconstructionandoptimization. arXivpreprintarXiv:2201.13360,2022.
[12] YukiInoueandHirokiOhashi. Prompter:Utilizinglargelanguagemodelpromptingforadataefficient
embodiedinstructionfollowing. arXivpreprintarXiv:2211.03267,2022.
[13] KrishnaMurthyJatavallabhula,AlihuseinKuwajerwala,QiaoGu,MohdOmama,TaoChen,AlaaMaalouf,
ShuangLi,GaneshIyer,SoroushSaryazdi,NikhilKeetha,etal. Conceptfusion:Open-setmultimodal3d
mapping. arXivpreprintarXiv:2302.07241,2023.
[14] JustinKerr,ChungMinKim,KenGoldberg,AngjooKanazawa,andMatthewTancik. Lerf:Language
embeddedradiancefields. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pages19729–19739,2023.
[15] Byeonghwi Kim, Jinyeon Kim, Yuyeong Kim, Cheolhong Min, and Jonghyun Choi. Context-aware
planningandenvironment-awarememoryforinstructionfollowingembodiedagents. InICCV,pages
10936–10946,2023.
[16] XiaotianLiu, HectorPalacios, andChristianMuise. Aplanningbasedneural-symbolicapproachfor
embodiedinstructionfollowing. Interactions,9(8):17,2022.
[17] GuanxingLu,ZiweiWang,ChangliuLiu,JiwenLu,andYansongTang. Thinkbot:Embodiedinstruction
followingwiththoughtchainreasoning. arXivpreprintarXiv:2312.07062,2023.
[18] BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoorthi,andRen
Ng. Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis. CommunicationsoftheACM,
65(1):99–106,2021.
[19] SoYeonMin,DevendraSinghChaplot,PradeepKumarRavikumar,YonatanBisk,andRuslanSalakhutdi-
nov. Film:Followinginstructionsinlanguagewithmodularmethods. InICLR,2021.
[20] DipendraMisra,JohnLangford,andYoavArtzi. Mappinginstructionsandvisualobservationstoactions
withreinforcementlearning. arXivpreprintarXiv:1704.08795,2017.
10[21] YaoMu,QinglongZhang,MengkangHu,WenhaiWang,MingyuDing,JunJin,BinWang,JifengDai,Yu
Qiao,andPingLuo. Embodiedgpt:Vision-languagepre-trainingviaembodiedchainofthought. NIPS,36,
2024.
[22] MichaelMurrayandMayaCakmak. Followingnaturallanguageinstructionsforhouseholdtaskswith
landmarkguidedsearchandreinforcedposeadjustment. RAL,7(3):6870–6877,2022.
[23] Van-QuangNguyen,MasanoriSuganuma,andTakayukiOkatani.Lookwideandinterprettwice:Improving
performanceoninteractiveinstruction-followingtasks. arXivpreprintarXiv:2106.00596,2021.
[24] AlexanderPashevich,CordeliaSchmid,andChenSun. Episodictransformerforvision-and-language
navigation. InICCV,pages15942–15952,2021.
[25] SanthoshKumarRamakrishnan,DevendraSinghChaplot,ZiadAl-Halah,JitendraMalik,andKristen
Grauman.Poni:Potentialfunctionsforobjectgoalnavigationwithinteraction-freelearning.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages18890–18900,2022.
[26] KrishanRana,JesseHaviland,SouravGarg,JadAbou-Chakra,IanReid,andNikoSuenderhauf. Sayplan:
Groundinglargelanguagemodelsusing3dscenegraphsforscalablerobottaskplanning. InCoRL,2023.
[27] DhruvShah,Błaz˙ejOsin´ski,SergeyLevine,etal. Lm-nav: Roboticnavigationwithlargepre-trained
modelsoflanguage,vision,andaction. InConferenceonrobotlearning,pages492–504.PMLR,2023.
[28] MohitShridhar,JesseThomason,DanielGordon,YonatanBisk,WinsonHan,RoozbehMottaghi,Luke
Zettlemoyer,andDieterFox. Alfred:Abenchmarkforinterpretinggroundedinstructionsforeveryday
tasks. InCVPR,pages10740–10749,2020.
[29] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew
Hausknecht. Alfworld:Aligningtextandembodiedenvironmentsforinteractivelearning. arXivpreprint
arXiv:2010.03768,2020.
[30] KunalPratapSingh,SuvaanshBhambri,ByeonghwiKim,RoozbehMottaghi,andJonghyunChoi. Factor-
izingperceptionandpolicyforinteractiveinstructionfollowing. InICCV,pages1888–1897,2021.
[31] ChanHeeSong,JiamanWu,ClaytonWashington,BrianMSadler,Wei-LunChao,andYuSu.Llm-planner:
Few-shotgroundedplanningforembodiedagentswithlargelanguagemodels. InICCV,pages2998–3009,
2023.
[32] TOVan-QuangNguyen. Ahierarchicalattentionmodelforactionlearningfromrealisticenvironments
anddirectives. InEuropeanConferenceonComputerVision(ECCV)EVALWorkshop,2020.
[33] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel Khashabi, and
HannanehHajishirzi. Self-instruct: Aligninglanguagemodelswithself-generatedinstructions. arXiv
preprintarXiv:2212.10560,2022.
[34] ZhenyuWu,ZiweiWang,XiuweiXu,JiwenLu,andHaibinYan. Embodiedtaskplanningwithlarge
languagemodels. arXivpreprintarXiv:2307.01848,2023.
[35] BangguoYu,HamidrezaKasaei,andMingCao. L3mvn: Leveraginglargelanguagemodelsforvisual
targetnavigation. In2023IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS),
pages3554–3560.IEEE,2023.
[36] BeichenZhang, PanZhang, XiaoyiDong, YuhangZang, andJiaqiWang. Long-clip: Unlockingthe
long-textcapabilityofclip. arXivpreprintarXiv:2403.15378,2024.
[37] YichiZhangandJoyceChai.Hierarchicaltasklearningfromlanguageinstructionswithunifiedtransformers
andself-monitoring. arXivpreprintarXiv:2106.03427,2021.
[38] XingyiZhou,RohitGirdhar,ArmandJoulin,PhilippKrähenbühl,andIshanMisra. Detectingtwenty-
thousandclassesusingimage-levelsupervision. InEuropeanConferenceonComputerVision, pages
350–368.Springer,2022.
11A TrainDetails
High-levelplanningandlow-levelcontroller: Inthesupervisedinstructionfine-tuningstage,we
reducememoryusageviaDeepSpeedZeRO-2. Thelearningrateforthefeaturemappinglayerand
theLLMbackbonenetworkissetto2×10−5,andthebatchsizeissetto8. Fine-tuningisperformed
foronlyoneepoch. SincethesemanticfeaturemapshavebeenconstructedthroughCLIP,thescene
visualtokensaredirectlyfedintothemappinglayerwithoutthevisualcoderduringthetraining
stage.
Visualperception: Weselected100kimagesfromthecapturedexperttrajectoriesasthetrainingset
forinstancesegmentationmodelDetic[38],fine-tuningthepre-trainedmodelwiththelearningrate
of1×10−4andperforming180kiterations. Thebatchsizeissetto16andtheAdamoptimiseris
applied.
B InferenceDetails
Overview: Atthei time,theagentsurroundstoperceivethesceneinformationandconstructs
th
semanticfeaturemapS andfrontiersmask. Then,theagentwillgeneratethei high-levelplanning
i th
p basedonthesceneinformation,userinstructionI andfinishedstep-by-stepplanning{p }i−1.
i k k=1
Thelow-levelcontrollergeneratesspecificinteractionactionsai,targetobjectsoi andpositionsli
j j j
basedonp ,semanticfeature{si }andtextualfeaturefi : 1)Iftheoi isobservedbytheagent,li
i m m j j
willbethelocationrecordedonthemap;2)Ifnotbeobserved,li willbethefrontierposition. During
j
theinferenceprocess,eachinstructionI performsupto30stepsofhigh-levelplanning.
Frontierrepresentation: Wefollow[35]
Algorithm1:InferenceProcess
togeneratefrontiermasksthatdistinguish
between known and unknown regions input :HumaninstructionI,highlevelplanner
basedontheoccupancymap. Throughcon- HP,low-levelcontrollerLP,scene
nectedcomponentanalysis,weobtainthe observationO,maximumnumberof
maskofeachfrontierinstance. Wefurther performingstepT.
remove frontiers with areas smaller than initialization: Randomloadintotheunknown
thethreshold(150pixels)toreduceredun- scene;
dancy exploration. We sample 32 visual fori←0toT do
embeddings as frontier tokens according ConstructingsemanticfeaturemapS ivia(3);
tothefrontierinstancemaskonthecorre- Generatestepplanningp ibasedonS ivia(1);
spondingregionofthefeaturemap,while ifendinp ithen
utilizingthecoordinatesoftheircentroids Break
forthe frontiertext description. Thespe- end
cificrepresentationisillustratedinFigure Computeattentionw iandupdateS iby(4);
6(a). Generateaction{ai,li,oi}τi via(2);
j j j j
forj ←0toτ do
RegionAttention: Sincetheimportance i
Executeai;
of the observed image for the completed j
instructionisdifferentforeachframe,itis end
desirabletoassignhigherweightstotask- end
relevantvisualembeddingstoenableLC
togeneratemoreefficientnavigationexplo-
rationplanning. HP generatestargetobjectsthatmightberequiredtointeracttocompleteinstruction
I whilegeneratingp andconvertsthemintoasentencedescribingL . However,measuringthe
i dec
relevanceofanimagetotheinstructionwithonlyasingledescriptionisnotdiscriminativeenough
to highlight task-relevant regions on the feature map. To this end, we add additional variants of
descriptionstocalibratetherelevanceofimagestotheircorrespondingdescriptionsforexploiting
thepriorknowledgeofthepre-trainedmodelsextensively. Specifically,wefurtherexpandL into
dec
L andL tomatchtheinputrequirementsofimageandtextalignmentmodelssuchasCLIP.
dec none
L andL describetheimageasnotcontainingthetargetobjectsandnotcontainingtheobjects,
dec none (cid:8) (cid:9)
respectively. WeadoptLongCLIP[36]toretrievethesimilaritybetween L ,L ,L and
dec dec none
RGBimagesasillustratedinFigure6(b),andconsiderthescoreofL astheattentionweight.
dec
12Frontier Representation Region Attention
Frontier A Frontier B Textual Feature Step planning target:[tomato,knife,sink] Attention
A. [163, 63] C. [316, 389] weight
B. [176, 233] D. [412, 205]
The image contains
0.618
Semantic Feature one of planning target
… The image does not P IL
Frontier C … … pc lo an nt na ii nn g o tn ae rg o ef t C g n 0.297
o
… L
Frontier D The image contains
32 × 1024 nothing 0.085
Frontiers Mask Frontiers Dict RGB Image Task Relevant Descriptions
(a) (b)
Figure6: Detailsoffrontierrepresentationandregionattentionweights.
C MoreResult
Influencew.r.t. navigationfrontierconstruction: Navigationfrontiermeanstheborderbetween
theknownandunknownregions,whicharerepresentedbymultiplesegments.
Weonlyselectthefrontiersthatarelongerthanathresholdasthecandidatesforagentnavigation,
becauseextremelyshortfrontiersusuallyindicatecornerregionsthatrevealsuninformativeinfor-
mation. Therefore,wecanenhancetheexplorationefficiencysignificantly. Table5illustratesthe
successrateandpathlengthfordifferentthresholds. Theresultsdemonstratethatlowthresholds
resultisredundantnavigationwithhighpathlength,whilehighthresholdsdegradethesuccessrate
becauseofimportantsceneinformation. Wesetthefrontierthresholdto150pixelstoachievehigher
performanceandnavigationcosttrade-off.
Qualitativeresults: WedemonstratemoreunknownenvironmentEIFexecutionsequencestoreflect
thesuperiorityofourapproach.
D Data
TrainingData: ExistingEIFdatasets Table5: Resultsregardingfrontierthresholds.
arestilllimitedininstructiondiversity
and scene scale. We design a dataset Thresholds SR Path(m)
synthesis framework to minimize the 70 45.77 36.50
generationcostandincreasethescaleof 100 46.27 30.41
EIF datasets, enabling agents to adapt 150 45.77 23.29
tolarge-scaleunknownscenesandcom- 200 43.28 19.32
plex tasks. Therefore, the dataset syn-
thesisframeworkconsistsoftwomainstages.ThefirststageistoemployGPT-4togenerateextensive
high-levelplanningwithcorrespondinglow-levelactionsbasedonpromptandsceneinformation,
thenfilterlogicalerrorsampleswithTextWorld[29]. Thesecondstageistoexecutetheinteractions
specifically with the oracle in the simulator, grounding the generated plans and actions into the
physicalsceneandcollectingexperttrajectories.
TextWorlddatageneration: Wecollectobjectlistscontainedineachsceneassceneinformation
inProcTHOR,consistingofthelocationandsizeofeachobject. GPT-4willgeneratetaskplans
basedontheobjectinformationandprompts. Specifically,weannotated22seedtasksmanuallyto
inspireGPT-4togenerateconfirmedresponses. Eachresponsecontainsinstructions,step-by-step
high-levelactions,andcorrespondinglow-levelactions. Wefurtheremployself-instruction[33]to
ensurethediversityofinstructions(Thesimilarityfilteringthresholdissetto0.9). Meanwhile,GPT-4
willgeneratePDDLparametersthatsatisfytheALFREDbenchmarkstoverifythefeasibilityofthe
planning. ThegeneratedcandidatesamplesaresenttoTextWorldandcheckwhetherthetaskcanbe
executedbasedonthePDDLparameterstoensurethequalityofthetrainingdataset.
Groundingthegeneratedplans: ThesyntheticdatasetthatpassesthePDDLcheckisfedinto
theProcTHORsimulatorforspecificinteractions. WecollectnavigationtrajectoriesinProcTHOR
basedontheplanninggeneratedinthefirststageunderoraclesettings,whichcontainRGBimages,
depthmaps,segmentationmasks,androbotposes. Accordingtothesemanticfeaturemapbuilding
13Nav path Frontiers
Instruction: Prepare a vegetable for a salad by slicing it
Agent position Target position
Move to a vegetable Move to a vegetable Move to a vegetable Move to a fridge Move to a fridge
B
G
R
c
ir
tn
e
c
o
g
E
p
a
M
Step
Open the fridge Pick up lettuce Put on the counter top Pick up the knife Slice the lettuce
B
G
R
c
ir
tn
e
c
o
g
E
p
a
M
Step
Figure7: Ourapproachactivesearchforlettuceinthefridgetocompletetheinstruction.
approachpresentedinSection4.3,weobtainreal-timesemanticfeaturemapsS ,frontiertextfeatures
i
{fi } ,andsemanticembedding{si } sequencesastheagentsperforminteractions. Basedonthe
m m m m
step-by-stepplanninggeneratedbyGPT-4,wesplittheabovesequencesintostep-by-stepinstruction-
followingsamples. AsforHP,wefeedinstructionI,sceneinformationS andcompletedsteps
i
{p }i−1 asprompts,expectingtogeneratethenextstepp tobedone. Meanwhile,inorderto
k k i+1
ensuretheconsistencyofthehigh-leveltaskplanning,werequireHP togivetheplanningofall
subsequentactionsasillustratedinFigure9(a). ForLC,wetakethecurrentp ,frontiertextfeatures
i
{fi } and semantic features {si } as prompt, and expect LC to output the action primitives
m m m m
executedbytheagentundertheoraclesetting. Specifically,iftheagentobservesthetargetobject,
LCgeneratesthespecifictargetandactionprimitivebasedontheinputp . Ifitdoesnotobserve,
i
werequireLC togeneratetheclosestfrontiertotheoraclepathasthenextexplorationregion. LC
trainingsamplesareasillustratedinFigure9(b).
E Prompt
Figure10brieflydemonstratesthepromptwordsemployedtoinspireGPT-4ingeneratingtheEIF
dataset,whichconsistsofthefollowingfourmainparts:
SystemPrompt: PrimarilydesignedtosetuptheGPT-4contextualenvironmentforgeneratingtask
planningbasedonavirtualrobot.
ActionPrimitive: ItisappliedtoconstrainthescopeoftheinteractionactiongeneratedbytheGPT
toensurethatitisexecutable.
14Nav path Frontiers
Instruction: Prepare a vegetable for breakfast
Agent position Target position
Move to a tomato Move to the fridge Open the fridge Move to a tomato Pick up tomato
B
G
R
c
ir
tn
e
c
o
g
E
p
a
M
Step
Move to faucet Put the tomato Turn on and clean Turn off faucet Pick up tomato
B
G
R
c
ir
tn
e
c
o
g
E
p
a
M
Step
Put to counter-top Move to the knife Pick up the knife Move to tomato Slice the tomato
B
G
R
c
ir
tn
e
c
o
g
E
p
a
M
Step
Figure 8: Our approach consistently completes complex instructions for preparing breakfast in
large-scaleunknownscenes.
ResponseFormat: Ensureconsistencyinresponseformattoparsespecificinstructions,planning
andactions.
PDDLParams: Recordtherequirementsforthecompletionoftheinstruction,includingthetarget
objectanditsstate. Aninteractionisonlysuccessfulifthestateofthetargetobjectinthescene
matchesthePDDLparameterrecord.
15Instruction: Step 12. Move to the location of the DiningTable with the ###System Prompt:
Take an Apple from the Fridge, clean it . cooked Potato and where to go? Human
A. [80, 100]
Value Design a conversation between you and the person you are
Object List: B. [60, 23]
serving in the room.
[Laptop, SprayBottle, Spoon, Plate, Fridge, Apple] Human
The answer should be the tone of the service robot located in the
Value The answer is A. Robot room and performing the action specifically. The generated
D Sto en pe 1 A . c Mti oo vn es : towards the Fridge to get the Apple. I will GotoLocation the DiningTable. Respone instructions can be described in different tones…….
Step 2. Open the Fridge to access the Apple. Explore Navigation
Step 3. Move to where the Apple is located inside the Fridge. ###Action Primitive:
Step 2. Pick up the Tomato and where to go?
Current Action: A. [114, 93] Human PickupObject(object)
Step 4. Pick up the Apple from the Fridge. B. [39, 30] Value Pick up an object. You can only hold one object at a time.
C. [120, 10] Augments:
Planning Action: D. [105, 57] - object: a string, the object to pick. Only the following objects can
Step 5. Move to the Fridge.
serve as the subject of this action: {PickupObject_list}
Step 6. Close the Fridge after taking the Apple out. Robot
Step 7. Head over to the Sink to wash the Apple. Respone The answer is C.
Step 8. Place the Apple in the Sink to wash it. I will PickupObject the Tomato so I need stay there.
###Response Format:
Step 9. Approach the Faucet to clean the Apple.
Robot
Step 10. Turn the Faucet on to wash the Apple.
Step 11. I finish the task Respone "instruction": "The task description in natural language",
"thought": "Your thoughts on the plan in natural language",
Planning Target: "action_list":[……]
[Apple, Fridge, DiningTable, ButterKnife, Faucet, Sink] Interactive Action
###PDDL_params:
(a) High-level planner sample (b) Low-level controller sample
object_target: The subject being executed for this task,
Figure9: Visualizationoftrainingsamplesforhigh-levelplannerandlow-levelcontroller. parent_target: This task requires placing the object-target into the
receptacle indicated by this parameter,
mrecep_target: Fill in the movable receptacles required for the task;
toggle_target: The 'toggle target' is the object that this task requires
Instruction: Step 12. Move to the location of the DiningTable with the ###System Prompt: to be opened
Take an Apple from the Fridge, clean it . cooked Potato and where to go? Human object_sliced: Should the object be sliced? This parameter can only
A. [80, 100]
Value Design a conversation between you and the person you are be True or False.
Object List: B. [60, 23]
serving in the room.
[Laptop, SprayBottle, Spoon, Plate, Fridge, Apple] Human
The answer should be the tone of the service robot located in the
Value The answer is A. Robot room and performing the action specifically. The generated
D Sto en pe 1 A . c Mti oo vn es : towards the Fridge to get the Apple. I will GotoLocation the DiningTable. Respone instructions can be described in different tones…….
Step 2. Open the Fridge to access the Apple. Explore Navigation
Step 3. Move to where the Apple is located inside the Fridge. ###Action Primitive:
Step 2. Pick up the Tomato and where to go?
Current Action: A. [114, 93] Human PickupObject(object)
Step 4. Pick up the Apple from the Fridge. B. [39, 30] Value Pick up an object. You can only hold one object at a time.
C. [120, 10] Augments:
Planning Action: D. [105, 57] - object: a string, the object to pick. Only the following objects can
Step 5. Move to the Fridge.
serve as the subject of this action: {PickupObject_list}
Step 6. Close the Fridge after taking the Apple out. Robot
Step 7. Head over to the Sink to wash the Apple. Respone The answer is C.
Step 8. Place the Apple in the Sink to wash it. I will PickupObject the Tomato so I need stay there.
###Response Format:
Step 9. Approach the Faucet to clean the Apple.
Robot
Step 10. Turn the Faucet on to wash the Apple.
Step 11. I finish the task Respone "instruction": "The task description in natural language",
"thought": "Your thoughts on the plan in natural language",
Planning Target: "action_list":[……]
[Apple, Fridge, DiningTable, ButterKnife, Faucet, Sink] Interactive Action
###PDDL_params:
(a) High-level planner sample (b) Low-level controller sample
object_target: The subject being executed for this task,
parent_target: This task requires placing the object-target into the
receptacle indicated by this parameter,
mrecep_target: Fill in the movable receptacles required for the task;
toggle_target: The 'toggle target' is the object that this task requires
to be opened
object_sliced: Should the object be sliced? This parameter can only
be True or False.
Figure10: PromptwordsforGPT-4syntheticEIFdataset.
16