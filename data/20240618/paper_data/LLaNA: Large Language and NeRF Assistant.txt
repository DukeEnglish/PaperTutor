LLaNA: Large Language and NeRF Assistant
AndreaAmaduzzi PierluigiZamaRamirez
UniversityofBologna UniversityofBologna
andrea.amaduzzi4@unibo.it pierluigi.zama@unibo.it
GiuseppeLisanti SamueleSalti
UniversityofBologna UniversityofBologna
giuseppe.lisanti@unibo.it samuele.salti@unibo.it
LuigiDiStefano
UniversityofBologna
luigi.distefano@unibo.it
https://andreamaduzzi.github.io/llana/
Abstract
MultimodalLargeLanguageModels(MLLMs)havedemonstratedanexcellent
understandingofimagesand3Ddata.However,bothmodalitieshaveshortcomings
in holistically capturing the appearance and geometry of objects. Meanwhile,
NeuralRadianceFields(NeRFs),whichencodeinformationwithintheweights
of a simple Multi-Layer Perceptron (MLP), have emerged as an increasingly
widespreadmodalitythatsimultaneouslyencodesthegeometryandphotorealistic
appearanceofobjects. Thispaperinvestigatesthefeasibilityandeffectivenessof
ingestingNeRFintoMLLM.WecreateLLaNA,thefirstgeneral-purposeNeRF-
languageassistantcapableofperformingnewtaskssuchasNeRFcaptioningand
Q&A. Notably, our method directly processes the weights of the NeRF’s MLP
to extract information about the represented objects without the need to render
imagesormaterialize3Ddatastructures. Moreover,webuildadatasetofNeRFs
withtextannotationsforvariousNeRF-languagetaskswithnohumanintervention.
Basedonthisdataset,wedevelopabenchmarktoevaluatetheNeRFunderstanding
capabilityofourmethod. ResultsshowthatprocessingNeRFweightsperforms
favourablyagainstextracting2Dor3DrepresentationsfromNeRFs.
1 Introduction
LargeLanguageModels(LLMs)[70,1]haverevolutionizedthefieldofNaturalLanguageProcessing,
demonstratingincredibletextcomprehensionandgenerationcapabilities. Theseresultshavefostered
the development of Multimodal LLMs (MLLMs) [14, 81, 44, 11, 10], which can ingest various
modalitiessuchasimages,videos,andaudio,togeneratethentextdescribingandreasoningaboutthe
contentofsuchmodalities. Recently,MLLMshavealsobeenextendedto3Ddata[58,77],primarily
representedthroughcoloredpointclouds,yieldingremarkableresultseveninthisscenario.
Beyondimagesand3Ddata,anotherparadigmisemergingtorepresentobjectsandscenes: Neural
RadianceFields(NeRFs)[50]. NeRFsarecoordinate-basedneuralnetworks,typicallyMulti-Layer
Perceptrons(MLPs),designedtocaptureboththegeometryandthephotorealisticappearanceofan
objectbylearningacontinuousradiancefieldateach3Dspatiallocation. Aftertraining,aNeRF
modelcanbequeriedtorenderrealisticimagesortoreconstructthe3Dsurfaceoftheencodedobject.
Preprint.Underreview.
4202
nuJ
71
]VC.sc[
1v04811.6042:viXraNeRF captioning NeRF Question Answering
NeRF NeRF
Des oc br ji eb ce t .the What is the color scheme ca o rf ? the interior design of the
A fr os nty t,l i aze sd tr, ac ia gr ht to bo an c-l kik , e a nd de p aic sti io nn g leo f c a a bb il nu .e T a hn ed b r oe ad t b feo aa tt u, rc eh sa ara sc it mer piz lie std ic b dy e a s ip go ni n wte ithd T gh ree y c , o wlo ithr s ac h coe nm tre a o stf i nth ge l ii gn hte teri ro cr od le os r i og nn ti hs ed a gr rk ill, e p ao ns dsi b hl ey a b dl la igc hk t so .r
no visible textures or details that suggest material or scale, giving it a toy-like or
model boat appear oa vn ec re a. ll T dh ee s im gna s ist i rs e mlik ie nl iy s cu es ne td o f fo ar cs ha ii ll din 'sg to or y .anchoring, and the How can this car be
used?
Zero-shot NeRF Classification
This luxury sedan can be used for transportation, providing a
What is the class NeRF comfortable and stylish ride for passengers.
of the NeRF?
What is the design of the roof of the
This is a sofa. car?
The roof of the luxury sedan features a sunroof, adding to its
sleek and sophisticated design.
Figure1: LLaNA.AnewMultimodalLargeLanguageModelthatunderstandsandreasonsonan
inputNeRF.Notably,ourframeworkprocessesdirectlytheNeRFweightsandperformstaskssuchas
captioning,Q&A,andzero-shotclassificationofNeRFs.
Therefore,capturinganobjectasaNeRFprovidesaninterestingalternativetocreateadigitaltwin
withrespecttostandardrepresentationssuchasmulti-viewimagesorpointclouds. Forinstance,
thankstoitscontinuousformulation,fromasingleNeRF,onecangenerateaninfinitenumberof
photorealisticimagesatanyresolutionwhilestoringonlytheweightsofanMLPinsteadoftheentire
imageset. SeeAppendixA.4formoreonthememoryadvantagesofusingNeRFs. Duetotheir
advantages,NeRFsareeffectivelybecominganewmodalitystoredandcommunicatedindependently,
withdatasetsofNeRFsbeingmadepubliclyavailable[25,61]andcompaniesprovidingdigitaltwins
ofobjectsrepresentedasNeRFs(e.g.,https://lumalabs.ai/).
TheincreasingadoptionofNeRFsandtheirappealingcharacteristicspromptedustothefollowing
researchquestion: isitpossibletobuildanMLLMabletoingestdirectlyNeRFs? Inspiredbyrecent
studies on meta-networks that can process neural fields [80, 42], we answer this question in the
positivebyshowingthatitispossibletoprocesstheweightsofagivenNeRFwithameta-network
encoder that projects the NeRF weights into the embedding space of a pre-trained LLM such as
LLaMA2[70]. Bydoingso,wecreatethefirstMLLMforNeRFs,dubbedLargeLanguageand
NeRFAssistant(LLaNA),whichcansolveNeRF-languagetaskssuchasNeRFcaptioning,Q&A
andzero-shotNeRFclassification(seeFig.1).
We also introduce a new NeRF–language dataset, that we will make publicly available, to train
LLaNAandtestthecapabilitiesofourassistant. Tocollectthisdataset,wedesignedanautomated
annotation framework that leverages MLLMs to produce text annotations for NeRFs trained on
Shapenet[8]. Usingthisdatasetalongsideanadditionalsplitcontainingmanuallycuratedtextual
descriptions[2],weestablishabenchmarkforNeRFtextualassistants.
SinceastraightforwardwaytocreateanassistantforNeRFswouldbetorenderimagesorextract3D
pointcloudsoutofitandprovidethemasinputtoexistingMLLMsspecificallydesignedtohandle
suchmodalities,wethoroughlycompareLLaNAagainstthesebaselinesontheproposedbenchmark.
Weshowhowtheresolutionoftheextracted3Dgeometryorimages,andforimagesalsothevantage
pointusedforrendering, negativelyimpactthequalityoftheMLLM’soutput. Importantdetails
mightbelostbyrenderingfromthewrongangle,ortheextractedgeometrymightnotbedetailed
enough. Viceversa,byoperatingdirectlyontheMLPweights,weextractalltheinformationthey
hold about the object without any other design decision. Our approach turns out to be the most
effectivewaytocreateaNeRFassistantasitconsistentlyoutperformsMLLMsprocessingimagesor
3DgeometriesextractedbyqueryingNeRFs. Ourcontributionscanbesummarizedasfollows:
•LLaNA,thefirstMLLMcapableofperformingtaskssuchascaptioningandQ&AonNeRFs.
•WeshowthatitispossibletobuildsuchanassistantbydirectlyprocessingtheNeRFsweightswith
ameta-encoder,whichisfasterandcapturesmoreinformationthanrenderingimagesorextracting
3Ddata.
2• We automatically create a NeRF-language benchmark based on ShapeNet, and we thoroughly
evaluateLLaNAonit,showingthatitperformsbetterthanapplyingpopularMLLMsondiscrete
representationsobtainedfromNeRFs.
2 Relatedwork
Multi-modalLargeLanguageModels(MLLMs). Significantadvancementshavebeenmade
by Large Language Models (LLMs) in language understanding, reasoning, and generalization
capabilities [62, 1, 54, 70, 75, 60]. These models have been extended into Multimodal Large
LanguageModels(MLLMs),whichbroadentheirreasoningabilitiesbyincludingothermodalities
likeimages[14,81,17,19],audio[26],andvideos[47,10]. MLLMsgenerallyaligntargetfeatures
withtextualonesandthenintegratethemintoLLMsforvarioustextinferencetasks. SomeMLLMs
are trained entirely from scratch [27, 56], others utilize pretrained LLMs [37, 4, 44, 38, 11]. 3D
MLLMsfocusonunderstandingthe3Dworldtypicallyrepresentedascoloredpointclouds[58,24,
85,20,77]ormulti-viewimages[23]. Someofthesemodelsaretrainedusing2Dimages[24,85,23]
whileothersdirectlyaligntextualphraseswithpoints[20,77,58].
Neuralradiancefields. NeRF[50]havebeenappliedinseveralvisualtaskssuchasnovelview
synthesis[48],generativemedia[57],androbotics[78]. ThebaseformulationemploysMLPsto
convertspatialcoordinatesintocolorsanddensities. Recentadvancementssubstituteorenhance
MLPswithexplicitdatastructures[9,68,16,52]forfastertrainingandinference.
Neural radiance fields and language. The interaction between NeRF and language has been
recentlyinvestigatedforseveralpracticalapplications.Manyworksaddresstheproblemofgenerating
geometricallyconsistentviewsofobjectsorscenesdescribedbytextualprompts[66,49,31,65,
40,36,57]. OtherapproachesfocusoneditingthescenerepresentedbyaNeRFfromtext,e.g.,by
changingtheappearanceandshapeofobjects[73,28,67,74,69,21,79,86],orbyinserting/removing
objectsinthescene[3,51]. Sometechniquesinvestigatenewtypesofradiancefieldsthatpredict
languagefeaturesforeachspatiallocationalongsidedensityandcolor[32,34]. Inparticular, by
distillingknowledgefromvision-languagemodelsintothesemodels,theneuralfieldscanbequeried
bytextualprompts. Unlikeallpreviousmethods,Ballerinietal.[5]isthefirsttoutilizeNeRFsasan
inputmodality. TheyaimtolearnamappingbetweentheNeRFandCLIP[59]embeddingspaces
toperformtaskssuchasNeRFretrievalfromtextualorimagequeries. Differently,ourgoalisto
developanMLLMcapableofreasoningaboutNeRFs.
Deeplearningonneuralnetworks. Severalstudieshaveexploredusingmeta-networks,i.e. neural
networksthatanalyzeotherneuralnetworks. Initially,researchersconcentratedonpredictingnetwork
characteristics,suchasaccuracyandhyperparameters,byprocessingtheirweights[71,64,33,30,45].
Several recent works focus on processing networks implicitly representing data (Implicit Neural
RepresentationsorNeuralFields).Thesemethodsperformtaskssuchasclassifyingorsegmentingthe
databyprocessingsolelytheweightsoftheinputneuralnetworks. Amongtheseworks,Functa[15]
trainsasharednetworkontheentiredatasetandthenlearnsacompactembeddingforeachsample
for downstream tasks. Later works concentrate on processing networks representing individual
datasamples,e.g.,aspecificobject. ByleveraginganovelencoderarchitectureforMLPweights,
inr2vec[12]extractscompactembeddingsfromINRsof3Dshapes,whichareemployedasinputs
fordownstreamtasks.nf2vec[61]extendsinr2vectoingesttheNeRF’snetworkweightstoclassify,
segment, orretrievesimilarNeRFs. Cardaceetal.[7]developastrategytoprocessneuralfields
representedbyahybridtri-planestructure. Otherapproaches[53,83,82,84]developequivariant
architecturestohandleMLPsbyexploitingweightspacesymmetries[22]asaninductivebias. Also,
GraphNeuralNetworkshavebeeninvestigatedtocomputeanetworkrepresentation[35,42]. Since
weaimtoprocessNeRFsdirectlyfromthenetworkweights,weemploynf2vecasourmeta-encoder
duetoitsefficientandscalablearchitecture.
3 Methodology
ThissectiondescribestheproposedLargeLanguageandNeRFAssistant(LLaNA).Weprovidean
overviewofNeRFsandthemeta-encoderthatmapsNeRFweightsintoaglobalembedding. Then,
wepresenttheoverallLLaNAframeworkanddiscussourtrainingprotocol.
3NeRF
USER: <n_start>
Projector It is a red sports car,
with steel wheels, with
black and red leather
seats
<n_end> What is the NeRF?
Figure2: Frameworkoverview. ExampleofNeRFcaptioning.
NeuralRadianceFields(NeRF) NeuralRadianceField(NeRF)[50]isaframeworkthatemploys
coordinate-based neural networks, typically MultiLayer Perceptrons (MLP) and is trained on a
collectionofimagesofanobjectorscenetakenfromvariousvantagepoints. Themainapplicationof
NeRFsisthetaskofnovelviewssynthesis,i.e.,photorealisticrenderingofimagesfromviewpoints
unseenattrainingtime. Initsbaseformulation,theMLPisafunctionofcontinuous3Dcoordinates
p=(x,y,z)∈R3,thatyieldsfour-dimensionaloutputs,RGBσ ∈[0,1]4. Thisoutputencodesthe
RGBcolorandthevolumedensityσofeach3Dlocationinthescene. Thevolumedensityσcanbe
interpretedasthedifferentialprobabilityofarayterminatingatpointp. Aftertraining,aNeRFcan
renderimagesfromanydesiredviewpointsatarbitraryresolutionbyqueryingitforthevaluesof
RGBandσatseveralpointsalongtheraycorrespondingtoeachpixelandapplyingthevolumetric
renderingequation[50].
Inthiswork,werealizeNeRFsasMLPscomposedofLhiddenlayers,aninputlayer,andanoutput
layer. AnexampleofMLPwith1input,1output,and1hiddenlayerisshowninFig.2(left). A
layerisparameterizedbyaweightmatrixplusabiasvector. Moreindetail,thehiddenlayersin
ourarchitecturehavethesamenumberofinputandoutputneurons,H,thushavingsquaredweight
matrices W ∈ RH×H for l = 1,...,L and H-dimensional biases b ∈ RH. As input p goes
l l
througha24-frequencyencoding[50],thefirstlayerhasW ∈R144×H andb ∈RH. Thefinal
in in
onehasW ∈RH×4andb ∈R4. RefertoAppendixAformoredetailsonNeRFs.
out out
Meta-encoder Inthiswork,weexplorehowaNeRFassistantcanberealizedbyprocessingthe
NeRFweightsdirectly. WeexpecttheNeRFweightstocontaincomprehensiveinformationaboutthe
representedobject,suchasitsgeometryandappearance. Thus,anencoderprocessingthemmight
extractallthenecessaryinformationfordownstreamlanguagetaskssuchascaptioningandQ&A.
Inspiredbytherecentdevelopmentofmeta-networkscapableofprocessingneuralfields[42,80],we
employasourmeta-encoderarchitecturenf2vec[80]. IttakesasinputtheweightsofaNeRFand
yieldsaglobalembeddingthatdistillsthecontentoftheinputNeRF.Inparticular,theweightmatrices
andbiasesoftheinputNeRFarestackedalongtherowdimensiontoformamatrixM ∈ RS×H,
whereS =144+1+L∗(H+1)+H+1=L∗H+L+H+146. Beforestacking,wepadthe
outputlayerweightsW andbiasesb withzerostoobtainH columns(seeFig.2,center).
out out
Themeta-encoderisparametrizedasanMLPwithbatchnormalizationlayers[29]andReLUnon-
linearities. To scale gracefully with the input MLP dimensions, the encoder processes each row
ofMindependently, extractingatotalofS tokens, eachoflengthG, fromaninputNeRF.They
arethenmax-pooledtoobtainaglobalrepresentationg ∈RGoftheNeRF,withG=1024inour
experiments. Theencoderispre-trainedusingtheself-trainingprotocolofnf2vec[80],i.e.,jointly
withadecoderarchitecturethat,givenasinputtheNeRFglobalembedding,reconstructsthesame
imagesastheinputNeRFfromarbitraryviewpoints. MoredetailsinAppendixB.
LargelanguageandNeRFassistant InspiredbyrecentapproachesthatcreatedeffectiveMul-
timodal Large Language Models, we build LLaNA by leveraging on a pre-trained LLM with a
Transformerbackbone[72],inourexperimentsLLaMA2[70],andinjectingtheNeRFmodalityinto
itsembeddinginputspace,asproposedforimagesand3Ddata[44,77](seeFig.2,right). Thanksto
theself-attentionmechanism,thetransformercanunderstandthecontextualrelationshipsbetween
textandNeRFtokens,enablingittogenerateresponsesbasedonbothtextandNeRFinputs.
4
Encoder
Meta
Max
Pool
Tokenizer
Tokenizer
LLMWe employ a trainable linear projection layer, ϕ, to project the embedding of the input NeRF
computedbythemeta-encoderintotheLLaMA2embeddingspace. Theprojectionlayerhasweights
W ∈RG×T,whereT isthewordembeddingdimensionoftheemployedLLaMAmodel. This
proj
embeddingisencapsulatedbetweentwospecialtokens,whoseembeddingsarelearnedend-to-end
whiletraining,namely<n_start>and<n_end>.
Then, given an input sequence of mixed NeRF and word tokens,
(<n_start>,ϕ(g),<n_end>,w ,w ,...,w ), where k is the number of word tokens, the large
1 2 k
languagemodelreturnsasequenceofpredictedwordtokens(wˆ ,wˆ ,...,wˆ ).
k+1 k+2 eos
Trainingprotocol Totrainourframework,weholdmulti-turnconversationsabouteachNeRF
available in the ShapeNeRF–Text dataset that we created (see Sec. 4). These conversations are
organizedintoasetofpromptsfromtheuserandexpectedground-truthanswersthatareusedto
optimizetheoriginalauto-regressiveobjectiveoftheLLM.Forthemeta-encoder,weemploythe
nf2vecencoderpre-trainedonShapeNetreleasedbytheauthors[80],andwekeepitfrozenduring
training. Wefollowthetwo-stagetrainingprotocoldelineatedinLiuetal.[44]:
Stage1: projectortraining. Inthefirststage, wetraintheprojectornetworkϕtoaligntheNeRF
andthewordembeddingspaceswhilekeepingtheLLMweightsfixed. Wetrainonaninstruction
datasetofbriefdescriptionstolearntheprojectionlayerefficiently. Wealsotraintheembeddings
ofthespecialtokensusedtoencapsulatetheNeRFone. Weoptimizetheprojectorweightsandthe
embeddingsfor3epochswithalearningrateof0.002andbatchsizeof64.
Stage2: instructiontuning. Duringthesecondstage,wetrainoncomplexinstructionstohelpthe
modelunderstandandreasonaboutNeRFdata. Inthisphase,weoptimizeboththeprojectorandthe
LLMfor3epochsonthedetaileddescriptions,single-roundandmulti-roundQ&Aconversations
availableinourdataset. Forthisphase,weemployalearningrateof0.0002andabatchsizeof16.
OurmodelisimplementedinPyTorchandtrainedon4NVIDIAA100with64GBofVRAMeach.
Completingbothstagesrequires∼1dayoftraining. Whenfine-tuningLLaMA2forourspecific
applications, westrictlyuseadatasetthatdoesnotcomefromscrapedwebdata. Thisapproach
shouldensurethattheintegrityandeffectivenessofthepre-existingsafeguardsaremaintained.
4 Benchmark
4.1 ShapeNeRF–Textdataset
TotrainandvalidateourNeRFassistant,weautomaticallycreatedadatasetofconversationsabout
NeRFs, the ShapeNeRF–Text dataset. It features paired NeRFs and language annotations for
ShapeNet objects [8], in particular for all the 40K NeRFs available in the nf2vec dataset [61].
WefollowedthestructuredefinedinPointLLM[77]tocreatethe
textualannotations. Moreindetail,wegeneratedabriefdescrip-
tion,adetaileddescription,3single-roundQ&As,andonemulti-
roundQ&Aforeachobject. Thebriefdescriptionsareconcise
0 n
captionsoftheobject,takingintoaccountitsglobalstructureand
appearance. Thedetaileddescriptionsarelongersentencesthatde-
1 ...
scribeallthedetailsoftheobject. Thesingle-roundQ&Asconsist
ofaquestionabouttheobjectandthecorrespondingground-truth
2 4
answer. Finally,themulti-roundQ&Asarelongerconversations LLaVA
formedby3questionsandtherelativeanswers. Theautomatic
dataannotationpipelineisinspiredbyCap3D[46]andisshown caption_0 caption_1 caption_2 caption_n
in Fig. 3. First, multiple views of each ShapeNet object have
Llama
beenrenderedfromdifferentperspectives. Then,eachviewhas
been provided as input to LLaVA (LLaVA2-13b) [44] to get a
Brief description Single-round Q&A
detaileddescriptionoftheobjectfromthatpointofview. After-
Detailed description Multi-round Q&A
ward,startingfromthecaptionsgeneratedbyLLaVA,LLaMA3
Figure 3: Automatic annota-
(LLaMA3-8B-chat)wasusedtogeneratethefinalground-truth
tionpipeline.
textdata(briefanddetaileddescriptions,singleandmulti-round
Q&As). BoththefrozenLLMsemployedtocreateourbenchmark
(LLaVA2-13b,LLaMA3-8b-chat)areequippedwithsafeguards.
5Whenbuildingtheground-truthdata,toensurediversityinthelanguageannotations,eachbriefand
detaileddescriptionhasbeenassociatedwithaquestionrandomlysampledfrom30instructionsfor
eachkindofdescription. Suchinstructions,togetherwiththecarefullyengineeredrequestprompts
forLLaVAandLLaMA,arereportedinthesupplementarymaterialinAppendixC.ShapeNeRF–Text
comprises40Kobjects,with240Ktextualannotations: 40Kbriefcaptions,40Kdetaileddescriptions,
120Ksingle-roundQ&A,and40Kthree-roundQ&A.Thesupplementarymaterialshowsanexample
oftheresultingtextsinFig.8. Wesplitthedatasetintotrain,validation,andtestsets,stratifiedon
theShapeNetclasses,containingdistinctobjectsinproportionsof80%,10%,and10%ofthetotal
numberofshapes,respectively.
4.2 Languagetasksandmetrics
WeevaluateNeRFassistantsonthreedifferentlanguagetasks,givenaninputNeRF:briefcaptioning,
detailedcaptioning,andsingle-roundQ&A.WeevaluatealltasksontheobjectsfromtheShapeNeRF–
Texttestset. Forbriefcaptioning,weadditionallyevaluatethemethodsontheGPT2ShapeHuman
ShapeText(HST)dataset[2],asubsetofShapeNetforwhichhuman-curatedbriefdescriptionsare
publiclyavailable. TogeneratethedialoguesforHST,werandomlypaireachofitsshapeswithone
ofthe30instructionsrequestingabriefdescription. Weemploystandardlanguagesimilaritymetrics
toevaluatethesemethods. Wecomputethecosinesimilaritybetweentheglobalembeddingsofthe
generated and ground-truth sentences provided by the pre-trained encoders Sentence-BERT [63]
andSimCSE[18]. Thesemetricsbasedonlearnednetworksarethemosteffectiveatmeasuring
thequalityofthegeneratedoutput. Wealsoincludestandardhandcraftedmetricsbasedonn-gram
statistics,likeBLEU-1[55],ROUGE-L[43],andMETEOR[6].
5 Experimentresults
5.1 Baselines
AsourmethodisthefirsttoinvestigatelanguagetasksonNeRF,therearenobaselinesintheliterature.
However,givenaNeRF,astraightforwardwaytocreateanassistantforitcouldbetorenderanimage
anduseanMLLMcapableofingestingimages. Alternatively,wecouldextractthe3Dshapefrom
theNeRFanduseoneoftherecent3DMLLMs. Hence,weuseMLLMsasoff-the-shelffoundation
models,trainedonhundredsofthousandsofshapesormillionsofimages,withoutperformingany
fine-tuningonthetrainingsetofShapeNeRF–Text,andconsidersuchpipelinesasnaturalbaselines.
Specifically, we use LLaVA (v1.6) [44], and BLIP-2 [39] for images, as well as PointLLM [77]
andGPT4Point[58]forcoloredpointclouds. Weemploytheofficialcodesandpre-trainedmodels
releasedbytherespectiveauthorsforsuchevaluations. 1. WenotethattheonlyofficialGPT4Point
weightsavailableatsubmissiontimewerethoseobtainedfromfine-tuningOPT-2.7BonCap3D[46].
Inthemainpaper,wepresenttheperformanceofallmethodsunderthemorerealisticscenariowhere
NeRFsaretreatedastheonlyinputdatatotheassistant. Hence,imagesandpointcloudscanonlybe
extractedfromNeRFs. DetailsontheextractionprocedureareprovidedinAppendixA.3. Moreover,
inAppendixD,wereporttheresultsdealingwiththeimagesusedtotraintheNeRFortheoriginal
3DpointcloudfromShapeNet,whichconfirmsthemethods’ranking. Whenrenderinganimage,
anon-obviousdesigndecisionforthepipelineisfromwhichvantagepointtorenderit. ShapeNet
artificiallysimplifiesthistasksinceallobjectshavebeencanonicallyalignedtoacommonreference
frame,butthismaynotbethecaseinageneralsetting. Toshowthevantagepoint’seffectonthe
assistant’sresults,wereportresultsprocessingafrontalorbackview.
5.2 NeRFcaptioning
Inthecaptioningexperiments,wetesttheassistants’abilitytodescribetheNeRFcontent. Weprompt
themwiththeNeRF,ortheimage/cloudextractedfromit,followedbythequestionwhichhasbeen
pairedwithitsground-truthdescription,asdetailedinsection4.2,e.g. "What’sthecontentofthis
NeRF/image/cloud?". Wethencollecttheanswersgeneratedbythemodelsandcomparethemwith
theground-truthdescriptionaccordingtotheselectedmetrics.
1LLaVA:https://github.com/haotian-liu/LLaVABLIP-2: https://github.com/salesforce/
LAVIS/tree/main/projects/blip2 PointLLM: https://github.com/OpenRobotLab/PointLLM
GPT4Point:https://github.com/Pointcept/GPT4Point.
6Model Modality Sentence-BERT SimCSE BLEU-1 ROUGE-L METEOR
LLaVA-vicuna-13b Image(FrontView) 61.00 61.16 14.30 20.00 23.31
LLaVA-vicuna-13b Image(BackView) 54.35 56.09 21.94 21.67 22.09
LLaVA-vicuna-7b Image(FrontView) 59.85 62.35 22.67 23.24 23.35
LLaVA-vicuna-7b Image(BackView) 55.68 58.46 21.97 22.46 22.50
BLIP-2FlanT5-xxl Image(FrontView) 56.13 58.21 5.46 18.69 9.67
BLIP-2FlanT5-xxl Image(BackView) 52.48 54.05 5.67 18.20 9.50
PointLLM-7b Pointcloud 49.59 48.84 16.74 17.92 14.56
GPT4Point-Opt-2.7b Pointcloud 41.85 40.22 11.76 16.54 11.63
LLaNA-7b NeRF 68.63 70.54 20.64 28.33 31.76
Table1: NeRFcaptioning-briefdescriptiononShapeNeRF-Text. Baselineresultsobtainedon
dataextractedfromNeRFs. Bestresultsinbold. Runner-upunderlined.
Model Modality Sentence-BERT SimCSE BLEU-1 ROUGE-L METEOR
LLaVA-vicuna-13b Image(FrontView) 55.62 55.56 6.56 11.81 14.52
LLaVA-vicuna-13b Image(BackView) 50.00 50.79 9.39 12.76 14.46
LLaVA-vicuna-7b Image(FrontView) 54.31 56.28 10.08 14.71 14.53
LLaVA-vicuna-7b Image(BackView) 51.75 52.29 8.13 13.96 14.18
BLIP-2FlanT5-xxl Image(FrontView) 57.11 59.43 8.21 18.02 12.14
BLIP-2FlanT5-xxl Image(BackView) 54.11 56.37 9.09 17.38 11.79
PointLLM-7b Pointcloud 43.40 44.50 8.53 11.64 9.97
GPT4Point-Opt-2.7B Pointcloud 43.15 42.22 12.02 18.73 13.69
LLaNA-7b NeRF 59.20 61.66 9.47 14.94 17.06
Table2: NeRFcaptioning-briefdescriptionontheHSTdataset. Baselineresultsobtainedon
dataextractedfromNeRFs. Bestresultsinbold. Runner-upunderlined.
Briefdescription. WereporttheresultsforthebriefdescriptiontasksonShapeNeRF–Textandthe
HSTdatasetinTab.1andTab.2respectively. ComparingLLaNAwiththebaselinesdescribedin
Sec.5.1,weappreciatehowLLaNAachievesthebestperformanceinmostmetrics,oftenbylarge
marginsagainstrunner-ups. Forinstance,fortheSentence-BERTsimilarityontheShapeNeRF–Text
dataset,LLaNAachieves68.63,7.63pointsmorethanLLaVA-vicuna13b,evenifLLaNAusesa
smallerLLM.ResultsontheHSTdataset,whichprovidesground-truthdescriptionsvalidatedby
humans, are generally lower for all methods. Yet, LLaNA provides again the best performance
accordingtomostmetrics. ThedifferenceinthequalityofthebriefdescriptionprovidedbyLLaNA
comparedtothebaselinesisshowcasedbythequalitativeresultreportedinthefirstrowofFig.4,
wherethedescriptionprovidedbyLLaNAisthemostaccurate.
Acleartrendinbothtablesandqualitativeresultsisthatimage-basedmodelstendtoperformbetter
thanmodelsprocessingpointclouds. Thisislikelyduetothelargeramountofdatausedduring
trainingofthemodalityencoder,i.e. millionsofimagesversushundredsofthousandsofshapes,
whichenhancestheirgeneralizationability,aswellasthecapabilityofimagestocapturemoredetails
than point clouds at the input resolutions required by image-based MLLMs versus 3D MLLMs.
Nonetheless,ourmethod,whichoperatesonNeRFs,benefitsfromaholisticviewoftheobjectand
providesthemostaccuratedescriptions.Remarkably,inLLaNA,allthenecessaryinformationforthis
languagetaskcanbeextractedfromasingleglobalembeddingobtainedbydirectlyprocessingthe
NeRFweights. Itisalsoworthpointingoutthat,whileLLaNAdirectlyprocessesweightsandthusis
independentbydesignfromspatialresolution,thebaselinesfaceacomputationaloverheadgrowing
withthedesiredresolutionduetothenecessityofextractingspatialdatafromNeRF(AppendixA.3).
Finally,comparingtheresultsofimage-basedMLLMswhenprocessingfrontversusbackviews,
wecanseethatthevantagepointhasanon-negligibleeffectontheperformanceofsuchbaselines,
withSentenceBERTandSimCSEmetricsdiminishingbyabout4pointsinallbaselines. Inadataset
withoutcanonicalposesforobjects,thiswouldbearelevantlimitationthatprocessingNeRFweights
seamlesslysidesteps.
Detaileddescription. Weevaluatetheperformanceforthedetaileddescriptiontasksonthepro-
posed ShapeNeRF–Text, reporting the results in Tab. 3 and the second row of Fig. 4. For this
task,thepoint-basedmodelPointLLM[77]performssimilarlytotheimage-basedoneLLaVA[44].
However,weappreciatethatLLaNAachievesthebestperformanceinallmetricsbylargemargins.
For instance, for the Sentence-BERT metric, LLaNA achieves 77.43, notably 18.35 points more
thanLLaVA-vicuna-13b. Theselargeimprovementsindicatethat,whileindividualimagesmaybe
7Model Modality Sentence-BERT SimCSE BLEU-1 ROUGE-L METEOR
LLaVA-vicuna-13b Image(FrontView) 59.08 58.87 23.63 23.55 22.55
LLaVA-vicuna-13b Image(BackView) 50.09 50.33 13.77 21.36 13.18
LLaVA-vicuna-7b Image(FrontView) 57.55 57.68 14.99 22.82 14.36
LLaVA-vicuna-7b Image(BackView) 53.11 54.46 14.73 22.47 14.05
BLIP-2FlanT5-xxl Image(FrontView) 41.27 40.69 0.18 7.83 2.60
BLIP-2FlanT5-xxl Image(BackView) 38.49 37.89 0.19 7.72 2.58
PointLLM-7b Pointcloud 59.02 58.30 10.28 19.26 10.55
GPT4Point-Opt-2.7b Pointcloud 42.44 38.33 3.72 9.21 5.13
LLaNA-7b NeRF 77.43 79.81 41.32 36.18 32.39
Table3: NeRFcaptioning-detaileddescriptiononShapeNeRF–Text. Baselineresultsobtained
ondataextractedfromNeRFs. Bestresultsinbold. Runner-upunderlined.
LLaNA PointLLM GPT4Point LLaVA BLIP-2
NeRF
What object is this NeRF What object is this point What object is this point What object is this What object is this image
rendering? cloud rendering? cloud rendering? image rendering? rendering?
w aT ai nth h dre e a a co ts yab l ee nje le g lc ok ut w li a as an r a nd s d hm s ao wo pp hd eh ie ti w er sn t i ci t, c h ih ra cai tg ue h b ld a- r re od n w fe rd os n i n gs e tp n x pe . t aea It nrk ih eoe a lrr .s Th spe li h k3 eeD r ic cm o ano l t od a be in jl e es crh , t o w pw li atc h ca eas d e y s ie n la slo ic dwr ea , t ite- White cubes with holes in them. iw sT it yh h ee la l oi t m hc wi era c wg lu eie tl ta h ts er ah re s o wm w "Jhbs Jil t e Aea m "b3 i oD no r n tdm h ei eto r. d cT ae enhl n deo t ef fe e ra m .a b tb uo le rx em s- the mo yd ee ll l ois w a lo s gp oe a ok ne ir t with a
NeRF
E ola fb to hr isa t Ne eo Rn F t ,h pe l ed ae sta ei .ls E tl ha ib so pra ot ie n to cn lo t uh de , d pe leta ai sls e .of Elabo pra ot ie n to cn lo t uh de , d pe leta ai sls e .of this Elab tho ir sa t ime ao gn e t ,h pe l ed ae sta ei .ls of E ola f b tho ir sa t ime ao gn e t ,h pe l ed ae sta ei .ls
This 3D object model represents a
b ta hla l odT an eTn b gw e ch d d Vo o Ts khe t m i Va it sg ibtoo se , cno o t mb a h r l d r oa ej t d die he n cn ec e dc ar a de nbnt r it t n n ee i b sias gd st dz ule d a e oa e ar rr d ocs e, r v l o. sf ki n tc em gl o a uT t ramn t uaa at nh h - , cnd lt da s e e lof hg ec ite dnueT bu r a o eo ge crV olt i fa sf h e au t i s ir tta hn l gr o .os oi i a nn n A m,c fT p s .g o w e hV s s rma l iha i m e i st gw h a b gs tb a h i i pt ci at al na l th a e l a n , an c c c , tca c o itdk u owi e e o r rs na . nf i nvn trl Tt ee ha e t t e oot re h m dh a r fk e oe iee tf sd b d h fa ed v ga ibTc ec sie rs ab ha lak es d cr er a pg re a t st kt h o hr r n d s go c teto i ot e t cru r y ln oea b s pn l edi uiean as lzd a venrh td c e. oe y st d k s dO f kii s s .g gm uc , e c Tv a r na ls , ea oe p h a,n i cn g u mr e i a a sl see ina n. np r e d oy T tt i if l ns moe ie i bsh wn sora jn t ae eg e eii nt ct o t u s c h i-j t ootu tr l tah ii i ft nd knxn mei hs e w et gg se oa i t i gt hv raps v ha d h ii i io o me b s la e tb es t t eun p hb sir di c ati i yeeg o,i ci ,o l m , r h ft r md u n i fa ro tt a ay s h, et ino g nrhn eb e lr a fe tea doo l au tt rm r efho s ie i ni n dae yf ti c ,ga . a small white box with a hole in it c twoI shot fc u aial is tlcl n gh nia c re ku eg eg ae e s op pese m ol ty yg rs. h m ei o trs n T ep si sf d eg rh o t ef s io - e ta r nbl i . ti t cc kb rs te Ti aa m oe e e i ho n ti us t iap ew oa s g. ld ga , n T n o c l et g a b h t vo osse i r ee evl fg pl r a e e rn ae aai armc n, r c e l dt d lw ei aa to iisoh gg. ng in tt fe iT iyh de tt v aalo h s p eae m llf e r a ra ie aoi n oym r ic s rfm ti re tnh eee ta w ha g ra ng i fd oem g r ace s e rr e e cka is ia t mzs e a n g o e oo .i ag ae lf s rr e g e r , aa e d ed images of faces
or to portray various concepts.
NeRF
What is the design of the wheels? What is the design of the wheels? What is the design of the wheels? What is the design of the wheels? What is the design of the wheels?
The wheels a ar le lo f yit t de ed s iw gi nth . a five-spoke The wheels ha dv ee s a ig b nl .a ck mesh-style a toy car bT luh rre y im to wa d hg i ese ec y le so r nu o ' rv a te n h y ep r s do p ev e sid c ige if nid c oi ds fe tt t ho a eo il s cs am a rb .a oll u a t n thd e The wheels are a tyre design
Figure4: QualitativeresultsofNeRFcaptioningandQ&A.ResultsonShapeNeRF-Text. From
toptobottom: briefanddetaileddescriptions,single-roundQ&A
sufficientforbriefdescriptions, theymaylackallthedetailsneededtoprovideacomprehensive
description. Moreover,thedependencyoftheoutputqualityontheselectedvantagepointremains
strong. Contrarily,theNeRFweightscontaindetailedandcompleteinformationabouttheobject,
which is fundamental for more granular description tasks, with the additional advantage of not
requiringtuningsuchhyperparameters. TheabilityofNeRFtocaptureholisticinformationaboutthe
objectisalsoshowninthesecondrowofFig.4,whereonlythedirectprocessingofNeRFweights
letsLLaNAunderstandthattheobjectisaTV.PointLLMandLLaVAprovidedetailedbutwrong
descriptions,likelybecauseoftheneedtoextracttheintermediatediscreterepresentationasapoint
cloudoranimage,losinginformation. Indeed,inbothcases,itishardevenforahumanobserverto
providetherightdescriptionfromtheintermediatemodalitiesshowninthefigure.
5.3 NeRFsingle-roundQ&A
Inthesingle-roundQ&Aexperiment,wetesttheabilityoftheassistantstoprovidepreciseanswers
tospecificquestionsabouttheobjectinsteadofopen-endedgeneraldescriptions. Wepromptthe
modelswiththeNeRF,ortheimage/cloudextractedfromit,followedbyoneofthequestionsin
thesingle-roundQ&AannotationsassociatedwiththeNeRF.Wethencollecttheanswergenerated
bythemodelandcompareitagainsttheground-truthanswerwiththeselectedmetrics. Resultsare
reportedinTab.4. Interestingly,PointLLM[77]performsbetterthanLLaVA[44]inthistask,likely
becauseithasbeenspecificallytrainedtoanswerdetailedquestionsaboutobjectsrepresentedas
pointclouds. Nevertheless,similarlytothedetaileddescriptionresults,LLaNAisthetop-performing
methodacrossallmetrics,againbylargemargins. Thisresultsuggeststhatthemeta-encoderandthe
8
noitpircseD
feirB
noitpircseD
deliateD
A&Q
dnuoR-elgniSModel Modality Sentence-BERT SimCSE BLEU-1 ROUGE-L METEOR
LLaVA-vicuna-13b Image(FrontView) 71.61 70.98 20.19 30.42 32.53
LLaVA-vicuna-13b Image(BackView) 68.25 69.06 20.03 29.84 32.27
LLaVA-vicuna-7b Image(FrontView) 71.79 71.96 25.79 34.04 34.86
LLaVA-vicuna-7b Image(BackView) 70.88 70.93 25.17 33.30 34.22
BLIP-2FlanT5-xxl Image(FrontView) 45.20 47.92 11.50 20.16 13.49
BLIP-2FlanT5-xxl Image(BackView) 45.06 47.66 11.50 19.98 13.44
PointLLM-7b Pointcloud 74.70 74.40 36.81 44.41 39.76
GPT4Point-Opt-2.7b Pointcloud 27.62 31.41 6.26 9.38 5.41
LLaNA-7b NeRF 81.03 81.56 46.16 53.17 50.15
Table4: NeRFQ&A-singleroundconversationonShapeNeRF–Text. Baselineresultsobtained
ondataextractedfromNeRFs. Bestresultsinbold. Runner-upunderlined.
NeRF
Model Modality Accuracy(%)
LLaVA-vicuna-13b Image(FrontView) 66.13
LLaVA-vicuna-13b Image(HardView) 63.90
LLaVA-vicuna-7b Image(FrontView) 60.25
What is the primary material used to make this table? LLaVA-vicuna-7b Image(HardView) 57.00
The primary material used to make this table is wood BLIP-2FlanT5-xxl Image(FrontView) 63.67
for the base and glass or marble for the top.
BLIP-2FlanT5-xxl Image(HardView) 61.47
What is the color of the table's top surface?
The table's top surface is white. PointLLM-7b Pointcloud 50.14
Can this table be used as a display piece? GPT4Point-Opt-2.7b Pointcloud 41.93
Yes, the table's unique design and the pattern on its LLaNA-7b NeRF 67.14
base make it suitable to be used as a display piece.
Table5: NeRFzero-shotclassification.
Figure5: NeRFmulti-roundQ&Aexample.
projectorcanextractfine-grainedinformationfromtheNeRF,eveniftheyareprocessingdirectly
NeRFweights. Remarkably,theamountofinformationtheycanextractletsLLaNAanswermore
preciselythanwhenimagesorpointcloudsareextractedfromtheNeRF.Indeed,asshowninthe
thirdrowofFig.4whichreportsaqualitativeexample,theonlyassistantabletoanswercorrectlyto
aprecisequestionabouttheappearanceofthetyresofthecarisLLaNA.Anotherqualitativeresult
confirmingtheabilityofLLaNAtoprovidehigh-qualityanswerstospecificquestions,inthiscasein
amulti-roundQ&Aexperiment,isreportedinFig.5.
5.4 Zero-shotNeRFclassification
Finally,wecompareassistantsonthetaskofzero-shotclassification. Wequerythemodelswiththe
sentence"WhatistheclassoftheNeRF/image/cloud? Chooseamongthese: <Shapenet_classes>"
where<Shapenet_classes>arethe10ShapeNetclassesavailableinourdataset. Weconsiderthe
answercorrectonlyifthegroundtruthclassappearsintheresponse. WereportresultsinTab.5on
theShapeNeRF–Textdataset. Notably,LLaNAachievesthebestperformance,demonstratingthe
effectivenessoftheproposedarchitectureinrealizingaNeRFassistant.
6 Limitationsandfuturedirections
Despite the promising results of our framework, it is the first study in this direction and several
limitations are yet to be addressed. First, the pre-trained nf2vec encoder, having been trained
exclusively on synthetic data from ShapeNet, may not generalize well to real-world objects. To
addressthis,futureworkshouldcreateaNeRF–Textdatasetincludingamorediversesetofobjects,
like the ones provided by Objaverse [13]. Another limitation is that nf2vec currently processes
only MLPs, restricting our model to MLP-only NeRFs. However, with the rapid advancements
in meta-networks, it may become very soon possible to extend LLaNA to more complex NeRF
architectures,suchasInstantNGP[52]. Forinstance,theapproachbyLimetal.[42]suggeststhe
feasibilityofprocessingvariousinputnetworkarchitectures,althoughitiscurrentlylimitedtosmall
networks. Finally,ourframeworkhasbeentestedsolelyonobject-centricNeRFs. Expandingits
applicationtoNeRFsrepresentingentiresceneswouldbeacompellingdirectionforfutureresearch.
97 Concludingremarks
ThispaperaddressedthenoveltaskofcreatingalanguageassistantforNeRF.Wehavetackledthis
problembyleveragingrecentadvancesinMLLMsandmeta-networksprocessingneuralfields. We
haveshownthatitisfeasibleandeffectivetoprocessdirectlytheweightsofaNeRFtoprojectitinto
theembeddingspaceoftokensoftheselectedLLM.Wehavebuiltandmadepubliclyavailablea
datasetoftextualannotationsofNeRFsandhaveshownthatourapproachcomparesfavourablyon
itstestsetwithrespecttoseveralfoundationalMLLMsusedasbaselinesforthenoveltasksofbrief
anddetailedcaptioning,questionanswering,andzero-shotclassificationofNeRFs.
Acknowledgements
We acknowledge the CINECA award under the ISCRA initiative, for the availability of high-
performancecomputingresourcesandsupport.
References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport. arXiv
preprintarXiv:2303.08774,2023.
[2] AndreaAmaduzzi,GiuseppeLisanti,SamueleSalti,andLuigiDiStefano. Lookingatwordsandpoints
withattention:abenchmarkfortext-to-shapecoherence. In2023IEEE/CVFInternationalConferenceon
ComputerVisionWorkshops(ICCVW),pages2860–2869.IEEEComputerSociety,2023.
[3] HaotianBai, YuanhuiyiLyu, LutaoJiang, SijiaLi, HaonanLu, XiaodongLin, andLinWang. Com-
ponerf: Text-guided multi-object compositional nerf with editable 3d scene layout. arXiv preprint
arXiv:2303.13843,2023.
[4] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,and
JingrenZhou. Qwen-vl: Afrontierlargevision-languagemodelwithversatileabilities. arXivpreprint
arXiv:2308.12966,2023.
[5] FrancescoBallerini,PierluigiZamaRamirez,RobertoMirabella,SamueleSalti,andLuigiDiStefano.
Connectingnerfs,images,andtext. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPR)Workshops,2024.
[6] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved
correlationwithhumanjudgments.InProceedingsoftheaclworkshoponintrinsicandextrinsicevaluation
measuresformachinetranslationand/orsummarization,pages65–72,2005.
[7] AdrianoCardace,PierluigiZamaRamirez,FrancescoBallerini,AllanZhou,SamueleSalti,andLuigidi
Stefano. Neuralprocessingoftri-planehybridneuralfields. InTheTwelfthInternationalConferenceon
LearningRepresentations,2024.
[8] AngelXChang,ThomasFunkhouser,LeonidasGuibas,PatHanrahan,QixingHuang,ZimoLi,Silvio
Savarese,ManolisSavva,ShuranSong,HaoSu,etal. Shapenet:Aninformation-rich3dmodelrepository.
arXivpreprintarXiv:1512.03012,2015.
[9] AnpeiChen,ZexiangXu,AndreasGeiger,JingyiYu,andHaoSu. Tensorf:Tensorialradiancefields. In
EuropeanConferenceonComputerVision(ECCV),2022.
[10] GuoChen,Yin-DongZheng,JiahaoWang,JilanXu,YifeiHuang,JuntingPan,YiWang,YaliWang,Yu
Qiao,TongLu,etal. Videollm: Modelingvideosequencewithlargelanguagemodels. arXivpreprint
arXiv:2305.13292,2023.
[11] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,Boyang
Li,PascaleNFung,andStevenHoi. Instructblip:Towardsgeneral-purposevision-languagemodelswith
instructiontuning. AdvancesinNeuralInformationProcessingSystems,36,2024.
[12] LucaDeLuigi,AdrianoCardace,RiccardoSpezialetti,PierluigiZamaRamirez,SamueleSalti,andLuigi
DiStefano. Deeplearningonimplicitneuralrepresentationsofshapes. InInternationalConferenceon
LearningRepresentations(ICLR),2023.
10[13] MattDeitke,DustinSchwenk,JordiSalvador,LucaWeihs,OscarMichel,EliVanderBilt,LudwigSchmidt,
KianaEhsani,AniruddhaKembhavi,andAliFarhadi. Objaverse:Auniverseofannotated3dobjects. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages13142–
13153,2023.
[14] DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,Ayzaan
Wahid,JonathanTompson,QuanVuong,TianheYu,etal. Palm-e:Anembodiedmultimodallanguage
model. InInternationalConferenceonMachineLearning,pages8469–8488.PMLR,2023.
[15] EmilienDupont,HyunjikKim,SMAliEslami,DaniloJimenezRezende,andDanRosenbaum. From
datatofuncta:Yourdatapointisafunctionandyoucantreatitlikeone. InInternationalConferenceon
MachineLearning,pages5694–5725.PMLR,2022.
[16] SaraFridovich-Keil,AlexYu,MatthewTancik,QinhongChen,BenjaminRecht,andAngjooKanazawa.
Plenoxels:Radiancefieldswithoutneuralnetworks. 2022IEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPR),2022.
[17] PengGao,JiamingHan,RenruiZhang,ZiyiLin,ShijieGeng,AojunZhou,WeiZhang,PanLu,Conghui
He,XiangyuYue,etal. Llama-adapterv2:Parameter-efficientvisualinstructionmodel. arXivpreprint
arXiv:2304.15010,2023.
[18] TianyuGao,XingchengYao,andDanqiChen.Simcse:Simplecontrastivelearningofsentenceembeddings.
In2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,EMNLP2021,pages6894–
6910.AssociationforComputationalLinguistics(ACL),2021.
[19] RohitGirdhar,AlaaeldinEl-Nouby,ZhuangLiu,MannatSingh,KalyanVasudevAlwala,ArmandJoulin,
andIshanMisra. Imagebind:Oneembeddingspacetobindthemall. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages15180–15190,2023.
[20] ZiyuGuo,RenruiZhang,XiangyangZhu,YiwenTang,XianzhengMa,JiamingHan,KexinChen,Peng
Gao,XianzhiLi,HongshengLi,etal. Point-bind&point-llm:Aligningpointcloudwithmulti-modality
for3dunderstanding,generation,andinstructionfollowing. arXivpreprintarXiv:2309.00615,2023.
[21] AyaanHaque,MatthewTancik,AlexeiA.Efros,AleksanderHolynski,andAngjooKanazawa. Instruct-
nerf2nerf:Editing3dsceneswithinstructions. InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision(ICCV),pages19740–19750,2023.
[22] RobertHecht-Nielsen. Onthealgebraicstructureoffeedforwardnetworkweightspaces. InAdvanced
NeuralComputers,pages129–135.Elsevier,1990.
[23] YiningHong,ChunruLin,YilunDu,ZhenfangChen,JoshuaB.Tenenbaum,andChuangGan. 3dconcept
learningandreasoningfrommulti-viewimages.InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR),pages9202–9212,2023.
[24] YiningHong,HaoyuZhen,PeihaoChen,ShuhongZheng,YilunDu,ZhenfangChen,andChuangGan.
3d-LLM:Injectingthe3dworldintolargelanguagemodels. InThirty-seventhConferenceonNeural
InformationProcessingSystems,2023.
[25] BenranHu,JunkaiHuang,YichenLiu,Yu-WingTai,andChi-KeungTang.Nerf-rpn:Ageneralframework
forobjectdetectioninnerfs. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages23528–23538,2023.
[26] RongjieHuang,MingzeLi,DongchaoYang,JiatongShi,XuankaiChang,ZhenhuiYe,YuningWu,Zhiqing
Hong,JiaweiHuang,JinglinLiu,etal.Audiogpt:Understandingandgeneratingspeech,music,sound,and
talkinghead. InProceedingsoftheAAAIConferenceonArtificialIntelligence,pages23802–23804,2024.
[27] ShaohanHuang,LiDong,WenhuiWang,YaruHao,SakshamSinghal,ShumingMa,TengchaoLv,Lei
Cui,OwaisKhanMohammed,BarunPatra,etal. Languageisnotallyouneed:Aligningperceptionwith
languagemodels. AdvancesinNeuralInformationProcessingSystems,36,2024.
[28] Sungwon Hwang, Junha Hyung, Daejin Kim, Min-Jung Kim, and Jaegul Choo. Faceclipnerf: Text-
driven3dfacemanipulationusingdeformableneuralradiancefields. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision(ICCV),pages3469–3479,2023.
[29] SergeyIoffeandChristianSzegedy. Batchnormalization:Acceleratingdeepnetworktrainingbyreducing
internalcovariateshift. InInternationalconferenceonmachinelearning,pages448–456.pmlr,2015.
[30] FlorianJaeckleandMPawanKumar. Generatingadversarialexampleswithgraphneuralnetworks. In
UncertaintyinArtificialIntelligence,pages1556–1564.PMLR,2021.
11[31] KyungminJo,GyuminShim,SanghunJung,SoyoungYang,andJaegulChoo. Cg-nerf: Conditional
generativeneuralradiancefieldsfor3d-awareimagesynthesis. InProceedingsoftheIEEE/CVFWinter
ConferenceonApplicationsofComputerVision(WACV),pages724–733,2023.
[32] Justin*Kerr,ChungMin*Kim,KenGoldberg,AngjooKanazawa,andMatthewTancik. Lerf:Language
embeddedradiancefields. InInternationalConferenceonComputerVision(ICCV),2023.
[33] BorisKnyazev,MichalDrozdzal,GrahamW.Taylor,andAdrianaRomero. Parameterpredictionfor
unseendeeparchitectures. InAdvancesinNeuralInformationProcessingSystems,2021.
[34] SosukeKobayashi,EiichiMatsumoto,andVincentSitzmann. Decomposingnerfforeditingviafeature
fielddistillation. InAdvancesinNeuralInformationProcessingSystems,pages23311–23330.Curran
Associates,Inc.,2022.
[35] Miltiadis Kofinas, Boris Knyazev, Yan Zhang, Yunlu Chen, Gertjan J Burghouts, Efstratios Gavves,
CeesGMSnoek,andDavidWZhang. Graphneuralnetworksforlearningequivariantrepresentationsof
neuralnetworks. InTheTwelfthInternationalConferenceonLearningRepresentations,2024.
[36] Han-HungLeeandAngelX.Chang. Understandingpureclipguidanceforvoxelgridnerfmodels,2022.
[37] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,FanyiPu,JingkangYang,ChunyuanLi,andZiwei
Liu. Mimic-it:Multi-modalin-contextinstructiontuning. arXivpreprintarXiv:2306.05425,2023.
[38] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.Blip-2:Bootstrappinglanguage-imagepre-training
withfrozenimageencodersandlargelanguagemodels. InInternationalconferenceonmachinelearning,
pages19730–19742.PMLR,2023.
[39] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. BLIP-2: Bootstrappinglanguage-imagepre-
trainingwithfrozenimageencodersandlargelanguagemodels. arXivpreprintarXiv:2301.12597,2023.
[40] Jianhui Li, Shilong Liu, Zidong Liu, Yikai Wang, Kaiwen Zheng, Jinghui Xu, Jianmin Li, and Jun
Zhu. Instructpix2neRF:Instructed3dportraiteditingfromasingleimage. InTheTwelfthInternational
ConferenceonLearningRepresentations,2024.
[41] RuilongLi,HangGao,MatthewTancik,andAngjooKanazawa. Nerfacc:Efficientsamplingaccelerates
nerfs. arXivpreprintarXiv:2305.04966,2023.
[42] DerekLim,HaggaiMaron,MarcT.Law,JonathanLorraine,andJamesLucas.Graphmetanetworksforpro-
cessingdiverseneuralarchitectures. InTheTwelfthInternationalConferenceonLearningRepresentations,
2024.
[43] Chin-YewLin. Rouge:Apackageforautomaticevaluationofsummaries. InTextsummarizationbranches
out,pages74–81,2004.
[44] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning.Advancesinneural
informationprocessingsystems,36,2024.
[45] JingyueLuandM.PawanKumar. Neuralnetworkbranchingforneuralnetworkverification. InInterna-
tionalConferenceonLearningRepresentations,2020.
[46] TiangeLuo,ChrisRockwell,HonglakLee,andJustinJohnson. Scalable3dcaptioningwithpretrained
models. arXivpreprintarXiv:2306.07279,2023.
[47] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFahadShahbazKhan. Video-chatgpt:Towards
detailedvideounderstandingvialargevisionandlanguagemodels. arXivpreprintarXiv:2306.05424,
2023.
[48] RicardoMartin-Brualla,NohaRadwan,MehdiSMSajjadi,JonathanTBarron,AlexeyDosovitskiy,and
Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages7210–7219,
2021.
[49] GalMetzer,EladRichardson,OrPatashnik,RajaGiryes,andDanielCohen-Or. Latent-nerfforshape-
guidedgenerationof3dshapesandtextures. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR),pages12663–12673,2023.
[50] BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoorthi,andRen
Ng. Nerf:Representingscenesasneuralradiancefieldsforviewsynthesis. InEuropeanconferenceon
computervision,pages405–421.Springer,2020.
12[51] AshkanMirzaei,TristanAumentado-Armstrong,MarcusA.Brubaker,JonathanKelly,AlexLevinshtein,
KonstantinosG.Derpanis,andIgorGilitschenski. Reference-guidedcontrollableinpaintingofneural
radiancefields. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision(ICCV),
pages17815–17825,2023.
[52] ThomasMüller,AlexEvans,ChristophSchied,andAlexanderKeller. Instantneuralgraphicsprimitives
withamultiresolutionhashencoding. ACMTrans.Graph.,41(4):102:1–102:15,2022.
[53] AvivNavon,AvivShamsian,IdanAchituve,EthanFetaya,GalChechik,andHaggaiMaron. Equivariant
architecturesforlearningindeepweightspaces. InInternationalConferenceonMachineLearning,2023.
[54] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,ChongZhang,
SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollowinstructionswith
humanfeedback. Advancesinneuralinformationprocessingsystems,35:27730–27744,2022.
[55] KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu. Bleu:amethodforautomaticevaluation
ofmachinetranslation. InProceedingsofthe40thannualmeetingoftheAssociationforComputational
Linguistics,pages311–318,2002.
[56] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,ShaohanHuang,ShumingMa,andFuruWei.Kosmos-2:
Groundingmultimodallargelanguagemodelstotheworld. arXivpreprintarXiv:2306.14824,2023.
[57] BenPoole,AjayJain,JonathanTBarron,andBenMildenhall.Dreamfusion:Text-to-3dusing2ddiffusion.
InTheEleventhInternationalConferenceonLearningRepresentations,2022.
[58] ZhangyangQi,YeFang,ZeyiSun,XiaoyangWu,TongWu,JiaqiWang,DahuaLin,andHengshuang
Zhao. Gpt4point:Aunifiedframeworkforpoint-languageunderstandingandgeneration. InCVPR,2024.
[59] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision.InInternationalConferenceonMachineLearning,pages8748–8763.PMLR,
2021.
[60] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.
Journalofmachinelearningresearch,21(140):1–67,2020.
[61] Pierluigi Zama Ramirez, Luca De Luigi, Daniele Sirocchi, Adriano Cardace, Riccardo Spezialetti,
Francesco Ballerini, Samuele Salti, and Luigi Di Stefano. Deep learning on 3d neural fields. arXiv
preprintarXiv:2312.13277,2023.
[62] ParthaPratimRay. Chatgpt:Acomprehensivereviewonbackground,applications,keychallenges,bias,
ethics,limitationsandfuturescope. InternetofThingsandCyber-PhysicalSystems,3:121–154,2023.
[63] NilsReimersandIrynaGurevych. Sentence-bert: Sentenceembeddingsusingsiamesebert-networks.
arXivpreprintarXiv:1908.10084,2019.
[64] KonstantinSchürholt,DimcheKostadinov,andDamianBorth. Self-supervisedrepresentationlearningon
neuralnetworkweightsformodelcharacteristicprediction. InAdvancesinNeuralInformationProcessing
Systems,2021.
[65] BipashaSen,GauravSingh,AdityaAgarwal,RohithAgaram,MadhavaKrishna,andSrinathSridhar.
Hyp-nerf: Learning improved nerf priors using a hypernetwork. In Advances in Neural Information
ProcessingSystems,pages51050–51064.CurranAssociates,Inc.,2023.
[66] HoigiSeo,HayeonKim,GwanghyunKim,andSeYoungChun. Ditto-nerf:Diffusion-basediterativetext
toomni-directional3dmodel,2023.
[67] HyeonseopSong,SeokhunChoi,HoseokDo,ChulLee,andTaehyeongKim. Blending-nerf:Text-driven
localizededitinginneuralradiancefields. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision(ICCV),pages14383–14393,2023.
[68] ChengSun,MinSun,andHwann-TzongChen. Directvoxelgridoptimization:Super-fastconvergence
forradiancefieldsreconstruction. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages5459–5469,2022.
[69] ChunyiSun,YanbinLiu,JunlinHan,andStephenGould. Nerfeditor:Differentiablestyledecomposition
for3dsceneediting. InProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputer
Vision(WACV),pages7306–7315,2024.
13[70] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama:Openandefficientfoundation
languagemodels. arXivpreprintarXiv:2302.13971,2023.
[71] ThomasUnterthiner,DanielKeysers,SylvainGelly,OlivierBousquet,andIlyaO.Tolstikhin. Predicting
neuralnetworkaccuracyfromweights. arXiv,abs/2002.11448,2020.
[72] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. InAdvancesinNeuralInformationProcessing
Systems.CurranAssociates,Inc.,2017.
[73] CanWang,MengleiChai,MingmingHe,DongdongChen,andJingLiao. Clip-nerf: Text-and-image
drivenmanipulationofneuralradiancefields. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages3835–3844,2022.
[74] Can Wang, Ruixiang Jiang, Menglei Chai, Mingming He, DongdongChen, and Jing Liao. Nerf-art:
Text-drivenneuralradiancefieldsstylization. IEEETransactionsonVisualizationandComputerGraphics,
pages1–15,2023.
[75] JasonWei,MaartenBosma,VincentZhao,KelvinGuu,AdamsWeiYu,BrianLester,NanDu,AndrewM
Dai,andQuocVLe. Finetunedlanguagemodelsarezero-shotlearners. InInternationalConferenceon
LearningRepresentations,2021.
[76] QiangengXu,WeiyueWang,DuyguCeylan,RadomirMech,andUlrichNeumann. Disn:Deepimplicit
surface network for high-quality single-view 3d reconstruction. In Advances in Neural Information
ProcessingSystems.CurranAssociates,Inc.,2019.
[77] RunsenXu,XiaolongWang,TaiWang,YilunChen,JiangmiaoPang,andDahuaLin. Pointllm:Empower-
inglargelanguagemodelstounderstandpointclouds. arXivpreprintarXiv:2308.16911,2023.
[78] LinYen-Chen,PeteFlorence,JonathanTBarron,Tsung-YiLin,AlbertoRodriguez,andPhillipIsola.
Nerf-supervision: Learningdenseobjectdescriptorsfromneuralradiancefields. In2022international
conferenceonroboticsandautomation(ICRA),pages6496–6503.IEEE,2022.
[79] Yingchen Yu, Rongliang Wu, Yifang Men, Shijian Lu, Miaomiao Cui, Xuansong Xie, and Chunyan
Miao. Morphnerf:Text-guided3d-awareeditingviamorphinggenerativeneuralradiancefields. IEEE
TransactionsonMultimedia,pages1–13,2024.
[80] Pierluigi Zama Ramirez, Luca De Luigi, Daniele Sirocchi, Adriano Cardace, Riccardo Spezialetti,
FrancescoBallerini, SamueleSalti, andLuigiDiStefano. Deeplearningon3Dneuralfields. arXiv
preprintarXiv:2312.13277,2023.
[81] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu,
HongshengLi, andYuQiao. Llama-adapter: Efficientfine-tuningoflanguagemodelswithzero-init
attention. arXivpreprintarXiv:2303.16199,2023.
[82] AllanZhou,KaienYang,KayleeBurns,AdrianoCardace,YidingJiang,SamuelSokota,JZicoKolter,
andChelseaFinn. Permutationequivariantneuralfunctionals. Advancesinneuralinformationprocessing
systems,37,2023.
[83] AllanZhou,KaienYang,YidingJiang,KayleeBurns,WinnieXu,SamuelSokota,JZicoKolter,and
ChelseaFinn. Neuralfunctionaltransformers. Advancesinneuralinformationprocessingsystems,37,
2023.
[84] Allan Zhou, Chelsea Finn, and James Harrison. Universal neural functionals. arXiv preprint
arXiv:2402.05232,2024.
[85] ZiyuZhu,XiaojianMa,YixinChen,ZhidongDeng,SiyuanHuang,andQingLi. 3d-vista:Pre-trained
transformerfor3dvisionandtextalignment. ICCV,2023.
[86] JingyuZhuang,ChenWang,LiangLin,LingjieLiu,andGuanbinLi. Dreameditor:Text-driven3dscene
editingwithneuralfields. InSIGGRAPHAsia2023ConferencePapers,pages1–10,2023.
14A DetailsonNeRFs
WereportheresomedetailsregardingtheNeRFsoftheShapeNeRF–Textdataset,whichweretrained
byZamaRamirezetal.[80]. TheNeRFcodeisimplementedleveragingtheNerfAcclibrary[41].
A.1 Architecture
AninstanceoftheemployedNeRFsconsistsofamulti-layerperceptron(MLP)thatcontainsthree
hiddenlayers,eachwith64neurons. TheReLUactivationfunctionisappliedbetweenalllayers
exceptforthelastone,whichcalculatesthedensityandRGBvaluesdirectlywithoutanyactivation
function. A frequency encoding [50] is applied to input 3D coordinates to improve the NeRF
reconstructionquality. NeRFsdonottakeininputtheviewdirection. TheMLPprocessesaninput
coordinatep∈R3,toproducea4-dimensionalvectorcontainingtheRGBσ.
A.2 Training
Training a NeRF consists of minimizing the error between the rendered images from the NeRF
andthegroundtruthimages. OurNeRFsweretrainedusinganL lossbetweenthepredictedand
1
groundtruthRGBpixelintensities,weightingbackgroundpixelslessthanforegroundpixels(0.8
foregroundsvs. 0.2background). Imagerenderinginvolvesqueryingtheneuralnetworkbyfeedingit
3DcoordinatestoobtainRGBcolorvaluesanddensityestimates. Byintegratingtheseoutputsalong
cameraraysusingvolumetricrenderingtechniques[50],colorsandopacitieshavebeenaccumulated
toproducethefinalrenderedimage. EachNeRFistraineduntilitreachesagoodreconstruction
quality,approximatelyfor2000steps.
A.3 GeneratingimagesandpointcloudsfromNeRFs
Tocomparewith2Dand3DMLLMsonthenewtasksofNeRFcaptioningandNeRFQ&A,we
needtorenderimagesorreconstructpointcloudsfromtheNeRF.Torenderimages,weemploythe
samevolumetricrenderingprocedureusedduringtheNeRF’straining. ToconvertNeRFintoapoint
cloud,themarchingcubesalgorithmisfirstappliedtothevolumetricdensityfieldderivedfromthe
NeRF.Thisprocessgeneratesameshbyidentifyingisosurfaceswithinthedensityfield. Themeshis
thenconvertedintoapointcloudbyconsideringonlythemeshvertices,uniformlydistributedinthe
3Dspace. WesampleRGBvaluesfromNeRFforeachpointcoordinatetoapproximatepointcloud
colors. AnexampleofdataextractedfromNeRFisdepictedinFig.6.
Generatingimagesandpointcloudsrequirestheusertomakesomedecisions,theeffectsofwhichon
theassistant’sperformancearenoteasytoanticipate. Inthecaseofimages,itisfirstofalldifficultto
decidetherenderingviewpoint. Itmighthappenthattheobjectisnotclearlyvisibleinthepicked
viewpointorthatimportantelementsaremissing. Anotherdecisionistheresolutionofthegenerated
image,which,iftoocoarse,maypreventtheidentificationoffine-graineddetails. Thesameconcerns
regardingtheresolutionalsoapplytopointclouds. Yet,themodalityencodermaynothandlelarge
resolutionsormaygreatlyincreasetheprocessingtime. Anotherimportantpointistheadditional
computationaltimerequiredtoextractdatafromNeRF.Forinstance,extractingpointcloudsfrom
NeRFwithonly8192pointsrequiresapproximately620ms. Moreover,thetimeforsamplingthe
MLPandrunningamarchingcubealgorithmscalescubicallywiththedesiredspatialresolution. On
theotherhand,thetimerequiredtoprocesstheMLPweightsisindependentofthespatialresolution.
GTFront RenderedFront GTBack RenderedBack GTPoints ExtractedPoints
Figure6: ExampleofdataextractedfromNeRF.Fromlefttoright: GTfrontview,renderedfront
view,GTbackview,renderedbackview,GTpointcloud,extractedpointcloud.
15A.4 NeRFmemoryoccupationcomparedtoimagesorpointclouds
AnimportantbenefitofusingNeRFtorepresentobjectsisthatmemoryconsumptionisdecoupled
fromspatialresolution. InFig.7,weanalyzethenumberofparametersneededforpointcloudsand
imagescomparedtoneuralfieldsbyalteringthespatialresolutionofthedata. Weaccountforall
variablesrequiredbyanexplicitrepresentationintheirparametercount. Forinstance,eachpoint
inapointcloudhassixparameterscorrespondingtoitscoordinates(x,y,z)andcolor(R,G,B),
whileeachpixelhasonlythreechannels(R,G,B). Theorangelinerepresentstheparametersofthe
NeRFMLP,whilethebluelinesindicatetheparametersfor3Dpoints(Fig.7-left)andimagepixels
(Fig.7-right).
WeobservethatthespaceoccupiedbytheNeRFMLPiscomparabletothatusedbypointclouds
in our experiments (i.e., 8192 points, the data size used in GPT4Point [58] and PointLLM [77]).
However,NeRFbecomesadvantageousforrepresentingdataassoonasthepointcloudsizeisgreater
than 8621 points. This is crucial, considering real datasets may contain point clouds or meshes
withsignificantlymorepointsorfaces;forexample,Objaverse[13]featuresmesheswithover107
polygons.
Theadvantagesareevenmorepronouncedforimages,whereasingleNeRFMLPcorrespondsto
36imagesataresolutionof22×22. Storingthe36picturesfromShapeNetRenderat256×256
resolution,usedtotrainourNeRFonasingleobject,requiressubstantiallymorememory.
Point cloud vs NeRF Pixels vs NeRF
60000 Point cloud 5419008 Images
NeRF MLP NeRF MLP
51728
51728
49152
224 x 224 x 36
8192 8621 10000 22 x 22 x 36 (ShapeNetRender)
Number of 3D Points Number of Pixels
Figure7: MemoryusageofNeRFcomparedtoimagesorpointclouds. Left: NeRFvspoint
clouds. Right: NeRFvspixels.
B DetailsontheMeta-Encoder
We employ nf2vec [80] as the meta-encoder of LLaNA. Thus, in the following paragraphs, we
describethedetailsofthenf2vecarchitectureandtrainingprotocol.
B.1 Architecture
Thenf2vecencoderconsistsof4linearlayerswith512,512,512,and1024outputneuronseach.
ThefinalembeddingextractedforagivenMLPisa1024-dimensionalarray. Eachlinearlayeris
followedbyabatchnormalizationandaReLUactivationfunction.
B.2 Training
We employ the official code and weights of nf2vec2 pre-trained on an augmented version of
ShapeNetRender[76]. Theencoderwastrainedinanend-to-endmannertogetherwithanimplicit
2https://cvlab-unibo.github.io/nf2vec/
16
sretemaraP
fo
rebmuN
sretemaraP
fo
rebmuNNeRF
Give a quick overview of the object represented by this NeRF. Could you elaborate extensively on what this represents?
A modern, silver-c eo nl go ir ne ed , f aig nh dt e ar sje int gw leit h ta a il p fino .inted nose, single Brief description e mng eA i t n as e ll le , ic e a k sn hda e n a ed ns m ,i n lio g kd l eee l yr ta n mi lf ai fg i dnh e,t e f oer f a je atut mrw in ait g th e a ra i a sp li lo v lii ken ert -e c sd o te ln o eo r le s ode r , b aa o lu ds mi yn ig w nl uie t mh .a
The aircraft has a single wing with a horizontal stabilizer at the
rear, and a cockpit area visible from the side perspective. The
color scheme is predominantly gray and white, with a white stripe
What is the purpose of t ph ee r sc po ec ck tp ivit e a ?rea visible from the side id ner du s icn ig an n ti in nag gp a p al e o san terg s a t lth to he -b ds e eid sae i g bi nn le e t n dh d e a o is rft c y r rel aiz a fte l i wsd m ir te h ap anr e d fos se ct un y st lia z ot ai no ti n o fu. n nT , ch p te o io sj ne s at ib' ls il ty y
and aerodynamics. Detailed description
The cockpit area is where the pilot sits and controls the aircraft.
What is the primary material used to make the fighter jet's body?
What is the design philosophy behind the fighter jet's
appearance? The fighter jet's body is likely made of a material like steel or
aluminum, giving it a metallic sheen.
The design appears to be a blend of realism and stylization,
possibly indica ft uin ng c ta io s nt ae la itl yth a-d ne ds aig en roe dd y a ni arc mra icft s .with a focus on What is the primary function of the single engine and tail fin?
The single engine provides propulsion, while the single tail fin
What is the purpose of the white stripe running along the side of helps stabilize the aircraft during flight.
the aircraft?
How can this fighter jet be used?
The white stripe is a stylized representation and may indicate the
jet's affiliation or nationality.
This fighter jet can be used for military operations, such as air-to-air
combat, air-to-ground strikes, or reconnaissance missions.
Single-round QA Multi-round QA
Figure8: ExampleofdatasamplefromShapeNeRF–Textdataset.
decoder. Thedecodertakesininput3Dcoordinatesafterafrequencyencodingandtheglobal1024-
dimensionaloutputoftheencoder. Itconsistsof5linearlayerswith1024neuronseach,followedby
ReLUactivationsexceptforthelastlayer. Ityieldsa4-dimensionaloutputRGBσ,similartothe
NeRFgivenininputtotheencoder. Theframeworksupervisioncomesfromthepixel-wiserendering
L errorcomputedbetweenthegroundtruthRGBimageandthepredictedimage,whichisobtained
1
throughvolumetricrenderingafterencodinganddecodingtheNeRF.
C DetailsonShapeNeRF–Textdataset
TheproposedShapeNeRF–Textdatasetconsistsof40KpairedNeRFsandlanguageannotationsfor
ShapeNetobjects[8]. Inparticular,forevery3Dmodel,multipleannotationshavebeenprovided: a
briefdescription,adetaileddescription,3single-roundQ&As,andonemulti-roundQ&A.Figure8
showsanexampleofsuchannotations. TheseannotationshavebeenobtainedbyexploitingLLaVA2
andLLaMA3asdescribedinsection4ofthemainpaper.
Instruction prompts for LLaVA and LLaMA to generate the dataset For constructing
ShapeNerf–Text,first,descriptivecaptionsformultipleviewsofeachobjecthavebeenobtainedusing
thefollowinginputrequesttoLLaVA:
•“USER:<image>\nYouwillbeprovidedtheimageofanobject,seenfromthe<view_point>.
Describetheobjectindetail. Includeasmuchinformationaspossible, butdonotinfer
anythingthatisnotintheimage. Avoiddescribingthebackground. Generateananswer
withamaximumlengthof30words.\nASSISTANT:”
Theplaceholder<view_point>wasreplacedwith“back”,“side”,or“front”accordingtotheviewpoint
oftheimageprovidedasinput.ToexpeditecomputationandleveragethehighsymmetryofShapeNet
objects,7viewshavebeenemployedforthisprocess.
After obtaining the captions for each view, LLaMA was queried to aggregate these single-view
captionsintocomprehensivedescriptionsandQ&Arounds. TheinputprovidedtoLLaMAwas:
• Youwillbeshown7differentdescriptionsofanobject,obtainedfromdifferentpointsofview.
Pleaseprovidetwodescriptions,whichaggregatesalltheseones. Thefirstdescriptionmust
17beconcise,thesecondonewillbemoredescriptive. Boththesedescriptionmustrefertothe
samesubject. Avoidrepetitions. Important: Theoutputdescriptionsmustbefollowedbythe
string"Finalconcisedescription:"and"Finalmoredetaileddescription:". Notice: There
areerrorsinsomedescriptions,duetoocclusionandimproperangle. Youneedtocombine
allthedescriptionsandeliminatepossiblewrongdetails(pleasefixtheerrorsdirectly,do
nottellme). Inputdescriptions: [listofthesingle-viewcaptionsgeneratedbyLLaVA]
ThedetaileddescriptionwasthenusedtogeneratemultipleQ&Arounds, throughthefollowing
request:
• Given this description of an object, generate 6 short Q&A dialogues regarding diverse
aspects of the object described, ensuring logical relevance between the questions and
answers. Includealwaysaquestionabouthowthisobjectcanbeused. Questionbegins
with’Q’.Answerbeginswith’A’.IMPORTANT:Donotmentionsize,background. Donot
mention"howmany". Donotaddtextafterthelastanswer.".
Fromthe6generatedQ&Apairs,3wererandomlysampledtobuildthesequenceofmulti-round
Q&A,whiletheremainingpairswereusedassingle-roundQ&A.
InstructionPrompts Tab.6andTab.7providethelistofquestionsusedtobuildtheground-truth
dataofShapeNeRF–Text,asexplainedinSec.4.1.
Table6: Listofquestionstopromptthemodeltoproducebriefdescriptions. Aninstructionfromthe
listisrandomlyselectedandcoupledwithaShapeNeRF–Textbriefcaptiontoformaground-truth
datasample.
•Summarizethe3Dobjectbriefly.
•WhatkindofobjectisdepictedbythisNeRF?
•Provideashortexplanationofthisobject.
•WhatdoesthisNeRFrepresent?
•Canyougiveabriefoverviewofthisobject?
•CharacterizetheobjectthisNeRFisillustrating.
•ShareabriefinterpretationofthisNeRF.
•Provideanoutlineofthis3Dshape’scharacteristics.
•WhatobjectisthisNeRFrendering?
•Deliveraquickdescriptionoftheobjectrepresentedhere.
•Howwouldyoudescribethe3DformshowninthisNeRF?
•WhatisthenatureoftheobjectthisNeRFisrepresenting?
•Presentacompactaccountofthis3Dobject’skeyfeatures.
•WhatcanyouinferabouttheobjectfromthisNeRF?
•Offeraclearandconcisedescriptionofthisobject.
•Howwouldyousummarizethis3Ddata?
•GiveabriefexplanationoftheobjectthatthisNeRFrepresents.
•WhatkindofstructuredoesthisNeRFdepict?
•CouldyoudelineatetheobjectindicatedbythisNeRF?
•Expressinbrief,whatthisNeRFisrepresenting.
•GiveaquickoverviewoftheobjectrepresentedbythisNeRF.
•Conveyasummaryofthe3DstructurerepresentedinthisNeRF.
•WhatkindofobjectisillustratedbythisNeRF?
•DescribetheobjectthatthisNeRFforms.
•HowwouldyouinterpretthisNeRF?
•CanyoubrieflyoutlinetheshaperepresentedbythisNeRF?
•Giveaconciseinterpretationofthe3Ddatapresentedhere.
•ExplaintheobjectthisNeRFdepictssuccinctly.
•Offerasummaryofthe3DobjectillustratedbythisNeRF.
18Table 7: List of questions to prompt the model to produce detailed descriptions. An instruction
fromthelistisrandomlyselectedandpairedwithaShapeNeRF–Textdetailedcaptiontoforma
ground-truthdatasample.
•Canyoutellmemoreaboutthis?
•Whatdoesthisrepresent?
•Canyoudescribethisinmoredetail?
•I’minterestedinthis. Canyouexplain?
•Couldyouprovidemoreinformationaboutthis?
•WhatexactlyamIlookingathere?
•Whatisthis?
•Couldyoudescribethedetailedstructureofthis?
•Thislooksinteresting. Canyouexpandonit?
•Canyouexplainmoreaboutthisform?
•Whatcanyoutellmeabouttheshapeofthisobject?
•Couldyoudelvedeeperintothis?
•Iwanttoknowmoreaboutthis. Canyouhelp?
•Canyouwalkmethroughthedetailsofthisobject?
•Canyouprovideacomprehensiveaccountofthisobject?
•OfferadetailedinterpretationofthisNeRF.
•Pleaseelucidateonthecharacteristicsofthisform.
•Couldyouprovideanin-depthdescriptionofthisstructure?
•WhatdoesthisNeRFrepresentinitsentirety?
•ElaborateonthedetailsofthisNeRF,please.
•Kindlyfurnishmewithmoreinformationaboutthisobject.
•Pleaseexpandontheintricatestructureofthisform.
•ProvideameticulousexplanationofwhatthisNeRFrepresents.
•ProvideadetailedexplanationofwhatthisNeRFrepresents.
•Irequestadetailedbreakdownofthisstructure.
•GiveathoroughrundownofthisNeRF.
•Canyouofferacompleteanalysisofthisobject?
•Iwouldlikeacomprehensiveexplanationofthisform.
•PleasedetailthespecificfeaturesofthisNeRF.
•Couldyouelaborateextensivelyonwhatthisrepresents?
D Groundtruthimagesandpointclouds
Thissectionpresentstheresultsofanexperimentinwhichthebaseline2Dand3DMLLMmodels
havebeenprovidedwithground-truthinputimagesandpointcloudsextractedfromtheoriginal3D
meshesinthedatasetratherthanfromtheNeRFs. Thisscenarioestimatesanupperboundforthe
performanceofsuchapproacheswhenusedasNeRFassistants,bysimulatingperfectextractionof
imagesorpointcloudsfromtheNeRFs. Inotherwords,itsimulatestheidealscenarioinwhichthe
encodingofinformationinsideaNeRFislossless,anon-realisitcsituationinwhichthebaselinescan
achievetheirbestperformance. Tab.8,Tab.9,andTab.10showtheresultsofthisexperimentsonthe
tasksofbriefdescription,detaileddescription,andsingle-roundQ&A,respectively. Forbrevity,the
best-performing2Dmodel,i.e.,LLaVA[44](onfrontviews)andthebest-performing3Dmodel,i.e.,
PointLLM[77],havebeentestedinthisscenario. Theresultsdemonstratethat,eveninthisidealized
andmostfavorablescenarioforthebaselines,LLaNAoutperformsthem.
Model Modality Sentence-BERT SimCSE BLEU-1 ROUGE-L METEOR
LLaVA-vicuna-13b Image(FrontView) 68.61 67.99 17.48 23.08 27.03
PointLLM-7b Pointcloud 51.99 51.70 17.19 18.63 15.03
LLaNA-7b NeRF 68.63 70.54 20.64 28.33 31.76
Table 8: NeRF captioning - brief descriptions on ShapeNeRF–Text dataset. Baseline results
obtainedondataextractedfromShapeNetmeshdata. Bestresultsinbold. Runner-upunderlined.
19Model Modality Sentence-BERT SimCSE BLEU-1 ROUGE-L METEOR
LLaVA-vicuna-13b Image(FrontView) 68.32 67.35 27.46 26.62 24.40
PointLLM-7b Pointcloud 61.87 61.77 10.65 19.90 10.93
LLaNA-7b NeRF 77.43 79.81 41.32 36.18 32.39
Table9: NeRFcaptioning-detaileddescriptionsonShapeNeRF–Textdataset. Baselineresults
obtainedondataextractedfromShapeNetmeshdata. Bestresultsinbold. Runner-upunderlined.
Model Modality Sentence-BERT SimCSE BLEU-1 ROUGE-L METEOR
LLaVA-vicuna-13b Image(FrontView) 78.40 75.68 22.65 33.04 35.70
PointLLM-7b Pointcloud 74.98 74.90 36.93 44.60 39.87
LLaNA-7b NeRF 81.03 81.56 46.16 53.17 50.15
Table10:NeRFQ&A-single-roundQ&AonShapeNeRF–Textdataset. Baselineresultsobtained
ondataextractedfromShapeNetmeshdata. Bestresultsinbold. Runner-upunderlined.
E Additionalqualitativeexamples
Thissectionprovidesadditionalqualitativecomparisonsbetweentheproposedmethod,i.e. LLaNA
whichdirectlyprocessesNeRF,andthebaselinesthattakeasinputimages[44,39]or3Drepresenta-
tions[77,58]. Inparticular,Figs.9to11showadditionalbriefdescriptions,detaileddescriptions,and
single-roundQ&Aprovidedasoutputbythedifferentmethods. Manyexamples,suchasthewhite
speakerinthethirdrowofFig.10,arenotdescribedproperlybyMLLMsoperatingonpointclouds.
Indeed,duetotheinputpointcloudcontainingonly8192points,thesemethodscannotperceivethe
objectdetails,suchasthecurvedsurfaceofthespeaker,thereforetheypredictthattheobjectisa
“cubicwhiteobject”oran“icecube”. Inotherexamples,suchasthewhitescreensampleinthelast
rowofFig.11,theLLMoperatingonimagescannotgivetherightanswertothequestiononthe
buttonlocationasitisnotvisiblefromthegivenviewpoint. Contrarily,byoperatingdirectlyonthe
holisticrepresentationprovidedbyNeRFs,LLaNAprovidestherightanswerinthesesituations.
F Informationaboutdatasets,models,andsourcecodelicenses
Thissectionprovidesdetailsaboutthedatasets,models,andsourcecodelicensesusedinthepaper,
ensuringpropercredittothecreatorsororiginalowners,andadherencetolicenseterms.
Datasets:thedatasetsemployedinourworkandtherelativelicensesarelistedbelow:
• ShapeNet: licensedunderGNUAfferoGeneralPublicLicensev3.0.
• GPT2Shape HST: licensed under Creative Commons Attribution-NonCommercial-
ShareAlike4.0InternationalLicense.
Models:themodelsusedinallourexperimentsandtheirrelativelicensesaredetailedinthefollowing:
• nf2vec: licensedunderMITLicense.
• PointLLM:licensedunderCreativeCommonsAttribution-NonCommercial-ShareAlike4.0
InternationalLicense.
• GPT4Point: licensedunderCreativeCommonsAttribution-NonCommercial-ShareAlike
4.0InternationalLicense.
• LLAMA-2: licensedunderMETALLAMA2COMMUNITYLICENSEAGREEMENT3.
• LLAMA-3: licensedunderMETALLAMA3COMMUNITYLICENSEAGREEMENT4.
• LLAVA:licensedunderApacheLicense2.0.
Propercarehasbeentakentoensurethatalllicensesandtermsofuseareexplicitlymentionedand
respectedthroughoutthispaper.
3https://ai.meta.com/llama/license/
4https://ai.meta.com/llama/license/
20LLaNA PointLLM GPT4Point LLaVA BLIP-2
NeRF
What orbejencdte irsin tgh?is NeRF What objercetn idse trhinisg p?oint cloud What objercetn idse trhinisg p?oint cloud What orbejencdte irsi nthgi?s image What orbejencdte irsi nthgi?s image
reTcthaen goubljae arc nts dies a ta rti a cp nuin gsk uh lil aoe rna sath,r mear r tesusofttfesad. w biathc ktwreost, It is a 3D model of a sofa. mv epa tairei lo ,c ueas pop o ifiee bp ccjae eepc ootes ffr , , p p ian llaa cp ssliu tte iicd cc ,ie wn a g io t phfa i we as c ot heao o cdolk e,f ao gin f lp a b iister sicc ,e k as on, fda a couch couch
NeRF
What orbejencdte irsin tgh?is NeRF What objercetn idse trhinisg p?oint cloud What objercetn idse trhinisg p?oint cloud What orbejencdte irsi nthgi?s image What orbejencdte irsi nthgi?s image
repTrehse se honabtpaje etico wtn i i s toh af a as ftbly aoli taz teto,d pf,e atah ntruder eian- gd fl aiam t re ben acs cti kao .nngaullar This point cloud is illustrating a large cruise ship. A gun with a red dot. ptirTxueh cle ka ti oem rd a a rg ecepa arre,p swpee itnhta ot ar a s tt h ioft leo an t bb bo ae ef c daa k .ovloerw htr-iacre illees,o r pl auott tsi aoscnibh oleyr d a a boat.
NeRF
What orbejencdte irsin tgh?is NeRF What objercetn idse trhinisg p?oint cloud What objercetn idse trhinisg p?oint cloud What orbejencdte irsi nthgi?s image What orbejencdte irsi nthgi?s image
Thceu orvbejedc bt iasc ak rmesotd, feerant,u erirnggo nao ymeilclo cwh afriar mweith. a The point cloud illustrates a single chair. A child's wooden chair. chair. the model is a speaker with a yellow logo on it
NeRF
n rTo uih dssnse ee enw s aifa n ii h gg tn g i nh ctd e oa t se ,t cl ow ur w k n gjo pie tg g iht te e ti wnrh ss eg t eisD ta d li h en s e i s a ve tin l dl n ei ise ov so ed l,e e b s ov aokr b j i f e n, os l ma a dc e ia f b c et oq e t lu hk ter du r ae n e o ei a ic wp d d lrfc kr ne u aiy c ne rn s nd e mds ea dee ne ol im as a ltn w risc c tegt i a sr ce h aoei r pd loyna .w i t s r Tiih ar tio mnh cc he in rg ar e .er c ae . n o r tf j. aI oaf ett fs pit tw tlh ihapi ewe t ah nr sii s t ,dm ha a a as ap n tr s ro dy hiipni in ic gtegt so he slelod -r T phplairsey si ffsD uoe lpe a un, ol ntni 3evi don Dde nt i r ni- mc nra l ea o osq anu dtu alid mie msi rc lktr e aik oe c rtw fp ci d odh hare nee i a s tc ss e nac a ie g dnr crni n it dp o sot te l eao ci ood .henrn . is-h l sI dtoe tht rf yr e ee e lt me nth. ic 'e b sf i otg tyodh pyiteie csr a j lae lyt, De al i fv ige hr c ta elo rq u ju edi tc ,r ke s p pdr aee css ecer sni hpt ite piod on h r o e afr ie rth p.e la np eo .int The iampD apge eel aivi rm se sh ra t ooag w e bq s eur e a i acp wk r s e hd ts yie te leis zn c eatre dnipd dat ih io rree pn dr l e ao o. nf beth j eoe cr tj,e wthich Dtheel ivime Ara ga fie gq hrue ti ecpk rr e jd ese te .snctreipdt ihoenr eo.f
Figure9: Additionalqualitativeexamplesforthebriefdescriptiontask.
LLaNA PointLLM GPT4Point LLaVA BLIP-2
NeRF
Please detoafi lt hthise NspeeRcFif.ic features fePaletuarsees doef ttahiils t hpeo isnpt eccloifuicd. fePaletuarsees doef ttahiils t hpeo isnpt eccloifuicd. Pfleeaatsuer edse toafi lt hthise imspaegceif.ic Pfleeaatsuer edse toafi lt hthise imspaegceif.ic
c fge whT rb aaaa oah Ta rtyi mrdu merc h k jr auwku asero e e dpr ts k bi se act s jht ih uj nas the a to i s st gob aca l tsst nl it t i eh ir n i hts tsm i e as git a s,a su r t p t ue a a r tr dpml had id n e tes r eady pd rol s sa e bde r v hf e i i ensi l ge siv e eaad fdt nke es i ete gf te - rm oa o qasf hw edo rn u u ni tn oh nr d oc ifd t cpope i oc h fm epm frt be o ii ssco em aa el moe .uef dlc r d a id g if s s k to owe l ot o rb ir r sr e nn id pt na tu. cs dhoe h s opT t ig ms i, ve as mp hr i ia, g t a dioe ic lpna c eny ur ut dc a u v , a,co th t esw leau bif a oe rf rinr li tai nc ,v frhd oce .e rkad ebT sal iwha sttri yhc i ds en ki e mdc gi s sha u.o i n cT ga p bid n hnr im l o ,eigw tv io y tch bi d d ahai aete dare cl os d i k pro ui s n e rf ip n s o x aac a d fc o o ei tm ne tn ol sho l ul so d ee fr io c.d u on whuIe r ntt i r or n aolf n ue srg lfk m ta o ssuspt ef ob rpu rf d ti o pa arc tye il hre c no s ies w gn sc u .a .tghh ip Bc eearp aua deir o tii l, ils ps or tp et ,e nw, rd fr o a aii t to ombi chdd a i ai li asn inc t n agek o sytr n lfiie enntls egyt k variousfr aongi,m aa hlsa,n idn,c launddin ag haa dnodg o, na ac arot,c ak bird, a cwTbhilh taa he icr a ki im s sa a dong eld ide p w iscyh eto ei autde t'v aile nin n p ae dr o sd bivr amai cdwpke lriedn e, g s a m tpo , ipf an ea nima dcr ah fs ola it suiortr. sb lT ete yh g lea se., chair with a black and white seat and backrest
NeRF
Can you desdcertiabiel? this in more Can you desdcertiabiel? this in more Can you desdcertiabiel? this in more Can ymouo rdee sdcertiabiel? this in Can you desdcertiabiel? this in more
ctA ueis srt n chs n tlrli ee iene krde,e e. krn lTo ye .fh a c f mT,ent ahw od tne eus itmg l uh ns eu r to c va erld ra oi dseser ui e oi or nf ln nr vnnda e if im h nasrla a g leb wt os - l tgs a aa hac on lc el r ,k mde o sw e ra ia cn in nb trif he mdrlta a ee aaat n nl, e lp d b.ir s v pe latni es c cdai ato ka emr nn ss be i g ow gt ou i rnni d lt ab h aetreh nra ed ilnoT bh tpccc rh ai eaao gki grvs r r h sra p e y hoi plos le- a np o r ova sa ae n re t l ba 3 e wlop erDs o ors u et f r ra ar m nt k as po .p eb noo s Tba cr ssd pieu phd eee e r oel fsl cbo ic t r, o iuc ya d tfm aif .ki s ce l ta Tt e aas i a o ih liknds lgu nyi it i s nns s oa oe eg t c mi rntp d n o h i sga t ac ue f tir at o y onl ti dbv r reuio e ana pp esf gdg ro rb ei o eei,t t cl f s ea fu .ds ae nc ud l ts fk t eoge osi a bgs rb i to l hi je slag eyn as egn c a ttb c t. i lt n tue h fT hoog r ah ear er t ae varioubso aotb, jaenctds ,a i nwchluitdei nbgo xa wbiotha ta, ah obleed i,n a i tsmall dsT iqsh uced ae a rrim ernk a o aag rn ne ryd e y s clo pa tau ec n'kcv gsie fl ie cdp e acrto ogav naili ti,d ne me snd tta . a ka I itp bn lp lgoae ociatkk rds s b i flat ifkoic ce u kb galet r tbovolue anr cy dk image>
NeRF
Can you oofffe trh ais c oobmjepclet?te analysis Caanna ylyosuis o offfe trh ais c oobmjepclet?te Caanna ylyosuis o offfe trh ais c oobmjepclet?te Caanna ylyosuis o offfe trh ais c oobmjepclet?te Can you oofffe trh ais c oobmjepclet?te analysis
pT ta fde arth ohix rrc se te nete p s u oc t o n lc sr at fpb eat r pyr ae aj n eae ea e ng l alc dn a n ecut k d r loo el i gia s n wnna re r sia t t rttrb s h hh ao amh le a e rl ua a eo c k c d sp yknd e pice oe oe n og r lebra t ls sre on i e,n y il tr w, l nt s id .e ow h ,tT , s e na i o ag pah tm e rnh am nel ida pd.- nd ie br eT on e g tan r h ni h lae rm . d ue esn e sa n lb de s icp tl nl i y hoea es n is nenkcta a t g, rtkd ek m r r soee m as ee ilrst dl c s oe ki a" etg nr naLw p en goonopo i. t fbg ge ih I tt ust i sa hht y a elf ra eae ss s crr sa it h cd ie t bdt o oa" eum bi n s ,ir bwse e p taw e rds hl ona i iat lda tyh e o eoc ov qfo o f e u dn f mr iis at pfT hf oi l mes lh e d tr cii ee esn oo nn r g bli ntos t j o e wrm aa f c osri a tt tmm c nh ti oh es uo imre t rl sd cit maa ui oep s l lecll s tl io e izoo . pr o e mTf e li e rn a dh ap tf a e eu nlc b epr n du lon xa pc b b ca c lt pii i ojl cc ko e i ns ea, ,n c e tctw cta ei c eoomlh xit nnti ti oio at .sete frn y , sa hs o d.sb o,b eOt euipj pn e n so a egc e es n t p htsf dhia oi eb ie nc lcdlye ge Water and ice cubes. r mae owtH sd o s nac iiT oe pdto lo kh yh wt ,el ena u e a u.ae stit la ni Ts v, io fm ii edle h tinn at s r e' ca e ps t, o h g r o obI i.dr m a e nsa cDih l f es las ay f eiu ii eg n go bc nce ,h euu l og i y anl lt' ly i tv io on f ss e let rs gi oot ,d m rth mp y t tpte yt oor hl hei ro p z naol a d v o ie os ctv t ii w s d od caiisdt c hre l er lr teh eyrde e e oo a r p as bnma mfsro l p hde amsal aup aes c tt ote re in kt oi cae ro an d , eaicbn lrt lw e netas e saad d t h pot ni hn o o i ed wrg cidn n ce hb u g h g itso e l fr ih a m iei tcf c p ee r a dr a e ,i d s o pl al a wa eo hb nt ntic tw ao h o asttk px ihn h- ci l ieo e. e sthof .r image>
NeRF
Whats chaanp eyo ouf ttehlils m oeb jeacbto?ut the Whats chaanp eyo ouf ttehlils m oeb jeacbto?ut the Whats chaanp eyo ouf ttehlils m oeb jeacbto?ut the Wthhea ts chaanp eyo ouf ttehlils m oeb jeacbto?ut Whats chaanp eyo ouf ttehlils m oeb jeacbto?ut the
d gc fbi r eos Tia lT avp lh tr ee toh l ueaa rue a. r ty p c i tT nio t n hhh t ghb hg soe ej e c e an a t rc deo t e sotb e p e ihp lil s vs nu aw ,i e g e sa w ii rnnt o ha aiw bt t nes n h aa h f urd tr cf r hi o gaht o kew en gco u cth eems tn- oofi s,m dt a e vpe tpesca e ob l idir renn ist u t p wf gt c st tie sh t o iio cbcr t o r hf a oan lna yn rm aec m ne a ae re e t vsonw , r rt i d s.haf ai i et Pe ie nh baa rh d lnbnt ea ou od a dnr cf t l i eet aa an vo v mit g s mm s is cip ea bc o eec r lr ds aeee a wee ne k lbila et,etn aher nrc c r sa.h k maac a ooa cp wT pdl onho ph hemdor ei is l le n ea mb ddi e rls o ao osm t e gc o na . k s oo Is t3 ty n ,b y e,D f ' mpi tel ra ae evsam ok s e ptp si o uno e cth a fr l d ac yeot s moe i l s n f luity l o n het sw o h b gae. ef e ih nT ldaea m i yeh t n n e f paae ob d ho ir p rr ndl taol yu bed e un ns ax- 2s sdef h ts0ia ieg , ni 0 ts b crn bh gh 0 u iuaf.nii s utso tttt no n ee iwr t cnee r r ar tfascd iaeso, t, cpm a n w pg ern so eohr .g e b spsiTcuy ei uul hhle- na cle at hr sr a handthaebldle te, lepchtoronnei,c a dnedv gicaem, ien ccloundtirnogll ear laptop, mferT e oac rhv td euteeai rf r ifen niy im csg f os u ua d am rllg t mua b ea ter lo ae l oly tt d ioa to b ooi nu js nte hd c p' v c oee re tl or o r w in vp maw iir dat- aco hr en lgev e yai es mad o 'sm rse oel p ud re qre et t uia c i ao mdap iln fl elip ai ic c t tie gy m a d.sa ei e lau Ir ,sf s tgr I.ayf eat mioo l cso u ie gbf o h. hea raI t t a v' bs ee rectangle
Figure10: Additionalqualitativeexamplesforthedetaileddescriptiontask.
21NeRF
What is the primary material used What is the primary material used What is the primary material used What is the primary material used What is the primary material used
to make the chair's seat? to make the chair's seat? to make the chair's seat? to make the chair's seat? to make the chair's seat?
Th the e p cri hm aa irr 'sy sm ea at te ir sia bl lu as ce kd le t ao t hm ea r.ke The chair's seat is made of wood. The chair's seat is made of fabric tT hh ee c hp ar oi im r r' sa a r s fy ae m ba rta i cat -e p lir p kia e el a mu rss a e t to ed r bt iao e m faa bk re ic plastic
NeRF
What is the sbhaaspee? of the table's What is the sbhaaspee? of the table's Whatt aisb tlhee's sbhaaspee? of the Whatt aisb tlhee's sbhaaspee? of the Whatt aisb tlhee's sbhaaspee? of the
The table's base is either cylindrical or straight. The base of the table is square. The shape is rectangular. The table has a square base. A square.
NeRF
What is the color of the taillight? What is the color of the taillight? What is the color of the taillight? What ist athileli gchotl?or of the
What is the color of the taillight?
The taillight is red. The color of the taillight is red. The tailight is black. The taillight color of the car in the image is white. blue
NeRF
The power buttW onh e isr e lo i cs a lot thc eea d t pe ood nw? te hr e b ru igtt ho tn side of the TV. The power bW uh tte or ne tii shs e l lot o h cc ce oa a nt tpe esod dow? loee n.r b thu ett o ton p surface of a 3dW mh oe dr ee l i os f lo t ahc ea n t ipe nod tew? ne dr o b du stton w wsaT b ewh eb ph u le bieu p te tr csct tle i t o hiatio tm rn e,nt on h .ica n o t eo Weg ir w t cr h,e b s W ioa esd t uw hy u u eth s eo oit lct vdowe xcu uhi nr c ah' ti b e vt ea c,c meie s . tshi i s t ol I lp ,l o oafwo rt rb ceh c yco ocu a ao e av uo ct t tu ti eoilmp d ei od 't dr no e'd n es p t tw d . w y e u on a Ipe xta if fo ste i tr hp yc ,tkt r h b i,op a i ic n n teu aue l 'l l sge y at 'a brt p a o ner ab usirs en o b e o ta f tc tt or f ol so f teo o puo w nk c m ob tiuo an a ose anf rgt rt s ee dmh a v isba d e i aoa wr3 lb? etpnci uD iom t ph ctaua oliam hlint c g d .b deao ae e u er d tp y v it ta oe t, ehi o cl nl o roy ne mo rsn o f oioa ie rc na rrna ael onW thh ee r be a i cs k lot h oce fa ttp heo edw ? de er v b icu etton
Figure11: Additionalqualitativeexamplesforthesingle-roundQ&Atask.
22