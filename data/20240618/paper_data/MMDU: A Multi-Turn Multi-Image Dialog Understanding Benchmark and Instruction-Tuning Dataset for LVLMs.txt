MMDU: A Multi-Turn Multi-Image Dialog
Understanding Benchmark and Instruction-Tuning
Dataset for LVLMs
ZiyuLiu1,2,TaoChu2,YuhangZang†2,XilinWei2,XiaoyiDong2,PanZhang2,
ZijianLiang1,YuanjunXiong4,YuQiao2,DahuaLin2,3,JiaqiWang†2
1WHU,2ShanghaiAILaboratory,3CUHK,4MThreads,Inc.
2020302121195@whu.edu.cn, {zangyuhang, wangjiaqi}@pjlab.org.cn
Github: https://github.com/Liuziyu77/MMDU
Abstract
Generatingnaturalandmeaningfulresponsestocommunicatewithmulti-modalhu-
maninputsisafundamentalcapabilityofLargeVision-LanguageModels(LVLMs).
Whilecurrentopen-sourceLVLMsdemonstratepromisingperformanceinsimpli-
fiedscenariossuchassingle-turnsingle-imageinput,theyfallshortinreal-world
conversation scenarios such as following instructions in a long context history
withmulti-turnandmulti-images. ExistingLVLMbenchmarksprimarilyfocus
onsingle-choicequestionsorshort-formresponses,whichdonotadequatelyas-
sessthecapabilitiesofLVLMsinreal-worldhuman-AIinteractionapplications.
Therefore,weintroduceMMDU,acomprehensivebenchmark,andMMDU-45k,
alarge-scaleinstructiontuningdataset,designedtoevaluateandimproveLVLMs’
abilitiesinmulti-turnandmulti-imageconversations. Weemploytheclustering
algorithmtofindtherelevantimagesandtextualdescriptionsfromtheopen-source
Wikipediaandconstructthequestion-answerpairsbyhumanannotatorswiththe
assistanceoftheGPT-4omodel.MMDUhasamaximumof18kimage+texttokens,
20images,and27turns,whichisatleast5×longerthanpreviousbenchmarks
andposeschallengestocurrentLVLMs. Ourin-depthanalysisof15representative
LVLMsusingMMDUrevealsthatopen-sourceLVLMslagbehindclosed-source
counterpartsduetolimitedconversationalinstructiontuningdata. Wedemonstrate
that fine-tuning open-source LVLMs on MMDU-45k significantly address this
gap,generatinglongerandmoreaccurateconversations,andimprovingscoreson
MMDUandexistingbenchmarks(MMStar: +1.1%,MathVista: +1.5%,ChartQA:
+1.2%). Our contributions pave the way for bridging the gap between current
LVLMmodelsandreal-worldapplicationdemands. Thisprojectisavailableat
https://github.com/Liuziyu77/MMDU.
1 Introduction
Human-AIinteractionisafundamentaltasktoensurethatAIcanbedeployedintherealworldfor
everyone,enablinginclusiveandeffectivecommunicationbetweenhumansandAIinvariousaspects
ofdailylife. CurrentLargeVision-LanguageModels(LVLMs)[1]havemadesignificantstridesin
understandingandgeneratingtextconditionedonvisualinputs,showingpromisingdirectionsinAI
assistantapplications.
Current open-source LVLMs primarily focus on single-turn, single-image inputs, which are far
fromthecomplexitiesofreal-worldscenarios. Incontrast,effectivehuman-AIinteractionindaily
lifedemandsarangeofessentialskills,includingtheabilitytoengageinmulti-turnconversations
that involve multiple image inputs and comprehend long-context histories to facilitate coherent
Preprint.Underreview.
4202
nuJ
71
]VC.sc[
1v33811.6042:viXra(a) Previous Benchmarks (b) Benchmarks Comparison
(i) MMVet Q bo: y I hs a pth pe y ? Q t s Q idh h e1 o 2e nwr: : teW P in
t
l
y
i eih n n oaa s ftt t h eh tf e hu e a er i n mbn oai e wlat ydu g nzr r e eee o ?
r
o ta ohr m fee 71 .5 5 Datase Ct o inS strt ua cttis Mt Mi Dc U(Ours) 68 00 ^^ 22Ave Wr e c a oh nag tv ee e x t thT le e e nlo gx n tg ht e .s tT o +k 5e .4n k Len 6g .t 4h k
Q im: a gW e h isi ch the room based on the 34 Convbench Spot-the-Diff 40^2 +910 1k
m too bs et l ai k ree aly l c Qo 3nt :e Hnt o wof tt oh i rs e p nh oo vt ao t. e it 2 1 MMVet NLVR2 SEED Dr be ea nm cs him 2 20^2 25+65 90
photograph? (iii) ConvBench to be more childlike ? 1 22.42.73 3.8 0 MMVet BLINK Convbench MMDU(Ours)
(ii) BLINK Average Images Datasets
M4C BenchmarkTokens
Statistic
14
12
10
8
6
Image 1 Image 2 Image 3 Image 4 Image 5 4
2
Q1："Describe the architectural features visible in <image-1>, <image-2>, and <image-3>." 0
2k 4k 6k 8k 10k 12k 14k 16k 18k
A1：<image-1>, <image-2> and <image-3> depict three distinct architectural marvels from different parts of the world, Tokens Length
showcasing unique styles and features. <image-1> shows the Colosseum in Rome, an ancient amphitheater renowned for
its grand scale and engineering prowess...... <image-2> depicts the Hall of Prayer for Good Harvests within the Temple Average Text Tokens: 6.4K
of Heaven complex in Beijing, China. This building, completed in the early 15th century, is a masterpiece of traditional Images : 2-20
Chinese architecture...... <image-3> showcases the Sydney Opera House, a modern architectural icon located on the Average 15 rounds of Q&A
shores of Sydney Harbour in Australia. Designed by Danish architect Jørn Utzon and completed in 1973, the Opera
House is celebrated for its innovative design and engineering......
Q12："Discuss the historical significance of <image-1> and <image-4> in their respective cultures."
A12：The historical significance of <image-1> and <image-4> in their respective cultures is profound, reflecting their roles as iconic symbols of their
civilizations. <image-1> shows the Colosseum, an ancient amphitheater in Rome, Italy, constructed in the first century AD. The Colosseum is a symbol of
the might and grandeur of the Roman Empire...... <image-4> depicts the Taj Mahal, a mausoleum built in the 17th century in Agra, India, by the Mughal
Emperor Shah Jahan in memory of his beloved wife Mumtaz Mahal. The Taj Mahal is a symbol of love and devotion, renowned for its architectural beauty
and elegance......
(C) MMDU(Ours)
Figure1: ComparingMMDUwithpreviousLVLMbenchmarks. OurMMDU(bottom)incorpo-
ratesmulti-turnandmulti-imageinputs,longcontext,andopen-endedquestion-answeringevaluation,
makingitmorechallengingandcomprehensivethanpreviousbenchmarks(upperleft).
andcontextuallyappropriateconversations. Whileexistingbenchmarkspartiallyassesstheseabil-
ities[2,3,4](Fig.1(a)),theyhavelimitationssuchaslimitednumberoftotaltokensanddonot
provideacompletepictureofamodel’shuman-AIinteractioncapabilities. Morechallengingand
comprehensivebenchmarksarenecessarytoevaluateandadvancetheseskills.
WepresentMMDU,acomprehensivebenchmarkformulti-turnmulti-imagedialogunderstanding.
Ourdatacollectionpipelineautomaticallyselectsrelevantimagesandtextdescriptionsfromopen-
sourceWikipedia[5],formingthebasisformulti-turndialogues. Weemployaclusteringalgorithm
toidentifyrelevantWikipediaentitiesanddesignprompttemplatesforGPT-4otogeneratemulti-turn
questions. HumanannotatorsassessandrefineGPT-4o’sresponses,producingground-truthanswers
forourbenchmark.
OurMMDUbenchmarkpossessesthefollowingdistinctivefeatures: (1)Multi-turnandMulti-
image: Ourbenchmarkshowcasesaconversationalsettingwithamaximumof20imagesand17
turns,therebysurpassingthescopeofprecedingworks(seeFig.1(b)andauthenticallyreplicating
real-worldchatassistantinteractions. (2)LongContext: Withamaximumof18ktext+imagetokens,
ourbenchmarkevaluatesthecapacityofLVLMstoprocessandcomprehendextendedcontextual
informationwithalongcontexthistory. (3)Open-endedEvaluation: Departingfromtraditional
benchmarksthatrelyonclose-endedquestionswithconciseoutputs(e.g.,multiple-choicequestions
orshortanswers),ourbenchmarkadoptsamorerealisticandnuancedapproach,assessingLVLM’s
performancethroughfree-formmulti-turnoutputsthatprioritizescalabilityandexplainability,inspired
byNLPresearchthatleveragesstrongLLMsasjudges[6].
Weevaluate15proprietaryandopen-sourceLVLMsonourMMDUbenchmark. Ourevaluation
reveals a significant performance disparity between proprietary and open-source LVLMs. The
bestopen-sourcemodelscores42.8%,farbehindtheproprietaryGPT-4oat70.2%. Notably,our
observationsprovideacleardirectionforimprovingtheopen-sourcemodelsonlong-context,multi-
turn, and multi-image scenarios to bridge the performance gap. Based on our findings from the
2
snruT
egarevA
srebmuN
snekoT
egarevAbenchmarkresultsonMMDU,thepracticalneedurgesthevisualinstructiontuningdatacontaining
multi-turnandmulti-imagesforopen-sourceLVLMs.
TogetonestepclosertoproprietaryLVLMmodels,wefurtherpresentMMDU-45k. Wecollect45k
high-qualityinstructiontuningdatausingthesameprocessemployedinbuildingMMDU,witha
randomsamplingofhumanverificationinsteadoftheexhaustivehumanevaluationusedinMMDU.
AddingourinstructiontuningdataMMDU-45kintotheLVLMsupervisedfine-tuning(SFT)stage
improvesperformanceonvariousbenchmarks,suchasboostingInternLM-XC2[7]’sperformanceby
14.5%/1.1%/1.5%/1.2%onMMDU/MMStar[8]/MathVista[9]/ChartQA[10],respectively.
Ourmaincontributionissummarized: (1)WeintroduceMMDUthatassessesthemulti-turn,multi-
imagedialogunderstandingcapabilitiesofLVLMs,specificallydesignedforhuman-AIinteraction.
(2)WeconductacomprehensiveevaluationofexistingLVLMsonMMDU,revealingsignificant
challengesinthistaskandprovidingvaluableinsightsforfutureLVLMdevelopment. (3)Wepresent
MMDU-45k, a large-scale instruction tuning dataset designed to enhance dialog understanding
abilities. Wedemonstratethatfine-tuningLVLMsonMMDU-45kleadstoimprovedperformanceon
bothMMDUandexistingbenchmarks.
2 MMDUBenchmark
2.1 BenchmarkOverview
Although many LVLMs now claim to handle tens of thousands, hundreds of thousands, or even
millionsoftokensinlength,theiractualperformancesignificantlydeclinesinreal-worldapplications
asthenumberofimagesorthelengthofthecontextincreases. Boththedialoguequalityandimage
recognitioncapabilitiesofLVLMsdeterioratenotablyundertheseconditions.
Toevaluatethemulti-imagemulti-turndialoguecapabilitiesofexistingmodels,wehavedeveloped
theMMDUBenchmark. Ourbenchmarkcomprises110high-qualitymulti-imagemulti-turndia-
logueswithmorethan1600questions,eachaccompaniedbydetailedlong-formanswers. Previous
benchmarkstypicallyinvolvedonlysingleimagesorasmallnumberofimages,withfewerroundsof
questionsandshort-formanswers. However,MMDUsignificantlyincreasesthenumberofimages,
thenumberofquestion-and-answerrounds,andthein-contextlengthoftheQ&A.Thequestions
inMMDUinvolve2to20images, withanaverageimage&texttokenlengthof8.2ktokens,and
amaximumimage&textlengthreaching18Ktokens,presentingsignificantchallengestoexisting
multimodallargemodels. FormoredatastatisticsaboutMMDU,pleaserefertoTab.1andFig.4.
MMDU aims to test models’ abilities to simultaneously understand multiple images and follow
instructionsinlongdialogues. Wedesignprecisepromptstoevaluatethemodels’responses,andour
evaluationcriteriadetailsarediscussedinSec.2.3.
2.2 BenchmarkConstruction
DataCollection. Ourgoalinconstructingthisbenchmarkistomeasurethecurrentmodels’ability
tounderstandmultipleimagesandgeneratelongtextsingeneralscenarios.
Thefirststepisselectingappropriatemultipleimagesandrelatedtextualinformationasthefoundation
formulti-turndialogues. Giventhatthegenerateddialoguecontentneedstobelogicallycoherentand
richincontent,wecannotuserandomsetsofimagestobuildtheQ&Apairs. Randomimageswould
leadtolow-qualityandillogicaldialogues,bothinthequestion-constructionandanswer-generation
processes.
To address this issue, we employ a clustering method to construct high-quality image sets. We
extensivelyscreenedentriesontheopen-sourceWikipedia[5],encodedtherelevanttagsofentries
usingasentencetransformer[11],andclusteredtheentriesusingtheobtainedembeddings. After
clustering enough entries of the same category together, we further matched them using image
captions to obtain highly relevant entries and image sets. Then, within each cluster, we selected
multipleimagesandtheirassociatedtextualinformationtocreatecombinationsofimage-textpairs,
rangingfrom2to20images. TheprocessofcollectingandclusteringentriesisillustratedinFig.2
(a).
3(a) Data preparation
WIT Item Collection Data Cluster Combine
Item1 Content:The Starry Night is an Cluster 1 Cluster 2 Two items pair
oil-on-canvas painting by ......
Paris[a] is the
Paris[a] Cison ttehnet: Tchae preidt apaln adan (Adi lurus Three items pair
Item2 fulgens), also known as ...... opulous city of France. capital and most most populous city of France.
Item3 C ano dn W t me on ist t: tP hpa or pis u li os u t sh ce i tc ya p oi ft a ..l . ... Cluster 3 WithCl uster n populoFouusr icteimtys poairf
FrancFeive. iWtemist hpa ir
Item4 Content:The Great Wall of
China is a series of ......
Items pair =
· · ·
(b) Data Genetation pipeline
Prompt for question generation:
# Role Setting
Information Prompt You are a question-design expert specializing in creating
Single Item Single Item Inputs Inputs questions based on different images and materials. Your
& Summaty task is to design......
InternLM-
GPT-4o
Chat-20B
Prompt for answer generation:
Items Pair Extract Key Information Multi Items # Role Setting
& Summary You are an answer expert, specializing in answering
questions based on images. Your task is to answer multiple
questions based on the images provided by the user......
Multi-turn multi-image dialogue：
Single item dialogue MMDU-45k
Human Check
Q1:Describe the main subject of each image.
Multi items dialogue Combine A1: The main subjects of each image are as follows...... Filter & Modify MMDU Benchmark
Q15:Describe the main subject of each image.
Figure 2: An overview of (a) data preparation and (b) generation pipeline for MMDU and
MMDU-45k. WefirstcollecttherelevantimageandtextdescriptionsfromWikipediausingthe
clusteringalgorithm. ThenwepromptGPT-4otodesignmulti-turnquestions. Thehumanannotators
revisetheGPT-4oresponseastheground-truthanswers.
ConstructionwithGPT-4o. Afterobtainingthecombinationsofmultipleimages,weusecarefully
craftedpromptstoguidetheGPT-4omodelingeneratingcorrespondingquestionsandanswersbased
ontheavailableimagesandtextinformation. Initially, weconstructedmulti-turnQ&Apairsfor
eachsingleimageanditsassociatedtext. Then,weinputthecombinationsofmultipleimagesinto
GPT-4otogeneratemulti-turnQ&Apairsbasedonmultipleimages,ensuringthroughpromptsthat
thequestionscoveredmultipledifferentimagessimultaneously.
Buildingonthis,wecombinedthemulti-turnQ&Apairsformultipleimageswiththoseforeach
individualimage,creatingdialoguesthatincludebothsingle-imageandmulti-imagequestions. To
ensurethequalityofthebenchmark,weinvitedexpertstometiculouslyreviewthegenerateddialogues,
selecting110high-qualitymulti-turn,multi-imagedialoguesforourbenchmark. Additionally,we
carefully edited these 110 samples to eliminate hallucinations and errors in GPT-4o’s responses,
ensuringtheaccuracyandrichnessofthebenchmarkcontent. OurpipelineisshowninFig.2(b).
Furthermore, our generated multi-turn, multi-image data is highly scalable. During the Q&A
constructionprocess,werequiredGPT-4otoorganizethegeneratedtextaccordingtoourspecified
Text-ImageInterleavingFormat,usingtagslike<image-1>,<image-2>,etc.,torefertodifferent
images. Ourdesignisflexibletotreatthegeneratedmulti-turn,multi-imagedialoguesasfundamental
components. Bymodifyingthevaluesin<image-i>,wecanconcatenatemultipledialogues,thereby
constructingdialoguesinvolvingdozensorevenhundredsofimages. Ourdataisnotlimitedtoafew
imagesperQ&Agenerationbutiscapableofsupportingdialoguesoftheoreticallyunlimitedlength.
Quality Control with Human Annotators In the process of constructing the dataset, we im-
plementedtwostringentmeasurestoensureitsquality: (1)Wecombinedautomatedandmanual
screeningmethodstoselectimagesandtextsthatmeetourstandards. Specifically,weperformedan
initialscreeningusingclusteringtechniquesonalarge-scaleimageandtextdatabase,automatically
removinglow-quality,blurry,orirrelevantimagesandtexts.Thisensuredthattheimagecombinations
4
retsulC
ni
tceleSQuestion
Question: What are the central visual elements present in Image1 and Image2 that
contribute to their horror theme?
Assitant‘s Answer & Reference Answer
Assitant： Reference：
In Image1, the large skeleton holding Image1 and Image2 share several central visual elements that contribute to their overarching horror
a woman by her hair creates a sense themes. In Image1, the primary visual elements include a large, foreboding skeleton holding a hanging
of horror by representing death and ... woman and a dilapidated, eerie mansion in the background......In Image2, strikingly oversized hands
In Image2, the large red hands dominate the composition, grasping towards a terrified woman. The disproportionate size of the hands
reaching out towards the woman conveys a sense of imminent danger, overwhelming helplessness, and the uncanny, which are common in
create a sense of horror by evoking horror imagery. The dark tunnel framing these elements adds to the claustrophobic atmosphere,
feelings of fear, danger, and ... heightening the sense of entrapment and dread......
GPT-4o Judgement
### Richness
The response includes specific details such as the large skeleton holding the woman by her hair... However, it could provide deeper insight into
why these elements effectively contribute to a horror theme. **Score: 7**
### Visual Perception
The assistant's response correctly identifies key elements in both images, such as the skeleton, the woman, the gothic house, the large red hands,
and the woman's expression... Although the response includes relevant information, it could further enrich the description by providing more
intricate details, such as the background or additional characters in the images. **Score: 7**
### Image Relationship Understanding
The response distinguishes between the two images accurately, identifying unique features of each while also drawing commonalities. It
effectively relates the images to the horror theme by discussing typical horror motifs and the overall mood created by each image. **Score: 8**
······
### Overall Score
Considering the scores in each dimension, the assistant's response is accurate, logically structured, and rich in specific details but could use more
creativity and depth to perfectly capture the horror theme. **Overall Score: 7**
{'Creativity': 6, 'Richness': 7, 'Visual Perception': 7, 'Logical Coherence': 7, 'Answer Accuracy': 8, 'Image Relationship Understanding':
8, 'Overall Score': 7}
Figure3: TheevaluationpipelineofMMDU.WeusetheGPT-4oasajudgetogivetheoverallscorebasedon
thereferencedanswer.Ineachevaluation,GPT-4owillrefertoboththemodel’sanswerandthereferenceanswer.
Itwillprovidecorrespondingscores(ingreen)foreachevaluationcriterion(inblue),andfinally,summarizethe
results(inlightorange).
andtheircorrespondingtextswereofhighqualityandrelevance. (2)Toavoidhallucinationsand
errorsinthemodel-generateddialogues,weenforcedstrictqualitycontrolonthetextsgeneratedby
GPT-4o. Weintroducedamulti-roundmanualreviewmechanism. EachsetofQ&Aunderwentat
leasttworoundsofmanualreview: thefirstroundinvolvedpreliminarychecksbyregularreviewers,
andthesecondroundinvolvedin-depthexaminationandmodificationbyexperts. Thisensuredthat
thefinaldatasetwasnotonlyaccuratebutalsoofhighacademicandpracticalvalue.
2.3 Evaluation
InspiredbyNLPresearchthatleveragesstrongLLMsasjudges[6],wehavedevelopedanevaluation
pipeline using GPT-4o to evaluate model performance. Specifically, following the generation of
model predictions on our benchmark dataset, GPT-4o evaluates these predictions across various
dimensionsforeachturnandsample,comparingthemagainstreferenceanswers. Theaggregated
resultsacrossmultipleturnsareaveragedtoderivesamplescores,andtheaveragescoresacrossall
samplesconstituteourbenchmarkscores.
Toensureacomprehensiveandnuancedevaluation,wehaveidentifiedsixdimensions: Creativity,
Richness,VisualPerception,LogicalCoherence,AnswerAccuracy,andImageRelationshipUnder-
standing. ToguideGPT-4oinprovidingbalancedandequitableassessments,wehavemeticulously
craftedevaluationpromptsforeachdimension.Eachdimension’sscorerangeof10isdividedintofive
intervals(0-2,2-4...8-10),withcorrespondingjudgmentcriteriaestablishedforeachinterval. GPT-4o
follows these criteria to conduct judgment processes and deliver final scores for each dimension.
AsillustratedinFig3, guidedbyourprompts, GPT-4oassessestheassistant’sresponsesagainst
referenceanswers,offeringbothareasonablescoreandatransparentjudgmentprocess. Pleaserefer
tothesupplementarymaterialBforourjudgmentprompts.
53 MMDU-45kforInstructTuning
3.1 DatasetConstruction
We follow the same process as constructing the benchmark to build our MMDU-45k. First, we
collectavastnumberofWikipediaentriesandextractedtagsfromtheseentries,includingwikitree
labelsandimagecaptions. Weusesentencetransformerstoencodethetextualinformationandthen
applytheclusteringalgorithmtoobtaintheembeddings. Duringtheclusteringprocess,wecalculate
thecosinesimilaritybetweendifferentembeddingsandgrouphighlyrelatedentriesintoclusters
bysettingathresholdτ =0.75. Fromtheclusterswithhighrelevance,weselectmultipleimages
andtheircorrespondingentryinformationandperforminformationextractionandfilteringusing
InternLM-chat-20B[12]. WedesignprecisepromptstoguideGPT-4oingeneratingmulti-image,
multi-round,longdialoguesbasedontheinformationfilteredbyInternLM-Chat-20B.
Duringthedatasetconstructionprocess,weobtainseveralclusterswithawiderangeofcategory
distributions. This ensures that our dataset comprehensively covers various aspects of real life,
includinggeography,history,culture,mathematics,physics,chemistry,animals,plants,food,and
more. ThisrichknowledgewillhelpLVLMlearnlong-contextconversationalabilitiesingeneral
scenariosoftherealworld.
3.2 DatasetStatistics
IntheMMDU-45k,weconstructatotalof45kinstructtuningdataconversations. Thedatastatistics
areshowninTab.1. EachdatainourMMDU-45kdatasetfeaturesanultra-longcontext,withan
averageimage&texttokenlengthof5kandamaximumimage&texttokenlengthof17ktokens. Each
dialoguecontainsanaverageof9turnsofQ&A,withamaximumof27turns. Additionally,eachdata
includescontentfrom2-5images. Thedatasetisconstructedinawell-designedformat,providing
excellentscalability. Itcanbeexpandedtogeneratealargernumberandlongermulti-image,multi-
turndialoguesthroughcombinations. Theimage-textlengthandthenumberofturnsinMMDU-45k
significantlysurpassthoseofallexistinginstructtuningdatasets. Thisenhancementgreatlyimproves
themodel’scapabilitiesinmulti-imagerecognitionandunderstanding,aswellasitsabilitytohandle
long-contextdialogues.
Statistic Number >20 turns 10-20 images
0-10 turns 10.0% 6.3%
MMDUBenchmark 110 24.6% 15-20 turns
-Avg./Max.Image&TextTokens 8.2k/18k 42.7%
-Avg./Max.Images 3.8/20
-Avg./Max.Turns 15/27
2 images
-NumberofQAPairs 1645 10-15 turns 47.3% 3-10 images
22.7% 46.3%
MMDU-45k 45k Number of Turns Number of Images
-Avg./Max.Image&TextTokens 5k/17k
<4k single image related
-Avg./MaxImages 3/5 6.3% >12k 27.1% multi image related
-Avg./Max.Turns 9/27 15.5% 62.9%
-NumberofQAPairs 410k
-Single-imageRelatedQuestions 40k
-Multi-imagesRelatedQuestions 369k
4-8k
-Avg./Max.QuestionLength 31/91 50.0% 8-12k
-Avg./Max.AnswerLength 368/1518 28.2%
Token Length Question Distribution
Table1: StatisticsonMMDUandMMDU-45k.
Figure4: DetaileddistributionofMMDU.
4 Experiments
WeevaluatepreviousrepresentativeLVLMsonourMMDUbenchmarkinSec.4.1andpresentthe
analysisofourfindings. TodemonstratethehighqualityofourinstructiontuningdataMMDU-45k,
weprovidethecomparisonresultsofaddingMMDU-45kintheLVLMSFTstageinSec.4.2.
Baselines We report the performance of four closed-source API models: QWen-VL-Max [13],
Claude3[14],GPT-4-turbo[1]andGPT-4o[15]. Wealsopresenttheperformanceof11LVLMs
6Table 2: Evaluation results of different LVLMs on MMDU. We report the metrics of Creativity (C),
Richness(R),VisualPerception(VP),LogicalCoherence(LC),AnswerAccuracy(AA),ImageRelationship
Understanding(IRU),andtheaveraged(Avg.)results.
Models Param C R VP LC AA IRU Avg.
Closed-sourceLVLMs
Qwen-VL-Max[25] - 40.3 40.2 46.2 62.5 51.6 45.9 46.9
Claude3-Opus[14] - 58.6 61.5 59.7 75.1 64.1 59.8 62.6
GPT-4-turbo[1] - 62.0 64.2 63.4 78.0 69.0 64.4 66.3
GPT-4o[15] - 63.7 69.6 66.7 80.6 73.3 68.1 70.2
Open-sourceLVLMs
Monkey[16] 10B 11.9 12.0 14.8 21.9 19.6 14.6 14.1
Idefics2[17] 8B 17.8 17.6 27.9 43.1 32.8 26.9 25.4
LLaVa1.5-7B[18] 7B 27.8 28.0 33.2 43.0 35.4 31.7 32.2
Deepseek-VL[19] 8B 27.3 27.7 31.2 38.7 33.2 30.0 30.8
MiniCPM-v-2.5[20,21] 8B 27.0 26.4 33.2 48.9 38.6 32.2 33.0
Yi-VL[22] 6B 31.7 32.2 30.6 47.5 34.0 30.0 33.2
LLaVa1.5-13B[18] 13B 31.5 31.2 35.1 46.2 38.1 34.3 35.3
InternVL-Chat-V1.5[23] 26B 31.2 31.5 37.4 52.6 41.7 36.1 37.4
InternLM-XC2[7] 7B 29.7 29.5 36.2 50.1 40.3 35.2 35.6
Qwen-VL-7B[13] 7B 33.4 33.6 39.2 53.8 43.1 38.1 39.3
LLaVa1.6-mistral[24] 7B 37.7 39.3 41.4 57.2 45.6 40.2 42.8
LLaVa1.5[18]+MMDU-45k 7B 34.3 34.5 36.7 47.2 38.5 35.5 37.2
∆ +6.5 +6.5 +3.5 +4.2 +3.1 +3.8 +5.0
InternLM-XC2[7]+MMDU-45k 7B 45.6 43.9 49.9 64.1 53.0 48.7 50.1
∆ +15.9 +14.4 +13.7 +14.0 +12.7 +13.5 +14.5
including Monkey [16], Idefics2 [17], LLaVa1.5 7B/13B [18], Deepseek-VL [19], MiniCPM-v-
2.5 [20, 21], Yi-VL [22], InternVL-Chat-V1.5 [23], InternLM-XC2 [7], Qwen-VL-7B [13] and
LLaVa1.6[24]. PleaserefertothesupplementarymaterialDforthedetailsofourbaselines.
4.1 MainResultsonMMDU
Table2presentsthebenchmarkingresultsonourMMDUbenchmark. Ourkeyfindingsaresumma-
rizedasfollows. (1)OurbenchmarkposessignificantchallengestocurrentLVLMs. Notably,even
theadvancedGPT-4omodelachievesanaverageaccuracyofonly70.2%,whileopen-sourceLVLMs
achieve merely 42.8% or lower, indicating substantial room for improvement. (2) We observe a
significantperformancegapbetweenclosed-sourceLVLMsandopen-sourceLVLMs. Wespeculate
thatthisdisparityarisesfromthescarcityofopen-sourceinstructiontuningdatawithmulti-turnand
multi-imagecapabilities,leadingtolimitedimprovementinopen-sourceLVLMs. Thisinspiredusto
collectandreleaseMMDU-45k,avaluableresourcefortheopen-sourcecommunity,tobridgethis
gap.
In addition, we conduct experiments to evaluate the quality of our evaluation with GPT4-o by
comparingittohumanjudgment. Specifically,expertsscoretheresultspredictedbyeachmodelon
ourbenchmarkusingthesamejudgmentcriteriaasourevaluation. Wecalculatedseveralsimilarity
metricsfortheoverallscoresbetweenexpertsandtheGPT4-osystem. ThePearsonsimilarityof
97.5%indicatesastronglinearrelationship,whiletheSpearmansimilarityof97.3%demonstrates
consistentscoringmonotonicity. TheKendallsimilarityof89.0%suggestssomevariabilityinthe
manualscorescomparedtothejudgmentrangeofGPT4-o,yettheconsistencyremainshigh.
4.2 Fine-tuningResultsusingMMDU-45k
WeshowcasethesuperiorqualityofMMDU-45kbypresentingcomparativeresultsatthebottomof
Tab.2,whereweincorporateMMDU-45kintotheSFTstageofLVLMssuchasLLaVA1.5[18]and
InternLM-XC2[7]. ResultsdemonstratethataddingMMDU-45kincreasestheoverallperformance
on MMDU, especially for the image relationship understanding ability. In Tab. 4.2, we further
demonstratethatintegratingMMDU-45kalsobenefitsexistingbenchmarksthatrequiremulti-image
understanding, such as MMMU [27] and MMStar [8], as well as short-form QA datasets like
MMVet[4]. Toexplaintheperformanceimprovement,weprovidequalitativeexamplesinFig.5,
7Table 3: Illustration of the benefits of adding our MMDU-45k data in the LVLM supervised fine-
tuning (SFT) stage. We report the performance on our MMDU and existing representative benchmarks
includingMMB(MMBench-Dev-EN[26]),MMMU(MMMU-Val[27]),MMStar[8],MathVista[9],AI2D[28],
HallBench(HallusionBench[29]),MMVet[4]andChartQA[10]. Thebestandsecond-bestresultsineach
sectionarecolored Green and Red,respectively.
MM Math Hall Chart
Method MMDU MMB MMMU AI2D MMVet Avg.
Star Vista Bench QA
LLaVa1.5[18] 32.2 66.5 35.7 33.1 25.2 55.5 48.8 31.6 21.2 38.9
LLaVa1.5+MMDU-45k 37.2 66.5 37.4 34.1 25.2 56.2 48.7 31.9 23.4 40.1
∆ +5.0 +0.0 +1.7 +1.0 +0.0 +0.7 -0.1 +0.3 +2.2 +1.2
InternLM-XC2[7] 35.6 79.5 41.4 56.2 57.2 81.2 60.0 37.6 62.6 56.8
InternLM-XC2+MMDU-45k 50.1 79.9 41.9 57.3 58.7 81.2 60.4 38.8 63.8 59.1
∆ +14.5 +0.4 +0.5 +1.1 +1.5 +0.0 +0.4 +1.2 +1.2 +2.3
illustratingthatincorporatingMMDU-45kenablesLVLMstoengageinlongerandmoreaccurate
dialogues.
5 RelatedWork
LVLM Evaluation Benchmarks The rapid advancements in Large Vision-Language Models
(LVLMs)[13,14,1,15,16,17,18,19,20,21,22,23,7,13,24,30,31]havespurredthedevelopment
ofcomprehensiveevaluationbenchmarkstoassesstheircapabilitiesacrossvarioustasksanddomains.
Numerousbenchmarks[32,26,33,34,35,36,37,4,8]aimtoprovideastandardizedandobjective
way to measure the performance of LVLMs and track their progress toward achieving general
multi-modalunderstandingandreasoning.
Recently, specialized benchmarks have emerged to evaluate specific abilities [38, 29], such as
forsciencereasoning[27],mathreasoning[9],OCRrecognition[39],anddiagramanalysis[28].
Some existing benchmarks require multi-turn [2] chatting with a maximum of three turns, and
othersonmulti-imagecomparison[3,40]withamaximumoffourimages. However,noneofthe
existingbenchmarkscombinethemulti-turnandmulti-imageabilitieswithalongcontextwindow
forconversationapplications,highlightingtheneedforamorecomprehensiveevaluationframework.
LVLMInstruction-TuningDatasets ThedevelopmentofinstructiontuningdatasetsforLLMs
(e.g.,Alpaca[41],Vicuna[42])hasbeeninstrumentalinenhancingtheinstruction-followingcapabil-
ities. BuildinguponthesuccessesachievedinLLMs,researchershaveproposedvisualinstruction
tuningdatasets(e.g.,LLaVA-Instruct-150K[32],LLaVA1.5-665K[18])toimprovetheinstruction-
following abilities of LVLMs. Moreover, several instruction-tuning datasets have been designed
toenhancespecificskills[43,44,45],suchasShareGPT4V[46]forcaptiongeneration,mPLUG-
DocOwl[47]fordocumentunderstanding,andVideoChatGPT[48]forvideocomprehension. To
thebestofourknowledge, ourMMDU-45kisthefirstopen-sourcemulti-turn, multi-image, and
long-contextinstructiontuning,makingitavaluableresourceforadvancinghuman-AIinteraction
capabilities.
6 Conclusion
Inthispaper,weintroduceMMDU,amulti-mage,multi-turn,andlong-contextbenchmarkdesigned
toenhancethedailyhuman-AIconversationexperience. Ourcomprehensiveevaluationof15LVLMs
revealsasignificantperformancedisparity,withclosed-sourcemodelslikeGPT-4o[15]outperforming
theopen-sourceLVLMs. Thisdisparitymaybeattributedtothescarcityofopen-sourceinstruction
tuningdatasetsthatadequatelyassesstherequiredmulti-turnandmulti-imageabilities. Toaddress
thislimitationandcontributetotheopen-sourcecommunity,weproposeMMDU-45k,aninstruction
tuningdatasetcomprising45kexampleswithamaximumof17Ktexttokens,5images,and9turns.
Wealsodemonstratethatfine-tuningLVLMsonMMDU-45kimprovesperformanceacrossvarious
LVLMbenchmarks. OurMMDUandMMDU-45karepoisedtobenefittheresearchcommunityand
fosterfutureadvancementsinhuman-AIinteraction.
8(a) MMDU Case (b) MMBench Case
Question1: What are the
brands of the vehicles in
Image 3 and 4? And what is
the license plate number of
the car in the Image 1?
Question2: What are the
differences between the
vehicles in Image 1 and 2?
InternLM-Xcomposer2: Question: Which is the main topic of the
A1:In Image 3, the brand of the vehicle is Hyundai. In Image 4, the brand image? There are several options: A. A toy
of the truck is Isuzu. The license plate number of the car in Image 1 is bear and a toy dog, B. A toy bear and a toy
S.MA 5560. chicken, C. A toy bear and a toy cat, D. A
A2:The vehicles in Image 1 and 2 are different types of trucks. The truck toy bear and a toy rabbit
in Image 1 is a Mercedes-Benz, while the one in Image 2 is a Hyundai.
InternLM-Xcomposer2
InternLM-Xcomposer2+MMDU-45k A: The main topic of the image is A. A toy
A1: The brands of the vehicles in Image 3 and Image 4 are Hyundai and bear and a toy dog.
Isuzu, respectively. In Image 3, the vehicle is a Hyundai excavator, which Why?: I can't provide a reason because the
is a type of construction equipment used for digging and moving materials image itself doesn't give any specific
such as soil, sand, or gravel. Hyundai is a South Korean multinational context or information to support this
conglomerate that produces various types of machinery, including heavy statement. It is important to consider the
equipment like excavators. The brand name "Hyundai" is prominently overall content of the image, including the
displayed on the side of the machine. In Image 4, the vehicle is an Isuzu objects and their relationships before
fire truck. Isuzu is another Japanese multinational corporation known for drawing conclusions.
its automotive products, including trucks, buses, and motorcycles. The fire
truck in the image is specifically designed for firefighting operations, InternLM-Xcomposer2+MMDU-45k
equipped with water tanks, hoses, and other firefighting equipment. The A: The main topic of the image is B. A toy
brand name "Isuzu" can be seen on the front of the truck. Regarding the bear and a toy chicken.
license plate number of the car in Image 1, it reads "S.MA 5560." ...... Why?: I deduced the main topic based on
A2: The vehicles in Image 1 and Image 2 are quite different in terms of the presence of two distinct toys in the
their design, purpose, and intended use. In Image 1, the vehicle is a image. The first toy is a bear, which can
Mercedes-Benz convertible sports car. Mercedes-Benz is a German be identified by its characteristic brown
multinational corporation known for its luxury automobiles...... In contrast, color and teddy-like appearance. The
Image 2 shows a Mercedes-Benz G-Class SUV. This vehicle is designed second toy is an orange chicken with a
for off-road capabilities and rugged terrain. The G-Class is renowned for yellow head and legs. These two toys are
its durability and ability to traverse challenging landscapes, including prominently featured in the image, making
rocky terrains, steep inclines, and muddy paths...... them the primary subjects.
Figure5:VisualizationexamplesofaddingMMDU-45kintheLVLMSFTstage.Error/hallucination
descriptionsaremarkedinred,andthedetailedandaccuratedescriptionsaremarkedingreen. The
caseontheleftisfromMMDU,andthecaseontherightisfromMMbench.
Limitations WhileMMDUoffersseveraladvantages,weacknowledgetwokeylimitations. (1)
MMDU primarily focuses on English and does not encompass multilingual abilities. (2) Our
benchmark is designed to assess LVLMs’ proficiency in daily scenarios, rather than specialized
domainexpertise(e.g.,mathematicalproblem-solvinginMathVista[9]). Byacknowledgingthese
limitations,wehopetoencouragefutureresearchdirectionsandexpansionsofourbenchmarksuch
asincorporatingmultilingualsupportforotherlinguisticpopulations.
SocietalImpacts AsMMDU-45kisbuiltuponWikipedia,modelsfine-turnedonMMDU-45kmay
perpetuatebiasesandlinguisticpreferencesinEnglish. Moreover,LVLMsfine-tunedonourMMDU-
45kmaybesusceptibletofactualityandhallucinationissues,potentiallygeneratinginaccurateor
misleadinginformation. Byrecognizingtheserisks,wecanworktowardscreatingmoreinclusive,
accurate,andreliableLVLMsthatfostertrustworthyhuman-AIinteractions.
Author Statement and Data License The authors bear all responsibility in case of violation
ofrightsandconfirmthatthisdatasetisopen-sourcedundertheAttribution-NonCommercial4.0
International(CCBY-NC4.0)license. UsingthisdatasetshouldabidebythepolicyofOpenAI.
9References
[1] OpenAI. GPT4technicalreport. arXivpreprintarXiv:2303.08774,2023.
[2] ShuoLiu,KainingYing,HaoZhang,YueYang,YuqiLin,TianleZhang,ChuanhaoLi,YuQiao,Ping
Luo,WenqiShao,etal. ConvBench:Amulti-turnconversationevaluationbenchmarkwithhierarchical
capabilityforlargevision-languagemodels. arXivpreprintarXiv:2403.20194,2024.
[3] AlaneSuhr,StephanieZhou,AllyZhang,IrisZhang,HuajunBai,andYoavArtzi. Acorpusforreasoning
aboutnaturallanguagegroundedinphotographs. arXivpreprintarXiv:1811.00491,2018.
[4] WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,ZichengLiu,XinchaoWang,and
LijuanWang. MM-Vet:Evaluatinglargemultimodalmodelsforintegratedcapabilities. InICML,2024.
[5] KrishnaSrinivasan,KarthikRaman,JiecaoChen,MichaelBendersky,andMarcNajork. WiT:Wikipedia-
basedimagetextdatasetformultimodalmultilingualmachinelearning. InSIGIR,2021.
[6] LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,ZiLin,
ZhuohanLi,DachengLi,EricXing,etal. Judgingllm-as-a-judgewithmt-benchandchatbotarena. In
NeurIPS,2023.
[7] XiaoyiDong,PanZhang,YuhangZang,YuhangCao,BinWang,LinkeOuyang,XilinWei,Songyang
Zhang,HaodongDuan,MaosongCao,etal. InternLM-XComposer2: Masteringfree-formtext-image
compositionandcomprehensioninvision-languagelargemodel. arXivpreprintarXiv:2401.16420,2024.
[8] LinChen,JinsongLi,XiaoyiDong,PanZhang,YuhangZang,ZehuiChen,HaodongDuan,JiaqiWang,
YuQiao,DahuaLin,etal. Areweontherightwayforevaluatinglargevision-languagemodels? arXiv
preprintarXiv:2403.20330,2024.
[9] PanLu,HritikBansal,TonyXia,JiachengLiu,ChunyuanLi,HannanehHajishirzi,HaoCheng,Kai-Wei
Chang,MichelGalley,andJianfengGao. MathVista:Evaluatingmathematicalreasoningoffoundation
modelsinvisualcontexts. InICLR,2024.
[10] AhmedMasry,DoXuanLong,JiaQingTan,ShafiqJoty,andEnamulHoque. ChartQA:Abenchmarkfor
questionansweringaboutchartswithvisualandlogicalreasoning. InACLFindings,2022.
[11] NilsReimersandIrynaGurevych. Sentence-bert:Sentenceembeddingsusingsiamesebert-networks. In
Proceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.Association
forComputationalLinguistics,112019.
[12] ZhengCai,MaosongCao,HaojiongChen,KaiChen,KeyuChen,XinChen,XunChen,ZehuiChen,Zhi
Chen,PeiChu,etal. Internlm2technicalreport. arXivpreprintarXiv:2403.17297,2024.
[13] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,and
JingrenZhou. Qwen-VL:Afrontierlargevision-languagemodelwithversatileabilities. arXivpreprint
arXiv:2308.12966,2023.
[14] Anthropic. Introducingthenextgenerationofclaude,2024.
[15] OpenAI. Hellogpt-4o,2024.
[16] ZhangLi,BiaoYang,QiangLiu,ZhiyinMa,ShuoZhang,JingxuYang,YaboSun,YuliangLiu,andXiang
Bai. Monkey:Imageresolutionandtextlabelareimportantthingsforlargemulti-modalmodels. arXiv
preprintarXiv:2311.06607,2023.
[17] HugoLaurençon,LéoTronchon,MatthieuCord,andVictorSanh. Whatmatterswhenbuildingvision-
languagemodels?,2024.
[18] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning. arXivpreprintarXiv:2310.03744,2023.
[19] HaoyuLu,WenLiu,BoZhang,BingxuanWang,KaiDong,BoLiu,JingxiangSun,TongzhengRen,
ZhuoshuLi,YaofengSun,etal. DeepSeek-VL:towardsreal-worldvision-languageunderstanding. arXiv
preprintarXiv:2403.05525,2024.
[20] TianyuYu,HaoyeZhang,YuanYao,YunkaiDang,DaChen,XiaomanLu,GanquCui,TaiwenHe,Zhiyuan
Liu,Tat-SengChua,andMaosongSun. RLAIF-V:Aligningmllmsthroughopen-sourceaifeedbackfor
supergpt-4vtrustworthiness. arXivpreprintarXiv:2405.17220,2024.
10[21] RuyiXu,YuanYao,ZonghaoGuo,JunboCui,ZanlinNi,ChunjiangGe,Tat-SengChua,ZhiyuanLiu,and
GaoHuang. LLaVA-UHD:anlmmperceivinganyaspectratioandhigh-resolutionimages. arXivpreprint
arXiv:2403.11703,2024.
[22] AlexYoung,BeiChen,ChaoLi,ChengenHuang,GeZhang,GuanweiZhang,HengLi,JiangchengZhu,
JianqunChen,JingChang,etal. Yi:Openfoundationmodelsby01.ai. arXivpreprintarXiv:2403.04652,
2024.
[23] ZheChen,WeiyunWang,HaoTian,ShenglongYe,ZhangweiGao,ErfeiCui,WenwenTong,Kongzhi
Hu,JiapengLuo,ZhengMa,etal. Howfararewetogpt-4v?closingthegaptocommercialmultimodal
modelswithopen-sourcesuites. arXivpreprintarXiv:2404.16821,2024.
[24] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee. Llava-next:
Improvedreasoning,ocr,andworldknowledge,January2024.
[25] QwenTeam. Introducingqwen1.5,February2024.
[26] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,Jiaqi
Wang,ConghuiHe,ZiweiLiu,etal. MMbench:Isyourmulti-modalmodelanall-aroundplayer? arXiv
preprintarXiv:2307.06281,2023.
[27] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,Dongfu
Jiang,WeimingRen,YuxuanSun,etal. MMMU:Amassivemulti-disciplinemultimodalunderstanding
andreasoningbenchmarkforexpertagi. arXivpreprintarXiv:2311.16502,2023.
[28] AniruddhaKembhavi,MikeSalvato,EricKolve,MinjoonSeo,HannanehHajishirzi,andAliFarhadi. A
diagramisworthadozenimages. InECCV,2016.
[29] FuxiaoLiu,TianruiGuan,ZongxiaLi,LichangChen,YaserYacoob,DineshManocha,andTianyiZhou.
Hallusionbench: You see what you think? or you think what you see? an image-context reasoning
benchmarkchallengingforgpt-4v(ision),llava-1.5,andothermulti-modalitymodels. arXivpreprint
arXiv:2310.14566,2023.
[30] ZiyuLiu,ZeyiSun,YuhangZang,WeiLi,PanZhang,XiaoyiDong,YuanjunXiong,DahuaLin,and
Jiaqi Wang. Rar: Retrieving and ranking augmented mllms for visual recognition. arXiv preprint
arXiv:2403.13805,2024.
[31] XiaoyiDong,PanZhang,YuhangZang,YuhangCao,BinWang,LinkeOuyang,SongyangZhang,Haodong
Duan,WenweiZhang,YiningLi,etal. Internlm-xcomposer2-4khd:Apioneeringlargevision-language
modelhandlingresolutionsfrom336pixelsto4khd. arXivpreprintarXiv:2404.06512,2024.
[32] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,2024.
[33] ShukangYin, ChaoyouFu, SiruiZhao, KeLi, XingSun, TongXu, andEnhongChen. Asurveyon
multimodallargelanguagemodels. arXivpreprintarXiv:2306.13549,2023.
[34] BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan.SEED-Bench:Benchmarking
multimodalllmswithgenerativecomprehension. arXivpreprintarXiv:2307.16125,2023.
[35] BohaoLi,YuyingGe,YixiaoGe,GuangzhiWang,RuiWang,RuimaoZhang,andYingShan. SEED-
Bench-2:Benchmarkingmultimodallargelanguagemodels. arXivpreprintarXiv:2311.17092,2023.
[36] PengXu,WenqiShao,KaipengZhang,PengGao,ShuoLiu,MengLei,FanqingMeng,SiyuanHuang,
YuQiao,andPingLuo. LVLM-eHub:Acomprehensiveevaluationbenchmarkforlargevision-language
models. arXivpreprintarXiv:2306.09265,2023.
[37] KainingYing,FanqingMeng,JinWang,ZhiqianLi,HanLin,YueYang,HaoZhang,WenboZhang,
YuqiLin,ShuoLiu,etal. MMT-bench: Acomprehensivemultimodalbenchmarkforevaluatinglarge
vision-languagemodelstowardsmultitaskagi. arXivpreprintarXiv:2404.16006,2024.
[38] PanLu,SwaroopMishra,TonyXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,OyvindTafjord,Peter
Clark,andAshwinKalyan.Learntoexplain:Multimodalreasoningviathoughtchainsforsciencequestion
answering. InNeurIPS,2022.
[39] YuliangLiu,ZhangLi,HongliangLi,WenwenYu,MingxinHuang,DezhiPeng,MingyuLiu,Mingrui
Chen,ChunyuanLi,LianwenJin,etal. Onthehiddenmysteryofocrinlargemultimodalmodels. arXiv
preprintarXiv:2305.07895,2023.
11[40] HaoningWu,HanweiZhu,ZichengZhang,ErliZhang,ChaofengChen,LiangLiao,ChunyiLi,Annan
Wang,WenxiuSun,QiongYan,etal. Towardsopen-endedvisualqualitycomparison. arXivpreprint
arXiv:2402.16641,2024.
[41] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,PercyLiang,
andTatsunoriB.Hashimoto. StanfordAlpaca:Aninstruction-followingllamamodel. https://github.
com/tatsu-lab/stanford_alpaca,2023.
[42] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
Zhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna: Anopen-source
chatbotimpressinggpt-4with90%*chatgptquality,March2023.
[43] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,Boyang
Li,PascaleNFung,andStevenHoi. InstructBLIP:Towardsgeneral-purposevision-languagemodelswith
instructiontuning. InNeurIPS,2024.
[44] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning large
multi-modalmodelwithrobustinstructiontuning. arXivpreprintarXiv:2306.14565,2023.
[45] Bo Zhao, Boya Wu, and Tiejun Huang. SVIT: Scaling up visual instruction tuning. arXiv preprint
arXiv:2307.04087,2023.
[46] LinChen,JisongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahuaLin.
ShareGPT4V:Improvinglargemulti-modalmodelswithbettercaptions. arXivpreprintarXiv:2311.12793,
2023.
[47] Jiabo Ye, Anwen Hu, Haiyang Xu, Qinghao Ye, Ming Yan, Yuhao Dan, Chenlin Zhao, Guohai Xu,
ChenliangLi,JunfengTian,etal. mPLUG-DocOwl:Modularizedmultimodallargelanguagemodelfor
documentunderstanding. arXivpreprintarXiv:2307.02499,2023.
[48] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFahadShahbazKhan. Video-ChatGPT:Towards
detailedvideounderstandingvialargevisionandlanguagemodels. arXivpreprintarXiv:2306.05424,
2023.
12SupplementaryMaterial
Inthissupplementarymaterial,weofferfurtherdetailsregardingtheproposedMMDUandMMDU-
45k,alongwithadditionalexperimentaldiscussionsaimedatcomprehensivebenchmarking. Specifi-
cally,SectionAincludesourprojectURLandbenchmarkdownloadURL.SectionsBandCdelve
into the specifics of MMDU and MMDU-45k, respectively. Our evaluation section D provides
in-depthanalysisanddiscussion. SectionEprovidesthedatasheetsforMMDUandMMDU-45k.
A Open-sourceLinks
All data from our MMDU and MMDU-45k are now available for viewing or download via the
followingURLs:
• Projectpage: https://liuziyu77.github.io/MMDU/
• GitHubrepository: https://github.com/Liuziyu77/MMDU
• MMDUbenchmark: https://huggingface.co/datasets/laolao77/MMDU
• MMDU-45kinstructiontuningdataset: https://huggingface.co/datasets/laolao77/MMDU
• URLtoCroissantmetadata: https://huggingface.co/datasets/laolao77/MMDU
B MoreDetailsofMMDU
WepresentthedetailsofourMMDU,encompassingthepipelineofourdatacluster, theprompt
designedfordialoguegeneration,visualizationsofourgeneratedexamples,andacomprehensive
comparisonbetweenourMMDUandexistingbenchmarks.
B.1 Datapreparation
Inthissection,weprovideadetailedexplanationofhowtousedatafromWikipedia[5]toconstruct
MMDU.AsshowninFig. 6,foraWikipediaentry,wefirstobtaintheentry’simages,imagecaptions,
maincontent,andcategories. Theprimaryfunctionofcaptionsandcategories(tags)istoclusterthe
entries.Thecaptions,maincontentoftheentries,andimagesaremainlyusedtogeneratemulti-image,
multi-rounddialogues.
Fig. 7illustrateshowweusecaptionsandcategories(tags)forclusteringtheentries. Subsequently,as
showninFig.2inthemaintext,themaincontentoftheentryisprocessedbyInternLM-chat-20B[12]
togenerateasummaryoftheentry. Thissummary,alongwiththeimagecaptionsandimages,are
theninputintoGPT-4otogeneratemulti-image,multi-rounddialoguecontent.
B.2 PromptofDialoguegeneration
To use the images and content clustered in Fig. 7 effectively, we design a prompt, illustrated in
Fig. 8,toassistGPT-4oincraftingmulti-turnquestions. Firstly,ourpromptincorporatesboththe
imageanditsaccompanyingcontent,facilitatingGPT-4otoposeinsightfulandpertinentquestions
pertainingtotheimage’stheme. WeemployGPT-4otogenerateamultitudeofquestionscentered
aroundtheimagetheme,drawingfromboththeimagesandtheirtextualcontext. Secondly,toensure
thewideusabilityofthedata,weavoidprovidingtextualcueswhengeneratinganswers. Instead,
wetaskGPT-4owithcomprehendingandaddressingmultiplequestionssolelybasedontheimages
themselvesandtheirinterrelations. Thisapproachyieldsmulti-turnquestionsandanswersthatevince
aprofoundgraspoftheimagesandareapplicableacrossvariouscontexts.
Specifically,inFig. 8,theupperpartillustratesthepromptforquestiongeneration. Inthe“Role
Setting”segment,weinstructGPT-4otoassumetheroleofa“question-designexpert”,taskedwith
formulatingquestionsinspiredbyavarietyofimagesandmaterials. Tofosterdepthandbreadth,
wedelineatethecontentparametersinthe“KeyRequirements”segment,encompassingdescriptors,
comparisons, social and cultural contexts, historical significance, emotional nuances, symbolic
interpretations,andrelationalinquiries.
13Data Source
Image:
For creating Q&A.
Image Caption：
Encoded for clustering.
Content：
Generate a summary to prepare
materials for creating Q&A.
Categories:
Encoded for clustering.
Figure6: UsageofWikipediainformation. WeprimarilyuseWikipedia’simages,captions,content,
andcategories.
......
Tag}s & -0.0278 0.0283 0.0052
WikiTree Embedding ......
0.0533 0.0177 0.0098
Sentence
Transfomer -0.0730 0.0472 0.0379 ......
...
Cluster
Im}age -0.0457 0.0159 0.0554 ......
Caption
Figure7: Clusteringpipeline. WeuseclusteringmethodstoprocessWikipediaentries,grouping
togetherentrieswithhighrelevance.
The prompt for generating answers is depicted in the lower part of Fig. 8. Similarly guided by
the“RoleSetting”segment,GPT-4oisrequiredtoserveasan“answerexpert”andrespondtothe
generatedquestionsbasedsolelyontheimages. Asthereisnorelianceonhighlyspecifictextual
knowledge, thecontentofGPT-4o’sanswerswilltendtobemoregeneralized. Additionally, we
employ“<image-i>”inboththegeneratedquestionsandanswerstodenotethepositionoftheimage,
allowingfortherearrangementofimagepositionsbysubstituting“i”. Thistheoreticallyallowsfor
thegenerationofconversationsthatincludemoreimagesandlongersequencesofquestionsand
answers.
B.3 ExampleVisualizationofMMDU
In this section, we illustrate several examples to qualitatively assess the quality of our MMDU.
Example(a)ofFig. 9illustratesnumerousturnsofquestionsandanswers. Question1andQuestion
14# Role Setting
You are a question-design expert specializing in creating questions based on different images and
materials. Your task is to design multiple relatively simple questions based on two images, their
corresponding captions, and supplementary materials provided by the user.
## Here are the image captions and supplementary materials you need to refer to:
Image caption1:
{image caption1}
Supplementary material1:
{supplementary material1}
Image caption2:
{image caption2}
Supplementary material2:
{supplementary material2}
......
## Key Requirements
- The questions must not reveal the specific content of the images and materials; instead, use "<image-
1>" and "<image-2>" to refer to them.
- Based on the image content, you can design questions from the following perspectives: description of
the image content, comparison of the image content, social, cultural, or historical significance of the
image, emotional expression in the image, symbolic meaning of elements in the image, interpretation of
actions or activities in the image, and relationships between objects or people in the image.
- Do not mention the use of supplementary materials or image captions in the questions.
- Ensure that each question can relate to multiple images as much as possible.
- Add a "Q" before each question.
# Role Setting
You are an answer expert, specializing in answering questions based on images. Your task is to answer
multiple questions based on the images provided by the user.
## Key Requirements
- You need to provide detailed answers to each question based on the given images.
- Add a "**A**" before each answer and use "**A1**", "**A2**" etc., to number the answers.
- You can use your own knowledge to answer the questions, providing rich and in-depth answers.
- Your answers should be very detailed, as rich and lengthy as possible, with each answer being 500
words long.
- Use "<image-1>" and "<image-2>" to refer to the two images.
- Do not repeat the question; just provide the answer directly.
## Here are the questions to be answered:
{questions}
Figure8: Dialoguegenerationprompt. Here,wepresentapromptforgeneratingadialoguebased
ontwoimages.
4pertaintothecontentofimages1and2,respectively,whileQuestion15revisitsthecontentofboth
images,challengingLVLMs’long-textcomprehensionandmemorycapabilities.
As depicted in Example (b) of Fig. 10, Question 1 entails analyzing five images. We begin by
identifyingthepositionofeachimageusingtheplaceholder“<image-i>”andsubsequentlyinsert
the corresponding images marked as “<ImageHere>”. The task in Question 1 entails providing
detaileddescriptionsforeachimage. Forinstance,image1depictsa“recreationalpowerboatona
trailer”,image2showsa“traditionalAdmiraltyorFisherman’sanchor”,image3featuresa“military
navalshipunderway”,image4displaysa“lifebuoy”,andimage5portraysa“viewwhereaharbor
tugboatassistsamuchlargerpassengership”. Question2andQuestion3expandtheinquirywith
3additionalimageseach,employingthesamewaytointroducethem. Question2delvesintothe
practicalapplicationsofthesubjectswithintheimages,whileQuestion3explorestheirapplicable
environments. Bothquestionselicitdetaileddescriptionsintheresponses. Thesequestions,along
withtheircomprehensiveanswers,serveasvaluablereferencesforevaluatingLVLMs’comprehension
ofimagecontentandtheirinterrelations.
15Image1
### Question1:
Image1: <ImageHere>. What ancient cultural practice is reflected in the remains depicted in the Image1?
### Assistant:
The remains depicted in the Image1 reflect the ancient cultural practice of Roman public bathing. The structure shown appears to be part of a Roman bath, a communal facility
integral to the daily life of Romans of all social classes. The Roman baths, or "thermae" and "balnea," served not just as places for bathing but as social hubs where people
gathered to relax, exercise, and engage in conversation. Roman public baths were characterized by their sophisticated architecture and elaborate design. They typically consisted
of multiple rooms with varying temperatures: the "caldarium" (hot bath), the "tepidarium" (warm bath), and the "frigidarium" (cold bath). These facilities were equipped with
advanced engineering marvels, such as hypocausts (underfloor heating systems), to maintain the desired temperatures. The remains in the image indicate substantial, thick walls
that may have been part of the extensive heating system essential to the functioning of these baths. Moreover, Roman baths often included spaces for exercise (the "palaestra"),
changing rooms (the "apodyterium"), and sometimes even libraries and meeting rooms. This illustrates the multifunctional nature of these facilities, which extended beyond mere
hygiene to encompass various aspects of social and cultural life. The frequent use of beautiful mosaics, marble, and sophisticated architectural elements in these constructions
underscored their importance within Roman society and demonstrated the Romans' dedication to public welfare and communal activities. In conclusion, the remains depicted in
the image highlight the significance of communal baths in Roman culture. These facilities were not only essential for personal cleanliness but also pivotal in fostering community
interaction and embodying the technological and architectural advancements of ancient Rome.
Image2
### Question4:
Image2: <ImageHere>. What architectural style is prominently featured in the buildings as seen in the Image2?
### Assistant:
The architectural style prominently featured in the buildings as seen in the Image2 is Gothic. This is evident through several key characteristics observable in the structure. Gothic
architecture is known for its height, light, and detailed ornamentation, all of which can be seen in the depicted building. One of the most recognizable features of Gothic
architecture is the pointed arches, which are prominently visible in the image. These arches not only add to the aesthetic beauty of the building but also distribute weight more
efficiently, allowing for the construction of taller and more slender structures. The picture shows a series of these pointed arches, particularly noticeable in the windows along the
facade. Another hallmark of Gothic architecture that is evident in the Image2 is the extensive use of large stained glass windows. These windows not only illuminate the interior
with natural light but also often depict biblical scenes and various motifs, adding both to the beauty and the spiritual ambiance of the place. The image shows a facade replete
with large windows, intricately designed, likely featuring stained glass. Furthermore, the presence of flying buttresses might be inferred. While not explicitly visible in the image,
flying buttresses are often a structural necessity in Gothic architecture, helping to support the weight of the tall walls and roofs by transferring force outward to separate piers. The
verticality of the building is emphasized through its towering spires, which reach skywards. These spires are both decorative and functional, contributing to the overall aesthetic
and directing structural stress downwards. Lastly, the presence of intricate stone carvings and statues is notable. Gothic architecture often features detailed sculptures, many of
which are religious in nature, adorning the exterior of the building. This can be seen in the array of statues and ornate detailing present around the facade and the spires. In
summary, the Gothic style is evidenced through its pointed arches, large stained glass windows, vertical emphasis, possibly flying buttresses, and detailed stone carvings, all
prominently visible in the Image2.
### Question15:
Discuss the architectural or artistic elements that make the buildings in Image1 and Image2 significant or unique.
### Assistant:
The architectural or artistic elements that make the buildings in Image1 and Image2 significant or unique highlight the distinct cultures and technological advancements of their
respective eras. The Roman ruins in Image1 showcase the practical and engineering skills of the Romans. Elements such as the use of concrete, the development of arches and
vaults, and the functional yet aesthetically pleasing designs underscore the Roman ability to create durable and versatile structures. The remnants of intricate stonework and the
layout suggest a civilization that valued both utility and the grandeur of public spaces. The Gothic cathedral in Image2 stands out for its ambitious architectural and artistic
achievements. The use of vertiginous arches, ribbed vaulting, and flying buttresses to create expansive interiors filled with light exemplifies Gothic innovation. Stained glass
windows depicting biblical scenes, intricate sculptures, and detailed stone carvings illustrate the period's dedication to religious storytelling, craftsmanship, and the creation of
spaces that evoke a sense of the divine. These elements highlight the medieval commitment to manifesting spiritual ideals through architecture, aiming to elevate the human soul
towards the heavens.
Figure9: BenchmarkExample(a). Here,wepresenta2-imagebenchmarkmulti-roundQ&Atask.
Duetospacelimitations,weonlydisplaythethreeroundsofdialogue.
B.4 RelatedWorkwithExistingBenchmarks
We conduct discussion with existing benchmarks, including MMVet [4], MMBench [26],
MMMU [27], MMStar [8], MathVista [9], AI2D [28], HallusionBench [29], Chart QA [10] and
ConvBench[2].
MMVetisabenchmarkdesignedtoevaluateLVLMs’abilityoncomplicatedmultimodaltasks. It
contains200images,and218questionswiththecorrespondinganswers,eachsetpertainingtoone
image. OurMMDUoffers421imageswith1645Q&Apairsandaround15turnsofeachQ&Apair
for2-20images.
MMBenchcontainsover3000multiple-choicequestionscovering20differentabilitydimensions,
suchasobjectlocalizationandsocialreasoning,forevaluatingLVLMs. However,multiple-choice
questionsfailtoadequatelyassessthegenerativeandconversationalcapabilitiesofLVLMs.
MMMUincludes11.5Kmeticulouslycollectedmultimodalquestionsfromcollegeexams,quizzes,
andtextbooks,coveringsixcoredisciplines. Thesequestionsspan30subjectsand183subfields,
comprising 30 highly heterogeneous image types, such as charts, diagrams, maps, tables, music
sheets,andchemicalstructures.
16Image1 Image2 Image3 Image4 Image5
### Question1:
Image 1: <ImageHere>. Image 2: <ImageHere>. Image 3: <ImageHere>. Image 4: <ImageHere>. Image 5: <ImageHere>. Describe image 1 to image 5.
### Assistant:
The first image, "image 1," showcases a recreational powerboat on a trailer, which is a typical sight at marinas and boatyards. The boat, mounted on a trailer, indicates that the
owner might use it for leisure activities like fishing, water sports, or just cruising around water bodies. This is evident from the design of the boat, which seems to have a center
console—a common feature that offers a good balance of open deck space and control. The presence of a bimini top, which is the canvas shade affixed on a collapsible frame, offers
protection from the sun and rain, making it more comfortable for day trips. The outboard motor, branded Yamaha, is visible at the stern, suggesting that this boat relies on an
external propulsion system, which is typical for boats of this size. The name "Rock" on the side of the boat could be the vessel's name, which is a custom among boat owners to
personalize their vessels. The setting suggests a coastal or lakeside region where boating is a common recreational activity. Given the boat's condition, it seems well-maintained,
which speaks to the owner's pride and care for their vessel.
The second picture, "image 2," shows an anchor—specifically a traditional Admiralty or Fisherman's anchor—resting on a pavement along a waterfront area surrounded by pebbles.
The anchor's size and placement indicate that it may now be serving as a nautical-themed monument or decoration rather than an actual functional piece. The white paint on the
anchor creates contrast with its backdrop, making it a noticeable feature in its landscape. Anchors like this are historically significant, representing the ancient technology used to
secure vessels to the seafloor. The visible structure and design, with its distinct arms and crown, would have allowed the anchor to dig into the bottom of the sea, effectively holding
a ship in place. This image may be set in a port city or historic area where maritime culture is celebrated or preserved, and such an anchor serves as a reminder of the region's
heritage related to sea travel and industry.
The third image, "image 3," displays a military naval ship underway. The vessel is likely a frigate or destroyer based on its size, streamlined superstructure, and the array of
antennas and radar domes. The gun mounts and other equipment on deck suggest it is designed for surface warfare, air defense, and anti-submarine operations. This type of ship
typically operates as part of a naval fleet, protecting other vessels, and could be equipped with various high-tech systems like missile launchers and advanced communication tools
for military engagement. The presence of the ship in a body of water close to urban areas might suggest it's either departing from or returning to its home port for exercises or
deployment. The vessel’s hull number is usually a clear identifier for the particular ship within its naval force, offering insights into its specific capabilities, history, and crew size.
Image four, "image 4," features a lifebuoy with the inscription "Sit in Breeches," which could be a life-saving apparatus or part of a water rescue kit stationed near a body of water,
such as a pool, marina, or along the coast. Lifebuoys are designed to be thrown to a person in the water to provide buoyancy and prevent drowning, while the individual waits for
further rescue. This device looks quite worn and aged, suggesting it may be a functional relic or a decorative piece with a nod to nautical safety measures. The rope attached to the
lifebuoy is intended to pull a person in distress toward safety. The lifebuoy's location near a pebbled ground and the absence of immediate water in view could indicate that this is
part of a display or a safety station not directly adjacent to water.
The fifth image, "image 5," captures a view where a harbor tugboat assists a much larger passenger ship, likely a cruise ship. Tugboats are powerful for their size and are specially
designed to maneuver other vessels by pushing or towing them. This is crucial in busy or narrow harbors where large ships have difficulty maneuvering due to their size. The
tugboat’s vibrant red color is visually striking and usually signifies utility and visibility in maritime environments. The cruise ship, with multiple decks and rows of windows and
balconies, is where guests enjoy amenities and accommodations while traveling on the open sea. This scene is typical of a port city where ocean liners make stops, allowing
passengers to embark on sightseeing or city tours. The interaction between the tug and the cruise ship underlines the coordinated efforts and skilled maritime operations required for
safe harbor navigation.
Image6 Image7 Image8
### Question2:
Image 6: <ImageHere>. Image 7: <ImageHere>. Image 8: <ImageHere>. Compare the size and potential uses of the watercraft in image 1, image 6, image 7, and image 8. What
activities might each be best suited for?
### Assistant:
The watercraft depicted in image 1, image 6, image 7, and image 8 vary greatly in size and potential uses. image 1 shows a small center-console fishing boat typically used for
nearshore or inland waters. It's well-suited for recreational fishing and small group outings, as its size allows for easy maneuvering in smaller waterways and ease of transport by
trailer.
Image 6 presents an aircraft carrier, a massive naval vessel that serves as a seagoing airbase. These colossal ships are integral parts of a country's naval fleet and are primarily used
for defense purposes, carrying a multitude of aircraft for operations that necessitate both offensive and defensive capabilities. Other potential uses include humanitarian aid and
disaster response due to their ability to operate independently for long periods.
Image 7 depicts a modern aircraft carrier, which, like the one in image 6, operates as a mobile airbase and is central to naval operations in projecting air power worldwide. The
size and structure of these carriers allow them to support and deploy fixed-wing aircraft and helicopters for various missions, including combat, reconnaissance, and supply
operations.
Finally, image 8 shows a group of individuals using a combination of a small wooden platform boat and kayaks, which are geared towards leisure and recreational activities such
as exploration, fitness, and relaxation. These watercraft are best utilized in calm, inland waters, and they allow for a close-to-nature experience.
### Question3:
Based on the type of water bodies that seem to be shown in image 1, image 7, and image 8, describe the different types of environments in which these vessels might operate.
### Assistant:
The water bodies in image 1, image 7, and image 8 showcase different types of environments suitable for various vessel operations. Image 1 appears to be set in a calm marina or
nearshore area, indicating that the small center-console boat is ideal for calm waters, such as lakes, rivers, and protected coastal areas, where its size is sufficient for personal
leisure activities including fishing. Image 7, featuring an aircraft carrier, necessitates deep, open ocean environments, as carriers need extensive space for operational maneuvering
and launching/recovering aircraft. They operate in international waters and near coastlines during times of conflict or tension to exert military presence and engage in training
exercises. Image 8 highlights a peaceful freshwater environment, likely a river or lake. These settings are perfect for kayaks and small boats, where the size and design of these
crafts allow for easy navigation through potentially narrow passages and are best suited for leisurely exploration, wildlife observation, and exercise.
Figure10: BenchmarkExample(b). Here,wepresentamulti-imagebenchmarkmulti-roundQ&A
task. Duetospacelimitations,weonlydisplaythefirstthreeroundsofdialogue.
MMStarisanelitevision-indispensablemulti-modalbenchmarkcomprising1,500samplesmeticu-
louslyselectedbyhumans. MMStarbenchmarks6corecapabilitiesand18detailedaxes,aimingto
evaluateLVLMs’multi-modalcapacitieswithcarefullybalancedandpurifiedsamples.
MathVista is a benchmark designed to combine challenges from diverse mathematical and vi-
sualtasks. Itconsistsof6,141examples,derivedfrom28existingmultimodaldatasetsinvolving
mathematicsand3newlycreateddatasets.
17Figure11: CategorywordcloudofMMDU-45k.
AI2Disadatasetofdiagramswithexhaustiveannotationsofconstituentsandrelationshipsforover
5,000 diagrams and 15,000 questions and answers. It is designed to evaluate LVLMs’ ability to
interpretandreasonaboutintricatediagramswithmeticulousattentiontodetailandclarity.
HallusionBenchisacomprehensivebenchmarkdesignedfortheevaluationofimage-contextreason-
ing,whichcomprises346imagespairedwith1129questions. HallusionBenchprimarilyteststhe
issuesoflanguagehallucinationandvisualillusionpresentinLVLMs.
Chart QA is a large-scale benchmark covering 9.6K human-written questions as well as 23.1K
questionsgeneratedfromhuman-writtenchartsummaries. ItfocusesonassessingLVLMs’abilities
withcharts.
ConvBenchevaluatesmulti-turnconversationsbyassessingperception,reasoning,andcreativity
progressively. Itcomprises577multi-turnconversationsalignedwiththeirrespectivesingleimages.
C MoreDetailsofMMDU-45k
In this section, we provide more detailed information about MMDU-45k. The data construction
methodforMMDU-45kisessentiallythesameasthatoftheMMDU.Thissectionmainlyintroduces
someofthenotablefeaturesofMMDU-45k.
C.1 ThepowerfulscalabilityofMMDU-45k
ThepowerfulscalabilityoftheMMDU-45kdatasetcanbeattributedtothewell-designeddataformat
weimplementedduringitsconstruction. Forallgenerateddata,weusetheidentifier"<image-i>"
to mark the positions and sequences of all images. For data generated in different batches, we
canstackdifferentmulti-image,multi-rounddialoguesbymodifyingthesequenceidentifier"i"in
"<image-i>". Thisallowsustoconstructdialogueswithlongercontextsandmoreimagesaccording
touserrequirementsfordialoguelength.
MMDU-45kactslikeafundamentalbuildingblock,enablinguserstoconstructdialoguesofany
desiredlengthwithouthavingtocollectimagesandtextualinformationfromscratch. Instead,users
canuseMMDU-45kasacomponenttobuildtrainingdataortestquestionstailoredtotheirspecific
needs.
C.2 TheRichnessandDiversityofMMDU-45k
DuringtheconstructionofMMDU-45k,weperformedclusteringondatafromWikipedia. Inthe
clusteringprocess,allWikipediaentrieswerecategorizedintovariousgroups. AsshowninFig. 11,
thesecategoriesincludegeography,history,culture,nature,animals,plants,vehicles,mathematics,
physics,chemistry,andmore. ThisdistributionensuresthattheMMDU-45kdatasethasaverybroad
coverage,encompassingvariousaspectsofdailylife. Consequently,usingMMDU-45kfortraining
allowsthemodeltolearnmoregeneralknowledgeandenhancesitscapabilitiesinlongdialoguesand
multi-imageunderstandingacrossmultipledomains.
18Table 4: Evaluatetheresultsofthefourmodelsusingdifferentjudgmentmodels. WeusedGPT-4o,
GPT-4-turbo,andClaude3-Opustoevaluatetheresultsofthefourmodels.
Models JudgmentModels C R VP LC AA IRU Avg.
GPT-4o 27.8 28.0 33.2 43.0 35.4 31.7 32.2
LLaVa1.5-7B[18] GPT-4-turbo 28.2 27.9 34.2 39.9 34.8 32.3 32.1
Claude3-opus 32.8 32.6 40.6 47.1 41.1 29.0 39.3
GPT-4o 34.3 34.5 36.7 47.2 38.5 35.5 37.2
LLaVa1.5-7B+MMDU-45k GPT-4-turbo 36.2 37.4 39.3 47.4 40.8 38.3 39.1
Claude3-opus 43.0 42.3 51.0 57.0 51.8 37.6 49.3
GPT-4o 58.6 61.5 59.7 75.1 64.1 59.8 62.6
Claude3-Opus[14] GPT-4-turbo 59.9 64.9 63.7 73.5 66.2 63.1 64.5
Claude3-opus 64.7 67.3 74.5 80.1 76.8 60.6 72.7
GPT-4o 62.0 64.2 63.4 78.0 69.0 64.4 66.3
GPT-4-turbo[1] GPT-4-turbo 63.9 67.6 67.7 76.1 70.8 67.0 68.4
Claude3-opus 65.9 67.9 74.5 80.5 76.9 60.9 73.7
GPT-4o 63.7 69.6 66.7 80.6 73.3 68.1 70.2
GPT-4o[15] GPT-4-turbo 64.9 70.7 68.7 77.2 73.2 68.6 70.1
Claude3-opus 67.5 71.9 76.2 82.3 79.4 64.1 75.9
D MoreEvaluationDetails
Inthissection,wepresentacomprehensiveoverviewofourevaluationdetails,includingspecific
judgmentprompts,aswellasquantitativeandqualitativeresults.
D.1 Judgmentprompt
InFig. 12, weillustratethejudgmentpromptemployedtoguideGPT-4oinconductingcompre-
hensiveevaluationsofLVLMresults. Thisprocessinvolvesdelineatingevaluationcriteriaacross
sixdimensions: Creativity,Richness,VisualPerception,LogicalCoherence,AnswerAccuracy,and
ImageRelationshipUnderstanding.Eachdimensionisfinelyscoredonascaleof0to10,withcriteria
setat2-pointintervals,andsupportedbyreferenceanswers. Furthermore,GPT-4oistaskedwith
assigninganOverallScore,alsoat2-pointintervals. Finally,wedividethetotalscorebythenumber
ofquestionsandmultiplyby10toobtainthefinalresult.
Throughthismeticulousguidance,GPT-4ocaneffectivelyevaluateLVLMresultsacrossvarious
dimensions,providingacomprehensiveassessmentprocesstovalidatethesoundnessofitsscoring.
Different Judgement Models. We conduct a comparative analysis of evaluation using GPT-4o,
GPT-4-turboandClaude3-OpusacrossvariousLVLMs,presentedinTab. 4andFig. 13. Fromthe
resultsinthetable,wecanobservethatthescoringtrendsofGPT-4oandGPT-4-turboaresimilar,
withminimaldifferences. ThescoresprovidedbytheClaude3-Opusmodelshowasimilartrend
to those of GPT-4o and GPT-4-turbo but are generally slightly higher. Additionally, for the IRU
(ImageRelationshipUnderstanding)metric,thescoresgivenbyClaude3-Opusaremoreconservative
compared to the other two models, being slightly lower than those of GPT-4o and GPT-4-turbo.
However,thefindingsoverallshowastrongsimilaritybetweentheevaluationoutcomesofGPT-4,
GPT-4Turbo,andClaude3-Opus,highlightingtherobustnessofourproposedjudgmentpromptand
evaluationpipeline.
ConsistencywithHumanScoring. Furthermore,wequantifytheconcordancebetweenthescoring
ofGPT-4-turboandClaude3-Opuswithhumanjudgment. IncontrasttoGPT-4o,whichexhibits
Pearson,Spearman,andKendallsimilaritiesof97.5%,97.3%,and89.0%respectively,Claude3-Opus
andGPT-4-turbodemonstratesPearson,Spearman,andKendallsimilaritiesof(91.4%,92.7%,89.0%)
and(97.2%,97.0%,88.5%),respectively. ThesemetricsindicatethatwhileClaudeandGPT-4-turbo
closelyalignwithhumanscores,itsperformanceslightlytrailsbehindthemorepotentGPT-4o.
D.2 Morecases
ToclarifythetestingandevaluationprocessofMMDU,wedisplaythreequestion-answerpairsfrom
MMDUinFig. 14, 15and16. Duetospacelimitations, wecannotshowacompletemulti-turn,
multi-imageconversation,soweselectedonequestion-answerpairfordemonstration. Eachcase
includestherelevantimagesandthegroundtruth. WealsolisttheresultsofInternLM-Xcomposer2
andInternLM-Xcomposer2+MMDU-45k,showcasingtheeffectivenessofMMDU-45kinimproving
19You are an assistant skilled at evaluating the quality of generative text.
Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant to the user question displayed below. You'll need to assess
the response on the following dimensions: Creativity, Richness, Visual Perception, Logical Coherence, Answer Accuracy and Image Relationship
Understanding. We will provide you with a creative question and the AI model's response and a reference answer for your evaluation. As you begin your
assessment, follow this process:
1. Evaluate the AI model's answers on different dimensions, pointing out its strengths or weaknesses in each dimension and assigning a score of 1 to 10 for
each.
2. Finally, based on the assessments across dimensions, provide an overall score of 1 to 10 for the AI model's response.
3. Your scoring should be as stringent as possible and follow the scoring rules below:
In general, the higher the quality of the model's response and its strict adherence to user needs, the higher the score. Responses that do not meet user needs
will receive lower scores.
Scoring rules:
Creativity:
Scores 1-2 when there is no innovation or uniqueness in the content.
Scores 3-4 when providing partially original content but with low creative quality.
Scores 5-6 when mostly creative but lacks significant novelty, with moderate quality.
Scores 7-8 when having novelty and high-quality content.
Scores 9-10 when highly novel and of exceptional quality compared to the reference answer.
Richness:
Scores 1-2 when lacking depth and breadth, with very limited information.
Scores 3-4 when limited in depth and breadth, with fewer explanations and examples, showing low diversity.
Scores 5-6 when limited in depth and breadth but provides basic necessary information.
Scores 7-8 when providing depth and useful additional information.
Scores 9-10 when providing exceptional depth, breadth, and high diversity compared to the reference answer.
Visual Perception:
Scores 1-2 when the description of the visual information in the image contains errors or is significantly inconsistent with the content of the image.
Scores 3-4 When the description of the visual information in the image reflects only a small amount of the image's information and contains some errors.
Scores 5-6 when the description of the visual information in the image includes the basic information of the image but contains minimal information.
Scores 7-8 when the description of the visual information in the image matches the image well and is rich in content, providing a substantial amount of
information about the image.
Scores 9-10 when the description of the visual information in the image not only matches the image but also is more detailed and informative compared to the
reference answer, providing more information about the image.
Logical Coherence:
Scores 1-2 when entirely incoherent, lacking any logic, and not matching the question or known information.
Scores 3-4 when somewhat coherent but with many logical errors or inconsistencies.
Scores 5-6 when mostly coherent, with few errors, but may struggle to maintain complete coherence in complex situations.
Scores 7-8 when excellent logical handling, very few errors.
Scores 9-10 when flawless logic, impeccable in handling complexity, and significantly higher logical coherence compared to the reference answer.
Answer Accuracy
Scores 1-2 when the answer is significantly inconsistent with the question or contains obvious errors.
Scores 3-4 when the answer is partially correct but contains some errors or is incomplete.
Scores 5-6 when the answer is basically correct but lacks details or is not sufficiently detailed.
Scores 7-8 when the answer is accurate and detailed, fully corresponding to the question.
Scores 9-10 when the answer is not only accurate and detailed but also provides additional useful information, exceeding expectations.
Image Relationship Understanding:
Scores 1-2 when there are significant errors or confusion in distinguishing and describing different images, unable to correctly identify and relate the content
of the images.
Scores 3-4 when the description of different images reflects only minimal distinguishing information, contains some errors and confusion, and fails to clearly
differentiate and relate the images.
Scores 5-6 when the description of different images includes basic distinguishing information, is able to correctly identify and relate the images in a basic
manner, but the information provided is minimal and lacks detail.
Scores 7-8 when the description of different images is accurate and detailed, clearly distinguishing and relating the images, with rich content that points out
the main commonalities and differences between the images.
Scores 9-10 when the description of different images is not only accurate and detailed but also provides richer information and analysis, clearly distinguishing
and relating the images, more comprehensively pointing out the commonalities and differences between the images compared to the reference answer.
Overall Score:
Scores 1-2 when irrelevant to the question, factually incorrect, or generates harmful content.
Scores 3-4 when no serious errors, mostly harmless, but of low quality and does not meet requirements.
Scores 5-6 when basically meeting requirements but performing poorly in some dimensions, with moderate quality.
Scores 7-8 when performing well in all dimensions.
Scores 9-10 when fully addressing user questions and all requirements, significantly surpassing the reference answer.
Please remember, you must evaluate and explain before scoring. After your explanation for each dimension, add the score for that dimension. Finally, at the
end of your response, in the format of the dictionary (including brackets), return all your scoring results, ensuring your scores are integers:
{'Dimension One': Score, 'Dimension Two': Score, ..., 'Overall Score': Score}, for example: {'Creativity': 9, 'Richness': 6, ..., 'Overall Score': 7}.\n
[Question]
Input Question
[The Start of Reference Answer]
Reference Answer
[The End of Reference Answer]
[The Start of Assistant’s Answer]
Assistant’s Answer
[The End of Assistant’s Answer]
Figure12: JudgmentpromptusedtotesttheresultsofGPT-4,GPT-4-Turbo,andClaude3-Opus.
themodel’sabilitytohandlemulti-image,multi-turnconversations. Additionally,weprovidethe
scoringresultsusingGPT-4o,includingthereasonsandgivenscores.
D.3 Finetunedetails
Intheexperimentalsection,wefinetunedtheLLaVa1.5-7BandInternLM-Xcomposer2usingour
MMDU-45k dataset. During the finetuning process, we mixed the llava-665k dataset with the
MMDU-45kdataset. Ourlearningratewassetto2e-4,andweranthetrainingfor1epoch.
20GPT-4-turbo GPT-4o Claude3-Opus
50 60
40 50
40
30
30
20
20
10
10
0 0
C R VP LC AA IRU Avg. C R VP LC AA IRU Avg.
LLaVa1.5-7B LLaVa1.5-7B+MMDU-5k
80 80
70 70
60 60
50 50
40 40
30 30
20 20
10 10
0 C R VP LC AA IRU Avg. 0 C R VP LC AA IRU Avg.
GPT-4-Turbo GPT-4o
80
70
60
50
40
30
20
10
0 C R VP LC AA IRU Avg.
Claude3-Opus
Figure13: JudgmentResults. WeusedGPT-4o, GPT-4-Turbo, andClaude3-Opusasjudgment
modelstotesttheperformanceofLLaVa1.5-7B,LLaVa1.5-7B+MMDU-45k,GPT-4o,GPT-4-Turbo,
andClaude3-OpusonMMDU.
E DatasheetforDatasets
Thefollowingsectioncontainsanswerstoquestionslistedindatasheetsfordatasets.
E.1 Motivation
• Forwhatpurposewasthedatasetcreated?
MMDUandMMDU-45karecreatedtoserveasabenchmarkforevaluateandimprove
LVLM’sabilitiesinmulti-turnandmulti-imageconversations
• Whocreatedthedataset(e.g.,whichteam,researchgroup)andonbehalfofwhichentity
(e.g.,company,institution,organization)?
Theauthorsofthispaper.
• Whofundedthecreationofthedataset? Ifthereisanassociatedgrant,pleaseprovidethe
nameofthegrantorandthegrantnameandnumber.
ThisworkwassupportedbyShanghaiAILab.
E.2 Composition
• Whatdotheinstancesthatcomprisethedatasetrepresent(e.g.,documents,photos,people,
countries)?
MMDU and MMDU-45k consist of multi-image, multi-turn dialogues. Each dialogue
includesadozenorsoroundsofquestionsandanswers,aswellasmultipleimages.
21
serocS
serocS
serocS
serocS
serocSQuestion:
How does the landscape in Image1 differ from that in Image2?
Ground Truth:
The landscapes in Image1 and Image2 exhibit significant contrasts that are noticeable even to a casual observer. Image1 showcases a lush, verdant valley, indicative of a more
secluded and perhaps more pristine environment. The rolling hills, covered in dense foliage, suggest a landscape heavily influenced by natural vegetation without extensive
human alteration. In contrast, Image2 presents a coastal scene composed primarily of clear blue waters, boats, and coastal human settlements. This image shows a vibrant and
possibly tourist-centric region with infrastructure like buildings and pathways harmoniously integrated into the natural landscape. The mood and ambiance of the locations differ
considerably. Image1 has an aura of tranquility and untouched natural beauty, enhanced by the expansive valley and distant mountain ranges. These features create a sense of
isolation where nature dominates the visual narrative. Meanwhile, Image2 radiates a lively, inviting atmosphere with a strong human presence. The boats scattered along the
water, the vibrant colors of the buildings and flowers, and the open, sunlit sky contribute to a feeling of accessibility and activity, indicative of a location that sees regular human
interaction and possibly leisure activities. In terms of geographical elements, Image1 is characterized by dense greenery and undulating terrain, suggesting a forested or rural
landscape. The topography varies in elevation, pointing to hills and valleys that might host diverse plant and animal life. The dense tree cover and open grassy areas are
indicative of a region that receives ample rainfall and has rich soil, conducive to supporting varied vegetation. On the other hand, Image2 focuses heavily on the marine
environment. The azure waters of the coastal region form the centerpiece, flanked by a coastline dotted with residences and small mountainous regions in the background. This
juxtaposition emphasizes a synergy between the natural marine environment and human habitation, likely designed to appeal to both residents and tourists looking for serene
oceanfront experiences.
InternLM-Xcomposer2:
The landscape in Image1 is a lush green valley with rolling hills, while the landscape in Image2 features a blue ocean surrounded by mountains. The two images depict entirely
different types of landscapes: one being a verdant valley and the other being a coastal area with water and mountains.
### Evaluation of the Assistant’s Answer
#### Creativity
The response is direct and lacks any innovative or unique descriptions that would make it stand out. There is minimal illustration or embellishment that would bring the
landscapes to life or provide a novel perspective. **Score: 2**
#### Richness
The answer provides very brief information about the landscapes. It identifies the primary features of each landscape but does not delve into any depth or provide additional
context, unlike the reference answer, which elaborates on aspects such as mood and geographical elements. **Score: 3**
#### Visual Perception
The assistant correctly identifies the fundamental visual elements of each image, such as the lush green valley and rolling hills in Image1 and the blue ocean with surrounding
mountains in Image2. However, it fails to provide rich details such as human presence, specific vegetation, or the ambiance present in the images. **Score: 5**
#### Logical Coherence
The answer is logically coherent and makes sense, stating the main differences in the landscapes in a clear, straightforward manner. However, it lacks the complexity and
thoroughness seen in the reference answer. **Score: 7**
#### Answer Accuracy
The answer is accurate but overly simplistic. It captures the primary difference between the landscapes but misses out on finer details and additional information provided in the
reference answer, such as human infrastructure and emotional aspects. **Score: 6**
#### Image Relationship Understanding
The assistant identifies the basic distinguishing features of the images but does not delve into the richness of their relationships, such as the interaction between nature and human
presence in Image2 or the untouched nature in Image1. The reference answer offers a deeper analysis that is lacking here. **Score: 5**
### Overall Score
The assistant’s response is basic and accurate but lacks the depth, creativity, and richness needed to match the reference answer's quality. It performs adequately but does not
excel in any particular dimension. **Score: 4**
### Summary
{'Creativity': 2, 'Richness': 3, 'Visual Perception': 5, 'Logical Coherence': 7, 'Answer Accuracy': 6, 'Image Relationship Understanding': 5, 'Overall Score': 4}
InternLM-Xcomposer2+MMDU-45k:
The landscape in Image1 differs significantly from that in Image2. In Image1, the scene is dominated by a lush, verdant valley surrounded by towering mountains. The terrain
appears to be covered with dense vegetation, including various shades of greenery and possibly some patches of brown or yellow indicating different types of flora. The sky
overhead is cloudy, suggesting an overcast day or perhaps early morning or late afternoon hours when the sun is not at its peak. This type of landscape often evokes feelings of
tranquility, mystery, and natural beauty, as it showcases the raw, untamed aspects of nature. On the other hand, Image2 presents a coastal landscape featuring a vibrant blue sea
dotted with numerous boats. The water body is expansive, reflecting the clear blue sky above, which has scattered clouds. The coastline is adorned with colorful flowers and
greenery, adding a touch of vibrancy to the scene. Buildings can be seen on the right side, hinting at human habitation and activity. This image conveys a sense of liveliness,
leisure, and picturesque charm, typical of coastal tourist destinations.
### Evaluation of Assistant's Response
#### Creativity
- The assistant's response demonstrates some creativity in comparing the landscapes of both images, using evocative language such as "lush, verdant valley" and "raw, untamed
aspects of nature."
- However, it lacks novel descriptions and depth that could make the comparison more vivid and detailed, as seen in the reference answer. The descriptions are rather
straightforward. **Score: 6**
#### Richness
- The response provides a reasonable amount of detail about each image, describing the landscape, vegetation, sky conditions, and presence of human habitation.
- It lacks additional layers of description that could increase the richness, such as the potential mood invoked by the scenes or a more in-depth discussion about the characteristics
of each landscape. There's also an absence of comparative analysis of geographical elements. **Score: 6**
#### Visual Perception
- The assistant accurately describes the general elements present in both images.
- Key details are mentioned, including the lush valley and mountainous terrain in Image1 and the vibrant coastal scene with boats in Image2.
- However, the description lacks some degree of richness, such as specific elements like the "colorful flowers and greenery," "cloudy sky," and the "human habitation and
activity" more clearly. **Score: 7**
#### Logical Coherence
- The response is logically coherent, with a format that separates the descriptions of the two images and explains the differences in a structured way.
- The flow of information is smooth, and each point made logically follows from the previous one. **Score: 8**
#### Answer Accuracy
- The descriptions are accurate and align with the content of the images, covering the key visual elements and characteristics of the landscapes.
- It correctly identifies the lush, natural setting of the first image and the coastal, human-influenced setting of the second image. **Score: 8**
#### Image Relationship Understanding
- The assistant effectively differentiates between the two images and appreciates the primary contrasts.
- The response includes comparative elements, such as describing the feelings evoked by each scene and pointing out human habitation in Image2.
- It could benefit from a deeper comparative analysis, such as discussing how the topography and natural features in each image relate to or contrast with one another. **Score:
7**
#### Overall Scoring
Based on the individual dimension scores, the assistant's answer is competent and mostly accurate but could be improved in terms of creativity and richness. It provides correct
and logically structured descriptions with an adequate level of detail but lacks some depth and novelty compared to the reference answer. **Overall Score: 7**
{'Creativity': 6, 'Richness': 6, 'Visual Perception': 7, 'Logical Coherence': 8, 'Answer Accuracy': 8, 'Image Relationship Understanding': 7, 'Overall Score': 7}
Figure14: Q&ACase1ofMMDU.Welistthequestions,images,andgroundtruth. Wecompared
theresultsofInternLM-XC2andInternLM-XC2+MMDU-45kandshowcasethescoringresultsand
reasonsforGPT-4o.
• Howmanyinstancesarethereintotal(ofeachtype,ifappropriate)?
MMDU consists of 110 dialogues, comprising 1,645 questions and their corresponding
answers. Additionally,these110dialoguesinMMDUcontain421images. Ontheother
hand,MMDU-45kiscomposedof45,000dialogues,witheachdialogueaveraging3images
and9roundsofquestionsandanswers,totaling410kQ&Apairs.
• Doesthedatasetcontainallpossibleinstancesorisitasample(notnecessarilyrandom)of
instancesfromalargerset?
22Question:
Image 1: <ImageHere>. Image 2: <ImageHere>. Image 3: <ImageHere>. Describe the primary activity or purpose of each animal depicted in Image1, Image2, and Image3
based on their physical traits and behaviors.
Ground Truth:
The primary activity or purpose of each animal depicted in Image1, Image2, and Image3 can be inferred based on their physical traits and behaviors. Starting with Image1, it
features a herding dog, most likely a Border Collie or another similar breed, actively herding a group of sheep. Herding dogs are bred and trained for their exceptional
intelligence, agility, and control over livestock. They are key in managing and directing the movement of sheep, guiding them to desired locations efficiently. The herding
behavior displayed by the dog is instinctual but honed with training, making it invaluable to farming operations. In Image2, the animal depicted is an Equus hemionus or Onager,
which is closely related to donkeys and wild asses. Onagers are known for their resilience and ability to endure harsh environments. They primarily graze on grasses, leaves, and
fruits, using their strong limbs to move across large distances in search of food and water. Their physical traits, such as strong legs and a unique body structure, enable them to
thrive in arid and semi-arid regions, showcasing their evolutionary adaptations for survival. Image3 features horses, domesticated animals with a long history of companionship
and service to humans. Horses have served various purposes throughout history, including transportation, agriculture, and sport. Their strong build, endurance, and speed make
them ideal for riding, pulling carts, and participating in competitive sports. The horses in the image appear to be in a relaxed state, possibly in a paddock, indicating their
domesticated and managed lifestyle.
InternLM-Xcomposer2:
In the image, there are three horses and a dog. The primary activity or purpose of each animal can be described as follows:
1. Horses: Horses are herbivorous animals known for their strength and speed. They are often used for transportation, racing, and various equestrian sports. In this image, the
horses appear to be standing in an enclosed area, possibly a stable or pasture, which suggests they might be kept for leisure, breeding, or training purposes.
2. Dog: Dogs are domesticated mammals that have been selectively bred for various tasks over thousands of years. In this image, the dog is seen near the horses, which could
indicate that it is a herding dog, trained to manage and control livestock like horses. Herding dogs help farmers and ranchers by keeping the animals together and guiding them
in specific directions.
### Evaluation of the Assistant's Answer
#### Creativity:
The AI assistant provides a basic description of the animals' activities or purposes but lacks any innovative or unique approach. The response is straightforward and does not
introduce any new or interesting insights beyond what is expected. Score: 2
#### Richness:
The response offers minimal information about the animals. It covers the basic traits and purposes of horses and the dog but does not go into detail or provide diverse
information about their behaviors, history, or characteristics. It misses out on identifying the second image as an Onager and does not provide enough depth. Score: 2
#### Visual Perception:
The assistant's response does not accurately describe the visual content of the images. Specifically, the assistant fails to identify the Onager (in the second image) and instead
focuses only on horses and a dog. The descriptions provided for the dog and horses are also fairly general and not rich in content. Score: 2
#### Logical Coherence:
Despite the issues in richness and visual perception, the response maintains logical coherence within the scope of the provided information. The sequence of explaining horses
before the dog is logically structured, even though it misses a significant part of the question. Score: 6
#### Answer Accuracy:
The assistant's response accurately describes the potential purpose of horses and the dog in a general sense but fails to identify the Onager and falls short in providing detailed
and accurate explanations for all images. Score: 4
#### Image Relationship Understanding:
The assistant struggles significantly in this dimension as it fails to accurately distinguish the animals in the images. It does not address the Onager in the second image and
instead gives a more generic description of horses. The understanding of how the image contents relate to each other is weak. Score: 2
#### Overall Score:
Given the shortcomings in creative depth, richness, visual accuracy, error in identifying the Onager, and limited understanding of the relationship between the images, the
assistant’s response is below average. Score: 3
### Final Scoring:
{'Creativity': 2, 'Richness': 2, 'Visual Perception': 2, 'Logical Coherence': 6, 'Answer Accuracy': 4, 'Image Relationship Understanding': 2, 'Overall Score': 3}
InternLM-Xcomposer2+MMDU-45k:
In Image1, the animals are dogs and sheep. The dog appears to be a Border Collie, which is known for its herding instincts. Border Collies are highly intelligent and agile,
making them excellent at managing livestock such as sheep. Their primary activity involves directing and controlling the movement of the sheep, ensuring they stay together
and move in the desired direction. This behavior is crucial for farmers who need to keep their flock safe and manageable. The sheep, on the other hand, are likely being herded
by the dog. Sheep are primarily raised for their wool, meat, and milk. They graze on grasses and other vegetation, contributing to their role in agriculture and providing essential
resources for human use. In Image2, the animal depicted is a donkey. Donkeys are often kept for various purposes, including transportation, work, and companionship.
Historically, they have been used to carry loads, pull carts, and even serve as pack animals in remote areas where vehicles may not be practical. The donkey's sturdy build and
endurance make it well-suited for these tasks. Additionally, donkeys are known for their gentle temperament, making them suitable as pets and companions. In Image3, the
animals are horses. Horses are versatile animals that have been domesticated for thousands of years. They are primarily used for riding, racing, and labor. The horse in the
image appears to be a pony, which is a smaller breed of horse. Ponies are often kept for leisure activities such as riding, showing, and sometimes even therapy. They are also
used in agricultural settings for plowing fields and pulling carts.
### Evaluation of Assistant's Answer
#### Creativity:
The assistant's response is fairly creative, especially in how it categorizes the animals and provides context to their traits and behaviors. It slightly extends the answer beyond
simple descriptions by incorporating elements such as the historical use of donkeys and specific activities involving different types of horses. However, it doesn't strike as
exceptionally novel. **Score: 6**
#### Richness:
The assistant's answer lacks some depth and breadth in comparison to the reference answer. For example, while it mentions the sheep being herded, it fails to elaborate on
the roles of sheep beyond their general use. The section about the donkey is somewhat incomplete and doesn't specify its adaptability like in the reference answer, which
mentions survival traits. **Score: 5**
#### Visual Perception:
The assistant correctly identifies the animals and their activities based on the images and provides basic information. However, it doesn't go into as much depth regarding the
visual cues and behaviors as the reference answer does. **Score: 6**
#### Logical Coherence:
The response is mostly logically coherent. Each paragraph is structured to describe the respective image, starting with the physical traits and then moving on to behaviors and
purposes. However, there are a few minor jumps in logic, such as abruptly mentioning donkeys' suitability as pets without much context. **Score: 7**
#### Answer Accuracy:
The assistant makes accurate claims about the animals in the images and their activities or purposes. However, by identifying the animal in Image 2 as a donkey instead of an
Onager, the credibility is slightly detracted from. The reference answer's more precise identification enhances its accuracy. **Score: 5**
#### Image Relationship Understanding:
The assistant partially demonstrates an understanding of the relationship between the images and descriptions but misses some nuances. For instance, it does not clearly
distinguish the Onager from a donkey, which indicates a lack of detail in differentiating the subjects across images. **Score: 5**
### Overall Score:
Given the factors above, the assistant's response is generally adequate but falls short of the depth and clarity provided in the reference answer. **Overall Score: 6**
### Final Scoring
{'Creativity': 6, 'Richness': 5, 'Visual Perception': 6, 'Logical Coherence': 7, 'Answer Accuracy': 5, 'Image Relationship Understanding': 5, 'Overall Score': 6}
Figure15: Q&ACase2ofMMDU.
MMDU-45kisanewdatasetgeneratedusingdatafromWikipedia. MMDU’sdataispartly
asampleselectedfromMMDU-45kandpartlymanuallygenerateddata.
• Whatdatadoeseachinstanceconsistof?
Each instance contains multiple images and rounds of question-answer dialogues. The
numberofimagesandthenumberofQ&Aroundsperinstancearenotconsistent. Each
instanceinMMDUcontains2-20images,whileeachinstanceinMMDU-45kcontains2-5
images. Onaverage,eachinstanceinMMDUcontains15roundsofQ&A,whereaseach
instanceinMMDU-45kcontainsanaverageof9roundsofQ&A.
23Question:
Image 1: <ImageHere>. Image 2: <ImageHere>. Image 3: <ImageHere>. Image 4: <ImageHere>. Image 5: <ImageHere>. Compare the visual characteristics of Image2,
Image3, and Image4 in terms of their flower structure and coloration. How do these variations potentially affect their pollinators?'
Ground Truth:
Image2, Image3, and Image4 display distinctive visual characteristics in their flower structures and coloration, which can significantly affect their interactions with pollinators.
Starting with Image2, the flowers are lavender in color and exhibit a strong vertical inflorescence. The flowers grow in whorls around a long, upright stem. Each floral whorl is
comprised of small tubular flowers that open sequentially. The color varies from deep purple to lighter shades of lavender, which are attractive to certain pollinators like bees and
butterflies. Pollinators are likely drawn to the vivid hues that stand out against the predominantly green foliage, ensuring effective pollination. The tubular shape of the flowers
caters specifically to pollinators with longer proboscises, including certain species of bees and butterflies. In Image3, the floral structure and coloration differ notably. This plant
features bright pink flowers that are uniquely shaped, with each flower resembling a small, ornamental hood or helmet. The flowers grow closely together on a short, dense spike.
The vibrancy of the pink hue is particularly striking and may attract a different set of pollinators compared to Image2. Insects such as bees and hummingbirds, which are attracted
to brightly colored flowers, are the likely pollinators. The compact arrangement and shorter inflorescence suggest a design for pollinators that prefer shorter landing platforms or
those that can hover, such as hummingbirds. Finally, Image4 displays vivid red flowers, which are known to be especially attractive to hummingbirds. The inflorescence is a spike
with flowers that are densely packed together. Each flower is tubular, with petals forming a kind of elongated flag-like structure protruding outward. Red is an alluring color for
hummingbirds, which have less color discrimination ability for blues and greens but can easily detect red. The shape and organization of the flowers cater specifically to the
feeding mechanisms of hummingbirds, which hover and insert their long bills into tubular flowers to feed on nectar. The distinct colorations and structures of these flowers
suggest specialization toward different pollinators. Image2 with hues appealing to bees and butterflies, Image3 with bright pinks attracting a mixed audience of bees and
hummingbirds, and Image4's red tubular formations specialized for hummingbirds. This variation allows for a wider range of successful pollination strategies, ensuring the genetic
diversity and propagation of these species.
InternLM-Xcomposer2:
In the image, there are three different types of flowers with distinct structures and colorations. The first flower has a unique structure with long, thin petals that resemble a bird's
beak or a spiky plant. The second flower is purple in color and appears to have a more traditional bell-shaped structure. The third flower is red and also exhibits a bell-shaped
structure. These variations in flower structure and coloration can potentially affect their pollinators differently. Flowers with unique structures like the one resembling a bird's
beak may attract specific types of pollinators, such as birds or insects with specialized mouthparts designed to access these unusual shapes. On the other hand, flowers with more
traditional bell-shaped structures might appeal to a broader range of pollinators, including bees, butterflies, and moths. The colors of the flowers can also play a role in attracting
certain pollinators. For example, purple flowers tend to attract pollinators like bees and hummingbirds, while red flowers often attract butterflies and moths. Therefore, the
variation in flower structure and coloration in these images could influence which pollinators visit each type of flower, ultimately affecting cross-pollination and seed production.
### Evaluation of the Assistant's Answer:
#### Creativity: The provided answer demonstrates some creativity by mentioning different types of pollinators and the potential attraction based on flower structure and color.
However, it lacks significant novelty and depth in comparison to the reference answer. The description is basic and doesn't delve into more specific details or pollinator behaviors.
*Score: 5*
#### Richness: The response touches on key aspects of flower structure and color but lacks depth and breadth. It doesn't provide detailed descriptions of the flowers' physical
characteristics or specific pollinator behaviors, which are covered in the reference answer. There is a need for more elaborate explanations and examples. *Score: 4*
#### Visual Perception: The assistant's visual descriptions are limited and incorrect in some instances. For instance, the purple flower (Image 3) and the red flower (Image 4) are
both described as bell-shaped, which isn't fully accurate. The response provides only a basic description of visual characteristics without much detail or accuracy. *Score: 4*
#### Logical Coherence: The assistant's answer is logically coherent, but it simplifies the problem, grouping flowers inaccurately based on their shapes and colors. *Score: 6*
#### Answer Accuracy: The assistant identifies some correct elements regarding how flower color affects pollinators but several inaccuracies exist within the description of
flower shapes and colors (e.g. misidentifying structures and grouping incorrectly). It also lacks details on specific pollinators and their behaviors. *Score: 5*
#### Image Relationship Understanding: The assistant's answer provides some basic understanding but doesn't effectively distinguish between the different images. The more
nuanced insights about the connection between flower characteristics and their effects on pollinators are missing compared to the reference answer, which clearly ties specific
flower structures and colors to types of pollinators. *Score: 3*
#### Overall Score: While the assistant's response touches on some relevant points, it lacks depth, richness, detailed visual description, and accuracy. It oversimplifies and
misinterprets some visual aspects. *Score: 4*
#### Final Scores: {'Creativity': 5, 'Richness': 4, 'Visual Perception': 4, 'Logical Coherence': 6, 'Answer Accuracy': 5, 'Image Relationship Understanding': 3, 'Overall Score': 4}
InternLM-Xcomposer2+MMDU-45k:
The visual characteristics of the flowers in Image2, Image3, and Image4 exhibit distinct differences in their structure and coloration, which can significantly impact their
pollinators. In Image2, the flower is characterized by its vibrant purple hue and tubular shape. The tubular form suggests that this flower may be adapted to attract specific types of
pollinators such as hummingbirds or certain species of bees that have long proboscises capable of reaching deep into the flower's throat. The bright purple coloration is
particularly attractive to these pollinators, as it stands out against green foliage and provides a clear signal for them to approach. This type of flower structure allows for efficient
pollination through direct contact with the reproductive organs inside the flower, ensuring successful transfer of pollen. Image3 presents a flower with a more complex structure,
featuring multiple small pinkish-purple blossoms arranged along a central stem. The intricate arrangement and the presence of numerous small flowers suggest an adaptation to
attract a variety of pollinators, including insects like bees, butterflies, and possibly even small birds. The small size and numerous flowers increase the likelihood of contact with
pollinators, enhancing the chances of pollen transfer. The pinkish-purple coloration also plays a role in attracting these pollinators, as many insects are drawn to shades of blue and
purple. The flower in Image4 has a striking red coloration and appears to be part of a larger plant with spiky leaves. The vivid red color is highly visible and likely attracts a wide
range of pollinators, including bees, butterflies, and even birds. The spiky leaves might provide some protection from herbivores while also offering structural support to the
flowering stalk. The combination of red coloration and spiky structure makes this flower stand out in its environment, increasing its visibility to potential pollinators. Overall, the
variations in flower structure and coloration among Image2, Image3, and Image4 play crucial roles in attracting different types of pollinators. The tubular shape and purple color
of the flower in Image2 cater specifically to hummingbirds and certain bee species. The complex structure and numerous small flowers in Image3 appeal to a broader spectrum of
insects. Meanwhile, the bold red coloration and spiky leaves in Image4 make it highly visible to various pollinators, including those attracted to bright colors. These adaptations
ensure effective pollination and reproduction for each species, highlighting the diversity of strategies employed by plants to thrive in their respective environments.
### Evaluation of the Assistant's Answer:
#### Creativity: The Assistant’s answer exhibits some creativity, particularly in describing the flower characteristics and pollinator interactions. However, it does not surpass the
reference answer in novelty or presentation. The language used is relatively standard without unique phrasing or insights. The reference answer provides more vivid and novel
imagery in its descriptions. **Score: 5**
#### Richness: The Assistant’s response contains a good level of detail regarding the flower structures and colors. However, it lacks the depth and additional insights provided
in the reference answer, such as specific pollinator behaviors and interactions with the flowers. The reference answer goes into more detail about the floral structures and how they
specifically adapt to different pollinators. **Score: 6**
#### Visual Perception: The Assistant accurately describes the visual elements in the images, identifying colors and flower structures correctly. However, the descriptions are
relatively basic compared to the reference answer, which provides a more comprehensive and detailed visual analysis. **Score: 6**
#### Logical Coherence: The Assistant’s response is logically coherent, with each paragraph addressing a different image and its characteristics. Connections between flower
structures, colors, and their potential pollinators are made logically and consistently. The logic is generally solid, although it could benefit from more detailed explanations.
**Score: 7**
#### Answer Accuracy: The accuracy of the Assistant’s answer is generally good. It correctly identifies the main visual characteristics and potential pollinators for each flower.
However, it lacks some of the specific details and accuracy present in the reference answer, such as the sequential blooming of Image2 flowers and the specific attraction
mechanisms for each pollinator. **Score: 6**
#### Image Relationship Understanding: The Assistant’s response shows a reasonable understanding of the relationships between the images and how their variations might
affect pollinators. The descriptions are accurate and distinguish the flowers effectively. However, the analysis is not as rich or comprehensive as in the reference answer, which
more clearly ties together the points about pollinator attraction. **Score: 7**
#### Overall Score: The Assistant’s answer is competent but lacks the depth, creativity, and richness found in the reference answer. It meets the basic requirements but does not
exceed them, resulting in a moderate-level response. **Score: 6**
#### Final Scores: {'Creativity': 5, 'Richness': 6, 'Visual Perception': 6, 'Logical Coherence': 7, 'Answer Accuracy': 6, 'Image Relationship Understanding': 7, 'Overall Score': 6}
Figure16: Q&ACase3ofMMDU.
• Istherealabelortargetassociatedwitheachinstance?
Yes,inMMDU,multiplequestionswithineachdialoguehavelabelsthathavebeenmanually
checkedandmodified.
• Isanyinformationmissingfromindividualinstances? Ifso,pleaseprovideadescription,
explainingwhythisinformationismissing(e.g.,becauseitwasunavailable). Thisdoesnot
includeintentionallyremovedinformation,butmightinclude,e.g.,redactedtext.
N/A.
24• Arerelationshipsbetweenindividualinstancesmadeexplicit(e.g., users’movieratings,
socialnetworklinks)?
N/A.
• Arethererecommendeddatasplits(e.g.,training,development/validation,testing)?
Yes,MMDU-45kisthetrainingset,whileMMDUisthetestset.
• Arethereanyerrors,sourcesofnoise,orredundanciesinthedataset?
N/A.
• Isthedatasetself-contained,ordoesitlinktoorotherwiserelyonexternalresources(e.g.,
websites,tweets,otherdatasets)?
Thedatasetisself-contained.
• Doesthedatasetcontaindatathatmightbeconsideredconfidential(e.g.,datathatisprotected
by legal privilege or by doctor– patient confidentiality, data that includes the content of
individuals’non-publiccommunications)?
N/A.
• Doesthedatasetcontaindatathat,ifvieweddirectly,mightbeoffensive,insulting,threaten-
ing,ormightotherwisecauseanxiety?
N/A.
• Doesthedatasetrelatetopeople?
Yes.
• Doesthedatasetidentifyanysubpopulations(e.g.,byage,gender)?
N/A.
• Isitpossibletoidentifyindividuals(i.e.,oneormorenaturalpersons),eitherdirectlyor
indirectly(i.e.,incombinationwithotherdata)fromthedataset?
N/A.
• Doesthedatasetcontaindatathatmightbeconsideredsensitiveinanyway(e.g.,datathat
revealsraceorethnicorigins, sexualorientations, religiousbeliefs, politicalopinionsor
unionmemberships,orlocations;financialorhealthdata;biometricorgeneticdata;forms
ofgovernmentidentification,suchassocialsecuritynumbers;criminalhistory)?
N/A.
E.3 CollectionProcess
• Howwasthedataassociatedwitheachinstanceacquired?
Weusedtheopen-sourcedatafromWikipedia,incorporatingitsentries(includingtextand
images),andappliedGPT-4toconstructourowndataset.
• Whatmechanismsorprocedureswereusedtocollectthedata(e.g.,hardwareapparatusesor
sensors,manualhumancuration,softwareprograms,softwareAPIs)?
WeusedentriescollectedfromWikipediaasourdatasourceandthenappliedclustering
methodstoprocessthedata.
• Ifthedatasetisasamplefromalargerset,whatwasthesamplingstrategy(e.g.,deterministic,
probabilisticwithspecificsamplingprobabilities)?
N/A.
• Whowasinvolvedinthedatacollectionprocess(e.g.,students,crowdworkers,contractors)
andhowweretheycompensated(e.g.,howmuchwerecrowdworkerspaid)?
Theco-authorsofthepaperparticipatedinthedatacollection,verification,andmodification
ofMMDU.
• Overwhattimeframewasthedatacollected?
ThedatawascollectedinMayof2024,buttheresultsdonotdependmuchonthedateof
datecollection.
• Wereanyethicalreviewprocessesconducted(e.g.,byaninstitutionalreviewboard)?
N/A.
• Doesthedatasetrelatetopeople?
Yes.
25• Didyoucollectthedatafromtheindividualsinquestiondirectly,orobtainitviathirdparties
orothersources(e.g.,websites)?
Weobtaineddatafromtheopen-sourceWikipedia.
• Weretheindividualsinquestionnotifiedaboutthedatacollection?
Wedidn’tcollectthedatafromtheindividuals.
• Didtheindividualsinquestionconsenttothecollectionanduseoftheirdata?
Wedidn’tcollectthedatafromtheindividuals.
• Ifconsentwasobtained, weretheconsentingindividualsprovidedwithamechanismto
revoketheirconsentinthefutureorforcertainuses?
N/A.
• Hasananalysisofthepotentialimpactofthedatasetanditsuseondatasubjects(e.g.,a
dataprotectionimpactanalysis)beenconducted?
Thedatasetdoesnothaveindividual-specificinformation.
E.4 Preprocessing/cleaning/labeling
• Wasanypreprocessing/cleaning/labelingofthedatadone(e.g.,discretizationorbucketing,
tokenization,part-of-speechtagging,SIFTfeatureextraction,removalofinstances,process-
ingofmissingvalues)?
Weperformedclusteringontheobtaineddata,selectingthosewithhighercoherence. We
alsoremoveddatawithlow-resolutionimages. Furthermore,weconductedmanualchecks
andmodificationsontheMMDUdatatoensureitsquality.
• Was the “raw” data saved in addition to the preprocessed/cleaned/labeled data (e.g., to
supportunanticipatedfutureuses)?
Yes.
• Isthesoftwarethatwasusedtopreprocess/clean/labelthedataavailable?
Preprocessing,cleaning,andlabelingaredoneviaPython.
E.5 Uses
• Hasthedatasetbeenusedforanytasksalready?
No.
• Istherearepositorythatlinkstoanyorallpapersorsystemsthatusethedataset?
No.
• What(other)taskscouldthedatasetbeusedfor?
N/A.
• Is there anything about the composition of the dataset or the way it was collected and
preprocessed/cleaned/labeledthatmightimpactfutureuses?
N/A.
• Aretheretasksforwhichthedatasetshouldnotbeused?
N/A.
E.6 Distribution
• Willthedatasetbedistributedtothirdpartiesoutsideoftheentity(e.g.,company,institution,
organization)onbehalfofwhichthedatasetwascreated?
No.
• Howwillthedatasetwillbedistributed(e.g.,tarballonwebsite,API,GitHub)?
ThedatasetwillbereleasedonHuggingface.
• Whenwillthedatasetbedistributed?
Thedatasetwillbereleasedinmid-June2024.
• Willthedatasetbedistributedunderacopyrightorotherintellectualproperty(IP)license,
and/orunderapplicabletermsofuse(ToU)?
ThedatasetwillbereleasedundertheAttribution-NonCommercial4.0International(CC
BY-NC4.0)license.
26• HaveanythirdpartiesimposedIP-basedorotherrestrictionsonthedataassociatedwiththe
instances?
No.
• Doanyexportcontrolsorotherregulatoryrestrictionsapplytothedatasetortoindividual
instances?
No.
E.7 Maintenance
• Whowillbesupporting/hosting/maintainingthedataset?
Theauthorsofthispaper.
• Howcantheowner/curator/managerofthedatasetbecontacted(e.g.,emailaddress)?
Contactthefirstauthororotherauthors.
• Isthereanerratum?
No.
• Will the dataset be updated (e.g., to correct labeling errors, add new instances, delete
instances)?
Ifanycorrectionisneeded,weplantouploadanewversion.
• If the dataset relates to people, are there applicable limits on the retention of the data
associatedwiththeinstances(e.g.,weretheindividualsinquestiontoldthattheirdatawould
beretainedforafixedperiodoftimeandthendeleted)?
N/A
• Willolderversionsofthedatasetcontinuetobesupported/hosted/maintained?
Yes.
• Ifotherswanttoextend/augment/buildon/contributetothedataset,isthereamechanismfor
themtodoso?
Contacttheauthorsofthepaper.
27