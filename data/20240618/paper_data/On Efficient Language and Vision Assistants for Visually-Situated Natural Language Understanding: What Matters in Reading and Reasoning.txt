On Efficient Language and Vision Assistants for Visually-Situated Natural
Language Understanding: What Matters in Reading and Reasoning
GeewookKim MinjoonSeo
NAVERCloudAI KAISTAI
KAISTAI minjoon@kaist.ac.kr
gwkim.rsrch@gmail.com
Abstract 60
Elva 13B LLaVA-NeXT-13B § 4.5 Elva 7B
55 Elva
E 3.l 8va
B
7B
LLaVA-NeXT-7B
55 44 .. 69
§ 4.4
Recentadvancementsinlanguageandvision 53.5
assistantshaveshowcasedimpressivecapabili- 50 Elva 1B 53 N.2 L eL Xa TV -7A B § 4.3
tiesbutsufferfromalackoftransparency,limit- 45
LLaVA-1.5-13B 50.7
ingbroaderresearchandreproducibility.While 40 LLaVA-1.5-7B
Elva 0.2B
open-sourcemodelshandlegeneralimagetasks
35
effectively,theyfacechallengeswiththehigh 200 400 600 800 1000 1200 1400 1600 1800
Latency (ms/img)
computational demands of complex visually- 60
Elva 13B
situatedtextunderstanding. Suchtasksoften Elva 7B LLaVA-NeXT-13B § 4.1.2
requireincreasedtokeninputsandlargevision 55 Elva 3.8B LLaVA-NeXT-7B
50
modules to harness high-resolution informa- Elva 1B
tion. Striking a balance between model size 45
LLaVA-1.5-13B
anddataimportanceremainsanopenquestion. LLaVA-1.5-7B
40 LLaVA-1.5-7B
Thisstudyaimstoredefinethedesignofvision- Elva 0.2B 34 90 .. 81
§ 4.1.1
language models by identifying key compo- 35 0 5 10 15 20 25 30 35 40 6.5 7.0 7.5 8.0
Memory (GB) Parameters (B)
nents and creating efficient models with con-
strainedinferencecosts.Bystrategicallyformu- Figure 1: Graphical comparison illustrating average
latingdatasets,optimizingvisionmodules,and scoreagainstlatencyandmemoryconsumptionforvari-
enhancingsupervisiontechniques,weachieve ousmodels. Scoresarederivedfromeightbenchmarks:
significantimprovementsininferencethrough- DocVQA(Mathewetal.,2021),ChartQA(Masryetal.,
putwhilemaintaininghighperformance. Ex- 2022),InfographicVQA(Mathewetal.,2022),SEED-
tensive experiments across models ranging IMG (Li et al., 2023), SEED-2-Plus (Li et al., 2024),
from 160M to 13B parameters offer insights MMStar (Chen et al., 2024), ScienceQA (Lu et al.,
intomodeloptimization. Wewillfullyopen- 2022), and HallusionBench (Guan et al., 2024). See
source our codebase, models, and datasets at Section4.2forbenchmarkdetails. ELVA excelswith
https://github.com/naver-ai/elva. highperformance,reducedlatency,andlowermemory
usage. Right: PerformanceimprovementsfromLLaVA
1 Introduction toELVA,achievedthroughstrategiesinSection4.
Recent advancements in integrating Large Lan-
guageModels(LLMs)withcomputervisionhave issues. However, as these models grow in com-
led to the creation of sophisticated Language- plexity, concerns about their reproducibility and
Vision Assistants. These systems are capable of resourceefficiencypersist(Kimetal.,2023;Dong
interpretingtextwithinimages, enablingthemto et al., 2024). Some open-source models provide
excel in complex tasks requiring both visual and only the model weights without comprehensive
textualunderstanding. Notably,modelslikeGPT- specifications, making replication and use more
4(V)(OpenAI,2023)areabletoleverageasingle challenging.
foundation model to handle various text-centric In the fast-evolving realm of Vision-Language
tasks. However,thesemodelsalsofacesignificant Models(VLMs),simplyexpandingmodelsizeand
challengesrelatedtotransparencyandaccessibility, consuming more resources does not necessarily
limitingbroaderutilization. enhance practical utility. It is crucial to strike a
Open-source alternatives such as LLaVA (Liu balancebetweenhighperformanceandresourceef-
etal.,2023c,2024b)haveemergedtoaddressthese ficiencytodemocratizeaccesstoadvancedVLMs.
1
4202
nuJ
71
]VC.sc[
1v32811.6042:viXra
erocS
egarevA
erocS
egarevAParticularly, inferencecostsareasignificantcon- Given A photo of a cake… You can usually…
cernforpractitionersdevelopingreal-worldappli- LM LM
cations. Despite the importance of this balance, MLP MLP Where should
thefundamentalelementsthatcontributetoVLM we buy this?
Enc. Enc.
successremainunderexplored.
: update model
Traditionally, to enhance text comprehension,
: do not update model
manyVLMsincreasetheirmodelresolution,often
Stage1. Alignment Stage2. Visual Instruct Tuning
leadingtolargerandmoreresource-intensivemod-
Figure 2: Training pipeline consists of two stages:
els. In this work, we challenge this approach by
alignment of visual and textual features through the
introducing ELVA (EfficientLanguageandVision
MLP,followedbyjointtrainingoftheLMandtheMLP.
Assistant),asuiteofVLMsdesignedtomaintain
highperformancewhilereducinginferencecosts 2 LargeLanguageandVisionAssistants
and focusing on text-centric tasks. While we do
Architecture. TheLLaVAframework(Figure2)
increasetrainingcoststoamanageableextent,the
employsapre-trainedVisionTransformer(ViT)as
primaryresearchtargetofELVAistocreatemodels
its vision encoder. Input images are resized and
capableofhandlinghigh-resolutiontaskswithlow
divided into patches of size n ˆ pp ˆ p ˆ cq,
inferencecosts. h w
where n “ ph{p qˆpw{p q. These patches are
Ourkeycontributionsareasfollows: h w
processedbytheencodertogenerateembeddings
1. EfficiencyandReproducibility: Wepresent tz i P Rdu,whicharethenadjustedusingamulti-
ELVA, anefficientandscalablemodelarchi- layerperceptron(MLP)forcompatibilitywiththe
tecture trained on open-source data, demon- language model. The AnyRes mechanism (Liu
strating superior reproducibility and cost- et al., 2024b) allows processing of larger images
effectivenessasshowninFigure1. by segmenting them into m parts. Each part is
processedthroughtheViT,localembeddingsare
2. EmpiricalValidation: Weconductthorough
generated,andallareutilized. Thisensurescom-
experiments to validate the effectiveness of
prehensiveunderstandingofvariedimagesizesand
ELVA’sprimarycomponents.
resolutions. However,thisincreasesthetotaltoken
counttonˆm,posingacomputationalchallenge.
3. Model Scalability: We develop ELVA ver-
sions ranging from 0.2B to 13B parameters, Training Objectives and Datasets. LLaVA is
showcasingitsscalabilityandadaptability. trainedtominimizeCross-Entropy(CE)loss. Dur-
ingpre-training,itgeneratescaptionsforimages,
4. DatasetContributions: Toevaluate ELVA as
with CE loss computed on the text. In the fine-
adocumentassistant,weintroducetwonew
tuning stage, given an image, question, and an-
datasets,CORD-InstructandParsing-Bench.
swer from the assistant, the loss is computed on
5. Open-Source Initiative: To foster further theanswertext. TheLLaVA-1.5dataset(Liuetal.,
communityresearchandensuremodelrepro- 2023b)iswidelyusedandthisstudyaimstofurther
ducibility, we will open-source the trained enhancethedataset. MoredetailsareinSection4.
modelsanddatasetsfromthisstudy.
3 EfficiencyChallengesinLLaVAModels
Our ultimate goal is to shed light on the com-
Thissectionaddressescommonoverheadissuesin
plexities of VLMs, helping readers identify the
LLaVAmodels,identifyingcriticallimitationsand
critical factors driving model success while pre-
defining the problem space for future work (Liu
sentingapractical,cost-effectivesolutionwithsig-
etal.,2024b;Dongetal.,2024).
nificant real-world applications. Following this
introduction, §2 provides an overview of founda- 3.1 InferenceOverheadSources
tionalLLaVAmodels;§3discussescomputational
InferenceoverheadinLLaVAmodelsstemsfrom
challenges;§4outlinesourproposedsolutions;§5
severalfactors:
presentsourempiricalresultsandanalysis;§6of-
fers further analysis and ablations; and §7 along • Model Scale: Larger models (e.g., 34B pa-
with §8 survey related works and conclude the rameters)offerenhancedcapabilitiesbutincur
study,respectively. significantcomputationalcosts.
2Model TokenUsage(#tok) s/img Memory(GB) Improvingthesemodelsforspeedandperformance
LLaVA-1.5-7B 576 0.46 15 remainsahigh-impactresearcharea. Formoreon
LLaVA-1.5-13B 576 0.66 27
LLaVA-NeXT-7B approx.1.7–2.9K 1.01 20 currenttrends,seeSection7.
LLaVA-NeXT-13B approx.1.7–2.9K 1.78 40
LLaVA-NeXT-34B approx.1.7–2.9K 4.00 88
4 EfficientLanguageandVisionAssistant
Table1: Inferencelatencyandmemorycostsfordiffer-
entLLaVAmodels. TestedwithNVIDIAV100GPUs. 4.1 Preliminary: BaseArchitecture
ModificationandInitialDataCuration
• Vision Encoder Complexity: Advanced
Toidentifythemosteffectivemodelarchitecture,
image encoders like SO400M and ViT-G
wetestvariousLLMsrangingfrom160Mto13Bas
(1.8B)improveperformancebutincreaseover-
follows: Llama-160M,TinyVicuna-1B,Phi3-3.8B,
head(Sunetal.,2024;Zhaietal.,2023). Vicuna-7B,andVicuna-13B.1
For the vision encoder, we replace the Ope-
• Image Resolution: High resolutions
nAICLIP-Large-336-14module(LLaVA-1.5)with
(e.g., 900px) for detailed visual tasks like
OpenAICLIP-Base-224-32,usingtheAnyrestech-
DocVQA (Mathew et al., 2021) increase
nique to retain resolution while summarizing in-
computationaldemandsonthevisionencoder.
putintosimilartokencount. OpenAICLIP-Large-
• VisionTokenQuantity: Higherresolutions 336-14processesa336x336areainto576tokens;
lead to more vision tokens, increasing the OpenAICLIP-Base-224-32processesa224x224
computationalloadontheLLM(e.g.,LLaVA- area into 49 tokens. Using Anyres, resolution is
NeXTusesupto2880tokens). increasedto896x676pxwithatokencountcapped
at637,comparedtoLLaVA-NeXT’s2880tokens
Higherimageresolutionsandcomplextasksfur-
for672x672px. However,thisresultsinslightper-
therincreasethecomputationaldemandsonvision
formancedegradation(fromLLaVA-1.5-7B’s40.1
encodersandlanguagemodels.
to39.8;see§4.1.1inFigure1).
3.2 BenchmarkingBaselineModels We then expand the dataset to enhance perfor-
Resource usage during inference for LLaVA mancebyincorporatingdatafromLLaVAR(Zhang
and LLaVA-NeXT models was evaluated on the etal.,2023),Idefics2(Laurençonetal.,2024),and
DocVQA and ChartQA (Masry et al., 2022) test several open-source datasets. This involves us-
sets. As shown in Table 1, LLaVA-1.5 models ing 1.1 million samples for alignment tasks and
showedmanageablecomputationalcosts,operable 1millionsamplesforinstructiontuning. Detailed
on a single V100 GPU. However, LLaVA-NeXT datasetquantitiesareprovidedintheAppendix. As
modelswithupto2.9Ktokenspresentedsignificant showninFigure1,thisstrategyissomewhateffec-
challenges. Testing on an NVIDIA V100 32GB tive(39.8to50.7)butdoesnotachievetheperfor-
showed that LLaVA-NeXT-13/34B could not be mancelevelsofLLaVA-NeXT-7Bwhichreaches
accommodated on a single GPU. These findings 53.2(referto§4.1.2inFigure1).
emphasize the challenges of larger models, espe-
4.2 ProblemDefinitionandStrategies
ciallyinresource-constrainedenvironments.
Despitemultipleoptimizations,themodeldisplays
3.3 ExistingApproachestoEfficiency
performanceissues,particularlygeneratinghalluci-
Existing computationally demanding methods nations—incorrectresponsesduetoinherentbias
come with trade-offs. Sampler modules like the rather than accurate visual interpretation. These
Perceiver resampler (Alayrac et al., 2022) re- problemsareevidentintasksrequiringstronginte-
duce token count but are resource-intensive to grationofvisualandtextinformation.
trainandmaystrugglewithgeneratingprecisere-
HypothesizedChallenges. Wehypothesizetwo
sponses (Liu et al., 2024b). Separate models for
mainchallenges: (1)inadequateembeddingsfrom
detailed or concise responses complicate deploy-
ment(Baietal.,2024;Laurençonetal.,2024).
1Llama-160M: https://huggingface.co/Felladrin/
Simpler approaches, such as those based on Llama-160M-Chat-v1,TinyVicuna:https://huggingface.
LLaVA(Liuetal.,2023c),arecompatiblewithli- co/Jiayi-Pan/Tiny-Vicuna-1B, Phi3-3.8B (Abdin
et al., 2024): https://huggingface.co/microsoft/
brarieslikeSGLang(Zhengetal.,2023b)andtrans-
Phi-3-mini-4k-instruct, Vicuna-7/13B (Zheng et al.,
formers(Wolfetal.,2020),facilitatingbroaderuse. 2023a):https://huggingface.co/lmsys
3thevisionencoder,and(2)poorunderstandingof VisionEncoderConfiguration Text-Centric General Overall
basic text comprehension tasks, which is crucial C1.CLIP-B-224-AnyRes(CLIP) 40.3 54.5 47.4
C2.UnfreezeCLIP 34.1 47.6 40.9
forcomplexdocumentinterpretation. C3.REncoder(RE) 45.2 52.2 48.7
C4.Avg(CLIP&RE) 45.6 54.4 50.0
C5.ELVA-encoder(Avg(CLIP&12REs)) 45.7 54.7 50.2
ImprovementStrategies. Toaddressthesechal-
Supplementaryablations
lenges,weimplement: (1)amoreefficientvision C6.CLIP-L-336(LLaVA-1.5onourdata) 37.5 58.6 48.1
C7.CLIP(7%)+REncoder(93%) 45.9 53.5 49.7
encodertoenhancethequalityofembeddings,and
(2)atrainingregimenthatprioritizestextcompre- Table 2: Ablation Study Results for Different Vision
hensionbeforeproceedingtomorecomplextasks. EncoderConfigurations. Averagescoresfortext-centric
To test our hypotheses, we conduct a series of tasks (DocVQA, ChartQA, InfoVQA, and SEED-2-
Plus), and general image tasks (SEED, MMStar, Sci-
comprehensiveablationexperiments. Figure1il-
enceQA,andHallusionBench)arereported. Thesere-
lustrates our development stages. We track the
sultsareobtainedwithPhi-3(3.8B).Theoverallscores
effectiveness of our model modifications using
forotherscales(from1Bto13B)areshowninFigure3.
various text-centric evaluation benchmarks, in-
cluding DocVQA (Doc) (Mathew et al., 2021),
C1 C2 C3 C4 C5 C6 C7
ChartQA(Chart)(Masryetal.,2022),Infograph- 50.851.651.052.152.9
49.450.6
51.253.552.853.853.9 50.352.6
icVQA (Info) (Mathew et al., 2022), and SEED-
43.0
43.945.145.5
42.544.0
2-Plus(SD2P)(Lietal.,2024). Additionally,we 38.4
employ widely-used general LVLM benchmarks Tiny-Vicuna-1B Vicuna-7B Vicuna-13B
suchasSEED-IMG(SD-I)(Lietal.,2023),MM-
Figure3: Performanceofvariousvisionencoderconfig-
Star(MMS)(Chenetal.,2024),ScienceQA-IMG
urationsatscalesof1B,7B,and13B.Averagescores
(SciQA) (Lu et al., 2022), and HallusionBench foreachconfiguration(C1toC7)across8benchmarks.
(Hall)(Guanetal.,2024). Ourprimaryobjective
is to enhance performance on text-focused tasks processes. WhenwetrainaVLMwiththeobtained
whilemaintainingcompetitiveperformanceongen- REncoder,weobservesignificantimprovementsin
eraltasksandensuringlowinferencecosts. text-centrictasks(C3), however, itsperformance
In the followings, we introduce each proposed ongeneralimagetasksstilldeclines.
module in detail and conduct extensive ablation Now we have two expert encoders, the origi-
studies, analyzing the impact of removing each nalCLIPforgeneraltasksandtheREncoder. In-
componentfromthefinalmodelconfiguration. spiredbypriorwork,Modelsoup(Wortsmanetal.,
2022), we test with averaging the weights of the
4.3 DevelopinganEnhancedVisionEncoder
originalencoderandtheREncoder. Surprisingly,
withWeightAveraging
thismethodproducespromisingresults(C4). Ad-
To improve visually-situated NLU, we develop a ditionally, further adjustingthe weight averaging
newvisionencoderoptimizedspecificallyforread- ratiostoleanmoretowardstheREncoderprovides
ing text within images. This encoder is designed marginallybettertext-centricperformance(C7).
toenhanceresolutionandcomputationalefficiency Tofurtherenhancerobustness,wetrain12REn-
whilemanagingcostconstraints. coderswithdifferentrandomseedsandthenaver-
Initially, we found that simply unfreezing the agetheirweights,apracticeinspiredbyWortsman
visionencoderwhiletrainingtheVLM(C2)does et al. (2022). This averaging process, taking ap-
notyieldsignificantimprovementsinperformance. proximately1.7dayson8V100GPUsperphase,
Next, we adopt a two-step approach. (1) We un- yieldsavisionencoderthatsubstantiallyimproves
freeze the vision encoder and train it on a small textcomprehensionwhilemaintainingrobustgen-
VLM (1B scale) using text-centric datasets such eralcapabilities(C5). Theadditionaltrainingde-
as OCR-IDL (Biten et al., 2022). This training tailsareinAppendixD.
focusesona"textreadingtask,"wherethemodel In summary, the core idea is simple. We (1)
is tasked with reading text within images. This unfreeze the encoder and train a small VLM
allows the vision encoder to adapt and enhance fortextreadingtasks,andretrievethespecial-
itstextrecognitioncapabilities. (2)Weextractthe izedencoder,and(2)makeitrobusttovarious
enhancedvisionencoder(REncoder)fromthistext- tasksbyapplyingweightaveraging. Finally,the
centricVLM.Notably,thetext-centricVLMused produced vision encoder is used to build an effi-
forachievingREncoderisnotutilizedfurtherinour cientlanguageandvisionassistant,ELVA. Ournew
4
erocS
egarevAELVA-encoder (C5) brings substantial enhance- 1B 3.8B 7B 13B
Text/Gen/All Text/Gen/All Text/Gen/All Text/Gen/All
ments in text-centric tasks compared to the origi-
R1. 41.6/48.7/45.2 43.7/53.9/48.8 47.5/56.0/51.8 48.5/56.5/52.5
nal base (C1). While there is still a reduction in
R2. 42.4/48.6/45.5 45.7/54.7/50.2 49.2/56.6/52.9 50.4/57.4/53.9
general image performance compared to merely
Supplementaryablations
trainingLLaVA-1.5onourdata(C6),understand- R3. 41.2/46.5/43.9 45.4/53.7/49.6 47.0/55.3/51.2 50.8/56.1/53.5
R4. 42.0/48.8/45.4 44.1/54.2/49.1 48.2/56.0/52.1 49.6/57.6/53.6
ingthetrade-offsinC6iskeytofullyappreciating
the balance we achieve. We effectively balance
Table3: ComparisonofdifferentmodelsizesandRR-
overallperformanceandcomputationalcostwithin Promptvariants. R1representsstandardmodelstrained
the scope of CLIP-Base parameters (88M). The without additional text reading prompt. R2 employs
ELVA-encoderconfigurationsdemonstratenotable explicitinitialtextreadingstepsfortext-richtaks. R3
successoverall,asshowninTable2andFigure3. carriesouttextreadingattheend,whileR4provides
OCRresultsjustascontextwithoutexplicitsupervision.
4.4 AugmentingTextUnderstandingin
ImageswithRead-and-ReasonPrompts 1B-R1 3.8B-R1 7B-R1 13B-R1
1B-R2 3.8B-R2 7B-R2 13B-R2
W (Ve DUe )xp mlo odre elsV ui ts ilu ia zl ingD po ic xu elm -be an st edU lan nd ge ur as gta en mdi on dg
- 37.539.340.640.040.8
50.851.252.252.452.051.9
42.94 +4 1. .4345.74 +6 0. .85 46.04 +6 0. .33
elingtasks,suchasexhaustiveimagetextreading
31.232.235.0 33.133.1 32.23 +2 0. .46
Text-Centric General Overall
duringtraining(Kimetal.,2022;Leeetal.,2023).
ModelslikeLLaVA(Liuetal.,2023c,b)useOCR Figure4: Usinga10%subsetofthedataset,theimpact
topreprocessimages,augmentinguserquerieswith ofintegratingtheRR-Promptduringtrainingisshown.
OCRresults. However,acomprehensiveinvestiga-
tionofmethodsforsupervisingtextualinformation
understandinginimages. Thisapproach,validated
duringvisualinstructiontuningislacking.
throughablationstudies,showssignificantperfor-
Table3showsablationstudiesinvestigatingtext
manceimprovements,especiallyintext-richtasks.
readingtasksduringvisualinstructiontuning. Set
NotethattheRR-Promptisusedduringtraining;
R1followsstandardpracticesusingdatasetswith-
during inference, the model directly engages in
out additional text reading components. In R2,
reasoning,leveragingtheenhancedcapabilitiesac-
inspired by prompting (Brown et al., 2020), we
quiredthroughtheRR-Prompt,ensuringefficiency
incorporateaninitialQAtask,"Whatiswrittenin
withoutneedinganexplicittextreadingstage.
this image?" before QA on text-rich images. For
example,withanimageofarestaurantmenu,the 4.5 BringingItAllTogether
modelfirstreadsalltextbeforequeryingabout
To develop a more robust model capable of han-
menu items or prices. This incremental addition
dlingawiderrangeoftasks,wescaleupthemodel
improvesperformancesignificantlyfromR1toR2,
developmentbyincorporatingdiversedatasetsbe-
especiallyintext-richtasks. Weannotatedatasets
yond merely text-centric tasks. Our final model
usingOCRenginesforthispurpose.
integratesfouradditionaldatasets: Vision-Flan(Xu
Ourfurtherexplorationsassessthisapproachin
etal.,2024),RefCOCO(Kazemzadehetal.,2014),
resource-scarce environments, using 10% of the
VG(Krishnaetal.,2017),andCORD(Parketal.,
original instruction tuning dataset size. Figure 4
2019). Byincorporatingtheseadditionaldatasets,
showsresultsacross1Bto13Bparametermodels.
weaimtoenhanceboththeperformanceandgener-
Wealsoexplorethesupervisionstructure’simpact
alizabilityofourmodel. Thefinaltraininginvolved
bycomparing"ReadandReason"versus"Reason
11K steps with a batch size of 128. The specific
and Read" approaches. R3 models perform text
dataset details and schedules are in the appendix.
readinglasttoevaluatethis. Resultsconfirmthat
As demonstrated in Figure 1, our final configura-
"ReadandReason"ismoreeffective,emphasizing
tionshowssolidperformance.
structuredprompting’simportanceinmodellearn-
ing. Lastly,weevaluatetheeffectofprovidingread
5 ExperimentalAssessment
text as context without explicit supervision (R4).
Explicit supervision with text information yields Inthissection,werigorouslytestandevaluateour
marginalimprovementsintext-centrictasks. ELVA models under varying conditions. We aim
In summary, the proposed core idea is to use tounderstandtheircapabilitiesandlimitationsby
RR-Promptduringmodeltrainingtoenhancetext benchmarkingthemagainstbaselinemodelsacross
5
erocS
egarevA#Param Text-CentricBenchmarks GeneralMultimodalBenchmarks
Model #tok s/img vram
Vision LM Doc Chart Info SD2P PBen SD-I MMS SciQA Hall AI2D Math LBen
LLaVA-v1-13B 300M 13B 576 1.43 26.9 9.8 7.0 19.9 39.5 14.0 51.2 32.9 62.4 43.0 43.9 25.9 69.9
LLaVA-1.5-7B 300M 7B 576 0.46 14.7 22.8 17.8 22.4 41.2 17.9 65.9 33.1 69.2 48.5 55.6 25.6 59.6
LLaVA-1.5-13B 300M 13B 576 0.66 27.0 24.5 18.5 24.9 44.4 19.6 68.2 34.1 72.3 45.7 60.7 27.7 66.1
LLaVA-NeXT-7B 300M 7B 1728-2880 1.01 20.0 68.3 51.9 31.6 51.7 49.6 69.8 38.2 69.0 44.8 66.8 31.8 72.3
LLaVA-NeXT-13B 300M 13B 1728-2880 1.78 40.1 69.8 59.0 34.9 55.6 57.3 71.5 41.2 73.4 46.7 71.7 34.1 72.3
ELVA-0.2B(ours) 88M 0.2B 98-637 0.24 1.4 44.7 50.3 14.8 31.4 12.3 37.8 31.5 39.0 48.1 31.0 27.0 28.4
ELVA-1B(ours) 88M 1B 98-637 0.41 3.3 62.6 57.7 23.7 36.8 27.3 52.3 32.6 63.3 50.4 46.9 31.7 36.0
ELVA-3.8B(ours) 88M 3.8B 98-637 0.45 9.1 66.5 62.5 24.3 43.4 24.4 61.1 35.9 73.4 53.7 64.2 34.9 40.7
ELVA-7B(ours) 88M 7B 98-637 0.54 14.5 69.1 61.8 30.7 47.7 45.0 62.6 35.4 74.7 56.8 66.2 36.6 50.7
ELVA-13B(ours) 88M 13B 98-637 0.72 27.0 71.7 65.2 34.6 52.6 59.2 65.3 37.9 77.7 56.8 69.3 38.1 51.0
Supplementarybaselines
QwenVL-7B 1882M 7B 224 0.50 19.2 65.1 60.2 – 41.0 – 56.5 33.9 60.6 37.4 57.2 15.5 12.9
QwenVL-7B-Chat 1882M 7B 224 0.56 19.2 62.6 49.3 – 46.9 – 62.9 34.0 64.0 40.8 59.7 34.9 67.7
PaliGemma-3B 428M 3B 1024 0.98 10.3 – 33.8 – 49.8 – 70.0 48.6 94.3 53.0 69.3 28.7 36.9
Table4: PerformanceComparisonAcrossDifferentModelsandBenchmarks. Thistablesummarizesmodel
sizes(VisionandLM),tokencounts(#tok),latency(s/img),andmemorycost(vram). Theperformancemetrics
acrossvariousbenchmarksarepresented,showcasingeachmodel’sstrengthsandweaknessesindifferentchallenges.
bothtext-basedandimage-basedtasks. testthemodel’sabilitytoextractinformationfrom
newdocuments,wecreatethisdatasetusing30im-
5.1 Framework
agesfromBrazilianIdentityDocuments(Álysson
Our evaluation process extends beyond our ini- Soaresetal.,2020)andSROIE(Huangetal.,2019),
tial eight datasets, utilized in our ablation stud- whicharenotusedduringtraining.
ies (See §4.2). To further enrich our examina-
5.3 Results
tion,wehaveincludedadditionaldiversedatasets
suchasAI2D(Kembhavietal.,2016),MathVista- Table4presentsadetailedcomparisonofourELVA
TestMini (Lu et al., 2024) (Math), LLaVA- modelsagainstbaselineslike PaliGemma(Beyer
Bench (Liu et al., 2023c) (LBen), along with et al., 2022), QwenVL (Bai et al., 2024),
Parsing-Bench(PBen)proposedinthiswork. and LLaVA across various benchmarks and
tasks. The results, either reproduced or sourced
5.2 GeneratedScenario-BasedBenchmarks
from original papers, have been validated using
In our research, we identify a significant gap in VLMEvalKit(Contributors,2023)andtheofficial
datasetsrepresentingreal-worlduserscenariosfor codebyLiuetal.(2023c).
documentassistants. Toaddressthis,wecreatethe The ELVA models consistently demonstrate
following datasets. These datasets will be open- strongperformanceintext-centricandgeneralim-
source,andmoredetailsareintheappendix: age benchmarks. The ELVA-0.2B model, despite
its smaller parameter count, performs admirably
CORD-Instruct. BuildingontheCORDdataset,
acrossvarioustasks. LargermodelslikeELVA-1B,
which consists of Indonesian receipts and their
ELVA-3.8B,and ELVA-7Bexhibitsuperiorperfor-
JSON annotations, CORD-Instruct provides in-
mance,showcasingthebenefitsofincreasedmodel
structional sets for models to generate outputs in
capacity. Notably,ELVA-7Bstrikesafinebalance
JSON,XML,orMarkdownformats. Weutilizethe
betweensizeandeffectiveness. Ourlargestmodel,
OpenAIGPT-3.5APItocreatetheseinstructional
ELVA-13B,achievestopscoresinbenchmarkssuch
sets,ensuringtheexclusionofanyerraticsamples.
as Doc, Chart, Hall, AI2D, Math, and PBen, un-
Parsing-Bench. InspiredbytheLLaVA-Bench derscoringtheefficacyofscalingmodelcapacity
andLLM-as-a-Judge(Zhengetal.,2023a),wede- forvision-and-languagetasks. Additionally, ELVA-
velop PARSING-BENCH toaddressthelimitations 13Bremainsefficientintermsoflatency,reinforc-
ofexistingbenchmarkslikeLLaVA-Bench,which ingitspracticalityfordiverseapplications.
includelimiteddocument-relatedsamplesanddo AnotablelimitationisobservedintheLLaVA-
not sufficiently reflect real user needs. Figure 6 Bench, where ELVA models underperform com-
presentsexamplecasesandmodelpredictions. To paredtotheLLaVAseries. Giventhisdatasetonly
6Method s/img vram Doc Chart Info SD2P 101
QwenVL LLaVA-1.5 Elva
LLaVA-NeXT-7B 1.01 20.0 68.3 51.9 31.6 51.7 QwenVL-Chat LLaVA-NeXT
–w/max.1728tokens 0.70 17.1 51.7 48.0 27.9 44.9
ELVA-7B(ours) 0.54 14.5 69.1 61.8 30.7 47.7
100
LLaVA-NeXT-13B 1.78 40.1 69.8 59.0 34.9 55.6
–w/max.1728tokens 1.11 30.4 53.9 52.3 30.9 49.2
ELVA-13B(ours) 0.72 27.0 71.7 65.2 34.6 52.6 SEED-IMG ChartQA DocVQA LLaVA-Bench
Table5: Ablationsonreducedvisiontokencounts. Figure5: Latencycomparisonacrossmultipledatasets.
comprises24images,interpretationrequirescau- uationslikeSD-Iarelessindicativeofactualuser
tion. Detailedanalysiswillbeprovidedin§6.3. scenarios, complicating meaningful latency com-
In summary, ELVA models deliver robust per- parisons. Task-orientedbenchmarkslikeDocVQA
formanceacrossawiderangeoftasksandbench- offer more user-centric metrics. Benchmarks re-
marks. Whileincreasedmodelcapacitygenerally quiring longer answers, such as LLaVA-Bench,
enhancesperformance,efficiencyandlatencycon- show inconsistent results due to varied answer
siderations are crucial for practical deployment. lengths across models. Consequently, we focus
Our results underscore ELVA’s proficiency in ef- onChartQAandDocVQAforlatencyassessments
ficiently addressing diverse vision-and-language butalsoincludeSD-IandLLaVA-Benchresultsin
challenges, providing valuable insights into their Figure5. ThesefindingsindicateELVAmaintains
performanceandreliabilityacrossvariouscontexts. commendablelatencyacrossvariedcontexts.
Whilethisstudyprimarilyfocusesoninference
6 FurtherAnalysesandDiscussions
time costs, training costs are equally important
6.1 AblationswithLLaVA-NeXTVariants forpractitioners. Despiteprocessingasubstantial
dataset,ourlightweightvisionencodingfacilitates
Totesttheimpactofreducingthenumberoftokens
hightrainingsamplethroughput. Ourtrainingtime
in LLaVA-NeXT models, we constraine the grid
costs are approximately 1.38 to 1.8 times that of
size,resultinginamaximumtokencountof1728
LLaVA-NeXT,representingareasonabletrade-off
(either 336x672 or 672x336 pixels). This adjust-
given the efficiency and performance gains. Our
ment speeds up processing while allowing us to
approachremainscompetitiveandmoreresource-
evaluateperformancechangesacrossbenchmarks.
efficientcomparedtoseveralcontemporarymodels.
As shown in Table 5, reducing the vision token
Fordetailedanalysis,pleaserefertoAppendixA.
countleadstosignificantperformancedropsacross
Regardingmemoryusage,practicaldeployment
allevaluatedtasks. Forexample,theperformance
oftenuses4-bitquantization(Dettmersetal.,2023),
ofthe13BmodelonDocVQAdecreasesfrom69.8
significantlyreducingmemorycosts. Forexample,
to 53.9 when the token count is restricted, with
theLLaVA-NeXT-13Bmodeloriginallyrequires
similartrendsobservedinothervariants.
two V100 GPUs but can run on a single V100
In contrast, ELVA models demonstrate strong
with quantization, though with increased latency
performance along with improved efficiency in
andperformancedegradation. Despitelimitations,
both speed and memory usage, underlining their
quantization is promising and likely to improve
robustnessinhandlingvision-and-languagetasks
efficiently. Thisanalysishighlightsthetrade-offbe-
with advancements. Our ELVA models, designed
for efficiency, complement these advancements,
tweentokencountandmodelperformance: while
promisingevengreatervaluewhencombinedwith
reducing tokens can enhance computational effi-
quantization techniques. Future work will likely
ciency, it may lead to a compromise in accuracy.
enhancethesemodels’efficacyindeployment.
TheELVAmodelseffectivelybalanceperformance
andefficiency,outperforminglargerLLaVA-NeXT
6.3 CaseStudyonLLM-as-a-JudgeSets
configurationsevenwithreducedtokencounts.
ELVAmodelsperformrobustlyacrossbenchmarks,
6.2 DiscussiononMemoryandTimeCosts
excelling in text-centric tasks, but show perfor-
We evaluate latency in seconds per sample for mancedipsonLLaVA-Bench. Theseissuesoften
benchmarkslikeChartQAandDocVQA,asthese stemfromfailuresinentityrecognition,resulting
tasksrelatecloselytoreal-worlddocumentinfor- in logically correct but penalized responses. For
mationextractionscenarios. Multiple-choiceeval- example, when asked to name an anime charac-
7
)ti/s(
ycnetaLWhat is the problem this city might be facing?
Traffic congestion.
What are some possible solutions? The image shows a very long line of
cars on a highway, indicating that the
city is experiencing significant traffic
congestions. This can lead to
increased travel times, air pollution,
and frustration for drivers. Possible
solutions to this problem include:
1. Encouraging public transportation:
By improving and promoting public
transport options like buses, trains,
(...)
What is the name of the character? The name of the character is not visible Method Doc Chart Info SD2P
in the image. The image shows a LLaVA-NeXT-7B 74.5(↑6.2) 53.7(↑1.8) 35.5(↑3.9) 55.3(↑3.6)
character from an anime or manga, but
the name of that character is ELVA-7B(ours) 77.8(↑8.7) 64.0(↑2.2) 39.5(↑8.8) 55.7(↑8.0)
unfortunately not visible. The image only
shows the character's appearance, LLaVA-NeXT-13B 76.5(↑6.7) 62.5(↑3.5) 40.4(↑5.5) 58.9(↑3.3)
which includes a boy with brown hair, ELVA-13B(ours) 81.1(↑9.4) 67.5(↑2.3) 44.8(↑10.2) 60.6(↑8.0)
blue eyes, and a red bowtie (...)
Elva received 5/10
Table6: PerformanceimprovementwithOCR.
What is the name of the man in the photo?
I'm sorry, I cannot provide the name of the man in the
photo. (...) the image does not provide any identifiable
information about the man's name or any context that
would allow me to provide it. The focus of the image is areobserved,particularlybenefiting ELVA. How-
on the man and the teddy animal, and there is no visible
text or context that could help identify the man. ever,OCRprocessinghascosts. UsingtheCLOVA
Elva received 2/10 OCR API2, our tests on the DocVQA dataset av-
Can you summarize the purchased item information, including the name, price,  erage about 4 seconds per sample. Faster OCR
and quantity, in XML format?
enginesexistbutoftenattheexpenseofaccuracy.
Certainly, here is the purchased item information in XML format:
```xml Additionally,upscalingVLMstohandleveryhigh
<purchased_items>
<item> resolutions (e.g., 4K, 8K) may not be practical.
<name>GREEN TEA</name>
<quantity>1</quantity> Thus,leveragingOCRandsimilartoolsremainsa
<price>3.00</price>
</item> valuableareaofexploration,aimingtobalancespe-
<item>
<name>GREEN TEA</name> cializedtoolsandVLMsforoptimalperformance.
<quantity>1</quantity>
<price>3.00</price>
</item>
<item> 7 RelatedWork
<name>BUFFET DINNER (ADULT)</name>
<quantity>3</quantity>
<price>49.90</price> Visually-SituatedNLUhasevolvedsignificantly,
</item>
</purchased_items> starting with heavy reliance on OCR for text ex-
```Elva received 9/10
traction. AmajoradvancementwasmarkedbyXu
Figure6: ResultsonLLaVA-BenchandParsing-Bench. et al. (2020), integrating textual and layout infor-
mationforsophisticateddocumentunderstanding.
The field progressed to OCR-free models (Kim
ter, ELVA scores 5/10 for failing to provide the
et al., 2022; Lee et al., 2023), enhancing flexibil-
correct name despite a detailed description. Sim-
ity and accuracy. Further, LVLMs aim to jointly
ilarly, it scores only 2/10 in identifying a person
process visual and textual data, enhancing cross-
inaphotowithouttextualclues. Thesedipshigh-
modal understanding. LLaVA (Liu et al., 2023c)
lightlimitationsinthe ELVA-encoder’s(88M)en-
demonstrateefficientcross-modalperformance. In
tity memorization capacity and the lack of con-
VDU, Kim et al. (2023) use a hybrid approach,
textual data within images. Adding targeted data
balancing OCR-free and OCR-dependent modes.
duringinstruct-tuningmightaddresstheseissues,
More recently, high-resolution processing is cen-
thoughfeasibilityisdebatable. Ontheotherhands,
traltomodels(Liuetal.,2024b;Dongetal.,2024),
ELVAexcelsininformationcomprehensiontasks,
which achieve superior results despite computa-
suchassummarizingreceiptdataintoXMLformat,
tionaldemands. Foradditionalcomparisonswith
scoring 9/10. This shows its ability to accurately
recentmodels,refertoAppendixA.
read,comprehend,andorganizetextinformation.
WhileELVAisefficient,itremainscompetitiveto 8 Conclusion
othermodelsratherthansignificantlysuperior. Fu-
ture research may focus on enhancing the vision This study introduces ELVA, a robust and effi-
encoderandimprovingcontextualunderstanding cient model framework excelling in text-centric
toovercometheselimitations. andvisualtasks. Ourempiricalstudiesshow ELVA
modelsconsistentlyoutperformexistingbaselines,
6.4 DiscussiononLeveragingOCR achievingnotableefficiencyinmemoryusageand
latency. Challengesinentityrecognitionandcon-
Incorporating OCR can be effective for handling
textualunderstandinghighlightareasforimprove-
text-rich high-resolution images. For instance,
ment. Future research should focus on refining
Kim et al. (2023) utilize OCR in their VLM for
theseaspectstoenhanceELVA’sperformanceand
VDUtasks,suggestingitspotentialformodelslike
robustness,ultimatelysolidifyingitspositionasa
LLaVAor ELVA. WhenOCRoutputsareincorpo-
leadingsolutioninvisually-situatedNLU.
rated as contextual information during inference,
asdemonstratedinTable6,notableenhancements 2https://clova.ai/ocr/en
89 Limitations meticulousproofreadingofthismanuscript. Their
input greatly increased its clarity and coherence.
Despitethesignificantadvancementsdemonstrated
Wearealsoimmenselygratefultoourcolleagues
byELVA,severallimitationsremain. Firstly,ELVA
in the NAVER Cloud Hyperscale AI Vision Un-
occasionallystruggleswithrecognizingspecificen-
derstanding Team and KAIST AI LKLab. Their
titiesandfullyunderstandingcontextualnuances
constantsupportandencouragementhavebeena
withinimages,leadingtopenalizedresponsesde-
greatsourceofmotivationthroughoutthiswork.
spitelogicalaccuracy. Thatis,whileELVAexcels
intext-centrictasks,itmighthavelimitationsinthe
vision encoder’s ability to memorize entities and References
handlecontextualdata.
MarahAbdin,SamAdeJacobs,AmmarAhmadAwan,
IncorporatingOCRisbeneficialforprocessing
Jyoti Aneja, Ahmed Awadallah, Hany Awadalla,
text-rich,high-resolutionimages,butitintroduces NguyenBach,AmitBahree,ArashBakhtiari,Jian-
latency and potential accuracy trade-offs. More- min Bao, Harkirat Behl, Alon Benhaim, Misha
over, managing very high-resolution images (4K Bilenko,JohanBjorck,SébastienBubeck,QinCai,
Martin Cai, Caio César Teodoro Mendes, Weizhu
or8K)stillremainschallengingduetothebalance
Chen, Vishrav Chaudhary, Dong Chen, Dongdong
requiredbetweenperformanceimprovementsand Chen,Yen-ChunChen,Yi-LingChen,ParulChopra,
computationalresources. Xiyang Dai, Allie Del Giorno, Gustavo de Rosa,
WhileELVAachieveslowerinferencecostsand MatthewDixon,RonenEldan,VictorFragoso,Dan
Iter,MeiGao,MinGao,JianfengGao,AmitGarg,
maintainsareasonabletrainingtime,thesubstan-
Abhishek Goswami, Suriya Gunasekar, Emman
tial data volume processed can lead to moderate
Haider, Junheng Hao, Russell J. Hewett, Jamie
timedifferences,asdiscussedinAppendixA.This Huynh, Mojan Javaheripi, Xin Jin, Piero Kauff-
underscorestheimportanceofcontinuedoptimiza- mann, Nikos Karampatziakis, Dongwoo Kim, Ma-
houdKhademi,LevKurilenko,JamesR.Lee,YinTat
tioninbothtrainingefficiencyandperformance.
Lee,YuanzhiLi,YunshengLi,ChenLiang,LarsLi-
Futureresearchshouldfocusonrefiningentity
den,CeLiu,MengchenLiu,WeishungLiu,EricLin,
recognition,contextualunderstanding,trainingef- ZeqiLin,ChongLuo,PiyushMadan,MattMazzola,
ficiency,andOCRhandling. Investigatingthebal- ArindamMitra,HardikModi,AnhNguyen,Brandon
Norick,BarunPatra,DanielPerez-Becker,Thomas
ancebetweenspecializedtoolslikeOCRandthe
Portet, Reid Pryzant, Heyang Qin, Marko Radmi-
coreVLMisessentialforoptimizingperformance.
lac,CorbyRosset,SambudhaRoy,OlatunjiRuwase,
Additionally,expandingELVA’scapabilitiestohan- OlliSaarikivi,AminSaied,AdilSalim,MichaelSan-
dle multilingual tasks will further increase its ap- tacroce, Shital Shah, Ning Shang, Hiteshi Sharma,
SwadheenShukla,XiaSong,MasahiroTanaka,An-
plicabilityandutility.
dreaTupini,XinWang,LijuanWang,ChunyuWang,
Yu Wang, Rachel Ward, Guanhua Wang, Philipp
10 EthicalConsiderations
Witte,HaipingWu,MichaelWyatt,BinXiao,Can
Xu,JiahangXu,WeijianXu,SonaliYadav,FanYang,
Developing ELVA involves important ethical re-
JianweiYang,ZiyiYang,YifanYang,DonghanYu,
sponsibilitiessuchasreducingdatabiasesanden-
Lu Yuan, Chengruidong Zhang, Cyril Zhang, Jian-
suringtransparency. Tomanagethese,weuseonly wenZhang,LiLynaZhang,YiZhang,YueZhang,
controlled and verified open-source datasets for YunanZhang,andXirenZhou.2024. Phi-3technical
model training. Currently, we rely on the autore- report: Ahighlycapablelanguagemodellocallyon
yourphone. Preprint,arXiv:2404.14219.
gressivemodels’directoutput,butwecouldalso
usepost-processingtechniquesoradditionaltrain- Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
ing methods to address biases and privacy issues Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc, Arthur Mensch, Katherine Millican, Mal-
better. Byopen-sourcingourmodelsanddatasets,
colm Reynolds, Roman Ring, Eliza Rutherford,
we encourage peer reviews and collaboration to
SerkanCabi,TengdaHan,ZhitaoGong,SinaSaman-
solveethicalchallenges,promotingaccountability. gooei,MarianneMonteiro,JacobMenick,Sebastian
These steps help ensure that ELVA upholds high Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-
handSharifzadeh,MikolajBinkowski,RicardoBar-
ethicalstandardsandisusedforbeneficialpurposes
reira,OriolVinyals,AndrewZisserman,andKaren
whileminimizingrisks.
Simonyan.2022. Flamingo: avisuallanguagemodel
forfew-shotlearning. InAdvancesinNeuralInfor-
Acknowledgements
mationProcessingSystems.
WeextendoursinceregratitudetoBadoLee,Dae-
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,
hee Kim, Taeho Kil, and Hodong Lee for their SinanTan, PengWang, JunyangLin, ChangZhou,
9and Jingren Zhou. 2024. Qwen-VL: A versatile Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang
vision-languagemodelforunderstanding, localiza- Cao, Bin Wang, Linke Ouyang, Songyang Zhang,
tion,textreading,andbeyond. Haodong Duan, Wenwei Zhang, Yining Li, Hang
Yan, YangGao, ZheChen, XinyueZhang, WeiLi,
LucasBeyer,XiaohuaZhai,andAlexanderKolesnikov.
JingwenLi,WenhaiWang,KaiChen,ConghuiHe,
2022. Big vision. https://github.com/
XingchengZhang,JifengDai,YuQiao,DahuaLin,
google-research/big_vision.
andJiaqiWang.2024. Internlm-xcomposer2-4khd:
Apioneeringlargevision-languagemodelhandling
Ali Furkan Biten, Ruben Tito, Lluis Gomez, Ernest
resolutionsfrom336pixelsto4khd. arXivpreprint
Valveny,andDimosthenisKaratzas.2022. Ocr-idl:
arXiv:2404.06512.
Ocrannotationsforindustrydocumentlibrarydataset.
arXivpreprintarXiv:2202.12985.
Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian,
ZongxiaLi,XiaoyuLiu,XijunWang,LichangChen,
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
FurongHuang,YaserYacoob,DineshManocha,and
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
TianyiZhou.2024. Hallusionbench: Anadvanced
Neelakantan,PranavShyam,GirishSastry,Amanda
diagnosticsuiteforentangledlanguagehallucination
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
andvisualillusioninlargevision-languagemodels.
Gretchen Krueger, Tom Henighan, Rewon Child,
InProceedingsoftheIEEE/CVFConferenceonCom-
AdityaRamesh,DanielZiegler,JeffreyWu,Clemens
puterVisionandPatternRecognition(CVPR),pages
Winter, Chris Hesse, Mark Chen, Eric Sigler, Ma-
14375–14385.
teusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, ChristopherBerner, SamMcCandlish, Alec
Yu-ChungHsiao,FedirZubach,MariaWang,andJin-
Radford, Ilya Sutskever, and Dario Amodei. 2020.
dongChen.2024. Screenqa: Large-scalequestion-
Language models are few-shot learners. In Ad-
answerpairsovermobileappscreenshots. Preprint,
vances in Neural Information Processing Systems,
arXiv:2209.08199.
volume 33, pages 1877–1901. Curran Associates,
Inc. Anwen Hu, Haiyang Xu, Jiabo Ye, Ming Yan, Liang
Zhang, BoZhang, ChenLi, JiZhang, QinJin, Fei
KeqinChen,ZhaoZhang,WeiliZeng,RichongZhang,
Huang,andJingrenZhou.2024. mplug-docowl1.5:
FengZhu,andRuiZhao.2023a. Shikra: Unleashing
Unifiedstructurelearningforocr-freedocumentun-
multimodalllm’sreferentialdialoguemagic. arXiv
derstanding. Preprint,arXiv:2403.12895.
preprintarXiv:2306.15195.
Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Di-
LinChen,JinsongLi,XiaoyiDong,PanZhang,Yuhang
mosthenisKaratzas,ShijianLu,andC.V.Jawahar.
Zang, Zehui Chen, Haodong Duan, Jiaqi Wang,
2019. Icdar2019competitiononscannedreceiptocr
Yu Qiao, Dahua Lin, et al. 2024. Are we on the
and information extraction. In 2019 International
rightwayforevaluatinglargevision-languagemod-
ConferenceonDocumentAnalysisandRecognition
els? arXivpreprintarXiv:2403.20330.
(ICDAR),pages1516–1520.
Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Con-
KushalKafle,ScottCohen,BrianPrice,andChristopher
ghui He, Jiaqi Wang, Feng Zhao, and Dahua
Kanan.2018. Dvqa: Understandingdatavisualiza-
Lin. 2023b. Sharegpt4v: Improving large multi-
tionsviaquestionanswering. InCVPR.
modalmodelswithbettercaptions. arXivpreprint
arXiv:2311.12793.
Sahar Kazemzadeh, Vicente Ordonez, Mark Matten,
XiangxiangChu,LimengQiao,XinyuZhang,Shuang andTamaraBerg.2014. ReferItGame: Referringto
Xu,FeiWei,YangYang,XiaofeiSun,YimingHu, objectsinphotographsofnaturalscenes. InProceed-
XinyangLin,BoZhang,andChunhuaShen.2024. ingsofthe2014ConferenceonEmpiricalMethodsin
Mobilevlmv2:Fasterandstrongerbaselineforvision NaturalLanguageProcessing(EMNLP),pages787–
languagemodel. Preprint,arXiv:2402.03766. 798, Doha, Qatar. Association for Computational
Linguistics.
OpenCompass Contributors. 2023. Opencompass:
A universal evaluation platform for foundation Aniruddha Kembhavi, Michael Salvato, Eric Kolve,
models. https://github.com/open-compass/ MinjoonSeo,HannanehHajishirzi,andAliFarhadi.
opencompass. 2016. Adiagramisworthadozenimages. ArXiv,
abs/1603.07396.
WenliangDai,JunnanLi,DongxuLi,AnthonyTiong,
Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Geewook Kim, Teakgyu Hong, Moonbin Yim,
Fung,andStevenHoi.2023. InstructBLIP:Towards JeongYeonNam,JinyoungPark,JinyeongYim,Won-
general-purposevision-languagemodelswithinstruc- seok Hwang, Sangdoo Yun, Dongyoon Han, and
tiontuning. InThirty-seventhConferenceonNeural Seunghyun Park. 2022. OCR-Free Document Un-
InformationProcessingSystems. derstanding Transformer. In Computer Vision –
ECCV2022,pages498–517,Cham.SpringerNature
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Switzerland.
LukeZettlemoyer.2023. QLoRA:Efficientfinetun-
ing of quantized LLMs. In Thirty-seventh Confer- GeewookKim,HodongLee,DaeheeKim,HaejiJung,
enceonNeuralInformationProcessingSystems. Sanghee Park, Yoonsik Kim, Sangdoo Yun, Taeho
10Kil,BadoLee,andSeunghyunPark.2023. Visually- HaotianLiu,ChunyuanLi,QingyangWu,andYongJae
situated natural language understanding with con- Lee.2023c. Visualinstructiontuning.
trastive reading model and frozen large language
models. InProceedingsofthe2023Conferenceon PanLu,HritikBansal,TonyXia,JiachengLiu,Chun-
Empirical Methods in Natural Language Process- yuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-
ing,pages11989–12010,Singapore.Associationfor WeiChang,MichelGalley,andJianfengGao.2024.
ComputationalLinguistics. Mathvista: Evaluating mathematical reasoning of
foundation models in visual contexts. In Inter-
RanjayKrishna,YukeZhu,OliverGroth,JustinJohn- national Conference on Learning Representations
son, Kenji Hata, Joshua Kravitz, Stephanie Chen, (ICLR).
Yannis Kalantidis, Li-Jia Li, David A. Shamma,
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-
Michael S. Bernstein, and Li Fei-Fei. 2017. Vi-
WeiChang,Song-ChunZhu,OyvindTafjord,Peter
sual genome: Connecting language and vision us-
ing crowdsourced dense image annotations. Int. J. Clark,andAshwinKalyan.2022. Learntoexplain:
Comput.Vis.,123(1):32–73. Multimodalreasoningviathoughtchainsforscience
questionanswering. InThe36thConferenceonNeu-
Hugo Laurençon, Léo Tronchon, Matthieu Cord, ralInformationProcessingSystems(NeurIPS).
and Victor Sanh. 2024. What matters when
AhmedMasry,XuanLongDo,JiaQingTan,ShafiqJoty,
building vision-language models? Preprint,
andEnamulHoque.2022. ChartQA:Abenchmark
arXiv:2405.02246.
forquestionansweringaboutchartswithvisualand
Kenton Lee, Mandar Joshi, Iulia Raluca Turc, Hexi- logicalreasoning. InFindingsoftheAssociationfor
angHu,FangyuLiu,JulianMartinEisenschlos,Ur-
ComputationalLinguistics: ACL2022,pages2263–
vashi Khandelwal, Peter Shaw, Ming-Wei Chang, 2279,Dublin,Ireland.AssociationforComputational
andKristinaToutanova.2023. Pix2Struct: Screen- Linguistics.
shotparsingaspretrainingforvisuallanguageunder-
MineshMathew,VirajBagal,RubènTito,Dimosthenis
standing. InProceedingsofthe40thInternational
Karatzas, Ernest Valveny, and C.V. Jawahar. 2022.
Conference on Machine Learning, volume 202 of
InfographicVQA. InProceedingsoftheIEEE/CVF
ProceedingsofMachineLearningResearch,pages
WinterConferenceonApplicationsofComputerVi-
18893–18912.PMLR.
sion(WACV),pages1697–1706.
Bohao Li, Yuying Ge, Yi Chen, Yixiao Ge, Ruimao
MineshMathew,DimosthenisKaratzas,andCVJawa-
Zhang, and Ying Shan. 2024. Seed-bench-2-plus:
har.2021. Docvqa: Adatasetforvqaondocument
Benchmarking multimodal large language models
images. InProceedingsoftheIEEE/CVFwintercon-
withtext-richvisualcomprehension. arXivpreprint
ference on applications of computer vision, pages
arXiv:2404.16790.
2200–2209.
BohaoLi,RuiWang,GuangzhiWang,YuyingGe,Yix-
OpenAI. 2023. GPT-4 Technical Report. Preprint,
iaoGe,andYingShan.2023. Seed-bench: Bench-
arXiv:2303.08774.
marking multimodal llms with generative compre-
hension. arXivpreprintarXiv:2307.16125. SeunghyunPark,SeungShin,BadoLee,JunyeopLee,
JaeheungSurh,MinjoonSeo,andHwalsukLee.2019.
FuxiaoLiu,KevinLin,LinjieLi,JianfengWang,Yaser Cord: A consolidated receipt dataset for post-ocr
Yacoob, and Lijuan Wang. 2023a. Aligning large parsing. InDocumentIntelligenceWorkshopatNeu-
multi-modal model with robust instruction tuning. ralInformationProcessingSystems.
arXivpreprintarXiv:2306.14565.
QuanSun,JinshengWang,QiyingYu,YufengCui,Fan
FuxiaoLiu,XiaoyangWang,WenlinYao,JianshuChen, Zhang,XiaosongZhang,andXinlongWang.2024.
Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Eva-clip-18b: Scalingclipto18billionparameters.
Dong Yu. 2024a. MMC: Advancing multimodal Preprint,arXiv:2402.04252.
chartunderstandingwithlarge-scaleinstructiontun-
ing. InProceedingsofthe2024Conferenceofthe RyotaTanaka,KyosukeNishida,andSenYoshida.2021.
NorthAmericanChapteroftheAssociationforCom- VisualMRC:MachineReadingComprehensionon
putationalLinguistics: HumanLanguageTechnolo- DocumentImages. InThirty-FifthAAAIConference
gies (Volume 1: Long Papers), pages 1287–1310, on Artificial Intelligence, AAAI 2021, Thirty-Third
MexicoCity,Mexico.AssociationforComputational ConferenceonInnovativeApplicationsofArtificial
Linguistics. Intelligence, IAAI 2021, The Eleventh Symposium
on Educational Advances in Artificial Intelligence,
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae EAAI2021,VirtualEvent,February2-9,2021,pages
Lee.2023b. Improvedbaselineswithvisualinstruc- 13878–13888.AAAIPress.
tiontuning.
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
HaotianLiu,ChunyuanLi,YuhengLi,BoLi,Yuanhan Chaumond,ClementDelangue,AnthonyMoi,Pier-
Zhang,ShengShen,andYongJaeLee.2024b. Llava- ricCistac,TimRault,RemiLouf,MorganFuntow-
next: Improvedreasoning,ocr,andworldknowledge. icz,JoeDavison,SamShleifer,PatrickvonPlaten,
11Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, A AnalysisonModelTrainingCostand
Teven Le Scao, Sylvain Gugger, Mariama Drame, ComparisontoOtherModels
QuentinLhoest,andAlexanderRush.2020. Trans-
formers:State-of-the-artnaturallanguageprocessing. Table 7 delivers a comprehensive analysis of the
InProceedingsofthe2020ConferenceonEmpirical
trainingcostsincurredwithdifferentmodelsizes
Methods in Natural Language Processing: System
Demonstrations,pages38–45,Online.Association
ofELVA,utilizing8A100GPUs. Asperestimates
forComputationalLinguistics. drawnfromtheofficialblogofLLaVA-NeXT3,the
trainingtimesofourmodelsareroughly1.38to1.8
MitchellWortsman,GabrielIlharco,SamirYaGadre,
timeslonger. Theyreportedrequiring20hoursto
RebeccaRoelofs,RaphaelGontijo-Lopes,AriSMor-
cos, Hongseok Namkoong, Ali Farhadi, Yair Car- traina7Bmodelwith8A100GPUs,and24hours
mon,SimonKornblith,andLudwigSchmidt.2022. fora13Bmodelusing16A100GPUs. Although
Model soups: averaging weights of multiple fine-
the precise training time may vary based on the
tunedmodelsimprovesaccuracywithoutincreasing
inferencetime. InProceedingsofthe39thInterna-
testingenvironment,ourdatasuggeststhat ELVA’s
tionalConferenceonMachineLearning,volume162 trainingcostscomparable.
ofProceedingsofMachineLearningResearch,pages The comparison is more evident when juxta-
23965–23998.PMLR.
posed with contemporary, data-intensive models
YihengXu,MinghaoLi,LeiCui,ShaohanHuang,Furu like QwenVL (Bai et al., 2024), which calls for
Wei,andMingZhou.2020. LayoutLM:Pre-training 1.4B data points for pretraining and 50M for in-
ofTextandLayoutforDocumentImageUnderstand- structiontuning. Givenconcernsaboutincreased
ing. In KDD ’20: The 26th ACM SIGKDD Con-
data and resources, our model offers a different
ferenceonKnowledgeDiscoveryandDataMining,
VirtualEvent,CA,USA,August23-27,2020,pages perspective. Forinstance,comparedtoQwenVL’s
1192–1200.ACM. significant resource investment, ELVA approach
exhibitsmodestyeteffectiveresourceuse.
Zhiyang Xu, Chao Feng, Rulin Shao, Trevor Ashby,
Models like Shikra (Chen et al., 2023a),
YingShen,DiJin,YuCheng,QifanWang,andLifu
Huang. 2024. Vision-flan: Scaling human-labeled Idefics2 (Laurençon et al., 2024) and InternLM-
tasks in visual instruction tuning. arXiv preprint XComposer2-4KHD(Dongetal.,2024),illustrate
arXiv:2402.11690.
various scales of resource utilization. Shikra uti-
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, lizes 600K data points for alignment and 5.5M
and Lucas Beyer. 2023. Sigmoid loss for lan- forinstructiontuning,Idefics2revealsimpressive
guage image pre-training. In Proceedings of the
outcomeswithover1Bdatapoints,andInternLM-
IEEE/CVF International Conference on Computer
XComposer2-4KHD demonstrates the ability to
Vision(ICCV),pages11975–11986.
scale up with a massive volume of training data
Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan
andmorethan8,000inputtokens.
Zhou, Nedim Lipka, Diyi Yang, and Tong Sun.
OurobservationsinSection4.3furtherreinforce
2023. Llavar: Enhanced visual instruction tun-
ing for text-rich image understanding. Preprint, theargumentforELVA’sefficiency. Despiteaddi-
arXiv:2306.17107. tionaldata,LLaVA-1.5failedtosurpasstheoverall
scoreweachievedwith ELVA(C5vs. C6),further
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, substantiating the efficiency and effectiveness of
Zhuohan Li, Dacheng Li, Eric Xing, Hao Zhang, ELVA. Furthermore, it is essential to accentuate
JosephE.Gonzalez,andIonStoica.2023a. Judging the inference cost’s importance. Models aiming
LLM-as-a-judgewithMT-benchandchatbotarena.
forreducedinferencecostsoftenencounterexpen-
InThirty-seventhConferenceonNeuralInformation
ProcessingSystemsDatasetsandBenchmarksTrack. sive training costs and challenges in maintaining
instruction-followingcapabilitiesacrossvariedre-
Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Jeff
sponselengths(Daietal.,2023;Liuetal.,2023b;
Huang,ChuyueSun,CodyHaoYu,ShiyiCao,Chris-
tosKozyrakis,IonStoica,JosephE.Gonzalez,Clark
Laurençon et al., 2024). Thus, ELVA presents as
Barrett,andYingSheng.2023b. Efficientlyprogram- aquick,lightweight,andcost-effectivealternative
minglargelanguagemodelsusingsglang. Preprint, withinLLaVA-likesimplearchitectures.
arXiv:2312.07104.
Moreover,weobserveongoinginitiativestode-
ÁlyssonSoares,RicardodasNevesJunior,andByron velopcompactyetrobustVLMswithintherange
Bezerra.2020. Biddataset: achallengedatasetfor of1Bto3Bparameters(Chuetal.,2024). These
documentprocessingtasks. InAnaisEstendidosdo
XXXIII Conference on Graphics, Patterns and Im- 3https://llava-vl.github.io/blog/
ages,pages143–146,PortoAlegre,RS,Brasil.SBC. 2024-01-30-llava-next
12ModelSize AlignmentTime InstructTuningTime TotalTime Dataset #Samples SamplingRatio%
0.2B 0.5hours 3hours 3.5hours LLaVA-1.5-Set(Liuetal.,2023b)(SeeTable8) 665,298 46.49
Vision-Flan-Set(Xuetal.,2024) 186,103 12.99
1B 1.5hours 6hours 7.5hours
WikiArt(Chenetal.,2023b) 500 0.03
3.8B 4.5hours 18.5hours 23hours Celebrity(Chenetal.,2023b) 498 0.03
7B 6.5hours 29.5hours 36hours Landmark(Chenetal.,2023b) 500 0.03
13B 11hours 55hours 66hours Share-TextVQA(Chenetal.,2023b) 500 0.03
DocVQA(Mathewetal.,2021) 11,480 1.60
ChartQA(Masryetal.,2022) 18,317 2.56
Table 7: Training times for various model sizes on 8 ˚Cauldron-Set-AI2D 2,434 0.17
A100GPUs. ˚Cauldron-Set-Chart2Text 26,961 1.88
˚Cauldron-Set-Diagram-Image-to-Text 300 0.02
˚Cauldron-Set-HITAB 2,500 0.17
˚Cauldron-Set-IAM 5,663 0.40
Dataset #Samples ˚Cauldron-Set-RenderedText 10,000 0.70
LLaVA 157,712 ˚Cauldron-Set-Robut-SQA 8,514 0.59
˚Cauldron-Set-Robut-WTQ 38,246 2.67
SG40k 40,688 ˚Cauldron-Set-ScienceQA 4,976 0.35
VQA-v2 82,783 ˚Cauldron-Set-Screen2words 15,730 1.10
GQA 72,140 ˚Cauldron-Set-STVQA 17,247 1.20
OKVQA 8,998 ˚Cauldron-Set-TabMWP 22,722 1.59
˚Cauldron-Set-InfoVQA 2,118 0.15
OCRVQA 80,000 ˚Cauldron-Set-TQA 1,493 0.10
A-OKVQA 66,160 CORD-Instruct(Proposedinthiswork,§5.2) 680 0.05
TextCaps 21,953 VisualMRC(Tanakaetal.,2021) 7,959 0.56
RefCOCO 48,447 LLaVAR-Inst(Zhangetal.,2023) 19,732 1.38
DocReason(Huetal.,2024) 25,877 1.81
VG 86,417 DocVQA-single: 44,815 9.38
ChartQA-single: 28,068 5.88
Table8: DatasetproportionsforLLaVA-1.5(Liuetal., Layout-en-sampled;(Kimetal.,2023) 50,000 3.49
DVQA-sampled;(Kafleetal.,2018) 10,000 0.70
2023b). RefCOCO and VG were not used in ELVA MMC-Chart-sampled;(Liuetal.,2024a) 10,000 0.70
ScreenQA-sampled;(Hsiaoetal.,2024) 10,000 0.70
ablationstudies.
LRV-Chart-sampled;(Liuetal.,2023a) 6,746 0.47
Table9: OverviewofdatasetsusedinELVA’sfinaldata
workspredominantlyconcentrateongeneralimage configuration. Alldatasetsareopen-sourceandfreely
processing. Remarkably,effectiveVLMsexplicitly accessible. Datasetsmarkedwith˚aresubsetscurated
designedforvisually-situatedNLUtasks,likepro- byLaurençonetal.(2024),withonlyselectedportions
cessinghigh-resolutiondocumentimages,arestill adopted in this work. : indicates that each question-
answerpairisconsideredasasinglesample. Datasets
limited,highlightingourwork’sprimaryfocus.
markedwith;hadalargevolumeofdata,hence,only
partialimageswererandomlysampled.
B ImplementationDetails
B.1 SoftwareandHardwareSetup
we used the respective alignment datasets from
Ourexperimentsareperformedusingtheofficial
LLaVA (Liu et al., 2023c) and LLaVAR (Zhang
code of LLaVA (Liu et al., 2023c)4. We utilize
etal.,2023),whichconsistof558Kand422Ksam-
NVIDIA V100 and A100 GPUs for the compu-
plesrespectively.
tations. Ablation studies are conducted on V100
GPUs,whereasthefinalconfigurationmodelsrun Hyperparameters. Table 10 and Table 11 out-
onA100GPUs. Wedonotobserveanysignificant linethehyperparametersappliedduringthealign-
performancedifferencebasedonthetypeofGPU ment and instruction tuning stages, noting that
used. However,trainingonV100GPUsisapprox- smaller models thrive with larger learning rates.
imately2to3timesslowerperstepcomparedto In the final model training configuration, we use
A100 GPUs. Although our codebase is based on thedataandsamplingratiosfromTable9andtrain
LLaVA, to ensure better reproducibility, we will the model for 11K steps. The exact number of
releasethescriptsusedfortrainingourmodelsand uniqueimagesisdifficulttocalculatesincesome
anynecessarycodemodificationsasopen-source. imagesaresharedacrossdatasets,butweestimate
usingapproximately1Muniqueimages. Notethat
B.2 DatasetsandHyperparameters LLaVA-NeXTreportedusing760Ksamples,5 indi-
InitialCuratedDatasetfromLLaVA-1.5. Ta- catingamodestincreaseinourcase. Moreover,as
ble 8 provides detailed quantities of the subsets weleveragemultiplecurateddatasetswithslightly
withinthedataset. different questions on the same images, we con-
siderasyntheticepochtoconsistof1.4Mexamples.
Datasets. Table9liststhedatasetsinElva’sfinal
Thus,withabatchsizeof128,werun11Ksteps
configuration. Meanwhile,forthealignmentphase,
5https://llava-vl.github.io/blog/
4https://github.com/haotian-liu/LLaVA 2024-01-30-llava-next
13Q: Can you extract the information from the provided image
ModelSize LR Epsilon GradClipNorm WeightDecay WarmupRatio and give it to me in JSON format?
0.2B 1e-3 1e-6 0.5 0.0 0.03
A: Certainly, here’s the extracted information in JSON format:
1B 1e-3 1e-6 0.5 0.0 0.03 ```json
3.8B 1e-3 1e-6 0.5 0.0 0.03 {
7B 1e-4 1e-6 0.5 0.0 0.03 "menu": [
{ "nm": "NASI + AYAM KATSU TER...", "cnt": "1", "price": "31,819" },
13B 1e-4 1e-6 0.5 0.0 0.03 { "nm": "TEH PANAS", "cnt": "1", "price": "5,455" },
{ "nm": "ES TEH MANIS", "cnt": "1", "price": "7,273" },
{ "nm": "CH CORDON BLEU NASI", "cnt": "1", "price": "42,728" }
Table10: Hyperparametersusedduringthealignment ],
"sub_total": {
stage. "subtotal_price": "87,275", "tax_price": "8,728", "etc": "-3"
},
"total": {
"total_price": "96,000", "creditcardprice":
"96,000", "menuqty_cnt": "ITEMS :4"
ModelSize LR Epsilon GradClipNorm WeightDecay WarmupRatio }
0.2B 3e-4 1e-6 0.5 1e-3 0.03 }
```
1B 3e-4 1e-6 0.5 1e-3 0.03
Q: Can you extract the information from the provided image and give it
3.8B 2e-4 1e-6 1.0 0.0 0.03 to me in Markdown format?
7B 2e-5 1e-6 1.0 0.0 0.03
13B 2e-5 1e-6 1.0 0.0 0.03 A: Sure, here's the extracted information in Markdown format:
```
# Menu
- Name: Gangster BZR
Table 11: Hyperparameters used during the instruct - Quantity: 1
- Price: $30.00
tuning stage. Larger learning rates were noted to be - Submenu: +Level 0
moreeffectiveforsmallermodels. # Subtotal
- Subtotal Price: $30.00
# Total
- Total Price: $30.00
- Cash Price: $30.00
(1.4M/128). FortheAblationstudies,weexclude - Change Price: $0
```
datasets like VG, RefCOCO, and Vision-Flan to
reducethecost,resultingin9Ktrainingsteps. Ad-
Figure7: ThegeneratedexamplesofCORD-Instruct.
ditionally,the0.2Bmodelconvergesmoreslowly,
sowerunthismodelfortwicethenumberofsteps
compared to the other models (1B to 13B). This spiredbyLLaVA-Bench,itincludestasksthatre-
increased number of steps is applied only to the quire careful interpretation and analysis of doc-
0.2Bmodel. uments. We incorporate Brazilian Identity Doc-
uments (BID) (Álysson Soares et al., 2020) and
C DetailsontheConstructionofthe
SROIE (Huang et al., 2019) in its creation. Ta-
NewlyGeneratedDatasets bles15andTables16showthepromptsweuseto
extractinformationfromimages.
C.1 CORD-Instruct
The evaluation process is visually depicted in
CORD(Parketal.,2019)providesIndonesianre-
Figure8forabetterunderstandingofourmethod.
ceiptdataalongwithstructuredinformationrepre-
Parsing-Benchincludesexamplesthatnecessitate
sentedinJSONformat. Ourgoalwastogenerate
themodeltoextractandinterpretinformationfrom
adatasetthatrequiresmodelstoproduceoutputs
diverse documents. These instances have been
inspecificformatssuchasJSON,XML,orMark-
specifically designed to test the models’ under-
down.
standingandreasoningskills.
The CORD-Instruct dataset is generated using
As part of our commitment to sharing our re-
the OpenAI GPT-3.5 API. A detailed prompt is
searchoutputforthebenefitofothers,thedatasets
crafted(asoutlinedinTable13)toguidetheAPIin
willbemadepubliclyavailable.
generatingsyntheticuserqueriesandcorrespond-
ingresponsesbasedonthegivenstructuredinfor-
D ELVA-encoderTraining
mation.
After generating initial instructions and Ope- Training Details. As explained in Section 4.3,
nAI’s response with synthetic questions and an- each REncoder variant is trained by unfreezing
swers,wemanuallyfilteredandannotatedthesam- thevisionencoderandfine-tuningitonaminimal
ples to ensure they accurately represented real- VLM(1Bscale)withtext-centricdatasets. Weopt
worldscenarios. Thisincludesremovingambigu- for a 1B parameter model to strike a balance be-
ousinstructionsandcorrectingpoorlytranscribed tweencomputationalrequirementsandmodelper-
JSON,XML,orMarkdownoutputs. formance. Alargermodelmightfurther enhance
textrecognition,butitsignificantlyincreasescom-
C.2 Parsing-Bench
putational needs and training time. The previous
Parsing-Bench is a dataset tailored to meet the attemptwitha160M-parametermodelworkedbut
practical needs of document assistants. In- showedroomforimprovementintermsofperfor-
14BatchSize LearningRate(LR) WeightDecay CORD-InstructGenerationPrompt
128 5e-5 1e-3
128 6e-5 1e-3 Createasyntheticuserqueryrequestinginformation
128 7e-5 1e-3 extraction from a given document image. The ex-
128 8e-5 1e-3 tracted information should be provided in various
256 5e-5 1e-3 formatssuchasJSON,XML,orMarkdownbased
256 6e-5 1e-3 on the user’s request. Users may ask for specific
256 7e-5 1e-3 partsoftheinformation,likemenuitemsorpayment
256 8e-5 1e-3 amounts.Yourtaskistogeneratebothauserquery
512 5e-5 0.0 andthecorrespondingresponsefromtheinformation
512 6e-5 0.0 extractionsystem.Ensurethequeriesandresponses
512 7e-5 0.0 varyindetailandformat.Sometimesincludeconcise
512 8e-5 0.0 responses,particularlywhenindicatedwiththeword
"concisely."
Table 12: The configurations for 12 difference REn- TheprovidedJSONistoguideyourresponsecre-
ation-donotdisplayormentionitintheuserqueries.
codertraining,includingbatchsize,learningrate,and
Specifically, do not create queries that ask for in-
weightdecay.Otherhyperparametersbasicallyidentical
formation extraction from a provided JSON (e.g.,
to§B.2. "Canyouextracttheinformationfromtheprovided
JSON" is not allowed). Additionally, you do not
needtostrictlyfollowthetagnamesintheprovided
JSONwhilecreatingyourresponses(e.g.,"nm"can
mance. Consequently,a1Bparametermodelwas
be"name"or"cnt"canbe"count"). Returnresults
selectedforsubstantialenhancements,yetafford- strictlyintheformatshownbelow:
ingefficiencyincomputationalresourceuse. Query: I need the payment amount
from the document in this image
Our training procedure mainly focuses on text
in JSON format, answer concisely.
reading tasks to fortify the vision encoder’s text
Answer: <<<json
recognition abilities. The training datasets are {
"payment_amount": "$123.45"
OCR-IDL (Biten et al., 2022) (837,922 samples
}
utilized), PDFA6 (1,048,569 samples utilized), >>>
LLaVA (Liu et al., 2023c) alignment set, and
Query: Please parse the input document
LLaVAR (Zhang et al., 2023) alignment set. We and provide the menu details in XML.
Answer: Certainly, here is the menu
allow duplicate sampling, treating 3.5M samples
information in XML format:...
asavirtualepoch,andperform1epochoftraining
Query: Can you provide the extracted
withtheconfigurationdepictedinTable12. This
customer information from the document
procedureisrepeatedcreating12distinctvariants image in Markdown?
oftheREncoder. Thechosenmodelscalesupports Answer: Sure, here it is:
<<<
rapiditerations,witheachsessiontakingroughly
# Customer Information
1.7daysusing8V100GPUs. - Name: John Doe
- Email: john.doe@example.com
>>>
WeightAveragingwithModelSoup. Although
Remember, the goal is to include appropriate for-
Wortsman et al. (2022) proposed a greedy soup
mattingsuchasJSON,XML,orMarkdowninyour
method involving validation where some models responsestocorrespondwiththeuser’squery.
areaveragedorrejected,weemployedauniform
averagingtechniquereferredtoas“uniformsoup”
Table13: CORD-InstructPromptforDataGeneration.
inthepaper.
E PromptsusedinRRPrompts mixofMSOCR7 andCLOVAOCR8 annotations.
Intext-heavysampleslikethoseinDocVQA,the
As seen in Table 14, we insert a preliminary QA CLOVAOCRAPItakesapproximately4seconds
turnwherethemodelispromptedtoreadthetext per call. However, we believe this cost can be
in the image using simple commands. Although optimizedinfutureiterations.
theideaisstraightforward,weobservethatitleads
tobettertrainingoutcomesforthemodel. Toavoid
specialization to a single OCR engine, we use a
7https://docs.microsoft.com/en-us/azure/
6https://huggingface.co/datasets/pixparse/ cognitive-services/computer-vision/overview-ocr.
pdfa-eng-wds 8https://clova.ai/ocr/en
15Prompt
Carefullydecipherthetextinthisimage. Providethetextintheimageonly.
Investigatetheimageforanytext. Providethetextintheimageonly.
Examinetheimageforanylettersorwords. Providethetextintheimageonly.
Identifyallwrittencharacterspresentintheimage. Providethetextintheimageonly.
Doacarefulreadingoftheimageandtranscribealltext. Providethetextintheimageonly.
Inspecttheimageandwritedownallreadablecharacters. Providethetextintheimageonly.
Translatetheimagecontentintowrittentext. Providethetextintheimageonly.
Reviewtheimageandofferatranscriptionofthetext. Providethetextintheimageonly.
Lookovertheimageandjotdownallvisibletext. Providethetextintheimageonly.
Scrutinizetheimageforanydiscerniblewordsorletters. Providethetextintheimageonly.
Studytheimageanddocumentallcharactersfoundwithin. Providethetextintheimageonly.
Assesstheimageandrecordanyvisiblewordsorletters. Providethetextintheimageonly.
Kindlyextractanytextorcharactersfromthisimage. Providethetextintheimageonly.
Evaluatetheimageandisolateallwrittencontent. Providethetextintheimageonly.
Assimilateallreadablecharacterswithintheimage. Providethetextintheimageonly.
Decodeanylegibletextvisibleintheimage. Providethetextintheimageonly.
Graspallwrittenelementswithintheimage. Providethetextintheimageonly.
Conductathoroughexaminationoftheimageandcapturealltext. Providethetextintheimageonly.
Peelallreadablecharactersoutfromtheimage. Providethetextintheimageonly.
Interpretallcharactersfromtheimage. Providethetextintheimageonly.
Understandandtranscribeanytextualcontentfromtheimage. Providethetextintheimageonly.
Dissecttheimageandrecoveranywrittencharacters. Providethetextintheimageonly.
Deduceandpresentallvisibletextfromtheimage. Providethetextintheimageonly.
Inspecttheimageandtranslateanyvisiblecharactersintotext. Providethetextintheimageonly.
Table14: PromptexamplesfortextreadingtasksusedinRead-and-ReasonPrompting.
BIDContextGenerationPrompt SROIEContextGenerationPrompt
UsingtheprovidedBrazilianIdentityDocumentim- Using the provided scanned receipt image, please
age,pleasecomposeacomprehensiveanddetailed composeacomprehensiveanddetailedcaptionthat
captionthatencapsulatesalltheelementsdepicted encapsulatesalltheelementsdepictedintheimage.
intheimage.Ensureprecisioninextractinganytext Ensureprecisioninextractinganytextpresent,main-
present,maintainingcasesensitivityandretainingthe tainingcasesensitivityandretainingtheexactorig-
exactoriginalform. Beginwithawell-writtencap- inalform. Beginwithawell-writtencaptioninnat-
tioninnaturallanguage,detailingtheimage’scontent, urallanguage,detailingtheimage’scontent,layout,
layout,andnuances.Concludewithawell-structured andnuances.Concludewithawell-structuredXML
XML format that meticulously documents the ex- formatthatmeticulouslydocumentstheextractedin-
tractedinformation,preservingtheimage’soriginal formation,preservingtheimage’soriginallayoutand
layoutanddetails. details.
Table 15: Parsing-Bench Context Generation Prompt Table 16: Parsing-Bench Context Generation Prompt
forBIDDataset(ÁlyssonSoaresetal.,2020). forSROIEDataset(Huangetal.,2019).
16Can you summarize the purchased item information, including the name, price,  REVIEW:
8 9
and quantity, in XML format?
Certainly, here is the purchased item information in XML format:
Assistant 1 provided a clear and accurate summary of the
```xml
purchased items in XML format. However, the XML tags used
<purchased_items>
<item> ("Receipt", "Item", "Name", "Price", "Quantity") are not consistent
<name>GREEN TEA</name> with the tags used in the original XML provided by the user. This
<quantity>1</quantity> inconsistency might cause confusion or require additional work to
<price>3.00</price> integrate with the user's existing data structure.
</item>
<item> Assistant 2 also provided a clear and accurate summary of the
<name>GREEN TEA</name> purchased items in XML format. The tags used ("purchased_items",
<quantity>1</quantity> "item", "name", "quantity", "price") are more consistent with the
<price>3.00</price> original XML structure provided by the user, making it easier to
</item> integrate and more relevant. Additionally, Assistant 2's response
<item> includes a root tag ("purchased_items") that encapsulates all items,
<name>BUFFET DINNER (ADULT)</name> which is a good practice for XML structure.
<quantity>3</quantity>
<price>49.90</price> Overall, both assistants performed well, but Assistant 2's response
</item>
is slightly more aligned with the user's existing data structure and
</purchased_items>
includes better XML formatting practices.
```
Elva received 9/10
The image is a scanned receipt from ROCKU YAKINIKU, a restaurant located at CPI ROCKU SDN. BHD. (1096317-P), LOT F355/356/357, FIRST
FLOOR, RAINFOREST, 1 UTAMA SHOPPING CENTRE, LEBUH BANDAR UTAMA, 47800 PETALING JAYA. The receipt includes the GST number
000446271488. The transaction took place on 19/02/2018 at 17:43, with the cashier identified as CASHIER and the server as BISWASH(ip). The table
number is A9, and there were 3 people (Pax: 3). The bill number is 063975.
The items ordered include:
- 1 Green Tea at RM 3.00
- 1 Green Tea at RM 3.00
- 3 Buffet Dinner (Adult) at RM 49.90 each, totaling RM 149.70
The subtotal is RM 155.70. Additional charges include a 10% service charge of RM 15.57 and a 6% GST of RM 10.28, bringing the total to RM 181.55.
The payment was made in cash, with RM 201.55 given, resulting in a change of RM 20.00.
The GST Summary shows:
- T: 6% GST on an amount of RM 171.27, with a tax of RM 10.28
- Z: Zero Rated with an amount of RM 0.00 and a tax of RM 0.00
The receipt concludes with a space for a signature and a thank you note, "Thank you. Please come again." The bill was closed at 19:08:04.
```xml
<receipt>
<restaurant>
<name>ROCKU YAKINIKU</name><company>CPI ROCKU SDN. BHD. (1096317-P)</company><gst_no>000446271488</gst_no>
<address><lot>LOT F355/356/357, FIRST FLOOR</lot><location>RAINFOREST, 1 UTAMA SHOPPING CENTRE</location><street>LEBUH
BANDAR UTAMA</street><city>47800 PETALING JAYA</city></address>
</restaurant>
<transaction>
<date>19/02/2018</date><time>17:43</time><terminal>T001</terminal><bill_no>A063975</bill_no>
<cashier>CASHIER</cashier><server>BISWASH(ip)</server><table>A9</table><pax>3</pax><bill>063975</bill>
</transaction>
<items>
<item><name>GREEN TEA</name><quantity>1</quantity><price>3.00</price><total>3.00</total><tax>T</tax></item>
<item><name>GREEN TEA</name><quantity>1</quantity><price>3.00</price><total>3.00</total><tax>T</tax></item>
<item><name>BUFFET DINNER (ADULT)</name><quantity>3</quantity><price>49.90</price><total>149.70</total><tax>T</tax></item>
</items>
<summary><subtotal>155.70</subtotal><service_charge>15.57</service_charge><gst>10.28</gst><total>181.55</total>
<payment><method>CASH</method><amount>201.55</amount><change>20.00</change></payment>
</summary>
<gst_summary>
<gst><type>T</type><rate>6%</rate><amount>171.27</amount><tax>10.28</tax></gst>
<gst><type>Z</type><rate>Zero Rated</rate><amount>0.00</amount><tax>0.00</tax></gst>
</gst_summary>
<footer>
<signature>_________________________</signature><note>Thank you. Please come again</note><bill_close>19:08:04</bill_close>
</footer>
</receipt>
```
Figure8: Thetopleftdisplaysthequestionandthemodelpredictionresults. Thetoprightcontainsthereceived
review. Thebottompart,asintroducedinSectionC.2,exhibitstheimage’scontextpreparedthroughprompting.
Theevaluationcomparestwomodelpredictionsbyinputtingthemintoahigh-performanceLLMalongwiththis
context. Inthisexamplereview,ELVAisassistant2.
17