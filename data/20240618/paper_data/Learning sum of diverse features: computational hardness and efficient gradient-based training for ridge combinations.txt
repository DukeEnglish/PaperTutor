Learning sum of diverse features: computational hardness and
efficient gradient-based training for ridge combinations
Kazusato Oko∗, Yujin Song†, Taiji Suzuki‡, Denny Wu§
June 18, 2024
Abstract
We study the computational and sample complexity of learning a target function f :Rd →R with
∗
additive structure, that is, f ∗(x) = √1
M
(cid:80)M m=1f m(⟨x,v m⟩), where f 1,f 2,...,f
M
: R → R are nonlinear
link functions of single-index models (ridge functions) with diverse and near-orthogonal index features
{v }M , and the number of additive tasks M grows with the dimensionality M ≍dγ for γ ≥0. This
m m=1
problemsettingismotivatedbytheclassicaladditivemodelliterature,therecentrepresentationlearning
theoryoftwo-layerneuralnetwork,andlarge-scalepretrainingwherethemodelsimultaneouslyacquires
a large number of “skills” that are often localized in distinct parts of the trained network. We prove
thatalargesubsetofpolynomialf canbeefficientlylearnedbygradientdescenttrainingofatwo-layer
∗
neuralnetwork,withapolynomialstatisticalandcomputationalcomplexitythatdependsonthenumber
oftasksM andtheinformation exponent off ,despitetheunknownlinkfunctionandM growingwith
m
the dimensionality. We complement this learnability guarantee with computational hardness result by
establishing statistical query (SQ) lower bounds for both the correlational SQ and full SQ algorithms.
1 Introduction
Westudytheproblemoflearningapolynomialfunctionf :Rd →RonisotropicGaussiandata. Specifically,
∗
given pairs of training example {(x ,y )}n , where x ∈ Rd is drawn from some distribution P and y =
i i i=1 i x i
f (x )+ε , we aim to construct an estimator fˆthat achieves small population error: E |f (x)−fˆ(x)|≤
∗ i i x∼Px ∗
ϵ. Since the space of degree-q polynomials in Rd is of dimension Θ(dq), without additional structural
assumptions on f , the sample complexity required to achieve small error should scale as n ≳ dq, which
∗
is computationally prohibitive for learning large-degree polynomials in high dimensions. Many prior works
have therefore imposed the constraint that f exhibits certain low-dimensional latent structure [APVZ14,
∗
CM20, DLS22], as in the multi-index model: f (x)=g(Vx) where V ∈Rk×d for k =O (1). However, such
∗ d
a low-dimensional restriction may rule out many interesting classes of target functions; for instance, when
specialized to the learning of two-layer neural network, these prior results only apply to the setting where
the network width k is much smaller than the ambient dimensionality d.
In this work, we investigate the efficient learning of polynomial f under a different kind of structural
∗
assumption: weallowthetargetfunctiontodependonalargenumberofdirectionswhichgrowswithd, but
we impose an additive structure; specifically, we consider f to be the sum of M ≍ dγ single-index models
∗
(also known as ridge functions) as follows:
M
1 (cid:88)
f (x)= √ f (⟨x,v ⟩), (1.1)
∗ m m
M
m=1
∗UniversityofTokyoandRIKENAIP.oko-kazusato@g.ecc.u-tokyo.ac.jp.
†UniversityofTokyo. song-yujin139@g.ecc.u-tokyo.ac.jp.
‡UniversityofTokyoandRIKENAIP.taiji@mist.i.u-tokyo.ac.jp.
§NewYorkUniversityandFlatironInstitute. dennywu@nyu.edu.
1
4202
nuJ
71
]GL.sc[
1v82811.6042:viXrawhere v ,v ,...,v ∈ Rd are the unknown index features, and f ,f ,...,f : R → R are the unknown link
1 2 M 1 2 M
(cid:112)
functions; the 1/M prefactor ensures that ∥f ∥ =Θ (1) when the set of directions {v }M is diverse,
∗ L2 d m m=1
i.e., ⟨v ,v ⟩=o (1) for i̸=j. Our setting is motivated by the following lines of literature:
i j d
• Estimation of additive model. In statistical learning theory, additive model is a classical method
employed in high-dimensional nonparametric regression [Sto85, HT87, RLLW09]; especially, when the
individual functions take the form of single-index model (ridge function), the estimation of ridge combi-
nations has been extensively studied [FS81, P+97, KB16]. While efficient algorithms have been proposed
when the basis is given a priori [Bac08, RJWY12, SS12], when the index features {v }M are unknown,
m m=1
mostexistingapproachesinvolvenon-convexoptimization, whichhasbeentreatedasablackboxwithout
convergence guarantees [KSP15, AMF+21], or solved with computationally inefficient algorithm [Bac17].
• Learning two-layer neural network. An important example in the model class (1.1) is a two-layer
neuralnetworkwithM neurons, anditisnaturaltoaskwhethersuchanetworkcanbeefficientlylearned
via standard gradient-based training. Prior works have shown that in the “narrow width” setting M =
Θ (1), gradient descent can learn f with polynomial sample complexity depending on the information
d ∗
exponent ofthetargetfunction[AAM22,BES+22,DLS22,BBSS22]. Ontheotherhand,theregimewhere
M,djointlydivergeisnotwell-understood,andmostexistinganalysesonthecomplexityofgradient-based
training require significant simplification such as quadratic activation [GKZ19, MVEZ20, MBB23].
• Skilllocalization&fine-tuning. Pretrainedlargeneuralnetworks(e.g.,languagemodels)canefficiently
adapt to diverse downstream tasks by fine-tuning a small set of trainable parameters [DCLT18, LL21,
HSW+21]. Recent works have shown that “skills” for each individual task are often localized in a subset
of neurons [DDH+21, EHO+22, WWZ+22, PSZA23, TLS+23, AG23]. The additive model (1.1) gives an
idealized setting where learning exhibits such skill localization (we interpret each f as corresponding
m
to one task). As we will see, after an appropriate hidden representation is obtained via gradient-based
training, the neural network can efficiently adapt to downstream tasks via fine-tuning the top layer.
Our goal is to characterize the statistical and computational complexity of learning the additive model
(1.1), when (i) the number of single-index tasks is large, that is, M grows with the ambient dimensionality
d, and (ii), the tasks are diverse, i.e., the index features of f do not significantly overlap with one another
m
(see Section 2 for precise definition). We ask the following questions:
1. Can we efficiently learn (1.1) via gradient descent (GD) training of a two-layer neural network?
2. What is the hardness of learning (1.1) as measured by statistical query (SQ) lower bounds?
1.1 Our Contributions
We address the two questions by providing complexity upper and lower bounds for learning the additive
model class (1.1), both of which depend on the number of tasks M, and the information exponent p ∈ N
of the link functions f defined as the lowest degree in the Hermite expansion [BAGJ21]. Our findings are
m
summarized as follows (see Table 1 for details).
• In Section 3 we show that a representative subclass of (1.1) can be efficiently learned by a gradient-
based algorithm (using correlational information) on two-layer neural network, even though the number
√
of single-index tasks M is large and the link functions f are not known. Specifically, for M = Ω˜( d)
m
tasks with information exponent p, we prove that a layer-wise (online) SGD algorithm similar to that
in [BBSS22, AAM23] can achieve small population loss using n = Θ˜(Mdp−1) samples. To establish this
learningguarantee,weshowthatthestudentneuronslocalize intothetaskdirectionsduringSGDtraining.
• In Section 4 we establish computational lower bounds for learning (1.1). For correlational SQ algorithms,
weprovethatatoleranceofτ−2 ≳Mdp/2isrequiredwhenlinkfunctionsf havedegreeqandinformation
m
exponent p≤q. We also provide a full SQ lower bound in the form of τ−2 ≳(Md)ρp,q, where ρ
p,q
can be
made arbitrarily large by varying p and q; under the standard τ ≈n−1/2 heuristic for concentration error,
this suggests that prior SQ algorithms that achieve linear-in-d sample complexity in the finite-M regime
[CM20] cannot attain the same statistical efficiency in our additive model setting with large M.
2SQ lower bound CSQ lower bound Online SGD Kernel methods
[Theorem 6] [Theorem 5] [Theorem 1] [GMMM21]
Information
theoreticlimit
Md (Md)ρp,q,γ Mdp/2 O˜(Mdp−1) dq
Table 1: Complexityupperboundforgradient-basedlearningand(C)SQlowerboundsfortheadditivemodel(1.1),
where the single-index tasks have degree q and information exponent p. Our upper bound applies to a subclass of
(1.1)specifiedinSection2. ForthefullSQlowerboundwetaked≍Mγ,andforanyfixedγ >0wemaysetp,q>0
p,q→∞
such that the exponent is arbitrarily large, that is, ρ → ∞. We translate the tolerance in the (C)SQ lower
p,q,γ
bounds to sample complexity using the concentration heuristic τ ≈n−1/2.
1.2 Related Works
Gradient-based learning. Recent works have shown that neural network trained by gradient descent
canlearnusefulrepresentationandadapttolow-dimensionaltargetfunctionssuchassingle-index[BES+22,
BBSS22, MHPG+23, BMZ23] and multi-index models [DLS22, AAM22, BBPV23]. In this finite-M setting,
the complexity of gradient-based learning is governed by the information exponent [BAGJ21] or leap com-
plexity [AAM23] of the target function. However, these learning guarantees for low-dimensional f yield
∗
superpolynomial dimension dependence when M grows with d. Another line of works goes beyond the low-
dimensional assumption by considering the well-specified setting where f and the student network have the
∗
same architecture and special activation function [GKZ19, ZGJ21, AS21, VSL+22, MBB23]. The setting we
consider (1.1) lies between the two regimes: we allow the width to diverge with dimensionality M = ω (1)
d
but do not assume the nonlinear activation is known, and we show that gradient descent can learn f with
∗
polynomialsamplecomplexitydependingontheinformationexponentwhenthetargetweightsare“diverse”.
We also note that beyond gradient descent training, various SQ algorithms have been introduced to solve
related polynomial regression tasks [DH18, CM20, GKS20, DKK+23].
Statistical query lower bound. Astatisticalquerylearner[Kea98,Rey20]canaccessthetargetfunction
via noisy queries ϕ˜ with error tolerance τ: |ϕ˜−E [ϕ(x,y)]| ≤ τ. Lower bound on the performance of
x,y
SQ algorithm is a classical measure of computational hardness. An often-studied subclass of SQ is the
correlational statistical query (CSQ) [BF02] where the query is restricted to (noisy version of) E [ϕ(x)y].
x,y
Many existing results on the CSQ complexity assume f is low-dimensional (M = O (1)), in which case
∗ d
the tolerance scales with τ ≍d−Ω(p), where p is the information exponent or leap complexity of f [DLS22,
∗
ABA22,AAM23]. Ontheotherhand,[VW19,DKKZ20,GGJ+20]establishedCSQlowerboundsforlearning
two-layerReLUnetworkwithoutstructuralassumptionontheweights,wheretheerrortoleranceτ ≍d−Ω(M)
implies a superpolynomial complexity when M = ω (1). Similar superpolynomial lower bound was shown
d
for three-layer neural networks in the full SQ model [CGKM22]. Our result connects these two lines of
analyses: we show that in the setting of diverse (near-orthogonal) weights {v }M , the (C)SQ complexity
m m=1
is polynomial in M,d, where the dimension dependence is specified by the information exponent of f .
m
2 Problem Setting
Notations. Throughout the analysis, ∥·∥ denotes the ℓ norm for vectors and the ℓ →ℓ operator norm
2 2 2
for matrices. O (·) and o (·) stand for the big-O and little-o notations, where the subscript highlights the
d d
asymptotic variable d and suppresses dependence on p,q; we write O˜(·) when (poly-)logarithmic factors are
ignored. Ω(·),Θ(·) are defined analogously. We say an event A happens with high probability when the
failure probability is bounded by exp(−Clogd) where C is a sufficiently large constant; the high probability
events are closed under taking union bounds over sets of size poly(d).
2.1 Assumptions on Target Function
Wefocusonlearningasumofsingle-indexpolynomialsoverGaussianinput,thecomplexityofwhichcrucially
depends on the notion of information exponent [DH18, BAGJ21] defined as the smallest degree of non-zero
3coefficients for the Hermite expansion of the link function.
Definition 1 (Information exponent). Let {He }∞ be the normalized Hermite polynomials. The informa-
j j=0
tionexponentofsquare-integrableg :R→R,whichwedenotebyIE(g):=p∈N ,istheindexofitsfirstnon-
+
zero Hermite coefficient, i.e., given the Hermite expansion
g(z)=(cid:80)∞
α He (z), p:=min{j >0:α ̸=0}.
j=0 j j j
For example, prior works have shown that online SGD can learn a single-index polynomial with infor-
mation exponent p over d-dimensional Gaussian input with O(dp−1) samples for p > 2 [BAGJ21], and the
complexity can be further improved to O(dp/2) via landscape smoothing [DNGL23]. With this definition,
we formally state the class of additive models (1.1) considered in this work.
Assumption 1 (Additive model). We consider the following problem class Fp,q :
d,M,ς
M
1 (cid:88)
x∼N(0,I ), y =f (x)+ν, where f (x)= √ f (v⊤x), ν ∼N(0,ς2),
d ∗ ∗ m m
M
m=1
where each f is a univariate polynomial with information exponent p > 2 and degree q, and we assume
m
proper normalization E [f (t)2] = 1, and ∥v ∥ = 1. We write the Hermite expansion of f as
t∼N(0,1) m m m
(cid:16) (cid:17) 2
f =(cid:80)q α He , and define the constant C = maxm|αm,p| p−2.
m i=p m,i i p min m′|α m′,p|
Remark.
• We restrict ourselves to {f }M with the same information exponent p; this simplifies the optimization
m m=1
dynamics in that all directions are learned at roughly the same rate. To handle heterogeneous tasks with
differentinformationexponents, onemayconsiderastudentmodelwithmixtureofdifferentnonlinearities.
• We focus on the high information exponent setting p > 2, which corresponds to target functions that are
more “difficult” to learn via gradient descent. To handle lower information exponent f , we may employ
m
a pre-processing procedure analogous to that in [DLS22]: we first fit f with a quadratic function, subtract
∗
it from the labels, and add the subtracted components back to the predictor after Algorithm 1 is executed.
In addition to the above specification of the link function, we place the following diversity assumption
on the index features {v }M .
m m=1
Assumption 2 (Task diversity). We assume the following diversity condition on {v }M :
m m=1
(cid:26) √ (cid:27)
M ≤c max (cid:0) max |v⊤v |(cid:1)−1 , d , where c ≍1/polylog(d).
v m m′ v
m̸=m′
Remark.
• This condition ensures that the single-index tasks are diverse, in that the index feature directions do not
significantly overlap; similar assumption also appeared in prior works on gradient-based feature learning
[WNL23]. When each v is an independent sample from the (d−1)-dimensional unit sphere Sd−1, via a
m
standard concentration argument, Assumption 2 is satisfied with high probability for M ≍dγ,γ ∈[0,1/2).
• The above assumption justifies the prefactor of √1
M
instead of M1 : since v
m
are almost orthogonal and
each f (t) is assumed not to have the first Hermite coefficient, f (v⊤x) are weakly dependent mean-zero
m m m
variables. Thus the scaling prefactor should be √1 for the output scaling to be Θ(1) due to CLT.
M
3 Complexity of Gradient-based Training
Neural Network Architecture. In this section we show that the additive model class specified in Sec-
tion2canbeefficientlylearnedviagradient-basedtrainingofaneuralnetwork. Specifically,weconsiderthe
following two-layer network with trainable parameters Θ=(a ,w ,b )J ∈R(1+d+1)×J:
j j j j=1
J
1 (cid:88)
f = a σ (w⊤x+b ). (3.1)
Θ J j j j j
j=1
4Algorithm 1: Gradient-based training of two-layer neural network
Input : Learning rates ηt, regularization parameter λ, sample size T , T , initialization scale C .
1 2 b
Initialize w0 ∼Unif(Sd−1(1)), a ∼Unif{±1}.
j j
Phase I: normalized SGD on first-layer parameters
for t=0 to T −1 do
1
Draw new sample (xt,yt).
wt+1 ←wt +ηtyt∇˜ f (xt),
j j w (aj,bj,w jt)J
j=1
wt+1 ←wt+1/∥wt+1∥, (j =1,...,J).
j j j
end
Initialize b ∼Unif([−C ,C ]), and let wˆ ←δ wT1 (δ ∼Unif({±1}))
j b b j j j j
Phase II: Convex optimization for second-layer parameters
Draw new samples (xt,yt)T1+T2−1.
t=T1
aˆ←argmin 1 (cid:80)T1+T2−1(cid:0) f (xt)−yt(cid:1)2 +λ¯∥a∥r, (r =1 or 2).
a∈RJ T2 t=T1 (aj,bj,wˆj)J j=1 r
Output: Prediction function x→f (x) with Θˆ =(aˆ ,wˆ ,b )J .
Θˆ j j j j=1
For each neuron, we define the Hermite expansion of a σ (·+b ) as a σ (·+b ) =
(cid:80)∞
β He (·); note
j j j j j j i=0 j,i i
that the Hermite coefficient β may differ across neurons. To ensure a descent path from weak recovery to
j,i
strongrecovery, weneedthefollowingtechnicalconditionontheHermitecoefficients: foreachtaskf , there
i
exist some neurons j such that
α β >0 for i=p, and α β ≥0 for p<i≤q. (3.2)
m,i j,i m,i j,i
Note that (3.2) is automatically satisfied in the well-specified setting, i.e., the student and teacher models
share the same nonlinearity as in [BAGJ21]. In the misspecified setting, such condition has been directly
assumedin[MHWSE24]. WedeferfurtherdiscussionstoAppendixB.2. Below,wegivetwoconcretesettings
where (3.2) hold despite the link mismatch.
Assumption 3 (Activation function). We consider the nonlinearity of the student model (3.1) and the link
functions {f }M that satisfy one of the following:
i i=1
1. σ is a randomized polynomial activation defined in Appendix B.2.2, and f satisfies Assumption 1.
j i
2. σ is the ReLU activation function, and we additionally require that for each f , all the non-zero Hermite
j i
coefficients α have the same sign.
m,i
In both settings, we utilize the “diversity” of student nonlinearities to deduce that when the network
width J is sufficiently large (quantified in Theorem 1), a subset of neurons can achieve alignment with each
target task even though the link functions are unknown. Similar use of overparameterization and diverse
activation functions to handle link misspecification also appeared in [BBSS22, BES+23].
Optimization procedure. The training algorithm is presented in Algorithm 1. First, the first-layer
parameters are trained to minimize the correlation loss L = −yf (x). We use the correlation loss to
Θ
ignoretheinteractionbetweenneuronsduringthefirst-layertraining; analogousstrategiesappearedinprior
analyses of feature learning, for example, by the “one-step” analysis [BES+22, DLS22] or by considering the
squared loss with sufficiently small second-layer initialization [AAM23]. Similar to [BAGJ21, DNGL23], we
use the spherical gradient ∇˜ f(w) := (I −ww⊤)∇ f(w), where ∇ denotes the Euclidean gradient. We
w w w
note that for σ =ReLU, we require an additional sign randomization step at the end of Phase I, to enhance
the expressivity of the feature map for the second-layer training.
After T steps, we can show that for each sub-problem there exist some students neurons aligning with
1
the class direction v . Then, we train the second-layer parameters (which is a convex problem) with either
m
of L2 or L1 regularization. While ridge regression is easier to implement, L1-regularization gives the better
generalization error due to the induced sparsity; more specifically, because not all neurons are aligned with
f afterfirst-layertraining,andredundantneuronsmightincreasethecomplexityofthenetwork,weneedL1
i
regularization to efficiently single out neurons that succeeded in aligning with one of the single-index tasks.
5Now we present our main theorem for gradient-based training of neural networks.
Theorem 1. Under Assumptions 1 2, and 3, take the number of neurons J = Θ˜(MCp+1 2ε−1), the number
of steps for first-layer training T
1
=Θ˜(Mdp−1∨Mdε−2∨M25ε−3), and the number of steps for second-layer
training (i) when r = 2 (ridge) T
2
= Θ˜(MCpε−2), and (ii) when r = 1 (LASSO) T
2
= Θ˜(M1+ιε−2(1+ι))
for arbitrary fixed ι > 0. Then, under appropriate choices of ηt and λ, with probability 1−o (1) over the
d
randomness of dataset and parameter initialization, Algorithm 1 outputs f (x) such that
Θˆ
E x∼N(0,Id)(cid:2)(cid:12) (cid:12)f ∗(x)−f Θˆ(x)(cid:12) (cid:12)(cid:3) ≤ε.
DuetotheonlineSGDupdate,theruntimecomplexityT +T directlytranslatestoasamplecomplexity
1 2
ofn=O˜(Mdp−1)(focusingonthetermwhoseexponentdependsontheinformationexponentp). Wemake
the following remarks on the obtained sample complexity.
• Comparison with prior results. For single-index model with known link function, [BAGJ21] proved
that the online SGD algorithm learns the degree-p Hermite polynomial with O˜(dp−1) samples; whereas
for the misspecified setting (generic and unknown f ), a sample complexity of n = O˜(dp) has been
m
established in [BBSS22]. Our bound with M = 1 matches the O˜(dp−1) complexity in [BAGJ21] despite
the link misspecification (under the additional restriction that f is polynomial as in [DLS22]).
m
• Superiority over kernel methods. By a standard dimension argument [KMS20, HSSVG21, AAM22],
we know that kernel methods (including neural networks in the lazy regime [JGH18]) require n ≳ dq
samples to learn degree-q polynomials in Rd. Since M <d and p≤q, the sample complexity of gradient-
based learning is always better than that achieved by kernels, due to the presence of feature learning.
• The role of additive structure. Withoutstructuralassumptions,learninganM-indexmodelofdegree
q requiresΩ(Mq)samples. Incontrast,inourboundtheexponentinthedimensiondoesnotdependonq,
and the exponent of M is independent of p and q; hence we achieve better complexity when q is large and
M diverging with dimensionality. This illustrates the benefit of the additive structure in Assumption 1.
3.1 Outline of Theoretical Analysis
3.1.1 Training of the first layer
We provide a proof sketch of the first-layer training, where the goal is to show that starting from random
initialization, a subset of neurons will achieve significant alignment with one of the target directions v .
m
The following lemma establishes that while the initial correlation with any target direction is small, student
neurons may have a constant-factor difference in the magnitude of alignment.
Lemma 2. Consider the network per Algorithm 1 and the hyperparameters per Theorem 1. Then with high
probability, for each m, there exist at least J
min
=Ω˜(M21ε−1) neurons at random initialization satisfying
(cid:12) (cid:12) 1
w j⊤v m ≥ mm ′̸=ax m(cid:12) (cid:12) (cid:12)ββ mm ′, ,p p(cid:12) (cid:12) (cid:12)p−2 |w j⊤v m′|+Ω˜(d−1/2).
Next we show that this small difference in the initial alignment is amplified during the online SGD
update, so that the student neurons will eventually specialize to the target direction v with largest overlap
m
at random initialization. Consider the dynamics of one neuron w , the update of which is written as
j
w jt+1 = ∥w wj jt t+ +η ηj jt ty yt ta aj jσ σ′ ′( (w wj jt t⊤ ⊤x xt t+ +b b) )( (I I− −w wj jt tw wj jt t⊤ ⊤) )x xt
t∥
≈w jt +η jt√1
M
(cid:80)M m=1α j,pβ m,p(v m⊤w jt)p−1v m+Zt,
where Zt is a mean-zero random variable corresponding to the SGD noise, and the approximation step is
due to Hermite expansion in which we ignored the effect of normalization for simplicity. For projection onto
a specific target direction v , we have the following estimate,
m
α β
v⊤wt+1 ≈v⊤wt +ηt j√,p m,p(v⊤wt)p−1+ηtv⊤Zt.
m j m j j M m j j m
6Following [BAGJ21], by using the optimal step size η jt = Θ˜(M−1 2d−p 2), the SGD dynamics approximately
follows the ODE:
d α β v⊤w0
v⊤wt ≈η j√,p m,p(v⊤wt)p−1 ⇒ v⊤wt ≈ m j .
dt m j M m j m j (cid:16) 1− η(p−2)αj,pβ √m,p(v m⊤w j0)p−2 t(cid:17)
M
This means that v⊤wt ≈ d−1/2 for a long period, but the SGD dynamics rapidly escapes the high-entropy
m j √
equator and achieves nontrivial overlap (i.e., weak recovery) just before the critical time t := Mη−1(p−
j
2)−1α−1β−1 (v⊤w0)−(p−2) =O˜(Mdp−1), as specified by the following lemma.
j,p m,p m j
Lemma 3. Consider the SGD dynamics with η jt = Θ˜(M− 21d−p 2). There exists some time t
0
= O˜(Mdp−1)
and small constant c=Θ˜(1), such that ⟨w jt0,v m⟩≥c, and |v m⊤w jt0|≤cM−1 2 for all m=2,...,M.
After a nontrivial overlap with v is obtained, we continue to run online SGD to further amplify the
m
alignment. A technical challenge here is that as the student neuron start to develop alignment with one
direction v , the influence from the other m−1 directions is no longer negligible, while the signal from v
m m
gets smaller because of the projection
(1−wtwt⊤
); this complication is addressed in Lemma 23.
Finally,inLemma26weprovealocalconvergenceresult(analogousto[ZGJ21,AS21])whichentailsthat
foreachtaskdirectionv ,thereexistsomestudentneuronsthatthatlocalizetothetask,i.e.,⟨v ,w ⟩>1−ε.
m m j
Here we exploit the local convexity (more specifically, L(cid:32) ojasiewicz condition) of the loss landscape due to
the small overlap between the single-index tasks as specified in Assumption 2. The following proposition
describes the configuration of parameters after first-layer training. Here, ε˜indicates the level of alignment,
which is later set to ε˜=Θ˜(M−1 2ε), where ε is the desired final generalization error E x[|f aˆ(x)−f ∗(x)|]≲ε.
Lemma 4 (Informal). Take T
1,1
=Θ˜(Mdp−1), T
1,2
=Θ˜(Mdp 2), T
1,3
=Θ˜(ε˜−2Md∨Mε˜−3), and the number
of neurons as J ≳J minMCplogd. Suppose that |v m⊤ ′v m|=O˜(M−1) for all m′ ̸=m, and M =O˜(d1/2). Then
under appropriate learning rate ηt, with high probability, for each class m, there exist at least J neurons
min
that achieve strong recovery v⊤wT1 ≥1−ε˜.
m j
3.1.2 Training of the second layer
Oncelocalizationtoeachtaskisachieved,weoptimizethesecond-layerparameterswithL2-orL1-regularization.
Becausetheobjectiveisconvex,gradient-basedoptimizationcanefficientlyminimizetheempiricalloss. The
learnabilityguaranteefollowsfromstandardanalysisanalogoustothatin[AAM22,DLS22,BES+22],where
we construct a “certificate” second-layer a∗ ∈RJ that achieves small loss
(cid:16) (cid:17)2
E f (x)− 1 (cid:80)J a∗σ(w⊤x+b ) ≤ε
x∼N(0,Id) ∗ J j=1 j j j
for r = 1,2. Then, the population loss of the regularized empirical risk minimizer can be bounded via
standard Rademacher complexity argument, using the norm ∥a∗∥r.
r
Observe that in Theorem 1, by using L1-regularization, we can avoid the dependency on C determined
p
bytheHermitecoefficientsoff . Thisisbecauseifsomef istoolarge,morestudentsneuronsarerequired
m m
to satisfy the initial alignment condition (Lemma 2). Since not all neurons are guaranteed to align with
the target directions, the sparsity-inducing L1-regularization allows us to ignore the redundant neurons and
hence obtain better generalization error rate.
Implication for fine-tuning. L1-regularizationisalsousefulforefficientfine-tuningonadifferentdown-
stream task, in which a subset of learned features of size M˜ is used to define the target function: f˜(x) =
∗
M˜−1/2(cid:80)M˜
g (v⊤x), whereg ,g ,...,g arelinkfunctionsthatmaydifferfromthetrainingobservations.
m=1 m m 1 2 m
In this setting, retraining the second-layer with L1-regularization requires O˜(M˜ε−2) sample to achieve an
L1-error of ε, which is especially useful when the number of the downstream features is small M˜ ≪ M.
This indeed has connections to practical fine-tuning where sparsity is induced to extract relevant features,
as seen in localization of skills [DDH+21, WWZ+22, PSZA23] (see also [EHO+22, BTB+23] in the context
of mechanistic interpretability) and LoRA [HSW+21].
7Remark. Prior theoretical results on fine-tuning & transfer learning for downstream tasks typically showed
that gradient-based pretraining “localizes” the neural network parameters to a low-dimensional subspace
spanned by the target functions (tasks), which enables efficient adaptation [DHK+20, DLS22, CHS+23];
for instance, fine-tuning for a degree-q polynomial task in M-dimensional subspace requires n≳Mq samples
[DLS22]. However, when the collection of tasks becomes sufficiently diverse (i.e. M diverges with dimension-
ality), which is the relevant regime for large-scale pretraining, the benefit of low-dimensional representation
diminishes. In contrast, in our setting we prove a different kind of localization, where each neuron aligns
with a different v depending on the initialization; this leads to efficient fine-tuning despite M being large.
m
3.2 Numerical Illustration
Weconductnumericalexperimentstoillustratethefeaturelearningprocessforadditivemodelsviagradient
descent on a two-layer network. The target function is an additive model (1.1), where we set M = 16 and
d = 64, f (x) = He (x), and {v }M are the canonical basis vectors. The student network is a two-
m 3 m m=1
layer ReLU network (3.1), where J = 8192 and weights are initialized as w0 ∼ Sd−1, a0 ∼ Unif{±1}, and
j j
b0 ∼Unif([−1,1]), respectively. First, we train the first-layer parameters via online SGD for T =106 steps.
j
The initial step size is chosen as η0 =0.3, and from T′ =T/2, we anneal the step size as ηt = η0 .
(t/T′)2
Figure 1: Alignment between student neurons and true signals v and v , before (blue) and after (purple) SGD
1 2
training. Left: neural network optimized by online SGD (Algorithm 1), Right: neural network in the NTK regime.
In Figure 1 we plot the alignment between the student neurons and the true signal: the horizontal
axis represents the alignment between w and v , that is, ⟨w ,v ⟩/∥w ∥, and the vertical axis represents
j 1 j 1 j
⟨w ,v ⟩/∥w ∥. Forcomparison, wealsotrainanetworkintheNTKregime([JGH18]), wherethemagnitude
j 2 j √
of second layer is Θ(1/ J) and feature learning is suppressed [YH20]. We observe that, via Algorithm 1,
a subset of student neurons almost perfectly aligned with one of the true signal directions (see Left figure).
This is in sharp contrast with the NTK model (Right figure) where no alignment is observed.
4 Statistical Query Lower Bounds
In Section 3 we showed that two-layer ReLU network optimized via online SGD can learn a representative
subset of the additive model class (1.1) with polynomial sample complexity. This being said, due to the
introduceddiversitycondition(Assumption2),itisnotcleariftheresultingfunctionclassisstillintrinsically
hard to learn. To understand the fundamental complexity of the problem we are addressing, in this section
wepresentseveralcomputationallowerbounds. Specifically,weconsiderthestatisticalquerylearner[Kea98,
BF02, DKKZ20], which submits a function of x and y, and receives its expectation within a tolerance τ.
The question we ask is how many accurate queries are needed to learn the target function.
4.1 Correlational Statistical Query
First, we derive lower bounds for the correlational statistical query (CSQ) learner, which is an important
subclassofSQlearnersoftendiscussedinthecontextoffeaturelearningofneuralnetworks[DLS22,AAM23].
8For a function g: X → R with ∥g∥ = 1 and parameter τ, a correlational SQ oracle CSQ(g,τ) returns
L2
E [yg(x)]+ε, where ε is an arbitrary (potentially adversarial) noise that takes value in ε∈[−τ,τ].
x,y
TheCSQlearnercanmodelgradient-basedtrainingasfollows: foraneuralnetworkf ,theSGDupdate
Θ
of the parameters with the minibatch size b using the squared loss is written as
b b b
1(cid:88) 2η (cid:88) 2η (cid:88)
Θt+1 ←Θt−ηt∇ (yt−f (xt))2 =Θt− yt∇ f (xt)+ f (xt)∇ f (xt).
Θb i Θt i b i Θ Θt i b Θt i Θ Θt i
i=1 i=1 i=1
Here, the network gains information of the true function from the correlation term 2η (cid:80)b yt∇ f (xt).
b i=1 i Θ Θt i
Roughly speaking, since each coordinate of the gradient concentrates around the expectation with O(√1 )
b
fluctuation, the noisy correlational query can be connected to gradient-based training by matching the
toleranceτ withtheconcentrationerror √1 . WenotethatthereisagapbetweenCSQandSGDupdatedue
b
tothedifferentnoisestructure,see[AAM23,Remark6];nevertheless,alowerboundonCSQlearnerservesas
abaselinecomparisonforthesamplecomplexityofSGDlearninginrecentworks[DLS22,ABA22,DNGL23].
We obtain the following CSQ lower bounds with different dependencies on the target L2 error and the
number of queries. Note that both bounds imply the sample complexity of Ω˜(Mdp 2) under the standard
τ ≈n−1/2 concentration heuristic.
Theorem 5 (CSQ lower bound). For any p≥1, ς >0 and C >0, there exists a problem class F ⊂Fp,p
d,M,ς
satisfyingAssumptions1and2suchthataCSQlearnerusingQqueries(outputtingfˆ)requiresthefollowing
to learn a random choice f ∼F:
∗
(a) tolerance of
τ ≲
(logQd)p
4 ,
M1 2dp
4
otherwise, we have ∥f −fˆ∥2 ≳1/M with probability at least 1−O(d−C).
∗ L2
(b) tolerance of
τ ≲
Q1 2(logdQ)p 4+1
,
M1 2dp
4
otherwise, we have ∥f −fˆ∥2 ≳1 with probability at least 1−O(d−C).
∗ L2
Recall that the CSQ lower bound for single-index model translates to a sample complexity of n ≳ dp 2;
intuitively, our theorem thus implies that for a CSQ algorithm, the difficulty of learning one additive model
with M tasks is the same as learning M single-index models separately. To establish this lower bound, we
construct a function class F by choosing f = He and randomly sampling the directions v from some
m p m
set on sphere S ∈ Sd−1. Here we briefly explain the difference between the two statements, both implying
the same dependence on the number of tasks M and ambient dimensionality d, but the target L2 error and
dependence on the number of queries Q differ.
Remark. The target error of the first lower bound is Ω( 1 ), and the dependency on the number of queries
M
is logarithmic; this is obtained by a straightforward extension of [DLS22] which handles single-index f .
∗
Ontheotherhand,establishingthelatterboundwithΩ(1)error,whichimpliesthefailuretoidentifyacon-
stant fraction of {v 1,··· ,v M}, is nontrivial. This is because in the additive setting √1
M
(cid:80)M
m=1
√1 p!He p(v m⊤x),
the query is not known beforehand, and hence the (adversarial) oracle need to simultaneously prevent the
learningofasmanydirectionsfromv 1,v 2,...,v
M
aspossible;whereasinthesingle-indexsetting √1 p!He p(v 1⊤x),
the oracle only need to “hide” one direction. Consequently, we cannot directly connect the identification of
Ω(1)-fraction of target directions in the additive setting to the CSQ lower bound for single-index model. To
overcome this issue, we introduce a sub-class of CSQ termed noisy correlational statistical query, which adds
a random (instead of adversarial) noise to the expected query. For this query model, proving the lower bound
for learning single-index models can indeed be translated to our additive setting. However, because the noise
is random, the dependency on the number of queries no longer logarithmic but Q21.
94.2 Full Statistical Query
Going beyond CSQ, in this subsection we present a lower bound for full SQ algorithms. For a function
g: X ×Y →[−1,1] and parameter τ, a full statistical query oracle SQ(g,τ) returns E [g(x,y)]+ε, where
x,y
ε is an arbitrary noise that is bounded by the tolerance τ. Note that unlike the CSQ learner, here the query
function can apply transformations to the target label y and hence reduce the computational complexity.
ThefullSQsettingispartiallymotivatedbytheobservationthatforsparsepolynomialswithaconstant
number of relevant dimensions, a more sample-efficient SQ algorithm that departs from gradient-based
training is available: [CM20] showed that general multi-index polynomials of degree q can be learned with
O˜(d) samples. The core of their analysis is a transformation of the label y that reduces the leap complexity
[AAM23]toatmost2(similartransformationsalsoappearedincontextofphaseretrieval[MM18,BKM+19]);
this enables a warm-start from the direction that overlaps with the relevant dimensions of f , from which
∗
point projected gradient descent can efficiently learn the target.
One might therefore wonder if the polynomial dimension dependence in Theorem 5 is merely an artifact
ofrestrictiontocorrelationalqueries. However,wedemonstratethatforgeneralSQalgorithms,ouradditive
model class is also challenging to learn despite the additive structure and near-orthogonality constraint. We
consider the scaling where M increases with d, i.e., M ≍dγ with some γ >0, and show that the dimension
exponent in the statistical complexity can be arbitrarily large for any fixed γ, by varying p,q =O (1).
d
Theorem 6 (SQ lower bound). Fix 0 < γ ≤ p and ς > 0 arbitrarily, and consider the number of tasks
M = Θ(dγ). For any ρ > 0, there exist constants p,q = O (1) depending only on γ and ρ, and a problem
d
class F ⊂ Fp,q satisfying Assumptions 1 and 2 for which an SQ learner outputting fˆ from Q queries
d,M,ς
requires the following tolerance to learn a random choice of f ∼F,
∗
τ ≲d−ρ,
√
otherwise, we have ∥f −fˆ∥2 ≳1 with probability at least 1−Qe−Ω( d).
∗ L2
This lower bound illustrates the intrinsic difficulty of our large-M setting compared to the well-studied
low-dimensional polynomial regression. For the latter, gradient-based training of polynomial-width network
can achieve low loss using n = Ω(dp−1) samples [AAM23], and the CSQ lower bound gives a Ω(dp/2)
complexity [DLS22]; however, by applying nonlinear transformation to the labels, an efficient SQ algorithm
can achieve O˜(d) complexity [CM20]. In contrast, while our additive model has O(Md) total parameters to
be estimated, the dimension dependence in the SQ lower bound can be arbitrarily larger than M times the
single-index complexity. Therefore, unlike the CSQ lower bound (Theorem 5), Theorem 6 suggests that for
an SQ algorithm, learning one additive model with M tasks is more difficult than learning M single-index
models separately. See Section 4.2.2 for more discussions.
Remark. A concurrent work [DPVLB24] established an SQ lower bound for single-index model that implies
a sample complexity of n≳dk∗/2, where k∗ is a prescribed generative exponent that can be made arbitrarily
large. WhilethisalsoentailstheexistenceoflinkfunctionsnotlearnablebySQalgorithmswithO˜(d)samples,
the source of computational hardness is fundamentally different than our setting: [DPVLB24] constructed
“hard” non-polynomial link functions that preserve high information exponent after arbitrary transforma-
tions; in contrast, we consider polynomial link functions which have generative exponent k∗ ≤2, so the lower
bound relies on the additive structure with a large number of tasks M =ω(1).
4.2.1 Outline of Theoretical Analysis
In the proof of the CSQ lower bounds, Hermite polynomials were used to construct the “worst-case” target
functions [DLS22]. Intuitively, this is because higher-order Hermite polynomials have small L2-correlation
underGaussianinput,andhencecorrelationalqueriesareineffective. FortheSQlearner,whichgoesbeyond
correlational queries, a hard function class should also “hide” information from nonlinear transformations
to the target labels. As we will see, for our additive model, the class of target functions should main-
tain orthogonality with respect to queries after polynomial transformations. We refer to this property as
superorthogonality, and the following proposition shows the existence of such functions.
10Proposition 7 (Superorthogonalpolynomials). For any K and L, there exists a polynomial f :R→R that
is not identically zero and satisfies the following:
(cid:90)
(f(x))kHe (x)e−x2/2dx=0,
l
for every 1≤k ≤K and 1≤l≤L.
Because the proof is not constructive, it is difficult to determine the specific form of f for general K and
K. Below, we present specific examples for small K,L:
Example 1. Examples of link function f in Proposition 7 are given as follows.
(i) For K =1 and L∈N, f(x)=He (x).
L+1
(ii) ForK =L=2,f(x)=He (x)− 4 He (x)+ 11 He (x)− 19 He (x)+ 311 He (x)− 719 He (x)+
4 15 6 280 8 4725 10 997920 12 37837800 14
(cid:113)
14297 He (x)− 35369 He (x)+( 35369 − 1 11163552839)He (x).
15567552000 16 1042053012000 18 41682120480000 83364240960000 38 20
Remark. While the K = 1 case follows from the orthogonality of Hermite polynomials, the K = L = 2
case is already nontrivial — it implies the existence of link functions with information exponent p > 2,
and furthermore, squaring the function cannot reduce the information exponent to 1 or 2; note that for
f(x)=He (x), the information exponent of f2 is at most 2.
k
Inthelowerboundconstructionweusethepolynomialf fromtheabovelemmawithsuitablylargeK and
L. Specifically,weletf =f forallmandsamplev fromasetS ⊂Sd−1 withalmostorthogonalelements.
m m
We prove that the two target functions √1 (cid:80)M f(v⊤x) and √1 (cid:80)M f(v′ ⊤x), where {v′ }M is an
M m=1 m M m=1 m m m=1
independent copy of {v }M , cannot be distinguished by SQ unless the tolerance is below d−ρ. To show
m m=1
this, we Taylor expand the target function and perform an entry-wise swapping of f(v⊤x) with f(v′ ⊤x), in
m m
which smoothness of the function is entailed by additive Gaussian noise. Specifically, for δ ≪1, we have
K
(cid:88)
E [g(x,z+δ+ε)]= a (x,z)δk+O(δK+1),
ε∼N(0,ς2) k
k=0
(cid:16) (cid:17)
where a (x,z)= 1 (cid:82) g(x,w) dk e−(w−z)2/2ς2 dw =O(1) (see Lemma 36 for derivation). Below we heuris-
k k! dzk
tically demonstrate how swapping is conducted for m=1:
(cid:104) (cid:16) (cid:17)(cid:105)
E g x,√1 (cid:80)M f(v⊤x)+ε
M m=1 m
( ≈i) E(cid:104) g(cid:16) x,√1
M
(cid:80)M m=2f(v m⊤x)+ε(cid:17)(cid:105) +(cid:80)K k=1E(cid:104) a k(cid:16) x,√1
M
(cid:80)M m=2f(v m⊤x)(cid:17) fk M(v 1⊤ 2ix)(cid:105)
( ≈ii) E(cid:104) g(cid:16) x,√1
M
(cid:80)M m=2f(v m⊤x)+ε(cid:17)(cid:105) +(cid:80)K k=1E(cid:104) a k(cid:16) x,√1
M
(cid:80)M m=2f(v m⊤x)(cid:17)(cid:105)E[fk M(v 2i1⊤x)]
(i ≈ii) E(cid:104) g(cid:16) x,√1 (cid:80)M−1f(v⊤x)+ √1 f(v′⊤x)+ε(cid:17)(cid:105) ,
M m=1 m M 1
where (i) follows from Taylor expansion, (ii) is due to fk being orthogonal to He ,...,He and hence its
1 L
correlationtoa (whichdoesnotcontaininformationofv )isapproximatedbyitsHe component,and(iii)
k 1 0
is obtained by swapping E[fk(v ⊤x)] and E[fk(v′⊤x)] and using the argument in reverse.
1 1
4.2.2 Statistical-to-computational Gap
Theadditivemodel(1.1)containsM directionsv ,v ,...v ∈RdandM univariatelinkfunctionsf ,f ,...f :
1 2 M 1 2 M
R → R to be estimated; therefore, we intuitively expect a sample size of n ≳ Md to be information-
theoretically sufficient to learn this function class. Indeed, following [Suz18], it is easy to check that the
covering number of width-J neural network is logN(δ,{1 (cid:80)M a σ (w⊤·+b )},∥·∥ )≲Jd(logJ+logd).
J j=1 j j j j ∞
11IfwetakeJ =Mqandletσ =He (·)((q−1)M <j ≤qM),thenetwork 1 (cid:80)M a σ (w⊤·+b )canperfectly
j i J j=1 j j j j
approximate the additive model (1.1). Therefore, by applying a standard generalization error bound (e.g.,
see [SH20, Lemma 4]), we can upper bound the squared loss by 1 logN(δ,{1 (cid:80)M a σ (w⊤·+b )},∥·∥ ),
n J j=1 j j j j ∞
which yields the following proposition (the detailed derivation of which we omit due to the standard proof).
Proposition 8. For the additive model (1.1) with any p and q, there exists an (computationally inefficient)
algorithm that returns a function fˆwith L2-error of ε using n=O˜(Mdε−2) samples.
The procedure in Proposition 8 involves finding the empirical risk minimizer of a neural network, which
can be computationally infeasible in polynomial time [BR88, ABMM16]. Nevertheless, the existence of a
statistically efficient algorithm that learns (1.1) in n ≳ Md samples suggests a statistical-to-computational
gap under the SQ class (with polynomial compute; see [DH22, DPVLB24] for related discussions), since the
lower bound in Theorem 6 implies a worse sample complexity of n≳(Md)ρ for SQ learners, where ρ can be
made arbitrarily large by varying p and q.
Importantly, the same statistical-computational gap is not present in the finite-M setting (in terms of
dimension dependence), due to the restriction of f being polynomial; specifically, when M = O (1), there
∗ d
exists an efficient SQ algorithm that learns arbitrary multi-index polynomials using n = O˜ (d) samples
d
[CM20], and in the single-index setting, polynomial link functions have generative exponent at most 2
[DPVLB24],andhencetheSQlowerboundonlyimpliesthatasamplesizeofn≳disnecessary. Incontrast,
in our large-M setting, the sample complexity of SQ algorithms may have large polynomial dimension
dependence, despite the link functions being polynomial. The key observation is that when the number of
tasks M is diverging, the nonlinear label transformation cannot obtain higher-order exponentiation of the
individualsingle-indextasks,whichisemployedbySQlearnerssuchas[MM18,CM20];hencetheinformation
exponent of the link function may still be large after the nonlinear transformation.
5 Conclusion and Future Directions
In this work, we studied the learning of additive models, where the number of (diverse) single-index tasks
M grows with the dimensionality d. We showed that a layer-wise SGD algorithm achieves O˜(Mdp−1)
samplecomplexity,andasubsetoffirst-layerneuronslocalize intoeachadditivetaskbyachievingsignificant
alignment. Wealsoinvestigatedthecomputationalandstatisticalbarrierinlearningtheadditivemodelclass
by establishing lower bounds for both Correlational SQ and Full SQ learners. Our lower bound suggests a
computational-to-statistical gap under the SQ class, which highlights the fundamental difference between
our large M setting and the previously studied finite-M multi-index regression.
We highlight a few future directions. First observe that there is a gap between our complexity upper
bound for gradient-based training and the computational lower bounds, and one would hope to design more
efficient algorithms that close such gap. It is also worth noting that in certain regimes, SGD on the squared
lossmayoutperformtheCSQcomplexityduetothepresenceofhigher-order(non-correlational)information,
as demonstrated in very recent works [DTA+24, LOSW24, ADK+24]; an interesting direction is to examine
if similar mechanisms can improve the statistical efficiency of SGD in our setting. It is also important to
relax the near-orthogonality condition for the true signals, and explore how our derived upper and lower
bounds are affected. Last but not least, we may theoretically analyze task localization in more complicated
architectures beyond two-layer neural network, such as multi-head attention [CSWY24].
Acknowledgement
The authors thank Alberto Bietti, Satoshi Hayakawa, Isao Ishikawa, Jason D. Lee, Eshaan Nichani, and
Atsushi Nitanda for insightful discussions. KO was partially supported by JST, ACT-X Grant Number JP-
MJAX23C4. TSwaspartiallysupportedbyJSPSKAKENHI(24K02905)andJSTCREST(JPMJCR2015).
12References
[AAM22] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. The merged-staircase prop-
erty: anecessaryandnearlysufficientconditionforsgdlearningofsparsefunctionsontwo-layer
neural networks. In Conference on Learning Theory, pages 4782–4887. PMLR, 2022.
[AAM23] Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. SGD learning on neural net-
works: leapcomplexityandsaddle-to-saddledynamics. InTheThirtySixthAnnualConference
on Learning Theory, pages 2552–2623. PMLR, 2023.
[ABA22] EmmanuelAbbeandEnricBoix-Adsera. Onthenon-universalityofdeeplearning: quantifying
the cost of symmetry. Advances in Neural Information Processing Systems, 35:17188–17201,
2022.
[ABMM16] Raman Arora, Amitabh Basu, Poorya Mianjy, and Anirbit Mukherjee. Understanding deep
neural networks with rectified linear units. arXiv preprint arXiv:1611.01491, 2016.
[ADK+24] Luca Arnaboldi, Yatin Dandi, Florent Krzakala, Luca Pesce, and Ludovic Stephan. Repetita
iuvant: Data repetition allows sgd to learn high-dimensional multi-index functions. arXiv
preprint arXiv:2405.15459, 2024.
[AG23] Sanjeev Arora and Anirudh Goyal. A theory for emergence of complex skills in language
models. arXiv preprint arXiv:2307.15936, 2023.
[Akh90] Naum Il’ich Akhiezer. Elements of the theory of elliptic functions, volume 79. American
Mathematical Soc., 1990.
[AMF+21] Rishabh Agarwal, Levi Melnick, Nicholas Frosst, Xuezhou Zhang, Ben Lengerich, Rich Caru-
ana, and Geoffrey E Hinton. Neural additive models: Interpretable machine learning with
neural nets. Advances in neural information processing systems, 34:4699–4711, 2021.
[APVZ14] Alexandr Andoni, Rina Panigrahy, Gregory Valiant, and Li Zhang. Learning sparse polyno-
mial functions. In Proceedings of the twenty-fifth annual ACM-SIAM symposium on Discrete
algorithms, pages 500–510. SIAM, 2014.
[AS21] Shunta Akiyama and Taiji Suzuki. On learnability via gradient method for two-layer relu
neural networks in teacher-student setting. In International Conference on Machine Learning,
pages 152–162. PMLR, 2021.
[Bac08] Francis R Bach. Consistency of the group lasso and multiple kernel learning. Journal of
Machine Learning Research, 9(6), 2008.
[Bac17] Francis Bach. Breaking the curse of dimensionality with convex neural networks. The Journal
of Machine Learning Research, 18(1):629–681, 2017.
[BAGJ21] Gerard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient de-
scent on non-convex losses from high-dimensional inference. The Journal of Machine Learning
Research, 22(1):4788–4838, 2021.
[BBPV23] Alberto Bietti, Joan Bruna, and Loucas Pillaud-Vivien. On learning Gaussian multi-index
models with gradient flow. arXiv preprint arXiv:2310.19793, 2023.
[BBSS22] AlbertoBietti,JoanBruna,ClaytonSanford,andMinJaeSong. Learningsingle-indexmodels
with shallow neural networks. Advances in Neural Information Processing Systems, 35:9768–
9783, 2022.
[BES+22] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-
dimensional asymptotics of feature learning: How one gradient step improves the representa-
tion. Advances in Neural Information Processing Systems, 35:37932–37946, 2022.
[BES+23] Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, and Denny Wu. Learning in the
presenceoflow-dimensionalstructure: Aspikedrandommatrixperspective. InThirty-seventh
Conference on Neural Information Processing Systems, 2023.
13[BF02] Nader H Bshouty and Vitaly Feldman. On using extended statistical queries to avoid mem-
bership queries. Journal of Machine Learning Research, 2(Feb):359–395, 2002.
[BKM+19] Jean Barbier, Florent Krzakala, Nicolas Macris, L´eo Miolane, and Lenka Zdeborov´a. Optimal
errors and phase transitions in high-dimensional generalized linear models. Proceedings of the
National Academy of Sciences, 116(12):5451–5460, 2019.
[BMZ23] Rapha¨el Berthier, Andrea Montanari, and Kangjie Zhou. Learning time-scales in two-layers
neural networks. arXiv preprint arXiv:2303.00055, 2023.
[BR88] Avrim Blum and Ronald Rivest. Training a 3-node neural network is np-complete. Advances
in neural information processing systems, 1, 1988.
[BTB+23] Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly,
Nick Turner, Cem Anil, Carson Denison, Amanda Askell, et al. Towards monosemanticity:
Decomposing language models with dictionary learning. Transformer Circuits Thread, page 2,
2023.
[CCM11] Seok-Ho Chang, Pamela C Cosman, and Laurence B Milstein. Chernoff-type bounds for the
Gaussian error function. IEEE Transactions on Communications, 59(11):2939–2944, 2011.
[CGKM22] Sitan Chen, Aravind Gollakota, Adam Klivans, and Raghu Meka. Hardness of noise-free
learning for two-hidden-layer neural networks. Advances in Neural Information Processing
Systems, 35:10709–10724, 2022.
[CHS+23] LiamCollins,HamedHassani,MahdiSoltanolkotabi,AryanMokhtari,andSanjayShakkottai.
Provable multi-task representation learning by two-layer relu neural networks. arXiv preprint
arXiv:2307.06887, 2023.
[CM20] SitanChenandRaghuMeka. Learningpolynomialsinfewrelevantdimensions. InConference
on Learning Theory, pages 1161–1227. PMLR, 2020.
[CSWY24] Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang. Training dynamics of multi-
headsoftmaxattentionforin-contextlearning: Emergence,convergence,andoptimality. arXiv
preprint arXiv:2402.19442, 2024.
[DCLT18] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of
deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,
2018.
[DDH+21] DamaiDai,LiDong,YaruHao,ZhifangSui,BaobaoChang,andFuruWei.Knowledgeneurons
in pretrained transformers. arXiv preprint arXiv:2104.08696, 2021.
[DH18] RishabhDudejaandDanielHsu.Learningsingle-indexmodelsingaussianspace.InConference
On Learning Theory, pages 1887–1930. PMLR, 2018.
[DH22] RishabhDudejaandDanielHsu. Statistical-computationaltrade-offsintensorpcaandrelated
problems via communication complexity. arXiv preprint arXiv:2204.07526, 2022.
[DHK+20] SimonSDu,WeiHu,ShamMKakade,JasonDLee,andQiLei. Few-shotlearningvialearning
the representation, provably. arXiv preprint arXiv:2002.09434, 2020.
[DKK+23] Ilias Diakonikolas, Daniel M Kane, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis. Ag-
nostically learning multi-index models with queries. arXiv preprint arXiv:2312.16616, 2023.
[DKKZ20] IliasDiakonikolas,DanielMKane,VasilisKontonis,andNikosZarifis. Algorithmsandsqlower
bounds for pac learning one-hidden-layer relu networks. In Conference on Learning Theory,
pages 1514–1539. PMLR, 2020.
[DLS22] Alexandru Damian, Jason Lee, and Mahdi Soltanolkotabi. Neural networks can learn repre-
sentationswithgradientdescent. InConference on Learning Theory,pages5413–5452.PMLR,
2022.
14[DNGL23] Alex Damian, Eshaan Nichani, Rong Ge, and Jason D. Lee. Smoothing the landscape boosts
the signal for SGD: Optimal sample complexity for learning single index models. In Thirty-
seventh Conference on Neural Information Processing Systems, 2023.
[DPVLB24] Alex Damian, Loucas Pillaud-Vivien, Jason D Lee, and Joan Bruna. The computational
complexity of learning gaussian single-index models. arXiv preprint arXiv:2403.05529, 2024.
[DTA+24] Yatin Dandi, Emanuele Troiani, Luca Arnaboldi, Luca Pesce, Lenka Zdeborov´a, and Florent
Krzakala. Thebenefitsofreusingbatchesforgradientdescentintwo-layernetworks: Breaking
the curse of information and leap exponents. arXiv preprint arXiv:2402.03220, 2024.
[EHO+22] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna
Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, et al. Toy models of
superposition. arXiv preprint arXiv:2209.10652, 2022.
[EY07] Alexandre Eremenko and Peter Yuditskii. Uniform approximation of sgn(x) by polynomials
and entire functions. Journal d Analyse Math´ematique, 101:313–324, 05 2007.
[FS81] Jerome H Friedman and Werner Stuetzle. Projection pursuit regression. Journal of the Amer-
ican statistical Association, 76(376):817–823, 1981.
[GGJ+20] Surbhi Goel, Aravind Gollakota, Zhihan Jin, Sushrut Karmalkar, and Adam Klivans. Super-
polynomial lower bounds for learning one-layer neural networks using gradient descent. In
International Conference on Machine Learning, pages 3587–3596. PMLR, 2020.
[GKS20] Ankit Garg, Neeraj Kayal, and Chandan Saha. Learning sums of powers of low-degree poly-
nomials in the non-degenerate case. In 2020 IEEE 61st Annual Symposium on Foundations of
Computer Science (FOCS), pages 889–899. IEEE, 2020.
[GKZ19] DavidGamarnik,ErenCKızılda˘g,andIliasZadik.Stationarypointsofshallowneuralnetworks
with quadratic activation function. arXiv preprint arXiv:1912.01599, 2019.
[GMMM21] Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. Linearized two-
layers neural networks in high dimension. The Annals of Statistics, 49(2):1029–1054, 2021.
[HSSVG21] Daniel Hsu, Clayton H Sanford, Rocco Servedio, and Emmanouil Vasileios Vlatakis-
Gkaragkounis. On the approximation power of two-layer networks of random relus. In Con-
ference on Learning Theory, pages 2423–2461. PMLR, 2021.
[HSW+21] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang,
Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv
preprint arXiv:2106.09685, 2021.
[HT87] TrevorHastieandRobertTibshirani. Generalizedadditivemodels: someapplications. Journal
of the American Statistical Association, 82(398):371–386, 1987.
[JGH18] Arthur Jacot, Franck Gabriel, and Cl´ement Hongler. Neural tangent kernel: Convergence and
generalizationinneuralnetworks. Advancesinneuralinformationprocessingsystems,31,2018.
[KB16] Jason M Klusowski and Andrew R Barron. Risk bounds for high-dimensional ridge function
combinations including neural networks. arXiv preprint arXiv:1607.01434, 2016.
[Kea98] Michael Kearns. Efficient noise-tolerant learning from statistical queries. Journal of the ACM
(JACM), 45(6):983–1006, 1998.
[KMS20] Pritish Kamath, Omar Montasser, and Nathan Srebro. Approximate is good enough: Prob-
abilistic variants of dimensional and margin complexity. In Conference on Learning Theory,
pages 2236–2262. PMLR, 2020.
[KSP15] Kirthevasan Kandasamy, Jeff Schneider, and Barnab´as P´oczos. High dimensional bayesian
optimisationandbanditsviaadditivemodels. InInternationalconferenceonmachinelearning,
pages 295–304. PMLR, 2015.
[LL21] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation.
arXiv preprint arXiv:2101.00190, 2021.
15[LOSW24] Jason D. Lee, Kazusato Oko, Taiji Suzuki, and Denny Wu. Neural network learns low-
dimensional polynomials with sgd near the information-theoretic limit. arXiv preprint
arXiv:2406.01581, 2024.
[Mau16] Andreas Maurer. A vector-contraction inequality for rademacher complexities. In Ronald
Ortner, Hans Ulrich Simon, and Sandra Zilles, editors, Algorithmic Learning Theory, pages
3–17, Cham, 2016. Springer International Publishing.
[MBB23] Simon Martin, Francis Bach, and Giulio Biroli. On the impact of overparameterization on
the training of a shallow neural network in high dimensions. arXiv preprint arXiv:2311.03794,
2023.
[MHPG+23] Alireza Mousavi-Hosseini, Sejun Park, Manuela Girotti, Ioannis Mitliagkas, and Murat A Er-
dogdu. Neural networks efficiently learn low-dimensional representations with SGD. In The
Eleventh International Conference on Learning Representations, 2023.
[MHWSE24] Alireza Mousavi-Hosseini, Denny Wu, Taiji Suzuki, and Murat A Erdogdu. Gradient-based
feature learning under structured data. Advances in Neural Information Processing Systems,
36, 2024.
[MM18] MarcoMondelliandAndreaMontanari. Fundamentallimitsofweakrecoverywithapplications
to phase retrieval. In Conference On Learning Theory, pages 1445–1450. PMLR, 2018.
[Muc70] Benjamin Muckenhoupt. Mean convergence of Hermite and Laguerre series. II. Transactions
of the American Mathematical Society, 147(2):433–460, 1970.
[MVEZ20] Sarao Stefano Mannelli, Eric Vanden-Eijnden, and Lenka Zdeborov´a. Optimization and gen-
eralizationofshallowneuralnetworkswithquadraticactivationfunctions. Advances in Neural
Information Processing Systems, 33:13445–13455, 2020.
[P+97] Allan Pinkus et al. Approximating by ridge functions. Surface fitting and multiresolution
methods, pages 279–292, 1997.
[PSZA23] Abhishek Panigrahi, Nikunj Saunshi, Haoyu Zhao, and Sanjeev Arora. Task-specific skill
localization in fine-tuned language models. arXiv preprint arXiv:2302.06600, 2023.
[Qi10] FengQi.Boundsfortheratiooftwogammafunctions.JournalofInequalitiesandApplications,
2010:1–84, 2010.
[Rey20] LevReyzin. Statisticalqueriesandstatisticalalgorithms: Foundationsandapplications. arXiv
preprint arXiv:2004.00557, 2020.
[RJWY12] GarveshRaskutti,MartinJWainwright,andBinYu.Minimax-optimalratesforsparseadditive
models over kernel classes via convex programming. Journal of machine learning research,
13(2), 2012.
[RLLW09] Pradeep Ravikumar, John Lafferty, Han Liu, and Larry Wasserman. Sparse additive models.
Journal of the Royal Statistical Society Series B: Statistical Methodology, 71(5):1009–1030,
2009.
[SH20] Anselm Johannes Schmidt-Hieber. Nonparametric regression using deep neural networks with
relu activation function. Annals of statistics, 48(4):1875–1897, 2020.
[SS12] Taiji Suzuki and Masashi Sugiyama. Fast learning rate of multiple kernel learning: Trade-off
between sparsity and smoothness. In Artificial Intelligence and Statistics, pages 1152–1183.
PMLR, 2012.
[Sto85] CharlesJStone. Additiveregressionandothernonparametricmodels. TheannalsofStatistics,
13(2):689–705, 1985.
[Suz18] Taiji Suzuki. Adaptivity of deep relu network for learning in besov and mixed smooth besov
spaces: optimal rate and curse of dimensionality. arXiv preprint arXiv:1810.08033, 2018.
16[Sz¨o09] Bal´azs Sz¨or´enyi. Characterizing statistical query learning: Simplified notions and proofs. In
Algorithmic Learning Theory,pages186–200,Berlin,Heidelberg,2009.SpringerBerlinHeidel-
berg.
[TLS+23] Eric Todd, Millicent L Li, Arnab Sen Sharma, Aaron Mueller, Byron C Wallace, and David
Bau. Function vectors in large language models. arXiv preprint arXiv:2310.15213, 2023.
[VSL+22] Rodrigo Veiga, Ludovic Stephan, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov´a.
Phase diagram of stochastic gradient descent in high-dimensional two-layer neural networks.
Advances in Neural Information Processing Systems, 35:23244–23255, 2022.
[VW19] Santosh Vempala and John Wilmes. Gradient descent for one-hidden-layer neural networks:
Polynomial convergence and sq lower bounds. In Conference on Learning Theory, pages 3115–
3117. PMLR, 2019.
[Wai19] MartinJ.Wainwright. High-DimensionalStatistics: ANon-AsymptoticViewpoint. Number48
inCambridgeSeriesinStatisticalandProbabilisticMathematics.CambridgeUniversityPress,
2019.
[WNL23] Zihao Wang, Eshaan Nichani, and Jason D Lee. Learning hierarchical polynomials with three-
layer neural networks. arXiv preprint arXiv:2311.13774, 2023.
[WWZ+22] Xiaozhi Wang, Kaiyue Wen, Zhengyan Zhang, Lei Hou, Zhiyuan Liu, and Juanzi Li.
Finding skill neurons in pre-trained transformer-based language models. arXiv preprint
arXiv:2211.07349, 2022.
[YH20] GregYangandEdwardJHu.Featurelearningininfinite-widthneuralnetworks.arXivpreprint
arXiv:2011.14522, 2020.
[ZGJ21] Mo Zhou, Rong Ge, and Chi Jin. A local convergence theory for mildly over-parameterized
two-layer neural network. In Conference on Learning Theory, pages 4577–4632. PMLR, 2021.
[Zol77] E Zolotarev. Application of elliptic functions to questions concerning functions more or less
deviating from zero. Notes of the Russian Science Academy, 30(5):1–71, 1877.
17Table of Contents
1 Introduction 1
1.1 Our Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.2 Related Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2 Problem Setting 3
2.1 Assumptions on Target Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3 Complexity of Gradient-based Training 4
3.1 Outline of Theoretical Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.2 Numerical Illustration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4 Statistical Query Lower Bounds 8
4.1 Correlational Statistical Query . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4.2 Full Statistical Query . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
5 Conclusion and Future Directions 12
A Preliminaries 19
A.1 Hermite Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.2 High Probability Bound on y . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
A.3 Bihari–LaSalle Inequality and Gronwall Inequality . . . . . . . . . . . . . . . . . . . . . . . . 21
A.4 Orthonormal Basis from Nearly Orthogonal Vectors . . . . . . . . . . . . . . . . . . . . . . . 22
B Proof of Gradient-based Training 24
B.1 Initialization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
B.2 Descent Path for Population Gradient . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
B.3 Decomposition of Gradient Update . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
B.4 Phase I: Weak Recovery for One Direction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
B.5 Phase II: Amplification of Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
B.6 Phase III: Strong Recovery and Localization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
B.7 Expressivity of the Trained Feature Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
B.8 Fitting the Second Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
C Proof of CSQ Lower Bounds 48
C.1 Proof of Theorem 5(a) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
C.2 Noisy CSQ and Proof of Theorem 5(b) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50
D Proof of SQ Lower Bound 53
D.1 Superorthogonal Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54
D.2 Reparameterization of Polynomials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56
D.3 Polynomial Approximation of h∗ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60
a
18A Preliminaries
A.1 Hermite Polynomials
For k ∈ Z , the k-th Hermite polynomial He : R → R is a univariate function defined as He (t) =
+ k k
et 22 dd tk ke−t 22 . The Hermite polynomials form an orthogonal basis for the Hilbert space of square-integrable
functions. We provide several basic properties of the Hermite polynomials.
Lemma 9. The Hermite polynomials satisfy the following properties:
• Derivatives:
d
He (t)=kHe (t).
dt k k−1
• Integration by parts (I):
(cid:90) He k(t)f(t)√1 e−t 22 dt=(cid:90) He k−1(t)f′(t)√1 e−t 22 dt.
2π 2π
• Integration by parts (II): For u∈Sd−1(1) and v ∈Rd,
(cid:90) He k(u⊤x)f(v⊤x)
(cid:112)
1 e−∥x 2∥2 dx=(u⊤v)(cid:90) He k−1(u⊤t)f′(v⊤x)
(cid:112)
1 e−∥x 2∥2 dx.
(2π)d (2π)d
• Orthogonality (I):
(cid:90) He k(t)He l(t)√1 e−t 22 dt=k!δ k,l.
2π
• Orthogonality (II): For u,v ∈Sd−1(1),
(cid:90) He k(u⊤x)He l(v⊤x)
(cid:112)
1 e−∥x 2∥2 dx=k!(u⊤v)kδ k,l.
(2π)d
• Hermite expansion: if f ∈R→R is square-integrable with respect to the standard Gaussian,
f(t)L =2 (cid:88)∞ α kk !He k(t), α
k
=(cid:90) f(t)He k(t)√1 2πe−t 22 dt.
k=0
A.2 High Probability Bound on y
Recall that in the definition of f ∗, we used a scaling factor of √1
M
instead of M1 in order to ensure that the
outputisoforderΘ(1)withhighprobability. ThissubsectionprovestheΘ(1)outputscalerigorously. Since
v are almost orthogonal and f (t) are mean-zero, f (v⊤x) are almost independent mean-zero variables.
m m m m
Thus we expect √1
M
(cid:80)M m=1f m(v m⊤x)→N(0,1) by central limit theorem.
Lemma 10. Let ∥v ∥=1 for all m=1,2,··· ,M. If M ≤max |v⊤v |−p and S is even, we have
m m̸=m′ m m′
(cid:34)(cid:13) (cid:13) 1 (cid:88)M (cid:13) (cid:13) (cid:35) (cid:13) (cid:13) 1 (cid:88)M (cid:13) (cid:13)S1/S
E (cid:13) (cid:13)√ f m(v m⊤x)(cid:13) (cid:13) ≤E (cid:13) (cid:13)√ f m(v m⊤x)(cid:13) (cid:13)  ≲Sq+1. (A.1)
(cid:13) M (cid:13) (cid:13) M (cid:13)
m=1 S m=1 S
Thus, by applying Lemma 11, we have
(cid:12) (cid:12)
(cid:12) (cid:12)√1 (cid:88)M
f
(v⊤x)(cid:12)
(cid:12)≲(log1/δ)q+1 =O˜(1), (A.2)
(cid:12) m m (cid:12)
(cid:12) M (cid:12)
m=1
with probability at least 1−δ.
19Proof. Recall the Hermite expansion of f : f
(t)=(cid:80)q
α He (t). We decompose the LHS of (A.1) as
m m i=p m,i i
(cid:12) (cid:12)S
(cid:12) 1 (cid:88)M (cid:12)
E (cid:12) (cid:12)√ f m(v m⊤x)(cid:12) (cid:12) 
(cid:12) M (cid:12)
m=1
≤ √ M1
S
(cid:88) E(cid:2) f m1(v m⊤ 1x)···f mS(v m⊤ Sx)(cid:3) .
(m1,···,mS)∈[M]S
≤√ M1
S
(cid:88) (cid:88) (α m1,i1···α mS,iS)E(cid:2) He i1(v m⊤ 1x)···He iS(v m⊤ Sx)(cid:3) . (A.3)
(m1,···,mS)∈[M]S(i1,···,iS)∈[q−p+1]S
We evaluate each term of (A.3). If i +···+i is odd, the term is 0. Otherwise, we bound the value of
1 S
E[He (v⊤ x)···He (v⊤ x)] recursively.
i1 m1 iS mS
For T ∈N, i=(i ,··· ,i ), and m=(m ,··· ,m ), let us define
1 T 1 T
A(T,i,m):=E[He (v⊤ x)···He (v⊤ x)],
i1 m1 iT mT
and
(cid:40) (cid:12) (cid:12) T (cid:88)−1 (cid:41)
B(T −1,i)= j =(j ,··· ,j )∈ZT−1(cid:12) j =i , j ≤i (t=1,··· ,T −1) .
1 T−1 + (cid:12) t T t t
(cid:12)
t=1
For j ∈ B(T −1,i), the multinomial coefficient for permuting a multiset of
(cid:80)T−1j
elements (where j is
t=1 t t
the multiplicity of each of the t-th element) is denoted by
((cid:80)T−1j
)!
c := t=1 t .
j (j )!···(j )!
1 T−1
By the basic property of Hermite polynomials, we have
(cid:20) ∂iT (cid:21)
A(T,i,m)=E (He (v⊤ x)···He (v⊤ x))
∂(v m⊤ Tx)iT i1 m1 iT−1 mT−1
T−1
= (cid:88) c (cid:89)(cid:2) (v⊤ v⊤ )jt P (cid:3) A(T −1,i −j,m ).
j mT mt it jt 1:T−1 1:T−1
j∈B(T−1,i) t=1
Wecanboundthecoefficientsasc ≤i !≤q!and(cid:81)T−1 P ≤qq. WealsoboundthesizeofB(T−1,i)by
j T t=1 it jt
Sq. We define I(m ,··· ,m ) as the number of m distinct in (m ,··· ,m ). Then, by recursively bounding
1 S 1 S
A, we have
(cid:18) (cid:19)pI(m1,···,mS)
A(S,(i ,··· ,i ),(m ,··· ,m ))≤(q!qqSq)S max |v⊤v | 2 . (A.4)
1 S 1 S m m′
m̸=m′
Moreover,
(cid:18) (cid:19)pI(m1,···,mS)
(cid:88) 2
max |v⊤v |
m m′
m̸=m′
(m1,···,mS)∈[M]S
S (cid:18) (cid:19)pi
(cid:88) (cid:88) 2
= 1[I(m ,··· ,m )=i] max |v⊤v |
1 S m m′
m̸=m′
i=0(m1,···,mS)∈[M]S
S (cid:20) (cid:21) (cid:18) (cid:19)S−i(cid:18) (cid:19)pi
(cid:88) S−i S−i S−i 2 2
≤ 1 i≥ , ∈Z P · P max |v⊤v |
2 2 + M M−i i S− 2i 2 m̸=m′ m m′
i=0
20(cid:18) (cid:19)pi
2
≤ max MiSS max |v⊤v | . (A.5)
m m′
0≤i≤S m̸=m′
If M ≤(max |v⊤v |)−p, applying (A.4) and (A.5) to (A.3), we obtain
m̸=m′ m m′
(cid:12) (cid:12)S
(cid:12) 1 (cid:88)M (cid:12)
E (cid:12) (cid:12)√ f m(v m⊤x)(cid:12) (cid:12) 
(cid:12) M (cid:12)
m=1
(cid:18) (cid:19)pI(m1,···,mS)
≤ √ M1
S
(cid:88) (cid:88) |α m1,i1···α mS,iS|(q!qqSq)S mm ̸=a mx ′|v m⊤v m′| 2
(m1,···,mS)∈[M]S(i1,···,iS)∈[q−p+1]S
≤
((q−p+1)q!q √q MSq Smax|α ms,is|)S (cid:88) (cid:18)
mm ̸=a mx ′|v m⊤v
m′|(cid:19)pI(m1, 2···,mS)
(m1,···,mS)∈[M]S
≤
((q−p+1)q!q √q MSq Smax|α ms,is|)S
0m ≤ia ≤x
SMiSS(cid:18)
mm ̸=a mx ′|v m⊤v
m′|(cid:19)p 2i
≤(cid:0) (q−p+1)q!qqSq+1max|α |(cid:1)S ,
ms,is
which yields (A.1).
By applying Lemma 11 with c =q+1, we have
1
(cid:12) (cid:12)
(cid:12) 1 (cid:88)M (cid:12)
(cid:12)√ f (v⊤x)(cid:12)≲(log1/δ)q+1,
(cid:12) m m (cid:12)
(cid:12) M (cid:12)
m=1
with probability 1−δ, which yields (A.3).
The above proof makes use of the following classical inequality.
Lemma 11. Let δ >0 and X be a mean-zero random variable satisfying
log1/δ
E[|X|S]≤C Sc1 for S =
1 c
1
for C ,c >0. Then, with probability at least 1−δ, we have
1 1
|X|≤C (eS)c1.
1
Proof. The proof follows from [DNGL23].
E[|X|S] CSSc1S
P[|X|≥C (eS)c1]=P[|X|S ≥CS(eS)c1S]≤ ≤ 1 ≤e−c1S =δ,
1 1 C 1S(eS)c1S C 1S(eS)c1S
which concludes the proof.
A.3 Bihari–LaSalle Inequality and Gronwall Inequality
For later use, we provide the proofs of the Bihari–LaSalle inequality and the Gr¨onwall’s inequality for
completeness. The proof of the Bihari–LaSalle inequality is borrowed from [BAGJ21].
Lemma 12 (Bihari–LaSalle inequality and Gronwall inequality). For p ≥ 3 and c > 0, consider a positive
sequence (at) such that
t≥0
at+1 =at+c(at)p−1.
21Then, we have
a0
at ≥ .
(cid:0) (cid:1) 1
1−c(p−2)(a0)(p−2)t p−2
Moreover, when at ≤1 holds for all t≤T −1, we have
a0
at ≤
(cid:0) (cid:1) 1
1−c(1+c)p−1(p−2)(a0)(p−2)t p−2
for all t≤T.
Proof. From definition, we have
at+1−at (cid:90) at+1 1 1 (cid:20) 1 1 (cid:21)
c= ≤ ≤ − .
(at)p−1 xp−1 p−2 (at)p−2 (at+1)p−2
t=at
Taking the summation and re-arranging the terms yield
(at)−(p−2) ≤(a0)−(p−2)−c(p−2)t,
a0
∴at ≥ ,
(cid:0) (cid:1) 1
1−c(p−2)(a0)(p−2)t p−2
which gives the lower bound.
On the other hand, when at ≤1, we have at+1 ≤(1+c)at, and therefore
at+1−at (cid:90) at+1 1 1 (cid:20) 1 1 (cid:21)
c= ≥(1+c)−(p−1) =(1+c)−(p−1) − .
(at)p−1 xp−1 p−2 (at)p−2 (at+1)p−2
t=at
By taking the summation and re-arranging the terms yield
(at)−(p−2) ≤(a0)−(p−2)−c(1+c)p−1(k−1)t,
a0
∴at ≤ ,
(cid:0) (cid:1) 1
1−c(1+c)p−1(p−2)(a0)(p−2)t p−2
which gives the upper bound.
A.4 Orthonormal Basis from Nearly Orthogonal Vectors
In case where the feature vectors v are not orthogonal, the following lemma shows an orthonormal basis
m
can be constructed as a linear combination of v .
m
Lemma13. Let{v }M ⊂Sd−1 beasetofunitvectorsinRd,andsupposethatmax |v⊤v |≤ 1M−1
m m=1 m̸=m′ m m′ 2
holds. Then, we can construct an orthonormal basis {v˜ }M ⊂Sd−1 so that v˜ is a linear combination of
m m=1 m
v ,··· ,v . Specifically, we can take v˜ =v and
1 m 1 1
m
(cid:88)
v˜ = c v , (A.6)
m m,m′ m′
m′=1
so that |c | ≤ 4max |v⊤ v | for m′ = 1,··· ,m−1, |1−c | ≤ 20Mmax |v⊤ v |, and
m,m′ m′,m′′ m′ m′′ m,m m′̸=m′′ m′ m′′
v˜⊤v =0 for m′ =1,··· ,m−1 hold.
m m′
22Proof. First, we show that dim(span{v ,··· ,v })=m for all m. Assume the opposite and then we have
1 m
m−1
(cid:88)
v = a v
m m,m′ m′
m′=1
for some m and {a } . Then, we have
m,m′ 1≤m′≤m−1
m−1
(cid:88) (cid:88)
1=∥v ∥2 ≥ a2 + a a v⊤ v
m m,m′ m,m′ m,m′′ m′ m′′
m′=1 m′̸=m′′
m−1
≥
(cid:88)
a2 −
(cid:88) |a m,m′||a m,m′′|
m,m′ 2M
m′=1 m′̸=m′′
m−1 (cid:32)m−1 (cid:33)2
(cid:88) 1 (cid:88)
≥ a2 − |a |
m,m′ 2M m,m′
m′=1 m′=1
m−1 m−1 m−1
(cid:88) 1 (cid:88) 1 (cid:88)
≥ a2 − a2 = a2 , (A.7)
m,m′ 2 m,m′ 2 m,m′
m′=1 m′=1 m′=1
√
where we used the Cauchy–Schwarz inequality for the last step. Thus, we have |a |≤ 2 and therefore
m,m′
m (cid:88)−1 √ 1
1=∥v ∥2= a v⊤ v ≤(M −1)max |a |·max |v⊤ v |≤(M −1)· 2· <1,
m m,m′ m′ m m′̸=m m,m′ m′̸=m m′ m 2M
m′=1
which yields the contradiction.
Therefore, we have an orthogonal basis {v˜ }M such that (A.6) and v˜⊤v =0 (1≤m≤M,1≤m′ ≤
m m=1 m m′ √
m−1)hold,withsome{c } . Similarlyto(A.7),wehave|c |≤ 2forall1≤m≤M
m,m′ 1≤m≤M,1≤m′≤m m,m′
and 1≤m′ ≤m. What remains is to bound the coefficients. Since we have
m
(cid:88)
v˜ = c v
m m,m′ m′
m′=1
for all m, taking an inner product with
(cid:80)m−1
sign(c )v , we get
m′=1 m,m′ m′
m−1 (cid:32) m (cid:33)⊤ m−1
(cid:88) (cid:88) (cid:88)
0=v˜⊤ sign(c )v = c v sign(c )v
m m,m′ m′ m,m′ m′ m,m′ m′
m′=1 m′=1 m′=1
m−1 m−1 m−1
(cid:88) (cid:88) (cid:88) (cid:88)
= |c |+ c sign(c )v⊤ v + c sign(c )v⊤v
m,m′ m,m′ m,m′′ m′ m′′ m,m m,m′ m m′
m′=1 m′=1m′′̸=m′,m m′=1
m (cid:88)−1 m (cid:88)−1 √ 1 √ 1
≥ |c |− m 2 − 2m
m,m′
2M 2M
m′=1 m′=1
(cid:18) (cid:19)m−1
1 (cid:88) 1
≥ 1− √ |c |− √ .
m,m′
2 2
m′=1
√ √ √
Thus,
(cid:80)m−1
|c | is bounded by 2+1≤3 and
(cid:80)m
|c | is bounded by 2+1+ 2≤4.
m′=1 m,m′ m′=1 m,m′
Also considering an inner product with sign(c )v (1≤m′ ≤m−1), we get
m,m′ m′
(cid:32) m (cid:33)⊤
(cid:88)
0=v˜⊤sign(c )v = c v sign(c )v
m m,m′ m′ m,m′′ m′′ m,m′ m′
m′′=1
(cid:88)
=|c |+ c sign(c )v⊤ v
m,m′ m,m′′ m,m′ m′ m′′
m′′̸=m′
23m
(cid:88)
≥|c |− |c | max |v⊤ v |.
m,m′ m,m′ m′ m′′
m′̸=m′′
m′=1
Thus, |c | (1 ≤ m′ ≤ m−1) is bounded by (cid:80)m |c |max |v⊤ v | ≤ 4max |v⊤ v |.
m,m′ m′=1 m,m′ m′̸=m′′ m′ m′′ m′̸=m′′ m′ m′′
Finally, we get
m m
(cid:88) (cid:88) (cid:88)
1=∥v˜ ∥2 = c2 + c c v⊤ v
m m,m′ m,m′ m,m′′ m′ m′′
m′=1 m′=1m′′̸=m′
m−1 m
(cid:88) (cid:88) (cid:88)
∴|1−c2 |≤ |c |+ |c | |c ||v⊤ v |
m,m m,m′ m,m′ m,m′′ m′ m′′
m′=1 m′=1 m′′̸=m′
≤4M max |v⊤ v |+16 max |v⊤ v |≤20M max |v⊤ v |,
m′ m′′ m′ m′′ m′ m′′
m′̸=m′′ m′̸=m′′ m′̸=m′′
which implies that |1−c |≤20Mmax |v⊤ v |.
m,m m′̸=m′′ m′ m′′
B Proof of Gradient-based Training
Overview of analysis. We define polylogarithmic constants with the following order of strength:
C ≲c−1 ≲c−1 ≲C ≲c−1 ≲c−1 =O˜(1).
1 1 2 2 3 4
Here c and C are different from those in Lemma 11. c is the same as c used in Assumption 2. C will
1 1 4 v 1
be used to represent any polylogarithmic factor that comes from high probability bounds. Also, Section B.1
will introduce another constant C .
p
In Algorithm 1 we first train the first-layer parameters, where we aim to show that a for each class m,
thereexistsufficientlymanyneuronsthatalmostalignwithv toapproximateeachf (v⊤x). Wedefinethe
m m m
alignmentforthem-thtaskattimetasκt =v wt. Thegoaloffirst-layertrainingistoprovethefollowing.
m m j
Weintroducetheerrorε˜=Θ˜(M− 21ε),whereεisthedesiredfinalgeneralizationerrorE x[|f aˆ(x)−f ∗(x)|]≲ε.
Lemma 4 (formal). Let T
1,1
= Θ˜(Mdp−1), T
1,2
= Θ˜(Mdp 2), T
1,3
= Θ˜(ε˜−2Md∨ε˜−3M), and T
1
= T 1,1+
T 1,2+T 1,3. Takethestepsizeasηt =Θ˜(M− 21d−p 2)for0≤t≤T 1,1+T 1,2−1andηt =Θ˜(ε˜M−1 2d−1∧ε˜2M−1 2)
for T
1,1
+T
1,2
≤ t ≤ T
1,1
+T
1,2
+T 1,3, and the number of neurons as J ≳ J minMCplogd. Suppose that
|v m⊤ ′v m| = O˜(M−1) for all m′ ̸= m, and M = O˜(d21). Then, with high probability, for each class m there
exist at least J neurons that achieves v⊤wT1 ≥1−3ε˜.
min m j
WeletT′ =T +T inTheorem1. TheproofforLemmaBwillbedividedintothefollowingparts. First
1 1 2
we consider the initialization and activation functions.
• In Section B.1 we analyze the random initialization. We show that, at the time of initialization, for each
class m, there exist J neurons classified into some class J (i.e., with slightly higher overlap with v ).
min m m
• Section B.2 discusses the assumptions on the target/activation functions. We show that Assumption 4 is
satisfied with a constant probability (i.e., Ω(1) fraction of neurons). Since neurons do not interact in the
correlation loss update, in the subsequent sections of the first-layer training, we focus on the dynamics of
one neuron in J that satisfies Assumption 4 and omit the subscript wt =wt without loss of generality.
1 j
Next we consider the training of the first-layer weights via a power-method like dynamics.
• In Section B.3, we decompose the gradient update into the population dynamics and noise fluctuation.
The training dynamics consist of following three different phases.
24• The first phase corresponds to Section B.4. Here, we show that neurons in J will obtain a small Ω˜(1)
m
alignment to the class v after t (≤T ) iterations.
m 1 1,1
• ThesecondphasecorrespondstoSectionB.5,whereweshowthatneuronscontinuetogrowinthedirection
of v and achieves 1−O˜(1) alignment within t (≤T ) iterations, while remaining almost orthogonal to
m 2 1,2
other directions.
• Finally, in Section B.6, we show that, after (T −t )+(T −t )+T iterations, neurons in J will
1,1 1 1,2 2 1,3 m
eventually achieve w⊤v ≥1−ε˜with high probability.
j m
After first-layer training is completed, in Section B.7 we prove the existence of suitable second-layer
parameterswithsmallnormthatcanapproximatef (Lemma31). SectionB.8concludesthegeneralization
∗
error analysis, which is stated as follows:
Lemma 14. Suppose that J = Θ˜(J minMCp), and σ be either of the ReLU activation or any univariate
polynomial with degree q. There exists λ>0 such that for ridge regression (p=2), we have
(cid:115)
E x[|f aˆ(x)−f ∗(x)|]≲M21(|J min|−1+ε˜)+
dM TCp
2
with probability 1−o d(1). Therefore, by taking T
2
=Θ˜(dMCpε−2), ε˜=Θ˜(M−1 2ε), J
min
=Θ˜(M21ε−1), and
J =Θ˜(MCp+ 21ε−1), we have E x[|f aˆ(x)−f ∗(x)|]≲ε.
On the other hand, for LASSO (p=1), we have
(cid:114)
dM
E x[|f aˆ(x)−f ∗(x)|]≲M1 2(|J min|−1+ε˜)+
T
2
with probability 1−o d(1). Therefore, by taking T
2
= Θ˜(dMε−2), ε˜= Θ˜(M−1 2ε), J
min
= Θ˜(M21ε−1), and
J =Θ˜(MCp+1 2ε−1), we have E x[|f aˆ(x)−f ∗(x)|]≲ε.
Combining Lemma 14 and Lemma B completes the proof of Theorem 1.
B.1 Initialization
To begin with, we need diversity of the neurons at initialization. Note that we do not require neurons to
achieve Θ(1) alignment with vm, as this requires exponential width. Instead, we prove that, for each class
vm, there exist some neurons w0 that are more aligned to this direction vm than others — in other words,
j
v⊤w0 ≳ max |v⊤ w0|. The statement is formalized as follows. In the proof, the subscript t is omitted
m j m′̸=m m′ j
because we only consider the initialization t=0.
Lemma 2 (formal). Suppose that the first layer weight of each neuron is initialized as an independent
(cid:16) (cid:17) 2
samplefromtheuniformdistributionoverSd−1(1). Letp>2, δ =(logd)−p− 22, andC
p
= mm ia nx mm ′|| ββ mm ′, ,p p|
|
p−2.
We define a set of indexes of neurons J that have the highest alignment with f (v⊤x) as
m m m
(cid:40) (cid:12) (cid:12) 1 p−2 (cid:18) 1 (cid:19)p−2(cid:41)
J
m
:= j ∈[J] (cid:12)
(cid:12)
w j⊤v
m
≥ √ , (w j⊤v m)p−2 ≥ max C p2 |w j⊤v m′|p−2+δ √ .
(cid:12) d m′̸=m d
For J >0, if
min
(cid:18) (cid:16) (cid:17)(cid:19)
J≥AJ minMCp(logd)23, with A=exp O max |v m⊤v m′|logd+1 ,
m̸=m′
then |J |≥J for all m with high probability.
m min
√
Since w⊤v = O( logd) with high probability, we have the following corollary, where we use a small
j m
constant c ≲(logd)(p−2).
1
25Corollary 15. When J ≳J minMCplogd, for each class m, we have at least J
min
neurons w
j
such that
1
w⊤v ≥ √ , and |β |(w⊤v )p−2 ≥max|β | max |w⊤v |p−2+c (w⊤v )p−2. (B.1)
j m m,p j m m′,p j m′′ 1 j m
d m′ m′′̸=m
To prove the lemma, we make use of the following upper and lower bounds.
Lemma 16 (Theorems 1 and 2 of [CCM11]). For any β >1 and x∈R, we have
(cid:112) 2e(β √−1) e−βx 22 ≤(cid:90) ∞ √1 e−t 22 dt≤ 1 e−x 22
2β π 2π 2
x
Proof of Lemma 2. Note that w ∼ Unif(Sd−1(1)) can be obtained by sampling w˜ ∼ N(0,1I ) and
j j d d
setting w
j
= ∥w w˜˜j j∥. With high probability, we have ∥w˜ j∥≈1≤2p−1 2. Thus we will instead show that
2 p−2 2δ
w˜ j⊤v
m
≥ √ d, and (w˜ j⊤v m)p−2 ≥ mm ′̸=ax mC p2 |w˜ j⊤v m′|p−2+
dp−
22.
Fix m ∈ [M]. For each m′ ̸= m, consider the value of w˜⊤(I − v v⊤)v and w˜⊤v v⊤v . The
j m m m′ j m m m′
distribution of w˜⊤(I−v v⊤)v follows N(0,∥(I−v v⊤)v ∥), therefore by Lemma 16,
j m m m′ m m m′
P(cid:2) for all m′ ̸=m, |w˜⊤(I−v v⊤)v |≤t(cid:3)
j m m m′
(cid:18) dt2 (cid:19)
≤1−(M −1)exp − . (B.2)
2max ∥(I−v v⊤)v ∥
m′̸=m m m m′
By taking
(cid:18) (cid:19)1
2
t=t := 2d−1 max ∥(I−v v⊤)v ∥log2M ,
1 m m m′
m′̸=m
(B.2) is bounded by M+1.
2M
√
Notethatw˜ j⊤v mv m⊤v
m′
=O(d− 21 logdm √ax m̸=m′|v m⊤v m′|)withhighprobability. When|w˜ j⊤(I−v mv √m⊤)v m′|≤
t 1 forallm′ ̸=mandw˜ j⊤v mv m⊤v m′ =O(d−1 2 logdmax m̸=m′|v m⊤v m′|),wehave|w˜ j⊤v m′|≤t 1+O(d−1 2 logd
p−2
max m̸=m′|v m⊤v m′|). To satisfy (w˜ j⊤v m)p−2 ≥C p2 |w˜ j⊤v m′|p−2+ dp2 −δ
22
and w˜ j⊤v
m
≥ √2 d, it suffices for w˜ j⊤v
m
to be w j⊤v
m
≥t
2
:=C p1 2t 1+C p1 2O(d−1 2√ logdmax m̸=m′|v m⊤v m′|)+ 2p−1 √2δ dp−1 2.
We lower bound the probability that w⊤v ≥t holds. For any β >1, Lemma 16 implies that
j m 2
P(cid:2) w˜⊤v ≥t (cid:3)
j m 2
(cid:112)
2e(β−1)
(cid:18) dβt2(cid:19)
≥ √ exp − 2
2β π 2
(cid:112) (cid:18) (cid:20)
2e(β−1)
≥ √ exp −C β max ∥(I−v v⊤)v ∥log2M
2β π p m′̸=m m m m′
(cid:21)(cid:33)
+O((cid:112) logd max |v m⊤v m′|+δp−1 2)(cid:112) logM +O(logd max |v m⊤v m′|2+δp−2 2)
m̸=m′ m̸=m′
≥ (√
log2M
+1
√ 1
)√
π
exp(cid:18) −C p(cid:20) log2M +O(cid:16) mm ̸=a mx ′|v m⊤v m′|logd+δp−2 2(cid:112) logd+1(cid:17)(cid:21)(cid:19) ,
log2M
wherewetookβ =1+ 1 andusedlogM ≲logd,δ <1,andmax |v⊤v |≤1forthelastinequality.
log2M m′̸=m m m′
To simplify the notation, by letting
A=exp(cid:18) O(cid:16) max |v m⊤v m′|logd+δp−1 2(cid:112) logd+1(cid:17)(cid:19) =exp(cid:18) O(cid:16) max |v m⊤v m′|logd+1(cid:17)(cid:19) ,
m̸=m′ m̸=m′
26we have
P(cid:2) w˜⊤v ≥t (cid:3) ≥ 2 √M−Cp .
j m 2 A log2M
Notethatthisargumentisindependentfromtheonefor(B.2),because(I−v v⊤)v andv areorthogonal.
m m m′ m
Tosumup,(B.2)ensuresthat|w˜⊤(I−v v⊤)v |≤t forallm′ ̸=mwithprobabilityatleast M+1,under
√ j m m m′ 1 2M
which w˜ j⊤v mv m⊤v m′ =O(d−1 2 logdmax m̸=m′|v m⊤v m′|) for all m′ ̸=m with high probability and w˜ j⊤v m ≥t 2
holds with probability at least A2√M lo− gC Mp . w˜ j⊤v
m
≥ t
2
implies that (w˜ j⊤v m)p−2 ≥ C|w˜ j⊤v m′|p−2 + p2 −δ
2
and
d 2
w˜ j⊤v
m
≥ √2 d. Therefore, over the randomness of initialization of w j, w j⊤v
m
≥ √1
d
and (w j⊤v m)p−2 ≥
C|w j⊤v m′|p−2 + δ(√1 d)p−2 for all m′ ̸= m with probability at least AM√ l− oC gp M. Taking the uniform bound
over all v , we know that the number of required neurons to satisfy |J | ≥ J for all m is at most
m m min
J ≥AJ minMCp(logd)3 2 (up to a constant factor, which can be absorbed in the definition of A).
B.2 Descent Path for Population Gradient
This section discusses the assumption on the target/activation functions. To translate weak recovery to
strong recovery and establish alignment, we require that at any (positive) level of alignment u=v⊤wt >0,
1
thepopulationcorrelationlossbetweentheneuronandthetargetsub-problemf (v⊤x)hasthedescentpath
1 1
on the sphere. In other words, the population correlation loss as a function of u should be monotonically
decreasing with respect to u ∈ (0,1]. Observe that without such monotonicity, if the alignment α becomes
large, higher-order terms in the Hermite expansion may generate a repulsive force that prevents the neuron
from further aligning with the target direction v . For the well-specified setting (matching activation), this
m
condition is automatically satisfied as shown in [BAGJ21]; whereas for the misspecified scenario, such a
condition appeared in [MHWSE24] as an assumption, which we restate below, and then verify for specific
choices of student activation functions.
Recall the Hermite expansion of one neuron a0σ(z+b0):
∞
a0σ(z+b0)=(cid:88) √α i He (z)
i
i!
i=0
and the Hermite expansion of each sub-problem of the target function
p
f
(z)=(cid:88)β
√m,iHe (z).
m i
i!
i=p
Assumption 4. The neuron satisfies α β >0 for all p≤i≤q.
i 1,i
This assumption ensures that
q
(cid:88)
E [a0σ(x⊤wt+b0)f (v⊤x)]= (α β )i
x∼N(0,Id) 1 1 i m,i
i=p
is monotonically increasing with respect to u = α β ∈ (0,1]. We show that for certain choices of (ran-
i m,i
domized) activation function, a Ω˜(1) fraction of student neurons satisfy Assumption 4. This indicates that
Assumption 4 only affects the required width up to constant factor. In the subsequent sections, we focus on
the training dynamics of individual neurons that satisfy Assumption 4 (without explicitly mentioning so).
B.2.1 ReLU Activation
For the ReLU activation, we verify this condition for target function in which at each task f , the non-
m
zero Hermite coefficients have the same sign, i.e., for β ,β ̸= 0, we have sign(β ) = sign(β ). For
m,i m,j m,i m,j
27example, this condition is met when the link functions are pure Hermite polynomial. Then, the following
lemma adapted from [BES+23] ensures that α > 0 for all i with probability at least 1. Because the
i 4
distribution of a0 is symmetric, Assumption 4 holds with probability at least 1.
8
Lemma 17. Given degree q ≥ 0 and b ∼ [−C ,C ], the i-th Hermite coefficient of ReLU(z+b) is positive
b b
with probability 1 for all p≤i≤q, if C is larger than some constant that only depends on q.
4 b
Proof. First note that for i=1, we have
E z∼N(0,1)[ReLU(z+b)He 1(z)]= 21(cid:90) ∞ √1 2πe−z 22 dz+ 21 >0.
z=b
Moreover, for i≥2, we have
E z∼N(0,1)[ReLU(z+b)He i(z)]= ( √−1)i e−b 22 He i−2(b),
2π
Because lim He (b) = ∞ for even i and −∞ for odd i, there exists some b such that if b ≤ C′ then
b→−∞ i 0 b
(−1)peb 22 He i(b)>0. By taking C
b
≥2C b′, b≤C b′ (and thus the assertion) holds with probability 1 4.
B.2.2 General Polynomial Link Functions
To deal with general polynomial link functions, we randomize the student activations as follows,
q
σ
(z)=(cid:88)ε
√i,jHe (z),
j i
i
i=p
where ε are independent Rademacher variables (taking 1,−1, and 0 with equiprobability).
i
Lemma 18. Given degree q ≥0 and b∼[−C ,C ], for each p ≤p′ ≤p , the i-th Hermite coefficient
b b min max
of a0σ(z+b0) is non-zero with probability Ω(C−1), for all p′ ≤i≤q. Here Ω hides constant only depending
b
on q.
Proof. Because of the randomized Hermite coefficient of the activation function, a0σ(z) has positive coeffi-
cientswithprobability2−(q−p+1). Aslongasthebiasb0issmall,a0σ(z+b0)alsohavepositivecoefficients.
B.3 Decomposition of Gradient Update
From now, we discuss the training dynamics of the first layer. We focus on one neuron in J that satisfies
1
Assumption 4. To track the alignment during the dynamics, we define κt = v⊤wt. We first consider the
m m j
decomposition of the update into population and stochastic terms:
Lemma 19. Suppose that ηt = η ≤ c 4d−1 and κt
1
≥ 1 2d−1 2. With high probability, the update of κt
m
can be
bounded as
M q
κt+1 ≥κt + √η (cid:88) (cid:88)(cid:2) iα β (κt )i−1(v⊤v −κtκt )(cid:3) −κtη2C2d+ηv⊤(I−wtwt⊤ )Zt. (B.3)
1 1 i m,i m 1 m 1 m 1 1 1
M
m=1i=p
Moreover, κt+1 is evaluated as
m
κt + √η (cid:88)M (cid:88)q (cid:2) iα β (κt )i−1(v⊤v −κt κt )(cid:3) − |κt m|+ηC 1d1 2 η2C2d
m M i m′,i m′ m m′ m m′ 2 1
m′=1i=p
+ηv⊤(I−wtwt⊤ )Zt
m
28≤κt+1
m
≤κt + √η (cid:88)M (cid:88)q (cid:2) iα β (κt )i−1(v⊤v −κt κt )(cid:3) + |κt m|+ηC 1d21 η2C2d
m M i m′,i m′ m m′ m m′ 2 1
m′=1i=p
+ηv⊤(I−wtwt⊤ )Zt.
m
Here Zt is a mean-zero random variable satisfying ∥Zt∥=O˜(1) with high probability. For any v ∈Rd with
∥v∥=O(1) that is independent from Zt, |v⊤Zt|=O˜(1) with high probability. Also, |κt −κt+1|=O˜(η) with
m m
high probability.
To prove Lemma 19, we first establish the following characterization of the stochastic gradient.
Lemma 20. The stochastic gradient −∇ ytatσ(wt⊤ xt+bt) is decomposed as
w
−∇ ytatσ(wt⊤ xt+bt)
w
M q
1 (cid:88) (cid:88)(cid:104) (cid:112) (cid:105)
=−√ iα β (κt )i−1v + (i+2)(i+1)α β (κt )iwt +Zt,
i m,i m m i+2 m,i m
M
m=1i=p
where Zt is a mean-zero random variable such that ∥Zt∥=O˜(d1 2) and |v⊤Zt|=O˜(1) for any fixed v ∈Sd−1
with high probability. Also, ∥∇ wytatσ(wt⊤ xt+bt)∥=O˜(d1 2) with high probability, and for any fixed v with
(cid:16) (cid:17)⊤
∥v∥=O(1), ∇ ytatσ(wt⊤ xt+bt) v =O˜(1) with high probability.
w
Proof. For i-th Hermite polynomial He and u∈Sd−1, we have that
i
E [He (x )f(u⊤x)x ]=iui−1E [f(i−1)(u⊤x)]+ui+1E [f(i+1)(u⊤x)],
x∼N(0,Id) i 1 1 1 x∼N(0,Id) 1 x∼N(0,Id)
E [He (x )f(u⊤x)x ]=uiu E [f(i+1)(u⊤x)].
x∼N(0,Id) i 1 2 1 2 x∼N(0,Id)
Therefore,
 iui−1
1
 0 
E x∼N(0,Id)[He i(x 1)f(u⊤x)x]=


.
.
.


E x∼N(0,Id)[f(i−1)(u⊤x)]+ui 1uE x∼N(0,Id)[f(i+1)(u⊤x)].
0
Using this fact, the population gradient is computed as
(cid:104) (cid:105)
∇ E ytatσ(wt⊤ x+bt)
w
(cid:34) M (cid:35)
=E √1 (cid:88) f (v⊤x)atσ′(wt⊤ x+bt)x
m m
M
m=1
  M p (cid:32) ∞ (cid:33) 
=E √1 (cid:88) (cid:88)β √m,iHe i(z) (cid:88) √iα iHe i−1(z) x
M i! i!
m=1i=p i=0
M q
1 (cid:88) (cid:88)(cid:104) (cid:112) (cid:105)
= √ i2α β (v⊤wt)i−1v + (i+2)(i+1)α β (v⊤wt)iwt
i m,i m m i+2 m,i m
M
m=1i=p
M q
1 (cid:88) (cid:88)(cid:104) (cid:112) (cid:105)
= √ iα β (κt )i−1v + (i+2)(i+1)α β (κt )iwt .
i m,i m m i+2 m,i m
M
m=1i=p
We define Zt as the difference between the population gradient and the empirical gradient:
(cid:104) (cid:105)
Zt =−∇ ytatσ(wt⊤ xt+bt)+∇ E ytatσ(wt⊤ x+bt) .
w w
29We have
−∇ ytatσ(wt⊤ xt+bt)
w
M q
1 (cid:88) (cid:88)(cid:104) (cid:112) (cid:105)
=−√ iα β (κt )i−1v + (i+2)(i+1)α β (κt )iwt +Zt
i m,i m m i+2 m,i m
M
m=1i=p
It is easy to see that E xt,yt[Zt] = 0, and ∥Zt∥ = O˜(d21) with high probability by (A.2) of Lemma 10.
(A.2) also yields that |v⊤Zt| = O˜(1) with high probability for any fixed v ∈ Sd−1. Finally, the norm of
∇ wytatσ(wt⊤ xt +bt) = ytatσ′(wt⊤ xt +bt)xt is of order O˜(d21) with high probability, and for any fixed v
(cid:16) (cid:17)⊤
with ∥v∥=O(1), ∇ ytatσ(wt⊤ xt+bt) v =O˜(1) with high probability.
w
Proof of Lemma 19. First, we consider κt. We have
1
∥wt−η(I−wtwt⊤ )∇ (−ytatσ(wt⊤ xt+bt))∥−1
w
(cid:16) (cid:17)−1
= 1+η2∥(I−wtwt⊤ )∇ ytatσ(wt⊤ xt+bt)∥2 2
w
η2
≥1− ∥(I−wtwt⊤ )∇ ytatσ(wt⊤ xt+bt)∥2
2 w
η2
≥1− ∥∇ ytatσ(wt⊤ xt+bt)∥2.
2 w
By using this, with high probability,
wt−η(I−wtwt⊤ )∇ (−ytatσ(wt⊤ xt+bt))
v⊤wt+1 =v⊤ w
1 1 ∥wt−η(I−wtwt⊤)∇ (−ytatσ(wt⊤xt+bt))∥
w
κtη2
≥κt +ηv⊤(I−wtwt⊤ )∇ ytatσ(wt⊤ xt+bt)− 1 ∥∇ ytatσ(wt⊤ xt+bt)∥2
1 1 w 2 w
η3
− |v⊤(I−wtwt⊤ )∇ (−ytatσ(wt⊤ xt+bt))|∥∇ ytatσ(wt⊤ xt+bt)∥2
2 1 w w
(cid:104) (cid:105) κtη2
≥κt +ηv⊤∇ E ytatσ(wt⊤ xt+bt) +v⊤Zt− 1 ∥∇ ytatσ(wt⊤ xt+bt)∥2
1 1 w 1 2 w
η3
− ∥∇ ytatσ(wt⊤ xt+bt)∥3
2 w
M q
( ≥i) κt +ηv⊤(I−wtwt⊤ )√1 (cid:88) (cid:88)(cid:2) iα β (κt )i−1v (cid:3) +ηv⊤(I−wtwt⊤ )Zt
1 1 i m,i m m 1
M
m=1i=p
−
κt 1η2C 12d
−
η3C 13d3
2
2 2
M q
( ≥ii) κt +η√1 (cid:88) (cid:88)(cid:2) iα β (κt )i−1(v⊤v −κtκt )(cid:3) −κtη2C2d
1 i m,i m 1 m 1 m 1 1
M
m=1i=p
+ηv⊤(I−wtwt⊤ )Zt,
1
where we used Lemma 20 in (i), and (ii) is due to the fact that we take η ≤c 4d−1 and κt
1
≥d− 21, and hence
η3C 13d23 ≤κt 1η2C 12d and −κt 1η2 2C 12d − η3C 213d3 2 ≥−κt 1η2C 12d. Thus we obtained (B.3).
In the same way, for the lower bound on κt , we have
m
|κt |η2
κt+1 ≥κt +ηv⊤(I−wtwt⊤ )∇ ytatσ(wt⊤ xt+bt)− m ∥∇ ytatσ(wt⊤ xt+bt)∥2
m m m w 2 w
η3
− |v⊤(I−wtwt⊤ )∇ (−ytatσ(wt⊤ xt+bt))|∥∇ ytatσ(wt⊤ xt+bt)∥2
2 m w w
30≥κt + √η (cid:88)M (cid:88)q (cid:2) iα β (κt )i−1(v⊤v −κt κt )(cid:3) − |κt m|+ηC 1d21 η2C2d
m M i m′,i m′ m m′ m m′ 2 1
m′=1i=p
+ηv⊤(I−wtwt⊤ )Zt.
m
As for the upper bound,
|κt |η2
κt+1 ≤κt +ηv⊤(I−wtwt⊤ )∇ ytatσ(wt⊤ xt+bt)+ m ∥∇ ytatσ(wt⊤ xt+bt)∥2
m m m w 2 w
η3
+ |v⊤(I−wtwt⊤ )∇ (−ytatσ(wt⊤ xt+bt))|∥∇ ytatσ(wt⊤ xt+bt)∥2
2 m w w
≤κt + √η (cid:88)M (cid:88)q (cid:2) iα β (κt )i−1(v⊤v −κt κt )(cid:3) + |κt m|+ηC 1d21 η2C2d
m M i m′,i m′ m m′ m m′ 2 1
m′=1i=p
+ηv⊤(I−wtwt⊤ )Zt.
m
Finally, we check that |κt −κt+1|=O˜(η). From the above argument, we have
m m
(cid:12) (cid:12) |κt |η2
|κt+1−κt |≤η(cid:12)v⊤(I−wtwt⊤ )∇ ytatσ(wt⊤ xt+bt)(cid:12)+ m ∥∇ ytatσ(wt⊤ xt+bt)∥2
m m (cid:12) m w (cid:12) 2 w
η3
+ |v⊤(I−wtwt⊤ )∇ (−ytatσ(wt⊤ xt+bt))|∥∇ ytatσ(wt⊤ xt+bt)∥2.
2 m w w
(cid:12) (cid:12)
The first term is bounded by O˜(η) because (cid:12)v⊤(I−wtwt⊤ )∇ ytatσ(wt⊤ xt+bt)(cid:12)=O˜(1). The second term
(cid:12) m w (cid:12)
is bounded by O˜(η) when η ≤c d−1, because ∥∇ ytatσ(wt⊤ xt+bt)∥2 =O˜(d). The third term is bounded
4 w
by O˜(η) when η ≤c d−1, because |v⊤(I−wtwt⊤ )∇ (−ytatσ(wt⊤ xt+bt))|=O˜(1) and ∥∇ ytatσ(wt⊤ xt+
4 m w w
bt)∥2 =O˜(d). Therefore, we obtained that |κt −κt+1|=O˜(η).
m m
B.4 Phase I: Weak Recovery for One Direction
Based on Lemma 19, we analyze the stochastic gradient update of the first-layer parameters. The goal of
this subsection is to prove the formal version of Lemma 3.
Lemma 3 (formal). Suppose that |v m⊤ ′v m| ≤ c 4M−1 for all m ̸= m′ and M ≤ c 4d1 2, and ηt = η ≤
c 4M− 21d−p 2. Then, with high probability, there exists some time t
1
≤ T
1,1
= Θ(η−1M1 2dp− 22) such that the
following holds:
(i) κt1 >c , and
1 2
(ii) |κt m1|≤5c 3M−1, for all m=2,··· ,M.
We start with the following lemma, which introduces (deterministic) auxiliary sequences that upper and
lower bound the stochastic updates of κt .
m
Lemma 21. Suppose that |v m⊤ ′v m| ≤ c 4M−1 for all m ̸= m′ and M ≤ c 4d1 2. For all s = 0,1,··· ,t, we
assume that
(a) κs ≤c (only required for (i): Lower bound),
1 2
(b) |κs |≤κs for all m=2,··· ,M, and
m 1
(c) |κs |≤C c M−1 for all m=2,··· ,M.
m 2 3
Then, taking ηt =η ≤c 4M−1 2d−p 2, we have the following bounds.
31(i): Lower bound of κs: For κt, we have
1 1
s
κs+1 ≥(1−c )κ0+η(1−c )(cid:88) pα √pβ 1,p(κs′ )p−1, (B.4)
1 2 1 2 1
M
s′=0
for s=0,1,··· ,t. Consequently, by introducing an auxiliary sequence (Ps)t+1 with P0 =(1−c )κ0, and
s=0 2 1
pα β
Ps+1 =Ps+ηc √p 1,p(Ps)p−1 (s=0,1,··· ,t), (B.5)
2
M
κs is lower-bounded by Ps for all s=0,1,··· ,t+1, with high probability.
1
(ii): Upper bound of max |κs |: Foranauxiliarysequence(Qs)t+1 withQ0 =(1+c )max{max |κ0 |,
m̸=1 m s=0 2 m̸=1 m
1d−1 2}, and
2
η η
Qs+1 =Qs+(1+c )√ pmax|α β |(Qs)p−1+c pα β (κs)p−1,
2 M m p m,i 3 M3 2 p 1,p 1
max |κs | is upper-bounded by Qs for all s=0,1,··· ,t+1, with high probability.
m̸=1 m
Proof.
(i) Lower bound of κs 1 by Ps. If κt 1 ≥ 1 2d−1 2, we have κt 1 ≥ηC 1d1 2 by the choice of η. If κs 1 ≥ 1 2d−1 2, by
Lemma 19, we have
M q
κs+1 ≥κs+ √η (cid:88) (cid:88)(cid:2) iα β (κs )i−1(v⊤v −κsκs )(cid:3) −κsη2C2d+ηv⊤(I−wsws⊤)Zs
1 1 i m,i m 1 m 1 m 1 1 1
M
m=1i=p
M q
≥κs+ √η pα β (1−(κs)2)(κs)p−1+ √η (cid:88) (cid:88)(cid:2) iα β (κs )i−1(v⊤v −κsκs )(cid:3)
1 p 1,p 1 1 i m,i m 1 m 1 m
M M
m=2i=p
−κsη2C2d+ηv⊤(I−wsws⊤)Zs
1 1 1
η √
≥κs+ √ pα β (1−(κs)2)(κs)p−1−q2η Mmax|κs |p−1max|v⊤v |max|α β |
1 p 1,p 1 1 m 1 m i m,i
M m̸=1 m̸=1 m,i
√
−q2η Mmax|κs |pmax|α β |−κsη2C2d+ηv⊤(I−wsws⊤)Zs (B.6)
m i m,i 1 1 1
m̸=1 m,i
η √
≥κs+ √ pα β (1−(κs)2)(κs)p−1−q2η M(κs)p−1c M−1
1 p 1,p 1 1 1 4
M
√
−q2η M(κs 1)p−1C 2c 3M−1−ηκs 1c 4M− 21d−p− 22 C 12+ηv 1⊤(I−wsws⊤)Zs.
√ √
Note that (κs 1)2 ≤ c2
2
≤ 51c 2, q2η M(κs 1)p−1c 4M−1 ≤ 1 5c 2√η Mpα pβ 1,p(κs 1)p−1, q2η M(κs 1)p−1C 2c 3M−1 ≤
1 5c 2√η Mpα pβ 1,p(κs 1)p−1, and ηκs 1c 4M−1 2d−p− 22C 12 ≤ 1 5c 2√η Mpα pβ 1,p(κs 1)p−1, (where we used κs 1 ≥ 21d−1 2 for
the last statement). Thus, we obtained that
4 η
κs+1 ≥κs+(1− c )√ pα β (κs)p−1+ηv⊤(I−wsws⊤)Zs. (B.7)
1 1 5 2 M p 1,p 1 1
We prove the assertion by induction. Suppose that (B.4) holds for s = 0,··· ,τ for some τ ≤ t. Note
that this implies κs
1
≥(1−c 2)κ0
1
and κs
1
≥ 21d−1 2. By applying (B.7), we have
4 η
κτ+1 ≥κτ +(1− c )√ pα β (κτ)p−1+ηv⊤(I−wτwτ⊤)Zτ
1 1 5 2 M p 1,p 1 1
τ τ
(cid:88) 4 η (cid:88)
≥κ0+ (1− c )√ pα β (κs)p−1+ ηv⊤(I−wsws⊤)Zs. (B.8)
1 5 2 M p 1,p 1 1
s=0 s=0
32If τ ≤C M(κ0)2−2p, then
2 1
(cid:88)τ √
ηv⊤(I−wsws⊤)Zs ≤ηC τ ≤c C κ0 ≤c κ0, (B.9)
1 1 4 1 1 2 1
s=0
with high probability. If τ >C M(κ0)2−2p,
2 1
(cid:88)τ ηv 1⊤(I−wsws⊤)Zs ≤ηC 1√ τ ≤ητC 1C 2− 21 M−1 2(κ0 1)p−1 ≤ 1 5c 2√η Mτpα pβ 1,p((1−c 2)κ0 1)p−1
s=0
τ
1(cid:88) η
≤ c √ pα β (κs)p−1, (B.10)
5 2 M p 1,p 1
s=0
with high probability. Applying the above evaluations to (B.8), we have
τ
(cid:88) η
(B.8)≥(1−c )κ0+ (1−c )√ pα β (κs)p−1.
2 1 2 p 1,p 1
M
s=0
Thus the (B.4) holds also for s=τ +1. The induction proves that (B.4) holds until s=t.
By repeatedly using (B.5), the update of (Pt)τ is equivalent to
t=0
t
(cid:88) η
Pt+1 =P0+ √ (1−c )pα β (Ps)p−1
2 p 1,p
M
s=0
By comparing this and (B.4), we conclude that κs is lower bounded by Ps for s=1,2,··· ,t+1.
1
(ii) Upper bound of max |κs | by Qs: According to Lemma 19, |κs+1 − κs | ≤ C η with high
m̸=1 m m m 1
probability. Thus, the sign of κs+1 is the same as that of κs , or |κs+1|≤C η. Therefore,
m m m 1
|κs+1|
m
(cid:26) (cid:12) M q
≤max C 1η,(cid:12) (cid:12) (cid:12)κs m+ √η
M
(cid:88) (cid:88)(cid:2) iα iβ m,i(κs m)i−1(v 1⊤v m−κt mκs m)(cid:3)
m=1i=p
+
|κs m|+ηC 1d1 2 η2C2d+ηv⊤(I−wsws⊤)Zs(cid:12) (cid:12) (cid:12)(cid:27)
2 1 m (cid:12)
(cid:26) (cid:12) q
≤max C 1η,(cid:12) (cid:12) (cid:12)κs m+ √η
M
(cid:88)(cid:2) iα iβ 1,i(κs 1)i−1(v m⊤v 1−κs mκs 1)(cid:3)
i=p
M q
+ √η (cid:88) (cid:88)(cid:2) iα β (κs )i−1(v⊤v −κs κs )(cid:3)
i m′,i m′ m m′ m m′
M
m′̸=1i=p
+
|κs m|+ηC 1d1 2 η2C2d+ηv⊤(I−wsws⊤)Zs(cid:12) (cid:12) (cid:12)(cid:27)
2 1 m (cid:12)
(cid:26) (cid:12) q
≤max C 1η,(cid:12) (cid:12) (cid:12)κt m+ √η
M
(cid:88) iα iβ 1,i(κs 1)i−1v m⊤v 1
i=p
M q
+ √η (cid:88) (cid:88)(cid:2) iα β (κs )i−1(v⊤v −κs κs )(cid:3)
i m′,i m′ m m′ m m′
M
m′̸=1i=p
+
|κs m|+ηC 1d1 2 η2C2d+ηv⊤(I−wsws⊤)Zs(cid:12) (cid:12) (cid:12)(cid:27)
.
2 1 m (cid:12)
We have
(cid:12) q M q (cid:12)
(cid:12) (cid:12) (cid:12)(cid:88) iα iβ 1,i(κs 1)i−1v m⊤v 1+ (cid:88) (cid:88)(cid:2) iα iβ m′,i(κs m′)i−1(v m⊤v m′ −κs mκs m′)(cid:3)(cid:12) (cid:12)
(cid:12)
i=p m′̸=1i=p
33(cid:12) q (cid:12) (cid:12) q (cid:12)
≤(cid:12) (cid:12) (cid:12)(cid:88) iα iβ 1,i(κs 1)i−1v m⊤v 1(cid:12) (cid:12) (cid:12)+(cid:12) (cid:12) (cid:12)(cid:88)(cid:2) iα iβ m,i(κs m)i−1(1−(κs m)2)(cid:3)(cid:12) (cid:12)
(cid:12)
i=p i=p
(cid:12) M q (cid:12)
+(cid:12) (cid:12)
(cid:12)
(cid:88) (cid:88)(cid:2) iα iβ m′,i(κs m′)i−1(v m⊤v m′ −κs mκs m′)(cid:3)(cid:12) (cid:12)
(cid:12)
m′̸=1i=p
≤q2max|α β |(κs)p−1max|v⊤v |+pmax|α β ||κs |p−1+q2max|α β ||κs |p
i 1,i 1 1 m′ p m′,p m i m′,i m
i m′ m′ m′,i
+Mq2max|α β |max|κs |p−1(max|v⊤v |+max|κs |)
i m′,i m m m′ m
m′,i m̸=1 m′,m m′̸=1
≤q2max|α β |(κs)p−1c M−1+pα β (κs )p−1+q2max|α β |(κs )p−1C c M−1
i 1,i 1 4 p m,p m i m,i m 2 3
i m,i
+Mq2max|α β |max(κs )p−1(c M−1+C c M−1)
i m,i m 4 2 3
m,i m̸=1
1 pα β
≤(1+ c )pmax|α β |max|κs |p−1+c p 1,p(κs)p−1.
3 2 m p m,p m̸=1 m 3 M 1
Also, |κs m|+ 2ηC1d1 2η2C 12d≤ |κs m|+c4M− 221 d−p 2C1d1 2ηc 4M− 21d−p− 22C 12 ≤ 31c 2ηpα pβ m,pmax{|κs m|,1 2d−1 2}p−1. There-
fore, κτ+1 is bounded as
m
(cid:26) (cid:12)
|κτ m+1|≤max C 1η,(cid:12) (cid:12) (cid:12)κτ m+(1+ 31 c 2)√η Mpm max|α pβ m,p|m m̸=ax 1|κs m|p−1+c 3 Mη 3 2pα pβ 1,p(κτ 1)p−1
(cid:12)(cid:27)
+ 1 3c 2ηpα pβ 1,pmax{|κs m|,1 2d−1 2}p−1+ηv m⊤(I−wsws⊤)Zτ(cid:12) (cid:12)
(cid:12)
(cid:26) (cid:12) τ
≤max C 1η,(cid:12) (cid:12) (cid:12)κ0 m+(1+ 32 c 2)(cid:88) √η Mpm max|α pβ m,p|max{m m̸=ax 1|κs m|p−1,(1 2d− 21)p−1}
s=0
τ τ (cid:12)(cid:27)
+c 3(cid:88)
s=0
Mη
3
2pα pβ 1,p(κs 1)p−1+(cid:88) s=0ηv m⊤(I−wsws⊤)Zs(cid:12) (cid:12)
(cid:12)
(cid:26) τ
≤max C 1η,m m̸=ax 1|κ0 m|+(1+ 32 c 2)(cid:88) √η Mpm max|α pβ m,p|max{m m̸=ax 1|κs m|p−1,(1 2d−1 2)p−1}
s=0
τ (cid:12) τ (cid:12)(cid:27)
+c 3(cid:88)
s=0
Mη
3
2pα pβ 1,p(κs 1)p−1+m m̸=ax 1(cid:12) (cid:12) (cid:12)(cid:88) s=0ηv m⊤(I−wsws⊤)Zs(cid:12) (cid:12)
(cid:12)
. (B.11)
According to the update of Qt, Qt ≥ 1 2d−1 2 ≥C 1η for all t. Moreover, by η ≤c 4M− 21d−p 2,
(cid:12) (cid:12)
(cid:12)(cid:88)τ (cid:12)
max(cid:12) ηv⊤(I−wsws⊤)Zs(cid:12)
(cid:12) m (cid:12)
m̸=1(cid:12) (cid:12)
s=0
√
≤ηC τ
1
1
 2c 2d−1 2 (τ ≤C 2Mdp−1)
≤ τc η 1
 3√2 Mpα pβ 1,p( 2d−1 2)p−1 (τ >C 2Mdp−1)
Therefore, (B.11) is further upper bounded as
1
m m̸=ax 1|κτ m+1|≤(1+c 2)max{m m̸=ax 1|κ0 m|, 2d− 21}
τ τ
(cid:88) η (cid:88) η
+(1+c ) √ pmax|α β |max{max|κs |,Qs}p−1+c pα β (κs)p−1 (B.12)
2 s=0 M m p m,p m̸=1 m 3 s=0 M3 2 p 1,p 1
On the other hand, Qτ+1 is written as
τ
Qτ+1 ≤(1+c 2)max{m m̸=ax 1|κ0 m|, 21 d−1 2}+(1+c 2)(cid:88) √η Mpm max|α pβ m,p|(Qs)p−1
s=0
34τ
(cid:88) η
+c pα β (κs)p−1 (B.13)
3
s=0
M3
2
p 1,p 1
Comparing(B.12)and(B.13)withtheupdateofQt,weconcludethatmax |κτ |≤Qτ (τ =0,1,··· ,t+1)
m̸=1 m
holds by induction.
The previous lemma assumed (a)-(c). Next we show that (b) and (c) hold along the trajectory via
induction. (here we use a different notation for the coefficient in (c).)
Lemma 22. Take ηt =η ≤c 4M− 21d−p 2. Suppose that, for all s=0,1,··· ,t,
(a) κs ≤c ,
1 2
(b) |κs |≤κs for all m=2,··· ,M, and
m 1
(c)’ |κs |≤4c M−1 for all m=2,··· ,M.
m 3
Then, if we have M ≤c 4d1 2 and (a) κt 1+1 ≤c 2 for t+1, (b) and (c)’ hold for s=t+1 with high probability.
Proof. First consider the case when
pmax |α β | c
(1+c ) m√ p m,p (Qs)p−1 > 3 pα β (Ps)p−1 (B.14)
2 M M3
2
p 1,p
holds for all s=0,1,··· ,t. Then, for s=0,1,··· ,t,
c pmax |α β | pmax |α β |
3 pα β (Ps)p−1 <c−1c (1+c ) m√ p m,p (Qs)p−1 <c m√ p m,p (Qt)p−1,
M3
2
p 1,p 2 3 2 M 2 M
and therefore
η
Qs+1 ≤Qs+(1+2c )√ pmax|α β |(Qs)p−1
2 p m,i
M m
for all s=0,1,··· ,t. According to Lemma 12, we have
Q0
Qs≤ (B.15)
(cid:16) (cid:17) 1
1−ηp(p−2)(1+3c 2)(max m|α pβ m,p|)M−1 2(Q0)(p−2)s p−2
for all s=0,1,··· ,t+1 (here (1+c)p−1 in the original bound is absorbed in (1+3c )).
2
On the other hand, Ps is lower bounded by
P0
Ps ≥ (B.16)
(cid:16) (cid:17) 1
1−ηp(p−2)(1−c 2)(α pβ 1,p)M−1 2(P0)(p−2)s p−2
for all s = 0,1,··· ,t+1. According to (B.1), Q0 = (1+c 2)max{max m̸=1|κ0 m|,1 2d−1 2} ≤ P0 = (1−c 2)κ0 1.
Moreover, (B.1) implies that max |α β |(1+3c )(max |κ0 |)p−2 ≤ (1−c )α β (κ0)p−2, yielding
m p m,p 2 m̸=1 m 2 p 1,p 1
that the denominator of (B.15) is larger than that of (B.16). Therefore, Qt+1 ≤ Pt+1 holds, which implies
that max |κt+1|≤κt+1.
m̸=1 m 1
NextwecheckQt+1 <c M−1 (RHSissmallerthan(c)’byafactorof4). FromPt ≤κt ≤c and(B.16),
3 1 2
t≤
η−1M21((P0)−(p−2)−(c 2)−(p−2))
≤
η−1M1 2(1+5c 2)maxm αp| βα 1p ,β pm,p|(P0)−(p−2)
. (B.17)
p(p−2)(1−c )(α β ) p(p−2)(1+3c )(max |α β |)
2 p 1,p 2 m p m,p
On the other hand, according to (B.15), Qt+1 >c M−1 holds only if
3
t>
η−1M1 2((Q0)−(p−2)−(c 3M−1)−(p−2)−ηM− 21p(p−2)(1+3c 2)(max m|α pβ m,p|))
. (B.18)
p(p−2)(1+3c )(max |α β |)
2 m p m,p
35If M ≤ c 4d1 2, (c 3M−1)−(p−2) ≤ (c 3c− 41d−1 2)−(p−2) ≤ c 2(κ0 1)−(p−2) ≤ c 2(P0)−(p−2) ≤ c 2(Q0)−(p−2). More-
over, ηM− 21p(p−2)(1+5c 2)(max m|α pβ m,p|)≤c 2(Q0)−(p−2). Thus, (B.18) is further bounded by
(B.18)≥
η−1M1 2(1−2c 2)(Q0)−(p−2)
. (B.19)
p(p−2)(1+3c )(max |α β |)
2 m p m,p
According to (B.1), (1+8c )max |α β |(max |κ0 |)p−2 < α β (κ0)p−2. By using this, we have
2 m p m,p m̸=1 m p 1,p 1
(RHS of (B.19)) > (RHS of (B.17)). Thus, Qt+1 > c M−1 does not hold, and we have obtained Qt+1 ≤
3
c M−1 (and (c)’).
3
Now we consider the case when (B.14) holds for s=0,1,··· ,τ −1 but
1
pmax |α β | c
(1+c ) m√ p m,p (Qs)p−1 ≤ 3 pα β (Ps)p−1 (B.20)
2 M M3
2
p 1,p
holdsfors=τ ≤t. Weshowthat(B.20)holdsforalls=τ ,··· ,t+1inthiscase. Assumingtheinequality
1 1
holds for all s=τ ,··· ,τ with τ ≤t. For s=τ ,··· ,τ, the update of Qs is evaluated as
1 1
η
Qs+1 ≤Qs+2c pα β (Ps)p−1.
3 M3
2
p 1,p
Thus, when p≥3,
τ
(cid:88) η
Qτ+1 ≤Qτ1 + 2c pα β (Ps)p−1
s=τ1
3 M3
2
p 1,p
≤Qτ1 + 2c 3(1−c 2)−1 (cid:0) Pτ+1−Pτ1(cid:1) (B.21)
M
≤(cid:18) α pβ 1,p (1+c )−1M−1(cid:19) p−1 1 Pτ1 + 2c 3(1−c 2)−1 (cid:0) Pτ+1−Pτ1(cid:1)
max |α β | 2 M
m p m,p
(cid:18) (cid:19) 1
≤ α pβ 1,p (1+c )−1M−1 p−1 Pτ+1,
max |α β | 2
m p m,p
whichyields(B.20)fors=τ+1. (B.20)withs=t+1impliesthatQt+1 ≤Pt+1,andhencemax |κt+1|≤
m̸=1 m
κt+1. Similar to (B.21), we have
1
Qt+1 ≤Qτ1 + 2c 3(1−c 2)−1 (cid:0) Pt+1−Pτ1(cid:1) ≤Qτ1 + 2c 3(1−c 2)−1 Pt+1 ≤Qτ1 +3c M−1.
M M 3
For Qτ1, as we proved Qt+1 ≤ c 3M−1 in the first case, we have Qτ1 ≤ c 3M−1. Thus, Qt+1 is bounded by
4c M−1, which yields (c)’.
3
Proof of Lemma 3. Suppose that (a) holds for all s=0,1,··· ,T , where
1,1
(cid:22)(cid:16) (cid:17)−1(cid:23)
T
1,1
= ηp(p−2)(1−5c 2)(α pβ 1,p)M−1 2(P0)(p−2) .
According to Lemma 22, if M ≤ c 4d21, η ≤ c 4M−1 2d−p 2, and (a) holds for all s = 0,1,··· ,T 1,1, (b) and (c)
of Lemma 22 holds with high probability for all s=0,1,··· ,T and the bounds of Lemma 21 holds for all
1,1
s=0,1,··· ,T .
1,1
From Lemma 21 and Lemma 12,
P0
κt ≥Pt ≥ . (B.22)
1 (cid:16) (cid:17) 1
1−ηp(p−2)(1−c 2)(α pβ 1,p)M−1 2(P0)(p−2)s p−2
36However, at t=T ,
1,1
P0
(RHS of (B.22))≥
(cid:16) (cid:17) 1
ηp(p−2)(1−c 2)(α pβ 1,p)M− 21(P0)(p−2) p−2
1
= ,
(cid:16) (cid:17) 1
ηp(p−2)(1−c 2)(α pβ 1,p)M− 21 p−2
and thus RHS of (B.22) is clearly larger than 1. This yields the contradiction because κT1,1 ≤1. Therefore,
1
with high probability, there exists some t ≤T such that κt1 >c and κt ≤c (t=0,1,··· ,t −1).
1 1,1 1 3 1 3 1
As for (ii), recall that |κt m1−1|≤4c 3M− 21. Moreover, according to Lemma 19,
|κt1 −κt1−1|
1 1
≤C 1η ≤C 1c 4M−1 2d−p 2 ≤c 3M−1 2.
Thus, |κt m1|≤|κt m1−1|+|κt 11 −κt 11−1|≤4c 3M−1 2 +c 3M− 21 ≤5c 3M−1 2.
B.5 Phase II: Amplification of Alignment
In the previous section (Lemma 3), we proved that neurons in J achieve a small constant (c ) alignment.
1 2
However, as the alignment κt becomes larger, the effect from other directions becomes non-negligible, while
1
the signal from v gets smaller because of the projection
(1−wtwt⊤
). Hence the previous weak recovery
1
analysis is not sufficient to show that the neurons will continue to grow in the direction of v .
1
The goal of this subsection is to prove the following lemma.
Lemma 23. Suppose η = ηt ≤ c 4M−1 2d−p 2, |v m⊤ ′v m| ≤ c 4M−1 for all m ̸= m′, and M ≤ c 4d1 2, and
consider a neuron that satisfies (i) and (ii) of Lemma 3. Then, with high probability, there exists some time
t
2
≤T
1,2
=Θ˜(M1 2η−1) such that κt 12 >1−c 2.
Similar to the Phase I analysis, we bound the update by deterministic auxiliary sequences. To simplify
the notation, we let t←t−t throughout this subsection.
1
Lemma 24. Suppose that |v m⊤ ′v m| ≤ c 4M−1 for all m ̸= m′ and M ≤ c 4d1 2. For all s = 0,1,··· ,t, we
assume that
(a) κs ≤1−c (only required for (i): Lower bound),
1 2
(b) |κs |≤κs for all m=2,··· ,M, and
m 1
(c) |κs |≤C c M−1 for all m=2,··· ,M.
m 2 3
Take ηt =η ≤c 4M−1 2d−p 2. Then, we have the following bounds.
(i): Lower bound of κs: For an auxiliary sequence (Ps)t+1 with P0 =(1−c )κ0, and
1 s=0 2 1
η
Ps+1 =Ps+c √ pα β (Ps)p−1,
2 p m,p
M
κs is lower-bounded by Ps for all s=0,1,··· ,t+1, with high probability.
1
(ii): Upper bound of max |κs |: For an auxiliary sequence (Qs)t+1 with Q0 =6c M−1, and
m̸=1 m s=0 3
η η
Qs+1 =Qs+(1+c )√ pmax|α β |(Qs)p−1+c pα β (κs)p−1,
2
M m
p m,i 3M p 1,p 1
max |κs | is upper-bounded by Qs for all s=0,1,··· ,t+1, with high probability.
m̸=1 m
Proof.
37(i) Lower bound of κs
1
by Ps: If κs
1
≥ 21d−1 2, by following the argument for (B.6) in Lemma 21, we have
η √
κs+1 ≥κs+ √ pα β (1−(κs)2)(κs)p−1−q2η Mmax|κs |p−1max|v⊤v |max|α β |
1 1 p 1,p 1 1 m 1 m i m,i
M m̸=1 m̸=1 m,i
√
−q2η Mmax|κs |pmax|α β |−κsη2C2d+ηv⊤(I−wsws⊤)Zs
m i m,i 1 1 1
m̸=1 m,i
η √
≥κs+ √ pα β (1−(1−c )2)(κs)p−1−q2η M|κs|p−1c M−1
1 p 1,p 2 1 1 4
M
√
−q2η M(κs 1)p−1C 2c 3M−1−ηκs 1c 4M− 21d−p− 22 C 12+ηv 1⊤(I−wsws⊤)Zs.
√ √
Note that 1−(1−κs 1)2 ≥ 59c 2, q2η M(κs 1)p−1c 4M−1 ≤ 1 5c 2√η Mpα pβ 1,p(κs 1)p−1, q2η M(κs 1)p−1C 2c 3M−1 ≤
51c 2√η Mpα pβ 1,p(κs 1)p−1 andηκs 1c 4M−1 2d−p− 22C 12 ≤ 51c 2√η Mpα pβ 1,p(κs 1)p−1 (whereweusedκs 1 ≥ 21d−1 2 forthe
last statement).
If κs 1 ≥ 1 2d−1 2 for all s=0,1,··· ,τ, following (B.9) and (B.10), we know that
τ τ
(cid:88) 1(cid:88) η
ηv⊤(I−wsws⊤)Zs ≤c κ0+ c √ pα β (κs)p−1,
1 2 1 5 2 M p 1,p 1
s=0 s=0
with high probability.
Given κs 1 ≥ 21d−1 2 for all s=0,1,··· ,τ, we obtain
τ
(cid:88) η
κτ+1 ≥(1−c )κ0+c √ pα β (κs)p−1, (B.23)
1 2 1 2 p 1,p 1
M
s=0
and κτ 1+1 ≥ 21d−1 2. Therefore, κτ 1 ≥ 1 2d−1 2 holds for all τ =0,1,··· ,t, and (B.23) holds for all τ =0,1,··· ,t
By comparing (B.23) with the update of Pτ, we obtain the desired bound.
(ii) Upper bound of max |κs | by Qs: We have already established the upper bound with Q0 =
m̸=1 m
(1+c 2)max{max m̸=1|κ0 m|,1 2d−1 2} in Lemma 21. Because the choice of Q0 = 6c 3M−1 is larger than that
when M ≤c 4d1 2, the desired bound follows.
Now we show that assumptions (b) and (c) in the previous lemma can be verified along the trajectory
via induction.
Lemma 25. Take ηt =η ≤c 4M− 21d−p 2. Suppose that, for all s=0,1,··· ,t,
(a) κs ≤1−c ,
1 2
(b) |κs |≤κs for all m=2,··· ,M, and
m 1
(c)’ |κs |≤C c M−1 for all m=2,··· ,M.
m 2 3
Then, if M ≤c 4d21 and (a) κt 1+1 ≤1−c
2
for t+1, (b) and (c)’ hold for s=t+1 with high probability.
Proof. We only need to prove (c)’: max |κt+1|≤7c M−1. This is because if (a) and (c)’ for t+1 hold,
m̸=1 m 3
then κ0 >c and Lemma 24 yields that κt+1 ≥Pt+1 ≥P0 =(1−c )κ0 ≥2c M−1 ≥max |κt+1|, which
1 2 1 2 1 3 m̸=1 m
proves (b) for t+1. If
pmax |α β | c
(1+c ) m√ p m,p (Qs)p−1 ≤ 3 pα β (Ps)p−1 (B.24)
2 M M3
2
p 1,p
holds for all s=0,1,··· ,τ, according to Lemma 24, the update of Qs is evaluated as
η
Qs+1 ≤Qs+2c pα β (Ps)p−1
3 M3
2
p 1,p
38foralls=0,1,··· ,τ. Notethat(B.24)holdsfors=0. Thus,if (B.24)holdsforalls=0,1,··· ,τ,wehave,
τ
(cid:88) η
Qτ+1 ≤Qτ1 + 2c pα β (Ps)p−1
s=τ1
3 M3
2
p 1,p
≤Qτ1 +
2c− 21c
3(cid:0) Pτ+1−Pτ1(cid:1) (B.25)
M
≤(cid:18) α pβ 1,p (1+c )−1M−1(cid:19) p−1 1 Pτ1 + 2c− 21c 3(cid:0) Pτ+1−Pτ1(cid:1)
max |α β | 2 M
m p m,p
(cid:18) (cid:19) 1
≤ α pβ 1,p (1+c )−1M−1 p−1 Pτ+1,
max |α β | 2
m p m,p
which implies (B.24) for s=τ +1. Thus, (B.24) holds for all s=0,1,··· ,t+1.
Moreover, similar to (B.25),
Qt+1 ≤Q0+
2c− 21c
3(cid:0) Pt+1−P0(cid:1) ≤Q0+
2c− 21c
3Pt+1 ≤Q0+2C c M−1.
M M 2 3
Q0 is bounded by 6c M−1 Thus, Qt+1 is bounded by C c M−1, which yields (c).
3 2 3
Proof of Lemma 23. Suppose that (a) holds for all s=0,1,··· ,T , where
1,2
(cid:22)(cid:16) (cid:17)−1(cid:23)
T
1,2
= ηp(p−2)c 2(α pβ 1,p)M− 21(P0)(p−2) .
According to Lemma 25, if M ≤ c 4d1 2, η ≤ c 4M− 21d−p 2, and (a) for all s = 0,1,··· ,T 1,22 hold, (b) and (c)
of Lemma 25 holds with high probability for all s=0,1,··· ,T and the bounds of Lemma 24 holds for all
1,2
s=0,1,··· ,T .
1,2
According to Lemma 24 and Lemma 12,
P0
κt ≥Pt ≥ . (B.26)
1 (cid:16) (cid:17) 1
1−ηp(p−2)c 2(α pβ 1,p)M−1 2(P0)(p−2)s p−2
However, at t=T ,
1,2
P0 1
(RHS of (B.26))≥ = ,
(cid:16) (cid:17) 1 (cid:16) (cid:17) 1
ηp(p−2)c 2(α pβ 1,p)M−1 2(P0)(p−2) p−2 η(p−2)c 2(α pβ 1,p)M−1 2 p−2
andthusRHSof (B.26)isclearlylargerthan1. ThisyieldsthecontradictionbecauseκT1,2 shouldbesmaller
1
than 1. Therefore, with high probability, there exists some t ≤T such that κt2 >1−c .
2 1,2 1 2
B.6 Phase III: Strong Recovery and Localization
In the previous section (Lemma 23), we proved that neurons can achieve alignment of 1−c , which sets up
2
the local convergence argument. To simplify the notation, we let t←t−t −t throughout this subsection.
1 2
We write v¯ =(I−v v⊤)v and κ¯t =v¯⊤wt.
m 1 1 m m m j
The goal of this subsection is to prove the following lemma.
Lemma 26. Take ηt = η 1 ≤ c 4M−1 2d−p 2 for 0 ≤ t ≤ (T 1,1 − t 1) + (T 1,2 − t 2) − 1, and ηt = η 2 ≤
min{c 4ε˜M− 21d−1,c 4ε˜2M− 21} for (T 1,1−t 1)+(T 1,2−t 2)≤t≤T 1,3+(T 1,1−t 1)+(T 1,2−t 2)−1. Suppose
|v m⊤ ′v m|≤c 4M−1 forallm′ ̸=mandT
1,3
=Θ˜(ε˜−1M1 2η−1), andconsideraneuronthatsatisfiesκ0
1
≥1−c 2.
Then, κ(T1,1−t1)+(T1,2−t2)+T1,3 >1−3ε˜with high probability.
1
39We bound the update by deterministic auxiliary sequences.
Lemma 27. Let 0 < ε¯ < c 2. If ηt = η ≤ min{c 4ε¯M−1 2d−1,c 4ε¯2M−1 2}, 1−2c
2
≤ κ0 1, κs
1
≤ 1−ε¯ for
s=0,1,··· ,t, and |v⊤ v |≤c M−1 for all m′ ̸=m, we have the following bound:
m′ m 4
• Lower bound of κs:
1
η
κs ≥κ0−c ε¯+sε¯√ pα β ,
1 1 2 p m,p
M
for all s=0,1,··· ,t+1, with high probability.
Proof. If κs
1
≥ 1 2d−1 2, by Lemma 21,
M q
κs+1 ≥κs+ √η (cid:88) (cid:88)(cid:2) iα β (κs )i−1(v⊤v −κsκs )(cid:3) −κsη2C2d+ηv⊤(I−wsws⊤)Zs
1 1 i m,i m 1 m 1 m 1 1 1
M
m=1i=p
M q
≥κs+ √η pα β (1−(κs)2)(κs)p−1+ √η (cid:88) (cid:88)(cid:2) iα β (κs )i−1(v⊤v −κsκs )(cid:3)
1 p 1,p 1 1 i m,i m 1 m 1 m
M M
m=2i=p
−κsη2C2d+ηv⊤(I−wsws⊤)Zs
1 1 1
M q
≥κs+ √η pα β (1−(κs)2)(κs)p−1+ √η (cid:88) (cid:88)(cid:2) iα β (κs )i−1v⊤v (1−(κs)2)(cid:3)
1 p 1,p 1 1 i m,i m 1 m 1
M M
m=2i=p
M q
+ √η (cid:88) (cid:88)(cid:2) iα β (κs )i−1κ¯s κs(cid:3) −κsη2C2d+ηv⊤(I−wsws⊤)Zs. (B.27)
i m,i m m 1 1 1 1
M
m=2i=p
We bound each term of (B.27) from now. If κs ≥1−3c , the second term is bounded by
1 2
9
pα β (1−(κs)2)(κs)p−1 ≥pα β (1−(κs)2)(κs)p−1 ≥ pα β (1−κs).
p 1,p 1 1 p 1,p 1 1 5 p 1,p 1
Next, the third term is bounded by
(cid:12) M q (cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:88) (cid:88)(cid:2) iα iβ m,i(κs m)i−1v 1⊤v m(1−(κs 1)2)(cid:3)(cid:12) (cid:12) (cid:12)≤q2Mm ma ,ix|α iβ m,i|m m̸=ax 1|v 1⊤v m|(1−(κs 1)2)
m=2i=p
≤2q2Mmax|α β |c M−1(1−(κs)2)
i m,i 4 1
m,i
1
≤ pα β (1−κs).
5 p 1,p 1
Then we consider the fourth term,
(cid:12) M q (cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:88) (cid:88)(cid:2) iα iβ m,i(κs m)i−1κ˜s mκs 1(cid:3)(cid:12) (cid:12)
(cid:12)
m=2i=p
M
(cid:88)
≤q2max|α β | |κs ||κ¯s |
i m,i m m
i,m
m=2
M M
(cid:88) (cid:88)
≤2q2max|α β | |κ¯s |3+2q2max|α β | |v⊤v |2|κ¯s |. (B.28)
i m,i m i m,i 1 m m
i,m i,m
m=2 m=2
Toupperboundtheabove, weconsiderthevalueof(cid:80)M |κ¯s |2. Thiscanberepresentedas(cid:80)M |κ¯s |2 =
m=2 m m=2 m
ws⊤(I−v v⊤)⊤A(I−v v⊤)ws, where A=(cid:80)M v v⊤.
1 1 1 1 m=2 m m
40Consider v˜ defined in Lemma 32 with coefficients {c }. Let us define B ∈RM×M as
m m,m′
(cid:40)
c (j ≤i≤M)
B = i,j
i,j
0 (otherwise).
Then we have
(cid:0) v˜ ··· v˜ (cid:1) =(cid:0) v ··· v (cid:1) B⊤.
1 M 1 M
Since the non-diagonal terms are bounded by c M−1 ≤ 1M−1 and the absolute value of diagonal terms
4 3
is no smaller than 1−20c ≥ 2, we know that B⊤ is invertible and (the absolute value of) the maximum
2 3
eigenvalue of (B⊤)−1 is bounded by 3. Each v is computed as
m
v =(cid:0) v˜ ··· v˜ (cid:1) (B⊤)−1e⊤
m 1 M m
and
 v˜⊤
M 1
A= (cid:88)(cid:0) v˜ ··· v˜ (cid:1) (B⊤)−1e e⊤((B⊤)−1)⊤ . . .
1 M m m  . 
m=1 v˜ ⊤
M
Thus,
(cid:18) (cid:19) M
λ (A)≤λ2 (cid:0) v˜ ··· v˜ (cid:1) λ2 ((B⊤)−1)λ (((cid:88) e e⊤))≤9.
max max 1 M max max m m
m=1
Therefore, (cid:80)M |κ¯s |2 = ws⊤(I −v v⊤)⊤A(I −v v⊤)ws ≤ 9∥(I −v v⊤)ws∥2 = 9(1−(κs)2). Based on
m=2 m 1 1 1 1 1 1 1
this, if κs ≥1−3c , we have
1 2
(B.28)≤54q2max|α iβ m,i|(1−(κs 1)2)23 +6q2max|α iβ m,i|max|v 1⊤v m|2(1−(κs 1)2)21
i,m i,m m̸=1
1
≤ pα β (1−κs).
5 p 1,p 1
The fifth term κs 1η2C 12d of (B.27) is bounded by 5√η Mpα pβ 1,p(1−κs 1), when η ≤c 4ε¯M− 21d−1.
Putting things together, if 1−3c ≤κs ≤1−ε¯for all s=0,1,··· ,τ, we have
2 1
6 η
κτ+1 ≥κτ + √ pα β (1−κτ)+ηv⊤(I−wτwτ⊤)Zτ
1 1 5 M p 1,p 1 1
τ τ
(cid:88)6 η (cid:88)
≥κ0+ √ pα β (1−κs)+η v⊤(I−wsws⊤)Zs. (B.29)
1 5 M p 1,p 1 1
s=0 s=1
Moreover, by η ≤c 4M− 21ε¯2,
(cid:12) (cid:12)
(cid:12)(cid:88)τ (cid:12)
(cid:12) ηv⊤(I−wsws⊤)Zs(cid:12)
(cid:12) 1 (cid:12)
(cid:12) (cid:12)
s=0
√
≤ηC τ
1

c 2ε¯ (τ ≤C 2Mε¯−2)
≤ τη
√ pα β (1−κs) (τ >C Mε¯−2)
 p 1,p 1 2
5 M
Therefore, if 1−3c ≤κs ≤1−ε¯for all s=0,1,··· ,τ, (B.29) is bounded by
2 1
τ
(cid:88) η
κτ+1 ≥κ0−c ε¯+ √ pα β (1−κs)
1 1 2 p 1,p 1
M
s=0
41τ
(cid:88) η
≥κ0−c ε¯+ √ pα β ε¯
1 2 p 1,p
M
s=0
η
≥κ0−c ε¯+τ√ pα β ε¯, (B.30)
1 2 p 1,p
M
and 1−3c ≤ κs holds for s = τ +1. By induction, 1−3c ≤ κs holds for all s = 0,1,··· ,t+1, and the
2 1 2 1
bound (B.30) holds for all τ =0,1,··· ,t.
Proof of Lemma 26. First, consider 0≤t≤(T 1,1−t 1)+(T 1,2−t 2). By the choice of η
1
≤c 4M−1 2d−p 2,
η ≤min{c 4M−1 2d−p 2,c 4ε¯2M−1 2}issatisfiedwithε¯=d− 21 inLemma27. Thus,accordingtoLemma27,until
κt >1−ε¯holds(weletτ betheearliesttimethisconditionholds), κt islowerboundedbyκ0−ε¯≥1−2c .
1 1 1 2
One can also see that κt ≥ 1−2c holds for all τ ≤ t ≤ (T −t )+(T −t ). Suppose that there
1 2 1,1 1 1,2 2
exists some τ′ < (T −t )+(T −t ) such that κt < 1−ε¯. Among such τ′, we focus on the earliest
1,1 1 1,2 2 1
time. According to Lemma 19, |κτ′ −κτ′−1| ≤ C η , which implies that κτ′ ≥ 1−2ε¯. Then, According to
1 1 1 1 1
Lemma 27, κt ≥ κτ′ −c ≥ 1−2c until κt gets larger than 1−ε¯again. By repeating this argument, we
1 1 2 2 1
obtain that κ0−ε¯≥1−2c for all 0≤t≥(T −t )+(T −t ).
1 2 1,1 1 1,2 2
Then, we consider (T − t ) + (T − t ) ≤ t ≤ (T − t ) + (T − t ) + T . By the choice of
1,1 1 1,2 2 1,1 1 1,2 2 1,3
η
2
≤ min{c 4ε˜M− 21d−1,c 4ε˜2M−1 2}, η ≤ min{c 4ε¯M− 21d−1,c 4ε¯2M−1 2} is satisfied with ε¯= ε˜in Lemma 27.
Suppose that κs ≤1−ε˜holds for all s=(T −t )+(T −t ),··· ,(T −t )+(T −t )+T , where
1 1,1 1 1,2 2 1,1 1 1,2 2 1,3
(cid:36) (cid:18)
η
(cid:19)−1(cid:37)
T = 3c √ ε˜pα β +1.
1,3 2 p 1,p
M
According to Lemma 27, the bound of Lemma 27 holds for all s=(T −t )+(T −t ),··· ,(T −t )+
1,1 1 1,2 2 1,1 1
(T −t )+T :
1,2 2 1,3
η η
κs ≥1−2c −c ε˜+sε˜√ pα β ≥1−3c +sε˜√ pα β . (B.31)
1 2 2 p 1,p 2 p 1,p
M M
However, at t=(T −t )+(T −t )+T ,
1,1 1 1,2 2 1,3
(RHS of (B.31))≥1,
and thus RHS of (B.31) is clearly larger than 1−ε˜. This yields the contradiction. Therefore, with high
probability, there exists some t ≤(T −t )+(T −t )+T such that κt3 >1−ε˜.
3 1,1 1 1,2 2 1,3 1
Finally, suppose that there exists some t>t such that κt <1−ε˜. Among such t>t , we focus on the
3 1 3
smallest τ. According to Lemma 19, |κτ −κτ−1|≤C η , which implies that κτ ≥1−2ε˜. Then, According
1 1 1 2 1
to Lemma 27, κt ≥1−3ε˜until κt gets larger than 1−ε˜again. By repeating this argument, we obtain that
1 1
κt >1−3ε˜for all t ≤t≤(T −t )+(T −t )+T , which concludes the proof.
1 3 1,1 1 1,2 2 1,3
B.7 Expressivity of the Trained Feature Map
In this section we discuss the expressivity of the feature map after first-layer training. First, we consider the
approximation of single-index polynomials and show the existence of suitable second-layer parameters with
small approximation error and low norm.
ReLU activation. For σ =ReLU, we have the following result.
Lemma 28. Suppose that b ∼Unif([−C ,C ]) with C =O˜(1), and consider the approximation of degree-q
j b b b
polynomial h(s), where q =O (1). Let v ∈Sd−1(1) and v =−v. Then, there exists a ,...,a such that
d − 1 2N
(cid:12) (cid:12)
(cid:12) N 2N (cid:12)
sup (cid:12) (cid:12) (cid:12)21
N
(cid:88) a jσ(v⊤xt+b j)− 21
N
(cid:88) a jσ(v −⊤xt+b j)−h(v⊤xt)(cid:12) (cid:12) (cid:12)=O˜(N−1),
t=T1+1,···,T1+T2(cid:12) j=1 j=N+1 (cid:12)
Moreover, we have (cid:80)2N a2 =O˜(N) and (cid:80)2N |a |=O˜(N).
j=1 j j=1 j
42Proof. According to Lemma 9 of [DLS22], if b∼Unif([−1,1]) and δ ∼Unif({−1,1}), for any k ≥0, there
exists v (δ,b) with |v (δ,b)|≲1 such that for all s with |s|≤1,
k k
E[v (δ,b)σ(δs+b)]=sk.
k
Thus, for C = O˜(1), if b ∼ Unif([−C ,C ]) and δ ∼ Unif({−1,1}), there exists v¯(δ,b;h) with |v¯(δ,b;h)| =
b b b
O˜(1) such that for all s with |s|≤C ,
b
E[v¯(δ,b;h)σ(δs+b)]=h(s).
We take C = Θ˜(1) sufficiently large so that for all xt (t = T + 1,··· ,T + T ), |v⊤xt| ≤ C holds,
b 1 1 2 b
with high probability. For A = Θ˜(N), we consider 2A intervals [−C ,C (−1+ 1)),[C (−1+ 1),C (−1+
b b A b A b
2)),··· ,[C (1− 1),C ]. By taking the hidden constant sufficiently small, for each interval there exists at
A b A b
leastoneb . Then,forb correspondingto[C (−1+ i),C (−1+i+1)),weseta = N
(cid:82)Cb(−1+i+ A1))
v¯(1,b;h)db
j j b A b A j 2 Cb(−1+ Ai)
for 1≤j ≤N, and a = N (cid:82)Cb(−1+i+ A1)) v¯(−1,b;h)db otherwise. Here we note that |a |=O˜(1) holds for all
j 2 Cb(−1+ Ai) j
j. If each interval contains more than one b , we ignore all but one component by letting a = 0. In doing
j j
so, since σ(s+b) is 1-Lipschitz with respect to s, we have
(cid:12) (cid:12)
(cid:12) N 2N (cid:12)
sup (cid:12) (cid:12) (cid:12)21
N
(cid:88) a jσ(v⊤xt+b j)− 21
N
(cid:88) a jσ(v −⊤xt+b j)−h(v⊤xt)(cid:12) (cid:12) (cid:12)=O˜(N−1),
t=T1+1,···,T1+T2(cid:12) j=1 j=N+1 (cid:12)
with high probability. Thus we obtain the assertion.
Polynomial Activation If σ is a degree-q polynomial, we have the following result.
Lemma 29. Suppose that b ∼Unif([−C ,C ]) with C =O˜(1), and consider the approximation of degree-q
j b b b
polynomial h(s), where q =O (1). Then, there exists a ,...,a such that
d 1 N
(cid:12) (cid:12)
(cid:12) N (cid:12)
sup (cid:12) (cid:12) (cid:12)N1 (cid:88) a jσ(v⊤xt+b j)−h(v⊤xt)(cid:12) (cid:12) (cid:12)=O˜(N−1)
t=T1+1,···,T1+T2(cid:12) j=1 (cid:12)
with high probability, where v ∈Sd−1. Moreover, we have (cid:80)N a2 =O˜(N) and (cid:80)N |a |=O˜(N).
j=1 j j=1 j
The lemma depends on the following result.
Lemma 30. Suppose that C ≥ q. For any polynomial h(s) with degree at most q, there exists v¯(b;h) with
b
|v¯(b;h)|≲C such that for all s,
b
E[v¯(b;h)σ(δs+b)]=h(s).
Proof. When g (s)=σ(s) is a degree-q polynomial,
q
(cid:90) 0
g (s)= σ(s+b)db
q
b=−q
is also a degree-q polynomial.
Let us repeatedly define
g (s):=g (s+1)−g (s) (i=1,2,··· ,q),
q−i q−(i−1) q−(i−1)
and let (c ) be coefficients so that (s−1)i = (cid:80)i c sj holds for all z. Then, by induction, g (s) is a
i,j j=0 i,j i
degree-i polynomial. Moreover, we have
i (cid:90) 0
(cid:88)
g (s)= c σ(s+b+j)db
q−i i,j
j=0 b=−q
43(cid:20)(cid:18) i (cid:19) (cid:21)
(cid:88)
=2C E c 1[j−q ≤b≤j] σ(s+b) ,
b b∼Unif([−Cb,Cb]) i,j
j=0
when C ≥ q. Therefore, for any polynomial h(s) with its degree at most q, there exists v¯(b;h) with
b
|v¯(b;h)|≲C such that for all s,
b
E[v¯(b;h)σ(δs+b)]=h(s).
Proof of Lemma 29. We now discretize Lemma 30. For A = Θ˜(N) (with a sufficiently small hidden
constant), we consider 2A intervals [−C ,C (−1+ 1)),[C (−1+ 1),C (−1+ 2)),··· ,[C (1− 1),C ]. By
b b A b A b A b A b
taking the hidden constant sufficiently small, for each interval there exists at least one b . Then, for b
j j
corresponding to [C (−1+ i),C (−1+ i+1)), we set a = N
(cid:82)Cb(−1+i+ A1))
v¯(b;h)db. Here we note that
b A b A j 2 Cb(−1+ Ai)
|a |=O˜(1) holds for all j. Due to Lipschitzness of σ, we have
j
(cid:12) (cid:12)
(cid:12) N (cid:12)
(cid:12) (cid:12) (cid:12)N1 (cid:88) a jσ(s+b j)−h(s)(cid:12) (cid:12) (cid:12)=O˜(N)
(cid:12) j=1 (cid:12)
for all s=O˜(1). Because |v⊤xt|=O˜(1) with high probability, we have
(cid:12) (cid:12)
(cid:12) N (cid:12)
sup (cid:12) (cid:12) (cid:12)N1 (cid:88) a jσ(v⊤xt+b j)−h(v⊤xt)(cid:12) (cid:12) (cid:12)=O˜(N−1)
t=T1+1,···,T1+T2(cid:12) j=1 (cid:12)
with high probability, which yields the assertion.
Next, by using the expressivity results above, we show that there exists some a∗ that can approximate
the additive target function f .
∗
Lemma 31. If J ≳ J minMCplogd, and σ is either of the ReLU activation or any univariate polynomial
with degree q, there exists some parameters a∗ =(a∗)J ∈RJ such that
j j=1
 2
1
T (cid:88)1+T2
1
(cid:88)J
1
(cid:88)M
T 2

J
a∗ jσ(wˆ j⊤xt+b j)− √
M
f m(v m⊤xt) ≤C 1M(|J min|−2+ε˜2),
t=T1+1 j=1 m=1
where ∥a∗∥2
2
=O˜(J2|J min|−1) and ∥a∗∥
1
=O˜(JM1 2).
Proof. We only discuss the case of the ReLU activation; the result for degree-q polynomial link functions
follows from the exact same analysis. Let J˜ be a set of neurons satisfying wˆ ≥ 1−3ε˜ and J˜ be
m,+ j m,−
a set of neurons with wˆ ≤ −1 + 3ε˜. Also, we let J˜ = J˜ ∪ J˜ . According to Lemma B, when
j m m,+ m,−
J ≳J minMCplogd, we have |J˜ m,+|,|J˜ m,−|≥J
min
with high probability.
If j ∈/ (cid:83) J˜ , we set a =0. Also, if J˜ or J˜ contains more than J neurons, we ignore the rest
m m j m,+ m,− min
by simply setting a =0. Then,
j
 2
1
T (cid:88)1+T2
1
(cid:88)J
1
(cid:88)M
T 2

J
a jσ(wˆ j⊤xt+b j)− √
M
f m(v m⊤xt)
t=T1+1 j=1 m=1
1
(cid:88)M
1
T (cid:88)1+T2 (cid:32)√
M (cid:88)
(cid:33)
= a σ(wˆ⊤xt+b )−f (v⊤xt)
M T J j j j m m
2
m,m′=1 t=T1+1 j∈J˜
m
44(cid:32)√ (cid:33)
M (cid:88)
a σ(wˆ⊤xt+b )−f (v⊤ xt) .
J j j j m′ m′
j∈J˜
m′
√
For each m, we evaluate | M (cid:80) a σ(wˆ⊤x+b∗)−f (v⊤x)|. For the j-th neuron in J˜ , by using a
J j∈J˜ m j j j m m m,+ j
in Lemma 28 with N =|J˜ min|, we define a∗
j
← 2|JmiJ n|√ Ma j; similarly for the j-th neuron in J˜ m,−, we define
a∗
j
←
2|J˜
miJ n|√ Ma N+j. We obtain that
(cid:12)√ (cid:12)
(cid:12) M (cid:88) (cid:12)
(cid:12) a∗σ(wˆ⊤x+b∗)−f (v⊤x)(cid:12)
(cid:12) J j j j m m (cid:12)
(cid:12) (cid:12)
j∈J˜
m
(cid:12)√ (cid:12) (cid:12)√ (cid:12)
(cid:12) M (cid:88) (cid:12) (cid:12) M (cid:88) (cid:12)
≤(cid:12) a∗σ(δ v⊤x+b∗)−f (v⊤x)(cid:12)+(cid:12) (a∗σ(wˆ⊤x+b∗)−a∗σ(δ v⊤x+b∗))(cid:12)
(cid:12) J j j m j m m (cid:12) (cid:12) J j j j j j m j (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
j∈J˜
m
j∈J˜
m
≤C (|J |−1+ε˜).
1 min
The norm can be calculated from the construction.
B.8 Fitting the Second Layer
This subsection proves the generalization error in Lemma 14, which concludes the proof of Theorem 1
together with the guarantee for the first-layer training (Lemma B).
Let aˆ be the regularized empirical risk minimizer with L1 or L2 norm regularization:
λ¯
aˆ:=argminLˆ(a)+ ∥a∥r,
r r
a∈RJ
where Lˆ(a):= 1 (cid:80)T1+T2−1(yt− 1 (cid:80)J a σ(wˆ⊤xt+b ))2 and r ∈{1,2}. Let f (x):= 1 (cid:80)J a σ(wˆ⊤x+
b ). Then, by
tT h2
e
eqt= uT iv1+ al1
ence
betJ weej n=1 conj
vex
j regularij
zer and
norm-constraint,a
there
eJ xistsj= λ¯1 >j
0
(wj
hich
j
can be data dependent) such that
Lˆ(aˆ)≤Lˆ(a∗), ∥a∥ ≤∥a∗∥ (r =1 or 2). (B.32)
r r
Indeed, let g : RJ → R∪{∞} be the indicator function of the Lp-norm ball with radius B > 0: g (x) =
B B
(cid:40)
0 (∥x∥ ≤B)
r . Then, the minimizer of the following norm constraint optimization problem
∞ (∥x∥ >B)
r
xˆ:=argminLˆ(x)+g (x),
B
x∈RJ
should satisfy ∇Lˆ(xˆ)∈−∂g (xˆ) where ∂g (xˆ) is the subgradient of g at xˆ. We notice that any element v
B B B
in ∂g (xˆ) can be expressed by v =cv′ for some c≥0 and v′ ∈∂∥x∥ | . This means that the minimizer xˆ
B r x=xˆ
ofthenorm-constrainedproblemisalsotheminimizeroftheregularizedobjectiveLˆ(x)+c∥x∥ . Byresetting
r
the value c as c←c/(p∥xˆ∥r−1), it is also the minimizer of the objective Lˆ(x)+c∥x∥r.
r
Lemma 14. SupposethatJ =Θ(J minMCplog(d)), andσ beeitheroftheReLUactivationoranyunivariate
polynomial with degree q. There exists λ>0 such that the ridge estimator aˆ satisfies
(cid:115)
E x[|f aˆ(x)−f ∗(x)|]≲M21(|J min|−1+ε˜)+
MCp Tlog(d)
2
with probability 1−o d(1). Therefore, by taking T
2
= Θ˜(MCpε−2), ε˜= Θ˜(M−1 2ε), J
min
= Θ˜(M1 2ε−1), and
J =Θ˜(MCp+ 21ε−1), we have E x[|f aˆ(x)−f ∗(x)|]≲ε.
45On the other hand, for LASSO (r =1) we have
(cid:115)
E x[|f aˆ(x)−f ∗(x)|]≲M1 2(|J min|−1+ε˜)+
J m2/ insM2Cp/ Ts+1log(d)2/s
2
with probability 1−o (1), for arbitrary s < ∞ (where the hidden constant may depend on s). Therefore,
d
by taking T
2
= Θ˜(M1+2Cp s+1 ε−2−2 s), ε˜= Θ˜(M−1 2ε), J
min
= Θ˜(M1 2ε−1), and J = Θ˜(MCp+1 2ε−1), we have
E [|f (x)−f (x)|]≲ε (here we ignore polylogarithmic factors).
x aˆ ∗
Proof. Let F := {f | ∥a∥ ≤ ∥a∗∥ } and P be the empirical distribution of the second stage: P :=
a∗ a r r T2 T2
1 (cid:80)T1+T2 δ . If we choose λ as mentioned above, aˆ satisfies the condition (B.32), which yields that
T2 t=T1+1 xt
f ∈F . Therefore, we have that
aˆ a∗
∥f −f ∥
aˆ ∗ L1(Px)
=∥f −f ∥ −∥f −f ∥ +∥f −f ∥
aˆ ∗ L1(Px) aˆ ∗ L1(PT2) aˆ ∗ L1(PT2)
(cid:104) (cid:105)
≤ sup ∥f −f ∥ −∥f −f ∥ +∥f −f ∥ . (B.33)
a∈RJ:∥a∥≤∥a∗∥
a ∗ L1(Px) a ∗ L1(PT2) aˆ ∗ L2(PT2)
(1) First, we bound the second term ∥f −f ∥ . Since Lˆ(f )≤Lˆ(f ), we have that
aˆ ∗ L2(PT2) aˆ a∗
2
T (cid:88)1+T2
∥f −f ∥2 ≤∥f −f ∥2 + (f (xt)−f (xt))νt.
aˆ ∗ L2(PT2) a∗ ∗ L2(PT2) T
2
a∗ aˆ
t=T1+1
By the Hoeffding inequality, we have that
(cid:115) 
T2
T (cid:88)1+T2
(f a∗(xt)−f aˆ(xt))ϵt =O˜ 
∥f
a∗
−f Taˆ∥2
L2(PT2) ,
2 2
t=T1+1
with high probability. The right hand side can be further bounded by
(cid:115)  (cid:115) 
∥f −f ∥2 ∥f −f ∥2 +∥f −f ∥2
O˜

a∗ aˆ L2(PT2) ≤O˜

a∗ ∗ L2(PT2) aˆ ∗ L2(PT2)

T T
2 2
(cid:18) (cid:19)
1 1 1
≤ ∥f −f ∥2 + ∥f −f ∥2 +O˜ ,
2 aˆ ∗ L2(PT2) 2 a∗ ∗ L2(PT2) T
2
by the Cauchy-Schwarz inequality. Then, by moving the term 1∥f −f ∥2 in the right hand side to
2 aˆ ∗ L2(PT2)
the left hand side, we have that
(cid:18) (cid:19)
1
∥f −f ∥2 ≤3∥f −f ∥2 +O˜ ,
aˆ ∗ L2(PT2) a∗ ∗ L2(PT2) T
2
with high probability. This also yields that
√ (cid:18) 1 (cid:19)
∥f −f ∥ ≤ 3∥f −f ∥ +O˜ √
aˆ ∗ L2(PT2) a∗ ∗ L2(PT2)
T
2
(cid:18) (cid:19)
1
=O˜ M1 2(|J min|−1 2 +ε˜1 2)+ √ ,
T
2
where we used ∥f a∗ −f ∗∥ L2(PT2) =O˜(M21(|J min|−1 2 +ε˜1 2)) by Lemma 31.
46(2) The first term in (B.33) can be bounded by the standard Rademacher complexity argument (e.g.,
Chapter 4 of [Wai19]). Specifically, its expectation can be bounded as
(cid:20) (cid:16) (cid:17)(cid:21)
E sup ∥f −f ∥ −∥f −f ∥ (B.34)
(xt) tT =1 T+ 1T +2
1 a∈RJ
a ∗ L1(Px) a ∗ L1(PT2)
(cid:34) (cid:32)
1
T (cid:88)1+T2 (cid:33)(cid:35)
≤2E sup σ |f (xt)−f (xt)|
(xt,σt) tT =1 T+ 1T +2
1 a∈RJ
T
2
t=T1+1
t a ∗
(cid:34) (cid:32)
1
T (cid:88)1+T2 (cid:33)(cid:35) (cid:34)
1
T (cid:88)1+T2 (cid:35)
≤4E sup σ f (xt) +4E σ f (xt) , (B.35)
(xt,σt) tT =1 T+ 1T +2
1 a∈RJ
T
2
t=T1+1
t a (xt,σt)T t=1 T+ 1T +2
1
T
2
t=T1+1
t ∗
(cid:124) (cid:123)(cid:122) (cid:125)
=:Rad(Fa∗)
where (σ )T1+T2 is the i.i.d. Rademacher sequence which is independent of (xt)T1+T2 and we used the
t t=T1+1 t=T1+1
vector valued contraction inequality of the Rademacher complexity in the last inequality [Mau16]. Unfortu-
nately, f (X) is neither bounded nor sub-exponential, and thus we cannot naively apply the Bernstein type
∗
concentration inequality to evaluate the right hand side. Hence, we utilize Markov’s inequality instead to
(cid:104) (cid:105)
convert (B.34) to a high probability bound on sup ∥f −f ∥ −∥f −f ∥ .
a∈RJ:∥a∥≤∥a∗∥ a ∗ L1(Px) a ∗ L1(PT2)
From Lemma 48 of [DLS22] and its proof, for either of ReLU and polynomial activation, we have
(cid:114) 1 ∥a∗∥
Rad(F )≲ r max{(JE [σ (wˆ⊤x+b )s])1/s},
a∗ T 2 J j x j j j
forarbitrarys≤1/(1−1/r). However,sincemax {wˆ ,b }=O(1),wehavemax {E [σ (wˆ⊤x+b )s]}=O(1)
j j j j x j j j
whenevers<∞bynotingthatwˆ⊤xisaGaussiandistributionwithvarianceVar(wˆ⊤x)=O(1),whichyields
j j
that the right hand side can be bounded as
(cid:114) 1 ∥a∗∥
Rad(F )≲ 2 (r =2),
a∗
T J1/2
2
(cid:114) 1 ∥a∗∥
Rad(F )≲ 1 (r =1),
a∗
T J1−1/s
2
where arbitrary s<∞ (the hidden constant depends on s).
Applying Lemma 10 to the second term of (B.35) yields that
(cid:118)
(cid:34)
1
T (cid:88)1+T2 (cid:35) (cid:117)
(cid:117)
(cid:34)
1
T (cid:88)1+T2 (cid:35)
E σ f (xt) ≤(cid:116)E f2(xt)
T t ∗ T ∗
2 2
t=T1+1 t=T1+1
1 (cid:112) 2q+1
=√ E[f2(X)]≲ √ .
T ∗ T
2 2
(3) By combining evaluations of (1) and (2) together and ignoring polylogarithmic factors, we obtain that
1 (cid:114) 1 ∥a∗∥
∥f aˆ−f ∗∥
L1(Px)
≲M1 2(|J min|−1+ε˜)+ √
T
+
T
J1/2r. (B.36)
2 2
We set J =Θ(J minMCplogd). Thus, for r =2, by using Lemma 31, we have
(cid:114)
1 1
(B.36)≲M21(|J min|−1+ε˜)+ √
T
+
T
J1 2|J min|−1 2
2 2
(cid:115)
≲M1 2(|J min|−1+ε˜)+
MCp Tlog(d)
.
2
47Thus, by setting T
2
=Θ˜(MCpε−2), ε˜=Θ˜(M−1 2ε), and J
min
=Θ˜(M1 2ε−1), we obtain that (B.36)≲ε.
Similarly, for r =1, by Lemma 31, we have
(B.36)≲M21(|J min|−1+ε˜)+ √1
T
+(cid:114) T1 JJ 1M −1/1 2
s
2 2
(cid:115)
≲M1 2(|J min|−1+ε˜)+
J m2/ insM2Cp/ Ts+1log(d)1/s
.
2
Thus, by setting T
2
= Θ˜(M1+2Cp s+1 ε−2−2 s), ε˜= Θ˜(M−1 2ε), and J
min
= Θ˜(M1 2ε−1) with a sufficiently large
s, we obtain that (B.36)≲ε.
C Proof of CSQ Lower Bounds
We consider the CSQ lower bound for the following class Fp,q :
d,M,ς
M
1 (cid:88)
x∼N(0,I ), y = √ f (v⊤x)+ν,
d m m
M
m=1
where f = (cid:80)q a He with v ∈ Sd−1, |a | ≳ 1, E [|f (t)|2] = 1 (m = 1,··· ,M), and
m i=p m,i i k m,p t∼N(0,1) m
ν ∼N(0,ς2). For the lower bound we may assume ς =0 since this is the easiest case for the learner.
Thecorrelationalstatisticalquery(CSQ)returnsanexpectationofthecorrelationbetweeny andaquery
function q: X →R up to an arbitrary (adversarial) error bounded by τ.
Definition 2 (Correlationalstatisticalquery). For a function g: X →R and parameters ς, the correlational
statistical query oracle CSQ(g,ς,τ) returns
E [yg(x)]+ν,
x,y
where ν is an arbitrary noise that takes any value in ν ∈[−τ,τ].
Without loss of generality, we assume ∥g∥ =1. We prove the lower bounds on CSQ learner below.
L2
C.1 Proof of Theorem 5(a)
We consider the following model with ς =0:
M
1 (cid:88) 1
x∼N(0,I ), y =f (x)= √ √ He (v⊤x),
d ∗ M p! p m
m=1
where {v ,··· ,v } is a randomly sampled subset (without duplication) of the set S specified below. Also,
1 M
the following lemma guarantees that when M =o˜(dp 4) we have |E[y2]−1|=o(1).
Lemma 32. For any A and d, there exists a set S of A unit vectors in Rd such that, for any u,v ∈ S,
√
u̸=v, the inner product |u⊤v| is bounded by d− 21 2logA.
Proof. Let us sample A independent vectors v 1,··· ,v
A
from the d-dimensional hypercube (cid:2) − √1 ,√1 (cid:3)d .
d d
For each pair of v and v (i̸=j), Hoeffding’s inequality yields
i j
P[|v⊤v |≥t]≤2e−t2d.
i j
48√ √
By setting t=d−1/2 2logA, we have |v⊤v |≤d−1/2 2logA with probability no less than 1− 2 . Taking
√ i j A2
the union bound, |v⊤v | ≤ d−1/2 2logA holds for all (i,j) with probability no less than 1− A(A−1) > 0.
i j A2
This proves the existence of the desired S.
(cid:110) (cid:111)
As a result, we obtain a set of functions √1 p!He p(v⊤x) | v ∈S with small pairwise correlation:
(cid:90) √1 p!He p(u⊤x)· √1 p!He p(v⊤x) (2π1
)d
2
e−∥x 2∥2 dx=(u⊤v)p ≤d−p 2(2logA)p 2.
Based on this calculation, we know that each correlational query cannot obtain information of the true
function, except for the case when the true function is in a polynomial-sized set, as shown in [Sz¨o09].
Lemma33. SupposethatF ={f ,··· ,f }isafinitesetoffunctionssuchthat|E [f (x)f (x)]|≤ε
1 K x∼N(0,Id) i j
for all pairs of f and f with f ̸= f . Then, for any query h satisfying ∥h∥ ≤ 1, there are at most 2
i j i j L2 τ2−ε
functions f
i
that satisfy (cid:12) (cid:12)E x∼N(0,Id)[f(x)h(x)](cid:12) (cid:12)≥τ.
Proof. Let
S :=(cid:8) i∈[K] | E [f (x)h(x)]>τ(cid:9) and S :=(cid:8) i∈[K] | E [f (x)h(x)]<−τ(cid:9) .
+ x∼N(0,Id) i − x∼N(0,Id) i
Then, because ∥h∥≤1, Cauchy-Schwarz inequality yields
 2  2
(cid:88) (cid:88)
|S +|2τ2≤E h(x) f i(x) ≤E  f i(x)  ≲|S +|+ε(|S +|2−|S +|).
i∈S+ i∈S+
Therefore, we have that
1−ε 1
|S |≤ ≤ .
+ τ2−ε τ2−ε
The same argument applies to |S |.
−
Proof of Theorem 5(a).
Consider the number of queries Q, the sequence of query {g ,··· ,g }, and tolerance τ.
1 Q
According to Lemmas 32 and 33, if
2dC
Q· ≤A, (C.1)
τ2−d−p 2(2logA)p
2
for some C >0, there exists at least A(1−d−C) vectors v ∈S such that
(cid:12) (cid:20) (cid:21)(cid:12)
(cid:12) (cid:12)E √1 He p(v⊤x)g i(x) (cid:12) (cid:12)≤τ (i=1,··· ,Q).
(cid:12) p! (cid:12)
Now we consider the minimum value of τ to satisfy (C.1). If we take
A≳Qdp 4+C and τ ≳d−p 4(2ClogQd)p 4,
we have
(LHS of
(C.1))≲Q·dp 4(ClogQd)−p
4
≲A(ClogQd)−p
4 ≤A,
which confirms (C.1).
Now consider the width-M additive model, for each query, we return the value of
(cid:34) M−1 (cid:35)
1 (cid:88) 1
E √ √ He (v⊤x)g (x)
M p! p m i
m=1
49so that the learner cannot find the true v among A(1−d−C) possible directions, with probability at least
M
1−d−C. Failing to do so incurs an L2-error of Ω(cid:0) 1 (cid:1) . This is because for two sets of vectors {v }M and
M m m=1
{v˜ }M in the set S, we have
m˜ m˜=1
(cid:32) M M (cid:33)2 M
1 (cid:88) 1 1 (cid:88) 1 1 (cid:88)
E √ √ He (v⊤x)− √ √ He (v˜⊤x) ≥2−2 (v˜⊤v )p,
M p! p m M p! p m M m˜ m
m=1 m=1 m,m˜=1
and v˜ m⊤ ˜v m = 1 holds for at most M −1 pairs and |v˜ m⊤ ˜v m| ≤ d−p 2(2logA)p 2 for the others if {v m}M m=1 ̸=
{ Lv˜ 2m -˜ e} rM m r˜ o= r1 s. mT alh lee rre tf ho are n, Ow (cid:0)e 1co (cid:1)n wcl iu thde prt oh ba at bi if litτ y≳ moM re− t1 2 hd a− np 4 d( −C Cl .ogQd)p 4, the CSQ learner cannot achieve an
M
C.2 Noisy CSQ and Proof of Theorem 5(b)
Now we prove the latter part of Theorem 5. We first explain why a lower bound with Ω(1) error cannot be
achieved by naively extending the argument for Theorem 5(a).
A naive argument would go as follows. Suppose we construct some fˆ(x) = √1
M
(cid:80)M
m=1
√1 p!He p(vˆ m⊤x),
where {vˆ ,··· ,vˆ }⊂S using queries with tolerance τ . Then ∥f (x)−fˆ(x)∥ ≤1 entails that a constant
1 M 0 ∗ L2
fraction of {v ,··· ,v } should be identified. Therefore, we may use such a CSQ learner to learn a single-
1 M
indexfunction √1
M
√1 p!He p(v 1⊤x)asfollows. Ifweadd √1
M
(cid:80)M
m=2
√1 p!He p(v m⊤x)andapplytheCSQlearnerfor
√1
M
(cid:80)M
m=1
√1 p!He p(v m⊤x),thelearnerwouldidentifyΩ(1)-fractionof{v 1,··· ,v M}withprobabilityΩ(1). On
the other hand, according to Theorem 5(a), learning √1
M
√1 p!He p(v 1⊤x) requires τ ≲M− 21d−p 4(ClogQd)p 4 =
O˜(M− 21d−p 4), with high probability. We may identify v
1
with high probability by repeating this process for
O˜(1) rounds; the CSQ lower bound for single-index model therefore implies that τ
0
≲M− 21d−p 4.
The mistake in the above derivation is that, for the additive model √1
M
(cid:80)M
m=1
√1 p!He p(v m⊤x), since the
target direction v to be hidden is not known by the oracle beforehand, the (adversarial) oracle should
1
prevent the identification of as many directions v ,v ,...,v as possible; whereas in the single-index setting
1 2 M
√1 p!He p(v 1⊤x), the oracle only need to “hide” one direction. Consequently, we cannot directly connect the
identification of Ω(1)-fraction of target directions in the additive model setting to the CSQ lower bound for
learning single-index model.
Toovercomethisissue, weintroducethefollowingsub-classofCSQalgorithmswithi.i.d.noise. Because
the noise is not adversarial but random, the oracle for the single index model does not use the information
of the target direction, and hence the lower bound for single-index model now implies the failure of learning
Ω(1) fraction of directions. On the other hand, since the noise is no longer adversarial, our lower bound in
Theorem 5(b) has weaker the query dependence compared to Theorem 5(a).
Definition 3 (Noisy CSQ). For a function g: X → R and parameters (ς,τ), noisy correlational statistical
query oracle NoisyCSQ(g,ς,τ) returns
E [yg(x)]+ν,
x,y
where ν follows from the following clipped Gaussian distribution:
ν =max{−τ,min{ν˜,τ}}, ν˜∼N(0,ς2).
The clipping operation matches the noisy CSQ with (ς,τ) with the standard CSQ with a tolerance τ.
The following theorem gives a lower bound for noisy CSQ algorithms to learn a single-index polynomial.
Theorem 34. For any p ≥ 0, ς > 0, 1 > τ > 0, Q > 0, C ≫ 0 and M = o˜(dp 4), consider learning
f(x)= √1 p!He p(v⊤x), where v is sampled from some distribution over Sd−1. Suppose that
τ
ς ≲ √ . (C.2)
logQd
50Then, for any learner using Q noisy correlational queries NoisyCSQ(g,ς,τ), the tolerance τ must satisfy
(cid:113)
τ ≲Q21 d−p 2(logdQ)p 2+2.
Otherwise, the learner cannot return fˆ(x) = √1 p!He p(vˆ⊤x) such that ∥f(x)−fˆ(x)∥
L2
≤ 1 with probability
more than O(d−C).
Proof. Due to the choice (C.2), we know that the clipping operation on the Gaussian noise does not make
a difference with probability 1−d−C. Thus in the following we simply consider that the pure Gaussian
noise is added to the expectation. We assume that the distribution where v is sampled from is the uniform
distribution over the set S consisting of A vectors, which is defined in Lemma 32. Here A is taken as
A≃Qdp 2+2,
so that it satisfies
2dC
Q· ≤A.
2d−p 2(2logA)p
2
−d−p 2(2logA)p
2
As an intermediate claim, we show that if E[q 1(x)y],··· ,E[q i(x)y] are bounded by d−p 2(2logA)p 2, then
E[q i+1(x)y] is also bounded by 2d−p 2(2logA)p 2 with probability at least 1 − O(Q−1d−C). Assume that
E[q 1(x)y],··· ,E[q i(x)y] are bounded by d−p 2(2logA)p 2. According to Lemma 33, for each query q(x), there
are at most
d−p
2(22
logA)p
2
vectors that has correlation larger than d−p 2(2logA)p 2. Thus, there are at least
(1−d−C) 2QdC possible vectors that satisfy the assumption. Under this, use Lemma 35 with a =
d−p 2(2logA)p
2
max
(cid:113)
2d−p 2(2logM)p 2 and D =Ω( d−p 2Q (ld ogC A)p
2
). Then, when
(cid:113)
ς ≳Q1 2 d−p 2(logdQ)p 2+1,
we cannot find the desired vector with probability more than O(D−1)=O( Q−1d−C ). This also implies
dp 2(2logA)−p
2
that we cannot find any vector that satisfies |E[q i+1(x)y]| > d−p 2(2logA)p 2 with probability more than
O(Q−1d−C); this is because otherwise we can select one of 2 vectors that satisfy
2d−p 2(2logA)p 2−d−p 2(2logA)p
2
|E[q i+1(x)y]| > d−p 2(2logA)p 2 and output as the prediction of the true vector, which would succeed with
probability more than O(
Q−1d−C
).
dp 2(2logA)−p
2
Now, we obtain that, with probability at least 1−O(d−C), E[q (x)y],··· ,E[q (x)y] are all bounded
1 Q
by d−p 2(2logA)p 2. Again, there are at least (1−d−C)
d−p
22 (2Q ld oC
gA)p
2
possible vectors that satisfy the condi-
tions E[q 1(x)y],··· ,E[q Q(x)y] ≤ d−p 2(2logA)p 2. Under this, we apply Lemma 35 with the same a
max
and
D−1 as previously, and hence we cannot identify the right vector with probability more than O(D−1) =
O( Q−1d−C )≲O(d−C).
dp 2(2logA)−p
2
Therefore, we cannot return the correct vector in A with probability more than O(d−C).
Lemma 35. Let A = (a ,··· ,a ) ∈ RQ (i = 1,··· ,D) be sequences of Q real values satisfying |a | ≤
i i,1 i,Q i,j
a . Suppose that one of A is uniformly randomly chosen an observation (b ,··· ,b ) ∈ RQ is generated
max i 1 Q
as
b ∼N(a ,ς2) (j =1,··· ,Q).
j i,j
Then, if
ς ≳Q21a max(cid:112) logD, (C.3)
any algorithm cannot identify which A is selected with probability more than 1−O(D−1).
i
51Proof. The optimal strategy is to calculate the likelihood function and select the one with which the index
i takes the largest value. Let the likelihood function of the i-th sequence be p (b) for B ∈ RQ. We aim to
i
bound the success probability by O(D−1).
(cid:90) 1 1
maxp (B)dB ≲ . (C.4)
D i i D
To simplify the discussion, we assume ς =1 (because scaling does not affect whether the statement holds).
Then, (C.3) implies a2 ≤2Q−1. We have
max
1 (cid:90) (cid:18) ∥B∥2 (cid:19)
(C.4)×D ≤ exp − +maxB⊤A dB. (C.5)
(2π)Q
2
2 i i
Weboundtheexpectationofexp(max B⊤A )conditionedon∥B∥. ByHoeffding’sinequality,max B⊤A ≤
i i i i
(cid:112)
a ∥B∥ logδ−1 with probability at least 1−δ. Thus,
max
∞
(cid:88) 1 (cid:16) (cid:112) (cid:17)
E [exp(maxB⊤A )]≤ exp a ∥B∥ log(D2i)
B∼SQ−1(∥B∥) i i 2i max
i=1
(cid:90) ∞ (cid:16) (cid:112) (cid:17)
≲ exp a ∥B∥ log(D2t)−tlog2 dt
max
t=1
√ (cid:90) ∞ (cid:16) (cid:112) (cid:17)
≲eamax∥B∥ logD exp a ∥B∥ tlog2−tlog2 dt
max
t=1
(cid:18) a2 ∥B∥2 (cid:112) (cid:19)
≲a ∥B∥exp max +a ∥B∥ logD +1.
max 4 max
Applying this to (C.5) yields
1 (cid:90) (cid:18) (1−Q−1)∥B∥2 (cid:112) (cid:19)
(C.5)≲ a ∥B∥exp − +a ∥B∥ logD dB+1.
(2π)Q
2
max 2 max
Here we used a2 ≤2Q−1. We bound the first term as follows:
max
1 (cid:90) (cid:18) (1−Q−1)∥B∥2 (cid:112) (cid:19)
a ∥B∥exp − +a ∥B∥ logD dB
(2π)Q
2
max 2 max
(cid:32) √ (cid:33)
=
(1− (Q 2π− )1)
Q
2−Q 2+1 (cid:90)
a max∥B′∥exp
−∥B 2′∥2
+
a
ma
(cid:112)x∥ 1B −′∥ Q−lo 1gD
dB′ (B′ =(1−Q−1)1 2B)
=
2a max(1−Q−1)−Q 2+1 (cid:90) ∞ e−s 22 +am√a 1x −s√ Ql −og 1D
sQds (s=∥B′∥)
2Q 2Γ(Q 2) s=0
=
a max2Q 2+1(1−Q−1)−Q
2+1(cid:34)√
2a
max√
logD Γ(cid:18) Q +1(cid:19)
F
(cid:18) Q +1;3
;
a2 maxlogD (cid:19)
2Q 2Γ(Q 2) (cid:112) 1−Q−1 2 1 1 2 2 2(1−Q−1)
(cid:18) Q+1(cid:19) (cid:18) Q+1 1 a2 logD (cid:19)(cid:35)
+Γ F ; ; max , (C.6)
2 1 1 2 2 2(1−Q−1)
where F (x ;x ;x ) is the confluent hypergeometric function of the first kind defined as
1 1 1 2 3
F (x ;x ;x )=
(cid:88)∞ x 1(x 1+1)···(x 1+n−1)xn
3 =
(cid:88)∞ 1 n (cid:89)−1 (x 1+i)x
3.
1 1 1 2 3 x (x +1)···(x +n−1) n! n! x +i
2 2 2 2
n=0 n=0 i=0
We can evaluate 1F 1(cid:16) Q
2
+1; 23;a 22 m (1a −x Qlo −g 1D )(cid:17) as
F
(cid:18) Q
+1:
3
;
a2 maxlogD (cid:19)
=
(cid:88)∞ 1 n (cid:89)−1(Q
2
+1+i)a 22 m (1a −x Qlo −g 1D
) ≲1,
1 1 2 2 2(1−Q−1) n! 3 +i
n=0 i=0 2
52if (Q
2
+1)a 22 m (1a −x Qlo −g 1D
)
≤ 3
2
⇔ 1 ≳ Qa2 maxlogD holds. In the same way, we have 1F 1(cid:16) Q 2+1; 21;a 22 m (1a −x Qlo −g 1D )(cid:17) ≲ 1 if
1≳Qa2 maxlogD holds. Also, (1−Q−1)−Q 2+1 ≲1. Thus, we have
(C.6)≲
a max2Q 2+1(cid:20)
a
(cid:112) logDΓ(cid:18) Q +1(cid:19) +Γ(cid:18) Q+1(cid:19)(cid:21)
2Q 2Γ(Q 2) max 2 2
≤
a max21 2(cid:20)
a
(cid:112) logDΓ(cid:18) Q +1(cid:19) +Γ(cid:18) Q+1(cid:19)(cid:21)
Γ(Q) max 2 2
2
√ (cid:112) (cid:18) Q (cid:19) √ (cid:18) Q(cid:19)1 2
≤a2 2 logD +1 +a 2
max 2 max 2
≲1
where we used Γ(x+ 1) = (cid:82)∞ e−ttx+1 2dt ≤ (cid:0)(cid:82)∞ e−ttxdt(cid:1)1 2(cid:0)(cid:82)∞ e−ttx−1dt(cid:1)1 2 ≤ (Γ(x+1))21(Γ(x))1 2 =
2 t=0 t=0 t=0 √
x1 2Γ(x) (by H¨older’s inequality; this argument is borrowed from [Qi10]) and a maxQ21 logD ≤1. Therefore,
we have successfully obtained (C.4) and the assertion follows.
Proof.[Proof of Theorem 5(b)] Consider learning the following model
M
1 (cid:88) 1
x∼N(0,I ), y =f (x)= √ √ He (v⊤x),
d ∗ M p! p m
m=1
where {v ··· ,v } is a randomly sampled subset (without duplication) of the set S used in the proof of
1 M
Lemma 34. Recall that Lemma 32 guarantees that when M =o˜(dp 4) we have |E[y2]−1|=o(1).
According to Theorem 34, for any learner using Q noisy correlational queries with parameters (ς,τ), to
learn a univariate polynomial √ p1 !MHe p(v⊤x), the tolerance must satisfy
τ ≲
Q21(logdQ)p 4+1
,
M1 2dp
4
otherwise, the learning will fail with probability more than 1−O(d−C).
If an algorithm learns F ⊂Fp,q with Q noisy correlation queries and returns a function with L2-error
d,M,ς
smallerthan1,weknowthatthealgorithmneedtoidentifyatleastΩ(1)-fractionofdirections{v ,··· ,v }.
1 M
If so, we can use such a learner to solve the single-index polynomials, by adding M −1 random functions
√1 p!He p(v m⊤x)tothegivensingle-indexpolynomialandthenapplythealgorithm. Thelowerboundtherefore
follows from the single-index CSQ lower bound stated in Theorem Theorem 34.
D Proof of SQ Lower Bound
This section considers the SQ lower bound for Fp,q . The statistical query oracle is formally defined as
d,M,ς
follows, which covers the previous CSQ as a special case.
Definition 4. For a function g: X ×Y →R and a tolerance τ >0, statistical query oracle SQ(g,τ) returns
any value in
[E [g(x,y)]−τ,E [g(x,y)]+τ].
x,y x,y
In the following, we assume bounded queries SQ: Rd×R→[−1,1].
53As mentioned in the main text, one motivation of our consideration of SQ learner is the existence of
efficient SQ algorithms that can solve multi-index regression beyond the CSQ complexity. Specifically,
the algorithm proposed in [CM20] learns single-index polynomials (i.e., the case of K = 1) with sample
complexity O˜(d) for any constants p,q = O (1); this result can also include the multi-index model up
d
to K = O(1), under a certain non-degeneracy condition. In the first stage of their algorithm, the labels
y are transformed so that the information exponent is reduced to 2, which enables a warm start; after
that, projected gradient descent exponentially converges to the relevant directions. Although [CM20] only
consideredthenoiselesscase(i.e., ς =0), itiseasytoextendtheirstrategytothenoisysetting. Specifically,
the warm-start algorithm can handle label noise with standard concentration arguments, and for the second
stage, O˜(d) sample complexity is also obtained despite the loss of exponential convergence.
However,Theorem6suggeststhatsuchlinear-in-dcomplexityisnolongerfeasibleforSQlearnerstolearn
our additive model class. Specifically, our lower bound implies that for M ≍ dγ with γ > 0, the exponent
in the dimension dependence can be made arbitrarily large by varying p,q = O (1). This highlights the
d
fundamental computational hardness of the larger M setting.
D.1 Superorthogonal Polynomials
Theorem 6 relies on the existence of superorthogonal polynomials defined in Lemma 7. Recall that super-
orthogonality means that a polynomial and its 2-,··· ,K-th exponentiations are orthogonal, in a sense of
inner product with respect to the standard Gaussian, to any polynomials up to degree L whose expectation
is0. ForK =1, f(x)=He (x)satisfiesthecondition. However, forK ≥2, itisfarfromtrivialthatsuch
L+1
a function exists. We defer the proof of Proposition 7 to Appendices D.2 and D.3, and proceed to explain
how Proposition 7 is used in the SQ lower bound.
We utilize the following fact that the expectation E[q(x,y)] can be Taylor expanded with respect to a
small perturbation of y, due to the Gaussian noise added to y.
Lemma 36. Suppose that |g(x,y)|≤1 for any (x,y)∈Rd×R. Then, for δ ≪1, we have
K
(cid:88)
E [g(x,z+δ+ε)]= a (x,z)δk+O(δK+1),
ε∼N(0,ς2) k
k=0
(cid:16) (cid:17)
where a (x,z)= 1 (cid:82) g(x,w) dk e−(w−z)2/2ς2 dw =O(1).
k k! dzk
Proof. The proof follows from the change-of-variables in integration. Specifically, by letting w =z+δ+ε,
E [g(x,z+δ+ε)]
ε∼N(0,ς2)
(cid:90)
=
g(x,z+δ+ε)e−ε2/2ς2
dε
(cid:90)
=
g(x,w)e−(w−z−δ)2/2ς2
dw
=(cid:88)K 1 dk (cid:90)
g(x,w)e−(w−z)2/2ς2 dw·δi
k!dzk
k=0(cid:124) (cid:123)(cid:122) (cid:125)
=:ak(x,z)
+ 1 dK+1 (cid:90) g(x,w)e−(w−z)2/2ς2 dw(cid:12) (cid:12) (cid:12) ·δK+1 (for some z′)
(K+1)!dzK+1 (cid:12)
z=z′
K (cid:90)
(cid:88)
= g(x,w)a (x,z)δk+O(δK+1).
k
k=0
Note that each a (x,z) is O(1) because |g(x,w)|≤1.
i
√
ProofofTheorem6. Supposeforsakeofcontradictionτ ≳dρ. LetustakeA=e dinLemma32. Then,
wehaveasetofunitvectorsS ⊆Sd−1 suchthattwodistinctvectorshaveaninnerproductatmostO(d−1 4).
54To construct the target function, we randomly draw {v }M from S and let f = f for m = 1,··· ,M,
m m=1 m
where f is a superorthogonal polynomial from Proposition 7 with K ≥ 2ρ +2 and L ≥ 4(γ +ρ)−1. In
γ
addition, we construct a different target function √1 (cid:80)M f(v′ ⊤x) with {v′ }M in the same fashion.
M i=1 m m m=1
√
Foreachm,weprovethatthefollowingholdsforatleast1−O˜(e− d(M2d2ρ))fractionofrandomchoices
of v ,v′ :
m m
(cid:34) (cid:32) m−1 M (cid:33)(cid:35)
E g x,√1 (cid:88) f(v⊤ x)+ √1 (cid:88) f(v′ ⊤ x)+ε
m′ m′
M M
m′=1 m′=m
(cid:34) (cid:32) m M (cid:33)(cid:35)
=E g x,√1 (cid:88) f(v⊤ x)+ √1 (cid:88) f(v′ ⊤ x)+ε +o(M−1d−ρ). (D.1)
m′ m′
M M
m′=1 m′=m+1
This is to say, when M is large, swapping one single-index task results in small change in the query value.
To see this, from Lemma 36, we have
(cid:34) (cid:32) m−1 M (cid:33)(cid:35)
E q x,√1 (cid:88) f(v′ ⊤ x)+ √1 (cid:88) f(v⊤ x)+ε
ε m′ m′
M M
m′=1 m′=m
(cid:34) (cid:32) m−1 M (cid:33)(cid:35)
=E q x,√1 (cid:88) f(v′ ⊤ x)+ √1 (cid:88) f(v⊤ x)+ε
ε m′ m′
M M
m′=1 m′=m+1
+ k(cid:88)K =1a k(cid:32) x,√1
M
mm (cid:88) ′− =1 1f(v m′ ′⊤ x)+ √1
M
m′(cid:88) =M m+1f(v m⊤ ′x)(cid:33) fk M(v m⊤ 2ix) +o(M−K 2 ). (D.2)
Note that |(cid:82) f(v⊤x)f(v⊤x)e−∥x∥2/2dx|≲d−(L+1)/4 ≲M−1d−ρ if v ̸=v . Now from Lemma 33, for each k,
i j i j
the number of v that satisfy
m
(cid:12) (cid:12) (cid:12)E(cid:34)
a
(cid:32) x,√1 m (cid:88)−1
f(v′ ⊤ x)+
√1 (cid:88)M
f(v⊤
x)(cid:33) fk(v m⊤x)(cid:35)(cid:12) (cid:12)
(cid:12)≥2M−1d−ρ,
(cid:12) (cid:12) k M
m′=1
m′ M
m′=m+1
m′ M2i (cid:12) (cid:12)
is at most O(M2d2ρ). Thus, except for O(KM2d2ρ) choices of v , taking expectation of (D.2) yields
m
(cid:34) (cid:32) m−1 M (cid:33)(cid:35)
(D.2)=E g x,√1 (cid:88) f(v′ ⊤ x)+ √1 (cid:88) f(v⊤ x)+ε
ε m′ m′
M M
m′=1 m′=m+1
+ k(cid:88)K =1E(cid:34) a k(cid:32) x,√1
M
mm (cid:88) ′− =1 1f(v m′ ′⊤ x)+ √1
M
m′(cid:88) =M m+1f(v m⊤ ′x)(cid:33)(cid:35) E(cid:2) fk M(v m⊤ 2ix)(cid:3) +o(M−K 2 ). (D.3)
Thus in similar fashion,
(cid:34) (cid:32) m M (cid:33)(cid:35)
(D.3)=E g x,√1 (cid:88) f(v⊤ x)+ √1 (cid:88) f(v′ ⊤ x)+ε +o(M−1d−ρ),
m′ m′
M M
m′=1 m′=m+1
which yields (D.1). By recursively applying (D.1), we have
(cid:34) (cid:32) M (cid:33)(cid:35) (cid:34) (cid:32) M (cid:33)(cid:35)
E q x,√1 (cid:88) f(v⊤x)+ε =E q x,√1 (cid:88) f(v′ ⊤ x)+ε +o(d−ρ)
m m
M M
m=1 m=1
√ √ √
for (e d−O˜(M2d2ρ))M ≥e dM(1−e−Ω( d)) choices of v ,··· ,v .
1 M
(cid:104) (cid:105)
Therefore, we may return the value of E g(x,√1
M
(cid:80)M m=1f(v¯ m⊤x)+ε) for a specific choice of {v¯ m}M
m=1
√
fixeda priori,sothateachqueryonlyremovesatmoste−Ω( d) fractionofthepossiblechoicesofv ,··· ,v ,
1 M
55+
% #& % !% % #% #λ#
1 1 1
#$ ! λ # +
$
" =λ − ! +
! !
! ! #$
!
% $% ! ! λ # ! ! − λ $
% !& −
%&
! $1 #λ # $# ! $1 $ $ ! $1
−1 $ −1 −1
−1 ! # 1 −1 −1 ! # 1 −1 −1 ! # 1 −1
Figure 2: Illustration of the proof for I = 3. For each i = 1,2,3, a 2-dimensional curved surface π on which
i
A (a)=0 divides the hypercube. First, we take λ =π . Then, we take the intersection between λ and π , which
i 1 1 1 2
is a curved line and connects one of its boundary on S+ and the other in S−. Finally, we consider the intersection
3 3
ofλ andπ . Becauseλ connectsthepointsinS+ andS− whileπ dividesthehypercubeintothepartcontaining
2 3 2 3 3 3
S+ and the one containing S−, λ =λ ∩π is not an empty set and A (a)=A (a)=A (a)=0 holds on λ .
3 3 3 2 3 1 2 3 3
√
but gives no information about the remaining directions. Thus, with probability at least 1−Qe−Ω( d),
√ √
e dM(1−Qe−Ω( d)) possible choices of v ,··· ,v are equally likely. On the other hand, for each choice of
√ 1 M
√1 (cid:80)M f(v⊤x),thereareatmoste dM/2 possiblechoicesoff(v′⊤),··· ,f(v′ ⊤)iftheL2-errorbetween
M m=1 m 1 M
√1 (cid:80)M f(v⊤x) and √1 (cid:80)M f(v′ ⊤x) is less than 1; so in order to output a function with small O(1)
M m=1 m M m=1 √m
error,weneedtoisolateoneoftheO(e dM)possiblehypotheses. ThiscompletestheproofofTheorem6.
D.2 Reparameterization of Polynomials
The proof of Proposition 7 requires several new techniques. We begin by introducing an auxiliary class of
polynomials {h (x)} parameterized by a = (a ) ∈ [−1,1]L×K that satisfies the three properties below.
a k,l
Thisistoavoidadjustingcoefficientsoff(x)directly,becausesolving(cid:82) (f(x))kHe (x)e−x2/2dx=0(1≤k ≤
l
K,1≤l≤L) as simultaneous high-order equations of the coefficients would be difficult.
(P1) h (x)̸≡0.
a
(P2) For every 1≤k ≤K,1≤l≤L, (cid:82) (h (x))kHe (x)e−x2/2dx is continuous with respect to a.
a l
(P3) (cid:82) (h (x))kHe (x)e−x2/2dx>0 holds if a =1, and (cid:82) (h (x))kHe (x)e−x2/2dx<0 holds if a =−1.
a l k,l a l k,l
The following lemma shows that these three properties entails the existence of a desired superorthogonal
polynomial.
Lemma37. If{h (x)}satisfies(P1)-(P3),thereexistssomecoefficientasuchthat(cid:82) (h (x))kHe (x)e−x2/2dx=
a a l
0 holds for every 1≤k ≤K and 1≤l≤L but (cid:82) (h (x))2e−x2/2dx>0.
a
Proof. As handling two subscripts k,l can be notation-heavy, we prove the following restated claim. For
I ∈N, we consider a vector a∈[−1,1]I and functions A : RI →R, satisfying
i
(P2)’ For every i, A (a) is continuous with respect to a, and
i
(P3)’ A (a)>0 holds if a =1, and A (a)<0 holds if a =−1,
i i i i
and prove that there exists some a ∈ [−1,1]I such that A (a) = 0 for all i. Regarding as (k,l) and A as
i i
(cid:82) (h (x))kHe (x)e−x2/2dx, this is equivalent to the assertion in the above lemma.
a l
56We name each surface of the hypercube [−1,1]I as follows: the surface with a = 1 is denoted as S+
i i
and the surface with a =−1 as S−. Because of (P2)’ and (P3)’, the hypercube [−1,1]I is divided into two
i i
parts by the curved surface π ⊆ [−1,1]I, on which A (a) = 0 holds, and the surface is homeomorphic to
i i
[−1,1]I−1. Clearly, one part contains S+ and the other contains S−.
i i
Now, we inductively see that there exists λ ⊂ [−1,1]I homeomorphic to [−1,1]I−i on which A (a) = 0
i j
holds. This is true for i = 1, by taking λ = π . When this is true for i, and the boundary of λ is on all
1 1 i
surfaces of the hypercube except for S+,··· ,S+ and S−,··· ,S−, we can take λ ⊆ λ ∩π , on which
1 i 1 i i+1 i i+1
A (a) = 0 holds for j = 1,··· ,i+1, which is homeomorphic to [−1,1]I−(i+1), and the boundary of which
j
is on all surfaces of the hypercube except for S+,··· ,S+ and S−,··· ,S− . Therefore by induction, we
1 i+1 1 i+1
obtain λ ⊆λ ∩π , which contains at least one point and on which A (a)=0 holds for j =1,··· ,I. See
I I−1 I j
Figure 2 for illustration of the case where I =3.
Ournextgoalistoconstruct{h (x)}thatsatisfytheaboveproperties. (P1)and(P2)areeasilychecked
a
in the following construction. To meet (P3), we introduce an auxiliary class of functions {h∗(x)} and will
a
later approximate it using polynomials in Section D.3.
We first provide a high-level sketch on the construction of the auxiliary class {h∗(x)} using Figure 3. In
a
particular, we need to adjust the following value to satisfy (P3)
(cid:90)
(f(x))kHe (x)e−x2/2dx. (D.4)
l
First, we fix the exponent k and consider to make the value (D.4) all 0 for l = 1,2,··· ,L. Directly
adjustingcoefficientsoff(x)byregarding(D.4)assimultaneoushigh-orderpolynomialsofthecoefficientsof
f(x) would be difficult. Instead, we re-parameterize the problem as an almost linear simultaneous equation
with respect to the new parameters.
Specifically, we consider a piecewise linear function defined as follows. First, we focus on (D.4) for a
specific k by considering a {0,1}-valued function (See Figure 3(a)). We divide (an interval of) the real line
into equal intervals, which are indexed by i and i , with width ε, and for each interval, we assign a value of
1 2
1 to the left half and assign the remaining portion a value of 0. Then, we consider the value of (D.4), which
is the expectation of a multiplication of this function and a Hermite polynomial He (x). If the interval gets
l
small, this is approximately equal to taking expectations of a multiplication between a constant function
and the Hermite polynomial. In other words, the value of (D.4) gets closer to 0 as the interval gets small
smaller.
To modify the integral value for a specific l, we move the right end of each interval beginning from x ,
i
proportionallytothevalueofHe (x ). IfwemoveeachrightendsO(a εHe (x ))(a isascalar),then(D.4)for
l i l l i l
the specific l changes almost linearly with respect to a , while (D.4) for the other l remains (approximately)
l
the same. In this way, we fine-tune the value of (D.4) for each l separately by changing the parameter a .
l
Next, we consider how to simultaneously address different exponentiations k =1,...,K. We divide each
interval of width ε into K different sub-intervals indexed by 1,...,K (see Figure 3(b)). For the j-th sub-
interval, we let the height of the indicator function to be j . If we change the length of the j-th sub-interval
K
proportionally to εα (j = 1,2,··· ,K), this is approximately equivalent to setting the value of each
i1,i2,j
(f(x))k around x proportionally to
i1,i2
  
α
i1,i2,1

V
· α i1, .i2,2 
 , (D.5)
  . 
  . 
α
i1,i2,K k
where V ∈RK×K is a variant of the Vandermonde matrix defined as follows:
 1 2 ··· 1
K K
(1)2 (2)2 ··· 1
V := K . K . ..
 . . .
 . . .
(1)K (2)K ··· 1
K K
57ε
ε 4&
1
1
!(#) ⋮
2
&
1
&
# ! !(# !) # !"# !(# !"#) # !"$ !(# !"$) # !!+!"ε !!+(!"+ &1 )ε !!+(!"+ &2 )ε !!+(!"+& &−1 )ε !!+(!"+1)# ε
(a) Approximating an integral value with
(b) Addressing different exponentiations
indicator functions
Figure 3: Approximation via piecewise constant function. Figure 3(a): shifting the right end of each indicator
function proportionally to f(x ) is approximately equivalent to subtracting O(f(x)) from (h∗(x))k in the sense of
i a
integral value. Figure 3(b): By considering the staircase function, we can simultaneously modify the contribution of
the different exponents of h∗(x) to the integral.
a
.
The following lemma implies that we can change each coordinate of V ·(α ,α ,. .,α )⊤ in
i1,i2,1 i1,i2,2 i1,i2,K
(D.5)arbitrarilybyadjustingthevaluesofα ,··· ,α . Inthisway, wecancontrolthecontribution
i1,i2,1 i1,i2,K
of (f(x))k to the integral values separately for different k at each interval.
Puttingitalltogether,wecanadjusttheintegralvaluesof (D.4)separatelyforeachkandltosatisfy(P3).
After obtaining a function class {h∗(x)} sketched above, approximating indicator functions by polynomials
a
yields the desired class {h (x)} that satisfies (P1)-(P3) in Section D.3.
a
Lemma 38. The matrix V is invertible.
Proof. Let
 
1 1 ··· 1
 x 1 x 2 ··· x K+1 
V˜ := (x 1)2 (x 2)2 ··· (x K+1)2 . (D.6)
x  . . . 
 . . . 
 . . . 
(x )K (x )K ··· (x )K
1 2 K+1
Because of the formula for the determinant of the block matrix, we have
  
1 1 ··· 1
 
 x 1 x 2 ··· x K  x K+1 
det(V˜ x)=(−1)n·det    (x 1 .)2 (x 2 .)2 ··· (x K . )2  −1·  . . .  (1,··· ,1)  .
   . . . . . .   xK  
K+1
(x )K (x )K ··· (x )K
1 2 K
It is a well-known fact on the Vandermonde matrix that det(V˜ ) ̸= 0 when x ̸= x for all i ̸= j. Let
x i j
x = 1,x = 2,··· ,x = 1,x = 0. Then, the LHS of (D.6) is det(V˜ ) ̸= 0 and the RHS of (D.6) is
1 K 2 K K K+1 x
equal to (−1)ndet(V). Therefore, we have obtained that det(V)̸=0.
Now we formalize the above proof sketch. We let
M 1 = a∈m [−a 1,x
1]K(cid:13) (cid:13)V−1a(cid:13)
(cid:13) ∞
and define the function class {h∗(x)}.
a
Definition 5 (Anauxiliaryclass{h∗(x)}). Let1 (x)(s<t)beanindicatorfunctionsatisfying1 (x)=1
a s,t s,t
for s ≤ x ≤ t and = 0 otherwise. Fix K,L,A , and A , and let ε := 1 . We define a class of functions
1 2 A2
58
⋮{h† α(x)}, parameterized by α=(α i1,i2,j)∈[−1,1]2A1×A2×K,
A (cid:88)1−1 A (cid:88)2−1 (cid:88)K
j
h†(x):= ·1 (x). (D.7)
α
i1=−A1 i2=0 j=1
K i1+i2ε+4( 4j K−1)ε,i1+i2ε+4(j−1)+ 42 K+αi1,i2,jε
Then, we construct a map from a∈RK×L to α as follows. For each i ,i , we define (α(a) )K as
1 2 i1,i2,j j=1
 α(a)  (cid:80)L a He (i +i ε)

 
α(a)i
i
. .
.1 1, ,i i2 2, ,1
2
  := M
11
M
2V−1
 
(cid:80)l
L
l= =1 1a1 2, ,l
lH
. .
.el l(i1 1+i2
2ε)
  ,
α(a) i1,i2,K (cid:80)L l=1a K,lHe l(i 1+i 2ε)
where M :=max max |He (x)|. Based on this, we define h∗(x) by
2 1≤l≤L −A1≤x≤A1 l a
h∗(x):=h† (x)
a α(a)
FromthedefinitionsofM , M , wehave∥α(a)∥ ≤1. Thusin (D.7), eachintervaloftheindicatorfunction
1 2 ∞
does not overlap with the others and the right end is contained in [i +i ε+ 4(j−1)ε,i +i ε+ 4(j−1)+2ε].
1 2 4K 1 2 4K
Lemma 39. There exist constants A and A such that h∗(a) defined in Definition 5 satisfy the property
1 2
(P3). Specifically, when a =1, we have
i,j
(cid:90)
(h∗(x))iHe (x)e−x2/2dx>0,
a j
and when a =−1, we have
i,j
(cid:90)
(h∗(x))iHe (x)e−x2/2dx<0,
a j
for all 1≤i≤L and 1≤j ≤K.
Proof. First, we decompose the integral as follows:
(cid:90)
(h∗(x))kHe e−x2/2dx=
A (cid:88)1−1 A (cid:88)2−1 (cid:88)K (cid:18) j (cid:19)k(cid:90) i1+i2ε+(4(j−1)+2 4+ Kαi1,i2,j)ε
He (x)e−x2/2dx
a l
i1=−A1 i2=0 j=1
K i1+i2ε+4(j 4− K1)ε l
=
A (cid:88)1−1 A (cid:88)2−1 (cid:88)K (cid:18)
j
(cid:19)k(cid:90) i1+i2ε+(4(j− 41 K)+2)ε
He
(x)e−x2/2dx
(D.8)
i1=−A1 i2=0 j=1
K i1+i2ε+4(j 4− K1)ε l
+
A (cid:88)1−1 A (cid:88)2−1 (cid:88)K (cid:18)
j
(cid:19)k(cid:90) i1+i2ε+(4(j−1)+2 4+ Kαi1,i2,j)ε
He
(x)e−x2/2dx
(D.9)
i1=−A1 i2=0 j=1
K i1+i2ε+4(j− 41 K)+2ε l
For the first term, we have
(D.8)=(cid:88)K 2 (cid:18) j (cid:19)k A (cid:88)1−1 A (cid:88)2−1(cid:90) i1+(i2+1)ε
He
(x)e−x2/2dx+O(A−1)
4K K l 2
j=1 i1=−A1 i2=0 i1+i2ε
=(cid:88)K 2 (cid:18) j (cid:19)k(cid:90) A1
He
(x)e−x2/2dx+O(A−1)
4K K l 2
j=1 −A1
=O(A− 21;A 1)O(e−A2 1/2)+O(A− 21), (D.10)
where O(A−1;A ) is the big-O notation that treats A as a constant. Moreover, by the definition of α(a),
2 1 1
(D.9)=
A (cid:88)1−1 A (cid:88)2−1(cid:90) i1+(i2+1)ε (cid:88)K (cid:18) j (cid:19)kα
i1,i2,jHe (x)e−x2/2dx+O(A−1;A )
K 4K l 2 1
i1=−A1 i2=0 i1+i2ε j=1
59=
A (cid:88)1−1 A (cid:88)2−1 1 (cid:90) i1+(i2+1)ε (cid:88)L
a He (i +i ε)He
(x)e−x2/2dx+O(A−1;A
)
M M k,j j 1 2 l 2 1
i1=−A1 i2=0 1 2 i1+i2ε j=1
=
A (cid:88)1−1 A (cid:88)2−1 1 (cid:90) i1+(i2+1)ε (cid:88)L
a He (x)He
(x)e−x2/2dx+O(A−1;A
)
M M k,j j l 2 1
i1=−A1 i2=0 1 2 i1+i2ε j=1
=
a
k,l
(cid:90) A1
He2(x)e−x2/2dx+O(A−1;A )
M M l 2 1
1 2 −A1
a (cid:90)
= Mk M,l He2 l(x)e−x2/2dx+O(A− 21;A 1)+O(e−A2 1/2) (D.11)
1 2
Now, (D.10) and (D.11) yield
(cid:90) a (cid:90)
(h∗ a(x))kHe le−x2/2dx= Mk M,l He2 l(x)e−x2/2dx+O(A− 21;A 1)+O(e−A2 1/2).
1 2
Since 1 (cid:82) He2(x)e−x2/2dx > 0, by taking A sufficiently large and then taking A sufficiently large, we
M1M2 l 1 2
obtain the assertion.
D.3 Polynomial Approximation of h∗
a
Finally,weconsiderthepolynomialapproximationofh∗,whichcanbereducedtopolynomialapproximation
a
ofeachoftheindicatorfunctions. Approximationofthestep/sign/indicatorfunctionshasbeenstudiedsince
thenineteenthcentury[Zol77,Akh90,EY07]. Amongthem,[EY07]consideredthepolynomialapproximation
ofsgn(x)andprovedthefollowingresult: theapproximationerrorL (δ)withdegree-mpolynomials, inthe
m
interval of [−1,−δ]∪[δ,1], satisfies
√ (cid:18) 1+δ(cid:19)m 1−δ
lim m L (δ)= √ .
m→∞ 1−δ m πδ
This entails that, until m becomes larger than δ−1, the error drops proportionally to O(m−1/2δ−1/2). After
that,theerrorexponentiallydecreases,proportionallytoe−δm. However,thisresultisnotdirectlyapplicable
to our Gaussian setting, since the error bound is for a fixed interval. Also, the coefficients of the polyno-
mial are not characterized (hence higher-order polynomials could become larger outside of the interval).
Consequently, increasing the order of polynomials may not give smaller approximation error in expectation.
Therefore, we instead make use of the following fact of Hermite expansion.
Proposition 40 (Theorems 3 and 6 of [Muc70]). Let U(x)=e−x2/2(1+|x|)b and V(x)=e−x2/2(1+|x|)B,
where b<0, B ≥−2/3, and b≤B−1/3. s (x;f) denotes the n-th partial sum of a Hermite series for the
n
target function f. If
(cid:90) ∞
|f(x)|V(x)(1+log+|x|+log+|f(x)|)<∞,
−∞
then we have that
(cid:90) ∞
lim |s (x;f)−f(x)|pU(x)pdx=0.
n
n→∞
−∞
We can therefore approximate 1 with an arbitrary accuracy with respect to the integral values.
0,∞
Lemma 41. There exists a sequence of polynomials {p } such that
n
(cid:90) ∞
lim |p (x)−1 (x)|kHe (x)e−x2/2dx=0
n 0,∞ l
n→∞
−∞
holds for all 1≤k ≤K and 1≤l≤L.
60Proof. Let us take b=−1 and B =0 in Proposition 40. Because 1 (x) is bounded, it is easy to see that
0,∞
(cid:82)∞ 1 (x)V(x)(1+log+|x|+log+|1 (x)|)<∞ holds. We have that
−∞ 0,∞ 0,∞
(cid:90) ∞
0= lim |s (x;1 )−1 (x)|KU(x)Kdx
n 0,∞ 0,∞
n→∞
−∞
(cid:90) ∞
≥ lim |s (x/(K+1);1 )−1 (x)|KHe (x)e−x2/2dx.
n 0,∞ 0,∞ l
n→∞
−∞
Therefore, we arrive at a sequence of polynomials g (x):=s (x/(K+1);1 (·×(K+1)) such that
n n 0,∞
(cid:90) ∞
lim |s (x/(K+1);1 )−1 (x)|kHe (x)e−x2/2dx=0
n 0,∞ 0,∞ l
n→∞
−∞
holds for all 1≤k ≤K and 1≤l≤L.
Nowwehaveobtainedsomep withwhichwecanapproximateeachoftheindicatorfunctionsthatconsistsof
n
h∗ uptoarbitraryaccuracy. Eachfunctionh∗ isnotidenticallyzero,andtheintegralvalueofh iscontinuous
a a a
with respect to x. Therefore, there exists a class of polynomials {h (x)} that satisfies (P1)-(P3).
a
61