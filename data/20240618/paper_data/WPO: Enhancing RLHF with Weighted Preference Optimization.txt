WPO: Enhancing RLHF with Weighted Preference Optimization
WenxuanZhou†,RaviAgrawal,ShujianZhang,SathishReddyIndurthi
SanqiangZhao,KaiqiangSong,SileiXu,ChenguangZhu
ZoomVideoCommunications
Abstract 2017; Ouyang et al. 2022; Glaese et al. 2022) is
a promising approach to better align LLMs with
Reinforcementlearningfromhumanfeedback
humanvalues.
(RLHF)isapromisingsolutiontoalignlarge
language models (LLMs) more closely with Depending on how the outputs are generated,
humanvalues. Off-policypreferenceoptimiza- RLHF can be categorized into on-policy and off-
tion, where the preference data is obtained policysettings. Intheon-policysetting(Schulman
from other models, is widely adopted due to etal.,2017;Yuanetal.,2024;Rossetetal.,2024;
its cost efficiency and scalability. However,
Wuetal.,2024),thepolicymodelusedtogenerate
off-policypreferenceoptimizationoftensuffers
outputsisthesameasthepolicymodelbeingopti-
from a distributional gap between the policy
mized. Duringthisprocess,apolicymodelisfirst
usedfordatacollectionandthetargetpolicy,
leadingtosuboptimaloptimization. Inthispa- initializedfromsupervisedfinetuning(SFT).Then,
per, we propose a novel strategy to mitigate arewardmodel(Schulmanetal.,2017;Gaoetal.,
thisproblembysimulatingon-policylearning 2023;Jiangetal.,2023)isobtainedbasedonhu-
withoff-policypreferencedata. OurWeighted man(Schulmanetal.,2017)orAI(Leeetal.,2023)
PreferenceOptimization(WPO)methodadapts
feedback. Finally, the policy model samples out-
off-policydatatoresembleon-policydatamore
putsduringtraining,whicharethenevaluatedusing
closelybyreweightingpreferencepairsaccord-
therewardmodel. Thepolicymodelisoptimizedto
ing to their probability under the current pol-
improvetheexpectedrewardusingtrainingobjec-
icy. This method not only addresses the dis-
tributionalgapproblembutalsoenhancesthe tivessuchasProximalPolicyOptimization(PPO;
optimization process without incurring addi- Schulmanetal.2017)andDirectPreferenceOpti-
tional costs. We validate our method on in- mization(DPO;Rafailovetal.2023). However,on-
structionfollowingbenchmarksincludingAl- policyRLreliesheavilyonpolicysamplingduring
paca Eval 2 and MT-bench. WPO not only
trainingandonlinerewards,whichcanincurhigh
outperforms Direct Preference Optimization
costs. Incontrast,intheoff-policysetting(Tunstall
(DPO) by up to 5.6% on Alpaca Eval 2 but
et al., 2023; Ivison et al., 2023), the outputs are
alsoestablishesaremarkablelength-controlled
winning rate against GPT-4-turbo of 48.6% generated from different models, and the policy
based on Llama-3-8B-Instruct, making it the modelisoptimizedbasedonthesedatainsteadof
strongest 8B model on the leaderboard. We itssampledoutputs. Consequently,off-policyRL
will release the code and models at https: offerssignificantadvantagesintermsofcostand
//github.com/wzhouad/WPO.
dataefficiencyandiseasiertoscaleup.
1 Introduction Nevertheless,off-policyRLoftenshowsworse
performancethanon-policyRL,duetothedistribu-
Largelanguagemodels(LLMs;Ouyangetal.2022;
tionalgapbetweenthepolicyusedtocollectdata
Achiam et al. 2023; Tunstall et al. 2023; Chung
andthetargetpolicybeingoptimized,whichleads
et al. 2024) have demonstrated remarkable capa-
toinstabilityandinefficiencyintraining(Fujimoto
bilitiesingeneratinghuman-likeresponses. How-
et al., 2019; Kumar et al., 2019, 2020; Xu et al.,
ever,theystillfacechallengesinscenariosdemand-
2024;Tangetal.,2024a;Tajwaretal.,2024). Inoff-
inghighstandardsofreliability,safety,andethics.
policypreferenceoptimization,theoptimizationis
Toaddressthesechallenges,reinforcementlearn-
typically performed on preference data sampled
ingfromhumanfeedback(RLHF;Christianoetal.
fromothermodels,andallthepreferencesingles
†Correspondenceto<wenxuan.zhou@zoom.us> areequallytreated. However,somepreferencedata,
4202
nuJ
71
]LC.sc[
1v72811.6042:viXradistantfromthecurrentpolicy,arelessinformative • We identify the distribution gap problem in
fortraining,resultingininefficientandsuboptimal off-policy preference optimization, and ac-
optimization. cordinglyintroduceamethodtosimulateon-
policyRLusingoff-policypreferencedata.
Inthispaper,weproposesimulatingon-policy
• We propose the WPO objective, which
preferenceoptimizationwithoff-policypreference
reweightspreferencepairsbasedontheirprob-
data, combining the efficiency of off-policy RL
abilities. Thisensuresthatthemostrelevant
withtheperformancebenefitsassociatedwithon-
andprobableoutputsareprioritizedduringop-
policyRL.Ourmethodismotivatedbythefollow-
timization,mitigatingthedistributiongapand
ingconceptualdatagenerationprocess. Thispro-
improvingtheeffectivenessofthepreference
cessbeginswithtransformingtheexistingprefer-
optimization.
encedatasetintoapreferencelabelingfunction. We
• We conduct extensive instruction following
canthenresampleanewpreferencedatasetthrough
benchmarks. Our results demonstrate that
bootstrappingfromtheexistingdata. Thisprocess
WPO significantly outperforms DPO and
involvesuniformlysamplinginputsfromthepref-
achievesnewSOTAresultsonAlpacaEval2
erence dataset and online sampling new pairs of
inthehybridRLsetting.
outputswiththecurrentpolicymodel. Eachpairis
retainedifitcanbelabeledbythelabelingfunction;
2 RelatedWork
otherwise,itisrejected. WethenperformDPOon
theregeneratedpreferencedataset. Inpractice,this Generalalignmentmethods. Theadvancement
bootstrappingprocesscanbeimplementedwiththe of ChatGPT has propelled significant advance-
Weighted Policy Optimization (WPO) objective, mentsinthefieldoflargelanguagemodels(LLMs).
wheredifferentpreferencepairsarereweightedac- Notable models such as Zephyr (Tunstall et al.,
cording to the joint probability of their outputs. 2023)andGPT-4(Achiametal.,2023)haveeffec-
We further devise a weighting alignment mecha- tivelydemonstratedtheapplicationoftechniques
nism to ensure that all on-policy generated pairs likereinforcementlearningfromhumanfeedback
are equally weighted. In this way, WPO can ef- (RLHF;Christianoetal.2017;Ouyangetal.2022;
fectively mitigate the distribution gap during RL Glaeseetal.2022)anddirectpreferenceoptimiza-
withoutincurringadditionalcosts. tion(DPO;Rafailovetal.2023),highlightingtheir
efficacy in achieving improved model alignment.
We evaluate WPO on instruction following
Theseapproaches,alongwithrelatedmethodssuch
benchmarks, including Alpaca Eval 2 (Li et al.,
as sequence likelihood calibration (Zhao et al.,
2023) and MT-bench (Zheng et al., 2023). In
2023) and Generalized Preference Optimization
theoff-policysettingbasedonUltrafeedback(Cui
(GPO)(Tangetal.,2024b),aimtorefinetheobjec-
etal.,2023),WPOimprovesthelength-controlled
tivesofRLHFbyclearlyenhancingthedistinction
winningrateagainstGPT-4-turboonAlpacaEval
between more and less preferred outputs. Addi-
2byupto14.9%overSFTmodel,outperforming
tionally,theintroductionoftheDirectNashOpti-
DPO by up to 5.6%. Particularly, in the hybrid
mization(DNO)algorithmbyRossetetal.(2024)
RL setting where the off-policy preference data
represents a further innovation. This algorithm
is further enriched with on-policy outputs, WPO
utilizescross-entropytoassessthegapbetweenac-
(Figure1)achievesanewSOTAlength-controlled
tualandpredictedwinrates. Practicalapplications
winningrateof48.6%onAlpacaEval2,making
morefrequentlyrelyontheiterativeframeworkof
it the strongest 8B model to date. Additionally,
DPO (Xu et al., 2023). Yet, DPO often reveals a
wefindthatWPOcanbeintegratedintootherloss
discrepancybetweentheoutputdistributionspro-
functions for preference optimization and shows
duced by the policy and those in the preference
consistent improvements. Furthermore, we sys-
dataset. To address this, we propose simulating
tematicallycomparethemodelperformanceindif-
on-policyreinforcementlearningusingoff-policy
ferent RL settings. Our analysis reveals that the
data,therebycombiningthebenefitsofon-policy
hybrid setting, which utilizes both on-policy and
RLwithenhancedefficiency.
off-policypreferencedata,achievesthebestresults,
andon-policy,dispreferreddataismoreimportant
On-policy reinforcement learning. Self-Play
forpreferenceoptimization.
Fine-Tuning(Chenetal.,2024)operatesunderan
Tosummarize,ourcontributionsarethree-fold: iterativeframeworkakintoDPO,utilizinghuman-Direct Preference Optimization (DPO) Weighted Preference Optimization (WPO)
: Write me a poem about the history of jazz : Write me a poem about the history of jazz
0.3
0.9
Policy LM Policy LM
Preference Data Preference Data
Figure 1: Overview of the Weighted Preference Optimization (WPO). Some notations are labeled along with
correspondingcomponents. ExistingDPOdirectlyoptimizesthepolicytobestsatisfythepreferenceswithoff-
policy data. In contrast, WPO adapts off-policy data to resemble on-policy data more closely by reweighting
preferencepairsaccordingtotheirprobabilityunderthecurrentpolicy.
labeled responses as "winners" and outputs from Algorithm 1: Weighted Preference Opti-
previous iterations as "losers" within each pair- mization(WPO)
ing. Similarly, Adversarial Preference Optimiza- Input: Dataset(D)withpromptsandrespon-
tion(Chengetal.,2023)incorporatescontrastive ses,policyLMπ ,totalnumberofiterations
θ
losses,whichobviatetheneedfordirectfeedback T,learningrateα ,
t
fromannotators. Thismethodintroducesatoken- fort = 0toT do
levellossfunctionknownasCringeLoss(Adolphs Sampleamini-batchoftuples(x,y ,y )
w l
etal.,2022),whichdifferentiatesthecorrectsub- fromD,
sequenttokenfromadeliberatelyincorrecttoken
CalculatethealignmentweightviaEq. (2),
from the vocabulary. Pairwise Cringe Loss (Xu ComputeL viaEq. (1),
WPO
et al., 2023) utilizes this cringe loss mechanism Updatepolicyparametersθ usinggradient
withinacontinuouslyimprovingiterativetraining descent: θ ← θ−α ∇θ(x,y ,y ,θ).
t w l
framework. Moreover, the recent introduction of endfor
SAMI(Fränkenetal.,2024)targetsoptimizinga
lower bound on the conditional mutual informa-
tionbetweenpromptsandresponsesthroughacon- Thispreferenceismodeledbyalatentrewardfunc-
trastiveestimationtechnique. Inourapproach,we tion r∗(x,y), which scores on how well the can-
adjust the importance of each pair in the training didate output y matches the input x. There are
processbyassigninggreaterweighttothosepairs variouswaystomodeltherewardfunction,among
morelikelytobesampledfromthepolicymodel, which the Bradley-Terry (BT; Bradley and Terry
thussimulatingon-policyreinforcementlearning. 1952) model is most commonly used. The BT
modelassumesthatthepreferencedistributionis
3 Method
characterizedbythefollowingequation:
In this section, we provide the theoretical back-
ground of RLHF and DPO in Section 3.1. We p(y w ≻ y l|x) = exp(r∗(e xx ,yp w(r )∗ )+(x e, xy pw () r) ∗(x,y l)).
thenintroducethedistributionalgapproblemand
propose the WPO method (Algorithm 1) in Sec- The parameters of the reward function can be es-
tion 3.2. Finally, we explore how to better sim- timatedbasedonmaximumlikelihoodestimation,
ulate on-policy RL through weight alignment in resulting in the reward model rˆ(x,y). Then, we
Section3.3. can use the fitted reward model to provide feed-
backtoalargelanguagemodelbyoptimizingthe
3.1 Preliminaries
followingobjective:
RLHF(Schulmanetal.,2017)aimstoalignalarge
(cid:104) (cid:105)
language model with human preferences. Given max E rˆ(x,y)−βlog πθ(·|x) ,
apreferencedatasetD = {(x(i),y(i) ,y(i) )}N ,in
πθ x∼D,y∼πθ(·|x) πref(·|x)
w l i=1
whichy andy areapairofoutputsgivenprompt whereβ controlsthedeviationbetweenthepolicy
w l
xsampledfromapolicymodel,andy isfavored model π and the reference model π , which is
w θ ref
overy asdeterminedbyhumanorAIannotators. usuallyinitializedfromtheSFTmodel.
lDPO. Direct optimization optimization (DPO; notprioritizelearningfromthemostrepresentative
Rafailovetal.2023)integratesthelearningofthe orprobableoutputofthepolicymodel.
rewardfunctionandthepolicymodeltoaunified To address this issue, we propose to simulate
objective. Specifically, suppose the optimal pol- on-policyRLusingoff-policydata,therebybeing
icyπ∗ isgiven,thecorrespondingrewardr∗ hasa bothfastandenjoyingbenefitsfromon-policyRL.
closedform:
Theoretical derivation. To simulate on-policy
π∗(y|x) RL,wefirsttransformthe(off-policy)preference
r∗(x,y) = βlog +βlogZ(x),
π (y|x) datasetD = {(x(i),y(i) ,y(i) )}N intothefollow-
ref w l i=1
ingpreferencelabelingfunction:
whereZ(x)isthepartitionfunction. Applyingthis
reparameterizationtotheBTmodel,wehave: 
y ≻ y , (x,y ,y ) ∈ D
 1 2 1 2

(cid:16) (cid:17)
p∗(y w ≻y l|x)=σ βlog ππ r∗ ef( (y yw w| |x x) ) −βlog ππ r∗ ef( (y yl l| |x x) ) . f(x,y 1,y 2) = y 2 ≻ y 1, (x,y 2,y 1) ∈ D
 NA, otherwise
Wecanthenformulateamaximumlikelihoodes-
timationobjectiveforthepolicymodelπ onthe
θ whereweassumethatthedatasetcontainsnocon-
preference dataset D, resulting in the following
flicting preferences, meaning that for any x, if
trainingobjective:
(x,y ,y ) ∈ D, then (x,y ,y ) ∈/ D. We then
1 2 2 1
conceptually generate a new preference dataset
L = −E [logp(y ≻ y |x)].
DPO (x,yw,y l)∼D w l throughabootstrappingapproachwithoutactually
carrying out the procedure. Suppose an input x
Here, the loss is calculated based on a uniform
isuniformlysampledfromtheoriginalpreference
sampling of the preference dataset. In practice,
dataset,andthenapairofoutputsy ,y issampled
the y and y in the preference dataset may be 1 2
w l
withthecurrentpolicymodel. Weretainthepair
generatedeitherwiththesamepolicymodelbeing
if it can be labeled by the labeling function, and
optimized,whichcorrespondstoanon-policyRL
setting(Xuetal.,2023;Yuanetal.,2024;Rosset
otherwiserejectthepairwhenf(x,y 1,y 2) = NA.
If we sample for an infinite amount of times, ac-
et al., 2024), or with other models (e.g., GPT-4;
cordingtothelawoflargenumbers,theoccurrence
Achiametal.2023),correspondingtoanoff-policy
rateofapair(x,y ,y )wouldbeproportionalto
RLsetting(Tunstalletal.,2023;Ivisonetal.,2023; w l
π (y |x)π (y |x)p(x). WethenapplyDPOtothe
Paletal.,2024). θ w θ l
newlygeneratedpreferencedataset.
3.2 WeightedPreferenceOptimization
Practical implementation. The conceptual pro-
DPOdoesnotrequireactivelygeneratingnewout- cessaboveisequivalenttooptimizingthefollowing
putsfromthecurrentpolicy,makingitmorecost- weighted preference optimization (WPO) objec-
effectiveandsuitableforoff-policysettings. How- tive,wheredifferentpairsintheoriginalpreference
ever, DPO introduces a notable discrepancy be- datasetarereweighed:
tweenthedistributionofoutputsproducedbythe
policyandthosepresentinthepreferencedataset. L WPO=−E (x,yw,yl)∼D[w(x,yw)w(x,yl)logp(yw≻yl|x)], (1)
This divergence can lead to less effective learn-
ing. Toillustrate,considertwoinstancesofprefer- where w(x,y) = π (y|x) and is detached from
(cid:16) (cid:17) (cid:16) (cid:17) θ
encedata: x(1),y w(1) ,y l(1) and x(2),y w(2) ,y l(2) , back propagation. Through this process, we ef-
where the first tuple is sampled directly from the fectivelyadjusttheimportanceofeachpairinthe
currentpolicymodel,whilethesecondtupleissam- training process, giving greater weight to those
pledfromadifferentdistributionfromthecurrent pairsthataremorelikelytobesampledfromthe
policymodel. Despitethisdifferenceinsampling policymodel,thussimulatingon-policyRL.
probability, DPO treats both instances equally in In language models where y and y are se-
w l
itslosscalculation,ignoringthefactthatthefirst quencesoftokens, theproductoftheconditional
tuple,representingamoreprobableoutputofthe probabilities π (y |x) · π (y |x) can be exceed-
θ w θ l
currentpolicy,shouldideallyexertagreaterinflu- ingly small and exhibit high variance among dif-
ence on the optimization process. This oversight ferentpairs. Toaddressthis,weutilizethelength-
canleadtosuboptimalperformance,asDPOdoes normalized sequence probability as a weightingGreedy alignment. Here, we adjust the weights
5 Alignment Methods
w/o weight alignment withrespecttogreedydecoding. Specifically,we
4 w/ greedy alignment adjustweightsbasedonthemaximumtokenprob-
w/ sampled alignment
ability among the set of all tokens in the subse-
3
quence,definedas:
2
(cid:32) (cid:33)
|y|
1 w(x,y)=exp 1 (cid:80) log πθ(yt|x,y<t) ,
|y|
t=1
maxv∈Vπθ(v|x,y<t)
0
0.2 0.4 0.6 0.8 1.0 1.2
w(x,y) whereV representsthesetofalltokensinthelan-
Figure2: Weightdistributionofoutputssampledusing guagemodel.
thepolicymodelwithdifferentalignmentmethods. Sampled alignment. In this approach, we ad-
factor: just weights based on outputs that are randomly
sampled from the policy model at a temperature
 
|y|
1 (cid:88) of 1.0. Since the probability for each token v
w(x,y) = exp
|y|
logπ θ(y t|x,y <t),
is computed as π θ(v|x,y <t), the expected prob-
t=1 ability of a randomly sampled token would be
(cid:80) π (v|x,y )2, and the calibrated weights
where |y| represents the number of tokens in the v∈V θ <t
arethengivenby:
output.
(cid:32) (cid:33)
|y|
3.3 WeightAlignment w(x,y)=exp 1 (cid:80) log πθ(yt|x,y<t) . (2)
|y|
t=1
(cid:80) v∈Vπθ(v|x,y<t)2
Theobjectiveofourweightingstrategyistosim-
ulate on-policy RL, where different outputs are Weusesampledalignmentasthedefaultalignment
weighedbytheiralignmentwithon-policybehav- method in WPO due to its superior performance,
ior. Therefore, after the outputs are sampled us- asconfirmedinSection4.2. Additionally,inFig-
ingthecurrentpolicymodel,theirweightsw(x,y) ure2,sampledalignmentleadstoamoreconcen-
shouldbeuniform,andtheweightsofoutputsfrom tratedweightdistributionofoutputsfromthepolicy
othermodelsshouldbesmaller. However,ourcur- model,therebybettersimulatingon-policyRL.
rent strategy assigns different weights to outputs
4 Experiment
sampled using the policy model, due to the fact
that LLMs exhibit different levels of confidence
Inthissection,weoutlineourexperimentalsettings
acrossdifferentinputs(Sietal.,2023;Xiongetal.,
(Section 4.1) and present the main results along
2024). This results in an input bias, where some
withablationstudies(Section4.2). Wethencom-
outputssampledusingthepolicymodelwillbeas-
paredifferentRLsettings(Section4.3). Additional
signedhigherweightsthanothers. Figure2shows
analysisofWPOisprovidedinAppendixA.
the weight distribution of sampled outputs based
on prompts from Ultrafeedback and the Mistral- 4.1 ExperimentalSettings
sft-beta model, in which we observe significant
Model configurations. Our methods are imple-
variability inw(x,y). To address thisandensure
mentedbasedontheofficialcodeofzephyr1. For
equal weighting of these outputs, we propose to
Mistral-base,weadopttheofficialhyperparameters
aligntheweightsinWPO.
fromzephyr. Specifically,weusetheSFTcheck-
Adirectmethodistoadjusttheweightsabove
point of zephyr2 as our SFT model. Training is
by the sequence probability of the on-policy out-
conducted over a single epoch with a batch size
puts sampled from the policy model. However,
of 128, a learning rate of 5e-7, a warm-up phase
generatingoutputsduringtrainingiscomputation-
for10%ofthetraining,andacosinedecaysched-
allyexpensive,andhence,weexploreapproxima-
ule. We set β to 0.01 for both DPO and WPO.
tionmethodsforthisalignment. Insteadofusing
ForLlama-3-Instruct,weperformahyperparame-
weightsofthewholesequencesasreference,weop-
tersearchwithintherangerecommendedbyMeng
erateatthetokenlevelandadjusttheprobabilityof
outputtokensaccordingtothetokendistributionin
1https://github.com/huggingface/
alignment-handbook
thepolicymodel,basedonthecurrentsubsequence.
2https://huggingface.co/HuggingFaceH4/
Weproposetwowaystoachievethealignment. mistral-7b-sft-beta
ytisneDMistral-Base(7B) Llama-3-Instruct(8B)
Method AlpacaEval2.0 MT-bench AlpacaEval2.0 MT-bench
Len-control. WinRate Avg. WinRate Len-control. WinRate Avg. WinRate
WinRate vsGPT-4 Score vsDPO WinRate vsGPT-4 Score vsDPO
SFT 9.5 5.8 6.64 - 26.0 25.3 7.97 -
ORPO 14.7 12.6 7.32 - - - - -
KTO 14.9 12.3 7.36 - - - - -
SimPO 21.5 21.4 7.32 - - - - -
DPO 20.6(0.7) 18.6(1.0) 7.36(0.04) 50(0) 28.2(0.5) 24.0(0.5) 8.10(0.05) 50(0)
WPO 24.4(1.4) 23.7(2.1) 7.37(0.10) 60.1(4.7) 33.8(1.3) 31.0(1.8) 8.14(0.05) 58.1(3.4)
DPO 37.9(1.2) 40.3(1.1) 7.14(0.41) 50(0) 44.2(1.2) 48.6(1.0) 8.16(0.10) 50(0)
WPO 42.0(1.7) 46.2(2.3) 7.38(0.08) 56.4(4.6) 45.8(1.3) 50.0(1.1) 8.18(0.22) 54.8(2.2)
+Ultrafeedback 43.1(1.1) 49.6(1.2) 7.23(0.19) 58.8(4.5) 48.6(1.3) 52.1(1.2) 8.14(0.10) 55.1(2.4)
Table1: AlpacaEval2.0andMT-benchresults. Wereporttheaverageandstandarddeviationoftheresultsfrom5
runsofdifferentrandomseeds. Scoresthatareunderlineddenotestatisticallysignificantgains(p<0.05).
etal.(2024). Ourfinalhyperparametersarealearn- model’sresponseandgpt-4-turbo’sresponseare
ingrateof1e-6,twotrainingepochs,andβ of0.01 compared head-to-head using an auto-evaluator.
forbothDPOandWPO.Foralltrainingconfigura- The win rate is the probability that the auto-
tions,weconducttrainingfor5runswithdifferent evaluatorpreferstheevaluatedmodel’sresponses.
randomseedsandreportboththeaverageresults AlpacaEval2alsointroducesalength-controlled
andtheirstandarddeviation. winrate(Duboisetal.,2024)toaddressthelength
bias of gpt-4-turbo. We follow the generation
Training data. We perform RLHF in off-policy
configurationsinTunstalletal.(2023)forMistral
and hybrid settings. In the off-policy setting, we
models and in Zheng et al. (2024) for Llama-3
usethebinarizedUltrafeedbackdataset3(Cuietal.,
models.
2023), which compromises 63k preference pairs
MT-benchisanLLM-basedautomatedevalua-
sampled from models other than our SFT model,
tion metric comprising 80 challenging questions.
suchasGPT-4andLlama-2(Touvronetal.,2023).
Wereportresultsusingtwoscoringmethods. Inthe
In the hybrid setting, we follow the approach in
singleanswergradingapproach,theauto-evaluator
DNO (Rosset et al., 2024), using data generated
(gpt-4-0613) assigns scores from 1 to 10 to re-
from both the policy model and other models.
sponses, and we report the average scores. In
Specifically, we sample 5 outputs from the SFT
the pairwise comparison approach, the evaluator
modelbasedonpromptsfromUltrafeedbackand
(gpt-4-0613) compares two responses to decide
addanotheroutputgeneratedbygpt-4-turbo. We
which is better or if it’s a tie (recorded as 0.5 in
employtop-psamplingwithp = 0.95andatemper-
win rate). The pairwise method can detect more
atureof0.7. Preferenceannotationsareproduced
subtle differences between responses than single
usinggpt-4-turbowithadditivescoringprompt.
answer grading. We use the official generation
For each prompt, we select outputs scoring 5 or
configurationsinMT-bench.
6 as y and then choose a random output with a
w
scoreatleastonepointlowerasy . Ifsuchapair
l 4.2 MainResultsandAblation
cannotbefound,thepromptisnotused. Thisdata
construction step produces a smaller preference WPOconsistentlyandsignificantlyoutperforms
dataset,sowefurtheremploythe+Ultrafeedback DPOanditsvariants. Themainresultsareshown
setting, where we add the missing prompts back in Table 1. We include the results of different
usingthepreferencepairsfromUltrafeedback. preferenceoptimizationalgorithmssuchasDPO,
ORPO(Hongetal.,2024),KTO(Ethayarajhetal.,
Evaluation. We evaluate the models on Alpaca
2024),andSimPO(Mengetal.,2024)onthetwo
Eval 2 and MT-bench. Alpaca Eval 2 is an au-
benchmarks. For ORPO, KTO, and SimPO, we
tomated metric that measures LLMs’ alignment
reporttheevaluationresultsoftheirofficialmodel
withhumanpreferencesusing805representative
checkpointsonMistral-base.4 WefindthatWPO
instructions. For each instruction, the evaluated
4WedonotincludetheirresultsonLlama-3-Instructinthe
3https://huggingface.co/datasets/ off-policysettingastheofficialcheckpointsareunavailable.
HuggingFaceH4/ultrafeedback_binarized Reproducingthesemethodsrequiresextensivehyperparameter
ycilop-ffO
dirbyHAlpacaEval2.0 MT-bench AlpacaEval2.0 MT-bench
Method Method
Len-control. WinRate WinRate Len-control. WinRate WinRate
WinRate vsGPT-4 vsDPO WinRate vsGPT-4 vsBaseline
WPOw/sampledalign. 24.4 23.7 60.1 IPO 25.0 21.2 50
WPOw/greedyalign. 23.0 21.4 57.9 SimPO 21.5 21.4 50
WPOw/oalign. 22.0 20.3 54.4 KTO 14.9 12.3 50
DPO 20.6 18.6 50
WPO 29.4 25.7 54.1
IPO
WPO 21.9 24.6 52.5
Table 2: Ablation of weight alignment methods on SIMPO
WPO 21.1 20.3 60.0
KTO
Mistral-base in the off-policy setting. sampled align-
ment,thedefaultweightalignmentmethod,yieldsthe Table3: ResultsofWPOwithdifferentlossfunctions
bestresults. forpreferenceoptimizationonMistral-baseintheoff-
policy setting, which show that incorporating WPO
generallyoutperformsDPOinallsettingsandalso
leadstoconsistentimprovements.
outperformsallitsvariantsonMistral-baseinthe
off-policysetting. Particularly,whentrainedwith inadditiontoDPO,thereareotherlossfunctions
the Llama-3-Instruct model and the hybrid +Ul- foraligningLLMs. SinceWPOworksbyweigh-
trafeedbacksetting,WPOachievesanewstate-of- ingpreferencedataandisindependenttotheloss
the-artlength-controlledwinrateof48.6%against functionbeingused,itcanbeeasilyintegratedinto
GPT-4-turboonAlpacaEval2. Theseresultshigh- them. We investigate whether the integration of
lighttheeffectivenessofWPO.Additionally,while WPOenhancestheperformanceofotherlossfunc-
DPO underperforms compared to SimPO, it still tions. Existinglossescanbecategorizedintothose
demonstratescompetitiveresults,providingasolid usingpairedpreferencedataandthoseutilizingun-
basisforWPO. pairedpreferencedata. Forlossesusingpaireddata,
Variedseparationofbenchmarks. OnMT-bench, we weigh each pair similarly to DPO. For losses
theaveragescoredoesnoteffectivelydistinguish usingunpaireddata,weweigheachoutputy inde-
theperformanceofdifferentmodels. Additionally, pendentlywithw(x,y)andnormalizetheweights
we observe variability in the average MT-bench sothatthetotalweightsoffavoredoutputsanddis-
score. EvenwhenusingGPT-4toscorethesame favoredoutputsareboth1withinthebatch. This
outputswithatemperatureof0,thescorecanvary normalizationensuresabalancebetweenfavored
by up to 0.1 at different times. Given the clearer anddisfavoredoutputsintheloss. Inthisstudy,we
separationinourexperimentsandthegreateralign- consideredIPO(Azaretal.,2024)andSimPOfor
mentwithhumanevaluations,asshownintheorigi- alignmentwithpaireddata,andKTOforalignment
nalpaper(Zhengetal.,2023),weconsiderpairwise withunpaireddata. TheresultsonMistral-basein
winratetobeamoresuitablemetricforassessing the off-policy setting, shown in Table 3, indicate
differentalignmentmethods. Therefore,weuseit thatintegratingWPOleadstoimprovedresultsfor
forMT-benchinthefollowingpartofthepaper. all loss functions. This demonstrates that WPO
providesuniversalimprovementsacrossdifferent
Sampled weight alignment works the best. Ta-
lossfunctionsforpreferenceoptimization.
ble 2 shows the results of WPO with different
weightalignmentmethodsonMistral-baseinthe
4.3 ComparisonofDifferentRLSettings
off-policysetting. Weobservethatsampledalign-
mentoutperformsothervariationsonbothbench-
Recent studies on RLHF have employed various
marks, while greedy sampling outperforms w/o
RLsettingswherepreferencedataisgeneratedin
alignment. Wealsofindthattherankingofperfor-
anoff-policy,on-policy,orhybridmanner. Exist-
mancematchestherankingofconcentrationlevels
ingwork(Tangetal.,2024a;Xuetal.,2024)has
intheweightdistributionshowninFigure2. This
demonstratedthaton-policypreferenceoptimiza-
indicates that weight alignment enables a more
tionoutperformsoff-policymethods,whileRosset
effective simulation of on-policy RL, leading to
etal.(2024)showthatincorporatinghigh-quality
improvedperformance.
off-policyoutputscanyieldsuperiorperformance,
WPOalsoimprovesotherlossfunctionsforpref- astheseoutputscanintroducevaluableinformation
erenceoptimization. Itisimportanttonotethat, thatthecurrentpolicymightnotencounteronits
own. Inthisstudy,wecomparemodelperformance
searches, whichmaynotyieldtheoptimalhyperparameter
valuesforafaircomparison. trained with WPO across these RL settings. TheMistral-Base Llama-3-Instruct Mistral-Base Llama-3-Instruct
70 70 50
AlpacaEval DPO
60 MtBench 60 45 WPOw
40
WPOl
50 50 WPO
35
40 40
30
30 30
25
20 20
20
10 10 15
0 0 10
Off-Policy On-Policy Hybrid Off-Policy On-Policy Hybrid Off-Policy On-Policy Hybrid Off-Policy On-Policy Hybrid
Settings Settings Settings Settings
Figure3: ResultsofWPOindifferentRLsettings. The Figure4: ResultsofvariationsofWPOindifferentRL
hybrid setting consistently yileds better results than settings.
otherRLsettings.
WPOisgivenby:
results are presented in Figure 3, showcasing the
∇L =−βw(x,y )w(x,y)σ(rˆ(x,y)−rˆ(x,y ))
WPO w l l w
length-controlled win rate on Alpaca Eval 2 and
[ ∇logπ(y |x) − ∇logπ(y |x) ].
w l
the pairwise win rate compared to the off-policy
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
settingonMT-bench. increasetheprobabilityofyw reducetheprobabilityofyl
Thatis,WPOwillmakethepolicymodelmimicy
w
Hybrid RL achieves the best results. Figure 3
whilemovingawayfromy . Giventheirdifferent
l
shows that for both Mistral-base and Llama-3-
optimizationdirections,weinvestigatetheimpor-
Instruct, the hybrid setting—utilizing both on-
tanceofon-policysamplingfory andy inprefer-
w l
policydataandhigh-qualityoff-policydatafrom
enceoptimization. Toachievethis,wefurtherstudy
gpt-4-turbo—consistentlydeliverssuperiorper-
twodifferentvariantsofWPO,namelyWPO and
W
formance. This suggests that combining high-
WPO . These losses are formulated as follows:
L
qualityoff-policydataandon-policydatacansig-
nificantlyenhancepreferenceoptimization,which L WPO=−E (x,yw,yl)∼D[w(x,yw)w(x,yl)logp(yw≻yl|x)],
isconsistenttotheresultsinRossetetal.(2024). L WPOW =−E (x,yw,yl)∼D[w(x,yw)logp(yw≻yl|x)],
L
WPOL
=−E (x,yw,yl)∼D[w(x,yl)logp(yw≻yl|x)],
On-policy is not always better than off-policy.
whereinWPO ,weonlyincreasetheweightsof
Our analysis reveals that the effectiveness of on- W
pairswherey ismoreclosedtoon-policyoutputs.
policy versus off-policy preference optimization w
ForWPO ,weonlyincreasetheweightsofpairs
ismodel-dependent(Munosetal.,2016;Voloshin L
wherey isclosertoon-policyoutputs. Resultson
etal.,2019). FortheMistral-basemodel,off-policy l
Mistral-baseandLlama-3-InstructareinFigure4.
settingyieldsslightlybetterperformance,whilefor
ItshowsthatWPO generallyachievessimilarre-
Llama-3-Instruct, on-policy setting shows better L
sultstoWPO.Conversely,WPO consistentlyun-
performance. Weattributethisvariationtothequal- W
derperformsWPOandevenunderperformsDPO
ityoftheSFTmodel. InthecaseofMistral-base,
in most settings. Therefore, making y on-policy
thesampledoutputsareoflowerquality,causing l
explainsmostoftheimprovementsofWPO,while
thepreferenceoptimizationprocesstomimicsub-
making y on-policy is still useful but not as im-
optimaloutputsandleadingtopoorerresults. This w
portant. Thisfindingsuggeststhatusingon-policy,
highlights the importance of the initial policy’s
dispreferreddataisimportantforpreferenceopti-
qualityandsuggeststhatmodelswithhigherinitial
mization,whileusingon-policypreferreddatamay
performance might benefit more from on-policy
bebeneficialbutnotascritical.
optimization,whilethosewithlowerinitialquality
maynotgainasmuch.
5 Conclusion
Thedispreferreddatashouldbeon-policy,the In this study, we tackled the distributional gap
preferred data benefits less. While WPO simu- probleminherentinoff-policypreferenceoptimiza-
lateson-policydatabyweighingbothy andy in tion. By introducing Weighted Preference Opti-
w l
thepreferencedata,thesetwooutputsplaydiffer- mization (WPO), we successfully simulated on-
entrolesduringoptimization. Thegradientofthe policy preference optimization using off-policy
etaR
niW
.lortnoc-neL
2
lavE
acaplA
gnitteS
ycilop-ffO
sv
etaR
niW
hcneb-TM
etaR
niW
.lortnoc-neL
2
lavE
acaplApreference data, merging the benefits of both ap- MohammadGheshlaghiAzar,ZhaohanDanielGuo,Bi-
proaches. Ourmethodnotonlyaddressedthedis- lalPiot,RemiMunos,MarkRowland,MichalValko,
andDanieleCalandriello.2024. Ageneraltheoret-
tributionalgapwithoutincurringadditionalcosts
ical paradigm to understand learning from human
butalsoenhancedtheeffectivenessofpreference
preferences. In International Conference on Arti-
optimization. Extensiveexperimentsdemonstrate ficialIntelligenceandStatistics, pages4447–4455.
thatWPOcanproducebetterLLMsthataremore PMLR.
closelyalignedwithhumanpreferences.
EdwardBeeching,ClémentineFourrier,NathanHabib,
SheonHan,NathanLambert,NazneenRajani,Omar
Limitations
Sanseviero,LewisTunstall,andThomasWolf.2023.
Openllmleaderboard. https://huggingface.co/
Theperformancegapbetweenoffandon-policy spaces/HuggingFaceH4/open_llm_leaderboard.
preference optimization remains. Although
RalphAllanBradleyandMiltonETerry.1952. Rank
WPOsimulateson-policyRLwithoff-policydata,
analysisofincompleteblockdesigns: I.themethod
it does not fully bridge the performance gap be- of paired comparisons. Biometrika, 39(3/4):324–
tweenoff-policyandon-policyRL.Asshowninthe 345.
results, evenwithWPO,off-policymethodsmay
ZixiangChen,YiheDeng,HuizhuoYuan,KaixuanJi,
stillunderperformcomparedtoon-policyandhy- andQuanquanGu.2024. Self-playfine-tuningcon-
bridmethods. Therefore,whileweproposeWPO vertsweaklanguagemodelstostronglanguagemod-
els. arXivpreprintarXiv:2401.01335.
asasolution,itdoesnotentirelyeliminatetheper-
formancedisparity,andon-policypreferencedata
Pengyu Cheng, Yifan Yang, Jian Li, Yong Dai, and
remainsimportant. Futureworkwillbeonhowto NanDu.2023. Adversarialpreferenceoptimization.
furtherreducethisperformancegapwithoutincur- arXivpreprintarXiv:2311.08045.
ringadditionaltrainingcosts.
PaulFChristiano,JanLeike,TomBrown,MiljanMar-
Comprehensivenessofpreferencedataset. The tic, Shane Legg, and Dario Amodei. 2017. Deep
reinforcementlearningfromhumanpreferences. Ad-
goalofourexperimentsistocompareWPOwith
vancesinneuralinformationprocessingsystems,30.
other preference optimization algorithms, not to
provide a comprehensively aligned LLM. In our HyungWonChung,LeHou,ShayneLongpre,Barret
experiments, we use Ultrafeedback as the prefer- Zoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi
Wang,MostafaDehghani,SiddharthaBrahma,etal.
encedata,whichprimarilyfocusesonhelpfulness,
2024. Scalinginstruction-finetunedlanguagemodels.
truthfulness, and instruction following, and does JournalofMachineLearningResearch,25(70):1–53.
not include safety aspects. Additionally, it does
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao,
notconsiderpreferenceoptimizationformulti-turn
WeiZhu,YuanNi,GuotongXie,ZhiyuanLiu,and
conversations. Futureworkshouldinvolvecollect-
MaosongSun.2023. Ultrafeedback: Boostinglan-
ing more comprehensive preference datasets and guage models with high-quality feedback. arXiv
integratingmultipleaspectsofpreferenceoptimiza- preprintarXiv:2310.01377.
tiontotrainbetter-alignedLLMs.
YannDubois,BalázsGalambosi,PercyLiang,andTat-
sunori B Hashimoto. 2024. Length-controlled al-
pacaeval: Asimplewaytodebiasautomaticevalua-
References tors. arXivpreprintarXiv:2404.04475.
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff,
Ahmad, Ilge Akkaya, Florencia Leoni Aleman, DanJurafsky,andDouweKiela.2024. Kto: Model
DiogoAlmeida,JankoAltenschmidt,SamAltman, alignmentasprospecttheoreticoptimization. arXiv
ShyamalAnadkat,etal.2023. Gpt-4technicalreport. preprintarXiv:2402.01306.
arXivpreprintarXiv:2303.08774.
Jan-Philipp Fränken, Eric Zelikman, Rafael Rafailov,
LeonardAdolphs,TianyuGao,JingXu,KurtShuster, KanishkGandhi, TobiasGerstenberg, andNoahD
SainbayarSukhbaatar,andJasonWeston.2022. The Goodman. 2024. Self-supervised alignment with
cringe loss: Learning what language not to model. mutual information: Learning to follow princi-
arXivpreprintarXiv:2211.05826. ples without preference labels. arXiv preprint
arXiv:2404.14313.
AmandaAskell,YuntaoBai,AnnaChen,DawnDrain,
DeepGanguli,TomHenighan,AndyJones,Nicholas ScottFujimoto,DavidMeger,andDoinaPrecup.2019.
Joseph,BenMann,NovaDasSarma,etal.2021. A Off-policydeepreinforcementlearningwithoutex-
generallanguageassistantasalaboratoryforalign- ploration. In International conference on machine
ment. arXivpreprintarXiv:2112.00861. learning,pages2052–2062.PMLR.LeoGao,JohnSchulman,andJacobHilton.2023. Scal- LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
inglawsforrewardmodeloveroptimization. InIn- CarrollWainwright,PamelaMishkin,ChongZhang,
ternationalConferenceonMachineLearning,pages SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
10835–10866.PMLR. 2022. Training languagemodelsto followinstruc-
tionswithhumanfeedback. Advancesinneuralin-
Amelia Glaese, Nat McAleese, Maja Tre˛bacz, John formationprocessingsystems,35:27730–27744.
Aslanides,VladFiroiu,TimoEwalds,MaribethRauh,
LauraWeidinger,MartinChadwick,PhoebeThacker, Arka Pal, Deep Karkhanis, Samuel Dooley, Man-
etal.2022. Improvingalignmentofdialogueagents ley Roberts, Siddartha Naidu, and Colin White.
via targeted human judgements. arXiv preprint 2024. Smaug: Fixing failure modes of prefer-
arXiv:2209.14375. enceoptimisationwithdpo-positive. arXivpreprint
arXiv:2402.13228.
Jiwoo Hong, Noah Lee, and James Thorne. 2024.
RafaelRafailov,YaswanthChittepu,RyanPark,Harshit
Orpo: Monolithic preference optimization without
Sikchi, Joey Hejna, Bradley Knox, Chelsea Finn,
referencemodel. arXivpreprintarXiv:2403.07691,
and Scott Niekum. 2024. Scaling laws for reward
2(4):5.
model overoptimization in direct alignment algo-
rithms. arXivpreprintarXiv:2406.02900.
Hamish Ivison, Yizhong Wang, Valentina Pyatkin,
Nathan Lambert, Matthew Peters, Pradeep Dasigi,
RafaelRafailov,ArchitSharma,EricMitchell,Christo-
Joel Jang, David Wadden, Noah A Smith, Iz Belt-
pherDManning,StefanoErmon,andChelseaFinn.
agy,etal.2023. Camelsinachangingclimate: En-
2023. Directpreferenceoptimization:Yourlanguage
hancing lm adaptation with tulu 2. arXiv preprint
modelissecretlyarewardmodel. AdvancesinNeu-
arXiv:2311.10702.
ralInformationProcessingSystems,36.
DongfuJiang,XiangRen,andBillYuchenLin.2023. Corby Rosset, Ching-An Cheng, Arindam Mi-
LLM-blender: Ensembling large language models tra, Michael Santacroce, Ahmed Awadallah, and
withpairwiserankingandgenerativefusion. InPro- Tengyang Xie. 2024. Direct nash optimization:
ceedingsofthe61stAnnualMeetingoftheAssocia- Teachinglanguagemodelstoself-improvewithgen-
tionforComputationalLinguistics(Volume1: Long eralpreferences. arXivpreprintarXiv:2404.03715.
Papers),pages14165–14178,Toronto,Canada.As-
sociationforComputationalLinguistics. John Schulman, Filip Wolski, Prafulla Dhariwal,
Alec Radford, and Oleg Klimov. 2017. Proxi-
AviralKumar,JustinFu,MatthewSoh,GeorgeTucker, malpolicyoptimizationalgorithms. arXivpreprint
and Sergey Levine. 2019. Stabilizing off-policy q- arXiv:1707.06347.
learningviabootstrappingerrorreduction. Advances
inneuralinformationprocessingsystems,32. Chenglei Si, Zhe Gan, Zhengyuan Yang, Shuohang
Wang,JianfengWang,JordanLeeBoyd-Graber,and
LijuanWang.2023. Promptinggpt-3tobereliable.
AviralKumar,AurickZhou,GeorgeTucker,andSergey
InTheEleventhInternationalConferenceonLearn-
Levine. 2020. Conservative q-learning for offline
ingRepresentations.
reinforcementlearning. AdvancesinNeuralInforma-
tionProcessingSystems,33:1179–1191.
Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael
Rafailov,JeffSchneider,TengyangXie,StefanoEr-
HarrisonLee,SamratPhatale,HassanMansoor,Kellie
mon,ChelseaFinn,andAviralKumar.2024. Prefer-
Lu, Thomas Mesnard, Colton Bishop, Victor Car-
encefine-tuningofllmsshouldleveragesuboptimal,
bune, and Abhinav Rastogi. 2023. Rlaif: Scaling
on-policydata. arXivpreprintarXiv:2404.14367.
reinforcementlearningfromhumanfeedbackwithai
feedback. arXivpreprintarXiv:2309.00267.
Yunhao Tang, Daniel Zhaohan Guo, Zeyu Zheng,
Daniele Calandriello, Yuan Cao, Eugene Tarassov,
XuechenLi,TianyiZhang,YannDubois,RohanTaori,
RémiMunos, BernardoÁvilaPires,MichalValko,
IshaanGulrajani,CarlosGuestrin,PercyLiang,and
Yong Cheng, et al. 2024a. Understanding the per-
TatsunoriB.Hashimoto.2023. Alpacaeval: Anau-
formancegapbetweenonlineandofflinealignment
tomatic evaluator of instruction-following models.
algorithms. arXivpreprintarXiv:2405.08448.
https://github.com/tatsu-lab/alpaca_eval.
Yunhao Tang, Zhaohan Daniel Guo, Zeyu Zheng,
Yu Meng, Mengzhou Xia, and Danqi Chen. Daniele Calandriello, Rémi Munos, Mark Row-
2024. Simpo: Simple preference optimization land, Pierre Harvey Richemond, Michal Valko,
with a reference-free reward. arXiv preprint BernardoÁvilaPires,andBilalPiot.2024b. General-
arXiv:2405.14734. izedpreferenceoptimization: Aunifiedapproachto
offlinealignment. arXivpreprintarXiv:2402.05749.
RémiMunos,TomStepleton,AnnaHarutyunyan,and
MarcBellemare.2016. Safeandefficientoff-policy Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
reinforcementlearning. Advancesinneuralinforma- bert, Amjad Almahairi, Yasmine Babaei, Nikolay
tionprocessingsystems,29. Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale, et al. 2023. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.
Lewis Tunstall, Edward Beeching, Nathan Lambert,
Nazneen Rajani, Kashif Rasul, Younes Belkada,
Shengyi Huang, Leandro von Werra, Clémentine
Fourrier, Nathan Habib, et al. 2023. Zephyr: Di-
rect distillation of lm alignment. arXiv preprint
arXiv:2310.16944.
CameronVoloshin,HoangMLe,NanJiang,andYisong
Yue.2019. Empiricalstudyofoff-policypolicyeval-
uation for reinforcement learning. arXiv preprint
arXiv:1911.06854.
YueWu,ZhiqingSun,HuizhuoYuan,KaixuanJi,Yim-
ingYang,andQuanquanGu.2024. Self-playpref-
erenceoptimizationforlanguagemodelalignment.
arXivpreprintarXiv:2405.00675.
MiaoXiong,ZhiyuanHu,XinyangLu,YIFEILI,Jie
Fu, Junxian He, and Bryan Hooi. 2024. Can llms
expresstheiruncertainty? anempiricalevaluationof
confidenceelicitationinllms. InTheTwelfthInter-
nationalConferenceonLearningRepresentations.
JingXu,AndrewLee,SainbayarSukhbaatar,andJason
Weston. 2023. Some things are more cringe than
others: Preference optimization with the pairwise
cringeloss. arXivpreprintarXiv:2312.16682.
ShushengXu,WeiFu,JiaxuanGao,WenjieYe,Weilin
Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and
Yi Wu.2024. Isdpo superiorto ppofor llmalign-
ment? a comprehensive study. arXiv preprint
arXiv:2404.10719.
WeizheYuan,RichardYuanzhePang,KyunghyunCho,
Sainbayar Sukhbaatar, Jing Xu, and Jason Weston.
2024. Self-rewarding language models. arXiv
preprintarXiv:2401.10020.
YaoZhao,RishabhJoshi,TianqiLiu,MishaKhalman,
MohammadSaleh,andPeterJLiu.2023. Slic-hf:Se-
quencelikelihoodcalibrationwithhumanfeedback.
arXivpreprintarXiv:2305.10425.
Chujie Zheng, Ziqi Wang, Heng Ji, Minlie Huang,
and Nanyun Peng. 2024. Weak-to-strong ex-
trapolation expedites alignment. arXiv preprint
arXiv:2404.16792.
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023.
Judging llm-as-a-judge with mt-bench and chatbot
arena. AdvancesinNeuralInformationProcessing
Systems,36.Method ARC TruthfulQA WinoGrande GSM8k HellaSwag MMLU Average
Mistral-Base(7B)
SFT 58.19 43.03 77.51 38.89 82.30 59.78 59.95
Off-policyDPO 64.42 52.44 79.48 30.17 85.36 59.78 61.94
Off-policyWPO 64.08 51.07 78.14 32.60 85.17 59.51 61.76
HybridDPO 64.76 60.46 78.22 32.15 85.30 58.75 63.27
HybridWPO 65.70 57.62 79.08 30.71 85.15 59.82 63.01
Llama-3-Instruct(8B)
SFT 61.60 51.65 76.72 75.82 78.68 65.65 68.35
Off-policyDPO 68.00 61.07 77.43 74.68 82.26 66.31 71.63
Off-policyWPO 66.98 58.91 75.45 71.95 81.87 65.97 70.19
HybridDPO 65.53 56.10 78.93 75.13 81.12 65.72 70.42
HybridWPO 65.27 55.47 79.72 66.72 81.02 65.97 69.03
Table4: ResultsontheOpenLLMleaderboard.
25
7
20
6
15
5
10
DPO Alpaca Eval 2 4
DPO MT-Bench
5
WPO Alpaca Eval 2
3
WPO MT-Bench
0
1 2 3 4 5
Epochs
Figure5: ResultsofDPOandWPOwhentrainedformoreepochs.
A AdditionalAnalysis
Resultsondownstreamtasks. WefurtherevaluatetheperformanceofSFT,DPO,andWPOmodelson
theOpenLLMleaderboard(Beechingetal.,2023)toassesstheircapabilitiesondownstreamtasks. For
thisevaluation,weusethelm-evaluation-harness5,theofficialcodebasefortheOpenLLMleaderboard.
Results are shown in Table 4. Generally, we find that preference optimization with DPO or WPO
outperformstheSFTmodel,whileLlama-3-InstructbasedmodelsoutperformMistral-base. However,
wedonotobserveacorrelationbetweenperformanceontheOpenLLMleaderboardandperformance
on instruction-following benchmarks such as Alpaca Eval 2 and MT-bench. For example, although
Llama-3-InstructwithDPOorWPOinthehybridsettingshowsthebestresultsoninstruction-following
benchmarks,itunderperformsitsoff-policycounterpartsontheOpenLLMleaderboard. Additionally,we
findthatpreferenceoptimizationmaynotimproveresultsonalldownstreamtasks. OnMMLU,theresults
aresimilartoSFT,andonGSM8K,theresultsareevenlowerthanSFTinallsettings. Ourfindingsare
consistentwiththealignmenttaxphenomenon(Askelletal.,2021),whichindicatesthatbetteralignment
maynotimproveandcansometimesevenhurtperformanceondownstreamtasks.
ComparisonbetweenDPOandWPOontrainingdynamics. Weinvestigatehowtheperformanceof
DPOandWPOchangeswithdifferentnumbersoftrainingepochs. BothDPOandWPOweretrained
usingtheSFTcheckpointofMistral-baseandtheUltrafeedbackdatasetforfiveepochs,withevaluation
results recorded at the end of each epoch, as shown in Figure 5. In this study, we use the same set of
hyperparametersasmentionedinSection4.1,withDPOandWPOusingthesamesetofhyperparameters.
WeobservedthatDPO’sperformancedeclinessharplyaftertwoepochs,suggestingstrongrewardmodel
overoptimization(Rafailovetal.,2024). Incontrast,WPOmaintainsconsistentperformanceovermore
5https://github.com/EleutherAI/lm-evaluation-harness
etaR
niW
CL
2
lavE
acaplA
erocS
gvA
hcneB-TMepochs,indicatingbettertrainingstability. Thissuggeststhatsimulatingon-policyRL,asdonebyWPO,
may mitigate issues related to reward model overoptimization and increase the stability of preference
optimization. Furthermore, a comparison of results between DPO and WPO, particularly on Alpaca
Eval 2, shows that the peak performance of DPO across various epochs still falls below that of WPO.
ThisindicatesthatWPOnotonlyprovidesmorestabletrainingdynamicsbutalsofindsadifferentand
better solution than DPO. This enhanced performance and stability highlight the advantages of WPO
ineffectivelyleveragingthepreferencedataandmaintainingstableandrobustpreferenceoptimization
throughoutthetrainingprocess.
B LinkofOpenSourcedModelsinExperiments
Thelistofopen-sourcedLLMsandtheirHuggingfaceIDsarelistedinTable5.
Model HuggingfaceID
Mistral-baseSFT HuggingFaceH4/mistral-7b-sft-beta
Mistral-baseORPO kaist-ai/mistral-orpo-beta
Mistral-baseKTO ContextualAI/zephyr_sft_kto
Mistral-baseSimPO princeton-nlp/Mistral-7B-Base-SFT-SimPO
Llama-3-instructSFT meta-llama/Meta-Llama-3-8B-Instruct
Table5: Listofopen-sourcemodelsinexperiments.
C AdditionalDetails
Scientificartifacts. Weusevariousscientificartifactsthroughoutthepaper,includingbaseLLMmodels,
preferencedatasets,andevaluationtools/benchmarks. Referencestoallusedartifactsareprovided,and
detailssuchastheirlicense,language,coverage,numberofparameters,andanysafetyissuescanbefound
byfollowingtherespectivereferences. NotethatcurrentLLMsandpreferencedatasetsmayencompassa
widerangeofdatatypesandutilizesdatafromdifferentdomainsandsources,sowedonotlistthedetails
inthispaperandencouragereaderstorefertotheoriginalsourcesformoreinformation. Inthispaper,we
primarilyusetheseartifactsfornon-distributiveandnon-commercialpurposes,whichisincompliance
withtheirlicenses.
Budget. Weconductallexperimentsusing8×H100GPUs. Theexperimentstakeapproximately1.5
hoursforMistral-baseandaround4hoursforLlama-3-Instruct.
Use of AI assistants. We used ChatGPT solely for revising the language of the paper. Note that the
revisionisexclusivelyforenhancingtheclarityandreadabilityofthetext,andnotforanyotherpurposes.