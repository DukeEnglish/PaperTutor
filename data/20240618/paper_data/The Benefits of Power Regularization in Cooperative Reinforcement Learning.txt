The Benefits of Power Regularization in Cooperative
Reinforcement Learning
MichelleLi MichaelDennis
MassachusettsInstituteofTechnology UniversityofCalifornia,Berkeley
Cambridge,MA Berkeley,CA
michelleli@alum.mit.edu michael_dennis@cs.berkeley.edu
ABSTRACT distributedratherthanconcentrated.Oneexampleofadistributed
CooperativeMulti-AgentReinforcementLearning(MARL)algo- systemofpoweristheUSgovernment:inprinciple,havingthree
rithms,trainedonlytooptimizetaskreward,canleadtoacon- brancheswithchecksandbalanceshelpspreventanysinglebranch
centrationofpowerwherethefailureoradversarialintentofa fromhavingtoomuchpower.
singleagentcoulddecimatetherewardofeveryagentinthesys- ThesamebasicideaappliesinMulti-AgentReinforcementLearn-
tem.Inthecontextofteamsofpeople,itisoftenusefultoexplicitly ing(MARL)systems:regardlessofwhetherthesettingisfullycoop-
considerhowpowerisdistributedtoensurenopersonbecomesa erative,fullycompetitive,orgeneralsum,itisoftenadvantageous
singlepointoffailure.Here,wearguethatexplicitlyregularizing foragentstolimittheamountofpowerotheragentshave.
theconcentrationofpowerincooperativeRLsystemscanresultin Powerresistsformalizationdespitebeingaprevalentandintu-
systemswhicharemorerobusttosingleagentfailure,adversarial itiveconcept.Inthispaper,wemakenonormativeclaimsabout
attacks,andincentivechangesofco-players.Tothisend,wedefine howpoweroughttobedefinedorregulated,justthatpowerisa
apracticalpairwisemeasureofpowerthatcapturestheabilityof conceptuallyusefultoolforcooperativemultiagentsystems.To
anyco-playertoinfluencetheegoagent’sreward,andthenpro- focusonempiricalprogress,welimitourattentiontopowerasin-
poseapower-regularizedobjectivewhichbalancestaskreward fluenceonreward.Butregardlessofhowitisformalized,weargue
andpowerconcentration.Giventhisnewobjective,weshowthat thatavoidingconcentrationofpowercansimultaneouslymitigate
therealwaysexistsanequilibriumwhereeveryagentisplayinga threeproblemsthataregenerallythoughtofseparately:system
power-regularizedbest-responsebalancingpowerandtaskreward. failure,adversarialattacks,andincentivechanges.Itdoessoby
Moreover,wepresenttwoalgorithmsfortrainingagentstowards mitigatingtheeffectsofoffpolicybehavior,regardlessofthecause.
thispower-regularizedobjective:SampleBasedPowerRegulariza- Consider the following toy example: a group of agents need
tion(SBPR),whichinjectsadversarialdataduringtraining;and toworkcooperativelytomaximizeproduction.Agentsproduce
PowerRegularizationviaIntrinsicMotivation(PRIM),whichadds outputbystartingfromrawmaterialandapplyingaseriesof𝑚
anintrinsicmotivationtoregulatepowertothetrainingobjective. actions.Theycaneitherworkindividuallyorformanassembly
Ourexperimentsdemonstratethatbothalgorithmssuccessfully line,whichismoreefficientbecauseofspecializationandbatch
balancetaskrewardandpower,leadingtolowerpowerbehavior productivitybutrequiresthateveryagentisasinglepointoffailure.
thanthebaselineoftask-onlyrewardandavoidcatastrophicevents Changesinanyagent’sbehaviorwouldbringthewholesystemto
incaseanagentinthesystemgoesoff-policy. ahalt.Dependingonhowmuchwecareabouttaskrewardversus
robustness,wemightpreferonebehaviorortheother.
Ourcontributionsinthisworkareasfollows:1)Weproposea
KEYWORDS
practicalmeasureofpoweramenabletooptimization–howmuch
Multi-Agent Reinforcement Learning; Cooperative Multi-Agent
anotheragentcandecreaseourreturnbychangingtheiractionfor
ReinforcementLearning;GameTheory;IntrinsicMotivation;Fault
onetimestep.2)Weproposeaframeworkforbalancingmaximizing
Tolerance;AdversarialRobustness;DistributionofPower
taskrewardandminimizingpowerbyregularizingthetaskobjec-
ACMReferenceFormat: tiveforpower,andthenshowanequilibriumalwaysexistswith
MichelleLiandMichaelDennis.2023.TheBenefitsofPowerRegularization thismodifiedobjective.3)Wepresenttwoalgorithmsforachiev-
inCooperativeReinforcementLearning.InProc.ofthe22ndInternational ingpowerregularization:one,SampleBasedPowerRegularization
ConferenceonAutonomousAgentsandMultiagentSystems(AAMAS2023), (SBPR),whichinjectsadversarialdataduringtrainingbyadversari-
London,UnitedKingdom,May29–June2,2023,IFAAMAS,9pages.
allyperturbingoneagent’sactionswithsomeprobabilityatany
timestep;andtwo,PowerRegularizationviaIntrinsicMotivation
1 INTRODUCTION
(PRIM),whichaddsanintrinsicrewardtoregularizepowerateach
Whenconsideringhowtooptimallystructureateam,institution, timestep.SBPRissimplerbutPRIMisbetterabletoachievethe
orsociety,akeyquestionishowresponsibility,power,andblame rightreward-powertradeoffsforverysmallvaluesof𝜆.Ourexper-
oughttobedistributed,andassuchitisabroadlystudiedconcept imentsinanOvercooked-inspiredenvironment[3]demonstrate
inthesocialsciences[21,22].Weoftenwanttoavoidtoomuch thatbothalgorithmscanachievevariouspower-rewardtradeoffs
powerlyinginthehandsofafewactors,preferringpowertobe andcanreducepowercomparedtothetaskreward-onlybaseline.
Proc.ofthe22ndInternationalConferenceonAutonomousAgentsandMultiagentSys-
tems(AAMAS2023),A.Ricci,W.Yeoh,N.Agmon,B.An(eds.),May29–June2,2023,
London,UnitedKingdom.©2023InternationalFoundationforAutonomousAgents
andMultiagentSystems(www.ifaamas.org).Allrightsreserved.
4202
nuJ
71
]GL.sc[
1v04211.6042:viXra𝑁𝑎𝑡𝑢𝑟𝑒
𝐸𝑔𝑜=𝑃1 𝐸𝑔𝑜=𝑃0
𝑁𝑎𝑡𝑢𝑟𝑒
...
𝑂𝑛−𝑝𝑜𝑙𝑖𝑐𝑦 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑖𝑎𝑙
P0 𝑃0 𝐴𝑑𝑣𝑒𝑟𝑠𝑎𝑟𝑖𝑎𝑙𝑃0
𝑈 𝐷 𝑈 𝐷 𝑈 𝐷
𝑃1 𝑃1
𝑈 𝐷 𝑈 𝐷 𝑈 𝐷 𝑈 𝐷 𝑈 𝐷 𝑈 𝐷
(𝑎,𝑏) (𝑐,𝑑) (𝑒,𝑓) (𝑔,ℎ) (𝑎,𝑏) (𝑐,𝑑) (𝑒,𝑓) (𝑔,ℎ) (𝑎,𝑏) (𝑐,𝑑) (𝑒,𝑓) (𝑔,ℎ)
(a)Simplegeneral-sumgame (b)p-AdversarialGameof(a)
Figure1:ExtensiveFormdiagramsofasimple1timestepgameanditscorrespondingp-AdversarialgameatswhereNature
choosesanegoagentandwhethertousetheon-policyoradversarialco-playertowardtheegoagent.Thedashedlinesencircle
nodesinthesameinformationsetforPlayer1becauseagentsactsimultaneously.WeomitafinallayerofNaturenodesthat
modeltheprobabilistictransitionfunction.
2 RELATEDWORK andthejointaction𝑎 = (𝑎1,𝑎2,...,𝑎 𝑛) ∈ 𝐴isusedtotransition
Powerhasbeenbroadlystudiedinthesocialsciences[21,22]and
theenvironmentaccordingto𝑇(𝑠′|𝑎,𝑠).Eachagent𝑖receivestheir
formulti-agentsystems,buthasnotbeenstudiedinthecontextof ownreward𝑟 𝑖 = 𝑅 𝑖(𝑠,𝑎).Whileourtheoreticalresultsholdfor
deepmulti-agentRLtothebestofourknowledge.Instead,prior general-sumMarkovgames,weonlyempiricallyevaluateonfully
work has focused on graph-theoretic analysis [10] or symbolic cooperativeenvironments,thusassumingthatallagentshavethe
formulations[5].Therehavealsobeenproductiveformulations samereward:𝑅 𝑖 =𝑅 𝑗∀𝑖,𝑗.Weoperateinthefully-observedsetting:
oftherelatedconceptsofresponsibilityandblame[1,4,7,8,11], eachagentcandirectlyobservethetruestate𝑠.Eachagent𝑖aims
whichhavestrongconnectionstopower. toindependentlymaximizetheirtime-discounted,expectedreward
InAI,powerhasbeenformalizedinasingleagentcontext,with 𝑈 𝑖 = E[(cid:205)𝑇 𝑡=0𝛾𝑡𝑟 𝑡] where𝑟 𝑡 istherewardattime𝑡.Throughout,
recentworktowardsdefiningpowerandregularizinganagent’s wewillassumefinite,discreteactionsandfinitetime.
ownbehaviorwithrespecttopower[27–29].Whileitisapromising Inanygame,itisusefulforaplayertoconsidertheirbestre-
directiontoextendtheseformalmeasurestoMARL,wefocuson sponses:optimalpoliciesgivenfixedpoliciesfortheco-players:
makingempiricalprogressonregularizingpowerinthiswork.
ThoughtheliteratureonpowerindeepMARLissparse,the 𝜋
𝑖
∈argmax{𝑈 𝑖(𝜋 𝑖′;𝜋 −𝑖)}=𝐵𝑅(𝜋 −𝑖).
literatureontheproblemsthatpowerregulationcanhelpmitigate 𝜋 𝑖′∈Π𝑖
ismorerobust.Forinstance,thereisalargebodyofworkonde-
signingMARLsystemsthatcooperaterobustlyinsequentialsocial where𝜋 referstothepoliciesforallplayersotherthan𝑖.Further-
−𝑖
dilemmas[6,14,15,17],inwhichbalancingpoweramongstthe more,wesaythatapolicy𝜋
𝑖
isalocalbestresponseatstate𝑠ifit
agentsiscritical.Thereisalsoasignificantbodyofworkshow- choosesanoptimaldistributionofactionsat𝑠giventherestofits
ingtheexistenceofadversarialattacksforneuralnetworks[25] policy𝜋 andco-playerpolicies𝜋 .Thatis,
𝑖 −𝑖
andsingle-andmulti-agentRL[9,13,16,18].Insuchcases,power
reg Fu il na ar li lz ya ,t ti ho en ac la gn orh ite hlp mb su wil ed pfa reu sl et nto tl fe or ra pn oc we.
erregularizationmay
𝜋 𝑖 ∈ argmax {𝑈 𝑖(𝜋 𝑖′;𝜋 −𝑖)}=𝐵𝑅𝑙𝑜𝑐𝑎𝑙 (𝜋 −𝑖).
bewellsuitedforsituationswhereonedesiresagentsthatwork
𝜋 𝑖′∈Π𝑖(𝜋𝑖;𝑠)
wellwithunknownteammates,ifcombinedwithapproachesfrom
ad-hocteamwork[2,24]andzero-shotcoordination[12,26].This
whereΠ 𝑖(𝜋 𝑖;𝑠)={𝜋 𝑖′ ∈Π 𝑖|∀𝑠′ ≠𝑠,𝜋 𝑖′(𝑠′)=𝜋(𝑠′)}
Asetofpolicies𝜋 foreachplayerwhereeachpolicy𝜋 isabest
isespeciallytrueinsituationswhereagentsmustinferwhototrust 𝑖
responsetotheotherstrategiesiscalledaNashequilibriumthat
(i.e.whotoentrustpowerto)[20,23].
is,∀𝑖,𝜋
𝑖
∈𝐵𝑅(𝜋 −𝑖)).Thisisa"stablepoint"wherenoagenthasan
incentivetounilaterallychangetheirpolicy.Furthermore,wesay
3 BACKGROUND
thatasetofpolicies𝜋 formalocalNashequilibriumatstate𝑠ifall
WemodeloursettingasaMarkovgame[19]definedbythetuple
policiesarealocalbest-responsetotheotherpoliciesat𝑠.
(𝑁,𝑆,𝐴,𝑇,𝑅,𝛾)where𝑁 isthenumberofplayers,𝑆 isthesetof
environmentstates,𝐴 = ×𝑖∈{0,...,𝑁}𝐴 𝑖 isthejointactionspace,
𝑇 : 𝑆 ×𝐴 → 𝑆 is the transition function,𝑅 = ×𝑖∈{0,...,𝑁}𝑅 𝑖 is 4 FORMALISM
therewardfunctionforeachplayer,and𝛾 ∈ (0,1]isthediscount Todesignsystemsthatregularizeforpower,itisimportanttobe
factor.Ateverytimestep,eachagent𝑖choosesanaction𝑎
𝑖
∈𝐴 𝑖, clearaboutourobjectiveandhowwedefinepower.Table1:Anexamplegamewhereplayer1hasnopowerover ofall 𝑗 on𝑖.Inourexperimentswithmorethan2agentswelet
player2becauseallof1’sactionsareequallybadfor2. 𝑅 𝑖𝑝𝑜𝑤𝑒𝑟 (𝑠,𝜋) =− 𝑁1 −1(cid:205) 𝑗power(𝑖,𝑗|𝑠,𝜋)(themeanfunction).We
leavetheproblemofdeterminingthemostappropriatechoiceof
𝑋 𝑌 𝑍 aggregationfunctiontofuturework.
𝑋 (3,−10) (3,−9) (3,−8) Considerthe2-playergeneral-summatrixgamedefinedinTa-
𝑌 (2,−10) (2,−9) (2,−8) ble 2 which we call the Attack-Defense game. This is a single
𝑍 (1,−10) (1,−9) (1,−8) timestepgameso𝑈power (𝜋|𝑠)=−power(𝑖,𝑗|𝑠,𝜋).Ifeitherplayer
𝑖
plays𝑋,theotherplayercouldplay𝑍 reducingtheutility.playing
𝑌 toguarantees2utility,payingasmallpricetoreducepower.
4.1 MeasuringPower
If optimizing purely for task reward, both agents play 𝑋 to
Ourmaingoalistomakeempiricalprogressonbuildingpower-
achievetheutility-maximizingNashequilibrium.However,playing
regularizingMARLsystems,sowewillnotaimtofindthemost
𝑋 incurs3powerwhileplaying𝑌 incurs0power.Thus,byEq1we
p for rop pe or wo er rm oo fs ct og -pen lae yr ea rld 𝑗e ofi vn ei rtio egn oof agp eo nw te 𝑖r ,. wW he icd hefi wn ee ca am lle 1a -s su ter pe have𝑈 𝑃𝑅(𝑋) =3−3𝜆and𝑈 𝑃𝑅(𝑌) =2,sofor𝜆 > 1
3
weprefer𝑌.
SeeTable3foralargervariantofthegamewithanontrivialPareto
adversarialpower,asthedifference𝑗 couldmakeon𝑖’srewardif𝑗
frontier.Figure2ashowsthevalueofeachactionasafunctionof𝜆
hadinsteadactedadversariallytoward𝑖foronetimestep.
assumingthecoplayerison-policy(i.e.doesn’tplayF).
Definition4.1(1-stepadversarialpower). Let𝑈 𝑖taskdenoteplayer Itisimportantfortheregularizedobjectivetoapplyatevery
𝑖’staskutility.Givenpolicies𝜋,the1-stepadversarialpoweragent state,eventhoseunreachableonpolicy.Anaiveapproachtopenal-
𝑗 hasonagent𝑖whenstartingfromstate𝑠is: izingpowerwouldbetoonlypenalizethe1-stepadversarialpower
overtheagentintheinitialstate,thatis,onlyaimingtomaximize
power(𝑖,𝑗|𝑠,𝜋)=𝑟+E[𝑈 𝑖task (𝑠′,𝜋)]− 𝑎m 𝑗∈i 𝐴n 𝑗(𝑟 𝑎𝑗+E[𝑈 𝑖task (𝑠 𝑎′ 𝑗,𝜋)]) 𝑈 𝑖(𝜋|𝑠0)where𝑠0istheinitialstate.However,suchameasurehas
afundamentalflaw,inthatonceanagentdeviatesfromtheusual
where𝑠′ =𝑇(𝑠,𝜋),𝑠 𝑎′
𝑗
=𝑇(𝑠,𝑎 𝑗;𝜋 −𝑗),and𝑟and𝑟
𝑎𝑗
are𝑖’srewards
strategy,thereisnolongeranyincentivetoregulatepowerand
obtainedon-policyandwith𝑗’sdeviationto𝑎 ,respectively.
𝑗 thusouragentwouldgaintrustinpotentiallyadversarialcoplayers.
Themin istakenoverthesetofdeterministicactions.Note Forinstance,supposetheoptimalpowerregularizedpolicywere
thatitisno𝑎 t𝑗 n∈𝐴 ec𝑗 essarytoconsiderstochasticpoliciesasthemost toworkindependentlyinsteadofforminganassemblyline.Oncean
powerfulstochastic𝜋 couldsimplyplaceprobability1onanyof agentdeviates,thesystemcouldbeinsomestate𝑠notreachableon-
𝑗
thedeterministicactionsthatachievethelowestutilityfor𝑖. policy,onlyreachableviaanadversarialdeviation𝑎.Theonlyway
Counterintuitively,itispossiblethatallof𝑗’simmediateactions behaviorinstate𝑠influencestheutilityattheinitialstate𝑈 𝑖(𝜋|𝑠0)
exertsomecausaleffecton𝑖’sutilitywithout𝑗 havinganypower isthroughtheadversarialactiontermofthepowerregularization
power
over𝑖.Thiscanhappenifallof𝑗’sactionsreduce𝑖’srewardby10, 𝑈 𝑖 (𝜋|𝑠0).Sincethistermincreaseswhentaskrewardincreases,
forexample.Sincewearedefiningpowerinrelativeterms,ifall afteranydeviationthepolicywillnolongerregulatepower.Thus
actionshavethesameeffecton𝑖,wesay𝑗 hasnopowerover𝑖as once one agent fails, all agents would revert to forming brittle
anycausaleffectsof𝑗’sactionson𝑖areinevitable.SeeTable1for andpower-concentratingassemblylines.Thisistheoppositeof
anexampleworkedoutexplicitly.Suchnuanceisreminiscentof thedesiredbehavior:wewouldtakeafailureofanagentasan
thedifficultiesindefiningblame[4].Exploringsuchconnections indicationthattheyshouldbeentrustedwithmorepowerbecause
in-depth could be a path towards better metrics for measuring ourmodeldoesnotallowthemtodeviateagain.
power. Luckily,thestateconditionedregularizationweproposeisa
simplefix.Ratherthanregularizingforpowerjustatthefirststate,
4.2 RegularizingforPower weregularizepowerviaoptimizingEq1atall𝑠.Thusevenafteran
agentfailsotherswillstillcontinuetoregularizeforpower.
Traditionally,cooperativeMARLalgorithmsaimtooptimizethe
discountedsumoftaskrewards,whichwecalltaskutility,without
5 EXISTENCEOFEQUILIBRIA
explicitconsiderationfortheamountofpowerheldbyotheragents.
Wearguethatwecanmakesystemsmorerobustbyoptimizingan InthestandardformulationofMarkovGames,theexistenceof
explicittrade-offbetweenmaximizingtaskrewardandminimizing anequilibriumsolutionisguaranteedbyNash’sTheorem,which
power.Thisframeworkhastheadvantageofaddressingsystem showsthateveryfinitegamehasamixedNashequilibrium.How-
failure,adversarialattacks,andincentivechangesallatonceby ever,onceweregularizeforpower,Nash’stheoremnolongerap-
mitigatingthenegativeeffectsofoff-policybehavior. pliesbecausethepayoffbecomesafunctionofthestrategy.
Wefocusonlineartrade-offs,thatisobjectivesoftheform Givenourpower-regularizedobjective,wecandefinenotionsof
bestresponseandequilibriumsimilartothestandardformulations.
𝑈 𝑖(𝜋|𝑠)=𝑈 𝑖task (𝜋|𝑠)+𝜆𝑈 𝑖power (𝜋|𝑠) (1)
𝑠
aw ndhe 𝑈r 𝑖e po𝑈 w𝑖t ea rs (k 𝜋(𝜋 |𝑠)|𝑠) =is (cid:205)th
𝑇
𝑡=e 0t 𝑅as 𝑖𝑝k 𝑜𝑤u 𝑒ti 𝑟li (t 𝑠y 𝑡,f 𝜋o )rp isla ty he er s𝑖 us mtar ot fin pg oi wn es rta rt ee
-
b ite
asD
ct
he rfi
ie
en
s
vpi et oi so nn
ts
he5 e. t1 oo. ptW
th
imee
p
as la oy
l ti
rct aih dea est
𝜋
oa ff−p
𝑖
bo
,
el ni tc
o
wy
ta
e𝜋
t
ee𝑖 ndis
ta
aa
s
sk𝜆 𝜋-
𝑖
rp e∈o ww
𝑃
ae
𝑅
rr d𝐵r
𝑅
ae n𝜆g d(u 𝜋l pa
−
or 𝑖i w)z ,e eid
rf
𝑝𝑜𝑤𝑒𝑟
wards 𝑅 at states starting from 𝑠 reached by unrolling 𝜋. minimizationineverystate.Thatis,forall𝑠wehave:
𝑖
𝑝𝑜𝑤𝑒𝑟
Inthe2-agentsetting,𝑅 𝑖 (𝑠,𝜋) = −power(𝑖,𝑗|𝑠,𝜋),butwith 𝜋 𝑖 ∈argmax{𝑈 𝑖(𝜋 𝑖′;𝜋 −𝑖|𝑠)}=𝑃𝑅𝐵𝑅 𝜆(𝜋 −𝑖|𝑠).
moreagents,𝑅 𝑝𝑜𝑤𝑒𝑟 mustaggregateinformationaboutthepowers 𝜋 𝑖′∈Π𝑖Table3:TheLargerAttack-DefenseGame.
Table2:TheAttack-DefenseGame:anopponentcantake
awayyourutilityifyouplay𝑋,butyoucanpayasmall
𝐴 𝐵 𝐶 𝐷 𝐸 𝐹
costtodefendagainstthatbyplaying𝑌.
𝐴 (3,3) (3,2.5) (3,2) (3,1.5) (3,1) (−2,0)
𝐵 (2.5,3) (2.5,2.5) (2.5,2) (2.5,1.5) (2.5,1) (0,0)
𝑋 𝑌 𝑍
𝐶 (2,3) (2,2.5) (2,2) (2,1.5) (2,1) (0.75,0)
𝑋 (3,3) (3,2) (0,0)
𝐷 (1.5,3) (1.5,2.5) (1.5,2) (1.5,1.5) (1.5,1) (1,0)
𝑌 (2,3) (2,2) (2,0)
𝐸 (1,3) (1,2.5) (1,2) (1,1.5) (1,1) (1,0)
𝑍 (0,0) (0,2) (0,0)
𝐹 (0,−2) (0,0) (0,0.75) (0,1) (0,1) (0,0)
Next,wedefinePowerRegularizingEquilibrium(PRE)tobea Proof. BaseCase.Withoutlossofgenerality,assumethatall
fixedpointofthepowerregularizedbestresponsefunctionand trajectories end in a single state where agents’ decisions affect
thenprovetheyareguaranteedtoexistinanygame. nothing.Suchastatecanbeaddedwithoutchangingthepoweror
utilityofanytrajectory.Atthisstateallpoliciesareequallyvalued,
Definition5.2(𝜆-PowerRegularizingEquilibrium). A𝜆-Power
sothebasecaseholdstrivially.
RegularizingEquilibrium(PRE)isapolicytuple𝜋 suchthatall
policiesarepowerregularizedbestresponsestotheothers.Thatis,
InductiveStep.Assumethat,atanystate𝑠′reachableattime
ineverystate𝑠,forall𝑖,wehave𝜋
𝑖
∈𝑃𝑅𝐵𝑅 𝜆(𝜋 −𝑖|𝑠). 𝑡−1fromtheend,𝑈 𝑖p-Adv (𝜋 𝑖;𝜋 −𝑖|𝑠′)=𝑈 𝑖(𝜋 𝑖;𝜋 −𝑖|𝑠′).Ourgoalis
toshowthisequivalencealsoholdsforstatesreachableattime𝑡.
Theorem5.3. Let𝐺 beafinite,discreteMarkovGame,thena Expandingoutthedefinitionofthep-Adversarialgame,wehave:
𝜆-powerregularizingequilibriumexistsforany𝜆.
Intuitively,weprovethisbyconstructinganothergame,which
𝑈 𝑖p-Adv (𝜋 𝑖′ ;𝜋 −𝑖|𝑠)
w cae nc ba ell at ph pe li𝑝 e- da ,d av ne drs sa hri oa wl g ta hm ate No af s𝐺 h, et qo uw ilih bi rc iah iN na ts hh i’ ss mth oe do ir fie em
d
=𝑅 𝑖task (𝑠,𝜋 −𝑖)+(1− 𝑇𝜆 )E[𝑈 𝑖p-Adv (𝜋 𝑖′;𝜋 −𝑖|𝑠′)]
gamecorrespondtopowerregularizingequilibriaintheoriginal 𝜆
game.Thebasicideaofthisgameistoaddadversarialplayersthat
+
𝑇
E[𝑈 𝑖task (𝜋 𝑖′;𝜋 −𝑖|𝑠 𝐴′ 𝑑𝑣)]
perturbco-players’actionsadversariallytowardtheegoagentwith 𝜆
probability𝑝.Wedefinethe𝑝-adversarialgameformallybelowand
=𝑅 𝑖task (𝑠) +(1− 𝑇)E[𝑈 𝑖(𝜋 𝑖′;𝜋 −𝑖|𝑠′)]
depicttheextensiveformofa1-timestepgameinFigure1b. 𝜆
Definition5.4(p-AdversarialGameof𝐺 at𝑠). Thep-adversarial
+
𝑇
E[𝑈 𝑖task (𝜋 𝑖′;𝜋 −𝑖|𝑠 𝐴′ 𝑑𝑣)]
gameof𝐺 atstate𝑠addsadversarialagents𝜋𝐴∗𝑖 foreachplayer
𝑗
𝑖 coa -n pd lac yo e- rp sla ty oe mr 𝑗 in.T imh ie zs ee 𝑖a ’g se rn et ts ura nre
.
Tra hn edo gm amly eg si tv ae rn tsc ao tn 𝑠tr .o Nl ao tf u𝑖 r’ es where𝑠′ ∼𝑇(𝜋 𝑖′;𝜋 −𝑖),𝑠 𝐴′
𝑑𝑣
∼𝑇(𝜋 𝑖′;𝜋𝐴 𝑗∗𝑖;𝜋 −{𝑖,𝑗}),and𝑈 𝑖p-Adv (𝜋|𝑠)
istheutilityofagent𝑖givenpolicies𝜋 startingatstate𝑠ofthe𝑝
randomlydecideswithprobability𝑝tolettheadversarywilltake
Adversarialgamebeforetheadversaryhastakencontrol.Thefirst
controlinthisepisode.Natureuniformlyrandomlyselectsanego-
lineabovefollowsfromthedefinitionofthe𝑝 adversarialgame
agent𝑖whichwillbetheonlyagenttoberewardedinthegame,
andthesecondlinefollowsfromtheinductivehypothesis.Wecan
anduniformlyrandomlyselectsatimesteponwhichtheadversary
continuebyexpandingoutthedefinitionsandrearrangingterms:
willtakecontrol,iftheadversarygetscontrolthisepisode.Atthat
ft oim re onst ee sp teN pa .t Ru er we aw ri dll s, fc oh ro 𝑖o as re ea cc ao lc- up ll aa ty ee dr a𝑗 st no ob re mr ae lp .lacedby𝜋𝐴 𝑗∗𝑖 𝑅 𝑖task (𝑠) +(1− 𝑇𝜆 )E[𝑈 𝑖𝑡𝑎𝑠𝑘 (𝜋 𝑖′;𝜋 −𝑖|𝑠′)]
Thefollowingtheoremestablishesacorrespondencebetween +𝜆(1− 𝑇𝜆 )E[𝑈 𝑖power (𝜋 𝑖′;𝜋 −𝑖|𝑠′)]+ 𝑇𝜆 E[𝑈 𝑖task (𝜋 𝑖′;𝜋 −𝑖|𝑠 𝐴′ 𝑑𝑣)]
thebestresponseinthep-adversarialgameof𝐺 andthepower
regularizedbestresponseintheoriginalgame.Wewillusethis
=𝑅 𝑖task (𝑠) +E[𝑈 𝑖task (𝜋 𝑖′;𝜋 −𝑖|𝑠′)]
correspondencetoproveTheorem5.3. +𝜆(1− 𝑇𝜆 )E[𝑈 𝑖power (𝜋 𝑖′;𝜋 −𝑖|𝑠′)]
Theorem5.5. Consideranagent𝑖inaMarkovgame𝐺withtime
horizon𝑇,andanarbitrarystates.Theutilityofpoliciesinthep- − 𝑇𝜆(cid:16) E[𝑈 𝑖task (𝜋 𝑖′;𝜋 −𝑖|𝑠′)] −E[𝑈 𝑖(task𝜋 𝑖′;𝜋 −𝑖|𝑠 𝐴′ 𝑑𝑣)](cid:17)
adversarialgameatstates,for𝑝 =𝜆isequivalenttothecorresponding
policiesintheoriginalgame.Thatis,forall𝜋 intheoriginalMarkov =𝑈 𝑖task (𝜋|𝑠) +𝜆𝑈 𝑖power (𝜋|𝑠)
game,wehave:𝑈 𝑖p-Adv(𝜋 𝑖;𝜋 −𝑖|𝑠)=𝑈 𝑖(𝜋 𝑖;𝜋 −𝑖|𝑠).
Theproofideaisto,forap-adversarialgameonafixedstates, wherethefinallinefollowsfromthedefinitionoftaskandpower
inductivelyarguefortheequivalencestartingfromthelaststepof utility.Thus,thevalueofpoliciesinthep-Adversarialgameatstate
thegame.Ateachstep,thesetof𝜋𝐴∗𝑖forallco-players𝑗canbeseen 𝑠timestep𝑡 isequivalenttothepower-regularizedvalue..
𝑗
asawaytocompute𝑖’spowerregularizationterm.Throughoutthe Byinduction,theequivalenceholdsforstatesreachableatany
p-Adv
proofweusestate-basedrewardstosimplifythenotation,which timestep.Thus,wehavethedesiredequivalence𝑈
𝑖
(𝜋 𝑖;𝜋 −𝑖|𝑠)=
otherwisedoesnoteffectthemainstructureoftheproof. 𝑈 𝑖(𝜋 𝑖;𝜋 −𝑖|𝑠). □Giventheequivalencebetweenthestandardbestresponsein Algorithm1SampleBasedPowerRegularization(SBPR)
thep-adversarialgameandthepower-regularizedbestresponsein 1: proceduretrainAgent(𝜋 𝑖,𝜋 𝑗,𝜋ˆ 𝑗,𝑉 𝑖,𝑝)
theoriginalgame,wecanreturntoourtaskofprovingTheorem 2: Collecttrajectoriesforagent𝑖:ateachtimestep,alwaysuse
5.3,toshowthatpowerregularizingequilibriaof𝐺 alwaysexist. 𝜋 ,andwithprobability𝑝use𝜋ˆ ,otherwiseuse𝜋 .
𝑖 𝑗 𝑗
3: UsePPOorotherRLalgorithmtoupdate𝜋 𝑖 and𝑉 𝑖.
Proof. Notethatwecanconstructatupleofpolicies𝜋 which 4: proceduretrainAdversarialAgent(𝜋 𝑖,𝜋ˆ 𝑗,𝑉 𝑖)
arealocalNashequilibriumat𝑠inthe𝑝-adversarialgameat𝑠by 5: Collecttrajectoriesforadversarialagent𝑗:𝜋ˆ 𝑗 hasonestep
standardbackwardsinductionargument–notingfirstthatthiscan toact,and𝑟 𝑖 =𝑉 𝑖(𝑠)−𝑟(𝑠,𝜋 𝑖(𝑠),𝜋ˆ 𝑗(𝑠))−𝛾𝑉 𝑖(𝑠′).
bedoneintheterminalstates,andthennotingthatitcanthenbe 6: UsePPOorotherRLalgorithmtoupdate𝜋ˆ 𝑗.
doneateachofthepriorstatesbackwardsbyinduction. 7: procedureSBPR(𝑝)
Considerthepolicy𝜋 𝑖 ofanarbitraryplayer𝑖,byTheorem5.5, 8: Initialize𝜋 𝑖,𝜋 𝑗,𝜋ˆ 𝑖,𝜋ˆ 𝑗,𝑉 𝑖,𝑉 𝑗 arbitrarily.
𝜋 mustbeapowerregularizedbestresponsein𝐺 atstate𝑠.Since
𝑖 9: loop
weassumed𝑖and𝑠tobearbitrary,thisappliestoall𝑖and𝑠.Thus 10: trainAgent(𝜋 𝑖,𝜋 𝑗,𝜋ˆ 𝑗,𝑝)
thetuple𝜋 𝑖 representsapowerregularizingequilibriumof𝐺 atall 11: trainAgent(𝜋 𝑗,𝜋 𝑖,𝜋ˆ 𝑖,𝑝)
states𝑠bydefinition. □ 12: trainAdversarialAgent(𝜋 𝑗,𝜋ˆ 𝑖,𝑉 𝑗)
13: trainAdversarialAgent(𝜋 𝑖,𝜋ˆ 𝑗,𝑉 𝑖)
Wehaveshownthatthepowerregularizingequilibriaweseek
do,infact,exist,butmoreover,Theorem5.5givessomeideaabout
6.2 PowerRegularizationviaIntrinsic
amethodtoactuallyobtainthem.Wecanfindpoliciesinpower-
Motivation(PRIM)
regularizingequilibriumbyfindingpoliciesinNashequilibriumin
thep-adversarialgameof𝐺.Thisintuitionisthemotivationbehind PRIMaddsaper-stepintrinsicmotivationrewardtermthatpenal-
oneofourmethods,SampleBasedPowerRegularization,which izespowerontheagent.Eachagent’srewardfunctionis
weintroduceinSection6.1andempiricallyevaluateinSection7. 𝑅 𝑖𝑃𝑅𝐼𝑀 (𝑠,𝑎,𝜋)=𝑅 𝑖𝑡𝑎𝑠𝑘 (𝑠,𝑎)+𝜆𝑅 𝑖𝑝𝑜𝑤𝑒𝑟 (𝑠,𝜋)
whichispreciselythepowerregularizationobjectiveofEq1.
6 METHODS Ratherthanprobabilisticallyconsideringtheeffectofanadversary
Weintroducetwomethodsforpowerregularization,SampleBased likeSBPR,PRIMconsidersitateverystepbutdownweightsthe
PowerRegularization(SBPR)andPowerRegularizationviaIntrin- effectaccordingto𝜆,whichreducesvariance.
sicMotivation(PRIM).SBPRisinspiredbythep-AdversarialGame Crucially,finding𝑗’sadversarialactionfor𝑖tocomputeadver-
formulationintroducedinDefinition5.4;itinjectsadversarialdata sarialpowerrequirescounterfactualsfromaresettablesimulator.
duringtrainingbyperturbingactionsateachstepwithprobabil- Inthefutureonecouldtrytolearnthesimulatorinstead.
ity𝑝 = 𝜆.PRIMtrainsagentsdirectlyonthepowerregularized
objective,interpretingthepowerpenaltyasintrinsicmotivation. Algorithm2PowerRegularizationviaIntrinsicMotivation(PRIM)
Agents do not share weights, but we train them together of-
1: procedureComputePower(𝜋 𝑖,𝜋 𝑗,𝜋ˆ 𝑗,𝑉 𝑖)
fline.Ourtheoryandmethodsareamenabletobecombinedwith 2: 𝑠′ ∼𝑇(·|𝑠,𝜋 𝑖(𝑠),𝜋 𝑗(𝑠))
approachesfromad-hocteamplay[2,24]andzero-shotcoordi- 3: 𝑟 ←𝑟(𝑠,𝜋 𝑖(𝑠),𝜋 𝑗(𝑠))+𝛾𝑉 𝑖(𝑠′)
n cha ati lo len ng[1 e2 s,, s2 o6] w, eth lo eau vg eh itth toes fe utd uo rm ewai on rs kb tr oin gg enw erit ah lizt ehe thm ist ah pe pir roo aw chn
.
4: 𝑠 𝑎′
𝑑𝑣
∼𝑇(·|𝑠,𝜋 𝑖(𝑠),𝜋ˆ 𝑗(𝑠))
WetraineachagentusingProximalPolicyOptimization(PPO).
5: 𝑟 𝑎𝑑𝑣 ←𝑟(𝑠,𝜋 𝑖(𝑠),𝜋ˆ 𝑗(𝑠))+𝛾𝑉 𝑖(𝑠 𝑎′ 𝑑𝑣)
6: 𝑝𝑜𝑤𝑒𝑟 ←𝑟−𝑟 𝑎𝑑𝑣
Theneuralnetworksparameterizingthepolicyconsistofseveral
7: return𝑝𝑜𝑤𝑒𝑟
convolutionallayers,fullyconnectedlayers,acategoricaloutput
headfortheactor,andalinearlayervaluefunctionoutputhead. 8: proceduretrainAgent(𝜋 𝑖,𝜋 𝑗,𝜋ˆ 𝑗,𝑉 𝑖,𝜆)
9: Collecttrajectoriesforagent𝑖withreward𝑟 𝑡𝑎𝑠𝑘+𝜆𝑟 𝑝𝑜𝑤𝑒𝑟.
10: UsePPOorotherRLalgorithmtoupdate𝜋 𝑖 and𝑉 𝑖.
6.1 Sample-BasedPowerRegularization(SBPR)
11: proceduretrainAdversarialAgent(𝜋 𝑖,𝜋ˆ 𝑗,𝑉 𝑖)
SBPRdirectlyinjectsadversarialrolloutsintothetrainingdata, 12: Collecttrajectoriesforadversarialagent𝑗:𝜋ˆ 𝑗 hasonestep
playingthep-AdversarialGameintroducedinDefinition5.4.Atthe toact,and𝑟
𝑖
=𝑉 𝑖(𝑠)−𝑟(𝑠,𝜋 𝑖(𝑠),𝜋ˆ 𝑗(𝑠))−𝛾𝑉 𝑖(𝑠′).
beginningofeveryrollout,wepickanagent𝑖tobetheegoagent 13: UsePPOorotherRLalgorithmtoupdate𝜋ˆ 𝑗.
andanotheragent 𝑗 tobetheadversary.Ateverytimestep,with
14: procedurePRIM(𝜆)
probability𝑝(independentofprevioustimesteps)weperturbagent
𝑗’sactionadversariallyto𝑖.Onlyagent𝑖receivestherollouttotrain 15: Initialize𝜋 𝑖,𝜋 𝑗,𝜋ˆ 𝑖,𝜋ˆ 𝑗,𝑉 𝑖,𝑉 𝑗 arbitrarily.
16: loop
on.Intuitively,thiscanbeseenastrainingonthe𝑝-Adversarial
Gameatrandomstates𝑠. 17: 𝜋 𝑖,𝑉 𝑖 ←trainAgent(𝜋 𝑖,𝜋 𝑗,𝜋ˆ 𝑗,𝑉 𝑖,𝜆)
SBPRhastheadvantageofsimplicitybutmaynotregularizefor 18: 𝜋 𝑗,𝑉 𝑗 ←trainAgent(𝜋 𝑗,𝜋 𝑖,𝜋ˆ 𝑖,𝑉 𝑗,𝜆)
powersuccessfullyifwewant 𝜆tobeverysmall.Thisisbecause 19: 𝜋ˆ 𝑖 ←trainAdversarialAgent(𝜋 𝑗,𝜋ˆ 𝑖,𝑉 𝑗)
wewon’tseeenoughdeviationexamplesperbatch,sothegradient 20: 𝜋ˆ 𝑗 ←trainAdversarialAgent(𝜋 𝑖,𝜋ˆ 𝑗,𝑉 𝑖)
signalisveryhighvariance,whichisreflectedinourexperiments.(a)Attack-DefenseGameActionPayoffs (b)CoinDivisionGameActionPayoffs (c)CoinDivisionGameParetoFrontier
Figure2:Power-regularizedobjectivevaluesachievedbydifferentactionsinsmallenvironments.
6.3 Optimizations
Ontheirown,bothPRIMandSBPRcanbesloworunstable,sowe
introduceanumberofoptimizations.
MonteCarloEstimatesofAdversarialPower.Whencom-
putingadversarialpowerforPRIM,weusetheinstantiatedactions
ineachrolloutratherthanthefullpolicies𝜋,effectivelyaMonte
Carlo estimate. This is because PPO looks at the advantages of
actionsacrossthebatch,soweneedtobeabletotellwhichactions
incurmoreorlesspower.Policiesdon’tgiveusthisinformation (a)StartingState (b)HighPowerTimestep
becausetheyareconstantthroughoutabatch;wegetnodifferential
information.Asabonus,usingactionsgivesaspeedupbyafactor Figure3:OvercookedClose-Pot-Far-Pot.Agentscanusethe
ofthejointco-playeractionspace. sharedmiddlepotortheirprivatepots.Usingthemiddlepot
LearningtheAdversary.BothPRIMandSBPRrequirefind- isfasterbutincurshighpower(see(b))whereoneagentcan
ingtheco-player 𝑗’sactionthatminimizestheegoagent𝑖’sre- messuptheother’sworkbyputtinginawrongingredient.
ward.Forenvironmentswithlargeorcontinuousactionspaces,
conductinganexhaustivesearchmaybeintractable,sowelearn bysubtractingthevalueestimateofthestartingstate:
thereward-minimizingaction:theadversarialco-player𝑗 forego
agent𝑖istrainedtominimize𝑖’sreturn:𝑈 𝑗𝑖 ,𝑎𝑑𝑣(𝑠,𝑎 𝑗)=−𝑅 𝑖(𝑠,𝑎)−
𝑈 𝑗𝑖 ,𝑎𝑑𝑣(𝑠,𝑎 𝑗)=𝑈 𝑖(𝑠)−𝑅 𝑖(𝑠,𝑎)−E[𝑈 𝑖(𝑇(𝑠,𝑎))]
E[𝑈 𝑖(𝑇(𝑠,𝑎))]where𝑎={𝑎 𝑗,𝜋 −𝑗(𝑠)}.Eachagentmustmaintain where𝑎={𝑎 𝑗,𝜋 −𝑗(𝑠)}.
anadversarialmodelofeachco-player.
7 EXPERIMENTS
UsingtheValueFunctiontoApproximateReturnfrom
Rollouts.BothPRIMandSBPRrequirecomputingthevalueof Wefirstvalidateourmethodsinsmallenvironmentswherewecan
statesafteranadversarialco-playerhasacted.Thebenefitofthis computetheoptimalactionsandthenmovetolargerenvironments.
trickistwo-fold:one,itreducesvariancebecauserolloutscanbe
extremelynoisy,especiallyinthebeginningoftraining,andtwo,it 7.1 SmallEnvironments
speedsupruntimesignificantly. We evaluate in the larger version of the Attack Defense Game
DomainRandomization(DR).DRisusefulforspeedingup (payoffmatrixgiveninTable3andpower-regularizedobjective
and stabilizing convergence in both methods. Overcooked is a valuesperactioninFigure2a)andanotherenvironmentcalled
highlysequentialenvironment,requiringalongstringofactions theCoinDivisiongame.Therearefouragents,one"divider"agent
toreceiveareward,soitishelpfultotrainstartingfromrandom (P0),andthree"accepter"agents(P1,P2,andP3).Therearesixbins
statesandlearntheoptimalpolicybackwards.Furthermore,cru- withthefollowingassignmentofagentstobins:([],[P0],[P0,P1],
ciallyforPRIM,DRenablesaccuratevalueestimatesofstatesthat [P0,P2],[P1,P2],[P1,P2,P3]).Thedivideragentmustallocatefive
areoff-policyandthusnormallynotvisited.Thisallowsagentsto coinsamongstthebins.Foreachbin,theagentsassignedtothat
learnhowtorecoverfromadversarialdeviationsandupdatetheir binhavetheoptionofacceptingorrejecting.Ifeveryoneaccepts,
valueestimatesofsuchstatesaccordingly. everyonetakeshomethenumberofcoinsassignedtothatbintimes
NormalizationfortheAdversary.Theadversary’sobjectiveis thenumberofagentsassignedtothatbin.Ifoneormoreagents
highlydependentonthestartingstatebecauseitonlygetstoactfor reject,thecoinsassignedtothatbinaredestroyed.Weconsiderthe
onetimestep,thusthevalueishighvariancewhichisonlyworsened divideragent’soptimalpolicyandassumethatallaccepteragents
byDR.Wereducevariancebynormalizingtheadversary’sreward alwaysaccept90%ofthetime(perbin).
SeeFigure2bforthepower-regularizedobjectivevaluesofeach
action(omittingactionswhicharestrictlydominated)andFigure2c(a)PRIMvsSBPRvsTask-OnlyBaseline (b)PRIMAblations (c)AdversaryPolicyConvergence
Figure4:ExperimentalResultsinOvercookedClose-Pot-Far-Pot.Errorbarsarestandarddeviationsover5trials.
Table4:End-of-trainingmetricsinOvercookedClose-Pot-Far-Pot.Errorvaluesarestandarddeviationsover5trials.
Name Taskreward PoweronAgent0 PoweronAgent1 PRObjectiveAgent0 PRObjectiveAgent1
Task-onlybaseline 104.9±0.2 217.1±47.9 203.8±26.1 50.6±12.1 53.9±6.6
SBPR 94.2±0.0 118.4±40.3 71.1±13.8 64.6±10.1 76.4±3.4
PRIM 94.4±0.1 86.0±9.2 76.6±18.6 72.9±2.2 75.2±4.7
PRIMablateadversarynorm 94.3±0.1 80.8±14.5 59.1±17.2 74.0±3.5 79.5±4.3
PRIMablateadversary 94.5±0.2 93.6±20.3 97.4±27.1 71.1±5.1 70.1±6.6
PRIMablateVF 0.0±0.0 0.0±0.0 0.0±0.0 0.0±0.0 0.0±0.0
forthecorrespondingPareofrontier.BothPRIMandSBPRachieve onionagentmakingfewersoups.ThestatedepictedinFigure3b
theoptimalactionsforvaluesof𝜆sampledintherange0to1. ison-policysincethetomatoagentmustmoveupbeforeturning
righttofaceitsprivatepottoplaceitstomatothere.
7.2 Overcooked:Close-Pot-Far-Pot Wecomparetheperformanceofourmethodstothetask-only
WeevaluatebothSBPRandPRIMinOvercooked,a2playergrid- baseline.Wecomputegroundtruthpowerthroughanexhaustive
worldgamewheretheobjectiveistoprepareanddeliversoups searchforthereturn-minimizingactionandconductfullrollouts
accordingtogivenrecipes.Recipesmaycallfortwotypesofin- toevaluateresultingstates.Thisisextremelyslowsoweonlycal-
gredients,tomatoesandonions.Agentsmustcollectandplaceall culateitonceeveryseveralhundredtrainingiterations.Ingeneral
ingredientsinapotoneatatime,cookthesoup,grabaplate,place rolloutsarehighvariancesomultipletrialsshouldbeperformed,
thefinishedsoupontotheplate,andfinallydeliverthesoup. butsinceouragentsconvergetowardsdeterministicpoliciesinour
Theactionspaceis{N,E,S,W,STAY,INTERACT}.Dependingon environments,wesimplydeterminizethepolicieswhenrollingout.
wheretheagentisfacing,INTERACTcanmeanpickupaningredient, For𝜆=0.25,Figure4ashowsthatPRIMoutperformsthebase-
placeaningredientintoapot,startcooking,pickupadish,place lineofoptimizingforjustthetaskreward.PRIMalsoperforms
soupontothedish,ordeliverasoup.It’simpossibletoremove betterthanSBPRforoneagentduetoitsinherentlylowervariance
ingredientsfromapotoncetheyareplaced. trainingdatawhichmakesthelearningproblemeasier.
Wedesignalayout"Close-Pot-Far-Pot"withtworecipes,3toma- Nextweranaseriesofablationexperimentstobetterunderstand
toesor3onions,eachgiving𝑅 reward.Thetopagentcanonly PRIM, shown in Figure 4b. Ablating the learned adversary and
accessonionsandthebottomagentcanonlyaccesstomatoes.Each insteadconductinganexhaustivesearchovertheactionspacedid
agentcanaccesstwopots,onesharedinthecenterandtheother notmakemuchdifferenceontheobjectivevalueachieved.Thisis
isprivate,inaccessibletotheotheragent,butfurther.Theagents expected;thegoaloflearningtheadversaryissimplytospeedup
sharearewardfunctionandatrajectoryis𝑇 steps. thepowercomputation:ratherthaniteratingovertheactionspace,
Inourexperimentsweset𝑇 =105and𝑅=20.Anassemblyline wepaya"fixedcost"totrainandquerytheadversary.Thisisis
(strategy1)usingthemiddlepotcanproduce7soups,oneagent necessaryinenvironmentswithlargeactionspaces.
independentlyusingthemiddlepotandtheotherusingtheirprivate Ablating normalization for the adversary’s objective did not
potcanproduce9soups(strategy2),andbothagentsindependently significantlychangetheobjectivevalueachieved,butitdidhurtthe
usingtheirprivatepotscanproduce8soups(strategy3). adversary’sconvergence.Figure4cdepictsthepoorconvergence
Strategy2maximizestaskrewardbutincurshighadversarial forthestateinFigure3bwheretheoptimalactionisINTERACT.
power:asshowninFigure3b,thetomatoagentcanmessupthe Finallyweablatedtheuseofvaluefunctiontoapproximatethe
onionagent’ssoupbyputtinginawrongingredient,leadingtothe returnfromarollout.Weranthisexperimentforthesameamount(a)PRIMvsBaseline(Explosion) (b)PRIMvsSBPR(Explosion) (c)PRIMvsSBPR(Explosion)
Figure5:ComparisonofPRIM,SBPR,andTask-OnlyBaselineinOvercookedExplosionwith𝜆=0.0001.Insomerunsonlyone
agentisvisiblebecausetheplotscoincidecompletely;thepowersincurredaretoosmalltobedistinguishableaftermultiplying
by𝜆.Errorbarsarestandarddeviationsover5trialsexceptforSBPRinteractoraclewhichonlyhas3trials.
Table5:ExperimentalResultsinExplosionEnvironment.Errorvaluesarestandarddeviationsover5trialsexceptforSBPR
interactoraclewhichonlyhas3trials.
Name Taskreward PoweronAgent0 PoweronAgent1 PRObjectiveAgent0 PRObjectiveAgent1
Task-onlybaseline 104.7±0.4 139949.5±171647.1 242363.6±182484.0 90.7±17.3 80.5±18.1
SBPR 105.0±0.0 297351.0±148585.0 78369.8±156414.3 75.2±14.8 97.1±15.7
SBPRINTERACTadversary 65.5±16.2 174.0±176.0 217.8±212.6 65.5±16.2 65.5±16.2
PRIM 94.2±0.0 138.4±2.7 132.3±40.5 94.2±2.2 94.2±0.0
oftimeastheotherexperiments,butitwassoslowitwasonly AsshowninFigure5c,SBPRreliesontheadversarynotyet
abletofinish2e6agentstepsandachieved0onthetaskreward. convergingatthebeginningbecausethisallowstheagentstosolve
WesummarizetheresultsofourexperimentsinTable4. enoughoftheexplorationproblembeforeconsistentlyincurring
thepenalty.Replacingtheadversarywithanagentthatalways
7.3 Overcooked:Explosion playsINTERACT(aninteractoracle)causesSBPRtofail.
WesummarizetheExplosionresultsinTable5.PRIMistheonly
IntheClose-Pot-Far-Potlayout,anadversarialdeviationdoesnot
methodthatavoidsincurringcatastrophicallyhighpoweratthe
havelargeconsequences,butpowerregularizationmaybemoreuse-
costofabitoftaskreward.
fulinhighstakesevents.WecreateavariantofClose-Pot-Far-Pot
calledExplosionwhereweinterprettheingredientsaschemicals,
8 CONCLUSION
thepotsastesttubes,andtherecipesaschemicalformulas.Ifunlike
chemicalsaremixedtogether,adangerouschemicalreactioncauses Wedefinedanotionofpoweramenabletooptimizationandshowed
anexplosionwhichincursanimmediatepenaltyof𝑃 =−100,000. thatequilibriaalwaysexistwhenagentsregularizeforpower.Next,
Figure5acomparesthetaskrewardonlybaselinetoPRIMwith wepresentedtwoalgorithms,SampleBasedPowerRegularization
𝜆 = 0.0001.Notethatthebluelineishiddenbeneaththeorange (SBPR)andPowerRegularizationviaIntrinsicMotivation(PRIM).
line.PRIMconvergestoverylowvariancewhilethebaselinehas Wevalidateourmethodsinaseriesofsmallenvironmentsand
highvariance.Thisisdueinlargeparttothefactthatagents0and intwovariantsofOvercooked,showingthatbothmethodsguide
1mayswitchrolesinwhousesthesharedpotsoeitheragentmay agentstowardlowerpowerbehavior.SBPRissimplerbutPRIMis
incurthelargepowerpenalty. betterabletohandleverylowvaluesof𝜆.
NowweexamineSBPR’sperformance(seeFigure5b).Weex- Therearemanyavenuesforfuturework,includingexploring
pectedSBPRtofailsincetheprobabilityofadeviation𝑝 =0.0001 different definitions of power (empirically and philosophically)
issolowyettheexplosionpenalty𝑃 = −100,000issohigh,but andmodelingmultipletimestepdeviations.Ourtheoreticalresults
theobservedperformancewasbetterthanexpected.However,a holdforgeneral-sumgamesbutwehavenotexploredgeneral-sum
significantamountofhyperparametertuningwasnecessary:we gamesempirically.
adjustedthePPOclipparamandmaximumgradnormdownto0.1
andlengthenedtheentropyschedule.Dependingontheparticular ACKNOWLEDGMENTS
hyperparametervalues,theagentswouldeitherfailtooptimizefor Wearegratefulforinsightfulconversationsfromthemembersof
poweratallorwouldconvergeonanassemblylinethatavoidsthe the Center for Human-Compatible AI, including Micah Carroll,
explosionrisk(butissuboptimaltoPRIM’ssolution). NiklasLauffer,AdamGleave,DanielFilan,LawrenceChan,andSamToyer,aswellasDerekYenfromMIT.Wearealsogratefulfor [14] EdwardHughes,JoelZLeibo,MatthewPhillips,KarlTuyls,EdgarDueñez-
fundingofthisworkasagiftfromtheBerkeleyExistentialRisk Guzman,AntonioGarcíaCastañeda,IainDunning,TinaZhu,KevinMcKee,
RaphaelKoster,etal.2018.Inequityaversionimprovescooperationinintertem-
Initiative.
poralsocialdilemmas. Advancesinneuralinformationprocessingsystems31
(2018).
REFERENCES [15] NatashaJaques,AngelikiLazaridou,EdwardHughes,CaglarGulcehre,Pedro
Ortega,DJStrouse,JoelZLeibo,andNandoDeFreitas.2019.Socialinfluenceas
[1] NatashaAlechina,JosephYHalpern,andBrianLogan.2020.Causality,responsi- intrinsicmotivationformulti-agentdeepreinforcementlearning.InInternational
bilityandblameinteamplans.arXivpreprintarXiv:2005.10297(2020). conferenceonmachinelearning.PMLR,3040–3049.
[2] SamuelBarrett,PeterStone,andSaritKraus.2011.Empiricalevaluationofad [16] JernejKosandDawnSong.2017.Delvingintoadversarialattacksondeeppolicies.
hocteamworkinthepursuitdomain.InThe10thInternationalConferenceon arXivpreprintarXiv:1705.06452(2017).
AutonomousAgentsandMultiagentSystems-Volume2.567–574. [17] JoelZLeibo,ViniciusZambaldi,MarcLanctot,JanuszMarecki,andThoreGraepel.
[3] MicahCarroll,RohinShah,MarkKHo,TomGriffiths,SanjitSeshia,PieterAbbeel, 2017.Multi-agentreinforcementlearninginsequentialsocialdilemmas.arXiv
andAncaDragan.2019.Ontheutilityoflearningabouthumansforhuman-ai preprintarXiv:1702.03037(2017).
coordination.Advancesinneuralinformationprocessingsystems32(2019). [18] Yen-ChenLin,Zhang-WeiHong,Yuan-HongLiao,Meng-LiShih,Ming-YuLiu,
[4] HanaChocklerandJosephYHalpern.2004. Responsibilityandblame:A andMinSun.2017.Tacticsofadversarialattackondeepreinforcementlearning
structural-modelapproach.JournalofArtificialIntelligenceResearch22(2004), agents.arXivpreprintarXiv:1703.06748(2017).
93–115. [19] MichaelLLittman.1994.Markovgamesasaframeworkformulti-agentrein-
[5] VirginiaDignumandFrankDignum.2006.Coordinatingtasksinagentorgani- forcementlearning.InMachinelearningproceedings1994.Elsevier,157–163.
zations.InInternationalWorkshoponCoordination,Organizations,Institutions, [20] MichaelLLittmanetal.2001.Friend-or-foeQ-learningingeneral-sumgames.
andNormsinAgentSystems.Springer,32–47. InICML,Vol.1.322–328.
[6] JakobNFoerster,RichardYChen,MaruanAl-Shedivat,ShimonWhiteson,Pieter [21] StevenLukes.2021.Power:Aradicalview.BloomsburyPublishing.
Abbeel,andIgorMordatch.2017.Learningwithopponent-learningawareness. [22] MichaelMann.2012.Thesourcesofsocialpower:volume1,ahistoryofpowerfrom
arXivpreprintarXiv:1709.04326(2017). thebeginningtoAD1760.Vol.1.Cambridgeuniversitypress.
[7] MeirFriedenbergandJosephYHalpern.2019.Blameworthinessinmulti-agent [23] JackSerrino,MaxKleiman-Weiner,DavidCParkes,andJoshTenenbaum.2019.
settings.InProceedingsoftheAAAIConferenceonArtificialIntelligence,Vol.33. Findingfriendandfoeinmulti-agentgames. AdvancesinNeuralInformation
525–532. ProcessingSystems32(2019).
[8] TobiasGerstenberg,JosephYHalpern,andJoshuaBTenenbaum.2015.Respon- [24] PeterStone,GalA.Kaminka,SaritKraus,andJeffreyS.Rosenschein.2010.Ad
sibilityjudgmentsinvotingscenarios..InCogSci. HocAutonomousAgentTeams:CollaborationwithoutPre-Coordination.In
[9] AdamGleave,MichaelDennis,CodyWild,NeelKant,SergeyLevine,andStuart ProceedingsoftheTwenty-FourthConferenceonArtificialIntelligence.
Russell.2019.Adversarialpolicies:Attackingdeepreinforcementlearning.arXiv [25] ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan,
preprintarXiv:1905.10615(2019). IanGoodfellow,andRobFergus.2013.Intriguingpropertiesofneuralnetworks.
[10] DavideGrossi,FrankDignum,VirginiaDignum,MehdiDastani,andLàmber arXivpreprintarXiv:1312.6199(2013).
Royakkers.2006.Structuralaspectsoftheevaluationofagentorganizations.In [26] JohannesTreutlein,MichaelDennis,CasparOesterheld,andJakobFoerster.
InternationalWorkshoponCoordination,Organizations,Institutions,andNormsin 2021.Anewformalism,methodandopenissuesforzero-shotcoordination.In
AgentSystems.Springer,3–18. InternationalConferenceonMachineLearning.PMLR,10413–10423.
[11] JosephHalpernandMaxKleiman-Weiner.2018.Towardsformaldefinitionsof [27] AlexTurner,NealeRatzlaff,andPrasadTadepalli.2020.Avoidingsideeffectsin
blameworthiness,intention,andmoralresponsibility.InProceedingsoftheAAAI complexenvironments.AdvancesinNeuralInformationProcessingSystems33
ConferenceonArtificialIntelligence,Vol.32. (2020),21406–21415.
[12] HengyuanHu,AdamLerer,AlexPeysakhovich,andJakobFoerster.2020.“Other- [28] AlexanderMattTurner,DylanHadfield-Menell,andPrasadTadepalli.2020.
Play”forZero-ShotCoordination.InInternationalConferenceonMachineLearn- Conservativeagencyviaattainableutilitypreservation.InProceedingsofthe
ing.PMLR,4399–4410. AAAI/ACMConferenceonAI,Ethics,andSociety.385–391.
[13] SandyHuang,NicolasPapernot,IanGoodfellow,YanDuan,andPieterAbbeel. [29] AlexanderMattTurner,LoganSmith,RohinShah,AndrewCritch,andPrasad
2017. Adversarial attacks on neural network policies. arXiv preprint Tadepalli. 2019. Optimal Policies Tend to Seek Power. arXiv preprint
arXiv:1702.02284(2017). arXiv:1912.01683(2019).