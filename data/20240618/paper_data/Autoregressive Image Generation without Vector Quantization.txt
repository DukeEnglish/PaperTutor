Autoregressive Image Generation without Vector Quantization
TianhongLi1 YonglongTian2 HeLi3 MingyangDeng1 KaimingHe1
1MITCSAIL 2GoogleDeepMind 3TsinghuaUniversity
Abstract
Conventional wisdom holds that autoregressive models for image generation are
typically accompanied by vector-quantized tokens. We observe that while a
discrete-valued space can facilitate representing a categorical distribution, it is
not a necessity for autoregressive modeling. In this work, we propose to model
the per-token probability distribution using a diffusion procedure, which allows
ustoapplyautoregressivemodelsinacontinuous-valuedspace. Ratherthanus-
ing categorical cross-entropy loss, we define a Diffusion Loss function to model
the per-token probability. This approach eliminates the need for discrete-valued
tokenizers. Weevaluateitseffectivenessacrossawiderangeofcases, including
standard autoregressive models and generalized masked autoregressive (MAR)
variants. By removing vector quantization, our image generator achieves strong
resultswhileenjoyingthespeed advantageofsequencemodeling. Wehope this
workwillmotivatetheuseofautoregressivegenerationinothercontinuous-valued
domainsandapplications.
1 Introduction
Autoregressivemodelsarecurrentlythedefactosolutiontogenerativemodelsinnaturallanguage
processing [38, 39, 3]. These models predict the next word or token in a sequence based on the
previous words as input. Given the discrete nature of languages, the inputs and outputs of these
modelsareinacategorical,discrete-valuedspace. Thisprevailingapproachhasledtoawidespread
beliefthatautoregressivemodelsareinherentlylinkedtodiscreterepresentations.
As a result, research on generalizing autoregressive models to continuous-valued domains—most
notably,imagegeneration—hasintenselyfocusedondiscretizingthedata[6,13,40]. Acommonly
adoptedstrategyistotrainadiscrete-valuedtokenizeronimages,whichinvolvesafinitevocabulary
obtained by vector quantization (VQ) [51, 41]. Autoregressive models are then operated on the
discrete-valuedtokenspace,analogoustotheirlanguagecounterparts.
In this work, we aim to address the following question: Is it necessary for autoregressive models
to be coupled with vector-quantized representations? We note that the autoregressive nature, i.e.,
“predictingnexttokensbasedonpreviousones”,isindependentofwhetherthevaluesarediscrete
or continuous. What is needed is to model the per-token probability distribution, which can be
measured by a loss function and used to draw samples from. Discrete-valued representations can
be conveniently modeled by a categorical distribution, but it is not conceptually necessary. If al-
ternativemodelsforper-tokenprobabilitydistributionsarepresented,autoregressivemodelscanbe
approachedwithoutvectorquantization.
With this observation, we propose to model the per-token probability distribution by a diffusion
procedure operating on continuous-valued domains. Our methodology leverages the principles of
diffusion models [45, 24, 33, 10] for representing arbitrary probability distributions. Specifically,
ourmethodautoregressivelypredictsavectorz foreachtoken, whichservesasaconditioningfor
adenoisingnetwork(e.g.,asmallMLP).Thedenoisingdiffusionprocedureenablesustorepresent
an underlying distribution p(xz) for the output x (Figure 1). This small denoising network is
|
Preprint.Underreview.
4202
nuJ
71
]VC.sc[
1v83811.6042:viXraFigure1: DiffusionLoss. Givenacontinuous-valuedto-
kenxtobepredicted,theautoregressivemodelproducesa
vectorz,whichservesastheconditionofadenoisingdif-
fusionnetwork(asmallMLP).Thisoffersawaytomodel
theprobabilitydistributionp(xz)ofthistoken. Thisnet-
conditionz |
work is trained jointly with the autoregressive model by
backpropagation. At inference time, with a predicted z,
noisyx t MLP ε running the reverse diffusion procedure can sample a to-
diffusionlossforp(xz) kenfollowingthedistribution: x p(xz). Thismethod
| ∼ |
eliminatestheneedfordiscrete-valuedtokenizers.
trainedjointlywiththeautoregressivemodel,withcontinuous-valuedtokensastheinputandtarget.
Conceptually, this small prediction head, applied to each token, behaves like a loss function for
measuringthequalityofz. WerefertothislossfunctionasDiffusionLoss.
Our approach eliminates the need for discrete-valued tokenizers. Vector-quantized tokenizers are
difficulttotrainandaresensitivetogradientapproximationstrategies[51,41,40,27]. Theirrecon-
struction quality often falls short compared to continuous-valued counterparts [42]. Our approach
allowsautoregressivemodelstoenjoythebenefitsofhigher-quality,non-quantizedtokenizers.
Tobroadenthescope,wefurtherunifystandardautoregressive(AR)models[13]andmaskedgener-
ativemodels[4,29]intoageneralizedautoregressiveframework(Figure3). Conceptually,masked
generativemodelspredictmultipleoutputtokenssimultaneouslyinarandomized order,whilestill
maintainingtheautoregressivenatureof“predictingnexttokensbasedonknownones”. Thisleads
toamaskedautoregressive(MAR)modelthatcanbeseamlesslyusedwithDiffusionLoss.
WedemonstratebyexperimentstheeffectivenessofDiffusionLossacrossawidevarietyofcases,
includingARandMARmodels. Iteliminatestheneedforvector-quantizedtokenizersandconsis-
tently improves generation quality. Our loss function can be flexibly applied with different types
oftokenizers. Further,ourmethodenjoystheadvantageofthefastspeedofsequencemodels. Our
MARmodelwithDiffusionLosscangenerateatarateof<0.3secondperimagewhileachieving
astrongFIDof<2.0onImageNet256 256. Ourbestmodelcanapproach1.55FID.
×
The effectiveness of our method reveals a largely uncharted realm of image generation: modeling
theinterdependenceoftokensbyautoregression,jointlywiththeper-tokendistributionbydiffusion.
Thisisincontrastwithtypicallatentdiffusionmodels[42,37]inwhichthediffusionprocessmodels
thejointdistributionofalltokens. Giventheeffectiveness,speed,andflexibilityofourmethod,we
hope that the Diffusion Loss will advance autoregressive image generation and be generalized to
otherdomainsinfutureresearch.
2 RelatedWork
SequenceModelsforImageGeneration. Pioneeringeffortsonautoregressiveimagemodels[17,
50,49,36,7,6]operateonsequencesofpixels. AutoregressioncanbeperformedbyRNNs[50],
CNNs[49,7],and,mostlatelyandpopularly,Transformers[36,6]. Motivatedbylanguagemodels,
another series of works [51, 41, 13, 40] model images as discrete-valued tokens. Autoregressive
[13,40]andmaskedgenerativemodels[4,29]canoperateonthediscrete-valuedtokenspace. But
discretetokenizersaredifficulttotrain,whichhasrecentlydrawnspecialfocus[27,54,32].
Related to our work, the recent work on GIVT [48] also focuses on continuous-valued tokens in
sequence models. GIVT and our work both reveal the significance and potential of this direction.
In GIVT, the token distribution is represented by Gaussian mixture models. It uses a pre-defined
number of mixtures, which can limit the types of distributions it can represent. In contrast, our
methodleveragestheeffectivenessofthediffusionprocessformodelingarbitrarydistributions.
Diffusion for Representation Learning. The denoising diffusion process has been explored as a
criterionforvisualself-supervisedlearning. Forexample,DiffMAE[53]replacestheL2lossinthe
originalMAE[21]withadenoisingdiffusiondecoder;DARL[30]trainsautoregressivemodelswith
a denoising diffusion patch decoder. These efforts have been focused on representation learning,
rather than image generation. In their scenarios, generating diverse images is not a goal; these
methodshavenotpresentedthecapabilityofgeneratingnewimagesfromscratch.
2DiffusionforPolicyLearning.OurworkisconceptuallyrelatedtoDiffusionPolicy[8]inrobotics.
Inthosescenarios, thedistributionoftakinganactionisformulatedasadenoisingprocessonthe
robot observations, which can be pixels or latents [8, 34]. In image generation, we can think of
generating a token as an “action” to take. Despite this conceptual connection, the diversity of the
generatedsamplesinroboticsislessofacoreconsiderationthanitisforimagegeneration.
3 Method
In a nutshell, our image generation approach is a sequence model operated on a tokenized latent
space[6,13,40]. Butunlikepreviousmethodsthatarebasedonvector-quantizedtokenizers(e.g.,
variantsofVQ-VAE[51,13]),weaimtousecontinuous-valuedtokenizers(e.g.,[42]). Wepropose
DiffusionLossthatmakessequencemodelscompatiblewithcontinuous-valuedtokens.
3.1 RethinkingDiscrete-ValuedTokens
To begin with, we revisit the roles of discrete-valued tokens in autoregressive generation models.
Denoteasxtheground-truthtokentobepredictedatthenextposition. Withadiscretetokenizer,x
canberepresentedasaninteger: 0 x < K,withavocabularysizeK. Theautoregressivemodel
producesacontinuous-valuedD-dim≤ vectorz RD,whichisthenprojectedbyaK-wayclassifier
matrixW RK×D. Conceptually,thisformula∈ tionmodelsacategoricalprobabilitydistributionin
∈
theformofp(xz)=softmax(Wz).
|
Inthecontextofgenerativemodeling,thisprobabilitydistributionmustexhibittwoessentialprop-
erties. (i)Alossfunctionthatcanmeasurethedifferencebetweentheestimatedandtruedistribu-
tions.Inthecaseofcategoricaldistribution,thiscanbesimplydonebythecross-entropyloss.(ii)A
samplerthatcandrawsamplesfromthedistributionx p(xz)atinferencetime.Inthecaseofcat-
∼ |
egoricaldistribution,thisisoftenimplementedasdrawingasamplefromp(xz)=softmax(Wz/τ),
|
inwhichτ isatemperaturethatcontrolsthediversityofthesamples. Samplingfromacategorical
distributioncanbeapproachedbytheGumbel-maxmethod[18]orinversetransformsampling.
Thisanalysissuggeststhatdiscrete-valuedtokensarenot necessaryforautoregressivemodels. In-
stead,itistherequirementofmodelingadistributionthatisessential.Adiscrete-valuedtokenspace
implies acategorical distribution, whoseloss function andsampler are simpleto define. What we
actuallyneedarealossfunctionanditscorrespondingsamplerfordistributionmodeling.
3.2 DiffusionLoss
Denoisingdiffusionmodels[24]offeraneffectiveframeworktomodelarbitrarydistributions. But
unlikecommonusagesofdiffusionmodelsforrepresentingthejointdistributionofallpixelsorall
tokens,inourcase,thediffusionmodelisforrepresentingthedistributionforeachtoken.
Consideracontinuous-valuedvectorx Rd,whichdenotestheground-truthtokentobepredicted
atthenextposition. Theautoregressive∈ modelproducesavectorz RD atthisposition. Ourgoal
∈
istomodelaprobabilitydistributionofxconditionedonz, thatis, p(xz). Thelossfunctionand
|
samplercanbedefinedfollowingthediffusionmodels[24,33,10],describednext.
Loss function. Following [24, 33, 10], the loss function of an underlying probability distribution
p(xz)canbeformulatedasadenoisingcriterion:
|
(cid:104) (cid:105)
(z,x)=E ε ε (x t,z) 2 . (1)
ε,t θ t
L ∥ − | ∥
Here, ε Rd is a noise vector sampled from (0,I). The noise-corrupted vector x is x =
t t
∈ N
√α¯ x+√1 α¯ ε,whereα¯ definesanoiseschedule[24,33].tisatimestepofthenoiseschedule.
t t t
−
The noise estimator ε , parameterized by θ, is a small MLP network (see Sec. 4). The notation
θ
ε (x t,z)meansthatthisnetworktakesx astheinput,andisconditionalonbothtandz. Asper
θ t t
|
[46,47],Eqn.(1)conceptuallybehaveslikeaformofscorematching: itisrelatedtoalossfunction
concerningthescorefunctionofp(xz), thatis, log p(xz). DiffusionLossisaparameterized
| ∇ x |
lossfunction,inthesameveinastheadversarialloss[15]orperceptualloss[56].
Itisworthnoticingthattheconditioningvectorz isproducedbytheautoregressivenetwork: z =
f(), as we will discuss later. The gradient of z = f() is propagated from the loss function in
· ·
Eqn.(1). Conceptually,Eqn.(1)definesalossfunctionfortrainingthenetworkf().
·
3WenotethattheexpectationE []inEqn.(1)isovert,foranygivenz. Asourdenoisingnetwork
ε,t
·
issmall,wecansampletmultipletimesforanygivenz. Thishelpsimprovetheutilizationofthe
lossfunction,withoutrecomputingz. Wesampletby4timesduringtrainingforeachimage.
Sampler. Atinferencetime,itisrequiredtodrawsamplesfromthedistributionp(xz). Sampling
(cid:16) (cid:17)|
isdoneviaareversediffusionprocedure[24]: x t−1 = √1 αt x t − √1 1− −α α¯t tε θ(x t |t,z) +σ tδ.Here
δissampledfromtheGaussiandistribution (0,I)andσ isthenoiselevelattimestept. Starting
t
N
withx (0,I),thisprocedureproducesasamplex suchthatx p(xz)[24].
T 0 0
∼N ∼ |
Whenusingcategoricaldistributions(Sec.3.1),autoregressivemodelscanenjoythebenefitofhav-
ingatemperatureτ forcontrollingsamplediversity. Infact,existingliterature,inbothlanguages
andimages,hasshownthattemperatureplaysacriticalroleinautoregressivegeneration.Itisdesired
forthediffusionsamplertoofferatemperaturecounterpart.Weadoptthetemperaturesamplingpre-
sentedin[10]. Conceptually,withtemperatureτ,onemaywanttosamplefromthe(renormalized)
probabilityofp(x |z)τ1 ,whosescorefunctionis τ1 ∇log xp(x |z). Inpractice,[10]suggeststoeither
divideε byτ,orscalethenoisebyτ. Weadoptthelatteroption: wescaleσ δinthesamplerbyτ.
θ t
Intuitively,τ controlsthesamplediversitybyadjustingthenoisevariance.
3.3 DiffusionLossforAutoregressiveModels
Next,wedescribetheautoregressivemodelwithDiffusionLossforimagegeneration. Givenase-
quenceoftokens x1,x2,...,xn wherethesuperscript1 i nspecifiesanorder,autoregressive
{ } ≤ ≤
models[17,50,49,36,7,6]formulatethegenerationproblemas“nexttokenprediction”:
n
(cid:89)
p(x1,...,xn)= p(xi x1,...,xi−1). (2)
|
i=1
Anetworkisusedtorepresenttheconditionalprobabilityp(xi x1,...,xi−1). Inourcase, xi can
|
becontinuous-valued. Wecanrewritethisformulationintwoparts. Wefirstproduceaconditioning
vectorzi byanetwork(e.g.,Transformer[52])operatingonprevioustokens: zi =f(x1,...,xi−1).
Then, we model the probability of the next token by p(xi zi). Diffusion Loss in Eqn. (1) can be
appliedonp(xi zi). Thegradientisbackpropagatedtozifo| rupdatingtheparametersoff().
| ·
3.4 UnifyingAutoregressiveandMaskedGenerativeModels
We show that masked generative models, e.g., MaskGIT [4] and MAGE [29], can be generalized
underthebroadconceptofautoregression,i.e.,nexttokenprediction.
Bidirectionalattentioncanperformautoregression. Theconceptofautoregressionisorthogonal
tonetworkarchitectures:autoregressioncanbedonebyRNNs[50],CNNs[49,7],andTransformers
[38,36,6]. WhenusingTransformers,althoughautoregressivemodelsarepopularlyimplemented
by causal attention, we show that they can also be done by bidirectional attention. See Figure 2.
Notethatthegoalofautoregressionistopredictthenexttokengiventheprevioustokens;itdoesnot
constrainhowtheprevioustokenscommunicatewiththenexttoken.
We can adopt the bidirectional attention implementation as done in Masked Autoencoder (MAE)
[21]. See Figure 2(b). Specifically, we first apply an MAE-style encoder1 on the known tokens
(with positional embedding [52]). Then we concatenate the encoded sequence with mask tokens
(withpositionalembeddingaddedagain),andmapthissequencewithanMAE-styledecoder. The
positional embedding on the mask tokens can let the decoder know at which positions are to be
predicted. Unlikecausalattention,herethelossiscomputedonlyontheunknowntokens[21].
WiththeMAE-styletrick,weallowallknowntokenstoseeeachother,andalsoallowallunknown
tokens to see all known tokens. This full attention introduces better communication across tokens
thancausalattention. Atinferencetime, wecangeneratetokens(oneormoreperstep)usingthis
bidirectionalformulation,whichisaformofautoregression. Asacompromise,wecannotusethe
key-value(kv)cache[44]ofcausalattentiontospeedupinference. Butaswecangeneratemultiple
tokenstogether,wecanreducegenerationstepstospeedupinference. Fullattentionacrosstokens
cansignificantlyimprovethequalityandofferabetterspeed/accuracytrade-off.
1Heretheterminologyofencoder/decoderisinthesenseofageneralAutoencoder,followingMAE[21].It
isnotrelatedtowhetherthecomputationiscasual/bidirectionalinTransformers[52].
4Figure 2: Bidirectional attention can do autoregression.
In contrast to conventional wisdom, the broad concept of
“autoregression” (next token prediction) can be done by
either causal or bidirectional attention. (a) Causal atten-
next token prediction tion restricts each token to attend only to current/previ-
ous tokens. With input shifted by one start token [s],
it is valid to compute loss on all tokens at training time.
[s] 1 2 3 4 5 1 2 3 4 5
(b)Bidirectional attention allows each token to see all to-
kens in the sequence. Following MAE [21], mask tokens
[m] are applied in a middle layer, with positional embed-
[m]
ding added. This setup only computes loss on unknown
tokens, but it allows for full attention capabilities across
1 2 3 4 5 1 2 3 4 5
thesequence,enablingbettercommunicationacrosstokens.
Thissetupcangeneratetokensonebyoneatinferencetime,
loss loss loss loss loss loss loss
whichisaformofautoregression. Italsoallowsustopre-
(a) causal (b) bidirectional
dictmultipletokenssimultaneously.
Figure 3: Generalized Autoregressive Models.
(a) AR, raster order
(a) A standard, raster-order autoregressive model
predictsonenexttokenbasedonthepreviousto-
kens. (b) A random-order autoregressive model
predicts the next token given a random order. It
(b) AR, random order
behaves like randomly masking out tokens and
thenpredictingone.(c)AMaskedAutoregressive
(MAR)modelpredictsmultipletokenssimultane-
(c) Masked AR ouslygivenarandomorder,whichisconceptually
analogoustomaskedgenerativemodels[4,29].In
allcases,thepredictionofonestepcanbedoneby
causalorbidirectionalattention(Figure2).
known/predicted to predict at this step unknown
Autoregressive models in random orders. To connect to masked generative models [4, 29], we
consider an autoregressive variant in random orders. The model is given a randomly permuted
sequence. Thisrandompermutationisdifferentforeachsample. SeeFigure3(b). Inthiscase,the
positionofthenexttokentobepredictedneedstobeaccessibletothemodel. Weadoptastrategy
similar to MAE [21]: we add positional embedding (that corresponds to the unshuffled positions)
tothedecoderlayers, whichcantellwhatpositionstopredict. Thisstrategyisapplicableforboth
causalandbidirectionalversions.
As shown in Figure 3 (b)(c), random-order autoregression behaves like a special form of masked
generation,inwhichonetokenisgeneratedatatime. Weelaborateonthisasfollows.
Maskedautoregressivemodels. Inmaskedgenerativemodeling[4,29],themodelspredictaran-
domsubsetoftokensbasedonknown/predictedtokens.Thiscanbeformulatedaspermutingtheto-
kensequencebyarandomorder,andthenpredictingmultipletokensbasedonprevioustokens. See
Figure3(c).Conceptually,thisisanautoregressiveprocedure,whichcanbewrittenasestimatingthe
conditionaldistribution: p( xi,xi+1...,xj x1,...,xi−1),wheremultipletokens xi,xi+1...,xj
{ }| { }
aretobepredicted(i j). Wecanwritethisautoregressivemodelas:
≤
K
(cid:89)
p(x1,...,xn)=p(X1,...,XK)= p(Xk X1,...,Xk−1). (3)
|
k
Here, Xk = xi,xi+1...,xj is a set of tokens to be predicted at the k-th step, with Xk =
k
x1,...,xn . { In this sense, t} his is essentially “next set-of-tokens prediction”, and thus∪ is also a
{ }
generalformofautoregression. WerefertothisvariantasMaskedAutoregressive(MAR)models.
MARisarandom-orderautoregressivemodelthatcanpredictmultipletokenssimultaneously.
MAR is conceptually related to MAGE [29]. However, MAR samples tokens by a temperature τ
applied on the probability distribution of each token (which is the standard practice in generative
languagemodelslikeGPT).Incontrast,MAGE(followingMaskGIT[4])appliesatemperaturefor
sampling the locations of the tokens to be predicted: this is not a fully randomized order, which
createsagapbetweentraining-timeandinference-timebehavior.
54 Implementation
Thissectiondescribesourimplementation. Wenotethattheconceptsintroducedinthispaperare
generalandnotlimitedtospecificimplementations. MoredetailedspecificsareinAppendixB.
4.1 DiffusionLoss
DiffusionProcess. Ourdiffusionprocessfollows[33]. Ournoiseschedulehasacosineshape,with
1000 steps at training time; at inference time, it is resampled with fewer steps (by default, 100)
[33]. Our denoising network predicts the noise vector ε [24]. The loss can optionally include the
variational lower bound term [33]. Diffusion Loss naturally supports classifier-free guidance
vlb
L
(CFG)[23](detailedinAppendixB).
DenoisingMLP.WeuseasmallMLPconsistingofafewresidualblocks[20]fordenoising. Each
blocksequentiallyappliesaLayerNorm(LN)[1],alinearlayer,SiLU[12],andanotherlinearlayer,
merging with a residual connection. By default, we use 3 blocks and a width of 1024 channels.
ThedenoisingMLPisconditionedonavectorz producedbytheAR/MARmodel(seeFigure1).
Thevectorz isaddedtothetimeembeddingofthenoisescheduletime-stept,whichservesasthe
conditionoftheMLPintheLNlayersviaAdaLN[37].
4.2 AutoregressiveandMaskedAutoregressiveImageGeneration
Tokenizer. WeusethepubliclyavailabletokenizersprovidedbyLDM[42]. Ourexperimentswill
involvetheirVQ-16andKL-16versions[42]. VQ-16isaVQ-GAN[13],i.e.,VQ-VAE[51]with
GANloss[15]andperceptualloss[56]; KL-16isitscounterpartregularizedbyKullback–Leibler
(KL)divergence,withoutvectorquantization. 16denotesthetokenizerstrides.
Transformer. OurarchitecturefollowstheTransformer[52]implementationinViT[11]. Givena
sequenceoftokensfromatokenizer,weaddpositionalembedding[52]andappendtheclasstokens
[cls];thenweprocessthesequencebyaTransformer. Bydefault,ourTransformerhas32blocks
andawidthof1024,whichwerefertoastheLargesizeor-L(∼400Mparameters).
Autoregressivebaseline. CausalattentionisimplementedfollowingthecommonpracticeofGPT
[38](Figure2(a)). Theinputsequenceisshiftedbyonetoken(here, [cls]). Triangularmasking
[52]isappliedtotheattentionmatrix. Atinferencetime,temperature(τ)samplingisapplied. We
usekv-cache[44]forefficientinference.
Masked autoregressive models. With bidirectional attention (Figure 2(b)), we can predict any
number of unknown tokens given any number of known tokens. At training time, we randomly
sampleamaskingratio[21,4,29]in[0.7,1.0]: e.g.,0.7means70%tokensareunknown. Because
thesampledsequencecanbeveryshort,wealwayspad64[cls]tokensatthestartoftheencoder
sequence,whichimprovesthestabilityandcapacityofourencoding. AsinFigure2,masktokens
[m]areintroducedinthedecoder,withpositionalembeddingadded. Forsimplicity,unlike[21],we
lettheencoderanddecoderhavethesamesize: eachhashalfofallblocks(e.g.,16inMAR-L).
Atinference,MARperforms“nextset-of-tokensprediction”. Itprogressivelyreducesthemasking
ratio from 1.0 to 0 with a cosine schedule [4, 29]. By default, we use 64 steps in this schedule.
Temperature(τ)samplingisapplied. Unlike[4,29],MARalwaysusesfullyrandomizedorders.
5 Experiments
We experiment on ImageNet [9] at a resolution of 256 256. We evaluate FID [22] and IS [43],
×
and provide Precision and Recall as references following common practice [10]. We follow the
evaluationsuiteprovidedby[10].
5.1 PropertiesofDiffusionLoss
DiffusionLossvs.Cross-entropyLoss. Wefirstcomparecontinuous-valuedtokenswithDiffusion
Loss and standard discrete-valued tokens with cross-entropy loss (Table 1). For fair comparisons,
thetokenizers(“VQ-16”and“KL-16”)arebothdownloadedfromtheLDMcodebase[42]. These
arepopularlyusedtokenizers(e.g.,[13,42,37]).
6Table 1: Diffusion Loss vs. Cross-entropy Loss. The tokenizers are VQ-16 (discrete) and KL-
16 (continuous), both from the LDM codebase [42] for fair comparisons. Diffusion Loss, with
continuous-valued tokens, is better than its cross-entropy counterpart with discrete-valued tokens,
consistentlyobservedacrossallvariantsofARandMAR.Allentriesareimplementedbyusunder
thesamesetting: AR/MAR-L( 400Mparameters),400epochs,ImageNet256 256.
∼ ×
w/oCFG w/CFG
variant order direction #preds loss FID↓ IS↑ FID↓ IS↑
CrossEnt 19.58 60.8 4.92 227.3
AR raster causal 1
DiffLoss 19.23 62.3 4.69 244.6
CrossEnt 16.22 81.3 4.36 222.7
MAR rand causal 1
DiffLoss 13.07 91.4 4.07 232.4
CrossEnt 8.75 149.6 3.50 280.9
MAR rand bidirect 1
DiffLoss 3.43 203.1 1.84 292.7
CrossEnt 8.79 146.1 3.69 278.4
MAR rand bidirect >1
(default) DiffLoss 3.50 201.4 1.98 290.3
Table 2: Flexibility of Diffusion Loss. Diffusion Loss can support different types of tokenizers.
(i) VQ tokenizers: we treat the continuous-valued latent before VQ as the tokens. (ii) Tokenizers
withamismatched stride(here,8): wegroup2 2tokensintoanewtokenforsequencemodeling.
(iii)ConsistencyDecoder[35],anon-VQtoken×
izerofadifferentdecoderarchitecture. Here,rFID
denotesthereconstructionFIDofthetokenizerontheImageNettrainingset. Settingsinthistable
forallentries: MAR-L,400epochs,ImageNet256 256. †:ThistokenizeristrainedbyusonImageNetusing[42]’s
×
code;theoriginalonesfrom[42]weretrainedonOpenImages.
tokenizer #tokens w/oCFG w/CFG
loss src arch raw seq rFID↓ FID↓ IS↑ FID↓ IS↑
[42] VQ-16 162 162 5.87 7.82 151.7 3.64 258.5
[42] KL-16 162 162 1.43 3.50 201.4 1.98 290.3
DiffLoss [42] KL-8 322 162 1.20 4.33 180.0 2.05 283.9
[35] Consistency 322 162 1.30 5.76 170.6 3.23 271.0
[42]† KL-16 162 162 1.22 2.85 214.0 1.97 291.2
ThecomparisonsareinfourvariantsofAR/MAR.AsshowninTable1,DiffusionLossconsistently
outperformsthecross-entropycounterpartinallcases.Specifically,inMAR(e.g.,thedefault),using
Diffusion Loss can reduce FID by relatively ∼50%-60%. This is because the continuous-valued
KL-16 has smaller compression loss than VQ-16 (discussed next in Table 2), and also because a
diffusionprocessmodelsdistributionsmoreeffectivelythancategoricalones.
Inthefollowingablations,unlessspecified,wefollowthe“default”MARsettinginTable1.
Flexibility of Diffusion Loss. One significant advantage of Diffusion Loss is its flexibility with
varioustokenizers. WecompareseveralpubliclyavailabletokenizersinTable2.
DiffusionLosscanbeeasilyusedevengivenaVQtokenizer.Wesimplytreatthecontinuous-valued
latent before the VQ layer as the tokens. This variant gives us 7.82 FID (w/o CFG), compared
favorablywith8.79FID(Table1)ofcross-entropylossusingthesameVQtokenizer. Thissuggests
thebettercapabilityofdiffusionformodelingdistributions.
ThisvariantalsoenablesustocomparetheVQ-16andKL-16tokenizersusingthesameloss. As
shown in Table 2, VQ-16 has a much worse reconstruction FID (rFID) than KL-16, which conse-
quentlyleadstoamuchworsegenerationFID(e.g.,7.82vs.3.50inTable2).
Interestingly,DiffusionLossalsoenablesustousetokenizerswithmismatchedstrides. InTable2,
we study a KL-8 tokenizer whose stride is 8 and output sequence length is 32 32. Without in-
×
creasingthesequencelengthofthegenerator,wegroup2 2tokensintoanewtoken. Despitethe
×
mismatch,weareabletoobtaindecentresults,e.g.,KL-8givesus2.05FID,vs.KL-16’s1.98FID.
Further, this property allows us to investigate other tokenizers, e.g., Consistency Decoder [35], a
non-VQtokenizerofadifferentarchitecture/stridedesignedfordifferentgoals.
Forcomprehensiveness,wealsotrainaKL-16tokenizeronImageNetusingthecodeof[42],noting
thattheoriginalKL-16in[42]wastrainedonOpenImages[28]. Thecomparisonisinthelastrow
ofTable2. Weusethistokenizerinthefollowingexplorations.
7MLP w/oCFG w/CFG Table3: DenoisingMLPinDiffusionLoss.
width params FID↓ IS↑ FID↓ IS↑ inferencetime The denoising MLP is small and efficient.
256 2M 3.47 195.3 2.45 274.0 0.286s/im. Here, the inference time involves the entire
512 6M 3.24 199.1 2.11 281.0 0.288s/im. generationmodel,andtheTransformer’ssize
1024 21M 2.85 214.0 1.97 291.2 0.288s/im. is407M.Settings: MAR-L,400epochs,Im-
1536 45M 2.93 207.6 1.91 289.3 0.291s/im. ageNet256 256,3MLPblocks.
×
w/o CFG CFG=3.0 Figure4:SamplingstepsofDiffusionLoss.
300
5.5 We show the FID (left) and IS (right) w.r.t.
4.5 250 thenumberofdiffusivesamplingsteps. Us-
3.5 ing100stepsissufficienttoachieveastrong
2.5 200 generationquality.
1.5
0 200 400 600 800 1000 0 200 400 600 800 1000
Steps Steps
w/o CFG CFG=2.0 CFG=3.0 CFG=4.0
4.5 350
Figure 5: Temperature of Diffusion Loss.
4.0 325 Temperature τ has clear influence on both
300
3.5 FID(left)andIS(right).Justlikethetemper-
275 3.0 ature in discrete-valued autoregression, the
250
temperature here also plays a critical role in
2.5
225
continuous-valuedautoregression.
2.0 200
1.5 175
0.90 0.95 1.00 1.05 0.90 0.95 1.00 1.05
Temperature Temperature
Denoising MLP in Diffusion Loss. We investigate the denoising MLP in Table 3. Even a very
smallMLP(e.g.,2M)canleadtocompetitiveresults. Asexpected,increasingtheMLPwidthhelps
improvethegenerationquality;wehaveexploredincreasingthedepthandhadsimilarobservations.
NotethatourdefaultMLPsize(1024width,21M)addsonly∼5%extraparameterstotheMAR-L
model. Duringinference,thediffusionsamplerhasadecentcostof∼10%overallrunningtime. In-
creasingtheMLPwidthhasnegligibleextracostinourimplementation(Table3),partiallybecause
themainoverheadisnotaboutcomputationbutmemorycommunication.
SamplingStepsofDiffusionLoss. OurdiffusionprocessfollowsthecommonpracticeofDDPM
[24,10]: wetrainwitha1000-stepnoiseschedulebutinferencewithfewersteps. Figure4shows
thatusing100diffusionstepsatinferenceissufficienttoachieveastronggenerationquality.
Temperature of Diffusion Loss. In the case of cross-entropy loss, the temperature is of central
importance. Diffusion Loss also offers a temperature counterpart for controlling the diversity and
fidelity. Figure5showstheinfluenceofthetemperatureτ inthediffusionsampler(seeSec.3.2)at
inferencetime. Thetemperatureτ playsanimportantroleinourmodels,similartotheobservations
on cross-entropy-based counterparts (note that the cross-entropy results in Table 1 are with their
optimaltemperatures).
5.2 PropertiesofGeneralizedAutoregressiveModels
From AR to MAR. Table 1 is also a comparison on the AR/MAR variants, which we discuss
next. First,replacingtherasterorderinARwithrandomorderhasasignificantgain,e.g.,reducing
FID from 19.23 to 13.07 (w/o CFG). Next, replacing the causal attention with the bidirectional
counterpartleadstoanothermassivegain,e.g.,reducingFIDfrom13.07to3.43(w/oCFG).
Therandom-order,bidirectionalARisessentiallyaformofMARthatpredictsonetokenatatime.
Predictingmultipletokens(‘>1’)ateachstepcaneffectivelyreducethenumberofautoregressive
steps. InTable1,weshowthattheMARvariantwith64stepsslightlytradesoffgenerationquality.
Amorecomprehensivetrade-offcomparisonisdiscussednext.
Speed/accuracyTrade-off. FollowingMaskGIT[4],ourMARenjoystheflexibilityofpredicting
multipletokensatatime. Thisiscontrolledbythenumberofautoregressivestepsatinferencetime.
Figure 6 plots the speed/accuracy trade-off. MAR has a better trade-off than its AR counterpart,
notingthatARiswiththeefficientkv-cache.
WithDiffusionLoss,MARalsoshowsafavorabletrade-offincomparisonwiththerecentlypopular
Diffusion Transformer (DiT) [37]. As a latent diffusion model, DiT models the interdependence
8
DIF
DIF
erocS
noitpecnI
erocS
noitpecnI5.0 Figure6:Speed/accuracytrade-offofthegener-
AR, CrossEnt
ation process. For MAR, a curve is obtained by
4.5
differentautoregressivesteps(8to128). ForDiT,
4.0 acurveisobtainedbydifferentdiffusionsteps(50,
75, 150, 250) using its official code. We com-
3.5 MAR, CrossEnt
pareourimplementationofARandMAR.ARis
3.0 withkv-cacheforfastinference. AR/MARmodel
sizeisLandDiTmodelsizeisDiT-XL.Thestar
2.5 DiT marker denotes our default MAR setting used in
2.0 MAR, Diff Loss otherablations. WebenchmarkFIDandspeedon
(default) ImageNet256 256usingoneA100GPUwitha
1.5 ×
0.0 0.4 0.8 1.2 1.6 batchsizeof256.
Inference Time (sec / image)
Table4: System-levelcomparisononImageNet256 256conditionalgeneration. DiffusionLoss
×
enablesMaskedAutoregressiontoachieveleadingresultsincomparisonwithprevioussystems.
†:LDMoperatesoncontinuous-valuedtokens,thoughthisresultusesaquantizedtokenizer.
w/oCFG w/CFG
#params FID↓ IS↑ Pre.↑ Rec.↑ FID↓ IS↑ Pre.↑ Rec.↑
pixel-based
ADM[10] 554M 10.94 101.0 0.69 0.63 4.59 186.7 0.82 0.52
VDM++[26] 2B 2.40 225.3 - - 2.12 267.7 - -
vector-quantizedtokens
Autoreg.w/VQGAN[13] 1.4B 15.78 78.3 - - - - - -
MaskGIT[4] 227M 6.18 182.1 0.80 0.51 - - - -
MAGE[29] 230M 6.93 195.8 - - - - - -
MAGVIT-v2[55] 307M 3.65 200.5 - - 1.78 319.4 - -
continuous-valuedtokens
LDM-4†[42] 400M 10.56 103.5 0.71 0.62 3.60 247.7 0.87 0.48
U-ViT-H/2-G[2] 501M - - - - 2.29 263.9 0.82 0.57
DiT-XL/2[37] 675M 9.62 121.5 0.67 0.67 2.27 278.2 0.83 0.57
DiffiT[19] - - - - - 1.73 276.5 0.80 0.62
MDTv2-XL/2[14] 676M 5.06 155.6 0.72 0.66 1.58 314.7 0.79 0.65
GIVT[48] 304M 5.67 - 0.75 0.59 3.35 - 0.84 0.53
MAR-B,DiffLoss 208M 3.48 192.4 0.78 0.58 2.31 281.7 0.82 0.57
MAR-L,DiffLoss 479M 2.60 221.4 0.79 0.60 1.78 296.0 0.81 0.60
MAR-H,DiffLoss 943M 2.35 227.8 0.79 0.62 1.55 303.7 0.81 0.62
acrossalltokensbythediffusionprocess. Thespeed/accuracytrade-offofDiTismainlycontrolled
by its diffusion steps. Unlike our diffusion process on a small MLP, the diffusion process of DiT
involvestheentireTransformerarchitecture. Ourmethodismoreaccurateandfaster. Notably,our
methodcangenerateatarateof<0.3secondperimagewithastrongFIDof<2.0.
5.3 BenchmarkingwithPreviousSystems
WecomparewiththeleadingsystemsinTable4. Weexplorevariousmodelsizes(seeAppendixB)
and train for 800 epochs. Similar to autoregressive language models [3], we observe encouraging
scaling behavior. Further investigation into scaling could be promising. Regarding metrics, we
report2.35FIDwithoutCFG,largelyoutperformingothertoken-basedmethods. Ourbestentryhas
1.55FIDandcomparesfavorablywithleadingsystems. Figure7showsqualitativeresults.
6 DiscussionandConclusion
The effectiveness of Diffusion Loss on various autoregressive models suggests new opportunities:
modeling the interdependence of tokens by autoregression, jointly with the per-token distribution
bydiffusion. Thisisunlikethecommonusageofdiffusionthatmodelsthejointdistributionofall
tokens. Our strong results on image generation suggest that autoregressive models or their exten-
sions are powerful tools beyond language modeling. These models do not need to be constrained
by vector-quantized representations. We hope our work will motivate the research community to
exploresequencemodelswithcontinuous-valuedrepresentationsinotherdomains.
9
DIFFigure7: QualitativeResults. Weshowselectedexamplesofclass-conditionalgenerationonIma-
geNet256 256usingMAR-HwithDiffusionLoss.
×
10References
[1] JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton.Layernormalization.arXiv:1607.06450,2016.
[2] FanBao, ChongxuanLi, YueCao, andJunZhu. Allareworthwords: avitbackboneforscore-based
diffusionmodels. InNeurIPS2022WorkshoponScore-BasedMethods,2022.
[3] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,Clemens
Winter,ChrisHesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,
ChristopherBerner,SamMcCandlish,AlecRadford,IlyaSutskever,andDarioAmodei. Languagemod-
elsarefew-shotlearners. InNeurIPS,2020.
[4] HuiwenChang,HanZhang,LuJiang,CeLiu,andWilliamTFreeman. MaskGIT:Maskedgenerative
imageTransformer. InCVPR,2022.
[5] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang,
KevinMurphy,WilliamTFreeman,MichaelRubinstein,YuanzhenLi,andDilipKrishnan. Muse:Text-
to-imagegenerationviamaskedgenerativeTransformers. InICML,2023.
[6] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.
Generativepretrainingfrompixels. InICML,2020.
[7] XiChen,NikhilMishra,MostafaRohaninejad,andPieterAbbeel.PixelSNAIL:Animprovedautoregres-
sivegenerativemodel. InICML,2018.
[8] ChengChi,SiyuanFeng,YilunDu,ZhenjiaXu,EricCousineau,BenjaminBurchfiel,andShuranSong.
Diffusionpolicy:Visuomotorpolicylearningviaactiondiffusion. InRSS,2023.
[9] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei. ImageNet: Alarge-scalehierar-
chicalimagedatabase. InCVPR,2009.
[10] PrafullaDhariwalandAlexanderNichol. DiffusionmodelsbeatGANsonimagesynthesis. InNeurIPS,
2021.
[11] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,ThomasUn-
terthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,andNeil
Houlsby. Animageisworth16x16words:Transformersforimagerecognitionatscale. InICLR,2021.
[12] StefanElfwing,EijiUchibe,andKenjiDoya. Sigmoid-weightedlinearunitsforneuralnetworkfunction
approximationinreinforcementlearning. Neuralnetworks,2018.
[13] PatrickEsser,RobinRombach,andBjornOmmer. TamingTransformersforhigh-resolutionimagesyn-
thesis. InCVPR,2021.
[14] ShanghuaGao,PanZhou,Ming-MingCheng,andShuichengYan. MaskeddiffusionTransformerisa
strongimagesynthesizer. InICCV,2023.
[15] IanJGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,Aaron
Courville,andYoshuaBengio. Generativeadversarialnets. InNeurIPS,2014.
[16] PriyaGoyal,PiotrDolla´r,RossGirshick,PieterNoordhuis,LukaszWesolowski,AapoKyrola,Andrew
Tulloch,YangqingJia,andKaimingHe. Accurate,largeminibatchSGD:TrainingImageNetin1hour.
arXiv:1706.02677,2017.
[17] KarolGregor,IvoDanihelka,AndriyMnih,CharlesBlundell,andDaanWierstra. Deepautoregressive
networks. InICML,2014.
[18] Emil Julius Gumbel. Statistical theory of extreme valuse and some practical applications. Nat. Bur.
StandardsAppl.Math.Ser.33,1954.
[19] Ali Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash Vahdat. DiffiT: Diffusion vision
Transformersforimagegeneration. arXiv:2312.02139,2023.
[20] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.Deepresiduallearningforimagerecognition.
InCVPR,2016.
[21] KaimingHe, XinleiChen, SainingXie, YanghaoLi, PiotrDolla´r, andRossGirshick. Maskedautoen-
codersarescalablevisionlearners. InCVPR,2022.
11[22] MartinHeusel,HubertRamsauer,ThomasUnterthiner,BernhardNessler,andSeppHochreiter. GANs
trainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. InNIP,2017.
[23] JonathanHoandTimSalimans. Classifier-freediffusionguidance. arXiv:2207.12598,2022.
[24] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. InNeurIPS,2020.
[25] TeroKarras,MiikaAittala,JaakkoLehtinen,JanneHellsten,TimoAila,andSamuliLaine. Analyzing
andimprovingthetrainingdynamicsofdiffusionmodels. arXiv:2312.02696,2023.
[26] Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the ELBO with simple data
augmentation. InNeurIPS,2023.
[27] Alexander Kolesnikov, Andre´ Susano Pinto, Lucas Beyer, Xiaohua Zhai, Jeremiah Harmsen, and Neil
Houlsby. UViM:Aunifiedmodelingapproachforvisionwithlearnedguidingcodes. NeurIPS,2022.
[28] IvanKrasin,TomDuerig,NeilAlldrin,VittorioFerrari,SamiAbu-El-Haija,AlinaKuznetsova,Hassan
Rom,JasperUijlings,StefanPopov,AndreasVeit,SergeBelongie,VictorGomes,AbhinavGupta,Chen
Sun,GalChechik,DavidCai,ZheyunFeng,DhyaneshNarayanan,andKevinMurphy. Openimages: A
publicdatasetforlarge-scalemulti-labelandmulti-classimageclassification. 2017.
[29] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. MAGE:
Maskedgenerativeencodertounifyrepresentationlearningandimagesynthesis. InCVPR,2023.
[30] Yazhe Li, Jorg Bornschein, and Ting Chen. Denoising autoregressive representation learning. arXiv
preprintarXiv:2403.05196,2024.
[31] IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. InICLR,2019.
[32] FabianMentzer,DavidMinnen,EirikurAgustsson,andMichaelTschannen. Finitescalarquantization:
VQ-VAEmadesimple. InICLR,2024.
[33] AlexanderQuinnNicholandPrafullaDhariwal. Improveddenoisingdiffusionprobabilisticmodels. In
ICML,2021.
[34] OctoModelTeam,DibyaGhosh,HomerWalke,KarlPertsch,KevinBlack,OierMees,SudeepDasari,
JoeyHejna,CharlesXu,JianlanLuo,TobiasKreiman,YouLiangTan,PannagSanketi,QuanVuong,Ted
Xiao,DorsaSadigh,ChelseaFinn,andSergeyLevine. Octo: Anopen-sourcegeneralistrobotpolicy. In
RSS,2024.
[35] OpenAI. ConsistencyDecoder,2024. URLhttps://github.com/openai/consistencydecoder.
[36] NikiParmar,AshishVaswani,JakobUszkoreit,LukaszKaiser,NoamShazeer,AlexanderKu,andDustin
Tran. ImageTransformer. InICML,2018.
[37] WilliamPeeblesandSainingXie. ScalablediffusionmodelswithTransformers. InICCV,2023.
[38] AlecRadford,KarthikNarasimhan,TimSalimans,andIlyaSutskever.Improvinglanguageunderstanding
bygenerativepre-training. 2018.
[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
modelsareunsupervisedmultitasklearners. 2019.
[40] AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,MarkChen,and
IlyaSutskever. Zero-shottext-to-imagegeneration. InICML,2021.
[41] AliRazavi,AaronVandenOord,andOriolVinyals. Generatingdiversehigh-fidelityimageswithVQ-
VAE-2. InNeurIPS,2019.
[42] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjo¨rnOmmer.High-resolution
imagesynthesiswithlatentdiffusionmodels. InCVPR,2022.
[43] TimSalimans,IanGoodfellow,WojciechZaremba,VickiCheung,AlecRadford,andXiChen.Improved
techniquesfortrainingGANs. InNeurIPS,2016.
[44] NoamShazeer. FastTransformerdecoding:Onewrite-headisallyouneed. arXiv:1911.02150,2019.
[45] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli.Deepunsupervisedlearn-
ingusingnonequilibriumthermodynamics. InICML,2015.
12[46] YangSongandStefanoErmon. Generativemodelingbyestimatinggradientsofthedatadistribution. In
NeurIPS,2019.
[47] YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBenPoole.
Score-basedgenerativemodelingthroughstochasticdifferentialequations. InICLR,2021.
[48] MichaelTschannen,CianEastwood,andFabianMentzer. GIVT:Generativeinfinite-vocabularyTrans-
formers. arXiv:2312.02116,2023.
[49] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, and Koray
Kavukcuoglu. ConditionalimagegenerationwithPixelCNNdecoders. InNeurIPS,2016.
[50] AaronvandenOord, NalKalchbrenner, andKorayKavukcuoglu. Pixelrecurrentneuralnetworks. In
ICML,2016.
[51] AaronvandenOord,OriolVinyals,andKorayKavukcuoglu. Neuraldiscreterepresentationlearning. In
NeurIPS,2017.
[52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. InNeurIPS,2017.
[53] ChenWei,KarttikeyaMangalam,Po-YaoHuang,YanghaoLi,HaoqiFan,HuXu,HuiyuWang,Cihang
Xie,AlanYuille,andChristophFeichtenhofer.Diffusionmodelsasmaskedautoencoders.InICCV,2023.
[54] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose´ Lezama, Han Zhang, Huiwen Chang, Alexander G Haupt-
mann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, and Lu Jiang. MAGVIT: Masked generative video
Transformer. InCVPR,2023.
[55] LijunYu,Jose´Lezama,NiteshBGundavarapu,LucaVersari,KihyukSohn,DavidMinnen,YongCheng,
Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, Boqing Gong, Ming-Hsuan Yang, David A. Ross
IrfanEssa,andLuJiang.Languagemodelbeatsdiffusion–tokenizeriskeytovisualgeneration.InICLR,
2024.
[56] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunreasonableeffec-
tivenessofdeepfeaturesasaperceptualmetric. InCVPR,2018.
13Ours DiT Ours DiT Ours DiT
Figure8:Failurecases.Similartoexistingmethods,oursystemcanproduceresultswithnoticeable
artifacts. For each pair, we show MAR-H and DiT-XL’s results of the same class. The leftmost
exampleofDiTistakenfromtheirpaper[37];theothersareobtainedfromtheirofficialcode.
A LimitationsandBroaderImpacts
Limitations. Beyond demonstrating the potential of our method for image generation, this paper
acknowledgesitslimitations.
First of all, our image generation system can produce images with noticeable artifacts (Figure 8).
This limitation is commonly observed in existing methods, especially when trained on controlled,
academicdata(e.g.,ImageNet).Research-drivenmodelstrainedonImageNetstillhaveanoticeable
gapinvisualqualityincomparisonwithcommercialmodelstrainedonmassivedata.
Second, our image generation system relies on existing pre-trained tokenizers. The quality of our
systemcanbelimitedbythequalityofthesetokenizers. Pre-trainingbettertokenizersisbeyondthe
scope of this paper. Nevertheless, we hope our work will make it easier to use continuous-valued
tokenizerstobedevelopedinthefuture.
Last,wenotethatgiventhelimitedcomputationalresources,wehaveprimarilytestedourmethod
ontheImageNetbenchmark. Furthervalidationisneededtoassessthescalabilityandrobustnessof
ourapproachinmorediverseandreal-worldscenarios.
BroaderImpacts. Ourprimaryaimistoadvancethefundamentalresearchongenerativemodels,
andwebelieveitwillbebeneficialtothisfield.Animmediateapplicationofourmethodistoextend
it to large visual generation models, e.g., text-to-image or text-to-video generation. Our approach
has the potential to significantly reduce the training and inference cost of these large models. At
the same time, our method may suggest the opportunity to replace traditional loss functions with
Diffusion Loss in many applications. On the negative side, our method learns statistics from the
training dataset, and as such may reflectthe bias in the data; the image generation system may be
misusedtogeneratedisinformation,whichwarrantsfurtherconsideration.
B AdditionalImplementationDetails
Classifier-free guidance (CFG). To support CFG [23], at training time, the class condition is re-
placedwithadummyclasstokenfor10%ofthesamples[23]. Atinferencetime,themodelisrun
with the given class token and the dummy token, providing two outputs z and z . The predicted
c u
noise ε is then modified [23] as: ε = ε (x t,z )+ω (ε (x t,z ) ε (x t,z )), where ω is
θ t u θ t c θ t u
| · | − |
theguidancescale. Atinferencetime,weuseaCFGschedulefollowing[5]. Wesweeptheoptimal
guidancescaleandtemperaturecombinationforeachmodel.
Training. By default, the models are trained using the AdamW optimizer [31] for 400 epochs.
TheweightdecayandmomentaforAdamWare0.02and(0.9,0.95). Weuseabatchsizeof2048
and a learning rate (lr) of 8e-4. Our models with Diffusion Loss are trained with a 100-epoch
linearlrwarmup[16],followedbyaconstant [37]lrschedule. Thecross-entropycounterpartsare
trainedwithacosinelrschedule,whichworksbetterforthem. Following[37,25],wemaintainthe
exponentialmovingaverage(EMA)ofthemodelparameterswithamomentumof0.9999.
Implementation Details of Table 4. To explore our method’s scaling behavior, we study three
modelsizesdescribedasfollows. InadditiontoMAR-L,weexploreasmallermodel(MAR-B)and
alargermodel(MAR-H).MAR-B,-L,and-Hrespectivelyhave24,32,40Transformerblocksand
a width of 768, 1024, and 1280. In Table 4 specifically, the denoising MLP respectively has 6, 8,
12blocksandawidthof1024,1280,and1536. Thetraininglengthisincreasedto800epochs. At
inferencetime,werun256autoregressivestepstoachievethebestresults.
Pseudo-codeofDiffusionLoss. SeeAlgorithm1.
14Algorithm1DiffusionLoss: PyTorch-likePseudo-code
class DiffusionLoss(nn.Module)
def __init__(depth, width):
# SimpleMLP takes in x_t, timestep, and condition, and outputs predicted noise.
self.net = SimpleMLP(depth, width)
# GaussianDiffusion offers forward and backward functions q_sample and p_sample.
self.diffusion = GaussianDiffusion()
# Given condition z and ground truth token x, compute loss
def loss(self, z, x):
# sample random noise and timestep
noise = torch.randn(x.shape)
timestep = torch.randint(0, self.diffusion.num_timesteps, x.size(0))
# sample x_t from x
x_t = self.diffusion.q_sample(x, timestep, noise)
# predict noise from x_t
noise_pred = self.net(x_t, timestep, z)
# L2 loss
loss = ((noise_pred - noise) ** 2).mean()
# optional: loss += loss_vlb
return loss
# Given condition and noise, sample x using reverse diffusion process
def sample(self, z, noise):
x = noise
for t in list(range(self.diffusion.num_timesteps))[::-1]:
x = self.diffusion.p_sample(self.net, x, t, z)
return x
Pseudo-codeillustratingtheconceptofDiffusionLoss. Heretheconditioningvectorz istheoutputfrom
theAR/MARmodel. Thegradientisbackpropagatedtoz. Forsimplicity,hereweomitthecodeforinference
rescheduling,temperatureandthelosstermforvariationallowerbound[10],whichcanbeeasilyincorporated.
ComputeResources. Ourtrainingismainlydoneon16serverswith8V100GPUseach. Training
a400epochsMAR-Lmodeltakes 2.6daysontheseGPUs. Asacomparison,trainingaDiT-XL/2
∼
andLDM-4modelforthesamenumberofepochsonthisclustertakes4.6and9.5days,respectively.
C ComparisonbetweenMARandMAGE
MAR(regardlessofthelossused)isconceptuallyrelatedtoMAGE[29]. Besidesimplementation
differences (e.g., architecture specifics, hyper-parameters), a major conceptual difference between
MARandMAGEisinthescanningorderatinferencetime. InMAGE,followingMaskGIT[4],the
locations of the next tokens to be predicted are determined on-the-fly by the sample confidence at
eachlocation, i.e., themoreconfidentlocationsaremorelikelytobeselectedateachstep[4,29].
Incontrast,MARadoptsafullyrandomized order,anditstemperaturesamplingisappliedtoeach
token. Table 5 compares this difference in controlled settings. The first line is our MAR imple-
mentationbutusingMAGE’son-the-flyorderingstrategy, whichhassimilarresultsasthesimpler
randomordercounterpart. Fullyrandomizedorderingcanmakethetrainingandinferenceprocess
consistent regarding the distribution of orders; it also allows us to adopt token-wise temperature
samplinginawaysimilartoautoregressivelanguagemodels(e.g.,GPT[38,39,3]).
order loss FID↓ IS↑
MAR,ourimpl. on-the-fly CrossEnt 8.72 145.6
MAR,ourimpl. random CrossEnt 8.79 146.1
MAR,ourimpl. random DiffLoss 3.50 201.4
Table5: TocompareconceptuallywithMAGE,werunMAR’sinferenceusingtheMAGEstrategy
ofdeterminingtheorderontheflybyconfidencesamplingacrossthespatialdomain. Theseentries
areallbasedonthetokenizersprovidedbytheLDMcodebase[42].
15Table 6: System-level comparison on ImageNet 512 512 conditional generation. MAR’s CFG
×
scaleissetto4.0;othersettingsfollowtheMAR-LconfigurationdescribedinTable4.
w/oCFG w/CFG
#params FID↓ IS↑ FID↓ IS↑
pixel-based
ADM[10] 554M 23.24 58.1 7.72 172.7
VDM++[26] 2B 2.99 232.2 2.65 278.1
vector-quantizedtokens
MaskGIT[4] 227M 7.32 156.0 - -
MAGVIT-v2[55] 307M 3.07 213.1 1.91 324.3
continuous-valuedtokens
U-ViT-H/2-G[2] 501M - - 4.05 263.8
DiT-XL/2[37] 675M 12.03 105.3 3.04 240.8
DiffiT[19] - - - 2.67 252.1
GIVT[48] 304M 8.35 - - -
EDM2-XXL[25] 1.5B 1.91 - 1.81 -
MAR-L,DiffLoss 481M 2.74 205.2 1.73 279.9
D AdditionalComparisons
D.1 ImageNet512 512
×
Followingpreviousworks,wealsoreportresultsonImageNetataresolutionof512 512,compared
×
withleadingsystems(Table6). Forsimplicity,weusetheKL-16tokenizer,whichgivesasequence
length of 32 32 on a 512 512 image. Other settings follow the MAR-L configuration described
× ×
inTable4. OurmethodachievesanFIDof2.74withoutCFGand1.73withCFG.Ourresultsare
competitivewiththoseofprevioussystems. Duetolimitedresources,wehavenottrainedthelarger
MAR-HonImageNet512 512,whichisexpectedtohavebetterresults.
×
D.2 L2Lossvs. DiffLoss
Ana¨ıvebaselineforcontinuous-valuedtokensistocomputetheMeanSquaredError(MSE,i.e.,L2)
lossdirectlybetweenthepredictionsandthetargettokens. Inthecaseofaraster-orderARmodel,
usingtheL2lossintroducesnorandomnessandthuscannotgeneratediversesamples. Inthecase
oftheMARmodelswiththeL2loss,theonlyrandomnessisthesequenceorder; thepredictionat
alocationisdeterministicforanygivenorder. Inourexperiment,wehavetrainedanMARmodel
withtheL2loss,whichasexpectedleadstoadisastrousFIDscore(>100).
Acknowledgements. WethankCongyueDengandXinleiChenforhelpfuldiscussion. Wethank
GoogleTPUResearchCloud(TRC)forgrantingusaccesstoTPUs,andGoogleCloudPlatformfor
supportingGPUresources.
16