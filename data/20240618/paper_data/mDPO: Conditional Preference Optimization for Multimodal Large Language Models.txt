MDPO: Conditional Preference Optimization for Multimodal
Large Language Models
FeiWang1 WenxuanZhou1 JamesY.Huang1 NanXu1
ShengZhang3 HoifungPoon3 MuhaoChen2
1UniversityofSouthernCalifornia 2UniversityofCalifornia,Davis
3MicrosoftResearch
{fwang598,zhouwenx,huangjam,nanx}@usc.edu {shezhan,hoifung}@microsoft.com muhchen@ucdavis.edu
Abstract
Direct preference optimization (DPO) has
shown to be an effective method for large
language model (LLM) alignment. Recent
works have attempted to apply DPO to mul-
timodalscenariosbuthavefounditchallenging
toachieve consistentimprovement. Through
acomparativeexperiment,weidentifytheun-
conditionalpreferenceprobleminmultimodal Figure1: WetrainBunny-v1.0-3B(Heetal.,2024)on
preferenceoptimization,wherethemodelover- 10KmultimodalpreferencedatafromSilkie(Lietal.,
lookstheimagecondition.Toaddressthisprob- 2023)withdifferentvariantsofDPO.WeperformDPO
lem, we propose MDPO, a multimodal DPO (No Image) where all images are removed from the
objectivethatpreventstheover-prioritizationof preference data. Counterintuitively, the overall score
language-onlypreferencesbyalsooptimizing ontheMMHalBench(Sunetal.,2023)forDPO(No
image preference. Moreover, we introduce a Image)issimilartothatofDPOwithimages. Thisfind-
rewardanchorthatforcestherewardtobeposi- ingsuggeststhatDPOmaysufferfromunconditional
tiveforchosenresponses,therebyavoidingthe preferences,neglectingthevisualmodalityduringop-
decreaseintheirlikelihood—anintrinsicprob- timization. Ourproposedmethod,MDPO,effectively
lemofrelativepreferenceoptimization. Exper- addressesthisissueandimprovesmodelperformance.
imentsontwomultimodalLLMsofdifferent
sizesandthreewidelyusedbenchmarksdemon-
stratethatMDPOeffectivelyaddressestheun-
enhanced multimodal preference data (Li et al.,
conditionalpreferenceprobleminmultimodal
2023; Zhao et al., 2023; Xiao et al., 2024; Zhou
preference optimization and significantly im-
et al., 2024; Pi et al., 2024; Sarkar et al., 2024;
provesmodelperformance,particularlyinre-
ducinghallucination. Yu et al., 2024b; Deng et al., 2024), we investi-
gatethepitfallsofmultimodalDPOfromadiffer-
1 Introduction
entperspective. Throughcontrolledcomparisons,
Direct preference optimization (DPO) (Rafailov we discover that multimodal LLMs can achieve
et al., 2023) has emerged as the predominating similarperformanceevenwhenallimagesarere-
methodforaligninglargelanguagemodels(LLMs) movedfromthemultimodalpreferencedataduring
with human preferences (Rafailov et al., 2023). DPO (see Fig. 1). This counterintuitive finding
Building on its success in the language modality, suggeststhatthefailureofDPOinmultimodalsce-
recentstudieshaveextendedDPOtomultimodal nariosmaynotbesolelyattributedtodataquality.
scenarios (Li et al., 2023; Yu et al., 2024a; Zhou Weattributethis toa systematicgap between the
et al., 2024; Zhao et al., 2023). However, trans- theoreticalexpectationsandpracticalimplementa-
ferring this approach across modalities presents tionsoftheDPOobjectiveinmultimodalsettings
significantchallenges. Merelysubstitutingtextual (refertoFig.2). WhileDPOaimstocomputeim-
preference data with multimodal preference data plicitrewardsconditionedonallinputmodalities,it
doesnotconsistentlyyieldpositiveoutcomesand mayprioritizelanguage-onlypreferencesandover-
can exacerbate issues such as hallucinations (Li looktheimagecondition(i.e.,unconditionalpref-
etal.,2023;Sarkaretal.,2024). erence),leadingtosuboptimalmodelperformance
While recent efforts in multimodal preference andincreasedhallucination. AfterapplyingDPO,
learningfocusonimprovingperformancethrough themodelmayshowanincreasedtendencytoig-
4202
nuJ
71
]VC.sc[
1v93811.6042:viXraFigure2: OverviewofMDPO. TopLeft: StandardDPOexpectsthemultimodalLLMtolearnresponsepreferences
conditioned on both the image and the question. Top Right: However, in practice, the learning process often
disregardstheimagecondition. Bottom: Toaddressthisissue,MDPOintroducesanadditionalimagepreference
learning objective to emphasize the relationship between the image and the response. Furthermore, MDPO
incorporatesarewardanchortoensurethattheprobabilityofthechosenresponsedoesnotdecrease.
nore the provided image and generate responses consistentlydemonstratethatMDPOoutperforms
basedsolelyonthequestion(illustratedinFig.3). standardDPOinmultimodalscenarios,effectively
reducinghallucinationsacrossvaryingmodeland
Inthispaper,weproposeMDPO,amultimodal
data scales. Detailed analyses reveal that con-
DPOobjectivethatutilizesconditionalpreference
ditional preference plays a crucial role in en-
optimizationonimagestopreventtheoverlypriori-
hancingtheeffectivenessofDPOformultimodal
tizationoflanguage-onlypreferences. Asdepicted
LLMs. Fine-grained and qualitative studies fur-
inFig.2,inadditiontotheoriginalpreferencepairs
contrastingresponses, MDPO introducesnewpref-
ther illustrate that MDPO significantly improves
themodel’sabilitytocomprehendimagesandmit-
erencepairscontrastingimages. Therejectedim-
igateslanguagebiasesinmodelresponses.
ageisderivedfromtheoriginal(i.e.,chosen)image
byreducingeffectivevisualinformation. Thisap- Ourcontributionsarethree-fold. First,weiden-
proach,combinedwiththestandardDPOobjective, tify unconditional preference towards the visual
encouragesthemultimodalLLMtosimultaneously modalityasaprimaryreasonforthepitfallsofDPO
emphasizebothvisualandlanguagefeatures. Fur- inmultimodalLLMs. Second,weproposeMDPO,
thermore,weobservethatDPOoftenexperiences amultimodalDPOobjectivethatincorporatescon-
a decrease in the likelihood of chosen responses ditionalpreferenceoptimizationandanchoredpref-
inmultimodalscenariosdespiteanincreaseinthe erenceoptimizationtomitigatethesepitfalls. Third,
implicitreward. Toaddressthis, MDPO incorpo- we verify the effectiveness of MDPO across dif-
ratesarewardanchorthatmaintainsthelikelihood ferent model and data scales. Using MDPO, we
ofchosenresponsesbyregularizingtherewardto achievethebest-performing3BmultimodalLLM
bepositive. intermsofreducinghallucinations.
TovalidatetheeffectivenessofMDPO,wecon-
ductexperimentsusingBunny-v1.0-3B(Heetal., 2 ThePitfallofPreferenceOptimization
2024) and LLaVA-v1.5-7B (Liu et al., 2024a).
BothautomaticandhumanevaluationsonMMHal- Inthissection,wefirstintroducethebackground
bench (Sun et al., 2023), Object HalBench (Yu ofDPO(§2.1)andthendelveintotheissueofun-
et al., 2024a), and AMBER (Wang et al., 2023) conditionalpreferenceinmultimodalDPO(§2.2).Figure3: QualitativeResultsfromMMHalBench. Top: WhentrainedwithstandardDPO,Bunnyoftenassumes
theimagedescriptioninthequestioniscorrect,respondingaccordingly,evenifthequestioncontainsanadversarial
premiseregardingtheimage. Incontrast, MDPO identifiesthefalsepremiseinthequestionbyreferencingthe
image. Bottom: BunnytrainedwithstandardDPOmaydisregardtheimageandprovideaneducatedguessforthe
answer. Conversely,MDPOdeliversacorrectanswerthatisconditionedontheimage.
2.1 Background: PreferenceOptimization whichisessentiallymaximizing
PreferenceoptimizationseekstoalignLLMswith
σ(r(q,y )−r(q,y)).
w l
humanpreferences,therebyenhancingtheircapa-
Inthemultimodalscenario,eachinstanceinthe
bilitiestorespondtohumanneeds. Inthecontext
preferencedatacontainsanimagem,inaddition
ofLLMs,itaimstoencouragethemodeltolearn
toq,y ,andy ,andthepreferencelabelisdecided
that, for a given question q, the response y cho- w l
w
based on both the image and the question. DPO
senbytheevaluatorispreferredovertherejected
expectsthemultimodalLLMtolearntomaximize
one y . DPO is the predominant method for this
l
purpose. DerivedfromrewardmodelinginRLHF
σ(r(m,q,y )−r(m,q,y)),
w l
(Ouyangetal.,2022),itseekstomaximizethedif-
ferencebetweentherewardforthechosenresponse andtheobjectivebecomes:
r(q,y )andthatfortherejectedresponser(q,y ).
w l (cid:16) (cid:17)
Specifically,givenamodeltobeoptimizedπ θ and L DPOm =−logσ βlog ππ rθ ef( (y yw w| |m m, ,q q) ) −βlog ππ rθ ef( (y yl l| |m m, ,q q) ) .
areferencemodelπ ,whichistypicallyinitialized
ref
2.2 Problem: UnconditionalPreference
fromasupervisedfinetuningmodel,DPOformu-
latestherewardasfollows: Recent studies have found inconsistent improve-
ments in model capabilities when applying DPO
π (y|q)
r(q,y)=βlog θ +Z(q), to multimodal LLMs, often attributing this issue
π (y|q)
ref
to the quality of preference data (Li et al., 2023;
where Z(q) is a partition function, β is a hyper- Sarkar et al., 2024). However, our controlled ex-
parameterthatcontrolsthedeviationfromtheref- perimentssuggestthattheproblemarisesbecause
erence model. Then, based on the Bradley-Terry DPOdoesnoteffectivelyutilizethevisualmodal-
model (Bradley and Terry, 1952), the preference ity in the preference dataset. To explore this, we
optimizationobjectivebecomes: introduceavariantcalledDPO(NoImage),which
istrainedonthepreferencedatasetwiththevisual
(cid:16) (cid:17)
L DPO =−logσ βlog ππ rθ ef( (y yw w| |q q) ) −βlog ππ rθ ef( (y yl l| |q q) ) , signal removed, forcing the model to maximizeσ(r(q,y )−r(q,y ))withoutvisualcues. Weap- multimodal LLM captures preferences based on
w l
plybothDPOandDPO(NoImage)totheBunny- bothvisualandlanguagecues. Notably,whilewe
v1.0-3Bmodeland10Kpreferenceinstancesfrom focus on the multimodal setting, this conditional
the LLaVA-Instruct (Liu et al., 2024b) subset of preferenceoptimizationcouldbebeneficialtoother
Silkie (Li et al., 2023). Results shown in Fig. 1 preferenceoptimizationscenariosinvolvingmulti-
indicatethatDPO(NoImage)performssimilarly, pleinputcomponents. Insuchsettings,DPOmay
orevenslightlybetter,thanDPOonMMHalBench. alsoignorespecificinputcomponentsandencour-
Thisfindingunderscorestheissueofunconditional agethemodeltolearnunconditionalpreferences.
preference learned by multimodal LLMs during
3.2 AnchoredPreferenceOptimization
DPO,wherethemodelmaydisregardimageinfor-
mation,asillustratedinFig.2. Wealsoobservethatthelikelihoodofthechosen
responseoftendecreasesduringtheoptimization
3 MDPO
processofDPO.Thisoccursbecausethestandard
Inthissection,weintroduceMDPO,animproved DPOobjectiveonlyencouragesthemodeltolearn
DPOapproacheddedicatedtomultimodalprefer- a relative preference. Without further regulariza-
encealignment. AsdepictedinFig.2,MDPOintro- tion, the model may reduce the likelihood of the
ducestwoadditionalpreferenceoptimizationobjec- chosenresponsetoenlargethelikelihoodgapbe-
tivestoDPO:conditionalpreferenceoptimization tweenthechosenandrejectedresponses. Thiscan
toaddresstheissueofignoringvisualinformation harmmodelperformance,asthechosenresponses
(see§3.1),andanchoredpreferenceoptimizationto areoftenofhighquality. Toaddressthisproblem,
preventadecreaseinthelikelihoodofthechosen we add an anchor to the preference optimization,
response(see§3.2). forcing the reward of the chosen response to be
higherthanaspecificvalue: σ(r(m ,q,y )−δ)
w w
3.1 ConditionalPreferenceOptimization with δ as the anchor value. The corresponding
Weproposeaconditionalpreferenceoptimization objectiveis
objective to address the issue of ignoring visual
(cid:16) π (y |m ,q) (cid:17)
information in preference data. The core idea is L =−logσ βlog θ w w −δ .
AncPO π (y |m ,q)
ref w w
to construct preference data where the image is
theonlyvariable,forcingthemodeltodetermine In this way, we introduce absolute reward regu-
the preference label based on visual information. larization to the preference optimization process,
Specifically,givenapairoftuples(m ,q,y )and effectivelyavoidingthelikelihooddecreaseofthe
w w
(m ,q,y ), where m is more compitible with q chosenresponse. Theanchorisdecidedbasedon
l w w
and y than m , the conditional preference opti- the data properties and expected model behavior.
w l
mizationobjectiveisformulatedas: While we keep it simple in the default setting of
MDPO,onecanalwayschangetheanchorvalues
(cid:16) (cid:17)
L CoPO=−logσ βlog ππ rθ ef( (y yw w| |m mw w, ,q q)
)
−βlog ππ rθ ef( (y yw w| |m ml l, ,q q)
)
. andsetmultipleanchorsfordifferentpurposes. The
anchorcanalsobeaddedtotherejectedresponse
Thechallengethenliesinconstructingappropriate
intheoppositedirection, forcingitsrewardtobe
pairsofm andm . Ontheonehand,m should
w l l lower than a specific value. We compare other
contain different visual information from m to
w anchorsin§4.4.
make it less compatible. On the other hand, it
Theobjectiveof MDPO isacombinationofthe
shouldalsosharesomecommonfeatureswithm
w standardDPO,conditionalpreferenceoptimization,
toserveasahardnegative. Wefindthatastraight-
andanchoredpreferenceoptimization:
forwardstrategy,usingtheoriginalimageasmand
creating m l by randomly cropping less than 20% L MDPO =L DPOm +L CoPO+L AncPO.
oftheoriginalimage,yieldsthebestperformance,
asshownin§4.4.
Tosummarize,thestandardDPOobjectivemax-
4 Experiment
imizesσ(r(m ,q,y )−r(m ,q,y )),whilethe
w w w l
conditionalpreferenceoptimizationobjectivemax- In this section, we begin with the experimental
imizes σ(r(m ,q,y )−r(m ,q,y )). The two setup(§4.1). Thenwepresentthemainresultson
w w l w
objectivesworkincollaborationtoensurethatthe threebenchmarks(§4.2)andthehumanevaluationMMHalBench ObjectHalBench AMBER
Score↑ HalRate↓ CHAIR ↓ CHAIR ↓ CHAIR ↓ Cover.↑ HalRate↓ Cog.↓
s i s
ReferencedResults(NotDirectlyComparable)
GPT-4V(Achiametal.,2023)†♯ 3.49 0.28 13.6 7.3 4.6 67.1 30.7 2.6
LLaVA-v1.5-7B(Liuetal.,2024a)‡♯ 2.11 0.54 53.6 25.2 7.8 51.0 36.4 4.2
+HACL(Jiangetal.,2024)‡ 2.13 0.50 - - - - - -
+POVID(Zhouetal.,2024)♯ 2.08 0.56 48.1 24.4 - - - -
+OPERA(Huangetal.,2024)♯ 2.15 0.54 45.1 22.3 - - - -
+VCD(Lengetal.,2024)♯ 2.12 0.54 48.8 24.3 - - - -
+EOS(Yueetal.,2024)‡♯ 2.03 0.59 40.3 17.8 5.1 49.1 22.7 2.0
+HA-DPO(Zhaoetal.,2023)‡♯ 1.97 0.60 39.9 19.9 6.7 49.8 30.9 3.3
+HALVA(Sarkaretal.,2024)‡ 2.25 0.54 - - 6.6 53.0 32.2 3.4
LLaVA-v1.5-13B(Liuetal.,2024a)† 2.42 - 46.3 22.6 7.8 51.0 36.4 4.2
+RLHF-V(Yuetal.,2024a)† 2.81 0.49 12.2 7.5 6.3 46.1 25.1 2.1
+HSA-DPO(Xiaoetal.,2024)† 2.61 0.48 5.2 3.2 2.1 47.3 13.4 1.2
+HALVA(Sarkaretal.,2024)‡ 2.58 0.45 - - 6.4 52.6 30.4 3.2
Qwen-VL-Chat(Baietal.,2023)† 2.89 0.43 36.0 21.3 6.6 53.2 31.0 2.9
+Silkie-80K(Lietal.,2023)† 3.01 0.41 25.3 13.9 5.4 55.8 29.0 2.0
3BMultimodalLLMs
Bunny-v1.0-3B(Heetal.,2024) 2.11 0.58 43.0 8.9 9.8 75.6 64.9 6.0
+DPO 2.28 0.56 44.3 7.6 7.9 74.1 58.9 4.8
+MDPO 2.96 0.42 27.0 4.6 4.9 67.4 37.7 2.4
7BMultimodalLLMs
LLaVA-v1.5-7B(Liuetal.,2024a) 2.19 0.57 54.7 15.9 7.4 51.8 34.7 4.1
+DPO 2.14 0.65 49.0 13.0 6.5 55.1 34.5 2.3
+MDPO 2.39 0.54 35.7 9.8 4.4 52.4 24.5 2.4
Table 1: Main results of Bunny-v1.0-3B and LLaVA-v1.5-7B trained with different preference optimization
objectives. We report overall score and hallucination rate (HalRate) on MMHalBench, CHAIR scores at both
responseandobjectlevelsonObjectHalBench,alongwithCHAIRscores,objectcoverage(cover.),hallucination
rate(HalRate),andcognition(Cog.) onAMBER.Thebestresultforeachmetricineachgroupisinbold. For
reference, we also provide additional results using various multimodal LLMs, preference data, and learning
objectives,althoughthesearenotdirectlycomparable. Resultsfromcontemporaryworkfocusingonmultimodal
preferencedata: †Xiaoetal.(2024),‡Sarkaretal.(2024),and♯Yuetal.(2024b).
(§4.3)ofMDPO. Wefurtherprovidein-depthanal- LLaVA-Instruct-150K(Liuetal.,2024a)fortrain-
ysis(§4.4)andfine-grainedresults(§4.5). Finally, ing. TheoriginaSilkiedatasetcontains80Kprefer-
weconductaqualitativestudy(§4.6). encedatacollectedon12multimodalLLMs. While
theoriginalSilkiepaperexplorestheeffectofex-
4.1 ExperimentalSetup treme data size, we follow the majority of prior
worksusingaround10Kdataforpreferenceopti-
Models. We apply MDPO on two multimodal mization(Sunetal.,2023;Zhaoetal.,2023).
LLMsindifferentsizes. Bunny-v1.0-3B(Heetal.,
EvaluationBenchmarks. Weevaluatetheperfor-
2024) is a 3B model building upon SigLIP (Zhai
manceofMDPOonthreewidelyusedbenchmarks
etal.,2023)andPhi-2(Javaheripietal.,2023). It
formultimodalLLMswithaspecialfocusonhal-
ispretrainedon2Mimage-textpairsandfinetuned
lucination. MMHalBench (Sun et al., 2023) is a
on1.3Minstructiontuningdata. LLAVA-v1.5-7B
practical question answering benchmark contain-
(Liu et al., 2024a) is a 7B model based on CLIP
ing eight question categories and 12 object top-
(Radfordetal.,2021)andVincuna(Chiangetal.,
ics. Following the official setting, we use GPT4
2023). It is pretrained on 558K image-text pairs
(Achiametal.,2023)toassesstheoverallquality
andfinetunedon665Kinstructiontuningdata.
ofresponseswithascorebetwenzeroandsix,and
PreferenceData. Wesample10Kpreferencedata thehallucinationrate. ObjectHalBench(Rohrbach
fromSilkie(Lietal.,2023)withinstructionsfrom etal.,2018)isawidelyadoptedbenchmarktoas-Figure4: HumanevaluationonMMHalBench.
sessobjecthallucination. Wefollowthesettingof
Yuetal.(2024a)toaugmentthebenchmarkwith
eight diverse prompts and evaluating on 300 in-
stances. Wereport the CHAIR scores (Rohrbach
Figure 5: Impact of data scale on the performance
etal.,2018)assessinghallucinationrateofresponse
ofstandardDPOandMDPO,usingBunnyasthebase
level(CHAIR )andobjectlevel(CHAIR ). AM-
s i model. We assess theoverall scoreand hallucination
BER (Wang et al., 2023) is a multimodal LLM
rateonMMHallBench. MDPOiseffectiveacrossdif-
hallucinationbenchmarkwithfine-grainedobject ferentscales,whereasstandardDPOdoesnotexhibita
annotation. We focus on the generative task con- scalingeffectinmultimodalscenarios.
sistingof1Kimages. Usingtheofficialevaluation
tool, we report a variant of CHAIR score, object
DPOforBunnyandLLaVA.Notably, MDPO en-
coverage,rateofhallucinatedresponses,andhallu-
hancesthe3Bmodel(Bunny)with10Kpreference
cinationrateoverlappingwithhumancognition.
datatobecomparabletoastronger7Bbasemodel
Baselines. We primarily compare MDPO with (Qwen-VL-Chat) trained with DPO on 80K data.
standardDPO.ThestandardDPObaselineshares Theformerpreferencedataisonlyasubsetofthe
the same training process, data, and hyper- latter. Thisresulthighlightsthataproperobjective
parameters, despite different learning objectives. can be more important than data scale and diver-
Wefurtherprovidetheresultsofothermultimodal sityinmultimodalpreferenceoptimization. More-
LLMsforreference,althoughtheyarenotdirectly over, MDPO is specifically effective in reducing
comparable due to different base models, prefer- hallucination, which aligns with the objective of
ence data, and alignment methods. This group conditionalpreferenceoptimization. WhileMDPO
contains GPT-4V (Achiam et al., 2023), LLaVA- mayleadtoadecreaseinobjectcoverage,thisde-
v1.5-13B(Liuetal.,2024a),Qwen-VL-Chat(Bai creaseisminorgiventhesignificantimprovement
et al., 2023), POVID (Zhou et al., 2024), HACL inoverallqualityandreductioninhallucination.
(Jiangetal.,2024),OPERA(Huangetal.,2024),
VCD (Leng et al., 2024), EOS (Yue et al., 2024), 4.3 HumanEvaluation
HA-DPO(Zhaoetal.,2023),HALVA(Sarkaretal.,
To further verify the effectiveness of MDPO, we
2024), RLHF-V (Yu et al., 2024a), HSA-DPO
conduct human evaluation on MMHalBench, in
(Xiao et al., 2024), and Silkie (Li et al., 2023).
which we ask domain experts to pick the better
Someofthemarecontemporaryworkstoours.
response generated by Bunny trained with either
Implementation Details. We train all the mod- DPOorMDPO. TheresultsarepresentedinFig.4.
els for 3 epochs with a batch size of 32. We use Overall, responses from MDPO are of better or
a learning rate of 0.00001, a cosine learning rate samequalityon89%instancescomparedtoDPO.
scheduler,andawarmupratioof0.1. Wesettheβ Incontrast,DPOonlyachievesbetterperformance
ofpreferenceoptimizationto0.1. Followingprior on11%instances.
work (Zhao et al., 2023; Li et al., 2023), we use
4.4 Analysis
LoRA(Huetal.,2021)totunethemodel. Specifi-
cally,wesettheαto128andrankto64forLoRA.
MDPOiseffectiveacrossdifferentscalesofpref-
MDPO andstandardDPOsharethesameconfigu-
erence data. We assess the overall score and
rationabove. ForMDPO,wesetδ = 0bydefault.
hallucinationrateonMMHallBench,asshownin
Fig. 5. We find that MDPO is effective and con-
4.2 MainResults
sistently outperforms DPO across different data
Tab.1presentsthemainresults. Onallthreebench- scales, demonstrating that our conditional prefer-
marks, MDPO consistently performs better than encemethodenhancesmultimodalpreferenceopti-MMHalBench ObjectHalBench MMHalBench ObjectHalBench
Score HalRate CHAIR CHAIR Anchor Score HalRate CHAIR CHAIR
s i s i
mDPO 2.96 0.42 27.0 4.6 y 2.96 0.42 27.0 4.6
w
-conditional 2.36 0.53 40.3 7.1 y &y 2.98 0.39 29.3 5.0
w l
-anchored 2.50 0.48 34.3 5.7 y &y &m 2.85 0.4 34.7 6.1
w l l
-both(i.e.,DPO) 2.28 0.56 44.3 7.6
Table 4: Comparison of anchors used in MDPO.
Table2: AblationresultsonMDPOwithconditional MDPOaddsananchortoregularizether(m w,q,y w)to
preference or/and anchored preference removed. bepositivebydefault. Addingadditionalanchorstoreg-
While both components are essential in MDPO, an- ularizetherewardsofinstanceswithrejectedresponses
choredpreferencealonebringsonlyslightimprovement (y ) or images (m ) to be negative does not show an
l l
overDPO.Thisindicatesthatconditionalpreferenceis obviousimprovement.
crucialinmultimodalscenarios.
MMHalBench ObjectHalBench
proves preference optimization. We evaluate
Score HalRate CHAIR s CHAIR i MDPOwithdifferentmethodsforconstructingthe
Randomimage 2.81 0.46 40.7 6.6 rejectedimage,asshowninTab.3. Amongallthe
Crop0-20% 2.96 0.42 27.0 4.6
strategies,thedefaultstrategyinMDPO,whichin-
Crop20%-50% 2.92 0.42 33.7 5.4
volvescropping0-20%ofthechosenimages,con-
MoCov2 2.82 0.44 32.3 5.9
sistentlyoutperformstheothers. Thisindicatesthat
Table3: Comparisonofstrategiestocreaterejected usinghardnegativeimages,whichretainsomesim-
images in MDPO. Among all the strategies, the de- ilaritieswiththeoriginalimagesbutalsohaveparts
faultstrategyinMDPO,Cropping0-20%ofthechosen
erased,provideseffectivepreferenceoptimization
images,retainssomesimilaritieswiththeoriginalim-
signals. In contrast, random images are too easy
agesbutcontainsinsufficientvisualinformation,thereby
to identify, while MoCo v2’s data augmentation
providingeffectivepreferenceoptimizationsignals. In
(Chenetal.,2020)isforcreatingsimilarimages.
contrast,randomimagesaretooeasytoidentify,while
MoCov2’sdataaugmentation(Chenetal.,2020)may
Adding anchors to rejected responses or im-
notproduceclearlyworseimages.
ages brings litter improvement. In MDPO, we
introduce an anchor to regularize r(m ,q,y )
w w
mization. Additionally,weobservethat MDPO’s to be positive by default. We also experi-
performanceincreaseswiththescaleofdata,while mented with adding additional anchors to regu-
DPO does not exhibit a scaling effect. This indi- larizer(m ,q,y )andr(m ,q,y )tobenegative.
w l l w
cates that MDPO better utilizes multimodal pref- However,theadditionalanchorsdonotyieldsignif-
erencedatacomparedtoDPO.Specifically,DPO icantimprovements. Theresults,showninTab.4,
strugglestofullyleveragemultimodalpreference indicatethatonlyusingtheanchoronr(m ,q,y )
w w
data,anditsneglectofthevisualmodalitycannot issufficient. Addinganchorstorejectedresponses
be mitigated by merely increasing the size of the orimagesmaycomplicatethetrainingprocesswith-
preferencedata. outprovidingclearadvantages.
Both designs in MDPO are effective, withcon-
ditionalpreferencebeingmorecrucial. Wecon-
4.5 Fine-grainedResults
ductanablationstudytoevaluatethecontributions
ofeachcomponentin MDPO,asshowninTab.2.
Wefurthercomparethefine-grainedresultsofDPO
While both anchored preference and conditional
andMDPOonMMHalBench. AsshowninTab.5,
preference enhance the overall performance of
among the eight question categories, MDPO out-
MDPO,theresultsindicatethatconditionalprefer-
performs standard DPO on six of them. MDPO
enceleadstogreaterimprovementsthananchored
showssignificantimprovementparticularlyonad-
preference. Thissuggeststhatconditionalprefer-
versarial questions with false premises about im-
enceisthekeyfactorinenhancingtheeffectiveness
ages. MDPOcanidentifytheincorrectinformation
ofDPOinmultimodalscenarios,ensuringthatthe
inthequestionaccordingtotheimage,whileDPO
modelbetterutilizesthevisualmodality.
failtodoso. Theseresultsalsoshowtheadvantage
Using hard negative images for rejection im- ofMDPO undervariouspracticalscenarios.overall attribute adversarial comparison counting relation environment holistic other
Bunny 2.11 3.92 0.83 2.17 2.33 2.67 2.25 1.75 1.00
+DPO 2.28 3.25 1.50 1.42 2.50 2.67 4.25 1.75 0.92
+MDPO 2.96 3.08 4.17 2.00 3.50 3.25 4.08 2.17 1.42
Table5: Fine-grainedresultsonMMHalBenchwithamaximumscoreofsix. MDPOoutperformsstandardDPO
onsixoutofeighttypesofquestions,showingsignificantimprovementparticularlyonadversarialquestionswith
falsepremisesaboutimages.
4.6 QualitativeStudy etal.,2024)andSPPO(Wuetal.,2024)propose
samplingpreferencedatainanon-policymanner,
InFig.3,wecomparetheWhentrainedwithstan-
achievingbetterresultsthanoff-policyDPO.More
dardDPO,Bunnyoftenassumestheimagedescrip-
closely related to our work, studies by Park et al.
tioninthequestioniscorrect,respondingaccord-
(2024)andDongetal.(2024)addressthereward
ingly,evenifthequestioncontainsanadversarial
hackingproblemintextualpreferenceoptimization,
premiseregardingtheimage. Incontrast, MDPO
where human preference may be biased towards
identifiesthefalsepremiseinthequestionbyrefer-
longer outputs. They propose to calibrate the re-
encingtheimage. Moreover, Bunnytrainedwith
wardsinDPOwithrespecttooutputlength. Inthis
standard DPO may disregard the image and pro-
work,wediscoveranovelchallengeinmultimodal
videaneducatedguessfortheanswer. Conversely,
DPO,wherepreferenceoptimizationoftenneglects
MDPOdeliversacorrectanswerthatisconditioned
images. Wethenproposeasolutiontothisproblem
ontheimage.
throughconditionalpreferenceoptimization.
5 RelatedWork
In multimodal scenarios, recent works mainly
focusoncreatingmultimodalpreferencedata(Li
Reinforcement learning from human feedback
et al., 2023; Zhao et al., 2023; Xiao et al., 2024;
(RLHF;Christianoetal.2017;Ouyangetal.2022)
Zhouetal.,2024;Pietal.,2024;Sarkaretal.,2024;
has proven to be an effective approach for align-
Yuetal.,2024b;Dengetal.,2024). Theseefforts
ing LLMs with human values. Direct preference
include collecting human preference (Sun et al.,
optimization(DPO;Rafailovetal.2023),whichin-
2023;Yuetal.,2024a),preferencefromadvanced
volvesdirectlyoptimizingLLMsbasedonhuman
multimodalLLMs(Lietal.,2023;Yuetal.,2024b),
preferences, has been widely adopted in RLHF
andpreferencefromthemodeltoalignitself(Deng
duetoitsstrongperformanceandtheelimination
etal.,2024). Intermsoflearningobjectives,recent
of the need for a separate reward model. Signif-
works mainly follows DPO for LLMs (Li et al.,
icant efforts have been made to further enhance
2023;Zhaoetal.,2023;Zhouetal.,2024). Some
theefficacyandefficiencyofDPO,whichcanbe
alsoapplyreinforcementlearning(Sunetal.,2023)
categorized into algorithmic and data-related ad-
andcontrastivelearning(Sarkaretal.,2024;Jiang
vancements. On the algorithmic side, various ap-
etal.,2024). Ourworkstudiesanoverlookedbut
proaches aim to improve the efficiency of DPO.
crucialprobleminthemultimodalDPOobjective.
For example, ORPO (Hong et al., 2024) models
preferences using an odds ratio and combines in-
structionfine-tuningandpreferenceoptimization 6 Conclusion
into a unified training process. Methods such as
CPO (Xu et al., 2024), TPO (Saeidi et al., 2024), We propose MDPO, a preference optimization
andSimPO(Mengetal.,2024)simplifyDPOby methoddedicatedtomultimodalscenarios. MDPO
eliminatingtheuseofareferencemodel,thereby leveragesconditionalpreferenceoptimizationtoen-
reducingcomputationalandmemoryoverhead. Ad- couragemultimodalLLMstocapturepreferencela-
ditionally,IPO(Azaretal.,2024)addressestheis- belsbasedonbothvisualandlanguagecues. Itfur-
sueofrewardoverfittinginDPO.Onthedataside, therintroducesanchoredpreferenceoptimizationto
approachessuchasKTO(Ethayarajhetal.,2024) preventthelikelihoodofpreferredresponsesfrom
and NCA (Chen et al., 2024) seek to overcome decreasing. Experimentsshowthat MDPO consis-
DPO’srequirementforpairedpreferencedataby tentlyenhancesmultimodalLLMperformanceand
designingoptimizationgoalsthatcanalsoutilize reduceshallucinationacrossdifferentmodelsizes
unpaireddata. IterativeDPO(Xuetal.,2023;Yuan onthreewidelyusedbenchmarks.Limitation gpt-4with90%*chatgptquality. Seehttps://vicuna.
lmsys.org(accessed14April2023),2(3):6.
While we have conducted comprehensive experi-
ments to show the effectiveness of MDPO, there PaulFChristiano,JanLeike,TomBrown,MiljanMar-
arestillseverallimitations. First,experimentson tic, Shane Legg, and Dario Amodei. 2017. Deep
reinforcementlearningfromhumanpreferences. Ad-
more multimodal LLMs will provide further ev-
vancesinneuralinformationprocessingsystems,30.
idence on the advantages and disadvantages of
MDPO,specificallyonmodelsacrossvarioussizes Yihe Deng, Pan Lu, Fan Yin, Ziniu Hu, Sheng Shen,
and different architectures. Second, we focus on James Zou, Kai-Wei Chang, and Wei Wang. 2024.
Enhancing large vision language models with self-
the unconditional preference problem in multi-
training on image comprehension. arXiv preprint
modal preference optimization. However, many
arXiv:2405.19716.
contemporary studies have explored enhancing
DPOfromotherperspectives,whichmaybecom- Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang,
plementarytoours. Weleavetheanalysisofcom- HanZhao,YingboZhou,NanJiang,DoyenSahoo,
CaimingXiong,andTongZhang.2024. Rlhfwork-
biningmethodsforfuturework. Third, whilewe
flow: From reward modeling to online rlhf. arXiv
haveevaluatedMDPOonthreebenchmarks,they
preprintarXiv:2405.07863.
stillrepresentalimitedrangeoftasksandsettings
comparedwiththenumerousscenariosinthereal Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff,
DanJurafsky,andDouweKiela.2024. Kto: Model
world. Furtherevaluationonmorebenchmarkscan
alignmentasprospecttheoreticoptimization. arXiv
deepenourunderstandingoftheproposedmethod.
preprintarXiv:2402.01306.
MuyangHe,YexinLiu,BoyaWu,JianhaoYuan,Yueze
References
Wang,TiejunHuang,andBoZhao.2024. Efficient
multimodal learning from data-centric perspective.
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama
arXivpreprintarXiv:2402.11530.
Ahmad, Ilge Akkaya, Florencia Leoni Aleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,
Jiwoo Hong, Noah Lee, and James Thorne. 2024.
ShyamalAnadkat,etal.2023. Gpt-4technicalreport.
Reference-freemonolithicpreferenceoptimization
arXivpreprintarXiv:2303.08774.
withoddsratio. arXivpreprintarXiv:2403.07691.
MohammadGheshlaghiAzar,ZhaohanDanielGuo,Bi-
lalPiot,RemiMunos,MarkRowland,MichalValko, Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan
andDanieleCalandriello.2024. Ageneraltheoret- Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
ical paradigm to understand learning from human and Weizhu Chen. 2021. Lora: Low-rank adap-
preferences. In International Conference on Arti- tation of large language models. arXiv preprint
ficialIntelligenceandStatistics, pages4447–4455. arXiv:2106.09685.
PMLR.
Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang,
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming
SinanTan, PengWang, JunyangLin, ChangZhou, Zhang,andNenghaiYu.2024. Opera: Alleviating
andJingrenZhou.2023. Qwen-vl:Aversatilevision- hallucinationinmulti-modallargelanguagemodels
languagemodelforunderstanding,localization,text viaover-trustpenaltyandretrospection-allocation. In
reading,andbeyond. ProceedingsoftheIEEE/CVFConferenceonCom-
puterVisionandPatternRecognition,pages13418–
RalphAllanBradleyandMiltonETerry.1952. Rank
13427.
analysisofincompleteblockdesigns: I.themethod
of paired comparisons. Biometrika, 39(3/4):324–
MojanJavaheripi,SébastienBubeck,MarahAbdin,Jy-
345.
oti Aneja, Sebastien Bubeck, Caio César Teodoro
Mendes, Weizhu Chen, Allie Del Giorno, Ronen
HuayuChen,GuandeHe,HangSu,andJunZhu.2024.
Eldan,SivakanthGopi,etal.2023. Phi-2: Thesur-
Noisecontrastivealignmentoflanguagemodelswith
prisingpowerofsmalllanguagemodels. Microsoft
explicitrewards. arXivpreprintarXiv:2402.05369.
ResearchBlog.
XinleiChen,HaoqiFan,RossGirshick,andKaiming
He.2020. Improvedbaselineswithmomentumcon- Chaoya Jiang, Haiyang Xu, Mengfan Dong, Jiaxing
trastivelearning. arXivpreprintarXiv:2003.04297. Chen, Wei Ye, Ming Yan, Qinghao Ye, Ji Zhang,
FeiHuang,andShikunZhang.2024. Hallucination
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, augmentedcontrastivelearningformultimodallarge
ZhanghaoWu,HaoZhang,LianminZheng,Siyuan languagemodel. InProceedingsoftheIEEE/CVF
Zhuang,YonghaoZhuang,JosephEGonzalez,etal. ConferenceonComputerVisionandPatternRecog-
2023. Vicuna: Anopen-sourcechatbotimpressing nition,pages27036–27046.Sicong Leng, Hang Zhang, Guanzheng Chen, Xin AmirSaeidi,ShivanshuVerma,AswinRRV,andChitta
Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Baral.2024. Triplepreferenceoptimization: Achiev-
2024. Mitigatingobjecthallucinationsinlargevision- ing better alignment with less data in a single step
language models through visual contrastive decod- optimization. arXivpreprintarXiv:2405.16681.
ing. In Proceedings of the IEEE/CVF Conference
onComputerVisionandPatternRecognition,pages Pritam Sarkar, Sayna Ebrahimi, Ali Etemad, Ahmad
13872–13882. Beirami, Sercan Ö Arık, and Tomas Pfister. 2024.
Mitigatingobjecthallucinationviadataaugmented
Lei Li, Zhihui Xie, Mukai Li, Shunian Chen, Peiyi contrastivetuning. arXivpreprintarXiv:2405.18654.
Wang, Liang Chen, Yazheng Yang, Benyou Wang,
andLingpengKong.2023. Silkie:Preferencedistilla- ZhiqingSun,ShengShen,ShengcaoCao,HaotianLiu,
tionforlargevisuallanguagemodels. arXivpreprint Chunyuan Li, Yikang Shen, Chuang Gan, Liang-
arXiv:2312.10665. YanGui,Yu-XiongWang,YimingYang,etal.2023.
Aligninglargemultimodalmodelswithfactuallyaug-
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae mentedrlhf. arXivpreprintarXiv:2309.14525.
Lee.2024a. Improvedbaselineswithvisualinstruc-
tiontuning. InProceedingsoftheIEEE/CVFCon- JunyangWang,YuhangWang,GuohaiXu,JingZhang,
ferenceonComputerVisionandPatternRecognition, Yukai Gu, Haitao Jia, Ming Yan, Ji Zhang, and
pages26296–26306. Jitao Sang. 2023. An llm-free multi-dimensional
benchmarkformllmshallucinationevaluation. arXiv
HaotianLiu,ChunyuanLi,QingyangWu,andYongJae preprintarXiv:2311.07397.
Lee.2024b. Visualinstructiontuning. Advancesin
neuralinformationprocessingsystems,36. YueWu,ZhiqingSun,HuizhuoYuan,KaixuanJi,Yim-
ingYang,andQuanquanGu.2024. Self-playpref-
Yu Meng, Mengzhou Xia, and Danqi Chen. erenceoptimizationforlanguagemodelalignment.
2024. Simpo: Simple preference optimization arXivpreprintarXiv:2405.00675.
with a reference-free reward. arXiv preprint
arXiv:2405.14734. Wenyi Xiao, Ziwei Huang, Leilei Gan, Wanggui He,
HaoyuanLi,ZhelunYu,HaoJiang,FeiWu,andLin-
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida, chaoZhu.2024. Detectingandmitigatinghallucina-
CarrollWainwright,PamelaMishkin,ChongZhang, tioninlargevisionlanguagemodelsviafine-grained
SandhiniAgarwal,KatarinaSlama,AlexRay,etal. aifeedback. arXivpreprintarXiv:2404.14233.
2022. Training languagemodelsto followinstruc-
tionswithhumanfeedback. Advancesinneuralin- Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan,
formationprocessingsystems,35:27730–27744. LingfengShen,BenjaminVanDurme,KentonMur-
ray, and Young Jin Kim. 2024. Contrastive prefer-
Ryan Park, Rafael Rafailov, Stefano Ermon, and ence optimization: Pushing the boundaries of llm
ChelseaFinn.2024. Disentanglinglengthfromqual- performanceinmachinetranslation. arXivpreprint
ityindirectpreferenceoptimization. arXivpreprint arXiv:2401.08417.
arXiv:2403.19159.
JingXu,AndrewLee,SainbayarSukhbaatar,andJason
Renjie Pi, Tianyang Han, Wei Xiong, Jipeng Zhang, Weston. 2023. Some things are more cringe than
Runtao Liu, Rui Pan, and Tong Zhang. 2024. others: Preference optimization with the pairwise
Strengtheningmultimodallargelanguagemodelwith cringeloss. arXivpreprintarXiv:2312.16682.
bootstrappedpreferenceoptimization. arXivpreprint
arXiv:2403.08730. TianyuYu,YuanYao,HaoyeZhang,TaiwenHe,Yifeng
Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao
AlecRadford,JongWookKim,ChrisHallacy,Aditya Zheng,MaosongSun,etal.2024a. Rlhf-v: Towards
Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas- trustworthymllmsviabehavioralignmentfromfine-
try, Amanda Askell, Pamela Mishkin, Jack Clark, grained correctional human feedback. In Proceed-
etal.2021. Learningtransferablevisualmodelsfrom ingsoftheIEEE/CVFConferenceonComputerVi-
naturallanguagesupervision. InInternationalconfer- sionandPatternRecognition,pages13807–13816.
enceonmachinelearning,pages8748–8763.PMLR.
Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang,
RafaelRafailov,ArchitSharma,EricMitchell,Christo- Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He,
pherDManning,StefanoErmon,andChelseaFinn. Zhiyuan Liu, Tat-Seng Chua, et al. 2024b. Rlaif-
2023. Directpreferenceoptimization:Yourlanguage v: Aligningmllmsthroughopen-sourceaifeedback
modelissecretlyarewardmodel. AdvancesinNeu- for super gpt-4v trustworthiness. arXiv preprint
ralInformationProcessingSystems,36. arXiv:2405.17220.
AnnaRohrbach,LisaAnneHendricks,KayleeBurns, WeizheYuan,RichardYuanzhePang,KyunghyunCho,
Trevor Darrell, and Kate Saenko. 2018. Object Sainbayar Sukhbaatar, Jing Xu, and Jason Weston.
hallucination in image captioning. arXiv preprint 2024. Self-rewarding language models. arXiv
arXiv:1809.02156. preprintarXiv:2401.10020.Zihao Yue, Liang Zhang, and Qin Jin. 2024. Less
ismore: Mitigatingmultimodalhallucinationfrom
an eos decision perspective. arXiv preprint
arXiv:2402.14545.
Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov,
andLucasBeyer.2023. Sigmoidlossforlanguage
imagepre-training. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages
11975–11986.
ZhiyuanZhao,BinWang,LinkeOuyang,XiaoyiDong,
JiaqiWang,andConghuiHe.2023. Beyondhallu-
cinations: Enhancing lvlms through hallucination-
awaredirectpreferenceoptimization. arXivpreprint
arXiv:2311.16839.
YiyangZhou,ChenhangCui,RafaelRafailov,Chelsea
Finn, and Huaxiu Yao. 2024. Aligning modalities
invisionlargelanguagemodelsviapreferencefine-
tuning. arXivpreprintarXiv:2402.11411.