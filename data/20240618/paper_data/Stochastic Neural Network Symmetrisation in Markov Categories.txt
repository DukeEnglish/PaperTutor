Stochastic Neural Network Symmetrisation in
Markov Categories
Rob Cornish
Department of Statistics, University of Oxford
Abstract
Weconsidertheproblemofsymmetrisinganeuralnetworkalongagrouphomomor-
phism: given a homomorphism φ : H → G, we would like a procedure that converts
H-equivariant neural networks into G-equivariant ones. We formulate this in terms
of Markov categories, which allows us to consider neural networks whose outputs may
be stochastic, but with measure-theoretic details abstracted away. We obtain a flex-
ible, compositional, and generic framework for symmetrisation that relies on minimal
assumptions about the structure of the group and the underlying neural network ar-
chitecture. Our approach recovers existing methods for deterministic symmetrisation
as special cases, and extends directly to provide a novel methodology for stochastic
symmetrisation also. Beyond this, we believe our findings also demonstrate the utility
of Markov categories for addressing problems in machine learning in a conceptual yet
mathematically rigorous way.
Contents
1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
2. Background on Markov categories . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3. Group theory in Markov categories . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4. Symmetrisation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
5. A general methodology. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
6. Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
7. Application and numerical results . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
1 Introduction
In many machine learning problems, it is useful to have a neural network that is equivariant
with respect to some group actions. That is, for some group G acting on some input and
output spaces X and Y of interest, we would like a neural network f : X → Y that satisfies
f(g·x) = g·f(x) (1)
for all x ∈ X and g ∈ G. A special case of this is invariance, which takes the action on
Y to be trivial, and so the requirement becomes f(g·x) = f(x) instead. Such constraints
arise in many applications involving some geometric structure, such as computer vision, or
1
4202
nuJ
71
]LM.tats[
1v41811.6042:viXrascientificproblemswherethedatainvolvedareknowntofollowcertainsymmetries[Bro+17;
Bro+21]. However, most off-the-shelf neural networks are not equivariant. Unless care is
taken, even after training on data that contains symmetries, typically (1) will fail to hold,
possibly to a large degree. This can reduce performance and robustness, and so an active
research area considers how to develop neural networks that are equivariant by design.
Intrinsic equivariance vs. symmetrisation Following [Yar18], it is helpful to distin-
guishbetweentwomajorapproachestoobtainingequivariantneuralnetworks. Asignificant
body of work has focussed on intrinsic equivariance, which imposes certain constraints on
individual layers of a neural network to ensure that the network as a whole is equivariant
[CW16; RSP17; FWW21]. In contrast, a recent line of work may be described as symmetri-
sation, which takes anunconstrainedneural networkand modifies itin somewayto become
equivariant. For example, when G is finite, the function
1 (cid:88)
x (cid:55)→ f(g−1·x)
|G|
g∈G
that averages over the elements of the group is seen always to be invariant, regardless of f
[Yar18]. This observation formed the basis of the Janossy pooling approach of [Mur+19b],
who took G to be the symmetric group of permutations, and obtained in this way a neural
network that does not depend on the ordering of its inputs. Subsequently, frame averaging
[Pun+22]extendedthistothecaseofequivarianceandtomoregeneralgroups,andprovided
a technique for reducing the cost of the averaging operation, which can be expensive when
G is large. An initially parallel approach of canonicalisation was proposed by [Kab+23],
which relies on a single representative element of the group that is chosen in an equivariant
way, and thereby avoids averaging altogether. Both techniques were then generalised by
probabilistic symmetrisation [Kim+23], which averages over a random element of the group
that is sampled in an equivariant way. A related approach of weighted frames was also
recently proposed by [DLS24]. Overall, symmetrisation approaches are attractive as they
can leverage unconstrained neural network architectures as their “backbone”, while still
ensuring equivariance overall. This leads to greater modelling flexibility, which these earlier
works have shown can often improve performance compared with intrinsic approaches.
Stochastic equivariance In this work, we consider a neural network that is allowed
to depend on some additional randomness, so that its outputs are stochastic. Various
closely-related notions of equivariance have been proposed for such models in the literature,
including [BT20, (8)] and [Xu+22, Proposition 1]. As a first definition for our purposes,
our goal is to obtain a neural network f and a random variable U such that
d
f(g·x,U) = g·f(x,U) (2)
d
for all x ∈ X, g ∈ G, where = denotes equality in distribution. In other words, we want a
model whose distribution of outputs is equivariant across repeated executions rather than
necessarily at any single one. This generalises our original condition (1): every deter-
ministically equivariant f is also stochastically equivariant in a degenerate way, but the
2converse is not true, since the distributional equality considered here does not imply that
f is equivariant in a pointwise or almost sure sense. In this way, stochastic equivariance
allows us to consider a broader class of models than could be obtained from the determin-
istic case directly. Stochastic equivariance is of interest in applications such as generative
modelling [Xu+22; Hoo+22; AA22; Yim+23] and reinforcement learning [Bre+23], and in
situations where uncertainty quantification is required. It is also relevant for determin-
istic symmetrisation itself, since although the probabilistic methods of [Kim+23; DLS24]
ultimately produce a deterministic output, they rely on a stochastically equivariant neural
network for a particular subcomponent.
In much of the following, it is convenient to regard the pair of f and U as a single entity
thought of as a generative process, rather than decoupling these as in (2). We formalise
this in terms of Markov kernels, defined in Section 2.1, which are a standard construction
in probability theory for modelling conditional distributions or stochastic maps. In short,
a Markov kernel k : X → Y encodes for each x ∈ X a probability distribution on Y that
we denote k(dy|x). The equivariance condition we will consider is then as follows:
k(dy|g·x) = (g·k)(dy|x) (3)
for all x ∈ X, g ∈ G, where g·k : X → Y denotes the Markov kernel that first samples from
k and then deterministically applies g to the result. If (2) holds, we can obtain a suitable
k by defining k(dy|x) as the distribution of f(x,U). Markov kernels therefore subsume our
discussion above, and will serve as our primary case of interest in what follows.
Markov categories WeuseMarkov categories [CJ19; Fri20]asaframeworkforreasoning
about stochastically equivariant neural networks. An overview of this topic is provided in
Section 2 below. At a high level, rather than dealing with Markov kernels directly, we study
the behaviour of abstract entities known as morphisms that behave like Markov kernels in
a precise sense. In doing so, we can prove results about Markov kernels using intuitive,
high-level, and often purely diagrammatic arguments, and without needing to worry about
low-level measure theoretic details. We also gain significant additional generality, and can
specialise to various other settings of interest in a seamless way. For example, although
we emphasise stochastic equivariance in what follows, our results are still valid in Markov
categories that happen to be purely deterministic, and so also apply to existing work on
deterministic symmetrisation as a result.
One consequence of this approach is that we need to generalise various concepts from
classical, set-theoretic group theory so that they make sense in a general Markov category.
We do so in Section 3 below, including for groups, homomorphisms, actions, equivariance,
semidirectand directproducts, orbits, andcosets. Ouraccount usesstandardconstructions
forthispurpose, althoughdoesrequiresomespecificconsiderationsfortheMarkovcategory
setting. We believe this may therefore be of interest in other work that combines groups
and Markov categories also.
Throughout the paper, we will make use of various standard concepts from category
theory, including as functors, coequalisers, and adjoints. We have sought to do so sparingly,
and only when this provides a large enough conceptual benefit to be justified. We have
also sought to present the overall methodology we obtain in a way that can be applied in
practice even without a detailed understanding of these concepts (see e.g. Sections 5.4 and
36). For readers unfamiliar with category theory, a highly accessible introduction can be
found in [Per21]. We will also provide more specific references in various places, as well as
examples of these concepts in more concrete settings where appropriate. For readers who
do know category theory, these parts can be skipped over without loss of continuity.
Symmetrising along a homomorphism Let C be a Markov category, and let
φ : H → G
be a homomorphism between groups in C. For practical purposes, φ may be thought of as
a subgroup inclusion. At a high level, the problem of symmetrisation we consider is to find
some mapping that converts H-equivariant morphisms to G-equivariant morphisms. Most
existing work has considered the specific case where H is the trivial group, in which case
H-equivariance always holds vacuously, and a mapping of this kind therefore takes as input
an arbitrary morphism in C, which may be regarded as an unconstrained neural network.
However, in the deterministic setting, [Kab+23, Section 3.3] also provide a sufficient condi-
tion for their canonicalisation procedure also to apply for general subgroup inclusions. This
allows already equivariant models to be made “more so” without inadvertently undoing
other existing symmetries that are already present. We take this more general problem as
our starting point, and our methodology applies uniformly for all choices of φ, including in
the stochastic setting. It can also be applied compositionally by symmetrising along multi-
ple homomorphisms in sequence, thereby building up more complex equivariance properties
in a structured way.
Our approach We give a high-level overview of our symmetrisation methodology here,
which relies on some concepts from category theory to state. Further background and
details are given in Sections 4 and 5 below. The group H canonically gives rise to a Markov
category CH, whose objects are objects of C equipped with some H-action, and whose
morphisms are morphisms of C that are H-equivariant. We obtain a Markov category CG
of G-objects and G-equivariant morphisms in a similar way. These categories are related
by the functor R : CG → CH that maps each G-object to an H-object by restricting its
φ
action via the homomorphism φ. Our goal is then to obtain functions of the form
CH(R X,R Y) → CG(X,Y), (4)
φ φ
whereX andY areG-objects,andweusethestandardnotationD(U,V)todenotethesetof
morphisms U → V in a category D. Notice that a function of this form maps H-equivariant
morphisms to G-equivariant ones, which is exactly what is desired of a symmetrisation
procedure (although previous work has not framed the problem in this way).
In many familiar settings, such as when dealing with set-theoretic groups, the functor
R admitsaleftadjointE oftenknownasextension orinduction (seee.g.[MPC97,Chapter
φ
I.1]). In this case, for all G-objects X and Y, we always obtain a bijection
CH(R X,R Y)
∼=
CG(ER X,Y). (5)
φ φ φ
As such, the original problem (4) becomes equivalently that of finding functions of the form
CG(ER X,Y) → CG(X,Y), (6)
φ
4since every function (6) now gives rise to a function (4) and vice versa. In other words,
whereasouroriginalproblemrequiredustotranslateH-equivariancetoG-equivariance,now
we only need to translate G-equivariance to (another kind of) G-equivariance, which seems
more tractable. In particular, functions of the form (6) can be obtained straightforwardly
by precomposing with any morphism of the form X → ER X in CG. End-to-end, by
φ
applying the bijection (5) and then precomposing the result in this way, we always obtain a
symmetrisation procedure of the desired form (4). This moreover applies for all G-actions
in C, and is therefore highly generic.
The requirement of obtaining a full left adjoint is quite strong, and in fact not necessary
to obtain a bijection of the form (5). In Theorem 5.1, we provide a weaker condition that is
sufficientandmoreconvenientforpracticalpurposes. WealsoshowinTheorem3.1thatthis
conditionissatisfiedforallgroupsandactionswhenCistheMarkovcategoryoftopological
spaces and continuous Markov kernels, which seems adequate for many applications. In
practice, it is also readily satisfied for many other groups of interest in other contexts, as we
demonstrate through various examples. In Section 5.5, we show that the existing methods
for deterministic symmetrisation mentioned above can all be recovered as instances of this
approachforspecificchoicesofC. Inaddition, byinstantiatingtheproceduredifferently, we
obtain for free a novel methodology for stochastic symmetrisation. We apply this procedure
toobtainthestochasticallyequivariantsubcomponentrequiredby[Kim+23],andshowthat
this improves its empirical performance on a synthetic numerical example.
Outline The rest of the paper is structured as follows. In Section 2.1 we provide a brief
introduction to Markov categories. We aim in particular to allow readers unfamiliar with
string diagrams to parse these easily, as they will be used throughout the paper. In Section
3, we develop some basic concepts from group theory in the context of a general Markov
category. For readers primarily interested in our methodology, this section can initially
be skipped over and referred back to on an as-needed basis. In Section 4, we consider
symmetrisation generally, giving the necessary definitions to formulate the problem as in
(4). In Section 5, we describe our overall approach to symmetrisation based on obtaining
bijections of the form (5). In Section 6, we provide various examples of how this approach
can be applied for different groups and actions of interest. Finally, in Section 7, we consider
implementation details and provide empirical results on a synthetic example.
2 Background on Markov categories
We provide a short introduction to Markov categories here, and refer the reader to [CJ19;
Fri20] for a more detailed treatment.
2.1 Markov kernels
A Markov category can be understood as an abstraction of the key structural behaviour of
Markov kernels, also known as stochastic maps. Given measurable spaces X and Y, recall
that a Markov kernel is a function
k : Σ ×X → [0,1], (7)
Y
5whereΣ denotestheσ-algebraofY, suchthatx (cid:55)→ k(B|x)ismeasurableforeachB ∈ Σ ,
Y Y
and B (cid:55)→ k(B|x) is a probability measure for each x ∈ X. The idea is that Markov kernels
formalisethenotionofaconditionalprobabilitydistributioninarigorous,measuretheoretic
way, thereby allowing more careful reasoning than is permitted by the usual “density”
notation p(y|x) often used in applications. In what follows, we will denote the probability
measure B (cid:55)→ k(B|x) more suggestively using “infinitesimal” notation. So, for example,
Y ∼ k(dy|x)
denotes a Y-valued random variable sampled from k given the input x ∈ X. In addition,
rather than writing out the full signature of a Markov kernel as in (7), we will simply write
k : X → Y.
This emphasises the interpretation of Markov kernels as stochastic maps, with X and Y
regarded as the domain and codomain of k respectively. Following other work on Markov
categories, we will also represent this graphically using string diagrams [Sel10] as follows:
Y
k
X
We will follow the same conventions as [CJ19, Section 2] when denoting string diagrams,
so that in particular these should always be read from bottom to top. Intuitively, a string
diagram represents a generative process, with each box denoting some kind of (potentially
stochastic) operation or computation, and whose wires track the flow of information, with
time flowing upwards. Such diagrams are used informally throughout the machine learning
literature already when describing (for example) neural network architectures, and readers
will likely find themselves comfortable with this notation after seeing the examples below.
2.2 The Markov category Stoch
The prototypical example of a Markov category is Stoch, the Markov category of measur-
able spaces and Markov kernels. We describe the structure of this Markov category now.
Our description will be somewhat informal, emphasising the “sampling” or “generative”
interpretation of Markov kernels rather than their rigorous definition as functions of the
form (7). A formal treatment can be found in [Fri20, Section 4].
Markov kernels k : X → Y and m : Y → Z can always be composed sequentially to
obtain a new Markov kernel m◦k : X → Z. We sample from (m◦k)(dz|x) by sampling
Y ∼ k(dy|x) Z ∼ m(dz|Y)
6and then returning Z. This make sense whenever the codomain of k matches the domain
of m. In string diagrams, m◦k is represented as follows:
Z
m
k
X
For every measurable space X, there is also an identity kernel id : X → X that simply
X
returns its input. In string diagrams, this is drawn simply as a wire:
X
X
However, there is more structure at play here. Given two measurable spaces X and Y, we
can always form the product measurable space, denoted X⊗Y. Now suppose we have two
Markov kernels k : X → Y and m : U → V. We can then always obtain a new Markov
kernel k ⊗ m : X ⊗ U → Y ⊗ V between these product spaces by parallel composition,
or in other words by sampling from each kernel independently. That is, to sample from
(k⊗m)(dy,dv|x,u), we independently sample
Y ∼ k(dy|x) V ∼ m(dv|u),
and then just return the pair (Y,V). In string diagrams, k⊗m is denoted
Y V
k m
X U
In addition, for any two measurable spaces X and Y, there is also always a kernel swap :
X,Y
X ⊗Y → Y ⊗X that deterministically swaps its two inputs, so that given an input (x,y),
it simply returns (y,x). In string diagrams, swap is denoted suggestively as
X,Y
Y X
X Y
Whilethismayseemlikeatrivialoperation, thesekernelsperformafundamentalrole: they
allow us to reorganise the layout of a string diagram in any way we want, provided we do
not change its overall topology (so each box is connected to the same inputs and outputs
before and afterwards). A similarly trivial but very useful operation is copying: for any
7measurable space X, there is a Markov kernel copy : X → X ⊗X that, when given x as
X
input, returns the pair (x,x). In string diagrams, we denote this kernel as:
X X
X
Finally,letI denotethetrivialmeasurablespaceconsistingofthesingletonset{•}equipped
with the trivial σ-algebra. Then for any measurable space X, there is always a (unique)
Markov kernel del : X → I. We represent this in string diagrams as:
X
X
and understand this as the Markov kernel that simply discards its input. The trivial space
I also plays another useful role: kernels p : I → X encode a single probability distribution
p(dx|•), which allows us to recover unconditional distributions as a special case of Markov
kernels. In string diagrams, we denote these kernels without an input wire as follows:
X
p
2.3 Markov categories
The general definition of a Markov category axiomatises the behaviour of Markov kernels
described in the previous section. We provide a high-level overview here, and refer the
reader to [Fri20, Definition 2.1] for a rigorous definition. Instead of measurable spaces and
Markov kernels, the data of a general Markov category C consists of a collection of objects
and a collection of morphisms. Like we did for Markov kernels, we denote these morphisms
by k : X → Y, where X and Y are objects in C referred to as the domain and codomain
of k respectively. We can compose morphisms sequentially (provided they are compatibly
typed), and each object X comes equipped with an identity morphism id . For any pair of
X
objects X and Y, we can form their monoidal product X ⊗Y, which plays the role of the
product measurable space. We can then compose morphisms k and m in parallel to obtain
a new morphism k ⊗m between the monoidal products of their domains and codomains.
There is also a distinguished object I referred to as the monoidal unit that plays the role
of the singleton set, as well as morphisms for swapping, copying, and deleting information.
We depict all these constructions using string diagrams in just the same way as we did for
Markov kernels.
The formal definition of a Markov category includes additional axioms that ensure these
constructions behave like Markov kernels do. For example, it always holds that:
X X
X X
= k =
X
X X
X
8The first condition says that copying some input and then swapping the result is the same
asjustcopyingtheinput. Thesecondconditionsaysthatsamplingfromk givensomeinput
and then discarding the result is the same as just discarding the input. Both are intuitively
always true for Markov kernels, at least when these are regarded informally as generative
processes. We will perform similar manipulations throughout the paper, which can also be
understood by analogy with Markov kernels in this way.
2.4 Examples of Markov categories
The Markov category Stoch described in Section 2.2, whose objects are measurable spaces
andwhosemorphismsareMarkovkernels,willserveasakeyexamplethroughoutthepaper.
However, there are other interesting Markov categories beyond this. For our purposes, it
will also be relevant to consider TopStoch [Fri+23b, Example A.1.4], whose objects are
topological spaces and whose morphisms are continuous Markov kernels, where a Markov
kernel between topological spaces k : X → Y is continuous if for every open subset U ⊆ Y,
thefunctionx (cid:55)→ k(U|x)islowersemicontinuous[FPR21,Section4]. Themonoidalproduct
⊗ returns the product topological space, the monoidal unit I is the singleton topological
space,andtheremainingcomponentsaredefinedanalogouslyasforStoch. Ourmainreason
for interest in TopStoch is that it allows us to prove Theorem 3.1 below, whereas we are not
sure whether the analogous result holds for Stoch. For practical purposes, TopStoch is still
very general, since neural networks used in practice are almost invariably continuous.
Beyond these, more basic examples of Markov categories include: Set, whose objects are
sets and whose morphisms are functions; Meas, whose objects are measurable spaces and
whose morphisms are measurable functions; and Top, whose objects are topological spaces
and whose morphisms are continuous functions. For Set, the monoidal product is just the
cartesian product and the monoidal unit is the singleton set. For Meas and Top these are
similar, but are now equipped with suitable σ-algebras and topologies respectively. That
these are indeed Markov categories follows from [Fri20, Remark 2.4]. For the purposes of
modelling stochastic phenomena, these examples are clearly less interesting than Stoch and
TopStoch, but they will be useful for examples and intuition in what follows.
Remark 2.1. All the preceding examples are positive Markov categories [FR20, Definition
11.22]. This is a mild technical condition that, roughly speaking, ensures determinism (see
below) has various implications that we would expect. That Stoch is positive is shown
in [FR20, Example 11.25], and this in turn also implies that TopStoch is positive [FR20,
Remark 11.26]. It is also straightforward to show that Set, Meas, and Top are all positive
too. We will only make use of this condition in a few places. The point of this remark is
that, when we do, we will not have sacrificed much generality for practical purposes.
92.5 Determinism
Following [Fri20, Definition 10.1], we will say that a morphism f : X → Y in a Markov
category C is deterministic if it holds that
Y Y Y Y
f f
=
f
X X
Intuitively, this says that repeated independent samples from f are always the same. For
example, in Stoch, if f is deterministic and we sample independently
Y,Y′ i ∼id f(dy|x),
then (Y,Y) =d (Y,Y′), where notice that the random variable Y appears twice on the
left-hand side. By [Fri20, Example 10.4], a Markov kernel is deterministic if and only if it
is zero-one, so that f(B|x) ∈ {0,1} for all measurable B ⊆ Y and x ∈ X.
Remark 2.2. Every measurable function f : X → Y gives rise to a deterministic Markov
kernel k : X → Y. Formally, this is defined as
f
k (B|x) := δ (B) (8)
f f(x)
for all measurable B ⊆ Y and x ∈ X, where δ denotes the Dirac measure at y ∈ Y. In this
y
way, the morphisms of Meas may be regarded as deterministic morphisms in Stoch also. A
similar story is also true for Top and TopStoch, where it is straightforward to show that (8)
becomes a lower semicontinuous Markov kernel whenever f is continuous.
Remark 2.3. By [Fri20, Remark 10.13], the deterministic morphisms themselves always
form a Markov category contained in C, which we denote by C .
det
3 Group theory in Markov categories
Wenowshowhowvariousconceptsfrombasicgrouptheorycanbeconsideredinthecontext
of a general Markov category. This section can be read somewhat nonlinearly and referred
back to as definitions appear in later sections.
3.1 Groups, homomorphisms, and actions
We begin by providing the basic definitions of group theory internal to a general Markov
category. A feature of our approach is that we require the various group operations all to
be deterministic. This allows us to recover standard results from set-theoretic group theory
that might otherwise not hold in a general Markov category. We discuss this in more detail
in Remark 3.2 below.
10Definition 3.1. A group in a Markov category C consists of an object G and deterministic
morphisms ∗ : G⊗G → G, e : I → G, and (−)−1 : G → G in C that satisfy the following:
G G
∗ ∗
=
∗ ∗
G G G G G G
G G
G G
G
G ∗ ∗
∗ ∗
e
= = (−)−1 = (−)−1 =
e e
G
G G G
G G
To streamline notation, we will refer to the overall group simply as G, leaving its op-
erations implicit. We will also mostly reuse the same symbols ∗, e, and (−)−1 to denote
these operations across all groups, even when these may be distinct. When we need to
disambiguate, we will use a subscript, e.g. ∗ for the multiplication operation of G.
G
Example 3.1. We spell out how this relates to the classical definition of a group. Under
Definition 3.1, a group in Set is a set G equipped with functions ∗, e and (−)−1. Writing
∗(g,h) simply as gh, the first axiom here says
(gh)n = g(hn)
for all g,h,n ∈ G. In other words, ∗ is associative. Similarly, recall that I in Set is the
singleton set {•}, and so e : I → G encodes a unique value e(•). Denoting e(•) simply as e,
the second axiom says
eg = ge = g
for all g ∈ G, and hence e serves as the unit. Finally, writing (−)−1(g) as g−1, the third
axiom says
g−1g = gg−1 = e
for all g ∈ G. In other words, every element in G has an inverse. This shows that groups
in Set in the sense of Definition 3.1 are precisely groups in the classical sense.
In what follows, we will denote the multiplication, unit, and inversion operations of
groups in Set in the same traditional way as we do here. We will do the same for groups in
Meas and Top (discussed next), where this notation also makes sense.
Example 3.2. For other Markov categories whose morphisms are functions, such as Meas
and Top, a similar story to Example 3.1 applies. However, in these cases, the group op-
erations are more restricted than in Set, since they are required to be morphisms of the
ambient category. For example, group operations in Meas are required to be measurable
functions, andgroupoperationsinToparerequiredtobecontinuousfunctions. Thisisvery
11natural: for example, in probability theory it is standard to study measurable groups (e.g.
[Kal02, Page 15]), which exactly correspond to groups in Meas here.
InStochandTopStoch, thesituationisslightlydifferent, sincethegroupoperationshere
are technically Markov kernels rather than functions. However, by Proposition 3.5 below,
groups in Meas and Top can always be regarded as groups in Stoch and TopStoch by lifting
their operations to deterministic Markov kernels as in (8). All the concrete examples of
groups in Stoch and TopStoch that we consider will be obtained in this way.
Example 3.3. In any Markov category C, the unit I is always a group, regarded as the
trivial group. This is clear in Set: the trivial group is simply the singleton set {•}, and
there is only one possible choice for each group operation, and the group axioms hold
immediately. For general C, the same idea holds because I is a terminal object [Fri20,
Remark 2.3]. Alternatively, this also follows from the case of Set by Remark 3.2 below.
Definition 3.2. A homomorphism in a Markov category C is a deterministic morphism
φ : H → G between groups G and H in C that satisfies
G G
∗ φ
= (9)
φ φ ∗
H H H H
Example 3.4. In Set, as well as in Meas and Top, the condition (9) translates as saying
φ(h)φ(n) = φ(hn)
for all h,n ∈ H, which recovers the usual definition of a set-theoretic homomorphism.
Remark 3.1. It is straightforward to check that the composition of homomorphisms is
again a homomorphism, and that identities are homomorphisms. In this way, we naturally
obtain a category whose objects are groups in C and whose morphisms are homomorphisms
between these.
Example 3.5. For every group G in any Markov category C, there always exists a unique
homomorphism I → G, namely the unit operation e : I → G. In Set, where I becomes the
singleton set {•}, this is a standard exercise to show. In turn, the same then holds for a
general C by Remark 3.2 below. In categorical terms, this says that I is an initial object in
the category of groups in C. (It is also a terminal object in this same category.)
Definition 3.3. Let C be a Markov category. An action of a group G on an object X in C
(or simply a G-action) is a deterministic morphism α : G⊗X → X that satisfies
X X X
X
α α α
= = (10)
α ∗ e
X
G G X G G X X
12Example 3.6. In Set, denoting α(g,x) as g·x, the first equation in (10) says that
g·(g′·x) = (gg′)·x for all g,g′ ∈ G and x ∈ X,
so α is associative. The second axiom says
e·x = x for all x ∈ X,
so α is unital. As a result, actions in Set are precisely group actions in the classical sense.
In what follows, we will often use the notation g·x to denote group actions in Set more
generally, as well as in Meas and Top, where this notation also makes sense. The specific
action this refers to (α here) will always be clear from context.
Example 3.7. A basic example that will nevertheless be useful for us is the trivial action.
For any group G and object X in a Markov category C, this is defined as follows:
X X
ε :=
G X G X
When C = Set, this becomes simply g·x = x for all g ∈ G and x ∈ X. In what follows, we
will use the same notation ε to denote all trivial actions, even though these are technically
distinct morphisms for different choices of G and X.
Example 3.8. Let α : G⊗X → X and α : G⊗Y → Y be actions in a Markov category
X Y
C. Then we always obtain a diagonal action defined as follows:
X Y
α α
X Y
(11)
G X Y
When C = Set, this may be written as g ·(x,y) = (g ·x,g ·y), where g ∈ G, x ∈ X, and
y ∈ Y, and some straightforward algebra shows that this is indeed an action. By Remark
3.2 below, this then also implies that (11) is an action for all C, which is otherwise tedious
to prove in terms of string diagrams directly.
Example 3.9. By comparing Definition (3.1) and Definition (3.3), it can be seen that for
any group G in a Markov category C, its multiplication operation ∗ : G⊗G → G is always
an action. In other words, every group acts on itself by left multiplication. Additionally,
every group acts on itself by the following action, which corresponds to right multiplication
by the inverse:
G
G ∗
∗ op = (−)−1
G G
G G
13For example, in Set, this is associative because
g·(h·n) = nh−1g−1 = n(gh)−1 = (gh)·n
for all g,h,n ∈ G. Unitality is similarly straightforward. This in fact also shows that ∗ is
op
an action for all Markov categories C, as we explain in Remark 3.2 below.
Remark 3.2. All the morphisms involved in the previous definitions are deterministic. At
a high level, we do this in order to ensure that all the familiar results from set-theoretic
group theory apply in our context also. In more technical terms (which are not necessary
to understand in detail), we have defined groups, homomorphisms, and actions internal to
the subcategory C of deterministic morphisms in C. Since C is cartesian monoidal
det det
[Fri20, Remark 10.13], equations involving these constructions in Set can be lifted directly
to equations in C (and hence C) also. This is a standard technique that makes use of
det
the Yoneda embedding: the idea is that a diagram in C commutes if and only if its image
under the Yoneda embedding does, which reduces to a question about set-theoretic groups,
homomorphisms, and actions. Note however that this approach relies on the fact that
∼
C (X,Y ⊗Z) = C (X,Y)×C (X,Z),
det det det
which holds because C is cartesian monoidal. The corresponding statement is not always
det
true for C itself, and more care is required when reasoning by analogy with Set in that case.
As an example of this technique, recall that for all groups G in Set and all actions
α : G⊗X → X of G, where X may be any set, we have
g·(g−1·x) = (gg−1)·x = e·x = x for all g ∈ G and x ∈ X. (12)
BytheapplyingtheYonedaembeddinginthisway, thecorrespondingequationinC then
det
follows automatically, namely:
X
α
X
α
= (13)
(−)−1
G X
G X
We will often use this technique to simplify our presentation, providing set-theoretic jus-
tifications like (12) for equations like (13) that hold in a general Markov category. If this
appears too opaque, then our set-theoretic proofs may alternatively be thought of as a
convenient shorthand for the “real” proofs in terms of string diagrams (which can be more
14tedious to write out).1 For example, (12) translates directly to string diagrams as follows:
X X
X
α α
α X
α ∗
= = e =
(−)−1
(−)−1
G X
G X
G X G X
Here we apply the associativity axiom from Definition 3.3, then the inversion axiom from
Definition 3.1, and finally the unital axiom from Definition 3.3.
3.2 Equivariance and invariance
The previous definitions suggest an obvious notion of equivariance given next. Notice that,
whereas before we defined everything in terms of deterministic morphisms, the notion of
equivariance here now applies to all morphisms in C.
Definition 3.4. Suppose α : G⊗X → X and α : G⊗Y → Y are actions in a Markov
X Y
category C. A morphism k : X → Y in C is G-equivariant (or simply equivariant) with
respect to α and α if it holds that
X Y
Y Y
k α Y
= (14)
α X k
G X G X
We will say that k is invariant if this holds when α is the trivial action ε.
Y
Example 3.10. In Set, the morphism k is just a function X → Y, and k is equivariant if
k(g·x) = g·k(x)
for all g ∈ G and x ∈ X. For invariance, this becomes instead k(g ·x) = k(x). In both
cases, we recover the classical definitions.
Example 3.11. Importantly for our purposes, Definition 3.4 also applies directly to the
stochastic setting. In Stoch (for example), the condition (14) says that
(cid:90) (cid:90)
k(B|x′)α (dx′|g,x) = α (B|g,y)k(dy|x)
X Y
for all measurable B ⊆ Y, g ∈ G, and x ∈ X. Our primary case of interest obtains α and
X
α by lifting the actions of some group in Meas via Proposition 3.5 below. In this case, the
Y
condition (14) becomes
k(dy|g·x) = (g·k)(dy|x)
1The cartesian monoidal structure of C in fact allows this correspondence to be made rigorous also.
det
See e.g. [Lei08] for a discussion.
15for all g ∈ G and x ∈ X, where (g ·k)(dy|x) denotes the pushforward of k(dy|x) by the
function y (cid:55)→ g ·y. This recovers the original equivariance condition (3) that we gave for
stochastic neural networks at the beginning of the paper.
In the machine learning literature, stochastic equivariance is often defined in a slightly
different way. In particular, a conditional density p(y|x) is said to be equivariant if
p(g·y|g·x) = p(y|x) (15)
for all g ∈ G, x ∈ X, and y ∈ Y (see e.g. [Xu+22, Proposition 2.1]). This is a special case
of Definition 3.4, as the following result shows.
Proposition 3.1. Let p(y|x) be conditional density given x ∈ X with respect to some base
measure µ on Y, and denote by k : X → Y the Markov kernel this induces, namely
(cid:90)
k(B|x) := p(y|x)µ(dy)
B
where B ⊆ Y is measurable and x ∈ X. Suppose G is a group acting on X and Y in Meas
such that (15) holds, and that moreover g ·µ = µ for all g ∈ G, where g ·µ denotes the
pushforward of µ by the function y (cid:55)→ g·y. Then k is equivariant in the sense of Definition
3.4, where α and α are obtained by lifting the actions of G to become Markov kernels
X Y
via Proposition 3.5 below.
Proof. See Section A.1 of the Appendix.
The condition g ·µ = µ used here is often not made explicit in the machine learning
literature. This is automatically satisfied in many cases that have appeared (e.g. [Xu+22;
Hoo+22]), which take µ to be the Lebesgue measure and G to be some group of transfor-
mationswithunitJacobian, suchastheorthogonalgroup. However, thisconditionmaynot
holdformoregeneralgroupactionssuchasscaletransformations,inwhichcaseequivariance
in the sense of Definition 3.4 need not follow from (15).
3.3 Compactness
Compact groups serve as a particularly useful class of groups in many applications. For
practical purposes, the most important feature of these is that they admit a Haar measure,
which is thought of as a uniform distribution over elements of the group. We use this as
the basis for our definition of compactness in a general Markov category.
Definition 3.5. A group G in a Markov category C is compact if there exists a morphism
λ : I → G in C, referred to as its Haar measure, that satisfies
G G
∗
λ
= (16)
λ
G G
16Example 3.12. Let G be a group in Meas, and lift this to a group in Stoch in the usual
way by Proposition 3.5 below. The condition (16) says that if G ∼ λ, then
d
gG = G
forallg ∈ G. Thisrecoverstheusualdefinitionofa(normalised)Haarmeasure. Astandard
result shows that a unique such λ exists for every compact second-countable Hausdorff
topological group [Kal02, Theorem 2.27]. In particular, this is true for all finite groups, as
well as the classical compact matrix groups (such as the orthogonal group).
3.4 Semidirect products
Many groups of interest in applications arise as semidirect products of simpler groups. The
following definition allows us to consider these in an abstract Markov category without
requiring further assumptions. We give the definition before showing how it corresponds to
the usual set-theoretic definition in Example 3.13 below.
Definition 3.6. Let C be a Markov category, N and H groups in C, and ρ : H ⊗N → N
some action of H on N such that
N
N
∗
N ρ
= (17)
ρ ρ
∗
N
H N N
H N N
The semidirect product of N and H, denoted N ⋊ H, is the group in C whose underlying
ρ
object is N ⊗H, and whose group operations ∗, e, and (−)−1 are defined respectively as
N H N H
∗ N N H ρ
(18)
ρ ∗ e e
H N H
(−)−1 (−)−1
N H
N H N H
N H
Technically, weshouldcheckthattheseoperationsalwayssatisfiesthegroupaxioms. By
Remark 3.2, since all the morphisms involved are deterministic, it suffices to do so in Set.
ThefollowingexampleshowshowDefinition3.6inSetcorrespondstotheusualset-theoretic
definition of outer semidirect products, after which this becomes standard to prove.
Example 3.13. In Set, an (outer) semidirect product is usually defined in terms of a
homomorphism φ : H → Aut(N), h (cid:55)→ φ , where Aut(N) denotes the automorphism group
h
of N. (Recall that this is the group of invertible homomorphisms N → N equipped with
functioncompositionasitsmultiplicationoperation.) TheideabehindDefinition3.6isthat
17every such φ canonically gives rise to an action ρ that satisfies (17) and vice versa. For
example, in one direction we may uncurry φ to obtain
ρ(h,n) := φ (n) (19)
h
for h ∈ H and n ∈ N. That ρ is indeed an action follows from the fact that φ is a
homomorphism, which means φ = id and φ ◦φ = φ , and moreover ρ satisfies (17)
eH N h h′ hh′
because each φ : N → N is itself a homomorphism. Conversely, by flipping (19) left-to-
h
right, each action ρ that satisfies (17) also defines a homomorphism φ : H → Aut(N), as
maybechecked. Underthiscorrespondence, thegroupoperations(18)thenalsocorrespond
to their usual set-theoretic definitions. For example, multiplication may be written as
(n,h)(n′,h′) = (nρ(h,n′),hh′)
wheren,n′ ∈ N andh,h′ ∈ H, whichrecoverstheusualdefinitionbyreplacingρ(h,n′)with
φ (n′). The usual argument for outer semidirect products in Set now translates directly to
h
show that the group axioms hold, although we omit the details.
Example 3.14. The Euclidean groups E(d) and SE(d) are well-known examples of semidi-
rect products. We show how these can be expressed as instances of Definition 3.6 in Set
(although this applies equally in Meas and Top). Let Td be the group of translations, or
in other words Rd equipped with vector addition, and let O(d) be the orthogonal group.
We then obtain an action ρ : O(d)⊗Td → Td in the obvious way with Q·t := Qt. The
condition (17) becomes Q·(t+t′) = Q·t+Q·t′, which is clearly satisfied. This allows us
to obtain the Euclidean group as follows:
E(d) := T ⋊ O(d).
d ρ
For example, the group multiplication in (18) becomes
(t,Q)(t′,Q′) = (t+Qt′,QQ′),
which recovers the usual definition. By substituting the special orthogonal group SO(d) for
O(d), we also obtain the special Euclidean group SE(d) in the same way.
Remark 3.3. The condition (17) amounts to saying that N is a group in C whose multi-
plication operation is H-equivariant, where H acts on N diagonally as in Example 3.8. For
example, in Set, writing ∗ as m, this condition becomes
N
m(h·n,h·n′) = h·m(n,n′)
for all h ∈ H and n,n′ ∈ N. By some straightforward manipulations, this in turn implies
that the inversion and identity operations of N are also H-equivariant in a particular sense:
weobtainh·n−1 = (h·n)−1andh·e = e inSet,andsimilarequationsmoregenerally. This
N N
gives rise to a particularly streamlined way to describe semidirect products: in terminology
from Definition 4.1 below, each semidirect product N ⋊ H in C corresponds to a group
ρ
(N,ρ) in the Markov category CH of H-equivariant morphisms, and vice versa. This is
known for example in the context of algebraic topology (see [SS22, Lemma 2.1.4]), and
shows that Definition 3.6 arises very naturally, despite its apparently complex structure.
18A familiar result says that actions of a classical semidirect product N ⋊ H can always
ρ
be computed as some H-action followed by some N-action. This translates directly to our
setting as follows.
Proposition 3.2. Let C be a Markov category. Every action of a semidirect product N⋊ H
ρ
on an object X in C can be written in the form
X
β
(20)
α
N H X
where α : H ⊗X → X and β : N ⊗X → X are actions in C, and moreover β satisfies:
X X
β α
ρ α =
β
H N X H N X
Conversely, for any α and β in C with these properties, (20) defines an action of N ⋊ H
ρ
that we will denote by β⋊ α.
ρ
Proof. After applying the correspondence from Example 3.13, this becomes standard to
show in Set. The result then applies more generally by Remark 3.2.
Example 3.15. Continuing Example 3.14, suppose we now have an action of SE(d) =
T ⋊SO(d) on an arbitrary object in C. Proposition 3.2 says this can always be computed
d
as an action of SO(d) followed by an action of T . Adopting the usual interpretation of
d
SO(d) as rotation matrices, this amounts to a rotation followed by a translation, which
recovers the usual result.
As for the classical case, equivariance with respect to the action of a semidirect product
is equivalent to equivariance with respect to its two induced actions separately.
Proposition 3.3. Let C be a Markov category, and k : X → Y a morphism in C. Then k
is equivariant with respect to some actions β ⋊ α and β ⋊ α of a semidirect product
X ρ X Y ρ Y
N ⋊ K in C if and only if it is both 1) equivariant with respect to α and α , and 2)
ρ X Y
equivariant with respect to β and β .
X Y
Proof. Wesketchthemainidea. ThatN-andK-equivarianceimplies(N⋊ K)-equivariance
ρ
follows immediately from two applications of the definition of equivariance. Conversely,
since group actions are unital, every action of the form (20) becomes an N-action when
e is attached its K-input, and a K-action when e is attached to its N-input. As such,
K N
(N ⋊ K)-equivariance implies both N- and K-equivariance.
ρ
193.5 Direct products
An important special case of semidirect products is the direct product. We give this its
own definition as follows.
Definition 3.7. Let G and H be groups in a Markov category C. The direct product of G
and H, denoted G×H, is the group in C consisting of G⊗H equipped with ∗, e, and (−)−1
defined respectively as
G H
G H
G H
∗ G ∗ H e e (−)−1 (−)−1
G H G H
G H
G H G H
Notice that, as classically, this is just a special case of the semidirect product where
the action of H on N is trivial. That is, N ×H = N ⋊ H. In this case, the equivariance
ε
condition (17) is trivially satisfied, and so N ×H is always a group in C since N ⋊ H is.
ε
Example 3.16. Suppose G and H are groups in Set. Then G × H consists of pairs
(g,h) with g ∈ G and h ∈ H. Multiplication, identities, and inversions are all computed
componentwise, so that for example
(g,h)(g′,h′) = (gg′,hh′)
where g,g′ ∈ G and h,h′ ∈ H, which recovers the usual definition.
Remark3.4. ByspecialisingProposition3.2tothiscontext,itholdsthatactionsofadirect
product G×H on an object X correspond bijectively to pairs of actions α : G⊗X → X
and β : H ⊗X → X such that
X
X
β
α
=
α
β
G H X
K N X
That is, an action of G×H is simply a pair of an G- and a H-action that commute with
eachother. Likewise, byspecialisingProposition3.3, itholdsthatequivariancewithrespect
to G×H is equivalent to equivariance with respect to G and H separately.
3.6 Orbits
A classical result from group theory is that the action of a group G on a set X always
induces an equivalence relation on X, with x ∼ x′ when x = g ·x′ for some g ∈ G. The
equivalenceclasses[x]obtainedinthiswayarereferredtoasorbits. Thefollowingdefinition
allows us to talk about orbits in the context of Markov categories also.
20Definition 3.8. Let α : G⊗X → X be an action in a Markov category C. An orbit map
is a deterministic morphism q : X → X/G, where X/G may be any object of C, such that
the following is a coequaliser diagram in C
α q
G⊗X X X/G (21)
ε
that is moreover preserved by the functor (−)⊗Y for every Y in C.
Coequalisers are a standard construction in category theory, and are often used as a
way to talk about quotients in an abstract setting. We provide the main idea here. For (21)
to be a coequaliser diagram, two conditions must hold. First, q must satisfy q◦α = q◦ε,
where recall that ε is the trivial action from Example 3.7. In string diagrams this becomes
X/G
X/G
q
q
=
α
G X
G X
which just says that q is invariant with respect to α. In addition, q must be universal with
this property in the sense that if k : X → Z is any other invariant morphism in C, then
there is a unique morphism k/G : X/G → Z such that the following diagram commutes:
q
X X/G
k/G
k
Z
Intuitively, this says that q is the most information-preserving invariant morphism out of
X, sinceanyotherinvariantmorphismcanbecomputedinsomewayintermsofitsoutput.
For a more detailed discussion of coequalisers, we refer the reader to [Per21, Section 3.4.2].
Remark3.5. Therequirementthatqisdeterministicisafairlynaturalone,astheexamples
belowwillshow. OurmaintechnicalreasonforimposingitisfortheproofofProposition3.4
below. In practice, determinism is not a major restriction. For example, if C is positive and
(21)isacoequaliserdiagram,thenqisautomaticallydeterministic,andsothisdoesnotneed
to be checked separately. This is true more generally for all coequalisers of deterministic
morphisms: see Proposition A.1 in the Appendix.
Remark 3.6. We briefly comment on the preservation condition in Definition 3.8. For
practical purposes, this may be understood as a mild technical requirement that ensures
parallel compositions of orbit maps are well-behaved. For various C of interest we can even
avoid checking this condition altogether (see Remark 3.8 below). Its key implication is
that orbit maps are closed under parallel composition with identity morphisms (which are
themselves always orbit maps by Example 3.18 below). In other words, if q is an orbit map
with respect to α : G⊗X → X, then q⊗id is also an orbit map, where G acts on X via
Y
α and on Y trivially. See Proposition A.2 in the Appendix for a proof.
21Example 3.17. Consider what Definition 3.8 means in Set. Let X/G := {[x] | x ∈ X} be
the set of orbits induced by α, and let q : X → X/G, x (cid:55)→ [x] map each element to its orbit.
We claim that q is an orbit map. Certainly it is deterministic. We need to show that it is
also a coequaliser, or in other words that it is invariant with respect to α, and universal.
Invariance is straightforward since
q(g·x) = [g·x] = [x] = q(x)
by the definition of orbits. For universality, if f is any other invariant function, then
f/G : X/G → Y defined as [x] (cid:55)→ f(x) is well-defined and invariant, and moreover
f(x) = (f/G)([x]) = (f/G)(q(x)),
which shows f = f/G◦q. Moreover, since q is surjective, f/G is the only function that can
satisfy this, and so it follows that q is a coequaliser.
Finally, to be an orbit map, we must show that q is preserved by every functor (−)⊗Y.
Recall that this functor sends each object Z in C to Z⊗Y, and each morphism f to f⊗id .
Y
The image of (21) under this functor is therefore
(G⊗X)⊗Y
α⊗idY
X ⊗Y
q⊗idY
X/G⊗Y (22)
ε⊗idY
and we want to show that this is also a coequaliser diagram. This can be done using a
similar argument as we just gave. (In fact, what we showed is really a special case of this
more general statement with Y := I the singleton set.) As a result, q is an orbit map.
Example 3.18. For any group G activing trivially on an object X in a Markov category
C, the identity id is always an orbit map. Indeed, it is easily checked that the following is
X
a coequaliser diagram:
G⊗X
ε
X
idX
X (23)
ε
Moreover, this is preserved by every functor (−)⊗Y since we have id ⊗id = id .
X Y X⊗Y
Although trivial, this example will nevertheless be useful for us in what follows.
Remark 3.7. Oftentherewillexistmorethanonepossiblechoiceoforbitmapforthesame
group action. For example, consider the rotation group G := SO(2) acting on the plane R2
in Set. By checking the various requirements, it follows that the function r : R2 → [0,∞),
x (cid:55)→ ∥x∥ is an orbit map. This is a different construction to the orbit map q : X → X/G
from Example 3.17, where X/G was defined as a set of equivalence classes. However, in
another sense, these two orbit maps are equivalent: a straightforward argument shows that
there exists a unique bijection X/G → [0,∞) such that the following commutes:
q
X X/G
r
∼=
[0,∞)
22In this way, r and q describe the same information, just with different “encodings”. More
generally, as a consequence of the Yoneda Lemma, if q and r are orbit maps with respect to
the same group action in a Markov category C, then there always exists a unique isomor-
phism ϕ such that r = ϕ◦q holds. For practical purposes, this means we are free to choose
whichever orbit map is most convenient, and will not sacrifice generality in doing so.
It is not guaranteed that a Markov category will admit all orbit maps: there may exist
some α for which a coequaliser diagram (21) does not exist, or is not preserved by every
functor (−)⊗Y. We therefore consider examples in which orbit maps are always available.
Example 3.19. Set admits all orbit maps. This is shown directly by Example 3.17. More
abstractly, this can be seen by noting that Set has coequalisers and is cartesian closed,
which means (−)⊗Y is a left adjoint and hence cocontinuous (see e.g. [Per21, Corollary
4.3.2]). The same idea applies more generally: a Markov category C admits all orbit maps
if it has coequalisers (or even reflexive coequalisers) and is monoidal closed.
Example 3.20. Top admits all orbit maps. Unlike with Set, a quick abstract proof is
less obvious to us here, but a standard lower-level argument can be used instead. For
completeness, we include this in Proposition A.3 in the Appendix.
In both Set and Top, every morphism is deterministic. For reasoning about stochastic
neural networks, we wouldlikea Markov category that admitsall orbit maps, but alsomore
general morphisms. The following provides a convenient and very general example.
Theorem 3.1. Every action in TopStoch admits an orbit map.
Proof. See Section A.4 of the Appendix.
We do not know whether Meas or Stoch admit all orbit maps. In Meas, it is true that
every action gives rise to a coequaliser (21) obtained from the usual measure-theoretic quo-
tient space. A similar statement also holds in Stoch as a consequence of [MP23, Proposition
3.7]. However, in both cases, we were not able to prove that these coequalisers are always
preserved by the functor (−)⊗Y, which our theory below requires.
Remark 3.8. For our symmetrisation methodology below, we will need to construct orbit
maps more explicitly than these existence results provide. However, it is still practically
useful to know that some orbit map does indeed exist. A straightforward argument shows
thatifanactionα admitssomeorbitmap, theneverycoequaliseroftheform(21)automat-
icallysatisfiesthepreservationconditionandisthereforeanorbitmap. SeePropositionA.8
in the Appendix. As such, to show a morphism is an orbit map in TopStoch (for example),
we just need to establish the coequaliser condition: the preservation condition does not
need to be checked separately.
3.7 Cosets
In set-theoretic group theory, a subgroup H of a group G gives rise to a set of (left) cosets,
whose elements have the form gH := {gh | h ∈ H} where g ∈ G. The following definition
allows us to talk about cosets in general Markov categories also.
23Definition 3.9. Let φ : H → G be a homomorphism in a Markov category C. A φ-coset
map is an orbit map with respect to the following action of H on G:
G
∗ op (24)
φ
H G
In terminology introduced in Definition 4.2 below, the morphism (24) is just the restric-
tion via φ of the action ∗ from Example 3.9, and is therefore indeed always an action.
op
Example 3.21. In Set, Definition 3.9 recovers cosets in the usual sense by letting φ : H →
G be a subgroup inclusion, so that φ(h) = h. The action (24) becomes
h·g = gφ(h)−1 = gh−1
for h ∈ H and g ∈ G. The cosets of H in G are then exactly the orbits obtained from this
action, since we have
gH = {gh | h ∈ H} = {gh−1 | h ∈ H} = {h·g | h ∈ H},
and the right-hand side is seen to be the orbit of g. Letting G/H be the set of these cosets,
it now follows from Example 3.17 that the function G → G/H defined as g (cid:55)→ gH is an
orbit map, and hence a φ-coset map.
Remark 3.9. We will often use the same suggestive notation G/H from Example 3.21 to
denote the codomain of a φ-coset map, where φ : H → G is a homomorphism in a general
Markov category C. However, we emphasise that this is not meant to imply G/H is a
quotient group in any sense. In particular, recall that G/H from Example 3.21 does not
inheritagroupstructureunlessH isanormal subgroup. Instead,G/H shouldbethoughtof
simplyasaspaceoforbits(ormorespecifically,cosets)withoutassociatedgroupoperations.
This makes sense to consider regardless of any additional properties H may have.
Example 3.22. Let G be a group in a Markov category C, and φ : I → G the unique
homomorphism out of the trivial group from Example 3.3. Then (24) becomes trivial. By
Example 3.18, it follows that id : G → G is an orbit map, and hence a φ-coset map.
G
Example 3.23. Let N ⋊ H be a semidirect product in a Markov category C. Recall from
ρ
Definition 3.6 that this has N ⊗H as its underlying object. We therefore always obtain
inclusion and projection morphisms
iN iH
N N ⋊ H H
ρ
pN pH
where for example
N H N H
N
N
e
i N := H p N :=
N H
N N N H
24and i and p are similar. By a straightforward argument, both i and i are homomor-
H H N H
phisms. Moreover, p is a i -coset map, and p is a i -coset map. See Proposition A.9
H N N H
in the Appendix. Heuristically, this says that (N ⋊ H)/N ∼ = H and (N ⋊ H)/H ∼ = N, so
ρ ρ
that after “quotienting out” one factor, we are left with the other.
Example 3.24. Letφ : O(d) → GL(d,R)betheinclusionhomomorphismoftheorthogonal
group into the general linear group in Set (for example). Let PD(d) denote the set of d-
dimensional positive-definite matrices, and define q : GL(d,R) → PD(d) by A (cid:55)→ AAT.
Then q is a φ-coset map. Indeed, this is invariant because
q(AQ−1) = AQTQAT = AAT
whenever Q is orthogonal. Now suppose r is also invariant. For A ∈ GL(d,R) a standard
matrix decomposition allows us always to write A = RQ for some orthogonal Q and upper
triangular R, which yields
r(A) = r(RQ) = r(R) = r(Chol(RRT)) = r(Chol(AAT)) = r(Chol(q(A))),
where Chol denotes the Cholesky decomposition. This shows that r(A) can be expressed
as a function of q(A), and this function must be unique since q is surjective. By Remark
3.8, the preservation condition now holds automatically, and so q is a φ-coset map.
3.8 Coset actions
If φ : H → G is a homomorphism, then by definition every φ-coset map is H-invariant.
However, importantly, a φ-coset map also becomes G-equivariant in a canonical way.
Proposition 3.4. Let q : G → G/H be a φ-coset map. There exists a unique action
∗/H : G⊗G/H → G/H that makes q equivariant in the sense that
G/H G/H
q ∗/H
=
∗ q
G G G G
Proof. See Section A.7.1 of the Appendix.
Example 3.25. Continuing Example 3.21, the action of G on G/H is given by
g·(g′H) = (gg′)H.
It is straightforward to check that this is well-defined and satisfies the axioms of an action.
Moreover,
q(gg′) = (gg′)H = g·(g′H) = g·q(g′),
which shows that q is G-equivariant with respect to this action.
25Example 3.26. Continuing Example 3.22, the action induced by id is simply the action
G
of G on itself by left multiplication. This is always an action by Example 3.9, and it holds
trivially that id is equivariant with respect to this action on its domain and codomain.
G
Example 3.27. ContinuingExample3.23onsemidirectproducts,recallthattheprojection
p : N ⋊ H → H is always an i -coset map. Proposition 3.4 then induces the following
H ρ N
action of N ⋊ H on H:
ρ
H H
∗/N := ∗
H
N H H N H H
Indeed, it can be seen from Proposition 3.2 that this is always an action. We therefore only
need to show that p is equivariant with respect to ∗ and ∗/N. By Remark 3.2, this may
H
be done in Set, where for n,n′ ∈ N and h,h′ ∈ H we have
p ((n,h)(n′,h′)) = p (nρ(h,n′),hh′) = hh′ = (n,h)·p (n′,h′),
H H H
where we use the definition of ∗/H in the last step. By a similar argument, the action on
N induced by the projection p : N ⋊ H → N, which is an i -coset map, is given by
N ρ H
N
N
∗
N
∗/H :=
ρ
N H N
N H N
Example 3.28. Continuing Example 3.24, the unique action of GL(d,R) on PD(d) that
makes q equivariant is given by
A·P = APAT,
where B ∈ GL(d,R) and P ∈ PD(d). It is straightforward to check that this indeed satisfies
the axioms of an action, and moreover
q(AB) = (AB)(AB)T = ABBTAT = A·(BBT) = A·q(B)
for all A,B ∈ GL(d,R), so that q is equivariant with respect to this action.
3.9 Lifting group constructions
It is somewhat tedious to specify groups, actions, and orbit maps in Stoch or TopStoch
directly, because the morphisms of these categories are Markov kernels, which are not as
straightforward to write down as ordinary functions are. The following technical result
provides a convenient way around this.
Proposition 3.5. Suppose C and D are Markov categories, where D is the Kleisli category
of a symmetric monoidal monad on C. Then the standard inclusion functor C → D lifts
groups, actions, equivariant morphisms, and orbit maps from C to D.
26Proof. See Section A.8 in the Appendix.
As is well known, Stoch may be obtained as the Kleisli category of the Giry monad on
Meas [Gir82], which is symmetric monoidal [Fri20, Lemma 4.1]. A similar statement is true
for TopStoch, where now the relevant monad is defined on Top [FPR21, Corollary 4.17].
As such, this result allows us to specify these various constructions in terms of functions
in Meas or Top, and then lift these to Stoch or TopStoch automatically. For example, the
following may be checked to be an action of the orthogonal group O(n) on Rn×n in Top,
and hence in TopStoch:
Q·A := QAQT.
Similarly, orbit maps always exist for actions defined in Top (see Example 3.20), and these
lift directly to TopStoch also.
4 Symmetrisation
4.1 Markov categories of equivariant morphisms
It is easily checked that the composition of equivariant morphisms is again equivariant, and
thattheidentitymorphismisequivariant. Inthisway, theequivariantmorphismsinCform
a category. By Proposition B.1 in the Appendix, this is in fact also a Markov category,
whose components are defined as follows.
Definition 4.1. Given a group G in a Markov category C, we denote by CG the Markov
category of G-equivariant morphisms, whose components are as follows:
• Objects are pairs (X,α ) of an object X and an action α : G⊗X → X in C
X X
• Morphisms (X,α ) → (Y,α ) are morphisms X → Y in C that are equivariant with
X Y
respect to α and α , with composition and identities inherited from C
X Y
• The monoidal product (X,α )⊗(Y,α ) is (X⊗Y,α ), where α is the diagonal
X Y X⊗Y X⊗Y
action defined in Example 3.8
• The monoidal unit is (I,ε), where ε is the trivial action defined in Example 3.7
• The structure morphisms (e.g. copy and discard) are inherited from C
Example 4.1. It is straightforward to check that in any Markov category C, the only
actions of the trivial group I are trivial. Moreover, a morphism k : X → Y in C is always
equivariantwithrespecttothetrivialactionsonX andY. Asaresult,CI consistsofobjects
of the form (X,ε), where X may be any object in C, and its morphisms (X,ε) → (Y,ε) are
all the morphisms X → Y in C. In this way, we may regard CI and C as being the same
(although more precisely they are isomorphic as categories).
It will often be cumbersome to write objects in CG always as pairs, as in (X,α ) or
X
(Y,α ). To streamline notation, we will sometimes denote these simply by X,Y,... and so
Y
on. We will always make clear when we are doing so.
274.2 Action restriction
A homomorphism φ : H → G always gives rise to a functor CG → CH. This provides
a link between G-equivariant and H-equivariant morphisms, and so is highly relevant for
symmetrisation. This functor acts on objects via restriction, which we define first.
Definition 4.2. Let φ : H → G be a homomorphism in a Markov category C. Given an
action α : G⊗X → X in C, its restriction via φ is the following action of H:
X
α
G (25)
φ
H X
Technically, we should check that (25) indeed always defines an action. The following
example establishes this in Set, and hence in general by Remark 3.2.
Example 4.2. In Set, the action (25) may be written as
h·x = φ(h)·x where h ∈ H and x ∈ X. (26)
Notice that the original G-action appears here on the right-hand side. This is associative
because
φ(h)·(φ(h′)·x) = (φ(h)φ(h′))·x = φ(hh′)·x,
where we use the fact that φ is a homomorphism in the second step. Unitality follows
by a similar argument. An important special case occurs when φ is a subgroup inclusion
H (cid:44)→ G. In this case, φ(h) = h, and the action (26) becomes the restriction of the original
G-action to the subgroup H, which motivates the terminology “restriction”.
With Definition 4.2 given, we are now able to define the functor CG → CH mentioned
previously.
Definition 4.3. Let φ : H → G be a homomorphism in a Markov category C. The restric-
tion functor
R : CG → CH
φ
is defined on objects as R (X,α) := (X,α ), where α denotes the action (25). On mor-
φ φ φ
phisms, R is just the identity, so that R (k) := k.
φ φ
To see that R is indeed a well-defined functor, just observe that, by definition, a
φ
morphism k : X → Y in C is a morphism R (X,α ) → R (Y,α ) in CH if it holds that
φ X φ Y
Y
Y
k
α
Y
α X = (27)
φ k
φ
H X
H X
This follows immediately when k is equivariant with respect to α and α , and hence for
X Y
all k : (X,α ) → (Y,α ) in CG. The other functor axioms are immediate.
X Y
284.3 Symmetrisation procedures
Our primary methodological aim in this paper is to obtain procedures for symmetrisation.
At a high-level, given some homomorphism φ : H → G that we specify, we would like
a mapping that sends H-equivariant morphisms to G-equivariant one. We formalise this
precisely as follows. Recall that we use the standard notation D(U,V) to denote the set of
morphisms U → V in a category D.
Definition 4.4. Let C be a Markov category. A symmetrisation procedure is a function of
the form
CH(R X,R Y) → CG(X,Y) (28)
φ φ
where φ : H → G is a homomorphism in C, and X and Y are objects in CG.
Observethattheleft-handsideof (28)consistsofmorphismssatisfying(27),whereasthe
right-handsideconsistsofmorphismsthatsatisfythisconditionwhenφisremoved. Forthis
reason,weintuitivelythinkofasymmetrisationprocedureastransporting“lessequivariant”
morphisms to “more equivariant” ones. In particular, if φ is a subgroup inclusion (for
example in Set), then (28) converts morphisms that are equivariant only with respect to
the subgroup H into morphisms that are equivariant with respect to the full group G.
Example 4.3. An important special case takes φ : I → G to be the unique homomor-
phism out of the trivial group from Example 3.5. By Example 4.1, we may identify CI
may be identified with C itself. As a result, a symmetrisation procedure of this type pro-
duces G-equivariant morphisms from arbitrary morphisms in C that are not subject to any
equivariance constraints at all.
4.4 Potential desiderata
Definition4.4isveryminimal,andimposesessentiallynostructureonthefunctioninvolved:
it must simply map H-equivariant morphisms to G-equivariant ones. In practice, there are
various additional properties that we might want a symmetrisation procedure to satisfy.
Recall from Section 4.2 that every morphism X → Y in CG is also a morphism of the form
R X → R Y in CH. In other words, it holds that
φ φ
CG(X,Y) ⊆ CH(R X,R Y).
φ φ
As such, a reasonable requirement of a symmetrisation procedure is that it restricts to the
identity on CG(X,Y), and so does not modify its input unless strictly necessary. Precisely:
Definition 4.5. A symmetrisation procedure sym : CG(R X,R Y) → CG(X,Y) is stable
φ φ
if it satisfies sym(k) = k for all k : X → Y in CG.
Beyond its intuitive appeal, stability has other desirable consequences. For example, it
follows essentially by definition that a stable symmetrisation procedure is surjective. This
is clearly of interest for machine learning applications, since it provides a basic guarantee
of the overall expressiveness of the procedure: it is possible to obtain every G-equivariant
morphism given some appropriate input. Additionally, a stable symmetrisation procedure
sym is always idempotent, so that sym(sym(k)) = sym(k). This is useful to know at imple-
mentation time: we can run all our experiments with sym applied just once, confident that
we have not sacrificed any performance by doing so.
294.5 Composing procedures
Symmetrisation procedures may be applied in sequence. For example, suppose we have two
homomorphisms
ϕ φ
K H G.
and two symmetrisation procedures of the form
sym sym
CK(R R X,R R Y) ϕ CH(R X,R Y) φ CG(X,Y). (29)
ϕ φ ϕ φ φ φ
We may then compose these in the obvious way by first applying sym and then applying
ϕ
sym . This allows us to start with a morphism that is K-equivariant and end up with
φ
one that is G-equivariant. It follows immediately from inspection of Definition 4.2 that
R R = R , and so this composition is a function
ϕ φ φ◦ϕ
CK(R X,R Y) CG(X,Y).
φ◦ϕ φ◦ϕ
This is still a symmetrisation procedure, now along the homomorphism φ◦ϕ : K → G.
Remark4.1. Itisclearthatstabilityandsurjectivityarebothpreservedundercomposition
of symmetrisation procedures. However, idempotence is not in general.
4.6 Deterministic symmetrisation
In many situations, it is desirable to obtain an equivariant neural network that is deter-
ministic. If this is required, one approach is simply to work in a Markov category whose
morphismsarealwaysdeterministic,suchasSetorTopratherthanStochorTopStoch. This
ensures any symmetrisation procedure will return a deterministic morphism by construc-
tion. However, recent work suggests that there is an advantage to working in a probabilistic
setting even when determinism is ultimately sought [Kim+23; DLS24]. An alternative
approach suggested by existing work [Pun+22; Kim+23] is instead to symmetrise proba-
bilistically, and then to make the result deterministic by computing its expectation. We
describe this now in Stoch.
Definition 4.6. LetY := Rd for somed ∈ N. The expectationoperator, denotedave, sends
a Markov kernel k : X → Y to the Markov kernel ave(k) : X → Y, where ave(k)(dy|x) is
Dirac on
(cid:90)
yk(dy|x), (30)
provided this integral exists for all x ∈ X.
Here the integral is meant componentwise. Since k is a Markov kernel, a standard
argument using Fubini’s theorem shows that the function that sends x ∈ X to (30) defines
a measurable function. The Markov kernel ave(k) is then obtained by lifting this to a
deterministic Markov kernel as described in Remark 2.2. A more general definition, where
Rd is replaced by a Banach space, also appears possible.
In general, ave(k) may not be equivariant even if k is. However, ave does preserve
equivariance with respect to affine group actions. This occurs very often in practice, since
30many actions are defined in terms of elementary matrix operations. The following result
makes this more precise.
Proposition 4.1. Let G be a group in Meas acting on measurable spaces X and Y := Rd,
where the action on Y is affine, and suppose k : X → Y is a Markov kernel for which the
integral (30) is everywhere defined. If k is equivariant in Stoch when these actions are lifted
to Stoch (see Proposition 3.5), then ave(k) is also equivariant with respect to these actions.
Proof. In the notation of Example 3.11, for all x ∈ X and g ∈ G we have
(cid:90) (cid:90) (cid:90) (cid:90)
yk(dy|g·x) = y(g·k)(dy|x) = (g·y)k(dy|x) = g· yk(dy|x),
wherethesecondstepusesthelawoftheunconsciousstatistician,andthethirdtheassump-
(cid:82)
tionthattheactiononY isaffine. Thisshowsthatthemeasurablefunctionx (cid:55)→ yk(dy|x)
is equivariant. But now observe that ave(k) is just the Markov kernel obtained by lifting
this measurable function to Stoch as described in Remark 2.2. It therefore follows that
ave(k) is equivariant by Proposition 3.5.
In this way, provided these conditions are met, a general strategy for obtaining a deter-
ministic G-equivariant neural network is to apply the following composition:
StochH(R X,R Y) sym StochG(X,Y) ave StochG (X,Y), (31)
φ φ det
where sym may be any symmetrisation procedure of the type shown.
Remark 4.2. Technically we should we should regard the composition (31) as a partial
function that is defined only on the subset of its domain for which the required integral
exists. However, in practice, this is not a major issue. One reason for this is that ave(k)
is always defined when k is deterministic, since then k(dy|x) is Dirac on (30) (see [Fri20,
Example 10.5]). As a result, even factoring in integrability caveats, it follows that ave◦sym
is always surjective if sym is, which provides a basic guarantee of its overall expressiveness.
Remark 4.3. The preceding discussion works very concretely in the category Stoch. A
more abstract treatment along the same lines seems possible for a general Markov category
that is representable in the sense of [Fri+23c]. However, we will not require this greater
generality in what follows.
5 A general methodology
5.1 Motivating idea
Suppose the restriction functor R admits a left adjoint E. For any pair of objects X and
φ
Y in CG, this yields a bijection between the morphisms in CH and CG as follows:
CH(R X,R Y)
∼=
CG(ER X,Y). (32)
φ φ φ
ThisallowsustomapH-equivariantmorphismsdirectlytoG-equivariantones,whichseems
promising for symmetrisation. However, (32) is not yet a symmetrisation procedure in the
31sense of Definition 4.4 since its output does not have the desired type X → Y. To address
this, we add a second step that simply precomposes by some arbitrary morphism
ω : X → ER X in CG.
φ
That is, given k : ER X → Y in CG obtained from (32), we return k◦ω : X → Y, which
φ
always has the desired type just by definition of composition. In this way, when a left
adjoint E exists, each ω gives rise to a symmetrisation procedure defined as follows:
CH(R X,R Y) Apply(32) CG(ER X,Y) Precomposebyω CG(X,Y). (33)
φ φ φ
To obtain the best results empirically, we want to choose ω “well” in some sense. For this,
wewillparameteriseω usinganeuralnetworkthatwewillthenoptimise, asdiscussedlater.
5.2 General approach
The requirement of a full left adjoint E is quite strong. The following result provides a
weaker condition that still yields bijections of the required form (32) without needing E
directly, thereby generalising the approach just described.
Theorem 5.1. Let C be a Markov category and φ : H → G a homomorphism in C. Suppose
a φ-coset map q : G → G/H exists, and let ∗/H be the action of G on G/H induced by
Proposition 3.4. Then for every (X,α ) and (Y,α ) in CG, there is a bijection
X Y
CH(R (X,α ),R (Y,α ))
∼=
CG((G/H,∗/H)⊗(X,α ),(Y,α )) (34)
φ X φ Y X Y
that sends k : R (X,α ) → R (Y,α ) in CH to the unique k♯ in C such that
φ X φ Y
Y
α
Y
Y
k
k♯
= α (35)
q X
(−)−1
G X
G X
which always exists, and is always a morphism (G/H,∗/H)⊗(X,α ) → (Y,α ) in CG.
X Y
Proof. See Section C.1 of the Appendix.
Remark 5.1. WeexplainhowTheorem5.1relatestheoreticallytothediscussioninSection
5.1. (It is not necessary to understand this in order to apply the result in practice.) Given
a left adjoint E, the bijections (32) amount to an equivalence of categories between the full
32image of R and the co-Kleisli category of the comonad ER .2 In classical settings (e.g.
φ φ
[MPC97, (1.6)]), we moreover have
∼
ER = (G/H,∗/H)⊗(−).
φ
The right-hand side here is part of the reader comonad on CG, and so (32) in turn implies
an equivalence of categories between the full image of R and the co-Kleisli category of
φ
the reader comonad. Theorem 5.1 in effect establishes this equivalence directly, without
needing to obtain E. By substituting the reader comonad for ER , we may carry out the
φ
steps (33) just as before, where our precomposition morphism now has the form
ω : (X,α ) → (G/H,∗/H)⊗(X,α ) in CG. (36)
X X
Thisapproachisbothmoregeneralandmoreconvenientthanconstructingafullleftadjoint,
and we will use it as the basis for the methodology we develop in what follows.
Technically, it does not follow from the statement of Theorem 5.1 alone that the bijec-
tions (34) do amount to an equivalence of categories as stated here. It is also necessary
to show that these bijections respects function composition, and so define a functor. For
theoretical interest, we show this as Corollary C.1 in the Appendix.
Remark 5.2. Aφ-cosetmapq : G → G/H oftenadmitsaright-inverse (orsection), which
is namely a morphism s : G/H → G in C such that
q◦s = id .
G/H
For example, a right-inverse of the coset map g (cid:55)→ gH from Example 3.21 selects a repre-
sentative element of each coset in G/H. This is useful computationally, since by attaching
s to the G-input on both sides of (35), we may write k♯ explicitly as follows:
Y
α
Y
Y k
α
k♯ = X (37)
(−)−1
G/H X
s
G/H X
Ingeneraltheremaybemanychoicesofs,buteachonewillproducethesamek♯. Moreover,
s need not be equivariant in any sense, so need not be a morphism in CG.
Remark 5.3. If C admits all orbit maps, then it is possible to obtain a left adjoint to R
φ
on all of CH, rather than just on its full image. We omit the proof of this, but the result
2See[Per21,Section5.3]foranintroductiontocomonads,includingthereadercomonadmentionedbelow.
33follows using very similar ideas as the proof of Theorem 5.1. Compared with (32), this
yields a natural bijection
CH(Z,R Y) ∼ = CG(E Z,Y)
φ φ
forallZ inCH, ratherthanjustwhenZ = R X. Thismayyieldinterestingpossibilitiesfor
φ
symmetrisation: for instance, we could generalise (33) to start with a morphism Z → R Y
φ
in CH, where now Z may be any object in CH, and in the second step precompose instead
by a morphism ω : X → E Z in CG. We leave this for future work to explore.
φ
5.3 Obtaining a precomposition morphism
Suppose Theorem 5.1 applies. To obtain an overall symmetrisation procedure, it remains
to select a precomposition morphism ω of the form (36). For this, we will set
G/H X
G/H X
γ
ω := (38)
X
X
where γ may be any morphism (X,α ) → (G/H,∗/H) in CG. By definition of composition
X
in CG, this means ω has the required type from (36) also.
Remark 5.4. We will denote by sym the overall symmetrisation procedure that first
γ
applies (34) and then precomposes by (38). End-to-end, this has the following type:
sym
CH(R (X,α ),R (Y,α )) γ CG((X,α ),(Y,α )).
φ X φ Y X Y
However, notice that same choice of γ may be reused across more than one (Y,α ). We
Y
will abuse notation slightly by denoting every such procedure using the same symbol sym ,
γ
even though technically these are distinct functions when their codomains differ.
The choice of (38) sacrifices some generality, since not every morphism in CG of the
form (36) can be expressed in this way. Our reason for this restriction is that it is sufficient,
and for positive Markov categories necessary, to ensure that the overall procedure is stable,
as the next result shows. As discussed in Section 4.4, stability in turn means the procedure
is surjective and idempotent, both of which are desirable for machine learning applications.
Proposition 5.1. Assuming Theorem 5.1 applies, the procedure sym is stable for every
γ
choice of γ. Conversely, if C is positive, then every instance of (33) that is stable can be
obtained as sym for some choice of γ.
γ
Proof. See Section C.2 of the Appendix.
Remark 5.5. As a morphism in CG, we require γ already to be G-equivariant. This in
effect pushes back the problem of symmetrisation to the choice of γ, which mirrors the
situation for deterministic symmetrisation approaches also [Pun+22; Kab+23; Kim+23].
Previous work has assumed γ is given directly in the form of an intrinsically equivariant
34neural network. However, our formalism suggests an alternative, recursive procedure for
obtaining γ, which adds a new layer of flexibility. In particular, we can set
γ := sym (m)
γ1
for some m : R (X,α ) → R (G/H,∗/H) in CH, where now γ : (X,α ) → (G/H,∗/H)
φ X φ 1 X
in CG. If desired, γ could itself be the result of recursive symmetrisation, and so on.
1
Of course, this still ultimately requires a base case, for which an intrinsically equivariant
neural network could be used. In Section 6, we also provide various examples of very
simplistic choices that provide a default option in cases where a more complex choice is not
available or not desired. We expect that in many cases the recursive approach will lead to
a more expressive γ than the base case alone, just as previous work on symmetrisation has
shownimprovedperformancecomparedwithintrinsicallyequivariantbaselines. Weprovide
empirical evidence of this in Section 7 below.
5.4 End-to-end procedure
Wenowsummarisethecompletestepsrequiredtosymmetriseamorphismk : R (X,α ) →
φ X
R (Y,α ) in CH along a homomorphism φ : H → G as described in this section.
φ Y
1. Obtain a φ-coset map q : G → G/H in C
2. Obtain a right-inverse s : G/H → G of q in C
3. Determine the action ∗/H : G⊗G/H → G/H induced by Proposition 3.4
4. Obtain a morphism γ : (X,α ) → (G/H,∗/H) in CG, either recursively or from
X
some base case
5. Return sym (k) : (X,α ) → (Y,α ) in CG computed as follows:
γ X Y
Y
α
Y
k
α
X
(−)−1 (39)
s
γ
X
Here the last diagram is obtained simply by attaching our precomposition morphism (38)
to k♯ as obtained using s in (37). More generally, k♯ is still defined even without s, although
then only implicitly by the expression (35) which makes its computation more difficult.
35Remark 5.6. By Examples 3.19 and 3.20 and Theorem 3.1, a φ-coset map exists for every
homomorphism φ in Set, Top, and TopStoch. This procedure is therefore always applicable
in all of these cases. For other Markov categories such as Stoch, the procedure still applies
whenever we can find a φ-coset map. This may not always be possible, although we suspect
that any such cases would be somewhat pathological and of less interest practically.
Example 5.1. Suppose H := I is the trivial group and φ is the unique homomorphism
I → G.
By identifying CI with C as in Example 4.1, symmetrising along φ allows us to convert
arbitrary morphisms in C to ones that are G-equivariant. Example 3.22 shows that id
G
is always a φ-coset map, and this is trivially its own right-inverse. Moreover, by Example
3.26, the canonical action ∗/I is just the action of G on itself by left multiplication. As
such, in this case, all that is required is to obtain a base case γ : (X,α ) → (G,∗) in CG.
X
We give some default options for specific groups and actions in Section 6.
Remark 5.7. If C = Stoch, so that its morphisms are actual Markov kernels, then a
sampling procedure for (39) may be read off directly. Suppose that G is a group acting on
X and Y in Meas, and that α and α are obtained by lifting these actions to Stoch via
X Y
Proposition 3.5. Given x ∈ X, we may then sample from sym (k)(dy|x) by sampling
γ
Γ ∼ γ(dc|x) G ∼ s(dg|Γ) Y ∼ k(dy|G−1·x) Y′ := G·Y
and returning Y′. The same story also holds for TopStoch.
5.5 Comparison with deterministic methods
Various existing methods for deterministic symmetrisation can be expressed as instances of
the framework we have described here. Consider again the setup from Example 5.1, where
φ is the trivial homomorphism I → G. When C = Set, all the morphisms involved are just
functions, and (39) becomes
sym (k)(x) = γ(x)·k(γ(x)−1·x), (40)
γ
which recovers the canonicalisation approach of [Kab+23, (2)].3 Likewise, suppose C =
Stoch and that the actions on X and Y are obtained by lifting the actions of some group G
in Meas according to Proposition 3.5. Moreover, suppose that k = k is the Markov kernel
f
obtained by lifting some measurable function f : X → Y as described in Remark 2.2. By
computing sym (k ) and then averaging its output over Y as described in Section 4.6, we
γ f
obtain a Markov kernel whose distribution at x ∈ X is Dirac on the following:
(cid:90) (cid:90)
sym (k )(dyˆ|x) = g·f(g−1·x)γ(dg|x), (41)
γ f
by the law of the unconscious statistician. Provided the action on Y is affine, Proposition
4.1 then shows that this is equivariant. This recovers the approach of [Kim+23, (4)], who
3Forothersubgroupinclusionsφ:H →Gwheresisnontrivial,sym (k)alsorecoverstheirmoregeneral
γ
setup [Kab+23, Theorem 3.1], with our s◦γ playing the role of their h.
36consider the case where G is compact and γ admits an equivariant (in the sense of (15))
conditional density with respect to the Haar measure on G. In turn, [Kim+23, Proposition
1] show that frame averaging [Pun+22, (3)] is an instance of their approach, where γ(dg|x)
is uniform on the set produced by the frame at the value x. The right-hand side of (41) also
recovers the (equivariant) weighted frames approach of [DLS24, Remark 3.4], explicating
its relationship to the approach of [Kim+23]. Finally, letting G := S be the group of
n
permutations of {1,...,n}, with γ(dg|x) uniform on S , and equipping Y with the trivial
n
action, (41) becomes
1 (cid:88)
k(σ−1·x),
n!
σ∈Sn
which recovers Janossy pooling [Mur+19b, Definition 2.1], an early example of a symmetri-
sation procedure in the machine learning literature.
The framework we have developed here therefore shows that all these existing methods
canberecoveredasspecialcasesofageneralprinciplebasedonTheorem5.1. Wealsoobtain
various possibilities for generalisation. For example, rather than averaging the output of
sym (k ) as (41), we can use the symmetrised kernel sym (k) directly as a model that
γ f γ
is overall stochastic. This may be useful in contexts where uncertainty quantification is
required, or when Y is nonconvex, and so averaging is not meaningful.
6 Examples
We now show how implement the steps described in Section 5.4 across for a variety of ho-
momorphisms φ : H → G. In all cases, φ amounts to some form of a subgroup inclusion.
Conceivably there are situations where other homomorphisms are of interest, but inclusions
already cover a wide range of practical use-cases. In addition to the other necessary compo-
nents, wewillprovidevariousexamplesofγ thatcouldbeusedasbasecasesintherecursive
proceduredescribedinRemark5.5. Thesearebynomeanstheonlychoice, andforinstance
an intrinsically equivariant neural network could be used instead for this purpose if desired.
Several of the following examples consider an abstract Markov category C. In some
other cases, we take C := Set. However, we do so mainly to simplify our presentation: Meas
or Top would work just as well, and in turn the various components we describe (coset
maps, right-inverses, etc.) lift to Stoch or TopStoch by Proposition 3.5,4 and can therefore
be used directly for stochastic symmetrisation also.
Example 6.1. Let G be a compact group, and consider the unique homomorphism
I → G
out of the trivial group. Symmetrising along this sends an arbitrary morphism in C to one
that is G-equivariant. By Example 5.1, all we require here is a choice of base case
γ : (X,α ) → (G,∗)
X
4Technically, Proposition 3.5 does not establish that right-inverses also lift, but this follows straightfor-
wardly from functoriality of the inclusions Meas→Stoch and Top→TopStoch.
37in CG. A default choice for this that applies for any (X,α ) in CG is to take
X
(G,∗)
(G,∗)
λ
γ := (42)
(X,α )
X
(X,α )
X
where λ : I → G is the Haar measure of G. From Definition 3.5, it follows straightforwardly
that λ is always a morphism (I,ε) → (G,∗) in CG. It therefore follows that γ here has the
type shown in (42) just by the definition of composition. In this way, we obtain a general
symmetrisation procedure for all compact groups, which is always available regardless of
the choice of (X,α ).
X
Example 6.2. InSet, letT denotethetranslationgroup, namelyRd equippedwithvector
d
addition. We consider how to symmetrise along the unique homomorphism
I → T ,
d
which allows us to convert an arbitrary morphism in Set into one that is T -equivariant.
d
By Example 5.1, all we require here is a choice of base case
γ : (X,α ) → (T ,+)
X d
in SetT d. This will depend on the choice of (X,α ). A common situation takes X := Rd×n,
X
and obtains α by columnwise addition. An obvious γ is then the columnwise mean
X
n
1 (cid:88)
γ(x ,...,x ) := x , (43)
1 n i
n
i=1
which is easily verified to be equivariant.
Example 6.3. From Example 3.23, a semidirect product always comes equipped with an
inclusion homomorphism:
i : N → N ⋊ H.
N ρ
Symmetrising along i allows us to convert N-equivariant morphisms to ones that are
N
(N ⋊ H)-equivariant. For this:
ρ
• The projection p is an i -coset map by Example 3.23
H N
• The other inclusion i is right-inverse of p (as may be checked)
H H
• The coset action is ∗/N defined in Example 3.27
All that remains then is to find a base case
γ : (X,α ) → (H,∗/N) (44)
X
38in CN⋊ ρH. When H is compact, a default choice of γ is available using its Haar measure λ
in a similar way to Example 6.1.5 Specifically, we can take:
(H,∗/N)
(H,∗/N)
λ
γ :=
(X,α )
X
(X,α )
X
By the defining property of λ from Definition 3.5 and inspection of the definition of ∗/N,
it is seen that λ is a morphism (I,ε) → (H,∗/N) in CN⋊ ρH, and so it follows that γ as
defined here indeed has the desired type (44). This approach works without assumptions
on N, which may be noncompact. As a result, symmetrisation can be performed in a fully
compositional way in this case: if we know how to obtain N-equivariant morphisms and H
is compact, then we know how to obtain (N ⋊ H)-equivariant morphisms also.
ρ
When C = Set, this recovers [Kab+23, Theorem 3.2]. Indeed, recall from Proposition
3.2 that every action of a semidirect product N ⋊ H can be decomposed into a H-action
ρ
followed by an N-action. Moreover, Proposition 3.3 shows that (N ⋊ H)-equivariance
ρ
reduces to N- and H-equivariance separately with respect to these decomposed actions.
But now the definition of ∗/N in Example 3.27 is already decomposed in this way, where
the N-action is trivial. As such, the base case γ here is required to be H-equivariant and
N-invariant, just as in [Kab+23]. However, in the context of Set, a Haar measure λ does
not exist unless G is trivial, and so the default option we provided is no longer available.
By considering a more general C, we therefore obtain a more flexible approach. We can also
apply this same idea more generally for stochastic symmetrisation too.
Example 6.4. In addition the the inclusion i considered in Example 6.3, a semidirect
N
product comes equipped with an inclusion homomorphism
i : H → N ⋊ H
H ρ
as defined in Example 3.23. Symmetrising along i allows us to convert H-equivariant
H
morphisms to (N ⋊ H)-equivariant ones. The situation here is dual to Example 6.3:
ρ
• The projection p is an i -coset map by Example 3.23
N H
• The other inclusion i is a right-inverse of p (as may be checked)
N N
• The coset action is ∗/H as defined in Example 3.27
To implement our procedure, we therefore only need to find a base case
γ : (X,α ) → (N,∗/H)
X
inCN⋊ ρH. Unlikewhensymmetrisingviatheotherinclusioni ,ageneral-purposeapproach
N
(that leverages, say, compactness) seems less forthcoming here, but case-by-case choices are
5It holds that G ∼= I ⋊ εG as groups in C, and so our approach for compact groups in Example 6.1 is
really a special case of the approach here.
39stillpossible. Forexample,inSet,considerthespecialEuclideangroupSE(d) = T ⋊ SO(d)
d ρ
defined in Example 3.14 (the case of E(d) is essentially the same). Then ∗/H becomes
(t,Q)·t′ = t+Qt′
for t,t′ ∈ T and Q ∈ SO(d). A common situation takes X := Rd×n and obtains α in a
d X
columnwise fashion, so that
(t,Q)·(x ,...,x ) := (Qx +t,...,Qx +t).
1 n 1 n
Intuitively, X here is thought of as a cloud of n points x ∈ Rd, and SE(d) acts by rotating
i
and then translating this cloud rigidly. In this case, it is straightforward to check that the
columnwise mean (43) is again equivariant with respect to α and ∗/H, and so provides
X
a suitable γ.6 Substituting this into (39) recovers a standard trick from the literature that
involves first subtracting off the centroid of a point cloud so that it is centered at the origin,
thenapplyingsomeH-equivariantneuralnetwork,beforefinallyaddingthecentroidbackon
to the result [Gar+21, Section 4], [Hoo+22, Section 3.1], [Pun+22, Section 3.1], [Kim+23,
Section 2.2]. This has previously been justified in an ad hoc way, but arises naturally from
the same underlying principle expressed in Theorem 5.1 as the other examples we consider.
Example 6.5. Rather than symmetrising via N or H as in the previous Examples, it is
also possible symmetrise directly along the unique homomorphism
I → N ⋊ H
ρ
intoasemidirectproduct. Aprocedureofthiskindallowsustoobtain(N⋊ H)-equivariant
ρ
morphisms from arbitrary morphisms in C. By Example 5.1, all we require in this instance
is a choice of base case
γ : (X,α ) → (N ⋊ H,∗) (45)
X ρ
inCN⋊ ρH. FromthedefinitionofthediagonalactioninExample3.8,aswellasthedefinition
of the semidirect product multiplication in Definition 3.6, it is easily checked that
(N ⋊ H,∗) = (N,∗/H)⊗(H,∗/N)
ρ
A general approach to obtaining γ is therefore to take
(N ⋊ H,∗) (N,∗/H) (H,∗/N)
ρ
γ γ
N H
γ :=
(X,α ) (X,α )
X X
whereγ andγ aremorphismsinCN⋊ ρH ofthetypesshown. Itthenfollowsautomatically
N H
thattheright-handsidehasrequiredtypeshownontheleft. FortheEuclideangroupinSet,
this recovers the approach of [Kab+23, (9), (10)], who obtain γ and γ using intrinsically
N K
equivariant neural networks.
6SimilartothepreviousExample,itholdsthatT
d
∼=T d⋊ εI asgroupsinC,andsoExample6.2isreally
a special case of the approach here.
40In a sense, this approach is the easiest possible here, since any other γ of the form
(45) immediately gives rise to suitable choices of γ and γ by projecting onto N and H
N H
respectively. Notice that this also is a stronger requirement than in Examples 6.3 and 6.4,
which required either γ or γ of this form, but not both. To some extent, this additional
N H
complexity is to be expected given we are now symmetrising arbitrary morphisms in C,
whereaspreviouslyweweresymmetrisingmorphismsthatwerealreadyN-orH-equivariant.
Example6.6. Interestingly,wecanevenobtainequivariancewithrespecttothefullgeneral
linear group GL(d,R). In Set, consider the inclusion homomorphism
O(d) → GL(d,R).
SymmetrisingalongthisconvertsO(d)-equivariantmorphismstoGL(d,R)-equivariantones.
Here a coset map GL(d,R) → PD(d) is given by A (cid:55)→ AAT from Example 3.24, and its
inducedcosetaction∗/O(d)iscomputedasA·P = APAT byExample3.28. Arightinverse
of the coset map may also obtained straightforwardly by the Cholesky decomposition. We
therefore only need a base case
γ : (X,α ) → (PD(d),∗/O(d))
X
inSetGL(d,R). ConsiderthespecificcaseX := Rd×n,whereα isobtainedbyleft-multiplication,
X
namely A·B := AB. For this we may take γ(B) := BBT, which is equivariant since
γ(A·B) = ABBTAT = Aγ(B)AT = A·γ(B).
As for other symmetrisation procedures, this could be used compositionally. For example,
we could symmetrise along the inclusion homomorphisms
I → O(d) → GL(d,R)
in sequence to obtain an GL(d,R)-equivariant morphism starting from an arbitrary mor-
phism in Set (as opposed to one that is already O(d)-equivariant). We are not sure of
the practical utility of this example, as we are unaware of applications where GL(d,R)-
equivariance has been considered, but believe it provides as an interesting demonstration
of the flexibility of our approach.
7 Application and numerical results
We now describe one concrete application of our stochastic symmetrisation approach. In
particular, we apply it to the method of [Kim+23], which has obtained state-of-the-art
results for deterministic symmetrisation across a variety of tasks. Although their overall
model is deterministic, [Kim+23] require a stochastically equivariant neural network as a
crucial subcomponent, and use an intrinsically equivariant neural network for this purpose.
We show how this component can instead be obtained using our methodology, which allows
us apply more flexible off-the-shelf architectures that are not subject to equivariant con-
straints. Empirically, this leads to improved performance over the intrinsic approach on a
synthetic example. More generally, we believe this case study also demonstrates the greater
41conceptual and notational clarity that arises by framing this problem in terms of Markov
categories, which may be useful for other machine learning applications also.
Other use-cases for stochastic symmetrisation appear possible beyond the one we con-
sider here. In particular, our methodology could also be used more directly to obtain an
equivariant model that is overall stochastic, rather than as a component of a deterministic
symmetrisation procedure. This may be of interest in applications such as deep generative
modelling, or where uncertainty quantification is required. We leave this for future work.
7.1 Architecture
We describe the approach of [Kim+23] within the context of our framework. For concrete-
ness, we formalise this entirely in Stoch, which means we think of all model components as
Markov kernels, including the deterministic neural networks that we use. Given i.i.d. sam-
ples from some distribution p(dx,dy) on X ⊗Y, the overall goal is to learn a deterministic
f : X → Y that we will use as a predictor. We assume that Y := Rd, and that both X
and Y are equipped with the actions of some group G in Stoch, and we would like f to be
equivariant with respect to these actions. We think of f as depending on some additional
parameters that correspond to neural network weights, although to streamline notation we
keep these implicit in what follows.
Baseline Theapproachof[Kim+23]ineffectappliesthestrategydescribedinSection4.6
above: they first symmetrise an unconstrained neural network in Stoch, and then average
over the output to obtain a deterministic predictor. More succinctly, they obtain f via:
k := sym (nn )
γ k
f := ave(k).
Here nn : X → Y is some unconstrained neural network. In both [Kim+23] and in our
k
own experiments, this component is deterministic, although this is not strictly necessary.
Likewise, γ : (X,α ) → (G,∗) is a morphism in StochG. For this component, [Kim+23]
X
use the following architecture:
G
pr
G
γ := nn γ (46)
X
η
X
Here, at a high level:
• nn is some deterministic neural network that forms the “backbone” of γ
γ
• η is some noise distribution that allows γ overall to be stochastic
• pr projects its input onto G, which is necessary because many groups live on a mani-
fold, whereas neural networks typically output values in some Euclidean space.
42If these components are all suitably equivariant, it follows immediately that their composi-
tion γ is too. To ensure this, [Kim+23] obtain nn by using some intrinsically equivariant
γ
architecture off-the-shelf. They also provide choices of η and pr suitable for several specific
groups of interest [Kim+23, Section 2.2].
Our approach Rather than relying on an intrinsically equivariant neural network, we
obtain γ itself through stochastic symmetrisation. All up, our architecture is as follows:
γ := sym (γ )
γ1 0
k := sym (nn )
γ k
f := ave(k)
This corresponds to symmetrising nn using the recursive approach described in Remark
k
5.5, and then again averaging the result as in Section 4.6. Here nn : X → Y is again
k
some unconstrained neural network, and γ : (X,α ) → (G,∗) is again some morphism in
1 X
StochG, which means that γ is a morphism in this category also. However, now γ : X → G
0
is an unconstrained morphism in Stoch, and so can be chosen more freely.
7.2 Training objective
We discuss how to train models of this kind. This has been considered by [Kim+23, (55)],
as well as [Mur+19b, (10)] and [Mur+19a, (9)], and the same underlying idea works here as
well, although takes on a different form in our notation. Overall, we would like to learn the
parametersofourpredictorf bystochasticgradientdescent. RecallfromDefinition4.6that
(cid:82)
to sample from f(dyˆ|x) = ave(k)(dyˆ|x) requires computing the integral yˆk(dyˆ|x). This
is usually intractable, which poses a challenge for obtaining unbiased gradient estimates of
the expected loss. However, for a real-valued loss function ℓ that is convex in its second
argument, Jensen’s inequality yields the following upper bound:
(cid:90) (cid:90) (cid:18) (cid:90) (cid:19)
ℓ(y,yˆ)f(dyˆ|x)p(dx,dy) = ℓ y, yˆk(dyˆ|x) p(dx,dy)
(cid:90) (cid:90)
≤ ℓ(y,yˆ) k(dyˆ|x)p(dx,dy) (47)
Following the earlier work mentioned, we optimise (47) in the parameters of the model
using unbiased estimates of its gradient obtained by Monte Carlo. This strategy may be
applied whenever k is reparameterisable [KW22, Section 2.4]. For both the baseline and
our approach, this is the case if η (which is the only source of randomness here) is some
fixed noise distribution that does not depend on the parameters of the model, as we assume
in our experiments.
Remark 7.1. We briefly explain why (47) is reasonable to use as a training objective.
Observe that this inequality becomes exact if k(dy|x) is Dirac for all x ∈ X, or equivalently
if k is deterministic [Fri20, Example 10.5]. It follows that if (47) is globally optimised over
the parameters of the model, the resulting k (which may be stochastic) will perform at least
as well as the best-performing deterministic k the model can express. But now k has at
least two opportunities to become deterministic, which occurs if:
43• nn is deterministic and G-equivariant. In this case k = sym (nn ) = nn is also
k γ k k
deterministic, where the second equality holds because sym is stable (Proposition
γ
5.1). This generalises an observation made by [Mur+19b, Section 2.3].
• nn and γ are both deterministic. In this case, k = sym (nn ) is deterministic be-
k γ k
cause sym restricts to a symmetrisation procedure in the determinsitic subcategory
γ
Stoch . This in effect reduces to the canonicalisation approach of [Kab+23].
det
This “double robustness” suggests that, under typical circumstances, it will be fairly easy
foramodeltoexpress(oratleastapproximate)arichfamilyofdeterministick, whichgives
reason to anticipate good performance overall even when optimising the bound (47) rather
than the true expected loss.
7.3 Numerical example
Weappliedthissetuptoasyntheticprobleminvolvingmatrixinversion. Forvariouschoices
of d ∈ N, we took both X and Y to be the general linear group GL(d,R), and our goal
was to learn the inversion map A (cid:55)→ A−1. We took G := O(d) to be the orthogonal group.
Observe that if Q is orthogonal, then we have straightforwardly
(QA)−1 = A−1Q−1 = A−1QT,
sothattheinversionmapisequivariantwhenO(d)actsonX byleftmultiplicationandonY
by right multiplication by the inverse (or equivalently the transpose, which is much cheaper
to compute). We therefore sought to make our predictor f equivariant with respect to
these actions also. We selected this task because it involves learning a difficult computation
that can be easily considered across a range of dimensions. We emphasise that is only one
example, and further empirical work is needed to establish that the improvements reported
here also carry over to other tasks. Nevertheless, we believe the results here serve as an
interesting proof of concept for our approach.
Baseline We compared against the method of [Kim+23], using the same choices as they
described for their experiments involving the orthogonal group. Specifically, η was a d-
dimensional standard Gaussian, and pr was the Gram-Schmidt procedure. We took nn
k
to be an MLP with two hidden layers of 500 hidden units and tanh activations.7 For the
intrinsicially equivariant nn , we tried two architectures, namely the EMLP approach of
γ
[FWW21], and the “scalars” method of [Vil+21]. For the EMLP, we used the same default
architectureinvolvingbilinearandgatednonlinearlayersasin[FWW21,Figure4],whilewe
used an MLP with tanh activations to obtain the unconstrained scalar functions of [Vil+21,
(6)] (f in their notation). In both cases, nn had one hidden layer with 500 hidden units,
t γ
following [Kim+23] in making this smaller than nn .
k
7Wetookforgrantedthatnn producedvaluesinGL(d,R). Thisisreasonableforaneuralnetworkwith
k
d2 outputs trained via stochastic gradient descent, since GL(d,R) is dense in Rd×d.
44Our method Forourapproach, wetooknn , η, andprtobethesameasforthebaseline.
k
For γ , we again used the architecture (46), except that now its “backbone” neural network
0
nn was an unconstrained MLP. For γ , which needed to be equivariant, we tried two
γ0 1
approaches. The first again used the architecture (46), with its “backbone” nn using the
γ1
“scalars” approach of [Vil+21, (6)]. The second used the Haar measure of the orthogonal
group to obtain γ directly as in (42). In the “scalars” case, both nn and nn had one
1 γ0 γ1
hidden layer with 250 hidden units, where we made these narrower to account for the fact
that we required two networks here instead of one. In the Haar case, nn was wider, with
γ0
500 hidden units. We used tanh activations in all cases.
Additional baselines As a further baseline, we trained nn directly, without applying
k
any symmetrisation. We also trained a version of the architecture of [Kim+23] where γ was
directly obtained using the Haar measure as in (42). Since these models did not require an
additional neural network for γ, we made nn deeper than before, using three hidden layers
k
of 500 units instead of two. We again used tanh activations in both cases.
Training and testing details Wetookp(dx,dy)tobethedistributionof(X,Y), where
X is a random element of GL(n,R) with i.i.d. standard Gaussian entries and Y := X−1.
We did not use a finite dataset, but instead sampled new training and testing examples
on the fly. In this way, we sought to determine the overall expressive power of the models
without concerns about overfitting. We trained all models using the loss function
ℓ(y,yˆ) := (cid:13) (cid:13)y−1yˆ−I d(cid:13) (cid:13)
F
,
where ∥·∥ denotes the Frobenius norm, and I is the d × d identity matrix. This loss
F d
measures how close yˆ is to y = x−1 in an “operational” sense. It is easily checked that
L(y,yˆ) is convex in yˆ, and accordingly we trained our symmetrised models using the upper
bound (47) as described earlier. For the baseline unsymmetrised MLP, we optimised the
expected loss directly. We used the Adam optimiser [KB14] with default hyperparameters
an a learning rate of 10−4 in all cases, and performed 105 gradient steps for each model.
At test time, given a test input x ∈ X, we computed the prediction of each symmetrised
modelf(dyˆ|x) = ave(k)(dyˆ|x)approximatelyusingMonteCarlo,averagingover100samples
drawn i.i.d. from k(dy|x).
Results Figure1showstheaveragetestlossaftertrainingforeachmodelacrosstherange
of dimension d that we considered. Our method using the “scalars” architecture for nn
γ1
outperformed all other models, including the baseline method of [Kim+23] that used the
same (but larger) architecture for nn . This indicates that the greater flexibility obtained
γ
by symmetrising γ, rather than using an intrinsically equivariant neural network, can lead
to improved performance. In comparison, our method using the Haar measure for γ did
1
not perform as well, but did perform comparably or better than the baseline method of
[Kim+23] that used an EMLP for its nn . This is still interesting, as in this case our model
γ
did not make use of an intrinsically equivariant neural network as a subcomponent, whereas
the baseline did. In other words, our recursive symmetrisation approach (Remark 5.5) can
stilloutperformintrinsicapproachesevenwhenusingahighlysimplisticbasecase(theHaar
45Unsymmetrised
Kim et al. (Haar)
7
Kim et al. (EMLP)
Us (Haar)
6 Kim et al. ("scalars")
Us ("scalars")
5
4
3
2
1
0
0 10 20 30 40 50 60
Dimension (d)
Figure 1: Average test loss obtained after training each model across the range of problem
dimensionsdconsidered. ThefinalEMLPdatapointismissingbecauseofthelargememory
requirements of its bilinear layers, which exceeded the capacity of our hardware.
measure, in this case). Finally, the remaining two additional baselines performed the worst,
which is expected given their more simplistic architectures.
Acknowledgements
This work would not have been possible without John Cornish, Catherine Macaulay, and
RachaelMorey,whosupportedmethroughoutandkeptmefocussedandontrack. Iamalso
verygratefultoPaoloPerroneforhisencouragement,enthusiasm,andhelpfulconversations.
Finally, my many thanks to Sam Staton and his research group more generally for creating
awelcomingandfriendlycommunitythathasonlymademyenthusiasmforthistopicgrow.
References
[AA22] NamrataAnandandTudorAchim.ProteinStructureandSequenceGeneration
with Equivariant Denoising Diffusion Probabilistic Models. 2022. arXiv: 2205.
15019.
[Bre+23] Johann Brehmer, Joey Bose, Pim de Haan, and Taco S Cohen. “EDGI: Equiv-
ariant Diffusion for Planning with Embodied Agents”. In: Advances in Neu-
ral Information Processing Systems. Ed. by A. Oh, T. Naumann, A. Glober-
son, K. Saenko, M. Hardt, and S. Levine. Vol. 36. Curran Associates, Inc.,
2023, pp. 63818–63834. url: https://proceedings.neurips.cc/paper_
files/paper/2023/file/c95c049637c5c549c2a08e8d6dcbca4b-Paper-
Conference.pdf.
46
ssol
tset
egarevA[Bro+17] Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre
Vandergheynst. “Geometric Deep Learning: Going beyond Euclidean data”.
In: IEEE Signal Processing Magazine 34.4 (2017), pp. 18–42. doi: 10.1109/
MSP.2017.2693418.
[Bro+21] Michael M. Bronstein, Joan Bruna, Taco Cohen, and Petar Velickovi´c. Geo-
metric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges. 2021.
arXiv: 2104.13478.
[Bro06] Ronald Brown. Topology and groupoids. 2006.
[BT20] Benjamin Bloem-Reddy and Yee Whye Teh. “Probabilistic symmetries and
invariant neural networks”. In: The Journal of Machine Learning Research
21.1 (2020), pp. 3535–3595.
[CJ19] Kenta Cho and Bart Jacobs. “Disintegration and Bayesian inversion via string
diagrams”.In:MathematicalStructuresinComputerScience 29.7(Mar.2019),
pp. 938–971. doi: 10.1017/s0960129518000488. url: https://doi.org/10.
1017%2Fs0960129518000488.
[CW16] Taco Cohen and Max Welling. “Group Equivariant Convolutional Networks”.
In: Proceedings of The 33rd International Conference on Machine Learning.
Ed. by Maria Florina Balcan and Kilian Q. Weinberger. Vol. 48. Proceedings
of Machine Learning Research. New York, New York, USA: PMLR, 20–22
Jun 2016, pp. 2990–2999. url: https://proceedings.mlr.press/v48/
cohenc16.html.
[DLS24] Nadav Dym, Hannah Lawrence, and Jonathan W. Siegel. Equivariant Frames
and the Impossibility of Continuous Canonicalization. 2024. arXiv: 2402.
16077 [cs.LG].
[FPR21] Tobias Fritz, Paolo Perrone, and Sharwin Rezagholi. “Probability, valuations,
hyperspace: Three monads on top and the support as a morphism”. In: Math-
ematical Structures in Computer Science 31.8 (Sept. 2021), pp. 850–897. issn:
1469-8072. doi: 10.1017/s0960129521000414. url: http://dx.doi.org/
10.1017/S0960129521000414.
[FR20] Tobias Fritz and Eigil Fjeldgren Rischel. “Infinite products and zero-one laws
in categorical probability”. In: Compositionality 2 (3 Aug. 2020). issn: 2631-
4444. doi: 10.32408/compositionality-2-3. url: https://doi.org/10.
32408/compositionality-2-3.
[Fri+23a] TobiasFritz,Tom´aˇsGonda,NicholasGauguinHoughton-Larsen,AntonioLoren-
zin, Paolo Perrone, and Dario Stein. “Dilations and information flow ax-
ioms in categorical probability”. In: Mathematical Structures in Computer
Science 33.10 (Oct. 2023), pp. 913–957. issn: 1469-8072. doi: 10.1017/
s0960129523000324.url:http://dx.doi.org/10.1017/S0960129523000324.
[Fri+23b] TobiasFritz,Tom´aˇsGonda,AntonioLorenzin,PaoloPerrone,andDarioStein.
Absolute continuity, supports and idempotent splitting in categorical probabil-
ity. 2023. arXiv: 2308.00651 [math.PR].
47[Fri+23c] Tobias Fritz, Tom´aˇs Gonda, Paolo Perrone, and Eigil Fjeldgren Rischel. “Rep-
resentable Markov categories and comparison of statistical experiments in
categorical probability”. In: Theoretical Computer Science 961 (June 2023),
p. 113896. doi: 10.1016/j.tcs.2023.113896. url: https://doi.org/10.
1016%2Fj.tcs.2023.113896.
[Fri20] Tobias Fritz. “A synthetic approach to Markov kernels, conditional indepen-
dence and theorems on sufficient statistics”. In: Advances in Mathematics 370
(Aug. 2020), p. 107239. doi: 10.1016/j.aim.2020.107239. url: https:
//doi.org/10.1016%2Fj.aim.2020.107239.
[FWW21] Marc Finzi, Max Welling, and Andrew Gordon Gordon Wilson. “A Practi-
cal Method for Constructing Equivariant Multilayer Perceptrons for Arbitrary
Matrix Groups”. In: Proceedings of the 38th International Conference on Ma-
chine Learning. Ed. by Marina Meila and Tong Zhang. Vol. 139. Proceedings
of Machine Learning Research. PMLR, 18–24 Jul 2021, pp. 3318–3328. url:
https://proceedings.mlr.press/v139/finzi21a.html.
[Gar+21] Victor Garcia Satorras, Emiel Hoogeboom, Fabian Fuchs, Ingmar Posner, and
Max Welling. “E(n) Equivariant Normalizing Flows”. In: Advances in Neu-
ral Information Processing Systems. Ed. by M. Ranzato, A. Beygelzimer, Y.
Dauphin, P.S. Liang, and J. Wortman Vaughan. Vol. 34. Curran Associates,
Inc., 2021, pp. 4181–4192. url: https://proceedings.neurips.cc/paper_
files/paper/2021/file/21b5680d80f75a616096f2e791affac6-Paper.
pdf.
[Gir82] Mich`ele Giry. “A categorical approach to probability theory”. In: Categorical
Aspects of Topology and Analysis.Ed.byB.Banaschewski.Berlin,Heidelberg:
Springer Berlin Heidelberg, 1982, pp. 68–85. isbn: 978-3-540-39041-1.
[Hoo+22] EmielHoogeboom,V´ıctorGarciaSatorras,Cl´ementVignac,andMaxWelling.
“Equivariant Diffusion for Molecule Generation in 3D”. In: Proceedings of
the 39th International Conference on Machine Learning. Ed. by Kamalika
Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan
Sabato. Vol. 162. Proceedings of Machine Learning Research. PMLR, 17–23
Jul 2022, pp. 8867–8887. url: https://proceedings.mlr.press/v162/
hoogeboom22a.html.
[Kab+23] S´ekou-Oumar Kaba, Arnab Kumar Mondal, Yan Zhang, Yoshua Bengio, and
Siamak Ravanbakhsh. “Equivariance with Learned Canonicalization Func-
tions”.In:Proceedingsofthe40thInternationalConferenceonMachineLearn-
ing. Ed. by Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara En-
gelhardt, Sivan Sabato, and Jonathan Scarlett. Vol. 202. Proceedings of Ma-
chineLearningResearch.PMLR,23–29Jul2023,pp.15546–15566.url:https:
//proceedings.mlr.press/v202/kaba23a.html.
[Kal02] Olav Kallenberg. Foundations of Modern Probability. 2nd ed. Springer, 2002.
[KB14] Diederik P. Kingma and Jimmy Ba. “Adam: A Method for Stochastic Opti-
mization”. In: arXiv e-prints, arXiv:1412.6980 (Dec. 2014), arXiv:1412.6980.
doi: 10.48550/arXiv.1412.6980. arXiv: 1412.6980 [cs.LG].
48[Kim+23] Jinwoo Kim, Tien Dat Nguyen, Ayhan Suleymanzade, Hyeokjun An, and Se-
unghoonHong.Learning Probabilistic Symmetrization for Architecture Agnos-
tic Equivariance. 2023. arXiv: 2306.02866 [cs.LG].
[KW22] DiederikPKingmaandMaxWelling.Auto-Encoding Variational Bayes.2022.
arXiv: 1312.6114 [stat.ML].
[Lei08] Tom Leinster. Doing without diagrams. https://www.maths.ed.ac.uk/
~tl/elements.pdf. Version 10 March 2008. Available online at University of
Edinburgh, School of Mathematics. Mar. 2008. url: https://www.maths.ed.
ac.uk/~tl/elements.pdf (visited on 04/01/2024).
[MP22] Sean Moss and Paolo Perrone. “Probability monads with submonads of deter-
ministic states”. In: Proceedings of the 37th Annual ACM/IEEE Symposium
on Logic in Computer Science. LICS ’22. ACM, Aug. 2022. doi: 10.1145/
3531130.3533355. url: http://dx.doi.org/10.1145/3531130.3533355.
[MP23] Sean Moss and Paolo Perrone. “A category-theoretic proof of the ergodic de-
compositiontheorem”.In:Ergodic Theory and Dynamical Systems 43.12(Feb.
2023), pp. 4166–4192. issn: 1469-4417. doi: 10.1017/etds.2023.6. url:
http://dx.doi.org/10.1017/etds.2023.6.
[MPC97] J.P. May, R.J. Piacenza, and M. Cole. Equivariant Homotopy and Cohomol-
ogy Theory: Dedicated to the Memory of Robert J. Piacenza. Regional con-
ference series in mathematics. American Mathematical Society, 1997. isbn:
9780821803197.url:https://books.google.co.uk/books?id=KOcZYVxkQO9C.
[Mur+19a] RyanMurphy,BalasubramaniamSrinivasan,VinayakRao,andBrunoRibeiro.
“Relational Pooling for Graph Representations”. In: Proceedings of the 36th
International Conference on Machine Learning. Ed. by Kamalika Chaudhuri
andRuslanSalakhutdinov.Vol.97.ProceedingsofMachineLearningResearch.
PMLR, Sept. 2019, pp. 4663–4673. url: https://proceedings.mlr.press/
v97/murphy19a.html.
[Mur+19b] RyanL.Murphy,BalasubramaniamSrinivasan,VinayakRao,andBrunoRibeiro.
“JanossyPooling:LearningDeepPermutation-InvariantFunctionsforVariable-
Size Inputs”. In: International Conference on Learning Representations. 2019.
url: https://openreview.net/forum?id=BJluy2RcFm.
[Per21] PaoloPerrone.Notes on Category Theory with examples from basic mathemat-
ics. 2021. arXiv: 1912.10642 [math.CT].
[Pun+22] Omri Puny, Matan Atzmon, Edward J. Smith, Ishan Misra, Aditya Grover,
HeliBen-Hamu,andYaronLipman.“FrameAveragingforInvariantandEquiv-
ariant Network Design”. In: International Conference on Learning Represen-
tations. 2022. url: https://openreview.net/forum?id=zIUyj55nXR.
[Rie17] Emily Riehl. Category theory in context. Courier Dover Publications, 2017.
49[RSP17] Siamak Ravanbakhsh, Jeff Schneider, and Barnab´as P´oczos. “Equivariance
Through Parameter-Sharing”. In: Proceedings of the 34th International Con-
ferenceonMachineLearning.Ed.byDoinaPrecupandYeeWhyeTeh.Vol.70.
ProceedingsofMachineLearningResearch.PMLR,June2017,pp.2892–2901.
url: https://proceedings.mlr.press/v70/ravanbakhsh17a.html.
[Sel10] P. Selinger. “A Survey of Graphical Languages for Monoidal Categories”. In:
New Structures for Physics. Springer Berlin Heidelberg, 2010, pp. 289–355.
doi: 10.1007/978-3-642-12821-9_4. url: https://doi.org/10.1007%
2F978-3-642-12821-9_4.
[SS22] Hisham Sati and Urs Schreiber. Equivariant principal infinity-bundles. 2022.
arXiv: 2112.13654 [math.AT].
[Vil+21] Soledad Villar, David W Hogg, Kate Storey-Fisher, Weichi Yao, and Ben
Blum-Smith. “Scalars are universal: Equivariant machine learning, structured
likeclassicalphysics”.In:Advances in Neural Information Processing Systems.
Ed.byA.Beygelzimer,Y.Dauphin,P.Liang,andJ.WortmanVaughan.2021.
url: https://openreview.net/forum?id=ba27-RzNaIv.
[Xu+22] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Ermon, and Jian
Tang. “GeoDiff: A Geometric Diffusion Model for Molecular Conformation
Generation”. In: International Conference on Learning Representations. 2022.
url: https://openreview.net/forum?id=PzcvxEMzvQC.
[Yar18] Dmitry Yarotsky. Universal approximations of invariant maps by neural net-
works. 2018. arXiv: 1804.10306 [cs.NE].
[Yim+23] Jason Yim, Brian L. Trippe, Valentin De Bortoli, Emile Mathieu, Arnaud
Doucet, Regina Barzilay, and Tommi Jaakkola. “SE(3) diffusion model with
application to protein backbone generation”. In: Proceedings of the 40th In-
ternational Conference on Machine Learning. Ed. by Andreas Krause, Emma
Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan
Scarlett. Vol. 202. Proceedings of Machine Learning Research. PMLR, 23–29
Jul 2023, pp. 40001–40039. url: https://proceedings.mlr.press/v202/
yim23a.html.
50We include here the proofs of various results, organised by their corresponding section from
the main text.
A Group theory in Markov categories
A.1 Proof of Proposition 3.1
Proof. For all measurable B ⊆ Y, g ∈ G, and x ∈ X, it follows that
(cid:90)
k(B|g·x) = p(y|g·x)µ(dy)
B
(cid:90)
= p(y|g·x)(g·µ)(dy)
B
(cid:90)
= p(g·y|g·x)µ(dy)
g−1·B
(cid:90)
= p(y|x)µ(dy)
g−1·B
= k(g−1·B|x)
= (g·k)(B|x),
where the third step uses the law of the unconscious statistician. (Here g−1·B := {g−1·y :
y ∈ B}.) By Example 3.11, this shows that k is equivariant.
A.2 Results on orbit maps
Proposition A.1. Let C be a Markov category, and suppose the following is a coequaliser
diagram in C:
f
X Y h Z
g
Iff, g, andh, aredeterministic, thenthisisalsoacoequaliserdiagraminC . Additionally,
det
if C is positive and f and g are deterministic, then h is deterministic.
Proof. For the first statement, suppose f, g, and h are deterministic, and let k : Y → W be
a morphism in C such that k◦f = k◦g. Then there is a unique morphism k′ : Z → W
det
in C such that k = k′◦h. Since h is a coequaliser, it is an epimorphism. By [FR20, Lemma
10.9], it follows that k′ is deterministic, and hence a morphism in C also.
det
For the second statement, suppose C is positive and f and g are deterministic. Then we
have
Z Z Z Z Z Z Z Z
h h h h h h h h
= f f = g g =
f g
X X X X
51where we use the fact that f and g are deterministic in the first and third steps. Since h is
a coequaliser, it follows that there exists a unique k : Z → Z ⊗Z in C such that
Z Z
Z Z
k
h h
= (48)
h
Y
Y
Marginalising out each output in turn and using the fact that h is epi (since it is a co-
equaliser), we obtain
Z Z
Z
k = k =
Z
Z Z
Since C is positive, [Fri+23a, Theorem 2.8] now implies that k = copy . Substutiting this
Z
into (48), it follows that h is deterministic.
Proposition A.2. Let C be a Markov category, and α : G⊗X → X an action and Y an
object in C. If q : X → X/G is an orbit map with respect to α, then q ⊗id : X ⊗Y →
Y
X/G⊗Y is an orbit map with respect to the action
X Y
α (49)
G X Y
Proof. To see that (49) is indeed an action, observe that it is just the diagonal action (see
Example 3.8) obtained from α and the trivial action ε. We now show the result in the
case that C is strictly monoidal, with the general case being similar but notationally more
complex. Since q is an orbit map, it is a coequaliser. Moreover, it is preserved by the
functor (−)⊗Y, and so the following is also a coequaliser diagram:
G⊗X ⊗Y
α⊗idY
X ⊗Y
q⊗idY
X/G⊗Y. (50)
ε⊗idY
Observe that α⊗id is just (49), and ε⊗id is just the trivial action on X ⊗Y. This
Y Y
shows that q⊗id is a coequaliser of the form required to be an orbit map with respect to
Y
(49). It remains to show that this is preserved by every functor (−)⊗Z. But this holds
because we have id ⊗id = id , and so the image of (50) under (−)⊗Z is just
Y Z Y⊗Z
G⊗X ⊗Y ⊗Z
α⊗idY⊗Z
X ⊗Y
q⊗idY⊗Z
X/G⊗Y ⊗Z,
ε⊗idY⊗Z
which is again a coequaliser diagram since q is an orbit map.
52A.3 Top has orbits
Proposition A.3. Every action in Top admits an orbit map.
Proof. We sketch the argument here, which uses standard ideas. Suppose α : G⊗X → X
is any action in Top. Let q : X → X/G be as defined in Example 3.17, where now X/G
is equipped with the final topology with respect to q, which is namely the finest topology
that makes q continuous. Then q becomes a coequaliser of α and ε by a similar argument
as was given in Example 3.17. (Slightly more care is needed in this case, because the final
topology is in general not the only possible topology on X/G that makes q continuous.)
For the preservation condition, given another topological space Y, we must show that
the following is also a coequaliser diagram:
(G⊗X)⊗Y
α⊗idY
X ⊗Y
q⊗idY
X/G⊗Y
ε
Now, certainly q is a surjection, and it is also an open map [Bro06, 11.1.2]. Likewise, id
Y
is an open surjection, and so it holds that q ⊗id is an open surjection too [Bro06, 4.2,
Y
Exercise 7]. Consequently X/G⊗Y must be equipped with the final topology with respect
to q ⊗id [Bro06, 4.2.4]. From here, it may be shown that q ⊗id is a coequaliser using
Y Y
essentially the same argument as for q itself.
A.4 Proof of Theorem 3.1
Our goal in this section is to prove that TopStoch admits all orbit maps. Since TopStoch is
theKleislicategoryofacertainmonadonTop[FPR21],thisalmost followsfromProposition
A.3. In particular, some straightforward diagram chasing shows that an orbit map exists
for every group action in TopStoch that is obtained by lifting some group action in Top
via Proposition 3.5. However, we are not sure whether all actions in TopStoch arise in this
way. This is because a deterministic Markov kernel is only required to be zero-one [Fri20,
Example 10.4], and in general not all zero-one Markov kernels can be obtained by lifting
some measurable function. (See [MP22, Example 3.9] for an example in the case of Meas
andStoch; asimilarideaappearstoholdforTopandTopStochalso.) Wethereforeprovidea
direct proof of the result here. Our approach takes its inspiration from [MP23, Proposition
3.7], which applies for Stoch. We show that, in the topological setting, this construction
plays nicely with products, which is required for the preservation condition of orbit maps.
Throughoutthissection,weusetheusualnotation1 todenotetheindicatorfunctionof
A
a measurable set A. We also recall that, for X a topological space, a function f : X → [0,1]
is lower semicontinuous if for every t ∈ [0,1] the following set is open:
f−1((t,1]) = {x ∈ X | f(x) > t}.
A.4.1 The invariant topology
Let α : G ⊗ X → X be an action in TopStoch. We will keep α fixed throughout this
subsection. Given an open U ⊆ X, we will say that U is invariant if it holds that
α(U|g,x) = 1 for all x ∈ U and g ∈ G.
We then have the following.
53Proposition A.4. The invariant open subsets of X form a topology.
Proof. Certainly X and ∅ are invariant (∅ vacuously so). Given a collection of invariant
(cid:83)
open U ⊆ X, letting U := U , we have that U is open, and moreover if g ∈ G and x ∈ U
i i i i
for some i, then
α(U|g,x) ≥ α(U |g,x) = 1,
i
whichmeansU isinvariant. Similarly,giveninvariantopensetsU andU ,theirintersection
1 2
U ∩U is open. Moreover, it satisfies α(U ∩U |g,x) = 1 since α(U |g,x) = α(U |g,x) = 1
1 2 1 2 1 2
and is hence invariant.
We may therefore define X/G to be the topological space whose underlying set of points
is the same as X, but whose topology consists of the invariant open subsets of X. The
following result shows that the Borel σ-algebra generated by X/G is then a sub-σ-algebra
of the invariant σ-algebra considered by [MP23, Definition 3.6].
Proposition A.5. For all invariant open U ⊆ X, it holds that α(U|g,x) = 1 (x) for all
U
g ∈ G and x ∈ X.
Proof. Let U be open and invariant. By definition, α(U|g,x) = 1 = 1 (x) if x ∈ U. On the
U
other hand, if x ̸∈ U, it holds for all g ∈ G that (denoting the Markov kernel (−)−1 by i)
(cid:90)
0 = id (U|x) = α(U|g′,x′)i(dg′|g)α(dx′|g,x)
X
(cid:90)
≥ 1 (x′)α(U|g′,x′)i(dg′|g)α(dx′|g,x)
U
(cid:90)
= 1 (x′)i(dg′|g)α(dx′|g,x)
U
= α(U|g,x).
Here the second step applies the group axioms, and the fourth step uses the fact that U is
invariant, so that α(U|g′,x′) = 1 when x′ ∈ U. It follows that α(U|g,x) = 0 = 1 (x) in
U
this case as well.
Suppose U ⊆ X is open, but not necessarily invariant. The following construction
provides a canonical way to obtain an invariant open set containing U. In particular, we
will define
G·U := {x ∈ X | α(U|g,x) = 1 for some g ∈ G}.
We then have the following.
Proposition A.6. For all open U ⊆ X, it holds that G · U is open and invariant, and
U ⊆ G·U.
Proof. Let U ⊆ X be open. Given g ∈ G, denote α (x) := α(U|g,x). Since α is determin-
g
istic, it is zero-one [Fri20, Example 10.4], and so
(cid:91) (cid:91)
G·U = α−1({1}) = α−1((0,1]).
g g
g∈G g∈G
54A standard argument shows x (cid:55)→ α (x) is lower semicontinuous (since (x,g) (cid:55)→ α (x) is).
g g
It follows that each α−1((0,1]) is open, and hence so is G·U, being the union of open sets.
g
To see that G·U is invariant, let x ∈ G·U and g ∈ G. By definition, there exists h ∈ G
such that (denoting the Markov kernels (−)−1 and ∗ by i and m)
(cid:90)
1 = α(U|h,x) = α(U|h,x′′)α(x′′|g′,x′)i(dg′|g)α(dx′|g,x)
(cid:90)
= α(U|h′,x′)m(dh′|h,g′)i(dg′|g)α(dx′|g,x)
(cid:90)
= 1 (x′)α(U|h′,x′)m(dh′|h,g′)i(dg′|g)α(dx′|g,x)
G·U
≤ α(G·U|g,x).
Here the second and third steps apply the group axioms, while the fourth uses the fact that
α(U|h′,x′) = 0 whenever x′ ̸∈ G·U, by the definition of G·U.
Finally, to show that U ⊆ G·U, let x ∈ U. Then the group axioms imply
(cid:90)
α(U|g,x)e(dg|•) = 1,
where • denotes the unique element of the one-point space I. Hence there exists g ∈ G with
α(U|g,x) = 1, so that x ∈ G·U.
Our key reason for developing this construction in TopStoch is that the invariant topol-
ogy plays well with products. To make this precise, let Y be any topological space. Then
we always obtain an action as follows:
X Y
α (51)
G X Y
AnalogouslytoX/G, wewilldenoteby(X⊗Y)/GthetopologicalspaceconsistingofX⊗Y
equipped with the invariant topology induced by (51). We then obtain the following result.
Proposition A.7. It holds that (X ⊗Y)/G = X/G⊗Y.
Proof. Suppose V ⊆ X is open and invariant with respect to α, and W ⊆ Y is open. Then
by definition of the product topology, V ×W is open in X⊗Y. It is moreover easily verified
that V ×W is invariant with respect to (51). Since the rectangle sets form a base for the
product topology, this shows that every open subset of X/G⊗Y is open in (X ⊗Y)/G.
Conversely, let U ⊆ X⊗Y be open and invariant with respect to α⊗id . By definition
Y
of the product topology, we can write
(cid:91)
U = V ×W
i i
i
for some open V ⊆ X and W ⊆ Y. We claim that
i i
(cid:91)
U = G·V ×W , (52)
i i
i
55fromwhichitfollowsthatU isopeninX/G⊗Y. Indeed,the⊆inclusionfollowsimmediately
by Proposition A.6. For the ⊇ inclusion, choose x ∈ G·V and y ∈ W arbitrarily. Since U
i i
is invariant to (51), Proposition A.5 implies the following for all g ∈ G:
1 (x,y) = (α⊗id )(U|(g,x),y)
U Y
≥ (α⊗id )(V ×W |(g,x),y)
Y i i
= α(V |g,x)1 (y)
i Wi
= α(V |g,x).
i
By definition of G·V , there exists g ∈ G such that α(V |g,x) = 1. Hence we must have
i i
1 (x,y) = 1, and so (x,y) ∈ U.
U
A.4.2 Existence of orbit map coequalisers
Lemma A.1. Suppose α : G ⊗ X → X be an action in TopStoch. Let X/G denote X
equipped with the invariant topology induced by α, and define q : X → X/G by
q(A|x) := δ (A) for x ∈ X and Borel A ⊆ X/G,
x
where δ denotes the Dirac measure at x. Then q is a coequaliser of the parallel arrows
x
α,ε : G⊗X ⇒ X.
Proof. We first show that q is indeed a well-defined morphism in TopStoch. Certainly
A (cid:55)→ q(A|x) is a probability measure for all x ∈ X. Additionally, since X/G is equipped
with a coarser topology than X, its Borel σ-algebra is coarser than that of X also. Hence
for Borel A ⊆ X/G, the function X → [0,1] defined as
x (cid:55)→ q(A|x) = 1 (x) (53)
A
isalwaysmeasurablewithrespecttotheBorelσ-algebraonX. Itfollowsthatq isaMarkov
kernel. Moreover, for invariant open A ⊆ X, the function (53) is lower semicontinuous with
respect to the topology on X, since it is the indicator function of an open subset of X. This
shows that q is a morphism X → X/G in TopStoch.
We now show that q is a coequaliser. For this, we must first show that q ◦α = q ◦ε.
This holds because for any invariant open U ⊆ X, as well as x ∈ X and g ∈ G, we have
(cid:90) (cid:90)
q(U|x′)α(dx′|g,x) = 1 (x′)α(dx′|g,x)
U
= α(U|g,x)
= 1 (x)
U
= q(U|x)
(cid:90)
= q(U|x′)ε(dx′|g,x),
where the third step applies Lemma A.5. Since the invariant open sets generate the Borel
σ-algebra on X/G, this shows q◦α = q◦ε.
56Now suppose k : X → Y in TopStoch also satisfies k ◦α = k ◦ε. Then a morphism
k′ : X/G → Y satisfies k′◦q = k if and only if
(cid:90)
k(V|x) = k′(V|x′)q(dx′|x)
(cid:90)
= k′(V|x′)δ (dx′)
x
= k′(V|x)
for all x ∈ X and open V ⊆ Y. We are therefore done if we can show that x (cid:55)→ k(V|x) is
lower semicontinuous with respect to the topology on X/G, so that we may take k′ := k.
For this, choose any open V ⊆ Y and t ∈ [0,1]. We would like to show that
U := {x ∈ X | k(V|x) > t}
is an invariant open subset of X. Since k is a morphism X → Y in TopStoch and hence
lower semicontinuous, certainly U is an open subset of X. To prove that U is invariant, we
will show that G·U ⊆ U, which is sufficient by Proposition A.6. Indeed, if x ∈ G·U, then
by definition there exists some g ∈ G such that α(U|g,x) = 1. This yields
(cid:90)
k(V|x) = k(V|x′)α(dx′|g,x)
(cid:90)
= k(V|x′)α(dx′|g,x)
U
> t,
where the first step uses the fact that k◦α = k◦ε, and the third uses the definition of U.
This gives x ∈ U, which is what we wanted to show.
A.4.3 Proof of Theorem 3.1
Proof. Suppose α : G ⊗ X → X is an action in TopStoch. By Lemma A.1, it holds
that α and ε have a coequaliser q : X → X/G defined as q(A|x) := δ (A), where X/G
x
denotes X equipped with the invariant topology induced by α. Moreover, since q is zero-
one by definition, it is deterministic [Fri20, Example 10.4]. Now let Y be an arbitrary
topologicalspace. Wewishtoshowthatthiscoequaliserispreservedbythefunctor(−)⊗Y.
ApplyingLemmaA.1again,nowwithrespecttotheaction(51),wealsoobtainthefollowing
coequaliser diagram:
(G⊗X)⊗Y
α⊗idY
X ⊗Y r (X ⊗Y)/G
ε
where (X ⊗Y)/G denotes X ⊗Y equipped with the invariant topology induced by (51),
and r(A|x,y) := δ (A). From Proposition A.7, we know that
(x,y)
(X ⊗Y)/G = X/G⊗Y.
57Additionally, given Borel A ⊆ X and B ⊆ Y, we have
r(A×B|x,y) = δ (A×B)
(x,y)
= δ (A)δ (B)
x y
= q(A|x)id (B|y).
Y
This shows r = q⊗id , and the result now follows.
Y
A.5 Orbit maps from coequalisers
Proposition A.8. Let α : G⊗X → X be an action in a Markov category C. If α admits
some orbit map, then every coequaliser q of the form
α q
G⊗X X X/G (54)
ε
is an orbit map.
Proof. Let r : X → Z be an orbit map for α, and suppose q : X → X/G a coequaliser of
the form (54) Since r is also a coequaliser, there exists a unique morphism q′ such that the
triangle in the following diagram commutes:
α
G⊗X X r Z
ε
∼= q′
q
X/G
Hereq′ ismoreoveranisomorphismbytheYonedalemmasinceq isacoequaliser. Likewise,
since r is an orbit map, r⊗id is a coequaliser, and so there exists a unique morphism q′′
Y
such that the triangle below commutes:
(G⊗X)⊗Y
α⊗idY
X ⊗Y
r⊗idY
Z ⊗Y
ε⊗idY
q′′
q⊗idY
X/G⊗Y
By uniqueness, we must have q′′ = q′ ⊗id , which is then an isomorphism since q′ is. It
Y
follows that q⊗id is a coequaliser of the parallel arrows in this last diagram, which shows
Y
that (54) is preserved by the functor (−)⊗Y.
A.6 Coset maps for semidirect products
Proposition A.9. Let C be a Markov category, and N ⋊ H a semidirect product in C.
ρ
Adopting the notation of Example 3.23, it holds that i and i are both homomorphisms.
N H
Moreover, p is an i -coset map, and p is an i -coset map.
H N N H
58Proof. A standard argument shows that i and i are homomorphisms when C = Set,
N H
which translates to the general case by Remark 3.2. We prove that p is an i -coset map,
H N
with p being similar. For this, we must first prove that p is a coequaliser of the form
N H
N ⊗(N ⋊ H) α N ⋊ H pH H (55)
ρ ρ
ε
where α denotes the action
N H
∗
op
i
H
H N H
This firstly requires showing that p is invariant with respect to α, so that p ◦α = p ◦ε.
H H H
By Remark 3.2, it suffices to do so when C = Set, where for n,n′ ∈ N and h ∈ H we have
p (n·(n′,h)) = p ((n′,h)(n−1,e ))
H H H
= p (n′ρ(h,n−1),h)
H
= h
= p (n′,k).
H
Next, suppose we have m : N ⋊ H → Y in C that is also invariant to α. Then
ρ
Y
Y
m
Y
m
∗
op
m = =
i
N
i
H
N H
N H
N H
Here the first step holds because m is invariant. The second step follows from some basic
manipulations: in Set, letting ψ denote the dashed box, we have for n ∈ N and h ∈ H
ψ(n,h) = (n,h)i (h)−1
H
= (n,h)(e ,h−1)
N
= (n,hh−1)
= (n,e ).
H
This shows that m = (m◦i )◦p . Since p is easily seen to be an epimorphism, it follows
N H H
that m◦i is unique with this property. All up, this means (55) is a coequaliser diagram.
N
A similar argument shows that this is preserved by every functor (−)⊗Y, from which it
follows that p is an i -coset map.
H N
59A.7 Induced actions on orbits and cosets
Proposition A.10. Let C be a Markov category, and α : G⊗X → X and β : H⊗X → X
actions in C that commute in the sense of Remark 3.4 from the main text. If q : X → X/H
is an orbit map for β, then there exists a unique action α/H : G⊗X/H → X/H that makes
q equivariant with respect to G as follows:
X/H X/H
q α/H
= (56)
α q
G X G X
Proof. The idea is that we obtain α/H as the unique morphism in C that makes the
det
square below commute:
G⊗(H ⊗X)
idG⊗β
G⊗X
idG⊗q
G⊗X/H
idG⊗ε
(57)
α α/H
q
X X/H
Notice that this says that (56) holds. Now, by the preservation condition of orbit maps, it
holds that id ⊗q is a coequaliser in C of the parallel arrows shown. As such, α/H exists
G
uniquely in C whenever
q◦α◦(id ⊗β) = q◦α◦(id ⊗ε).
G G
By Remark 3.2, it suffices to show this in Set, where for g ∈ G, h ∈ H, and x ∈ X we have
simply
q(g·(h·x)) = q(h·(g·x)) = q(g·x).
where the first step uses the assumption that α and β commute, and the second uses the
fact that q is invariant to β.
We are therefore done if we can show that α/H is an action. By Proposition A.1, we
know that α/H is deterministic, since all the other morphisms appearing in (57) are. To
see that α/H is associative, observe that
X/H X/H X/H
X/H
α/H q q
α/H
α/H = α = α =
∗ q
q α ∗
G G X
G G X G G X G G X
Herethefirststepuses(56)twice, thesecondusesassociativityofα, andthethirduses(56)
again. Now, since q is an orbit map, id ⊗q is a coequaliser. Since all coequalisers are
K⊗K
epimorphisms,itfollowsthatbothsideshereareequalevenwhenq isremoved,whichshows
associativity. A similar argument shows that α/H is unital and completes the proof.
60Proposition A.11. Under the same setup of Proposition A.10, let k/H : X/H → Y be
a morphism in C such that k/H ◦ q : X → Y is equivariant with respect to α and some
additional G-action α : G⊗Y → Y. Then k/H is equivariant with respect to α/H and
Y
α also.
Y
Proof. We have
Y Y Y
k/H k/H α Y
α/H = q = k/H
q α q
G X G X G X
where the first step uses the fact that q is equivariant by Proposition A.10, and the second
step uses the assumption that k/H ◦q is equivariant. Since q is an orbit map, id ⊗q is
G
a coequaliser, and hence an epimorphism. It follows that both sides are equal when q is
removed, which gives the result.
A.7.1 Proof of Proposition 3.4
Proof. We first show that the action ∗ of G on itself by left multiplication commutes with
the action
G
∗
op
φ
H G
usedinthedefinitionofaφ-cosetmap. ByRemark3.2, itsufficestoshowthisinSet, where
for all g,g′ ∈ G and h ∈ H we have
g·(h·g′) = g·(g′φ(h)−1) = g(g′φ(h)−1) = (gg′)φ(h)−1 = h·(gg′) = h·(g·g′).
The result now follows directly from Proposition A.10.
A.8 Proof of Proposition 3.5
Proof. Let ι : C → D denote the standard inclusion functor, i.e. the left adjoint of the
Kleisliadjunction(seee.g.[Per21, Section5.1.2]foradefinition). Thenιisstrictsymmetric
monoidal [Fri20, Proposition 3.1], and so for any group G in C with operations
∗ : G⊗G → G e : I → G (−)−1 : G⊗G
C
we obtain an object ι(G) in D along with morphisms
ι(∗) : ι(G)⊗ι(G) → ι(G) ι(e) : I → ι(G) ι((−)−1) : ι(G) → ι(G),
D
61whereI andI = ι(I )denotethemonoidalunitsofCandDrespectively. Byfunctoriality
C D C
of ι, these morphisms in D are deterministic and satisfy the group axioms. In this way ι(G)
becomes a group in D. A similar argument shows that ι lifts group actions and equivariant
morphisms. Finally, ι lifts orbit maps because it is a left adjoint and so preserves colimits
[Per21, Corollary 4.3.2], and therefore preserves coequalisers in particular.
B Symmetrisation
Proposition B.1. Let G be a group in a Markov category C. Then CG as described in
Definition 4.1 is always a Markov category.
Proof. Itisstraightforwardtocheckthatthecompositionofequivariantmorphismsisequiv-
ariant,andthatidentitymorphismsarealwaysequivariantwhentheirdomainandcodomain
areequippedwiththesameactionofG. Inthisway, CG isacategory. Itisalsoclearthat⊗
is a bifunctor on CG. We therefore only need to show that the structure maps as defined for
CG satisfy the axioms of a Markov category (including those of a symmetric monoidal cat-
egory). By definition, these structure maps are inherited from C, and so by [Fri20, Lemma
10.12] are all deterministic, which means we can equivalently show this for (C )G. But
det
now the latter is just the Eilenberg-Moore category of the action monad G⊗(−) on C
det
(see e.g. [Per21, Section 5.2]), which is cartesian monoidal [Fri20, Remark 10.13]. Since the
forgetful functor (C )G → C is monadic, it creates limits [Rie17, Theorem 5.6.5], and so
det det
this cartesian monoidal structure lifts to (C )G in the way described in Definition 4.1.
det
C A general methodology
C.1 Proof of Theorem 5.1
Remark C.1. Recall from Remark 3.2 that the Yoneda Lemma allows us to lift equations
that hold for all groups, actions, etc. in Set to equations that hold more generally in the
deterministic subcategory of an arbitrary Markov category. At several points in this sub-
section, we will abuse this technique by applying it even when some morphisms involved are
not deterministic. This will streamline our proofs considerably, which become long-winded
when expressed in terms of string diagrams. It will also demonstrate how our construction
follows the classical set-theoretic arguments, which are standard [MPC97, Chapter I.1]. In
the few cases where we do this (which we will flag), it will be clear how to translate our
set-theoretic manipulations into a general string-diagrammatic argument. Our approach
here may therefore be regarded essentially as a convenient shorthand for the “real” proof.
A more formal justification may be possible: the key idea seems to be that, in cases where
this approach is valid, we do not reuse the output of any nondeterministic morphism more
than once, which seems to be where potential issues could arise.
Lemma C.1. Let C be a Markov category, and φ : H → G a homomorphism and q : G →
G/H a φ-coset map in C. For all k : R (X,α ) → R (Y,α ) in CH, there exists a unique
φ X φ Y
62k♯ : G/H ⊗X → Y in C such that
Y
α
Y
Y
k
k♯
= α (58)
q X
(−)−1
G X
G X
Proof. For brevity, let ρ : H ⊗(G⊗X) → G⊗X denote the following:
G X
∗ op (59)
φ
G G X
This is seen to be the diagonal action (Example 3.8) obtained from the action (24) and the
trivial action on X. Also denote by m : G⊗X → Y denote the right-hand side of (58). We
claim that m◦ρ = m◦ε, or in other words that m is invariant with respect to ρ. Noting
the caveat of Remark C.1, we show this in Set, where for g ∈ G, h ∈ H, and x ∈ X we have
m(h·(g,x)) = m(gφ(h)−1,x)
= (gφ(h)−1)·k((gφ(h)−1)−1·x)
= g·(h−1·k(h·(g·x)))
= g·(h−1·h·k(g·x))
= g·k(g·x)
= m(g,x).
Here the first two steps apply the definitions of ρ and m, and the third uses the definition
of the H-actions that equip R (X,α ) and R (Y,α ). The fourth step then uses the fact
φ X φ Y
that k is H-equivariant. From invariance of m and the universal property of orbit maps, we
obtainauniquemorphismk♯ inCsuchthatthetriangleinthefollowingdiagramcommutes:
ρ
H ⊗(G⊗X) G⊗X
q⊗idX
G/H ⊗X
ε
k♯
m
Y
The commuting triangle here says exactly that (58) holds, which gives the result.
63Lemma C.2. The morphism k♯ described in Lemma C.1 is always a morphism in CG of
the form
k♯ : (G/H,∗/H)⊗(X,α ) → (Y,α )
X Y
where ∗/H : G⊗G/H → G/H denotes the unique action induced by Proposition 3.4.
Proof. We adopt the same notation as in the proof of Lemma C.1. By definition of CG, the
action that equips (G,∗/H)⊗(X,α ) is as follows:
X
G X
∗/H α X
(60)
G G X
We will first show that this commutes with the H-action (59). By Remark 3.2, it suffices
to do so in Set: given g,g′ ∈ G, h ∈ H and x ∈ X, we have
g·(h·(g′,x)) = g·(g′φ(h)−1,g·x)
= (gg′φ(h)−1,g·x)
= h·(gg′,g·x)
= h·(g·(g′,x)).
Next, we show that m is equivariant with respect to (60) and α . Noting the caveat of
Y
Remark C.1, we demonstrate this in Set, where we have
m(g·(g′,x)) = m(gg′,g·x)
= (gg′)·k((gg′)−1·g·x)
= g·(g′·k((g′)−1·x))
= g·m(g′,x).
Now recall that q⊗id is an orbit map with respect to the H-action (59). Since (58) says
X
m = k♯◦(q⊗id ),
X
Proposition A.11 implies that k♯ is equivariant with respect to (60) and α as desired.
Y
C.1.1 Proof of Theorem 5.1
Proof. By Lemmas C.1 and C.2, the assignment k (cid:55)→ k♯ defines a function of the required
form
CH(R (X,α ),R (Y,α )) → CG((G/H,∗/H)⊗(X,α ),(Y,α )). (61)
φ X φ Y X Y
We will show that this has an inverse m (cid:55)→ m◦η, where η is the morphism
G/H X
G/H X
q
η :=
e
X
X
64We will show the assignment k (cid:55)→ k♯ from Lemma A.1, which is a function of the form
requiredbythestatementofthisTheorembyLemmaC.2,hasaninverse,namelym (cid:55)→ m◦η.
First, we must check that m (cid:55)→ m◦η is actually well-typed. We do so by showing that
η is a morphism in CH of the following type:
R (X,α ) → R ((G/H,∗/H)⊗(X,α )). (62)
φ X φ X
Since R is the identity on morphisms, it follows by definition of composition in CH that
φ
m◦η = R (m)◦η is a element of the left-hand side of (61) whenever m is an element of the
φ
right-hand side. To show (62), since all the morphisms involved in the definition of η are
deterministic, Remark 3.2 allows us to work in Set, where for h ∈ H and x ∈ X we have
η(h·x) = (q(e),φ(h)·x)
= (q(φ(h)φ(h)−1),φ(h)·x)
= (φ(h)·q(φ(h)−1),φ(h)·x)
= (φ(h)·q(e),φ(h)·x)
= h·η(x).
Here the third step uses the fact that q is G-equivariant with respect to ∗ and ∗/H by
Proposition A.10, and the fourth uses fact that q is H-invariant since it is an orbit map.
Now we claim that for any morphisms k and m living in the left- and right-hand sides of
(61)respectively, itholdsthatk = m◦η ifandonlyifm = k♯. The“if”directionestablishes
that m (cid:55)→ m◦η is surjective, while the “only if” direction establishes injectivity. Noting
the caveat of Remark C.1, we demonstrate the “if” direction in Set as follows, where for
x ∈ X we have
(k♯◦η)(x) = k♯(q(e),x)
= e·k(e−1·x)
= k(x).
Here the second step uses (58). For the “only if” direction, suppose m◦η = k. From the
uniqueness part of Lemma C.1, it follows that m = k♯ if we can show that (58) holds when
m◦η is substituted for k on its right-hand side. Again noting Remark C.1, we demonstrate
this in Set: given any g ∈ G and x ∈ X, we have
g·(m◦η)(g−1·x) = g·m(q(e),g−1·x)
= m(g·q(e),g·g−1·x)
= m(q(g),x),
where the first step uses the assumption that m is H-equivariant, and the second uses the
factthatq isG-equivariantwithrespectto∗and∗/H. Sincek♯ isuniquewiththisproperty
by Lemma C.1, it follows that m = k♯ as desired, which gives the result.
Corollary C.1. The assignment k (cid:55)→ k♯ from Theorem 5.1 defines an equivalence of cat-
egories between the full image of R and the co-Kleisli category of the reader comonad
φ
(G/H,∗/H)⊗(−) on CG.
65Proof. Recall that the full image of R and the co-Kleisli category of the reader comonad
φ
both have the same objects as CG. Moreover, their morphisms (X,α ) → (Y,α ) are
X Y
respectively the left- and right-hand sides of (61). Since each assignment k (cid:55)→ k♯ from
Theorem 5.1 is a bijection, we are therefore done if we can show that this assignment is
functorial, or in other words that the following holds for compatibly typed morphisms:
(m◦k)♯ = m♯◦ k♯, (63)
ck
where the right-hand side denotes co-Kleisli composition. But now observe
Y
Y Y
α
Y
k♯ k♯ k
m
m♯ = m♯ =
α
X
q q
(−)−1
q
G X G X
G X
Here the second second equality uses the fact that q is deterministic, and the third uses
Lemma C.1, together with some basic group-theoretic manipulations. By definition of co-
Kleisli composition here (see e.g. [Per21, Example 5.3.11]), the dashed box is precisely
m♯◦ k♯, and so this shows (63) by the uniqueness part of Lemma C.1.
ck
C.2 Proof of Proposition 5.1
Proof. Given arbitrary ω : (X,α ) → (G/H,∗/H)⊗(X,α ) in CG, we will denote
X X
sym (k) := k♯◦ω, (64)
ω
where k♯ is obtained via Theorem 5.1. In other words, this is just like sym , but where its
γ
precomposition morphism is allowed to be arbitrary, rather than taking the specific form
(38) from the main text. Now let k : (X,α ) → (Y,α ) be a morphism in CG. It is then
X Y
easily verified that sym is natural in the following sense:
ω
sym (k) = sym (k◦id ) = k◦sym (id ).
ω ω X ω X
As a result, sym is stable if and only if sym (id ) = id . Now recall that (id )♯ is the
ω ω X X X
unique morphism in C such that
X
X α
X
X
(id X)♯ α
X
= = (65)
q
(−)−1
G X
G X
G X
66(where the second step is shown in Remark 3.2), and so we must have
X
X
(id )♯ =
X
G/H X
G/H X
since certainly (65) holds in this case. It follows that sym is stable if and only if
ω
X X X
sym (id ) = =
ω X ω
X X X
This condition is always satisfied for ω of the form (38) from the main text. Conversely, if C
is positive, this condition implies that ω has the form (38) by [Fri+23a, Theorem 2.8].
67