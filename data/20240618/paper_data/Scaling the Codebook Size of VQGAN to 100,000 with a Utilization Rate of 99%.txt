Scaling the Codebook Size of VQGAN to 100,000
with a Utilization Rate of 99%
LeiZhu1 FangyunWei2∗ YanyeLu1 DongChen2
1PekingUniversity 2MicrosoftResearchAsia
zhulei@stu.pku.edu.cn fawe@microsoft.com yanye.lu@pku.edu.cn doch@microsoft.com
Generation-DiT↓
Ours
Best-rFID: 2.62 ↑ G
Best-rFID: 3.41 Classification
75.5
73.9
73.6
11
1
30
3
..
.
78
4
10.39.38.4
e n
er
ati
o n-
Si
T
↓
VQGAN-EMA
9.8 ↓
Best-rFID:4.29
VQGAN-FC
R e c
o
n str
u
cti
o
n2.63.44.3 1 117 56. ..3
43
9.1 8.4 eneration-LDM
Codebook Size ↓ G
Generation-GPT↓
(a) Codebooksizev.s.utilizationrate. (b) Evaluationondownstreamtasks.
Figure1: (a)TwoenhancedversionsofVQGAN[1],namelyVQGAN-FC(FactorizedCodes)and
VQGAN-EMA(ExponentialMovingAverage),experienceadeclineincodebookutilizationrate
and performance as their codebook sizes expand. In contrast, our method, VQGAN-LC (Large
Codebook),effectivelyleveragesanextremelylargecodebook,persistentlymaintainingautilization
rateofupto99%andachievinghigherperformance. WehighlightthebestreconstructionrFIDfor
eachmodel. (b)Comparisonamongthreemodelsacrossvarioustasks. Forimagegeneration,we
evaluatetheapplicationsofthesethreeVQGANvariantstoGPT[2],LDM[3],DiT[4]andSiT[5].
Abstract
IntherealmofimagequantizationexemplifiedbyVQGAN,theprocessencodes
imagesintodiscretetokensdrawnfromacodebookwithapredefinedsize. Recent
advancements,particularlywithLLAMA3,revealthatenlargingthecodebook
significantlyenhancesmodelperformance. However,VQGANanditsderivatives,
suchasVQGAN-FC(FactorizedCodes)andVQGAN-EMA,continuetograpple
withchallengesrelatedtoexpandingthecodebooksizeandenhancingcodebook
utilization. Forinstance,VQGAN-FCisrestrictedtolearningacodebookwith
a maximum size of 16,384, maintaining a typically low utilization rate of less
than 12% on ImageNet. In this work, we propose a novel image quantization
modelnamedVQGAN-LC(LargeCodebook),whichextendsthecodebooksizeto
100,000,achievinganutilizationrateexceeding99%. Unlikepreviousmethods
thatoptimizeeachcodebookentry,ourapproachbeginswithacodebookinitialized
with100,000featuresextractedbyapre-trainedvisionencoder. Optimizationthen
focuses on training a projector that aligns the entire codebook with the feature
∗Correspondingauthor.
Preprint.Underreview.
4202
nuJ
71
]VC.sc[
1v73811.6042:viXra
etaRnoitazilitU
koobedoCdistributionsoftheencoderinVQGAN-LC.Wedemonstratethesuperiorperfor-
manceofourmodeloveritscounterpartsacrossavarietyoftasks,includingimage
reconstruction,imageclassification,auto-regressiveimagegenerationusingGPT,
andimagecreationwithdiffusion-andflow-basedgenerativemodels. Codeand
modelsareavailableathttps://github.com/zh460045050/VQGAN-LC.
Table 1: We conduct a comparative analysis of our VQGAN-LC against two advanced variants
of VQGAN [1], namely VQGAN-FC and VQGAN-EMA, focusing on the effects of enlarging
theircodebooksizesfrom1,024to100K.Theonlydifferenceamongthethreemodelsliesinthe
initialization and optimization of the codebook. The evaluation covers both reconstruction and
generationusingthelatentdiffusionmodel(LDM)[3]ontheImageNetdataset.
Reconstruction(rFID) GenerationwithLDM[3](FID)
Method
1,024 16,384 50K 100K 1,024 16,384 50K 100K
VQGAN-FC 4.82 4.29 4.96 4.65 10.81 9.78 10.37 10.12
VQGAN-EMA 4.93 3.41 3.88 3.46 10.16 9.13 9.29 9.50
VQGAN-LC(Ours) 4.97 3.01 2.75 2.62 9.93 8.84 8.61 8.36
1 Introduction
Image quantization [1, 6, 7] refers to the process of encoding an image into a set of discrete
representations,alsoknownasimagetokens,eachderivedfromacodebookofapre-definedsize.
VQGAN[1]standsoutasaprominentarchitecture,withanencoder-quantizer-decoderstructure,
playingapivotalroleinvariousapplications,including: (1)trainingaGPT[2,8,9,10,11]onimage
tokens to create images; (2) serving as an autoencoder in latent diffusion models (LDMs) [3, 4]
and generative models [12, 13, 14], with flow matching [5, 15]; and (3) functioning within large
multi-modalitymodels[16,17,18,19],whereitsencoderprocessesinputimagesanditsdecoder
assistsinimagegeneration.
Incontrasttonaturallanguages,whichtypicallymaintainastaticvocabulary,imagequantization
modelsnecessitateacodebookofapre-definedsizetoconvertimagesintodiscreteimagetokens.
Thenatureofimagesignals—complexandcontinuous—makestranslatingimagesintotokenmapsa
formoflossycompressionthatisgenerallymoreseverethanconvertingthemintocontinuousfeature
maps. Thecapabilityofthesemodelstorepresentimageslargelydependsonthecodebooksize.
Previousstudies,suchasVQGAN[1],itsimprovedversions,includingVQGANwithexponential
movingaverage(EMA)update(VQGAN-EMA)andVQGANusingfactorizedcodes(VQGAN-FC),
and its predecessors, like VQVAE [6] and VQVAE-2 [7], have demonstrated that they can only
learn a codebook with a maximum size of 16,384. These models often face unstable training or
performance saturation issues when the codebook size is further increased, as shown in Table 1.
Additionally, they typically exhibit a low codebook utilization rate—for instance, under 12% in
VQGAN-FC,asshowninFigure1(a)—indicatingthatasignificantportionofthecodebookremains
unused,therebydiminishingthemodel’srepresentationalcapacity. Furthermore,studiesonlarge
language models suggest that employing a tokenizer with an expanded vocabulary significantly
enhancesmodelefficacy. Forexample,thetechnicalreportforLLAMA32shows,"LLAMA3uses
atokenizerwithavocabularyof128Ktokensthatencodeslanguagemuchmoreefficiently,which
leadstosubstantiallyimprovedmodelperformance."
Inthisstudy,weinvestigatethescalabilityofcodebooksizeinVQGANandtheimprovementof
its codebook utilization rate, thereby substantially enhancing the representational capabilities of
VQGAN.Typically,asshowninFigure2(a),imagequantizationmodelslikeVQGANarestructured
withanencoder-quantizer-decoderarchitecture,wherethequantizerisconnectedtoacodebook. For
agivenimage,theencoderproducesafeaturemapthatthequantizerthenconvertsintoatokenmap.
Eachtokeninthismapcorrespondstoanentryinthecodebook,basedontheircosinesimilarity. This
tokenmapissubsequentlyusedbythedecodertoreconstructtheoriginalimage.
Generally,thecodebookinVQGANbeginswitharandominitialization. Eachentry(a.k.a. atoken
embedding)inthecodebookisdesignatedastrainableandundergoesoptimizationthrougheither
gradientdescent[6,1,20,21](Figure2(b))oranexponentialmovingaverage(EMA)update[7,22]
2https://ai.meta.com/blog/meta-llama-3/
2(Figure2(c))duringthetrainingphase. Nevertheless,ineachiteration,onlyasmallamountoftoken
embeddings,correspondingtothetokenmapsofthecurrenttrainingbatch,areoptimized. Astraining
progresses, these frequently optimized token embeddings gradually align more closely with the
distributionsofthefeaturemapsgeneratedbytheencoder,comparedtothoselessfrequentlyornever
optimized(referredtoasinactivetokenembeddings). Consequently,theseinactivetokenembeddings
areexcludedfromthetrainingprocessandsubsequentlyremainunusedduringtheinferencephase,
resultinginpoorcodebookutilization.
Ourapproachdeviatesfromconventionalimagequantizationmodelsbyinitiatingwithacodebook
composedofN frozenfeatures,sourcedfromapretrainedimagebackboneliketheCLIP-vision-
encoder[23],andutilizingdatasetslikeImageNet[24]. Aprojectoristhenemployedtotransition
theentirecodebookintoalatentspace,producingtokenembeddings. Duringthetrainingprocess,
itistheprojectorthatisoptimized,notthecodebookitself,whichdistinguishesourmethodfrom
traditionalmodels. Byoptimizingtheprojector,weadapttheaggregatedistributionofthecodebook
entriestoalignwiththefeaturemapsgeneratedbytheencoder. Thiscontrastswithmethodslike
VQGAN [1], where adaptations are made to a limited number of codebook entries to match the
featuremapdistributionsduringeachiteration. Oursimplequantizationtechniqueensuresthatalmost
alltokenembeddings(over99%)remainactivethroughoutthetrainingphase.Theprocessisdepicted
inFigure2(d).
OurnewlydevelopedquantizercanbeintegrateddirectlyintotheexistingVQGANarchitecture,
replacing its standard quantizer without requiring any changes to the encoder and decoder. This
innovativequantizerenablestheexpansionofthecodebooktosizesupto100,000,whilemaintaining
an impressive utilization rate of 99%. By comparison, the conventional VQGAN is limited to a
codebooksizeof16,384withautilizationrateofonly11.2%whenappliedtoImageNet. Theadvan-
tagesofalargercodebookwithenhancedutilizationaredemonstratedacrossvariousapplications,
includingimagereconstruction,imageclassification,auto-regressiveimagegenerationusingGPT,
andimagecreationwithdiffusionmodelsandflowmatching. Figure1illustratestheperformanceof
ourimprovedVQGAN,termedVQGAN-LC(LargeCodebook),comparedtoitscounterparts.
2 RelatedWork
ImageQuantization. Imagequantizationfocusesoncompressinganimageintodiscretetokens
derivedfromacodebook[6,1,22,20,25,26,27]. VQVAE[6]introducesamethodofquantizing
patch-levelfeaturesusingthenearestcodebookentry,withthecodebooklearnedjointlywiththe
encoder-decoderstructurethroughreconstructionloss. VQVAE2[7]enhancesthisbyemploying
exponentialmovingaverageupdatesandamulti-scalehierarchicalstructuretoimprovequantization
performance. VQGAN[1]furtherrefinesVQVAEbyintegratingadversarialandperceptuallosses,
enablingmoreaccurateanddetailedrepresentations. ViT-VQGAN[21]replacestheCNN-based
encoder-decoder[28]withavisiontransformerandshowsthatusingafactorizedcodemechanism
withl regularizationcanimprovecodebookutilization. Reg-VQ[29]introducespriordistribution
2
regularizationtopreventcollapseandlowcodebookutilization. Additionally,someapproaches[26,
30,31]usecodebookresetstrategiestoresetunusedcodebookentriesduringtrainingtoenhance
utilizationrates. Incontrasttothesemethods,theproposedVQGAN-LCinitializescodebookentries
using a pre-trained vision encoder on target datasets, ensuring nearly full codebook utilization
throughoutthetrainingprocessandallowingforscalingupthecodebooksizetomorethan100K.
TokenizedImageSynthesis. Substantialadvancementshavebeenachievedintherealmofimage
synthesis,particularlythroughimagequantizationtechniques. InitialeffortssuchasPixelRNN[32]
employsLSTMnetworks[33]toautoregressivelymodeldependenciesbetweenquantizedpixels.
Buildinguponthis,thegroundbreakingVQVAE[6]introducesthequantizationofimagepatches
intodiscretetokens,significantlyenhancinggenerationcapabilitieswhenpairedwithPixelCNN[34].
TheiGPT[35]furtheradvancesthefieldbyleveragingthepowerfulTransformer[36]forsequence
modeling of VQ-VAE tokens. Recently, there has been a shift towards using non-autoregressive
Transformers for image synthesis [12, 13, 14, 11], which provide efficiency improvements over
traditionalraster-scan-basedgenerationmethods. Innovativeapproachessuchasdiscretediffusion
models,includingD3PMs[37]andVQ-Diffusion[38],utilizediscretediffusionprocessestomodel
thedistributionofimagetokens. Additionaldiffusion-basedtechniques[3,15,4,5,39,40]compress
imagesintolatentrepresentationsusingquantizers, therebyreducingbothtrainingandinference
costs. Moreover,imagequantizerscanenhancelargelanguagemodelsforbothimagesynthesisand
3VQGAN
Encoder Quantizer + … Decoder
Feature Map Token Map
Codebook
Reconstruction
(a) Architecture of VQGAN.
Gradient Descent EMA Update
Argmin Argmin
Gradient EMA
… …
Update
Random Initialization Random Initialization
(Trainable) (Updateable)
(b) Codebook optimization in VQGAN(-FC). (c) Codebook update in VQGAN-EMA.
Ours Class Centers :Gradient
Argmin
Pre-trained
… Projector …
Vision Encoder
Trainable
Patch-Level
Image Dataset Feature Clustering Frozen
(d) Codebook initialization and quantization process in our VQGAN-LC.
Figure2: (a)Theencoder-quantizer-decoderstructureofVQGAN,withacodebooklinkedtothe
quantizer. (b) The codebook optimization strategy employed in VQGAN and VQGAN-FC. (c)
ThecodebookupdatemechanismutilizedinVQGAN-EMA.(d)Thecodebookinitializationand
quantizationprocessimplementedinourVQGAN-LC.
understanding[17,16,18,41,19]. Ourworkintroducesasuperiorimagequantizer,furtherrefining
theimagesynthesisprocess.
3 Method
3.1 Preliminary
VQGAN.LetB ={b ∈RD}N denoteacodebookcontainingN entries,witheachentryb being
n n=1 i
aD-dimensionaltrainableembeddingwithrandominitialization. AsshowninFigure2(a),VQ-GAN
adoptsanencoder-quantizer-decoderstructure. Inthissetup,anencoderprocessesanimageX of
heightH andwidthW togenerateafeaturemapZ ∈Rh×w×D,with(h,w)representingthelatent
dimensions. Subsequently,thequantizermapsZ toatokenmapZˆ,whereeachtokeninZˆ isan
entry in B based on the cosine distance between Z and B. Finally, the decoder reconstructs the
originalimagefromthetokenmapZˆ. Theentirenetworkisoptimizedusingacombinationoflosses,
expressedasfollows:
L=∥|Xˆ −X||2+α||sg(Zˆ)−Z||+β||sg(Z)−Zˆ||+L +L , (1)
P GAN
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
LR LQ
wheresg(·)denotesthestop-gradientoperation. ThetermsL ,L ,L andL representthe
R Q P GAN
reconstructionloss,quantizationloss,VGG-basedperceptualloss[1],andGANloss[1],respectively.
Hyper-parametersαandβaresetto1.0and0.33bydefault. AsshowninFigure2(b),werefertothe
codebookoptimizationstrategyusedintheoriginalVQGANas“gradientdescent”.
VQGAN-FC.VQGANfacessignificantchallengeswithinefficientcodebookutilization. Toaddress
thisissue,thefactorizedcode(FC)mechanism,initiallyproposedbyViT-VQGAN[21],isemployed.
WerefertoVQGANintegratedwiththeFCmechanismasVQGAN-FC.Thekeydifferencesbetween
VQGANandVQGAN-FCaretwo-fold: 1)alinearlayerisaddedtoprojecttheencoderfeature
Z ∈Rh×w×D intoalow-dimensionalfeatureZ′ ∈Rh×w×D′,whereD′ ≪D;2)thecodebookB,
consistingofN D′-dimensionaltrainableembeddings,israndomlyinitialized. Consequently,the
quantizationlossinEq.1isreformulatedas:
L =α||sg(Zˆ)−Z′||+β||sg(Z′)−Zˆ||. (2)
Q
41.0
VQGAN-LC (Ours)
0.8
VQGAN-LC (Ours)
0.6
0.4 VQGAN-EMA
VQGAN-EMA
VQGAN-FC 0.2
0.0
TrainingEpoch VQGAN-FC
Figure3:(Left)Thecodebookutilizationrateoverthetrainingepoch.Acodebookentryisconsidered
utilizedfortheepochifitisusedatleastonce. (Right)Theaverageutilizationfrequencyofeach
codebook entry over all epochs, with each pixel representing a single entry. All models adopt a
codebookwithasizeof100Kanduseimageswitharesolutionof256×256onImageNet.
However,asillustratedinFigure1(a),theutilizationrateofVQGAN-FCisonly11.2%onImageNet
whenthecodebooksizeisconfiguredto16,384, andincreasingthesizeofthecodebookfailsto
enhanceperformanceasdemonstratedinTable1.
VQGAN-EMA.AsdepictedinFigure2(c),thisvariantofVQGANadoptsanexponentialmoving
average(EMA)strategytooptimizethecodebook. Specifically,LetBˆ⊂Bdenotethesetoftoken
embeddingsusedforalltokenmapsinthecurrenttrainingbatch. ThesetBˆisupdatedthroughthe
EMA mechanism using the corresponding encoder features Z in each iteration. As a result, the
codebookdoesnotreceiveanygradients. Therefore,thequantizationlossinEq.1isdefinedas:
L =α||sg(Zˆ)−Z||. (3)
Q
Ourresults,highlightedinFigure1,indicatethatVQGAN-EMAoutperformsVQGAN-FConvarious
downstreamtasks,leadingtoenhancedutilizationofthecodebook.However,expandingthecodebook
sizecontinuestoposeasignificantchallengeforVQGAN-EMA,asdetailedinTable1.
3.2 VQGAN-LC
AnalysisofVQGAN-FCandVQGAN-EMA.IntheseenhancedversionsofVQGAN,thecodebook
isinitializedrandomly. Duringeachiteration,onlyasmallsubsetofentriesrelatedtothecurrent
trainingbatchareoptimized. Asaresult,thefrequentlyoptimizedentriesbecomemorealignedwith
thefeaturemapdistributionsgeneratedbytheencoder,whilethelessfrequentlyoptimizedentries
remainunderutilized. Consequently,asignificantportionofthecodebookremainsunusedduring
boththetrainingandinferencestages. Figure3showsthecodebookutilizationrateoverthetraining
epochandvisualizestheutilizationfrequencyofeachcodebookentryoncetrainingiscompleted.
Overview. We present VQGAN-LC (Large Codebook), which allows for the expansion of the
codebook to sizes of up to 100,000 while achieving a remarkable utilization rate of 99%. As
illustratedinFigure2(d),ourmethoddivergesfromVQGAN-FCandVQGAN-EMAinitsdesign
ofthequantizer. Wemaintainastaticcodebookandtrainaprojectortomaptheentirecodebook
intoa latentspace, aligningthe distributionsof thefeature mapsgenerated bythe encoder. This
approachallowsustoscalethecodebooksizeeffectivelywithoutmodifyingtheencoderanddecoder,
achievinganextremelyhighutilizationrateandresultinginsuperiorperformanceacrossvarious
tasks,asshowninFigure1,Table1andFigure3. Itisimportanttonotethatincreasingthecodebook
sizeincursalmostnoadditionalcomputationalcost.
CodebookInitialization. Toinitializeastaticcodebook,wefirstutilizeapre-trainedvisionencoder
(e.g.,CLIPwithaViTbackbone)toextractpatch-levelfeaturesfromthetargetdataset(e.g.,ImageNet)
containing M images. This extraction results in a set of features denoted as F = {F(i,j) ∈
m
RD}h¯,w¯,M ,whereF(i,j) representsaD-dimensionalpatch-levelfeatureatlocation(i,j)in
i=1,j=1,m=1 m
them-thimage,and(h¯,w¯)indicatethespatialdimensionsofF. Subsequently,weapplyK-means
clusteringtoF,resultinginN clustercenters(withadefaultvalueofN =100,000). Thesecluster
centers form the set C = {c ∈ RD}N , where c is the n-th center. Our codebook B is then
n n=1 n
initializedusingC.
5
etaR
noitazilitU
koobedoCQuantization. UnlikeVQGAN,VQGAN-FCandVQGAN-EMA,whichoptimizethecodebook
directly,ourapproachinvolvestrainingaprojectorP(·),implementedasasimplelinearlayer,to
align the static codebook B with the feature distributions generated by the encoder E(·) of our
VQGAN-LC.LetB′ =P(B)={b′ ∈RD′}N denotetheprojectedcodebook. Foragiveninput
n n=1
imageX,thequantizertransformsthefeaturemapZ = E(X) ∈ Rh×w×D′ intoatokenmapZˆ.
ThisquantizationprocesscanbeexpressedasZˆ :=argmin||Z(i,j)−b′ ||.
n
b′∈B′
n
LossFunction. WeemploythesamelossfunctionasspecifiedinEq.1. However,thekeydistinc-
tionisthatourcodebookB remainsfrozen,whilethenewlyintroducedprojectorP(·)undergoes
optimization.
3.3 EvaluationofImageQuantizationModels
WeevaluatetheperformanceofVQGAN-FC,VQGAN-EMA,andourproposedVQGAN-LCacross
imagereconstruction,imageclassificationandimagegenerationtasks.
ImageReconstruction. Imagesareprocessedthroughtheencoder,quantizer,anddecodertoproduce
reconstructedimages. Thesereconstructedimagesarethencomparedtotheiroriginalimagesusing
therFIDmetricastheevaluationcriterion.
Image Classification. Initially, the encoder and quantizer convert each image into a token map.
Subsequently,weutilizeaViT-Bmodel[36],pre-trainedwithMAE[42],totrainonalltokenmaps
forthepurposeofimageclassification. Top-1accuracyisusedastheevaluationmetric.
Image Generation. Image quantization models can be integrated with different image genera-
tionframeworks, suchasauto-regressivecausalTransformers(GPT[2]), latentdiffusionmodels
(LDM[3]),diffusionTransformers(DiT[4]),andflow-basedgenerativemodels(SiT[5]),tofacilitate
imagecreation.
GPT.TheencoderandquantizertransformeachimageintoatokenmapZˆ,whichisthenflattened
intoatokensequence. Ultimately,GPTistrainedonthecollectionofthesetokensequences.
LDM.ItprogressivelyaddsnoiseontotheencoderfeatureZ. Thetrainingobjectiveistodenoiseand
reconstructZ. Duringtheinferencephase,theoutputfromLDMisinputtedintothequantizerand
decoderofimagequantizationmodelstogenerateimages.
DiT.ThismodelisavariantofLDM,distinguishedbyitsuseofaTransformerarchitectureasthe
backbone. TheincorporationofimagequantizationmodelsintoDiTfollowsthesameapproachas
theirintegrationintoLDM.
SiT. This method presents a flow-based generative framework utilizing the DiT backbone. The
integrationofimagequantizationmodelsinSiTfollowsthesamemethodologyasinLDMandDiT.
4 Experiments
4.1 Setup
ImplementationDetailsofImageQuantization.Allimagequantizationmodels,includingVQGAN,
VQGAN-FC,VQGAN-EMA,andourproposedVQGAN-LC,utilizethesameencoderanddecoder
oftheoriginalVQGAN.Theinputimagesareprocessedataresolutionof256×256pixels. The
encoder (U-Net [28]) downsamples the input image by a factor of 16, yielding a feature map Z
withdimensionsof16×16. ThequantizerthenconvertsthisfeaturemapintoatokenmapZˆ of
thesamesize,whichissubsequentlyfedintothedecoder(U-Net)forimagereconstruction. Inour
observations,theoptimalcodebooksizeforVQGAN,VQGAN-FC,andVQGAN-EMAis16,384,
whereasforourVQGAN-LC,theoptimalcodebooksizeis100,000. Trainingisconductedonthe
ImageNet-1K[24]andFFHQ[43]datasets,utilizing32NvidiaV100GPUs. ForImageNet-1K,we
trainfor20epochs,whereasforFFHQ,wetrainfor800epochs. TheAdamoptimizer[44]isused,
startingwithaninitiallearningrateof5e−4. Thislearningratefollowsahalf-cyclecosinedecay
scheduleafteralinearwarm-upphaseof5epochs.
CodebookInitializationofOurVQGAN-LC.Unlessotherwisespecified,weusetheCLIPimage
encoder[23]withaViT-L/14backbone,addinganadditional4×4averagepoolinglayer,toextract
patch-levelfeaturesfromimagesinthetrainingsplitofthetargetdataset(eitherImageNetorFFHQ).
ThesefeaturesarethenclusteredintoN groupsusingtheK-MeansalgorithmwithCUDAacceleration.
Theclustercentersconstitutethecodebook. Bydefault,N isconfiguredto100,000. Wespecifythe
codebookentriestohaveadimensionof8.
6Table2: ReconstructionperformanceonImageNet-1K.Theterm“#Tokens”referstothenumber
oftokensusedtorepresentanimage. Thecodebookutilizationrateiscomputedacrossalltraining
images.TheFCandEMAmechanismsareoriginallyintroducedbyViT-VQGAN[21]andVQVAE[6,
7],respectively. Itisimportanttonotethatincreasingthecodebooksizeincursalmostnoadditional
computationalcost.
Method #Tokens CodebookSize Utilization(%) rFID LPIPS PSNR SSIM
DQVAE[20] 256 1,024 - 4.08 - - -
DF-VQGAN[45] 256 12,288 - 5.16 - - -
DiVAE[46] 256 16,384 - 4.07 - - -
RQVAE[22] 256 16,384 - 3.20 - - -
RQVAE[22] 512 16,384 - 2.69 - - -
RQVAE[22] 1,024 16,384 - 1.83 - - -
DF-VQGAN[45] 1,024 8,192 - 1.38 - - -
256 16,384 3.4 5.96 0.17 23.3 52.4
VQGAN[1] 256 50,000 1.1 5.44 0.17 22.5 52.5
256 100,000 0.5 5.44 0.17 22.3 52.5
256 16,384 11.2 4.29 0.17 22.8 54.5
VQGAN-FC[21] 256 50,000 3.6 4.96 0.15 23.1 54.7
256 100,000 1.9 4.65 0.15 22.9 55.1
256 16,384 83.2 3.41 0.14 23.5 56.6
VQGAN-EMA[7] 256 50,000 40.2 3.88 0.14 23.2 55.9
256 100,000 24.2 3.46 0.13 23.4 56.2
256 16,384 99.9 3.01 0.13 23.2 56.4
256 50,000 99.9 2.75 0.13 23.8 58.4
VQGAN-LC(Ours)
256 100,000 99.9 2.62 0.12 23.8 58.9
1,024 100,000 99.5 1.29 0.07 27.0 71.6
Table3: ReconstructionperformanceonFFHQ.
Method #Tokens CodebookSize Utilization(%) rFID LPIPS PSNR SSIM
RQVAE[22] 256 2,048 - 7.04 0.13 22.9 67.0
VQWAE[47] 256 1,024 - 4.20 0.12 22.5 66.5
MQVAE[48] 256 1,024 78.2 4.55 - - -
VQGAN[1] 256 16,384 2.3 5.25 0.12 24.4 63.3
VQGAN-FC[21] 256 16,384 10.9 4.86 0.11 24.8 64.6
VQGAN-EMA[7] 256 16,384 68.2 4.79 0.10 25.4 66.1
VQGAN-LC(Ours) 256 100,000 99.5 3.81 0.08 26.1 69.4
ImageGenerationModels. ForLDM[3],DiT[4]andSiT[5],weadopttheiroriginalarchitectures.
ForgenerationusingGPT[2], wefollowVQGAN[1], usingacausalTransformerdecoderwith
24layers,16headsperattentionlayer,alatentdimensionof1,024andatotalof404Mparameters.
ForImageNet,weemployclass-conditionalgeneration,whereasforFFHQ,weuseunconditional
generation. InLDM,DiT,andSiT,classifier-freeguidance[3]isimplementedforclass-conditional
generation. MoreimplementationdetailscanbefoundinSectionA.
Evaluation. Intheimagereconstructiontask,weevaluateperformanceusingrFID,LPIPS,PSNR,
andSSIMmetricsonthevalidationsetsofImageNetandFFHQ.Forimageclassification,wemeasure
thetop-1accuracyonImageNet. Forimagegeneration,wecalculatetheFIDscoreonImageNet
using50KgeneratedimagescomparedagainsttheImageNettrainingset. ForFFHQ,theFIDscoreis
determinedusing50Kgeneratedimagesincomparisonwiththecombinedtrainingandvalidation
setsofFFHQ.
4.2 MainResults
Image Reconstruction. Tables 2 and 3 present the reconstruction performance for ImageNet
andFFHQ,respectively. Wemakethreekeyobservations: 1)Ourmethodconsistentlyachievesa
codebookutilizationrateofover99%acrossallcodebooksizesonbothdatasets.2)Thereconstruction
performanceimprovesconsistentlywiththescalingofcodebooksizeusingourmethod. 3)Increasing
the codebook size (e.g., VQGAN-LC with a codebook size of 100,000 and 256 tokens), and the
7Table4: ImagegenerationonImageNet-1K.
Method #Tokens CodebookSize Utilization(%) FID
RQTransformer(GPT-480M)[22] 256 16,384 - 15.7
ViT-VQGAN(GPT-650M)[21] 256 8,192 - 11.2
DQTransformer(GPT-355M)[20] 640 1,024 - 7.34
DQTransformer(GPT-655M)[20] 640 1,024 - 5.11
ViT-VQGAN(GPT-650M)[21] 1,024 8,192 - 8.81
Stackformer(GPT-651M)[48] 1,024 1,024 - 6.04
LDM[3] 1,024 16,384 - 8.11
withGPT-404M[2]
VQGAN-FC[21] 256 16,384 11.2 17.3
VQGAN-EMA[7] 256 16,384 83.1 16.3
VQGAN-LC(Ours) 256 100,000 97.0 15.4
withSiT-XL[5]
VQGAN-FC[21] 256 16,384 11.2 10.3
VQGAN-EMA[7] 256 16,384 83.1 9.31
VQGAN-LC(Ours) 256 100,000 99.6 8.40
withDiT-XL[4]
VQGAN-FC[21] 256 16,384 11.2 13.7
VQGAN-EMA[7] 256 16,384 85.3 13.4
VQGAN-LC(Ours) 256 100,000 99.4 10.8
withLDM[3]
VQGAN-FC[21] 256 16,384 11.2 9.78
VQGAN-EMA[7] 256 16,384 83.1 9.13
VQGAN-LC(Ours) 256 100,000 99.4 8.36
VQGAN-LC(Ours) 1,024 100,000 99.4 4.81
Table5: ImagegenerationonFFHQ.
Method #Tokens CodebookSize Utilization(%) FID
Stackformer(GPT-307M)[48] 256 1,024 - 7.67
DQTransformer(GPT-308M)[20] 640 1,024 - 4.91
Stackformer(GPT-307M)[48] 1,024 1,024 - 6.84
Stackformer(GPT-651M)[48] 1,024 1,024 - 5.67
ViT-VQGAN(GPT-650M)[21] 1,024 8,192 - 3.13
LDM[3] 4,096 8,192 - 4.98
withLDM[3]
VQGAN-FC 256 16,384 11.2 13.2
VQGAN-EMA 256 16,384 68.2 12.5
VQGAN-LC(Ours) 256 100,000 99.7 12.3
withGPT(404M)[2]
VQGAN-FC 256 16,384 10.9 3.23
VQGAN-EMA 256 16,384 68.2 4.87
VQGAN-LC(Ours) 256 100,000 99.1 2.61
number of tokens to represent an image (e.g., RQVAE with a codebook size of 16,384 and 512
tokens)bothenhanceperformance,withtheformerintroducingalmostnoadditionalcomputational
costcomparedtothelatter.
ImageGeneration. Table4showstheresultsofclass-conditionalimagegenerationonImageNet.
Allmodels(GPT,LDM,DiT,andSiT)demonstrateimprovedperformancewiththeintegrationof
ourVQGAN-LC,regardlessoftheirunderlyingarchitectures,whichincludeauto-regressivecausal
Transformers, diffusion models, diffusion models with Transformer backbones, and flow-based
generativemodels. Thediversityofthegeneratedimagesincreasesduetotheutilizationofalarge
codebook,whichhasasizeofupto100,000andautilizationrateexceeding99%. Table5displays
the unconditional generation results on the FFHQ dataset. Notably, DiT and SiT, which use the
Transformerarchitecture,requiremoreextensivetrainingdataforoptimizingdiffusion-andflow-
based generative models. Given that FFHQ is significantly smaller than ImageNet, we limit our
trainingonFFHQtoGPTandLDM.
8Table6: AblationstudyofusingvariouscodebookinitializationstrategiesonImageNet.
Strategy Dataset Model Utilization(%) rFID LPIPS PSNR SSIM
RandomInitialization - - 5.4 108.7 0.46 18.2 36.4
RandomSelection ImageNet ViT-L 99.8 2.95 0.12 23.8 58.9
K-MeansClutering ImageNet ResNet-50 99.8 2.71 0.12 23.7 58.3
K-MeansClutering ImageNet ViT-B 99.9 2.70 0.12 23.8 58.7
K-MeansClutering ImageNet ViT-L 99.9 2.62 0.12 23.8 58.9
Table7: AblationstudyofusingdifferentcodebooksizesonImageNet.
CodebookSize Utilization(%) rFID LPIPS PSNR SSIM
1,000 100.0 4.98 0.17 22.9 55.3
10,000 99.8 3.80 0.14 23.3 57.2
50,000 99.9 2.75 0.13 23.8 58.4
100,000 99.9 2.62 0.12 23.8 58.9
200,000 99.8 2.66 0.12 23.9 59.2
Table 8: Ablation study on codebook transferability. The term “source dataset→target dataset”
indicatesthatthecodebookisinitializedusingthesourcedataset,whileourVQGAN-LCistrained
onthetargetdataset.
Setting Utilization(%) rFID LPIPS PSNR SSIM
FFHQ→FFHQ 99.5 3.81 0.08 26.1 69.4
ImageNet→FFHQ 99.4 4.08 0.08 26.1 69.4
ImageNet→ImageNet 99.9 2.62 0.12 23.8 58.9
FFHQ→ImageNet 99.9 2.91 0.12 23.8 58.9
Image Classification. In Section 3.3, wediscuss the training of an image classifier on a dataset
containing tokenized images. We fine-tune three ViT-B [36] models, pre-trained by MAE [42],
using the tokenized images produced by the top-performing VQGAN-FC, VQGAN-EMA, and
our proposed VQGAN-LC, on ImageNet. Both VQGAN-FC and VQGAN-EMA demonstrate
optimalreconstructionperformancewhenutilizingacodebookwith16,384entries. Asillustratedin
Figure1(b),ourmethodachievesatop-1accuracyof75.7onImageNet,surpassingVQGAN-FCand
VQGAN-EMAbymarginsof1.6and1.9,respectively.
Visualizations. SectionCpresentsimagesgeneratedbyGPT[2],LDM[3],DiT[4],andSiT[5],
incorporatingourVQGAN-LC.
4.3 AblationStudies
Unlessotherwisespecified,weevaluatereconstructionperformanceonImageNetacrossallstudies.
CodebookInitialization. InSection3.2,wedescribethedefaultcodebookinitializationapproach.
ThisinvolvesusingaCLIPvisionencoderwithaViT-Lbackbonetoextractpatch-levelfeatures
fromImageNet,followedbyaK-meansclusteringalgorithmtogenerateN clustercenters,resulting
inthestaticcodebookB. InTable6,weevaluatetwofactors: 1)employingaCLIPvisionencoder
withdifferentbackbones(e.g.,ViT-B[36]andResNet-50[49])toextractpatch-levelfeatures;and2)
utilizingnon-clusteringstrategiestoinitializethestaticcodebook,includingrandominitializationand
randomselection,whereN featuresarerandomlychosenfromallpatch-levelfeaturesofImageNet.
Ourfindingsarethreefold: 1)randominitializationleadstoextremelypoorperformancesincethe
codebookremainsfrozeninourVQGAN-LC;2)usingaCLIPvisionencoderwithaViT-Lbackbone
outperformsthoseusingViT-BandResNet-50backbones;and3)K-meansclusteringproducesa
morerobustcodebook.
Codebook Size. In Table 7, we incrementally increase the codebook size in our VQGAN-LC
from1,000to200,000. Theperformanceshowsminimalimprovementsbeyondacodebooksizeof
100,000—specifically,only0.01PSNRand0.03SSIMgainsareobserved,whenthecodebooksize
reaches200,000. Therefore,weconsistentlyuseacodebooksizeof100,000inallexperiments. The
codebookutilizationrateconsistentlyexceeds99%acrossallconfigurations.
CodebookTransferability. Inourstandardsetup,boththecodebookinitializationandtheVQGAN-
LCtrainingareconductedusingthesamedataset,eitherImageNetorFFHQ.InTable8,weexamine
thetransferabilityofthecodebookbyinitializingitwithonedatasetandtrainingourVQGAN-LCon
9adifferentdataset. Ourfindingsindicatethatourmethodexhibitssignificantcodebooktransferability,
highlightingtherobustnessofourcodebookinitializationprocess.
5 Conclusion
In this work, we introduce a novel image quantization model, VQGAN-LC, which extends the
codebooksizeto100,000,achievingautilizationrateexceeding99%. Ourapproachsignificantly
outperforms prior models like VQGAN, VQGAN-FC and VQGAN-EMA, across various tasks
includingimagereconstruction,imageclassificationandimagesynthesisusingnumerousgeneration
modelssuchasauto-regressivecausalTransformers(GPT),latentdiffusionmodels(LDM),diffusion
Transformers(DiT),andflow-basedgenerativemodels(SiT),whileincurringalmostnoadditional
costs. ExtensiveexperimentsonImageNetandFFHQverifytheeffectivenessofourapproach.
References
[1] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image
synthesis. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages
12873–12883,2021.
[2] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
[3] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages10684–10695,2022.
[4] WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,pages4195–4205,2023.
[5] NanyeMa,MarkGoldstein,MichaelSAlbergo,NicholasMBoffi,EricVanden-Eijnden,andSainingXie.
Sit:Exploringflowanddiffusion-basedgenerativemodelswithscalableinterpolanttransformers. arXiv
preprintarXiv:2401.08740,2024.
[6] AaronVanDenOord,OriolVinyals,etal. Neuraldiscreterepresentationlearning. Advancesinneural
informationprocessingsystems,30,2017.
[7] AliRazavi,AaronVandenOord,andOriolVinyals.Generatingdiversehigh-fidelityimageswithvq-vae-2.
Advancesinneuralinformationprocessingsystems,32,2019.
[8] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal.Improvinglanguageunderstanding
bygenerativepre-training. 2018.
[9] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners.
Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
[10] OpenAI. Gpt-4technicalreport,2023.
[11] YutongBai,XinyangGeng,KarttikeyaMangalam,AmirBar,AlanYuille,TrevorDarrell,JitendraMalik,
andAlexeiAEfros. Sequentialmodelingenablesscalablelearningforlargevisionmodels. arXivpreprint
arXiv:2312.00785,2023.
[12] HuiwenChang,HanZhang,LuJiang,CeLiu,andWilliamTFreeman.Maskgit:Maskedgenerativeimage
transformer. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages11315–11325,2022.
[13] TianhongLi,HuiwenChang,ShlokMishra,HanZhang,DinaKatabi,andDilipKrishnan. Mage:Masked
generativeencodertounifyrepresentationlearningandimagesynthesis. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages2142–2152,2023.
[14] KeyuTian,YiJiang,ZehuanYuan,BingyuePeng,andLiweiWang. Visualautoregressivemodeling:
Scalableimagegenerationvianext-scaleprediction. arXivpreprintarXiv:2404.02905,2024.
[15] QuanDao, HaoPhung, BinhNguyen, andAnhTran. Flowmatchinginlatentspace. arXivpreprint
arXiv:2307.08698,2023.
[16] LijunYu,YongCheng,ZhiruoWang,VivekKumar,WolfgangMacherey,YanpingHuang,DavidARoss,
IrfanEssa,YonatanBisk,Ming-HsuanYang,etal. Spae:Semanticpyramidautoencoderformultimodal
generationwithfrozenllms. arXivpreprintarXiv:2306.17842,2023.
[17] Hao Liu, Wilson Yan, and Pieter Abbeel. Language quantized autoencoders: Towards unsupervised
text-imagealignment. arXivpreprintarXiv:2302.00902,2023.
10[18] Lei Zhu, Fangyun Wei, and Yanye Lu. Beyond text: Frozen large language models in visual signal
comprehension. arXivpreprintarXiv:2403.07874,2024.
[19] JiasenLu,ChristopherClark,RowanZellers,RoozbehMottaghi,andAniruddhaKembhavi. Unified-io:A
unifiedmodelforvision,language,andmulti-modaltasks. InTheEleventhInternationalConferenceon
LearningRepresentations,2022.
[20] MengqiHuang,ZhendongMao,ZhuoweiChen,andYongdongZhang. Towardsaccurateimagecoding:
Improved autoregressive image generation with dynamic vector quantization. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages22596–22605,2023.
[21] JiahuiYu,XinLi,JingYuKoh,HanZhang,RuomingPang,JamesQin,AlexanderKu,YuanzhongXu,
JasonBaldridge,andYonghuiWu. Vector-quantizedimagemodelingwithimprovedvqgan. arXivpreprint
arXiv:2110.04627,2021.
[22] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image
generationusingresidualquantization. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages11523–11532,2022.
[23] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalconferenceonmachinelearning,pages8748–8763.PMLR,
2021.
[24] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehierarchical
imagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,pages248–255.
Ieee,2009.
[25] ChuanxiaZheng,Tung-LongVuong,JianfeiCai,andDinhPhung. Movq:Modulatingquantizedvectors
forhigh-fidelityimagegeneration. AdvancesinNeuralInformationProcessingSystems,35:23412–23425,
2022.
[26] WillWilliams,SamRinger,TomAsh,DavidMacLeod,JamieDougherty,andJohnHughes. Hierarchical
quantizedautoencoders. AdvancesinNeuralInformationProcessingSystems,33:4524–4535,2020.
[27] JialunPeng,DongLiu,SongcenXu,andHouqiangLi. Generatingdiversestructureforimageinpainting
withhierarchicalvq-vae. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages10775–10784,2021.
[28] OlafRonneberger,PhilippFischer,andThomasBrox. U-net: Convolutionalnetworksforbiomedical
imagesegmentation.InMedicalimagecomputingandcomputer-assistedintervention–MICCAI2015:18th
internationalconference,Munich,Germany,October5-9,2015,proceedings,partIII18,pages234–241.
Springer,2015.
[29] JiahuiZhang,FangnengZhan,ChristianTheobalt,andShijianLu. Regularizedvectorquantizationfor
tokenizedimagesynthesis. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages18467–18476,2023.
[30] PrafullaDhariwal,HeewooJun,ChristinePayne,JongWookKim,AlecRadford,andIlyaSutskever.
Jukebox:Agenerativemodelformusic. arXivpreprintarXiv:2005.00341,2020.
[31] ChuanxiaZhengandAndreaVedaldi. Onlineclusteredcodebook. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages22798–22807,2023.
[32] AäronVanDenOord,NalKalchbrenner,andKorayKavukcuoglu. Pixelrecurrentneuralnetworks. In
Internationalconferenceonmachinelearning,pages1747–1756.PMLR,2016.
[33] AlexGravesandAlexGraves. Longshort-termmemory. Supervisedsequencelabellingwithrecurrent
neuralnetworks,pages37–45,2012.
[34] AaronVandenOord,NalKalchbrenner,LasseEspeholt,OriolVinyals,AlexGraves,etal. Conditional
imagegenerationwithpixelcnndecoders. Advancesinneuralinformationprocessingsystems,29,2016.
[35] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.
Generativepretrainingfrompixels. InInternationalconferenceonmachinelearning,pages1691–1703.
PMLR,2020.
[36] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessingsystems,
30,2017.
[37] JacobAustin,DanielDJohnson,JonathanHo,DanielTarlow,andRianneVanDenBerg. Structured
denoisingdiffusionmodelsindiscretestate-spaces. AdvancesinNeuralInformationProcessingSystems,
34:17981–17993,2021.
[38] ShuyangGu,DongChen,JianminBao,FangWen,BoZhang,DongdongChen,LuYuan,andBainingGuo.
Vectorquantizeddiffusionmodelfortext-to-imagesynthesis. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages10696–10706,2022.
11[39] ZhicongTang,ShuyangGu,JianminBao,DongChen,andFangWen.Improvedvectorquantizeddiffusion
models. arXivpreprintarXiv:2205.16007,2022.
[40] AndreasBlattmann,RobinRombach,HuanLing,TimDockhorn,SeungWookKim,SanjaFidler,and
Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages22563–
22575,2023.
[41] ChenWei,ChenxiLiu,SiyuanQiao,ZhishuaiZhang,AlanYuille,andJiahuiYu. De-diffusionmakestext
astrongcross-modalinterface. arXivpreprintarXiv:2311.00618,2023.
[42] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick.Maskedautoencoders
arescalablevisionlearners. arXiv:2111.06377,2021.
[43] TeroKarras,SamuliLaine,andTimoAila. Astyle-basedgeneratorarchitectureforgenerativeadversarial
networks. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages
4401–4410,2019.
[44] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980,2014.
[45] MinhengNi,XiaomingLi,andWangmengZuo. Nuwa-lip:language-guidedimageinpaintingwithdefect-
freevqgan. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages14183–14192,2023.
[46] JieShi,ChenfeiWu,JianLiang,XiangLiu,andNanDuan. Divae:Photorealisticimagessynthesiswith
denoisingdiffusiondecoder. arXivpreprintarXiv:2206.00386,2022.
[47] Tung-LongVuong,TrungLe,HeZhao,ChuanxiaZheng,MehrtashHarandi,JianfeiCai,andDinhPhung.
Vectorquantizedwassersteinauto-encoder. arXivpreprintarXiv:2302.05917,2023.
[48] MengqiHuang,ZhendongMao,QuanWang,andYongdongZhang. Notallimageregionsmatter:Masked
vectorquantizationforautoregressiveimagegeneration. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition(CVPR),pages2002–2011,June2023.
[49] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecognition.
InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages770–778,2016.
12Table9: TheimpactofmaintainingastaticcodebookandincorporatingaprojectoronImageNet.
Static Projector Utilization(%) rFID LPIPS PSNR SSIM
✓ 3.9 18.6 0.36 19.2 40.4
✓ 99.1 2.65 0.12 23.7 58.0
✓ ✓ 99.9 2.62 0.12 23.8 58.1
Table10: AblationstudyonthedimensionoftheprojectedcodebookonImageNet.
Dimension Utilization(%) rFID LPIPS PSNR SSIM
8 99.8 2.66 0.12 23.9 59.2
16 99.8 2.36 0.12 23.8 58.8
32 99.8 2.75 0.13 23.6 58.7
128 99.8 2.37 0.12 23.6 58.3
256 99.8 2.49 0.12 23.9 59.4
512 99.9 2.76 0.12 23.5 58.1
A MoreImplementationDetails
GPT.WetrainGPTwithabatchsizeof1024across32NvidiaV100GPUs. TheAdamoptimizeris
employedwithaninitiallearningrateof4.5e−4,andthemodelistrainedfor100epochs. Moreover,
alineardecayscheduleisusedforadjustingthelearningrate. A5-epochlinearwarm-upphaseis
adopted. Top-ksamplingisadoptedforauto-regressivegeneration, wherek issetas10%ofthe
vocabularysize.
LDM. The implementation utilizes four UNet layers with channel dimensions of
{256,1024,1024,256}. Conditions are integrated through a cross-attention mechanism at
each UNet layer. The model is trained using the Adam optimizer with an initial learning rate of
4.5e−4 andabatchsizeof448, distributedacross 8NvidiaV100GPUs. Thetrainingprocessis
conducted over 100 epochs. The classifier-free guidance scale is set as 1.4 for class-conditional
generation.
DiT. We employ the 28-layer DiT-XL model with a patch size of 2, consisting of 675 million
parameters. Themodelfeatures16attentionheadsandanembeddingdimensionof1152. Tohandle
classconditions,weutilizetheAdaLN-Zeroblock. Foroptimization,theAdamoptimizerisused
withaninitiallearningrateof4.5e−4,andthemodelistrainedfor400,000iterationsontheImageNet
dataset. The training is performed with a batch size of 256 across 8 Nvidia V100 GPUs. The
classifier-freeguidancescaleissetas8forclass-conditionalgeneration.
SiT.WeutilizeSiT-XLasourflow-basedgenerativemodel,mirroringthearchitectureofDiT-XL.
Foroptimization,theAdamoptimizerisemployedwithaninitiallearningrateof4.5e−4. Themodel
undergoestrainingfor400,000iterationsontheImageNetdataset. Thetrainingprocessisconducted
withabatchsizeof256,distributedacross8NvidiaV100GPUs. Theclassifier-freeguidancescale
issetto8forclass-conditionalgeneration.
B MoreExperiments
Projector and Static Codebook. As described in Section 3.2, the codebook is initialized using
a CLIP vision encoder to extract patch-level features on ImageNet. During the training of our
VQGAN-LC,weoptimizeaprojectortomaptheentirecodebooktoalatentspace. Table9illustrates
thesignificanceoftuningtheprojectoronthestaticcodebookbycomparingourdefaultstrategy
withtwoalternatives: 1)omittingtheprojector;and2)makingeachentryintheinitializedcodebook
trainable. Ourresultsshowthatincorporatingtheprojectormarkedlyimprovesperformance,whereas
makingthecodebookentriestrainablehasminimalimpact.
DimensionoftheProjectedCodebook. InourVQGAN-LC,theprojectortransformsthecodebook
B into a latent codebook B′, with each entry in B′ having a dimension denoted as D′. Table 10
showstheresultsofvaryingD′from8to512. Wefindthatourmodel’sperformanceremainsstable
regardlessofthevalueofD′,consistentlyachievingacodebookutilizationrateofover99%inall
configurations.
13Codebook Size (1024) Codebook Size (16384) Codebook Size (50,000) Codebook Size (100,000)
100% Utilization Rate 11.2% Utilization Rate 3.6% Utilization Rate 1.9% Utilization Rate
100% Utilization Rate 83.2% Utilization Rate 40.2% Utilization Rate 24.2% Utilization Rate
100% Utilization Rate 99.9% Utilization Rate 99.9% Utilization Rate 99.9% Utilization Rate
Figure4: Visualizationoftheactiveandinactivecodesforthreemodels(VQGAN-FC,VQGAN-
EMA,andourVQGAN-LC)usingt-SNE.
VisualizationofActiveandInactiveCodes.Figure4showsthedistributionoftheactiveandinactive
codesforthreemodels: VQGAN-FC,VQGAN-EMA,andourVQGAN-LC.Thevisualizationis
createdusingt-SNE.Activecodebookentriesarehighlightedingreen,whileinactiveonesareshown
inblue. Asthecodebooksizeincreases,morecodestendtobeinactiveinbothVQGAN-FCand
VQGAN-EMAmodels.
TokenReplacement. VQGAN,VQGAN-FC,andVQGAN-EMAallutilizeacodebookof16,384
entriestoachieveoptimalperformance. Incontrast,ourVQGAN-LCcanscalethecodebooksize
upto100,000whilemaintaininganexceptionallyhighutilizationrateofover99%,allowingeach
tokentorepresentmoredetailedvisualelements. Thisisverifiedthroughanablationstudy: foreach
inputimage,weuseVQGAN-FC(-EMA/-LC)’sencodertoconverttheimageintoatokenmap. We
thenreplaceeachtokeninthemapwiththeMth nearestentryfromitscodebook. Themodified
tokenmapisfedintothedecoderforreconstruction. Figure5showsthePSNRresultsonasubset
ofImageNet,usingimagesfrom100randomlyselectedcategories,andvisualizestheresultsofour
VQGAN-LC,VQGAN-FC,andVQGAN-EMAwhenM issetto1,50,100,and1000.
C Visualizations
InFigures6-9,wepresenttheclass-conditionalgenerationresultsataresolutionof256×256forour
VQGAN-LCwithGPT[2],LDM[3],DiT[4],andSiT[5],respectively,using256(16×16)tokens
onImageNet. Additionally,Figure10illustratestheresultsofVQGAN-LCwithLDMusing1024
(32×32)tokensonImageNet. Figure11showstheunconditionalgenerationresultsataresolution
of 256×256 for VQGAN-LC with LDM on the FFHQ dataset, utilizing 256 (16×16) tokens.
Theintroductionofalarge-scalecodebookfacilitatesthegenerationofimageswithdiverseposes,
intricatetextures,andcomplexbackgrounds.
D LimitationsandBroadImpacts
WepresentanewimagequantizationtechniquecalledVQGAN-LC,whichexpandsthecodebook
size to 100,000 with a utilization rate of 99%. This approach has the potential to enhance any
downstreamapplicationsthatinvolveimagequantizationmodels. However,VQGAN-LCistrained
ontheImageNetandFFHQdatasets,whichrestrictsdownstreamapplications,likeimagegeneration,
toproducingimagesfromthelimitedcategoriesfoundinthesedatasets. WhiletrainingVQGAN-LC
onlargerdatasetslikeLAION-5Bmayimproveitsutilityindownstreamapplications,itwouldalso
bemorecostlyandcomputationallydemanding. Furthermore,pleaserefrainfromusingthismodel
togeneratemaliciousorinappropriatecontent. Itisintendedsolelyforpositiveandconstructive
purposesinresearchandcreativity.
14
CF-NAGQV
AME-NAGQV
)sruO(
CL-NAGQVOurs
VQGAN-EMA
V
Q
GAN-FC
Replacement with the MthNearest Entry
Raw Image M=1 M=50 M=100 M=1000 Raw Image M=1 M=50 M=100 M=1000
Figure5: Foragivenimage,weemployanimagequantizationmodel(VQGAN-FC,VQGAN-EMA,
orourVQGAN-LC)totransformitintoatokenmap. Eachtokeninthismapisthensubstitutedwith
theMthnearestentryfromthecodebook.Thisalteredtokenmapissubsequentlyfedintothedecoder
forreconstruction. (Top)PSNRforeachconfiguration. (Bottom)Reconstructionvisualizationsfor
thethreemodels.
15
RNSP
CF-NAGQV
AME-NAGQV
sruO
CF-NAGQV
AME-NAGQV
sruO
CF-NAGQV
AME-NAGQV
sruO
VQGAN-FC
VQGAN-EMA
Ours
VQGAN-FC
VQGAN-EMA
Ours
VQGAN-FC
VQGAN-EMA
OursAustralian terrier (193) Rapeseed (984) Agaric (992)
Figure6: Qualitativeresultsofclass-conditionalgenerationusingourVQGAN-LCwithGPT[2]on
ImageNet,utilizing256(16×16)tokens. Wedisplaythecategorynameandcorrespondingcategory
IDforeachgroup.
Volcano (980) ArcticFox (279) Pug (254)
Figure7: Qualitativeresultsofclass-conditionalgenerationusingourVQGAN-LCwithLDM[3]on
ImageNet,utilizing256(16×16)tokensandaclassifier-freeguidancescaleof1.4. Wedisplaythe
categorynameandcorrespondingcategoryIDforeachgroup.
Lake Shore (975) Geyser (974) Space Shuttle (812)
Figure8: Qualitativeresultsofclass-conditionalgenerationusingourVQGAN-LCwithDiT[4]on
ImageNet,utilizing256(16×16)tokensandaclassifier-freeguidancescaleof8.0. Wedisplaythe
categorynameandcorrespondingcategoryIDforeachgroup.
16Cliff (972) Balloon (417) Guinea Pig (338)
Figure9: Qualitativeresultsofclass-conditionalgenerationusingourVQGAN-LCwithSiT[5]on
ImageNet,utilizing256(16×16)tokensandaclassifier-freeguidancescaleof8.0. Wedisplaythe
categorynameandcorrespondingcategoryIDforeachgroup.
Valley (979) Red Panda (387) Macaw (88)
Figure10: Qualitativeresultsofclass-conditionalgenerationusingourVQGAN-LCwithLDM[3]
onImageNet,utilizing1024(32×32)tokensandaclassifier-freeguidancescaleof1.4. Wedisplay
thecategorynameandcorrespondingcategoryIDforeachgroup.
Figure11: QualitativeresultsofunconditionalgenerationusingourVQGAN-LCwithLDM[3]on
FFHQ,utilizing256(16×16)tokens.
17