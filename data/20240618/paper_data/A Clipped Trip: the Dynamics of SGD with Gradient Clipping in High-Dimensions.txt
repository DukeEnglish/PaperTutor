A Clipped Trip: the Dynamics of SGD with Gradient
Clipping in High-Dimensions
Noah Marshall1, Ke Liang Xiao1, Atish Agarwala2, and Elliot Paquette1
1Department of Mathematics and Statistics, McGill University, Montreal, Canada
{noah.marshall2, keliangxiao}@mail.mcgill.ca, elliot.paquette@mcgill.ca
2Google DeepMind, thetish@google.com
Abstract
Thesuccessofmodernmachinelearningisdueinparttotheadaptiveoptimiza-
tion methods that have been developed to deal with the difficulties of training
large models over complex datasets. One such method is gradient clipping: a
practical procedure with limited theoretical underpinnings. In this work, we
study clipping in a least squares problem under streaming SGD. We develop a
theoretical analysis of the learning dynamics in the limit of large intrinsic di-
mension—amodelanddatasetdependentnotionofdimensionality. Inthislimit
we find a deterministic equation that describes the evolution of the loss. We
show that with Gaussian noise clipping cannot improve SGD performance. Yet,
in other noisy settings, clipping can provide benefits with tuning of the clipping
threshold. In these cases, clipping biases updates in a way beneficial to training
whichcannotberecoveredbySGDunderanyschedule. Weconcludewithadis-
cussion about the links between high-dimensional clipping and neural network
training.
1 Introduction
Stochasticgradientdescent(SGD)methodsarethestandardfornearlyalllargescalemodern
optimization tasks. Even with the ever growing complexities of neural nets, with sufficient
hyper-parameter tuning, SGD often outperforms other more complex methods. To deal
with the difficulties of training large models over complex datasets, adaptive SGD methods
havebeendeveloped. Oneofthesimplestsuchmethodsisgradientclipping[1,2]. Gradient
clipping replaces any stochastic gradient ∇ f (x) with a clipped gradient clip (∇ f (x)),
θ θ c θ θ
for some threshold c, where
(cid:18) (cid:19)
c
clip (z)=min 1, z. (1)
c ∥z∥
While gradient clipping was first introduced to address the problem of exploding gradients
in recurrent neural networks, it has become an integral part of training models for NLP [3].
Ithasalsofounduseinotherdomainssuchasdifferentialprivacy[4,5]andcomputervision
[6, 7].
Despite widespreaduse, the reasons behind the effectiveness ofclipping remain somewhata
mystery. Forinstance,itisunclearexactlyhowthegradientdistributionaffectstraining,or
for which distributions clipping can offer benefits. It is hypothesized that the distribution
of the gradient norms plays a large role [8]. Also, it is unknown how one should adjust the
clipping threshold as the problem scales. There has been growing interest in how models
andtheiroptimalhyper-parametersscalewithdimension[9]. Understandingthisbehaviour
would allow one to perform hyperparameter tuning on smaller, more efficient models before
scaling to a potentially very large final architecture.
1
4202
nuJ
71
]LM.tats[
1v33711.6042:viXraInthisworkwedevelopatheoryofclippedSGDinhigh-dimensionsunderthemean-squared
error loss (MSE) over a class of random least-squares problems. After formally introducing
the class of considered problems (Sec. 2), we show the following:
• Inhigh-dimensionsthedynamicsofclippedSGD(C-SGD)arewelldescribedbyanSDE,
clippedhomogenizedSGD(C-HSGD).Weprovideanon-asymptoticboundonthediffer-
ence of the risk curves between C-SGD and C-HSGD. Under C-HSGD the risk evolution
can be described by a system of ODEs (Sec. 3).
• UsingC-HSGD,weshowthatthedifferencesbetweenclippedandunclippedSGDcanbe
described by two unitless reduction factors µ and ν which encode the effect of clipping
(Sec. 3).
• The reduction factors control the stability of the algorithm. They describe the precise
clipping-learning rate combinations which are convergent. Moreover, we identify some
clipping schedules that improve stability (Sec. 4).
• We find a general criterion for when clipping can speed up optimization, described by a
different ratio of the reduction factors. We then identify a problem setup where clipping
never helps as well as one where clipping improves performance. (Sec. 5).
We conclude with a discussion about the links between our analysis and quantities measur-
able in real neural networks.
Related work: The distribution of noise in stochastic gradients and its effect on training
was studied by Zhang et al. [8]. They argue that this noise is well approximated by a
Gaussian for ResNets [10] trained on Imagenet [11], while a heavy-tailed distribution is
more appropriate with BERT [12] on an NLP dataset. They show that for heavy-tailed
noise unclipped SGD diverges while clipped SGD can converge. Other theoretical analyses
on clipping often focus on imposing smoothness conditions on the loss function, and then
performing analysis for fixed learning rates [13, 14, 15]. These works have shown that fixed
rate clipped SGD can outperform unclipped SGD under certain conditions. Other works
have also studied SGD through the lens of SDEs [16, 17, 18]. More recently, partly spurred
bythesheersizeofmodernmodelsaswellastheapparentregularityatwhichtheyscale[9],
therehasbeenaninterestinstudyingstochasticoptimizationinhigh-dimensionswithSDEs.
There is a formal correspondence between the dynamics of learning-relevant quantities like
the loss and the trajectory of an equivalent SDE. These relationships have been worked out
for SGD in the streaming setup over a variety of losses [19, 20], and the resulting analyses
lead to quantities which can be useful for understanding learning dynamics in practical
models [21].
2 Problem setup
In this work, we consider linear regression using the mean-squared loss
L(θ,x,y)=∥⟨x,θ⟩−y∥2/2, (2)
in the streaming or one-pass scenario, where data is not reused. Clipped SGD (C-SGD),
without mini-batching, is described by the iteration
θ =θ −η clip (∇ L(θ,x,y)), (3)
k+1 k k ck θ
where ∇ L(θ,x,y)=(⟨x ,θ ⟩−y )x with initialization θ ∈Rd. We assume that
x k+1 k k+1 k+1 0
the samples {(x ,y )} , consisting of data x and targets y , satisfy the following:
k k k≥0 k k
Assumption 1. The data x ∈ Rd are Gaussian with covariance K. The targets y are
generated by y =⟨x,θ∗⟩+ϵ, where ϵ represents noise and θ∗ is the ground-truth.
Thenoiseiscenteredandsubgaussianwithsubgaussiannorm∥ϵ∥ ≤vandvarianceE[ϵ2]=
ψ2
σ2 for some v,σ ≥0.
We formulate a more general version of our results for non-Gaussian data in Appendix A.
2Definition 1. Define the population risk and the noiseless risk:
P(θ)=E (cid:2) (⟨x,θ−θ∗⟩−ϵ)2(cid:3) /2 and R(θ)=E (cid:104) ⟨x,θ−θ∗⟩2(cid:105) /2, (4)
(x,ϵ) x
as well as the distance to optimality
D(θ)=∥θ−θ∗∥2.
Ourtheoryisphrasedintermsoftheintrinsicdimension,astatisticalnotionofdimensional-
ity which is occasionally much smaller than the ambient dimension d. There are interesting
settingswherethesedimensionsareeffectivelyinterchangeable,andassuch,thereadermay
wishto,atfirstglance,considertheresultstobephrasedintermsoftheambientdimension.
Definition 2 (IntrinsicDimension). Let the data x∈Rd have covariance matrix K. Define
the intrinsic dimension of the data to be
d=Tr(K)/∥K∥, (5)
where ∥K∥ refers to the operator norm. Note that d≤d. We will refer to d as the ambient
dimension.
Assumption 2. The covariance matrix K is normalized such that ∥K∥=1. Note that this
assumption may always be satisfied by rescaling the problem.
Thedefinitionofdcanbeextendedtoandmeasuredinrealneuralnetworkstrainedonreal
datasets; see Appendix C for more details.
Weallowfortheschedulingofboththeclippingthresholdandthelearningrate. Specifically,
Assumption 3. There are continuous bounded functions η : R+ → R+ and c : R+ → R+
such that
√
c =c(k/d) d η =η(k/d)/d. (6)
k k
We note that while it is reasonable for c(t) = ∞ (which is to say that no clipping occurs),
for technical reasons, we shall not allow this in our main theorem.
3 Clipped homogenized SGD
Our main result shows that the risk of C-SGD is well-approximated by the solution to an
SDE which we call clipped homogenized SGD (C-HSGD):
Definition 3 (Clipped Homogenized SGD). Denote the stochastic gradient as ℓ x, where
θ
ℓ =⟨x,θ−θ∗⟩−ϵ. Define the descent reduction factor and the variance reduction factor
θ
∥E[clip (ℓ )x]∥ E[clip2(ℓ )]
µ (θ)= c θ and ν (θ)= c θ . (7)
c ∥E[ℓ x]∥ c E[ℓ2]
θ θ
Then C-HSGD is defined to be the solution to
(cid:114)
2ν (Θ )P(Θ )K
c(t) t t
dΘ =−η(t)µ (Θ )∇P(Θ )dt+η(t) dB , (8)
t c(t) t t d t
where initialization is taken to be the same as SGD and B is a standard Brownian motion.
t
ThishassimilarstructuretoanSDEpreviouslyestablishedforunclippedSGD[20],withthe
addition of reduction factors µ and ν that capture the effects of clipping. The reduction
c c
factors take on values in [0,1] with the limits
lim µ = lim ν =0 and lim µ = lim ν =1. (9)
c c c c
c→0+ c→0+ c→∞ c→∞
In essence, the homogenized SGD suggests that in the limit d → ∞, clipped SGD is still
driven by a drift term in the direction of ∇ R—but clipping provides a bias against the
θ
3gradientwhichshrinksthedescentterm(hereafterthenegativetermin(8)). Meanwhile,the
diffusion term is shrunk by ν (θ) due to the reduction of variance of the clipped gradients.
c
These terms imply a tradeoff: clipping should aim to reduce variance (decrease ν ) more
c
than it shrinks the descent term (decrease µ ). We investigate this trade-off in detail in
c
Section 5.
Some computed examples of µ and ν for select data and noise distributions are given
c c
in Appendix D. Although computing ν is generally straightforward, calculating the risk-
c
coefficient µ is often more challenging. However, for Gaussian data, Stein’s Lemma shows
c
that
µ (θ)=P(|ℓ |≤c). (10)
c θ
A key point here is that this quantity depends only on the fraction of unclipped gradients.
We now state our main theorem.
Theorem 1. Suppose that Assumptions 1, 2 and 3 hold. Suppose that Θ and θ are
t k
independentrealizationsofC-HSGDandC-SGDwithequal, deterministicinitialconditions.
Let c = sup c(t) and η = sup η(t). There is a constant C = C(v,(n/d),c,η,∥θ −θ∗∥2), a
t t 0
stochastic process E, and a constant m=m(v) so that for any 1≤u≤md and any n
(cid:13)(cid:20) (cid:21) (cid:20) (cid:21)(cid:13)
sup (cid:13) (cid:13) R(θ k) − R(Θ k/d) (cid:13) (cid:13)≤CE(n/d)ulog(d)d−1/2, (11)
0≤k≤n(cid:13) D(θ k) D(Θ k/d) (cid:13)
with probability 1−e−u and provided the right hand side is less than 1. The stochastic
process E is given by
(cid:32) (cid:33)
(cid:90) t Cη(s)2σds
E(t)=exp
(cid:112)
R(Θ )+R(θ )
0 s sd
for an absolute constant C >0. The constant C can be bounded by
C ≤C(cid:112) n/dηv2·((1+∥θ −θ∗∥2)v2+c2(cid:112) n/d)·exp(cid:0) Cmax{η,η2}(n/d)(cid:1)
0
for an absolute constant C >0.
Informally, this theorem says that
(cid:13)(cid:20) (cid:21) (cid:20) (cid:21)(cid:13)
sup (cid:13) (cid:13) R(θ k) − R(Θ k/d) (cid:13) (cid:13)=O(log(d)d−1/2).
0≤k≤n(cid:13) D(θ k) D(Θ k/d) (cid:13)
In particular, as d grows, the risk curves of C-SGD and C-HSGD look closer to one another
for longer time windows and with higher probability. Under additional assumptions,1 such
thattheC-HSGDcurveconvergestoadimension-independentdeterministiclimit,thiswould
show convergence of the risk curves of C-SGD to a dimension-independent limit.
The presence of the E(t), while not desirable, should also not be alarming: when σ = 0
(recall Assumption 1), this disappears entirely. On the other hand, when σ ̸= 0, the risk
cannot decrease to 0 too quickly, and so in many setups (for example when η(s)≡η), this
will be no larger than em×(n/d) for a constant m that depends on σ,η,c, with very high
probability.
The complete proof is detailed in Appendix B along with the theorem statement and proof
for non-Gaussian data.
Extracting deterministic dynamics. For any twice differentiable function q, we have
η(t)2
dq(Θ )=−η(t)µ (Θ )∇P(Θ )T∇q(Θ )dt+ ν (Θ )P(Θ )Tr(K∇2q)dt+dM ,
t c(t) t t t d c(t) t t t
(12)
whereM isamartingalewhichvanishesasd→∞,whichisanexampleoftheconcentration
t
of measure phenomenon seen throughout high-dimensional probability. Hence, we have a
good deterministic approximation for the evolution of q(Θ ) by setting M ≡0.
t t
1The spectrum of K converges and the initialization θ −θ∗ converges
0
4Based on this idea, we can construct a coupled system of ODEs which will describe the
dynamics of the risk. A priori this is an infinite system of ODEs, but this difficulty can
be avoided through the use of the resolvent formalism R(z;K) = (K−zI)−1 for z ∈ C.
Consider
q (Θ
)=(cid:10)
(Θ
−θ∗)⊗2,R(z;K)(cid:11)
/2. (13)
z t t
We use this to define deterministic equivalents R and D for R(Θ ) and D(Θ ). Moreover,
t t t t
Theorem 1 holds as written with (R(Θ ),D(Θ )) replaced by (R ,D ) (see Theorem 8 in
t t t t
Appendix F, where we also elaborate on the system of ODEs). A numerical comparison of
C-SGD, C-HSGD, and the ODEs is provided in Figure 1.
The situation becomes much simpler when the data have identity covariance (aka isotropic
data).
Example 1 (Isotropic data). When the data is isotropic Gaussian, the R solves an au-
t
tonomous ODE:
R˙ =−2η(t)µ R +η(t)2ν (R +σ2/2), (14)
t c(t) t c(t) t
where R = R(Θ ). Here, we have used that since the data is Gaussian, it is possible
0 0
to express µ and ν as functions of the risk. As a slight abuse of notation we shall also
c c
write (µ (R(θ)),ν (R(θ))) for (µ (θ),ν (θ)), and we will suppress the dependence where
c c c c
appropriate. In particular, in (14), we have applied µ and ν to R .
c(t) c(t) t
0.18 C-HSGD 80.0% CI
C-HSGD 80.0% CI
C-SGD 80.0% CI
C-SGD 80.0% CI 0.25
0.16 ODE
ODE
Unclipped ODE
Unclipped ODE
0.14 0.20
0.12 0.15
0.10
0.10
0.0 2.5 5.0 7.5 10.0 12.5 15.0 0.0 2.5 5.0 7.5 10.0 12.5 15.0
Iterations Iterations
(a) d=179.74 (b) d=281.63
Figure 1: Comparison of C-SGD, C-HSGD and their deterministic equivalent (ODE) with
Gaussian noise, with the solution to the unclipped ODE for reference. The ambient dimen-
sion is d = 500 in both figures, but the intrinsic dimension d changes. The covariance has
eigenvalues following a power law j−α,j = 1,...,d with α = 1/5 and α = 1/9 for the left
and the right figures respectively. We have σ = 0.7, c = 0.9, η = 0.7. Plotted is the 80%
t
confidence interval across 100 runs.
4 Stability analysis
In this section we establish stability conditions for streaming SGD with clipping. Stability
thresholds from convex models are useful for understanding dynamics in deep learning [21,
22]. Additionally,alargerrangeofstablelearningratescanpreventfailuresincostlytraining
runs. We show that the largest stable learning rate is structurally similar to that of the
unclipped SGD case, but with the introduction of the reduction factors µ and ν which
c c
account for the effects of clipping.
From Equation (12), we observe that for either the risk R or the distance to optimality
D, the instantaneous time derivative is quadratic in the learning rate η(t). This implies
that we can compute a stability threshold for the learning rate, determining whether, in
high-dimensions, these measures of suboptimality increase or decrease. We find the critical
5
ksiR
ksiRvalues η∗(t) and η∗(t) such that E[dR(Θ )]=0 and E[dD(Θ )]=0 . In particular,
R D t t
d∥∇P(Θ )∥2 µ (Θ ) R(Θ )µ (Θ )
η∗(t)= t c(t) t and η∗(t)= t c(t) t . (15)
R Tr(K2)P(Θ )ν (Θ ) D P(Θ ) ν (Θ )
t c(t) t t c(t) t
This implies that clipping increases instantaneous stability (for both R and D) relative to
unclipped SGD when
µ (Θ )
c(t) t
>1. (CSC)
ν (Θ )
c(t) t
We refer to this as the clipped-stability-criterion (CSC). This can be interpreted as as a
relative signal-to-noise-ratio; the fraction of clipped gradients µ reduces the signal, while
clipping reduces the noise through the reduction factor ν. Stability is increased when the
relativesignal-to-noise-ratioisgreaterthan1. Clippingsignificantlyenhancesstabilitywhen
a small fraction of samples contribute disproportionately to the gradient norm.
Clipping will increase the stability of SGD with a small enough choice of c:
Theorem 2. For data x ∼ N(0,K) one may always choose the clipping schedule c small
enough to satisfy the (CSC).
The proof of this theorem follows from an application of L’Hˆopital’s rule and is available in
AppendixE.Weprovideplotsofthe(CSC)inFigure2undervarioussettings. Weconjecture
thatthisresultextendsbeyondGaussiandata,butthecurrentintractabilityofµforgeneral
data makes precise claims difficult.
Counterintuitively,clippingcanalsodecrease stabilityinsomecases,whenthebiastowards
∇ R (µ) is reduced more than the overall gradient norms (ν). This shows that some care
θ
must be taken to avoid clipping being detrimental. The proof of the following theorem
straightforwardly uses the definitions of µ and ν and can be found in Appendix E.
c c
Theorem 3. Consider x∼N(0,K) and noise with the distribution given by
P(ϵ=−λ)=p/2, P(ϵ=0)=1−p, P(ϵ=λ)=p/2, (16)
for p∈(0,1) and λ>0. Then, there is a constant r depending on p,λ so that when R ≤r
t
there always exists c(t) such that the (CSC) is less than 1. Therefore, clipped SGD can be
less stable than unclipped SGD.
5 When does clipped SGD outperform unclipped SGD?
Wenowask: underwhatsettingscanclippingimprovetheperformanceofSGD?Specifically,
with the optimal learning rate schedule for unclipped SGD, does there exist a clipping-
learning rate combination such that clipping achieves a lower loss at time T?
We will use Equation (12) to answer this question. We first present detailed calculations
in the isotropic case to find an exact condition on the gradient distribution where clipping
improves training. We then show that this condition still applies under anisotropic data.
Weprovideexamplesandplotsofthisconditiontodevelopintuitiononwhenclippinghelps
to improve training.
5.1 Isotropic data
Consider the case of isotropic data where x ∼ N(0,I). Define R∞ to be the deterministic
t
equivalentofR(φ ),whereφ isC-HSGDwithc(t)≡∞(whichistosayunclippedHSGD).
t t
Example 1 shows that R and R∞ solve the following ODEs,
t t
dR η2(t) dR∞ η2(t)
t =−2η(t)µ R + ν (2R +σ2), t =−2η(t)R∞+ (2R∞+σ2).
dt c(t) t 2 c(t) t dt t 2 t
(17)
These results enable a comparison between clipped and unclipped SGD. Since these ODEs
are quadratic in η(t), it is straightforward to greedily maximize their instantaneous rate of
6descent,resultinginthegloballyoptimallearningrateschedule. OptimizingeachODEover
η(t) yields
dR R2 µ2 dR∞ (R∞)2
t =− t c(t) t =− t . (18)
dt R +σ2/2ν dt R∞+σ2/2
t c(t) t
When R = R∞, we see that the rate of descent is faster and thus clipping improves SGD
t t
exactly when there exists a c(t) such that
µ2 (R )
c(t) t
>1. (CCC)
ν (R )
c(t) t
Wecallthisinequalitytheclipping-comparison-criterion(CCC). Therefore,inoursettingwe
can exactly understand when clipping is helpful to training. Informally, the improvement
criterion tells us that clipping is effective when it can reduce the variance of the gradient
norms, via ν more than it reduces the squared reduction to the descent term µ2 .
c(t) c(t)
This is consistent with previous observations that, in practice, clipping is effective when
the distribution of the gradient norms is heavy-tailed [8], but gives a quantitative rule for
comparison. To give some intuition, we provide some plots of these thresholds over various
types of noise distributions in Figure 2.
102 Gau 2.5 Gau
Rad Rad
Uni 2.0 Uni
Exp Exp
101 1.5
1.0
100
0.5
0 5 10 15 20 25 30 0.0 2.5 5.0 7.5 10.0 12.5 15.0
c c
(a) The (CSC) (b) The (CCC)
Figure 2: The (CSC) and (CCC) across various noise distributions: Gaussian (Gau),
Rademacher-like (Rad), uniform on [−M,M] (Uni), and symmetrized exponential (Exp)
noise. The (CSC) is computed with R = 3, σ = 9, p = 0.7; the (CCC) figure uses
R = 3, σ = 5, p = 0.2 (where p is a parameter for Rademacher-like noise). Parame-
ters are chosen to illustrate different behaviours.
5.2 Anisotropic data
The previous results show that with isotropic data, the optimal clipping schedule can be
foundbymaximizingthe(CCC)ateachtimepoint. Inspiredbythisobservation,wedescribe
a procedure which, given a learning rate schedule for unclipped SGD, gives us a learning
rate-clipping schedule pair which performs at least as well as unclipped SGD—and has a
simple condition for showing better performance.
Consider a learning rate schedule η(t), used to train unclipped SGD. We define the max-
(CCC) clipping threshold schedule as follows: At step t, we first set the clipping threshold
to
µ2(R )
c∗(t)=argmax c t . (19)
c ν (R )
c t
Given a clipping threshold c, we define a compensated learning rate for clipped SGD by
η˜(t,c)=η(t)/µ (R ). (20)
c t
7
CSC CCCEffectively, this learning rate compensates for the fact that clipping biases SGD against the
gradient such that clipped SGD now has the same instantaneous descent term as unclipped
SGD. We now choose η∗(t)=η˜(t,c∗(t)) as our learning rate.
The basic idea is that this schedule will never underperform unclipped SGD. If the (CCC) is
never satisfied we have c∗(t)≡∞ and η∗(t)=η(t), recovering the original, unclipped SGD.
However, if the (CCC) is satisfied at any time the max-(CCC) schedule will take advantage of
thisandprovideimprovementstooptimization. Inordertoshowthis, wefirsthavetosolve
for R under anisotropic data. In this setting, the risk is the sum of two parts: a gradient
t
flow term and an integrated correction term. The gradient flow term is associated with
the infinitesimal learning rate limit of SGD. It decreases the risk and comes from solving
the underlying problem. The correction term arises because the actual learning rate is not
infinitesimal. It encodes the errors made by SGD and increases the risk. Gradient flow is
defined to be,
dΦgf =−∇R(Φgf)dt, (21)
t t
with Φgf = θ . Then the gradient flow term is R(Φgf). In Appendix F, we show R with
0 0 t t
any learning rate η(t) and clipping schedule c(t) solves
1(cid:90) t
R
t
=R(Φg Γf
c
T)+
d
0
η2(s)ν c(s)Tr(K2e−2K(Γc t−Γc s))(R s+σ2/2)ds, (22)
where Γc = (cid:82)t η(s)µ ds is the clipped integrated learning rate. The integral term in
t 0 c(s)
Equation (22) is the finite learning rate correction. The risk of unclipped SGD can be
computed using c(t)≡∞:
1(cid:90) t
R∞ =R(Φgf )+ η2(s)Tr(K2e−2K(Γt−Γs))(R∞+σ2/2)ds. (23)
t ΓT d s
0
(cid:82)t
where Γ = η(s)ds. This gives us the following theorem:
t 0
Theorem 4. Given SGD with learning rate schedule η(t) and clipped SGD with learning
and clipping schedules η∗(t) and c∗(t), then R ≤ R∞. If there exists a t ∈ [0,T] such
T T
that the (CCC) holds then R < R∞. Conversely, if µ2(R)/ν (R) ≤ 1 for all R > 0 and
T T c c
c>0, then for any learning and clipping schedules η(t) and c(t), SGD with the compensated
learning rate schedule η(t)µ has R∞ ≤R .
c(t) T T
Proof. With these choices, the clipped risk (22) solves
1(cid:90) T ν
R =R(Φgf )+ η2(s) c(s) Tr(K2e−2K(ΓT−Γs))(R +σ2/2)ds. (24)
T ΓT d µ2 s
0 c(s)
Note that the gradient flow term is identical to that of unclipped SGD. Since the (CCC) is
satisfied,theintegratedcorrectiontermisnolargerthanunclippedSGDandthusR ≤R∞.
T T
If the (CCC) occurs at some t, then in fact R <R∞. For the converse, one substitutes the
T T
learning rate schedule into (23) and sees it is smaller than (22).
Wenotethatthisresultholdsforanychoiceoftheunclippedlearningrate,eventheoptimal
one. Therefore,ifthe(CCC)holdsatsomepointalongtheoptimalunclippedSGDtrajectory
then the benefits of gradient clipping cannot be matched by unclipped SGD.
The following theorems give concrete examples of our results and apply in the both the
isotropicandanisotropicsetting. Weshowthatthe(CCC)cannotbesatisfiedwhenthedata
areGaussian. Then,weshowthat,asbefore,broadlydistributedgradientscanbenefitfrom
clipping (even with all finite moments). Both proofs straightforwardly apply the definitions
of µ and ν (Appendix E).
c c
Theorem 5. If x∼N(0,K) and ϵ∼N(0,σ2) then µ2(R)/ν (R)≤1 for all R,c>0.
c c
Hence,inthiscaseclippedSGDneverimprovesoverunclippedSGD(inthesenseofTheorem
4).
8Gau
Rad
Uni
Exp
101
100
0 2 4 6 8 10
Risk
Figure 3: The maximum over c of the (CCC) for various values of the risk. Notice that the
maximum value of the (CCC) for both uniform and Gaussian noise is 1, corresponding to
unclipped SGD. Plots are computed with σ = 7, p = 0.5 where p is a parameter in the
Rademacher-like noise.
Theorem6. Considerx∼N(0,K)andtheRademacher-likenoiseasdescribedinTheorem
3. Then, there is an r > 0 depending on p and λ so that when R ≤ r there always exists
t
c(t) such that the (CCC) is satisfied.
It is interesting to note that the (CSC) is automatically satisfied if the (CCC) is, implying
thatwhengradientclippingimprovesSGD’sperformance,italsoenhancesitsstability. This
dual benefit suggests that in some settings clipping can be used to achieve both efficient
and stable training.
We illustrate these theorems with numerical examples. Comparing constant learning rate
SGD to clipped SGD with the max-(CCC) schedule and compensated learning rate, we see
no improvement for Gaussian noise (Figure 4a). In contrast, with Rademacher-like noise,
clipping with compensated learning rate learns faster and reaches a lower value of the risk
(Figure 4b). In practice, computing optimal schedules (for learning rate alone or jointly
with clipping schedules) remains challenging and is left for future work.
C-SGD 80.0% CI C-SGD 80.0% CI
0.20 0.20
-SGD 80.0% CI -SGD 80.0% CI
∞ ∞
0.18 ODE ODE
Unclipped ODE 0.15 Unclipped ODE
0.16
0.14
0.10
0.12
0.10 0.05
0.08
0.00
0.0 2.5 5.0 7.5 10.0 12.5 15.0 0.0 2.5 5.0 7.5 10.0 12.5 15.0
Iterations Iterations
(a) Gaussian noise (b) Rademacher-like noise
Figure 4: Results of clipped versus unclipped SGD with anisotropic data under the setting
of Theorem 4. The data follow a power law with d = 179.74 and d = 500. The unclipped
learning rate is constantly η = 0.4 while σ = 0.8. We compare Gaussian and Rademacher-
like noise with p = 0.2. Notice that clipping cannot improve SGD in the setting with
Gaussian noise while it noticeably improves performance with Rademacher-like noise. SGD
is presented with 80% confidence intervals over 100 runs.
9
ksiR
CCC
xaM
ksiR6 Conclusion
Our analysis of high-dimensional streaming settings shows that the effectiveness of clipping
hinges on two key quantities: descent and variance reduction factors µ and ν . The struc-
c c
ture of the noise, model, and data then determine the dynamics of µ and ν for a given
c c
clippingthreshold. ThisallowsustocompareclippedSGDtounclippedSGDwithlearning
rateandclippingschedules. Clippingcanbebeneficialinthesettingofnon-Gaussiannoise;
in certain noisy regimes, clipping helps filter noisy datapoints more than non-noisy ones.
The key is that the gradient norm becomes a strong-enough proxy for the “quality” of a
datapoint, and can be used to effectively filter each point.
The local stability of clipped SGD depends on the ratio of µ to ν , the (CSC). The maxi-
c c
mumstablelearningratecanbeincreasedbyclippingifclippingreducestheaveragesquare
gradient norm more than the probability of clipping. This can be achieved for broad dis-
tributions of gradients. Similarly, clipping improves optimization if the ratio of µ2 to ν
c c
exceeds 1, the (CCC). This quantity informs when the tradeoff between biasing training
against the gradient and reducing the variance pays off.
One future direction is to consider more complex models and losses. Exact risk curves
have been derived in the unclipped SGD setting on more general losses [19]; some of these
results are likely adaptable to the clipped SGD setting. Additionally, important quantities
from the analysis of high-dimensional linear models can be measured in real networks (via
linearization) and can be used to analyze learning dynamics [23]. We believe that the
generalized versions of µ and ν may be interesting to study in real networks.
c c
More generally, our work suggests that operations which filter gradients at the level of
individual examples can be beneficial to training. For example, our analysis hints at a
possibly more effective strategy of processing large gradients; simply ignore them. This
stems from the observation that µ would be unchanged by this alternative method, while
c
ν would be smaller. Currently, filtering largely happens either during data pre-processing
c
or at the batch level during training due to the limitations of autodifferentiation setups.
A promising avenue for future research is to find efficient ways of applying operations like
clipping element-wise rather than batch-wise.
Acknowledgements
We thank Lechao Xiao for his insightful feedback on the clarity and accuracy of our pre-
sentation, which significantly improved the manuscript. We also extend our gratitude to
CourtneyPaquetteforhermeticulousproofreadingandassistanceinshapingthefinaldraft.
10References
[1] Tom´avsMikolov.“StatisticalLanguageModelsbasedonNeuralNetworks”.PhDthe-
sis. Brno University of Technology, 2012.
[2] Razvan Pascanu, Tom´avs Mikolov, and Yoshua Bengio. “On the difficulty of training
recurrent neural networks”. In: Proceedings of the 30th International Conference on
Machine Learning. 2013.
[3] Tom Brown et al. “Language Models are Few-Shot Learners”. In: Advances in Neural
InformationProcessingSystems.Ed.byH.Larochelleetal.Vol.33.CurranAssociates,
Inc., 2020, pp. 1877–1901. url: https://proceedings.neurips.cc/paper_files/
paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.
[4] Martin Abadi et al. “Deep Learning with Differential Privacy”. In: Proceedings of the
2016 ACM SIGSAC Conference on Computer and Communications Security. ACM,
2016.
[5] Venkatadheeraj Pichapati et al. AdaCliP: Adaptive Clipping for Private SGD. 2019.
arXiv: 1908.07643 [cs.LG].
[6] Ilya O Tolstikhin et al. “MLP-Mixer: An all-MLP Architecture for Vision”. In: Ad-
vances in Neural Information Processing Systems. Ed. by M. Ranzato et al. Vol. 34.
2021, pp. 24261–24272.
[7] Alexey Dosovitskiy et al. “An Image is Worth 16x16 Words: Transformers for Im-
age Recognition at Scale”. In: International Conference on Learning Representations.
2021.
[8] Jingzhao Zhang et al. “Why are Adaptive Methods Good for Attention Models?”
In: Advances in Neural Information Processing Systems. Ed. by H. Larochelle et al.
Vol. 33. Curran Associates, Inc., 2020, pp. 15383–15393.
[9] Jared Kaplan et al. Scaling Laws for Neural Language Models. 2020. arXiv: 2001.
08361 [cs.LG].
[10] Kaiming He et al. “Deep Residual Learning for Image Recognition”. In: 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (2015), pp. 770–
778. url: https://api.semanticscholar.org/CorpusID:206594692.
[11] JiaDengetal.“ImageNet:Alarge-scalehierarchicalimagedatabase”.In:2009 IEEE
Conference on Computer Vision and Pattern Recognition. 2009, pp. 248–255. doi:
10.1109/CVPR.2009.5206848.
[12] Jacob Devlin et al. “BERT: Pre-training of Deep Bidirectional Transformers for Lan-
guage Understanding”. In: North American Chapter of the Association for Compu-
tational Linguistics. 2019. url: https://api.semanticscholar.org/CorpusID:
52967399.
[13] Anastasia Koloskova, Hadrien Hendrikx, and Sebastian U Stich. “Revisiting Gradient
Clipping: Stochastic bias and tight convergence guarantees”. In: Proceedings of the
40th International Conference on Machine Learning. 2023.
[14] Bohang Zhang et al. “Improved Analysis of Clipping Algorithms for Non-convex Op-
timization”. In: Advances in Neural Information Processing Systems. 2020.
[15] Xiangyi Chen, Steven Z. Wu, and Mingyi Hong. “Understanding Gradient Clipping
in Private SGD: A Geometric Perspective”. In: Advances in Neural Information Pro-
cessing Systems. 2020.
[16] QianxiaoLi,ChengTai,andWeinanE.“StochasticModifiedEquationsandAdaptive
StochasticGradientAlgorithms”.In:Proceedingsofthe34thInternationalConference
on Machine Learning. Ed. by Doina Precup and Yee Whye Teh. Vol. 70. Proceedings
of Machine Learning Research. PMLR, 2017, pp. 2101–2110.
[17] Stephan Mandt, Matthew D. Hoffman, and David M. Blei. A Variational Analysis of
Stochastic Gradient Algorithms. 2016. arXiv: 1602.02666 [stat.ML].
[18] David Barrett and Benoit Dherin. “Implicit Gradient Regularization”. In: Interna-
tional Conference on Learning Representations. 2021.
11[19] Elizabeth Collins-Woodfin et al. “Hitting the High-Dimensional Notes: An ODE
for SGD learning dynamics on GLMs and multi-index models”. In: arXiv e-prints,
arXiv:2308.08977 (Aug. 2023), arXiv:2308.08977. doi: 10.48550/arXiv.2308.08977.
arXiv: 2308.08977 [math.OC].
[20] CourtneyPaquetteetal.HomogenizationofSGDinhigh-dimensions:Exactdynamics
and generalization properties. 2022. arXiv: 2205.07069 [math.ST].
[21] Atish Agarwala, Fabian Pedregosa, and Jeffrey Pennington. “Second-order regression
models exhibit progressive sharpening to the edge of stability”. In: Proceedings of the
40th International Conference on Machine Learning. Ed. by Andreas Krause et al.
Vol.202.ProceedingsofMachineLearningResearch.PMLR,2023,pp.169–195.url:
https://proceedings.mlr.press/v202/agarwala23b.html.
[22] Jeremy Cohen et al. “Gradient Descent on Neural Networks Typically Occurs at the
Edge of Stability”. In: International Conference on Learning Representations. 2021.
url: https://openreview.net/forum?id=jh-rTtvkGeM.
[23] Atish Agarwala and Jeffrey Pennington. High Dimensional Analysis Reveals Conser-
vative Sharpening and a Stochastic Edge of Stability. Apr. 2024. doi: 10.48550/
arXiv.2404.19261. arXiv: 2404.19261 [physics, stat]. (Visited on 05/09/2024).
[24] Roman Vershynin. High-Dimensional Probability: An Introduction with Applications
in DataScience.CambridgeSeriesinStatisticalandProbabilisticMathematics.Cam-
bridge University Press, 2018.
[25] Elizabeth Collins-Woodfin and Elliot Paquette. High-dimensional limit of one-pass
SGD on least squares. 2023. arXiv: 2304.06847 [math.PR].
[26] Kaiming He et al. “Deep Residual Learning for Image Recognition”. In: Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition.2016,pp.770–
778. (Visited on 01/28/2020).
[27] Alexey Dosovitskiy et al. “An image is worth 16x16 words: Transformers for image
recognition at scale”. In: arXiv preprint arXiv:2010.11929 (2020).
12A Full formulation of Theorem 1 with non-Gaussian data
To state the more general version of Theorem 1, we require some additional technical as-
sumptions. AlongwithalloftheassumptionsdescribedinSection2wewillfurtherassume:
Assumption4. Forsomeconstantv≥1andanyfixedθ with∥θ∥≤1,wehave∥xTθ∥ ≤
ψ2
v and the data satisfy a Hanson-Wright inequality: for all t≥0 and any fixed matrix B,
(cid:32) (cid:40) (cid:41)(cid:33)
t2 t
P(|xTBx−E[xTBx]|≥t)≤2exp −min √ √ , √ √ , (25)
v4∥ KB K∥2 v2∥ KB K∥
F
where K is the covariance of the data.
Assumption 5. µ and ν satisfy the following Lipschitz-like bounds for some constants C
µ
and C .
ν
|R(x)−R(y)|
|µ(x)−µ(y)|≤C (26)
µmin R(z)
z∈{x,y}
(cid:32) (cid:33)
σ
|ν(x)P(x)−ν(y)P(y)|≤C 1+ |R(x)−R(y)| (27)
ν (cid:112)
R(x)+R(y)
where R is the risk.
We may now state our more general version of Theorem 1. Here we use c = max c(t) and
t
η =max η(t).
t
Theorem 7. There is a constant C =C(v,(n/d),c,η,(1+∥V ∥2)) and a constant c=c(v)
0
so that for any 1≤u≤cd
0≤su k≤p n(cid:13) (cid:13) (cid:13) (cid:13)(cid:20) R D(( θθ kk ))(cid:21) −(cid:20) R D(( ΘΘ kk // dd ))(cid:21)(cid:13) (cid:13) (cid:13) (cid:13)≤Cexp(cid:32) (cid:90) 0n/d (cid:112) RC (ν Θη s2 )(s +)σ Rd (s
θ
sd)(cid:33) ulog(d)d−1/2, (28)
withprobabilityatleast1−e−u andprovidedtherighthandsideislessthan1. Thecoefficient
C can be bounded by
C ≤C(cid:112) n/dηv2((1+∥V ∥2)v2+c2(cid:112) n/d)exp(cid:0) C×(1+C )max{η,η2}(n/d)(cid:1)
0 µ
for an absolute constant C >0.
We note that when σ =0 (so there is no noise) we arrive at the simpler conclusion that
(cid:13)(cid:20) (cid:21) (cid:20) (cid:21)(cid:13)
sup (cid:13) (cid:13) R(θ k) − R(Θ k/d) (cid:13) (cid:13)=O(d−1/2).
0≤k≤n(cid:13) D(θ k) D(Θ k/d) (cid:13)
We note also that if η(s) ≡ η, the risk will be bounded below by a constant that depends
onlyonη,c,σ withhighprobability(andprovidedthereisnowarmstart), andhenceagain
this coefficient can be bounded with high probability in a similar way to C. Moreover, for
any desired d-independent risk threshold R , if one makes d sufficiently large, then with
0
very high probability, two risk curves will agree up to the point they cross below this risk
threshold.
13B Proof of main theorems
In this section we prove both Theorem 1 and Theorem 7.
B.1 Proof of Theorem 1
In order to prove the version of our Theorem with Gaussian data, it suffices to check that
Gaussians satisfy both Assumption 4 and 5.
It is a standard fact that the Hanson-Wright inequality is satisfied for Gaussians [24].
Let z be standard Gaussian then recall from (10),
µ (θ)=P(|⟨x,θ−θ∗⟩|≤c) (29)
c
(cid:112)
=P(| 2R(θ)z−ϵ|≤c). (30)
With a slight abuse of notation, we condition on ϵ and use I =(ϵ−c,ϵ+c) to express
|µ c(θ 1)−µ c(θ 2)|=(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)P(cid:32) z ∈ (cid:112) 2RI
(θ
1)(cid:12) (cid:12) (cid:12) (cid:12)ϵ(cid:33) −P(cid:32) z ∈ (cid:112) 2RI
(θ
2)(cid:12) (cid:12) (cid:12) (cid:12)ϵ(cid:33)(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(31)
≤(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)P(cid:32) z ≤ (cid:112)c R+ (θϵ 1)(cid:12) (cid:12) (cid:12) (cid:12)ϵ(cid:33) −P(cid:32) z ≤ (cid:112)c R+ (θϵ 2)(cid:12) (cid:12) (cid:12) (cid:12)ϵ(cid:33)(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(32)
+(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)P(cid:32) z ≤ (cid:112)c R− (θϵ 1)(cid:12) (cid:12) (cid:12) (cid:12)ϵ(cid:33) −P(cid:32) z ≤ (cid:112)c R− (θϵ 2)(cid:12) (cid:12) (cid:12) (cid:12)ϵ(cid:33)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12). (33)
Withoutlossofgenerality,weassumec+ϵ/R(θ )≤c+ϵ/R(θ ),thentheformertermmay
2 1
be bounded by
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)P(cid:32) z ≤ (cid:112)c R+ (θϵ 1)(cid:12) (cid:12) (cid:12) (cid:12)ϵ(cid:33) −P(cid:32) z ≤ (cid:112)c R+ (θϵ 2)(cid:12) (cid:12) (cid:12) (cid:12)ϵ(cid:33)(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(34)
(cid:12)(cid:112) (cid:112) (cid:12)
≤ |c 2√+ πϵ| e− 4( Rc+ (ϵ θ) 12 ) (cid:12) (cid:12)
(cid:12)
(cid:12)
R (cid:112)( Rθ 1 () θ− 1)R(R
θ
2( )θ 2)(cid:12) (cid:12)
(cid:12)
(cid:12). (35)
Maximizing te−t2/4R(θ1) in t yields
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)P(cid:32) z ≤ (cid:112)c R+ (θϵ 1)(cid:12) (cid:12) (cid:12) (cid:12)ϵ(cid:33) −P(cid:32) z ≤ (cid:112)c R+ (θϵ 2)(cid:12) (cid:12) (cid:12) (cid:12)ϵ(cid:33)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)≤ (cid:112) 2πR(θ|R 2)( (θ (cid:112)1) R− (θR 1)(θ +2) (cid:112)|
R(θ 2))
(36)
|R(θ )−R(θ )|
≤C 1 2 . (37)
µmin R(z)
z∈{θ1,θ2}
By applying the same argument to the latter term of (33), we see that
|R(θ )−R(θ )|
|µ(θ )−µ(θ )|≤C 1 2 , (38)
1 2 µmin R(z)
z∈{θ1,θ2}
as desired.
14(cid:20) (cid:12) (cid:21)
To show (27), let us first first define f(ξ,ϵ) = E clip2(ξz−ϵ)(cid:12) (cid:12)ϵ ]. Upon conditioning on
c (cid:12)
(cid:16)(cid:112) (cid:17)
ϵ, it follows that ν(θ)P(θ)=f 2R(x),ϵ . Differentiating with respect to ξ, we see that
∂ f(ξ)=E(cid:2) 2zclip (ξz−ϵ)1 (cid:3) (39)
∂ξ c |ξz−ϵ|≤c
2 (cid:90) (c+ϵ)/ξ
= √ (ξz−ϵ)ze−z2/2dz (40)
2π
(−c+ϵ)/ξ
= √−2 (cid:34) (ξz−ϵ)e−z2/2(cid:12) (cid:12) (cid:12)z=(c+ϵ)/ξ (cid:35) +2ξP(|ξz−ϵ|≤c|ϵ) (41)
2π (cid:12)
z=(−c+ϵ)/ξ
−2 (cid:104) (cid:105)
= √ (c−ϵ+ϵ)e−(c−ϵ)2/2ξ2 +(c+ϵ−ϵ)e−(c+ϵ)2/2ξ2 (42)
2π
+2ξP(|ξz−ϵ|≤c|ϵ). (43)
(cid:18) (cid:19)
Uponnotingthatξ c√+ϵ
e−(c 2+ ξϵ 2)2
≤ξC,forsomeabsoluteconstantC >0wemaybound
ξ 2π
the absolute value of ∂ f(ξ,ϵ) by
∂ξ
(cid:12) (cid:12)
(cid:12) ∂ (cid:12)
(cid:12) f(ξ,ϵ)(cid:12)≤4Cξ+2|ϵ|. (44)
(cid:12)∂ξ (cid:12)
(cid:112) (cid:112)
Hence, withoutlossofgenerality, ifweassumethat 2R(θ )≤ 2R(θ )andconditioning
1 2
on ϵ, we obtain
√
(cid:90) 2R(x2)(cid:12) (cid:12) ∂ (cid:12) (cid:12)
|ν(θ 1)P(θ 1)−ν(θ 2)P(θ 2)|≤ √ (cid:12) (cid:12)∂ξf(ξ,ϵ)(cid:12)
(cid:12)
dξ (45)
2R(θ1)
(cid:12)(cid:112) (cid:112) (cid:12)
≤2c|R(θ )−R(θ )|+2E|ϵ|(cid:12) 2R(θ )− 2R(θ )(cid:12), (46)
1 2 (cid:12) 1 2 (cid:12)
which completes the proof of (27).
B.2 Proof of Theorem 7
We now prove the general version of our main result.
Wesimplifynotationbystudyingtheiterationsv =θ −θ∗. Weshallalsowriteη˜ =η(k/d)
k k k
sothatη˜ /d=η . Beforeprovingtheorem7wefirstshowaseriesoflemmasfollowingclosely
k k
the proof techniques of [25].
Notation 1. It is helpful to formulate some results in terms of tensor products. We use
x⊗y to refer the tensor product of x and y.
Notation 2. We use C to refer to a generic constant which may change from line to line.
With a slight abuse of notation, extend {v } to be indexed by continuous time t ∈ R+
k k≥0
by v = v . Let q be a quadratic. Via its Taylor expansion, we may write the updates of
t ⌊t⌋
q by
q(v )−q(v )=−η˜ k∇q(v )T clip √ (ℓ x )+ η˜ k2 (cid:10) ∇2q,(clip √ (ℓ x ))⊗2(cid:11) . (47)
k+1 k d k c d k k+1 2d2 c d k k+1
This update can be decomposed into errors, martingale parts and predictable parts
η˜ η˜2
q(v )−q(v )=− kµ(v )∇q(v )TKv + kν(v )P(v )Tr(∇2qK)
k+1 k d k k k d2 k k (48)
+∆Mlin+∆Mquad+∆E .
k k k
15Wherewehavemartingaleanderrorincrementsbeingcontributedfromboththelinearand
quadraticterms. ThespecificformofthesetermsmaybeseeninsectionB.3. Wewillrelate
these quadratics to a manifold of functions which will close under the gradient and Hessian
operations above. Choose this family of functions to be
Q={v (cid:55)→vTR(z;K)v, ∀z ∈Ω} (49)
where Ω is a circle of radius 2 and thus enclosing the eigenvalues of K. We further define
the stopping time τ for a parameter M.
τ =inf{k :∥v ∥≥M}∪{td:∥V ∥≥M}, (50)
k t
and the stopped processes,
vτ =v Vτ =V . (51)
k k∧τ t t∧(τ/d)
WewillfirstproveTheorem7forthestoppedprocess{vτ} and{Vτ} andthenbound
k k≥0 t t≥0
the probability that τ ≤n.
Lemma 1. There is an absolute constant C >0 so that
(cid:16) (cid:17)
sup |q(vτ )−q(Vτ)|≤ sup |Mτ,lin|+|Mτ,quad|+|Eτ |+|Mτ,SDE| (52)
td t td td td t
0≤t≤n/d 0≤t≤n/d
(cid:90) n/d(cid:18) C η2(s)(m +σ) (cid:19)
+ Cmax{η,η2}+ ν s +2C η sup|q(vτ )−q(Vτ)|ds, (53)
m µ sd s
0 s q∈Q
(cid:112)
where m is sum of risks m = R(θτ )+R(Θτ ).
s s sd sd
Proof. When context is clear, we will write R(z;K) = R(z). We begin by noting that for
all q ∈Q since for eigenvalues and eigenvectors (λ ,ω ) of K,
i i
(cid:12) (cid:12)
(cid:12) (cid:12)∇q(v)TKv(cid:12) (cid:12)=(cid:12) (cid:12) (cid:12)(cid:88) (λλ −i z)⟨v,ω i⟩2(cid:12) (cid:12) (cid:12)≤(cid:88) λ i⟨v,ω i⟩2 =⟨K,v⊗2⟩.
(cid:12) i (cid:12)
i i
The same bound holds for the gradient term, and we conclude that for all q ∈Q
|∇q(v)TKv|≤2R(v+θ∗).
Given a g ∈Q, by (48), we obtain
(cid:90) t η(s) (cid:90) t η2(s)
g(vτ)=g(vτ)− µ(vτ)∇g(vτ)TKvτds+ ν(vτ)P(vτ)Tr(K∇2g)ds (54)
t 0 d s s s d2 s s
0 0
+Mτ,lin+Mτ,quad+Eτ.
t t t
Similarly, by Itˆo’s lemma
(cid:90) t
g(Vτ)=g(Vτ)− η(s)µ(Vτ)∇g(Vτ)TKVτds
t 0 s s s
0 (55)
(cid:90) t η2(s)
+ ν(Vτ)P(Vτ)Tr(K∇2g)ds+Mτ,SDE,
d2 s s t
0
where
Mτ,SDE =(cid:90) t η √(s) ∇g(Vτ)T(cid:112) 2Kν(Vτ)P(Vτ)dB . (56)
t s s s s
d
0
First, we will show that for any g ∈Q and any x ,x ∈Rd
1 2
16|∇g(x )TKx −∇g(x )TKx |≤4sup|g(x )−g(x )|. (57)
1 1 2 2 1 2
g∈Q
Thestatementisobviousifg(x)=q(x). Ifg(x)=∇q(x)TR(z)xthen∇g(x)=∇2qR(z)x+
R(z)∇q(x) and using Cauchy’s integral formula we can see,
∇g(x)TKx=xTR(z)∇2qKx+∇q(x)TR(z)Kx (58)
1 (cid:73) 1 (cid:73)
=− yxTR(z)∇2qR(y)xdy− ∇q(x)TR(z)xdz+z∇q(x)TR(z)x.
2πi 2πi
Ω Ω (cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) T3
T1 T2
(59)
For any z on Ω we have that ∥R(z)∥ ≤ 1. Furthermore, the arc-length of Ω is 8π.
op
Therefore, we have
|T 1(x 1)−T 1(x 2)|≤ 21
π
(cid:73) |y|(cid:12) (cid:12)xT 1R(z)∇2qR(y)x 1−xT 2R(z)∇2qR(y)x 2(cid:12) (cid:12)dy (60)
Ω
≤8sup|g(x )−g(x )|, (61)
1 2
g∈Q
and
1 (cid:73)
|T (x )−T (x )|≤ |∇q(x )TR(z)x −∇q(x )TR(z)x |dz (62)
2 1 2 2 2π 1 1 2 2
Ω
≤4sup|g(x )−g(x )|. (63)
1 2
g∈Q
If g(x)=xTR(z)∇2qR(y)x, then using the identity R(z)K=I+zR(z),
∇g(x)TKx=xTR(y)∇2qx+zxTR(y)∇2qR(z)x+xTR(z)∇2qx+yxTR(z)∇2qR(y)x.
(64)
By the same methods as above, we see
|∇g(x )TKx −∇g(x )TKx |≤24sup|g(x )−g(x )|. (65)
1 1 2 2 1 2
g∈Q
It is simple to account for the presence of the functions µ and ν. Using Assumptions 5
C (m +σ)
|ν(vτ )P(vτ )−ν(Vτ)P(Vτ)|≤ sup|g(vτ )−g(Vτ)| ν t . (66)
td td t td td t m
g∈Q t
As for µ, adding and subtracting µ(vτ )g(Vτ), using µ≤1 and g(Vτ)≤2R(Θτ )
td td td td
(cid:18) C 2R(Θτ ) (cid:19)
|µ(vτ )g(vτ )−µ(Vτ)g(Vτ)|≤ sup|g(vτ )−g(Vτ)| 1+ µ td . (67)
td td t t td t min{R(θτ ),R(Θτ )}
g∈Q td td
Note we could have also added and subtracted µ(Vτ)g(vτ ), and so picking whichever is
td td
better, we arrive at
|µ(vτ )g(vτ )−µ(Vτ)g(Vτ)|≤ sup|g(vτ )−g(Vτ)|(1+2C ).
td td t t td t µ
g∈Q
This completes the claim.
17Lemma 2. There is an absolute constant C >0 so that for any quadratic q with ∥q∥ ≤1
C2
any n≤dT with T ≥1, any 1≤u≤d,
(cid:12) (cid:12) √
(cid:12) (cid:12) sup Mτ,lin(cid:12) (cid:12)≤C Tη(2+M)2v2d−1/2u, (68)
(cid:12) k (cid:12)
0≤k≤n
(cid:12) (cid:12) √
(cid:12) (cid:12) sup Mτ,quad(cid:12) (cid:12)≤C Tηc2v2d−1/2u, (69)
(cid:12) t (cid:12)
0≤k≤n
(cid:12) (cid:12)
(cid:12) (cid:12) sup Eτ(cid:12) (cid:12)≤CTη(2+M)2v4d−1/2, (70)
(cid:12) t(cid:12)
0≤k≤n
(cid:12) (cid:12)
(cid:12) (cid:12) √
(cid:12) sup Mτ,SDE(cid:12)≤C Tη(2+M)2v2d−1/2u. (71)
(cid:12) t (cid:12)
(cid:12)0≤t≤n/d (cid:12)
with probability at least 1−e−u.
The proof of lemma 2 is deferred to appendix B.3.
Lemma3. ThereisanabsoluteconstantC >0sothatforanym>0, thereexistsaQ¯ ⊆Q
with|Q¯|≤Cd2m suchthatforallq ∈Q,thereissomeq¯∈Q¯ thatsatisfies∥q¯−q∥ ≤d−2m.
C2
Proof. With assumption 2, the arc length of Ω is fixed independent of d. Thus, we may
construct Q¯ by restricting Q to a minimal d−2m-net of Ω.
The proof of Theorem 7 now follows easily from these results. By Lemmas 1 and 2, there is
an absolute constant C so that for any u≥1
√ (cid:16) √ (cid:17) (cid:90) t
|q¯(vτ )−q¯(Vτ)|≤C Tηv2d−1/2 (2+M)2v2u+c2 T + L max|q(vτ )−q(Vτ)|ds,
td t s sd s
0
q∈Q¯
(72)
on an event of probability at least 1−e−u, and where we have set
(cid:18) C η2(s)(m +σ) (cid:19)
L := Cmax{η,η2}+ ν s +2C η .
s m µ
s
Then, from Lemma 3 with m = 1 and increasing the absolute constant C > 0 so that for
all t≤T
√ (cid:16) √ (cid:17) (cid:90) t
sup|q(vτ )−q(Vτ)|≤C Tηv2d−1/2 (2+M)2v2u+c2 T + L max|q(vτ )−q(Vτ)|ds,
td t s sd s
q∈Q 0 q∈Q¯
(73)
except on an event of probability Cd8e−u.
An application of Gronwall’s inequality gives
(cid:32) (cid:33)
√ (cid:16) √ (cid:17) (cid:90) T
sup sup |q(vτ )−q(Vτ)|≤C Tηv2d−1/2 (2+M)2v2u+c2 T exp L ds . (74)
td t s
q∈Q0≤t≤T 0
Now we note that by contour integration, both the risk v (cid:55)→ ⟨K,v⊗2⟩ and suboptimality
v (cid:55)→∥v∥2 both can be estimated by
max{(cid:12) (cid:12)∥θτ −θ∗∥2−∥Θτ −θ∗∥2(cid:12) (cid:12),2|R(θτ )−R(Θτ )|}≤4sup|q(vτ )−q(Vτ)|,
td td td td td t
q∈Q
proving our claim for the stopped processes. Now, it will be shown that with overwhelming
τ does not occur for n≤dT. It suffices to show that following lemma.
Lemma 4. There is an absolute constant C > 0 so that for all r ≥ 0 and all T ≥ 0 with
probability at least 1−2e−r2/2 for all 0≤s≤T,
√ ∥V ∥2 √
e−Cmax{η,η2}s−Cη Td−1/2r ≤ s ≤eCmax{η,η2}s+Cη Td−1/2r. (75)
∥V ∥2
0
18Proof. Consider φ(V )=log(1+∥V ∥2). Then,
t t
µ(X ) η2(t)2ν(V )P(V )Tr(K)
dφ(V )=−2η(t) t ∇P(V )TV dt+ t t dt
t 1+∥V ∥2 t t 1+∥V ∥2 d
t t
2η2(t)ν(V )P(V ) 2η(t)(cid:112) 2ν(V )P(V )(cid:68) √ (cid:69) (76)
− t t ⟨V ⊗V ,K⟩dt+ √ t t V , KdB .
d(1+∥V t∥2)2 t t d(1+∥V t∥2) t t
Note that, Tr(K)/d = ∥K∥ = 1 so the drift terms are all bounded above and below by
absoluteconstantsmultipliedbymax{η,η2}. Meanwhile,thequadraticvariationisbounded
by
(cid:90) t 8η2(s) ⟨V ⊗V ,K⟩
⟨φ(V)⟩ = ν(V )P(V ) s s ds (77)
t d s s (1+∥V ∥2)2
0 s
η2
≤8C t, (78)
d
for C an absolute constant.
And so, for all r ≥0 , setting f(t) to be the integrated drift terms from (76)
√ √
P( max |φ(V )−f(t)|≥Cη T/ dr)≤2exp(−r2/2). (79)
t
1≤t≤T
This implies the claim immediately as |f(t)|≤Cmax{η,η2}t for all t.
We can now conclude the main theorem, noting that if for some fixed T, if we pick M˜ so
that
(cid:40) (cid:32) (cid:33) (cid:41)
(cid:90) T
(2+M˜)2 =max C(1+∥V ∥2)exp Cs ,(2+2)2
0
0
then with probability at least 1−e−d, ∥V ∥ remains below M˜ up time T. As single steps of
t
clipped SGD cannot increase the norm of v by more than a factor of 2 (with probability
k
at least 1−e−cd), we conclude that if τ ≤Td, using (74)
(cid:32) (cid:33)
√ (cid:16) √ (cid:17) (cid:90) T
M2 =∥v ∥2 ≤4(M˜)2+4C Tηv2d−1/2 (2+M)2v2u+c2 T exp L ds .
τ s
0
Provided M ≥2 and provided that
(cid:32) (cid:33)
√ (cid:16) √ (cid:17) (cid:90) T 1
4C Tηv2 v2u+c2 T exp L ds d−1/2 ≤ , (80)
s 8
0
we have
1
M2 ≤4(M˜)2+ M2,
2
hence we conclude that √
M ≤ 8M˜.
√
SoifwepickM largerthan 8M˜ (whichislargerthan2byhowM˜ waspicked)weconclude
that τ >Td.
B.3 Bounding martingales and errors
Lemma 5. Martingale Bernstein inequality For {M }N a martingale, we define
k k=0
σ =inf{t>0:E[exp(|M −M |p/tp)|F ]≤2}, (81)
k,p k k−1 k−1
then there exists an absolute constant C >0 such that for all t>0
(cid:18) (cid:19) (cid:32) (cid:40) t t2 (cid:41)(cid:33)
P sup |M −E[M ]|>t ≤2exp −min , . (82)
1≤k≤N
k 0 Cmaxσ
k,1
C(cid:80)N σ2
i=1 i,1
19This section is dedicated to bounding the martingale and error terms present in Equations
(54) and (55). These terms are
η˜ η˜
∆Mτ,lin :=− k∇q(vτ)T clip √ (ℓ x )+ k∇q(vτ)TE[clip √ (ℓ x )|F ] (83)
k+1 d k c d k k+1 d k c d k k+1 k
η˜ η˜
=− k∇q(vτ)T clip √ (ℓ x )+ k∇q(vτ)TKvτµ(vτ)−∆Eτ,lin, (84)
d k c d k k+1 d k k k k
and
∆Mτ,quad := η˜ k (cid:10) ∇2q,clip √ (ℓ x )⊗2(cid:11) − η˜ k (cid:10) ∇2q,E(cid:2) clip √ (ℓ x )⊗2(cid:3)(cid:11) (85)
k+1 2d2 c d k k+1 2d2 c d k k+1
= η˜ k (cid:10) ∇2q,clip √ (ℓ x )⊗2(cid:11) − η˜ k (cid:10) K,∇2q(cid:11) ν(v )P(v )−∆Eτ,quad, (86)
2d2 c d k k+1 d2 k k k
where we recall ℓ = ⟨x ,vτ⟩−ϵ . The error increment has contributions from both
k k+1 k k+1
the linear—in η˜ —and quadratic terms. More precisely,
k
∆Eτ =∆Eτ,lin+∆Eτ,quad, (87)
k k k
where
η˜
∆Eτ,lin =− k∇q(θτ)TE[clip √ (x ℓ )−x clip (ℓ )]
k d k c d k+1 k k+1 c k
and
∆Eτ,quad = η˜ k (cid:16) E(cid:2)(cid:10) ∇2q,x⊗2 (cid:11) ℓ21 (cid:3) −(cid:10) ∇2q,K(cid:11)E(cid:104) ℓ21 (cid:105)(cid:17) .
k 2d2 k+1 k ∥ℓkxk+1∥2≤c2d k ℓ2 kTr(K)≤c2d
B.4 Martingale for the linear terms
We’llbegintheproofforthelineartermsintheincrements. First,notethatusingthe∥q∥
C2
norm we can bound
∥∇q(x)∥≤∥∇2q∥∥x∥+∥∇q(0)∥≤∥q∥ (1+∥x∥). (88)
C2
Thus,
|∇q(vτ)TKvτµ(vτ)|≤(1+M). (89)
k k k
From Equation (133) in the following section, for an absolute constant C >0
|∆Eτ,lin|≤C(2+M)2ηd−3/2v3. (90)
k
Meanwhile, we can get subexponential bounds for the former terms of (84),
η˜ η˜
− k∇q(vτ)T clip √ (ℓ x )=− k∇q(vτ)Tx ℓ 1 √ (91)
d k c d k k+1 d k k+1 k ∥ℓkxk+1∥≤c d
cη˜ ℓ x
− √k∇q(vτ)T k k+1 1 √ . (92)
d k ∥ℓ kx k+1∥ ∥ℓkxk+1∥>c d
So, by Assumptions 1 and 4, as well as Equation (88), we have
(cid:13) (cid:13)
(cid:13)∇q(vτ)Tx ℓ 1 √ (cid:13) ≤∥∇q(vτ)Tx ℓ ∥ (93)
(cid:13) k k+1 k ∥ℓkxk+1∥<c d(cid:13)
ψ1
t k+1 k ψ1
≤∥∇q(vτ)Tx ∥ ∥ℓ ∥ (94)
t k+1 ψ2 k ψ2
≤(1+M)v×(2+M)v. (95)
20Likewise,
(cid:13) (cid:13) (cid:13) (cid:13)c√ d∇q(v kτ)T ∥ℓℓ kk xx kk ++ 11 ∥1 ∥ℓkxk+1∥>c√ d(cid:13) (cid:13) (cid:13) (cid:13) ψ1 ≤(cid:13) (cid:13) (cid:13)∇q(v kτ)Tℓ kx k+11 ∥ℓkxk+1∥>c√ d(cid:13) (cid:13) (cid:13) ψ1 (96)
≤(cid:13) (cid:13)∇q(v kτ)Tℓ kx k+1(cid:13) (cid:13)
ψ1
(97)
≤(2+M)2v2. (98)
Thus, for some absolute constant C >0
(cid:18) (cid:19)
η v
σ =inf{t>0:E[exp(|∆Mτ,lin|/t)|F ]≤2}≤C (2+M)2v2 1+ √ . (99)
k,1 k k−1 d d
√
for all k. Hence once d≥v we may further bound away this additional fraction incurring
a further loss of a factor of 2. We may apply Lemma 5 to see that for all t > 1, and some
absolute constant c>0
(cid:18) (cid:19)
P sup |Mτ,lin−E[Mτ,lin]|>η(2+M)2v2(n/d)t ≤2exp(cid:0) −cnmin(cid:8) t2,t(cid:9)(cid:1) . (100)
k 0
1≤k≤n
In the case that n ≤ dT, this implies that there is an absolute constant so that for any
1≤u≤d
√
T
sup |Mτ,lin|≤Cη(2+M)2v2√ u (101)
1≤k≤n k d
with probability at least 1−exp(−u).
B.5 Martingale for the quadratic terms
We write
∆Mτ,quad = η˜ k (cid:10) ∇2q,clip √ (ℓ x )⊗2(cid:11) − η˜ k (cid:10) ∇2q,E(cid:2) clip √ (ℓ x )⊗2|F (cid:3)(cid:11) (102)
k+1 2d2 c d k k+1 2d2 c d k k+1 k
=T +T , (103)
1 2
where
(cid:20) (cid:21)
T = η˜ k (cid:10) ∇2q,x⊗2 (cid:11) ℓ21 −E η˜ k (cid:10) ∇2q,x⊗2 (cid:11) ℓ21 |F . (104)
1 2d2 k+1 k ℓ2 k∥xk+1∥2≤c2d 2d2 k+1 k ℓ2 k∥xk+1∥2≤c2d k
Notice that
(cid:12) (cid:12)
(cid:12)(cid:10) ∇2q,x⊗2 (cid:11) ℓ21 (cid:12)≤∥∇2q∥c2d, (105)
(cid:12) k+1 k ℓ2 k∥xk+1∥2≤c2d(cid:12)
so that
|T |≤c2η2d−1. (106)
1
As for T ,
2
η˜ c2 (cid:42) (cid:18) x (cid:19)⊗2(cid:43)
T = k ∇2q, k+1 1
2 2d ∥x k+1∥ ℓ2 k∥xk+1∥2≥c2d
(107)
(cid:34) η˜ c2 (cid:42) (cid:18) x (cid:19)⊗2(cid:43) (cid:12) (cid:12) (cid:35)
−E k ∇2q, k+1 1 (cid:12)F .
2d ∥x k+1∥ ℓ2 k∥xk+1∥2≥c2d(cid:12)
(cid:12)
k
Similarly,
(cid:12) (cid:12)η˜ c2 (cid:42) (cid:18) x (cid:19)⊗2(cid:43) (cid:12) (cid:12) ηc2
(cid:12) k ∇2q, k+1 1 (cid:12)≤ . (108)
(cid:12)
(cid:12)
2d ∥x k+1∥ ℓ2 k∥xk+1∥2≥c2d(cid:12)
(cid:12)
2d
So that overall,
2ηc2
|∆Mτ,quad|≤ (109)
k+1 d
21for all k. Then, by Lemma 5 we have for t≥1
(cid:18) (cid:19)
P sup |Mτ,quad−E[Mτ,quad]|>2ηc2(n/d)t ≤2exp(cid:0) −cnmin(cid:8) t,t2(cid:9)(cid:1) . (110)
k 0
1≤k≤n
Hence we conclude that for some absolute constant C and all 1≤u≤d
√
T
sup |Mτ,quad|≤Cηc2√ u (111)
1≤k≤n k d
with probability at least 1−exp(−u).
B.6 Martingale for the SDE
Recall equation (56)
Mτ,SDE
=(cid:90) t η √(s)(cid:112) 2ν(Vτ)P(Vτ)∇g(Vτ)T√
KdB . (112)
t s s s s
d
0
We may compute the quadratic variation of Mτ,SDE as
t
(cid:10) Mτ,SDE(cid:11)
=(cid:90) t 2η2(s)
ν(Vτ)P(Vτ)∇g(Vτ)TK∇g(Vτ)ds (113)
t d s s s s
0
using (88) we see that
(cid:10) Mτ,SDE(cid:11) ≤Cη2d−1(1+M)4t (114)
t
so that
sup (cid:10) Mτ,SDE(cid:11) ≤Cη2d−1(1+M)4T (115)
t
0≤t≤T
then using the sub-Gaussian tail bound for continuous martingales with bounded quadratic
variation gives for u≥1
(cid:18) (cid:12) (cid:12) √ √ (cid:19)
P sup (cid:12)Mτ,SDE(cid:12)>Cη(1+M)2 Tu/ d ≤2exp(cid:0) −u2(cid:1) (116)
(cid:12) t (cid:12)
0≤s≤T
so that increasing the absolute constant C >0 as needed, for all u≥1
√
(cid:12) (cid:12) T
sup (cid:12)Mτ,SDE(cid:12)≤Cη(1+M)2√ u (117)
(cid:12) t (cid:12)
0≤t≤T d
with probability at least 1−exp(−u).
B.7 Bounding the error terms
The remaining technical difficulty is in bounding the error terms. We will first focus on the
linear error term.
B.7.1 Linear error terms
η˜
∆Eτ,lin =− k∇q(vτ)TE[clip √ (x ℓ )−x clip (ℓ )] (118)
k d k c d k+1 k k+1 c k
=−η˜ k∇q(vτ)T (cid:0)E(cid:2) x ℓ 1 −x ℓ 1 (cid:3)(cid:1) (119)
d k k+1 k ∥ℓkxk+1∥2≤c2d k+1 k ℓkTr(K)≤c2d
(cid:34) (cid:35)
η˜ c x sgn(ℓ ) x sgn(ℓ )
− √k ∇q(vτ)TE k+1 k 1 − k+1 k 1 (120)
d k ∥x k+1∥ ∥ℓkxk+1∥2>c2d (cid:112) Tr(K) ℓ2 kTr(K)>c2d
η˜
=:− kE[D ]. (121)
d k
22For clarity, we will write ∇q(vτ) as ∇q . We see that
k k
0, ℓ2∥x ∥2 ≤c2d and ℓ2Tr(K)≤c2d,
D
=∇
√q kTx k+1ℓ k−
c√
|ℓd k∇ |√q kT Tx rk (K+1 )ℓk,
ℓk
2
k∥xk k+ +1
1∥2 ≤c2d and
ℓk
2 kTr(K)>c2d,
(122)
k c c√| |d dℓ ℓ∇ ∇k k| |q q∥ ∥k kT Tx xk kx x+ +k k+ +1 1∥ ∥1 1ℓ ℓk
k
− −∇ c√ |q ℓdkT
k∇
|x √qk
kT
T+
x
r1
k
(K+ℓ k
1
)ℓ,
k,
ℓℓ
2
k2 k ∥∥ xa k k+ +1 1∥ ∥2 2> >c c2 2d da an nd dℓ ℓ2 k
2
kT Tr r( (K K) )≤ >c c2 2d d,
.
Now, considering each case, we see that:
When ℓ2∥x ∥2 ≤c2d and ℓ2Tr(K)>c2d, we have
k k+1 k
(cid:12) √ (cid:12)
(cid:12) c d (cid:12)
|D |=|∇qTx ℓ |(cid:12)1− (cid:12) (123)
k k k+1 k (cid:12) (cid:112) (cid:12)
(cid:12) |ℓ k| Tr(K)(cid:12)
(cid:12) (cid:12)
(cid:12) ∥x ∥ (cid:12)
≤|∇qTx ℓ |(cid:12)1− k+1 (cid:12). (124)
k k+1 k (cid:12) (cid:112) (cid:12)
(cid:12) Tr(K)(cid:12)
When ℓ2∥x ∥2 >c2d and ℓ2Tr(K)≤c2d, we have
k k+1 k
(cid:12) √ (cid:12)
(cid:12) c d (cid:12)
|D |=|∇qTx ℓ |(cid:12) −1(cid:12) (125)
k k k+1 k (cid:12)|ℓ |∥x ∥ (cid:12)
(cid:12) k k+1 (cid:12)
(cid:12) (cid:12)
(cid:12) ∥x ∥ (cid:12)
≤|∇qTx ℓ |(cid:12)1− k+1 (cid:12). (126)
k k+1 k (cid:12) (cid:112) (cid:12)
(cid:12) Tr(K)(cid:12)
When ℓ2∥x ∥2 >c2d and ℓ2Tr(K)>c2d,
k k+1 k
|D k|=|∇q kTx k+1ℓ k|(cid:12) (cid:12) (cid:12) (cid:12)c√ d |ℓ1 k|(cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)∥x k1 +1∥ − (cid:112) T1 r(K)(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(127)
(cid:12) (cid:12)
(cid:12) ∥x ∥ (cid:12)
≤|∇qTx ℓ |(cid:12)1− k+1 (cid:12). (128)
k k+1 k (cid:12) (cid:112) (cid:12)
(cid:12) Tr(K)(cid:12)
Now using the numerical inequality |1−z|>t =⇒ |1−z2|>max{t,t2},
P(cid:32)(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)1− (cid:112)∥x Tk r+ (K1∥ )(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)>t(cid:33) ≤P(cid:18)(cid:12) (cid:12) (cid:12) (cid:12)1− ∥ Tx rk (+ K1∥ )2(cid:12) (cid:12) (cid:12) (cid:12)>max{t,t2}(cid:19) . (129)
Since,byassumptionTr(K)=d,andusingAssumption4weseethat,settings=max{t,t2}
(cid:32)(cid:12) (cid:12) (cid:33)
P (cid:12) (cid:12) (cid:12)1− (cid:112)∥x k+1∥ (cid:12) (cid:12) (cid:12)>t ≤P(cid:0)(cid:12) (cid:12)Tr(K)−∥x k+1∥2(cid:12) (cid:12)>ds(cid:1) (130)
(cid:12) Tr(K)(cid:12)
(cid:18) (cid:26) d2s2 ds (cid:27)(cid:19)
≤2exp −min , . (131)
v4∥K∥2 v2∥K∥
F
Since d2/∥K∥2 ≥d, we conclude that for all u≥1
F
(cid:32)(cid:12) (cid:12) (cid:33)
P (cid:12) (cid:12)1− ∥x k+1∥ (cid:12) (cid:12)>vu ≤2exp(cid:0) −du2(cid:1) . (132)
(cid:12) (cid:112) (cid:12)
(cid:12) Tr(K)(cid:12)
23The term |∇qTx ℓ | has a second moment bounded by (compare with (95))
k k+1 k
E|∇qTx ℓ |≤C(2+M)2v2
k k+1 k
for an absolute constant C >0, and hence we conclude for an absolute constant C >0
|∆Eτ,lin|≤C(2+M)2ηd−3/2v3. (133)
k
Thus, taking n≤dT steps, we get
max |Eτ,lin|≤CTηv3(2+M)2d−1/2. (134)
k
0≤k≤n
B.7.2 Quadratic error terms
This follows a similar path as the linear terms. We again express
∆Eτ,quad = η˜ k (cid:16) E(cid:2)(cid:10) ∇2q,x⊗2 (cid:11) ℓ21 (cid:3) −(cid:10) ∇2q,K(cid:11)E(cid:104) ℓ21 (cid:105)(cid:17) (135)
k 2d2 k+1 k ∥ℓkxk+1∥2≤c2d k ℓ2 kTr(K)≤c2d
+ η˜ kc2 (cid:32) E(cid:34)(cid:10) ∇2q,x⊗ k+2 1(cid:11) 1 (cid:35) − (cid:10) ∇2q,K(cid:11) P(cid:0) ℓ2Tr(K)>c2d(cid:1)(cid:33)
2d ∥x ∥2 ∥ℓkxk+1∥2>c2d TrK k
k+1
(136)
η˜
:= k E[D′] (137)
2d2 k
with
D′ =(cid:10) ∇2q,x⊗2 (cid:11) ℓ21 −(cid:10) ∇2q,K(cid:11) ℓ21 (138)
k k+1 k ℓ2 k∥xk+1∥2≤c2d k ℓ2 kTr(K)≤c2d
(cid:10) ∇2q,x⊗2 (cid:11) (cid:10) ∇2q,K(cid:11)
+c2d k+1 1 −c2d 1 (139)
∥x k+1∥2 ℓ2 k∥xk+1∥2>c2d Tr(K) ℓ2 kTr(K)>c2d
 ℓ2(cid:0)(cid:10) ∇2q,x⊗2 (cid:11) −(cid:10) ∇2q,K(cid:11)(cid:1) , ℓ2∥x ∥2 ≤c2d and ℓ2Tr(K)≤c2d,
(cid:10)k
∇2q,x⊗ k+2
1(cid:11)k+
ℓ2
k1
−c2d⟨∇ Tr2 (q K,K )⟩ ,
ℓk
2
k∥xk k+ +1
1∥2 ≤c2d and
ℓk
2 kTr(K)>c2d,
= (140)
c c2 2d d⟨ (cid:18)∇
∥
⟨x2 ∇q
k
∥,
+
x2x
q1
k⊗ k
,
+∥+ x2
2
11
⊗ k
∥+⟩
2
21−
⟩
−(cid:10) ∇ ⟨2
∇
Tq r2,
(q
KK
,K
)(cid:11) ⟩ℓ (cid:19)2 k,
,
ℓℓ2
k
2
k∥ ∥x
xk k+ +1
1∥ ∥2
2
> >c c2 2d
d
a an nd
d
ℓ ℓ2
k
2
kT Tr r( (K K) )≤ >c c22 dd .,
Consider the function by cases. On ℓ2∥x ∥2 ≤c2d and ℓ2Tr(K)>c2d we have
k k+1 k
(cid:12)
(cid:12)
(cid:10) ∇2q,K(cid:11)(cid:12)
(cid:12)
(cid:12)(cid:10) ∇2q,x⊗2 (cid:11) ℓ2 −c2d (cid:12) (141)
(cid:12) k+1 k Tr(K) (cid:12)
(cid:12) (cid:12)
=(cid:12)
(cid:12) (cid:12)ℓ2(cid:0)(cid:10) ∇2q,x⊗2 (cid:11) −(cid:10) ∇2q,K(cid:11)(cid:1) +(cid:10)
∇2q,K(cid:11)(cid:18)
ℓ2 −
c2d (cid:19)(cid:12)
(cid:12) (cid:12) (142)
(cid:12) k k+1 k Tr(K) (cid:12)
≤ℓ2 k(cid:12) (cid:12)(cid:10) ∇2q,x⊗ k+2 1(cid:11) −(cid:10) ∇2q,K(cid:11)(cid:12) (cid:12)+(cid:10) ∇2q,K(cid:11) cℓ 24 k d(cid:12) (cid:12)Tr(K)−∥x k+1∥2(cid:12) (cid:12). (143)
Similarly, if ℓ2∥x ∥2 >c2d and ℓ2Tr(K)≤c2d
k k+1 k
24(cid:12) (cid:12) (cid:10) ∇2q,x⊗2 (cid:11) (cid:12) (cid:12)
(cid:12)c2d k+1 −(cid:10) ∇2q,K(cid:11) ℓ2(cid:12) (144)
(cid:12) ∥x ∥2 k(cid:12)
(cid:12) k+1 (cid:12)
=(cid:12)
(cid:12) (cid:12)
c2d
(cid:0)(cid:10) ∇2q,x⊗2 (cid:11) −(cid:10) ∇2q,K(cid:11)(cid:1) +(cid:10)
∇2q,K(cid:11)(cid:18) c2d −w2(cid:19)(cid:12)
(cid:12) (cid:12) (145)
(cid:12)∥x ∥2 k+1 ∥x ∥2 (cid:12)
k+1 k+1
≤ℓ2 k(cid:12) (cid:12)(cid:10) ∇2q,x⊗ k+2 1(cid:11) −(cid:10) ∇2q,K(cid:11)(cid:12) (cid:12)+(cid:10) ∇2q,K(cid:11) cℓ 24 k d(cid:12) (cid:12)Tr(K)−∥x k+1∥2(cid:12) (cid:12). (146)
and finally when ℓ2∥x ∥2 >c2d and ℓ2Tr(K)>c2d we have
k k+1 k
(cid:12) (cid:12) (cid:12) (cid:12)c2d(cid:32)(cid:10) ∇ ∥2 xq,x⊗ k ∥+2 21(cid:11) − (cid:10) ∇ T2 r(q K,K )(cid:11)(cid:33)(cid:12) (cid:12) (cid:12) (cid:12)≤ℓ2 k(cid:12) (cid:12)(cid:10) ∇2q,x⊗ k+2 1(cid:11) −(cid:10) ∇2q,K(cid:11)(cid:12) (cid:12) (147)
(cid:12) k+1 (cid:12)
+(cid:10) ∇2q,K(cid:11) cℓ 24 k d(cid:12) (cid:12)Tr(K)−∥x k+1∥2(cid:12) (cid:12)1
ℓ2
kTr(K)>c2d. (148)
So overall,
|D k′|≤ℓ2 k(cid:12) (cid:12)(cid:10) ∇2q,x⊗ k+2 1(cid:11) −(cid:10) ∇2q,K(cid:11)(cid:12) (cid:12)+(cid:10) ∇2q,K(cid:11) cℓ 24 k d(cid:12) (cid:12)Tr(K)−∥x k+1∥2(cid:12) (cid:12)1
ℓ2
kTr(K)>c2d. (149)
Now, we may use the Hanson-Wright inequality (Assumption 4) along with the inequality
√ √
∥ K∇2q K∥2 ≤Tr(K)∥∇2q∥2 ≤Tr(K)=d (150)
F
to see that for all t≥0
P(cid:0)(cid:12) (cid:12)(cid:10) ∇2q,x⊗2 (cid:11) −(cid:10) ∇2q,K(cid:11)(cid:12) (cid:12)>tv2(cid:1)
≤2exp(cid:18) −min(cid:26) t2 ,t(cid:27)(cid:19)
. (151)
k+1 d
We also recall from (131) that
P(cid:0)
|∥x
∥2−Tr(K)|>tv2(cid:1)
≤2exp(cid:18) −min(cid:26) t2 ,t(cid:27)(cid:19)
.
k+1 d
Hence overall, we conclude that for some absolute constant C >0
√
|ED′|≤Cv4(1+M)2 d.
k
So that overall
(cid:12) (cid:12)
(cid:12)∆Eτ,quad(cid:12)≤Cηv4(1+M)2d−3/2. (152)
(cid:12) k (cid:12)
and summing over k ≤n≤Td,
(cid:12) (cid:12)
max (cid:12)Eτ,quad(cid:12)≤CTηv4(1+M)2d−1/2. (153)
(cid:12) k (cid:12)
0≤k≤n
This completes the proof of Lemma 2.
C Measuring the intrinsic dimension in real networks
Recall that the intrinsic dimension d is defined in terms of the spectrum of K:
d:=Tr(K)/∥K∥.
We can extend the definition of the intrinsic dimension to the non-linear setting by con-
sidering a linearization of the dynamics. Given a loss function L(z) and a model z(θ) on
parameters θ, the Gauss Newton matrix G of the loss is defined by:
G:=∇ z⊤∇2L∇ z. (154)
θ z θ
25Here ∇ z is the model Jacobian, and ∇2L is the Hessian of the loss with respect to the
θ z
model outputs. G encodes the second derivative of the loss with respect to a linearized
model z˜=z(θ )+∇ z(θ−θ ).
0 θ 0
For a linear model on MSE loss (as we studied in the main text), we have K = G. If we
tookanon-linearmodelduringtraining,andlocallylinearizedthemodelandloss,wewould
measure the intrinsic dimension with G as well. Therefore, on non-linear models, we will
define
d :=Tr(G)/∥G∥ (155)
nl
as the non-linear intrinsic dimension.
With this definition, we can measure the intrinsic dimension on neural network models
during training. We measured d on ResNet18 [26] and ViT S/16 [27] for networks trained
nl
onCIFAR10usingMSEloss(Figure5). WeseethatforResNet18,d increasesfrom∼100
nl
to 103, while for ViT d stays steady at ∼ 300. In both cases d is large, but it is very
nl nl
model dependent.
Thissuggeststhatrealneuralnetworkmodelsareintheeffectivelyhigh-dimensionalregime;
we leave to future work the question of which concepts from the basic theory generalize to
the non-linear setting.
103
102 ResNet18
ViT S/16
0 50 100 150 200
Epochs
Figure 5: Non-linear intrinsic dimension d for models trained on MSE loss. For ResNet18
nl
(blue),d increasesbyafactorof10overtraining,whileforViTS/16(orange)d remains
nl nl
relatively constant.
D Some examples of µ and ν
In this section we give some examples of µ and ν as defined in equation (7) under various
common distributions. First, we will describe how to define µ and ν as functions of the
c c
risk.
law(cid:112)
Notice that, with Gaussian data, ℓ = 2R(θ)ξ−ϵ where ξ is a standard Gaussian. Thus
θ
we can define µ˜ and ν˜ such that
∥E[clip ((cid:112) 2R(Θ )ξ−ϵ)x]∥ E[clip2((cid:112) 2R(Θ )ξ−ϵ)]
µ˜(Θ )= c t ν˜(Θ )= c t (156)
t ∥E[((cid:112)
2R(Θ )ξ−ϵ)x]∥
t E[((cid:112)
2R(Θ )ξ−ϵ)2]
t t
and µ (Θ )=µ˜ (R(Θ )) and ν (Θ )=ν˜ (R(Θ )). In what follows r =R(θ) for some Θ .
c t c t c t c t t
In the following examples, we will simplify notation and simply let R(Θ )=r.
t
D.1 Gaussian data and Gaussian noise
Consider a∼N(0,K) and ϵ∼N(0,σ2). First define
(cid:18) (cid:19) (cid:114)
z 2
F(z)=erf √ − ze−z2/2 (157)
2 π
26
noisnemiD
cisnirtnIThen, we have
(cid:32) (cid:33)
c
µ (r)=erf (158)
c (cid:112)
4(r+σ2/2)
(cid:32) (cid:33) (cid:32) (cid:33)
c c
(2r+σ2)ν (r)=2rF +c2erfc (159)
c (cid:112) (cid:112)
2(r+η2/2) 4(r+η2/2)
D.2 Gaussian data and Rademacher-like noise

−λ with probability q/2

ϵ = 0 with probability 1−q (160)
k
λ with probability q/2
Notethatσ2 =Var(ϵ)=λ2q. ForsomestandardGaussianrandomvariablez, µandν may
be computed as
(cid:18) (cid:19) (cid:18) (cid:19)
c c
µ (r)=qP |z−λ|≤ √ +(1−q)P |z|≤ √ (161)
c
2r 2r
(cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19) (cid:18) (cid:19)
q c−λ c+λ c
= erf √ +erf √ +(1−q)erf √ (162)
2 4r 4r 4r
(cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19) (cid:18) (cid:19)
q c−λ c+λ c
(2r+σ2)ν (r)=2r F √ +F √ +2r(1−q)F √ (163)
c 2 2r 2r 2r
qλ
(cid:18) (cid:18) (c+λ)2(cid:19) (cid:18) (c−λ)2(cid:19)(cid:19)
+ √ exp − −exp − (164)
πr 2r 2r
qλ2 (cid:18) (cid:18) c−λ(cid:19) (cid:18) c+λ(cid:19)(cid:19)
+ erf √ +erf √ (165)
2 4r 4r
qc2 (cid:18) (cid:18) c−λ(cid:19) (cid:18) c+λ(cid:19) (cid:18) c (cid:19)(cid:19)
+ erfc √ +erfc √ +(1−q)P |z|> √ (166)
2 2r 2r 2r
D.3 Gaussian data and uniform noise
For a∼N(0,K) and uniform noise supported on [−M,M] we have σ2 =M2/3 and
(cid:32) (cid:114) (cid:33)
µ c(r)=1− 2M1 c2 e−(c+ 4M r)2 (ecM r −1) 4 πr (167)
(cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
1 c−M c+M
− c2 (M −c)erfc √ +(c+M)erfc √ (168)
2M 4r 4r
27(cid:114)
(2r+σ2)ν c(r)= 6− M1 e−(c+ 4M r)2 4 πr (cid:16) −c2+cM −M2−4r+ecM/r(c2+cM +M2+4r)(cid:17)
(169)
(cid:18) (cid:19) (cid:18) (cid:19)
1 c−M 1 c+M
− (c3−M3−6Mr)erf √ + (c3+M3+6Mr)erf √
6M 4r 6M 4r
(170)
(cid:32) (cid:114) (cid:33)
+ 1 c2 e−(c+ 4M r)2 (ecM r −1) 2σ2 (171)
2M π
(cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
1 c−M c+M
+ c2 (M −c)erfc √ +(c+M)erfc √ (172)
2M 4r 4r
D.4 Gaussian data and symmetric exponential noise
For a∼N(0,K) and symmetric exponential noise, also known as Laplacian, with density
f(x)=λe−|x|λ/2 (173)
then we have
(cid:18) (cid:19)
c
µ (r)=2erf √ (174)
c
2r2
(cid:18) (c−λr2)(cid:19) (cid:18) (c+λr2)(cid:19)
+eλ(−2c+λr2)/2(e2cλ−erf √ −e2cλerf √ −1)/2 (175)
2r2 2r2
Then, if T (r) satisfies
c
(8+4λ2r2)T(r)=2eλ(2c+λr2)/2(2−2cλ+c2λ2)−2eλ(−2c+λr2)/2(2+2cλ+c2λ2)
(176)
(cid:18) (cid:19)
−4ce− 2rc 22 λ2(cid:112) 2r2/π+2λ2r2+2λ2r2(erf √c −1) (177)
2r2
(cid:18) (cid:19)
c
+(4+2λ2r2+8)erf √ (178)
2r2
(cid:18) c−λr2(cid:19)
−2eλ(−2c+λr2)/2(2+2cλ+c2λ2)erf √ (179)
2r2
(cid:18) c+λr2(cid:19)
−2eλ(2c+λr2)/2(2−2cλ+c2λ2)erf √ (180)
2r2
we have
ν (r)=T (r)+c2(1−µ (r))/(2r+σ2) (181)
c c c
28E Proof of stability and effectiveness theorems
E.1 Proof of Theorem 2
To see there exists c>0 such that (CSC) holds, it is simpler to work with the inverse of the
ratio. We remark that
ν E(ℓ21 +c21 )
lim c = lim |ℓ|≤c |ℓ|>c (182)
c→0+ µ c c→0+ P(|ℓ|≤c)
1 (cid:90) c2(1−P(|ℓ|≤c))
= lim ℓ2dP+ (183)
c→0+
P(|ℓ|≤c)
|ℓ|≤c
P(|ℓ|≤c)
1 (cid:90) c2
= lim ℓ2dP+ . (184)
c→0+
P(|ℓ|≤c)
|ℓ|≤c
P(|ℓ|<c)
Therefore, it suffices to show that (184) is less than 1. Indeed, the former term converges
to 0 by the Lebesgue-Differentiation Theorem. For the latter, let us assume that ϵ∼π for
some probability-measure π. Given that x∼N(0,K), let us denote f to be the (Gaussian)
density of ⟨x,θ−θ∗⟩. It follows that
P(|ℓ|≤c)=P(−c+ϵ≤⟨x,θ−θ∗⟩≤c+ϵ) (185)
(cid:90) (cid:90) c+ϵ
= f(x)dxdπ(ϵ). (186)
R −c+ϵ
Differentiating with respect to c yields,
d (cid:18)(cid:90) (cid:90) c+ϵ (cid:19) (cid:90)
f(x)dxdπ(ϵ) = f(c+ϵ)+f(−c+ϵ)dπ(ϵ). (187)
dc
R −c+ϵ R
By L’Hˆopital’s rule, the latter term of (184) becomes
c2 2c
cl →im 0P(|ℓ|<c) = cl →im 0(cid:82) Rf(c+ϵ)+f(−c+ϵ)dπ(ϵ) (188)
2c
= lim (cid:82) (189)
c→0+ R2f(ϵ)dπ(ϵ)
=0. (190)
E.2 Proof of Theorem 3
First, notice that σ2 =qλ2. In the limit as |R |→0 we have,
t
µ(R )=P(|ϵ|≤c) (191)
t
(cid:26)
1 λ≤c
= (192)
1−q λ>c
(cid:26)
1 λ≤c
ν(R )= (193)
t c2/λ2 λ>c
thus
µ(R )
t =1 λ<c (194)
ν(R )
t
µ(R t)
>1
c<(cid:112)
(1−q)λ (195)
ν(R )
t
µ(R t)
≤1
(cid:112)
(1−q)λ≤c<λ (196)
ν(R )
t
thus c may always be chosen such that the (CSC) is less than 1.
29E.3 Proof of Theorem 5
With µ and ν given by equations (158) and (159) we have that
µ2(R ) 2
lim t = <1 (197)
c→0 ν(R t) π
Meanwhile, it can be seen that for all R ≥0 µ2(R )/ν (R ) is increasing and continuous in
t c t c t
c. Since,
µ2(R )
lim c t =1 (198)
c→∞ ν(R t)
we are done.
E.4 Proof of Theorem 6
In light of Section E.2 above, we see that
µ (R )2
c t =1 λ<c (199)
ν (R )
c t
µ (R )2
c t >1 c<(1−q)λ (200)
ν (R )
c t
µ (R )2
c t ≤1 (1−q)λ≤c<λ (201)
ν (R )
c t
thus c may always be chosen such that the (CCC) holds.
F The risk under anisotropic data
Inthissection,wedescribehowtouseequation(12)tosolvefortherisk. UsingItˆo’sLemma
and the resolvent identity R(z;K)(K−z)=I.
dq (Θ )=−η(t)(cid:0) ∥Θ −θ∗∥2+2zq (Θ )(cid:1) µ (R(Θ ))dt
z t t z t c(t) t
η2(t) (202)
+ Tr(KR(z;K))ν (R(Θ ))dt+dM .
d c(t) t t
We shall let Q (t) be the deterministic equivalent of this equation, that is
z
d η2(t)
Q (t)=−η(t)(D +2zQ (t))µ (R )+ Tr(KR(z;K))ν (R ), (203)
dt z t z c(t) t d c(t) t
where (recalling Ω is the circle of radius 2)
−1 (cid:73) −1 (cid:73)
D = Q (t)dz and R = zQ (t)dz.
t 2πi z t 2πi z
Ω Ω
These are analogues of the same formulas that hold exactly for D(Θ ) and R(Θ ) when
t t
replacing Q by q (Θ ).
z z t
Now it is possible to precisely compare the solution of these ODEs to SGD, as the same
machinery developed for Theorem 1 applies. In particular, Lemma 1 bounds the supremum
differencesup |q (Θ )−Q (t)|(althoughnowwithMSDE ≡0). Hence, weconcludethe
z∈Ω z t z t
following:
30Theorem 8. Suppose that Assumptions 1, 2 and 3 hold. Suppose that {θ } is C-SGD.
k
Let c = sup c(t) and η = sup η(t). There is a constant C = C(v,(n/d),c,η,∥θ −θ∗∥2), a
t t 0
stochastic process E, and a constant m=m(v) so that for any 1≤u≤md
(cid:13)(cid:20) (cid:21) (cid:20) (cid:21)(cid:13)
sup (cid:13) (cid:13) R(θ k) − R k/d (cid:13) (cid:13)≤CE(n/d)ulog(d)d−1/2, (204)
0≤k≤n(cid:13) D(θ k) D k/d (cid:13)
with probability 1−e−u and provided the right hand side is less than 1. The stochastic
process E is given by
(cid:32) (cid:33)
(cid:90) t Cη(s)2σds
E(t)=exp
(cid:112)
R(Θ )+R
0 s s
for an absolute constant C >0. The constant C can be bounded by
C ≤C(cid:112) n/dηv2·((1+∥θ −θ∗∥2)v2+c2(cid:112) n/d)·exp(cid:0) Cmax{η,η2}(n/d)(cid:1)
0
for an absolute constant C >0.
We note that further details in this direction are shown in [19].
F.1 Getting a system of ODEs
WemayuseEquation(203)togetanequivalentcoupledsystemofd ODEswhichcansolve
for R . First, we may diagonalize,
t
d d
(cid:88) (cid:88) 1
K= λ w wT R(z;K)= w wT (205)
i i i λ −z i i
i
i=1 i=1
Where {λ }d and {w }d are the eigenvalues and eigenvectors of K respectively. There-
i i=1 i i=1
fore,
d
Q (Θ )=
1(cid:88) 1
⟨Θ −θ∗,w ⟩2. (206)
z t 2 λ −z t i
i
i=1
Define v (t)=⟨Θ −θ∗,w ⟩2/2. Then, R =(cid:80)d v (t)λ and D =(cid:80)d 2v (t). Now, we
i t i t i=1 i i t i=1 i
can find a system of ODEs which describes the evolution of {v }d .
i i=1
Choose Ω to be a complex curve enclosing only the i-th eigenvalue of K. Integrating over
i
both sides of equation (203) and using Cauchy’s integral formula, we see that
dv η(t)2
i =−2η(t)v λ µ (R )+ λ ν (R )(R +σ2/2), ∀i∈[d]. (207)
dt i i c(t) t d i c(t) t t
This final system of ODEs is used in all experiments to solve for R .
t
F.2 Getting an Integral Equation
Here we follow techniques of existing theory [20]. Using Equation (203) and an integrating
factor we see that
1(cid:90) t
Q z(t)=Q z(0)e−2zΩc t +
d
η(s)2ν c(s)(R s)Tr(KR(z;K)e−2z(Ωc t−Ωt s))(R s+σ2/2)ds
(208)
0
−η(t)D
te−2zΩc
tµ c(t)(R t)
31where Ωc =(cid:82)t η(s)µ (R )ds is the integrated clipped learning rate. Now, multiplying by
t 0 c(s) s
z, integrating both sides around Ω, and multiplying by −1/2πi, we get
1(cid:90) t
R
t
=R(Φg Γf
c
T)+
d
0
η˜2(s)ν csTr(K2e−2K(Γc t−Γc s))(R s+σ2/2)ds, (209)
where the first term is identified with gradient flow as in [20].
G Experimental details
G.1 Clipped SGD and Homogenized Clipped SGD
The experiments creating Figure 1 were carried out on a standard Google Colab CPU
runtime. Homogenized clipped SGD is solved via a standard Euler-Maruyama algorithm.
The procedure for solving for the risk is described in Appendix F.
TheexperimentscreatingFigure4wereagaincarriedoutonastandardGoogleColabCPU
runtime. Numerical optimization of the max-(CCC) clipping schedule (Equation (19)) was
done via the Nelder-Mead algorithm using standard python libraries.
Thecodetoreproducetheseresults,includingallplotsofstabilitythresholds(Figure2,and
Figure 3), is supplied in the Supplementary Materials.
G.2 Intrinsic dimension experiments
The experiments in Appendix C were carried out on 8 P100 GPUs trained in parallel with
batch size 128. This allowed for efficient computation of the full batch Gauss-Newton
operator norm via power iteration. Both networks were trained for 200 epochs. ResNet18
wastrainedwithcosinelearningratedecay(baselearningrate0.05), whileViTwastrained
withlinearwarmupfor2epochsfollowedbyacosinelearningratedecay(baselearningrate
0.00625). Both networks used GELU activation function.
32