Fine-Tuning Linear Layers Only Is a Simple yet
Effective Way for Task Arithmetic
RuochenJin1,2,BojianHou2,JiancongXiao2,WeijieSu2,andLiShen2
1EastChinaNormalUniversity,Shanghai,China,
2UniversityofPennsylvania,Philadelphia,PA,USA
{kyrie.jin, li.shen}@pennmedicine.upenn.edu
Abstract
Taskarithmetichasrecentlyemergedasacost-effectiveandscalableapproachto
editpre-trainedmodelsdirectlyinweightspace,byaddingthefine-tunedweights
ofdifferenttasks. Theperformancehasbeenfurtherimprovedbyalinearproperty
which is illustrated by weight disentanglement. Yet, conventional linearization
methods(e.g.,NTKlinearization)notonlydoublethetimeandtrainingcostbutalso
haveadisadvantageonsingle-taskperformance. Weproposeasimpleyeteffective
and efficient method that only fine-tunes linear layers, which improves weight
disentanglement and efficiency simultaneously. Specifically, our study reveals
thatonlyfine-tuningthelinearlayersintheattentionmodulesmakesthewhole
modeloccurinalinearregime,significantlyimprovingweightdisentanglement.To
furtherunderstandhowourmethodimprovesthedisentanglementoftaskarithmetic,
we present a comprehensive study of task arithmetic by differentiating the role
ofrepresentationmodelandtask-specificmodel. Inparticular, wefindthatthe
representationmodelplaysanimportantroleinimprovingweightdisentanglement
whereasthetask-specificmodelssuchastheclassificationheadscandegeneratethe
weightdisentanglementperformance. Overall,ourworkuncoversnovelinsights
intothefundamentalmechanismsoftaskarithmeticandoffersamorereliableand
effectiveapproachtoeditingpre-trainedmodels1.
1 Introduction
Theemergenceoflargepre-trainedmodelsintheopen-sourcecommunitymaximizesthepotential
toboostperformanceondownstreamtasks[1,2,3], alignwithhumanpreferences[4,5,6,7,8],
and enhance robustness [9, 10, 11, 12]. Traditional methods involve expensive joint fine-tuning
acrossvarioustasks[3]andrelianceonhumanfeedback[5],limitingtheirscalabilityandwidespread
adoption. Moreover,optimizingperformanceforspecificdownstreamtasksusuallycompromisesthe
model’sinitialpre-trainingperformanceorzero-shotaccuracy[13,14].
Basedontheextensiveresourcesofopen-sourcemodels,weoftenaimtoeditthepre-trainedmodels
withoutrequiringaccesstoadditionaltrainingdata,inordertoimproveperformanceonmultiple
downstream tasks. Task arithmetic [1] is a method proposed for this goal. The central to task
arithmeticisaconceptcalledthetaskvectorthatenablesmodelstoadapttonewtasks[1]. Atask
vectorcanbeviewedasasetofweightadjustmentsspecificallycalibratedforagiventaskthrough
fine-tuning,obtainedbysubtractingthetask-specificweightsfromtheoriginalpre-trainedweights.
Essentially,eachtaskvectorencodesauniquerepresentationalsignaturetailoredtoaparticulartask.
TheillustrationoftaskarithmeticisshowninFigure1. Recentresearchinthisdomain,regarding
taskvector-centricapproaches[1,15,16,17,18],hasdemonstratedthatbyaggregatingmultipletask
1Thecodeisavailableathttps://github.com/kyrie-23/linear_task_arithmetic.
Preprint.Underreview.
4202
luJ
9
]GL.sc[
1v98070.7042:viXraDisentanglement:
Nearly Equal Performance
Task Arithmetic
Inference Inference
Finetuning
Task 1
Pre-trained Model
Inference Inference
Task 2
... ...
Inference Inference
Task K
Figure1: Illustrationoftaskarithmetic,thatisaddingτ doesnotmodifytheoutputofmodelout-
t
sideD . Thisallowsamodeltomanipulateparametersindependentlybyaddinglinearcombinations
t
ofτ toapre-trainedcheckpointθ . g(·;ω )referstokthtask-specificmodelparameterizedbyω .
t 0 k k
vectorsandintegratingthemintoapre-trainedmodel,itispossibletocreateanewmodel,whichwe
refertoasaunifiedmodel,andadaptittomulti-tasklearning.
However,taskarithmeticfromnon-linearfine-tuningisnotwithoutlimitations. Foreachindividual
task,althoughtheunifiedmodelshowssomeimprovementoverthepre-trainedmodel,itsperformance
isusuallynotcomparabletothatofamodelspecificallytrainedforthattask. Thisisbecauseatask
vector for one particular task usually has a negative effect on the performance of the other tasks.
Therefore,thefirstquestionofthispaperarises:
Question1: Howcanweimprovetheperformanceoftaskarithmetic?
Ideally,Question1issupposedtoberesolvedbyweightdisentanglement,wheretheunifinedmodel’s
performanceshouldnotbeaffectedbyothertaskswhenitisappliedtoonespecifictask(illustrated
intherighthandsideofFigure1). However,weightdisentanglementisthemostchallengingproblem
in task arithmetic. It has been found that enforcing models to be fine-tuned in the tangent space
significantly improves weight disentanglement, thanks to the fact that models inherently operate
in a linear regime [17]. Here the linear regime refers to the behavior of neural networks during
thebeginningofthefine-tuningphase,wherethenetworkupdatesoccurprimarilyaroundthepre-
trainedparameterinitialization. Thisphenomenonisformalizedbytheneuraltangentkernel(NTK)
theory[19]. WhileNTKlinearizationiseffective,itrequirestwotothreetimesmorecomputational
resourcesanddoublesthememoryfootprintcomparedtoitsnonlinearcounterpart. Theadditional
costofNTKlinearizationviolatestheoriginalgoaloftaskarithmeticwedescribedabove. Asaresult,
Question1boilsdowntothefollowing:
Question2: Howtoimprovethedisentanglementandefficiencyoftaskarithmeticsimultaneously?
To answer Question 2, we aim to propose a novel fine-tuning method with the following two
properties. Fordisentanglement,thefine-tuningmethodissupposedtooperatebetterinthelinear
regime. Forefficiency,thefine-tuningmethodshouldonlybeappliedtoapartofthenetworkto
reducecomputationalcost. Hence,weproposeasimpleyetefficientandeffectivemethodthatis
to fine-tune only linear layers to achieve both of the two goals simultaneously. Specifically, our
study reveals that only fine-tuning the linear layers can make the attention modules occur in a
linearregime,significantlyimprovingbothweightdisentanglementandaccuracycomparedtoboth
nonlinearcounterpartsandNTKlinearization[17](seeTable1). Meanwhile,ourmethodismuch
moreefficient,asitfine-tunesonlyasmallportionoflayers,therebyreducingthecomputational
burdenandmemoryusage. Additionally,ourapproachmitigatesthenon-linearadvantagewhere
non-linearfine-tuningconsistentlyachievesthehighestaccuracy,offeringabalancedandefficient
alternative.
Tofurtherunderstandhowourmethodimprovesthedisentanglementoftaskarithmetic,wepresent
astudybydifferentiatingtheroleofrepresentationmodelandtask-specificmodel,whileexisting
literature[17]formulatedtaskarithmeticusingasinglemodelwithoutclearlydifferentiatingthem.We
2conductacomprehensivestudyoftaskarithmeticincontrastivelypre-trainedvision-language(ViT)
modelslikeCLIP[20],providingnewinsightsintoitsfundamentalmechanismsandproposingnovel
methodstoimprovetheperformanceofpre-trainedmodelsthroughtaskarithmetic. Specifically,we
illustratethattherepresentationmodelplaysanimportantroleinimprovingweightdisentanglement
whereasthishasbeenconstrainedbytask-specificmodels,suchasclassificationheads.
Notably,wedemonstratethattheattentionmoduleliesinastronglinearregimewithinitslinearlayers.
Withoutanypriorknowledge,wecanachievesuperiorperformancetothecurrentstate-of-the-art
methodsbysimplyfine-tuningalllinearlayersintheattentionmodule. However,theperformance
caneitherimproveordegradedependingonwhetherthebiasparametersarefine-tuned. Thebest
resultsareobtainedwhenthesettingsaligncloselywiththoseusedinLoRA[21],whichfine-tunes
onlyQ,K,V,andoutputprojectionweights,indicatingthatthisaspectwarrantsfurtherexploration.
Inparticular,ourmaincontributionsareasfollows:
• Weproposeasimpleyeteffectiveandefficientmethodthatonlyfine-tuneslinearlayers,
whichimprovesweightdisentanglementandmulti-taskperformanceupto2.38%improve-
mentcomparedtothestate-of-the-artmethodsand8.37%overthenonlinearbaselineon
severalvision-languagebenchmarks.
• Wedemonstratethatfine-tuningalllinearlayerswithintheattentionmoduleoccurringin
alinearregime,withoutselectivelyfreezingoromittingcertainlayersachievessuperior
performancecomparedtocurrentstate-of-the-artmethodsonthisbenchmarktask.
• Wereformulatethearchitectureoftaskarithmeticin[17]intoarepresentationmodeland
severaltask-specificmodels,aligningournotationwithpreviouswork. Thisallowsusto
moreclearlyillustratetheirindividualcontributions.
• We illustrate that the representation model plays an important role in improving weight
disentanglement,whereastheeffectivenessoftaskarithmetichasbeenconstrainedbythe
task-specificmodels,suchasclassificationheads.
Overall,ourworkprovidesnewinsightsintothefundamentalmechanismsoftaskarithmetic,enhanc-
ingthereliabilityandscalabilityofmodelediting. Ourfindingsindicatethatfine-tuningpre-trained
models,particularlyfocusingontheorthogonalityoftaskvectors,deservesfurtherinvestigationdue
toitssignificantpotentialforimprovingmodeleditingeffectiveness. Theseinsightscanleadtothe
developmentofmoreefficientandaccuratemodeleditingtechniques,enablingpractitionerstoadapt
pre-trainedmodelstoawiderarrayoftasks.
2 Taskarithmeticisareflectionofweightdisentanglement
Existingliterature[17]formulatedtaskarithmeticusingasinglemodel,whichactuallycombinesa
representationmodel(e.g.,CLIP)withseveraltask-specificmodels(e.g.,classificationheads). This
combinationleadstodifferentpre-trainedparametersθ foreachtaskduetotheuseofdifferent
0
task-specificmodels(e.g.,differentclassificationheadsfordifferentdatasets). Theimplementationof
taskarithmeticfocusesontherepresentationmodel,extractingtaskvectors,fine-tuning,andreplacing
differenttask-specificmodels.
Tobetterunderstandtherelationshipbetweentherepresentationmodelandthetask-specificmodels,
weseparatetheirdefinitions. Thisallowsustomoreclearlyillustratetheirindividualcontributions
totaskarithmetic. Weformulatethetaskarithmeticpropertyalongwithweightdisentanglement,
providing distinct definitions for the representation and task-specific models while aligning our
notationwithpreviouswork.
Let F : X ×Θ → Y be a neural network taking inputs x ∈ X and parameterized by a set of
weightsϑ ∈ Θ,whichconsistsofarepresentationmodelf(·;θ)andatask-specificmodelg(·;ω)
whereϑ={θ,ω}. WewillassumeX ⊆Rd,Θ⊆Rm andY ⊆Rc. ConsiderT tasks,withevery
task t consisting of a triplet (D ,µ ,F∗), where D ⊆ X is a data support (e.g., ImageNet [22]
t t t t
images), µ an input distribution such that supp(µ ) = D , and F∗ : D → Y a target function
t t t t t
(e.g., labels). In practice, each task is identified with a training set {(x ,F∗(x ))} where
v t v v∈[n]
F∗(x )=g(f(x ;θ∗);ω )withx∼µ ,thatisusedtofine-tunetherepresentationmodelsstarting
t v v t t t
fromthepre-trainedweightsθ andobtainthefine-tunedweightsθ∗,whilethetaskspecifiedmodels
0 t
arefixedatω .
t
3Taskarithmetic. Letthetaskvectoroftasktbethedifferencebetweenthefine-tunedandthepre-
trainedweights,i.e.,τ =θ∗−θ . Thefollowingpropertyformalizesthenotionoftaskarithmetic
t t 0
introducedinOrtiz-Jimenezetal. [17],wheretheauthorsobservedthattheaccuraciesofpre-trained
modelsondifferentdatasetscanbemodifiedindependentlythroughtheadditionorremovaloftask
vectors.
Property1(Taskarithmetic) ConsiderasetoftaskvectorsT = {τ } withassociatednon-
t t∈[T]
intersectingtasksupportsD = {D ⊂ X} ,i.e.,∀t,t′ift ̸= t′thenD ∩D = ∅. Wesaya
t t∈[T] t t′
networkF satisfiesthetaskarithmeticpropertyaroundϑ withrespecttoT andDif
0
(cid:32) T (cid:33) (cid:40)
F x;ϑ
+(cid:88)
α τ =
F(x;ϑ 0+α tτ t) ifx∈D t,
(1)
0 t t
F(x;ϑ ) ifx∈/
(cid:83)T
D ,
t=1 0 t=1 t
whereϑ ={θ ,ω }andϑ +α τ ={θ +α τ ,ω }incertaintaskt,with(α ,...,α )∈A⊆
0 0 t 0 t t 0 t t t 1 T
RT.
Inshort,amodelsatisfiesProperty1ifaddingτ doesnotmodifytheoutputofthemodeloutsideD .
t t
Property2(Weightdisentanglement) A parametric function F : X × Θ → Y is weight dis-
entangled with respect to a set of task vectors T = {τ } and the corresponding supports
t t∈[T]
D ={D } if
t t∈[T]
(cid:32) T (cid:33)  
(cid:88) (cid:91)
F x;ϑ 0+ α tτ
t
=F(x;ϑ 0+α tτ t)1(x∈D t)+F(x;ϑ 0)1 x∈/ D t.
t=1 t∈[T]
FromProperty1andProperty2wecanseethatweightdisentanglementisnotnecessarilylinkedto
performanceonvarioustasks. Inotherwords,amodelcanachieveweightdisentanglementrelative
toagroupoftaskvectorsbutstillunderperformsonagiventask. Forexample,iff(·;θ +ατ)does
0
notgeneralizewellforsomeα,themodelmaystillfailtoperformeffectivelydespitebeingweight
disentangled[17].
3 Fine-tuninglinearlayersonlyismuchmoreefficientthanNTK
linearization
Theobjectiveofthisworkistoillustratetherelationshipbetweenthelinearregimewhilefine-tuning
ViTmodels(attentionmodule)andtaskarithmeticperformance. Existingwork[17]demonstrated
thatthelinearregimeisnotnecessaryfortaskarithmetic,butmodelswithinthelinearregimeexhibit
superiordisentanglementperformance.
Wehaveseenthatlinearizedmodelsaremoreweight-disentangledthannon-linearones[17].However,
NTKlinearizationoftendegradessingle-taskperformance,whichisdemonstratedasanon-linear
advantage. In this study, we show that fine-tuning only the linear layers significantly improves
taskarithmeticperformancebyreducingthesingle-taskaccuracygap. Thisapproachmaintainsthe
benefitsofweightdisentanglementwhileenhancingtheoveralleffectivenessoftaskarithmeticacross
varioussettings.
Neuraltangentkernel. Aroundtheinitializationweightsθ ,arepresentationmodelf(x;θ)canbe
0
approximatedwithafirst-orderTaylorexpansion:
f(x;θ)=f(x;θ )+(θ−θ )⊤∇ f(x;θ )+higherorderterms. (2)
0 0 θ 0
This approximation is equivalent to the neural tangent kernel (NTK) [19], k (x,x′) =
NTK
∇ f(x;θ )⊤∇ f(x′;θ ), and defines a neural tangent space in which the relationship between
θ 0 θ 0
weightsandfunctionsislinear. However,thislinearapproximationisofteninvalidatfinitewidths,
astheevolutionofparametersduringtrainingisinadequatelycapturedbyEq. (2). Insuchcases,
trainingoccursinanon-linearregime. Conversely,oftenduringfine-tuning,parameterevolutionin
manypre-trainedmodelsisfrequentlyminimal,meaningthattrainingdoesnotexitthetangentspace
andEq. 2closelyapproximatesthenetworkbehavior[17]. Insuchcases,trainingoccursinalinear
regime.
4Trainable Fixed Linearized
Parameters Parameters Part
Layer Norm Layer Norm Layer Norm
Feed Forward Feed Forward Feed Forward
Layer Norm Layer Norm Layer Norm
Attention Attention Attention
(a) (b) (c)
Figure 2: Three types of fine-tuning paradigms. (a) Full parameter fine-tuning where all the
parameterswillbeupdated. (b)Full-modellinearization. (c)Linearlayersonlyfine-tuningwhere
onlyW ,W ,W andW willbeupdated. Inthispaper,weexplorelinearlayersonlyfine-tuning.
q v k o
Specifically,ratherthanconstrainingmodelstofine-tunewithinthetangentspaceasproposedby
[17],weenhancethelinearregimewithintheattentionmodule,whichnaturallyarisesfromthelinear
architecture. This enhancement leverages the inherent properties of linear layers to simplify the
model’sadaptationprocess. Byfine-tuningonlyaselectnumberoflinearlayers,themodelnotonly
liesinapronouncedlinearregimebutalsomaintainscomputationalefficiency.Thisapproachachieves
performancelevelscomparabletofullfine-tuningwhilesignificantlyreducingthecomplexityand
resourcesrequired.
Figure2illustratesthreedifferenttypesoffine-tuningparadigms. Thefirsttwoareexistingmethods,
while the third is our proposed partial linearization fine-tuning method. (a) The full fine-tuning
paradigmwhereallparametersθareupdatedduringfine-tuning. (b)Thefull-modellinearization
paradigmwherewefine-tunethemodelinthetangentspace. Itisworthnotingthatalthoughthe
Jacobian-vectorproductscanbecomputedinasingleforwardpass[23],trainingandinferencein
thisparadigmareusuallytwiceorthreetimesasexpensiveasfullfine-tuning[18]. (c)Thelinear
layersonlyfine-tuningparadigminwhichonlyasmallnumberoflinearlayersareupdated,which
exhibitsalinearregime,whichissimilartolinearfine-tuning. Thisapproachincursonlyafractionof
thetrainingandinferencecostscomparedtoNTKlinearization.
TheresultsinFigure3indicatethatourmethodre-
ducesthegapbetweenlinearandnon-linearmodels 1.00
and has a better performance than NTK lineariza-
tion respectively2. Specifically, we fine-tune (FT) 0.95
severalCLIPpre-trainedViTs[24]ofdifferentsizes
0.90
following the same setup as Ilharco et al. [1] on 8
tasks: Cars[25],DTD[26],SUN397[27],EuroSAT 0.85
Ours vs. Non-linear (ViT-B-32)
[28], GTSRB [29], MNIST [30], SVHN [31] and Ours vs. Non-linear (ViT-B-16)
RESISC45[32]. Therounddotsrepresentthecom- 0.80 Ours vs. Non-linear (ViT-L-14)
NTK Linearization vs. Non-linear (ViT-B-32)
parisonbetweenourmethodandnon-linearmodels,
0.75 NTK Linearization vs. Non-linear (ViT-B-16)
whilethetriangledotsshowthecomparisonbetween NTK Linearization vs. Non-linear (ViT-L-14)
NTKlinearizationandnon-linearmodels. Theprox- 0.70 Equal Accuracy Line
imityofdotstothediagonaldashedlineindicatesthe 0.70 0.75 0.80 0.85 0.90 0.95 1.00
Non-linear Finetuning Accuracy(%)
accuracyofthelinearizationmethod. Ourmethod,
representedbyrounddots,consistentlyappearscloser Figure3: Single-taskaveragedaccuraciesof
tothediagonaldashedlinethantheNTKlineariza- non-linearandlinearmodels. Thediagonal
tion(triangledots),suggestingsuperiorperformance. dashedlineindicateslinearfine-tuningperfor-
mancemeetsnon-linear.
2PleaseseeAppendixB.1forperformanceoneachtask
5
)%(ycaruccA
gninuteniF
raeniLTable1:Taskaddition. Averageabsolute(%)andnormalizedaccuracies(%)ofdifferentViTsedited
byaddingthesumofthetaskvectorsof8tasks. Wereportresultsforthenon-linearandlinearized
modelsnormalizingperformancebytheirsingle-taskaccuracies.
ViT-B-32 ViT-B-16 ViT-L-14
Method
Abs.(↑) Norm.(↑) Abs.(↑) Norm.(↑) Abs.(↑) Norm.(↑)
Pre-trained 48.40 - 55.25 - 66.40 -
Non-linear 70.00 77.04 74.75 80.59 84.40 89.47
Linear 76.26 85.82 79.01 86.32 85.53 91.44
Ours 78.37 87.42 80.44 87.25 87.91 93.66
EventhoughNTKlinearizationmodelsexhibitalinearregimeandachieveimpressivedisentangle-
mentperformance,theyomithigher-orderterms,leadingtoaccumulatederrorsasthenumberof
tasksincreases. Toaddressthis,weproposeleveraginglinearlayerstoenhancedisentanglement
performance. OurmethodretainsthebenefitsofNTKlinearizationwhilemitigatingthedrawbacks
associatedwithneglectedhigher-orderterms. Byfine-tuningthelinearlayerswithintheattention
module,weimprovecomputationalefficiencyandrobustnessacrossmultipletasks.
Consideringacompletelylinearrepresentationmodelf,wecaneasilydeterminethat∇ f(x;θ )=x.
θ 0
This foundational insight supports our approach, as the gradient with respect to the parameters
simplifiestotheinputdataitself. Thus,byfine-tuningonlythelinearlayers,weenhanceweight
disentanglementandmaintainmodelefficiencywithoutextensiveretraining,aligningwiththeconcept
ofweightdisentanglementtoimproveperformanceacrossvarioustasks.
Proposition1(OrthogonalitybetweenVectorsandDataPoints) For a linear representation
model f, the gradient with respect to the parameters is given by ∇ f(x;θ ) = x. Therefore,
θ 0
themodelcanbeexpressedas:
f(x;θ)=f(x;θ )+(θ−θ )⊤x. (3)
0 0
Foraspecifictaskt=i,thefunctionf canbefurtherexpressedas:
(cid:32) T (cid:33)
(cid:88) (cid:88)
f x;θ + τ =f(x;θ +τ )+ τ⊤x.
0 t 0 i t
t=1 t̸=i
When(cid:80) τ⊤x→0,weightdisentanglementisachieved.
t̸=i t
Proposition 1 implies that the orthogonality of data points x and task vectors τ leads to perfect
t
disentanglement. Thisallowsfortheevaluationofcosinesimilaritybetweendatapointsandtask
vectors,providingaclearadvantageoverNTKlinearization,wheresuchperfectdisentanglementis
notguaranteed.
InspiredbyLoRA[21]whichshouldbeconsideredasaspecialformoflinearization,wechoosethe
matrixofQ,K,V,andtheoutputprojection. Concentratingonattentionmodules,giveanexample
ofasingleattentionlayerasappliedinVaswanietal. [33],duetothelinearpropertyoftheweight
matrix,wecaneasilygetadisentangleoutputfromeachattentionheadforacertaintaski:
head=Attention(qWQ,kWK,vWV)
(cid:88) (cid:88) (cid:88)
=Attention(Q + q WQ,K + k WK,V + v WV)
i i t i i t i i t
t̸=i t̸=i t̸=i
=Attention(Q ,K ,V ),
i i i
whereq,k,vrepresentthequery,key,valueinput,WQ,WK,WV representeachweight,e.g. Q =
i
q(W +WQ),WQ =WQ+(cid:80) WQ.
0 i 0 t t
Remarkably,asweshowinSection4,thisincreaseinsingle-taskperformancedoesnotcompromise
weightdisentanglement,instead,it’smuchhigherthanNTKlinearization. Asaresult,linearfine-
tuningallowsforimprovedtaskarithmeticcomparedtostandardnon-linearfine-tuning. Tobetter
6illustratethemulti-taskperformance,weemploythebenchmarkproposedbyIlharcoetal. [1]to
evaluatethetaskarithmeticabilityofapre-trainedmodel,whichconsistsofthe8tasksdescribed
(cid:80)
before: Thesumofthetaskvectorsτ = τ isaddedtoapre-trainedcheckpointtoproducea
t t
multi-taskmodel. Thesuccessofthisbenchmarkismeasuredintermsofthemaximumaverage
accuracyoverthedifferenttasks. ResultsareshowninTable1. Incontrasttorecentwork[16,18],
ourmethodfocusesontheoriginaltaskarithmeticsetting,evaluatingmulti-taskperformanceand
furtherexaminingweightdisentanglement,whichdoesnotrequireaccesstoalltestdataandisnotin
aparameter-efficientsettingeither.
Toobtainthetaskvectors,weusethefine-tunedweightsofdifferentViTsfrombefore,andusea
singlemixingcoefficientα=α =···=α optimizedseparatelyforeachfine-tuningparadigmto
1 T
ensureafaircomparison. WeprovideallthedetailsofthisexperimentinAppendixA.
Inparticular,Table1initslastrowsshowsthatourmethodsignificantlyoutperformsitsnon-linear
counterparts[1]andachievesstate-of-the-artresultsonthetaskadditionbenchmarks. Ourmodel
achieveshighermulti-taskaccuraciesthroughtaskaddition(upto2.38%more). Additionally,our
methodnotonlyoutperformsonaveragedaccuracybutalsoonnormalizationaccuracy.
In addition to input and output projection layers, can we improve disentanglement performance
byfine-tuningalllinearlayersintheattentionmodule,andhowwouldperformancegowhenbias
parametersarealsofine-tuned?Todigintothisquestion,weconductanablationexperimentwithfour
paradigms: (1)onlyfine-tuningQ,K,V,andoutputlayerweights(ours),(2)fine-tuningQ,K,V,
andoutputlayerweightsandbias,(3)fine-tuningalllinearlayersweightsintheattentionmodule,(4)
fine-tuningalllinearlayersweightsandbiasintheattentionmodule.
Surprisingly, all four paradigms outperform Table2: Single-taskandmulti-taskperformance(%)
NTK linearization in both performance and onfourdifferentsettings.
disentanglement,demonstratingthatViTmod-
elsexhibitastronglinearregimewithinthelin- Single-task Multi-task
Paradigm
earlayersoftheattentionmodule(seeTable2). Accuracy(↑) Abs.(↑) Norm.(↑)
However,theperformancecaneitherimprove
(1) 89.55 78.37 87.42
ordegradedependingonwhetherthebiaspa-
(2) 89.48 77.71 86.79
rametersarefine-tuned. Thebestresultsare
achievedwhenthesettingsaligncloselywith (3) 88.95 76.52 86.11
thoseusedinLoRA,suggestingthatthisisan (4) 89.43 77.80 86.93
areadeservingfurtherexploration.
4 Weightdisentanglementemergesfromrepresentationmodel
Existingworksdemonstratetheeffectivenessoftaskarithmeticprimarilyinthecontextofdown-
streamtasks. However,therelianceondownstreamtasksandtask-specificmodelsmaylimitthe
generalizabilityandscalabilityofthesemethods. Tofurtherillustratethepoweroftaskarithmetic
andbroadenitsapplicability,weaimtoinvestigatewhethertherepresentationmodelitselfcansatisfy
Property3withouttheneedforatask-specificmodel.
UnliketheapproachoutlinedinSection2,weredefinetaskarithmeticandweightdisentanglement
withintherepresentationmodelitself. Ourhypothesisisthatpre-trainedmodelscanexhibittask
arithmeticpropertiesindependentlyofdownstreamtasks,maintainingasimilarrepresentationthrough
taskarithmetic. Byfocusingontherepresentationmodelalone,weseektoshowthattheinherent
propertiesofthepre-trainedmodelsaresufficienttosupporttaskarithmetic,potentiallysimplifying
theprocessandbroadeningitsapplicability.
Property3(Taskarithmetic*) ConsiderasetoftaskvectorsT ={τ } withassociatednon-
t t∈[T]
intersectingtasksupportsD = {D ⊂ X} ,i.e.,∀t,t′ift ̸= t′thenD ∩D = ∅. Wesaya
t t∈[T] t t′
representationmodelf satisfiesthetaskarithmeticpropertyaroundθ withrespecttoT andDif
0
(cid:32) T (cid:33) (cid:40)
f x;θ
+(cid:88)
α τ =
f(x;θ 0+α tτ t) ifx∈D t;
0 t t
f(x;θ ) ifx∈/
(cid:83)T
D .
t=1 0 t=1 t
Property4(Weightdisentanglement*) A parametric function f : X × Θ → Y is weight dis-
entangled with respect to a set of task vectors T = {τ } and the corresponding supports
t t∈[T]
7Standard NTK Linearization Ours
Figure4: Visualizationofweightdisentanglement. Theheatmapsshowthedisentanglementerror
ξ(α ,α )ofasinglerepresentationmodelCLIPViT-B/32(top)andacombinationofrepresentation
1 2
modelandclassificationmodel(bottom)onDTD-SUN397taskpair. Threefine-tuningparadigms
areusedfromlefttoright: standard,NTKlinearization,andours. Thelightregionsdenoteareasof
theweightspacewhereweightdisentanglementisstronger.
D ={D } if
t t∈[T]
(cid:32) T (cid:33) T  
(cid:88) (cid:88) (cid:91)
f x;θ 0+ α tτ
t
= f(x;θ 0+α tτ t)1(x∈D t)+f(x;θ 0)1 x∈/ D t. (4)
t=1 t=1 t∈[T]
Insteadofevaluatingweightdisentanglementonpredictors,weightdisentanglementisapropertyof
therepresentationmodelsandisnotrelatedtotheperformanceondifferenttasks. Thatis,amodel
couldbeweightdisentangledwithrespecttoasetoftaskvectorsandstillperformpoorlyonatask,
e.g.,iff(·;θ +ατ)doesnotgeneralizeforsomeα. Moregenerally,wecanvisualizethelevelof
0
weightdisentanglementofamodelbymeasuringitsdiscrepancywithEq. (4). Todoso,giventwo
tasks,onecancheckthedisentanglementerrorofamodel,
2
(cid:88)
ξ(α ,α )= E [dist(f(x;θ +α τ ),f(x;θ +α τ +α τ ))],
1 2 x∼µt 0 t t 0 1 1 2 2
t=1
wheredistdenotesanydistancemetricbetweenoutputvectors. Aswearedealingwithrepresentation
distributions, inwhatfollowsweusetheKLdivergenceasthedistancemetric3. Ingeneral, the
smallerthevalueofξ(α ,α )themoreweightdisentangledamodelisat(α ,α ).
1 2 1 2
Figure4displaysthedisentanglementerrorofaCLIPViT-B/32modelconcerningseveraltaskvector
pairs. WeobservethattheViTmodelexhibitsaminimaldisentanglementerrorwithinasmallregion
surroundingθ ,whichenablestaskarithmetic. Differentfromdisentanglementerroratdownstream
0
tasks,itremainsrelativelysmallevenforα ,α >1,whichindicatesthepoweroftaskarithmetic
1 2
hasbeenlimitedbytheperformanceoftask-specificmodels(classificationhead).
ComparingthedisentanglementerrorofourmodelsandNTKlinearizationrevealsaninteresting
finding: linearizedmodelsexhibitgreaterdisentanglementthantheirnon-linearcounterparts. Thisis
evidentfromthemoreextensiveregionswithlowdisentanglementerrorsinFigure4(right). This
3WeusepredictionerrorforthecombinedmodelforclassificationtaskasOrtiz-Jimenezdid[17].
8
noitatneserpeR
noitacifissalCStandard NTK Linearization Ours
Figure 5: Similarity heatmaps. These figures show heatmaps of the cosine similarity between
taskvectorsfromtask-specificCLIPmodels[20]fine-tunedondifferenttasks. Threefine-tuning
paradigmsfromlefttoright: Fullparameterfine-tuning(Standard),Full-modellinearization(NTK
linearization),andLinearlayersonlyfine-tuning(Ours).
explainswhythelinearlayersfine-tuningonlymodelsachievehighernormalizedaccuraciesviatask
addition(cf.Table1).Thecombinationofgreaterdisentanglementandbettersingle-taskperformance
comeswithhighermulti-taskperformance.
TheresultsinFigures5showthecosinesimilaritybetweentaskvectorsfromViT,whicharethree
typesoffine-tuning(cf. Figure2)onimageclassificationtasks. Weobservethatvectorsfromlinear
layersonlyfine-tuningareclosertoorthogonalthanthosefrombothstandardandNTKlinearization,
whichindicatesthatmodelsfine-tunedwithfullfine-tuningaremoreindependentthanothers. This
findingisconsistentwiththediscussionabouttaskadditionin[1,18],theexperimentalresultsfrom
Table1alsosupportourstatement. TheexperimentaldetailsaredescribedinAppendix.
5 Relatedwork
WeightInterpolationandTaskArithmetic. Recentresearchhasexploredtheuseofinterpolations
betweenmodelweightsandtaskarithmetictomanipulateandenhancethecapabilitiesofpre-trained
models. Studieshaveshownthatinterpolatingbetweenamodel’sfine-tunedweightsanditspre-
trained initialization can improve performance on single tasks, often surpassing the accuracies
achieved through fine-tuning alone [34, 35, 36, 37, 38, 39]. In multi-task settings, averaging the
parametersofmultiplefine-tunedmodelshasbeenproposedtocreatesuperiormulti-taskmodels
[40,15,41,2,1],whichhelpavoidcatastrophicforgetting[13,14,42,43]andprovidebetterstarting
pointsforsubsequentfine-tuning[44,45]. Thesebenefitsalsoextendtomodelstrainedfromscratch,
provided they are properly aligned before merging [46, 47]. This technique has been observed
to enhance downstream performance, highlighting the potential of weight interpolation and task
arithmetic.
ModelFusionTechniques. Modelfusionintegratesknowledgefrommultiplemodelsintoasingle
unified model. One approach focuses on fusing entire models through weight interpolation. By
averaging or combining the weights of multiple models, effective model fusion can be achieved,
asdemonstratedinpriorworks[48,2,1,36,40,49]. Whenmodelsarenotwell-alignedorliein
differentlossbasins,featurealignmenttechniquesareemployedbeforefusiontomatchthemodels’
behaviorsortransformthemtoasimilarlossbasin[46,50]. Althoughfusingentiremodelsleverages
knowledgefromalllayers,itcanbecomputationallyexpensive.
AdvancesinModelMergingforMTL.Modelmerginghasemergedasapromisingsolutionto
enhancemodelgeneralizationandfacilitatemulti-tasklearning(MTL).Researchinthisareaincludes
mergingmodelstrainedonthesametasktoimproveoverallgeneralization[51,52]ortoperform
federatedlearning[53,54].Anotherapproachfocusesonmergingmodelsfordifferenttaskstoenable
MTL [36, 46, 55, 1, 15, 56, 57]. However, simple model averaging can significantly deteriorate
performance across multiple tasks. To address this, advanced techniques have been developed.
For example, Fisher Merging uses the Fisher information matrix to measure the importance of
individualmodelparametersandguidemodelmerging[36].Althougheffective,computingtheFisher
informationmatrixiscomputationallyandmemory-intensive.
96 Conclusion
Inthiswork,weconductedathoroughanalysisoftaskarithmeticindeepneuralnetworks,delvinginto
itsfundamentalmechanismsandenhancingitsperformance. Ourfindingsdemonstratethatattention
moduleliesinastronglinearregimewithinitslinearlayers,whichimprovebothdisentanglement
andefficiency.
Understandingthenuancedimpactoffine-tuningbiasonmodelperformanceanddisentanglement
remainsanopenquestion. Futureresearchcouldprovidevaluableinsightsintooptimizingthese
settings,potentiallyleadingtomorerobustandefficientmethodsforadaptingpre-trainedmodels
tovarioustasks. Thiscouldsignificantlyenhancetheirapplicabilityandeffectivenessinreal-world
scenarios.
References
[1] GabrielIlharco,MarcoTúlioRibeiro,MitchellWortsman,SuchinGururangan,LudwigSchmidt,
HannanehHajishirzi,andAliFarhadi. Editingmodelswithtaskarithmetic. InInternational
ConferenceonLearningRepresentations(ICLR),2023.
[2] GabrielIlharco,MitchellWortsman,SamirYitzhakGadre,ShuranSong,HannanehHajishirzi,
Simon Kornblith, Ali Farhadi, and Ludwig Schmidt. Patching open-vocabulary models by
interpolatingweights. InAdvancesinNeuralInformationProcessingSystems(NeurIPS),2022.
[3] FuzhenZhuang,ZhiyuanQi,KeyuDuan,DongboXi,YongchunZhu,HengshuZhu,HuiXiong,
andQingHe. Acomprehensivesurveyontransferlearning. ProceedingsoftheIEEE,2020.
[4] Ximing Lu, Sean Welleck, Liwei Jiang, Jack Hessel, Lianhui Qin, Peter West, Prithviraj
Ammanabrolu,andYejinChoi. Quark: Controllabletextgenerationwithreinforcedunlearning.
InAdvancesinNeuralInformationProcessingSystems(NeurIPS),2022.
[5] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin,
ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelsto
followinstructionswithhumanfeedback,2022.
[6] MarcoTulioRibeiroandScottLundberg. Adaptivetestinganddebuggingofnlpmodels. In
AnnualMeetingoftheAssociationforComputationalLinguistics(ACL),2022.
[7] AmeliaGlaese,NatMcAleese,MajaTrebacz,JohnAslanides,VladFiroiu,TimoEwalds,Mari-
bethRauh,LauraWeidinger,MartinChadwick,PhoebeThacker,LucyCampbell-Gillingham,
Jonathan Uesato, Po-Sen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth
Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green,
Sona Mokra, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel,
WilliamIsaac,JohnMellor,DemisHassabis,KorayKavukcuoglu,LisaAnneHendricks,and
Geoffrey Irving. Improving alignment of dialogue agents via targeted human judgements.
https://www.deepmind.com/blog/building-safer-dialogue-agents,2022.
[8] JiancongXiao,ZiniuLi,XingyuXie,EmilyGetzen,CongFang,QiLong,andWeijieJSu.
Onthealgorithmicbiasofaligninglargelanguagemodelswithrlhf: Preferencecollapseand
matchingregularization. arXivpreprintarXiv:2405.16455,2024.
[9] Bo-Jian Hou, Lijun Zhang, and Zhi-Hua Zhou. Learning with feature evolvable streams.
AdvancesinNeuralInformationProcessingSystems,30,2017.
[10] Guillermo Ortiz-Jiménez, Apostolos Modas, Seyed-Mohsen Moosavi-Dezfooli, and Pascal
Frossard. Optimism in the face of adversity: Understanding and improving deep learning
throughadversarialrobustness. ProceedingsoftheIEEE,2021.
[11] ShibaniSanturkar,DimitrisTsipras,MahalaxmiElango,DavidBau,AntonioTorralba, and
AleksanderMadry. Editingaclassifierbyrewritingitspredictionrules. InAdvancesinNeural
InformationProcessingSystems(NeurIPS),2021.
[12] MatthewTancik,PratulP.Srinivasan,BenMildenhall,SaraFridovich-Keil,NithinRaghavan,
UtkarshSinghal, RaviRamamoorthi, JonathanT.Barron, andRenNg. Fourierfeatureslet
networkslearnhighfrequencyfunctionsinlowdimensionaldomains. InAdvancesinNeural
InformationProcessingSystems(NeurIPS),2020.
10[13] Robert M French. Catastrophic forgetting in connectionist networks. Trends in Cognitive
Sciences,1999.
[14] MichaelMcCloskeyandNealJCohen. Catastrophicinterferenceinconnectionistnetworks:
Thesequentiallearningproblem. InPsychologyofLearningandMotivation.Elsevier,1989.
[15] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. Resolving
interferencewhenmergingmodels. InAdvancesinNeuralInformationProcessingSystems
(NeurIPS),2023.
[16] E. Yang, Z. Wang, L. Shen, S. Liu, G. Guo, X. Wang, and D. Tao. Adamerging: Adaptive
modelmergingformulti-tasklearning. InTheTwelfthInternationalConferenceonLearning
Representations,October2023.
[17] G.Ortiz-Jimenez,A.Favero,andP.Frossard. Taskarithmeticinthetangentspace: Improved
editingof pre-trainedmodels. In Advancesin NeuralInformation ProcessingSystems, vol-
ume36,2024.
[18] A.Tang,L.Shen,Y.Luo,Y.Zhan,H.Hu,B.Du,andD.Tao. Parameter-efficientmulti-task
modelfusionwithpartiallinearizeation. InTheTwelfthInternationalConferenceonLearning
Representations,October2023.
[19] ArthurJacot,FranckGabriel,andClémentHongler. Neuraltangentkernel: Convergenceand
generalization in neural networks. In Advances in Neural Information Processing Systems
(NeurIPS),2018.
[20] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgar-
wal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlya
Sutskever. Learningtransferablevisualmodelsfromnaturallanguagesupervision. InInterna-
tionalConferenceonMachineLearning(ICML),2021.
[21] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Seyeon Wang, Lu Wang, and
WeizhuChen.Lora:Low-rankadaptationoflargelanguagemodels.InInternationalConference
onLearningRepresentations,October2021.
[22] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei. Imagenet: Alarge-scale
hierarchicalimagedatabase. InIEEEConferenceonComputerVisionandPatternRecognition
(CVPR),2009.
[23] BarakA.Pearlmutter. Fastexactmultiplicationbythehessian. NeuralComputation,6(1):147–
160,January1994.
[24] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,
Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,
JakobUszkoreit,andNeilHoulsby. Animageisworth16x16words: Transformersforimage
recognitionatscale. InInternationalConferenceonLearningRepresentations(ICLR),2021.
[25] JonathanKrause,MichaelStark,JiaDeng,andLiFei-Fei. 3dobjectrepresentationsforfine-
grainedcategorization. InInternationalConferenceonComputerVisionWorkshops(ICCVw),
2013.
[26] MirceaCimpoi,SubhransuMaji,IasonasKokkinos,SammyMohamed,andAndreaVedaldi.De-
scribingtexturesinthewild. InIEEEConferenceonComputerVisionandPatternRecognition
(CVPR),2014.
[27] JianxiongXiao,KristaAEhinger,JamesHays,AntonioTorralba,andAudeOliva.Sundatabase:
Exploring a large collection of scene categories. International Journal of Computer Vision
(IJCV),2016.
[28] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel
dataset and deep learning benchmark for land use and land cover classification. Journal of
SelectedTopicsinAppliedEarthObservationsandRemoteSensing,2019.
[29] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic
signrecognitionbenchmark: Amulti-classclassificationcompetition. InInternationalJoint
ConferenceonNeuralNetworks(IJCNN),2011.
[30] YannLeCun. Themnistdatabaseofhandwrittendigits,1998.
[31] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y Ng.
Readingdigitsinnaturalimageswithunsupervisedfeaturelearning. InAdvancesinNeural
InformationProcessingSystems(NeurIPS)Workshops,2011.
11[32] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification:
Benchmarkandstateoftheart. ProceedingsoftheIEEE,2017.
[33] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformation
processingsystems,30,2017.
[34] JonathanFrankle,GintareKarolinaDziugaite,DanielRoy,andMichaelCarbin. Linearmode
connectivityandthelotterytickethypothesis. InInternationalConferenceonMachineLearning
(ICML),2020.
[35] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon
Wilson. Averagingweightsleadstowideroptimaandbettergeneralization. InConferenceon
UncertaintyinArtificialIntelligence(UAI),2018.
[36] MichaelMatenaandColinRaffel.Mergingmodelswithfisher-weightedaveraging.InAdvances
inNeuralInformationProcessingSystems(NeurIPS),2021.
[37] AlexandreRamé,KartikAhuja,JianyuZhang,MatthieuCord,LéonBottou,andDavidLopez-
Paz. Modelratatouille: Recyclingdiversemodelsforout-of-distributiongeneralization. In
InternationalConferenceonMachineLearning(ICML),2022.
[38] AlexandreRamé,MatthieuKirchmeyer,ThibaudRahier,AlainRakotomamonjy,PatrickGalli-
nari,andMatthieuCord. Diverseweightaveragingforout-of-distributiongeneralization. In
AdvancesinNeuralInformationProcessingSystems(NeurIPS),2022.
[39] Mitchell Wortsman, Gabriel Ilharco, Mike Li, Jong Wook Kim, Hannaneh Hajishirzi, Ali
Farhadi,HongseokNamkoong,andLudwigSchmidt. Robustfine-tuningofzero-shotmodels.
InIEEEConferenceonComputerVisionandPatternRecognition(CVPR),2022.
[40] MitchellWortsman,GabrielIlharco,SamirYitzhakGadre,RebeccaRoelofs,RaphaelGontijo-
Lopes,AriSMorcos,HongseokNamkoong,AliFarhadi,YairCarmon,SimonKornblith,etal.
Model soups: averaging weights of multiple fine-tuned models improves accuracy without
increasinginferencetime. InInternationalConferenceonMachineLearning(ICML),2022.
[41] MargaretLi,SuchinGururangan,TimDettmers,MikeLewis,TimAlthoff,NoahASmith,and
LukeZettlemoyer. Branch-train-merge: Embarrassinglyparalleltrainingofexpertlanguage
models,2022.
[42] Bo-JianHou, Yu-HuYan, PengZhao, andZhi-HuaZhou. Storagefitlearningwithfeature
evolvablestreams. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume35,
pages7729–7736,2021.
[43] Bo-JianHou,LijunZhang,andZhi-HuaZhou. Predictionwithunpredictablefeatureevolution.
IEEETransactionsonNeuralNetworksandLearningSystems,33(10):5706–5715,2021.
[44] Shachar Don-Yehiya, Elad Venezian, Colin Raffel, Noam Slonim, Yoav Katz, and Leshem
Choshen. Coldfusion: Collaborativedescentfordistributedmultitaskfinetuning,2022.
[45] LeshemChoshen,EladVenezian,NoamSlonim,andYoavKatz. Fusingfinetunedmodelsfor
betterpretraining,2022.
[46] SamuelKAinsworth,JonathanHayase,andSiddharthaSrinivasa.Gitre-basin:Mergingmodels
modulopermutationsymmetries. InInternationalConferenceonLearningRepresentations
(ICLR),2023.
[47] SidakPalSinghandMartinJaggi. Modelfusionviaoptimaltransport. InAdvancesinNeural
InformationProcessingSystems(NeurIPS),2020.
[48] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry Vetrov, and Andrew Gordon
Wilson. Loss surfaces, mode connectivity, and fast ensembling of dnns. In Conference on
UncertaintyinArtificialIntelligence(UAI),2018.
[49] TianYuLiuandStefanoSoatto. Tangentmodelcompositionforensemblingandcontinual
fine-tuning,July2023. arXiv:2307.08114[cs].
[50] Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. Dataless knowledge
fusion by merging weights of language models. In International Conference on Learning
Representations(ICLR),2023.
12[51] Vipul Gupta, Santiago Akle Serrano, and Dennis DeCoste. Stochastic weight averaging in
parallel:Large-batchtrainingthatgeneralizeswell.In8thInternationalConferenceonLearning
Representations(ICLR).OpenReview.net,2020.
[52] JunbumCha,SanghyukChun,KyungjaeLee,Han-CheolCho,SeunghyunPark,YunsungLee,
andSungraePark. Swad: Domaingeneralizationbyseekingflatminima. InAdvancesinNeural
InformationProcessingSystems,volume34,pages22405–22418,2021.
[53] XiangLi,KaixuanHuang,WenhaoYang,ShusenWang,andZhihuaZhang.Ontheconvergence
offedavgonnon-iiddata. InInternationalConferenceonLearningRepresentations(ICLR),
2019.
[54] HongyiWang,MikhailYurochkin,YuekaiSun,DimitrisPapailiopoulos,andYasamanKhaz-
aeni. Federatedlearningwithmatchedaveraging. InInternationalConferenceonLearning
Representations(ICLR),2020.
[55] GeorgeStoica,DanielBolya,JakobBjorner,TaylorHearn,andJudyHoffman. Zipit! merg-
ing models from different tasks without training. In International Conference on Learning
Representations(ICLR),2023.
[56] WeishiLi,YongPeng,MiaoZhang,LiangDing,HanHu,andLiShen. Deepmodelfusion: A
survey,2023. arXivpreprintarXiv:2309.15698.
[57] Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He. Composing parameter-efficient
moduleswitharithmeticoperations. InAdvancesinNeuralInformationProcessingSystems
(NeurIPS),2023.
[58] MehdiCherti,RomainBeaumont,RossWightman,MitchellWortsman,GabrielIlharco,Cade
Gordon,ChristophSchuhmann,LudwigSchmidt,andJeniaJitsev. Reproduciblescalinglaws
for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition(CVPR),pages2818–2829,June2023.
[59] IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. InInternational
ConferenceonLearningRepresentations(ICLR),2019.
13A Experimentaldetails
Allourexperimentswereperformedusingthesamehardwareconsistingoffour3090NVIDIAGPUs
with24GBofmemoryeachwhichcanbereproducedinlessthan150GPUhours. Thedetailsof
eachexperimentarethefollowing.
Datasets. Weevaluatetaskarithmeticonasetofpopularbenchmarkdatasetsfromvariousdomains.
Thedatasetcollectionincludes:
• SVHN [31]: The Street View House Numbers dataset is a real-world image dataset for
developingmachinelearningandobjectrecognitionalgorithmswithminimalrequirement
ondatapreprocessingandformatting.
• MNIST[30]: Adatabaseofhandwrittendigits,with60,000trainingimagesand10,000
testingimages.
• EuroSAT[28]: AdatasetbasedonSentinel-2satelliteimagescovering13spectralbands,
with10classesandatotalof27,000labeledandgeo-referencedimages.
• RESISC45[32]:Theremotesensingimagesceneclassificationdataset,consistingof31,500
imagesin45sceneclasses.
• Cars[25]: Thisdatasetcontainsimagesofcarscategorizedintovariousfine-grainedclasses.
Itiswidelyusedforfine-grainedimageclassificationtasks,providingarichsetofvehicle
imagesfortrainingandevaluation.
• DTD(DescribableTexturesDataset)[26]: Thisdatasetisdesignedfortexturerecognition
andcategorization. Itconsistsoftextureimagesorganizedinto47categories,eachlabeled
with attributes describing the texture patterns. It is commonly used to evaluate texture
recognitionalgorithms.
• SUN397[27]: TheSceneUNderstanding(SUN)datasetisalarge-scaledatasetforscene
recognition,containing397categorieswithatotalofover100,000images. Itisusedto
evaluatesceneunderstandingmodelsandtobenchmarksceneclassificationalgorithms.
• GTSRB(GermanTrafficSignRecognitionBenchmark)[29]: Thisdatasetcomprises
imagesofGermantrafficsigns,classifiedintoover40categories. Itisusedtodevelopand
evaluatetrafficsignrecognitionsystems,particularlyinthecontextofautonomousdriving
andintelligenttransportationsystems.
Fine-tuning. Allthefine-tuningexperimentsfollowthesametrainingprotocolspecifiedinIlharco
etal. [2]withminormodificationstothetrainingcodetouselinearizedmodelswhenneeded. In
particular,wefine-tunealldatasetsstartingfromthesameCLIPpre-trainedcheckpointdownloaded
fromtheopen_cliprepository[58]. Wefine-tunefor2,000iterationswithabatchsizeof128,a
learningrateof10−5 andacosineannealinglearningrateschedulewith200warm-upstepsand
theAdamWoptimizer[59]. AsintroducedinIlharcoetal. [2],duringfine-tuning,wefreezethe
weightsoftheclassificationlayerobtainedbyencodingastandardsetofzero-shottemplateprompts
foreachdataset. Freezingthislayerdoesnotharmaccuracyandensuresthatnoadditionallearnable
parametersareintroducedduringfine-tuning[2]. Weusethisexactsameprotocoltofine-tunethe
non-linear and linearized models and do not perform any form of hyperparameter search in our
experiments.
Tuningofαintaskarithmeticbenchmarks. AsinIlharcoetal. [2],weuseasinglecoefficient
αtotunethesizeofthetaskvectorsusedtomodifythepre-trainedmodels. Thisisequivalentto
settingα=α =...=α inEq. 1. Inthetaskadditionbenchmarks,afterfine-tuning,weevaluate
1 T
differentscalingcoefficientsα ∈ {0.0,0.05,0.1,...,1.0}andchoosethevaluethatachievesthe
highesttargetmetriconasmallheld-outproportionofthetrainingsetasspecifiedinIlharcoetal. [2].
Namely,maximumnormalizedaverageaccuracy,andminimumtargetaccuracyoneachdatasetthat
stillretainsatleast95%oftheaccuracyofthepre-trainedmodelonthecontroltask. Thetuningofα
isdoneindependentlyfornon-linearFT,linearizedFT,andpost-hoclinearization.
Normalizedaccuraciesintaskaddition. Table1showsthenormalizedaccuraciesafterediting
(cid:80)
differentmodelsbyaddingthesumofthetaskvectorson8tasksτ = τ . Here,thenormalization
t t
isperformedwithrespecttothesingle-taskaccuraciesachievedbythemodelfine-tunedoneachtask.
Mathematically,
14T (cid:80)
Normalizedaccuracy=
1 (cid:88)[acc(f(x;θ 0+ tτ t))]
. (5)
T [acc(f(x;θ +τ ))]
0 t
t=1
Disentanglement error. To produce the weight disentanglement visualizations of Figure 4, we
compute the value of ξ(α ,α ) on a 15×15 grid of equispaced values in [−2,2]×[−2,2]. To
1 2
estimatethedisentanglementerror,weusearandomsubsetof2,048testpointsforeachdataset.
B Furtherexperimentalresults
Wenowpresentadditionalexperimentsthatexpandthefindingsdiscussedinthemaintext.
B.1 Fine-tuningaccuracies
InFigure6,wereportthesingle-taskaccuraciesachievedbydifferentCLIPmodelsafterfine-tuning
withdifferentdynamics(referredtoasnon-linear,NTKlinearization,andourmethod).
B.2 Weightdisentanglementondifferenttaskpairs
InFigure7,weillustrateweightdisentanglementondifferenttaskpairs.
Standard NTK Linearization Ours
Figure7: Visualizationofweightdisentanglement. Theheatmapsshowthedisentanglementerror
ξ(α ,α )ofasinglerepresentationmodelCLIPViT-B/32(top)andacombinationofrepresentation
1 2
modelandclassificationmodel(bottom)onCars-RESISC45taskpair. Threefine-tuningparadigms
areusedfromlefttoright: standard,NTKlinearization,andours. Thelightregionsdenoteareasof
theweightspacewhereweightdisentanglementisstronger. Theredboxdelimitsthesearchspace
usedtocomputethebestαinallourexperiments.
C ImpactStatement
ThispaperpresentsworkwhosegoalistoadvancethefieldofMachineLearning. Therearemany
potentialsocietalconsequencesofourwork,nonewhichwefeelmustbespecificallyhighlighted
here.
15
noitatneserpeR
noitacifissalC(a)ViT-B/32
(b)ViT-B/16
(c)ViT-L/14
Figure 6: Single-task accuracies (CLIP). Accuracy of different models obtained using different
strategiesoneachofthetasks.
16