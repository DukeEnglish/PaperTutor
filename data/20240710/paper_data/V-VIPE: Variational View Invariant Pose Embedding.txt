V-VIPE: Variational View Invariant Pose Embedding
MaraLevy AbhinavShrivastava
UniversityofMaryland,CollegePark UniversityofMaryland,CollegePark
mlevy@umd.edu abhinav@cs.umd.edu
Abstract
V-VIPE
Learning to represent three dimensional (3D) human
pose given a two dimensional (2D) image of a person, is 3D Pose Retreival
a challenging problem. In order to make the problem less
ambiguousithasbecomecommonpracticetoestimate3D
poseinthecameracoordinatespace. However,thismakes
the task of comparing two 3D poses difficult. In this pa- 3D Pose Generation
per, we address this challenge by separating the problem
of estimating 3D pose from 2D images into two steps. We
useavariationalautoencoder(VAE)tofindanembedding
thatrepresents3Dposesincanonicalcoordinatespace. We
2D to 3D Pose Estimation
refer to this embedding as variational view-invariant pose
embedding(V-VIPE).UsingV-VIPEwecanencode2Dand
3Dposesandusetheembeddingfordownstreamtasks,like Figure1. TheseveralfunctionsV-VIPEiscapableof. Thepurple
pathrepresents3Dposeretrieval.Thebluepathrepresentsgenera-
retrievalandclassification. Wecanestimate3Dposesfrom
tionbyaddingnoisetothepurplepath.Theresultisavariationof
theseembeddingsusingthedecoderaswellasgenerateun-
theoriginalpose.Thegreenpathshows2Dto3Dposeestimation
seen 3D poses. The variability of our encoding allows it
fromseveralviewpoints.
to generalize well to unseen camera views when mapping
from 2D space. To the best of our knowledge, V-VIPE is
movetowardsenvironmentswherewehaveverylittlecon-
the only representation to offer this diversity of applica-
troloverthecameraviewpoint,suchasphotostakenwitha
tions. Codeandmoreinformationcanbefoundathttps://v-
phoneorARglasses. Insuchscenarios,wecanmakevery
vipe.github.io/.
fewassumptionsaboutthecameraspace.
In this paper, we address this challenge by separating
the problem of estimating 3D pose from 2D images into
1.Introduction
two steps. First, we learn an embedding to represent 3D
Learning to represent three dimensional (3D) human pose posesincanonicalcoordinatespace. Next, welearntoen-
givenatwodimensional(2D)imageofaperson,isachal- code 2D poses, from different camera viewpoints, to the
lengingproblemwithseveralimportantdownstreamappli- embeddingfromthefirststep. Thisleadstoacanonical3D
cationssuchasteachingapersontomimicavideo, action poseembeddingthatisinvarianttocameraviewpoints.This
recognition and imitation learning for robotics. The key view-invariantposeembeddingishighlyflexible, allowing
challenge arises from the fact that different camera view- us to do 3D pose retrieval, 3D pose generation, and most
points observing the same 3D pose lead to very different importantly, estimating consistent 3D pose from different
projectionsina2Dimage. Thecommonpracticeistocir- 2Dviewpoints1.
cumventthischallengebyestimating3Dposeinthecamera Inourapproachweuseavariationalautoencoder(VAE)
coordinatespace[7,15,32]. However,thisleadstodiffer- to learn an embedding for 3D human poses. This VAE is
ences in scale and rotation between the estimated 3D rep- trainedtoreconstruct3Dposesandhastwokeybenefits:(a)
resentationsfromimagesofthesame3Dposefromdiffer- we can leverage loss functions to ensure similar 3D poses
ent camera viewpoints. Without the knowledge of camera arecloseintheembeddingspace,and(b)welearnembed-
parameters, it is not possible to establish correspondence dings that can generalize better to unseen 3D poses due to
betweenthese3Drepresentations. Thisisimportantaswe the variational training paradigm. Next, we learn a map-
4202
luJ
9
]VC.sc[
1v29070.7042:viXrapingfrom2Dposes(eitherground-truthorestimatedusing tion can improve 3D pose estimation [3, 31], typically by
off-the-shelfdetectors)tothis3Dposeembeddingspaceby processing a sequence of images using a transformer [31].
traininga2Dposeencoderthatestimatesthe3Dposeem- However,ourfocusis3Dposeestimationusingasingle2D
bedding. Thisembeddingisusedasinputtothepre-trained imagewhichissimilarto[26].
decoder from the VAE to estimate the corresponding 3D The key distinction between our approach and prior
pose, thus leading to “lifting” the 2D pose from different works in estimating 3D poses using 2D images is view-
camera viewpoints to3D [10, 18]. We refer to embedding invariant embedding that can be estimated from a monoc-
asvariatonalview-invariantposeembedding(V-VIPE). ular viewpoint. Several works have attempted to ad-
Our proposed V-VIPE is highly flexible and generaliz- dress view invariant estimation by leveraging many view-
able. We can encode 3D poses and use the embedding for points[14,20,25]becauseitismucheasiertoplaceaper-
downstreamtasks, likeretrievalandclassification. Wecan son in canonical coordinate space when you have access
also map 2D poses from unseen camera viewpoints to this tomanyviews. However, accesstomultipleviewpointsof
embedding. Wecanestimate3Dposesfromtheseembed- the same scene is an unrealistic assumption in the canoni-
dings using the decoder. Finally, we can generate unseen calsettings. Therefore, theseapproachescanonlybeused
3D poses. To the best of our knowledge, V-VIPE is the in environments that have multiple cameras observing the
onlyrepresentationtoofferthisdiversityofapplications. samescene. Incontrast,ourapproachcanbeappliedtoany
We perform an extensive experimental evaluation over arbitrary 2D image. Some works only use a single view-
two datasets: Human 3.6M [8] and MPI-3DHP [16]. We pointduringinferencetime,butstillrequiremultipleviews
show quantitative results on 2D to 3D pose retrieval and foreachposeduringtraining[11]. Whereasourmethodis
qualitativeresultson3Dposegenerationand2Dto3Dpose more flexible and can be trained on any dataset with both
estimation. WeshowthatV-VIPEperforms1%betterthan 2D and 3D information, even if there is only one camera
other embedding methods on seen camera viewpoints and viewpoint available. Similar to our work, [26] performs
about 2.5% better for unseen camera viewpoints. In addi- view-invariant pose estimation from one view, but their
tion,weshowgeneralizationofourapproachbytrainingon method requires localized transformations that fundamen-
onedatasetandtestingontheother. tallychangethe3Dposeandmustbereversedattheendto
Tosummarize,ourmaincontributionsareasfollows: getthefinalpose.Ourapproach,ontheotherhand,requires
• Welearnavariationalview-invariantposeembedding(V- only one global rotation to a canonical camera viewpoint
VIPE)bytrainingaVAEtorepresent3Dposesincanoni- thatdoesnotchangetheintegrityofthepose.
calcoordinatespace,whichallowsittobecamerainvari- 3DPoseGeneration. Trainingamodelcapableofgenerat-
ant. ingnew3Dposesisimportantforrepresentingunseendata
• We propose a model to map from 2D poses to V-VIPE, in addition to training data. There are two main types of
whichenablesustoestimate3Dposesof2Dimages.Ad- generatorsthatcanbeused,GenerationalAdversarialNet-
ditionally,becauseV-VIPEiscamerainvariant,ourmap- works(GANs)andVariationalAutoEncoders(VAEs).
pingcangeneralizetounseencameras. Several works have used GANs [26, 27, 29] to gen-
• We also estimate and generate 3D poses using V-VIPE erate training data for 3D poses. However, they are not
viaadecoderthatcanbeusedfordownstreamtasks. well suited for our task which also requires encoding 3D
Intherestofthepaperweexpandupontheseideas. We poses in an embedding space. VAEs, on the other hand,
summarizetherelatedworksinSection2. InSection3,we are better suited for learning embedding by auto-encoding
describe our proposed method, and Section 4 provides the 3D poses. [17] learns a latent network, where they go di-
experimental evaluations. Section 5 looks at ablations of rectlyfrom2Dto3Dwithoutusingthe3Ddataasinputto
ourmethod. Finally,Section6derivestheconclusions. the model, whereas [22] learns a latent representation us-
ingavariantofVAEandgenerate3Dposesusing2Dpose
2.RelatedWork as a precondition to their decoder. [10] employs a basic
autoencoder insteadof a VAE,which leadsto an inconsis-
Human Pose Estimation. There are two family of ap-
tentembeddingspacethatishardertomapto2Dinputs.[6]
proachesforhumanposeestimation. Oneistodirectlyes-
also learns an autoencoder instead of a VAE, but addition-
timate3Dposesfroman2Dimages[19,24],andtheother
ally,theychoosetoregressontheembeddingandperform
istoliftpre-detected2Dposesto3Dposes[15,23,30]. In
littlenormalizationpriortotrainingwhichleadstoapoorly
recentyears,state-of-the-artapproacheshavealmostexclu-
regularizedoutputspace.
sivelyfocusedontheliftingstrategy.
Ourgoalistospecificallyfindcorrespondencebetween
3.ProposedMethod
2Dposesinimagesfromdifferentcameraviewpointswith-
outanyknowledgeofcameraparametersortemporalcon- Ourmethodconsistsofthreemainparts. In3.1wereview
text. Recent works have explored how temporal informa- the input data pre-processing to ensure that the output isFigure2. Ontheleftwecanseethe3Dposeintheoriginalglobalcoordinateswith4differentcameras. Thenext4imagesarethe3D
posesasseenfromthese4cameras.
independent of camera view. In section 3.2, we describe left hip, right hip and spine respectively and b ,b ,b
1 2 3
how we define V-VIPE through a VAE model. In section equal[[0,−1,0],[0,1,0],[0,0,1]]. Aligningtothesepoints
3.3wecoverhowwelearnV-VIPEfromthedetectedkey- causes the hips to align to the y axis and the spine to the
points. The final model is a network that takes as input a zaxis. Wespecificallyalignthehipsbecausetheyareina
singleframemonocularimageandestimatesaviewinvari- straight line so it is easy to align to one axis and the spine
ant pose, which can be used to compare any two human because it is directly above the root and therefore can be
posesindependentofthecontextoftheoriginalimage. easilyalignedtoaperpendicularaxis. Inordertominimize
Equation1,weusetheKabschalgorithm[9].
3.1.DataProcessing
Scaling and Pose Normalization. In this work, we are
Beforewepassanydatathroughourmodelweperformtwo onlyconcernedwithestimatingposesuchthatitiseasyto
keysteps.First,wemodifytheglobalrotationoftheimage; compare how similar two poses are. This is because pose
second,wescalethekeypointssothattheoriginalsizedoes comparisoniswhatisneededfordownstreamtaskssuchas
notaffectthemodel. action recognition. To account for this, we scale and nor-
Global Rotation Realignment. Predicting 3D pose in malize the input, such that it becomes independent from
canonicalspaceisextraordinarilydifficultasmentionedin factors2 that should not affect the pose similarity estima-
[15]. We believe this is mostly due to the global rotation1 tion.
of any 3D pose. Global rotation is hard to estimate due We use the universal skeleton provided by the dataset
to its ambiguity. We can see in Figure 2 that a pose in to remove the size factor. In this representation all joints
global space can have a very different appearance in cam- arescaledtothesameproportions. Thismakesthesizeof
eraspace. Withoutanyinformation,suchasagroundtruth the3Doutputindependentoftheinputted2Dimageorthe
pose, which we can align the output to or any camera pa- original3Dpose.
rameters,itwouldbedifficulttodeterminethatanytwoof Moreover,tocompletethenormalizationofthedatawe
theseposesarethesame. useaprocesssimilarto [1]wherewecentertherootjoint
Wearguethatglobalrotationisirrelevantforhumanpose andscalealloftheotherjoints,accordingly.
comparison. Specifically,whenwearetryingtodetermine
iftwoposesarethesamewedonotneedtounderstandhow 3.2.3DPoseVAE
thoseareorientedinrelationtotheworldtheyarein. Ifone
Theproposedmodelconsistsoftwoparts,a3DPoseVAE
poseisfacingthex-axisandtheotherisfacingthey-axis,it
Network and a 2D Mapping Network. The 3D Pose VAE
isstillpossiblethattheiroverallposeisthesame. Wethus
Network, Figure4.a, consistsofanencodernetworkanda
removerotationdependencebyaligningthecoordinatesof
decoder network, which make up the VAE model. To stay
the left hip, right hip and the spine to the same points in
consistent with other papers we choose [15] as the back-
everyposeofthedataset.ThiscanbevisualizedinFigure3.
boneforbothourencoderandourdecoder.
Inordertoachievesuchalignmentwefindtherotationthat
The benefit of using a VAE for the 3D Pose VAE Net-
minimizestheequation:
work is its ability to generalize to new poses. This is be-
n causethegoalofaVAEistosynthesizeunseenposes. Al-
1(cid:88)
L(C)= ||a −Cb ||2 (1) though this is not our main goal, we do want our network
2 i i
i=1 topotentiallybeabletorepresentunseenposes,whichisa
where a ,a ,a equal the 3D points representing the realisticsettinginrealworldapplications.
1 2 3
1Byglobalrotationwemeanhowahumanisrotatedinrelationtothe 2Intuitively,twopeoplearecapableofbeinginthesameposenomatter
canonicalspace. theirheightorweight.iftwoposesareclosetogetherin3Dspacebyobservingif
Original Pose Rotated Pose
theyareclosetogetherintheembeddingspace.
We define a distance function, D, which represents the
Mean Per Joint Position Error (MPJPE). MPJPE measures
the distance between two 3D points by taking the L2 dis-
tance between each joint location and then computing the
meanofthosedistancesforalljoints.
Duringtrainingwethusoptimizeforthreefactors:
• A reconstruction loss, which is equivalent to the Mean
SquaredError(MSE)lossbetweenS andSˆ . L =
3D 3D mse
1 (cid:80) (S −Sˆ )2
N 3D 3D
• TheKLDivergencelossL =KL[q(z|S )|p(z)]. This
KL 3D
lossrepresentsthedistancebetweenthedistributionofthe
encoderandthepriordistribution,p(z). Inthisworkwe
useaGaussiandistributionastheprior.
• Thethirdisatripletloss. Tocomputethetripletlosswe
Figure3.Howposeschangewhenwealignthepointsandmodify
first find the 3D distances, D within a batch between
therotation. Ontheleftistheoriginalposeandontherightisthe i,j
all elements. For each pose we then set the closest pose
poseafterwehaverotatedit.
inthebatchtobethepositiveexample(j)andthesecond
Normalizing the rotation, as defined in the step above, closestposetobethenegativeexample(k). Wemakesure
helpstheVAEbyreducingtherangeofvaluesthattheout- the positive and negative poses are at least .1 apart from
put can be. We want the VAE to learn all possible human each other and if they aren’t we select the next closest
poseswithintherangeandbymakingthatrangesmallerwe pose as the negative example. We do this because we
make it easier to learn an embedding that spans the whole want the examples to be hard, but not too hard that they
space. Ifweomittherotationrealignmentthenourembed- introducenoise. Wecomputetripletlossbetweeni,jand
ding space would have to learn not only joint location in kbydoingL triplet = max[0,D i,k−D i,j +m],wherem
relationtoallotherjoints,butalsojointlocationinrelation isourmargin.Thislossisusefulbecauseitcausessimilar
totheglobalspace. Thisisingeneralunnecessaryasloca- posestomoveclosertogetherintheembeddingspace.
tioninglobalspaceisnotrelevantwhencomparingiftwo Thismakestheoveralllossfunctiontotrainthe3DPose
poses are equal. Additionally, learning a normalized rota- VAE:
tion means that the output is all in one space and can be L =L +L +L (2)
V-VIPE mse triplet KL
comparedeasilywithoutadditionalalignments.
3.3.2DMappingNetwork
The3DPoseVAENetworkhastwoparts:(i)anencoder,
which takes as input a 3D pose, S 3D = {s i ∈ R3|i = Once we have trained the 3D Pose VAE Network we uti-
1...N}, where N is the number of keypoints, and out- lize its embedding space to learn a 2D Mapping Network
puts a mean for possible embeddings, µ e ∈ Rn, and a (see Figure 4.b). In particular, we take the 3D Pose VAE
variance for the embedding, σ e ∈ Rn. Using these val- Network decoder model and we freeze it so that it trans-
ues and a Gaussian distribution prior we take a sample, e. latesfromthepre-definedV-VIPEspaceto3Dcoordinates.
We denote the distribution of the latent space modeled by Next,wetrainanewencoderEnc for2Dcoordinates.The
2D
theencoderwithq(e|S 3D);(ii)adecoder,whichtakesinin- new encoder takes in input S
2D
= {p
i
∈ R2|i = 1...N}
putanembedding,e,andoutputsanestimationof3Dpose and outputs a V-VIPE, e ∈ Rn. We pass e through the
Sˆ 3D = {sˆ i ∈ R3|i = 1...N}. The distribution of the frozendecodertogetwhattheembeddingrepresentsin3D
decoderisrepresentedasp(S 3D|e). spaceaccordingtothemodeltrainedinthepreviousphase,
The goal of the 3D Pose VAE Network is to find a V- Sˆ ={p ∈R3|i=1...N}.
3D i
VIPEspacethatisrepresentativeoftheentirerangeof3D To train the 2D Mapping Network we use two losses.
humanposesforaspecificscaleandnormalization. Afea- Given the input, S , the output Sˆ and the ground truth
2D 3D
tureofthe3DPoseVAENetworkshouldbethatposesthat 3D keypoints, S , we compute MSE(S ,Sˆ ). We
3D 3D 3D
areclosetogetherintheoriginal3Dspaceareclosetogether combinethislosswithatripletloss,whichwecomputesim-
in the embedding space. An important part of learning an ilarlyasinSection3.2. Themaindifferenceisthatweuse
accurate mapping from 2D space is that even if there is a the output from the 2D encoder and the ground truth 3D
slighterrorintheV-VIPEestimationtheoutputwillstillbe keypoints. We then back-propagate this loss through the
aposethatissimilartotheoriginal3Dpose. Additionally, whole network, but do not apply the gradient losses to the
definingasmoothspaceforV-VIPEenablesustointerpret decodernetwork. ThisisbecausewedonotwanttochangeData 3D Pose μe 3D Pose
Processing Encoder σe Decoder
(a) 3D Pose VAE
2D
D
K ete ey cp too rint 2 ED
n
cP oo ds ee
r
μe PoC sl eo n De ed
c
3 oD
der
(b) 2D Mapping Network
Figure4. Thenetworkontopisour”3DPoseVAENetwork.”Firstwepassthe3Dinputthroughourdataprocessingphase. Oncewe
havetheoutputwecanpassthatasinputtoourVAEnetwork,whichgeneratesV-VIPEandthenattemptstoreconstructthepose. Onthe
bottomisour”2DMappingNetwork.”2Dkeypointsareextractedusingadetector.Wethenpassthesethroughour2Dencoderandthena
lockedcloneofthedecodernetworkfromthe3DPoseVAENetwork.Thisreconstructstheoriginal3Dpose.
the embedding space, but we just want to train the 2D en- pair of embeddings a hit if their original 3D pose satis-
codertomakeitcompliantwiththelatentspace. fies MPJPE(A(Si ),A(Sj )) < .1. We report Hit@k for
3D 3D
We find that it is beneficial to pre-train the decoder as k=1,10,20andaverageoverallpairsofcameras. Thismet-
described in 3.2 because we want to construct a space for ricrepresentsviewinvariancebecauseitshowshowwellwe
V-VIPEthatissmooth,withoutalsoneedingtolearna2D canmatchposesfromoneviewpointtosimilarposesfrom
to 3D mapping. Because we train our 3D Pose VAE on anotherviewpoint.
normalized3Dposesitwillonlylearnhowtomaptoanor- The second is the Mean Per Joint Position Error
malizedpose.Thereforetheoutputofthe2DMappingNet- (MPJPE),whichwedefineinSection3.2.Thiserrorisused
workisalsonormalized. Thismeanstheoutputisrotation todeterminethedistancebetweentwosetsof3Dkeypoints.
and scale invariant, making it easy to compare 2D poses
4.3.Datasets
fromdifferentcameraviewpoints.
Inalltheexperimentswetrainonthestandardtrainingset
4.ExperimentsandResults
oftheHuman3.6Mdataset(H3.6M)[8]. Forourhitmetric
weusethetestsetofH3.6Masthevalidationsetandshow
4.1.ExperimentalSetup
resultsontheMPI-INF-3DHPdataset[16](3DHP).
The model uses a backbone network described described Human3.6M. The H3.6M dataset [8] contains 3.6 million
in[15]. Westack2blocksofthisnetworktogetherforboth human poses taken from 4 different cameras. All of these
the encoder and the decoder network of both the 3D Pose camerasareatchestlevel. Thestandardtrainingsetforthis
VAE Network and the 2D Mapping Network. We set the datasetismadeupofsubjects1,5,6,7and8. Thestandard
linearsizeto1024, andweusea0.1dropout. Thedimen- testsetcontainsposesfromsubjects9and11. Fortheeval-
sion of a V-VIPE is 32 and the margin for the triplet loss uation of the hit metric, we follow the method described
is 1.0. Any 2D keypoint detector could be used, but we in[23],wheretheyremoveposesthataresimilar.
choseAlphaPose[5,12,13,28]. WeuseCOCOkeypoints MPI-INF-3DHP.3DHP[16]contains14differentcamera
because they are widely used for 2D detectors. We imple- angles. For our tasks we remove the overhead cameras,
mentedthemodelinPyTorchandwetrainediton1GPU. which leaves us with 11 cameras. Of these cameras, 5 are
at chest height and the others have a slight vertical angle.
4.2.Metrics
Thisdatasetisusedtoshowwhetherornotourmethodwill
Weevaluatethemodelusingtwometrics. Thefirstisahit generalizetodatathatisdifferentfromthetrainingdata.
metric, inspired from [26], which we use to measure how
4.4.Augmentation
oftenweareabletoretrieveaposethatissimilartoaquery
pose. GiventwonormalizedkeypointsSi andSj wefirst In order to improve the model’s ability to generalize we
3D 3D
apply a Procrustes alignment [21] between the two to get introduce camera augmentation similar to the work done
A(Si )andA(Sj ). Givenadatasetwithmanyviewswe in[23]. Tocalculatethisaugmentationwetaketheground
3D 3D
select two camera views. We find all embeddings for the truth 3D pose and randomly rotate it. We then project
2Dposesfromtheselectedcameras. Then, wequeryeach thisposeinto2D.Weaddaugmentedposestoeachofour
embeddingfromcamera1andfindtheknearestneighbors batches during training time. We found that it was best to
from the set of embeddings for camera 2. We consider a addaugmentedposesforhalfoftheposesineachbatch.Table 1. Hit metric results for different values of k. The upper part of the table shows the Hit metrics when using ground truth (GT)
keypoints. Thebottompartofthetableshowsthemetricswhenusingkeypointdetection(D)andaugmentation(A).ForPr-VIPEandour
methodAlphaPoseisthekeypointdetector.EpipolarPoseusesitsowndetector.The∗versionofEpipolarPoseistrainedontheHuman3.6
datasetandthe#versionistrainedonthe3DHPDataset.Epipolarposedoesnotgeneralizetounseendatasets.
Dataset→ H3.6M 3DHP(All) 3DHP(Unseen)
k→ 1 10 20 1 10 20 1 10 20
PR-VIPE(GT) 97.6 99.9 100.0 42.6 72.8 79.1 43.7 73.2 82.0
Ours(GT) 89.7 98.8 99.4 45.3 76.2 83.1 47.9 77.9 84.5
2Dkeypoints 28.7 47.1 50.9 9.80 21.6 25.5 - - -
EpipolarPose∗ 69.0 89.7 92.7 - - - - - -
EpipolarPose# - - - 24.6 53.2 61.3 - - -
PR-VIPE(D) 72.1 94.3 96.8 17.9 44.7 64.1 19.2 46.6 55.6
PR-VIPE(D+A) 70.9 93.1 96.0 25.4 55.6 64.1 27.8 57.7 65.8
Ours(D) 70.0 92.7 95.6 23.5 54.3 64.0 26.2 57.0 66.4
Ours(D+A) 69.0 93.5 96.3 26.9 59.0 68.2 30.1 61.6 70.3
4.5.QuantitativeResults training set, 3DHP, as well as to unseen cameras. For ex-
ample,whenk =10ourmethodoutperformsPR-VIPEby
Similar Pose Retrieval Experiments. We compare our
about 5.6 percent for all 3DHP cameras, and by about 7
model for hit metrics against 3 baselines. The first base-
percentfortheunseencategory.
line is the PR-VIPE model, which attempts to define an
In this section we also show results for detected key-
embedding space without reconstructing the 3D pose; we
points plus additional training data generated by augment-
adopted their open source code and re-trained their model
ingthe3Dposes.Weseeanincreasefromjustourdetection
sowewouldhaveresultsonthesame2Dposedetector,i.e.,
model for 3DHP because we have introduced new camera
AlphaPose. Thesecondbaselineissimplyfindingthenear-
viewpoints to the training data. We see an improvement
estneighborofthedetected2Dkeypoints. Thethirdbase-
overPR-VIPEwhentheyuseaugmenteddata,althoughwe
lineusesEpipolarPose[11]todetect3Dkeypoints. Inthis
do not get as much of a boost from augmentation because
case, Procrustes alignment is performed between all poses
ourmodelalreadygeneralizesbetterthantheirs. Fork =1
andtheclosestalignedposeisselectedasthematch.
ourmodeloutperformstheirsby1.5percent.
We show the hit metrics for the different k values in
Table 1. The top section of the table shows the results of Additionally,inthetablewereportthe2Dkeypointsand
our method and of PR-VIPE when trained and tested with Epipolar Pose results. We can observe that using the 2D
ground truth (GT) 3D keypoints. The left part of the table keypoints is not effective, as demonstrated by the low hit
reportstheresultsonthetestsetofH3.6M.Wecanseethat metric for all k values. The Epipolar Pose# method per-
ourapproachisslightlyworsethanthePR-VIPEapproach. formsbetterthanbothourmethodandthePR-VIPEmethod
This is because we are testing on very similar data to the before any augmentation is applied to the data because it
original training set. Our model, however, is designed to is trained on the 3DHP dataset and does not need to gen-
generalize.Thegeneralizationofthemodelisdemonstrated eralize. When you try to run the Epipolar Pose∗ model
inthemiddlepartofthetable,wherewereporttheperfor- on 3DHP data the output does not resemble human pose.
manceonthe3DHPdatasetwhenconsideringallavailable We do not report generalized results for Epipolar pose be-
cameras. In this case, our model gets higher values for all
causeofthis.DespitethefactthatEpipolarPose#istrained
valuesofk. Moreover,whenwepaironechestcamerawith specificallyfordetectiononthe3DHPdatasetwhenweadd
a camera that is not at chest height, i.e., unseen cameras augmentation of the data to our model we are able to beat
withrespecttothetrainingdata(rightpartofthetable),we theirresultsbyabout2percent.
canseethatthegapisevenlarger. Forexample,whencon- 3DPoseEstimationExperiments. Inadditiontocalculat-
sidering k = 1, the gap between the two models is about ing the hit metric described above our model also outputs
4.2percentforunseencamerasand2.7percentforallcam- thepredicted3Dpose. Wefindthattheaverageerrorofthis
eras. This demonstrates that the latent space we acquired model is 62.1 millimeters. We calculated this number us-
duringtheVAEtrainingisabletogeneralizetounseencam- ingamodeltrainedonkeypointsdetectedbytheCascaded
eraviewpointsbetterthanexistingmodels. PyramidNetwork[2]asthisiscommonly[4,19,26]used
Inthebottomsectionofthetable,weshowresultswhen for3DPoseEstimation. Wefindthatwhilethisnumberis
the keypoints are automatically detected(D). For PR-VIPE not competitive with current methods for pose estimation
and our model we use AlphaPose. Epipolar Pose detects thatusemorecomplexmodelsortakeinmoreinformation,
itsownkeypoints. AgainourmethodoutperformsthePR- such as sequences, it is similar to the error found in [15],
VIPE model when generalizing to data different from the whichweuseasthebackboneforournetwork.Figure5. PoseEstimationfrom2Dimagesofourmodelappliedtodifferentcameraviewpoints. Weshow4setsofresults. Theground
truthisonthelefthandsideofeachexample,whileontherightweprovidethe4originalviewsaswellasourmodel3Doutputforeach
view.
0.0 0.1 0.11 0.26 MPJPE
Figure6.Thisfiguredemonstrateswhataqueryandretrievallooklike.Ontheleftofeachpairofimagesisthequeryposeandtheimage
ontherightistheimagethatisconsideredtheclosestmatchbyourmodel. EachpairofimagesislabeledwiththeMPJPEbetweenthe
twoposes.Itseasytoseethatsomeposes,suchastheoneonthefarleft,areeasytoretrievebecausetheyaresodistinct.Andothers,such
astheoneonthefarright,haveoccludedpointsaswellasotherfactorsthatmakethenearestneighborhardtofind.
4.6.QualitativeResults theretrievedposethenwehaveahit.
2D to 3D Pose Estimation. Figure 5 shows examples of
our 3D estimations given a 2D image as input. We show Visualizing V-VIPE. Figure 8 shows a t-SNE visualiza-
examplesof4differentposeseachwith4differentcamera tion, which we use to show the smoothness of the learned
angles. In the two examples on the left we have very ac- V-VIPE space, where each dot represents a V-VIPE. In
curateretrievals. Allofthecamerashavesimilarretrievals ordertoproperlyshowtheclusteringweselect10visually
thatallowustodeterminethatthepersonisinthesamepose different 3D poses and color our visualization based on
despitetheverydifferentoriginalcameraangles.Theexam- which of the 10 poses is the most similar to the pose
plesontherightaretheoneswhereourmodelstrugglesto that each point represents. It is easy to see from this
findthewholepose. Intheexampleonthetopweareable graph that similar colors are typically found in clusters.
to find the hand position because the hands are visible in This means that the space well represents the notion of
everyimage,howeverourmodelstrugglestodetectthatthe similarity between poses. We can see this even clearer in
bodyisslightlyangled.Thisislikelybecausethedifference the expansion of the visualization where we show three
in 2D keypoints between an angled and not angled body poses and their locations in the cluster. The two poses on
areverysmallandour2Dkeypointdetectorisnotaccurate the right are colored the same and are very close together.
enough. Intheexampleonthebottomourmodelsucceeds These are slightly different, but the overall pose is very
with the arms, except for one camera viewpoint where the similar. We then select a point that is very far away and
arm is not visible in the image at all. The other way our herewecanseethattheposeisquitedifferent.
modelstrugglesiswiththeheadtilt. Thisislikelybecause
thisisdifficulttovisualizefrommostcameraangles.
3D Pose Retrieval. We show how our model is able to 3D Pose Generation. Our model is able to generate new
retrievesimilarposesfromdifferentviewpoints. InFigure posesbyaddingnoisetotheembeddingspaceofanexisting
6 you can see the query pose as well as the pose that is pose. InFigure7wedefineanoisearrayz andaddittoan
retrievedfromadifferentviewpoint. Ideally,thetwoposes embeddingwithincreasingmagnitudes.Theposecontinues
will be identical. This is the visualization of what the Hit tomoveinonedirectionasweincreasemagnitudeshowing
metricrepresents. Ifthequeriedposeissufficientlycloseto thatourembeddingspaceissmooth.Figure7.Posegeneration,startingfroma3Dposeweselecttworandomnoisedirectionsz andgenerateposesusingincreasingmagnitudes
i
ofnoiseαz ,whereα∈{0.2,0.3,0.4,0.5}.V-VIPEleadstosmoothposevariationsandcanbeusedtogenerateunseen3Dposes.
i
18.0percent,a5.5percentdeclinefromourapproach.
Pretraining the Decoder. Finally, we studied whether or
notpretrainingaVAEandusingadefinedembeddingspace
contributedtoourfinalhitmetric. WefoundthattheHit@1
value for the model with no pretraining is 23.4 versus the
23.5weobtainedbycompletingthepretrainingstep. How-
ever, this step is important anyways because it enables the
modeltodo3DPoseRetrieval. Withoutitwewouldnotbe
abletomapour3Dposestoourembeddingspace. There-
fore we would not be able to generate similar poses to a
given3Dposeorquerya3Dposetofindasimilar2Dpose
fromasetofimages.
6.Conclusion
Inthisworkweshowedthatbyusingonly3Dposestode-
fineaV-VIPEspacewecandefineabettercamerainvariant
spacethanifweweretoonlyuse2Dposes. Wedefineda
Figure8. t-SNEvisualizationoftheV-VIPEspaceofourmodel
proceduremadeoftwosteps: firstwetrainaVAEmodelto
forposesintheH3.6Mdataset. Eachcolorrepresentssimilarity
learn a latent space of 3D poses; then, we train a 2D key-
tooneof10“key”posesthatweselected. Intheexpansion,three
pointsencoderthatislinkedtotheVAEdecodertoallow3D
differentposesandtheirplaceinthevisualizationareshown.
reconstructionsof2Dimages. WeadoptedaVAEmodelas
itcreatesasmoothlatentspacethatcangeneralizebetterto-
5.AblationStudy
wardsunseenposesduringtraining. Inordertoachievethis
goal,wetrainaVAEwithathreecomponentlossfunction.
We performed an ablative analysis in order to understand
Weperformedanextensiveexperimentalevaluation,byus-
whichofourdesignchoicesbestcontributedtoourresults.
ingtwodatasets,i.e.,Human3.6MandMPI-INF-3DHP.We
TripletLoss. Firstweexaminehowimportantitisthatwe demonstratedthatthelatentspaceismodelingameaningful
include the triplet loss term in our method. We remove it notionofsimilarityoftheembeddings. Thisisreflectedin
from the loss term and find that the new Hit@1 value is thePoseRetrievalexperimentswhereweimproveabout2.5
17.41 with no augmented data. This is a drop of 6.1 from percentintheHit@1metricwhenconsideringunseencam-
theHit@1valuewhentripletlossisincluded.Thereforethe eras. We also showed qualitative examples demonstrating
tripletlossvalueisimportanttotheoveralllossterm. the capability of our embedding space to capture the no-
tionofsimilarityofposes. Thisisimportantindownstream
DataProcessing. Weexaminehowimportantitisforusto
tasks.Inthefuturewebelievethatthisapproachhasalotof
rotatethe3Dposebeforetrainingonourmodel.Thisstepis
promiseforapplicationtodownstreamtaskssuchasaction
importantbecauseitenablesustocomparethesimilarityof
segmentationanddetection.
poseswithtwodifferentglobalrotationswithoutneedingto Acknowledgements: This work was partially sup-
do a time consuming Procrustes Alignment between every ported by NSF CAREER Award (#2238769) to AS
pairofposes. WefindthattheHit@1valueon3DHPwith and the DARPA SAIL-ON (W911NF2020009) pro-
noaugmentationobtainedwhenusingnonrotatedpointsis gram.References manposeestimation. CoRR,abs/1705.03098,2017. 1,2,3,
5,6
[1] Ching-Hang Chen, Ambrish Tyagi, Amit Agrawal, Dy-
[16] Dushyant Mehta, Helge Rhodin, Dan Casas, Pascal
lan Drover, M. V. Rohith, Stefan Stojanov, and James M.
Fua, Oleksandr Sotnychenko, Weipeng Xu, and Christian
Rehg.Unsupervised3dposeestimationwithgeometricself-
Theobalt. Monocular3dhumanposeestimationinthewild
supervision. CoRR,abs/1904.04812,2019. 3
usingimprovedcnnsupervision. In3DVision(3DV),2017
[2] Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang
FifthInternationalConferenceon.IEEE,2017. 2,5
Zhang,GangYu,andJianSun. Cascadedpyramidnetwork
[17] Aditya Panda and Dipti Prasad Mukherjee. Monocular 3d
for multi-person pose estimation. CoRR, abs/1711.07319,
human pose estimation by multiple hypothesis prediction
2017. 6
and joint angle supervision. In 2021 IEEE International
[3] YuCheng,BoYang,BoWang,andRobbyT.Tan.3dhuman
ConferenceonImageProcessing(ICIP),pages3243–3247,
poseestimationusingspatio-temporalnetworkswithexplicit
2021. 2
occlusiontraining. CoRR,abs/2004.11822,2020. 2
[18] SungheonPark,JihyeHwang,andNojunKwak. 3dhuman
[4] Haoshu Fang, Yuanlu Xu, Wenguan Wang, Xiaobai Liu,
poseestimationusingconvolutionalneuralnetworkswith2d
and Song-Chun Zhu. Learning knowledge-guided pose
poseinformation. CoRR,abs/1608.03075,2016. 2
grammar machine for 3d human pose estimation. CoRR,
[19] Georgios Pavlakos, Xiaowei Zhou, Konstantinos G. Der-
abs/1710.06513,2017. 6
panis, and Kostas Daniilidis. Coarse-to-fine volumet-
[5] Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu.
ric prediction for single-image 3d human pose. CoRR,
RMPE: Regional multi-person pose estimation. In ICCV,
abs/1611.07828,2016. 2,6
2017. 5
[20] EdoardoRemelli,ShangchenHan,SinaHonari,PascalFua,
[6] RohitGirdhar,DavidF.Fouhey,MikelRodriguez,andAb-
and Robert Wang. Lightweight multi-view 3d pose esti-
hinav Gupta. Learning a predictable and generative vector
mationthroughcamera-disentangledrepresentation. CoRR,
representationforobjects. CoRR,abs/1603.08637,2016. 2
abs/2004.02186,2020. 2
[7] Mir Rayat Imtiaz Hossain and James J. Little. Exploit-
[21] PeterScho¨nemann.Ageneralizedsolutionoftheorthogonal
ing temporal information for 3d pose estimation. CoRR,
procrustesproblem. Psychometrika,31(1):1–10,1966. 5
abs/1711.08585,2017. 1
[22] Saurabh Sharma, Pavan Teja Varigonda, Prashast Bindal,
[8] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian
Abhishek Sharma, and Arjun Jain. Monocular 3d human
Sminchisescu.Human3.6m:Largescaledatasetsandpredic-
pose estimation by generation and ordinal ranking. CoRR,
tivemethodsfor3dhumansensinginnaturalenvironments.
abs/1904.01324,2019. 2
IEEETransactionsonPatternAnalysisandMachineIntelli-
[23] Jennifer J. Sun, Jiaping Zhao, Liang-Chieh Chen, Florian
gence,2014. 2,5
Schroff,HartwigAdam,andTingLiu. View-invariantprob-
[9] W.Kabsch. Adiscussionofthesolutionforthebestrotation
abilisticembeddingforhumanpose.CoRR,abs/1912.01001,
torelatetwosetsofvectors. ActaCrystallographicaSection
2019. 2,5
A,34(5):827–828,1978. 3
[24] XiaoSun,BinXiao,ShuangLiang,andYichenWei.Integral
[10] IsinsuKatircioglu,BugraTekin,MathieuSalzmann,Vincent
humanposeregression. CoRR,abs/1711.08229,2017. 2
Lepetit, andPascalV.Fua. Learninglatentrepresentations
of3dhumanposewithdeepneuralnetworks. International [25] TaoWang,JianfengZhang,YujunCai,ShuichengYan,and
JournalofComputerVision,126:1326–1341,2018. 2 JiashiFeng. Directmulti-viewmulti-person3dposeestima-
tion. CoRR,abs/2111.04076,2021. 2
[11] MuhammedKocabas,SalihKaragoz,andEmreAkbas.Self-
supervisedlearningof3dhumanposeusingmulti-viewge- [26] Guoqiang Wei, Cuiling Lan, Wenjun Zeng, and Zhibo
ometry. CoRR,abs/1903.02330,2019. 2,6 Chen. View invariant 3d human pose estimation. CoRR,
abs/1901.10841,2019. 2,5,6
[12] Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu
Fang,andCewuLu. Crowdpose: Efficientcrowdedscenes [27] HailunXiaandMengXiao. 3dhumanposeestimationwith
pose estimation and a new benchmark. In Proceedings of generative adversarial networks. IEEE Access, 8:206198–
the IEEE/CVF conference on computer vision and pattern 206206,2020. 2
recognition,pages10863–10872,2019. 5 [28] YuliangXiu,JiefengLi,HaoyuWang,YinghongFang,and
[13] JiefengLi,ChaoXu,ZhicunChen,SiyuanBian,LixinYang, Cewu Lu. Pose Flow: Efficient online pose tracking. In
and Cewu Lu. Hybrik: A hybrid analytical-neural inverse BMVC,2018. 5
kinematicssolutionfor3dhumanposeandshapeestimation. [29] Wei Yang, Wanli Ouyang, Xiaolong Wang, Jimmy S. J.
In Proceedings of the IEEE/CVF Conference on Computer Ren, Hongsheng Li, and Xiaogang Wang. 3d human pose
VisionandPatternRecognition,pages3383–3393,2021. 5 estimation in the wild by adversarial learning. CoRR,
[14] HaoyuMa,LiangjianChen,DeyingKong,ZheWang,Xing- abs/1803.09722,2018. 2
wei Liu, Hao Tang, Xiangyi Yan, Yusheng Xie, Shih-Yao [30] AilingZeng,XiaoSun,FuyangHuang,MinhaoLiu,Qiang
Lin, and Xiaohui Xie. Transfusion: Cross-view fusion Xu,andStephenLin. Srnet: Improvinggeneralizationin3d
with transformer for 3d human pose estimation. CoRR, humanposeestimationwithasplit-and-recombineapproach.
abs/2110.09554,2021. 2 CoRR,abs/2007.09389,2020. 2
[15] Julieta Martinez, Rayat Hossain, Javier Romero, and [31] Ce Zheng, Sijie Zhu, Mat´ıas Mendieta, Taojiannan Yang,
James J. Little. A simple yet effective baseline for 3d hu- Chen Chen, and Zhengming Ding. 3d human pose es-timation with spatial and temporal transformers. CoRR,
abs/2103.10455,2021. 2
[32] XingyiZhou,QixingHuang,XiaoSun,XiangyangXue,and
YichenWei. Weakly-supervisedtransferfor3dhumanpose
estimationinthewild. CoRR,abs/1704.02447,2017. 1