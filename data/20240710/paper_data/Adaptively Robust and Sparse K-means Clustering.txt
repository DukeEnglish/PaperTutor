Adaptively Robust and Sparse
K-means Clustering*
Hao Li1, Shonosuke Sugasawa2 and Shota Katayama2
1GraduateSchoolofEconomics,KeioUniversity
2FacultyofEconomics,KeioUniversity
Abstract
While K-means is known to be a standard clustering algorithm, it may be compromised
duetothepresenceofoutliersandhigh-dimensionalnoisyvariables. Thispaperproposes
adaptively robust and sparse K-means clustering (ARSK) to address these practical lim-
itations of the standard K-means algorithm. We introduce a redundant error component
for each observation for robustness, and this additional parameter is penalized using a
group sparse penalty. To accommodate the impact of high-dimensional noisy variables,
theobjectivefunctionismodifiedbyincorporatingweightsandimplementingapenaltyto
control the sparsity of the weight vector. The tuning parameters to control the robustness
andsparsityareselectedbyGapstatistics. Throughsimulationexperimentsandrealdata
analysis,wedemonstratethesuperiorityoftheproposedmethodtoexistingalgorithmsin
identifyingclusterswithoutoutliersandinformativevariablessimultaneously.
Keywords: Clustering;K-means;Variableselection;Gapstatistic;Robuststatistics
*Thisversion: July10,2024
1
4202
luJ
9
]OC.tats[
1v54960.7042:viXra1 Introduction
Identifying clustering structures is recognized as an important task in pursuing potential
heterogeneityindatasets. Whilethereareavarietyofclusteringmethods,K-meansclus-
tering(Forgy,1965)isthemoststandardclusteringalgorithmandiswidelyusedinmany
scientific applications. However, real-world datasets present significant difficulties, such
asthepresenceofoutliersandhigh-dimensionalnoisycharacteristics. Regardingtheout-
lier issues, there are some attempts to robustify the standard K-means such as trimmed
K-means(Cuesta-Albertosetal.,1997)androbustK-means(Klochkovetal.,2021). On
the other hand, existence of high-dimensional noisy characteristics is typically addressed
through clustering approaches that focus on sparsity for variable selection. In particular,
WittenandTibshirani(2010)proposedalasso-typepenaltytothevariableweightsincor-
poratedintheclusteringobjectivefunction. However,theefficacyofthisalgorithmmight
be lost if the dataset contains a significant number of outliers. While there are several
methodstoaddresseitheroftwoaspects(existenceofoutliersandnoisyvariables),prac-
tically useful methods to address two aspects simultaneously are scarce. One exception
is the method proposed by Kondo et al. (2016). However, this approach requires the as-
sumption of a trimming level in advance. In contrast, Brodinova´ et al. (2019) Introduces
an approach beginning with eliminating outliers identified by the model. However, some
helpful information can get thrown out with the outliers. Moreover, the model learns to
handle unusual data better by adjusting outliers instead of removing them. This helps
make it more robust with new data. This is particularly crucial when dealing with real-
worlddata.
Apart from clustering methods based on objective functions, probabilistic clustering
usingmixturemodelsisalsopopular. ItistypicallydonebyfittingmultivariateGaussian
mixtures. While several approaches have been proposed for stable estimation and clus-
tering under existence of outliers (e.g. Coretto and Hennig, 2016; Fujisawa and Eguchi,
2006; Punzo and McNicholas, 2016; Sugasawa and Kobayashi, 2022; Yang et al., 2012),
2such approach suffers from a huge number of parameters (especially for modeling co-
variance matrix) to be estimated when the dimension is large. Therefore, these methods
wouldnotbeareasonablesolutionwhenourprimaryfocusisonclustering.
To solve the difficulty of a series of sparse trimming-cluster and probabilistic-cluster
approaches,weproposeanewapproachbasedonregularizationtechniquesforsparsityas
well as robustness, called adaptively robust and sparse K-means (ARSK). We introduce
an error component for each observation as an additional parameter to absorb the unde-
sirable effects caused by outliers, making the clustering algorithm robust. We employ a
group lasso penalty and a group SCAD penalty to estimate these parameters during clus-
tering steps. Meanwhile, to reduce the data’s dimension, we also introduce a lasso-type
penalty or SCAD penalty to weights for each variable to introduce sparsity and eliminate
noise variables irrelevant to clustering. We then develop an efficient computation algo-
rithm to minimize the proposed objective function. While the proposed method has two
tuningparameterscontrollingrobustness(thenumberofoutlierstobedetected)andspar-
sity (the level of the sparsity of the variable weight), we adopt a modified version of the
Gap statistics (Tibshirani et al., 2001) to determine the optimal values of them. Hence,
the level of robustness and sparsity is adaptively determined in a fully data-dependent
manner.
The paper is organized as follows. Section 2 reviews the K-means algorithm and
its extended model introduces the proposed method ARSK and presents its details and
pseudocode. Section 3 also presents the selection approach of tuning parameters and
its pseudocode. Section 4 discusses ARSK’s robustness, while Section 5 presents the
tuning parameter selection result. The two algorithms are applied to various scenarios
of artificial datasets and several real-world datasets. This paper ends in Section 5 with
concluding remarks. R code implementing the proposed method is available at GitHub
repository(https://github.com/lee1995hao/ARSK).
32 Robust and Sparse K-means Clustering
2.1 ConventionalK-meansalgorithm
Suppose that we observe a p-dimensional vector, x = (x ,...,x ), for i = 1,...,n,
i· i1 ip
and that we are interested in clustering x . Given the number of clusters, K, the conven-
i·
tional K-means algorithm is to find the optimal clustering by minimizing the following
objectivefunction:
K
(cid:88) 1 (cid:88)
min (x −x )2, (1)
i· i′·
c1,...,cK n k
k=1 i,i′∈c
k
where ∥x − x ∥ is the L -distance between two observed vectors. Here c is a set of
i· i′· 2 k
indices representinga cluster ofobservations andn is thesize ofc , such that∪K c =
k k k=1 k
{1,...,n} and (cid:80)K n = n, where c ∩c = ϕ for k ̸= k′. The objective function (1)
k=1 k k k′
canbeeasilyoptimizedbyiterativelyupdatingclustermeansandassignment(e.g.Lloyd,
1982).
AsdemonstratedinWittenandTibshirani(2010),theobjectivefunction(1)canbere-
formulatedintermsofthebetween-clustersumofsquares,givenbymax
(cid:80)p
Q (x ;Θ),
c1,..,c k j=1 j ·j
where
n n K
1 (cid:88)(cid:88) (cid:88) 1 (cid:88)
Q (x ;Θ) = (x −x )2 − (x −x )2, (2)
j ·j ij i′j ij i′j
n n
k
i=1 i′=1 k=1 i,i′∈c
k
with x = (x ,...,x ) and Θ being a collection of c ,...,c . Witten and Tib-
·j 1j nj 1 K
shirani (2010) employed the modified version of the objective function (2), given by
(cid:80)p
ω Q (x ;Θ) with an appropriate constrained on ω , for conducting variable se-
j=1 j j ·j j
lectionandclusteringsimultaneously.
2.2 RobustandsparseK-meansalgorithm
The objective function (2) is sensitive to outliers since the approach relies on the L
2
distance between each observation. As a result, even a small quantity of deviating obser-
vations can affect the K-means algorithm. In order to robustify the K-means approach,
4wemodifythefunctionQ (x ;Θ)asfollows:
j ·j
n n
1 (cid:88)(cid:88)
QR(x ;Θ,E) = {(x −E )−(x −E )}2
j ·j n ij ij i′j i′j
i=1 i′=1
(3)
K
(cid:88) 1 (cid:88)
− {(x −E )−(x −E )}2,
ij ij i′j i′j
n
k
k=1 i,i′∈c
k
where E is an n × p error matrix, which is a collection of E and E is an additional
ij ij
location parameter for each x such that E will be non-zero values for outlying obser-
ij ij
vationsx topreventanundesirableeffectsfromoutliers. Whilethenumberofelements
ij
of E is the same as the number of observations, it can be assumed that E is sparse in the
sense that most elements of E are 0. That is, most elements are not outliers. To stably
estimate E, we consider penalized estimation for E when optimizing the function (3).
ij
Such approaches are used in the robust fitting of regression models (e.g. She and Owen,
2011;KatayamaandFujisawa,2017).
Basedontherobustobjectivefunction(3),weproposeanobjectivefunctionforrobust
andsparseK-meansalgorithmasfollows:
p n p
(cid:88) (cid:88) (cid:88) 1
L(Θ,E,W;λ) = w QR(x ;Θ,E )− P (∥E ∥ ;λ )− (P (w ;λ )+ w2),
j j ·j ·j 1 i· 2 1 2 j 2 2 j
j=1 i=1 j=1
(4)
whereP andP aregrouppenaltyfunctionsandpenaltyfunctionsforsparse(shrinkage)
1 2
estimation of E and w , respectively, and λ = (λ ,λ ) is a set of tuning parameters. To
i· j 1 2
find a unique solution for the weight coefficients w , we add a quadratic term 1 (cid:80)p w2
j 2 j=1 j
to the loss function. Then, the robust and sparse K-means clustering can be defined as Θ(cid:98)
suchthat
(Θ(cid:98),E(cid:98),W(cid:99)) = argmax L(Θ,E,W;λ)
Θ,E,W∈H
 (cid:118) 
(cid:117) p
H =  w ∈ R+ : (cid:117) (cid:116)(cid:88) w2 = 1 .
j j
 
j=1
Inwhatfollows,weemploytwospecificformsofgrouppenaltyfunctionsP (∥E ∥ ;λ ):
1 i· 2 1
5onetypeistheconvexgrouppenaltyfunction,whichisthegrouplassopenaltyasλ ∥E ∥
1 i· 2
(Yuan and Lin, 2006), and the other type is the nonconvex group penalty function, which
isthegroupSCADpenalty(Huangetal.,2012)as

   λ 1∥E i·∥ 2 if∥E i·∥ 2 ≤ λ 1;

 (cid:16) (cid:17)
P λS 1CAD(∥E i·∥ 2) = − ∥Ei·∥2 2−2 2a (aλ −1∥ 1E )i·∥2+λ2 1 ifλ
1
< ∥E i·∥
2
≤ aλ 1;


   (a+1)λ2 1 if∥E ∥ > aλ .
2 i· 2 1
WealsoconsiderbothconvexandnonconvexpenaltiesforthepenaltytermP (w ;λ ):
2 j 2
theconvexstandardlassopenaltyP (w ;λ ) = λ |w |and,thenon-convexSCADpenalty
2 j 2 2 j
expressedasP (w ;λ ) = PSCAD(w ),wherePSCAD(w )defineas:
2 j 2 λ2 j λ2 j

 

λ 2|w j| if |w j| ≤ λ 2;

 (cid:16) (cid:17)
P λS 2CAD(w j) = − |wj|2− 22 (a aλ −2 1|w )j|+λ2 2 ifλ
2
< |w j| ≤ aλ 2;


   (a+1)λ2 2 if |w | > aλ .
2 j 2
WhenapplyingtheL orSCADpenalties,weseektheoptimalweightvectorofW =
1
(cid:113)
(w ,...,w ), which is normalized to have a unit L2-norm, i.e.,
(cid:80)p
w2 = 1. This
1 p j=1 j
approachhelpstopromoteamorebalancedsolution(ZouandHastie,2010).
2.3 Optimizationalgorithm
Theproposedobjectivefunction(4)canbeoptimizedbyiterativelyupdatingeachparam-
eter. Regarding the updating of the clustering assignment Θ, we note that the maximiza-
tionofL(Θ,E,W;λ)withrespecttoΘundergivenE andW isequivalenttominimizing
p K
(cid:88) (cid:88) 1 (cid:88) (cid:110) (cid:111)2
L(Θ|E,W;λ) = w (x −E )−(x −E ) .
j ij ij i′j i′j
n
k
j=1 k=1 i,i′∈c
k
The optimal solution to this equation is achieved by assigning each adjusted observation
pointtothesamplemeanoftheadjustedobservationwiththesmallestsquaredEuclidean
6distance(e.g.Lloyd,1982),givenby
p K
(cid:88)(cid:88)(cid:88) (cid:110) 1 (cid:88) (cid:111)2
argmin w x −E − (x −E ) , (5)
Θ j ij ij n ij ij
k
j=1 k=1 i∈c k i∈c k
whichcanbeeasilycomputedbyusingthestandardK-meansalgorithm.
Given clustering assignment Θ and variable weight W, the error matrix E can be
updatedas
(cid:40) (cid:41)
p K n
argmin
1 (cid:88)
w
(cid:88)(cid:88)(cid:0)
x −E∗ −µ∗
(cid:1)2 +(cid:88)
P (∥E ∥ ;λ ) , (6)
E 2 j ij ij kj 1 i· 2 1
j=1 k=1 i∈c k i=1
whereµ∗ = n−1(cid:80) (x −E∗)andE∗ istheoptimizedvalueofE fromtheprevious
kj k i∈k ij ij ij ij
iteration.
Aspreviouslydiscussed,thevariableE playsacrucialroleinrobustness. Ifanobser-
vation does not fall into any cluster, the presence of E will lead to the assumption that
i·
this observation pertains to a particular cluster. The penalty P (∥E ∥ ;λ ) controls the
1 i· 2 1
robustness of the model. We also note that under λ → ∞, all E equal a p-dimensional
1 i·
zero vector. As a result, the loss function (6) will be equivalent to traditional K-means
clustering.
Intheminimizationproblem(6),aspreviouslymentioned,weemploythegrouplasso
penalty and group SCAD penalty (Tibshirani, 1996; Antoniadis and Fan, 2001). The
optimizersareobtainedbyapplyingthethresholdingfunctioncorrespondingtothegroup
penaltytox −µ . Thesetwotypesofgrouppenaltyfunctionscorrespondtotwodistinct
i· k·
categories of thresholding functions: the group lasso penalty function corresponds to the
multivariatesoft-thresholdoperator,whereE canbecalculatedthrough
i·
(cid:18) (cid:19)
λ
1
E = (x −µ )max 0,1− (7)
i· i· k·
∥x −µ ∥
i· k· 2
asproposedbyWitten(2013),whilethegroupSCADpenaltyfunctioncorrespondstothe
multivariate SCAD-threshold operator introduced by Huang et al. (2012), which E can
i·
7becalculatedas:


  S(z;λ 1), if∥z∥ 2 ≤ 2λ 1,



E i· = a a− −1 2S(z; aa −λ1 1), if2λ 1 < ∥z∥ 2 ≤ aλ 1, (8)




 z, if∥z∥
2
> aλ 1.
where z = x − µ and S(z;λ) represents multivariate soft-threshold operator. In this
i· k·
paper,wesetaasequalto3.7,asrecommendedinFanandLi(2001).
In addition to the previously mentioned multivariate soft-threshold operator and mul-
tivariate SCAD-threshold operator, The application of the aforementioned methods can
alsobeextendedtothegroupMCPpenaltyassociatedwiththemultivariateMCP-threshold
operator. However,duetoitssimilaritytotheSCADpenalty,weexclusivelyemployedthe
SCAD penalty. Moreover, we suggest avoiding the application of the group hard penalty
as optimizing it is inherently difficult. Furthermore, during our simulation process, we
discoveredthatitiseasytoconvergetolocalminima.
AsdiscussedbyWitten(2013)andSheandOwen(2011),thisformulationestablishes
a connection between the framework of robust M-estimation and the proposed model (6)
withclusteringcenterµ∗ foreachk heldfixed.
kj
As described above, the matrix of errors acquired from the algorithm is weighted
based on the current weights. Therefore, it is necessary to restore the weighted error
matrix E to its original unweighted state by dividing the current weight before we go
to the next step to calculate the new weight for each variable (if the weight of a certain
variableiszero,thenE willdivideoneinsteadofzero).
·j
Finally, given E and Θ, the minimization of L(Θ,E,W;λ) with respect to W is
equivalentto
(cid:40) (cid:41)
p p
(cid:88) (cid:88) 1
argmax w QR(x ;Θ,E )− (P (w ;λ )+ w2) , (9)
W∈H j j ·j ·j 2 j 2 2 j
j=1 j=1
8(cid:110) (cid:113) (cid:111)
where H = w ∈ R+ : (cid:80)p w2 = 1 . We show in the Appendix that the final solu-
j j=1 j
tionto(9)canbedonatedas:
S(QR(x ;Θ,E );λ )
j ·j ·j 2
w =
j (cid:113)
(cid:80)p
(S(QR(x ;Θ,E );λ ))2
j=1 j ·j ·j 2
When applying the P (w ;λ ) = λ |w | penalty in (9), the superscript S denotes the
2 j 2 2 j
soft-thresholdingoperatoras:

   x−λ 2, ifx > λ 2


S(x;λ 2) = 0, if|x| ≤ λ
2



  x+λ , ifx < −λ
2 2
WhenapplyingtheP (w ;λ ) = PSCAD(w )penaltyin(9),thesuperscriptS denotes
2 j 2 λ2 j
SCAD-thresholdingoperatoras:

   sgn(x)(|x|−λ 2) + if|x| ≤ 2λ 2


S(x;λ 2) = {(a−1)x−aλ
2
·sign(x)}/(a−2) if2λ
2
≤ |x| < aλ
2



  x ifaλ ≤ |x|
2
The parameter λ has a crucial role in controlling the sparsity in the variable weight
2
vector. Ahighervalueofλ resultsinincreasedsparsityofthevariablevectorW. Specif-
2
ically, a higher weight value w indicates greater importance of the jth variable in the
j
clustering process. Conversely, when w is equal to zero, it signifies that the jth variable
j
doesnotcontributetotheclustering.
Inordertogainadeepercomprehensionofthealgorithmicprocesswehaveproposed,
wesummarizetheaforementionedproceduresinapseudocodeinAlgorithm1.
9Algorithm1: IterativealgorithmforARSK
√
1: InitializetheweightvectorW asw = 1/ p,settoleranceε > 0andr = 0.
j
2: Initialize the error matrix by setting
E(0)
of the 80% of data farthest from all data
ij
centerpointstox ;theotherssetas0.
ij
3: repeat
4: Runtheclusteringalgorithmontheweighteddatasetx∗ = w(r)(x −E(r))by
ij j ij ij
(cid:40) (cid:41)
p n K
(cid:88)(cid:16)(cid:16) 1 (cid:88) (cid:17)2 (cid:88)(cid:88)(cid:16) 1 (cid:88) (cid:17)2(cid:17)
max x∗ − x∗ − x∗ − x∗
c1,...,c
k
ij n ij ij |n k| ij
j=1 i=1 k=1 i∈c k i∈c k
5: CalculatethenewE by(7)or(8),whichincorporatesthecluster-specificmeans
ij
µ = |n |−1(cid:80) (cid:80) x∗
k· k j i∈c ij
k
6: Until the (6) converges, we can obtain the error matrix
E(r+1)
and the cluster
ij
arrangementΘ(r+1). Keepthoseresultsforthenextvariableselectionphase.
7: Restoretheerrormatrixbysetting:
(cid:113)
E(r+1) := E(r+1)/ w(r)
ij ij j
8: Arrange the cluster Θ(r+1) and compute the QR(x ;Θ(r+1),E) for different vari-
j ·j
ablesby:
n n K
(cid:88)(cid:16) 1 (cid:88) (cid:17)2 (cid:88) (cid:88) (cid:16) 1 (cid:88) (cid:17)2
QR(x ;Θ(r+1),E) = x′ − x′ − x′ − x′ ,
j ·j ij n ij ij |n | ij
k
i=1 i=1 k=1 i∈c(r+1) i∈c(r+1)
k k
wherex′ = x −E(r+1) .
ij ij ij
9: Computenewvariableweightby:
S(QR(x ;Θ,E );λ )
w(r+1) = j ·j ·j 2
j (cid:113) (cid:80)p
(S(QR(x ;Θ,E );λ ))2
j=1 j ·j ·j 2
10: r = r+1
11: untilconvergenceofthecriterion((cid:80) |w(r)|)−1(cid:80) |w(r+1) −w(r)| < εismet.
j j j j j
2.4 AdaptationofthetuningparametersbytherobustGapstatistics
There are two tuning parameters in the proposed algorithm: λ and λ . The tuning pa-
1 2
rameter λ is crucial for finding the error matrix and impacts outliers’ detection, and the
1
tuning parameter λ plays a crucial role in filtering out variables contributing to the clus-
2
teringprocess. WehereprovidetherobustGapstatisticstodeterminetheseparameters.
TheGapstatisticswereoriginallyproposedinTibshiranietal.(2001)forselectingthe
10number of clusters in the K-means algorithm. Note that the Gap statistics is constructed
asafunctionofthebetween-clustersumofsquares,D =
(cid:80)p
w
(n−1(cid:80)n (cid:80)n
(x −
j=1 j i=1 i′=1 ij
x )2 − (cid:80)K n−1(cid:80) (x − x )2), for the original dataset. However, since D is
i′j k=1 k i,i′∈c ij i′j
k
sensitive to outliers, it is not suitable for selecting the tuning parameters under existence
of outliers. Hence, we propose a robust version of Gap statistics by adding the error part
to each observation when calculating the DR . As we mentioned before, the error part
λ2,λ1
canoutweightheinfluenceoftheoutlier,sothatwedefineGap as
λ2,λ1
B p
1 (cid:88) (cid:88)
Gap = log(DR )− log((b)DR ), DR = w QR(x ;Θ,E ),
λ2,λ1 λ2,λ1 B λ2,λ1 λ2,λ1 j j ·j ·j
b=1 j=1
(10)
and the optimal value of λ and λ corresponding to the largest value of Gap . Note
2 1 λ2,λ1
thatDR isaweightedrobustbetween-clustersumofsquaresdefinedin(3),and(b)DR
λ2,λ1 λ2,λ1
is a version for permuted datasets. Here B denotes the number of permuted datasets,
whicharegeneratedbyrandomlyselectingfromtheoriginaldataset. Increasingthenum-
ber of permuted datasets leads to improved accuracy in selecting the true parameters.
The between-cluster sum of squares, taking account of the variable weight, is adopted in
Witten and Tibshirani (2010), so our version used in (10) can be regarded as its robust
extension.
Sinceitmaybecomputationallyintensivetosearchtheoptimalvalueof(λ ,λ )bya
2 1
grid search method, we conduct an alternating optimization algorithm for tuning param-
eter search. Specifically, we first set a suitable value λ† for λ and compute the optimal
1 1
value λ∗ of λ by maximizing Gap . Then, we obtain the optimal value λ∗ of λ
2 2 λ2,λ†
1
1 1
by maximizing Gap . Using this search algorithm instead of the grid search method
λ∗ 2,λ1
can save a significant huge of computation time. We also offer a pseudocode for this
procedureforeasyunderstandingofthissearchalgorithmgiveninAlgorithm2.
11Algorithm2: Selectionof(λ ,λ )bytherobustGapstatistics
2 1
1: Setsomevalueforλ† andmaximizethefollowingGapstatisticswithrespecttoλ :
1 2
B
1 (cid:88)
Gap = log(DR )− log((b)DR )
λ2 λ2,λ†
1
B λ2,λ†
1
b=1
toobtaintheoptimalλ∗.
2
2: Fixλ = λ∗ andoptimizethefollowingGapstatisticswithrespecttoλ :
2 2 1
B
1 (cid:88)
Gap = log(DR )− log((b)DR )
λ1 λ∗ 2,λ1 B λ∗ 2,λ1
b=1
toobtaintheoptimalλ∗.
1
3: Outputtheoptimalsetoftuningparameters,(λ∗,λ∗)
2 1
3 Numerical Studies
In this section, we explore the ability of the proposed clustering method. We consider
that each observation x is generated independently from a multivariate normal distribu-
i
tion, given that the observation belongs to cluster k. Specifically, for an observation x in
i
cluster k, we have x ∼ N(µ ,Σ ), where µ ∈ Rp denotes the mean vector for cluster
i k· p k·
k,andΣ ∈ Rp×p representsthecovariancematrix. Eachelementofµ isindependently
p k·
sampledfromfromeitherU(−6,−3)orU(3,6). Tosimulatescenariosinvolvingoutliers,
weintroduceacontaminatederrordistributionwithamultivariatenormalmixture,repre-
sentedas(1−π)N(µ ,Σ )+πN(µ +b ,Σ ),j = 1,...,pwhereπ ∈ [0,1]represents
k· p k· j p
theproportionofoutliersforeachcluster. Inthesimulationstudy,Weconsidertwotypes
of Σ . In the first scenario, the assumption is made that all variables are independent.
p
In another scenario, we assume there is a correlation structure among variables. To in-
troduce randomness to the appearance of outliers, b is generated with a random number
j
fromoneoftwouniformdistributions: U(−13,−7)orU(7,13). Inthissetup,weassume
that some explanatory variables are significantly associated with the response, while the
remainingvariablesareredundantforvariableselection. Tomimicthissituation,weran-
domly set q elements of µ to non-zero values while the remaining elements are set to
k·
12zero. Consequently,asuccessfulmethodshouldaccuratelyidentifytheq positionswhere
theweightsw correspondingtothetruesignificantvariablesarenon-zerowhileensuring
j
thattheweightsw oftheremainingvariablesaresettozero.
j
To evaluate the approach’s clustering accuracy capability, we employ the clustering
error rate (CER) (Rand, 1971). The CER measures the extent to which the model’s pre-
dicted partition C(cid:98) for a set of n observations is consistent with the true labels C. The
CERisdefinedas
(cid:18) n(cid:19)−1
(cid:88)(cid:12) (cid:12)
CER(C(cid:98),C) = (cid:12)I −I (cid:12),
2 C(cid:98)(i,i′) C (i,i′)
i<i′
where,I andI indicatewhethertheithandi′thobservationsbelongtothesame
C (i,i′) C(cid:98)(i,i′)
cluster. Lower CER indicates better performance in clustering accuracy or outlier detec-
tion.
Furthermore, in order to evaluate each method’s proficiency in variable sparsity, we
apply two criteria for variable selection. The variable true positive rate (TPR) indicates
the approach’s success in finding informative variables. The true negative rate (TNR)
represents the successful identification of non-informative variables. If both criteria are
closer to 1, it indicates that the model provides a more comprehensive explanation of the
structureofthevariablespredictedinthestudy.
3.1 Simulation1: selectionofturningparameter
In this subsection explores the new Gap statistics’ capability to directly search for the
tuning parameters of robustness and variable vector sparsity using Algorithm 2. Ini-
tially, we considered 3 clusters, each containing 50 observations, number of variables
as p = 50, with q = 5, and all variables are independent, i.e., Σ = I . A proper of
p p
hyperparameters λ and λ should enable the model to accurately identify the number of
1 2
outliers and the number of informative variables in different contamination levels for π
in {0,0.1,0.2,0.3}. The proposed tuning parameter search strategy generates both the
13robustness parameter λ and the variable selection tuning parameter λ using an expo-
1 2
nential decay. We consider the number of permuted datasets to be 25 (i.e., B = 25).We
employed different thresholding function types, where the first thresholding function is
used to control model sparsity, and the second thresholding function is used to control
modelrobustness. Thetuningparametersearchprocessisrepeated30timesfor30differ-
entdatasets,andtheresultingtableispresentedinTable1.
threshold contamination numberof numberofdetected
type level detectedoutliers informativevariable
soft-soft π = 0 0(0) 4.300(0.000)
π = 0.1 15.56(6.425) 4.533(0.618)
π = 0.2 28.73(2.434) 4.200(0.871)
π = 0.3 33.10(14.76) 13.60(7.203)
soft-SCAD π = 0 0(0) 4.400(0.000)
π = 0.1 14.43(0.495) 6.310(1.854)
π = 0.2 28.68(1.349) 4.566(0.495)
π = 0.3 42.36(2.287) 6.800(2.946)
SCAD-soft π = 0 0(0) 4.545(0.687)
π = 0.1 14.30(0.483) 5.400(0.469)
π = 0.2 27.11(6.166) 5.941(1.748)
π = 0.3 38.63(16.22) 9.272(4.540)
SCAD-SCAD π = 0 0.090(0.333) 4.272(0.881)
π = 0.1 13.33(1.258) 5.833(3.125)
π = 0.2 24.10(5.065) 4.500(2.068)
π = 0.3 44.00(4.2110) 7.187(1.223)
Table 1: Using the robust Gap statistic to select the optimal tuning parameters for the
RSKC algorithm based on soft-soft-thresholding, soft-SCAD-thresholding, SCAD-soft-
thresholding,andSCAD-SCAD-thresholding,wereporttheaverageanditsstandarderror
(inparentheses)ofthenumberofdetectedoutliersandthenumberofdetectedinformative
variables.
Table 1 summarizes the results of the evaluation measures. Overall, a good tuning
parameterλ shouldaccuratelyidentifythenumberofoutliersunderdifferentcontamina-
1
tionlevels. Asthecontaminationlevelπ increasesfrom0to0.3,weobservethatthefour
thresholding methods demonstrate varying outlier detection capabilities across different
contamination levels. At π = 0.1, the detected number of outliers ranges from 13.33 to
15.56, with soft-soft-thresholding achieving the highest average of 15.56 (standard error:
6.425). When π increases to 0.2, the detected number of outliers falls between 24.10 and
28.73, with soft-soft and soft-SCAD-thresholding showing similar performance. This
14demonstrates that the Gap statistics can help the ARKC method determine λ to some
1
extent. However, when the contamination level reaches 0.3, the soft-soft-thresholding
ARSK method detects 33.10 outliers on average, which is lower than the SCAD-SCAD-
thresholdingARSKmethod(44.00outliers)anddeviatesfromthetruenumberofoutliers.
WeconsiderthattheL normpenaltyhaslowerrobustnesscomparedtotheSCADpenalty
1
(Hampeletal.(1986)). Regardingtheselectionofthetuningparameterλ ,agoodtuning
2
parameter should accurately identify the number of informative variables under different
contamination levels. When the data is clean (π = 0), all four methods identify around
4 to 5 informative variables, which is close to 5. As the contamination level increases,
thenumberofdetectedinformativevariablesgenerallyincreases,withsomefluctuations.
At π = 0.3, the soft-soft-thresholding ARSK method detects 13.60 informative vari-
ables on average, which significantly deviates from the 5. In contrast, the SCAD-SCAD-
thresholding ARSK method detects 7.187 informative variables, showing better perfor-
manceinthepresenceofhighcontamination. Consideringtheseresults,wecanconclude
that the proposed parameter selection algorithm based on the Gap statistic works rela-
tively well under low to moderate contamination levels. However, its performance may
degrade when the contamination level is high, especially for the soft-thresholding-based
methods. The SCAD-thresholding-based methods generally show better robustness and
accuracy in identifying outliers and informative variables under various contamination
levels.
3.2 Simulation2: comparisonwithotherapproaches
In this subsection, we present the findings of a comprehensive simulation study con-
ducted to investigate the cluster data structure and properties by the ARSK algorithm
based on soft-thresholding and SCAD-thresholding. The study extensively applied Gap
statisticstoestimateoptimalparametersettingsandcomparedwithseveralbenchmarkap-
proaches, including K-means (KC), PCA-K-means (PCA-KC) clustering using only the
first principal component, trimmed K-means (TKM), robust and sparse K-means RSKC
15(α = 0.1,0.2) (Kondo et al., 2016), and the weighted robust K-means (WRCK) algo-
rithms (Brodinova et al., 2019). Note that WRCK and RSKC are both trimming-based
clusteringapproaches.
Considering the mixture error model mentioned at the beginning, we also generated
data for 3 clusters, each containing 50 observations. We set the number of variables as
p = 50,withq = 5,andp = 500,withq = 50. Meanwhile,weconsiderπ in{0,0.1,0.2}
for specifying the distribution of outliers. As previously stated, we study two types of
covariancematrices: onewithΣ = I andtheanothergeneratedaccordingtothemethod
p p
proposedbyHiroseetal.(2017)as
 
1 ρ ··· ρ
t t
 

ρ 1
...
ρ


t t
Σ = Q QT,
p   . . . ... 1 . . .  
 
 
ρ ··· ρ 1
t t
where the Q denotes a p×p random rotation matrix satisfying QT = Q−1 and the ρ are
t
randomlygeneratedformanUniformdistributionU(0.1,1).
In order to evaluate the clustering accuracy and outlier detection capability of each
approach, we employ the clustering error rate (CER) as described above. To assess the
outlier detection performance, the outliers identified by each model are assigned to the
(K +1)-th cluster group. Table 2 presents the results of 100 simulations for the scenario
where all variables are independent, i.e., Σ = I , while Table 3 summarizes the results
p p
of100simulationsforthescenariowherevariablesexhibitacertaincorrelationstructure.
In Tables 2 and 3, the results show that KC, PCA-KC, and TKM approaches, which
lack robustness for high dimensions or outliers, are the worst performers, particularly
whenthedimensionalitypandproportionofoutliersπ increase. Forinstance,intheinde-
pendent case with p = 500 and q = 50, the CER of KC increases from 0.050 to 0.341 as
π increases from 0 to 0.2. Similarly, the CER of PCA-KC and TKM (α = 0.1) increase
from 0.058 to 0.300 and 0.128 to 0.302, respectively, under the same scenario. Con-
16p=50 p=500
q=5 q=50
π=0 π=0.1 π=0.2 π=0 π=0.1 π=0.2
KC 0.073(0.119) 0.191(0.124) 0.285(0.107) 0.050(0.105) 0.228(0.154) 0.341(0.145)
PCA-KC 0.109(0.113) 0.326(0.167) 0.383(0.132) 0.058(0.113) 0.219(0.098) 0.300(0.123)
TKM(α=0.1) 0.140(0.027) 0.093(0.036) 0.220(0.087) 0.128(0.005) 0.098(0.044) 0.302(0.146)
RSKC(α=0.1) 0.119(0.029) 0.008(0.031) 0.169(0.118) 0.105(0.007) 0(0) 0.123(0.129)
RSKC(α=0.2) 0.208(0.029) 0.127(0.015) 0.005(0.021) 0.187(0.007) 0.123(0.009) 0(0)
WRCK 0.129(0.037) 0.208(0.104) 0.136(0.071) 0.118(0.024) 0.084(0.028) 0.046(0.018)
soft-soft-ARSK 0.010(0.043) 0.014(0.043) 0.032(0.064) 0(0) 0(0) 0(0)
soft-SCAD-ARSK 0.010(0.038) 0.013(0.035) 0.026(0.033) 0(0) 0(0) 0(0)
SCAD-soft-ARSK 0.021(0.059) 0.016(0.036) 0.031(0.061) 0(0) 0(0) 0(0)
SCAD-SCAD-ARSK 0.017(0.057) 0.023(0.052) 0.019(0.045) 0(0) 0(0) 0(0)
Table2: Whenvariablesareindependent,theaveragevaluesoftheCERandtheirstandard
errors(inparentheses)basedon100MonteCarloreplications.
p=50 p=500
q=5 q=50
π=0 π=0.1 π=0.2 π=0 π=0.1 π=0.2
KC 0.078(0.122) 0.199(0.123) 0.318(0.138) 0.037(0.096) 0.219(0.172) 0.306(0.136)
PCA-KC 0.112(0.113) 0.284(0.169) 0.372(0.126) 0.113(0.131) 0.212(0.075) 0.278(0.046)
TKM(α=0.1) 0.083(0.055) 0.039(0.080) 0.240(0.104) 0.065(0.002) 0.139(0.053) 0.136(0.286)
RSKC(α=0.1) 0.082(0.021) 0.005(0.016) 0.235(0.128) 0.082(0.008) 0(0) 0.071(0.089)
RSKC(α=0.2) 0.194(0.021) 0.123(0.021) 0.008(0.015) 0.162(0.010) 0.122(0.009) 0(0)
WRCK 0.119(0.021) 0.203(0.116) 0.146(0.071) 0.126(0.019) 0.102(0.020) 0.022(0.081)
soft-soft-ARSK 0.009(0.038) 0.014(0.039) 0.037(0.047) 0(0) 0(0) 0.000(0.002)
soft-SCAD-ARSK 0.005(0.020) 0.015(0.039) 0.032(0.026) 0(0) 0(0) 0.001(0.002)
SCAD-soft-ARSK 0.007(0.021) 0.009(0.014) 0.047(0.069) 0(0) 0(0) 0.000(0.002)
SCAD-SCAD-ARSK 0.005(0.019) 0.004(0.005) 0.029(0.033) 0(0) 0(0) 0.002(0.018)
Table 3: When variables are correlated, the average values of the CER and their standard
errors(inparentheses)basedon100MonteCarloreplications.
versely, RSCK, WRCK, and ARSK perform exceptionally well in high-dimension and
high-contamination scenarios. However, the effectiveness of theRSCK approach heavily
depends on the choice of the parameter α. When α equals the true outlier proportion,
RSCK performs impressively, with CER close to 0. For example, in the correlated case
with p = 500 and q = 50, the CER of RSKC (α = 0.1) is 0 when π = 0.1. How-
ever, the approach loses effectiveness if α is incorrectly specified, leading to higher CER
values. In the same scenario, the CER of RSKC (α = 0.2) is 0.122 when π = 0.1.
The WRCK approach, based on the density clustering method, generally outperforms
RSCK when outliers are present. Still, it may erroneously classify more normal data
points as outliers when no contamination exists. In contrast, the ARSK approach, with
different combinations of thresholding functions (e.g., soft-soft, soft-SCAD, SCAD-soft
and SCAD-SCAD), consistently maintains a low Clustering Error Rate (CER) across all
17simulateddatascenarios.
In summary, the simulation results demonstrate the efficacy and robustness of the
AdaptiveRobustSparseK-means(ARSK)methodinclusteringhigh-dimensionaldatasets,
particularly in the presence of outliers and variable correlations. Across diverse data
scenarios, the ARSK variants consistently outperform traditional methods, including K-
means (KC), PCA-K-means (PCA-KC), Trimmed K-means (TKM), Robust and Sparse
K-means(RSKC),and WeightedRobustK-means(WRCK),highlighting thedistinctad-
vantagesoftheARSKapproach.
Both In Table 2 and Table 3, we observe that the CER of KC is worse than ARSK
when there are no outliers in the dataset. Moreover, when π = 0.1, the CER of the TKM
method is significantly higher than that of the RSCK method. We attribute this result to
the presence of noisy variables, which contribute to the increased CER. Therefore, it is
essentialtoidentifythesenoisyvariablestoreducetheirimpact.
Through 100 repetitions, the results of the TPR and TNR are presented in Tables 4
and 5. When variables are independent (Table 4), the TPR and TNR of all methods are
generally higher compared to the correlated case (Table 5). This indicates that when
variables are correlated with each other, all models tend to make less accurate judgments
about informative and non-informative variables, emphasizing the challenges posed by
variablecorrelationsinthecontextofclusteringandvariableselection.
As the proportion of outliers increases from 0 to 0.2, both the TPR and TNR of most
approachesexhibitadecreasingtrendacrossdifferentscenarios,suggestingthatthepres-
ence of outliers can negatively influence the variable selection performance. However,
the TPR and TNR of each type of ARSK approach variant remain relatively high and
stable across various data scenarios, demonstrating ARSK’s superior variable selection
capabilities in the presence of outlier contamination in datasets. For example, in the
independent case with p = 500 and q = 50, the TPR of soft-soft-ARSK, soft-SCAD-
ARSK, SCAD-soft-ARSK, and SCAD-SCAD-ARSK remain above 0.797, 0.810, 0.798,
and 0.810, respectively, even when π = 0.2. Similarly, their TNR values stay above
180.977, 0.980, 0.978, and 0.981 in the same scenario. These results emphasize the supe-
rior ability of soft-ARSK and SCAD-ARSK methods to accurately identify informative
and non-informative variables in the presence of outliers, making them valuable tools for
handlingcontaminatedclusteringdatasets.
In contrast, during the simulation process, we observed that traditional trimmed ap-
proaches such as WRCK and RSKC often struggled to maintain their variable selection
performance under high outlier proportions. In many cases, these methods incorrectly
assigned non-zero weights to variables that originally had zero weights, leading to an in-
creased number of false positives. This limitation makes it challenging for WRCK and
RSKCtoeffectivelydetectinformativevariablesinthepresenceofoutliers,emphasizing
theneedformoreeffectivemethodslikeARSK.
p=50 p=500
q=5 q=50
π=0 π=0.1 π=0.2 π=0 π=0.1 π=0.2
TPR RSKC(α=0.1) 0.924(0.038) 0.890(0.100) 0.710(0.267) 0.845(0.101) 0.908(0.100) 0.780(0.255)
RSKC(α=0.2) 0.654(0.109) 0.728(0.143) 0.880(0.098) 0.690(0.043) 0.993(0.027) 0.917(0.095)
WRCK 0.888(0.099) 0.664(0.172) 0.834(0.106) 0.734(0.044) 0.978(0.048) 0.841(0.101)
soft-soft-ARSK 0.922(0.127) 0.842(0.169) 0.810(0.183) 0.961(0.035) 0.853(0.053) 0.797(0.084)
soft-SCAD-ARSK 0.944(0.103) 0.910(0.162) 0.800(0.171) 0.932(0.020) 0.836(0.084) 0.818(0.018)
SCAD-soft-ARSK 0.962(0.092) 0.906(0.153) 0.766(0.144) 0.964(0.029) 0.969(0.024) 0.798(0.054)
SCAD-SCAD-ARSK 0.970(0.076) 0.802(0.150) 0.810(0.160) 0.969(0.026) 0.820(0.054) 0.810(0.055)
TNR RSKC(α=0.1) 0.671(0.427) 0.660(0.475) 0.926(0.102) 0.806(0.377) 0.548(0.490) 0.817(0.235)
RSKC(α=0.2) 0.978(0.142) 0.880(0.326) 0.753(0.430) 1(0) 0.086(0.259) 0.532(0.482)
WRCK 0.660(0.476) 0.911(0.199) 0.837(0.309) 1(0) 0.261(0.416) 0.536(0.478)
soft-soft-ARSK 1(0) 0.969(0.048) 0.981(0.042) 0.999(0.001) 0.986(0.012) 0.977(0.009)
soft-SCAD-ARSK 1(0) 0.954(0.040) 0.996(0.008) 0.972(0.001) 0.988(0.014) 0.980(0.009)
SCAD-soft-ARSK 1(0) 0.906(0.153) 0.976(0.021) 0.999(0.001) 0.999(0.001) 0.978(0.008)
SCAD-SCAD-ARSK 1(0) 0.976(0.021) 0.969(0.017) 0.999(0.000) 0.981(0.010) 0.981(0.010)
Table 4: When variables are independent, the average of 100 tests of TPR and TNP for
variableselectionsanditsstandarderror(inparentheses)arepresentedforthevariousdata
scenarios
3.3 Applicationstorealdata
In this subsection, we apply ARSK with different thresholding configurations to real-
life datasets and compare it with the benchmark methods previously mentioned in the
simulationstudy. WeconsiderthedatasetallfromtheUCIMachineLearningRepository
(DuaandGraff(2019)). Thedatasetsevaluatedincludevariousapplications,suchasglass
identification,breastcancerdiagnosis,acousticsignalclassification,spambasedetection,
19p=50 p=500
q=5 q=50
π=0 π=0.1 π=0.2 π=0 π=0.1 π=0.2
TPR RSKC(α=0.1) 0.917(0.013) 0.839(0.125) 0.761(0.236) 0.813(0.125) 0.934(0.092) 0.802(0.294)
RSKC(α=0.2) 0.705(0.132) 0.608(0.260) 0.860(0.015) 0.695(0.232) 0.903(0.057) 0.884(0.072)
WRCK 0.813(0.126) 0.604(0.212) 0.734(0.126) 0.613(0.213) 0.932(0.074) 0.812(0.126)
soft-soft-ARSK 0.953(0.086) 0.860(0.175) 0.769(0.042) 0.974(0.031) 0.843(0.071) 0.733(0.095)
soft-SCAD-ARSK 0.940(0.105) 0.855(0.181) 0.806(0.177) 0.968(0.022) 0.849(0.071) 0.773(0.109)
SCAD-soft-ARSK 0.928(0.096) 0.830(0.177) 0.772(0.169) 0.973(0.029) 0.877(0.051) 0.813(0.063)
SCAD-SCAD-ARSK 0.928(0.104) 0.918(0.120) 0.758(0.186) 0.968(0.036) 0.874(0.053) 0.793(0.058)
TNR RSKC(α=0.1) 0.680(0.387) 0.730(0.325) 0.864(0.226) 0.776(0.218) 0.632(0.363) 0.743(0.206)
RSKC(α=0.2) 0.837(0.232) 0.935(0.102) 0.794(0.400) 1(0) 0.136(0.454) 0.542(0.436)
WRCK 0.697(0.437) 0.954(0.083) 0.803(0.231) 1(0) 0.476(0.376) 0.644(0.306)
soft-soft-ARSK 1(0) 0.961(0.085) 0.985(0.026) 0.998(0.006) 0.977(0.011) 0.984(0.015)
soft-SCAD-ARSK 1(0) 0.876(0.080) 0.997(0.008) 0.999(0.001) 0.978(0.012) 0.991(0.008)
SCAD-soft-ARSK 1(0) 0.978(0.035) 0.992(0.017) 0.983(0.010) 0.977(0.011) 0.983(0.010)
SCAD-SCAD-ARSK 1(0) 0.968(0.020) 0.994(0.013) 0.997(0.005) 0.986(0.010) 0.981(0.009)
Table 5: When variables are correlated, the average of 100 tests of TPR and TNP for
variable selections and its standard error(in parentheses) are presented for the various
datascenarios
RSKC ARSK(proposed)
dataset K (n,p) KC PCA-KC TKM α=0.1 α=0.2 WRCK soft-soft soft-SCAD SCAD-soft SCAD-SCAD
glass 7 (214,9) 0.334 0.299 0.324 0.324 0.304 0.319 0.259 0.297 0.159 0.142
BreastCancer 2 (699,9) 0.080 0.080 0.107 0.071 0.081 0.104 0.065 0.057 0.079 0.084
Acoustic 4 (400,50) 0.309 0.443 0.392 0.293 0.280 0.382 0.354 0.360 0.355 0.346
Spambase 2 (4601,57) 0.476 0.481 0.310 0.406 0.563 0.493 0.434 0.432 0.364 0.413
mortality 3 (198,90) 0.227 0.185 0.228 0.153 0.149 0.106 0.131 0.136 0.143 0.149
DARWIN 2 (174,451) 0.391 0.400 0.447 0.402 0.434 0.465 0.385 0.398 0.454 0.401
Parkinson 2 (756,754) 0.477 0.462 0.500 0.482 0.563 0.392 0.445 0.441 0.429 0.462
Table6: ComparisonofCERsofdifferentalgorithmsforthereal-worlddata. Thesmallest
andsecondsmallestCERvaluesareshowninbold.
mortalityanalysis,andParkinson’sdiseasedetection. Thereal-lifedatasetcomprisesdata
with a wide range of dimensions, ranging from low to high. As in the previous analysis,
we also applied Gap statistics to estimate the hyperparameter. Due to the different units
of the continuous variables, we normalize all of continuous variables before conducting
thestudy.
Table6summarizestheresultofthereal-lifedatasetcomparison. Thereportedvalues
are CERs for each approach. In order to better illustrate the performance of that method,
wehaveemphasizedthefirstandsecondbestCERvaluesinboldwhenappliedtoacertain
real-lifedataset. WhiletraditionalmethodssuchasKC,PCA-KC,andTKMproveeffec-
tive in certain contexts, they generally yield higher CERs across the datasets compared
to the proposed ARSK algorithm and its variants. For instance, in the glass and Breast
Cancer dataset, the CER values obtained by KC, PCA-KC, and TKM are considerably
higher than those of the best-performing ARSK. In seven real-world datasets, our ARSK
20method ranked among the top two best methods in six of the datasets. Therefore, the
proposed ARSK algorithm consistently achieves superior performance with lower CERs
across most of the real datasets, particularly when using soft-thresholding to obtain the
errormatrixE. ARSKdemonstratescompetitiveperformanceinthisregard.
This comparative analysis emphasizes the ARSK algorithm’s success in significantly
reducing classification error rates across diverse real-world datasets, showcasing the ver-
satility and adaptability of our proposed clustering method. Through its different con-
figurations (e.g., soft-soft, soft-SCAD, SCAD-soft and SCAD-SCAD), allow it to handle
various data structures and sizes effectively. The consistent performance across different
real-worlddatasetsunderscoresthebroadapplicabilityofourapproachinvariousfields.
4 Concluding remarks
Inthispaper,weproposeanAdaptivelyRobustandSparseK-meansClusteringalgorithm
designed to handle multiple tasks within a unified framework for a comprehensive anal-
ysis of contaminated high-dimensional datasets. Our approach exhibits greater flexibility
inhandlingoutlierswithinreal-worlddatasets. WealsoproposedamodifiedGapstatistic
withanalternatingoptimizationalgorithmtosearchfortuningparameters. Thisapproach
significantly reduces the computational time required for parameter tuning compared to
traditionalmethods. Theexperimentresultsdemonstratethesignificantcapabilitiesofthe
proposedapproachinrevealinggroupstructuresinhigh-dimensionaldata. Ourmodelnot
only performs well in detecting outliers but also identifies significant variables directly,
leadingtoamorethoroughunderstandingofcomplexdatasets.
In our current study, we assume that the number of clusters is known, consistent with
the traditional K-means method. Additionally, Chen and Witten (2023) proposed a test
fordetectingdifferencesinmeansbetweeneachclusterestimatedfromK-meanscluster-
ing. Thistechniquecanbeappliedtothemodifieddatasetx −E ,wherenoisevariables
ij ij
(i.e.,w = 0)areremoved,tohelpdeterminewhichclustersshouldberetained.
j
Furthermore, We have consistently utilized a single penalty factor λ in the objective
1
21function(4). Thisensuresauniformapproachtodetectingoutliersacrossvariousgroups.
However, given the complexity and diversity of the data, a more sophisticated strategy
is needed. This might involve using different penalty factors, or distinct group penalty
(cid:80) (cid:80)
functions for different groups. One might employ P (E ;λ ) in (4). Incorporat-
k i k i· k
ing this approach into unsupervised learning presents challenges in selecting not just the
penalty factor but also the group penalty function. These extensions are left to our future
work.
Acknowledgement
This work is supported by the Japan Society of the Promotion of Science (JSPS KAK-
ENHI)grantnumbers,20H00080and21H00699.
5 Appendix
5.1 Proofssolutionto(9)
Considermaximizingthefollowingregularizedobjectivefunctionforestimatingtheweights
w .
j
p p
(cid:88) (cid:88) 1
L(w ,λ ) = W Q − (P (w ;λ )+ w2),
j 2 j j 2 j 2 2 j
j=1 j=1
whereQ = QR(x ;Θ,E ).
j j ·j ·j
Takingthederivativewithrespecttow andsettingittozero,weobtain:
j
Q −w −Γ = 0
j j j
WhentheL penaltyP (w ;λ )isapplied,Γ representsthesubgradientoftheL penalty
1 2 j 2 j 1
withrespecttow . Solvingtheaboveequationyields:
j
22
   Q j −λ 2, ifQ j > λ 2


Ssoft(x;λ 2) = w
j
= 0, if|Q j| ≤ λ
2



  Q +λ , ifQ < −λ .
j 2 j 2
When the SCAD penalty P (w ;λ ) is applied, Γ represents the subgradient of the
2 j 2 j
SCADpenaltywithrespecttow . Solvingtheaboveequationyields:
j

   sgn(Q j)(|Q j|−λ 2) + if|Q j| ≤ 2λ 2


SSCAD(x;λ 2) = w
j
= (a−1)Q (j a− −a 1λ )−2· 1sign(Qj) if2λ
2
≤ |Q j| < aλ
2



  Q ifaλ ≤ |Q |
j 2 j
(cid:113)
Projectwˆ ontotheparameterspace
(cid:80)p
w = 1. Thefinalsolutioncanberewrit-
j j=1 j
tenas:
S(Q ;λ )
j 2
w =
j (cid:113)
(cid:80)p
(S(Q ;λ ))2
j=1 j 2
References
Antoniadis, A. and J. Fan (2001). Regularization of wavelet approximations. Journal of
theAmericanStatisticalAssociation96(455),939–967.
Brodinova´, Sˇ., P. Filzmoser, T. Ortner, C. Breiteneder, and M. Rohm (2019). Robust and
sparse k-means clustering for high-dimensional data. Advances in Data Analysis and
Classification13,905–932.
Chen,Y.T.andD.M.Witten(2023). Selectiveinferencefork-meansclustering. Journal
ofMachineLearningResearch24(152),1–41.
Coretto,P.andC.Hennig(2016). Robustimpropermaximumlikelihood: tuning,compu-
23tation, and a comparison with other methods for robust gaussian clustering. Journal of
theAmericanStatisticalAssociation111(516),1648–1659.
Cuesta-Albertos, J. A., A. Gordaliza, and C. Matra´n (1997). Trimmed k-means: an at-
tempttorobustifyquantizers. TheAnnalsofStatistics25(2),553–576.
Dua, D. and C. Graff (2019). Uci machine learning repository irvine. University of
California.
Fan, J. and R. Li (2001). Variable selection via nonconcave penalized likelihood and its
oracleproperties. JournaloftheAmericanstatisticalAssociation96(456),1348–1360.
Forgy,E.W.(1965). Clusteranalysisofmultivariatedata: efficiencyversusinterpretabil-
ityofclassifications. biometrics21,768–769.
Fujisawa, H. and S. Eguchi (2006). Robust estimation in the normal mixture model.
JournalofStatisticalPlanningandInference136(11),3989–4011.
Hampel, F. R., E. M. Ronchetti, P. J. Rousseeuw, and W. Stahel (1986). The approach
basedoninfluencefunctions. RobustStatistics.Wiley.
Hirose, K., H. Fujisawa, and J. Sese (2017). Robust sparse gaussian graphical modeling.
JournalofMultivariateAnalysis161,172–190.
Huang, J., P. Breheny, and S. Ma (2012). A selective review of group selection in high-
dimensionalmodels. Statisticalscience: areviewjournaloftheInstituteofMathemat-
icalStatistics27(4).
Katayama, S. and H. Fujisawa (2017). Sparse and robust linear regression: An optimiza-
tionalgorithmanditsstatisticalproperties. StatisticaSinica,1243–1264.
Klochkov, Y., A. Kroshnin, and N. Zhivotovskiy (2021). Robust k-means clustering for
distributionswithtwomoments. TheAnnalsofStatistics49(4),2206–2230.
24Kondo,Y.,M.Salibian-Barrera,andR.Zamar(2016). RSKC:AnRpackageforarobust
andsparsek-meansclusteringalgorithm. JournalofStatisticalSoftware72,1–26.
Lloyd, S. (1982). Least squares quantization in PCM. IEEE transactions on information
theory28(2),129–137.
Punzo,A.andP.D.McNicholas(2016). Parsimoniousmixturesofmultivariatecontami-
natednormaldistributions. BiometricalJournal 58(6),1506–1537.
Rand, W. M. (1971). Objective criteria for the evaluation of clustering methods. Journal
oftheAmericanStatisticalassociation66(336),846–850.
She, Y. and A. B. Owen (2011). Outlier detection using nonconvex penalized regression.
JournaloftheAmericanStatisticalAssociation106(494),626–639.
Sugasawa,S.andG.Kobayashi(2022). Robustfittingofmixturemodelsusingweighted
completeestimatingequations. ComputationalStatistics&DataAnalysis174,107526.
Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the
RoyalStatisticalSocietySeriesB:StatisticalMethodology58(1),267–288.
Tibshirani, R., G. Walther, and T. Hastie (2001). Estimating the number of clusters in a
data set via the gap statistic. Journal of the Royal Statistical Society: Series B (Statis-
ticalMethodology)63(2),411–423.
Witten, D. M. (2013). Penalized unsupervised learning with outliers. Statistics and its
Interface6(2),211.
Witten, D. M. and R. Tibshirani (2010). A framework for feature selection in clustering.
JournaloftheAmericanStatisticalAssociation105(490),713–726.
Yang, M.-S., C.-Y. Lai, and C.-Y. Lin (2012). A robust EM clustering algorithm for
Gaussianmixturemodels. PatternRecognition45(11),3950–3961.
25Yuan, M. and Y. Lin (2006). Model selection and estimation in regression with grouped
variables. Journal of the Royal Statistical Society Series B: Statistical Methodol-
ogy68(1),49–67.
Zou, H. and T. Hastie (2010). Addendum: ”regularization and variable selection via the
elasticnet”[j.r.stat.soc.ser.bstat.methodol.67(2005),no.2,301–320;mr2137327].
journaloftheroyalstatisticalsociety67(5),768–768.
26