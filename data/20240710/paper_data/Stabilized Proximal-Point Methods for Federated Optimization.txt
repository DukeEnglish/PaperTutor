Stabilized Proximal-Point Methods for Federated
Optimization
XiaowenJiang AntonRodomanov SebastianU.Stich
SaarlandUniversityandCISPA∗ CISPA∗ CISPA∗
xiaowen.jiang@cispa.de anton.rodomanov@cispa.de stich@cispa.de
Abstract
Indevelopingefficientoptimizationalgorithms,itiscrucialtoaccountforcommu-
nicationconstraints—asignificantchallengeinmodernfederatedlearningsettings.
Thebest-knowncommunicationcomplexityamongnon-acceleratedalgorithms
is achieved by DANE, a distributed proximal-point algorithm that solves local
subproblemsineachiterationandthatcanexploitsecond-ordersimilarityamong
individual functions. However, to achieve such communication efficiency, the
accuracyrequirementforsolvingthelocalsubproblemsisslightlysub-optimal. In-
spiredbythehybridprojection-proximalpointmethod,inthiswork,wei)propose
anoveldistributedalgorithmS-DANE.Thismethodadoptsamorestabilizedprox-
centerintheproximalstepcomparedwithDANE,andmatchesitsdeterministic
communicationcomplexity. Moreover,theaccuracyconditionofthesubproblem
ismilder,leadingtoenhancedlocalcomputationefficiency. Furthermore,itsup-
portspartialclientparticipationandarbitrarystochasticlocalsolvers,makingit
moreattractiveinpractice. Wefurtherii)accelerateS-DANE,andshowthatthe
resultingalgorithmachievesthebest-knowncommunicationcomplexityamong
allexistingmethodsfordistributedconvexoptimization,withthesameimproved
localcomputationefficiencyasS-DANE.
1 Introduction
Federatedlearningisarapidlyemerginglarge-scalemachinelearningframeworkthatallowstraining
fromdecentralizeddatasources(e.g. mobilephonesorhospitals)whilepreservingbasicprivacyand
security[41,22,29]. Developingefficientfederatedoptimizationalgorithmsbecomesoneofthe
centralfocusesduetoitsdirectimpactontheeffectivenessofglobalmachinelearningmodels.
One of the key challenges in modern federated optimization is to tackle communication bottle-
necks [30]. The large-scale model parameters, coupled with relatively limited network capacity
andunstableclientparticipation,oftenmakecommunicationhighlyexpensive. Therefore,thepri-
maryefficiencymetricofafederatedoptimizationalgorithmisthetotalnumberofcommunication
roundsrequiredtoreachadesiredaccuracylevel. Iftwoalgorithmsshareequivalentcommunication
complexity,theirlocalcomputationefficiencybecomesthesecondimportantmetric.
TheseminalalgorithmDANE[55]isanoutstandingdistributedoptimizationmethod. Itachievesthe
best-knowndeterministiccommunicationcomplexityamongexistingnon-acceleratedalgorithms(on
theserverside)[20]. ThisefficiencyprimarilyhingesuponamildpreconditionregardingtheSecond-
orderDissimilarityδ. Innumerousscenarios,likestatisticallearningforgeneralizedmodel[18]and
semi-supervisedlearning[6],δtendstoberelativelysmall. However,toensuresuchfastconvergence,
DANErequireseachiterationsubproblemtobesolvedwithsufficientlyhighaccuracy. Thisleadsto
sub-optimallocalcomputationeffortacrossthecommunicationrounds,whichisinefficientinpractice.
FedRed[20]improvesthisweaknessbyusingdoubleregularization. However,thistechniqueisonly
effectivewhenusinggradientdescentasthelocalsolverbutcannoteasilybecombinedwithother
optimizationmethods. Forinstance,applyinglocalacceleratedgradientorsecond-ordermethods
cannot improve its local computation efficiency. Moreover, it is also unclear how to extend this
methodtothepartialclientparticipationsettingrelevanttofederatedlearning.
∗CISPAHelmholtzCenterforInformationSecurity,Saarbrücken,Germany
4202
luJ
9
]GL.sc[
1v48070.7042:viXra104 104
103 103
102 102
101 101
100 100
101 101
0 100 200 300 400 0 20000 40000 60000
Communication rounds Total number of gradient oracle calls
S-DANE-GD (ours) Acc-S-DANE-GD (ours) DANE-GD
Figure1:ComparisonofS-DANEandAcc-S-DANEwithDANEforsolvingaconvexquadraticminimization
problem.AllthreemethodsuseGDasthelocalsolver.S-DANEhasimprovedlocalcomputationefficiency
thanDANEwhileAcc-S-DANEfurtherimprovesthecommunicationcomplexity.
Ontheotherhand,thecommunicationcomplexitiesachievedbythecurrentacceleratedmethods
typically cannot be directly compared with those attained by DANE, as they either depend on
sub-optimalconstantsoradditionalquantitiessuchasthenumberofclientsn. Themostrelevant
andstate-of-the-artalgorithmAcc-Extragradient[31]achievesabettercomplexityintermsofthe
accuracyεbutwithdependencyonthemaximumSecond-orderDissimilarityδ whichcanin
max
principlebemuchlargerthanδ(seeRemark2). Unlikemostfederatedlearningalgorithms,suchas
FedAvg[41],thismethodrequirescommunicationwithallthedevicesateachroundtocompute
thefullgradientandthenassignsonedeviceforlocalcomputation. Incontrast,FedAvgandsimilar
algorithmsperformlocalcomputationsonparallelandutilizethestandardaveragingtocomputethe
globalmodel. Thefollow-upworkAccSVRS[38]appliesvariancereductiontoAcc-Extragradient
whichresultsinlessfrequentfullgradientupdates. However,thecommunicationcomplexityincurs
adependencyonnwhichisprohibitiveforcross-devicesetting[22]wherethenumberofclients
canbepotentiallyverylarge. Thus,thereexistsnoacceleratedfederatedalgorithmthatisuniformly
betterthanDANEintermsofcommunicationcomplexity.
Inthiswork,weaimtodevelopfederatedoptimizationalgorithmsthatcanachievethebestcommuni-
cationcomplexitywhileretainingefficientlocalcomputation. Tothisend,wefirstrevisitthesimple
proximal-pointmethodonasinglemachine. Theaccuracyrequirementforsolvingthesubproblem
definedinthisalgorithmisslightlysub-optimal.Drawinginspirationfromhybridprojection-proximal
pointmethodforfindingzeroesofamaximalmonotoneoperator[57],weobservethatusingamore
stabilizedprox-centerimprovestheaccuracyrequirement. Wemakethefollowingcontributions.
(Table1summarizesourmaincomplexityresultsinthefullclientparticipationsetting.)
Maincontributions:
• We develop a novel federated optimization algorithm S-DANE that achieves the best-known
communicationcomplexity(fornon-acceleratedmethods)whilealsoenjoyingimprovedlocal
computationefficiencyoverDANE[55].
• WedevelopanacceleratedversionofS-DANEbasedontheMonteiro-Svaiteracceleration[44].
TheresultingalgorithmAcc-S-DANEachievesthebest-knowncommunicationcomplexityamong
allexistingmethodsfordistributedconvexoptimization.
• Bothalgorithmssupportpartialclientparticipationandarbitrarystochasticlocalsolvers,making
themattractiveinpracticeforfederatedoptimization.
• We provide a simple analysis for both algorithms. We derive convergence estimates that are
continuousinthestrongconvexityparameterµ.
• Weillustratestrongpracticalperformanceofourproposedmethodsinexperiments.
Relatedwork. Moreaufirstproposedthenotionoftheproximalapproximationofafunction[45].
Basedonthisoperation,Martinetdevelopedthefirstproximal-pointmethod[40]. Thismethodwas
firstacceleratedbyGüller[16],drawingtheinspirationfromNesterov’sFastgradientmethod[47].
Later,Linetal.[39]introducedthecelebratedCatalystframeworkthatbuildsuponGüller’saccelera-
tion. UsingCatalystacceleration,alargeclassofoptimizationalgorithmscandirectlyachievefaster
convergence. Inasimilarspirit,DoikovandNesterov[11]proposecontractingproximalmethods
2
f-)x(f f-)x(fGeneralconvex µ-stronglyconvex
Algorithm Guarantee
#commrounds #localgradientqueries #commrounds #localgradientqueries
Scaffnew[42]a LD2b 1b (cid:113) Llog(D2+H02/(µL)) (cid:113) L inexpectation
ε µ ε µ
SONATA[58]c unknown unknown δmaxlog(D2) -d deterministic
µ ε
DANE[55] δD2 (cid:113) Llog(cid:0)LD2(cid:1) δlog(D2) (cid:113) Llog(cid:0)Llog(D2)(cid:1) deterministic
ε δ ε µ ε δ µ ε
FedRed[20] δD2 L δlog(D2) L inexpectation
ε δ µ ε δ
S-DANE(thiswork,Alg.1) δD2 (cid:113) L δlog(D2) (cid:113) L deterministic
ε δ µ ε δ
InexactAcc-SONATA[59]c unknown unknown (cid:113) δmaxlog(δmax)log(D2) (cid:113) Llog(D2) deterministic
µ µ ε µ ε
Acc-Extragradient[31]c (cid:113) δmaxD2 (cid:113) L (cid:113) δmaxlog(D2) (cid:113) L deterministic
ε δmax µ ε δmax
CatalyzedSVRP[26]e unknown unknown (cid:0)n+n3 4(cid:113) µδ(cid:1)log(L µ)log(D ε2) -f inexpectation
AccSVRS[38]ec unknown unknown (cid:0)n+n3 4(cid:113) µδ(cid:1)log(D ε2) 1+(cid:113) δ√L
n
inexpectation
Acc-S-DANE(thiswork,Alg.2) (cid:113) δD2 (cid:113) L (cid:113) δlog(D2) (cid:113) L deterministic
ε δ µ ε δ
aForScaffnewandFedRed,thecolumn‘#commrounds’representstheexpectednumberoftotalcommunicationsrequiredtoreachεaccuracy.Thecolumn‘#localgradient
queries’isreplacedwiththeexpectednumberoflocalstepsbetweentwocommunications.
n1(cid:80)b cT
Sn
i=h Oe 1N∥g Ahe Tin A,0e ,r −a Inl e∇c xo
f
an
i
c(v txe Ax
⋆)
c∥r ce -≤s Su Ol Lt N2o
D
Af
T2
AS
.
,c
T
Aa hf
e
cf
n
cn -e
t
Ehw
e xtb
ri as
es
gte rs
p
at da isb iel oi ns
f
th
o
ae
r
nd
d
deri An
1 c.
cT She Vo Rre Sm on1 l1 ynin eedth te oaR sa sun md eP sro trx onp ga cp oe nr ve[7 x] it. yoW ffe .assumethathi,0 = ∇fi(x0)andestimateH02 :=
dExactproximallocalstepsareusedinSONATA
eCatalyzedSVRPandAccSVRSaimatminimizingadifferentmeasurewhichisthetotalamountofinformationtransmittedbetweentheserverandtheclients.Their
iterationcomplexityisequivalenttothecommunicationroundsinournotations.WerefertoRemark8fordetails.
fKhaledandJin[26]assumeexactevaluationsoftheproximaloperatorfortheconvenienceofanalysis.
Table1:Summaryoftheworst-caseconvergencebehaviorsoftheconsidereddistributedoptimizationmethods
(intheBigO-notation)assumingeachf isL-smoothandµ-convexwithµ ≤ Θ(δ),whereδ,δ ,ζ2 are
i max
definedin(2),Remark2and(3),andD2 :=(cid:13) (cid:13)x0−x⋆(cid:13) (cid:13)2.The’#localgradientqueries’columnrepresentsthe
numberofgradientoraclequeriesrequiredbetweentwocommunicationroundstoachievethecorresponding
complexity,assumingthemostefficientlocalfirst-orderalgorithmsareused.Thecolumn’Guarantee’means
whethertheconvergenceguaranteeholdsinexpectationordeterministically.Thesuboptimalityεisdefinedvia
(cid:13) (cid:13)xˆR−x⋆(cid:13) (cid:13)2 andf(xˆR)−f⋆forstrongly-convexandgeneralconvexfunctionswherexˆRisacertainoutput
producedbythealgorithmafterRnumberofcommunications.
thatcanacceleratehigher-ordertensormethods. WhileGüller’saccelerationhasbeensuccessfully
appliedtomanysettings,itslocalcomputationissub-optimal. Specifically,whenminimizingsmooth
convexfunctions,alogarithmicdependenceonthefinalaccuracyisincurredinitslocalcomputation
complexity[11]. Solodov[57]proposedahybridprojection-proximalpointmethodthatallowssig-
nificantrelaxationoftheaccuracyconditionfortheproximal-pointsubproblems. Morerecentworks
suchasAdaptiveCatalyst[4]andRECAPP[4]successfullygetridoftheadditionallogarithmic
factorforacceleratedproximal-pointmethodsaswell.
Anothertypeofaccelerationbasedontheproximalextra-gradientmethodwasintroducedbyMonteiro
andSvaiter[44]. Thismethodismoregeneralinthesensethatitallowsarbitrarylocalsolversandthe
convergenceratesdependonthethesesolvers. Forinstance,underconvexityandLipschitzsecond-
orderderivative,theratecanbeacceleratedtoO(1/k3.5)byusingNewton-typemethod. Moreover,
when the gradient method is used, Monteiro-Svaiter Acceleration recovers the rate of Güller’s
accelerationanditsaccuracyrequirementfortheinexactsolutionisweaker. Forminimizingsmooth
convexfunctions,onegradientstepisenoughforapproximatelysolvingthelocalsubproblem[46].
Thistechniquehasbeenappliedtocentralizedcompositeoptimization,knownasgradientsliding[33,
34,31]. Acomprehensivestudyofaccelerationcanbefoundin[13].
WedefertheliteraturereviewondistributedandfederatedoptimizationalgorithmstoAppendixA.
2 Problemsetupandbackground
Weconsiderthefollowingdistributedminimizationproblem:
(cid:34) n (cid:35)
1 (cid:88)
min f(x):= f (x) , (1)
x∈Rd n i
i=1
where each function f : Rd → R is assumed to be convex. We focus on the standard federated
i
settingwherethefunctions{f }aredistributedamongndevices. Aservercoordinatestheglobal
i
3optimizationprocedureamongthedevices. Ineachcommunicationround, theserverbroadcasts
certaininformationtotheclients. Theclients,inturn,performlocalcomputationinparallelbasedon
theirowndataandtransmittheresultinglocalmodelsbacktotheservertoupdatetheglobalmodel.
Mainobjective: Giventhehighcostofestablishingconnectionsbetweentheserverandtheclients,
ourparamountobjectiveistominimizethenumberofrequiredcommunicationroundstoachievethe
desiredaccuracylevel. Thisrepresentsacentralmetricinfederatedcontexts,asoutlinedinreferences
suchas[41,25]. Secondaryobjective: Efficiencyinlocalcomputationrepresentsanotherpivotal
metricforoptimizationalgorithms. Forinstance,iftwoalgorithmsshareequivalentcommunication
complexity,thealgorithmwithlesslocalcomputationalcomplexityisthemorefavorablechoice.
Notations: Weabbreviate[n] := {1,2,...,n}. ForasetAandanintegers ≤
|A|,weuse(cid:0)A(cid:1)
to
s
denotethesetcontainingalls-elementsubsetsofA. Weuse∥·∥todenoteL norm. Weassume
2
eachfunctionf isµ-convex(B.1). Weusex⋆ todenoteargmin f(x)andassumeitexists. We
abbreviatef⋆ :=i f(x⋆). Weusef := 1(cid:80) f todenotethex averagefunctionoverthesetS
S s i∈S i
withs=|S|. Weusertodenotetheindexofthecommunicationroundandktodenotetheindex
ofthelocalstep. Finally,weusesuperscriptandsubscripttodenotetheglobalandthelocalmodel
respectively. Forinstance,xr representstheglobalmodelatroundrwhilex isthelocalmodel
i,r
computedbythedeviceiatroundr.
2.1 Proximal-pointmethodsonasinglemachine
Inthissection,weprovideabriefbackgroundonproximal-pointmethods[45,40,52,49],whichare
thefoundationformanydistributedoptimizationalgorithms.
Proximal-pointmethod. Givenaniteratex , themethoddefinesx tobean(approximate)
k k+1
minimizeroftheproximalsubproblem:
(cid:110) λ (cid:111)
x ≈argmin F (x):=f(x)+ ∥x−x ∥2 , (2)
k+1 k 2 k
x∈Rd
foranappropriatelychosenparameterλ ≥ 0. Theparameterλallowsforatrade-offbetweenthe
complexityofeachiterationandtherateofconvergence. Ifλ=0,thenthesubproblemineachitera-
tionisasdifficultassolvingtheoriginalproblembecausenoregularizationisapplied. However,asλ
increases,moreregularizationisadded,simplifyingthesubproblemsineachiteration. Forexample
foraconvexfunctionf,theproximal-pointmethodguaranteesf(x¯ )−f⋆ ≤O(cid:0)λ ∥x −x⋆∥2(cid:1)
K K 0
wherex¯ := 1 (cid:80)K x [49,4]. However,toachievethisiterationcomplexity,thesubproblem(2)
K K k=1 k
hastobesolvedtoafairlyhighaccuracy[52,4]. Forinstance,theaccuracyconditionshouldeither
dependontargetaccuracyε,orincreaseoverk: ∥∇F (x )∥=O(λ∥x −x ∥)[56]. Indeed,
k k+1 k k+1 k
whenf issmoothandthestandardgradientdescentisusedasalocalsolver,thenthenumberof
gradientstepsrequiredtosolvethesubproblemhasalogarithmicdependenceontheiterationsk. The
sameissuealsoariseswhenconsideringacceleratedproximal-pointmethods[11,16].
Stabilized proximal-point method. One of the key insights that we use in this work is the
observationthatusingadifferentprox-centermakestheaccuracyconditionofthesubproblemweaker.
Thestabilizedproximal-pointmethoddefinesx tobean(approximate)minimizeroftheproximal
k+1
subproblem:
λ
x ≈argmin{F (x):=f(x)+ ∥x−v ∥2},
k+1 k 2 k
x
(3)
(cid:26) (cid:27)
µ λ
v =argmin ⟨∇f(x ),x⟩+ ∥x−x ∥2+ ∥x−v ∥2 ,
k+1 k+1 2 k+1 2 k
x
whereλ ≥ 0isaparameterofthemethodandµ ≥ 0isthestrong-convexityconstantoff. This
algorithmupdatestheprox-centerfortheproximalstepsv byanadditionalgradientstepineach
k
iteration. Forinstance,whenµ=0,thentheprox-centersareupdatedasv =v − 1∇f(x ),
k+1 k λ k+1
which is sometimes called an extra-gradient update. The stabilized proximal-point method has
the same iteration complexity as the original method (2), but with a relaxed accuracy condition:
∥∇F (x )∥ ≤ O(λ∥x −v ∥). When f is L-smooth, the oracle complexity has no extra
k k+1 k+1 k
logarithmic factor of k. Specifically, by setting λ = Θ(L), such condition can be satisfied after
onegradientstep[46]. Thisshowsthatthestabilizedproximal-pointmethodshaveabetteroracle
complexitythantheproximal-pointmethodonconvexproblems(c.f.Thm.1forthespecialcase
4Algorithm1S-DANE:StabilizedDANE
1: Input:λ>0,µ≥0,s∈[n],x0 =v0 ∈Rd.
2: forr=0,1,2...do
3: SampleS
∈(cid:0)[n](cid:1)
uniformlyatrandomwithoutreplacement.
r s
4: foreachdevicei∈S inparalleldo
5: x ≈argminr (cid:8) F (x):=f (x)+⟨∇f (vr)−∇f (vr),x⟩+ λ∥x−vr∥2(cid:9) .
i,r+1 x∈Rd i,r i Sr i 2
6: xr+1 = 1(cid:80) x .
7: vr+1 :=s argmi∈ iS nr(cid:8) Gi,r+ (x1 ):= 1 (cid:80) [⟨∇f (x ),x⟩+ µ∥x−x ∥2]+ λ∥x−vr∥2(cid:9) .
r s i i,r+1 2 i,r+1 2
x∈Rd i∈Sr
n=1). Thismethodoriginatesfromthehybridprojection-proximalpointalgorithm[57]thatwas
developedtosolvethemoregeneralproblemoffindingzeroesofamaximalmonotoneoperator. In
thiswork,weapplythismethodinthedistributedsetting(n≥2).
2.2 Distributedproximal-pointmethods
Theproximal-pointmethodcanbeadaptedtosolvethedistributedoptimizationproblem(1). Thisis
theideabehindtheFedProx[37]algorithm.Itreplacestheglobalproximalstep(2)bynsubproblems,
definedas: x :=argmin {f (x)+ λ∥x−xr∥2},whichcanbesolvedindependentlyoneach
i,r+1 x i 2
device,followedbytheaveragingstepxr+1 = 1 (cid:80)n x . Hereweswitchthenotationfromk
n i=1 i,r+1
tortohighlightthatoneiterationoftheproximal-pointmethodcorrespondstoacommunication
round in this setting. To ensure convergence, FedProx has to use a large λ that depends on the
target accuracy as well as the heterogeneity among {f }, which slows down the communication
i
efficiency[37].DANE[55]improvesthisbyincorporatingadriftcorrectiontermintothesubproblem.
(cid:110) λ (cid:111)
x :=argmin F˜ (x):=f (x)+⟨∇f(xr)−∇f (xr),x⟩+ ∥x−xr∥2 . (4)
i,r+1 i,r i i 2
x
Consequently, DANE allows to choose a much smaller λ in the algorithm. Moreover, it can ex-
ploitsecond-ordersimilarityandachievethebest-knowncommunicationcomplexityamongnon-
acceleratedmethods[20]. However,theaccuracyofthesubproblemhastosatisfythesamehigh
requirementastheoriginalproximal-pointmethod. Thismotivatesustodevelopnewalgorithms,
whichwedescribeinthefollowingsection.
3 StabilizedDANE
WenowdescribeS-DANE(Alg.1),ourproposedfederatedproximal-pointmethodthatemploys
stabilizedprox-centersinitssubproblems. Duringeachcommunicationroundr,theserversamplesa
subsetofclientsuniformlyatrandomandsendsvr totheseclients. Thentheservercollects∇f (vr)
i
fromtheseclients,computes∇f (vr)andsends∇f (vr)backtothem. Eachdeviceintheset
Sr Sr
thencallsaarbitrarylocalsolver(whichcanbedifferentoneachdevice)toapproximatelysolveits
localsubproblem. Finally,eachdevicetransmits∇f (x )andx totheserverandtheserver
i i,r+1 i,r+1
aggregatesthesepointsandcomputesthenewglobalmodels.
SimilartoDANE,S-DANEcanalsoachievecommunicationspeed-upifthefunctionsamongdevices
shareacertainsimilaritycondition. Formally,weconsiderthefollowingassumptions.
Definition1(Second-orderDissimilarity). Letf : Rd → Rbeafunctionforanyi ∈ [n]andlet
i
s∈[n]. Then{f }aresaidtohaveδ -SODofsizesifforanyx,y∈RdandanyS ∈(cid:0)[n](cid:1) ,itholds
i s s
that
1(cid:88)(cid:13) (cid:13)∇hS(x)−∇hS(y)(cid:13) (cid:13)2 ≤δ2∥x−y∥2 , (5)
s i i s
i∈S
wherehS :=f −f andf := 1(cid:80) f .
i S i S s i∈S i
Definition 1 quantifies the dissimilarity between the functions in any subset of size s and their
averagedfunction,i.e.,theinternaldifferencebetweenf andf . Clearly,δ =0andwhens=n,
i S 1
werecoverthestandardnotionofsecond-orderdissimilarityintroducedinpriorworks:
Definition2(δ-SOD[26,38,20]). δ-SOD:=δ -SODofsizen.
n
5Wheneachfunctionf istwicecontinuouslydifferentiable,asimplesufficientconditionfor(5)is:
i
foranyx∈Rd,itholdsthat: 1(cid:80) (cid:13) (cid:13)∇2hS(x)(cid:13) (cid:13)2 ≤δ2. However,thisisnotanecessarycondition.
s i∈S i s
Wereferthereaderstothediscussionin[20]fordetails.
Ifeachfunctionf isL-smooth,thenδ ≤ 2Lforanys ∈ [n]. Iffurtherassumingthateachf is
i s i
convex,thenδ ≤L. Therefore,Assumption1isweakerthanassumingthateachf isL-smooth.
s i
Fullclientparticipation. Wefirstconsiderthecross-silosettingthatcorrespondstoarelatively
smallnumberofclientsbutarehighlyreliable,typicallyorganizationsandinstitutions,thathave
strongcomputingresourcesandstablenetworkconnections[22]. Wehavethefollowingconvergence
guaranteeforS-DANEwiths=n.
Theorem1. ConsiderAlgorithm1.Letf : Rd →Rbeµ-convexwithµ≥0foranyi∈[n]. Assume
i
that{f }n haveδ-SOD.Lets=nandλ=2δ. Foranyr ≥0,supposewehave:
i i=1
(cid:88)n
∥∇F (x )∥2 ≤
λ2 (cid:88)n
∥x −vr∥2. (6)
i,r i,r+1 4 i,r+1
i=1 i=1
ThenforanyR≥1,itholdsthat:
µD2 δD2
f(x¯R)−f⋆ ≤ ≤ ,
2[(1+ µ)R−1] R
2δ (cid:124)(cid:123)(cid:122)(cid:125)
(cid:124) (cid:123)(cid:122) (cid:125)
sublinearratewhenµ=0
linearratewhenµ>0
wherex¯R :=(cid:80)R prxr/(cid:80)R pr,p:=1+µ andD :=(cid:13) (cid:13)x0−x⋆(cid:13) (cid:13).Toachievef(x¯R)−f⋆ ≤ε,
r=1 r=1 λ
itsufficestomakeR=O(cid:16) δ+µln(cid:0)
1+
µD2(cid:1)(cid:17)
communicationrounds.
µ ε
Theorem1providestheconvergenceguaranteeforS-DANEintermsofthenumberofcommunication
rounds. Notethattherateiscontinuousinµ(since(1+ µ)R ≥ µR),anddependsonlyonδ.
2δ 2δ
Remark2. Somepreviousworksproveconvergencerateintermsofanotherconstantδ [24,31,
max
58],definedas: ∥∇h (x)−∇h (y)∥ ≤ δ ∥x−y∥foranyx,y ∈ Rd andanyi ∈ [n],where
i i max
h =f −f . (SeeforinstancethesecondlineinTable1). Notethatδisalwayssmallerthanδ ,
i i √ max
andcaninprinciplebemuchsmaller(upto ntimes).
The proven communication complexity is the same as that of DANE [20]. However, the
accuracy condition is milder. Specifically, to achieve the same guarantee, DANE requires
(cid:80)n (cid:13) (cid:13)∇F˜ (x )(cid:13) (cid:13)2 ≤ O(cid:0)δ2 (cid:80)n ∥x −xr∥2(cid:1) where F˜ is defined in (4), which in-
i=1(cid:13) i,r i,r+1 (cid:13) r2 i=1 i,r+1 i,r
cursanr2overheadinthedenominator,asinthegeneraldiscussiononproximal-pointmethodsin
Section2.1. ThenextcorollaryprovidesthelocalcomputationefficiencyofS-DANE.
Corollary3. ConsiderthesamesettingsasinTheorem1. Furtherassumethateachf isL-smooth.
i
Toachievetheaccuracyrequirement(6),withacertainfirst-orderalgorithm,eachdeviceirequires
(cid:16)(cid:113) (cid:17)
O L computationsof∇f ateachroundr.
δ i
Remark4. NotethatCorollary3isstatedforspecialalgorithms,OGM-OGbyKimandFessler
[28] or an accumulative regularization method by Lan et al. [35], that are designed for the fast
minimizationofthegradientnorm. Forthestandardgradientdescent,therequirednumberoforacle
callsisO(cid:0)L(cid:1) . Thefastgradientmethod[47]canfurtherdecreasethecomplexitytoO(cid:0)(cid:113) L lnL(cid:1) .
δ µ+δ δ
(DetailscanbefoundinRemark19.) Essentially, thedifferenceisonlyinthelogarithmicfactor
of L. Thisimpliesthateachdevicecanrunaconstantnumberofstandardlocal(F)GDstepsto
δ
approximatelysolvetheirsubproblemsinS-DANE.
Partial client participation. We next consider the cross-device setting that corresponds to a
potentially very large number of clients, typically mobile phones, that have relatively unstable
network connections and weak computation power [22]. In such scenarios, the server typically
cannotaskalltheclientstoparticipateinthecommunicationateachround. Furthermore,theclients
may typically be visited only once in the training and they are stateless [25]. Thus, we consider
S-DANEwithpartialparticipationandwithoutstoringstatesondevicesinthissetting. Toprove
convergence,itisnecessarytoassumeacertainlevelofdissimilarityamongclients. Here,weusethe
sameassumptionasin[25]thatmeasuresthegradientvariance.
6Definition3(BoundedGradientVariance[25]). Letf bearealfunctionforanyi ∈ [n]andlet
i
f := 1 (cid:80)n f . Wesaythat{f }haveζ-BGVifforanyx∈Rd:
n i=1 i i
n
1 (cid:88) ∥∇f (x)−∇f(x)∥2 ≤ζ2 . (7)
n i
i=1
Definition (3) is similar to the notion of stochastic noise considered in the centralized stochastic
gradientmethods[2]. Thedifferenceisthatwenowhavefinitenumberofsamplesandweconsider
samplingasetuniformlywithoutreplacement,whichwillberelatedtotheζ definition.
WealsousethefollowingExternalDissimilaritynotionasacomplementofDefinition1.
Definition4(ExternalDissimilarity). Letf : Rd → Rbearealfunctionforanyi ∈ [n]andlet
i
s∈[n]. Then{f }aresaidtohave∆ -EDofsizesifforanyx,y∈RdandanyS ∈(cid:0)[n](cid:1) ,itholds
i s s
that:
∥∇m (x)−∇m (y)∥≤∆ ∥x−y∥ , (8)
S S s
wherem :=f −f andf :=
1(cid:80)
f .
S S S s i∈S i
Note that Definition 1 can be seen as ’internal’ dissimilarity as the it measures the difference of
thefunctionswithinasubset. Ontheotherhand,Definition4quantifiesthesimilaritybetweenthe
functionswithinasubsetandtheoriginalfunction,whichistheexternalcomparison. Wheneach
f istwice-continuouslydifferentiable,thencondition(8)isequivalentto: ||∇2m (x)||≤∆ ,for
i S s
anyx∈Rd. Ifeachfunctionf isL-smoothandconvex,then∆ isnolargerthanLforanys∈[n].
i s
Therefore,usingbothAssumptions1and4arestillweakerthanassumingthateachf isL-smooth.
i
In what follows, we work with a new second-order dissimilarity measure: δ +∆ . Note that:
s s
δ +∆ =δ andδ +∆ =δ.
1 1 max n n
Theorem5. ConsiderAlgorithm1. Letf : Rd → Rbeµ-convexwithµ ≥ 0foranyi ∈ [n]and
i
letn≥2. Assumethat{f }n haveδ -SOD,∆ -EDandζ-BGV.Letλ= 4(n−s)ζ2 +2(δ +∆ ).
i i=1 s s s(n−1) ε s s
Foranyr ≥0,supposewehave:
1 (cid:88) ∥∇F (x )∥2 ≤ λ21 (cid:88) ∥x −vr∥2 . (9)
s i,r i,r+1 4 s i,r+1
i∈Sr i∈Sr
ToreachE[f(x¯R)]−f(x⋆)≤ε,weneedatmostthefollowingnumberofcommunicationroundsin
expectation:
(cid:18) δ +∆ +µ n−s ζ2 (cid:19) (cid:16) µD2(cid:17) (δ +∆ )D2 n−sζ2D2
R∼ s s + ln 1+ ≤ s s + ,
µ n−1sεµ ε ε n−1 sε2
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
whenµ>0 whenµ=0
wherex¯R :=(cid:80)R prxr/(cid:80)R pr,p:=1+ µ,andD :=(cid:13) (cid:13)x0−x⋆(cid:13) (cid:13).
r=1 r=1 λ
Theorem5providesthecommunicationcomplexityofS-DANEwithclientsamplingandarbitrary
(deterministic)localsolvers. Therateisagaincontinuousinµsinceln(1+x)≤xforanyx≥0.
Comparedwiththecasewheres=n,theefficiencynowdependsonthegradientvarianceζ. Note
thatthiserrortermgetsreducedwhenincreasings.
Specifically,toachievetheO(cid:0) log(1)(cid:1) andO(cid:0)1(cid:1)
ε ε
rates,itissufficienttoensurethats=Θ(cid:0) nζ2 (cid:1)
. Notably,thealgorithmcanstillreachany
ζ2+nε(δs+∆s)
targetaccuracywhenn→∞since n−s ≤1.
n−1
Deterministiclocalsolver. Notethattheaccuracyrequirement(9)isthesameas(6). Therefore,the
discussionsthereinarealsovalidinthepartialclientparticipationsettings. Ifeachf isL-smooth,
i
then according to Corollary 3, with certain gradient methods, the number of oracle calls of ∇f
i
(cid:16)(cid:113) (cid:17)
requiredateachroundisO L . Ifthestandard(fast)gradientdescentisused,thenwereferto
λ
thediscussioninRemark4(δisreplacedwithλeverywhere).
Stochasticlocalsolver. Wehighlightthatitisalsopossibletousestochasticoptimizationalgorithms
asthelocalsolvers. WereferthereaderstothedetaileddiscussionsinSectionC.3.1.
7Algorithm2Acc-S-DANE
1: Input:λ>0,µ≥0,x0 =v0 ∈Rd,s∈[n].
2: SetB =1,A =0.
0 0
3: forr=0,1,2...do
4: Finda >0fromtheequationλ= (Ar+ar+1)Br.
r+1 a2
r+1
5: A =A +a ,B =B +µa .
r+1 r r+1 r+1 r r+1
6: yr = Ar xr+ ar+1vr.
Ar+1 Ar+1
7: SampleS
∈(cid:0)[n](cid:1)
uniformlyatrandomwithoutreplacement.
r s
8: foreachdevicei∈S inparalleldo
9: x ≈argminr (cid:8) F (x):=f (x)+⟨∇f (yr)−∇f (yr),x⟩+ λ∥x−yr∥2(cid:9) .
i,r+1 x∈Rd i,r i Sr i 2
10: xr+1 = 1(cid:80) x .
11: vr+1
=as rgmi∈ inSr(cid:8) Gi,r (+ x1
):= ar+1 (cid:80) [⟨∇f (x ),x⟩+ µ∥x−x ∥2]+ Br ∥x−vr∥2(cid:9) .
r s i i,r+1 2 i,r+1 2
x∈Rd i∈Sr
4 AcceleratedS-DANE
Inthissection,weintroducetheacceleratedversionofS-DANEthatachievesbettercommunication
complexity. WefirstpresentthemainresultofAcc-S-DANE(Alg.2)forfullclientparticipation.
Theorem6. ConsiderAlgorithm2.Letf :Rd →Rbeµ-convexwithµ≥0foranyi∈[n].Assume
i
that{f }n haveδ-SOD.Lets=nandλ=2δ.Foranyr ≥0,suppose(cid:80)n ∥∇F (x )∥2 ≤
i i=1 i=1 i,r i,r+1
δ2(cid:80)n ∥x −yr∥2. Ifµ≤8δ,thenforanyR≥1,itholdsthat:
i=1 i,r+1
2µD2 4δD2
f(xR)−f⋆ ≤ ≤ ,
(cid:104)(cid:0) 1+(cid:112)µ(cid:1)R −(cid:0) 1−(cid:112)µ(cid:1)R(cid:105)2 (cid:124)R (cid:123)(cid:122)2
(cid:125)
8δ 8δ
whenµ=0
(cid:124) (cid:123)(cid:122) (cid:125)
whenµ>0
whereD :=(cid:13) (cid:13)x0−x⋆(cid:13) (cid:13). Ifµ≥8δ,thenwehave,foranyR≥1: f(xR)−f⋆ ≤
(cid:0)
1+√4δ µD (cid:1)2 2R−2. To
(cid:18)(cid:113) (cid:16) (cid:113) (cid:17)(cid:19)8δ
achievef(xR)−f⋆ ≤ε,itsufficestoperformR=O δ+µln 1+ min{µ,δ}D2 communi-
µ ε
cationrounds.
InTheorem6,wepresentconvergenceratesforthefunctionresidual(thesquareddistancetothe
solutioncanbederivedvia: µ(cid:13) (cid:13)xR−x⋆(cid:13) (cid:13)2 ≤f(xR)−f⋆). Letusconsidertheinterestingregime
2
whereµ≤8δ. Theestimateisagaincontinuousinµ. Weseethatthecommunicationcomplexity
oftheacceleratedversionisimprovedfromO(cid:0)δ log(1)(cid:1) toO(cid:0)(cid:113) δ log(1)(cid:1) whenµ>0,andfrom
µ ε µ ε
O(cid:0)δD2(cid:1)
to
O(cid:0)δ√D2(cid:1)
when µ = 0, with the same accuracy condition for solving the subproblem.
ε ε
ComparedwithAcc-Extragradient,theratedependsonabetterconstantδinsteadofδ .
max
Localcomputation. NotethatwecansatisfytheaccuracyconditioninTheorem6inexactlythe
samewayasinCorollary3. Ifeachf isL-smooth,withcertainfirst-orderalgorithms,eachdevice
i
(cid:16)(cid:113) (cid:17)
i requires O L computations of ∇f at each round r. The standard (fast) gradient method
δ i
discussedinRemark4canalsobeappliedtothissetting.
StabilizedCatalyst. LetushighlightthatAlgorithm2givesadistributedframeworkforageneric
accelerationscheme,thatappliestoalargeclassoflocaloptimizationmethods—inthesamespiritas
inthefamousCatalyst[39]frameworkthatappliestothecasewheren=1. However,incontrastto
Catalyst,thisstabilizedversionremovesthelogarithmicoverheadpresentintheoriginalmethod.
Specifically,whenapplyingTheorem6withn=1andλ=Lforasmoothconvexfunctionf,we
(cid:13) (cid:13) (cid:13) (cid:13)
recover the same rate as Catalyst. The accuracy condition (cid:13)∇F r(xr+1)(cid:13) ≤ L(cid:13)xr+1−yr(cid:13), or
equivalently(cid:10) ∇f(xr+1),yr−xr+1(cid:11) ≥ 1 (cid:13) (cid:13)∇f(xr+1)(cid:13) (cid:13)2 canbeachievedwithonegradientstep:
2L
xr+1 :=yr− 1∇f(yr)(Lemma5[46]).
L
Partialclientparticipation. Itiswellknownthatacceleratedstochasticgradientmethodsarenot
abletoimprovethecomplexityinthestochasticpartcomparedwiththebasicmethods[10]. Asimilar
resultisalsoshownforouraccelerateddistributedmethod.
8104 104
103 103
102 102
101 101
100 100
0 1000 2000 3000 4000 5000 6000 0 10000 20000 30000 40000 50000 60000
Communication rounds Total number of gradient oracle calls
DANE-GD S-DANE-GD (ours)
Figure2:ComparisonofS-DANEagainstDANEforsolvingaconvexquadraticminimizationproblemwiththe
samenumberoflocalsteps.
Theorem 7. Consider Algorithm 2 under the same settings as in Theorem 5. Let λ =
(cid:16) (cid:17)
Θ (δ + ∆ ) + (n−s)Rζ2 . For any r ≥ 0, suppose we have 1(cid:80) ∥∇F (x )∥2 ≤
s s s(n−1) ε s i∈Sr i,r i,r+1
O(cid:16) λ 421 s(cid:80) i∈SrE ξi,r[∥x i,r+1−vr∥2](cid:17) . DenoteD := (cid:13) (cid:13)x0−x⋆(cid:13) (cid:13). Assumethatµ ≤ 4λ. Toreach
E[f(xR)−f(x⋆)]≤ε,weneedatmostthefollowingnumberofcommunicationroundsinexpecta-
tion:
(cid:115) δ +∆ (cid:18) (cid:114) µD2(cid:19) n−s ζ2 (cid:18) (cid:114) µD2(cid:19) (cid:114) (δ +∆ )D2 n−sζ2D2
R∼ s s ln 1+ + ln2 1+ ≤ s s + .
µ ε n−1sεµ ε ε n−1 sε2
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) whenµ=0
(whenµ>0)
Theerrortermthatdependsonζ2andεisatthesamescaleasS-DANE,i.e. O(ζ2 )whenµ>0and
ε
O(ζ2
)whenµ=0. Nevertheless,whensislargeenoughsuchthatthisseconderrorbecomesno
ε2
largerthanthefirstoptimizationterm,thenAcc-S-DANEcanstillbefasterthanS-DANE.
5 Numericalexperiments
Inthissection,weillustratetheperformanceoftwoproposedmethodsinnumericalexperiments.
Convex quadratic minimization. We first illustrate the properties of our algorithms as ap-
plied to minimizing a simple quadratic function: f(x) := 1 (cid:80)n f (x) where f (x) :=
n i=1 i i
1 (cid:80)m 1⟨A (x−b ),x−b ⟩whereb ∈RdandA ∈Rd×d. Wegeneraterandomvec-
m j=1 2 i,j i,j i,j i,j i,j
tors{b }anddiagonalmatrices{A }inthesamewayasin[20]suchthatmax {∥A ∥}=100
i,j i,j i,j i,j
andδ ≈5. Weusen=10,m=5andd=1000. WecompareS-DANEandAcc-S-DANEwith
DANE.Weusethestandardgradientdescent(GD)withconstantstepsize 1 ≤ 1 forallthree
200 2L
methodsasthelocalsolver,whereListhesmoothnessconstantofeachf . Weuseλ = 5forall
i
threemethods. Weusethestoppingcriterion∥∇F (x )∥≤λ∥x −vr∥forourmethods
i,r i,r+1 i,r+1
(cid:13) (cid:13)
(vr becomesyr fortheacceleratedmethod). Weuse(cid:13)∇F˜ (x )(cid:13) ≤ λ ∥x −xr∥for
(cid:13) i,r i,r+1 (cid:13) r+1 i,r+1
DANE.FromFigure1,weseethatS-DANEconvergesasfastasDANEintermsofcommunication
rounds,butwithmuchfewerlocalgradientoraclecalls. Acc-S-DANEachievesthebestperformance
amongthethreemethods. WealsotestS-DANEandDANEwiththesamefixednumberoflocal
steps. TheresultcanbeseeninFigure2whereS-DANEisagainmoreefficient.
Strongly-convexpolyhedronfeasibilityproblem.Wenowconsidertheproblemoffindingafeasible
point x⋆ inside a polyhedron: P = ∩n P , where P = {x : ⟨a ,x⟩ ≤ b ,∀j = 1,...,m }
i=1 i i i,j i,j i
anda ,b ∈Rd. Eachindividualfunctionisdefinedasf := n (cid:80)mi [⟨a ,x⟩−b ]2 where
i,j i,j i m j=1 i,j i,j +
(cid:80)n m =m. Weusem=105 andd=103. Wefirstgeneratex⋆ randomlyfromaspherewith
i=1 i
radius106. Wethenfollow[53]togenerate(a ,b )suchthatx⋆isafeasiblepointofP andthe
i,j i,j
initialpointofalloptimizersisoutsidethepolyhedron. Wechoosethebestλfrom{10i}3 . We
i=−3
firstconsiderthefullclientparticipationsettingandusen = s = 10. Wecompareourproposed
methodswithGD,DANEwithGD[55],Scaffold[24],Scaffnew[42],FedProxwithGD[37]and
Acc-Extragradient[31]. TheresultisshowninthefirstplotofFigure3whereourproposedmethods
9
f-)x(f f-)x(fn=10, s=10 n=100, s=80 n=100, s=40 n=100, s=10
109
1011
1010 1010
108
105
105
107 11 00 68
101 102 104 104
103 101 101 102
107 104 102 100
0 25 50 75 100 0 50 100 150 200 0 50 100 150 200 0 100 200 300 400
Communication rounds Communication rounds Communication rounds Communication rounds
S-DANE-GD (ours) GD DANE-GD Scaffold Scaffnew FedProx-GD Acc-S-DANE-GD (ours) AccGradSliding
Figure3:Comparisonsofdifferentalgorithmsforsolvingthepolyhedronfeasibilityproblem.
areconsistentlythebestamongallthesealgorithms.Wenextexperimentwithclientsamplinganduse
n=100. Wedecreasethenumberofsampledclientsfroms=80tos=10. Fromthesamefigure,
weseethattheimprovementofAcc-S-DANEoverS-DANEgraduallydisappearsassdecreases.
Adaptive choice of λ. The current version of S-DANE uses a constant λ that should be set
proportionaltothesecond-orderdissimilarity.NotethatDefinition1assumesthattheinequalityholds
foranyxandyinRd,whichistheworst-caseconstantdissimilarityquantity. However,intheproof
forS-DANEanditsacceleratedversion,itissufficienttoonlyconsiderthedissimilaritybetweentwo
localpointsateachround. Letusillustratethepossibilityofimprovement. Here,weconsiders=n
(cid:114)
andsetλ
0
=10−2andλ
r
= n1 (cid:80)n i=1∥∇ ∥vhi r( −v vr) r− −1∇ ∥h 2i(vr−1)∥2 forS-DANEtoapproximatethelocal
dissimilarity,whichcanbeeasilyimplementedbytheserver. Weconsiderthestandardregularized
logisticregression: f(x) = 1 (cid:80)n f (x)withf (x) := n (cid:80)mi log(1+exp(−y ⟨a ,x⟩))+
n i=1 i i M j=1 i,j i,j
1 ∥x∥2where(a ,y )∈Rd+1arefeaturesandlabelsandM :=(cid:80)n m isthetotalnumber
2M i,j i,j i=1 i
ofdatapointsinthetrainingdataset. WeusetheijcnndatasetfromLIBSVM[5]. Wesplitthedataset
into 10 subsets according to the Dirichlet distribution with α = 2 (i.i.d) and α = 0.2 (non-i.i.d).
FromFigure5,weseethatS-DANEwithadaptiveλconvergesmuchfasterthantheotherbest-tuned
algorithms. In particular, the only parameter that is different from DANE-GD is λ. Therefore,
exploitinglocalsecond-orderdissimilarityseemsapromisingfuturedirection,
Deep learning task. Finally, We consider the
CIFAR10-ResNet18
multi-classclassificationtaskswithCIFAR10[32] 0.9
using ResNet-18 [17]. We simulate the experi- 100 0.8
0.7
ment on one NVIDIA DGX A100. We split the
0.6
trainingdatasetinton=10partsaccordingtothe
0.5
Dirichletdistributionwithα=0.5. WeuseSGD 101 0.4
withabatchsizeof512asalocalsolverforeach 0.3
0 20 40 60 80 0 20 40 60 80
device. ForallthemethodsconsideredinFigure4, Communication rounds Communication rounds
S-DANE (DL)-SGD (ours) Scaffold FedProx-SGD
wechoosethebestnumberoflocalstepsamong DANE-SGD Scaffnew FedAvg
{10,20,...80}(forScaffnew,thisbecomesthein-Figure4: ComparisonofS-DANEwithoutcontrolvari-
verseoftheprobability)andthebestlearningrateateagainstotherpopularoptimizersonmulti-classclas-
among{0.02,0.05,0.1}. Forthisparticulartask, sificationtaskswithCIFAR10datasetsusingResNet18.
it is often observed that using control variate makes the training less efficient [36, 20].
We here remove the control variate term on line 6 in S-DANE which is defined as
⟨x,∇f (vr)−∇f (vr)⟩. Moreover, if we write the explicit formula for vr+1 on line 8, it
becomei s vr+1 = S γr xr+1 +(1−γ)vr −η1(cid:80) ∇f (x ) with γ ∈ [0,1] and η > 0. We
set γ = 0.99 and η to be the local
learnings ratei∈ inSr ouri expi e,r r+ im1
ent. The method can be found in
Algorithm3. NotethattheonlydifferencebetweenitandFedProxisthechoiceoftheprox-center.
Thebestnumberoflocalstepsforthealgorithmswithoutusingcontrolvariatesis70whileforthe
othersis10(otherwise,thetraininglossexplodes). FromFigure4,weseethatS-DANEreaches
90%accuracywithin50communicationroundswhilealltheothermethodsarestillbelow90%after
80 epochs. The effectiveness of S-DANE on the training of other deep learning models such as
Transformerrequiresfurtherexploration.
Implementation. To implement Algorithms 1 and 2 (Algorithm 3 with option 2 is the same as
Algorithm1),eachdevicehasthefreedomtoemployanyefficientoptimizationalgorithm,depending
on its computation power and the local data size. At each communication round r, these local
algorithmsarecalledtoapproximatelysolvethesub-problemsdefinedby{F },untilthegradient
i,r
10
f-)x(f f-)x(f f-)x(f
ssol
niarT
f-)x(f
yrauccA
tseTijcnn with non-iid splitting Second-order Dissimilarity ijcnn with iid splitting Second-order Dissimilarity
101
102
102 102
103 106
103
105
103
1010
107 1014
104
0 20 40 0 20 40 0 20 40 0 20 40
Communication rounds Communication rounds Communication rounds Communication rounds
S-DANE-GD (ours) GD DANE-GD Scaffold Scaffnew FedProx-GD
Figure5:IllustrationoftheimpactofadaptiveλusedinS-DANEontheconvergenceofaregularizedlogistic
regressionproblemontheijcnndataset[5].
Algorithm3S-DANE(DL)
1: Input:λ>0,η>0,γ ∈[0,1],x0 =v0 ∈Rd,s∈[n]
2: forr=0,1,2...do
3: SampleS
∈(cid:0)[n](cid:1)
uniformlyatrandomwithoutreplacement
r s
4: foreachdevicei∈S inparalleldo
r (cid:8) (cid:9)
5: Setx ≈argmin F (x) ,where
i,r+1 x∈Rd i,r
6: λ
F (x):=f (x)−⟨x,∇f (vr)−∇f (vr)⟩+ ∥x−vr∥2 . (option1)
i,r i i Sr 2
7: λ
F (x):=f (x)+ ∥x−vr∥2 . (option2)
i,r i 2
8: Setxr+1 = 1(cid:80) x
9: Setvr+1 =γs xr+i∈ 1S +r (1i,r −+1 γ)vr−η1(cid:80) ∇f (x )
s i∈Sr i i,r+1
normsatisfiesacertainaccuracyconditionthatisstatedinthecorrespondingtheorems. Theserver
onlyneedstoperformbasicvectoroperations. NotethatG definedinthosealgorithmshasaunique
r
solutionsothatvr+1canbeexplicitlyderived(inthesameformasline9inAlgorithm3).
6 Conclusion
Wehaveproposednewfederatedoptimizationalgorithms(bothbasicandaccelerated)thatsimultane-
ouslyachievethebest-knowncommunicationandlocalcomputationcomplexities. Moreover,thenew
algorithmshaveseveralfeaturessuchasallowingpartialparticipationandarbitrarystochasticlocal
solvers,makingthemattractiveinpractice. However,weassumethateachfunctionf isµ-convexin
i
allthetheorems. Thisisstrongerthanassumingonlyµ-convexityoff,whichisusedinsomeprior
works. Thefuturedirectionsincludeconsiderationofweakerassumptions,studyofadaptiveλ,and
bothempiricalandtheoreticalanalysisfornon-convexproblems.
Acknowledgments
TheauthorsaregratefultoAdrienTaylorandThomasPethickforthereferenceto[57].
References
[1] DurmusAlpEmreAcar,YueZhao,RamonMatas,MatthewMattina,PaulWhatmough,andVenkatesh
Saligrama. Federatedlearningbasedondynamicregularization. InInternationalConferenceonLearning
Representations,2021. URLhttps://openreview.net/forum?id=B7v4QMR6Z9w.
[2] LéonBottou,FrankECurtis,andJorgeNocedal. Optimizationmethodsforlarge-scalemachinelearning.
SIAMreview,60(2):223–311,2018.
[3] SébastienBubecketal. Convexoptimization:Algorithmsandcomplexity. FoundationsandTrends®in
MachineLearning,8(3-4):231–357,2015.
[4] YairCarmon,ArunJambulapati,YujiaJin,andAaronSidford. Recapp:Craftingamoreefficientcatalyst
forconvexoptimization. InInternationalConferenceonMachineLearning,pages2658–2685.PMLR,
2022.
11
||)x(f
||
r
||)x(f
||
r[5] Chih-ChungChangandChih-JenLin. LIBSVM:Alibraryforsupportvectormachines. ACMTrans-
actions on Intelligent Systems and Technology, 2:27:1–27:27, 2011. Software available at http:
//www.csie.ntu.edu.tw/~cjlin/libsvm.
[6] ElMahdiChaytiandSaiPraneethKarimireddy. Optimizationwithaccesstoauxiliaryinformation. arXiv
preprintarXiv:2206.00395,2022.
[7] LaurentCondatandPeterRichtárik. Randprox: Primal-dualoptimizationalgorithmswithrandomized
proximalupdates. InOPT2022:OptimizationforMachineLearning(NeurIPS2022Workshop),2022.
URLhttps://openreview.net/forum?id=mejxBCu9EXc.
[8] LaurentCondat,GrigoryMalinovsky,andPeterRichtárik. Tamuna:Acceleratedfederatedlearningwith
localtrainingandpartialparticipation. arXivpreprintarXiv:2302.09832,2023.
[9] AaronDefazio,FrancisBach,andSimonLacoste-Julien. Saga:Afastincrementalgradientmethodwith
supportfornon-stronglyconvexcompositeobjectives. Advancesinneuralinformationprocessingsystems,
27,2014.
[10] OlivierDevolder. Stochasticfirstordermethodsinsmoothconvexoptimization. COREDiscussionPapers,
2011/70,2011.
[11] NikitaDoikovandYuriiNesterov. Contractingproximalmethodsforsmoothconvexoptimization. SIAM
JournalonOptimization,30(4):3146–3169,2020.
[12] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and
stochasticoptimization. JournalofMachineLearningResearch,12(61):2121–2159,2011. URLhttp:
//jmlr.org/papers/v12/duchi11a.html.
[13] Alexandred’Aspremont,DamienScieur,AdrienTaylor,etal. Accelerationmethods. Foundationsand
Trends®inOptimization,5(1-2):1–245,2021.
[14] LiangGao,HuazhuFu,LiLi,YingwenChen,MingXu,andCheng-ZhongXu. Feddc:Federatedlearning
withnon-iiddatavialocaldriftdecouplingandcorrection. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pages10112–10121,2022.
[15] MichałGrudzien´,GrigoryMalinovsky,andPeterRichtárik. Can5thgenerationlocaltrainingmethods
supportclientsampling?yes! InInternationalConferenceonArtificialIntelligenceandStatistics,pages
1055–1092.PMLR,2023.
[16] OsmanGüler. Newproximalpointalgorithmsforconvexminimization. SIAMJournalonOptimization,2
(4):649–664,1992. doi:10.1137/0802032. URLhttps://doi.org/10.1137/0802032.
[17] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecognition.
In2016IEEEConferenceonComputerVisionandPatternRecognition(CVPR),pages770–778,2016.
doi:10.1109/CVPR.2016.90.
[18] Hadrien Hendrikx, Lin Xiao, Sebastien Bubeck, Francis Bach, and Laurent Massoulie. Statistically
preconditionedacceleratedgradientmethodfordistributedoptimization. InInternationalconferenceon
machinelearning,pages4203–4227.PMLR,2020.
[19] ZhengmianHuandHengHuang. TighteranalysisforProxSkip. InAndreasKrause,EmmaBrunskill,
KyunghyunCho,BarbaraEngelhardt,SivanSabato,andJonathanScarlett,editors,Proceedingsofthe
40thInternationalConferenceonMachineLearning,volume202ofProceedingsofMachineLearning
Research,pages13469–13496.PMLR,23–29Jul2023.URLhttps://proceedings.mlr.press/v202/
hu23a.html.
[20] XiaowenJiang,AntonRodomanov,andSebastianUStich.Federatedoptimizationwithdoublyregularized
drift correction. In Forty-first International Conference on Machine Learning, 2024. URL https:
//openreview.net/forum?id=JD03zxWZzs.
[21] RieJohnsonandTongZhang. Acceleratingstochasticgradientdescentusingpredictivevariancereduction.
InProceedingsofthe26thInternationalConferenceonNeuralInformationProcessingSystems-Volume
1,NIPS’13,page315–323.CurranAssociatesInc.,2013.
[22] PeterKairouz,H.BrendanMcMahan,BrendanAvent,AurélienBellet,MehdiBennis,ArjunNitinBhagoji,
KeithBonawitz,ZacharyCharles,GrahamCormode,RachelCummings,RafaelG.L.D’Oliveira,SalimEl
Rouayheb,DavidEvans,JoshGardner,ZacharyGarrett,AdriàGascón,BadihGhazi,PhillipB.Gibbons,
MarcoGruteser,ZaidHarchaoui,ChaoyangHe,LieHe,ZhouyuanHuo,BenHutchinson,JustinHsu,
MartinJaggi,TaraJavidi,GauriJoshi,MikhailKhodak,JakubKonecˇný,AleksandraKorolova,Farinaz
12Koushanfar,SanmiKoyejo,TancrèdeLepoint,YangLiu,PrateekMittal,MehryarMohri,RichardNock,
AyferÖzgür,RasmusPagh,MarianaRaykova,HangQi,DanielRamage,RameshRaskar,DawnSong,
Weikang Song, Sebastian U. Stich, Ziteng Sun, Ananda Theertha Suresh, Florian Tramèr, Praneeth
Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, and Sen Zhao.
Advancesandopenproblemsinfederatedlearning. FoundationsandTrends®inMachineLearning,14
(1–2):1–210,2021. URLhttps://arxiv.org/pdf/1912.04977.pdf.
[23] AvetikKaragulyan,EgorShulgin,AbdurakhmonSadiev,andPeterRichtárik. Spam:Stochasticproximal
pointmethodwithmomentumvariancereductionfornon-convexcross-devicefederatedlearning. arXiv
preprintarXiv:2405.20127,2024.
[24] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian Stich, and
Ananda Theertha Suresh. Scaffold: Stochastic controlled averaging for federated learning. In Inter-
nationalconferenceonmachinelearning,pages5132–5143.PMLR,2020.
[25] SaiPraneethKarimireddy,MartinJaggi,SatyenKale,MehryarMohri,SashankReddi,SebastianU.Stich,
andAnandaTheerthaSuresh. Breakingthecentralizedbarrierforcross-devicefederatedlearning. In
AdvancesinNeuralInformationProcessingSystems,2021.
[26] Ahmed Khaled and Chi Jin. Faster federated optimization under second-order similarity. In The
EleventhInternationalConferenceonLearningRepresentations,2023.URLhttps://openreview.net/
forum?id=ElC6LYO4MfD.
[27] AhmedKhaled,KonstantinMishchenko,andPeterRichtárik. Firstanalysisoflocalgdonheterogeneous
data. arXivpreprintarXiv:1909.04715,2019.
[28] DonghwanKimandJeffreyAFessler. Generalizingtheoptimizedgradientmethodforsmoothconvex
minimization. SIAMJournalonOptimization,28(2):1920–1950,2018.
[29] JakubKonecˇny`,HBrendanMcMahan,DanielRamage,andPeterRichtárik. Federatedoptimization:
Distributedmachinelearningforon-deviceintelligence. arXivpreprintarXiv:1610.02527,2016.
[30] Jakub Konecˇny`, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha Suresh, and
DaveBacon. Federatedlearning: Strategiesforimprovingcommunicationefficiency. arXivpreprint
arXiv:1610.05492,2016.
[31] DmitryKovalev,AleksandrBeznosikov,EkaterinaBorodich,AlexanderGasnikov,andGesualdoScutari.
Optimalgradientslidinganditsapplicationtooptimaldistributedoptimizationundersimilarity. Advances
inNeuralInformationProcessingSystems,35:33494–33507,2022.
[32] AlexKrizhevsky,VinodNair,andGeoffreyHinton. Cifar-10(canadianinstituteforadvancedresearch).
URLhttp://www.cs.toronto.edu/~kriz/cifar.html.
[33] GuanghuiLan. Gradientslidingforcompositeoptimization. MathematicalProgramming,159:201–235,
2016.
[34] GuanghuiLanandYuyuanOuyang.Acceleratedgradientslidingforstructuredconvexoptimization.arXiv
preprintarXiv:1609.04905,2016.
[35] GuanghuiLan, YuyuanOuyang, andZheZhang. Optimalandparameter-freegradientminimization
methodsforsmoothoptimization. arXivpreprintarXiv:2310.12139,2023.
[36] BoLi,MikkelNSchmidt,TommySAlstrøm,andSebastianUStich. Ontheeffectivenessofpartial
variance reduction in federated learning with heterogeneous data. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages3964–3973,2023.
[37] TianLi,AnitKumarSahu,ManzilZaheer,MaziarSanjabi,AmeetTalwalkar,andVirginiaSmith.Federated
optimizationinheterogeneousnetworks. ProceedingsofMachinelearningandsystems,2:429–450,2020.
[38] DachaoLin,YuzeHan,HaishanYe,andZhihuaZhang. Stochasticdistributedoptimizationunderaverage
second-ordersimilarity:Algorithmsandanalysis. AdvancesinNeuralInformationProcessingSystems,36,
2024.
[39] Hongzhou Lin, Julien Mairal, and Zaid Harchaoui. A universal catalyst for first-order optimization.
Advancesinneuralinformationprocessingsystems,28,2015.
[40] B.Martinet. Perturbationdesméthodesd’optimisation.Applications. RAIRO.Analysenumérique,12(2):
153–171,1978. URLhttp://www.numdam.org/item/M2AN_1978__12_2_153_0/.
13[41] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas.
Communication-efficientlearningofdeepnetworksfromdecentralizeddata. InArtificialintelligenceand
statistics,pages1273–1282.PMLR,2017.
[42] KonstantinMishchenko,GrigoryMalinovsky,SebastianStich,andPeterRichtárik. Proxskip:Yes!local
gradientstepsprovablyleadtocommunicationacceleration! finally! InInternationalConferenceon
MachineLearning,pages15750–15769.PMLR,2022.
[43] Konstantin Mishchenko, Rui Li, Hongxiang Fan, and Stylianos Venieris. Federated learning under
second-orderdataheterogeneity,2024. URLhttps://openreview.net/forum?id=jkhVrIllKg.
[44] Renato D. C. Monteiro and B. F. Svaiter. An accelerated hybrid proximal extragradient method for
convexoptimizationanditsimplicationstosecond-ordermethods. SIAMJournalonOptimization,23(2):
1092–1125,2013. doi:10.1137/110833786. URLhttps://doi.org/10.1137/110833786.
[45] J.J.Moreau.Proximitéetdualitédansunespacehilbertien.BulletindelaSociétéMathématiquedeFrance,
93:273–299,1965. doi: 10.24033/bsmf.1625. URLhttp://www.numdam.org/articles/10.24033/
bsmf.1625/.
[46] Yu.Nesterov. Gradientmethodsforminimizingcompositefunctions. MathematicalProgramming,140
(1):125–161,2013. doi:10.1007/s10107-012-0629-5. URLhttps://doi.org/10.1007/s10107-012-
0629-5.
[47] YuriiNesterov.LecturesonConvexOptimization.SpringerPublishingCompany,Incorporated,2ndedition,
2018. ISBN3319915770.
[48] YuriiNesterovandSebastianU.Stich.Efficiencyoftheacceleratedcoordinatedescentmethodonstructured
optimizationproblems. SIAMJournalonOptimization,27(1):110–123,2017. doi:10.1137/16M1060182.
URLhttps://doi.org/10.1137/16M1060182.
[49] NealParikhandStephenBoyd. Proximalalgorithms. FoundationsandTrends®inOptimization,1(3):
127–239, 2014. ISSN 2167-3888. doi: 10.1561/2400000003. URL http://dx.doi.org/10.1561/
2400000003.
[50] Kumar Kshitij Patel, Lingxiao Wang, Blake E Woodworth, Brian Bullins, and Nati Sre-
bro. Towards optimal communication complexity in distributed non-convex optimization. In
S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances
in Neural Information Processing Systems, volume 35, pages 13316–13328. Curran Asso-
ciates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
56bd21259e28ebdc4d7e1503733bf421-Paper-Conference.pdf.
[51] SashankJReddi,JakubKonecˇny`,PeterRichtárik,BarnabásPóczós,andAlexSmola. Aide: Fastand
communicationefficientdistributedoptimization. arXivpreprintarXiv:1608.06879,2016.
[52] RTyrrellRockafellar. Monotoneoperatorsandtheproximalpointalgorithm. SIAMjournaloncontroland
optimization,14(5):877–898,1976.
[53] AntonRodomanov,XiaowenJiang,andSebastianStich. Universalityofadagradstepsizesforstochastic
optimization:Inexactoracle,accelerationandvariancereduction. arXivpreprintarXiv:2406.06398,2024.
[54] AbdurakhmonSadiev,DmitryKovalev,andPeterRichtárik. Communicationaccelerationoflocalgradient
methodsviaanacceleratedprimal-dualalgorithmwithaninexactprox. InAliceH.Oh,AlekhAgarwal,
DanielleBelgrave,andKyunghyunCho,editors,AdvancesinNeuralInformationProcessingSystems,
2022. URLhttps://openreview.net/forum?id=W72rB0wwLVu.
[55] OhadShamir,NatiSrebro,andTongZhang. Communication-efficientdistributedoptimizationusingan
approximatenewton-typemethod. InInternationalconferenceonmachinelearning,pages1000–1008.
PMLR,2014.
[56] M.V.SolodovandB.F.Svaiter. Aunifiedframeworkforsomeinexactproximalpointalgorithms*. Nu-
mericalFunctionalAnalysisandOptimization,22(7-8):1013–1035,2001. doi:10.1081/NFA-100108320.
URLhttps://doi.org/10.1081/NFA-100108320.
[57] SvaiterB.F.Solodov,M.V. Ahybridprojection-proximalpointalgorithm. JournalofConvexAnalysis,6
(1):59–70,1999. URLhttp://eudml.org/doc/120958.
[58] YingSun,GesualdoScutari,andAmirDaneshmand. Distributedoptimizationbasedongradienttracking
revisited:Enhancingconvergencerateviasurrogation. SIAMJournalonOptimization,32(2):354–385,
2022.
14[59] YeTian,GesualdoScutari,TianyuCao,andAlexanderGasnikov. Accelerationindistributedoptimization
undersimilarity. InInternationalConferenceonArtificialIntelligenceandStatistics,pages5721–5756.
PMLR,2022.
[60] FarshidVarno,MarzieSaghayi,LayaRafieeSevyeri,SharutGupta,StanMatwin,andMohammadHavaei.
Adabest:Minimizingclientdriftinfederatedlearningviaadaptivebiasestimation.InEuropeanConference
onComputerVision,pages710–726.Springer,2022.
[61] BlakeEWoodworth,KumarKshitijPatel,andNatiSrebro. Minibatchvslocalsgdforheterogeneous
distributedlearning. AdvancesinNeuralInformationProcessingSystems,33:6281–6292,2020.
[62] HonglinYuanandTengyuMa. Federatedacceleratedstochasticgradientdescent. AdvancesinNeural
InformationProcessingSystems,33:5332–5344,2020.
15Appendix
Contents
1 Introduction 1
2 Problemsetupandbackground 3
2.1 Proximal-pointmethodsonasinglemachine . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 Distributedproximal-pointmethods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3 StabilizedDANE 5
4 AcceleratedS-DANE 8
5 Numericalexperiments 9
6 Conclusion 11
A Morerelatedwork 17
B TechnicalPreliminaries 17
B.1 BasicDefinitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B.2 UsefulLemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
C ProofsforS-DANE(Alg.1) 21
C.1 One-steprecurrence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.2 Fullclientparticipation(proofofTheorem1) . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.3 Partialclientparticipation(proofofTheorem5) . . . . . . . . . . . . . . . . . . . . . . . . 23
C.3.1 Stochasticlocalsolver. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D ProofsforAcceleratedS-DANE(Alg.2) 26
D.1 One-steprecurrence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
D.2 Fullclientparticipation(proofofTheorem6) . . . . . . . . . . . . . . . . . . . . . . . . . . 27
D.3 Partialclientparticipation(proofofTheorem7) . . . . . . . . . . . . . . . . . . . . . . . . 27
D.3.1 Stochasticlocalsolver. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
16A Morerelatedwork
Inthefirstseveralyearsofthedevelopmentforfederatedlearningalgorithms,theconvergenceguaranteesare
focusedonthesmoothnessparameterL.ThedefactostandardalgorithmforfederatedlearningisFedAvg.It
reducesthecommunicationfrequencybydoingmultipleSGDstepsonavailableclientsbeforecommunication,
whichworkswellinpractice[41].However,intheory,iftheheterogeneityamongclientsislarge,thenitsuffers
fromtheso-calledclientdriftphenomenon[24]andmightbeworsethancentralizedmini-batchSGD[61,27].
Numerouseffortshavebeenmadetomitigatethisdriftimpact.FedProxaddsanadditionalregularizertothe
subproblemofeachclientbasedontheideaofcentralizedproximalpointmethodtolimitthedriftofeach
client.However,thecommunicationcomplexitystilldependsontheheterogeneity.Thecelebratedalgorithm
Scaffoldappliesdriftcorrection(similartovariance-reduction)totheupdateofFedAvganditsuccessfully
removestheimpactoftheheterogeneity. Afterwards,theideaofdriftcorrectionisemployedinmanyother
works[14,60,36,1].Scaffnewusesaspecialchoiceofcontrolvariate[42]andfirstillustratestheusefulnessof
takingstandardlocalgradientstepsunderstrongy-convexity,followedwithmoreadvancedmethodswithrefined
analysisandfeaturessuchasclientsamplingandcompression[19,8,15,7].Later,Sadievetal.[54]proposed
APDAwithInexactProxthatretainsthesamecommunicationcomplexityasScaffnew,butfurtherprovably
reducesthelocalcomputationcomplexity. FedAC[62]appliesnesterov’saccelerationinthelocalstepsand
showsprovablybetterconvergencethanFedAvgundercertainassumptions.
MorerecentworkstrytodevelopalgorithmswithguaranteesthatrelyonapotentiallysmallerconstantthanL.
ScaffoldfirstillustratestheusefulnessoftakinglocalstepsforquadraticsunderBoundedHessianDissimilarity
δ [24].SONATA[58]anditsacceleratedversion[59]proveexplicitcommunicationreductionintermsofδ
B B
understrongconvexity.Mime[25]andCE-LGD[50]workonnon-convexsettingsandshowthecommunication
improvementonδ andthelatterachievesthemin-maxoptimalrates.AcceleratedExtraGradientsliding[31]
B
appliesgradientsliding[31]techniqueandshowscommunicationreductionintermsofδ forstrongly-convex
B
and convex functions, the local computation of which is also efficient without logarithmic dependence on
the target accuracy, DANE with inexact local solvers [55, 20, 51] has been shown recently to achieve the
communicationdependencyonδunderconvexityandδ undernon-convexity.Forsolvingconvexproblems,
B
the local computation efficiency depends on the target accuracy ε. Otherwise, the accuracy condition for
thesubproblemshouldincreaseacrossthecommunicationrounds. Hendrikxetal.[18]proposedSPAG,an
acceleratedmethod,andproveabetteruniformconcentrationboundoftheconditioningnumberwhensolving
strongly-convexproblems.SVRPandCatalyzedSVRP[26]transfertheideaofusingthecentralizedproximal
pointmethodtothedistributedsettingandtheyachievecommunicationcomplexity(withadifferentnotion)w.r.t
δ.Linetal.[38]furtherimprovesthesetwomethodseitherwithabetterrateorwithweakerassumptionsbased
ontheAcceleratedExtraGradientslidingmethod.Underthesamesettingsbutfornon-convexoptimization,
SABER[43]achievescommunicationcomplexityreductionwithbetterdependencyonδ andn.Karagulyan
B
etal.[23]proposedSPAMthatallowspartialclientparticiption.
Remark8. KhaledandJin[26]andLinetal.[38]considerthetotalamountofinformationtransmitted
betweentheserverandclientsasthemainmetric,whichissimilartoreducingthetotalstochasticoraclecalls
incentralizedlearningsettings.Theterm’clientsampling’intheseworksreferstosamplingoneclienttodothe
localcomputation.However,alltheclientsstillneedtoparticipateinthecommunicationfromtimetotimeto
providethefullgradientinformation.Thisisorthogonaltothesetupofthisworksinceweassumeeachdevice
candothecalculationinparallel.Inthescenarioswherethenumberofdevicesistoolargesuchthatreceiving
alltheupdatesbecomesproblematic,weconsiderinsteadthestandardpartialparticipationsetting.
B TechnicalPreliminaries
B.1 BasicDefinitions
Weusethefollowingdefinitionsthroughoutthepaper.
Definition5. Adifferentiablefunctionf :Rd →Riscalledµ-convexforsomeµ≥0ifforallx,y∈Rd,
µ
f(y)≥f(x)+⟨∇f(x),y−x⟩+ ∥x−y∥2 . (B.1)
2
Letf :Rd →Rbeµ-convex.Then,foranyx,y∈Rd,wehave(Nesterov[47],Theorem2.1.10):
µ∥x−y∥≤∥∇f(x)−∇f(y)∥ . (B.2)
Definition6. Adifferentiablefunctionf :Rd →RiscalledL-smoothforsomeL≥0ifforallx,y∈Rd,
∥∇f(x)−∇f(y)∥≤L∥x−y∥ . (B.3)
Letf :Rd →RbeL-smooth.Then,foranyx,y∈Rd,wehave(Nesterov[47],Lemma1.2.3):
L
f(y)≤f(x)+⟨∇f(x),y−x⟩+ ∥y−x∥2 . (B.4)
2
17Lemma9(Nesterov[47],Theorem2.1.5). Letf :Rd →RbeconvexandL-smooth.Then,foranyx,y∈Rd,
wehave:
1
∥∇f(x)−∇f(y)∥2 ≤⟨∇f(x)−∇f(y),x−y⟩ . (B.5)
L
B.2 UsefulLemmas
Wefrequentlyusethefollowinghelpfullemmasfortheproofs.
Lemma10. Foranyx,y∈Rdandanyγ >0,wehave:
γ 1
|⟨x,y⟩|≤ ∥x∥2+ ∥y∥2 , (B.6)
2 2γ
(cid:16) 1(cid:17)
∥x+y∥2 ≤(1+γ)∥x∥2+ 1+ ∥y∥2 . (B.7)
γ
Lemma11(Jiangetal.[20],Lemma14). Let{x }n beasetofvectorsinRdandletx¯ := 1 (cid:80)n x .Let
i i=1 n i=1 i
v∈Rdbeanarbitraryvector.Then,
n n
1 (cid:88) ∥x −v∥2 =∥x¯−v∥2+ 1 (cid:88) ∥x −x¯∥2 . (B.8)
n i n i
i=1 i=1
Lemma12. Letq>0andlet(F )+∞,(D )+∞ betwonon-negativesequencessuchthat:F +D ≤
k k=1 k k=0 k+1 k+1
qD +εforanyk≥0,whereε≥0isaconstant.ThenforanyK ≥1,itholdsthat:
k
K
1 (cid:88)F
k +
1−q
D ≤
1−q
D +ε, (B.9)
S q 1−qK K 1/qK−1 0
K k
k=1
whereS :=(cid:80)K 1 .
K k=1 qk
Proof. Indeed,foranyk≥0,wehave:
F D D ε
k+1 + k+1 ≤ k + .
qk+1 qk+1 qk qk+1
Summingupfromk=0tok=K−1,weget:
K
(cid:88)F
k +
D
K ≤D +S ε.
qk qK 0 K
k=1
DividingbothsidesbyS andsubstitutingS =
1/qK−1,wegettheclaim.
K K 1−q
Lemma13(c.f.Lemma2.2.4in[47]). LetA beanon-negativenon-decreasingsequencesuchthatA =0
r 0
andforanyr≥0,
c(A −A )2
A ≤ r+1 r ,
r+1 1+µA
r
wherec>0andµ≥0.Ifµ≤4c,thenforanyR≥0,wehave:
1 (cid:34)(cid:18) (cid:114) µ(cid:19)R (cid:18) (cid:114) µ(cid:19)R(cid:35)2 R2
A ≥ 1+ − 1− ≥ . (B.10)
R 4µ 4c 4c 4c
Otherwise,foranyR≥1,itholdsthat:
1
(cid:18) (cid:114) µ(cid:19)2(R−1)
A ≥ 1+ . (B.11)
R 4c 4c
√
Proof. DenoteC = µA .Foranyr≥0,itholdsthat:
r r
µC2 (1+C2)≤c(C2 −C2)2 ≤c(cid:0) 2(C −C )C (cid:1)2 =4c(C −C )2C2 .
r+1 r r+1 r r+1 r r+1 r+1 r r+1
Therefore,foranyr≥0:
(cid:114)
µ(cid:112)
C −C ≥ 1+C2 .
r+1 r 4c r
18Whenµ≤4c,byinduction,onecanshowthat,foranyR≥0(seetheproofofTheorem1in[48]fordetails):
(cid:114)
C ≥
1(cid:104)(cid:0) 1+(cid:112) µ/4c(cid:1)R−(cid:0) 1−(cid:112) µ/4c(cid:1)R(cid:105)
≥
µ
R.
R 2 4c
Whenµ>4c,wehaveC −C
≥(cid:112)µC
.Itfollowsthat,foranyR≥1:
r+1 r 4c r
(cid:114)
C ≥(1+(cid:112) µ/4c)R−1C ≥(1+(cid:112) µ/4c)R−1 µ .
R 1 4c
PlugginginthedefinitionofC ,wegettheclaims.
R
Lemma14. Let{x }n bevectorsinRdwithn≥2.Lets∈[n]andletS ∈(cid:0)[n](cid:1) besampleduniformlyat
i i=1 s
randomwithoutreplacement.Letζ2 := 1 (cid:80)n ∥x −x¯∥2wherex¯ := 1 (cid:80)n x .Thenitholdsthat:
n i=1 i n i=1 i
(cid:104) (cid:105) n−sζ2
E[x¯ ]=x¯ and E ∥x¯ −x¯∥2 = , (B.12)
S S n−1 s
wherex¯ := 1(cid:80) x .
S s j∈S j
Proof. Let(cid:0)n(cid:1) := n! bethebinomialcoefficientforanyn≥m≥1.Bythedefinitionofx¯ ,wehave:
m m!(n−m)! S
n
1(cid:88) 1(cid:88)
x¯ = x = 1 x ,
S s j s {i∈S} i
j∈S i=1
where1istheindicatorfunction.Takingtheexpectationonbothsides,weget:
n n (cid:0)n−1(cid:1) n
E[x¯ ]=
1(cid:88)
Pr[i∈S]x =
1(cid:88)
s−1 x =
1(cid:88) s
x =x¯ .
S s i s (cid:0)n(cid:1) i s n i
i=1 i=1 s i=1
Further,foranys∈[n],wehave:
E(cid:104)
∥x¯
−x¯∥2(cid:105) =E(cid:20) 1 (cid:88)(cid:88)(cid:68)
x −x¯,x
−x¯(cid:69)(cid:21)
S s2 i j
i∈Sj∈S
=E(cid:20) 1 (cid:88) ∥x −x¯∥2+ 1 (cid:88) (cid:68) x −x¯,x −x¯(cid:69)(cid:21)
s2 i s2 j j
i∈S i,j∈S,i̸=j
=E(cid:20) 1 (cid:88) 1 ∥x −x¯∥2+ 1 (cid:88) 1 (cid:68) x −x¯,x −x¯(cid:69)(cid:21)
s2 {i∈S} i s2 {i,j∈S,i̸=j} j j
i∈n i,j∈[n],i̸=j
= 1 (cid:88) Pr[i∈S]∥x −x¯∥2+ 1 (cid:88) Pr[i,j ∈S,i̸=j](cid:68) x −x¯,x −x¯(cid:69)
s2 i s2 j j
i∈n i,j∈[n],i̸=j
= 1 (cid:88)(cid:0)n s−− 11(cid:1) ∥x −x¯∥2+ 1 (cid:88) (cid:0)n s−− 22(cid:1) (cid:68) x −x¯,x −x¯(cid:69)
s2 (cid:0)n(cid:1) i s2 (cid:0)n(cid:1) j j
i∈n s i,j∈[n],i̸=j s
ζ2 s2−s 1 (cid:88) (cid:68) (cid:69)
= + x −x¯,x −x¯ .
s s2 n2−n j j
i,j∈[n],i̸=j
Whens=n,itfollowsthat:
0=E(cid:104) ∥x¯−x¯∥2(cid:105)
=
ζ2
+
n2−n 1 (cid:88) (cid:68)
x −x¯,x
−x¯(cid:69)
.
n n2 n2−n j j
i,j∈[n],i̸=j
Pluggingthisidentityintothepreviousdisplay,wegettheclaim.
Lemma15. Suppose{f }n satisfy∆ -EDandζ-BGVwithn≥2.Letf := 1(cid:80) f wheres∈[n]and
i i=1 s S s i∈S i
S
∈(cid:0)[n](cid:1)
besampleduniformlyatrandomwithoutreplacement.Letybeanarbitraryfixedpointandx bea
s S
pointcomputedbyadeterministicfunctionofS.Thenforanyγ >0,itholdsthat:
E (cid:2) f(x )−f (x )(cid:3) ≤ n−sγζ2 +(cid:16) 1 + ∆ s(cid:17) E (cid:2) ∥x −y∥2(cid:3) . (B.13)
S S S S n−1 2s 2γ 2 S S
19Proof. Leth :=f −f .Bytheassumptionthat{f }satisfy∆ -ED(Definition4),usingLemma??,forany
S S i s
S(cid:0)[n](cid:1)
,wehave:
s
∆
h (x )≤h (y)+⟨∇h (y),x −y⟩+ s ∥x −y∥2
S S S S S 2 S
(B ≤.6) h (y)+ γ ∥∇h (y)∥2+ 1 ∥x −y∥2+ ∆ s ∥x −y∥2
S 2 S 2γ S 2 S
Rearrangingandtakingtheexpectationonbothsides,wehave,foranyγ >0:
E (cid:2) h (x )−h (y)(cid:3) ≤ γ E [∥∇h (y)∥2]+ 1 E [∥x −y∥2]+ ∆ s E [∥x −y∥2]
S S S S 2 S S 2γ S S 2 S S
≤(7) n−sγζ2 +(cid:16) 1 + ∆ s(cid:17) E (cid:2) ∥x −y∥2(cid:3) ,
n−1 2s 2γ 2 S S
wherethelastinequalityisdueto(B.12).UsingthefactthatE [f(y)−f (y)]=0,wegettheclaim.
S S
Lemma16. Suppose{f }n satisfyδ -SOD.Letf := 1(cid:80) f wheres∈[n]andS ∈(cid:0)[n](cid:1) .Letv∈Rd
i i=1 s S s i∈S i s
beafixedpoint,λ>δ beareal,andlet
s
λ
F (x):=f (x)+⟨∇hS(v),x⟩+ ∥x−v∥2 ,
i i i 2
wherehS :=f −f .Let{x } beasetofpointsinRdandletx¯ = 1(cid:80) x .Thenitholdsthat:
i S i i i∈S S s i∈S i
1(cid:88)(cid:10) ∇f (x )+∇hS(x¯ ),v−x (cid:11) − 1 (cid:13) (cid:13)1(cid:88) ∇f (x )(cid:13) (cid:13)2 ≥ λ−δ s1(cid:88) ∥v−x ∥2−11(cid:88) ∥∇F (x )∥2 .
s i i i S i 2λ(cid:13)s i i (cid:13) 2 s i λs i i
i∈S i∈S i∈S i∈S
Proof. UsingthedefinitionofF ,weget:
i
∇F (x )=∇f (x )+∇hS(v)+λ(x −v).
i i i i i i
Hence,
(cid:10) ∇f (x )+∇hS(x¯ ),v−x (cid:11)
i i i S i
=λ∥v−x ∥2+(cid:10) ∇hS(x¯ )−∇hS(v),v−x (cid:11) +⟨∇F (x ),v−x ⟩ .
i i S i i i i i
Takingtheaverageoverionbothsidesofthefirstdisplay,wehave:
1(cid:88) 1(cid:88)
∇f (x )=λ(v−x¯ )+ ∇F (x ). (B.14)
s i i S s i i
i∈S i∈S
Therefore,
1 (cid:13)1(cid:88) (cid:13)2 1 (cid:13) 1(cid:88) (cid:13)2
(cid:13) ∇f (x )(cid:13) = (cid:13)λ(v−x¯ )+ ∇F (x )(cid:13)
2λ(cid:13)s i i (cid:13) 2λ(cid:13) S s i i (cid:13)
i∈S i∈S
=
λ
∥v−x¯ ∥2+
1(cid:88)
⟨∇F (x ),v−x¯ ⟩+
1 (cid:13) (cid:13)1(cid:88)
∇F (x
)(cid:13) (cid:13)2
.
2 S s i i S 2λ(cid:13)s i i (cid:13)
i∈S i∈S
Itfollowsthat:
1(cid:88)(cid:10) ∇f (x )+∇hS(x¯ ),v−x (cid:11) − 1 (cid:13) (cid:13)1(cid:88) ∇f (x )(cid:13) (cid:13)2
s i i i S i 2λ(cid:13)s i i (cid:13)
i∈S i∈S
=λ1(cid:88) ∥v−x ∥2− λ ∥v−x¯ ∥2+ 1(cid:88)(cid:10) ∇hS(x¯ )−∇hS(v),v−x (cid:11)
s i 2 S s i S i i
i∈S i∈S
1(cid:88) 1 (cid:13)1(cid:88) (cid:13)2
+ ⟨∇F (x ),x¯ −x ⟩− (cid:13) ∇F (x )(cid:13)
s i i S i 2λ(cid:13)s i i (cid:13)
i∈S i∈S
(B=.8) λ ∥v−x¯ ∥2+λ1(cid:88) ∥x −x¯ ∥2+ 1(cid:88)(cid:10) ∇hS(x¯ )−∇hS(v),x¯ −x (cid:11)
2 S s i S s i S i S i
i∈S i∈S
1(cid:88) 1 (cid:13)1(cid:88) (cid:13)2
+ ⟨∇F (x ),x¯ −x ⟩− (cid:13) ∇F (x )(cid:13)
s i i S i 2λ(cid:13)s i i (cid:13)
i∈S i∈S
(B ≥.6) λ
∥v−x¯
∥2+λ1(cid:88)
∥x −x¯ ∥2−
1 1(cid:88)(cid:13)
(cid:13)∇hS(x¯
)−∇hS(v)(cid:13) (cid:13)2 −δ s1(cid:88)
∥x −x¯ ∥2
2 S s i S 2δ s (cid:13) i S i (cid:13) 2 s i S
s
i∈S i∈S i∈S
20−
λ1(cid:88)
∥x −x¯ ∥2−
1 1(cid:88)
∥∇F (x )∥2−
1 (cid:13) (cid:13)1(cid:88)
∇F (x
)(cid:13) (cid:13)2
2s i S 2λs i i 2λ(cid:13)s i i (cid:13)
i∈S i∈S i∈S
(5), ≥(B.8) λ−δ
s ∥v−x¯ ∥2+
λ−δ s1(cid:88)
∥x −x¯ ∥2−
11(cid:88)
∥∇F (x )∥2
2 S 2 s i S λs i i
i∈S i∈S
(B=.8) λ−δ s1(cid:88) ∥v−x ∥2− 11(cid:88) ∥∇F (x )∥2 ,
2 s i λs i i
i∈S i∈S
whereinthesecondequality,weusethefactthat 1(cid:80) (cid:0) ∇hS(x¯ )−∇hS(v)(cid:1) =0.
s i∈S i S i
C ProofsforS-DANE(Alg.1)
C.1 One-steprecurrence
Lemma17. ConsiderAlgorithm1. Letf :Rd →Rbeµ-convexwithµ≥0foranyi∈[n]. Assumethat
i
{f }n haveδ -SOD.Then,foranyr≥0,wehave:
i i=1 s
λ1(cid:2) f Sr(xr+1)−f Sr(x⋆)(cid:3) + 1+ 2µ/λ(cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2
(C.1)
≤ 1 ∥vr−x⋆∥2− 1−δ s/λ1 (cid:88) ∥vr−x ∥2+ 1 1 (cid:88) ∥∇F (x )∥2 .
2 2 s i,r+1 λ2 s i,r i,r+1
i∈Sr i∈Sr
Proof. Byµ-convexityoff ,foranyr≥0,itholdsthat:
i
1 f (x⋆)+ 1 ∥vr−x⋆∥2 = 11 (cid:88) f (x⋆)+ 1 ∥vr−x⋆∥2
λ Sr 2 λs i 2
i∈Sr
(B ≥.1) 11 (cid:88)(cid:16)
f (x )+⟨∇f (x ),x⋆−x ⟩+
µ
∥x
−x⋆∥2(cid:17)
+
1
∥vr−x⋆∥2 .
λs i i,r+1 i i,r+1 i,r+1 2 i,r+1 2
i∈Sr
Recallthatvr+1istheminimizerofthefinalexpressioninx⋆.Thisexpressionisa(1+µ/λ)-convexfunction
inx⋆.Wecanthenestimateitby:
1 1
f (x⋆)+ ∥vr−x⋆∥2
λ Sr 2
(B ≥.1) λ11
s
(cid:88)(cid:16) f i(x i,r+1)+(cid:10) ∇f i(x i,r+1),vr+1−x i,r+1(cid:11) + µ
2
(cid:13) (cid:13)x i,r+1−vr+1(cid:13) (cid:13)2(cid:17) + 1 2(cid:13) (cid:13)vr−vr+1(cid:13) (cid:13)2
i∈Sr
+
1+µ/λ(cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2
.
2
Usingtheconvexityoff ianddroppingthenon-negative 2µ λ1 s(cid:80) i∈Sr(cid:13) (cid:13)x i,r+1−vr+1(cid:13) (cid:13)2,wefurtherget:
1 1
f (x⋆)+ ∥vr−x⋆∥2
λ Sr 2
(B ≥.1) λ11
s
(cid:88)(cid:16) f i(xr+1)+(cid:10) ∇f i(xr+1),x i,r+1−xr+1(cid:11)(cid:17) + 1+ 2µ/λ(cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2
i∈Sr
+ λ11
s
(cid:88) ⟨∇f i(x i,r+1),vr−x i,r+1⟩+ λ1(cid:68)1
s
(cid:88) ∇f i(x i,r+1),vr+1−vr(cid:69) + 21(cid:13) (cid:13)vr+1−vr(cid:13) (cid:13)2
i∈Sr i∈Sr
(B ≥.6) λ1 f Sr(xr+1)+ 1+ 2µ/λ(cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2 + λ11
s
(cid:88)(cid:10) ∇f i(xr+1),x i,r+1−xr+1(cid:11)
i∈Sr
+ 11 (cid:88) ⟨∇f (x ),vr−x ⟩− 1 (cid:13) (cid:13)1 (cid:88) ∇f (x )(cid:13) (cid:13)2 .
λs i i,r+1 i,r+1 2λ2(cid:13)s i i,r+1 (cid:13)
i∈Sr i∈Sr
Denotehr :=f −f .Notethat:
i Sr i
(cid:88)(cid:10) ∇f (xr+1),x −xr+1(cid:11) = (cid:88)(cid:10) −∇hr(xr+1),x −vr(cid:11) ,
i i,r+1 i i,r+1
i∈Sr i∈Sr
21wherewehaveused:
xr+1 = 1 (cid:88) x and (cid:88) ∇hr(xr+1)=0.
s i,r+1 i
i∈Sr i∈Sr
Itfollowsthat:
λ1 f Sr(x⋆)+ 1 2∥vr−x⋆∥2 ≥ λ1 f Sr(xr+1)+ 1+ 2µ/λ(cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2
+ 11 (cid:88)(cid:10) ∇f (x )+∇hr(xr+1),vr−x (cid:11) − 1 (cid:13) (cid:13)1 (cid:88) ∇f (x )(cid:13) (cid:13)2 . (C.2)
λs i i,r+1 i i,r+1 2λ2(cid:13)s i i,r+1 (cid:13)
i∈Sr i∈Sr
WenowapplyLemma16(withx =x ,v=vr,S =S andx=xr+1)toget:
i i,r+1 r
1 (cid:88)(cid:10) ∇f (x )+∇hr(xr+1),vr−x (cid:11) − 1 (cid:13) (cid:13)1 (cid:88) ∇f (x )(cid:13) (cid:13)2
s i i,r+1 i i,r+1 2λ(cid:13)s i i,r+1 (cid:13)
i∈Sr i∈Sr
≥ λ−δ s1 (cid:88) ∥vr−x ∥2− 11 (cid:88)(cid:13) (cid:13)∇F (x )(cid:13) (cid:13)2 .
2 s i,r+1 λs (cid:13) i,r i,r+1 (cid:13)
i∈Sr i∈Sr
Substitutingthislowerboundintothepreviousdisplay,wegettheclaim.
C.2 Fullclientparticipation(proofofTheorem1)
Proof. ApplyingLemma17andusing(cid:80)n ∥∇F (x )∥2 ≤δ2(cid:80)n ∥x −vr∥2andλ=2δ,for
i=1 i,r i,r+1 i=1 i,r+1
anyr≥0,wehave:
1(cid:2) f(xr+1)−f(x⋆)(cid:3)
+
1+µ/λ(cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2
λ 2
≤
21
∥vr−x⋆∥2−
1− 2δ/λ n1 (cid:88)n
∥vr−x i,r+1∥2+
λ1 2n1 (cid:88)n
∥∇F i,r(x i,r+1)∥2 (C.3)
i=1 i=1
1 (cid:16)1−1/2 1(cid:17) 1
≤ ∥vr−x⋆∥2− − ∥vr−x ∥2 = ∥vr−x⋆∥2 .
2 2 4 i,r+1 2
Rearranging,weget:
2q(cid:0) f(xr+1)−f(x⋆)(cid:1) ≤q∥vr−x⋆∥2−(cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2
.
λ
whereq:= 1 .ApplyingLemma12withε=0andusingconvexityoff,weobtain:
1+µ/λ
2q(cid:0) f(x¯R)−f(x⋆)(cid:1) + 1−q (cid:13) (cid:13)vR−x⋆(cid:13) (cid:13)2 ≤ 1−q (cid:13) (cid:13)v0−x⋆(cid:13) (cid:13)2 .
λ 1−qR (1/q)R−1
Droppingthenon-negativeterm 1−q (cid:13) (cid:13)vR−x⋆(cid:13) (cid:13)2andrearranging,weget:
1−qR
f(x¯R)−f(x⋆)≤ (cid:2)(1−q)λ (cid:3)(cid:13) (cid:13)v0−x⋆(cid:13) (cid:13)2 .
2q (1/q)R−1
Plugginginthechoiceofλandthedefinitionofq,wegettheclaim.
Corollary18. UnderthesamesettingasinTheorem1,toachievef(x¯R)−f(x⋆)≤ε,weneedatmostthe
followingnumberofcommunicationrounds:
(cid:32)
µ+δ
(cid:18) µD2(cid:19)(cid:33)
R=O ln 1+ .
µ ε
Proof. Usingthefactthat:(1+q)k ≥exp( q k)foranyq≥0andk>0,weget:
1+q
µD2 µD2
f(x¯R)−f⋆ ≤ ≤ ≤ε.
2[(1+ µ)R−1] 2[exp( µ R)−1]
2δ µ+2δ
Rearranging,wegettheclaim.
22ProofofCorollary3.
Proof. Toachieve(cid:80)n ∥∇F (x )∥2 ≤ δ2(cid:80)n ∥x −yr∥2,foreachi ∈ [n],itissufficientto
i=1 i,r i,r+1 i=1 i,r+1
ensurethat∥∇F (x )∥ ≤ δ∥x −vr∥. Letx⋆ := argmin {F (x)}. Since∥x −vr∥ ≥
i,r i,r+1 i,r+1 i,r x i,r i,r+1
(cid:13) (cid:13)vr−x⋆ i,r(cid:13) (cid:13)−(cid:13) (cid:13)x i,r+1−x⋆ i,r(cid:13) (cid:13)(B ≥.2)(cid:13) (cid:13)vr−x⋆ i,r(cid:13) (cid:13)− λ1 ∥∇F i,r(x i,r+1)∥andλ=2δ,itsufficestoensurethat:
∥∇F i,r(x i,r+1)∥≤ 2 3δ (cid:13) (cid:13)vr−x⋆ i,r(cid:13) (cid:13) . (C.4)
for any i ∈ [n]. According to Theorem 2 from [31] (or Theorem 3.2 from [35]), there exists a certain
algorithmsuchthatwhenstartedfromthepointvr,afterK queriesto∇F ,itgeneratesthepointv
i,r i,r+1
suchthat∥∇F (x
)∥≤O(cid:16)(L+λ)∥vr−x⋆ i,r∥(cid:17) =O(cid:16)L∥vr−x⋆ i,r∥(cid:17)
sinceδ≤L.SettingK
=Θ(cid:16)(cid:113) L(cid:17)
i,r i,r+1 K2 K2 δ
concludestheproof.
Remark19. RecallthatF is(L+λ)-smoothand(µ+λ)-convex,λ=Θ(δ)andδ≤L.Supposeworkeri
i,r
usesthestandardGDtoapproximatelysolvethelocalsubproblematroundrstartingatvrforKstepsand
returnthelastpoint,thenbyLemma20,wehavethat∥∇F i,r(x i,r+1)∥2 ≤O(cid:16) (L+λ)2(cid:13) (cid:13)vr−x⋆ i,r(cid:13) (cid:13)2/K2(cid:17) .
(cid:16) (cid:17)
To satisfy the accuracy condition (C.4), it is sufficient to make K = Θ L local steps. Suppose worker
δ
iusesthefastgradientmethod, thenbyTheorem3.18from[3], wehavethat∥∇F (x )∥2 ≤ 2(L+
i,r i,r+1
λ)(cid:0) F i,r(x i,r+1)−F i,r(x⋆ i,r)(cid:1) ≤ O(cid:16) (L+λ)2exp(cid:0) −(cid:113) Lµ+ +λ λK(cid:1)(cid:13) (cid:13)vr−x⋆ i,r(cid:13) (cid:13)2(cid:17) . To satisfy the accuracy
(cid:16)(cid:113) (cid:17) (cid:16)(cid:113) (cid:17)
condition(C.4),itsufficestomakeK =Θ L+δln(L+δ) =Θ L ln(L) gradientoraclecalls.
µ+δ δ µ+δ δ
Lemma20(Theorem2.2.5in[47]). Letf : Rd → RbeaconvexandL-smoothfunction. Byrunningthe
standardgradientdescentwithconstantstepsize 1:x =x − 1∇f(x ),foranyK ≥1,itholdsthat:
L k+1 k L k
(cid:18)
L∥x
−x⋆∥(cid:19)
∥∇f(x )∥≤O 0 . (C.5)
K K
Proof. ByTheorem2.2.5in[47],wehavethat:
(cid:18)
L∥x
−x⋆∥(cid:19)
min ∥∇f(x )∥≤O 0 .
k∈[K] k K
Notethatthealgorithmgeneratesnon-increasing||∇f(x )||since
k
∥∇f(x )∥2 =∥∇f(x )−∇f(x )+∇f(x )∥2
k+1 k+1 k k
=∥∇f(x )−∇f(x )∥2+2⟨∇f(x )−∇f(x ),∇f(x )⟩+∥∇f(x )∥2
k+1 k k+1 k k k
=∥∇f(x )−∇f(x )∥2−2L⟨∇f(x )−∇f(x ),x −x ⟩+∥∇f(x )∥2
k+1 k k k+1 k k+1 k
(B.5)
≤ ∥∇f(x )∥2−∥∇f(x )−∇f(x )∥2 ≤∥∇f(x )∥2 .
k k+1 k k
Hence,wehavemin ∥∇f(x )∥=∥∇f(x )∥.
k∈[K] k K
C.3 Partialclientparticipation(proofofTheorem5)
ThefollowingtheoremisaslightextensionofTheorem5,whichincludestheuseofstochasticlocalsolvers.
Theorem21. ConsiderAlgorithm1.Letf :Rd →Rbeµ-convexwithµ≥0foranyi∈[n]andletn≥2.
i
Assumethat{f }n haveδ -SOD,∆ -EDandζ-BGV.Letλ = 4(n−s)ζ2 +2(δ +∆ ). Foranyr ≥ 0,
i i=1 s s s(n−1) ε s s
supposewehave:
1 (cid:88) E [∥∇F (x )∥2]≤ λ21 (cid:88) E [∥x −vr∥2]+ λε , (C.6)
s ξi,r i,r i,r+1 4 s ξi,r i,r+1 4
i∈Sr i∈Sr
forsomeε>0,whereξ denotestherandomnesscomingfromdeviceiwhensolvingitssubproblematround
i,r
r.Weassumethat{ξ }areindependentrandomvariables.ToreachE[f(x¯R)−f(x⋆)]≤ε,weneedatmost
i,r
thefollowingnumberofcommunicationroundsinexpectation:
(cid:18) δ +∆ +µ n−s ζ2 (cid:19) (cid:16) µD2(cid:17) (δ +∆ )D2 n−sζ2D2
R∼ s s + ln 1+ ≤ s s + ,
µ n−1sεµ ε ε n−1 sε2
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
whenµ>0 whenµ=0
wherex¯R :=(cid:80)R prxr/(cid:80)R pr,p:=1+ µ,andD:=(cid:13) (cid:13)x0−x⋆(cid:13) (cid:13).
r=1 r=1 λ
23Proof. AccordingtoLemma17,wehaveforanyr≥0,
λ1(cid:2) f Sr(xr+1)−f Sr(x⋆)(cid:3) + 1+ 2µ/λ(cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2
≤ 1 ∥vr−x⋆∥2− 1−δ s/λ1 (cid:88) ∥vr−x ∥2+ 1 1 (cid:88) ∥∇F (x )∥2 .
2 2 s i,r+1 λ2 s i,r i,r+1
i∈Sr i∈Sr
AccordingtoLemma15(withS =S ,x=xr+1andy=vr),foranyγ >0,wehave:
r
E Sr(cid:2) f(xr+1)−f Sr(xr+1)(cid:3) ≤ nn −− 1sγ 2ζ s2 +(cid:16) 21
γ
+ ∆ 2s(cid:17) E Sr[(cid:13) (cid:13)xr+1−vr(cid:13) (cid:13)2 ]
(B ≤.8) n−sγζ2 +(cid:16) 1
+
∆ s(cid:17)
E
(cid:104)1 (cid:88)
∥x
−vr∥2(cid:105)
.
n−1 2s 2γ 2 Sr s i,r+1
i∈Sr
Adding 1f(xr+1)tobothsidesofthefirstdisplay,takingtheexpectationoverS onbothsides,substituting
λ r
thepreviousupperboundandsettingγ = s(n−1)ε ,weget:
2ζ2(n−s)
λ1(cid:16) E Sr[f(xr+1)]−f(x⋆)(cid:17) + 1+ 2µ/λ E Sr[(cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2 ]≤ 21 ∥vr−x⋆∥2
−(cid:16)1 − δ s+∆ s − 1 (cid:17) E (cid:104)1 (cid:88) ∥x −vr∥2(cid:105) + ε + 1 E (cid:104)1 (cid:88) ∥∇F (x )∥2(cid:105) .
2 2λ 2γλ Sr s i,r+1 4λ λ2 Sr s i,r i,r+1
i∈Sr i∈Sr
Denote all the randomness {ξ } by ξ . Since ξ is independent of the choice of S for any
i,r i∈Sr r i,r r
i ∈ [n],takingtheexpectationoverξ onbothsidesofthepreviousdisplayandusingtheassumptionthat
r
(cid:104) (cid:105) (cid:104) (cid:105)
E 1(cid:80) ∥∇F (x )∥2 ≤E λ21(cid:80) ∥x −yr∥2 + λε ,weobtain:
Sr,ξr s i∈Sr i,r i,r+1 Sr,ξr 4 s i∈Sr i,r+1 4
λ1(cid:16) E Sr,ξr[f(xr+1)]−f(x⋆)(cid:17) + 1+ 2µ/λ E Sr,ξr[(cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2 ]≤ 21 ∥vr−x⋆∥2
−(cid:16)1 − δ s+∆ s − 1 (cid:17) E (cid:104)1 (cid:88) ∥x −vr∥2(cid:105) + ε .
4 2λ 2γλ Sr,ξr s i,r+1 2λ
i∈Sr
Byourchoiceofλ,wehavethat λ − δs+∆s − 1 ≥0.Takingthefullexpectationonbothsides,weget:
4 2 2γ
1 E[f(xr+1)−f⋆]+ 1+µ/λ E[(cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2 ]≤ 1 E[∥vr−x⋆∥2]+ ε .
λ 2 2 2λ
AccordingtoLemma12,weget:
2 E(cid:2) f(x¯R)−f(x⋆)(cid:3) +(1−q)E(cid:2)(cid:13) (cid:13)vR−x⋆(cid:13) (cid:13)2(cid:3)
≤
1−q (cid:13) (cid:13)v0−x⋆(cid:13) (cid:13)2
+
1
ε.
µ+λ (1/q)R−1 µ+λ
whereq:= 1 .Rearranginganddroppingthenon-negativeE[(cid:13) (cid:13)vR−x⋆(cid:13) (cid:13)2 ],weget,foranyR≥1:
1+µ/λ
E(cid:2) f(x¯R)−f(x⋆)(cid:3)
≤
µ (cid:13) (cid:13)x0−x⋆(cid:13) (cid:13)2
+
ε
.
2[(µ +1)R−1] 2
λ
Toreachε-accuracy,itissufficienttolet 2[(µ+µ 1)R−1](cid:13) (cid:13)x0−x⋆(cid:13) (cid:13)2 ≤
2[exp(
µ
µ R)−1]
≤ 2ε.Rearranginggives
λ µ+λ
theclaim.
C.3.1 Stochasticlocalsolver.
Notethatthereexistmanystochasticoptimizationalgorithmsthatcanalsoachievetheaccuracycondition(C.6)
such as variance reduction methods [21, 9], adaptive SGD methods [12], etc. Here, we take the simplest
algorithm:SGDwithconstantstepsizeasanexample.
Corollary22. ConsiderAlgorithm1underthesamesettingsasinTheorem21.Furtherassumethateachf is
i
L
a
apn- pdsm roEo xξ¯o
ii
mt [h
(cid:13) (cid:13)
aa
g
ten
i
l(d yxe b,a yξ¯c
i
uh
)
sd
i−
ne gv ∇i mc fe
ii
nh
(
ia
x
-bs
) a(cid:13)
(cid:13)a tc
2
cc
]
he
≤
Sss Gt
σ
Do
2
:m
.
zi Sni u- pb pa :ot =c sh
e
zs ft oo −rch aa 1ns yt (i gc rg
(≥
xra ,d
0
ξ¯i
,
ren eat )g
c
−hi( ∇x de, fvξ¯ ii (c) vew
ri
)h +e ∈re ∇SE
r
fξ¯
i
s[ og (li
v
v(
e
rx
s
),
i
+ξ¯ tsi) λs] (u= zbp∇
r
−of
b
vi l(
e
rx
m
))
)
k+1 k H i i,k i Sr k
withz =vr forK steps,whereH >(L+λ)isthestepsize. Letξ denote(ξ¯r ) . Toachieveaccuracy
0 i,r i,k k
condition(C.6),withcertainstepsizeH,eachdeviceirequiresatmostthefollowingnumberofstochastic
mini-batchoraclecallsinexpectation:
(cid:32)(cid:18)
L+λ
(L+λ)σ2(cid:19) (cid:18) L(cid:19)(cid:33)
K =Θ + ln 1+ .
µ+λ (µ+λ)λε λ
24Proof. Toensurethat 1(cid:80) E [∥∇F (x )∥2]≤ λ21(cid:80) E [∥x −vr∥2]+ λε,Itsuf-
s i∈Sr ξi,r i,r i,r+1 4 s i∈Sr ξi,r i,r+1 4
ficestoensurethat,foranyi∈S ,E [∥∇F (x )∥2]≤ λ2 E [∥x −vr∥2]+ λε. Forthis,it
r ξi,r i,r i,r+1 4 ξi,r i,r+1 4
sufficestoensurethat:
E ξi,r[∥∇F i,r(x i,r+1)∥2]≤ λ 102 (cid:13) (cid:13)vr−x⋆ i,r(cid:13) (cid:13)2+ λ 5ε . (C.7)
where x⋆ := argmin {F (x)}. Indeed, suppose (C.7) holds, then we have: ∥x −vr∥ ≥
i,r x i,r i,r+1
(cid:13) (cid:13)vr−x⋆ i,r(cid:13) (cid:13) − (cid:13) (cid:13)x i,r+1−x⋆ i,r(cid:13) (cid:13) (B ≥.2) (cid:13) (cid:13)vr−x⋆ i,r(cid:13) (cid:13) − λ1 ∥∇F i,r(x i,r+1)∥. Hence, (cid:13) (cid:13)vr−x⋆ i,r(cid:13) (cid:13)2 ≤
2 ∥∇F (x )∥2+2∥x −vr∥2.Plugginginthisinequalityinto(C.7)andtakingexpectationw.r.tξ
λ2 i,r i,r+1 i,r+1 i,r
onbothsides,weget:E [∥∇F (x )∥2]≤ 1E [∥∇F (x )∥2]+ λ2 E [∥x −vr∥2]+
ξi,r i,r i,r+1 5 ξi,r i,r i,r+1 5 ξi,r i,r+1
λε.Rearranginggivestheweakercondition.
5
Wenextconsiderthenumberofmini-batchstochasticgradientoraclesrequiredforSGDtoachieve(C.7).Since
F is(L+λ)-smoothand(µ+λ)-convex,accordingtoLemma23,wehave:
i,r
E [∥∇F (z¯ )∥2]≤2(L+λ)E [F (z¯ )−F⋆ ]
ξi,r i,r K ξi,r i,r K i,r
≤2(L+λ)(cid:18) (µ+λ)(cid:13) (cid:13)vr−x⋆ i,r(cid:13) (cid:13)2
+
σ2 (cid:19)
,
2[exp(cid:0) (µ+λ)K/H(cid:1) −1] 2(H−L−λ)
wherez¯ :=(cid:80)K zk/(cid:80)K 1 andq= H−µ−λ.
K k=1 qk k=1 qk H
BychoosingH
=(L+λ)+5(L+λ)σ2
,andlettingthecoefficientoffirstpartofthepreviousdisplaybesmaller
λε
than
λ2
,wegettheclaim.
10
Lemma23. Letf beaµ-convexandL-smoothfunction.ConsiderSGDwithconstantstepsizeH >L:
H
x :=argmin{⟨g ,x⟩+ ∥x−x ∥2},
k+1 k 2 k
x∈Rd
whereg :=g(x ,ξ )withE [g(x,ξ)]=∇f(x)andE [∥g(x,ξ)−∇f(x)∥2]≤σ2foranyx∈Rd.Then
k k k ξ ξ
foranyK ≥1,wehave:
µ∥x −x⋆∥2 σ2
E[f(x¯ K)−f⋆]≤ 2(cid:2) exp(µ0 K/H)−1(cid:3) + 2(H−L) . (C.8)
wherex¯ :=(cid:80)K xk/(cid:80)K 1 andq= H−µ.
K k=1 qk k=1 qk H
Proof. Indeed,foranyk≥0,wehave:
H
f(x )+⟨g ,x⋆−x ⟩+ ∥x −x⋆∥2
k k k 2 k
H H
≥f(x )+⟨g ,x −x ⟩+ ∥x −x ∥2+ ∥x −x⋆∥2
k k k+1 k 2 k+1 k 2 k+1
(B.3) H−L H
≥ f(x )+⟨g −∇f(x ),x −x ⟩+ ∥x −x ∥2+ ∥x −x⋆∥2
k+1 k k k+1 k 2 k+1 k 2 k+1
(B.6) ∥g −∇f(x )∥2 H
≥ f(x )− k k + ∥x −x⋆∥2 .
k+1 2(H−L) 2 k+1
Takingtheexpectationonbothsidesandusingµ-convexityoff,weget:
H H−µ σ2
E[f(x )−f⋆]+ E[∥x −x⋆∥2]≤ E[∥x −x⋆∥2+ .
k+1 2 k+1 2 k 2(H−L)
ApplyingLemma12,wehaveforanyK ≥1:
µ∥x −x⋆∥2 σ2 µ∥x −x⋆∥2 σ2
E[f(x¯ )−f⋆]≤ 0 + ≤ 0 + .
K 2(cid:2) (1/q)K−1(cid:3) 2(H−L) 2(cid:2) exp(µK/H)−1(cid:3) 2(H−L)
25D ProofsforAcceleratedS-DANE(Alg.2)
D.1 One-steprecurrence
Lemma24. ConsiderAlgorithm2. Letf :Rd →Rbeµ-convexwithµ≥0foranyi∈[n]. Assumethat
i
{f }n haveδ -SOD.Foranyr≥0,wehave:
i i=1 s
A rf Sr(xr)+a r+1f Sr(x⋆)+ B 2r ∥vr−x⋆∥2 ≥A r+1f Sr(xr+1)+ B r 2+1 (cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2
(cid:32) (cid:33) (D.1)
+A λ−δ s1 (cid:88) ∥x −yr∥2− 11 (cid:88) ∥∇F (x )∥2 .
r+1 2 s i,r+1 λs i,r i,r+1
i∈Sr i∈Sr
Proof. Byµ-convexityoff ,foranyr≥0,itholdsthat:
i
B
A f (xr)+a f (x⋆)+ r ∥vr−x⋆∥2
r Sr r+1 Sr 2
=A 1 (cid:88) f (xr)+a 1 (cid:88) f (x⋆)+ B r ∥vr−x⋆∥2
rs i r+1s i 2
i∈Sr i∈Sr
(B ≥.1) A 1 (cid:88)(cid:16) f (x )+⟨∇f (x ),xr−x ⟩(cid:17) + B r ∥vr−x⋆∥2
rs i i,r+1 i i,r+1 i,r+1 2
i∈Sr
+a
1 (cid:88)(cid:16)
f (x )+⟨∇f (x ),x⋆−x ⟩+
µ
∥x
−x⋆∥2(cid:17)
.
r+1s i i,r+1 i i,r+1 i,r+1 2 i,r+1
i∈Sr
Recallthatvr+1 istheminimizerofthefinalexpressioninx⋆. Thisexpressionisa(µa +B )-convex
r+1 r
functioninx⋆.ByconvexityandusingthefactthatA =A +a andB =µa +B ,weobtain:
r+1 r r+1 r+1 r+1 r
A rf Sr(xr)+a r+1f Sr(x⋆)+ B 2r ∥vr−x⋆∥2 ≥A r+11
s
(cid:88) f i(x i,r+1)+ µa 2r+11
s
(cid:88)(cid:13) (cid:13)x i,r+1−vr+1(cid:13) (cid:13)2
i∈Sr i∈Sr
+ B 2r (cid:13) (cid:13)vr−vr+1(cid:13) (cid:13)2 + 1
s
(cid:88)(cid:10) ∇f i(x i,r+1),A rxr+a r+1vr+1−A r+1x i,r+1(cid:11) + B r 2+1 (cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2 .
i∈Sr
Recallthatyr = Ar xr+ ar+1vr.Itfollowsthat:
Ar+1 Ar+1
B 2r (cid:13) (cid:13)vr−vr+1(cid:13) (cid:13)2 + 1
s
(cid:88)(cid:10) ∇f i(x i,r+1),A rxr+a r+1vr+1−A r+1x i,r+1(cid:11)
i∈Sr
= B 2r (cid:13) (cid:13)vr−vr+1(cid:13) (cid:13)2 +a r+1(cid:68)1
s
(cid:88) ∇f i(x i,r+1),vr+1−vr(cid:69) +A r+11
s
(cid:88) ⟨∇f i(x i,r+1),yr−x i,r+1⟩
i∈Sr i∈Sr
(B ≥.6) −a2 r+1(cid:13) (cid:13)1 (cid:88) ∇f (x )(cid:13) (cid:13)2 +A 1 (cid:88) ⟨∇f (x ),yr−x ⟩ .
2B (cid:13)s i i,r+1 (cid:13) r+1s i i,r+1 i,r+1
r
i∈Sr i∈Sr
Substituting this lower bound, using convexity of f and dropping the non-negative
i
µar 2+11 s(cid:80) i∈Sr(cid:13) (cid:13)x i,r+1−vr+1(cid:13) (cid:13)2,wefurtherget:
A f (xr)+a f (x⋆)+ B r ∥vr−x⋆∥2 (B ≥.1) A 1 (cid:88)(cid:16) f (xr+1)+(cid:10) ∇f (xr+1),x −xr+1(cid:11)(cid:17)
r Sr r+1 Sr 2 r+1s i i i,r+1
i∈Sr
(cid:13) (cid:13)2
+A r+11
s
(cid:88) ⟨∇f i(x i,r+1),yr−x i,r+1⟩− a 22 r B+1 (cid:13) (cid:13) (cid:13)1
s
(cid:88) ∇f i(x i,r+1)(cid:13) (cid:13)
(cid:13)
+ B r 2+1 (cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2 .
r (cid:13) (cid:13)
i∈Sr i∈Sr
Denotehr :=f −f .Bysubstituting
i Sr i
(cid:88)(cid:10) ∇f (xr+1),x −xr+1(cid:11) = (cid:88)(cid:10) −∇hr(xr+1),x −yr(cid:11)
i i,r+1 i i,r+1
i∈Sr i∈Sr
intothepreviousdisplay,weget:
A rf Sr(xr)+a r+1f Sr(x⋆)+ B 2r ∥vr−x⋆∥2 ≥A r+1f Sr(xr+1)+ B r 2+1 (cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2
+A 1 (cid:88)(cid:10) ∇f (x )+∇hr(xr+1),yr−x (cid:11) − a2 r+1(cid:13) (cid:13)1 (cid:88) ∇f (x )(cid:13) (cid:13)2 . (D.2)
r+1s i i,r+1 i i,r+1 2B (cid:13)s i i,r+1 (cid:13)
r
i∈Sr i∈Sr
26WenowapplyLemma16(withx =x ,v=yr,S =S andx=xr+1)toget:
i i,r+1 r
1 (cid:88)(cid:10) ∇f (x )+∇hr(xr+1),yr−x (cid:11) − 1 (cid:13) (cid:13)1 (cid:88) ∇f (x )(cid:13) (cid:13)2
s i i,r+1 i i,r+1 2λ(cid:13)s i i,r+1 (cid:13)
i∈Sr i∈Sr
≥ λ−δ s1 (cid:88) ∥yr−x ∥2− 11 (cid:88) ∥∇F (x )∥2 .
2 s i,r+1 λs i,r i,r+1
i∈Sr i∈Sr
SubstitutingthislowerboundintothepreviousdisplayandusingthechoiceofA =
a2 r+1λ
,wegettheclaim.
r+1 Br
D.2 Fullclientparticipation(proofofTheorem6)
Proof. ApplyingLemma24andusing(cid:80)n ∥∇F (x )∥2 ≤δ2(cid:80)n ∥x −yr∥2andλ=2δ,for
i=1 i,r i,r+1 i=1 i,r+1
anyr≥0,wehave:
A rf(xr)+a r+1f(x⋆)+ B 2r ∥vr−x⋆∥2 ≥A r+1f(xr+1)+ B r 2+1 (cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2
+A (cid:16)λ−δ − δ2(cid:17)1 (cid:88) ∥x −yr∥2
r+1 2 λ s i,r+1
i∈Sr
=A r+1f(xr+1)+ B r 2+1 (cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2 .
SubtractingA f(x⋆)onbothsides,weget:
r+1
A r(cid:104) f(xr)−f(x⋆)(cid:105) + B 2r ∥vr−x⋆∥2 ≥A r+1(cid:104) f(xr+1)−f(x⋆)(cid:105) + B r 2+1 (cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2 .
Recursivelyapplyingthepreviousdisplayfromr=0tor=R−1,weget:
A R(cid:104) f(xR)−f(x⋆)(cid:105) + B 2R (cid:13) (cid:13) (cid:13)vR−x⋆(cid:13) (cid:13) (cid:13)2 ≤A 0(cid:104) f(x0)−f(x⋆)(cid:105) + 21(cid:13) (cid:13)v0−x⋆(cid:13) (cid:13)2 = 21(cid:13) (cid:13)x0−x⋆(cid:13) (cid:13)2 .
ItremainstoapplyLemma13andplugintheestimationofthegrowthofA .
R
Corollary25. UnderthesamesettingasinTheorem6,toachievef(xR)−f(x⋆)≤ε,weneedatmostthe
followingnumberofcommunicationrounds:
(cid:32)√ δ+√ µ (cid:18) (cid:114) min{µ,δ}D2(cid:19)(cid:33)
R=O √ ln 1+ .
µ ε
Proof. Whenµ≤8δ,byusing(cid:0) 1+(cid:112) 8µ δ(cid:1)R−(cid:0) 1−(cid:112) 8µ δ(cid:1)R ≥(cid:0) 1+(cid:112) 8µ δ(cid:1)R−1≥exp(cid:16) √ 8√ δµ +R √ µ(cid:17) −1,we
have:
2µD2 2µD2
f(xR)−f⋆ ≤ ≤ ≤ε.
(cid:104)(cid:0) 1+(cid:112) 8µ δ(cid:1)R−(cid:0) 1−(cid:112) 8µ δ(cid:1)R(cid:105)2 (cid:18) exp(cid:16) √√ µR
√
(cid:17) −1(cid:19)2
8δ+ µ
Rearranging,wegettheclaim.Whenµ≥8δ,itsufficestoensurethat
δ∥x0−x⋆∥2
≤ε.
4R−2
D.3 Partialclientparticipation(proofofTheorem7)
ThefollowingtheoremisaslightextensionofTheorem7,whichincludestheuseofstochasticlocalsolvers
Theorem 26. Consider Algorithm 2 under the same settings as in Theorem 21. Let λ =
(cid:16) (cid:17)
Θ (δ + ∆ ) + (n−s)Rζ2 . For any r ≥ 0, suppose 1(cid:80) E [∥∇F (x )∥2] ≤
s s s(n−1) ε s i∈Sr ξi,r i,r i,r+1
O(cid:16) λ 421 s(cid:80) i∈SrE ξi,r[∥x i,r+1−vr∥2]+ 4λ Rε(cid:17) . DenoteD :=(cid:13) (cid:13)x0−x⋆(cid:13) (cid:13). ToreachE[f(xR)−f(x⋆)]≤ε,
weneedatmostthefollowingnumberofcommunicationroundsinexpectation:
R∼
√ δ s+ √∆ s+√ µ ln(cid:18) 1+(cid:114) min{µ,λ}D2(cid:19)
+
n−s ζ2 ln2(cid:18) 1+(cid:114) min{µ,λ}D2(cid:19)
(D.3)
µ ε n−1sεµ ε
(cid:124) (cid:123)(cid:122) (cid:125)
(whenµ>0)
(cid:114)
(δ +∆ )D2 n−sζ2D2
≤ s s + .
ε n−1 sε2
(cid:124) (cid:123)(cid:122) (cid:125)
whenµ=0
27Proof. AccordingtoLemma24,wehaveforanyr≥0,
A rf Sr(xr)+a r+1f Sr(x⋆)+ B 2r ∥vr−x⋆∥2 ≥A r+1f Sr(xr+1)+ B r 2+1 (cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2
+A λ−δ s1 (cid:88) ∥x −yr∥2−A 11 (cid:88) ∥∇F (x )∥2 .
r+1 2 s i,r+1 r+1λs i,r i,r+1
i∈Sr i∈Sr
AccordingtoLemma15(withS =S ,x=xr+1andy=yr),foranyγ >0,wehave:
r
E (cid:2) f(xr+1)−f (xr+1)(cid:3)
Sr Sr
≤ nn −− 1sγ 2ζ s2 +(cid:16) 21
γ
+ ∆ 2s(cid:17) E Sr[(cid:13) (cid:13)xr+1−yr(cid:13) (cid:13)2 ],
(B ≤.8) n−sγζ2 +(cid:16) 1
+
∆ s(cid:17)
E
(cid:104)1 (cid:88)
∥x
−yr∥2(cid:105)
.
n−1 2s 2γ 2 Sr s i,r+1
i∈Sr
AddingA f(xr+1)tobothsidesofthefirstdisplay,takingtheexpectationoverS onbothsides,substituting
r+1 r
thepreviousupperbound,andsettingγ = s(n−1)ε′ withε′ >0,weget:
2ζ2(n−s)
A rf(xr)+a r+1f(x⋆)+ B 2r ∥vr−x⋆∥2 ≥A r+1E Sr[f(xr+1)]+ B r 2+1 E Sr[(cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2 ]
+A (cid:16)λ − δ s+∆ s − 1 (cid:17) E (cid:104)1 (cid:88) ∥x −yr∥2(cid:105) − A r+1ε′− A r+1 E (cid:104)1 (cid:88) ∥∇F (x )∥2(cid:105) .
r+1 2 2 2γ Sr s i,r+1 4 λ Sr s i,r i,r+1
i∈Sr i∈Sr
Denote all the randomness {ξ } by ξ . Since ξ is independent of the choice of S for any
i,r i∈Sr r i,r r
i ∈ [n],takingtheexpectationoverξ onbothsidesofthepreviousdisplayandusingtheassumptionthat
r
(cid:104) (cid:105) (cid:104) (cid:105)
E 1(cid:80) ∥∇F (x )∥2 ≤E λ21(cid:80) ∥x −yr∥2 + λε′ ,weobtain:
Sr,ξr s i∈Sr i,r i,r+1 Sr,ξr 4 s i∈Sr i,r+1 4
A rf(xr)+a r+1f(x⋆)+ B 2r ∥vr−x⋆∥2 ≥A r+1E Sr,ξr[f(xr+1)]+ B r 2+1 E Sr,ξr[(cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2 ]
+A (cid:16)λ − δ s+∆ s − 1 (cid:17) E (cid:104)1 (cid:88) ∥x −yr∥2(cid:105) − A r+1ε′ .
r+1 4 2 2γ Sr,ξr s i,r+1 2
i∈Sr
Bychoosingλ= 4ζ2(n−s) +2(δ +∆ ),wehavethat λ − (δs+∆s) − 1 ≥0.Takingthefullexpectation
s(n−1)ε′ s s 4 2 2γ
onbothsides,weget:
A rE[f(xr)]+a r+1f(x⋆)+ B 2r E[∥vr−x⋆∥2]≥A r+1E[f(xr+1)]+ B r 2+1 E[(cid:13) (cid:13)vr+1−x⋆(cid:13) (cid:13)2 ]− A r 2+1ε′ .
SubtractingA f(x⋆)onbothsides,summingupfromr=0tor=R−1andusingthefactthatA =0,
r+1 0
v =x andB =1,weget:
0 0 0
A R(cid:16) E[f(xR)]−f(x⋆)(cid:17) + B 2R E(cid:104)(cid:13) (cid:13)vR−x⋆(cid:13) (cid:13)2(cid:105) ≤ 1 2(cid:13) (cid:13)x0−x⋆(cid:13) (cid:13)2 +(cid:88)R A rε 2′ .
r=1
DividingbothsidesbyA ,settingε′ = ε andusingthefactthatthesequence{A }isnon-decreasing,weget:
R R r
E[f(xR)]−f(x⋆)≤ 1 (cid:13) (cid:13)x0−x⋆(cid:13) (cid:13)2 + ε .
2A 2
R
√ √ √
[(1+ µ)R−(1− µ)R]2 [(1+ µ)R−1]2
We now apply Lemma 13 with c = λ to get: A ≥ 4λ 4λ ≥ 4λ ≥
R 4µ 4µ
(cid:16) (cid:16) √ (cid:17) (cid:17)2
exp √ 4λµ +R √
µ
−1
when µ ≤ 4λ, and A ≥ 1
(cid:16) 1+(cid:112)µ(cid:17)2(R−1)
when µ ≥ 4λ . Let these lower
4µ R 4λ 4λ
∥x0−x⋆∥2
boundsbelargerthan ,weget:
ε
(cid:32)√ µ+√ λ (cid:18) (cid:115) min{µ,λ}∥x −x⋆∥2(cid:19)(cid:33)
R=Ω √ ln 1+ 0 .
µ ε
Pluggingλ=Ω(cid:0)ζ2(n−s)R
+(δ +∆
)(cid:1)
intothelastdisplayandrearranging,wegettheconditionforR.
snε s s
28D.3.1 Stochasticlocalsolver
Corollary 27. Consider Algorithm 2 under the same settings as in Theorem 26. Consider the
same stochastic local solver used in Corollary 22. To achieve 1(cid:80) E [∥∇F (x )∥2] ≤
(cid:16) (cid:17)
s i∈Sr ξi,r i,r i,r+1
O λ21(cid:80) E [∥x −yr∥2]+λε ,eachdeviceirequiresatmostthefollowingnumberofstochastic
4 s i∈Sr ξi,r i,r+1 4R
mini-batchoraclecallsinexpectation:
(cid:32)(cid:18)
L+λ
(L+λ)σ2R(cid:19) (cid:18) L(cid:19)(cid:33)
K =Θ + ln 1+ .
µ+λ (µ+λ)λε λ
Proof. TheproofisthesameastheoneforCorollay22.
29