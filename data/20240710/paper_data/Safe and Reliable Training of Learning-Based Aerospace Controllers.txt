Safe and Reliable Training
of Learning-Based Aerospace Controllers
Udayan Mandal Guy Amir Haoze Wu Ieva Daukantas
Center for AI Safety School of CS & Engineering Center for AI Safety Department of Computer Science
Stanford University The Hebrew University of Jerusalem Stanford University IT University of Copenhagen
Stanford, USA Jerusalem, Israel Stanford, USA Copenhagen, Denmark
udayanm@stanford.edu guyam@cs.huji.ac.il haozewu@stanford.edu daukantas@itu.dk
Fletcher Lee Newell Umberto Ravaioli Baoluo Meng Michael Durling Kerianne Hobbs
Center for AI Safety Google GE Aerospace Research GE Aerospace Research Air Force Research Laboratory
Stanford University Mountain View, USA Niskayuna, USA Niskayuna, USA US Air Force
Stanford, USA uravaioli@google.com baoluo.meng@ge.com durling@ge.com Dayton, USA
flnewell@stanford.edu kerianne.hobbs@afrl.af.mil
Milan Ganai Tobey Shim Guy Katz Clark Barrett
Department of Computer Science Department of Data Science School of CS & Engineering Center for AI Safety
Stanford University Stanford University The Hebrew University of Jerusalem Stanford University
Stanford, USA Stanford, USA Jerusalem, Israel Stanford, USA
mganai@stanford.edu tshim24@stanford.edu guykatz@cs.huji.ac.il barrett@stanford.edu
Abstract—In recent years, deep reinforcement learning (DRL) To cope with this urgent need, a myriad of DRL training
approaches have generated highly successful controllers for a techniques have been put forth in recent years to enhance
myriad of complex domains. However, the opaque nature of
the performance of such systems. However, these current ap-
these models limits their applicability in aerospace systems and
proaches suffer from two main drawbacks: (i) they are usually
sasfety-critical domains, in which a single mistake can have dire
consequences. In this paper, we present novel advancements in not geared towards improving safety and reliability (which
both the training and verification of DRL controllers, which is key in aerospace systems); and (ii) they are heuristic in
can help ensure their safe behavior. We showcase a design-for- nature and do not afford any formal guarantees. At the same
verificationapproachutilizingk-inductionanddemonstrateitsuse
time, the formal methods community has been developing
in verifying liveness properties. In addition, we also give a brief
methods for formally and rigorously assessing the reliability
overview of neural Lyapunov Barrier certificates and summarize
their capabilities on a case study. Finally, we describe several of DRL systems. However, although such methods are useful
othernovelreachability-basedapproacheswhich,despitefailingto for identifying whether a system is safe, they are typically not
provide guarantees of interest, could be effective for verification incorporated into the DRL training process, but are rather used
of other DRL systems, and could be of further interest to the
only afterwards.
community.
Inthiswork,webeginbridgingthisgapbyproposinganovel
IndexTerms—AISafety,DeepReinforcementLearning,Formal
Verification, Deep Neural Network Verification design-for-verificationapproachthatcanbeincorporatedduring
the DRL training process. Our approach both modifies the
I. INTRODUCTION training loop to be more verification-friendly and also utilizes
Deep reinforcement learning (DRL) has gained significant formal verification (in our case, k-induction), to ensure the
popularityinrecentyears,reachingstate-of-the-artperformance correctness of the training. We also report a summary of our
in various domains. One such domain is aerospace systems, in recent efforts to use Neural Lyapunov Barrier certificates [26]
whichDRLmodelsareunderconsiderationforreplacingyears- to generate DRL agents that not only perform well on large
oldsoftwarebylearningtoefficientlycontrolairborneplatforms batches of data, but also meet rigorous correctness criteria as
and spacecraft. However, although they perform well empiri- measured by state-of-the-art verification tools.
cally, DRL systems have an opaque decision-making process, Finally, we introduce additional novel reachability-based ap-
making them challenging to reason about. More importantly, proaches for providing safety and liveness guarantees about a
this opacity raises critical questions about safety and security DRL system. These approaches are derived from prior work
(e.g., How can we ensure that the spacecraft will never violate on backward-tube reachability, forward-tube reachability, and
a velocity constraint? Will it always reach its destination?) abstraction-based reachability methods. Moreover, these ap-
which are difficult to answer. These reliability concerns are a proaches all follow a similar paradigm: the reachable space
significant obstacle to deploying DRL controllers in real-world covered by all possible paths from the starting state space is
systems, where even a single mistake cannot be tolerated. over-approximated using a verification engine, and safety and
4202
luJ
9
]IA.sc[
1v88070.7042:viXraliveness properties are checked over this over-approximated Infinite-length trajectories which contain no “good” states (i.e.,
state space. no states where P holds) constitute the set of trajectories in
2
To demonstrate the usefulness of our approaches, we apply violation of the liveness property.
them to a benchmark satellite-control model developed in
B. DNNs, DNN Verification, and Dynamical Systems.
collaboration with industry partners (GE Aerospace Research
and the U.S. Air Force). We demonstrate that liveness can be Deep Learning. Deep neural networks (DNNs) consist of
verifiedusingourk-inductionapproach.Additionally,asapoint layers of neurons that perform some (usually nonlinear) trans-
ofcomparison,weshowcasethatthecertificate-basedapproach formation of the input [38]. In this paper, we investigate deep
is indeed able to generate a controller that provably behaves reinforcementlearning(DRL),wherewetrainaDNNtoobtain
safely. Notably, the problem setting and controller complexity apolicy,whichmapsstatestoactionsthatcontrolasystem[54].
are beyond that acheived in previous work on formally verified
DNNVerification. Given(i)atrainedDNN(e.g.,aDRLagent)
controllers.
N; (ii) a pre-condition P on the DNN’s inputs, limiting the
Theotherreachability-basedmethodsfailonthisbenchmark.
input assignments; and (iii) a post-condition Q on the DNN’s
However, we believe that these failed attempts: (i) demonstrate
outputs, the goal of DNN verification is to determine whether
the merits of our successful approaches in handling complex,
the property P(x) → Q(N(x)) holds for any neural network
nontrivial properties; (ii) can be of value to the community, by
input x. In many DNN verifiers (a.k.a., verification engines),
shedding light on vulnerabilities of alternate methods; and (iii)
thistaskisequivalentlyreducedtodeterminingthesatisfiability
couldbepotentiallysuccessfulwhenappliedoverdifferentDRL
of the formula P(x)∧¬Q(N(x)). If the formula is satisfiable
systems.
(SAT), then there is an input that satisfies the pre-condition
We view this work as an important step towards the safe and
and violates the post-condition, which means the property is
reliable deployment of DRL controllers in real-world systems,
violated. On the other hand, if the formula is unsatisfiable
especially in the complex domain of avionics. We additionally
(UNSAT), then the property holds. It has been shown [49] that
hope that our work will further motivate additional research
verification of piece-wise-linear DNNs is NP-complete. In re-
in neural network verification, DRL safety, and specifically,
centyears,theformalmethodscommunityhasputforthvarious
theirroleintheimportantdomainofDRL-controlledaerospace
techniques for verifying and improving DNN reliability [1],
systems.
[5], [6], [9], [13], [17], [23], [70]. These techniques include
The rest of the paper is organized as follows. In Sec. II,
SMT-based methods [8], [45], [50], [52], optimization-based
we cover background on deep learning, DRL, and verification,
methods [15], [30], [55], [68], methods based on abstraction-
and we also introduce Neural Lyapunov Barrier functions. In
refinement[10],[22],[31],[32],[58],[59],[65],methodsbased
Sec. III, we introduce our benchmark problem, a 2D spacecraft
on shielding [24], [51], [63], and more.
docking challenge. We subsequently introduce our k-induction
technique in Sec. IV, and we present alternative verification Discrete-Time Dynamical Systems. We consider discrete-
approaches in Sec. V. 1 Finally, we conclude in Sec. VI. timedynamicalsystems,particularlysystemswhosetrajectories
satisfy the equation:
II. PRELIMINARIESANDRELATEDWORK
A. Safety and Liveness
x t+1=f(x t,u t), (3)
In this paper, we are interested in obtaining DRL controllers in which the transition function f takes as inputs the current
that satisfy safety and liveness properties [2] in discrete-time state x t ∈ X and a control u t ∈ U and produces as output the
settings. subsequent state x t+1. To control these systems, we employ
Safety. In a sequence satisfying a safety property, a bad state a policy π ∶ X → U that takes in a state x ∈ X and
is never reached. For the set of system states X, let τ ⊆X∗ be outputs a control action u = π(x). In DRL, the controller
the set of potential system trajectories. We say a trajectory α π is realized by a trained DNN agent. These learning-based
satisfiessafetypropertyP ifandonlyifeachstateinαsatisfies controllers have proven to be effective in many real-world
1
property P . More formally: settings including robotics [26], biomedical systems [28], and
1
energy management [44], due to their expressive power and
∀α∶α∈τ.∀x∈α.x⊧P 1. (1)
ability to generalize to unseen, complex environments [67].
Finite-lengthtrajectoriesterminatingina“bad”state(whereP
1 C. Control Lyapunov Barrier Functions
does not hold) constitute the set of trajectories in violation of
the safety property. The problem of verifying safety and liveness properties in
Liveness. On the other hand, a liveness property indicates a a dynamical system can be solved by finding a function V ∶
good state is eventually reached. A liveness property P is X ↦ R with certain properties. Control theory identifies two
2 fundamental types of functions [53].
satisfied by trajectory α if and only if there exists a state x
in α where P holds. Defining τ∞ as the set of infinite-length Lyapunov Functions. Lyapunov functions, a.k.a., Control
2
trajectories, we formally specify liveness property P as: Lyapunov functions, capture the energy level at a particular
2
state: over time, energy is dissipated along a trajectory until
∞
∀α∶α∈τ .∃x∈α.x⊧P 2. (2)
the system attains zero-energy equilibrium [41]. Lyapunov
functions can guarantee asymptotic stability, which ensures
1Code for these approaches is available at:
github.com/NeuralNetworkVerification/artifact-dasc-docking the system eventually converges to some goal state (therebysatisfying a liveness property). Lyapunov functions must be (i) reformulating the dynamics of a system for easier reachability
equalto0atequilibrium,(ii)strictlypositiveatallotherstates; verification [37].
and (iii) monotonically decreasing [18], [19], [36].
Bounded Model Checking and k-induction. Bounded model
Barrier Functions. Barrier functions [4], a.k.a., Control Bar- checking uses a symbolic analysis over k copies of a system
rier Functions, guarantee that a system never enters an unsafe to check whether a bad state is reachable in k or fewer steps
region(i.e.,a“bad”state)inthestatespace.Thisisachievedby fromthestartingsetofstates.k-inductionissimilar,exceptthat
settingthefunctionvaluetobeabovesomethresholdforunsafe it starts from an arbitrary state and can thus be used to prove
states and then verifying that the system can never transition that a bad state is never reached. Bounded model checking has
to a state where the function is above the threshold [3], [12], been explored in the WhiRL tool [33] using the neural network
[72]. Previous work [60], [61], [69], [75] demonstrates how to verifier Marabou [50], [71]. [64] implements another tool for
obtain Barrier functions for various safety-critical tasks such checking adversarial cases and coverage using bounded model
as pedestrian avoidance, neural radiance field-based obstacle checking for artificial neural networks. WhiRL 2.0 [7] adds k-
navigation [57], and multi-agent control. induction capabilities to WhiRL.
Control Lyapunov Barrier Functions. Often, it is necessary Design-for-Verification. Design-for-verification broadly en-
to ensure both safety and liveness properties simultaneously. compasses any method which aims to modify the design and
In such cases, we can employ a Control Lyapunov Barrier trainingprocesstomakeverificationeasier.TheTrainifyframe-
Function (CLBF), which integrates the properties and guaran- work [47] uses a CEGAR-based approach to grow an easily
tees of both Control Lyapunov functions and Control Barrier verifiable state space by repeatedly retraining the DRL system.
functions [27]. CLBFs can solve reach-while-avoid tasks [29], [25] motivates an optimized DRL training approach to reduce
which we discuss next. thenumberofsafetyviolations,easingformalverification.This
Reach-while-Avoid Tasks. The goal of Reach-while-Avoid approach was also implemented in Marabou [50], [71].
(RWA) tasks is to find a controller π for a dynamical system
such that every trajectory {x 1,x 2...} produced under this con-
III. 2DDOCKINGPROBLEM
troller(i)neverentersanunsafe(“bad”)state;and(ii)eventually
enters a goal (“good”) region or state. We can formally define
We adopt as a motivating case study benchmark the 2D
the problem as:
docking problem presented in [62]. The goal is to train a DRL
controller to safely navigate a deputy spacecraft to a chief
Definition 1 (Reach-while-Avoid Task).
spacecraft within two-dimensional space. The reference frame
Input: A dynamical system with a set of initial states
is defined such that the chief spacecraft is always at the origin
XI ⊆X, a set of goal states XG⊆X, and a set of unsafe (0,0). The state of the deputy spacecraft is x = [x,y,x˙,y˙],
states XU ⊆X, where XI ∩XU =∅ and XG∩XU =∅ where (x,y) are the position of the spacecraft and (x˙,y˙) are
Output: A controller π such that for every trajectory τ = the respective directional velocities.
{x 1,x 2...} satisfying x 1∈XI:
1) Reach: ∃t∈N.x t∈XG
2) Avoid: ∀t∈N.x t∈/XU A. Dynamics
Some solutions for RWA tasks rely on control theoretic The system dynamics are defined according to the linearly-
principles. The approach in [27] trains Lyapunov and Bar- approximate Clohessy-Wiltshire relative orbital motion equa-
rier certificates to solve RWA tasks. Hamilton-Jacobi (HJ) tions in a non-inertial Hill’s reference frame [21], [42]. The
reachability-based methods [11]) have also been employed to control input to the system is u=[F x,F y], where F
x
and F
y
solveRWAtasks[34],[43],[66].SafeDRLiscloselyconnected are the thrust forces applied to the deputy spacecraft in the x
to RWA, with its goal being to maximize cumulative rewards and y directions. We follow [62], setting the deputy spacecraft
while minimizing costs along a trajectory [14]. It has been mass to m=12 kg and the mean motion to n=0.001027 rad/s.
solved with both Lyapunov/Barrier methods [20], [73] and HJ The continuous time state dynamics of the system are given by
reachability methods [35], [74]. the following differential equations:
D. Other Verification Approaches
Reachability Analysis. Reachability analysis methods aim to x˙ =[x˙,y˙,x¨,y¨] (4)
define and compute the set of final reachable states and then F
verifythatthisset(i)doesnotincludeanybadstates,and(ii)is
x¨=2ny˙+3n2x+ mx (5)
contained within the goal region. Reachability methods include F
y
forward-tube and backward-tube verification [40], which either y¨=−2nx˙ + m (6)
propagatestatesforwardfromthestartingsetorbackwardfrom
thegoalset.Otherrelatedworkinreachabilityanalysisincludes Integration using a discrete time step T yields a closed-form
hybridsystemverifiers[46],growingthesetofreachablestates next-statefunction.Givenastatex=[x,y,x˙,y˙]andcontrolin-
over a discrete action space [48], approximating reachable putsu=[F x,F y],thespacecraft’snextstatex′ i=[x′,y′,x˙′,y˙′ ]
states during forward and backward reachability [39], and after an elapsed time T is:where n is a positive integer. Larger values of
directions
2y˙ F 2F F 2y˙ n yield more precise approximations. We can simplify
x′ =( n +4x+ mnx 2)+( mny )+(− mnx 2 − n −3x) thd ii sre bc ytio nn os ting that:
(7)
−2F y x˙ √ √
⋅cos(nT)+( mn2 + n)sin(nT) u2 1+u2 2= ∣u 1∣2 +∣u 2∣2,
and then focusing our search only on vectors in the first
y′ =(−2 nx˙ +y+ m4F ny 2)+(− m2F nx −3y˙−6nx)T +−3 2F my t2 quadrant. Assuming n
directions
is a multiple of 4, we get:
+(− m4F ny 2 + 2 nx˙ )cos(nT)+( m2F nx 2 + 4 ny˙ +6x) (8) under(u 1,u 2)= i∈[1,ndm ireca tix ons/4+1](∣u 1∣⋅cos( n2 d( ii re− ct1 io) nπ s)
⋅sin(nT) 2(i−1)π
(15)
+∣u 2∣⋅sin( ))
n
x˙′ =(2 mF nx )+(− m2F ny +x)cos(nT)+( mF x
n
+2y˙
(9)
≤√
u2 1+u2
2
directions
+3nx)sin(nT)
and
y˙′ =(− m2F nx −3y˙−6nx)+(−3 mF y )T +(2 mF nx +4y˙
(10)
over(u 1,u 2)=
cos(π/n
d1 irections)i∈[1,ndm ireca tix ons/4+1](∣u 1∣
+6nx)cos(nT)+(4F
y −2x˙)sin(nT)
⋅cos(2(i−1)π
)+∣u
2∣⋅sin(2(i−1)π
))
mn n n
directions directions
√
B. Liveness —– Docking Region
≥ u2 1+u2 2.
The problem as given in [62] defines a docking region which (16)
is a circle of radius 0.5 meters centered at the origin. The goal
Using these constraints, we can over-approximate the unsafe
is for the deputy spacecraft to eventually enter this region. To
region as
simplify the verification query, it is easier to use linear bounds
for the goal region, so we use a square centered at the origin
over(x˙ t,y˙ t)>0.2+2n⋅under(x t,y t). (17)
with sides parallel to the axes of length 0.7 meters (note that
thissquarefitsinsidethedockingregionof[62]).Formally,our This is a piece-wise linear constraint. Moreover, both the
liveness condition is: absolutevaluefunctionandthemaximumfunctioncanbeeasily
∞ encoded in neural network verification tools such as Marabou.
∀α∶α∈τ .∃t.∣α t.x∣≤0.35∧∣α t.y∣≤0.35, (11)
In our experiments, we use n
directions
=400.
where α is the state at time t in trajectory α, and α .x and
t t
D. DNN Setup
α .y are the x and y components of α .
t t
Asin[62],weuseRayRLib’sProximalPolicyOptimization
C. Safety — Velocity Threshold
(PPO) reinforcement learning algorithm to learn the system
To minimize the risk to both spacecraft, a safety constraint
dynamics, but we make four important alterations to improve
is imposed on the magnitude of the velocity of the deputy
downstream verification, part of our design for verification
spacecraft. The constraint depends on the distance from the
scheme.
deputy. Formally, [62] requires the following state invariant:
1) Scenario Regions: To improve performance near the
√ √
x˙2 +y˙2 ≤0.2+2n x2 +y2 (12) docking region, we reduce the docking distance during training
from 0.5 meters to 0.25 meters. We also simplify the problem
Wethereforedefinetheunsaferegiontobethenegationof(12).
by reducing the initial position of the deputy spacecraft from a
Again, we desire to instead use a linear constraint in order
radiusof150meterstoonly5meters.Scalingbackuptolarger
to be compatible with our formal tools. We use the Euclidean
initial positions is part of an ongoing research effort.
norm approximation of [16], which approximates the norm
2) Speed Observations: We limit the observations of the
by projecting it onto vectors in all different directions and
agent to its x and y positions and respective x˙ and y˙ velocities,
taking the one with the maximum magnitude. We use the two
eliminating the agent’s observations of its current speed and
inequalities:
thedistance-dependentvelocityconstraintdescribedinEquation
2(i−1)π 12. This makes it less likely that irregular trajectories will be
max (u 1⋅cos( )+u
2
i∈[1,ndirections] n
directions (13)
learned because of observations of the safety constraint. As a
2(i−1)π √ result, liveness verification becomes easier.
⋅sin( n ))≤ u2 1+u2 2 3) DistanceReward: Wekeeptherewardsrelatingtosuccess
directions
orfailure,thesafetyconstraint,anddelta-vaspresentedin[62],
and
but we alter the distance change reward to use the L1 norm of
1 2(i−1)π
max (u 1⋅cos( ) the position of the deputy — i.e., the Manhattan distance from
cos(π/n directions)i∈[1,ndirections] n directions (14) thedeputytothechief,ratherthanthenonlinearL2 norm.This
+u 2⋅sin( n2 d( ii re− ct1 io) nπ s))≥√ u2 1+u2 2, i as ccto oum ntat fc oh rt th he ein nd ewuct dio isn tai nn cv ear mian ett rid ces ac nr dibe pd rei vn ioS ue sc lyti -o dn esI cV ri. bT eo
dsmaller initial distances, we developed a novel reward function then take n > V 0/ϵ, we get that V jn < 0, which is impossible.
for distance change:
R tdnew =2(e−a1dm t −e−a1dm t−1)+2(e−a2dm t −e−a2dm t−1), (18) Algorithm. We verify (19) using Algorithm 1. We gradually
increase k until the property holds, a maximum of k=k
max
is
where dm i =∣x i∣+∣x i∣, a 1= ln( 52) , and a 2= ln 0( .52) . reached, or a timeout is exceeded.
4) Model Architecture: Our DRL controller should be suf- Algorithm 1: Algorithm for k-induction.
ficiently small to keep verification time reasonable and suffi- Require: Bounds on state components x ,y ,x˙ ,y˙ , values
0 0 0 0
ciently large to be able to learn the necessary behavior. We for k ,k
min max
found that reducing the hidden layer widths from 256 neurons Ensure: If result = UNSAT, then property (19) holds for all
to 20 neurons, while maintaining two hidden layers, acheives a states within the defined bounds.
good balance between verification time and expressive power. 1: for each k∈[k min,k max] do
Also,weswapthetanhactivationfunctionsforReLUactivation 2: Verify the negation of the distilled property:
′ ′
functions since ReLU is supported by most neural network ⎛(∣x∣+∣y ∣)–(∣x 0∣+∣y 0∣)<–ϵ
⎞
verification tools (such as Marabou). ¬⎜
⎜⋁
⎟
⎟
′ ′
IV. USINGk-INDUCTIONFORLIVENESSGUARANTEES ⎝(∣x˙ ∣+∣y˙ ∣)–(∣x˙ 0∣+∣y˙ 0∣)<–ϵ)⎠
3: if UNSAT then
Inthissection,wepresentanapproachforscalablyverifying
4: result = [UNSAT, k]
a liveness property for the 2D docking problem presented
5: break;
in Section III using k-induction. We describe the conceptual
6: else
approach, the experimental framework, and the results.
7: result = [SAT, k, counterexample k-step trajectory].
A. Proving Liveness by k-induction 8: end if
In order to apply k-induction, we must find a way to reduce 9: end for
a liveness property to a k-inductive property. Typically, this 10: return result
is done by finding a ranking function, a function with a Input bounds for the state space can be chosen according
well-founded co-domain, which can be shown to always be to the problem specification. It is also important to note that
decreasing by k-induction. different k and k values can be chosen. In practice, in
min max
For the spacecraft, an obvious choice for a ranking function order to make the verification more tractable, we first split the
is the distance from the deputy to the chief. In order to make state space into subregions, then call the algorithm on each
the function easier to reason about, we use a linear proxy subregion. For each subregion of the state space, we explore
functionfortheactualdistance,namelytheManhattandistance. values of k from k to k . For each k, a neural network
min max
Unfortunately, it is not the case that this measure always verifierisinvokedtocheckifthenegationofthepropertyholds
decreases, as the spacecraft may move away from the target. after k steps. There are three possible results of the algorithm.
Thus, we instead propose a property that ensures the space-
1) If the negation of the property is satisfiable for each k, the
craft eventually starts moving towards the target. The property
algorithm returns SAT along with a counter-example.
is expressed as a logical disjunction: after k steps, either the
2) If the negation of the property is unsatisfiable for some k,
Manhattan distance decreases or the magnitude of the velocity
this means that the property holds for that value of k. In
decreases.Again,weapproximatethevelocitymagnitudebythe
this case, the algorithm returns UNSAT together with the
L1 norm, the sum of the absolute values of x˙ and y˙. Formally,
value of k for which unsatisfiability was determined. In
if the current state is (x 0,y 0,x˙ 0,y˙ 0) and the future state after this case, verification of the region is complete.
k steps is (x′,y′,x˙′,y˙′ ), we must show:
3) If a predefined timeout is exceeded, the algorithm termi-
nates and a timeout result is returned.
′ ′ ′ ′
(∣x∣+∣y ∣)–(∣x 0∣+∣y 0∣)<–ϵ ⋁ (∣x˙ ∣+∣y˙ ∣)–(∣x˙ 0∣+∣y˙ 0∣)<–ϵ, Experimental Setup. We use Marabou for the neural
(19) network verification step. We set the following parameters
where ϵ is some positive value. for Marabou: “verbosity=0, timeoutInSeconds=5000, num-
Workers=10,tighteningStrategy=“sbt”,solveWithMILP=True”.
Proposition 1. If property (19) holds (for some k) for every
Marabou also requires a back-end linear programming engine.
state,theneventuallythespacecraftwillbemovingtowardsthe
goal (i.e., the L1 norm of the position will decrease). We use Gurobi 9.5.
We start with positional bounds of ∣x∣,∣y∣ ∈ [−25,25] and
Proof. Suppose that from some starting state, (x 0,y 0,x˙ 0,y˙ 0), velocity bounds of x˙,y˙ ∈[−0.2,0.2]). We initially divide these
the spacecraft follows a trajectory that never moves towards into25subregionsbyfocusingon5×5regionsinthepositional
the goal in the sense that the L1 norm never decreases. Let space. A subregion is further subdivided if Algorithm 1 times
(x i,y i,x˙ i,y˙ i) be the state after i time steps. This means that out. We set k min to 1, k max to 20, and use a timeout of 1.4
for all i, ∣x i∣+∣y i∣≤∣x i+1∣+∣y i+1∣. Let V
i
=∣x˙ i∣+∣y˙ i∣. By (19), hours for each loop iteration (i.e., 30 hours if all values of k
we know that for each V , there must be some k, such that time out).
i
V i+k−V i < −ϵ. Thus, for any n, we can construct a sequence Results. We end up with 71 subregions. For each subregion,
V j0,V j1,V j2,...V
jn
such that j
0
=0 and V
ji
−V
ji+1
>ϵ. If we Algorithm 1 returns UNSAT. The minimum returned value forthe verified controller, starting from the same starting state.
Notehowthespacecraftmovesnearlydirectlytowardsthegoal
region.
Thesuccessfulverificationof(19)isnotsufficienttoestablish
that the deputy eventually reaches the chief. We would need to
establish a second property, namely that once the spacecraft
is moving towards its goal, it always gets closer (by at least
some ϵ) within k steps. Let x ,y be the position i steps from
i i
somestartingposition(x 0,y 0).Thiscanbeformalizedwiththe
property:
(a) Initial neural network.
(∣x 1∣+∣y 1∣)–(∣x 0∣+∣y 0∣)<0 (cid:212)⇒
(20)
∃k.(∣x k∣+∣y k∣)–(∣x 0∣+∣y 0∣)<–ϵ.
Formally verifying this property is left to future work.
B. An Alternative Approach using Polar Coordinates
Before moving to the Manhattan distance, we explored an
alternative approach using polar coordinates, which allows the
L2 norm to be used directly in the invariant while maintaining
linearity.Morespecifically,ifr isthedistancetotheoriginand
θ isthe anglefromthe x-axis,then wecanwrite theequivalent
(b) Retrained neural network.
of property (19) as:
Fig. 1: Design for Verification: An initial controller trajectory
compared to a final controller trajectory, with the same initial r′ −r<−ϵ∨r˙′ −r˙<−ϵ. (21)
state. The final controller has a more direct trajectory which is
more conducive to verification via k-induction. Note how much simpler property 21 is compared with prop-
erty(19).However,thereremaintwochallenges:trainingapolar
k is 1, the maximum is 12, the average is 5, and the median is controller and converting the dynamics to polar coordinates.
3. Training a controller for the polar system is not straight-
forward; it requires complex parameter changes, for example,
Notably, regions close to the goal region are more difficult:
adjusting the learning rate, observation vector order, and the
they require more subregions and take longer, whereas regions
length and normalization constants. However, these challenges
more distant can sometimes be verified without utilizing addi-
areultimatelysolvable,andwewereabletotrainanetworkthat
tional subregions. The minimum runtime (in seconds) for any
takes polar coordinate inputs. The output is still F and F , as
subregion is 0.02, the maximum is 4295.86, the average is x y
we did not envision changing the physical spacecraft system.
193.62, and the median is 1.76.
Thesecondchallengeprovedmoredifficult.Weneededaway
As a sanity check, we validated our results experimentally
to calculuate new values of r and θ, given current values of r,
by running a simulation framework. Starting from randomly
θ, r˙, and θ˙, as well as F and F . We did not find closed-form
sampled points in the state space, we confirmed that the k- x y
solutions in the literature for the Clohessy–Wiltshire Equations
inductivepropertyholdsonthetrajectorystartingateachpoint.
utilizing polar coordinates. We thus converted equations (7)
These checks also succeeded.
through (10)topolarcoordinatesusingthestandardconversion
Discussion. Initially, we applied our approach to the neural
equations:
network controller described in [62]. The original network
topology (two hidden layers with 256 nodes each) resulted √ y
in lengthy verification times. Moreover, for many regions, the x=rcosθ, y=rsinθ, r= x2 +y2, θ=tan−1 (22)
x
verificationfailed:wediscoveredcounter-examplesforalltested
WeencodedthederivationoftheequationsdirectlyinPython,
values of k.
which allowed usto confirm in simulation thatour polar neural
Figure 1a shows an example counterexample
network had behavior similar to that of the original model.
trajectory from the original neural network. The starting
However,attemptingformalverificationwiththenewdynamics
state is [x = 0.5347935396499356,y = 0.51,x˙ = proved difficult. The new dynamics are highly non-linear. We
0.00038615766226848813,y˙ = 0.00038615766226848813]. attemptedtousetheOVERT tool2 forthepurposeoflinearizing
The controller moves steadily away from the goal, and only
r and θ. However, the results were too complex and ultimately
after many steps turns the spacecraft around to move towards
unsuccessful.Itwasatthispointthatwedecidedtoinsteaduse
the goal.
the L1 norm and revert to standard rectangular coordinates.
Such trajectories provided motivation for the design changes
We report this effort here in order to highlight both the
mentioned in Section III-D. In particular, the changes to the
potential benefits and pitfalls of using a different coordinate
reward function strongly incentivize the controller to move
towards the goal region. Figure 1b shows the trajectory using 2https://github.com/sisl/OVERT.jlrepresentation.Ifthedynamicshadbeenmoretractableinpolar be used to tune how strongly the certificate over-approximates
space, this would have been an attractive direction. adherencetoeachconstraint.Similarly,constantsc ,c ,c can
s d u
be used to tune the relative weight of the two objectives. The
V. ALTERNATEVERIFICATIONAPPROACHES
final training objective O in (30) is what the optimizer seeks to
Whileexploringthek-inductionapproachesdescribedabove, minimize, by using stochastic gradient descent (SGD) or other
we concurrently explored an alternative approach using Neural optimization techniques.
LyapunovBarriercertificates.Theresultsofthateffortrepresent
γ lower bound. It is important to note that the RWA
the most complete verification results we have obtained to date
training objective does not explicitly penalize deviations from
and are reported in [56]. Here, for convenience, we review that
Equation (23). Instead, because V is implemented as a neural
approach at a high level and present some details not reported
network using floating-point arithmetic, it has only a finite
there. We also discuss several reachability-based approaches,
number of possible inputs and outputs, so Equation (23) must
which we also applied to the 2D docking problem, but which
hold for some γ. In practice, we can use Marabou to find γ
were, ultimately, unsuccessful.
by doing a linear search for the minimum value of V: we
A. RWA Certificates simplysetγ tosomeinitialvalue,sayα,thenrepeatedlycheck
∃x.V(x) < γ, updating γ with the new value each time the
Definition 2. A function V ∶X ↦R is an RWA certificate query is satisfiable, and repeat until the query is unsastisfiable.
for the task defined in Definition 1 if there exist some α>
SamplingfromXU andX∖XG. WhileXI istypicallydefined
β ≥ γ and ϵ > 0, such that the following constraints are as having both upper and lower bounds on state variables, this
satisfied.
is not the case for XU, which often has only lower bounds on
statevariables(thisisthecase,forexample,forthe2Ddocking
∀x∈X. V(x)≥γ (23) problem defined in Section III).
However, during training, we do impose an upper bound
∀x∈XI. V(x)≤β (24)
on the states sampled from XU. Specifically, if the controller
∀x∈X ∖XG. V(x)≤β→V(x)−V(f(x,π(x)))≥ϵ
operates over n-dimensional states x = [x 1,x 2,..,x n], we
(25)
sample points satisfying the following constraints:
∀x∈XU. V(x)≥α (26)
(x 1>p 1)∨(x 2>p 2)∨...∨(x n>p n) (31)
Anytupleofvalues(α,β,ϵ,γ)forwhichtheseconditionshold (x 1<p 1+γ 1)∧(x 2<p 2+γ 2)∧...∧(x n<p n+γ n) (32)
is called a witness for the certificate.3 RWA certificates provide
the following guarantee. Here, 31 represents the (given) lower bounds on the unsafe
region XU, and γ 1,...,γ
n
are chosen to be strictly greater than
Lemma 1. If V is an RWA certificate for a dynamical system
0.
with witness (α,β,ϵ,γ), then for every trajectory τ starting
A similar issuearises when sampling from X∖XG. This can
from a state x∈X ∖XG such that V(x)≤β, τ will eventually oftenbesolvedsimplybysamplinginsteadfromX∖(XG∪XU),
contain a state in XG without ever passing through a state in asthelowerboundsonvariablesinXU thencreateupperbounds
XU.
for the sampling step.
We use reinforcement learning to jointly train neural networks Masking out XU. For objective 28, if x′
i
lies in XU, we
for both the controller and the corresponding RWA certificate. replace the actual value of V(x′ i) with α. This is because we
RWA Training Loss. The training objective for RWA certifi- learn correct functional behaviors of XU through objective 29
cates is described below: regardless,andthususingtheactualvalueofV(x′ i)wouldlead
O s=c
s ∑
ReLU(δ 1+V(x 1i)−β)
(27)
t Co eu rtn in fie cc ae ts esa Wry at rr main ui pn .g e Tff oor it ma pn rd ove exc te rs as iniv ie ngp ,en tha elti oes b.
jective is
O d=c
di∣xi∈X
I
∑
∑i∣xi∈X
I ReLU(δ 2+ϵ+V(x′ i)−V(x i)) u ws he id chto trt ar ia ni in ngth ie ncc le ur dti efi sca bt oe thV tha elo cn ee rtf io fir caa tefe aw ndit te hr eati co on ns t, roa lf lt ee rr
.
1
i∣xi∈X∖(X U∪X G),V(xi)<β ∑i∣xi∈X∖(X U∪X G),V(xi)<β This is done to avoid erratic training of the controller when V
(28) has random weights.
O u=c
u ∑
ReLU(δ 3−V(x i)+α)
(29)
RWA Verification. In order to obtain formal guarantees, we
i∣xi∈X
U
∑i∣xi∈X
U
1 use Marabou to formally verify the constraints in Definition 2.
VerificationofRWAconstraintsisgenerallystraightforward,but
O=O s+O d+O
u
(30)
wehavetosimilarlyboundXU andX∖XG toverifyconstraints
Equation (27) penalizes deviations from constraint (24), 26 and 25 respectively. Instead of using X ∖XG as the input
Equation (28) penalizes deviations from constraint (25), and spacefor25,weuseinsteadX∖(XG∪XU),whichprovidesthe
Equation (29) penalizes deviations from constraint (26). We same guarantees. Moreover, instead of using XU as the input
incorporate parameters δ 1 > 0, δ 2 > 0, and δ 3 > 0, which can space for 26, we use the bounded space, call it XUS, used for
data sampling. To ensure this provides the same guarantees,
tim3 eTh syes se temco sn ast nr dain dt os na ore tps li am ci ela cr ot no stt rh ao ins te si on na[2 c9 o] mb pu at ca tr se afs epe sc ei tfi ,c opt to ind gis tc ore ut se e- we check that no states beyond the upper bound of XUS are
anunsafesetinstead. reachable.Insteadofencodingverificationasasinglepropertypassedto
theDNNverifier,verificationispartitionedintomuliplequeries.
This is done by paritioning the input space in the original
property into equally sized smaller state spaces, over which the
same property is checked. This helps avoid unreasonably long
verification times that can occur with a large monolithic query.
Retraining. If any of the RWA verification checks return
counterexamples, these are used to augment the training data
set,andthentrainingisdoneagain.Thisprocessrepeatsuntilno
more counterexamples are found. We weight counterexamples
more heavily in the objective function 30 (compared to points Fig. 2: Grid reachability, with a cell navigating towards the
in the initial training dataset) in order to focus the training on docking region (in green)
removing the counterexamples.
Results and Analysis. As shown in prior work in [56], RWA Algorithm 2: APPLYINGGRIDREACHABILITY
certificates can provide liveness and safety guarantees for the
1 Let IS be the input space
2D spacecraft docking problem defined in Section III. More
2 Let k be the step size
details and a pointer to the code can be found in [56].
3 Divide IS into cells C =c 0,c 1,...,c n
4 Let vertices V =C
B. Reachability Analysis Approaches 5 Initialize edge set E to be the empty set
In this subsection, we discuss approaches based on reacha- 6 i=0
bility analysis. While these approaches were ultimately unsuc- 7 for i≤n do
cessful on the case study problem outlined in section III, we 8 Denote set of adjacent cells to c i as C r
still mention them here, as the reasons for their failure may be 9 Add c i to C r if self-cycles are possible
of interest, and they may be useful on other problems. 10 for c r ∈C r do
11 if c r is reachable from c i in k steps then
Forward-tube and Backward-tube Reachability. Forward- 12 Add directed edge (c i,c r) to E
tube and backward-tube reachability attempt to generate a path 13 i=i+1
over abstract state spaces (i.e., sets of states) from the starting 14 Let G∶=(V,E)
state space to the goal state space. At each step along the 15 Check for cycles in G
abstract path, we check that every state in the abstract state 16 if G is acyclic then
set meets any safety guarantees. 17 Determine cells C s with no paths leaving input
In forward-tube reachability, a starting set of states XF0 and space
step size k is defined. Then, a set of states XF1 is constructed 18 return C s as cells meeting liveness property
such that all states reachable from XF0 in k steps are contained
within XF1. This process is continued, and additional sets of
states XFi+1 are constructed, each with the property that they no cycles and that it is not possible to reach any cells beyond
contain the states reachable from XFi in k steps. If at some the partitioned state space.
point, the constructed set is a subset of the goal region, then We applied this technique to the spacecraft example. A
the liveness property is ensured. However, it can be very chal- challenge is preventing self-cycles in the graph. One strategy
lenging to find a sequence of sets of states XFi that eventually for doing this is to construct cells where at least one velocity
lead to a subset of the goal region. This was the case for the component never changes sign. It is easy to see that for such
spacecraft example. cells, the spacecraft cannot remain in the cell forever, so we
On the other hand, in backward-tube reachability, we start can ignore self-loops on such cells. For cells containing a
with XB0 set equal to the goal states and define a step size velocity sign-change, we use a very narrow velocity range,
k. Then, a set of states XB1 is constructed such that all states narrowenoughtoensurethatthespacecraftleavestherangeink
reachable from XB1 in k steps are contained within XB0. Again, steps. It is also desirable to limit the number of cells reachable
this process can be repeated until the set of states includes the from a given cell, to avoid the need to do many reachability
initial states. A difficulty with this approach is computing a checks. This can be ensured by making the cells large enough
sufficiently large previous set of states at each step. that it is impossible to cross more than one cell in a single set
of k steps.
Grid Reachability. Grid reachability is a process which first
partitions a bounded subset of the state space into cells, then AnalysisofGridReachability. Weappliedgridreachabilityto
computesadirectedgraphwhereeachcellisavertex,andeach a state space with x,y ∈[−10,10] and x˙,y˙ ∈[−1.6,1.6] using
directed edge (a,b) denotes that vertex b is reachable from Algorithm 2. A binary search was conducted using Marabou to
vertex a in k steps, for a specific k, as shown in Fig. 2. The determine cell bounds such that cells could only reach adjacent
goal is to show that for all paths constructed from cells in the cells. The step size k was chosen to be 1.
defined initial state space, a goal region reachable. However, to We found a variety of cycles of increasing lengths, even
ensure liveness, it is also necessary to show that the graph has as cells were divided further in an attempt to refine the grid[6] G. Amir, O. Maayan, T. Zelazny, G. Katz, and M. Schapira. Verifying
Generalization in Deep Learning. In Proc. 35th Int. Conf. on Computer
AidedVerification(CAV),pages438–455,2023.
[7] G.Amir,M.Schapira,andG.Katz.TowardsScalableVerificationofDeep
ReinforcementLearning. InProc.21stInt.Conf.onFormalMethodsin
Computer-AidedDesign(FMCAD),pages193–203,2021.
[8] G.Amir,H.Wu,C.Barrett,andG.Katz. AnSMT-BasedApproachfor
Verifying Binarized Neural Networks. In Proc. 27th Int. Conf. on Tools
Fig. 3: Spurious trajectory with grid reachability and Algorithms for the Construction and Analysis of Systems (TACAS),
pages203–222,2021.
abstraction.Moreover,wefoundthatallcellshadpathsleaving
[9] G.Amir,T.Zelazny,G.Katz,andM.Schapira. Verification-AidedDeep
the input space. We showcase one such trajectory of cells with Ensemble Selection. In Proc. 22nd Int. Conf. on Formal Methods in
this behavior in Fig. 3. In this trajectory, we see that for the Computer-AidedDesign(FMCAD),pages27–37,2022.
[10] G. Anderson, S. Pailoor, I. Dillig, and S. Chaudhuri. Optimization
first three steps, the velocity component ranges are negative,
and Abstraction: a Synergistic Approach for Analyzing Neural Network
therebyguidingthespacecrafttowardsthegoalregion,butthere Robustness. In Proc. 40th ACM SIGPLAN Conf. on Programming
is a path from cell 3 to cell 4 that induces a positive velocity LanguagesDesignandImplementations(PLDI),pages731–744,2019.
[11] S. Bansal, M. Chen, S. Herbert, and C. J. Tomlin. Hamilton-Jacobi
component, allowing the path to diverge.
reachability:Abriefoverviewandrecentadvances. InConf.onDecision
Ultimately, the grid abstraction does not lend itself well to andControl,2017.
thelivenesstaskbecausesuchspuriouspathsaredifficulttorule [12] G.BasileandG.Marro. Controlledandconditionedinvariantsubspaces
out. While further refinement of the grid approach is possible inlinearsystemtheory.JournalofOptimizationTheoryandApplications,
3:306–315,1969.
andcouldeventuallyyieldaworkableapproach,wedetermined
[13] S.Bassan,G.Amir,D.Corsi,I.Refaeli,andG.Katz.FormallyExplaining
thatthecomplexityanddifficultyweretoohigh,andabandoned Neural Networks within Reactive Systems. In Proc. 23rd Int. Conf.
it in favor of the certificate approach mentioned earlier. onFormalMethodsinComputer-AidedDesign(FMCAD),pages10–22,
2023.
VI. CONCLUSION [14] L. Brunke, M. Greeff, A. W. Hall, Z. Yuan, S. Zhou, J. Panerati, and
A. P. Schoellig. Safe learning in robotics: From learning-based control
We have presented methods for verifying safety and liveness tosafereinforcementlearning. AnnualReviewofControl,Robotics,and
propertiesforDRLsystemsusingk-induction,NeuralLyapunov AutonomousSystems,5:411–444,2022.
Barrier Certificates, and reachability analysis. We explore their [15] R. Bunel, I. Turkaslan, P. Torr, P. Kohli, and P. Mudigonda. A Unified
View of Piecewise Linear Neural Network Verification. In Proc. 32nd
effectiveness on a 2D spacecraft docking problem posed in
Conf.onNeuralInformationProcessingSystems(NeurIPS),pages4795–
previous work. For this problem, we show how a k-induction 4804,2018.
based approach can be used alongside a design-for-verification [16] J.-T. Camino, C. Artigues, L. Houssin, and S. Mourgues. Linearization
ofeuclideannormdependentinequalitiesappliedtomultibeamsatellites
trainingschemetoprovidelivenessguarantees.Wealsodiscuss
design.ComputationalOptimizationandApplications,73:679–705,2019.
how Neural Lyapunov Barrier Certificates can be used to [17] M. Casadio, E. Komendantskaya, M. Daggitt, W. Kokke, G. Katz,
provide both liveness and safety guarantees. While reachability G. Amir, and I. Refaeli. Neural Network Robustness as a Verification
Property:APrincipledCaseStudy. InProc.34thInt.Conf.onComputer
analysis ultimately did not provide any formal guarantees, we
AidedVerification(CAV),pages219–231,2022.
discusstheapproachanditslimitations.Infuturework,weplan
[18] Y.-C. Chang and S. Gao. Stabilizing neural control using self-learned
to explore scaling these methods to more complex and realistic almostlyapunovcritics.2021IEEEInternationalConferenceonRobotics
control systems. andAutomation(ICRA),pages1803–1809,2021.
[19] Y.-C. Chang, N. Roohi, and S. Gao. Neural lyapunov control. In
VII. ACKNOWLEDGEMENTS H.Wallach,H.Larochelle,A.Beygelzimer,F.d'Alche´-Buc,E.Fox,and
R.Garnett,editors,AdvancesinNeuralInformationProcessingSystems,
This work was supported by AFOSR (FA9550-22-1-0227), volume32.CurranAssociates,Inc.,2019.
the Stanford CURIS program, the NSF-BSF program (NSF: [20] Y. Chow, O. Nachum, E. Duenez-Guzman, and M. Ghavamzadeh. A
lyapunov-based approach to safe reinforcement learning. In S. Bengio,
1814369,BSF:2017662),andtheStanfordCenterforAISafety.
H.Wallach,H.Larochelle,K.Grauman,N.Cesa-Bianchi,andR.Garnett,
The work of Amir was further supported by a scholarship editors, Advances in Neural Information Processing Systems 31, pages
from the Clore Israel Foundation. We thank Thomas Henzinger 8092–8101.CurranAssociates,Inc.,2018.
(ISTA), Chuchu Fan (MIT), and Songyuan Zhang (MIT) for [21] W. Clohessy and R. Wiltshire. Terminal guidance system for satellite
rendezvous. Journaloftheaerospacesciences,27(9):653–658,1960.
useful conversations and advice, which contributed to the suc-
[22] E.Cohen,Y.Elboher,C.Barrett,andG.Katz. TighterAbstractQueries
cess of this project. in Neural Network Verification. In Proc. 24th Int. Conf. on Logic for
Programming,ArtificialIntelligenceandReasoning(LPAR),2023.
REFERENCES
[23] D. Corsi, G. Amir, G. Katz, and A. Farinelli. Analyzing Adversarial
Inputs in Deep Reinforcement Learning, 2024. Technical Report. https:
[1] P. Alamdari, G. Avni, T. Henzinger, and A. Lukina. Formal Methods
//arxiv.org/abs/2402.05284.
with a Touch of Magic. In Proc. 20th Int. Conf. on Formal Methods in
Computer-AidedDesign(FMCAD),pages138–147,2020. [24] D. Corsi, G. Amir, A. Rodriguez, C. Sanchez, G. Katz, and R. Fox.
[2] B.AlpernandF.Schneider. Recognizingsafetyandliveness. Distributed Verification-Guided Shielding for Deep Reinforcement Learning, 2024.
Computing,2:117–126,091987. TechnicalReport.http://arxiv.org/abs/2406.06507.
[3] A.Ames,X.Xu,J.W.Grizzle,andP.Tabuada. Controlbarrierfunction [25] D.Corsi,E.Marchesini,A.Farinelli,andP.Fiorini.Formalverificationfor
basedquadraticprogramsforsafetycriticalsystems. Trans.onAutomatic safedeepreinforcementlearningintrajectorygeneration. In2020Fourth
Control,2017. IEEEInternationalConferenceonRoboticComputing(IRC),pages352–
[4] A. D. Ames, S. Coogan, M. Egerstedt, G. Notomista, K. Sreenath, and 359,2020.
P. Tabuada. Control barrier functions: Theory and applications. In [26] C.Dawson,S.Gao,andC.Fan. Safecontrolwithlearnedcertificates:A
EuropeanControlConf.,2019. survey of neural lyapunov, barrier, and contraction methods for robotics
[5] G.Amir,D.Corsi,R.Yerushalmi,L.Marzari,D.Harel,A.Farinelli,and andcontrol. IEEETransactionsonRobotics,2023.
G.Katz.VerifyingLearning-BasedRoboticNavigationSystems.InProc. [27] C. Dawson, Z. Qin, S. Gao, and C. Fan. Safe nonlinear control
29thInt.Conf.onToolsandAlgorithmsfortheConstructionandAnalysis using robust neural lyapunov-barrier functions. In Conference on Robot
ofSystems(TACAS),pages607–627,2023. Learning,pages1724–1735.PMLR,2022.[28] J.L.C.B.deFariasandW.M.Bessa. Intelligentcontrolwithartificial [53] B.Li,S.Wen,Z.Yan,G.Wen,andT.Huang. Asurveyonthecontrol
neuralnetworksforautomatedinsulindeliverysystems. Bioengineering, lyapunovfunctionandcontrolbarrierfunctionfornonlinear-affinecontrol
9(11):664,2022. systems. IEEE/CAAJournalofAutomaticaSinica,10(3):584–602,2023.
[29] A.Edwards,A.Peruffo,andA.Abate. Ageneralverificationframework [54] Y. Li. Deep Reinforcement Learning: An Overview, 2017. Technical
fordynamicalandcontrolmodelsviacertificatesynthesis,2023. Report.http://arxiv.org/abs/1701.07274.
[30] R.Ehlers.FormalVerificationofPiece-WiseLinearFeed-ForwardNeural [55] A. Lomuscio and L. Maganti. An Approach to Reachability Analysis
Networks. In Proc. 15th Int. Symp. on Automated Technology for forFeed-ForwardReLUNeuralNetworks,2017. TechnicalReport.http:
VerificationandAnalysis(ATVA),pages269–286,2017. //arxiv.org/abs/1706.07351.
[31] Y. Elboher, E. Cohen, and G. Katz. Neural Network Verification using [56] U. Mandal, G. Amir, H. Wu, I. Daukantas, F. Newell, U. Ravaioli,
Residual Reasoning. In Proc. 20th Int. Conf. on Software Engineering B. Meng, M. Durling, M. Ganai, T. Shim, G. Katz, and C. Barrett.
andFormalMethods(SEFM),pages173–189,2022. Formally Verifying Deep Reinforcement Learning Controllers with Lya-
[32] Y.Elboher,J.Gottschlich,andG.Katz.AnAbstraction-BasedFramework punov Barrier Certificates. In Proc. 24th Int. Conf. on Formal Methods
for Neural Network Verification. In Proc. 32nd Int. Conf. on Computer inComputer-AidedDesign(FMCAD),2024.
AidedVerification(CAV),pages43–65,2020. [57] B.Mildenhall,P.P.Srinivasan,M.Tancik,J.T.Barron,R.Ramamoorthi,
[33] T. Eliyahu, Y. Kazak, G. Katz, and M. Schapira. Verifying learning- andR.Ng. Nerf:Representingscenesasneuralradiancefieldsforview
augmented systems. Proceedings of the 2021 ACM SIGCOMM 2021 synthesis. CommunicationsoftheACM,65(1):99–106,2021.
Conference,2021. [58] M. Ostrovsky, C. Barrett, and G. Katz. An Abstraction-Refinement
[34] J.F.Fisac,M.Chen,C.J.Tomlin,andS.S.Sastry.Reach-avoidproblems Approach to Verifying Convolutional Neural Networks. In Proc. 20th.
withtime-varyingdynamics,targetsandconstraints.InProceedingsofthe Int. Symposium on Automated Technology for Verification and Analysis
18thinternationalconferenceonhybridsystems:computationandcontrol, (ATVA),pages391–396,2022.
pages11–20,2015. [59] P. Prabhakar and Z. Afzal. Abstraction Based Output Range Analysis
forNeuralNetworks,2020. TechnicalReport.https://arxiv.org/abs/2007.
[35] M. Ganai, Z. Gong, C. Yu, S. L. Herbert, and S. Gao. Iterative
reachability estimation for safe reinforcement learning. In Advances in 09527.
NeuralInformationProcessingSystems,2023. [60] Z. Qin, T.-W. Weng, and S. Gao. Quantifying safety of learning-based
self-driving control using almost-barrier functions. In 2022 IEEE/RSJ
[36] M.Ganai,C.Hirayama,Y.-C.Chang,andS.Gao. Learningstabilization
InternationalConferenceonIntelligentRobotsandSystems(IROS),pages
controlfromobservationsbylearninglyapunov-likeproxymodels. 2023
12903–12910.IEEE,2022.
IEEEInternationalConferenceonRoboticsandAutomation(ICRA),pages
[61] Z.Qin,K.Zhang,Y.Chen,J.Chen,andC.Fan.Learningsafemulti-agent
2913–2920,2023.
controlwithdecentralizedneuralbarriercertificates. InICLR,2021.
[37] O. Gates, M. Newton, and K. Gatsis. Scalable forward reachability
[62] U.J.Ravaioli,J.Cunningham,J.McCarroll,V.Gangal,K.Dunlap,and
analysisofmulti-agentsystemswithneuralnetworkcontrollers,2023.
K. L. Hobbs. Safe reinforcement learning benchmark environments for
[38] I.Goodfellow,Y.Bengio,andA.Courville. DeepLearning. MITPress,
aerospacecontrolsystems. In2022IEEEAerospaceConference(AERO),
2016.
pages1–20.IEEE,2022.
[39] S. Govindaraju and D. Dill. Verification by approximate forward and
[63] A. Rodriguez, G. Amir, D. Corsi, C. Sanchez, and G. Katz. Shield
backward reachability. In 1998 IEEE/ACM International Conference
Synthesis for LTL Modulo Theories, 2024. Technical Report. http:
on Computer-Aided Design. Digest of Technical Papers (IEEE Cat.
//arxiv.org/abs/2406.04184.
No.98CB36287),pages366–370,1998.
[64] L. H. Sena, I. V. Bessa, M. R. Gadelha, L. C. Cordeiro, and E. Mota.
[40] A.GuptaandI.Hwang.Safetyverificationofmodelbasedreinforcement
Incrementalboundedmodelcheckingofartificialneuralnetworksincuda.
learningcontrollers,2020.
In 2019 IX Brazilian Symposium on Computing Systems Engineering
[41] W.HaddadandV.Chellaboina.Nonlineardynamicalsystemsandcontrol:
(SBESC),pages1–8,2019.
Alyapunov-basedapproach. NonlinearDynamicalSystemsandControl:
[65] G.Singh,T.Gehr,M.Puschel,andM.Vechev. AnAbstractDomainfor
ALyapunov-BasedApproach,012008.
Certifying Neural Networks. In Proc. 46th ACM SIGPLAN Symposium
[42] G. W. Hill. Researches in the lunar theory. American journal of onPrinciplesofProgrammingLanguages(POPL),2019.
Mathematics,1(1):5–26,1878. [66] O. So and C. Fan. Solving stabilize-avoid optimal control via epigraph
[43] K.-C. Hsu, V. Rubies-Royo, C. J. Tomlin, and J. F. Fisac. Safety form and deep reinforcement learning. In Proceedings of Robotics:
and liveness guarantees through reach-avoid reinforcement learning. In ScienceandSystems,2023.
ProceedingsofRobotics:ScienceandSystems,Virtual,72021. [67] V.Talpaert,I.Sobh,B.R.Kiran,P.Mannion,S.Yogamani,A.El-Sallab,
[44] T.Huang,S.Gao, andL.Xie. Aneurallyapunovapproachtotransient and P. Perez. Exploring applications of deep reinforcement learning for
stabilityassessmentofpowerelectronics-interfacednetworkedmicrogrids. real-worldautonomousdrivingsystems,2019.
IEEEtransactionsonsmartgrid,13(1):106–118,2021. [68] V. Tjeng, K. Xiao, and R. Tedrake. Evaluating Robustness of Neural
[45] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu. Safety Verification Networks with Mixed Integer Programming. In Proc. 7th Int. Conf. on
of Deep Neural Networks. In Proc. 29th Int. Conf. on Computer Aided LearningRepresentations(ICLR),2019.
Verification(CAV),pages3–29,2017. [69] M. Tong, C. Dawson, and C. Fan. Enforcing safety for vision-based
[46] R.Ivanov,J.Weimer,R.Alur,G.J.Pappas,andI.Lee.Verisig:verifying controllers via control barrier functions and neural radiance fields. In
safetypropertiesofhybridsystemswithneuralnetworkcontrollers,2018. 2023IEEEInternationalConferenceonRoboticsandAutomation(ICRA),
[47] P.Jin,J.Tian,D.Zhi,X.Wen,andM.Zhang. Trainify:Acegar-driven pages10511–10517.IEEE,2023.
trainingandverificationframeworkforsafedeepreinforcementlearning. [70] M.Usman,D.Gopinath,Y.Sun,Y.Noller,andC.Paˇsaˇreanu. NNrepair:
InComputerAidedVerification:34thInternationalConference,CAV2022, Constraint-based Repair of Neural Network Classifiers, 2021. Technical
Haifa, Israel, August 7–10, 2022, Proceedings, Part I, page 193–218, Report.http://arxiv.org/abs/2103.12535.
Berlin,Heidelberg,2022.Springer-Verlag. [71] H.Wu,O.Isac,A.Zeljic´,T.Tagomori,M.Daggitt,W.Kokke,I.Refaeli,
[48] K.D.JulianandM.J.Kochenderfer. Areachabilitymethodforverifying G. Amir, K. Julian, S. Bassan, et al. Marabou 2.0: A Versatile Formal
dynamicalsystemswithdeepneuralnetworkcontrollers,2019. AnalyzerofNeuralNetworks.InProc.36thInt.Conf.onComputerAided
[49] G.Katz,C.Barrett,D.Dill,K.Julian,andM.Kochenderfer.Reluplex:An Verification(CAV),2024.
EfficientSMTSolverforVerifyingDeepNeuralNetworks. InProc.29th [72] X.Xu,P.Tabuada,J.W.Grizzle,andA.D.Ames. Robustnessofcontrol
Int.Conf.onComputerAidedVerification(CAV),pages97–117,2017. barrierfunctionsforsafetycriticalcontrol. Int.FederationofAutomatic
[50] G. Katz, D. Huang, D. Ibeling, K. Julian, C. Lazarus, R. Lim, P. Shah, Control,2015.
S.Thakoor,H.Wu,A.Zeljic´,D.Dill,M.Kochenderfer,andC.Barrett. [73] Y. Yang, Y. Jiang, Y. Liu, J. Chen, and S. E. Li. Model-free safe
The Marabou Framework for Verification and Analysis of Deep Neural reinforcement learning through neural barrier certificate. IEEE Robotics
Networks.InProc.31stInt.Conf.onComputerAidedVerification(CAV), andAutomationLetters,2023.
pages443–452,2019. [74] D.Yu,H.Ma,S.Li,andJ.Chen. Reachabilityconstrainedreinforcement
[51] B. Ko¨nighofer, F. Lorber, N. Jansen, and R. Bloem. Shield Synthesis learning.InInternationalConferenceonMachineLearning,pages25636–
for Reinforcement Learning. In Proc. Int. Symposium on Leveraging 25655.PMLR,2022.
Applications of Formal Methods, Verification and Validation (ISoLA), [75] H. Yu, C. Hirayama, C. Yu, S. Herbert, and S. Gao. Sequential neural
pages290–306,2020. barriers for scalable dynamic obstacle avoidance. In 2023 IEEE/RSJ
[52] L.Kuper,G.Katz,J.Gottschlich,K.Julian,C.Barrett,andM.Kochen- InternationalConferenceonIntelligentRobotsandSystems(IROS),pages
derfer. Toward Scalable Verification for Safety-Critical Deep Networks, 11241–11248.IEEE,2023.
2018. TechnicalReport.https://arxiv.org/abs/1801.05950.