AnyTaskTune: Advanced
Domain-Specific Solutions through Task-Fine-Tuning
DataTager∗
Abstract
ThepervasivedeploymentofLargeLanguageModels-LLMsinvarioussectors
oftenneglectsthenuancedrequirementsofindividualsandsmallorganizations,
whobenefitmorefrommodelspreciselytailoredtotheirspecificbusinesscontexts
ratherthanthosewithbroadlysuperiorgeneralcapabilities.Thisworkintroduces
AnyTaskTune,anovelfine-tuningmethodologycoinedasTask-Fine-Tune,specif-
icallydevelopedtoelevatemodelperformanceonadiversearrayofdomain-specific
tasks. Thismethodinvolvesameticulousprocesstoidentifyanddefinetargeted
sub-taskswithinadomain,followedbythecreationofspecializedenhancement
datasetsforfine-tuning,therebyoptimizingtask-specificmodelperformance.We
conductedcomprehensivefine-tuningexperimentsnotonlyinthelegaldomainfor
taskssuchaskeywordextractionandsentencepredictionbutacrossovertwenty
differentsub-tasksderivedfromthedomainsoffinance,healthcare,law,psychology,
consumerservices,andhumanresources.Tosubstantiateourapproachandfacilitate
communityengagement,wewillopen-sourcethesebilingualtaskdatasets. Our
findingsdemonstratethatmodelsfine-tunedusingtheTask-Fine-Tunemethodology
notonlyachievesuperiorperformanceonthesespecifictasksbutalsosignificantly
outperformmodelswithhighergeneralcapabilitiesintheirrespectivedomains.Our
workispubliclyavailableathttps://github.com/PandaVT/DataTager.
1 Introduction
Recently,therapidadvancementanddeploymentofLargeLanguageModels(LLMs)havetransformed
varioussectorsbyprovidingunprecedentednaturallanguageprocessingcapabilities[21].Traditionally,
thesemodelshavebeendevelopedwithafocusonenhancingtheirgeneralabilities,aimingtocreate
universallypowerfultoolsthatexcelacrossabroadspectrumoftasksanddomains. However,this
generalized approach often fails to meet the specific requirements of individual users and small
organizations,whoseneedsareintricatelytiedtotheiruniqueoperationalcontexts. Toaddressthis
issue,oneverysimplewayistofine-tuneLLMsfordifferenttasks.Inthemedicalfield,researchers
haveoptimizedLLMsusingmedicaldatasetstosupportfunctionalitiessuchasmedicaldocument
interpretation and diagnostic consultations. For example, ChatMed [23], DISC-MedLLM [2],
HyKGE[9], IvyGPT[12], andHuatuoGPT[20]arenotableexamples, demonstratingsignificant
advancementswithintheirspecializedmedicaldomainscomparedtogenericLLMs.Similarly,inthe
legalfield,researchershaveintroducedfine-tunedLLMstrainedonlegalcorporatosupportactivities
suchaslegalresearch,contractanalysis,andlegaldocumentsummarization.ModelslikeChatLaw[7],
LawGPT[22],andDISC-LawLLM[19]exemplifythistrend,demonstratingadeepergraspoflegal
language and principles compared to their generic counterparts. Within finance, researchers are
leveragingdiversefinancialdatasets,includingconversationaldataandmarketreports,tofine-tune
LLMsforapplicationssuchasfinancialforecasting,riskassessment,andfrauddetection.Examples
includeFinGPT[16],DISC-FinLLM[4],andPIXIU[13].Additionally,inotherfields,modelslike
∗Pleasecitethisworkas“DataTager(2024)". Fullauthorshipcontributionstatementsappearattheendof
thedocument.Correspondenceregardingthistechnicalreportcanbesenttoreport@datatager.com
Preprint.Underreview.
4202
luJ
9
]LC.sc[
1v49070.7042:viXraFaiMA[17],designedforMulti-domainapplications,andKuaiji[10],tailoredforaccountingtasks,
illustratetheversatilityandpotentialoffine-tunedLLMsinspecializeddomains.
Toaddress thisdiscrepancy, weintroduce anovelfine-tuning paradigmspecifically designedfor
ExplicitDataSets,whichweterm"taskfinetune."Thismethoddivergesfromconventionaltraining
techniquesbyemphasizingprecisionandspecificityovergeneralperformance.ByfocusingonExplicit
Data Sets that contain clear, directive input-output pairs with specific instructions, this approach
allowsforprecisetailoringofmodelstoperformdesignatedtaskseffectively. Suchfine-tuningnot
onlyenhancesthemodel’sabilitytoexecuteparticularfunctionsbutalsosignificantlyimprovesits
applicabilityandefficiencyinreal-worldscenarios.
Thecornerstoneofthiswork,"AnyTaskTune,"istovalidateandelaborateonthe"taskfinetune"
method,utilizingExplicitDataSetsacrossvariousdomains. Thismethodinvolvesidentifyingthe
spectrumoftasksrequiredindifferentfieldsandscenariosandmeticulouslydevelopingnumerous
ExplicitDataSetstoconducttask-specificfine-tuning.Theaimistooptimizethemodel’sperformance
forindividualandcorporateuse,ensuringthatitmeetsthenuanceddemandsofusersinfieldssuch
asfinance,healthcare,law,psychology,consumerservices,andhumanresources.
Toshowcasetheeffectivenessandversatilityofthisapproach,weconductedexperimentsusingover
twentydifferentExplicitDataSetsthatwewillopen-source.Thesedatasetsarebilingualanddesigned
tocatertoadiverseglobalaudience,providingavaluableresourceforthecommunitytoengagein
furtherresearchandenhanceapplication-specificmodelimprovements.
Byconcentratingonspecific,clearlydefinedtasksratherthangeneralcapabilities,"AnyTaskTune"
significantlyoutperformstraditionalmodelsthatprioritizebreadthoverdepth. Thisstrategicshift
towards more specialized, task-oriented model training represents a critical evolution in how we
developanddeployLLMs,makingthemmorerelevantandvaluableinindustry-specificapplications.
Theprimarycontributionsofthispapercanbesummarizedasfollows:
• Introductionofthe"taskfinetune"paradigm,specificallydesignedforExplicitDataSets,
emphasizingprecisionandspecificityovergeneralperformance.
• Development and utilization of over twenty bilingual Explicit Data Sets across various
domainstovalidatetheeffectivenessofthe"taskfinetune"method.
• Evaluation of multiple models and datasets across various domains, demonstrating that
task-specificfine-tuningyieldsbetterresults.
• Open-sourcingofthedevelopedExplicitDataSets,providingavaluableresourceforthecom-
munitytoengageinfurtherresearchandenhanceapplication-specificmodelimprovements.
• Implementation of our data processing model, DataTager-LLM, as an online accessible
website,https://datatager.com.
2 AnyTaskTune
2.1 ModelRequirementsinBusinessApplications
Inpracticalapplications,simplypursuingtheperformanceofageneralmodelordeployingadomain-
specificmodelaloneisofteninsufficienttoaddressreal-worldproblems. Whilescientificresearch
outputs need to be diverse and comprehensive, businesses require standardized and controllable
solutions.Formostenterprisesandorganizations,theirneedsarehighlyspecificandcontextualized,
whichcannotbefullymetbyasinglegenerallanguagemodel. Insuchcases,generalmodelsmay
underperformonspecifictasksorfailtoachievetheexpectedefficiencyandaccuracy.Moreover,while
somedomain-specificmodelsmayprovidegoodperformancewithintheirfields,theylackflexibility
andscalability,makingitdifficulttoadapttoever-changingbusinessneeds.Therefore,thereisaneedfor
anewmethodtofine-tuneandoptimizemodels,ensuringtheynotonlypossessthebroadapplicabilityof
generalmodelsbutalsomeetthespecificneedsofparticulardomainsandtasks.AnyTaskTuneaddresses
thisissuebyusingspeciallydesignedexplicitdatasetsfortaskfine-tuning, ensuringthemodel’s
precisionandefficiencyinspecifictasks,therebyenhancingitsvalueinreal-worldbusinessapplications.
2Fields DataSets DataTagerLLM
Traditional Interaction Mode TaskPool DataSet
action2 action1 action1-a action1 action2
query a a…
Analysis action3 a. First xxx …… action1-b b c b c d…
b. Second xxx action1-c
summary
action4 c. …… action2-a ……
…… ……
AI-powered Interaction Mode Task-LM
action1
action2 action1-LM action1-a-LM
a. Task-LM🤖 action2-LM action1-b-LM
action3 ……
Field b.Task-LM🤖 action1-c-LM
action4 c. Task-LM🤖 Mixed LLM action2-a-LM
……
……
Figure1:AnyTaskTuneFramework
2.2 DataTager
First,leveragingourpriorexperienceandexpertiseinverticaldomains,wecollectedrawdatafrommul-
tiplefields,includingfinance,healthcare,law,psychology,consumerservices,andhumanresources.By
surveyingindustryprofessionalstounderstandtheirexpectationsandrequirementsformodels,wesyn-
thesizedmultiplebranchdatasets.Thesedatasetscovercoretasksacrossvariousdomainsandinclude
detailedandvariantdatafromreal-worldscenarios.Throughthetrainingofthesebranchdatasets,we
developedDataTager-LLM,alarge-scaledatasynthesismodel.DataTager-LLMformsthefoundation
ofourproductandservesasacoretoolforfurtherresearchandapplication.Currently,wehavenoplans
toopen-sourceDataTager-LLMtobetterprotectourintellectualpropertyandcommercialinterests.
2.3 ExplicitandImplicitDataSets
Wedefinedwhatconstitutesa"good"datasetforbusinessesanddistinguishedbetweenthesedatasets
basedonthepresenceofexplicitinstructions. Specifically,datasetswithclearinstructions,suchas
"instruction:Pleasesummarizethisnewsarticleandextractthekeypoints;input:xxx,"aredesigned
formodelstoperformspecifictasks.Explicitdatasetsprovidemodelswithclearguidelines,enhancing
theirabilitytounderstandandexecutespecifictasksaccuratelyandefficientlyinpracticalapplications.
Ontheotherhand,datasetswithoutexplicitinstructions,suchas"instruction:,input:xxx,"involve
randomquerieswithoutfixedtasktypes,thusconsideredimplicitdatasets.Implicitdatasetsarebetter
suitedforhandlingopen-endedquestionsandunstructureddatabutaremorechallengingtotrainand
apply.ForbusinesseslookingtoenhancetheiroperationswithLLMs,thefirststepistoidentifythe
varioustasktypeswithintheircurrentoperationalcontext.Thishelpsdeterminethenumberofexplicit
datasetsneeded. Eachdatasetisthensynthesizedindividually,andageneralLLMthatalignswith
thebusiness’susualinteractionpatternsisfine-tunedacrossmultipletaskstocreateamodelthatbest
fitsthespecificbusinessscenarios.
2.4 AnyTaskTune
Differentfieldshavemultipleinteraction(action)modes. Takethemedicalfield, forexample: if
triageisconsideredanaction,theactualtasksitinvolvesinclude:a.Patientsdescribingtheirissues
innon-standardlanguage;b. Triagestafftranslatingthisintomedicalstandardlanguage;c. Staff
analyzing the completeness of the information and asking follow-up questions if necessary; d.
Extractingmedicalkeypointsanddirectingthepatienttotheappropriatedepartment. Thistriage
actionthusincludesfoursub-tasks(a,b,c,d). WecanuseDataTagertogeneratedatasetsforeach
3Explicit DataSet
🌞
Instruction: Please extract three to six key medical information from the user's description.
Input: I've also been caught up in the flu wave these past few days. Now I've been coughing for almost three months without getting better.
I want to ask, what should I do if I've been coughing for nearly three months?
Output: Symptom Description: Long-term cough (lasting for three months)
Duration of Symptoms: Three months
Possible Cause: Initially may have been caused by a cold infection
Help Sought: Seeking a solution for a persistent cough
Implicit DataSet 🌛
Instruction:
Input: I've also been caught up in the flu wave these past few days. Now I've been coughing for almost three months without getting better.
I want to ask, what should I do if I've been coughing for nearly three months?
Output: I'm sorry to hear about your persistent cough. A cough lasting three months is considered chronic and may require medical
evaluation to determine the underlying cause. Here are some general steps you can take:
1.Consult a Doctor: It's essential to see a healthcare professional. They can perform a thorough examination and may order tests such as
chest X-rays, spirometry, or blood tests to…
Figure2:ExplicitandImplicitDataSets
sub-task.Thesedatasetscollectivelyrepresentthetriageactionprocess.Then,wetrainmodelsbased
onthesedatasets,withmultipleapproachesavailable:
Ifwetrainaseparatemodelforeachsub-task,themodelcanaddressthatspecificsub-taskeffectively.
Forinstance,amodelspecificallytrainedforsub-taskacanhandlethetranslationofpatientdescriptions,
whileanothermodeltrainedforsub-taskbcanconvertnon-standardlanguageintomedicalstandard
language.Ifwecombinethesesub-taskdatasetstotrainalargemixedmodel,differentinstructionswill
triggerdifferenttasktypes.Thismethodcanhandlemultipletaskswithinonemodelbutrequiresamore
complextrainingprocessandmorecomputationalresources.Itisimportanttonotethatifwechoose
totrainmultiplesmallmodels,basedonourexperience,a1.5billionor3billionmodelcanhandleless
complextaskswellwithoutdatadrift.Iftheperformanceofthemixedmodeldeclines,addingsome
generaldatasetstothetrainingprocesscanhelp. Thisway,wecanusehigh-performingmodelsto
replacetraditionalsub-tasksinactioninteractions,creatinganewinteractionmodelandreducingcosts.
3 Experiments
TovalidatetheeffectivenessandversatilityoftheAnyTaskTuneparadigm,weundertookextensive
experiments across multiple domains: finance, healthcare, law, psychology, and role-play. We
comparedtheperformanceofAnyTaskTuneagainstvariousmodelsincludingclosed-sourcelarge
languagemodels(LLMs),open-sourceLLMs,anddomain-specificmodels.Critically,ourexperiments
maintainedastrictseparationbetweentrainingandtestingdatasetstoensureunbiasedevaluationand
reproducibilityofresults.
3.1 ExperimentalSetup
Ourexperimentswerestructuredasfollows:
• ModelBase:WeutilizedQwen2-7B[1]asthebasemodelforAnyTaskTunetraining.This
modelwasfine-tunedonspecificsub-taskswithineachdomain,andthentestedtoobtain
theexperimentalresults.
• ModelCategories:
– ClosedSourceLLMs:GPT-4[11],GPT-3.5,LLaMA3-8b,andLLaMA3-70b.
– OpenSourceLLMs:Qwen,Baichuan[15],ChatGLM[8],andinternalLM[3].
4– Domain Models: ChatMed [23], DISC-MedLLM [2], FinGPT [16], ChatLaw [7],
DISC-LawLLM[18],MindChat[14],SoulChat[5],andMachineMindset(MBTI)[6].
• EvaluatedDomainsandTasks:
– Medical:Sub-tasksM1,M2,M3.
– Finance:Sub-tasksF1,F2,F3.
– Law:Sub-tasksL1,L2,L3,L4.
– Psychology:Sub-tasksP1,P2.
– Role-Play:Sub-taskMBTI.
3.2 ResultsandDomain-SpecificAnalysis
TheresultsoftheexperimentsaresummarizedinTable1.OurAnyTaskTunemodels,fine-tunedon
specificdomaindata,demonstratedsignificantperformanceimprovements,evensometimessurpassing
well-regarded models like GPT-4 and LLaMA3-70b. The following points detail cross-domain
influencesandtask-specificadaptabilitiesobservedduringtesting:
Table1:PerformanceComparisonAcrossVariousDomainsandModels
Med Finance Law Psychology Role-Play
M1 M2 M3 F1 F2 F3 L1 L2 L3 L4 P1 P2 MBTI
ClosedSourceLLMs
GPT4 0.526 0.671 0.643 0.492 0.615 0.719 0.592 0.593 0.636 0.662 0.521 0.661 0.731
GPT3.5 0.493 0.482 0.536 0.312 0.641 0.623 0.471 0.526 0.596 0.562 0.482 0.472 0.652
LLaMA3-8b 0.267 0.389 0.419 0.316 0.523 0.462 0.195 0.325 0.286 0.353 0.312 0.278 0.259
LLaMA3-70b 0.631 0.687 0.527 0.48 0.625 0.731 0.629 0.582 0.738 0.516 0.622 0.585 0.526
OpenSourceLLMs
Qwen2-7B 0.338 0.347 0.27 0.301 0.497 0.516 0.318 0.379 0.236 0.295 0.34 0.362 0.429
Baichuan2-13B 0.382 0.326 0.392 0.332 0.326 0.459 0.415 0.426 0.274 0.347 0.363 0.264 0.452
ChatGLM4-9B 0.143 0.197 0.242 0.295 0.257 0.236 0.289 0.357 0.322 0.268 0.159 0.304 0.219
internLM2-20B 0.429 0.357 0.32 0.259 0.238 0.326 0.325 0.373 0.391 0.305 0.283 0.362 0.345
DomainModel
ChatMed 0.739 0.659 0.631
DISC-MedLLM 0.744 0.631 0.759
FinGPT 0.625 0.593 0.639
ChatLaw 0.672 0.549 0.721 0.754
DISC-LawLLM 0.625 0.587 0.648 0.805
MindChat 0.542 0.583
SoulChat 0.319 0.497
MachineMindset(MBTIGPT) 0.873
TaskFineTunedModel
AnyTaskTune-Qwen2-7B-Med 0.835↑ 0.751↑ 0.719↑ 0.247↓ 0.234↓ 0.251↓ 0.242↓ 0.372↓ 0.196↓ 0.513↑ 0.429↑ 0.358↓ 0.326↓
AnyTaskTune-Qwen2-7B-Finance 0.316↓ 0.295↓ 0.283↑ 0.458↑ 0.739↑ 0.816↑ 0.462↑ 0.35↓ 0.429↑ 0.391↑ 0.163↓ 0.193↓ 0.252↓
AnyTaskTune-Qwen2-7B-Law 0.42↑ 0.318↓ 0.228↓ 0.429↑ 0.523↑ 0.318↓ 0.75↑ 0.743↑ 0.793↑ 0.798↑ 0.451↑ 0.392↓ 0.392↓
AnyTaskTune-Qwen2-7B-Psychology 0.195↓ 0.228↓ 0.217↓ 0.232↓ 0.418↓ 0.385↓ 0.314↓ 0.252↓ 0.158↓ 0.329↓ 0.469↑ 0.659↑ 0.572↑
AnyTaskTune-Qwen2-7B-RolePlay 0.275↓ 0.284↓ 0.315↑ 0.219↓ 0.238↓ 0.316↓ 0.413↑ 0.294↓ 0.429↑ 0.362↑ 0.459↑ 0.592↑ 0.711↑
3.3 DetailedComparativeAnalysis
3.3.1 ImpactAcrossDomains
MedicalDomain:TheAnyTaskTune-Qwen2-7B-Medmodelshowcasedexceptionalperformance
inmedicalsub-tasks,notablyachievingaF1scoreof0.835inM1,whichisasubstantialimprovement
overthebaseQwen2-7Bmodel’sscoreof0.338.Thisprecisionunderscoresitsreliabilityinhandling
medicaldata. However,whenthismodelwastestedinfinanceandlawdomains,itsperformance
droppedto0.247and0.242respectively,illustratingasignificantdeclineduetothemodel’sparameters
beinghighlyspecializedtomedicaldata.
Financial Domain: AnyTaskTune-Qwen2-7B-Finance performed excellently in financial tasks,
especiallyF2andF3,withscoresof0.739and0.816,outperformingthebasemodel’sscoresof0.497
and0.516inthesetasks.Applyingthismodeltolegaltasksresultedinmoderateperformancedrops
(from0.462inlegaltasksdownfrom0.458infinancetasks),likelyduetosomeoverlapinstructured
dataandterminologicalprecisionbetweenthefinanceandlegalfields.However,performanceinthe
medicaldomainwasnotablypoorer,confirmingthedomain-specificnatureofthefine-tuning.
5Legal Domain: The AnyTaskTune-Qwen2-7B-Law excelled in legal tasks with scores of 0.75,
0.743, 0.793, and 0.798 across L1, L2, L3, and L4. Its performance in the finance domain was
competitive,withscoresslightlylowerbutstillrespectable. However,whenappliedtothemedical
and psychological domains, the performance was limited, further highlighting the challenges of
cross-domainapplicabilityduetodifferentknowledgebasesanddatacharacteristics.
PsychologyandRole-PlayDomainsTheAnyTaskTune-Qwen2-7B-Psychologymodelshowedgood
resultsinpsychology,particularlyintheP2taskwithascoreof0.659.Thenatureofpsychological
tasks,whichofteninvolvecomplexlanguageandemotionalanalysis,allowsthismodeltoperform
reasonablywellinrole-playscenariosaswell,evidencedbyascoreof0.572intheMBTItask.
Role-PlayDomain:Similartothepsychologymodel,AnyTaskTune-Qwen2-7B-RolePlaydemon-
stratedgoodperformanceduetooverlappingrequirementsinemotionalandlanguageprocessing
betweenthetwofields.
3.3.2 TaskAdaptabilityandGeneralizability
Whiledomain-specificfine-tuningledtoexceptionalperformancewithinrespectivefields,itgenerally
resultedinreducedeffectivenesswhenmodelswereappliedacrossunrelateddomains.Forinstance,
whileAnyTaskTune-Qwen2-7B-Medexcelledinmedicaltasks,itsapplicationinnon-medicalfields
showedsignificantperformancereductions,aclearindicationofthetrade-offsinvolvedinspecialized
fine-tuning.
Comparison with Base Model: The original Qwen2-7B model, without fine-tuning, generally
showedlowerperformanceacrossallspecifictaskscomparedtothefine-tunedAnyTaskTunemodels.
Thishighlightstheeffectivenessoftask-specificfine-tuninginenhancingmodelcapabilities,although
itrestrictsthemodel’sflexibilityacrossvariousdomains.
3.4 Sub-taskDetails
Thespecifictypesofsub-tasksevaluatedinthisexperimentwillbedetailedintheaccompanying
GitHubrepository. Thisrepositorywillprovidecomprehensivedescriptionsandexamplesofeach
sub-task,offeringvaluableinsightsforfurtherresearchandapplicationdevelopment.
Explicit
I O fin lmup t su p at u l: s t o :C e a xIn f p y ly ooo ruu e e cre n oc j moo y pm e lem d x e ' I nn n ad c r e rs p ao ttm ii voe en s'm , ayo nov dui e mms is i ngi dm h -ti b l l ea ik nr e dto i' nI ' n gIn t e cc r oe s np te cti l eo l pan r t’ s',? . 'The Matrix', 'Memento', and 'Shutter Island'. These I O r ee fn d fu ip u ct cu ip eet n u h: t t e h:H a e t ao iT n tw io g n c gr wea shdn yeu I snc tr e ene med onu t s e c n cre e ag e nm y d sy ec ido gh ,n n o as im u fn im d ce a p se netn ti a le o ylr n i g n l i oy gn w c lw eo ea rn i k ns ystu oe m a ur r, r p o c et uoi nno n edn s r i gwd d yu e i nrr u i din sn o ag s w g uw esl .a i an ti nt ne dgr ? dy oo ou rr s .h Aom dde i, t iu os nin ag ll yp ,r uo pg gra rm adm ina gb l te o t eh ne er rm gyo -stats to
Implicit
Instruction:You are an experienced lawyer. Please help me break down a complex case into several sub-cases. The Instruction: Please extract the emotional characteristic descriptive words from users' e-commerce reviews.
sub-cases should be independent of each other. Input: Good！！！
Input:A woman had a sterilization procedure after giving birth to a son in 1992, during the era of the 'one-child Output: Good, recommendable, great value for money
policy', and now at 53, she wants to assert her rights. How should she proceed?
Output:1. Legality and effectiveness of the sterilization procedure: Consider the legality and outcome of the Input:This is the second bottle, I doubt if it's a fake because the packaging has changed, the new packaging
sterilization procedure conducted in 1992 to determine if there is a basis for a claim. 2. Medical malpractice and feels low quality, greasy after use, the first bottle was refreshing, so I repurchased, but the second bottle is
disputes: If there were issues with the sterilization, it might involve medical malpractice and disputes, requiring completely different, causes acne after use, greasy! Regardless of authenticity, I won't repurchase
consideration of legal avenues and evidence. 3. Inheritance rights and property division: The child has rights to Output:Fake, low-quality packaging, greasy, causes acne, will not repurchase
inheritance, which necessitates consideration of how to approach rights assertion and issues of property division.
Instruction: Please rate different aspects of the hotel based on the following reviews and summarize the users'
Instruction: Identify key information within the consultation, assess the possible psychological state of the intended meanings.
counselee, and then provide targeted support and advice. Input: A peaceful place amidst the hustle, you can see nice scenery from the window. The hotel's price is indeed a bit
Input:How can I relieve my nervousness? I get nervous whenever I go on stage, and even when I am called to answer high
questions in class, I start shaking all over and my voice trembles. I don't know how to relieve this. Output: Peacefulness: Score: 85/100, the review mentions the hotel as a peaceful haven amidst the hustle,
O s ap s eu da et k ep i pnu g bt r: i enR a p ti hus ik b n l gl ie c , v , m e wl i: nh M dic fh uo ld m ne ear sa y st e i mn. d\ en i dcT ia th atee ti osin o nd c , i i ov a rli d a pu n ra oxl gi ed rt ee y ss . sc iIr vti b eis e m s r e us ci sg o cn m leif m ri ec e la n an xdt ae n td ie otr o nv .o p Au rs a dn c de ti is c ts ie o a nrn e ad l la l yp x ,h a sty i es o ei nc k a it nl e c gre h ha n ec i lqt pi uo e fn rss o s mw uh c ahe n i t on h fd e t i hwc ea i t n si tdn aog yw ;a v ise pry ra p iso es dit ,i v she o a wtt ir nib gu tt he a t th ta ht e c han o ta et lt ’r sa lc ot cg au tie os nts a s ne de k di en sg ig t nra on fq fu ei rl i bt ey a; uS tc ife un le vry ie: w S sc ,o er ne: h a8 n0 c/1 in0 g0 , t hth ee a v tti re aw ct if vro enm e ss
psychologist is advised to explore the root causes of anxiety and learn more effective coping strategies. High price: Score: 60/100, the review notes that the hotel is pricey, which could be a barrier for budget-sensitive
guests, despite potentially high-quality services or facilities offered, price remains a crucial factor for guests.
Instruction: Please extract three to six key medical information from the user's description.
Instruction: Convert lengthy or complex consultation texts in the finance field into concise, structured questions.
Input:I've also been caught up in the flu wave these past few days. Now I've been coughing for almost three months
Input:How long does it take for the transfer to arrive, and can I track the progress? without getting better. I want to ask, what should I do if I've been coughing for nearly three months?
Output: How long does it usually take for the transferred funds to be credited to the account? Is there a Output:Symptom Description: Long-term cough (lasting for three months)
service to track the progress of the transfer? Duration of Symptoms: Three months
Possible Cause: Initially may have been caused by a cold infection
Help Sought: Seeking a solution for a persistent cough
Figure3:Taskexamples
63.5 Analysis
ThisanalysisconfirmsthatwhileAnyTaskTunecandramaticallyimproveperformanceonspecific
tasks,itscross-domainapplicabilityislimited.Thishighlightstheimportanceoftask-specificdatasets
infine-tuningprocesses,ensuringthatmodelsarenotonlyhighlyeffectivebutalsotailoredtothe
particularcharacteristicsandrequirementsoftheirintendedoperationaldomains.
4 Conclusion
In this work, we introduced AnyTaskTune, a novel fine-tuning paradigm specifically designed
forExplicitDataSets. Ourapproachaddressesthelimitationsoftraditionalgeneralmodelsand
domain-specificmodelsbyemphasizingprecisionandspecificity,whicharecriticalforreal-world
applications.Byleveragingourextensiveexperienceinvariousverticaldomains,wedevelopedand
utilizedovertwentybilingualExplicitDataSets,enablingustofine-tunemodelsforspecifictasks
withhighaccuracyandefficiency.
OurexperimentsdemonstratedthatAnyTaskTunesignificantlyoutperformsbothclosed-sourceand
open-sourceLLMs,aswellasexistingdomain-specificmodels,acrossarangeoftasksinfinance,health-
care,law,psychology,androle-play.TheresultsshowcasedthesuperiorperformanceofAnyTaskTune
inhandlingcomplexandnuancedtasks,therebyvalidatingtheeffectivenessoftask-specificfine-tuning.
Moreover,wehighlightedthepracticalimplicationsofAnyTaskTuneforbusinessesandorganizations.
Bydefiningclearanddirectiveinput-outputpairsthroughExplicitDataSets,weenableprecisemodel
tailoring,whichnotonlyenhancestaskperformancebutalsoimprovesthemodel’sapplicabilityand
efficiencyinspecificoperationalcontexts.Ourmethodologyensuresthatmodelsarenotjustbroadly
capablebutarefinelytunedtomeettheexactneedsofvariousbusinessscenarios.
We also introduced DataTager, our foundational data synthesis model, which forms the basis
for generating high-quality datasets used in AnyTaskTune. While we have not open-sourced
DataTager-LLM,wehaveprovidedcomprehensivedocumentationandresourcesinouraccompanying
GitHubrepository,detailingthesub-tasksanddatasetsusedinourexperiments. Thistransparency
aimstofosterfurtherresearchanddevelopmentinthecommunity.
Inconclusion,AnyTaskTunerepresentsasignificantadvancementinthedevelopmentanddeployment
ofLargeLanguageModels.Byshiftingthefocusfromgeneralcapabilitiestospecialized,task-oriented
modeltraining,AnyTaskTuneoffersarobustandefficientsolutionforindustry-specificapplications.
Thisstrategicevolutioninmodelfine-tuningnotonlyenhancesperformancebutalsoensuresthat
LLMsaremorerelevantandvaluableinreal-worldbusinesscontexts.
WebelievethatAnyTaskTunewillpavethewayformorepreciseandeffectiveapplicationsofAIin
variousdomains,ultimatelycontributingtothebroaderadoptionandintegrationofAItechnologies
ineverydaybusinessoperations.
7Authorship,CreditAttribution,andAcknowledgements
Pleasecitethisworkas“DataTager(2024)”.
AuthorshipandContributions
PaperWriting
• JiaxiCui-Founder,Providedthemainideas,dataconstruction,modeltraining,paperwriting.
• WentaoZhang-Organizedideasandoutline,andcontributedtopaperwriting.
Engineering
• XudongTong-SoftwareengineeringatDataTager.
• ZhenweiZhang-Tencent-SoftwareengineeringatDataTager.
OtherContributions
• JingTang-HuazhongUniversityofScienceandTechnology-Responsibleforexternal
presentationsandresourceacquisition.
• Amie-PublicAI&BeihangUniversity-Web3andexternalresourceacquisition
• JingWen-Providedindustryinsights.
• RongshengWang-Qiyuan.Tech-Providedassistanceinthemedicalfield.
• PengfeiWu-PekingUniversity&Tencent-Offeredpartialassistance.
Acknowledgements
Wewouldliketoexpressoursinceregratitudetothefollowingindividualsandorganizationsfortheir
invaluablesupportandcontributionstothisproject:
• Modelscope’sChenChengandHuggingFace’sTiezhenWangfortheirtremendoussupport.
• PublicAI,ourWeb3partner,forprovidingdatasupport.
• Sci-Learning,ourchannelpartner,forreachingstudentgroups.
• Yuan-Groupforsupportingthefreeexplorationofearlyideas.
• FudanNLPteamfortheirrelatedresearchcontributions.
• Theopen-sourcecommunityandcontributorstothevarioussoftwarelibrariesusedinthis
project.Yourdedicationandhardworkaredeeplyappreciated.
SpecialThanks
Aspecialthankstotheopen-sourcecommunityandcontributorstothevarioussoftwarelibrariesused
inthisproject.Yourdedicationandhardworkaredeeplyappreciated.
ContactInformation
Forfurtherinformationorinquiries,pleasecontactusatreport@datatager.com.
Thankyouforyourinterestandsupportinourwork.
TheDataTagerTeam
July10,2024
8References
[1] J.Bai,S.Bai,Y.Chu,Z.Cui,K.Dang,X.Deng,Y.Fan,W.Ge,Y.Han,F.Huang,B.Hui,L.Ji,
M.Li,J.Lin,R.Lin,D.Liu,G.Liu,C.Lu,K.Lu,J.Ma,R.Men,X.Ren,X.Ren,C.Tan,S.Tan,
J.Tu,P.Wang,S.Wang,W.Wang,S.Wu,B.Xu,J.Xu,A.Yang,H.Yang,J.Yang,S.Yang,
Y.Yao,B.Yu,H.Yuan,Z.Yuan,J.Zhang,X.Zhang,Y.Zhang,Z.Zhang,C.Zhou,J.Zhou,
X.Zhou,andT.Zhu. Qwentechnicalreport,2023.
[2] Z.Bao,W.Chen,S.Xiao,K.Ren,J.Wu,C.Zhong,J.Peng,X.Huang,andZ.Wei. Disc-medllm:
Bridginggenerallargelanguagemodelsandreal-worldmedicalconsultation,2023.
[3] Z.Cai, M.Cao, H.Chen, K.Chen, K.Chen, X.Chen, X.Chen, Z.Chen, Z.Chen, P.Chu,
X.Dong,H.Duan,Q.Fan,Z.Fei,Y.Gao,J.Ge,C.Gu,Y.Gu,T.Gui,A.Guo,Q.Guo,C.He,
Y.Hu,T.Huang,T.Jiang,P.Jiao,Z.Jin,Z.Lei,J.Li,J.Li,L.Li,S.Li,W.Li,Y.Li,H.Liu,J.Liu,
J.Hong,K.Liu,K.Liu,X.Liu,C.Lv,H.Lv,K.Lv,L.Ma,R.Ma,Z.Ma,W.Ning,L.Ouyang,
J.Qiu,Y.Qu,F.Shang,Y.Shao,D.Song,Z.Song,Z.Sui,P.Sun,Y.Sun,H.Tang,B.Wang,
G.Wang,J.Wang,J.Wang,R.Wang,Y.Wang,Z.Wang,X.Wei,Q.Weng,F.Wu,Y.Xiong,
C.Xu,R.Xu,H.Yan,Y.Yan,X.Yang,H.Ye,H.Ying,J.Yu,J.Yu,Y.Zang,C.Zhang,L.Zhang,
P.Zhang,P.Zhang,R.Zhang,S.Zhang,S.Zhang,W.Zhang,W.Zhang,X.Zhang,X.Zhang,
H.Zhao,Q.Zhao,X.Zhao,F.Zhou,Z.Zhou,J.Zhuo,Y.Zou,X.Qiu,Y.Qiao,andD.Lin.
Internlm2technicalreport,2024.
[4] W.Chen,Q.Wang,Z.Long,X.Zhang,Z.Lu,B.Li,S.Wang,J.Xu,X.Bai,X.Huang,etal.
Disc-finllm: Achinesefinanciallargelanguagemodelbasedonmultipleexpertsfine-tuning.
arXivpreprintarXiv:2310.15205,2023.
[5] Y. Chen, X. Xing, J. Lin, H. Zheng, Z. Wang, Q. Liu, and X. Xu. Soulchat: Improving
llms’empathy, listening, andcomfortabilitiesthroughfine-tuningwithmulti-turnempathy
conversations,2023.
[6] J.Cui, L.Lv, J.Wen, R.Wang, J.Tang, Y.Tian, andL.Yuan. Machinemindset: Anmbti
explorationoflargelanguagemodels,2024.
[7] J.Cui, M.Ning, Z.Li, B.Chen, Y.Yan, H.Li, B.Ling, Y.Tian, andL.Yuan. Chatlaw: A
multi-agentcollaborativelegalassistantwithknowledgegraphenhancedmixture-of-experts
largelanguagemodel,2024.
[8] T.GLM,A.Zeng,B.Xu,B.Wang,C.Zhang,D.Yin,D.Rojas,G.Feng,H.Zhao,H.Lai,H.Yu,
H.Wang,J.Sun,J.Zhang,J.Cheng,J.Gui,J.Tang,J.Zhang,J.Li,L.Zhao,L.Wu,L.Zhong,
M.Liu,M.Huang,P.Zhang,Q.Zheng,R.Lu,S.Duan,S.Zhang,S.Cao,S.Yang,W.L.Tam,
W.Zhao,X.Liu,X.Xia,X.Zhang,X.Gu,X.Lv,X.Liu,X.Liu,X.Yang,X.Song,X.Zhang,
Y.An,Y.Xu,Y.Niu,Y.Yang,Y.Li,Y.Bai,Y.Dong,Z.Qi,Z.Wang,Z.Yang,Z.Du,Z.Hou,and
Z.Wang. Chatglm:Afamilyoflargelanguagemodelsfromglm-130btoglm-4alltools,2024.
[9] X.Jiang,R.Zhang,Y.Xu,R.Qiu,Y.Fang,Z.Wang,J.Tang,H.Ding,X.Chu,J.Zhao,and
Y.Wang. Hykge:Ahypothesisknowledgegraphenhancedframeworkforaccurateandreliable
medicalllmsresponses,2024.
[10] J.Luo,S.Yang,X.Qiu,P.Chen,Y.Nai,W.Zeng,W.Zhang,andX.Jiang. Kuaiji: thefirst
chineseaccountinglargelanguagemodel,2024.
[11] OpenAI,J.Achiam,S.Adler,S.Agarwal,L.Ahmad,I.Akkaya,F.L.Aleman,D.Almeida,
J.Altenschmidt,S.Altman,S.Anadkat,R.Avila,I.Babuschkin,S.Balaji,V.Balcom,P.Baltescu,
H. Bao, M. Bavarian, J. Belgum, I. Bello, J. Berdine, G. Bernadett-Shapiro, C. Berner,
L.Bogdonoff,O.Boiko,M.Boyd,A.-L.Brakman,G.Brockman,T.Brooks,M.Brundage,
K. Button, T. Cai, R. Campbell, A. Cann, B. Carey, C. Carlson, R. Carmichael, B. Chan,
C.Chang,F.Chantzis,D.Chen,S.Chen,R.Chen,J.Chen,M.Chen,B.Chess,C.Cho,C.Chu,
H.W.Chung,D.Cummings,J.Currier,Y.Dai,C.Decareaux,T.Degry,N.Deutsch,D.Deville,
A.Dhar,D.Dohan,S.Dowling,S.Dunning,A.Ecoffet,A.Eleti,T.Eloundou,D.Farhi,L.Fedus,
N.Felix,S.P.Fishman,J.Forte,I.Fulford,L.Gao,E.Georges,C.Gibson,V.Goel,T.Gogineni,
G.Goh, R.Gontijo-Lopes, J.Gordon, M.Grafstein, S.Gray, R.Greene, J.Gross, S.S.Gu,
Y. Guo, C. Hallacy, J. Han, J. Harris, Y. He, M. Heaton, J. Heidecke, C. Hesse, A. Hickey,
9W.Hickey,P.Hoeschele,B.Houghton,K.Hsu,S.Hu,X.Hu,J.Huizinga,S.Jain,S.Jain,J.Jang,
A.Jiang,R.Jiang,H.Jin,D.Jin,S.Jomoto,B.Jonn,H.Jun,T.Kaftan,ŁukaszKaiser,A.Kamali,
I.Kanitscheider,N.S.Keskar,T.Khan,L.Kilpatrick,J.W.Kim,C.Kim,Y.Kim,J.H.Kirchner,
J.Kiros,M.Knight,D.Kokotajlo,ŁukaszKondraciuk,A.Kondrich,A.Konstantinidis,K.Kosic,
G.Krueger,V.Kuo,M.Lampe,I.Lan,T.Lee,J.Leike,J.Leung,D.Levy,C.M.Li,R.Lim,
M.Lin,S.Lin,M.Litwin,T.Lopez,R.Lowe,P.Lue,A.Makanju,K.Malfacini,S.Manning,
T. Markov, Y. Markovski, B. Martin, K. Mayer, A. Mayne, B. McGrew, S. M. McKinney,
C.McLeavey,P.McMillan,J.McNeil,D.Medina,A.Mehta,J.Menick,L.Metz,A.Mishchenko,
P.Mishkin,V.Monaco,E.Morikawa,D.Mossing,T.Mu,M.Murati,O.Murk,D.Mély,A.Nair,
R.Nakano,R.Nayak,A.Neelakantan,R.Ngo,H.Noh,L.Ouyang,C.O’Keefe,J.Pachocki,
A.Paino,J.Palermo,A.Pantuliano,G.Parascandolo,J.Parish,E.Parparita,A.Passos,M.Pavlov,
A.Peng,A.Perelman,F.deAvilaBelbutePeres,M.Petrov,H.P.deOliveiraPinto,Michael,
Pokorny,M.Pokrass,V.H.Pong,T.Powell,A.Power,B.Power,E.Proehl,R.Puri,A.Radford,
J.Rae,A.Ramesh,C.Raymond,F.Real,K.Rimbach,C.Ross,B.Rotsted,H.Roussez,N.Ryder,
M.Saltarelli,T.Sanders,S.Santurkar,G.Sastry,H.Schmidt,D.Schnurr,J.Schulman,D.Selsam,
K.Sheppard,T.Sherbakov,J.Shieh,S.Shoker,P.Shyam,S.Sidor,E.Sigler,M.Simens,J.Sitkin,
K.Slama,I.Sohl,B.Sokolowsky,Y.Song,N.Staudacher,F.P.Such,N.Summers,I.Sutskever,
J.Tang,N.Tezak,M.B.Thompson,P.Tillet,A.Tootoonchian,E.Tseng,P.Tuggle,N.Turley,
J. Tworek, J. F. C. Uribe, A. Vallone, A. Vijayvergiya, C. Voss, C. Wainwright, J. J. Wang,
A.Wang,B.Wang,J.Ward,J.Wei,C.Weinmann,A.Welihinda,P.Welinder,J.Weng,L.Weng,
M.Wiethoff,D.Willner,C.Winter,S.Wolrich,H.Wong,L.Workman,S.Wu,J.Wu,M.Wu,
K.Xiao,T.Xu,S.Yoo,K.Yu,Q.Yuan,W.Zaremba,R.Zellers,C.Zhang,M.Zhang,S.Zhao,
T.Zheng,J.Zhuang,W.Zhuk,andB.Zoph. Gpt-4technicalreport,2024.
[12] R.Wang,Y.Duan,C.Lam,J.Chen,J.Xu,H.Chen,X.Liu,P.C.-I.Pang,andT.Tan. Ivygpt:
Interactive chinese pathway language model in medical domain. In CAAI International
ConferenceonArtificialIntelligence,pages378–382.Springer,2023.
[13] Q.Xie,W.Han,X.Zhang,Y.Lai,M.Peng,A.Lopez-Lira,andJ.Huang.Pixiu:Alargelanguage
model,instructiondataandevaluationbenchmarkforfinance. arXivpreprintarXiv:2306.05443,
2023.
[14] D. X. Xin Yan. Mindchat: Psychological large language model. https://github.com/
X-D-Lab/MindChat,2023.
[15] A.Yang,B.Xiao,B.Wang,B.Zhang,C.Bian,C.Yin,C.Lv,D.Pan,D.Wang,D.Yan,F.Yang,
F.Deng,F.Wang,F.Liu,G.Ai,G.Dong,H.Zhao,H.Xu,H.Sun,H.Zhang,H.Liu,J.Ji,J.Xie,
J.Dai,K.Fang,L.Su,L.Song,L.Liu,L.Ru,L.Ma,M.Wang,M.Liu,M.Lin,N.Nie,P.Guo,
R.Sun,T.Zhang,T.Li,T.Li,W.Cheng,W.Chen,X.Zeng,X.Wang,X.Chen,X.Men,X.Yu,
X.Pan,Y.Shen,Y.Wang,Y.Li,Y.Jiang,Y.Gao,Y.Zhang,Z.Zhou,andZ.Wu. Baichuan2:
Openlarge-scalelanguagemodels,2023.
[16] H.Yang,X.-Y.Liu,andC.D.Wang. Fingpt:Open-sourcefinanciallargelanguagemodels,2023.
[17] S.Yang,X.Jiang,H.Zhao,W.Zeng,H.Liu,andY.Jia.Faima:Feature-awarein-contextlearning
formulti-domainaspect-basedsentimentanalysis,2024.
[18] S.Yue,W.Chen,S.Wang,B.Li,C.Shen,S.Liu,Y.Zhou,Y.Xiao,S.Yun,X.Huang,andZ.Wei.
Disc-lawllm:Fine-tuninglargelanguagemodelsforintelligentlegalservices,2023.
[19] S. Yue, W. Chen, S. Wang, B. Li, C. Shen, S. Liu, Y. Zhou, Y. Xiao, S. Yun, W. Lin, et al.
Disc-lawllm: Fine-tuninglargelanguagemodelsforintelligentlegalservices. arXivpreprint
arXiv:2309.11325,2023.
[20] H.Zhang,J.Chen,F.Jiang,F.Yu,Z.Chen,J.Li,G.Chen,X.Wu,Z.Zhang,Q.Xiao,etal. Hu-
atuogpt,towardstaminglanguagemodeltobeadoctor. arXivpreprintarXiv:2305.15075,2023.
[21] W.X.Zhao,K.Zhou,J.Li,T.Tang,X.Wang,Y.Hou,Y.Min,B.Zhang,J.Zhang,Z.Dong,
etal. Asurveyoflargelanguagemodels. arXivpreprintarXiv:2303.18223,2023.
[22] Z.Zhou,J.-X.Shi,P.-X.Song,X.-W.Yang,Y.-X.Jin,L.-Z.Guo,andY.-F.Li.Lawgpt:Achinese
legalknowledge-enhancedlargelanguagemodel. arXivpreprintarXiv:2406.04614,2024.
10[23] W. Zhu and X. Wang. Chatmed: A chinese medical large language model.
https://github.com/michael-wzhu/ChatMed,2023.
11