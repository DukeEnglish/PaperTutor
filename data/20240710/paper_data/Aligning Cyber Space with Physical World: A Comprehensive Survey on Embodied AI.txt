IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 1
Aligning Cyber Space with Physical World: A
Comprehensive Survey on Embodied AI
Yang Liu, Member, IEEE, Weixing Chen, Yongjie Bai, Jingzhou Luo, Xinshuai Song, Kaixuan Jiang, Zhida Li,
Ganlong Zhao, Junyi Lin, Guanbin Li, Member, IEEE, Wen Gao, Fellow, IEEE, Liang Lin, Fellow, IEEE
Abstract—Embodied Artificial Intelligence (Embodied AI) is 16000
crucial for achieving Artificial General Intelligence (AGI) and sre14000
serves as a foundation for various applications that bridge p
cyberspace and the physical world. Recently, the emergence of a P d12000
Multi-modal Large Models (MLMs) and World Models (WMs) e h10000
have attracted significant attention due to their remarkable
silb
8000
u
perception, interaction, and reasoning capabilities, making them P fo 6000
a promising architecture for the brain of embodied agents. re
b 4000 However, there is no comprehensive survey for Embodied AI m
in the era of MLMs. In this survey, we give a comprehensive u N 2000
exploration of the latest advancements in Embodied AI. Our 0
analysis firstly navigates through the forefront of representative 2014 2015 2016 2017 2018 2019 2020 2021 2022 2023
works of embodied robots and simulators, to fully understand Multimodal Embodied Robot Multimodal Embodied AI
theresearchfocusesandtheirlimitations.Then,weanalyzefour Multimodal Embodied Perception Multimodal Embodied Manipulation
Multimodal Embodied Planning Multimodal Embodied Agent
main research targets: 1) embodied perception, 2) embodied
interaction, 3) embodied agent, and 4) sim-to-real adaptation, Fig.1. GoogleScholarsearchresultsforrelatedtopicsofEmbodiedAI.The
covering the state-of-the-art methods, essential paradigms, and verticalandhorizontalaxesdenotethenumberofrelatedpublicationsandthe
comprehensivedatasets.Additionally,weexplorethecomplexities year,respectively.Thepublicationsgrowexponentiallysincethebreakthrough
ofMLMsinvirtualandrealembodiedagents,highlightingtheir ofMLMsin2023.
significance in facilitating interactions in dynamic digital and
physicalenvironments.Finally,wesummarizethechallengesand to as disembodied AI, while those in the physical space are
limitations of embodied AI and discuss their potential future
embodiedAI(TableI).RecentadvancesinMulti-modalLarge
directions. We hope this survey will serve as a foundational
Models (MLMs) have injected strong perception, interaction
reference for the research community and inspire continued
innovation.Theassociatedprojectcanbefoundathttps://github. and planning capabilities to embodied models, to develop
com/HCPLab-SYSU/Embodied AI Paper List. general-purpose embodied agents and robots that actively
interactwithvirtualandphysicalenvironments[2].Therefore,
Index Terms—Embodied AI, Cyber Space, Physical World,
Multi-modal Large Models, World Models, Agents, Robotics theembodiedagentsarewidelyconsideredasthebestcarriers
for MLMs. The recent representative embodied models are
RT-2 [3] and RT-H [4]. Nevertheless, the capabilities of
I. INTRODUCTION
long-termmemory,understandingcomplexintentions,andthe
EMBODIED AI was initially proposed from the Embod-
decompositionofcomplextasksarelimitedforcurrentMLMs.
ied Turing Test by Alan Turing in 1950 [1], which is
To achieve Artificial General Intelligence (AGI), the de-
designed to determine whether agents can display intelligence
velopment of embodied AI stands as a fundamental avenue.
that is not just limited to solving abstract problems in a
Different from conversational agents like ChatGPT [5], em-
virtual environment (cyber space1), but that is also capable of
bodied AI believes that the true AGI can be achieved by
navigating the complexity and unpredictability of the physical
controlling physical embodiments and interacting with both
world. The agents in the cyber space are generally referred
simulated and physical environments [6]–[9]. As we stand
This work is supported in part by the National Key R&D Program of at the forefront of AGI-driven innovation, it is crucial to
China under Grant No.2021ZD0111601, in part by the National Natural delve deeper into the realm of embodied AI, unraveling their
Science Foundation of China under Grant No.62002395, in part by the
complexities, evaluating their current developmental stage,
Guangdong Basic and Applied Basic Research Foundation under Grant
No.2023A1515011530.(Correspondingauthor:LiangLin.) and contemplating the potential trajectories they may follow
Yang Liu, Weixing Chen, Yongjie Bai, Jingzhou Luo, Xinshuai Song, in the future. Nowadays, embodied AI contains various key
Kaixuan Jiang, Zhida Li, Junyi Lin, Guanbin Li and Liang Lin are with
techniques across Computer Vision (CV), Natural Language
the School of Computer Science and Engineering, Sun Yat-sen University,
Guangzhou,China. Processing (NLP), and robotics, with the most representative
Ganlong Zhao is with The University of Hong Kong, China. (E-mail: being embodied perception, embodied interaction, embodied
zhaogl@connect.hku.hk)
agents, and sim-to-real robotic control. Therefore, it is imper-
WenGaoiswiththeInstituteofDigitalMedia,PekingUniversity,Beijing,
China,andalsowiththePengChengLaboratory,Shenzhen,China(E-mail: ative to capture the evolving landscape of embodied AI in the
wgao@pku.edu.cn) pursuit of AGI through a comprehensive survey.
1TheagentsarethefoundationofbothdisembodiedandembodiedAI.The
Embodied agent is the most prominent basis of embodied
agents can exist in both cyber and physical spaces, integrated with various
entities.Theentitiesincludenotonlyrobotsbutalsootherdevices. AI. For an embodied task, the embodied agent must fully un-
4202
luJ
9
]VC.sc[
1v68860.7042:viXraIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 2
Type Environment PhysicalEntities Description RepresentativeAgents
DisembodiedAI CyberSpace No Cognitionandphysicalentitiesaredisentangled ChatGPT[10],RoboGPT[11]
EmbodiedAI PhysicalSpace Robots,Cars,Otherdevices Cognitionisintegratedintophysicalentities RT-1[12],RT-2[3],RT-H[4]
TABLEI
COMPARISONBETWEENDISEMBODIEDAIANDEMBODIEDAI.
Brain: Embodied World Model
Human Value
Virtual Environment (Cyber Space)
Configurator Human
Value
Execute Memory World Domain
Model Knowledge
Feedback Perception Actor F Ea qc uil ii pt my a en nd t
Cost
Embodiments Sim2Real Intrinsic Critic Objects
Cost
Align Physical Law
Interaction
Multi-modal Human-Robot Task
Active Perception Interaction Planning
Multi-modal Large Models
Robots Physical Environments Multi-modal Elements
Fig. 2. The overall framework of the embodied agent based on MLMs and WMs. The embodied agent has a embodied world model as its “brain”. It has
thecapabilitytounderstandthevirtual-physicalenvironmentandactivelyperceivemulti-modalelements.Itcanfullyunderstandhumanintention,alignwith
humanvalue,decomposecomplextasks,andexecuteaccurateactions,aswellasinteractwithhumansandutilizeknowledgebasesandtools.
derstandthehumanintentioninlanguageinstructions,actively survey from different perspectives including embodied robots,
explore the surrounding environments, comprehensively per- simulators, four representative embodied tasks (visual active
ceivethemulti-modalelementsfrombothvirtualandphysical perception,embodiedinteraction,multi-modalagentsandsim-
environments, and execute appropriate actions for complex to-real robotic controlling), and future research directions. We
tasks [13], [14], as shown in Fig. 2. The rapid progress in believe that this survey will provide a clear big picture of
multi-modalmodelsexhibitssuperiorversatility,dexterity,and what we have achieved, and we could further achieve along
generalizability in complex environments compared to tra- this emerging yet very prospective research direction.
ditional deep reinforcement learning approaches. Pre-trained Differences from previous works: Although there have
visual representations from state-of-the-art vision encoders been several survey papers [7], [19]–[21] for embodied AI,
[15], [16] provide precise estimations of object class, pose, most of them are outdated as they were published before
and geometry, which makes the embodied models thoroughly the era of MLMs, which started around 2023. To the best
perceive complex and dynamic environments. Powerful Large of our knowledge, there is only one survey paper [9] after
Language Models (LLMs) make robots better understand the 2023,whichonlyfocusedonvision-language-actionembodied
linguistic instructions from humans. Numerous multi-modal AI models. However, the MLMs, WMs and embodied agents
fusion methods from MLMs give feasible approach for align- are not fully considered. Additionally, recent developments
ing the visual and linguistic representations from embodied in embodied robots and simulators are also overlooked. To
robots. The world models [17], [18] exhibit remarkable sim- address the scarcity of comprehensive survey papers in this
ulation capabilities and promising comprehension of phys- rapidly developing field, we propose this comprehensive sur-
ical laws, which makes embodied models comprehensively vey that covers representative embodied robots, simulators,
understand both the physical and real environments. These and four main research tasks: embodied perception, embodied
innovations empower embodied agents to comprehensively interaction, embodied agents, and sim-to-real robotic control.
perceivecomplexenvironment,interactwithhumansnaturally, In summary, the main contributions of this work are three-
and execute tasks reliably. fold. First, it presents a systematic review of embodied AI
The advancement of embodied AI has exhibited rapid includingembodiedrobots,simulators,andfourmainresearch
progress, capturing significant attention within the research tasks:visualactiveperception,embodiedinteraction,embodied
community (Fig. 1), and it is recognized as the most feasible agents and sim-to-real robotic control. To the best of our
path for achieving AGI. Google Scholar reports a substantial knowledge,thisisthefirstcomprehensivesurveyofembodied
volume of embodied AI publications, with approximately AIfromtheperspectiveofthealignmentofcyberandphysical
10,700 papers published in 2023 alone. This accounts for an spaces based on MLMs and WMs, offering a broad overview
average of 29 papers per day or more than one paper per with a thorough summary and categorization of existing
hour. Despite the intensive interest in harvesting the powerful studies. Second, it examines the latest progress in embodied
perception and reasoning ability from MLMs, the research AI, providing comprehensive benchmarking and discussion of
community is short of a comprehensive survey that can help current work across multiple simulators and datasets. Third, it
sortoutexistingembodiedAIstudies,thefacingchallenges,as identifies several research challenges and potential directions
wellasfutureresearchdirections.IntheeraofMLMs,weaim for future research in AGI for embodied AI.
tofillupthisgapbyperformingasystematicsurveyofembod- The rest of this survey is organized as follows. Section 2
ied AI across cyber space to physical world. We conduct the introduces various embodied robots. Section 3 describes gen-IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 3
Embodied AI: Aligning Cyber Space with Physical World
Embodied Robots Simulators Embodied Perception Embodied Interaction Embodied Agent
Fix-base Robots,
Active Visual Exploration,
Wheeled &Tracked Robots, General Simulator, Embodied Question Embodied Multi-modal
3D Visual Grounding,
Quadruped Robots, Real-Scene based Answering, Foundation Model,
Visual-Language Navigation,
Humanoid Robots, Simulator, Embodied Grasping, Embodied Task Planning,
Non-Visual Perception,
Biomimetic Robots, … … …
…
…
Sim-to-Real Adaptation
Embodied World Model, Data Collection and Training, Embodied Control
Applications Robotics Autonomous Driving Healthcare Domestic Assistance Industrial Automation Search and Rescue
Fig.3. ThissurveyfocusesoncomprehensiveanalysisofthelatestadvancementsinEmbodiedAI.
achievemicron-levelprecision,makingthemsuitablefortasks
(a) Fixed-base Robots (b) Wheeled Robots (c) Tracked Robots
(Franka Emika Panda) (Jackal robot) (iRobot PackBot) that require high accuracy and repeatability [22]. Moreover,
fixed-base robots are highly programmable, allowing users to
adapt them for various task scenarios, such as Franka (Franka
Emika panda) [23], Kuka iiwa (KUKA) [24], and Sawyer
(d) Quadruped Robots (e) Humanoid Robots (Rethink Robotics) [25]. Despite their excellent performance
(Boston Dynamics Spot) (Tesla Optimus) in many fields, fixed-base robots have certain disadvantages.
Firstly, their fixed-base design limits their operational range
and flexibility, preventing them from moving or adjusting
positions over large areas and leading to their collaboration
(f) Biomimetic Robots
with humans and other robots. Secondly, fixed-base robots
are generally expensive and require specialized personnel
for installation and maintenance, which increases their initial
investment and operational costs [22], [26].
Fig.4. TheEmbodiedRobotsincludeFixed-baseRobots,QuadrupedRobots,
HumanoidRobots,WheeledRobots,TrackedRobots,andBiomimeticRobots.
B. Wheeled Robots and Tracked Robots
eral and real-scene embodied simulators. Section 4 introduces
For mobile robots, they can face more complex and diverse
embodied perception, including active visual perception, 3D
application scenarios. Wheeled robots, as shown in Fig. 4 (b),
visual grounding, visual language navigation and non-visual
known for their efficient mobility, are widely employed in lo-
perception.Section5introducesembodiedinteraction.Section
gistics,warehousing,andsecurityinspections.Theadvantages
6 introduces embodied agents including the embodied multi-
ofwheeledrobotsincludetheirsimplestructure,relativelylow
modal foundation model and embodied task planning. Section
cost, high energy efficiency, and rapid movement capabilities
7 introduces sim-to-real adaptation including embodied world
onflatsurfaces[22].Theserobotsaretypicallyequippedwith
model,datacollectionandembodiedcontrol.Finally,weshare
high-precision sensors such as LiDAR and cameras, enabling
several promising research directions in Section 8.
autonomousnavigationandenvironmentalperception,making
them highly effective in automated warehouse management
II. EMBODIEDROBOTS and inspection tasks, e.g., Kiva robots (Kiva Systems) [27]
andJackalrobot(ClearpathRobotics)[28].However,wheeled
Embodied AI actively interacts with the physical environ-
robotshavelimitedmobilityincomplexterrainsandharshen-
ment and encompasses a broad spectrum of embodiments,
vironments, particularly on uneven ground. Additionally, their
includingrobots,smartappliances,smartglasses,autonomous
load capacity and maneuverability are somewhat restricted.
vehicles, etc. Among them, robots stand out as one of the
In comparison, tracked robots, with their powerful off-road
most prominent embodiments. Depending on the application,
capabilities and high maneuverability, show significant poten-
robotsaredesignedinvariousformstoleveragetheirhardware
tial in fields such as agriculture, construction, and disaster
characteristics for specific tasks, as shown in Fig. 4.
recovery, as shown in Fig. 4 (c). The track system provides
a larger ground contact area, distributing the robot’s weight
A. Fixed-base Robots
and reducing the risk of sinking in soft terrain such as mud
Fixed-base robots, as shown in Fig. 4 (a), are extensively and sand. Moreover, tracked robots are typically equipped
employed in laboratory automation, educational training, and with robust power and suspension systems, allowing them
industrial manufacturing due to their compactness and high- to maintain stability and traction on complex terrains [29].
precision operations. These robots feature robust bases and Consequently,reliabletrackedrobotsarealsousedinsensitive
structuresthatensurestabilityandhighaccuracyduringopera- areas such as the military. The iRobot PackBot is a versatile
tion.Equippedwithhigh-precisionsensorsandactuators,they military-tracked robot capable of performing tasks such asIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 4
reconnaissance, explosive ordnance disposal, and search and Among current humanoid robots, Atlas (Boston Dynamics) is
rescue missions [30]. However, due to the high friction of renowned for its exceptional mobility and stability. Atlas can
the track system, tracked robots often suffer from low energy perform complex dynamic actions such as running, jumping,
efficiency.Additionally,theirmovementspeedonflatsurfaces androlling,demonstratingthepotentialofhumanoidrobotsin
is not as fast as wheeled robots, and their flexibility and highly dynamic environments [39]. The HRP series (AIST) is
maneuverability are relatively lower. utilized in various research and industrial applications, with
a design focus on high stability and flexibility, making it
C. Quadruped Robots effective in complex environments, particularly for collabo-
rative tasks with humans [40]. ASIMO (Honda), one of the
Quadruped robots, known for their stability and adaptabil-
most famous humanoid robots, can walk, run, climb stairs,
ity, are well-suited for complex terrain exploration, rescue
and recognize faces and gestures, making it suitable for
missions, and military applications. Inspired by quadrupedal
reception and guide services [41]. Additionally, a small social
animals, these robots can maintain balance and mobility on
robot,Pepper(SoftbankRobotics)canrecognizeemotionsand
uneven surfaces, as shown in Fig. 4 (d). The multi-jointed
engageinnaturallanguagecommunicationandiswidelyused
designallowsthemtomimicbiologicalmovements,achieving
in customer service and educational settings [42].
complex gaits and posture adjustments. High adjustability
Despite their excellent performance in many fields, hu-
enablestherobotstoautomaticallyadapttheirstancetochang-
manoidrobotsfacesignificantchallengesinmaintainingoper-
ing terrain, enhancing maneuverability and stability. Sensing
ationalstabilityandreliabilityincomplexenvironmentsdueto
systems, such as LiDAR and cameras, provide environmental
their sophisticated control systems. These challenges include
awareness, allowing the robots to navigate autonomously and
robust bipedal walking control algorithms and dexterous hand
avoidobstacles[31].Researcherscommonlyuseseveraltypes
grasping algorithms [37]. Furthermore, traditional humanoid
of quadruped robots as research platforms: Unitree Robotics,
robots based on hydraulic systems, with their bulky structures
Boston Dynamics Spot, and ANYmal C. Unitree Robotics’
and high maintenance costs, are gradually being replaced by
Unitree A1 and Go1 are noted for their cost-effectiveness and
motor-driven systems. Recently, Tesla and Unitree Robotics
flexibility. The A1 [32] and Go1 [33] possess strong mobility
have introduced their humanoid robots based on motor sys-
and intelligent obstacle avoidance capabilities, suitable for
tems. With the integration of LLMs, humanoid robots are
various applications. Boston Dynamics’ Spot is renowned for
expected to handle various complex tasks more intelligently,
itssuperiorstabilityandoperationalflexibility,whicharecom-
fillinglaborgapsinmanufacturing,healthcare,andtheservice
monly used in industrial inspections and rescue missions. It
industry, thereby improving efficiency and safety [43].
features powerful load-carrying capacity and adaptability, ca-
pableofperformingcomplextasksinharshenvironments[34]. E. Biomimetic Robots
ANYbotics’ ANYmal C, with its modular design and high
Unlike the previously mentioned robots, biomimetic robots
durability, is widely employed in industrial inspection and
perform tasks in complex and dynamic environments by
maintenance. The ANYmal C is equipped with autonomous
simulating the efficient movements and functions of natu-
navigation and remote operation capabilities, suitable for pro-
ral organisms. By emulating biological entities’ forms and
longed outdoor tasks and even extreme lunar missions [35].
movement mechanisms, these robots demonstrate significant
Like fixed-base robots, quadruped robots face similar draw-
potential in fields such as healthcare, environmental moni-
backs, such as high costs. The complex design and high
toring, and biological research [22]. Typically, they utilize
manufacturing costs of quadruped robots result in substantial
flexible materials and structures to achieve lifelike, agile
initial investments, limiting their use in cost-sensitive areas.
movements. These materials not only enhance the robots’
Additionally,quadrupedrobotshavelimitedbatteryendurance
adaptability and flexibility but also minimize environmental
in complex environments, requiring frequent recharging or
impact. Furthermore, biomimetic robots are often equipped
battery replacement for prolonged operation [36].
withadvancedsensorsandcontrolsystems,enablingreal-time
environmental sensing and rapid response, thereby enhancing
D. Humanoid Robots
their autonomous navigation and task execution capabilities.
Following the discussion on fixed-base and quadruped Importantly, biomimetic designs can significantly improve the
robots, humanoid robots are distinguished by their human- robots’ energy efficiency by mimicking the efficient move-
like form and are increasingly prevalent in sectors such as the mentmechanismsofbiologicalorganisms,makingthemmore
service industry, healthcare, and collaborative environments. economical regarding energy consumption [54], [55]. These
These robots can mimic human movements and behavioral biomimetic robots include fish-like robots [56], [57], insect-
patterns, providing personalized services and support. Their likerobots[58],[59],andsoft-bodiedrobots[60],asshownin
dexterous hand designs enable them to perform intricate and Fig. 4 (f). Despite their impressive performance, biomimetic
complextasks,distinguishingthemfromothertypesofrobots, robots face several challenges. First, their design and man-
as shown in Fig. 4 (e). These hands typically have multi- ufacturing processes are often complex and costly, limiting
ple degrees of freedom and high-precision sensors, allowing large-scale production and widespread application. Second,
them to emulate the grasping and manipulation capabilities due to their use of flexible materials and complex movement
of human hands, which is particularly crucial in fields such mechanisms,thedurabilityandreliabilityofbiomimeticrobots
as medical surgery and precision manufacturing [37], [38]. need improvement in extreme environments.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 5
Simulator Year HFPS HQGR RRL DLS LSPC ROS MSS CP PhysicsEngine MainApplications
IsaacSim[44] 2023⃝ ⃝ ⃝ ⃝ ⃝ ⃝ ⃝ ⃝ PhysX Nav,AD
IsaacGym[45] 2019 ⃝ ⃝ ⃝ PhysX RL,LSPS
Gazebo[46] 2004 ⃝ ⃝ ⃝ ⃝ ⃝ ODE,Bullet,Simbody,DARTNav,MR
PyBullet[47] 2017 ⃝ ⃝ Bullet RL,RS
Webots[48] 1996 ⃝ ⃝ ⃝ ⃝ ODE RS
MuJoCo[49] 2012⃝ ⃝ ⃝ Custom RL,RS
UnityML-Agents[50] 2017 ⃝ ⃝ ⃝ Custom RL,RS
AirSim[51] 2017 ⃝ ⃝ Custom Dronesim,AD,RL
MORSE[52] 2015 ⃝ ⃝ Bullet Nav,MR
CoppeliaSim(V-REP)[53]2013 ⃝ ⃝ ⃝ ⃝ Bullet,ODE,Vortex,Newton MR,RS
TABLEII
GENERALSIMULATOR.HFPS:HIGH-FIDELITYPHYSICALSIMULATION;HQGR:HIGH-QUALITYGRAPHICSRENDERING;RRL:RICHROBOT
LIBRARY;DLS:DEEPLEARNINGSUPPORT;LSPC:LARGE-SCALEPARALLELCOMPUTING;ROS:TIGHTINTEGRATIONWITHROS;MSS:
MULTIPLESENSORSIMULATION;CP:CROSS-PLATFORMNAV:ROBOTNAVIGATIONAD:AUTODRIVING;RL:REINFORCEMENTLEARNINGLSPS:
LARGE-SCALEPARALLELSIMMR:MULTI-ROBOTSYSTEMSRS:ROBOTSIMULATION.⃝INDICATESTHATTHESIMULATOREXCELSINTHIS
ASPECT.
III. EMBODIEDSIMULATORS
DatascarcityhasbeenapersistentchallengeinembodiedAI
research. Nonetheless, collecting real world robot data poses
numerouschallenges.First,realworldrobottrainingproceeds Isaac Sim Gazebo Pybullet V-REP (CoppeliaSim)
slowly due to its real-time nature, which cannot be paral-
lelized.Theassociatedcostsareprohibitivelyhigh,demanding
dedicateddeploymentsites,expertoperationalcontrolfordata MuJoCo Unity ML-Agents AirSim MORSE Webots
collection, and substantial hardware expenses. Moreover, the Fig.5. ExamplesofGeneralSimulators.
most significant challenge lies in reproducibility, stemming
from vast differences in robot hardware configurations, con-
(Universal Scene Description) format is also introduced to
trol methods, and implementation frameworks, impeding data
describerobotsandcomplexscenes.IsaacSimoffersavariety
transferability.Insuchcircumstances,simulatorsofferanovel
of pre-built robotic models and environments, and it supports
solution for collecting and training data for embodied AI.
user-defined models. Its application scenarios include robotic
EmbodiedsimulatorsarevitalforembodiedAIastheyoffer
navigation and control, autonomous driving, industrial au-
cost-effective experimentation, ensuring safety by simulating
tomation, and human-robot interaction. By providing a robust
potentially hazardous scenarios, scalability for testing in di-
and versatile platform, Isaac Sim significantly enhances the
verse environments, rapid prototyping capabilities, accessibil-
efficiency and effectiveness of robotics and AI research.
itytoawiderresearchcommunity,controlledenvironmentsfor
precise studies, data generation for training and evaluation, Gazebo [61] is an open-source simulator developed by
and standardized benchmarks for algorithm comparison. To Open Robotics, is widely used in robotics research and edu-
enable agents to interact with the environment, it is necessary cation. It offers high-fidelity physical simulation and rich fea-
to construct a realistic simulated environment. This requires tures,makingitapreferredtoolforresearchersanddevelopers.
consideration of the physical characteristics of the environ- Gazebo’s key features include high-fidelity physical simula-
ment, the properties of objects, and their interactions. tion, diverse sensor simulation, extensive robot libraries, and
This section will introduce the commonly used simulation tight integration with ROS. Gazebo supports the simulation
platforms in two parts: the general simulator based on under- of various sensors, including cameras, LiDAR, and sonar, and
lying simulation and the simulator based on real scenes. offersnumerouspre-builtrobotmodelsandenvironmentswith
support for custom models. Its application scenarios include
robot navigation and control and multi-robot systems.
A. General Simulator
PyBullet [47] is the Python interface for the Bullet physics
The physical interactions and dynamic changes present
engine, providing an easy-to-use simulation environment. Py-
in real environments are irreplaceable. However, deploying
Bullet’s key features include ease of use, real-time physi-
embodiedmodelsinthephysicalworldoftenincurshighcosts
cal simulation, diverse sensor simulation, and deep learning
andfacesnumerouschallenges.Theultimategoalofembodied
integration. PyBullet supports real-time physical simulation,
AI is to transfer findings from virtual environments to real-
including rigid body dynamics, collision detection, and con-
worldapplications.Researcherscanselectsimulatorsthatbest
straint solving. Its application scenes include robot navigation
suittheirneedstoaidtheirstudies.General-purposesimulators
and control, reinforcement learning, and computer graphics.
provideavirtualenvironmentthatcloselymimicsthephysical
world,allowingforalgorithmdevelopmentandmodeltraining, Table. II presents the key features and primary application
which offers significant cost, time, and safety advantages. scenarios of 10 general-purpose simulators. They each offer
Isaac Sim [44] developed by NVIDIA, is an advanced unique advantages in the field of embodied AI. Researchers
simulation platform tailored for robotics and AI research. can select the most appropriate simulator based on their
The primary features of Isaac Sim include high-fidelity phys- specific research needs, thereby accelerating the development
ical simulation, real-time ray tracing, an extensive library and application of embodied AI technologies. Fig. 5 shows
of robotic models, and deep learning support. Pixar’s USD the visualization effects of the general simulators.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 6
one of the most important benchmarks.
Virtualhome[65]isahomeactivityembodiedAIsimulator
brought by Puig et al. in 2018. What makes Virtualhome
specialmostisitsenvironmentrepresentedbyanenvironment
AI2-THOR Matterport 3D Virtualhome SAPIEN graph. Specifically, the environment graph is a dictionary
composedofnodes(correspondingtoobjects)andedges(cor-
responding to relationships). Nodes include all objects in the
current environment, as well as corresponding ID, status, and
other information. This kind of environment graph provides a
Habitat iGibson TDW
new way for embodied agents to understand the environment.
Fig.6. ExamplesofReal-SceneBasedSimulators. In addition, users can customize and modify the environment
graph to achieve the custom configuration of scene objects.
B. Real-Scene Based Simulators
SimilartoAI2-THOR,Virtualhomealsoprovidesalargenum-
Achievinguniversalembodiedagentsinhouseholdactivities ber of interactive objects, and embodied agents can interact
hasbeenaprimaryfocusinthefieldofembodiedAIresearch. with them and change their status. Virtualhome also provides
Theseembodiedagentsneedtodeeplyunderstandhumandaily 6 humanoid agents that can be deployed at the same time and
life and perform complex embodied tasks such as navigation support the acquisition of RGB-D and semantic information
and interaction in indoor environments. To meet the demands from multiple perspectives. Another feature of Virtualhome
ofthesecomplextasks,thesimulatedenvironmentsneedtobe is its simple and easy-to-use API. The actions of embodied
as close to real world as possible, which places high demands agents are simplified to the format of ‘operation + object’.
on the complexity and realism of the simulators. This led to This feature makes Virtualhome widely used in the research
the creation of simulators based on real world environments. fields of embodied planning, instruction decomposition, etc.
These simulators mostly collect data from the real world, Habitat [66] is an open-source simulator for large-scale
create photorealistic 3D assets, and build scenes using 3D human-robot interaction research launched by Meta in 2019.
gameengineslikeUE5andUnity.Therichandrealisticscenes Habitat includes three parts: Habitat-sim, Habitat-lab, and
make simulators based on real world environments the top Habitat-challenge. Among them, Habitat-sim is a high-
choice for research on embodied AI in household activities. performance3DsimulatorbasedontheBulletphysicsengine,
AI2-THOR [62] is an indoor embodied scene simulator whichisthefoundationofHabitat;Habitat-labisanembodied
based on Unity3D, launched in 2017 and led by the Allen simulationframeworkforreinforcementlearning,encapsulated
InstituteforArtificialIntelligence.Asahigh-fidelitysimulator based on Habitat-sim; Habitat-challenge is a series of bench-
built in the real world, the biggest feature of AI2-THOR is marks based on Habitat. The biggest feature of Habitat is its
its richly interactive scene objects and the physical properties extremely high degree of openness. Researchers can import
assigned to them (such as open/close or even cold/hot). AI2- and create their 3D scenes in Habitat or use the rich open
THORconsistsoftwoparts:iTHORandRoboTHOR.iTHOR resources on the Habitat platform for expansion. This gives
contains 120 rooms categorized as kitchens, bedrooms, bath- Habitat several scenes that other simulation platforms cannot
rooms, and living rooms, with over 2000 unique interactive match. Habitat has a wealth of customizable sensors and
objects,andsupportsmulti-agentsimulation;RoboTHORcon- supports multi-agent simulation. Multiple embodied agents
tains89modularapartmentswith600+objects,theuniqueness from open resources or customizations (such as humans and
of which is that these apartments correspond to real scenes robotdogs)cancooperateinthesimulationscene,movefreely,
in the real world. This means that researchers can remotely andperformsimpleinteractionswiththescene.Basedonthese
deploytheirmodelsintherealenvironment.Sofar,morethan advantages, Habitat is attracting increasing attention.
a hundred works have been published based on AI2-THOR. Different from other simulators that focus more on the
Matterport 3D [63] is proposed in R2R [64] in 2018, is scene, SAPIEN [67] pays more attention to simulating the
more commonly used as a large-scale 2D-3D visual dataset interaction of objects in the scene. Based on the PhysX
rather than an embodied simulator. The Matterport3D dataset physicsengine,SAPIENprovides fine-grainedembodiedcon-
includes 90 architectural indoor scenes, comprises 10,800 trol, which can implement joint control based on force and
panoramas and 194,400 RGB-D images, and provides surface torque through the ROS interface, and the high-level action
reconstruction, camera posture, and 2D and 3D semantic interface can also support the motion planning for embodied
segmentation annotations. In embodied AI, Matterport3D is agents to avoid collisions. Based on the PartNet-Mobility
mainly used for visual language navigation. Matterport3D Dataset, SAPIEN provides indoor simulation scenes contain-
transforms3Dscenesintodiscrete‘viewpoints’,andembodied ing rich interactive objects and supports the import of custom
agents move between adjacent ‘viewpoints’ in Matterport3D resources. Different from the API provided by simulators like
scenes. At each ‘viewpoint’, embodied agents can obtain a AI2-THORthatdirectlychangesthestatusofobjects,SAPIEN
1280x1024 panorama image (18× RGB-D) centered on the supportssimulatedphysicalinteractions,andembodiedagents
‘viewpoint’. The advantage of Matterport3D lies in its large, can control the hinged parts of objects through physical
diverse,anddetailedannotated2D-3Ddata,aswellastheease actions, thereby changing the status of objects. These features
of use and flexibility brought about by simulator-deploy-free. makeSAPIENverysuitablefortrainingthefine-grainedobject
In the field of embodied navigation, Matterport3D is already operation of embodied AI.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 7
Simulation Numof Continuous 3DScene Multiple Object 3D
Year Sensors Physics
Platform Scenes Action Scans Agents States Assets
√ √ √ √ √
AI2-THOR[62] 2017 120 √× RGB-D,S
Matterport3D[63] 2018 90 √× √ RGB-D,S √× √× × √×
Habitat[66] 2019 1000+ RGB-D,S √ √ √× √
VirtualHome[65] 2018 50 √× × RGB-D,S √ √ √
SAPIEN[67] 2020 46 √ √× RGB-D,S √ × √ √
iGibson[68][69] 2021 15 √ √ RGB-D,S,L √ √× √ √
TDW[70] 2021 - RGB-D,S,A
TABLEIII
COMPARISONOFREAL-SCENEBASEDSIMULATORS.FORTHESENSOR,SREFERSTOSEMANTIC,LREFERSTOLIDARANDAREFERSTOAUDIO.
iGibson [68] [69] is an open-source simulator launched which iGibson provides more abundant and realistic large-
by the Li team at Stanford in 2021. iGibson is built on the scale scenes, which makes iGibson suitable for more complex
Bulletphysicsengine,provides15high-qualityindoorscenes, and long-term mobile operation tasks, while TDW provides
and supports the import of assets from other datasets such users with higher freedom, users can freely expand the scene,
as Gibson and Matterport3D, enabling a total of over 10,000 andTDW’suniqueaudioandflexible,fluidsimulationmakeit
simulated scenes. As an object-oriented simulator, iGibson irreplaceableinrelatedsimulationscenarios;Matterport3D,as
assignsrichchangeableattributestoobjects,notlimitedtothe a basic 2D-3D visual dataset, is widely used and extended in
kinematic properties of objects (posture, speed, acceleration, thecurrentembodiedAIbenchmark;inHabitat,theembodied
joint configuration of articulated objects), but also includes agent lacks interaction capabilities, but Habitat’s huge indoor
temperature, humidity, cleanliness, switch status, etc. This scenes and easy-to-use interfaces, open framework, make it
allows iGibson to implement more complex embodied inter- highly regarded in the field of embodied navigation.
actions and build more difficult, long-term embodied tasks. Besides, automated simulation scene construction is greatly
In addition, besides the depth and semantic sensors that are beneficialforobtaininghigh-qualityembodieddata.RoboGen
standard in other simulators, iGibson also provides LiDAR [71] customizes tasks from randomly sampled 3D assets
for embodied agents, allowing embodied agents to obtain 3D through large language models, thereby creating scenes and
point clouds in the scene easily. Regarding embodied agent automatically training agents; HOLODECK [72] can au-
configuration, iGibson supports continuous action control and tomatically customize corresponding high-quality simulation
fine-grained joint control. This allows the embodied agents in scenes in AI2-THOR based on human instructions; PhyScene
iGibson to interact delicately with the objects in the scene [73] generates interactive and physically consistent high-
while moving freely. quality 3D scenes based on conditional diffusion. The Allen
TDW [70] was launched by MIT in 2021. As one of Institute for Artificial Intelligence expanded AI2-THOR and
the latest embodied AI simulators, TDW combines high- proposed ProcTHOR [74], which can automatically generate
fidelity video and audio rendering, realistic physical effects, simulated scenes with sufficient interactivity, diversity, and
and a single flexible controller, making certain progress in rationality. These methods provide new insight into the de-
the perception and interaction of the simulated environment. velopment of embodied AI.
TDWintegratesmultiplephysicsenginesintooneframework,
whichcanrealizethephysicalinteractionsimulationofvarious
IV. EMBODIEDPERCEPTION
materials such as rigid bodies, soft bodies, fabrics, and fluids, The “north stars” of the future of visual perception is
andprovidessituationalsoundswheninteractingwithobjects. embodied-centricvisualreasoningandsocialintelligence[75].
Inthisrespect,TDWhastakenanimportantstepcomparedto Unlike merely recognizing objects in images [76]–[78], agent
other simulators. TDW supports the deployment of multiple with embodied perception must move in the physical world
intelligent agents and provides users with a rich API library and interact with the environment. This requires a deeper
and asset library, allowing users to freely customize scenes understanding of 3D space and dynamic environments. Em-
and tasks according to their own needs, even outdoor scenes bodied perception requires visual perception and reasoning,
andrelatedtasks.Theextremelyhighdegreeoffreedomgives understanding the 3D relations within a scene, and predicting
TDW higher potential. and performing complex tasks based on visual information.
Table III summarizes all the simulators based on the real
A. Active Visual Perception
scenarios mentioned above. Sapien is quite special among
these, it is completely designed for simulating interactions Active visual perception systems require fundamental ca-
with joint objects (such as doors, cabinets, and drawers); pabilities such as state estimation, scene perception, and
Virtualhome’s unique environment graph makes it widely environmentexploration.AsshowninFig.7,thesecapabilities
used for high-level embodied planning based on the natural- havebeenextensivelystudiedwithinthedomainsofVisualSi-
language-described environment; AI2Thor provides a wealth multaneousLocalizationandMapping(vSLAM)[119],[120],
of interactive scenes, but these interactions, like Virtualhome, 3D Scene Understanding [121], and Active Exploration [13].
are only implemented through scripts and do not have real These research areas contribute to developing robust active
physical interactions. However, such a design is sufficient visualperceptionsystems,facilitatingimprovedenvironmental
for embodied tasks that do not focus on fine-grained interac- interaction and navigation in complex, dynamic settings. We
tions; both iGibson and TDW provide fine-grained embodied briefly introduce these three components and summarize the
control and highly simulated physical interactions, among methods mentioned in each part in Table IV.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 8
Function Type Methods
MonoSLAM[79],MSCKF[80],PTAM[81],
TraditionalvSLAM
ORB-SLAM[82],DTAM[83],LSD-SLAM[84]
vSLAM
SLAM++[85],CubeSLAM[86],HDP-SLAM[87],QuadricSLAM[88],So-SLAM[89],
SemanticvSLAM
DS-SLAM[90],DynaSLAM[91],SG-SLAM[92],OVD-SLAM[93],GS-SLAM[94]
Projection-based MV3D[95],PointPillars[96],MVCNN[97]
Voxel-based VoxNet[98],SSCNet[99]),MinkowskiNet[100],SSCNs[101],Embodiedscan[102]
3DSceneUnderstanding
PointNet[103],PointNet++[104],PointMLP[105]),PointTransformer[106],Swin3d[107],
Point-based
PT2[108],PT3[109],PointMamba[110],PCM[111],Mamba3D[112]
Interactingwiththeenvironment Pintoetal.[113],Tatiyaetal.[114]
ActiveExploration
Changingtheviewingdirection Jayaramanetal.[115],NeU-NBV[116],Huetal.[117],Fanetal.[118]
TABLEIV
THECOMPARISONOFTHEACTIVEVISUALPERCEPTIONMETHODS.
construct a lightweight semantic map. QuadricSLAM [88]
Active Visual Perception
employssemantic3Dellipsoidstoachieveprecisemodelingof
Passive Visual Perception Obersvation objectshapesandposesincomplexgeometricalenvironments.
3D Scene Undestanding Improving Observational Abilities So-SLAM [89] incorporates fully coupled spatial structure
constraints (coplanarity, collinearity, and proximity) in indoor
Improving the Accuracy of
Localization and Mapping Activate Exploration environments. To meet the challenges of dynamic environ-
ments, DS-SLAM [90], DynaSLAM [91] and SG-SLAM [92]
Improving Semantic
Analysis Capabilities employ semantic segmentation for motion consistency checks
Improving Environmental State Certainty
Visual SLAM and multiview geometry algorithms to identify and filter
Action dynamic objects, ensuring stable localization and mapping.
OVD-SLAM [93] leverages semantic, depth, and optical flow
Fig.7. Theschematicdiagramofactivevisualperception.VisualSLAMand informationtodistinguishdynamicregionswithoutpredefined
3DSceneUnderstandingprovidethefoundationforpassivevisualperception,
labels, achieving more accurate and robust localization. GS-
while active exploration can provide activeness to the passive perception
system. These three elements complement each other and are essential to SLAM [94] utilizes 3D Gaussian representation that balances
theactivevisualperceptionsystem. efficiency and accuracy through a real-time differentiable
splatting rendering pipeline and adaptive expansion strategy.
1) Visual Simultaneous Localisation and Mapping: Simul-
2) 3D Scene Understanding: 3D scene understanding aims
taneous Localization and Mapping (SLAM) is a technique
to distinguish objects’ semantics, identify their locations, and
that determines a mobile robot’s position in an unknown
infer the geometric attributes from 3D scene data, which is
environment while concurrently constructing a map of that
fundamental in autonomous driving [127], robot navigation
environment [122], [123]. Range-based SLAM [124]–[126]
[128], and human-computer interaction [129] etc. A scene
creates point cloud representations using rangefinders (e.g.,
may be recorded as 3D point clouds using 3D scanning
laser scanners, radar, and/or sonar), but is costly and provides
tools like LiDAR or RGB-D sensors. Unlike images, point
limited environmental information. Visual SLAM (vSLAM)
cloudsaresparse,disordered,andirregular,[121]makesscene
[119], [120] uses on-board cameras to capture frames and
interpretation extremely challenging.
construct a representation of the environment. It has gained
In recent years, numerous deep learning methods for 3D
popularity due to its low hardware cost, high accuracy in
sceneunderstandinghavebeenproposed,whichcanbedivided
small-scalescenarios,andabilitytocapturerichenvironmental
into projection-based, voxel-based, and point-based meth-
information. Classical vSLAM techniques can be divided into
ods. Concretely, projection-based methods (e.g., MV3D [95],
Traditional vSLAM and Semantic vSLAM [120].
PointPillars [96], MVCNN [97] ) project 3D points onto
Traditional vSLAM systems estimate the robot’s pose in various image planes and employ 2D CNN-based backbones
an unknown environment using image information and multi- for feature extraction. Voxel-based methods convert point
view geometry principles to construct a low-level map (e.g., clouds into regular voxel grids to facilitate 3D convolution
sparse maps, semi-dense maps, and dense maps) composed of operations (e.g., VoxNet [98], SSCNet [99]), and some works
point clouds, such as filter-based methods (e.g., MonoSLAM improve their efficiency through sparse convolution (e.g.,
[79], MSCKF [80]), keyframe-based methods (e.g., PTAM MinkowskiNet [100], SSCNs [101], Embodiedscan [102]). In
[81], ORB-SLAM [82]), and direct tracking methods (e.g., contrast, point-based methods process point clouds directly
DTAM [83], LSD-SLAM [84]). Since the point clouds in (e.g., PointNet [103], PointNet++ [104], PointMLP [105]).
theselow-levelmapsdonotcorrespondtoobjectsintheenvi- Recently, to achieve model scalability, Transformers-based
ronment, they are difficult for embodied robots to understand (e.g., PointTransformer [106], Swin3d [107], PT2 [108], PT3
and use directly. With the rise of semantic concepts, semantic [109]) and Mamba-based (e.g., PointMamba [110], PCM
vSLAM systems combined with semantic information solu- [111], Mamba3D [112]) architectures have emerged.
tions have significantly enhanced robots’ ability to perceive 3) ActiveExploration: Thepreviouslyintroduced3Dscene
the unexplored environment. understanding methods endow robots with the ability to per-
Early works, such as SLAM++ [85], use real-time 3D ceive the environment in a passive manner. In such cases,
object recognition and tracking to create efficient object the perception system’s information acquisition and decision-
graphs,enablingrobustloopclosure,relocalization,andobject making do not adapt to the evolving scene. However, passive
detection in cluttered environments. CubeSLAM [86] and perceptionservesasacrucialfoundationforactiveexploration.
HDP-SLAM [87] introduce 3-D rectangular into the map to Given that robots are capable of movement and frequentIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 9
Type Method Years VisualInput LLM-base
ScanRefer[130] 2020 3D ×
(a) (b)
ReferIt3D[131] 2020 3D × Two-Stage Detection Stage Matching Stage
TGNN[132] 2021 3D ×
SAT[133] 2021 3D+2D × Sparse Proposals
Ground Truth
FFL-3DOG[134] 2021 3D ×
Wrong Prediction
3DVG-Transformer[135] 2021 3D ×
Two-stage
LanguageRefer[136] 2022 3D ×
LAR[137] 2022 3D ×
MVT[138] 2022 3D √× Dense Proposals
LLM-Grounder[139] 2023 3D √
ZSVG3D[140] 2023 3D Wrong Prediction
3D-SPS[141] 2022 3D+2D ×
BUTD-DETR[142] 2022 3D ×
One-stage
EDA[143] 2023 3D √×
ReGround3D[144] 2024 3D+2D (c) One-Stage
TABLEV
Description:
COMPARISONOFDIFFERENT3DVGMETHODS.
There is a sofa chair near a
couch. The sofa chair has a Correct Prediction
interaction with their surroundings, they should also be able
table on each side
to explore and perceive their environment actively. The rela-
tionship between them is shown in Fig. 7. Current methods Fig.8. Thediagramoftwo-stage(upper)andone-stage(bottom)3Dvisual
groundingmethods[141].(a)showstheexampleof3Dvisualgrounding.(b)
addressing active perception focus on interacting with the
two-stage method includes Sparse proposals that may overlook the target in
environment[113],[114]orbychangingtheviewingdirection thedetectionstageandDenseproposalsthatmayconfusethematchingstage.
to obtain more visual information [115]–[118]. (c)one-stagemethodscanprogressivelyselectkeypoints(bluepoints−→red
points−→greenpoints)withtheguidanceofthelanguagedescription.
Forexample,Pintoetal.[113]proposedacuriousrobotthat
learns visual representations through physical interaction with within a 3D environment using natural language descriptions
the environment rather than relying solely on category labels [130],[131].AssummarizedinTableV,recentmethodologies
inadataset.Toaddressthechallengeofinteractiveobjectper- in 3D visual grounding can be roughly divided into two
ception across robots with varying morphologies, Tatiya et al. categories: two-stage and one-stage methods [145].
[114]innovativelyproposeamulti-stageprojectionframework
1) Two-stage 3D Visual Grounding methods: Similar to
that transfers implicit knowledge through learned exploratory
corresponding2Dtasks[146],earlyresearchin3Dgrounding
interactions, enabling robots to effectively recognize object
predominantlyutilizedatwo-stagedetect-then-matchpipeline.
properties without the need to relearn from scratch. Recog-
They initially employ pretrained detector [147] or segmentor
nizing the challenge of autonomously capturing informative
[148]–[150] to extract features from numerous object propos-
observations, Jayaraman et al. [115] propose a reinforcement
als within a 3D scene, which are then fused with linguistic
learning method where an agent learns to actively acquire
queryfeaturestomatchthetargetobject.Thefocusofthetwo-
informative visual observations by reducing its uncertainty
stageresearchismainlyonthesecondstage,suchasexploring
about unobserved parts of its environment, using recurrent
thecorrelationbetweenobjectproposalfeaturesandlinguistic
neuralnetworksfortheactivecompletionofpanoramicscenes
query features to select the best-matched object. ReferIt3D
and 3D object shapes. NeU-NBV [116] introduces a mapless
[131] and TGNN [132] not only learn to match the proposal
planning framework that iteratively positions an RGB camera
featureswithtextualembeddingbutalsoencodethecontextual
to capture the most informative images of an unknown scene,
relationship among the objects via graph neural networks. To
using a novel uncertainty estimation in image-based neural
enhance 3D visual grounding in free-form descriptions and
rendering to guide data collection towards the most uncertain
irregular point cloud, FFL-3DOG [134] utilizes a language
views. Hu et al. [117] develop a robot exploration algorithm
scene graph for phrase correlations, a multi-level 3D proposal
that predicts the value of future states using a state value
relation graph for enriching visual features, and a description-
function,combiningofflineMonte-Carlotraining,onlineTem-
guided 3D visual graph for encoding global contexts.
poral Difference adaptation, and an intrinsic reward function
Recently, as the transformer architecture has demonstrated
based on sensor information coverage. To be able to address
outstandingperformanceinnaturallanguageprocessing[151],
the issue of accidental input in open-world environments, Fan
[152] and computer vision tasks [15], [153], research has
et al. [118] propose a strategy that treats active recognition
increasingly focused on using transformers for extracting and
as a sequential evidence-gathering process, providing step-by-
fusing visual language features in 3D visual grounding tasks.
step uncertainty quantification and reliable prediction under
For example, LanguageRefer [136] employs a transformer-
evidence combination theory while effectively characterizing
basedarchitecturecombining3Dspatialembeddings,language
the merit of actions in open-world environments through a
descriptions, and class label embeddings to achieve robust 3D
specially developed reward function.
visualgrounding.3DVG-Transformer[135]isarelation-aware
visual grounding method for 3D point clouds, featuring a
B. 3D Visual Grounding
coordinate-guided contextual aggregation module for relation-
Unlike traditional 2D visual grounding (VG), which oper- enhanced proposal generation and a multiplex attention mod-
ates within the confines of flat images, 3D VG incorporates ule for cross-modal proposal disambiguation. To enable more
depth, perspective, and spatial relationships between objects, fine-grainedreasoningof3Dobjectsandreferringexpressions,
providing a more robust framework for agents to interact with TransRefer3D [154] enhances cross-modal feature represen-
theirenvironment.Thetaskof3DVGinvolveslocatingobjects tation using entity-and-relation aware attention, incorporatingIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 10
(a) (b) step-by-step instruction described goal navigation navigation with interaction
Walk along the hallway past the
s c w p Kh o a eho r i enene pn te i r n wc , g a p aa b a ln ti ks on d s
i
e nint et gh, ng e t st n e tu rr tr
a
htn u ite gh r hn el
t
e d lf b ueet e c nf dt o ta irrat loa g o yta it m ovh in uee . F d r hie ai gn s lh lkd wt ait sn yh i d .e t e h ep o in s ft k u td hl y ea p rt eoo nop dm o oan ft t tt h hh e ee G a w t rh op ao e op s ml h pt eo l
.
i a t t t ef ih nr e o o tm nhk e i tt hc st eih h n e e k tn a , baa lr n en e d d f or pi fgt ua e dtk r ie ia nt t i o noa r gn n,
Environment reach the vacuum cleaner.
Interactive
Noninteractive
Study room Study room Study room
Bed room Bed room Bed room
Natural Language
Hallway Bath room Hallway Bath room Hallway Bath room
Living room Dining room K Living room Dining room K Living room Dining room K
Planning Dialog hcti hcti hcti
ne ne ne
Agent Oracle
Fig.9. (a)OverviewofVLN.Theembodiedagentcommunicateswithhumansthroughnaturallanguage.Humansissueinstructionstotheembodiedagent,
whocompletestaskssuchasplanninganddialog.Subsequently,throughcollaborativecooperationortheembodiedagent’sindependentactions,actionsare
madeininteractiveornon-interactiveenvironmentsbasedonvisualobservationsandinstructions,(b)DifferenttasksofVLN.
self-attention, entity-aware attention and relation-aware atten- 3D-SPS[141]takesthe3DVGtaskasakeypointselection
tion. Most of the above methods for 3D VG focus on specific problem and avoids the separation of detection and match-
viewpoints, but the learned visual-linguistic correspondences ing. Specifically, 3D-SPS initially coarsely samples language-
may fail when the viewpoint changes. In order to learn more relatedkeypointsthroughthedescription-awarekeypointsam-
view-robust visual representations, MVT [138] proposes a pling module. Subsequently, it finely selects target keypoints
multi-view transformer that learns view-independent multi- andpredictsthefoundationusingthegoal-orientedprogressive
modal representations.To mitigate the limitations of sparse, mining module. Inspired by the 2D image language pre-train
noisy, and incomplete point clouds, various methods have model such as MDETR [155] and GLIP [156], BUTD-DETR
exploredtheincorporationofdetailed2Dvisualfeaturesfrom [142] proposes a bottom-up top-down detection transformer
captured (e.g., SAT [133] or synthesized (e.g., LAR [137]) that can be used for 2D and 3D VG. Concretely, BUTD-
images to enhance 3D visual grounding tasks. DETRutilizeslabeledbottom-upboxproposalsandtop-down
Existing 3D VG methods often rely on extensive labeled language descriptions to guide the decoding of target objects
data for training or show limitations in processing com- andcorrespondinglanguagespansthroughthepredictionhead.
plex language queries. Inspired by the impressive language However, the aforementioned methods either extract
understanding capabilities of LLMs, LLM-Grounder [139] sentence-levelfeaturesthatcoupleallwordsorfocusmoreon
proposes an open vocabulary 3D visual grounding pipeline object names in the description, which would lose the word-
that requires no labeled data, leveraging LLM to decompose level information or neglect other attributes. To address these
queries and generate plans for object identification, followed issues,EDA[143]explicitlydecouplesthetextualattributesin
by evaluating spatial and commonsense relations to select the a sentence and conducts dense alignment between such fine-
best matching object. To capture view-dependent queries and grainedlanguageandpointcloudobjects.Firstly,thelongtext
decipherspatialrelationsin3Dspace,ZSVG3D[140]designs is decoupled into five semantic components, including main
a zero-shot open-vocabulary 3D visual grounding method that object, auxiliary object, attributes, pronoun, and relationship.
uses LLM to identify relevant objects and perform reasoning, Subsequently, the dense alignment is designed to align all
transforming this process into a scripted visual program and object-related decoupled textual semantic components with
then into executable Python code to predict object locations. visual features. To be able to reason human intentions from
However, as shown in Fig. 8 (b), these two-stage methods implicit instructions, ReGround3D [144] designs a visual-
face the dilemma of determining the number of proposals centric reasoning module, powered by a Multi-modal Large
because the 3D detectors used in the first stage require sam- Language Model, and a 3D grounding module that accurately
pling several keypoints to represent the entire 3D scene and obtains object locations by revisiting enhanced geometry and
generate corresponding proposals for each keypoint. Sparse fine-grained details from 3D scenes. Additionally, a Chain-of-
proposals may overlook targets in the first stage, making Grounding mechanism is employed to improve 3D reasoning
them unmatchable in the second stage. Conversely, dense grounding through interleaved reasoning and grounding steps.
proposals may contain inevitable redundant objects, leading
to difficulties in distinguishing targets in the second stage C. Visual Language Navigation
due to overly complex inter-proposal relationships. Moreover, VisualLanguageNavigation(VLN)standsasakeyresearch
the keypoint sampling strategy is language-agnostic, which problem of Embodied AI, aiming at enabling agents to navi-
increases the difficulty for detectors to identify language- gate in unseen environments following linguistic instructions.
related proposals. VLNrequiresrobotstounderstandcomplexanddiversevisual
2) One-stage 3D Visual Grounding methods: As shown in observations and meanwhile interpret instructions at different
Fig. 8 (c), in contrast to two-stage 3D VG methods, one- granularities. The input for VLN typically consists of two
stage 3D VG methods integrate object detection and feature parts: visual information and natural language instructions.
extraction guided by language queries, making it easier to Thevisualinformationcaneitherbeavideoofpasttrajectories
locate objects relevant to the language. or a set of historical-current observation images. The naturalIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 11
Dataset Year Simulator Environment Feature Size
R2R[64] 2018 Matterport3D Indoor,Discrete Step-by-stepinstructions 21,567
R4R[157] 2019 Matterport3D Indoor,Discrete Step-by-stepinstructions 200,000+
VLN-CE[158] 2020 Habitat Indoor,Continuous Step-by-stepinstructions -
TOUCHDOWN[159] 2019 - Outdoor,Discrete Step-by-stepinstructions 9,326
REVERIE[160] 2020 Matterport3D Indoor,Discrete Describedgoalnavigation 21,702
SOON[161] 2021 Matterport3D Indoor,Discrete Describedgoalnavigation 3,848
DDN[162] 2023 AI2-THOR Indoor,Continuous Demand-drivennavigation 30,000+
ALFRED[163] 2020 AI2-THOR Indoor,Continuous Navigationwithinteraction 25,743
OVMM[164] 2023 Habitat Indoor,Continuous Navigationwithinteraction 7,892
BEHAVIOR-1K[165] 2023 OmniGibson Indoor,Continuous Long-spannavigationwithinteraction, 1,000
CVDN[166] 2020 Matterport3D Indoor,Discrete Dialog,oracle 2,050
DialFRED[167] 2022 AI2-THOR Indoor,Continuous Dialog,oracle 53,000
TABLEVI
COMPARISONOFDIFFERENTVLNDATASETS.
language instructions include the target that the embodied follow instructions to navigate in the street view rendering
agent needs to reach or the task that the embodied agent is simulation of New York City to find the specified object.
expectedtocomplete.Theembodiedagentmustusetheabove Similar to R2R, the REVERIE dataset [160] is also built
information to select one or a series of actions from a list of based on the Matterport3D simulator. REVERIE requires
candidates to fulfill the requirements of the natural language embodied agents to accurately locate the distant invisible
instructions. This process could be represented as: targetobjectspecifiedbyconcise,human-annotatedhigh-level
natural language instructions, which means that embodied
Action=M(O,H,I) (1) agents need to find the target object among a large number of
objectsinthescene.InSOON[161],agentsreceivealongand
where Action is the chosen action or a list of action can-
complexinstructionfromcoarsetofinetofindthetargetobject
didates, O is the current observation, H is the historical
in the 3D environment. During navigation, agents first search
information, and I is the natural language instruction.
a larger area, and then gradually narrow the search range
SR (Success Rate), TL (Trajectory Length), and SPL
according to the visual scene and instructions. This makes
(Success Weighted by Path Length) are the most commonly
SOON’s navigation target-oriented and independent of the
used metrics in VLN. Among them, SR directly reflects the
initial position. DDN [162] moves a step further beyond these
navigationperformanceoftheembodiedagent,TLreflectsthe
datasets, only providing human demands without specifying
navigation efficiency, and SPL combines both to indicate the
explicitobjects.Theagentneedstonavigatethroughthescene
overall performance of the embodied agent.
to find objects that meet human needs.
Below, we will introduce visual-linguistic navigation di- Shridhar et al. proposed the ALFRED dataset [163] based
vided into two parts: datasets and methods. on the AI2-THOR simulator. In ALFRED, embodied agents
1) Datasets: In visual-linguistic navigation, natural lan- need to understand environmental observations and complete
guage instructions can be a series of detailed action descrip- household tasks in an interactive environment according to
tions, a fully described goal, or just a roughly described task, coarse-grained and fine-grained instructions. The core task of
even only the demands of human. The tasks that embodied OVMM[164]istopickanyobjectinanyunseenenvironment
agents need to complete maybe just a single navigation, or and place it in a specified location. Embodied agents need to
navigation with interaction, or multiple navigation tasks that locatethetargetobjectinthehomeenvironment,navigateand
need to be completed in sequence. These differences bring grabit,andthennavigatetothetargetlocationtoputdownthe
different challenges to visual-linguistic navigation, and many object. OVMM provides a simulation based on Habitat and a
different datasets have been born as a result. Based on these framework for implementation in the real world. Behavior-
differences,wewillintroducetheimportantdatasetsinvisual- 1K [165], based on the survey of human needs, designed
linguistic navigation separately. 1000 long-sequence, complex skill-dependent daily tasks in
RoomtoRoom[64]isavisual-linguisticnavigationdataset OmniGibson, which is an extension of iGibson. Embodied
created by Anderson et al. based on Matterport3D. In R2R, agentsneedtocompletelong-spannavigation-interactiontasks
embodied agents navigate according to step-by-step instruc- which may contain thousands of low-level action steps based
tions, choosing the next adjacent navigation graph node to on visual information and natural language instructions. Such
advance based on visual observations until they reach the complextasksposehigher requirementsfortheunderstanding
target location. Embodied agents need to dynamically track and memory of embodied agents.
progress to align the navigation process with fine-grained There are also some more special datasets. CVDN [166]
instructions. Room-for-Room [157] extends the paths in R2R requires embodied agents to navigate to the target based on
to longer trajectories, requires higher for the long-distance dialogue history, and ask questions for help to decide the
instruction and history alignment capabilities of embodied next action when uncertain. DialFRED [167], an extension of
agents. VLN-CE [158] extends R2R and R4R to continuous ALFRED,allowsagentstoaskquestionsduringthenavigation
environments, embodied agents can move freely in the scene. andinteractionprocesstogethelp.Thesedatasetsallintroduce
Thismakestheactiondecisionofembodiedagentsmorediffi- additional oracles, and embodied agents need to obtain more
cult.Differentfromtheabovedatasetsbasedonindoorscenes, information beneficial to navigation by asking questions.
Chen et al. created the TOUCHDOWN dataset [159] based 2) Method: VLN has made great strides in recent years.
on Google Street View. In TOUCHDOWN, embodied agents With the astonishing performance of large models in variousIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 12
Method Model Year Feature
requiring the model to model time-ordered information at
LVERG[168] 2020 GraphLearning
CMG[169] 2020 AdversarialLearning differentgranularities,therebyachievingadeepunderstanding
RCM[170] 2021 Reinforcementlearning
of historical trajectories and memories.
FILM[171] 2022 SemanticMap
Memory- LM-Nav[172] 2022 GraphLearning The navigation graph discretizes the environment, but con-
Understanding HOP[173] 2022 HistoryModeling
currently understanding and encoding the environment is also
Based NaviLLM[174] 2024 LargeModel
FSTT[175] 2024 Test-TimeAugmentation important.FILM[171]usesRGB-Dobservationsandsemantic
DiscussNav[176] 2024 LargeModel
segmentation to gradually build a semantic map from 3D
GOAT[177] 2024 CausalLearning
VER[178] 2024 EnvironmentEncoder voxels during the navigation. VER [178] quantifies the phys-
NaVid[179] 2024 LargeModel
ical world into structured 3D units through 2D-3D sampling,
LookBY[180] 2018 ReinforcementLearning
Future- NvEM[181] 2021 EnvironmentEncoder providing fine-grained geometric details and semantics.
Prediction BGBL[182] 2022 GraphLearning Different learning schemes also explore how to utilize his-
Based Mic[183] 2023 LargeModel
HNR[184] 2024 EnvironmentEncoder torical trajectories and memories better. Through adversarial
ETPNav[185] 2024 GraphLearning learning,CMG[169]alternatesbetweenimitationlearningand
MCR-Agent[186] 2023 Multi-LevelModel
Else OVLM[187] 2023 LargeModel exploration encouragement schemes, effectively strengthening
TABLEVII
the understanding of instructions and historical trajectories,
COMPARISONOFVLNMETHODS.
shortening the difference between training and inference.
GOAT [177] directly trains unbiased models through Back-
fields,thedirectionandfocusofVLNresearchhavebeenpro-
door Adjustment Causal Learning (BACL) and Frontdoor
foundlyinfluenced.Butoverall,theresearchmethodsofVLN
Adjustment Causal Learning (FACL), conducts contrastive
can still be roughly divided into two directions: Memory-
learning with vision, navigation history, and their combina-
Understanding Based, and Future-Prediction Based.
tion to instructions, enabling the agent to make fuller use
Memory-Understanding based methods focus on the per- of information. The enhanced cross-modal matching method
ception and understanding of the environment, as well as proposed by RCM [170] uses goal-oriented external rewards
model design based on historical observations or trajectories, and instruction-oriented internal rewards to perform cross-
which is a method based on past learning. Future-Prediction modal grounding globally and locally and learns from its own
basedmethodspaymoreattentiontomodeling,predicting,and historical good decisions through self-supervised imitation
understanding the future state, which is a method for future learning. FSTT [175] introduces TTA into visual-linguistic
learning.SinceVLNcanberegardedasapartiallyobservable navigation and optimizes the model in terms of gradients
Markovdecisionprocess,wherefutureobservationsdependon and model parameters at two scales of time steps and tasks,
the current environment and actions of the intelligent agent, effectively improving model performance.
historical information has important significance for navi- The specific application of large models in Memory-
gation decisions, especially long-span navigation decisions, Understanding based methods is to understand the representa-
hence Memory-Understanding based methods have always tion of historical memory and to understand the environment
been the mainstream of VLN. However, Future-Prediction and tasks based on its extensive world knowledge. NaviLLM
based methods still have important significance. Its essential [174] integrates the historical observation sequence into the
understanding of the environment has great value in VLN embeddingspacethroughthevisualencoder,inputsthemulti-
in continuous environments, especially with the rise of the modal information of the fusion encoding into the large lan-
concept of world model, Future-Prediction based methods are guage model and fine-tunes it, reaching the state-of-the-art on
receiving more and more attention from researchers. multiplebenchmarks.NaVid[179]makesimprovementsinthe
Memory-Understanding based. Graph-based learning is encoding of historical information, achieves different degrees
a very important part of the methods based on understand- ofinformationretentiononhistoricalobservationsandcurrent
ing and memory of the past. Graph-based learning usually observationsthroughdifferentdegreesofpooling.DiscussNav
represents the navigation process in the form of a graph, [176] assigns large model experts with different abilities to
where the information obtained by the embodied agent at different roles, drives the large models to discuss before navi-
each time step is encoded as nodes of the graph. Embodied gation actions to complete navigation decisions, and achieves
agent obtains global or partial navigation graph information excellentperformanceinzero-shotvisual-linguisticnavigation.
as a representation of the historical trajectory. In LVERG Future-Prediction Based. Graph-based learning is also
[168],theauthorsencodethelanguageinformationandvisual widelyusedinFuture-Predictionbasedmethods.BGBL[182]
information of each node separately, design a new language and ETPNav [185] use a similar method to design a waypoint
and visual entity relationship graph to model the inter-modal predictor that can predict movable path points in a continuous
relationship between text and vision, and the intra-modal re- environmentbasedontheobservationofthecurrentnavigation
lationship between visual entities. LM-Nav [172] uses a goal- graph node. They aim to migrate complex navigation in
conditioned distance function to infer connections between a continuous environment to node-to-node navigation in a
original observation sets and construct a navigation graph, discrete environment, thereby bridging the performance gap
and extracts landmarks from the instructions through a large from discrete environments to continuous environments.
language model, uses a visual language model to match them Improving the understanding and perception of the future
withthenodesofthenavigationgraph.AlthoughHOP[173]is environment through environmental encoding is also one of
notbasedongraphlearning,itsmethodissimilartothegraph, theresearchdirectionsforpredictingandexploringthefuture.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 13
NvEM [181] uses a theme module and a reference module to
perform fusion encoding of neighbor views from the global Tactile-Sensors Data Design Appearances
and local perspectives. This is actually an understanding and Non-Vision
Vision
learning of future observations. HNR [184] uses a large-
scale pre-trained hierarchical neural radiation representation Applications (a)
model to directly predict the visual representation of the
future environment rather than pixel-level images using three- Data Design Appearances
dimensional feature space encoding, and builds a navigable (b) (c)
futurepathtreebasedontherepresentationofthefutureenvi-
(b) (c) (d) (e)
ronment. They predict the future environment from different
levels, providing effective references for navigation decisions.
Some reinforcement learning methods are also applied to (d) (e)
predict and explore future states. LookBY [180] employs
reinforcement prediction to enable the prediction module to Estimation Manipulation Classification/Recognition
imitate the world and forecast future states and rewards. This
allows the agent to directly map “current observations” and
“predictionsoffutureobservations”toactions,achievingstate-
of-the-art performance at the time. The rich world knowledge
and zero-shot performance of large models provide many
possibilities for Future-Prediction based methods. MiC [183]
Fig.10. DifferenttypesofTactileSensors.Non-visionsensors(a)mainlyuse
requires the large language model to directly predict the sensorsofforce,pressure,vibrationandtemperaturetogettactileknowledge.
target and its possible location from the instructions and Vision-based tactile sensors ((b)-(e)) are based on optical principles. A
cameraisplacedbehindthegeltorecordtheimageofitsdeformation,using
provides navigation instructions through the description of
illumination from light sources at different directions. (a)-(e) are the details
scene perception. This method requires the large language fromBioTac,Gelsight,DIGIT,9DTactandGelsilm.
model to fully exert its ‘imagination’ and build an imagined
scene through prompts. computer vision had a wonderful performance, there has been
In addition, there are some methods that both learn from a growing focus on vision-based tactile sensors which obtains
thepastandforthefuture.MCR-Agent[186]designsathree- tactile through optical principles. Using images of the gel’s
layer action strategy, which requires the model to predict the deformationastactileinformation,vision-basedtactilesensors
target from the instructions, predict the pixel-level mask for such as GelSight [200], Gelslim [201], DIGIT [202], 9DTact
thetargettobeinteract,andlearnfromthepreviousnavigation [203], TacTip [204], GelTip [205] and AllSight [206] have
decision; OVLM [187] requires the large language model to been used for numerous applications. At the same time, the
predict the corresponding operations and landmark sequences simulation of tactile sensors like TACTO [207] and Taxim
for the instructions. During the navigation process, the visual [208], incorporating elastomer deformation together with the
language map will be continuously updated and maintained, optical simulation, are also widely used. Recent work has
andtheoperationswillbelinkedtothewaypointsonthemap. focused on low costs [203] and the installation on robotic
hands [202], [209], [210]. Recently, multi-modal tactile sen-
D. Non-Visual Perception: Tactile sors are also emerging. Inspired by the touch mechanism
The skin facilitates human tactile perception. The skin of human skin, works progressed in multi-modal tactile skin
changes shape when touched, and its abundant nerve cells which comprised multi information like pressure, proximity,
send electrical signals [188]. This tactile perception allows accelerationandtemperaturedetectors.Solutionsofteninvolve
humans to grasp handy work fully. Therefore, touch is vital techniques like flexible materials and modular design.
for robots to interact with the real world. A sense of touch 2) Datasets: The datasets of non-vision sensors are con-
enables robots to acquire information such as material, shape, tained with electrode values 3D net force vectors, and contact
temperature, and even objects’ contact force and gravity. location. Therefore, the objects in the datasets are usually
Current work on tactile perception focuses on three areas: force samples and grasping samples. Its tasks are mainly the
sensor design, dataset construction, and application. Tactile estimation of force types, force value and grasping details.
perception undoubtedly enhances the human-computer inter- The datasets mainly collected by BioTac series [198]. As
action experience and holds great promise [189]–[191]. for vision-based sensors, with their high-resolution images
1) Sensor Design: Tactile sensor design methods can be of deformation gel, except estimating the force information
divided into three categories: non-vision-based, vision-based, and sliding, they focus more on texture classification and
and multi-modal. At early time, tactile sensors were chiefly 3D reconstruction. The objects in the datasets are usually
engineered to register fundamental, low-dimensional sensory household object, wildlife environments, different materials
outputs such as force, pressure, vibration, and temperature andgraspingitems.Atthesametime,astheimageinformation
[192]–[197]. Their principles are mostly related to electricity can be easily aligned and bound with the other modalities
and physical mechanics and their data is often low-dimension (images, language, audio, etc) [15], [211], the perception of
series with temporal correlations. One of the notable rep- touch in embodied agent mainly revolves around visual-based
resentatives is BioTac [198] and its simulator [199]. Since sensors. The datasets revolves around the Geisight sensors,IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 14
Sensor Real/ Sensor/
Dataset Year DataFormat Type Size Continuous
Types Simulated Simulator
√
TheFeelingofSuccess[212] 2017 Vision Tac,Vision,Label DailyNecessities Real Gelsight 106Objects
SynTouch[199] 2019 Non-vision Tac,Force,Location - Real BioTac 300kReadings -
√
BioTacForceEstimation[199] 2019 Non-vision Tac,Force - Real BioTac 20kSamples
√
DecodingtheBioTac[213] 2020 Non-vision Tac,Force,Location - Real BioTac 50kPoints
√
ObjectFolder1.0[214] 2021 Vision Tac,Vision,Audio HouseholdObject Simulate TACTO 100Objects
√
ObjectFolder2.0[215] 2022 Vision Tac,Vision,Audio HouseholdObject Simulate Taxim 1000Objects
Clothings&
SSVTP[216] 2022 Vision Tac,Vision Real DIGIT 4500Pairs ×
MetalMaterial
TACTO √
YCB-Slide[217] 2022 Vision Tac,Vision,Other DailyNecessities BOTH 10Objects
&DIGIT
√
ObjectFolderReal[218] 2023 Vision Tac,Vision,Audio HouseholdObject Real Gelslim 100Objects
Clothings&
TVL[219] 2024 Vision Tac,Vision,Text Real DIGIT 44KPairs ×
MetalMaterial
TABLEVIII
COMPARISONOFDIFFERENTVISION-BASEDTACTILEDATASETS.
DIGIT sensor and their simulators [200], [202], [203], [207]. and high cost, it is challenging for practical application.
We introduce ten commonly used tactile datasets, which are 2. Classification&Recognition. Tactile representations
summarized in Table VIII. learning focused on material classification work and multi-
3) Methods: Tactile perception has numerous applications. modal understanding. Method can be divied into 2 cate-
The information acquired from sensor enables robots to per- gories:Traditional Methods and LLMs&VLMs Methods.
form accurate robotic manipulation task, finish multi-modal Traditional Methods. Various traditional approaches have
work and even enhance their abilities in 3D reconstruction been employed to enhance tactile representation learning. Au-
and localization. toencoder frameworks have been instrumental in developing
1. Robotic Manipulation. In these task, bridging the sim- compact tactile data representations. Polic et al. [227] used a
to-realgapismorethanimportant.Reinforcementlearningand convolutional neural network autoencoder for dimensionality
GAN-basedmethodshavebeenproposedtoaddressvariations reduction of optical-based tactile sensor images. Gao et al.
in accurate, on-time robotic manipulation tasks. [228] created a supervised recurrent autoencoder to handle
Reinforcement Learning method. Visuotactile-RL [220] heterogeneous sensor datasets, while Cao et al. [229] created
proposed several methods to existing RL methods, including TacMAE used a masked autoencoder for incomplete tactile
tactilegating,tactiledataaugmentationandvisualdegradation. data. Zhang et al. [230] introduced MAE4GM, a multimodal
Rotateit [221] is a system that enables fingertip-based object autoencoderintegratingvisuo-tactiledata.Sincetactileactsas
rotationalongmultipleaxesbyleveragingmultimodalsensory acomplementtoothermodes,JointTrainingmethodsareused
inputs. It trained the network by reinforcement learning poli- to fuse multiple modalities. Yuan et al. [231] trained CNNs
cieswithprivilegedinformationandenabledonlineinference. with modalities included depth, vision, and tactile data. Simi-
[222] proposed a deep RL approach to object pushing using larly,Leeetal.[232]usedavariationalBayesianapproachfor
only tactile perception. It came up with a goalconditioned modalities like force sensors series and end-effector metrics.
formulationthatallowsbothmodel-freeandmodelbasedRLto For better learning representation, Self-supervised methods
obtain accurate policies for pushing an object to a goal. Any- like contrastive learning are also a key technique in binding
Rotate [223] focused on in-hand manipulation. It is a system modalities together. Researches differ in contrastive methods.
for gravity-invariant multi-axis in-hand object rotation using Lin et al. [233] simply paired tactile inputs with multiple
dense featured sim-to-real touch, constructing a continuous visual inputs and Yang et al. [234] employed visuo-tactile
contact feature representation to provide tactile feedback for contrastivemultiviewfeatures.Kerretal.[216]usedInfoNCE
training a policy in simulation and introduce an approach to loss inspired by CLIP and Guzey et al. [235] used BYOL.
perform zero-shot policy transfer by training an observation These traditional methods have established a solid foundation
model to bridge the sim-to-real gap. for tactile representation learning.
GAN-based method. ACTNet [224] proposed an unsu- LLMs&VLMs Methods. Large Language Models (LLM)
pervised adversarial domain adaptation method to narrow and Vision-Language Models (VLM) shows the amazing un-
the domain gap for pixel-level tactile perception tasks. An derstanding of cross-modal interactions and strong zero-shot
adaptively correlative attention mechanism was introduced to performance recently. Recent works from Yang et al. [190],
improve the generator, which is capable of leveraging global Fu et al. [219] and Yu et al. [236] encoded and aligned
information and focusing on salient regions. However, pixel- tactiledatawithvisualandlanguagemodalitiesbycontrastive
level domain adaptation lead to error accumulation, degrade pretrainedmethod.ThenalargelanguagemodelslikeLLaMA
performance, and increased structural complexity and training wouldbeapplied,usingfine-tunemethodtofittasksliketactile
costs. In comparison, STR-Net [225] proposed a feature-level description. The advent of LLM and VLM techniques has
unsupervised framework for tactile images, narrowing the further advanced the field, enabling more comprehensive and
domain gap for feature-level tactile perception tasks. robust cross-modal tactile representations.
Moreover, some methods focuse on sim-to-real. For exam- 3. 3D Reconstruction. Suresh et al. [237] incrementally
ple,theTactileGym2.0[226].However,duetoitscomplexity reconstructed the local shape of 3D household objects from aIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 15
Exploring...
... rt iu gr hn t rt iu gr hn t rt iu gr hn t fom rwov ae rd
turn move move
... left forward forward
Single Objective Multiple Objectives Multi-agent Interaction
Question: What room is the vase Question: Is the dinner table the Question: Is there a washing Question: Are there any apples
located in? same color as the TV cabinet? machine in the house? in the fridge?
: A1 : A2 : A3
Action:
open the fridge
ANSWER
Answer: Living room. Answer: No. Answer: Yes. Answer: No.
Knowledge-based Episodic Memory Object States
Q rou oe ms t uio sen d: I tos t lh oe wre
e
ra tn
h
eo b teje mc pt ein
ra
t th ue
r
el ?iving E Hp ii ss to od ryic Question: Where is the clock? Q livu ines gt i ro on o:
m
C ?ould someone be watching TV in the
TV air conditioner curtain
Answer: It is mounted on the wall above the on on withdrawn
Answer: Yes. cabinets, directly across from the stove. Answer: Yes.
Fig.11. Thetopgrayboxdisplaysthescenesanagentobservesduringexploration.Belowarevarioustypesofquestionansweringtasks.Exceptforthetask
ofansweringquestionsbasedonepisodicmemory,theagentceasesexplorationonceithasgatheredsufficientinformationtoanswerthequestion.
sequenceoftactileimagesandanoisydepthmap,representing standardized formats for visual and auditory data, tactile
the 3D shape as a signed distance function sampled from a data formats vary widely among sensor manufacturers, which
Gaussianprocessandreformulateditasprobabilisticinference makes it difficult for large-scale learning on data collected
on a spatial graph, providing a robust method for local shape fromheterogeneoussensors,limitingtheusefulnessofpublicly
reconstruction. Smith et al. [238] presented an effective chart- available tactile datasets.
basedapproach.Ittookgraspingparameters,agroupoftouch
readingsandanRGBimageoftheobjectasinputandencoded V. EMBODIEDINTERACTION
them to charts, using a neural network to reconstruct 3D
Embodied interaction tasks refer to scenarios where agents
shapeestimate.Then,thegropupdatedthemintoactivetouch
interact with humans and environment in a physical or sim-
learned models with sliding sensors [239]. Comi et al. [240]
ulated space. The typical embodied interaction tasks are Em-
employed deep learning for 3D shape reconstruction solely
bodied Question Answering (EQA) and embodied grasping.
basedontactileinput.TheyusedaCNNtomaptactileimages
into local meshes and a DeepSDF-based model to predict the
complete 3D shape. A. Embodied Question Answering
4) Difficulties: 1) Disadvantages of sensors with different For Embodied Question Answering (EQA) task, the agent
principles: traditional sensors provides simple, limited and needs to explore the environment from a first-person perspec-
low-dimension data, posing challenges for multi-modal learn- tivetogatherinformationnecessarytoanswerthegivenques-
ing. Meanwhile, vision-based sensors and electronic skins, tions. An agent with autonomous exploration and decision-
although offering high accuracy, are cost-prohibitive. And making capabilities must not only consider which actions to
vision-based sensors are unable to provide temperature infor- take to explore the environment but also determine when to
mation. 2) Difficulties in data acquisition: tactile datasets are stop exploring to answer questions. Existing works focus on
rare and heterogeneous, lacking the standardized, extensive different types of questions, some of which are shown in Fig.
repositories found in fields like vision. Additionally, data 11. In this section, we first introduce the existing datasets and
collection is difficult. It is difficult to gather both tactile then discuss related methods.
and visual information together, although some efforts have 1) Datasets: Conductingrobotexperimentsinrealenviron-
been made in simplified collection devices. 3) Difficulties in ments is often constrained by scenarios and robot hardware.
inconsistent standards: there are a variety of sensors on the As virtual experimental platforms, simulators offer suitable
market, with inconsistent standards and principles. Even with environmental conditions for constructing embodied question
similar imaging patterns, vision-based tactile sensors’ design answering datasets. Training and testing models on datasets
andcalibrationstillresultsinasignificantdomaingap.Unlike created in simulators significantly reduce experimental costsIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 16
Dataset Year Type DataSources Simulator QueryCreation Answer Size
EQAv1[241] 2018 ActiveEQA SUNCG House3D Rule-Based open-ended 5,000+
MT-EQA[242] 2019 ActiveEQA SUNCG House3D Rule-Based open-ended 19,000+
MP3D-EQA[243] 2019 ActiveEQA MP3D SimulatorbasedonMINOS Rule-Based open-ended 1,136
IQUADV1[244] 2018 InteractiveEQA - AI2THOR Rule-Based multi-choice 75,000+
VideoNavQA[245] 2019 EpisodicMemoryEQA SUNCG House3D Rule-Based open-ended 101,000
K-EQA[246] 2023 ActiveEQA - AI2THOR Rule-Based open-ended 60,000
OpenEQA[247] 2024 ActiveEQA,EpisodicMemoryEQA ScanNet,HM3D Habitat Manual open-ended 1,000+
HM-EQA[248] 2024 ActiveEQA HM3D Habitat VLM multi-choice 500
S-EQA[249] 2024 ActiveEQA - VirtualHome LLM binary -
TABLEIX
COMPARISONOFDIFFERENTEQADATASETS.
and enhance the success rate of deploying models on real four types of questions: existence, counting, enumeration, and
machines. We briefly introduces nine embodied question an- comparison. Each entity is mapped to a knowledge base and
swering datasets, which are summarized in Table IX. a knowledge graph is further constructed. In this work, the
EQA v1 [241] is the first dataset designed for embodied templates provided in IQA and MT-EQA are extended to a
question answering. Built on synthetic 3D indoor scenes from setofgrammars.Afterspecifyingobjectsandlogicalrelation-
the SUNCG dataset [99] within the House3D [250] simulator, ships, knowledge graphs, scene graphs, etc. are introduced to
EQA v1 comprises four types of questions: location, color, generate questions and compute the ground truth answer. The
color room, and preposition. It features over 5,000 questions resulting K-EQA dataset consists of 60,000 questions across
distributed across more than 750 environments. The questions 6000 different environment setups.
are constructed via functional program execution, using tem- OpenEQA [247] is the first open-vocabulary dataset for
plates to select and combine basic operations. EQA,supportingbothepisodicmemoryandactiveexploration
cases.TheepisodicmemoryEQA(EM-EQA)tasksinvolvean
Similar to EQA v1, MT-EQA [242] is built in House3D
agentdevelopinganunderstandingoftheenvironmentfromits
usingSUNCGbyexecutingfunctionalprogramsconsistingof
episodicmemorytoanswerquestions,similartoVideoNavQA.
some basic operations. However, it further extends the single-
In active EQA (A-EQA) tasks, the agent answers questions
object question answering task to a multi-object setting. Six
by taking exploratory actions to gather necessary information.
types of questions are designed, involving the comparison of
Using ScanNet [253] and HM3D [254], human annotators
color,distance,andsizebetweenmultipleobjects.Thedataset
constructed over 1,600 high-quality questions from more than
contains 19,287 questions in 588 environments.
180 real world environments in Habitat.
MP3D-EQA [243] is built on a simulator developed based
Utilizing GPT4-V, HM-EQA [248] is constructed in the
on MINOS [251] using the Matterport3D dataset [252], ex-
Habitat simulator using the Habitat-Matterport 3D dataset. It
pandingthequestion-answeringtasktoarealistic3Denviron-
includes 500 questions across 267 different scenes, which can
ment.ReferringtoEQAv1,MP3D-EQAutilizesthreetypesof
beroughlycategorizedintoidentification,counting,existence,
templates: location, color, and color room, generating a total
status, and location. For consistency, each question in the
of 1,136 questions in 83 home environments.
dataset has four multiple choices.
IQUAD V1 [244] is built upon AI2-THOR and consists
S-EQA [249] leverages GPT-4 in VirtualHome for data
of three types of questions: existence, counting, and spatial
generation and employs cosine similarity calculations to de-
relationships. It uses a set of templates written down a priori
cide whether to retain the generated data, thereby enhancing
to generate more than 75,000 multiple choice questions, each
dataset diversity. In S-EQA, answering questions requires the
accompanied by a unique scene configuration. Unlike other
assessment of a collection of consensus objects and states to
datasets, answering IQUAD V1 questions requires the agent
reach an existential “Yes/No” answer.
to have a good understanding of affordances and interact with
2) Methods: Theembodiedquestionansweringtaskmainly
the dynamic environment.
involves navigation and question-answering subtasks, with
VideoNavQA[245]decouplesthevisualreasoningfromthe implementation methods broadly categorized into two types:
navigation aspect of the EQA problem. In this task, the agent neural network-based and LLMs/VLMs-based.
accesses videos corresponding to exploration trajectories with Neural Network Methods. In early work, researchers
sufficient information to answer questions. Still referring to mainly addressed the embodied question answering task by
EQAv1,VideoNavQAgeneratesquestionsaccordingtofunc- building deep neural networks. They trained and fine-tuned
tional, template-style representation. It also renders shortest these models using techniques such as imitation learning and
trajectoriestosimulatenear-optimalnavigationpaths,creating reinforcement learning to improve performance.
videoscorrespondingtowhatanagentwouldseewhileexplor- The EQA task was first proposed by Das et al. [241].
ing the environment. VideoNavQA generates about 101,000 In their work, the agent consists of four main modules:
pairs of videos and questions in the House3D environment vision, language, navigation, and answering. These modules
using SUNCG, covering 28 types of questions belonging to 8 are primarily constructed using traditional neural building
categories such as existence, counting, and localization. blocks - Convolutional Neural Networks (CNNs) and Re-
Unlikepreviousdatasetsthatexplicitlyspecifytargetobjects current Neural Networks (RNNs). These modules undergo
in questions, K-EQA [246] features complex questions with training in two phases. Initially, the navigation and response
logical clauses and knowledge-related phrases, requiring prior modules are trained independently on automatically generated
knowledge to answer. It is built in AI2Thor and includes expertnavigationdemonstrationsusingimitationorsupervisedIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 17
(a) Language-guided Grasping (b)Human-Agent-Object Interaction (c)Publication status
Instruction Scene Grounding & Grasp Pose Generation 700
Grounding & Grasp Pose Generation
600
Baymax.
Direct Object 500
Specification
Instruction
Grasp the knife at its 400
handle. Action
Agent with Arm
300
Grasp the keyboard Observation Grasping
Spatial that is to the right of 200
Inference Reasoning the brown kleenexbox. 100
Required
L Ro eg asic oa nl i ng I g ia vm e mth eir ss oty m, c ea thn i ny go u to 0 2017 20182019 20202021 20222023
drink? Scene
Total published papers Number of papers published per year
Fig. 12. The overview of the embodied grasping task. (a) demonstrates examples of language-guided grasping for different types of tasks, (b) provides an
overviewofhuman-agent-objectinteraction,(c)showsGoogleScholarsearchresultsfortopicsof“Language-guidedGrasping”.
learning. Subsequently, in the second phase, the navigation The A-EQA task extends EM-EQA methods with frontier-
architecture is fine-tuned using policy gradients. Some sub- based exploration (FBE) [258] for problem-independent en-
sequent works [255], [256] retain modules like the question vironment exploration. Some other works [248], [259] also
answering module proposed by Das et al. [241] and improve employfrontier-basedexplorationmethodtoidentifyareasfor
the model. Additionally, Wu et al. [256] proposed integrating subsequent exploration and to build semantic maps. They end
the navigation and QA modules into a unified SGD training the exploration early utilizing conformal prediction or image-
pipeline for joint training, thereby avoiding employing deep text matching to avoid over-exploration. Patel et al. [260]
reinforcement learning to simultaneously train the separately emphasize the question answering aspect of the task. They
trained navigation and question answering modules. leverage multiple LLM-based agents to explore the environ-
There are also some works that attempt to increase the mentandenablethemtoindependentlyanswerquestionswith
complexity and completeness of question answering tasks. “yes”or“no”answers.Theseindividualresponsesareutilized
From the perspective of task singularity, several works [242], to train a Central Answer Model, responsible for aggregating
[257]expandthetasktoincludemultipleobjectivesandmulti- the responses and generating robust answers.
agent, respectively, making it necessary for the model to
3) Metrics: Researchers primarily assess the model’s per-
store and integrate the information obtained by the agent’s
formance based on two key aspects: navigation and question
exploration through methods such as feature extraction and
answering. In the navigation aspect, many researchers adhere
scene reconstruction. Taking into account the interaction be-
to the approach introduced by Das et al [241]. and utilize in-
tween the agent and the dynamic environment, Gordon et al.
dicators like the distance to the target object upon completion
[244]introducetheHierarchicalInteractiveMemoryNetwork.
ofnavigation(d ),thechangeindistancetotargetfrominitial
Control alternates between the planner, responsible for task T
tofinal position(d )and thesmallestdistance tothetarget at
selection, and the low level controllers, which carry out task ∆
any point in the episode (d ) to evaluate the performance
execution. During this process, an Egocentric Spatial GRU min
of the model. They are tested at 10, 30, or 50 actions
(esGRU)isutilizedtostorespatialmemory,enablingtheagent
away from the target. There are also works that measure it
to navigate and provide answers. There is also a limitation
basedonindicatorssuchastrajectorylength,intersection-over-
in previous works where agents are unable to use external
union score for target object (IoU), etc. Regarding question
knowledge to answer complex questions and lack knowledge
answering, the evaluation mainly involves mean rank (MR)
of the explored parts of the scene. To address this, Tan et al.
of the ground-truth answer in the answer list and accuracy
[246] propose a framework that leverages the neural program
of the answers. Recently, Majumdar et al. [247] introduce
synthesis method and the table converted from the knowledge
the concept of an aggregate LLM-based correctness metric
and 3D scene graphs, allowing the action planner to access
(LLM-Match) to evaluate the accuracy of open-vocabulary
object-relatedinformation.Additionally,anapproachbasedon
answers. Additionally, they assess efficiency by incorporating
Monte Carlo Tree Search (MCTS) is used to determine the
the normalized length of the agent’s path as a weight for the
next location for the agent to move to.
correctness metric.
LLMs/VLMs Methods. In recent years, large language
models (LLMs) and visual language models (VLMs) have 4) Limitations: 1) Dataset: Constructing datasets requires
madecontinuousprogressanddemonstratedoutstandingcapa- substantial manpower and resources. Additionally, there are
bilitiesacrossvariousfields.Consequently,researchersattempt still few large-scale datasets, and the metrics for evaluating
to apply these models to solve embodied question answering model performance vary across different datasets, complicat-
tasks without any additional fine-tuning. ing the testing and comparison of performance, 2) Model:
Majumdar et al. [247] explore using LLMs and VLMs for Despite the advancements brought by LLMs, the performance
episodic memory EQA (EM-EQA) task and Active EQA (A- of these models still lags significantly behind human levels.
EQA) task. For EM-EQA task, they consider Blind LLMs, Future work may focus more on effectively storing environ-
Socratic LLMs with language descriptions of the episodic mental information explored by agents and guiding them to
memory, Socratic LLMs with descriptions of the constructed plan actions based on environmental memory and questions,
scene graph , and VLMs processing multiple scene frames. while also enhancing the interpretability of their actions.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 18
Dataset Year Type Modality GraspLabel Gripperfinger Objects Grasps Scenes Language
Cornell[265] 2011 Real RGB-D Rect. 2 240 8K Single ×
Jacquard[264] 2018 Sim RGB-D Rect. 2 11K 1.1M Single ×
6-DOFGraspNet[269] 2019 Sim 3D 6D 2 206 7.07M Single ×
ACRONYM[268] 2021 Sim 3D 6D 2 8872 17.7M Multi ×
MultiGripperGrasp[270] 2024 Sim 3D - 2-5 345 30.4M Single ×
OCID-Grasp[263] 2021 Real RGB-D Rect. 2 89 75K Multi √×
OCID-VLG[271] 2023 Real RGB-D,3D Rect. 2 89 75K Multi √
ReasoingGrasp[272] 2024 Real RGB-D 6D 2 64 99.3M Multi √
CapGrasp[274] 2024 Sim 3D - 5 1.8K 50K Single
TABLEX
EMBODIEDGRASPINGDATASETS.
B. Embodied Grasping which include image data, point clouds, or scene models.
While these datasets are useful for grasping models, they
Embodied interaction, in addition to engaging in question-
lack semantic information. To bridge this gap, these datasets
answering interactions with humans, can also involve per-
have been augmented or extended with semantic expressions
forming operations based on human instructions, such as
[271], [275], thereby linking language, vision, and grasping.
grasping and placing objects, thereby completing interactions
By incorporating semantic information, agents can better un-
among the robots, humans and objects. Embodied grasping
derstandandexecutegraspingtasks.Thisenhancementallows
requires comprehensive semantic understanding, scene per-
for the development of more sophisticated and semantically
ception, decision-making, and robust control planning. The
awaregraspingmodels,facilitatingmoreintuitiveandeffective
embodied grasping methods integrate traditional robotic kine-
interactionwiththeenvironment.TableXpresentsthedatasets
matic grasping with large models such as large language
described above, including traditional grasping datasets and
models (LLMs) [261] and vision-language foundation models
language-based grasping datasets.
[15], which enables agents to perform grasping tasks under
3) Language-guided grasping: The concept of Language-
multi-sensory perceptions, including visual active perception,
guided grasping [271], [272], [275], which has evolved from
languageunderstandingandreasoning.Figure12(b)illustrates
this integration, combines MLMs to provide agents with the
an overview of human-agent-object interaction, where the
capability of semantic scene reasoning. This allows the agent
agent accomplishes embodied grasping tasks.
to execute grasping operations based on implicit or explicit
1) Gripper: Thecurrentresearchfocusingraspingtechnol-
human instructions. Figure 12 (c) illustrates the publica-
ogyisontwo-fingerparallelgrippersandfive-fingerdexterous
tion trends in recent years on the topic of language-guided
hands. For two-finger parallel grippers, grasping postures are
grasping. With the advancement of LLMs, researchers have
generallycategorizedintotwotypes:4-DOFand6-DOF[262].
shown increasing interest in this topic. Currently, grasping
The 4-DOF grasp synthesis [263]–[265] defines the grasp
research is increasingly focused on open-world scenarios,
usingathree-dimensionalpositionandatop-downhandorien-
emphasizing the open-set generalization [276] methods. By
tation(yaw),commonlyreferredtoas“top-downgrasping”.In
leveraging the generalization capabilities of MLMs, robots
contrast,6-DOFgraspsynthesis[266]–[268]definesthegrasp
can perform grasping tasks in open-world environments with
posturethroughasix-dimensionalpositionandorientation.For
greater intelligence and efficiency.
five-fingerdexteroushandgrippers,theShadowHand,awidely
In language-guided grasping, semantics can originate from
used five-finger robotic dexterous hand, features 26 degrees
explicit instructions [276], [277] and implicit instructions
of freedom (DOF). This high dimensionality significantly
[272], [274]. Explicit instructions clearly specify the category
increasesthecomplexityofgeneratingeffectivegrasppostures
of the object to be grasped, such as a banana or an apple.
and planning execution trajectories.
Implicit instructions, however, require reasoning to identify
2) Datasets: Recently, a substantial number of grasping the object or a part of the object to be grasped, involving
datasets[264],[265],[268]–[270]havebeengenerated.These spatial reasoning and logical reasoning.
datasets typically contain annotated grasping data based on Spatial reasoning [271] refers to instructions that may
images (RGB, depth), point clouds, or 3D scenes. With include the spatial relationship of the object or part to be
the advent of multimodal large models and the application grasped, necessitating the inference of grasping posture based
of foundational language models to robotic grasping, there on the spatial relationships of objects within the scene. For
is an urgent need for datasets that include linguistic text. example,“Graspthekeyboardthatistotherightofthebrown
Consequently, existing datasets have been extended or re- kleenex box” involves understanding and inferring the spatial
constructed to create semantic-grasping datasets [271]–[274]. arrangement of objects.
These datasets are instrumental in studying grasping models Logicalreasoning[272],ontheotherhand,involvesinstruc-
grounded in language, enabling agents to develop a broad tionsthatmaycontainlogicalrelationshipsrequiringinference
understanding of semantics. todiscernhumanintentandsubsequentlygraspthetarget.For
Traditionalgraspingdatasetsencompassdataforbothsingle instance, “I am thirsty, can you give me something to drink?”
objects [265] and cluttered scenes [263], providing stable would prompt the agent to potentially hand over a glass of
grasp annotations (4-DOF or 6-DOF) that conform to kine- water or a bottle of a beverage. The agent must ensure that
matics for each object. These data can be collected from real the liquid does not spill during the handover, thus generating
desktop environments [265], typically including RGB, depth, a reasonable grasping posture.
and point cloud data, or from virtual environments [268], In both cases, the integration of semantic understandingIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 19
withspatialandlogicalreasoningenablestheagenttoperform VI. EMBODIEDAGENT
complex grasping tasks effectively and accurately. Figure 12
Anagentisdefinedasanautonomousentitycapableofper-
(a) depicts instances of various types of language-guided
ceiving its environment and acting to achieve specific objec-
grasping tasks described above.
tives.Initially,SymbolicAgents,rootedinsymbolicreasoning,
4) End-to-EndApproaches: CLIPORT[275]isalanguage- and Reactive Agents, known for their rapid responsiveness,
conditionedimitationlearningagentthatcombinesthevision- were widely utilized. However, these agents were limited
language pre-trained model CLIP with the Transporter Net in handling complex strategies under uncertainty. Learning-
to create an end-to-end dual-stream architecture for semantic based agents were subsequently developed to mitigate this
understanding and grasp generation. It is trained using a large limitation, yet they remained inadequate for large-scale real-
numberofexpertdemonstrationdatacollectedfromvirtualen- world problems. Recent advancements in MLMs have further
vironments,enablingtheagenttoperformsemanticallyguided expanded the application of agents to practical scenarios.
grasping.BasedontheOCIDdataset,CROG[271]proposesa When these MLM-based agents are embodied in physical
vision-language-grasping dataset and introduces a competitive entities, they can effectively transfer their problem-solving
end-to-end baseline. It leverages CLIP’s visual foundation capabilities from virtual space to the physical world, thereby
capabilities to learn grasp synthesis directly from image-text becoming Embodied Agents [279].
pairs. Reasoning Grasping [272] introduces the first reasoning To enable embodied agents to operate in the information-
grasping benchmark dataset based on the GraspNet-1 Bil- richandcomplexrealworld,theEmbodiedMultimodalFoun-
lion dataset and proposes an end-to-end reasoning grasping dation Model has been developed to provide these agents
model. The model integrates multimodal large language mod- with multimodal perception and reasoning capabilities. To
els (LLMs) with vision-based robotic grasping frameworks complete a task, embodied agents typically involves the fol-
to generate grasps based on semantics and vision. SemGrasp lowing process: 1) decomposing the abstract and complex
[274] is a method for semantic-based grasp generation that task into specific subtasks, which is referred to as high-level
incorporates semantic information into grasp representations Embodied Task Planning. 2) gradually implementing these
to generate dexterous hand grasp postures. It introduces a subtasks by effectively utilizing Embodied Perception and
discrete representation aligning grasp space with semantic Embodied Interaction models or leveraging the Foundation
space, enabling the generation of grasp postures according Model’s policy function, named low-level Embodied Action
to language instructions. To facilitate training, a large-scale Planning. It is worth noting that task planning involves think-
grasp-text alignment dataset CapGrasp is proposed. ing before acting, and is therefore typically considered in
cyber space. In contrast, action planning must account for
5) Modular Approaches: F3RM [276] seeks to elevate
effective interaction with the environment and feedback on
CLIP’s text-image priors into 3D space, using extracted fea-
this information to the task planner to adjust task planning.
turesforlanguagelocalizationfollowedbygraspgeneration.It
Thus,itiscrucialforembodiedagentstoalignandgeneralize
combines precise 3D geometry with rich semantics from 2D
their abilities from the cyber space to the physical world.
foundational models, utilizing features extracted from CLIP
to specify objects for manipulation through free-text natural
language. It demonstrates the ability to generalize to unseen A. Embodied Multimodal Foundation Model
expressionsandnewobjectcategories.GaussianGrasper[277] Embodied agents are required to recognize their environ-
utilizesa3DGaussianfieldtoachievelanguage-guidedgrasp- ment visually, understand instructions audibly, and compre-
ingtasks.Theproposedmethodologybeginswiththeconstruc- hend their own state to enable complex interactions and oper-
tion of a 3D Gaussian field, followed by feature distillation. ations. This demands a model that integrates multiple sensory
Subsequently, language-based localization is performed using modalities and natural language processing capabilities to
theextractedfeatures.Finally,graspposegenerationiscarried enhance the agent’s understanding and decision-making by
out based on a SOTA pre-trained grasping network [278]. It synthesizing diverse data types. Thus, the Embodied Mul-
integrates open-vocabulary semantics with precise geometry, timodal Foundation Model is emerging. Google DeepMind
enabling grasping based on language instructions. initiatedresearchintheRoboticsFoundationModelfieldeight
These approaches advance the field of language-guided years ago, continually exploring ways to scale models and
grasping by leveraging both end-to-end and modular frame- data more effectively. Their findings revealed that leveraging
works, thereby enhancing the ability of robotic agents to un- foundation models and large, diverse datasets is the optimal
derstand and execute complex grasping tasks through natural strategy. They developed a series of works based on the
language instructions. Embodied grasping allows robots to Robotic Transformer (RT) [12], offering substantial insights
interact with objects, thus improving their intelligence and for future research on embodied agents.
utility in home services and industrial manufacturing. How- Significantprogresshasbeenmadeinfoundationalrobotics
ever, existing embodied grasping methods have limitations, models, evolving from the initial approach in SayCan [280],
such as reliance on extensive data and poor generalization whichusedthreeseparatemodelsforplanning,affordance,and
to unseen data. Future research will focus on improving low-level policy. Q-Transformer [281] later unified affordance
the generality of agents, enabling robots to understand more and low-level policy, and PaLM-E [282] integrated planning
complex semantics, grasp a wider variety of unseen objects, and affordance. Then, RT-2 [283] achieved a breakthrough by
and complete intricate grasping tasks. consolidating all three functions into a single model, enablingIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 20
(a) (b) Visual Description Visual Representation
Take an apple sofa, desk, drawer, There are two sofas
to my room pillow, … in front and a door in
the left corner, …
Yes, madam chair, desk, drawer, …
sink, fridge, …
There is a laptop and
Task Planning two monitors on the
desk, drawer, potted desktop. There are no agent desk
Action Planning cop mlan pt u, tt er ra ,s bh e c da , n …, i dt ee sm ks to pp l ta oc te hd e …on the door bed …
Object List Image Caption Scene Graph Visual Token
(c)High-Level Task Planning Visual Context (d)Low-Level Action Planning Action
Better Performance Text Token & Sub-Task Sub-Task Perception
Prompt& Related Case & Visual Description Visual Token Action: Grounding
Target: Apple
LLM/VLM VLA Model Location: [2.45, 2.65]
LLM VLM Exploration
Action: Navigation
Replan Replan Location: [1.20, 3.10]
Embodiment
Grounding, VQA,
Step 1. go to the kitchen Step 1. go to the kitchen Grasping, Navigation, … Grasping
Step 2. find the apple Replan Step 2. find the fridge Sub-task Action: Pick up
Step 3. pick up the apple Step 3. open the fridge Location: [2.45, 2.65]
… … Embodiment Target: Apple
Fig.13. Theoverallarchitectureoftheembodiedagent.Itconsistsoftheembodiedmultimodalfoundationmodel,visualperceptionmodule,high-leveltask
planningmodule,andlow-levelactionplanningmodule.
jointscalingandpositivetransfer.Thisrepresentsasubstantial a novel model fine-tuning method called “up-training,” which
advancementinroboticsfoundationalmodels.RT-2introduced convertsquadraticcomputationalcomplexityintosimplelinear
the Vision-Language-Action (VLA) model, featuring “chain- complexity, significantly enhancing model efficiency. This
of-thought”reasoningabilitiesthatenablemulti-stepsemantic makes RT faster and more streamlined, with the potential
reasoning, such as selecting alternative tools or beverages in for large-scale adoption of RT technology. In addition to RT,
various contexts. Ultimately, RT-H[4] achieved an end-to-end the Mamba architecture is used for tasks requiring longer
robot transformer with action hierarchies, to reason about the sequences.RoboMamba[289]employsanefficientfine-tuning
task planning at a fine-grained level. strategycalledPolicyHead,whichmastersvariousoperational
To address the generalization limitations of embodied mod- skillswithminimalfine-tuningparameters(0.1%ofthemodel)
els, Google collaborated with 33 leading academic insti- and time (20 minutes). Furthermore, its inference speed is
tutions to create the comprehensive Open X-Embodiment seven times faster than existing robotic MLMs.
dataset [284], integrating 22 diverse data types. Using this Due to RT being based on generative models, it demon-
dataset, they trained the universal large model RT-X. The strates significant advantages in understanding abstract tasks
diverse, cross-entity training data enabled RT-1 and RT-2 to and high-level task planning, but it has shortcomings in low-
achieve superior performance, demonstrating better general- level action planning. One issue is that generative models
ization and new functionalities compared to models trained cannot precisely generate control action parameters, and an-
on domain-specific data. This has also promoted the par- otheristhegapbetweenhigh-leveltaskplanningandlow-level
ticipation of more open-source VLMs in the robotics com- actionplanning,whichpreventsagentsfromeffectivelyreplan-
munity, such as EmbodiedGPT [285] based on LLaVA and ning. To address this, Google proposed RT-Trajectory [290],
RoboFlamingo [286] based on Flamingo. Although Open X- whichcanautomaticallyaddrobottrajectories,providinglow-
Embodiment provides a vast array of datasets, constructing level, practical visual cues for the model to learn robot
datasets remains a challenge given the rapid evolution of em- control strategies and helping robots generalize better. These
bodied robotic platforms. To address this issue, AutoRT [287] technologies enable robots to make decisions faster, better
created a system for deploying robots in new environments understand their environments, and more effectively guide
to collect training data, leveraging LLMs to enhance learning themselvesincompletingtasks.Moreover,buildingontheRT-
capabilities through more comprehensive and diverse data. 2 framework, the Robot Transformer with Action Hierarchies
Additionally, transformer-based architectures face ineffi- (RT-H) introduced a hierarchical action framework, linking
ciency problems, especially since embodied models require high-level task descriptions with low-level robotic motions
long contexts that include information from vision, language, through intermediate linguistic actions [4]. This innovation
and embodied states, as well as memory related to the cur- improved cross-task data sharing and enhanced task perfor-
rently executed tasks. For instance, RT-2, despite its strong mance by 15%. RT-H also exhibits greater adaptability and
performance, has an inference frequency of only 1-3Hz. Sev- generalizationindynamicenvironments,excellingincomplex
eral efforts are being made to improve this, such as deploying multi-taskscenarios.Furthermore,theemergentcapabilitiesof
models on the edge through quantization and distillation, and VLAmodelsarecurrentlyconfinedtohigh-levelplanningand
usingtheMixtureofExperts(MoE)architecturetoutilizeonly affordance tasks related to VLMs. They cannot exhibit new
a subset of parameters during inference, resulting in faster skills in low-level physical interactions and are limited by the
inference speeds compared to dense models with the same skillcategoriesintheirdatasets.Physicalactionsoftenexhibit
number of parameters. Moreover, SARA-RT [288] employs clumsiness,suchasunstablegrippingorinaccurateplacement.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 21
Futureeffortsshouldaimtoincorporatereinforcementlearning generation, providing more logical action generation. And
intothetrainingframeworkoflargemodelstoenhancegener- some works utilize code as the reasoning medium instead of
alization. This would enable VLA models to autonomously natural language. The Chain of Code method is particularly
learn and optimize low-level physical interaction strategies suitable for embodied agents in reasoning and executing
in real-world environments, resulting in more dexterous and tasks, where task planning is generated as code based on
precise execution of various physical actions. theavailableAPIlibrary[308]–[310].Furthermore,multi-turn
reasoning can effectively correct potential hallucinations in
B. Embodied Task Planning task planning, a focus of many LLM-based agent studies. For
As previously discussed, a task “put an apple on a plate”, instance, Socratic Models [311] and Socratic Planner [312]
the task planner will divide it into sub-tasks “find the apple, use Socratic questioning to derive reliable planning.
pick the apple”, “find the plate”, “put down the apple”. However, if LLM is considered as the plan generator
Since how to find (navigation task) or pick/put down actions since LLM’s plan generation is based on token probability
(graspingtask)arenotwithinthescopeoftaskplanning.These distribution rather than logical inference, it cannot ensure
actions are typically predefined within simulators or executed that the plans it generates are logically correct. Therefore,
in real-world scenarios using pre-trained policy models, such some works consider LLMs as world models rather than plan
as using CLIPort [275] for grasping tasks. generators,usingMCTS[293]forplansequencesearch[313]–
Traditionalembodiedtaskplanningmethodsareoftenbased [315]. Another line of work considers LLMs as instruction
onexplicitrulesandlogicalreasoning.Forexample,symbolic translators, leveraging their powerful semantic understanding
planning algorithms such as STRIPS [291] and PDDL [292], capabilities to translate plans into planning-specific languages
and search algorithms like MCTS [293] and A* [294], are such as PDDL, than leveraging traditional AI planners for
used to generate plans. However, these methods often rely planning [316]–[319].
on predefined rules, constraints, and heuristics that are rigid During task planning, potential failures that may occur
and may not adapt well to dynamic or unforeseen changes during execution must be considered. These failures often
in the environment. With the popularity of LLMs trained on resultfromtheplannernotfullyaccountingforthecomplexity
large-scale data, many researchers have recently attempted to of the real environment and the difficulty of task execu-
use LLMs for planning or to combine traditional methods tion [299], [302]. Multi-agent collaboration framework ReAd
with LLMs, leveraging the rich world knowledge embedded [320] is proposed for efficient self-refinement of plans. Due
within them for reasoning and planning without the need to a lack of visual information, planned subtasks may deviate
for handcrafted definitions, greatly enhancing the model’s from the actual scenario, leading to task failure. Therefore,
generalization capabilities. integrating visual information into planning or replanning
1) Planning utilizing the Emergent Capabilities of LLMs: during execution is necessary. This approach can significantly
Before the scale-up of natural language models, task planners enhance the accuracy and feasibility of task planning, better
were similarly implemented by training models like BERT addressing the challenges of real-world environments.
on embodied instruction datasets such as Alfred [295] and 2) Planning utilizing the visual information from embodied
Alfworld [296], as demonstrated by FILM [297]. However, perception model: Based on the above discussion, it is par-
this approach was limited by the examples in the training set ticularly important to further integrate visual information into
andcouldnoteffectivelyalignwiththephysicalworld.Nowa- task planning (or replanning). In this process, object labels,
days, thanks to the emergent capabilities of large language locations, or descriptions provided by visual input can offer
models (LLMs), LLMs can decompose abstract tasks using critical references for task decomposition and execution by
their internal world knowledge and chain-of-thought reason- large language models (LLMs). For instance, through visual
ing, similar to how humans reason through task completion information,LLMscanmoreaccuratelyidentifytargetobjects
steps before acting. For example, Translated LM [298] and and obstacles in the current environment, thereby optimizing
Inner Monologue [299] can break down complex tasks into task steps or modifying subtask objectives. Some works use
manageable steps and devise solutions using their internal an object detector to query the objects present in the en-
logic and knowledge systems without additional training. vironment during task execution and feed this information
To increase the success rate of these solutions, some meth- back to the LLM, allowing it to modify unreasonable steps
ods provide more reference examples as context, such as in the current plan [302], [311], [321]. RoboGPT considers
pre-including a successful plan in the prompt [300]. The the different names of similar objects within the same task,
semanticrelevanceoftheseexamplesalsoaffectstaskplanning further improving the feasibility of replanning [322]. How-
accuracy [301]. Therefore, methods like LLM-Planner have ever, the information provided by labels is still too limited.
been developed to retrieve the most appropriate examples as Can further scene information be provided? SayPlan [323]
context using KNN to find task-type similar and semantically proposes using hierarchical 3D scene graphs to represent the
relevant examples [302]. Additionally, some approaches ab- environment,effectivelymitigatingthechallengesoftaskplan-
stract past successful examples into a series of skills stored ning in large, multi-floor, and multi-room settings. Similarly,
in a memory bank to consider during inference and improve ConceptGraphs [324] also adopts 3D scene graphs to provide
planning success rates [303]–[305] . Interestingly, improving environmental information to LLMs. Compared to SayPlan, it
chain-of-thought (CoT) reasoning [306] is also a viable ap- offers more detailed open-world object detection and presents
proach. ReAct [307] incorporates CoT reasoning with plan task planning in a code-based format, which is more efficientIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 22
and better suited to the demands of complex tasks. to the goal, thereby improving planning effectiveness.
However,limitedvisualinformationmayleadtoinsufficient However, task planning is only the first step for an agent
environmental understanding by the agent. Although visual in completing an instruction task; subsequent action planning
input can provide important environmental cues, its limitation determines whether the task can be accomplished. In the ex-
lies in the inability to capture the complexity and dynamic perimentsfromRoboGPT[322],theaccuracyoftaskplanning
changes of the environment fully. Without comprehensive reached 96%, but the overall task completion rate was only
visual information, the agent may misinterpret the environ- 60%, limited by the performance of the low-level planner.
mental conditions, resulting in task failure. For instance, if Therefore, whether an embodied agent can transition from
a towel is locked in the bathroom cabinet, the agent may the cyber space of “imagining how tasks are completed” to
repeatedly search the bathroom without realizing this possi- the physical world of “interacting with the environment and
bility [322]. Therefore, it is necessary to develop more robust completing tasks” hinges on effective action planning.
algorithms that integrate multiple sensory data to compensate
for the shortcomings of visual information. By incorporating
C. Embodied Action Planning
multimodal perception technologies, such as vision, touch,
and hearing, the agent’s understanding of the environment Section VI-B discusses the definitions and differences be-
can be enhanced. Additionally, leveraging historical data and tween task planning and action planning. It is evident that
contextualreasoningcanassisttheagentinmakingreasonable action planning must address real-world uncertainties because
judgmentsanddecisionsevenwithlimitedvisualinformation. the granularity of subtasks providedby task planning is insuf-
This planning approach, combining multimodal fusion and ficient to guide agents in efficient environmental interaction.
contextual reasoning, not only increases the success rate of Generally, agents can approach action planning in two ways:
task execution but also offers new perspectives for the further first, by using pre-trained embodied perception models and
development of embodied AI. embodied intervention models as tools, incrementally com-
3) Planning utilizing the VLMs: Compared to converting pleting the subtasks specified by task planning through APIs;
environmentalinformationintotextusingexternalvisualmod- second, by utilizing the VLA model’s inherent capabilities to
els, VLM models can capture visual details in latent space, derive action planning. Furthermore, the results of the action
particularlycontextualinformationthatisdifficulttorepresent planner’s execution are fed back to the task planner to adjust
with object labels, as per Lecun’s insights on world models. and improve task planning.
VLMs can discern rules underlying visual phenomena; for 1) Action utilizing APIs: A typical approach involves pro-
instance, even if a towel is not visible in the environment, it viding LLMs with the definitions and descriptions of various
canbeinferredthatthetowelmightbestoredinacabinet.This well-trainedpolicymodelsascontext,enablingthemtounder-
process essentially demonstrates how abstract visual features standthesetoolsanddeterminehowandwhentoinvokethem
andstructuredtextualfeaturescanbemoreeffectivelyaligned for specific tasks [280], [302]. Additionally, by generating
in latent space. code, a series of more granular tools can be abstracted into a
In the EmbodiedGPT framework, the Embodied-Former function library for invocation rather than directly passing the
module aligns embodied, visual, and textual information, parameters needed for sub-tasks to navigation and grasping
effectively considering the agent’s state and environmental models [310]. Given the uncertainty of the environment,
information during task planning [325]. Similarly, the EIF- Reflexion can further adjust these tools during execution to
Unknow model utilizes Semantic Feature Maps extracted achievebettergeneralization[329].Optimizingthesetoolscan
from Voxel Features as visual tokens, which are input along enhance the robustness of the agent, and new tools may be
with text tokens into a trained LLaVA model for task plan- requiredtocompleteunknowntasks.DEPS,underthepremise
ning [326]. Furthermore, embodied multimodal foundation ofzero-shotlearning,endowsLLMswithvariousrolesettings
models, or VLA models, have been extensively trained with to learn diverse skills while interacting with the environment.
large datasets in studies like the RT series [12], [283] and During subsequent interactions, LLMs can learn to select and
PaLM-E [282] to achieve alignment of visual and textual combine these skills to develop new ones [330].
features in embodied scenarios. Additionally, innovative re- This hierarchical planning paradigm allows agents to fo-
search such as Matcha [327] explores the integration of cus more on high-level task planning and decision-making
large language models (LLMs) with multimodal perception while delegating specific action execution to policy models,
modules(e.g.,weight,touch,sound)toenhancerobotplanning simplifying the agent’s development process. Additionally,
capabilities. For example, when executing the task ”pick up this modularization of task planners and action planners
a plastic block,” the robot can assess its weight by lifting enables independent development, testing, and optimization,
it, determine if the sound it makes when tapped matches thereby enhancing system flexibility and maintainability. This
expected plastic sounds, or use tactile feedback to sense the approach allows agents to adapt to various tasks and environ-
block’s hardness or softness. Moreover, the VLP [328] model ments by invoking different action planners and facilitating
enhances planning by considering historical information and modificationstotheactionplannerwithoutsignificantchanges
predicting future visual states. Once an LLM generates an to the agent’s structure. However, invoking external policy
action, the video model can simulate multiple potential video models may introduce latency, particularly in real-time tasks,
representationsforeachaction.Thesevideomodelsthenserve potentiallyaffectingsystemresponsespeedandefficiency.The
as heuristic functions to evaluate the proximity of each action most critical issue to address is that the agent’s performanceIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 23
internet datasets to achieve high-level emergent capabilities
(a) Generation-based Methods (b) Prediction-based Methods (c) Knowledge-driven Methods
and then co-finetuned with real world robot data. In contrast,
k
Optional worldmodelsaretrainedfromscratchonphysicalworlddata,
Decoder L Predictor L /P Dre ed ci oc dto er r L gradually developing high-level capabilities as the amount of
Encoder α Encoder α Encoder Encoder α Encoder dataincreases.However,theyremainlow-levelphysicalworld
models, somewhat akin to the mechanism of human neural
x Transformation y x Transformation y x Transformation y
reflex systems. This makes them more suitable for scenarios
Fig. 14. Embodied world models can be roughly divided into three type. where both inputs and outputs are relatively structured, such
(a)Generation-basedMethodstraintolearnthetransformationrelationship
as autonomous driving (input: vision, output: throttle, brake,
betweentheinputspaceandtheoutputspaceusinganautoencoderframework.
(b) Prediction-based Methods can be seen as a more general framework steering wheel) or object sorting (input: vision, instructions,
where a world model is trained in latent space. (c) Knowledge-driven numerical sensors, output: grasping the target object and
Methodsinjectartificiallyconstructedknowledgeintothemodel,givingthe
placing it in the target location). They are less suited for
model world knowledge to obtain output that meets the given knowledge
constraints.Notethatthecomponentswithinthedashedlineareoptional. generalization to unstructured, complex embodied tasks.
Learning world models is promising of the physical simu-
is highly dependent on the quality of the policy model. If the
lation field. Compared to traditional simulation methods, it
policy model fails to solve the tasks effectively, the overall
offers significant advantages, such as the ability to reason
performance of the agent will be compromised.
aboutinteractionswithincompleteinformation,meetreal-time
2) ActionutilizingVLAmodel: Thisparadigmleveragesthe
computation requirements, and improve prediction accuracy
capabilities of embodied multimodal foundation models for
over time. The predictive capability of such world models
planning and executing actions, unlike the previous approach,
is crucial, enabling robots to develop the physical intuition
wheretaskplanningandactionexecutionareperformedwithin
necessary to operate in the human world. As shown in Fig.
thesamesystem,reducingcommunicationlatencyandimprov-
14, according to the learning pipeline of the world environ-
ingsystemresponsespeedandefficiency.InVLAmodels,the
ment, they can be divided into Generation-based Methods,
tightintegrationofperception,decision-making,andexecution
Prediction-based Methods and Knowledge-driven Methods.
modules allows the system to handle complex tasks and adapt
We briefly summarize the methods mentioned in Table XI.
to changes in dynamic environments more efficiently. This in-
1) Generation-based Methods: As the scale of models and
tegrationalsofacilitatesreal-timefeedback,enablingtheagent
data progressively increases, generative models have demon-
to self-adjust strategies, thereby enhancing the robustness and
strated the ability to understand and generate images (e.g.,
adaptability of task execution [3], [285], [331]. However, this
World Models [332]), videos (e.g., Sora [18], Pandora [333]),
paradigmisundoubtedlymorecomplexandcostly,particularly
point clouds (e.g., 3D-VLA [334]) or other formats of data
when dealing with intricate or long-term tasks. Additionally,
(e.g., DWM [335]) that conform to physical laws. This ability
a key issue is that an action planner, without an embodied
indicates that generative models can learn and internalize
world model, cannot simulate physical laws using only the
world knowledge. Specifically, after being exposed to vast
internal knowledge of an LLM. This limitation hinders the
amounts of data, generative models can not only capture the
agent to accurately and effectively complete various tasks in
statistical properties of the data but also simulate the physical
thephysicalworld,preventingtheseamlesstransferfromcyber
and causal relations of the real world through their intrinsic
space to physical world.
structuresandmechanisms.Therefore,thesegenerativemodels
can be considered more than simple pattern recognition tools:
VII. SIM-TO-REALADAPTATION
theyexhibitcharacteristicsofworldmodels.Consequently,the
Sim-to-RealadaptationinEmbodiedAIreferstotheprocess
worldknowledgeembeddedingenerativemodelscanbelever-
of transferring capabilities or behaviors learned in simulated
aged to enhance the performance of other models. By mining
environments (cyber space) to real-world scenarios (physical
and utilizing the world knowledge represented in generative
world). It involves validating and improving the effectiveness
models, we can improve model generalization and robustness.
ofalgorithms,models,andcontrolstrategiesdevelopedinsim-
This approach not only enhances the model’s adaptability to
ulationtoensuretheyperformrobustlyandreliablyinphysical
new environments but also increases its predictive accuracy
environments. To achieve sim-to-real adaptation, embodied
on unknown data [333], [334]. However, generative models
world models, data collection and training methods, and
also have certain limitations and drawbacks. For instance,
embodied control algorithms are three essential components.
when data distribution is significantly biased or training data
is insufficient, generative models may produce inaccurate or
A. Embodied World Model
distorted outputs. Additionally, the training process for these
Sim-to-Real involves creating world models in simulation models typically requires substantial computational resources
that closely resemble real-world environments, helping algo- and time, and the models often lack interpretability, which
rithms generalize better when transferred. The world model complicates their practical application. Overall, while gen-
approach aims to build an end-to-end model that maps vision erative models have shown great potential in understanding
to action, or even anything to anything, by predicting the next andgeneratingcontentthatconformstophysicallaws,several
state in a generative or predictive manner to make decisions. technical and practical challenges must be addressed for
The biggest difference between such world models and VLA theireffectiveapplication.Thesechallengesincludeimproving
models is that VLA models are first trained on large-scale model efficiency, enhancing interpretability, and addressingIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 24
Type Method Years Maintasks
WorldModels[332] 2018 CarRacing
Sora[18] 2024 VideoGeneration
Generation-based Pandora[333] 2024 Real-timeControllableVideoGeneration
3D-VLA[334] 2024 EmbodiedReasoningandLocalization,MultimodalGoalGeneration,RobotPlanning
DWM[335] 2024 D4RLOfflineRL
I-JEPA[17] 2023 VisualRepresentationLearning,ImageClassification,ObjectCounting,DepthPrediction
MC-JEPA[336] 2023 VisualRepresentationLearning,OpticalFlowEstimation,InstanceSegmentation,VideoSegmentation
A-JEPA[337] 2023 AudioRepresentationLearning,AudioandSpeechClassification
Prediction-based IWM[338] 2024 VisualRepresentationLearning,ImageClassification,ImageSegmentation
iVideoGPT[339] 2024 VideoPrediction,VisualPlanning,VisualModel-basedRL
STP[340] 2024 RoboticMotorControl
MuDreamer[341] 2024 DeepMindVisualControlSuite,NaturalBackgroundSetting
Lioneletal.[342] 2023 ProbabilisticReasoning,RelationalReasoning,PerceptualandPhysicalReasoning,SocialReasoning
ElastoGen[343] 2024 4DElastodynamicsGeneration
Knowledge-driven Liuetal.[344] 2024 Single-image3DReconstruction
Holodeck[72] 2024 3DEnvironmentsGeneration
LEGENT[345]) 2024 3DEnvironmentsGeneration
TABLEXI
SUMMARYOFTHEEMBODIEDWORLDMETHODSDISCUSSEDINVII-A.
issues related to data bias. With ongoing research and devel- els, endowing them with world knowledge. This method has
opment, generative models are expected to demonstrate even shownbroadapplicationpotentialinthefieldofembodiedAI.
greater value and potential in future applications. For example, in the real2sim2real approach [349], real-world
2) Prediction-based Methods: The prediction-based world knowledge is used to build physics-compliant simulators,
modelpredictsandunderstandstheenvironmentbyconstruct- which are then used to train robots, enhancing model robust-
ing and utilizing internal representations. By reconstructing ness and generalization capabilities. Additionally, artificially
corresponding features in the latent space based on provided constructing common sense or physics-compliant knowledge
conditions, it captures deeper semantics and associated world and applying them to generative models or simulators is a
knowledge. This model maps input information to a latent common strategy (e.g., ElastoGen [343], Liu et al. [344],
space and operates within that space to extract and utilize Lionel et al. [342]). This approach imposes more physically
high-levelsemanticinformation,therebyenablingtherobotsto accurate constraints on the model, enhancing its reliability
perceive the essential representation of the world environment and interpretability in generation tasks. These constraints
(e.g.,I-JEPA[17],MC-JEPA[336],A-JEPA[337],Point-JEPA ensurethemodel’sknowledgeisbothaccurateandconsistent,
[346], IWM [338]) and more accurately perform embodied reducing uncertainty during training and application. Some
downstream tasks (e.g., iVideoGPT [339]), STP [340], Mu- approaches combine artificially created physical rules with
Dreamer[341]).Comparedtopixel-levelinformation,features large language models (LLMs) or large multimodal models
in the latent space can abstract and decouple various forms (LMMs). By leveraging the commonsense capabilities of
of knowledge, allowing the model to handle complex tasks LLMs and LMMs, these approaches (e.g., Holodeck [72],
and scenes more effectively and improve its generalization LEGENT[345])generatediverseandsemanticallyrichscenes
capability [347]. For instance, in spatiotemporal modeling, through automatic spatial layout optimization. This greatly
the world model needs to predict the post-interaction state advancesthedevelopmentofgeneral-purposeembodiedagents
of an object based on its current state and the nature of by training them in novel and diverse environments.
the interaction, combining this information with its internal
knowledge. Specifically, an embodied world model generates
B. Data Collection and Training
dynamicpredictionsoftheenvironmentbyintegratingpercep-
tualinformationandpriorknowledge.Thisapproachreliesnot For sim-to-real adaptation, the high-quality data is impor-
only on sensory data but also on inherent world knowledge to tant. Traditional data collection methods involve expensive
infer and predict environmental changes, thereby producing equipment, precise operations, and are time-consuming and
more accurate spatiotemporal predictions [339]–[341]. This labor-intensive, often lacking flexibility. Recently, some effi-
process considers both the current state of objects and their cientandcost-effectivemethodshavebeenproposedforhigh-
historical data and contextual information. qualitydemonstrationdatacollectionandtraining.Thissection
Similarly, leveraging the world knowledge embedded in its will discuss various methods for data collection in both real-
representations can further enhance the model’s perception world and simulated environments. Fig. 15 presents demon-
and robustness [17], [336], [338], [348]. By operating in strationdatafrombothreal-worldandsimulatedenvironments.
latent space, it is expected that robots can maintain high
performance in different environments at a lower cost [341]. 1) Real-World Data: Training large, high-capacity models
Thekeytothisapproachliesinabstractprocessingandknowl- on high-volume, rich datasets has demonstrated remarkable
edge decoupling, enabling efficient adaptation to complex capabilities and significant success in effectively addressing
situations. However, such models may exhibit limitations and downstream applications. For instance, large language models
instability when dealing with previously unseen environments such as ChatGPT, GPT-4, and LLaMA have not only excelled
and conditions. Additionally, the world knowledge decoupled in the field of NLP but have also provided excellent problem-
in the latent space may have interpretability issues. solving capabilities for downstream tasks. Therefore, is it
3) Knowledge-driven Methods: Knowledge-driven world possibletotrainanembodiedlargemodelintheroboticsfield,
models inject artificially constructed knowledge into the mod- one that possesses strong generalization capabilities throughIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 25
Real-World Data Simulated Data
Robot Demonstration Data Human Demonstration Data Expert Demonstration Data Labeled Data
Open the cabinet. Cut bell pepper. Put the blue blocks in a green bowl. Open the door.
Use only two fingers to gripping
Put the carrots on a plate. Wash glass. Heat the soup using microwave. the knife‘s handle.
Data Format
Video Point Cloud RGB + Depth Text Voice Tactile Simulation
Fig.15. Theillustrationofdemonstrationdatacollection.TheyellowboxontheleftfeaturesoperationaldemonstrationsoftheFrankaandWidowXrobotic
arms,whilehumandemonstrationsareshowninthebluebox.Ontheright,theyellowboxshowcasesoperationalscenariosoftheUR5eandFrankarobotic
armsinsimulationenvironments,andtheblueboxdisplayslabeledsimulationdata.Theyellowboxatthebottompresentsthedataformatsofthesedatasets.
trainingandcanadapttonewscenariosandrobotictasks?This 2) Simulated Data: The aforementioned data collection
requires a large volume of embodied datasets to provide data methods involve directly collecting demonstration data in the
for model training. Open X-Embodiment [284]: A collabora- real world for agent training. Such collection methods often
tion of 21 institutions collected an embodied dataset from 22 require significant manpower, material resources, and time,
differentrobots,showcasing527skillsand160,266tasks.The leading to inefficiency. Therefore, in most cases, researchers
data they collected consisted of authentic demonstration data can choose to collect datasets in simulation environments for
from robots, obtained by recording the process of executing model training. Collecting data in simulation environments
operations. This primarily focused on domestic and kitchen does not require extensive resources and can generally be
settings,involvingitemssuchasfurniture,food,andtableware. automatedbyprograms,savingalotoftime.CLIPORT[275],
The operations mainly centered around pick-and-place tasks, Transporter Networks [353]: They collected demonstration
with a small portion involving more complex maneuvers. The datafromthePybulletsimulatorforend-to-endnetworkmodel
high-capacitymodelRT-Xtrainedonthisdatasetdemonstrated training and successfully transferred the models from simu-
excellent transfer capabilities. UMI [350]: proposed a data lation to real world. GAPartNet [354]: constructed a large-
collection and policy learning framework. They designed a scalepart-centricinteractivedatasetGAPartNet,providingrich
handheld gripper and an elegant interface for data collection, part-level annotations for perception and interaction tasks.
enabling portable, low-cost, and information-rich data col- They proposed a pipeline for domain-generalized 3D part
lection for challenging bimanual and dynamic demonstration segmentation and pose estimation, which can generalize well
data. By simply modifying the training data, the robot can to unseen object categories in both simulators and the real
achieve zero-shot generalizable, bimanual, and precise tasks. world. SemGrasp [274]: built a large-scale grasping text-
Mobile ALOHA [351]: proposed Mobile ALOHA, a low-cost aligned dataset CapGrasp, a semantically rich dexterous hand
full-bodymobilemanipulationsystem.Itcanbeusedtocollect grasping dataset from virtual environments.
task data for bimanual operations under full-body mobility,
3) Sim2Real Paradigms: Recently, several sim2real
suchasfryingshrimpandservingdishes.Trainingagentswith
paradigms have been introduced. These methods aim to
data collected by this system and static ALOHA can improve
mitigate the need for extensive and costly real-world
the performance of mobile manipulation tasks. Such agents
demonstration data by conducting extensive learning in
can serve as home assistants or work assistants. In human-
simulation environments firstly, followed by migration to
agent collaboration [352], humans and agents learn together
real-world settings. This section outlines five paradigms for
duringdatacollection,reducinghumanworkload,accelerating
sim2real transfer, as shown in Fig. 16.
data acquisition, and improving data quality. Specifically, in
Real2Sim2Real[355]isanapproachthatenhancesimitation
anembodiedscenario,during datacollection,humansprovide
learning in real-world scenarios by leveraging reinforcement
initial action inputs. Subsequently, the agent refines these
learning trained in a “digital twin” simulation environment.
actionsthroughiterativeperturbationanddenoisingprocesses,
The method involves strengthening strategies through exten-
gradually optimizing them to produce precise, high-quality
sive RL within the simulation, followed by transferring these
operational demonstrations. This entire process can be sum-
strategiestotherealworldtoaddressdatascarcityandachieve
marized as follows: humans contribute intuition and diversity
effective robotic operational imitation learning. Initially, ex-
in operations, while agents handle optimization and stability,
isting methods such as Nerf and VR are used for scene
reducing reliance on operators, enabling execution of more
scanning and reconstruction, and the constructed scene assets
complex tasks, and gathering higher-quality data.
are imported into the simulator to achieve real-to-simulation
fidelity. Subsequently, RL in the simulation fine-tunes theIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 26
Real2Sim2Real TRANSIC
1.Real2Sim 2.Simulation fine-tuning 3.Sim2Real transfer
1.Simulation 2.Direct 3.Human-in-the- 4.Successful Transfer
Policy Deployment Loop Correction with Learned Residual
Scan Reconstruct
Policy
Lang4Sim2Real
Domain Randomization
1.Train in the simulator 2.Test in the real world 1.Image Language Pretraining 2.Imitation Learning Training
Task Instruction
LLM
sim
Random Scenario Configuration Without Additional Training gripper holding
bread over square
System Identification
1.Train in the simulator 2.Test in the real world Image Encoder
real
gripper holding
carrot over yellow
More Realistic Simulation Environment Sim2real Transfer Easily mat
Fig.16. Fivepipelinestoachievesim2realgap.“Sim2Real2Sim”reducesthegapbyreconstructingrealscenes.“TRANSIC”compensatesforthesim2real
transfergapthroughhuman-correctedinterventions.“Domain Randomization”enhancesmodeltransferadaptabilitybysimulatingenvironmentaldiversity.
“SystemIdentification”improvessim-to-realenvironmentsimilarity,therebymitigatingthesim-realgap.“Lang4Sim2Real”usesnaturallanguagetobridge
twodomains,learninginvariantimagerepresentationsandreducingvisualgaps.
initial strategies derived from sparse expert demonstrations simulation and real-world environments, specifically by us-
collected in the real world. Finally, the refined strategies ing textual descriptions of images as a cross-domain unified
are transferred to real-world settings. TRANSIC [356] is signal. This approach aids in learning domain-invariant image
a novel idea that narrows the sim-to-real gap by enabling representations,therebyimprovinggeneralizationperformance
real-time human intervention to correct robot behaviors in across simulation and real environments. Initially, an encoder
real-world scenes. The method enhances sim-to-real transfer is pretrained on image data annotated with cross-domain lan-
performance through a series of steps: initially, robots are guage descriptions. Subsequently, using the learned domain-
trained using RL to establish foundational strategies within invariantrepresentations,amulti-domain,multi-tasklanguage-
a simulation environment. Subsequently, these strategies are conditioned behavioral cloning policy is trained. This method
implemented on real robots, with humans intervening and compensates for the scarcity of real-world data by leveraging
correcting behaviors in real-time via remote control when additional information from abundant and inexpensive simu-
errors occur. The data collected from these interventions lated data, thereby enhancing sim-to-real transfer.
are used to train a residual policy. Finally, integrating both
foundational and residual policies ensures smoother trajecto-
C. Embodied Control
ries in real-world applications following sim-to-real transfer.
This approach significantly reduces the need for real-world Embodied control aims to enable robots to acquire new
data collection, thereby mitigating the burden while achieving skillsthroughinteractionandlearningfromtheirenvironment,
sim2real. Domain Randomization [357]–[359] enhances the thereby adapting to and completing complex tasks. Embodied
generalization of models trained in simulated environments to control learns through interaction with the environment and
real-world scenarios by introducing parameter randomization optimizes behavior using a reward mechanism to obtain the
duringsimulation.Whilebothsimulatedandrealenvironments optimal policy, thereby avoiding the drawbacks of traditional
involve perception through camera-acquired visual images, physical modeling methods. Embodied control methods can
differences such as object friction and gloss make accurate be divided into two types:
simulationchallenging.Therefore,byrandomizingparameters 1) Deep Reinforcement Learning (DRL). DRL can han-
during simulation training, a wide range of conditions can be dle high-dimensional data and learn complex behavior pat-
covered, potentially encompassing variations that might occur terns, making it suitable for decision-making and control.
in real-world settings. This approach boosts the robustness The hybrid and dynamic policy gradient (HDPG) [363] is
of trained models, enabling direct deployment from simula- proposed for biped locomotion, allowing the control policy to
tion to real environments. System Identification [360], [361] be simultaneously optimized by multiple criteria dynamically.
constructs an accurate mathematical model of physical scenes DeepGait [364] is a neural network policies for terrain-aware
in real-world environments, encompassing parameters such locomotion,whichcombinesmethodsformodel-basedmotion
as dynamics and visual rendering. This effort aims to make planning and reinforcement learning. It includes a terrain-
simulation environments closely resemble real-world settings, aware planner for generating gait sequences and base motions
facilitatingsmoothtransitionsofmodelsandstrategiestrained guiding the robot towards target directions, along with a gait
insimulationtorealenvironments.Lang4Sim2Real[362]uses and base motion controller for executing these sequences
natural language as a bridge to address the gap between whilemaintainingbalance.BoththeplannerandcontrollerareHumanoid Robot
Humanoid Robot
Visual Whole-Body Control (Picking up the ball)
ANYmal Robot Motion（Walking, Trotting, Stair Climbing） Atlas Robot :Full Body Control (Stair Climbing)
IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 27
Expressive Whole-Body Control (Waving hello, High five)
Embodied Control on the control of humanoid robots, enabling them to perform
various actions like humans and mimic human behaviors. Fig
Bipedal & Quadruped Robot Humanoid Robot
.17 illustrates some examples the robotic control.
Embodied control incorporates RL and sim2real techniques
to optimize strategies through interaction with the environ-
Pick up the ball ment, explore unknown domains, potentially surpass human
capabilities, and adapt to unstructured environments. While
robots can mimic many human behaviors, tasks often cannot
Climb Wave
be effectively completed without RL training based on en-
vironmental feedback. The most challenging scenes include
Walk
contact-intensive tasks where manipulation requires real-time
adjustments based on feedback such as the state, deformation,
material, and force of manipulated objects, tasks where only
Walk High Five Leap
RLprovesadequate.IntheeraofMLMs,largemodelspossess
a generalized understanding of scene semantics, which can
Fig. 17. Examples of embodied control for various locomotion modes,
demonstrating the robots’ agile movement and interaction capabilities. The provide excellent reward functions for RL. Moreover, RL
orange box on the left showcases the locomotion of two quadruped robots is also important for aligning large models. In the future,
(Unitree B1, ANYmal) and one biped robot (ATRIAS). The red box on the
embodied agents, after pre-training and fine-tuning, still need
rightdisplays motionscenesof theAtlas Robotclimbingstairs, UnitreeH1
wavingandhigh-fiving,andtheBostonrobotleaping. RL to align with the physical world in order to better deploy
themselves in real-world.
parameterized using neural network function approximators
and optimized using deep reinforcement learning algorithms. VIII. CHALLENGESANDFUTUREDIRECTIONS
ATRIAS （Walking）2) Imitation Learning. The DRL has its drawbacks of Despite of the rapid progress of embodied AI, it faces
requiring a large amount of data from numerous trials. To several challenges and presents exciting future directions.
address this issue, imitation learning was introduced, which High-quality Robotic Datasets: Obtaining sufficient real
aimstominimizedatausagebycollectinghigh-qualitydemon- world robotic data remains a significant challenge. Collecting
strations. To improve data efficiency, Offline RL + Online RL thisdataisbothtime-consumingandresource-intensive.Rely-
was proposed to reduce interaction costs and ensure safety. ingsolelyonsimulationdataworsensthesim-to-realgapprob-
This method first employs offline RL to learn policies from lem. Creating diverse real world robotic datasets necessitates
static, pre-collected large datasets. These policies are then close and extensive collaboration among various institutions.
deployed in the real environment for real-time interaction Additionally, the development of more realistic and efficient
and exploration, with adjustments made based on feedback. simulators is essential for improving the quality of simulated
The representative imitation learning methods from human data. Current work RT-1 [12] used pre-trained models based
demonstrationsareALOHA[365]andMobileALOHA[351]. on robot images and natural language commands. RT-1 has
Although embodied AI encompasses high-level algorithms, achieved good results in navigation and grasping tasks, but
models, and planning modules, its most fundamental and acquiring real world robot datasets is very challenging. For
essential component is embodied control. Therefore, it is building generalizable embodied models capable of cross-
imperative to consider how to control physical entities and scenario and cross-task applications in robotics, it is essen-
endow them with physical intelligence. Embodied control is tial to construct large-scale datasets, leveraging high-quality
closely related to hardware, such as controlling joint move- simulated environment data to assist real world data.
ments,end-effectorpositions,andwalkingspeeds.Forrobotic Efficient Utilization of Human Demonstration Data:
arms, knowing the end-effector’s position, how to plan joint Efficient utilization of human demonstration data involves
trajectories to move the arm to the target? For humanoid leveraging the actions and behaviors demonstrated by humans
robots, knowing the motion patterns, how to control the joints to train and improve robotic systems. This process includes
to achieve the target posture? These are critical issues that collecting, processing, and learning from large-scale, high-
need to be addressed in control. Several works focus on quality datasets where humans perform tasks that robots
robotic control, enhancing the flexibility of robotic actions. are intended to learn. Current work R3M [379] used action
[366] proposed a vision-based full-body control framework. labels and human demonstration data to learn generalizable
By connecting a robotic arm and a robotic dog, utilizing all representations has shown high success rates in some robot
degrees of freedom (12 joints in the legs, 6 joints in the arm, grasping tasks, but the efficiency for complex tasks still needs
and 1 in the gripper), it tracks the robot dog’s speed and the improvement. Therefore, it is important to effectively utilize
robotic arm’s end-effector position, achieving more flexible large amounts of unstructured, multi-label, and multi-modal
control. Some works [367], [368] employ traditional methods human demonstration data combined with action label data to
to control bipedal robot walking. MIT’s Cheetah 3 [369], trainembodiedmodelsthatcanlearnvarioustasksinrelatively
ANYmal[370],andAtlas[371]userobustwalkingcontrollers short periods. By efficiently utilizing human demonstration
to manage the robots. These developed robots can be used data,roboticsystemscanachievehigherlevelsofperformance
for more agile motion tasks, such as jumping or overcoming and adaptability, making them more capable of performing
variousobstacles[372]–[376].Otherworks[377],[378]focus complex tasks in dynamic environments.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 28
Cognition of Complex Environment: Cognition of com- tions and state predictions, forming a representation learning
plex environment refers to the ability of embodied agents in system based on interaction and deduction. Moreover, agents
physical or virtual environments, to perceive, understand, and needtounderstandtheaffordancesofobjectstoachieveadap-
navigatecomplexrealworldenvironments.Basedonextensive tivetaskplanningandlong-distanceautonomousnavigationin
commonsense knowledge, the Say-Can [380] utilized pre- dynamic scenes. To optimize decision-making, it is necessary
trained LLM models’ task decomposition mechanism, which to combine counterfactual and causal intervention strategies
relies heavily on large amounts of commonsense knowledge [381]–[384], trace causality from counterfactual and causal
for simple task planning but lacking understanding of long- intervention perspectives, reduce exploration iterations, and
term tasks in complex environments. For unstructured open optimize decisions. Constructing a causal graph based on
environments,currentworksusuallyrelyonpre-trainedLLMs’ world knowledge and driving sim-to-real transfer of agents
taskdecompositionmechanismusingextensivecommon-sense throughactivecausalreasoningwillformaunifiedframework
knowledge for simple task planning, while lacking specific for embodied perception, reasoning, and interaction.
sceneunderstanding.Itisvitaltoenhancetheabilityofknowl- Continual Learning: In robotics applications, continual
edge transfer and generalization in complex environments. A learning [385] is crucial for deploying robot learning policies
truly versatile robotics system should be capable of compre- in diverse environments, yet it remains a largely unexplored
hending and executing natural language instructions across domain. While some recent studies have examined sub-topics
diverse and unseen scenes. This necessitates the development of continual learning—such as incremental learning, rapid
of adaptable and scalable embodied agent architectures. motor adaptation, and human-in-the-loop learning—these so-
Long-Horizon Task Execution: Executing single instruc- lutionsare oftendesignedfor asingle taskorplatform anddo
tions can often entail long-horizon tasks for robots, exem- notyetconsiderfoundationalmodels.Openresearchproblems
plified by commands like “clean the kitchen,” which involve andviableapproachesinclude:1)mixingdifferentproportions
activities such as rearranging objects, sweeping floors, wiping ofpriordatadistributionwhenfine-tuningonthelatestdatato
tables, and more. Accomplishing such tasks successfully ne- alleviate catastrophic forgetting [386], 2) developing efficient
cessitates the robot’s ability to plan and execute a sequence prototypes from prior distributions or curricula for task infer-
of low-level actions over extended time spans. While current enceinlearningnewtasks,3)improvingtrainingstabilityand
high-leveltaskplannershaveshowninitialsuccess,theyoften sampleefficiencyofonlinelearningalgorithms,4)identifying
prove inadequate in diverse scenarios due to their lack of principledwaystoseamlesslyincorporatelarge-capacitymod-
tuning for embodied tasks. Addressing this challenge requires els into control frameworks, potentially through hierarchical
the development of efficient planners equipped with robust learning or slow-fast control, for real-time inference.
perception capabilities and much commonsense knowledge. Unified Evaluation Benchmark: While numerous bench-
Unified Embodied Foundation Model: Exploring founda- marks exist for evaluating low-level control policies, they
tion models for embodied robot tasks remains a nascent area often vary significantly in the skills they assess. Furthermore,
of research, primarily due to the wide array of embodiments, the objects and scenes included in these benchmarks are
environments, and tasks inherent in robotics. Compounding typicallylimitedbysimulatorconstraints.Tocomprehensively
this challenge are isolated datasets and evaluation setups. Es- evaluateembodiedmodels,thereisaneedforbenchmarksthat
tablishingarobustandunifiedfoundationmodelforembodied encompass a diverse range of skills using realistic simulators.
robotics demands leveraging large-scale internet datasets and Regarding high-level task planners, many benchmarks focus
cutting-edge LLMs, MLMs and WMs. on assessing planning capability through question-answering
CausalRelationDiscovery:Existingdata-drivenembodied tasks.However,amoredesirableapproachinvolvesevaluating
agents make decisions based on the intrinsic correlations both the high-level task planner and the low-level control
within the data. However, this modeling approach does not policytogetherforexecutinglong-horizontasksandmeasuring
allow the models to truly understand the causal relations successrates,ratherthanrelyingsolelyonisolatedassessments
between knowledge, behavior, and environment, resulting in of the planner. This integrated approach offers a more holistic
biased strategies. This makes it difficult to ensure that they assessment of the capabilities of embodied AI systems.
can operate in real-world environments in an interpretable,
robust, and reliable manner. Therefore, it is important for IX. CONCLUSION
embodiedagentstoconstructembodiedperception,reasoning,
Embodied AI allows agents to sense, perceive, and interact
andinteractionframeworkdrivenbyworldknowledge,capable
with various objects from both cyber space and physical
of autonomous causal reasoning. By understanding the world
world, which exhibits its vital significance toward achiev-
through interaction and learning its workings via abductive
ing AGI. This survey extensively reviews embodied robots,
reasoning, we can further enhance the adaptability, decision
simulators, four representative embodied tasks: visual active
reliability, and generalization capabilities of multimodal em-
perception, embodied interaction, embodied agents and sim-
bodied agents in complex real-world environments.
to-real robotic control, and future research directions. The
For embodied tasks (such as embodied question answering,
comparative summary of the embodied robots, simulators,
visual language navigation, and instruction following), it is
datasets, and approaches provides a clear picture of the recent
necessary to introduce embodied interactive causal represen-
developmentinembodiedAI,whichgreatlybenefitsthefuture
tation learning. This involves establishing spatial-temporal
researchalongthisemergingandpromisingresearchdirection.
causal relations across modalities through interactive instruc-IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 29
REFERENCES [24] C. Li, S. Zhu, Z. Sun, and J. Rogers, “Bas optimized elm for kuka
iiwa robot learning,” IEEE Transactions on Circuits and Systems II:
[1] C. Machinery, “Computing machinery and intelligence-am turing,” ExpressBriefs,vol.68,no.6,pp.1987–1991,2020.
Mind,vol.59,no.236,p.433,1950. [25] M. Kennedy, K. Schmeckpeper, D. Thakur, C. Jiang, V. Kumar,
[2] L. London˜o, J. V. Hurtado, N. Hertz, P. Kellmeyer, S. Voeneky, and and K. Daniilidis, “Autonomous precision pouring from unknown
A. Valada, “Fairness and bias in robot learning,” Proceedings of the containers,”IEEERoboticsandAutomationLetters,vol.4,no.3,pp.
IEEE,2024. 2317–2324,2019.
[3] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choro- [26] Z. Feng, G. Hu, Y. Sun, and J. Soon, “An overview of collaborative
manski, T. Ding, D. Driess, A. Dubey, C. Finn et al., “Rt-2: Vision- roboticmanipulationinmulti-robotsystems,”AnnualReviewsinCon-
language-action models transfer web knowledge to robotic control,” trol,vol.49,pp.113–127,2020.
arXivpreprintarXiv:2307.15818,2023. [27] P.R.Wurman,R.D’Andrea,andM.Mountz,“Coordinatinghundreds
[4] S. Belkhale, T. Ding, T. Xiao, P. Sermanet, Q. Vuong, J. Tompson, of cooperative, autonomous vehicles in warehouses,” AI magazine,
Y. Chebotar, D. Dwibedi, and D. Sadigh, “Rt-h: Action hierarchies vol.29,no.1,pp.9–9,2008.
usinglanguage,”arXivpreprintarXiv:2403.01823,2024. [28] B. Reily, P. Gao, F. Han, H. Wang, and H. Zhang, “Real-time
[5] T. Wu, S. He, J. Liu, S. Sun, K. Liu, Q.-L. Han, and Y. Tang, “A recognitionofteambehaviorsbymultisensorygraph-embeddedrobot
briefoverviewofchatgpt:Thehistory,statusquoandpotentialfuture learning,” The International Journal of Robotics Research, vol. 41,
development,”IEEE/CAAJournalofAutomaticaSinica,vol.10,no.5, no.8,pp.798–811,2022.
pp.1122–1136,2023. [29] A.Ugenti,R.Galati,G.Mantriota,andG.Reina,“Analysisofanall-
[6] L. Smith and M. Gasser, “The development of embodied cognition: terrain tracked robot with innovative suspension system,” Mechanism
Six lessons from babies,” Artificial life, vol. 11, no. 1-2, pp. 13–29, andMachineTheory,vol.182,p.105237,2023.
2005. [30] B.M.Yamauchi,“Packbot:aversatileplatformformilitaryrobotics,”
inUnmannedgroundvehicletechnologyVI,vol.5422. SPIE,2004,
[7] J.Duan,S.Yu,H.L.Tan,H.Zhu,andC.Tan,“Asurveyofembodied
ai:Fromsimulatorstoresearchtasks,”IEEETransactionsonEmerging pp.228–237.
TopicsinComputationalIntelligence,vol.6,no.2,pp.230–244,2022. [31] M. Raibert, K. Blankespoor, G. Nelson, and R. Playter, “Bigdog, the
rough-terrain quadruped robot,” IFAC Proceedings Volumes, vol. 41,
[8] Y. Liu, X. Song, K. Jiang, W. Chen, J. Luo, G. Li, and L. Lin,
no.2,pp.10822–10825,2008.
“Multimodalembodiedinteractiveagentforcafescene,”arXivpreprint
[32] G. Bellegarda, Y. Chen, Z. Liu, and Q. Nguyen, “Robust high-speed
arXiv:2402.00290,2024.
runningforquadrupedrobotsviadeepreinforcementlearning,”in2022
[9] Y. Ma, Z. Song, Y. Zhuang, J. Hao, and I. King, “A survey
IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems
on vision-language-action models for embodied ai,” arXiv preprint
(IROS). IEEE,2022,pp.10364–10370.
arXiv:2405.14093,2024.
[33] S.LeCleac’h,T.A.Howell,S.Yang,C.-Y.Lee,J.Zhang,A.Bishop,
[10] J.Achiam,S.Adler,S.Agarwal,L.Ahmad,I.Akkaya,F.L.Aleman,
M.Schwager,andZ.Manchester,“Fastcontact-implicitmodelpredic-
D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., “Gpt-4
tivecontrol,”IEEETransactionsonRobotics,2024.
technicalreport,”arXivpreprintarXiv:2303.08774,2023.
[34] A.Bouman,M.F.Ginting,N.Alatur,M.Palieri,D.D.Fan,T.Touma,
[11] Y.Chen,W.Cui,Y.Chen,M.Tan,X.Zhang,D.Zhao,andH.Wang,
T. Pailevanian, S.-K. Kim, K. Otsu, J. Burdick et al., “Autonomous
“Robogpt:anintelligentagentofmakingembodiedlong-termdecisions
spot: Long-range autonomous exploration of extreme environments
fordailyinstructiontasks,”arXivpreprintarXiv:2311.15649,2023.
withleggedlocomotion,”in2020IEEE/RSJInternationalConference
[12] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn,
on Intelligent Robots and Systems (IROS). IEEE, 2020, pp. 2518–
K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu et al., “Rt-1:
2525.
Robotics transformer for real-world control at scale,” arXiv preprint
[35] P.Arm,G.Waibel,J.Preisig,T.Tuna,R.Zhou,V.Bickel,G.Ligeza,
arXiv:2212.06817,2022.
T. Miki, F. Kehl, H. Kolvenbach et al., “Scientific exploration of
[13] Y. Hu, Q. Xie, V. Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim,
challenging planetary analog environments with a team of legged
Y. Xie, T. Zhang, Z. Zhao et al., “Toward general-purpose robots
robots,”Sciencerobotics,vol.8,no.80,p.eade9548,2023.
via foundation models: A survey and meta-analysis,” arXiv preprint
[36] T. F. Nygaard, C. P. Martin, J. Torresen, K. Glette, and D. Howard,
arXiv:2312.08782,2023.
“Real-world embodied ai through a morphologically adaptive
[14] R.McCarthy,D.C.Tan,D.Schmidt,F.Acero,N.Herr,Y.Du,T.G.
quadrupedrobot,”NatureMachineIntelligence,vol.3,no.5,pp.410–
Thuruthel,andZ.Li,“Towardsgeneralistrobotlearningfrominternet
419,2021.
video:Asurvey,”arXivpreprintarXiv:2404.19664,2024.
[37] Y. Tong, H. Liu, and Z. Zhang, “Advancements in humanoid robots:
[15] A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal, Acomprehensivereviewandfutureprospects,”IEEE/CAAJournalof
G.Sastry,A.Askell,P.Mishkin,J.Clarketal.,“Learningtransferable AutomaticaSinica,vol.11,no.2,pp.301–328,2024.
visual models from natural language supervision,” in International [38] A.K.Vaskov,P.P.Vu,N.North,A.J.Davis,T.A.Kung,D.H.Gates,
conferenceonmachinelearning. PMLR,2021,pp.8748–8763. P. S. Cederna, and C. A. Chestek, “Surgically implanted electrodes
[16] J.Li,D.Li,S.Savarese,andS.Hoi,“Blip-2:Bootstrappinglanguage- enable real-time finger and grasp pattern recognition for prosthetic
image pre-training with frozen image encoders and large language hands,”IEEETransactionsonRobotics,vol.38,no.5,pp.2841–2857,
models,” in International conference on machine learning. PMLR, 2022.
2023,pp.19730–19742. [39] S.Maniatopoulos,P.Schillinger,V.Pong,D.C.Conner,andH.Kress-
[17] M.Assran,Q.Duval,I.Misra,P.Bojanowski,P.Vincent,M.Rabbat, Gazit, “Reactive high-level behavior synthesis for an atlas humanoid
Y. LeCun, and N. Ballas, “Self-supervised learning from images robot,”in2016IEEEinternationalconferenceonroboticsandautoma-
withajoint-embeddingpredictivearchitecture,”inProceedingsofthe tion(ICRA). IEEE,2016,pp.4192–4199.
IEEE/CVF Conference on Computer Vision and Pattern Recognition, [40] D.J.Agravante,A.Cherubini,A.Sherikov,P.-B.Wieber,andA.Khed-
2023,pp.15619–15629. dar,“Human-humanoidcollaborativecarrying,”IEEETransactionson
[18] Z. Zhu, X. Wang, W. Zhao, C. Min, N. Deng, M. Dou, Y. Wang, Robotics,vol.35,no.4,pp.833–846,2019.
B. Shi, K. Wang, C. Zhang et al., “Is sora a world simulator? a [41] S. Shigemi, A. Goswami, and P. Vadakkepat, “Asimo and humanoid
comprehensive survey on general world models and beyond,” arXiv robot research at honda,” Humanoid robotics: A reference, vol. 55,
preprintarXiv:2405.03520,2024. p.90,2018.
[19] R. Pfeifer and F. Iida, “Embodied artificial intelligence: Trends and [42] F.Tanaka,K.Isshiki,F.Takahashi,M.Uekusa,R.Sei,andK.Hayashi,
challenges,”Lecturenotesincomputerscience,pp.1–26,2004. “Pepperlearnstogetherwithchildren:Developmentofaneducational
[20] J.Haugeland,Artificialintelligence:Theveryidea. MITpress,1989. application,” in 2015 IEEE-RAS 15th International Conference on
[21] R.PfeiferandJ.Bongard,Howthebodyshapesthewaywethink:a HumanoidRobots(Humanoids). IEEE,2015,pp.270–275.
newviewofintelligence. MITpress,2006. [43] J. Xiang, T. Tao, Y. Gu, T. Shu, Z. Wang, Z. Yang, and Z. Hu,
[22] B.Siciliano,O.Khatib,andT.Kro¨ger,Springerhandbookofrobotics. “Languagemodelsmeetworldmodels:Embodiedexperiencesenhance
Springer,2008,vol.200. languagemodels,”Advancesinneuralinformationprocessingsystems,
[23] S.Haddadin,S.Parusel,L.Johannsmeier,S.Golz,S.Gabl,F.Walch, vol.36,2024.
M.Sabaghian,C.Ja¨hne,L.Hausperger,andS.Haddadin,“Thefranka [44] NVIDIA,“Nvidiaisaacsim:Roboticssimulationandsyntheticdata,”
emikarobot:Areferenceplatformforroboticsresearchandeducation,” 2023.[Online].Available:https://developer.nvidia.com/isaac-sim
IEEE Robotics & Automation Magazine, vol. 29, no. 2, pp. 46–64, [45] V.Makoviychuk,L.Wawrzyniak,Y.Guo,M.Lu,K.Storey,M.Mack-
2022. lin, D. Hoeller, N. Rudin, A. Allshire, A. Handa et al., “Isaac gym:IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 30
High performance gpu-based physics simulation for robot learning,” [68] B.Shen,F.Xia,C.Li,R.Mart´ın-Mart´ın,L.Fan,G.Wang,C.Pe´rez-
arXivpreprintarXiv:2108.10470,2021. D’Arpino,S.Buch,S.Srivastava,L.Tchapmi,M.Tchapmi,K.Vainio,
[46] N.KoenigandA.Howard,“Designanduseparadigmsforgazebo,an J. Wong, L. Fei-Fei, and S. Savarese, “igibson 1.0: A simulation
open-source multi-robot simulator,” in 2004 IEEE/RSJ International environment for interactive tasks in large realistic scenes,” in 2021
ConferenceonIntelligentRobotsandSystems(IROS),Apr2005. IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems
[47] E. Coumans and Y. Bai, “Pybullet, a python module for physics (IROS),2021,pp.7520–7527.
simulationforgames,roboticsandmachinelearning,”2016. [69] C.Li,F.Xia,R.Mart´ın-Mart´ın,M.Lingelbach,S.Srivastava,B.Shen,
[48] Cyberbotics, “Webots: open-source robot simulator.” [Online]. K. E. Vainio, C. Gokmen, G. Dharan, T. Jain, A. Kurenkov, K. Liu,
Available:https://github.com/cyberbotics/webots H. Gweon, J. Wu, L. Fei-Fei, and S. Savarese, “igibson 2.0: Object-
[49] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for centricsimulationforrobotlearningofeverydayhouseholdtasks,”in
model-basedcontrol,”in2012IEEE/RSJInternationalConferenceon Proceedingsofthe5thConferenceonRobotLearning,ser.Proceedings
IntelligentRobotsandSystems. IEEE,2012,pp.5026–5033. of Machine Learning Research, A. Faust, D. Hsu, and G. Neumann,
[50] A.Juliani,V.-P.Berges,E.Teng,A.Cohen,J.Harper,C.Elion,C.Goy, Eds.,vol.164. PMLR,08–11Nov2022,pp.455–465.
Y.Gao,H.Henry,M.Mattar,andD.Lange,“Unity:Ageneralplatform [70] C.Gan,J.Schwartz,S.Alter,M.Schrimpf,J.Traer,J.Freitas,J.Kubil-
forintelligentagents,”arXivpreprintarXiv:1809.02627,2020. ius,A.Bhandwaldar,N.Haber,M.Sano,K.Kim,E.Wang,D.Mrowca,
[51] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-fidelity M.Lingelbach,A.Curtis,K.Feigelis,D.Bear,D.Gutfreund,D.Cox,
visualandphysicalsimulationforautonomousvehicles,”inFieldand J. DiCarlo, J. McDermott, J. Tenenbaum, and D. Yamins, “Threed-
ServiceRobotics,2017. world: A platform for interactive multi-modal physical simulation,”
[52] ISAE-SUPAERO,“Morse:themodularopenrobotssimulatorengine.”
NeuralInformationProcessingSystems2021,Dec2021.
[Online].Available:https://github.com/morse-simulator/morse [71] Y. Wang, Z. Xian, F. Chen, T.-H. Wang, Y. Wang, K. Fragkiadaki,
Z. Erickson, D. Held, and C. Gan, “Robogen: Towards unleashing
[53] E. Rohmer, S. P. Singh, and M. Freese, “V-rep: A versatile and
infinite data for automated robot learning via generative simulation,”
scalablerobotsimulationframework,”in2013IEEE/RSJinternational
arXiv:2311.01455,Nov2023.
conferenceonintelligentrobotsandsystems. IEEE,2013,pp.1321–
[72] Y. Yang, F.-Y. Sun, L. Weihs, E. VanderBilt, A. Herrasti, W. Han,
1326.
J.Wu,N.Haber,R.Krishna,L.Liuetal.,“Holodeck:Languageguided
[54] Y.Yang,Z.He,P.Jiao,andH.Ren,“Bioinspiredsoftrobotics:Howdo
generation of 3d embodied ai environments,” in Proceedings of the
we learn from creatures?” IEEE Reviews in Biomedical Engineering,
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2022.
2024,pp.16227–16237.
[55] M.Ilami,H.Bagheri,R.Ahmed,E.O.Skowronek,andH.Marvi,“Ma-
[73] Y. Yang, B. Jia, P. Zhi, and S. Huang, “Physcene: Physically in-
terials, actuators, and sensors for soft bioinspired robots,” Advanced
teractable 3d scene synthesis for embodied ai,” in Proceedings of
Materials,vol.33,no.19,p.2003139,2021.
Conference on Computer Vision and Pattern Recognition (CVPR),
[56] R. K. Katzschmann, J. DelPreto, R. MacCurdy, and D. Rus, “Explo-
2024.
ration of underwater life with an acoustically controlled soft robotic
[74] M.Deitke,E.VanderBilt,A.Herrasti,L.Weihs,J.Salvador,K.Ehsani,
fish,”ScienceRobotics,vol.3,no.16,p.eaar3449,2018.
W. Han, E. Kolve, A. Farhadi, A. Kembhavi, and R. Mottaghi,
[57] F. Berlinger, M. Gauci, and R. Nagpal, “Implicit coordination for
“ProcTHOR:Large-ScaleEmbodiedAIUsingProceduralGeneration,”
3d underwater collective behaviors in a fish-inspired robot swarm,”
inNeurIPS,2022,outstandingPaperAward.
ScienceRobotics,vol.6,no.50,p.eabd8668,2021.
[75] L.Fei-FeiandR.Krishna,“Searchingforcomputervisionnorthstars,”
[58] G.C.deCroon,J.Dupeyroux,S.B.Fuller,andJ.A.Marshall,“Insect-
Daedalus,vol.151,no.2,pp.85–99,2022.
inspiredaiforautonomousrobots,”Sciencerobotics,vol.7,no.67,p.
[76] Y.Liu,Z.Lu,J.Li,T.Yang,andC.Yao,“Deepimage-to-videoadap-
eabl6334,2022.
tationandfusionnetworksforactionrecognition,”IEEETransactions
[59] X.Zhou,X.Wen,Z.Wang,Y.Gao,H.Li,Q.Wang,T.Yang,H.Lu,
onImageProcessing,vol.29,pp.3168–3182,2019.
Y. Cao, C. Xu et al., “Swarm of micro flying robots in the wild,”
[77] Y. Liu, K. Wang, L. Liu, H. Lan, and L. Lin, “Tcgl: Temporal
ScienceRobotics,vol.7,no.66,p.eabm5954,2022.
contrastive graph for self-supervised video representation learning,”
[60] N. R. Sinatra, C. B. Teeple, D. M. Vogt, K. K. Parker, D. F. Gruber, IEEETransactionsonImageProcessing,vol.31,pp.1978–1993,2022.
andR.J.Wood,“Ultragentlemanipulationofdelicatestructuresusing [78] H. Yan, Y. Liu, Y. Wei, Z. Li, G. Li, and L. Lin, “Skeletonmae:
asoftroboticgripper,”ScienceRobotics,vol.4,no.33,p.eaax5425,
graph-basedmaskedautoencoderforskeletonsequencepre-training,”in
2019. ProceedingsoftheIEEE/CVFInternationalConferenceonComputer
[61] N.KoenigandA.Howard,“Designanduseparadigmsforgazebo,an Vision,2023,pp.5606–5618.
open-source multi-robot simulator,” in 2004 IEEE/RSJ international [79] A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse, “Monoslam:
conference on intelligent robots and systems (IROS), vol. 3. Ieee, Real-timesinglecameraslam,”IEEEtransactionsonpatternanalysis
2004,pp.2149–2154. andmachineintelligence,vol.29,no.6,pp.1052–1067,2007.
[62] E.Kolve,R.Mottaghi,D.Gordon,Y.Zhu,A.Gupta,andA.Farhadi, [80] A.I.MourikisandS.I.Roumeliotis,“Amulti-stateconstraintkalman
“Ai2-thor: An interactive 3d environment for visual ai,” arXiv: Com- filter for vision-aided inertial navigation,” in Proceedings 2007 IEEE
puter Vision and Pattern Recognition,arXiv: Computer Vision and international conference on robotics and automation. IEEE, 2007,
PatternRecognition,Dec2017. pp.3565–3572.
[63] A.Chang,A.Dai,T.Funkhouser,M.Halber,M.Niebner,M.Savva, [81] G.KleinandD.Murray,“Paralleltrackingandmappingforsmallar
S.Song,A.Zeng,andY.Zhang,“Matterport3d:Learningfromrgb-d workspaces,”in20076thIEEEandACMinternationalsymposiumon
datainindoorenvironments,”in2017InternationalConferenceon3D mixedandaugmentedreality. IEEE,2007,pp.225–234.
Vision(3DV),Oct2017. [82] R.Mur-Artal,J.M.M.Montiel,andJ.D.Tardos,“Orb-slam:aversatile
[64] P.Anderson,Q.Wu,D.Teney,J.Bruce,M.Johnson,N.Sunderhauf, andaccuratemonocularslamsystem,”IEEEtransactionsonrobotics,
I. Reid, S. Gould, and A. van den Hengel, “Vision-and-language vol.31,no.5,pp.1147–1163,2015.
navigation: Interpreting visually-grounded navigation instructions in [83] R.A.Newcombe,S.J.Lovegrove,andA.J.Davison,“Dtam:Dense
realenvironments,”in2018IEEE/CVFConferenceonComputerVision tracking and mapping in real-time,” in 2011 international conference
andPatternRecognition,Jun2018. oncomputervision. IEEE,2011,pp.2320–2327.
[65] X.Puig,K.Ra,M.Boben,J.Li,T.Wang,S.Fidler,andA.Torralba, [84] J. Engel, T. Scho¨ps, and D. Cremers, “Lsd-slam: Large-scale di-
“Virtualhome:Simulatinghouseholdactivitiesviaprograms,”in2018 rect monocular slam,” in European conference on computer vision.
IEEE/CVF Conference on Computer Vision and Pattern Recognition, Springer,2014,pp.834–849.
Jun2018. [85] R. F. Salas-Moreno, R. A. Newcombe, H. Strasdat, P. H. Kelly, and
[66] M. Savva, A. Kadian, O. Maksymets, Y. Zhao, E. Wijmans, B. Jain, A.J.Davison,“Slam++:Simultaneouslocalisationandmappingatthe
J.Straub,J.Liu,V.Koltun,J.Malik,D.Parikh,andD.Batra,“Habitat: levelofobjects,”inProceedingsoftheIEEEconferenceoncomputer
Aplatformforembodiedairesearch,”in2019IEEE/CVFInternational visionandpatternrecognition,2013,pp.1352–1359.
ConferenceonComputerVision(ICCV),Oct2019. [86] S.YangandS.Scherer,“Cubeslam:Monocular3-dobjectslam,”IEEE
[67] F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu, H. Jiang, TransactionsonRobotics,vol.35,no.4,pp.925–938,2019.
Y. Yuan, H. Wang, L. Yi, A. X. Chang, L. J. Guibas, and H. Su, [87] J.Zhang,M.Gui,Q.Wang,R.Liu,J.Xu,andS.Chen,“Hierarchical
“Sapien: A simulated part-based interactive environment,” in 2020 topicmodelbasedobjectassociationforsemanticslam,”IEEEtrans-
IEEE/CVF Conference on Computer Vision and Pattern Recognition actions on visualization and computer graphics, vol. 25, no. 11, pp.
(CVPR),Jun2020. 3052–3062,2019.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 31
[88] L. Nicholson, M. Milford, and N. Su¨nderhauf, “Quadricslam: Dual inNeuralInformationProcessingSystems,vol.35,pp.33330–33342,
quadricsfromobjectdetectionsaslandmarksinobject-orientedslam,” 2022.
IEEERoboticsandAutomationLetters,vol.4,no.1,pp.1–8,2018. [109] X. Wu, L. Jiang, P.-S. Wang, Z. Liu, X. Liu, Y. Qiao, W. Ouyang,
[89] Z. Liao, Y. Hu, J. Zhang, X. Qi, X. Zhang, and W. Wang, “So-slam: T. He, and H. Zhao, “Point transformer v3: Simpler faster stronger,”
Semanticobjectslamwithscaleproportionalandsymmetricaltexture inProceedingsoftheIEEE/CVFConferenceonComputerVisionand
constraints,”IEEERoboticsandAutomationLetters,vol.7,no.2,pp. PatternRecognition,2024,pp.4840–4851.
4008–4015,2022. [110] D.Liang,X.Zhou,X.Wang,X.Zhu,W.Xu,Z.Zou,X.Ye,andX.Bai,
[90] C. Yu, Z. Liu, X.-J. Liu, F. Xie, Y. Yang, Q. Wei, and Q. Fei, “Ds- “Pointmamba: A simple state space model for point cloud analysis,”
slam:Asemanticvisualslamtowardsdynamicenvironments,”in2018 arXivpreprintarXiv:2402.10739,2024.
IEEE/RSJ international conference on intelligent robots and systems [111] T. Zhang, X. Li, H. Yuan, S. Ji, and S. Yan, “Point could
(IROS). IEEE,2018,pp.1168–1174. mamba: Point cloud learning via state space model,” arXiv preprint
[91] B.Bescos,J.M.Fa´cil,J.Civera,andJ.Neira,“Dynaslam:Tracking, arXiv:2403.00762,2024.
mapping, and inpainting in dynamic scenes,” IEEE Robotics and [112] X. Han, Y. Tang, Z. Wang, and X. Li, “Mamba3d: Enhancing local
AutomationLetters,vol.3,no.4,pp.4076–4083,2018. features for 3d point cloud analysis via state space model,” arXiv
[92] S. Cheng, C. Sun, S. Zhang, and D. Zhang, “Sg-slam: A real-time preprintarXiv:2404.14966,2024.
rgb-dvisualslamtowarddynamicsceneswithsemanticandgeometric [113] L.Pinto,D.Gandhi,Y.Han,Y.-L.Park,andA.Gupta,“Thecurious
information,”IEEETransactionsonInstrumentationandMeasurement, robot: Learning visual representations via physical interactions,” in
vol.72,pp.1–12,2022. ComputerVision–ECCV2016:14thEuropeanConference,Amsterdam,
[93] J. He, M. Li, Y. Wang, and H. Wang, “Ovd-slam: An online visual The Netherlands, October 11-14, 2016, Proceedings, Part II 14.
slamfordynamicenvironments,”IEEESensorsJournal,2023. Springer,2016,pp.3–18.
[94] C.Yan,D.Qu,D.Xu,B.Zhao,Z.Wang,D.Wang,andX.Li,“Gs- [114] G. Tatiya, J. Francis, and J. Sinapov, “Transferring implicit knowl-
slam: Dense visual slam with 3d gaussian splatting,” in Proceedings edge ofnon-visual object propertiesacross heterogeneousrobot mor-
oftheIEEE/CVFConferenceonComputerVisionandPatternRecog- phologies,” in 2023 IEEE International Conference on Robotics and
nition,2024,pp.19595–19604. Automation(ICRA). IEEE,2023,pp.11315–11321.
[115] D.JayaramanandK.Grauman,“Learningtolookaround:Intelligently
[95] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object
detectionnetworkforautonomousdriving,”inProceedingsoftheIEEE
exploringunseenenvironmentsforunknowntasks,”inProceedingsof
conference on Computer Vision and Pattern Recognition, 2017, pp.
theIEEEconferenceoncomputervisionandpatternrecognition,2018,
pp.1238–1247.
1907–1915.
[116] L. Jin, X. Chen, J. Ru¨ckin, and M. Popovic´, “Neu-nbv: Next best
[96] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom,
view planning using uncertainty estimation in image-based neural
“Pointpillars: Fast encoders for object detection from point clouds,”
rendering,”in2023IEEE/RSJInternationalConferenceonIntelligent
in Proceedings of the IEEE/CVF conference on computer vision and
RobotsandSystems(IROS). IEEE,2023,pp.11305–11312.
patternrecognition,2019,pp.12697–12705.
[117] Y. Hu, J. Geng, C. Wang, J. Keller, and S. Scherer, “Off-policy
[97] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, “Multi-view
evaluationwithonlineadaptationforrobotexplorationinchallenging
convolutional neural networks for 3d shape recognition,” in Proceed-
environments,”IEEERoboticsandAutomationLetters,2023.
ings of the IEEE international conference on computer vision, 2015,
[118] L. Fan, M. Liang, Y. Li, G. Hua, and Y. Wu, “Evidential active
pp.945–953.
recognition:Intelligentandprudentopen-worldembodiedperception,”
[98] D.MaturanaandS.Scherer,“Voxnet:A3dconvolutionalneuralnet-
inProceedingsoftheIEEE/CVFConferenceonComputerVisionand
workforreal-timeobjectrecognition,”in2015IEEE/RSJinternational
PatternRecognition,2024,pp.16351–16361.
conferenceonintelligentrobotsandsystems(IROS). IEEE,2015,pp.
[119] S.Mokssit,D.B.Licea,B.Guermah,andM.Ghogho,“Deeplearning
922–928.
techniques for visual slam: A survey,” IEEE Access, vol. 11, pp.
[99] S.Song,F.Yu,A.Zeng,A.X.Chang,M.Savva,andT.Funkhouser,
20026–20050,2023.
“Semantic scene completion from a single depth image,” in Pro-
[120] K.Chen,J.Zhang,J.Liu,Q.Tong,R.Liu,andS.Chen,“Semanticvi-
ceedings of the IEEE conference on computer vision and pattern
sualsimultaneouslocalizationandmapping:Asurvey,”arXivpreprint
recognition,2017,pp.1746–1754.
arXiv:2209.06428,2022.
[100] C. Choy, J. Gwak, and S. Savarese, “4d spatio-temporal convnets:
[121] P.K.Vinodkumar,D.Karabulut,E.Avots,C.Ozcinar,andG.Anbar-
Minkowski convolutional neural networks,” in Proceedings of the
jafari, “A survey on deep learning based segmentation, detection and
IEEE/CVF conference on computer vision and pattern recognition,
classificationfor3dpointclouds,”Entropy,vol.25,no.4,p.635,2023.
2019,pp.3075–3084.
[122] H.Durrant-WhyteandT.Bailey,“Simultaneouslocalizationandmap-
[101] B. Graham, M. Engelcke, and L. Van Der Maaten, “3d semantic ping: part i,” IEEE robotics & automation magazine, vol. 13, no. 2,
segmentation with submanifold sparse convolutional networks,” in
pp.99–110,2006.
Proceedings of the IEEE conference on computer vision and pattern
[123] T.BaileyandH.Durrant-Whyte,“Simultaneouslocalizationandmap-
recognition,2018,pp.9224–9232.
ping(slam):Partii,”IEEErobotics&automationmagazine,vol.13,
[102] T.Wang,X.Mao,C.Zhu,R.Xu,R.Lyu,P.Li,X.Chen,W.Zhang, no.3,pp.108–117,2006.
K. Chen, T. Xue et al., “Embodiedscan: A holistic multi-modal 3d [124] X. Zhang, J. Lai, D. Xu, H. Li, and M. Fu, “2d lidar-based slam
perceptionsuitetowardsembodiedai,”inProceedingsoftheIEEE/CVF and path planning for indoor rescue using mobile robots,” Journal of
Conference on Computer Vision and Pattern Recognition, 2024, pp. AdvancedTransportation,vol.2020,no.1,p.8867937,2020.
19757–19767. [125] D. Droeschel and S. Behnke, “Efficient continuous-time slam for 3d
[103] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning lidar-based online mapping,” in 2018 IEEE International Conference
on point sets for 3d classification and segmentation,” in Proceedings onRoboticsandAutomation(ICRA). IEEE,2018,pp.5000–5007.
of the IEEE conference on computer vision and pattern recognition, [126] J. Ruan, B. Li, Y. Wang, and Z. Fang, “Gp-slam+: real-time 3d lidar
2017,pp.652–660. slambasedonimprovedregionalizedgaussianprocessmapreconstruc-
[104] C.R.Qi,L.Yi,H.Su,andL.J.Guibas,“Pointnet++:Deephierarchical tion,”in2020IEEE/RSJInternationalConferenceonIntelligentRobots
feature learning on point sets in a metric space,” Advances in neural andSystems(IROS). IEEE,2020,pp.5171–5178.
informationprocessingsystems,vol.30,2017. [127] V.Mittal,“Attngrounder:Talkingtocarswithattention,”inComputer
[105] X.Ma,C.Qin,H.You,H.Ran,andY.Fu,“Rethinkingnetworkdesign Vision–ECCV 2020 Workshops: Glasgow, UK, August 23–28, 2020,
andlocalgeometryinpointcloud:Asimpleresidualmlpframework,” Proceedings,PartII16. Springer,2020,pp.62–73.
arXivpreprintarXiv:2202.07123,2022. [128] X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y.-F. Wang,
[106] H.Zhao,L.Jiang,J.Jia,P.H.Torr,andV.Koltun,“Pointtransformer,” W. Y. Wang, and L. Zhang, “Reinforced cross-modal matching and
inProceedingsoftheIEEE/CVFinternationalconferenceoncomputer self-supervised imitation learning for vision-language navigation,” in
vision,2021,pp.16259–16268. Proceedings of the IEEE/CVF conference on computer vision and
[107] Y.-Q. Yang, Y.-X. Guo, J.-Y. Xiong, Y. Liu, H. Pan, P.-S. Wang, patternrecognition,2019,pp.6629–6638.
X. Tong, and B. Guo, “Swin3d: A pretrained transformer backbone [129] C. Bermejo, L. H. Lee, P. Chojecki, D. Przewozny, and P. Hui,
for3dindoorsceneunderstanding,”arXivpreprintarXiv:2304.06906, “Exploring button designs for mid-air interaction in virtual reality: A
2023. hexa-metric evaluation of key representations and multi-modal cues,”
[108] X. Wu, Y. Lao, L. Jiang, X. Liu, and H. Zhao, “Point transformer ProceedingsoftheACMonHuman-ComputerInteraction,vol.5,no.
v2: Grouped vector attention and partition-based pooling,” Advances EICS,pp.1–26,2021.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 32
[130] D. Z. Chen, A. X. Chang, and M. Nießner, “Scanrefer: 3d object [151] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova,“Bert:Pre-training
localization in rgb-d scans using natural language,” in European ofdeepbidirectionaltransformersforlanguageunderstanding,”arXiv
conferenceoncomputervision. Springer,2020,pp.202–221. preprintarXiv:1810.04805,2018.
[131] P.Achlioptas,A.Abdelreheem,F.Xia,M.Elhoseiny,andL.Guibas, [152] Y.Liu,M.Ott,N.Goyal,J.Du,M.Joshi,D.Chen,O.Levy,M.Lewis,
“Referit3d:Neurallistenersforfine-grained3dobjectidentificationin L.Zettlemoyer,andV.Stoyanov,“Roberta:Arobustlyoptimizedbert
real-world scenes,” in Computer Vision–ECCV 2020: 16th European pretrainingapproach,”arXivpreprintarXiv:1907.11692,2019.
Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I [153] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
16. Springer,2020,pp.422–440. T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gellyetal.,
[132] P.-H.Huang,H.-H.Lee,H.-T.Chen,andT.-L.Liu,“Text-guidedgraph “Animageisworth16x16words:Transformersforimagerecognition
neuralnetworksforreferring3dinstancesegmentation,”inProceedings atscale,”arXivpreprintarXiv:2010.11929,2020.
oftheAAAIConferenceonArtificialIntelligence,vol.35,no.2,2021, [154] D. He, Y. Zhao, J. Luo, T. Hui, S. Huang, A. Zhang, and S. Liu,
pp.1610–1618. “Transrefer3d: Entity-and-relation aware transformer for fine-grained
[133] Z.Yang,S.Zhang,L.Wang,andJ.Luo,“Sat:2dsemanticsassisted 3d visual grounding,” in Proceedings of the 29th ACM International
training for 3d visual grounding,” in Proceedings of the IEEE/CVF ConferenceonMultimedia,2021,pp.2344–2352.
InternationalConferenceonComputerVision,2021,pp.1856–1866. [155] A.Kamath,M.Singh,Y.LeCun,G.Synnaeve,I.Misra,andN.Carion,
[134] M.Feng,Z.Li,Q.Li,L.Zhang,X.Zhang,G.Zhu,H.Zhang,Y.Wang, “Mdetr-modulated detection for end-to-end multi-modal understand-
andA.Mian,“Free-formdescriptionguided3dvisualgraphnetwork ing,” in Proceedings of the IEEE/CVF International Conference on
forobjectgroundinginpointcloud,”inProceedingsoftheIEEE/CVF ComputerVision,2021,pp.1780–1790.
InternationalConferenceonComputerVision,2021,pp.3722–3731. [156] L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang,
[135] L. Zhao, D. Cai, L. Sheng, and D. Xu, “3dvg-transformer: Relation L.Yuan,L.Zhang,J.-N.Hwangetal.,“Groundedlanguage-imagepre-
modelingforvisualgroundingonpointclouds,”inProceedingsofthe training,” in Proceedings of the IEEE/CVF Conference on Computer
IEEE/CVF International Conference on Computer Vision, 2021, pp. VisionandPatternRecognition,2022,pp.10965–10975.
2928–2937. [157] V.Jain,G.Magalhaes,A.Ku,A.Vaswani,E.Ie,andJ.Baldridge,“Stay
[136] J.Roh,K.Desingh,A.Farhadi,andD.Fox,“Languagerefer:Spatial- on the path: Instruction fidelity in vision-and-language navigation,”
language model for 3d visual grounding,” in Conference on Robot in Proceedings of the 57th Annual Meeting of the Association for
Learning. PMLR,2022,pp.1046–1056. ComputationalLinguistics,Jan2019.
[137] E. Bakr, Y. Alsaedy, and M. Elhoseiny, “Look around and refer: 2d [158] J.Krantz,E.Wijmans,A.Majumdar,D.Batra,andS.Lee,Beyondthe
synthetic semantics knowledge distillation for 3d visual grounding,” Nav-Graph: Vision-and-Language Navigation in Continuous Environ-
Advances in neural information processing systems, vol. 35, pp. ments,Jan2020,p.104–120.
37146–37158,2022. [159] H. Chen, A. Suhr, D. Misra, N. Snavely, and Y. Artzi, “Touchdown:
[138] S. Huang, Y. Chen, J. Jia, and L. Wang, “Multi-view transformer for Natural language navigation and spatial reasoning in visual street
3dvisualgrounding,”inProceedingsoftheIEEE/CVFConferenceon environments,”in2019IEEE/CVFConferenceonComputerVisionand
ComputerVisionandPatternRecognition,2022,pp.15524–15533. PatternRecognition(CVPR),Jun2019.
[139] J.Yang,X.Chen,S.Qian,N.Madaan,M.Iyengar,D.F.Fouhey,and [160] Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Y. Wang, C. Shen, and
J. Chai, “Llm-grounder: Open-vocabulary 3d visual grounding with A. van den Hengel, “Reverie: Remote embodied visual referring
large language model as an agent,” arXiv preprint arXiv:2309.12311, expressioninrealindoorenvironments,”in2020IEEE/CVFConference
2023. onComputerVisionandPatternRecognition(CVPR),Jun2020.
[140] Z. Yuan, J. Ren, C.-M. Feng, H. Zhao, S. Cui, and Z. Li, “Visual [161] F. Zhu, X. Liang, Y. Zhu, Q. Yu, X. Chang, and X. Liang, “Soon:
programming for zero-shot open-vocabulary 3d visual grounding,” Scenario oriented object navigation with graph-based exploration,” in
arXivpreprintarXiv:2311.15383,2023. 2021 IEEE/CVF Conference on Computer Vision and Pattern Recog-
[141] J.Luo,J.Fu,X.Kong,C.Gao,H.Ren,H.Shen,H.Xia,andS.Liu, nition(CVPR),Jun2021.
“3d-sps:Single-stage3dvisualgroundingviareferredpointprogressive [162] H.Wang,A.G.H.Chen,X.Li,M.Wu,andH.Dong,“Findwhatyou
selection,”inProceedingsoftheIEEE/CVFConferenceonComputer want:Learningdemand-conditionedobjectattributespacefordemand-
VisionandPatternRecognition,2022,pp.16454–16463. driven navigation,” Advances in Neural Information Processing Sys-
[142] A.Jain,N.Gkanatsios,I.Mediratta,andK.Fragkiadaki,“Bottomup tems,2023.
topdowndetectiontransformersforlanguagegroundinginimagesand [163] M.Shridhar,J.Thomason,D.Gordon,Y.Bisk,W.Han,R.Mottaghi,
pointclouds,”inEuropeanConferenceonComputerVision. Springer, L. Zettlemoyer, and D. Fox, “Alfred: A benchmark for interpreting
2022,pp.417–433. groundedinstructionsforeverydaytasks,”in2020IEEE/CVFConfer-
[143] Y. Wu, X. Cheng, R. Zhang, Z. Cheng, and J. Zhang, “Eda: Explicit enceonComputerVisionandPatternRecognition(CVPR),Jun2020.
text-decoupling and dense alignment for 3d visual grounding,” in [164] S. Yenamandra, A. Ramachandran, K. Yadav, A. Wang, M. Khanna,
Proceedings of the IEEE/CVF Conference on Computer Vision and T. Gervet, T.-Y. Yang, V. Jain, A. Clegg, J. Turner, Z. Kira,
PatternRecognition,2023,pp.19231–19242. M. Savva, A. Chang, D. Chaplot, D. Batra, R. Mottaghi, Y. Bisk,
[144] C. Zhu, T. Wang, W. Zhang, K. Chen, and X. Liu, “Empowering 3d and C. Paxton, “Homerobot: Open-vocabulary mobile manipulation,”
visualgroundingwithreasoningcapabilities,”2024. arXiv:2306.11565,Jun2023.
[145] Y. Lei, Z. Wang, F. Chen, G. Wang, P. Wang, and Y. Yang, “Recent [165] C.Li,R.Zhang,J.Wong,C.Gokmen,S.Srivastava,R.Mart´ın-Mart´ın,
advances in multi-modal 3d scene understanding: A comprehensive C. Wang, G. Levine, M. Lingelbach, J. Sun et al., “Behavior-1k: A
surveyandevaluation,”arXivpreprintarXiv:2310.15676,2023. benchmarkforembodiedaiwith1,000everydayactivitiesandrealistic
[146] Y. Zhan, Z. Xiong, and Y. Yuan, “Rsvg: Exploring data and models simulation,”inConferenceonRobotLearning. PMLR,2023,pp.80–
for visual grounding on remote sensing data,” IEEE Transactions on 93.
GeoscienceandRemoteSensing,vol.61,pp.1–13,2023. [166] J. Thomason, M. Murray, M. Cakmak, and L. Zettlemoyer, “Vision-
[147] Y.ZhouandO.Tuzel,“Voxelnet:End-to-endlearningforpointcloud and-dialog navigation,” in Conference on Robot Learning. PMLR,
based3dobjectdetection,”inProceedingsoftheIEEEconferenceon 2020,pp.394–406.
computervisionandpatternrecognition,2018,pp.4490–4499. [167] X.Gao,Q.Gao,R.Gong,K.Lin,G.Thattai,andG.Sukhatme,“Di-
[148] Z. Ding, X. Han, and M. Niethammer, “Votenet: A deep learning alfred: Dialogue-enabled agents for embodied instruction following,”
label fusion method for multi-atlas segmentation,” in Medical Image arXiv:2202.13330,2022.
Computing and Computer Assisted Intervention–MICCAI 2019: 22nd [168] Y. Hong, C.Rodriguez, Y. Qi, Q.Wu, and S. Gould,“Language and
International Conference, Shenzhen, China, October 13–17, 2019, visual entity relationship graph for agent navigation,” vol. 33, 2020,
Proceedings,PartIII22. Springer,2019,pp.202–210. pp.7685–7696.
[149] L. Jiang, H. Zhao, S. Shi, S. Liu, C.-W. Fu, and J. Jia, “Pointgroup: [169] W.Zhang,C.Ma,Q.Wu,andX.Yang,“Language-guidednavigation
Dual-setpointgroupingfor3dinstancesegmentation,”inProceedings via cross-modal grounding and alternate adversarial learning,” IEEE
oftheIEEE/CVFconferenceoncomputervisionandPatternrecogni- Transactions on Circuits and Systems for Video Technology, vol. 31,
tion,2020,pp.4867–4876. pp.3469–3481,2020.
[150] J.Schult,F.Engelmann,A.Hermans,O.Litany,S.Tang,andB.Leibe, [170] X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y.-F. Wang,
“Mask3d: Mask transformer for 3d semantic instance segmentation,” W.Y.Wang,andL.Zhang,“Vision-languagenavigationpolicylearning
in 2023 IEEE International Conference on Robotics and Automation andadaptation,”IEEETransactionsonPatternAnalysisandMachine
(ICRA). IEEE,2023,pp.8216–8223. Intelligence,vol.43,no.12,pp.4205–4216,2021.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 33
[171] S.Y.Min,D.S.Chaplot,P.K.Ravikumar,Y.Bisk,andR.Salakhut- [194] S. J. Lederman and R. L. Klatzky, “Haptic perception: A tutorial,”
dinov,“FILM:Followinginstructionsinlanguagewithmodularmeth- Attention,Perception,&Psychophysics,vol.71,no.7,pp.1439–1459,
ods,”inInternationalConferenceonLearningRepresentations,2022. 2009.
[172] D. Shah, B. Osinski, B. Ichter, and S. Levine, “Lm-nav: Robotic [195] S. Stassi, V. Cauda, G. Canavese, and C. F. Pirri, “Flexible tactile
navigation with large pre-trained models of language, vision, and sensingbasedonpiezoresistivecomposites:Areview,”Sensors,vol.14,
action,”inConferenceonRobotLearning,2022. no.3,pp.5296–5332,2014.
[173] Y.Qiao,Y.Qi,Y.Hong,Z.Yu,P.Wang,andQ.Wu,“Hop:History- [196] Z. Kappassov, J.-A. Corrales, and V. Perdereau, “Tactile sensing in
and-order aware pretraining for vision-and-language navigation,” in dexterous robot hands,” Robotics and Autonomous Systems, vol. 74,
2022 IEEE/CVF Conference on Computer Vision and Pattern Recog- pp.195–220,2015.
nition(CVPR),2022,pp.15397–15406. [197] Y. Zhou, Z. Yan, Y. Yang, Z. Wang, P. Lu, P. F. Yuan, and B. He,
[174] D. Zheng, S. Huang, L. Zhao, Y. Zhong, and L. Wang, “Towards “Bioinspired sensors and applications in intelligent robots: a review,”
learning a generalist model for embodied navigation,” in IEEE/CVF RoboticIntelligenceandAutomation,vol.44,no.2,pp.215–228,2024.
Conference on Computer Vision and Pattern Recognition 2024, Jun [198] J. A. Fishel and G. E. Loeb, “Sensing tactile microvibrations with
2024. the biotac—comparison with human sensitivity,” in 2012 4th IEEE
[175] J.Gao,X.Yao,andC.Xu,“Fast-slowtest-timeadaptationforonline RAS & EMBS international conference on biomedical robotics and
vision-and-languagenavigation,”2024. biomechatronics(BioRob). IEEE,2012,pp.1122–1127.
[176] Y. Long, X. Li, W. Cai, and H. Dong, “Discuss before moving: [199] P. Ruppel, Y. Jonetzko, M. Go¨rner, N. Hendrich, and J. Zhang,
Visual language navigation via multi-expert discussions,” in IEEE “Simulationofthesyntouchbiotacsensor,”inIntelligentAutonomous
InternationalConferenceonRoboticsandAutomation2024,2024. Systems15:Proceedingsofthe15thInternationalConferenceIAS-15.
[177] R.D.M.S.C.L.Q.C.LiuyiWang,ZongtaoHe,“Vision-and-language Springer,2019,pp.374–387.
navigationviacausallearning,”inIEEE/CVFConferenceonComputer [200] W.Yuan,S.Dong,andE.H.Adelson,“Gelsight:High-resolutionrobot
VisionandPatternRecognition2024,Jun2024. tactile sensors for estimating geometry and force,” Sensors (Basel,
[178] Y.Y.RuiLiu,WenguanWang,“Volumetricenvironmentrepresentation Switzerland),vol.17,2017.
forvision-languagenavigation,”inIEEE/CVFConferenceonComputer
[201] I. Taylor, S. Dong, and A. Rodriguez, “Gelslim3.0: High-resolution
VisionandPatternRecognition2024,Jun2024.
measurement of shape, force and slip in a compact tactile-sensing
[179] J. Zhang, K. Wang, R. Xu, G. Zhou, Y. Hong, X. Fang, Q. Wu, finger,”2021.
Z.Zhang,andW.He,“Navid:Video-basedvlmplansthenextstepfor
[202] M. Lambeta, P.-W. Chou, S. Tian, B. Yang, B. Maloon, V. R. Most,
vision-and-languagenavigation,”ArXiv,vol.abs/2402.15852,2024.
D. Stroud, R. Santos, A. Byagowi, G. Kammerer, D. Jayaraman, and
[180] X.E.Wang,W.Xiong,H.Wang,andW.Y.Wang,“Lookbeforeyou
R. Calandra, “Digit: A novel design for a low-cost compact high-
leap:Bridgingmodel-freeandmodel-basedreinforcementlearningfor
resolution tactile sensor with application to in-hand manipulation,”
planned-ahead vision-and-language navigation,” in European Confer-
IEEE Robotics and Automation Letters, vol. 5, no. 3, p. 3838–3845,
enceonComputerVision,2018.
Jul.2020.
[181] D.An,Y.Qi,Y.Huang,Q.Wu,L.Wang,andT.Tan,“Neighbor-view
[203] C. Lin, H. Zhang, J. Xu, L. Wu, and H. Xu, “9dtact: A compact
enhanced model for vision and language navigation,” in Proceedings
vision-based tactile sensor for accurate 3d shape reconstruction and
of the 29th ACM International Conference on Multimedia, 2021, pp.
generalizable6dforceestimation,”2023.
5101–5109.
[204] B. Ward-Cherrier, N. Pestell, L. Cramphorn, B. Winstone, M. E.
[182] Y. Hong, Z. Wang, Q. Wu, and S. Gould, “Bridging the gap be-
Giannaccini, J. Rossiter, and N. F. Lepora, “The tactip family: Soft
tween learning in discrete and continuous environments for vision-
opticaltactilesensorswith3d-printedbiomimeticmorphologies,”Soft
and-languagenavigation,”in2022IEEE/CVFConferenceonComputer
robotics,vol.5,no.2,pp.216–227,2018.
VisionandPatternRecognition(CVPR),2022,pp.15418–15428.
[205] D.F.Gomes,Z.Lin,andS.Luo,“Geltip:Afinger-shapedopticaltactile
[183] Y.Qiao,Y.Qi,Z.Yu,J.Liu,andQ.Wu,“Marchinchat:Interactive
sensor for robotic manipulation,” in 2020 IEEE/RSJ International
promptingforremoteembodiedreferringexpression,”inProceedings
Conference on Intelligent Robots and Systems (IROS). IEEE, 2020,
oftheIEEE/CVFInternationalConferenceonComputerVision(ICCV),
pp.9903–9909.
October2023,pp.15758–15767.
[206] O. Azulay, N. Curtis, R. Sokolovsky, G. Levitski, D. Slomovik,
[184] Z. Wang, X. Li, J. Yang, Y. Liu, J. Hu, M. Jiang, and S. Jiang,
G. Lilling, and A. Sintov, “Allsight: A low-cost and high-resolution
“Lookahead exploration with neural radiance representation for con-
roundtactilesensorwithzero-shotlearningcapability,”IEEERobotics
tinuousvision-languagenavigation,”inProceedingsoftheIEEE/CVF
andAutomationLetters,vol.9,no.1,pp.483–490,2023.
ConferenceonComputerVisionandPatternRecognition,2024.
[207] S. Wang, M. Lambeta, P.-W. Chou, and R. Calandra, “Tacto: A fast,
[185] D.An,H.Wang,W.Wang,Z.Wang,Y.Huang,K.He,andL.Wang,
flexible, and open-source simulator for high-resolution vision-based
“Etpnav:Evolvingtopologicalplanningforvision-languagenavigation
tactilesensors,”IEEERoboticsandAutomationLetters,vol.7,no.2,
in continuous environments,” IEEE Transactions on Pattern Analysis
pp.3930–3937,2022.
andMachineIntelligence,2024.
[186] S.Bhambri,B.Kim,andJ.Choi,“Multi-levelcompositionalreasoning [208] Z. Si and W. Yuan, “Taxim: An example-based simulation model for
for interactive instruction following,” in Proceedings of the AAAI gelsighttactilesensors,”IEEERoboticsandAutomationLetters,vol.7,
Conference on Artificial Intelligence, vol. 37, no. 1, 2023, pp. 223– no.2,pp.2361–2368,2022.
231. [209] S.Athar,G.Patel,Z.Xu,Q.Qiu,andY.She,“Vistactowardsaunified
[187] C.Xu,H.T.Nguyen,C.Amato,andL.L.Wong,“Visionandlanguage multi-modal sensing finger for robotic manipulation,” IEEE Sensors
navigation in the real world via online visual language mapping,” Journal,2023.
ArXiv,vol.abs/2310.10822,2023. [210] Y. Ma, E. Adelson et al., “Gellink: A compact multi-phalanx finger
[188] Y. Liu, K. Wang, G. Li, and L. Lin, “Semantics-aware adaptive with vision-based tactile sensing and proprioception,” arXiv preprint
knowledge distillation for sensor-to-vision action recognition,” IEEE arXiv:2403.14887,2024.
TransactionsonImageProcessing,vol.30,pp.5573–5588,2021. [211] R. Girdhar, A. El-Nouby, Z. Liu, M. Singh, K. V. Alwala, A. Joulin,
[189] T. Lin, Y. Zhang, Q. Li, H. Qi, B. Yi, S. Levine, and J. Malik, and I. Misra, “Imagebind: One embedding space to bind them all,”
“Learning visuotactile skills with two multifingered hands,” arXiv inProceedingsoftheIEEE/CVFConferenceonComputerVisionand
preprintarXiv:2404.16823,2024. PatternRecognition,2023,pp.15180–15190.
[190] F. Yang, C. Feng, Z. Chen, H. Park, D. Wang, Y. Dou, Z. Zeng, [212] R.Calandra,A.Owens,M.Upadhyaya,W.Yuan,J.Lin,E.H.Adelson,
X. Chen, R. Gangopadhyay, A. Owens et al., “Binding touch to andS.Levine,“Thefeelingofsuccess:Doestouchsensinghelppredict
everything:Learningunifiedmultimodaltactilerepresentations,”arXiv graspoutcomes?”arXivpreprintarXiv:1710.05512,2017.
preprintarXiv:2401.18084,2024. [213] B. Sundaralingam, A. S. Lambert, A. Handa, B. Boots, T. Hermans,
[191] P. Achenbach, S. Laux, D. Purdack, P. N. Mu¨ller, and S. Go¨bel, S.Birchfield,N.Ratliff,andD.Fox,“Robustlearningoftactileforce
“Givemeasign:Usingdataglovesforstatichand-shaperecognition,” estimationthroughrobotinteraction,”in2019InternationalConference
Sensors,vol.23,no.24,p.9847,2023. onRoboticsandAutomation(ICRA). IEEE,2019,pp.9035–9042.
[192] S.J.LedermanandR.L.Klatzky,“Handmovements:Awindowinto [214] R.Gao,Y.-Y.Chang,S.Mall,L.Fei-Fei,andJ.Wu,“Objectfolder:A
haptic object recognition,” Cognitive psychology, vol. 19, no. 3, pp. datasetofobjectswithimplicitvisual,auditory,andtactilerepresenta-
342–368,1987. tions,”arXivpreprintarXiv:2109.07991,2021.
[193] M.R.Cutkosky,R.D.Howe,andW.R.Provancher,“Forceandtactile [215] R.Gao,Z.Si,Y.-Y.Chang,S.Clarke,J.Bohg,L.Fei-Fei,W.Yuan,and
sensors,”SpringerHandbookofRobotics,pp.455–476,2008. J. Wu, “Objectfolder 2.0: A multisensory object dataset for sim2realIEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 34
transfer,” in Proceedings of the IEEE/CVF conference on computer [236] S. Yu, K. Lin, A. Xiao, J. Duan, and H. Soh, “Octopi: Object
visionandpatternrecognition,2022,pp.10598–10608. propertyreasoningwithlargetactile-languagemodels,”arXivpreprint
[216] J. Kerr, H. Huang, A. Wilcox, R. Hoque, J. Ichnowski, R. Calandra, arXiv:2405.02794,2024.
and K. Goldberg, “Self-supervised visuo-tactile pretraining to locate [237] S.Suresh,Z.Si,J.G.Mangelson,W.Yuan,andM.Kaess,“Shapemap
andfollowgarmentfeatures,”arXivpreprintarXiv:2209.13042,2022. 3-d:Efficientshapemappingthroughdensetouchandvision,”in2022
[217] S.Suresh,Z.Si,S.Anderson,M.Kaess,andM.Mukadam,“Midas- InternationalConferenceonRoboticsandAutomation(ICRA). IEEE,
touch: Monte-carlo inference over distributions across sliding touch,” 2022,pp.7073–7080.
inConferenceonRobotLearning. PMLR,2023,pp.319–331. [238] E. Smith, R. Calandra, A. Romero, G. Gkioxari, D. Meger, J. Malik,
[218] R.Gao,Y.Dou,H.Li,T.Agarwal,J.Bohg,Y.Li,L.Fei-Fei,andJ.Wu, and M. Drozdzal, “3d shape reconstruction from vision and touch,”
“The objectfolder benchmark: Multisensory learning with neural and Advances in Neural Information Processing Systems, vol. 33, pp.
realobjects,”inProceedingsoftheIEEE/CVFConferenceonComputer 14193–14206,2020.
VisionandPatternRecognition,2023,pp.17276–17286. [239] E.Smith,D.Meger,L.Pineda,R.Calandra,J.Malik,A.RomeroSori-
[219] L. Fu, G. Datta, H. Huang, W. C.-H. Panitch, J. Drake, J. Ortiz, ano,andM.Drozdzal,“Active3dshapereconstructionfromvisionand
M.Mukadam,M.Lambeta,R.Calandra,andK.Goldberg,“Atouch, touch,” Advances in Neural Information Processing Systems, vol. 34,
vision,andlanguagedatasetformultimodalalignment,”arXivpreprint pp.16064–16078,2021.
arXiv:2402.13232,2024. [240] M.Comi,A.Church,K.Li,L.Aitchison,andN.F.Lepora,“Implicit
[220] J.Hansen,F.Hogan,D.Rivkin,D.Meger,M.Jenkin,andG.Dudek, neural representation for 3d shape reconstruction using vision-based
“Visuotactile-rl:Learningmultimodalmanipulationpolicieswithdeep tactilesensing,”2023.
reinforcementlearning,”in2022InternationalConferenceonRobotics [241] A.Das,S.Datta,G.Gkioxari,S.Lee,D.Parikh,andD.Batra,“Em-
andAutomation(ICRA). IEEE,2022,pp.8298–8304. bodiedquestionanswering,”inProceedingsoftheIEEEconferenceon
[221] H. Qi, B. Yi, S. Suresh, M. Lambeta, Y. Ma, R. Calandra, and computervisionandpatternrecognition,2018,pp.1–10.
J. Malik, “General in-hand object rotation with vision and touch,” [242] L. Yu, X. Chen, G. Gkioxari, M. Bansal, T. L. Berg, and D. Batra,
in Proceedings of The 7th Conference on Robot Learning, ser. “Multi-target embodied question answering,” in Proceedings of the
ProceedingsofMachineLearningResearch,J.Tan,M.Toussaint,and IEEE/CVF Conference on Computer Vision and Pattern Recognition,
K.Darvish,Eds.,vol.229. PMLR,06–09Nov2023,pp.2549–2564. 2019,pp.6309–6318.
[Online].Available:https://proceedings.mlr.press/v229/qi23a.html
[243] E. Wijmans, S. Datta, O. Maksymets, A. Das, G. Gkioxari, S. Lee,
[222] M.Yang,Y.Lin,A.Church,J.Lloyd,D.Zhang,D.A.Barton,andN.F.
I. Essa, D. Parikh, and D. Batra, “Embodied question answering in
Lepora,“Sim-to-realmodel-basedandmodel-freedeepreinforcement photorealisticenvironmentswithpointcloudperception,”inProceed-
learning for tactile pushing,” IEEE Robotics and Automation Letters, ings of the IEEE/CVF Conference on Computer Vision and Pattern
2023. Recognition,2019,pp.6659–6668.
[223] M. Yang, C. Lu, A. Church, Y. Lin, C. Ford, H. Li, E. Pso-
[244] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox, and
mopoulou, D. A. Barton, and N. F. Lepora, “Anyrotate: Gravity-
A. Farhadi, “Iqa: Visual question answering in interactive environ-
invariantin-handobjectrotationwithsim-to-realtouch,”arXivpreprint
ments,” in Proceedings of the IEEE conference on computer vision
arXiv:2405.07391,2024.
andpatternrecognition,2018,pp.4089–4098.
[224] X. Jing, K. Qian, T. Jianu, and S. Luo, “Unsupervised adversarial
[245] C. Cangea, E. Belilovsky, P. Lio`, and A. Courville, “Videonavqa:
domain adaptation for sim-to-real transfer of tactile images,” IEEE
Bridging the gap between visual and embodied question answering,”
TransactionsonInstrumentationandMeasurement,2023.
arXivpreprintarXiv:1908.04950,2019.
[225] B. Duan, K. Qian, Y. Zhao, D. Zhang, and S. Luo, “Feature-level
[246] S. Tan, M. Ge, D. Guo, H. Liu, and F. Sun, “Knowledge-based
sim2realregressionoftactileimagesforrobotmanipulation.”
embodiedquestionanswering,”IEEETransactionsonPatternAnalysis
[226] Y. Lin, J. Lloyd, A. Church, and N. F. Lepora, “Tactile gym 2.0:
andMachineIntelligence,2023.
Sim-to-realdeepreinforcementlearningforcomparinglow-costhigh-
[247] A.Majumdar,A.Ajay,X.Zhang,P.Putta,S.Yenamandra,M.Henaff,
resolutionrobottouch,”IEEERoboticsandAutomationLetters,vol.7,
S. Silwal, P. Mcvay, O. Maksymets, S. Arnaud et al., “Openeqa:
no.4,pp.10754–10761,2022.
Embodied question answering in the era of foundation models,” in
[227] M. Polic, I. Krajacic, N. Lepora, and M. Orsag, “Convolutional
Proceedings of the IEEE/CVF Conference on Computer Vision and
autoencoder for feature extraction in tactile sensing,” IEEE Robotics
PatternRecognition,2024,pp.16488–16498.
andAutomationLetters,vol.4,no.4,pp.3671–3678,2019.
[248] A.Z.Ren,J.Clark,A.Dixit,M.Itkina,A.Majumdar,andD.Sadigh,
[228] R. Gao, T. Taunyazov, Z. Lin, and Y. Wu, “Supervised autoencoder
“Explore until confident: Efficient exploration for embodied question
jointlearningonheterogeneoustactilesensorydata:Improvingmaterial
answering,”arXivpreprintarXiv:2403.15941,2024.
classification performance,” in 2020 IEEE/RSJ International Confer-
ence on Intelligent Robots and Systems (IROS). IEEE, 2020, pp. [249] V. S. Dorbala, P. Goyal, R. Piramuthu, M. Johnston, D. Manocha,
and R. Ghanadhan, “S-eqa: Tackling situational queries in embodied
10907–10913.
questionanswering,”arXivpreprintarXiv:2405.04732,2024.
[229] G. Cao, J. Jiang, D. Bollegala, and S. Luo, “Learn from incomplete
tactiledata:Tactilerepresentationlearningwithmaskedautoencoders,” [250] Y. Wu, Y. Wu, G. Gkioxari, and Y. Tian, “Building generalizable
in2023IEEE/RSJInternationalConferenceonIntelligentRobotsand agents with a realistic and rich 3d environment,” arXiv preprint
Systems(IROS). IEEE,2023,pp.10800–10805. arXiv:1801.02209,2018.
[230] Z. Zhang, G. Zheng, X. Ji, G. Chen, R. Jia, W. Chen, G. Chen, [251] M.Savva,A.X.Chang,A.Dosovitskiy,T.Funkhouser,andV.Koltun,
L. Zhang, and J. Pan, “Mae4gm: Visuo-tactile learning for property “Minos:Multimodalindoorsimulatorfornavigationincomplexenvi-
estimationofgranularmaterialusingmultimodalautoencoder.” ronments,”arXivpreprintarXiv:1712.03931,2017.
[231] W. Yuan, S. Wang, S. Dong, and E. Adelson, “Connecting look and [252] A.Chang,A.Dai,T.Funkhouser,M.Halber,M.Niessner,M.Savva,
feel:Associatingthevisualandtactilepropertiesofphysicalmaterials,” S.Song,A.Zeng,andY.Zhang,“Matterport3d:Learningfromrgb-d
inProceedingsoftheIEEEconferenceoncomputervisionandpattern datainindoorenvironments,”arXivpreprintarXiv:1709.06158,2017.
recognition,2017,pp.5580–5588. [253] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and
[232] M. A. Lee, Y. Zhu, P. Zachares, M. Tan, K. Srinivasan, S. Savarese, M. Nießner, “Scannet: Richly-annotated 3d reconstructions of indoor
L. Fei-Fei, A. Garg, and J. Bohg, “Making sense of vision and scenes,” in Proceedings of the IEEE conference on computer vision
touch: Learning multimodal representations for contact-rich tasks,” andpatternrecognition,2017,pp.5828–5839.
IEEETransactionsonRobotics,vol.36,no.3,pp.582–596,2020. [254] S. K. Ramakrishnan, A. Gokaslan, E. Wijmans, O. Maksymets,
[233] J. Lin, R. Calandra, and S. Levine, “Learning to identify object A. Clegg, J. Turner, E. Undersander, W. Galuba, A. Westbury, A. X.
instances by touch: Tactile recognition via multimodal matching,” in Changetal.,“Habitat-matterport3ddataset(hm3d):1000large-scale
2019 International Conference on Robotics and Automation (ICRA). 3d environments for embodied ai,” arXiv preprint arXiv:2109.08238,
IEEE,2019,pp.3644–3650. 2021.
[234] F.Yang,C.Ma,J.Zhang,J.Zhu,W.Yuan,andA.Owens,“Touchand [255] A.Das,G.Gkioxari,S.Lee,D.Parikh,andD.Batra,“Neuralmodular
go: Learning fromhuman-collected vision and touch,”arXiv preprint control for embodied question answering,” in Conference on Robot
arXiv:2211.12498,2022. Learning. PMLR,2018,pp.53–62.
[235] I.Guzey,B.Evans,S.Chintala,andL.Pinto,“Dexterityfromtouch: [256] Y. Wu, L. Jiang, and Y. Yang, “Revisiting embodiedqa: A simple
Self-supervised pre-training of tactile representations with robotic baselineandbeyond,”IEEETransactionsonImageProcessing,vol.29,
play,”arXivpreprintarXiv:2303.12076,2023. pp.3984–3992,2020.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 35
[257] S.Tan,W.Xiang,H.Liu,D.Guo,andF.Sun,“Multi-agentembodied [279] Z.Xi,W.Chen,X.Guo,W.He,Y.Ding,B.Hong,M.Zhang,J.Wang,
questionansweringininteractiveenvironments,”inComputerVision– S.Jin,E.Zhouetal.,“Theriseandpotentialoflargelanguagemodel
ECCV2020:16thEuropeanConference,Glasgow,UK,August23–28, basedagents:Asurvey,”arXivpreprintarXiv:2309.07864,2023.
2020,Proceedings,PartXIII16. Springer,2020,pp.663–678. [280] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho,
[258] B.Yamauchi,“Afrontier-basedapproachforautonomousexploration,” J.Ibarz,A.Irpan,E.Jang,R.Julianetal.,“Doasican,notasisay:
inProceedings1997IEEEInternationalSymposiumonComputational Grounding language in robotic affordances,” in Conference on robot
IntelligenceinRoboticsandAutomationCIRA’97.’TowardsNewCom- learning. PMLR,2023,pp.287–318.
putationalPrinciplesforRoboticsandAutomation’. IEEE,1997,pp. [281] Y.Chebotar,Q.Vuong,K.Hausman,F.Xia,Y.Lu,A.Irpan,A.Kumar,
146–151. T. Yu, A. Herzog, K. Pertsch et al., “Q-transformer: Scalable offline
[259] K.Sakamoto,D.Azuma,T.Miyanishi,S.Kurita,andM.Kawanabe, reinforcement learning via autoregressive q-functions,” in Conference
“Map-based modular approach for zero-shot embodied question an- onRobotLearning. PMLR,2023,pp.3909–3928.
swering,”arXivpreprintarXiv:2405.16559,2024. [282] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,
[260] B.Patel,V.S.Dorbala,andA.S.Bedi,“Embodiedquestionanswering A.Wahid,J.Tompson,Q.Vuong,T.Yuetal.,“Palm-e:Anembodied
viamulti-llmsystems,”2024. multimodallanguagemodel,”arXivpreprintarXiv:2303.03378,2023.
[261] T.Brown,B.Mann,N.Ryder,M.Subbiah,J.D.Kaplan,P.Dhariwal, [283] B.Zitkovich,T.Yu,S.Xu,P.Xu,T.Xiao,F.Xia,J.Wu,P.Wohlhart,
A.Neelakantan,P.Shyam,G.Sastry,A.Askelletal.,“Languagemod- S. Welker, A. Wahid et al., “Rt-2: Vision-language-action models
elsarefew-shotlearners,”Advancesinneuralinformationprocessing transfer web knowledge to robotic control,” in Conference on Robot
systems,vol.33,pp.1877–1901,2020. Learning. PMLR,2023,pp.2165–2183.
[262] R.Newbury,M.Gu,L.Chumbley,A.Mousavian,C.Eppner,J.Leitner, [284] Padalkar et al., “Open x-embodiment: Robotic learning datasets and
J. Bohg, A. Morales, T. Asfour, D. Kragic et al., “Deep learning rt-xmodels,”arXivpreprintarXiv:2310.08864,2023.
approaches to grasp synthesis: A review,” IEEE Transactions on [285] Y.Mu,Q.Zhang,M.Hu,W.Wang,M.Ding,J.Jin,B.Wang,J.Dai,
Robotics,2023. Y. Qiao, and P. Luo, “Embodiedgpt: Vision-language pre-training
[263] S.AinetterandF.Fraundorfer,“End-to-endtrainabledeepneuralnet- via embodied chain of thought,” Advances in Neural Information
workforroboticgraspdetectionandsemanticsegmentationfromrgb,” ProcessingSystems,vol.36,2024.
in 2021 IEEE International Conference on Robotics and Automation [286] X. Li, M. Liu, H. Zhang, C. Yu, J. Xu, H. Wu, C. Cheang, Y. Jing,
(ICRA). IEEE,2021,pp.13452–13458. W. Zhang, H. Liu et al., “Vision-language foundation models as
[264] A. Depierre, E. Dellandre´a, and L. Chen, “Jacquard: A large scale effectiverobotimitators,”arXivpreprintarXiv:2311.01378,2023.
dataset for robotic grasp detection,” in 2018 IEEE/RSJ International [287] M. Ahn, D. Dwibedi, C. Finn, M. G. Arenas, K. Gopalakrishnan,
Conference on Intelligent Robots and Systems (IROS). IEEE, 2018, K. Hausman, B. Ichter, A. Irpan, N. Joshi, R. Julian et al., “Autort:
pp.3511–3516. Embodied foundation models for large scale orchestration of robotic
[265] Y. Jiang, S. Moseson, and A. Saxena, “Efficient grasping from rgbd agents,”arXivpreprintarXiv:2401.12963,2024.
images:Learningusinganewrectanglerepresentation,”in2011IEEE [288] I. Leal, K. Choromanski, D. Jain, A. Dubey, J. Varley, M. Ryoo,
International conference on robotics and automation. IEEE, 2011, Y. Lu, F. Liu, V. Sindhwani, Q. Vuong et al., “Sara-rt: Scaling
pp.3304–3311. up robotics transformers with self-adaptive robust attention,” arXiv
[266] J.Varley,C.DeChant,A.Richardson,J.Ruales,andP.Allen,“Shape preprintarXiv:2312.01990,2023.
completionenabledroboticgrasping,”in2017IEEE/RSJinternational [289] J. Liu, M. Liu, Z. Wang, L. Lee, K. Zhou, P. An, S. Yang,
conferenceonintelligentrobotsandsystems(IROS). IEEE,2017,pp. R.Zhang,Y.Guo,andS.Zhang,“Robomamba:Multimodalstatespace
2442–2447. modelforefficientrobotreasoningandmanipulation,”arXivpreprint
[267] H.-S.Fang,C.Wang,M.Gou,andC.Lu,“Graspnet-1billion:Alarge- arXiv:2406.04339,2024.
scale benchmark for general object grasping,” in Proceedings of the [290] J. Gu, S. Kirmani, P. Wohlhart, Y. Lu, M. G. Arenas, K. Rao,
IEEE/CVF conference on computer vision and pattern recognition, W.Yu,C.Fu,K.Gopalakrishnan,Z.Xuetal.,“Rt-trajectory:Robotic
2020,pp.11444–11453. task generalization via hindsight trajectory sketches,” arXiv preprint
[268] C.Eppner,A.Mousavian,andD.Fox,“Acronym:Alarge-scalegrasp arXiv:2311.01977,2023.
dataset basedon simulation,” in2021 IEEEInternational Conference [291] R. E. Fikes and N. J. Nilsson, “Strips: A new approach to the appli-
onRoboticsandAutomation(ICRA). IEEE,2021,pp.6222–6227. cationoftheoremprovingtoproblemsolving,”ArtificialIntelligence,
[269] A. Mousavian, C. Eppner, and D. Fox, “6-dof graspnet: Variational vol.2,no.3,pp.189–208,1971.
grasp generation for object manipulation,” in Proceedings of the [292] D. McDermott, M. Ghallab, A. E. Howe, C. A. Knoblock, A. Ram,
IEEE/CVF international conference on computer vision, 2019, pp. M. M. Veloso, D. S. Weld, and D. E. Wilkins, “Pddl-the planning
2901–2910. domaindefinitionlanguage,”1998.
[270] L. F. C. Murrilo, N. Khargonkar, B. Prabhakaran, and Y. Xiang, [293] N.C.MetropolisandS.M.Ulam,“Themontecarlomethod.”Journal
“Multigrippergrasp: A dataset for robotic grasping from parallel jaw oftheAmericanStatisticalAssociation,vol.44247,pp.335–41,1949.
gripperstodexteroushands,”arXivpreprintarXiv:2403.09841,2024. [294] P. E. Hart, N. J. Nilsson, and B. Raphael, “A formal basis for the
[271] G. Tziafas, Y. Xu, A. Goel, M. Kasaei, Z. Li, and H. Kasaei, heuristicdeterminationofminimumcostpaths,”IEEETrans.Syst.Sci.
“Language-guidedrobotgrasping:Clip-basedreferringgraspsynthesis Cybern.,vol.4,pp.100–107,1968.
inclutter,”arXivpreprintarXiv:2311.05779,2023. [295] M.Shridhar,J.Thomason,D.Gordon,Y.Bisk,W.Han,R.Mottaghi,
[272] S.Jin,J.Xu,Y.Lei,andL.Zhang,“Reasoninggraspingviamultimodal L. Zettlemoyer, and D. Fox, “Alfred: A benchmark for interpreting
largelanguagemodel,”arXivpreprintarXiv:2402.06798,2024. grounded instructions for everyday tasks,” in Proceedings of the
[273] J.Xu,S.Jin,Y.Lei,Y.Zhang,andL.Zhang,“Reasoningtuninggrasp: IEEE/CVF conference on computer vision and pattern recognition,
Adapting multi-modal large language models for robotic grasping,” 2020,pp.10740–10749.
in 2nd Workshop on Language and Robot Learning: Language as [296] M. Shridhar, X. Yuan, M.-A. Coˆte´, Y. Bisk, A. Trischler, and
Grounding,2023. M.Hausknecht,“Alfworld:Aligningtextandembodiedenvironments
[274] K. Li, J. Wang, L. Yang, C. Lu, and B. Dai, “Semgrasp: Semantic forinteractivelearning,”arXivpreprintarXiv:2010.03768,2020.
grasp generation via language aligned discretization,” arXiv preprint [297] S.Y.Min,D.S.Chaplot,P.Ravikumar,Y.Bisk,andR.Salakhutdinov,
arXiv:2404.03590,2024. “Film: Following instructions in language with modular methods,”
[275] M. Shridhar, L. Manuelli, and D. Fox, “Cliport: What and where arXivpreprintarXiv:2110.07342,2021.
pathwaysforroboticmanipulation,”inConferenceonrobotlearning. [298] W.Huang,P.Abbeel,D.Pathak,andI.Mordatch,“Languagemodels
PMLR,2022,pp.894–906. as zero-shot planners: Extracting actionable knowledge for embodied
[276] W.Shen,G.Yang,A.Yu,J.Wong,L.P.Kaelbling,andP.Isola,“Dis- agents,” in International conference on machine learning. PMLR,
tilledfeaturefieldsenablefew-shotlanguage-guidedmanipulation,”in 2022,pp.9118–9147.
7thAnnualConferenceonRobotLearning,2023. [299] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,
[277] Y. Zheng, X. Chen, Y. Zheng, S. Gu, R. Yang, B. Jin, P. Li, J.Tompson,I.Mordatch,Y.Chebotar,P.Sermanet,N.Brown,T.Jack-
C. Zhong, Z. Wang, L. Liu et al., “Gaussiangrasper: 3d language son,L.Luu,S.Levine,K.Hausman,andB.Ichter,“InnerMonologue:
gaussiansplattingforopen-vocabularyroboticgrasping,”arXivpreprint Embodied Reasoning through Planning with Language Models,” Jul.
arXiv:2403.09637,2024. 2022.
[278] H.-S.Fang,C.Wang,H.Fang,M.Gou,J.Liu,H.Yan,W.Liu,Y.Xie, [300] W.Huang,P.Abbeel,D.Pathak,andI.Mordatch,“LanguageModels
andC.Lu,“Anygrasp:Robustandefficientgraspperceptioninspatial asZero-ShotPlanners:ExtractingActionableKnowledgeforEmbodied
andtemporaldomains,”IEEETransactionsonRobotics,2023. Agents,”Mar.2022.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 36
[301] J.-W. Choi, Y. Yoon, H. Ong, J. Kim, and M. Jang, “Lota-bench: [325] Y.Mu,Q.Zhang,M.Hu,W.Wang,M.Ding,J.Jin,B.Wang,J.Dai,
Benchmarking language-oriented task planners for embodied agents,” Y. Qiao, and P. Luo, “Embodiedgpt: Vision-language pre-training via
arXivpreprintarXiv:2402.08178,2024. embodiedchainofthought,”ArXiv,vol.abs/2305.15021,2023.
[302] C.H.Song,J.Wu,C.Washington,B.M.Sadler,W.-L.Chao,andY.Su, [326] Z. Wu, Z. Wang, X. Xu, J. Lu, and H. Yan, “Embodied instruction
“Llm-planner:Few-shotgroundedplanningforembodiedagentswith followinginunknownenvironments,”2024.
largelanguagemodels,”2023IEEE/CVFInternationalConferenceon [327] X. Zhao, M. Li, C. Weber, M. B. Hafez, and S. Wermter, “Chat
ComputerVision(ICCV),pp.2986–2997,2022. withtheEnvironment:InteractiveMultimodalPerceptionUsingLarge
[303] G. Sarch, Y. Wu, M. J. Tarr, and K. Fragkiadaki, “Open-Ended In- LanguageModels,”Oct.2023.
structableEmbodiedAgentswithMemory-AugmentedLargeLanguage [328] Y.Du,M.Yang,P.Florence,F.Xia,A.Wahid,B.Ichter,P.Sermanet,
Models,”Nov.2023. T. Yu, P. Abbeel, J. B. Tenenbaum, L. Kaelbling, A. Zeng, and
[304] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, J.Tompson,“VideoLanguagePlanning,”Oct.2023.
andA.Anandkumar,“Voyager:AnOpen-EndedEmbodiedAgentwith [329] N. Shinn, B. Labash, and A. Gopinath, “Reflexion: an au-
LargeLanguageModels,”Oct.2023. tonomousagentwithdynamicmemoryandself-reflection,”ArXiv,vol.
[305] P.Sharma,A.Torralba,andJ.Andreas,“Skillinductionandplanning abs/2303.11366,2023.
with latent language,” in Annual Meeting of the Association for [330] Z.Wang,S.Cai,A.Liu,X.Ma,andY.Liang,“Describe,explain,plan
ComputationalLinguistics,2021. and select: Interactive planning with large language models enables
[306] J. Wei, X. Wang, D. Schuurmans, M. Bosma, E. H. Chi, Q. Le, open-worldmulti-taskagents,”ArXiv,vol.abs/2302.01560,2023.
and D. Zhou, “Chain of thought prompting elicits reasoning in [331] A. Padalkar, A. Pooley, A. Jain, A. Bewley, A. Herzog, A. Irpan,
large language models,” CoRR, vol. abs/2201.11903, 2022. [Online]. A.Khazatsky,A.Rai,A.Singh,A.Brohanetal.,“Openx-embodiment:
Available:https://arxiv.org/abs/2201.11903 Roboticlearningdatasetsandrt-xmodels,”2023.
[307] S.Yao,J.Zhao,D.Yu,N.Du,I.Shafran,K.Narasimhan,andY.Cao, [332] D. Ha and J. Schmidhuber, “World models,” arXiv preprint
“ReAct:SynergizingReasoningandActinginLanguageModels,”Mar. arXiv:1803.10122,2018.
2023. [333] J. Xiang, G. Liu, Y. Gu, Q. Gao, Y. Ning, Y. Zha, Z. Feng,
[308] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, T. Tao, S. Hao, Y. Shi et al., “Pandora: Towards general world
D.Fox,J.Thomason,andA.Garg,“ProgPrompt:GeneratingSituated modelwithnaturallanguageactionsandvideostates,”arXivpreprint
RobotTaskPlansusingLargeLanguageModels,”Sep.2022. arXiv:2406.09455,2024.
[309] S.Vemprala,R.Bonatti,A.F.C.Bucker,andA.Kapoor,“Chatgptfor [334] H.Zhen,X.Qiu,P.Chen,J.Yang,X.Yan,Y.Du,Y.Hong,andC.Gan,
robotics:Designprinciplesandmodelabilities,”IEEEAccess,vol.12, “3d-vla: A 3d vision-language-action generative world model,” arXiv
pp.55682–55696,2023. preprintarXiv:2403.09631,2024.
[310] J.Liang,W.Huang,F.Xia,P.Xu,K.Hausman,B.Ichter,P.R.Flo- [335] Z.Ding,A.Zhang,Y.Tian,andQ.Zheng,“Diffusionworldmodel,”
rence,andA.Zeng,“Codeaspolicies:Languagemodelprogramsfor arXivpreprintarXiv:2402.03570,2024.
embodied control,” 2023 IEEE International Conference on Robotics [336] A. Bardes, J. Ponce, and Y. LeCun, “Mc-jepa: A joint-embedding
andAutomation(ICRA),pp.9493–9500,2022. predictive architecture for self-supervised learning of motion and
[311] A.Zeng,M.Attarian,B.Ichter,K.Choromanski,A.Wong,S.Welker, contentfeatures,”arXivpreprintarXiv:2307.12698,2023.
F.Tombari,A.Purohit,M.Ryoo,V.Sindhwani,J.Lee,V.Vanhoucke, [337] Z. Fei, M. Fan, and J. Huang, “A-jepa: Joint-embedding predictive
andP.Florence,“SocraticModels:ComposingZero-ShotMultimodal architecturecanlisten,”arXivpreprintarXiv:2311.15830,2023.
ReasoningwithLanguage,”May2022. [338] Q. Garrido, M. Assran, N. Ballas, A. Bardes, L. Najman, and Y. Le-
[312] S. Shin, S. jeon, J. Kim, G.-C. Kang, and B.-T. Zhang, “Socratic Cun, “Learning and leveraging world models in visual representation
planner: Inquiry-based zero-shot planning for embodied instruction learning,”arXivpreprintarXiv:2403.00504,2024.
following,”2024. [339] J. Wu, S. Yin, N. Feng, X. He, D. Li, J. Hao, and M. Long,
[313] Z. Zhao, W. S. Lee, and D. Hsu, “Large Language Models as Com- “ivideogpt: Interactive videogpts are scalable world models,” arXiv
monsenseKnowledgeforLarge-ScaleTaskPlanning,”Oct.2023. preprintarXiv:2405.15223,2024.
[314] S.Hao,Y.Gu,H.Ma,J.J.Hong,Z.Wang,D.Z.Wang,andZ.Hu, [340] J. Yang, B. Liu, J. Fu, B. Pan, G. Wu, and L. Wang, “Spatiotempo-
“ReasoningwithLanguageModelisPlanningwithWorldModel,”Oct. ral predictive pre-training for robotic motor control,” arXiv preprint
2023. arXiv:2403.05304,2024.
[315] H.Chang,K.Gao,K.Boyalakuntla,A.Lee,B.Huang,H.U.Kumar, [341] M. Burchi and R. Timofte, “Mudreamer: Learning predictive world
J. Yu, and A. Boularias, “Lgmcts: Language-guided monte-carlo tree models without reconstruction,” arXiv preprint arXiv:2405.15083,
searchforexecutablesemanticobjectrearrangement,”2023. 2024.
[316] Y.Xie,C.Yu,T.Zhu,J.Bai,Z.Gong,andH.Soh,“TranslatingNatural [342] L.Wong,G.Grand,A.K.Lew,N.D.Goodman,V.K.Mansinghka,
LanguagetoPlanningGoalswithLarge-LanguageModels,”Feb.2023. J.Andreas,andJ.B.Tenenbaum,“Fromwordmodelstoworldmodels:
[317] B.Liu,Y.Jiang,X.Zhang,Q.Liu,S.Zhang,J.Biswas,andP.Stone, Translating from natural language to the probabilistic language of
“LLM+P:EmpoweringLargeLanguageModelswithOptimalPlanning thought,”arXivpreprintarXiv:2306.12672,2023.
Proficiency,”Sep.2023. [343] Y.Feng,Y.Shang,X.Feng,L.Lan,S.Zhe,T.Shao,H.Wu,K.Zhou,
[318] T.Silver,S. Dan,K.Srinivas,J. B.Tenenbaum,L. P.Kaelbling,and H.Su,C.Jiangetal.,“Elastogen:4dgenerativeelastodynamics,”arXiv
M. Katz, “Generalized Planning in PDDL Domains with Pretrained preprintarXiv:2405.15056,2024.
LargeLanguageModels,”Dec.2023. [344] M. Liu, C. Xu, H. Jin, L. Chen, M. Varma T, Z. Xu, and H. Su,
[319] G. Dagan, F. Keller, and A. Lascarides, “Dynamic Planning with a “One-2-3-45: Any single image to 3d mesh in 45 seconds without
LLM,”Aug.2023. per-shape optimization,” Advances in Neural Information Processing
[320] Y.Zhang,S.Yang,C.Bai,F.Wu,X.Li,X.Li,andZ.Wang,“Towards Systems,vol.36,2024.
efficientllmgroundingforembodiedmulti-agentcollaboration,”arXiv [345] Z.Cheng,Z.Wang,J.Hu,S.Hu,A.Liu,Y.Tu,P.Li,L.Shi,Z.Liu,and
preprintarXiv:2405.14314,2024. M.Sun,“Legent:Openplatformforembodiedagents,”arXivpreprint
[321] Z.Wu,Z.Wang,X.Xu,J.Lu,andH.Yan,“EmbodiedTaskPlanning arXiv:2404.18243,2024.
withLargeLanguageModels,”Jul.2023. [346] A.SaitoandJ.Poovvancheri,“Point-jepa:Ajointembeddingpredictive
[322] Y.Chen,W.Cui,Y.Chen,M.Tan,X.Zhang,D.Zhao,andH.Wang, architectureforself-supervisedlearningonpointcloud,”arXivpreprint
“Robogpt:anintelligentagentofmakingembodiedlong-termdecisions arXiv:2404.16432,2024.
fordailyinstructiontasks,”2023. [347] Y. LeCun, “A path towards autonomous machine intelligence version
[323] K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. D. Reid, and 0.9.2,2022-06-27,”OpenReview,vol.62,no.1,pp.1–62,2022.
N.Su¨nderhauf,“Sayplan:Groundinglargelanguagemodelsusing3d [348] A. Dawid and Y. LeCun, “Introduction to latent variable energy-
scene graphs for scalable task planning,” in Conference on Robot basedmodels:Apathtowardsautonomousmachineintelligence,”arXiv
Learning,2023. preprintarXiv:2306.02572,2023.
[324] Q. Gu, A. Kuwajerwala, S. Morin, K. M. Jatavallabhula, B. Sen, [349] V. Lim, H. Huang, L. Y. Chen, J. Wang, J. Ichnowski, D. Seita,
A.Agarwal,C.Rivera,W.Paul,K.Ellis,R.Chellappa,C.Gan,C.M. M.Laskey,andK.Goldberg,“Real2sim2real:Self-supervisedlearning
de Melo, J. B. Tenenbaum, A. Torralba, F. Shkurti, and L. Paull, of physical single-step dynamic actions for planar robot casting,” in
“Conceptgraphs:Open-vocabulary3dscenegraphsforperceptionand 2022 International Conference on Robotics and Automation (ICRA).
planning,”ArXiv,vol.abs/2309.16650,2023. IEEE,2022,pp.8282–8289.IEEETRANSACTIONSONPATTERNANALYSISANDMACHINEINTELLIGENCE 37
[350] C.Chi,Z.Xu,C.Pan,E.Cousineau,B.Burchfiel,S.Feng,R.Tedrake, [371] S.Feng,E.Whitman,X.Xinjilefu,andC.G.Atkeson,“Optimization
and S. Song, “Universal manipulation interface: In-the-wild robot based full body control for the atlas robot,” in 2014 IEEE-RAS
teachingwithoutin-the-wildrobots,”arXivpreprintarXiv:2402.10329, InternationalConferenceonHumanoidRobots. IEEE,2014,pp.120–
2024. 127.
[351] Z. Fu, T. Z. Zhao, and C. Finn, “Mobile aloha: Learning bimanual [372] Q.Nguyen,M.J.Powell,B.Katz,J.DiCarlo,andS.Kim,“Optimized
mobile manipulation with low-cost whole-body teleoperation,” arXiv jumpingonthemitcheetah3robot,”in2019InternationalConference
preprintarXiv:2401.02117,2024. onRoboticsandAutomation(ICRA). IEEE,2019,pp.7448–7454.
[352] S.Luo,Q.Peng,J.Lv,K.Hong,K.R.Driggs-Campbell,C.Lu,and [373] C.Nguyen,L.Bao,andQ.Nguyen,“Continuousjumpingforlegged
Y.-L.Li,“Human-agentjointlearningforefficientrobotmanipulation robotsonsteppingstonesviatrajectoryoptimizationandmodelpredic-
skillacquisition,”arXivpreprintarXiv:2407.00299,2024. tivecontrol,”in2022IEEE61stConferenceonDecisionandControl
[353] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian, (CDC). IEEE,2022,pp.93–99.
T.Armstrong,I.Krasin,D.Duong,V.Sindhwanietal.,“Transporter [374] C.Gehring,S.Coros,M.Hutter,C.D.Bellicoso,H.Heijnen,R.Di-
networks: Rearranging the visual world for robotic manipulation,” in ethelm,M.Bloesch,P.Fankhauser,J.Hwangbo,M.Hoepflingeretal.,
ConferenceonRobotLearning. PMLR,2021,pp.726–747. “Practicemakesperfect:Anoptimization-basedapproachtocontrolling
[354] H. Geng, H. Xu, C. Zhao, C. Xu, L. Yi, S. Huang, and H. Wang, agile motions for a quadruped robot,” IEEE Robotics & Automation
“Gapartnet:Cross-categorydomain-generalizableobjectperceptionand Magazine,vol.23,no.1,pp.34–43,2016.
manipulationviageneralizableandactionableparts,”inProceedingsof [375] Q.Nguyen,A.Agrawal,X.Da,W.C.Martin,H.Geyer,J.W.Grizzle,
the IEEE/CVF Conference on Computer Vision and Pattern Recogni- and K. Sreenath, “Dynamic walking on randomly-varying discrete
tion,2023,pp.7081–7091. terrainwithone-steppreview.”inRobotics:ScienceandSystems,vol.2,
[355] M. Torne, A. Simeonov, Z. Li, A. Chan, T. Chen, A. Gupta, no.3,2017,pp.384–399.
and P. Agrawal, “Reconciling reality through simulation: A real- [376] R.Antonova,A.Rai,andC.G.Atkeson,“Deepkernelsforoptimizing
to-sim-to-real approach for robust manipulation,” arXiv preprint locomotion controllers,” in Conference on Robot Learning. PMLR,
arXiv:2403.03949,2024. 2017,pp.47–56.
[356] Y.Jiang,C.Wang,R.Zhang,J.Wu,andL.Fei-Fei,“Transic:Sim-to- [377] X. Cheng, Y. Ji, J. Chen, R. Yang, G. Yang, and X. Wang, “Ex-
realpolicytransferbylearningfromonlinecorrection,”arXivpreprint pressive whole-body control for humanoid robots,” arXiv preprint
arXiv:2405.10315,2024. arXiv:2402.16796,2024.
[357] J.Tobin,R.Fong,A.Ray,J.Schneider,W.Zaremba,andP.Abbeel, [378] M. Chignoli, D. Kim, E. Stanger-Jones, and S. Kim, “The mit
“Domain randomization for transferring deep neural networks from humanoid robot: Design, motion planning, and control for acrobatic
simulation to the real world,” in 2017 IEEE/RSJ international con- behaviors,” in 2020 IEEE-RAS 20th International Conference on Hu-
ference on intelligent robots and systems (IROS). IEEE, 2017, pp. manoidRobots(Humanoids). IEEE,2021,pp.1–8.
23–30. [379] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta, “R3m: A
[358] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. Mc- universalvisualrepresentationforrobotmanipulation,”arXivpreprint
Grew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray et al., arXiv:2203.12601,2022.
“Learningdexterousin-handmanipulation,”TheInternationalJournal [380] A.Irpan,A.Herzog,A.T.Toshev,A.Zeng,A.Brohan,B.A.Ichter,
ofRoboticsResearch,vol.39,no.1,pp.3–20,2020. B.David,C.Parada,C.Finn,C.Tanetal.,“Doasican,notasisay:
[359] J. Matas, S. James, and A. J. Davison, “Sim-to-real reinforcement Grounding language in robotic affordances,” in Conference on Robot
learningfordeformableobjectmanipulation,”inConferenceonRobot Learning,no.2022,2022.
Learning. PMLR,2018,pp.734–743. [381] Y.Liu,G.Li,andL.Lin,“Cross-modalcausalrelationalreasoningfor
[360] M. Kaspar, J. D. M. Osorio, and J. Bock, “Sim2real transfer for event-levelvisualquestionanswering,”IEEETransactionsonPattern
reinforcement learning without dynamics randomization,” in 2020 AnalysisandMachineIntelligence,vol.45,no.10,pp.11624–11641,
IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems 2023.
(IROS). IEEE,2020,pp.4383–4388. [382] Y. Liu, Y.-S. Wei, H. Yan, G.-B. Li, and L. Lin, “Causal reasoning
[361] W. Yu, J. Tan, C. K. Liu, and G. Turk, “Preparing for the unknown: meets visual representation learning: A prospective study,” Machine
Learning a universal policy with online system identification,” arXiv IntelligenceResearch,vol.19,no.6,pp.485–511,2022.
preprintarXiv:1702.02453,2017. [383] Y. Wei, Y. Liu, H. Yan, G. Li, and L. Lin, “Visual causal scene
[362] A.Yu,A.Foote,R.Mooney,andR.Mart´ın-Mart´ın,“Naturallanguage refinement for video question answering,” in Proceedings of the 31st
can help bridge the sim2real gap,” arXiv preprint arXiv:2405.10020, ACMInternationalConferenceonMultimedia,2023,pp.377–386.
2024. [384] Z. Tang, R. Wang, W. Chen, K. Wang, Y. Liu, T. Chen, and L. Lin,
[363] C. Huang, G. Wang, Z. Zhou, R. Zhang, and L. Lin, “Reward- “Towards causalgpt: A multi-agent approach for faithful knowledge
adaptivereinforcementlearning:Dynamicpolicygradientoptimization reasoning via promoting causal consistency in llms,” arXiv preprint
for bipedal locomotion,” IEEE transactions on pattern analysis and arXiv:2308.11914,2023.
machineintelligence,vol.45,no.6,pp.7686–7695,2022. [385] S. V. Mehta, D. Patil, S. Chandar, and E. Strubell, “An empirical
[364] V.Tsounis,M.Alge,J.Lee,F.Farshidian,andM.Hutter,“Deepgait: investigation of the role of pre-training in lifelong learning,” Journal
Planning and control of quadrupedal gaits using deep reinforcement ofMachineLearningResearch,vol.24,no.214,pp.1–50,2023.
learning,” IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. [386] A. Kumar, A. Raghunathan, R. M. Jones, T. Ma, and P. Liang,
3699–3706,2020. “Fine-tuning can distort pretrained features and underperform out-of-
[365] T. Z. Zhao, V. Kumar, S. Levine, and C. Finn, “Learning fine- distribution,”inInternationalConferenceonLearningRepresentations.
grainedbimanualmanipulationwithlow-costhardware,”arXivpreprint
arXiv:2304.13705,2023.
[366] M. Liu, Z. Chen, X. Cheng, Y. Ji, R. Yang, and X. Wang, “Visual
whole-body control for legged loco-manipulation,” arXiv preprint
arXiv:2403.16967,2024.
[367] H. Miura and I. Shimoyama, “Dynamic walk of a biped,” The Inter-
nationalJournalofRoboticsResearch,vol.3,no.2,pp.60–74,1984.
[368] K.Sreenath,H.-W.Park,I.Poulakakis,andJ.W.Grizzle,“Acompliant
hybrid zero dynamics controller for stable, efficient and fast bipedal
walking on mabel,” The International Journal of Robotics Research,
vol.30,no.9,pp.1170–1193,2011.
[369] G. Bledt, M. J. Powell, B. Katz, J. Di Carlo, P. M. Wensing, and
S. Kim, “Mit cheetah 3: Design and control of a robust, dynamic
quadruped robot,” in 2018 IEEE/RSJ International Conference on
IntelligentRobotsandSystems(IROS). IEEE,2018,pp.2245–2252.
[370] M.Hutter,C.Gehring,D.Jud,A.Lauber,C.D.Bellicoso,V.Tsounis,
J. Hwangbo, K. Bodie, P. Fankhauser, M. Bloesch et al., “Anymal-
a highly mobile and dynamic quadrupedal robot,” in 2016 IEEE/RSJ
international conference on intelligent robots and systems (IROS).
IEEE,2016,pp.38–44.