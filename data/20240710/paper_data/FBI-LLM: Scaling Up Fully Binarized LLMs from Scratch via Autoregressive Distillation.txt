FBI-LLM: Scaling Up Fully Binarized LLMs from
Scratch via Autoregressive Distillation
LiqunMa1, MingjieSun2,ZhiqiangShen1
1MohamedbinZayedUniversityofAI
2CarnegieMellonUniversity
{Liqun.Ma,Zhiqiang.Shen}@mbzuai.ac.ae,mingjies@andrew.cmu.edu
Abstract
ThisworkpresentsaFullyBInarizedLargeLanguageModel(FBI-LLM),demon-
stratingforthefirsttimehowtotrainalarge-scalebinarylanguagemodelfrom
scratch(notthepartialbinaryorternaryLLMlikeBitNetb1.58[1])tomatchthe
performanceofitsfull-precisioncounterparts(e.g.,FP16orBF16)intransformer-
basedLLMs. Itachievesthisbyemployinganautoregressivedistillation(AD)loss
withmaintainingequivalentmodeldimensions(130M,1.3B,7B)andtrainingdata
volumeasregularLLMpretraining,whiledeliveringcompetitiveresultsinterms
ofperplexityandtask-specificeffectiveness. Intriguingly,byanalyzingthetraining
trajectory,wefindthatthepretrainedweightisnotnecessaryfortrainingbinarized
LLMsfromscratch. Thisresearchencouragesanewcomputationalframework
andmayfacilitatethefuturedesignofspecializedhardwaretailoredforfully1-bit
LLMs. Wemakeallourmodels,code,andtrainingdatasetfullyaccessibleand
transparenttosupportfurtherresearch1.
1 Introduction
Benefitingfromthehugeparameterscaleand
massivetrainingcorpora, transformer-based 70 FBI-LLM (Ours) BiLLM-OPT
BitNet b1.58 BiLLM-LLaMA
Large Language Models (LLMs), like Chat- 60 OneBit-OPT BiLLM-LLaMA2
GPT[2]andLLaMA[3,4],performgreatin 50 OneBit-LLaMA
tasksrequiringdomainknowledgeandcom- 40
plexreasoning. Moreover,thecapabilitiesof
30
LLMstendtoenhanceastheirparametersizes
expand. Thissubstantialscaleinparameters 20
resultsinconsiderablestorageandcomputa- 10
tionaldemands,whichsubstantiallyrestricts 0 10 20 30 40 50 60 70
# Model Parameters (B)
LLMs’broaderapplicationanddevelopment. Figure 1: Perplexity on Wikitext2 of existing bi-
Quantizationefficientlymitigatestheselimita- narizedLLMsandourFBI-LLMs. FBI-LLMsget
tionsbymapping32-bitparameterstosmaller similarorlowermagnitudeofperplexityonsimilar
bitsizes. Itsubstantiallycutsstoragerequire- sizeofmodelscomparedwithotherbinarizedLLMs.
mentsandenhancescomputationalspeedandenergyefficiencyduringinference.
Asthemostextremecaseofquantization,binarizationrepresentseachparameterbyjust{-1,1}. It
maximizescompressionandefficiencybutatthecostofaccuracy.Prioreffortstopreservetheefficacy
ofbinarizedLLMsincluderetainingsalientparameters[5]orusingnear-one-bittorepresenteach
parameter[1]. Whiletheseapproacheshaveshownpromise,theystillleaveroomforoptimizationin
storageandefficiency,andadditionalfull-precisionparametersorparameterencodingsexpressedin
non-powersof2cancauseextraoverheadwhenadaptingtoedgehardware.
1Code:https://github.com/LiqunMa/FBI-LLM.Model:https://huggingface.co/LiqunMa/.
Preprint.Underreview.
4202
luJ
9
]LC.sc[
1v39070.7042:viXra
ytixelprePTransforming a standard LLM
into an FBI-LLM by replacing
Linear with FBI-Linear LLM into an FBI-LLM by
replacing Linear with FBI-
Linear
LLM FBI-LLM
LLM Block Autoregressive
Activation 16bit Linear FBI-Linear Distillation
FBI-Linear #"
FBI-Linear $ and % are learnable %% %& … &'()
for Down column-wise scale parameters FBI-LLM
! 0.71 -0.34 0.12 -0.56 update Other Modules vocabulary
FB fI o- rL i Un pear F fB oI r- L Gin ae tear
-0.71 0.34 0.12 -0.56
!&!
FBI-Linear
##
RMSNorm 16bit -0.71 0.34 -0.12 0.56 1 -1 1 -1 !!
$× -1 1 1 -1 +%
($,%) vocabulary
FBI-Linear for O 0.01 0.04 +0.02 0.06 % '(- )1 *(,1 ) -1 1 %% %& … &'()
Softm KVa x
cache 16bit KV cache 16bit 0.7 0.3 ×0.1 -0.5 $ -0 0. .1 8 - 00 .. 21 0 0. .3 5 - -0 0. .2
3!$
FBI f- oL ri n Qear FBI f- oL ri n Kear FBI f- oL ri n Vear 1 -1 1 -1 -0.6 0.7 -0.4 0.9 FuT le l-a pc rh ee cr
i
sM ioo nd Le Ll
M
RMSNorm 16bit -1 1 1 -1
!! Other Modules
Activation 16bit -1 1 -1 1 %*,%%,…,%+
(a) The structure of FBI-LLM (b) The training procedure of FBI-LLM
Figure2: IllustrationoftheFBI-LLMframework. WetakethestructureofLLaMAasanexample.
Left: theLLMblockwiththeproposedFBI-Linearusinglearnableαandβ. Right: ourautoregres-
sivedistillationandmodelpertainingprocedure.
SomeworksonfullybinarizedLLMsarebasedontheoptimizationgoalofminimizingthelayer-wise
ℓ loss [6, 5] or performing binarization while continuing training a full-precision LLM with a
2
smallamountofdata[7]. Thesemethodsfaceseveralissues: 1)Thebinarizationprocessgreatly
compressestheparameterspaceoftheoriginalmodel,damagingsomeoftheknowledgestoredinthe
full-precisionmodel. Adequatetrainingdataisneededtoallowthebinarizedmodeltorelearnthis
knowledgeandadaptittothepatternofbinarizedparameters;2)Derivingbinarizedmodelsfrom
existingpretrainedmodelsdoesnotallowfortheselectionofdifferentparameterscalesorvocabulary
sizes,limitingtheirflexibilityandpracticalapplication.
Inthiswork,weproposeastreamlinedprocessfortrainingFullyBInarizedLLMsfromscratch,
termedFBI-LLM.ToenablestabletrainingofbinaryLLMsfromscratch,weproposeanoveltraining
procedurebasedondistillation. Specifically,duringtraining,wegraduallydistillfromafull-precision
teacherandadoptanautoregressivedistillation-basedschemetomatchthepredictedprobabilitiesof
theteachermodelateachtokenlocation. Withthissimpleautoregressivedistillationloss,wecan
successfullytrainbinarizedLLMsfromrandominitializations. Sinceourmodificationsarefocused
onthelossfunction,FBI-LLMcanbeeasilyincorporatedintotheexistingLLMpre-trainingpipeline.
Moreover, the binarization operation is decoupled from model training in this method, thus any
techniquesthatenhanceLLMtrainingefficiencycanbedirectlyadaptedforFBI-LLM.
WeempiricallyevaluatetheeffectivenessofourframeworkFBI-LLM,wherewetrainedmodels
withsizesrangingfrom130M,1.3B,to7B.Weusethewidely-usedTransformerarchitecturefor
LLMs,ascanbeseeninFig.2. WeshowthatwecantrainfullybinarizedLLMsfromscratch,witha
smallperformancegapascomparedtofull-precisioncounterparts. Comparedtobaselinemethods,
ourtrainingprocessleadstofullybinarizedLLMswithbetterperformanceonperplexity(asshown
inFig. 1)andmultipledownstreamtasks. Weshowthatautoregressivedistillationiskeytotraining
binarizedLLMs. Further,analysisofpretrainingcheckpoints(e.g.,flip-flopratioandgradientnorms)
suggeststhereisnomajordifferencebetweeninheritingtheweightsfromfull-precisionLLMsand
trainingbinarizedLLMsfromscratch.
Overall,thecontributionofthispapercanbesummarizedasfollows: first,wedemonstrateforthe
firsttimethatwecansuccessfullytrainLLMswithbinaryweightsfromscratch;second,wepropose
anovellossformulationforstabilizethetrainingofbinarizedLLMs,whereweadoptautoregressive
distillation to match the probability distribution of a teacher model; third, we conduct extensive
experimentalandanalysistodemonstrateandbetterunderstandtheeffectivenessofourmethod.
2 RelatedWork
Neural Network Binarization. Binarization, the most extreme form of network quantization,
converts model parameters into a 1-bit format. Many studies have focused on Binary Neural
2
ETS
htiw
drawkcaB
FeedforwardNetworks(BNNs)toimprovetheiraccuracydespiteinherentlimitations. BinaryConnect[8]converts
full-precisionweightsinneuralnetworksto1-bitbinaryweightsusingstochasticmethodsduring
trainingandsimulatestheeffectsofbinaryweightsduringinference. Theyalsoimplementaclipping
functioninbackwardpropagationtopreventexcessivegrowthofreal-valuedweights. Expanding
on this, they develop the Binarized Neural Network (BNN) [9], which includes detailed training
andaccelerationtechniques,demonstratingtheefficiencyandpracticalityofBNNsthroughreduced
storageandfasterprocessingtimesinimageclassification.
However,thesemethodstypicallysufferfromaccuracyloss,promptingnumerousoptimization-based
solutionsoverrecentyearstomitigatethis. BinaryWeightNetworks(BWN)andXNOR-Net[10]
introduceascalingfactorthatapproximatesfloating-pointparameterstoreducequantizationerrors.
FurtherdevelopmentslikeDoReFa-Net[11],WRPN[12],andABC-Net[13]introducestrategies
to minimize information loss and quantization errors. Innovations such as XNOR-Net++ [10]
andvariousmimicsolutionslikeDistillationandQuantization(DQ)[14]continuetorefinethese
approaches,emphasizingstabilityandhighaccuracyintrainingbinarymodels. Toaddressthenon-
differentiabilityofthebinarizationfunction,techniqueslikethestraight-throughestimator(STE)[15]
are used for backpropagation. The ReActNet [16] improves BNNs with generalized activation
functions,andtheBNN+[17]introducesanenhancedderivativeapproximationofthesignfunction
alongwitharegularizationstrategytooptimizeweightlearning.
LargeLanguageModelBinarization. PB-LLM[5]implementspartialbinarizationoftheLLMs,
retainingthesalientparametersatfullprecision,occupyingonlyasmallportionofallparameters,
tomaintainthelinguisticreasoningcapacity. BiLLM[18]alsoconsidersthedistributionpattern
oftheweightscale. Itusesabinaryresidualapproximationstrategytobinarizesalientparameters,
whichconsistsofanoriginalbinarytensorandaresidualbinarizedmatrixtopresentthebinarization
resultofsalientparameters. BitNetb1.58[1]quantizesallparameterstothesetof{-1,0,1},where
theyfindthatquantizedLLMsachievecompetitiveperformancetotheirfull-precisioncounterparts.
However,PB-LLMandBiLLMusetheextracostofstoragetoprocesssalientweights,andBitNet
b1.58usesanaverageof1.58bitstorepresentweights. Noneofthemhavereachedthelimitsof
binarymodels. Thereisroomforfurtherimprovementinmodelstoragesizeandinferencespeed.
OurworkfocusesonachievingfullybinarizedLLMswhilepreservingthemodel’scapabilitiesas
muchaspossible.
BitNet[19]andOneBit[7]employquantization-awaretraining(QAT)tobinarizeLLMs. BitNet
utilizesgroupquantizationbyapplyingdifferentscalestotheparametersofvariousgroups,which
acceleratesmodeltraining. Itstraininglossalignswiththatofpretrainingautoregressivelanguage
models. Conversely, OneBitpreservesthefull-precisionmodelknowledgethroughquantization-
awareknowledgedistillation,usingafull-precisionmodelastheteacherandguidingthebinarized
modeltrainingwithtwodistinctlossfunctions. Unlikethesetwomethods,ourapproachachieves
similarorbetterresultsthroughamorestreamlinedandefficienttrainingprocess.
3 Methodology
In this section, we first provide an overview of the architecture of our FBI-LLM in Section 3.1.
Then,inSection3.2,wedetailtheFBI-linearmodule,themaincomponentofFBI-LLM.Finally,we
elaboratetheFBI-LLMautoregressivedistillation-basedtrainingprocedureinSection3.3.
3.1 ArchitectureofFBI-LLM
We illustrate the overall architecture of FBI-LLM in Fig. 2 (a). In transformer-based LLMs, the
majorityofparametersarefoundwithinthelinearmodules. FBI-LMreplacesalllinearmodules,
exceptforthecausalhead,withFBI-linear(FullyBInarizedLinear). Sincethecausalheaddirectly
influencestheoutputtokendistributionineachstep,binarizingitsparameterswouldsignificantly
affecttheaccuracyofthemodel’soutput,soweretainitsprecision.
Additionally,theparametersintwoothercoremodulesofLLMs,embeddingandlayernorm,also
needtobekeptatfullprecision.Thisisbecausetheembeddingmodulecontainssemanticinformation
aboutalltokensand,asthefirstlayerofthemodelinput,determinesthetext’sinitialrepresentation.
Layernorm,ontheotherhand,scalestheactivationvaluesdirectly. Binarizingitsparameterswould
significantlyreducethesemanticexpressivenessoftheactivationvaluesateachlayer. Similarsettings
arealsoadoptedinotherwork[19]aboutthebinarizationofLLMs.
33.2 FBI-Linear
ThemainparametersinFBI-linearareamatrixWb ∈Rm×nconsistingonlyof{1,−1},whichis
binarizedfromthefull-precisionlinearmoduleWf ∈Rm×noftheLLMs. Duringthetraining,the
binarizationprocesscanbeformulatedas:
Wb =sign(Wf) (1)
wheresignfunctionisformulatedas:
(cid:40) 1, Wf >0
sign(Wf)= ij (2)
ij −1, Wf ≤0
ij
We follow the previous works [10, 19] to scale the binarized parameter with full-precision scale
factors. Scalefactorscaneffectivelyreducetheerrorbetweenthebinarizedandoriginalparameters,
therebypreservingthemorerepresentationalcapacityofthecorrespondingmodule. Theyconstitute
asmallfractionofthetotalparameters,makingthemahighlyefficientenhancementtothemodel’s
performancewithoutsignificantlyincreasingtheparametercountandcomputationaldemand.
Specifically, in the FBI-linear, we apply scaling at the granularity of the matrix columns. The
calculationprocesscanbeformulatedas:
W(cid:102) ·b
,j
=α jW ·b
,j
+β
j
(3)
whereW(cid:102) ·b
,j
donatethejth columnofthescaledbinarizedweightmatrixW(cid:102)b ∈Rm×n. α
j
andβ
j
arethejthelementinlearnablescalevectorsα∈Rnandβ ∈Rnrespectively.
Toacceleratethemodel’sconvergencespeed,weinitializeαandβbeforetrainingasfollowing:
α =a (4)
j j
m
1 (cid:88)
β = |Wf −a | (5)
j m ij j
i
wherea = 1 (cid:80)mWf,denotestheaverageofjthcolumninWf.
j m i ij
3.3 AutoregressiveDistillation
Givenatrainingcorpusoftokensx={x ,...,x },astandardautoregressivelanguagemodeling
1 n
objective[20]istomaximizethelikelihood:
(cid:88)
L(x)= logp(x |x ,...,x ;θ) (6)
i i−k i−1
i
where k represents the size of the context window and the conditional probability p is modeled
throughaneuralnetworkcharacterizedbytheparametersθ. Unlikeconventionalautoregressive
language models, we train FBI-LLM using autoregressive distillation (AD). In the training, a
full-precisionpre-trainedLLMisusedastheteachermodel,andthebinarizedtargetmodelactsas
thestudent. Supposeeachinstanceoftrainingdataconsistsofasequenceofinputtokensx1,...,xm,
theteacherpredictionprobabilityforthenexttokencanbeformulatedas:
pT (cid:0) xm+1 |x1,...,xm(cid:1) =softmax(hmW ). (7)
l m+1
wherehmrepresentstheactivationofthefinaltransformerblock,W representsparametersof
l m+1
theaddedlinearoutputlayertopredictthenexttoken’sprobability.
Thecross-entropybetweentheoutputsofthestudentmodelandtheteachermodeliscalculatedas
thefinallossfunctionateachstepofpredictingthenexttoken. Itcanbeformulatedas:
n
1 (cid:88)
L=− pT(xi+1)·logpS(xi+1) (8)
n
i
4Table1: Hyper-parametersforFBI-LLMsinxxperiments.
ModelSize #layers hiddensize #attentionheads intermediatesize batchsize(tokens)
FBI-LLM130M 12 768 12 2,048 2M
FBI-LLM1.3B 24 2,048 32 5,632 2.4M
FBI-LLM7B 32 4,096 32 11,008 3.9M
Here n denotes the number of input tokens. pT(xi+1) denotes the token distribution over the
j
vocabulary at the ith step predicted by the teacher model, while pS(xi+1) is the corresponding
predicteddistributionofthestudentmodel.
QATutilizingknowledgedistillationhasbeenshowntobeeffectiveinvariousstudies[21,22,23,24,
7]. However,unliketheseworks,ourtrainingprocessexclusivelyusestheautoregressivedistillation
losswithoutaddinganyotherlossestomaintainsimplicity. Ourexperimentsverifiedthatusingonly
thedistillationlossyieldsbetterresultsthanthevanillaone-hotlabelbasedautoregressivelosswhile
maintainingmethodologicalsimplicitywhenworkingwithfullybinarizedLLMs.
Sincethesign(·)isnon-differentiableatzero,itcausesthegradientchaintobreakduringbackpropa-
gation,preventingoptimizationofthemodelparameters. Therefore,weusetheStraight-Through
Estimator(STE)method[15]duringbackpropagation,wherethegradientoftheoutputofthenon-
differentiablefunctionisusedasanestimateforthegradientoftheinput,thusallowingthegradient
tobeeffectivelypropagated. Thisestimationcanbeformulatedas:
∂L ∂L
= (9)
∂Wf ∂Wb
4 Experiments
Inourexperiment,wefollowtheW1A16setup[18,7],quantizingonlytheparametersto1-bitwhile
keepingtheactivationvaluesat16-bit. WetrainFBI-LLMswithsizesof130M,1.3B,and7B,testing
theirperformanceacrossmultipletasks.
4.1 Setup
Dataset. WetrainFBI-LLMswiththeAmberdataset[25]. AmberdatasetisamixtureofRefined-
Web [26], StarCoder [27], and RedPajama-v1 [28] and contains the total 1.26 trillion tokens. It
dividesthedatainto360chunks,witheachchunkcontaininganaverageof3.5billiontokens2.
Trainingdetails. OurmodelsusedforexperimentsadoptasimilarstructureasLLaMA2[3]. Forthe
specifichyper-parameterssettingsofFBI-LLMsofdifferentsizes,refertoTable1. Themaximum
sequencelengthissetto2048. TheoptimizerisAdamwithβ =0.9,β =0.98. Theinitiallearning
1 2
rateforallmodelsizesissetat3e−4,followingacosinelearningrateschedulethatdecreasesto
afinalrateof3e−5asitiswarmedupover2,000steps. Weusegradientclippingat1.0. Weuse
LLaMA2-7BastheteachermodelforallsizeFBI-LLMstocalculateautoregressivedistillationloss.
Wetrainmodelswith64NVIDIAA100GPUsintotalandmaintainBF16precisionwhiletraining.
PleaserefertoAppendixCformoredetails.
Baselines. WecompareourworkwithpriorbinarizedLLMsBi-LLM[18], OneBit[7], andBit-
Net[19]. WealsoincludetheBitNetb1.58[1],whichisaternaryquantizationLLM,asourbaseline
model for comparison3. We further include results from open-sourced full-precision models of
varioussizes,suchasOPT[29],LLaMA[3,4],andTinyLLaMA[30],asreferences.
EvaluationMetrics. Weevaluatethemodelsbasedontheirzero-shotperformanceinsomedown-
streamtasks,includingBoolQ[31],PIQA[32],HellaSwag[33],Winogrande[34],ARC[35],and
OpenbookQA[36]. Wealsouseperplexityastheevaluationmetric. Perplexitymeasureshowwella
probabilitymodelpredictsatoken,quantitativelymeasuringthemodel’sgenerationpower.
2As shown in Fig. 3, in our experiment, about 10% of the training data chunks have already achieved
competitiveperformance.Furthertrainingisnaturallyexpectedtoyieldevenhigheraccuracy.
3AstheoriginalBitNetb1.58[1]hasnotopen-sourcedtheirmodel,weuseathird-partyopen-sourcedone
https://huggingface.co/1bitLLM/toevaluatecertainindicatorsforthecomparison.Thismodelachieves
slightlybetterresultsthanthosereportedintheoriginalpaper.
5Table2: Performanceondownstreamtasksandperplexity. Here,BWmeansbit-width,whichrefers
totheaveragenumberofbitsoccupiedbyeachparameter. HS,WG,andOBQAareabbreviationsfor
HellaSwag,Winogrande,andOpenbookQA,respectively. Wedividethetableintothreeblocksbased
onmodelsize. Ineachblock,theboldvaluesrepresentthebestvaluesamongthenon-high-precision
models,whilethevalueswithanunderlinerepresentthebestvaluesamongthehigh-precisionmodels.
Zero-shotAccuracy↑ Perplexity↓
Model Size BW
BoolQ PIQA HS WG ARC-e ARC-c OBQA Ave. Wiki2 PTB C4
BitNetb1.58[1] 700M 1.59 58.2 68.1 35.1 55.2 51.8 21.4 20.0 44.3 17.1 72.1 17.5
FBI-LLM(Ours) 130M 1.01 62.1 59.3 28.7 51.0 34.9 20.5 26.4 40.4 28.2 136.6 26.9
TinyLLaMA[30] 1.1B 16 57.8 73.3 59.2 59.1 55.3 30.1 36.0 53.0 7.8 30.5 9.9
OPT[29] 1.3B 16 57.8 72.5 53.7 59.5 51.0 29.5 33.4 51.1 14.6 20.3 16.1
OneBit-OPT[7] 1.3B 1.02 59.5 62.6 34.3 51.1 41.3 24.1 - - 25.4 - 23.0
BitNetb1.58[1] 1.3B 1.59 56.7 68.8 37.7 55.8 54.9 24.2 19.6 45.4 24.1 145.1 21.8
FBI-LLM(Ours) 1.3B 1.01 60.3 69.0 42.3 54.0 43.6 25.3 29.6 46.3 12.6 39.3 13.8
OPT[29] 7B 16 66.1 76.5 67.2 65.4 60.0 34.7 37.4 58.2 10.9 15.8 12.7
LLaMA[3] 7B 16 75.1 79.2 76.2 69.9 72.9 44.9 44.4 66.0 5.7 41.2 7.3
LLaMA2[4] 7B 16 77.7 79.1 76.0 69.1 74.6 46.2 44.2 66.7 5.5 37.9 7.3
OneBit-LLaMA2[7] 7B 63.1 68.1 52.6 58.4 41.6 29.6 - - 9.7 - 11.1
BitNet[19] 7B - - - 38.9 51.4 - - - - - - -
BiLLM-OPT[18] 7B 1.11 62.2 58.6 31.9 51.5 34.1 23.9 29.0 41.6 35.4 73.6 43.2
BiLLM-LLaMA[18] 7B 1.08 62.7 61.2 36.8 51.1 36.0 25.7 31.8 43.6 35.0 421.3 39.6
BiLLM-LLaMA2[18] 7B 1.08 61.8 60.6 34.8 52.4 36.2 24.4 33.2 43.3 32.5 3877.4 40.5
FBI-LLM(Ours) 7B 1.01 61.5 72.6 57.7 58.9 53.0 29.9 36.8 52.9 9.1 29.6 10.5
4.2 MainResults
Table2presentsthemainresultscomparingourFBI-LLMstovariousstate-of-the-artbaselinemodels.
Wealsoreporttheaveragebit-widthoccupiedbymodelparameters,excludingtheembeddinglayer
andthehead,fordifferentmodels.DetailsonthecalculationprocesscanbefoundinAppendixB.Our
FBI-LLMsmaintainthelowestaveragebit-widthacrossdifferentmodelsizeswhiledemonstrating
remarkableperformance. Weprovidezero-shotaccuracy,whichisafoundationforunderstanding
howwellamodelcanperformwithoutadditionaltask-specificinformation. Thismetriciscommonly
usedtoassessthemodel’sinitialcapabilitiesandalignswithcertainbenchmarkingtasksaimedat
measuringthepre-trainedLLM’sgeneralcomprehensionandknowledge-reservingcapabilitiesacross
diversedownstreamtaskswithoutadditionalfew-shotinformation.
Sincethereisnobinarybaselineforthe130Msize,wecompareour130MmodelwiththeBitNet
b1.58atthe700Mscale. Despitethefivefolddifferenceinmodelsizeandsignificantvariationsin
quantizationdegree,ourmodelstilloutperformsBitNetb1.58inBoolQAandOpenbookQA.For
the 1.3B-scale binary models, our FBI-LLM achieves the best performance in most downstream
tasks and perplexity, even matching or exceeding the capacity of some 7B-scale binary models
likeBiLLM-LLaMA2-7B.Comparedtothefull-precisionmodelsofasimilarscale,theproposed
FBI-LLM1.3Bcanachieveupto87%oftheirperformanceindownstreamtasks. Inthe7Bscale,our
modelsignificantlyoutperformednearlyallbaselines.
Furthermore,limitedbycomputationalresources,thecur- 60
rentresultsforFBI-LLM7Barenotfinal. Weonlyuse Accuracy
0.50 50
8.6%(31chunks)oftheAmberdataset. Fig. 3illustrates
thechangesindownstreamtaskaccuracyandperplexity 0.45 40
during the training process of FBI-LLM-7B. It is clear 30
that,asofthecurrenttrainingprogress,theperformance 0.40
20
ofFBI-LLM-7Bwillbeimprovedconsistently,andfurther
0.35 Perplexity
trainingcouldyieldbetterresults. 0 5 10 15 20 25 30 10
Training chunk num
Figure 3: Changes in average perplex-
4.3 EffectivenessofAutoregressiveDistillation ity and downstream task accuracy dur-
ing the training of FBI-LLM 7B. The
Todemonstratetheeffectivenessofusingonlyautoregres- horizontalaxisrepresentsthenumberof
sive distillation as the training objective, we train two Amberdatablocksusedfortraining.
models: one using solely the autoregressive distillation
loss and the other using only the standard autoregressive loss. All other training procedures are
identicaltothoseusedforFBI-LLM.Weevaluatetheperformanceofthesemodelsondownstream
tasksandperplexity, asshowninFig.4. Theevaluationtasksanddatasetsarethesameasthose
listedinTable2. Forclarity,wepresentonlytheaverageaccuracyacrossdifferenttasksandthe
6
ycaruccA ytixelprePaverageperplexityacrossdifferentdatasetshere. Detailedperformanceforeachtaskisprovidedin
AppendixD.
Itcanbeobservedthatthroughoutthetrainingprocess,modelstrainedwithautoregressivedistil-
lationobjectiveconsistentlyoutperformthosetrainedwiththestandardautoregressiveschemein
downstreamtasksandperplexity. Thisindicatesthatusingautoregressivedistillationobjectiveis
more effective in training binarized LLMs. The utilized soft labels from the output probabilities
oftheteachermodelcontainmoreinformationthanhardlabels(i.e.,thevocabularylabels). They
provideadistributionoverallpossiblevocabulary,indicatingnotjustthetargetwordbutalsothe
relativeconfidenceinotherpossiblewords. Thisricherinformationhelpsthetargetmodellearn
nuancedpatternsandrelationshipsinthedatathatarecapturedbythestrongteachermodel. Since
ourtargetmodelislearningsolelyfromasmoothedversionofthegroundtruth,itislesslikelyto
overfittothenoiseorspecificdetailsinthetrainingdatathatmaynotgeneralizewelltonewdata.
Therefore,retainingonlyautoregressivedistillationasthetrainingobjectiveensuresthesimplicity
andeffectivenessoftheentiretrainingworkflow.
0.42
Autoregressive Distillation 120 Autoregressive Distillation
0.41 Normal Autoregressive 110 Normal Autoregressive
0.40
100
0.39 90
0.38 80
0.37 70
0.36 60
0.35 50
1 2 3 4 5 6 7 8 9 10111213141516 1 2 3 4 5 6 7 8 9 10111213141516
Training chunk num Training chunk num
(a)Averagedownstreamtaskaccuracy (b)Averageperplexity
Figure4: Themodelperformancefordifferenttrainingloss.
5 Analysis
Inthissection,weanalyze1)thebetterchoiceoftrainingfromscratchorcontinuingtrainingfrom
pretrainedLLMforbinarizedLLMs. 2)traininginstabilityandoursolution. 3)storageefficiencyof
ourmodels. 4)generationcasedemonstrations.
5.1 TrainingfromScratchorContinueTrainingfromPretrainedLLM?
0.16 Continue training 12 Continue training
0.14 Training from scatch 10 Training from scatch
0.12
8
0.10
0.08 6
0.06
4
0.04
0.02 2
0.00
0
0 500 1000 1500 2000 0 500 1000 1500 2000
Step Step
(a)Averageflip-flopratio (b)Trainingloss
Figure5: Theflip-flopratioandlossfordifferenttrainingprocedures.
Intuitively, continuing training from a pretrained LLM to obtain a binarized model can inherit
knowledgefromtheoriginalmodel,potentiallyachievingbetterresultsthantrainingfromscratch. To
evaluatethishypothesis,weconductanalyticalexperimentstorecordandcomparethebehaviorsof
modelsunderbothtrainingprocedures.
7
ycaruccA
)%(
oitaR
polF-pilF
ytixelpreP
ssoLToquantifyandexaminethemodelbehaviorsusingpretrainedparametersortrainingfromscratch,
aswellastheirstabilityandinitializationdependencyofthem,weapplytheflip-flop(FF)ratio[37].
TheFFratiomeasuresoptimizationstabilitybehavior,whichisdefinedasfollows:
(cid:12) (cid:12)Sign(cid:0) wb (cid:1) −Sign(cid:0) wb(cid:1)(cid:12) (cid:12)
C = t+1 t abs, (10)
FF 2
whereC isanindicatorthatshowswhetherabinaryweightchangesitssignaftertheupdateat
FF
iterationt,and|.| istheabsoluteoperation.
abs
(cid:80)L (cid:80)
C
FF = l=1 wb∈W lb FF , (11)
ratio N
total
whereN representsthetotalnumberofparametersinamodelwithLlayers. FF denotesthe
total ratio
flip-flopratio,whichisthepercentageofparametersthatchangetheirsigns.
In this experiment, we select TinyLLaMA as the base LLM and initialize the binarized LLM
parametersusingeitherpretrainedvaluesorrandomassignments. Wemaintainconsistencyinall
othertrainingdetailswiththeFBI-LLM.InadditiontotheFF ,wemonitortraininglossesand
ratio
gradientnormspriortogradientclippingacrossbothtrainingprocedures.
FromFig.5a,itisobservedthatduringthebeginningof 600
training, the trend of FF remains largely consistent Continue training
ratio 500 Training from scatch
betweenthetwoprocedures. Throughoutthetrainingpro- 400
cess, the FF for both methods are on similar scales
ratio 300 andthevaluesarerelativelysmall. Itindicatesthatthetwo
200
differentparameterinitializationapproachdonotsignif-
icantlydifferintheireffectsonbinarizationoptimization 100
process. Fig.5bshowschangesintraininglossfortwo 0
training procedure. At the initial stage of training, the 0 500 1000 1500 2000
Step
traininglossesforthetwoapproachesareessentiallythe Figure 6: Gradient norm curves from
same,showsthatthemodel’straininglossisnotnotably differenttrainingprocedures.
influencedbytheinitializationapproach. Althoughthelossoftrainingfromscratchisslightlyhigher
thanthatofcontinuingtrainingafteraperiodoftime,aftersomemoretime,thelossoftrainingfrom
scratchonceagainmatchesorevenbecomeslessthanthatofcontinuingtraining,andthetrendof
losschangeintrainingfromscratchismorestable. Significantly,atapproximatelythe1000thstep,as
illustratedinFig.5a,theFF beginstoshownotablefluctuationswhencontinuingtrainingfroma
ratio
pretrainedLLM.Similarly,ataroundthe1700thstepshowninFig.5b,thetraininglossexperiences
similarissues. Additionally,Fig.6highlightsmorepronouncedchangesinthegradientnormsduring
thecontinuingtraining.
ThesefindingschallengeourinitialhypothesisthatstartingwithapretrainedLLMwouldendowthe
binarizedmodelwithinheritedknowledge,thusenhancingperformance. Instead,theyimplythat
binarizationthroughtrainingisnotsensitivetothewayofparameterinitialization. Furthermore,
wespeculatethatbinarizedandfull-precisionLLMsemploydifferentparametercombinationsand
configurationstoencodesemantics,whichresultsinsubstantialdivergencesintheirparameterspace
pattern. Toadaptthispattern,theoptimizationprocessforbinarizationbycontinuingtrainingfroma
pretrainedLLMmightnecessitatemoreprofoundparameteradjustments. Thiscouldpartlyexplain
whyitismoreunstablecomparedtotrainingfromscratchduringthetraining.
5.2 TrainingInstability
Bothbinaryandfull-precisionLLMtraininghavebeenfoundtoexhibitunstabletrainingbehav-
iors[19,7,38]. OurFBI-LLMexhibitssimilarissues,specificallymanifestingassuddenspikesin
traininglosswhentraining1.3Band7BFBI-LLMs,whichsometimesfailtoconvergeafterthat. We
adoptthesolutionsimilartoPaLM[38]: ifthelossnolongertendstoconverge,themodelrevertsto
thepreviouscheckpointandskipsthedatachunkthattriggeredtheunstablelosstocontinuetraining.
Themodelnolongerencountersissuesatthesametrainingstepsusingthisapproach. Weobserve
thatpretrainingthe7BFBImodelfromscratchhasapproximatelya6%probabilityofcausingloss
spikes. Forthe1.3Bmodel,trainingismoreunstableduetothelowercapability,withabouta15%
probabilityoflossspikes. Thisisconsistentwiththepretrainingbehaviorseeninreal-valuedLLMs
whiletheprobabilityofspikingissignificantlyhigher,whichmayberelatedtothelimitedexpressive
capabilityofbinaryparameters. Tohandlethis,weskipanydatablockswherelossspikesoccur.
8
mroN
tneidarG5.3 StorageEfficiency
Table3: Compressionandextraparametersratio.
Model ModelSize StorageSize CompressionRatio ExtraParameterRatio
LLaMA 130M 0.25GB
59.26% 0.119%
FBI-LLM 130M 0.10GB
LLaMA 1.3B 2.54GB
84.67% 0.063%
FBI-LLM 1.3B 0.39GB
LLaMA 7B 12.55GB
90.07% 0.034%
FBI-LLM 7B 0.39GB
Table3presentsthetheoreticalstoragespacerequiredbyFBI-LLMsofvarioussizescomparedtothe
full-precisionLLaMAwiththesamestructure. Italsodetailstheproportionofadditionalparameters
(αandβ)introducedbyFBI-LLM.ThecomparisoninthetabledemonstratesthatFBI-LLMcan
achieveahighcompressionratio,significantlyreducingthestorageburdenofLLMs. Althoughthe
extraparametersforscalingandshiftingintroducedbyFBI-LLMneedtoberetainedinfullprecision,
theirproportionisminimal,renderingtheirimpactonstoragenegligible.
5.4 GenerationCases
AsillustratedinFig.7,althoughthegenerationqualityofFBI-LLMsdoesnotfullymatchthatof
full-precision LLMs, FBI-LLMs can still generate fluent and meaningful content. Compared to
BitNetb1.58,whichhasahigherparameterbit-width,FBI-LLMsdemonstrateabetterunderstanding
ofpromptsandincludemoreknowledgeinsomegeneratedexamples. ThisindicatesthatFBI-LLMs
possessstronggenerativecapabilitiesandcontainsufficientknowledge. Furthermore,FBI-LLMs
demonstratethepotentialtoscaleupfurther,reachingnewlevelsofintelligencewhilebeingmore
hardware-friendlyfordeployment.
Prompt: Create a Twitter post to promote your new product: Alisha's All NaturalJam
(1.3B) OPT: I'm having a jam sale today! Come check it out! I'm offering 4 different jams for $5 each. This is your
last chance to get them before I sell out!\n\nIfyou are a follower of mine on Twitter or Facebook, you
will be notified when the sale is live.\n\nI'mhaving a jam sale today! Come check it out! I'm offering
4 different jams for $5 each. This is your last chance to get them before I sell out!\n\nIfyou are a
follower of mine on Twitter or Facebook, you will be notified when the sale is live.
(1.1B)TinyLLaMA: The New Product You Want to Sell\nAlisha'sAll NaturalJam The New Product You Want to Sell\nAlisha's
All NaturalJam\nTheNew Product You Want to Sell\nAlisha'sAll NaturalJam is a handcrafted jam using
only the finest ingredients in an easy to use 1-cup package that tastes like a 12-cup jar. It's perfect
for a gift, a snack, or you can use it for a delicious dessert at home.\nAlisha'sAll NaturalJam is
made using real fruit, no preservatives, no added sugar, no GMOs, and is free of allergens. It's
sweetened with coconut sugar, and gluten-free. It's a great addition to any breakfast, snack, or dessert.
(1.3B)BitNet1.58bit:Alisha's All Natural Jam and Conditioner - 4 oz.
(1.3B)FBI-LLM: Alisha's All Natural Jam is a delicious jam that is made with only the finest ingredients. It has a
subtle sweetness that pairs well with all types of fruit. It's perfect for juicing, sourcing, or making
jam from scratch. It's a healthy snack you can have all year round, and it's a perfect way to
incorporate fresh fruits and veggies into your meals.\nAlisha's All Natural Jam is a great addition to
your salad dressing, but can also be used as a dip for vegetables. Its light texture makes it perfect
for juicing!\nAlisha's All Natural Jam is a delicious jam that is made with only the finest ingredients.
Prompt: Correct the grammatical errors in the sentence: She come to the store for supplies.
(7B) LLaMa2: The grammatical error in the sentence is "She come ". It should be "She came ".
(3B) BitNet1.58bit:She can not go to the store for supplies.\nShecomes to the store for supplies.\nShecan not go to the
store for supplies.\nShecomes to the store for supplies. She can not go to …
(7B) FBI-LLM: Correct: She comes to the store for supplies.
Prompt: Design a logo for a web development company.
(7B) LLaMa2: The logo should be simple and elegant. The color scheme should be red, white, and blue.
(3B) BitNet1.58bit:I was asked to create a logo for a web development company. From the brief I came up with2 options.\n-
A stylised'H' and 'D' which could be used in the logo and text.\n-A more modern 'H' and 'D' which
would be more suitable for a web development company.\nIchose option 2 as it would be more suitable for
a web development company.\nThefinal logo I came up withis below.\nI'mhappy to have created this logo
for a web development company.
(7B) FBI-LLM: The client is a web development company with a large and loyal customer base. They require a logo, which
incorporates their company's branding and colours, and is available on print and on the web.\nThelogo
will need to adhere to the company's brand, colourpalette and style guide.\nThelogo should be simple,
clean and consistent, and should not be too busy.\nThelogo will be used on the company website, on
business cards and on all promotional material.\nTheclient has a strong preference for a logo that can
be easily recognisableand that can be used at all stages of the development cycle.\nLogodesign: Please
provide a logo design.
Figure7: Generationcases. Wecomparetheoutputsofthefull-precisionmodel,BitNetb1.58,and
ourFBI-LLMwhengiventhesameprompts.
96 Conclusion
Wehaveproposedalearningframeworkusingautoregressivedistillationfor1-bitweightbinarization
ofLLMsfromscratch. Extensiveexperimentsonmodelsofvarioussizesof130M,1.3B,and7B
demonstratethatFBI-LLMoutperformsstrongbaselinesandstrikesagoodbalancebetweenmodel
sizeandperformance. Wealsoanalyzethecapabilities,properties,andpotentialoftheseextremely
low-bitmodels,providingguidanceforfutureresearch.
Limitations. Ourproposedbinarizationframeworksignificantlyreducesthememoryandcomputa-
tionconsumptionsofLLMs,providingpotentialfortheirefficientdeployment. However,thereare
severallimitationsofourmodels. Firstly,our1-bitbinarizationinevitablyincursaperformanceloss
comparedtotheoriginalfull-precisionmodel. Additionally,thetrainingprocess,whichincludes
knowledgedistillation,bringsadditionalcomputationalcosts. Moreover,duetotheuniquenatureof
binarization,currenthardwaremakesitdifficulttodirectlysupportbinarizedLLMstoachievereal
speedup. Wealsohavenotyetconsideredintermediateactivationbinarizationwhichisthesameas
previousstudies. Finally,potentialethicalissuesofpretrainedLLMs,suchasharmfulbiases,privacy
concerns,andthespreadofdisinformation,arelikelytopersistafterbinarizationinourLLMs.
References
[1] ShumingMa,HongyuWang,LingxiaoMa,LeiWang,WenhuiWang,ShaohanHuang,LiDong,
RuipingWang,JilongXue,andFuruWei. Theeraof1-bitllms: Alllargelanguagemodelsare
in1.58bits. arXivpreprintarXiv:2402.17764,2024.
[2] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4
technicalreport. arXivpreprintarXiv:2303.08774,2023.
[3] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[4] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[5] YuzhangShang,ZhihangYuan,andZhenDong. Pb-llm: Partiallybinarizedlargelanguage
models. InTheTwelfthInternationalConferenceonLearningRepresentations,2023.
[6] EliasFrantarandDanAlistarh. Optimalbraincompression: Aframeworkforaccuratepost-
trainingquantizationandpruning. ArXiv,abs/2208.11580,2022.
[7] YuzhuangXu,XuHan,ZonghanYang,ShuoWang,QingfuZhu,ZhiyuanLiu,WeidongLiu,
andWanxiangChe. Onebit: Towardsextremelylow-bitlargelanguagemodels. arXivpreprint
arXiv:2402.11295,2024.
[8] MatthieuCourbariaux,YoshuaBengio,andJean-PierreDavid. Binaryconnect: Trainingdeep
neural networks with binary weights during propagations. Advances in neural information
processingsystems,28,2015.
[9] ItayHubara,MatthieuCourbariaux,DanielSoudry,RanEl-Yaniv,andYoshuaBengio.Binarized
neuralnetworks. Advancesinneuralinformationprocessingsystems,29,2016.
[10] MohammadRastegari,VicenteOrdonez,JosephRedmon,andAliFarhadi. Xnor-net: Imagenet
classificationusingbinaryconvolutionalneuralnetworks. InEuropeanconferenceoncomputer
vision,pages525–542.Springer,2016.
[11] ShuchangZhou,YuxinWu,ZekunNi,XinyuZhou,HeWen,andYuhengZou. Dorefa-net:
Traininglowbitwidthconvolutionalneuralnetworkswithlowbitwidthgradients. arXivpreprint
arXiv:1606.06160,2016.
[12] AsitMishra,ErikoNurvitadhi,JeffreyJCook,andDebbieMarr.Wrpn:Widereduced-precision
networks. arXivpreprintarXiv:1709.01134,2017.
[13] XiaofanLin,CongZhao,andWeiPan. Towardsaccuratebinaryconvolutionalneuralnetworks.
arXivpreprintarXiv:1711.11294,2017.
10[14] AntonioPolino,RazvanPascanu,andDanAlistarh. Modelcompressionviadistillationand
quantization. arXivpreprintarXiv:1802.05668,2018.
[15] YoshuaBengio,NicholasLéonard,andAaronCourville. Estimatingorpropagatinggradients
throughstochasticneuronsforconditionalcomputation. arXivpreprintarXiv:1308.3432,2013.
[16] ZechunLiu,ZhiqiangShen,MariosSavvides,andKwang-TingCheng. Reactnet: Towards
precisebinaryneuralnetworkwithgeneralizedactivationfunctions. InComputerVision–ECCV
2020: 16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,PartXIV
16,pages143–159.Springer,2020.
[17] Sajad Darabi, Mouloud Belbahri, Matthieu Courbariaux, and Vahid Partovi Nia. BNN+:
Improvedbinarynetworktraining,2019.
[18] WeiHuang,YangdongLiu,HaotongQin,YingLi,ShimingZhang,XianglongLiu,Michele
Magno,andXiaojuanQi. Billm: Pushingthelimitofpost-trainingquantizationforllms. arXiv
preprintarXiv:2402.04291,2024.
[19] HongyuWang,ShumingMa,LiDong,ShaohanHuang,HuaijieWang,LingxiaoMa,FanYang,
RuipingWang,YiWu,andFuruWei. Bitnet: Scaling1-bittransformersforlargelanguage
models. arXivpreprintarXiv:2310.11453,2023.
[20] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal. Improvinglanguage
understandingbygenerativepre-training. 2018.
[21] JanghoKim,YashBhalgat,JinwonLee,ChiragPatel,andNojunKwak. Qkd: Quantization-
awareknowledgedistillation. arXivpreprintarXiv:1911.12491,2019.
[22] YoonhoBoo,SunghoShin,JungwookChoi,andWonyongSung.Stochasticprecisionensemble:
self-knowledgedistillationforquantizeddeepneuralnetworks. InProceedingsoftheAAAI
ConferenceonArtificialIntelligence,volume35,pages6794–6802,2021.
[23] Cuong Pham, Tuan Hoang, and Thanh-Toan Do. Collaborative multi-teacher knowledge
distillationforlearninglowbit-widthdeepneuralnetworks. InProceedingsoftheIEEE/CVF
WinterConferenceonApplicationsofComputerVision,pages6435–6443,2023.
[24] Kartikeya Bhardwaj, Nilesh Prasad Pandey, Sweta Priyadarshi, Kyunggeun Lee, Jun Ma,
and Harris Teague. Oh! we freeze: Improving quantized knowledge distillation via signal
propagationanalysisforlargelanguagemodels. arXivpreprintarXiv:2403.18159,2024.
[25] ZhengzhongLiu,AurickQiao,WillieNeiswanger,HongyiWang,BowenTan,TianhuaTao,
JunboLi,YuqiWang,SuqiSun,OmkarPangarkar,etal. Llm360: Towardsfullytransparent
open-sourcellms. arXivpreprintarXiv:2312.06550,2023.
[26] GuilhermePenedo,QuentinMalartic,DanielHesslow,RuxandraCojocaru,AlessandroCappelli,
HamzaAlobeidli,BaptistePannier,EbtesamAlmazrouei,andJulienLaunay. Therefinedweb
datasetforfalconllm: outperformingcuratedcorporawithwebdata,andwebdataonly. arXiv
preprintarXiv:2306.01116,2023.
[27] RaymondLi,LoubnaBenAllal,YangtianZi,NiklasMuennighoff,DenisKocetkov,Chenghao
Mou,MarcMarone,ChristopherAkiki,JiaLi,JennyChim,etal. Starcoder: maythesourcebe
withyou! arXivpreprintarXiv:2305.06161,2023.
[28] TogetherComputer. Redpajama: Anopensourcerecipetoreproducellamatrainingdataset,
2023.
[29] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,
Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained
transformerlanguagemodels. arXivpreprintarXiv:2205.01068,2022.
[30] PeiyuanZhang,GuangtaoZeng,TianduoWang,andWeiLu. Tinyllama: Anopen-sourcesmall
languagemodel. arXivpreprintarXiv:2401.02385,2024.
[31] ChristopherClark,KentonLee,Ming-WeiChang,TomKwiatkowski,MichaelCollins,and
Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.
ArXiv,abs/1905.10044,2019.
[32] YonatanBisk,RowanZellers,JianfengGao,YejinChoi,etal. Piqa: Reasoningaboutphys-
icalcommonsenseinnaturallanguage. InProceedingsoftheAAAIconferenceonartificial
intelligence,volume34,pages7432–7439,2020.
11[33] RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. Hellaswag: Cana
machinereallyfinishyoursentence? InAnnualMeetingoftheAssociationforComputational
Linguistics,2019.
[34] KeisukeSakaguchi,RonanLeBras,ChandraBhagavatula,andYejinChoi. Winogrande: An
adversarialwinogradschemachallengeatscale. CommunicationsoftheACM,64(9):99–106,
2021.
[35] PeterClark,IsaacCowhey,OrenEtzioni,TusharKhot,AshishSabharwal,CarissaSchoenick,
andOyvindTafjord. Thinkyouhavesolvedquestionanswering? tryarc,theai2reasoning
challenge. ArXiv,abs/1803.05457,2018.
[36] TodorMihaylov,PeterClark,TusharKhot,andAshishSabharwal. Canasuitofarmorconduct
electricity? a new dataset for open book question answering. In Conference on Empirical
MethodsinNaturalLanguageProcessing,2018.
[37] ZechunLiu,ZhiqiangShen,ShichaoLi,KoenHelwegen,DongHuang,andKwang-TingCheng.
Howdoadamandtrainingstrategieshelpbnnsoptimization. InInternationalconferenceon
machinelearning,pages6936–6946.PMLR,2021.
[38] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,Adam
Roberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal. Palm:
Scalinglanguagemodelingwithpathways. JournalofMachineLearningResearch,24(240):1–
113,2023.
12Appendix
A BroaderImpacts
Ourproposedfullybinarizedlargelanguagemodels(FBI-LLM)requirelesscomputationalpower
andmemoryintrainingandinference,makingadvancedAItechnologyaccessibletoorganizations
andindividualswithlimitedresources. Withreducedhardwarerequirements,smallerbusinesses,
educationalinstitutions,andnon-profitorganizationscanimplementLLMs,democratizingaccess
tocutting-edgeAI.Binarizedmodelsaremoreenergy-efficient,whichcansignificantlylowerthe
carbonfootprintassociatedwithrunninglarge-scaleAIapplications. However,evenbinarizedLLMs
canstillinheritandexistbiasespresentintrainingdata,leadingtounfairoutcomesinapplications
likehiring,lawenforcement,andlending.
B AverageBit-widthofBinarizedLLM
Thissectionexplainshowtocalculatetheaveragebit-widthofabinarizedLLM.Sincetheembedding
layerandheadhavealargenumberofparametersandarenotbinarized,wedonotconsiderthem
whencalculatingtheaveragebit-width. ConsideramodulecontaininganRMSNormandalinear
layerwithaparametermatrixA∈Rn×n,whichintotalhasn+n2parameters. UsingtheFBI-LLM
binarizationprocessasanexample,afterbinarization,thismodulegainsadditionallearnablescale
parametersαandβ,totaling2nparameters. Duringinference,theparametersofRMSNorm,α,and
βremainat16bits,whileAisquantizedto1bit. Therefore,theaveragebit-widthofthismodule
cancalculatedasfollows:
1×n2+16×3n
AverageBit-width= (12)
3n+n2
C ModelConfigurationandTrainingDetails
Inthissection,welistthemodelconfigurationsandtrainingdetailsforthreescalesofFBI-LLM
modelswetrainedinTable4.
Table4: TheconfigurationandtrainingdetailsforFBI-LLM.
FBI-LLM130M FBI-LLM1.3B FBI-LLM7B
hiddensize 768 2,048 4,096
intermediatesize 2,048 5,632 11,008
maxsequencelength 2,048 2,048 2,048
#attentionheads 12 32 32
#hiddenlayers 12 24 32
#keyvalueheads 12 32 32
initializerrange 0.02 0.02 0.02
vocabularysize 32,000 32,000 32,000
learningrate 3e-4 3e-4 3e-4
batchsize(token) 2M 2.4M 3.9M
teachermodel LLaMA2-7B LLaMA2-7B LLaMA2-7B
#GPUs(A10080G) 16 16 32
GPUhoursforeachdatachunk 130h 189h 729h
trainingspeed(tokens/s/GPU) 7,800 5,300 1,200
D DetailExperimentResults
WelistthedetailedexperimentresultsabouttheeffectivenessofautoregressivedistillationinSection
4.3inTable5.
13Table5: Performanceondownstreamtasksandperplexityfordifferenttrainingobjectives. Here,NA
meansnormalautoregressivetrainingobjectiveandADmeansautoregressivedistillationtraining
objective. Basedonthenumberoftrainingchunk,thetableisdividedintoseveralblocks. Ineach
block,thevalueswithanunderlinerepresentthebestvalue.
Zero-shotAccuracy↑ Perplexity↓
TrainingChunk LossType
BoolQ PIQA HS WG ARC-e ARC-c OBQA Ave. Wiki2 PTB C4
NA 42.2 53.9 26.2 52.3 29.3 21.3 25.4 35.8 85.9 204.4 63.9
1
AD 50.1 54.1 26.6 51.0 29.2 21.9 24.4 36.8 81.2 193.4 61.9
NA 60.8 56.7 27.0 52.2 31.9 20.2 23.2 38.9 42.9 128.8 37.5
2
AD 61.5 56.1 27.3 50.8 31.1 20.6 22.8 38.6 43.4 137.9 37.3
NA 62.0 57.1 27.5 51.3 31.5 21.2 24.0 39.2 36.7 170.2 32.8
3
AD 62.2 58.1 27.7 51.1 33.0 21.3 25.4 39.8 34.9 145.2 32.9
NA 61.0 57.9 27.6 52.7 32.6 21.0 23.4 39.5 34.3 159.6 31.3
4
AD 62.0 57.7 27.7 50.1 33.0 21.2 26.8 39.8 32.9 142.1 31.0
NA 61.9 57.6 27.7 49.6 32.7 21.2 23.6 39.2 32.3 157.9 29.7
5
AD 62.2 58.4 28.0 50.8 32.9 21.3 25.8 39.9 31.7 137.0 29.4
NA 62.1 59.5 27.9 53.1 32.9 21.8 24.0 40.2 32.4 147.4 29.6
6
AD 62.2 59.2 27.7 49.9 33.9 22.9 26.0 40.2 31.3 129.0 29.4
NA 62.0 59.5 27.8 52.4 33.8 20.9 25.6 40.3 30.0 155.1 28.5
7
AD 62.1 59.5 28.3 51.2 33.8 21.8 25.4 40.3 31.0 119.3 28.8
NA 61.6 58.8 28.3 51.0 33.6 21.3 23.4 39.7 29.5 140.0 28.0
8
AD 62.2 59.0 28.1 50.8 32.3 20.6 24.0 39.6 30.1 113.8 28.9
NA 60.9 58.9 28.2 51.7 34.6 20.0 23.4 39.7 29.7 129.3 28.4
9
AD 62.2 60.1 28.2 51.8 33.5 21.1 26.6 40.5 29.9 129.8 27.9
NA 61.8 59.5 27.9 50.3 34.0 20.8 25.8 40.0 29.5 144.7 28.1
10
AD 62.2 58.5 28.0 51.4 34.2 21.6 26.2 40.3 30.1 122.8 28.1
NA 62.1 59.9 28.4 52.7 34.1 21.6 25.4 40.6 28.8 138.2 27.5
11
AD 62.2 59.0 28.0 49.9 34.0 20.7 25.2 39.9 29.0 119.9 27.6
NA 62.2 59.1 27.9 51.9 34.6 21.2 24.8 40.2 29.1 150.3 27.2
12
AD 62.1 59.2 28.1 52.3 34.0 21.2 24.2 40.2 28.6 113.0 27.3
NA 62.2 58.6 27.9 49.6 34.6 22.6 25.4 40.1 28.7 135.0 27.0
13
AD 62.1 58.8 28.2 49.4 33.5 21.6 25.0 39.8 28.4 134.0 27.3
NA 62.2 58.3 28.4 49.4 34.3 20.7 23.8 39.6 28.6 138.5 26.9
14
AD 61.5 58.5 28.3 51.0 33.9 21.2 26.0 40.1 28.3 123.6 27.3
NA 62.2 58.8 28.1 51.5 34.1 21.3 26.4 40.4 28.0 138.9 26.9
15
AD 62.2 58.8 28.0 51.5 34.8 20.9 26.2 40.4 28.5 138.3 27.4
NA 61.3 58.4 28.3 51.7 34.6 21.7 25.8 40.3 28.6 151.9 27.0
16
AD 62.1 60.2 28.1 51.9 33.2 22.3 25.2 40.4 28.2 124.6 27.2
14