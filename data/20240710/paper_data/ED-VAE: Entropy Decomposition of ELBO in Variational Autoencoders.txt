ED-VAE: Entropy Decomposition of ELBO in Variational
Autoencoders
Fotios Lygerakis1 and Elmar Rueckert1
Abstract—Traditional Variational Autoencoders Log-likelihood
(VAEs)areconstrainedbythelimitationsoftheEvidence
Lower Bound (ELBO) formulation, particularly when
utilizing simplistic, non-analytic, or unknown prior
distributions. These limitations inhibit the VAE’s
ability to generate high-quality samples and provide
clear, interpretable latent representations. This work
introduces the Entropy Decomposed Variational ELBO
Autoencoder (ED-VAE), a novel re-formulation of the
ELBO that explicitly includes entropy and cross-entropy
components. This reformulation significantly enhances
model flexibility, allowing for the integration of complex
and non-standard priors. By providing more detailed
control over the encoding and regularization of latent
spaces, ED-VAE not only improves interpretability
but also effectively captures the complex interactions
between latent variables and observed data, thus leading
to better generative performance.
Fig.1. VisualdescriptionoftheEvidenceLowerBOund(ELBO)
inaED-VAE.Figure1describestheminimizationofELBO,which
I. INTRODUCTION
weshowconsistsofamutualinformation,anentropyandacross-
Variational Autoencoders (VAEs) [1] have entropy term, as shown in Section III-B.
emerged as a compelling framework for generative
modeling, offering a principled approach to learn-
ing probabilistic representations of complex data
a critical role in shaping the learned latent space,
distributions. At the heart of VAEs lies the ELBO,
yet the traditional formulation does not offer an in-
which serves as the objective function guiding the
tuitiveunderstandingofhowthechosenpriorinter-
optimization of the model parameters. The ELBO
acts with the learned latent distribution. Moreover,
intertwines reconstruction accuracy with a regu-
the standard ELBO does not explicitly account
larization term that encourages the learned latent
for the amount of information shared between
variables to adhere to a specified prior distribution,
the data and the latent variables, which can be
typically a standard normal distribution. This dual
instrumental in understanding and controlling the
objective underpins the VAE’s capacity to gener-
representational power of the VAE [3], [4]. This
ate new, plausible data samples while uncovering
formulation often leads to additional limitations in
interpretable and disentangled representations of
model flexibility and interpretability, particularly
the underlying data-generating process. However,
when the prior distributions cannot be captured
theconventionalformulationoftheELBOpresents
as normal Gaussians or when the chosen priors
certain limitations that may hinder the flexibility
do not have an analytic form for computing the
and interpretability of VAEs [2]. Primarily, the
components of the ELBO. Additionally, the tradi-
choice of the prior distribution in the ELBO plays
tional ELBO does not effectively handle situations
where the prior distribution is unknown or where
1ChairofCyber-PhysicalSystems,UniversityofLeoben,Austria it cannot be easily integrated due to its complex-
4202
luJ
9
]GL.sc[
1v79760.7042:viXraity. This restricts the VAE’s ability to adequately allows for an improved understanding of how
capture the intricate interactions between latent different elements influence training and inference
variables and observed data, thereby hindering the in VAEs.
model’s capacity to generate high-quality, diverse Similarly, [5] addressed the imbalance in tradi-
samplesandprovideinterpretableanddisentangled tional ELBO formulations in their development of
representations of the underlying data-generating InfoVAE, which often prioritizes either reconstruc-
process. tionaccuracyorlatentregularizationattheexpense
Motivated by these challenges, we propose a of the other.
novel reformulation of the ELBO that seeks to en- Building on the need for robustness in ELBO
hance the expressivity, interpretability, and adapt- computation, especially with complex and non-
ability of VAEs. Our formulation decomposes the analytic priors, the utilization of advanced sam-
ELBO into distinct terms that separately account pling methods has proven crucial. [6] explored the
for reconstruction accuracy, mutual information use of multiple-importance sampling within the
between data and latent variables, and a marginal ELBO, improving the accuracy and stability of
KL divergence to prior. We further decompose the the variational inference. Their approach to deep
marginal KL divergence term into entropy and ensembles for variational approximations presents
cross-entropy terms, which facilitates the more a novel way to handle uncertainties in model
explicit control of the inherent uncertainty in the predictions.
latent variables and the alignment of the learned The issues of model robustness and flexibility
latent distribution with the prior. Our proposed are further compounded by challenges such as
Entropy Decomposed VAE (ED-VAE) offers, thus, posterior collapse, which affects the quality of
a more granular perspective on the latent vari- generated samples and the diversity of the latent
able modeling in VAEs, allowing for a deeper space. [7] provided an in-depth analysis of pos-
understanding of the intricate balance between terior collapse. Additionally, the work on learning
data reconstruction, information preservation, and optimal priors for task-invariant representations by
regularization. The explicit depiction of entropy [8] underscores the importance of flexible and
and cross-entropy terms enables a more informed adaptive prior settings in VAEs. Approaches like
and flexible choice of priors, extending the ex- theCR-VAE[4],whichusescontrastiveregulariza-
ploration beyond standard normal priors to in- tion to prevent posterior collapse ensure a robust
corporate domain-specific knowledge or desirable representation in the latent space and contribute to
properties into the latent space. Furthermore, the a more robust and adaptive generative modeling
delineation of mutual information facilitates direct framework.
control over the expressiveness and diversity of the
III. METHOD
latent space, which is crucial for enhancing the
generative capabilities of the VAE. In our experi- In this section, we present a novel formulation
ments, we compared the performance of the tradi- of the ELBO used in VAEs, which enhances in-
tional VAE with our entropy-decomposed variation terpretability and flexibility in choosing the latent
using two synthetic datasets. These datasets were variable prior. We start from the original ELBO
created to test how each model handles priors formulation, introduce an intermediate form that
of varying complexities, from simple Gaussian to includes a mutual information term and a KL
complex non-Gaussian distributions. divergence to the prior term, and finally propose a
new form of ELBO that decomposes the later KL
II. RELATED WORK divergence into entropy and cross-entropy terms.
The development of VAEs [1] has seen signifi- A. Original ELBO Formulation
cant advancements, particularly in the formulation
The original ELBO is formulated as:
and optimization of the ELBO. Foundational work
by [2] introduced the concept of dissecting the L(θ;ϕ) = E [logp (x|z)]−KL(q (z|x)||p(z))
q (z|x) θ ϕ
ϕ
ELBO into more interpretable components, which (1)TABLE I
Table I contains the description of all the terms
NAMINGOFREFERENCEDNOTATION.
in this and the following forms of the ELBO.
Following the analysis in [2], the ELBO can be
p (x|z) likelihood
θ
reformulated to make the mutual information term q (z|x) approximate posterior
ϕ
explicit: p(z|x) true posterior
p(z) prior
L(θ;ϕ) = E [logp (x|z)]−logN q (z) encoding distribution
q (z|x) θ ϕ
ϕ (2) I (x,z) mutual information of x and z
+E [H[q (n|z)]]−KL(q (z)||p(z)) q
q ϕ(z) ϕ ϕ D KL(q ϕ(z|x)||p(z)) KL divergence to prior
D (q (z)||p(z)) marginal KL divergence to prior
where: H[q (n|z)] is the entropy of KL ϕ
ϕ
the conditional distribution and logN and
E [H[q (n|z)]] together form the mutual
q (z) ϕ
ϕ alignment or divergence between the learned
information term. Consequently Equation 2 takes
latent distribution and the predefined prior. This
the form
explicit depiction allows for a more intuitive
L(θ;ϕ) = E [logp (x|z)]−I (x,z)
q ϕ(z|x) θ q
(3)
and adaptable choice of priors, enabling the
−KL(q ϕ(z)||p(z)) exploration beyond standard normal priors, and
B. Entropy Decomposed ELBO allowing the incorporation of domain-specific
knowledge or desirable properties into the latent
We further decompose the marginal KL diver-
space. It is important to state here that unlike
gence to the prior into an entropy and a cross-
the KL divergence, where choosing a prior that
entropy term as:
allows for an analytic form is crucial, using the
KL(q ϕ(z)||p(z)) cross-entropy term only requires the ability to
= E [logq (z)−logp(z)] sample from the considered prior distribution.
q (z) ϕ
ϕ (4)
= E [logq (z)]−E [logp(z)]
q (z) ϕ q (z) C. ELBO Optimization
ϕ ϕ
= −H[q (z)]+H[q (z),p(z)]
ϕ ϕ We adopt a Gaussian distribution assumption for
We thus propose a new form of ELBO by sub- the likelihood p (x|z), simplifying the reconstruc-
θ
stituting the above decomposed KL divergence in tion loss to the Mean Squared Error (MSE) be-
3: tween the original and reconstructed data (A). This
L(θ;ϕ) = E [logp (x|z)]−I (x,z) choice is congruent with the continuous nature of
q (z|x) θ q
ϕ (5) the datasets, ensuring well-behaved gradients for
+H[q (z)]−H[q (z),p(z)]
ϕ ϕ backpropagation.
This novel formulation of the ELBO, through
the decomposition of the KL divergence into en- L = E (cid:2) ∥x−xˆ∥2(cid:3)
recon q ϕ(z|x)
tropy and cross-entropy terms, unveils a more
For optimizing the mutual information term, we
granular perspective of the latent variable model-
employtheInfoNCEloss,providingalowerbound
ing in VAEs. It fosters a deeper understanding of
to it. In the work of [9], it has been established
the interplay between the latent variable posterior
that the InfoNCE loss, L , serves as a lower
and the chosen prior, thereby allowing for more InfoNCE
bound to the mutual information, I (x,z):
informed and flexible prior selections. q
Firstly, the explicit representation of the entropy
I (x,z) ≥ log(K)−L
q InfoNCE
term, H[q (z)], reveals the inherent uncertainty
ϕ
or randomness in the encoded latent variables. where K represents the batch size. L is
InfoNCE
This facilitates a more direct analysis and control computed utilizing positive and negative sample
overthelatentspace’sexpressivenessanddiversity, pairs, encouraging the encoder to generate repre-
which is crucial for the generative capabilities of sentations that are more informative of the data.
the VAE.
exp(zTz )
Secondly, the cross-entropy term, L = −log i j
H[q (z),p(z)], acts as a clear measure of
InfoNCE (cid:80)K
exp(zTz )
ϕ k=1 i kSimilarly, the entropy L and cross-entropy in handling complex, structured non-Gaussian dis-
Ent
L terms are computed using samples from the tributions.
XEnt
batch to facilitate their estimation.
B. Model Configurations
The total loss of ED-VAE is
The traditional VAE is configured with a stan-
L (θ;ϕ) = L −L −L +L (6)
ED-VAE recon InfoNCE Ent XEnt dard Gaussian prior, which is expected to perform
All the sub-losses are jointly optimized through adequately on Dataset 1 but may struggle with
backpropagation, with gradients propagated Dataset 2 due to its simplicity. In contrast, the ED-
through the encoder and decoder parameters. VAE is set up with a flexible prior configuration
for Dataset 2, aiming to closely approximate the
IV. EXPERIMENTS
complex distributions involved, while maintaining
Our experimental design evaluates the tradi- a similar setup as Dataset 1 for direct comparison.
tional VAE and the ED-VAE using synthetic We evaluate the two models based on the ELBO
datasets designed to highlight different complex- of held-out data, which consists of the lower
ities in data distributions. These experiments are boundforthelog-likelihoodofthedatadistribution
specifically tailored to assess the models’ ability learned.
to handle varying complexities of priors, from
Gaussian to non-Gaussian complex distributions. C. Experimental Setup
To ensure robust and reliable findings, each
A. Synthetic datasets
experiment is conducted five times with different
We utilize two distinct synthetic datasets to
seeds, and results are averaged to offset the effects
evaluate the performance of the traditional VAE
of random initialization and stochastic optimiza-
and ED-VAE:
tion. Both models consist of a two-layer fully
a) Dataset 1: Gaussian Prior: This
connected multilayer perception, with 400 hidden
dataset consists of data generated from a multi-
dimensions and a five-dimensional latent space,
dimensional Gaussian distribution. The latent
withthedecoderfollowingtheinversearchitecture.
variables z are sampled from a standard Gaussian
The VAEs are trained using the Adam optimizer
distribution N(0,1), and the observed data is
with a learning rate of 0.001 and a batch size of
created by a linear transformation of z followed
512. Training continues for 1000 epochs with early
by Gaussian noise. Additionally, positive samples
stopping based on validation loss to prevent over-
are generated by adding small Gaussian noise
fitting. Input features are normalized to prevent
around the original data points, simulating slight
bias.
variations within the data distribution that are still
The computation of the entropy and cross-
characteristic of the Gaussian prior. This serves
entropy terms, critical for the ED-VAE model, is
as a baseline, testing the models under standard
handled distinctively to ensure accurate represen-
conditions where the data aligns well with the
tation and alignment with the model’s theoretical
models’ Gaussian prior assumptions.
foundations. For the traditional VAE, the entropy
b) Dataset 2: Complex Non-Gaussian Prior:
term is derived directly from the latent variable’s
This dataset features a complex structured prior
log-variance output by the encoder, representing
modeled as a mixture of Gaussians modulated
the inherent uncertainty of the encoded represen-
by sinusoidal functions. The latent variables are
tations.Specifically,entropyiscalculatedusingthe
sampled from multiple Gaussian distributions with
formula:
varying means and scales, modulated by a sinu-
(cid:88)
soidal function to introduce non-linear interactions H[q (z)] = 0.5 (1+log(σ2))
ϕ
within the latent space. The observed data is again
generated by a linear transformation followed by where σ2 is the variance of the latent variables.
Gaussian noise, with positive samples created sim- For the ED-VAE, the cross-entropy term
ilarly to Dataset 1. This dataset tests the standard H[q (z),p(z)] is computed to measure how well
ϕ
VAE’s capability against the ED-VAE’s flexibility the encoded latent distribution aligns with theTABLE II
The differences between the models became
COMPARATIVEPERFORMANCEMETRICSFORVAEAND
more pronounced when faced with the complex,
ED-VAEMODELSONTWODATASETSWITHDISTINCTPRIOR
structured non-Gaussian prior. Here (Table II), the
DISTRIBUTIONS.METRICSINCLUDEMEANSQUAREDERROR
traditional VAE struggled to adapt, reflecting its
(MSE),KULLBACK-LEIBLERDIVERGENCE(KLD),AND
challenges with modeling a more complex prior
EVIDENCELOWERBOUND(ELBO),AVERAGEDOVERFIVE
distribution with a normal one. Conversely, the
TRIALS.LOWERVALUESINDICATEBETTERPERFORMANCEFOR
ED-VAE managed to maintain a much higher level
MSEANDKLD,WHEREASHIGHERVALUESAREBETTERFOR
of data fidelity (higher ELBO) and minimal diver-
ELBO,COMPUTEDASELBO=−MSE−KLD.
gence from the complex prior. This performance
underscores the ED-VAE’s robust adaptability and
its effective management of the latent space to
METRIC MODEL DATASET1 DATASET2
align closely with even highly irregular priors.
VAE 2.78±0.1 20.7±0.5
MSE ThesuperiorperformanceoftheED-VAEacross
ED-VAE 1.54±0.05 19.36±0.3
both datasets underscores its effectiveness in man-
VAE 10.2±0.2 12.5±0.4 aging complex data distributions. The proposed
KLD
ED-VAE 0.0±0.0 0.05±0.01
objective for training the VAE, incorporating en-
VAE -12.98±0.3 -33.2±0.6 tropy and cross-entropy components, allows for
ELBO
ED-VAE -1.54±0.02 -19.41±0.2 better control over latent space regularization. This
not only results in better alignment with the pri-
ors but also enhances the interpretability and the
specified prior. This involves evaluating the neg- quality of the data reconstructions.
ative log-likelihood of the latent variables under
VI. DISCUSSION
the chosen prior distribution, using a Gaussian
In this work, we presented a novel approach to
mixture model (GMM) [10] when non-standard
training VAEs by decomposing the ELBO formu-
complex priors are employed. The GMM param-
lation into an entropy and cross-entropy term. Our
eters are estimated from the prior data, and the
approach showcases better adaptability and per-
cross-entropyiscalculatedastheexpectednegative
formance on two synthetic datasets with variable
log-likelihood under this model. When the prior is
complexity of prior distributions. This is achieved
a standard Gaussian, the cross-entropy simplifies
through a reformulated ELBO that explicitly ac-
evaluating the Gaussian density function at the
counts for the entropy in the latent variables and
values of the encoded latent variables.
their alignment with respective priors, enhancing
the model’s ability to handle intricate data distri-
V. RESULTS
butions.
The evaluation of two VAEs, the traditional one Our findings emphasize the necessity of se-
and the entropy-decomposed one, was conducted lecting suitable prior models for generative tasks,
across two datasets designed to test model efficacy particularly highlighted by the traditional VAE’s
under different prior assumptions: a standard Nor- difficulties with complex data that deviates from
mal prior and a complex, non-Gaussian prior. normal distributions. The ED-VAE, with its flexi-
On the dataset with the standard Normal prior, ble framework, enables a nuanced interaction be-
both models performed competently (Table II), yet tween the model and the underlying characteristics
the ED-VAE showed a distinct advantage in its of the data, leading to improved learning outcomes
encoding and regularization capabilities. This was and more precise reconstructions.
evident from its more efficient data representation Despite the benefits, the introduction of entropy
and notably superior regularization, leading to a and cross-entropy terms does increase computa-
higher ELBO. The traditional VAE, while effec- tional demands, particularly affecting memory us-
tive, demonstrated less optimal alignment with age and processing power. Addressing these chal-
the normal prior, indicative of its comparatively lenges will be essential to optimize the model’s
limited capacity to regulate the latent space. efficiency and facilitate its wider adoption.In the future, we plan to apply the ED-VAE to APPENDIX
real-worldimagedatasetsthatpresentcomplexdis-
tributionalcharacteristicsandrefineitsarchitecture
to enhance its efficacy. Additionally, we aim to
develop methods for computing the cross-entropy
term without knowing the dataset’s prior, using
techniques like unsupervised learning to infer the
prior distribution, enabling the ED-VAE’s use in
scenarios with undefined priors.
REFERENCES
[1] D. P. Kingma and M. Welling, “Auto-encoding variational
bayes,” CoRR, vol. abs/1312.6114, 2014.
[2] M.D.HoffmanandM.J.Johnson,“Elbosurgery:yetanother
waytocarveupthevariationalevidencelowerbound,”2016.
[3] S. Menon, D. Blei, and C. Vondrick, “Forget-me-not! con-
trastive critics for mitigating posterior collapse,” 2022.
[4] F. Lygerakis and E. Rueckert, “Cr-vae: Contrastive regular-
ization on variational autoencoders for preventing posterior
collapse,” in 2023 7th Asian Conference on Artificial Intelli-
gence Technology (ACAIT), pp. 427–437, 2023.
[5] S.Zhao,J.Song,andS.Ermon,“Infovae:Balancinglearning
andinferenceinvariationalautoencoders,”Proceedingsofthe
AAAIConferenceonArtificialIntelligence,vol.33,pp.5885–
5892, Jul. 2019.
[6] O. Kviman, H. Melin, H. Koptagel, V. Elvira, and J. Lager-
gren,“Multipleimportancesamplingelboanddeepensembles
ofvariationalapproximations,”inProceedingsofThe25thIn-
ternationalConferenceonArtificialIntelligenceandStatistics
(G. Camps-Valls, F. J. R. Ruiz, and I. Valera, eds.), vol. 151
of Proceedings of Machine Learning Research, pp. 10687–
10702, PMLR, 28–30 Mar 2022.
[7] J. Lucas, G. Tucker, R. B. Grosse, and M. Norouzi, “Don’t
blame the elbo! A linear VAE perspective on posterior col-
lapse,” CoRR, vol. abs/1911.02469, 2019.
[8] H. Takahashi, T. Iwata, A. Kumagai, S. Kanai, M. Yamada,
Y. Yamanaka, and H. Kashima, “Learning optimal priors
fortask-invariantrepresentationsinvariationalautoencoders,”
in Proceedings of the 28th ACM SIGKDD Conference on
Knowledge Discovery and Data Mining, KDD ’22, (New
York, NY, USA), p. 1739–1748, Association for Computing
Machinery, 2022.
[9] A. van den Oord, Y. Li, and O. Vinyals, “Representa-
tion learning with contrastive predictive coding,” CoRR,
vol. abs/1807.03748, 2018.
[10] D.Reynolds,GaussianMixtureModels,pp.659–663.Boston,
MA: Springer US, 2009.A. Analysis of the Cross-Entropy Term with a Standard Normal Prior
Given a standard normal prior p(z) = N(0,I), the probability density function is expressed as
p(z) = √1 e−1 2z2. The log of this probability density function simplifies to logp(z) = −1 log(2π)−1z2.
2π 2 2
The cross-entropy term is defined as the expectation of the negative log-likelihood of the latent
variables encoded by q (z) under the standard normal prior:
ϕ
H[q (z),p(z)] = −E [logp(z)] (7)
ϕ q (z)
ϕ
Substituting the log probability density of p(z) into the equation, we get:
(cid:20) (cid:21)
1 1 1 1 1
H[q (z),p(z)] = −E − log(2π)− z2 = E [log(2π)+z2] = log(2π)+ E [z2] (8)
ϕ q (z) q (z) q (z)
ϕ 2 2 2 ϕ 2 2 ϕ
Here, log(2π) is a constant and can be taken out of the expectation. The term E [z2] represents
q (z)
ϕ
the expected squared norm of the latent variables under the distribution q (z). By minimizing the cross-
ϕ
entropy term, the encoded latent variables are encouraged to align with the characteristics of the standard
normal prior, particularly having a unit average squared distance from the origin.
B. Latent Variable Generation
The latent variables z are generated from a non-Gaussian prior, specifically a modulated mixture of
Gaussians. The generation process involves the following steps:
1) Mixture Components: The latent variables are drawn from multiple Gaussian distributions, where
each component i of the mixture has a mean µ and scale σ . The means and scales are linearly
i i
spaced between specified bounds:
µ = linspace(−3,3,3)
i
σ = linspace(0.5,1.0,3)
i
2) Latent Sampling: For each component, samples are drawn as follows:
z[i] = N(µ ,σ2)
i i
3) Modulation: Each component is modulated by a sinusoidal function to introduce non-linear
interactions:
z [i] = z[i]+sin(0.5πz[i])
modulated
4) Dimension Matching: If the desired latent dimensionality latent dim is greater than the number of
components, additional Gaussian noise is added. If latent dim is less, the dimensions are truncated.
C. Data Generation
Observable data points x are generated using a linear transformation of the latent variables followed
by the addition of Gaussian noise:
1) Transformation Matrix: A transformation matrix W is sampled:
W ∼ N(0,1), W ∈ Rlatentdim×datadim
2) Data Construction: The data points are constructed as:
x = z W +N(0,0.5)
modulated
D. Positive Sample Generation
Positive samples are generated by adding Gaussian noise to simulate slight variations within the data
distribution:
x = x+N(0,radius)
positive
where radius determines the variability of the positives from the anchor data points.