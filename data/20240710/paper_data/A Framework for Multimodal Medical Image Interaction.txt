ToappearinIEEETransactionsonVisualizationandComputerGraphics.
A Framework for Multimodal Medical Image Interaction
LauraSchütz* ,SasanMatinfar* ,GideonSchafroth ,NavidNavab,MerleFairhurst ,
ArthurWagner,MD ,BenediktWiestler,MD ,UlrichEck ,andNassirNavab
Fig.1:PhysicallyinformedMultimodalMedicalImageInteraction(MMII)Framework
Abstract—Medicaldoctorsrelyonimagesofthehumananatomy,suchasmagneticresonanceimaging(MRI),tolocalizeregionsof
interestinthepatientduringdiagnosisandtreatment.Despiteadvancesinmedicalimagingtechnology,theinformationconveyance
remainsunimodal.Thisvisualrepresentationfailstocapturethecomplexityofthereal,multisensoryinteractionwithhumantissue.
However,perceivingmultimodalinformationaboutthepatient’sanatomyanddiseaseinreal-timeiscriticalforthesuccessofmedical
proceduresandpatientoutcome.WeintroduceaMultimodalMedicalImageInteraction(MMII)frameworktoallowmedicalexperts
adynamic,audiovisualinteractionwithhumantissueinthree-dimensionalspace.Inavirtualrealityenvironment,theuserreceives
physicallyinformedaudiovisualfeedbacktoimprovethespatialperceptionofanatomicalstructures. MMIIusesamodel-based
sonificationapproachtogeneratesoundsderivedfromthegeometryandphysicalpropertiesoftissue,therebyeliminatingtheneedfor
hand-craftedsounddesign.Twouserstudiesinvolving34generalandnineclinicalexpertswereconductedtoevaluatetheproposed
interactionframework’slearnability,usability,andaccuracy.Ourresultsshowedexcellentlearnabilityofaudiovisualcorrespondenceas
therateofcorrectassociationssignificantlyimproved(p<0.001)overthecourseofthestudy.MMIIresultedinsuperiorbraintumor
localizationaccuracy(p<0.05)comparedtoconventionalmedicalimageinteraction.Ourfindingssubstantiatethepotentialofthis
novelframeworktoenhanceinteractionwithmedicalimages,forexample,duringsurgicalprocedureswhereimmediateandprecise
feedbackisneeded.
IndexTerms—Multimodalinteraction,Audiovisualfeedback,Sonification,Physicalmodelingsynthesis,Virtualreality,Augmented
reality,Human-centereddesign,Human-computerinteraction,HCI,Medicalimages,Medicalimageinteraction,Surgicalnavigation,
Brainsurgery,Braintumor,Tumorlocalization.
1 INTRODUCTION
Manyclinicaltasks,rangingfromdiagnosistosurgery,necessitateso-
phisticatedinteractionwithmultimodalmedicalimagedata,presenting
• LauraSchütziswiththeTechnicalUniversityofMunich(TUM).
asignificantchallengeduetotheintricatenatureofthesedataandthe
E-mail:laura.schuetz@tum.de
highprecisionandexpertiserequired.Thesedataareoftenpresented
• SasanMatinfariswithTUM.E-mail:sasan.matinfar@tum.de.
asstaticvolumesand,insomecases,astime-indexedsequences.For
• GideonSchafrothiswithTUM.E-mail:gideon.schafroth@tum.de.
instance,aradiologistmightbefacedwithassessingapatient’scondi-
• NavidNavabiswithConcordiaUniversity.E-mail:navid.nav@gmail.com.
tionbasedonMagneticResonanceImaging(MRI)scans,aseriesof
• MerleFairhurstiswithTUDresden.E-mail:
two-dimensional(2D)imagesofthepatient’sanatomy.Uptothree2D
merle.fairhurst@tu-dresden.de.
MRIslicesneedtobenavigatedandprocessedsimultaneouslybythe
• ArthurWagneriswithTUM.E-mail:arthur.wagner@tum.de.
radiologistsduringdiagnosis.Surgicalproceduresposeanevengreater
• BenediktWiestleriswithTUM.E-mail:b.wiestler@tum.de.
challenge, requiringunparalleledspatialandtemporalcoordination,
• UlrichEckiswithTUM.E-mail:ulrich.eck@tum.de.
precision,andadvancedmotorskills.Thispushessurgeons’cognitive
• NassirNavabiswithTUM.E-mail:nassir.navab@tum.de.
*contributedequallytothiswork.
obtainingreprintsofthisarticle,pleasesende-mailto:reprints@ieee.org.
Manuscriptreceivedxxxxx.201x;acceptedxxxxx.201x.DateofPublication DigitalObjectIdentifier:xx.xxxx/TVCG.201x.xxxxxxx
xxxxx.201x;dateofcurrentversionxxxxx.201x.Forinformationon
1
4202
luJ
9
]CH.sc[
1v51070.7042:viXraabilitiestotheirlimits.Considertheexampleofbraintumorsurgery, Thethreestandardplanesaresagittal,coronal,andaxial.Thesagittal
wherethesurgeonmustconsidermedicalimagedatatonavigatesur- planedividesthebodyintotheleftandrightsides.Thecoronalplane
gicaltoolstoatumorviaarestrictedaccesspointwhilemonitoring dividesthebodyintoposterior(back)andanterior(front)parts,while
numerouscriticalstructures,suchasvesselsandnervesthatclosely theaxialplanedividesthebodyintoitsupperandlowerparts[30].The
surroundthetargetarea,makingthetaskexceedinglydifficult. downsideofcross-sectionalanatomyisthelackofthree-dimensional
Apreciseanddynamicdeliveryofcrucialinformationregarding spatialrelationships.Atthesametime,interpretationof3Dstructures
thepatient’sanatomyandphysiology,facilitatingrobustandreliable fromanatomicalfeaturesincross-sectionalimagesischallenging[5].
perceptionofhigh-dimensionalmedicaldata,isessentialforthesuccess Toovercomethesedrawbacks,3Danatomyviewshavebeenintro-
ofmedicalprocedures. ducedtoenhancethevisuospatialperceptionofanatomicalstructures.
Research incognitive psychology hasshown thatcausally corre- 3Dviewsareadvantageousinunderstandingtheposition,shape,size,
latedmultimodalfeedbackcanactivatespecificbrainregions[37,47], andrelationshipwithneighboringanatomicalstructures,fosteringthree-
leadingtoenhancedtaskperformanceandreducedcognitiveload[44], dimensionalanatomyunderstanding.Especiallyforuserswithlower
particularlyintasksinvolvingthelocalizationandtrackingofmoving visual-spatialabilities,stereoscopicAugmentedReality(AR)3Dviews
objects. Thehumanvisualsystemoperatesefficientlybyorganizing havebeenshowntoimproveanatomylearningcomparedtodesktop-
objects spatially, yet it struggles with tracking multiple streams of based3Dvisualizations[7]. Combining3Danatomyviewsand2D
dynamicobjects[15]. Furthermore,visualfeedbackcansometimes sectionalimageshasalsoproventoenhancemedicalimageinterpreta-
distractusersfromtheprimarytaskareaandchallengetheuser’shand- tionandanatomylearning[29].Inaddition,directmanipulationof3D
eyecoordinationifnotdisplayedefficiently. Conversely,thehuman anatomyviewshasbeenreportedsuperiortopassivelyviewinginterac-
auditorysystemexcelsatperceivingsubtlechangesinmulti-layered tions[25].Anotherapproachto3Dmedicalimageinteractionutilizing
dataonafinetemporalscaleinanomnidirectionalmanner. Weaim embodimenttoimprovethelearningexperienceisthescreen-basedAR
toleveragethebenefitsofthevisualandtheauditorydomainthrough systemcalledMagicMirrorthatenablesuserstoexploreanatomyin
multimodalinteraction. relationtotheirownbody[6,8,10].Apartfromeducation,3Dviews
WhilethereareworkscombiningaudioandvisualfeedbackinMixed presented in medical AR [36] are used in image-guided surgery to
Realitysettingsformedicaltasks[9,26,43],theyprovidesimplified enhancesurgicalplanninganddecision-makingduringsurgicaltasks.
interactionsthroughnavigationalcuesonthelocationororientationof MedicalARhasproventhepotentialtoincreaseprecision,forexam-
medicalinstruments.Thisinteractionapproachcanberestrictive,asit ple,inorthopedicsurgery[2,14].Anotherstudyprovedsignificantly
prescribesspecificactionstotheuser.Instead,ourgoalistoaugment enhancedprecisionwhenusing3Danatomyviewsover2Dviewsfora
themedicalexpert’sperceptionwhilepreservingtheirdecision-making contouringtaskduringVR-basedradiotherapytreatmentplanning[12].
autonomy. Toachievethis,weproposeamethodthatenhancestheir However,mostoften,thespatialaugmentationsremainvisual.Some
perceptionofhumananatomyduringthemedicaltaskthroughmulti- workshavefocusedonaudiovisualinteractionstoleveragethepotential
modalinteraction.Byintroducinganinteractiveaudiovisualtechnique, of multiple senses in conveying spatial information during medical
weaimtocapturethecomplexityofmedicaldataandintuitivelypresent tasks. Among these works is an audiovisual AR system that uses
ittocliniciansinadynamicmanner.Theproposedmodulescombine auditoryandvisuotemporalguidancetoimprovethe3Dlocalization
anatomicaldataderivedfrombiomechanicsresearchwithmedicalimag- ofoccludedanatomy.Thestudyshowedenhancedneedleplacement
ingdata,offeringexpertuserscomprehensiveandrealisticinformation. accuracy when using audiovisual guidance [9]. A work from 2017
To evaluate the method’s effectiveness, we carried out two user showedanaudiovisualARsystemforlaparoscopicprocedures.How-
studies. Thefirststudyaimedtoassessthecapabilityof34general ever,nosignificantimprovementsofaudiovisualovervisualARwere
userstolearncorrelationsbetweenthevisualanatomyrepresentation reported[26]. Astudythatsonifiedandvisualizedthepositionand
anditsauditorycues,whilethesecondstudyfocusedondetermining angle of a magnetic coil used in transcranial magnetic stimulation
theusabilityandaccuracyofthemethodbytestingitwithnineexpert resultedinimprovedusabilityofaudiovisualoverunimodal, visual
users specialized in neuroradiology and neurosurgery. The results guidance[43].
of our studies confirmed that the proposed framework can enhance Notably, theabovestudiesprovidedaudiovisualfeedbackonthe
medicalimageinteraction. placementofmedicalinstrumentsratherthaninformationonthenature
oftheanatomicalstructures.Tothebestofourknowledge,noaudiovi-
Ourworkprovidesthefollowingmaincontributions: sualfeedbackbasedonanatomicalstructures’geometricandphysical
1. Weintroduceaphysicallyinformedmultimodalinteractionframe- propertieshasbeenproposedsofar.
workformedicalexpertsthatprovidesspatialaudiovisualfeed-
2.2 Sonification
back.
Researchhasexplorednon-visualARsolutions[31],focusingoncom-
2. We generate auditory representations of anatomical structures
parisonsbetweenaudio-tactileandvisualguidance,andhasinvestigated
usingaphysicallyinformedsonificationmodel, optimizedfor
auralaugmentedreality[51],highlightingitsuseinnavigation,edu-
real-timeapplications.
cation,andhealthcare. Sonification-theprocessofconveyingdata
3. We report results from a user study involving 34 volunteers, throughsoundinasystematic,objective,andreproducibleway-can
demonstrating a significant learning effect of the audiovisual significantlyimproveuserexperienceininteractivesystemsandinter-
representationsofanatomicalstructures. faces[22]. Thisapproachiswidelyusedininteractiondesigntonot
onlycreatedynamicandimmersiveexperiencesbutalsototransmit
4. Wereportresultsfromauserstudywithfourneurosurgeonsand
information,offerfeedback,andshapeuserbehavior[18].Sonification
fiveneuroradiologistsprovingsignificantlyenhancedaccuracyin
techniquessuchasaudificationandparameter-mappingsonification
braintumorlocalizationwhenusingMMII.
(PMSon),alongwithmorerecentdevelopmentssuchasmodel-based
sonification(MBS)[11,23],havebeencrucialintransformingdiverse
2 RELATEDWORK
datapatternsintoauditoryexperiencesthatarebothperceptibleand
2.1 MedicalImageRepresentationandInteraction intuitive.Thesetechniquesenableuserstounderstandthedata,track
Traditionally,three-dimensional(3D)medicalimages,e.g.,fromcom- itschanges,gaininsights,anddetectpatternswithinit.
putedtomography(CT)imagingorMRI,aredisplayedassectional Inthiswork,weexclusivelyexaminemedicalapplicationsofinter-
images.Sectionalimagesdepictasliceofthebody.Comparedto3D activesonification,investigatingessentialmethodologiesgroundedin
anatomyviews,theypreventstructuresfromoverlappingandthusallow psychoacousticsasthebackboneoftheseapplications.
foraclearviewofalltissueonthesectionalimagewhileimprovingthe Audification, the process of directly translating data waveforms
examinationofsofttissue.Threesectionalimagesarecommonlyused into the audible domain, serves as a foundational technique in the
tonavigatethemedicalimagedataduringdiagnosisortreatmenttasks. medicalfield,remindingusoftraditionaltoolslikethestethoscopeor
2ToappearinIEEETransactionsonVisualizationandComputerGraphics.
heartratemonitors.Thismethodhasbeenemployedtoconveysimple methodincorporatesthemultifacetednatureofmedicalimagesintoa
physiologicalsignalsinanauditoryform.Researchershaveexplored sound-basedframework,producingarichandnuancedauditoryoutput.
audifyingbioelectricsignalstolistentobrainactivities[46].However, Despiteitspotential,thefeasibilityofapplyingthesetechniquesinreal-
directlyconvertingthesedataintoaudiosignalsoftenresultsinnoisy timeinteractivescenariosremainsuncertain,callingforfurtherinvesti-
andhard-to-understandoutputs.Achievingmeaningfulresultsthrough gationtofullyunderstandandleveragetheircapabilitiesindynamic
audificationtypicallynecessitatesadvanced,sometimesinvasive,sen- environments.TheconstructionofsoundmodelsinMBSiscommonly
sorytechnologiescapableofextractingsignificantinformationfrom achievedthroughphysicalmodelingsynthesis,asdocumentedby[13].
thebodyataudiorates. Thistechniqueaimstomimicthephysicalcharacteristicsofreal-world
Thechallengeofgeneratingunderstandablesoundsfromdatahas instruments,effectivelyemulatingthebehaviorofactualobjects. A
promptedresearcherstoturntoalternativesensorymodalities,such prominentinstanceofsuchmathematicalsimulationsistheModalys
asvisualinputsfromcameras, convertingthemintoasimpler, low- softwareenvironment[17],whichfacilitatesphysicalmodelingsynthe-
dimensionalspacetoestablishmeaningfulmappingstoacousticpa- sisthroughtheapplicationofthefiniteelementmethod.Thisinvolves
rameterssuchaspitchandamplitude.Themethod,calledparameter solvingdifferentialequationsrelatedtoavibratingsystem,allowing
mappingsonification(PMSon),leveragesanexplicitmappingfunc- themodeltocapturethesystem’sdynamicattributes,includingnatural
tiontoaccuratelycontroltheoutput,reducingnoiseandenhancingthe frequencies,dampingfactors,andmodeshapes.
clarityoftheauditoryrepresentation.PMSonhasbecomethepreferred Thisliteraturereviewhighlightsagapinsystemsthatprovidedy-
techniqueinsonification,especiallyinthemedicalfield. namic,interactivefeedbackthateffectivelyincorporatesthephysical
Inmedicalsonification,PMSonhasbeenwidelyusedintranslat- characteristics of human anatomy in a distinct, multi-layered, and
ingthepositionorstateofnavigatedsurgicalinstrumentsrelativeto intuitive manner. There is a lack of systems that are easy to learn,
particularstructures[33,41,49],convertingspatialcharacteristicsof combiningbothauditoryandvisualfeedback.Indiscussingsonifica-
medicalimagingdataintoacousticfeatures[1],andintransforming tionformedicalapplications,itiscrucialtoconsiderthephysicsof
signalsderivedfromsurgicaldevices,suchasbloodlossmonitorsor theunderlyinganatomicalstructures. Thisconsiderationisessential
oximeters,intosound[32].PMSonhasprovenparticularlyeffectivein fordesigningmodelsthatarenotonlyextendablebutalsomaintain
augmentingimage-guidednavigation,offeringnotableimprovements systemintuitiveness.Furthermore,themajorityofexistingstudieshave
inaccuracyandguidance[19,35,52]. adoptedabroadapproachwithoutsufficientlyincorporatingtheusers’
Fundamentalstudies[16]ontheimpactofmappingstrategiesin backgroundknowledge,particularlytheirmedicalexpertise.
sonificationhaveidentifiedpitchastheprimaryauditorydimension
commonlyusedinmappings. Effortstoexpandmappingfromone- 3 MULTIMODALMEDICALIMAGEINTERACTIONFRAMEWORK
dimensional [39] and two-dimensional data [35,53,55] to three or- Weproposeamultimodalmedicalimageinteraction(MMII)framework
thogonaldimensions[52,54]haveshownpromiseinenhancingspatial thatprovidesdynamicaudiovisualfeedbackduringinteractionwith
awareness. However,thelearnabilityandintuitivenessofthesetech- medicalimagedata(Figure1).Unlikeconventionalunimodalinterac-
niques,particularlyforcriticalapplicationssuchassurgery,remain tionmethodsthisaudiovisualfeedbackprovidesdetailedinformation
uncertain. ontheanatomy’sgeometry,textureandsize. TheMMIIframework
Conversely,theissueofeffectivelyimplementingauditoryguidance takesmedicalimagesandphysicalpropertiesoftherespectivetissue
formultidimensionaltaskshasbeenaddressedbyKantanetal.[28], asinputtocreateavisualizationandsonificationmodel.Throughuser
suggestingthatsimplifyingmultidimensionaltasksintounidimensional interactionwiththesemodelsreal-timephysicallyinformedaudiovisual
taskscouldleadtomoreefficientoutcomes. Inthisapproach,users feedbackisgenerated.
processeachdimensionanditscorrespondingsoundpropertysequen-
tiallyratherthandealingwithconcurrentguidancepresentations.This 3.1 Causality-InformedFramework
methodhasbeenshowntoreducecompletiontimesandinterruptions, Medicalprofessionalshaveadeepunderstandingofcomplexanatom-
ultimatelyimposingalowercognitiveload. icalstructuresthroughextensiveeducationandpracticalexperiences.
However,sincemostmedicalapplicationsfocusontoolnavigation, Thisknowledgeenablesthemtoeffectivelyassociateauditoryrepre-
requiringthemodelingofdataacrossCartesianorpolarcoordinate sentationswithanatomicaldata,providedthesonificationmodelhas
systemsinmultidimensionalspace,neithersimultaneousmultidimen- causalrelevancetothecontext.Thisaspecthighlightstheimportanceof
sionalmappingnorsequentializingintomultiplenavigationstepsis user-centereddesign,whichaccountsforusers’pre-existingknowledge
optimal. Thiscomplexityhindersthescalabilityofthesemodels,es- toenhancetheirinterpretationandinteractionwithmedicaldata.
peciallywhentrackingmultipleobjectsofinterest,leadingtoreduced Weproposeacausality-informedmethodforanatomicalsonification,
intuitivenessandprolongedlearningperiods.Furthermore,thetaskof focusingontheenergeticsofsoundevents-thediverseenergeticflows
enhancingdatadimensionalitywithPMSontoaccuratelydepictcom- involvedingeneratingsonicphenomena. Sound,inrealityiscausal,
plexphysicalfeaturessuchasanatomicalshapesandtexturespresents event-based,andrelational. Itoccurswhenenergeticphenomenaos-
asignificantchallenge. cillateatspecificratesandintensities,withthesewavespropagating
Schützetal.[42]developedamethodthat,contrarytothesonifica- acrossmultipleactivityzones. Asoundeventisconsideredtohave
tionofatool’sdistancetoapredefinedtarget,employsaninteractive occurredonlywhenlistenersintroducetheirperceptualpointofaccess
sonificationforshapeexplorationtoconveyanatomicalshapes,thus tothesoundscapethroughtheirentanglementwithitscomplexdevelop-
improvingtheunderstandingofgeometricalconfigurationsthrough ment.Ourapproachchallengestheoftentechnicalabstraction-focused
sound. Nevertheless,thisapproachonlyprovidessonificationof2D literature,advocatingforarelationalontologythatconsiderstheener-
shapes,limitingitssuitabilityformorecomplexapplications. getic,matter,resonance,andperception-actioninterplay. Designing
Yet,sounduniquelyenablestheconveyanceofmultilayeredinfor- acausality-informedframeworkrequiresaddressingkeyontological
mationthroughthesimultaneouspresentationofmultipledatastreams, questions: What generates sound, why does it occur, and how do
capitalizingononeofitsinherentstrengths.Auditorysensations,char- listenersengagewithit?
acterized by qualities such as brightness, roughness, fullness, and Thecausality-informedmethodenablesustouseproximity-based
sensorypleasantness,asdescribedin[56],areinherentlymultidimen- sonificationinanefficientanddistinctivemanner.Insteadoftreating
sional.Thisisindeedduetotheirrelianceonthespectraldistribution proximityasanabstractn-dimensionalconceptthatinitiatessoundpro-
offrequencies.Suchmultidimensionalitygreatlyexpandsthescopefor ductionuponchangesinrelationalproximity,weintegrateanatomical
developingsonificationmodelsandmappings,allowingforacompre- knowledgefromtheinteractioncontext. Bylocalizingandadjusting
hensiverepresentationofdatacomplexitythroughdetailedmappings. the perceptual access point to objects based on the location of the
Model-basedsonification(MBS)transformshigh-dimensionaldata user-guidedinteractionpoint,weaimtoimprovetheperceptibilityof
intoauditorymodels,facilitatingtherepresentationofintricatedata proximity. Thisapproachisconsideredparticularlyeffectivefordy-
patterns,includingmedicalimagedata. Ashighlightedin[34],this namicallyactivephenomenasuchasheartratevariability.Followinga
3Table1:PhysicalpropertiesoftheanatomicalstructuresusedinStudy1basedonresearchinbiomechanics1[3,4,27]:stiffness(Youngmodulus
(kPa)),density(kg/m3),Poisson’sratio
Tissue Youngmodulus(kPa) Density(kg/m³) Poisson’sratio
Rigid Vertebra(bone) 50-20,000 1908±133 0.2-0.3
BloodVesselWall 1,000-20,000 1102±64 0.2-0.3
IntervertebralDisc(cartilage) 0.3-1,000 1100±1 0.1-0.49
Semi-rigid SpinalCord 0.5-20 1075±52 0.45-0.49
Soft WhiteMatter 0.5-3.5 1041±2 0.4-0.5
GreyMatter 0.5-2.5 1045±8 0.4-0.5
Glioma(braintumor) 0.1-1.5 tumorgradedependent 0.3-0.5
causalapproach,wefacilitateuserinteractionwitharesonatingmodel adjustingthealbedovalueofthemeshes’materialsinUnity2.Besides
todeducestructuralinsightsfromsonicfeedback.Thus,distincttimbral changesinmaterialorcolor,wecanimagineawidearrayofvisual
differencesarisebetweenobjectswithvaryingtexturesandstructures, feedback. 3Dmodeldeformationcould,forexample,besuitablefor
engaginglistenersasactiveparticipantsintheexplorationprocess.ers achievingcausality-informedvisualfeedback.
asactiveparticipantsinthesoundgenerationprocess.
3.4 Interaction
3.2 HumanAnatomyandTissueCharacteristics
Acausalrelationisalsomaintainedintheuser’sinteractionwiththe
HumananatomyconstitutestheinputtotheMMIIframework.Begin- MMIIframeworkbyadoptinganevent-basedinteraction. Justlike
ningwithchemicalsandprogressingtocellularstructures,weobserve playinganinstrumentcreatestime-basedoutputfromtime-baseduser
ahierarchicalorganizationinthehumanbody. Atahigherlevelof input,asingleinputeventtotheMMIIframeworkwillalsocreatea
abstraction,tissuesserveasthefoundationalelementsofallanatomi- singular,real-timeoutput.Analogous,continuoususerinputwillresult
calstructureswithinthehumanbody.Thecomplexhumanorganism inacontinuousaudiovisualoutput.
comprises four basic tissue types: nervous, epithelial, muscle, and
connectivetissue. Examplesofnervoustissuearethespinalcordor 3.5 SonificationModel
greymatterinthebrain.Anexampleofepithelialtissueisthesimple
Theproposedsonificationmodeldrawsinspirationfromthegeneral
squamousepitheliumformingthewallofanartery.Thecardiacmuscle
frameworkintroducedin[34],yetdistinguishesitselfthroughthefol-
isanexemplarymuscletissue;connectivetissuecanbefoundinbones,
lowingpoints:
blood,andevencartilage.Thesetissuesarethebuildingblocksofevery
Theframeworkoutlinedin[34]presentsabroaddefinition,claiming
organ,suchasthespinalcordandthebrain,whichinturnformorgan
thatanymedicalimagedatacanbesonifiedusingthisapproach.The
systems,exemplifiedbythehumancentralnervoussystem.
processinvolvesextractingallrequisitedetailsformodeldefinition
Tissuesactasthefoundationalfabricofthebody,intricatelywoven directlyfrommedicalimages,leadingtoahighlyintricateanddetailed
intothesmallestunitsvitalformodelinganatomicalstructures.These modelingprocess. Itintroducesapredominantlyconceptualdesign
structuressignificantlyaffectthedynamicbehavioroforganismsand frameworkthat, whileintriguingfromamethodologicalstandpoint,
areessentialinphysicalmodelingsoundsynthesis.Utilizingtheme- provideslimitedexamplestoshowcaseitsapplicability.Moreover,its
chanicalpropertiesoftissues,suchasdensity,elasticity,energyloss, feasibility,particularlyforreal-timeapplicationsandrapidprototyping
andPoisson’sratio,enablesthesimulationandcreationofanatomi- forrealisticexperimentalscenarios,remainsunexplored.
callyrelevantsoundscapes.Thereisasignificantbodyofresearchin
Toaddressthesechallenges,westreamlinedthemodelingprocess
biomechanicsprovidingdetaileddataonthesephysicalpropertiesof
byutilizingtwodistinctdatasets:thephysicalpropertiesofthetargeted
tissue1 [3,4,27],suitableforourmodelingpurposes,asindicatedin
tissue (Table 1) and the geometrical shape of the relevant anatomi-
Table1.Figure2depictsaroughplacementoftheanatomicalstructures
calstructures(Figure2). Thesestructuresarederivedfrommedical
usedinStudy1alongtheirphysicalproperties.
images, asdescribedintheprevioussection. Theyaredefinedasa
Moreover,thegeometricalconfigurationandmorphologicalchar- polygonmesh,acollectionofvertices,edges,andfacesthatdetermines
acteristics of anatomical structures are pivotal in determining their theshapeofanobjectusingafinitenumberofquadrilaterals. This
dynamicbehaviors.Theseaspectssignificantlyimpacttheprocessesof setupallowsforthenumericalsolutionofthemodel,particularlythe
visualizationandsonificationthroughwavepropagationwithinstruc- calculation of the displacement of each vertex, through differential
tures,emphasizingthedynamicinteractionbetweentexture,shape,and equations.Consequently,themodelcancapturethesystem’sdynamic
functioninbiologicalsystems.Suchinterconnectionsarecriticalinthe attributes,includingnaturalfrequencies,dampingfactors,andmode
developmentoftheproposedcausality-informedframework.Medical shapes.Toinitializethemodel,thephysicalpropertiesofthetargeted
imagingtechniques,includingCT,MRI,andultrasound,allowforvisu- tissue,includingelasticity,density,andPoisson’sratio,aremappedto
alizinganatomicalstructuresbytransformingsignalsfromtheimaging themodelparameters.
systemintoimageintensityvalues.Thistransformationfacilitatesthe
Throughthis,weoptimizethemodel’scomplexitywhileensuring
identificationofshapesandpatterns,makingthesemethodsessential
itretainsthenecessaryeffectivenessingeneratingdistinctacousticre-
foraccuratelydepictingthegeometricalconfigurationsofanatomical
sponsessuitablefortheproposedapplication.Asaresult,thisapproach
structuresandprovidingvitaldatafortheirprecisemapping.
expandsthescopeofsonificationmodeling,encompassingadiverse
arrayofanatomicalstructuresforreal-timeapplications.
3.3 VisualizationModel
Forsoundgeneration,themodelmustbeexcitedbyapplyingforce
Followingacausality-informedapproach,thevisualizationmodelaims tooneormorevertices. Thisenablesthecalculationofeachnode’s
toportrayarealisticrepresentationofthegeometryofhumantissue. displacementinanoscillatoryform,whichisthenextractedasanaudio
Segmentationsoftask-relevantanatomicalstructuresareobtainedfrom signalfromoneormorevertices. Alteringthepositionsoftheinput
themedicalimagedata. Thesegmentationswereturnedintosurface andoutputverticessignificantlyaffectstheresultingsound.Utilizing
meshes.Triggeredbyuserinteraction,real-timevisualfeedbackwas thismodel, wecreateacousticvirtualmodelscapableofproducing
created.Amongdynamicvisualchanges,weexperimentedwithscal- anatomicallyinformedaudiosignalsthroughuserinteraction. This
ing and colorization effects. Colorization effects were achieved by approachintegratescrucialanatomicalinformationintothesynthesis
1https://itis.swiss/virtual-population/tissue-properties/database/density/ 2Unity(https://unity.com/)
4ToappearinIEEETransactionsonVisualizationandComputerGraphics.
Fig.2:SchemaofthetissuesusedinStudy1alongwiththeirphysicalpropertiesofstiffness,densityandpoisson’sratioaswellastheirresulting
melspectograms.
process, offering users rich and realistic insights. Finally, through thefollowingresearchquestions:
minimalreal-timepreprocessingofthesignals,includingpitchshifting
• Canuserslearntheaudiovisualcorrespondenceoftheauditory
andamplification,weaimedtoenhancetheperceptibilityofthesounds.
andvisualrepresentationofanatomy?(Study1)
Forimplementingthisapproach,weutilizedtheModalyssoftware[17].
Theaudiosignalsproducedthroughthistechniquearerepresentedin • Isphysicalmodelingsynthesisasuitablesonificationapproach
Figure2,usingaMel-spectrogram. Readersarealsoencouragedto tocreatedistinguishableauditoryrepresentationsofanatomy?
refertothesupplementaryvideotolistentothecorrespondingaudio (Study1)
examples.
• Canaudiovisualinteractionimprovetheusabilityandaccuracyof
There’sanotablescenarioinvolvingstructuresthatareinherently
amedicallocalizationtask?(Study2)
dynamic, such as blood flow, which naturally generates vibrations
withoutanyexternalinteraction.Inthesecases,weuseasignalasthe
modelexcitor,specificallygeneratedbyaregularheartbeat.Employing
5 STUDY1-MULTIMODALCORRESPONDENCELEARNING
advancedsamplingtechniquessuchasgranularsynthesis,weadjustthe We chose to conduct an online study to test the learnability of our
fine-grainedsamplesofaprerecordedaudiofileofKorotkoffsounds. frameworkanddeterminehowwellthedifferentanatomicalstructures
Thisadjustmentmimicsthevariationsinbloodflowrelativetoheart canbedistinguishedbasedonsound. Theonlinequestionnairewas
rate,toolposition,andthesurroundinganatomicalstructuresofthe takenondesktopandlaptopcomputers. Totestourframeworkfora
bloodvessels. Korotkoffsoundsarecapturedusingastethoscopeor widerangeofhumantissue,wechosetwoareasofthebodycontain-
Dopplerdevicewhenabloodpressurecuff,positioneddistaltothe ingstructuresofvaryingphysicalproperties:thespineandthebrain.
cuff, modifies arterial flow [40]. Since the circulation depends on Fourdiversespinestructuresandthreebrainstructureswereselected:
thestructuralcompositionofthevasculature,weutilizethegranular vertebras(bone),vertebralarteries(bloodvessels),intervertebraldiscs
synthesizertostimulateamodelthatmimicsthevesselwalls. This (cartilage),spinalcord(nerves),greymatter,whitematter,andbrain
methodallowsforthesimulationoftwodistinctscenarios:thesurgical tumor.
tool’sproximitytothevesselsandthecuttingofthevessels. Itdoes
sobyalteringtheaudiobasedonwhetherornotthephysicalmodelis 5.1 StudyImplementation
engaged. TheonlinequestionnairewascreatedwithSoSciSurvey3.Videoclips
andsoundsampleswereusedtosimulatetheaudiovisualinteraction.
4 EXPERIMENTS ThevideoandsoundfilesarerecordingsofasystembuiltinUnity
andMax4. TwosceneswerepreparedinUnity, onecontainingthe
Weconductedtwouserstudiestoincrementallytestourinteraction structuresofthecervicalspine5andonecontainingthestructuresofthe
framework.Thefirststudyevaluatedtheperceptibilityoftheaudiovi-
brain.The3DbrainmodelswerecreatedfromMRIimagedatainthe
sualfeedbackinanisolatedmanner.Toseparatethetaskcomplexity
UniversityofCaliforniaSanFranciscoPreoperativeDiffuseGlioma
fromtheaudiovisualfeedback,wechoseapassiveinteractivesetupin
MRI(UCSF-PDGM)datasetpubliclyavailableontheNationalCancer
theformofanonlinestudy.Thisenabledustotestthegeneralfeasibil- Institute’s Cancer Imaging Archive6. FAST, FMRIB’s Automated
ityoftheframeworkandthesuitabilityofphysicalmodelingsynthesis SegmentationTool7wasusedtosegmentameshofthebrain’swhite
torepresenthumananatomyintheauditorydomain. Inthesecond
matterandgreymatterfromoneofthe3DMRIimagesinthedataset.
study,wewereinterestedinevaluatingthepotentialoftheproposed
methodinamorerealisticinteractionsetup,wheretheuserisprovided 3SoSciSurvey(https://www.soscisurvey.de/)
withmultimodalfeedbackwhilefocusingonatime-sensitivemedical 4Max(https://cycling74.com/products/max)
taskinaVirtualRealityenvironment.Study2focusedontheusability 5CervicalSpine3DModel(https://rigmodels.com/)
andaccuracyoftheframeworkforamedicallocalizationtask,more 6CancerImagingArchive(https://www.cancerimagingarchive.net/)
precisely,braintumorlocalization.Thetwostudiessoughttoanswer 7FAST(https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FAST)
5A3Dobjectofthebraintumorsegmentationwasalreadyprovidedas afewopenquestionstocapturequalitativefeedbackonthevisualand
partofthepatientdata.Blender8wasusedtocleanandeditthebrain sounddesign.
meshespriortoUnityimport.Themesheswerevisualizedusingsurface
shadersrenderedusingphysically-basedrenderinginUnity(Version 5.4 Results
2022.3.17f1LTS).Theuseofraycastingandmeshcollidersenabled
5.4.1 PerformanceandTaskLoad
amouseclickinteraction.Tocreatevisualfeedback,theobjects’size
andmaterialchangedonclick.AtthesametimeUnityOSC9,aUnity Participantswhoseratesofcorrectanswersfelloutsidetherangeofthe
pluginbasedonOpenSoundControl(OSC),anetworkprotocolfor
mean±threestandarddeviationswereconsideredoutliers.Ourcriteria
interactivecomputermusic[50],wasusedtosendmessagesfromUnity foroutlierdetectiondidnotidentifyanyoutliers. Giventhebinary
to Max. In Max, the incoming OSC messages were routed to the natureofthestudy’soutputvalues,withresponsescategorizedaseither
tissue’scorrespondingphysicalmodeltogeneratetheaudiofeedback. ’correct’or’incorrect,’weemployedMcNemar’stesttoassessthesta-
Modalys10forMax,asoftwareforvirtualmusicalinstrumentsbasedon tisticalsignificanceofthecorrectanswersacrosstwopairedconditions:
physicalmodels,wasusedtocreateinstrumentsfromthe3Danatomy V2Avs. A2V(Table2),andsinglevs. multiplestructures(Table3).
models.TheModalysmodelswereparameterizedusingthephysical McNemar’stestrevealedastatisticallysignificanteffectofmodalityon
propertiesofthedifferenttissuesfoundinTabel1.
responseaccuracy(p<0.01).A2Vresultedinasignificantlyhigher
rateofcorrectanswersthanV2A(Table2). Similarly,acomparison
One significant challenge we encountered was that certain struc-
betweenthesingleandmultiplestructuresstagedemonstratedasig-
turesdidnotproduceaudiblevibrationsorwereparticularlydifficultto
nificanteffect(p<0.001),suggestingthatthenumberofstructures
sonifyduetotheirinherentcharacteristics.Forinstance,braintissue,
influencedresponsecorrectness(Table3).
beingsofttissue,oftenresultedinsoundsthatwerenoisyorinaudible.
Evenwhenthesesoundsareaudible,theymightnotbeeasilyinter- Weconductedpairedsamplest-testsforeachNASA-TLXsubscale
pretable. Toaddressthis,wemodifiedthesignalintoanaudibleand sincetheassumptionofnormalityforthedifferencesbetweenpaired
perceptiblesoundspacebytransforming,scaling,andsometimesinvert- scoreswasverifiedusingtheShapiro-Wilktest.Theanalysisrevealed
ingtheinformation,whilecruciallypreservingtherelationalcontextof
significantdifferences(p<0.001)inoveralltaskload(NASA-TLX)
thesestructuresincomparisontootherobjectswithinthescene.This betweenV2AandA2Vtasks(Table2). Participantsreportedsignifi-
approachensuresthatuserscanperceivethedatawhilemaintainingthe
cantlyloweroveralltaskload(p<0.001)andFrustration(p<0.001),
essentialcausalrelationshipoftheunderlyingstructures. and significantly better Performance (p<0.001) for the A2V task
whencomparedtoV2A.Conversely,MentalDemand(p<0.05),Phys-
5.2 Participants ical Demand (p<0.05), Temporal Demand (p<0.05), and Effort
(p<0.001)wereratedsignificantlyhigherundertheA2Vcondition
34volunteerswithameanageof32.4±5.9yearsparticipatedinthe
(Figure4).Nosignificantdifferencesintaskloadwerereportedforthe
study. Twelveindicatedtobewomenand22tobemen. Thestudy
singleversusmultiplestructuresstage(Figure5).
includedparticipantswithmedicalandnon-medicalbackgrounds.Their
occupationsrangedfromradiologisttobiomedicalresearcher,graphic 5.4.2 QualitativeFeedback
designer, and music teacher. None of the participants indicated to
Theparticipantsfoundidentifyingsoundsofhardstructureslikebone
sufferfromahearingorvisionimpairment.Regardingtheparticipants’
easier than soft structures such as brain tissue. They especially re-
musicalexperience, 12indicatedlisteningtomusicbutnotmaking
quested more apparent sound differences between the brain tissues
musicthemselves,13indicatedplayingoneinstrument,fiveindicated
(whitematter,greymatter,tumor).Theynotedthatsomesoundswere
playingmorethanoneinstrumentonaregularbasis,andfourindicated
beingprofessionalmusicians.
Table2: Meansandstandarddeviationsofrateofcorrectanswers(in
5.3 StudyProcedure
%),overalltaskloadandNASA-TLXsubscales([0,100];smallerbetter)
The online study followed a within-subject study design. The par- fortheV2AandA2VquestiontypesinStudy1summarizedacrossthe
ticipants were first asked some questions about their demographic singleandmultiplestructuresstages
background,suchasage,gender,theirmedicalandmusicalknowledge,
beforetheywereguidedthroughatrainingsectionexplainingthein-
n=34 VisualtoAudio AudiotoVisual
teractionframeworkandprovidingvideosshowcasingthesimulated
audiovisual interaction with all the spine and brain structures. The Rateofcorrectanswers(%) 0.67±0.47 0.75±0.44
trainingsectionwasnottime-constrained,andtheparticipantshadthe NASA-TLX-Overall 42.76±30.37 41.78±29.52
chancetogetacquaintedwiththesoundsandvisualsuntiltheyfelt NASA-TLX-MentalDemand 55.64±28.69 57.48±26.10
confidentenoughtoproceedtothetestsection. NASA-TLX-PhysicalDemand 13.03±14.19 14.23±15.92
NASA-TLX-TemporalDemand 32.53±26.34 34.05±28.03
The test section was subdivided into two parts: Audio to Visual
NASA-TLX-Performance 52.47±27.17 46.75±27.76
andVisualtoAudio(Figure3).IntheAudiotoVisual(A2V)partthe
NASA-TLX-Effort 55.06±27.37 52.11±25.02
participantswereprovidedwithasoundsampleandaskedtochoosethe
NASA-TLX-Frustration 48.30±30.96 46.06±30.80
correctvisualcorrespondencefromasetofthreevideoswithoutsound.
IntheVisualtoAudio(V2A)part,theyweregivenavideoclipwithout
sound,forwhichtheyhadtoselectthecorrespondingaudiofilefrom Table3:Meansandstandarddeviationsofrateofcorrectanswers(in%),
asetofthreeansweroptions. Bothparts,A2VandV2A,contained overalltaskload,andNASA-TLXsubscales([0,100];smallerbetter)for
seven questions followed by a raw NASA-TLX [20,21]. The two thesingleandmultiplestructuresstagesinStudy1summarizedacross
questiontypeswerecontainedinbothstagesofthestudy:singleand V2AandA2Vquestiontypes.
multiplestructures.Thestageswereexecutedintheordermentioned.
Inthesinglestructurephase,theparticipantshadtoselectonevisual n=32 Single Multiple
correspondingtoonesound(A2V)orviceversa(V2A);inthemultiple
Rateofcorrectanswers(%) 0.63±0.48 0.78±0.42
structuresphase,theyhadtoselectthecorrectsequenceoftwosounds
NASA-TLX-Overall 42.91±29.36 41.63±30.52
correspondingtotheorderoftwovisuals(V2A)orviceversa(A2V).
NASA-TLX-MentalDemand 57.98±25.41 55.14±29.26
TheorderoftheA2VandV2Apartswasrandomizedwithinthesingle
NASA-TLX-PhysicalDemand 12.42±12.79 14.84±17.00
andmultiplestructuresstage.Theonlinequestionnaireconcludedwith
NASA-TLX-TemporalDemand 33.03±26.39 33.55±28.00
NASA-TLX-Performance 51.25±27.10 47.97±28.03
8Blender(https://www.blender.org/)
NASA-TLX-Effort 54.92±23.43 51.75±28.72
9UnityOSC(https://thomasfredericks.github.io/UnityOSC/)
NASA-TLX-Frustration 47.86±30.60 46.50±31.18
10Modalys(https://support.ircam.fr/docs/Modalys/current/)
6ToappearinIEEETransactionsonVisualizationandComputerGraphics.
Fig.3:TwoquestiontypesinStudy1:AudiotoVisual(A2V)andVisualtoAudio(V2A).InA2Vasoundwasplayedanditsvisualcorrespondence
hadtobeselected;inV2Aavisualwasshownanditsaudiocorrespondencehadtobeselectedfromachoiceofthreeaudiofiles.Melspectograms
areusedtosymbolizetheaudiofilesinthisfigure.
our second research question by proving the suitability of physical
modelingsynthesisforanatomysonification.
TheNASA-TLXPerformancesubscaleshowedthattheparticipants’
subjectivelyperceivedperformancealignedwiththeirobjectiveper-
formance. TheywereoverallmoreaccurateintheA2Vtaskandthe
multiplestructuresstage.
Whiletheonlineformathelpedthegeneralisabilityofourresults
byreachingalargerandmorediversegroupofparticipants,offeringa
directinsteadofapassiveinteractionmighthaveimprovedthetransfer-
abilityandrelevanceoftheStudy1resultstoStudy2.Furthermore,a
studycomparingmusicalexpertswithnon-musiciansmightresultin
interestingfindings.
Fig.4:NASA-TLXtaskloadratingsforVisualtoAudio(V2A)andAudio
toVisual(A2V)questiontypesinStudy1. 6 STUDY2-MULTIMODALBRAINTUMORLOCALIZATION
AfterhavingproventhatMMIIhasthepotentialtoestablishaudiovisual
associationsforanatomystructures,weconductedasecondstudyto
examinetheframework’scapabilitiesforarealmedicaltask.Weeval-
uatedtheframeworkforbraintumorlocalizationwithneurosurgeons
andneuroradiologists.Localizationofthetumorareais,forexample,
performedduringdiagnosisorsurgery.Duringdiagnosis,thetumoris
identified,anditslocationisdeterminedinaradiologyreport.During
surgery,thetumorneedstobelocalizedtoplanasurgicalapproachthat
willpreventdamagetofunctionalregionsofthebrainanddetermine
thevolumetoberesectedfromthepatient’sbrain.Thistaskrequires
simultaneous integration of information from several sources, such
assliceviews,instrumentlocation,andmonitoringnumerouscritical
structuresinthebrain.Thistaskpushessurgeons’cognitiveabilities
andthuspresentsastellarusecaseformultimodalfeedback.Totestthe
Fig.5:NASA-TLXtaskloadratingsgroupedbythesingleandmultiple
localizationtaskinasimulatedmanner,webuiltaVirtualReality(VR)
structuresstageinStudy1.
applicationfortheMetaQuest211.Wecomparedconventionalvisual
medicalimageinteractionwithaudiovisualinteractionusingMMII.
dominantanddistinct,makingthemeasytorecognize.However,others
6.1 StudyImplementation
weretoosimilar,andthus,distinctionwashampered.Participantswere
morecertainoftheiranswerswhenthequestionsinvolvedthesedom- TheapplicationwasbuiltinUnityandMax.Toreplicatethestandard
inant,distinctsounds. Whenaskedhowtoimprovethesonification, interactionwithmedicalimagedata,weincludedsliceviewsofthe
theyresponded,"makethesoundsmoredifferentfromeachother"and threeplanes(axial,coronal,sagittal)intotheVRenvironmentusingthe
"makebrainsoundssofter".Overall,theyfoundthevisualizationswere
UnityVolumeRenderingpackage12(Figure6).Inaddition,weplaced
appropriateforthepurpose. a3DmodelofthebrainintotheUnityscene.Thesliceviewsandthe
brainmodelwerecreatedfrom3DMRIimagesoftheUCSF-PDGM
5.5 DiscussionStudy1
dataset.nii2mesh13wasusedtoconverttheNIfTI3Dvoxelimagesof
thetumorandbrainsegmentationstotriangulatedmeshes.Besidesthe
Ourresultsshowedthatusersarecapableofcreatingaudiovisualas-
greymatterandtumorsegmentation,wefurtherincludedmodelsofthe
sociations between visual and audio representations of anatomical
cerebralarteriesaswellastherightandleftcorticospinaltract(motor
structuresinarelativelyshortamountoftimeregardlessofmusical
pathways). Thesecriticalstructuresmustnotbecutduringsurgery,
expertise.Asignificantincreaseincorrectanswers(p<0.001)from
thereforewedecidedtoincludetheminourVRenvironment.
63.4%instageone(singlestructures)to77.9%instagetwo(multiple
structures)indicatesthattheMMIIframeworkislearnable.Although 11MetaQuest2(https://www.meta.com/de/quest/products/quest-2/)
thecomplexityofthetaskincreasedinstage2(multiplestructures) 12UnityVolumeRendering(https://github.com/mlavik1/UnityVolumeRendering)
therateofcorrectanswersincreased.Thestudyfurthermoreanswered 13nii2mesh(https://github.com/neurolabusc/nii2mesh)
7Fig.6:Left:Sceneviewinsidethehead-mounteddisplayofthethreemedicalslicesandthebrainduringthevisualandaudiovisual(MMII)condition
inStudy2.Right:Illustrationofthesoundsphere-theaudiblerangearoundthepointofinteractionattheendofthecontrollerray.Thedistance
fromtheinteractionpointtotheanatomicalstructuresdefinestheamplitudeofthestructures’sound.
Acontroller-basedinteractionwiththebrainanatomywaschosenfor graphicbackground(age, gender, profession), medicalandmusical
thelocalizationtask.UnityXRInteractionToolkit(version2.5.2)was expertise,thetrainingphaseofthefirstoftwostudyconditionsstarted.
used.Aninteractionatafixeddistancefromthecontrollerpreventedthe Duringtraining,thedoctorscouldfamiliarizethemselveswiththeVR
controllerfromoccludingthebrainmodelvisualization.Twoconcentric environment,thecontrollerinteraction,andthevisualandaudiofeed-
sphereswereattachedtotheendpointofarayinlinewiththepointing back.Thesametaskwasperformedinbothconditions.Themedical
directionofthecontroller(Figure6).Theinnerwhitespheremarked usershadtolocalizeandmarkthetumorvolumebyevenlyplacing
thecurrentpointofinteractionwiththebrainmodelanddeterminedthe spheresonthesurfaceofthetumor. Theyhadtolocalizesixtumors
locationofthethreeslicingplanes.Thelargegreenspheredetermined percondition.Boththeorderofthetumorsandtheorderofthevisual
theaudibleradiusaroundthepointofinteraction.Allstructuresinside andaudiovisualconditionswererandomized. Aftereachcondition,
the sound sphere were sonified. The amplitude of the sounds was thedoctorsfilledoutarawNASA-TLXandafewspecificquestions
influencedbythedistancebetweentheinteractionpointandtheclosed regardingthetaskandthe(audio)visualinteraction.
pointontheobject’smeshinsidethesoundsphere.Thedistancewas
normalizedfortheradiusofthesphere. 6.4 DataAnalysis
TheaudiofeedbackwastriggeredbysendingOSCmessagesfrom Weusedanoverlap-basedmetrictodeterminetheaccuracyofthetumor
theAndroidapplication(runningonaMetaQuest2)toMax(running localizationtask.Theoverlapofthemarkedtumorarea(MT)withthe
onalaptop).Themessagesincludedthenamesofthestructuresinside groundtruthtumorarea(GT)wasmeasuredusingtheSørensen–Dice
thesoundsphereandtheirrespectivedistancetothepointofinteraction. coefficient,themostcommonlyusedmetricinvalidatingmedicalvol-
Again,ModalysforMaxwasusedtocreatephysicalmodelsfromthe umesegmentations[45].Westoredboththelocationofthe3Dtumor
anatomystructures. Unlikethepreviousstudy,wealsomadeuseof objects and the locations of the user-placed marking spheres in the
granularsynthesistocreateapulsatingsoundtorepresentthecerebral Unitycoordinateframe.Weusedthespherelocationsasinputpointsto
arteries. AllsoundsweresynthesizedinMaxandplayedviastereo createameshusingtheConvexHullfunctioninSciPy14.Thevolumeof
speakersconnectedtothelaptopwithAuxcable.Thestreaminglatency thishullwasusedasthemarkedtumorvolumeintheDicecalculation.
wasminimalandnotperceptibletothehumanear.
2|MT∩GT|
6.2 Participants Dicecoefficient= (1)
|MT|+|GT|
Ninemedicaldoctorstookpartinthestudy,onewomanandeightmen.
The participants had an average age of 30.1 years, with a standard 6.5 Results
deviationof2.8years. Thestudyincludedoneseniorneurosurgeon 6.5.1 PerformanceandTaskLoad
andthreeattendingneurosurgeons,aswellasoneseniorneuroradiol-
A Shapiro-Wilk test showed the normality of the Dice and NASA-
ogistandfourattendingneuroradiologists. Whiletheneurosurgeons
TLXdata.Valuesoutsidethemean±threestandarddeviationswere
indicatedperformingbraintumorsurgeryonadailytoweeklybasis,
considered outliers and removed from the sample. To account for
theneuroradiologistsstatedtotakebrainMRIsandcreateradiology
differingsamplelengthsafteroutlierremoval,valueswererandomly
reportsonadailytoweeklybasis. Noneofthedoctorsindicatedto
sampledfromthelargersampletomatchthesizeofthesmallersample.
haveavisionorhearingimpairment.Fiveparticipantshadneverused
Sincethestudyfollowedawithin-subjectsdesign,apairedsamples
AugmentedReality(AR)orVirtualReality(VR)before,twohadused
t-testwasusedtotestforsignificance. Asignificantdifferencewas
AR or VR once before, and two indicated to have used AR or VR
foundbetweenthevisualandaudiovisualDicedata(Figure7). The
acoupleoftimes. Twodoctorsindicatedlisteningtomusicbutnot
tumor markings placed using the audiovisual feedback resulted in
playinganyinstrument.Threeindicatedthattheyknewhowtoplayan
a significantly higher Dice coefficient value (p<0.05), indicating
instrumentbutrarelyplayedit,andthreephysiciansansweredthatthey
improvedlocalizationaccuracywhenusingtheaudiovisualcompared
regularlyplayedoneormultipleinstruments.
tothevisualfeedback.
6.3 StudyProcedure AShapiro-Wilktestshowedanon-normaldistributionofthetask
timedata. Eightoutlierswereremovedusingtheinterquartilerange
Inthiscomparativestudythatfollowedawithin-subjectdesign,the
method.AWilcoxonsignedranktestshowednosignificantdifference
medicaldoctorsusedtheconventionalvisualinteractionandthenovel
intasktimepertumorbetweenthevisualandaudiovisualcondition
audiovisualinteractiontolocalizetumorsinasimulatedVRsetting.
(Figure7).
TheuserswerefirstgivenanintroductiontotheMMIIframeworkand
thestudyprocedure.Aftercompletingashortsurveyassessingdemo- 14SciPy(https://scipy.org/)
8ToappearinIEEETransactionsonVisualizationandComputerGraphics.
Wefurthersawnon-significantdifferencesintaskloadandanon-
significantincreaseintasktimewhenusingmultimodaloverunimodal
interaction.Astudyinvolvingalargergroupofmedicalexpertscould
give insights into the validity of the observed tendency. A further
limitationofourstudyistheunequalrepresentationofgender.Afuture
studyshouldtrytoachieveabalanceofgenderamongthemedical
participantstoensurethatourclaimsarevalidfortheentirepopulation
ofusers.
7 DISCUSSION
Fig. 7: Box plots of dice coefficient and task time per trial for both Weextensivelytestedtheinteractionframeworkintwouserstudies.
conditionsinStudy2:VisualandAudiovisual(MMII);*=p<0.05. Eighttissueswithvaryingphysicalpropertiesandcharacteristics,from
rigid(vertebra)tosoft(braintissue)andfromstatic(vertebra)tody-
namic(cerebralarteries),weresonified.Wesawthataphysicalmodel-
Table4:Meansandstandarddeviationsoflocalizationaccuracyandtask
loadforthevisualandaudiovisualconditioninStudy2:Sørensen–Dice ingsynthesisapproachthattakestheanatomy’sgeometryandphysical
coefficient([0,1];largerbetter),overalltaskload,individualNASA-TLX propertiesintoaccountissuitabletocreateintuitiveanddiversesounds.
subscales([0,100];smallerbetter,exceptPerformance-largerbetter), However,wealsoexperiencedthatsometissuesduetotheirsimilarity
tasktimeinseconds. inshapeandphysicalproperties,e.g.,greymatterandwhitematterof
thebraincreatesoundsthatarehardtodistinguishforthegeneraluser.
Inthosecases,weproposenormalization,scaling,andtransformation
n=9 Visual Audiovisual(MMII)
ofthetissues’physicalvaluesintoanaudiblydistinguishablerange.
Sørensen–DiceCoefficient 0.48±0.25 0.60±0.28 Althoughthepresentedvisualfeedbackwasreceivedwellbythe
NASA-TLX-Overall 59.07±14.32 52.96±10.99 participants, we would suggest a more rigorous application of the
NASA-TLX-MentalDemand 82.22±9.72 68.89±15.36 causality-informednatureoftheframeworkforthevisualizationmodel,
NASA-TLX-PhysicalDemand 48.89±17.64 44.44±13.33 e.g.,byequallydynamicvisualresponsestochangesinanatomygeom-
NASA-TLX-TemporalDemand 53.34±27.84 48.89±24.72
etryandphysicalproperties.
NASA-TLX-Performance 48.89±20.89 54.44±13.33
NASA-TLX-Effort 68.89±21.47 61.11±15.37
7.1 Limitations
NASA-TLX-Frustration 52.23±29.91 40.00±18.71
TaskTime 83.73±66.24 100.40±83.84 Althoughwetriedtodiversifytherangeofanatomicalstructuresfrom
rigidtosoft,weacknowledgethatthisframeworkhassofaronlybeen
tested on structures of the spine and the brain. Evaluation of this
methodonfurtherpartsofthebodywouldberequiredtoclaimgeneral
Nosignificantdifferencewasfoundforthetaskload(NASA-TLX) applicationforallhumantissues.
results. Table4liststhemeansandstandarddeviationsoftheDice,
AnotherlimitationofourworkistheevaluationofMMIIformedical
taskload,andtasktimeresultsforthevisualandaudiovisual(MMII)
tasksinasimulatedVRenvironment.Althoughusefulforinitialtesting
condition.
ofthemethoditlacksrealism.EvaluatingMMIIonanARHMDand
providingaphysicalmodelofthepatienttothemedicalexpertswould
6.5.2 QualitativeFeedback certainlyincreasetherelevanceoftheresultstoclinicalpractice.
Theparticipantsstatedthatthesonificationhelpedthemtobetterper-
ceivethedistancetothetumor. Theyfurthersaidthatthechangein 7.2 Outlook
amplitudedependingonthedistancewashelpful.Whenaskedabout Buildinguponthefindingsofthiswork,futurestudiesshouldevaluate
waystoimprovetheaudiovisualinteraction,theysuggesteda"more theframeworkinanactualclinicalsettingandincludeawiderrange
nuancedsoundscape,""moredistinctsounds,"and"asharponsetofa ofanatomicalstructures. Futureworkcouldalsofocusonincorpo-
differentsound"oncetheinteractionpointentersthetumorvolumeto ratingmoredetaileddatafromtissueassuggestedin[34]. However,
allowformorepreciselocalization. Themajorityoftheparticipants thisrequiresoptimizedmethodologiescapableofincorporatingsuch
feltthatthesoundcharacterizedthedifferentanatomicalstructuresvery detailedinformationinfiniteelementmodels, whileprovidinglow-
well.Whenaskedwhichofthetwoconditionstheywouldliketousein latencyfeedbacksuitableforsurgicalapplicationsatafinetemporal
thefuture,allninedoctorsansweredtoprefertheaudiovisualfeedback. scale.Additionally,intraoperativediagnostictechniquessuchasmass
spectroscopy,Ramanspectrometry[48],oracousticlistening[24,38]
6.6 DiscussionStudy2 couldprovidereal-timetissueinformationasinputtotheMMIIframe-
Our study showed significantly increased (p<0.05) task accuracy workandthusfurtherenrichtheaudiovisualfeedback.
whenusingMMII.Althoughthelocalizationaccuracyimprovedwhen Furthermore,incorporatingphysiologicaldata,whichisinherently
usingtheaudiovisualfeedback,theDicecoefficientisstillrelativelylow. dynamicandmorecomplex,hasthepotentialtoaddsignificantvalue
Thisalignswiththedoctors’suggestionstoadapttheaudiofeedbackto totheMMIIframework.Physiologyprovidesamoreholisticviewof
bemoresensitivetosubtlechangesinlocation.Suchadaptationwould thebody’sdynamics. Forinstance,aneventintheheartcouldhave
beneededtoincreasetheprecisionofMMIIandmakeitsuitablefor effectsatadistalposition,influencingtissueselsewhereinthebody.
applicationinrealsurgicalprecisiontasks. Inthiswork, abloodvesselsoundwasproposedthatdemonstrates
aninitalphysiologically-basedapproachforbloodflowsonification.
WhileworkssuchastheonebyChenetal.[12]showcasetheannota-
Offeringdynamicfeedbackondatasuchascerebralbloodflowmea-
tionprecisionthatcanbeachievedusing3Dvisualizationsforsurgical
suredthroughMRIormetabolicchangesobservedthroughPETscans
planning,ourworkelicitsthebenefitofreducedmentaldemandand
wouldpresentadditionalapplicationsthatshowcasetheadvantagesof
frustrationwhenusingaudiovisualinteractionduringreal-timesurgical
theproposedmultimodalfeedbackmethod.
interactions.Combiningtheadvantagesofboth3Dvisualizationsand
audiovisualinteractioncouldguaranteespatialprecisionwhileenabling
perceptionofdynamicchangesduringsurgeon-anatomyinteractions,
8 CONCLUSION
leadingtoincreasedtaskperformanceandconfidence. TheMultimodalMedicalImageInteraction(MMII)frameworkshow-
Fromabalanceddistributionofmusicalexperienceamongthemedi- cased in this work presents a promising method for engaging with
calparticipants,wecanassumethattheframeworkisaccessibleregard- humananatomythroughaudiovisualmeans. Theproposedphysical
lessofmusicalexpertise. modelingapproachtoanatomysonificationhasproventobeeasyto
9learnandtocharacterizeanatomicalstructureswell. Ourworkpre- [16] G.DubusandR.Bresin. Asystematicreviewofmappingstrategiesfor
sentedanexemplaryusecaseofphysicallyinformedmultimodalin- thesonificationofphysicalquantities.PloSone,8(12):e82491,2013.3
teractioninasurgicaltask.Evaluationinvolvingninemedicaldoctors [17] G.Eckel. Soundsynthesisbyphysicalmodellingwithmodalys. Proc.
indicatedincreasedaccuracyinbraintumorlocalizationwhenusing ISMA’95,pp.478–482,1995.3,5
MMII. [18] K.FraninovicandS.Serafin.Sonicinteractiondesign.MitPress,2013.2
[19] C.Hansen,D.Black,C.Lange,F.Rieber,W.Lamadé,M.Donati,K.J.
Ourfindingsunderscoretheadvantagesofaudiovisualinteractions
Oldhafer,andH.K.Hahn. Auditorysupportforresectionguidancein
comparedtounimodal,visualfeedback,highlightingthepotentialof
navigatedliversurgery. TheInternationalJournalofMedicalRobotics
multimodal approaches as viable alternatives to traditional medical
andComputerAssistedSurgery,9(1):36–43,2013.3
imageinteractionmethods.Wehopethisworkwillinspiremoreinves-
[20] S.G.Hart.Nasa-taskloadindex(nasa-tlx);20yearslater.InProceedings
tigationandbroaderadoptionofmultimodalinteractionsinmedical
ofthehumanfactorsandergonomicssocietyannualmeeting,vol.50,pp.
applicationsandbeyond.
904–908.SagepublicationsSageCA:LosAngeles,CA,2006.6
[21] S.G.HartandL.E.Staveland.Developmentofnasa-tlx(taskloadindex):
Resultsofempiricalandtheoreticalresearch.InAdvancesinPsychology,
REFERENCES
vol.52,pp.139–183.Elsevier,Amsterdam,Netherlands,1988.6
[1] A.Ahmad,S.G.Adie,M.Wang,andS.A.Boppart.Sonificationofoptical [22] T.Hermann,A.Hunt,J.G.Neuhoff,etal. Thesonificationhandbook,
coherencetomographydataandimages. OpticsExpress,18(10):9934– vol.1.LogosVerlagBerlin,2011.2
9944,2010.3 [23] T.HermannandH.Ritter.Listentoyourdata:Model-basedsonification
[2] S.Andress,A.Johnson,M.Unberath,A.F.Winkler,K.Yu,J.Fotouhi, for data analysis. Advances in intelligent computing and multimedia
S.Weidert, G.Osgood, andN.Navab. On-the-flyaugmentedreality systems,8:189–194,1999.2
fororthopedicsurgeryusingamultimodalfiducial. JournalofMedical [24] A.Illanes,A.Boese,I.Maldonado,A.Pashazadeh,A.Schaufler,N.Navab,
Imaging,5(2):021209–021209,2018.2 andM.Friebe.Novelclinicaldevicetrackingandtissueeventcharacteri-
[3] R.D.Bartlett,D.Choi,andJ.B.Phillips.Biomechanicalpropertiesofthe zationusingproximallyplacedaudiosignalacquisitionandprocessing.
spinalcord:implicationsfortissueengineeringandclinicaltranslation. Scientificreports,8(1):12070,2018.9
Regenerativemedicine,11(7):659–673,2016.4 [25] S.Jang,J.M.Vitale,R.W.Jyung,andJ.B.Black.Directmanipulationis
[4] R.D.Bartlett,D.Eleftheriadou,R.Evans,D.Choi,andJ.B.Phillips.Me- betterthanpassiveviewingforlearninganatomyinathree-dimensional
chanicalpropertiesofthespinalcordandbrain:Comparisonwithclinical- virtualrealityenvironment.Computers&Education,106:150–165,2017.
gradebiomaterialsfortissueengineeringandregenerativemedicine.Bio- 2
materials,258:120303,2020.4 [26] F.Joeres,D.Black,S.Razavizadeh,andC.Hansen. AudiovisualAR
[5] A.BenAwadh,J.Clark,G.Clowry,andI.D.Keenan.Multimodalthree- conceptsforlaparoscopicsubsurfacestructurenavigation. InGraphics
dimensionalvisualizationenhancesnovicelearnerinterpretationofbasic Interface2021,2021.2
cross-sectionalanatomy.Anatomicalscienceseducation,15(1):127–142, [27] W.Kang,L.Wang,andY.Fan. Viscoelasticresponseofgraymatter
2022.2 andwhitematterbraintissuesundercreepandrelaxation. Journalof
[6] T.Blum, V.Kleeberger, C.Bichlmeier, andN.Navab. mirracle: An Biomechanics,162:111888,2024.4
augmentedrealitymagicmirrorsystemforanatomyeducation.In2012 [28] P.R.Kantan,S.Dahl,andE.G.Spaich. Sound-guided2-dnavigation:
IEEEVirtualRealityWorkshops(VRW),pp.115–116.IEEE,2012.2 Effectsofinformationconcurrencyandcoordinatesystem. InNordic
[7] K. Bogomolova, I. J. van der Ham, M. E. Dankbaar, W. W. van den Human-ComputerInteractionConference,pp.1–11,2022.3
Broek,S.E.Hovius,J.A.vanderHage,andB.P.Hierck.Theeffectof [29] I.D.KeenanandM.Powell.InterdimensionalTravel:Visualisationof3D-
stereoscopicaugmentedrealityvisualizationonlearninganatomyandthe 2DTransitionsinAnatomyLearning,pp.103–116.SpringerInternational
modifyingeffectofvisual-spatialabilities:Adouble-centerrandomized Publishing,Cham,2020.2
controlledtrial.Anatomicalscienceseducation,13(5):558–567,2020.2 [30] M.E.Madden.Introductiontosectionalanatomy.LippincottWilliams&
[8] F.Bork, R.Barmaki, U.Eck, P.Fallavolita, B.Fuerst, andN.Navab. Wilkins,2008.2
Exploringnon-reversingmagicmirrorsforscreen-basedaugmentedreality [31] A.Marquardt,C.Trepkowski,T.D.Eibich,J.Maiero,E.Kruijff,and
systems.In2017IEEEvirtualreality(VR),pp.373–374.IEEE,2017.2 J.Schöning. Comparingnon-visualandvisualguidancemethodsfor
[9] F.Bork,B.Fuers,A.-K.Schneider,F.Pinto,C.Graumann,andN.Navab. narrowfieldofviewaugmentedrealitydisplays. IEEETransactionson
Auditoryandvisio-temporaldistancecodingfor3-dimensionalperception VisualizationandComputerGraphics,26(12):3389–3401,2020.2
inmedicalaugmentedreality. In2015IEEEInternationalSymposium [32] S. Matinfar, T. Hermann, M. Seibold, P. Fürnstahl, M. Farshad, and
onMixedandAugmentedReality,pp.7–12.IEEE,NewYork,NY,USA, N.Navab. Sonificationforprocessmonitoringinhighlysensitivesur-
2015.doi:10.1109/ISMAR.2015.162 gicaltasks. InProceedingsoftheNordicSoundandMusicComputing
[10] F.Bork,L.Stratmann,S.Enssle,U.Eck,N.Navab,J.Waschke,and Conference2019(NordicSMC2019),2019.3
D.Kugelmann.Thebenefitsofanaugmentedrealitymagicmirrorsystem [33] S.Matinfar,M.A.Nasseri,U.Eck,H.Roodaki,N.Navab,C.P.Lohmann,
forintegratedradiologyteachingingrossanatomy.Anatomicalsciences M.Maier,andN.Navab.Surgicalsoundtracks:Towardsautomaticmusical
education,12(6):585–598,2019.2 augmentationofsurgicalprocedures.InMedicalImageComputingand
[11] T.Bovermann,T.Hermann,andH.Ritter. Tangibledatascanningsoni- Computer-AssistedIntervention-MICCAI2017:20thInternationalCon-
ficationmodel. InProceedingsofthe12thInternationalConferenceon ference,QuebecCity,QC,Canada,September11-13,2017,Proceedings,
AuditoryDisplay,2006.2 PartII20,pp.673–681.Springer,2017.3
[12] C.Chen,M.Yarmand,V.Singh,M.V.Sherer,J.D.Murphy,Y.Zhang,and [34] S.Matinfar,M.Salehi,S.Dehghani,andN.Navab.Fromtissuetosound:
N.Weibel.Vrcontour:Bringingcontourdelineationsofmedicalstructures Model-basedsonificationofmedicalimaging.InInternationalConference
intovirtualreality.In2022IEEEInternationalSymposiumonMixedand onMedicalImageComputingandComputer-AssistedIntervention,pp.
AugmentedReality(ISMAR),pp.64–73,2022.doi:10.1109/ISMAR55827 207–216.Springer,2023.3,4,9
.2022.000202,9 [35] S.Matinfar,M.Salehi,D.Suter,M.Seibold,S.Dehghani,N.Navab,
[13] P.R.Cook. Physicallyinformedsonicmodeling(phism): Percussive F.Wanivenhaus,P.Fürnstahl,M.Farshad,andN.Navab.Sonificationasa
synthesis. InProceedingsofthe1996InternationalComputerMusic reliablealternativetoconventionalvisualsurgicalnavigation.Scientific
Conference,pp.228–231.TheInternationalComputerMusicAssociation, Reports,13(1):5930,2023.3
1996.3 [36] N.Navab,A.Martin-Gomez,M.Seibold,M.Sommersperger,T.Song,
[14] C.Dennler,L.Jaberg,J.Spirig,C.Agten,T.Götschi,P.Fürnstahl,and A. Winkler, K. Yu, and U. Eck. Medical augmented reality: Defini-
M.Farshad.Augmentedreality-basednavigationincreasesprecisionof tion,principlecomponents,domainmodeling,anddesign-development-
pediclescrewinsertion. Journaloforthopaedicsurgeryandresearch, validationprocess.JournalofImaging,9(1),2023.2
15:1–8,2020.2 [37] M. K. Ngo and C. Spence. Auditory, tactile, and multisensory cues
[15] B.J.Dixon,M.J.Daly,H.H.Chan,A.Vescan,I.J.Witterick,andJ.C. facilitatesearchfordynamicvisualstimuli. Attention, Perception, &
Irish. Inattentionalblindnessincreasedwithaugmentedrealitysurgical Psychophysics,72(6):1654–1665,2010.2
navigation. Americanjournalofrhinology&allergy,28(5):433–437, [38] D.Ostler,M.Seibold,J.Fuchtmann,N.Samm,H.Feussner,D.Wilhelm,
2014.2 andN.Navab.Acousticsignalanalysisofinstrument–tissueinteraction
10ToappearinIEEETransactionsonVisualizationandComputerGraphics.
forminimallyinvasiveinterventions.InternationalJournalofComputer
AssistedRadiologyandSurgery,15:771–779,2020.9
[39] G. Parseihian, C. Gondre, M. Aramaki, S. Ystad, and R. Kronland-
Martinet.Comparisonandevaluationofsonificationstrategiesforguid-
ancetasks. IEEETransactionsonMultimedia, 18(4):674–686, 2016.
3
[40] R.Porter. TheCambridgeillustratedhistoryofmedicine. Cambridge
UniversityPress,2001.5
[41] H.Roodaki,N.Navab,A.Eslami,C.Stapleton,andN.Navab. Sonif-
eye: Sonificationofvisualinformationusingphysicalmodelingsound
synthesis. IEEEtransactionsonVisualizationandComputerGraphics,
23(11):2366–2371,2017.3
[42] L. Schütz, T. El Chemaly, E. Weber, A. T. Doan, J. Tsai, C. Leuze,
B.Daniel, andN.Navab. Interactiveshapesonificationfortumorlo-
calizationinbreastcancersurgery.InProceedingsoftheCHIConference
onHumanFactorsinComputingSystems,CHI’24.AssociationforCom-
putingMachinery,NewYork,NY,USA,2024.doi: 10.1145/3613904.
36422573
[43] L.Schütz,E.Weber,W.Niu,B.Daniel,J.McNab,N.Navab,andC.Leuze.
Audiovisualaugmentationforcoilpositioningintranscranialmagnetic
stimulation. ComputerMethodsinBiomechanicsandBiomedicalEn-
gineering: Imaging&Visualization,11(4):1158–1165,2023.doi: 10.
1080/21681163.2022.21542772
[44] L.ShamsandA.R.Seitz. Benefitsofmultisensorylearning. Trendsin
cognitivesciences,12(11):411–417,2008.2
[45] A.A.TahaandA.Hanbury. Metricsforevaluating3dmedicalimage
segmentation:analysis,selection,andtool.BMCmedicalimaging,15:1–
28,2015.8
[46] A.Väljamäe,T.Steffert,S.Holland,X.Marimon,R.Benitez,S.Mealla,
A.Oliveira,andS.Jordà.Areviewofreal-timeeegsonificationresearch.
InInternationalConferenceonAuditoryDisplay2013(ICAD2013),2013.
3
[47] E. Van der Burg, C. N. Olivers, A. W. Bronkhorst, and J. Theeuwes.
Pipandpop: nonspatialauditorysignalsimprovespatialvisualsearch.
JournalofExperimentalPsychology:HumanPerceptionandPerformance,
34(5):1053,2008.2
[48] L.VanHese,S.DeVleeschouwer,T.Theys,S.Rex,R.M.Heeren,and
E.Cuypers.Thediagnosticaccuracyofintraoperativedifferentiationand
delineationtechniquesinbraintumours.DiscoverOncology,13(1):123,
2022.9
[49] C.M.WegnerandD.B.Karron.Surgicalnavigationusingaudiofeedback.
InMedicineMeetsVirtualReality,pp.450–458.IOSPress,1997.3
[50] M.Wright. Opensoundcontrol: anenablingtechnologyformusical
networking.OrganisedSound,10(3):193–200,2005.6
[51] J.Yang,A.Barde,andM.Billinghurst.Audioaugmentedreality:asys-
tematicreviewoftechnologies,applications,andfutureresearchdirections.
journaloftheaudioengineeringsociety,70(10):788–809,2022.2
[52] T.Ziemer. Three-dimensionalsonificationasasurgicalguidancetool.
JournalonMultimodalUserInterfaces,17:253–262,2023.3
[53] T.Ziemer,D.Black,andH.Schultheis.Psychoacousticsonificationdesign
fornavigationinsurgicalinterventions. InProceedingsofMeetingson
Acoustics,vol.30.AIPPublishing,2017.3
[54] T.ZiemerandH.Schultheis.Threeorthogonaldimensionsforpsychoa-
cousticsonification.arXivpreprintarXiv:1912.00766,2019.3
[55] T.Ziemer,H.Schultheis,D.Black,andR.Kikinis. Psychoacoustical
interactivesonificationforshortrangenavigation.ActaAcusticaunited
withAcustica,104(6):1075–1093,2018.3
[56] E.ZwickerandH.Fastl. Psychoacoustics: Factsandmodels,vol.22.
SpringerScience&BusinessMedia,2013.3
11