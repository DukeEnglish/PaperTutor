Bayesian Federated Learning with
Hamiltonian Monte Carlo: Algorithm and
Theory
Jiajun Liang∗ Qian Zhang † Wei Deng ‡ Qifan Song §
Guang Lin ¶
July 10, 2024
Abstract
This work introduces a novel and efficient Bayesian federated learning algorithm,
namely, the Federated Averaging stochastic Hamiltonian Monte Carlo (FA-HMC),
for parameter estimation and uncertainty quantification. We establish rigorous con-
vergence guarantees of FA-HMC on non-iid distributed data sets, under the strong
convexity and Hessian smoothness assumptions. Our analysis investigates the effects
of parameter space dimension, noise on gradients and momentum, and the frequency
of communication (between the central node and local nodes) on the convergence
and communication costs of FA-HMC. Beyond that, we establish the tightness of our
analysisbyshowingthattheconvergenceratecannotbeimprovedevenforcontinuous
FA-HMC process. Moreover, extensive empirical studies demonstrate that FA-HMC
outperforms the existing Federated Averaging-Langevin Monte Carlo (FA-LD) algo-
rithm.
Keywords: Hamiltonian Monte Carlo, Federated Learning, Bayesian sampling, Federated
averaging, Stochastic Gradient Langevin Dynamics
∗ByteDance Inc, China
†Department of Statistics, Purdue University, West Lafayette, IN
‡Machine Learning Research, Morgan Stanley, New York, NY
§Department of Statistics, Purdue University, West Lafayette, IN
¶Department of Mathematics & School of Mechanical Engineering, Purdue University, West Lafayette,
IN. G.L. gratefully acknowledges the support of the National Science Foundation (DMS-2053746, DMS-
2134209, ECCS-2328241, and OAC-2311848), and U.S. Department of Energy (DOE) Office of Science
Advanced Scientific Computing Research program DE-SC0023161, and DOE – Fusion Energy Science,
under grant number: DE-SC0024583.
1
4202
luJ
9
]GL.sc[
1v53960.7042:viXra1 Introduction
Standard learning algorithms usually require centralizing the training data, in the sense
that the learning machine can directly access all pieces of the data. Federated learning
(FL), on the other hand, enables multiple parties to collaboratively train a consensus
model without directly sharing confidential data (Koneˇcny` et al., 2015, 2016; Bonawitz
et al., 2019; Li et al., 2020a). The framework of FL is quite appealing to applications
where data confidentiality is of vital importance, such as aggregating user app data from
mobile phones to learn a shared predictive model (e.g., Tran et al., 2019; Chen et al., 2020a)
or analyzing medical data from multiple healthcare stakeholders (e.g., hospitals, research
centers, life science companies) (e.g., Li et al., 2020c; Rieke et al., 2020).
FL shares a similar algorithmic architecture to parallel optimization. First, parallel
algorithms are commonly based on the divide-and-combine strategy, i.e., the learning sys-
tem assigns (usually i.i.d.) training samples to each worker node, say via simple random
sampling. As such, the training data sets are similar in nature across worker nodes. But
under the FL framework, the data sets of each worker node are generated or collected
locally and are not homogeneous, which poses challenges for convergence analysis. Sec-
ondly, parallel computing is commonly practiced in the same physical location, such as
a data center, where high throughput computer networking communications are available
between worker nodes. In contrast, FL has either a vast number of worker nodes (e.g.,
mobile devices) or geographically separated worker nodes (e.g., hospitals), which limits the
connectivity between the central nodes and worker nodes. Due to the unavailability of fast
or frequent communication, FL needs to be communication-efficient.
Federated Averaging (FedAvg, McMahan et al., 2017) is one of the most widely used
FL optimization algorithms. It trains a global model by synchronously averaging multi-
step local stochastic gradient descent (SGD) updated parameters of all the worker nodes.
Various attempts have been made to enhance the robustness and efficiency of FedAvg
(e.g., Li et al., 2020b; Wang et al., 2020). However, optimization-based approaches often
fail to provide proper uncertainty quantification for their estimations. Reliable uncertainty
quantification, suchasintervalestimationsorhypothesistesting, providesavitaldiagnostic
2for both developers and users of an AI system.
The Bayesian counterpart naturally integrates an inference component, thus it provides
a unified solution for both estimations and uncertainty quantification. This paper stud-
ies a Bayesian computing algorithm aiming to obtain samplers from the global posterior
distribution by infrequently aggregating samples drawn from local posterior distributions.
Unlike existing results that utilize stochastic gradient Langevin dynamics (Welling and
Teh, 2011), this work considers (stochastic gradient) Hamiltonian Monte Carlo (HMC,
Neal, 2012). While the second-order nature of HMC poses more theoretical difficulties, it
has been demonstrated to be more computationally efficient through numerous empirical
studies (see, e.g., Girolami and Calderhead, 2011; Chen et al., 2014). Readers can refer
to Section A in Supplementary Material for a review of related literature on federated
sampling and HMC.
The contributions of the presented work are three-fold:
(1) We propose the Federated Averaging Hamiltonian Monte Carlo (FA-HMC) algo-
rithm which is effective for global posterior inferences in federated learning. It utilizes
stochastic gradient HMC on individual local nodes and combines the local samples ob-
tained infrequently to yield global samples.
(2) Under strong log-concavity and proper smoothness assumptions, we have proven
a non-asymptotic convergence result under the Wasserstein metric for various training
settings. Furthermore, wedemonstratethatthisupperboundoftheconvergencerateofthe
FA-HMC sampling algorithm is tight (i.e., best achievable for certain sampling problems).
(3) We conduct simulation and real data experiments to validate our theoretical find-
ings. Additionally, the numerical studies show that FA-HMC is easy to tune, improves
communication efficiency, and can outperform FA-LD in different settings.
Roadmap: The paper is organized as follows: In Section 2, we summarize the problem
setup and provide the necessary background on HMC. In Section 3, we present the FA-
HMC algorithm and the assumptions used for its analysis. In Section 4, we provide the
key theoretical findings and examine the effects of SGD noise and the correlation between
momentum. Furthermore, we prove that our analysis is tight and cannot be improved
3for certain sampling problems, even for continuous FA-HMC. In Section 5, we compare
the FA-HMC algorithm with the FA-LD algorithm through extensive simulations and real-
data experiments. Finally, in Section 6, we conclude our work and suggest potential future
directions.
2 Preiminary
2.1 Problem Setup
Letzc,1 i n betheavailabledataofthec-thnodeandℓ(θ;zc)beauser-specifiednega-
i ≤ ≤ c i
tive log-likelihood function. Define n = n , w = n /n, and f(c)(θ) := n nc ℓ(θ;zc)/n
c c c i=1 i c
is the local loss function of parameter θ PRd accessible to the c-th local nodPe (e.g., the nor-
∈
malized negative log-likelihood function based on the data set available at c-th local node)
for 1 c N. The goal is to simulate the global target distribution π(θ) exp( f(θ)),
≤ ≤ ∝ −
where f(θ) = N w f(c)(θ), w 0 and w = 1.
c=1 c c ≥ c c
P P
2.2 Hamilton’s Equations and HMC
Hamiltonian (Hybrid) Monte Carlo (HMC) was first proposed by Duane et al. (1987) for
simulations of quantum chromodynamics and was then extended to molecular dynamics
and neural networks Neal (2012). To alleviate the random-walk behavior in the vanilla
Langevin dynamics, HMC simulates the trajectory of a particle according to Hamiltonian
dynamics and obtains a much faster convergence rate than Langevin dynamics Mangoubi
and Vishnoi (2018). In specific, HMC introduces a set of auxiliary momentum variables
p Rd to capture second-order information, whereas Langevin Monte Carlo is only a first-
∈
order method. In this way, HMC generates samples from the following joint distribution
1
π(θ,p) exp( f(θ) pΣ 1p),
′ −
∝ − − 2
where f(θ) + pΣ 1p/2 is the Hamiltonian function and quantifies the total energy of a
′ −
physical system. To further generate more efficient proposals, HMC simulates according to
4Algorithm 1 Stochastic gradient leapfrog approximation h
LF
Input: Energy function f( ); Initial parameters θ , momentum p ; learning rate η;
0 0
·
e
leapfrog step K; k = 0
while k K: do
≤
θ = θ +ηp
η2
f(θ ,ξ )
k+1 k k − 2 ∇ k k
p = p η f(θ ,ξ ) η f(θ ,ξ )
k+1 k − 2∇ k ke− 2∇ k+1 k+1 2
k = k +1;
e e
Output: h (f,θ ,p ,η,K) = θ
LF 0 0 K
e
the following Hamilton’s equations
dθ(t) dp(t)
= Σ 1/2p(t), = Σ1/2 f(θ(t)), (1)
− θ
dt dt − ∇
which satisfy the conservation law and are time reversible. Such properties leave the dis-
tribution invariant and the nature of Hamiltonian conservation always makes the proposal
accepted ideally. Note that commonly, one chooses Σ = Id such that the momentum follows
the standard multivariate normal distribution.
To numerically implement the continuous HMC process, a popular numerical integrator
is the “leapfrog” approximation, see Algorithm 1. Here, to enhance the computational effi-
ciency, f(θ ,ξ ) and f(θ ,ξ ) are the stochastic versions of f(θ ) and f(θ ),
k k k+1 k+1/2 k k+1
∇ ∇ ∇ ∇
respectively. The arguments ξ and ξ denote random variables that control the ran-
e e k k+1/2
domness of the stochastic gradients. For example, given f(θ) = n ℓ(θ;z ) with data
i=1 i
z n , we let f(θ,ξ ) = n ℓ(θ;z )/ S(ξ ) +Z(ξ ) wherPe S(ξ ) is a random in-
{ i }i=1 ∇ k i ∈S(ξk)∇ i | k | k k
dex subset, Z(ξ ) is an injectePd Gaussian noise, and ξ is the random seed. When the exact
k e k
gradients are used, it holds that f(θ ,ξ ) = f(θ ) and f(θ ,ξ ) = f(θ ).
k k k k+1 k+1/2 k+1
∇ ∇ ∇ ∇
Note that throughout this paper, when the exact gradient is used instead of a stochastic
e e
gradient, the algorithm is referred to as the vanilla version, e.g., vanilla FA-HMC.
For convenience in analysis, the leapfrog method without Metropolis correction (see
Algorithm 2), is commonly studied in the literature (Mangoubi and Vishnoi, 2018; Chen
and Vempala, 2022; Zou and Gu, 2021). One may also add an additional accept/reject step
according to the Metropolis ratio (Chen et al., 2020b).
5Algorithm 2 HMC algorithm (without Metropolis correction)
Input: Energy function f( ); Initial point θ ; Stepsize function η = η(t); Leapfrog step
0 t
·
K; t = 0;
while the stopping rule is not satisfied do
sample momentum p
t
N(0,Id)
∼
update θ = h (f,θ ,p ,η ,K), t = t+1;
t+1 LF t t t
Output: θ t
{ i }i=1 e
Note that in the literature, Chen et al. (2014) proposed a different HMC algorithm,
based on Euler integrator of Hamilton dynamics. Their implementation includes variance
adjustment to counteract the noise of the stochastic gradient, which can negatively impact
the stationary distribution. This adjustment eventually leads to an underdamped Langevin
Monte Carlo algorithm with stochastic gradient (see also e.g., Ma et al., 2015; Zou et al.,
2019;ChauandRasonyi,2022;AkyildizandSabanis,2020;NemethandFearnhead,2021).
3 FA-HMC Algorithm and Assumptions
Ensuring the confidentiality of the data utilized for training a model is a vital concern in
federated learning. To safeguard against potential gradient leakage (Zhu et al., 2019) and
breaches of local data privacy, it is preferable to use noisy gradients and less-correlated
momentum among local nodes (see Deng et al., 2021; Vono et al., 2022). This could make
it more difficult to recover local data information through accumulated communication.
With these considerations, we propose Federated Averaging via HMC algorithm that
utilizes general stochastic gradients and non-necessarily identical momentum across nodes.
We let all local devices run HMC (Algorithm 1), and synchronize their model parameters
every T iteration. All devices may use stochastic gradients and share part of the initial
momentum of leapfrog approximation. Note that in practice, correlated momentum be-
tween devices can be easily achieved by sending a common random seed to all devices for
momentum generation. This FA-HMC algorithm is formalized in Algorithm 3. It is worth
mentioning that when leapfrog step K = 1, the leapfrog approximation of the unadjusted
6Algorithm 3 FA-HMC algorithm
(c)
Input: θ = θ , t = 0; stepsize function η = η(t); Local update step T; leapfrog update
0 0 t
step K;
while the stopping rule is not satisfied do
(c)
sample momentum p
t
if t 0(mod T) then
≡
Broadcast θ := N w θ(c) and set θ(c) = θ
t c=1 c t t+1,0 t
else P
(c) (c)
θ = θ
t+1,0 t
update
θ(c)
= h
(f(c),θ(c) ,p(c)
,η ,K) in parallel for all devices, t = t+1
t+1 LF t+1,0 t t
e
HMC algorithm (i.e., Algorithm 1) reduces to θ
t+1
= θ
t
−
(η t2/2) ∇θf(θ t,ξ t) + η tN(0,Id),
which is exactly the unadjusted Langevin Monte Carlo with dynamic learning rate η2/2.
e t
And the FA-HMC reduces to FA-LD Deng et al. (2021).
3.1 Assumptions
To establish the convergence performance of the aggregated model with respect to θ , we
t
adopted the following assumptions.
Assumption 3.1 (µ-Strongly Convex). For each c = 1,2,...,N, f(c) is µ-strongly convex
for some µ > 0, i.e., x,y Rd, f(c)(y) f(c)(x)+ f(c)(x),y x + µ y x 2.
∀ ∈ ≥ ⟨∇ − ⟩ 2∥ − ∥2
Assumption 3.2 (L-Smoothness). For each c = 1,2,...,N, f(c) is L-smooth for some
L > 0, i.e., x,y Rd, f(c)(y) f(c)(x) L x y .
∀ ∈ ∥∇ −∇ ∥ ≤ ∥ − ∥
Assumption 3.3 (L -Hessian Smoothness). For each c = 1,2,...,N, f(c) is L Hessian
H H
smoothness, i.e., for any θ 1,θ 2,p
∈
Rd,
∥
∇2f(c)(θ 1) −∇2f(c)(θ 2) p ∥2
≤
L2 H∥θ
1
−θ
2
∥2 ∥p ∥2 .
∞
(cid:0) (cid:1)
Assumptions 3.1-3.2 are commonly used for the convergence analysis of gradient-based
MCMCalgorithms(e.g.,Dalalyan,2017;MangoubiandVishnoi,2018;DalalyanandKarag-
ulyan, 2019; Erdogdu and Hosseinzadeh, 2021, and references therein). The strong convex-
ity condition, in some theoretical literature of stochastic Langevin Monte Carlo, has also
7been relaxed to the dissipativity condition (e.g., Raginsky et al., 2017; Zou et al., 2021)
for non-log-concave target distributions. But such an extension is beyond the scope of
this paper and will be investigated in future works. Assumption 3.3 ensures second-order
smoothness of energy functions beyond gradient Lipchitzness. Similar Hessian smoothness
conditions are used in the literature. For example, Dalalyan and Karagulyan (2019); Chen
et al. (2020b); Zou et al. (2021) required the Hessian matrix of energy function to be Lip-
chitz under ℓ operator norm. In comparison, Assumption 3.3 is a stronger requirement
2
since ℓ norm appears on the RHS. Our assumption is somewhat comparable to Assump-
∞
tion 1 of Mangoubi and Vishnoi (2018) which defines a semi-norm with respect to a set of
pre-specified unit vectors.
(c)
We require an additional assumption to model stochastic gradients. Denote θ as
t,k
(c)
the position parameter of the c-th local node at iteration t and leapfrog step k, and ξ
t,x
(x = k 1/2,k) as the corresponding variable that controls the randomness of gradient.
−
Assumption 3.4 (σ -Bounded Variance). For local device c = 1,2,...,N, and leapfrog
g
step k = 1,2,...,K, t = 1,2,..., we have max tr(Var(
f(c)(θ(c) ,ξ(c)
)
θ(c)
))
x=k −1/2,k ∇ t,k t,x | t,k ≤
σ2Ld, for some σ > 0.
g g e
This is a common assumption in the literature (see Gu¨rbu¨zbalaban et al., 2021; Vono
et al., 2022; Deng et al., 2021). It is worth noting that in practice, the stochastic gradient
is computed based on a random subsample of the whole dataset, thus the variability of the
stochastic gradient can be naturally controlled by adjusting the batch sizes.
Under our framework, we can also relax the above assumption to
max tr(Var(
f(c)(θ(c) ,ξ(c)
)
θ(c)
))
σ2(G(c)
+d),
x=k 1/2,k ∇ t,k t,x | t,k ≤ g t,k
−
without significant changes to our proeof, where G(c) denote f(c)(θ(c) ) 2. The extension
t,k ∥∇ t,k ∥
of the proof to accommodate this assumption is discussed in Section K in the appendix.
Before presenting our main result, we emphasize that this paper examines the conver-
gence of the FA-HMC sampling algorithm, specifically in regard to dimension d and error ϵ.
It also explores ways to adjust the algorithm to maintain its effectiveness when considering
variations in gradient and momentum noise. Adapting the FA-HMC algorithm to more
general settings like non-convexity will be our future study.
84 Theoretical Results
InSection4.1,wedescribethegeneralconvergencerateofFA-HMCondifferentsettingsand
point out the setting where FA-HMC achieves the fastest speed and least communication
cost. In Section C of the supplementary material, we argue that that the upper bound on
the nearly ideal case is tight by giving a matching lower bound result. In Section 4.2, we
present a detailed result of the convergence behavior of the FA-HMC algorithm.
4.1 Main Results
Define θ := argmin f(θ) and denote the marginal distribution of θ by π . Given two
∗ θ t t
probability measures µ and ν, the 2-Wasserstein distance is 2(µ,ν) = inf
X µ,Y
ν(E X
W ∼ ∼ ∥ −
Y 2)1/2. The following theorem describes the general convergence rate of FA-HMC.
∥
Theorem 4.1. Assume 3.1-3.4, and (π ,π)2 = O1(d) and N w f(c)(θ ) 2 = O(d).
W2 0 c=1 c ∥∇ ∗ ∥
For a given local iteration step T, there exists some constant CPdepending on L,L/µ,L2 /L3
H
such that if we choose η(t) η and (denote γ = (Kη)2)
≡
γ 1 ϵ ϵ2 ϵ2
η2 = = Cmin , , ,
K2 K2L K2√dT K2dT2(1 ρ)N Kd N w2σ2
n − c=1 c g o
then (π ,π) ϵ for any ϵ > 0, with iteration number P
W2 tϵ
≤
dlog(d/ϵ2) N w2σ2
t = O1 T2 γ +(1 ρ)N + c=1 c g
ϵ ϵ2 − K
P
(cid:16) (cid:17)
(cid:0) (cid:1)
e
and corresponding communication times
t dlog(d/ϵ2) N w2σ2
ϵ = O T γ +(1 ρ)N + c=1 c g .
T ϵ2 − KT
P
(cid:16) (cid:17)
(cid:0) (cid:1)
e
When one uses smallbatch stochastic gradients (i.e., large σ ) or less correlatedmomen-
g
tum (i.e, small ρ) to improve computational feasibility and protect privacy, the proposed γ
is negligible. Under this scenario, the required number of iterations is of rate O(d/ϵ2) with
respect to the dimension d and precision level ϵ.
e
1 As d , we say f = O(g) if f Cg for some constant C, and say f = O(g) for C being a
→ ∞ ≤
polynomial of log(d).
e
9Remark 4.2. Regardingthestoppingruleofalgorithms2and3, Theorem4.1doesprovidea
nonasymptoticchoiceoft toachieveanϵ-W errorintheory. Butthisboundisimpractical,
ϵ 2
as it relies on the unknown distributional properties of the target distribution. For more
practical rules, various suggestions have been made in the literature (e.g., Gelman et al.,
1995). For example, (i) From a visual inspection perspective, we can randomly pick some
dimensions and visually compare the trace plots between two parts of a single chain (by
splitting one chain in half) or between two chains. We keep running the chains until they
become “approximately” stationary; (ii) From a quantitative perspective, we can compute
the between- and within-sequence variances following the potential scale reduction factor
R defined in Eq.(11.4) of Gelman et al. (1995), the stopping rule can be triggered when
R 1. Note that it is beyond the scope of this paper to design a stopping rule with
b ≈
statistical guarantees.
b
The result of Theorem 4.1 also shows that for a fixed ϵ, under proper tuning, the com-
munication cost t /T may initially decrease and then increase as the number of local HMC
ϵ
iterationstepsT increases(i.e., a‘U’curvew.r.t, T). Therefore, thereisatrade-offbetween
communication and divergence, and an optimal choice for local iteration can be made. Sim-
ilar discoveries were also argued by Deng et al. (2021) for Bayesian Federated Averaging
Langevin system. The above results provide a certain level of direction for optimizing the
performance of FA-HMC algorithms, considering any well-defined federated learning loss
that accounts for total running time, overall communication cost, and divergence.
Forinstance, byreducingthenoiseofthestochasticgradientsandimprovingcorrelation
between momentum to a certain level, we can achieve significant improvement on the
convergence speed from O(d/ϵ2) to O(√d/ϵ), which is argued by the following proposition.
e e
Proposition 4.3. With the assumptions as stated in Theorem 4.1, if we choose η(t) η
≡
and (denote γ = (Kη)2)
1 ϵ γ
γ = Cmin , , ρ = 1 O( ), σ2 = O(Kγ) (2)
L T√d − N g
n o
then it achieves that (π ,π) ϵ, where π denotes the marginal distribution of θ , with
W2 tϵ
≤
t t
10iteration t and corresponding communication times t /T as
ϵ
√dlog(d/ϵ2) t √dlog(d/ϵ2)
ϵ
t = O(T), = O .
ϵ
ϵ T ϵ
(cid:0) (cid:1)
e e
Under the setting (2), referred to as vanilla FA-HMC, the obtained convergence rate
matches that of the underdamped Langevin Monte Carlo algorithm on a single device
in Cheng et al. (2018) and is superior to that of Federated Averaging of underdamped
LangevinMonteCarloalgorithmunderdecentralizedsetting(i.e.,rateO(d/ϵ2)inGu¨rbu¨zbalaban
et al., 2021). It also matches existing results about Federated Langevin algorithm tackling
e
heterogeneity under the federated learning framework Plassier et al. (2023) and is better
than those without hessian smoothness assumption Deng et al. (2021).
Furthermore, inSectionCofthesupplementarymaterial, weestablishalowerboundfor
t = Ω(√dT log(d/ϵ)/ϵ) for some log-concave target distribution. In other words, our result
ϵ
in Proposition 4.3 is tight w.r.t. dimension d and local iteration T. This tight result implies
that (1) Unlike the “U” curve with respect to T discovered in Theorem 4.1, when there are
smallstochasticgradientsandlargecorrelationsbetweenmomentum, communicationtimes
havelimitedvariationsinT. Therefore,thetradeoffbetweencommunicationanddivergence
will not exist for vanilla FA-HMC and it suggests a small local iteration T to minimize
unnecessary computation; and (2)In terms of rate dependency w.r.t. the dimension, under
similar conditions on the Hessian matrix, the rate of single-device HMC is as low as O(d1/4)
Mangoubi and Vishnoi (2018), which is strictly better than our rate O(d1/2) under the
federated learning setting. This intrinsic gap is caused by (i) FA algorithm design and (ii)
the use of stochastic gradient.
4.2 Convergence Behaviour for FA-HMC Algorithm
For correlated momentum, for simplicity of analysis, we consider the following setting
(c) (c)
p = √ρξ + 1 ρξ /√w , for all c [N],t 1,
t t − t c ∈ ≥
p
(c)
where ξ ,ξ are independent standard Gaussian and the ξ are the shared across all local
t t t
(c)
nodes and ξ ’s are private to each local node c.
t
11(c)
Here the factor 1/√w on ξ is a scaling treatment such that the average momentum
c t
is a standard Gaussian. To see this, note that the average momentum p = N w p(c) has
t c=1 c t
(c)
a smaller variance due to the correlation between p . By direct calculaPtions, we have
{ t }c
1 ρ
E ∥p( tc) ∥2 = (ρ+ w− )d, E ∥p t ∥2 = d.
c
Note that for FA-HMC, the momentum of each local device is not standard Gaussian. This
is to ensure that the center momentum (i.e., p = N w p(c) aggregated from local mo-
t c=1 c t
mentum) is close to the standard Gaussian. This is aPspecial setting induced by distributed
sampling and the goal of privacy preservation.
We define the aggregated global model θ := N θ(c) for all t 1. Note that θ ,
t c=1 t ≥ t
in practice, is not accessible unless t 0(mod T).PFor t 0, we also define θπ as the
≡ ≥ t+1
parameter resulting from the evolution over Kη time following dynamic (1) with initial
t
position θπ and momentum p . With the above preparations, to intuitively understand the
t t
convergence of the distribution of θ , we take the vanilla FA-HMC as an example. We can
t
decompose θ(c) θπ as follow:
t+1 − t+1
K 1
−
θ(c) θπ = (I ) η2 (K k)(I ) (I ),
t+1 − t+1 1 − − 2 k − 3
k=1
X
where
(Kη)2 (K3 K)η3
(I ) =
θ(c) f(c)(θ(c)
) −
2f(c)(θ(c)
)
1 t,0 − 2 ∇ t,0 − 6 ∇ t,0
(Kη)2 (Kη)3
p θπ f(θπ) 2f(θπ)p ;
· t − t − 2 ∇ t − 6 ∇ t t
(cid:16) (cid:17)
(I ) =
f(c)(θ(c)
)
f(c)(θ(c)
)
2f(c)(θ(c)
)ηp k
2 k ∇ t,k −∇ t,0 −∇ t,0 t
Kη s
(I ) = f(θπ(u)) f(θπ) 2f(θπ)p ududs.
3 ∇ t −∇ t −∇ t t
Z0 Z0
Here (I ) represents second-order random approximation of θ(c) θπ through θ(c) and θπ,
1 t+1− t+1 t t
and we expect that
N
E
∥
w c(I 1) ∥2
≤
α tE ∥θ t −θ tπ ∥2 +ε2 t,
c=1
X
where the contraction factor α (0,1) and one-iteration divergence error ε > 0.
t t
∈
12On the other hand, (I ) and (I ) represent second-order approximation error and
2 k 3
∥ ∥ ∥ ∥
are expected to be O((Kη )2√d).
t
By utilizing Lemma D.1, the overall behavior is summarized in the following theorem.
Theorem 4.4 (Convergence). Under Assumptions 3.1-3.4, if we set η η 1/(K√L)
t t
′′ ≤ ′ ≤
for any t t in Algorithm 3, then θ satisfies
′ ′′ t t
≤ { }
µ(Kη )2
E ∥θ t+1 −θ tπ +1∥2
≤
(1
− 4
t )t E ∥θ 0 −θ 0π ∥2 +η t2∆ t
where there exist constants C ,C > 0 depending on L,L/µ,L2 /L3 and c = log2(d), such
1 2 H d
that
N (c) N
w B 1 ρ
∆ =C T2K2 c (Kη )2 + − d +C K w2σ2d
t 1 L∇ t L 2 c g
Xc=1(cid:16) (cid:17) Xc=1
Bias Correlation
Stoc.Grad.
| {z } | {z }
with B(c) := sup E f(c)(θ(c) ) 2. | {z }
t ∥∇ t,0 ∥
∇
The proof is postponed to Section G in the supplementary material. The divergence
error is made up of three main components: error resulting from bias across local nodes
(which includes heterogeneity and sampling cost), momentum noise, and gradient noise. In
the absence of stochastic gradients and when momentum is identical across nodes, the only
errors present are lower-order biases. Similar intermediate contraction results have been
derived in the literature on gradient-based sampling algorithms (e.g., Deng et al., 2021;
Plassier et al., 2023).
By the definition of Wasserstein metric, Theorem 4.4 immediately establishes a conver-
gence result of the marginal distribution of θ , denoted by π , towards π under Wasserstein-
t t
2 distance. The convergence result involves a term N c=1w csup tE ∥∇f(c)(θ t( ,c 0) ) ∥2/L. In
LemmaD.7intheappendix,weshowsthatuniformly,EP ∥∇f(c)(θ t( ,c 0) ) ∥2 = O( N c=1w
c
∥∇f(c)(θ ∗) ∥2+
LE ∥θ
0
−θ 0π ∥2+d) omitting its dependency on constants L,L/µ and L2 H/ eLP3, and in conse-
quence, solving the two inequalities
(1 −µ(Kη t)2/4)t E ∥θ 0 −θ 0π ∥2
≤
ϵ2/2, η t2∆ t
≤
ϵ2/2,
we obtain Theorem 4.1.
13On the other hand, in literature, people design settings for converging learning rate
such that the extra logarithmic factor in the convergence result can be removed. We also
obtain a similar result on a learning rate design as stated in the following proposition.
Proposition 4.5 (Dynamic stepsize). Under Assumptions 3.1-3.4, there is a setting of
{η t }t for Algorithm 3 such that E ∥θ t −θ tπ ∥2
≤
ϵ2 at some t
≤
Clog2(d)d T2(γ+(1 −ρ)N +
N w2σ2/K /ϵ2, with γ = min 1/√L,ϵ/√dT,ϵ2/(dT2(1 ρ)N),ϵ2K(cid:0)/(d N w2σ2) (cid:1).
c=1 c g { − c=1 c g }
P (cid:1) P
By this proposition, we see that the log(d/ϵ2) factors are removed in the convergent
iterationcomparedtoTheorem4.1. Onesettingofη thatsatisfiestheclaimsinProposition
t
4.6 is specified in the proof (i.e., Section H in the supplement file).
5 Experiments
In this section, we first compare the empirical performance of FA-HMC and FA-LD on
simulated data. Then we examine the relationship between dimension and communication
round in our theoretical suggested setting of the learning rate. Last we present the per-
formance of FA-HMC on the real datasets. We apply FA-HMC with constant stepsize η
and the same momentum initialization across devices. We conduct the synchronization of
the model parameters every T local leapfrog step in the implementation of FA-HMC. Due
to the significant computational costs involved in evaluating performance at each cohort
level, some results in this section are obtained from a single run and others are obtained
by averaging multiple runs. We defer part of the results with error bars to Section L in the
supplementary materials.
5.1 Simulation: FA-HMC vs FA-LD
We first sample from the posterior of a Bayesian logistic regression on a simulated dataset
of dimension d = 1000 (Mangoubi and Vishnoi, 2018). Specifically, we split the dataset of
size 1000 equally into 20 local nodes; we run the experiments using both exact gradients
(i.e., vanilla version) and stochastic gradients, where the later ones are simulated by adding
an independent zero-mean Gaussian noise of variance σ2 = 100 to each coordinate of the
14true gradients. Note that simulating the randomness of the stochastic gradient by a normal
variable is consistent with the experiment setting in Mangoubi and Vishnoi (2018). We
argue that Gaussian noise is a reasonable approximation when invoking the central limit
theorem with a large enough batch size.
As the benchmark, we run Metropolis-adjusted HMC (MHMC) for a sufficient number
of iterations. To evaluate the performance of FA-HMC, we use the computable metric
1 d (µ ,ν ) as a measure of marginal error (ME) of two sets of samples, as proposed
d i=1W1 i i
byPMangoubi and Vishnoi (2018); Faes et al. (2011). This metric compares the empirical
distributions of the i-th coordinate of the two sets of samples, represented by µ and ν ,
i i
respectively.
We first compare FA-HMC with FA-LD (i.e., FA-HMC with K = 1). Noticing that
the communication limit is a major bottleneck for federated learning, we suppose the
local computation cost is negligible compared with the communication cost. Therefore, the
comparisonbetweenFA-HMCand FA-LDisbasedon thesamenumberofcommunications,
or equivalently, the same number of steps t. Fixing local step T = 10, we try different
stepsizes η and leapfrog steps K. For FA-HMC, we set K = π/(3η) following Mangoubi
⌊ ⌋
and Vishnoi (2018) when η 0.01 and tune K (such that the performance is optimal w.r.t
≤
the choice of K) when η 0.02. Each run consists of 2 107 steps and we collect the same
≥ ×
numberofsamplesfromthelast107 steps. WeplotthecurvesofthecalculatedMEsagainst
η in Figure 1(a) (exact gradients, G) and 1(b) (stochastic gradients, SG). We observe that
in this task, where FA-LD is already a competitive baseline, FA-HMC still significantly
outperforms FA-LD with around 5% improvement on the performance. Moreover, werealize
that a wide range of stepsizes for FA-HMC yields pretty decent performance. As such, FA-
HMC appears to be more robust w.r.t. its hyperparameters around the optimal choices,
suggesting that FA-HMC is easier to tune than FA-LD, and a small stepsize usually leads
to a good performance.
Next, we study the impact of local steps T on communication efficiency in FA-HMC
with SG. Fixing leapfrog step K = 100 and stepsize η = 0.01, we run FA-HMC with
T ranging from 1 to 100. For each run, we collect one sample after a fixed number of
15communication rounds and calculate the MEs in an online manner. Then, we report the
required rounds R to achieve ME = ϵ under different settings and present the results in
ϵ
Figure 1(c). As we can see, the optimal local step T is 70; setting T too large or too small
leads to more communication costs. We also notice that under the optimal local step, a
smaller ϵ leads to more improvement on the communication cost R compared with the
ϵ
result of T = 100. Moreover, compared with the communication efficiency of T = 1, the
optimal communication efficiency improves by more than 65 times when ϵ is around 0.101.
Furthermore, we reduce the dimension d to 10 in the simulated data and run FA-HMC
as well as FA-LD with different stepsizes η on this new dataset fixing local step T = 10.
Apart from the dimension d, the other settings are the same as those in the experiments for
Figure 1(b). To list a few, stochastic gradients are adopted, and we choose the leapfrog step
K = π/(3η) when η 0.01 and tune K > 1 when η 0.02 for FA-HMC. The curves of
⌊ ⌋ ≤ ≥
the MEs against η are plotted in Figure 1(d). We observe that the general pattern in Figure
1(d) is similar to Figure 1(b). The optimal performance of FA-HMC is better than that of
FA-LD, and the performance gap is larger for smaller step sizes. Comparing Figure 1(d)
with Figure 1(b), we comment that FA-HMC is more advantageous under high-dimensional
settings. This observation is consistent with our theoretical results that FA-HMC has a
better convergence rate in terms of the dimension.
106.5
1.079 1.079 e=0.1012
FA−HMC FA−HMC e=0.1050 FA−HMC
FA−LD FA−LD 106.0 e=0.1100 1.007 FA−LD
0.179
0.179
105.5 0.107
0.089 0.089
105.0 0.017
0.08 0.080
104.5 0.008
0.001 S0 te.0 p10 size h 0.100 1.000 0.001 S0 te.0 p10 size h 0.100 1.000 0 20 Lo4 c0 al step6 0 (T) 80 100 0.001 S0 te.0 p10 size h 0.100 1.000
(a) Study of η and K (G)(b) StudyofηandK (SG) (c) Study of T (SG) (d) d=10 (SG)
Figure 1: Experimental results of FA-HMC and FA-LD on the simulated dataset using
exact gradients (G) and stochastic gradients (SG). Dimension d = 1000 in Figure (a)-(c)
and d = 10 in Figure (d).
16
)EM(
rorrE
lanigraM
)EM(
rorrE
lanigraM
)eR(
dnuor
deriuqeR
)EM(
rorrE
lanigraM5.2 Simulation: Dimension vs Communication for FA-HMC
Inthisexperiment, underthesuggestedsettingoflearningrateinProposition4.3, weexam-
inetherelationshipbetweencommunicationroundst /T requiredtoachievea (θ ,θπ)2 <
ϵ W2 tϵ
0.1 and dimension d.
To obtain an accurate computation of the (θ ,θπ), we consider a distributed het-
W2 tϵ
erogeneous Gaussian model where the (θ ,θπ) can be explicitly calculated in terms of
W2 tϵ
the population mean and variance of the parameter. Specifically, we assume that the pos-
terior distribution of half of the local nodes’ parameters is N(201 d,Id), and for the other
half, it is N(1 d,2Id). One can check that the overall posterior distribution of parameters
is N(16.21 d,1.6Id). We use leapfrog steps K = 5, local steps T = 10, and a learning rate
η = 0.02/d1/4. For different dimensions d = 2,50,100,150,...,950,1000, we repeat the ex-
periment 200 d(d 1)/2 times and sample the parameter θ at the last iteration t for each
t
· −
time. The sampled parameters allow us to estimate the population mean and variance on
the calculation of (θ ,θπ).
W2 tϵ
The simulation results in Figure 2 suggest that the square of communication round
(t /T)2 is approximately proportional to dimension d. This aligns well with our the-
ϵ
oretical discovery in Proposition 4.3, where under the suggested learning rate setting
t /T = O(√dlog(d/ϵ2)/ϵ).
ϵ
4e7
3e7
2e7
1e7
200 400 600 800
Dimensions d
Figure 2: Experimental results of FA-HMC to achieve < 0.1 at different dimensions d.
2
W
17
2)T/t(
2snoitacinummoC5.3 Application: Logistic Regression Model for FMNIST
In this section, we apply FA-HMC to train a logistic regression on the Fashion-MNIST
dataset. The data points are randomly split into 10 subsets of equal size for N = 10
clients. We run FA-HMC under different settings of local step T and leapfrog step K with
stochastic gradients that are calculated using a batch size of 1000 in each local device. In
each run, one parameter sample is collected after a fixed number of communication rounds,
and the predicted probabilities made by all the previously collected parameter samples
are averaged to calculate four test statistics: prediction accuracy, Brier Score (BS) (Brier
et al., 1950), Expected Calibration Error (ECE) (Guo et al., 2017), and Negative Log
Likelihood (NLL) on the test dataset. We tune the step size η in each setting for the best
test statistic. We conduct 5 independent runs in each setting and report the average results
of those chains. The standard deviations of the results across multiple runs are displayed
in Section L in the supplementary materials.
Specifically, to study the impact of leapfrog step K on the performance of FA-HMC,
we fix local step T = 50, run FA-HMC with K = 1, 10, 50, and 100, and plot the curves of
the calculated test statistics (accuracy, BS, ECE, and NLL) against communication rounds
in Figure 3. As we can see, under the same budgets of communication and computation,
FA-LD (K = 1) performs the worst in terms of BS, ECE, and NLL and the second worst
in terms of accuracy, which shows the superiority of FA-HMC with K > 1 over FA-LD.
Moreover, FA-HMC with K = 50 performs the best in terms of accuracy, BS, and NLL
and achieves a small ECE. In particular, the improvement on ECE and NLL over K = 1
can be as large as 26% and 2% respectively, indicating that the optimal choice of leapfrog
step is around 50 in this setting.
To study the impact of local step T on the performance of FA-HMC, we fix leapfrog
step K = 10, run FA-HMC with T = 1, 10, 20, 50, and 100, and plot the curves of the
calculated test statistics (accuracy, BS, ECE, and NLL) against communication rounds in
Figure 4. According to the figure, FA-HMC with T = 1 performs the worst in terms of all
four statistics, which shows the necessity of multiple local updates in this setting. Besides,
the optimal local step T differs with testing evaluation metrics; e.g., the optimal T is 50 in
180.847
K=1 0.1050 K=1 K=1
K=10 K=10 14300 K=10
K=50 K=50 K=50 0.838 0.318 K=100 0.0366 K=100 K=100 5300
K=1 0.228 0.748 0.0150 K=10
K=50 4400
K=100
0.219 0.0082
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
Communication rounds Communication rounds Communication rounds Communication rounds
(a) Accuracy (b) BS (c) ECE (d) NLL
Figure 3: The impact of leapfrog steps K on FA-HMC applied on the Fashion-MNIST
dataset.
terms of BS, while the optimal T is 20 in terms of NLL.
0.846 T=1 T=1 T=1
T=10 T=10 14300 T=10
T=20 T=20 T=20
0.1050
0.318 T=50 T=50 7462 T=50
0.837 T=100 T=100 T=100
0.0366 5300
T=1
0.747 T=10 0.228 4616
T=20 0.0150
4400
T=50
T=100
0.219 0.0082 4332
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
Communication rounds Communication rounds Communication rounds Communication rounds
(a) Accuracy (b) BS (c) ECE (d) NLL
Figure 4: The impact of local steps T on FA-HMC applied on the Fashion-MNIST dataset.
5.4 Application: Neural Network Model for FMNIST
To further assess the performance of FA-HMC on non-convex problems, we apply FA-HMC
and FA-LD to train a fully connected neural network with two hidden layers2 and the ReLU
activation function on the Fashion-MNIST dataset. Other settings of the experiments are
the same as the logistic regression experiments in Section 5.3 except that only one chain
is simulated in each case. We also calculate prediction accuracy, Brier Score (BS), and
Expected Calibration Error (ECE) on the test dataset. The step size η is tuned in each
setting for the best test statistic. Fixing local step T = 50 and choosing leapfrog step
K = 1, 10, 50, and 100, the curves of the calculated test statistics against communication
2The widths of two layers are 512 times input dimension and 512 times the number of classification
labels respectively.
19
ycaruccA
ycaruccA
)SB( erocs
reirB
)SB(
erocs
reirB
)ECE(
rorre noitarbilac
detcepxE
)ECE(
rorre
noitarbilac
detcepxE
)LLN(
doohilekil
gol
evitageN
)LLN(
doohilekil
gol
evitageNrounds are plotted in Figure 5(a), 5(b), and 5(c). As is shown in the figures, the optimal
leapfrog step differs among different test statistics. For accuracy and ECE, the optimal
K = 10 (i.e., FA-HMC notably outperforms FA-LD), while the optimal K = 1 for BS (i.e.,
FA-LD sightly outperforms FA-HMC). Fixing K = 10 and choosing T = 1, 10, and 50, the
curves of the calculated test statistics against communication rounds are plotted in Figure
5(d), 5(e), and 5(f). We can see that the best local step is T = 50 and the worst local step
is T = 1, indicating that the communication cost can be greatly reduced.
00 .. 88 99 88 09 0.245 K K K K= = = =1 1 5 10 0 00 0.105 K K K K= = = =1 1 5 10 0 00 00 .. 88 99 88 09 1.14 T T T= = =1 1 50 0 0.104 T T T= = =1 1 50 0
0.8890 0.015 0.8890 0.24 0.014
0.7990 K K K K= = = =1 1 5 10 0 00 0.155 0.7990 T T T= = =1 1 50 0
0.146 0.006 0.15 0.005
0 Co5m0munic1a0t0ion rou1n50ds 200 0 Co50mmunic1a0t0ion rou1n5d0s 200 0 Co50mmunic1a0t0ion rou1n5d0s 200 0 Co5m0munic1a0t0ion rou1n50ds 200 0 C5o0mmunic1a0t0ion rou1n5d0s 200 0 Co50mmunic1a0t0ion rou1n5d0s 200
(a) Accuracy (b) BS (c) ECE (d) Accuracy (e) BS (f) ECE
Figure 5: The impact of leapfrog step K and local step T on FA-HMC applied to train a
two-hidden-layer neural network on the Fashion-MNIST datasets.
5.5 Application: Logistic Regression Model on KMNIST/CIFAR2
WealsoapplyFA-HMCtotrainlogisticregressionontheKuzushiji-MNIST(KM)(Clanuwat
et al., 2018) and CIFAR10 dataset (Krizhevsky et al., 2009). Specifically, we only use the
first two classes (airplane and automobile) of the CIFAR10 dataset in the experiments to
simplify the problem and denote it by CF2. The data points in each dataset are randomly
split into 10 subsets of equal size for N = 10 clients. We run FA-HMC under different
settings of local step T and leapfrog step K with stochastic gradients that are calculated
using a batch size of 1000 in each local device. As usual, we tune the step size η in each
setting and report the best statistics: prediction accuracy (AC), Brier Score (BS), and
Expected Calibration Error (ECE) on the test dataset. The choices of local step T and
leapfrog step K are the same as those in Section 5.4.
The performance of FA-HMC using different leapfrog steps K is shown in Figure 6. We
see that the optimal leapfrog step K varies with different test statistics and datasets, for
example, the best K is 10 for AC and BS and the best K is larger than 10 for ECE on
20
ycaruccA
)SB(
erocs
reirB
)ECE( rorre
noitarbilac
detcepxE
ycaruccA
)SB(
erocs
reirB
)ECE( rorre
noitarbilac
detcepxE0.837 0.703
K=1 K=1 K=1 0.114 K=1
0.3392 K K= =1 50 0 0.113 K K= =1 50 0 K K= =1 50 0 K K= =1 50 0 K=100 K=100 0.694 0.518 K=100 K=100
0.828 0.2492
0.024
K=1 0.023 0.604 K=1 0.428 K=10 0.2402 K=10
0.738 K=50 K=50
K=100 K=100
0.2393 0.014 0.419 0.015
0 Co50mmunic1a0t0ion rou1n5d0s 200 0 Co5m0munic1a0t0ion rou1n50ds 200 0 Co50mmunic1a0t0ion rou1n5d0s 200 0 Co50mmunic1a0t0ion rou1n5d0s 200 0 Co50mmunic1a0t0ion rou1n5d0s 200 0 Co50mmunic1a0t0ion rou1n5d0s 200
(a) CF2: AC (b) CF2: BS (c) CF2: ECE (d) KM: AC (e) KM: BS (f) KM: ECE
Figure 6: The impact of leapfrog step K on FA-HMC applied on the CIFAR2 and KMNIST
datasets.
0.837 0.705
T=1 T=1 T=1 T=1
0.337 T=10 0.116 T=10 T=10 0.114 T=10
0.828 T=50 T=50 0.696 0.517 T=50 T=50
0.247 0.026 0.024
T=1 0.606 T=1 0.427 0.738 T=10 T=10
T=50 T=50
0.238 0.017 0.418 0.015
0 Co50mmunic1a0t0ion rou1n5d0s 200 0 Co50mmunic1a0t0ion rou1n5d0s 200 0 Co50mmunic1a0t0ion rou1n5d0s 200 0 Co50mmunic1a0t0ion rou1n5d0s 200 0 Co50mmunic1a0t0ion rou1n5d0s 200 0 Co50mmunic1a0t0ion rou1n5d0s 200
(a) CF2: AC (b) CF2: BS (c) CF2: ECE (d) KM: AC (e) KM: BS (f) KM: ECE
Figure 7: The impact of local step T on FA-HMC applied on the CIFAR2 and KMNIST
datasets.
the CF2 dataset, and none of the experiments support K = 1 (i.e., FA-LD) as the optimal
leapfrog step, showcasing the advantage of FA-HMC (i.e., K > 1) over FA-LD. We also
study the impact of different local steps T as shown in Figure 7. We observe that except for
the AC metric on CF2, federated learning with T > 1 outperforms the standard baseline
T = 1 on the rest of the metrics on both the CF2 and KM datasets.
6 Conclusions and Future Work
Inthispaper, wedevelopatighttheoreticalguaranteeforFA-HMCandprovidesuggestions
to speed up FA-HMC. Through experimentation, we demonstrate that FA-HMC outper-
forms FA-LD. We believe that FA-HMC potentially captures the similarities between local
nodes, giving it an advantage over FA-LD. For future directions, it would be interesting
to explore if further improvements can be achieved by addressing heterogeneity in local
leapfrog steps. Note that for second-order methods, one would need to tackle heterogene-
ity both on local positions and local momentum parameters. For example, motivated by
Karimireddy et al. (2020), suppose at t -th iteration (communication round), each local
0
21
ycaruccA
ycaruccA
)SB(
erocs
reirB
)SB( erocs
reirB
)ECE(
rorre
noitarbilac
detcepxE
)ECE(
rorre noitarbilac
detcepxE
ycaruccA
ycaruccA
)SB(
erocs
reirB
)SB( erocs
reirB
)ECE(
rorre
noitarbilac
detcepxE
)ECE(
rorre noitarbilac
detcepxEdevice obtain θ and f(θ ) := N w f(c)(θ ). Then each local device with lo-
t0+T ∇ t0 c=1 c ∇ t0
cal loss function f(c) is going to perPform the following update for k = 0,1,...,K 1,
−
t = t +T +1,...,t +2T
0 0
η2
θ(c) =θ(c) +η p(c) t f(c)(θ(c) ) f(c)(θ )+ f(θ ) ,
t,k+1 t,k t t,k − 2 ∇ t,k −∇ t0 ∇ t0
η
p(c) =p(c) t f(c)(θ(cid:0)(c) )+ f(c)(θ(c) ) 2 f(c)(θ )+(cid:1) 2 f(θ ) .
t,k+1 t,k − 2 ∇ t,k ∇ t,k+1 − ∇ t0 ∇ t0
(cid:0) (cid:1)
The complete version is deferred to Algorithm B.2 in the Supplementary Material.
Anotherdirectionweareworkingonistoconsidertheprivacyguaranteeofthesampling
algorithms and compare them with optimization algorithms.
It would also be interesting to examine the above directions and the application of
underdamped Langevin Monte Carlo algorithm to federated learning as future research.
References
¨
Akyildiz, O. D. and Sabanis, S. (2020), “Nonasymptotic Analysis of Stochastic Gradient
Hamiltonian Monte Carlo under Local Conditions for Nonconvex Optimization,” arXiv
preprint arXiv:2002.05465.
Bonawitz, K., Eichner, H., Grieskamp, W., Huba, D., Ingerman, A., Ivanov, V., Kiddon,
C., Koneˇcny`, J., Mazzocchi, S., and McMahan, B. (2019), “Towards Federated Learning
at Scale: System design,” Proceedings of Machine Learning and Systems, 1, 374–388.
Brier, G. W. et al. (1950), “Verification of Forecasts Expressed in Terms of Probability,”
Monthly weather review, 78, 1–3.
Chau, H. N. and Rasonyi, M. (2022), “Stochastic gradient Hamiltonian Monte Carlo for
non-convex learning,” Stochastic Processes and their Applications, 149, 341–368.
Chen, M., Yang, Z., Saad, W., Yin, C., Poor, H. V., and Cui, S. (2020a), “A Joint Learning
and Communications Framework for Federated Learning over Wireless Networks,” IEEE
Transactions on Wireless Communications, 20, 269–283.
22Chen, T., Fox, E., and Guestrin, C. (2014), “Stochastic Gradient Hamiltonian Monte
Carlo,” in International Conference on Machine Learning (ICML).
Chen, Y., Dwivedi, R., Wainwright, M. J., and Yu, B. (2020b), “Fast Mixing of
Metropolized Hamiltonian Monte Carlo: Benefits of Multi-step Gradients.” Journal of
Machine Learning Research, 21, 92–1.
Chen, Z. and Vempala, S. S. (2022), “Optimal convergence rate of hamiltonian monte carlo
for strongly logconcave distributions,” Theory of Computing, 18, 1–18.
Cheng, X., Chatterji, N. S., Bartlett, P. L., and Jordan, M. I. (2018), “Underdamped
Langevin MCMC: A Non-asymptotic Analysis,” in Conference on Learning Theory
(COLT).
Clanuwat,T.,Bober-Irizar,M.,Kitamoto,A.,Lamb,A.,Yamamoto,K.,andHa,D.(2018),
“Deep learning for classical japanese literature,” arXiv preprint arXiv:1812.01718.
Dalalyan, A. S. (2017), “Theoretical Guarantees for Approximate Sampling from Smooth
and Log-concave Densities,” Journal of the Royal Statistical Society: Series B, 79, 651–
676.
Dalalyan, A. S. and Karagulyan, A. (2019), “User-friendly Guarantees for the Langevin
Monte Carlo with Inaccurate Gradient,” Stochastic Processes and their Applications,
129, 5278–5311.
Deng, W., Ma, Y.-A., Song, Z., Zhang, Q., and Lin, G. (2021), “On Convergence of
Federated Averaging Langevin Dynamics,” arXiv preprint arXiv:2112.05120.
Duane, S., Kennedy, A., Pendleton, B., andRoweth(1987), “HybridMonteCarlo,” Physics
Letters B, 195, 216–222.
Erdogdu, M. A. and Hosseinzadeh, R. (2021), “On the Convergence of Langevin Monte
Carlo: the Interplay between Tail Growth and Smoothness,” in Proc. of Conference on
Learning Theory (COLT).
23Faes, C., Ormerod, J. T., and Wand, M. P. (2011), “Variational Bayesian inference for
parametric and nonparametric regression with missing data,” Journal of the American
Statistical Association, 106, 959–971.
Gelman, A., Carlin, J. B., Stern, H. S., and Rubin, D. B. (1995), Bayesian data analysis,
Chapman and Hall/CRC.
Girolami, M. and Calderhead, B. (2011), “Riemann Manifold Langevin and Hamiltonian
Monte Carlo Methods,” Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 73, 123–214.
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. (2017), “On Calibration of Modern
Neural Networks,” in International Conference on Machine Learning (ICML).
Gu¨rbu¨zbalaban, M., Gao, X., Hu, Y., and Zhu, L. (2021), “Decentralized Stochastic Gra-
dient Langevin Dynamics and Hamiltonian Monte Carlo,” Journal of Machine Learning
Research, 22, 1–69.
Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh, A. T. (2020),
“Scaffold: Stochastic Controlled Averaging for Federated Learning,” in International
Conference on Machine Learning (ICML).
Koneˇcny`, J., McMahan, B., and Ramage, D. (2015), “Federated optimization: Distributed
optimization Beyond the Datacenter,” arXiv preprint arXiv:1511.03575.
Koneˇcny`, J., McMahan, H. B., Yu, F. X., Richtarik, P., Suresh, A. T., and Bacon, D.
(2016), “Federated Learning: Strategies for Improving Communication Efficiency,” in
NIPS Workshop on Private Multi-Party Machine Learning, URL https://arxiv.org/
abs/1610.05492.
Krizhevsky, A., Hinton, G., et al. (2009), “Learning multiple layers of features from tiny
images,” Technical report, University of Toronto, ON, Canada.
Li,T.,Sahu,A.K.,Talwalkar,A.,andSmith,V.(2020a),“FederatedLearning: Challenges,
Methods, and Future Directions,” IEEE Signal Processing Magazine, 37, 50–60.
24Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smithy, V. (2020b),
“Federated Optimization in Heterogeneous Networks,” in Proceedings of the 3rd MLSys
Conference.
Li, X., Gu, Y., Dvornek, N., Staib, L. H., Ventola, P., and Duncan, J. S. (2020c), “Multi-
sitefMRIAnalysisusingPrivacy-preservingFederatedLearningandDomainAdaptation:
ABIDE Results,” Medical Image Analysis, 65, 101765.
Ma, Y.-A., Chen, T., and Fox, E. (2015), “A Complete Recipe for Stochastic Gradient
MCMC,” in Advances in Neural Information Processing Systems (NeurIPS), volume 28.
Mangoubi, O. and Vishnoi, N. K. (2018), “Dimensionally Tight Running Time Bounds for
Second-order Hamiltonian Monte Carlo,” in Advances in Neural Information Processing
Systems (NeurIPS).
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. (2017),
“Communication-efficient Learning of Deep Networks from Decentralized Data,” in Proc.
of the International Conference on Artificial Intelligence and Statistics (AISTATS).
Neal, R. M. (2012), “MCMC Using Hamiltonian Dynamics,” in Handbook of Markov Chain
Monte Carlo, volume 54, Chapman and Hall/CRC, 113–162.
Nemeth, C. and Fearnhead, P. (2021), “Stochastic Gradient Markov Chain Monte Carlo,”
Journal of the American Statistical Association, 116, 433–450.
Plassier,V.,Moulines,E.,andDurmus,A.(2023),“Federatedaveraginglangevindynamics:
Toward a unified theory and new algorithms,” in Proc. of the International Conference
on Artificial Intelligence and Statistics (AISTATS).
Raginsky, M., Rakhlin, A., and Telgarsky, M. (2017), “Non-convex Learning via Stochastic
Gradient Langevin Dynamics: a Nonasymptotic Analysis,” in Proc. of Conference on
Learning Theory (COLT).
25Rieke, N., Hancox, J., Li, W., Milletari, F., Roth, H. R., Albarqouni, S., Bakas, S., Galtier,
M. N., Landman, B. A., and Maier-Hein, K. (2020), “The Future of Digital Health with
Federated Learning,” NPJ digital medicine, 3, 1–7.
Tran, N. H., Bao, W., Zomaya, A., Nguyen, M. N., and Hong, C. S. (2019), “Federated
Learning over Wireless Networks: Optimization Model Design and Analysis,” in IEEE
INFOCOM 2019-IEEE Conference on Computer Communications, IEEE.
´
Vono, M., Plassier, V., Durmus, A., Dieuleveut, A., and Eric Moulines (2022), “QLSD:
Quantised Langevin Stochastic Dynamics for Bayesian Federated Learning,” in Proc. of
the International Conference on Artificial Intelligence and Statistics (AISTATS).
Wang,H.,Yurochkin,M.,Sun,Y.,Papailiopoulos,D.,andKhazaeni,Y.(2020),“Federated
Learning with Matched Averaging,” in International Conference on Learning Represen-
tations (ICLR).
Welling, M. and Teh, Y. W. (2011), “Bayesian Learning via Stochastic Gradient Langevin
Dynamics,” in International Conference on Machine Learning (ICML).
Zhu, L., Liu, Z., and Han, S. (2019), “Deep Leakage from Gradients,” in Advances in
Neural Information Processing Systems (NeurIPS), volume 32.
Zou,D.andGu,Q.(2021),“OntheConvergenceofHamiltonianMonteCarlowithStochas-
tic Gradients,” in International Conference on Machine Learning (ICML).
Zou, D., Xu, P., and Gu, Q. (2019), “Stochastic Gradient Hamiltonian Monte Carlo Meth-
ods with Recursive Variance Reduction,” Advances in Neural Information Processing
Systems (NeurIPS), 32.
— (2021), “Faster Convergence of Stochastic Gradient Langevin Dynamics for Non-log-
concave Sampling,” in Proc. of the Conference on Uncertainty in Artificial Intelligence
(UAI).
26Supplementary Material for “Bayesian Federated
Learning with Hamiltonian Monte Carlo: Algorithm
and Theory”
In this supplementary file, we first review related literature on federated sampling and
HMC in Section A. Then, we include the algorithms omitted in the main text in Section B.,
and the low bound result of t in Section C. Subsequently, we first organize the settings,
ϵ
notations and preliminary lemmas in Section D. Following that, we provide the proof for
the main result in Section E and offer a sketch proof for the convergence of vanilla FA-HMC
in Section F. This is done to enlighten the understanding of more complex cases. We then
present the formal proofs for the main convergence result and their associated preliminary
lemmasinSectionsG-J.Wealsodiscusshowtoextendtheproofstomoregeneralstochastic
gradientassumptionsinSectionK.Finally, inSectionL,weshowadditionalplotswitherror
bars that are omitted from the main text.
A Related Work
Federated optimization Federated optimization is a collaborative learning that trains
a model without a direct share of user data. In addition to encryption techniques to
ensure secure communications, a major focus is to minimize communications in distributed
computing (Dean et al., 2012; Shokri and Shmatikov, 2015; McMahan et al., 2016, 2017;
Yu et al., 2019; Karimireddy et al., 2020; Li et al., 2021; Huang et al., 2021). In particular,
federated averaging (FedAvg, McMahan et al., 2017) proposed a scalable paradigm by
conducting more local steps to achieve this target, which further motivates the study based
on non-iid data (Zhao et al., 2018; Sattler et al., 2020; Li et al., 2020b,a) and asynchronous
computing (Xie et al., 2020).
Federated Langevin sampling: Vanilla-distributed Monte Carlo methods require fre-
quent communications of user data, which has data privacy concerns. To tackle this issue
while maintaining model uncertainty, posterior averaging is empirically studied in feder-
ated learning to reduce data leakage risks Al-Shedivat et al. (2021); Mekkaoui et al. (2021);
Chen and Chao (2021). To guarantee theoretical properties in privacy and communication
efficiency, further analysis has been established in Deng et al. (2021); Plassier et al. (2023);
Vono et al. (2022) based on multiple local steps and compressed operators, respectively.
1
4202
luJ
9
]GL.sc[
1v53960.7042:viXraDistributed Monte Carlo methods Despite the advances of Monte Carlo methods
in big data problems (e.g., Welling and Teh, 2011; Ahn et al., 2012; Chen et al., 2014,
2015; Cheng et al., 2018; Mou et al., 2021; Deng et al., 2020, 2022), the computation is still
inefficient given limited devices. To mitigate this issue, Neiswanger et al. (2014); Wang and
Dunson (2013); Minsker et al. (2014) proposed sub-posterior aggregation to speed-up the
computations with distributed devices. In addition, other types of parallel paradigms, such
as synchronous, asynchronous, decentralized computing, are also conducted with Monte
Carlo computations Nishihara et al. (2014); Ahn et al. (2014); Chen et al. (2016); Chowd-
hury and Jermaine (2018); Li et al. (2019).
Hamiltonian Monte Carlo: HMC is widely known as a state-of-the-art sampling algo-
rithm (Neal, 2012; Hoffman and Gelman, 2014; Chen et al., 2014). However, a theoretical
underpinning for the convergence rate study has been lacking until recently. The irre-
ducibility and geometrical ergodicity have been studied in Durmus et al. (2020). Mangoubi
and Vishnoi (2018) showed that the number of gradient evaluations for unadjusted HMC
only depends on the dimension with an order as low as d1/4 given proper smoothness. On
another direction, Mangoubi and Smith (2017) presented the convergence rate with an
upper bound of O(κ2) for the unadjusted HMC; Lee et al. (2018) improved that result
to O(κ1.5) by considering an ODE solver in the HMC algorithm and Chen and Vempala
(2022) further refined the mixing time to O(κ).
Without the Metropolis-Hastings correction, the corresponding Markov chains lead to
a consistent bias and require a mixing time that is polynomially dependent on O(1/ϵ)
to control such errors; by contrast, the mixing time for Metropolized HMC can be fur-
ther improved to O(log(1/ϵ)). To analyze the Metropolized HMC algorithms, Bou-Rabee
et al. (2020) proposed the coupling methods and showed that the mixing-time bound scales
in the order of O(d1.5); Chen et al. (2020) presented the non-asymptotic convergence and
confirmedthatMetropolishedHMCisstrictlyfasterthanMALAandotherbasicMetropol-
ished algorithms.
In big data settings, querying the entire dataset becomes quite expensive and increases
the challenge to obtain the desired performance (Betancourt, 2015; Bardenet et al., 2017;
Dang et al., 2019); to tackle this issue, Zou and Gu (2021) studied the convergence of HMC
based on stochastic gradients achieves the target distribution with an error up to O(√η)
(learning rate); and variance reduction techniques were further proposed to reduce that
error; in decentralized settings, Gu¨rbu¨zbalaban et al. (2021) studied non-asymptotic guar-
anteesofSGHMCbyconstructingaproperLyapunovfunctionandappropriateparameters;
they further view decentralized SGHMC as a noisy heavy ball algorithm (Flammarion and
Bach, 2015; Bugra Can, 2019; Xin and Khan, 2020).
B Algorithms Omitted in the Main Text
2Algorithm B.1 de-bias leapfrog h
de-LF
Input: Energy function f(c)( ); Shared gradient f(θ ) and parameter θ ; Initial pa-
· ∇
t0 t0
rameters θ , momentum p ; learning rate η; leapfrog step K; k = 0
t0 0
while k K: do
θ =≤ θ +ηp η2 f(c)(θ ) f(c)(θ )+ f(θ )
k+1 k k − 2 ∇ k −∇ t0 ∇ t0
p = p η( f(c)(θ )+ f(c)(θ ) 2 f(c)(θ )+2 f(θ ))
k+1 k − 2 ∇ (cid:0)k ∇ k+1 − ∇ t0 (cid:1) ∇ t0
k = k +1;
Output: θ
K
Algorithm B.2 debias FL-HMC algorithm
(c)
Input: Initial parameters θ = θ , θ = f(θ ) = 0, t = 0; Stepsize function
0 0 −T ∇ −T
η = η(t); Local update step T; Leapfrog update step K
t
while stopping conditions are not satisfied do
(c) (c)2
sample momentum p
t ∼
N(0,σ
t
Id)
if t 0(mod T) then
≡
Broadcast θ := N w θ(c) , f(θ ) := N w f(c)(θ ) and set θ(c) = θ
t c=1 c t ∇ t −T c=1 c ∇ t −T t+1,0 t
else
(c) (c) P P
θ = θ
t+1,0 t
update
θ(c)
= h
(f(c),θ(c)
, f(θ ),θ
,p(c)
,η ,K) in parallel for all devices,
t+1 de-LF t+1,0 ∇ t −T t −T t t
t = t+1
C Lower Bound for FA-HMC Algorithm
In the main text, we claim a trade-off between communication and divergence based on the
upper bounds results in Theorem 4.1. In this section, we show that for certain sampling
problems, the upper bound results on vanilla FA-HMC above also match the lower bound
below.
Theorem C.1 (Dimensional tight lower bound for ideal FA-HMC process). Under As-
sumptions 3.1-3.3, if we initialize θ
0
N(0,σ2 Id) for any σ2 > 0, then there is a sampling
∈
task where the least running time t for continuous FA-HMC to achieve (θ ,θπ)2 ϵ2 is
2 t
W ≤
taken when t = Ω(√dT log(d/ϵ)/ϵ).
The result provides a lower bound on the continuous FA-HMC algorithm, where the
discretization error does not exist. Therefore, even if FA-HMC runs with an infinitely small
learning rate, the convergence rate cannot be improved in general.
3D Preliminary Settings
D.1 Continuous HMC Process of Single Device
The dynamic of HMC regrading the loss(energy) function f( ) is characterized by the
·
following two ODEs:
dθ(t)
=p(t),
dt
(1)
dp(t)
= f(θ(t)).
θ
dt −∇
Noting that the solution of
t t s
θ(t) = θ(0)+ p(s)ds = θ(0)+ p(0)+ f(θ(u))du ds (2)
θ
∇
Z0 Z0 Z0
(cid:16) (cid:17)
involves double integration with respect to f(θ(u)). Denote π(θ(0)) = θ(s), the tra-
∇θ Hs
jectory starting from θ(0) with an independently sampled momentum p(0). Further, we
define
θ tπ = ( HKπ ηt)Kt ηt(θπ), t
∈
N ·Kη t.
Note that each mapping uses an independently sampled momentum.
HKηt
D.2 Notations for FA-HMC
(c) (c)
For iteration t = 0,1,..., k = 0,1,..., define θ and p to be the position and momentum
t,k t,k
parameters of the c-th local device at iteration t and k-th leapfrog step, respectively and
θ =
N
w
θ(c)
, p =
N
w
p(c)
,
p(c)
=
p(c)
,
θ(c)
=
θ t, if Tt
∈
Z
t c t t c t t,0 t t,0 θ(c) , if t / Z
Xc=1 Xc=1 n t T ∈
We use the following short hand
1 ρ
σ t(c) := (E ∥p( tc) ∥2)1/2 = ((ρ+ w− )d)1/2, σ
t
:= (E ∥p
t
∥2)1/2 = d1/2.
c
For iteration t = 0,1,... and leapfrog step k = 1,2...,K, from the Algorithm 3, by
recursive calculations, if the true gradient is used, then
k 1
η − η
p(c) =p(c) t f(c)(θ(c)
) η
f(c)(θ(c)
)
t f(c)(θ(c)
)
t,k t,0 − 2∇ t,0 − t ∇ t,j − 2∇ t,k
j=1
X (3)
kη2 k −1
θ(c) =θ(c) +kη p(c) t f(c)(θ(c) ) η2 (k j) f(c)(θ(c) );
t,k t,0 t t,0 − 2 ∇ t,0 − t − ∇ t,j
j=1
X
4and if stochastic gradient is used, then
k 1
η η − η
(c) (c) t (c) t (c) (c) t (c)
p =p g (ξ ) g (ξ )+g (ξ ) g (ξ )
t,k t,0 − 2 t,0 0 − 2 t,j j −21 t,j j − 2 t,k k −21
Xj=1(cid:16) (cid:17) (4)
e kη2 e η2 k −1 e e
θ(c) =θ(c) +kη p(c) t g(c) (ξ ) t (k j) g(c) (ξ )+g(c) (ξ )
t,k t,0 t t,0 − 2 t,0 0 − 2 − t,j j −1 2 t,j j
Xj=1 (cid:16) (cid:17)
e e e
where we denote
N
g(c)
(ξ ) =
f(c)(θ(c) ,ξ(c)
),
g(c) (ξ(c)
) =
f(c)(θ(c) ,ξ(c)
), g ( ) = w
g(c)
( ).
t,k k −1 2 ∇ t,k t,k −21 t,k k ∇ t,k t,k t,k · c t,k ·
c=1
X
e e
e e e e
D.3 Assumptions
Assumption 3.1 (µ-Strongly Convex). For each c = 1,2,...,N, f(c) is µ-strongly convex
for some µ > 0, i.e., x,y Rd, f(c)(y) f(c)(x)+ f(c)(x),y x + µ y x 2.
∀ ∈ ≥ ⟨∇ − ⟩ 2∥ − ∥2
Assumption 3.2 (L-Smoothness). For each c = 1,2,...,N, f(c) is L-smooth for some
L > 0, i.e., x,y Rd, f(c)(y) f(c)(x) L x y .
∀ ∈ ∥∇ −∇ ∥ ≤ ∥ − ∥
Assumption 3.3 (L -Hessian Smoothness). For each c = 1,2,...,N, f(c) is L Hessian
H H
smoothness, i.e., for any θ 1,θ 2,p
∈
Rd,
∥
∇2f(c)(θ 1) −∇2f(c)(θ 2) p ∥2
≤
L2 H∥θ
1
−θ
2
∥2 ∥p ∥2 .
∞
Assumption 3.4 (σ -Bounded Varianc(cid:0)e). For local device c =(cid:1) 1,2,...,N, and leapfrog
g
step k = 1,2,...,K, t = 1,2,..., we have max tr(Var(
f(c)(θ(c) ,ξ(c)
)
θ(c)
))
σ2Ld, for some σ > 0.
x=k −1/2,k ∇ t,k t,x | t,k ≤
g g
e
D.4 Preliminary Lemmas
Lemma D.1 (Jensen’s inequality). For any vectors x n and positive constants λ n
{ i }i=1 { i }i=1
with λ = 1, due to the convexity of 2, we have
i
∥·∥
P
λ x 2 λ x 2;
i i i i
∥ ∥ ≤ ∥ ∥
i i
X X
1
x 2 λ x /λ 2 = x 2.
i i i i i
∥ ∥ ≤ ∥ ∥ λ ∥ ∥
i
i i i
X X X
For any vector-value function x(t) and non-negative function λ(t), if λ(t)dt = 1, then
R
λ(t)x(t)dt 2 λ(t) x(t) 2dt.
∥ ∥ ≤ ∥ ∥
Z Z
The next lemma is Proposition 1 in Durmus and Moulines (2019):
5Lemma D.2. Let x
∗
Rd be the global minimizer of a loss function f( ) satisfying Assump-
∈ ·
tion 3.1 and xπ be a random variable following distribution e f(x), then
−
∝
d
E ∥x ∗ −xπ ∥2 2 ≤ µ.
¯
Lemma D.3. Under Assumption 3.2 consider two HMC systems θ(t) and θ(t) starting
with the same initial positions and different initial momentum p(0) and p(0) respectively.
Suppose σ2 is the variance of the momentum, we have
1 e
θ(t) θ¯ (t) sinh( L/σ2t) p(0) p(0) ,
2 2
∥ − ∥ ≤ L/σ2 ∥ − ∥
p
Further if we have Assumption 3.1,p consider two HMC systems θ(et) and θ(t) starting with
initial positions θ(0) and θ(0), and initial momentum p(0) and p(0) respectively, then for
0 t 1 , we have e
≤ ≤ 2√L/σ2
e
e
µ
θ(t) θ(t) (1 t2) θ(0) θ(0) +Ct p(0) p(0) ,
∥ − ∥2 ≤ − 4σ2 ∥ − ∥2 ∥ − ∥2
where C is any constaent sinh(0.5). e
e
≥
The proof is postponed to Section H.1.
Lemma D.4 (One-step update). Under the same conditions of Theorem 4.4. Denote
N (Kη )2 (K3 K)η3
(I) = w θ(c) t f(c)(θ(c) ) − t 2f(c)(θ(c) )p(c)
c t,0 − 2 ∇ t,0 − 6 ∇ t,0 t
Xc=1 (cid:16) (cid:17)
(Kη )2 (Kη )3
θπ t f(θπ) t 2f(θπ)p ;
− t − 2 ∇ t − 6 ∇ t t
(cid:16) (cid:17)
we have
E ∥(I) ∥2
≤
(1
−µ(K 2η t)2
)E ∥θ t −θ tπ ∥2+2(Kη t)6L2 T
t −1 N
w c(1 −w c)∆( ic) +
N c=1w 9cσ t(c)2 d
(cid:0)
Xi=t0Xc=1 P
(cid:1)
(c)
where ∆ is defined in (13).
i
The proof is postponed to Section H.2.
Lemma D.5 (0-th order Approximation Bound). Let q(0),p(0) Rd be two vectors that
∈
independent of {ξ k,ξ k −21 }k. For any F( ·) : Rd 7→ R satisfying smoothness Assumption 3.2
and variance Assumption 3.4 with parameter σ2, under step size assumption Kη 1/√L,
≤
the output q(k) := h(F( ),q(0),p(0),η,k) yield by Algorithm 3 with gradients randomized
·
by ξ ,ξ satisfies
{ k k −21 }k
Eξ q(k) q(0) 2 2(kη)2 p(0) 2 +(kη)4 F(q(0)) 2 +(kη)4σ2d (5)
∥ − ∥ ≤ ∥ ∥ ∥∇ ∥
Eξ F(q(k)) F(q(0)) 2 L2(2(kη)2 p(0) 2 +(kη)4 F(q(0)) 2 +(kη)4σ2d) (6)
∥∇ −∇ ∥ ≤ ∥ ∥ ∥∇ ∥
where Eξ[ ·] denotes the expectation over random variables {ξ k,ξ
k −21
}k.
6The proof is postponed to Section H.3.
Lemma D.6 (1-th order Approximation Bound). Under Assumptions 3.2-3.4, for any iter-
ation t and any local device c in Algorithm 3, we have
1 B(c) σ2
E ∥θ t( ,c k) −θ t(c) −η tp( tc) k ∥2
≤
3(kη t)4L L∇ +σ t(c)2 d+ Lkg d (7)
E ∥∇f(c)(θ t( ,c k) ) −∇f(c)(θ t(c) ) −∇2f(c)(cid:0)(θ t(c) )η tp t(c) k ∥2
≤
(kη t)(cid:1)4L3δ k(c) (8)
where B(c) := sup E f(c)(θ(c) ) 2 and for 1 k K
t ∥∇ t,0 ∥ ≤ ≤
∇
c L2 B(c) c L2 σ2d c L2
δ(c)
:= (1+
d Hσ(c)2
)
+(σ(c)2
+
d Hσ(c)4
)d+
g
(1+
d Hσ(c)2
). (9)
k 8L3 t L∇ t L3 t kL L3 t
The proof is postponed to Section H.4.
Lemma D.7 (Uniform bound). Define θ = argmin f(θ). Under Assumptions 3.1-3.4 and
∗ θ
3.4, for any local device 1 c N in Algorithm 3 and any iteration t such that t
≤ ≤ ̸≡
0(mod T), we have
µ(Kη )2 2(Kη )6L4 (Kη )2L
E ∥θ t( +c) 1 −θ ∗ ∥2 ≤ (1 − 4 t + µt I {K ≥2 })E ∥θ t( ,c 0) −θ ∗ ∥2 + µt B a(c), (10)
where
2L 4 f(c)(θ ) 2 σ2
B(c) = d + ∥∇ ∗ ∥ + g d .
a µ 3µ 7Kµ
samplingcost localheterogeneity stochasticgradienteffect
Further if we choose (Kη )|2 {z} µ ,|and f{ozr any}t such th|at{tz } 0(mod T), we set η =
t ≤ 4L2 ≡ t
η = = η
t+1 t+T 1
··· −
N
8
supE ∥θ t( ,c 0) −θ
∗
∥2
≤
D+ µ(B a(c) + w cB a(c)) (11)
t 0
≥ Xc=1
N
8
supE ∥∇f(c)(θ t( ,c 0) ) ∥2
≤
2L2(D+ µ(B a(c) + w cB a(c)))+2 ∥∇f(c)(θ ∗) ∥2, (12)
t 0
≥ Xc=1
where D = θ θ 2, representing the initialization effect.
0 ∗
∥ − ∥
Lemma D.8 (Local bound). Under Assumptions 3.2-3.4, for any iteration k and any local
device c in Algorithm 3, we have
N t 1 N
−
w cE ∥θ t(c) −θ t ∥2
≤
T (Kη i)4 w c(1 −w c)L∆( ic)
Xc=1 Xi=t0 Xc=1
where t is the latest communication step before t, and
0
5B(c) 1 1 N w σ(c)2 1
∆(c) := + σ(c)2 d+ σ2d+ i=1 c t − d (13)
i 9 L∇ 81 t 6KL g L(Kη )2
P i
with B(c) = sup E f(c)(θ(c) ) 2.
∇
t ≥0 ∥∇ t,0 ∥
7The proof is postponed to Section H.6.
Lemma D.9 (Oracle bounds). If f satisfies Assumptions 3.1-3.4, then for any iteration t
and any u Kη , we have
t
≤
L2d
E ∥θ tπ(u) −θ tπ −p tu ∥2
≤ 4µ
u4, (14)
1
E ∥θ tπ(u) −θ tπ ∥2 ∥p t ∥2
≤
3u2c dd, (15)
E ∥∇f(θ tπ(u)) −∇f(θ tπ) −∇2f(θ tπ)p tu ∥2
≤
δπu4, (16)
where c = 128+32log2(2d) and
d
5 L
δπ := +L2 c /L3 d. (17)
4 µ H d
(cid:0) (cid:1)
The proof is postponed to Section H.7.
Lemma D.10 (Maximal Gaussian bounds). Suppose p 1,p
2
N(0,σ2 Id), then we have
∼
E p 1 2 c dσ2, E p 1 2 p 2 2 c ddσ4, E p 2 4 c dσ4,
∥ ∥ ≤ ∥ ∥ ∥ ∥ ≤ ∥ ∥ ≤
∞ ∞ ∞
with c = 128+32log2(2d).
d
The proof is postponed to Section H.8.
E Proof of Theorem 4.1
Theorem 4.1. Assume 3.1-3.4, and (π ,π)2 = O1(d) and N w f(c)(θ ) 2 = O(d).
W2 0 c=1 c ∥∇ ∗ ∥
For a given local iteration step T, there exists some constant C depending on L,L/µ,L2 /L3
P H
such that if we choose η(t) η and (denote γ = (Kη)2)
≡
γ 1 ϵ ϵ2 ϵ2
η2 = = Cmin , , ,
K2 K2L K2√dT K2dT2(1 ρ)N Kd N w2σ2
n − c=1 c g o
then (π ,π) ϵ for any ϵ > 0, with iteration number P
W2 tϵ
≤
dlog(d/ϵ2) N w2σ2
t = O1 T2 γ +(1 ρ)N + c=1 c g
ϵ ϵ2 − K
P
(cid:16) (cid:17)
(cid:0) (cid:1)
and corresponding communication etimes
t dlog(d/ϵ2) N w2σ2
ϵ = O T γ +(1 ρ)N + c=1 c g .
T ϵ2 − KT
P
(cid:16) (cid:17)
1 As d , we say f = O(g) if f Cg(cid:0) for some const(cid:1)ant C, and say f = O(g) for C being a
→ ∞ e≤
polynomial of log(d).
e
8Proof. By the assumptions and Theorem 4.4 and Lemma D.7, we have
1 (1 ρ)N N w2σ2d
∆ C T2 + − d+C c g
t 1 2
≤ L Lγ Kγ
t t
(cid:16) (cid:17) Xc=1
define D := E ∥θ 0 −θπ ∥2, then to have W(θ t,θπ)2
≤
E ∥θ t −θ tπ ∥2
≤
ϵ2, it is feasible to let
(η,t) to satisfy for some constant C
ϵ2 µ(Kη)2 ϵ2
C(Kη)2∆ , C(1 )tD = . (18)
t
≤ 2 − 4 2
By the fact that 1 1 for any 1 > x > 0, it follows that
−
log(1 x) ≥ x/(1 x)
− −
log(ϵ/(2D)) (1 µ(Kη )2/4)log(ϵ/(2D))
ϵ
t = −
ϵ log(1 µ(Kη )2/4) ≥ µ(Kη )2/4
ϵ ϵ
−
when µ(Kη )2/4 < 1. Further, by the stepsize assumption (i.e., Kη 1/√L up to a
ϵ
≤
constant) and L µ, we have 1 µ(Kη )2/4 15/16; and by (18) we have (Kη)2
ϵ
≥ − ≥ ≤
Cmin ϵ , ϵ2 , ϵ2K . Eventually, we get
√dT dT2(1 ρ)N d N w2σ2
− c=1 c g
n o
P
√dlog(ϵ/(2D))(κ2.5T
+κ3(1+(cdL2
H)0.5))
t C L3 .
ϵ
≥ ϵ√L
We can pick one t that satisfies the above inequality. This finishes the proof.
ϵ
F Sketch Proof of Convergence of Vanilla FA-HMC
In this section, we present a sketch proof for Theorem 4.4 without considering SG and
correlation between momentum. A formal proof is postponed in Section G.
Let θπ be the a random variable following the target distribution π, and θπ( ) be the
0 t ·
solution of the Hamilton’s dynamic (1) initialized from θπ and momentum p . Note that
t t
this p is the same momentum used in the vanilla FA-HMC (i.e., Algorithm 3 with ρ = 1
t
and σ = 0). Define θπ = θπ(Kη). Define θ(c) be the intermediate value that is yielded
g t+1 t t,j
after j steps of leapfrog approximation within the t-th iteration of Algorithm 3.
The main idea to bound the difference between w θ(c) and θπ.
c t t
Recursively applying the definitions of leapfrog approximation and Hamilton’s dynamic
P
respectively, it is easy to derive that
Kη2 K −1
θ(c) =θ(c) +Kηp f(c)(θ(c) ) η2 (K j) f(c)(θ(c) ),
t+1 t,0 t − 2 ∇ t,0 − − ∇ t,j
j=1
X
Kη s
θπ =θπ +Kηp f(θπ(u))duds.
t+1 t t − ∇ t
Z0 Z0
9Note that intermediate positions θ(c) , k = 1,2,...K and θπ(u) are correlated to momentum
t,k t
(c)
p . For simplicity of the representation, we consider the first-order approximation θ =
t t,j
θ(c) +jηp +O((jη)2) and θπ(u) = θπ +up +O(u2), and apply Taylor’s expansion to the
t,0 t t t t
gradients f(c)(θ(c) ) and f(θπ(u)) respectively. This yield the following approximation:
∇ t,j ∇ t
(Kη)2 (K3 K)η3
θ(c) θ(c)
+Kηp
f(c)(θ(c)
) −
2f(c)(θ(c)
)p , (19)
t+1 ≈ t,0 t − 2 ∇ t,0 − 6 ∇ t,0 t
(Kη)2 (Kη)3
θπ θπ +Kηp f(θπ) 2f(θπ)p . (20)
t+1 ≈ t t − 2 ∇ t − 6 ∇ t t
where the RHS of the equalities has a more tractable form in terms of p .
t
It follows that
N
w θ(c) θπ = (I) η2(II) (III) ,
c t+1 − t+1 − −
c=1
X second-orderapproximation discreteapprox. error continuousapprox. error
where |{z} | {z } |{z}
N (Kη)2 (K3 K)η3
(I) = w
θ(c) f(c)(θ(c)
) −
2f(c)(θ(c)
)p
c t,0 − 2 ∇ t,0 − 6 ∇ t,0 t
Xc=1 (cid:16) (cid:17)
(Kη)2 (Kη)3
θπ f(θπ) 2f(θπ)p ;
− t − 2 ∇ t − 6 ∇ t t
N K(cid:16) 1 (cid:17)
−
(II) = w (K k) f(c)(θ ) f(c)(θ(c) ) 2f(c)(θ(c) )ηp k ;
c − ∇ k −∇ t,0 −∇ t,0 t
Xc=1 Xk=1 (cid:16) (cid:17)
Kη s
(III) = f(θπ(u)) f(θπ) 2f(θπ)p u duds.
∇ t −∇ t −∇ t t
Z0 Z0
(cid:0) (cid:1)
To interpret the above decomposition, we comment that (I) is the second-order approxi-
mation of N w θ(c) θπ ; (II) is the error yielded by the second-order approximation
c=1 c t+1 − t+1
on the gradients; (III) can be viewed as the difference between the continuous process and
P
the discrete process of the global HMC system.
Now, by Lemma D.1, for any c ,c ,c > 0 with c = 1, we can separate the errors
1 2 3 i i
N P
1 1 1
w θ(c) θπ 2 (I) 2 + η4 (II) 2 + (III) 2. (21)
∥ c t+1 − t+1∥ ≤c ∥ ∥ c ∥ ∥ c ∥ ∥
1 2 3
c=1
X
Term (I) is the difference of the second-order approximation (i.e., (19)) of the discrete
FA-HMC system and that of the continuous process (i.e., (20)). Intuitively, both of them
converge in roughly the same direction. Therefore, this term is expected to have a contrac-
tion relationship with respect to the initial deviation, i.e., N w θ(c) θπ, and contains
c=1 c t − t
additional errors due to the stochastic momentum and slightly different form of recursive
P
10functions (19) and (20). Indeed, by Lemma D.4 in the appendix,
µ(Kη)2 N (Kη)6L2d
E ∥(I) ∥2
≤
(1
− 2
)E
∥
w cθ t(c) −θ tπ ∥2 +2T2(Kη)6L2∆+
9
, (22)
c=1
X
where t is the latest communication step before t-th iteration and ∆ := N w (1
0 c=1 c −
w c) 95B(c) /L+ 8d
1
with B(c) := sup tE ∥∇f(c)(θ t( ,c 0) ) ∥2.
P
Term∇s (II) and (III) stu∇dy the approximation errors of (19) and (20) respectively. The
(cid:0) (cid:1)
errors, intuitively are small given the small step size η.
For (II), due to Lemma D.1 and the fact that N c=1w c K k=− 11 K( (K K− 2k 1)k )/6 = 1, we have
−
P P
N K −1 (K k)K3
E ∥(II) ∥2
≤
w c −
6k
E ∥∇f(c)(θ k) −∇f(c)(θ t(c) ) −∇2f(c)(θ t(c) )ηp tk ∥2
c=1 k=1
X X (23)
N K −1 (K k)K3 K4(Kη)4 N
w − (kη)4δ(c) w δ(c)
c c
≤ 6k ≤ 120
c=1 k=1 c=1
X X X
where the second inequality is due to Lemma D.6 with δ(c) = (1+ cdL2 H)(d+B(c) /L).
L3
Last, for (III), note that
Kη s 12u2
duds = 1, by Lemma D.1,
∇
0 0 (Kη)4
R R
Kη s (Kη)4 (Kη)8L3δπ
E ∥(III) ∥2
≤ 12u2
E ∥∇f(θ tπ(u)) −∇f(θ tπ) −∇2f(θ tπ)p tu ∥2duds
≤ 144
.
Z0 Z0
where δπ := 5 L + cdL2 H sinh2(1) d and the last inequality is by (16) of Lemma D.9.
4 µ L3 4
Combining this with (21)-(23) and set
(cid:0) (cid:1)
µ(Kη)2/4 6 µ(Kη)2/4 5 µ(Kη)2/4
(c ,c ,c ) = (1 , , ),
1 2 3 − 1 µ(Kη)2/4 111 µ(Kη)2/4 111 µ(Kη)2/4
− − −
we have
1 µ(Kη)2 µ(Kη)2 1 15 1 22 1 44
(1 ) = 1 , , , ,
c − 2 − 4 c ≤ 14 c ≤ 3µ(Kη)2 c ≤ 5µ(Kη)2
1 1 2 3
and
N µ(Kη)2 N t −1
E ∥ w cθ t( +c) 1 −θ tπ +1∥2 ≤(1 − 4 )E ∥ w cθ t(c) −θ tπ ∥2 +3T(Kη)6L2 ∆ i
Xc=1 Xc=1 Xi=t0
(Kη)6L2d (Kη)6L3(Kη)6 N
+ + w δ(c) +δπ , (24)
c
8 15µ µ
(cid:16)Xc=1 (cid:17)
where we recall that ∆ = N w (1 w ) 5B(c) /L+ d , δ(c) = (1+cdL2 H)(d+B(c) /L) and
i c=1 c − c 9 81 L3
δπ = 5 L + cdL2 H sinh2(1) d. Rearranging the c∇ oefficients of B(c) /L and d, and r∇ ecursively
4 µ L3 4 P (cid:0) (cid:1)
∇
applying (24) concludes the proof.
(cid:0) (cid:1)
11G Proof of Theorem 4.4
G.1 Proof of Theorem 4.4
Theorem 4.4 (Convergence). Under Assumptions 3.1-3.4, if we set η η 1/(K√L)
t t
′′ ≤ ′ ≤
for any t t in Algorithm 3, then θ satisfies
′ ′′ t t
≤ { }
µ(Kη )2
E ∥θ t+1 −θ tπ +1∥2
≤
(1
− 4
t )t E ∥θ 0 −θ 0π ∥2 +η t2∆ t
where there exist constants C ,C > 0 depending on L,L/µ,L2 /L3 and c = log2(d), such
1 2 H d
that
N (c) N
w B 1 ρ
∆ =C T2K2 c (Kη )2 + − d +C K w2σ2d
t 1 L∇ t L 2 c g
Xc=1(cid:16) (cid:17) Xc=1
Bias Correlation
Stoc.Grad.
| {z } | {z }
with B(c) := sup E f(c)(θ(c) ) 2. | {z }
t ∥∇ t,0 ∥
∇
Proof. To start with, we rewrite the expression of θ(c) and θπ . By iterative formula (4)
t+1 t+1
and (2),
Kη2 η2 K −1
θ(c) =θ(c) +Kη p(c) t g(c) (ξ ) t (K j) g(c) (ξ )+g(c) (ξ ) ,
t+1 t,0 t t − 2 t,0 0 − 2 − t,j j −21 t,j j
Xj=1 (cid:16) (cid:17)
Kηt es
e e
θπ =θπ +Kη p f(θπ(u))duds,
t+1 t t t − ∇ t
Z0 Z0
where we recall the stochastic gradients are defined by
g(c)
(ξ ) =
f(c)(θ(c) ,ξ(c)
),
g(c)
(ξ ) =
f(c)(θ(c) ,ξ(c)
), k = 1,2,...,K.
t,k k −21 ∇ t,k t,k −21 t,k k ∇ t,k t,k
Noteethat θ(c) contae ins stochastic gradieents and intee rmediate positions θ(c) , k = 1,2,...K
t+1 t,k
that correlate to momentum p . These cause analytical troubles. To overcome these, we
t
(c)
use the second-order approximation with non-stochastic gradients to approximate θ as
t+1
12follow.
Kη2
θ(c) =θ(c) +Kη p(c) t (g(c) (ξ ) f(c)(θ(c) )+ f(c)(θ(c) ))
t+1 t,0 t t − 2 t,0 0 −∇ t,0 ∇ t,0
η2 K −1
t (K j) g(ce) (ξ )+g(c) (ξ ) 2 f(c)(θ(c) ) +2 f(c)(θ(c) )
− 2 − t,j j −1 2 t,j j − ∇ t,j ∇ t,j
Xj=1 (cid:16)
(cid:0) (cid:1) (cid:0)
f(c)(θ(c)
) jη
e2f(c)(θ(c) )p(ec)
+
f(c)(θ(c)
)+jη
2f(c)(θ(c) )p(c)
−∇ t,0 − t ∇ t,0 t ∇ t,0 t ∇ t,0 t
Kη2 η2 K −1 (cid:1)(cid:17)
=θ(c) +Kη p(c) t f(c)(θ(c) ) t (K j)2 f(c)(θ(c) )+jη 2f(c)(θ(c) )p(c)
t,0 t t − 2 ∇ t,0 − 2 − ∇ t,0 t ∇ t,0 t
j=1
X (cid:0) (cid:1)
Kη2 η2 K −1
t (g(c) (ξ ) f(c)(θ(c) )) t (K j) g(c) (ξ )+g(c) (ξ ) 2 f(c)(θ(c) )
− 2 t,0 0 −∇ t,0 − 2 − t,j j −1 2 t,j j − ∇ t,j
Xj=1 (cid:16) (cid:17)
η2 K −e1 e e
t (K j)2 f(c)(θ(c) ) f(c)(θ(c) ) jη 2f(c)(θ(c) )p(c)
− 2 − ∇ t,j −∇ t,0 − t ∇ t,0 t
j=1
X (cid:0) (cid:1)
η2
=(I)(c) t (II)(c) η2(III)(c),
− 2 − t
where by K j=− 11(K −j) = K(K 2−1) and K j=− 11(K −j)j = K3 6−K,
P (Kη )2 P (K3 K)η3
(I)(c) =θ(c) +Kη p(c) t f(c)(θ(c) ) − t 2f(c)(θ(c) )p(c)
t,0 t t − 2 ∇ t,0 − 6 ∇ t,0 t
K 1
− K j
(II)(c) =K(g(c) (ξ ) f(c)(θ(c) )) − g(c) (ξ )+g(c) (ξ ) 2 f(c)(θ(c) )
t,0 0 −∇ t,0 − 2 t,j j −21 t,j j − ∇ t,j
Xj=1 (cid:16) (cid:17)
K e1 e e
−
(III)(c) = (K j) f(c)(θ(c) ) f(c)(θ(c) ) jη 2f(c)(θ(c) )p(c)
− ∇ t,j −∇ t,0 − t ∇ t,0 t
j=1
X (cid:0) (cid:1)
Here we can interpret (I)(c) as the second-order approximation of θ(c) , (II)(c) as the random
t+1
error induced by stochastic gradient and (III)(c) as the numerical error brought by second-
order approximation.
Combining this with FL setting N w p(c) = p , we can rewrite
c=1 c t t
N P η2
w θ(c) θπ = (I) t (II) η2(III)+(IV),
c t+1 − t+1 − 2 − t
c=1
X
13where
N (Kη )2 (K3 K)η3
(I) = w θ(c) t f(c)(θ(c) ) − t 2f(c)(θ(c) )p(c)
c t,0 − 2 ∇ t,0 − 6 ∇ t,0 t
Xc=1 (cid:16) (cid:17)
(Kη )2 (Kη )3
θπ t f(θπ) t 2f(θπ)p ;
− t − 2 ∇ t − 6 ∇ t t
N (cid:16) K 1 (cid:17)
−
(II) = w K
g(c)
(ξ )
f(c)(θ(c)
) + (K k)
g(c)
(ξ
)+g(c)
(ξ ) 2
f(c)(θ(c)
) ;
c t,0 0 −∇ t,0 − t,k k −1 2 t,k k − ∇ t,k
Xc=1 (cid:16)
(cid:0) (cid:1)
Xk=1
(cid:0)
(cid:1)(cid:17)
N K 1
− e e e
(III) = w (K k)
f(c)(θ(c)
)
f(c)(θ(c)
)
2f(c)(θ(c)
)η
p(c)
k ;
c − ∇ t,k −∇ t,0 −∇ t,0 t t
Xc=1 Xk=1 (cid:16) (cid:17)
Kηt s (Kη )2 (Kη )3
(IV) = f(θπ(u))duds t f(θπ) t 2f(θπ)p .
∇ t − 2 ∇ t − 6 ∇ t t
Z0 Z0
Tointerprettheabovedecomposition, wecommentthat(I)isthenon-randomsecond-order
approximation of N w θ(c) θπ ; (II) is the random noise induced by the stochastic
c=1 c t+1 − t+1
gradients; (III) is the error yielded by the second-order approximation on the gradients;
P
(IV)canbeviewedasthedifferencebetweenthecontinuousprocessandthediscreteprocess
of the global HMC system.
By Lemma D.1, for any c ,c ,c > 0 with c = 1,
1 2 3 i i
N 1 η2 P 1 1
w θ(c) θπ 2 (I) t (II) 2 + η4 (III) 2 + (IV) 2. (25)
∥ c t+1 − t+1∥ ≤ c ∥ − 2 ∥ c t∥ ∥ c ∥ ∥
1 2 3
c=1
X
η2
In what follows, to show the goal, we will bound (I) t(II), (III) and (IV) separately.
− 2
Specifically, we claim that (I) η t2 (II) has a contraction relationship with N w θ(c) θπ
− 2 c=1 c t − t
plus some errors, and
P
E (III) 2 = O(K4(Kη t)C), E (IV) 2 = O((Kη t)C), for some constant C > 0.
∥ ∥ ∥ ∥
η2 (c) (c)
Let’s consider the term (I)
−
2t(II) first. Denote Eξt be the expectation over {(ξ
t,k
1,ξ t,k) :
−2
(c) (c)
c [N],k = 0,1,...,K 1 . Note that (II) is a mean-zero martingale w.r.t., (ξ ,ξ ) :
∈ − } { t,k 1 t,k
−2
c
∈
[N],k = 0,1,...,K
−
1
}
and Eξt(I) = (I). By definitions and taking conditional
14(c) (c)
expectation on (ξ ,ξ ) following order k = K 1,K 2,...,0, sequentially, we have
t,k 1 t,k − −
−2
η2 η4 N
Eξt∥(I)
−
2t (II) ∥2 = ∥(I) ∥2 + 4t w c2 K2Var ξt(g t( ,c 0) (ξ 0))
Xc=1 (cid:16)
K 1
− e
+ (K k)2 Var (g(c) (ξ ))+Var (g(c) (ξ ))
− ξt t,k k −1 2 ξt t,k k
Xk=1 (cid:0) (cid:1)(cid:17) (26)
η4 N K(2Ke2 +1) e
(I) 2 + t w2 σ2d
≤∥ ∥ 4 c 3 g
Xc=1 (cid:16) (cid:17)
(Kη )4 N
(I) 2 + t w2σ2d,
≤∥ ∥ 4K c g
c=1
X
where the first inequality is by variance Assumption 3.4 and K (K k)2 = (2K3 3K2+
k=1 − −
K)/6.
P
At the same time, by Lemma D.4, we have
E ∥(I) ∥2
≤
(1
−µ(K 2η t)2
)E ∥θ t −θ tπ ∥2+2(Kη t)6L2 T
t −1 N
w c(1 −w c)∆( ic) +
N
c=1
1w 8cσ t(c)2 d
(cid:0)
Xi=t0Xc=1 P
(cid:1)
(c)
where ∆ is defined in (13).
i
Next we consider (III). Due to Lemma D.1 and the fact that N c=1w c K k=− 11 (K(K 3− Kk) )k /6 =
1, thus −
P P
N K −1 6(K k)k K3 K 2
(III) 2 w − − f(c)(θ(c) ) f(c)(θ(c) ) 2f(c)(θ(c) )η p(c) k
∥ ∥ ≤ c K3 K 6k ∇ t,k −∇ t,0 −∇ t,0 t t
Xc=1 Xk=1 − (cid:13)
(cid:0)
(cid:1)(cid:13)
(cid:13) (cid:13)
N K −1 (K k)K3(cid:13) (cid:13)
w − f(c)(θ(c) ) f(c)(θ(c) ) 2f(c)(θ(c) )η p(c) k 2.
≤ c 6k ∥∇ t,k −∇ t,0 −∇ t,0 t t ∥
c=1 k=1
X X
(c)
Combining the above inequality with Lemma D.6 with δ defined in (9), we have
k
N K −1 (K k)K3 K4(Kη )4 N
E ∥(III) ∥2
≤
w c −
6k
(kη t)4L3δ k(c)
≤ 72
t L3 w cδ K(c) , (27)
c=1 k=1 c=1
X X X
where the last inequality is due to K −1(K k)k2 K4/12 and kδ(c) Kδ(c) .
k=1 − ≤ k ≤ K
Last, consider (IV). Note that (Kηt)2 f(θπ) + (Kηt)3 2f(θπ)p = Kηt s f(θπ) +
P 2 ∇ t 6 ∇ t t 0 0 ∇ t
2f(θπ)p u duds, we have
∇ t t R R (cid:0)
(cid:1) Kηt s
(IV) = f(θπ(u)) f(θπ) 2f(θπ)p u duds.
∇ t −∇ t −∇ t t
Z0 Z0
(cid:16) (cid:17)
15By direct calculations,
Kηt s 12u2 (Kη )4
E ∥(IV) ∥2
≤ (Kη
)4E
∥
12ut
2
( ∇f(θ tπ(u)) −∇f(θ tπ) −∇2f(θ tπ)p tu) ∥2duds
Z0 Z0 t
(Kη )4 Kηt s
t L3δπ u2duds (28)
≤ 12
Z0 Z0
(Kη )8
t L3δπ.
≤ 144
where the first inequality is by Lemma D.1 and note that
Kηt s 12u2
duds = 1; the
0 0 (Kηt)4
second inequality is by result (16) of Lemma D.9 with δπ defined in (17).
R R
Now combining (25)-(28) we get
1 µ(Kη )2 t −1 N
E ∥θ t+1 −θ tπ +1∥2
≤ c
(1
− 2
t )E ∥θ t −θ tπ ∥2 +2(Kη t)6L2T w c(1 −w c)∆( ic)
1
(cid:16) Xi=t0Xc=1
(Kη )6L2 N w σ(c)2d (Kη )4 N (Kη )8L3 N (Kη )8L3
+ t c=1 c + t w2σ2d + t w δ(c) + t δπ.
9 4K c g 72c c K 144c
P Xc=1 (cid:17) 2 Xc=1 3
Setting
µ(Kη )2/4 6 µ(Kη )2/4 5 µ(Kη )2/4
t t t
(c ,c ,c ) = 1 , , ,
1 2 3 − 1 µ(Kη )2/4 111 µ(Kη )2/4 111 µ(Kη )2/4
t t t
(cid:16) − − − (cid:17)
we have
1 µ(Kη )2 µ(Kη )2 1 15 1 22 1 44
t t
(1 ) = 1 , , , ,
c − 2 − 4 c ≤ 14 c ≤ 3µ(Kη )2 c ≤ 5µ(Kη )2
1 1 2 t 3 t
and
µ(Kη )2 t −1 N
E ∥θ t+1 −θ tπ +1∥2
≤
(1
− 4
t )E ∥θ t −θ tπ ∥2 +3(Kη t)6L2T w c(1 −w c)∆ i(c)
Xi=t0Xc=1
(Kη )6L2 N w σ(c)2d (Kη )4 N (Kη )6L3 N
+ t c=1 c + t w2σ2d+ t ( w δ(c) +δπ).
8 3K c g 9µ c K
P c=1 c=1
X X
Inserting the definitions of ∆(c) defined in (13), δ(c) defined in (9), and δπ defined in (17),
i k
and rearranging the coefficients yield the claim of this theorem.
H Proof of Lemmas D.3-D.10
H.1 Proof of Lemma D.3
¯
Lemma D.3. Under Assumption 3.2 consider two HMC systems θ(t) and θ(t) starting
with the same initial positions and different initial momentum p(0) and p(0) respectively.
16 eSuppose σ2 is the variance of the momentum, we have
1
θ(t) θ¯ (t) sinh( L/σ2t) p(0) p(0) ,
2 2
∥ − ∥ ≤ L/σ2 ∥ − ∥
p
p e
Further if we have Assumption 3.1, consider two HMC systems θ(t) and θ(t) starting with
initial positions θ(0) and θ(0), and initial momentum p(0) and p(0) respectively, then for
0 t 1 , we have e
≤ ≤ 2√L/σ2
e
e
µ
θ(t) θ(t) (1 t2) θ(0) θ(0) +Ct p(0) p(0) ,
∥ − ∥2 ≤ − 4σ2 ∥ − ∥2 ∥ − ∥2
where C is any constaent sinh(0.5). e
e
≥
Proof. ThislemmaisageneralizationofLemmaD.3inChenandVempala(2022). Consider
the first claim. By definition (1) and Lipschitz assumption 3.2,
d p(t) p¯(t) (p(t) p¯(t))T d(p(t) p¯(t)) L
2 ¯
∥ − ∥ = − − θ(t) θ(t) .
2
dt p(t) p¯(t) dt ≤ σ∥ − ∥
(cid:12) (cid:12) (cid:12)∥ − ∥2 (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
This implie(cid:12)s that (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
t L
¯
p(t) p¯(t) p(0) p¯(0) + θ(u) θ(u) du.
2 2 2
∥ − ∥ ≤ ∥ − ∥ σ∥ − ∥
Z0
Combining the above inequality with (1), it is easy to obtain that
t t s L
¯ ¯ ¯
θ(t) θ(t) θ(0) θ(0) + p(0) p¯(0) + θ(u) θ(u) duds.
∥ − ∥2 ≤ ∥ − ∥2 σ∥ − ∥2 σ2∥ − ∥2
Z0 Z0
Therefore,
¯
θ(t) θ(t) x(t),
2
∥ − ∥ ≤
where x(t) satisfies x(0) = θ(0) θ¯ (0) , x(0) = p(0) p¯(0) /σ and x (t) = Lx(t)
∥ − ∥2 ′ ∥ − ∥2 ′′ σ2
ByelementaryODE,thesolutionofthexprocessisx(t) = θ(0) θ¯ (0) cosh( L/σ2t)+
2
∥ − ∥
∥p( √0) − L/p¯ σ(0 2) ∥2 sinh( L/σ2t) and thus p
p
p(0) p¯(0)
θ(t) θ¯ (t) θ(0) θ¯ (0) cosh( L/σ2t)+ ∥ − ∥2 sinh( L/σ2t),
2 2
∥ − ∥ ≤ ∥ − ∥ L/σ2
p p
where the first term in the RHS is 0 since θ¯ (0) = θ(0). Thpis finishes the proof of the first
claim
Consider the second claim. Note that by Lemma 6 in Chen and Vempala (2022), we
immediately obtain
θ¯
(t) θ(t) (1
µt2
)
θ¯
(0) θ = (1
µt2
) θ(0) θ . Therefore
∥ − ∥2 ≤ −4σ2 ∥ − 0 ∥2 −4σ2 ∥ − 0 ∥2
it suffices to show that
¯
e θ(t) θ(t) Ct p(0)e p(0) . e
2 2
∥ − ∥ ≤ ∥ − ∥
which is implies by the first claim and elementary algebra that sinh( L/σ2t) C L/σ2t
≤
for all C > sinh(0.5) and 0 Lt/σ2 0.5. This concle udes the proof.
≤ ≤ p p
p
17H.2 Proof of Lemma D.4
Lemma D.4 (One-step update). Under the same conditions of Theorem 4.4. Denote
N (Kη )2 (K3 K)η3
(I) = w θ(c) t f(c)(θ(c) ) − t 2f(c)(θ(c) )p(c)
c t,0 − 2 ∇ t,0 − 6 ∇ t,0 t
Xc=1 (cid:16) (cid:17)
(Kη )2 (Kη )3
θπ t f(θπ) t 2f(θπ)p ;
− t − 2 ∇ t − 6 ∇ t t
(cid:16) (cid:17)
we have
E ∥(I) ∥2
≤
(1
−µ(K 2η t)2
)E ∥θ t −θ tπ ∥2+2(Kη t)6L2 T
t −1 N
w c(1 −w c)∆( ic) +
N c=1w 9cσ t(c)2 d
(cid:0)
Xi=t0Xc=1 P
(cid:1)
(c)
where ∆ is defined in (13).
i
Proof. Noting that p(c) N are mean-zero Gaussian and independent of θ(c) N ,
{ t }c=1 { t,0}c=1
E (I) 2 = E (I 1) 2 +0+E (I 2) 2, (29)
∥ ∥ ∥ ∥ ∥ ∥
where we denote
N (Kη )2 (Kη )2
(I ) = w θ(c) t f(c)(θ(c) ) θπ + t f(θπ),
1 c t,0 − 2 ∇ t,0 − t 2 ∇ t
Xc=1 (cid:16) (cid:17)
(K3 K)η3 N
(I ) = − t w 2f(c)(θ(c) ) 2f(c)(θπ) p(c) .
2 6 c ∇ t,0 −∇ t t
Xc=1 (cid:16) (cid:17)
Note that for (I ), by Lemma B.1 in Deng et al. (2021),
1
µ(Kη )2 N N
(I ) 2 (1 t ) w θ(c) θπ 2 +2(Kη )2L w θ(c) θ 2.
∥ 1 ∥ ≤ − 2 ∥ c t,0 − t∥ t c ∥ t,0 − t ∥
c=1 c=1
X X
Further, combining this with Lemma D.8 and θ = N w θ(c) , it follows that
t c=1 c t,0
µ(Kη )2 P t −1 N
E ∥(I 1) ∥2
≤
(1
− 2
t )E ∥θ t −θ tπ ∥2 +2(Kη t)2L2T (Kη i)4 w c(1 −w c)∆( ic) . (30)
Xi=t0 Xc=1
As for (I ), by direct calculations
2
N w (K3 K)η3 (Kη )3
E ∥(I 2) ∥2
≤
2c E ∥2 −
6
t ∇2f(c)(θ t(c) )p( tc) ∥2 +E ∥2 6t ∇2f(c)(θ tπ)p( tc) ∥2
Xc=1 (cid:16) (cid:17)
≤(K 1η 8t)6 N w c E ∥∇2f(c)(θ t(c) ) ∥2 F +E ∥∇2f(c)(θ tπ) ∥2 F σ t(c)2 (31)
Xc=1 (cid:16) (cid:17)
(Kη )6L2d
t (c)2
σ ,
≤ 9 t
18where the first inequality is due to Lemma D.1; the second inequality is by numerical bound
and that E Ap 2 = E A 2 for any matrix A independent of Gaussian vector p; the last
∥ ∥ ∥ ∥F
inequality is due to the facts that 2f( ) 2 d 2f( ) 2 and 2f( ) 2 L2 (ensured
∥∇ · ∥F ≤ ∥∇ · ∥ ∥∇ · ∥ ≤
by smoothness Assumption 3.2).
H.3 Proof of Lemma D.5
Lemma D.5 (0-th order Approximation Bound). Let q(0),p(0) Rd be two vectors that
∈
independent of {ξ k,ξ k −21 }k. For any F( ·) : Rd 7→ R satisfying smoothness Assumption 3.2
and variance Assumption 3.4 with parameter σ2, under step size assumption Kη 1/√L,
≤
the output q(k) := h(F( ),q(0),p(0),η,k) yield by Algorithm 3 with gradients randomized
·
by ξ ,ξ satisfies
{ k k −21 }k
Eξ q(k) q(0) 2 2(kη)2 p(0) 2 +(kη)4 F(q(0)) 2 +(kη)4σ2d (5)
∥ − ∥ ≤ ∥ ∥ ∥∇ ∥
Eξ F(q(k)) F(q(0)) 2 L2(2(kη)2 p(0) 2 +(kη)4 F(q(0)) 2 +(kη)4σ2d) (6)
∥∇ −∇ ∥ ≤ ∥ ∥ ∥∇ ∥
where Eξ[ ·] denotes the expectation over random variables {ξ k,ξ
k −21
}k.
Proof. For simplicity of notation, whenever it is clear from the context, for any k R, we
∈
misuse q := q(k), p := p(k), ξ := ξ(k) and
k k k
g (ξ ) = F(q ,ξ ), g (ξ ) = F(q ,ξ ).
k k −1 2 ∇ k k −1 2 k k ∇ k k
Recall that by definitions (4), we have
e e
e e
kη2 η2 k −1
q = q +kηp F(q ,ξ ) (k j) F(q ,ξ )+ F(q ,ξ ) .
k 0 0 − 2 ∇ 0 0 − 2 − ∇ j j −1 2 ∇ j j
Xj=1 (cid:16) (cid:17)
e e e
We prove (5) by induction. For k = 1, by simple algebra, we have that
η2 η2 η2
Eξ q 1 q 0 2 = Eξ ηp 0 F(q 0,ξ 0) 2 = ηp 0 F(q 0) 2 +0+( )22σ2d
∥ − ∥ ∥ − 2 ∇ ∥ ∥ − 2 ∇ ∥ 2
η2 η2
2 ηp 2 +2 F(q ) 2 +( e)22σ2d,
0 0
≤ ∥ ∥ ∥ 2 ∇ ∥ 2
where the first two quality follow the expectation calculations conditional on ξ first and
0
variance Assumption 3.4; the inequality is due to Lemma D.1.
Next, we assume that the claim (5) holds for j = 1,2,...,k 1. By iterative formula
−
(4), we can write
(kη)2 η2 k −1
q q = kηp F(q ) (I)+η2 (k j)( F(q ) F(q )) (32)
k 0 0 0 j 0
− − 2 ∇ − 2 − ∇ −∇
j=1
X
f
where
k 1
−
(I) = k( F(q ,ξ ) F(q ))+ (k j)( F(q ,ξ )+ F(q ,ξ ) 2 F(q ))
∇ 0 0 −∇ 0 − ∇ j j −21 ∇ j j − ∇ j
j=1
X
f e e e
19By direct calculations, for any c ,c ,c > 0 such that c = 1 we have
1 2 3 i i
Eξ q k q 0 2 P
∥ − ∥
1 η2 (kη)4 k −1 (k j)η4k3
Eξ kηp 0 (I) 2 + F(q 0) 2 + − F(q j) F(q 0) 2
≤c ∥ − 2 ∥ 4c ∥∇ ∥ 6jc ∥∇ −∇ ∥
1 2 3
j=1
X
1 f k3η4 (kη)4 k −1 (k j)η4k3
(kη)2 p 2 + σ2d + F(q ) 2 + − F(q ) F(q ) 2
0 0 j 0
≤c ∥ ∥ 4 4c ∥∇ ∥ 6jc ∥∇ −∇ ∥
1 2 3
(cid:16) (cid:17) Xj=1
1 k3η4 (kη)4 k −1 (k j)η4k3L2
(kη)2 p 2 + σ2d + F(q ) 2 + − q q 2
0 0 j 0
≤c ∥ ∥ 4 4c ∥∇ ∥ 6jc ∥ − ∥
1 2 3
(cid:16) (cid:17) Xj=1
1 k3η4 (kη)4 k −1 (k j)η4k3L2
(kη)2 p 2 + σ2d + F(q ) 2 + −
0 0
≤c ∥ ∥ 4 4c ∥∇ ∥ 6jc
1 2 3
(cid:16) (cid:17) Xj=1
(2(jη)2 p 2 +(jη)4 F(q ) 2 +j3η4σ2d)
0 0
· ∥ ∥ ∥∇ ∥
1 L2 2(kη)4 1 L2 (kη)4 1
( + )(kη)2 p 2 +( + )(kη)4 F(q ) 2 +(
0 0
≤ c 6c 6 ∥ ∥ 4c 6c 20 ∥∇ ∥ 4c
1 3 2 3 1
L2 (kη)4 (kη)4
+ ) σ2d
6c 12 k
3
where the first inequality is due to Lemma D.1 and noting that c +c +
k j=−11(k −j)j
c = 1;
1 2 (k3 k)/6 3
P −
the second inequality is by similar argument of (26); the third inequality is due to smooth-
ness Assumption 3.2 and numeric bound; the fourth inequality holds by the induction
assumption; the fifth inequality is by jk =− 11(k −j)j = k3 6−k
≤
k 63 , k j=− 11(k −j)j2
≤
k 124 and
k −1(k j)j3 = k5 k3 + 1 k k5 . The completion of the induction follows by step
j=1 − 20 − 12 20 ≤ 20P P
size assumption Kη 1/√L and setting (c ,c ,c ) = (3, 1, 1 , 1 ) in the last inequality.
P ≤ 1 2 3 5 3 15 24
Therefore, we conclude the proof of (5).
The claim (6) follows directly from (5) and smoothness Assumption 3.2.
H.4 Proof of Lemma D.6
Lemma D.6 (1-th order Approximation Bound). Under Assumptions 3.2-3.4, for any iter-
ation t and any local device c in Algorithm 3, we have
1 B(c) σ2
E ∥θ t( ,c k) −θ t(c) −η tp( tc) k ∥2
≤
3(kη t)4L L∇ +σ t(c)2 d+ Lkg d (7)
E ∥∇f(c)(θ t( ,c k) ) −∇f(c)(θ t(c) ) −∇2f(c)(cid:0)(θ t(c) )η tp t(c) k ∥2
≤
(kη t)(cid:1)4L3δ k(c) (8)
where B(c) := sup E f(c)(θ(c) ) 2 and for 1 k K
t ∥∇ t,0 ∥ ≤ ≤
∇
c L2 B(c) c L2 σ2d c L2
δ(c)
:= (1+
d Hσ(c)2
)
+(σ(c)2
+
d Hσ(c)4
)d+
g
(1+
d Hσ(c)2
). (9)
k 8L3 t L∇ t L3 t kL L3 t
20Proof. We first prove (7). By the decomposition in (32), we can write
(kη )2 η2 (c) k −1
θ(c) θ(c) η p(c) k = t f(c)(θ(c) ) t (I) η2 (k j) f(c)(θ(c) ) f(c)(θ(c) ) ,
t,k− t,0 − t t − 2 ∇ t,0 − 2 − t − ∇ t,j −∇ t,0
j=1
X (cid:0) (cid:1)
f
(c)
where (I) is the random noise induced by stochastic gradient
(cf) 1 k −1
(I) =
k(g(c)
(ξ )
f(c)(θ(c)
)) (k j)
g(c)
(ξ
)+g(c)
(ξ ) 2
f(c)(θ(c)
) .
t,0 0 −∇ t,0 − 2 − t,j j −1 2 t,j j − ∇ t,j
Xj=1 (cid:16) (cid:17)
f
e e e
Then by direct calculations, for any c ,c > 0 such that c +c = 1, we have
1 2 1 2
E ∥θ t( ,c k) −θ t( ,c 0) −η tp( tc) k ∥2
1 (kη )2 η2 (c) k −1 (k j)(k3 k)
E t f(c)(θ(c) ) t (I) 2 + − − E η2 f(c)(θ(c) ) f(c)(θ(c) ) 2
≤c ∥ 2 ∇ t,0 − 2 ∥ 6c j ∥ t ∇ t,j −∇ t,0 ∥
1 2
j=1
X (cid:0) (cid:1)
(kη )4 1 f k −1 (k j)k3η4
t (E f(c)(θ(c) ) 2 + σ2d)+ − t E f(c)(θ(c) ) f(c)(θ(c) ) 2
≤ 4c ∥∇ t,0 ∥ k g 6c j ∥∇ t,j −∇ t,0 ∥
1 2
j=1
X
where the first inequality is by Lemma D.1 and noting that c 1 + k j=− 11 (k( 3k − kj) )j /6c 2 = 1; the
second inequality is by similar argument as that in (26) and k3 k k3. −
−P≤
Further combining it with result (6) in Lemma D.5, we can upper bound the above
inequality as follows:
(kη )4 E f(c)(θ(c) ) 2 σ2d k −1 2(k j)
E ∥θ t( ,c k) −θ t( ,c 0) −kη tp( tc) ∥2
≤
4t ∥∇
c
t,0 ∥ + cg
k
+ 3c−
kj
L2(2(jη t)2 E ∥p( tc) ∥2
1 1 2
j=1
(cid:0) X
+(jη t)4 E ∥∇f(c)(θ t( ,c 0) ) ∥2 +j3η t4σ g2d)
(kη t)4 1
E f(c)(θ(c) ) 2 +
σ g2d
+
(cid:1)2L2 2(kη t)2σ t(c)2 d
≤ 4 c ∥∇ t,0 ∥ c k 3c 6
1 1 2
(kη(cid:16) )4 (kη )4 (cid:0)
+ t E f(c)(θ(c) ) 2 + t σ2d
20 ∥∇ t,0 ∥ 12k g
1
σ(cid:1)(cid:17)2d
≤3(kη t)4 E ∥∇f(c)(θ t( ,c 0) ) ∥2 +Lσ t(c)2 d+ kg
(cid:0) (cid:1)
where the second inequality is due to the facts k j=− 11(k −j)j = k3 6−k
≤
k 63 , k j=− 11(k −j)j2
≤
k4 and k −1(k j)j3 = k5 k3 + 1 k k5 ; the last inequality is due to the step size
12 j=1 − 20 − 12 30 ≤ 20P P
assumption Kη 1/√L and the choice (c ,c ) = (18, 1 ).
P ≤ 1 2 19 19
Next consider (8). By Lemma D.1,
3
f(c)(θ(c) ) f(c)(θ(c) ) 2f(c)(θ(c) )kη p(c) 2 = (b )+(b ) 2 3 (b ) 2+ (b ) 2, (33)
∥∇ t,k −∇ t,0 −∇ t,0 t t ∥ ∥ 1 2 ∥ ≤ ∥ 1 ∥ 2∥ 2 ∥
21where
1
(b ) = 2f(c) θ(c) +s(θ(c) θ(c) ) (θ(c) θ(c) kη p(c) )ds,
1 ∇ t,0 t,k − t,0 t,k − t,0 − t t
Z0
(cid:16) (cid:17)
1
(b ) = 2f(c) θ(c) +s(θ(c) θ(c) ) 2f(c)(θ(c) ) kη p(c) ds.
2 ∇ t,0 t,k − t,0 −∇ t,0 t t
Z0
(cid:16) (cid:16) (cid:17) (cid:17)
Consider (b ), by simple algebra,
1
1 2
(b ) 2 2f(c) θ(c) +s(θ(c) θ(c) ) (θ(c) θ(c) kη p(c) ) ds
∥ 1 ∥ ≤ ∇ t,0 t,k − t,0 t,k − t,0 − t t
Z0
(cid:13) (cid:13)
1(cid:13) (cid:0) (cid:1) (cid:13)
(cid:13)L2 θ(c) θ(c) kη p(c) 2ds (cid:13)
≤ ∥ t,k − t,0 − t t ∥
Z0
=L2 θ(c) θ(c) kη p(c) 2,
∥ t,k − t,0 − t t ∥
where the first inequality is due to Jensen’s inequality, and the second is implied by As-
sumption 3.2.
Combining this with (7) lead to
1 σ2d
E ∥(b 1) ∥2 ≤3L2(kη t)4 E ∥∇f(c)(θ t( ,c 0) ) ∥2 +Lσ t(c)2 d+ kg . (34)
(cid:16) (cid:17)
Similarly for (b ), we have that
2
1 2
E ∥(b 2) ∥2
≤
E ∇2f(c)(θ t( ,c 0) +s(θ t( ,c k) −θ t( ,c 0) )) −∇2f(c)(θ t( ,c 0) ) kη tp t(c) ds
Z0
(cid:13) (cid:13)
1 (cid:13)(cid:0) (cid:1) (cid:13)
≤
L2 H(cid:13)E ∥s(θ t( ,c k) −θ t( ,c 0) ) ∥2 ∥kη tp t(c) ∥2 ds (cid:13)
Z0 ∞
≤L 32 H E 2(kη t)2 ∥p( tc) ∥2 +(kη t)4 ∥∇f(c)(θ t( ,c 0) ) ∥2 +k3η t4σ g2d ∥kη tp t(c) ∥2 (35)
∞
L2 (cid:2)(cid:0) (cid:1) (cid:3)
≤
3H E 2(kη t)4c dσ(c)4d+(kη t)6 E ∥∇f(c)(θ t( ,c 0) ) ∥2c dσ t(c)2 +k5η t6σ g2c dσ t(c)2 d
2c h 1 1 i
≤
3d L2 H(kη t)4 σ(c)4d+ 8LE ∥∇f(c)(θ t( ,c 0) ) ∥2σ t(c)2 + 8Lkσ g2σ t(c)2 d
(cid:16) (cid:17)
where the first inequality is due to Jensen’s inequality; the second inequality is induced by
Hessian Assumption 3.3; the third inequality is due to Lemma D.5; the fourth equality is
by Lemma D.10; the last inequality is due to the step size assumption Kη 1/√L and
≤
d 1.
≥
Combining (33)-(35), we get (8). This finishes the proof.
H.5 Proof of Lemma D.7
Lemma D.7 (Uniform bound). Define θ = argmin f(θ). Under Assumptions 3.1-3.4 and
∗ θ
3.4, for any local device 1 c N in Algorithm 3 and any iteration t such that t
≤ ≤ ̸≡
220(mod T), we have
µ(Kη )2 2(Kη )6L4 (Kη )2L
E ∥θ t( +c) 1 −θ ∗ ∥2 ≤ (1 − 4 t + µt I {K ≥2 })E ∥θ t( ,c 0) −θ ∗ ∥2 + µt B a(c), (10)
where
2L 4 f(c)(θ ) 2 σ2
B(c) = d + ∥∇ ∗ ∥ + g d .
a µ 3µ 7Kµ
samplingcost localheterogeneity stochasticgradienteffect
Further if we choose (Kη )|2{z} µ ,|and f{ozr any}t such th|at{tz } 0(mod T), we set η =
t ≤ 4L2 ≡ t
η = = η
t+1 t+T 1
··· −
N
8
supE ∥θ t( ,c 0) −θ
∗
∥2
≤
D+ µ(B a(c) + w cB a(c)) (11)
t 0
≥ Xc=1
N
8
supE ∥∇f(c)(θ t( ,c 0) ) ∥2
≤
2L2(D+ µ(B a(c) + w cB a(c)))+2 ∥∇f(c)(θ ∗) ∥2, (12)
t 0
≥ Xc=1
where D = θ θ 2, representing the initialization effect.
0 ∗
∥ − ∥
Proof. We show (10), (11) and (12) sequentially. Recall that, by definitions (4),
Kη2 η2 K −1
θ(c) = θ(c) +Kη p(c) t g(c) (ξ ) t (K j) g(c) (ξ )+g(c) (ξ ) ,
t+1 t,0 t t − 2 t,0 0 − 2 − t,j j −21 t,j j
Xj=1 (cid:16) (cid:17)
e e e
hence, we can decompose
η2
θ(c) θ = (I) t (I)+(II),
t+1 − ∗ − 2
where
f
(Kη )2 (Kη )2
(I) =θ(c) +Kη p(c) t f(c)(θ(c) ) θ + t f(c)(θ ),
t,0 t t − 2 ∇ t,0 − ∗ 2 ∇ ∗
K 1
−
(I) =K
g(c)
(ξ )
f(c)(θ(c)
) + (K k)
g(c)
(ξ
)+g(c)
(ξ ) 2
f(c)(θ(c)
) ,
t,0 0 −∇ t,0 − t,k k −21 t,k k − ∇ t,k
k=1
(cid:0) (cid:1) X (cid:0) (cid:1)
f (Keη )2 K −1 e e
(II) = t f(c)(θ ) η2 (K k) f(c)(θ(c) ) f(c)(θ(c) ) .
2 ∇ ∗ − t − ∇ t,k −∇ t,0
Xk=1 (cid:16) (cid:17)
First, we prove inequality (10). By Lemma D.1, for any c ,c > 0 with c +c = 1,
1 2 1 2
1 η2 1
E ∥θ t( +c) 1 −θ ∗ ∥2 ≤ c E ∥(I) − 2t (I) ∥2 + c E ∥(II) ∥2. (36)
1 2
We first derive the bound on E (I) η t2 (I) 2. f
∥ − 2 ∥
f 23Similar to the argument as that in (26), we have
η2 K3η4
Eξ ∥(I)
−
2t (I) ∥2
≤
∥(I) ∥2 +
4
t σ g2d.
For (I), noting that p(c) is mean-zero Gfaussian and independent of θ(c) , we have
t t,0
(Kη )2 (Kη )2
E ∥(I) ∥2 =E ∥θ t( ,c 0)
−
2t ∇f(c)(θ t( ,c 0) ) −θ ∗ + 2t ∇f(c)(θ ∗) ∥2 +2 ∗0+(Kη t)2σ t(c)2 d
µ(Kη )2
≤(1
− 2
t )E ∥θ t( ,c 0) −θ
∗
∥2 +(Kη t)2σ t(c)2 d,
(37)
where the last inequality is due to Lemma B.1 in Deng et al. (2021) with only one local
device in the Federated Learning system.
On the other hand, given some c ,c ,c ,c > 0 with c +c = c +c = 1, we have
3 4 5 6 3 4 5 6
1 (Kη )2 1 K −1
E ∥(II) ∥2
≤c ∥
2t ∇f(c)(θ ∗) ∥2 +
c
E ∥η t2 (K −k) ∇f(c)(θ t( ,c k) ) −∇f(c)(θ t( ,c 0) ) ∥2
3 4
k=1
X (cid:0) (cid:1)
(Kη )4 η4 K −1 6(K k)k K3 K 2
≤
4ct ∥∇f(c)(θ ∗) ∥2 + ct K3−
K
E 6−
k
∇f(c)(θ t( ,c k) ) −∇f(c)(θ t( ,c 0) )
3 4
Xk=1 − (cid:13) (cid:16) (cid:17)(cid:13)
(cid:13) (cid:13)
(Kη )4 η4K3 K −1 K k (cid:13) (cid:13)
≤
4ct ∥∇f(c)(θ ∗) ∥2 + 6t
c
k− L2 2(kη t)2 E ∥p( tc) ∥2
3 4
Xk=1 (cid:16)
+(kη t)4 E ∥∇f(c)(θ t( ,c 0) ) ∥2 +k3η t4σ g2d
(Kη )4 η4K3 K −1 K(cid:17) k
≤
4ct ∥∇f(c)(θ ∗) ∥2 + 6t
c
k− L2 2(kη t)2 E ∥p( tc) ∥2
3 4
Xk=1 (cid:16)
1 1
+(kη t)4(
c
L2 E ∥θ t( ,c 0) −θ ∗ ∥2 +
c
∥∇f(c)(θ ∗) ∥2)+k3η t4σ g2d
5 6
1 (Kη )4L2 2(Kη )2L2 (cid:17)
≤(
4c
+
6
2t
0c c
)(Kη t)4 ∥∇f(c)(θ ∗) ∥2 +
6
t
6c
(Kη t)4 E ∥p( tc) ∥2
3 4 6 4
∗ ∗
(Kη )8L4 (Kη )8L2
+
6
2t
0c c
E ∥θ t( ,c 0) −θ ∗ ∥2 +
6
1t
2c
Kσ g2d
4 5 4
(Kη∗ )4 (Kη )4L∗ (K(cid:17) η )8L4 (Kη )4
t f(c)(θ ) 2 + t σ(c)2 d+ t θ(c) θ 2 + t σ2d,
≤ 3 ∥∇ ∗ ∥ 3 t 2 ∥ t,0 − ∗ ∥ 48K g
(38)
where the first two inequalities are due to Lemma D.1 and the fact that K −1(K k)k =
k=1 −
K(K2 1)/6; the third inequality is by Lemma D.5; the fourth inequality implied by that
− P
f(c)(θ(c) ) 2 1 f(c)(θ(c) ) f(c)(θ ) 2+ 1 f(c)(θ ) 2 (LemmaD.1)andsmoothness
∥∇ t,0 ∥ ≤ c5∥∇ t,0 −∇ ∗ ∥ c6∥∇ ∗ ∥
Assumption 3.2; the fifth inequality follows by the step size assumption Kη 1/√L, the
≤
fact that K −1(K k)k2 K4/12 and K −1(K k)k3 K5/20, and the choices that
k=1 − ≤ k=1 − ≤
(c ,c ) = (18, 1 ), (c ,c ) = (19, 41).
3 4 P19 19 5 6 60 60 P
24Combining (36)-(38) with (c ,c ) = (1
µ(Kηt)2
/(1
µ(Kηt)2
),
µ(Kηt)2
/(1
µ(Kηt)2
)) and
1 2 − 4 − 4 4 − 4
step size assumption Kη 1/√L, we have
≤
1 µ(Kη )2 µ(Kη )2 1 15 1 4
t t
(1 ) = (1 ), ,
c − 2 − 4 c ≤ 14 c ≤ µ(Kη )2
1 1 2 t
and
µ(Kη )2 2(Kη )6L4 15 15Kη2
E ∥θ t( +c) 1 −θ ∗ ∥2 ≤(1 − 4 t + µt )E ∥θ t( ,c 0) −θ ∗ ∥2 +(Kη t)2( 14σ t(c)2 d+ 14 4t σ g2d
4 f(c)(θ ) 2 2L 2 σ2
∗ (c)2 g
+ ∥∇ ∥ + σ d+ d).
3µ 3µ t 3Kµ
This holds for any iteration t such that t 0(mod T) and implies (10).
̸≡
Next we bound sup t ≥0E ∥θ t( ,c 0) −θ ∗ ∥2 (i.e., (11)), by the condition (Kη t)2 ≤ 4Lµ 2 we have
µ(Kη )2 2(Kη )6L4 µ(Kη )2
t t t
.
4 − µ ≥ 8
It follows that
µ(Kη )2
E ∥θ t( +c) 1 −θ ∗ ∥2 ≤ (1 − 8 t )E ∥θ t( ,c 0) −θ ∗ ∥2 +(Kη t)2B a(c). (39)
with
4 f(c)(θ ) 2 2L σ2
B(c) = ∥∇ ∗ ∥ + σ(c)2 d+ g d.
a 3µ µ t Kµ
Note that for any t / Z, θ(c) = θ(c) . Then apply inductions on (39), it follows that for
T ∈ t,0 t
s = 0,1,...
µ(Kη )2
E ∥θ (( sc +) 1)T,0 −θ ∗ ∥2 ≤(1 − 8sT )E ∥θ s(c T) +T 1,0 −θ ∗ ∥2 +(Kη sT)2B a(c)
−
≤···
µ(Kη )2 T −1 µ(Kη )2
≤(1 − 8sT )T −1 E ∥θ s(c T) +1,0 −θ ∗ ∥2 + (1 − 8sT )t′(Kη sT)2B a(c)
t=1
X′
µ(Kη )2 8 µ(Kη )2
≤(1
−
8sT )T E ∥θ sT −θ ∗ ∥2 +
µ
1 −(1
−
8sT )T B a(c)
(cid:16) (cid:17)
(c)
where we use the step size setting η
t
= η
t+1
=
···
= η
t+T −1
for any t
∈
TZ and θ
sT,0
= θ sT.
By Lemma D.1, it follows that
N N N
E ∥θ (s+1)T −θ ∗ ∥2 = E ∥ w cθ (( sc +) 1)T − w cθ ∗ ∥2 ≤ w cE ∥θ (( sc +) 1)T −θ ∗ ∥2
c=1 c=1 c=1
X X X
µ(Kη )2 8 µ(Kη )2 N
≤(1
−
8sT )T E ∥θ sT −θ ∗ ∥2 +
µ
1 −(1
−
8sT )T w cB a(c)
(cid:16) (cid:17)Xc=1
25Further by induction on s = 0,1,... and recall that we define D = E θ 0 θ ∗ 2
∥ − ∥
s µ(Kη )2 s s µ(Kη )2
E θ (s+1)T θ ∗ 2 (1 s ′T )TD+ (1 s ′′T )T
∥ − ∥ ≤ − 8 − 8
s=0 s=0s =s
Y′ X′ Y′′ ′
µ(Kη )2 8 N w B(c)
1 (1 s ′T )T c=1 c a
· − − 8 µ
P
(cid:16) (cid:17)
s µ(Kη )2 s µ(Kη )2 8 N w B(c)
= (1 s ′T )TD+ 1 (1 s ′T )T c=1 c a
− 8 − − 8 µ
s Y′=0 (cid:16) s Y′=0 (cid:17) P
N
8
D+ w B(c).
≤ µ c a
c=1
X
Combining the two induction procedures above, it is seen that for any s = 0,1,..., 0 t
≤ ≤
T 1
−
µ(Kη )2 8 N t −1 µ(Kη )2
E ∥θ sT+t −θ ∗ ∥2 ≤(1
−
8s ′T )t(D+
µ
w cB a(c))+ (1
−
8sT )t′(Kη sT)2B a(c)
c=1 t=1
X X′
N
8 8
D+ w B(c) + B(c).
≤ µ c a µ a
c=1
X
This finishes the proof of (11).
For (12), Lemma D.1 and smoothness Assumption 3.2 imply that
E ∥∇f(c)(θ t( ,c 0) ) ∥2
≤
2E ∥∇f(c)(θ t( ,c 0) ) −f(c)(θ ∗) ∥2+2 ∥f(c)(θ ∗) ∥2
≤
2L2 E ∥θ t −θ ∗ ∥2+2 ∥∇f(c)(θ ∗) ∥2.
Taking supremum over t 0 on both sides, then (12) directly follows by the above inequal-
≥
ity and (11).
H.6 Proof of Lemma D.8
Lemma D.8 (Local bound). Under Assumptions 3.2-3.4, for any iteration k and any local
device c in Algorithm 3, we have
N t 1 N
−
w cE ∥θ t(c) −θ t ∥2
≤
T (Kη i)4 w c(1 −w c)L∆( ic)
Xc=1 Xi=t0 Xc=1
where t is the latest communication step before t, and
0
5B(c) 1 1 N w σ(c)2 1
∆(c) := + σ(c)2 d+ σ2d+ i=1 c t − d (13)
i 9 L∇ 81 t 6KL g L(Kη )2
P i
with B(c) = sup E f(c)(θ(c) ) 2.
∇
t ≥0 ∥∇ t,0 ∥
26Proof. Denote t be the largest communication step before t. By definitions (4), we have
0
that
t −1 Kη2 η2 K −1
θ(c) = θ(c) + Kη p(c) i g(c) (ξ ) i (K k) g(c) (ξ )+g(c) (ξ ) .
t t0,0 i i − 2 i,0 0 − 2 − i,k k −21 i,k k
Xi=t0h Xk=1 (cid:16) (cid:17)i
e e e
To tackle the stochastic gradient and its dependence on stochastic gradient, we rewrite it
into
t −1 Kη2 η2 K −1
θ(c) =θ(c) + Kη p(c) i g(c) (ξ ) f(c)(θ(c) )+ f(c)(θ(c) ) i (K k)
t t0,0 i i − 2 i,0 0 −∇ i,0 ∇ i,0 − 2 −
Xi=t0 h
(cid:0) (cid:1)
Xk=1
g(c)
(ξ
)+g(c)
(ξ ) 2
fe(c)(θ(c)
)+2(
f(c)(θ(c)
)
f(c)(θ(c)
)+
f(c)(θ(c)
))
i,k k −1 2 i,k k − ∇ i,k ∇ i,k −∇ i,0 ∇ i,0
(cid:16) t 1 (cid:17)i
− (c)
=θ(ec) + (eKη )2(I)(c) +Kη (I)
t0,0 − i i i i
Xi=t0(cid:16) (cid:17)
f
where
K 1
1 − K k
(I)(c) = f(c)(θ(c) )+ − f(c)(θ(c) ) f(c)(θ(c) ) ,
i 2∇ i,0 K2 ∇ i,0 −∇ i,k
k=1
X (cid:0) (cid:1)
K 1
(c) η − K k
(I)
=p(c) i (g(c)
(ξ )
f(c)(θ(c)
))+ −
g(c)
(ξ
)+g(c)
(ξ ) 2
f(c)(θ(c)
) .
i i − 2 i,0 0 −∇ i,0 K i,k k −1 2 i,k k − ∇ i,k
k=1
(cid:2) X (cid:0) (cid:1)(cid:3)
f
e e e
(c)
Here is (I)(c) as the first-order approximation with non-stochastic gradient and (I) is the
i i
random error brought by stochastic gradient.
Denote f
N N
w 1 w 1 (s)
(I) = w ( s − )I s=c (I)(s), (I) = w ( s − )I s=c (I) .
i s w { } i i s w { } i
s s
s=1 s=1
X X
f f
Combining this with FL setting that θ(c) = θ = N w θ(s) , and that w N w =
t0,0 t0 s=1 s t0,0 c − s=1 s
N s=1w s(w ws − s1)I {s=c }, we can write P P
P
t 1
−
θ(c) θ = (Kη )2(I) +(Kη )(I) .
t,0 − t − i i i i
Xi=t0
(cid:0) (cid:1)
f
By Lemma D.1, we have for any c ,c > 0 such that c +c = 1
1 2 1 2
N N t
1 1
w cE ∥θ t( ,c 0) −θ t ∥2
≤
w c(t −t 0) (
c
E ∥(Kη i)2(I) i∥2 +
c
E ∥Kη i(I) i∥2).
1 2
Xc=1 Xc=1 Xi=t0
f
27Taking (c ,c ) = (4, 5), to prove the claim of the Lemma, it suffices to prove that
1 2 9 9
N N 5 Lσ(c)2 d σ2d
w cE ∥(I) i∥2
≤
w c(1 −w c)2 4E ∥∇f(c)(θ i( ,c 0) ) ∥2 + 3t
6
+ 288g
K
, (40)
c=1 c=1
X X (cid:0) (cid:1)
N N Kη2 N
w cE ∥(I) i∥2
≤
w c (1 −w c) 4i σ g2d+( w cσ t(c)2 −1)d . (41)
Xc=1 Xc=1 (cid:16) Xc=1 (cid:17)
f
We consider N c=1w cE ∥(I) i∥2 first since it is easier to bound. Similar to the argument as
that in (26), we have
P
f
N N N w 1 Kη2 N w 1
w cE ∥(I) i∥2
≤
w c w s( s w− )I {s=c } 2 4i σ g2d+E
∥
w s( s w− )I {s=c }p( is) ∥2
s s
Xc=1 Xc=1 (cid:16)Xs=1
(cid:0) (cid:1)
Xs=1 (cid:17)
f N Kη2 N
= w (1 w ) i σ2d+( w σ(c)2 1)d,
c − c 4 g c t −
c=1 c=1
X X
2
wherethefirstterminthesecondequalityisby N c=1w c N s=1 w s(w ws − s1)I {s=c } = N c=1w c(1 −
w c)andthesecondtermisbynormalityof {p i(c) }Pc andE[pP i(c1) p( i(cid:16)c2) ] = ρ id,fora(cid:17)nyc 1,Pc 2
∈
[N].
This proves (41).
Next, consider N c=1w cE ∥(I) i∥2. By direct calculations, we have
N P
w cE ∥(I)( ic) ∥2
c=1
X
N N
w 1 w 2(1 w )
≤ w c 2(1 s w )( − w s )I {s=c } E ∥ − 2 c ∇f(c)(θ i( ,c 0) ) ∥2
c s
c=1 s=1 −
X X (cid:0)
+K −1 6(K −k)k
E
2(1 −w c)(K3 −K)
(
f(s)(θ(s)
)
f(s)(θ(s)
))
2
K3 K 6kK2 ∇ i,0 −∇ i,k
k=1 −
X (cid:13) (cid:13) (cid:1)
N (cid:13) K 1 (cid:13)
− 2(K k)
≤
w c(1 −w c)2 E ∥∇f(c)(θ i( ,c 0) ) ∥2 + 3K−
k
E ∥∇f(c)(θ i( ,c 0) ) −∇f(c)(θ i( ,c k) ) ∥2
c=1 k=1
X (cid:0) X (cid:1)
N K 1
− 2(K k)
≤
w c(1 −w c)2 E ∥∇f(c)(θ i( ,c 0) ) ∥2 + 3K−
k
L2(2(kη i)2 E ∥p( ic) ∥2
c=1 k=1
X (cid:0) X
(kη )4
+(kη i)4 E ∥∇f(c)(θ i( ,c 0) ) ∥2 + ki σ g2d)
N 2L2(Kη )4 2L2(Kη )2 2L2(Kη )4
≤
w c(1 −w c)2 (1+
3
20i )E ∥∇f(c)(θ i( ,c 0) ) ∥2 +
3
6i σ t(c)2 d+
3
12Ki σ g2d
c=1 ∗ ∗ ∗
X (cid:0) (cid:1)
wherethefirstinequalityisbyLemmaD.1andnotingthat N s=1 2(1ws wc)(1 − ww ss)I {s=c } = 1and
−
K k=− 11 (K(K 3− Kk) )k /6 = 1; theequalityisbynotingthat N c=1w c PN s=1 2(1ws wc)(1 −w c)2(1 − ww ss)I {s=c } =
− −
P P P
28N w (1 w )2; the second inequality is by (6) in Lemma D.5; the third inequality is by
c=1 c − c
k −1(k j)j k3 , k −1(k j)j2 k4 and k −1(k j)j3 k5 . Now by assumption
Pj=1 − ≤ 6 j=1 − ≤ 12 j=1 − ≤ 20
Kη 1/√L and elementary numerical calculations, we get (40) and finish the proof.
P ≤ P P
H.7 Proof of Lemma D.9
Lemma D.9 (Oracle bounds). If f satisfies Assumptions 3.1-3.4, then for any iteration t
and any u Kη , we have
t
≤
L2d
E ∥θ tπ(u) −θ tπ −p tu ∥2
≤ 4µ
u4, (14)
1
E ∥θ tπ(u) −θ tπ ∥2 ∥p t ∥2
≤
3u2c dd, (15)
E ∥∇f(θ tπ(u)) −∇f(θ tπ) −∇2f(θ tπ)p tu ∥2
≤
δπu4, (16)
where c = 128+32log2(2d) and
d
5 L
δπ := +L2 c /L3 d. (17)
4 µ H d
(cid:0) (cid:1)
Proof. To prove (14), we notice that
u4 u s 2 2
E ∥θ tπ(u) −θ tπ −p tu ∥2 =
4
E u2∇f(θ tπ(r))drds
(cid:13)Z0 Z0
(cid:13)
u4 (cid:13) u s 2 2 (cid:13) u2 u s
E(cid:13) f(θπ(r)) dr(cid:13)ds = E f(θπ(r)) 2drds
≤ 4 u2 ∇ t 2 ∥∇ t ∥
Z0 Z0
(cid:13) (cid:13)
Z0 Z0
u2 u s (cid:13) (cid:13) L2d
≤ 2
L2 E ∥(cid:13)θ tπ(r) −θ ∗ ∥(cid:13)2drds
≤ 4µ
u4,
Z0 Z0
where the first inequality is due to Lemma D.1; the second inequality is by Assumption 3.2
and the fact that f(θ ) = 0; the third inequality is by Lemma D.2.
∗
∇
Next, we show (15) holds. By direct calculations
u 1
E ∥θ tπ(u) −θ tπ ∥2 ∥p t ∥2 ≤E ∥Cup t( 2) ∥2 ∥p t ∥2
≤
3u2c dd
∞ ∞
where the first inequality is by noting that θπ(u), θπ are positions under HMC system
t t H
at time u with the same initial θπ(u) but with opposite momentum p (u) and by Lemma
2 t 2 t 2
D.3; the second inequality is by Lemma D.10.
Next consider (16), by f(θπ(u)) = f(θπ)+ 1 2f(θπ+s(θπ(u) θπ))(θπ(u) θπ)ds
∇ t ∇ t 0 ∇ t t − t t − t
R 1
f(θπ(u)) f(θπ) 2f(θπ)p u = ((a)+(b))ds
∇ t −∇ t −∇ t t
Z0
where
(a) = 2f(θπ +s(θπ(u) θπ))(θπ(u) θπ p u),
∇ t t − t t − t − t
(b) = 2f(θπ +s(θπ(u) θπ)) 2f(θπ) p u.
∇ t t − t −∇ t t
(cid:16) (cid:17)
29By Lemma D.1, for any c ,c > 0 such that c +c = 1, we have
1 2 1 2
E ∥∇f(θ tπ(u)) −∇f(θ tπ) −∇2f(θ tπ)p tu ∥2
1 1 1
( E (a) 2 + E (b) 2)ds
≤ c ∥ ∥ c ∥ ∥
Z0 1 2
1 1 1
≤
(
c
L2 E ∥θ tπ(u) −θ tπ −p tu ∥2 +
c
L2 HE ∥s(θ tπ(u) −θ tπ) ∥2 ∥p tu ∥2 )ds
Z0 1 2 ∞
1 1 L4d 1 1
( u4σ4 + L2 s2 u2c du2)ds
≤ c 4µ ρt c H 3 d
Z0 1 2
5 L4
L3 +L2 c u4d
≤4 µ H d
(cid:0) (cid:1)
where in the second inequality, the first term is by Assumption 3.2 and the second term is
by Assumption 3.3; the third inequality is by (14) and (15) and the last inequality is by
direct calculations with (c ,c ) = (1, 4).
1 2 5 5
H.8 Proof of Lemma D.10
Lemma D.10 (Maximal Gaussian bounds). Suppose p 1,p
2
N(0,σ2 Id), then we have
∼
E p 1 2 c dσ2, E p 1 2 p 2 2 c ddσ4, E p 2 4 c dσ4,
∥ ∥ ≤ ∥ ∥ ∥ ∥ ≤ ∥ ∥ ≤
∞ ∞ ∞
with c = 128+32log2(2d).
d
Proof. First, we introduce some preliminary results, for any t > 0 and for any gaussian
vector p N(0,σ2 Id)
∼
t2
Borell–TIS inequality: P p E p t 2exp( ),
∥ ∥∞ − ∥ ∥∞ ≥ ≤ −2σ2
(cid:12) (cid:12)
(cid:0) (cid:1)
and (cid:12) (cid:12)
(cid:12) (cid:12)
E p 2log(2d)σ.
∥ ∥∞ ≤
Now, by direct calculations, p
t2
E[ p E p ]2 = ∞ 2tP p E p t dt ∞ 2t2exp( )dt = 4σ2,
∥ ∥∞ − ∥ ∥∞ ∥ ∥∞ − ∥ ∥∞ ≥ ≤ −2σ2
Z0
(cid:16)(cid:12) (cid:12) (cid:17)
Z0
E[ p E p ]4 = ∞ 4t3 P(cid:12) (cid:12) p E p (cid:12) (cid:12) t dt ∞ 4t32exp( t2 )dt = 16σ4.
∥ ∥∞ − ∥ ∥∞ ∥ ∥∞ − ∥ ∥∞ ≥ ≤ −2σ2
Z0
(cid:16)(cid:12) (cid:12) (cid:17)
Z0
(cid:12) (cid:12)
and noting that
(cid:12) (cid:12)
E p 2 =E2 p +E[ p E p ]2,
∥ ∥
∞
∥ ∥∞ ∥ ∥∞ − ∥ ∥∞
E p 4 =E[ p E p +E p ]4 8 E p 4 +E4 p ,
∥ ∥
∞
∥ ∥∞ − ∥ ∥∞ ∥ ∥∞ ≤ ∥ ∥
∞
∥ ∥∞
It follows that (cid:0) (cid:1)
E p 2 (2log(2d)+4)σ2, E p 4 128+32log2(2d) σ4.
∥ ∥ ≤ ∥ ∥ ≤
∞ ∞
(cid:0) (cid:1)
30Moreover, by Cauchy-Schwarz inequality
p 4 d p 4
E p 2 p 2 E ∥ ∥ + ∥ ∥ ∞ d+2+64d+16log2(2d)d σ4.
∥ ∥ ∥ ∥ ≤ 2d 2 ≤
∞
(cid:2) (cid:3) (cid:0) (cid:1)
Let c := 128+32log2(2d). Combining the above results, we finish the proof.
d
I Proof of Theorem C.1
We consider a special case. Without loss of generality, we assume N is even. The loss
function f(c)(θ) are as follow, for some constants C ,C > 0
1 2
f(c)(θ) =
L ∥θ −θ L∗Id ∥2/2+C 1, c = 1,2,..., N
2
(cid:26)
µ ∥θ −θ µ∗Id ∥2/2+C 2, c = N
2
+1,...,N.
and weights are
1
w = , c = 1,2,...,N
c
N
Then we have the following result on the tightness of our analysis on the upper bound.
Theorem I.1 (DimensionaltightlowerboundforidealHMCprocess). Under Assumptions
3.1-3.3, if we initialize θ
0
∈
N(0,σ2 Id) for any σ2 > 0, suppose θ
L∗
> θ
µ∗
> 0, then we have
1 γt1 (1 cos2(√LKη))
θ =θ γt + − ( −
t 0
1 γ 2 √L
−
(1 cos2(√µKη)) 1 γ2t
+ −
√µ
)Id +γ s1−
γ2
p
−
where p
∼
N(0,Id), γ = 1
2
cosT(√LKη)+cosT(√µKη) .
As a result, the least t for (θ ,θπ)2 ϵ2 is taken when
t
(cid:0) W ≤ (cid:1)
√dT log(d/ϵ)
t = Ω( ).
ϵ
By definitions and elementary calculus, a parameter q that follows HMC system :
0
H
f(q) = L q q 2 with length η := Kη with momentum p will give
2∥ − ∗ ∥ 0
p
0
Kη(q 0) = q 0cos(e Lη)+q
∗
1 cos( Lη)e+ sin( Lη)
H − √L
p (cid:0) p (cid:1) e p
By induction, a parameter q that follows HMC system with independent momentum
0 e e e
H
p , t = 0,1,...T 1 will give
t
−
T 1
− p
e T (q ) =q cosT( Lη)+q 1 cosT( Lη) + t sin( Lη)cosT t 1( Lη)
HKη 0 0 ∗ − √L − −
t=0
p (cid:0) p (cid:1) X e p p
e e 1 cos2T( Leη) 1/2 e
=q cosT( Lη)+q 1 cosT( Lη) +p −
0 ∗ 0
− √L
(cid:0) p (cid:1)
p (cid:0) p (cid:1) e
e e
31where the last equality is by the Gaussian property that the sum of T Gaussian’s is still a
Gaussian random variable.
Let γ = 1 cosT( Lη)+cosT( µη) be the contraction factor and θ = 1 θ 1
2 η∗ 2(1 γ) L∗ −
−
cosT( Lη) +(cid:0) θ 1pcosT( µη)p. It(cid:1)follows that for FA continuous HMC, we h(cid:16) ave(cid:0)
µ∗
− e e
e
p 1(cid:1) (cid:0) p (cid:1)(cid:17) 1
θ =θe cosT( Lη)+cosT(e µη) + θ 1 cosT( Lη) +θ 1 cosT( µη)
t+1 t
2 2
L∗
−
µ∗
−
(cid:0) 1p 1 cos2T( p Lη) 1 2(cid:1) 1(cid:16) (cid:0) cos2T( µηp ) 1 2 (cid:1) (cid:0) p (cid:1)(cid:17)
Id +p t e− e + − e e
· 2 √L √µ
(cid:0) p (cid:1) (cid:0) p (cid:1)
(cid:16) (cid:17)
e 1 e 1
1 1 cos2T( Lη) 2 1 cos2T( µη) 2
=θ tγ +θ η∗(1 −γ)Id +p
t
2
−
√L
+ −
√µ
(cid:0) p (cid:1) (cid:0) p (cid:1)
(cid:16) (cid:17)
= e e e
···
t −1 1 1 cos2T( Lη) 1 2 1 cos2T( µη) 21
=θ 0γt +θ η∗(1 −γt)Id + γip
i
2
−
√L
+ −
√µ
Xi=0 (cid:16)(cid:0) p
e
(cid:1) (cid:0) p
e
(cid:1) (cid:17)
e 1 1
1 γ2t1 1 cos2T( Lη) 2 1 cos2T( µη) 2
=θ 0γt +θ η∗(1 −γt)Id +p s1−
−γ2 2
(cid:16)(cid:0)
−
√L
p (cid:1)
+
(cid:0)
−
√µ
p (cid:1)
(cid:17)
e e
e
whereby the property of Gaussian, we combine t Gaussians into one Gaussian p. This
proves the first claim for t being the multiples of T.
By the closed form solution (N(µ ,Σ ),N(µ ,Σ ))2 = µ µ 2 + tr Σ + Σ
1 1 2 2 1 2 1 2
W ∥ − ∥ −
2(Σ 221 Σ 2Σ 221 )1
2
≥
∥µ
1
−µ
2
∥2, and recall that we initialize θ
0
∼
N(0,σ2 Id), we g (cid:0)et
W(θ t,θ(cid:1)π)2
≥
∥(θ η∗(1 −γt) −θ ∗)Id ∥2 = γ2t(θ η∗)2d −2γtθ η∗(θ
η∗
−θ ∗)d+(θ
η∗
−θ ∗)2d,
where θ
∗
= Lθ LL∗ ++ µµθ µ∗ ise the global minimum of tehe FA-HMeC esystem (Thise can be easily
checked).
We claim that by the conditions L µ and θ θ > 0, we have 0 < θ θ , which
≥
L∗
≥
µ∗ η∗
≤
∗
will be proved after the main proof. Then
e
(θ ,θπ)2 (θ )2γ2td+(θ θ )2d,
W
t
≥
η∗ η∗
−
∗
At the same time, by elementary calculus, for η = Kη 1 , there exist C ,C such that
e e ≤ √L 1 2
T(L+µ) L2θ +µ2θ
γ 1 C η2, (θ eθ )2 C T2( L∗ µ∗ )2η4.
≥ −
1
2
η∗
−
∗
≥
2
L+µ
We have e
e e
T(L+µ) L2θ +µ2θ
(θ ,θπ)2 (θ )2(1 C η2)2td+C T2( L∗ µ∗ )2η4d,
W
t
≥
η∗
−
1
2
2
L+µ
Let (θ ,θπ)2 ϵ, the leaest t/T on the RHS is taken when
t e e
W ≤
√dlog(d/ϵ)
t/T = Ω( ).
ϵ
32Last, recall that
θ 1 cosT( Lη) +θ 1 cosT( µη) θ 1 cosT( Lη) +θ 1 cosT( µη)
θ =
L∗
−
µ∗
− =
L∗
−
µ∗
−
η∗
(cid:0) p 2( (cid:1)1 γ) (cid:0) p (cid:1) (cid:0) 2 cospT( L (cid:1)η) c (cid:0)osT( µη)p (cid:1)
− − −
e e e e
e
To show θ θ , it suffices to show that for any a,b > 0 such thatpa/(a+b) Lp/(L+µ)
η∗
≤
∗
e ≤ e
we have
e
a b L µ 1 cosT( Lη) L
θ + θ θ + θ , and −
a+b
L∗
a+b
µ∗
≤ L+µ
L∗
L+µ
µ∗
2 cosT( Lη) cosT( µη) ≤ L+µ
p
− −
e
The first inequality is implied by θ θ . It is left to show thepsecond inequaplity, which is
L∗
≥
µ∗
e e
equivalent to show
µ 1 cosT( Lη)
− 1
L 1 cosT( µη) ≤
(cid:0) p (cid:1)
−
e
Note that (cid:0) p (cid:1)
e
1 cosT( Lη) 1 cos( Lη) T −1cos( Lη)i 1 cos( Lη)
− = − i=0 − 1
1 −cosT( pµη) 1 −cos( pµη) PT i=− 01cos( pµη)i ≤ 1 −cos( pµη) ·
e e e e
It suffices to show fpor all η 1 p P p p
e ≤ 2√L e e e
µ 1 cos( Lη)
− 1, or µ 1 cos( Lη) L 1 cos( µη)
L 1 cos( µη) ≤ − ≤ −
(cid:0) p (cid:1)
− e (cid:0) p (cid:0) p (cid:1)
Note that
b(cid:0)
y
elemenp
tary
c(cid:1)
alculus, g(η) := µ 1
cos(√Lη)e
L 1 cos(√µη)
se
atisfies that
e − − −
g(0) = 0, g (0), g (η) = Lµ cos(√Lη) cos(√µη) 0, for all η 1 , which implies
′ ′′ − (cid:0) ≤ (cid:0) ≤ 2√L (cid:1)
that g(η) 0 for all η 1 and proves the above inequality. This finishes the proof.
≤ ≤ 2√L(cid:0) (cid:1)
J Proof of Proposition 4.5
We show a more general version of Proposition 4.5 as follow
Proposition J.1 (Dynamic stepsize). Suppose for the θ yielded by Algorithm 3, we
t t
{ }
have that for some C > 1,
µ(Kη )2
E ∥θ t+1 −θ tπ +1∥2
≤
(1
− 4
t )tD+C(Kη t)2c d∆
with e
N
∆ = d(T2(γ +(1 ρ)N)+ w2σ2/K).
− c g
c=1
X
If we set e
D log(8)
(Kη)2 = , t = − T
8c ∆L 1 ⌈T log(1 µ(Kη)2/4)⌉
d −
33
eand
η = η, for 0 t t 1.
t 1
≤ ≤ −
and for j = 2,3,... and for s = 1,2,
···
η
t
t = 2t ,η = ′ ,
s+1 s t
′′ √2
for any s j=− 11t j
≤
t ′
≤
s j=1t j −1, s j=1t j
≤
t ′′
≤
s j=+ 11t j −1. then we have
P P P P
Cc ∆
E ∥θ t −θ tπ ∥2 ≤ ϵ2, at some t ≤ ϵd 2 .
e
Proof. The procedure follows the proof of the Theorem 14 in Cheng et al. (2018). For
completeness, we give the proof of a stronger result below, for any a > 0
4log(2a)c ∆ D
E ∥θ t −θ tπ ∥2
≤
ϵ2, at t =
µ
Dd ( ϵ2)log a(2). (42)
e
if we change to
D log(2a)
(Kη)2 = , t = − T and η = η, for 0 t t 1.
2ac ∆L 1 ⌈T log(1 µ(Kη)2/4)⌉ t ≤ ≤ 1 −
d −
and for s = 1,2,
e ···
s 1 s s s+1
η −
t
t = 2t ,η = ′ , for any t t t 1, t t t 1.
s+1 s t ′′ a1/4 j ≤ ′ ≤ j − j ≤ ′′ ≤ j −
j=1 j=1 j=1 j=1
X X X X
Then the claim of this proposition follows by setting a = 2.
Note that by the definitions of t and η assumptions,
j j
(1 µ(Kη tj)2 )tj =(1 µ(Kη t1)2 )2j −1t1 (1 µ(Kη t1)2 )2j −1 log(1−l µo (g K(2 ηa )) 2/4)
− 4 − 4 2j 1 ≤ − 4 2j 1 −
− −
· ·
1 2j+1log(1 µ(Kη)2/2j+1) 1
=( ) log(1− µ(Kη)2/4) ,
2a − ≤ 2a
where the first inequality is by inserting the definition of t ; the last inequality is by
1
ylog(1 x/y)/log(1 x) 1 for any x > 0,y 1.
− − ≥ ≥
Based on this, (let t(s) = s t ), it is seen that
j=1 j
P 1
E ∥θ t(s) −θ tπ (s)∥2 ≤2aE ∥θ t(s −1) −θ tπ (s −1)∥2 +(Kη ts)4Lc d∆
1 D
= 2aE ∥θ t(s −1) −θ tπ (s −1)∥2 + 2as e
≤···
s 1
1 − D D D
( )sD+ + = .
≤ 2a (2a)j 2s j 2as as
−
j=1 ·
X
34Choosing s such that D > ϵ2, D ϵ2, then we get
as as+1 ≤
s s 1
− log(2a)
E ∥θ t(s) −θ tπ (s)∥2
≤
ϵ2, t(s) = t j = t 1
·
2j
≤
t 1 ·2s
≤
2
·
log(1− µ(Kη)2/4)2log a(D/ϵ2).
s=1 j=0 −
X X
Further by log(1 x) x for any x < 1, and (Kη)2 = D
− − ≥ 2acd∆L
8log(2a) 32log(2a)aec ∆LD
t(s) − 2log a(D/ϵ2) = d .
≤ µ(Kη)2 µ D ϵ2
e
This is equivalent to (42).
K Relaxation of Variance of Stochastic Gradients
If we relax
Assumption 3.4 (σ -Bounded Variance). For local device c = 1,2,...,N, and leapfrog
g
step k = 1,2,...,K, t = 1,2,..., we have max tr(Var(
f(c)(θ(c) ,ξ(c)
)
θ(c)
))
σ2Ld, for some σ > 0.
x=k −1/2,k ∇ t,k t,x | t,k ≤
g g
e
to assumption:
x=m kax
1,k
Eξ ∥∇f(c)(θ t( ,c k) ,ξ t( ,c x) ) −∇f(c)(θ t( ,c k) ) ∥2
≤
σ g2 Eξ ∥∇f(c)(θ t( ,c k) ) ∥2 +d ,
−2 n o
(cid:2) (cid:3)
similar results w.r.t. dimeensionality will still hold as long as we can show that
E f(c)(θ(c) ) 2 CLd, for some C. (43)
∥∇ t,k ∥ ≤
The above goal is equivalent to generalizing the results of Lemma D.7 to that adapted to
(43) and Lemma D.5 which is used in the proof of Lemma D.7.
For Lemma D.5 with the weaker Assumption 3.4, if we repeat the procedure in Section
H.3, we can get (for some constants C ,C ,C > 0
1 2 3
σ2
Eξ ∥q(k) −q(0) ∥2
≤
C 1(kη)2 ∥p(0) ∥2 +C 2(kη)4(1+ Kg ) ∥∇F(q(0)) ∥2 +C 3(kη)4σ g2d
σ2
Eξ ∥∇F(q(k)) −∇F(q(0)) ∥2
≤
L2(C 1(kη)2 ∥p(0) ∥2 +C 2(kη)4(1+ Kg ) ∥∇F(q(0)) ∥2 +C 3(kη)4σ g2d)
The difference between this result and Lemma D.5 is that the coefficient of the term
F(q(0)) 2 is related to σ2. This won’t affect the use of Lemma D.5 as long as we
|∇ ∥ g
have σ2 = O(K) which is a mild assumption.
g
Further base on this, if we repeat the procedure of the proof of Lemma D.7 in Section
H.3, we can get (for some constants C ,C ,C ,C > 0)
4 5 6 7
µ(Kη )2 σ2 (Kη )6L4 (Kη )2L
E ∥θ t( +c) 1 −θ ∗ ∥2 ≤ (1 − 4 t +C 4(1+ Kg ) µt I {K ≥2 })E ∥θ t( ,c 0) −θ ∗ ∥2 + µt B a(c),
e
35where
L σ2 f(c)(θ ) 2 σ2
B(c) =C d+C (1+ g )∥∇ ∗ ∥ +C g d.
a 5 µ 6 K µ 7 Kµ
As long as we have σe2 = O(K), then there is not much significant difference between this
g
result and Lemma D.7.
L Supplementary Figures to Main Text
For a robust comparison, we add standard deviation information to some figures in the
main text, based on multiple runs. Specifically, we present the shaded error: [average-
standard deviation, average+standard deviation] based on independent runs and add them
to the figures.
In Figures 1(a) - 1(d), we add shaded error based on 5 independent runs to Figures 3(a)
- 3(b).
In Figures 2(a) - 2(d), we add shaded error based on 5 independent runs to Figures 4(a)
- 4(b).
0.847 K=1 K=1 K=1
K=10 0.1050 K=10 14300 K=10
K=50 K=50 K=50 0.318 K=100 K=100 K=100
0.838 0.0366 5300
K=1 0.228 0.0150
0.748 K=10
K=50 4400
0.0082
K=100
0.219
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
Communication rounds Communication rounds Communication rounds Communication rounds
(a) Accuracy (b) BS (c) ECE (d) NLL
Figure 1: The impact of leapfrog steps K on FA-HMC applied on the Fashion-MNIST
dataset. The shaded error represents the standard deviation based on 5 independence
runs.
0.8469 T=1 T=1 T=1
T=10 T=10 14300 T=10
0.8460 T=20 0.1050 T=20 T=20 0.318 T=50 T=50 7462 T=50
T=100 T=100 T=100
0.8370 0.0366 5300
T=1
0.228 4616 T=10
0.7470 T=20 0.0150
4400
T=50
T=100
0.219 0.0082 4332
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
Communication rounds Communication rounds Communication rounds Communication rounds
(a) Accuracy (b) BS (c) ECE (d) NLL
Figure 2: The impact of local steps T on FA-HMC applied on the Fashion-MNIST dataset.
The shaded error represents the standard deviation based on 5 independence runs
36
ycaruccA
ycaruccA
)SB(
erocs
reirB
)SB(
erocs
reirB
)ECE(
rorre
noitarbilac
detcepxE
)ECE(
rorre
noitarbilac
detcepxE
)LLN(
doohilekil
gol
evitageN
)LLN(
doohilekil
gol
evitageNReferences
Ahn, S., Korattikara, A., and Welling, M. (2012), “Bayesian Posterior Sampling via
Stochastic Gradient Fisher Scoring,” in International Conference on Machine Learning
(ICML).
Ahn, S., Shahbaba, B., and Welling, M. (2014), “Distributed Stochastic Gradient MCMC,”
in International Conference on Machine Learning (ICML).
Al-Shedivat, M., Gillenwater, J., Xing, E., andRostamizadeh, A.(2021), “FederatedLearn-
ing via Posterior Averaging: A New Perspective and Practical Algorithms,” in Interna-
tional Conference on Learning Representation (ICLR).
Bardenet, R., Doucet, A., andHolmes, C.(2017), “OnMarkovChainMonteCarloMethods
for Tall Data,” Journal of Machine Learning Research, 18, 1–43.
Betancourt, M. (2015), “The Fundamental Incompatibility of Scalable Hamiltonian Monte
Carlo and Naive Data Subsampling,” in International Conference on Machine Learning
(ICML).
Bou-Rabee, N., Eberle, A., and Zimmer, R. (2020), “Coupling and convergence for Hamil-
tonian Monte Carlo,” Annals of Applied Probability, 30, 1209–1250.
Bugra Can, Mert Gurbuzbalaban, L. Z. (2019), “Accelerated Linear Convergence of
Stochastic Momentum Methods in Wasserstein Distances,” in International Conference
on Machine Learning (ICML).
Chen, C., Ding, N., and Carin, L. (2015), “On the Convergence of Stochastic Gradient
MCMC Algorithms with High-order Integrators,” in Advances in Neural Information
Processing Systems (NeurIPS).
Chen, C., Ding, N., Li, C., Zhang, Y., and Carin, L. (2016), “Stochastic Gradient MCMC
withStaleGradients,”inAdvances in Neural Information Processing Systems (NeurIPS).
Chen, H.-Y. and Chao, W.-L. (2021), “FedBE: Making Bayesian Model Ensemble Appli-
cable to Federated Learning,” in International Conference on Learning Representations
(ICLR).
Chen, T., Fox, E., and Guestrin, C. (2014), “Stochastic Gradient Hamiltonian Monte
Carlo,” in International Conference on Machine Learning (ICML).
Chen, Y., Dwivedi, R., Wainwright, M.J., andYu, B.(2020), “FastMixingofMetropolized
Hamiltonian Monte Carlo: Benefits of Multi-step Gradients.” Journal of Machine Learn-
ing Research, 21, 92–1.
Chen, Z. and Vempala, S. S. (2022), “Optimal convergence rate of hamiltonian monte carlo
for strongly logconcave distributions,” Theory of Computing, 18, 1–18.
37Cheng, X., Chatterji, N. S., Bartlett, P. L., and Jordan, M. I. (2018), “Underdamped
Langevin MCMC: A Non-asymptotic Analysis,” in Conference on Learning Theory
(COLT).
Chowdhury, A.andJermaine, C.(2018), “ParallelandDistributedMCMCviaShepherding
Distributions,” in International Conference on Artificial Intelligence and Statistics.
Dang, K.-D., Quiroz, M., Kohn, R., Tran, M.-N., and Villani, M. (2019), “Hamiltonian
Monte Carlo with Energy Conserving Subsampling,” Journal of Machine Learning Re-
search, 1–31.
Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M., Senior,
A., Tucker, P., Yang, K., et al. (2012), “Large Scale Distributed Deep Networks,” in
Advances in neural information processing systems (NeurIPS).
Deng, W., Feng, Q., Gao, L., Liang, F., and Lin, G. (2020), “Non-Convex Learning via
Replica Exchange Stochastic Gradient MCMC,” in International Conference on Machine
Learning (ICML).
Deng,W.,Liang,S.,Hao,B.,Lin,G.,andLiang,F.(2022),“InteractingContourStochastic
Gradient Langevin Dynamics,” in International Conference on Learning Representation
(ICLR).
Deng, W., Ma, Y.-A., Song, Z., Zhang, Q., and Lin, G. (2021), “On Convergence of
Federated Averaging Langevin Dynamics,” arXiv preprint arXiv:2112.05120.
´
Durmus, A., Eric Moulines, and Saksman, E. (2020), “Irreducibility and Geometric Ergod-
icity of Hamiltonian Monte Carlo,” Annals of Statistics, 48, 3545–3564.
Durmus, A. and Moulines, E. (2019), “High-dimensional Bayesian Inference via The Un-
adjusted Langevin Algorithm,” Bernoulli, 25, 2854–2882.
Flammarion, N. and Bach, F. (2015), “From Averaging to Acceleration, There is Only a
Step-size ,” in Proc. of Conference on Learning Theory (COLT).
Gu¨rbu¨zbalaban, M., Gao, X., Hu, Y., and Zhu, L. (2021), “Decentralized Stochastic Gra-
dient Langevin Dynamics and Hamiltonian Monte Carlo,” Journal of Machine Learning
Research, 22, 1–69.
Hoffman, M. D. and Gelman, A. (2014), “The No-U-Turn Sampler: Adaptively Setting
Path Lengths in Hamiltonian Monte Carlo,” Journal of Machine Learning Research,
1593–1623.
Huang, B., Li, X., Song, Z., and Yang, X. (2021), “FL-NTK: A Neural Tangent Kernel-
based Framework for Federated Learning Convergence Analysis,” in International Con-
ference on Machine Learning (ICML).
38Karimireddy, S. P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh, A. T. (2020),
“Scaffold: Stochastic Controlled Averaging for Federated Learning,” in International
Conference on Machine Learning (ICML).
Lee, Y. T., Song, Z., and Vempala, S. S. (2018), “Algorithmic Theory of ODEs and Sam-
pling from Well-conditioned Logconcave Densities,” ArXiv:1812.06243.
Li, C., Chen, C., Pu, Y., Henao, R., and Carin, L. (2019), “Communication-Efficient
Stochastic Gradient MCMC for Neural Networks,” in Proc. of the National Conference
on Artificial Intelligence (AAAI).
Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A., and Smithy, V. (2020a),
“Federated Optimization in Heterogeneous Networks,” in Proceedings of the 3rd MLSys
Conference.
Li, X., Huang, K., Yang, W., Wang, S., and Zhang, Z. (2020b), “On the Convergence
of FedAvg on Non-IID Data,” in International Conference on Learning Representation
(ICLR).
Li, X., Jiang, M., Zhang, X., Kamp, M., and Dou, Q. (2021), “FedBN: Federated Learn-
ing on Non-IID Features via Local Batch Normalization,” in International Confer-
ence on Learning Representations (ICLR), URL https://openreview.net/forum?id=
6YEQUn0QICG.
Mangoubi,O.andSmith,A.(2017),“RapidmixingofHamiltonianMonteCarloonstrongly
log-concave distributions,” arXiv preprint arXiv:1708.07114.
Mangoubi, O. and Vishnoi, N. K. (2018), “Dimensionally Tight Running Time Bounds for
Second-order Hamiltonian Monte Carlo,” in Advances in Neural Information Processing
Systems (NeurIPS).
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y Arcas, B. A. (2017),
“Communication-efficient Learning of Deep Networks from Decentralized Data,” in Proc.
of the International Conference on Artificial Intelligence and Statistics (AISTATS).
McMahan, H. B., Moore, E., Ramage, D., and y Arcas, B. A. (2016), “Federated learning
of deep networks using model averaging,” arXiv preprint arXiv:1602.05629, 2.
Mekkaoui, K. e., Mesquita, D., Blomstedt, P., and Kaski, S. (2021), “Federated Stochastic
Gradient Langevin Dynamics,” in Proc. of the Conference on Uncertainty in Artificial
Intelligence (UAI).
Minsker, S., Srivastava, S., Lin, L., and Dunson, D. B. (2014), “Scalable and Robust
Bayesian Inference via the Median Posterior,” in International Conference on Machine
Learning (ICML).
Mou, W., Ma, Y.-A., Wainwright, M. J., Bartlett, P. L., and Jordan, M. I. (2021), “High-
Order Langevin Diffusion Yields an Accelerated MCMC Algorithm,” Journal of Machine
Learning Research (JMLR), 22, 1–41.
39Neal, R. M. (2012), “MCMC Using Hamiltonian Dynamics,” in Handbook of Markov Chain
Monte Carlo, volume 54, Chapman and Hall/CRC, 113–162.
Neiswanger, W., Wang, C., and Xing, E. P. (2014), “Asymptotically exact, embarrassingly
parallel MCMC,” in Proc. of the Conference on Uncertainty in Artificial Intelligence
(UAI).
Nishihara, R., Murray, I., and Adams, R. P. (2014), “Parallel MCMC with Generalized
Elliptical Slice Sampling,” Journal of Machine Learning Research, 15, 2087–2112.
Plassier,V.,Moulines,E.,andDurmus,A.(2023),“Federatedaveraginglangevindynamics:
Toward a unified theory and new algorithms,” in Proc. of the International Conference
on Artificial Intelligence and Statistics (AISTATS).
Sattler, F., Wiedemann, S., Mu¨ller, K.-R., and Samek, W. (2020), “Robust and
Communication-efficient Federated Learning from Non-iid Data,” IEEE Transactions
on Neural Networks and Learning Systems, 31.
Shokri, R. and Shmatikov, V. (2015), “Privacy-preserving Deep Learning,” in SIGSAC
conference on computer and communications security (CCS), ACM.
´
Vono, M., Plassier, V., Durmus, A., Dieuleveut, A., and Eric Moulines (2022), “QLSD:
Quantised Langevin Stochastic Dynamics for Bayesian Federated Learning,” in Proc. of
the International Conference on Artificial Intelligence and Statistics (AISTATS).
Wang, X.andDunson, D.B.(2013), “ParallelizingMCMCviaWeierstrassSampler,” arXiv
preprint arXiv:1312.4605.
Welling, M. and Teh, Y. W. (2011), “Bayesian Learning via Stochastic Gradient Langevin
Dynamics,” in International Conference on Machine Learning (ICML).
Xie, C., Koyejo, O., and Gupta, I. (2020), “Asynchronous Federated Optimization,” in
OPT2020: 12th Annual Workshop on Optimization for Machine Learning.
Xin, R. and Khan, U. A. (2020), “Distributed Heavy-Ball: A Generalization and Accelera-
tion of First-Order Methods With Gradient Tracking,” IEEE Transactions on Automatic
Control, 65, 2627–2633.
Yu, H., Yang, S., andZhu, S.(2019), “ParallelRestartedSGDwithFasterConvergenceand
Less Communication: Demystifying Why Model Averaging Works for Deep Learning,”
in In Proc. of Conference on Artificial Intelligence (AAAI).
Zhao, Y., Li, M., Liangzhen Lai, N. S., Civin, D., and Chandra, V. (2018), “Federated
Learning with Non-iid Data,” arXiv:1806.00582.
Zou,D.andGu,Q.(2021),“OntheConvergenceofHamiltonianMonteCarlowithStochas-
tic Gradients,” in International Conference on Machine Learning (ICML).
40