A Generalization Bound for Nearly-Linear Networks
Eugene Golikov∗
´
Ecole Polytechnique F´ed´erale de Lausanne
July 10, 2024
Abstract
existenceofpoorones. Inotherwords,thesealgorithms
are implicitly biased towards good solutions.
We consider nonlinear networks as perturbations of Unfortunately, implicit bias of gradient descent is
linear ones. Based on this approach, we present novel not fully understood yet. This is because the training
generalization bounds that become non-vacuous for dynamicsisveryhardtointegrate,orevencharacterize,
networks that are close to being linear. The main analytically. There are two main obstacles we could
advantage over the previous works which propose non- identify. First, modern neural networks have many
vacuous generalization bounds is that our bounds are layers, resulting in a high-order weight evolution equa-
a-priori: performing the actual training is not required tion. Second,activationfunctionsweuseareappliedto
forevaluatingthebounds. Tothebestofourknowledge, hidden representations elementwise, destroying a nice
they are the first non-vacuous generalization bounds algebraic structure of stacked linear transformations.
for neural nets possessing this property. If we remove all activation functions, the training
dynamics of gradient descent can be integrated ana-
lytically [9] under certain assumptions. However, the
1 Introduction
resulting model, a linear network, is as expressive as a
linearmodel,thusloosingoneofthecrucialadvantages
Despite huge practical advancements of deep learning,
of neural nets.
the main object of this field, a neural network, is not
yet fully understood. As we do not have a complete
understanding of how neural networks learn, we are Idea. The idea we explore in the present paper is to
not able to answer the main question of deep learning consider nonlinear nets as perturbations of linear ones.
theory: why do neural networks generalize on unseen Weshowthattheoriginalnetworkcanbeapproximated
data? withaproxy-modelwhoseparameterscanbecomputed
Whiletheabovequestionisvalidforanysupervised using parameters of a linear network trained the same
learning model, it is notoriously difficult to answer for way as the original one. Since the proxy-model uses
neural nets. The reason is that modern neural nets the corresponding linear net’s parameters, its general-
have billions of parameters and as a result, huge ca- izationgapcanbemeaningfullyboundedwithclassical
pacity. Therefore among all parameter configurations approaches. Indeed, if the initial weights are fixed, the
that fit the training data, there provably exist such result of learning a linear network to minimize square
configurations that do not fit the held-out data well loss on a dataset (X ∈ Rd×m,Y ∈ Rdout×m) is solely
[12]. This is the reason why classical approaches for determined by YX⊤ ∈ Rdout×d and XX⊤ ∈ Rd×d,
bounding a generalization gap, i.e. the difference be- whered,d out aretheinputandoutputdimensions,and
tweendistributionandtrainerrors, fallshortonneural m is the dataset size. The number of parameters in
networks: such approaches bound the gap uniformly these two matrices is much less than the total num-
over a model class. That is, if weights for which the ber of parameters in the network, making classical
network performs poorly exist, we bound our trained counting-based approaches meaningful.
network’s performance by performance of that poor
one. Contributions. Our main contribution is a gener-
Asweobserveempirically,networkscommonlyused alization bound given by Theorem 4.2, which is ready
in practice do generalize, which means that training to apply in the following setting: (1) fully-connected
algorithms we use (i.e. gradient descent or its variants) networks, (2) gradient descent with vanishing learning
choose ”good” parameter configurations despite the rate(gradientflow), (3) binary classificationwithMSE
loss. The main disadvantage of our bound is that it
∗evgenii.golikov@epfl.ch
1
4202
luJ
9
]GL.sc[
1v56760.7042:viXradiverges as training time t goes to infinity. We discuss hold. First, a simple counting-based generaliza-
how to choose the training time such that the bound tion bound for a linear model evaluated in the
stays not too large while the training risk reduces sig- same setting should be non-vacuous. Such a
nificantly, in Section 4.3. Our generalization bound bound is vacuous even for binary classification
becomes non-vacuous as long as (1) a linear model on the standard MNIST dataset, but becomes
provably generalizes well under the same setting, (2) non-vacuous if we downsample the images.
the network we consider is close enough to being linear.
2. Second, the activation function has to be suffi-
We validate our bound on a simple fully-connected
ciently close to being linear. To be specific, for
network trained on a downsampled MNIST dataset,
a two-layered leaky ReLU neural net trained on
and demonstrate that it becomes non-vacuous in this
MNISTdownsampledto7x7,oneneedsthenega-
scenario (Section 5). We discuss assumption we use,
tive slope to be not less than 0.99 (1 corresponds
as well as possible improvements of our bound, in Sec-
to a linear net, while ReLU corresponds to 0).
tion 7. Finally, we discuss how far the approach we
use, i.e. generalizationboundsbasedondeviationfrom
3. One may hope for the bound to be non-vacuous
specificproxy-models,couldleadusinthebestscenario
only for a partially-trained network, while for a
(Section 8).
fully-trained network the bound diverges.
4. Even in the most optimistic scenario, when we
2 Comparison to previous work
manage to tighten the terms of our bound as
much as possible, our bound stays non-vacuous
Our bounds has the following advantages over some
only during the early stage of training when the
other non-vacuous bounds available in the literature
network has not started ”exploiting” its nonlin-
(e.g. [1, 3, 2, 13]):
earity yet (Section 8). However, the minimal
1. They are a-priori bounds, i.e. getting the actual negative ReLU slope for which the bound stays
trained the model is not required for evaluating non-vacuous is much smaller, 0.6.
them. Tothebestof ourknowledge, theyare the
first non-vacuous a-priori bounds available for 3 Related work
neural nets. All works mentioned above, despite
providing non-vacuous bounds, could be evalu-
Non-vacuous generalization bounds. While first
ated only on a trained network thus relying on
generalization bounds date back to [7], the bounds
the implicit bias phenomenon which is not well
whicharenon-vacuousforrealisticallylargeneuralnets
understood yet.
appeared quite recently. Specifically, [2] constructed a
PAC-Bayesian bound which was non-vacuous for small
2. The bound of our Theorem 4.2 does not require
fully-connected networks trained on MNIST and Fash-
a held-out dataset to evaluate it, compared to [3]
ionMNIST. The bound of [13], also of PAC-Bayesian
and coupled bounds of [1]. Indeed, if one had a
nature, relies on quantization and compression after
held-out dataset, one could just use it directly to
training. It ends up being non-vacuous for VGG-like
evaluate the trained model, thus questioning the
nets trained on large datasets of ImageNet scale. The
practical utility of such bounds.
boundof[1]isthefirstnon-vacuousboundthatapplies
3. Itdoesnotgrowwithnetworkwidth. Incontrast, directly to the learned model and not to its proxy. It
PAC-Bayesianbounds,[1,2,13],mightgrowwith is not obvious whether their construction could be gen-
width. eralized to neural nets with more than two layers. The
boundof[3]isnotPAC-Bayesianincontrasttothepre-
4. Similarly to [1, 3], we bound the generalization
viousthree. Itisbasedonthenotionofeffectivedepth:
gap of the original trained model, not its proxy.
it assumes that a properly trained network has small
In contrast, [2] introduces a Gaussian noise to
effectivedepth,evenifitisdeep. Thereforeitseffective
the learned weights, while [13] crucially relies on
capacity is smaller than its total capacity, which allows
quantization and compression after training.
for a non-vacuous bound. See the previous section for
discussion of some of the features of these bounds.
For a fair comparison, we also list the disadvantages
our present bound has:
Linear networks training dynamics. The pio-
1. The bound of our Theorem 4.2 becomes non- neering work that integrates the training dynamics
vacuous only when the following two conditions of a linear network under gradient flow to optimize
2square loss is [9]. This work crucially assumes that the Data. Data points (x,y) come from a distribution D.
initial weights are well aligned with the eigenvectors We assume all x from D to lie on a unit ball, ∥x∥≤1,
of the optimal linear regression weights YX+, where and all y to equal ±1. During the training phase, we
(X,Y) is the train dataset. As the initialization norm sample a set of m points iid from D to form a dataset
approaches zero, the learning process becomes more (X,Y), where X ∈Rd×m and Y ∈R1×m.
sequential: components of the data are learned one by
one starting from the strongest. [6] conjecture that the Training. Assuming rkX = d (which implies m ≥
same happens for any initialization approaching zero d, i.e. the data is abundant), we train our model
excluding some set of directions of measure zero. They on (X,Y) with gradient flow to optimize square loss
provethisresultforthefirst,thestrongest,component, on whitened data, i.e. on (X˜,Y) for X˜ = Σ−1/2X,
but moving further seems more challenging. See also where Σ = 1XX⊤ ∈ Rd×d is an empirical fX eature
[11, 4]. X m
correlation matrix. That is,
Nearly-linear neural nets. Our generalization gap
∂(cid:18)
1
(cid:13)
(cid:13)Y −fϵ
(X˜)(cid:13) (cid:13)2(cid:19)
bound decreases as activation functions get closer to dW lϵ(t)
=−
2m(cid:13) θϵ(t) (cid:13) F
∀l∈[L].
linearity. While nearly-linear activations do not con- dt ∂W
l
form with the usual practice, nearly-linear networks (4)
have been studied before. That is, [5] demonstrated Note that X˜X˜⊤ =mI d.
that when width n and depth L go to infinity at the
same time proportionally, the hidden layer covariances Inference. To conform with the above training pro-
at initialization admit a meaningful limit as long as cedure, we take the model output at a point x to be
the ReLU slopes behave as 1± √c , i.e. become closer fϵ(cid:0) Σ−1/2x(cid:1) , where Σ is a (distribution) feature cor-
n θ
and closer to linearity. See also [8] for a similar limit relation matrix: Σ = E (x,y)∼D(xx⊤) ∈ Rd×d. We
for Transformers [10]. assume this matrix to be known; in practice, we could
substitute it with Σ .
X
4 Main result
Performancemeasure. Wetaketheriskfunctionto
be misclassification loss: r(z,y)=Ind[zy <0], where
Notations. For integer L ≥ 0, [L] denotes the set z,y ∈R and Ind[·] is an indicator of a condition. The
{1,...,L}. For integers l ≤ L, [l,L] denotes the set
empirical (train) risk on the dataset (X,Y) and the
{l,...,L}. For a vector x, ∥x∥ denotes its Euclidean
distribution risk of the model trained for time t are
norm. For a matrix X, ∥X∥ denotes its maximal sin-
then defined as
gular value, while ∥X∥ denotes its Frobenius norm.
F
m
Rˆϵ(t)= 1 (cid:88) r(cid:16) fϵ (Σ−1/2x ),y (cid:17) , (5)
4.1 Setup m θϵ(t) k k
k=1
Model. The model we study is a fully-connected (cid:16) (cid:17)
Rϵ(t)=E r fϵ (Σ−1/2x),y . (6)
LeakyReLU network with L layers: (x,y)∼D θϵ(t)
For a given model f, we also define R(f) and Rˆ(f)
fϵ(x)=W xϵ (x), xϵ (x)=x, (1)
θ L θ,L−1 θ,0 accordingly.
xϵ (x)=ϕϵ(cid:0) W xϵ (x)(cid:1) ∀l∈[L−1], (2) In addition, for γ > 0, we define γ-margin loss:
θ,l l θ,l−1 r (z,y)=Ind[zsgny <γ], and its continuous version:
γ
where θ ∈ RN denotes the vector of all weights, i.e.

θ = cat({vec(W l)}L l=1), W
l
∈ Rnl×nl−1 ∀l ∈ [L], and 1, zy <0,
ϕϵ is a Leaky ReLU with a negative slope 1−ϵ, that rC(z,y)= 1−zy/γ, zy ∈[0,γ], (7)
γ
is: 0, zy >γ
ϕϵ(x)=x−ϵmin(0,x). (3)
We define Rγ and RC analogously to R, and Rˆγ and
Sincewearegoingtoconsideronlybinaryclassification γ
in the present work, we take n = 1. We also define RˆC analogously to Rˆ.
L γ
d=n to denote the input dimension.
0
4.2 Generalization bound
We will need the following assumption on the training
process:
3Assumption 4.1. ∀t≥0 (cid:13) (cid:13) (cid:13)(cid:16) Y −f θϵ ϵ(t)(X˜)(cid:17) X˜⊤(cid:13) (cid:13) (cid:13)2
F
≤ v β(t)= L L−1 sˆ2− LL uL β−1(t)[w(u β(t))−w(β)]euL β sˆ(t) ,
(cid:13) (cid:13)(cid:16)
Y −fϵ
(X˜)(cid:17) X˜⊤(cid:13) (cid:13)2
.
(13)
(cid:13) θϵ(0) (cid:13) F where 1
Note that (cid:13) (cid:13)Y −fϵ (X˜)(cid:13) (cid:13)2 ≤ (cid:13) (cid:13)Y −fϵ (X˜)(cid:13) (cid:13)2 ¯1 (cid:18) 2−L uL(cid:19) sˆ (cid:18) 2 uL(cid:19)
(cid:13) θϵ(t) (cid:13) (cid:13) θϵ(0) (cid:13) w(u)=− Γ , −ρ Γ , . (14)
F F s¯ L sˆ s¯ L sˆ
since we minimize the loss monotonically with gradient
flow. This implies that the above assumption holds
This theorem gives an a-priori bound for the gen-
automatically, whenever X˜ is a (scaled) orthogonal eralization gap Rϵ −Rˆϵ, i.e. it could be computed
matrix (which happens when m = d). It is easy to γ
without performing the actual training.
showthatitprovablyholdsforalinearnetwork(ϵ=0),
see Appendix C, and we found this assumption to hold
Proof idea and meaning of terms. In order to
empirically for all of our experiments with nonlinear
prove Theorem 4.2, we approximate our nonlinear net-
networks too, see Figure 7.
work with a model which uses weights of a linear net-
We are now ready to formulate our main results:
work trained the same way as the original one we con-
Theorem4.2. Fixβ,γ >0,t≥0,δ ∈(0,1),ϵ∈[0,1], sider. We consider two proxy-models, of order 1 and 2,
and κ ∈ {1,2}. Let p be the floating point arithmetic indicated by κ. Since they use linear networks weights,
precision (32 by default). Under the setting of Sec- the number of parameters of these proxy-models hap-
tion 4.1 and Assumption 4.1, for any weight initial- pens to be only κ times larger than the one of a linear
ization satisfying ∥W lϵ(0)∥≤β ∀l∈[L], w.p. ≥1−δ model. ThisgivesΥ κ,whichisasimplecounting-based
over sampling the dataset (X,Y), bound for the generalization gap. The last term of the
bound accounts for a deviation of these proxy-models
Rϵ(t)≤Rˆϵ(t)+Υ + ∆ κ,β(t)ϵκ , (8) from the original one.
γ κ γ
Dimension dependency. Our bound does not de-
where
(cid:114) pendonwidthn ∀l∈[L−1],incontrasttothebounds
κpdln2+ln(1/δ) l
Υ = , (9) of [2, 13, 1]. However, both penalty terms of Theo-
κ 2m √
rem 4.2, Υ and ∆ (t), grow as d with the input
κ β,κ
and dimension.
∆ (t)=Φ v (t)uL−1(t), (10)
κ,β κ β β
where 4.3 Choosing the training time
 √
L d+ (cid:104)1+(L−1)ρ,
√
κ=1; A fos rw Le =see 2,∆ anκ d(t a) sdi tv →erge βs ¯2s −u Lper f- oe rxp Lo ≥nen 3t ,i sa olly tha est b→ oun∞
d
Φ = (L−1) (L+1+(L−1)ρ) d (11) (L−2)s¯
κ κ=2, eventually becomes vacuous. For the bound to make
 +2(1+(L−1)ρ)(cid:105)
, sense, we should be able to find t small enough for
∆ (t)tostaynottoolarge,andatthesametime,large
κ
where ρ= ∥W 1ϵ(0)∥F is the square root of the stable rank enough for the training risk Rˆ γϵ(t) to get considerably
∥W 1ϵ(0)∥ reduced.
of the input layer at initialization, and the definitions
In the present section, we are going to demonstrate
of u and v are given below.
β β that for values of t which correspond to partially learn-
Define
ing the dataset (i.e. for which Rˆϵ(t)∈(0,1)), the last
γ
(cid:13) (cid:13)
• s=(cid:13)YX⊤Σ−1/2(cid:13); ¯1=1+ρβL; term of the bound, ∆ κ,β(t)/γ, admits a finite limit as
(cid:13) X (cid:13)
β →0 (the saddle-to-saddle regime of [4]).
• Foragivenr ≥0,s¯
r
=(1−ϵ)(s+ρβL)+ϵ√ r(L− We do not know how Rˆ γϵ(t) decreases with t in
1)(1+ρβL); our case. However, when the network is linear, this
can be computed explicitly when either the weight
• s¯=s¯ ; sˆ= L s¯; β¯=βs¯ρ. initialization is properly aligned with the data, or the
1 1+(L−1)ρ s¯1
initializationnormβ vanishes. Wearegoingtoperform
We have
ourwholeanalysisforL=2inthemain, anddeferthe
(cid:40) β¯es¯t, L=2; case L≥3 to Appendix B.2.
u (t)= (12)
β (cid:0) β¯2−L−(L−2)s¯t(cid:1) 2−1 L , L≥3; 1Γ is an upper-incomplete gamma-function defined as
Γ(s,x)=(cid:82)∞ts−1e−tdt.
x
4Thatis,consideranSVD:YX˜+ = √1 YX⊤(XX⊤)−1/2 = Let us compute the quantities which appear in our
m
PSQ⊤, where P and Q are orthogonal and S is di- bounds, at t=t∗ α(β):
agonal; note that S = s. Observe also that s =
∥YX˜+∥ ≤
∥Y∥∥X˜+∥11
≤ 1 since y = ±1. [9]2 have
u=β¯(cid:18) α(s−β2)(cid:19) 2s¯
s = s¯ ρr2s¯ sβ1+s s¯(λ 2−1) (1+O(β)),
demonstrated for linear nets (ϵ = 0) and L = 2 that (1−α)β2 s¯ 1
when the weight initialization is properly aligned with (19)
P and Q3, ∥W 10(t)∥=∥W 20(t)∥=u¯(t), where u¯ satis- where we omitted the argument t∗ α(β) for brevity.
fies4 Since Γ(0,x)=−Ei(−x)=−lnx+O x→0(1) and
Γ(1,x)=e−x =O (1), we have w(β)= ¯1ln(β2)+
du¯(t)
=u¯(t)(s−u¯2(t)), u¯(0)=β, (15) O(1). Similarly,
whx→ en0
ever 1+
s¯(cid:0)λ −1(cid:1) >0s¯
, we have
dt (cid:16) s 2 (cid:17)
u=o(β), and w(u)= ¯1ln β2+s s¯(λ−2) +O(1).
s¯
which gives the solution in implicit form:
Consider first λ < 2 and suppose ν = 1. In this
(cid:90) du¯ 1 (cid:18) u¯2(t)(s−β2)(cid:19) case, w(u)−w(β)= ¯1 s(cid:0)λ 2 −1(cid:1) ln(β2)+O(1), and
t= = ln . (16)
u¯(s−u¯2) 2s β2(s−u¯2(t)) 2uv
=
s¯2
ρ
qrs s¯−1¯1 (cid:18) λ −1(cid:19)
ln(β2)(1+O(1/lnβ)).
One could resolve u¯ explicitly to get γ s¯2 1β(s s¯−1)(2−λ) 2
(20)
se2st
Since s¯≥ s, this expression diverges as β → 0. Note
u¯(t)= . (17)
e2st−1+s/β2 that we get the same order divergence even also for
1+s¯(cid:0)λ −1(cid:1)
≤0. Clearly,forν >1,wegetevenfaster
[9]observedthisexpressiontoholdwithgoodprecision s 2
divergence.
forrandom(notaligned)initializationswhenβ issmall
On the other hand, if λ = 2 then w(u)−w(β) =
wen ho eu ng ah. liT neh ai rs nis ets wu op rp kor iste id nitb iy alif zu er dth ce lr osw eo tr oks tho ef o[6 ri, g4 in]: ¯1 s¯ln(cid:16) s s¯ ¯2 ρ 2rs s¯(cid:17) +O(β), and
1
and the initial weights do not lie on a ”bad” subspace,
(cid:32) (cid:33)
gradient flow nearly follows the same trajectory as 2uv sνs¯2 s¯2
studied by [9]. γ
= s¯3ρqrs s¯−ν¯1ln s¯ρ 2rs s¯ β2(1−ν)(1+O(β)).
1 1
Plugging u¯2(t)=αs into Equation (16), we get the
(21)
time required for a linear network to learn a fraction α
We get a finite limit only for ν = 1, i.e. when γ ∝ α.
of the strongest mode:
In this case, the last term of the bound Equation (8)
1
(cid:18) α(s−β2)(cid:19) becomes
t∗(β)= ln . (18)
α 2s (1−α)β2 ∆ ϵκ ¯1ss¯2 (cid:32) s¯2 (cid:33)
κ γ,β = 2s¯3ρqrss¯−1Λ κln s¯ρ 2rs s¯ ϵκ(1+O(β)).
Clearly,thelearningtimet∗(β)divergeswheneverβ → 1 1
α
0, or α→1. (22)
Since t∗ α(β) is the time sufficient to learn a network Summing up, we expect the empirical risk Rˆ γ(t∗ α)
forϵ=0,wesupposeitalsosufficestolearnanonlinear to be smaller than 1 for large enough q (i.e. for small
2
network. So, we are going to evaluate our bound at γ compared to α), and the last term to stay finite as
t=t∗ α. Since we need Rˆ γ(t∗ α)<1 for the bound to be β → 0 and vanish as ϵκ. Therefore as long as Υ κ is
non-vacuous, we should take γ small relative to α. We not large enough (i.e. when κpd is small compared to
consider γ =αν/q for ν,q ≥1. m), the overall bound becomes non-vacuous for small
Since the linear network learning time t∗ α(β) is cor- enough ϵ at least at the (linear) training time t∗ α. We
rect for almost all initialization only when β vanishes, are going to evaluate our bound empirically in the
we are going to work in the limit of β →0. Since we upcoming section.
need α∈(β2/s,1), otherwise the linear training time
is negative, we take α= rβλ for λ∈(0,2] and r >1.
s 5 Experiments
2[9] assumed XX⊤ =I and ∥YX⊤∥=s, while not intro-
d
ducingthefactor 1 aswedoinEquation(4). Itiseasytosee Setup. WeconsideranL-layerbias-freefully-connected
m
thattheourgradientflowhasexactlythesamedynamicsasthe network of width 64 and train it to classify 0-4 versus
onestudiedby[9].
5-9 digits of MNIST (i.e. m = 60000). In order to
3That is, when W2(0) = PS¯ 21/2R⊤, W1(0) = RS¯ 11/2Q⊤,
approximate the gradient flow dynamics, we run gra-
whereRisorthogonal,andS¯ 1 andS¯ 2 areconstructedfromS
dient descent with learning rate 0.001. By default, we
byaddingorremovingzerorowsandcolumns.
4Ouru¯correspondstou1/(Nl−1) of[9]. take L=2, the floating point precision to be p=32,
5Figure 1: We consider 7x7 binary MNIST, L=2, κ=2, ϵ=0.001, and vary β. The bound of Theorem 4.2
converges as β vanishes and increases as β grows. The bound stays non-vacuous for a small enough β and a
properly choosen γ. We consider γ =β2/q for q ∈{1,10,100}.
Figure 2: We consider 7x7 binary MNIST, L = 2, β = 0.001, ϵ = 0.001, and compare different kappas of
Theorem 4.2. The bound for κ=2 is much stronger than that for κ=1.
Figure 3: We consider 7x7 binary MNIST, L = 2, β = 0.001, ϵ = 0.01, κ = 2, and vary the stable rank at
initialization ρ and floating point precision p. Initializing the input layer with a rank one matrix considerably
improves the bound. Moreover, it also improves the convergence speed.
Figure 4: We consider 7x7 binary MNIST, L=2, β =0.001, ϵ=0.01, κ=2, and compare different components
of the bound. The left figure corresponds to the full bound, while for the central one we forget about the
generalization gap bound for the proxy model Υ , and for the rightmost one, we forget about the deviation term
κ
∆κ,βϵκ
. We see that both terms are of the same order; one therefore has to work on reducing both in order to
γ
reduce the overall bound.
6Figure 5: We consider binary MNIST, L = 2, β = 0.001, ϵ = 0.001, κ = 2, p = 16, rank one input layer
initialization, and vary image dimensions. In this ”gentle” scenario, the bound stays non-vacuous for 14x14
MNIST, and only slightly exceeds the random guess risk for the full-sized, 28x28 MNIST.
Figure6: Weconsiderbinary7x7MNIST,β =0.001,ϵ=0.001,κ=2,p=16,rankoneinputlayerinitialization,
and vary depth. Even in this ”gentle” scenario, the bound gets considerably worse with depth.
Figure 7: We consider binary MNIST, L=2, β =0.001, ϵ=0.001, and measure the differences L(0)−L(t)
(cid:13) (cid:13)2
(solid lines) and L (0)−L (t) (dashed lines). Here L(t) = 1 (cid:13)Y −fϵ (X˜)(cid:13) is the loss at time t, while
X X 2m(cid:13) θϵ(t) (cid:13)
F
(cid:13)(cid:16) (cid:17) (cid:13)2
L (t) = 1 (cid:13) Y −fϵ (X˜) X˜⊤(cid:13) is the ”projected” loss at time t. While L(t) should clearly decrease for
X 2m(cid:13) θϵ(t) (cid:13)
F
gradient descent with small enough steps, it is not a-priori clear that L (t) also does. As we see from the plots, it
X
does for β large and small, and for ϵ up to 1, which corresponds to conventional ReLU activations. These results
validate our Assumption 4.1. Note that we added a small quantity 10−8 in order to make zero visible.
7downsampletheimagesto7x7, andinitializethelayers since we use Leaky ReLUs. We omitted the argument
randomly in a standard Pytorch way (plus, we rescale t for brevity. We get a similar deviation bound on the
the weights to match the required layer norm β). For training dataset:
some experiments, we consider deeper networks, half-
(cid:13) (cid:13)
precision p = 16, downsample not so aggresively, or 1(cid:13)
(cid:13)fϵ (X˜)−fϵ
(X˜)(cid:13)
(cid:13)≤ sup
(cid:13) (cid:13)∂W 1τX˜(cid:13)
(cid:13)
(cid:89)L
∥Wτ∥
enforce the input layer weight matrix to have rank 1. ϵ (cid:13) θϵ θ0 (cid:13) (cid:13) ∂τ (cid:13) k
τ∈[0,ϵ](cid:13) (cid:13)
F k=2
Observations: + sup
(cid:88)L (cid:13)
(cid:13) (cid:13) (cid:13)∂ ∂W
τlτ(cid:13)
(cid:13) (cid:13) (cid:13)∥W 1τX˜∥ F (cid:89) ∥W kτ∥.
• When we take κ = 2 and β ≤ 0.1, the bound τ∈[0,ϵ] l=2 k∈[2:L]\{l}
becomes nonvacuous up to ϵ=0.001 (Figure 1) (24)
and even up to ϵ = 0.01 if we enforce ρ = 1 Wecompletetheabovedeviationboundwithbounding
(Figure 3); weight norms and norms of weight derivatives:
• We can have a non-vacuous bound also for 14x14 Lemma 6.1. Under Assumption 4.1, ∀τ ∈[0,1],t≥
MNIST if we take ϵ=0.001, half-precision (p= 0 √1 m∥W 1τ(t)X˜∥ F ≤ ρu(t), √1 m(cid:13) (cid:13) (cid:13)∂W ∂1 ττX˜ (t)(cid:13) (cid:13) (cid:13) ≤ v(t),
16), and enforce ρ = 1 (Figure 5); the bound (cid:13) (cid:13) F
and ∀l ∈ [L] ∥Wτ(t)∥ ≤ u(t), (cid:13)∂W lτ (t)(cid:13) ≤ v(t) for u,
slightly exceeds the random guess risk for the l (cid:13) ∂τ (cid:13)
full-sized, 28x28 MNIST; v defined in Theorem 4.2.
• The bound for κ=2 is much stronger than that See Section 6.3 and Appendix B.1 for the proof.
for κ=1 (Figure 2); Now we can relate the risk of the original model
with the risk of the approximation. Since r ≤rC ≤r
• The bound improves and converges as β vanishes γ γ
and rC is 1/γ-Lipschitz,
(Figure 1); γ
• Assumption 4.1 holds empirically (Figure 7); R(f θϵ ϵ)−Rˆ γ(f θϵ ϵ)
≤RC(fϵ )−RˆC(fϵ )≤RC(fϵ )−RˆC(fϵ )
• Theboundimprovesifweenforceρ=1; thisalso γ θϵ γ θϵ γ θ0 γ θ0
1 1 (cid:13) (cid:13)
results in faster convergence (Figure 3); + E ∥fϵ (x˜)−fϵ (x˜)∥+ (cid:13)fϵ (X˜)−fϵ (X˜)(cid:13) ,
γ θϵ θ0 γm(cid:13) θϵ θ0 (cid:13) 1
• The bound slightly improves for the half floating (25)
point precision (Figure 3); where the expectation is over the data distribution D,
and x˜=Σ−1/2x is the actual input of the network.
• For ϵ=0.01 at the training time, all components
As for the last term, the deviation on the train
of our bound are of the same order, while for √
dataset, we use that ∥z∥ ≤ m∥z∥ for any z ∈ Rm,
larger ϵ, the last one starts to dominate (Fig- 1
and Equation (24). As for the deviation on the test
ure 4);
dataset, due to Equation (23) and Lemma 6.1, in order
• The bound deteriorates consideraly when we in- toboundthelastterm,itsufficestoboundE∥x˜∥. Since
crease depth (Figure 6). Σ=E[xx⊤], we get
E∥Σ−1/2x∥2 =E (cid:2) x⊤Σ−1x(cid:3)
6 Proof of the main result (26)
=E tr(cid:2) xx⊤Σ−1(cid:3) =tr[I ]=d.
d
6.1 Proof of Theorem 4.2 for κ = 1
This gives
E∥Σ−1/2x∥≤(cid:112)E∥Σ−1/2x∥2 ≤√
d.
We start with approximating our learned network fϵ The first two terms is a generalization gap of the
θϵ
with fϵ , i.e. with a nonlinear network that uses the proxy-model. We use a simple counting-based bound:
θ0
weights learned by the linear one. This approximation (cid:114)
(cid:16) (cid:17) (cid:16) (cid:17) ln|Fϵ(t;θ(0))|−lnδ
deviates from the original network the following way: RC fϵ −RˆC fϵ ≤ 1 ,
∀x∈Rd,ϵ∈[0,1], γ θ0(t) γ θ0(t) 2m
(27)
1
ϵ
∥f θϵ ϵ(x)−f θϵ 0(x)∥= 1
ϵ
(cid:13) (cid:13) (cid:13) (cid:13)(cid:90) 0ϵ ∂f θ ∂ϵ τ τ(x) dτ(cid:13) (cid:13) (cid:13)
(cid:13)
w F. 1ϵp (. t;≥ θ(1 0)−
)
dδ eo nv oe tr essa tm hepl sin etg ot fhe fud na ct ta iose nt s( rX ep, rY es) e, nw th aber lee
with fϵ for a given initial weights θ(0), where θ0(t)
≤ sup (cid:13) (cid:13) (cid:13)∂f θϵ τ(x)(cid:13) (cid:13) (cid:13)≤ sup (cid:88)L (cid:13) (cid:13) (cid:13)∂W lτ(cid:13) (cid:13) (cid:13)(cid:89) ∥Wτ∥∥x∥ is a reθ s0 u( lt t) of running the gradient flow Equation (4)
(cid:13) ∂τ (cid:13) (cid:13) ∂τ (cid:13) k
τ∈[0,ϵ] τ∈[0,ϵ] for time t. As long as we work with finite precision,
l=1 k̸=l
(23) this class is finite:
8Lemma 6.2. ∀θ(0),ϵ,t≥0, |Fϵ(t;θ(0))|≤2pd. 6.3.1 Weight norms
1
Proof. Since we run our gradient flow Equation (4) Let us expand the weight evolution Equation (4):
on whitened data to optimize squared loss, the initial
dWτ (cid:104) (cid:105)
weights are fixed, and the network we train is linear, 1 = Dτ ⊙Wτ,⊤Ξτ X˜⊤, (32)
the resulting weights depend only on YX˜⊤ which has dt 2
d parameters. Since each function in our class is com- dWτ (cid:104) (cid:105)⊤
2 =Ξτ Dτ ⊙WτX˜ , (33)
pletely defined with the resulting weights, and each dt 1
weight occupies p bits, we get the above class size.
where we define
WecouldhaveusedaclassicalVC-dimension-based 1 (cid:16) (cid:104) (cid:105)(cid:17)
Ξτ = Y −Wτ Dτ ⊙WτX˜ , (34)
bound instead. However, we found it to be numerically m 2 1
larger compared to the counting-based bound above.
(cid:16) (cid:17)
This finalizes the proof of Theorem 4.2 for κ=1. andDτ =(ϕϵ)′ WτX˜ isan×mmatrixwithentries
1
equalto1or1−τ. WeexpressitasDτ =(1−τ)1 +
n×m
6.2 Proof of Theorem 4.2 for κ = 2 τ∆, where ∆ is a n×m 0-1 matrix.
Let us bound the evolution of weight norms:
What changes for κ = 2 is a proxy-model. Consider
the f˜ θϵf 0o ,l θl ϵo (w xi )n =g: f θϵ 0(x)+(cid:16) f θϵ ϵ(X˜)−f θϵ 0(X˜)(cid:17) X˜+x. (28) d∥W dt1τ∥ ≤(cid:13) (cid:13) (cid:13) (cid:13)dW dt1τ(cid:13) (cid:13) (cid:13) (cid:13) +≤ τ(1 (cid:13) (cid:13)− ∆τ ⊙)∥ WW τ2 ,τ ⊤∥ Ξ(cid:13) (cid:13) (cid:13) τΞ (cid:13) (cid:13)τ (cid:13) (cid:13)X˜ X˜⊤ (cid:13) (cid:13)(cid:13) (cid:13) (cid:13)
.
(35)
(cid:13) 2 (cid:13)(cid:13) (cid:13)
That is, we take the same proxy-model as before, but
Sincemultiplyingbya0-1matrixelementwisedoesnot
we add a linear correction term. This correction term
aims to fit the proxy model to the original one, fϵ , on increase Frobenius norm, we get
θϵ
the training dataset X˜. We prove the following lemma (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)∆⊙Wτ,⊤Ξτ(cid:13)≤(cid:13)∆⊙Wτ,⊤Ξτ(cid:13) ≤∥Wτ∥∥Ξτ∥ .
in Appendix A.1: (cid:13) 2 (cid:13) (cid:13) 2 (cid:13) 2 F
F
(36)
Lemma 6.3. Under the premise of Lemma 6.1, ∀t≥0 NotingthatΞτ andΞτX˜⊤arerowmatrices,thisresults
∀ϵ∈[0,1] ∀x∈Rd we have in
(cid:13) (cid:13) (cid:13)f θϵ ϵ(t)(x)−f˜ θϵ 0(t),θϵ(t)(x)(cid:13) (cid:13) (cid:13) ≤uL−1(t)v(t)∥x∥ϵ2; (29) d∥W dt1τ∥ ≤(cid:16) (1−τ)∥ΞτX˜⊤∥+τ∥Ξτ∥∥X˜∥(cid:17) ∥W 2τ∥.
(L−1)(L+1+ρ(L−1)) (37)
By a similar reasoning,
(cid:13) (cid:13)
(cid:13) (cid:13)f θ 2ϵ ϵ (( Lt)( −X˜ 1) )− (1f˜ +θϵ 0 ρ(t () L,θϵ −(t) 1( )X˜ ))(cid:13) (cid:13) ≤uL−1(t)v(t)√ mϵ2. d∥W dt2τ∥
≤(1−τ)∥ΞτX˜⊤∥∥W 1τ∥+τ∥Ξτ∥∥W 1τX˜∥ F.
(30) (38)
The deviation now scales as ϵ2 instead of ϵ.
Bounding norms, option one. A this point, we
What remains is to bound the size of Fϵ(t;θ(0)),
2 could bound ∥WτX˜∥ ≤∥Wτ∥∥X˜∥ . We then make
which denotes the set of functions representable with 1 F 1 F
ournewproxy-modelf˜ϵ forgiveninitialweights use of the following lemma which we prove in Ap-
θ0(t),θϵ(t) pendix A.2:
θ(0). Since f˜ϵ is a sum of fϵ and a linear
θ0(t),θϵ(t) θ0(t)
model, its size is at most 2pd times larger: Lemma 6.4. ∥Ξ τ∥ = ∥Ξ τ∥ F ≤ √1 m(1 + ρβL). If
we additionally take Assumption 4.1 then ∥Ξ X˜⊤∥=
τ
|F 2ϵ(t;θ(0))|≤|F 1ϵ(t;θ(0))|2pd ≤22pd. (31) ∥Ξ τX˜⊤∥
F
≤s+ρβL.
√
This finalizes the proof of Theorem 4.2 for κ=2. Since X˜X˜⊤ = mI , we have ∥X˜∥ = m and
√ d
∥X˜∥ = md. Fromtheabovelemmaandsinceτ ≤ϵ,
F
6.3 Proof of Lemma 6.1 W 1τ ,2(t)≤u(t), where u satisfies
We are going to present the proof for L = 2 in the du(t)
=s¯ u(t), u(0)=β. (39)
present section, and defer the case of L ≥ 3 to Ap- dt d
pendix B.1.
Its solution is given by u(t)=βesˆdt.
9Bounding norms, option two. We could bound 6.3.2 Norms of weight derivatives
theweightnormswithoutusingthebound∥WτX˜∥ ≤
1 F In Appendix A.3, we follow the same logic to demon-
O∥W
D1
Eτ∥ s∥ :X˜∥ F. Indeed, consider the following system of
strate that
(cid:13)
(cid:13) (cid:13)dW
dτ1τ(cid:13)
(cid:13) (cid:13), √1
m(cid:13)
(cid:13) (cid:13)dW dτ1τ
X˜(cid:13)
(cid:13)
(cid:13)
, and
(cid:13)
(cid:13) (cid:13)dW
dτ2τ(cid:13)
(cid:13)
(cid:13)
are all
F
bounded by the same v which satisfies
dg (t)
1 =s¯2g (t), g (0)=βs¯ ; (40)
dt 2 1 ρ dv =v(cid:2) (1+ρ)u2+s¯(cid:3) +u(cid:2)¯1+ρu2(cid:3)
, v(0)=0.
dg (t) dt
d2
t
=g 1(t), g 2(0)=β. (41) (48)
It is a linear ODE in v(t), and u(t) is given: u(t) =
From Lemma 6.4, g 1(t)≥(1−τ)(s+ρβ2)∥W 1τ(t)∥+ β¯es¯t. We solve it in Appendix A.4 to get v(t) from
τ(1+ρβ2)∥Wτ(t)X˜∥ and g (t)≥∥Wτ(t)∥.
1 F 2 2 Theorem 4.2.
The above system of ODEs could be solved analyti-
cally:
7 Discussion
(cid:18) (cid:18) (cid:19)(cid:19)
g (t)=β(cid:112) s¯ −s¯ cosh s¯t+tanh−1 s¯ 1 ; (42)
1 ρ 1 s¯
ρ 7.1 Assumptions
(cid:113) (cid:18) (cid:18) s¯ (cid:19)(cid:19)
g (t)= β¯2−β2sinh s¯t+tanh−1 1 , (43) Whiteneddata. Overall,theassumptiononwhitened
2 s¯
ρ dataisnotnecessaryforaresultsimilartoTheorem4.2
where β¯ = βs¯ρ. This gives the bound for the input to hold. We assume the data to be whitened for two
layer weight ns¯ o1 rms: reasons. First,itlegitimatesthechoiceoftrainingtime
t∗(β) since it is based on the analysis of [9], which
(cid:90) t α
∥Wτ(t)∥≤β+s¯ g (t)dt assumed the data to be whitened. If we dropped it,
1 2
0 (44) we could still evaluate the bound of Theorem 4.2 at
(cid:113) (cid:18) (cid:18) s¯ (cid:19)(cid:19) t=t∗(β), but it would be less clear whether the train-
=β−β¯+ β¯2−β2cosh s¯t+tanh−1 1 . α
s¯ ining risk Rˆ becomes already small by this time.
ρ γ
Second,wehadtobound∥X˜∥throughouttheproof
For further analysis, we will need simpler-looking
of Theorem 4.2 and ∥X˜+∥ in the proof of Lemma 6.3.
bounds. Consider a looser bound: g (t) ≤ u(t) and √ √
2 For whitened data, these are simply m and 1/ m,
g (t)≤s¯u(t), where
1 which is a clear dependence on m, making the final
du(t) s¯ bound look cleaner. Otherwise, they would be random
=s¯u(t), u(0)=β ρ. (45)
dt s¯ variableswhosedependenceonmwouldbenotobvious.
1
Third, if we considered training on the original
This ODE solves as u(t)=β¯es¯t. dataset (Y,X) instead of the whitened one, (Y,X˜), we
So, we have ∥W 2τ(t)∥≤u(t). As for the input layer would have to know YX⊤ and XX⊤ in order to deter-
weights, we get mine θ0(t) for a given t and initialization θ0(0). These
(cid:90) t two matrices have d+ d(d+1) parameters, compared to
∥W 1τ(t)∥≤β+s¯ u(t)dt=β−β¯+u(t) (46) just d for YX˜⊤. This way2 , Υ would grow as d instead
0 √ κ
of d.
Similarly, √1 m∥W 1τ(t)X˜∥
F
= βρ − β¯+ u(t). As we
do not want additive terms, we bound from above:
Gradient flow. We expect our technique to follow
∥W 1τ(t)∥≤u(t) and √1 m∥W 1τ(t)X˜∥≤ρu(t).
through smoothly for finite-step gradient descent. In-
troducingmomentumalsoseemsdoable. However,gen-
Choosing between the two options. We could eralizing it to other training procedures, e.g. the ones
take u to be the minimum between the two: u(t) = which use normalized gradients, might pose problems
min(βes¯dt,β¯es¯1t). Since β¯ ≥ β, the first option is sinceitisnotclearhowtoreasonablyupper-boundthe
smaller for small t, but since s¯ > s¯ , it becomes norm of the elementwise ratio of two matrices.
d 1
eventually larger. The time when it happens is given
by Assumption 4.1. We use this assumption to prove
lns¯ −lns¯
t= ρ 1, (47) the second part of Lemma 6.4. If we dropped it, the
s¯ d−s¯ 1 bound would be ∥ΞτX˜⊤∥ ≤∥Ξτ∥ ∥X˜⊤∥≤1+ρβL
F F
which has a finite limit as β →0. On the other hand, insteadofs+ρβL. Thiswouldresultinlargerexponents
the training time goes to infinity in this limit. For this for the definition u in Theorem 4.2.
reason, we pick the second option for our theorem.
108 How far could we get with our
As an argument in favor of this assumption, we
demonstrate it empirically first (Figure 7), and second, approach?
we prove it for the linear case, see Appendix C.
Broadly speaking, the approach we apply in this work
7.2 Proof couldbedescribedasfollows. Supposewehaveamodel
f learned on a dataset (X,Y), and another ”proxy”
Weexpecttheboundsonweightnormsu(t)tobequite
model g which we construct having an access to the
loose since we use Lemma 6.4 to bound the loss. This
same dataset (X,Y). We bound the distribution risk
lemma bounds the loss with its value at the initial-
of the ”original” model using the following relation:
ization, while the loss should necessarily decrease. If
we could account for the loss decrease, the resulting R(f)−Rˆ (f)≤RC(f)−RˆC(f)≤RC(g)−RˆC(g)
γ γ γ γ γ
u(t) would increase with a lower exponent, or even
E |g(x)−f(x)|+ 1∥g(X)−f(X)∥
stay bounded as u¯(t), which corresponds to a linear + x∼D m 1 .
model, does. This way, we would not have to assume γ
ϵ to vanish as β vanishes in order to keep the bound (49)
non-diverging for small β at the training time t∗(β). In words, we say that performance of f is worse than
α
Also, the general bound of Theorem 4.2 would diverge that of g at most by some deviation term.
withtrainingtimetmuchslower. Weleaveitforfuture The bound ends up to be good whenever (a) the
work. generalization gap of g could be well-bounded, and (b)
Asweseefromourestimates,Υ becomesthemain g does not deviate from f much. That is why we con-
κ
bottleneck of our bound for small ϵ. The bound we sidered proxy-models based on linear learned weights:
usedforΥ isverynaive;webelievethatbetterbounds their generalization gap could be easily bounded an-
κ
are possible. Improving the bound for Υ will increase alytically, and they do not deviate much from corre-
κ
the maximal ϵ for which our bound is non-vacuous. sponding leaky ReLU nets as long as ReLU negative
slopes are close to one.
The biggest conceptual disadvantage of this ap-
7.3 Other architectures
proach is that, given both f and g learn the training
As becomes apparent from the proof in Appendix A.1, dataset, we have no chance proving that f performs
the proxy-model for κ = 2, Equation (28), deviates better than g, we could only prove that f performs not
from the original model fϵ as O(ϵ2) for any map much worse thang. Dotheproxy-modelsweuseinthe
θϵ
(ϵ,θ,x)→fϵ(x) as long as the following holds: present paper perform well, and how much do they de-
θ
viatefromoriginalmodels? Ourmaintheoreticalresult,
1. f0(x) is linear in x for any θ;
θ Theorem 4.2, bounds the proxy-model generalization
2.
∂2f θϵ(x)
is continuous as a function of (ϵ,θ,x);
gap and the deviation from above. These bounds are
∂ϵ∂θ arguably not optimal. It is therefore instructive to ex-
3. the result of learning θϵ(t) is differentiable in ϵ amine how well the bound would perform, if we could
for any t. estimate Equation (49) exactly.
This is directly applicable to convolutional networks
8.1 Empirical validation
with no other nonlinearities except for ReLU’s; in par-
tiular, without max-pooling layers. One may intro- Setup. We work under the same setup as in Sec-
duce max-poolings by interpolating between average- tion 55, but instead of evaluating the bound of Theo-
poolings (which are linear) for ϵ=0 and max-poolings rem 4.2, we actually train a linear model with exactly
for ϵ=1. This is not applicable to Transformers [10] the same procedure as for the original model, in order
since attention layers are inherently nonlinear: queries to get trained linear weights θ0. We then evaluate the
and keys have to be multiplied. proxy-models considered in the present work: (1) the
Comparedtothefully-connectedcaseofthepresent one for κ = 1, fϵ , (2) the one for κ = 2, see Equa-
θ0
work, our bound might become even tighter for con- tion(28),andalso(3)thelinearnetwork,f0 . Wethen
θ0
volutional nets since d becomes the number of color evaluate the rhs of Equation (49) using a test part of
channels (up to 3) instead of the whole image size in the MNIST dataset. For this ”optimistic” bound, we
pixels. However,thecorrespondingproxy-modelsmight
5WealsodownsampleMNISTimagesto14x14insteadof7x7.
beover-simplistic: thelinearnettheywilldeviatefrom
Thereasonwhywedoitisthatononehand,wewantedtotest
is just a global average-pooling followed by a linear
our bounds on more realistic scenarios, while on the other, X
Rd → R map. We leave exploring the convolutional doesnotappeartobefull-rankfortheoriginal28x28MNIST.
net case for future work.
11Figure 8: We examine the optimistic bound of Equation (49) for the proxy-models proposed in our paper: the
κ=1 one, fϵ , the κ=2 one of Equation (28), and also a linear proxy, f0 . The bottom row demonstrates the
θ0 θ0
full bound (green lines), while the top one depicts the two components of the bound, namely, the proxy model
generalization gap RC(g)−RˆC(g) and the proxy model deviation E|f(x)−g(x)|+Eˆ|f(x)−g(x)|, separately.
γ γ
Different lines of the same color (e.g. solid green and dashed black lines on the bottom row) correspond to
differentvalues ofγ. Proxygeneralization gapstayslowduringthe wholetraining(topleft figure), while thetrain
risk and the model deviation over gamma contribute significantly (bottom row, two groups of lines correspond to
the minimal and the maximal γ we considered). The optimistic bound for the 1st-order proxy (bottom left) gets
non-vacuous only at the moment when GF escapes the origin and reaches the linear model loss. The bound for
the 2nd-order proxy (bottom right) becomes non-vacuous soon after the original model becomes non-vacuous
(but still stays near the origin), and stays non-vacuous until the model starts exploiting its nonlinearity to reduce
the loss below the optimal linear model loss level (the last drop of purple and black lines).
Figure 9: The optimistic bound of Equation (49) based on our κ = 2 proxy stays non-vacuous up to ϵ = 0.4
until the gradient flow starts ”exploiting” the nonlinearity (the last drop of purple and black lines).
12consider much larger values of epsilon: ϵ≥0.1, i.e. our • The resulting optimistic bound for κ=2 (green
model much less ”nearly-linear” now than before. linesofFigure8,bottomright)staysnon-vacuous
during the first two phases for ϵ=0.1, while this
Figures. We present the results on Figures 8 and 9. is not the case for κ=1 (green lines of Figure 8,
Dashed lines correspond to the train set, while solid bottom left);
ones correspond to the test set. Black lines are risks
• Theoptimisticboundforκ=2staysnon-vacuous
of the actual trained model fϵ : R(t) and Rˆ (t),
θϵ(t) γ up to ϵ=0.4 (green lines of Figure 9).
respectively. Green lines are our ”optimistic” bound
Equation(49)evaluatedatdifferentvaluesofγ. Purple It is tempting to assume that the weights θϵ follow
lines denote MSE loss of the actual trained model. the same trajectory as the weights of the linear model,
Wealsoputthreebaselinesontheplots. Thedotted θ0, during the first two phases. However, if it was the
blacklineistheclassificationrisk(andtheMSEloss)of case, the κ=1 proxy-model, fϵ , would coincide with
θ0
azeromodelf ≡0. ThebrowndashedlineistheMSE the original one, fϵ , during this period. Then their
(train)lossoftheoptimallinearmodel,f :x→YX˜+x. qualitywouldbetheθϵ same;however,Figure8,topright,
Finally, the red dashed line is the (train) classification demonstrates the opposite.
risk of the optimal linear model.
9 Conclusion
Training phases. As we observe on risk plots (Fig-
ure 9 and the bottom row of Figure 8), the training
WehavederivedanovelgeneralizationboundforLekyReLU
process could be divided into three phases. During
networks. Our bound could be evaluated before the
the first phase, the risk decreases until it reaches the
actual training and does not depend on network width.
risk of the optimal linear model, while the loss stays
Our bound becomes non-vacuous for partially-trained
at the level of f ≡ 0. This indicates that while the
nets with activation functions close to being linear.
weights stay very close to the origin, the network out-
putsalreadyalignwiththeoutputsoftheoptimallinear
model. During the second phase, both the loss and References
the risk stay at the level of the optimal linear model.
As for the following phase, both the risk and the loss [1] Felix Biggs and Benjamin Guedj. Non-vacuous
drop below the optimal linear model level. Therefore generalisation bounds for shallow neural networks.
from this point on, the network starts to ”exploit” its In International Conference on Machine Learning,
nonlinearity in order to reduce the train loss. pages 1963–1981. PMLR, 2022.
[2] Gintare Karolina Dziugaite and Daniel M Roy.
Observations:
Computing nonvacuous generalization bounds for
• The generalization gap stays negligible for all deep(stochastic)neuralnetworkswithmanymore
models and γ’s considered (Figure 8, top left); parameters than training data. arXiv preprint
arXiv:1703.11008, 2017.
• The proxy-model for κ = 2 approximates the
original model best among all three proxies con- [3] Tomer Galanti, Liane Galanti, and Ido Ben-Shaul.
sidered (Figure 8, top right); Comparative generalization bounds for deep neu-
ral networks. Transactions on Machine Learning
• While the linear approximation deviates from
Research, 2023.
the original model more than the one for κ=2
duringthefirstphase,theirdeviationsaresimilar [4] Arthur Jacot, Fran¸cois Ged, Franck Gabriel,
during the subsequent phases; Berfin S¸im¸sek, and Cl´ement Hongler. Deep linear
networks dynamics: Low-rank biases induced by
• At the same time, the κ=1 approximation devi-
initialization scale and l2 regularization. arXiv
ates more than that of κ=2 during the second
preprint arXiv:2106.15933, 2021.
phase;
[5] Mufan Li, Mihai Nica, and Dan Roy. The neural
• The transition between the first and the second
covariance sde: Shaped infinite depth-and-width
phases results in a nonmonotonic behavior of the
networks at initialization. Advances in Neural
deviation from the original model for κ=1 and
Information Processing Systems, 35:10795–10808,
linear proxy-models;
2022.
13[6] Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. To-
wards resolving the implicit bias of gradient de-
scent for matrix factorization: Greedy low-rank
learning. arXiv preprint arXiv:2012.09839, 2020.
[7] V A Marchenko and L A Pastur. Raspredelenije
sobstvennyh znachenij v nekotoryh ansambliah
sluchajnyh matric (in russian). Matematicheskij
sbornik, 72(4):507–536, 1967.
[8] Lorenzo Noci, Chuning Li, Mufan Bill Li, Bobby
He, Thomas Hofmann, Chris Maddison, and
Daniel M Roy. The shaped transformer: Atten-
tion models in the infinite depth-and-width limit.
arXiv preprint arXiv:2306.17759, 2023.
[9] Andrew M Saxe, James L McClelland, and Surya
Ganguli. Exact solutions to the nonlinear dy-
namics of learning in deep linear neural networks.
arXiv preprint arXiv:1312.6120, 2013.
[10] Ashish Vaswani, Noam Shazeer, Niki Parmar,
Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
L(cid:32) ukasz Kaiser, and Illia Polosukhin. Attention
is all you need. Advances in neural information
processing systems, 30, 2017.
[11] Chulhee Yun, Shankar Krishnan, and Hossein
Mobahi. A unifying view on implicit bias in
training linear neural networks. arXiv preprint
arXiv:2010.02501, 2020.
[12] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Ben-
jamin Recht, and Oriol Vinyals. Understanding
deep learning requires rethinking generalization.
arXiv preprint arXiv:1611.03530, 2016.
[13] Wenda Zhou, Victor Veitch, Morgane Austern,
Ryan P. Adams, and Peter Orbanz. Non-vacuous
generalization bounds at the imagenet scale: a
PAC-bayesian compression approach. In Inter-
national Conference on Learning Representations,
2019.
14A Missing calculations in Section 6
A.1 Proof of Lemma 6.3
We have:
(cid:13) (cid:32) (cid:33) (cid:13) (cid:13) (cid:13)
1 (cid:13) (cid:13) 1 (cid:13)(cid:90) ϵ ∂fϵ (x) ∂fϵ (X˜)X˜+x (cid:13) 1 (cid:13)∂fϵ (x) ∂fϵ (X˜) (cid:13)
(cid:13)fϵ (x)−f˜ϵ (x)(cid:13)= (cid:13) θτ − θτ dτ(cid:13)≤ sup (cid:13) θτ − θτ X˜+x(cid:13). (50)
ϵ2 (cid:13) θϵ θ0,θϵ (cid:13) ϵ2 (cid:13) ∂τ ∂τ (cid:13) ϵ (cid:13) ∂τ ∂τ (cid:13)
(cid:13) 0 (cid:13) τ∈[0,ϵ](cid:13) (cid:13)
Since f0 is a linear network and rkX˜ = d, we have f0 (x) = f0 (X˜)X˜+x and ∂f θ0 τ(x) = ∂f θ0 τ(X˜)X˜+x. This
θτ θτ θτ ∂τ ∂τ
implies
(cid:13) (cid:13) (cid:13) (cid:32) (cid:33) (cid:13)
1(cid:13)∂fϵ (x) ∂fϵ (X˜) (cid:13) 1(cid:13)(cid:90) ϵ ∂2fρ (x) ∂2fρ (X˜)X˜+x (cid:13)
(cid:13) θτ − θτ X˜+x(cid:13)= (cid:13) θτ − θτ dρ(cid:13)
ϵ (cid:13) ∂τ ∂τ (cid:13) ϵ (cid:13) ∂ρ∂τ ∂ρ∂τ (cid:13)
(cid:13) (cid:13) (cid:13) 0 (cid:13)
(cid:13) (cid:13)
(cid:13)∂2fρ (x) ∂2fρ (X˜) (cid:13)
≤ sup (cid:13) θτ − θτ X˜+x(cid:13) (51)
(cid:13) ∂ρ∂τ ∂ρ∂τ (cid:13)
ρ∈[0,ϵ](cid:13) (cid:13)
≤ ρ∈su [0p ,ϵ](cid:13) (cid:13) (cid:13) (cid:13)∂2 ∂f ρθρ ∂τ( τx)(cid:13) (cid:13) (cid:13) (cid:13)+ ρ∈su [0p ,ϵ](cid:13) (cid:13) (cid:13) (cid:13) (cid:13)∂2 ∂f ρθρ τ ∂( τX˜)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)∥X˜+x∥.
Since we use LeakyReLU,
(cid:13) (cid:13) (cid:13)∂2f θρ τ(x)(cid:13) (cid:13) (cid:13)≤(L−1)(cid:88)L (cid:13) (cid:13) (cid:13)∂W lτ(cid:13) (cid:13) (cid:13)(cid:89) ∥Wτ∥∥x∥; (52)
(cid:13) ∂ρ∂τ (cid:13) (cid:13) ∂τ (cid:13) k
l=1 k̸=l
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)∂2 ∂f ρθρ τ ∂( τX˜)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)≤(L−1)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)∂W ∂1 ττX˜(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:89) ∥W kτ∥+(L−1)∥W 1τX˜∥ F (cid:88)L (cid:13) (cid:13) (cid:13) (cid:13)∂ ∂W τlτ(cid:13) (cid:13) (cid:13) (cid:13) (cid:89) ∥W kτ∥. (53)
F k∈[2,L] l=2 k∈[2,L]\{l}
Finally, since X˜X˜⊤ =mI d, we have ∥X˜+∥= √1 m. Combining everything together, we arrive into
(cid:13) (cid:13) (cid:13)f θϵ
ϵ
(( Lx) −− 1)f˜ ϵθϵ
20 ∥,θ xϵ
∥(x)(cid:13) (cid:13)
(cid:13)
≤ sup
 (cid:88)L (cid:13)
(cid:13)
(cid:13)
(cid:13)∂ ∂W
τlτ(cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:89)
∥W
kτ∥

τ∈[0,ϵ]
l=1 k̸=l
 
+ sup √1 m∥W 1τX˜∥ F
(cid:88)L (cid:13)
(cid:13) (cid:13) (cid:13)∂ ∂W
τlτ(cid:13)
(cid:13) (cid:13)
(cid:13)
(cid:89) ∥W kτ∥ (54)
τ∈[0,ϵ]
l=2 k∈[2,L]\{l}
 (cid:13) (cid:13) 
1 (cid:13)∂WτX˜(cid:13) (cid:89)
+ sup √ m(cid:13)
(cid:13)
∂1
τ
(cid:13)
(cid:13)
∥W kτ∥.
τ∈[0,ϵ] (cid:13) (cid:13)
F k∈[2,L]
Plugging the bounds from Lemma 6.1 then gives
(cid:13) (cid:13)
(cid:13)fϵ (x)−f˜ϵ (x)(cid:13)≤(L−1)(L+1+(L−1)ρ)uL−1v∥x∥ϵ2. (55)
(cid:13) θϵ θ0,θϵ (cid:13)
By a similar reasoning,
(cid:13) (cid:13) (cid:13)f θϵ ϵ(X˜ (L)− −f 1˜ θϵ
)0 ϵ,
2θϵ(X˜)(cid:13) (cid:13)
(cid:13) ≤2 sup

∥W 1τX˜∥ F
(cid:88)L (cid:13)
(cid:13) (cid:13) (cid:13)∂ ∂W
τlτ(cid:13)
(cid:13) (cid:13)
(cid:13)
(cid:89) ∥W
kτ∥

τ∈[0,ϵ]
l=2 k∈[2,L]\{l}
(56)
(cid:13) (cid:13) 
(cid:13)∂WτX˜(cid:13)
(cid:89)
+2 sup (cid:13)
(cid:13)
∂1
τ
(cid:13)
(cid:13)
∥W kτ∥.
τ∈[0,ϵ] (cid:13) (cid:13)
F k∈[2,L]
Plugging the same bounds,
(cid:13) (cid:13) √
(cid:13)fϵ (X˜)−f˜ϵ (X˜)(cid:13)≤2(L−1)(1+(L−1)ρ)uL−1vϵ2 m. (57)
(cid:13) θϵ θ0,θϵ (cid:13)
15A.2 Proof of Lemma 6.4
Recall the definition of Ξτ:
1 (cid:16) (cid:104) (cid:104) (cid:104) (cid:105)(cid:105)(cid:105)(cid:17)
Ξτ(t)= Y −Wτ(t) Dτ (t)⊙Wτ (t) ...Wτ(t) Dτ(t)⊙Wτ(t)X˜ . (58)
m L L−1 L−1 2 1 1
Since m∥Ξτ∥2 is the loss function we optimize, it does not increase under gradient flow. Hence, since multiplying
F
by Dτ elementwise does not increase Frobenius norm,
l
L
m∥Ξτ(t)∥ ≤m∥Ξτ(0)∥ ≤∥Y∥ +∥Wτ(0)X˜∥ (cid:89) ∥Wτ(0)∥. (59)
F F F 1 F l
l=2
We have ∥Wτ(0)X˜∥ =ρ∥Wτ(0)X˜∥≤∥Wτ(0)∥∥X˜∥. Since all y from suppD are ±1 and X˜X˜⊤ =mI , we get
1 F 1 1 d
∥Ξτ(t)∥
F
= √1 m(1+ρβL).
Due to Assumption 4.1, m∥ΞτX˜⊤∥2 does not increase under gradient flow:
F
L
m∥Ξτ(t)X˜⊤∥ ≤m∥Ξτ(0)X˜⊤∥ ≤∥YX˜⊤∥ +∥X˜∥∥Wτ(0)X˜∥ (cid:89) ∥Wτ(0)∥. (60)
F F F 1 F l
l=2
Since YX˜+ =s, we have YX˜⊤ =ms. Therefore ∥Ξτ(t)X˜⊤∥ =s+ρβL.
F
Finally, since Ξτ ∈R1×m, ∥Ξτ∥=∥Ξτ∥ and ∥ΞτX˜⊤∥=∥ΞτX˜⊤∥ .
F F
A.3 Bounding derivatives in the proof of Lemma 6.1
Let us start with Wτ:
1
(cid:34) (cid:32) (cid:33)(cid:35)
d2Wτ dWτ,⊤ dΞτ (cid:104) (cid:105)
1 = Dτ ⊙ 2 Ξτ +Wτ,⊤ X˜⊤− ∆¯ ⊙Wτ,⊤Ξτ X˜⊤; (61)
dtdτ dτ 2 dτ 2
(cid:13) (cid:13) (cid:13) (cid:13)d d2 tW dτ1τ(cid:13) (cid:13) (cid:13) (cid:13)≤(cid:13) (cid:13) (cid:13) (cid:13)d dW τ2τ(cid:13) (cid:13) (cid:13) (cid:13)(cid:16) (1−τ)∥ΞτX˜⊤∥+τ∥Ξτ∥ F∥X˜∥(cid:17) +∥W 2τ∥(cid:18)(cid:13) (cid:13) (cid:13) (cid:13)d dΞ ττ(cid:13) (cid:13) (cid:13)
(cid:13)
+∥Ξτ∥ F(cid:19) ∥X˜∥. (62)
F
We need a bound for
(cid:13) (cid:13)dΞτ(cid:13)
(cid:13):
dτ
dΞτ (cid:104) (cid:105) (cid:20) dWτ (cid:21) dWτ (cid:104) (cid:105)
m =Wτ ∆¯ ⊙WτX˜ −Wτ Dτ ⊙ 1 X˜ − 2 Dτ ⊙WτX˜ ; (63)
dτ 2 1 2 dτ dτ 1
m(cid:13) (cid:13) (cid:13) (cid:13)d dΞ ττ(cid:13) (cid:13) (cid:13) (cid:13)≤∥W 2τ∥∥W 1τX˜∥ F +∥W 2τ∥(cid:13) (cid:13) (cid:13) (cid:13)d dW τ1τ X˜(cid:13) (cid:13) (cid:13)
(cid:13)
+(cid:13) (cid:13) (cid:13) (cid:13)d dW τ2τ(cid:13) (cid:13) (cid:13) (cid:13)∥W 1τX˜∥ F
F (64)
≤u2ρ√ m+u(cid:13) (cid:13) (cid:13)dW 1τ X˜(cid:13) (cid:13)
(cid:13)
+uρ√ m(cid:13) (cid:13) (cid:13)dW 2τ(cid:13) (cid:13)
(cid:13).
(cid:13) dτ (cid:13) (cid:13) dτ (cid:13)
F
This results in
d (cid:13) (cid:13) (cid:13)dW 1τ(cid:13) (cid:13) (cid:13)≤(cid:13) (cid:13) (cid:13)d2W 1τ(cid:13) (cid:13) (cid:13)≤u(cid:18) 1+ρβ2+ρu2+u√1 (cid:13) (cid:13) (cid:13)dW 1τ X˜(cid:13) (cid:13) (cid:13) (cid:19) +(cid:13) (cid:13) (cid:13)dW 2τ(cid:13) (cid:13) (cid:13)(s+(1−s)τ +ρβ2+ρu2). (65)
dt(cid:13) dτ (cid:13) (cid:13) dtdτ (cid:13) m(cid:13) dτ (cid:13) (cid:13) dτ (cid:13)
F
Similarly, we have an evolution of WτX˜:
1
dd t(cid:13) (cid:13) (cid:13) (cid:13)d dW τ1τ X˜(cid:13) (cid:13) (cid:13)
(cid:13)
≤(cid:13) (cid:13) (cid:13) (cid:13)d dW τ2τ(cid:13) (cid:13) (cid:13) (cid:13)(cid:16) (1−τ)∥ΞτX˜⊤∥ F∥X˜∥+τ∥Ξτ∥ F∥X˜⊤X˜∥(cid:17) +∥W 2τ∥(cid:18)(cid:13) (cid:13) (cid:13) (cid:13)d dΞ ττ(cid:13) (cid:13) (cid:13)
(cid:13)
+∥Ξτ∥ F(cid:19) ∥X˜⊤X˜∥
F F
≤u(cid:18) 1+ρβ2+ρu2+u√1 (cid:13) (cid:13) (cid:13)dW 1τ X˜(cid:13) (cid:13)
(cid:13)
(cid:19)√ m+(cid:13) (cid:13) (cid:13)dW 2τ(cid:13) (cid:13)
(cid:13)(s+(1−s)τ
+ρβ2+ρu2)√
m.
m(cid:13) dτ (cid:13) (cid:13) dτ (cid:13)
F
(66)
16Finally, consider Wτ:
2
d2Wτ dΞτ (cid:104) (cid:105)⊤ (cid:20) dWτ (cid:21)⊤
2 = Dτ ⊙WτX˜ +Ξτ Dτ ⊙ 1 X˜ −∆¯ ⊙WτX˜ ; (67)
dtdτ dτ 1 dτ 1
(cid:13) (cid:13) (cid:13) (cid:13)d d2 tW dτ2τ(cid:13) (cid:13) (cid:13) (cid:13)≤(cid:13) (cid:13) (cid:13) (cid:13)d dW τ1τ(cid:13) (cid:13) (cid:13) (cid:13)(cid:16) (1−τ)∥ΞτX˜⊤∥+τ∥Ξτ∥ F∥X˜∥(cid:17) +(cid:18) ∥Ξτ∥+(cid:13) (cid:13) (cid:13) (cid:13)d dΞ ττ(cid:13) (cid:13) (cid:13) (cid:13)(cid:19) ∥W 1τX˜∥ F; (68)
d (cid:13) (cid:13) (cid:13)dW 2τ(cid:13) (cid:13) (cid:13)≤(cid:13) (cid:13) (cid:13)d2W 2τ(cid:13) (cid:13) (cid:13)≤u(cid:18) 1+ρβ2+ρu2+u√1 (cid:13) (cid:13) (cid:13)dW 1τ X˜(cid:13) (cid:13)
(cid:13)
(cid:19) +(cid:13) (cid:13) (cid:13)dW 1τ(cid:13) (cid:13)
(cid:13)(s+(1−s)τ
+ρβ2)+ρu2(cid:13) (cid:13) (cid:13)dW 2τ(cid:13) (cid:13)
(cid:13).
dt(cid:13) dτ (cid:13) (cid:13) dtdτ (cid:13) m(cid:13) dτ (cid:13) (cid:13) dτ (cid:13) (cid:13) dτ (cid:13)
F
(69)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
We see that (cid:13)dW 1τ(cid:13), √1 (cid:13)dW 1τ X˜(cid:13) , and (cid:13)dW 2τ(cid:13) are all bounded by the same v which satisfies
(cid:13) dτ (cid:13) m(cid:13) dτ (cid:13) (cid:13) dτ (cid:13)
F
dv(t)
=v(t)(s¯+(1+ρ)u2(t))+u(t)(¯1+ρu2(t)), v(0)=0, (70)
dt
where s¯=s+(1−s)τ +ρβ2 and ¯1=1+ρβ2.
A.4 Solving the ODE for v(t)
Recall u(t)=β¯es¯t. We solve the homogeneous equation to get
v(t)=C(t)es¯t+1 2+ s¯ρβ¯2e2s¯t =C(t)e(L−1)lnu(t)+1+(L s¯L−1)ρuL(t) =C(t)β¯−1u(t)e1 2+ s¯ρu2(t), (71)
where C(t) satisfies
dC(t) u(t)e1 2+ s¯ρu2(t) =β¯u(t)(cid:2)¯1+ρu2(t)(cid:3)
. (72)
dt
Recall for L=2, sˆ= 2 s¯. Then
1+ρ
(cid:90)
C(t)=β¯ e−u2 sˆ(t) (cid:2)¯1+ρu2(t)(cid:3) dt
(73)
=
β¯(cid:20)¯1(cid:18) Ei(cid:18) −u2(t)(cid:19) −Ei(cid:18) −β¯2(cid:19)(cid:19) −ρsˆ(cid:18)
e−u2 sˆ(t) −e−β¯
sˆ2(cid:19)(cid:21)
.
2 s¯ sˆ sˆ s¯
This gives the final solution:
1 u2(t)
v(t)= u(t)[w(u(t))−w(β)]e sˆ , (74)
2
where we took
w(u)=
¯1 Ei(cid:18) −u2(t)(cid:19)
−
2ρ e−u2 sˆ(t)
. (75)
s¯ sˆ 1+ρ
B Deep networks
B.1 Proof of Lemma 6.1 for L ≥ 3
Bounding weight norms. For l∈[2,L],
dWτ (cid:104) (cid:104) (cid:105)(cid:105)(cid:104) (cid:104) (cid:105)(cid:105)⊤
l = Dτ ⊙Wτ,⊤... Dτ ⊙Wτ,⊤Ξτ Dτ ⊙Wτ ... Dτ ⊙WτX˜ ; (76)
dt l l+1 L−1 L l−1 l−1 1 1
(cid:13)
(cid:13) (cid:13) (cid:13)dW
dtlτ(cid:13)
(cid:13) (cid:13) (cid:13)≤(cid:16) (1−τ)∥ΞτX˜⊤∥∥W 1τ∥+τ(L−1)∥Ξτ∥ F∥W 1τX˜∥ F(cid:17) (cid:89) ∥W kτ∥. (77)
k∈[2,L]\{l}
For l=1,
dWτ (cid:104) (cid:104) (cid:105)(cid:105)
1 = Dτ ⊙Wτ,⊤... Dτ ⊙Wτ,⊤Ξτ X˜⊤; (78)
dt 1 2 L−1 L
17(cid:13)
(cid:13) (cid:13) (cid:13)dW
dt1τ(cid:13)
(cid:13) (cid:13) (cid:13)≤(cid:16) (1−τ)∥ΞτX˜⊤∥+τ(L−1)∥Ξτ∥ F∥X˜∥(cid:17) (cid:89) ∥W kτ∥; (79)
k∈[2,L]
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:13)dW d1 tτX˜(cid:13) (cid:13)
(cid:13)
≤(cid:16) (1−τ)∥ΞτX˜⊤∥ F∥X˜∥+τ(L−1)∥Ξτ∥ F∥X˜⊤X˜∥(cid:17) (cid:89) ∥W kτ∥. (80)
(cid:13) (cid:13)
F k∈[2,L]
Using Lemma 6.4, we get (1−τ)∥ΞτX˜⊤∥∥Wτ∥+τ(L−1)∥Ξτ∥∥WτX˜∥ ≤g (t) and ∥Wτ∥≤g (t) ∀l∈[2:L],
1 1 F 1 l 2
where
dg (t) dg (t)
1 =s¯2gL−1(t), 2 =g (t)gL−2(t), g (0)=β((1−τ)(s+βL)+τ(L−1)(1+βL)ρ), g (0)=β.
dt 2 dt 1 2 1 2
(81)
We have the following first integral:
d (cid:0) g2(t)−s¯2g2(t)(cid:1)
=0,
g2(0)−s¯2g2(0)=β2(cid:0) ((1−τ)(s+βL)+τ(L−1)(1+βL)ρ)2−s¯2(cid:1)
. (82)
dt 1 2 1 2
Therefore
dg (t) (cid:113)
2 =gL−2(t) s¯2g2(t)−s¯2g2(0)+g2(0), g (0)=β. (83)
dt 2 2 2 1 2
Suppose ρ>1. Then g2(0)−s¯2g2(0)>0 and the solution is given in the following implicit form:
1 2
g 23−L(t) 2F 1(cid:16) 21,3− 2L,5− 2L,− g2s (¯ 02 )g −22( s¯t 2) β2(cid:17) −β 23−LF 1(cid:16) 1 2,3− 2L,5− 2L,− g2(0s¯ )2 −β s2 ¯2β2(cid:17)
t= 1 1 . (84)
(cid:112)
(3−L) g2(0)−s¯2β2
1
The above expression cannot be made explicit for general L (but we could get explicit expression for
L∈{2,3,4}). As an alternative, we consider a looser bound u(t):
du(t) s¯
=s¯ uL−1(t), u(0)=β ρ, (85)
dt 1 s¯
1
where s¯ :=(1−τ)(s+βL)+τ(L−1)(1+βL)ρ; note that s¯ =s¯. We have u(t)≥g (t) and s¯u(t)≥g (t) ∀t≥0.
ρ 1 2 1
This ODE solves as
u(t)=(cid:0) s¯(2−L)t+β¯2−L(cid:1) 2−1
L , (86)
where β¯=βs¯ρ. Note that the solution exists only for t< β¯2−L .
s¯1 (L−2)s¯
For the input layer weights, we get
d∥Wτ(t)∥
1 ≤s¯uL−1(t). (87)
dt
The solution is given by
(cid:90) t
∥Wτ(t)∥≤β+s¯ (cid:0) s¯(2−L)t+β¯2−L(cid:1) 2L −− L1 dt=β−β¯+(cid:0) s¯(2−L)t+β¯2−L(cid:1) 2−1 L =β−β¯+u(t). (88)
1
0
Similarly, we have
√1 ∥Wτ(t)X˜∥ ≤βρ+s¯(cid:90) t (cid:0) s¯(2−L)t+β¯2−L(cid:1)L 2−− L1 dt=βρ−β¯+u(t). (89)
m 1 F
0
For brevity, we define
(cid:18) s¯ (cid:19) 1+βL
b=β−β¯=β 1− ρ =−βτ(L−1)(ρ−1) , (90)
s¯ s¯
1
and
(cid:18) s¯ (cid:19) s+βL
b =βρ−β¯=β ρ− ρ =β(1−τ)(ρ−1) . (91)
ρ s¯ s¯
1
As a simpler option, we could just say ∥Wτ(t)∥≤u(t) and √1 ∥Wτ(t)X˜∥≤ρu(t).
1 m 1
18Bounding norms of weight derivatives. Recall the definition of Ξτ:
1 (cid:16) (cid:104) (cid:104) (cid:105)(cid:105)(cid:17)
Ξτ = Y −Wτ Dτ ⊙Wτ ... Dτ ⊙WτX˜ ; (92)
m L L−1 L−1 1 1
m(cid:13) (cid:13) (cid:13) (cid:13)d dΞ ττ(cid:13) (cid:13) (cid:13) (cid:13)≤(L−1)∥W 1τX˜∥ F (cid:89)L ∥W lτ∥+(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)dW d1 ττX˜(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)(cid:89)L ∥W lτ∥+∥W 1τX˜∥ F (cid:88)L (cid:13) (cid:13) (cid:13) (cid:13)d dW τkτ(cid:13) (cid:13) (cid:13) (cid:13) (cid:89) ∥W lτ∥
l=2 l=2 k=2 l∈[2:L]\{k}
(93)
≤uL−1(cid:34) (L−1)√ ρmu+(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)dW d1 ττX˜(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
+√ ρm(cid:88)L (cid:13) (cid:13) (cid:13) (cid:13)d dW τkτ(cid:13) (cid:13) (cid:13) (cid:13)(cid:35) .
F k=2
For l∈[2,L],
(cid:13) (cid:13) (cid:13) (cid:13)d d2 tW dτlτ(cid:13) (cid:13) (cid:13) (cid:13)≤(cid:18)(cid:13) (cid:13) (cid:13) (cid:13)d dΞ ττ(cid:13) (cid:13) (cid:13)
(cid:13)
+(L−1)∥Ξτ∥ F(cid:19) ∥W 1τX˜∥ F (cid:89) ∥W kτ∥
F k∈[2,L]\{l}
+ (cid:88) (cid:16) (1−τ)∥ΞτX˜⊤∥∥W 1τ∥+τ(L−1)∥Ξτ∥ F∥W 1τX˜∥ F(cid:17)(cid:13) (cid:13) (cid:13) (cid:13)d dW τjτ(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:89) ∥W kτ∥
j∈[2:L]\{l} k∈[2,L]\{l,j}
+(cid:32) (1−τ)∥ΞτX˜⊤∥(cid:13) (cid:13) (cid:13) (cid:13)d dW τ1τ(cid:13) (cid:13) (cid:13) (cid:13)+τ(L−1)∥Ξτ∥ F (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)dW d1 ττX˜(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
(cid:33) (cid:89) ∥W kτ∥ (94)
F k∈[2,L]\{l}
≤(cid:32) uL−1(cid:34) (L−1)ρu+ √1 m(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)dW d1 ττX˜(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
+ρ(cid:88)L (cid:13) (cid:13) (cid:13) (cid:13)d dW τkτ(cid:13) (cid:13) (cid:13) (cid:13)(cid:35) +¯1(L−1)(cid:33) ρuL−1
F k=2
+s¯uL−2 (cid:88) (cid:13) (cid:13) (cid:13) (cid:13)d dW τjτ(cid:13) (cid:13) (cid:13) (cid:13)+(cid:32) (1−τ)(s+βL)(cid:13) (cid:13) (cid:13) (cid:13)d dW τ1τ(cid:13) (cid:13) (cid:13) (cid:13)+τ¯1(L−1)√1 m(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)dW d1 ττX˜(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
(cid:33) uL−2.
j∈[2:L]\{l} F
For l=1,
(cid:13) (cid:13) (cid:13) (cid:13)d d2 tW dτ1τ(cid:13) (cid:13) (cid:13) (cid:13)≤(cid:18)(cid:13) (cid:13) (cid:13) (cid:13)d dΞ ττ(cid:13) (cid:13) (cid:13)
(cid:13)
+(L−1)∥Ξτ∥ F(cid:19) ∥X˜∥ (cid:89) ∥W kτ∥
F k∈[2,L]
+(cid:88)L (cid:16) (1−τ)∥ΞτX˜⊤∥+τ(L−1)∥Ξτ∥ F∥X˜∥(cid:17)(cid:13) (cid:13) (cid:13) (cid:13)d dW τjτ(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:89) ∥W kτ∥
j=2 k∈[2,L]\{j}
≤(cid:32) uL−1(cid:34) (L−1)ρu+ √1 m(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)dW d1 ττX˜(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)
+ρ(cid:88)L (cid:13) (cid:13) (cid:13) (cid:13)d dW τkτ(cid:13) (cid:13) (cid:13) (cid:13)(cid:35) +¯1(L−1)(cid:33) uL−1+s¯uL−2(cid:88)L (cid:13) (cid:13) (cid:13) (cid:13)d dW τjτ(cid:13) (cid:13) (cid:13) (cid:13).
F k=2 j=2
(95)
(cid:13)
(cid:13)dW
lτ(cid:13)
(cid:13) ∀l∈[L], as well as √1
(cid:13)
(cid:13)dW
1τX˜(cid:13)
(cid:13) , are all bounded by v(t) which satisfies
(cid:13) dτ (cid:13) m(cid:13) dτ (cid:13)
F
dv =(cid:2) (L−1)ρuL+(1+(L−1)ρ)uL−1v+(L−1)¯1(cid:3) uL−1+s¯(L−1)uL−2v
dt (96)
=v(cid:2) (1+(L−1)ρ)u2L−2+s¯(L−1)uL−2(cid:3) +(L−1)uL−1(cid:2)¯1+ρuL(cid:3)
, v(0)=0,
where ¯1=1+βL.
Solving the ODE for v(t). Recall
u(t)=(cid:0) s¯(2−L)t+β¯2−L(cid:1) 2−1
L . (97)
We solve the homogeneous equation to get
v(t)=C(t)e2L −− L1ln(s¯(2−L)t+β¯2−L)+1+(L s¯L−1)ρ(s¯(2−L)t+β¯2−L)2−L L
(98)
=C(t)e(L−1)lnu(t)+1+(L s¯L−1)ρuL(t) =C(t)uL−1(t)e1+(L s¯L−1)ρuL(t),
19where C(t) satisfies
dC(t) uL−1(t)e1+(L s¯L−1)ρuL(t) =(L−1)uL−1(t)(cid:2)¯1+ρuL(t)(cid:3)
. (99)
dt
Let us introduce sˆ= L s¯. Then
1+(L−1)ρ
(cid:90)
C(t)=(L−1)
e−uL sˆ(t) (cid:2)¯1+ρuL(t)(cid:3)
dt
(100)
L−1
(cid:20)¯1(cid:18) (cid:18)
2−L
βL(cid:19) (cid:18)
2−L
uL(t)(cid:19)(cid:19) sˆ(cid:18) (cid:18)
2
βL(cid:19) (cid:18)
2
uL(t)(cid:19)(cid:19)(cid:21)
2−L
= sˆ L Γ , −Γ , +ρ Γ , −Γ , .
L s¯ L sˆ L sˆ s¯ L sˆ L sˆ
This gives the final solution:
v(t)=
L−1 sˆ2− LL uL−1(t)[w(u(t))−w(β)]euL sˆ(t)
, (101)
L
where we took
¯1 (cid:18) 2−L uL(cid:19) Lρ (cid:18) 2 uL(cid:19)
w(u)=− Γ , − Γ , . (102)
s¯ L sˆ 1+(L−1)ρ L sˆ
B.2 Evaluating the solution at the learning time
For a properly initialized linear network, ∀l∈[L] ∥W0(t)∥=u¯(t), where u(t) satisfies [9]
l
du¯(t)
=u¯L−1(t)(s−u¯L(t)), u¯(0)=β, (103)
dt
which gives the solution in implicit form6:
(cid:90) du¯ u¯2−L(t) (cid:18) 2 2 u¯L(t)(cid:19) β2−L (cid:18) 2 2 βL(cid:19)
t∗(β)= = F 1, −1, ; − F 1, −1, ; . (104)
α u¯L−1(s−u¯L) s(2−L)2 1 L L s s(2−L)2 1 L L s
Suppose we are going to learn a fixed fraction of the data, i.e. take u¯(t)=(αs)1/L for α∈(0,1). Then
t∗(β)=
β2−L 2F 1(cid:16) 1, L2 −1, L2;β sL(cid:17) −(αs)L2−1 2F 1(cid:0) 1, L2 −1, L2;α(cid:1)
. (105)
α s(L−2)
Since t∗(β) is the time sufficient to learn a network for ϵ = 0, we suppose it also suffices to learn a nonlinear
α
network. So, we are going to evaluate our bound at t = t∗. Since we need Rˆ (t∗) < 1 for the bound to be
α γ α
non-vacuous, we should take γ small relative to α. We consider γ =αν/q for ν,q ≥1.
Sincethelinearnetworklearningtimet∗(β)iscorrectforalmostallinitializationonlywhenβ vanishes, weare
α
going to work in the limit of β →0. Since we need α∈(βL/s,1), otherwise the linear training time is negative,
we take α= rβλ for λ∈(0,L] and r >1.
s
Consider first λ<L:
β2−L
t∗(β)= +O(β2λ/L). (106)
α s(L−2)
Let us evaluate u at this time:
(cid:32) (cid:33) 1
(cid:16) s¯ (cid:17) 1 s¯2−L s¯ 2−L (cid:16) (cid:16) (cid:17)(cid:17)
u(t∗(β))= β¯2−L− β2−L+O(β2λ/L) 2−L =β ρ − 1+O βL−2+2λ/L . (107)
α s s¯2−L s
1
Apparently, this expression does not make sense for ρ=1 and even close to it, so we switch to λ=L, which
we expect to be the right exponent:
t∗(β)=β2−L1−rL2−1
+O(β2). (108)
α s(L−2)
6 2F1 isahypergeometricfunctiondefinedasaseries 2F1(a,b,c,z)=1+(cid:80)∞
k=1
(a) (k c)( kb)kz kk !,where(q) k=q(q+1)...(q+k−1).
20Let us evaluate u at this time:
(cid:32) (cid:33) 1
u(t∗ α(β))=(cid:16) β¯2−L− s s¯(cid:16) 1−rL2−1(cid:17) β2−L+O(β2)(cid:17) 2−1 L =β ss ¯¯ 22 ρ −− LL − s s¯(cid:16) 1−rL2−1(cid:17) 2−L (cid:0) 1+O(cid:0) βL(cid:1)(cid:1) . (109)
1
This expression makes sense whenever s s¯ ¯ρ2 2− −L
L
− s s¯(cid:16) 1−rL2−1(cid:17) >0, i.e. when r is close enough to 1.
1
Let us evaluate w at the training time now. Since Γ(a,x)=Γ(a)− xa +O(xa+1), we get
a
w(u(t∗ α(β)))−w(β)= ¯1 s¯2−L LsˆL L−2 (cid:0) u2−L(t∗ α(β))−β2−L(cid:1) +O(β2).
(cid:32) (cid:33) (110)
¯1 L s¯2−L s¯(cid:16) (cid:17)
= sˆL L−2 β2−L ρ − 1−rL2−1 −1 +O(β2).
s¯2−L s¯2−L s
1
Then the quantity of interest becomes
(cid:32) (cid:33)
LuL−1(t∗(β))v(t∗(β)) L−1 ¯1 L s¯2−L s¯(cid:16) (cid:17)
α
γ
α =
γ
u2L−2(t∗ α(β)) s¯2−Lβ2−L s¯ρ
2−L
−
s
1−rL2−1 −1 (1+O(βL))
1
(111)
(cid:32) (cid:33)2L−2 (cid:32) (cid:33)
qsν ¯1L(L−1) s¯2−L s¯(cid:16) (cid:17) 2−L s¯2−L s¯(cid:16) (cid:17)
= βL(1−ν) ρ − 1−rL2−1 ρ − 1−rL2−1 −1 (1+O(βL)).
rν s¯ 2−L s¯2−L s s¯2−L s
1 1
This expression does not diverge as β → 0 when ν = 1. We will also have a finite lim lim whenever
L→∞ β→0
ϵ∝(L−1)−1.
C Proving Assumption 4.1 for linear nets
We have for l=1,
(cid:13) (cid:13)2
∂(cid:13)Y −fϵ (X˜)(cid:13)
1 (cid:13) θϵ (cid:13) (cid:104) (cid:104) (cid:105)(cid:105)
∇ := F =− Dϵ ⊙Wϵ,⊤... Dϵ ⊙Wϵ,⊤Ξϵ X˜⊤. (112)
1 2m ∂Wϵ 1 2 L−1 L
1
For l∈[2,L],
(cid:13) (cid:13)2
∂(cid:13)Y −fϵ (X˜)(cid:13)
1 (cid:13) θϵ (cid:13) (cid:104) (cid:104) (cid:105)(cid:105)(cid:104) (cid:104) (cid:105)(cid:105)⊤
∇ := F =− Dϵ⊙Wϵ,⊤... Dϵ ⊙Wϵ,⊤Ξϵ Dϵ ⊙Wϵ ... Dϵ ⊙WϵX˜ . (113)
l 2m ∂Wϵ l l+1 L−1 L l−1 l−1 1 1
l
We also have
(cid:13)(cid:16) (cid:17) (cid:13)2
∂(cid:13) Y −fϵ (X˜) X˜⊤(cid:13)
1 (cid:13) θϵ (cid:13) (cid:104) (cid:104) (cid:105)(cid:105)
∇X := F =− Dϵ ⊙Wϵ,⊤... Dϵ ⊙Wϵ,⊤ΞϵX˜⊤X˜ X˜⊤. (114)
1 2m ∂Wϵ 1 2 L−1 L
1
(cid:13)(cid:16) (cid:17) (cid:13)2
∂(cid:13) Y −fϵ (X˜) X˜⊤(cid:13)
1 (cid:13) θϵ (cid:13) (cid:104) (cid:104) (cid:105)(cid:105)(cid:104) (cid:104) (cid:105)(cid:105)⊤
∇X := F =− Dϵ⊙Wϵ,⊤... Dϵ ⊙Wϵ,⊤ΞϵX˜⊤X˜ Dϵ ⊙Wϵ ... Dϵ ⊙WϵX˜ .
l 2m ∂Wϵ l l+1 L−1 L l−1 l−1 1 1
l
(115)
The statement of Assumption 4.1 follows from
Conjecture C.1. ∀ϵ∈[0,1] ∀t≥0
(cid:80)L tr(cid:2) ∇X∇⊤(cid:3)
≥0.
l=1 l l
Indeed, the above conjecture states that loss gradients wrt weights and ”projected” loss gradients wrt weights
are positively aligned, so, whenever loss does not increase, neither does projected loss. Since we use gradient flow,
loss is guaranteed to not increase. Below, we prove the conjecture for linear nets.
21Proof of Conjecture C.1 for ϵ=0. Since ϵ is zero, we omit the corresponding sup-index:
tr(cid:2) ∇X∇⊤(cid:3) =tr(cid:104)(cid:104) W⊤...W⊤ΞX˜⊤X˜(cid:105) X˜⊤X˜(cid:2) W⊤...W⊤Ξ(cid:3)⊤(cid:105)
1 1 2 L 2 L
=tr(cid:20)(cid:104) W⊤...W⊤ΞX˜⊤(cid:105)(cid:104) W⊤...W⊤ΞX˜⊤(cid:105)⊤(cid:21) =tr(cid:2)
W ...W
W⊤...W⊤(cid:3) tr(cid:2) ΞX⊤XΞ⊤(cid:3) (116)
2 L 2 L L 2 2 L
by the circular property of trace, and the fact that Ξ is a matrix with a single row. Since d =1, both traces
out
are just squared Euclidean norms of vectors, hence they are non-negative:
tr(cid:2) ∇X∇⊤(cid:3)
≥0.
1 1
Let us do the same for the other layers:
tr(cid:2) ∇X∇⊤(cid:3) =tr(cid:20)(cid:104) W⊤ ...W⊤ΞX˜⊤X˜(cid:105)(cid:104) W ...W X˜(cid:105)⊤(cid:104) W ...W X˜(cid:105)(cid:2) W⊤ ...W⊤Ξ(cid:3)⊤(cid:21)
l l l+1 L l−1 1 l−1 1 l+1 L
(cid:20)(cid:104) (cid:105)(cid:104) (cid:105)⊤(cid:21) (117)
=tr W⊤ ...W⊤ΞX˜⊤W⊤...W⊤ W⊤ ...W⊤ΞX˜⊤W⊤...W⊤
l+1 L 1 l−1 l+1 L 1 l−1
=tr(cid:2) W ...W W⊤ ...W⊤(cid:3) tr(cid:2) ΞX⊤W⊤...W⊤ W ...W XΞ⊤(cid:3) ≥0
L l+1 l+1 L 1 l−1 l−1 1
for the same reasons as before.
Clearly, Conjecture C.1 should also hold for small enough ϵ whenever it holds for ϵ=0. However, the bound
for the maximal ϵ for which we were able to guarantee the conjecture statement, vanishes with time t, as our
weight bounds are too loose. For this reason, we do not include it here. See Section 5 for empirical validation of
Assumption 4.1.
22