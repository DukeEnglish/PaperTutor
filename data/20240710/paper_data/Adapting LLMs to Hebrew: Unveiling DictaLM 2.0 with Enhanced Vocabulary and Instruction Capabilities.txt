TechnicalReport
Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced
Vocabulary and Instruction Capabilities
ShaltielShmidman1,†,AviShmidman1,2,‡,AmirDNCohen2,†,MosheKoppel1,2,†
1DICTA/Jerusalem,Israel
2BarIlanUniversity/RamatGan,Israel
†{shaltieltzion,moishk,amirdnc}@gmail.com
‡avi.shmidman@biu.ac.il
Abstract
Traininglargelanguagemodels(LLMs)inlow-resourcelanguagessuchasHebrewposesuniquechallenges.
Inthispaper,weintroduceDictaLM2.0andDictaLM2.0-Instruct,twoLLMsderivedfromtheMistral
model,trainedonasubstantialcorpusofapproximately200billiontokensinbothHebrewandEnglish.
Adaptingapre-trainedmodeltoanewlanguageinvolvesspecializedtechniquesthatdiffersignificantly
fromtrainingamodelfromscratchorfurthertrainingexistingmodelsonwell-resourcedlanguagessuchas
English.Weoutlinethesenoveltrainingmethodologies,whichfacilitateeffectivelearningandadaptationto
thelinguisticpropertiesofHebrew.Additionally,wefine-tunedDictaLM2.0-Instructonacomprehensive
instructdatasettoenhanceitsperformanceontask-specificinstructions. Torigorouslyevaluateourmodels,
weintroduceanewbenchmarksuiteforHebrewLLMevaluation,coveringadiversesetoftasksincluding
QuestionAnswering,SentimentAnalysis,WinogradSchemaChallenge,Translation,andSummarization.
OurworknotonlyaddressestheintricaciesoftrainingLLMsinlow-resourcelanguagesbutalsoproposes
aframeworkthatcanbeleveragedforadaptingotherLLMstovariousnon-Englishlanguages,contributing
tothebroaderfieldofmultilingualNLP.
1 Introduction
Thedevelopmentofgenerativelanguagemodelshassignificantlyadvancednaturallanguageprocessing
(NLP),enhancingthesophisticationandcontextualunderstandinginhuman-computerinteractions(Ope-
nAIetal.,2024;Teametal.,2024;Anthropic,2024). Leadingstate-of-the-artopen-weightgenerative
LLM models, such as Gemma and Llama (Gemma Team et al., 2024; AI@Meta, 2024), are trained
ontrillionsoftokens, butonlyaverysmallpercentageofthatdatarepresentslow-resourcelanguages
suchasHebrew(Touvronetal.,2023b). Consequently,thesemodelssignificantlyunderperforminsuch
languages.
Traininglargelanguagemodels(LLMs)inlow-resourcelanguagespresentsuniquechallenges,stem-
mingfromlimiteddataavailability,complexmorphologicalstructures,andthelackofrobustevaluation
frameworkstailoredtotheselanguages. Hebrew,withitsrichmorphologyandlimitedlarge-scalecorpora,
exemplifiesthesedifficulties. Thetokenizationprocessisparticularlyproblematic,asitisnotoptimized
forlowresourcelanguageslikeHebrew,leadingtoinefficientcompressionratiosandsuboptimalmodel
performance(Petrovetal.,2023).
Inresponsetothesechallenges,weintroduceDictaLM2.0andDictaLM2.0-Instruct,twogenerative
languagemodelsspecificallyoptimizedforHebrew. BuiltupontheMistralmodel,theseversionswere
trained on approximately 100 billion tokens each of Hebrew and English data. Adapting pre-trained
modelstoalow-resourcelanguagelikeHebrewinvolvesspecializedtechniquesthatdiffersignificantly
fromtrainingmodelsfromscratchorfurthertrainingexistingmodelsonwell-resourcedlanguagessuchas
English. TheseincludeextendingthetokenizerwithHebrew-specifictokensandperformingembedding
distillationtoensureeffectivelearningandadaptation.
Additionally,wefine-tunedDictaLM2.0-Instructonaninstructdatasettoenhanceitsperformance
on task-specific instructions. This fine-tuning process is crucial for improving the model’s ability to
understandandexecutetask-specificcommandsaccurately.
ToaddresstheevaluationgapinHebrewNLP,weintroduceanewbenchmarksuiteforassessingHebrew
languagemodels. ThissuiteincludestaskssuchasQuestionAnswering,SentimentAnalysis,Winograd
SchemaChallenge,Translation,andSummarization. Ourcomprehensiveevaluationdemonstratesthat
4202
luJ
9
]LC.sc[
1v08070.7042:viXraTechnicalReport
DictaLM2.0 and DictaLM2.0-Instruct achieve state-of-the-art performance on these tasks, thereby
settingnewstandardsforHebrewNLP.
Themethodologiesandevaluationframeworkspresentedinthisworkprovideinsightsandpotential
pathwaysforadaptingotherLLMstovariousnon-Englishlanguages. Thisresearchcontributestothe
broaderfieldofmultilingualNLPbyaddressingtheuniquechallengesposedbylow-resourcelanguages
andofferingscalablesolutionsfortheirintegrationintoadvancedlanguagemodels.
2 RelatedWork
Interest in the community for adapting existing generative LLMs to languages other than English has
beenincreasing. ThefirstnotableworkisChinese-LLaMA(Cuietal.,2024),whichextendedtheLLaMA
model(Touvronetal.,2023a)andtailoreditforChinese. Inthatwork,theyextendedthetokenizerofthe
modeltoincludeanadditional20,000Chinesetokens,significantlyimprovingthecompressionratio,and
subsequentlycontinuedpretrainingthemodelonalargeamountofChinesetext. Similarworkwasdone
byGroupetal.(2024),extendingLLaMA2toJapanese. However,asshownbyZhuetal.(2023),training
on cross-lingual translation data produces better performance compared to training on a monolingual
corpus. TheperformancedropinChinese-LLaMAseemstostemfromthevocabularyextension,which
requiresalargemonolingualcorpustotrain.
AsimilarapproachwastakenbyBasileetal.(2023),whoattemptedtoadapttheLLaMA2models
(Touvron et al., 2023b) to Italian, leaving the tokenizer as is. As they note, the model demonstrated
impressivereasoninginItalian,albeitthefluencyandlanguageskillswerenotuptoparwiththeEnglish
models.
Csakietal.(2023)attemptedadifferentapproachwhenadaptinglanguagemodelstonewlanguages.
Insteadofextendingthetokenizer,theyreplacedinfrequenttokenswithnewtokenstailoredforthetarget
language. IncontrastwithChinese-LLaMA,theyonlyreplacedaround5,000tokens,astheysawthat
replacingmoretokensresultedinmodelqualitydegradation.
Overall,therehavebeenmanyattemptstoadaptlanguagemodelstonewlanguages. Inthiswork,we
employahybridmethod,incorporatinglessonslearnedfromthesepreviousefforts.
3 Pre-trainingData
Ourpre-trainingcorpusconsistedof50%Hebrewand50%Englishdata. TheEnglishdatawassourced
from the Slim-Pajama corpus (Soboleva et al., 2023), sampled to match the quantity of Hebrew data
available. Intotal,ourHebrewcorpuscomprises35billionwords,thelargestcollectionofHebrewtexts
knowntous.
Thiscorpustranslatesinto95billiontokensusingourextendedtokenizer(seeSection4). Forcompari-
son,thesamecorpustranslatesintoapproximately204billiontokensusingtheMistraltokenizer(Jiang
etal.,2023),or196billiontokensusingtheGPT-4tokenizer.
InspiredbytheworksofZhuetal.(2023);Wendleretal.(2024),wealsoincludedacorpusofaligned
translation sentence pairs to help the model map between its internal English knowledge and Hebrew.
WeusedtheCCMatrixcorpusofHebrew-Englishpairs(Schwenketal.,2020)andconvertedeachpair
intotextusingsettemplates. Anexampletemplateis: "The translation of {X} in Hebrew: {Y}",
whereXisthesentenceinEnglish,andYisthesentenceinHebrew.
3.1 HebrewData
3.1.1 Sources
TheHebrewdatawascollectedfromawiderangeofsources,includingavailableopen-sourcecorpora,
internalwebscraping,Hebrewbooksscannedanddigitizedin-house,andpartnershipswithcompanies
that graciously provided their internal data for training. The data can be broken down into four main
sources:
• Internet: Approximately50%ofthedata. ThiscategoryincludeswebcrawlssuchasC4(Raffel
et al., 2019), OSCAR (Ortiz Suárez et al., 2019) and Wikipedia. We processed over 150TB of
unprocessedcrawldatawhenextractingthisdata.TechnicalReport
• SocialMedia: Approximately28%ofthedata. Thisdataincludesvarioussocialmediacorporasuch
asHebrewtwitterpostsandonlineHebrewblogs.
• News: Approximately16%ofthedata. ThisincludeshumantranscriptionsofTVandRadio,aswell
asvariousnewssites.
• Other: Approximately6%ofthedata. ThisincludesvarioussmallcorporasuchastheBenYehuda
project (Ben-Yehuda, 2024), the Sefaria project (Sefaria, 2024), and additional Hebrew books
digitizedin-house.
3.1.2 Cleaning&Filtering
Our entire Hebrew corpus underwent a rigorous cleaning and filtering process to eliminate gibberish,
low-quality,andirrelevantdata,andtocensorprivateinformation.
Cleaning: We began by replacing all sequences of content from languages other than Hebrew or
Englishwithadesignated<foreign>token,ensuringthatourcorpuscontainspurelyHebrewandEnglish
text. ThisstepwasparticularlyimportantforcorporalikeWikipedia,whereentriesoftenincludeforeign
languagetextinparentheses,especiallyatthebeginningofarticles.
Next, weremovedtextmarkerssuchasXML/HTMLtags, controlcharacters(e.g., LTRmark,non-
breakingspace),etc. Finally,wereplacedphonenumbers,URLs,andemailswithdesignatedtokens(e.g.,
<url>),andremovedduplicateparagraphs,keepingonlythefirstinstanceifthesameparagraphrepeated
morethantwiceinarow.
Filtering: We apply two levels of filtering to each document. The first level filters out documents
whichdonot meetaspecifiedthresholdintermsof thepercentageofHebrewcharacters. Thesecond
level involves statistical checks which verify that the text within the document reflects the normative
expectationsofaHebrewdocument;forinstance,weconstructedahistogramofalltheHebrewwordsin
agoldcorpusof 1Bwords,andthenforeachdocumentwecheckedtomakesurethatamajorityofthe
wordsinthenewdocumentwerefrequentinthegoldcorpus,thusensuringthatthecontentcontained
real Hebrew words. The filtering thresholds were adjusted for each data source, with a human expert
reviewingasampleofthefiltereddatatoconfirmthatthethresholdsweresetcorrectly.
3.1.3 De-duplication
WeranourentireHebrewcorpusthroughadeduplicationprocesstoremoveanydocumentduplicates.
WeusedtheMinHashalgorithm(Broder,1997)tocomputedocumentsimilarities,employinganLSH
index(IndykandMotwani,1998)forfastlookups. Wesetathresholdof0.8,meaningtwodocumentsare
consideredduplicatesiftheyhaveanestimatedJaccardsimilarityof80%orhigher.
4 ModelSetup
4.1 BaseModel
Leadingstate-of-the-artgenerativeLLMmodelsaretrainedwithtrillionsoftokens(GemmaTeametal.,
2024;AI@Meta,2024),makingtheirreproductionveryexpensive. Therefore,weinitializeourmodel
fromanexistingSOTAmodelandcontinuepretrainingit,leveragingthevastamountofdatathemodel
wasalreadytrainedon. WechosetoinitializeourmodelfromtheMistral-7B-v0.1checkpoint(Jiangetal.,
2023),whichwastheleadingSOTAopen-sourcemodelatthetimewebeganourexperiments.
4.2 TokenizerExtension
One major pitfall of the existing open-source SOTA models is the tokenization process. All modern
generativeLLMsuseatokenizerbasedontheBPEalgorithm(Sennrichetal.,2016),whichisastatistical
methodthatcreatestokensbasedonfrequencyinacorpus. DuetothescarcityofHebrewinthetraining
corpora of these models, there are very few Hebrew tokens in their vocabularies, resulting in a very
inefficientcompressionratio. Thisinefficiencyaffectsbothcontextlengthandcomputationalrequirements,
makingthemodelslesspracticalforuse. Specifically,theMistraltokenizerhasacompressionrateof5.81
tokensperHebrewword,whichaveragesapproximatelyonetokenpercharacter.TechnicalReport
Toovercomethis,wetakeinspirationfromtheworksofTouvronetal.(2023a);Csakietal.(2023),and
extendtheMistraltokenizerwith1,000Hebrew-specifictokens. AsshowninFigure1,addingjust1,000
tokensmorethanhalvesthecompressionrate,afterwhichthegainsfromaddingmoretokensdiminish
significantly.
Figure1: Thisfigureillustratestherelationshipbetweenthenumberofaddedtokensandthecompressionratioin
Hebrew(tokensperword).
Additionally, we heed the warning from Zhu et al. (2023) that extending the tokenizer can degrade
modelperformance,andperformadedicatedtrainingproceduretomitigatethisissue. Thetuningofthe
addedparametersissplitintotwostages:
1. Embedding Distillation: As noted by Zhu et al. (2023), models perform well when continuing
trainingwithoutextendingthetokenizerindicatingthatthereisknowledgeintheexistingembeddings.
Therefore,weaimtodistilltheexistingembeddingsinthebasemodelintothenewlyaddedtokens. We
sample a corpus of approximately 500,000 sentences in Hebrew and train the model to minimize the
followingloss:
L = ∥h −h ∥
old new 2
Where h denotes the last hidden state (as in, the last hidden state of the last token) of the output
old
fromthemodelwhenencodingasentenceusingtheMistraltokenizer,andh isthelasthiddenstateof
new
theoutputfromthemodelwhenencodingthesamesentenceusingourextendedtokenizer. Duringthis
trainingprocess, wefreezealltheweightsfromthemodelexceptthe1,000addedembeddingsonthe
embeddinglayer. Weranoneepochonthisdata,usinganinitiallearningrateof5e−6.
2. LM-HeadCalibration: Extendingthetokenizeraddsparameterstoboththeembeddinglayerand
thefinalLMHeadlayer. Aftertuningtheembeddinglayerparameters,theparametersaddedtotheLM
Headlayerremaininitializedatrandom. Inordertoefficientlycalibratethoseparameterstoalignwith
therestofthetrainedmodel,weperformanotherstageoftrainingwherewefreezeallmodelweights
excepttheembeddingandLMheadlayersallowingthemtotrainwithoutaffectingtherestofthemodel.
Wesampleacorpusof100,000documentsandtrainthemodelusingtheregularnext-tokenprediction
objective. Weranoneepochonthisdata,usinganinitiallearningrateof5e−6.
Weevaluatedthemodelafterthesetwostages,andtheresultsareshownintheevaluationsectionin
Figure 4. The model is titled dicta-il/dictalm2.0-untrained in the chart, and as can be seen this
processalonesignificantlyimprovesthemodel’sabilitytoprocessHebrewtext.1
1AsimilarapproachwasusedbyKimetal.(2024)intheirworkonefficientandeffectivevocabularyexpansion.Sincewe
hadalreadyreleasedourmodeltothepublicupondiscoveringthispaper,wedidnotdirectlycomparethemethods.TechnicalReport
5 ContinuousPre-training
WiththepreparedmodelfromSection4andthedatafromSection3,webeganthemaintrainingsession.
Thisprocess,calledcontinuouspre-training,involvescontinuingtopre-trainthemodelonthegeneral
next-token-predictionobjectiveusinglargeamountsofunsuperviseddata.
We conducted the pre-training on a compute cluster of 48 Intel Gaudi-2 chips, using the Optimum-
Habana codebase2 with DeepSpeed Zero-2 (Rajbhandari et al., 2020). The total training time for one
epochonthedatawas15days. DetailedtraininghyperparametersarelistedinTable1.
Globalbatchsize 384
Microbatchsize 8
Sequencelength 4096
Tokensperglobalbatch ~1.5M
Initiallearningrate 5e-6
Weightdecay 0.1
Warmup 1000steps
Scheduletype Cosine
Table1: Traininghyperparameters
To maximize the utilization of available compute power, we pre-processed the data into training
sequencesusingthepackingmethod(Lietal.,2021). EachdocumentwaswrappedwithaBOS(<s>)and
EOS(</s>)token. Toavoidcross-documentattentionduringtraining,weemployedadocument-attention
causalmask,ensuringeachdocumentattendedonlytoitself(seeFigure2).
ThetraininglossgraphisshowninFigure3. Thelossstartsataround2.5-3,thankstotheinitialtraining
doneduringthetokenizerextensionstage,andcontinuestodecreasethroughouttheepoch. Weselected
thefinalcheckpointbyreviewingthe10checkpointswiththelowestlosswithinthelast10thousandsteps,
andofthosewechosethecheckpointwiththehighestevaluationscores. Theresultingmodel, named
DictaLM2.0,isfreelyaccessibleandavailablefordownloadonHuggingFace3.
6 InstructTuning
Following the creation of our DictaLM2.0 model via continuous pre-training, we produced a second
model - a chat model - by transitioning to instruct tuning. Instruct tuning involves fine-tuning the
modelwithacurateddatasetcontaininginstructionsandcorrespondingresponses,enablingthemodel
to better understand and execute tasks based on explicit human instructions. This process enhances
themodel’sabilitytofollowcomplexcommandsandprovidemoreaccurateandcontextuallyrelevant
outputs. Leveraging transfer learning principles, instruct tuning uses the knowledge acquired during
continuouspre-trainingtoefficientlyadapttotask-specificdataandimproveperformanceondownstream
applications.
Fortheprocess,wefollowedthemethodologypresentedbyTunstalletal.(2023)usedtotraintheir
Zephyr-7B-betamodel. Thatmodeldemonstratedimpressiveperformance,anditsdetaileddocumentation
allowedustoreplicatetheirworkonourmodel. Alloftheircode,models,data,andtutorialsareavailable
onGitHub.4
6.1 SupervisedFine-tuning(SFT)
The first stage of instruct tuning is supervised fine-tuning (SFT). SFT involves training the model on
labeleddatasetswithexplicitlydefineddialogues. Thisphaseiscrucialtodevelopthemodel’scapability
togenerateaccurateresponsesandtoadheretodesiredbehaviorpatternsinreal-worldapplications.
WestartedbycollectingthetrainingdatasetforSFT.Totrainthemodeltofollowinstructionsinboth
EnglishandHebrew,wecuratedadatasetcontaininginstructionsinbothlanguages.
2https://github.com/huggingface/optimum-habana
3https://huggingface.co/dicta-il/dictalm2.0
4https://github.com/huggingface/alignment-handbook/tree/main/recipes/zephyr-7b-betaTechnicalReport
Figure2: Thisgraphdepictsthechangesappliedtotheregularcausalmask(darkgray)forourdocument-attention
causalmask(lightpurple),ensuringtokensfromseparatedocumentsaremaskedtorestrictcross-documentattention.
For the English corpus, we used the UltraChat_200k5 dataset provided by Tunstall et al. (2023), a
filtered version of the original UltraChat dataset (Ding et al., 2023) containing 1.4 million dialogues
generatedbyChatGPT.Additionally,wesampled100,000dialoguesfromtheOpenHermes2.5dataset
(Teknium,2023),followingthesuccessoftheNoun-Hermesmodels.
CollectingtheHebrewinstructioncorpuswasmorechallengingduetothelackofavailableinstruction
datasets in Hebrew. During our evaluation of the DictaLM2.0 base model (detailed in Section 5), we
determined that it is capable of producing high-quality translations between English and Hebrew (see
evaluationsinSections7.1and7.2). WeleveragedthiscapabilitytocreateourHebrewinstructiondataset,
byusingourbasemodeltotranslatethe100,000dialogueswesampledfromtheOpenHermes2.5dataset
and120,000dialoguessampledfromtheUltraChat_200kdatasetfromEnglishintoHebrew. Additionally,
we included 2,000 high-quality Hebrew dialogues with complex reasoning tasks, which we curated
manuallyin-house.
Wealsoincluded25,000synthetically-createddialoguesoftranslationsbetweenEnglishandHebrew,
usingapproximatley20differenttemplatestoconverttranslationpairsfromtheCCMatrixcorpusinto
dialogues.
SFT Training was conducted on a DGXA100 server with 8x40GB A100-SXM GPUs, using the
SFTTrainerfromtheTRLpackage(vonWerraetal.,2020)andDeepSpeedZero-3(Rajbhandarietal.,
2020). We trained with LoRA adapters (Hu et al., 2021), with rank = 64 and α = 16, applied to all
linearlayersexcepttheembeddinglayers. Weusedsequencesof4096tokenscombinedwiththepacking
method,abatchsizeof64,andtrainedforatotalof3epochs.
5https://huggingface.co/datasets/HuggingFaceH4/ultrachat_200kTechnicalReport
Figure3: Thisgraphdepictsthelossvalueduringthecontinuouspre-trainingstage.
6.2 DirectPreferenceOptimization(DPO)
ThenextstageinourmodelrefinementprocesswasDirectPreferenceOptimization(DPO)(Rafailovetal.,
2023). DPOfocusesonoptimizingthemodelbasedonuserpreferencesandfeedback,enhancingitsability
togenerateresponsesthatarebothaccurateandalignedwithuserexpectations. Byintegratingreal-world
feedbackintothetrainingloop,DPOhelpsfine-tunethemodel’sbehavior,makingitmoreadaptableand
responsivetonuanceduserneeds. Thisphaseensuresthemodelnotonlyfollowsinstructionsbutalso
evolvestoprovidecontextuallyappropriateanduser-centricoutputs.
WefollowedthemethodologyofTunstalletal.(2023)andusedtheUltraFeedback_binarizeddataset,a
preprocessedversionoftheUltraFeedbackdataset(Cuietal.,2023)preparedforDPOtraining. Asthis
datasetincludesonlyEnglishpairs,wetranslatedtheentiredatasetintoHebrewusingtheDictaLM2.0
basemodel,andthencombineditwiththeEnglishdata.
Duringourexperiments,wenoticedthatthemodelsometimesswitchedlanguageswithinaconversation.
Toaddressthis,weaddedacorpusofconversationpairswheretheacceptedandrejectedanswerswere
identical except for language, ensuring the model responded in the same language as the prompt, and
penalizingitifitdidn’t. WecreatedthisdatasetbysamplingentriesfromtheSFTdatasetinbothEnglish
andHebrew,makingupapproximately10%ofthefinalDPOcorpus.
DPO training was conducted on a DGXA100 server with 8x40GB A100-SXM GPUs, using the
DPOTrainerfromtheTRLpackage(vonWerraetal.,2020)andDeepSpeedZero-3(Rajbhandarietal.,
2020). WetrainedwithLoRAadapterss(Huetal.,2021), withrank = 64andα = 16, appliedtoall
linearlayersexcepttheembeddinglayers. Wesetbetato0.01(followingtheexperimentsdonebyTunstall
etal.(2023))andtrainedforoneepochwithalearningrateof5e−7andawarmupratioof10%.
WenamedthefinalmodelproducedfromtheSFTandDPOtrainingDictaLM2.0-instruct,anditis
freelyaccessibleandavailablefordownloadonHuggingFace.6
7 Evaluation
Evaluationisacriticalcomponentofmodeldevelopment,providingessentialmetricstoassessquality
and performance. Despite the advancements in developing DictaLM2.0 and DictaLM2.0-Instruct,
the initial scarcity of relevant evaluation sets for Hebrew LLMs posed a challenge in fully validating
their effectiveness, reliability, and robustness in various applications. To address this gap, we created
comprehensiveevaluationmethodsanddatasetstailoredspecificallyforbenchmarkingHebrewLLMs.
6https://huggingface.co/dicta-il/dictalm2.0-instructTechnicalReport
To thoroughly assess our models, we developed an evaluation framework that encompasses three
primarycategories,detailedbelow:
7.1 AutomaticEvaluation
Our first method of evaluation employs automatic assessment of the base model’s responses to tasks
usingfew-shotlearningwithahuman-curatedtestset. Automaticevaluationisessentialforproviding
scalable,consistent,andobjectivemetricstomeasuremodelperformanceacrossdiversetasks. Evaluating
generativelanguagemodelsposesuniquechallengesduetotheirtrainingobjectiveofnext-tokenprediction,
whichcomplicatesreliableassessmentonspecificdownstreamapplications. However,asdemonstratedby
Brownetal.(2020),languagemodelspossesstheabilityforin-contextlearning(ICL),allowingthem
to perform tasks with minimal examples (few-shot prompts). This capability facilitates the creation
offew-shotpromptsthateffectivelyguidethemodeltocomprehendthetaskandgenerateappropriate
responses. To rigorously evaluate the model’s capabilities, we curated four datasets designed to test
differentaspectsofthemodel’sperformance:
• Hebrew Question Answering: This task evaluates a model’s ability to understand and process
informationpresentedinHebrew,focusingoncomprehensionandtheaccurateretrievalofanswers
based on context. It checks the model’s grasp of Hebrew syntax and semantics through direct
question-and-answerformats.
– Source: ThetestsubsetoftheHeQdataset(Cohenetal.,2023),whichcontains1,436entries.
– Scoring: ResultsarescoredautomaticallyusingthetlnlsscoringmethodproposedbyCohen
etal.(2023),whichaccountsforthelinguisticpropertiesofHebrewlanguage.
– Few-ShotFormat: Eachcontextparagraphisfollowedbythreequestionsandanswers,and
finallythedesiredquestionunanswered.
• SentimentAnalysis: Thistaskteststhemodel’sabilitytodetectandinterpretsentimentsinHebrew
text. It assesses the model’s capability to classify statements accurately as positive, negative, or
neutralbasedonlinguisticcues.
– Source: HebrewSentiment(Project,2024)-aSentiment-AnalysisDatasetinHebrew. Because
manyofthesentencesinthedatasetaredubiousorambiguous,weemployedaprofessional
linguistwhoselectedasetof3,000fairlyunambiguouscasesfromthedataset.
– Scoring: Themodelmustpredictoneoftheclasses(Positive,Negative,Neutral),andaccuracy
iscomputedautomatically.
– Few-ShotFormat:Eachpromptincludesthesameninefew-shotexamples,threefromeach
category,randomlyshuffled.
• WinogradSchemaChallenge: Thistaskmeasuresthemodel’sunderstandingofpronounresolution
andcontextualambiguityinHebrew. Itteststhemodel’sabilitytouselogicalreasoningandgeneral
worldknowledgetodisambiguatepronounscorrectlyincomplexsentences.
– Source: AtranslationoftheWinogradSchemaChallengetotheHebrewlanguage(Shwartz,
2021).
– Scoring: Themodelmustchoosewhichoftwoentitiesthequestionrefersto,withtheoptions
presentedintheprompt. Accuracyiscomputedautomatically.
– Few-Shot Format: Each prompt includes five few-shot examples, with the input sentence,
question,possibleanswers,andexpectedanswer.
• Translation: Thistaskassessesthemodel’sproficiencyintranslatingbetweenEnglishandHebrew.
Itevaluatesthelinguisticaccuracy,fluency,andtheabilitytopreservemeaningacrosslanguages,
highlightingthemodel’scapabilityinbilingualtranslationtasks.TechnicalReport
– Source: NeuLabs-TedTalksalignedtranslationcorpus(Tiedemann,2012). Wesampled1,000
sentencesconfirmednottoappearinCCMatrix(sincetheCCMatrixwasincludedinourtraining
corpus). Wetestedtheabilityofthemodeltotranslateinbothdirections.
– Scoring: TranslationsarecomparedtothegoldtranslationfromthecorpususingtheBLEU
score(Papinenietal.,2002),thestandardscoringmethodformachinetranslation.
– Few-ShotFormat: Eachpromptincludesthreefew-shotexamplesofanEnglishsentenceand
itsHebrewequivalent(ortheconverse,forcasesofHebrewtoEnglishtranslation).
We present the results comparing our two models (DictaLM2.0 and DictaLM2.0-Instruct) to other
SOTAbasemodelsinthe7Bparameterrange,aswellastoothermodelsthatclaimtoperformwellon
Hebrew. TheresultsaredisplayedinFigure4. OurmodelsproduceSOTAresultsonmostofthesetasks,
particularlyregardingtheWinogradandTranslationbenchmarks.
Figure4: ComparisonofevaluationresultsbetweenourmodelandotherbasemodelsonHebrewfew-shottasks.
7.2 HumanEvaluation
Thesecondmethodofevaluationemployedwashumanjudgment,alsoknownas"HumanasaJudge."We
conductedablindtestinwhichhumanevaluatorswerepresentedwithtranslationsfromourbasemodel
alongside translations from Google Translate. The base model was provided with a few-shot prompt
containing three translation pairs preceding each input sentence to indicate the nature of the task. To
ensureunbiasedassessments,thetranslationswereanonymized,andevaluatorswereaskedtochoosethe
bettertranslationwithoutknowingwhichsystemproducedit. Thismethodoffersvaluableinsightintothe
comparativequalityofourmodel’stranslations.
Weconductedtheteston1,000sentencestranslatedfromEnglishtoHebrew. Theresults,presentedin
Figure5,showastrongpreferenceforthetranslationsproducedbyourmodeloverthosefromGoogle
Translate. Thesesuccessfulresultsledustosetupatranslationsiteforthepublictousefreeofcharge,
fortranslatingbetweenEnglishandHebrew,inbothdirections. Thesiteisavailableonline.7
7.3 LLM-basedEvaluation
Forthefinalevaluationmethod,weaimtoassessaHebrewchatmodel’sabilitytounderstandandfollow
instructions,aswellasthequalityofthegeneratedcontent. Inordertodoso,weutilizeasummarization
test set. We collected a corpus of 75 Hebrew news documents with human-curated summaries. We
then instruct each of the chat models to generate summaries for each of the documents. Summaries
aregeneratedwithatemperatureof0.7usingagenericpromptdescribingthetaskofsummarizingthe
documentinashort4-5sentenceparagraph. WeuseGPT-4asajudgetoscorethesummaries.
7https://translate.dicta.org.ilTechnicalReport
Figure5: HumanevaluationresultsfromablindtestcomparingtranslationsfromourmodelandGoogleTranslate.
For each model, the original documents and the generated summaries were presented to GPT-4 for
evaluation. Thehuman-curatedsummarieswerescoredaswellinordertoprovideabaselinetocompare
themodelsummariesto. WeusedthescoringpromptsdescribedinOpenAI’scookbook,8 adjustingthem
toa1-10scale. GPT-4scoredeachsummaryonfourmeasures:
• Relevance-Selectingimportantcontentfromtheoriginalsource.
• Coherence-Thecollectivequalityofallsentences.
• Consistency-Factualalignmentbetweenthesummaryandtheoriginalcontent.
• Fluency - Quality of the summary in terms of grammar, spelling, punctuation, word choice, and
sentencestructure.
The results are presented in Table 2. We compare the performance of our chat model to popular
proprietarymodelsfromOpenAIandAnthropic,aswellastopopularopen-sourceinstruct-tunedmodels
likeGemma-7B-it(GemmaTeametal.,2024)andLLaMA-3-8B-Instruct(AI@Meta,2024). Weevaluate
ourchatmodelbothafterSFTinstructtrainingandaftertheDPOtraining. TheDPOtrainingsignificantly
improvesthemodel’sperformance. Whileourmodelisnotyetcompetitivewithproprietarymodels,it
providesahigh-qualityopen-sourcealternativeforthoselookingtofine-tuneamodelforspecificneeds.
Model Relevance Coherence Consistency Fluency
google/gemma-7b-it 6.29 5.37 7.25 7.19
meta-llama/Llama-3-8B-Instruct 7.7 6.96 8.53 8.14
GPT3.5Turbo 7.55 6.76 8.4 8.12
DictaLM2.0Instruct(SFT) 7.45 6.85 7.93 8.17
DictaLM2.0Instruct(SFT+DPO) 8.1 7.45 8.34 8.54
Claude3Haiku 8.73 7.95 9.67 8.69
GPT411-06Preview 8.65 7.96 9.48 8.69
Human 8.33 7.66 9.42 8.59
Table2: Evaluationofinstruct-tunedmodelsontheLLM-evaluatedsummarizationtask. TheDictaLM2.0-Instruct
modelispresentedbothafterSFTandafterDPOtodemonstratetheimportanceandefficacyoftheDPOtraining.
7.4 HebrewOpenLLMLeaderboard
We compiled all the benchmarks from Section 7.1 and created a Hebrew Open LLM Leaderboard,
whichisnowpubliclyavailable. Thisleaderboardservesasacalltoactionforresponsibleresearchand
development in Hebrew LLMs. For the first time, it offers a consistent benchmark setup that ensures
reliableresults,makingiteasierforeveryonetoevaluateandcomparemodelseffectively. Byproviding
8https://cookbook.openai.com/examples/evaluation/how_to_eval_abstractive_summarizationTechnicalReport
thisresource,weaimtofosterinnovationandcollaborationwithinthecommunity. Theleaderboardcan
beaccessedhere: https://huggingface.co/spaces/hebrew-llm-leaderboard/leaderboard.
8 Conclusion
ThedevelopmentandreleaseofDictaLM2.0andDictaLM2.0-Instructrepresentmajoradvancements
in the field of Hebrew natural language processing. By addressing the unique challenges of Hebrew
andotherlowresourcelanguges, wehaveoptimizedthesegenerativemodelsforhighperformancein
Hebrew. Throughrigorousevaluation,bothautomatedandhuman-judged,ourmodelshavedemonstrated
state-of-the-artperformanceacrossvariousNLPtaskssuchasquestionanswering,sentimentanalysis,
andmachinetranslation.
Furthermore,weintroducedacomprehensivebenchmarksuiteandanopenHebrewLLMleaderboard,
providingessentialresourcesforfutureresearchandevaluationinHebrewNLP.Thisworknotonlysets
newstandardsforHebrewlanguagemodelsbutalsooffersmethodologiesthatcanbeadaptedtoother
low-resourcelanguages,contributingtothebroaderfieldofmultilingualNLP.
Inconclusion,DictaLM2.0andDictaLM2.0-Instructrepresentrobust,open-sourcesolutionsthat
pavethewayforfurtherinnovationsinHebrewNLP.Byfosteringongoingresearchanddevelopment
within the community, these models and resources support the advancement of NLP capabilities for
low-resourcelanguages,enhancingtheinclusivityanddiversityoflanguagetechnologies.
Acknowledgements
We would like to express our thanks to Intel NLP Labs Israel: Peter Izsak, Daniel Fleischer, Moshe
BerchanskyandMosheWasserblat,fortheirexpertiseandsupportintrainingthemodelontheIntelGaudi
systems.
WewouldalsoliketothankAvnerAlgomandtheIsraeliAssociationofHumanLanguageTechnologies
(IAHLT)formakingtheconnectionwithIntelandtherebypavingthewayforthecreationofthismodel.
WewouldalsoliketothankTalGevaandtheDDR&DIMOD/TheIsraeliNationalProgramforNLP
inHebrewandArabicfortheirinvaluablecontributionswithoutwhichthisprojectwouldn’thavebeen
possible.
References
AI@Meta.2024. Llama3modelcard.
Anthropic.2024. Theclaude3modelfamily: Opus,sonnet,haiku. PaperswithCode.
PierpaoloBasile,ElioMusacchio,MarcoPolignano,LuciaSiciliani,GiuseppeFiameni,andGiovanniSemeraro.
2023. Llamantino: Llama2modelsforeffectivetextgenerationinitalianlanguage.
Ben-Yehuda.2024. Benyehudaproject.
AndreiZ.Broder.1997. Ontheresemblanceandcontainmentofdocuments. InCompressionandComplexityof
Sequences1997.Proceedings,pages21–29.IEEE.
TomB.Brown,BenjaminMann,NickRyder,MelanieSubbiah,JaredKaplan,PrafullaDhariwal,ArvindNeelakan-
tan,PranavShyam,GirishSastry,AmandaAskell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,Tom
Henighan,RewonChild,AdityaRamesh,DanielM.Ziegler,JeffreyWu,ClemensWinter,ChristopherHesse,
MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,JackClark,ChristopherBerner,Sam
McCandlish,AlecRadford,IlyaSutskever,andDarioAmodei.2020. Languagemodelsarefew-shotlearners.
AmirCohen,HillaMerhav-Fine,YoavGoldberg,andReutTsarfaty.2023. HeQ:alargeanddiverseHebrewreading
comprehensionbenchmark. InFindingsoftheAssociationforComputationalLinguistics: EMNLP2023,pages
13693–13705,Singapore.AssociationforComputationalLinguistics.
ZoltanCsaki,PianPawakapan,UrmishThakker,andQiantongXu.2023. Efficientlyadaptingpretrainedlanguage
modelstonewlanguages.
GanquCui,LifanYuan,NingDing,GuanmingYao,WeiZhu,YuanNi,GuotongXie,ZhiyuanLiu,andMaosong
Sun.2023. Ultrafeedback: Boostinglanguagemodelswithhigh-qualityfeedback.TechnicalReport
YimingCui,ZiqingYang,andXinYao.2024. Efficientandeffectivetextencodingforchinesellamaandalpaca.
NingDing,YulinChen,BokaiXu,YujiaQin,ZhiZheng,ShengdingHu,ZhiyuanLiu,MaosongSun,andBowen
Zhou.2023. Enhancingchatlanguagemodelsbyscalinghigh-qualityinstructionalconversations.
Thomas Mesnard Gemma Team, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Laurent Sifre, Morgane
Rivière,MihirSanjayKale,JulietteLove,PouyaTafti,LéonardHussenot,andetal.2024. Gemma.
Rakuten Group, Aaron Levine, Connie Huang, Chenguang Wang, Eduardo Batista, Ewa Szymanska, Hongyi
Ding,HouWeiChou,Jean-FrançoisPessiot,JohanesEffendi,JustinChiu,KaiTorbenOhlhus,KaranChopra,
KeijiShinzato,KojiMurakami,LeeXiong,LeiChen,MakiKubota,MaksimTkachenko,MirokuLee,Naoki
Takahashi,PrathyushaJwalapuram,RyutaroTatsushima,SaurabhJain,SunilKumarYadav,TingCai,Wei-Te
Chen,YandiXia,YukiNakayama,andYutakaHigashiyama.2024. Rakutenai-7b: Extendinglargelanguage
modelsforjapanese.
EdwardJ.Hu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhu
Chen.2021. Lora: Low-rankadaptationoflargelanguagemodels.
PiotrIndykandRajeevMotwani.1998. Approximatenearestneighbors: Towardsremovingthecurseofdimen-
sionality. In Proceedings of the thirtieth annual ACM symposium on Theory of computing, pages 604–613.
ACM.
AlbertQ.Jiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSinghChaplot,Diegodelas
Casas,FlorianBressand,GiannaLengyel,GuillaumeLample,LucileSaulnier,LélioRenardLavaud,Marie-Anne
Lachaux,PierreStock,TevenLeScao,ThibautLavril,ThomasWang,TimothéeLacroix,andWilliamElSayed.
2023. Mistral7b.
SeungdukKim,SeungtaekChoi,andMyeonghoJeong.2024. Efficientandeffectivevocabularyexpansiontowards
multilinguallargelanguagemodels.
MingLi,WeiZhang,JianMa,etal.2021. Efficientsequencepackingwithoutcross-contamination: Accelerating
largelanguagemodelswithoutimpactingperformance. arXivpreprintarXiv:2107.02027.
OpenAI,JoshAchiam, StevenAdler, SandhiniAgarwal, LamaAhmad, IlgeAkkaya, FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,RedAvila,IgorBabuschkin,SuchirBalaji,
etal.ValerieBalcom,PaulBaltescu,HaimingBao,MohammadBavarian,JeffBelgum,andetal.2024. Gpt-4
technicalreport.
Pedro Javier Ortiz Suárez, Benoît Sagot, and Laurent Romary. 2019. Asynchronous pipelines for processing
hugecorporaonmediumtolowresourceinfrastructures. ProceedingsoftheWorkshoponChallengesinthe
ManagementofLargeCorpora(CMLC-7)2019.Cardiff,22ndJuly2019,pages9–16,Mannheim.Leibniz-
InstitutfürDeutscheSprache.
KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002. Bleu: amethodforautomaticevaluationof
machinetranslation. InProceedingsofthe40thAnnualMeetingoftheAssociationforComputationalLinguistics,
pages311–318,Philadelphia,Pennsylvania,USA.AssociationforComputationalLinguistics.
Aleksandar Petrov, Emanuele La Malfa, Philip H. S. Torr, and Adel Bibi. 2023. Language model tokenizers
introduceunfairnessbetweenlanguages.
Hebrew-ArabicNLPProject.2024. Hebrewsentiment-asentiment-analysisdatasetinhebrew.
RafaelRafailov,ArchitSharma,EricMitchell,StefanoErmon,ChristopherD.Manning,andChelseaFinn.2023.
Directpreferenceoptimization: Yourlanguagemodelissecretlyarewardmodel.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,
andPeterJ.Liu.2019. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer. arXiv
e-prints.
SamyamRajbhandari,JeffRasley,OlatunjiRuwase,andYuxiongHe.2020. Zero: Memoryoptimizationstoward
trainingtrillionparametermodels.
HolgerSchwenk,GuillaumeWenzek,SergeyEdunov,EdouardGrave,andArmandJoulin.2020. Ccmatrix: Mining
billionsofhigh-qualityparallelsentencesontheweb.
Sefaria.2024. Sefaria: Alivinglibraryofjewishtexts.TechnicalReport
RicoSennrich,BarryHaddow,andAlexandraBirch.2016. Neuralmachinetranslationofrarewordswithsubword
units. InProceedingsofthe54thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:
LongPapers),pages1715–1725,Berlin,Germany.AssociationforComputationalLinguistics.
VeredShwartz.2021. Atranslationofthewinogradschemachallengetohebrew.
DariaSoboleva,FaisalAl-Khateeb,RobertMyers,JacobRSteeves,JoelHestness,andNolanDey.2023. SlimPa-
jama: A 627B token cleaned and deduplicated version of RedPajama. https://www.cerebras.net/blog/
slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama.
GeminiTeam,RohanAnil,SebastianBorgeaud,Jean-BaptisteAlayrac,JiahuiYu,RaduSoricut,JohanSchalkwyk,
AndrewM.Dai,AnjaHauth,andetal.2024. Gemini: Afamilyofhighlycapablemultimodalmodels.
Teknium.2023. Openhermes2.5: Anopendatasetofsyntheticdataforgeneralistllmassistants.
JörgTiedemann.2012. Paralleldata,toolsandinterfacesinOPUS. InProceedingsoftheEighthInternational
ConferenceonLanguageResourcesandEvaluation(LREC’12),pages2214–2218,Istanbul,Turkey.European
LanguageResourcesAssociation(ELRA).
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,Baptiste
Rozière,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,ArmandJoulin,EdouardGrave,and
GuillaumeLample.2023a. Llama: Openandefficientfoundationlanguagemodels.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,NikolayBashlykov,
SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,LukasBlecher,CristianCantonFerrer,Moya
Chen, GuillemCucurull, DavidEsiobu, JudeFernandes, JeremyFu, WenyinFu, BrianFuller, CynthiaGao,
VedanujGoswami,NamanGoyal,AnthonyHartshorn,SagharHosseini,RuiHou,HakanInan,MarcinKardas,
ViktorKerkez,MadianKhabsa,IsabelKloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,
ThibautLavril,JenyaLee,DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,TodorMihaylov,Pushkar
Mishra,IgorMolybog,YixinNie,AndrewPoulton,JeremyReizenstein,RashiRungta,KalyanSaladi,Alan
Schelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,BinhTang,RossTaylor,
AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,Melanie
Kambadur,SharanNarang,AurelienRodriguez,RobertStojnic,SergeyEdunov,andThomasScialom.2023b.
Llama2: Openfoundationandfine-tunedchatmodels.
LewisTunstall,EdwardBeeching,NathanLambert,NazneenRajani,KashifRasul,YounesBelkada,ShengyiHuang,
LeandrovonWerra,ClémentineFourrier,NathanHabib,NathanSarrazin,OmarSanseviero,AlexanderM.Rush,
andThomasWolf.2023. Zephyr: Directdistillationoflmalignment.
LeandrovonWerra, YounesBelkada, LewisTunstall, EdwardBeeching, TristanThrush, NathanLambert, and
ShengyiHuang.2020. Trl: Transformerreinforcementlearning. https://github.com/huggingface/trl.
ChrisWendler,VeniaminVeselovsky,GiovanniMonea,andRobertWest.2024. Dollamasworkinenglish? onthe
latentlanguageofmultilingualtransformers.
WenhaoZhu,YunzheLv,QingxiuDong,FeiYuan,JingjingXu,ShujianHuang,LingpengKong,JiajunChen,and
LeiLi.2023. Extrapolatinglargelanguagemodelstonon-englishbyaligninglanguages.