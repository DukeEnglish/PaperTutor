Can Learned Optimization Make Reinforcement
Learning Less Difficult?
AlexanderD.Goldie∗12 ChrisLu1 MatthewT.Jackson12
ShimonWhiteson†2 JakobN.Foerster†1
1FLAIR,UniversityofOxford 2WhiRL,UniversityofOxford
Abstract
Whilereinforcementlearning(RL)holdsgreatpotentialfordecisionmakinginthe
realworld,itsuffersfromanumberofuniquedifficultieswhichoftenneedspecific
consideration. Inparticular: itishighlynon-stationary;suffersfromhighdegrees
ofplasticityloss; andrequiresexplorationtopreventprematureconvergenceto
localoptimaandmaximizereturn. Inthispaper,weconsiderwhetherlearnedopti-
mizationcanhelpovercometheseproblems. Ourmethod,LearnedOptimization
forPlasticity,ExplorationandNon-stationarity(OPEN1),meta-learnsanupdate
rulewhoseinputfeaturesandoutputstructureareinformedbypreviouslyproposed
solutionstothesedifficulties. Weshowthatourparameterizationisflexibleenough
toenablemeta-learningindiverselearningcontexts,includingtheabilitytouse
stochasticityforexploration. Ourexperimentsdemonstratethatwhenmeta-trained
onsingleandsmallsetsofenvironments,OPENoutperformsorequalstradition-
ally used optimizers. Furthermore, OPEN shows strong generalization across a
distributionofenvironmentsandarangeofagentarchitectures.
1 Introduction
Reinforcementlearning[1,RL]hasundergonesignificantadvancesinrecentyears,scalingfrom
solvingcomplexgames[2,3]towardsapproachingrealworldapplications[4–7]. However,RLis
limitedbyanumberofdifficultieswhicharenotpresentinothermachinelearningdomains,requiring
thedevelopmentofnumeroushand-craftedworkaroundstomaximizeitsperformance.
Here,wetakeinspirationfromthreedifficultiesofRL:non-stationarityduetocontinuouslychanging
inputandoutputdistributions[8];highdegreesofplasticitylosslimitingmodelcapacities[9,10];
andexploration,whichisneededtoensureanagentdoesnotconvergetolocaloptimaprematurely
[1, 11]. Overcoming these challenges could enable drastic improvements in the performance of
RL,potentiallyreducingthebarrierstoapplicationsofRLinthereal-world. Thusfar,approaches
totackletheseproblemshavereliedonhumanintuitiontofindhand-crafted solutions. However,
thisisfundamentallylimitedbyhumanunderstanding. Meta-RL[12]offersanalternativeinwhich
RLalgorithmsthemselvesarelearnedfromdataratherthandesignedbyhand. Meta-learnedRL
algorithmshavepreviouslydemonstratedimprovedperformanceoverhand-craftedones[13–15].
Onarelatednote,learnedoptimizationhasprovensuccessfulinsupervisedandunsupervisedlearning
(e.g.,VeLO[16]). Learnedoptimizersaregenerallyparameterizedupdaterulestrainedtooutperform
handcraftedalgorithmslikegradientdescent. However,currentlearnedoptimizersperformpoorlyin
RL[16,17]. WhilesomemayarguethatRLissimplyanout-of-distributiontask[16],thelackof
∗Correspondencetogoldie@robots.ox.ac.uk.
†Equalsupervision.
1Open-sourcecodeisavailablehere.
AcceptedtotheAutoRLWorkshop@ICML2024.
4202
luJ
9
]GL.sc[
1v28070.7042:viXraconsiderationfor,andinabilitytotarget,specificdifficultiesofRLinnaïvelearnedoptimizersleads
ustobelievethattheywouldstillunderperformevenifRLwereinthetrainingdistribution.
Weproposeanalgorithm,showninFigure1,formeta-learningoptimizersspecificallyforRLcalled
LearnedOptimizationforPlasticity,ExplorationandNonstationarity(OPEN). OPENmeta-learns
updateruleswhoseinputsarerootedinpreviouslyproposedsolutionstothedifficultiesabove,and
whoseoutputsuselearnablestochasticitytoboostexploration. OPENistrainedtomaximizefinal
returnonly,meaningitdoesnotuseregularizationtomitigatethedifficultiesitisdesignedaround.
Inourresults(Section6),webenchmarkOPENagainsthandcraftedoptimizers(Adam[18],RMSProp
[19]), open-source discovered/learned optimizers (Lion [20], VeLO [16]) and baseline learned
optimizerstrainedforRL(Optim4RL[17],‘NoFeatures’). WeshowthatOPENcanfittosingleand
smallsetsofenvironmentsandalsogeneralizein-andout-of-supportofitstrainingdistribution. We
furtherfindthatOPENgeneralizesbetterzero-shotthanAdam[18]inourgridworldsetting.
Figure1: AvisualizationofOPEN. WetrainN agents,replacingthehandcraftedoptimizerofthe
RLloopwithonessampledfromthemeta-learner(i.e.,evolution). Eachoptimizerconditionson
gradient,momentumandadditionalinputs,detailedinSection5.3,tocalculateupdates. Thefinal
returnsfromeachloopareoutputtothemetalearner,whichimprovestheoptimizerbeforerepeating
theprocess. AsingleinnerloopstepisdescribedalgorithmicallyinAppendixB.1.
2 Background
ReinforcementLearning TheRLproblemisformulatedasaMarkovDecisionProcess[1,MDP]
described by the tuple ⟨A,S,P,R,ρ,γ⟩. At discrete timestep t, the agent takes action a ∈ A
t
sampledfromits(possiblystochastic)policy, π(·|s ) ∈ Π, whichconditionsonthecurrentstate
t
s ∈S(wheres ∼ρ).Aftereachaction,theagentreceivesrewardR(s ,a )andthestatetransitions
t 0 t t
tos ,basedonthetransitiondynamicsP(s |s ,a ). Anagent’sobjectiveistomaximizeits
t+1 t+1 t t
discountedexpectedreturn,Jπ,correspondingtoadiscountfactorγ ∈ [0,1),whichpreventsthe
agentmakingmyopicdecisions. Thisisdefinedas
(cid:34) ∞ (cid:35)
(cid:88)
Jπ :=E γtR . (1)
a0:∞∼π,s0∼ρ,s1:∞∼P t
t=0
Proximalpolicyoptimization[21,PPO]isanalgorithmdesignedtomaximizeJπ. Itusesadvantage,
Aπ(s,a),whichiscalculatedfromthestatevaluefunction,Vπ =E [(cid:80)∞ γtR |S =s],andstate-
π t=0 t t
actionvaluefunction,Qπ(s,a)=E [(cid:80)∞ γtR |S =s,A =a]. Itmeasurestheimprovementof
π t=0 t t t
aspecificactionoverthecurrentpolicyandtakestheform
Aπ(s,a)=Qπ(s,a)−Vπ(s). (2)
PPOintroducesalossforoptimizingthepolicy,parameterizedbyθ,thatpreventsextremepolicy
updatesingradientascent. Thisusesclipping,whichensuresthereisnobenefittoupdatingθbeyond
wherethepolicyprobabilityratio,r (θ)= πθ(at|st) ,exceedstherange[1±ϵ]. Theclippedlossis
t πθold(at|st)
LCLIP(θ)=E[min(r (θ)Aπ(s ,a ),clip(r (θ),1±ϵ)Aπ(s ,a ))]. (3)
t t t t t t
PPOisanactor-critic[1]method,wherethepolicyandvaluefunctionsaremodeledwithdifferent
neural networks, or separate heads of the same neural network, conditioning on state. The PPO
objectivetomaximizecombinestheclippedloss,avaluefunctionerror,andanentropybonusinto
L (θ)=E[LCLIP(θ)−c LVF(θ)+c S[π ](s )]. (4)
t t 1 t 2 θ t
2Meta-LearningOptimizersinRL AlgorithmslikeAdam[18]orRMSprop[19]aredesignedto
maximizeanobjectivebyupdatingtheparametersofaneuralnetworkinthedirectionofpositive
gradientwithrespecttothefunction. Theyareoftenappliedwithaugmentationssuchasmomentum
or learning rate schedules to better converge to optima. Learned optimizers offer an alternative:
parameterizedupdaterules,conditioningonmorethanjustgradient,whicharetrainedtomaximize
anobjective[22,16,23]. Foranyparameterizedoptimizeropt ,whichconditionsonasetofinputs
ϕ
x,theupdateruletoproduceupdateucanbedescribedasafunction,opt (x)→u.
ϕ
We treat learning to optimize as a meta-RL problem [12]. In meta-RL, the goal is to maximize
Jπ overadistributionofMDPsP(M). Forourtask, anoptimizertrainedtomaximizeJ(ϕ) =
E (cid:2) Jπ|opt (cid:3) yieldstheoptimalmeta-parameterizationopt .
M ϕ ϕ∗
EvolutionStrategies Evolutionalgorithms(EA)areabackpropagation-free,black-boxmethod
foroptimization[24]whichusesapopulationofperturbedparameterssampledfromadistribution
(here,θˆ∼N(θ,σ2I)). ThispopulationisusetomaximizeafitnessF(·). EAencompassesarange
oftechniques(e.g.,evolutionstrategies(ES)[25,26],geneticalgorithms[27]orCMA-ES[28]).
Naturalevolutionstrategies(NES)[29]areaclassofESmethodsthatusethepopulationfitness
toestimateanaturalgradientforthemeanparameters,θ. Thiscanthenbeoptimizedwithtypical
gradientascentalgorithmslikeAdam[18]. Salimansetal.[25]introduceOpenAIESforoptimizing
θusingtheestimator
1
∇ E F(θ+σϵ)= E {F(θ+σϵ)ϵ}, (5)
θ ϵ∼N(0,I) σ ϵ∼N(0,I)
whichisapproximatedusingapopulationaverage. Inpractice,weuseantitheticsampling(i.e.,for
eachsampledϵ, evaluating+ϵand−ϵ)[30]. Antithetictask samplingenableslearningonatask
distribution,byevaluatingandrankingeachantitheticpairondifferenttasks[31].
Historically,RLwastooslowforEStobepracticalformeta-training. However,PureJaxRL[15]re-
centlydemonstratedthefeasibilityofESformeta-RL,owingtothespeedupenabledbyvectorization
inJax[32]. WeusetheimplementationofOpenAIES[25]fromevosax[33].
3 RelatedWork
3.1 OptimizationinRL
Wilsonetal.[34]andReddietal.[35]showthatadaptiveoptimizersstruggleinhighlystochastic
processes. Henderson et al. [36] indicate that, unlike other learning regimes, RL is sufficiently
stochasticforthesefindingstoapply,whichsuggestsRL-specificoptimizerscouldbebeneficial.
TheideathatoptimizersdesignedforsupervisedlearningmaynotperfectlytransfertoRLisreflected
byBengioetal.[37],whoproposeanamendedmomentumsuitablefortemporaldifferencelearning
[38]. ThisisrelatedtoworkbySarigülandAvci[39],whoexploretheimpactofdifferenttypesof
momentumonRL.WhiletheseworksmotivatedesigningoptimizationtechniquesspecificallyforRL,
wetakeamoreexpressiveapproachbyreplacingthewholeoptimizerinsteadofjustitsmomentum
calculation,andusingmeta-learningtofitouroptimizertodataratherthanrelyingonpotentially
suboptimalhumanintuition.
3.2 Meta-learning
DiscoveringRLAlgorithms Ratherthanusinghandcraftedalgorithms,arecentobjectiveinmeta-
RLisdiscoveringRLalgorithmsfromdata.Whiletherearemanysuccessesinthisarea(e.g.,Learned
PolicyGradient[13,LPG],MetaGenRL[14]andLearnedPolicyOptimisation[15,LPO]),wefocus
onmeta-learningareplacementtotheoptimizerduetotheoutsizedimpactalearnedupdaterulecan
haveonlearning. WealsousespecificdifficultiesofRLtoguidethedesignofourmethod,rather
thansimplyapplyingend-to-endlearning.
Jacksonetal.[31]learntemporally-awareversionsofLPOandLPG.Whiletheirapproachoffers
inspirationfordealingwithnon-stationarityinRL,theyalsorelyonAdam[18],anoptimizerdesigned
forstationaritythatissuboptimalforRL[9]. Instead,weproposereplacingtheoptimizeritselfwith
anexpressiveanddynamicupdaterulethatisnotsubjecttotheseproblems.
3LearnedOptimization Learningtooptimize[22,40,41,L2O]strivestolearnreplacementsto
handcraftedgradient-basedoptimizerslikeAdam[18],generallyusingneuralnetworks(e.g.,[23,42–
44]). Whiletheyshowstrongperformanceinsupervisedandunsupervisedlearning[16,45],previous
learnedoptimizersdonotconsidertheinnatedifficultiesofRL,limitingtransferability. Furthermore,
whileVeLOused4000TPU-monthsofcompute[16],OPENrequiresontheorderofoneGPU-month.
Lion[20]isasymbolicoptimizerdiscoveredbyevolutionthatoutperformsAdamW[46]usinga
similarupdateexpression. Whilethesimplistic,symbolicformofLionletsitgeneralizetoRLbetter
thanmostlearnedoptimizers,itcannotconditiononfeaturesadditionaltogradientandparameter
value. Thislimitsitsexpressibility,abilitytotargetthedifficultiesofRLand,therefore,performance.
LearnedOptimizationinRL LearnedoptimizationinRLissignificantlymoredifficultthanin
otherlearningdomainsduetotheproblemsoutlinedinsection4. Forthisreason,SOTAlearned
optimizers fail in transfer to RL [16]. Optim4RL [17] attempts to solve this issue by learning
tooptimizedirectlyinRL.However, intheirtestsandours, Optim4RLfailstoconsistentlybeat
handcraftedbenchmarks. Also,itreliesonaheavilyconstrainedupdateexpressionbasedonAdam
[18],anditneedsexpensivelearningratetuning. Instead,weachievemuchstrongerresultswitha
completelyblack-boxsetupinspiredbypreexistingmethodsformitigatingspecificdifficultiesinRL.
4 DifficultiesinRL
WebelievethatfundamentaldifferencesexistbetweenRLandotherlearningparadigmswhichmake
RLparticularlydifficult. Here,webrieflycoveraspecificsetofprominentdifficultiesinRL,which
aredetailedwithadditionalreferencesinappendixA.Ourmethodtakesinspirationfromhandcrafted
heuristicstargetingthesechallenges(Section5.3). Weshowviathoroughablation(Section7)that
explicitlyformulatingourmethodaroundthesedifficultiesleadstosignificantperformancegains.
(Problem1)Non-stationarity RLissubjecttonon-stationarityoverthetrainingprocess[1]asthe
updatingagentcauseschangestothetrainingdistribution. Wedenotethistrainingnon-stationarity.
Lyleetal.[9]suggestoptimizersdesignedforstationarysettingsstruggleundernonstationarity.
(Problem2)Plasticityloss Plasticityloss,ortheinabilitytofitnewobjectivesduringtraining,
hasbeenathemeinrecentdeepRLliterature[9,47,48,10]. Here,wefocusondormancy[47],a
measurementtrackinginactiveneuronsusedasametricforplasticityloss[10,49–51]. Itisdefinedas
E |hl(x)|
sl = x∈D i , (6)
i 1 (cid:80) E |hl(x)|
Hl k∈h x∈D k
wherehl(x)istheactivationofneuroniinlayerlwithinputx ∈ DfordistributionD. Hl isthe
i
totalnumberofneuronsinlayerl. Thedenominatornormalizesaveragedormancyto1ineachlayer.
Aneuronisτ-dormantifsl ≤ τ,meaningtheneuron’soutputmakesuplessthanτ ofitslayer’s
i
output. ForReLUactivationfunctions,τ =0meansaneuronisinthesaturatedpartoftheReLU.
Sokaretal.[47]findthatdormantneuronsgenerallystaydormantthroughouttraining,motivating
approacheswhichtrytoreactivatedormantneuronstoboostplasticity.
(Problem3)Exploration ExplorationisakeyprobleminRL[1]. Topreventprematureconver-
gencetolocaloptima,andthusmaximizefinalreturn,anagentmustexploreuncertainstatesand
actions. Here,wefocusonparameterspacenoiseforexploration[11],wherenoiseisappliedtothe
parametersoftheagentratherthantoitsoutputactions,likeϵ-greedy[1].
5 Method
TherearethreekeyconsiderationswhendoinglearnedoptimizationforRL:whatarchitecturetouse;
howtotraintheoptimizer;andwhatinputstheoptimizershouldconditionon. Inthissection,we
systematicallyconsidereachofthesequestionstoconstructOPEN,withjustificationforeachofour
decisionsgroundedinourcoredifficultiesofRL.
5.1 ArchitectureandParameterization
Toenableconditioningonhistory,whichisrequiredtoexpressbehaviorlikemomentum,forexample,
OPENusesagatedrecurrentunit[52,GRU].Thisisfollowedbytwofullyconnectedlayerswith
4LayerNorm[53],whichweincludeforstability. Alllayersaresmall;thisisimportantforlimiting
memoryusage,sincetheGRUstoresseparatestatesforeveryagentparameter,andformaintaining
computationalefficiency[43]. WevisualizeanddetailthearchitectureofOPENinAppendixB.
UpdateExpression WesplitthecalculationoftheupdateinOPENintothreestages,eachofwhich
servesadifferentpurpose. Thefirststage,whichfollowsMetzetal.[23],is
uˆ =α m expα e , (7)
i 1 i 2 i
wherem ande areoptimizeroutputsforparameteri,andα aresmallscalingfactorsusedfor
i i (1,2)
stability. Thisupdatecancovermanyordersofmagnitudewithoutrequiringlargenetworkoutputs.
(P3) Secondly,weaugmenttheupdaterulefortheactoronlytoincreaseexploration;thereisno
needforthecritictoexplore. Wetakeinspirationfromparameterspacenoiseforexploration[11],
sinceitcaneasilybeappliedviatheoptimizer. Tobeprecise,weaugmenttheactor’supdateas
uˆactor :=uˆactor+α δactorϵ. (8)
i i 3 i
Here, α isasmall, stabilizingscalingfactor, δactor isathirdoutputfromtheoptimizer, andϵ ∼
3 i
N(0,1) is sampled Gaussian noise. By multiplying δactor and ϵ, we introduce a random walk of
i
learned,per-parametervariancetotheupdatewhichcanbeusedforexploration. Sinceδactordepends
i
ontheoptimizer’sinputs,thiscanpotentiallylearncomplexinteractionsbetweenthenoiseschedule
andthefeaturesoutlinedinSection5.3. UnlikePlappertetal.[11],whoremoveandresamplenoise
betweenrollouts,OPENaddspermanentnoisetotheparameters. Thisbenefitsplasticityloss(i.e.,
(P2)),inprincipleenablingtheoptimizertoreactivatedormantneuronsintheabsenceofgradient.
Unfortunately,naïveapplicationoftheaboveupdatescancauseerrorsasagentparameterscangrow
toapointofnumericalinstability. Thiscancauseparticulardifficultyindomainswithcontinuous
actionspaces,whereactionselectionofteninvolvesexponentiationtogetanon-negativestandard
deviation. Therefore,thefinalstageofourupdatestabilizesmeta-optimizationbyzero-meaning,as
u=uˆ−E[uˆ]. (9)
Whilethislimitstheupdate’sexpressiveness,wefindthateventraditionaloptimizerstendtoproduce
nearlyzero-meanupdates. Inpractice,thisenableslearninginenvironmentswithcontinuousactions
withoutharmingperformancefordiscreteactions. Parameteriisupdatedasp(t+1) =p(t)−u .
i i i
5.2 Training
We train our optimizers with OpenAI ES [25], using final return as the fitness. We apply the
commonly-usedranktransform,whichinvolvesmappingrankingsoverthepopulationtotherange
[−0.5,0.5],tothefitnessesbeforeestimatingtheESgradient;thisisaformoffitnessshaping[29,25],
andmakeslearningbotheasierandinvarianttorewardscale.
Fortrainingonmultipleenvironmentssimultaneously(i.e.,multi-tasktraining,Section6),weevaluate
everymemberofthepopulationonallenvironments. Afterevaluation,we: 1)Dividebythereturn
Adamachievesineachenvironment;2)Averagethescoresoverenvironments;3)Doaranktransform.
NormalizingbyAdammapsreturnstoaroughlycommonscale,enablingcomparisonbetweendiverse
environments. However,thisbiaseslearningtoenvironmentswhereAdamunderperformsOPEN. We
believefindingbettercurriculaformulti-tasktrainingwouldbehighlyimpactfulfuturework.
5.3 Inputs
CarefullyselectingwhichinputstoconditionOPENoniscrucial;theyshouldbesufficientlyexpres-
sivewithoutsignificantlyincreasingthecomputationalcostormeta-learningsamplerequirementsof
theoptimizer,andshouldallowthetrainedoptimizerstosurgicallytargetspecificproblemsinRL.To
satisfytheserequirements,wetakeinspirationfrompriorworkaddressingourfocaldifficultiesofRL.
Inspirit,wedistillcurrentmethodstoa‘lowestcommondenominator’whichischeaptocalculate.
WeprovideadditionaldetailsofhowthesefeaturesarecalculatedinAppendixB.2.
(P1)Twotrainingtimescales Manylearnedoptimizersforstationaryproblemsincorporatesome
versionofprogressthroughtrainingasaninput[16,43,44]. SincePPOlearnsfromsuccessively
collected,stationarybatchesofdata[54],weconditionOPENonhowfaritisthroughupdatingwith
5thecurrentbatch(i.e.,batchproportion). Thisenablesbehaviorlikelearningratescheduling,which
hasprovedeffectiveinstationaryproblems(e.g.,[55,56]),andbiascorrectionfromAdamtoaccount
forinaccuratemomentumestimates[18].
InspiredbyJacksonetal.[31],whodemonstratetheefficacyoflearningdynamicversionsofLPG
[13]andLPO[15],wealsoconditionOPENonhowfarthecurrentbatchisthroughthetotalnumber
ofbatchestobecollected(i.e.,trainingproportion). Thisdirectlytargetstrainingnon-stationarity.
(P2)LayerProportion Nikishinetal.[57,58]operateonhigher(i.e.,closertotheoutput)network
layersintheirattemptstoaddressplasticitylossinRL.Furthermore,theytreatinterventiondepthas
ahyperparameter. Toreplicatethis,andenablevariedbehaviorbetweenthedifferentlayersofan
agent’snetwork,weconditionOPENontherelativepositionoftheparameter’slayerinthenetwork.
(P2)Dormancy AsSokaretal.[47]reinitializeτ-dormantneurons,weconditionOPENdirectlyon
dormancy;thisenablessimilarbehaviorbyallowingOPENtoreactasneuronsbecomemoredormant.
Infact,intandemwithlearnablestochasticity,thisenablesOPENtoreinitializedormantneurons,just
likeSokaretal.[47]. Sincedormancyiscalculatedforneurons,ratherthanparameters,weusethe
valueofdormancyfortheneurondownstreamofeachparameter.
6 Results
Inthissection,webenchmarkOPENagainstaplethoraofbaselinesonlarge-scaletrainingdomains.
6.1 ExperimentalSetup
Duetocomputationalconstraints,wemeta-trainanoptimizeronasinglerandomseedwithoutES
hyperparameter tuning. This follows standard evaluation protocols in learned optimization (e.g.,
[16,17,45]),whicharealsoconstrainedbythehighcomputationalcostofmeta-learnedoptimization.
WedetailourhyperparametersinAppendixC.3. Wedefinefourevaluationdomains,basedonKirk
etal.[59],inwhichaneffectivelearnedoptimizationframeworkshouldprovecompetent:
Single-TaskTraining Alearnedoptimizermustbecapableoffittingtoasingleenvironment,and
beingevaluatedinthesameenvironment,todemonstrateitisexpressiveenoughtolearnanupdate
rule. Wetestthisinfiveenvironments: Breakout,Asterix,SpaceInvadersandFreewayfromMinAtar
[60,61];andAntfromBrax[62,63]. Thisisreferredtoas‘singleton’traininginKirketal.[59].
Multi-Task Training To show an optimizer is able to perform under a wide input distribution,
it must be able to learn in a number of environments simultaneously. Therefore, we evaluate
performancefromtraininginallfourenvironmentsfromMinAtar[60,61].2 Weevaluatetheaverage
normalizedscoreacrossenvironmentswithrespecttoAdam.
In-DistributionTaskGeneralization Anoptimizershouldgeneralizetounseentaskswithinits
trainingdistribution. Tothisend,wetrainOPENonadistributionofgridworldsfromJacksonetal.
[64]withantithetictasksampling[31],andevaluateperformancebysamplingtasksfromthesame
distribution. WeincludedetailsinAppendixD.
Out-Of-SupportTaskGeneralization Crucially,anoptimizerunabletogeneralizetonewsettings
haslimitedreal-worldusefulness. Therefore,weexploreout-of-support(OOS)generalizationby
testingOPENonspecifictaskdistributionsdefinedbyOhetal.[13],andasetofmazesfromminigrid
[65],whichdonotexistinthetrainingdistribution,andwithunseenagentparameters.
Baselines Wecompareagainstopen-sourceimplementations[66]ofAdam[18],RMSProp[19],
Lion[20]andVeLO[16,43]. Wealsolearntwooptimizersforthesingle-andmulti-tasksettings:
‘NoFeatures’,whichonlyconditionsongradientandmomentum;andOptim4RL[17](usingES
insteadofmeta-gradients,asinLanetal.[17]). SinceOptim4RLisinitializedclosetosgn(Adam),
andtuningitslearningrateistoopracticallyexpensive,wesetalearningrateof0.1×LR based
Adam
onLion[20](whichmovesfromAdamWtosgn(AdamW)). Theoptimizer’sweightscanbescaledto
compensateifthisissuboptimal. Weprimarilyconsiderinterquartilemean(IQM)offinalreturnwith
95%stratifiedbootstrapconfidenceintervals[67]. AllhyperparameterscanbefoundinAppendixC.
2Seaquest,whichisapartofMinAtar[60],doesnothaveaworkingimplementationingymnax[61].
66.2 Single-TaskTrainingResults
Figure2showstheperformanceofOPENaftersingle-tasktraining. InthreeMinAtarenvironments,
OPENsignificantlyoutperformallbaselines,farexceedingpreviousattemptsatlearnedoptimization
for RL. Additionally, OPEN beat both learned optimizers in every environment. Overall, these
experimentsshowthecapabilityofOPENtolearnhighlyperformantupdaterulesforRL.
Returncurves(AppendixE)showthat OPEN doesnotalwaysachievehighreturnsfromthestart
oftraining. Instead,OPENcansacrificegreedy,short-termgainsinfavoroflong-termfinalreturn,
possiblyduetoitsdynamicupdaterule. Wefurtheranalyzetheoptimizers’behaviorinAppendixF.
freeway spaceinvaders breakout asterix ant
No Features
Optim4RL
VeLO
Lion
RMSprop
Adam
OPEN
58 60 62 64 75 100 125 150 175 15 30 45 60 8 16 24 32 40 5.6 6.0 6.4 6.8
1e3
Figure2:IQMoffinalreturnsforthefivesingle-tasktrainingenvironments,evaluatedover16random
environmentseeds. Weplot95%stratifiedbootstrapconfidenceintervalsforeachenvironment.
6.3 Multi-TaskTrainingResults
Figure 3 shows each optimizer’s ability to fit to multiple MinAtar environments [61, 60], where
handcraftedoptimizersaretunedper-environment. WenormalizereturnswithrespecttoAdam,and
aggregatescoresoverenvironmentstogiveasingleperformancemetric. Here,weincreasethesize
ofthelearnedoptimizers,withdetailsinAppendixB.5. InadditiontoIQM,weconsiderthemean
normalizedreturntoexploretheexistenceofoutliers(whichoftencorrespondtoasterix,whereOPEN
stronglyoutperformsAdam),andoptimalitygap,ametricfromAgarwaletal.[67]measuringhow
closetooptimalalgorithmsare. Unlikesingle-tasktraining,whereoptimizerstrainuntilconvergence,
werunmulti-taskexperimentsforafixednumberofgenerations(300)tolimitcompute.
OPENdrasticallyoutperformseveryoptimizerformulti-tasktraining. Inparticular,OPENproduces
theonlyoptimizerwithanaggregatescorehigherthanAdam,demonstratingitsabilitytolearnhighly
expressiveupdateruleswhichcanfittoarangeofcontexts;nootherlearnedoptimizersgetcloseto
OPENinfittingtomultipleenvironmentssimultaneously. Asexpected,OPENprioritizesoptimization
inasterixandbreakout,accordingtothereturncurves(AppendixG);webelievebettercurricula
wouldhelptoovercomethisissue. Interestingly,Optim4RLseemstoperformbetterinthemulti-task
settingthanthesingle-tasksetting. Thismaybeduetotheincreasednumberofsamples.
Mean IQM Optimality Gap
No Features
Optim4RL
VeLO
Lion
RMSprop
Adam
OPEN
0.75 0.90 1.05 1.20 0.75 0.90 1.05 1.20 0.08 0.16 0.24 0.32
Figure3: Mean,IQMandoptimalitygap(smaller=better),evaluatedover16randomseedsper
environmentfortheaggregated,Adam-normalizedfinalreturnsaftermulti-tasktrainingonMinAtar
[60,61]. Weplot95%stratifiedbootstrapconfidenceintervalsforeachmetric.
6.4 GridworldGeneralization
Figure4showsOPEN’sabilitytogeneralize. Weexplorein-distributiongeneralizationontheleft,
andOOSgeneralizationontheright,wherethetoprow(rand_dense,rand_sparseandrand_long)are
fromLPG[13,64]andthebottomroware3mazesfromMinigrid[65,64]. Weexploreanadditional
dimensionofOOSgeneralizationintheagent’snetworkhiddensize;OPENonlysawagentswith
network hidden sizes of 16 in training, but is tested with larger and smaller agent networks. We
includesimilartestsfordifferent,OOStraininglengthsinAppendixH.WenormalizeOPENagainst
Adam,whichwastunedforthesamedistributionandagentthatOPENlearnedfor.
OPEN learns update rules that consistently outperform Adam in-distribution and out-of-support
withregardstoboththeagentandenvironment;ineveryOOSenvironmentandagentsize,OPEN
7rand_dense rand_sparse rand_long
in distribution 1.30 2.0 1.30
a)1.20
OPEN
11 .. 22 05 1.8 11 .. 22 05
1.15 1.6 1.15
1.15 1.10 1.4 1.10
1.05 1.2 1.05
1.00 1.00 1.10 0.95 1.0 0.95
8 16 32 64 128 8 16 32 64 128 8 16 32 64 128
Hidden Size Hidden Size Hidden Size
1.05 standard_maze sixteen_rooms labyrinth
1.00 1.15 1.15 1.15
1.10 1.10 1.10
0.95 1.05 1.05 1.05
8 16 32 64 128 1.00 1.00 1.00
Hidden Size 0.95 8 16 32 64 128 0.95 8 16 32 64 128 0.95 8 16 32 64 128
Hidden Size Hidden Size Hidden Size
Figure4: IQMofreturn,normalizedbyAdam,insevengridworlds,with95%stratifiedbootstrap
confidenceintervalsfor64randomseeds. Ontheleft,weshowperformanceinthedistributionOPEN
andAdamweretrainedandtunedin. Ontheright,weshowOOSperformance: thetoprowshows
gridworldsfromOhetal.[13],andthebottomrowshowsmazesfromChevalier-Boisvertetal.[65].
WemarkHiddenSize=16asthein-distributionagentsizeforOPENandAdam.
outperformedAdam. Infact,inallbutrand_dense,itsgeneralizationimprovescomparedtoAdam
forsomedifferentagentwidths,increasingitsnormalizedreturn. Incombinationwithourfindings
fromSection6.3,whichdemonstrateourapproachcanlearnexpressiveupdaterulesforhard,diverse
environments, these results demonstrate the effectiveness of OPEN: if trained on a wide enough
distribution,OPENhasthepotentialtogeneralizeacrossawidearrayofRLproblems.
7 AblationStudy
Inthisablation,weexplorehoweachconstituentofOPENcontributestoimprovedperformance. We
focusontwospecific,measurablechallenges: plasticitylossandexploration.
7.1 IndividualAblations
We ablate each individual design decision of OPEN in Figure 5. For each ablation, we train 17
optimizers in a shortened version of Breakout [60, 61] and evaluate performance after 64 PPO
trainingrunsperoptimizer. FurtherdetailsoftheablationmethodologyareinAppendixI.
While measuring plasticity loss is important, dormancy alone is not an appropriate performance
metric; a newly initialized network has near-zero dormancy but poor performance. Instead, we
includedormancyhereasonepossiblejustificationforwhysomeelementsofOPENareuseful.
(P1) Ablatingtrainingproportiondirectlydisablestheoptimizer’sabilitytotackletrainingnon-
stationarity. Similarly,removingbatchproportionpreventsdynamicbehaviorwithina(stationary)
batch. Thecorrespondingdecreasesinperformanceshowthatbothtimescalesofnon-stationarityare
beneficial,potentiallyovercomingtheimpactofnon-stationarityinRL.
(P2) Removingdormancyasaninputhasadrasticimpactontheagent’sreturn,correspondingtoa
largeincreaseinplasticityloss.Whiledormancyplaysnodirectroleintheoptimizer’smeta-objective,
includingitasaninputgivestheoptimizerthecapabilitytoreactasneuronsgrowdormant,asdesired.
No Features
No Stochasticity
No Training Proportion
No Batch Proportion
No Dormancy
No Layer Proportion
OPEN
9.6 10.4 11.2 12.0 12.8 16 24 32 40
Final Return Dormant Neurons [%]
Figure5: IQMofmeanfinalreturnfor17trainedoptimizersperablation,evaluatedon64random
seedseach,alongsidemeanτ =0dormancyforoptimizersintheinterquartilerange. Weshow95%
stratifiedbootstrapconfidenceintervals.
8
nruteR
dezilamroN
madA
nruteR
dezilamroN
madA
nruteR
dezilamroN
madA
nruteR
dezilamroN
madA
nruteR
dezilamroN
madA
nruteR
dezilamroN
madA
nruteR
dezilamroN
madA(P2) Notconditioningonlayerproportionhasanegativeeffectonfinalreturn. Theincreasein
dormancyfromitsremovalsuggeststhatallowingtheoptimizertobehavedifferentlyforeachlayer
inthenetworkhassomepositiveimpactonplasticityloss.
(P2)/(P3) Stochasticitybenefitsaretwo-fold:besidesenhancingexploration(Section7.2),itnotably
lowersdormancy. Thoughlimitedtotheactor,thisreductionlikelyalsocontributestoimprovereturn.
7.2 Exploration
+0.8
(P3)Weverifythebenefitsoflearnablestochas- +0.7
ticity in Deep Sea, an environment from bsuite +0.6
[68,61]designedtoanalyzeexploration. Thisen-
+0.5
vironmentreturnsasmallpenaltyforeachright
+0.4
actionbutgivesalargepositiverewardifrightis
selectedforeveryaction. Therefore,agentsneedto +0.3
explorebeyondthelocaloptimumof‘leftateach +0.2
timestep’tomaximizereturn. DeepSea’ssizecan +0.1
be varied, such that an agent needs to take more
0.0
consecutiverightactionstoreceivereward.
-0.1
NaïvelyapplyingOPENtoDeepSeastruggles;we -0.2
positthatoptimizingtowardsthegradientcanbe
detrimentaltotheactorsincethegradientleadsto 10 20 30 40 50
alocaloptimum,whereasinthecriticthegradient Deep Sea Size
Figure 6: IQM performance improvement of
alwayspointsto(beneficially)moreaccuratevalue
anoptimizerwithlearnablestochasticityover
functions. Therefore,weaugmentOPENtolearn
onewithout. Weplot95%stratifiedbootstrap
different updates for the actor and critic (‘sepa-
confidenceintervalsover128randomseeds.
rated’). InFigure6,weshowthedisparitybetween
aseparatedoptimizertrainedwithandwithoutlearnablestochasticityaftertrainingforsizesbetween
[4,26]inasmallnumberofgenerations(48),notingthatlargersizesareOOSfortheoptimizer. We
includefullresultsinAppendixI.2.
Theoptimizerwithlearnablestochasticityconsistentlyachieveshigherreturninlargeenvironments
comparedtowithout,suggestingsignificantexplorationbenefits. Infact,ouranalysis(AppendixF.5)
findsthat, despitebeingunawareofsize, stochasticityincreasesinlargerenvironments. Inother
words,theoptimizerpromotesmoreexplorationwhensizeincreases. However,stochasticitydoes
marginallyreduceperformanceinsmallerenvironments;thismaybeduetotheoptimizerpromoting
explorationevenafterachievingmaximumreward,ratherthanconvergingtotheoptimalpolicy.
8 LimitationsandFutureWork
WhileOPENdemonstratesstrongsuccessinlearningamulti-taskobjective,ourcurrentapproach
ofnormalizingreturnsbyAdambiasesupdatestowardsenvironmentswhereAdamunderperforms
learned optimizers. We believe developing better curricula for learning in this settings, akin to
unsupervisedenvironmentdesign[64,69],wouldbehighlyimpactfulfuturework. Additionally,the
OPENframeworkneednotbelimitedtothespecificdifficultieswefocusonhere. Exploringwaysto
includeotherdifficultiesofRLwhichcanbemeasuredandincorporatedintotheframework(e.g.,
sampleefficiencyorgeneralizationcapabilitiesofapolicy)couldpotentiallyelevatetheusefulnessof
OPENevenfurther. Finally,wehaveconstrainedourexperimentstoPPOonly;exploringtheimpact
ofOPENonotherRLalgorithms(e.g.,SAC[70],A2C[71])wouldbeuseful.
9 Conclusion
Inthispaper, wesetouttoaddresssomeofthemajordifficultiesinRLbymeta-learningupdate
rulesdirectlyforRL.Inparticular,wefocusedonthreemainchallenges: non-stationarity,plasticity
loss,andexploration. Todoso,weproposedOPEN,amethodtotrainparameterizedoptimizersthat
conditionsonasetofinputsanduseslearnablestochasticityinitsoutput,tospecificallytargetthese
difficulties. Weshowedthatourmethodoutperformsarangeofbaselinesinfourproblemsettings
designedtoshowtheexpressibilityandgeneralizabilityoflearnedoptimizersinRL.Finally, we
demonstratedthateachdesigndecisionofOPENimprovedtheperformanceoftheoptimizerinan
ablationstudy,andthatthestochasticityinourupdateexpressionsignificantlybenefitedexploration.
9
yticitsahcotS
htiw
tnemevorpmIContributions
AGimplementedthecode,designedandperformedallexperimentsandmadethearchitecturaland
trainingdesigndecisionsofOPEN. AGalsoprovidedallanalysisandwroteallofthepaper.
CL proposed the initial project, contributed the initial implementation framework and provided
assistancethroughouttheproject’stimeline.
MJcontributedtoexperimentalandablationdesignandhelpedwitheditingthemanuscript.
SWandJFprovidedadvicethroughouttheproject,andinwritingthepaper.
Acknowledgements
WeliketothankBenjaminEllisforofferingadvicethroughoutthedurationoftheproject,andPablo
Samuel Castro for providing feedback in the early stages of the project. We are also grateful to
MichaelMatthews,ScottleRoux,JuliuszZiomekandouranonymousreviewersfortheircomments,
whichwehaveincorporatedintothepaper.
AG and MJ are funded by the EPSRC Centre for Doctoral Training in Autonomous Intelligent
Machines and Systems EP/S024050/1. MJ is also sponsored by Amazon Web Services. JF is
partiallyfundedbytheUKIgrantEP/Y028481/1(originallyselectedforfundingbytheERC).JFis
alsosupportedbytheJPMCResearchAwardandtheAmazonResearchAward. Ourexperiments
werealsomadepossiblebyagenerousequipmentgrantfromNVIDIA.
References
[1] RichardS.SuttonandAndrewG.Barto.ReinforcementLearning:AnIntroduction.TheMITPress,second
edition,2018. URLhttp://incompleteideas.net/book/the-book-2nd.html.
[2] DavidSilver, AjaHuang, ChrisJ.Maddison, ArthurGuez, LaurentSifre, GeorgevandenDriessche,
JulianSchrittwieser,IoannisAntonoglou,VedaPanneershelvam,MarcLanctot,SanderDieleman,Dominik
Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray
Kavukcuoglu,ThoreGraepel,andDemisHassabis. MasteringthegameofGowithdeepneuralnetworks
andtreesearch. Nature,529(7587):484–489,January2016. ISSN1476-4687. doi:10.1038/nature16961.
[3] DavidSilver,ThomasHubert,JulianSchrittwieser,IoannisAntonoglou,MatthewLai,ArthurGuez,Marc
Lanctot,LaurentSifre,DharshanKumaran,ThoreGraepel,TimothyLillicrap,KarenSimonyan,andDemis
Hassabis. Ageneralreinforcementlearningalgorithmthatmasterschess,shogi,andGothroughself-play.
Science(NewYork,N.Y.),362(6419):1140–1144,2018. doi:10.1126/science.aar6404.
[4] Matt Barnes, Matthew Abueg, Oliver F. Lange, Matt Deeds, Jason Trader, Denali Molitor, Markus
Wulfmeier,andShawnO’Banion. MassivelyScalableInverseReinforcementLearninginGoogleMaps.
arXivpreprintarXiv:2305.11290,2023. doi:10.48550/arXiv.2305.11290.
[5] Daniel J. Mankowitz, Andrea Michi, Anton Zhernov, Marco Gelmi, Marco Selvi, Cosmin Paduraru,
Edouard Leurent, Shariq Iqbal, Jean-Baptiste Lespiau, Alex Ahern, Thomas Köppe, Kevin Millikin,
StephenGaffney,SophieElster,JacksonBroshear,ChrisGamble,KieranMilan,RobertTung,Minjae
Hwang,TaylanCemgil,MohammadaminBarekatain,YujiaLi,AmolMandhane,ThomasHubert,Julian
Schrittwieser,DemisHassabis,PushmeetKohli,MartinRiedmiller,OriolVinyals,andDavidSilver. Faster
sortingalgorithmsdiscoveredusingdeepreinforcementlearning. Nature,618(7964):257–263,June2023.
ISSN1476-4687. doi:10.1038/s41586-023-06004-9.
[6] Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep
reinforcementlearningfromhumanpreferences,February2023.
[7] LongOuyang,JeffWu,XuJiang,DiogoAlmeida,CarrollL.Wainwright,PamelaMishkin,ChongZhang,
SandhiniAgarwal,KatarinaSlama,AlexRay,JohnSchulman,JacobHilton,FraserKelton,LukeMiller,
MaddieSimens,AmandaAskell,PeterWelinder,PaulChristiano,JanLeike,andRyanLowe. Training
languagemodelstofollowinstructionswithhumanfeedback,March2022.
[8] MaximilianIgl,GregoryFarquhar,JelenaLuketina,WendelinBoehmer,andShimonWhiteson. Transient
non-stationarityandgeneralisationindeepreinforcementlearning.InInternationalConferenceonLearning
Representations,2021.
10[9] Clare Lyle, Zeyu Zheng, Evgenii Nikishin, BernardoAvilaPires, Razvan Pascanu, and Will Dabney.
Understandingplasticityinneuralnetworks. InProceedingsofthe40thInternationalConferenceon
MachineLearning,ICML’23.JMLR.org,2023.
[10] ClareLyle,ZeyuZheng,KhimyaKhetarpal,HadovanHasselt,RazvanPascanu,JamesMartens,andWill
Dabney. Disentanglingthecausesofplasticitylossinneuralnetworks,2024.
[11] MatthiasPlappert,ReinHouthooft,PrafullaDhariwal,SzymonSidor,RichardY.Chen,XiChen,Tamim
Asfour,PieterAbbeel,andMarcinAndrychowicz.ParameterSpaceNoiseforExploration.InInternational
ConferenceonLearningRepresentations,February2018.
[12] JacobBeck,RistoVuorio,EvanZheranLiu,ZhengXiong,LuisaZintgraf,ChelseaFinn,andShimon
Whiteson. ASurveyofMeta-ReinforcementLearning. arXivpreprintarXiv:2301.08028,2023.
[13] JunhyukOh,MatteoHessel,WojciechMCzarnecki,ZhongwenXu,HadoPvanHasselt,SatinderSingh,
and David Silver. Discovering reinforcement learning algorithms. Advances in Neural Information
ProcessingSystems,33:1060–1070,2020.
[14] LouisKirsch,SjoerdvanSteenkiste,andJuergenSchmidhuber. Improvinggeneralizationinmetarein-
forcementlearningusinglearnedobjectives. InInternationalConferenceonLearningRepresentations,
2020.
[15] ChrisLu, JakubKuba, AlistairLetcher, LukeMetz, ChristianSchroederdeWitt, andJakobFoerster.
Discoveredpolicyoptimisation. AdvancesinNeuralInformationProcessingSystems,35:16455–16468,
2022.
[16] LukeMetz,JamesHarrison,C.DanielFreeman,AmilMerchant,LucasBeyer,JamesBradbury,Naman
Agrawal,BenPoole,IgorMordatch,AdamRoberts,andJaschaSohl-Dickstein. VeLO:TrainingVersatile
LearnedOptimizersbyScalingUp. arXivpreprintarXiv:2211.09760,2022.
[17] Qingfeng Lan, A. Rupam Mahmood, Shuicheng Yan, and Zhongwen Xu. Learning to Optimize for
ReinforcementLearning. arXivpreprintarXiv:2302.01470,2023.
[18] Diederik P. Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. arXiv preprint
arXiv:1412.6980,2017.
[19] TijmenTieleman,GeoffreyHinton,etal. Lecture6.5-rmsprop:Dividethegradientbyarunningaverage
ofitsrecentmagnitude. COURSERA:Neuralnetworksformachinelearning,4(2):26–31,2012.
[20] XiangningChen,ChenLiang,DaHuang,EstebanReal,KaiyuanWang,YaoLiu,HieuPham,Xuanyi
Dong,ThangLuong,Cho-JuiHsieh,YifengLu,andQuocV.Le. SymbolicDiscoveryofOptimization
Algorithms. arXivpreprintarXiv:2302.06675,2023.
[21] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal Policy
OptimizationAlgorithms. arXivpreprintarXiv:1707.06347,2017.
[22] MarcinAndrychowicz,MishaDenil,SergioGomezColmenarejo,MatthewW.Hoffman,DavidPfau,
TomSchaul,andNandodeFreitas. Learningtolearnbygradientdescentbygradientdescent. InNeural
InformationProcessingSystems,2016.
[23] LukeMetz,NiruMaheswaranathan,C.DanielFreeman,BenPoole,andJaschaSohl-Dickstein. Tasks,
stability,architecture,andcompute:Trainingmoreeffectivelearnedoptimizers,andusingthemtotrain
themselves. arXivpreprintarXiv:2009.11243,2020.
[24] IngoRechenberg. Evolutionsstrategie:Optimierungtechnischersystemenachprinzipienderbiologischen
evolution. 1973.
[25] TimSalimans,JonathanHo,XiChen,SzymonSidor,andIlyaSutskever.EvolutionStrategiesasaScalable
AlternativetoReinforcementLearning. arXivpreprintarXiv:1703.03864,2017.
[26] PaulVicol,LukeMetz,andJaschaSohl-Dickstein. Unbiasedgradientestimationinunrolledcomputation
graphswithpersistentevolutionstrategies. InInternationalConferenceonMachineLearning, pages
10553–10563.PMLR,2021.
[27] FelipePetroskiSuch,VashishtMadhavan,EdoardoConti,JoelLehman,KennethO.Stanley,andJeff
Clune. DeepNeuroevolution:GeneticAlgorithmsAreaCompetitiveAlternativeforTrainingDeepNeural
NetworksforReinforcementLearning. arXivpreprintarXiv:1712.06567,2018.
11[28] NikolausHansenandAndreasOstermeier.Completelyderandomizedself-adaptationinevolutionstrategies.
EvolutionaryComputation,9(2):159–195,2001. doi:10.1162/106365601750190398.
[29] DaanWierstra,TomSchaul,TobiasGlasmachers,YiSun,JanPeters,andJürgenSchmidhuber. Natural
evolutionstrategies. JournalofMachineLearningResearch,15(27):949–980,2014.
[30] John Geweke. Antithetic acceleration of Monte Carlo integration in Bayesian inference. Journal of
Econometrics,38(1):73–89,1988.
[31] MatthewJackson,ChrisLu,LouisKirsch,RobertLange,ShimonWhiteson,andJakobFoerster. Discover-
ingtemporally-awarereinforcementlearningalgorithms. InSecondAgentLearninginOpen-Endedness
Workshop,2023.
[32] JamesBradbury, RoyFrostig, PeterHawkins, MatthewJamesJohnson, ChrisLeary, DougalMaclau-
rin,GeorgeNecula,AdamPaszke,JakeVanderPlas,SkyeWanderman-Milne,andQiaoZhang. JAX:
ComposabletransformationsofPython+NumPyprograms,2018.
[33] RobertTjarkoLange. Evosax:JAX-basedEvolutionStrategies. arXivpreprintarXiv:2212.04180,2022.
[34] AshiaCWilson,RebeccaRoelofs,MitchellStern,NatiSrebro,andBenjaminRecht. Themarginalvalue
ofadaptivegradientmethodsinmachinelearning. Advancesinneuralinformationprocessingsystems,30,
2017.
[35] SashankJ.Reddi,SatyenKale,andSanjivKumar.Ontheconvergenceofadamandbeyond.InInternational
ConferenceonLearningRepresentations,2018.
[36] PeterHenderson,JoshuaRomoff,andJoellePineau.WhereDidMyOptimumGo?:AnEmpiricalAnalysis
ofGradientDescentOptimizationinPolicyGradientMethods. arXivpreprintarXiv:1810.02525,2018.
[37] EmmanuelBengio,JoellePineau,andDoinaPrecup. CorrectingMomentuminTemporalDifference
Learning. arXivpreprintarXiv:2106.03955,2021.
[38] RichardS.Sutton. Learningtopredictbythemethodsoftemporaldifferences. MachineLearning,3:9–44,
1988.
[39] MehmetSarigülandMutluAvci. Performancecomparisionofdifferentmomentumtechniquesondeep
reinforcementlearning. In2017IEEEInternationalConferenceonINnovationsinIntelligentSysTemsand
Applications(INISTA),pages302–306,2017.
[40] KeLiandJitendraMalik. Learningtooptimize. InInternationalConferenceonLearningRepresentations,
2017.
[41] DiogoAlmeida,ClemensWinter,JieTang,andWojciechZaremba.AGeneralizableApproachtoLearning
Optimizers. arXivpreprintarXiv:2106.00958,2021.
[42] OlgaWichrowska,NiruMaheswaranathan,MatthewW.Hoffman,SergioGómezColmenarejo,Misha
Denil,NandodeFreitas,andJaschaSohl-Dickstein. Learnedoptimizersthatscaleandgeneralize. In
Proceedingsofthe34thInternationalConferenceonMachineLearning-Volume70,ICML’17,pages
3751–3760.JMLR.org,2017.
[43] Luke Metz, C Daniel Freeman, James Harrison, Niru Maheswaranathan, and Jascha Sohl-Dickstein.
Practicaltradeoffsbetweenmemory,compute,andperformanceinlearnedoptimizers. InConferenceon
LifelongLearningAgents,pages142–164.PMLR,2022.URLhttp://github.com/google/learned_
optimization.
[44] Erik Gärtner, Luke Metz, Mykhaylo Andriluka, C. Daniel Freeman, and Cristian Sminchisescu.
Transformer-based learned optimization. In Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition(CVPR),pages11970–11979,June2023.
[45] LukeMetz,NiruMaheswaranathan,BrianCheung,andJaschaNarainSohl-Dickstein. Meta-learning
updaterulesforunsupervisedrepresentationlearning. arXivpreprintarXiv:1804.00222,2018.
[46] IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. InInternationalConferenceon
LearningRepresentations,2019.
[47] GhadaSokar,RishabhAgarwal,PabloSamuelCastro,andUtkuEvci. Thedormantneuronphenomenon
indeepreinforcementlearning. InProceedingsofthe40thInternationalConferenceonMachineLearning,
ICML’23.JMLR.org,2023.
12[48] ClareLyle,MarkRowland,andWillDabney. Understandingandpreventingcapacitylossinreinforcement
learning. InInternationalConferenceonLearningRepresentations,2022.
[49] HojoonLee,HanseulCho,HyunseungKim,DaehoonGwak,JoonkeeKim,JaegulChoo,Se-YoungYun,
andChulheeYun. PLASTIC:ImprovingInputandLabelPlasticityforSampleEfficientReinforcement
Learning. InThirty-SeventhConferenceonNeuralInformationProcessingSystems,November2023.
[50] GuozhengMa,LuLi,SenZhang,ZixuanLiu,ZhenWang,YixinChen,LiShen,XueqianWang,and
DachengTao. RevisitingPlasticityinVisualReinforcementLearning:Data,ModulesandTrainingStages.
arXivpreprintarXiv:2310.07418,2023.
[51] ZaheerAbbas,RosieZhao,JosephModayil,AdamWhite,andMarlosC.Machado. LossofPlasticityin
ContinualDeepReinforcementLearning. arXivpreprintarXiv:2303.07507,2023.
[52] KyunghyunCho,BartvanMerriënboer,DzmitryBahdanau,andYoshuaBengio. Onthepropertiesof
neuralmachinetranslation:Encoder–Decoderapproaches. InProceedingsofSSST-8,EighthWorkshopon
Syntax,SemanticsandStructureinStatisticalTranslation,pages103–111,2014.
[53] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer Normalization. arXiv preprint
arXiv:1607.06450,2016.
[54] ShengyiHuang,RousslanFernandJulienDossa,AntoninRaffin,AnssiKanervisto,andWeixunWang.
The37implementationdetailsofproximalpolicyoptimization. InICLRBlogTrack,2022. URLhttps:
//iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/. https://iclr-blog-
track.github.io/2022/03/25/ppo-implementation-details/.
[55] IlyaLoshchilovandFrankHutter. SGDR:Stochasticgradientdescentwithwarmrestarts. InInterna-
tionalConferenceonLearningRepresentations,2017. URLhttps://openreview.net/forum?id=
Skq89Scxx.
[56] LeslieN.Smith. Cyclicallearningratesfortrainingneuralnetworks. In2017IEEEWinterConferenceon
ApplicationsofComputerVision(WACV),pages464–472,2017. doi:10.1109/WACV.2017.58.
[57] EvgeniiNikishin,MaxSchwarzer,PierlucaD’Oro,Pierre-LucBacon,andAaronCourville. Theprimacy
biasindeepreinforcementlearning. InKamalikaChaudhuri,StefanieJegelka,LeSong,CsabaSzepesvari,
Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine
Learning,volume162ofProceedingsofMachineLearningResearch,pages16828–16847.PMLR,July
2022.
[58] EvgeniiNikishin,JunhyukOh,GeorgOstrovski,ClareLyle,RazvanPascanu,WillDabney,andAndreBar-
reto. Deepreinforcementlearningwithplasticityinjection. InWorkshoponReincarnatingReinforcement
LearningatICLR2023,2023.
[59] RobertKirk,AmyZhang,EdwardGrefenstette,andTimRocktäschel.ASurveyofZero-shotGeneralisation
inDeepReinforcementLearning. JournalofArtificialIntelligenceResearch,76:201–264,January2023.
[60] KennyYoungandTianTian. MinAtar:Anatari-inspiredtestbedforthoroughandreproduciblereinforce-
mentlearningexperiments. arXivpreprintarXiv:1903.03176,2019.
[61] RobertTjarkoLange. gymnax:AJAX-basedreinforcementlearningenvironmentlibrary,2022.
[62] C.DanielFreeman,ErikFrey,AntonRaichuk,SertanGirgin,IgorMordatch,andOlivierBachem. Brax-
adifferentiablephysicsengineforlargescalerigidbodysimulation,2021.
[63] EmanuelTodorov,TomErez,andYuvalTassa. MuJoCo:Aphysicsengineformodel-basedcontrol. In
2012IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems,pages5026–5033.IEEE,
2012.
[64] MatthewThomasJackson,MinqiJiang,JackParker-Holder,RistoVuorio,ChrisLu,GregoryFarquhar,
ShimonWhiteson,andJakobNicolausFoerster. Discoveringgeneralreinforcementlearningalgorithms
withadversarialenvironmentdesign. InAdvancesinNeuralInformationProcessingSystems,volume36,
2023.
[65] MaximeChevalier-Boisvert,BolunDai,MarkTowers,RodrigodeLazcano,LucasWillems,SalemLahlou,
SumanPal,PabloSamuelCastro,andJordanTerry. Minigrid&Miniworld: Modular&Customizable
ReinforcementLearningEnvironmentsforGoal-OrientedTasks,June2023.
13[66] DeepMind,IgorBabuschkin,KateBaumli,AlisonBell,SuryaBhupatiraju,JakeBruce,PeterBuchlovsky,
DavidBudden,TrevorCai,AidanClark,IvoDanihelka,AntoineDedieu,ClaudioFantacci,Jonathan
Godwin,ChrisJones,RossHemsley,TomHennigan,MatteoHessel,ShaoboHou,StevenKapturowski,
ThomasKeck,IuriiKemaev,MichaelKing,MarkusKunesch,LenaMartens,HamzaMerzic,Vladimir
Mikulik, Tamara Norman, George Papamakarios, John Quan, Roman Ring, Francisco Ruiz, Alvaro
Sanchez,LaurentSartran,RosaliaSchneider,ErenSezener,StephenSpencer,SrivatsanSrinivasan,Miloš
Stanojevic´,WojciechStokowiec,LuyuWang,GuangyaoZhou,andFabioViola. TheDeepMindJAX
Ecosystem,2020. URLhttp://github.com/google-deepmind.
[67] RishabhAgarwal,MaxSchwarzer,PabloSamuelCastro,AaronCourville,andMarcGBellemare. Deep
reinforcementlearningattheedgeofthestatisticalprecipice. AdvancesinNeuralInformationProcessing
Systems,2021.
[68] IanOsband,YotamDoron,MatteoHessel,JohnAslanides,ErenSezener,AndreSaraiva,KatrinaMcKinney,
TorLattimore,CsabaSzepesvári,SatinderSingh,BenjaminVanRoy,RichardSutton,DavidSilver,and
HadovanHasselt. Behavioursuiteforreinforcementlearning. InInternationalConferenceonLearning
Representations,2020. URLhttps://openreview.net/forum?id=rygf-kSYwH.
[69] MichaelDennis,NatashaJaques,EugeneVinitsky,AlexandreBayen,StuartRussell,AndrewCritch,and
SergeyLevine. Emergentcomplexityandzero-shottransferviaunsupervisedenvironmentdesign. In
Proceedingsofthe34thInternationalConferenceonNeuralInformationProcessingSystems,NIPS’20,
pages13049–13061,RedHook,NY,USA,December2020.CurranAssociatesInc. ISBN978-1-71382-
954-6.
[70] TuomasHaarnoja,AurickZhou,PieterAbbeel,andSergeyLevine.SoftActor-Critic:Off-PolicyMaximum
EntropyDeepReinforcementLearningwithaStochasticActor,August2018.
[71] VolodymyrMnih,AdriàPuigdomènechBadia,MehdiMirza,AlexGraves,TimothyP.Lillicrap,Tim
Harley,DavidSilver,andKorayKavukcuoglu. AsynchronousMethodsforDeepReinforcementLearning,
June2016.
[72] JohnSchulman,PhilippMoritz,SergeyLevine,MichaelJordan,andPieterAbbeel. High-Dimensional
ContinuousControlUsingGeneralizedAdvantageEstimation,October2018.
[73] KavoshAsadi,RasoolFakoor,andShohamSabach. ResettingtheoptimizerindeepRL:Anempirical
study. arXivpreprintarXiv:2306.17833,2023.
[74] JohanObando-Ceron,MarcG.Bellemare,andPabloSamuelCastro. Smallbatchdeepreinforcement
learning. arXivpreprintarXiv:2310.03882,2023.
[75] PierlucaD’Oro,MaxSchwarzer,EvgeniiNikishin,Pierre-LucBacon,MarcG.Bellemare,andAaron
Courville.Sample-EfficientReinforcementLearningbyBreakingtheReplayRatioBarrier.InTheEleventh
InternationalConferenceonLearningRepresentations,September2022.
[76] MaxSchwarzer,JohanSamirObandoCeron,AaronCourville,MarcGBellemare,RishabhAgarwal,and
PabloSamuelCastro.Bigger,better,faster:Human-levelatariwithhuman-levelefficiency.InInternational
ConferenceonMachineLearning,pages30365–30380.PMLR,2023.
[77] RonenI.BrafmanandMosheTennenholtz. R-MAX-AGeneralPolynomialTimeAlgorithmforNear-
OptimalReinforcementLearning. JournalofMachineLearningResearch,3:213–231,Oct2002.
[78] MichaelKearnsandSatinderSingh. Near-OptimalReinforcementLearninginPolynomialTime. Machine
Learning,49(2):209–232,November2002.
[79] PeterAuer,ThomasJaksch,andRonaldOrtner. Near-optimalregretboundsforreinforcementlearning. In
D.Koller,D.Schuurmans,Y.Bengio,andL.Bottou,editors,AdvancesinNeuralInformationProcessing
Systems,volume21,2008.
[80] HaoranTang,ReinHouthooft,DavisFoote,AdamStooke,OpenAIXiChen,YanDuan,JohnSchulman,
FilipDeTurck,andPieterAbbeel.#Exploration:Astudyofcount-basedexplorationfordeepreinforcement
learning. Advancesinneuralinformationprocessingsystems,30,2017.
[81] GeorgOstrovski,MarcGBellemare,AäronOord,andRémiMunos. Count-basedexplorationwithneural
densitymodels. InInternationalConferenceonMachineLearning,pages2721–2730.PMLR,2017.
[82] BradlyC.Stadie,SergeyLevine,andPieterAbbeel. IncentivizingExplorationInReinforcementLearning
WithDeepPredictiveModels. arXivpreprintarXiv:1507.00814,2015.
14[83] SainbayarSukhbaatar,ZemingLin,IlyaKostrikov,GabrielSynnaeve,ArthurSzlam,andRobFergus.
Intrinsicmotivationandautomaticcurriculaviaasymmetricself-play. InInternationalConferenceon
LearningRepresentations,2018.
[84] ReinHouthooft,XiChen,XiChen,YanDuan,JohnSchulman,FilipDeTurck,andPieterAbbeel. VIME:
Variationalinformationmaximizingexploration. InD.Lee,M.Sugiyama,U.Luxburg,I.Guyon,and
R.Garnett,editors,AdvancesinNeuralInformationProcessingSystems,volume29.CurranAssociates,
Inc.,2016.
[85] LukeMetz,NiruMaheswaranathan,JeremyNixon,DanielFreeman,andJaschaSohl-Dickstein. Under-
standingandcorrectingpathologiesinthetrainingoflearnedoptimizers. InKamalikaChaudhuriand
RuslanSalakhutdinov,editors,Proceedingsofthe36thInternationalConferenceonMachineLearning,
volume97ofProceedingsofMachineLearningResearch,pages4556–4565.PMLR,June2019.
15A AdditionalDetails: DifficultiesinRL
(Problem1)Non-stationarity Igletal.[8]highlightsdifferentsourcesofnon-stationarityacrossa
rangeofRLalgorithms. ForPPO,non-stationaryarisesduetoachangingstatevisitationdistribution,
the target value function Vπ(s) depending on the updating policy and bootstrapping from the
generalizedadvantageestimate[72]. SincePPObatchesrollouts,non-stationarityoccursbetween
eachbatch;withinthesamebatch,theagentsolvesastationaryproblem. Asthenon-stationarity
occursoverthecourseoftraining,wedenoteittrainingnon-stationarity.
Optimizersdesignedforstationarity,likeAdam[18],havebeenshowntostruggleinnon-stationary
settings[9]andtechniquestodealwithnon-stationarityhavebeenproposed. Asadietal.[73]prevent
contaminationbetweenbatchesbyresettingtheiroptimizerstate,includingresettingtheirmomentum.
However,thishandcraftedapproachispotentiallytoosevere,asitfailstotakeadvantageofpotentially
usefulcommonalitybetweenbatches.
(Problem2)Plasticityloss Plasticityloss,whichreferstoaninabilitytofittonewobjectiveover
thecourseoftraining,isanimportantprobleminRL.Asanagentlearns,bothitsinputandtarget
distributionschange(i.e.,nonstationarity,(P1)). Thismeanstheagentneedstofitnewobjectives
duringtraining,emphasizingtheimportanceofmaintainingplasticitythroughouttraining. Therefore,
therehavebeenmanyhandcraftedattemptstoreduceplasticitylossinRL.Abbasetal.[51]findthat
differentactivationfunctionscanhelppreventplasticityloss,Obando-Ceronetal.[74]suggestusing
smallerbatchestoincreaseplasticityandNikishinetal.[58]introducesnewoutputheadsthroughout
training. Manyhavedemonstratedtheeffectivenessofresettingpartsofthenetwork[75,57,76,47],
thoughthisrunstheriskoflosingsomepreviouslylearned,usefulinformation. Additionally,these
approachesareallhyperparameter-dependentandunlikelytoeliminateplasticitylossrobustly.
(Problem3)Exploration ExplorationinRLhasarichhistoryofmethodologies: [77–79],toname
afew. Whiletherearesimple, heuristicapproaches, likeϵ-greedy[1], recentmethodshavebeen
designed to deal with complex, high dimensional domains. These include count based methods
[80,81],learnedexplorationmodels[82,83]orusingvariationaltechniques[84]. Parameterspace
noise[11]involvesnoisingeachparameterinanagenttoenableexplorationacrossdifferentrollouts
whilebehavingconsistentlywithinagivenrollout. Thisinspiredourapproachtoexplorationsinceit
isalgorithm-agnosticandcanbeimplementeddirectlyintheoptimizer.
16B OptimizerDetails
B.1 Algorithm
WedetailanexampleupdatestepwhenusingOPEN. WeusenotationfromAppendixB.2,anduse
zGRU torefertotheGRUhiddenstateatupdatet. Inthisalgorithm,weassumethedormancyfor
t
eachneuronhasbeentiledovertherelevantparameters. Inpractice,webuildouroptimizeraround
thelibraryfromMetzetal.[43].
Algorithm1:ExampleupdatestepfromOPEN.
Given: opt,N ,N ,L,H,β,α ,α ,α
batch minibatches 1 2 3
Input: g ,m ,p ,t,dormancies ,h,zGRU
t t−1 t t t−1
Output: (p ,m ,zGRU)
t+1 t t
/* Compute new momentums */
foreachβ do
mβ =β×mβ +(1−β)×g
t t−1 t
end
/* Compute training and batch proportions */
TP =(t//(L∗N )/N
minibatches batch
BP =((t//N )modL)/L
minibatches
/* Compute first stage of update */
foreachparameteriinagentdo
input=[sgn(g ),log(g ),sgn(m ),log(m ),p ,TP,BP,D ,h ]
(t,i) (t,i) (t,i) (t,i) (t,i) (t,i) i
GRU ,zGRU =optGRU(input,zGRU )
out (t,i) (t−1,i)
m ,e ,δ =optMLP(GRU )
i i i out
uˆ =α m expα e
i 1 i 2 i
ifiinactorthen
uˆ =uˆ +α δ ϵ
i i 3 i
end
end
/* Zero-mean updates and apply them */
foreachparameteriinagentdo
u =uˆ −E [uˆ ]
i i i i
p =p −u
(t+1,i) (t,i) i
end
return(p ,m ,zGRU)
t+1 t t
B.2 InputFeaturestotheOptimizer
Ourfulllistoffeaturesisconcatenatedandfedintotheoptimizer,asinalgorithm1below. Weuse
somenotationfromTable3. Thesefeaturesare:
• Gradientg,processedwiththetransformationg →{sign(g),log(g+ϵ)}.
• Momentum m calculated with the following β coefficients:
[0.1,0.5,0.9,0.99,0.999,0.9999]. This is processed with the transformation
m →{sign(m ),log(m +ϵ)}foreachβ .
i i i i
• Currentvalueoftheparameterp.
• Trainingproportion,orhowfarthecurrentbatchisthroughthetotalnumberofbatches.
SincethetotalnumberofbatchesiscalculatedasN = (T//N )//N ,thisis
batch steps envs
definedas(t//(L∗N )/N ,wheretisthecurrentupdateiteration. Weusea
minibatches batch
floordivisionoperatorastrainingproportionshouldbeconstantthroughabatch.
• Batchproportion, orhowfarthecurrentupdateisthroughtrainingwiththesamebatch,
calculatedas((t//N )modL)/L.
minibatches
17• Dormancy, D, calculated for the neuron downstream of the parameter using equation 6.
Thisisrepeatedoverparameterswiththesamedownstreamneuron.
• Proportionofthecurrentlayerthroughthenetwork,h/H.
Thegradient, momentumandparametervaluefeaturesarenormalizedtohaveasecondmoment
of1acrossthetensor,followingMetzetal.[85,43],asanormalizationstrategywhichpreserves
direction. LikeLanetal.[17],weprocessgradientsandmomentumasx→{sign(x),log(x+ϵ)}to
magnifythedifferencebetweensmallvalues.
B.3 OptimizerArchitectureFigure
Figure7visuallydemonstratesthearchitectureofOPEN. Sinceonlytheactorisupdatedwithlearned
stochasticity(δactor),wemultiplythesamplednoiseϵwitha1/0maskfortheactor/critic.
Gradient m
Momentum ψ θ e
Features δactor
Figure7: AvisualizationofthearchitectureusedbyOPEN. ψareparametersofaGRUandθare
parametersofanMLP.Sinceδactoronlyappliestotheactor’supdateexpression,wesetthenoiseto
zeroforthecritictokeepitsupdatesdeterministic.
B.4 Single-TaskandGridworldOptimizerArchitectureDetails
ThedetailsofthelayersizesofOPENinboththesingle-taskandgridworldsettingsareinTable1.
Table1: OptimizerlayersizesforOPENinthesingle-taskandgridworldexperiments.
LayerType Dimensionality
GRU [19,8]
FullyConnected [8,16]
Layernorm –
FullyConnected [16,16]
Layernorm –
FullyConnected [16,3]
Inourupdaterule, weletα = α = α = 0.001. Wemaskoutthethirdoutputinthecriticby
1 2 3
settingthenoise(ϵ)tozero;otherwise,updatestoourcriticwouldbenon-deterministic.
Weuselayersofthesamesizein‘NoFeatures’,butconditiononlessinputs(14)anddonothave
thethirdoutput. WeusethesamearchitectureasLanetal.[17]forOptim4RL,whichincludestwo
networks,eachcomprisingasize8GRU,andtwosize16fullyconnectedlayers.
B.5 Multi-TaskOptimizerArchitectureDetails
Whentrainingoptimizersforthemulti-tasksetting(Section6.3),wefindincreasingthenetworksizes
consistentlybenefitsperformance. WeshowthearchitectureforOPENinthisexperimentinTable2,
maskingoutthenoiseforthecriticbysettingϵ=0. WealsoenlargeNoFeaturesandOptim4RLby
doublingtheGRUsizesto16andthehiddenlayerstosize32.
Table2: OptimizerlayersizesforOPENinthemulti-taskexperiment.
LayerType Dimensionality
GRU [19,16]
FullyConnected [16,32]
Layernorm –
FullyConnected [32,32]
Layernorm –
FullyConnected [32,3]
18C Hyperparameters
C.1 PPOHyperparameters
PPOhyperparametersfortheenvironmentsincludedinourexperimentsareshowninTable3. Forour
gridworldexperiments,welearnedOPENandtunedAdam[18]foraPPOagentwithW =16,but
evaluatedonwidthsW ∈[8,16,32,64,128]. HyperparametersforPPOaretakenfrom[31]where
possible.
Table3: PPOhyperparameters. AllMinAtarenvironmentsusedcommonPPOparameters,andare
thusunderoneheader.
Environment
Hyperparameter
MinAtar Ant Gridworld
NumberofEnvironmentsN 64 2048 1024
envs
NumberofEnvironmentStepsN 128 10 20
steps
TotalTimestepsT 1×107 5×107 3×7
NumberofMinibatchesN 8 32 16
minibatch
NumberofEpochsL 4 4 2
DiscountFactorγ 0.99 0.99 0.99
GAEλ 0.95 0.95 0.95
PPOClipϵ 0.2 0.2 0.2
ValueFunctionCoefficientc 0.5 0.5 0.5
1
EntropyCoefficientc 0.01 0 0.01
2
MaxGradientNorm 0.5 0.5 0.5
LayerWidthW 64 64 16
NumberofHiddenLayersH 2 2 2
Activation ReLU tanh tanh
C.2 OptimizationHyperparameters
For Adam, RMSprop and Lion, we tune hyperparameters with fixed PPO hyperparameters. For
eachenvironmentwerunasweepandevaluateperformanceoverfourseeds(eightinthegridworld),
pickingthecombinationwiththehighestfinalreturn. Inmanycases,wefoundtheoptimizersto
befairlyrobusttoreasonablehyperparameters. ForLion[20],wesearchoveralearningraterange
from3-10×smallerthanAdamandRMSpropassuggestedbyChenetal.[20]. ForOptim4RL[17],
hyperparametertuningistooexpensive;instead,wesetlearningratetobe0.1×LR following
Adam
theheuristicfrom[20].
ForAdam,RMSpropandLion,wealsotestwhetherannealinglearningratefromitsinitialvalueto0
overthecourseoftrainingwouldimproveperformance.
Afullbreakdownofwhichhyperparametersaretuned,andtheirvalue,isshowninthefollowing
tables. Weusenotationandimplementationsforeachoptimizerfromoptax[66].
19Table 4: Optimization hyperparameters for MinAtar environments. ‘Range’ covers the range of
valuesusedinourhyperparametertuning.
Hyper Environment
Optimizer Range
Parameter asterix freeway breakout spaceinvaders
LR 0.003 0.001 0.01 0.007 {0.001,0.01}
Adam β 0.9 0.9 0.9 0.9 {0.9,0.999}
1
β 0.999 0.99 0.99 0.99 {0.9,0.999}
2
AnnealLR { , }
LR 0.002 0.001 0.002 0.009 {0.001,0.01}
RMSprop
Decay 0.99 0.999 0.99 0.99 {0.9,0.999}
AnnealLR { , }
LR 0.0003 0.0003 0.0008 0.0008 {0.0001,0.001}
Lion β 0.99 0.9 0.9 0.9 {0.9,0.999}
1
β 0.9 0.9 0.9 0.99 {0.9,0.999}
2
AnnealLR { , }
Optim4RL LR 0.0003 0.0001 0.001 0.0007 {0.1×LR }
Adam
Table 5: Optimization hyperparameters for Ant. ‘Range’ covers the range of values used in our
hyperparametertuning.
Environment
Optimizer Hyperparameter Range
Ant
LR 0.0003 {0.0001,0.001}
Adam β 0.99 {0.9,0.999}
1
β 0.99 {0.9,0.999}
2
AnnealLR { , }
LR 0.0008 {0.0001,0.005}
RMSprop
Decay 0.99 {0.9,0.999}
AnnealLR { , }
LR 0.00015 {0.00001,0.0005}
Lion β 0.9 {0.9,0.999}
1
β 0.9 {0.9,0.999}
2
AnnealLR { , }
Optim4RL LR 0.00003 {0.1×LR }
Adam
Table6: OptimizationhyperparametersforGridworld. ‘Range’coverstherangeofvaluesusedin
ourhyperparametertuning.
Environment
Optimizer Hyperparameter Range
Gridworld
LR 0.0001 {0.00005,0.001}
Adam β 0.99 {0.9,0.999}
1
β 0.99 {0.9,0.999}
2
AnnealLR { , }
C.3 ESHyperparameters
Duetothelengthofmeta-optimization,itisnotpracticaltotunehyperparametersformeta-training.
We,however,findthefollowinghyperparameterseffectiveandrobustforourexperiments. Weuse
commonhyperparameterswhenlearningOPEN,Optim4RLandNoFeatures.
Forthesingle-taskandgridworldsettings,wetrainalloptimizersforanumberofgenerationsafter
theirperformancestabilizes,whichtookdifferenttimesbetweenoptimizers,toensureeachlearned
optimizerhasreachedconvergence;thisoccasionallymeansoptimizersweretrainedfordifferent
20numberofgenerations,butenablesefficientuseofcomputationalresourcesbynotcontinuingtraining
farpastanyperformancegainscanberealized. Forthemulti-tasksetting,duetotheextremecostof
training(i.e.,havingtoevaluatesequentiallyonfourMinAtarenvironments),wetraineachoptimizer
withanequivalentlevelofcompute(i.e.,300generations)andpickthebestperforminggenerationin
thatperiod.
We periodically evaluate the learned optimizers during training to find the one which generation
performsbestonasmallin-distributionvalidationset. Table7showsroughlyhowmanygenerations
wetrainedeachoptimizerfor,andthegenerationusedattesttime,whichvariedforeachenvironment.
Wefindthatinsomeenvironments,Optim4RLmadenolearningprogressfromthebeginningof
training,withitsperformancebeingpracticallyidenticaltoitsinitialization.
Table7: EShyperparametersformeta-training. ForMinAtar,thenumberofgenerationscorresponds
to{Asterix,Breakout,Freeway,SpaceInvaders}. Weshowthemaxgenerationsforeachoptimizer
(i.e.,howlongitwastrainedfor),aswellasthegenerationused(i.e.,whichtraininggenerationwas
usedatinferencetime).
Environments
Hyperparameter
MinAtar Ant Multi-Task Gridworld
σ 0.03 0.01 0.03 0.01
init
σ 0.999 0.999 0.998 0.999
decay
LearningRate 0.03 0.01 0.03 0.005
LearningRateDecay 0.999 0.999 0.998 0.990
PopulationSize 64 32 64 64
NumberofRollouts 1 1 1 1
MaxGens(OPEN) ∼{350,500,700,450} ∼700 300 ∼350
GenUsed(OPEN) {336,312,648,144} 168 234 333
MaxGens(Optim4RL) ∼{500,550,300,400} ∼700 300 N/A
GenUsed(Optim4RL) {288,552,216,24} 480 288 N/A
MaxGens(NoFeatures) ∼{800,850,500,600} ∼400 300 N/A
GenUsed(NoFeatures) {456,816,408,264} 240 270 N/A
EvaluationFrequency 24 24 9 9
21D GridworldDetails
WetrainOPENongridworldsbysamplingenvironmentparametersfromadistribution. Here,weuse
adistributionimplementedby[64,31]whichisdetailedinTable8. InthecodebaseofJacksonetal.
[64],thisisreferredtoas‘all’.
Table8: DistributionparametersforGridworld. Intraining,thevaluesfortheexperiencedenviron-
mentaresampledfromthevaluerange.
Parameter Value[range]
MaxStepsinEpisode [20,750]
ObjectRewards [−1.0,1.0]
Objectp(terminate) [0.01,1.0]
Objectp(respawn) [0.001,0.1]
NumberofObjects [1,6]
GridSize [4,11]
NumberofWalls 15
WealsoevaluateOPENonthreedistributionsfromOhetal.[13],andthreemazesfromChevalier-
Boisvertetal.[65],whichwerenotinthedistributionof‘all’. Werunallexperimentson64seeds
pergridworld. Thesearealldetailedbelow,withthemazesalsovisualized.
Table 9: Distribution parameters for Gridworld. Curly brackets denote a list of the true values,
correspondingtoeachobject,usedintesting.
Name Parameter Values
MaxStepsinEpisode 500
ObjectRewards (1.0,1.0,−1.0,−1.0)
Objectp(terminate) (0.0,0.0,0.5,0.0)
rand_dense Objectp(respawn) (0.05,0.05,0.1,0.5)
NumberofObjects 4
GridSize 11
NumberofWalls 0
MaxStepsinEpisode 50
ObjectRewards (−1.0,1.0)
Objectp(terminate) (1.0,1.0)
rand_sparse Objectp(respawn) (0.0,0.0)
NumberofObjects 2
GridSize 13
NumberofWalls 0
MaxStepsinEpisode 1000
ObjectRewards (1.0,1.0,−1.0,−1.0)
Objectp(terminate) (0.0,0.0,0.5,0.5)
rand_long Objectp(respawn) (0.01,0.01,1.0,1.0)
NumberofObjects 4
GridSize 11
NumberofWalls 0
22Table10: DistributionparametersforGridworld. Atinference,truevaluesaresampled fromthe
givenrange,suchthateachseedisevaluatedinaslightlydifferentsetting.
Name Parameter Value
MaxStepsinEpisode [25,50]
ObjectRewards [0.0,1.0]
Objectp(terminate) [0.01,1.0]
standard_maze Objectp(respawn) [0.001,0.1]
NumberofObjects 3
GridSize 13
MaxStepsinEpisode [25,50]
ObjectRewards [0.0,1.0]
Objectp(terminate) [0.01,1.0]
sixteen_rooms
Objectp(respawn) [0.001,0.1]
NumberofObjects 3
GridSize 13
MaxStepsinEpisode [25,50]
ObjectRewards [0.0,1.0]
Objectp(terminate) [0.01,1.0]
labyrinth
Objectp(respawn) [0.001,0.1]
NumberofObjects 3
GridSize 13
WevisualizethethreemazesinFigure8.
Figure8: Thethreemazesfromminigrid[65]usedforourOOStests. Fromlefttoright,themazes
are: ‘standard_maze’,‘sixteen_rooms’and‘labyrinth’.
23E SingleEnvironmentReturnCurves
InFigure9,whichshowsreturncurvesovertrainingforthefive‘single-task’environments(section
6),OPENsignificantlybeats3ofthe5baselines,performssimilarlytoLion[20]inspaceinvaders
andperformscomparablytohand-craftedoptimizersand‘NoFeatures’inant. OPENisclearlyable
tolearnstronglyperformingupdaterulesinarangeofsingle-tasksettings.
freeway asterix 1e3 ant
60 6
30
50
40 OPEN 20 4
30 Adam
RMSprop
20 Lion 10 2
VeLO
10 Optim4RL
0 No Features 0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0 1 2 3 4 5
Frames 1e7 Frames 1e7 Frames 1e7
breakout spaceinvaders
60 150
40 100
20 50
0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Frames 1e7 Frames 1e7
Figure9: RLtrainingcurvescomparingourlearnedoptimizersandallotherbaselines,eachtrained
ortunedonasingleenvironmentandevaluatedonthesameenvironment. Weshowmeanreturnover
trainingwithstandarderror,evaluatedover16seeds.
24
nruteR
nruteR
nruteR
nruteR
nruteRF Analysis
Throughoutthissection,weincludeanalysisandfiguresexploringthebehaviorofoptimizerslearned
byOPEN. Inparticular,weconsiderthebehaviorofOPENwithregardstodormancy,momentum,
updatesizeandstochasticityindifferentenvironments. Whererelevant,wealsomakecomparisons
tothe‘NoFeatures’optimizer,introducedasabaselineinSection6,toexplorepossibledifferences
introducedbytheadditionalelementsofOPEN.
Due to the number of moving parts in OPEN, it is difficult to make strong claims regarding the
behaviourofthelearnedoptimizersitproduces. Instead,weattempttodrawsomeconclusionsbased
onwhatwebelievethedataplotsincludedbelowsuggest,whilerecognizingthattheblack-boxnature
ofitsupdaterules,whichleadstoalackofinterpretability,isapotentialdrawbackofthemethodin
thecontextofanalysis.
Allplotsincludedbelowpertaintothesingletaskregime,besidessectionF.5whichfocusesspecifi-
callyondeepsea.
F.1 Dormancy
Figure 10 shows the τ = 0 dormancy curves during training for each of the MinAtar [60, 61]
environments. These environments were selected as the only ones where the agent uses ReLU
activationfunctions;ant[63,62]usedatanhactivationfunctionforwhichτ =0dormancyisnot
applicable.
freeway asterix breakout spaceinvaders
0.4 0.10 0.6 0.6
0.3 0.08 0.5
0.2 O NoP E FN eatures 0.06 0.4 00 .. 34
Optim4RL 0.04
0.1 A R Ved M La S Om prop 0.02 0.2 00 .. 12
0.0 Lion 0.00 0.0 0.0
0.00 0.25 0.50 0.75 1.00 1.25 0.00 0.25 0.50 0.75 1.00 1.25 0.00 0.25 0.50 0.75 1.00 1.25 0.00 0.25 0.50 0.75 1.00 1.25
Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3
Figure10: τ =0dormancycurvesforalloptimizersonthefourMinAtarenvironments,evaluated
over16seedseach. Weplotthemeandormancywithstandarderror.
Inallenvironmentsexcludingfreeway,dormancyofallagentsquicklyconvergedto0. Additionally,
ineveryenvironment,OPENproducedmoreplastic(i.e.,lessdormant)agentsthanthefeatureless
optimizer. Wedrawconclusionsaboutthisbehaviorbelow:
• OPENoptimizestowardszerodormancyinasubsetofenvironments,demonstratingitis
capableofdoingso. Thefactthatthisonlyheldinthreeoutoffourenvironments,despite
OPEN outperforming or matching baselines in every environment, suggests it optimizes
towardszerodormancyonlywhentheagentplasticityislimitingperformance(i.e.,when
plasticitylosspreventstheagentfromimproving). Therefore,thoughadormancyregular-
izationterminthefitnessfunctionformeta-trainingmayhavebeenbeneficialforasterix,
breakoutandspaceinvaders,itispossiblethatthiscouldharmtheperformanceinfreeway
and would require expensive hyperparameter tuning of the regularization coefficient to
maximizeperformance. Ourmeta-learningapproachcompletelysidestepsthisproblem.
• Despitehavingthesameoptimizationobjective,NoFeaturesalwayshadhigherdormancy
thanOPEN. IntandemwithourablationfromFigure5,itislikelythattheinclusionofeach
additionalfeaturehasgivenOPENthecapabilitytominimizeplasticityloss.
25
ycnamroD ycnamroD ycnamroD ycnamroDF.2 SimilarityBetweenUpdateandGradient/Momentum
WeevaluatethecosinesimilaritybetweentheupdatefromOPEN and‘NoFeatures’withmomentum
over different timescales, and the gradient, in Figure 11. Momentum is calculated as mt+1 =
βmt+(1−β)gt,whereg tisthegradientattimet. InOPEN,inspiredby[23],weinputmomentum
usingβsin[0.1,0.5,0.9,0.99,0.999,0.9999]. Fortheithtimescale,wedenotethemomentumasm
i
inFigure11. Forthegradient,weincludeanadditionalcomparisonagainstAdam.
freeway asterix ant breakout spaceinvaders
00 .. 22 05 O N AdoP a E F mN eatures 00 .. 22 05 0.20 0.15 0.15
0.15 0.15 0.15 0.10 0.10
0.10 0.10 0.10 0.05 0.05
0.05 0.05
0.05
0.00 0.25 0.50 0.75 1.00 1.25 0.00 0.25 0.50 0.75 1.00 1.25 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.25 0.50 0.75 1.00 1.25 0.00 0.25 0.50 0.75 1.00 1.25
Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3
freeway asterix ant breakout spaceinvaders
00 .. 22 05 O NoP E FN eatures 00 .. 11 02 000 ... 122 702 505 00 .. 00 68 000 ... 111 025 050
0.15 0.08 0.150 0.075
00 .. 01 50 00 .. 00 46 000 ... 011 702 505 00 .. 00 24 00 .. 00 25 50
0.00 0.25 0.50 0.75 1.00 1.25 0.00 0.25 0.50 0.75 1.00 1.25 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.25 0.50 0.75 1.00 1.25 0.00 0.25 0.50 0.75 1.00 1.25
Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3
freeway asterix ant breakout spaceinvaders
000000 ...... 011223 505050 O NoP E FN eatures 000000 ...... 000111 468024 0000 .... 1122 0505 0000000 ....... 0000111 2468024 0000 .... 0112 5050
0.00 0.25 0.50 0.75 1.00 1.25 0.00 0.25 0.50 0.75 1.00 1.25 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.25 0.50 0.75 1.00 1.25 0.00 0.25 0.50 0.75 1.00 1.25
Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3
freeway asterix ant breakout spaceinvaders
0.4 O NoP E FN eatures 0.30 00 .. 34 50 0.20 0.3
0.3 0.25 0.30 0.15
0.20 0.25 0.2
0.2 0.15 0.20 0.10
0.1
0.00 0.25 0.50 0.75 1.00 1.25
00 .. 01 50
0.00 0.25 0.50 0.75 1.00 1.25
00 .. 11 05
0.0 0.5 1.0 1.5 2.0 2.5
0.05
0.00 0.25 0.50 0.75 1.00 1.25
00 .. 01
0.00 0.25 0.50 0.75 1.00 1.25
Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3
freeway asterix ant breakout spaceinvaders
0.30 O NoP E FN eatures 0.4 0.25 0.15
0.25 0.3 0.20 0.3 0.20 0.10 0.2
0.15 0.2 0.15
0.10 0.1 0.10 0.05 0.1
0.050.00 0.25 0.50 0.75 1.00 1.25 0.00 0.25 0.50 0.75 1.00 1.25 0.0 0.5 1.0 1.5 2.0 2.5 0.000.00 0.25 0.50 0.75 1.00 1.25 0.00.00 0.25 0.50 0.75 1.00 1.25
Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3
freeway asterix ant breakout spaceinvaders
0000 .... 0111 8024 O NoP E FN eatures 000 ... 122 505 00 .. 12 50 000 ... 000 468 0000 .... 1122 0505
0.06 0.10 0.10 0.02 0.05
0.04 0.00 0.25 0.50 0.75 1.00 1.25 0.05 0.00 0.25 0.50 0.75 1.00 1.25 0.05 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.00 0.25 0.50 0.75 1.00 1.25 0.000.00 0.25 0.50 0.75 1.00 1.25
Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3
freeway asterix ant breakout spaceinvaders
000 ... 011 802 O NoP E FN eatures 00 .. 11 02 05 00 .. 12 50 00 .. 00 68 000 ... 111 025 050
00 .. 00 46 00 .. 00 57 05 0.10 00 .. 00 24 00 .. 00 57 05
0.02 0.025 0.05 0.025
0.00 0.00 0.25 0.50 0.75 1.00 1.25 0.0000.00 0.25 0.50 0.75 1.00 1.25 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.00 0.25 0.50 0.75 1.00 1.25 0.0000.00 0.25 0.50 0.75 1.00 1.25
Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3
Figure11: CurvesshowingthecosinesimilaritybetweentheupdatesfromOPENand‘NoFeatures’
withthegradientormomentumatdifferentβ timescales. Eachcolumncorrespondstoadifferent
environment,andeachrowtoadifferenttimescale. Inorder,therowsshowcosinesimilaritywith
gradient,followedbymomentumatβ = [0.1,0.5,0.9,0.99,0.999,0.9999]. Weplotmeancosine
similaritieswithstandarderrorover16runs.
FromFigure11,wecanconcludethat:
26
)g,u(soc
)1.0m,u(soc
)5.0m,u(soc
)9.0m,u(soc
)99.0m,u(soc
)999.0m,u(soc
)9999.0m,u(soc
)g,u(soc
)1.0m,u(soc
)5.0m,u(soc
)9.0m,u(soc
)99.0m,u(soc
)999.0m,u(soc
)9999.0m,u(soc
)g,u(soc
)1.0m,u(soc
)5.0m,u(soc
)9.0m,u(soc
)99.0m,u(soc
)999.0m,u(soc
)9999.0m,u(soc
)g,u(soc
)1.0m,u(soc
)5.0m,u(soc
)9.0m,u(soc
)99.0m,u(soc
)999.0m,u(soc
)9999.0m,u(soc
)g,u(soc
)1.0m,u(soc
)5.0m,u(soc
)9.0m,u(soc
)99.0m,u(soc
)999.0m,u(soc
)9999.0m,u(soc• For both OPEN and ‘No Features’, alignment with momentum seems to be maximized
at similar (though not always the same) timescales as the β values tuned for Adam in
1
SectionC.2.
• OPENand‘NoFeatures’rarelyhadsimilarcosinesimilarities. Whileitisdifficulttodiscern
whether this arises as a result of randomness (optimizers trained from a single seed) or
somethingmorefundamental,itmaydemonstratehowtheadditionalelementsof OPEN
havechangeditsoptimizationtrajectory.
• Forbothlearnedoptimizers,cosinesimilaritywithgradientandmomentumgenerallypeaked
anddecreasedoverthetraininghorizon. WhileOPENincludeslifetimeconditioning,which
may partially influence this behavior, it is likely that both optimizers rely on their own
hiddenstates,whichcanbethoughtofasalearnedmomentum,moreastrainingprogresses.
F.3 Non-StochasticUpdate
In the following experiments (i.e., Appendices F.3, F.4, F.5), we find it useful to divide updates
bytheirrespectiveparametervalue. Unlikemosthandcraftedoptimizerswhichproduceshrinking
updatesfromanannealedlearnedrateandgenerallyoutputparameterswithmagnitudesclosetozero,
OPENgenerallyincreasesthemagnitudeofparametersovertrainingwithroughlyconstantupdate
magnitudes. Assuch,normalizingupdateswithrespecttotheparametervalueshowstherelative
effect(i.e.,change)whenanupdateisapplied. Inallcases,weplottheaverageabsolutenormalized
valueovertime.
InFigure12,weexplorehowthemagnitudeofthenon-stochasticupdatechangesovertime(i.e.,
the update before any learned stochasticity is applied, defined as uˆ = α m expα e ). This is
i 1 i 2 i
(cid:104) (cid:105)
normalizedwithrespecttotheparametervalue,andsoiscalculated|uˆ/p|=E
i
|u pˆ ii| .
freeway asterix ant breakout spaceinvaders
0.35 OPEN 1.0 0.18 0.35 0.05
00 .. 23 50 0.8 00 .. 11 46 00 .. 23 50 0.04
0.20 0.6 0.12 0.20 0.03
0.15 0.4 0.10 0.15 0.02
0.10 0.2 00 .. 00 68 00 .. 01 50 0.01
0.050.00 0.25 0.50 0.75 1.00 1.25 0.00 0.25 0.50 0.75 1.00 1.25 0.0 0.5 1.0 1.5 2.0 2.5 0.00 0.25 0.50 0.75 1.00 1.25 0.00 0.25 0.50 0.75 1.00 1.25
Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3
Figure12: Plotsoftheaveragenormalized,non-stochasticupdatemagnitudewithrespecttotime,
withstandarderrorover16seeds.
WefindthatoptimizerslearnedwithOPENallannealtheirstepsizes. Inparticular,therelativesize
oftheupdatewithrespecttotheparametersizedecreasesovertime,withfinalupdatesgenerally
beingmuchsmallerthanatthebeginningoftraining. Thisholdsinallenvironments,thoughwas
weakerinantwhichhadsignificantlysmallerupdatesfromthestart. Interestingly,forallhandcrafted
optimizers,hyperparametertuningalsoconsistentlyproducessmallerlearningratesforant.
F.4 Stochasticity
Figure13exploreshowtheweightofthelearnedstochasticity,δactor,changeswithrespecttotime.
i
Thisstochasticityisincorporatedintotheupdaterulesasuˆactor =uˆactor+α δactorϵ,withϵ∼N(0,1).
i i 3 i
Asinthepreviousplot,thisisnormalizedwithrespecttotheparametervalue,andiscalculatedas
(cid:104) (cid:105)
randomness/p=E
i
| pδ ia ac ct to or r|
i
freeway asterix ant breakout spaceinvaders
0.25 OPEN 0.150 0.030 0.150
00 .. 12 50 000 ... 011 702 505 000 ... 000 122 505 00 .. 11 05 000 ... 011 702 505
0.10 0.050 0.010 0.05 0.050
0.05
0.00 0.25 0.50 0.75 1.00 1.25
00 .. 00 02 05
0.00 0.25 0.50 0.75 1.00 1.25
0.005
0.0 0.5 1.0 1.5 2.0 2.5
0.00
0.00 0.25 0.50 0.75 1.00 1.25
0.025
0.00 0.25 0.50 0.75 1.00 1.25
Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3 Updates 1e3
Figure13: Plotsofthenormalizedstochasticityweightwithrespecttotime. Weplotmeanvalues
withstandarderrorover16seeds.
27
|p/u|
p/ssenmodnar
|p/u|
p/ssenmodnar
|p/u|
p/ssenmodnar
|p/u|
p/ssenmodnar
|p/u|
p/ssenmodnarAsexpected,thisdecreasessignificantlyovertime,suggestingtheoptimizerspromoteslessexplo-
rationovertime. Thismakessense;itisbeneficialtoexploreearlyintraining,whenthereistimeto
continuesearchingforbetterlocalorglobaloptima,butlaterintrainingitisimportanttoconvergeto
thenearestoptimum.
Wealsonotethesignificantlydifferentscalesofnoisewhichareappliedtodifferentenvironments. In
particular,antusesverysmallnoise,suggestingitisnotanenvironmentwhichneedsasignificant
amount of exploration. This may be one reason why OPEN performed similarly to handcrafted
optimizers,and‘NoFeatures’.
F.5 Deepsea
Finally,weconsiderhowtherandomnessusedbyOPENchangesduringtraininginenvironments
ofdifferentsizesfromDeepsea[68,61]. Thisrandomnesswasshowntobecrucialforlearningin
largerenvironmentsinSection7.2. Toanalyzethis,weconsider5differentsizes(10,20,30,40,50),
withthesizecorrespondingtohowmany‘rights’theagenthastotakeinarowtoreceivealarge
reward. Welookatrandomness/p=E i(cid:104) | pδ ia ac ct to or r|(cid:105) ,asinAppendixF.4.
i
10 20 30 40 50
0.09 OPEN Separated 0.09 0.11 0.16 0.22
0.08 0.08 0.10 0.14 0.20
0.07 0.07 0.09 0.12 00 .. 11 68
0.06 0.06 0.08 0.14
0.05 0.05 0.07 0.10 0.12
00 .. 00 34 0.04 0.06 0.08 00 .. 01 80
0 200 400 600 0.030 200 400 600 0 200 400 600 0.060 200 400 600 0 200 400 600
Updates Updates Updates Updates Updates
Figure14:MeannormalizedrandomnessappliedbyOPENfordifferentsizedDeepSeaenvironments,
over128seeds,withstandarderror. Notably,thescaleforlargerenvironmentsisbigger,suchthat
OPENappliesmorenoise(i.e.,moreexploration)inlargerenvironmentsdespitehavingnoawareness
oftheenvironmentsize.
Wefocusontwobehaviorsoftheoptimizer. Firstly,theappliedrandomnessprogressivelydecreases
overtraining,reducingtheexplorationastheagentconverges. Thisfollowssimilarbehaviortofigure
13,wherethelevelofrandomnessdecreasesduringthetraininghorizon. Secondly,theamountof
noiseseemstogrowwiththeenvironmentsize;theoptimizerpromotesmoreexplorationinlarger
environments,despitelackinganyawarenessabouttheenvironmentsize. Thecombinationofthese
twoelementsallowstheagenttoexploresufficientlyinlargerenvironments,whileconvergingto
good,closetooptimalpoliciestowardstheendoftraining.
28
p/ssenmodnar p/ssenmodnar p/ssenmodnar p/ssenmodnar p/ssenmodnarG MultiEnvironmentReturnCurves
Figure15showsreturncurvesthefourMinAtar[60]environmentsincludedin‘multi-task’training
(Section6. Asexpected,themethodofnormalizationusedintraining(i.e.,dividingscoresineach
environmentbyscoresachievedbyAdamandaveragingacrossenvironments)leadsOPEN,andthe
otherlearnedoptimizers,toprincipallyfocusonthosewhichtheycanstronglyoutperformAdamin.
Inparticular,OPENmarginallyunderperformsAdamandotherhandcraftedoptimizersinfreewayand
spaceinvaders. However,itsscoreinasterix,whichisapproximatelydoubleAdam’s,willoutweigh
theseotherenvironments. Weleavefindingbettermethodsformulti-tasktraining,suchasusingmore
principledcurriculatonormalizebetweenenvironments,tofuturework. However,westillnotethat
thisdemonstratesthatOPENiscapableoflearninghighlyexpressiveupdateruleswhichcanfitto
manydifferentcontexts;itperformscomparativelytohandcraftedoptimizersintwoenvironments
andsignificantlyoutperformsthemintwoothers,inadditiontoconsistentlymatchingorbeatingthe
otherlearnedbaselines.
freeway asterix
25
60
50 20
40
15
OPEN
30 Adam
RMSprop 10
20 Lion
VeLO 5
10 Optim4RL
No Features
0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Frames 1e7 Frames 1e7
breakout spaceinvaders
60
150
50
40
100
30
20 50
10
0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Frames 1e7 Frames 1e7
Figure15: RLtrainingcurvesforthemulti-tasksetting. Weshowmeanreturnovertrainingwith
standarderror,evaluatedover16seeds.
29
nruteR
nruteR
nruteR
nruteRH AdditionalGridworlds
SimilartoFigure4,whichlookedathow OPEN generalizedtodifferentagentsizes,weconsider
howOPENadaptstodifferenttraininglengthsinFigure16. Inparticular,wenotethatOPENwas
onlyevertrainedforgridworlds(‘all’,AppendixD)whichranfor3e7trainingsteps,soalllengths
outsideofthisareOOStasks. WealsoconsiderthatinE,thereturnofOPENoftenstartedlowerthan
otheroptimizersandincreasedonlylaterintraining. Assuch,thelifetimeconditioning(i.e.,training
proportion)ofOPENneedstobeleveragedtobeabletoperformwellatshortertraininglengths. If
OPENdidnotmakeuseofthisfeature,itmayendtrainingwithouthavingconvergedtothenearby
optima,insteadfocusingonexploration.
WefindthatOPENisabletobeatAdaminallofthegridsconsideredatthein-distributiontraining
length. For OOS training lengths, OPEN generalizes better than Adam, and only in a couple of
environments(rand_dense, standard_maze)does itnot outperform Adamin the shortesttraining
length. For every other training length, OPEN performs better than Adam in every environment,
furtherdemonstratingitscapabilitytogeneralizetoOOSenvironmentsandtrainingsettings.
in distribution
a)
1.15
1.10
1.05
1.00
0.95
1 2 3 4 5 6
Total Timesteps 1e7
rand_dense rand_sparse rand_long
1.4
OPEN 1.6 1.6 1.3 1.5 1.5
b) 1.2 1.4 1.4
1.1 1.3 1.3
1.2 1.2 1.0
1.1 1.1
0.9 1.0 1.0
1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6
Total Timesteps 1e7 Total Timesteps 1e7 Total Timesteps 1e7
standard_maze sixteen_rooms labyrinth
1.15 1.125
1.10 1.100
1.10 1.075
1.05 1.050
1.05
1.025
1.00
1.00 1.000
0.975
0.95
0.95 0.950
1 2 3 4 5 6 1 2 3 4 5 6 1 2 3 4 5 6
Total Timesteps 1e7 Total Timesteps 1e7 Total Timesteps 1e7
Figure16: IQMofreturnachievedbyOPEN,normalizedbythereturnofAdam,againstarangeof
differenttraininglengthswith95%stratifiedbootstrapconfidenceintervalsfor64seeds. a)shows
performance on the distribution which OPEN was trained on, and Adam was tuned in. b) shows
performanceonOOSgridworlds,withthetoprowcomingfrom[13]andthebottomrowinspired
bymazesfrom[65],allimplementedby[64]. Wemark3e7timestepsasthein-distributiontraining
lengthforbothOPENandAdam.
30
nruteR
dezilamroN
madA
nruteR
dezilamroN
madA
nruteR
dezilamroN
madA
nruteR
dezilamroN
madA
nruteR
dezilamroN
madA
nruteR
dezilamroN
madA
nruteR
dezilamroN
madAI AblationDetails
I.1 FeatureAblationSetup
Wetrain17optimizersforeachfeatureablation,andevaluateperformanceon64seedsperoptimizer,
usingashortenedversionofBreakout. WeusethesamePPOhyperparametersforBreakoutasin
Table 3 besides the total timesteps T, which we shorten to 2×105. Each optimizer takes ∼ 60
minutestotrainononeGPU,andwetrainatotalof119optimizers.
Wetraineachoptimizerfor250generations,keepingallotherMinAtarEShyperparametersfrom
Table7.
I.2 DeepSeaExpandedCurve
We show the final return against size for the Deep Sea environment [68, 61] in Figure 14 for all
optimizers;bothsharedandseparated,withstochasticityandwithout.Theonewithoutcanbethought
ofasadeterministicoptimizer,andisdenoted‘ablated’inthefigure. Forthisexperiment,wetrained
eachoptimizerforonly48iterationsonsizesintherange[4,26];anysizesoutsideofthisrangeat
testtimeareoutofsupportofthetrainingdistribution.
Clearlytheseparatedoptimizerwithlearnedstochasticityistheonlyonewhichisabletogeneralize
acrossmanydifferentsizesofDeepSea. Thisoccursmarginallyatthebehestofoptimizationin
smallerenvironments,wherethestochasticoptimizerstillexploresafterobtainingrewardandsodoes
notconsistentlyreachmaximumperformance. InDeepSea,thefinalreturnscaleisbetween0.99
and−0.01.
Stochastic Shared
Stochastic Separated
Ablated Shared
1.0 Ablated Separated
0.8
0.6
0.4
0.2
0.0
10 20 30 40 50
Deep Sea Size
Figure17: IQMofreturnagainstsizeofenvironmentinDeepSea,withstratifiedbootstrap95%
confidence intervals for 128 random seeds. The only optimizer which is able to generalize has
separateparametersbetweentheactorandcriticandincorporateslearnedstochasticityintoitsupdate.
31
nruteRJ ExperimentalCompute
J.1 Runtimes
We include runtimes (inference) for our experiments with the different optimizers on 4 × L40s
GPUs. Wenotethat,whileOPENtakeslongertoruncomparedtohandcraftedoptimizers,itoffers
significantspeedupoverVeLOandtakesasimilarruntimeasOptim4RLandNoFeaturesinaddition
tosignificantlystrongerperformance. Importantly,thissuggeststheadditionalcomputationusedin
OPENtocalculatefeatures,suchasdormancy,doesnottranslatetosignificantoverheadcomparedto
otherlearnedoptimizers.
Table11: Runtimeforeachoptimizer. Weevaluateeachover16seeds,on4L40sGPUs. Thesetake
advantageofJax’sabilitytoparallelizeovermultipleGPUs[32].
RunTime(s)
Optimizer
freeway breakout spaceinvaders asterix ant
OPEN(single-task) 138 87 94 137 453
OPEN(multi-task) 154 98 109 148 N/A
Adam 91 53 58 105 333
RMSprop 84 38 52 97 322
Lion 83 44 50 96 303
VeLO 190 135 145 185 581
Optim4RL(single-task) 127 85 90 131 404
Optim4RL(multi-task) 168 99 116 157 N/A
NoFeatures(single-task 123 79 88 132 416
NoFeatures(multi-task) 154 93 106 147 N/A
J.2 TrainingCompute
Weusedarangeofhardwarefortraining:NvidiaA40s,NvidiaL40ses,NvidiaGeForceGTX1080Tis,
NvidiaGeForceRTX2080TisandNvidiaGeForceRTX3080s. Theseareallpartofaninternal
cluster. Whilstthismakesitdifficulttodirectlycomparemeta-trainingtimesper-experiment,we
findtrainingtookroughlybetween∼[16,32]GPUdaysofcomputeperoptimizerinthesingle-task
setting,and∼60GPUdaysofcomputeformulti-tasktraining. Wemeasuredinferencetimeforall
optimizersinAppendixJ.
J.3 TotalCompute
Forallofourhyperparametertuningexperiments,weusedatotalof∼ 16GPUdaysofcompute,
usingNvidiaA40GPUs.
Foreachlearnedoptimizerinoursingle-taskexperiments,weusedapproximately∼[16,32]GPU
days of compute. This equates to around 1 GPU year of compute for 3 learned optimizers on 5
environments(=15optimizerstotal). ThiswasrunprincipallyonNvidiaGeForceGTX1080Tis,
NvidiaGeForceRTX2080TisandNvidiaGeForceRTX3080s.
Foreachmulti-taskoptimizer,weusedaround60GPUdaysofcompute,onNvidiaGeForceRTX
2080Tis. Thistotals180GPUdaysofcompute. Wealsoranexperimentsforeachoptimizerwiththe
smallerarchitecture,whichtookasimilarlengthoftime. Thesearenotincludedinthepaper;intotal
multi-tasktrainingusedanother1GPUyearofcompute,takingintoaccounttheomittedresults.
Totrainonadistributionofgridworlds,weused7GPUdaysofcomputeonNvidiaGeForceRTX
2080Tis.
Ourablationstudyused∼8GPUdaysofcompute,runningonNvidiaA40GPUs.
Eachoptimizertook2daystotrainindeepseaonanNvidiaA40GPU.Intotal,thissectionofthe
ablationstudytook8GPUdays.
AsstatedinTable11,inferencewasanegligiblecost(ontheorderofseconds)inthispaper.
32Totalingtheabove,allexperimentsinthispaperusedapproximately2.1GPUyearstorun,thoughit
isdifficulttoaggregateresultsproperlyduetothevarietyofhardwareused.
We ran a number of preliminary experiments to guide the design of OPEN, though these are not
includedinthefinalmanuscript. Weareunabletoquantifyhowmuchcomputewasusedtothisend.
Duetotheexcessivecostoflearnedoptimization,weranonlyonerunforeachsettingandoptimizer
(besidesthemulti-tasksetting,wherewetestedbothsmallandlargearchitecturesandfoundthat
thelargerarchitecturesperformedbestforeveryoptimizeronaverage). Afterconverginguponour
method,andfinishingtheimplementationsofOptim4RLandNoFeatures,wedidnothaveanyfailed
runsandsoincludedallexperimentsinthepaper.
33K CodeRepositoriesandAssetLicenses
Belowweincludeafulllistofassetsusedinthispaper,inadditiontothelicenseunderwhichitwas
madeavailable.
Table12: Mainlibrariesusedinthispaper,linkstothegithubrepositoryatwhichtheycanbefound
andthelicenseunderwhichtheyarereleased.
Name Link License
evosax[33] evosax Apache-2.0
gymnax[61] gymnax Apache-2.0
brax[62] brax Apache-2.0
learned_optimization[43] learned_optimization Apache-2.0
groove(gridworlds)[64] groove Apache-2.0
purejaxrl[15] purejaxrl Apache-2.0
optax[66] optax Apache-2.0
rliable[67] rliable Apache-2.0
34