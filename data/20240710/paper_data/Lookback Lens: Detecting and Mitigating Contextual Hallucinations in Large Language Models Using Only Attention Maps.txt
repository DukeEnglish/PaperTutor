Lookback Lens: Detecting and Mitigating Contextual Hallucinations in
Large Language Models Using Only Attention Maps
Yung-SungChuang† LinluQiu† Cheng-YuHsieh‡ RanjayKrishna‡
YoonKim† JamesGlass†
MassachusettsInstituteofTechnology† UniversityofWashington‡
yungsung@mit.edu
Abstract thissetup,LLMsstrugglewithcontextualhalluci-
nations,frequentlyproducingerrorsintaskssuch
When asked tosummarize articles oranswer
assummarizationanddocument-basedquestionan-
questionsgivenapassage,largelanguagemod-
swering (e.g., Table 1), which can cause serious
els(LLMs)canhallucinatedetailsandrespond
issuesinapplicationssuchasretrieval-augmented
withunsubstantiatedanswersthatareinaccu-
ratewithrespecttotheinputcontext. Thispa- generation(RAG)(Lewisetal.,2020),evenwhen
perdescribesasimpleapproachfordetecting correctdocumentsareretrieved.
such contextual hallucinations. We hypothe- Mostpriorstudiesthatproposemethodstocom-
sizethatcontextualhallucinationsarerelated bathallucinationfocusonthescenariowithoutany
to the extent to which an LLM attends to in-
inputcontext,wherethehallucinationsarisefrom
formation in the provided context versus its
the LLMs’ parametric knowledge. These works
owngenerations. Basedonthisintuition, we
detectandmitigatehallucinationsbygenerallyus-
proposeasimplehallucinationdetectionmodel
ing the LLM’s representations, such as hidden
whoseinputfeaturesaregivenbytheratioof
attentionweightsonthecontextversusnewly states (Burns et al., 2023; Azaria and Mitchell,
generatedtokens(foreachattentionhead). We 2023), MLP outputs (Zhang et al., 2024; Simhi
findthatalinearclassifierbasedontheselook- etal.,2024),attentionblockoutputs(Zhangetal.,
back ratio features is as effective as a richer 2024; Simhi et al., 2024) and attention head out-
detector that utilizes the entire hidden states
puts (Li et al., 2024; Chen et al., 2024b; Simhi
of an LLM ora text-basedentailment model.
et al., 2024). In contrast, the provided contex-
Thelookbackratio-baseddetector—Lookback
tualinformationplaysakeyroleindetectingcon-
Lens—is found to transfer across tasks and
evenmodels,allowingadetectorthatistrained textual hallucinations. Insofar as attention (more
ona7Bmodeltobeapplied(withoutretrain- so than other model internals) provides a human-
ing)toalarger13Bmodel. Wefurtherapply meaningfulmeasureofhowmuchweightisgiven
thisdetectortomitigatecontextualhallucina- tothecontextduringgeneration,thismotivatesthe
tions, andfindthatasimpleclassifier-guided
useofsignalsfromtheattentionmapsforhalluci-
decodingapproachisabletoreducetheamount
nationdetectionandmitigation.
of hallucination, for example by 9.6% in the
Toleveragesignalsfromattentionmaps,westart
XSumsummarizationtask.1
byhypothesizingthatcontextualhallucinationsare
1 Introduction related to the extent to which an LLM attends to
the provided contextual information. Concretely,
Despite the utility and impressive capabilities of
weproposeasimplefeaturecalledlookbackratio,
largelanguagemodels(LLMs),theirtendencyto
whichiscomputedastheratioofattentionweights
generatehallucinations,i.e.,contentthatdeviates
onthegivencontextversusthenewlygeneratedto-
fromfactsorcontextuallyrelevantinformation(Ji
kens. Ateachtimestep,wecalculatethislookback
et al., 2023), presents a significant challenge in
ratioforeachattentionhead,andtrainalinearclas-
their deployment. In this work, we focus on the
sifier,whichwecalltheLookbackLens,todetect
scenarioswherethemodelisprovidedwiththecor-
contextual hallucinations based on the lookback
rectfactswithintheinputcontextbutstillfailsto
ratiofeatures,asillustratedinFigure1. TheLook-
generateaccurateoutputs,aphenomenonweterm
back Lens performs on par with, and sometimes
contextualhallucination. Despitethesimplicityof
evensurpasses,morecomplexfeature-baseddetec-
1Sourcecode:github.com/voidism/Lookback-Lens tors that utilize hidden states from LLMs or text-
1
4202
luJ
9
]LC.sc[
1v17070.7042:viXraTransformer
v~v
N× Attention Weights t t+T-1
x x x … … x x y y y … y y
Add & Norm 1 2 3 N-1N 1 2 3 t-2t-1
Feed
Forward
average average
over X over Y Linear P(Factual)
Classifier Add & Norm
Attention Map H heads
Multi-Head
Attention L layers
Lookback Ratio =
+ T tokens
in a span Lookback Lens
Document: [...] Summary: … T
Figure1: AnillustrationoftheLookbackLens. Weextractattentionweightsandcalculatethelookbackratiosforall
layersandallheads. Wetrainalinearclassifierontheconcatenatedfeaturestopredicttruthfulnessofthegeneration.
basedentailmentmodelstrainedonextensivelyan- kensY respectively. ThelookbackratioLRl,h for
t
notateddatasets. Wecanfurtherintegratethisde- headhinlayerlattimesteptisthencalculatedas:
tectorduringdecodingtoderiveaLookbackLens
GuidedDecodingstrategywhichcanreducecon- LRl,h =
Al t,h(context)
.
textual hallucinations by 9.6% from LLaMA-2- t Al,h(context)+Al,h(new)
t t
7B-Chat in the XSum summarization task. Fur-
To utilize these lookback ratios as input fea-
thermore,ouruseof“higherlevel”attentionmap
tures in detecting hallucinations, we concatenate
features makes it possible to transfer the detec-
thelookbackratiosacrossallheadsandlayersinto
tor across models without retraining, allowing a
afeaturevectorforthetimestept:
LLaMA2-13B-Chatmodeltousethesamedetec-
tor that has been trained on LLaMA-2-7B-Chat, v = [LR1,1,LR1,2,...,LRL,H].
t t t t
and still reduce hallucinations by 3.2% in XSum.
Theseresultscollectivelyhighlightthepotentialof Givenatextspanofinterest{y ,y ,...,y },
t t+1 t+T−1
combatingcontextualhallucinationbyleveraging weaveragethecorrespondinglookbackratiovec-
theinformationfromattentionmaps. tors {v ,v ,...,v } into a single vector v¯.
t t+1 t+T−1
WethenemployalogisticregressionclassifierF
2 ContextualHallucinationsDetection
topredictifthespanisfactual(1)orhallucinated
2.1 LookbackLens (0)basedontheaveragedlookbackratiovector.
To detect contextual hallucinations in LLMs, we
P(y = 1|v¯) = F(v¯) = σ(w⊤v¯+b),
introduce a lookback ratio, a measure based on
the attention distribution of a transformer model. where σ denotes the sigmoid function, w is the
Given a transformer with L layers, each with H weightvector,andbisthebiastermoftheclassifier.
heads, the model processes an input sequence of
DefiningSpan TheLookbackLenspredictsthe
context tokens X = {x ,x ,...,x } of length
1 2 N
probabilityofhallucinationsoverspans. Wecon-
N followed by a set of newly generated tokens
sidertwowaystoobtainspansforagivensequence:
Y = {y ,y ,...,y }togeneratethenexttoken
1 2 t−1
predefinedspansorslidingwindow.
y . For time step t, and for each head, we calcu-
t
1) Predefined Spans: When the hallucinated
late the ratio of attention weights focused on the
and non-hallucinated span annotations are avail-
contexttokensversusthenewlygeneratedtokens.
able,wedirectlytraintheclassifiertodifferentiate
Formally,foreachheadhinlayerl,wedefine:
between them. This is a clean setting where all
N
Al,h(context) = 1 (cid:88) αl , spansareeitherhallucinatedornon-hallucinated.
t N h,i 2)SlidingWindow: Inpractice,wedonothave
i=1
any predefined spans during decoding, thus we
Al,h(new) =
1 N (cid:88)+t−1
αl ,
needaslidingwindowsetupthatiteratesoverall
t t−1 h,j possible spans. Specifically, we process the sen-
j=N+1 tencesintofixed-sizedchunksandtraintheclassi-
where αl and αl are softmax-ed attention fiertopredictalabelof0ifanyhallucinatedcon-
h,i h,j
weightsassignedtocontexttokensX andnewto- tentexistswithinachunk,and1otherwise. Here,
2
L
x
H
over
T
averageDataset Examples Correct samesettingastheLookbackLensbutusedinput
features from the hidden states of LLaMA-2-7B-
CNN/DM 1000 49.6%
NQ 2655 67.8% Chatfromits24th,28th,and32ndlayersinsteadof
thelookbackratio. Thisbaselineresemblesabroad
Table1:DatasetstatisticsandGPT-4oevaluationresults
rangeofexistingmethodsintheliterature(Azaria
onresponsesgreedydecodedbyLLaMA-2-7B-chat.
andMitchell,2023;Simhietal.,2024).
theannotateddataisonlyusedforcreatinglabels,
2.3 Results
not for the span segmentation. This is more real-
isticforclassifier-guideddecoding,butitpresents OurresultsarepresentedinTable2. Weconsider
greater challenges because a chunk can contain bothpredefinedspansegmentationandslidingwin-
bothhallucinatedandnon-hallucinatedcontent. dowwithawindowsizeof8. Weincludethetwo-
fold validation setting on the source task and the
2.2 ExperimentalSetup out-of-domain transfer setting on the target task,
with the tasks either question answering (QA) or
Data TrainingtheLookbackLensrequireslabels
summarization(Sum.). WefindthattheLookback
forhallucinatedandnon-hallucinatedexamples. To
Lensachievesslightlybetterperformancethanthe
obtain these examples, we first prompt LLaMA-
hiddenstates-basedclassifierandsignificantlyout-
2-7B-Chat (Touvron et al., 2023) to greedy de-
performs the NLI models (SoTA and our impl.).
code responses for 1,000 summarization exam-
TheadvantageoftheLookbackLensoverthehid-
plesfromtheCNN/DMdataset(Seeetal.,2017)
denstates-basedclassifierismoresignificantinthe
and 2,655 QA examples from the Natural Ques-
slidingwindowsettings,asshownintheright-hand
tions(Kwiatkowskietal.,2019)followingthesetup
sideofTable2.
of Liu et al. (2024). More details are shown in
Appendix A. Although being prompted to gener- Additionally,weobservethatthehiddenstates-
atecorrectresponses,thedecodedresponseswill based classifier tends to overfit the training sets
containbothhallucinatedandnon-hallucinatedin- duringthetwo-foldvalidation,andpresentasub-
formationastheLLaMAmodelisstillnotperfect. stantialperformancedropwhentransferredtoout-
Then,weemployedGPT-4o(OpenAI,2024)tover- of-domaintasks. Incontrast,LookbackLens,while
ifythetruthfulnessoftheseresponsesandprovide notalwaysfittingthetrainingsetperfectly,consis-
span-level annotations on hallucinated segments tentlyexhibitsbetterperformancewhenappliedto
(detailed prompts in Appendix B). Additionally, out-of-domaintasks. Thiscontrasthighlightsthe
weperformedapilotstudyofhumanannotationon effectivenessandgeneralizabilityofthelookback
asubsetof70examplesofthesummarizationtask ratiofeaturesweextractfromtheattentionmaps.
(detailsinAppendixC),confirminga97%consis-
3 ContextualHallucinationsMitigation
tencyratebetweenGPT-4oannotationsandhuman
judgments,andvalidatingthereliabilityoftheauto-
3.1 LookbackLensGuidedDecoding
matedannotations. WeshowLLaMA-2-7B-Chat’s
Tomitigatetheimpactofcontextualhallucinations
results on both tasks, as evaluated by GPT-4o, in
identified by the Lookback Lens, we introduce a
Table1. Theresultsshowthatthegeneratedsum-
classifier-guideddecodingstrategytoguidethegen-
mariesfromLLaMA-2-7B-Chatstillexhibithallu-
erationtowardmorecontextuallyaccurateoutputs.
cinations about half of the time, highlighting the
This approach serves as a robustness test of the
challengeofsummarizationtasks.
LookbackLens’abilitytohandlevarioustextgener-
Baselines We compare our detection method ationscenarios. Whilepriorstudiesoncontrollable
against several baselines: 1) Text-based entail- textgenerationadjusttheoutputprobabilitiesusing
ment classifier: We fine-tune the DeBERTa-v3- classifiers based on the output tokens (Yang and
base(Heetal.,2021)modelonthesamedatasetof Klein,2021),ourmethodfundamentallydiffersby
CNN/DMandNQasanaturallanguageentailment not using the tokens themselves but rather their
(NLI) task. Additionally, we include the results attentionmapsduringgeneration.
fromastate-of-the-artentailmentmodel(Vectara, We propose Lookback Lens Guided Decoding,
2023)trainedonahugeamountofannotatedNLI whichincorporatesLookbackLens(F)intothede-
data(seedetailsinAppendixE.2)Hiddenstates- codingprocess. Sincealltokensinthevocabulary
based classifier: We train classifiers using the sharethesameattentionpatternduringonedecod-
3PredefinedSpan SlidingWindow=8
Source−−−→Target Source−−−→Target
Method Source Target
Train Test Transfer Train Test Transfer
TextbasedNLI
SoTANLI – Sum. – – 76.6 – – 57.1
SoTANLI – QA – – 58.6 – – 61.8
NLI(ourimpl.) QA Sum. – – 55.1 – – 53.0
NLI(ourimpl.) Sum. QA – – 71.0 – – 64.9
Hiddenstatesbased
32ndLayer QA Sum. 100.0 89.6 79.4 99.0 97.1 56.1
32ndLayer Sum. QA 100.0 82.5 81.8 97.0 94.8 59.4
28thLayer QA Sum. 100.0 91.4 83.6 99.2 97.3 57.7
28thLayer Sum. QA 100.0 83.3 84.7 97.2 95.2 58.8
24thLayer QA Sum. 100.0 92.0 81.3 99.2 97.4 58.3
24thLayer Sum. QA 100.0 83.1 83.0 99.2 97.4 58.3
Attentionmapsbased(Ours)
LookbackLens QA Sum. 98.3 91.2 85.3 88.3 87.1 66.1
LookbackLens Sum. QA 97.7 88.8 82.0 86.2 85.3 66.0
Table2: AUROCoftheclassificationtasksusingpredefinedspansegmentationandslidingwindow(size=8)on
NQ(QA)andCNN/DM(Sum.). Thesourcetaskscores(Train/Test)areaveragedovertwo-foldvalidation.
New Chunk Extract Averaged Lookback Lens Scores
Candidates Lookback Ratios
_ _ Concatenate New Chunk
…
P Cre hv uio nu ks Sample v v_1
2
F F( (v v_1 2) )= =0 0. .1
3 …
to Previous Chunks
Linear
_ _ ✔
v3 Classifier F(v3)=0.9
_ _
v4 F(v4)=0.6 (...repeat until EOS)
Figure2: LookbackLensGuidedDecoding: samplemultiplechunkcandidates, computelookbackratiosfrom
attentionmapstobescoredbyLookbackLens,andselectthebestcandidatethatislesslikelytobehallucinations.
ing step, F cannot directly influence one-step to- NQ(Kwiatkowskietal.,2019),andmulti-turncon-
kenchoice. Instead,F canevaluatemultiple-token versationswithMT-bench(Zhengetal.,2024).
chunks, as each chunk causes different attention FortestingthegeneralizationabilityoftheLook-
patternsinmultipledecodingsteps. backLens,weonlytrainitwiththeCNN/DMsum-
Given the context and partially generated text, marizationdatasetfromthedetectiontaskinSec-
we independently sample a set of k candidate tion2.2. Thus,onlytheXSumdatasetwillbethe
chunks {C 1,C 2,...,C k} at the same decoding same-tasktransfersetting,whileNQandMT-bench
stept. ForeachchunkC j,theassociatedlookback willbecross-tasktransfersetting.
ratiosareaveragedtoformafeaturevectorv¯j. As
showninFigure2,weselectthebestcandidateC∗
XSum TotesttheLookbackLens’seffectiveness
predictedbyF andappendtothegeneration,
attransferringacrossdatadistributionsforthesame
task(summarization),weuse1,000examplessam-
C∗ = argmax F(v¯j).
pled from the testing set of XSum. Prior stud-
Cj∈{C1,C2,...,C k}
ies (Maynez et al., 2020) indicate that traditional
WerepeatthisprocessuntilitgeneratestheEOS evaluationmetricssuchasROUGE(Lin,2004)or
tokenorreachesthemaximumlength. BERTScore(Zhangetal.,2019a)correlatedpoorly
with human evaluation on faithfulness and factu-
3.2 ExperimentalSetup
ality. Recent studies (Chiang and Lee, 2023; Liu
WeevaluateLookbackLensGuidedDecodingon etal.,2023)alsoshowastrongcorrelationbetween
three tasks that involve generating texts condi- GPT-4(OpenAI,2023)evaluationandhumaneval-
tioned on given contexts, including summariza- uation. Thus,wereporttheaveragedaccuracyfrom
tion with XSum (Narayan et al., 2018), QA with thebinaryjudgmentsofGPT-4o,withtheprompts
4inAppendixB.Wealsoconductapilotstudyfor MT-Bench
Method XSum NQ
human evaluation on GPT-4o’s judgment in Ap-
Hallu. Ori.
pendix C, finding that 97% of the GPT-4o judg-
GreedyDecoding 49.0 71.2 6.08 5.10
mentsareconsistentwithhumanjudgment.
Text-basedclassifierguideddecoding
Natural Questions We use the NQ data from SoTANLI† 59.0 74.2 6.12 5.03
the setup of Liu et al. (2024) we describe in Ap- NLI(ourimpl.) 44.1 72.5 5.72 4.99
pendixGandevaluatethebestspanexactmatchfol- Hiddenstatesbasedclassifierguideddecoding
lowingKandpaletal.(2023);Mallenetal.(2023). 32ndlayer 48.3 73.9 5.49 4.91
28thlayer 48.9 73.0 5.71 5.06
MT-Bench We consider a multi-turn conversa- 24thlayer 47.5 73.9 5.65 5.16
tionssetupwherethemodelneedstofollowprevi- LookbackLensguideddecoding
ouschathistory. WeuseMT-bench(Zhengetal.,
Ours 58.6 74.2 6.27 5.10
2024), a multi-turn instruction-following bench-
markcovering8categories. Wefocusexclusively Table3: Decodingresultsusing8candidatesperchunk
ongeneratingresponsesforthesecondturnanduse in a chunk size of 8. We compare our methods with
greedydecodingandclassifier-guideddecodingusing
GPT-3.5’sresponsesasthedefaultforthefirstturn.
the NLI models, and hidden state representations of
We use GPT-4 to score the model’s answers on a
different layers. †The SoTA NLI is trained on 731k
scaleof1to10basedonvariousfactors,including
examplessoitmaynotbedirectlycomparable.
helpfulness,relevance,accuracy,depth,creativity,
andlevelofdetailoftheresponse. istrainedonroughly731kannotatedsummariza-
Additionally,sinceweareparticularlyinterested tionexamples,whichis700×largercomparedto
inmitigatingcontextualhallucinations,wefurther our1ktrainingset. (SeeAppendixE.)Incontrast,
excludemathquestionsandevaluatetheremaining decodingguidedbyhiddenstates-basedortheNLI
50generalquestions. WespecificallyinstructGPT- (our implementation) classifiers, both trained on
4otofocusonwhethertheresponsesarefaithfulto thesamedataofourmethod,canonlyslightlyim-
the chat history (see prompt in Appendix B). We provetheperformanceonNQ,butnotforXSum,
refertothissetupasMT-Bench(hallu.). probablyduetotheissueofdistributionshift,high-
lightingtheadvantagesofLookbackLensingener-
Baselines Toevaluatetheperformanceofourpro-
alizationability.
posedmethod,wecompareditagainstthefollow-
For MT-bench, we evaluate both settings: the
ingbaselines: 1)GreedyDecoding: generatingre-
originalsetting(ori.) andthesettingthatisspecifi-
sponsesusingtheLLaMA-2-7B-Chatmodel(Tou-
callyforjudgingcontextualhallucinations(hallu.).
vron et al., 2023) through greedy decoding. 2)
Wedonotexpectourmethodcanimproveonthe
OtherClassifier-GuidedDecoding: usingexactly
originalsetting,becauseitevaluatesmanyfactors
thesamesettingbutwithdifferentclassifiersintro-
suchashelpfulness,relevance,etc. Butweexpect
duced in Section 2.2, including text-based entail-
toseeanimprovementonthehallucinationsetting.
mentclassifiersandhiddenstates-basedclassifiers.
The results shown in Table 3 suggest that our de-
3.3 MainResults codingmethodcanboosttheperformanceonthe
hallucination setting while maintaining the same
Weshowourresultsusing8candidatesperchunk
performanceintheoriginalsetting, whichshows
inachunksizeof8inTable3,andtheablationwith
thatourdecodingmethodiseffectiveinreducing
different chunk sizes is shown in Table 6. Look-
hallucinations without compromising the overall
backLensGuidedDecodingcanimprovetheper-
generationquality.
formanceonbothin-domaintask(XSum,by9.6%)
andout-of-domaintasks(NQ,by3%). Theoriginal
4 Cross-modelTransfer
greedydecodingresultsonXSumachieved49.0%
correct which means 510 examples were halluci- Onebenefitofusingthelookbackratiotocapture
nated. Ourdecodingmethodsignificantlyreduced higher-levelmodelpatternsforhallucinationdetec-
thenumberofhallucinatedexamplesfrom510to tionisitspotentialtobettertransferacrossmodels.
414, resultinginan18.8%reductioninthehallu- Aclassifiertrainedwithonemodel’slookbackra-
cinatedexamples. Thisresultisonparwithusing tio could potentially be applied to another model
SoTANLItoguidethedecoding,whereSoTANLI without retraining, provided correlation between
5Predefined Sliding Method XSum NQ
Source Target
Span Window
Greedy 52.9 74.0
LookbackLens:Train13B→Test13B
Text-basedclassifierguideddecoding
QA Sum. 84.0 60.4
SoTANLI† 59.6 74.4
Sum. QA 84.3 60.8
QA-train QA 93.3 63.7 CNN/DM NQ-train CNN/DM
Method
→XSum →NQ →NQ
LookbackLens:Train7B→Test13B
LookbackLensguideddecoding
QA Sum. 73.5 58.8
Sum. QA 78.2 60.5 13B→13B 57.9 75.6 74.8
QA-train QA 80.6 62.4 7B→13B 56.1 76.4 73.7
Table4: Crossmodeltransferresultsondetectiontasks. Table5: CrossmodeltransferfromLLaMA-2-7B-chat
toLLaMA-2-13B-chatusinggreedydecodingandclas-
thetargetmodel’sattentionpatternandthatofthe sifierguidedsamplingmethodswithchunksize8.
originalmodel. Here,weshowthatwecantransfer
a Lookback Lens trained on attention maps from Guided Decoding. We conduct the same-task
LLaMA-2-7B-ChattoLLaMA-2-13B-Chatwith- transfersetting: NQ-train(7B)toNQ(13B),and
outanyretraining. CNN/DM(7B)toXSum(13B).InTable5,weob-
Since the total numbers of attention heads are serveaperformanceimprovementsimilartosame-
different in 7B and 13B models, and there is no modeltransferusing13Bitself,orusingtheSoTA
obvious one-to-one mapping between the heads, NLI model applied on the 13B decoding. How-
weusealinearregressionmodeltomaptheheads ever,oncross-task+cross-modeltransfersettings:
from the 13B model to the heads in 7B model. CNN/DM (7B) to NQ (13B), we do not observe
Concretely, we have 1024 heads in 7B and 1600 significantimprovementswhereweattributetothe
heads in 13B. We extract the averaged lookback largerdistributionshift. Weleavethischallenging
ratio per head for all the |D| training examples, settingforfuturework.
resultingina1024×|D|matrixanda1600×|D|
matrix.2 We then fit a linear regression model to 5 DiscussionsandAblations
map the heads to reconstruct the 7B heads from
Inthissection,wefurtherconductvariousexperi-
13Bheads. Afterapplyingthelineartransformation
mentsandablationstudiesontheLookbackLens
to the lookback ratio from 13B, the transformed
anditscorrespondingclassifierguideddecoding.
headscanbedirectlyusedby7B’sclassifiers. See
detailsinAppendixE.
Effect of Chunk Size In Section 3.3 (Table 3),
ThedetectionresultsareshowninTable4. We weexperimentwithchunksize=8. Here,westudy
first show the same-model (13B→13B) + cross- theeffectofvaryingchunksizes,from4,8,to16.
tasktransferresult,andthecross-model(7B→13B) We see that there is a slight trend that Lookback
+cross-tasktransferresult. Althoughcross-model Lens guided decoding prefers shorter chunk size
transferyieldsslightlyworseresultscomparedto forNQandlongerchunksizeforXSum. However,
same-modeltransfer, theAUROCscoresarestill ingeneraltheimprovementsareconsistentacross
non-triviallyhigh. Considerthatdoingcross-model different chunk sizes, thus reducing the need to
+cross-tasktransferatthesametimemaybetough optimizeforchunksizes.
toLookbackLens,wealsoincludeonemoresetting
that does training on 2.5K examples of the NQ Predictive Power of Different Heads In the
trainingset3 andthentransfertotheNQtestingset. aforementioned experiments, we utilize all atten-
Weseethecross-modelsame-tasktransferresults tionheadstotraintheLookbackLens. Wearethus
areevenclosertothesame-modeltransferresults. interestedinhowthepredictivepowerisdistributed
amongdifferentheadsinmakingpredictions. That
Given promising results on detection tasks,
is, how much performance can we recover if we
we apply cross-model transfer to Lookback Lens
onlyutilizeasubsetofheads? Toanswerthis,we
2Toensurethattwomodelsaregeneratingthesamecontent use the coefficients in the linear classifier of the
whenextractinglookbackratio,wedecodefrom7Bandrun LookbackLens(inSection2)toestimatetheimpor-
the13Bmodelonthe7Boutputs.
tanceofeachheadindetectinghallucinations.
3TheNQ-train2.5Kdataisannotatedinthesamemethod
toannotateNQtestingset,asdescribedinSection2.2. InTable7,weshowtheresultsondetectiontasks
6Method NQ XSum PredefinedSpan
Layers
Chunksize= 4 8 16 4 8 16 QA→Sum. Sum.→QA
Greedy 71.2 49.0 Layer1-4 69.6 64.0
Layer5-8 75.6 70.1
Text-basedclassifierguideddecoding
Layer9-12 75.4 68.3
SoTANLI† 73.7 74.2 74.4 57.3 59.0 62.1 Layer13-16 81.2 78.2
Layer17-20 80.8 78.2
Hiddenstatesbasedclassifierguideddecoding Layer21-24 64.4 73.1
32ndlayer 72.6 73.9 72.7 48.9 48.3 48.3 Layer25-28 66.0 74.4
28thlayer 72.9 73.0 74.1 47.2 48.9 47.1 Layer29-32 66.4 71.4
24thlayer 75.0 73.9 72.5 47.6 47.5 51.2 Layer1-32 85.3 82.0
LookbackLensguideddecoding
Table8: Cross-tasktransferAUROCamonglayers.
Ours 75.4 74.2 74.3 53.2 58.6 57.7
that the positive heads may specialize at context
Table6: Performancecomparisononvariousdatasets grounding,andthushigherlookbackratioonthese
usingdifferentmethodsandchunksizes.
headsleadstomorefactualresponse. Ontheother
hand,thenegativeheadsmaybecriticalatensuring
PredefinedSpan
consistencyinitsowngeneration,andthusshould
Method QA→Sum. Sum.→QA
attend to the generated tokens more. We leave
Allheads 85.3 82.0
furtherinvestigationonthisinterestingbalancefor
Top-kheadsonly
futurework. Meanwhile,wevisualizethelookback
withk= 10 50 100 10 50 100
ratioofpositive/negativeheadsinAppendixF.
Largestmag. 71.2 82.3 82.8 79.2 80.3 81.1
Mostpositive 65.1 74.9 75.4 66.3 70.3 74.4 Reducing Number of Layers We experiment
Mostnegative 59.5 67.5 74.4 66.4 70.2 73.0
with using only a subset of layers for Lookback
Lens, as shown in Table 8. We can see that the
Table 7: Cross-task transfer AUROC using top-k at-
tentionheadsselectedaccordingto: coefficientswith predictivepowerisnotconcentratedinanysubset
thelargestmagnitude(largestmag.),mostpositive,and oflayers,asnoneofthemcanrecovertheperfor-
mostnegative. Weconsiderk=10,50,and100. manceofthefullmodelthatusesalllayers. How-
ever,weobservethatthemiddlelayers(13-16,17-
achievedbydifferentdetectorstrainedusingonlya
20)areslightlymoreusefulthanotherlayers.
subsetoftop-kheadswiththelargestmagnitudeof
coefficientsintheoriginalLookbackLenstrained Qualitative Study We show qualitative exam-
will all heads. The results show that the predic- plesfromXSuminFigure3toillustratehowLook-
tivepowerisnotconcentratedonlyonasubsetof backLensguideddecodingimprovesperformance.
heads. Usingonlytop-10headsisworsethanusing GreedydecodingfromLLaMA-2-7B-Chatresults
allheads,andincreasingk consistentlyimproves inahallucination,i.e. $100m(£64m),thatdoesnot
performanceandtop-100headslargelyrecoverthe exist in the input document. However, the Look-
model’sperformanceusingallheads. backLensisabletoassignlowscoresforthechunk
candidatesthathavecontextualhallucinations(as
More interestingly, we also include the results
markedinred). Therefore,LookbackLensGuided
that only select the top-k heads among the heads
Decodingisabletohelpthemodelgenerateasum-
withmostpositive/negativecoefficients,whichare
marythatisfactualtothegivencontext.
positive/negativelycorrelatedtofactuality. Onthe
heads with positive coefficients, higher lookback
6 RelatedWork
ratio (i.e., when the heads attend at the context
more)indicateshigherfactualityandlesshalluci- HallucinationsinLLMs. Simhietal.(2024)de-
nation;conversely,headswithnegativecoefficients fined close-book hallucination vs open-book hal-
suggest a lower lookback ratio (i.e., attending to lucination for settings of relying on parametric
generatedtokensmore)ismorelikelytobetruth- knowledgevsknowledgeincontext. Wetermopen-
ful. Table7showsthatnoneofpositiveornegative bookhallucinationascontextualhallucinationfor
heads alone can be on par with using the top-k betterclarity. Previousstudiesinhallucinationspri-
largest magnitude heads. This result implies that marily focus on close-book hallucinations (Chen
bothpositiveandnegativeheadsarecriticalfora etal.,2023;Minetal.,2023;Chernetal.,2023)and
modeltogeneratefactualresponses. Weconjecture theirdetection(AzariaandMitchell,2023;Simhi
7Document
She had the "biggest year of her career", playing 95 shows, bringing in an average $2.4m (£1.5m) per city, according to Forbes.
Endorsement deals with companies like Pepsi and H&M, along with the surprise album she released in December 2013, helped her to the top spot.
The self-titled album was released on iTunes with no prior promotion.
Beyonce has just announced she'll be releasing a platinum version of that album later this month, which will include new songs and a concert video.
Source: Forbes
Taylor Swift came in second on the Forbes list, taking home an estimated $64m (£40m). …[truncated]
Summary: Greedy Decoding
Beyonce topped the Forbes list of highest-paid female musicians, earning an estimated $100m (£64m) in the past year.
Summary: Lookback Lens Guided Decoding
According to Forbes, Beyonce had the , thanks to endorsement deals and her
biggest year of her career in terms (0.99) surprise album. Taylor Swift came (0.94)
According to Forbes, Beyonce had the
According to Forbes, Beyonce came in first ⃛⃛ biggest year of her career in terms of and earning an estimated $80m overall.
place on their list of highest (0.97) earnings, bringing in an average of Endorsement deals (0.12)
$2.4m per city
According to Forbes, Beyonce earned an and earning an estimated $100m overall.
estimated $30m (£ (0.65) (0.05)
Figure3: QualitativeexampleonXSumusingtheLLaMA-2-7B-ChatmodelwithgreedydecodingandLookback
LensGuidedDecoding. ThenumbersintheparenthesisshowthepredictedscoresfromtheLookbackLens.
etal.,2024)andmitigation(Lietal.,2024;Chuang Classifierguidedgeneration. Classifierguided
etal.,2024;Chenetal.,2024a;Zhangetal.,2024). generationaims tocontrol attributeslike topicor
MostofthestudiesfocusonleveragingLLM’sin- sentiment in text generation. PPLM (Dathathri
ternalrepresentations,suchashiddenstates(Burns etal.,2019)usesgradientascenttoadjustLMprob-
etal.,2023;AzariaandMitchell,2023),MLPout- abilitiesviaattributeclassifiers. FUDGE(Yangand
puts (Zhang et al., 2024; Simhi et al., 2024), at- Klein,2021)usesanattributepredictoronpartial
tention block outputs (Zhang et al., 2024; Simhi sequencestomodifyLMprobabilities. Ourmethod
et al., 2024) and attention head outputs (Li et al., uniquelyguidesgenerationusingclassifiersonat-
2024;Chenetal.,2024b;Simhietal.,2024). Our tentionmaps,settingitapartfrompriorapproaches.
work, however, focuses on contextual hallucina-
Self-attentionandmodelbehavior. Theatten-
tions,wheremodelsproducecontentinconsistent
tion mechanism, initially introduced in RNN-
with the provided context (Maynez et al., 2020;
based encoder-decoder for neural machine trans-
Fabbrietal.,2021;Shietal.,2023). Thus,differ-
lation(Bahdanauetal.,2015;Luongetal.,2015),
ent from prior studies, we focus on the attention
was later adopted in the Transformer model’s
mapsinsteadofinternalrepresentations,aswebe-
self-attention module (Vaswani et al., 2017), en-
lieve that the attention maps patterns record how
abling greater parallelization. Self-attention’s in-
theLLMprocessthegivencontextualinformation.
terpretability has led researchers to use it for un-
Mostofthepriorstudiestreatdetectionandmiti-
derstanding model behaviors (Clark et al., 2019;
gationastwoseparatetasks,expectforSimhietal.
Hao et al., 2021; Vashishth et al., 2019). Our
(2024);Chenetal.(2024a). Ourworkfocusesnot
work demonstrates that attention maps in LLMs
onlyondetection,butalsotriestoincorporatethe
areeffectivefordetectingcontextualhallucinations,
detectorintothedecodingprocesstofurthermiti-
providingalightweightandinterpretablesolution
gatethecontextualhallucinations. Recently,Simhi
comparedtocomplexhiddenrepresentationmeth-
et al. (2024) also explored detecting and mitigat-
ods(Zhangetal.,2024;Chenetal.,2024b).
ingbothclose-bookandopen-bookhallucinations.
However,theiropen-bookhallucinationsettingis 7 Conclusion
limitedtoDisentQA(Neemanetal.,2023),which
WeintroducetheLookbackLens,alightweightclas-
creates knowledge conflicts between parametric
sifierdesignedtodetectcontextualhallucinations
knowledgeandgivencontext. Incontrast,wefocus
byutilizingthelookbackratio,whichiscomputed
on LLaMA-2’s naturally generated responses to
solely from attention weights. This classifier not
capture general cases where LLMs fail to follow
onlyeffectivelyidentifiescontextualhallucinations
thecontext,notjustduetoknowledgeconflicts.
but also mitigates them through Lookback Lens
8
⃛⃛ ⃛⃛GuidedDecodingfromtheLLM.Remarkably,the other applications that rely on LLMs. When de-
method is transferable across various tasks, and ployed,however,ourapproachstillcarriestheis-
even across models after mapping their attention suesstemmingfromLLMs,whichmeansthatthere
heads. This research opens up new possibilities isariskthattheLLMcanproducebiased,harmful,
forleveragingattentionmapinformationtocombat oroffensiveoutput. Therefore,cautionshouldbe
hallucinationsinlargelanguagemodels. exercisedbeforeimplementingsimilarapproaches
inreal-worldapplications.
Limitations
DespitetheeffectivenessoftheLookbackLensand
its decoding, there are several limitations to con-
sider.
• First,theperformanceupperboundofLook-
backLensGuidedDecodingislimitedbythe
samplingcapabilitiesoftheLLMitself. Ifthe
LLMfailstosamplethecorrectchunkamong
theeightcandidates,theLookbackLenscan-
notcorrecttheerror.
• Second, although the Lookback Lens is a
lightweightclassifierwithnegligibleinference
time,therequirementtosamplemultiplecan-
didates from the LLM increases the total in-
ference time. We argue that Lookback Lens
Guided Decoding is a preliminary approach
thatdemonstratesthefeasibilityofintegrating
theLookbackLensintothedecodingprocess,
aswellasarobustnesstestfortheLookback
Lenstohandlevarioustextgenerationscenar-
ios. However, other options, such as inter-
veningintheattentionmapmechanismbased
on Lookback Lens signals, could potentially
achievefasterinference,andweleavethisfor
futurework.
• Lastly,theLookbackLensreliesonannotated
examplesofaround1k-2ktotraintheclassi-
fier. Whileotherend-to-endmethods(Chuang
etal.,2024)canmitigateclose-bookhalluci-
nationswithouttrainingdata,theylackinter-
pretability due to the absence of a detection
step. Nevertheless,webelievethatrequiring
1,000annotatedexamplesisafeasiblesetting.
EthicsStatement
Inthisresearch,weusedpubliclyavailabledatasets
and we did not collect any personal information.
All datasets and models are used in accordance
withtheirintendeduseandlicenses. Ourmethod
isdesignedtoimprovethefactualityoflargelan-
guagemodels(LLMs),whichcanhaveapositive
impact on various applications, such as question-
answering systems, summarization systems, and
9References International Conference on Learning Representa-
tions.
Amos Azaria and Tom Mitchell. 2023. The internal
stateofanllmknowswhenit’slying. InFindings Alexander R Fabbri, Chien-Sheng Wu, Wenhao Liu,
of the Association for Computational Linguistics: and Caiming Xiong. 2021. Qafacteval: Improved
EMNLP2023,pages967–976. qa-basedfactualconsistencyevaluationforsumma-
rization. arXivpreprintarXiv:2112.08542.
DzmitryBahdanau,KyungHyunCho,andYoshuaBen-
gio. 2015. Neural machine translation by jointly YaruHao,LiDong,FuruWei,andKeXu.2021. Self-
learningtoalignandtranslate. In3rdInternational attentionattribution:Interpretinginformationinterac-
Conference on Learning Representations, ICLR tionsinsidetransformer. InProceedingsoftheAAAI
2015. Conference on Artificial Intelligence, volume 35,
pages12963–12971.
CollinBurns,HaotianYe,DanKlein,andJacobStein-
hardt. 2023. Discovering latent knowledge in lan- PengchengHe,JianfengGao,andWeizhuChen.2021.
guagemodelswithoutsupervision. InTheEleventh Debertav3:Improvingdebertausingelectra-stylepre-
International Conference on Learning Representa- trainingwithgradient-disentangledembeddingshar-
tions. ing. Preprint,arXiv:2111.09543.
OrHonovich, RoeeAharoni, JonathanHerzig, Hagai
JifanChen,GraceKim,AniruddhSriram,GregDurrett,
Taitelbaum,DoronKukliansy,VeredCohen,Thomas
andEunsolChoi.2023. Complexclaimverification
Scialom, Idan Szpektor, Avinatan Hassidim, and
withevidenceretrievedinthewild. arXivpreprint
Yossi Matias. 2022. True: Re-evaluating factual
arXiv:2305.11859.
consistencyevaluation. InProceedingsofthe2022
ShiqiChen,MiaoXiong,JuntengLiu,ZhengxuanWu, Conference of the North American Chapter of the
Teng Xiao, Siyang Gao, and Junxian He. 2024a. AssociationforComputationalLinguistics: Human
In-context sharpness as alerts: An inner represen- LanguageTechnologies,pages3905–3920.
tationperspectiveforhallucinationmitigation. arXiv
ZiweiJi,NayeonLee,RitaFrieske,TiezhengYu,Dan
preprintarXiv:2403.01548.
Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea
Madotto,andPascaleFung.2023. Surveyofhalluci-
ZhongzhiChen,XingwuSun,XianfengJiao,Fengzong
nationinnaturallanguagegeneration. ACMComput-
Lian,ZhanhuiKang,DiWang,andChengzhongXu.
ingSurveys,55(12):1–38.
2024b. Truth forest: Toward multi-scale truthful-
nessinlargelanguagemodelsthroughintervention
Nikhil Kandpal, Haikang Deng, Adam Roberts, Eric
without tuning. In Proceedings of the AAAI Con-
Wallace, and Colin Raffel. 2023. Large language
ferenceonArtificialIntelligence,volume38,pages
modelsstruggletolearnlong-tailknowledge. InIn-
20967–20974.
ternationalConferenceonMachineLearning,pages
15696–15707.PMLR.
IChern,SteffiChern,ShiqiChen,WeizheYuan,Kehua
Feng,ChuntingZhou,JunxianHe,GrahamNeubig, TomKwiatkowski, JennimariaPalomaki, OliviaRed-
PengfeiLiu,etal.2023. Factool: Factualitydetec- field,MichaelCollins,AnkurParikh,ChrisAlberti,
tion in generative ai–a tool augmented framework DanielleEpstein,IlliaPolosukhin,JacobDevlin,Ken-
for multi-task and multi-domain scenarios. arXiv tonLee,etal.2019. Naturalquestions: abenchmark
preprintarXiv:2307.13528. forquestionansweringresearch. Transactionsofthe
Association for Computational Linguistics, 7:453–
Cheng-HanChiangandHung-YiLee.2023. Canlarge
466.
languagemodelsbeanalternativetohumanevalua-
tions? InProceedingsofthe61stAnnualMeetingof PhilippeLaban,TobiasSchnabel,PaulNBennett,and
theAssociationforComputationalLinguistics(Vol- Marti A Hearst. 2022. Summac: Re-visiting nli-
ume1: LongPapers),pages15607–15631. basedmodelsforinconsistencydetectioninsumma-
rization. TransactionsoftheAssociationforCompu-
Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon tationalLinguistics,10:163–177.
Kim,JamesRGlass,andPengchengHe.2024. Dola:
Decodingbycontrastinglayersimprovesfactualityin PatrickLewis,EthanPerez,AleksandraPiktus,Fabio
largelanguagemodels. InTheTwelfthInternational Petroni,VladimirKarpukhin,NamanGoyal,Hein-
ConferenceonLearningRepresentations. richKüttler, MikeLewis, Wen-tauYih, TimRock-
täschel,etal.2020. Retrieval-augmentedgeneration
Kevin Clark, Urvashi Khandelwal, Omer Levy, and forknowledge-intensivenlptasks. AdvancesinNeu-
ChristopherDManning.2019. Whatdoesbertlook ralInformationProcessingSystems,33:9459–9474.
at? an analysis of bert’s attention. arXiv preprint
arXiv:1906.04341. JunyiLi,XiaoxueCheng,WayneXinZhao,Jian-Yun
Nie, and Ji-Rong Wen. 2023. Halueval: A large-
SumanthDathathri,AndreaMadotto,JaniceLan,Jane scalehallucination evaluationbenchmark forlarge
Hung,EricFrank,PieroMolino,JasonYosinski,and languagemodels. InProceedingsofthe2023Con-
RosanneLiu.2019. Plugandplaylanguagemodels: ferenceonEmpiricalMethodsinNaturalLanguage
Asimpleapproachtocontrolledtextgeneration. In Processing,pages6449–6464.
10Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter OpenAI.2023. Gpt-4technicalreport.
Pfister, and Martin Wattenberg. 2024. Inference-
time intervention: Eliciting truthful answers from OpenAI.2024. Hellogpt-4o.
alanguagemodel. AdvancesinNeuralInformation
ProcessingSystems,36. TalSchuster,AdamFisch,andReginaBarzilay.2021.
Get your vitamin C! robust fact verification with
Chin-YewLin.2004. Rouge: Apackageforautomatic contrastive evidence. In Proceedings of the 2021
evaluation of summaries. In Text summarization Conference of the North American Chapter of the
branchesout,pages74–81.
AssociationforComputationalLinguistics: Human
LanguageTechnologies,pages624–643,Online.As-
NelsonFLiu,KevinLin,JohnHewitt,AshwinParan-
sociationforComputationalLinguistics.
jape,MicheleBevilacqua,FabioPetroni,andPercy
Liang.2024. Lostinthemiddle: Howlanguagemod-
AbigailSee,PeterJLiu,andChristopherDManning.
elsuselongcontexts. TransactionsoftheAssociation
2017. Gettothepoint: Summarizationwithpointer-
forComputationalLinguistics,12:157–173.
generatornetworks. InProceedingsofthe55thAn-
nualMeetingoftheAssociationforComputational
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang,
Linguistics (Volume 1: Long Papers), pages 1073–
Ruochen Xu, and Chenguang Zhu. 2023. G-eval:
1083.
Nlgevaluationusinggpt-4withbetterhumanalign-
ment. In Proceedings of the 2023 Conference on
Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia
EmpiricalMethodsinNaturalLanguageProcessing,
Tsvetkov, Luke Zettlemoyer, and Scott Wen-tau
pages2511–2522.
Yih. 2023. Trusting your evidence: Hallucinate
Minh-Thang Luong, Hieu Pham, and Christopher D less with context-aware decoding. arXiv preprint
Manning. 2015. Effective approaches to attention- arXiv:2305.14739.
based neural machine translation. In Proceedings
of the 2015 Conference on Empirical Methods in AdiSimhi,JonathanHerzig,IdanSzpektor,andYonatan
NaturalLanguageProcessing,pages1412–1421. Belinkov.2024. Constructingbenchmarksandinter-
ventionsforcombatinghallucinationsinllms. arXiv
AlexMallen,AkariAsai,VictorZhong,RajarshiDas, preprintarXiv:2404.09971.
Daniel Khashabi, and Hannaneh Hajishirzi. 2023.
When not to trust language models: Investigating James Thorne, Andreas Vlachos, Christos
effectivenessofparametricandnon-parametricmem- Christodoulopoulos, and Arpit Mittal. 2018.
ories. InProceedingsofthe61stAnnualMeetingof FEVER:alarge-scaledatasetforfactextractionand
theAssociationforComputationalLinguistics(Vol- VERification. InNAACL-HLT.
ume1: LongPapers),pages9802–9822.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Ryan McDonald. 2020. On faithfulness and factu- Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
ality in abstractive summarization. arXiv preprint Bhosale, et al. 2023. Llama 2: Open founda-
arXiv:2005.00661. tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.
Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike
Lewis, Wen-tau Yih, Pang Wei Koh, Mohit Iyyer,
Shikhar Vashishth, Shyam Upadhyay, Gaurav Singh
Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023.
Tomar, and Manaal Faruqui. 2019. Attention in-
Factscore: Fine-grainedatomicevaluationoffactual
terpretability across nlp tasks. arXiv preprint
precisioninlongformtextgeneration. arXivpreprint
arXiv:1909.11218.
arXiv:2305.14251.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Shashi Narayan, Shay B Cohen, and Mirella Lapata.
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
2018. Don’tgivemethedetails,justthesummary!
Kaiser,andIlliaPolosukhin.2017. Attentionisall
topic-aware convolutional neural networks for ex-
youneed. Advancesinneuralinformationprocessing
treme summarization. In Proceedings of the 2018
systems,30.
Conference on Empirical Methods in Natural Lan-
guageProcessing,pages1797–1807.
Vectara.2023. vectarahallucination_valuation_model.
Ella Neeman, Roee Aharoni, Or Honovich, Leshem https://huggingface.co/vectara/hallucina
Choshen, Idan Szpektor, and Omri Abend. 2023. tion_evaluation_model. Accessed: 2024-06-12.
Disentqa: Disentanglingparametricandcontextual
knowledgewithcounterfactualquestionanswering. KevinYangandDanKlein.2021. Fudge: Controlled
In Proceedings of the 61st Annual Meeting of the text generation with future discriminators. In Pro-
AssociationforComputationalLinguistics(Volume ceedingsofthe2021ConferenceoftheNorthAmer-
1: LongPapers),pages10056–10070. icanChapteroftheAssociationforComputational
Linguistics: HumanLanguageTechnologies,pages
OpenAI.2022. Introducingchatgpt. 3511–3535.
11Shaolei Zhang, Tian Yu, and Yang Feng. 2024. A DataCreationforLookbackLens
Truthx: Alleviating hallucinations by editing large
language models in truthful space. arXiv preprint Ourexperimentalsetupaimstoevaluatetheability
arXiv:2402.17811. ofLookbackLenstodetecthallucinationsinlarge
languagemodelswithattentionmaps. Weconsider
TianyiZhang,VarshaKishore,FelixWu,KilianQWein-
the summarization task and question-answering
berger,andYoavArtzi.2019a. Bertscore:Evaluating
text generation with bert. In International Confer- (QA)taskindatacreation.
enceonLearningRepresentations. Forthesummarizationtask,wesampled1,000
examples from the CNN/DM dataset (See et al.,
YuanZhang,JasonBaldridge,andLuhengHe.2019b.
PAWS: Paraphrase Adversaries from Word Scram- 2017). For QA, we use 2,655 examples from the
bling. InProc.ofNAACL. NaturalQuestions(Kwiatkowskietal.,2019)from
thesetupofLiuetal.(2024)tomixthegolddocu-
LianminZheng,Wei-LinChiang,YingSheng,Siyuan
mentwithirrelevantdocuments. Tokeepourfocus
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. moreonLLMhallucinationsratherthanbeingdis-
Judging llm-as-a-judge with mt-bench and chatbot tractedbyassessingLLMs’long-contextutilization
arena. AdvancesinNeuralInformationProcessing
ability,welimitedcontexttothreedocumentsper
Systems,36.
questionwherethegolddocumentcontainingthe
answer was placed in the middle, surrounded by
twoirrelevantdocuments.
WepromptLLaMA-2-7B-Chat(Touvronetal.,
2023)togeneratecorrectresponsesbygreedyde-
coding for both tasks to ensure that both halluci-
natedandnon-hallucinatedexamplesderivefrom
the same source distribution. The max length of
generation is set to 256 tokens, or until the EOS
tokenisgenerated.
After the annotation was collected, we extract
hallucinated and non-hallucinated spans, as well
asthecorrespondingattentionmaplookbackratio,
from the LLaMA-2-7B-Chat model, to train the
LookbackLensclassifiers.
In the predefined span setting, three types of
spansareconsideredasnon-hallucinatedspans: 1)
thetextsegmentbeforethefirsthallucinatedspan
in the response 2) the text segment after the last
hallucinatedspanintheresponse3)theresponse
annotated as non-hallucinated. All the annotated
hallucinatedspansareusedasnegativedatatotrain
theLookbackLens.
Intheslidingwindowsetting,weconsiderallthe
possiblefixedsizedchunkwithsize=8. Ifachunk
is overlapping with any of the annotated halluci-
natedspans,thenitisconsideredashallucinated,
otherwiseitisnon-hallucinated.
Whynotuseexistingdata? Initially,weconsid-
ered using the HaluEval dataset (Li et al., 2023),
whichwascreatedbypromptingGPT-3.5(OpenAI,
2022)togenerate“hallucinatedexamples”against
human-annotatednon-hallucinatedresponses, on
summarization,QA,anddialoguetasks. However,
we have concerns that their method introduces a
biasbycreatingfundamentallydifferentdatadistri-
12butionsbetweenhallucinatedandnon-hallucinated
examples. Thisdiscrepancycouldpotentiallylead
theclassifiertolearntodistinguishthesourcesof
responsesratherthanaccuratelydetectinghalluci-
nations.
Additionally,wearguethattheLLM’sattention
weightwillbemoremeaningfulifthetextisgen-
eratedbythesameLLMitself,notfromexternal
sourcesandteacherforcingtoobtaintheattention
weights. Toensureanunbiasedandcontrolledeval-
uationenvironment,wegeneratedourowndataset
onsummarizationandQAtasks.
B EvaluationPromptforGPT-4o
We show the templates used to prompt GPT-4o
(gpt-4o-2024-05-13) in annotating the truthful-
nessofaresponseandthespan-levelhallucination
segmentpredictioninTable9andTable10,respec-
tivelyforCNN/DMandNaturalQuestions.
Thispromptisusedfor1)collectingthedatato
traintheLookbackLensinTable1,and2)evaluat-
ingtheXSumsummarizationtaskinSections3,4,
and5.
C PilotStudyofHumanEvaluationon
GPT-4oEvaluation
To assess the quality of GPT-4o’s evaluation in
AppendixB,weconductedapilotstudyusing70
examplesfromXSumdata,involvingtheauthors Figure4: Screenshotofhumanannotationinterface.
andcolleagueswhoparticipatedvoluntarilyaseval-
uators. Evaluators are all native English speak-
GPT-4modelgpt-4-0613andourproposedMT-
ers. The human evaluators are provided with the
Bench(hallucination)withthelatestGPT-4omodel
document,groundtruthsummary,summaryfrom
(gpt-4o-2024-05-13).
LLaMA-2-7B-Chat,andthejudgmentfromGPT-
4o. And the task is to give a binary judgment on
E ModelDetails
whetherthejudgmentfromGPT-4oiscorrect. The
screenshot of our interface is shown in Figure 4. State-of-the-artNLIModel Wegivefurtherde-
Everyevaluatorisinformedandconsentsthatthe tail on the pretrained SoTA NLI model 5 used as
annotationresultwillonlybeusedtoreportstatis- our topline hallucination detector. Specifically,
ticsinthispaper. Ourpilotstudyshowsthat68out the model is based on DeBERTa-V3-base (He
of70(97%)GPT-4ooutputsarecorrect,justifying et al., 2021) and further finetuned on a range
theuseofGPT-4oasanautomaticevaluator. of NLI and summarization datasets with exam-
plesannotatedwithfactualconsistency,including
D EvaluationPromptforMT-Bench
FEVER (Thorne et al., 2018), Vitamin C (Schus-
We show the evaluation prompt for MT-Bench ter et al., 2021) and PAWS (Zhang et al., 2019b).
(hallucination) in Table 11. We follow stan- Roughly731kdataexamplescanbecollectedfrom
dardpracticeforMT-Bench(original)evaluation4 the training set of the above three datasets. The
and show evaluation prompts in Table 12. We model is reported to have superior performance
evaluate MT-bench (original) with their default whenevaluatedonTRUE(Honovichetal.,2022)
4https://github.com/lm-sys/FastChat/tree/main/ 5https://huggingface.co/vectara/hallucination
fastchat/llm_judge. _evaluation_model
13You will be provided with a document and a proposed summary. Your task is to determine if the
proposed summary can be directly inferred from the document. If the summary contains any information
not found in the document, it is considered false. Even if the summary is different from a ground
truth summary, it might still be true, as long as it doesn’t contain false information.
For each proposed summary, explain why it is true or false based on the information from the
document. Focus only on the original document’s content, disregarding any external context.
After your explanation, give your final conclusion as Conclusion: True if the proposed summary is
completely accurate based on the document, or Conclusion: False if it contains any incorrect or
unsupported information. If your conclusion is ’False’, identify the exact phrases or name entities
from the summary that is incorrect by stating Problematic Spans: [the inaccurate text spans from
the summary, in Python list of strings format].
#Document#: {document}
#Ground Truth Summary#: {ground_truth_summary}
#Proposed Summary#: {response}
Write your explanation first, and then give your final conclusion as Conclusion: True if
the proposed summary is completely accurate based on the document, or Conclusion: False if it
contains any incorrect or unsupported information. Add Problematic Spans: [the exact inaccurate
text spans from the summary, in a list of strings] if your conclusion is ’False’.
Table9: PrompttemplateforGPT-4oinannotatingthetruthfulnessandpredictingspan-levelhallucinationson
summarizationtasks. UsedforCNN/DMandXSum.
You will be provided with a document and a proposed answer to a question. Your task is to determine
if the proposed answer can be directly inferred from the document. If the answer contains any
information not found in the document, it is considered false. Even if the answer is different from
a ground truth answer, it might still be true, as long as it doesn’t contain false information.
For each proposed answer, explain why it is true or false based on the information from the document.
Focus only on the original document’s content, disregarding any external context.
After your explanation, give your final conclusion as Conclusion: True if the proposed answer is
completely accurate based on the document, or Conclusion: False if it contains any incorrect or
unsupported information. If your conclusion is ’False’, identify the exact phrases or name entities
from the answer that is incorrect by stating Problematic Spans: [the inaccurate text spans from the
answer, in Python list of strings format].
#Document#: {document}
#Ground Truth Answers (a list of valid answers)#: {ground_truth_answers}
#Proposed Answer#: {response}
Write your explanation first, and then give your final conclusion as Conclusion: True if
the proposed answer is completely accurate based on the document, or Conclusion: False if it
contains any incorrect or unsupported information. Add Problematic Spans: [the exact inaccurate
text spans from the answer, in a list of strings] if your conclusion is ’False’.
Table10: PrompttemplateforGPT-4oinannotatingthetruthfulnessandpredictingspan-levelhallucinationson
question-answeringtasks. UsedforNaturalQuestions.
SummaC Benchmark (Laban et al., 2022) and that is instruction fine-tuned. HuggingFace
AnyScaleRankingTestforHallucinations6. ID:meta-llama/Llama-2-13b-chat-hf.
OtherModelDetailsandLicense • hallucination_evaluation_model:
Based on microsoft/deberta-v3-base
• Llama-2-7B-Chat: A 7B parameter model
whichhas86Mparameters. HuggingFaceID:
that is instruction fine-tuned. HuggingFace
vectara/hallucination_evaluation_model.
ID:meta-llama/Llama-2-7b-chat-hf.
• DeBERTa-V3-Base: a 86M parameters en-
• Llama-2-13B-Chat: A13Bparametermodel
coder based model. HuggingFace ID:
6https://www.anyscale.com/blog/llama-2-is-abo microsoft/deberta-v3-base.
ut-as-factually-accurate-as-gpt-4-for-summaries
-and-is-30x-cheaper Theabovemodelshavethefollowinglicenses.
14Please act as an impartial judge and evaluate the faithfulness and consistency of the response
provided by an AI assistant to the user question displayed below. Your evaluation should consider
whether the assistant’s answer to the second user question is faithful and consistent to the chat
history. If the answer contains any misinformation not found or not supported by the chat history,
it is considered a hallucination. You evaluation should focus on the assistant’s answer to the
second user question. Begin your evaluation by providing a short explanation. Be as objective as
possible. After providing your explanation, you must rate the response on a scale of 1 to 10 by
strictly following this format: “[[rating]]", for example: “Rating: [[5]]".
<|The Start of Assistant A’s Conversation with User|>
### User:
{question_1}
### Assistant A:
{answer_1}
### User:
{question_2}
### Assistant A:
{answer_2}
<|The End of Assistant A’s Conversation with User|>
Table11: GPT-4oevaluationpromptforMT-bench(hallucination).
• Llama-2-7B-ChatisundertheLlama2Com- to train the classifiers of Lookback Lens on CPU
munityLicenseAgreement. machine. Weuseallthedefaulthyperparameters,
such as L2 penalty, etc, but we change the
• Llama-2-13B-Chat is under the Llama 2
max_iterto1000toensureitisconverged.
CommunityLicenseAgreement.
Heads Mapping Details We use Scikit-Learn
• vectara/hallucination_evaluation_model sklearn.linear_model.LinearRegression9 in
isundertheApache2.0License. Section 4, to fit a linear transformation from
LLaMA-2-13B-Chat’sattentionheadstoLLaMA-
• DeBERTa-V3-BaseisunderMITLicense.
2-7B-Chat’s attention heads. It is computed to
Inference Details We run all the models on solvetheclose-formOrdinaryLeastSquaresopti-
NVIDIAA6000(48GB)andV100(32GB)GPUs. mization problem, without gradient descent. We
Wedonottrainthemodel,butonlyruntheinfer- use all the default hyperparameters and run it on
encepart. Eachoftheexamplestakesaround20- ourCPUmachine.
30secondsfor7Bmodel,40-60secondsfor13B
F Visualization
model to generate responses using our Lookback
LensGuidedDecoding. PleasecheckAppendixG Wevisualizethelookbackratioofthetop-10most
to estimate the total running time on each of the positive/negativeheadswhenLLaMA-2-7B-Chat
datasets,asitdependsonnumberofexamples. decodestheanswerforanNQexample. Thetop-10
All the inferences are run with either greedy mostpositive/negativeheadsareselectedwiththe
decoding or sampling using temperature 0.9 and most positive/negative coefficients from the clas-
top-psamplingwithp = 0.95. Theimplementation sifier. The green rectangle frames the part that
isbasedonHuggingfaceTransformerspackages.7
containsthehallucinations,i.e. andinGermanyin
Allthescoresinthepaperarefromasinglerundue the14thcentury. Wecanseethatduringthegener-
tothelimitedcomputationforthelargemodels. ationofthehallucinatedspan,thepositiveheads,
especially for the top-1 heads (topmost), show a
ClassifierTrainingDetails WeuseScikit-Learn
lowerlookbackratio(inblue),whilethenegative
sklearn.linear_model.LogisticRegression8
headsshowaslightlyhigherlookbackratio(inred).
7https://github.com/huggingface/transformers However,thebehaviorofLookbackLensstillneeds
8https://scikit-learn.org/stable/modules/gene
rated/sklearn.linear_model.LogisticRegression.ht 9https://scikit-learn.org/stable/modules/gene
ml rated/sklearn.linear_model.LinearRegression.html
15Please act as an impartial judge and evaluate the quality of the response provided by an AI
assistant to the user question displayed below. Your evaluation should consider factors such as
the helpfulness, relevance, accuracy, depth, creativity, and level of detail of the response. You
evaluation should focus on the assistant’s answer to the second user question. Begin your evaluation
by providing a short explanation. Be as objective as possible. After providing your explanation,
you must rate the response on a scale of 1 to 10 by strictly following this format: "[[rating]]",
for example: "Rating: [[5]]".
<|The Start of Assistant A’s Conversation with User|>
### User:
{question_1}
### Assistant A:
{answer_1}
### User:
{question_2}
### Assistant A:
{answer_2}
<|The End of Assistant A’s Conversation with User|>
Please act as an impartial judge and evaluate the quality of the response provided by an AI assistant
to the user question. Your evaluation should consider correctness and helpfulness. You will be
given a reference answer and the assistant’s answer. You evaluation should focus on the assistant’s
answer to the second question. Begin your evaluation by comparing the assistant’s answer with the
reference answer. Identify and correct any mistakes. Be as objective as possible. After providing
your explanation, you must rate the response on a scale of 1 to 10 by strictly following this format:
"[[rating]]", for example: "Rating: [[5]]".
<|The Start of Reference Answer|>
### User:
{question_1}
### Reference answer:
{ref_answer_1}
### User:
{question_2}
### Reference answer:
{ref_answer_2}
<|The End of Reference Answer|>
<|The Start of Assistant A’s Conversation with User|>
### User:
{question_1}
### Assistant A:
{answer_1}
### User:
{question_2}
### Assistant A:
{answer_2}
<|The End of Assistant A’s Conversation with User|>
Table12: GPT-4evaluationpromptforgeneralquestions(top)andmathquestions(bottom)onMT-bench(original).
16Figure5: Top-10positive/negativeheadsrankedfrom
toptothebottombythemagnitudeoftheircoefficients
intheLookbackLensclassifier.
tobedeterminedbythecollectivebehaviorofall
headsandtheweightandbiasoftheclassifier.
G DatasetDetails
Thedatasetsweusedinthepaperhavethefollow-
ingdetails:
• CNN/DM:sampled1000examplesfromthe
testingset. Apache-2.0license. https://hu
ggingface.co/datasets/abisee/cnn_dai
lymail
• NaturalQuestions: Apache-2.0license. Test-
ingset: 2655examplesfromhttps://gith
ub.com/nelson-liu/lost-in-the-middl
e. NQ-train: sampled 2499 examples from
its training set, using the positive document
providedbyhttps://github.com/faceboo
kresearch/DPR
• XSum: 1000examplessampledfromthetest-
ingset. MITlicense. https://github.com
/EdinburghNLP/XSum
• MT-bench: 80examples. Apache-2.0license.
https://github.com/lm-sys/FastChat/
tree/main/fastchat/llm_judge
17