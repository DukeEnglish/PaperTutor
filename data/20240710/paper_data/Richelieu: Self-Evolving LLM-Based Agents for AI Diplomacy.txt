Richelieu: Self-Evolving LLM-Based Agents for AI
Diplomacy
ZhenyuGuan XiangyuKong(cid:66)
PekingUniversity BeijingInformationScienceandTechnologyUniversity
gzyxxn@stu.pku.edu.cn xykong@bistu.edu.cn
FangweiZhong(cid:66) YizhouWang
PekingUniversity PekingUniversity
zfw@pku.edu.cn yizhou.wang@pku.edu.cn
Abstract
Diplomacy is one of the most sophisticated activities in human society. The
complexinteractionsamongmultipleparties/agentsinvolvevariousabilitieslike
socialreasoning,negotiationarts,andlong-termstrategyplanning. PreviousAI
agentssurelyhaveprovedtheircapabilityofhandlingmulti-stepgamesandlarger
actionspacesontasksinvolvingmultipleagents. However,diplomacyinvolvesa
staggeringmagnitudeofdecisionspaces,especiallyconsideringthenegotiation
stagerequired. Recently,LLMagentshaveshowntheirpotentialforextendingthe
boundaryofpreviousagentsonacoupleofapplications,however,itisstillnot
enoughtohandleaverylongplanningperiodinacomplexmulti-agentenvironment.
Empoweredwithcutting-edgeLLMtechnology,wemakethefirststabtoexplore
AI’supperboundtowardsahuman-likeagentforsuchahighlycomprehensive
multi-agentmissionbycombiningthreecoreandessentialcapabilitiesforstronger
LLM-basedsocietalagents: 1)strategicplannerwithmemoryandreflection;2)
goal-orientednegotiatewithsocialreasoning;3)augmentingmemorybyself-play
gamestoself-evolvingwithoutanyhumanintheloop.
1 Introduction
Diplomacy,asacornerstoneofinternationalrelations,isanintricateandmultifacetedactivitythat
lies at the heart of human society’s most complex interactions. It encompasses a wide array of
skillsandstrategies,includingsocialreasoning,negotiation,andlong-termplanning,tonavigatethe
intricatewebofrelationshipsandalliancesbetweenmultipleparties. Mirroringthiscomplexity,the
boardgameDiplomacy[59]involvessevenplayerstocontrolEuropeanpowers,presentingacomplex
strategicchallengethatrequiresbothsophisticatednegotiationandstrategicplanningtotriumph.
TheAIcommunityhasshownanincreasinginterestinthedeploymentofAIagentstomastersuch
games[45,26,29,15,36,28]. Therecentbreakthrough[6]hasturnedintopressdiplomacy,which
allowscommunicationbetweenplayers. However,thepreviousmethods[6]heavilyrelyondomain-
specifichumandata,leadingtoitspoorgeneralizationtootherscenarios/applications. Thequestion
thenarises: CanwebuildanAIagentthatexcelsintheartofdiplomacywithoutrelyingon
domain-specifichumandata?
Recently,agentsbasedontheLargeLanguageModel(LLM)haveemergedasapromisingdevel-
opmentforAIagents. Thepreviousapplicationsonpersonalassistants[32],robotics[10,64],and
videogames[48]haveshownthesurprisingabilityofLLM-basedagentsincommunicationand
planning, benefiting from the emergent ability of common sense reasoning, in-context/ few-shot
Preprint.Underreview.
4202
luJ
9
]IA.sc[
1v31860.7042:viXraExperience
Self-Play
Acquisition
Memory
Evolution
Update
2019 DipNet 2022 Cicero 2024 Richelieu
RL RL LLMs-based-Agent
No Press Diplomacy Reasoning with Negotiation Evolution in Self-Play
Human Data Human Data No Human Data
Figure1: AnewparadigmforbuildingAIDiplomacyagent.
learning,andsophisticatednaturallanguageprocessingonLLMs. However,diplomacypresentsa
uniquesetofchallenges. Itnotonlyrequiresplanninglong-horizonstrategic[40]andcommuni-
catingwithnaturallanguage, butalsoreasoningandadoptingthecomplexsocialdynamicswith
partialobservations,includinggainingtrustandreputation,buildingrapport,detectingdeception,and
assessingthereliabilityofotherplayers.
Inthiswork,weaimtomakethefirstattempttoexploreLLMs’potentialtodevelopahuman-likeAI
diplomacyagent. WenametheagentRichelieuinmemorizingapivotalfigureinEuropeanhistory
whohadenduringimpactsonFrenchpolitics,foreignaffairs,andstatebuilding. Toachievethisgoal,
wehaveidentifiedthreecoreandessentialcapabilitiesthatarecrucialforbuildinganLLM-based
societal agent: 1) Social reasoning This is the basic function for a social agent to interact with
others,particularlyforadaptingtothedynamicchangesinthenation’sintentionsandrelationships.
2)Balancelong-andshort-termplanningDiplomacyoftenrequiresadelicatebalancebetween
short-termtacticsandlong-termstrategies.AneffectiveAIagentmustbeabletorecognizeandweigh
theimmediateconsequencesofitsactionsagainstthepotentiallong-termoutcomes. 3)Powerful
memoryArobustmemorysystemisacriticalcomponentoflearningandimprovement. TheAI
agentmustbeabletorecallandintegrateinformationfrompastnegotiationsandactionstoinform
itscurrentandfuturedecision-makingprocesses. Thisendowstheagentwiththeabilitytoevolve.
4)ProfoundreflectionAnAIagentcapableofprofoundreflectioncananalyzeitsowndecisions,
learnfromitsmemoryexperience,andadaptitsstrategiesaccordingly. Byintegratingthesethree
capabilities,theagentcanoperateatthehighestlevelofdiplomaticsophistication,outperformingthe
state-of-the-artAIdiplomats[6].
Ourcontributionscanbesummarizedinthree-fold: 1)Weintroducedanewparadigmforbuilding
AIdiplomacyagents,comparedtopreviouswork(Fig.1). Theagentcanself-evolvebygenerating
experienceviaself-playgames,withoutanytask-specifichumandata. 2)Wedemonstratethesuperior
performanceofouragentplayingagainsttheSOTAmethod,e.g.,Cicero[6],thatreliesonalarge-
scalehumandemonstrationfortraining. 3)Wefurtheranalyzetheeffectivenessofeachmoduleinour
agentandthegeneralizationofouragentinadoptingdifferentLLMs,suchasGPT4.0andLlamma3.
2 Relatedwork
AIDiplomacy. ThegameinvolvessevenplayerscontrollingdifferentpowersinEurope. Ineach
turn,playerscannegotiateforcooperationbeforemakingmovestotakeasmuchsupplycentersas
theycan. Apparently, thischallengingstrategytaskrequiresbothcomplexnegotiationskillsand
superior planning capability for player agents to achieve final victory. So far, most AI research
on this task remain focused on the planning strategies (a.k.a. No-Press Diplomacy where no
communicationchannelsareallowed). Thesettingremainschallengingconsideringitsenormous
actionspaceof1021to1064perturn(comparedwithChess,whichhasmuchfewerthan100actions
perturn). Nowonderexistingeffortsrelyonhumandatatoplaythegame. Amongthemethods,
onetypicalresearchisDipNet[39]whichusessupervisedandreinforcementlearning. Basedon
DipNet,BRPI[3],SearchBot[18],DORA[5],andKL-Regularizedsearch(Diplodocus)[24]were
conducted. Untilveryrecently,researchhasalsoemergedforthefull-settingofDiplomacy,orPress
Diplomacy where players are allowed to communicate with each before making their moves in
eachturn. Suchstudies[13][6][25][29]mainlybenefitfromtherecentthrivinglanguagemodels.
2Specifically,notableadvancementsincludepolicyiterationmethodsfromDeepMindandFacebook
AI Research’s equilibrium search agent [25]. However, Deepmind propose to learn negotiation
agentsbasedonpredefinedcontracts/protocols[29]. AndMetaAI’swork,insteadofoneunified
architecture,Cicero[6]integratesalanguagemodelfornegotiationandanRLmodelforplanning
respectively. Suchseparatelytrainedmodelsmakeitinconvenientforagents’continualevolution.
What’smore,likeno-pressmethods,theseapproachesheavilyrelyonhumanplayerdataforagent
training. Unliketheseapproaches,thispaperdelvesintosolvingthenegotiationandplanninginone
singleself-evolvingLLM-basedagentmodel,withoutanypre-collectedhumanexperttrainingdata.
LLM-basedAgents. Withtheemergenceandgrowthoflargelanguagemodels(LLM),thereisa
growingtrendinutilizingLLMsasfundamentalcontrollersforautonomousagents[52]. Onewide
application genre is LLM-based answering engines, which merely cover the negotiation aspects
ofDiplomacy. SuchsystemsincludeHuggingGPT[44],GPT4Tools[63]andToT[65],etc. They
leverageLLMstomanageAlmodels,usetools,implementpolicyiteration,andenhanceproblem-
solvingacrossvarioustasks.RelatedworkincludingAutoGPT,AgentGPT,BabyAGl[47],Toolformer
[43],andVisualChatGPTaimtoimproveLLMcapabilitiesintaskautomationandtoolusage. And
Reflexion,aframeworkthatimprovesLLMsthroughlinguisticfeedbackandepisodicmemory[68],
facilitatingbetterdecision-makingacrossdiversetasksisproposed. Besides[55][50][56][76][62]
applyLLMagentstothecomplexplanningtasksinthewell-knownopen-worldgameMinecraft[16].
UnliketheseLLM-basedagentswhichonlyfocusonthenegotiation/planningaspect,theproposed
approachinvolvesmultipleself-evolvingschemestohandlebothofthemsimultaneously.
3 ProblemStatement
The Diplomacy game [59, 8] is set in pre-World War I Europe and involves each player (agent)
representingoneofthesevenGreatPowersofEurope, suchasGermany, France, England, Italy,
Austria-Hungary, Russia, and Turkey. Each player has a set of military units, including armies
andfleets,whichtheycanmoveandusetocaptureothersupplycenters. Theultimategoalforthe
agentistocontrolamajorityofthetotalsupplycentersontheboardbytheendofthegame’sFall
phase. It’simportanttonotethatitisnotwonbyeliminatingotherplayersortheirunits;itiswon
bycontrollingtherequisitenumberofsupplycenters. Thisofteninvolvesformingandbreaking
alliances,negotiating,andsometimesbetrayingotherplayerstoachieveone’sowngoals.
In each turn,[58, 53] the agent i gets the current state s ∈ S, the actions of other players from
t
thepreviousturn⃗a−i ,andthemessagesm⃗−i,i fromotherplayersduringthisturn’snegotiations.
t−1 t
Thestates fortheenvironmentincludestheownershipofeachterritoryonthemapbyaparticular
t
countryandwherethearmiesofeachcountryarelocated. Basedonthisinformation,theagentneeds
toengageinnegotiationswithotherplayers,sendingmessagesm⃗i,−itochatwithotherplayers,and
t
thentaketheactionsai inthisturn. Thepossibleactionsanagentcantakeai ∈Aarecommandsto
t t
thearmies,suchasmovingintoanadjacentterritory,supportinganotherunit,orholdingaposition.
Actions can also include diplomatic moves, such as proposing or withdrawing from an alliance,
althoughthesearelessformalizedinthegamemechanics.[39,22]
4 Self-EvolvingLLM-basedDiplomat
Wehaveconstructedacomprehensiveframeworkwithmodulesformemorymanagement,social
reasoning,strategicplanning,negotiation,decision-making,memoryupdate,andself-evolvingto
fully leverage the capabilities of LLMs. Richelieu starts by setting up with map details, game
rules,domainknowledge,andthelong-termgoal.[74,58,53]Ateachturn,theagentwillruninthe
followingsteps: 1)SocialReasoning: Firstofall,theagentundergoesacomprehensiveanalysis
of the game state s to build the social belief, including the intention of other players and their
t
relationship ϕ⃗ ∈ Φn.[71, 19] 2) Planner with Reflection: Then, the agent proposes sub-goals
t
χi ∈X thatisstrategicallyalignedwiththelong-termgoalsΥ,withthesocialbeliefandrefiningthe
t
proposedgoalwithexperienceη⃗ ∈ Hm abstractfromthememoryM viaself-reflection.[51,57]
t
3) Negotiator: To achieve the sub-goals, the negotiator will start a dialogue session with some
players,andevaluatetheirtruenessψ⃗−ibyreferringtotheirwordsm⃗−i,i,thecurrentstates ,their
t t t
sincerity⃗γ−iandtheexperienceξ⃗ .[1,7]4)Actor: Afternegotiation,theactordecidesitscourseof
t t
actionai,basedonthesub-goalχi andupdatedsocialstates ,markingtheendofthatturn. 5)
t t t+1
3Planner with Reflection Social
Memory Reasoning
Experience
Abstract
Social Relationship
Summarized Sub-goals Belief
Intention
Negotiator
States u___ u___
Actions
Sub-goals INIT
Messages
Updated MAP
Social Belief
RULE
Actor
Domian
v.s. Knowledge
SELF-PLAY
ENVIRONMENT
Figure2: TheframeworkoftheproposedLLM-based-agent,Richelieu. Theagentcanexplicitly
reasonsocialbeliefs,proposesub-goalswithreflection,negotiatewithothers,andtakeactionsto
masterdiplomacy. Itaugmentsmemoriesbyself-playgamesforself-evolvingwithoutanyhuman
annotation.
MemoryManagement: Thestateofthecurrentturns ,thecontentofnegotiationsm⃗ ,theactions
t t
takenbyallplayersa⃗ ∈ An,andthesub-goalssetforthχi areallloggedwithinthememoryas
t t
µ∈M. Thisloggeddataservesasahistoricalexperience,guidingRichelieu’ssubsequentactionsin
futureturns[20,73]. 6)Self-evolution: Theagent’sevolutionishighlydependentonthediversityof
experiencesstoredinitsmemory. Asthisdiversitygrows,sodoestheagent’scapability. Without
humandemonstrations,weemploymulti-agentself-playgames,i.e.,ouragentsrespectivelycontrol
allthecountriestosimulateandacquirediverseexperiencesforself-evolving. Notably,theagentcan
furtherevolveduringtestingtoadapttodifferentplayers.
4.1 SocialReasoning
Therearenopermanentenemies,nopermanentallies. Therelationshipamongcountriesisdynami-
callychangingupontheevolvingglobalstate. However,itisdifficulttodeterminetheappropriate
alliesandenemieswithpartialobservation. Forexample,thereisuncertaintyabouttheintentions
of potential allies, which could lead to betrayal at pivotal moments. Consequently, we need to
identifytheintentionandrelationshipofthecurrentstatebysocialreasoningtoshapethesocial
belief[71,19].
1)ModelingRelationship: Beforesettingsub-goals, Richelieuevaluatesitsrelationswithother
players,identifyingenemiessuchasaggressivenations,vulnerableneighborsforexpansion,and
thosewithlong-termpotentialthreats.Italsoseeksoutpotentialalliestocounterthesethreats.[46,72]
Simultaneously,Richelieualsotriestoidentifypotentialalliesthatcouldbeinstrumentalincountering
theseadversaries. Byisolatingtheanalysisofinter-playerrelationshipsasadiscreteelement,Riche-
lieustrategicallyexploitstheactionsofotherplayersinsubsequentstagesofthegametoreachits
goals. 2)InferringIntention: Thesocialbeliefisalsousedbytheplanner,ensuringthatRichelieu’s
sub-goals are formulated with a comprehensive consideration of the behaviors and intentions of
otherintelligentagentswithinthegame.[14,21]Richelieu’ssub-goalswillparticularlyemphasizeon
thosewhoareidentifiedaspotentialadversariesorallies,fosteringmoreeffectivecollaborationwith
potentialalliesandparticipationinstrategicoppositionagainstadversaries. Furthermore,theinsights
gleanedfromthisanalysisareinstrumentalinthesubsequentnegotiationphases. Theyareemployed
to assess the authenticity of the statements made by other players, as well as to aid Richelieu in
reachingcooperativeagreements.
4.2 StrategicPlannerwithReflection
Thestrategicplannerspecifiesthesub-goals,whichservesasanintermediarybetweenimmediate
actionsandtheoverarchinggoalofsecuringvictoryinthegame. Thatisbecauseweobservethat
4words for cooperation
Receive F F
Enemy?
Words no words
T F
Deception?
T T words for fake
cooperation
other's T
words
F words to change
Enemy?
conftontation other’s intention
lies? true intention
cooperation Enemy? T change F
relationship
MEMORY F T
Figure 3: The social reasoning flow for negotiation. With the received words and memory, the
agentwillreasonbyansweringthefollowingquestions: “Istheopponentlying?",“Whatisthetrue
intentionoftheopponent?",“istheopponentenemy?",“Isitnecessarytodeceivetheopponent?",
and“Isitnecessarytochangetherelationshipwiththeopponent?", andthengeneratethewords
accordinglyfornegotiation.
LLMsareoftencharacterizedbytheirpropensitytoprioritizeshort-termgainsindecision-making
processes, with a notable deficiency in incorporating the future into their strategic calculations.
[41,70]Forexample,itiscommonforanon-neighboringcountrytobecometoopowerful. Formally,
χ⃗ ← SR(s ,ϕ⃗,Υ) where χ⃗ = (χi,χ1,...,χn) represents the proposed sub-goals and other
t t t t t t t
players’intentionthatweinferred,ϕ⃗ ∈Φnrepresentstheinferredrelationshiponthesocialbelief.
t
Thesegoalsmayencompassarangeoftacticalconsiderations,suchasthecontainmentofaformidable
rival’sadvancementorthestrategicexpansioninaparticulardirectiontoconsolidatepower.
ReflectionwithMemory. Wefurtherdevelopareflectionmechanismtoenhancetherationalityand
effectivenessofouragent’ssub-goalsinachievinglong-termgoals.[33]Thisreflectionmechanism
reliesonthepastexperiencestocritiqueandenhanceproposedsub-goals. Weemployasimilarity-
based function to find relevant historical experiences that match the current game state from its
memory. This function considers two factors: goal similarity and state similarity, to select the
mostcomparableexperiences. Theprocesscanbewrittenas: η⃗ ←h(s ,χi,M),whereη⃗ ∈Hm.
t t t t
In practice, considering the limited context windows of LLM, we retrieve the most analogous
experiences from the memory based on these metrics. Experiences with high evaluative scores
reinforcesuccessfulstrategiesandsupportthecontinuityofexistingsub-goals. Ontheotherhand,
lowerscoresindicateareasthatneedimprovementandpromptthenecessaryadjustments. Asour
agent,Richelieu,undergoesmoretrainingsessions,itsreflectionabilitiesimprove. Thegrowingpool
ofhistoricalexperiencesconsistentlyenhancesitsperformance.
4.3 NegotiatorandActor
Bychattingwithotherplayers,thegoalofthenegotiationistoupdatethesocialbeliefaccording
tothereceivedwordsandreachthesub-goalbymanipulatingother’sintentions,suchassecuring
cooperativeagreementswithothernations,terminatingongoingconflictswithaspecificcountry,or
deterringtheformationofalliancesdirectedagainstitsinterests.[37,67]However,itisdifficultto
reachaconsensus,astheinterestsandstrategiesofthevariousnationsoftenconflict,andtrustbetween
playerscanbescarce,makingitchallengingtoestablishandmaintaincooperativeagreements. Inthis
case,wearguethatthenegotiatorshouldidentifythetrueintentionsandrelationshipoftheopponent
beforegeneratingthewordsforthenegotiation.
TofullyutilizethepowerofLLMs,weconstructasocialreasoningflowfornegotiation,asshown
inFigure3. Duringthenegotiationprocess, weguideRichelieutoconsidertheveracityofwhat
otherplayerssaidandtheirtrueintentions,andinconjunctionwithourestablishedsub-goalsand
5analysisofourrelationshipswithotherplayers,tonegotiateandformallianceswithpotentialallies
andattempttodeceiveenemies.[60,35]
Tocounteractthechallengeofnon-bindingagreementsandpotentialdeception,weincorporatea
discrete module dedicated to the assessment of the veracity of statements made by other players
duringnegotiations. Todeterminethetruthinessofotherplayers’statementsψj,threemainfactors
t
areconsidered. Themostimportantistheconsistencybetweentheplayer’ssub-goalsχj thatour
t
agentinferredbeforeandtheintentionsconveyedthroughhisstatementsmj,i. Toaidinthejudgment,
t
ouragentalsogoesthroughthememorytoretrievetheconsistentexperiencesξ⃗. Additionally,the
t
player’soverallhonestyscoreγ istakenintoaccount. Hence,wegetthetruthinessoftheopponentj:
i
ψj ←g(s ,χj,mj,i,ϕ⃗,γ ,ξ⃗),whereξ⃗ =w(s ,mj,i,M). Withsuchareasoningflow,ouragent
t t t t t j t t t t
canadeptlynavigatediplomaticdiscourse. Afterthenegotiation,theactorwillgettheupdatedsocial
beliefsandchooseaspecificactionforthearmy.
4.4 MemoryManagementandSelf-Evolution
Thismemoryisthefoundationoftheframeworkthataccumulatesthehistoricalexperienceofthe
agentandsummarizesthemforothermodules.[17,30,66,23]Itsupportsothermodules,suchas
plannerandnegotiator,toprovidelong-tailexperiences.
RawExperienceManagement. Specifically, thememorymoduleistaskedwiththeacquisition
andarchivalofhistoricaldata,encompassingtheobservedgamestates ateachturn,itssub-goals
t
χi, the messages during the negotiation m⃗ , and the actions of all the players a⃗. Subsequently,
t t t
therawexperienceissummarizedinashortercontentwithanevaluationλ ∈ Λoftheproposed
t
sub-goalsandanassessmentofthecredibilityofotherplayersγ ∈ Γ. λ servestoreflectupon
j t
theagent’ssub-goals. Itevaluateswhethersub-goalsarereasonablebasedonthesubsequentstate
andlong-termgoalsΥ. Asthegameprogresses,itiscontinuouslyupdatedinresponsetochanges
in the state λ ← f(χi,Υ,⃗s), where ⃗s = (s ,s ,...s ). The formula represents the update
t t t t+1 T
of the evaluation λ for the sub-goal in turn t by the memory in turn T. The updates will cease
t
whenthereisafundamentalchangeinthesub-goalcomparedtothegoalatturnt. Thisprevents
subsequentdecisionsfromimpactingtheassessmentofthecurrentdecision-making. Weemploy
γ ∈Γtoevaluatethecredibilityofplayerj andutilizeτj ∈{0,1}todenotethetruthfulness,i.e.,
j t
whetherthestatementsmadebytheplayerj duringthenegotiationprocessattimetaretruthful.
Thetruthinessofplayerj’sstatementsisupdatedaccordingtothememoryfromthepreviousturns,
τj ← T(s ,s ,aj,mj,i). The credibility of player j γ will be updated based on player j’s
t t t+1 t t j
statementsτj, writtenasγ ← p(γ ,τj ). Players’credibility⃗γ isashort-termmemorythatis
t j j t−1
applicableonlytothecurrentturn. Otherdatacollectedorgeneratedconstituteslong-termmemory.
Thesedatawillbecombinedtoformahistoryµ∈M,andthenisincorporatedintomemory.
Acquisition Experience via Self-Play Games. Self-play allows the agent to accumulate more
experiencesforself-evolution.[33,69]Aftertraining,whenRichelieuisfacedwithacertainstate,it
candrawonalargerpoolofsimilarhistoricalexperiences. DiverseevaluationsenableRichelieuto
reflectmorecomprehensivelyonthestrategiesitcurrentlydevises,leadingtoastrongeroptimization
ofdecisionmaking. Asself-playcontinues,theacquisitionofnewandbetterhistoricalexperiences
byRichelieuwilldiminish. ThismeansthatRichelieu’scapabilitieswillnotimproveindefinitely. At
thesametime,asthememorygrows,selectingappropriatehistoricalexperiencesbecomesanew
challenge. Thechosenmexperienceη⃗ maybealmostidentical,whichcouldactuallyreducethe
t
amountofusefulinformationavailabletoRichelieu. AsshowninFigure5,Richelieu’sperformance
against Cicero [6] becomes better with increasing training iterations. With the accumulation of
experiences,Richelieu’swinrateexhibitedasteadyincreasewithaccumulatedtrainingiterations,
ultimatelyplateauingatastableperformancelevel. Incontrast,thedefeatedrateshowedaconsistent
decrease,approachinganasymptoticvalue. Theseobservationsconfirmtheeffectivenessofself-play
inRichelieu’sevolution.
5 Experiment
In the experiments, our goal is to answer the following questions: 1) Mastery of Non-Press
Diplomacy: Canouragentmasterthenon-pressdiplomacyagainstbaselines? 2)Competingwith
State-of-the-Art: Canouragentsurpasstheperformanceofthecurrentstate-of-the-artagentsin
6pressdiplomacy? 3)CompatibilitywithLLMs: Canourself-evolvingframeworkbecompatible
withdifferentLLMs? 4)ContributionofFrameworkModules: Dotheindividualmoduleswithin
ourframeworkcontributetotheoverallimprovementofouragent’sperformance?
5.1 ExperimentalSetup
Environment. The widely-used open source Diplomacy game platform introduced by [39] is
adoptedforevaluatingRichelieuagainstothermodels. Itiseasytoswitchbetweenno-press(with
communication/negotiationbetweenplayers)andpress(nocommunicationbetweenplayers)games
basedonthisplatform,facilitatingcomparisononbothsettings. Theplatformalsocontainsover
10,000humangamedataonwhichpreviousapproachesaretrained. Notethatourmethoddoesnot
needthem. Ineachgame,amodelwillplaytheroleofonerandomlyselectedcountrytocompete
againstcountriescontrolledbyothermethods. Itwinsifoccupyingallthesupplycentersandloses
viceversa.
EvaluationMetrics. Weevaluatethemodelsbasedontheresultsofmultipleroundsofgames. In
eachround,themodelisrandomlyassignedacountrytocontrol. Typically,1000roundsareplayed
toobtaintheaverageresults. Weevaluatethemodelsintwometrics. Oneisbasedonthewinrate,
MostSCrate,survivedrate,anddefeatedrate. Therearefourpossibleoutcomesforeachcountryin
thegame. Ifacountrylosesallitssupplycenters(SC),itiseliminatedandrecordedas“defeated". If
acountryoccupies18ormoreoutof34supplycenters,thegameends,andthatcountryisrecorded
as“win",whileothercountriesarerecordedas“defeated". Inothercases,thegameendsinadraw.
Thecountrywiththemostsupplycentersisrecordedas“MostSC",thecountriesthathavebeen
eliminatedarerecordedas“defeated",andtheothercountriesarerecordedas“Survived". Theother
isbasedonthescoresobtainedbythemodelsaftermultipleroundsofcompetition. Tocomparethe
capabilitiesofmultiplemodels,weuseC-DiploArgir[4],ascoringsystem. Thissystemisusedin
manyinternationaldiplomacycompetitions. Thescoringmethodisasfollows: Ifaplayerwinsby
occupying18ormoresupplycenters,theplayerscores93points,andeachoftheothersixplayers
scores1point. Ifthegameendsinadraw,theplayerwiththemostcentersscores37points. The
secondplayerwiththemostcentersscores14points. Thethirdplayerwiththemostcentersscores7
points. Eachplayerscores1pointpercenterowned. Eachplayeralsoscores1pointforparticipating.
Inthisway,regardlessofthegameoutcome,atotalof99pointswillbedistributedamongtheplayers
ineachgame.
Baselines. Weselectsixpreviousmodelsasbaselinesforcomparison. Amongthem,Cicero[6]by
Metaisadiplomacymodelwithanegotiationmodule. Theotherfiveareno-pressdiplomacymodels,
includingtheSL-DipNetandRL-DipNet[39],theBRPI[3],theSearchBot[18],andtheDORA[5].
5.2 Results
Massively Play with Baselines on no-press setting. We let Richelieu compete with the other
sixmodelsincludingCicero[6], SL-DipNetandRL-DipNet[39], BRPI[3], SearchBot[18], and
DORA[5]onNo-PressDiplomacy,inwhichplayersmakemoveswithoutcommunication. Figure
4 indicates that Richelieu outperforms other previous models relying on human game data. In
contrast,Richelieudoesnotneedsuchdatabutoutperformsthesemethodsbyaclearmargin,which
demonstratestheoutstandingplanningcapabilityofRichelieu.
PlayagainstCiceroonpresssetting. WealsoevaluateRichelieuthroughcompetitionagainstCicero
inthechallengingscenariowherenegotiationisenabled. Specifically, werandomlyassignthree
countriestoonemodelandtheremainingfourtoanother. Afterplayingseveralroundsofthegame,
thewinrate,mostSCrate,survivedrate,andthedefeatedrateiscalculatedusingaweightedaverage
forevaluation. Table 1demonstratesthecompetitiveperformanceofRichelieuincomparisonto
Cicero. Richelieu’swinrateisapproximately0.7%higherthanCicero’s. IftheMostSCrateisalso
takenintoaccount,Richelieuisabout2%higherthanCicero. Atthesametime,Richelieu’slossrate
isalso0.6%lower. Accordingtoourscoringsystem,Richelieu’sscoreisabout10%higherthan
Cicero’s. ThisisnontrivialespeciallywhenRichelieuistrainedinaself-playgamewithouthumans
andtheopponentsaretrainedwiththedatafromhumanplayers.
Generalization of self-evolving framework to diverse LLMs. Llamma 3 To demonstrate the
effectivenessofourframeworkinavarietyofLLM,weconductedexperimentsusingfourmodels:
GPT4.0, ERNIE Bot, Spark Desk, and Llamma 3. The experimental results show that, despite
7Table1: TheresultsofourmethodplayingagainstCicero.
Model Win↑ MostSC↑ Survived↑ Defeated↓ Model Win↑ MostSC↑ Survived↑ Defeated↓
Richelieu_1 6.20% 9.40% 38.90% 45.50% Richelieu_1 6.30% 7.90% 39.40% 46.40%
Richelieu_2 6.60% 7.80% 40.80% 44.80% Richelieu_2 6.60% 8.30% 41.20% 43.90%
Richelieu_3 7.10% 9.30% 39.90% 43.70% Richelieu_3 7.20% 8.70% 41.70% 42.40%
Richelieu_4 7.40% 8.00% 40.20% 44.40% Cicero_1 5.80% 6.70% 41.20% 46.30%
Cicero_1 5.90% 6.50% 41.50% 46.10% Cicero_2 6.50% 7.20% 42.50% 43.80%
Cicero_2 6.30% 7.20% 42.50% 44.00% Cicero_3 6.00% 7.00% 41.60% 45.40%
Cicero_3 5.90% 7.00% 41.60% 45.50% Cicero_4 6.10% 7.20% 42.30% 44.40%
Richelieu 6.83% 8.63% 39.95% 44.60% Richelieu 6.70% 8.30% 40.77% 44.23%
Cicero 6.03% 6.90% 41.87% 45.20% Cicero 6.10% 7.03% 41.90% 44.98%
variations in Richelieu’s performance due to the inherent differences in the capabilities of these
LLMs, as illustrated in Figure 5, our framework and training approach significantly enhance the
capabilitiesofalllargelanguagemodels. Aftertraining,thewinrateusingGPT4.0increasedfrom
1.5%lowerthanCicero’stoabout0.7%higherthanCicero’s. Thewinrateusingllama3increased
from2.3%lowerthanCicero’stoalmostequaltoCicero’s. ThewinratesusingModelsSparkDesk
andERNIEBotincreasedfrom3%and4%lowerthanCicero’sto0.7%and1.6%lowerthanCicero’s,
respectively. Thisindicatesthegeneralizationofaself-evolvingframeworktovariousLLMs.
AblationStudy. Weconductcomprehensiveab-
SL-DipNet
lationstudiesonRichelieubyanalyzingtheben- 3.5
efit of incorporating Richelieu’s various mod- RL-DipNet
3.0
ules,likeplannersormemory,intobasicLLMs.
BRPI
TheresultsareshowninTable2.Ascanbeseen, 2.5
directuseofvanillaLLMyieldsrelativelypoor SearchBot 2.0
results. Richelieu’sperformanceobtainssteady DORA 1.5
and significant improvement by incorporating
Cicero 1.0
each individual module. This indicates that
Richelieuisabletoleverageotherplayers’ac- Richelieu 0.5
t
s
Ri ho
io
cn
r
hs
t
e-d
t le
iu err
m
uin ’sg
an
nd
d
ee gc
l
oois
n
ti igo a-n tit-
e
om
r
nma ak bbin
ie
lg
n ite
ya fin htd
s
a.c sAo bn
d
es
d
ei
i
nd tie
o
sr
in
gb
a
no
ll
it
y
fh
-,
SL-DipN Re Lt -DipNet BRPI SearchBot
D
O RA Cicero Richelieu
Figure 4: The relative scores among 7 different
icantlyimproved,allowingittoeffectivelyex-
agents when massively playing on the no-press
pressintentionstocooperatewithotherplayers
setting. Eachpointshowstheratioofthemodel’s
andavoiddeceptionduringnegotiations. And
scoreontheverticalaxistothescoregainedbythe
afterself-play,Richelieu’sexperiencemakesit
modelonthehorizontalaxis.
performbetter.
Win rate Win & Most SC rate Defeated rate
8.50% 18.0% 51.0%
8.00% 50.0%
17.0%
7.50% 49.0%
16.0%
7.00% 48.0%
15.0%
6.50%
47.0%
14.0%
6.00% GPT4.0-R
13.0% 46.0% GPT4.0-C
5.50% ERNIE Bot-R
5.00% 12.0% 45.0% E S SR p pa aN r rI k kE D DB e eo s st- k kC - -R C
4.50% 11.0% 44.0% l ll la am ma a3 3- -R C
9 10 11 12 9 10 11 12 9 10 11 12
Figure 5: Richelieu modules benefit different LLMs. The solid line represents the experimental
results for Richelieu, while the dashed line corresponds to Cicero. Different colors are used for
differentLLMs. Thehorizontalaxisrepresentsthelogarithmofthenumberoftrainingsessions,and
theverticalaxisdenotestherate.
8Table2: Ablationstudy: averageresultsof3Richelieuvs. 4Cicero.
Modeling Negotiation Reflection
sub-goals Self-play Win↑ MostSC↑ Survived↑ Defeated↓
others pipeline withMemory
✗ ✗ ✗ ✗ ✗ 0.4% 0.7% 4.3% 94.6%
✓ ✗ ✗ ✗ ✗ 0.7% 1.2% 10.6% 87.5%
✓ ✓ ✗ ✗ ✗ 3.3% 4.7% 26.7% 65.3%
✓ ✓ ✓ ✗ ✗ 3.8% 5.8% 33.1% 57.3%
✓ ✓ ✓ ✓ ✗ 5.2% 6.6% 39.5% 48.7%
✓ ✓ ✓ ✓ ✓ 6.7% 8.5% 40.4% 44.4%
Richelieu
FRE: Russia is too powerful
now. We must form an alliance
to prevent Russia's further
expansion. So let's cease
hostilities. I can also help you
defend against England's
attacks, allowing you to fully
focus on countering Russia. AUS: I agree to the alliance, but I
hope you can also participate in
the attack on Russia.
FRE: But I do not share a
border with Russia. AUS: I can give Munich to you,
but you need to exchange it with
Napoli. Also, I hope your fleets
will not be used to attack
England, but rather to attack
Russia's territories in Denmark,
Sweden, Norway, and other
regions.
FRE: Alright, I agree.
AUS: OK.
Figure 6: An example case of negotiating with a nation to form an alliance to confront a strong
enemy.
6 ExampleCases
AsisshowninFigure 6,RichelieucontrolsFrance. Atthistime,FranceisatwarwithAustriaover
controloftheApenninePeninsula. However, Russiaisonthevergeofvictoryinitswaragainst
Turkey,whichwillleadtosignificantterritorialexpansionforRussia. AlthoughFranceandRussia
currentlydonotshareaborder,arenotatwar,andhavenoconflictsofinterest,Richelieuforesees
Russiabecomingthemostthreateningenemyinthefuture. Therefore,Richelieusetsasub-goalof
weakeningRussia.
Inthesubsequentnegotiationphase,RichelieuproactivelyproposesendingthewarwithAustria,
despiteholdinganadvantageinthisconflict. RichelieupromisesAustriathatifitceaseshostilities
andattacksRussia,RichelieuwillassistAustriaindefendingagainstanyattacksfromEngland. The
negotiationsaresuccessful. AustriaacceptedRichelieu’sproposal,andthetwocountriesreachedan
agreementtoexchangethesupplycentersofNapoliandMunich.
Duringtheactionphase,AustriamovesitstroopsfromVenicetoApuliainpreparationforcapturing
Napoliinthenextturn,whiletherestofitsforcesarerepositionedtotheeasternregionsbordering
RussiatodefendagainstRussianattacksandcompeteforsupplycenters.FrenchunitsoccupyMunich
andpreparetoadvanceonRussianterritoriessuchasBerlin.Meanwhile,FrenchunitssupportAustria
intheHollandandBelgiumregions.
AsshowninFigure 7,RichelieucontrolsGermany. Duringthenegotiationphase,Englandproposed
aceasefiretoGermanyandinvitedGermanytoformanalliancetojointlyattackFrance. England
hopedtoceasethewarwithGermanyinHollandandBelgium. Subsequently,Germanunitswould
supportEnglandinattackingBrest,andthenEnglandwouldutilizeitsfleetstoassistGermanyin
attackingSpainandPortugal. RichelieususpectedthatEnglandwasdeceivingGermany,asEngland
9was likely to attack territories in the north such as Belgium and Berlin after German units were
redirectedtosupportBrest. Therefore,wepretendedtoacceptEngland’sallianceproposalduringthe
negotiationprocess. However,atthesametime,wesoughtoutFranceandexpressedourwillingness
toceasehostilities,allowingFrancetofocusentirelyondefendingagainstEngland’sattacks. Inthe
actionphase,England’sactionsconfirmedRichelieu’ssuspicions. EnglandattackedBelgiumfrom
Holland,butbecauseRichelieudidn’tmoveunitsinBelgium,England’sattackfailed.
7 Conclusion
In this paper, we introduce Richelieu, a self-evolving LLM-based agent for AI diplomacy. Our
modelenableshierarchicalplanningformulti-agenttasksandutilizesamemorymoduleforreflective
optimization. Ourmodeldoesnotrequirehumandataandcanevolvethroughself-play. Itultimately
outperforms existing models like Cicero in the Diplomacy. Our ablation study demonstrates the
effectivenessofthemoduleswehaveestablished. ByconductingexperimentsusingdifferentLLMs,
we validate the generalization of our framework to various LLMs. We believe that the use of
LLM-basedagentswillbecomeaneffectiveapproachinsocialscienceinthefuture.
8 LimitationsandFutureWork
Ourstudyissubjecttocertainlimitations. Weutilizediplomacyastheplatformforconstructingour
model. However,thespaceofactionswithindiplomacyisconstrained,whereasthedecision-making
space in real-world diplomacy is virtually boundless. In Diplomacy, apart from the negotiation
information exchanged between players, all other information is public and certain. Conversely,
real-worlddiplomacyoperateswithinaframeworkofincompleteinformation.
ThepotentialapplicationsofsuchanAIagentarevast,rangingfromsimulateddiplomaticenviron-
mentstoreal-worldassistanceandanalysis. Infutureresearch,weintendtodevelopamorerealistic
gamespace,characterizedbyincompleteinformationandmulti-playergames,toenhanceandrefine
our model further. We will also extend the framework to other multi-agent scenarios, including
embodiedinteractions[75,11,9],sensornetworks[54,61,38,31],andvideogames[49,34]. This
framework can also be employed to develop various applications. For instance, in the fields of
economicsandfinance,weintendtoutilizeittocreatebusinessanalyticsandnegotiationmodels.
ENG: Let's attack France
together. Please support my
attack on Brest. I'm willing to
Richelieu follow up with naval support
from the Mid-Atlantic and land
support from Brest to aid your
GRE to ENG: Alright, I can attack on Spain and Portugal.
move my army from Belgium to
Picardy to support your attack
on Brest afterward.
GRE to FRE: I hope to
defend against the British attack
together with you. I will move the
army from Marseilles to
Burgundy. I hope you can cease
the standoff with my army in
Marseilles.
FRE: OK, I agree to ally with you.
Figure7: Anexamplecaseofavoidingbeingdeceivedbyothercountriesduringnegotiations.
10References
[1] Sahar Abdelnabi, Amr Gomaa, Sarath Sivaprasad, Lea Schönherr, and Mario Fritz. Llm-
deliberation: Evaluatingllmswithinteractivemulti-agentnegotiationgames. arXivpreprint
arXiv:2309.17234,2023.
[2] CalhamerAllan. TheGames&puzzlesbookofmodernboardgames. W.Luscombe,1975.
ISBN978-0860020592.
[3] ThomasAnthony,TomEccles,AndreaTacchetti,JánosKramár,IanGemp,ThomasHudson,
NicolasPorcel,MarcLanctot,JulienPérolat,RichardEverett,etal. Learningtoplayno-press
diplomacywithbestresponsepolicyiteration. AdvancesinNeuralInformationProcessing
Systems,33:17987–18003,2020.
[4] Bruno-AndrÃ© Giraudon & Vincent Archer. C-diplo argir, 2024. URL https://
world-diplomacy-database.com/php/scoring/scoring_class.php?id_scoring=7.
Accessed:2024-05-02.
[5] AntonBakhtin,DavidWu,AdamLerer,andNoamBrown. No-pressdiplomacyfromscratch.
AdvancesinNeuralInformationProcessingSystems,34:18063–18074,2021.
[6] Anton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried,
AndrewGoff,JonathanGray,HengyuanHu,etal. Human-levelplayinthegameofdiplomacy
bycombininglanguagemodelswithstrategicreasoning. Science,378(6624):1067–1074,2022.
[7] FedericoBianchi,PatrickJohnChia,MertYuksekgonul,JacopoTagliabue,DanJurafsky,and
James Zou. How well can llms negotiate? negotiationarena platform and analysis. arXiv
preprintarXiv:2402.05863,2024.
[8] Allan Calhamer. The invention of diplomacy. https://web.archive.org/web/
20090910012615/http://www.diplom.org/~diparch/resources/calhamer/
invention.htm,1974. Accessed: 2024-05-18.
[9] YuanpeiChen,YiranGeng,FangweiZhong,JiamingJi,JiechuangJiang,ZongqingLu,Hao
Dong,andYaodongYang.Bi-dexhands:Towardshuman-levelbimanualdexterousmanipulation.
IEEETransactionsonPatternAnalysisandMachineIntelligence,2023.
[10] Guangran Cheng, Chuheng Zhang, Wenzhe Cai, Li Zhao, Changyin Sun, and Jiang Bian.
Empoweringlargelanguagemodelsonroboticmanipulationwithaffordanceprompting. arXiv
preprintarXiv:2404.11027,2024.
[11] HaiCi,MickelLiu,XuehaiPan,FangweiZhong,andYizhouWang. Proactivemulti-camera
collaborationfor3dhumanposeestimation. arXivpreprintarXiv:2303.03767,2023.
[12] Hill David. The board game of the alpha nerds. https://grantland.com/features/
diplomacy-the-board-game-of-the-alpha-nerds/,2014. Accessed: 2024-05-18.
[13] DaveDeJongeandCarlesSierra. D-brane: adiplomacyplayingagentforautomatednegotia-
tionsresearch. AppliedIntelligence,47(1):158–177,2017.
[14] I de Zarzà, J de Curtò, Gemma Roig, Pietro Manzoni, and Carlos T Calafate. Emergent
cooperationandstrategyadaptationinmulti-agentsystems: Anextendedcoevolutionarytheory
withllms. Electronics,12(12):2722,2023.
[15] Edgar A Duéñez-Guzmán, Suzanne Sadedin, Jane X Wang, Kevin R McKee, and Joel Z
Leibo. Asocialpathtohuman-likeartificialintelligence. NatureMachineIntelligence,5(11):
1181–1188,2023.
[16] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu,
Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building
open-ended embodied agents with internet-scale knowledge. In Thirty-sixth Conference
on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL
https://openreview.net/forum?id=rc8o_j8I8PX.
11[17] HangGaoandYongfengZhang. Memorysharingforlargelanguagemodelbasedagents. arXiv
preprintarXiv:2404.09982,2024.
[18] JonathanGray,AdamLerer,AntonBakhtin,andNoamBrown. Human-levelperformanceinno-
pressdiplomacyviaequilibriumsearch. InternationalConferenceonLearningRepresentations,
2020.
[19] ÖnderGürcan. Llm-augmentedagent-basedmodellingforsocialsimulations: Challengesand
opportunities. HHAI2024: HybridHumanAISystemsfortheSocialGood,pages134–144,
2024.
[20] KostasHatalis,DespinaChristou,JoshuaMyers,StevenJones,KeithLambert,AdamAmos-
Binks,ZohrehDannenhauer,andDustinDannenhauer. Memorymatters: Theneedtoimprove
long-termmemoryinllm-agents.InProceedingsoftheAAAISymposiumSeries,pages277–280,
2023.
[21] Junda He, Christoph Treude, and David Lo. Llm-based multi-agent systems for software
engineering: Visionandtheroadahead. arXivpreprintarXiv:2404.04834,2024.
[22] Avalon Hill. Diplomacy rules 4th edition. https://web.archive.org/web/
20140915055823/http://www.diplomacy-archive.com/resources/rulebooks/
2000AH4th.pdf,2014. Accessed: 2024-05-18.
[23] YukiHou,HarukiTamoto,andHomeiMiyashita. "myagentunderstandsmebetter": Integrat-
ingdynamichuman-likememoryrecallandconsolidationinllm-basedagents. InExtended
AbstractsoftheCHIConferenceonHumanFactorsinComputingSystems,pages1–7,2024.
[24] AthulPaulJacob,DavidJWu,GabrieleFarina,AdamLerer,HengyuanHu,AntonBakhtin,Ja-
cobAndreas,andNoamBrown. Modelingstrongandhuman-likegameplaywithkl-regularized
search. InInternationalConferenceonMachineLearning,pages9695–9728.PMLR,2022.
[25] KokilJaidka,HansinAhuja,andLynnetteHuiXianNg. Ittakestwotonegotiate: Modeling
socialexchangeinonlinemultiplayergames. ProceedingsoftheACMonHuman-Computer
Interaction,8(CSCW1):1–22,2024.
[26] AndrewKonya,DegerTuran,AvivOvadya,LinaQui,DaanishMasood,FlynnDevine,Lisa
Schirch,andIsabellaRoberts. Deliberativetechnologyforalignment. ArXiv,abs/2312.03893,
2023.
[27] ConorKostick. TheArtofCorrespondenceintheGameofDiplomacy. Curses&Magic,2nd
edition,2015. ISBN978-0993415104.
[28] Grgur Kovacˇ, Rémy Portelas, Peter Ford Dominey, and Pierre-Yves Oudeyer. The socialai
school: Insightsfromdevelopmentalpsychologytowardsartificialsocio-culturalagents. arXiv
preprintarXiv:2307.07871,2023.
[29] JánosKramár,TomEccles,IanGemp,AndreaTacchetti,KevinRMcKee,MateuszMalinowski,
ThoreGraepel,andYoramBachrach. Negotiationandhonestyinartificialintelligencemethods
fortheboardgameofdiplomacy. NatureCommunications,13(1):7214,2022.
[30] HaoLi,ChenghaoYang,AnZhang,YangDeng,XiangWang,andTat-SengChua. Helloagain!
llm-powered personalized agent for long-term dialogue. arXiv preprint arXiv:2406.05925,
2024.
[31] JingLi,JingXu,FangweiZhong,XiangyuKong,YuQiao,andYizhouWang. Pose-assisted
multi-cameracollaborationforactiveobjecttracking. InProceedingsoftheAAAIConference
onArtificialIntelligence,volume34,pages759–766,2020.
[32] YuanchunLi,HaoWen,WeijunWang,XiangyuLi,YizhenYuan,GuohongLiu,JiachengLiu,
WenxingXu,XiangWang,YiSun,etal. Personalllmagents: Insightsandsurveyaboutthe
capability,efficiencyandsecurity. arXivpreprintarXiv:2401.05459,2024.
[33] ZhiweiLiu,WeiranYao,JianguoZhang,LiangweiYang,ZuxinLiu,JuntaoTan,PrafullaK
Choubey,TianLan,JasonWu,HuanWang,etal. Agentlite: Alightweightlibraryforbuilding
andadvancingtask-orientedllmagentsystem. arXivpreprintarXiv:2402.15538,2024.
12[34] Long Ma, Yuanfei Wang, Fangwei Zhong, Song-Chun Zhu, and Yizhou Wang. Fast peer
adaptationwithcontext-awareexploration. arXivpreprintarXiv:2402.02468,2024.
[35] Farhad Moghimifar, Yuan-Fang Li, Robert Thomson, and Gholamreza Haffari. Modelling
politicalcoalitionnegotiationsusingllm-basedagents. arXivpreprintarXiv:2402.11712,2024.
[36] GabrielMukobi,Ann-KatrinReuel,Juan-PabloRivera,andChandlerSmith. Assessingrisks
ofusingautonomouslanguagemodelsinmilitaryanddiplomaticplanning. InMulti-Agent
SecurityWorkshop@NeurIPS’23,2023.
[37] Sean Noh and Ho-Chun Herbert Chang. Llms with personalities in multi-issue negotiation
games. arXivpreprintarXiv:2405.05248,2024.
[38] XuehaiPan,MickelLiu,FangweiZhong,YaodongYang,Song-ChunZhu,andYizhouWang.
Mate: Benchmarkingmulti-agentreinforcementlearningindistributedtargetcoveragecontrol.
AdvancesinNeuralInformationProcessingSystems,35:27862–27879,2022.
[39] PhilipPaquette,YuchenLu,SetonStevenBocco,MaxSmith,SatyaO-G,JonathanKKummer-
feld,JoellePineau,SatinderSingh,andAaronCCourville. No-pressdiplomacy: Modeling
multi-agentgameplay. AdvancesinNeuralInformationProcessingSystems,32,2019.
[40] SiyuanQi,ShuoChen,YexinLi,XiangyuKong,JunqiWang,BangchengYang,PringWong,
Yifan Zhong, Xiaoyuan Zhang, Zhaowei Zhang, Nian Liu, Yaodong Yang, and Song-Chun
Zhu. Civrealm: Alearningandreasoningodysseyincivilizationfordecision-makingagents.
In The Twelfth International Conference on Learning Representations, 2024. URL https:
//openreview.net/forum?id=UBVNwD3hPN.
[41] MatthewRenzeandErhanGuven. Self-reflectioninllmagents: Effectsonproblem-solving
performance. arXivpreprintarXiv:2405.06682,2024.
[42] SharpRichard. Thegameofdiplomacy. ArthurBarker,1979. ISBN978-0213166762.
[43] TimoSchick,JaneDwivedi-Yu,RobertoDessi,RobertaRaileanu,MariaLomeli,EricHambro,
LukeZettlemoyer,NicolaCancedda,andThomasScialom. Toolformer: Languagemodelscan
teachthemselvestousetools. InThirty-seventhConferenceonNeuralInformationProcessing
Systems,2023.
[44] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.
HuggingGPT:SolvingAItaskswithchatGPTanditsfriendsinhuggingface. InThirty-seventh
ConferenceonNeuralInformationProcessingSystems,2023.
[45] SarahShoker,AndrewW.Reddie,SarahBarrington,MilesBrundage,HusanjotChahal,Michael
Depp,BillDrexel,RitwikGupta,MarinaFavaro,JakeJ.Hecla,AlanHickey,MargaritaKonaev,
KIPavanKumar,NathanLambert,AndrewJ.Lohn,CullenO’Keefe,NazneenRajani,Michael
Sellitto,RobertF.Trager,LeahA.Walker,AlexaWehsener,andJessicaYoung. Confidence-
buildingmeasuresforartificialintelligence: Workshopproceedings. ArXiv,abs/2308.00862,
2023.
[46] ChuannengSun,SongjunHuang,andDarioPompili. Llm-basedmulti-agentreinforcement
learning: Currentandfuturedirections. arXivpreprintarXiv:2405.11106,2024.
[47] YasharTalebiradandAmirhosseinNadiri. Multi-agentcollaboration: Harnessingthepowerof
intelligentllmagents. ArXiv,abs/2306.03314,2023.
[48] HongyuWan,JindaZhang,AbdulazizArifSuria,BingshengYao,DakuoWang,YvonneCoady,
andMirjanaPrpa. Buildingllm-basedaiagentsinsocialvirtualreality. InExtendedAbstracts
oftheCHIConferenceonHumanFactorsinComputingSystems,pages1–7,2024.
[49] DongziWang,FangweiZhong,MinglongLi,MuningWen,YuanxiPeng,TengLi,andAdam
Yang. Romat: Role-basedmulti-agenttransformerforgeneralizableheterogeneouscooperation.
NeuralNetworks,174:106129,2024.
13[50] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi
Fan,andAnimaAnandkumar. Voyager: Anopen-endedembodiedagentwithlargelanguage
models. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. URL
https://openreview.net/forum?id=P8E4Br72j3.
[51] HaoyuWang,TaoLi,ZhiweiDeng,DanRoth,andYangLi. Devil’sadvocate: Anticipatory
reflectionforllmagents. arXivpreprintarXiv:2405.16334,2024.
[52] LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,JingsenZhang,ZhiyuanChen,
JiakaiTang,XuChen,YankaiLin,etal. Asurveyonlargelanguagemodelbasedautonomous
agents. FrontiersofComputerScience,18(6):1–26,2024.
[53] XuezhiWang, JasonWei, DaleSchuurmans, QuocLe, EdChi, SharanNarang, Aakanksha
Chowdhery,andDennyZhou.Self-consistencyimproveschainofthoughtreasoninginlanguage
models. arXivpreprintarXiv:2203.11171,2022.
[54] YuanfeiWang,FangweiZhong,JingXu,andYizhouWang.Tom2c:Target-orientedmulti-agent
communicationandcooperationwiththeoryofmind. arXivpreprintarXiv:2111.09189,2021.
[55] ZihaoWang,ShaofeiCai,GuanzhouChen,AnjiLiu,Xiaojian(Shawn)Ma,andYitaoLiang.
Describe,explain,planandselect: Interactiveplanningwithllmsenablesopen-worldmulti-task
agents. InA.Oh,T.Naumann,A.Globerson,K.Saenko,M.Hardt,andS.Levine,editors,
AdvancesinNeuralInformationProcessingSystems,volume36,pages34153–34189.Curran
Associates,Inc.,2023. URLhttps://proceedings.neurips.cc/paper_files/paper/
2023/file/6b8dfb8c0c12e6fafc6c256cb08a5ca7-Paper-Conference.pdf.
[56] Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei
Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, Xiaojian Ma, and Yitao Liang. Jarvis-
1: Open-worldmulti-taskagentswithmemory-augmentedmultimodallanguagemodels. ArXiv,
abs/2311.05997,2023.
[57] ZiyanWang,YingpengDu,ZhuSun,HaoyanChua,KaidongFeng,WenyaWang,andJieZhang.
Re2llm: Reflectivereinforcementlargelanguagemodelforsession-basedrecommendation.
arXivpreprintarXiv:2403.16427,2024.
[58] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,
DennyZhou, etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels.
Advancesinneuralinformationprocessingsystems,35:24824–24837,2022.
[59] Wikipedia. Diplomacy(game). https://en.wikipedia.org/wiki/Diplomacy_(game),
2024. Accessed: 2024-05-18.
[60] TianXia,ZhiweiHe,TongRen,YiboMiao,ZhuoshengZhang,YangYang,andRuiWang.
Measuringbargainingabilitiesofllms: Abenchmarkandabuyer-enhancementmethod. arXiv
preprintarXiv:2402.15813,2024.
[61] JingXu,FangweiZhong,andYizhouWang. Learningmulti-agentcoordinationforenhancing
targetcoverageindirectionalsensornetworks. AdvancesinNeuralInformationProcessing
Systems,33:10053–10064,2020.
[62] MingYan,RuihaoLi,HaoZhang,HaoWang,ZhilanYang,andJiYan. Larp: Language-agent
roleplayforopen-worldgames. arXivpreprintarXiv:2312.17653,2023.
[63] RuiYang,LinSong,YanweiLi,SijieZhao,YixiaoGe,XiuLi,andYingShan. GPT4tools:
Teachinglargelanguagemodeltousetoolsviaself-instruction. InThirty-seventhConference
onNeuralInformationProcessingSystems,2023.
[64] ZiyiYang,ShreyasSRaman,AnkitShah,andStefanieTellex.Pluginthesafetychip:Enforcing
constraintsforllm-drivenrobotagents. arXivpreprintarXiv:2309.09919,2023.
[65] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and
Karthik R Narasimhan. Tree of thoughts: Deliberate problem solving with large language
models. InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
14[66] Yangyang Yu, Haohang Li, Zhi Chen, Yuechen Jiang, Yang Li, Denghui Zhang, Rong Liu,
JordanWSuchow,andKhaldounKhashanah. Finmem: Aperformance-enhancedllmtrading
agentwithlayeredmemoryandcharacterdesign. InProceedingsoftheAAAISymposiumSeries,
volume3,pages595–597,2024.
[67] HaolanZhan,YufeiWang,TaoFeng,YunchengHua,SurajSharma,ZhuangLi,LizhenQu,
ZhalehSemnaniAzad,IngridZukerman,andGholamrezaHaffari. Let’snegotiate! asurveyof
negotiationdialoguesystems. arXivpreprintarXiv:2402.01097,2024.
[68] DanyangZhang,LuChen,SituoZhang,HongshenXu,ZihanZhao,andKaiYu.Largelanguage
modelissemi-parametricreinforcementlearningagent. CoRR,abs/2306.07929,2023. doi:
10.48550/arXiv.2306.07929. URLhttps://doi.org/10.48550/arXiv.2306.07929.
[69] DanyangZhang,LuChen,SituoZhang,HongshenXu,ZihanZhao,andKaiYu.Largelanguage
modelsaresemi-parametricreinforcementlearningagents. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[70] WenqiZhang,KeTang,HaiWu,MengnaWang,YongliangShen,GuiyangHou,ZeqiTan,Peng
Li,YuetingZhuang,andWeimingLu. Agent-pro: Learningtoevolveviapolicy-levelreflection
andoptimization. arXivpreprintarXiv:2402.17574,2024.
[71] YadongZhang,ShaoguangMao,TaoGe,XunWang,AdriandeWynter,YanXia,WenshanWu,
TingSong,ManLan,andFuruWei. Llmasamastermind: Asurveyofstrategicreasoningwith
largelanguagemodels. arXivpreprintarXiv:2404.01230,2024.
[72] YangZhang,ShixinYang,ChenjiaBai,FeiWu,XiuLi,XuelongLi,andZhenWang.Towardsef-
ficientllmgroundingforembodiedmulti-agentcollaboration. arXivpreprintarXiv:2405.14314,
2024.
[73] ZeyuZhang,XiaoheBo,ChenMa,RuiLi,XuChen,QuanyuDai,JiemingZhu,ZhenhuaDong,
andJi-RongWen. Asurveyonthememorymechanismoflargelanguagemodelbasedagents.
arXivpreprintarXiv:2404.13501,2024.
[74] ZhuoshengZhang,AstonZhang,MuLi,andAlexSmola.Automaticchainofthoughtprompting
inlargelanguagemodels. arXivpreprintarXiv:2210.03493,2022.
[75] Fangwei Zhong, Xiao Bi, Yudi Zhang, Wei Zhang, and Yizhou Wang. Rspt: reconstruct
surroundingsandpredicttrajectoryforgeneralizableactiveobjecttracking. InProceedingsof
theAAAIConferenceonArtificialIntelligence,volume37,pages3705–3714,2023.
[76] Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su, Chenyu Yang, Gao Huang,
BinLi,LeweiLu,XiaogangWang,YuQiao,ZhaoxiangZhang,andJifengDai. Ghostinthe
minecraft: Generallycapableagentsforopen-worldenvironmentsvialargelanguagemodels
withtext-basedknowledgeandmemory. arXivpreprintarXiv:2305.17144,2023.
15A ImplementationDetails
A.1 RulesofDiplomacyGame
• Youneedtooccupyasmanysupplycentersaspossible. Ifyouoccupy18ormoresupply
centers, youwillwinthegamedirectly. Ifyouloseallyoursupplycenters, youwillbe
eliminatedimmediately.
• Theunitsconsistofarmiesandfleets. Armiescanonlymovetoadjacentareas,whilefleets
canmovetoadjacentseazonesorcoastalareasandcanmovealongthecoast.
• Tooccupyasupplycenter,yourunitsmustmoveintothatareaintheautumn.
• Whenaunitmovestoanarea,ifanotherunitisinthedestinationorifotherunitsarealso
movingtothatdestination,themovefails,resultinginastandoff. Insuchcases,youcan
seeksupportfromunitsinadjacentareastothedestination. Ifanotherunitmovesintothe
regionfromwhichsupportiscoming,thesupportiscutoff. Theunitwiththemostsupport
movesintothearea,whileotherunitsmustretreattoanadjacentprovinceordisband. If
thereisnoplacetoretreat,theunitmustdisband. Fleetscantransportarmiesacrosssea
zonesfromonecoastalregiontoanother. However,ifanotherfleetmovesintothatseazone,
thetransportiscutoff.
• The number of units a country can have cannot exceed the number of supply centers it
controls. Ifthenumberofsupplycentersdecreases,excessunitsmustbedisbanded. Each
autumn,newunitscanbebuiltatsupplycenters. Coastalsupplycenterscanproducefleets
orarmies,whileotherscanonlyproducearmies. [22]
A.2 DomainKnowledge
Richelieucanadoptastrategyofallyingwithdistantcountrieswhileattackingneighboringones
tooccupyadjacentterritoriesandachieverapidexpansion. Richelieushouldpayattentiontothe
BalanceofPowerbyformingallianceswithothercountriesorsupportingweakerstatestoprevent
anysinglecountryoralliancefrombecomingtoopowerful. [12]Tothisend,Richelieucanalsoadopt
astrategyofattackingdistantcountrieswhileallyingwithnearbyones,sacrificingshort-termbenefits
toavoidtheemergenceoffuturehegemonicstatesthatcouldthreatenhisownsurvival. Whenfacing
multipleenemies,Richelieucanfindwaystodivideothercountriesandincitewarsamongthem.
Whetherinoffenseordefense,Richelieushouldactivelychoosesuitableallies. Richelieucanalso
introduceathirdpartytoachievegoalssuchasceasefire,alliance,orjointattack. Toachievealliances
orceasefires,Richelieucansacrificesomeintereststotheotherpartyaslongastheultimatebenefits
aregreater. Othersmaylieanddeceive[27];theirwordsinnegotiationsarenotbinding. Richelieu
mustavoidbeingdeceivedorbetrayed. Atthesametime,Richelieucanalsoactivelydeceiveothers
toachievehisowngoals.[42,2]
A.3 PromptTemplates
Fortheconvenienceofreproducingtheresultsoftheexperimentsofthispaper,herewegivethe
prompttemplateofdifferentmodulesofRichelieu.
1)INIT
1 You will control {country} and compete with six other countries
on the map for supply centers.
2 The map consists of different regions and sea areas. Their
adjacency relationships are shown in the matrix. The numbers
for the regions and sea areas are ......
3 Different regions are occupied by different countries. The
ownership of the regions is shown in the matrix.
4 The region Berlin, ........ are supply centers.
5 You need to follow these rules ......
6 To help you achieve victory, these diplomatic strategies might
be of assistance. ......
162)SocialReasoning
1 France occupies Portugal Ruhr, Paris, Burgundy, ......
2 France has armies in Brest, Belgium, ...... And France has
fleets in Mid Atlantic, England Channel, ......
3 England ......
4 ......
5 Based on the current state, what do you think are the current
strategic intentions of the other countries?
6 Which country do you think needs to be attacked or weakened the
most right now?
7 And which country do you think is most suitable for you to ally
with in order to deal with this country?
3)PlannerwithReflection
1 In the current state, with {ally and enemy}, what sub-goal do
you think should be set for {country} ?
2 I have found some useful historical experiences for you. Please
reflect on and optimize your sub-goal based on these
historical experiences.
3 The sub-goal you formulated when {state} was to {sub-goal}. The
eventual result was {future}. The evaluation for this sub-goal
is {score}.
17