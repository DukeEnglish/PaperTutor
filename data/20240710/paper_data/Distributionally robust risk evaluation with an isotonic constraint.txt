Distributionally robust risk evaluation with an isotonic constraint
Yu Gui, Rina Foygel Barber, and Cong Ma
Department of Statistics, University of Chicago
July 10, 2024
Abstract
Statistical learning under distribution shift is challenging when neither prior knowledge nor
fully accessible data from the target distribution is available. Distributionally robust learning
(DRL)aimstocontroltheworst-casestatisticalperformancewithinanuncertaintysetofcandi-
date distributions, but how to properly specify the set remains challenging. To enable distribu-
tionalrobustnesswithoutbeingoverlyconservative,inthispaperweproposeashape-constrained
approach to DRL, which incorporates prior information about the way in which the unknown
target distribution differs from its estimate. More specifically, we assume the unknown density
ratio between the target distribution and its estimate is isotonic with respect to some partial
order. At the population level, we provide a solution to the shape-constrained optimization
problem that does not involve the isotonic constraint. At the sample level, we provide consis-
tency results for an empirical estimator of the target in a range of different settings. Empirical
studies on both synthetic and real data examples demonstrate the improved accuracy of the
proposed shape-constrained approach.
1 Introduction
Evaluatingtheperformanceofstatisticalestimatorsisofsignificantimportanceinstatistics. Togive
several motivating examples, we first consider supervised learning settings, where our observations
consist of features X ∈ X ⊆ Rd and a response Y ∈ Y ⊆ R:
• Given a fitted model µ : X → R, we may want to estimate its average squared error, i.e., the
(cid:98)
expected value of (Y −µ(X))2 with respect to target distribution on (X,Y).
(cid:98)
• Or, in predictive inference, suppose we have constructed a prediction band C(cid:98)1−α (where
C(cid:98)1−α(X) ⊆ R is a confidence region for the response Y ∈ Y given features X ∈ X, and
1−α denotes the target coverage level, e.g., 90% predictive coverage). Then to determine
whether C(cid:98)1−α does in fact achieve coverage at level 1−α for data points drawn from some
target distribution, we would like to estimate the the expected value of 1{Y ̸∈ C(cid:98)1−α(X)} with
respect to this target distribution.
1
4202
luJ
9
]EM.tats[
1v76860.7042:viXraWe can also consider unsupervised learning settings, with observations X ∈ X ⊆ Rd:
• In principal component analysis (PCA), suppose we have obtained a set of pre-fitted principal
componentsV = {v ,...,v }whichformsanorthogonalbasisforaK-dimensionalsubspace
K 1 K
ofX. ToevaluatehowwellthevarianceinX isexplainedbytheprincipalcomponents,itwould
beofinteresttoanalyzetheexpectedvalueofthereconstructionerror∥X−(cid:80)K (X⊤v )v ∥2
k=1 k k
with respect to the distribution of X.
• Another example is density estimation. In this case, given a density estimate P learned from
θ
data, we may want to evaluate its performance using the expected log-likelihood −logdP (X)
θ
over a target distribution P . It is worth pointing out that E [−logdP (X)] is the
target Ptarget θ
cross-entropy of P relative to P .
θ target
A key challenge for any of these problems is that the target distribution (say, the distribution of
the general population) may be unknown, and our available data (say, individuals who participate
in our study) may be drawn from a different distribution than the general population.
1.1 Problem formulation
To make the problem more concrete, and unify the examples mentioned above, here we introduce
some notation to formulate the question at hand.
The unsupervised setting. Let R : X → R denote a risk function, where our goal is to evaluate
the expected value E [R(X)] with respect to some target distribution P on X. However,
Ptarget target
the available data only provides information about P, a potentially different distribution.
Forinstance,indensityestimation,afterobtainingthedensityestimateP ,wecanthenestimate
θ
E [R(X)]usingacalibrationdataset,whichconsistsofsamplesX ,...,X drawnfromP. Instead,
P 1 n
our aim is to provide a bound on the risk E [R(X)], or in other words, to bound the difference
Ptarget
in risks (or excess risk) E [R(X)]−E [R(X)]. If we assume that the unknown distribution
Ptarget P
P lies in some class Q (to be specified later on), then defining the worst-case excess risk
target
∆(R;Q) = sup E [R(X)]−E [R(X)], (1)
Q P
Q∈Q
we can bound
E [R(X)] ≤ E [R(X)]+∆(R;Q).
Ptarget P
The right hand side provides an upper bound on the risk of our estimator under the target distri-
bution P .
target
The supervised setting: covariate shift assumption. In the supervised learning setting, the
data contains both features X and a response Y, so the setup is somewhat different. Here we will
consider a loss function r : X ×Y → R, for instance, r(x,y) = (y −µ(x))2 for the squared error
(cid:98)
in a regression, or r(x,y) = 1{y ̸∈ C(cid:98)1−α(x)} for characterizing the (mis)coverage of a prediction
2interval. Inthispaper,forthesupervisedlearningsetting,wewillassumethecovariate shift setting,
where the distribution of the available data and the target distribution may differ in the marginal
distribution of the covariates X, but share the same conditional distribution Y | X. To make this
concrete, ifourcalibrationdataconsistsofndatapoints(X 1,Y 1),...,(X n,Y n)drawnfromP(cid:101), while
our goal is to control the expected loss with respect to some target distribution P(cid:101)target on (X,Y),
we will assume that we can write
data distribution: P(cid:101) = P ×P Y|X,
target distribution: P(cid:101)target = P target×P Y|X,
so that P(cid:101) and P(cid:101)target share the same conditional distribution P
Y|X
for Y | X.
In fact, under covariate shift, this supervised setting can be unified with the unsupervised one
by defining the risk
R(X) = E[r(X,Y) | X],
which is the conditional expectation of r(X,Y) under either P(cid:101) or P(cid:101)target (since we have assumed
that both of these distributions share the same conditional, P ). The quantity of interest is then
Y|X
given by E [R(X)] = E [r(X,Y)], but our calibration data instead enables us to estimate
Ptarget P(cid:101)target
E [R(X)] = E [r(X,Y)]. If we again assume that P ∈ Q, then ∆(R;Q) again allows us to
P P(cid:101) target
bound the risk of our estimator under the target distribution, which is now given by P(cid:101)target.
Estimating the error or tuning the model? In this paper, we consider the setting where our
estimator—say, a prediction band C(cid:98)1−α—is pretrained, meaning that we have available calibration
data sampled from P (in the unsupervised setting) or P(cid:101) (in the supervised setting) that is inde-
pendent of the fitted estimator. Consequently, our available calibration data provides us with an
unbiased estimate of E [R(X)] (or, equivalently in the supervised setting, E [r(X,Y)].
P P(cid:101)
In some settings, the goal may be to estimate the risk of each estimator within a family of
(pretrained) options, in order to tune the choice of estimator. Returning again to the example of
a prediction band, suppose we actually are given a nested family of prediction bands, {C(cid:98)1−a : a ∈
[0,1]}, i.e., each C(cid:98)1−a aims to provide coverage at some nominal confidence level 1−a. Choosing
R a(X) = P
P
(Y ̸∈ C(cid:98)1−a(X)) (i.e., r a(X,Y) = 1{Y ̸∈ C(cid:98)1−a(X)}), then, if we can compute a
Y|X
bound on E Ptarget[R a(X)], the miscoverage rate of C(cid:98)1−a relative to the target distribution, for each
a, then we can choose a value of a that achieves some desired level of coverage. More generally,
we may do the same in other settings as well—that is, given a family of candidate estimators,
bounding the risk of each one under the target distribution P may be an intermediate step
target
towards choosing the tuning parameter.
1.2 Prior work: distributionally robust learning
Our work builds upon the distributionally robust learning (DRL) literature (Ben-Tal and Ne-
mirovski, 1998; El Ghaoui et al., 1998; Lam, 2016; Duchi and Namkoong, 2018), which is a well
3established framework for performance evaluation under distribution shift. In this framework, the
target distribution P is assumed to lie in some neighborhood around the distribution P of the
target
available data—for instance, we might assume that D (P ∥P) ≤ ρ, where D denotes the
KL target KL
Kullbeck–Leibler (KL) divergence. DRL takes a conservative approach and evaluate the perfor-
mance on P via its upper bound, i.e., the worst-case performance over all distributions within
target
the specified neighborhood of P,
E [R(X)] ≤ sup{E [R(X)] : D (Q∥P) ≤ ρ}. (2)
Ptarget Q KL
Equivalently, we can write this upper bound as
E [R(X)] ≤ E [R(X)]+∆(R;Q (ρ)),
Ptarget P KL
where ∆(R;Q (ρ)) is defined as in (1) above by defining the constraint set as Q = Q (ρ) = {Q :
KL KL
D (Q∥P) ≤ ρ}. (More generally, we can consider divergence measures beyond the KL distance,
KL
as we will describe in more detail below.)
1.3 Our proposal: iso-DRL
If the assumption D (P ∥P) ≤ ρ is correct, then the upper bound (2) is a valid upper bound
KL target
on the target risk E [R(X)]. However, since this bound uses only a KL divergence to define the
Ptarget
constraint P ∈ Q on the target distribution, it could be quite conservative. In many practical
target
settings, additional side information or prior knowledge on the structure of the distribution shift
may allow for a tighter bound, which would be less pessimistic than the worst-case excess risk of
DRL (2). This raises the following key question:
Can we use side information on the distribution shift (between the data distribution P and the
target distribution P ) to improve the worst-case excess risk of DRL in risk evaluation?
target
Inthispaper, westudyonespecificexampleofthistypeofsetting: weassumethatthedensityratio
dPtarget(x)betweenthetargetdistributionandthedatadistributionisisotonic(i.e., monotone)with
dP
respect to some order or partial order on X.
Motivation: recalibration of an estimated density ratio. To motivate the use of such side
information, consider a practical setting where we have an estimate w for the density ratio, i.e.,
0
dP
target
w (x) ≈ (x).
0
dP
For instance, suppose that in addition to labeled data (i.e., (X,Y) pairs) sampled from the data
distributionP×P ,wealsohaveaccesstounlabeled(i.e.,X only)datafromthetargetpopulation
Y|X
P . Wemayusethesetwodatasetstotrainw . Althoughthereisnoguaranteethattheestimate
target 0
w is accurate, the shape or relative magnitude of w may provide us with useful side information,
0 0
e.g., large values of w can identify portions of the target population that are underrepresented
0
4under the distribution P of the the available labeled data. This motivates us to recalibrate w
0
within the set of density ratios that are isotonic in w .
0
To express this scenario in the notation of the problem formulation above, we are assuming that
the target distribution P satisfies an isotonicity constraint, P ∈ Q (w ), where
target target iso 0
(cid:26) (cid:27)
dQ
Q (w ) = Q : (x) is a monotonically nondecreasing function of w (x) .
iso 0 0
dP
If we assume as before that the target distribution P satisfies D (P ∥P) ≤ ρ, and addi-
target KL target
tionally assume the isotonic constraint P ∈ Q (w ), then we can bound
target iso 0
E [R(X)] ≤ E [R(X)]+∆(R;Q (ρ)∩Q (w )). (3)
Ptarget P KL iso 0
The benefits of iso-DRL. What are the benefits of iso-DRL, as compared to the existing DRL
framework? Of course, thus far the idea is quite straightforward—if we have stronger constraints on
P , then we can place a tighter bound on the excess risk E [R(X)]−E [R(X)]. But as we
target Ptarget P
willseebelow, addingtheisotonicconstraintplaysacrucialroleinenablingDRLtoprovidebounds
that are useful in practical scenarios. Specifically, consider a practical setting where the bound ρ on
the distribution shift is a positive constant. As we will see below, the existing worst-case excess risk
of DRL ∆(R;Q (ρ)) is often quite large, leading to extremely conservative statistical conclusions;
KL
incontrast,theworst-caseexcessrisk∆(R;Q (ρ)∩Q (w ))givenbyiso-DRLisoftenvanishingly
KL iso 0
small,leadingtomuchmoreinformativeconclusions. Moreover,thisimprovementinthebounddoes
notincuranyadditionalcomputationalchallenges—wewillseethat∆(R;Q (ρ)∩Q (w ))canbe
KL iso 0
computed as easily as the original ∆(R;Q (ρ)). In addition, we further show that the worst-case
KL
excess risk of iso-DRL can be consistently estimated with noisy observations of R(X) (while the
estimation of the worst-case excess risk of DRL can be challenging even with bounded risks as will
be shown in the appendix).
Example: predictive inference for the wine quality dataset. To illustrate the advantage of
the proposed approach, Figure 1 presents a numerical example for a predictive inference problem on
the wine quality dataset1 (full details for this experiment are given in Section 5.2). We are given
a pretrained family of prediction bands C(cid:98)1−a, indexed by the target coverage level 1−a. At each
value a ∈ [0,1], we define R a(X) = P(Y ̸∈ C(cid:98)1−a(X) | X), the probability of the prediction band
failing to cover the true response value Y given features X ∈ X. Our goal is to return a prediction
band with 90% coverage—that is, we would like to choose a value of a such that the expected risk
E Ptarget[R a(X)] = P P(cid:101)target(Y ̸∈ C(cid:98)1−a(X))
is bounded by 0.1 = 1 − 90%. In our experiment, the available data is the white wine (with
distribution P), while the target population is the red wine (with a different distribution P ).
target
In Figure 1 below, we compare four methods (see Section 5.2 for all details):
1https://archive.ics.uci.edu/dataset/186/wine+quality
5• An uncorrected interval—using conformal prediction (CP), the value a is chosen by tuning on
the calibration data set (i.e., we choose a to satisfy E [R (X)] ≤ 0.1), without correcting for
P a
the distribution shift.
• A corrected interval—using weighted conformal prediction (WCP), the value a is chosen by
tuning on the calibration data set using an estimated density ratio w to correct for the
0
covariate shift between P and P . Since w is estimated from data, this correction is
target 0
imperfect.
• TheDRLinterval—wechooseatosatisfyE [R (X)]+∆(R ;Q (ρ)) ≤ 0.1,whereE [R (X)]
P a a KL P a
and∆(R ;Q (ρ))areestimatedusingthecalibrationdata,andρisanestimateofD (P ∥P).
a KL KL target
• The iso-DRL interval—we choose a to satisfy E [R (X)]+∆(R ;Q (ρ)∩Q (w )) ≤ 0.1,
P a a KL iso 0
where E [R (X)] and ∆(R ;Q (ρ)∩Q (w )) are estimated using the calibration data.
P a a KL iso 0
As we can see in Figure 1 below, the CP and WCP intervals both undercover—for CP, this is
because the method does not correct for distribution shift, while for WCP, this is because the ratio
w that corrects for distribution shift is imperfectly estimated. In contrast, DRL shows substantial
0
overcoverage (and correspondingly returns extremely wide prediction intervals) due to the worst-
case nature of the bound ∆(R ;Q (ρ)). In contrast, Our proposed method, iso-DRL, achieves the
a KL
target coverage rate 90% without excessive increase in the size of the prediction interval, showing
the benefit of adding shape constraints to the DRL framework.
CP
WCP
DRL ( )
iso-DRL ( )
0.8 0.9 1.0 0.0 2.5 5.0 7.5
coverage rate average width
Figure 1: Coverage rate and average width of intervals for the wine quality dataset.
The motivating example validates that, when we have access to meaningful—but imperfect—
side information (in the form of the density ratio w ), adding the shape constraint to iso-DRL can
0
provide an estimate of the risk that is more reliable than a non-distributionally-robust approach,
but less conservative than the original DRL approach.
1.4 Organization of paper
Section2introducesageneralclassofuncertaintysetsforcandidatedistributionsandfurtherstudies
the property of the worst-case excess risk defined in (1) for generic DRL. For the worst-case excess
6risk with the isotonic constraint, we prove that it is equivalent to the worst-case excess risk for a
projected risk function without the isotonic constraint in Section 3. In Section 4, we propose an
estimator of the worse-case excess risk with isotonic constraint and establish the estimation error
bounds. Numerical results for both synthetic and real data are shown in Section 5 and additional
relatedworksaresummarizedinSection6. Technicalproofsandadditionalsimulationsaredeferred
to the appendix.
Notation. Before proceeding, we introduce useful notation for theoretical developments later on.
To begin with, we denote by L (P) (1 ≤ p ≤ ∞) the L function space under the probability
p p
measure P, i.e., when p ̸= ∞,
(cid:40) (cid:41)
(cid:18)(cid:90) (cid:19)1/p
L (P) = f : ∥f∥ = fp(x)dP(x) < ∞ .
p p
X
When p = ∞, the set L (P) consists of measurable functions that are bounded almost surely
∞
underP. Inaddition,forameasurablefunctionw : X → W andameasureP onX,thepushforward
measure w P denotes the measure satisfiying that (w P)(B) = P(w−1(B)) for any B ∈ Z, where
# #
w−1(B) = {x ∈ X : w(x) ∈ B} denotes the preimage of B under w. In other words, if X ∼ P, then
w(X) has distribution w P. We say a function h is A -bounded if |h(x)| ≤ A .
# h h
Fix a partial (pre)order ≼ on X ⊆ Rd. A function g is isotonic in x if g(x ) ≤ g(x ) for any
1 2
x ≼ x . Correspondingly, we define the cone of isotonic functions by
1 2
Ciso = {w : w is isotonic w.r.t. partial order ≼}.
≼
cvx
Lastly, to compare two probability distributions Q and P, we define the convex ordering ≼ by
cvx
Q ≼ P if and only if E [ψ(X)] ≤ E [ψ(X)] for all convex functions ψ.
Q P
2 The distributional robustness framework
As we have explained in Section 1.1, both the unsupervised and supervised (i.e., covariate shift)
settings can be unified—namely, in the covariate shift setting, by defining R(X) = E[r(X,Y) |
X]. Therefore, from now on, to develop our theoretical results we will use the notation of the
unsupervised setting, where we aim to study R(X), with the understanding that this describes the
risk of the supervised setting as well under a covariate shift model.
RecallthatX isthefeaturedomain. GivenariskfunctionR : X → R, thegoalistoevaluate(or
bound)thetargetriskE [R(X)]usingsamplesfromP,byassumingthatthetargetdistribution
Ptarget
P is(insomesense)similartotheavailabledistributionP—moreconcretely, assumingthatthe
target
target distribution P lies in some neighborhood Q around the distribution P of the available
target
data.
7Reformulating the neighborhood. To unify the different examples of constraints described in
Section 1, we will start by considering settings where we can express the constraint Q ∈ Q via some
condition on the density ratio w = dQ. This type of framework includes the sensitivity analysis
dP
settingviatheboundsconstraintonw(Cornfieldetal.,1959;Rosenbaum,1987;Tan,2006;Dingand
VanderWeele, 2016; Zhao et al., 2019b; Yadlowsky et al., 2018; Jin et al., 2022, 2023; Sahoo et al.,
2022), and f-divergence constraints (e.g., bounding D (Q∥P)) (Duchi et al., 2021; Namkoong and
KL
Duchi, 2017; Duchi and Namkoong, 2018; Cauchois et al., 2020).2.
Toenablethis, wewillusethefollowingnotation: byconsideringthedensityratiow(x) = dQ(x)
dP
for any Q ∈ Q, the constraint set Q can be expressed via constraints on this density ratio. We will
now consider settings where the condition Q ∈ Q can be defined via conditions on the distribution
of the density ratio, i.e.,
Q ∈ Q ⇐⇒ w P ∈ B.
#
To make this concrete, let us consider several examples.
Example 1: bound-constrained distribution shift. In sensitivity analysis, it is common to
assume that the likelihood ratio dPtarget is bounded from above and below. This corresponds to a
dP
constraint set of the form
(cid:26) (cid:27)
dQ
Q = Q : a ≤ (X) ≤ b P-almost surely ,
dP
for some constants 0 ≤ a < 1 < b < +∞. (Note that if a = Γ−1 and b = Γ for some Γ > 1, then this
constraint set represents the marginal Γ-selection model for the density ratio in sensitivity analysis
(Rosenbaum, 1987; Tan, 2006).) By defining
(cid:110) (cid:111)
B = B
a,b
= Q(cid:101) : E Z∼Q(cid:101)[Z] = 1, P Z∼Q(cid:101)(a ≤ Z ≤ b) = 1 ,
we can verify that
dQ
Q ∈ Q ⇐⇒ w P ∈ B for w(x) = (x).
# a,b
dP
Example 2: f-constrained distribution shift. For f-constrained distribution shift, we con-
sider the constraint set
(cid:26) (cid:20) (cid:18) (cid:19)(cid:21) (cid:27)
dQ
Q = Q : E f (X) ≤ ρ .
P
dP
Choosing
B = B
f,ρ
= {Q(cid:101) : E Z∼Q(cid:101)[Z] = 1, E Z∼Q(cid:101)[f(Z)] ≤ ρ, P Z∼Q(cid:101)(Z ≥ 0) = 1},
we can verify that
dQ
Q ∈ Q ⇐⇒ w P ∈ B for w(x) = (x).
# f,ρ
dP
As an example, if we take Q = Q (ρ) = {Q : D (Q∥P) ≤ ρ}, this corresponds to choosing
KL KL
f(x) = xlog(x).
2We note that DRL with optimal transport discrepancy is not covered in this framework (Shafieezadeh Abadeh
et al., 2015; Blanchet and Murthy, 2019; Blanchet et al., 2019; Esfahani and Kuhn, 2015) We view this as future
work and have discussion on Section 7.
82.1 Worst-case excess risk with DRL
In this section, we explore some properties of the generic DRL, without the isotonic constraint—
building this framework will help us to introduce the isotonic constraint in the next section.
BasedontheequivalenceofQandBinrepresentingtheuncertaintyset,wefocusonthefollowing
quantity:
∆(R;B) = sup E [w(X)R(X)]−E [R(X)]
P P
w∈L2(P)
subject to w P ∈ B, (4)
#
where abusing notation we now write ∆(·;B) to express that B is a constraint on the distribution
of the density ratio w(X) = dQ(X), where previously we instead wrote ∆(·;Q). We will say that
dP
∆(R;B) is attainable if this supremum is attained by some w∗ in the constraint set.
Throughout the paper, we assume that the set B satisfies the following condition.
cvx
Condition 2.1. The set B is closed under convex ordering, that is, if P ∈ B, then for any Q ≼ P,
it holds that Q ∈ B.
This condition enables the following reformulation of the quantity of interest, ∆(R;B):
Proposition 2.2. Assume Condition 2.1 holds. Then ∆(R;B) can be equivalently written as
∆(R;B) = sup E [(ϕ◦R)(X)R(X)]−E [R(X)]
P P
ϕ:R→R
+
subject to (ϕ◦R) P ∈ B, ϕ is nondecreasing.
#
Moreover, if ∆(R;B) is attainable (i.e., the supremum is attained by some w∗ satisfying the con-
straints), then the equivalent formulation is attainable as well (i.e., the supremum is attained by
some w∗ = ϕ∗◦R where ϕ∗ is nondecreasing).
In other words, this proposition shows that the excess risk is maximized by considering func-
tions w(x) that are monotonically nondecreasing with respect to R(x) (i.e., w = ϕ◦R for some
nondecreasing ϕ). This is intuitive, since maximizing the expected value of w(X)R(X) implies that
we should choose a function w that is large when R is large.
Mostimportantly,Proposition2.2impliesthatforaclassofconstrainedsetsB,theoptimalvalue
to (4) only depends on covariates x through the risk function R(x), or equivalently, only depends
on the distribution of X through the distribution of R(X). As a corollary of Proposition 2.2, the
worst-case excess risk ∆(R;B) is also monotonically nondecreasing in R. This property of ∆(R;B)
is commonly known as the (strict) monotonicity of the functional ∆(R;B) in R(X) under the usual
stochastic order, which is treated as a condition on the functional ∆(R;B) in Shapiro and Pichler
(2023);Shapiro(2017). Wenotethat, whenB isspecifiedintermofanf-divergence(asinExample
2 above), this result is established by Donsker and Varadhan (1976); Lam (2016); Namkoong et al.
(2022).
Next, we return to our two earlier examples of the constraint set B to verify that this result
holds in those settings.
9Example 1: bound-constrained distribution shift. Recall that in this example, we take the
constraint set B to be
(cid:110) (cid:111)
B = B
a,b
= Q(cid:101) : E Z∼Q(cid:101)[Z] = 1, P Z∼Q(cid:101)(a ≤ Z ≤ b) = 1 ,
for some 0 ≤ a < 1 < b < +∞. It is straightforward to verify that B satisfies Condition 2.1,
a,b
meaning that Proposition 2.2 can be applied.
Moreover, in this specific example, we can actually calculate the maximizing density ratio w∗(x)
explicitly (i.e., to achieve the supremum ∆(R;B)). If the distribution of R(X) is continuous, the
worst-case density ratio that attains the worst-case excess risk takes the form
(cid:26) (cid:18) (cid:19)(cid:27) (cid:26) (cid:18) (cid:19)(cid:27)
b−1 b−1
w∗(x) = a·1 R(x) ≤ q +b·1 R(x) > q ,
R R
b−a b−a
where q (t) = inf{r ∈ R | F (r) ≥ t} and F is the cumulative distribution function of R P. For
R R R #
general R(X), we have
w∗(x) = (a−η∗)·1{R(x) < q (t∗)}+(b−η∗)·1{R(x) > q (t∗)}+η∗,
R R
where w∗ is nondecreasing in R(x) and
t∗ = inf(cid:26) t ∈ range(F R) (cid:12) (cid:12) (cid:12)
(cid:12)
t ≥ bb −− a1(cid:27) and η∗ = a+ ( Pb (R− (a X)t )∗ =− q(b (− t∗1 ))) .
R
In particular, we can see that w∗(x) is nondecreasing in R(x), i.e., we can write w = ϕ∗◦R for some
nondecreasing ϕ∗, thus validating that the conclusion of Proposition 2.2 holds in this example.
Example 2: f-constrained distribution shift. Recall that for an f-divergence constraint, we
define
B = B
f,ρ
= {Q(cid:101) : E Z∼Q(cid:101)[Z] = 1, E Z∼Q(cid:101)[f(Z)] ≤ ρ, Z ≥ 0}.
Since f must be convex (for f-divergence to be well-defined), this immediately implies that B
f,ρ
satisfies Condition 2.1. By the results of Donsker and Varadhan (1976); Lam (2016), the worst-case
excess risk ∆ (R;B ) is attained at
ρ f,ρ
(cid:26) (cid:18) R(x)−ν∗(cid:19)(cid:27)
w∗(x) = w(x;λ∗,ν∗) = (f′)−1 ,
λ∗
+
where a is the positive part of a ∈ R and λ∗, ν∗ are the solutions to the dual problem
+
(cid:26) (cid:20) (cid:21)(cid:27)
inf λρ+ν +E w(X;λ,ν)(R(X)−ν)−λf(w(X;λ,ν)) . (5)
P
λ≥0,ν
Since f is convex, the inverse of derivative (f′)−1 is then nondecreasing, meaning that w∗(x) is
nondecreasing in R(x) (i.e., we can write w = ϕ∗ ◦R for some nondecreasing ϕ∗ ), which validates
the result in Proposition 2.2.
103 Worst-case excess risk with an isotonic constraint
In this section, we will now formally introduce our iso-DRL method, adding an isotonic constraint
to the DRL framework developed in Section 2 above.
Recall that
Ciso = {w : X → R : w is isotonic w.r.t. partial order ≼}.
≼
(In this paper, we actually allow ⪯ to be a partial preorder rather than a partial order, meaning
that it may be the case that x ⪯ x′ and x′ ⪯ x even when x ̸= x′.) As an example, we denote
Ciso = {w : w is a monotonically nondecreasing function of w (x)}.
w0 0
Our focus is the worst-case excess risk with the isotonic constraint:
∆iso(R;B) = sup E [w(X)R(X)]−E [R(X)]
P P
w∈L2(P)
subject to w P ∈ B, w ∈ Ciso. (6)
# ≼
To make this more concrete, the example in the bound (3) corresponds to choosing B = B for
f,ρ
the f-divergence f(x) = xlogx that is related to the KL distance. In particular, the bound (3)
assumed two constraints on the distribution P —first, D (P ∥P) ≤ ρ (which corresponds
target KL target
to assuming (dP /dP) P ∈ B , in our new notation), and second, P ∈ Q (w ) (which is
target # f,ρ target iso 0
expressed by assuming w ∈ Ciso, in our new notation, when we take the partial (pre)order defined
w0
as x ⪯ x′ whenever w (x) ≤ w (x′)).
0 0
3.1 Equivalent formulation
Optimization problems with isotonic constraints may be difficult to tackle both theoretically and
computationally, since the isotonic cone may be challenging to optimize over when working with a
density ratio (i.e., an infinite-dimensional object) despite being a convex constraint. In this section,
wewillshowthatthemaximizationproblem(6)canequivalentlybereformulatedasanoptimization
problem without an isotonic constraint, by drawing a connection to the original (not isotonic) DRL
maximization problem (4).
Given the probability measure P, we will define π as the projection to the isotonic cone Ciso
≼
with respect to L (P):
2
(cid:90)
π(a) = argmin (a(x)−b(x))2dP(x).
b∈Ciso
≼
As L (P) is reflexive, the projection π(a) exists and is unique (up to sets of measure zero) for all
2
a ∈ L (P).
2
With the projection π in place, we are now ready to state our main equivalence result.
Theorem 3.1. For any B, it holds that
∆iso(R;B) ≤ ∆(π(R);B).
11If in addition Condition 2.1 holds, then it holds that
∆iso(R;B) = ∆(π(R);B),
and moreover, ∆iso(R;B) is attainable if and only if ∆(π(R);B) is attainable.
To interpret this theorem, recall from the definition (4) that we have
∆(π(R);B) = sup E [w(X)π(R)(X)]−E [R(X)]
P P
w∈L2(P)
subject to w P ∈ B. (7)
#
Compared with the formulation (6) that defines the isotonic worst-case risk ∆iso(R;B), we see that
this equivalent formulation is able to remove the constraint w ∈ Ciso by replacing R with its isotonic
≼
projection π(R).
3.2 Setting: iso-DRL with estimated density ratio
We now return to the scenario described in (3) in Section 1.3, where we would like to recalibrate
a pretrained density ratio w that estimates the distribution shift dPtarget. As the shape or relative
0 dP
magnitude of w could contain useful information about the true density ratio, we assume that the
0
true density ratio is an isotonic function of w —that is, we assume
0
dP
target
(x) = ϕ(w (x))
0
dP
forsomenondecreasingfunctionϕ,forP-almosteveryx. Equivalently,definingthepartial(pre)order
x ⪯ x′ ⇐⇒ w (x) ≤ w (x′),
0 0
we are assuming that dPtarget ∈ Ciso for this particular partial order. We will denote this specific
dP ≼
cone as Ciso and its isotonic projection as π , and abusing notation, we write ∆iso(R;B,w ) to
w0 w0 0
denote the excess risk for this particular setting, to emphasize the role of w .
0
By Theorem 3.1, if we assume B satisfies Condition 2.1 then we have
∆iso(R;B,w ) = ∆(π (R);B).
0 w0
To understand the projection onto the cone Ciso more straightforwardly, we can derive a further
w0
simplification, with a few more definitions. First, write π to denote the isotonic projection for
1
functions R → R under the measure (w 0) #P, and define a function R(cid:101) : R → R to satisfy
R(cid:101)(w 0(X)) = E P[R(X) | w 0(X)]
P-almost surely. We then have the following simplified equivalence:
12Proposition 3.2. Assume Condition 2.1 holds. We have the equivalence
∆iso(R;B,w 0) = ∆(π 1(R(cid:101))◦w 0;B,w 0),
where we define
∆(R;B,w ) = sup E [(h◦w )(X)R(X)]−E [R(X)]
0 P 0 P
h: h◦w0∈L2(P)
subject to (h◦w ) P ∈ B.
0 #
3.3 A misspecified isotonic constraint
When the true distribution shift does not meet the isotonic constraint exactly, it is of interest to
provide a bound for the obtained worst-case excess risk—does the isotonic constraint provide a
reasonable approximation?
Denotew∗astheunderlyingdensityratiodP /dP and∆∗(R) = E [w∗(X)R(X)]−E [R(X)]
target P P
as the true excess risk. Then, we have the following connections between ∆∗(R) and ∆iso(R;B).
Proposition 3.3. Assume Condition 2.1 holds. If w∗ P ∈ B, then we have
#
∆∗(R) ≤ ∆iso(R;B)+E [(w∗−π(w∗))(X)·(R−π(R))(X)].
P
In particular, if either w∗ ∈ Ciso or R ∈ Ciso, it holds that ∆∗(R) ≤ ∆iso(R;B).
≼ ≼
The result states that when the isotonic constraint is violated, the worst-case excess risk of
iso-DRL will be no worse than the true excess risk minus a gap which can be controlled by the
correlation between (w∗ −π(w∗))(X) and (R−π(R))(X). In particular, if either the risk or the
true density ratio is itself isotonic, the excess risk calculation ∆iso(R;B) will never underestimate
the true risk ∆∗(R).
4 Estimation of worst-case excess risk with isotonic constraint
So far, our focus is on the population level, namely we assume full access to the data distribution
P and the risk function R. In practice, however, we may only access the data distributiion P via
samples from it and we may only (approximately) evaluate the risk function R on the sampled
points, instead of over the entire domain X.
In this section, focusing on the supervised setting, we propose a fully data dependent estimator
fortheworst-caseexcessrisk∆iso(R;B). Moreover,wecharacterizetheestimationerrorfordifferent
choices of B, including the bounds constraint and the f-divergence constraint for the distribution
shift.
However, we may not observe R(X) directly. For example, recall the supervised setting with
covariate shift, in which we observe {(X ,Y )} drawn i.i.d. from P ×P . In this case, we only
i i i≤n Y|X
13observe the risk r : X ×Y → R that approximates the true underlying risk function R in the sense
that
R(X) = E[r(X,Y) | X], almost surely.
Given a truncation level 0 < Γ < +∞, we propose to estimate the worst-case excess risk via the
following optimization problem:
1 (cid:88) 1 (cid:88)
∆(cid:98)i Γso(r;B) := max
n
w ir(X i,Y i)−
n
r(X i,Y i)
w∈L2(P(cid:98)n)
i≤n i≤n
subject to w #P(cid:98)n ∈ B, w ∈ C ≼iso, w ≤ Γ. (8)
Here, P(cid:98)n denotes the empirical distribution of i.i.d. observations {X i}
i≤n
drawn from P. Since the
feasible set is compact, the maximum is attainable, and we denote it by wiso. When Γ = +∞, we
(cid:98)r
write ∆(cid:98)iso(r;B) = ∆(cid:98)iso(r;B).
∞
4.1 Computation: estimation after projection
In view of Theorem 3.1, we may accelerate the computation of (8) via an equivalent optimization
problem without the isotonic constraint.
To be more specific, denote (riso) as the isotonic projection of (r(X ,Y )) with respect to
i i≤n i i i≤n
the empirical distribution P(cid:98)n under the partial order ≼. Then, consider the optimization problem
1 (cid:88) 1 (cid:88)
∆(cid:98)Γ(riso;B) := max
n
w ir iiso−
n
r(X i,Y i)
w∈L2(P(cid:98)n)
i≤n i≤n
subject to w #P(cid:98)n ∈ B, w ≤ Γ. (9)
By Theorem 3.1, we have ∆(cid:98)Γ(riso;B) = ∆(cid:98)i Γso(r;B). Moreover, for concrete choices of B, we can
solve the dual problem of (9) to further improve the computational efficiency. Note that in the
reweighted setting, we can simply apply the isotonic regression for (r(X ,Y )) on (w (X )) to
i i i≤n 0 i i≤n
obtain the projected risk.
4.2 Reduction to noiseless risk
To control the estimation error, we will consider an oracle estimator with perfect knowledge of the
noiseless risk R. That is, we consider the following optimization problem:
1 (cid:88) 1 (cid:88)
∆(cid:98)i Γso(R;B) := max
n
w iR(X i)−
n
r(X i,Y i)
w∈L2(P(cid:98)n)
i≤n i≤n
subject to w #P(cid:98)n ∈ B, w ∈ C ≼iso, w ≤ Γ. (10)
In comparison to (8), the noisy risk r(X ,Y ) is replaced by the noiseless counterpart {R(X )} .
i i i i≤n
Recall our population-level target ∆iso(R;B) defined in (6). It is clear via the triangle inequality
that the estimation error can be decomposed into two parts:
(cid:12) (cid:12)∆(cid:98)iso(r;B)−∆iso(R;B)(cid:12)
(cid:12) ≤
(cid:12) (cid:12)∆(cid:98)iso(r;B)−∆(cid:98)iso(R;B)(cid:12) (cid:12)+(cid:12) (cid:12)∆(cid:98)iso(R;B)−∆iso(R;B)(cid:12)
(cid:12).
Γ Γ Γ Γ
14We first show that the first term can be bounded by the order of n−1/3 and the convergence
does not depend on the specific choice of B.
Theorem 4.1. Assume R is B -bounded and {r(X ,Y ) − R(X )} are independent and sub-
R i i i i≤n
Gaussian with the variance proxy σ2. Then, with probability at least 1−n−1, one has
e
(cid:115)
(cid:12) (cid:12) (cid:12) (cid:12)∆(cid:98)i Γso(r;B)−∆(cid:98)i Γso(R;B)(cid:12) (cid:12) (cid:12)
(cid:12)
≤ nΓ
1/3
48(B Rσ e2log(2n))2/3+96σ e2log n2 1( /2 3n) .
As a result, bounding ∆(cid:98)iso(r;B)−∆iso(R;B) is reduced to bounding the estimation error with
Γ
the noiseless risk R, i.e., controlling the second term (cid:12) (cid:12)∆(cid:98)iso(R;B)−∆iso(R;B)(cid:12) (cid:12).
Γ
4.3 Estimation error with the noiseless risk
In this section, we bound the error ∆(cid:98)iso(R;B) − ∆iso(R;B) induced by the sampling of X. To
Γ
simplify the presentation, we focus on two canonical examples with bounds constraints and f-
divergence constraints. For the population level worst-case excess risk, we also denote ∆iso(R;B) as
Γ
a modification of ∆iso(R;B) with an additional constraint w ≤ Γ. We will start with the analysis
of the error ∆(cid:98)iso(R;B)−∆iso(R;B) and then validate that the bias term ∆iso(R;B)−∆iso(R;B) is
Γ Γ Γ
zero when Γ is sufficiently large.
Recall the definitions of B and B in Section 2. We further denote the function class G =
a,b f,ρ B a,b
{w ∈ L (P) : a ≤ w ≤ b, w ∈ Ciso} and G = G ∪ G , where G = {w ∈ L (P) : w ∈
2 ≼ B f,ρ f,1 f,2 f,1 2
Ciso, w ∈ [0,Γ]} and G = {f ◦w : w ∈ G }. Define the Rademacher complexity of the function
≼ f,2 f,1
class G by
 
1 (cid:88)
R n(G) := E σsup σ ig(Z i),
n
g∈G
i≤n
where {Z } is a sample of size n from P. Then, we have the following theorem for the estimation
i i≤n
error bound of ∆(cid:98)iso(R;B)−∆iso(R;B).
Γ Γ
Theorem 4.2. Assume R is B -bounded. For any Γ > 0 and B ∈ {B ,B } with sup f(t) =
R a,b f,ρ t∈[0,Γ]
B < +∞, there exists a constant C that does not depend on n such that with probability at least
f
1−2n−1, it holds that
(cid:12) (cid:12) (cid:32)(cid:114) (cid:33)
(cid:12) (cid:12) (cid:12)∆(cid:98)i Γso(R;B)−∆i Γso(R;B)(cid:12) (cid:12)
(cid:12)
≤ C lo ngn +R n(G B) . (11)
We provide two concrete examples to make apparent the dependence of the Rademacher com-
plexity on the sample size.
(1) Whend = 1, e.g., inthesettingofdensityratiorecalibrationinSection3.2, bothB andB
a,b f,ρ
are contained in the set of uniformly bounded unimodal functions, then, similar to Chatterjee
and Lafferty (2019), one can show that R (G ) ≲ n−1/2, ignoring logarithmic factors, by
n B
Dudley’s theorem (Dudley, 1967).
15(2) For Rd with a fixed dimension d ≥ 2, by the result in Dutta and Nguyen (2018) and the
one-step discretization, if X ⊆ Rd is bounded, we further have B and B contained in
a,b f,ρ
the set of functions with bounded variation. This implies that R (G ) ≲ n−1/(d+2), ignoring
n B
logarithmic factors, for which we provide more details in Appendix A.5 and A.6.
To conclude, we note that the estimation error in Theorem 4.2 is for the truncated population
worst-case excess risk ∆iso(R;B). We will show that the bias term ∆iso(R;B)−∆iso(R;B) is zero
Γ Γ
for B and B as follows:
a,b f,ρ
• For B a,b, the bound (11) still holds for ∆(cid:98)iso(R;B a,b)−∆iso(R;B a,b) when Γ ≥ b.
• For the f-constrained problem, with sup f(t) = B < +∞, as we will show in Appendix
t∈[0,Γ] f
A.6 that the worst-case excess risk ∆iso(R;B ) is attained at w∗iso ∈ Ciso with ∥w∗iso∥ < ∞
f,ρ f,ρ ≼ f,ρ ∞
almost surely, for which we have the following result.
Corollary 4.3. Under the conditions in Theorem 4.2, if Γ ≥ ∥w∗iso∥ , there exists a constant
f,ρ ∞
C = C (ρ) > 0 that does not depend on n such that with probability at least 1−2n−1,
2 2
(cid:12) (cid:12) (cid:32)(cid:114) (cid:33)
(cid:12) (cid:12) (cid:12)∆(cid:98)i Γso(r;B f,ρ)−∆iso(R;B f,ρ)(cid:12) (cid:12)
(cid:12)
≤ C 2 lo ngn +R n(G B f,ρ) .
Combining the results in Theorem 4.1 and 4.2, it holds that with B = B or B , the estimation
a,b f,ρ
error |∆(cid:98)iso(r;B)−∆iso(R;B)| ≲ n−1/(d+2), where we ignore logarithmic factors.
Γ
5 Numerical simulations
In this section, following the example in Section 1.3, we demonstrate the benefits of iso-DRL in
calibrating prediction sets under covariate shift. In this case, we have D = {(X ,Y )} drawn
train i i i≤N
fromthedatadistributionandthetestsetD
test
= {(X(cid:101)i,Y(cid:101)i)}
i≤M
drawnfromthetargetdistribution
with M ≪ N. We consider both synthetic and real datasets. Our reproducible codes are available
at https://github.com/yugjerry/iso-DRL.
Background. When covariate shift is present, Tibshirani et al. (2019) proposes the weighted
conformal prediction (WCP) that produces a prediction set Cw0 (X) with an estimated density
1−α
ratio w 0, which is valid for distribution P(cid:98) with dP(cid:98) = w
0
·dP and the validity for P
target
is only
guaranteed up to a coverage gap due to the estimation error or potential misspecification in w (Lei
0
and Candès, 2020; Candès et al., 2023; Gui et al., 2023, 2024). In comparison to our approach,
(Cauchois et al., 2020; Ai and Ren, 2024) share similar idea with the generic DRL to adjust the
target level α, but focuses on a different setting with distribution shift on the joint distribution of
(X,Y). More related works on conformal prediction are discussed in Section 6.
16Dataset partition. We set N = 1000 and M = 500 across all experiments. In this section, we
adopt a ratio η ∈ (0,1) and use only covariates of 100×η% samples from both training and test sets
to fit a density ratio w . More concretely, we draw a Bernoulli random variable B ∼ Bern(η) for
0 i
each test data point in D
test
and obtain a partition D
test
= D test,0∪D
test,1
with D
test,j
= {(X(cid:101)i,Y(cid:101)i) |
B
i
= j} =: {(X(cid:101) i(j) ,Y(cid:101) i(j) )} i≤mj. Wealsoconsiderapartitionofthetrainingset: D
train
= D 1∪D 2∪D
3
such that
1{i ∈ D } | D ∼ Bern(η) and 1{i ∈ D } | Dc ∼ Bern(0.5).
1 train 2 1
Denote D = {(X(j) ,Y(j) )} , j = 1,2,3.
j i i i≤nj
Initial density ratio estimation. Using data from D and D , we first construct a classifi-
1 test,1
cation dataset D(cid:101)X = {(Z i,L i)}
i≤n1+m
where

(1)
X , i ≤ n ,
L = 1{i > n }, Z = i 1
i 1 i
(j)
X(cid:101) i−n1, n 1+1 ≤ i ≤ n 1+m 1.
With D(cid:101)X in place, similar to Tibshirani et al. (2019), we fit a logistic regression model and obtain
the estimated probability p(z) for P(L = 1 | Z = z), with which we define
(cid:98)
p(x) n
w (x) = (cid:98) · 1 .
0
1−p(x) m
(cid:98) 1
(Weighted) Split conformal prediction. With data from D , we use Ordinary Least Squares
2
(OLS) as the base algorithm and apply split conformal prediction to obtain the following prediction
intervals for comparison:
• CP: conformal prediction interval C without adjusting for covariate shift;
1−α
• WCP-oracle: weighted conformal prediction interval Cw∗ with true density ratio w∗ =
1−α
dP /dP;
target
• WCP: weighted conformal prediction interval Cw0 with estimated w ;
1−α 0
Estimation of worst-case excess risks. With D = {(X(3) ,Y(3) )} , the observed risks can
3 i i i≤n3
be calculated by
(cid:110) (cid:111)
r = 1 Y(3) ∈/ C (X(3) ) , 1 ≤ i ≤ n .
i i 1−α i 3
We adopt the KL-constraint D (Q||P) ≤ ρ to measure the magnitude of distribution shift, with
KL
which we can obtain the following estimated worst-case excess risk3
1 (cid:88) 1 (cid:88)
∆(cid:98)(α) = max w ir i− r
i
n n
w∈L2(P(cid:98)n)
i≤n i≤n
1 (cid:88) 1 (cid:88)
subject to w = 1, w logw ≤ ρ, 0 ≤ w ≤ Γ. (12)
i i i i
n n
i≤n3 i≤n3
3We choose the upper bound Γ=50 throughout out simulation.
17Given the estimated density ratio w , we run isotonic regression for (r ) on (w (X(3) )) to
0 i i≤n3 0 i i≤n3
obtain the projected risk (riso) , with which we can calculate the worst-case excess risk
i i≤n3
1 (cid:88) 1 (cid:88)
∆(cid:98)iso(α) = max
n
w ir iiso−
n
r
i
w∈L2(P(cid:98)n)
i≤n i≤n
1 (cid:88) 1 (cid:88)
subject to w = 1, w logw ≤ ρ, 0 ≤ w ≤ Γ. (13)
i i i i
n n
i≤n3 i≤n3
Given the worst-case excess risks, we include the following methods into comparison4:
• DRL: CP interval C
1−α
with α
(cid:101)
= max{0,α−∆(cid:98)(α)};
(cid:101)
• iso-DRL-w 0: CP interval with calibrated target level α
iso
= max{0,α−∆(cid:98)iso(α)}.
We will specify other choices of the partial (pre)order ≼ later, in which case (riso) is the isotonic
i i≤n3
projection of (r i)
i≤n3
with the measure P(cid:98)n under ≼.
For each prediction set C introduced in this section, we focus on the coverage rate on D :
test,0
Coverage rate(C,α) =
1 (cid:88) 1(cid:110) Y(cid:101)(0)
∈
C(cid:16) X(cid:101)(0)(cid:17)(cid:111)
.
|D | i i
test,0
i≤|Dtest,0|
5.1 Synthetic dataset
We start with a synthetic example. For the marginal distribution, we set

data distribution P : X ∼ N(0 ,I ),
d d
target distribution P : X ∼ N(µ,I +ζ ·Ω),
target d
√
where d = 5, Ω = 11⊤, and µ = (2/ d)·(1,··· ,1)⊤. The parameter ζ controls the misspecification
level of estimated density ratios via logistic regression. When ζ = 0, the underlying log-density
ratio is linear in the covariates X, thus the estimated density ratio based on logistic regression is
well-specified. When ζ ̸= 0, the underlying density ratio is a function of both µ⊤X and X⊤ΩX,
which cannot be characterized by logistic regression.
For the conditional distribution, we set
(cid:16) (cid:17)
Y | X ∼ N X⊤β +sin(X )+0.2X2,1
1 3
for both training and target distributions.
5.1.1 Results with varying splitting ratio η
We first consider the scenario with an estimated w using η × 100% samples from both P and
0
P . By varying the splitting ratio, we aim to investiigate the robustness of WCP and iso-DRL
target
w.r.t. the accuracy in w . The splitting ratio η varies in {0.1,0.2,0.3,0.4,0.5}, where we denote
0
n = ⌊η·N⌋+⌊η·M⌋asthetotalsamplesizeforestimatingw . Wefixρ = ρ∗ := D (P ||P).
pre 0 KL target
4Morediscussionofthesetwoapproaches,especiallywhythecalibratedconfidencelevelswillguaranteethevalidity,
are shown in Appendix C.1.
18Well-specified w . In Figure 2b, we set ζ = 0 under which w is well-specified. We can see that
0 0
the uncorrected CP exhibits undercoverage due to the mismatch between P and P while the
target
coverage of WCP using w increases to 90% as n increases, in which w becomes more accurate.
0 pre 0
The generic DRL, even with ρ = ρ∗, tends to be conservative and has the widest interval. In
comparison, iso-DRL-w has coverage very close to the target level and the width is even shorter
0
than the oracle weighted approach due to the limited effective sample size of WCP-oracle.
Misspecified w . In Figure 3b with ζ = 1, the density ratio w is misspecified as is shown
0 0
in the plot that D KL(P target||P(cid:98)) does not converge to zero as n
pre
increases. As a result, both
uncorrected and w -weighted approach exhibit undercoverage. Proposed iso-DRL-w has coverage
0 0
slightly above 90% but has interval width close to that of oracle weighted approach, while DRL is
overly conservative.
1.00
0.3
6
0.95
0.2
0.90 4
0.1
0.85 2 CP DRL
WCP iso-DRL-w0
WCP-oracle
0.0 0.80 0
200 400 600 200 400 600 200 400 600
npre npre npre
(a) D KL(P target||P(cid:98)) versus n pre. (b) Comparison with varying n
pre
(ζ =0).
Figure 2: Results with well-specified density ratio (ζ = 0).
1.00
0.6
6
0.95
0.4 0.90 4
0.2 0.85 2 CP DRL
WCP iso-DRL-w0
WCP-oracle
0.0 0.80 0
200 400 600 200 400 600 200 400 600
npre npre npre
(a) D KL(P target||P(cid:98)) versus n pre. (b) Comparison with varying n
pre
(ζ =1).
Figure 3: Results with misspecified density ratio (ζ = 1).
19
)P||tegratP(LKD
)P||tegratP(LKD
etar
egarevoc
etar
egarevoc
htdiw
egareva
htdiw
egareva5.1.2 Results with varying ρ
In this section, we investigate the sensitivity of each approach to the choice of ρ. To demonstrate
the informativeness of the isotonic constraint with respect to w and compare with other choices
0
of the partial (pre)order, we further consider the componentwise order in Rd: a ≼ b if and only if
a ≤ b for all j ∈ [d]. To differentiate two partial orders, let
j j
• iso-DRL-comp: CP interval with calibrated target level under the componentwise order;
• iso-DRL-w : CPintervalwithcalibratedtargetlevelundertheisotonicconstraintwithrespect
0
to w (X).
0
1.00
6
0.95
4
0.90
2 DRL WCP
0.85 iso-DRL-comp WCP-oracle
iso-DRL-w0 true divergence
CP
0.80 0
0 1 2 3 4 0 1 2 3 4
Figure 4: Results with varying ρ: well-specified density ratio (ζ = 0).
1.00
6
0.95
4
0.90
2 DRL WCP
0.85 iso-DRL-comp WCP-oracle
iso-DRL-w0 true divergence
CP
0.80 0
0 1 2 3 4 0 1 2 3 4
Figure 5: Results with varying ρ: misspecified density ratio (ζ = 1).
Fixingη = 0.1,wevaryρin[0.002,4]andtheunderlyingtrueKL-divergenceρ∗ = D (P ||P)
KL target
is marked by the vertical dashed line. The uncorrected CP, WCP with the true density ratio and
estimated density ratio w behave in the same way as shown in the previous section.
0
We can see from both plots that the prediction intervals produced by DRL is very conservative
and is much wider than the oracle interval. In comparison, iso-DRL with the componentwise order
is less sensitive to the change of ρ, but in general, is still conservative compared to the oracle
prediction interval. Moreover, we notice that for iso-DRL-w , when ρ = ρ∗, the width of intervals is
0
20
etar
egarevoc
etar
egarevoc
htdiw
egareva
htdiw
egarevacomparable to the oracle interval in both cases, from which we can see that it can offer a significant
gain in accuracy if the side information is useful.
5.2 Real data: wine quality dataset
Wealsoconsiderarealdataset: thewinequalitydataset(https://archive.ics.uci.edu/dataset/
186/wine+quality). Thedatasetincludes12variablesthatmeasurethephysicochemicalproperties
of wine and we treat the variable quality as the response of interest. The entire dataset consists of
twogroups: thewhiteandredvariantsofthePortuguese“VinhoVerde” wine, whichareunbalanced
(1599 data points for the red wine and 4898 data points for the white wine). The subset of red wine
is treated as the test dataset and that of white wine is viewed as the training set. All variables are
nonnegative and we scale each variable by its largest value such that the entries are bounded by 1.
10.0 1.00 6
7.5
5.0 0.95 4
2.5
0.0 0.90 2 DRL
2.5
iso-DRL-w0
CP
WCP
5.0 0.85 0
5 0 5 10 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Classification-based log-density ratio
(a) Log-density ratio estima- (b) Comparison with varying ρ (split ratio η =0.02).
tion: KDE versus logistic re-
gression.
Figure 6: Results for wine quality dataset.
We first fit a kernel density estimator (Gaussian kernel with a bandwidth suggested by cross-
validation) using the entire dataset as a proxy of the oracle density ratio. Figure 6a plots this
against the log-density ratio obtained from logistic regression fitted on 2% of the dataset. It can be
seen that the two density ratios exhibit an isotonic trend. This motivates us to consider the isotonic
constraint with respect to w .
0
To assess the performance of the proposed approach, the same procedure with the synthetic
dataset is conducted with the split ratio η = 0.02 for estimating w . We consider the uncertainty
0
set of distribution shifts defined by KL-divergence and choose ρ from 100 uniformly located grid
points in [0.005,2]. In Figure 6b, similar to the performance in Section 5.1, DRL tends to be
conservative: the coverage rate quickly approaches 1 while ρ is still below 0.1. Moreover, the widths
of intervals are nearly three times those produced by iso-DRL-w . In the meantime, iso-DRL-w
0 0
captures the approximate isotonic trend in Figure 6a and achieves valid coverage by recalibrating
the weighted approach. The key message is that in the real data case, even when there is no oracle
information for selecting ρ and the isotonic trend is not exact, the proposed iso-DRL-w with the
0
21
oitar
ytisned-gol
desab-EDK
etar
egarevoc
htdiw
egarevaisotonic constraint with respect to the pre-fitted density ratio is robust to the selection of ρ. More
details of this simulation is shown in Appendix C.
6 Additional related work
In this section, we discuss some additional related work including literature on transfer learning,
DRL, sensitive analysis, shape-constrained learning, and conformal prediction.
Transfer learning. Transferlearningisrooteddeeplyinnowadaysscientificresearchfromvarious
fields, which are extensively discussed in Hu et al. (2019); Hu and Lei (2020); Mei et al. (2010);
Sun and Hu (2016); Turki et al. (2017); Weng et al. (2020), etc. In the supervised setting where we
have (X,Y) ∈ X ×Y, transfer learning is usually categorized into domain adaptation and inductive
transfer learning (Redko et al., 2020).
Domain adaptation focuses on the scenario with covariate shift, where the conditional distribu-
tion of Y | X is assumed to be unchanged. From the theoretical side, the performance of machine
learning models including hardness results is analyzed in Ben-David et al. (2010); Ben-David and
Urner (2012, 2013); Johansson et al. (2019); Zhao et al. (2019a); Pathak et al. (2022); Pathak and
Ma (2024), etc. The covariate shift assumption is further relaxed in Hanneke and Kpotufe (2019) to
study the value of target data in adaptation. To implement efficient predictions, weighted methods
are adopted as the first trial to draw P closer to Q after re-weighting the labeled samples (Cortes
et al., 2008; Gretton et al., 2009; Ma et al., 2023; Ge et al., 2023). Another attempt is to require a
small number of labeled target samples, which can be feasible in reality and related works include
Chen et al. (2011); Chattopadhyay et al. (2013); Yang et al. (2012), etc.
For inductive transfer learning, the marginal distribution of X is assumed to be the same for
training and target distributions. In the regression setting, the performance of the least square
estimator with side information from the target domain is studied in Bastani (2021). The minimax
theorem is further presented for nonparametric classification in Cai and Wei (2019). In the high-
dimensional case, Li et al. (2021) considers transfer learning with Lasso, and Tian and Feng (2021)
extends transfer learning with generalized linear models.
Distributionally robust learning (DRL). When no knowledge of the target distribution is
assumed, the DRL framework seeks the worst-case performance in an uncertainty set of distribu-
tion shifts (Ben-Tal and Nemirovski, 1998; El Ghaoui and Lebret, 1997; El Ghaoui et al., 1998).
Different choices of the uncertainty set are also studied in the literature: in one line of research, the
distribution shift is measured in terms of the optimal transport discrepancy (Shafieezadeh Abadeh
et al., 2015; Blanchet and Murthy, 2019; Blanchet et al., 2019; Esfahani and Kuhn, 2015); another
line of research adopts the uncertainty set defined by f-divergence (Duchi et al., 2021; Namkoong
and Duchi, 2017; Duchi and Namkoong, 2018; Cauchois et al., 2020; Ai and Ren, 2024).
Further constraints on the uncertainty set as the improvement of DRL are explored in Duchi
et al. (2019); Setlur et al. (2023); Esteban-Pérez and Morales (2022); Liu et al. (2023) and in an
22earlier work, Popescu (2007) considers certain families of uncertainty sets in which distributions
preserve similar structural properties. A recent work Wang et al. (2023) considers the constraint
that the unseen target distribution is a weighted average of data distribution from multiple sources.
Besides, Shapiro and Pichler (2023) proposes the conditional distributional robust optimization
to incorporate side information. Some recent works based on the DRL framework also focus on
the quantification of stability against distribution shift, among which Gupta and Rothenhäusler
(2021);Namkoongetal. (2022); Rothenhäuslerand Bühlmann(2023)quantifythesmallest possible
divergence from P (e.g. D (Q||P)) with a fixed lower bound of the worst-case risk, which can be
KL
viewed as a dual formulation of (1).
It is also worth mentioning related works addressing distribution shift based on multicalibration
(Hébert-Johnsonetal.,2018;Dengetal.,2023;Kimetal.,2022), whichguaranteestheperformance
(e.g. coverage in uncertainty quantification) within certain function classes.
Sensitivity analysis. Sensitivity analysis is closely related to DRL but is particularly widely
studied in the field of causal inference (Cornfield et al., 1959; Rosenbaum, 1987; Tan, 2006; Ding
and VanderWeele, 2016; Zhao et al., 2019b; De Bartolomeis et al., 2023) with the goal of evaluating
the effect of unmeasured confounders and relaxing untestable assumptions. Sensitivity models can
be viewed as a specific example of constraints on distribution shift. For example, if we consider a
treatment T ∈ {0,1}, the marginal Γ-selection model (Tan, 2006) implies that
1 dP
Y(1)|X,T=0
≤ ≤ Γ,
Γ dP
Y(1)|X,T=1
whichimposestheboundsconstraintonthedistributionshiftfromthedatadistributionP
Y(1)|X,T=1
to the counterfactual P . Recent works investigate the performance of estimation, predic-
Y(1)|X,T=0
tion, and inference under the sensitivity model from the perspective of DRL: Yadlowsky et al.
(2018); Jin et al. (2022, 2023); Sahoo et al. (2022).
Statistical learning with shape constraints. Shape constraints, including monotonicity, con-
vexityandlog-concavityconstraints, areusuallyimposedinvariousappliedfields(Grenander,1956;
Schell and Singh, 1997; Matzkin, 1991), out of which monotonicity (or isotonic) constraint is most
common which is closely related to the widely adopted smoothness assumption. The nonstan-
dard asymptotic behavior of estimator with the isotonic constraint is identified in Rao (1969), since
whichthepropertiesofisotonicregressionarewellstudiedintheliterature(Brunketal.,1957,1972;
Zhang, 2002; Han et al., 2019; Yang and Barber, 2019; Durot and Lopuhaä, 2018; Bogdan et al.,
2015; Su and Candes, 2016). Moreover, the isotonic constraint is also widely applied to calibration
for distributions in regression and classification settings (Zadrozny and Elkan, 2002; Niculescu-Mizil
and Caruana, 2012; van der Laan et al., 2023; Henzi et al., 2021).
Conformal prediction. Conformal prediction proposed by Vovk et al. (2005); Shafer and Vovk
(2008) provides a framework for distribution-free uncertainty quantification, which constructs confi-
denceintervalsthatarevalidwithexchangeabledatafromanyunderlyingdistributionandwithany
23“black-box” algorithm. When covariate shift is present between training and target distributions,
Tibshirani et al. (2019) firstly introduces the notion of weighted exchangeability and the weighted
conformal prediction approach to maintain validity with the oracle information of the density ratio.
However, with the estimated density ratio, the validity of WCP only holds up to a coverage gap
(Lei and Candès, 2020; Candès et al., 2023; Gui et al., 2024), built upon which, Jin et al. (2023)
further establishes a robust guarantee via sensitivity analysis. Besides the weighted approaches,
there are other solutions in the literature: Cauchois et al. (2020); Ai and Ren (2024) address the
issue of joint distribution shift via the DRL; Qiu et al. (2023); Yang et al. (2024); Chen and Lei
(2024) formulate the covariate shift problem within the semiparametric/nonparametric framework
and utilize the doubly-robust theory to correct the distributional bias.
7 Discussion
Inthispaper,wefocusondistributionalrobustriskevaluationwiththeisotonicconstraintondensity
ratio. We provide an efficient approach to solve the shape-constrained optimization problem via an
equivalent reformulation. Estimation error bounds for the worst-case excess risk are also provided
when only noisy observations of the risk function can be accessed. To conclude, we provide further
discussions on the proposed iso-DRL framework and single out several open questions.
Isotonic constraint as regularization on distribution shift. Our theoretical and empirical
results also motivate problems for further study. The isotonic constraint on the density ratio is
relatedtotheregularizationondistributionshifts. Theworst-casedensityratioforthegenericDRL
willalwaysalignwiththeriskfunctionevenwhentheriskishighlynon-smooth, whichresultsinthe
over-conservativeness. The isotonic constraint, e.g. with respect to a pre-fitted density ratio, tries
to avoid the over-pessimistic choices of the density ratio. This shares very similar spirits with the
high-dimensionalstatisticallearningwhereregularization/inductivebiascanimprovegeneralization.
More broadly, how to explicitly quantify the validity-accuracy tradeoff under distribution shift is
an important problem.
Excess risk can also be interpreted from the perspective of stability against distribution shift
(Lam, 2016; Namkoong et al., 2022). Suppose we have a fixed budget ε ≪ 1 for the excess risk,
it will be of interest to characterize the largest tolerance of distribution shift such that the excess
risk is under control. Taking the f-constrained problem as an example, if we aim at the budget
∆ (R;B ) ≤ ε ≪ 1, then only the infinitesimal ρ that is quadratic in ε will be allowed (Lam, 2016;
ρ f,ρ
Duchi and Namkoong, 2018; Blanchet and Shapiro, 2023):
f′′(1)
ρ ≤ ·ε2+o(ε2).
2Var(R(X))
However, with the additional isotonic constraint on the density ratio and the same budget ε, we
have a relaxed upper bound for the magnitude of distribution shift:
f′′(1)
ρ ≤ ·ε2+o(ε2).
2Var(π(R)(X))
24This upper bound drives the following findings:
1. When side information of the underlying distribution shift is provided, e.g. the shape con-
traints of the density ratio, risk evaluation will be less sensitive to the hyperparameters de-
scribing the uncertainty set (e.g. ρ), thus is more robust with the presence of distribution
shift.
2. Moreover, the denominator Var(π(R)(X)) also implies that when the shape of the uncer-
tainty set is well-designed such that the projected risk π(R)(X) has small variance, then the
out-of-sample risk within the uncertainty set will be more distributionally robust. Thus, it
remains an open question on how to construct the variance-reduction projection and design
the uncertainty set based on noisy observations of the risk function.
From risk evaluation to distributionally robust optimization. Different from risk evalua-
tion, distributionally robust optimization (DRO) focuses on the optimization problem with a loss
function ℓ (x):
θ
θ(cid:98)∈ argmin sup E
Q
ℓ θ(X).
θ∈Θ Q∈Q
Under smoothness conditions on ℓ θ, asymptotic normality for θ(cid:98) is established in the literature
(Duchi and Namkoong, 2018). The DRO framework is shown to regularize θ(cid:98)in terms of variance
penalization(Lam,2016;DuchiandNamkoong,2018)orexplicitnormregularization(Blanchetand
Murthy, 2019). Then, for the DRO solution θ(cid:98)iso with an additional isotonic constraint on dQ/dP,
understanding the role of the isotonic constraint via the asymptotics of θ(cid:98)iso will be an open question
to be explored.
Extension to the optimal transport discrepancy. Finally, we should note that there is a
rich literature on DRL with the optimal transport discrepancy, in which case the distribution shift
cannot be simply represented by density ratios (Shafieezadeh Abadeh et al., 2015; Blanchet and
Murthy, 2019; Blanchet et al., 2019; Esfahani and Kuhn, 2015). Suppose we have side information
about the functional σ(P,P ) of two distributions, of which w = dP /dP is an example,
target 0 target
it will be an open question regarding how to utilize σ(P,P ) in guiding the constraint on the
target
candidate distributions or the choice of the cost function in the optimal transport discrepancy.
Acknowledgement
R.F.B. was supported by the Office of Naval Research via grant N00014-20-1-2337, and by the
NationalScienceFoundationviagrantDMS-2023109. C.M.waspartiallysupportedbytheNational
Science Foundation via grant DMS-2311127.
25Contents
A Proofs of main results 26
A.1 Proof of Proposition 2.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
A.2 Proof of Theorem 3.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
A.3 Proof of Proposition 3.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
A.4 Proof of Theorem 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
A.5 Proof of Theorem 4.2 with B = B . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
a,b
A.6 Proof of Theorem 4.2 with B = B and Corollary 4.3 . . . . . . . . . . . . . . . . . 35
f,ρ
B Additional technical proofs 39
B.1 Proof of Proposition A.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
B.2 Proof of Lemma A.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40
B.3 Proof of Lemma A.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
B.4 Proof of Lemma A.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
B.5 Proof of results in Section 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
B.6 Extension of Lemma 3.1 to general cones . . . . . . . . . . . . . . . . . . . . . . . . . 43
C Additional simulation results 44
C.1 Validity guarantee of the DRL with true excess risk . . . . . . . . . . . . . . . . . . . 44
C.2 Wine quality simulation: a proxy of the oracle KL-divergence . . . . . . . . . . . . . 44
A Proofs of main results
A.1 Proof of Proposition 2.2
For any w such that w P ∈ B, we define g as a measurable function satisfying that
#
g(R(X)) = E[w(X) | R(X)], almost surely,
for which we have
E [w(X)R(X)] = E [g(R(X))R(X)].
P P
Since w P ∈ B, combining Condition 2.1 and Jensen’s inequality, for any convex function ψ, we
#
have
E {ψ(g(R(X)))} = E{ψ(E[w(X) | R(X)])} ≤ E [ψ(w(X))],
P P
which implies (g◦R) P ∈ B.
#
Denote F and F as the cumulative distribution functions of g(R(X)) and R(X), respectively.
1 2
Let U ∼ Unif([0,1]). Then, we have F−1(U) =d g(R(X)) and F−1(U) =d R(X), where F−1 is the
1 2 k
generalized inverse of F , k = 1,2. Moreover, F−1 is nondecreasing and
k 1
g(F−1(U)) =d g(R(X)) =d F−1(U),
2 1
26which implies that F−1 is the rearrangement of g◦F−1. By inequality (378) in Hardy et al. (1952),
1 2
we have
E [g(R(X))R(X)] = E(cid:2) g(F−1(U))F−1(U)(cid:3) ≤ E(cid:2) F−1(U)F−1(U)(cid:3) .
P 2 2 1 2
Define ϕ as a measurable function such that
ϕ(t) = E (cid:2) F−1(U) | F−1(U) = t∗(cid:3) , t∗ = sup{s ∈ range(F−1) | s ≤ t}.
P 1 2 2
Since ϕ(R(X)) =d g(R(X)), we have (ϕ◦R) P ∈ B. For any t ≤ t and t ,t ∈ range(F−1), denote
# 1 2 1 2 2
E = {u | F−1(u) = t } and E = {u | F−1(u) = t }. Since both F−1 and F−1 are nondecreasing,
1 2 1 2 2 2 1 2
for any u ∈ E and u ∈ E , we have u ≤ u and F−1(u ) ≤ F−1(u ), thus ϕ(t ) ≤ ϕ(t ). Then,
1 1 2 2 1 2 1 1 1 2 1 2
on the entire domain, ϕ(t) is nondecreasing in t.
We note that
E(cid:2) F−1(U)F−1(U)(cid:3) = E(cid:2)E(cid:2) F−1(U) | F−1(U)(cid:3) F−1(U)(cid:3)
1 2 1 2 2
= E(cid:2) ϕ(F−1(U))F−1(U)(cid:3) = E [ϕ(R(X))R(X)].
2 2 P
Then, we already show that for any w P ∈ B, there exists a nondecreasing function ϕ such that
#
E [w(X)R(X)] ≤ E [ϕ(R(X))R(X)] and (ϕ◦R) P ∈ B.
P P #
Therefore, the worst-case excess risk ∆(R;B) can be equivalently written as
∆(R;B) = sup E [ϕ(R(X))R(X)]−E [R(X)]
P P
ϕ:R→R
+
subject to (ϕ◦R) P ∈ B, ϕ is nondecreasing.
#
A.2 Proof of Theorem 3.1
This section presents the proof of a general result where the attainability of excess risks is not
assumed. We start from the case where ∆iso(R;B) is attained at w∗iso and ∆(π(R);B) is attained
at w. Then, we will show that the attainability of one implies the other and further show the
(cid:101)
equivalence without attainability.
Step 1: equivalence when both ∆iso(R;B) and ∆(π(R);B) are attained. To ease the no-
tation, we denote ⟨a,b⟩ = (cid:82) a(x)b(x)dP(x) for any a, b ∈ L (P). By Proposition 2.2, we have
P X 2
w ∈ Ciso. By the optimality of w∗iso and w, we have
(cid:101) ≼ (cid:101)
⟨w,R⟩ ≤ ⟨w∗iso,R⟩ , ⟨w∗iso,π(R)⟩ ≤ ⟨w,π(R)⟩ .
(cid:101) P P P (cid:101) P
Further, by the properties of the projection onto a convex and closed cone, we have
⟨w∗iso−π(R),R−π(R)⟩ = ⟨w∗iso,R−π(R)⟩ ≤ 0,
P P
27which implies that
⟨w∗iso,R⟩ ≤ ⟨w∗iso,π(R)⟩ .
P P
Then, we obtain the chain of inequalities:
⟨w,R⟩ ≤ ⟨w∗iso,R⟩ ≤ ⟨w∗iso,π(R)⟩ ≤ ⟨w,π(R)⟩ . (14)
(cid:101) P P P (cid:101) P
We should note that it always holds that ⟨w∗iso,R⟩ ≤ ⟨w,π(R)⟩ regardless the property of w,
P (cid:101) P (cid:101)
which proves the first claim of the theorem.
Moreover, by Proposition 2.2, let w = g(π(R)) where g(t) is nondecreasing in t. Then, we can
(cid:101)
rewrite
⟨w,R⟩ −⟨w,π(R)⟩ = ⟨g(π(R)),R−π(R)⟩ .
(cid:101) P (cid:101) P P
Our next goal is to show that ⟨g(π(R)),R−π(R)⟩ = 0.
P
Representation of the isotonic cone with sigma lattice. For X ⊆ Rd with d ≥ 1, consider
any partial order ≼ on X. Recall the isotonic cone
Ciso = {g | g(x) ≤ g(x′) for all x ≼ x′}.
≼
Consider a class of subsets A ⊆ Rd such that for each A ∈ A, we have {x | a ≼ x} ⊆ A for any
a ∈ A. Denote the set
R(A) = {g | {x | g(x) > τ} ∈ A for all τ ∈ R}.
OnecanverifythatAisclosedundercountableunionandcountableintersectionandforanyg ∈ Ciso
≼
and any τ ∈ R, we have
{x | g(x) > τ} ∈ A =⇒ Ciso ⊆ R(A).
≼
On the other hand, for any g ∈ R(A). Suppose g ∈/ Ciso, i.e. there exists x ≼ x such that
≼ 1 2
g(x ) > g(x ). By choosing τ = (g(x )+g(x ))/2, we have A := {x | g(x) > τ} ∈ A and x ∈ A ,
1 2 1 2 τ 1 τ
x ∈/ A . However, since x ≼ x , we have {x | x ≼ x} ⊈ A which draws the contradiction to
2 τ 1 2 1 τ
A ∈ A. Hence, we have verified that Ciso = R(A).
τ ≼
Orthogonality of isotonic projection. Based on the representation of the isotonic cone Ciso
≼
with sigma lattice R(A), by Brunk (1963) (Theorem 1) and Brunk (1965) (Corollary 3.1), we have
⟨h(π(R)),R−π(R)⟩ = 0,
P
whichholdsforanymeasurablefunctionhsuchthath(π(R)) ∈ L (P). Particularly,sinceg(π(R)) =
2
w ∈ L (P), we have ⟨g(π(R)),R−π(R)⟩ = 0.
(cid:101) 2 P
Consequently, each inequality in the chain (14) becomes equality and we obtain
⟨w,π(R)⟩ = ⟨w∗iso,R⟩ ,
(cid:101) P P
which implies that ∆iso(R;B) = ∆(π(R);B).
28Step2: attainabilityofoneof∆iso(R;B)or∆(π(R);B)impliestheother. Considerthecase
where ∆(π(R);B) is attained at w but the supremum in ∆iso(R;B) cannot be achieved. Suppose
(cid:101)
the equivalence does not hold, i.e. there exists ε > 0 such that
⟨w,R⟩ = ⟨w,π(R)⟩ ≤ ∆iso(R;B)−ε.
(cid:101) P (cid:101) P
Here the first equality is based on the orthogonality implied by Brunk (1963) (Theorem 1) and
Brunk (1965) (Corollary 3.1).
By the definition of supremum, there exists w∗ ∈ Ciso and (w∗) P ∈ B such that ⟨w∗,R⟩ >
ε ≼ ε # ε P
∆iso(R;B)−ε, which yields
⟨w,R⟩ < ⟨w∗,R⟩ .
(cid:101) P ε P
Moreover, by the property of projection onto a closed set and the optimality of w, we obtain a
(cid:101)
similar chain of inequalities in Step 1:
⟨w,R⟩ < ⟨w∗,R⟩ ≤ ⟨w∗,π(R)⟩ ≤ ⟨w,π(R)⟩ .
(cid:101) P ε P ε P (cid:101) P
Similarly, by Proposition 2.2 and Brunk (1963, 1965), we have ⟨w,R−π(R)⟩ = 0, thus
(cid:101) P
⟨w,R⟩ = ⟨w∗,R⟩ > ∆iso(R;B)−ε,
(cid:101) P ε P
which draws the contradiction.
Ontheotherhand,suppose∆iso(R;B)isattainedatw∗iso ∈ Cisobutthesupremumin∆(π(R);B)
≼
cannot be achieved. In this case, suppose the equivalence does not hold, i.e. there exits ε > 0 such
that
(cid:12) (cid:12)⟨w∗iso,R⟩ P −∆(π(R);B)(cid:12) (cid:12) ≥ ε.
By the property of projection ⟨w∗iso,R⟩ ≤ ⟨w∗iso,π(R)⟩ and the optimality ⟨w∗iso,π(R)⟩ ≤
P P P
∆(π(R);B), we further have
⟨w∗iso,R⟩ ≤ ∆(π(R);B)−ε.
P
BythedefinitionofsupremumtogetherwiththeequivalentformulationinProposition2.2, there
exists w = g ◦R, where g (t) is nondecreasing in t, such that
(cid:101)ε ε ε
⟨w ,π(R)⟩ > ∆(π(R);B)−ε.
(cid:101)ε P
By the property of projection and the optimality of w∗iso, we again obtain
⟨w ,R⟩ ≤ ⟨w∗iso,R⟩ ≤ ⟨w∗iso,π(R)⟩ < ⟨w ,π(R)⟩ .
(cid:101)ε P P P (cid:101)ε P
Proposition 2.2 and Brunk (1963, 1965) gives us ⟨g ◦R,R−π(R)⟩ = 0, which yields ⟨w∗iso,R⟩ =
ε P P
⟨w ,π(R)⟩ > ∆(π(R);B)−ε and draws the contradiction.
(cid:101)ε P
29Step 3: none of ∆iso(R;B) and ∆(π(R);B) is attained. For any ε > 0, there exists w = g ◦R
(cid:101)ε ε
where g (t) is nondecreasing in t (Proposition 2.2) such that
ε
⟨w ,π(R)⟩ ≥ ∆(π(R);B)−ε.
(cid:101)ε P
As ⟨w ,R⟩ < ∆iso(R;B), for η > 0 that is small enough, there exists w∗ ∈ Ciso such that
(cid:101)ε P η ≼
⟨w ,R⟩ ≤ ∆iso(R;B)−η < ⟨w∗,R⟩ < ∆iso(R;B).
(cid:101)ε P η P
By Brunk (1963, 1965), we have ⟨g ◦R,π(R)⟩ = ⟨g ◦R,R⟩ . Moreover, by the property of the
ε P ε P
projection, we have ⟨w∗,R⟩ ≤ ⟨w∗,π(R)⟩ ≤ ∆(π(R);B). Combining the pieces above yields
η P η P
∆(π(R);B)−ε ≤ ∆iso(R;B) ≤ ∆(π(R);B)+η.
Since ε and η are arbitrary, letting ε,η → 0 yields ∆iso(R;B) = ∆(π(R);B).
A.3 Proof of Proposition 3.3
Wefirstpresentthefollowinglemma, whichisimpliedbytheclosednessofB underconvexordering.
Lemma A.1. Assume B satisfies Condition 2.1. The set B is closed under the isotonic projection
such that for any w P ∈ B, it holds π(w) P ∈ B.
# #
Recall that w∗ is the underlying density ratio dP /dP. Suppose ∆iso(R;B) is attained at
target
w∗iso ∈ Ciso suchthat(w∗iso) P ∈ B. SinceB isclosedunderπ byLemmaA.1,wehaveπ(w∗) ∈ Ciso
≼ # ≼
and π(w∗) P ∈ B. Then, by optimiality of w∗iso, we have
#
∆iso(R;B) = E [w∗iso(X)R(X)]−E [R(X)]
P P
≥ E [π(w∗)(X)R(X)]−E [R(X)],
P P
which yields
∆iso(R;B) ≥ E [w∗(X)R(X)]−E [R(X)]
P P
+{E [π(w∗)(X)R(X)]−E [w∗(X)R(X)]}
P P
= ∆∗(R)−E [(w∗(X)−π(w∗)(X))R(X)].
P
In addition, by the property of the projection onto a convex and closed cone, we have
E [(w∗(X)−π(w∗)(X))π(R)(X)] ≤ 0,
P
which further yields
∆∗(R) ≤ ∆iso(R;B)+E [(w∗(X)−π(w∗)(X))R(X)]−E [(w∗(X)−π(w∗)(X))π(R)(X)]
P P
= ∆iso(R;B)+E [(w∗(X)−π(w∗)(X))(R(X)−π(R)(X))].
P
In particular, if either w∗ ∈ Ciso or R ∈ Ciso, it holds that ∆∗(R) ≤ ∆iso(R;B).
≼ ≼
30A.3.1 Proof of Lemma A.1
According to Condition 2.1, for any w such that w P ∈ B, in order to show π(w) P ∈ B, it suffices
# #
to show that for any convex function ψ, we have E [ψ(w(X))] ≥ E [ψ(π(w)(X))].
P P
To see this, recall that, with a convex function ψ, the expectation of the Bregman divergence is
nonnegative, i.e.
E (cid:8) ψ(w(X))−ψ(π(w)(X))−[ψ′(π(w))(w−π(w))](X))(cid:9) ≥ 0.
P
Moreover, due to the orthogonality property mentioned before in (Brunk, 1963, 1965), it holds that
E (cid:8) [ψ′(π(w))(w−π(w))](X))(cid:9) = 0.
P
Consequently, we obtain
E [ψ(π(w)(X))] ≤ E [ψ(w(X))],
P P
which implies π(w) P ∈ B.
#
A.4 Proof of Theorem 4.1
The noiseless risk R cannot be accessed, and instead, we observe {r = r(X ,Y )} as the noisy
i i i i≤n
observation. Denote r = (r i) i≤n, R(cid:98) = (R(X i)) i≤n, and π
n
as the isotonic projection in Rn. To ease
the notation, define a
i
= [π n(R(cid:98))]
i
and (cid:101)a
i
= [π n(r)] i.
We abuse the notation and denote w = wiso as the maximizer to (10) and w = wiso as the
(cid:98) (cid:98)R,Γ (cid:101) (cid:98)r,Γ
maximizer to (8), which are both attainable due to the compactness of a truncated feasible set.
Then,
1 (cid:88) 1 (cid:88)
∆(cid:98)i Γso(r;B)−∆(cid:98)i Γso(R;B) =
n
w (cid:98)ia i−
n
w (cid:101)i(cid:101)a
i
i≤n i≤n
1 (cid:88) 1 (cid:88)
= w (a −a )+ (w −w )a .
(cid:98)i i (cid:101)i (cid:98)i (cid:101)i (cid:101)i
n n
i≤n i≤n
We can also rewrite
1 (cid:88) 1 (cid:88)
∆(cid:98)i Γso(r;B)−∆(cid:98)i Γso(R;B) =
n
w (cid:101)i(a i− (cid:101)a i)+
n
(w (cid:98)i−w (cid:101)i)a i.
i≤n i≤n
Moreover, by optimality,
1 (cid:88) 1 (cid:88) 1 (cid:88)
(w −w )a = w a − w a ≤ 0,
(cid:101)i (cid:98)i i (cid:101)i i (cid:98)i i
n n n
i≤n i≤n i≤n
1 (cid:88) 1 (cid:88) 1 (cid:88)
(w −w )a = w a − w a ≥ 0.
(cid:101)i (cid:98)i (cid:101)i (cid:101)i(cid:101)i (cid:98)i(cid:101)i
n n n
i≤n i≤n i≤n
Consequently, we have the lower and upper bounds
1 (cid:88) 1 (cid:88)
n
w (cid:98)i(a i− (cid:101)a i) ≤ ∆(cid:98)i Γso(r;B)−∆(cid:98)i Γso(R;B) ≤
n
w (cid:101)i(a i− (cid:101)a i).
i≤n i≤n
31Then, the error term can be bounded by
(cid:12) (cid:12)∆(cid:98)iso(r;B)−∆(cid:98)iso(R;B)(cid:12)
(cid:12) ≤
max{∥w (cid:98)∥ ∞,∥w (cid:101)∥ ∞} (cid:88)
| (cid:101)a i−a i| ≤
Γ
∥ (cid:101)a−a∥ 1.
n n
i≤n
Recall the assumption that e (X ) = r(X ,Y )−R(X ) is independent and sub-Gaussian with the
i i i i i
variance proxy bounded by σ . Theorem 5 in Yang and Barber (2019) provides an upper bound for
e
the ℓ distance between a and a which further gives that, with the probability at least 1−1/n,
2 (cid:101)
(cid:115)
1 (cid:88) 1 log2(2n)
|a −a | ≤ 48(B σ2log(2n))2/3+96σ2 ,
n
(cid:101)i i
n1/3
R
n1/3
i≤n
which finishes the proof.
Hardnessofestimationwithoutisotonicconstraints. Weshouldnotethattheconcentration
bound in Theorem 4.1 with the presence of the noisy risk is not possible without the isotonic
constraint. To see this, consider a counterexample with the risk function R(x) ∈ [0,1] and the noisy
risk r = Bern(R(X )) independently. We consider the bound-constraint B with 0 ≤ a ≤ 1 ≤ b.
i i a,b
To estimate the excess risk without the isotonic constraint, consider the optimization problem
1 (cid:88) 1 (cid:88)
∆(cid:98)(r;B a,b) = max w ir i− r
i
subject to w #P(cid:98)n ∈ B a,b,
n n
w∈L2(P(cid:98)n)
i≤n i≤n
for which we have the following proposition.
Proposition A.2. Assume 0 < E [R(X)] < 1 and min{1−a,b−1} > 0. Then, there exists a
P
strictly positive constant c > 0 depending on E [R(X)] and a,b such that
P
∆(cid:98)(r;B a,b)−0 ≥ c·E P[R(X)] > 0 with probability at least 1−n−1.
We defer the proof to Section B.1.
A.5 Proof of Theorem 4.2 with B = B
a,b
Define the following optimization problems (0 < a < 1 < b), where we leave out the dependence on
R in notations for simplicity:
1 (cid:88) 1 (cid:88)
∆(cid:101)[a,b](γ) := max
n
w(X i)R(X i)−
n
r(X i,Y i)
w∈L2(P(cid:98)n)
i≤n i≤n
1 (cid:88)
subject to w(X ) ≤ 1+γ, a ≤ w ≤ b,
i
n
i≤n
w ∈ Ciso. (15)
≼
∆ (γ) := sup E [w(X)R(X)]−E [R(X)]
[a,b] P P
w∈L2(P)
subject to E [w(X)] ≤ 1+γ, a ≤ w ≤ b,
P
w ∈ Ciso. (16)
≼
32Denote the maximizer to (15) by w(x,γ) and the maximizer to (16) by w∗(x,γ). We start with the
(cid:98)
following lemma.
Lemma A.3. Define ∆(cid:101)∗ [a,b](γ) by replacing the inequality condition on n−1(cid:80) i≤nw(X i) with the
equality condition and ∆∗ (γ) by replacing the inequality condition on E [w(X)] with the equality
[a,b] P
condition. Then,
∆∗ [a,b](γ) = ∆ [a,b](γ), ∆(cid:101)∗ [a,b](γ) = ∆(cid:101)[a,b](γ).
Particularly, we have
∆iso(R;B a,b) = ∆ [a,b](0), ∆(cid:98)iso(R;B a,b) = ∆(cid:101)[a,b](0).
Step 1: Bounding the deviation. Bythelawoflargenumbers,thereexistsauniversalconstant
C such that with probability at least 1−n−1,
1
(cid:12) (cid:12) (cid:114)
(cid:12) (cid:12)1 (cid:88) w∗(X i,0)−E P [w∗(X,0)](cid:12) (cid:12) ≤ C 1 logn ,
(cid:12)n (cid:12) n
i≤n
(cid:12) (cid:12) (cid:114)
(cid:12) (cid:12)1 (cid:88) w∗(X i,0)R(X i)−E P [w∗(X,0)R(X)](cid:12) (cid:12) ≤ C 1 logn .
(cid:12)n (cid:12) n
i≤n
which implies that, with γ := C (cid:112) logn/n, with probability at least 1 − n−1, the density ratio
n 1
w∗(x,−γ n) is feasible for (15) with γ = 0. Thus, we have the lower bound for ∆(cid:101)[a,b](0):
∆(cid:101)[a,b](0) ≥ ∆ [a,b](−γ n)−γ n.
On the other side, denote G = {w ∈ L (P) | a ≤ w ≤ b, w ∈ Ciso}. For the maximizer
B 2 ≼
a,b
w(x,0), we have
(cid:98)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)E P[w (cid:98)(X,0)]−1(cid:12) (cid:12)−(cid:12) (cid:12)1 (cid:88) w (cid:98)(X i,0)−1(cid:12) (cid:12) ≤ sup (cid:12) (cid:12)(E n−E P)[w(X)](cid:12) (cid:12),
(cid:12) (cid:12) (cid:12)n (cid:12) (cid:12) (cid:12)
i≤n
w∈GBa,b
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)1 (cid:88) w (cid:98)(X i,0)R(X i)−E P [w (cid:98)(X,0)R(X)](cid:12) (cid:12) ≤ sup (cid:12) (cid:12)(E n−E P)[w(X)R(X)](cid:12) (cid:12).
(cid:12)n (cid:12) (cid:12) (cid:12)
i≤n
w∈GBa,b
Moreover,asRisnon-negativeandB -bounded,wehaveR ({w·R | w ∈ G }) ≤ |B |·R (G ),
R n B R n B
a,b a,b
and with probability at least 1−n−1,
(cid:12) (cid:12) (cid:114)
sup (cid:12) (cid:12)(E n−E P)[w(X)](cid:12) (cid:12) ≤ C 2 logn +C 3R n(G B ) =: ε n,
(cid:12) (cid:12) n a,b
w∈GBa,b
where C , C are universal constants and R (G ) is the Rademacher complexity of G . There-
2 3 n B B
a,b a,b
fore, the maximizer w(x,0) is feasible for (16) with a relaxed γ = ε and we have
(cid:98) n
∆ [a,b](ε n) ≥ ∆(cid:101)[a,b](0)−ε n.
33Combining the pieces above, we have
(cid:8) (cid:9)
− ∆ (0)−∆ (−γ ) −γ
[a,b] [a,b] n n
≤ ∆(cid:101)[a,b](0)−∆ [a,b](0) ≤ (cid:8) ∆ [a,b](ε n)−∆ [a,b](0)(cid:9) +ε n, (17)
which holds with probability at least 1−2n−1. Then, our goal is to control the terms ∆ (0)−
[a,b]
∆ (−γ ) and ∆ (ε )−∆ (0).
[a,b] n [a,b] n [a,b]
Step 2: Perturbation analysis. Assume 1−a < γ < b−1. The maximizer of (16) takes the
form
w∗(x,γ) = (a−η∗)·1(cid:8) π(R)(x) < q∗(cid:9) +(b−η∗)·1(cid:8) π(R)(x) > q∗(cid:9) +η∗,
γ γ γ γ γ
where q∗ = q (t∗) with
γ π(R) γ
(cid:26) (cid:12) (cid:27)
t∗
γ
= inf t ∈ range(F π(R)) (cid:12) (cid:12)
(cid:12)
t ≥ b− b−1−
a
γ .
Accordingly, we have
(b−a)t∗ −(b−1−γ)
η∗ = a+ γ .
γ P(π(R)(X) = q∗)
γ
We denote q∗ = q∗ and t∗ = t∗, for which we have t∗ ≥ t∗ and q∗ ≥ q∗ by definition. Then, we can
0 0 γ γ
show that
∆ (γ) = aE [R(X)]+(b−a)E(cid:2) π(R)(X)·1(cid:8) π(R)(X) > q∗(cid:9)(cid:3) +q∗(b−a)t∗ −q∗(b−1−γ).
[a,b] P γ γ γ γ
Thus, for any 1−a < γ < b−1, it holds that
∆ (γ)−∆ (0)
[a,b] [a,b]
= (b−a)E(cid:2) π(R)(X)·1(cid:8) q∗ < π(R)(X) ≤ q∗(cid:9)(cid:3)
γ
+(b−a)(q∗t∗ −q∗t∗)+(q∗−q∗)(b−1)+q∗γ
γ γ γ γ
≤ (b−a)q∗(t∗−t∗)+(b−a)(q∗t∗ −q∗t∗)+(q∗−q∗)(b−1)+q∗γ
γ γ γ γ γ
= (q∗−q∗)(cid:2) b−1−(b−a)t∗(cid:3) +q∗γ
γ γ γ
≤ q∗γ.
The last inequality is due the fact that t∗ ≥ (b−1−γ)/(b−a). Similarly, we have the bound
γ
∆ (0)−∆ (−γ)
[a,b] [a,b]
= (b−a)E(cid:2) π(R)(X)·1(cid:8) q∗ < π(R)(X) ≤ q∗ (cid:9)(cid:3)
−γ
+(b−a)(q∗t∗−q∗ t∗ )+(q∗ −q∗)(b−1)+q∗ γ
−γ −γ −γ −γ
≤ q∗ γ.
−γ
Consequently, with n sufficiently large, for ε and γ ≤ (b−1)/2, we obtain the bounds
n n
∆ (ε )−∆ (0) ≤ q∗ε ,
[a,b] n [a,b] n
∆ (0)−∆ (−γ ) ≤ q∗ γ .
[a,b] [a,b] n −(b−1)/2 n
34Summary. With the calculated bounds above, the inequalities in (17) can be written as
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)∆(cid:98)iso(R;B a,b)−∆iso(R;B a,b)(cid:12) (cid:12)
(cid:12)
= (cid:12) (cid:12) (cid:12)∆(cid:101)[a,b](0)−∆ [a,b](0)(cid:12) (cid:12)
(cid:12)
≤ max(cid:110)(cid:16) 1+q −∗ (b−1)/2(cid:17) γ n, (1+q∗)ε n(cid:111) ,
which holds with probability at least 1−2n−1.
Then, it suffices to bound the Rademacher complexity.
• When d ≥ 2, by Dutta and Nguyen (2018) (Theorem 3.1), for the class G(cid:101)B of uniformly
a,b
bounded isotonic functions defined on a closed set [−L,L]d, they have shown that the metric
entropy
Ld2
logN(τ,G(cid:101)B a,b,L 2(P)) ≲
τd
.
Under the sub-Gaussian assumption on X, there exists L ≍ log(d/τ) such that P(X ∈
τ
[−L ,L ]d) ≥ 1−τ/2. Then, we have the metric entropy
τ τ
(log(d/τ))d2
logN(τ,G ,L (P)) ≲ ,
B a,b 2 τd
thus by discretization, we have
(cid:40) (cid:114) (cid:41)
2logN(τ,Rd,P)
R (G ) ≲ inf τ + ,
n B
a,b τ n
which yields
(cid:18) logn(cid:19)d2/2(cid:18) 1(cid:19)1/(d+2)
R (G ) ≲ .
n B
a,b d n
• Moreover, when d = 1, by Dudley’s theorem (Dudley, 1967), we can obtain a tighter bound
R (G ) ≲ n−1/2 (Birgé, 1987; Chatterjee and Lafferty, 2019).
n B
a,b
To sum up, for any fixed d ≥ 1, there exists constant C(cid:101) that does not depend on n such that we
have the bound for the estimation error
(cid:12)
(cid:12) (cid:12)∆(cid:98)iso(R;B a,b)−∆iso(R;B
a,b)(cid:12)
(cid:12) (cid:12) ≤
C(cid:101)(logn)d2/2(cid:18) 1(cid:19)max{1/(2d),1/(d+2)}
.
(cid:12) (cid:12) n
A.6 Proof of Theorem 4.2 with B = B and Corollary 4.3
f,ρ
We fix a truncation threshold Γ for the L -norm of density ratio. We first state the lemma that
∞
guarantees the attainability of the excess risk.
Lemma A.4. Assume R is B -bounded and convex function f is differentiable with f(1) = 0. The
R
excess risk ∆iso(R;B ) is attained at w∗iso ∈ B ∩Ciso with ∥w∗iso∥ < ∞ almost surely.
f,ρ f,ρ f,ρ ≼ f,ρ ∞
35To ease the notation, we denote
∆(ρ) := ∆iso(R;B ) = sup E [w(X)R(X)]−E [R(X)]
f,ρ P P
w∈L2(P)
subject to E [w(X)] = 1, 0 ≤ w ≤ Γ,
P
E [f(w(X))] ≤ ρ, w ∈ Ciso, (18)
P ≼
where we write ρ as an argument for further perturbation analysis. Without loss of generality, we
assume t = 1 is the minimizer of f5.
Define the following optimization problems:
1 (cid:88) 1 (cid:88)
∆(cid:101)(γ,ρ) := max w(X i)R(X i)− r(X i,Y i)
n n
w∈L2(P(cid:98)n)
i≤n i≤n
1 (cid:88)
subject to w(X ) ≤ 1+γ, 0 ≤ w ≤ Γ,
i
n
i≤n
1 (cid:88)
f(w(X )) ≤ ρ, w ∈ Ciso. (19)
i ≼
n
i≤n
∆(γ,ρ) := sup E [w(X)R(X)]−E [R(X)]
P P
w∈L2(P)
subject to E [w(X)] ≤ 1+γ, 0 ≤ w ≤ Γ,
P
E [f(w(X))] ≤ ρ, w ∈ Ciso. (20)
P ≼
Denote the maximizer to (20) by w∗(x,γ,ρ) and maximizer to (19) by w(x,γ,ρ). We first state the
(cid:98)
following lemma to show the connection with the unperturbed problem.
Lemma A.5. Define ∆(cid:101)∗(γ,ρ) by replacing the inequality condition on n−1(cid:80) i≤nw(X i) with the
equality condition and ∆∗(γ,ρ) by replacing the inequality condition on E [w(X)] with the equality
P
condition. Then, for −1 < γ ≤ 0,
∆∗(γ,ρ) = ∆(γ,ρ), ∆(cid:101)∗(γ,ρ) = ∆(cid:101)(γ,ρ).
Particularly, we have
∆iso(R;B f,ρ) = ∆(0,ρ) and ∆(cid:98)iso(R;B f,ρ) = ∆(cid:101)(0,ρ).
5To see this, for any c∈R and f(cid:101)(t)=f(t)−c(t−1), we have ∆iso(R;B f,ρ)=∆i f(cid:101)s ,o ρ,R. Specifically, the equality
holds for f(cid:101)(t)=f(t)−f′(1)(t−1)
36Step 1: Bounding the deviation. Bythelawoflargenumbers,thereexistsauniversalconstant
C(cid:101)1 such that with probability at least 1−n−1,
(cid:12) (cid:12) (cid:114)
(cid:12) (cid:12)1 (cid:88) w∗(X i,0,ρ)−E P [w∗(X,0,ρ)](cid:12) (cid:12) ≤ C(cid:101)1 logn ,
(cid:12)n (cid:12) n
i≤n
(cid:12) (cid:12) (cid:114)
(cid:12) (cid:12)1 (cid:88) f(w∗(X i,0,ρ))−E P [f(w∗(X,0,ρ))](cid:12) (cid:12) ≤ C(cid:101)1 logn ,
(cid:12)n (cid:12) n
i≤n
(cid:12) (cid:12) (cid:114)
(cid:12) (cid:12)1 (cid:88) w∗(X i,0,ρ)R(X i)−E P [w∗(X,0,ρ)R(X)](cid:12) (cid:12) ≤ C(cid:101)1 logn .
(cid:12)n (cid:12) n
i≤n
which implies that, letting γ
n
:= C(cid:101)1(cid:112) logn/n, with probability at least 1 − n−1, the minimizer
w∗(x,−γ ,ρ−γ ) is feasible for (19) with γ = 0. Thus, we have
n n
∆(cid:101)(0,ρ) ≥ ∆(−γ n,ρ−γ n)−γ n.
On the other hand, recall the notation G = {w ∈ L (P) | w ∈ Ciso, w ∈ [0,Γ]}, G =
f,1 2 ≼ f,2
{f ◦w | w ∈ G } and G = G ∪G . Since we assume f is bounded on [0,Γ], we further have
f,1 f f,1 f,2
G ⊂ {f ◦w | w ∈ L (P), w ∈ Ciso, w ∈ [0,Γ]}. For the maximizer w(x,0,ρ), we have
f,2 2 ≼ (cid:98)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)E P[w (cid:98)(X,0)]−1(cid:12) (cid:12)−(cid:12) (cid:12)1 (cid:88) w (cid:98)(X i,0)−1(cid:12) (cid:12) ≤ sup (cid:12) (cid:12)(E n−E P)[w(X)](cid:12) (cid:12),
(cid:12) (cid:12) (cid:12)n (cid:12) (cid:12) (cid:12)
i≤n
w∈GBf,ρ
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)1 (cid:88) f(w (cid:98)(X i,0))−E P [f(w (cid:98)(X,0))](cid:12) (cid:12) ≤ sup (cid:12) (cid:12)(E n−E P)[f(w(X))](cid:12) (cid:12),
(cid:12)n (cid:12) (cid:12) (cid:12)
i≤n
w∈GBf,ρ
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)1 (cid:88) w (cid:98)(X i,0)R(X i)−E P [w (cid:98)(X,0)R(X)](cid:12) (cid:12) ≤ B R sup (cid:12) (cid:12)(E n−E P)[w(X)](cid:12) (cid:12).
(cid:12)n (cid:12) (cid:12) (cid:12)
i≤n
w∈GBf,ρ
Moreover, with probability at least 1−n−1, we have
(cid:12) (cid:12) (cid:40)(cid:114) (cid:41)
sup (cid:12) (cid:12)(E n−E P)[w(X)](cid:12) (cid:12) ≤ C 2 logn +R n(G f,1) =: ε n,
(cid:12) (cid:12) n
w∈GBf,ρ
(cid:12) (cid:12) (cid:40)(cid:114) (cid:41)
sup (cid:12) (cid:12)(E n−E P)[f(w(X))](cid:12) (cid:12) ≤ C 3 logn +R n(G f,2) =: ε (cid:101)n.
(cid:12) (cid:12) n
w∈GBf,ρ
where C , C are universal constants and R (G ), R (G ) are the Rademacher complexities of
2 3 n f,1 n f,2
G and G , respectively. We also note that R (G ),R (G ) ≤ R (G ).
f,1 f,2 n f,1 n f,2 n B f,ρ
Therefore, the maximizer w(x,0,ρ) is feasible for (20) with γ = ε and ρ+ε , thus we have the
(cid:98) n (cid:101)n
upper bound for ∆(cid:101)(0,ρ):
∆(ε n,ρ+ε (cid:101)n) ≥ ∆(cid:101)(0,ρ)−ε n.
Combining the pieces above, we obtain
−{∆(0,ρ)−∆(−γ ,ρ−γ )}−γ
n n n
≤ ∆(cid:101)(0,ρ)−∆(0,ρ) ≤ {∆(ε n,ρ+ε (cid:101)n)−∆(0,ρ)}+ε n, (21)
which holds with probability at least 1−2n−1. The next step is to characterize the deviation after
perturbation for ∆(γ,ρ).
37Step 2: Perturbation analysis. The optimization problem (20) has the dual formulation:
(cid:26) (cid:20) (cid:21)(cid:27)
∆(γ,ρ) = inf λρ+ν(1+γ)+E w(X)(π(R)(X)−ν)−λf(w(X)) , (22)
P
λ≥0,ν
where
(cid:26) (cid:18) (cid:19)(cid:27)
π(R)(x)−ν
w(x) = P (f′)−1 .
[0,+∞) λ
Denote λ∗ and ν∗ as the minimizers and
(cid:20) (cid:21)
H(λ,ν;ρ,γ) = λρ+ν(1+γ)+E w(X)(π(R)(X)−ν)−λf(w(X)) ,
P
for which we have ∆(0,ρ) = H(λ∗,ν∗;ρ,0). Then, for the upper bound, we have
∆(ε ,ρ+ε )−∆(0,ρ) ≤ H(λ∗,ν∗;ρ+ε ,ε )−H(λ∗,ν∗;ρ,0) (23)
n (cid:101)n (cid:101)n n
≤ λ∗ε +ν∗ε . (24)
(cid:101)n n
Ourgoalistoderiveupperboundsforλ∗ andν∗. Notethatν∗ istheparameterforstandardization,
thus to guarantee E [w∗(X,0,ρ)] = 1, we have
P
(cid:18)
B
−ν∗(cid:19)
sup w∗(x,0,ρ) = (f′)−1 R ≥ 1.
λ∗
x∈X
This implies that
ν∗ ≤ B −λ∗f′(1), (25)
R
then it suffices to show that λ∗ is finite. Moreover, as min w∗(x,0,ρ) = (f′)−1(−ν∗/λ∗) ≤ 1,
x∈X
we have ν∗ ≥ −λ∗f′(1).
Step3: Boundingthedualminimizer. Denoteϕ(ξ) = P (cid:8) (f′)−1(ξ)(cid:9)andψ(ξ) = ξϕ(ξ)−
[0,+∞)
f(ϕ(ξ)). Recall the dual formulation
(cid:26) (cid:20) (cid:18) (cid:19)(cid:21)(cid:27)
π(R)(X)−ν
∆(γ,ρ) = inf λρ+ν(1+γ)+λ·E ψ .
P
λ≥0,ν λ
Denote ν = ν(λ) as the solution to E [ϕ(ξ)] = 1+γ and
P
(cid:20) (cid:18) (cid:19)(cid:21)
π(R)(X)−ν(λ)
L(λ,ρ,γ) = λρ+ν(λ)(1+γ)+λ·E ψ .
P
λ
Suppose the optimal dual variable λ is unbounded. Due to the fact that π(R) is bounded and
(cid:20) (cid:18) (cid:19)(cid:21)
π(R)(X)−ν(λ)
E ϕ = 1+γ,
P
λ
we have ν(λ) = −c·λ as λ → ∞, which satisfies that (f′)−1(c) = 1+γ, which yields c = f′(1+γ).
Then, we can verify that
(cid:26) (cid:18) (cid:18) (cid:19)(cid:19)(cid:27)
π(R)(X)−ν(λ)
E f ϕ → f(1+γ), as λ → +∞.
P
λ
38When γ is small enough, which is vanishing when n increases, we have shown that
(cid:26) (cid:18) (cid:18) (cid:19)(cid:19)(cid:27)
∂ π(R)(X)−ν(λ)
L(λ,ρ,γ) = ρ−E f ϕ → ρ > 0, as λ → +∞.
P
∂λ λ
Then, there exists λ¯ > 0 such that when λ > λ¯, we have ∂ L(λ,ρ,γ) ≥ ρ/2, which draws the
λ
contradiction to the assumption that the dual problem is minimized at λ∗ = ∞. This implies that
λ∗(ρ+γ,γ) ≤ λ¯ foranysmallγ andγ. Consequently,whennissufficientlylargesuchthatγ ≤ 1/2,
(cid:101) (cid:101) n
we denote L = sup |f′(t)|. Then, by (23) and (25),
f |t−1|≤1/2
∆(ε n,ρ+ε (cid:101)n)−∆(0,ρ) ≤ λ¯ε (cid:101)n+(cid:0) B R+λ¯(cid:12) (cid:12)f′(1)(cid:12) (cid:12)(cid:1) ε n,
∆(0,ρ)−∆(−γ ,ρ−γ ) ≤ λ¯γ +(cid:0) B +λ¯L (cid:1) γ .
n n n R f n
The bounds in (21) can be written as
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)∆(cid:101)(0,ρ)−∆(0,ρ)(cid:12)
(cid:12) (cid:12)
(cid:26) (cid:27)
≤ max (cid:0) 1+B R+λ¯L
f
+λ¯(cid:1) γ n, (cid:0) 1+B R+λ¯(cid:12) (cid:12)f′(1)(cid:12) (cid:12)(cid:1) ε n+λ¯ε
(cid:101)n
.
Summary. We have shown that for any 0 < Γ < ∞ and n sufficiently large, there exist a constant
C(cid:101) = C(cid:101)(ρ) > 0 such that with probability at least 1−2n−1,
(cid:12) (cid:12) (cid:32)(cid:114) (cid:33)
(cid:12) (cid:12) (cid:12)∆(cid:98)i Γso(r;B f,ρ)−∆i Γso(R;B f,ρ)(cid:12) (cid:12)
(cid:12)
≤ C(cid:101) lo ngn +R n(G B f,ρ) .
Then, it suffices to bound the Rademacher complexity. Since G is also contained in the
B
f,ρ
bounded variation function class, then when X is bounded, similar to the proof of the previous
theorem, we have
(cid:18) logn(cid:19)d2/2(cid:18) 1(cid:19)1/(d+2)
R (G ) ≲ .
n B
f,ρ d n
Moreover, when d = 1, by Dudley’s theorem (Dudley, 1967), we can obtain a tighter bound
R (G ) ≲ n−1/2 (Birgé, 1987; Chatterjee and Lafferty, 2019). To sum up, for any fixed d ≥ 1,
n B
f,ρ
there exists a constant C(cid:101) that does not depend on n such that we have the bound for the estimation
error
(cid:12)
(cid:12) (cid:12) (cid:12)∆(cid:98)i Γso(r;B f,ρ)−∆i Γso(R;B
f,ρ)(cid:12)
(cid:12) (cid:12)
(cid:12)
≤
C(cid:101)(logn)d2/2(cid:18) n1(cid:19)max{1/(2d),1/(d+2)}
.
B Additional technical proofs
B.1 Proof of Proposition A.2
Wefollowthesetupwithr ∼ Bern(R(X ))independently. ConsidertheboundsconstraintB = B .
i i a,b
The estimated excess takes the form
1 (cid:88) 1 (cid:88)
∆(cid:98)(r;B a,b) = max w ir i− R(X i) subject to w #P(cid:98)n ∈ B a,b.
n n
w∈L2(P(cid:98)n)
i≤n i≤n
39According to Section 2, the worst-case weights take the form w = c ·1{r = 0}+c ·1{r = 1},
i 1 i 2 i
where a ≤ c ≤ 1 ≤ c ≤ b. Moreover, by the KKT condition, at least one of c = a and c = b
1 2 1 2
holds. Then, the estimated excess risk can be expressed as
c 2 (cid:88) 1 (cid:88)
∆(cid:98)(r;B a,b) = r i− R(X i).
n n
i≤n i≤n
Since n−1(cid:80) w = 1, we have
i≤n i
1 (cid:88) c 2−1
(1−r ) = .
i
n c −c
2 1
i≤n
In the meantime, by the Law of Large Numbers, there exists a universal constant c > 0 such that
(cid:101)
with probability at least 1−n−1,
(cid:12) (cid:12) (cid:114) (cid:12) (cid:12) (cid:114)
(cid:12) (cid:12)1 (cid:88) r i− 1 (cid:88) R(X i)(cid:12) (cid:12) ≤ (cid:101)c logn , (cid:12) (cid:12)1 (cid:88) R(X i)−R P[R(X)](cid:12) (cid:12) ≤ (cid:101)c logn ,
(cid:12)n n (cid:12) 2 n (cid:12)n (cid:12) 2 n
i≤n i≤n i≤n
which implies
(cid:32) (cid:114) (cid:33)
logn
c −1 ≥ E [1−R(X)]−c ·min{1−a,b−1}.
2 P (cid:101)
n
If n is large enough such that min{(E [1 − R(X)])2,(E [R(X)])2}n ≥ (1 − κ)−2c2logn, where
P P (cid:101)
1/2 ≤ κ < 1 satisfies that
κ+κ2E [1−R(X)]·min{1−a,b−1} > 1+c, (26)
P
for some 0 < c < E [1−R(X)]·min{1−a,b−1}. Then, with probability at least 1−n−1, we have
P
c ≥ 1+κE [1−R(X)]·min{1−a,b−1}.
2 P
Consequently, for the excess risk, with probability at least 1−n−1, we have
c 2−1 (cid:88) 1 (cid:88)
∆(cid:98)(r;B a,b) = r i+ (r i−R(X i))
n n
i≤n i≤n
= (c −1)E [R(X)]+
c 2−1 (cid:88)
(r −E [R(X)])+
1 (cid:88)
(r −R(X ))
2 P i P i i
n n
i≤n i≤n
(cid:114)
logn
≥ (c −1)E [R(X)]−c ·c
2 P 2 (cid:101)
n
≥ (κc −1)E [R(X)] ≥ c·E [R(X)],
2 P P
where the last inequality holds according to (26).
B.2 Proof of Lemma A.3
Suppose ∆ (γ) is attained at w ∈ Ciso in the interior of the feasible set, i.e.
[a,b] (cid:101) ≼
E [w(X)] < 1+γ, a ≤ w ≤ b.
P (cid:101) (cid:101)
40Then, let w = (w+η)∧b, which satisfies that E [w (X)] = 1+γ and
η (cid:101) P η
E [w (X)R(X)]−E [R(X)]−∆ (γ) = η·E [R(X)1{w(X) < b}] > 0.
P η P [a,b] P (cid:101)
This draws the contradiction. Thus ∆ (γ) is attained on the boundary of the feasible set and
[a,b]
particularly, when γ = 0, we have ∆iso(R;B ) = ∆ (0).
a,b [a,b]
B.3 Proof of Lemma A.4
We only show that the excess risk can be attained. Denote ϕ(ξ) = P (cid:8) (f′)−1(ξ)(cid:9) and ψ(ξ) =
[0,Γ]
ξϕ(ξ)−f(ϕ(ξ)). Let {w (x) = ϕ(ξ )} be a sequence such that ξ = (R(X)−ν )/λ and
k k k≥1 k k k
∆ (R;B ) = lim E [w (X)R(X)]−E [R(X)].
ρ f,ρ P k P
k→+∞
Recall the dual formulation
(cid:26) (cid:20) (cid:18) (cid:19)(cid:21)(cid:27)
R(X)−ν
∆ (R;B ) = inf λρ+ν +λ·E ψ .
ρ f,ρ P
λ≥0,ν λ
Denote ν = ν(λ) (also ν = ν(λ )) as the solution to E [ϕ(ξ)] = 1 and
k k P
(cid:20) (cid:18) (cid:19)(cid:21)
R(X)−ν(λ)
L(λ,ρ) = λρ+ν(λ)+λ·E ψ .
P
λ
Suppose {λ } is unbounded. There exists a subsequence {λ } such that λ → +∞.
k k≥1 km m≥1 km
Due to the fact that R is bounded and
(cid:20) (cid:18) (cid:19)(cid:21)
R(X)−ν(λ )
E ϕ k = 1,
P
λ
k
we have ν(λ )/λ → −c as k → ∞ with c satisfying that c = f′(1). Then, we can verify that
k k
(cid:26) (cid:18) (cid:18) (cid:19)(cid:19)(cid:27)
R(X)−ν(λ )
E f ϕ k → f(1) = 0, as k → +∞,
P
λ
k
which is a contradiction to the KKT condition that λ ·(E [f(w (X))]−ρ) = 0. Hence, there exists
k P k
an upper bound λ¯ < ∞ such that sup λ ≤ λ¯ and (λ∗,ν∗) = (λ∗,ν(λ∗)) can be attained.
k k
B.4 Proof of Lemma A.5
With −1 < γ < 0, recall the KKT condition of problem (20):
−π(R)(x)+λf′(w∗(x,γ))+ν = 0,
λ(E [f(w∗(X,γ))]−ρ) = 0,
P
ν(E [w∗(X,γ)]−1−γ) = 0.
P
Suppose E [w∗(X,γ)] < 1+γ, then ν = 0, which implies that
P
λf′(w(x)) = π(R)(x).
Denote G = {w ∈ L (P) | E [f(w(X))] ≤ ρ}, which is a convex set.
2 P
41Case 1. Ifλ = 0, theoptimaldensityratiow∗(x,γ)isan interiorpoint ofG andthere existsη > 0
such that w(x,γ) = w∗(x,γ)+η ∈ G. In the meantime,
(cid:101)
E [w(X,γ)π(R)(X)] = E [w∗(X,γ)π(R)(X)]+ηE [π(R)(X)] > E [w∗(X,γ)π(R)(X)],
P (cid:101) P P P
which draws the contradiction.
Case 2. If λ > 0, we have f′(w∗(x,γ)) ∝ π(R)(x). Since π(R) ≥ 0 and f′(t) ≥ 0 if and only if
t ≥ 1, we obtain w∗(x,γ) ≥ 1. However, when γ ≤ 0, as E [w∗(X,γ)] < 1+γ ≤ 1, it draws the
P
contradiction.
B.5 Proof of results in Section 7
Assume ρ ≪ 1. Recall the dual problem
(cid:26) (cid:20) (cid:21)(cid:27)
inf λρ+ν +E w(X)(R(X)−ν)−λf(w(X))
P
λ≥0,ν
with
(cid:26) (cid:18) (cid:19)(cid:27)
R(x)−ν
w(x) = P (f′)−1 .
[0,+∞) λ
Denote λ∗ and ν∗ as the minimizers of the dual problem. We further denote
ϕ(ξ) = P {(f′)−1(ξ)}, ψ(ξ) = ξϕ(ξ)−f(ϕ(ξ)).
[0,+∞)
Recall the KKT condition, we have
(cid:20) (cid:18) (cid:18) R(X)−ν∗(cid:19)(cid:19)(cid:21)
ρ = E f ϕ .
P λ∗
As ρ → 1, we have ϕ((R(X)−ν∗)/λ∗) → 1 almost surely, which implies λ∗ ≫ 1. Also, ν∗ satisfies
(cid:20) (cid:18) R(X)−ν∗(cid:19)(cid:21)
E [w(X)] = E ϕ .
P P λ∗
As λ∗ ≫ 1 and R(x) is bounded, we have
(cid:20) (cid:18) ν∗(cid:19)(cid:21) (cid:18)
1
(cid:19) (cid:18) ν∗(cid:19) (cid:18)
1
(cid:19)
1 = E ϕ − +o = ϕ − +o ,
P λ∗ λ∗ λ∗ λ∗
which yields ν∗ = −f′(1)λ∗+o(1).
Then, as ψ(f′(1)) = f′(1), ψ′(ξ) = ϕ(ξ) and ψ′′(ξ) = [f′′(ϕ(ξ))]−1, consider the expansion
(cid:18) R(X)−ν∗(cid:19) (cid:18)E [R(X)]−ν∗(cid:19) (cid:18)E [R(X)]−ν∗(cid:19)(cid:18)E [R(X)]−ν∗(cid:19)
ψ = ψ P +ψ′ P P
λ∗ λ∗ λ∗ λ∗
1
(cid:18)E [R(X)]−ν∗(cid:19)(cid:18)E [R(X)]−ν∗(cid:19)2 (cid:18)
1
(cid:19)
+ ψ′′ P P +o
2 λ∗ λ∗ λ∗2
(cid:18)E [R(X)]−ν∗(cid:19)
1
(cid:18)E [R(X)]−ν∗(cid:19)2 (cid:18)
1
(cid:19)
= f′(1)+ P + P +o .
λ∗ 2f′′(1) λ∗ λ∗2
42When λ ≫ 1, the dual problem can be approximated by
(cid:20) (cid:18) (cid:19)(cid:21) (cid:26) (cid:27) (cid:18) (cid:19)
R(X)−ν 1 Var(R) 1
λρ+ν +λE ψ = λ ρ+ +o
P λ 2f′′(1) λ2 λ
(cid:18) (cid:19)
1 Var(R) 1
= λρ+ +o .
2f′′(1) λ λ
The dominating term is minimized at
(cid:115)
Var(R)
λ(cid:101) = .
2ρf′′(1)
Consequently, we have
(cid:115) (cid:115)
(cid:18) (cid:19)
2Var(R) 1 2Var(R) √
∆ (R;B )(ρ) = +o = +o( ρ).
ρ f,ρ ρf′′(1) λ(cid:101) ρf′′(1)
B.6 Extension of Lemma 3.1 to general cones
By Proposition 2.2, the solution to (4) can be written as a nondecreasing function of the risk, i.e.
w∗(x) = ψ(R(x)), where ψ : R → R is nondecreasing. Then, the result in Lemma 3.1 can be
+
extended to general cones which satisfy the following conditions.
Condition B.1. Let C ⊆ Rd be a closed cone that is equipped with the projection π .
C
(C1) For any g ∈ C, we have ψ(g) ∈ C, which means C is closed under the composition with ψ.
(C2) For any g ∈ C and τ ∈ R, we have {x ∈ X | g(x) > τ} ∈ A where A is closed under countable
union and countable intersection.
(C3) Denote the set R(A) = {g | {x | g(x) > τ} ∈ A for all τ ∈ R}. Then, we have C = R(A).
Then, we can extend the result in Lemma 3.1 to general cones satisfying the conditions above.
Denote w as the solution to the following optimization problem:
(cid:101)C
∆(π (R);B) = max E [w(X)R(X)]−E [R(X)]
C P P
w∈L2(R)
subject to w P ∈ B. (27)
#
Then, we have the following lemma.
Lemma B.2. Assume a closed cone C satisfies Conditions (C1) (C2) (C3). Then, for any B
satisfying Condition 2.1, we have
∆C(R;B) = ∆(π (R);B).
C
43C Additional simulation results
C.1 Validity guarantee of the DRL with true excess risk
To see that DRL and iso-DRL with the true excess risks are valid, we denote R (X) = P(Y ∈/
α
C (X) | X) and we should note that R (x) is nondecreasing in α for any fixed x. Further, by
1−α α
Proposition 2.2,
∂ ∆(R ) = E (cid:8) ϕ′(R (X))R (X)+ϕ(R (X))−1(cid:9) ,
α P α α α
∂R
α
where ϕ is nondecreasing and E [ϕ(R (X))] = 1. Then, we have
P α
∂ ∆(R ) = E (cid:8) ϕ′(R (X))R (X)(cid:9) ≥ 0,
α P α α
∂R
α
which guarantees that ∆(R ) is also nondecreasing in α. Thus, if (dP /dP) P ∈ B,
α target #
supE [w(X)R (X)] ≤ E [R (X)]+∆(R ) ≤ α−∆(R )+∆(R ) = α.
P α P α α α α
(cid:101) (cid:101) (cid:101)
w B
#
More generally, define
(cid:40) (cid:41)
A = β ∈ B(cid:101) : sup E Q[R β(X)] ≤ α .
Q∈Q
Then, based on the calculation above, we have α ∈ A, thus α ≤ supA, which means that the
(cid:101) (cid:101)
aforementioned calibrated level α is a slightly conservative but more tractable choice.
(cid:101)
C.2 Wine quality simulation: a proxy of the oracle KL-divergence
AsisshowninSection5.2, wedenotew asthedensityratioobtainedbykerneldensityestimation
(cid:98)kde
(Gaussian kernel with bandwidth 0.125). Accordingly, let dQ(cid:98)kde = w
(cid:98)kde
·dP as a proxy of P target.
With a subsample {X } drawn the group of white wine (data distribution P), the proxy ρ can
i i≤K (cid:98)
be calculated by
1 (cid:88)
ρ = w (X )log(w (X ))
(cid:98) (cid:98)kde i (cid:98)kde i
K
i≤K
(cid:40) (cid:32) (cid:33)(cid:41)
≈ E
P
dQ(cid:98)kde
log
dQ(cid:98)kde
= D
KL(cid:16) Q(cid:98)kde||P(cid:17)
.
dP dP
To show the range for values of ρ, we repeatedly fit KDE on the 50% samples from each group
(cid:98)
(white and red wine groups respectively). The following plot shows the histogram of ρ with 1000
(cid:98)
repetitions, of which the median is approximately 0.859.
References
Ai, J. and Ren, Z. (2024). Not all distributional shifts are equal: Fine-grained robust conformal
inference. arXiv preprint arXiv:2402.13042.
4460
50
40
30
20
10
0
0.50 0.75 1.00 1.25 1.50 1.75
Figure 7: Histogram of ρ.
(cid:98)
Bastani, H. (2021). Predicting with proxies: Transfer learning in high dimension. Manag. Sci.,
67:2964–2984.
Ben-David, S., Lu, T., Luu, T., and Pál, D. (2010). Impossibility theorems for domain adaptation.
In AISTATS.
Ben-David, S. and Urner, R. (2012). On the hardness of domain adaptation and the utility of
unlabeled target samples. In ALT.
Ben-David, S. and Urner, R. (2013). Domain adaptation–can quantity compensate for quality?
Annals of Mathematics and Artificial Intelligence, 70:185–202.
Ben-Tal, A. and Nemirovski, A. (1998). Robust convex optimization. Mathematics of operations
research, 23(4):769–805.
Birgé, L. (1987). Estimating a density under order restrictions: Nonasymptotic minimax risk. The
Annals of Statistics, pages 995–1012.
Blanchet,J.,Kang,Y.,andMurthy,K.(2019). Robustwassersteinprofileinferenceandapplications
to machine learning. Journal of Applied Probability, 56(3):830–857.
Blanchet, J. and Murthy, K. (2019). Quantifying distributional model risk via optimal transport.
Mathematics of Operations Research, 44(2):565–600.
Blanchet, J. and Shapiro, A. (2023). Statistical limit theorems in distributionally robust optimiza-
tion. arXiv preprint arXiv:2303.14867.
Bogdan, M., Van Den Berg, E., Sabatti, C., Su, W., and Candès, E. J. (2015). Slope—adaptive
variable selection via convex optimization. The annals of applied statistics, 9(3):1103.
45Brunk, H. (1963). On an extension of the concept conditional expectation. Proceedings of the
American Mathematical Society, 14(2):298–304.
Brunk, H. (1965). Conditional expectation given a σ-lattice and applications. The Annals of
Mathematical Statistics, 36(5):1339–1350.
Brunk,H.,Barlow,R.E.,Bartholomew,D.J.,andBremner,J.M.(1972).Statisticalinferenceunder
order restrictions.(the theory and application of isotonic regression). International Statistical
Review, 41:395.
Brunk, H., Ewing, G., and Utz, W. (1957). Minimizing integrals in certain classes of monotone
functions. Pacific Journal of Mathematics.
Cai, T. T. and Wei, H. (2019). Transfer learning for nonparametric classification: Minimax rate
and adaptive classifier. ArXiv, abs/1906.02903.
Candès, E., Lei, L., and Ren, Z. (2023). Conformalized survival analysis. Journal of the Royal
Statistical Society Series B: Statistical Methodology, 85(1):24–45.
Cauchois, M., Gupta, S., Ali, A., and Duchi, J. C. (2020). Robust validation: Confident predictions
even when distributions shift. arXiv preprint arXiv:2008.04267.
Chatterjee, S. and Lafferty, J. (2019). Adaptive risk bounds in unimodal regression. Bernoulli.
Chattopadhyay, R., Fan, W., Davidson, I., Panchanathan, S., and Ye, J. (2013). Joint transfer and
batch-mode active learning. In ICML.
Chen, M., Weinberger, K. Q., and Blitzer, J. (2011). Co-training for domain adaptation. In NIPS.
Chen, Y. and Lei, J. (2024). De-biased two-sample u-statistics with application to conditional
distribution testing. arXiv preprint arXiv:2402.00164.
Cornfield, J., Haenszel, W., Hammond, E. C., Lilienfeld, A. M., Shimkin, M. B., and Wynder, E. L.
(1959). Smoking and lung cancer: recent evidence and a discussion of some questions. Journal
of the National Cancer institute, 22(1):173–203.
Cortes, C., Mohri, M., Riley, M., and Rostamizadeh, A. (2008). Sample selection bias correction
theory. ArXiv, abs/0805.2775.
De Bartolomeis, P., Abad, J., Donhauser, K., and Yang, F. (2023). Hidden yet quantifiable: A
lower bound for confounding strength using randomized trials. arXiv preprint arXiv:2312.03871.
Deng, Z., Dwork, C., and Zhang, L. (2023). Happymap: A generalized multi-calibration method.
arXiv preprint arXiv:2303.04379.
Ding, P. and VanderWeele, T. J. (2016). Sensitivity analysis without assumptions. Epidemiology
(Cambridge, Mass.), 27(3):368.
46Donsker, M. D. and Varadhan, S. S. (1976). Asymptotic evaluation of certain markov process
expectations for large time—iii. Communications on pure and applied Mathematics, 29(4):389–
461.
Duchi,J.andNamkoong,H.(2018). Learningmodelswithuniformperformanceviadistributionally
robust optimization. arXiv preprint arXiv:1810.08750.
Duchi, J. C., Glynn, P. W., and Namkoong, H. (2021). Statistics of robust optimization: A gener-
alized empirical likelihood approach. Mathematics of Operations Research, 46(3):946–969.
Duchi, J. C., Hashimoto, T., and Namkoong, H. (2019). Distributionally robust losses against
mixture covariate shifts. Under review, 2(1).
Dudley, R. M. (1967). The sizes of compact subsets of hilbert space and continuity of gaussian
processes. Journal of Functional Analysis, 1(3):290–330.
Durot, C. and Lopuhaä, H. P. (2018). Limit Theory in Monotone Function Estimation. Statistical
Science, 33(4):547 – 567.
Dutta, P. and Nguyen, K. T. (2018). Covering numbers for bounded variation functions. Journal
of Mathematical Analysis and Applications, 468(2):1131–1143.
El Ghaoui, L. and Lebret, H. (1997). Robust solutions to least-squares problems with uncertain
data. SIAM Journal on matrix analysis and applications, 18(4):1035–1064.
El Ghaoui, L., Oustry, F., and Lebret, H. (1998). Robust solutions to uncertain semidefinite
programs. SIAM Journal on Optimization, 9(1):33–52.
Esfahani, P. M. and Kuhn, D. (2015). Data-driven distributionally robust optimization using
the wasserstein metric: Performance guarantees and tractable reformulations. arXiv preprint
arXiv:1505.05116.
Esteban-Pérez, A. and Morales, J. M. (2022). Partition-based distributionally robust optimization
via optimal transport with order cone constraints. 4OR, 20(3):465–497.
Ge, J., Tang, S., Fan, J., Ma, C., and Jin, C. (2023). Maximum likelihood estimation is all you
need for well-specified covariate shift. arXiv preprint arXiv:2311.15961.
Grenander, U. (1956). On the theory of mortality measurements. Skandinavisk Aktuarietidskrift,
39:1–55.
Gretton, A., Smola, A., Huang, J., Schmittfull, M., Borgwardt, K. M., Schölkopf, B., Candela, Q.,
Sugiyama, M., Schwaighofer, A., and Lawrence, N. D. (2009). Covariate shift by kernel mean
matching. In NIPS 2009.
47Gui, Y., Barber, R., and Ma, C. (2024). Conformalized matrix completion. Advances in Neural
Information Processing Systems, 36.
Gui, Y., Hore, R., Ren, Z., and Barber, R. F. (2023). Conformalized survival analysis with adaptive
cut-offs. Biometrika, page asad076.
Gupta, S. and Rothenhäusler, D. (2021). The s-value: evaluating stability with respect to distribu-
tional shifts. arXiv preprint arXiv:2105.03067.
Han, Q., Wang, T., Chatterjee, S., and Samworth, R. J. (2019). Isotonic regression in general
dimensions. The Annals of Statistics.
Hanneke, S. and Kpotufe, S. (2019). On the value of target data in transfer learning. In NeurIPS.
Hardy, G. H., Littlewood, J. E., and Pólya, G. (1952). Inequalities. Cambridge university press.
Hébert-Johnson,U.,Kim,M.,Reingold,O.,andRothblum,G.(2018).Multicalibration: Calibration
for the (computationally-identifiable) masses. In International Conference on Machine Learning,
pages 1939–1948. PMLR.
Henzi, A., Ziegel, J. F., and Gneiting, T. (2021). Isotonic distributional regression. Journal of the
Royal Statistical Society Series B: Statistical Methodology, 83(5):963–993.
Hu, X. and Lei, J. (2020). A distribution-free test of covariate shift using conformal prediction.
arXiv: Methodology.
Hu, Y., Li, M., Lu, Q., Weng, H., Wang, J., Zekavat, S. M., Yu, Z., Li, B., Gu, J., Muchnik, S. K.,
Shi, Y., Kunkle, B. W., Mukherjee, S., Natarajan, P., Naj, A. C., Kuzma, A., Zhao, Y., Crane,
P. K., Lu, H., and Zhao, H. (2019). A statistical framework for cross-tissue transcriptome-wide
association analysis. Nature Genetics, 51:568–576.
Jin, Y., Ren, Z., and Candès, E. J. (2023). Sensitivity analysis of individual treatment ef-
fects: A robust conformal inference approach. Proceedings of the National Academy of Sciences,
120(6):e2214889120.
Jin, Y., Ren, Z., and Zhou, Z. (2022). Sensitivity analysis under the f-sensitivity models: a
distributional robustness perspective. arXiv preprint arXiv:2203.04373.
Johansson, F. D., Sontag, D. A., and Ranganath, R. (2019). Support and invertibility in domain-
invariant representations. ArXiv, abs/1903.03448.
Kim, M. P., Kern, C., Goldwasser, S., Kreuter, F., and Reingold, O. (2022). Universal adaptability:
Target-independent inference that competes with propensity scoring. Proceedings of the National
Academy of Sciences, 119(4):e2108097119.
48Lam, H. (2016). Robust sensitivity analysis for stochastic systems. Mathematics of Operations
Research, 41(4):1248–1275.
Lei, L. and Candès, E. J. (2020). Conformal inference of counterfactuals and individual treatment
effects. arXiv preprint arXiv:2006.06138.
Li, S., Cai, T. T., and Li, H. (2021). Transfer learning for high-dimensional linear regression:
Prediction, estimation and minimax optimality. Journal of the Royal Statistical Society: Series
B (Statistical Methodology).
Liu, J., Wu, J., Wang, T., Zou, H., Li, B., and Cui, P. (2023). Geometry-calibrated dro: Combating
over-pessimism with free energy implications. arXiv preprint arXiv:2311.05054.
Ma, C., Pathak, R., and Wainwright, M. J. (2023). Optimally tackling covariate shift in rkhs-based
nonparametric regression. The Annals of Statistics, 51(2):738–761.
Matzkin, R. L. (1991). Semiparametric estimation of monotone and concave utility functions for
polychotomous choice models. Econometrica: Journal of the Econometric Society, pages 1315–
1327.
Mei, S., Fei, W., and Zhou, S. (2010). Gene ontology based transfer learning for protein subcellular
localization. BMC Bioinformatics, 12:44 – 44.
Namkoong, H. and Duchi, J. C. (2017). Variance-based regularization with convex objectives.
Advances in neural information processing systems, 30.
Namkoong, H., Ma, Y., and Glynn, P. W. (2022). Minimax optimal estimation of stability under
distribution shift. arXiv preprint arXiv:2212.06338.
Niculescu-Mizil, A. and Caruana, R. A. (2012). Obtaining calibrated probabilities from boosting.
arXiv preprint arXiv:1207.1403.
Pathak, R. and Ma, C. (2024). On the design-dependent suboptimality of the lasso. arXiv preprint
arXiv:2402.00382.
Pathak, R., Ma, C., and Wainwright, M. (2022). A new similarity measure for covariate shift with
applications to nonparametric regression. In International Conference on Machine Learning,
pages 17517–17530. PMLR.
Popescu, I. (2007). Robust mean-covariance solutions for stochastic optimization. Operations Re-
search, 55(1):98–112.
Qiu, H., Dobriban, E., and Tchetgen Tchetgen, E. (2023). Prediction sets adaptive to unknown
covariate shift. Journal of the Royal Statistical Society Series B: Statistical Methodology, page
qkad069.
49Rao, B. P. (1969). Estimation of a unimodal density. Sankhy¯a: The Indian Journal of Statistics,
Series A, pages 23–36.
Redko, I., Morvant, E., Habrard, A., Sebban, M., and Bennani, Y. (2020). A survey on domain
adaptation theory: learning bounds and theoretical guarantees. arXiv: Learning.
Rosenbaum, P. R. (1987). Sensitivity analysis for certain permutation inferences in matched obser-
vational studies. Biometrika, 74(1):13–26.
Rothenhäusler, D. and Bühlmann, P. (2023). Distributionally robust and generalizable inference.
Statistical Science, 38(4):527–542.
Sahoo, R., Lei, L., and Wager, S. (2022). Learning from a biased sample. arXiv preprint
arXiv:2209.01754.
Schell, M. J. and Singh, B. (1997). The reduced monotonic regression method. Journal of the
American Statistical Association, 92(437):128–135.
Setlur, A., Dennis, D., Eysenbach, B., Raghunathan, A., Finn, C., Smith, V., and Levine, S. (2023).
Bitrate-constrained dro: Beyond worst case robustness to unknown group shifts. arXiv preprint
arXiv:2302.02931.
Shafer, G. and Vovk, V. (2008). A tutorial on conformal prediction. Journal of Machine Learning
Research, 9(3).
Shafieezadeh Abadeh, S., Mohajerin Esfahani, P. M., and Kuhn, D. (2015). Distributionally robust
logistic regression. Advances in Neural Information Processing Systems, 28.
Shapiro, A. (2017). Interchangeability principle and dynamic equations in risk averse stochastic
programming. Operations Research Letters, 45(4):377–381.
Shapiro, A. and Pichler, A. (2023). Conditional distributionally robust functionals. Operations
Research.
Su, W. and Candes, E. (2016). Slope is adaptive to unknown sparsity and asymptotically minimax.
Sun, Y. and Hu, Y.-J. (2016). Integrative analysis of multi-omics data for discovery and functional
studies of complex human diseases. Advances in genetics, 93:147–90.
Tan, Z. (2006). A distributional approach for causal inference using propensity scores. Journal of
the American Statistical Association, 101(476):1619–1637.
Tian, Y. and Feng, Y. (2021). Transfer learning under high-dimensional generalized linear models.
ArXiv, abs/2105.14328.
Tibshirani, R. J., Barber, R. F., Candès, E. J., and Ramdas, A. (2019). Conformal prediction under
covariate shift. In NeurIPS.
50Turki, T., Wei, Z., and Wang, J. T.-L. (2017). Transfer learning approaches to improve drug
sensitivity prediction in multiple myeloma patients. IEEE Access, 5:7381–7393.
van der Laan, L., Ulloa-Pérez, E., Carone, M., and Luedtke, A. (2023). Causal isotonic calibration
for heterogeneous treatment effects. arXiv preprint arXiv:2302.14011.
Vovk, V., Gammerman, A., and Shafer, G. (2005). Algorithmic learning in a random world, vol-
ume 29. Springer.
Wang, Z., Bühlmann, P., and Guo, Z. (2023). Distributionally robust machine learning with multi-
source data. arXiv preprint arXiv:2309.02211.
Weng, C., Shah, N. H., and Hripcsak, G. (2020). Deep phenotyping: Embracing complexity and
temporality—towards scalability, portability, and interoperability. Journal of Biomedical Infor-
matics, 105:103433 – 103433.
Yadlowsky, S., Namkoong, H., Basu, S., Duchi, J., and Tian, L. (2018). Bounds on the con-
ditional and average treatment effect with unobserved confounding factors. arXiv preprint
arXiv:1808.09521.
Yang, F. and Barber, R. F. (2019). Contraction and uniform convergence of isotonic regression.
Electronic Journal of Statistics.
Yang, L., Hanneke, S., and Carbonell, J. G. (2012). A theory of transfer learning with applications
to active learning. Machine Learning, 90:161–189.
Yang, Y., Kuchibhotla, A. K., and Tchetgen Tchetgen, E. (2024). Doubly robust calibration of
prediction sets under covariate shift. Journal of the Royal Statistical Society Series B: Statistical
Methodology, page qkae009.
Zadrozny,B.andElkan,C.(2002). Transformingclassifierscoresintoaccuratemulticlassprobability
estimates. In Proceedings of the eighth ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 694–699.
Zhang, C.-H. (2002). Risk bounds in isotonic regression. The Annals of Statistics, 30(2):528–555.
Zhao, H., des Combes, R. T., Zhang, K., and Gordon, G. J. (2019a). On learning invariant repre-
sentations for domain adaptation. In ICML.
Zhao, Q., Small, D. S., and Bhattacharya, B. B. (2019b). Sensitivity analysis for inverse probability
weighting estimators via the percentile bootstrap. Journal of the Royal Statistical Society Series
B: Statistical Methodology, 81(4):735–761.
51