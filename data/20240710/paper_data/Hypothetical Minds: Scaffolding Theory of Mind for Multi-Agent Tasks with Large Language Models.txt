Hypothetical Minds: Scaffolding Theory of Mind for
Multi-Agent Tasks with Large Language Models
LoganCross1∗ VioletXiang2 AgamBhatia1 DanielL.K.Yamins1,2 NickHaber3
StanfordUniversity
1DepartmentofComputerScience
2DepartmentofPsychology
3GraduateSchoolofEducation
Abstract
Multi-agent reinforcement learning (MARL) methods struggle with the non-
stationarityofmulti-agentsystemsandfailtoadaptivelylearnonlinewhentested
withnovelagents. Here,weleveragelargelanguagemodels(LLMs)tocreatean
autonomousagentthatcanhandlethesechallenges.Ouragent,HypotheticalMinds,
consistsofacognitively-inspiredarchitecture,featuringmodularcomponentsfor
perception,memory,andhierarchicalplanningovertwolevelsofabstraction. We
introducetheTheoryofMindmodulethatscaffoldsthehigh-levelplanningprocess
bygeneratinghypothesesaboutotheragents’strategiesinnaturallanguage. Itthen
evaluatesanditerativelyrefinesthesehypothesesbyreinforcinghypothesesthat
makecorrectpredictionsabouttheotheragents’behavior. HypotheticalMinds
significantlyimprovesperformanceoverpreviousLLM-agentandRLbaselineson
arangeofcompetitive,mixedmotive,andcollaborativedomainsintheMeltingPot
benchmark,includingbothdyadicandpopulation-basedenvironments. Addition-
ally,comparisonsagainstLLM-agentbaselinesandablationsrevealtheimportance
ofhypothesisevaluationandrefinementforsucceedingoncomplexscenarios.
1 Introduction
AprimarygoalofAIresearchistocreateautonomousagentsthatactadaptivelyinrichembodied
socialworlds. Multi-agentreinforcementlearning(MARL)methodssufferfromvariousdrawbacks
intheseregimes,includinghighsamplecomplexity,poorgeneralizationtoagentsnotseenintraining,
andlimitedreasoningcapabilities.
LLMsarenotonlypowerfulreasonersandin-contextlearners,buttheyarealsoparticularlysuitedfor
thesetasksgiventheutilityoflanguageforscaffoldingToM[AstingtonandBaird,2005,deVilliers,
2007,2021,Kosinski,2023,Gandhietal.,2024]. WeadvancerecentresearchonLLM-basedagents
inembodiedsocialworlds[Wangetal.,2023b,a,Brohanetal.,2023a,Parketal.,2023]withthe
HypotheticalMindsmodel. Ouragentproducesadaptivepoliciesincompetitive,cooperative,and
mixed-motive multi-agent scenarios with concealed information by generating hypotheses about
otheragents’latentstateswithaTheoryofMindmodule.
TheTheoryofMindmoduleinHypotheticalMindsgenerateshypothesesaboutotheragents’strate-
gies,goals,andcapabilitiestofacilitateeffectivecoordinationorcounter-strategies. High-levelplans
are subsequently passed to a subgoal module to sequence together embodied action plans. The
ToMmodulesimultaneouslyevaluatesmultiplehypothesesuntilahypothesisprovidesasufficient
explanationofthe data [O’Dohertyetal.,2021,Tenenbaumetal.,2006, Gershmanetal.,2015].
∗Correspondingauthor:locross@stanford.edu
Code:https://github.com/locross93/Hypothetical-Minds/
4202
luJ
9
]IA.sc[
1v68070.7042:viXraFigure1: HypotheticalMindsarchitectureanddepictionofmodelworkflow
HypothesesareevaluatedbyassessingthequalityofthepredictionstheLLMmakesaboutother
agents’ behavior conditional on each hypothesis. Additionally, by displaying the highest-valued
hypothesesintheprompt,theLLMisabletoself-improveitsreasoningbasedonpreviouslygenerated
hypotheses. Thus,ouragentfindsusefulexplanationsofotheragents’behaviorsin-context,affording
ittheabilitytoadapttoinferredstrategiesandachievehighrewards.
WeevaluateourmodelonfoursubstratesintheMeltingPotMARLbenchmarkthatencompasses
diversechallengesandsocialdynamics[Agapiouetal.,2022]. CollaborativeCookingAsymmetric
requireseffectivecoordinationanddivisionoflaboramongagents. RunningWithScissorsneces-
sitatesstrategicreasoningaboutopponents’policiesandtheabilitytoexploitpredictablepatterns
inacompetitivesetting, withtheeight-playerversionofferingauniquechallengetothemodel’s
scalability. Moreover,themixed-motiveenvironmentPrisoner’sDilemmainvolvesatensionbetween
individualandcollectiveinterests. Diverseevaluationscenariosstresstestplayingwithawidearray
ofagentswithafixedoradaptivepolicies,necessitatingcontextualadaption. Ourcontributionsareas
follows:
• WeproposetheHypotheticalMindsmodel(HM),anembodiedLLM-basedagentformulti-
agent environments with concealed information that integrates modular components for
perception,memory,andhierarchicalplanningovertwolevelsofabstraction.
• HMincorporatesanovelTheoryofMind(ToM)module,whichgenerates,evaluates,and
refines hypotheses about other agents’ strategies or goals in natural language. Through
ablations and comparisons against LLM-agent baselines, we identify the critical role of
hypothesisevaluationandrefinementwithintheToMModule.
• WedemonstratetheeffectivenessofHypotheticalMindsacrossmultiplemulti-agentenviron-
mentsintheMeltingPotbenchmark,includingcompetitive,collaborative,andmixed-motive
domains,and30distinctevaluationscenarios. OuragentsignificantlyoutperformsLLM-
agentandRLbaselinesineveryenvironmentandinalargemajorityofevaluationscenarios,
showcasingitsgeneralizability.
2 RelatedWork
2.1 LLM-basedAgents
Aburgeoningareaofresearchinvolvesbuildingautonomousagentsrootedinlargelanguagemodels
Wang et al. [2023b], Sumers et al. [2023]. This involves deploying LLMs as central controllers
acrossmanydifferentdomainsbyleveragingtheirextensivebackgroundknowledgefromtraining.
2ApplicationsspanawiderangefromequippingLLMswithexternaltoolstointerfacewithdatabases
andAPIs[Schicketal.,2023,Shenetal.,2023,Qinetal.,2023,Geetal.,2023,Yaoetal.,2022]to
usingthemforhigh-levelplanningandcontrolinrobotics[Huangetal.,2022,Brohanetal.,2023b,
Ranaetal.,2023,Brohanetal.,2023a]. Themostrelevantbranchofthisresearchdirectionincludes
workswhereLLMsareusedasplannersinembodiedvirtualenvironments. Voyagerautonomously
buildscomplexskillsinMinecraftbystoringandretrievingbehaviorsinaskilllibraryofexecutable
codeandusestheskilllibrarytosolveprogressivelyhardertasks[Wangetal.,2023a]. Octopusand
Jarvis-1usevision-languagemodelstoplanlonghorizonbehaviorsinvirtualenvironments[Yang
etal.,2023,Wangetal.,2023d]. DynalangandDECKARDuselanguagetoguideworldmodeling
formodel-basedRLpoliciesinvirtualenvironments[Linetal.,2023,Nottinghametal.,2023]. In
thiswork,weuseanLLMforlonghorizonhigh-levelplanningandpredictingthefuturestatesof
otheragentsinmulti-agentenvironments.
PreviouspapershavealsoincorporatedLLM-basedagentsintoembodiedmulti-agentenvironments.
Parketal.[2023]introduceainteractivesimulationofarichsocialenvironment,whereeachagent
autonomouslyselectsgoalsandbuildsrelationshipswithothers. Weextendthecognitivemodule
frameworkdevelopedinthisworkformulti-agentenvironmentsofvarieddynamics. SAMAuses
an LLM to plan out sequences of subgoals for language-based goal-conditioned RL policies in
environmentsrequiringmulti-agentcoordinationLietal.[2023]. Anotherstudybuildscooperative
embodiedagents,byusinganLLMforplanningandcommunicationbetweenagentsZhangetal.
[2023b]. ProAgentdevelopsamethodforimprovingzero-shotcoordinationinOvercookedbyusing
an LLM to infer the intentions of teammates based on the present state Zhang et al. [2023a]. In
thepresentwork,wepresentageneralizablemethodthataddressesthechallengeofinferringother
agentsintentions’acrossaspectrumofsocialdynamics.
2.2 ReasoningandHypothesisSearchwithLLMs
LLMshaveshownimpressivereasoningabilities, augmentedbyChain-of-Thoughtmethodsthat
scaffoldthethoughtprocess[Weietal.,2022,Zhangetal.,2023c]. [Wangetal.,2023c]investigates
LLMs’inductivereasoningbygeneratingandevaluatinghypothesesontheAbstractionandReasoning
Corpus (ARC), while [Qiu et al., 2023] refines LLM-generated hypotheses using task-specific
symbolicinterpreters. Similarly,wegenerate,evaluate,andrefinehypothesesbasedonfeedback,
computingvaluesforeachhypothesisbypredictinganotheragent’sgoals. STaR[Zelikmanetal.,
2022]alsolearnsfromfeedbackbyfinetuninglanguagemodelsonrationalesthatproducedcorrect
answers. [Akata et al., 2023] assessed LLMs’ ability to reason in matrix games requiring ToM,
showing mixed results. We demonstrate that asking the LLM to predict the opponent’s actions
improvesresults,highlightingtheimportanceofToMreasoningingame-theoretictasks.
2.3 Cognitivemodelsinpartially-observableenvironments
Ourmethodhasanalogsinthecomputationalmodelingofanimalandhumandecision-making,and
ourworkaddstoemergingpromisingevidencethatLLMscanoperateascognitivemodels[Binzand
Schulz,2023]. Numerousmethodshavebeenproposedforgeneratinghypothesesabouthiddenor
latentstatesinpartially-observablesettings. MethodsfromBayesiannonparametricstats,suchas
theChineserestaurantprocess,suggestthatwhenanimalsandhumansarelearningtheygeneratean
unboundednumberofpossiblelatentcausesoftheobserveddata,untilasatisfactoryexplanationis
found[Gershmanetal.,2015]. Thisframeworkexplainsawealthofbehavioralphenomenafrom
categorization[Anderson,1991,Sanbornetal.,2010,GershmanandNiv,2013,Gershmanetal.,
2015],Pavlovianconditioning,andmore. OurLLM-basedagentsimilarlyiterativelygeneratesand
refineshypothesesabouttheopponent’sstrategy,untilaexplanationalignswiththeobserveddata’s
likelihood. Mixture models and mixture of experts models are similarly useful for modeling the
humandecision-makingprocessunderuncertainty,byarbitratingbetweenmultiplehypothesesin
proportion to the reliability of their predictions [Frank and Badre, 2012, O’Doherty et al., 2021,
Archambeauetal.,2023]. ResearchershaveextendedthisPOMDPmodelingframeworktomulti-
agentsettingsbyconstructingmodelsthatexplicitlymodelotheragents’beliefsandgoalswhen
interactingwiththem,handlinguncertaintyoverthishiddeninformationwithaBayesianframework
[Baker et al., 2017, Rusch et al., 2020]. Our framework also explicitly prompts a LLM to infer
its opponent’s goals/strategy and weighs the uncertainty over these inferences with a hypothesis
evaluationmechanism. ThismechanismisalsoinspiredbycognitivemodelsandemploysaRescorla-
3Figure2: TheoryofMind(ToM)ModuleforRunningWithScissors. Thiscognitivemodulereceives
inputintheformofinteractionhistoryandoutputsatargetinventoryasagoalforthesubgoalmodule.
Informationisprocessedin5steps,includingusingtheavailableinformationtogenerate,evaluate,
andrefinehypothesesabouttheopponent’sstrategy.
Wagnerlearningrule,amethodcommonlyuseformodelingthedecision-makingprocess[Rescorla,
1972,DawandTobler,2014].
3 Method
3.1 Partially-observableMarkovgames
Ourmethodisdirectlyapplicabletoanymulti-agentenvironmentwherestatesarepartiallyobservable
and agent(s)’ policies are hidden. We formally define this as a Markov game for N players in a
partially observable setting. Let the finite set S represent the possible states of the game. Each
playerireceivesobservationsgivenanobservationfunctionχi :S →O,representingtheirlimited
pointofview. Additionally,eachplayericantakeactionsfromtheiractionspaceAi,andwhenall
playerschooseactions(a1,...,aN) ∈ A1 ×···×AN := A,thestatetransitionsaccordingtoa
probabilitydistributionT :S×A→D(S). Therewardfunctionforeachplayeriisrepresentedas
ri :S×A→R,mappingthecurrentstateandjointactionstoareal-valuedreward.
3.2 HypotheticalMindsModel
TheHypotheticalMindsmodelconsistsofseveralcognitivemodulesthataltogetherformanem-
bodiedLLMagent(Figure1). Theegocentricobservationsarerepresentedbyatextualmap/state
representation,whichisaddedtoamemorysystemaftereverystep. Thememorysystemalsologs
rewards and other important state information like the inventories from previous interactions in
RunningWithScissors. TwocognitivemodulesdependonanLLM,aTheoryofMindmoduleanda
Subgoalmodule,whichoutputhigh-levelgoalsandactionplansrespectively. Anactionplannertakes
anactionplan(i.e. "movetocoordinate(13,5)")andcreatesasequenceofactionsthatachievesthat
actionplanwithapathfindingalgorithm. Eachcognitivemoduleisexplainedinmoredetailinthe
Appendix. Belowwedescribethekeynovelcontributionsofourmethodthatimplementhierarchical
planning.
4TheoryofMindModule Inmulti-agentenvironments,otheragents’behaviorcanbeinfluenced
by various latent variables, such as their strategies, goals, competence levels, and locations in
space. Theselatentvariablesareoftennotdirectlyobservableandmustbeinferredfromtheagent’s
observations. WerepresenttheselatentvariablesasamultidimensionalspaceΘ=θ ,θ ,...,θ ,
1 2 m
whereeachdimensionθ correspondstoaspecificlatentvariable. TheToMmodulemaintainsaset
i
ofhypothesesH = h ,h ,...,h initsworkingmemory, whereeachhypothesish isanatural
1 2 n t
languagerepresentationofabeliefaboutthelatentvariablesh =p(Θ)(Figure2). Ahypothesisat
i
timetisgeneratedbyaskinganLLMtoinferanotheragent’sstrategy,conditionedonHM’smemory
MofimportantpastobservationsO. Additionally,theLLMisshownthetopkvaluedpreviously
generatedhypotheses,suchthatitcanperformhypothesisrefinement(seeAppendixandcodefor
moredetailsandprompts):
h =LLM(M,h t) (1)
t <
whereMisamemorybufferstoringinformationaboutanagent’sactions,observations,andrewards.
Eachhypothesish isscoredbasedonhowwellitpredictstheotheragent’sfuturebehavior,noted
i
hereformallyasadistributionoftrajectoriesp(τ). Weformalizethisscoringmechanismusinga
likelihoodfunctionp(τ|h )representingtheprobabilityofanagentexhibitingtrajectoryτ giventhe
i
hypothesish . Thebesthypothesish∗isselectedusingtheMaximumaPosteriori(MAP)estimate:
i
p(τ|h )p(h )
h∗ =argmaxp(h |τ)=argmax i i (2)
i p(τ)
hi∈H hi∈H
where p(h ) is the prior probability of hypothesis h and p(τ) is the marginal probability of the
i i
observedactionaandhasnoeffectontheargmax. Thelikelihoodisapproximatedbyahypothesis
evaluationmechanismdescribedbelow. TheLLMpredictstheotheragent’sfuturebehaviorcon-
ditionedoneachhypothesisseparately. Hypothesesleadingtocorrectpredictionswillhavehigher
valuesreflectinghigherlikelihoods. Thepriorp(h )correspondsbothtothebackgroundknowledge
i
embeddedintheweightsofanLLMfrompretrainingandtotherefinementmechanismthatshows
the top valued hypotheses to the LLM when the LLM generates a hypothesis. By continuously
updating and selecting the best hypothesis based on observed information, the ToM module can
effectivelyinferthelatentvariablesgoverningtheotheragents’behaviorandadaptitsownstrategies
accordingly.
HypothesisEvaluation Drawingoncognitivemodelingapproaches[Rescorla,1972,Dawand
Tobler, 2014], multiple hypotheses are scored with a value system V = E[r] where r reflects
hi
intrinsic reward based on the accuracy of the predictions the hypothesis generates. We compute
self-supervised intrinsic rewards bootstrapped from the LLM’s own predictions. Let ϕ(τ) be a
ˆ
particularbehavior,afeaturefromanobservedtrajectory,andϕ(τ)bethepredictedbehaviorbythe
LLM,suchastheinventoryplayedbyanagentinMeltingPotenvironments. Theintrinsicreward
functionr canthenbedefinedas:
i
(cid:40)
ˆ
c ifϕ(τ)=ϕ(τ)
r =
i ˆ
−c ifϕ(τ)̸=ϕ(τ)
wherecisahyperparameter. V isthendynamicallyupdatedwithaRescorlaWagnerupdaterule
hi
[Rescorla,1972],expressedas:
δ =r −V
i hi
V ←V +α·δ
hi hi
modulatedbylearningrateαviaapredictionerrorδ. Thelearningratedictateshowmuchtoweigh
recentinteractions,ausefulpropertywhenplayingagainstevaluationscenarioswhereagentschange
theirstrategywithinanepisode. WhenthevalueofahypothesismeetsathresholdV ,theToM
thr
module marks the hypothesis as validated and uses this hypothesis to condition high-level plans
(usingthehighest-valuedoneifmultiplepassthethreshold). Thishypothesiswillthencontinuetobe
usedforplanninguntilitnolongermakesgoodpredictionsanditsvaluesubsequentlyfallsbelowthe
threshold. Ifnohypothesismeetsthethreshold,thenthelatestgeneratedhypothesis(withthemost
updatedinformation)isusedforconditioninghigh-levelplans.
5Figure3: Resultsforallmodels. Averagerewardperepisode(withnormalizedstepsforvariable
lengthepisodes)foreachenvironmentandscenario. 5seedsaregeneratingforeachmodel,with
errorbarsreflectingtheSEMacrossthose5episodes.
Conditioning High-Level Plans The ToM module then conditions its high-level plans on the
inferredlatentvariablesrepresentedbythehypotheses. Ahigh-levelplanz isanaturallanguage
descriptionofHM’soverallstrategy,goal,orintention,conditionedonthebesthypothesish∗and
memoryofpastevents:
z =LLM(M,h∗) (3)
By conditioning the high-level plans on the hypotheses, HM can adapt its strategy based on its
understandingoftheotheragents’latentstates.
SubgoalModuleFinally,theSubgoalmoduleselectsasequenceofsubgoals. Letg=g ,g ,...,g
1 2 k
beasequenceofsubgoals,whereeachsubgoalg isanactionorshortsequenceofactionsthatthe
i
agentneedstotaketoachievethehigh-levelplanz:
g=LLM(O,M,z) (4)
The sequence is generated by conditioning the LLM on the high-level plan, observations, and
memory, and prompt to achieve the high-level plans. The LLM outputs a sequence of specified
subgoal function calls, which are then parsed and mapped to the corresponding actions in the
environmentbyahardcodedActionPlanner(seeAppendix).
4 Experiments
Hereweinvestigatethefollowingtoanalyzethegeneralizabilityandscalabilityofourmethod:
Q1. HowdoesHypotheticalMindsperformcomparedtoLLMagentandRLbaselinesinembodied
competitivezero-sumenvironments?
Q2. HowdoesHypotheticalMindsperformcomparedtoLLMagentandRLbaselinesinacollabora-
tivedomainthatrequiresadaptationtoapartner’sroleandcompetence?
Q3. HowdoesHypotheticalMindsperformcomparedtoLLMagentandRLbaselinesinamixed-
motivesetting?
Q4. DoestheHypotheticalMindsagentscaleeffectivelytoenvironmentswithlargerpopulationsof
agents?
Q5. How do the different components of the Hypothetical Minds agent and the Theory of Mind
modulecontributetoitsoverallperformance?
WedirectlytestourLLM-basedagentontheevaluationscenariosinfourMeltingPotenvironments
(Fig. 3). Thekeyevaluationmethodistotestagentsagainstdifferentbotswithvariouspolicies.
Acrossenvironments,thisconsistsof30distinctevaluationscenarios. Crucially,ouragenthasno
knowledgeaboutwhichstrategiestheymaybeplayinginthepromptsgiven. Strategieshavetobe
ascertainedonlinewithinanepisodeviain-contextlearning.
6Figure4: HM’srewardpernumberofinteractionsbeforeorafterahypothesismeetsthevalidation
threshold and is used for high-level strategy selection in RWS. Vertical green line indicates the
averagerewardatthepointwhereahypothesisisvalidated,andpositiveandnegativenumbersonthe
x-axisindicatehowmanyinteractionsbeforeorafterthispoint. Shadedregionrepresentstherange
wherethegoodhypothesisistypicallyfirstgeneratedwitha95%confidenceinterval.
4.1 Baselines
ReActsynergizesreasoningandactinginlanguagemodels,allowingthemtogeneratebothreasoning
tracesandtask-specificactionsinaninterleavedmanner[Yaoetal.,2022]. Reflexionincludesthree
maincomponents: anActormodulethatgeneratesactionsandtext,anEvaluatorthatscoresthese
actions, and a Self-Reflection module that uses the evaluations to provide constructive feedback
storedforsubsequentuse[Shinnetal.,2024]. PlanReActToprovideahierarchicalbaselinetotest
againstourhierarchicalmodel,weincludethePlanReActarchitectureintroducedin[Liuetal.,2023].
Thisstructureallowstheagenttoplanbeforeinteractingwiththeenvironment. PPOisamodel-free
RLbaseline[Schulmanetal.,2017]andwetrainagentsinapopulationofmodelswiththesame
parameters. Promptsandarchitecturesaresharedacrossbaselinestoprovideafaircomparisonand
naturalablationsofourmodel. TheSubgoalmoduleprovidesabaselineactorsharedbetweenLLM
baselines, and the only difference between PlanReAct and Hypothetical Minds is that high-level
planningismediatedbytheTheoryofMindmodule,includinghypothesisgeneration,evaluation,
andrefinement.
4.2 CompetitiveEnvironments
RunningWithScissorsintheMatrixRepeated(RWS).Azero-sumcompetitiveenvironmentwith
twoplayersmovingaroundamapandcollectingyellow,purple,orblueresourcesthatcorrespond
torock, paper, andscissorsrespectively. Zappingyouropponentcausesaninteraction, withone
agentgettingpositiverewardandtheotheragentgettinganoppositenegativerewardaccordingtothe
inventoriesofresourcespickedupbyeachplayer,mirroringtherock,paper,scissorsmatrixgame.
RWSpresentsninedistinctevaluationscenarios,whichrangeincomplexity. Theseincludethree
straightforwardstrategieswhereopponentsconsistentlyplayrock,paper,orscissors. Theremaining
scenariosintroducemoredynamicandadaptivestrategies(seeAppendixfordetailsandfulllist).
Thereforeinordertosucceedonthescenarios, anagentneedstocorrectlyinferthestrategyand
exploitit. Againstthesimplepolicies,theagentshouldplaythesametypeofinventoryeveryround
ratherthanplayingrandomlyoranticipatingachangeinitsopponent’spolicy. Incontrast,success
againsttheadaptivestrategiesdemandsnotonlytheanticipationoftheopponent’snextmovebased
onpersonalpreviousplaysbutalsoselectingthemostadvantageouscounter.
Figure3andAppendixTable1demonstratehowHypotheticalMindsmodelconsistentlyachieves
largemagnituderewardsandperformsreliablybetterthanthebaselinesoneverysinglescenario.
HypotheticalMindsperformsthebestonthestaticstrategies,scenarios0,6,7,8representingfixed
policiesthatplayforthesameinventoryoneveryinteraction(6:rock,7:paper,8:scissors,0:random
samplefrom6-8). Thereforeitisabletoexploitthestaticstrategyreliablyonceitcorrectlyinfers
it. Theagentisalsoabletoconsistentlyreturnpositiverewardsagainstthedifficultscenario1,the
adaptive bot that plays the best response to your last round. In contrast, baselines are failing to
7Figure5: ComparingbaseLLMmodels. 3seedsaregeneratedperLLM.
achieveconsistentpositiverewards. PlanReActistheonlyothermodeltoachievepositiverewards
onthebestresponsebot,illustratingtheusefulnessofahierarchicalstructureforthisevaluationin
particular. Reflexionperformssecondbestoverall,underscoringthevalueofevaluativereflectionfor
thisenvironment.
Figure4showcasesHM’sdynamics,depictingtheagent’srewardperinteractionbeforeandafter
hypothesisvalidation. Uponvalidation,theagentconsistentlyachieveshighpositivereturns,while
rewardsarenearzeroornegativeduringtheinformation-gatheringphase. Theupwardtrajectory
inrewardaftergeneratingagoodhypothesisandthesignificantincreasejustbeforethevalidation
thresholddemonstratetheagent’sabilitytoexploitaccuratehypotheseseffectively.
RunningWithScissorsintheMatrixArena(RWSArena). Aneight-playerextensionofRWS,
where the agent controls one player against a population of 7 strategies. This adds additional
complexitytothedecision-makingprocessandteststhescalabilityofmodels,asagentsnowmust
inferthestrategiesofseparateagentsandmaintainthisinformationinmemory. Thus,Hypothetical
Mindsmaintainsseparatehypothesisevaluationstreamsforeveryagent. Additionally,inorderto
maximizereward,agentsshouldonlyinteractwithopponentsthatitknowsitcanbeatwithitscurrent
inventory. Modelsarethereforetaskedwithintegratinguncertaintyandseekingoutopponentsfor
whichtheyhavehighconfidenceabouttheirstrategy.
HypotheticalMindsachieveshigherrewardsthanthebaselinesinRWSArena(Figure3)andisthe
bestmodelon6/8scenarios(AppendixTable1). HMperformsparticularlywellonhomogeneous
populations of rock, paper, or scissors (scenarios 5-7). Scenarios 2-4 consist of heterogeneous
populationswhere2/3rdsofthepopulationareonepurestrategyandtheremaining1/3rdrepresents
thepurestrategythatwouldbeatthebestresponsetothemajoritystrategy(e.g.,scenario2is2/3rock
and1/3scissors). HMperformsthebeston2outof3ofthesedifficultscenarios,andperformance
forscenario4isdraggeddownbyonehighlynegativeepisodeinwhichtheagentwasexploitedby
theminoritystrategy. Incontrast,allbaselinemodelsstruggletoachieverewardsofmorethan10on
averageinnearlyeveryscenario. Thishighlightsthedifficultyoftheenvironment,forwhichproper
coordinationbetweenhigh-levelplans,theembodiment,andmemoryiscrucialforsuccess. These
resultsalsosuggestthatHypotheticalMindsscaleswelltopopulation-basedenvironmentsinwhich
youneedtoeitherhandledistinctagentsdifferentlyormakepopulation-levelinferences.
4.3 CollaborativeCookingAsymmetric
IntheCollaborativeCooking: Asymmetricenvironment,twoplayersondistinctsidesofadivided
kitchenmustcollaboratetoefficientlycooktomatosoup. Thelayoutprovidesdistinctadvantagesto
eachside—onesideisclosertothegoaldeliverybutfartherfromthetomatodispenser,andviceversa
fortheotherside. Tomaximizerewards,thetwoplayersshouldspecializebasedontheirproximity
toresources: onehandlestomatodispensinganddelivery,andtheothermanagesdishretrievaland
soupdelivery. Evaluationscenarioschallengethefocalagenttodemonstratedualformsofadaptation:
adjustingtothespecializedroledictatedbytheirsideofthekitchenandtothevaryingcompetenceof
theirpartner,fromskilledandspecializedtoentirelyunresponsive.
Again, HypotheticalMindsachieveshigherrewardsthanthebaselinesoneveryscenario(Figure
3). Interestingly,HMperformssignificantlybetterthanthebaselinesonthescenarioswherethere
is a functional partner (Appendix Table 1). This suggests that if there is value in a partner, HM
cantakeadvantageofthisandadaptitsbehavioraccordingly,highlightingthemodel’susefulness
forcomplex,dynamicenvironmentswherecooperativeinteractioniscrucial. TheLLMbaselines
performrelativelywellwiththenegligentpartner, wheresuccesshingesonrepeatedlyexecuting
8Figure6: ComparingdifferentversionsofHM.ErrorbarsreflectSEMacross5episodes.
anintuitivesequenceofactions—fillinganddeliveringpots—withouttheneedtotakeintoaccount
theactionsofanothermind. Forinstance,baselineLLMagentsstruggledwhenencounteringapot
alreadyatcapacitywhileattemptingtoaddtomatoes. Ontheotherhand,RLshowedtheopposite
relative weakness. PPO performs moderately with the skilled partner but has a dramatic lack of
generalizationtotheunhelpfulpartnerscenario,asitisusedtorolespecializationfromselfplay.
4.4 MixedMotive: Prisoner’sDilemmaintheMatrixRepeated
In the Prisoner’s Dilemma in the Matrix (PD) environment, agents navigate a similar grid world
toRWSandcollectresourcescorrespondingtocooperationordefectionintheiteratedprisoner’s
dilemma game. The payoff matrix incentivizes mutual defection in a single interaction, but the
highesttotalwelfareisachievedthroughmutualcooperationacrossanepisode.
HypotheticalMindsachievesthehighestrewardoverallandin5/10scenarios(Figure3,Appendix
Table1). Thishighlightsitsabilitytoperceivethebackgroundagent’sstrategyandadaptaccordingly.
HMoutperformsthebaselinesrelativelymorewithdynamicpartners. Withtit-for-tatforexample
(scenarios5and6),HypotheticalMindsachievesthehighestscore,byengaginginmoreconsistent
cooperationwhiledemonstratingsomeforgivenesstoavoidcyclesofalternatingdefection,apattern
thatplaguesbaselines. Inscenarios8and9,HypotheticalMindsshowcasesacapacityfor"corrective
punishment." By initially defecting against exploitative partners, it persuades them to switch to
conditional cooperation. The agent then shifts to a forgiving cooperation strategy to maintain a
mutuallybeneficialequilibrium.
Relativetotheotherthreeenvironmentstested,HMperformscomparablytothebaselinesinPD,and
allLLMbaselinesperformsimilarlyinthemajorityofthescenarios. Thissimilaritylikelyarises
fromtheirconvergenceonsimpleheuristicstrategies,suchasconsistentlydefectingagainststatic
bots(theoptimalapproach)andemployingatit-for-tatstrategyagainstadaptivebots. Again,this
highlightstheimportanceoffocusingonthemoredynamicstrategiesaspreviouslydiscussed. All
modelsstrugglewiththegrimreciprocators,astestingdefectionisneededtoperformwellonthe
taskoverall,butisnottoleratedbythesebots.
4.5 Ablations
Aspreviouslydiscussed, wecomparedbaselineLLMagentswithourmodelstoperformnatural
ablations,maintainingsimilarpromptsbutvaryinginformationflows. WealsocomparebaseLLMs
betweenGPT4(default),GPT3.5,andLlama3(Figure5). GPT4performsbyfarthebestacross
environments,andGPT3.5performspoorly. Llama3’sperformanceliesinbetweenthesetwoon
RWSRepeatedandCollaborativeCooking,andslightlybelowGPT3.5onPrisoner’sDilemma. Due
toinputcontextlimitations,wecouldnotrunLlama3onRWSArena.
We conducted an additional detailed ablation analysis on the ToM module for the challenging
environment,RWSRepeated. VanillaMindPrompting(VMP):UtilizesasingleAPIcallforall
ToMmodulesteps,excludinghypothesisevaluationandrefinement(seeAppendixandcodeformore
9details). ModularMindPrompting(MMP):Ablateshypothesisevaluationandrefinement,while
introducingamoremodularapproachwheretheLLMisqueriedseparatelyforeachstep. Hypothesis
EvaluationandRefinementw/oMindPrompting(HE+HR):AblatesToMpromptingandisolates
hypothesisevaluationandrefinement. Promptsdonotincludeopponentstrategypredictionsbutask
forahigh-levelstrategy,treatedasahypothesisandevaluatedbasedonrewards(seeAppendixfor
details). ModularMindPrompting+HypothesisEvaluation+HypothesisRefinement(MMP+
HE+HR):FullHypotheticalMindsmodel,addinghypothesisrefinementtoMMP+HE.
Figure 6 shows key scenario results and Figure 9 (Appendix) shows all scenarios, with MMP
outperformingVMPinmostcases. MMP+HEandMMP+HE+HRachievedpositivereturnsof
10+ineveryscenario,withthefullmodelperformingwellagainstthechallengingbestresponse
bot (SC 1). This suggests that modular structures and hypothesis evaluation enhance reasoning
stability and strategy optimization. GPT-4 could identify correct hypotheses through interaction
historyreasoning,thoughnotconsistently,highlightingtheimportanceofhypothesisevaluationin
maintainingmultiplepotentialstrategies(seeFigure7and8inAppendix).
TheHE+HRmodel,whichlacksToMprompts,excelledagainstadaptiveopponentsbutstruggled
withfixedstrategies. Thisindicatesthatwithoutexplicitmodelingoftheopponent’sstrategy,the
LLMfailstorecognizeit’sfacingastaticstrategy,insteadexpectinganadaptiveopponentlikeitself.
Beingabletorepresentanotheragent’sbeliefsandpreferencesasbeingdifferentthanyourownisa
hallmarkofTheoryofMind[Rakoczy,2022]. Additionally,HE+HRproducedlonger,disorganized
responseswithirrelevantguidelines,resultinginlesseffectivehypotheses(seeAppendixFig. 10).
5 Conclusion
HereweevaluatetheHypotheticalMindsmodelonchallengingmulti-agentenvironmentswithvaried
dynamics. Onelimitationofourmethodisthehumanintheloopcomponentnecessarytosetupthe
scaffoldingandpromptingfortheagent. Additionally,knowledgeofthegamerulesandmechanics
arelistedintheprompts. Anavenueforfutureresearchislearningtheseconceptsandtheappropriate
typesofscaffoldingwithinthecognitivemodulesautonomouslyfromenvironmentalfeedback.
10References
JohnPAgapiou,AlexanderSashaVezhnevets,EdgarADuéñez-Guzmán,JaydMatyas,YiranMao,
PeterSunehag,RaphaelKöster,UdariMadhushani,KavyaKopparapu,RamonaComanescu,etal.
Meltingpot2.0. arXivpreprintarXiv:2211.13746,2022.
Elif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz.
Playingrepeatedgameswithlargelanguagemodels. arXivpreprintarXiv:2305.16867,2023.
JohnRAnderson. Theadaptivenatureofhumancategorization. Psychologicalreview,98(3):409,
1991.
KimArchambeau,JoaquinaCouto,andLeendertVanMaanen. Non-parametricmixturemodelingof
cognitivepsychologicaldata: Anewmethodtodisentanglehiddenstrategies. BehaviorResearch
Methods,55(5):2232–2248,2023.
Janet Wilde Astington and Jodie A Baird. Why language matters for theory of mind. Oxford
UniversityPress,2005.
ChrisLBaker,JulianJara-Ettinger,RebeccaSaxe,andJoshuaBTenenbaum. Rationalquantitative
attributionofbeliefs,desiresandperceptsinhumanmentalizing. NatureHumanBehaviour,1(4):
0064,2017.
MarcelBinzandEricSchulz. Turninglargelanguagemodelsintocognitivemodels. arXivpreprint
arXiv:2306.03917,2023.
AnthonyBrohan,NoahBrown,JusticeCarbajal,YevgenChebotar,XiChen,KrzysztofChoromanski,
TianliDing, DannyDriess, AvinavaDubey, ChelseaFinn, etal. Rt-2: Vision-language-action
modelstransferwebknowledgetoroboticcontrol. arXivpreprintarXiv:2307.15818,2023a.
AnthonyBrohan,YevgenChebotar,ChelseaFinn,KarolHausman,AlexanderHerzog,DanielHo,
JulianIbarz,AlexIrpan,EricJang,RyanJulian,etal.Doasican,notasisay:Groundinglanguage
inroboticaffordances. InConferenceonRobotLearning,pages287–318.PMLR,2023b.
Nathaniel D Daw and Philippe N Tobler. Value learning through reinforcement: the basics of
dopamineandreinforcementlearning. InNeuroeconomics,pages283–298.Elsevier,2014.
JillGdeVilliers. Theinterfaceoflanguageandtheoryofmind. Lingua,117(11):1858–1878,2007.
JillGdeVilliers. Therole(s)oflanguageintheoryofmind. InTheneuralbasisofmentalizing,
pages423–448.Springer,2021.
MichaelJFrankandDavidBadre. Mechanismsofhierarchicalreinforcementlearningincorticostri-
atalcircuits1: computationalanalysis. Cerebralcortex,22(3):509–526,2012.
KanishkGandhi, Jan-Philipp Fränken, Tobias Gerstenberg, andNoahGoodman. Understanding
social reasoning in language models with language models. Advances in Neural Information
ProcessingSystems,36,2024.
YingqiangGe,WenyueHua,JianchaoJi,JuntaoTan,ShuyuanXu,andYongfengZhang. Openagi:
Whenllmmeetsdomainexperts. arXivpreprintarXiv:2304.04370,2023.
SamuelJGershmanandYaelNiv.Perceptualestimationobeysoccam’srazor.Frontiersinpsychology,
4:623,2013.
SamuelJGershman,KennethANorman,andYaelNiv. Discoveringlatentcausesinreinforcement
learning. CurrentOpinioninBehavioralSciences,5:43–50,2015.
WenlongHuang,FeiXia,TedXiao,HarrisChan,JackyLiang,PeteFlorence,AndyZeng,Jonathan
Tompson,IgorMordatch,YevgenChebotar,etal. Innermonologue: Embodiedreasoningthrough
planningwithlanguagemodels. arXivpreprintarXiv:2207.05608,2022.
MichalKosinski. Evaluatinglargelanguagemodelsintheoryofmindtasks. arXive-prints,pages
arXiv–2302,2023.
11Wenhao Li, Dan Qiao, Baoxiang Wang, Xiangfeng Wang, Bo Jin, and Hongyuan Zha. Se-
mantically aligned task decomposition in multi-agent reinforcement learning. arXiv preprint
arXiv:2305.10865,2023.
JessyLin,YuqingDu,OliviaWatkins,DanijarHafner,PieterAbbeel,DanKlein,andAncaDragan.
Learningtomodeltheworldwithlanguage. arXivpreprintarXiv:2308.01399,2023.
ZhiweiLiu,WeiranYao,JianguoZhang,LeXue,ShelbyHeinecke,RitheshMurthy,YihaoFeng,
ZeyuanChen,JuanCarlosNiebles,DevanshArpit,etal. Bolaa: Benchmarkingandorchestrating
llm-augmentedautonomousagents. arXivpreprintarXiv:2308.05960,2023.
KolbyNottingham,PrithvirajAmmanabrolu,AlaneSuhr,YejinChoi,HannanehHajishirzi,Sameer
Singh,andRoyFox. Doembodiedagentsdreamofpixelatedsheep?: Embodieddecisionmaking
usinglanguageguidedworldmodelling. arXivpreprintarXiv:2301.12050,2023.
JohnPO’Doherty,SangWanLee,RezaTadayonnejad,JeffCockburn,KyoIigaya,andCarolineJ
Charpentier.Whyandhowthebrainweightscontributionsfromamixtureofexperts.Neuroscience
&BiobehavioralReviews,123:14–23,2021.
JoonSungPark,JosephO’Brien,CarrieJunCai,MeredithRingelMorris,PercyLiang,andMichaelS
Bernstein. Generativeagents: Interactivesimulacraofhumanbehavior. InProceedingsofthe36th
AnnualACMSymposiumonUserInterfaceSoftwareandTechnology,pages1–22,2023.
YujiaQin,ShihaoLiang,YiningYe,KunlunZhu,LanYan,YaxiLu,YankaiLin,XinCong,Xiangru
Tang,BillQian,etal. Toolllm: Facilitatinglargelanguagemodelstomaster16000+real-world
apis. arXivpreprintarXiv:2307.16789,2023.
Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula,
Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et al. Phenomenal yet puzzling: Testing
inductivereasoningcapabilitiesoflanguagemodelswithhypothesisrefinement. arXivpreprint
arXiv:2310.08559,2023.
HannesRakoczy. Foundationsoftheoryofmindanditsdevelopmentinearlychildhood. Nature
ReviewsPsychology,1(4):223–235,2022.
KrishanRana, JesseHaviland, SouravGarg, JadAbou-Chakra, IanReid, andNikoSuenderhauf.
Sayplan:Groundinglargelanguagemodelsusing3dscenegraphsforscalabletaskplanning. arXiv
preprintarXiv:2307.06135,2023.
Robert A Rescorla. A theory of pavlovian conditioning: Variations in the effectiveness of rein-
forcementandnon-reinforcement. Classicalconditioning,Currentresearchandtheory,2:64–69,
1972.
TessaRusch,SaurabhSteixner-Kumar,PrashantDoshi,MichaelSpezio,andJanGläscher. Theoryof
mindanddecisionscience: Towardsatypologyoftasksandcomputationalmodels. Neuropsy-
chologia,146:107488,2020.
AdamNSanborn,ThomasLGriffiths,andDanielJNavarro. Rationalapproximationstorational
models: alternativealgorithmsforcategorylearning. Psychologicalreview,117(4):1144,2010.
TimoSchick,JaneDwivedi-Yu,RobertoDessì,RobertaRaileanu,MariaLomeli,LukeZettlemoyer,
NicolaCancedda,andThomasScialom. Toolformer: Languagemodelscanteachthemselvesto
usetools. arXivpreprintarXiv:2302.04761,2023.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
YongliangShen,KaitaoSong,XuTan,DongshengLi,WeimingLu,andYuetingZhuang.Hugginggpt:
Solvingaitaskswithchatgptanditsfriendsinhuggingface. arXivpreprintarXiv:2303.17580,
2023.
NoahShinn,FedericoCassano,AshwinGopinath,KarthikNarasimhan,andShunyuYao. Reflexion:
Languageagentswithverbalreinforcementlearning. AdvancesinNeuralInformationProcessing
Systems,36,2024.
12TheodoreRSumers,ShunyuYao,KarthikNarasimhan,andThomasLGriffiths. Cognitivearchitec-
turesforlanguageagents. arXivpreprintarXiv:2309.02427,2023.
JoshuaBTenenbaum,ThomasLGriffiths,andCharlesKemp. Theory-basedbayesianmodelsof
inductivelearningandreasoning. Trendsincognitivesciences,10(7):309–318,2006.
GuanzhiWang,YuqiXie,YunfanJiang,AjayMandlekar,ChaoweiXiao,YukeZhu,LinxiFan,and
AnimaAnandkumar. Voyager: Anopen-endedembodiedagentwithlargelanguagemodels. arXiv
preprintarXiv:2305.16291,2023a.
LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,JingsenZhang,ZhiyuanChen,Jiakai
Tang,XuChen,YankaiLin,etal. Asurveyonlargelanguagemodelbasedautonomousagents.
arXivpreprintarXiv:2308.11432,2023b.
RuochengWang, EricZelikman, GabrielPoesia, YewenPu, NickHaber, andNoahDGoodman.
Hypothesissearch: Inductivereasoningwithlanguagemodels. arXivpreprintarXiv:2309.05660,
2023c.
ZihaoWang,ShaofeiCai,AnjiLiu,YonggangJin,JinbingHou,BoweiZhang,HaoweiLin,Zhaofeng
He,ZilongZheng,YaodongYang,etal. Jarvis-1: Open-worldmulti-taskagentswithmemory-
augmentedmultimodallanguagemodels. arXivpreprintarXiv:2311.05997,2023d.
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,Denny
Zhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. Advancesin
NeuralInformationProcessingSystems,35:24824–24837,2022.
JingkangYang,YuhaoDong,ShuaiLiu,BoLi,ZiyueWang,ChenchengJiang,HaoranTan,Jiamu
Kang,YuanhanZhang,KaiyangZhou,etal. Octopus: Embodiedvision-languageprogrammer
fromenvironmentalfeedback. arXivpreprintarXiv:2310.08588,2023.
ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuanCao.
React: Synergizingreasoningandactinginlanguagemodels. arXivpreprintarXiv:2210.03629,
2022.
Eric Zelikman, Jesse Mu, Noah D Goodman, and Yuhuai Tony Wu. Star: Self-taught reasoner
bootstrappingreasoningwithreasoning. 2022.
CeyaoZhang,KaijieYang,SiyiHu,ZihaoWang,GuangheLi,YihangSun,ChengZhang,Zhaowei
Zhang,AnjiLiu,Song-ChunZhu,etal. Proagent: Buildingproactivecooperativeaiwithlarge
languagemodels. arXivpreprintarXiv:2308.11339,2023a.
HongxinZhang,WeihuaDu,JiamingShan,QinhongZhou,YilunDu,JoshuaBTenenbaum,Tianmin
Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language
models. arXivpreprintarXiv:2307.02485,2023b.
ZhuoshengZhang,YaoYao,AstonZhang,XiangruTang,XinbeiMa,ZhiweiHe,YimingWang,
MarkGerstein,RuiWang,GongshenLiu,etal. Ignitinglanguageintelligence: Thehitchhiker’s
guide from chain-of-thought reasoning to language agents. arXiv preprint arXiv:2311.11797,
2023c.
13A Appendix
Scenario AgentType
HM(ours) Reflexion ReAct PlanReAct PPO
RunningWith
ScissorsRepeated
0:Mixofall3 50.8±8.6 21.0±10.2 -1.0±3.9 5.5±3.6 -2.9±1.8
1:BestResponse 23.2±4.7 -3.5±3.5 -2.3±4.2 21.1±5.0 0.5±1.6
2:0∪1 54.6±4.3 29.3±12.5 17.1±7.2 16.6±6.1 -1.1±2.0
3:FlipAfter2 40.6±11.1 30.9±5.4 12.8±4.7 13.2±6.4 0.8±0.5
4:FlipAfter1 48.5±6.3 1.9±4.2 -1.5±6.1 9.0±5.1 -2.4±1.8
5:Gullible 12.9±5.1 8.2±3.2 2.5±1.9 7.2±3.4 0.1±0.6
6:Rock 50.5±4.4 17.9±9.5 14.3±8.6 15.6±4.3 1.2±2.0
7:Paper 59.6±4.0 13.2±6.8 2.7±2.3 13.0±4.4 -0.1±1.0
8:Scissors 63.8±5.1 31.0±6.5 29.0±4.3 14.4±3.3 -4.2±1.4
RunningWith
ScissorsArena
0:Mixofall3 13.3±6.9 1.1±3.6 3.5±3.2 -8.0±4.1 -0.5±1.5
1:Gullible -2.6±3.4 -1.8±2.3 -5.6±4.0 0.9±3.6 -0.4±1.0
2:Rock+FlipAfter2 16.2±3.9 -1.7±2.2 -4.4±2.0 2.1±3.2 -1.0±1.1
3:Paper+FlipAfter2 11.6±4.5 1.6±2.2 -4.2±3.6 0.9±3.6 1.3±1.4
4:Scissors+FlipAfter2 -2.6±5.7 10.6±2.2 -1.2±3.3 -2.4±3.1 -2.1±1.0
5:Rock 21.8±3.0 1.4±2.0 -4.3±3.1 -7.1±2.0 1.7±0.8
6:Paper 24.9±2.5 -1.1±1.4 -10.7±2.5 -4.0±3.1 -2.4±1.0
7:Scissors 32.1±1.8 11.8±4.5 11.6±2.0 -0.2±2.6 0.9±0.9
CollaborativeCooking
Asymmetric
0:SkilledPartner 628.3±35.1 292.6±38.2 225.4±60.0 402.9±70.1 244.0±76.1
1:Semi-skilledPartner 498.8±78.2 402.9±65.0 254.2±39.1 455.6±55.7 86.0±28.9
2:UnhelpfulPartner 426.8±42.5 398.1±33.6 383.7±21.4 292.6±36.7 0.0±0.0
Prisoner’sDilemmaintheMatrix
0:1∪2 40.6±12.9 44.5±13.2 57.9±13.0 37.0±15.3 20.7±5.7
1:Cooperator 78.7±2.4 87.2±1.7 77.3±1.0 78.8±2.2 40.0±5.4
2:Defector 21.9±0.5 22.1±0.8 24.0±0.3 22.5±0.5 9.1±2.0
3:GrimReciprocator 31.6±4.7 28.8±0.4 29.2±1.3 25.2±0.4 13.0±1.1
4:GrimReciprocator(2strikes) 27.8±1.2 29.3±1.7 30.7±1.7 28.0±1.2 16.5±1.4
5:Tit-for-tat 52.5±1.1 33.9±0.7 35.8±0.7 42.2±2.1 15.4±2.1
6:Noisytit-for-tat 46.7±2.2 40.3±2.4 41.9±2.7 42.0±1.3 17.3±2.7
7:Cooperatesthendefects 20.9±0.5 25.1±1.0 28.4±0.7 24.9±0.6 8.8±0.9
8:Defectorthatturnstit-for-tat 42.1±1.5 31.0±1.4 30.1±1.4 36.8±0.9 8.1±2.0
9:Defectorthatturnstonoisytit-for-tat 45.3±1.8 34.7±1.7 36.6±1.6 40.6±1.1 15.1±2.5
Table1: AveragerewardandSEMfordifferentagentsacrosssubstratesandscenarios(5seeds). Note
thatthevarianceinscenariosthatareaunionoftwoscenariosmayberelatedtowhichscenarios
weresampled.
14Figure7: ExampleofGPT4-basedagentfindingagoodhypothesisandemployinggoodreasoningto
selectitsnexttargetinventory.
: 2 : 5 Hypothesis based on Interaction 1-3: Highest-valued hypothesis:{'Opponent_strategy':
Interaction 3 : 1vs. : 1 { a' daO pp tp io nn ge n tt o_ s mt yr a pt re eg vy i' o: u s' M iy n vo ep np to on re yn t b ys e pe lm as y it no g b te h e ' aB pa ps ee ad r so n t ot h be e i cn ot ne sr ia sc tt ei no tn l yh i ps lt ao yr iy n, g t ah e p uo rp ep onent
: 6 : 1 c so tu rn at te er g yr e bs uo tu r ac re e. rT eh se py o na sr ie v en o tt o s mt yi c lk ai sn tg mt oo v ea . 'p }ure p ra ep se or u/ rp cu er sp l ee a cs ht r ta it me eg .y ', , c 'o vl al le uc et 'i :n g 0 .a 0r 9o }und 3 purple
Reward: -3.7
… … … …
: 5 : 1 Hypothesis based on Interaction 1-5: Highest-valued hypothesis:{'Opponent_strategy': Other hypothesis:{'Opponent_strategy': 'My
Interaction 5 : 1vs. : 5 { a' daO pp tp io nn ge n tt o_ s mt yr a pt re eg vy i' o: u sM y i no vp ep no tn oe rn yt bs ye e pm ls a yt io n gb e t he ' aB pa ps ee ad r so n t ot h be e i cn ot ne sr ia sc tt ei no tn l yh i ps lt ao yr iy n, g t ah e p uo rp ep onent o ip np vo en ne tn ot r ys e be ym s p lt ao y ib ne g a td ha ep t ci on ug n tt eo r m ry e sp or ue rv ci eo .u s T hey
: 1 : 1 c so tu rn at te er g yr e bs uo tu r ac re e. rT eh se py o na sr ie v en o tt o s mt yi c lk ai sn tg mt oo v ea . 'p }ure p ra ep se or u/ rp cu er sp l ee a cs ht r ta it me eg .y ', , c 'o vl al le uc et 'i :n g 0 .a 3r 6o }und 3 purple a rr ee s pn oo nt s is vt ei c tk oi n mg y t lo a sa t p mu or ve e .s 't ,r a 't ve ag ly u eb 'u :t 0a .r 0e 9 }
Reward: -6.4
Interaction 6 R:: e: 21 w7 av rds :. 0.63:: : 11 3 H { p a r sy ' l r o tp a o c rOo y u k apt s n / tph d y eoe a e gns 3 l yei p l ns a p o ft p u w r_b e r osa r p i mts / l f re p e mad u a et r r n .eo p e t 'gn l s i }y e o c'I u i:n s r p t t c a'e r e tTr a s iha t , nec e g t g b oi y u apo , t pn so o m cn1 f a ie t y sn- e st6 n s o : w rp c i sr o t /i l c bm l h la e ur c t ei t o l i y n g H ' a p ri B p a eg a p p sh s e e oe e a r us d r / rt s p co u e- n t r sv o p a t l el h b e au e e ce s hd i c t n o r th t n a iy e s t mp r i e eo a s g .t c t y 'h t e , ,e i n s o t c 'i n l o vs y l a: h l li p e u{ s l c e' t a t 'O o y i :p r i n p y n g 0o , g .n a 5e t a r 5n h o }t e p u_ u ns o r dt p e r p 3a o t n pe e ug n ry t p' l: e O o i a rt p n r eh p v e se o e pr n n n o e t o nh n o t sy t r ip y s vo s t et e b i h e y c te m k os s p i i l n ms t a g y: o y i t lb n o a{ e g s' a tO a t p d h p mp a e u oo p r vn t c e ee i o .n n u s 't g n t ,_ t r s t e at 'o r tr v ea am r gt ly e ye u s g ep o by 'r u u :' e r t : v c 0 i e a .' o . r 1M u e 3y s T } hey
… … … … …
Interaction 13 :: : R 11 5 ewv as r. d: 0 :: : 11 5 H o s p c ny p t a o op p r p u to o o e n t n n r t ch e g / e oe n p r ns t m u si o r s is a v p c s d e l i tb a , e s ea p s ns t s b o te s h u r ld o t s y t w / o h i s b pn e n w l l i g i u aI r t e yn a c . t s h ae t p i T r r r n h pa a e g e uc t f y rt e e t ei g r o a o y e r sn n r e t b c o ra e c r a1 s k e t- e f / s e1 d o y p g3 r e o y: o l n .n l s }{ o iT o w vh u ee r t o al na ds t d o H ' s r a 'i T t e n vg h r s t ah e a o i le t u c us o e r i et p g c p 'p y e a :- o , s t v n , i 0a e o n .l n f b g 6u t t u 1e e t a .d p n } r m sh i c a cy m o y ip a l so r l s st i e w oh l c i re y t t ss i c /i p n h bs l g l: a t uy a o e{ s r ' o r sO a u o tp n c rp p d k ao a / tn p 3 y ee e e gn r p l yt / u l _ p r o fs u p w rt r l or p e i ma l f t e me eg .y '' ,: O o i a rt p n r eh p v e se o e pr n n n o e t o nh n o t sy t r ip y s vo s t et e b i h e y c te m k os s p i i l n ms t a g y: o y i t lb n o a{ e g s' a tO a t p d h p mp a e u oo p r vn t c e ee i o .n n u s 't g n t ,_ t r s t e a 't o r t vr e aa m r g lt y e y ue s eg p o b 'y r u u :' e r t : v c 0 i e a .' o . r 5M u e 9y s T } hey
: 1 : 1 Validated hypothesis:{'Opponent_strategy': 'My
Interaction 14 : 1vs. : 3 o ip np vo en ne tn ot r ys e be ym s p lt ao y ib ne g a td ha ep t ci on ug n tt eo r m ry e sp or ue rv ci eo .u s T hey
: 4 : 4 a rr ee s pn oo nt s is vt ei c tk oi n mg y t lo a sa t p mu or ve e .s 't ,r a 't ve ag ly u eb ’u :t 0a .r 7e 9 }
Reward: 0.94
Figure8: ExampleofasuccessfulevolutionofahypothesisforMMP+HE+HRplayingaBest
Responsebot. ItshowstheprocessofhowtheLLM-basedagentgeneratedagoodhypothesisabout
theopponent’sstrategy,whichwaseventuallyvalidated.
15Figure9: AveragerewardperepisodeonallevaluationscenariosforeachversionoftheToMmodule
using the full version. Errorbars reflect SEM across 5 episodes. Reward is normalized per 1200
timestepstoreducevariancewithvariablelengthepisodes.
ExamplegeneratedstrategyforHE+HRplayingrockbot
Given the interaction history and the current state of the game, it seems that a strategy that
adapts to the opponent’s recent choices while maintaining an element of unpredictability could be
effective. The opponent’s strategy is not entirely clear from the interaction history, but there
are patterns of losses and wins that can be used to inform our strategy.
Here’s a high-level strategy for player_0:
1. Observe the opponent’s recent strategy: Look at the last few interactions to determine if
the opponent is favoring a particular resource. If a pattern is detected, prepare to counter it.
2. Adapt based on interaction history: If we win an interaction, introduce a slight randomness
to our next choice to avoid being predictable. If we lose, counter the opponent’s last winning
resource. In the case of a tie, either maintain the current resource or switch based on the
observed pattern of the opponent.
3. Efficient resource collection: Focus on collecting resources that align with our strategy
while avoiding unnecessary collection of other resources. This will help in building a strong
inventory for the next duel.
4. Strategic duel initiation: Initiate duels when we have a strong inventory that counters
the opponent’s likely choice. Use memory and observation to infer the opponent’s strategy and
position.
5. Avoid unnecessary resource collection: Once we have enough of a particular resource to play
a strong strategy, avoid collecting more of that resource and instead focus on positioning for a
duel or collecting a different resource if our strategy requires it.
6. Maintain flexibility: Be ready to adapt our strategy based on new information from the
opponent’s actions and the outcomes of interactions.
Now, let’s summarize this strategy in a Python dictionary format:
“‘python
{
’my_strategy’: ’Adaptive Strategy with Randomness and Countering’,
’observe_recent_opponent_strategy’: True, ’adapt_based_on_interaction_history’:
True, ’introduce_randomness_post_win’: True, ’efficient_resource_collection’: True,
’strategic_duel_initiation’: True, ’avoid_unnecessary_resource_collection’: True,
’maintain_flexibility’: True, ’interaction_guidelines’: { ’if_win_next’: ’introduce slight
randomness to avoid predictability’, ’if_lose_next’: ’counter opponent’s last winning resource’,
’if_tie_next’: ’maintain or switch based on observed opponent pattern’
}
} “‘
Figure10: ExampleofafailuremodeforHE+HRplayingrockbot. Generatedstrategiesmake
littlesensegiventheinteractionhistoryandrequestedformat.
16Exampleofasuccessfulembodiedexplorationatthebeginningofanepisode
, First subgoals: Subgoal Plan: Since I am currently at (21, 10) with an orientation facing
North and my high-level strategy is to collect 5 yellow resources to play a pure rock strategy, I
need to find and move towards the nearest yellow box locations. However, there are no observable
yellow box locations in my immediate vicinity. Therefore, my first subgoal is to explore the
environment to find yellow boxes. I should move towards the center of the map where I am more
likely to find resources and encounter the other player for a potential duel.
Given that I am at the edge of the map, I should move left (West) towards the center. I will
move to (20, 10), then to (19, 10), and continue in this direction until I find yellow boxes
or reach a more central location on the map. If I encounter yellow boxes along the way, I
will adjust my path to collect them. Once I have collected 5 yellow boxes, I will look for the
opponent to initiate a duel.
Action Plan:
“‘python
{
action_plan: [ move_to((21, 10), (20, 10)), move_to((20, 10), (19, 10)), move_to((19, 10), (18,
10)), move_to((18, 10), (17, 10)), move_to((17, 10), (16, 10))]
} “‘
Figure11: Exampleofasubgoalmoduleexploringtheenvironmentatthebeginningofanepisode
tobuildupacognitivemapofwhereresourcesarelocated.
B Environments
B.1 RunningWithScissorsRepeated
Specifically, here we evaluate our model on the Running With Scissors in the matrix: Repeated
environment (RWS) in the Melting Pot multi-agent decision-making benchmark [Agapiou et al.,
2022]. This is a zero-sum competitive environment with two players moving around a map and
collectingyellow,purple,orblueresourcesthatcorrespondtorock,paper,andscissorsrespectively.
Inadditiontomovement,theagentshaveanactiontofirean"interaction"beamwhichinitiatesa
duelwiththeotherplayerwhenthatplayeriswithinrangeofthebeam. Aninteractionresultsinone
agentgettingpositiverewardandtheotheragentgettinganoppositenegativerewardaccordingto
theinventoriesofresourcespickedupbyeachplayer. Specificallyaplayerwillcollectaninventory,
whichisonlyobservablebythatplayer:
ρ=(ρ ,ρ ,ρ ).
yellow purple blue
Rewardisdeterminedbymatrixmultiplicationoperationsmirroringtherock,paper,scissorsmatrix
game:
r =vT A v , r =−r
row row row col col row
wherev = ρi and
i (cid:80)K j=1ρj
(cid:34) 0 −10 +10(cid:35)
A = +10 0 −10 .
row
−10 +10 0
Thepartially-observableinputinMeltingPotconsistsofa5x5windowaroundtheagentsuchthatit
canseethreegridsinfrontofitselfandonebehindit,andtwooneachside.
B.1.1 Scenarios
Descriptionofscenariosforeachsubstratearereproduceddirectlyfrom[Agapiouetal.,2022]:
SC0: Versusmixedstrategyopponent. Herethefocalagentmustdefeatanopponentthatwas
trainedtoplayapurestrategy:eitherrock,paper,orscissors.However,thespecificopponent
issampledattesttimesoitcouldbeanyofthose. Allopponentscommitstronglytotheir
choice, aimingtocollectatleastthreeresourcesbeforeinteracting. Todefeatthem, the
focalagentshouldscoutoutwhichpurestrategyitsopponentisplayingandthencollect
theresourcestoimplementitscounterstrategy. Sincethisisaone-shotinteraction,success
requiresthefocalagenttopaycloseattentiontowhichresourcesaremissingsincethey
provideacluetowhichstrategytheiropponentisimplementing.
17SC1: Versusopponentwhoplaysthebestresponsetowhatthefocalplayerdidinthelastround.
Here the focal agent must defeat an opponent who may change their strategy with each
interaction. The opponent will always select the best response to what the focal player
selected in the previous interaction. For instance, if the focal player plays rock in one
interactionthenitsopponentwillplaypaperinthenextinteraction. Onthefirstinteraction
ofeachepisodeitchoosesoneofthethreepurestrategiesatrandom. Theopponentalways
commitsstronglytoitschoice,aimingtocollectatleastfiveresourcesbeforeinteracting. To
winthefocalplayercantrickitsopponentintochoosingaspecificstrategyandcountering
it. Thisrequireschangingstrategyfrominteractiontointeraction,cyclingaroundthethree
options.
SC2: Versusopponentwhosometimesplaysapurestrategybutsometimesplaysthebestresponse
towhatthefocalplayerdidinthelastround. Focalplayermustdefeatanopponentsampled
fromtheunionofthebackgroundpopulationsusedinSC0andSC1. Theprobabilityof
samplingapureopponentis3/4whiletheprobabilityofsamplingabestresponseopponent
is1/4.
SC3: Versusmixtureofopponentswhooftenfliptootherstrategiesaftertwointeractions. Focal
playermustdefeatanopponentthatmayinitiallyplayanypurestrategyand,withprobability
1/3,mayflipafterthesecondinteractiontothebestresponsetothebestresponsetoitsinitial
strategy. Forexampleifitstartsoutplayingrockthenthebestresponsetothatwouldbe
paper,soafterthesecondinteractionitwouldswitchtoplayingthebestresponsetopaper
i.e. scissors. Itonlyweaklycommitstoitsstrategyforthefirsttwointeractions. Thatis,
itaimstocollectonlyoneresourcebeforeinteracting. Aftertwointeractions,atthepoint
whenitchangesstrategy,italsostartscommittingmorestronglytoitschoice,aimingto
collectfiveresourcesbeforeeachinteraction. Withprobability2/3,theopponentinstead
playsapurestrategythroughouttheentireepisode. Inhalfofthepureopponentepisodes
thebotfullycommitstoitspurestrategy,aimingtocollectfiveresourcesbeforeinteracting,
whiletheotherhalfofthetimeitcommitslessstrongly,aimingtocollectonlyoneresource
beforeinteracting. Notethatopponentsmaybeweaklycommittedtotheirstrategyforthe
firsttwointeractionsregardlessofwhethertheywillultimatelyflipstrategyornotsoit’s
notpossibleforthefocalagenttoobserveweakcommitmentearlyonasacuetopredict
whetherornottheiropponentwilllaterflipstrategies.
SC4: Versusmixtureofopponentswhoeitherfliptoanotherstrategyafteroneinteractionandkeep
itforeverorcontinuetochange,alwaysbestrespondingtowhatthefocalplayerjustdid.
Twokindsofopponentsarepossible. Bothchangetheirstrategyafterthefirstinteraction.
Withprobability3/4theopponentwillbeabotthatflipstoadifferentstrategyafterthe
firstinteractionandthenfollowsittilltheendoftheepisode. Italwaysflipstothebest
responsetothebestresponsetoitsinitialstrategy(soifitinitiallyplaysrockthenitwillflip
toscissors). Withprobability1/4theotherkindofopponentissampled. Thisopponentis
identicaltotheoneinSC1. Bothkindsofopponentsalwaysfullycommittotheirchoice,
aimingtocollectatleastfiveresourcesbeforeinteractingsoitsnotpossibletoobservethe
opponent’scommitmentleveltopredictwhichkindtheyare. Towinthefocalplayermust
figureoutwhichkindofopponentitisplayingagainstandeitherbestrespondbyselecting
thesamechoiceinallinteractionsafterthefirstifpairedwiththefirstkindofopponent,or
applythecyclicstrategydescribedasthesolutiontoSC1ifpairedwiththesecondkindof
opponent.
SC5: Versus gullible opponent. Here the focal agent must defeat an opposing agent that was
trainedtobestrespondtoagentsplayingpurestrategies. Theopponentshouldattemptto
scoutoutwhatstrategythefocalagentisplayingsoitcanpicktheappropriatecounter. To
defeatit,thefocalagentshouldfeinttowardoneresourceandthencollectthecounterto
itscounter. Soforexample,ifthefocalagentsuccessfullyfeintedthatitwouldpickrock,
inducingitsopponenttopickpaper,thefocalagentshouldthencollectandplayscissors.
Thisopponentisfairlyweak.
SC6: Versuspurerockopponent. Opponentalwaysplaysrock,andcommitstoitstrongly,aiming
tocollectfiveresourcesbeforeinteracting. Thefocalplayergetsahighscorewhenitpicks
paperandcommitsstronglytothatchoice.
SC7: Versuspurepaperopponent. SameasSC6butopponentplayspapersofocalplayershould
playscissors.
18SC8: Versuspurescissorsopponent. SameasSC6butopponentplaysscissorssofocalplayer
shouldplayrock."
Itshouldbenotedthatscenario5includesagullibleopponent,thatattemptstoscoutoutwhatstrategy
youareplayingandpicktheappropriatecounter. Fromfirstprinciples,tobeatthisopponentanagent
should feint towards one resource to fool the opponent and then select the counter to its counter.
SinceourToMmoduleselectsstrategiesonahigher-levelofabstractionthantheembodiment,it
wasnotspecificallydesignedtobeatthisopponent. Integratinghigh-levelstrategyinformationwith
relevantembodiedinformationisthesubjectoffutureresearch. However,inpracticethegulliblebot
doesnoteffectivelyscoutoutouragent,anditfrequentlyplaysthesamestrategy,whichisexploited
byouragent,leadingtopositiverewardsonthisscenarioonaverageforallmodelversions.
B.2 RunningWithScissorsArena
WealsoevaluateourmodelonRunningWithScissorsinthematrix:ArenaenvironmentintheMelting
Potmulti-agentdecision-makingbenchmark[Agapiouetal.,2022]. Thisenvironmenthasthesame
dynamicsasRunningWithScissorsinthematrix: Repeatedwiththemainexceptionbeingthattheir
are 8 players in this substrate playing on a larger 25 by 24 matrix with a 11 by 11 observability
window,skewedtowardsviewingmoreinfrontoftheagent. Allscenariosunderthisagentrepresent
onefocalresident(agent)playingagainst7otherswithvaryingfixedanddynamicpolicies,attempting
tomaximizerewarddecidedbythepayoffmatrixequivalenttotheoneinRunningwithScissors
Repeated.
B.2.1 Scenarios
SC0: Versusabackgroundpopulationcontainingbotsimplementingallthreepurestrategies.Here
onefocalplayerjoinssevenfromthebackgroundpopulation. Thebackgroundpopulation
contains bots who implement all three pure strategies: rock, paper, and scissors. They
mayeithercommittotheirstrategymoderately(aimingtocollectthreeresourcesbefore
interacting)ormorestrongly(aimingtocollectfive). Thetaskforthefocalagentistowatch
itsopponents,seewhatstrategyoneofthemisimplementing,andactaccordingly.
SC1: Versusgulliblebots. Hereonefocalplayerjoinssevenfromthebackgroundpopulation. The
backgroundpopulationconsistsentirelyofweakbotswhoweretrainedtobestrespondto
agentsplayingpurestrategies. Theyareweakopponents.
SC2: Versusmixtureofopponentswhoplayrockandsomewhofliptoscissorsaftertwointerac-
tionsHereonefocalplayerjoinssevenfromthebackgroundpopulation. Thefocalplayer
shouldpayattentiontowhateachprospectivepartnerhascollectedsince2/3ofthemplay
rockwhile1/3playscissorsafterthefirsttwointeractions. Choosingpapertobestrespond
torockisabadchoiceifaccidentallypairedwithanopponentplayingscissors.
SC3: Versusmixtureofopponentswhoplaypaperandsomewhofliptorockaftertwointeractions.
LikeSC2butwithbotsplayingpaperandbotsswitchingfrompapertorock.
SC4: Versus mixture of opponents who play scissors and some who flip to paper after two in-
teractions. LikeSC2butwithbotsplayingscissorsandbotsswitchingfromscissorsto
paper.
SC5: Visiting a population of pure paper bots. Here one focal player joins seven from the
backgroundpopulation. Allsevenbackgroundbotsplaypapersothefocalplayercangeta
highscorebyplayingscissors.
SC6: VisitingapopulationofpurerockbotsHereonefocalplayerjoinssevenfromthebackground
population. Allsevenbackgroundbotsplayrocksothefocalplayercangetahighscoreby
playingpaper.
SC7: Visiting a population of pure scissors bots. Here one focal player joins seven from the
backgroundpopulation. Allsevenbackgroundbotsplayscissorssothefocalplayercanget
ahighscorebyplayingrock.
B.3 PrisonersDilemmaRepeated
The Prisoners Dilemma in the Matrix Repeated environment is a mixed-motive one where two
individualscollectresourcesthatrepresent‘defect’(red)or‘cooperate’(green)andcompareinven-
19tories in an encounter, analogous to the Running With Scissors substrates. Consequences of the
inventory comparison are congruent with the classic Prisoner’s Dilemma matrix game, exposing
tensionbetweenrewardforthegroupandrewardfortheindividual. Rewardisdeliveredafteran
interactionweightedbytheproportionofitemsineachagent’sinventory,asdescribedabovefor
RunningWithScissors. Thepayoffmatrixfortheinteractionis
(cid:20) (cid:21)
3 0
A =AT =
row col 5 1
B.3.1 Scenarios
SC0: PartnermayplayeithercooperateordefectTheoptimalstrategyissimplytounconditionally
defect. However,giventhatthefocaldoesn’tknowthestrategyofthebackgroundplayer,a
goodstrategyismoresubtle. Areasonablestrategyistobeagrimreciprocatorcooperator,
whichwouldcooperatewiththecooperator,anddefecttothedefector. Alternativelythe
focalplayermighttrytoascertainwhetherthebackgroundplayerisexploitable. Doingso,
however,carriesarisk,forifthebackgroundplayerweretobeaGrimreciprocator(likein
otherscenarios),thiswouldcausethemtodefectfortherestoftheepisode.
SC1: Partnertypicallyplayscooperate. Theoptimalstrategyissimplytounconditionallydefect.
Thesameconsiderationsaboutuncertaintyofthebackgroundplayer’sstrategyfromScenario
0applyhere.
SC2: Partner typically plays defect The optimal strategy is simply to unconditionally defect.
However, becausethefocalplayerdoesn’taprioriknowthestrategyofthebackground
player,theymustfirsttrytofindouttheirstrategy. Thiscanbedonebylookingatwhich
resourcestheycollectorbypayingattentiontotheresultsofthefirstfewinteractions. Once
the focal has identified its background partner is defecting then it may have confidence
thatitshoulddefectaswell. Thefocalplayershouldalsoconsiderthepossibilitythatthe
background bot is corrigible, i.e. that it could be persuade to switch from defection to
cooperation. ThisisnotthecaseherebutthebackgroundpopulationsusedinSC8andSC
arecorrigible.
SC3: Partnerisahair-triggergrimreciprocator,i.e. onewhoinitiallycooperatesbut,ifdefected
ononce,willretaliatebydefectingforeverafter. Theoptimalstrategyissimplytocooperate.
Grimreciprocatorbackgroundplayersarenon-exploitable, andthereisnowaytoknow
howtheywillreacttoadefectionaheadoftime. Becauseofthisuncertainty,testingfor
exploitabilitycanleadtopoorperformanceofthefocalplayer. Conditionalcooperatorswho
cooperatefirstbutretaliateifdefectedonshouldachieveahighscore.
SC4: Partnerisatwo-strikesgrimreciprocator,i.e. onewhoinitiallycooperates,butifdefected
ontwice,willretaliatebydefectingforeverafterTheoptimalstrategyissimplytocooperate.
Grimreciprocatorbackgroundplayersarenon-exploitable, andthereisnowaytoknow
howtheywillreacttoadefectionaheadoftime. Becauseofthisuncertainty,testingfor
exploitabilitycanleadtopoorperformanceofthefocalplayer. Inprinciple,itwouldbe
possibletodefectonceagainstthebackgroundplayerleadingtohigherreward. Butsinceit
isnotpossibletoknowthebackgroundplayerisatwo-strikesgrimreciprocator,andtesting
itagainstahairtriggergrimreciprocatorleadstodefection,inpracticeisbettersimplyto
cooperate. Conditionalcooperatorswhocooperatefirstbutretaliateifdefectedonshould
achieveahighscore.
SC5: Partnerisatit-for-tatconditionalcooperatorTheoptimalstrategyissimplytocooperate.
Defectingagainstatit-for-tatagent,evenoccasionally,mightleadtomiscoordinatedinterac-
tionswhereoneplayercooperatesandtheotherdefects,inanalternatingway. Forgiveness
is one way to break out of such cycles of recrimination. Conditional cooperators who
cooperatefirstbutretaliateifdefectedonshouldalsobeforgivingtoensuretheydowellin
thisscenario.
SC6: Partner is a tit-for-tat conditional cooperator who occasionally plays defect instead of
cooperate. Likethepreviousscenario,exceptthetit-for-tatbackgroundplayeroccasionally
willdefectinsteadofcooperate. Thisisknownastremblinghandingametheory. Astrict
tit-for-tat focal player would occasionally fall into miscoordinated interactions with the
background player resulting in alternating cooperation and defection. As in SC5, focal
20conditional cooperators must also be forgiving to ensure they do well in this scenario.
Forgivenessisevenmoreimportantheresincethebackgroundplayerwilldefectrelatively
frequentlyitselfbutwillstillimplementtit-for-tatretaliationwhendefectedonitself.
SC7: PartnerplayscooperateforawhilethenswitchestodefectSimilarconsiderationstothe
previousscenarios. Agoodstrategyisagrimreciprocator,ortit-for-tatforthefocalplayer.
Unconditionalcooperationwouldbeexploitedbythebackgroundplayer.
SC8: Partnertriestotakeadvantageofthefocalplayerbyplayingdefect,butifpunished,partner
thenswitchestotit-for-tatconditionalcooperationRelatedtoScenario2,theoptimalstrategy
isforthefocalplayertopersuadethebackgroundplayertostopdefectingbypunishingit
throughdefectingitself. Oncepersuaded,thebackgroundplayerimplementsaconditional
cooperation(tit-fortat)strategy. Soitissafetostartcooperatingwiththemonceyouhave
verifiedthattheyarethemselvesconsistentlycooperating.
SC9: Partnertriestotakeadvantageofthefocalplayerbyplayingdefect,butifpunished,partner
thenswitchestonoisytit-for-tatconditionalcooperationLikethepreviousscenario,except
the focal player must implement a more generous form of conditional cooperation after
persuadingthebackgroundplayertoswitchfromdefection.
B.4 CollaborativeCookingAsymmetric
IntheCollaborativeCookingAsymmetricsubstrate,playersneedtocollaboratetofollowrecipes.
Theenvironmentdescribedin[Agapiouetal.,2022]followstheregularpseudorewardscheme,which
isturnedoffbydefault. TheasymmetricenvironmentisaversionoftheCollaborativeCookingwith
anasymmetricadvantagesmap. Thisistotestwhetherplayerscanchoosehigh-levelstrategiesthat
playtotheirstrengths.
B.4.1 Scenarios
SC0: Collaboratewithaskilledchef Herethebackgroundplayerimplementsaparticularpolicy
thatcanbeveryeffectivewhenitspartnerdoesitspart. Thetwoplayersareontwodistinct
anddisconnectedsidesofthemap. Ononesidethegoaldeliverylocationisclosetocooking
potsandthetomatodispenserisfarawaywhereasontheothersidethethegoaldelivery
locationisfarfromthecookingpotsbutthetomatodispenserisclose. Theplayersshould
collaborate,eachspecializinginthepartofthetaskthatitismostefficientforthemtodoon
thesideofthemapwheretheyspawned. Thebackgroundplayerimplementsthiskindof
policy,whichdependsontheactionsofitspartnertocompletethetask. Thebackground
playerwastrainedwiththeV-MPOalgorithm.
SC1: Collaboratewithasemi-skilledapprenticechef ThisscenarioissimilartoSC0butthe
backgroundplayerisnotaswelltrained. Infactthebackgroundpopulationusedhereis
the same as in SC0 but from an earlier point in training. The importance of evaluating
cooperationwithbotsofvaryingskilllevels,anddifferentpointsintraining.
SC2: SucceeddespiteanunhelpfulpartnerInthisscenariothebackgroundplayernevermovesor
helpsinanyway. Onthismapitislessefficienttoimplementallstepsoftherecipealone
versustoworktogetherwithapartner. Butitisstillpossibleforeitherplayertoperformall
thestepsontheirown. Thetaskistorealizethatthebackgroundplayerwon’tdotheirpart
ofthejointpolicysothefocalagenthadbetterdoeverythingitself.
C Methods
C.1 TextualMap
Pixelimagesoftheglobalstatearepreprocessedintoatext-basedstaterepresentation. Theimages
aredividedinto8x8patches,eachcorrespondingtoacellwithintheMeltingPotgridthatentities
canoccupy. Eachpatchmayrepresentoneoffourtypes: anagent,aresource,awall,orablank
space. Thepatchesarelabeledbycomparingthemtomanuallylabeledreferencepatchesofeach
typeofentity,includingthenumerouspossiblebodyorientationsandhatcolorsanagentcanembody
inRunningWithScissors. Theglobalstatecanthenbefullyrepresentedintextbycoordinatesin
a23x15gridandtheentitylabelateachcoordinate. Egocentricstatesarethencreatedfromthis
21accordingtothepartially-observable5x5boxaroundanagent(dependentonitsorientation). Wetried
severalrepresentationsoffeedingthistextualmaptoGPT,andinpracticethebestrepresentation
consistsofprintingeachentitytypewithalistofallthecoordinateswherethatentitytypeispresent.
Forexample“PlayerPosition: {’player_0-S’:[(21,4)]},ObservableYellowBoxLocations: [(13,
10),(14,11)]},ObservableBlueBoxLocations: [],ObservablePurpleBoxLocations: [(13,11),(15,
11)]“encodestheplayerpositionandorientation(south)alongwiththeobservableboxlocations.
Theplayer’scurrentinventoryisalsoincludedinthetextualstaterepresentation,asitisinthedefault
RunningWithScissorsstaterepresentation.
C.2 Memory
Thememorysystemconsistsoftwoparts. Thefirstdatastructureappendstheobservedstatesin
theprevioussteptolistsofeachentitytypeinatuplewiththestepitwasobserved. Forexample:
‘yellow_box’: [((13,3),‘Step: 1087’),((13,4),‘Step: 1087’),((7,3),‘Step: 1091’)]. TheLLMis
promptedthatitsmemorycanbeoutdatedandthereforeshouldtakethestepitwaslastobservedinto
account.
Theseconddatastructureinthememorysystemcontainsalistoftheagent’sinventoriesandrewards
fromtheinteractionsthatoccurredsofar. Thisspecificinformation, distinctfromthepreviously
observedstates,isrelayedtotheToMmodule.
C.3 TheoryofMindModule
TheTheoryofMindModuleisqueriedperiodicallyafterdiscreteevents. Forthe*intheMatrix
substrates,thisoccurredafteraninteraction. Forcollaborativecooking,thisoccurredafteradishwas
delivered.
The ToM module consisted of a 5 step process, as depicted in Figure 2 for the * in the Matrix
substrates:
1. Recordtheobservedbehaviorfromtheotheragent’strajectoryϕ(τ). Herethisrefersto
whethertheyplayedrock,paper,orscissors,theargmaxoftheinventory(orcooperate/defect
in Prisoner’s Dilemma). Since the opponent’s inventory is never observed, we have to
estimateitgiventheinventoryHypotheticalMindsplayedandtherewarditreceived. Thus,
weasktheLLMtoestimatetheopponent’sinventorygiventhisinformation,andnotethe
outputastheempiricalopponent’sinventory.
2. EvaluateHypothesesaboutopponent’sstrategy. Inthepreviousinteraction,thetopk
ˆ
hypothesesareusedtogeneratepredictionsabouttheopponent’snextinventoryϕ(τ). These
argmaxoftheinventorypredictionsarecomparedtotheargmaxoftheempiricalopponent’s
inventory(didtheyplayrock,paper,orscissors). Asdescribedinthemaintext,Hypotheses
thatledtocorrectpredictionsgetapositiveintrinsicrewardandnegativeotherwise. Ifa
hypothesisisvalidated,meetingV ,thenstep3isskippedandthishypothesisisusedfor
thr
step4untilthehypothesisfallsbelowthethreshold(meaningitsnotlongermakinggood
predictions)
3. Generatenewandrefineoldhypotheses. TheLLMistaskedwithgeneratingahypothesis
about the other agent’s strategy given the entire interaction history (see prompt below
formoredetails). Thepromptalsoincludesthetopkhypothesesgeneratedsofarifthe
hypotheseshaveavalueabove0(meaningatleastonecorrectprediction)suchthattheLLM
canrefinepreviouslygeneratedhypotheses.
4. Guessopponent’snextgoal. TheLLMispromptedtoguesstheopponent’snextinventory
givenahypothesisandtheinteractionhistory. IfnohypothesishasyetsurpassedV ,then
thr
guessesaremadeforthetopkhypothesesandthelastgeneratedhypothesis. Theprediction
fromthelastgeneratedhypothesiswouldbeusedtoselectacounterinventoryinthenext
step. Moreover,allthepredictionswillbeusedforstep2(hypothesisevaluation)afterthe
nextinteraction. IfahypothesiscrossesV ,thenonlyitwillbeusedinthisstep.
thr
5. Select goal to counter opponent. The LLM is prompted to select a counter inventory
giventhepredictionabouttheopponent’snextinventory. Sincethisstepinvolvesstraightfor-
wardreasoninggiventheopponent’spredictedinventory,itisdonesimultaneouslyinthe
22API/LLMcallwiththepreviousstep. Therefore,theLLMisspecificallytaskedwithout-
ˆ
puttingboththepredictedopponent’snextinventoryϕ(τ)anditsowngoal/targetinventory
zinasingleAPI/LLMcall.
InRWSArena,step4isdoneseparatelythanstep5. TheLLMisfirstpromptedtoguessthenext
inventoryfortheopponenttheyjustplayed. Theninasubsequentstep,itisaskedagaintoselect
whichopponentstoseekout,andtoguesswhatinventorytheywillplay(thiscouldbeadifferent
opponentthantheoneinteractedwithinthelastround). Therefore,theToMmodulecanevaluatethe
hypothesesforeachopponentbasedonthequalityofthepredictionstheymakeforthatparticular
opponent,andthisprocessisseparablefromselectingthetargetinventoryinthenextinteraction(see
prompts).
AsimilarprocessoccursforCollaborativeCookingAsymmetric. Step1ishardcodedandnotLLM
dependent;theotheragent’sactions/behaviorϕ(τ)arelabeledbycustomcodegiventheobservations
(abstractingawaytheproblemofactionrecognitionfromtextualobservations). Theseactionsinclude
"Teammatepickedupadish","Teammateputdownadish","Teammatepickedupcookedsoupin
dish","Teammatedeliveredcookedsoup","Teammatepickedupatomato",ornothing. Instep4,the
ˆ
LLMispromptedtoguesstheteammate’snextbehaviorϕ(τ)innaturallanguage. AnotherLLM
instanceisusedinstep2toassesswhetherthepredictionwascorrectornot,promptingtheLLMto
outputTrueorFalse,andgivingeachhypothesistheappropriateintrinsicreward. Step5iscompleted
inaseparateAPIcallforCollaborativeCooking,andtheLLMisprompted: "whatstrategydoyou
wanttotakenextandwhy? Teammate’sobservedstrategy: ”. Thinkstepbystepabouthowtoadapt
totheirbehaviorandmaximizeallresourcesandefficiencyaccordingly."
C.4 SubgoalModule
Thesubgoalmoduleisresponsibleforgeneratingefficientsubgoalplansfortheagent. Giventhe
high-level strategy and the current state of the game, the module decomposes the strategy into a
sequenceofsubgoalsintheformofactionfunctioncallstoefficientlyimplementthestrategy. The
subgoalmoduleusestheLLMtogeneratetheseplansasasequenceofactionfunctioncalls(usually
3-6). The prompt includes the current step of the game, the high-level strategy/target inventory
previouslydecidedupon,detailsaboutthecurrentobservations(includingplayerposition,orientation,
inventory,observableresourcelocations,otheragentlocations),validmovementlocations,memory,
andinstructionsabouttheactionfunctions.
Actionfunctionsforthe*intheMatrixsubstrates:
• move_to(src_coord,target_coord): Efficientlymoveagentfromsourcecoordinatetotarget
coordinate.
• fire_at(target_coord): Stayaroundspecifiedcoordinateandfireinteractionwhenopponent
isspottedtoinitiateduel.
ActionfunctionsforCollaborativeCookingAsymmetric:
• move_to(src_coord,target_coord): Efficientlymoveagentfromsourcecoordinatetotarget
coordinate. Onlymovetovalidmove_tolocationswherecountersorobjectsarenotpresent.
• interact(target_coord): Movetoandinteractwiththeentityatthetargetcoordinate,suchas
pickingupingredientsordeliveringdishesofcookedsoup. Toplaceanobjectonacounter
tofreeyourhands,useinteract(counter_coord).
• wait(target_coord): Waitforthepotattarget_coordtofinishcooking. Checktheprogressof
thepotsandonlyusevalidlocationswherepotsarepresent.
C.5 ActionPlanner
The action planner turns the sequence of subgoals specified by the subgoal module into a se-
quence of atomic actions compatible with the Melting Pot environment. These actions include
step forward, backward, left, or right, turn left or right, fire zapping beam, and noop. For the
move_to(source_coordinate, target_coordinate)function,theA*algorithmfindthemost
efficientpathgivenasetofobstacles. Wallsarealwaysconsideredobstacles,andotherresourcesare
23conditionallyaddedasobstacles. Ifapathcanbefoundwithoutpickingupanotherresource,the
actionplannerwillreturnit. However,inmanycasesthelanguagemodelpicksatargetcoordinate
whereotherresourcescannotbeavoided. Theactionplannerwilluseaprioritysysteminthiscase,
privilegingpathswhereonlyresourcesofthesametypeasthetargetcoordinatewillbepickedup.
Thefire_atfunctionreturnsasequenceofactionstoturntowardstheopponentandzapthemwhen
theagentiswithintheextentofthezappingbeam’srange. Iftheopponentisnotinview,thenthe
fire_atfunctionturnstheagentcontinuallyclockwiseuntiltheotheragentisfound.
C.6 Self-Reflection-CollaborativeCooking
ForCollaborativeCookingAsymmetric,anevaluatorandself-reflectionmechanismwasaddedasin
theReflexionbaseline[Shinnetal.,2024]. Thiswasaddedsuchthatiftheagentwasmakingaction
plansthatdidnotchangethestateoftheworld,forexampletryingtopickupatomatowhileholding
adish,theseactionplanswerenotrepeatedwiththesamestateinformation. Byfirstreflectingon
whetherthepreviousactionplanwassuccessful,theagentwasabletomakelessmistakesoverthe
courseoftheepisode. Thisadditionalcognitivemoduleforself-reflectioncouldinprinciplealsobe
addedfortheothersubstrates,butwasnotnecessaryforgoodperformanceonthe*inthematrix
gamesbecausethestatetransitionsweresimplerforLLMstounderstand.
D Baselines
D.1 ReAct
The ReAct [Yao et al., 2022] agent combines reasoning traces with task-specific actions in an
interleavedmanner. Thisapproachallowstheagenttogeneratebothreasoningstepsandactions
withinthesamelanguagemodelframework. Inourcontext,thereasoningconsistsofchainofthought
reasoningprecedingasubgoalplaninthespecifiedformatofutilizingactionfunctions. Theagentis
promptedtothinkabouttheotheragents’strategiesandcomeupwithasubgoalplanaccordingly.
Thus, ReAct is functionally an ablation of Hypothetical Minds such that there is only a subgoal
moduleandnotatheoryofmindmodule. Threeexampleresponsesareshownasfew-shotprompts,
consistentwiththeReActframework.
D.2 Reflexion
Reflexion adds evaluation and self-reflection to the ReAct agent backbone serving as the Actor
module. Afterasubgoalplaniscompleted,theLLMisqueriedtoevaluatetheoutcomesofthatplan.
Outcomesarerepresentedastherewardduringtheplan,andsalientstateinformationpreandpost
plan. Thisstateinformationincludesthepositionandtheinventoryoftheagentforthe*inthematrix
games. ForCollaborativeCookingAsymmetric,thegivenstateinformationincludedposition,what
theagentisholding,andthestateofthetwopots.
D.3 PlanReAct
WeincludethePlanReActarchitectureintroducedin[Liuetal.,2023]asahierarchicalbaseline. This
modelfirstgeneratesahigh-levelplaninlanguageandthenfeedsthisplantoasubgoalmodulethat
outputsasubgoalplanbasedonthehigh-levelplan. Thus,theonlydifferencebetweenPlanReAct
andHypotheticalMindsisthathigh-levelplanningismediatedbythemultipleprocessingstepsof
thetheoryofmindmodule,includinghypothesisgeneration,evaluation,andrefinement.
D.4 PPO
WetrainRLagentsinapopulationofPPOagents[Schulmanetal.,2017]oneachsubstrate. The
weightsarerandomlyinitializedforeachagentinthepopulationandweightsarenotshared.Therefore
theagentsarenotplayingagainstidenticalcopiesofthemselvesandseeagreaterdiversityduring
trainingthantraditionalself-play. ModelsweretrainedinPyTorchusingtheRayRllibpipelineand
thisstartercodehttps://github.com/rstrivedi/Melting-Pot-Contest-2023. Optimalparameterswere
searchedoverandthefinalmodelsweretrainedfor1e8steps.
24E AblationDetails
TheHypothesisEvaluation+HypothesisRefinementmodelhadadifferentevaluationprocedurethan
theothermodels. Ratherthancomputingvaluesbasedonpredictingtheopponent’sinventory,here
weuseextrinsicrewardandcounterfactualreward. Ifahypothesisisusedonlineforgoalselection,
thentherewardsreceivedinthenextinteractioncanbedirectlyusedforevaluatingit. Fortheother
consideredhypotheses,wesimulatecounterfactualrewardby1. askingGPTtogenerateatarget
inventorygiventhehypothesis/strategyandthegivensituationand2. afterthenextinteractionweask
GPTagaintoreasonaboutwhattherewardwouldhavebeenifitplayedtheinventoryfrom1. GPT
isaskedtooutput,positive,negative,orneutral,whichweconverttorewardwiththecparameter.
F Hyperparameters
Hyperparameter Value
Alpha(α) 0.3
Intrinsicrewardc 1.0
ThresholdforHypothesisValue(V ) 0.7
thr
TopkHypothesesConsidered(k) 5
Table2: HyperparametersoftheHypotheticalMindsmodel
Hyperparameter Value
Alpha(α) 0.3
Counterfactualrewardc 3.0
ThresholdforHypothesisValue(V ) 3.0
thr
TopkHypothesesConsidered(k) 5
Table3: HyperparametersoftheHE+HRmodel
Hyperparameter GPT-4 GPT-3.5 Llama3
Model "gpt-4-1106-preview" "gpt-3.5-turbo-1106" "Meta-Llama-3-70B-Instruct"
Maxtokens 4000 2000 2000
Temperature 0.1 0.2 0.2
Topp 1.0 1.0 1.0
n 1 1 10
Table4: HyperparametersofVariousModels
G Prompts
Allpromptscanbeseeninourpublicrepository: https://github.com/locross93/Hypothetical-Minds/.
Themostrepresentativepromptsarealsoreproducedbelow.
25G.1 RunningWithScissors: ArenaPrompts
RWSArenaSystemMessage
You are Agent 0 in the eight player ’running_with_scissors’ Melting Pot multiagent
reinforcement learning environment that is a 25x24 (x by y) grid with resources to
collect and walls to navigate around. 8 Players can move around the map and collect
resources of 3 discrete types corresponding to rock, paper, and scissors strategies
- Yellow box = rock - Purple box = paper - Blue box = scissors. Rock/yellow beats
scissors/blue, paper/purple beats rock/yellow, and scissors/blue beats paper/purple.
In addition to movement, the agents have an action to fire an "interaction" beam which
initiates a duel with one player getting positive reward and the other agent getting
an opposite negative reward according to their inventories.
All players carry an inventory with the count of resources picked up since last
respawn and for each respawn start with an inventory of 1 resource each. This
inventory is visible in the state with the key ’inventory’.
To play a pure strategy strongly, pick up at least 5 resources or more of the color
and then fire the interaction beam at another player. To commit less strongly to a
strategy, pick up around 3 resources of the color and then fire the interaction beam
at another player.
Usually you will only want to pick up one type of resource before an interaction, in
order to gain the most information about the other players’ strategies and to not
waste time collecting other resources.
You also want to maximize the number of interactions so after you pick up 4-6
resources, you should seek out a duel to reset your inventory and gain more
information about the other players’ strategies.
Your opponents will also almost always only pick up one type of resource before an
interaction.
For example, player0_inventory = [7, 1, 1] (Yellow, Purple, Blue) is a good inventory
that will lead to an informative duel, whereas player0_inventory = [2, 2, 2] (Yellow,
Purple, Blue) will not be informative.
Your reward is the result of a matrix multiplication involving your inventory in a
vector format, and your opponent’s inventory vector, and a payoff matrix similar to
rock paper scissors.
r_t = transpose(your_inventory) * A_payoff * opponent_inventory where A_payoff =
np.array([[0, -10, 10], [10, 0, -10], [-10, 10, 0]])
The reward usually ranges from (5, -5) depending on the inventories of both players
(the min is -10 and max 10, but it is rare to get these magnitudes). Typically +/-
3-5 is a high magnitude, and a reward near 0 suggests both players played a similar
inventory.
State Description: This environment is partially-observable, you can observe an 11x11
grid around your agent depending on your position and orientation (you can see more in
front of you than behind).
Previously seen states will be represented in memory, but note that these states could
potentially be outdated. For example, the other agent could collect a resource that
you previously saw.
Given the partially-observable nature of the environment, you will need to explore the
environment appropriately and select goals based on the information you’ve gathered.
Also pay attention to your opponents’ positions when you see them in order to duel
with them and gain information about their strategy.
To find a specific player, you can first move towards the last known location of the
player and then move randomly around the map.
Hanging around the center of the map and waiting for a player to come to you is not a
good strategy for this environment.
After you gather information about your opponents’ strategies, seek out opponents
whose strategy you know and can exploit and play a counter-strategy.
26G.1.1 HypotheticalMinds
SubgoalModuleMessage
Current State Description:
- Global Map Size: {map_size} grid (Walls are located at the boundaries of the map
and in other places that are invalid for move_to).
- Valid Locations for move_to: {movable_locations}
- Player Position: {player_position}
- Player Orientation: {player_orientation}
- Player Inventory (yellow, purple, blue): {player_inventory}
- Egocentric Observations Size: 11x11 grid around your agent. You currently can
observe the following based on your position and orientation:
- Observable Yellow Box Locations (format: ((x,y), distance from current location)):
{yellow_locations_with_distance}
- Observable Blue Box Locations: {blue_locations_with_distance}
- Observable Purple Box Locations: {purple_locations_with_distance}
- Observable Opponent Locations: {opponent_locations}
- Previously seen states from memory (format: ((x,y), step last observed, distance
from current location)): {self.memory_states}
Execution Outcomes:
{execution_outcomes}
Error for extracting and executing actions from the response:
{get_action_from_response_errors}
Rewards:
{rewards_str}
Strategy Request:
You are at step {step} of the game.
You have decided to execute a high-level strategy/target inventory in a previous
response given what you predicted your opponent will do.
Select subgoals in order to achieve the strategy, including first achieving a target
my_next_inventory: {self.hls_next_inventories}.
Once you achieve the target inventory, STOP picking up resources and immediately seek
out a duel with an opponent close to you that you can exploit based on your hypothesis
about their strategy and your current inventory.
So once you’ve picked up about 5-7 resources in total, seek out a duel to receive
rewards, get more information about strategies, and reset your inventory.
Here are your hypotheses about each player’s strategy: {self.opponent_hypotheses}
If you’ve generated a hypothesis about a player’s strategy, you can use this to inform
your strategy about whether to interact with them or not.
Each strategy is paired with a value on how well it explains the data observed so far,
starting at 0.
A hypothesis is validated when its value is greater than: {self.good_hypothesis_thr}.
Your task is to devise efficient action plans for player {self.agent_id}, reason
through what the next subgoals should be given the state information.
Your response should be broken up into two parts:
1. Subgoal Plan - based on the current state and the high-level strategy you
previously specified above, decompose this strategy into a sequence of subgoals and
actions to efficiently implement this strategy. Think step by step about this. This
could be fairly long.
2. Action Plan - output this sequence of actions in the following Python dictionary
format, parsable by ast.literal_eval() starting with:
{{ ’action_plan’: [’move_to((11, 7), (9, 5))’, ’move_to((9, 5), (13, 5))’] }}
Example response 1, 2, and 3 are formatted similarly, detailing other strategies and
actions.
27ToMModuleUserMessage1
An interaction with another player has occurred at step {step},
{self.interaction_history[self.last_played_id][-1]}.
What was my opponent’s likely inventory in the last round given the inventory
I played and the reward received?
Think step by step about this. First think about what resource you had the
most of in your inventory, and then think about which resource would beat
that if you received a negative reward of -1 or worse or which resource would
lose to yours if you received a positive reward of 1 or more.
If you received a small magnitude reward near 0 and in between (-1, 1), then
your opponent may have played a similar inventory to you.
Then depending on the magnitude of the reward and the number of resources you
played, you can infer the opponent’s inventory and whether they played that
strategy strongly (5+ of that resource) or weakly ( 3 of that resource).
An inventory of {’rock/yellow’: 1, ’paper/purple’: 1, ’scissors/blue’: 1}
is not possible because you need at least 2 resources of a type to play a
duel.
Here are some example interactions to help you reason about how the reward
function works:
’your_inventory’: {’rock/yellow’: 3, ’paper/purple’: 1, ’scissors/blue’:
1}, ’rewards’: -2.285, ’possible_opponent_inventory’: {’rock/yellow’: 1,
’paper/purple’: 5, ’scissors/blue’: 1}
’your_inventory’: {’rock/yellow’: 5, ’paper/purple’: 1, ’scissors/blue’:
1}, ’rewards’: 3.571, ’possible_opponent_inventory’: {’rock/yellow’: 1,
’paper/purple’: 1, ’scissors/blue’: 6}
’your_inventory’: {’rock/yellow’: 1, ’paper/purple’: 4, ’scissors/blue’:
1}, ’rewards’: 2.0, ’possible_opponent_inventory’: {’rock/yellow’: 3,
’paper/purple’: 1, ’scissors/blue’: 1}
In the 2nd part of your response, output the predicted opponent’s inventory
in following Python dictionary format, parsable by ast.literal_eval()
starting with: ’possible_opponent_inventory’: {’rock/yellow’: 1,
’paper/purple’: 1, ’scissors/blue’: 5}
Example output:
Given that I last played a strong paper strategy with an inventory of
{’rock/yellow’: 1, ’paper/purple’: 5, ’scissors/blue’: 1} and received
a reward of -3.428, I believe my opponent played a strong scissors strategy.
The reward suggests that my paper was beaten by their scissors, which means
their inventory likely had a higher count of blue/scissors resources.
A possible inventory for them could be {’rock/yellow’: 1, ’paper/purple’: 1,
’scissors/blue’: 5} or a similar distribution favoring scissors.
28ToMModuleUserMessage2
Total Rewards: {rewards_str}
Strategy Request:
An interaction with another player has occurred at step {step},
{self.interaction_history[self.last_played_id][-1]}.
The total interaction history with this opponent is:
{self.interaction_history[self.last_played_id]}.
If self-improvement is a focus:
Here are your previous hypotheses about the algorithm this opponent is
playing: {self.top_hypotheses[self.last_played_id]}.
What is your opponent’s likely policy given the inventories and the reward
function? Think step by step about this given the interaction history.
If your previous hypotheses are useful, you can iterate and refine them to
get a better explanation of the data observed so far.
If a hypothesis already explains the data very well, then repeat the
hypothesis in this response.
They may be playing the same pure policy every time, a complex strategy to
counter you, or anything in between.
They are not necessarily a smart agent that adapts to your strategy, you are
just playing an algorithm.
Are you getting high positive or negative reward when playing the same type
of inventory? For example, getting high positive reward every time you play
many paper resources. If so, this opponent may be playing a pure strategy
and you can exploit this by playing the counter strategy.
Once you have output a hypothesis about this opponent’s strategy with step by
step reasoning, you can use the hypothesis to inform your strategy.
In the 2nd part of your response, summarize your hypothesis in a concise
message following Python dictionary format, parsable by ast.literal_eval()
starting with: ’rock/yellow’: 1, ’paper/purple’: 1, ’scissors/blue’: 5
{’Opponent_strategy’: ’I think my opponent is always playing a pure scissors
strategy and collecting around 5 blue resources.’}
Otherwise:
What is this opponent’s likely policy given the inventories and the reward
function? Think step by step about this given the interaction history.
They may be playing the same pure policy every time, a complex strategy to
counter you, or anything in between.
They are not necessarily a smart agent that adapts to your strategy.
Are you getting high positive or negative reward when playing the same type
of inventory? For example, getting high positive reward every time you play
many
paper resources. If so, this opponent may be playing a pure strategy and you
can exploit this by playing the counter strategy.
Once you have output a hypothesis about this opponent’s strategy
with step by step reasoning, you can use hypothesis to inform your strategy.
In the 2nd part of your response, summarize your hypothesis in a concise
message following Python dictionary format, parsable by ast.literal_eval()
starting with: {’Opponent_strategy’: ’I think my opponent is always playing
a pure scissors strategy and collecting around 5 blue resources.’}
You will be prompted again shortly to select subgoals and action plans to
execute this strategy that achieves the target inventory, so do not include
that in your response yet right now.
29ToMModuleUserMessage3
An interaction with self.last_played_id has occurred at step {step}, The
total interaction history with {self.last_played_id{ is: You previously
made the following guess about this player’s strategy: Think step by step
and predict what this opponent will play the next time you interact with
them. Given the above mentioned guess about the opponent’s policy/strategy,
and the last inventory you played (if their strategy is adaptive, it may
not be), what is their likely inventory in the next round. In the 2nd
part of your response, output the predicted opponent’s next inventory
in following Python dictionary format, parsable by ‘ast.literal_eval()‘
starting with “‘python. Example response 1: ’Opponent_next_inventory’:
Given that my opponent is playing a rock policy, I believe their next
inventory will be inventory={’rock/yellow’: 5, ’paper/purple’: 1,
’scissors/blue’: 1}. “‘python { ’predicted_opponent_next_inventory’:
{’rock/yellow’: 5, ’paper/purple’: 1, ’scissors/blue’: 1} } “‘ Example
response 2: ’Opponent_next_inventory’: Since my guess is that this player
is playing a scissors policy, I predict that their next inventory will be
{’rock/yellow’: 1, ’paper/purple’: 1, ’scissors/blue’: 5}. “‘python {
’predicted_opponent_next_inventory’: {’rock/yellow’: 1, ’paper/purple’: 1,
’scissors/blue’: 5} } “‘ Example response 3: ’Opponent_next_inventory’:
Since my opponent is following a paper strategy, I predict their upcoming
inventory will be inventory={’rock/yellow’: 1, ’paper/purple’: 5,
’scissors/blue’: 1}. “‘python { ’predicted_opponent_next_inventory’:
{’rock/yellow’: 1, ’paper/purple’: 5, ’scissors/blue’: 1} }
30ToMModuleUserMessage4
An interaction with self.last_played_id has occurred at step {step}, The
total interaction history with self.last_played_id is: The total interaction
history overall is: . You previously made the following guesses about all
the other players’ strategies: possible_opponent_strategy. High-level
strategy Request: Provide the next high-level strategy for your player
self.agent_id. This response should include step by step reasoning in
parts 1-3 about which strategy to select based on the entire interaction
history in the following format: 1. ’Opponents_to_seekout’: Given
the hypotheses about your opponent’s strategies and their values, which
players should you seek out to duel with next and why? If possible,
select opponents you have a good hypothesis about so you can exploit it
and maximize your reward. Try to select multiple players if possible as
one player might be hard to find or is respawning. Are you noticing any
patterns across the population as a whole? 2. ’Opponent_next_inventory’:
Given the above mentioned guess about the opponent’s policy/strategy what
is their likely inventory in the next round. 3. ’My_next_inventory’:
Given the opponent’s likely inventory in the next round, what should your
next inventory be to counter this? 4. In the 4th part of your response,
output the opponent to seekout, the predicted opponent’s next inventory,
and your next inventory in following Python dictionary format, parsable
by ‘ast.literal_eval()‘ starting with “‘python. Example response 1: 1.
’Opponent_to_seekout’: Given that I am fairly certain that player_1 and
player_5 is playing a rock policy, I believe I should seek out either
player_1 or player_5 to duel with next. 2. ’Opponent_next_inventory’:
Given that these opponents are playing a rock policy, I believe their next
inventory will be something like inventory={’rock/yellow’: 5, ’paper/purple’:
1, ’scissors/blue’: 1}. 3. ’My_next_inventory’: Given that these players
are playing a rock policy, I believe my next inventory should be a paper
policy inventory={’rock/yellow’: 1, ’paper/purple’: 5, ’scissors/blue’:
1}. “‘python { ’opponents_to_seekout’: [’player_1’, ’player_5’],
’predicted_opponent_next_inventory’: {’rock/yellow’: 5, ’paper/purple’: 1,
’scissors/blue’: 1} ’my_next_inventory’: {’rock/yellow’: 1, ’paper/purple’:
5, ’scissors/blue’: 1} } Example response 2: 1. ’Opponents_to_seekout’:
Considering all the interactions, player_2, player_3, and player_5
seem to heavily favor the scissors/blue strategy with consistent picks.
Engaging either of these players could offer a high reward opportunity. 2.
’Opponent_next_inventory’: Based on the observed behavior of player_2,
player_3, and player_5, it is likely they will continue with a strong
scissors/blue strategy, potentially having an inventory of {’rock/yellow’:
1, ’paper/purple’: 1, ’scissors/blue’: 5}. 3. ’My_next_inventory’:
To effectively counter the scissors/blue strategy, my optimal approach
would be to adopt a rock/yellow strategy. An inventory geared towards
this would be {’rock/yellow’: 5, ’paper/purple’: 1, ’scissors/blue’: 1}.
“‘python { ’opponents_to_seekout’: [’player_2’, ’player_3’, ’player_5’],
’predicted_opponent_next_inventory’: {’rock/yellow’: 1, ’paper/purple’:
1, ’scissors/blue’: 5}, ’my_next_inventory’: {’rock/yellow’: 5,
’paper/purple’: 1, ’scissors/blue’: 1} }
G.2 RunningwithScissors: Repeated
The prompts for Running with Scissors Repeated remain very similar as Running with Scissors:
Arenawiththemodificationofthepromptsbeingina2playersetting.
31G.3 Prisoner’sDilemma: Repeated
SystemMessage
You are Agent {self.agent_id} in the two player ’prisoner_dilemma_in_the_matrix’ Melting
Pot multiagent reinforcement learning environment that is an 23x15 (x by y) grid with
resources to collect and walls to navigate around. Your goal is to maximize reward.
Players can move around the map and collect resources of 2 discrete types corresponding
to cooperate and defect strategies - Green box = cooperate - Red box = defect. You and
another agent will collect resources and compare inventories in an encounter in repeated
rounds. In addition to movement, the agents have an action to fire an "interaction"
beam which initiates a duel to compare inventories and distribute rewards. All players
carry an inventory with the count of resources picked up since last respawn and for each
respawn start with an inventory of 1 resource each. This inventory is visible in the
state with the key ’inventory’. Here is how the payoff matrix in the generic prisoner
dilemma works:
• If both players cooperate, they both receive a reward of around 3,
• If one player cooperates and the other defects, the defector receives a reward of
around 5 and the cooperator receives a reward of around 0,
• If both players defect, they both receive a reward of around 1.
The nuance of ’prisoner_dilemma_in_the_matrix__repeated’ is that the rewards are using
the payoff rules of the classic prisoner dilemma, but the interaction is repeated and the
rewards are distributed based on the inventory of resources collected by each player.
• If both agents cooperate, the one with more cooperate resources will receive a
reward lower than the one with less cooperate resources,
• If one agent cooperates and the other defects, the more defect resources the
defector has, the higher the reward for the defector,
• If both agents defect, the one with more defect resources will receive a higher
reward than the one with less defect resources.
Your goal before each interaction is to try and infer what the other player will play
and how their strategy over time is affected by your plays. You will only want to
pick up one type of resource before an interaction. For example, the inventories
{’cooperate/green’: 1, ’defect/red’: 1} and {’cooperate/green’: 3, ’defect/red’: 3}
will both result in the same reward, so don’t waste time collecting more than you need.
Player 1 will also always only pick up one type of resource before an interaction. To
play a strategy strongly, pick up at least 6 resources or more of only one color and then
fire the interaction beam at the other player. To commit less strongly to a strategy,
pick up around 2 resources of only one color and then fire the interaction beam at the
other player. State Description: This environment is partially-observable, you can
observe a 5x5 grid around your agent depending on your position and orientation (you can
see more in front of you than behind). Previously seen states will be represented in
memory, but note that these states could potentially be outdated. For example, the other
agent could collect a resource that you previously saw. Given the partially-observable
nature of the environment, you will need to explore the environment appropriately and
select goals based on the information you’ve gathered. Also pay attention to Player
1’s position when you see it in order to duel with them and gain information about their
strategy. Your goal is to maximize reward attained over an entire episode, so keep in
mind the long-term consequences of your actions. Look at events in a gestalt manner.
32G.4 CollaborativeCookingPrompts
SystemMessageforCollaborativeCookingAsymmetric
You are Player {self.agent_id} in the Collaborative Cooking Asymmetric environment, the goal is
to cook and deliver tomato soup dishes with a partner. The environment consists of a kitchen
with a tomato dispenser, pots, delivery locations, and dish dispensers. Each agent (of 2) has
access to specific parts of the kitchen and can perform actions like picking up ingredients,
putting soup in a dish, and delivering cooked soup dishes. There is an impassable barrier in
the middle of the kitchen that separates the agents’ sides at x=4, where the pots are located.
The goal is to work together with the other agent to efficiently cook and serve as many dishes
of tomato soup as possible to maximize the collective reward. However, communication is not
possible, so you must infer your partner’s strategy from their actions and adapt accordingly to
coordinate tasks. To cook tomato soup, 1. put 3 tomatoes in a pot, 2. pick up a dish when
it is finished cooking, 3. put the cooked soup in a dish, and 4. deliver it to the delivery
location. Your team receives a reward of 20 for each successfully delivered dish. Only interact
with objects on your side of the kitchen. You can only hold one tomato at once. You cannot
pick up a tomato from the tomato dispenser with another item like a dish in your hand. You
need to pick up a dish before you pick up cooked soup from a pot. The environment is partially
observable, and you can only see a 5x5 grid around your agent. You will be prompted at different
points to provide high-level strategies and lower-level action plans to achieve them.
Use these three functions for lower-level action plans:
• move_to(src_coord, target_coord): Efficiently move agent from source coordinate to
target coordinate. Only move to valid move_to locations where counters or objects are
not present. Use sparingly.
• interact(target_coord): Move to and interact with the entity at the target coordinate,
such as picking up ingredients or delivering dishes of cooked soup. To place an object
down on a counter to free your hands, use interact(counter_coord). Mostly use this
function.
• wait(target_coord): Wait for the pot at target_coord to finish cooking. Check the
progress of the pots and only use valid locations where pots are present. You probably
only want to use this when both pots are full to maximize efficiency.
Most of the time you will just want to use the interact function because it both moves to and
interacts with objects, therefore all the cooking steps can be completed with the interact
function. To put down an item to pick something else up, interact with a counter to free your
hands. Do not put down items on the floor or the delivery location.
G.4.1 HypotheticalMinds
HighLevelStrategyMessage(inToMModuleofHM)forCollaborativeCookingAsymmetric
Strategy Request:
You are at step {step} of the game.
Provide a strategy for agent {self.agent_id}.
Your response should outline a high-level strategy - what strategy do you
want to take next and why?
Teammate’s observed strategy: {self.teammate_strategy}
Think step by step about how to adapt to their behavior and maximize all
resources and efficiency accordingly.
This response will be shown to you in the future in order for you to select
lower-level actions to implement this strategy.
Example response:
High-level strategy: I want to focus on cooking tomato soup dishes.
You will be prompted again shortly to select subgoals and action plans to
execute this strategy, so do not include that in your response yet.
33SubgoalModuleMessage
Strategy Request:
You are at step {step} of the game.
Your task is to devise efficient action plans for agent {self.agent_id},
reason through what the next subgoals should be given the state information.
Your previously specified high-level strategy is: {self.my_strategy}
Your response should be broken up into two parts:
1. Subgoal Plan - Based on the current state and the high-level strategy
you previously specified, decompose this strategy into a sequence
of subgoals and actions to efficiently implement this strategy. For
every subgoal, think step by step about the best action function and
parameter to use for that function. This could be fairly long.
2. Action Plan - Output this sequence of actions in the following Python
dictionary format, parsable by ast.literal_eval() starting with:
{{ ’action_plan’: [’interact((5, 1))’] }}
Example response 1:
Subgoal Plan: Given the current state and my high-level strategy to focus on
cooking tomato soup dishes, I should:
Move to the tomato dispenser and pick up a tomato.
{{ ’action_plan’: [’interact((5, 1))’] }}
Example response 2:
Subgoal Plan: Given the current state and my high-level strategy to focus on
delivering tomato soup dishes, I should:
Move to the dish dispenser and pick up a dish, then plate the cooked soup.
{{ ’action_plan’: [’interact((3, 4))’, ’interact((4, 2))’] }}
Example response 3:
Subgoal Plan: Next I should move to the delivery location and deliver the
cooked soup.
{{ ’action_plan’: [’interact((3, 1))’] }}
34EvaluateActionOutcomes/Self-Reflection
User Message Preamble:
If subgoal failed:
You are an action plan evaluator.
The last subgoal included an interact action that failed.
Your task is to look at the subgoal the agent took, the state of the
environment before and after the subgoal,
and evaluate why the subgoal was unsuccessful and provide feedback about what
the agent should do next time.
We will next plan an entire new action plan, so suggest specific action plans
and action functions to use next when applicable.
If subgoal succeeded:
You are an action plan evaluator.
Your task is to look at the action plan the agent took, the state of the
environment before the plan and the state of the environment after the plan,
and evaluate whether the action plan was successful, and if not, provide
feedback about what failed and what the agent should do next time.
Take into account that your teammate could have influenced the outcome of the
subgoal in some circumstances.
Suggest specific action plans and action functions to use next when
applicable.
InferTeammateStrategyMessage
User Message:
Based on the observed actions of your teammate (player_1), what do you think
their strategy is?
Are they specializing in any specific activity or subtask?
Teammate’s observed actions:
{self.teammate_actions}
Here are your previous hypotheses about the strategy your partner is playing:
{self.top_hypotheses}.
Think step by step and provide an analysis of their strategy, any
specialization you infer from their behavior, and their competence.
Then analyze how you can adapt your strategy to maximize efficiency and
coordination with your teammate.
Remember communication is not allowed.
35PredictTeammateBehaviorMessage
A dish has been delivered at step {step}.
You previously guessed that your teammate’s (player_1) policy is:
{possible_teammate_strategy}
Based on the proposed hypothesis about your teammate (player_1), what do you
think they will do next?
Output a concise label about your teammate’s next behavior in the following
Python dictionary format, parsable by ast.literal_eval() starting with:
python
{{ ’predicted_next_behavior’: ’placing tomatoes into pot (4,2)’ }}
EvaluatePredictedBehavior
A dish has been delivered at step {step}.
You previously guessed that your teammate’s (player_1) would perform this
behavior in this round: {predicted_next_behavior}
Here is the observed behavior of your teammate (player_1) in this round:
{latest_teammate_actions}
Did your prediction match the observed behavior?
Concisely output True or False in the below Python dictionary format,
parsable by ast.literal_eval() starting with:
{{ ’evaluate_predicted_behavior’: True }}
36