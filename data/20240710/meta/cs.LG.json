[
    {
        "title": "FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation",
        "authors": "Liqun MaMingjie SunZhiqiang Shen",
        "links": "http://arxiv.org/abs/2407.07093v1",
        "entry_id": "http://arxiv.org/abs/2407.07093v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07093v1",
        "summary": "This work presents a Fully BInarized Large Language Model (FBI-LLM),\ndemonstrating for the first time how to train a large-scale binary language\nmodel from scratch (not the partial binary or ternary LLM like BitNet b1.58) to\nmatch the performance of its full-precision counterparts (e.g., FP16 or BF16)\nin transformer-based LLMs. It achieves this by employing an autoregressive\ndistillation (AD) loss with maintaining equivalent model dimensions (130M,\n1.3B, 7B) and training data volume as regular LLM pretraining, while delivering\ncompetitive results in terms of perplexity and task-specific effectiveness.\nIntriguingly, by analyzing the training trajectory, we find that the pretrained\nweight is not necessary for training binarized LLMs from scratch. This research\nencourages a new computational framework and may facilitate the future design\nof specialized hardware tailored for fully 1-bit LLMs. We make all models,\ncode, and training dataset fully accessible and transparent to support further\nresearch (Code: https://github.com/LiqunMa/FBI-LLM. Model:\nhttps://huggingface.co/LiqunMa/).",
        "updated": "2024-07-09 17:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07093v1"
    },
    {
        "title": "Fine-Tuning Linear Layers Only Is a Simple yet Effective Way for Task Arithmetic",
        "authors": "Ruochen JinBojian HouJiancong XiaoWeijie SuLi Shen",
        "links": "http://arxiv.org/abs/2407.07089v1",
        "entry_id": "http://arxiv.org/abs/2407.07089v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07089v1",
        "summary": "Task arithmetic has recently emerged as a cost-effective and scalable\napproach to edit pre-trained models directly in weight space, by adding the\nfine-tuned weights of different tasks. The performance has been further\nimproved by a linear property which is illustrated by weight disentanglement.\nYet, conventional linearization methods (e.g., NTK linearization) not only\ndouble the time and training cost but also have a disadvantage on single-task\nperformance. We propose a simple yet effective and efficient method that only\nfine-tunes linear layers, which improves weight disentanglement and efficiency\nsimultaneously. Specifically, our study reveals that only fine-tuning the\nlinear layers in the attention modules makes the whole model occur in a linear\nregime, significantly improving weight disentanglement. To further understand\nhow our method improves the disentanglement of task arithmetic, we present a\ncomprehensive study of task arithmetic by differentiating the role of\nrepresentation model and task-specific model. In particular, we find that the\nrepresentation model plays an important role in improving weight\ndisentanglement whereas the task-specific models such as the classification\nheads can degenerate the weight disentanglement performance. Overall, our work\nuncovers novel insights into the fundamental mechanisms of task arithmetic and\noffers a more reliable and effective approach to editing pre-trained models.",
        "updated": "2024-07-09 17:59:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07089v1"
    },
    {
        "title": "CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation",
        "authors": "Tong ChenAkari AsaiNiloofar MireshghallahSewon MinJames GrimmelmannYejin ChoiHannaneh HajishirziLuke ZettlemoyerPang Wei Koh",
        "links": "http://arxiv.org/abs/2407.07087v1",
        "entry_id": "http://arxiv.org/abs/2407.07087v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07087v1",
        "summary": "Evaluating the degree of reproduction of copyright-protected content by\nlanguage models (LMs) is of significant interest to the AI and legal\ncommunities. Although both literal and non-literal similarities are considered\nby courts when assessing the degree of reproduction, prior research has focused\nonly on literal similarities. To bridge this gap, we introduce CopyBench, a\nbenchmark designed to measure both literal and non-literal copying in LM\ngenerations. Using copyrighted fiction books as text sources, we provide\nautomatic evaluation protocols to assess literal and non-literal copying,\nbalanced against the model utility in terms of the ability to recall facts from\nthe copyrighted works and generate fluent completions. We find that, although\nliteral copying is relatively rare, two types of non-literal copying -- event\ncopying and character copying -- occur even in models as small as 7B\nparameters. Larger models demonstrate significantly more copying, with literal\ncopying rates increasing from 0.2% to 10.5% and non-literal copying from 2.3%\nto 6.9% when comparing Llama3-8B and 70B models, respectively. We further\nevaluate the effectiveness of current strategies for mitigating copying and\nshow that (1) training-time alignment can reduce literal copying but may\nincrease non-literal copying, and (2) current inference-time mitigation methods\nprimarily reduce literal but not non-literal copying.",
        "updated": "2024-07-09 17:58:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07087v1"
    },
    {
        "title": "Stabilized Proximal-Point Methods for Federated Optimization",
        "authors": "Xiaowen JiangAnton RodomanovSebastian U. Stich",
        "links": "http://arxiv.org/abs/2407.07084v1",
        "entry_id": "http://arxiv.org/abs/2407.07084v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07084v1",
        "summary": "In developing efficient optimization algorithms, it is crucial to account for\ncommunication constraints -- a significant challenge in modern federated\nlearning settings. The best-known communication complexity among\nnon-accelerated algorithms is achieved by DANE, a distributed proximal-point\nalgorithm that solves local subproblems in each iteration and that can exploit\nsecond-order similarity among individual functions. However, to achieve such\ncommunication efficiency, the accuracy requirement for solving the local\nsubproblems is slightly sub-optimal. Inspired by the hybrid projection-proximal\npoint method, in this work, we i) propose a novel distributed algorithm S-DANE.\nThis method adopts a more stabilized prox-center in the proximal step compared\nwith DANE, and matches its deterministic communication complexity. Moreover,\nthe accuracy condition of the subproblem is milder, leading to enhanced local\ncomputation efficiency. Furthermore, it supports partial client participation\nand arbitrary stochastic local solvers, making it more attractive in practice.\nWe further ii) accelerate S-DANE, and show that the resulting algorithm\nachieves the best-known communication complexity among all existing methods for\ndistributed convex optimization, with the same improved local computation\nefficiency as S-DANE.",
        "updated": "2024-07-09 17:56:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07084v1"
    },
    {
        "title": "Can Learned Optimization Make Reinforcement Learning Less Difficult?",
        "authors": "Alexander David GoldieChris LuMatthew Thomas JacksonShimon WhitesonJakob Nicolaus Foerster",
        "links": "http://arxiv.org/abs/2407.07082v1",
        "entry_id": "http://arxiv.org/abs/2407.07082v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07082v1",
        "summary": "While reinforcement learning (RL) holds great potential for decision making\nin the real world, it suffers from a number of unique difficulties which often\nneed specific consideration. In particular: it is highly non-stationary;\nsuffers from high degrees of plasticity loss; and requires exploration to\nprevent premature convergence to local optima and maximize return. In this\npaper, we consider whether learned optimization can help overcome these\nproblems. Our method, Learned Optimization for Plasticity, Exploration and\nNon-stationarity (OPEN), meta-learns an update rule whose input features and\noutput structure are informed by previously proposed solutions to these\ndifficulties. We show that our parameterization is flexible enough to enable\nmeta-learning in diverse learning contexts, including the ability to use\nstochasticity for exploration. Our experiments demonstrate that when\nmeta-trained on single and small sets of environments, OPEN outperforms or\nequals traditionally used optimizers. Furthermore, OPEN shows strong\ngeneralization across a distribution of environments and a range of agent\narchitectures.",
        "updated": "2024-07-09 17:55:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07082v1"
    }
]