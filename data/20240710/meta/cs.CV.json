[
    {
        "title": "V-VIPE: Variational View Invariant Pose Embedding",
        "authors": "Mara LevyAbhinav Shrivastava",
        "links": "http://arxiv.org/abs/2407.07092v1",
        "entry_id": "http://arxiv.org/abs/2407.07092v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07092v1",
        "summary": "Learning to represent three dimensional (3D) human pose given a two\ndimensional (2D) image of a person, is a challenging problem. In order to make\nthe problem less ambiguous it has become common practice to estimate 3D pose in\nthe camera coordinate space. However, this makes the task of comparing two 3D\nposes difficult. In this paper, we address this challenge by separating the\nproblem of estimating 3D pose from 2D images into two steps. We use a\nvariational autoencoder (VAE) to find an embedding that represents 3D poses in\ncanonical coordinate space. We refer to this embedding as variational\nview-invariant pose embedding V-VIPE. Using V-VIPE we can encode 2D and 3D\nposes and use the embedding for downstream tasks, like retrieval and\nclassification. We can estimate 3D poses from these embeddings using the\ndecoder as well as generate unseen 3D poses. The variability of our encoding\nallows it to generalize well to unseen camera views when mapping from 2D space.\nTo the best of our knowledge, V-VIPE is the only representation to offer this\ndiversity of applications. Code and more information can be found at\nhttps://v-vipe.github.io/.",
        "updated": "2024-07-09 17:59:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07092v1"
    },
    {
        "title": "3D Gaussian Ray Tracing: Fast Tracing of Particle Scenes",
        "authors": "Nicolas Moenne-LoccozAshkan MirzaeiOr PerelRiccardo de LutioJanick Martinez EsturoGavriel StateSanja FidlerNicholas SharpZan Gojcic",
        "links": "http://arxiv.org/abs/2407.07090v1",
        "entry_id": "http://arxiv.org/abs/2407.07090v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07090v1",
        "summary": "Particle-based representations of radiance fields such as 3D Gaussian\nSplatting have found great success for reconstructing and re-rendering of\ncomplex scenes. Most existing methods render particles via rasterization,\nprojecting them to screen space tiles for processing in a sorted order. This\nwork instead considers ray tracing the particles, building a bounding volume\nhierarchy and casting a ray for each pixel using high-performance GPU ray\ntracing hardware. To efficiently handle large numbers of semi-transparent\nparticles, we describe a specialized rendering algorithm which encapsulates\nparticles with bounding meshes to leverage fast ray-triangle intersections, and\nshades batches of intersections in depth-order. The benefits of ray tracing are\nwell-known in computer graphics: processing incoherent rays for secondary\nlighting effects such as shadows and reflections, rendering from\nhighly-distorted cameras common in robotics, stochastically sampling rays, and\nmore. With our renderer, this flexibility comes at little cost compared to\nrasterization. Experiments demonstrate the speed and accuracy of our approach,\nas well as several applications in computer graphics and vision. We further\npropose related improvements to the basic Gaussian representation, including a\nsimple use of generalized kernel functions which significantly reduces particle\nhit counts.",
        "updated": "2024-07-09 17:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07090v1"
    },
    {
        "title": "MoSt-DSA: Modeling Motion and Structural Interactions for Direct Multi-Frame Interpolation in DSA Images",
        "authors": "Ziyang XuHuangxuan ZhaoZiwei CuiWenyu LiuChuansheng ZhengXinggang Wang",
        "links": "http://arxiv.org/abs/2407.07078v1",
        "entry_id": "http://arxiv.org/abs/2407.07078v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07078v1",
        "summary": "Artificial intelligence has become a crucial tool for medical image analysis.\nAs an advanced cerebral angiography technique, Digital Subtraction Angiography\n(DSA) poses a challenge where the radiation dose to humans is proportional to\nthe image count. By reducing images and using AI interpolation instead, the\nradiation can be cut significantly. However, DSA images present more complex\nmotion and structural features than natural scenes, making interpolation more\nchallenging. We propose MoSt-DSA, the first work that uses deep learning for\nDSA frame interpolation. Unlike natural scene Video Frame Interpolation (VFI)\nmethods that extract unclear or coarse-grained features, we devise a general\nmodule that models motion and structural context interactions between frames in\nan efficient full convolution manner by adjusting optimal context range and\ntransforming contexts into linear functions. Benefiting from this, MoSt-DSA is\nalso the first method that directly achieves any number of interpolations at\nany time steps with just one forward pass during both training and testing. We\nconduct extensive comparisons with 7 representative VFI models for\ninterpolating 1 to 3 frames, MoSt-DSA demonstrates robust results across 470\nDSA image sequences (each typically 152 images), with average SSIM over 0.93,\naverage PSNR over 38 (standard deviations of less than 0.030 and 3.6,\nrespectively), comprehensively achieving state-of-the-art performance in\naccuracy, speed, visual effect, and memory usage. Our code is available at\nhttps://github.com/ZyoungXu/MoSt-DSA.",
        "updated": "2024-07-09 17:50:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07078v1"
    },
    {
        "title": "ConceptExpress: Harnessing Diffusion Models for Single-image Unsupervised Concept Extraction",
        "authors": "Shaozhe HaoKai HanZhengyao LvShihao ZhaoKwan-Yee K. Wong",
        "links": "http://arxiv.org/abs/2407.07077v1",
        "entry_id": "http://arxiv.org/abs/2407.07077v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07077v1",
        "summary": "While personalized text-to-image generation has enabled the learning of a\nsingle concept from multiple images, a more practical yet challenging scenario\ninvolves learning multiple concepts within a single image. However, existing\nworks tackling this scenario heavily rely on extensive human annotations. In\nthis paper, we introduce a novel task named Unsupervised Concept Extraction\n(UCE) that considers an unsupervised setting without any human knowledge of the\nconcepts. Given an image that contains multiple concepts, the task aims to\nextract and recreate individual concepts solely relying on the existing\nknowledge from pretrained diffusion models. To achieve this, we present\nConceptExpress that tackles UCE by unleashing the inherent capabilities of\npretrained diffusion models in two aspects. Specifically, a concept\nlocalization approach automatically locates and disentangles salient concepts\nby leveraging spatial correspondence from diffusion self-attention; and based\non the lookup association between a concept and a conceptual token, a\nconcept-wise optimization process learns discriminative tokens that represent\neach individual concept. Finally, we establish an evaluation protocol tailored\nfor the UCE task. Extensive experiments demonstrate that ConceptExpress is a\npromising solution to the UCE task. Our code and data are available at:\nhttps://github.com/haoosz/ConceptExpress",
        "updated": "2024-07-09 17:50:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07077v1"
    },
    {
        "title": "MADE-for-ASD: A Multi-Atlas Deep Ensemble Network for Diagnosing Autism Spectrum Disorder",
        "authors": "Md Rakibul HasanXuehan LiuTom GedeonMd Zakir Hossain",
        "links": "http://arxiv.org/abs/2407.07076v1",
        "entry_id": "http://arxiv.org/abs/2407.07076v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07076v1",
        "summary": "In response to the global need for efficient early diagnosis of Autism\nSpectrum Disorder (ASD), this paper bridges the gap between traditional,\ntime-consuming diagnostic methods and potential automated solutions. We propose\na multi-atlas deep ensemble network, MADE-for-ASD, that integrates multiple\natlases of the brain's functional magnetic resonance imaging (fMRI) data\nthrough a weighted deep ensemble network. Our approach integrates demographic\ninformation into the prediction workflow, which enhances ASD diagnosis\nperformance and offers a more holistic perspective on patient profiling. We\nexperiment with the well-known publicly available ABIDE (Autism Brain Imaging\nData Exchange) I dataset, consisting of resting state fMRI data from 17\ndifferent laboratories around the globe. Our proposed system achieves 75.20%\naccuracy on the entire dataset and 96.40% on a specific subset $-$ both\nsurpassing reported ASD diagnosis accuracy in ABIDE I fMRI studies.\nSpecifically, our model improves by 4.4 percentage points over prior works on\nthe same amount of data. The model exhibits a sensitivity of 82.90% and a\nspecificity of 69.70% on the entire dataset, and 91.00% and 99.50%,\nrespectively, on the specific subset. We leverage the F-score to pinpoint the\ntop 10 ROI in ASD diagnosis, such as \\emph{precuneus} and anterior\n\\emph{cingulate/ventromedial}. The proposed system can potentially pave the way\nfor more cost-effective, efficient and scalable strategies in ASD diagnosis.\nCodes and evaluations are publicly available at TBA.",
        "updated": "2024-07-09 17:49:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07076v1"
    }
]