[
    {
        "title": "Adaptively Robust and Sparse K-means Clustering",
        "authors": "Hao LiShonosuke SugasawaShota Katayama",
        "links": "http://arxiv.org/abs/2407.06945v1",
        "entry_id": "http://arxiv.org/abs/2407.06945v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06945v1",
        "summary": "While K-means is known to be a standard clustering algorithm, it may be\ncompromised due to the presence of outliers and high-dimensional noisy\nvariables. This paper proposes adaptively robust and sparse K-means clustering\n(ARSK) to address these practical limitations of the standard K-means\nalgorithm. We introduce a redundant error component for each observation for\nrobustness, and this additional parameter is penalized using a group sparse\npenalty. To accommodate the impact of high-dimensional noisy variables, the\nobjective function is modified by incorporating weights and implementing a\npenalty to control the sparsity of the weight vector. The tuning parameters to\ncontrol the robustness and sparsity are selected by Gap statistics. Through\nsimulation experiments and real data analysis, we demonstrate the superiority\nof the proposed method to existing algorithms in identifying clusters without\noutliers and informative variables simultaneously.",
        "updated": "2024-07-09 15:20:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06945v1"
    },
    {
        "title": "Bayesian Federated Learning with Hamiltonian Monte Carlo: Algorithm and Theory",
        "authors": "Jiajun LiangQian ZhangWei DengQifan SongGuang Lin",
        "links": "http://arxiv.org/abs/2407.06935v1",
        "entry_id": "http://arxiv.org/abs/2407.06935v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06935v1",
        "summary": "This work introduces a novel and efficient Bayesian federated learning\nalgorithm, namely, the Federated Averaging stochastic Hamiltonian Monte Carlo\n(FA-HMC), for parameter estimation and uncertainty quantification. We establish\nrigorous convergence guarantees of FA-HMC on non-iid distributed data sets,\nunder the strong convexity and Hessian smoothness assumptions. Our analysis\ninvestigates the effects of parameter space dimension, noise on gradients and\nmomentum, and the frequency of communication (between the central node and\nlocal nodes) on the convergence and communication costs of FA-HMC. Beyond that,\nwe establish the tightness of our analysis by showing that the convergence rate\ncannot be improved even for continuous FA-HMC process. Moreover, extensive\nempirical studies demonstrate that FA-HMC outperforms the existing Federated\nAveraging-Langevin Monte Carlo (FA-LD) algorithm.",
        "updated": "2024-07-09 15:10:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06935v1"
    },
    {
        "title": "Distributionally robust risk evaluation with an isotonic constraint",
        "authors": "Yu GuiRina Foygel BarberCong Ma",
        "links": "http://arxiv.org/abs/2407.06867v1",
        "entry_id": "http://arxiv.org/abs/2407.06867v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06867v1",
        "summary": "Statistical learning under distribution shift is challenging when neither\nprior knowledge nor fully accessible data from the target distribution is\navailable. Distributionally robust learning (DRL) aims to control the\nworst-case statistical performance within an uncertainty set of candidate\ndistributions, but how to properly specify the set remains challenging. To\nenable distributional robustness without being overly conservative, in this\npaper, we propose a shape-constrained approach to DRL, which incorporates prior\ninformation about the way in which the unknown target distribution differs from\nits estimate. More specifically, we assume the unknown density ratio between\nthe target distribution and its estimate is isotonic with respect to some\npartial order. At the population level, we provide a solution to the\nshape-constrained optimization problem that does not involve the isotonic\nconstraint. At the sample level, we provide consistency results for an\nempirical estimator of the target in a range of different settings. Empirical\nstudies on both synthetic and real data examples demonstrate the improved\naccuracy of the proposed shape-constrained approach.",
        "updated": "2024-07-09 13:56:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06867v1"
    },
    {
        "title": "ED-VAE: Entropy Decomposition of ELBO in Variational Autoencoders",
        "authors": "Fotios LygerakisElmar Rueckert",
        "links": "http://arxiv.org/abs/2407.06797v1",
        "entry_id": "http://arxiv.org/abs/2407.06797v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06797v1",
        "summary": "Traditional Variational Autoencoders (VAEs) are constrained by the\nlimitations of the Evidence Lower Bound (ELBO) formulation, particularly when\nutilizing simplistic, non-analytic, or unknown prior distributions. These\nlimitations inhibit the VAE's ability to generate high-quality samples and\nprovide clear, interpretable latent representations. This work introduces the\nEntropy Decomposed Variational Autoencoder (ED-VAE), a novel re-formulation of\nthe ELBO that explicitly includes entropy and cross-entropy components. This\nreformulation significantly enhances model flexibility, allowing for the\nintegration of complex and non-standard priors. By providing more detailed\ncontrol over the encoding and regularization of latent spaces, ED-VAE not only\nimproves interpretability but also effectively captures the complex\ninteractions between latent variables and observed data, thus leading to better\ngenerative performance.",
        "updated": "2024-07-09 12:09:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06797v1"
    },
    {
        "title": "A Generalization Bound for Nearly-Linear Networks",
        "authors": "Eugene Golikov",
        "links": "http://arxiv.org/abs/2407.06765v1",
        "entry_id": "http://arxiv.org/abs/2407.06765v1",
        "pdf_url": "http://arxiv.org/pdf/2407.06765v1",
        "summary": "We consider nonlinear networks as perturbations of linear ones. Based on this\napproach, we present novel generalization bounds that become non-vacuous for\nnetworks that are close to being linear. The main advantage over the previous\nworks which propose non-vacuous generalization bounds is that our bounds are\na-priori: performing the actual training is not required for evaluating the\nbounds. To the best of our knowledge, they are the first non-vacuous\ngeneralization bounds for neural nets possessing this property.",
        "updated": "2024-07-09 11:20:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.06765v1"
    }
]