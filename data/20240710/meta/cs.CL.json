[
    {
        "title": "AnyTaskTune: Advanced Domain-Specific Solutions through Task-Fine-Tuning",
        "authors": "Jiaxi CuiWentao ZhangJing TangXudong TongZhenwei ZhangAmieJing WenRongsheng WangPengfei Wu",
        "links": "http://arxiv.org/abs/2407.07094v1",
        "entry_id": "http://arxiv.org/abs/2407.07094v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07094v1",
        "summary": "The pervasive deployment of Large Language Models-LLMs in various sectors\noften neglects the nuanced requirements of individuals and small organizations,\nwho benefit more from models precisely tailored to their specific business\ncontexts rather than those with broadly superior general capabilities. This\nwork introduces \\textbf{AnyTaskTune}, a novel fine-tuning methodology coined as\n\\textbf{Task-Fine-Tune}, specifically developed to elevate model performance on\na diverse array of domain-specific tasks. This method involves a meticulous\nprocess to identify and define targeted sub-tasks within a domain, followed by\nthe creation of specialized enhancement datasets for fine-tuning, thereby\noptimizing task-specific model performance. We conducted comprehensive\nfine-tuning experiments not only in the legal domain for tasks such as keyword\nextraction and sentence prediction but across over twenty different sub-tasks\nderived from the domains of finance, healthcare, law, psychology, consumer\nservices, and human resources. To substantiate our approach and facilitate\ncommunity engagement, we will open-source these bilingual task datasets. Our\nfindings demonstrate that models fine-tuned using the \\textbf{Task-Fine-Tune}\nmethodology not only achieve superior performance on these specific tasks but\nalso significantly outperform models with higher general capabilities in their\nrespective domains. Our work is publicly available at\n\\url{https://github.com/PandaVT/DataTager}.",
        "updated": "2024-07-09 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07094v1"
    },
    {
        "title": "FBI-LLM: Scaling Up Fully Binarized LLMs from Scratch via Autoregressive Distillation",
        "authors": "Liqun MaMingjie SunZhiqiang Shen",
        "links": "http://arxiv.org/abs/2407.07093v1",
        "entry_id": "http://arxiv.org/abs/2407.07093v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07093v1",
        "summary": "This work presents a Fully BInarized Large Language Model (FBI-LLM),\ndemonstrating for the first time how to train a large-scale binary language\nmodel from scratch (not the partial binary or ternary LLM like BitNet b1.58) to\nmatch the performance of its full-precision counterparts (e.g., FP16 or BF16)\nin transformer-based LLMs. It achieves this by employing an autoregressive\ndistillation (AD) loss with maintaining equivalent model dimensions (130M,\n1.3B, 7B) and training data volume as regular LLM pretraining, while delivering\ncompetitive results in terms of perplexity and task-specific effectiveness.\nIntriguingly, by analyzing the training trajectory, we find that the pretrained\nweight is not necessary for training binarized LLMs from scratch. This research\nencourages a new computational framework and may facilitate the future design\nof specialized hardware tailored for fully 1-bit LLMs. We make all models,\ncode, and training dataset fully accessible and transparent to support further\nresearch (Code: https://github.com/LiqunMa/FBI-LLM. Model:\nhttps://huggingface.co/LiqunMa/).",
        "updated": "2024-07-09 17:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07093v1"
    },
    {
        "title": "CopyBench: Measuring Literal and Non-Literal Reproduction of Copyright-Protected Text in Language Model Generation",
        "authors": "Tong ChenAkari AsaiNiloofar MireshghallahSewon MinJames GrimmelmannYejin ChoiHannaneh HajishirziLuke ZettlemoyerPang Wei Koh",
        "links": "http://arxiv.org/abs/2407.07087v1",
        "entry_id": "http://arxiv.org/abs/2407.07087v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07087v1",
        "summary": "Evaluating the degree of reproduction of copyright-protected content by\nlanguage models (LMs) is of significant interest to the AI and legal\ncommunities. Although both literal and non-literal similarities are considered\nby courts when assessing the degree of reproduction, prior research has focused\nonly on literal similarities. To bridge this gap, we introduce CopyBench, a\nbenchmark designed to measure both literal and non-literal copying in LM\ngenerations. Using copyrighted fiction books as text sources, we provide\nautomatic evaluation protocols to assess literal and non-literal copying,\nbalanced against the model utility in terms of the ability to recall facts from\nthe copyrighted works and generate fluent completions. We find that, although\nliteral copying is relatively rare, two types of non-literal copying -- event\ncopying and character copying -- occur even in models as small as 7B\nparameters. Larger models demonstrate significantly more copying, with literal\ncopying rates increasing from 0.2% to 10.5% and non-literal copying from 2.3%\nto 6.9% when comparing Llama3-8B and 70B models, respectively. We further\nevaluate the effectiveness of current strategies for mitigating copying and\nshow that (1) training-time alignment can reduce literal copying but may\nincrease non-literal copying, and (2) current inference-time mitigation methods\nprimarily reduce literal but not non-literal copying.",
        "updated": "2024-07-09 17:58:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07087v1"
    },
    {
        "title": "Adapting LLMs to Hebrew: Unveiling DictaLM 2.0 with Enhanced Vocabulary and Instruction Capabilities",
        "authors": "Shaltiel ShmidmanAvi ShmidmanAmir DN CohenMoshe Koppel",
        "links": "http://arxiv.org/abs/2407.07080v1",
        "entry_id": "http://arxiv.org/abs/2407.07080v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07080v1",
        "summary": "Training large language models (LLMs) in low-resource languages such as\nHebrew poses unique challenges. In this paper, we introduce DictaLM2.0 and\nDictaLM2.0-Instruct, two LLMs derived from the Mistral model, trained on a\nsubstantial corpus of approximately 200 billion tokens in both Hebrew and\nEnglish. Adapting a pre-trained model to a new language involves specialized\ntechniques that differ significantly from training a model from scratch or\nfurther training existing models on well-resourced languages such as English.\nWe outline these novel training methodologies, which facilitate effective\nlearning and adaptation to the linguistic properties of Hebrew. Additionally,\nwe fine-tuned DictaLM2.0-Instruct on a comprehensive instruct dataset to\nenhance its performance on task-specific instructions. To rigorously evaluate\nour models, we introduce a new benchmark suite for Hebrew LLM evaluation,\ncovering a diverse set of tasks including Question Answering, Sentiment\nAnalysis, Winograd Schema Challenge, Translation, and Summarization. Our work\nnot only addresses the intricacies of training LLMs in low-resource languages\nbut also proposes a framework that can be leveraged for adapting other LLMs to\nvarious non-English languages, contributing to the broader field of\nmultilingual NLP.",
        "updated": "2024-07-09 17:51:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07080v1"
    },
    {
        "title": "Lookback Lens: Detecting and Mitigating Contextual Hallucinations in Large Language Models Using Only Attention Maps",
        "authors": "Yung-Sung ChuangLinlu QiuCheng-Yu HsiehRanjay KrishnaYoon KimJames Glass",
        "links": "http://arxiv.org/abs/2407.07071v1",
        "entry_id": "http://arxiv.org/abs/2407.07071v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07071v1",
        "summary": "When asked to summarize articles or answer questions given a passage, large\nlanguage models (LLMs) can hallucinate details and respond with unsubstantiated\nanswers that are inaccurate with respect to the input context. This paper\ndescribes a simple approach for detecting such contextual hallucinations. We\nhypothesize that contextual hallucinations are related to the extent to which\nan LLM attends to information in the provided context versus its own\ngenerations. Based on this intuition, we propose a simple hallucination\ndetection model whose input features are given by the ratio of attention\nweights on the context versus newly generated tokens (for each attention head).\nWe find that a linear classifier based on these lookback ratio features is as\neffective as a richer detector that utilizes the entire hidden states of an LLM\nor a text-based entailment model. The lookback ratio-based detector -- Lookback\nLens -- is found to transfer across tasks and even models, allowing a detector\nthat is trained on a 7B model to be applied (without retraining) to a larger\n13B model. We further apply this detector to mitigate contextual\nhallucinations, and find that a simple classifier-guided decoding approach is\nable to reduce the amount of hallucination, for example by 9.6% in the XSum\nsummarization task.",
        "updated": "2024-07-09 17:44:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07071v1"
    }
]