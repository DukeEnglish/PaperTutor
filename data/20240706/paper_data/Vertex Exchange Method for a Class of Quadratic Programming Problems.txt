Vertex Exchange Method for a Class of Quadratic
Programming Problems
Ling Liang liang.ling@u.nus.edu
Department of Mathematics
University of Maryland, College Park
4176 Campus Drive, College Park, MD 20742, USA
Kim-Chuan Toh mattohkc@nus.edu.sg
Department of Mathematics and Institute of Operations Research and Analytics
National University of Singapore
10 Lower Kent Ridge Road, Singapore 119076
Haizhao Yang hzyang@umd.edu
Department of Mathematics and Department of Computer Science
University of Maryland, College Park
4176 Campus Drive, College Park, MD 20742, USA
Abstract
A vertex exchange method is proposed for solving the strongly convex quadratic program
subject to the generalized simplex constraint. We conduct rigorous convergence analysis
for the proposed algorithm and demonstrate its essential roles in solving some important
classes of constrained convex optimization. To get a feasible initial point to execute the
algorithm, we also present and analyze a highly efficient semismooth Newton method for
computingtheprojectionontothegeneralizedsimplex. Theexcellentpracticalperformance
of the proposed algorithms is demonstrated by a set of extensive numerical experiments.
Our theoretical and numerical results further motivate the potential applications of the
considered model and the proposed algorithms.
Keywords: Vertex Exchange Method, Convex Quadratic Programming, Generalized
Simplex Constraint, Semismooth Newton Method, Projection
1 Introduction
A convex quadratic program (QP) involves the minimization of a convex quadratic objec-
tive functionsubjecttolinearequality andinequality constraints (Boyd and Vandenberghe,
2004). Solving convex QPs efficiently, robustly and accurately has long been one of the
most fundamental tasks in the field of optimization thanks to the numerous important
application areas, including optimal control (Qin and Badgwell, 2003), operations research
(Hillier and Lieberman, 2015), and statistical and machine learning (James et al., 2013;
Mohri et al., 2018), and its roles in developing more advanced algorithmic frameworks for
generaloptimization tasks(Pang and Qi,1995;Nocedal and Wright,1999;Liu et al.,2022).
In this paper, we shall focus on the following strongly convex QP problem subject to a
generalized simplex constraint:
1
min q(x) := xTQx+cTx s.t. eTx = b, ℓ ≤ x ≤ u, (1)
x∈Rn 2
©2024LingLiang,Kim-ChuanTohandHaizhaoYang.
License: CC-BY4.0,seehttps://creativecommons.org/licenses/by/4.0/.
4202
luJ
3
]CO.htam[
1v49230.7042:viXraLiang, Toh and Yang
where e ∈ Rn denotes the vector of all ones, b ∈ R, ℓ,u ∈ Rn satisfying −∞ < ℓ ≤ u < ∞,
and Q ∈ Sn . Obviously, when b = 1, ℓ = 0 and u = e, the feasible set of problem (1)
++
reduces to the standard simplex. In view of this, we call the feasible set of problem (1)
as the generalized simplex for the rest of this paper. For notational simplicity, we denote
C(ℓ,u) := {x ∈ Rn : ℓ ≤ x ≤ u} and assume that it is not empty. Though the constraint
set of problem (1) is relatively simple, it admits strong modeling power due to the fact
that many importantproblems can bemodeled by usingthegeneralized simplex constraint,
includingportfoliooptimization (Shepp and Vardi,1982;Algoet and Cover,1988),positron
emissiontomography(Ben-Tal et al.,2001;Cornuejols and Tu¨tu¨ncu¨,2006),andoptimalex-
perimental design (Pukelsheim, 2006; Damla Ahipasaoglu et al., 2008; Lu and Pong, 2013;
Yang et al., 2020; Hendrych et al., 2023), to mention just a few. Motivated by the essential
role of the problem (1) in solving these practical applications, our goal in this work is to
develop an efficient and reliable solution method for solving the QP problem (1).
Clearly, general-purpose solvers for solving convex QPs can be applied for solving (1)
directly. In the following, we provide a brief review of existing methods for solving general
convex QPs. Active set methods (Wolfe, 1959) represents one of the earliest approaches
for solving QPs, which extends the idea of the simplex method for solving linear programs
(Dantzig, 2016). Interior point methods (IPMs) (Nesterov and Nemirovskii, 1994; Wright,
1997)havegainedprominenceduetotheirstrongtheoreticalfoundationsandexcellentprac-
tical performance for small to medium scale problems. However, these methods become
inefficient when the problem size is large. The augmented Lagrangian methods (ALMs)
(Hermans et al., 2022; Liang et al., 2022) have gained increasing attentions in recent years
because of the promising practical performance and decent scalability. However, ALMs
can still be inefficient when solving very large-scale problems due to the need of solving a
sequence of linear systems, which is time-consuming. To design scalable algorithmic frame-
works, recent years have witnessed significant advancements in developing and analyzing
first-order methods (FOMs) for solving QPs. These FOMs includethe Frank-Wolfe method
(also known as the conditional gradient method) (Frank and Wolfe, 1956), projected gradi-
ent method and its variants (Nesterov, 1983; Beck and Teboulle, 2009), operator splitting
method (Stellato et al., 2020), and alternating direction method of multipliers (ADMM)
(O’donoghue et al., 2016; Li et al., 2016).
The Frank-Wolfe method guarantees global convergence; however, its convergence rate
can be extremely slow due to the ”zigzagging phenomenon.” Recently, the projection-free
nature of the Frank-Wolfe method has renewed interest, leading to significant theoretical
and practical advancements. In particular, (Lacoste-Julien and Jaggi, 2015) showed that
when the feasible set is the convex hull of a set of points (which needs to be given ex-
plicitly), then the away-step Frank-Wolfe (ASFW) method has a global linear convergence
rate when the objective function is strongly convex. For example, for problem (1) with
the standard simplex constraint, the ASFW method has shown great efficiency, leading to
many interesting applications (Liu et al., 2022). However, the method can not be directly
applied for solving problem (1) since there is no explicit convex hull characterization for
the generalized simplex constraint. Similarly, for the projected gradient-type methods, the
convergence rate can also be slow. Combining the active-set strategy with the projected
gradient method turns out to be much more efficient; see, e.g., (Hager and Zhang, 2023)
and references therein along this direction. In light of this, developing highly efficient al-
2Vertex Exchange Method for QPs
gorithms for computing projections onto the generalized simplex can further enhance the
performance of projection-based methods and is of independent interest. For the operator
splitting method and ADMM, although usually only one linear system is solved, the cost of
solving this linear system can dominate when dealing with large-scale problems.
Following theresearch themeofdevelopingefficientandscalableFOMs forsolvinglarge-
scale QPs, the primary goal of this paper is in proposing a vertex exchange method and
analyzing it is convergence. We shall mention that the proposed method is motivated by
the algorithm developed in (B¨ohning, 1986) for solving the D-optimal experimental design
problem, whose feasible set is the standard simplex. However, the convergence properties
provided by (B¨ohning, 1986) can not be applied to the generalized simplex constraint since
the objective function is different and the upper bound of the step-length is no longer 1 but
depends on the current iterate. Thus, whether the vertex exchange method is convergent
for convex optimization subject to the generalized simplex constraint remains unknown.
In this work, we provide a partial answer to this question by showing that the vertex
exchange method applied for solving the problem (1) admits global convergence. This fur-
ther results in global convergent algorithms for several important classes of constrained
convex optimization problems, including constrained convex SC1 minimization problems
(Pang and Qi, 1995; Chen et al., 2010) and constrained self-concordant minimization prob-
lems (Sun and Tran-Dinh, 2019; Tran-Dinh et al., 2022; Liu et al., 2022), whose essential
task is to solve astrongly convex QP problemof theform(1). Sincetheproposedalgorithm
requires a feasible starting point, we also present and analyze a highly efficient algorithm
for computing the projection onto the generalized simplex. Finally, our extensive numer-
ical study shows that the proposed algorithm is easy to implement and admits excellent
practical performance when compared with state-of-the-art solvers.
The rest of this paper is organized as follows. We first present some notation and back-
ground information that is needed for our later exposition in Section 2. Then in Section 3,
we present the main method, i.e., a vertex exchange method, for solving the problem (1),
and analyze its convergence. In the same section, we also present and analyze a fast semis-
mooth Newton for computing the projection on the generalized simplex as defined in (1).
To further demonstrate the potential applications of the proposed model and algorithms,
we explain how to apply the algorithms for solving two classes of important constrained
convex optimization problemsinSection 4. Moreover, toevaluatethepractical performance
of the proposed method, we conduct a set of extensive numerical experiments in Section 5.
Finally, we provide some concluding remarks in Section 6.
2 Preliminary
We first introduce some notation that will be used throughout this paper. Let x ∈ Rn be
a column vector and i ∈ [n] := {1,...,n}, we use x to denote the i-th entry of x. For a
i
given matrix A ∈ Rn×n, the (i,j)-entry of A is denoted as A and the i-th column of A is
ij
denoted as A , for 1 ≤ i,j ≤ n. Moreover, the vector formed by all the diagonal entries
:,i
of A is denoted as diag(A). Similarly, for any vector x ∈ Rn, the diagonal matrix in Rn×n
whose diagonal entries are those of x is denoted as Diag(x).
LetH ∈ Sn ,weuseh·,·i todenotetheinnerproductwithrespecttoH,i.e.,hx,yi =
++ H H
xTHy. Correspondingly, the induced weighted norm is denoted as k·k . Conventionally,
H
3Liang, Toh and Yang
when H is the identity matrix, we use h·,·i and k·k to denote the Euclidean inner product
and Euclidean norm, respectively.
Let f : Rn → R∪{±∞} be an extended real-valued function, we denote the effective
domain of f as dom(f) := {x ∈ Rn : f(x) < +∞}. Let f : Rn → R ∪ {±∞} be an
extended-valued convex function, we denote the convex conjugate function of f as f∗(z) :=
sup{hx,zi−f(x) : x ∈ Rn}, and the subdifferential of f at a point x ∈ Rn as ∂f(x) :=
{v ∈ Rn : f(x′)≥ f(x)+vT(x′−x), ∀x′ ∈ Rn}.
For a given closed convex and nonempty subset C of Rn, we use I (·) to denote the
C
indicatorfunctionofC,whichisaclosedproperconvexfunction. Thus,theconvexconjugate
function of I (·) is denoted as I∗(·), which is also known as the support function of the
C C
set C. Note that ∂I (x) coincides with the indicator function of the normal cone of C
C
at the reference point x (i.e., N (x)). In particular, we have that N (x) := {v ∈ Rn :
C C
vT(x′ −x) ≤ 0, ∀x′ ∈ C}. Moreover, the projection onto the set C is denoted as Proj (·).
C
When C = C(ℓ,u), it is known that the support function of C(ℓ,u) is given as (see e.g.,
(Banjac et al., 2019, Section 6.1))
I∗ (z) = ℓT min{z,0}+uT max{z,0}, ∀z ∈ Rn,
C(ℓ,u)
where “min” and “max” are applied element-wisely. Moreover, by the definition of the
normal cone, one can verify that for any x ∈ C(ℓ,u), v ∈ N (x) admits the following
C(ℓ,u)
expression (see e.g., (Rockafellar and Wets, 2009, Example 6.10))
= 0, ℓ < x < u ,
i i i
v ≤ 0, x = ℓ , ∀ i ∈[n], (2)
i i i

≥ 0, x = u ,
 i i
provided that ℓ < u. We referthe reader to (Rockafellar, 1997;Rockafellar and Wets, 2009)
for more comprehensive studies of convex analysis and variational analysis.
Next,wepresentthedefinitionof(strong)semismoothnessofanonsmoothmappingthat
is needed in analyzing the convergence of a certain Newton-type method; see (Mifflin, 1977;
Qi and Sun, 1993; Sun and Sun, 2002; Facchinei and Pang, 2003) and references therein for
more details on semismoothness.
Definition 1 LetX and Y be two finite dimensional Euclidean spaces. LetM :O ⊆ X → Y
be a locally Lipschitz continuous function on the open set O 1 whose generalized Jacobian
(Clarke, 1990) is defined as ∂M(x) := conv(∂ M(x)), where ∂ M(x) denotes the B-
B B
subdifferential (Qi, 1993) of M at x ∈O that is defined as
∂ M(x) := lim M′(xk) : lim xk = x, M′(xk) exists ,
B
k→∞ k→∞
(cid:26) (cid:27)
and “conv” stands for the convex hull in the usual sense of convex analysis (Rockafellar,
1997). Then, M is said to be semismooth at x ∈ O if M is directionally differentiable at x
and for any V ∈ ∂M(x+∆x) with ∆x → 0,
M(x+∆x)−M(x)−V∆x = o(k∆xk).
′
1. Hence,theJacobiofM,namelyM,iswell-definedalmosteverywhereinthesenseofLebesguemeasure
on O (Rademacher, 1919).
4Vertex Exchange Method for QPs
Moreover, M is said to be strongly semismooth at x ∈ O if M is semismooth and for any
V ∈ ∂M(x+∆x) with ∆x → 0,
M(x+∆x)−M(x)−V∆x =O(k∆xk2).
Finally, M is said to be semismooth (respectively, strongly semismooth) on O if M is
semismooth (respectively, strongly semismooth) at every point x ∈ O.
Roughly speaking, (strongly) semismooth mappings are locally Lipschitz continuous
mappings for which the generalized Jacobians define legitimate approximation schemes,
which turn out to be broad enough to include most of the interesting cases. Particularly,
the following result is well-known.
Lemma 2 ((Facchinei and Pang, 2003, Propositions 7.4.4 & 7.4.7)) Everypiecewise
affine mapping from one finite dimensional Euclidean space to another is strongly semis-
mooth everywhere. Moreover, the composition of strongly semismooth mappings is still
strongly semismooth.
3 A vertex exchange method
It is clear that C(ℓ,u) is not empty if and only if eTℓ ≤ b ≤ eTu. Moreover, if either eTℓ = b
or eTu= b holds, then we see that the feasible set of problem (1) is a singleton (i.e., {ℓ} or
{u}) and there is no need to solve the problem. Moreover, if there exists an index i ∈ [n]
such that ℓ = u , then we see that x = ℓ = u is fixed. In this case, we can drop this
i i i i i
variable and consider a smaller problem that takes the same form of problem (1). Hence,
without loss of generality, we make the following assumption for the rest of this paper.
Assumption 1 The constraints satisfy eTℓ < b < eTu and −∞ < ℓ < u < ∞.
Under Assumption 1, we can also see that the Slater’s condition (Slater, 2013) for
problem (1) holds, and for any feasible solution x, both sets {i ∈ [n] : x > ℓ } and
i i
{i ∈ [n] : x < u } are nonempty. For notational simplicity, we also denote F := {x ∈ Rn :
i i
eTx = b, ℓ ≤ x ≤u} as the feasible set of problem (1). Under the above conditions, we see
that F is nonempty, convex and compact. We shall mention here that when the feasible
set is given by aTx = b, ℓ ≤ x ≤ u where all entries of a ∈ Rn are nonzero, by a change of
decision variable x′ := Diag(a)x, we get a problem that is of the same form as (1).
In this section, we shall present our main algorithmic framework and analyze its con-
vergence. Since the proposed algorithm requires the computation of the projection onto
the feasible set F, we also develop a highly efficient method for this task, which leverages
tools developed in (Li et al., 2020). The convergence analysis for the latter method is also
provided to make the paper self-contained.
The proposed algorithm relies on the duality and optimality associated with problem
(1). To see this, we first derive the associated Lagrange dual problem of (1) as follows (see
e.g., (Liang et al., 2022)):
1
max by− wTQw−I∗ (−z) s.t. −Qw+ye+z = c. (3)
y∈R,w∈Rn,z∈Rn 2 C(ℓ,u)
5Liang, Toh and Yang
Consequently, the first-order optimality conditions (also known as the KKT conditions) for
problems (1) and (3) are given as:
eTx = b, −Qw+ye+z = c, Qx−Qw = 0, x ∈ C(ℓ,u), −z ∈ N (x). (4)
C(ℓ,u)
SincetheSlaterconditionholdsforproblem(1)underAssumption1,weseefrom(Bonnans and Shapiro,
2013, Theorem 2.165) that theoptimality conditions (4)admitat least onesolution. Hence,
by using (2) and the fact that Q is nonsingular, we see that the KKT conditions provided
in (4) can be reformulated as
eTx = b, x ∈ C(ℓ,u), −Qx+ye+z = c,
(5)
z ≥ 0, ∀ i ∈ J (x), z ≤ 0, ∀i ∈ J (x), z = 0, ∀ i∈/ J (x)∪J (x).
i ℓ i u i ℓ u
Here, J (x) and J (x) denote the index sets:
ℓ u
J (x) := {i ∈ [n] : x = ℓ }, J (x) := {i ∈ [n] : x = u }, ∀x ∈ Rn. (6)
ℓ i i u i i
Using the reformulated optimality conditions (5) and being motivated by (B¨ohning,
1986), we are able to provide an equivalent characterization of these conditions. Moreover,
whenthenewconditionisnotsatisfied, itthenoffers adescentdirection fortheQPproblem
(1). These results are summarized in the following lemma, which plays the essential role in
developing and analyzing our main algorithm, as we shall see shortly.
Lemma 3 Suppose that Assumption 1 holds. Let x ∈ Rn be a feasible point for problem
(1) and g := ∇q(x) = c+Qx be the gradient of q at x. Select
s∈ argmax{g : x > ℓ }, t ∈ argmin{g : x < u }.
i i i i i i
Then, x is an optimal solution of problem (1) if and only if g ≤ g .
s t
Define a new point x+ ∈ Rn via:
x+ := x+η(e −e ), η ∈ [0,min{x −ℓ ,u −x }].
t s s s t t
Then, x+ is a feasible solution for problem (1), and if g > g , e −e is a descent direction
s t t s
of the function q.
Proof By the conditions that eTℓ < b < eTu and ℓ < u as stated in Assumption 1, the
feasible set of problem (1) is nonempty and the indices s and t are well-defined. First,
suppose that g ≤ g . Let us consider the following two cases. Case 1: {i : ℓ < x < u }=6
s t i i i
∅; and Case 2: {i : ℓ < x < u } = ∅.
i i i
For Case 1, we see that
g = max{g : ℓ < x ≤ u } ≥ max{g : ℓ < x < u }
s i i i i i i i i
≥ min{g : ℓ < x < u } ≥ min{g : ℓ ≤ x < u } = g .
i i i i i i i i t
This, together with the assumption that g ≤ g , implies that g = g and all inequalities in
s t s t
the above are in fact equalities. In particular, there exists a constant y ∈ R such that
g = y, ∀ i ∈ {i : ℓ ≤ x ≤ u } = [n].
i i i i
6Vertex Exchange Method for QPs
By setting z = 0 for i ∈ [n], we can construct a vector z ∈ Rn and a real number y ∈ R
i
such that the triple (x,y,z) satisfies the KKT condition (5), i.e., (x,y,z) is a KKT point.
Thus, x is the optimal solution for the primal problem (1) and (y,z) is an optimal solution
for the dual problem (3).
For Case 2, i.e., {i : ℓ < x < u }= ∅, we know that
i i i
g = max{g : x > ℓ } = max{g : i∈ J (x)},
s i i i i u
g = min{g : x < u } = min{g : i∈ J (x)},
t i i i i ℓ
which, together with the assumption that g ≤ g , implies that
s t
max{g : i ∈ J (x)} ≤ min{g : i ∈ J (x)}.
i u i ℓ
Let y ∈ [g ,g ] be an arbitrary real number, we see that for any i ∈ J (x), there exists
s t u
z ≤ 0 such that g =y+z , and for any i ∈ J (x), there exists z ≥ 0 such that g = y+z .
i i i ℓ i i i
Consequently, we have constructed a KKT point (x,y,z), and due to the same reason as in
Case1, x is theoptimal solution for theprimalproblem (1)and (y,z) is an optimal solution
for the dual problem (3).
Next, we assume that x is an optimal solution of problem (1). Then there exist y ∈ R
and z ∈ Rn satisfying the KKT condition (5). It holds that
g = max{y+z : i ∈/ J (x)} ≤ y, g = min{y+z : i ∈/ J (x)} ≥ y.
s i ℓ t i u
Hence, g ≤ g . This proves the first statement of the lemma.
s t
For the second part of the proof, we verify that x+ is a feasible solution of problem (1).
It is clear that
eTx+ = eTx+η−η = b,
and from the condition that η ∈ [0,min{x −ℓ ,u −x }], it holds that
s s t t
x+ = x ∈ [ℓ ,u ], ∀i ∈/ {s,t},
i i i i
x+ = x −η ∈ [x −(x −ℓ ),x ] ⊆ [ℓ ,u ],
s s s s s s s s
x+ = x +η ∈ [x ,x +u −x ] ⊆ [ℓ ,u ].
t t t t t t t t
Therefore, x+ is a feasible solution for problem (1).
Finally, to verify that e −e is a descent direction of the function q(x) := 1xTQx+cTx
t s 2
when g > g , we only need to verify that gT(e −e )< 0. This is straightforward since
s t t s
gT(e −e ) = g −g < 0.
t s t s
Therefore the proof is completed.
Now, Lemma 3 naturally leads to the main algorithm, as presented in the following
Algorithm 1.
We note that the termination condition gk ≤ gk holds only theoretically. In fact, this
sk tk
condition may never be met if the algorithm is executed on a system using floating point
arithmetic. Hence, in practice, one can use |gk − gk | < tol or a similar condition for
sk tk
7Liang, Toh and Yang
Algorithm 1 A Vertex Exchange Method for problem (1).
1: Input: Q ∈ Sn , c ∈ Rn, b ∈ R, ℓ,u ∈ Rn satisfying Assumption 1, and x˜0 ∈ Rn.
++
2: Compute x0 = Proj F(x˜0).
3: Compute g0 = ∇q(x0) = c+Qx0.
4: for k ≥ 0 do
5: Select sk ∈argmax g ik : xk i > ℓ i and tk ∈ argmin g ik : xk i < u i .
6: if gk ≤ gk then
sk tk (cid:8) (cid:9) (cid:8) (cid:9)
7: Output: xk.
8: end if
9: Set dk = e tk −e sk and η mk ax := min{xk sk −ℓ sk,u tk −xk tk}.
10: Set xk+1 = xk +ηkdk where η k is the optimal solution of
min q(xk +ηdk) s.t. 0≤ η ≤ ηk . (7)
η∈R max
11: Update gk+1 = gk +ηk(Q :,tk −Q :,sk).
12: end for
termination, where tol > 0 is a given small tolerance specified by the user. Additionally,
the algorithm requires only a single projection and one matrix-vector multiplication at
the beginning of the algorithm. Thus, when Q is large-scale and fully dense, the per-
iteration cost of the Algorithm 1 is extremely low, as long as one can access any column
of Q efficiently. For the computation of the step size ηk, we need to solve an optimization
problem in one variable subject to a box constraint. Excitingly, this optimization problem
admits an analytical solution, which is described in the next lemma.
Lemma 4 The optimal step size for problem (7) is given by:
gk −gk
ηk := min ηk , sk tk .
max Q +Q −Q −Q
( sk,sk tk,tk sk,tk tk,sk)
Moreover, ηk > 0 before the termination of Algorithm 1.
Proof Note that
1
q(xk +ηdk)= (xk +ηdk)TQ(xk +ηdk)+cT(xk +ηdk)
2
η2
= (dk)TQdk +η(cTdk +(xk)TQdk)+q(xk)
2
Q +Q −Q −Q
= sk,sk tk,tk sk,tk tk,sk η2+(gk −gk )η+q(xk).
2 tk sk
Since Q ∈ Sn and gk > gk before the termination of Algorithm 1, we see that Q +
++ sk tk sk,sk
Q −Q −Q > 0 and gk −gk < 0 (see Lemma 3). Hence, it is easy to check that
tk,tk sk,tk tk,sk tk sk
the optimal step-size is given by
gk −gk
ηk := min ηk , sk tk .
max Q +Q −Q −Q
( sk,sk tk,tk sk,tk tk,sk)
8Vertex Exchange Method for QPs
To show that ηk > 0, it suffices to verify that ηk is positive which is obvious by the
max
definitions of sk and tk. Therefore, the proof is completed.
After all the previous preparations, we can state and analyze the global convergence
of Algorithm 1. Here, we assume without loss of generality that Algorithm 1 generates
an infinite sequence, denoted as {xk} . Otherwise, there exists K such that gK ≤ gK .
k≥0 sK tK
Then, Lemma 3 implies that the output point xK is the optimal solution for problem (1).
Theorem 5 Let {xk} be an infinite sequence generated by Algorithm 1. Then the whole
sequence converges to the global optimal solution of problem (1).
Proof By Lemma 3, we see that dk is always a descent direction before termination.
Moreover, Lemma 4 implies that ηk > 0, for k ≥ 0. Then, it is easy to verify that {q(xk)}
is monotonically decreasing. Since the function q is strongly convex and the feasible set
of problem (1) is nonempty and compact, we see that problem (1) admits a unique global
optimalsolution,denotedasx∗,andinparticular{q(xk)}islowerboundedbyq(x∗). Bythe
monotoneconvergence theorem, weseethatthesequence{q(xk)}converges tosomeq˜> ∞.
Since the feasible set of problem (1) is compact, we see that {xk} is bounded and admits at
least one accumulation point. Let x˜ ∈ Rn be an accumulation point of {xk} corresponding
tothesub-sequence{xkj} j≥0. By thecontinuity ofq, weseethatq(x˜) = lim j→∞q(xkj) = q˜.
Fromthis, weconcludethatanyconvergent sub-sequenceof{xk}converges tox˜. Therefore,
the whole sequence {xk} converges to x˜.
Moreover, we see that
Q +Q −Q −Q
lim sk,sk tk,tk sk,tk tk,sk (ηk)2+(g −g )ηk = lim q(xk+1)−q(xk)= 0.
k→∞ 2
tk sk
k→∞
If lim ηk > 0, we see that
k→∞
Q +Q −Q −Q
lim sk,sk tk,tk sk,tk tk,sk ηk +(g −g ) = 0,
k→∞ 2
tk sk
which implies that
Q +Q −Q −Q
lim g −g = lim sk,sk tk,tk sk,tk tk,sk ηk.
k→∞
sk tk
k→∞ 2
By the definition of ηk, we see that
(Q +Q −Q −Q )ηk ≤ gk −gk .
sk,sk tk,tk sk,tk tk,sk sk tk
Hence,
lim (Q +Q −Q −Q )ηk
sk,sk tk,tk sk,tk tk,sk
k→∞
Q +Q −Q −Q
≤ lim gk −gk = lim sk,sk tk,tk sk,tk tk,sk ηk.
k→∞ sk tk k→∞ 2
However, since Q ∈Sn , it holds that Q +Q −Q −Q > 0 and is uniformly
++ sk,sk tk,tk sk,tk tk,sk
bounded away from zero. Hence, the above inequality implies that lim ηk ≤ 0, contra-
k→∞
dictingtheassumptionthatlim ηk > 0. Consequently,itmustholdthatlim ηk = 0.
k→∞ k→∞
9Liang, Toh and Yang
Suppose on the one hand that there exists an infinite sub-sequence {k } (with slightly
j
abuse of notation) such that ηkj = η mkj
ax
for all j ≥ 0. By taking a subsequence if necessary,
we may assume without loss of generality that ηkj = xkj −ℓ and there exists an index
skj skj
s ∈ [n] such that skj = s for all j ≥ 0 2. Then, by the updating rule of x in Line 10
of Algorithm 1, we see that
xkj
> ℓ and
xkj+1
= ℓ for all j ≥ 0. In order to allow
s s s s
skj+1 = s, there must exist an index k¯ j ∈ (k j,k j+1) such that tk¯ j = s so that xk s¯ j < u s
and x sk¯ j+1 = xk s¯ j +ηk¯ j > ℓ s. Let us denote g˜ := ∇q(x˜) = c+Qx˜. Clearly, it holds that
lim j→∞ gkj = lim j→∞ gk¯ j = g˜. Let δ˜ := min{x˜ i −ℓ i : x˜ i > ℓ i} > 0. Then, for j ≥ 0
sufficiently large, {i : xk ij > ℓ i+ δ 2˜ } = {i : x˜
i
> ℓ i}. By the definition of skj, it is easy to
verify that
δ˜
g˜ = lim
gkj
= lim
max{gkj
:
xkj
> ℓ }≥ lim
max{gkj
:
xkj
> ℓ + }
s j→∞ skj j→∞ i i i j→∞ i i i 2
= max{g˜ : x˜ > ℓ }.
i i i
Similarly, by the definition of
tk¯
j, it holds that g˜
s
≤ min{g˜
i
: x˜
i
< u i}. Thus, max{g˜
i
:
x˜ > ℓ } ≤ min{g˜ : x˜ < u } and by Lemma3, we see that x˜ is the uniqueoptimal solution
i i i i i
of problem (1), i.e., x˜ = x∗.
Suppose on the other hand that there exists an infinite sequence {k } (with slightly
j
abuse of notation) such that
gkj −gkj
ηkj = skj tkj , ∀j ≥0.
Q +Q −Q −Q
skj,skj tkj,tkj skj,tkj tkj,skj
By the facts that lim ηk = 0 and Q ∈ Sn , we see that Q +Q −Q −
k→∞ ++ skj,skj tkj,tkj skj,tkj
Q is uniformly bounded away from zero, and hence,
tkj,skj
lim
gkj −gkj
= 0.
j→∞
skj tkj
By taking subsequences if necessary, we may assume without loss of generality that there
exist indices s,t ∈ [n] such that skj = s and tkj = t for all j ≥ 0. Therefore, it holds that
lim
gkj −gkj
= g˜ −g˜ = 0.
s t s t
j→∞
By the definition of skj and tkj, one can verify that
max{g˜ : x˜ > ℓ } ≤ g˜ = g˜ ≤ min{g˜ : x˜ < u },
i i i s t i i i
which together with Lemma 3 implies that x˜ = x∗, i.e., the unique optimal solution of
problem (1). Therefore, the whole sequence {xk} generated by Algorithm 1 converges to
the unique optimal solution of problem (1). Hence, the proof is completed.
2. Thecaseforηkj =u
tkj
−xk tkj j,forallj ≥0canbearguedinexactlythesameway. Hence,itisomitted
for simplicity.
10Vertex Exchange Method for QPs
Note that there are significant differences between our analysis and the one provided in
(B¨ohning, 1986) for D-optimal experimental design problem subjected to the standard sim-
plexconstraint. Themainchallengehereisthattheupperboundofthestepsizeisnolonger
1. Instead, ηk could also converge to zero. Hence, additional consideration is needed to
max
deal with this situation. Also note that the convergence rate of Algorithm 1 is currently not
available in this work. However, based on our empirical numerical experience, Algorithm 1
performs better than the away-step Frank-Wolfe method (Lacoste-Julien and Jaggi, 2015)
when the constraint set of problem (1) is the standard simplex; see Section 5.3 for more
details. This suggests that a linear convergence rate may be established for Algorithm 1,
at least for the standard simplex constraint. We shall leave it as a future research topic.
3.1 Fast projection onto F
Recall that the feasible set of problem (1) is denoted as
F := {x ∈ Rn : eTx = b, ℓ ≤ x ≤u},
where eTℓ < b < eTu and −∞ < ℓ < u < ∞ under Assumption 1. Our goal in this
subsection is to propose an efficient algorithm for computing the projection onto the set F
to high precision (e.g., machine precision), which is needed at the beginningof Algorithm 1.
Mathematically speaking, we aim at solving the following special form of a strongly convex
QP problem:
1
min kx−x¯k2 s.t. x ∈ F, (8)
x∈Rn 2 2
where x¯ ∈ Rn is the given point to be projected. It is clear that problem (8) is a special
case of problem (1) with Q := I and c := −x¯, respectively. According to (3), the Lagrange
n
dual problem (reformulated as an equivalent minimization problem) of problem (8) reads
as follows:
1
min −by+ kwk2 +I∗ (−z) s.t. −w+ye+z = −x¯, (9)
y∈R,w∈Rn,z∈Rn 2 C(ℓ,u)
and the associated KKT conditions are given by
eTx = b, x ∈ C(ℓ,u), −w+ye+z = −x¯, x−w = 0, −z ∈N (x). (10)
C(ℓ,u)
Recall that the Slater condition implies that the above system of KKT conditions (10)
admits at least one solution (Bonnans and Shapiro, 2013, Theorem 2.165). Note also that
the conditions x ∈ C(ℓ,u) and −z ∈ N (x) are equivalent to the following nonsmooth
C(ℓ,u)
equation:
x−Proj (x−z) = 0,
C(ℓ,u)
which implies that for any KKT point (x,y,w,z) satisfying (10), it holds that
w = Proj (ye+x¯), z = Proj (ye+x¯)−(ye+x¯). (11)
C(ℓ,u) C(ℓ,u)
Substituting the above relations (11) into the dual problem (9), we derive an equivalent
optimization with only one decision variable:
1
min φ(y) := kProj (ye+x¯)k2−by+δ∗ (ye+x¯−Proj (ye+x¯)). (12)
y∈R 2 C(ℓ,u) C(ℓ,u) C(ℓ,u)
11Liang, Toh and Yang
It is clear that the objective function φ : R → R is continuously differentiable and convex,
thus solving the problem (12) is equivalent to solving the following nonsmooth equation:
φ′(y) := eTProj (ye+x¯)−b = 0, y ∈R. (13)
C(ℓ,u)
Here,thecontinuousdifferentiabilityandtheexpressionofthederivativeofφisbasedonthe
elegantpropertiesoftheMoreauenvelopeforaconvexfunction;seee.g.,(Rockafellar and Wets,
2009, Theorem 2.26) for more details. After obtaining an optimal solution y¯ of the above
problem (12), the optimal solution of problem (8) can be constructed using
Proj (x¯)= Proj (y¯e+x¯).
F C(ℓ,u)
The remaining task in this section is to describe a Newton-type method for solving the
nonsmooth equation (13) efficiently and accurately.
Though the derivative of φ is nonsmooth, one is able to show that it is strongly semis-
mooth, which facilitates our design of efficient second-order algorithms, as we shall see
shortly. Indeed,weseefromLemma2thatthepiecewiseaffinemappingProj isstrongly
C(ℓ,u)
semismooth on Rn. Hence, φ′ defined in (13), as the composition of strongly semismooth
functions, is also strongly semismooth. To derive a second-order approximation of the func-
tion φ, we then need to characterize the generalized Jacobian of φ′, i.e., ∂(φ′). Note that
∂(φ′) is also known as the generalized Hessian of φ in the literature (Hiriart-Urruty et al.,
1984), hence, it is usually denoted as ∂2φ. Using ∂2φ, one is able to establish a “second-
order Taylor expansion” of the continuously differentiable function φ with local Lipschitz
continuous derivatives, as stated in the following lemma.
Lemma 6 ((Hiriart-Urruty et al., 1984)) For any y and yˆ, it holds that
1
φ(yˆ)= φ(y)+φ′(y)(yˆ−y)+ v(yˆ−y)2,
2
where v ∈ ∂2φ(y′′), for some y′′ lying between y and yˆ.
However, an explicit description of the set ∂2φ(y) at a given y is generally not available.
Fortunately, (Hiriart-Urruty et al., 1984) shows that
∂2φ(y)∆y ⊆ ∂ˆ2φ(y)∆y, ∀∆y ∈ R,
where ∂ˆ2φ(y) is defined as
∂ˆ2φ(y) := eT∂Proj (ye+x¯)e ⊆ R , ∀y ∈R,
C(ℓ,u) +
and a diagonal matrix V ∈ Sn defined below is an element of ∂Proj (ye+x¯):
+ C(ℓ,u)
1, if ℓ ≤ y+x¯ ≤ u and i = j,
i i i
V := (14)
ij
0, otherwise.
(cid:26)
Consequently, we can replace v ∈ ∂2φ(y′′) with eTVe in the above lemma.
With the previous preparations, we can now present the semismooth Newton (SSN)
method (Qi and Sun, 1993; Li et al., 2020) for solving the nonsmooth equation (13) in
12Vertex Exchange Method for QPs
Algorithm 2 A semismooth Newton method for problem (12).
1: Input: x¯ ∈ Rn, b ∈ R, ℓ,u ∈ Rn satisfying Assumption 1, δ ∈ (0,1), µ ∈ (0,1/2) and
τ ,τ ∈(0,1).
1 2
2: Choose y0 ∈ R.
3: for k ≥ 0 do
4: if |φ′(y)| = 0 then
5: Output: xk.
6: end if
7: Choose Vk ∈ ∂Proj C(ℓ,u)(yke+x¯) as in (14).
8: Set vk := eTVke and εk := τ 1min{τ 2,|φ′(yk)|}.
9: Compute ∆yk := − 1 φ′(yk).
vk+εk
10: Set αk := δmk, where m k is the smallest nonnegative integer m for which
φ(yk +δm∆yk) ≤ φ(yk)+µδmφ′(yk)∆yk.
11: Set yk+1 = yk +αk∆yk.
12: end for
Algorithm 2. In particular, Algorithm 2 together with its convergence properties is a direct
application of the one proposed in (Li et al., 2020). However, to make the paper self-
contained, we have included an analysis tailored specifically to our special settings; see also
(Zhao et al., 2010; Liang et al., 2021).
The convergence properties of Algorithm 2 are summarized in the following theorem.
Here again, we assume without loss of generality that Algorithm 2 generates an infinite
sequence, denoted as {yk} . Otherwise, there exists K such that φ′(yK) = 0. Then, due
k≥0
to the convexity of the function φ, the output point yK must be an optimal solution for
problem (12).
Theorem 7 The Algorithm 2 is well-defined. Let {yk} be an infinite sequence generated by
Algorithm 2. Then the whole sequence {yk} is bounded and any accumulation point of the
sequence is an optimal solution to problem (12). Let y¯ be an arbitrary accumulation point
of {yk}, assume that {i ∈ [n] : ℓ < y¯+x¯ < u } =6 ∅. Then, the whole sequence {yk}
i i i
converges to y¯ and
|yk+1−y¯|= O(|yk −y¯|2), as k → ∞. (15)
Proof See Appendix A.
4 Application to constrained convex optimization
Minimizing a convex function f over a compact set, including the generalized simplex,
plays important roles in the field of optimization (Boyd and Vandenberghe, 2004). Specific
speaking, we aim to solve the following constrained convex optimization problem:
min f(x) s.t. eTx = b, ℓ ≤ x ≤ u, (16)
x∈Rn
13Liang, Toh and Yang
whose feasible region is the same as that of problem (1). One can easily check that a point
x∗ ∈ F solves the problem (16) if and only if G(x∗) = 0, where the mapping G : Rn → Rn
is called the natural mapping and is defined as
G(x) := x−Proj (x−∇f(x)), ∀x ∈ Rn. (17)
F
For the rest of this section, we consider two classes of objective functions f, namely the
class of SC1 functions and the class of self-concordant functions, whose formal definitions
will be given shortly. We present two inexact Newton-type methods, which are originated
from (Pang and Qi, 1995; Chen et al., 2010; Liu et al., 2022), for solving these problems.
At each iteration of a Newton method, one needs to solve a strongly convex quadratic
programming problem of the form in (1). Thus, Algorithm 1 and Algorithm 2 set up the
backbones of the presented inexact Newton-type methods.
4.1 Constrained convex SC1 minimization
Let O bean open subset of Rn. A function f :O → R is said to be LC1 if it is differentiable
everywhere on O and its gradient ∇f : O → Rn is locally Lipschitz. Moreover, f is said
to be SC1 if ∇f is semismooth everywhere on O; see Definition 1. In this subsection, we
consider the problem (16) with f being SC1 and O containing F.
There are many interesting problems that can be modeled as convex SC1 minimization
problems (Pang and Qi, 1995). In the same work, the authors proposed a globally con-
vergent SQP Newton method whose subproblems need to be solved exactly. Moreover, in
order to ensure the local fast convergence rate, a stringent BD-regularity assumption was
required. Later, a globally and superlineraly convergent inexact SQP Newton method was
proposed in (Chen et al., 2010), and the BD-regularity assumption was relaxed by to a
more realistic one. Furthermore, the subproblemswere only requiredto besolved inexactly.
The detailed description of the inexact SQP Newton method is presented in the following
Algorithm 3.
The convergence properties of Algorithm 3 are summarized as follows; see (Chen et al.,
2010) for a detailed analysis.
Theorem 8 Suppose that Algorithm 3 generates an infinite sequence {xk}. Then, it holds
that lim f(xk) = f(x∗) where x∗ ∈ F is an optimal solution of problem (16). More-
k→∞
over, suppose that x∗ is an accumulation point of the sequence {xk} and I − S(I −V)
n n
is nonsingular for any S ∈ ∂ Proj (x∗ −∇f(x∗)) and V ∈ ∂ ∇f(x∗). Then, the whole
B F B
sequence {xk} converges to x∗ superlinearly, i.e.,
kxk+1−xkk= o(kxk −x∗k).
Moreover, if ∇f is strongly semismooth, then the rate of convergence is quadratic, i.e.,
kxk+1−xkk = O(kxk −x∗k2).
4.2 Constrained self-concordant minimization
A function f : Rn → R ∪ {+∞} is said to be self-concordant (SC) with a parameter
M ≥ 0 if it is three-time continuously differentiable and the function φ(τ) := f(x +τv)
14Vertex Exchange Method for QPs
Algorithm 3 An inexact SQP Newton method for constrained SC1 minimization.
1: Input: x˜0 ∈ Rn, and parameters µ ∈ (0,1/2) and γ, ρ, δ, τ 1, τ 2 ∈ (0,1).
2: Set x0 = Proj F(x˜0), fpre = kG(x0)k.
3: for k ≥ 0 do
4: if kG(xk)k = 0 then
5: Output: xk.
6: end if
7: Compute εk := τ 1min{τ 2,kG(xk)k} and ρk := min{ρ,kG(xk)k}.
8: Select V k ∈ ∂ B∇f(xk) and apply Algorithm 1 (using x˜k as the initial point) for
computing an approximate solution x˜k+1 of the following QP:
1
min q (x) := (x−xk)T(V +εkI )(x−xk)+∇f(xk)T(x−xk) s.t. x ∈ F,
k k n
x∈Rn 2
in the sense that x˜k+1 ∈ F, q (x˜k+1)≤ 0, and
k
kx˜k+1−Proj (x˜k+1−∇q (x˜k+1))k ≤ ρkkG(xk)k.
F k
9: if kG(x˜k+1)k ≤ γfpre then
10: Set xk+1 = x˜k+1, fpre = kG(xk+1)k.
11: else
12: Set ∆xk := x˜k+1 −xk and xk+1 = xk +αk∆xk, where αk := δmk and m k is the
smallest nonnegative integer m for which
f(xk +δm∆xk) ≤ f(xk)+µδm∇f(xk)T∆xk.
13: if kG(xk+1)k ≤ γfpre then
14: Set fpre = kG(xk+1)k.
15: end if
16: end if
17: end for
satisfies |φ′′′(τ)| ≤ Mφ′′(τ)3 2 for any τ ∈ dom(φ), x ∈ dom(f), and v ∈ Rn. Particu-
larly, if M = 2, we say that f is standard self-concordant; see (Tran-Dinh et al., 2015;
Sun and Tran-Dinh, 2019; Liu et al., 2022) for a more detailed description of the class of
self-concordantfunctionsandmanyinterestingexamplesarisingfrompracticalapplications,
including optimal experimental design (Pukelsheim, 2006; Damla Ahipasaoglu et al., 2008;
Lu and Pong, 2013; Hendrych et al., 2023), quantum tomography (Gross et al., 2010), lo-
gisticregression(Tran-Dinh et al.,2022),portfoliooptimization(Ryu and Boyd,2014),and
optimal transport (Peyr´e et al., 2019).
The following notation will be useful in our later exposition. Let x ∈ dom(f) and
assumethatdom(f)doesnotcontain straightlines, itholdsthat∇2f(x)ispositivedefinite.
In this case, we denote the local norm associated with f together with its dual norm as
k·k
x
:= k·k
∇2f(x)
and k·k∗
x
:= k·k (∇2f(x))−1, respectively, for notational simplicity. The
15Liang, Toh and Yang
following univariate function h :R → R defined as follows is also useful:
+ +
τ(1−2τ +2τ2)
h(τ) := , τ ≥ 0.
(1−2τ)(1−τ)2−τ2
It is easy to see that h is monotonically increasing and hence its inverse function h−1(τ) is
well-define for τ > 0.
In this subsection, we consider solving the problem (16) with f being a self-concordant
function such that dom(f)∩F 6= ∅ via an inexact projected Newton method as proposed in
(Liu et al., 2022). It is worth noticing that dom(f) does not necessarily contain F, which is
one of the reasons why the method presented in Section 4.1 is not directly applicable to the
self-concordance setting. Another essential point we should make is that since f is assumed
to be self-concordant, which is a stronger condition than the SC1 property, an Armijo line
search as in Line 9 of Algorithm 3 is not necessary. Instead, an explicit step-size formula
can be obtained, which could be an advantage in practical implementation. The inexact
projected Newton method is presented in Algorithm 4.
Algorithm 4 An inexact projected Newton method for constrained self-concordant mini-
mization.
1: Input: x0 ∈ dom(f) ∩ F, parameters β ∈ (0,1/20), σ ∈ (0,1),C > 1 such that
1/(C(1−β))+β/((1−2β)(1−β)2) ≤ σ and 1/C +1/(1−2β) ≤ 2, C ∈ (0,1/2) and
1
δ ∈ (0,1).
2: Set λ−1 = β
σ
and ξ0 = min{β/C,C 1h−1(β)}.
3: for k ≥ 0 do
4: Compute an approximate solution x˜k+1 of the following QP by using Algorithm 1
with x˜k as the initial point:
1
min q (x):= (x−xk)T∇2f(xk)(x−xk)+∇f(xk)T(x−xk) s.t. x ∈ F,
k
x∈Rn 2
in the sense that
max ∇q (x˜k+1)T(x˜k+1−x)≤ ξk. (18)
k
x∈F
5: Set ∆xk = x˜k+1−xk and compute γk = k∆xkk xk.
6: if γk +ξk ≤ h−1(β) or λk−1 ≤ β then
7: Set λk = σλk−1, ξk+1 =σξk, and xk+1 = x˜k+1.
8: else
9: Set λk = λk−1, ξk+1 = ξk, and xk+1 = xk + δ((γk)2−(ξk)2) ∆xk.
(γk)3+(γk)2−(ξk)2γk
10: end if
11: if λk = 0 then
12: Output: xk+1.
13: end if
14: end for
The convergence properties of Algorithm 4 are summarized as follows; see (Liu et al.,
2022) for the detailed analysis.
16Vertex Exchange Method for QPs
Theorem 9 Let ω(τ):= τ −log(1+τ) and set
f(x0)−f(x∗)
K := ,
δω(h−1(β)(1−2C )/(1−C ))
1 1
where x∗ is an optimal solution of problem (16). Then, there exists at least one k ∈ [K]
such that γk +ξk ≤ h−1(β). Moreover, for k ≥ 0 sufficiently large, it holds that
kxk −x∗k x∗ = O(σk).
5 Numerical experiments
This section is dedicated to a set of comprehensive numerical studies to evaluate the prac-
tical performance of the proposed algorithms compared with existing state-of-the-art coun-
terparts.
Termination conditions. For the semismooth Newton (SSN) method proposed in
Algorithm 2, we terminate the algorithm if it computes a point y ∈ R such that |φ′(y)| ≤
grad tol, wheregrad tol> 0isagiven tolerance specifiedbytheuser. Inourexperiments
in Section 5.1, we always set grad tol= 10−12. The maximal number of iterations allowed
for the SSN method is set to be 50. For the main Algorithm 1, denoted as VEM, we offer
several options. Given a tolerance tol > 0, one could terminate the algorithm if one of the
following conditions is met at the k-th iteration:
• |gk −gk |/max{1,kQk }≤ tol.
sk tk F
• The relative optimality residue
kxk −Proj (xk −Qxk −c)k
F ≤tol.
1+kxkk
• err(xk) ≤ tol where err :Rn → R is any user-defined error function.
+
If the projection onto the feasible set F is time-consuming, then one is suggested to employ
the first termination condition. Otherwise, the second condition is more robust and com-
monlyusedintheliteratureasitdirectlymeasuretheviolation oftheoptimality conditions.
Readers are referred to (18) for one particular example that employs the third condition.
If not stated explicitly, we use the first termination condition as the default option with
tol = 10−12. Moreover, The maximal number of iterations allowed for the VEM method is
set to be 106.
Computational platform. Our numerical experiments are all conducted on Matlab
(R2023b) running on a machine with 16GB of RAMs and a 2.2GHz Quad-Core Intel Core
i7 processor.
5.1 Projection onto the generalized simplex
We first verify the practical performance of the SSN method, which is needed to provide a
feasible point for the main Algorithm 1. To this end, we first describe how to generate the
testing problems of various sizes. Then, we introduce the state-of-the-art baseline solvers
17Liang, Toh and Yang
that we would like to compare with. Finally, we present the numerical results and provide
detailed comparisons.
Testing instances. We generate a set of large-scale random projection problems onto
the feasible set F as follows (here, n ∈ N denotes the vector size) using Matlab:
rng("default"); % for reproducitivity
l = max(0, randn(n,1));
u = l + rand(n,1);
b = sum(l + u)/2;
x0 = rand(n,1); % the point to project
Here, x0 represents the point to be projected. In our experiments, we test n ∈ {106,2×
106,...,107}.
Baseline solvers. The baseline solvers we choose to compare Algorithm 2 with are the
commercial solver GUROBI(v11.0.1) 3 withan academic licenseandadualactive setmethod
thatexploitssparsityoftheproblem,namelyPPROJ(Hager and Zhang,2016)4. TheGUROBI
solver is currently one of the most efficient and robust solvers for solving general linear and
quadratic programming problems, which is able to provide highly accurate solutions 5. On
the other hand, PPROJ makes special effort in identifying the solution sparsity to achieve
superior practical performance, as indicated in (Hager and Zhang, 2016). The settings for
GUROBI and PPROJ are summarized in the following Table 1. Particularly, only the barrier
method in GUROBI (which offers the best performance among the supported algorithms in
GUROBI) is tested.
Solver Settings
Method = 2; BarConvTol = 1e-12;
GUROBI
FeasibilityTol = 1e-9; OptimalityTol = 1e-9;
PPROJ grad tol = 1e-12;
Table 1: Settings for the baseline solvers.
The computational results are presented in Table 2 in which we report the linear con-
straint violations and the total computational times (in seconds) for the three algorithms.
Note that all solvers are able to return a point that belongs to the set C(ℓ,u), based on our
observation. Hence, for simplicity, we refrain from reporting the box constraint violations.
Obviously, the SSN method outperforms both GUROBI and PPROJ in terms of both com-
putational time and linear constraint violations. In fact, our computational results show
that the SSN method is able to compute a projection point nearly exactly using much
less computational time. PPROJ, which exploits the solution sparsity, also shows promising
numerical performance and is able to generate solutions as accurate as those of GUROBI.
GUROBI, on the other hand, is less efficiently due to the need of solving a linear system at
each iteration, even though the linear system is highly sparse. We conclude that the SSN
method for computing the projection onto the generalized simplex is highly efficient and
3. Solverwebsite: https://www.gurobi.com/
4. Available at: https://people.clas.ufl.edu/hager/software/
5. Seehttps://plato.asu.edu/bench.html for related benchmark.
18Vertex Exchange Method for QPs
keTx −bk Time
Sizes PPROJ GUROBI SSN PPROJ GUROBI SSN
1E6 1E−8 3E−8 < eps 0.32 4.93 0.03
2E6 3E−8 2E−8 < eps 0.68 9.28 0.06
3E6 2E−7 6E−8 < eps 1.04 14.31 0.09
4E6 2E−8 1E−6 < eps 1.46 20.25 0.13
5E6 2E−7 2E−9 < eps 1.88 25.17 0.14
6E6 2E−7 2E−8 < eps 2.26 30.86 0.17
7E6 5E−7 3E−7 < eps 2.73 35.71 0.25
8E6 3E−7 3E−7 < eps 3.12 40.55 0.21
9E6 5E−8 1E−6 < eps 3.48 44.06 0.33
1E7 3E−7 5E−7 < eps 3.91 50.88 0.32
Table 2: Linear constraint violations. Here the eps means the machine epsilon. For our
computational platform, eps:= 2.2204×10−16.
accurate due to the fast convergence rate and can be used as a fundamental toolbox for
other algorithms for which the projection is needed.
5.2 QPs with generalized simplex constraints
We nextevaluate thenumericalperformanceoftheAlgorithm 1viasolvingasetofartificial
QP problems with known optimal solution and comparing it with current state-of-the-art
first-order methods. To this end, we firstdescribe how to generate the testing QP instances
and thenmention the baselinesolvers to becompared with. Finally, wepresentthe detailed
computational results with some discussions.
Testing instances. Given a matrix size n ∈ N, we generate a set of symmetric positive
definite matrices Q ∈ Sn with different condition numbers, denoted in short by cond. In
++
particular,letQ ∈ Rn×n bearandomsquarematrix,wefirstcomputetheQRfactorization
0
of Q to get an orthogonal matrix U ∈ Rn×n. Moreover, for a given cond, we generate a
0
random vector d ∈ Rn of integers from the closed interval [1,cond] whose minimum and
maximum entries are then scaled to 1 and cond, respectively. Then, the matrix Q is set
as Q := Q/kQk with Q := UDiag(d)UT, which is automatically positive definite with a
F
condition numbercond. Theoptimal solutionx¯ ∈ Rn is arandomvector sampleduniformly
from the ientereval [−1,1].eThen, we set b := eTx¯. Let ratio∈ (0,1) be given, we can define
two index sets J := {i ∈ [n] : x¯ ≤ −ratio} and J := {i ∈ [n] : x¯ ≥ ratio}. We now
ℓ i u i
construct the lower bound and upper bound vectors ℓ and u as follows
x¯ , i∈ J , x¯ , i∈ J ,
i ℓ i u
ℓ = u = ∀ i∈ [n].
i i
−1 otherwise, 1 otherwise,
(cid:26) (cid:26)
Given the generated Q, x¯, b, ℓ and u, we next show how to generate c ∈ Rn by utilizing
the first-order optimality conditions given in (5). We propose to set y¯as a random number
drawn from the standard normal distribution N(0,1). Furthermore, we construct a vector
19Liang, Toh and Yang
z¯∈Rn as follows:
rand(1), i ∈ J ,
ℓ
z¯ = 0, i ∈/ J ∪J , ∀ i ∈[n],
i ℓ u

−rand(1), i ∈ J ,
 ℓ
where rand denotes the random number generator in Matlab that samples a random

number from the uniform distribution on the interval [0,1]. Then, the vector c is con-
structed as c := −Qx¯ + y¯e + z¯. By construction, (x¯,y¯,z¯) becomes a solution for the
first-order optimality conditions (5). In our experiment, we choose n ∈ {104,2 × 104},
ratio∈ {0.2,0.4,0.6,0.8}, and cond∈ {102,104,106,108}. Consequently, the total number
of test instances is 2×4×4 = 32.
Baseline solvers. NotethatthematrixQ ∈ Sn inourexperimentisoflarge-scale and
++
fully dense. Hence, an algorithm that requires solving linear systems would be inefficient.
In view of this, we only compare the proposed algorithm with state-of-the-art first-order
methods that do not need to solve linear systems 6. These methods include the following:
• The classical projected gradient method (Goldstein, 1964) (denoted as PG) is a fun-
damental tool for many other advanced methods. As an representative example, the
accelerated projectedgradientmethod(Nesterov,1983;Beck and Teboulle,2009)(de-
noted as FISTA),which built uponPG,is a hugely popularmethod in the optimization
community in the past decades. In our experiment, we use the implementations of PG
and FISTA provided by (Beck and Guttmann-Beck, 2019) 7. The termination toler-
ance for PG and FISTA is set to be 10−8, and the time limit is set as 300 seconds. To
allow both methods to gain their best performance, we use the economical versions in
which function values are not computed. Moreover, the projection onto the feasible
set at each iteration is computed using Algorithm 2, and the exact Lipschitz constant
with respect to ∇q(·) needed for determining the step-size is given explicitly by the
generated vector d. Note that in general, the estimation of the Lipschitz constant
can also be time-consuming, since it involves computing the extreme eigenvalue of
Q. Moreover, one may also apply a certain backtracking scheme to estimate the Lips-
chitzconstant. However, thisrequirescomputingtheobjectivevalueateach iteration,
which could also be time-consuming, especially when Q is fully dense and large-scale.
• When dealing with polyhedral constraints, it is desirable to employ the active-set
strategy to greatly improve the efficiency and reduce the dimensionality of the prob-
lem. In particular, we consider the solver called PASA developed in the package
SuiteOPT (Hager and Zhang, 2023) 8, which implements a projected gradient-based
polyhedral active set method that is shown to have excellent practical numerical per-
formance. In our experiments, we adapt the default settings of PASA except that we
set grad tol = 10−12. Note that the projection onto the feasible set F in PASA is
computed by the integrated routine PPROJ.
6. WealsotriedOSQP(Stellato et al.,2020),anoperator-splittingmethod,whichpresentsasoneofthestate-
of-the-art first-order methods for solving general convex quadratic programming problems. Typically,
OSQP only requires one matrix factorization when setting up the solver. However, the time needed for
a single matrix factorization is already very large. For simplicity, we do not report the computational
results obtained from OSQP.
7. Available at: https://www.tau.ac.il/~becka/home.
8. Available at: https://people.clas.ufl.edu/hager/software/.
20Vertex Exchange Method for QPs
• Though computing a single projection onto the feasible set F can be done efficiently
by using Algorithm 2, projection-based algorithms, including FISTA and PASA, may
require numerous iterations to converge. In this case, the cost taken for comput-
ing the projection can be a limiting factor on the practical efficiency. In view of
this, we consider comparing with the standard projection-free Frank-Wolfe algorithm
(Frank and Wolfe, 1956), denoted as FW, which has gained tremendous popularity
lately due to its scalability and appealing convergence properties. The step-size for
FW at the k-th iteration is set as 1/(k+2), for k ≥ 0 9. We terminate the algorithm
when kxk+1−xkk ≤ 10−3 or the computational time exceeds a 300-second time limit.
For the choice of termination tolerance, we tried some smaller values, but the com-
putational time will increase dramatically without significantly improving the quality
of the solutions. Finally, we mention that the linear program to be solved at each
iteration of FW is computed efficiently by the simplex method implemented in Gurobi.
For all the solvers, we use the zeros vector in Rn as the common initial point. The
computational results for n = 104 and n = 2×104 are presented in Table 3 and Table 4,
respectively. In both tables, we report the relative error with respect to the known optimal
solution x¯, i.e., relerr := kx −x¯k/(1 + kx¯k), where x ∈ Rn is the output of a method.
Moreover, the total computational time (in seconds) taken by each solver is also recorded.
We observefrom thecomputational resultsthat PG,FISTA,PASAandVEMsharea similar
performance in terms of the accuracy of the returned solutions. This suggests that all the
four methods are robust for computing high quality solutions with respect to the number
of active constraints and the condition number of Q. On the other hand, FW shows poor
performance in terms of the quality of the solution due to the slow convergence speed.
This is further reflected by the recorded computational time. Indeed, FW consumes much
more computational time than those of the remaining methods and always reaches the time
limit of 300 seconds. We note here that the slow convergence of FW is mainly due to its
zigzagging phenomenon–a well-known phenomenon in the literature. We also observe that
by exploiting the problem structure via identifying the active-set in the projected gradient-
based methods, the efficiency can be greatly improved. This explains why PASA is more
efficient than PG and FISTA. For the comparison between PG and FISTA, we observe that
PG is slightly better than FISTA in terms of computational efficiency. This is mainly due to
the fact PG typically converges faster than FISTA at the early stage given the same initial
condition. In our numerical experiments, we observe that both methods only require a few
iterations to converge. For VEM, we see that it is highly efficient when ratio is small, say
ratio≤ 0.6, and is less efficient when ratio is larger (but still comparable to PASA). This
suggests that VEM is highly suitable for problems having optimal solutions with many active
constraints.
9. Recentworks(e.g.,(Lacoste-Julien and Jaggi,2015))haveshownthatwhenthefeasiblesetofastrongly
convexconstrained optimization istheconvexhullofaset ofgivenpoints,theAway-StepsFrank-Wolfe
algorithm admits a linear convergence rate which leads to promising numerical performance. However,
for the feasible set F considered in the present paper, it is challenging to find a set of points, say
A:={a1,...,a p}suchthat theconvexhullofAisF, i.e., conv{a1,...,a p}=F. Thisexplainswhy we
only consider the standard Frank-Wolfe algorithm.
21Liang, Toh and Yang
cond = 1E2 cond = 1E4
ratio PG FISTA PASA FW VEM PG FISTA PASA FW VEM
5E−10 3E−10 1E−11 2E−4 6E−12 4E−10 4E−10 3E−12 2E−4 3E−12
0.2
2.5 3.7 2.0 300.0 1.1 2.6 3.7 2.2 300.1 1.0
9E−10 4E−10 4E−11 4E−4 9E−11 9E−10 4E−10 3E−11 4E−4 1E−10
0.4
4.4 6.3 3.5 300.0 1.8 4.6 6.5 3.3 300.1 1.9
2E−9 5E−10 6E−11 5E−4 2E−10 2E−9 6E−10 8E−11 5E−4 2E−10
0.6
7.8 11.0 4.9 300.1 3.9 8.1 11.3 4.6 300.0 4.1
4E−9 1E−9 1E−10 9E−4 8E−10 5E−9 1E−9 2E−10 1E−3 9E−10
0.8
15.8 21.1 8.2 300.0 10.6 19.6 26.2 8.4 300.0 11.7
cond = 1E6 cond = 1E8
ratio PG FISTA PASA FW VEM PG FISTA PASA FW VEM
4E−10 4E−10 1E−11 2E−4 2E−11 4E−10 4E−10 2E−11 2E−4 7E−12
0.2
2.6 3.7 2.0 300.0 1.0 2.7 3.9 2.2 300.1 1.0
7E−10 5E−10 4E−11 4E−4 9E−11 9E−10 3E−10 3E−11 4E−4 1E−10
0.4
4.7 6.5 2.9 300.0 1.9 4.2 6.0 2.9 300.1 1.8
2E−9 5E−10 8E−11 5E−4 3E−10 2E−9 5E−10 4E−11 6E−4 3E−10
0.6
8.0 11.1 4.9 300.0 3.8 7.8 10.8 5.1 300.0 3.9
5E−9 1E−9 2E−10 1E−3 9E−10 5E−9 8E−10 2E−10 1E−3 9E−10
0.8
21.0 27.9 8.6 300.0 12.0 19.3 25.9 9.0 300.0 11.7
Table 3: Computational results for n = 104. For each ratio ∈ {0.2,0.4,0.6,0.8}, the first
row represents the relative error with respect to the true solution, and the second row
records the computational time taken by each solver.
5.3 D-optimal experimental design
In this subsection, we consider solving the D-optimal experimental design that is a method-
ology commonly usedinstatistics andengineeringtooptimize theefficiency ofexperimental
designs:
min f(x):= −logdet(ADiag(x)AT) s.t. eTx = 1, 0 ≤ x ≤ 1, (19)
x∈Rn
where A = a ... a ∈ Rp×n represents the features (characterized by vectors in Rp)
1 n
of n experiments. Note that the dual problem of (19) can be interpreted as the prob-
(cid:2) (cid:3)
lem of finding the ellipsoid with the minimum volume that covers a given set of points
{a ,...,a }, which is well known as the minimum-volume enclosing ellipsoid problem
1 n
(Damla Ahipasaoglu et al., 2008). Developing efficient algorithmic frameworks for solving
problem (19) and/or its dual problem has been actively researched for decades; see, e.g.,
(Silvey et al., 1978; Fedorov and Lee, 2000; Kumar and Yildirim, 2005; Pukelsheim, 2006;
Damla Ahipasaoglu et al.,2008;Lu and Pong,2013;Liu et al.,2022;Zhao and Freund,2023)
and references therein.
The gradient and the Hessian of f is given as (see, e.g., (Lu et al., 2018)):
∇f(x)= −diag AT(ADiag(x)AT)−1A ,
∇2f(x)= AT(AD(cid:0)iag(x)AT)−1A ◦ AT (cid:1)(ADiag(x)AT)−1A ,
(cid:0) (cid:1) (cid:0) (cid:1)
22Vertex Exchange Method for QPs
cond = 1E2 cond = 1E4
ratio PG FISTA PASA FW VEM PG FISTA PASA FW VEM
3E−10 3E−10 1E−13 7E−4 5E−11 3E−10 2E−10 4E−11 7E−4 6E−11
0.2
10.4 14.8 20.3 300.1 3.1 10.4 16.0 18.4 300.2 3.1
6E−10 3E−10 4E−11 1E−3 1E−10 6E−10 2E−10 4E−11 1E−3 2E−10
0.4
16.4 23.4 23.3 300.1 5.7 16.9 24.4 23.3 300.1 5.9
1E−9 5E−10 1E−10 2E−3 4E−10 1E−9 4E−10 9E−11 2E−3 4E−10
0.6
30.1 41.9 29.7 300.2 12.5 31.8 44.8 20.4 300.2 12.8
3E−9 5E−10 2E−10 3E−3 1E−9 3E−9 8E−10 2E−10 3E−3 1E−9
0.8
64.2 87.9 44.5 300.0 34.1 76.1 101.8 32.9 300.1 38.7
cond = 1E6 cond = 1E8
ratio PG FISTA PASA FW VEM PG FISTA PASA FW VEM
3E−10 3E−10 1E−13 7E−4 5E−11 3E−10 2E−10 4E−11 7E−4 6E−11
0.2
10.4 14.8 20.3 300.1 3.1 10.4 16.0 18.4 300.2 3.1
6E−10 3E−10 4E−11 1E−3 1E−10 6E−10 2E−10 4E−11 1E−3 2E−10
0.4
16.4 23.4 23.3 300.1 5.7 16.9 24.4 23.3 300.1 5.9
1E−9 5E−10 1E−10 2E−3 4E−10 1E−9 4E−10 9E−11 2E−3 4E−10
0.6
30.1 41.9 29.7 300.2 12.5 31.8 44.8 20.4 300.2 12.8
3E−9 5E−10 2E−10 3E−3 1E−9 3E−9 8E−10 2E−10 3E−3 1E−9
0.8
64.2 87.9 44.5 300.0 34.1 76.1 101.8 32.9 300.1 38.7
Table 4: Computational results for n = 2×104. For each ratio ∈ {0.2,0.4,0.6,0.8}, the
first row represents the relative errors with respect to the true solution, and the second row
records the computational time taken by each solver.
for any x ∈ dom(f). Here, “◦” denotes the Hadamard product. Moreover, it is clear
that the function f is self-concordant (Sun and Tran-Dinh, 2019). Clearly, evaluating the
gradient andtheHessian matrix of f(·)can betime-consuming, especially whenn andp are
large. Moreover, an algorithm that requires solving linear systems at each iteration is not
efficientforsolving(19). Indeed,(Liu et al.,2022)recentlydemonstratesthatbydeveloping
algorithmicframeworksthatreducethenumberofgradientandHessianevaluationswithout
requiringsolving linear systems is a highly efficient approach for solving (19). In particular,
(Liu et al.,2022)appliestheprojectedNewtonmethod(seeAlgorithm4)whosesubproblem
(astronglyconvexQPproblemwiththestandardsimplexconstraint)issolvedbytheAway-
Steps Frank-Wolfe (ASFW) method (Lacoste-Julien and Jaggi, 2015) that admits analytical
expressions for calculating the step sizes (Khachiyan, 1996) 10. ASFW is highly efficient for
solving strongly convex optimization problems with the standard simplex constraint, since
it has a global linear convergence guarantee.
In our experiment, we consider replacing the QP solver used in (Liu et al., 2022) with
the proposedVEM and compare its practical performancewith ASFW.Thestoppingcondition
for both ASFW and VEM is the same and specified by (18). The testing data set is generated
in exactly the same way as (Damla Ahipasaoglu et al., 2008; Liu et al., 2022), i.e., the
data points {a ,...,a } ⊆ Rp are generated using an independent multinomial Gaussian
1 n
10. Theimplementation of thealgorithm is available at: https://github.com/unc-optimization/FWPN.
23Liang, Toh and Yang
distribution. The feasible initial point for Algorithm 4 is chosen as x0 = 1e. Note that
n
p should be less than n in order to ensure that the problem (19) is well-defined. Table 5
presents the computational results with n ∈ {103,2×103,...,104} and p = 1 n. We note
10
that in practical applications, p ≪ n. In the table, we report the total computational time
(in seconds), denoted as TTime, taken by the Algorithm 4 with the QP subproblems being
solved by ASFW and VEM, respectively. Furthermore, we also report the computational time
taken forsolvingtheQPsubproblems(inseconds), denoted asQPTime,bythetwomethods.
Lastly, the terminating λk for each testing instance is also reported. In our experiment, we
terminate the Algorithm 4 when λk ≤ 10−3, as also used in (Liu et al., 2022).
QP Solver ASFW VEM
n TTime QPTime λk TTime QPTime λk
1E3 0.37 0.10 7.0E−4 0.29 0.07 5.6E−4
2E3 1.53 0.37 3.9E−4 0.72 0.11 3.1E−4
3E3 3.02 0.72 3.0E−4 1.56 0.19 9.5E−5
4E3 6.14 1.48 2.1E−4 3.02 0.44 1.0E−4
5E3 11.84 3.02 1.7E−4 5.47 0.65 1.1E−4
6E3 17.17 4.11 1.4E−4 8.57 0.91 1.2E−4
7E3 24.57 5.62 1.2E−4 12.45 1.20 1.3E−4
8E3 32.62 7.09 1.1E−4 17.03 1.55 9.7E−5
9E3 44.59 9.27 9.1E−5 21.76 1.89 7.6E−5
1E4 50.91 10.21 8.7E−5 28.29 2.26 9.0E−5
Table 5: Computational results on D-optimal experiment design.
We see from Table 5 that Algorithm 4 solves all the problems successfully to the desired
accuracy by using either ASFW or VEM. For the comparison of the computational time, we
see that VEM is much more efficient than ASFW based on the recorded QPTime. The efficiency
gained in solving the QP subproblems further reduces the total computational time for
solving the problem (19) to the desired accuracy, as indicated by the recorded TTime.
Careful readers may also observe that the reduction in the total computational time is
more than the reduction in computational time for solving the QP subproblems via VEM.
The reason for this scenario is that VEM tends to return sparser solutions, which further
reduces the cost in computing the gradient and the Hessian of the function f. However,
another bottleneck of applyingtheAlgorithm 4 for solving problem (19) is in theevaluation
of ∇f(·) and ∇2f(·). We leave it as a further research topic on how to efficiently implement
Algorithm 4 such that the cost for evaluating the gradient and Hessian of f is minimized.
Lastly,wewanttohighlightthatVEMisabletohandleproblemswiththegeneralizedsimplex
constraint dueto thesamereason as mentioned in Section 5.2, which is morefavorable than
ASFW in practice.
6 Conclusions
We have proposed a vertex exchange method for solving the strongly convex quadratic pro-
gramming (QP) problem with a generalized simplex constraint, which plays a fundamental
role in many real-world applications. We conducted rigorous theoretical analysis and es-
24Vertex Exchange Method for QPs
tablished the global convergence guarantee of the method. Since the method requires a
feasible starting point, we have also designed a highly efficient semismooth Newton method
for computing the projection onto the feasible set. Moreover, the convergence properties
of the semismooth Newton method has also been provided. To support our motivations
and contributions, we have demonstrated how the considered QP and the proposed method
can be applicable for solving some important classes of constrained optimization problems.
Numerically, we have conducted extensive numerical experiments to validate the promising
practicalperformanceoftheproposedmethodswhencomparedwithstate-of-the-artsolvers.
Finally,werecallthattheconvergencerateoftheproposedmethodremainsunknown,which
deserves further investigations.
Acknowledgments and Disclosure of Funding
Ling Liang and Haizhao Yang were partially supported by the US National Science Foun-
dation underawards DMS-2244988, DMS-2206333, and theOffice of Naval Research Award
N00014-23-1-2007.
25Liang, Toh and Yang
Appendix A. Proof of Theorem 7
Since vk +εk > 0, φ′(yk) 6= 0, and
1
φ′(yk)∆yk = − (φ′(yk))2 < 0, (20)
vk +εk
we see that ∆yk is always a descent direction of φ for all k ≥ 0, and hence, the Algorithm
2 is well-defined. Since the constraint matrix for problem (8) eT has full row rank, ℓ < u,
and eTℓ < b < eTu, we see from (Rockafellar, 1974, Theorem 17’ & Theorem 18’) that the
level set {y ∈ R : φ(y) ≤ φ(y0)} is a closed and bounded convex set. Thus, the fact that
φ(yk) ≤ φ(y0) for all k ≥ 0 implies that the whole sequence {yk} is bounded, and hence
it admits at least one accumulation point. For the rest of the proof, we denote y¯ as an
arbitrary accumulation point of {yk}, i.e., there exists an infinite sequence {k } such
j j≥0
that lim j→∞ykj = y¯.
We next prove that φ(y¯) = 0 by contradiction. To this end, we assume on the contrary
that φ(y¯) 6= 0. By the continuity of φ′ and the boundedness of the sequence {yk}, we see
that there exists positive constants γ and γ such that γ ≤ |φ′(ykj)|≤ γ , and there exists
1 2 1 2
a positive constant ε¯ such that εkj ≥ ε¯, for j ≥ 0 sufficiently large. Since vkj ≤ n and
εkj ≤ τ τ ≤ 1, we see form (20) that
1 2
1 1
φ′(ykj)∆ykj = − (φ′(ykj))2 ≤ − γ2 < 0, (21)
vkj +εkj n+1 1
and
1 1 1
|∆ykj| = |φ′(ykj)| ∈ γ , γ ,
vkj +εkj n+1 1 ε¯ 2
(cid:20) (cid:21)
for j ≥ 0 sufficiently large. Hence, by taking a subsequence if necessary, we may assume
that lim j→∞∆ykj = ∆y¯, for some ∆y¯∈ R. As a result, (21) implies that
φ′(y¯)∆y¯< 0. (22)
Moreover, it holds from (21) that
limsup φ′(ykj)∆ykj < 0. (23)
j→∞
By the continuity of the function φ and the boundedness of the sequence {ykj}, we see that
lim φ(ykj+1)−φ(ykj)= 0.
j→∞
Now since the sequence {φ(yk)} is non-increasing, we see that
µδmkjφ′(ykj)∆ykj ≥ φ(ykj+1)−φ(ykj) ≥ φ(ykj+1)−φ(ykj),
which implies that
liminf
δmkjφ′(ykj)∆ykj
≥ 0. (24)
j→∞
26Vertex Exchange Method for QPs
Consequently, (23) and (24) imply that limsup
δmkj
≤ 0. Hence, it holds that
j→∞
lim
δmkj
= 0. (25)
j→∞
Then, for the line search scheme described in Line 10 of Algorithm 2, we see that
φ(ykj +δmkj−1 ∆ykj)−φ(ykj)> µδmkj−1 φ′(ykj)∆ykj. (26)
By dividing
δmkj−1
on the both side of (26) and using (25), we derive that
φ(ykj +δmkj−1 ∆ykj)−φ(ykj)
φ′(y¯)∆y¯= lim ≥ µφ′(y¯)∆y¯,
j→∞
δmkj−1
which together with the fact that µ ∈ (0,1/2) shows that φ′(y¯)∆y¯≥ 0, hence contradicting
(22). Therefore, φ′(y¯) = 0, and by the convexity of φ, y¯must be an optimal solution to the
problem (12).
Next, we assume that {i ∈ [n] : ℓ < y¯+ x¯ < u } =6 ∅ and prove (15). The latter
i i i
condition indicates thatthereexistsanopenneighborhoodofy¯, denotedasN, suchthatfor
any y ∈ N, the set ∂2φ(y) is uniformly bounded and |φ′(y)| <τ , and for j ≥ 0 sufficiently
2
large, vkj ≥ 1. By the strong semismoothness of φ′, we see that, for j ≥ 0 sufficiently large,
φ′(ykj)−φ′(y¯)−vkj(ykj −y¯)= O(|ykj −y¯|2).
Notice that for j ≥ 0 sufficiently large, εkj = τ |φ(ykj)|, and hence, it holds that
1
|ykj +∆ykj −y¯|
= |ykj −y¯−(vkj +εkj)−1φ′(ykj)|
= (vkj +εkj)−1|φ′(ykj)−φ′(y¯)−(vkj +εkj)(ykj −y¯)|
≤ O(|φ′(ykj)−φ′(y¯)−(vkj +εkj)(ykj −y¯)|) (27)
≤ O(|φ′(ykj)−φ′(y¯)−vkj(ykj −y¯)|)+O(|φ′(ykj)||ykj −y¯|)
≤ O(|ykj −y¯|2)+o(|ykj −y¯|)
= o(|ykj −y¯|).
By Lemma 6, we have that
1
φ(ykj)= φ(y¯)+ vkj (ykj −y¯)2,
2 (1)
(28)
1
φ(ykj +∆ykj)= φ(y¯)+ vkj (ykj +∆ykj −y¯)2,
2 (2)
where vkj ∈ ∂2φ(ykj ) and vkj ∈ ∂2φ(ykj ) for some ykj lying between ykj and y¯, and ykj
(1) (1) (2) (2) (1) (2)
lying between ykj +∆ykj and y¯, respectively. Note from (Facchinei, 1995, Lemma 3.1) that
(28) implies that
|∆ykj|
|ykj +∆ykj −y¯| = o(|∆ykj|), lim = 1, (29)
j→∞|ykj −y¯|
27Liang, Toh and Yang
for j ≥ 0 sufficiently large. Therefore, for j ≥ 0 sufficiently large,
ykj ,ykj
∈ N, and hence,
(1) (2)
vkj ,vkj are uniformly bounded. Then, (27), (28), (29) and the fact that φ′ is strongly
(1) (2)
semismooth yield that
1
φ(ykj +∆ykj)−φ(ykj)− φ′(ykj)∆ykj
2
1 1
= − vkj (ykj −y¯)2− φ′(ykj)∆ykj +o(|∆ykj|2)
2 (1) 2
1 1
= − vkj (ykj +∆ykj −y¯)(ykj −y¯)− (φ′(ykj)−vkj (ykj −y¯))∆ykj +o(|∆ykj|2)
2 (1) 2 (1)
1
= − (φ′(ykj)−vkj (ykj −y¯))∆ykj +o(|∆ykj|2)
2 (1)
1
= − (φ′(ykj)−φ′(y¯)−vkj (ykj −y¯))∆ykj +o(|∆ykj|2)
2 (1)
= O(|ykj −y¯|2)o(|∆ykj|)+o(|∆ykj|2)
= o(|∆ykj|2),
for j ≥ 0 sufficiently large. The above relation further shows that
φ(ykj +∆ykj)−φ(ykj)≤ µφ′(ykj)∆ykj,
for j ≥ 0 sufficiently large since µ ∈ (0,1/2). Hence, for j ≥ 0 sufficiently large, we can take
m = 0, i.e., α = 1, which indicates that ykj+1 = ykj+∆ykj, and |ykj+1−y¯| < 1|ykj−y¯|
kj kj 2
(due to (27)). The last inequality shows that ykj+1 ∈ N and lim j→∞ykj+1 = y¯. Repeating
the above process on {ykj+1}, we see that ykj+2 = ykj+1 + ∆ykj+1, for sufficiently large
j. By mathematical induction, it holds that the whole sequence {yk} converges to y¯, and
for k ≥ 0 sufficiently large, αk = 1, i.e., the unit step size is eventually achieved. Now, by
applying (29) to the whole sequence {yk}, similar to (27), we see that, for k ≥ 0 sufficiently
large,
|yk+1−y¯|≤ O(|φ′(yk)−φ′(y¯)−vk(yk −y¯)|)+O(|φ′(yk)||yk −y¯|) = O(|yk −y¯|2),
which proves (15). Therefore, the proof is completed.
References
P. H. Algoet and T. M. Cover. Asymptotic optimality and asymptotic equipartition prop-
erties of log-optimum investment. The Annals of Probability, pages 876–898, 1988.
G. Banjac, P. Goulart, B. Stellato, and S. Boyd. Infeasibility detection in the alternating
direction method of multipliers for convex optimization. Journal of Optimization Theory
and Applications, 183:490–519, 2019.
A.Beck andN.Guttmann-Beck. FOM–a matlabtoolbox of first-ordermethodsforsolving
convex optimization problems. Optimization Methods and Software, 34(1):172–193, 2019.
A.BeckandM.Teboulle. Afastiterativeshrinkage-thresholdingalgorithmforlinearinverse
problems. SIAM journal on imaging sciences, 2(1):183–202, 2009.
28Vertex Exchange Method for QPs
A. Ben-Tal, T. Margalit, and A. Nemirovski. The ordered subsets mirror descent optimiza-
tion method with applications to tomography. SIAM Journal on Optimization, 12(1):
79–108, 2001.
D.B¨ohning. Avertex-exchange-method inD-optimaldesigntheory. Metrika,33(1):337–347,
1986.
J. F. Bonnans and A. Shapiro. Perturbation analysis of optimization problems. Springer
Science & Business Media, 2013.
S. P. Boyd and L. Vandenberghe. Convex optimization. Cambridge university press, 2004.
Y. Chen, Y. Gao, and Y.-J. Liu. An inexact SQP Newton method for convex SC1 mini-
mization problems. Journal of optimization theory and applications, 146:33–49, 2010.
F. H. Clarke. Optimization and nonsmooth analysis. SIAM, 1990.
G. Cornuejols and R. Tu¨tu¨ncu¨. Optimization methods in finance, volume 5. Cambridge
University Press, 2006.
S. Damla Ahipasaoglu, P. Sun, and M. J. Todd. Linear convergence of a modified Frank–
Wolfealgorithmforcomputingminimum-volumeenclosingellipsoids. Optimisation Meth-
ods and Software, 23(1):5–19, 2008.
G.B.Dantzig. Linearprogrammingandextensions. InLinear programming and extensions.
Princeton university press, 2016.
F. Facchinei. Minimization of SC1 functions and the Maratos effect. Operations Research
Letters, 17(3):131–137, 1995.
F. Facchinei and J.-S. Pang. Finite-dimensional variational inequalities and complementar-
ity problems. Springer, 2003.
V. Fedorov and J. Lee. Design of experiments in statistics. In Handbook of Semidefinite
Programming: Theory, Algorithms, and Applications, pages 511–532. Springer, 2000.
M. Frank and P. Wolfe. An algorithm for quadratic programming. Naval research logistics
quarterly, 3(1-2):95–110, 1956.
A. A. Goldstein. Convex programming in hilbert space. 1964.
D. Gross, Y.-K. Liu, S. T. Flammia, S. Becker, and J. Eisert. Quantum state tomography
via compressed sensing. Physical review letters, 105(15):150401, 2010.
W. W. Hager and H. Zhang. Projection onto a polyhedron that exploits sparsity. SIAM
Journal on Optimization, 26(3):1773–1798, 2016.
W. W. Hager and H. Zhang. Algorithm 1035: a gradient-based implementation of the
polyhedral active set algorithm. ACM Transactions on Mathematical Software, 49(2):
1–13, 2023.
29Liang, Toh and Yang
D.Hendrych,M.Besanc¸on, andS.Pokutta. Solvingtheoptimalexperimentdesignproblem
with mixed-integer convex methods. arXiv preprint arXiv:2312.11200, 2023.
B. Hermans, A. Themelis, and P. Patrinos. QPALM: a proximal augmented Lagrangian
method for nonconvex quadratic programs. Mathematical Programming Computation, 14
(3):497–541, 2022.
F. S. Hillier and G. J. Lieberman. Introduction to operations research. McGraw-Hill, 2015.
J.-B. Hiriart-Urruty, J.-J. Strodiot, and V. H. Nguyen. Generalized Hessian matrix and
second-order optimality conditions for problems with C1,1 data. Applied mathematics
and optimization, 11(1):43–56, 1984.
G. James, D. Witten, T. Hastie, R. Tibshirani, et al. An introduction to statistical learning,
volume 112. Springer, 2013.
L. G. Khachiyan. Rounding of polytopes in the real number model of computation. Math-
ematics of Operations Research, 21(2):307–320, 1996.
P. Kumar and E. A. Yildirim. Minimum-volume enclosing ellipsoids and core sets. Journal
of Optimization Theory and applications, 126(1):1–21, 2005.
S. Lacoste-Julien and M. Jaggi. On the global linear convergence of Frank-Wolfe optimiza-
tion variants. Advances in neural information processing systems, 28, 2015.
X. Li, D. Sun, and K.-C. Toh. A Schur complement based semi-proximal admm for convex
quadratic conic programming and extensions. Mathematical Programming, 155(1):333–
373, 2016.
X. Li, D. Sun, and K.-C. Toh. On the efficient computation of a generalized Jacobian of the
projector over the Birkhoff polytope. Mathematical Programming, 179(1):419–446, 2020.
L. Liang, D. Sun, and K.-C. Toh. An inexact augmented Lagrangian method for second-
order cone programming with applications. SIAM Journal on Optimization, 31(3):1748–
1773, 2021.
L. Liang, X. Li, D. Sun, and K.-C. Toh. QPPAL: a two-phase proximal augmented La-
grangian method for high-dimensional convex quadratic programming problems. ACM
Transactions on Mathematical Software (TOMS), 48(3):1–27, 2022.
D. Liu, V. Cevher, and Q. Tran-Dinh. A Newton Frank–Wolfe method for constrained
self-concordant minimization. Journal of Global Optimization, pages 1–27, 2022.
H.Lu,R.M.Freund,andY.Nesterov. Relatively smoothconvex optimization byfirst-order
methods, and applications. SIAM Journal on Optimization, 28(1):333–354, 2018.
Z. Lu and T. K. Pong. Computing optimal experimental designs via interior point method.
SIAM Journal on Matrix Analysis and Applications, 34(4):1556–1580, 2013.
R. Mifflin. Semismooth and semiconvex functions in constrained optimization. SIAM Jour-
nal on Control and Optimization, 15(6):959–972, 1977.
30Vertex Exchange Method for QPs
M. Mohri, A. Rostamizadeh, and A. Talwalkar. Foundations of machine learning. MIT
press, 2018.
Y. Nesterov. A method for unconstrained convex minimization problem with the rate of
convergence O(1/k2). In Dokl. Akad. Nauk. SSSR, volume 269, page 543, 1983.
Y. Nesterov and A. Nemirovskii. Interior-point polynomial algorithms in convex program-
ming. SIAM, 1994.
J. Nocedal and S. J. Wright. Numerical optimization. Springer, 1999.
B. O’donoghue, E. Chu, N. Parikh, and S. Boyd. Conic optimization via operator splitting
andhomogeneousself-dualembedding. Journal of Optimization Theory and Applications,
169:1042–1068, 2016.
J.-S. Pang and L. Qi. A globally convergent Newton method for convex SC1 minimization
problems. Journal of Optimization Theory and Applications, 85(3):633–648, 1995.
G. Peyr´e, M. Cuturi, et al. Computational optimal transport: With applications to data
science. Foundations and Trends® in Machine Learning, 11(5-6):355–607, 2019.
F. Pukelsheim. Optimal design of experiments. SIAM, 2006.
L. Qi. Convergence analysis of some algorithms for solving nonsmooth equations. Mathe-
matics of operations research, 18(1):227–244, 1993.
L. Qi and J. Sun. A nonsmooth version of Newton’s method. Mathematical programming,
58(1):353–367, 1993.
S. J. Qin and T. A. Badgwell. A survey of industrial model predictive control technology.
Control engineering practice, 11(7):733–764, 2003.
H. Rademacher. U¨ber partielle und totale differenzierbarkeit von funktionen mehrerer vari-
abeln und u¨ber die transformation der doppelintegrale. Mathematische Annalen, 79(4):
340–359, 1919.
R.Rockafellar. ConvexAnalysis. PrincetonLandmarksinMathematicsandPhysics.Prince-
ton University Press, 1997. ISBN 978-0-691-01586-6.
R. T. Rockafellar. Conjugate duality and optimization. SIAM, 1974.
R. T. Rockafellar and R. J.-B. Wets. Variational analysis, volume 317. Springer Science &
Business Media, 2009.
E.K.RyuandS.Boyd. Stochastic proximaliteration: anon-asymptoticimprovement upon
stochastic gradient descent. Author website, early draft, 2014.
L. A. Shepp and Y. Vardi. Maximum likelihood reconstruction for emission tomography.
IEEE transactions on medical imaging, 1(2):113–122, 1982.
S. D. Silvey, D. Titterington, and B. Torsney. An algorithm for optimal designs on a design
space. Communications in Statistics-Theory and Methods, 7(14):1379–1389, 1978.
31Liang, Toh and Yang
M. Slater. Lagrange multipliers revisited. In Traces and emergence of nonlinear program-
ming, pages 293–306. Springer, 2013.
B.Stellato, G.Banjac,P.Goulart,A.Bemporad,andS.Boyd. OSQP:Anoperatorsplitting
solver for quadratic programs. Mathematical Programming Computation, 12(4):637–672,
2020.
D. Sun and J. Sun. Semismooth matrix-valued functions. Mathematics of Operations
Research, 27(1):150–169, 2002.
T. Sun and Q. Tran-Dinh. Generalized self-concordant functions: a recipe for Newton-type
methods. Mathematical Programming, 178(1):145–213, 2019.
Q. Tran-Dinh, A. Kyrillidis, and V. Cevher. Composite self-concordant minimization. J.
Mach. Learn. Res., 16(1):371–416, 2015.
Q. Tran-Dinh, L. Liang, and K.-C. Toh. A new homotopy proximal variable-metric frame-
work for composite convex minimization. Mathematics of Operations Research, 47(1):
508–539, 2022.
P. Wolfe. The simplex method for quadratic programming. Econometrica: Journal of the
Econometric Society, pages 382–398, 1959.
S. J. Wright. Primal-dual interior-point methods. SIAM, 1997.
C. Yang, J. Fan, Z. Wu, and M. Udell. Efficient automl pipeline search with matrix and
tensor factorization. arXiv preprint arXiv:2006.04216, 2020.
R. Zhao and R. M. Freund. Analysis of the Frank–Wolfe method for convex composite op-
timization involving a logarithmically-homogeneous barrier. Mathematical programming,
199(1):123–163, 2023.
X.-Y. Zhao, D. Sun, and K.-C. Toh. A Newton-CG augmented Lagrangian method for
semidefinite programming. SIAM Journal on Optimization, 20(4):1737–1765, 2010.
32