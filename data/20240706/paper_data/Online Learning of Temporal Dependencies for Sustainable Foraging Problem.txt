Online Learning of Temporal Dependencies
for Sustainable Foraging Problem
John Payne Aishwaryaprajna Peter R. Lewis
Department of Computer Science Department of Computer Science Faculty of Business and IT,
University of Exeter University of Exeter Ontario Tech University
Exeter, UK Exeter, UK Oshawa, Canada
Email: jhp206@exeter.ac.uk Email: aishwaryaprajna@exeter.ac.uk Email: peter.lewis@ontariotechu.ca
Abstract—The sustainable foraging problem is a dynamic en- the resource has been overused or exhausted it is not pos-
vironment testbed for exploring the forms of agent cognition sible to go back and attempt to use it more sustainably.
in dealing with social dilemmas in a multi-agent setting. The Likewise, a possible solution to the sustainable foraging
agents need to resist the temptation of individual rewards problem [6] could allow the agents to learn a sustainable
through foraging and choose the collective long-term goal of strategyofresourcecollectioninaone-shotmannerwithina
sustainability. We investigate methods of online learning in single lifetime rather than with several episodes of resource
Neuro-Evolution and Deep Recurrent Q-Networks to enable depletion and agent deaths. This paper explores online
agents to attempt the problem one-shot as is often required learning methods to investigate if agents are able to resist
theindividualtemptationsofmaximisingresourcecollection
by wicked social problems. We further explore if learning
and achieve the long-term collective goal of sustainability.
temporal dependencies with Long Short-Term Memory may
Extensive research has been conducted on the social
be able to aid the agents in developing sustainable foraging
dilemmas present in multi-agent systems, where a social
strategies in the long term. It was found that the integration
dilemma arises when there is tension between individual
of Long Short-Term Memory assisted agents in developing
and collective goals. An example of this is the Tragedy of
sustainablestrategiesforasingleagent,howeverfailedtoassist
Commons where individual agents with open access to a
agentsinmanagingthesocialdilemmathatarisesinthemulti-
shared resource independently adopt selfish strategies con-
agent scenario.
trary to cooperative strategies that serve the common good
of the agents, leading to depletion of the common resource.
1. Introduction
However, much of this work on multi-agent social dilem-
mas involved simplified environments within the context of
Episodicreinforcementlearning[1]andneuro-evolution game theory and is also focused on agent-agent interactions
algorithms [2] have been widely studied for multi-agent relating to the social dilemma rather than the long-term
systems with cooperative task requirements. Multi-agent implications of the agent-environment interactions.
foraging problem is such a testbed [3] where the impact Episodiclearningisnotenoughforone-shotproblemsor
of agent-agent interactions in an episode can be used by thosewhereagentsneedtoadapttoadynamicenvironment
the agents to decide on an optimal policy to cooperate for or the actions of other agents. The Open Racing Car Simu-
task success in the following episodes. The agent tasks in lator (TORCS) is such a problem, where offline controllers
foraging problems often involve environment exploration as were found to be too predictable and unable to adapt to
well as identification, transport and sharing of resources varying environments [7]. An approach of neuro-evolution
which can be achieved through episodic learning. How- withaugmentingtopologies(NEAT)[8]tobothevolveafast
ever, considering a long-term goal like the sustainability of controller from scratch and optimise an existing controller
resources in the foraging problem using episodic learning for a new track using online learning was discussed.
requires agents to die again and again to possibly identify The sustainable foraging problem can be considered a
unsustainable actions at every episode [4]. Partially Observable Markov Decision Process (POMDP)
Climate change and sustainable resource management where agents only have partial observability of the environ-
are often considered wicked social problems, the solutions ment. There has been significant research in exploring the
towhichrequireone-shotoperationswithouttheopportunity inclusion of Long Short-Term Memory (LSTM) in a neural
to learn by trial and error [5]. This is because, unlike tame network to compensate for partial observability and capture
problems where a variety of solutions can be attempted long-term dependencies in sequences of observations [9]
without consequence, any attempted solution for a wicked [10]. DRQN networks with LSTM were shown to compen-
problem has irreversible consequences. For example in the sateformissingframesinanarcadegameenvironmentfrom
context of the real-world sustainability of a resource, once historical information, where agents were only given access
4202
luJ
1
]AM.sc[
1v10510.7042:viXrato a limited fraction of the frames. the imminent Tragedy of Commons. The collective non-
In this work, we first discuss the one-shot nature of the exploitativebehaviourhastobeensuredbeforetheresources
sustainable foraging problem. This leads us to explore the reach a point of no return. This one-shot nature of the
differences in agent actions based on episodic and online sustainable foraging problem makes it challenging for the
learning algorithms for this one-shot problem. We imple- agentstoreasonaboutthedepletinggradientoftheresources
ment an online neuro-evolution as well as an online Deep within the right time and act accordingly.
Recurrent Q-Network (DRQN) as the agent’s deliberative
architecture. We then augment both online neuro-evolution 3. Episodic vs. Online neuro-evolution
and DRQN with LSTM using sequences of observations to
explore whether agents are aware of the dynamic environ- Episodic neuro-evolution has been previously applied to
ment and can make long-term decisions. the sustainable foraging problem [4]. Episodic (or offline)
approaches require agents to experience several episodes of
interaction with the environment where the weights of the
2. A brief on Sustainable Foraging Problem
neural network are updated at the end of each episode. The
and its one-shot nature
weightupdatesareacceptedwhenthereisanincreaseinthe
cumulative reward in comparison to the original network
Thesustainableforagingproblemhasbeenproposedasa weights. The results from the previous study show that a
dynamic social environment testbed for exploring the forms deliberative agent architecture with offline neuro-evolution
of agent cognition needed to achieve sustainability [6]. The is not enough for the agents to ensure the sustainability of
agents need to collect resources from the environment to the resources. The n-player game arising from the dilemma
gaintheenergynecessaryforsurvival.Theprobleminvolves makes individual resource collection more tempting than
three environment types: forest, pasture, and desert char- the collective goal of survival through sustainability. This
acterised by the replenishment rate of the resources. The impliesthattheagentshavetodieseveraltimestolearnthe
rate of replenishment is directly proportional to the amount impact of their actions.
of available resources. Agents within the problem have a We implement a method of online neuro-evolution sim-
choiceoftwoactionsateverytimestep:greedyormoderate. ilar to [11], where the networks are updated at each time
Greedy agents continuously gather resources regardless of step rather than at the end of an episode. In general, neuro-
whether they are in immediate need to survive, whereas evolution can be used to evolve the weights or topology of
moderateagentsonlygatherresourceswhentheyareneeded a network or both, here we have only evolved the weights
for survival. Regardless of the foraging strategies adopted of the network. The network used by the agents consists of
by agents, agents will be able to survive indefinitely in the a two-layer fully connected neural network of three input
forest environment type but they are not be able to survive neurons corresponding to the state parameters: the resource
in the desert environment type. The pasture environment level in the environment, the number of alive agents, and
type can support agents indefinitely provided too many the number of resource collecting agents, as well as an
resourcesarenotremovedfromtheenvironmenttooquickly. additional input neuron for each choice of energy threshold
Agents will only be able to survive if they adopt moderate availabletotheagentcorrespondingtothenumberofagents
foraging as greedy strategies lead to the rapid depletion of thatchoseeachrespectivethresholdduringthelasttimestep.
the resources and a reduction in the replenishment rate to The second layer in the network is a hidden layer of three
the extent that it will no longer be able to support agents. hidden neurons, and the output layer consists of one neuron
Thechosenenvironmenttypeslieonatradeoffspectrum per choice of energy threshold available to the agents. The
based on the relationship between resource availability and energy threshold decides whether the action is greedy or
the consumption rate of the agents. A compromise solution moderate.
of maximising resource availability and minimising con- Each agent independently maintains a population of 30
sumption to achieve sustainability is only required in the neuralnetworks,wherethenetworkusedtodecideanaction
pastureenvironmenttype.Tocapturethistradeoffwhileex- fortheagentisbasedonasoftmaxdistributionofthefitness
ploringtheagent-environmentinteractions,thispaperisonly values of the networks. This means that a network with a
focussedonthepastureenvironmenttype.Itshouldbenoted higher fitness has a higher probability of being chosen. The
that transitions between the environment types may occur observation is then passed to the selected network to obtain
when the number of agents changes in the environment. the decided action for that time step. At the end of each
Death, reproduction or migration from other environment time step the fitness for the network used during the time
types can be possible causes for change in the number of step is updated with the reward received by the agent based
agents, but the latter two causes are not considered in this on the agent’s energy level. Tournament selection is used to
paper. obtaintwoparentnetworkswhichareusedtoproduceanew
It is assumed that the agents are successful in this prob- network via arithmetic crossover of the network weights,
lemiftheycanactandreasonbasedontheenvironmenttype this new network is mutated and then used to replace the
they are in and achieve sustainability. This means that, in network with the lowest fitness in the population.
the pasture environment type, the agents will be successful Theenvironmentisasoutlinedintheimplementationfor
iftheyallcantakemoderateactionscollectivelyandescape the pasture environment type in [4] with an initial resourceFigure 1. Mean simulation results across 100 independent runs showing Figure2.Meansimulationresultsforasingleonlineneuro-evolutionagent
baselinebehaviourofmoderateandgreedyagents withachoiceofgreedyormoderateactions,averagedover100independent
runsof1000timestepseach.
level of 500 per agent and a resource growth rate of 1.005.
Agents were given a choice of three moderate actions with
energythresholdsof30,50,and80respectivelyaswellasa
greedyactionrepresentedbyanenergythresholdof5000.It
istobenotedthattheenergythresholdof5000islargerthan
the maximum amount an agent can obtain during the entire
simulation run with - time steps. In all of the experiments,
the initial energy of an agent is chosen as 100. The agents
can consume 5 units of resource at every time step at most
and the cost of surviving each step is 2 units.
Simulationswereconductedtoobtainthebaselineswith
moderate and greedy agents that chose the same action for
everytimestep.Figure1showstheaveragebehaviourfrom
100independentrunsofasinglemoderateandgreedyagent
over 1000 time steps. The energy level threshold for the Figure3.Meansimulationresultsfor10onlineneuro-evolutionagentswith
achoiceofgreedyormoderateactions,averagedover100independentruns
moderate action is set to 50. The moderate agent waits
of500timestepseach.
untilitsenergylevelisbelowthethresholdbeforegathering
and then only gathers when its energy level is below the
threshold. This results in sustainable use of the resource dying and stays moderate for the rest of the episodes [4].
where the agent can balance its energy needs with the For the case of 10 agents with online NE, illustrated
replenishment rate of the environment. The greedy agent in figure 3, agents initially choose random actions but all
agent continuously gathers resources. This results in the quickly learn greedy strategies in an attempt to maximise
overuse of the resources characterised by a rapid decline their cumulative reward. This leads to rapid overuse of the
in the resource level until it is depleted. At this point, the resource resulting in its rapid depletion and the Tragedy of
agent is no longer able to obtain any resources from the Commons. Once the resource has been depleted, agents can
environment as there is no longer any replenishment of the no longer increase their energy level and attempt a mix of
resources, the agent’s energy level then inevitably declines actions until their reserves are expended and all agents die.
until the agent dies. This replicates the results obtained in the previous offline
ForthecaseofasingleagentusingonlineNE,illustrated neuro-evolution approach [4] whilst maintaining a one-shot
in figure 2, agents start with random actions and rapidly approach to the sustainable foraging problem. Whilst the
learn a greedy strategy resulting in the resource depletion online neuro-evolution agents were not able to find sustain-
and death of agents typical in the case of greedy agents, able strategies, we would not expect them to as they only
2% of agents ‘luckily’ survived for the full 1000 time steps have knowledge of the current time step and incentive to
by failing to learn the greedy strategy to maximise their maximise their immediate reward. However, these results
rewards.Interestingly,theagentlearnstotransitiontoacting showthatonlineneuro-evolutionagentscanlearnthegreedy
moderatelyconsequentlytheresourcesstarttoclimbup.But strategy we expect of them within a single lifetime, which
the agent fails to hold up the temptation of being greedy is a requirement of wicked social problems.
during the gradual resource buildup in most of the runs, as Simulations were performedwiththe implementationof
a result, the agent dies in most of the cases. It is interesting DRQNgiventhepartiallyobservablenatureofthisproblem.
to refer the case of a single agent with episodic NE, that Each agent has the same topology as in the online NE,
the agent becomes moderate after a few initial episodes of where the network weights were instead updated using Q-Figure4.MeansimulationresultsforasingleDRQNagentwithachoice Figure6.Meansimulationresultsforasingleonlineneuro-evolutionagent
ofgreedyormoderateactions,averagedover100independentrunsof1000 withachoiceofarangeofactionsfromspecifiedthresholds,averagedover
timestepseach. 100independentrunsof1000timestepseach.
Figure 5. Mean simulation results for 10 DRQN agents with a choice of Figure7.MeansimulationresultsforasingleDRQNagentwithachoiceof
greedyor moderateactions,averagedover 100independentruns of1000 arangeofactionsfromspecifiedthresholds,averagedover30independent
timestepseach. runsof1000timestepseach.
learning where the quality of the solution was dictated by Figure 6 illustrates the case of a single online neuro-
the agent’s reward, as in the online neuro-evolution case evolutionagentwithoutLSTMaveragedover1000indepen-
network update was applied at each time step. The results dent runs. As in the earlier case, we see that the addition of
forthisimplementationareshowninfigure4,wherewesee multiple thresholds for the agent to choose from reduces
that after starting the initial time step with random actions the speed at which the agents can determine the action
the DRQN agents migrate towards a greedy strategy. We that maximises their reward however the vast majority of
also see that the DRQN agents take longer to develop the agents still rapidly adopt greedy strategies leading to the
expectedstrategyandasaresult,a‘lucky’10%oftheagents severe depletion of the resource and ultimately the death of
survive all 1000 time steps by failing to learn a greedy agents. Once resources are depleted the number of greedy
strategy. Similar to the case of the online neuro-evolution agents drops sharply as agents try to find more moderate
agents, we see in figure 5 that for the case of 10 DRQN actions however as the resource is already depleted this
agents, all agents rapidly adopt greedy strategies leading to does not help and all greedy agents die leaving only the
the depletion of the resources and death of all agents. ‘lucky’ minority of agents that failed to maximise their
reward. Figure 7 shows the behaviour of the DRQN agent
4. Can LSTM provide insights into the nega-
forthesamescenario,herewecanseethattheDRQNagent
tive gradient of resources? takes much longer than the neuro-evolution agent to learn
a greedy strategy, potentially indicating it is less able to
To explore if learning temporal dependencies allows the handle the larger action space resulting from a range of
agents to make better decisions, we replace the first fully thresholds. DRQN agents in this case are unable to change
connected layer in the agent’s deliberative network with a their behaviour before the resources are depleted and all
single-layer LSTM with the same number of nodes. The agents that were initially able to learn a greedy strategy
addition of LSTM allowed agents to be passed a rolling then died.
sequence of the last 25 observations. WiththeadditionofanLSTMintheneuralnetwork,theFigure8.Meansimulationresultsforasingleonlineneuro-evolutionagent Figure 10. Mean simulation results for 10 simultaneous online neuro-
withLSTMandachoiceofarangeofactionsfromspecifiedthresholds, evolutionagentsandachoiceofarangeofactionsfromspecifiedthresh-
averagedover30independentrunsof1000timestepseach. olds,averagedover100independentrunsof1000timestepseach.
Figure 9. Mean simulation results for a single DRQN agent with LSTM Figure 11. Mean simulation results for 10 simultaneous online neuro-
andachoiceofarangeofactionsfromspecifiedthresholds,averagedover evolution agents with LSTM and a choice of a range of actions from
30independentrunsof1000timestepseach. specified thresholds, averaged over 100 independent runs of 3000 time
stepseach.
single agent demonstrates significantly different behaviour
as shown in figure 8. The agent initially prefers the greedy Whenonlineneuro-evolutionagentsareaugmentedwith
action however, the distribution of agent decisions is more LSTM, shown in figure 11, the distribution of agent deci-
mixed than the case without LSTM. When the resources in sions early in the simulation is more mixed as in the single
the environment start depleting rapidly, the agent learns to agentscenario.Astheresourcesdeplete,agentsreducetheir
reduce greedy actions for stabilising the resource gradient. useofgreedyactionswhichprolongstheirlifetimebutdoes
In a few runs, the agent continues to prioritise immediate nothaveasignificantenoughimpacttobringaboutsustain-
rewards and continues a greedy strategy resulting in death. ability.Onceresourcesaredepletedagentsbegintodie,and
Figure9showsthechangeinbehaviourwiththeaddition the number of alive agents reduces steadily until stabilising
ofLSTMtotheDRQNagent.Agentsarelesseagertoadopt at 2 on average. The single-agent case demonstrates that
very greedy strategies and, as in the case of online NE, the addition of LSTM enables agents to learn sustainable
adopt more mixed strategies to maximise reward without actions through observing of the impact their actions on the
compromising the environment. In a significant portion of environment over time, which is not the case for the multi-
runs,theagentstilladoptsstrategiesthataretoogreedyand agentscenario.Thisisbecausewhilsttheagentsmaybeable
ultimatelydieoff,however,agentsinmostoftherunsadopt tobalancelong-termandshort-termrewardsindividually,as
moderate strategies with the awareness of the depleting partofagrouptheyfallvictimtotheTragedyofCommons,
gradient of resources. inwhichagentsthatdefectfromthesustainablestrategycan
In the case of 10 online neuro-evolution agents that can obtain a greater reward than the cooperators.
choose from a range of energy thresholds, illustrated in Figure12showsasingleagentcomparisonofmeanalive
figure 10, agents take longer to decide on the greedy action agents and resource level across 100 independent runs for
howeverthisischosenbythemajorityofagentsearlyinthe all agent types given a choice of actions from a range of
simulation. Once resources are depleted, the agents attempt moderateandgreedyactions.Fromthis,wecanseethatthe
otherstrategiesbutdonotmakeanimpactonsustainability. addition of LSTM enables agents to find significantly moreto learn the expected strategies within one episode where
many smaller, more frequent network updates are used in
comparisontoepisodicneuro-evolution.Thiswascompared
to a DRQN implementation that showed DRQN agents are
also able to learn the expected strategy within a single
lifetime, however, online neuro-evolution agents were able
to learn the expected greedy behaviour both more often and
fasterthanDRQN.However,bothDRQNandonlineneuro-
evolution agents were not able to balance short-term and
long-term goals and learn a sustainable strategy for either
thesingleor10-agentscenario,indicatingthatwhilstonline
learning enables the problem to be attempted one-shot it
does not aid the agents in finding new strategies other than
those that maximise their immediate reward.
Figure 12. Comparison of agent type performance for the single agent
We also investigated the potential for LSTM to grant
scenario, where agents choose from a range of actions corresponding to
agentstemporalawarenessfromhistoricalinformation.Both
specified thresholds. The plot shows the mean resource level and mean
numberofaliveagentsateachtimestep,whereresultswereobtainedvia online neuro-evolution and DRQN agents, when augmented
100independentrunsof1000timestepseachforeachagenttype. with LSTM can make sustainable actions in a single-agent
scenario. However, the temporal knowledge from LSTM
is not enough to deal with the social dilemma present in
the n-player game of the sustainable foraging problem. The
LSTM implemented here explores temporal dependencies
from sequences of observations within the network itself.
Exploring temporal awareness through an explicit meta-
layer to enable agents to have reflective capabilities would
be an obvious next step.
References
[1] S.GronauerandK.Diepold,“Multi-agentdeepreinforcementlearn-
ing:asurvey,”ArtificialIntelligenceReview,vol.55,no.2,pp.895–
943,2022.
[2] E.Papavasileiou,J.Cornelis,andB.Jansen,“Asystematicliterature
Figure 13. Comparison of agent type performance for the single agent review of the successors of “neuroevolution of augmenting topolo-
scenario, where agents choose from a range of actions corresponding to gies”,”Evolutionarycomputation,vol.29,no.1,pp.1–73,2021.
specified thresholds. The plot shows the mean resource level and mean [3] O. Zedadra, N. Jouandeau, H. Seridi, and G. Fortino, “Multi-agent
numberofaliveagentsateachtimestep,whereresultswereobtainedvia foraging:state-of-the-artandresearchchallenges,”ComplexAdaptive
30independentrunsof3000timestepseachforeachagenttype. SystemsModeling,vol.5,pp.1–24,2017.
[4] Aishwaryaprajna and P. R. Lewis, “Exploring intervention in co-
evolving deliberative neuro-evolution with reflective governance for
sustainable strategies than their non-LSTM counterparts,
thesustainableforagingproblem,”inArtificialLifeConferencePro-
additionally whilst online neuro-evolution agents tend to be
ceedings35,vol.2023,no.1. MITPress,2023,p.140.
greedier early in the simulation they learn strategies and
[5] H. W. Rittel and M. M. Webber, “Dilemmas in a general theory of
recognise the impact of their actions on the environment planning,”Policysciences,vol.4,no.2,pp.155–169,1973.
sooner than the DRQN agents leading to a higher survival [6] Aishwaryaprajna and P. R. Lewis, “The sustainable foraging prob-
rate on average. lem,” in 2023 IEEE International Conference on Autonomic Com-
Figure13showsthecomparisonfor10agents.Noonline putingandSelf-OrganizingSystemsCompanion(ACSOS-C). IEEE,
2023,pp.98–103.
neuro-evolution agents survive as they quickly learn greedy
[7] L.Cardamone,D.Loiacono,andP.L.Lanzi,“On-lineneuroevolution
strategies to maximise their short-term reward. The survival
appliedtotheopenracingcarsimulator,”062009,pp.2622–2629.
rate of agents with LSTM is slightly higher than those
[8] S. Whiteson and P. Stone, “On-line evolutionary computation for
without, however, all types show agent count declining to
reinforcement learning in stochastic domains,” 07 2006, pp. 1577–
approximately 20% with none managing to overcome the 1584.
Tragedy of Commons. [9] M.HausknechtandP.Stone,“Deeprecurrentq-learningforpartially
observablemdps,”in2015AAAIFallSymposiumSeries,2015.
5. Conclusions [10] P. Zhu, X. Li, P. Poupart, and G. Miao, “On improving deep rein-
forcement learning for pomdps,” arXiv preprint arXiv:1704.07978,
2017.
In this work on the sustainable foraging problem, we
[11] L. Cardamone, D. Loiacono, and P. L. Lanzi, “Learning to drive in
first investigate online learning methods that allow agents
the open racing car simulator using online neuroevolution,” IEEE
to learn the impact of their actions within a single lifetime. TransactionsonComputationalIntelligenceandAIinGames,vol.2,
Animplementationofonlineneuro-evolutionenabledagents no.3,pp.176–190,2010.