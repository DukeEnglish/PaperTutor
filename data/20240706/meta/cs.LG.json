[
    {
        "title": "Planetarium: A Rigorous Benchmark for Translating Text to Structured Planning Languages",
        "authors": "Max ZuoFrancisco Piedrahita VelezXiaochen LiMichael L. LittmanStephen H. Bach",
        "links": "http://arxiv.org/abs/2407.03321v1",
        "entry_id": "http://arxiv.org/abs/2407.03321v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03321v1",
        "summary": "Many recent works have explored using language models for planning problems.\nOne line of research focuses on translating natural language descriptions of\nplanning tasks into structured planning languages, such as the planning domain\ndefinition language (PDDL). While this approach is promising, accurately\nmeasuring the quality of generated PDDL code continues to pose significant\nchallenges. First, generated PDDL code is typically evaluated using planning\nvalidators that check whether the problem can be solved with a planner. This\nmethod is insufficient because a language model might generate valid PDDL code\nthat does not align with the natural language description of the task. Second,\nexisting evaluation sets often have natural language descriptions of the\nplanning task that closely resemble the ground truth PDDL, reducing the\nchallenge of the task. To bridge this gap, we introduce \\benchmarkName, a\nbenchmark designed to evaluate language models' ability to generate PDDL code\nfrom natural language descriptions of planning tasks. We begin by creating a\nPDDL equivalence algorithm that rigorously evaluates the correctness of PDDL\ncode generated by language models by flexibly comparing it against a ground\ntruth PDDL. Then, we present a dataset of $132,037$ text-to-PDDL pairs across\n13 different tasks, with varying levels of difficulty. Finally, we evaluate\nseveral API-access and open-weight language models that reveal this task's\ncomplexity. For example, $87.6\\%$ of the PDDL problem descriptions generated by\nGPT-4o are syntactically parseable, $82.2\\%$ are valid, solve-able problems,\nbut only $35.1\\%$ are semantically correct, highlighting the need for a more\nrigorous benchmark for this problem.",
        "updated": "2024-07-03 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03321v1"
    },
    {
        "title": "Value-Penalized Auxiliary Control from Examples for Learning without Rewards or Demonstrations",
        "authors": "Trevor AblettBryan ChanJayce Haoran WangJonathan Kelly",
        "links": "http://arxiv.org/abs/2407.03311v1",
        "entry_id": "http://arxiv.org/abs/2407.03311v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03311v1",
        "summary": "Learning from examples of success is an appealing approach to reinforcement\nlearning that eliminates many of the disadvantages of using hand-crafted reward\nfunctions or full expert-demonstration trajectories, both of which can be\ndifficult to acquire, biased, or suboptimal. However, learning from examples\nalone dramatically increases the exploration challenge, especially for complex\ntasks. This work introduces value-penalized auxiliary control from examples\n(VPACE); we significantly improve exploration in example-based control by\nadding scheduled auxiliary control and examples of auxiliary tasks.\nFurthermore, we identify a value-calibration problem, where policy value\nestimates can exceed their theoretical limits based on successful data. We\nresolve this problem, which is exacerbated by learning auxiliary tasks, through\nthe addition of an above-success-level value penalty. Across three simulated\nand one real robotic manipulation environment, and 21 different main tasks, we\nshow that our approach substantially improves learning efficiency. Videos,\ncode, and datasets are available at https://papers.starslab.ca/vpace.",
        "updated": "2024-07-03 17:54:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03311v1"
    },
    {
        "title": "Universal Length Generalization with Turing Programs",
        "authors": "Kaiying HouDavid BrandfonbrenerSham KakadeSamy JelassiEran Malach",
        "links": "http://arxiv.org/abs/2407.03310v1",
        "entry_id": "http://arxiv.org/abs/2407.03310v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03310v1",
        "summary": "Length generalization refers to the ability to extrapolate from short\ntraining sequences to long test sequences and is a challenge for current large\nlanguage models. While prior work has proposed some architecture or data format\nchanges to achieve length generalization, these proposals typically apply to a\nlimited set of tasks. Building on prior scratchpad and Chain-of-Thought (CoT)\ntechniques, we propose Turing Programs, a novel CoT strategy that decomposes an\nalgorithmic task into steps mimicking the computation of a Turing Machine. This\nframework is both universal, as it can accommodate any algorithmic task, and\nsimple, requiring only copying text from the context with small modifications.\nWe show that by using Turing Programs, we obtain robust length generalization\non a range of algorithmic tasks: addition, multiplication and in-context SGD.\nWe then demonstrate that transformers achieve length generalization on random\nTuring Programs, suggesting that length generalization is possible for any\nalgorithmic task. Finally, we theoretically prove that transformers can\nimplement Turing Programs, constructing a simple RASP (Weiss et al.) program\nthat simulates an arbitrary Turing machine.",
        "updated": "2024-07-03 17:53:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03310v1"
    },
    {
        "title": "DisCo-Diff: Enhancing Continuous Diffusion Models with Discrete Latents",
        "authors": "Yilun XuGabriele CorsoTommi JaakkolaArash VahdatKarsten Kreis",
        "links": "http://arxiv.org/abs/2407.03300v1",
        "entry_id": "http://arxiv.org/abs/2407.03300v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03300v1",
        "summary": "Diffusion models (DMs) have revolutionized generative learning. They utilize\na diffusion process to encode data into a simple Gaussian distribution.\nHowever, encoding a complex, potentially multimodal data distribution into a\nsingle continuous Gaussian distribution arguably represents an unnecessarily\nchallenging learning problem. We propose Discrete-Continuous Latent Variable\nDiffusion Models (DisCo-Diff) to simplify this task by introducing\ncomplementary discrete latent variables. We augment DMs with learnable discrete\nlatents, inferred with an encoder, and train DM and encoder end-to-end.\nDisCo-Diff does not rely on pre-trained networks, making the framework\nuniversally applicable. The discrete latents significantly simplify learning\nthe DM's complex noise-to-data mapping by reducing the curvature of the DM's\ngenerative ODE. An additional autoregressive transformer models the\ndistribution of the discrete latents, a simple step because DisCo-Diff requires\nonly few discrete variables with small codebooks. We validate DisCo-Diff on toy\ndata, several image synthesis tasks as well as molecular docking, and find that\nintroducing discrete latents consistently improves model performance. For\nexample, DisCo-Diff achieves state-of-the-art FID scores on class-conditioned\nImageNet-64/128 datasets with ODE sampler.",
        "updated": "2024-07-03 17:42:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03300v1"
    },
    {
        "title": "Vertex Exchange Method for a Class of Quadratic Programming Problems",
        "authors": "Ling LiangKim-Chuan TohHaizhao Yang",
        "links": "http://arxiv.org/abs/2407.03294v1",
        "entry_id": "http://arxiv.org/abs/2407.03294v1",
        "pdf_url": "http://arxiv.org/pdf/2407.03294v1",
        "summary": "A vertex exchange method is proposed for solving the strongly convex\nquadratic program subject to the generalized simplex constraint. We conduct\nrigorous convergence analysis for the proposed algorithm and demonstrate its\nessential roles in solving some important classes of constrained convex\noptimization. To get a feasible initial point to execute the algorithm, we also\npresent and analyze a highly efficient semismooth Newton method for computing\nthe projection onto the generalized simplex. The excellent practical\nperformance of the proposed algorithms is demonstrated by a set of extensive\nnumerical experiments. Our theoretical and numerical results further motivate\nthe potential applications of the considered model and the proposed algorithms.",
        "updated": "2024-07-03 17:28:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.03294v1"
    }
]