[
    {
        "title": "MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding",
        "authors": "Fei WangXingyu FuJames Y. HuangZekun LiQin LiuXiaogeng LiuMingyu Derek MaNan XuWenxuan ZhouKai ZhangTianyi Lorena YanWenjie Jacky MoHsiang-Hui LiuPan LuChunyuan LiChaowei XiaoKai-Wei ChangDan RothSheng ZhangHoifung PoonMuhao Chen",
        "links": "http://arxiv.org/abs/2406.09411v1",
        "entry_id": "http://arxiv.org/abs/2406.09411v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09411v1",
        "summary": "We introduce MuirBench, a comprehensive benchmark that focuses on robust\nmulti-image understanding capabilities of multimodal LLMs. MuirBench consists\nof 12 diverse multi-image tasks (e.g., scene understanding, ordering) that\ninvolve 10 categories of multi-image relations (e.g., multiview, temporal\nrelations). Comprising 11,264 images and 2,600 multiple-choice questions,\nMuirBench is created in a pairwise manner, where each standard instance is\npaired with an unanswerable variant that has minimal semantic differences, in\norder for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our\nresults reveal that even the best-performing models like GPT-4o and Gemini Pro\nfind it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy.\nOpen-source multimodal LLMs trained on single images can hardly generalize to\nmulti-image questions, hovering below 33.3% in accuracy. These results\nhighlight the importance of MuirBench in encouraging the community to develop\nmultimodal LLMs that can look beyond a single image, suggesting potential\npathways for future improvements.",
        "updated": "2024-06-13 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09411v1"
    },
    {
        "title": "Visual Sketchpad: Sketching as a Visual Chain of Thought for Multimodal Language Models",
        "authors": "Yushi HuWeijia ShiXingyu FuDan RothMari OstendorfLuke ZettlemoyerNoah A SmithRanjay Krishna",
        "links": "http://arxiv.org/abs/2406.09403v1",
        "entry_id": "http://arxiv.org/abs/2406.09403v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09403v1",
        "summary": "Humans draw to facilitate reasoning: we draw auxiliary lines when solving\ngeometry problems; we mark and circle when reasoning on maps; we use sketches\nto amplify our ideas and relieve our limited-capacity working memory. However,\nsuch actions are missing in current multimodal language models (LMs). Current\nchain-of-thought and tool-use paradigms only use text as intermediate reasoning\nsteps. In this work, we introduce Sketchpad, a framework that gives multimodal\nLMs a visual sketchpad and tools to draw on the sketchpad. The LM conducts\nplanning and reasoning according to the visual artifacts it has drawn.\nDifferent from prior work, which uses text-to-image models to enable LMs to\ndraw, Sketchpad enables LMs to draw with lines, boxes, marks, etc., which is\ncloser to human sketching and better facilitates reasoning. Sketchpad can also\nuse specialist vision models during the sketching process (e.g., draw bounding\nboxes with object detection models, draw masks with segmentation models), to\nfurther enhance visual perception and reasoning. We experiment with a wide\nrange of math tasks (including geometry, functions, graphs, and chess) and\ncomplex visual reasoning tasks. Sketchpad substantially improves performance on\nall tasks over strong base models with no sketching, yielding an average gain\nof 12.7% on math tasks, and 8.6% on vision tasks. GPT-4o with Sketchpad sets a\nnew state of the art on all tasks, including V*Bench (80.3%), BLINK spatial\nreasoning (83.9%), and visual correspondence (80.8%). All codes and data are in\nhttps://visualsketchpad.github.io/.",
        "updated": "2024-06-13 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09403v1"
    },
    {
        "title": "Improving Autoregressive Training with Dynamic Oracles",
        "authors": "Jianing YangHarshine VisvanathanYilin WangXinyi HuMatthew Gormley",
        "links": "http://arxiv.org/abs/2406.09393v1",
        "entry_id": "http://arxiv.org/abs/2406.09393v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09393v1",
        "summary": "Many tasks within NLP can be framed as sequential decision problems, ranging\nfrom sequence tagging to text generation. However, for many tasks, the standard\ntraining methods, including maximum likelihood (teacher forcing) and scheduled\nsampling, suffer from exposure bias and a mismatch between metrics employed\nduring training and inference. DAgger provides a solution to mitigate these\nproblems, yet it requires a metric-specific dynamic oracle algorithm, which\ndoes not exist for many common metrics like span-based F1, ROUGE, and BLEU. In\nthis paper, we develop these novel dynamic oracles and show they maintain\nDAgger's no-regret guarantee for decomposable metrics like span-based F1. We\nevaluate the algorithm's performance on named entity recognition (NER), text\nsummarization, and machine translation (MT). While DAgger with dynamic oracle\nyields less favorable results in our MT experiments, it outperforms the\nbaseline techniques in NER and text summarization.",
        "updated": "2024-06-13 17:59:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09393v1"
    },
    {
        "title": "DiscreteSLU: A Large Language Model with Self-Supervised Discrete Speech Units for Spoken Language Understanding",
        "authors": "Suwon ShonKwangyoun KimYi-Te HsuPrashant SridharShinji WatanabeKaren Livescu",
        "links": "http://arxiv.org/abs/2406.09345v1",
        "entry_id": "http://arxiv.org/abs/2406.09345v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09345v1",
        "summary": "The integration of pre-trained text-based large language models (LLM) with\nspeech input has enabled instruction-following capabilities for diverse speech\ntasks. This integration requires the use of a speech encoder, a speech adapter,\nand an LLM, trained on diverse tasks. We propose the use of discrete speech\nunits (DSU), rather than continuous-valued speech encoder outputs, that are\nconverted to the LLM token embedding space using the speech adapter. We\ngenerate DSU using a self-supervised speech encoder followed by k-means\nclustering. The proposed model shows robust performance on speech inputs from\nseen/unseen domains and instruction-following capability in spoken question\nanswering. We also explore various types of DSU extracted from different layers\nof the self-supervised speech encoder, as well as Mel frequency Cepstral\nCoefficients (MFCC). Our findings suggest that the ASR task and datasets are\nnot crucial in instruction-tuning for spoken question answering tasks.",
        "updated": "2024-06-13 17:28:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09345v1"
    },
    {
        "title": "ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models",
        "authors": "David AnugrahaGenta Indra WinataChenyue LiPatrick Amadeus IrawanEn-Shiun Annie Lee",
        "links": "http://arxiv.org/abs/2406.09334v1",
        "entry_id": "http://arxiv.org/abs/2406.09334v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09334v1",
        "summary": "Performance prediction is a method to estimate the performance of\nmultilingual language models (LMs), mitigating computational costs associated\nwith model capacity and data for fine-tuning. Our paper introduces ProxyLM, a\nscalable framework for predicting LM performance using proxy models in\nmultilingual tasks. These proxy models act as surrogates, approximating the\nperformance of fine-tuned LMs on specific downstream natural language\nprocessing (NLP) tasks. By leveraging proxy models, ProxyLM significantly\nreduces computational overhead on task evaluations, achieving up to a 37.08x\nspeedup compared to traditional methods, even with our smallest proxy models.\nAdditionally, our methodology showcases adaptability to previously unseen\nlanguages in pre-trained LMs, outperforming the state-of-the-art performance by\n1.89x as measured by root-mean-square-error (RMSE). This framework streamlines\nmodel selection, enabling efficient deployment and iterative LM enhancements\nwithout extensive computational resources.",
        "updated": "2024-06-13 17:15:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09334v1"
    }
]