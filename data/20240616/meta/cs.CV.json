[
    {
        "title": "VideoGPT+: Integrating Image and Video Encoders for Enhanced Video Understanding",
        "authors": "Muhammad MaazHanoona RasheedSalman KhanFahad Khan",
        "links": "http://arxiv.org/abs/2406.09418v1",
        "entry_id": "http://arxiv.org/abs/2406.09418v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09418v1",
        "summary": "Building on the advances of language models, Large Multimodal Models (LMMs)\nhave contributed significant improvements in video understanding. While the\ncurrent video LMMs utilize advanced Large Language Models (LLMs), they rely on\neither image or video encoders to process visual inputs, each of which has its\nown limitations. Image encoders excel at capturing rich spatial details from\nframe sequences but lack explicit temporal context, which can be important in\nvideos with intricate action sequences. On the other hand, video encoders\nprovide temporal context but are often limited by computational constraints\nthat lead to processing only sparse frames at lower resolutions, resulting in\nreduced contextual and spatial understanding. To this end, we introduce\nVideoGPT+, which combines the complementary benefits of the image encoder (for\ndetailed spatial understanding) and the video encoder (for global temporal\ncontext modeling). The model processes videos by dividing them into smaller\nsegments and applies an adaptive pooling strategy on features extracted by both\nimage and video encoders. Our architecture showcases improved performance\nacross multiple video benchmarks, including VCGBench, MVBench and Zero-shot\nquestion-answering. Further, we develop 112K video-instruction set using a\nnovel semi-automatic annotation pipeline which further improves the model\nperformance. Additionally, to comprehensively evaluate video LMMs, we present\nVCGBench-Diverse, covering 18 broad video categories such as lifestyle, sports,\nscience, gaming, and surveillance videos. This benchmark with 4,354\nquestion-answer pairs evaluates the generalization of existing LMMs on dense\nvideo captioning, spatial and temporal understanding, and complex reasoning,\nensuring comprehensive assessment across diverse video types and dynamics.\nCode: https://github.com/mbzuai-oryx/VideoGPT-plus.",
        "updated": "2024-06-13 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09418v1"
    },
    {
        "title": "An Image is Worth More Than 16x16 Patches: Exploring Transformers on Individual Pixels",
        "authors": "Duy-Kien NguyenMahmoud AssranUnnat JainMartin R. OswaldCees G. M. SnoekXinlei Chen",
        "links": "http://arxiv.org/abs/2406.09415v1",
        "entry_id": "http://arxiv.org/abs/2406.09415v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09415v1",
        "summary": "This work does not introduce a new method. Instead, we present an interesting\nfinding that questions the necessity of the inductive bias -- locality in\nmodern computer vision architectures. Concretely, we find that vanilla\nTransformers can operate by directly treating each individual pixel as a token\nand achieve highly performant results. This is substantially different from the\npopular design in Vision Transformer, which maintains the inductive bias from\nConvNets towards local neighborhoods (e.g. by treating each 16x16 patch as a\ntoken). We mainly showcase the effectiveness of pixels-as-tokens across three\nwell-studied tasks in computer vision: supervised learning for object\nclassification, self-supervised learning via masked autoencoding, and image\ngeneration with diffusion models. Although directly operating on individual\npixels is less computationally practical, we believe the community must be\naware of this surprising piece of knowledge when devising the next generation\nof neural architectures for computer vision.",
        "updated": "2024-06-13 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09415v1"
    },
    {
        "title": "Alleviating Distortion in Image Generation via Multi-Resolution Diffusion Models",
        "authors": "Qihao LiuZhanpeng ZengJu HeQihang YuXiaohui ShenLiang-Chieh Chen",
        "links": "http://arxiv.org/abs/2406.09416v1",
        "entry_id": "http://arxiv.org/abs/2406.09416v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09416v1",
        "summary": "This paper presents innovative enhancements to diffusion models by\nintegrating a novel multi-resolution network and time-dependent layer\nnormalization. Diffusion models have gained prominence for their effectiveness\nin high-fidelity image generation. While conventional approaches rely on\nconvolutional U-Net architectures, recent Transformer-based designs have\ndemonstrated superior performance and scalability. However, Transformer\narchitectures, which tokenize input data (via \"patchification\"), face a\ntrade-off between visual fidelity and computational complexity due to the\nquadratic nature of self-attention operations concerning token length. While\nlarger patch sizes enable attention computation efficiency, they struggle to\ncapture fine-grained visual details, leading to image distortions. To address\nthis challenge, we propose augmenting the Diffusion model with the\nMulti-Resolution network (DiMR), a framework that refines features across\nmultiple resolutions, progressively enhancing detail from low to high\nresolution. Additionally, we introduce Time-Dependent Layer Normalization\n(TD-LN), a parameter-efficient approach that incorporates time-dependent\nparameters into layer normalization to inject time information and achieve\nsuperior performance. Our method's efficacy is demonstrated on the\nclass-conditional ImageNet generation benchmark, where DiMR-XL variants\noutperform prior diffusion models, setting new state-of-the-art FID scores of\n1.70 on ImageNet 256 x 256 and 2.89 on ImageNet 512 x 512. Project page:\nhttps://qihao067.github.io/projects/DiMR",
        "updated": "2024-06-13 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09416v1"
    },
    {
        "title": "Rethinking Score Distillation as a Bridge Between Image Distributions",
        "authors": "David McAllisterSongwei GeJia-Bin HuangDavid W. JacobsAlexei A. EfrosAleksander HolynskiAngjoo Kanazawa",
        "links": "http://arxiv.org/abs/2406.09417v1",
        "entry_id": "http://arxiv.org/abs/2406.09417v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09417v1",
        "summary": "Score distillation sampling (SDS) has proven to be an important tool,\nenabling the use of large-scale diffusion priors for tasks operating in\ndata-poor domains. Unfortunately, SDS has a number of characteristic artifacts\nthat limit its usefulness in general-purpose applications. In this paper, we\nmake progress toward understanding the behavior of SDS and its variants by\nviewing them as solving an optimal-cost transport path from a source\ndistribution to a target distribution. Under this new interpretation, these\nmethods seek to transport corrupted images (source) to the natural image\ndistribution (target). We argue that current methods' characteristic artifacts\nare caused by (1) linear approximation of the optimal path and (2) poor\nestimates of the source distribution. We show that calibrating the text\nconditioning of the source distribution can produce high-quality generation and\ntranslation results with little extra overhead. Our method can be easily\napplied across many domains, matching or beating the performance of specialized\nmethods. We demonstrate its utility in text-to-2D, text-based NeRF\noptimization, translating paintings to real images, optical illusion\ngeneration, and 3D sketch-to-real. We compare our method to existing approaches\nfor score distillation sampling and show that it can produce high-frequency\ndetails with realistic colors.",
        "updated": "2024-06-13 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09417v1"
    },
    {
        "title": "Interpreting the Weight Space of Customized Diffusion Models",
        "authors": "Amil DravidYossi GandelsmanKuan-Chieh WangRameen AbdalGordon WetzsteinAlexei A. EfrosKfir Aberman",
        "links": "http://arxiv.org/abs/2406.09413v1",
        "entry_id": "http://arxiv.org/abs/2406.09413v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09413v1",
        "summary": "We investigate the space of weights spanned by a large collection of\ncustomized diffusion models. We populate this space by creating a dataset of\nover 60,000 models, each of which is a base model fine-tuned to insert a\ndifferent person's visual identity. We model the underlying manifold of these\nweights as a subspace, which we term weights2weights. We demonstrate three\nimmediate applications of this space -- sampling, editing, and inversion.\nFirst, as each point in the space corresponds to an identity, sampling a set of\nweights from it results in a model encoding a novel identity. Next, we find\nlinear directions in this space corresponding to semantic edits of the identity\n(e.g., adding a beard). These edits persist in appearance across generated\nsamples. Finally, we show that inverting a single image into this space\nreconstructs a realistic identity, even if the input image is out of\ndistribution (e.g., a painting). Our results indicate that the weight space of\nfine-tuned diffusion models behaves as an interpretable latent space of\nidentities.",
        "updated": "2024-06-13 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09413v1"
    }
]