[
    {
        "title": "Explore the Limits of Omni-modal Pretraining at Scale",
        "authors": "Yiyuan ZhangHandong LiJing LiuXiangyu Yue",
        "links": "http://arxiv.org/abs/2406.09412v1",
        "entry_id": "http://arxiv.org/abs/2406.09412v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09412v1",
        "summary": "We propose to build omni-modal intelligence, which is capable of\nunderstanding any modality and learning universal representations. In specific,\nwe propose a scalable pretraining paradigm, named Multimodal Context (MiCo),\nwhich can scale up the numbers of modalities and amount of data, together with\nthe model parameters, in the pretraining process. With MiCo, the pretrained\nmodels show significant emergent abilities in multimodal learning, which are\nevaluated on the following tasks: i) single-modality perception benchmarks of\n10 different modalities, ii) 25 cross-modality understanding tasks of\nretrieval, question-answering, captioning, and iii) 18 multimodal large\nlanguage model benchmarks. Our models establish 37 new records for\nstate-of-the-art performance. We hope that our research could contribute to the\ndevelopment of omni-modal intelligence. Code and Models are at\nhttps://github.com/invictus717/MiCo",
        "updated": "2024-06-13 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09412v1"
    },
    {
        "title": "MuirBench: A Comprehensive Benchmark for Robust Multi-image Understanding",
        "authors": "Fei WangXingyu FuJames Y. HuangZekun LiQin LiuXiaogeng LiuMingyu Derek MaNan XuWenxuan ZhouKai ZhangTianyi Lorena YanWenjie Jacky MoHsiang-Hui LiuPan LuChunyuan LiChaowei XiaoKai-Wei ChangDan RothSheng ZhangHoifung PoonMuhao Chen",
        "links": "http://arxiv.org/abs/2406.09411v1",
        "entry_id": "http://arxiv.org/abs/2406.09411v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09411v1",
        "summary": "We introduce MuirBench, a comprehensive benchmark that focuses on robust\nmulti-image understanding capabilities of multimodal LLMs. MuirBench consists\nof 12 diverse multi-image tasks (e.g., scene understanding, ordering) that\ninvolve 10 categories of multi-image relations (e.g., multiview, temporal\nrelations). Comprising 11,264 images and 2,600 multiple-choice questions,\nMuirBench is created in a pairwise manner, where each standard instance is\npaired with an unanswerable variant that has minimal semantic differences, in\norder for a reliable assessment. Evaluated upon 20 recent multi-modal LLMs, our\nresults reveal that even the best-performing models like GPT-4o and Gemini Pro\nfind it challenging to solve MuirBench, achieving 68.0% and 49.3% in accuracy.\nOpen-source multimodal LLMs trained on single images can hardly generalize to\nmulti-image questions, hovering below 33.3% in accuracy. These results\nhighlight the importance of MuirBench in encouraging the community to develop\nmultimodal LLMs that can look beyond a single image, suggesting potential\npathways for future improvements.",
        "updated": "2024-06-13 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09411v1"
    },
    {
        "title": "Scene Graph Generation in Large-Size VHR Satellite Imagery: A Large-Scale Dataset and A Context-Aware Approach",
        "authors": "Yansheng LiLinlin WangTingzhu WangXue YangJunwei LuoQi WangYouming DengWenbin WangXian SunHaifeng LiBo DangYongjun ZhangYi YuJunchi Yan",
        "links": "http://arxiv.org/abs/2406.09410v1",
        "entry_id": "http://arxiv.org/abs/2406.09410v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09410v1",
        "summary": "Scene graph generation (SGG) in satellite imagery (SAI) benefits promoting\nintelligent understanding of geospatial scenarios from perception to cognition.\nIn SAI, objects exhibit great variations in scales and aspect ratios, and there\nexist rich relationships between objects (even between spatially disjoint\nobjects), which makes it necessary to holistically conduct SGG in large-size\nvery-high-resolution (VHR) SAI. However, the lack of SGG datasets with\nlarge-size VHR SAI has constrained the advancement of SGG in SAI. Due to the\ncomplexity of large-size VHR SAI, mining triplets <subject, relationship,\nobject> in large-size VHR SAI heavily relies on long-range contextual\nreasoning. Consequently, SGG models designed for small-size natural imagery are\nnot directly applicable to large-size VHR SAI. To address the scarcity of\ndatasets, this paper constructs a large-scale dataset for SGG in large-size VHR\nSAI with image sizes ranging from 512 x 768 to 27,860 x 31,096 pixels, named\nRSG, encompassing over 210,000 objects and more than 400,000 triplets. To\nrealize SGG in large-size VHR SAI, we propose a context-aware cascade cognition\n(CAC) framework to understand SAI at three levels: object detection (OBD), pair\npruning and relationship prediction. As a fundamental prerequisite for SGG in\nlarge-size SAI, a holistic multi-class object detection network (HOD-Net) that\ncan flexibly integrate multi-scale contexts is proposed. With the consideration\nthat there exist a huge amount of object pairs in large-size SAI but only a\nminority of object pairs contain meaningful relationships, we design a pair\nproposal generation (PPG) network via adversarial reconstruction to select\nhigh-value pairs. Furthermore, a relationship prediction network with\ncontext-aware messaging (RPCM) is proposed to predict the relationship types of\nthese pairs.",
        "updated": "2024-06-13 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09410v1"
    },
    {
        "title": "4M-21: An Any-to-Any Vision Model for Tens of Tasks and Modalities",
        "authors": "Roman BachmannOğuzhan Fatih KarDavid MizrahiAli GarjaniMingfei GaoDavid GriffithsJiaming HuAfshin DehghanAmir Zamir",
        "links": "http://arxiv.org/abs/2406.09406v1",
        "entry_id": "http://arxiv.org/abs/2406.09406v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09406v1",
        "summary": "Current multimodal and multitask foundation models like 4M or UnifiedIO show\npromising results, but in practice their out-of-the-box abilities to accept\ndiverse inputs and perform diverse tasks are limited by the (usually rather\nsmall) number of modalities and tasks they are trained on. In this paper, we\nexpand upon the capabilities of them by training a single model on tens of\nhighly diverse modalities and by performing co-training on large-scale\nmultimodal datasets and text corpora. This includes training on several\nsemantic and geometric modalities, feature maps from recent state of the art\nmodels like DINOv2 and ImageBind, pseudo labels of specialist models like SAM\nand 4DHumans, and a range of new modalities that allow for novel ways to\ninteract with the model and steer the generation, for example image metadata or\ncolor palettes. A crucial step in this process is performing discrete\ntokenization on various modalities, whether they are image-like, neural network\nfeature maps, vectors, structured data like instance segmentation or human\nposes, or data that can be represented as text. Through this, we expand on the\nout-of-the-box capabilities of multimodal models and specifically show the\npossibility of training one model to solve at least 3x more tasks/modalities\nthan existing ones and doing so without a loss in performance. This enables\nmore fine-grained and controllable multimodal generation capabilities and\nallows us to study the distillation of models trained on diverse data and\nobjectives into a unified model. We successfully scale the training to a three\nbillion parameter model using tens of modalities and different datasets. The\nresulting models and training code are open sourced at 4m.epfl.ch.",
        "updated": "2024-06-13 17:59:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09406v1"
    },
    {
        "title": "ConsistDreamer: 3D-Consistent 2D Diffusion for High-Fidelity Scene Editing",
        "authors": "Jun-Kun ChenSamuel Rota BulòNorman MüllerLorenzo PorziPeter KontschiederYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2406.09404v1",
        "entry_id": "http://arxiv.org/abs/2406.09404v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09404v1",
        "summary": "This paper proposes ConsistDreamer - a novel framework that lifts 2D\ndiffusion models with 3D awareness and 3D consistency, thus enabling\nhigh-fidelity instruction-guided scene editing. To overcome the fundamental\nlimitation of missing 3D consistency in 2D diffusion models, our key insight is\nto introduce three synergetic strategies that augment the input of the 2D\ndiffusion model to become 3D-aware and to explicitly enforce 3D consistency\nduring the training process. Specifically, we design surrounding views as\ncontext-rich input for the 2D diffusion model, and generate 3D-consistent,\nstructured noise instead of image-independent noise. Moreover, we introduce\nself-supervised consistency-enforcing training within the per-scene editing\nprocedure. Extensive evaluation shows that our ConsistDreamer achieves\nstate-of-the-art performance for instruction-guided scene editing across\nvarious scenes and editing instructions, particularly in complicated\nlarge-scale indoor scenes from ScanNet++, with significantly improved sharpness\nand fine-grained textures. Notably, ConsistDreamer stands as the first work\ncapable of successfully editing complex (e.g., plaid/checkered) patterns. Our\nproject page is at immortalco.github.io/ConsistDreamer.",
        "updated": "2024-06-13 17:59:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09404v1"
    }
]