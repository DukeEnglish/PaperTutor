[
    {
        "title": "Characterising Interventions in Causal Games",
        "authors": "Manuj MishraJames FoxMichael Wooldridge",
        "links": "http://arxiv.org/abs/2406.09318v1",
        "entry_id": "http://arxiv.org/abs/2406.09318v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09318v1",
        "summary": "Causal games are probabilistic graphical models that enable causal queries to\nbe answered in multi-agent settings. They extend causal Bayesian networks by\nspecifying decision and utility variables to represent the agents' degrees of\nfreedom and objectives. In multi-agent settings, whether each agent decides on\ntheir policy before or after knowing the causal intervention is important as\nthis affects whether they can respond to the intervention by adapting their\npolicy. Consequently, previous work in causal games imposed chronological\nconstraints on permissible interventions. We relax this by outlining a sound\nand complete set of primitive causal interventions so the effect of any\narbitrarily complex interventional query can be studied in multi-agent\nsettings. We also demonstrate applications to the design of safe AI systems by\nconsidering causal mechanism design and commitment.",
        "updated": "2024-06-13 16:55:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09318v1"
    },
    {
        "title": "Applying Multi-Agent Negotiation to Solve the Production Routing Problem With Privacy Preserving",
        "authors": "Luiza Pellin BiasotoVinicius Renan de CarvalhoJaime Simão Sichman",
        "links": "http://arxiv.org/abs/2406.09214v1",
        "entry_id": "http://arxiv.org/abs/2406.09214v1",
        "pdf_url": "http://arxiv.org/pdf/2406.09214v1",
        "summary": "This paper presents a novel approach to address the Production Routing\nProblem with Privacy Preserving (PRPPP) in supply chain optimization. The\nintegrated optimization of production, inventory, distribution, and routing\ndecisions in real-world industry applications poses several challenges,\nincluding increased complexity, discrepancies between planning and execution,\nand constraints on information sharing. To mitigate these challenges, this\npaper proposes the use of intelligent agent negotiation within a hybrid\nMulti-Agent System (MAS) integrated with optimization algorithms. The MAS\nfacilitates communication and coordination among entities, encapsulates private\ninformation, and enables negotiation. This, along with optimization algorithms,\nmakes it a compelling framework for establishing optimal solutions. The\napproach is supported by real-world applications and synergies between MAS and\noptimization methods, demonstrating its effectiveness in addressing complex\nsupply chain optimization problems.",
        "updated": "2024-06-13 15:15:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.09214v1"
    },
    {
        "title": "Multi-Agent Software Development through Cross-Team Collaboration",
        "authors": "Zhuoyun DuChen QianWei LiuZihao XieYifei WangYufan DangWeize ChenCheng Yang",
        "links": "http://arxiv.org/abs/2406.08979v1",
        "entry_id": "http://arxiv.org/abs/2406.08979v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08979v1",
        "summary": "The latest breakthroughs in Large Language Models (LLMs), eg., ChatDev, have\ncatalyzed profound transformations, particularly through multi-agent\ncollaboration for software development. LLM agents can collaborate in teams\nlike humans, and follow the waterfall model to sequentially work on\nrequirements analysis, development, review, testing, and other phases to\nperform autonomous software generation. However, for an agent team, each phase\nin a single development process yields only one possible outcome. This results\nin the completion of only one development chain, thereby losing the opportunity\nto explore multiple potential decision paths within the solution space.\nConsequently, this may lead to obtaining suboptimal results. To address this\nchallenge, we introduce Cross-Team Collaboration (CTC), a scalable multi-team\nframework that enables orchestrated teams to jointly propose various decisions\nand communicate with their insights in a cross-team collaboration environment\nfor superior content generation. Experimental results in software development\nreveal a notable increase in quality compared to state-of-the-art baselines,\nunderscoring the efficacy of our framework. The significant improvements in\nstory generation demonstrate the promising generalization ability of our\nframework across various domains. We anticipate that our work will guide LLM\nagents towards a cross-team paradigm and contribute to their significant growth\nin but not limited to software development. The code and data will be available\nat https://github.com/OpenBMB/ChatDev.",
        "updated": "2024-06-13 10:18:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08979v1"
    },
    {
        "title": "Adaptive Swarm Mesh Refinement using Deep Reinforcement Learning with Local Rewards",
        "authors": "Niklas FreymuthPhilipp DahlingerTobias WürthSimon ReischLuise KärgerGerhard Neumann",
        "links": "http://arxiv.org/abs/2406.08440v1",
        "entry_id": "http://arxiv.org/abs/2406.08440v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08440v1",
        "summary": "Simulating physical systems is essential in engineering, but analytical\nsolutions are limited to straightforward problems. Consequently, numerical\nmethods like the Finite Element Method (FEM) are widely used. However, the FEM\nbecomes computationally expensive as problem complexity and accuracy demands\nincrease. Adaptive Mesh Refinement (AMR) improves the FEM by dynamically\nallocating mesh elements on the domain, balancing computational speed and\naccuracy. Classical AMR depends on heuristics or expensive error estimators,\nlimiting its use in complex simulations. While learning-based AMR methods are\npromising, they currently only scale to simple problems. In this work, we\nformulate AMR as a system of collaborating, homogeneous agents that iteratively\nsplit into multiple new agents. This agent-wise perspective enables a spatial\nreward formulation focused on reducing the maximum mesh element error. Our\napproach, Adaptive Swarm Mesh Refinement (ASMR), offers efficient, stable\noptimization and generates highly adaptive meshes at user-defined resolution\nduring inference. Extensive experiments, including volumetric meshes and\nNeumann boundary conditions, demonstrate that ASMR exceeds heuristic approaches\nand learned baselines, matching the performance of expensive error-based oracle\nAMR strategies. ASMR additionally generalizes to different domains during\ninference, and produces meshes that simulate up to 2 orders of magnitude faster\nthan uniform refinements in more demanding settings.",
        "updated": "2024-06-12 17:26:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08440v1"
    },
    {
        "title": "Efficient Adaptation in Mixed-Motive Environments via Hierarchical Opponent Modeling and Planning",
        "authors": "Yizhe HuangAnji LiuFanqi KongYaodong YangSong-Chun ZhuXue Feng",
        "links": "http://arxiv.org/abs/2406.08002v1",
        "entry_id": "http://arxiv.org/abs/2406.08002v1",
        "pdf_url": "http://arxiv.org/pdf/2406.08002v1",
        "summary": "Despite the recent successes of multi-agent reinforcement learning (MARL)\nalgorithms, efficiently adapting to co-players in mixed-motive environments\nremains a significant challenge. One feasible approach is to hierarchically\nmodel co-players' behavior based on inferring their characteristics. However,\nthese methods often encounter difficulties in efficient reasoning and\nutilization of inferred information. To address these issues, we propose\nHierarchical Opponent modeling and Planning (HOP), a novel multi-agent\ndecision-making algorithm that enables few-shot adaptation to unseen policies\nin mixed-motive environments. HOP is hierarchically composed of two modules: an\nopponent modeling module that infers others' goals and learns corresponding\ngoal-conditioned policies, and a planning module that employs Monte Carlo Tree\nSearch (MCTS) to identify the best response. Our approach improves efficiency\nby updating beliefs about others' goals both across and within episodes and by\nusing information from the opponent modeling module to guide planning.\nExperimental results demonstrate that in mixed-motive environments, HOP\nexhibits superior few-shot adaptation capabilities when interacting with\nvarious unseen agents, and excels in self-play scenarios. Furthermore, the\nemergence of social intelligence during our experiments underscores the\npotential of our approach in complex multi-agent environments.",
        "updated": "2024-06-12 08:48:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.08002v1"
    }
]