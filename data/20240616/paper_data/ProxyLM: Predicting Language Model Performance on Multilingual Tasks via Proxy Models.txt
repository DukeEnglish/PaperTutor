ProxyLM: Predicting Language Model Performance on Multilingual
Tasks via Proxy Models
DavidAnugraha1,GentaIndraWinata2∗,ChenyueLi1,
PatrickAmadeusIrawan3†,En-ShiunAnnieLee1,4
1UniversityofToronto 2CapitalOne
3InstitutTeknologiBandung 4OntarioTechUniversity
{david.anugraha,chenyueli}@mail.utoronto.ca, genta.winata@capitalone.com,
†13520109@std.stei.itb.ac.id, annie.lee@ontariotechu.ca
Abstract presentsarangeofchallenges. Onesignificantchal-
lenge is the limited data availability, which ham-
Performance prediction is a method to esti-
perseffectivefine-tuningprocesses(Guetal.,2018;
matetheperformanceofmultilinguallanguage
Adilazuardaetal.,2024),makingmodeladaptation
models(LMs),mitigatingcomputationalcosts
throughfine-tuningachallengingtask(Zophetal.,
associated with model capacity and data for
fine-tuning. OurpaperintroducesPROXYLM, 2016;Liuetal.,2021). Anothercriticalissueisthe
a scalable framework for predicting LM per- lackofpre-trainingdatafornumerousregionallan-
formanceusingproxymodelsinmultilingual guages,suchasSoutheastAsianlanguages(Winata
tasks. These proxy models act as surrogates, et al., 2022, 2024; Yong et al., 2024), with many
approximatingtheperformanceoffine-tuned
languages being omitted during the pre-training
LMsonspecificdownstreamnaturallanguage
phaseofmultilingualLMs.
processing(NLP)tasks. Byleveragingproxy
Given the limited academic computational re-
models,PROXYLMsignificantlyreducescom-
putationaloverheadontaskevaluations,achiev- sources for LM fine-tuning and inadequate LRL
inguptoa37.08×speedupcomparedtotradi- datasets,efficientmethodsinpredictingmodelper-
tionalmethods,evenwithoursmallestproxy formance alleviate the dependency on extensive
models. Additionally,ourmethodologyshow- resources. While linear regression and gradient-
cases adaptability to previously unseen lan-
boosting hold promise in performance predic-
guagesinpre-trainedLMs,outperformingthe
tion (Birch et al., 2008a; Srinivasan et al., 2021;
state-of-the-artperformanceby1.89×asmea-
Xia et al., 2020a; Ye et al., 2021a; Schram et al.,
suredbyroot-mean-square-error(RMSE).This
2023;Khiuetal.,2024),existingsolutionsprimar-
framework streamlines model selection, en-
abling efficient deployment and iterative LM ilyfocusonhomogeneousdatasettingsandprior-
enhancementswithoutextensivecomputational itize high-resource languages using Transformer
resources. models(Vaswanietal.,2017). Khiuetal.(2024)
examinediversedatasetsandLRLsbutencounter
1 Introduction
limitationsinthenumberofexperiments,language
LanguageModels(LMs)havebecomeincreasingly diversity, and model scope, focusing solely on
valuableforassessingNaturalLanguageProcess- mBART(Liuetal.,2020). Recentadvancements
ing(NLP)tasks(Raffeletal.,2020;Brownetal., inlargermultilingualmodels, likeNLLB(Costa-
2020; Costa-jussà et al., 2022b; Touvron et al., jussàetal.,2022a)andM2M100(Fanetal.,2021),
2023a,b; Le Scao et al., 2023). However, fine- have significantly improved machine translation
tuning and evaluating these models are resource- capabilities,exceedingthoseofmBART.
intensiveprocessesintermsofbothcomputation Inthispaper,wepropose PROXYLM,1 aframe-
and time. These costs escalate with model size, work to predict LMs performance by utilizing
especially when experimenting across multiple proxymodelsonLRLs. Proxymodelsaredefined
datasets. As highlighted in Kaplan et al. (2020), assubstitutemodels,whereintheperformanceof
thereisascalinglawthatappliestobothmodeland thesesubstitutemodelsareusedtoestimatetheper-
datasetsizes,andcomputationaldemands,indicat- formanceofanotherLM.Thisothermodelcanbe
ingthatlargermodelsandbroaderdatasetsrequire significantlylargerthanourproxymodels. Foropti-
increasedcomputationalresources. Modelinglow- mizingtheprediction,weutilizemuchsmallerLMs
resourcelanguages(LRLs)inmultilingualcontexts
1We release our code at https://github.com/
∗TheworkwasconductedoutsideCapitalOne. davidanugraha/proxylm
4202
nuJ
31
]LC.sc[
1v43390.6042:viXraFigure1: PROXYLMframeworkforLMperformanceprediction. (Top)Theevaluationmetriciscomputedonthe
testsetusingaproxymodelMi. (Bottom)Theregressorgistrainedusingproxymodelscoresaswellasdataset
p
andlanguagefeaturesbyminimizingtheRMSEdifferenceofy andyˆ .
M M
asproxymodelsandoff-the-shelfmodelswithout achieveuptoa37.08×speedupontaskeval-
further tuning. This approach is very scalable to uationcomparedtothetraditionalapproach,
multiple proxy models and task-agnostic to any highlightingtheefficiencyofourapproach.
modalities, thus it can be applied to any down-
2 Methodology
streamtasks. Thisstudyfocusesonmachinetrans-
lationtasksandourapproachoutperformstheexist-
In this section, we formally define the LM per-
ingworkfromXiaetal.(2020a);Yeetal.(2021a);
formancepredictionproblemandourproposalto
Schram et al. (2023); Khiu et al. (2024), which
improveperformanceprediction.
opensanewavenuetoemployLMsformodelper-
formanceprediction. Thereforethecontributionof 2.1 PROXYLM
ourpapercanbesummarizedinthree-fold:
Recallthatperformancepredictionisataskofesti-
matingasystem’sperformancebasedonthemodel
1. We introduce PROXYLM, an efficient and
and its training strategy, training and test dataset,
scalable framework designed to predict the
andlanguageused. Formally,letLMMbeoures-
performanceofLMs. Thisframeworksignifi-
timatedmodel. Mistrainedoveratrainingdataset
cantlyreducesthecomputationalcostsasso-
DwithsourcelanguageL andtargetlanguageL ,
s t
ciatedwithfine-tuningandinferenceduring
andthentestedusingdatasetD′. M’sperformance,
modelselection.
denotedy ,canbeformulatedunderfunctionf
M
thatrelatesbetweenthesevariables:
2. Wedemonstratetheeffectivenessandrobust-
nessofPROXYLMacross18datasetsources y = f(M,D,D′,L ,L ). (1)
M s t
and50languagesontwoestimatedLMarchi-
tectures. Our framework substantially out- Wecanapproximatef bytransformingEquation1
performs all existing baselines in English- intoaregressiontaskwitharegressorfunctiong,
centric,many-to-manylanguages,andcross- whichwillbetrainedonpastperformancerecords.
datasetsettings,includingscenariosinvolving The regressor takes dataset features Φ(D,D′) to
extremely LRLs that remain unseen by pre- identifythecharacteristicsofthetrainingandtest
trained LMs, surpassing the state-of-the-art datasets, aswellasthedistributionshiftbetween
performancemeasuredwithRMSEby1.89×. them. It also takes language features Ψ(L ,L )
s t
tomeasurethedissimilaritiesbetweenthesource
3. We also provide a time analysis comparing and target languages. This can be formulated as
the fine-tuning duration of proxy models to follows:
directmodelfine-tuning. Ourresultsindicate
that,withoursmallestproxymodels,wecan yˆ = g(Φ(D,D′);Ψ(L ,L )). (2)
M s tWe present PROXYLM, a framework that lever- 3 ExperimentalSetup
agesthepastperformanceofothermodels,referred
Inthissection,wedescribethedatasetsandLMs
to as proxy models, as additional context for our
usedtoobtainLMs’performancerecords. These
regressor. Intuitively, proxy models can provide
records were then used to train various regressor
valuable insights that assist in predicting the per-
modelsunderdifferentexperimentalsettingstoin-
formance of the estimated model M. Formally,
vestigateourapproachtoperformancepredictions.
let M = [M1,...,MN] be a set of N proxy
p p p The details of the hyper-parameters for both the
models. To integrate the information from these
LMsandtheregressorsareprovidedinA.3.
proxymodels,weproposemodifyingEquation2
asfollows: 3.1 Datasets
We evaluate our approach through two machine
yˆ = g(yˆ ;Φ(D,D′);Ψ(L ,L )), (3)
M Mp s t translation benchmarks: MT560 (Gowda et al.,
2021) and NusaTranslation (Cahyawijaya et al.,
wherey Mp = [y M1 p,...,y MN
p
]representstheper- 2023). The MT560 dataset is English-centric,
formancerecordsofN proxymodels. Theadvan- where English can serve as either the source or
tageofusingproxymodelsarisesfromtheirfaster target language. We curated 20 datasets and se-
fine-tuning and evaluation compared to the esti- lected 44 languages out of 500 for evaluation in
matedmodelM. Thisalsomeansthatoff-the-shelf MT560. In contrast, the NusaTranslation dataset
modelscanbeuseddirectlywithoutadditionaltun- comprisesparalleltextsin12Indonesianregional
ing if they already perform the task adequately, languageswithinaMany-to-ManyLanguagesset-
furtherenhancingefficiency. ting, allowing any language to act as the source
or target. As many of these languages are absent
2.2 PROXYLM Features in pre-trained multilingual models, we analyze 8
outofthe12languagesduetolimiteddatainthe
LanguageFeatures WeuseURIELTypological
remaining4. Thedatasetsencompass50languages
Database(Littelletal.,2017)similartoXiaetal.
across various domains such as economics, tech-
(2020a)includinggeographic, genetic, inventory,
nology,andmedicine. Detailedlanguageinsights
syntactic,phonological,andfeaturaldistance. The
areavailableinTables8and9inA.1forreference.
languagefeaturesareusefultoprovidealanguage-
specificrepresentationtotheregressormodel. 3.2 Models
EstimatedLMs WeemploytwoestimatedLMs:
Dataset Features We extract 6 features from
M2M100 1.2B (Fan et al., 2021) and NLLB
the dataset, including train size, vocab size, av-
1.3B (Costa-jussà et al., 2022b). Each estimated
eragesentencelength,wordoverlap,Type-Token
model is fine-tuned using a standard next token
Ratio (TTR), and TTR distance from D and D′
predictionobjectiveonthetrainingset.
based on Xia et al. (2020a). We will refer to
thesefeaturesandlanguagefeaturescombinedas Proxy Models We utilize four different
NLPerffeatures. Furthermore,weincorporatethe transformer-based models: an encoder-decoder
distributionshiftinformationbetweenthetraining randominitializedTransformers(100M)(Vaswani
andtestdatasetsusingJensen-ShannonDivergence et al., 2017), SMaLL-100 (330M) (Mohammad-
(JSD)asdescribedby (Khiuetal.,2024). Inaddi- shahietal.,2022),M2M100(Fanetal.,2021),and
tion,weincludetermfrequency-inversedocument NLLB (Costa-jussàetal.,2022b). ForM2M100
frequency (TF-IDF) and sentence similarity with and NLLB, we use the models without any
Sentence-BERT (ReimersandGurevych,2019). additional tuning (No FT) in a zero-shot fashion.
ModeldetailsareprovidedinAppendixA.2. The
ProxyModelsFeatures Weleveragetheperfor- evaluation is done using SentencePiece BLEU
mance data from proxy models, derived by aver- (spBLEU) (Goyal et al., 2022), as it has been
agingresultsfrommultiplefine-tuningandevalua- demonstrated to be a fair metric in multilingual
tioniterationsonidenticaldatasetsandlanguages. settings,particularlyinlow-resourcesettings. For
Moreover, we retain the flexibility to adjust the simplicity, the term “fine-tuning" will be used
numberofproxymodelsemployed,facilitatingef- throughoutthispapertorefertoboththeprocess
ficientandadaptableperformanceestimation. of training from scratch (as in the case of theTransformer (100M) model) and the process of andtestitusingrecordsfromtheNusaTrans-
fine-tuningpre-trainedLMs. lationdataset. Weoptnottoreversethissetup
as the dataset exhibits no domain shift and
Regressors We utilize XGBoost (Chen and
containsfewerperformancerecords.
Guestrin, 2016), LGBM (Ye et al., 2021a),
Poly2 (Khiu et al., 2024), and Poly3 (Khiu et al.,
4 ResultsandAnalysis
2024)asourregressors. Inmostofourexperiments,
weapplyXGBoostasourdefaultregressorbecause In this section, we present the results of the per-
wefindittobethebest-performingmodel,while formancepredictionsfor PROXYLMandbaselines
theotherregressorsserveasbaselines. Specifically overthreesettings: English-centric,Many-to-Many
fortheMany-to-ManyLanguagessetting,Matrix Languages,andCross-Dataset,asdescribedabove.
Factorizationwithcontextfeatures(MF)isusedas Further, we discuss the robustness, effectiveness,
an additional baseline (Schram et al., 2023). We andefficiencyofPROXYLM inthecontextofper-
donotapplyMFtoourEnglish-centricsettingbe- formanceprediction.
causeMFrequirestheperformancerecordstobe
structuredintwodimensions—oneforthesource 4.1 English-centricResults
language and one for the target language. In the
Table 1 shows the overall results on MT560.
English-centricsetting,thiswouldresultinasparse
PROXYLM remarkably outperforms all existing
matrix with only one fully populated row or col-
baselines. We find that incorporating all proxy
umn, corresponding to English, making MF im-
models (Ensemble) is the most effective for pre-
practicalforthissetup.
diction,leadingtoa2.29×averagedreductionin
RMSEacrossallexperimentalsettingscompared
3.3 ExperimentalSettings
tothebestbaseline. Weobservethatusingthe“No
Each regressor is evaluated using RMSE as our
FT" estimated model to predict the performance
performance metric and evaluated 5 times with
oftheirfine-tunedmodelsissurprisinglyusefulin
different seeds to obtain the mean and standard
allsettings,especiallyforNLLB,wherethemodel
deviation of the performance results. We set our
alreadyhasdecentmachinetranslationqualityon
experimentsettingsasfollows:
LRLs. This observation is supportedby our find-
• Random: We randomly sample the perfor- ingswithintheXGBoostmodelthattheNLLBNo
mancerecordsintotrainingandtestsetswith FTfeaturehasthehighestimportancescoreamong
a ratio of 7:3. Then, we run 10-fold cross- allfeatures,asshowninFigure16. Further,using
validationonthetrainingsettofindthebest SMaLL-100fine-tunedperformanceprovidesuse-
hyper-parametersforeachregressor. Thebest- fulestimationsforsettingsinvolvingM2M100as
performingregressorwouldsubsequentlybe the estimated model. This may indicate that the
evaluatedonthetestset. performanceofamodelwithsimilararchitecture
canbeagoodestimatorfortheperformanceofthe
• Leave-One-Language-Out(LOLO):Wese- largerestimatedmodel. Inotherwords,thechoice
lectonelanguageasthetestset,whichisnot ofproxymodeltohelppredictionmatters. Feature
encounteredduringtraining. importanceanalysisfromtheXGBoostmodelsup-
portsthis,revealingthattheSMaLL-100fine-tuned
• Unseen: Theperformancerecordscanbedi-
featurehasthehighestimportancescoreamongall
vided into two categories: (1) records with
features,asshowninFigure15.
“seen" languages and (2) records with “un-
OuranalysisalsoindicatesthatXGBoostoutper-
seen" languages. “Unseen" languages refer
formsotherregressionmodels,acrossallevaluated
to languages that are not present in the pre-
settings. Both XGBoost and LGBM, which are
trainingLMdata,while“seen"languagesde-
gradient-boostingandtree-basedlearningmethods,
notethosethatarepresent. Inthissetting,the
demonstratesuperiorperformancemetricsacross
regressor is trained using records of “seen"
allsettings. Theirrobustnessandefficiencyasnon-
languages and tested using records of “un-
linear models are evident when compared to lin-
seen"languages.
ear models, such as Poly2 and Poly3. Poly2 and
• Cross-Dataset: Wetraintheregressorusing Poly3regressors,whichemploysecond-degreeand
performancerecordsfromtheMT560dataset third-degreepolynomialregressionapproachesre-Models Random LOLO Unseen
M2M100↓ NLLB↓ M2M100↓ NLLB↓ M2M100↓ Avg.
NLPerf(Xiaetal.,2020a)withdifferentregressors
XGBoost 8.09±0.44 8.40±0.41 8.81±0.39 12.35±0.52 8.22±0.46 9.17
Poly2(Khiuetal.,2024) 10.00±0.46 12.30±0.48 10.54±0.00 13.28±0.00 9.91±0.00 11.21
Poly3(Khiuetal.,2024) 9.93±0.52 12.33±0.46 10.66±0.00 14.06±0.00 10.57±0.00 11.51
LGBM(Yeetal.,2021a) 8.18±0.34 8.59±0.33 9.00±0.49 12.41±0.53 8.59±0.69 9.35
PROXYLM(Ours)‡ withdifferentproxymodels
Transformer 5.29±0.38 7.65±0.47 6.25±0.29 11.33±0.50 6.54±0.30 7.41
SMaLL-100 4.26±0.35 7.02±0.44 4.33±0.24 9.70±0.37 4.54±0.41 5.97
SMaLL-100(NoFT) 5.93±0.23 6.73±0.49 6.33±0.28 10.66±0.44 5.77±6.06 7.08
EstimatedModel(NoFT) 5.76±0.19 4.29±0.31 6.23±0.34 4.59±0.22 5.67±0.41 5.31
Ensemble† 3.79±0.30 3.79±0.25 3.90±0.22 4.14±0.24 4.36±0.22 4.00
Table1:English-centrictestresultsinaverageRMSE±standarddeviation(lowerisbetter). Boldnumbersindicate
thebestperformance,whileunderlinednumbersrepresentthesecond-bestperformance. Thecolumnsshowthe
settingandestimatedmodel. “NoFT"denotes“nofine-tuning"andthemodelinferenceisdoneinazero-shot
fashion. WeonlyshowM2M100resultsfortheUnseensettingsinceNLLBcoversalllanguagesintheMT560
dataset. Avgrepresentstheaverageoftheresultsacrosstherow. ‡ThereportedresultsuseXGBoostastheregressor.
†Ensembledenotescombiningallfourproxymodels,thedetailedbreakdownofthisresultcanbeseeninB.
Models M2M100 NLLB to-Manylanguagessetting. Theresultsrevealthat
LRL MRL LRL MRL the Ensemble model achieves the lowest RMSE,
Transformer 6.14 6.67 11.60 10.26 witha1.70×averagedreductioninRMSEacross
SMaLL-100 4.28 4.55 10.18 7.83 allexperimentalsettingscomparedtothebestbase-
SMaLL-100(NoFT) 6.17 6.96 7.90 11.38
line,indicatingsuperioraccuracyinperformance
EstimatedModel(NoFT) 6.15 6.57 4.66 4.32
predictions. An exception occurs in the random
Ensemble 3.93 3.80 4.10 4.15
NLLB setting, where the model utilizing only
Table2: English-centricLOLOtestresultsinaverage NLPerffeaturesoutperformstheensemblemodel,
RMSE by language vitality (lower is better). “No achieving the best performance. Note that no do-
FT"denotes“nofine-tuning". “LRL"and“MRL"de-
mainshiftoccurswithinthedataset.
notelow-resourcelanguagesandmedium-resourcelan-
A comparative analysis shows that predicting
guages,basedon(Joshietal.,2021). Thefullmapping
the performance of M2M100 models in the ran-
isintheAppendixonTable8&9.
domsettingpresentsagreaterchallengecompared
spectively, tend to generate lower scores. This topredictingfortheNLLBmodels. Thisdiscrep-
diminished performance is largely attributed to ancysuggeststhatthecomplexityofperformance
theirlimitationsincapturingthenonlinearrelation- predictioncanvarysubstantiallydependingonthe
ships inherent in the data, leading to suboptimal specific LM and the conditions under which it is
results. We further present the results by the lan- evaluated. Aparticularlynoteworthyfindingisthe
guage vitality on Table 2. The overall difference effectiveness of using “No FT" models for esti-
inRMSEbetweenLRLsandmedium-resourcelan- mating LM performance. The “No FT" models,
guages (MRLs) is relatively small, except when which do not require any additional fine-tuning,
usingSMaLL-100asproxymodels. Aninteresting demonstrate high accuracy in their performance
observationhereonSMaLL-100inpredictingthe predictions. This method offers substantial effi-
NLLBmodelisthatthe“NoFT"modelcanpredict ciency benefits, as it eliminates the need for ex-
“LRL"muchbetterthanthefine-tunedcounterpart, tensivecomputationalresourcestypicallyrequired
andthefine-tunedmodelbetterpredicts“MRL". for model training. In contrast, we find similar
results between the LOLO setting for Many-to-
4.2 Many-to-ManyLanguagesResults
ManylanguagesandEnglish-centricresults,where
Table3presentstheperformanceofdifferentmod- PROXYLM using Ensemble remarkably outper-
elsontheNusaTranslationdatasetwithintheMany- forms all existing baselines. In addition, we findModels Random LOLO
M2M100↓ NLLB↓ M2M100↓ NLLB↓ Avg.
NLPerf(Xiaetal.,2020a)withdifferentregressors
XGBoost 2.45±0.30 1.21±0.29 7.83±0.23 8.28±0.31 4.94
Poly2 (Khiuetal.,2024) 4.70±0.40 4.68±0.51 7.07±0.00 7.90±0.00 6.09
Poly3 (Khiuetal.,2024) 4.60±0.41 4.64±0.49 7.26±0.00 8.01±0.00 6.13
LGBM (Yeetal.,2021a) 2.66±0.22 1.76±0.29 7.91±0.01 8.06±0.00 5.10
MF(Schrametal.,2023) 3.65±0.26 2.60±0.39 7.08±0.23 7.14±0.22 5.12
PROXYLM‡ (Ours)withdifferentproxymodels
Transformer 2.56±0.43 1.70±0.20 5.65±0.23 6.24±0.34 4.04
SMaLL-100 2.56±0.33 1.65±0.44 4.85±0.36 5.14±0.46 3.55
SMaLL-100(NoFT) 2.44±0.21 1.34±0.38 6.93±0.34 7.25±0.37 4.49
EstimatedModel(NoFT) 2.38±0.36 1.27±0.30 5.10±0.28 5.50±0.26 3.56
Ensemble† 2.41±0.28 1.56±0.35 3.73±0.23 3.79±0.19 2.90
Table3: Many-to-ManyLanguagestestresultsinaverageRMSE±standarddeviation(lowerisbetter). Bold
numbersindicatethebestperformance,whileunderlinednumbersrepresentthesecond-bestperformance. The
columnsshowthesettingandestimatedmodel. “NoFT"denotes“nofine-tuning". WedonotcovertheUnseen
settingasmostofthelanguagesarenotcoveredbybothM2M100andNLLBintheNusaTranslationdataset. Avg
representstheaverageoftheresultsacrosstherow. ‡ThereportedresultsareexperimentsusingXGBoostregressor.
†Ensembledenotescombiningallfourproxymodels.
Models M2M100↓ NLLB↓ Theresultshighlightthat PROXYLM withEnsem-
ble significantly reduces RMSE compared to the
NLPerf(Xiaetal.,2020a)withdifferentregressors
bestbaselineby2×and1.69×forM2M100and
XGBoost 9.97±0.35 10.52±0.73
NLLB,respectively. Thisdisplaysconsistentper-
Poly2 (Khiuetal.,2024) 9.27±0.00 21.43±0.00
formanceacrossdatasetsandlanguagesthatwere
Poly3 (Khiuetal.,2024) 8.58±0.00 19.81±0.00
not encountered during the regressor’s training,
LGBM (Yeetal.,2021a) 9.24±0.24 15.19±1.00
including “unseen" languages for the pre-trained
PROXYLM‡(Ours)withdifferentproxymodels
LMs. Moreover,the“NoFT"modelsexhibitvari-
Transformer 6.65±0.26 10.69±0.90 abilitycomparedtootherproxymodels. Theperfor-
SMaLL-100 5.53±0.98 7.68±2.11
mancevariationbetweenM2M100andNLLBmay
SMaLL-100(NoFT) 9.08±0.63 16.66±1.08
beattributedtotheMT560datasetsolelycontain-
EstimatedModel(NoFT) 9.45±0.78 7.10±0.69
ing"seen"languagesforNLLB,lacking“unseen"
Ensemble† 4.30±0.17 6.21±0.50
languages examples for the regressor. This high-
Table4: Cross-DatasettestresultsinaverageRMSE± lights the significance of incorporating “unseen"
standarddeviation(lowerisbetter). Boldnumbersin- languageinstancesintrainingformoredependable
dicatethebestperformance,whileunderlinednumbers predictions.
representthesecond-bestperformance. Thecolumns
showthesettingandestimatedmodel. “NoFT"denotes 4.4 AblationStudy
“nofine-tuning". ‡Thereportedresultsareexperiments
usingLGBMregressor. †Ensembledenotescombining Figure2highlightstheimpactoffeaturesusedin
allfourproxymodels. PROXYLM in the LOLO setting with XGBoost.
Utilizing proxy models as features leads to a sig-
thatusingSMaLL-100fine-tunedperformancere- nificant reduction in RMSE across all scenarios,
sultsinbetterpredictionscomparedtothoseofthe showcasingtheirimportancecomparedtootherfea-
“NoFT"estimatedmodel. tures. FortheMT560dataset,includinglanguage
and dataset features alongside proxy models en-
4.3 Cross-DatasetResults
hancesperformance. Datasetfeaturesaloneshow
Table4illustratesmodelperformanceintheCross- betterimprovementthanlanguagefeaturesalone,
Dataset setup, showcasing the superior perfor- butthecombinationofbothyieldsthebestperfor-
manceof PROXYLM withLGBMoverXGBoost. mance. Ontheotherhand,fortheNusaTranslationDatasets Inference Fine-tuning
MT560 NusaTranslation
Estimatedmodels
M2M100 338s 3.94hrs(4.77×) 1.42hrs(7.32×)
NLLB 492s 9.4hrs(11.48×)7.21hrs(37.08×)
Proxymodels
SMaLL-100 201s 2.5hrs(3.03×) 1.03hrs(5.29×)
Transformer 60s 0.83hrs(1×) 0.19hrs(1×)
Table 5: Comparison of LMs’ inference time (in sec-
onds)andfine-tuningtime(inhours)foroneexperimen-
tal run. The multiplier of fine-tuning time is relative
(a)English-centrictestresultsonMT560dataset. to the Transformer model. All times were calculated
using the interquartilemean to ignore outliers. Since
theinferencetimeforMT560andNusaTranslationare
roughlythesame,onlyonecolumnisprovided.
RegressorModel MT560 NusaTranslation
XGBoost 129.29s 69.81s
Poly2 0.06s 0.03s
(b)Many-to-ManyLanguagestestresultsonNusaTransla-
Poly3 0.07s 0.06s
tiondataset.
LGBM 702.12s 173.17s
Figure2: AblationstudyontheLOLOsettingwithXG- MF N/A 465.37s
BoostonMT560andNusaTranslationdataset. “Proxy
Models" here indicates “Ensemble", which is a com- Table6: Regressormodelstrainingtimecomparisonin
bination of all four proxy models. “Proxy Models" seconds(s).
significantlyreduceRMSEacrossallscenarios.
setsurpassesabout400pastperformancerecords.
Thisobservationimpliesthat,acrossdatasets,there
existsathresholdwheretheadvantagesofincorpo-
ratingadditionalpastperformancerecordsbeginto
exhibitdiminishingreturns.
4.6 TimeEfficiency
Table 5 compares the fine-tuning and inference
times required for the estimated and proxy mod-
els. Theresultsdemonstratethatfine-tuningproxy
Figure3: RMSEdecreasesasXGBoost’strainingset
models or direct inference from any model is re-
size,whichcorrespondstothenumberofMT560past
markablyfasterthanfine-tuningallestimatedmod-
performancerecords,increases.
els. Table6furtherillustratesthispoint,showing
onlyaminimaltrade-offinthetimeneededtotrain
dataset, the benefits of incorporating dataset and
theregressormodels. Thisadditionaltrainingtime
languagefeaturesarelesspronounced,especially
isrelativelynegligible,highlightingtheefficiency
fortheM2M100model,andtheremayevenbea
ofusingproxymodels.
performance dip for the NLLB model due to the
dataset’slackofdomainshift. 4.7 PerformancebyLanguageCategories
In Figure 4, we present detailed XGBoost re-
4.5 DiminishingReturnswithIncreasing
sultswith PROXYLM EnsembleontheM2M100
TrainingSetSize
modelundertheEnglish-centricLOLOexperiment,
In Figure 3, we examine the training of the XG- groupedbylanguagecategories. BasedontheLo-
BoostregressorusingdifferentnumbersofMT560 callyWeightedScatterplotSmoothing(LOWESS)
past performance records as the training dataset. (Cleveland, 1979) curve depicted in Figure 4(c),
Whiletheregressor’sperformanceshowsenhance- ourmethodconsistentlymaintainsunbiasedpredic-
ment with an expanding training size, the incre- tionsforspBLEUscoresbelow40acrossvarious
mentalbenefitsstartdiminishingoncethetraining language types. However, as the spBLEU score(a) (b) (c)
Figure4: DetailedresultsofXGBoostwithPROXYLMEnsembleonM2M100modelundertheLOLOsetting
usingtheMT560datasetfromTable1. Theresultsaregroupedby(a)JoshiClassand(b)languagefamilythat
followsthemappingwhichisprovidedinAppendixA.1;(c)showsthescatterplotillustratingthecorrelationof
spBLEUscoresbetweenthePROXYLM’spredictionandestimatedLM,withthegraydashedlinerepresenting
the line of equality (y = x) with R2 = 0.88 and black dashed line representing Locally Weighted Scatterplot
Smoothing(LOWESS)curvetorepresentthetrend.
increases,theavailabilityofdatapointsdiminishes, theamountofreordering,themorphologicalcom-
leadingtoourmethodunder-predictingtheperfor- plexity of the target language, and the historical
mancecomparedtothetruespBLEUscore. Out- relatednessofthetwolanguages. Xiaetal.(2020a)
liers observed in Kartvelian languages and Indo- leverage extracted dataset features and typologi-
European languages with Joshi class 3 may have cal database language representations. Ye et al.
contributedtothisdiscrepancyinprediction. These (2021a) introduce the use of confidence intervals
observationssuggestthatincreasingthenumberof andcalibrationwithvariousregressoralgorithms
data points covering higher spBLEU scores may forreliableperformanceprediction. Schrametal.
helpmitigatethebiasinprediction. Furtherexperi- (2023)applyBayesianmatrixfactorizationforper-
mentdetailsareavailableinAppendixB. formancepredictiononmultilingualNLPtasks. In
thiswork,wefocustoexplorethelatter. Existing
5 RelatedWork
approaches have shown promise using linear re-
Thepredictionperformanceofmachinelearningal- gressionandgradient-boostingtrees(Birchetal.,
gorithmshasbeenmainlyexploredintworesearch 2008b;Xiaetal.,2020b;Srinivasanetal.,2021;Ye
directions: (1)predictthemodelperformancedur- etal.,2021b). Thesestudieshaveconsidereddata
ingthetrainingruntime,and(2)predictthemodel size,typologicalfeatures,andlanguagesimilarity
performancebyprovidingextractedfeaturesfrom asfactorscontributingtothemodelperformance.
thedataset(Xiaetal.,2020a).
6 Conclusion
Performance Prediction During the Training
Runtime The former aims to infer and extrap-
In this paper, we introduce PROXYLM, a novel
olate the learning curve to approximate training
frameworkdesignedtopredicttheperformanceof
resultsusingevaluationmetricmeasurements(Ko-
LMs by leveraging proxy models specifically for
lachina etal., 2012). Domhanet al.(2015) study
LRLs. By utilizing proxy models as substitutes
the quick detection of poor hyper-parameters in
to estimate the performance of the target model,
probabilisticmodelsafterafewstepsofStochastic
westrategicallyemploysmallerLMsandoff-the-
GradientDescent(SGD).Adriaensenetal.(2024)
shelfmodelswithoutadditionalfine-tuning. This
extrapolatelearningcurvesfromaparametricprior
framework is highly scalable to multiple proxy
usingMarkovChainMonteCarlo(MCMC).
modelsandistask-agnostic,makingitapplicableto
Performance Prediction Using Extracted Fea- awiderangeofdownstreamtasks. Ourstreamlined
tures The latter aims to predict the model per- approachshowcasessubstantialadvancementsin
formancebylearningacorrelationbetweeninput prediction accuracy compared to standard base-
features and final evaluation metric. Birch et al. linesandexhibitsstronggeneralizationcapabilities
(2008a)identifystrongpredictivefeaturessuchas acrossvariedscenarios.Limitations learning curve extrapolation using prior-data fitted
networks. AdvancesinNeuralInformationProcess-
This paper focuses exclusively on two estimated ingSystems,36.
models: M2M100andNLLB,toevaluateourpro-
posedframework. Fordemonstrationpurposes,we Alexandra Birch, Miles Osborne, and Philipp Koehn.
2008a. Predictingsuccessinmachinetranslation. In
concentrateontheusageofspecificmodels,namely
Proceedings of the 2008 Conference on Empirical
the Transformer model, the SMaLL-100 model, MethodsinNaturalLanguageProcessing,pages745–
and the No FT models, to illustrate the effective- 754.
nessofourproxymodels. TheM2M100andNLLB
Alexandra Birch, Miles Osborne, and Philipp Koehn.
modelswereselectedduetotheirprominenceand
2008b. Predicting success in machine translation.
relevance in the field of multilingual translation
In Proceedings of the 2008 Conference on Empiri-
tasks. These models serve as robust benchmarks calMethodsinNaturalLanguageProcessing,pages
for assessing the performance and reliability of 745–754,Honolulu,Hawaii.AssociationforCompu-
tationalLinguistics.
ourproxy-basedframework. Byusingthesewell-
regardedmodels,weaimtoprovidecompellingev-
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
idenceofthecapabilitiesandadvantagesofPROX- Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
YLM.Whileourproposedframeworkisevaluated Neelakantan,PranavShyam,GirishSastry,Amanda
Askell,etal.2020. Languagemodelsarefew-shot
solely within the context of machine translation,
learners. Advancesinneuralinformationprocessing
it is not confined to this application alone. The
systems,33:1877–1901.
framework is designed to be versatile and can be
extended to a variety of other downstream tasks. Samuel Cahyawijaya, Holy Lovenia, Fajri Koto, Dea
Adhista,EmmanuelDave,SarahOktavianti,Salsabil
Weplantoexploretheseadditionalapplicationsin
Akbar, Jhonson Lee, Nuur Shadieq, Tjeng Wawan
futurework.
Cenggoro, et al. 2023. Nusawrites: Constructing
Some other possible avenues for future work high-quality corpora for underrepresented and ex-
could involve a deeper investigation into which tremelylow-resourcelanguages. InProceedingsof
proxymodelsaremoreeffectiveforenhancingper- the13thInternationalJointConferenceonNatural
LanguageProcessingandthe3rdConferenceofthe
formancepredictioninspecificsettings. Ourfind-
Asia-PacificChapteroftheAssociationforCompu-
ingssuggestthatoneproxymodelcanoutperform tationalLinguistics(Volume1: LongPapers),pages
anotherindifferentscenarios,makingitcrucialto 921–945.
carefullyselectthemostrelevantproxymodelsto
Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A
maximizethebenefitsofourapproach. Addition-
scalable tree boosting system. In Proceedings of
ally,developingmethodologiesforcollectingrele-
the 22nd acm sigkdd international conference on
vantpastperformancerecordscouldprovidebetter knowledge discovery and data mining, pages 785–
insightsandimprovethegeneralizationandaccu- 794.
racyofourframework. Pastperformancerecords
Christos Christodouloupoulos and Mark Steedman.
may provide better information gain than others,
2015. A massively parallel corpus: the bible in
potentiallyminimizingthenumberofperformance 100languages. Languageresourcesandevaluation,
records required for a more robust and accurate 49:375–395.
predictor.
WilliamSCleveland.1979. Robustlocallyweighted
regressionandsmoothingscatterplots. J.Am.Stat.
Acknowledgements
Assoc.,74(368):829–836.
WeextendoursinceregratitudetoViktoriaSchram
Marta R. Costa-jussà, Christine Basta, and Gerard I.
forprovidingassistancetoreproducebaselines.
Gállego. 2022a. Evaluating gender bias in speech
translation. In Proceedings of the Thirteenth Lan-
guageResourcesandEvaluationConference,pages
References
2141–2147,Marseille,France.EuropeanLanguage
ResourcesAssociation.
MuhammadFaridAdilazuarda,SamuelCahyawijaya,
AlhamFikriAji,GentaIndraWinata,andAyuPur-
MartaRCosta-jussà,JamesCross,OnurÇelebi,Maha
warianti.2024. Lingualchemy: Fusingtypological
Elbayad,KennethHeafield,KevinHeffernan,Elahe
andgeographicalelementsforunseenlanguagegen-
Kalbassi, JaniceLam, DanielLicht, JeanMaillard,
eralization. arXivpreprintarXiv:2401.06034.
et al. 2022b. No language left behind: Scaling
StevenAdriaensen,HerilalainaRakotoarison,Samuel human-centeredmachinetranslation. arXivpreprint
Müller,andFrankHutter.2024. Efficientbayesian arXiv:2207.04672.TobiasDomhan,JostTobiasSpringenberg,andFrank Teven Le Scao, Angela Fan, Christopher Akiki, El-
Hutter.2015. Speedingupautomatichyperparameter lie Pavlick, Suzana Ilic´, Daniel Hesslow, Roman
optimizationofdeepneuralnetworksbyextrapola- Castagné,AlexandraSashaLuccioni,FrançoisYvon,
tion of learning curves. In Twenty-fourth interna- Matthias Gallé, et al. 2023. Bloom: A 176b-
tionaljointconferenceonartificialintelligence. parameteropen-accessmultilinguallanguagemodel.
Angela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi PatrickLittell,DavidRMortensen,KeLin,Katherine
Ma,AhmedEl-Kishky,SiddharthGoyal,Mandeep Kairis,CarlisleTurner,andLoriLevin.2017. Uriel
Baines, Onur Celebi, Guillaume Wenzek, Vishrav andlang2vec:Representinglanguagesastypological,
Chaudhary,etal.2021. Beyondenglish-centricmul- geographical,andphylogeneticvectors. InProceed-
tilingual machine translation. Journal of Machine ingsofthe15thConferenceoftheEuropeanChap-
LearningResearch,22(107):1–48. teroftheAssociationforComputationalLinguistics:
Volume2,ShortPapers,pages8–14.
Thamme Gowda, Zhao Zhang, Chris Mattmann, and
JonathanMay.2021. Many-to-englishmachinetrans-
YinhanLiu,JiataoGu,NamanGoyal,XianLi,Sergey
lationtools,data,andpretrainedmodels. InProceed-
Edunov, Marjan Ghazvininejad, Mike Lewis, and
ingsofthe59thAnnualMeetingoftheAssociationfor
LukeZettlemoyer.2020. Multilingualdenoisingpre-
ComputationalLinguisticsandthe11thInternational
training for neural machine translation. Transac-
JointConferenceonNaturalLanguageProcessing:
tionsoftheAssociationforComputationalLinguis-
SystemDemonstrations,pages306–316.
tics,8:726–742.
NamanGoyal,CynthiaGao,VishravChaudhary,Peng-
ZihanLiu,GentaIndraWinata,andPascaleFung.2021.
Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-
Continualmixed-languagepre-trainingforextremely
ishnan,Marc’AurelioRanzato,FranciscoGuzmán,
low-resource neural machine translation. In Find-
and Angela Fan. 2022. The Flores-101 evaluation
ingsoftheAssociationforComputationalLinguis-
benchmark for low-resource and multilingual ma-
tics: ACL-IJCNLP2021,pages2706–2718.
chinetranslation. TransactionsoftheAssociationfor
ComputationalLinguistics,10:522–538.
AlirezaMohammadshahi,VassilinaNikoulina,Alexan-
J Gu, H Hassan, J Devlin, and VOK Li. 2018. Uni- dre Bérard, Caroline Brun, James Henderson, and
versalneuralmachinetranslationforextremelylow LaurentBesacier.2022. Small-100:Introducingshal-
resourcelanguages. InProceedingsofthe2018Con- lowmultilingualmachinetranslationmodelforlow-
ferenceoftheNorthAmericanChapteroftheAsso- resourcelanguages. InProceedingsofthe2022Con-
ciationforComputationalLinguistics: HumanLan- ferenceonEmpiricalMethodsinNaturalLanguage
guageTechnologies,Volume1(LongPapers).Asso- Processing,pages8348–8359.
ciationforComputationalLinguistic.
YeQi,DevendraSachan,MatthieuFelix,SargunaPad-
BarryHaddowandFaheemKirefu.2020. Pmindia–a manabhan, and Graham Neubig. 2018. When and
collectionofparallelcorporaoflanguagesofindia. whyarepre-trainedwordembeddingsusefulforneu-
arXivpreprintarXiv:2001.09907. ralmachinetranslation? InProceedingsofthe2018
Conference of the North American Chapter of the
PratikJoshi, SebastinSanty, AmarBudhiraja, Kalika
AssociationforComputationalLinguistics: Human
Bali,andMonojitChoudhury.2021. Thestateand
Language Technologies, Volume 2 (Short Papers),
fate of linguistic diversity and inclusion in the nlp
pages529–535.
world.
ColinRaffel,NoamShazeer,AdamRoberts,Katherine
JaredKaplan,SamMcCandlish,TomHenighan,TomB
Lee,SharanNarang,MichaelMatena,YanqiZhou,
Brown,BenjaminChess,RewonChild,ScottGray,
Wei Li, and Peter J Liu. 2020. Exploring the lim-
AlecRadford,JeffreyWu,andDarioAmodei.2020.
its of transfer learning with a unified text-to-text
Scaling laws for neural language models. arXiv
transformer. Journalofmachinelearningresearch,
preprintarXiv:2001.08361.
21(140):1–67.
Eric Khiu, Hasti Toossi, David Anugraha, Jinyu Liu,
JiaxuLi,JuanArmandoParraFlores,LeandroAcros NilsReimersandIrynaGurevych.2019. Sentence-bert:
Roman,ASezaDog˘ruöz,andEn-ShiunAnnieLee. Sentenceembeddingsusingsiamesebert-networks.
2024. Predictingmachinetranslationperformance InProceedingsofthe2019ConferenceonEmpirical
onlow-resourcelanguages: Theroleofdomainsimi- MethodsinNaturalLanguageProcessing.Associa-
larity. arXivpreprintarXiv:2402.02633. tionforComputationalLinguistics.
PrasanthKolachina,NicolaCancedda,MarcDymetman, ViktoriaSchram,DanielBeck,andTrevorCohn.2023.
andSriramVenkatapathy.2012. Predictionoflearn- Performancepredictionviabayesianmatrixfactori-
ing curves in machine translation. In Proceedings sationformultilingualnaturallanguageprocessing
of the 50th Annual Meeting of the Association for tasks. InProceedingsofthe17thConferenceofthe
ComputationalLinguistics(Volume1: LongPapers), EuropeanChapteroftheAssociationforComputa-
pages22–30. tionalLinguistics,pages1790–1801.Holger Schwenk, Vishrav Chaudhary, Shuo Sun, ZihuiwenYe,PengfeiLiu,JinlanFu,andGrahamNeu-
HongyuGong,andFranciscoGuzmán.2021. Wiki- big.2021a. Towardsmorefine-grainedandreliable
matrix: Mining135mparallelsentencesin1620lan- nlpperformanceprediction. InProceedingsofthe
guagepairsfromwikipedia. InProceedingsofthe 16thConferenceoftheEuropeanChapteroftheAsso-
16thConferenceoftheEuropeanChapteroftheAsso- ciationforComputationalLinguistics: MainVolume,
ciationforComputationalLinguistics: MainVolume, pages3703–3714.
pages1351–1361.
ZihuiwenYe,PengfeiLiu,JinlanFu,andGrahamNeu-
AnirudhSrinivasan, SunayanaSitaram, TanujaGanu, big.2021b. Towardsmorefine-grainedandreliable
SandipanDandapat,KalikaBali,andMonojitChoud- NLPperformanceprediction. InProceedingsofthe
hury.2021. Predictingtheperformanceofmultilin- 16thConferenceoftheEuropeanChapteroftheAsso-
gualnlpmodels. arXivpreprintarXiv:2110.08875. ciationforComputationalLinguistics: MainVolume,
pages3703–3714,Online.AssociationforComputa-
Jörg Tiedemann. 2012. Parallel data, tools and inter- tionalLinguistics.
faces in opus. In Proceedings of the Eighth Inter-
national Conference on Language Resources and Zheng-Xin Yong, Cristina Menghini, and Stephen H
Evaluation(LREC’12),pages2214–2218. Bach. 2024. Lexc-gen: Generating data for ex-
tremelylow-resourcelanguageswithlargelanguage
HugoTouvron,ThibautLavril,GautierIzacard,Xavier models and bilingual lexicons. arXiv preprint
Martinet,Marie-AnneLachaux,TimothéeLacroix, arXiv:2402.14086.
BaptisteRozière,NamanGoyal,EricHambro,Faisal
Azhar, et al. 2023a. Llama: Open and effi- BiaoZhang,PhilipWilliams,IvanTitov,andRicoSen-
cient foundation language models. arXiv preprint nrich.2020. Improvingmassivelymultilingualneu-
arXiv:2302.13971. ralmachinetranslationandzero-shottranslation. In
Proceedingsofthe58thAnnualMeetingoftheAsso-
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- ciationforComputationalLinguistics,pages1628–
bert, Amjad Almahairi, Yasmine Babaei, Nikolay 1639.
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Bhosale, et al. 2023b. Llama 2: Open founda- Barret Zoph, Deniz Yuret, Jonathan May, and Kevin
tion and fine-tuned chat models. arXiv preprint Knight. 2016. Transfer learning for low-resource
arXiv:2307.09288. neural machine translation. In Proceedings of the
2016ConferenceonEmpiricalMethodsinNatural
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob LanguageProcessing,pages1568–1575.
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser,andIlliaPolosukhin.2017. Attentionisall A ExperimentalDetails
youneed. Advancesinneuralinformationprocessing
systems,30. A.1 LanguagesUnderStudy
Genta Winata, Shijie Wu, Mayank Kulkarni, Thamar Welistallthelanguagesusedinthetrainingfrom
Solorio, and Daniel Preo¸tiuc-Pietro. 2022. Cross- theMT560(Gowdaetal.,2021)andNusaTransla-
lingual few-shot learning on unseen languages. In tion(Cahyawijayaetal.,2023)datasetsinTable8
Proceedingsofthe2ndConferenceoftheAsia-Pacific
andTable9,respectively. Thelanguagecodefol-
Chapter of the Association for Computational Lin-
lows ∗ISO639-3 coding. All languages are also
guisticsandthe12thInternationalJointConference
onNaturalLanguageProcessing(Volume1: Long complemented by their †rarity taxonomy based
Papers),pages777–791. on (Joshi et al., 2021) into two vitality classes:
0-2→low resource language (LRL), and 3→mid
Genta Indra Winata, Ruochen Zhang, and David Ife-
oluwa Adelani. 2024. Miners: Multilingual lan- resourcelanguage(MRL).Wealsoprovideinfor-
guagemodelsassemanticretrievers. arXivpreprint mationaboutwhetherthelanguagewaspartofthe
arXiv:2406.07424.
pretrainedM2M100modeldatasettohighlightthe
Mengzhou Xia, Antonios Anastasopoulos, Ruochen modelknowledgecoverage.
Xu,YimingYang,andGrahamNeubig.2020a. Pre-
dictingperformancefornaturallanguageprocessing A.2 Models
tasks. InProceedingsofthe58thAnnualMeetingof
Here are the details on the proxy LMs we use in
theAssociationforComputationalLinguistics,pages
theexperimentsasfollows:
8625–8646.
Mengzhou Xia, Antonios Anastasopoulos, Ruochen • Transformer(100M)(Vaswanietal.,2017): a
Xu,YimingYang,andGrahamNeubig.2020b. Pre- standardencoder-decodertransformer-based
dictingperformancefornaturallanguageprocessing model with 6 encoder layers and 6 decoder
tasks. InProceedingsofthe58thAnnualMeetingof
layerswithanembeddingdimensionof512.
theAssociationforComputationalLinguistics,pages
Wetrainthemodelfromrandomlyinitialized
8625–8646,Online.AssociationforComputational
Linguistics. parameterswiththetrainingset.Datasets LanguagesUnderStudy Domain
MT560(Gowdaetal.,2021)
JoshuaIndianCorpus tam Wiki
NeulabTEDTalk(Qietal.,2018) bel,hye,kat,kaz,mar,mya,tam TED
NewsCommentary kaz Economics
OPUS100(Zhangetal.,2020) afr,amh,bel,cym,guj,hau,hye,ibo,ind,kan, Multi-domain
kat,kaz,khm,kin,kir,mar,mya,oci,pan,sin,
tam,tat,tuk,xho,yor,zul
OPUSBible afr,amh,ceb,dik,ewe,guj,hye,ind,kan,mar, Religion
(ChristodouloupoulosandSteedman,2015) mri,mya,plt,sin,sna,som,ssw,tam,tgl,wol,
xho,zul
OPUSOpenSubtitles afr,hye,kat,sin,tgl Movies
OPUSTatoeba(Tiedemann,2012) afr,amh,arz,bak,bel,ceb,fao,hau,hye,ibo, Conversational
ind,jav,kan,kat,kaz,khm,kin,kir,lmo,ltz,
mar,mri,mya,pan,sna,tat,tgl,tuk,yor,zul
OPUSTanzil(Tiedemann,2012) amh,hau,ind,snd,som,tat Religion
OPUSGnome(Tiedemann,2012) fao,mri,som Technology
OPUSGlobalVoices(Tiedemann,2012) amh,mya News
OPUSEUBookshop(Tiedemann,2012) cym Government
OPUSSPC afr Government
OPUSMemat xho Medicine
OPUSXhosaNavy xho Government
OPUSKDE4 hne Technology
OPUSinfopankki som Immigration
PMIndia(HaddowandKirefu,2020) guj,kan,mar,pan,sin,tam Government
WikiTitles guj,kaz,tam Wiki
WikiMatrix(Schwenketal.,2021) arz,bak,bel,ceb,fao,ind,jav,kat,kaz,lmo, Wiki
ltz,mar,oci,sin,tam,tat,tgl
FLoRes(Costa-jussàetal.,2022b) afr,amh,arz,bak,bel,ceb,cym,dik,ewe,fao, Multi-domain
guj,hau,hne,hye,ibo,ind,jav,kan,kat,kaz,
khm,kin,kir,lmo,ltz,mar,mri,mya,oci,pan,
plt,sin,sna,snd,som,ssw,tam,tat,tgl,tuk,
wol,xho,yor,zul
NusaTranslation(Cahyawijayaetal.,2023) bew,btk,ind,jav,mad,mak,min,sun SocialMedia
Table7: Listofdatasetsunderstudycovering50differentlanguages. Weonlyoptfor44outof500languages
availableinMT560and8outof12languagesavailableinNusaTranslation.
• SMaLL-100(330M)(Mohammadshahietal., apre-trainedestimatedmodelofNLLB-200
2022):2 a distilled version of the M2M100 Distilled(1.3B)withoutanyfine-tuning. We
(12B) model. We utilize the model in two runthemodelinazero-shotfashion.
ways: fine-tuned on training data and zero-
shotinference.
A.3 Hyper-parameters
• M2M100(NoFT)(Fanetal.,2021):3 apre- LM Eachfine-tuningandevaluationforLMsis
trained estimated model of M2M100 (1.2B) done with an NVIDIA Tesla V100 32GB GPU.
withoutanyfine-tuning. Werunthemodelin Thehyper-parametersusedduringfine-tuningfrom
azero-shotfashion. theMT560(Gowdaetal.,2021)andNusaTransla-
tion(Cahyawijayaetal.,2023)datasetsarelistedin
• NLLB(NoFT)(Costa-jussàetal.,2022b):4
Table10,11,12,and13forSMaLL100,M2M100,
2SMaLL-100 (330M) is taken from https://github. NLLB,andTransformermodels,respectively.
com/alirezamshi/small100.
3M2M100 (1.2B) is taken from https://github.com/ Regressor Each regressor is trained on an
facebookresearch/fairseq/tree/main/examples/m2m_ AMD Ryzen Threadripper 2990WX with 128
100.
GB of RAM and 16 threads. Regressors’ hyper-
4NLLB (1.3B) is taken from https://github.com/
facebookresearch/fairseq/tree/nllb. parametersusedareprovidedinTable14,15,16,Language LanguageCode∗ Family JoshiClass† Vitality† SeenbyM2M100
Afrikaans afr indo-european 3 MRL ✓
Amharic amh afro-asiatic 2 LRL ✓
Armenian hye indo-european 1 LRL ✓
Bashkir bak turkic 1 LRL ✓
Belarusian bel indo-european 3 MRL ✓
Burmese mya sino-tibetan 1 LRL ✓
Cebuano ceb austronesian 3 MRL ✓
Chhattisgarhi hne indo-european 0 LRL ✗
Dinka dik nilo-saharan 1 LRL ✗
EgyptianArabic arz afro-asiatic 3 MRL ✓
Ewe ewe niger-congo 1 LRL ✗
Faroese fao indo-european 1 LRL ✗
Georgian kat kartvelian 3 MRL ✓
Gujarati guj indo-european 1 LRL ✓
Hausa hau afro-asiatic 2 LRL ✓
Igbo ibo niger-congo 1 LRL ✓
Indonesian ind austronesian 3 MRL ✓
Javanese jav austronesian 1 LRL ✓
Kannada kan dravidian 1 LRL ✓
Kazakh kaz turkic 3 MRL ✓
Khmer khm austro-asiatic 1 LRL ✓
Kirghiz kir turkic 1 LRL ✗
Kinyarwanda kin niger-congo 1 LRL ✗
Lombard lmo indo-european 1 LRL ✗
Luxembourgish ltz indo-european 1 LRL ✓
Malagasy plt austronesian 1 LRL ✓
Maori mri austronesian 1 LRL ✗
Marathi mar indo-european 2 LRL ✓
Occitan oci indo-european 1 LRL ✓
Punjabi pan indo-european 2 LRL ✓
Shona sna niger-congo 1 LRL ✗
Sindhi snd indo-european 1 LRL ✓
Sinhala sin indo-european 0 LRL ✓
Somali som afro-asiatic 1 LRL ✓
Swati ssw niger-congo 1 LRL ✓
Tagalog tgl austronesian 3 MRL ✓
Tamil tam dravidian 3 MRL ✓
Tatar tat turkic 1 LRL ✗
Turkmen tuk turkic 1 LRL ✗
Welsh cym indo-european 1 LRL ✓
Wolof wol niger-congo 2 LRL ✓
Xhosa xho niger-congo 2 LRL ✓
Yoruba yor niger-congo 2 LRL ✓
Zulu zul niger-congo 2 LRL ✓
Table 8: List of languages from the MT560 dataset, including their rarity category mapping and an indication
ofwhethertheyareinvolvedinthepretrainingprocessforM2M100. Notethatalllanguagesareinvolvedinthe
pretrainingprocessforNLLB.
and17forXGB,Poly2/Poly3,LGBM,andMF,re- multiple language groupings in Figure 5 - 10 for
spectively. Thesehyper-parameterswereobtained English-centricresult,andFigure11-14forMany-
basedonthebestcross-validationRMSEscoreus- to-ManyLanguagesresult. Eachlanguagegroup-
ing10folds. ingsplotcomprisesmultiplesubplots,including(a)
vitalityclass,(b)Joshiclass,(c)languagefamily,
B MoreDetailedResults and(d)individuallanguages. Themappingofvi-
tality,Joshiclass,andlanguagefamilyfollowsthe
We provide detailed visualizations of the results
classificationsinTable8and9.
ofXGBoostwithPROXYLM EnsemblebasedonLanguage LanguageCode∗ Family JoshiClass† Vitality† SeenbyM2M100 SeenbyNLLB
Indonesian ind austronesian 3 MRL ✓ ✓
Javanese jav austronesian 1 LRL ✓ ✓
Betawi bew creole 0 LRL ✗ ✗
Batak btk austronesian 0 LRL ✗ ✗
Madurese mad austronesian 0 LRL ✗ ✗
Makassarese mak austronesian 0 LRL ✗ ✗
Minangkabau min austronesian 0 LRL ✗ ✓
Sundanese sun austronesian 1 LRL ✓ ✓
Table9: ListoflanguagesfromtheNusaTranslationdatasetalongwiththeirraritycategorymappingandindication
ofwhethertheyareincludedinthepretrainingprocessforeachrespectivemodel.
Hyper-parameter MT560 NusaTranslation
EncoderLayers 12 12
DecoderLayers 3 3
EncoderEmbedDim 1024 1024
DecoderEmbedDim 1024 1024
EncoderFFNEmbedDim 4096 4096
DecoderFFNEmbedDim 4096 4096
EncoderAttentionHeads 16 16
DecoderAttentionHeads 16 16
EncoderLayerdrop 0.05 0.05
DecoderLayerdrop 0.05 0.05 Hyper-parameter MT560 NusaTranslation
Optimizer Adam Adam
EncoderLayers 24 24
AdamEps 1e-6 1e-6
DecoderLayers 24 24
AdamBetas (0.9,0.98) (0.9,0.98)
EncoderEmbedDim 1024 1024
Patience 6 6
DecoderEmbedDim 1024 1024
BatchSize 16 16
EncoderFFNEmbedDim 8192 8192
Dropout 0.1 0.1
DecoderFFNEmbedDim 8192 8192
AttentionDropout 0.1 0.1
EncoderAttentionHeads 16 16
ReLUDropout 0.1 0.1
DecoderAttentionHeads 16 16
WeightDecay 0.0 0.0
EncoderLayerdrop 0.05 0.05
LabelSmoothing 0.1 0.1
DecoderLayerdrop 0.05 0.05
ClipNorm 1.0 1.0
Optimizer Adam Adam
LearningRate 0.0001 0.0003
AdamEps 1e-6 1e-6
MaxTokens(perGPU) 1,000 1,000
AdamBetas (0.9,0.98) (0.9,0.98)
Patience 6 6
Table10: Listofhyper-parametersusedforSMaLL100
BatchSize 32 32
withMT560andNusaTranslationdatasets.
Dropout 0.1 0.1
AttentionDropout 0.1 0.1
ReLUDropout 0.1 0.1
C FurtherAnalysis
WeightDecay 0.0 0.0
LabelSmoothing 0.1 0.1
C.1 ModelFeatureImportance
ClipNorm 0.0 0.0
WeprovidefeatureimportancescoresofXGBoost LearningRate 0.0002 0.0002
MaxTokens(perGPU) 1,792 1,792
withPROXYLMEnsemblefortherandomMT560
experimentinFigure15and16. Eachcombination
Table11: Listofhyper-parametersusedforM2M100
consists of one most influential feature followed withMT560andNusaTranslationdatasets.
byotherswithmarginalcontributionstothemodel,
eachwithanimportancescoreof0.12orless. We
observe that proxy models are always the most
influentialfeaturesinprediction.(a) (b) (c)
Figure5: DetailedresultsofXGBoostwithPROXYLMEnsembleonNLLBmodelundertheLOLOsettingusing
theMT560datasetfromTable1. Theresultsaregroupedby(a)JoshiClassand(b)languagefamilythatfollowsthe
mappingwhichisprovidedinAppendixA.1;(c)showsthescatterplotillustratingthecorrelationofspBLEUscores
betweenthePROXYLM’spredictionandestimatedLM,withthegraydashedlinerepresentingthelineofequality
(y =x)withR2 =0.89andblackdashedlinerepresentingLocallyWeightedScatterplotSmoothing(LOWESS)
curvetorepresentthetrend.
Hyper-parameter MT560 NusaTranslation Hyper-parameter MT560 NusaTranslation
EncoderLayers 24 24 EncoderLayers 6 6
DecoderLayers 24 24 DecoderLayers 6 6
EncoderEmbedDim 1024 1024 EncoderEmbedDim 512 512
DecoderEmbedDim 1024 1024 DecoderEmbedDim 512 512
EncoderFFNEmbedDim 8192 8192 EncoderFFNEmbedDim 2048 2048
DecoderFFNEmbedDim 8192 8192 DecoderFFNEmbedDim 2048 2048
EncoderAttentionHeads 16 16 EncoderAttentionHeads 8 8
DecoderAttentionHeads 16 16 DecoderAttentionHeads 8 8
EncoderLayerdrop 0.05 0.05 EncoderLayerdrop 0.05 0.05
DecoderLayerdrop 0.05 0.05 DecoderLayerdrop 0.05 0.05
Optimizer Adam Adam Optimizer Adam Adam
AdamEps 1e-6 1e-6 AdamEps 1e-6 1e-6
AdamBetas (0.9,0.98) (0.9,0.98) AdamBetas (0.9,0.98) (0.9,0.98)
Patience 6 6 Patience 6 6
BatchSize 32 32 BatchSize 32 32
Dropout 0.1 0.1 Dropout 0.1 0.1
AttentionDropout 0.1 0.1 AttentionDropout 0.1 0.1
ReLUDropout 0.0 0.0 ReLUDropout 0.1 0.1
WeightDecay 0.01 0.01 WeightDecay 0.0001 0.0001
LabelSmoothing 0.1 0.1 LabelSmoothing 0.1 0.1
ClipNorm 1.0 1.0 ClipNorm 0 0
LearningRate 0.00002 0.0001 LearningRate 0.001 0.0005
MaxTokens(perGPU) 1,000 1,000 MaxTokens(perGPU) 1,000 1,000
Table12: Listofhyper-parametersusedforNLLBwith Table13:Listofhyper-parametersusedforTransformer
MT560andNusaTranslationdatasets. withMT560andNusaTranslationdatasets.Hyper-parameter NusaTranslation MT560
M2M100 NLLB M2M100 NLLB
maxn_estimators 2000 2000 5000 5000
maxeta 0.1 0.1 0.1 0.1
Hyper-parameter Specification
min_child_weight 5 2.5 3 3.5
max_depth 3 3 5 4 maxalpha 0.01
gamma 0 0 0.2 0.1 beta_w 0.1
subsample 0.7 0.9 0.95 0.8 beta_h 0.1
colsample_bytree 0.6 0.6 0.85 1.0 beta_z 0.01
reg_alpha 0 0 0.1 0.1 beta_s 0.01
reg_lambda 0.35 0.15 0.1 0.3 beta_t 0.01
lr_decay 0.001
Table14: Listofhyper-parametersusedforXGBoost iterations 2000
Regressor with M2M100 and NLLB models trained
withMT560andNusaTranslationdatasets. Table 17: List of hyper-parameters used for MF Re-
gressorwithM2M100andNLLBmodelstrainedwith
MT560andNusaTranslationdatasets. “Max"indicates
themaximumvaluesetforthehyper-parameterduring
thehyper-parametersearch.
Hyper-parameter Value
alpha 0.1
l1_ratio 0.9
Table15:Listofhyper-parametersusedforPoly2/Poly3
Regressor with M2M100 and NLLB models trained
withMT560andNusaTranslationdatasets.
(a) (b)
Hyper-parameter Value
maxlearning_rate 0.3
maxnum_leaves 64
n_estimators 100
max_bin 200000
max_depth 10
min_child_weight 0.001
min_child_samples 20
min_split_gain 0.0
colsample_bytree 1.0
subsample 1.0
reg_alpha 0.1 Figure 6: Detailed results of XGBoost with PROX-
reg_lambda 0.1 YLM Ensemble under the LOLO setting using the
MT560datasetfromTable1,groupedbyvitality,for
Table16: Listofhyper-parametersusedforLGBMRe- (a)M2M100and(b)NLLBmodel. Vitalityisoutlined
gressorwithM2M100andNLLBmodelstrainedwith inTable8.
MT560andNusaTranslationdatasets. “Max"indicates
themaximumvaluesetforthehyper-parameterduring
thehyper-parametersearch.Figure7: DetailedresultsofXGBoostwithPROXYLMEnsembleontheM2M100modelundertheLOLOsetting
usingtheMT560datasetfromTable1,categorizedbylanguage.
Figure8: PerformanceheatmapofXGBoostwithPROXYLMEnsembleontheM2M100modelundertheLOLO
settingusingtheMT560datasetfromTable1. Theheatmapshowcasestheperformancebasedonvitalityleveland
languagefamily,asoutlinedinTable8.Figure9: DetailedresultsofXGBoostwithPROXYLMEnsembleontheNLLBmodelundertheLOLOsetting
usingtheMT560datasetfromTable1,categorizedbylanguage.
Figure10: PerformanceheatmapofXGBoostwithPROXYLMEnsembleontheNLLBmodelundertheLOLO
settingusingtheMT560datasetfromTable1. Theheatmapillustratesthemodel’sperformancebasedonvitality
levelsandlanguagefamilyclassificationsasspecifiedinTable8.(a) (b) (c)
Figure11: DetailedresultsofXGBoostwithPROXYLMEnsembleontheM2M100modelundertheLOLOsetting
usingtheNusaTranslationdatasetfromTable3,organizedbylanguagegroupings. Themappingof(a)Vitality,(b)
JoshiClass,and(c)languagefamilyalignwiththeclassificationsprovidedinTable9.
Figure12: DetailedresultsofXGBoostwithPROXYLMEnsembleontheM2M100modelundertheLOLOsetting
usingtheNusaTranslationdatasetfromTable3,categorizedbylanguage.(a) (b) (c)
Figure13: DetailedresultsofXGBoostwithPROXYLMEnsembleontheNLLBmodelundertheLOLOsetting
usingtheNusaTranslationdatasetfromTable3,organizedbylanguagegroups. Themappingof(a)Vitality,(b)
JoshiClass,and(c)languagefamilycorrespondstotheclassificationsprovidedinTable9.
Figure14: DetailedresultsofXGBoostwithPROXYLMEnsembleontheNLLBmodelundertheLOLOsetting
usingtheNusaTranslationdatasetfromTable3,categorizedbylanguage.Figure15: FeatureimportanceanalysisofXGBoostwithPROXYLMEnsembleontheM2M100modelusingthe
MT560dataset.Figure16: FeatureimportanceanalysisofXGBoostwith PROXYLM EnsembleontheNLLBmodelusingthe
MT560dataset.