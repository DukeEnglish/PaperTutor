Advancing Graph Generation through Beta Diffusion
Yilin He1,∗ , Xinyang Liu2,*, Bo Chen2, and Mingyuan Zhou1
yilin.he@mccombs.utexas.edu, xinyangatk@gmail.com
bchen@mail.xidian.edu.cn, mingyuan.zhou@mccombs.utexas.edu
1The University of Texas at Austin, 2Xidian University
Abstract
Diffusionmodelshavedemonstratedeffectivenessingeneratingnaturalimagesandhavebeenextended
to generate diverse data types, including graphs. This new generation of diffusion-based graph generative
models has demonstrated significant performance improvements over methods that rely on variational
autoencoders or generative adversarial networks. It’s important to recognize, however, that most of these
models employ Gaussian or categorical diffusion processes, which can struggle with sparse and long-tailed
data distributions. In our work, we introduce Graph Beta Diffusion (GBD), a diffusion-based generative
model particularly adept at capturing diverse graph structures. GBD utilizes a beta diffusion process,
tailored for the sparse and range-bounded characteristics of graph adjacency matrices. Furthermore, we
havedevelopedamodulationtechniquethatenhancestherealismofthegeneratedgraphsbystabilizingthe
generationofcriticalgraphstructures,whilepreservingflexibilityelsewhere. Theoutstandingperformance
of GBD across three general graph benchmarks and two biochemical graph benchmarks highlights its
capabilitytoeffectivelycapturethecomplexitiesofreal-worldgraphdata. Thecodewillbemadeavailable
on GitHub. The code will be made available at https://github.com/YH-UtMSB/Graph_Beta_Diffusion.
1 Introduction
In recent years, the field of machine learning-driven graph generation has witnessed a surge in interest and
activity. This growing attention is driven by the recognition of graph data’s pervasive presence and utility
across diverse real-world applications, ranging from social network study [Newman et al., 2002, Conti et al.,
2011, Abbe, 2018, Shirzadian et al., 2023] to biochemical molecular research [Jin et al., 2018, Liu et al., 2018,
Bongini et al., 2021, Guo et al., 2024, Li and Yamanishi, 2024]. Additionally, the rapid evolution of machine
learning tools has introduced powerful techniques for data generation, among which diffusion models [Ho
et al., 2020, Song et al., 2021, Austin et al., 2021, Avdeyev et al., 2023, Chen and Zhou, 2023, Zhou et al.,
2023] stand out as a notable example. As these advanced tools intersect with the task of graph generation,
we witness the emergence of numerous diffusion-based graph generative models [Niu et al., 2020a, Jo et al.,
2022, Haefeli et al., 2022, Huang et al., 2022, Vignac et al., 2023, Jo et al., 2023, Cho et al., 2023, Chen et al.,
2023, Kong et al., 2023].
While diffusion-based graph generative models often demonstrate superior performance compared to their
predecessors [You et al., 2018, De Cao and Kipf, 2018, Li et al., 2018, Simonovsky and Komodakis, 2018,
Liu et al., 2019, Shi et al., 2020, Luo et al., 2021, Martinkus et al., 2022], there is still potential for further
enhancement in the quality of generated graphs. Among the latest advancements in these methods, it is
widely recognized that incorporating inductive bias from the graph data is generally beneficial for model
design [Jo et al., 2023]. One promising direction of incorporating this bias involves considering the statistical
∗Thefirsttwoauthorscontributeequally.
1
4202
nuJ
31
]GL.sc[
1v75390.6042:viXracharacteristics of the distribution of graph data. For instance, both Graph D3PM [Haefeli et al., 2022] and
DiGress [Vignac et al., 2023] have demonstrated that when considering the binary or categorical nature of
the graph adjacency matrix and modeling it in the discrete space, it provides benefits for generating more
realistic graphs.
Accounting for the discreteness of the graph adjacency matrix has shown enhancement to model perfor-
mance. However, it is crucial to recognize that the complexity and flexibility of distribution characteristics of
graph data extend beyond mere discreteness. Real-world graphs typically exhibit sparsity in edge distribution
and long-tailedness in nodal statistical properties such as degrees and categories [Barabási et al., 2000, Ciotti
et al., 2015, Liang et al., 2023, Wang et al., 2023]. Moreover, when mapping the graph structure into an
adjacency matrix, the values within the matrix are also bounded by the range of edge weights. Given these
uniquecharacteristicsinherenttographdata,itbecomesapparentthatGaussianandcategoricaldistributions,
which are typically the default choices for building the diffusion processes, may not align well with these
graph characteristics. Consequently, this mismatch could potentially introduce limitations when modeling
the distribution over graphs.
Considering the desirable statistical characteristics of graph data, we find that the beta distribution
emerges as a particularly suitable modeling choice. With properties such as being range-bounded and flexible
to model data at all sparsity levels, the beta distribution aligns well with the inherent traits of graphs, hence
making itself a promising candidate to surpass the potential limitations imposed by utilizing Gaussian or
categorical distributions. In this paper, we introduce Graph Beta Diffusion (GBD) as a novel addition to
diffusion-based graph generative models. GBD models the joint distribution of node attributes and edge
connections within a graph through beta diffusion [Zhou et al., 2023], a generative diffusion process developed
upon the thinning and thickening of beta random variables. In the forward diffusion process, the original
data is gradually attenuated toward zero. This process starts with a modified form of the data, which is
scaled and shifted to ensure it is bounded between 0 and 1. At each step, a random proportion, sampled from
a beta distribution with a predefined noise schedule, is multiplied with the attenuated data. Conversely, the
reverse diffusion process starts from zero and attempts to recover the original data. It does this by adding a
random proportion of the difference between one and the current value to the current value at each step.
We underscore two major contributions arising from the development of GBD. First, our experiments
generating data on various synthetic and real-world graphs confirm the effectiveness of beta diffusion as a
strategic choice within the design framework of the backbone diffusion model, especially for graph generation
tasks. Second, our exploration of the model’s design space has yielded a set of recommended practices,
notably a novel modulation technique that bolsters the stability of generating essential graph structures.
We demonstrate that these practices, when implemented together, lead to consistent enhancements in
model performance.
2 The Methodology
2.1 Data description and mathematical notations
In this study, our primary focus lies in generating two types of graphs: generic graphs and molecular graphs.
Generic graphs are characterized as undirected, simple graphs with N nodes. Each pair of nodes (u,v) in
these graphs can only have two possible edge statuses: connected or disconnected. Therefore, the entire graph
structure can be fully described by a symmetric binary adjacency matrix A∈{0,1}n×n, where “1”s denote
connected node pairs and “0”s denote pairs that are not directly connected. Molecular graphs are also simple
graphs, but they typically feature multiple types of edges. As a result, their graph structures need to be
represented as adjacency matrices with dummy-encoded categorical variable elements, which is a common
practice established in previous research [Jo et al., 2022, Vignac et al., 2023, Jo et al., 2023]. This expression
results in a multitude of channels in the graph adjacency matrix, we use A(1:K) to represent the structure of
a graph with K types of edges.
2𝑮 =𝑮 ⊙𝑸
! !"# !
𝑮 𝑮 𝑮
! # 𝑮 !"#=𝑮 !+𝑷 !⊙(𝟏−𝑮 !) "
𝐺"
forward process !
reverse process
𝑸 #,𝑷 # Beta variable minimize 𝓛 𝒔𝒂𝒎𝒑𝒍𝒊𝒏𝒈 and𝓛 𝒄𝒐𝒓𝒓𝒆𝒄𝒕𝒊𝒐𝒏
Figure1: OverviewofGBD.ThebetarandomvariablesQ andP aresampledfromthebetadistributionparameterized
t t
by G and clean graph predicted by the neural network G˜ , respectively. G˜ is learned through minimizing L
0 θ θ sampling
and L defined in Section 2.3.
correction
While the primary goal of graph generation and quality evaluation centers around graph architecture,
incorporating the node features can be advantageous for learning the distribution of the adjacency matrices.
Node features are typically represented by a matrix X of shape n×d, where d denotes the number of features.
The choices of node features offers high flexibility, ranging from raw-data-provided node categories to some
hand-crafted features, such as node-level statistics [Jo et al., 2022] or spectral graph signals [Jo et al., 2023].
The features within X exhibit great diversity in their nature, including numerical, categorical, and ordinal
types. Through preprocessing methods including dummy-encoding and empirical CDF transformation, we
standardize them as continuous variables bounded by [0,1]. In this section, we denote the target datum as G.
For generic graphs, G comprises (A,X); for molecular graphs, G is defined as (A(1:K),X). In the sequel, We
by default employ the generic graph scenario to illustrate the methodology.
2.2 Forward and reverse beta diffusion processes
Forward beta diffusion process. Such a process can be characterized by the transition probability
q(G |G ,G ), with G denoting the combination of the original adjacency matrix and node feature
t t−1 0 0
matrix. Following recent diffusion-based graph generative models [Jo et al., 2022, Vignac et al., 2023, Jo
et al., 2023, Cho et al., 2023], we assume q(G |G ,G ) to be factorizable such that q(G |G ,G ) =
t t−1 0 t t−1 0
q(A |A ,A )·q(X |X ,X ). Constructing the forward beta diffusion process [Zhou et al., 2023] for
t t−1 0 t t−1 0
graph modeling, we have:
A =A ⊙Q , Q ∼Beta(η α A ,η (α −α )A ), (1)
t t−1 A,t A,t A t 0 A t−1 t 0
X =X ⊙Q , Q ∼Beta(η α X ,η (α −α )X ), t∈[1,T]. (2)
t t−1 X,t X,t X t 0 X t−1 t 0
Hereη ,η arepositivescalarsadjustingtheconcentrationofbetadistributions,withhighervaluesleadingto
A X
enhanced concentration and reduced variability. The diffusion noise schedule is defined with {α | t∈[1,T]},
t
which represent a sequence of values descending from 1 towards 0 as t increases. Elements in the fractional
multiplier Q or Q are independently sampled from their respective beta distributions. With the forward
A,t A,t
diffusion process defined in Equations (1) and (2), we characterize the stochastic transitions of an element g
within G as:
(cid:18) (cid:19)
1 g
q(g | g ,g )= Beta t | ηα g ,η(α −α )g , (3)
t t−1 0 g g t 0 t−1 t 0
t−1 t−1
where depending on whether g is an element in A or X, we have either η = η or η = η . Derived from
A X
Equation (3), the joint distribution q(G |G ) has analytical format in the marginal distribution on each
1:T 0
time stamp t, specifically,
q(G | G )=Beta(ηα G ,η(1−α G )). (4)
t 0 t 0 t 0
3Reverse beta diffusion process. It is important to note that the joint distribution q(G |G ) can
1:T 0
be equivalently constructed in reverse order through ancestral sampling, which directs samples from the
terminus states G towards the initial states G by incrementally applying the changes δG at each reversed
T 0 t
time stamp. With the changes at a given time t parameterized as δG := P ⊙(1−G ), where P are
t t t t
beta-distributed fractional multipliers, the time-reversal sampling process can be mathematically defined as:
for t=T,T −1,··· ,1,
G =G +P ⊙(1−G ), P ∼Beta(η(α −α )G ,η(1−α G )). (5)
t−1 t t t t t−1 t 0 t 0
Similartotheforwardsamplingprocess, wecanderivethetransitiondistributioncorrespondingtothereverse
sampling process described in Equation (5) as following:
(cid:18) (cid:19)
1 G −G
q(G | G ,G )= Beta t−1 t | η(α −α )G ,η(1−α G ) . (6)
t−1 t 0 1−G 1−G t−1 t 0 t−1 0
t t
Following previous work [Austin et al., 2021, Haefeli et al., 2022, Vignac et al., 2023, Zhou et al., 2023], we
construct the reverse diffusion process through the definition of ancestral sampling distribution as following:
p (G | G ):=q(G | G ,Gˆ (G ,t)), (7)
θ t−1 t t−1 t θ t
where Gˆ (G ,t) is a neural network that predicts the conditional expectation of G given G . Following
θ t 0 t
Vignac et al. [2023], we instantiate Gˆ (G ,t) as a graph transformer network [Dwivedi and Bresson, 2020a].
θ t
We present the complete sampling process in Algorithms 1 and 2.
Algorithm 1 Sampling, in original domain. Algorithm 2 Sampling, in logit domain
Require: Number of timesteps T = 1000, default 1: Sample logit(G T)∼p(logit(A T),logit(X T))
concentration parameter η=30, predictor Gˆ θ. 2: for t=1 to 1 do
1: (Optional) Assign value to η via Equation (16) 3: G in = logi√t(Gt)−E[logit(Gt)]
3 42 : :: fS oa rm Gtp = il ne TG =T t G√o=
t
V1
−
a(
E
rdA
[[
GGoT
tt
]],X T)∼p(A T,X T) 4 65:
::
(
XA
ˆˆAˆ
0
0′ 0
←
←,Xˆ
w
w′ 0)
A
X←
· ·A
XˆV ˆGˆa
′ 0
′
0r θ[
+
+l (o Gg
b
bit Ai X( nG ,t t) )]
5: (Aˆ′ 0,Xˆ′ 0)←Gˆ θ(G in,t) 7: Gˆ 0 ←(Aˆ 0,Xˆ 0)
6: Aˆ 0 ←w A·Aˆ′ 0+b A 8: U t ∼Gamma(η(α t−1−α t)Gˆ 0,1)
7: Xˆ 0 ←w X ·Xˆ′ 0+b X 9: V t ∼Gamma(η(1−α t−1Gˆ 0),1)
8: Gˆ 0 ←(Aˆ 0,Xˆ 0) 10: logit(P t)←lnU t−lnV t
9: P t ∼Beta(η(α t−1−α t)Gˆ 0,η(1−α t−1Gˆ 0)) 11: Obtain logit(G t−1) from Equation (17)
10: G t−1 ←G t+P t⊙(1−G t) 12: end for
11: end for 13: G 0 ←sigmoid(logit(G 0))
12: return (A 0−b A)/w A and (X 0−b X)/w X 14: return (A 0−b A)/w A and (X 0−b X)/w X
2.3 Training GBD
The overall training procedure of GBD is described in Algorithms 3 and 4. We employ the objective function
proposed by beta diffusion [Zhou et al., 2023], specifically,
(cid:88)T
L= (1−ω)L (t,G )+ω L (t,G ), ω ∈[0,1]. (8)
sampling 0 correction t
t=2
In Equation (8), the loss terms associated with sampling and correction are defined as
L (t,G )=∆ E KL(p (G | G )∥q(G | G ,G )), (9)
sampling 0 q(Gt,G0) θ t−1 t t−1 t 0
(cid:16) (cid:17)
L (t,G )=∆ E KL q(G | Gˆ (G ,t))∥q(G | G ) . (10)
correction 0 q(Gt,G0) τ θ t τ 0
4In Equation (10), the KL divergence is evaluated between the following distributions: q(G | Gˆ (G ,t))
τ θ t
is Beta(ηα Gˆ (G ,t),η(1−α Gˆ (G ,t))), and q(G | G ) is the same as q(G | G ) in distribution. The
t θ t t θ t τ 0 t 0
subscriptτ isintroducedtorepresentagenericgraphsampleotherthanG thatisalsoobtainedattimetfrom
t
the forward diffusion process. The core principle behind the loss function terms can be described as follows:
L drives the empirical ancestral sampling distribution towards the destination-conditional posterior
sampling
distribution, while L corrects the bias on marginal distribution at each time stamp accumulated
correction
through the ancestral sampling. These two types of loss terms collectively reduce the divergence between
the empirical joint distribution on two graphs sampled from adjacent time stamps in the reverse process,
and their joint distribution derived from the forward diffusion process. A positive weight ω is introduced to
balance the effects of these two types of loss terms. We set it to 0.01, following Zhou et al. [2023], and found
that this configuration is sufficient to produce graphs that closely resemble the reference graphs without
further tuning. To better elucidate the optimization objective, we list out the following analytical expressions:
KL(p (G | G )∥q(G | G ,G ))
θ t−1 t t−1 t 0
=lnΓ(η(α −α )G )+lnΓ(η−ηα G )+lnΓ(η−ηα G )
t−1 t 0 t−1 0 t 0
−lnΓ(η(α −α )Gˆ )−lnΓ(η−ηα Gˆ )−lnΓ(η−ηα Gˆ )
t−1 t 0 t−1 0 t 0
+η(α −α )(Gˆ −G )·ψ(η(α −α )Gˆ ) (11)
t−1 t 0 0 t−1 t 0
+ηα (G −Gˆ )·ψ(η−ηα Gˆ )
t−1 0 0 t−1 0
+ηα (G −Gˆ )·ψ(η−ηα Gˆ ),
t 0 0 t 0
(cid:16) (cid:17)
KL q(G | Gˆ (G ,t))∥q(G | G )
τ θ t τ 0
=lnΓ(ηα G )+lnΓ(η−ηα G )−lnΓ(ηα Gˆ )−lnΓ(η−ηα Gˆ ) (12)
t 0 t 0 t 0 t 0
(cid:16) (cid:17)
+ηα (Gˆ −G )· ψ(ηα Gˆ )−ψ(η−ηα Gˆ ) ,
t 0 0 t 0 t 0
where Gˆ :=Gˆ (G ,t).
0 θ t
It is demonstrated in Zhou et al. [2023] that the KL divergence between two beta distributions can
∆
be expressed in the format of a Bregman divergence. Namely, considering a convex function ϕ(α,β) =
lnBeta(α,β), where Beta(α,β)= Γ(α)Γ(β) is the beta function, the loss term L can be expressed as
Γ(α+β) sampling
L (t,G )=E E d (cid:0) [a ,b ],[a∗ ,b∗ ](cid:1) ,
sampling 0 q(Gt) q(G0|Gt) ϕ sampling sampling sampling sampling
a =η(α −α )G , b =η(1−α G ), (13)
sampling t−1 t 0 sampling t−1 0
a∗ =η(α −α )Gˆ , b∗ =η(1−α Gˆ ).
sampling t−1 t 0 sampling t−1 0
Likewise, we can express the correction loss term L as
correction
L (t,G )=E )E d ([a ,b ],[a∗ ,b∗ ]),
correction 0 q(Gt q(G0|Gt) ϕ correction correction correction correction
a =ηα G , b =η(1−α G ), (14)
correction t 0 correction t 0
a∗ =ηα Gˆ , b∗ =η(1−α Gˆ ).
correction t 0 correction t 0
Here we reference the d notation of Banerjee et al. [2005] to represent the Bregman divergence. As stated
ϕ
in Lemmas 3-5 of Zhou et al. [2023], one can apply Proposition 1 of Banerjee et al. [2005] to show that
both L and L yield the same optimal solution that legitimates the usage of Gˆ in the reverse
sampling correction 0
diffusion process.
Property 1. Both L and L are uniquely minimized at
sampling correction
Gˆ =Gˆ (G ,t)=E [G ].
0 θ t q(G0|Gt) 0
5Algorithm 3 Training, in original domain. Algorithm 4 Training, in logit domain.
Require: Number of timesteps T = 1000, default Require: Number of timesteps T, concentration pa-
concentration parameter η = 30, predictor Gˆ , rameter η, predictor Gˆ , node influence factor γ,
θ θ
defaultnodeinfluencefactorγ =0.5,inputgraph inputgraphbatchB,learningrateλ,optimization
batch B = {G(k) = (A(k),X(k))} , default steps M. Same default values with Algorithm 3.
[K]
learning rate λ=0.002, optimization steps M. 1: for step =1 to M do
1: (Optional) Assign value to η via Equation (16) 2: Initialize L X and L A with 0
2: for step =1 to M do 3: for k=1 to K do
3: Initialize L X and L A with 0 4: t∼Unif(1,...,T)
4: for k=1 to K do 5: α t,α t−1 ←schedule(t),schedule(t−1)
5: t∼Unif(1,...,T) 6: A 0 ←w A·A(k)+b A
6: α t,α t−1 ←schedule(t),schedule(t−1) 7: X 0 ←w X ·X(k)+b X
7: A 0 ←w A·A(k)+b A 8: G 0 ←(A 0,X 0)
8: X 0 ←w X ·X(k)+b X 9: U t ∼Gamma(ηα tG 0,1)
9: G 0 ←(A 0,X 0) 10: V t ∼Gamma(η(1−α tG 0),1)
10: G t ∼Beta(ηα tG 0,η(1−α tG 0)) 11: logit(G t)←lnU t−lnV t
11: G in ←
G√t−E[Gt]
12: G in ←
logi√t(Gt)−E[logit(Gt)]
Var[Gt] Var[logit(Gt)]
12: (Aˆ′ 0,Xˆ′ 0)←Gˆ θ(G in,t) 13: (Aˆ′ 0,Xˆ′ 0)←Gˆ θ(G in,t)
13: Aˆ 0 =w A·Aˆ′ 0+b A 14: Aˆ 0 =w A·Aˆ′ 0+b A
14: Xˆ 0 =w X ·Xˆ′ 0+b X 15: Xˆ 0 =w X ·Xˆ′ 0+b X
15: L A ←L A+L(A 0,Aˆ 0,η,α t,α t−1) 16: L A ←L A+L(A 0,Aˆ 0,η,α t,α t−1)
16: L X ←L X +L(X 0,Xˆ 0,η,α t,α t−1) 17: L X ←L X +L(X 0,Xˆ 0,η,α t,α t−1)
17: end for 18: end for
18: θ←θ− Kλ ∇ θ(L A+γL X) 19: θ←θ− Kλ ∇ θ(L A+γL X)
19: end for 20: end for
2.4 Exploring the design space of GBD
Many diffusion-based graph generative models offer great flexibility with technical adjustment to enhance
their practical performances. Here we list four impactful dimensions among the design space of GBD. Namely,
data transformation, concentration modulation, logit-domain computation, and neural-network precondition.
We elaborate each design dimension below and discuss our choices in these aspects in the appendix.
Data transformation. We convert the raw data (A,X) to G through linear transformations, i.e.,
0
G =(A ,X ), where A =w ·A+b , X =w ·X+b , (15)
0 0 0 0 A A 0 X X
with the constraints that min(w ,b ,w ,b )>0 and max(w +b ,w +b )≤1. This operation not only
A A X X A A X X
ensure that all data values fall within the positive support of beta distributions, avoiding gradient explosion
when optimizing the loss function, but also provide an effective means to adjust the rate at which diffusion
trajectories mix. A forward diffusion trajectory reaches a state of “mix” when it becomes indistinguishable to
discern the initial value from its counterfactual given the current value. A suitable mixing rate ensures that
the signal-to-noise ratio (SNR) of the final state in the forward diffusion process approaches zero, meeting
the prerequisite for learning reverse diffusion while preserving the learnability of graph structural patterns.
The scaling parameter provides a macro control for the mixing rate, with a smaller value contracting the
data range and promoting the arrival of the mixing state.
Concentration modulation. Anotherhyperparameterthatoffersamorerefinedadjustmenttothemixing
rate is the concentration parameter η. Higher values of η reduce the variance of the fractional multipliers P
t
sampled from their corresponding beta distributions, thus delaying the arrival of the mixing state. Leveraging
60.05T 0.1T 0.15T 0.25T 0.5T T
Figure2: ReverseProcessonGraphEdges: Eachedge(u,v)undergoesareverseprocesswithspecificη parameters,
u,v
influenced by deg(u) and deg(v). The colorbar indicates the normalized degree of nodes in the final graph, with
brighter colors (shifting towards yellow) denoting higher degrees. Nodes in the adjacency matrices shown in the
bottom row are reordered based on decreasing node degree.
this property, we have devised a simple yet effective modulation strategy to differentiate the mixing time for
various graph substructures.
Specifically, we assign higher η values to “important positions” within a graph, such as edges connecting
high-degree nodes or edges redeemed as significant based on domain knowledge, such as the carbon-carbon
bondinchemicalmolecules. Forinstance,whenmodulatingη fromnodedegrees,theexactoperationexecuted
upon the η values for edge (u,v) and for the features of node u can be mathematically expressed as
η =g (max(deg(u),deg(v))), η =g (deg(u)). (16)
u,v A u X
Here we first prepare several levels of η values, then utilize two assignment functions, namely g (·) and g (·),
A X
to map the node degrees (or their percentile in the degree population) to one of the choices of the η values.
We have observed that this operation indeed prolongs the presence of these substructures during the forward
diffusion process, which in turn leads to their earlier emergence compared to the rest of the graph during the
reverse process.
We visualize the reverse process from two perspectives in Figure 2. We first obtain the η by degrees
u,v
retrieved from the training set before sampling and then generate graph through reverse beta diffusion. From
the top row, we observe that edges linked to nodes with higher degrees (indicated by brighter colors) appear
first, followed by other edges. From the bottom row, it is evident that edges connected to the first five
nodes, which have higher degrees, are identified first and then progressively in descending order of degree.
Notably, the nodes of the adjacency matrices in the bottom row are reordered by decreasing node degree of
the final graph. Additionally, we can also find the predicted graph of GBD converges in an early stage to the
correct topology. We attribute the enhanced quality of generated graphs to the early emergence of these
“important substructures,” which potentially enhances the reliability of generating realistic graph structures.
Furthermore, we find this approach particularly appealing because it allows for the flexible integration of
graph inductive biases within the diffusion model framework.
Logit domain computation. Another noteworthy designing direction lies in the computation domain.
Although the reverse sampling process directly implemented from Equation (5) is already effective to generate
realistic graph data, we observe that migrating the computation to the logit space further enhances model
performance and accelerates training convergence. One potential explanation is that the logit transformation
amplifies the structural patterns of the graph when all edge weights are very close to zero at the beginning of
theancestralsamplingprocess. EquivalenttoEquation(5),thelogit-domaincomputationcanbeexpressedas
(cid:16) (cid:17)
logit(G )=ln elogit(Gt)+elogit(Pt)+elogit(Gt)+logit(Pt) . (17)
t−1
7Neural-network precondition. A drawback of the logit-domain computation is its introduction of
variability to model training, we address this issue by employing neural-network precondition techniques
[Karrasetal.,2022]. Specifically,preconditioninvolvesstandardizingG beforepassingthemtotheprediction
t
network Gˆ (·). In other words, we modify Equation (7) as
θ
G −E[g ] logit(G )−E[logit(g )]
p (G | G ):=q(G | G ,Gˆ (G˜ ,t)), G˜ = t t or t t , (18)
θ t−1 t t−1 t θ t t (cid:112) (cid:112)
Var[g ] Var[logit(g )]
t t
where g denotes the elements within G . Based upon the law of total expectation and the law of total
t t
variance, Zhou et al. [2023] have derived the following attribute of beta diffusion
Attribute1. Giventhatg |g ∼Beta(ηα g ,η(1−α g )),onecanderivethatE[g |g ]=α g andVar[g |g ]=
t 0 t 0 t 0 t 0 t 0 t 0
αtg0(1−αtg0). Representing E[g ] and Var[g ] with µ and σ2, we have
η+1 0 0
α µ−α2(µ2+σ2)
E[g ]=α µ, Var[g ]= t t +α2σ2.
t t t η+1 t
Their counterparts in the logit domain are expressed as
E[logit(g )]=E[ψ(ηα g )]−E[ψ(η−ηα g )],
t t 0 t 0
Var[logit(g )]=E[ψ(1)(ηα g )]+E[ψ(1)(η(1−α g ))]+Var[ψ(ηα g )]+Var[ψ(η(1−α g ))],
t t 0 t 0 t 0 t 0
with ψ(·) and ψ(1)(·) denoting digamma and trigamma functions.
In addition to Attribute 1, the specific values for E[g ], Var[g ], E[logit(g )] and Var[logit(g )] are also
t t t t
reliant on the distribution of the datum in its initial state. Recall that all variables within G have been
0
preprocessed using either dummy encoding or CDF transformation. Consequently, the processed variables
follow either a discrete distribution or a uniform distribution. With this context, if the original datum follows
a discrete distribution, we present the following conclusion.
Remark 1. If g has two potential outcomes {g ,g } with P(g =g )=p, then
0 min max 0 max
E[g ]=α (p·g +(1−p)·g ),
t t max min
Var[g ]= 1 (cid:0)E[g ]−E[g ]2(cid:1) + η (cid:0) α2(p(1−p))(g −g )2(cid:1) . (19)
t η+1 t t η+1 t max min
The calculation of E[logit(g )] and Var[logit(g )] relies on a series of components, namely
t t
E[ψ(ηα g )]=p·ψ(ηα g )+(1−p)·ψ(ηα g ),
t 0 t max t min
E[ψ(η−ηα g )]=p·ψ(η−ηα g )+(1−p)·ψ(η−ηα g ),
t 0 t max t min
Var[ψ(ηα g )]=p(1−p)(ψ(ηα g )−ψ(ηα g ))2,
t 0 t max t min
Var[ψ(η−ηα g )]=p(1−p)(ψ(η−ηα g )−ψ(η−ηα g ))2,
t 0 t max t min
E[ψ(1)(ηα g )]=p·ψ(1)(ηα g )+(1−p)·ψ(1)(ηα g ),
t 0 t max t min
E[ψ(1)(η−ηα g )]=p·ψ(1)(η−ηα g )+(1−p)·ψ(1)(η−ηα g ), (20)
t 0 t max t min
with which one can easily deduce the expressions of E[logit(g )] and Var[logit(g )] following Attribute 1.
t t
Alternatively, if the original datum follows a uniform distribution upon the support [g ,g ], then as
min max
demonstrated in Zhou et al. [2023], we have
Remark 2. If g follows the uniform distribution Unif[g ,g ], then
0 min max
1
E[g ]= α (g +g ),
t 2 t min max
Var[g ]= 1 (cid:0)E[g ]−E[g ]2(cid:1) + η (cid:0) α2(g −g )2(cid:1) . (21)
t η+1 t t 12(η+1) t max min
8We denote by K the number of sub-intervals used in the numerical integration based on the Trapezoidal rule.
Similar to Remark 1, on the logit domain, we list out the expressions for the components as
1
E[ψ(ηα g )]= (lnΓ(ηα g )−lnΓ(ηα g )),
t 0 ηα (g −g ) t max t min
t max min
1
E[ψ(η−ηα g )]= (lnΓ(η−ηα g )−lnΓ(η−ηα g )),
t 0 ηα (g −g ) t min t max
t max min
Var[ψ(ηα g
)]≈max(cid:32) 1 (cid:88)K ψ2(cid:0) ηα t(cid:0) g min+ Ki (g max−g min)(cid:1)(cid:1)
−E[ψ(ηα g
)]2,0(cid:33)
,
t 0 K i=0 2δ(i=0)+δ(i=K) t 0
Var[ψ(η−ηα g
)]≈max(cid:32) 1 (cid:88)K ψ2(cid:0) η−ηα t(cid:0) g min+ Ki (g max−g min)(cid:1)(cid:1)
−E[ψ(η−ηα g
)]2,0(cid:33)
,
t 0 K i=0 2δ(i=0)+δ(i=K) t 0
1
E[ψ(1)(ηα g )]= (ψ(ηα g )−ψ(ηα g )),
t 0 ηα (g −g ) t max t min
t max min
1
E[ψ(1)(η−ηα g )]= (ψ(η−ηα g )−ψ(η−ηα g )). (22)
t 0 ηα (g −g ) t min t max
t max min
Utilizing Attribute 1, one can derive the mathematical expressions that we use to calculate E[logit(g )]
t
and Var[logit(g )].
t
3 Related Work
Graph generative models. EarlyattemptsatmodelinggraphdistributionstracebacktotheErdős–Rényi
random graph model [ERDdS and R&wi, 1959, Erdős et al., 1960], from which a plethora of graph generative
models have emerged. These models employ diverse approaches to establish the data generative process
and devise optimization objectives, which in turn have significantly expanded the flexibility in modeling
the distribution of graph data. Stochastic blockmodels [Holland et al., 1983, Lee and Wilkinson, 2019],
more advanced latent variable models [Airoldi et al., 2009, Zhou, 2015, Caron and Fox, 2017], and their
variational-autoencoder-based successors [Kipf and Welling, 2016, Hasanzadeh et al., 2019, Mehta et al.,
2019, He et al., 2022] assume that edges are formed through independent pairwise node interactions, and
thus factorize the probability of the graph adjacency matrix into the dot product of factor representations
of nodes. Sequential models [You et al., 2018, Wang et al., 2018, Jin et al., 2018, Han et al., 2023] adopt
a similar concept of node interactions but correlate these interactions by organizing them into a series of
connection events. Additionally, some models treat the graph adjacency matrix as a parameterized random
matrix and generate it by mapping a random vector through a feed-forward neural network [Simonovsky and
Komodakis, 2018, De Cao and Kipf, 2018]. In terms of optimization targets, many utilize log-likelihood-based
objectives such as negative log-likelihood [Liu et al., 2019] or evidence lower bound objectives [Kipf and
Welling, 2016], while others employ generative adversarial losses [Wang et al., 2018] or reinforcement learning
losses [De Cao and Kipf, 2018]. Diffusion-based graph generative models [Jo et al., 2022, Haefeli et al., 2022,
Vignac et al., 2023, Jo et al., 2023, Chen et al., 2023, Cho et al., 2023, Kong et al., 2023], including this
work, feature a unique data generation process compared to previous models. They map the observed graph
structures and node features to a latent space through a stochastic diffusion process, whose reverse process
can be learned by optimizing a variational lower bound [Vignac et al., 2023] or numerically solving a reverse
stochastic differential equation [Jo et al., 2022].
Diffusion models. The stochastic diffusion process is introduced by Sohl-Dickstein et al. [2015] for deep
unsupervised learning, and its foundational connection with deep generative models is laid down by denoising
diffusion probabilistic model (DDPM) [Ho et al., 2020]. DDPM maps a data sample to the latent space via a
Markov process that gradually applies noise to the original sample, and learns a reverse process to reproduce
the sample in finite steps. The optimization and sampling processes in DDPM can be interpreted through
the lens of variational inference [Sohl-Dickstein et al., 2015, Ho et al., 2020] or can be formulated as score
9matching with Langevin dynamics [Song et al., 2021, Song and Ermon, 2019]. Both approaches are focused
on diffusion processes that define the transition between normally distributed variables, which are proven
effective for generating natural images. As the scope of generative tasks expands to discrete domains like text,
diffusion models transitioning between discrete states emerge [Austin et al., 2021], which demonstrates that
the choice of probabilistic distribution for modeling each noise state could significantly impact the learning
task. This conclusion is also validated in the application of graph generation [Haefeli et al., 2022, Vignac
et al., 2023]. Further studies [Chen and Zhou, 2023, Zhou et al., 2023] are conducted to improve diffusion
models by introducing novel diffusion processes based on probabilistic distributions that better capture the
intrinsic characteristics of the generation target. Among these, the beta diffusion of Zhou et al. [2023] is
chosen as the foundation of our method, due to the beta distribution’s proficiency in capturing sparsity and
long-tailed characteristics in range-bounded data. These traits are commonly observed in real-world graphs
[Barabási et al., 2000, Ciotti et al., 2015, Liang et al., 2023, Wang et al., 2023].
4 Experiments
We validated GBD on both molecular and non-molecular benchmarks, demonstrating its ability to generate
valid graphs using various evaluation metrics.
4.1 Generic graph generation
To verify that GBD can generate graphs accurately reflecting the underlying data distribution, we first
evaluate our method on a range of generic generation tasks across various datasets.
Datasets and metrics. We consider three synthetic and real datasets of varying size and connectivity
used as benchmarks in previous works: Ego-small consists of 200 small real sub-graphs from the Citeseer
network dataset with 4≤N ≤18, where N denotes the number of nodes. Community-small consists of 100
randomly generated synthetic graphs with 12≤N ≤20. Grid consists of randomly generated 100 standard
2D grid graphs with 100≤N ≤400.
For a fair comparison, we follow the experimental and evaluation setting of Jo et al. [2022, 2023] with the
same train/test split. We adopt maximum mean discrepancy (MMD) [Gretton et al., 2012] as our evaluation
metric to compare three graph property distributions between the test graphs and the same number of
generated graphs: degree (Deg.), clustering coefficient (Clus.), count of orbits with 4 nodes (Orbit), and
their average score (Avg.). Further details about these metrics can be found in Appendix A.
Baselines. We compare GBD against the following autoregressive and one-shot graph generation methods:
DeepGMG [Li et al., 2018], GraphRNN [You et al., 2018], GraphAF [Shi et al., 2020], GraphDF [Luo
et al., 2021], GraphVAE [Simonovsky and Komodakis, 2018], and GNF [Liu et al., 2019]. We also compare
GBD against several state-of-the-art diffusion-based graph generative models: EDP-GNN [Niu et al., 2020a],
a score-based model for adjacency matrix, GDSS [Jo et al., 2022] and ConGress [Vignac et al., 2023],
continuous diffusion models, DiGress [Vignac et al., 2023], a discrete diffusion model, and Wave-GD [Cho
et al., 2023], a wavelet-based diffusion model. We describe implementation details in Appendix A.
Results. As shown in Table 1, the proposed GBD achieves superior or comparable performance to the
MMD of the sampled graphs from the training set. Compared to previous diffusion-based methods, GBD
outperforms them on 7/9 MMD metrics and achieves the best average results on each dataset. In particular,
GBD surpasses DiGress on all MMD metrics, demonstrating that our model is capable of modeling discrete
graph data and even achieves better results. We attribute this to our model’s superior ability to capture the
sparsity of the graph data distribution, which makes GBD distinctive. For the larger graph dataset Grid,
10Table 1: Generic graph generation results.
Ego-small Community-small Grid
Method Deg. ↓ Clus. ↓ Orbit. ↓ Avg. ↓ Deg. ↓ Clus. ↓ Orbit. ↓ Avg. ↓ Deg. ↓ Clus. ↓ Orbit. ↓ Avg. ↓
DeepGMG 0.040 0.100 0.020 0.053 0.220 0.950 0.400 0.523 - - - -
GraphRNN 0.090 0.220 0.003 0.104 0.080 0.120 0.040 0.080 0.064 0.043 0.021 0.043
GraphAF 0.030 0.110 0.001 0.047 0.180 0.200 0.020 0.133 - - - -
GraphDF 0.040 0.130 0.010 0.060 0.060 0.120 0.030 0.070 - - - -
GraphVAE 0.130 0.170 0.050 0.117 0.350 0.980 0.540 0.623 1.619 0.0 0.919 0.846
GNF 0.030 0.100 0.001 0.044 0.200 0.200 0.110 0.170 - - - -
EDP-GNN 0.052 0.093 0.007 0.051 0.053 0.144 0.026 0.074 0.455 0.238 0.328 0.340
GDSS 0.021 0.024 0.007 0.017 0.045 0.086 0.007 0.046 0.111 0.005 0.070 0.062
ConGress∗ 0.037 0.064 0.017 0.039 0.020 0.076 0.006 0.034
DiGress 0.017 0.021 0.010 0.016 0.028 0.115 0.009 0.050 - - - -
Wave-GD 0.012 0.010 0.005 0.009 0.007 0.058 0.002 0.022 0.144 0.004 0.021 0.056
0.011 0.014 0.002 0.009 0.002 0.060 0.002 0.021 0.045 0.011 0.040 0.032
GBD
(±0.008) (±0.013) (±0.004) - (±0.003) (±0.004) (±0.003) - (±0.004) (±0.002) (±0.014) -
GBD obtains a superior average score and other competitive MMDs compared to Wave-GD, demonstrating
its potential in generating larger graphs.
Evaluation with larger sample size. For Ego-small and Community-small, it is worth noting that for
most evaluation metrics, the MMDs of the graphs sampled from the training set still have large standard
deviations,likelyduetothesmallnumberofnodesinthegraphandtheinsufficientsizeofthesampledgraphs.
Therefore, it is necessary to assess the quality of generated samples more definitively using a larger number of
generated graphs compared to the test data. Similar to the previous works [Cho et al., 2023, Jo et al., 2022,
Liu et al., 2019], we sampled 1024 graphs for each smaller dataset and evaluated the MMD metrics with
their means and standard deviations reported in Table 2. We observe that our proposed GBD outperforms
previous continuous and discrete diffusion models on both smaller datasets. Furthermore, GBD significantly
surpasses the wavelet-based diffusion model (Wave-GD) by a wide margin on the Community-small dataset,
as evidenced by both means and standard deviations. Specifically, GBD achieves 85.0%, 90.5%, and 40.0%
improvements over Wave-GD in the MMDs means of Degree, Cluster, and Orbit, respectively, indicating that
our model is capable of generating smaller graphs that are closer to the data distribution with better stability.
Table 2: Generic graph generation results with larger sample size.
Ego-small Community-small
Method Deg. ↓ Clus. ↓ Orbit. ↓ Avg. ↓ Deg. ↓ Clus. ↓ Orbit. ↓ Avg. ↓
GraphRNN 0.040 0.050 0.060 0.050 0.030 0.010 0.010 0.017
GNF 0.010 0.030 0.001 0.014 0.120 0.150 0.020 0.097
EDP-GNN 0.010 0.025 0.003 0.013 0.006 0.127 0.018 0.050
GDSS 0.023 0.020 0.005 0.016 0.029 0.068 0.004 0.034
0.030 0.050 0.008 0.030 0.004 0.047 0.001 0.017
ConGress∗
(±0.001) (±0.003) (±0.001) - (±0.000) (±0.000) (±0.000) -
0.009 0.031 0.003 0.014 0.003 0.009 0.001 0.004
DiGress∗
(±0.000) (±0.002) (±0.000) - (±0.000) (±0.001) (±0.000) -
0.010 0.018 0.005 0.011 0.016 0.077 0.001 0.031
Wave-GD
(±0.001) (±0.003) (±0.002) - (±0.000) (±0.006) (±0.002) -
0.007 0.011 0.003 0.010 0.002 0.007 0.001 0.003
GBD
(±0.000) (±0.001) (±0.000) - (±0.000) (±0.001) (±0.000) -
11Table 3: 2D molecule generation results
QM9 ZINC250K
(|V|≤9)
Method Valid(%)↑ FCD↓ NSPDK↓ Scaf. ↓ Valid(%)↑ FCD↓ NSPDK↓ Scaf. ↓
MoFlow 91.36 4.467 0.0169 0.1447 63.11 20.931 0.0455 0.0133
GraphAF 74.43 5.625 0.0207 0.3046 68.47 16.023 0.0442 0.0672
GraphDF 93.88 10.928 0.0636 0.0978 90.61 33.546 0.1770 0.0000
EDP-GNN 47.52 2.680 0.0046 0.3270 82.97 16.737 0.0485
GDSS 95.72 2.900 0.0033 0.6983 97.01 14.656 0.0195 0.0467
GDSS+Transformer 99.68 0.737 0.0024 0.9129 96.04 5.556 0.0326 0.3205
DiGress 98.19 0.095 0.0003 0.9353 94.99 3.482 0.0021 0.4163
DruM 99.69 0.108 0.0002 0.9449 98.65 2.257 0.0015 0.5299
GBD 99.88 0.093 0.0002 0.9510 97.87 2.248 0.0018 0.5042
4.2 Molecule Generation
We validate GBD on 2D molecule generation tasks for attributed graphs, demonstrating its capability to
model graph structure with both node and edge attributes.
Datasets and Metrics We consider two widely-used molecule datasets as benchmarks in Jo et al. [2022]:
QM9 [Ramakrishnan et al., 2014], which consists of 133,885 molecules with N ≤9 nodes from 4 different node
types, and ZINC250k [Irwin et al., 2012], which consists of 249,455 molecules with N ≤ 38 nodes from 9
different node types. Molecules in both datasets have 3 edge types, namely single bond, double bond, and
triple bond.
Following the evaluation setting of Jo et al. [2022], we generated 10,000 molecules for each dataset and
evaluate them with four metrics: the ratio of valid molecules without correction (Val.), Fréchet ChemNet
Distance (FCD), Neighborhood subgraph pairwise distance kernel NSPDK, and Scaffold similarity (Scaf.).
Baselines We compare GBD against the following autoregressive and one-shot graph generation methods:
MoFlow [Zang and Wang, 2020], GraphAF [Shi et al., 2020], GraphDF [Luo et al., 2021], and several
state-of-the-art diffusion-based graph generative models discussed previously: EDP-GNN [Niu et al., 2020a],
GDSS [Jo et al., 2022] and ConGress [Vignac et al., 2023], DiGress [Vignac et al., 2023], and DruM [Jo
et al., 2023]. We describe the implementation details in Appendix A.
Results As shown in Table 3, we observe that our GBD outperforms most previous diffusion-based models
and is competitive with the current state-of-the-art Gaussian-based diffusion model, DruM. In particular,
compared to the basic continuous diffusion model, GBD significantly outperforms it (GDSS+Transformer)
under the same GraphTransformer architecture. Additionally, we observe that our proposed beta-based
diffusion model is superior to the discrete diffusion model on both 2D molecule datasets, demonstrating that
the beta-based diffusion model is also capable of modeling complex structures of attributed graphs (even
more effectively when comparing GBD to DiGress). We attribute this to the excellent modeling ability of
beta-based diffusion model for sparse and long-tailed data distributions.
4.3 Ablation studies
With all other hyperparameter choices kept constant, we vary the options regarding computation domain
and the application of preconditioning, and summarize the outcome of model performance in Table 4. The
combination of adopting logit domain computation without using preconditioning can sometimes increase
the challenge in model convergence, and therefore, it is not recommended. The listed results on Ego-small
and Community-small demonstrate that both techniques are in general beneficial for achieving better model
12Table 4: The effect of practicing logit domain computation and preconditioning
Ego-small Community-small
logitdomain preconditioning Deg. ↓ Clus. ↓ Orbit. ↓ Avg. ↓ Deg. ↓ Clus. ↓ Orbit. ↓ Avg. ↓
- - 0.015 0.018 0.004 0.012 0.010 0.076 0.004 0.030
- ✓ 0.013 0.017 0.002 0.011 0.004 0.044 0.007 0.018
✓ ✓ 0.011 0.014 0.002 0.009 0.002 0.060 0.002 0.021
performance, and the effect of preconditioning is more evident when the computation is perfomed in the
logit domain.
4.4 Visualization
We provide the visualization of the generative process and generated graphs of GBD in Appendix B. For
generative process with general datasets shown in Appendix B.1, we follow the implementation described in
Section 2.4 and the nodes in all adjacency matrices are reordered by decreasing node degree. Apparently, we
can find that edges associated with nodes with large degree will be the first to be identified, and then spread
in decreasing order of degree on both datasets. It’s worth noting that the reverse beta diffusion can converge
rapidly, leading to generated graphs with correct topology at an early stage. This demonstrates that our
proposed GBD can further explore the potential benefits of beta diffusion, resulting in valid graphs with
stability and high quality. For generated molecule graphs shown in Appendix B.1, we can observe that GBD
can successfully generate valid and high-quality 2D molecules, verifying its ability to model attributed graphs.
5 Conclusion, Limitations and Broader Impact
We introduce graph beta diffusion (GBD), a novel graph generation framework developed upon beta diffusion.
We demonstrate that the utilization of beta distribution to define the diffusion process is beneficial for
modeling the distribution of graph data, and outline four crucial designing elements—data transformation,
concentrationmodulation,logit-domaincomputation,andneural-networkprecondition—thatthatconsistently
enhance model performance.
With these contributions achieved, we identify several areas for potential improvement. First, while
the beta distribution can adeptly model various data distributions within the range of [0,1], using it to
construct the diffusion model necessitates careful selection of scaling and shifting parameters to ensure model
convergence. This requirement can complicate the transfer of experience between different tasks. Second,
unlike discrete diffusion processes, the beta diffusion process does not naturally generate discrete intermediate
adjacency matrices for computing graph statistics. This necessitates a quantization strategy if one aims
to incorporate statistics from these intermediates when predicting the graph at the initial state. Thirdly,
diffusion-based graph generative models, including GBD, currently rely on a reverse diffusion process that
takes hundreds to thousands of iterative refinement steps to generate a single sample. Recent advancements
in score-based distillation techniques, originally developed for image diffusion models, could be adapted to
distill the graph teacher model [Luo et al., 2023, Zhou et al., 2024]. Such an adaptation has the potential to
significantly accelerate the graph generation process while maintaining or even improving performance. We
plan to explore these possibilities in future studies.
Finally, we explore the potential impact of GBD. Since much real-world data can be structured as graphs,
an effective tool for generating realistic graphs could significantly benefit researchers in the natural and social
sciences, enabling the economical creation of high-quality simulated data. However, there is a concern: if the
generated content is misused for fraudulent activities, distinguishing it from authentic data could become
increasingly challenging for recipients.
13References
Emmanuel Abbe. Community detection and stochastic block models: recent developments. Journal of Machine
Learning Research, 18(177):1–86, 2018.
EdoMAiroldi,DavidBlei,StephenFienberg,andEricXing. Mixedmembershipstochasticblockmodels. InAdvances
in Neural Information Processing Systems (NeurIPS), volume 21, 2009.
Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising
diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:17981–17993,
2021.
PavelAvdeyev,ChenlaiShi,YuhaoTan,KseniiaDudnyk,andJianZhou. Dirichletdiffusionscoremodelforbiological
sequence generation. In Proceedings of the Fortieth International Conference on Machine Learning, Proceedings of
Machine Learning Research. PMLR, 2023.
Arindam Banerjee, Srujana Merugu, Inderjit S Dhillon, Joydeep Ghosh, and John Lafferty. Clustering with Bregman
divergences. Journal of machine learning research, 6(10), 2005.
Albert-LászlóBarabási,RékaAlbert,andHawoongJeong. Scale-freecharacteristicsofrandomnetworks: thetopology
of the world-wide web. Physica A: statistical mechanics and its applications, 281(1-4):69–77, 2000.
Guy W Bemis and Mark A Murcko. The properties of known drugs. 1. molecular frameworks. Journal of medicinal
chemistry, 39(15):2887–2893, 1996.
PietroBongini,MonicaBianchini,andFrancoScarselli. Moleculargenerativegraphneuralnetworksfordrugdiscovery.
Neurocomputing, 450:242–252, 2021.
FrançoisCaronandEmilyBFox. Sparsegraphsusingexchangeablerandommeasures. Journal of the Royal Statistical
Society Series B: Statistical Methodology, 79(5):1295–1366, 2017.
Tianqi Chen and Mingyuan Zhou. Learning to jump: Thinning and thickening latent counts for generative modeling.
In International Conference on Machine Learning, pages 5367–5382. PMLR, 2023.
Xiaohui Chen, Jiaxing He, Xu Han, and Li-Ping Liu. Efficient and degree-guided graph generation via discrete
diffusion modeling. In International Conference on Machine Learning. PMLR, 2023.
Hyuna Cho, Minjae Jeong, Sooyeon Jeon, Sungsoo Ahn, and Won Hwa Kim. Multi-resolution spectral coherence
for graph generation with score-based diffusion. In Thirty-seventh Conference on Neural Information Processing
Systems, 2023.
Valerio Ciotti, Ginestra Bianconi, Andrea Capocci, Francesca Colaiori, and Pietro Panzarasa. Degree correlations in
signed social networks. Physica A: Statistical Mechanics and its Applications, 422:25–39, 2015.
Marco Conti, Andrea Passarella, and Fabio Pezzoni. A model for the generation of social network graphs. In 2011
IEEE International Symposium on a World of Wireless, Mobile and Multimedia Networks, pages 1–6. IEEE, 2011.
Nicola De Cao and Thomas Kipf. Molgan: An implicit generative model for small molecular graphs. arXiv preprint
arXiv:1805.11973, 2018.
Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. arXiv preprint
arXiv:2012.09699, 2020a.
Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. arXiv preprint
arXiv:2012.09699, 2020b.
P ERDdS and A R&wi. On random graphs i. Publ. math. debrecen, 6(290-297):18, 1959.
Paul Erdős, Alfréd Rényi, et al. On the evolution of random graphs. Publ. math. inst. hung. acad. sci, 5(1):17–60,
1960.
14ArthurGretton,KarstenMBorgwardt,MalteJRasch,BernhardSchölkopf,andAlexanderSmola.Akerneltwo-sample
test. The Journal of Machine Learning Research, 13(1):723–773, 2012.
Zhiye Guo, Jian Liu, Yanli Wang, Mengrui Chen, Duolin Wang, Dong Xu, and Jianlin Cheng. Diffusion models in
bioinformatics and computational biology. Nature reviews bioengineering, 2(2):136–154, 2024.
Kilian Konstantin Haefeli, Karolis Martinkus, Nathanaël Perraudin, and Roger Wattenhofer. Diffusion models for
graphs benefit from discrete state spaces. In The First Learning on Graphs Conference, 2022.
Xu Han, Xiaohui Chen, Francisco JR Ruiz, and Li-Ping Liu. Fitting autoregressive graph generative models through
maximum likelihood estimation. Journal of Machine Learning Research, 24(97):1–30, 2023.
Arman Hasanzadeh, Ehsan Hajiramezanali, Krishna Narayanan, Nick Duffield, Mingyuan Zhou, and Xiaoning Qian.
Semi-implicit graph variational auto-encoders. In Advances in Neural Information Processing Systems (NeurIPS),
volume 32, pages 10711–10722, 2019.
YilinHe,ChaojieWang,HaoZhang,BoChen,andMingyuanZhou. Avariationaledgepartitionmodelforsupervised
graph representation learning. Advances in Neural Information Processing Systems, 35:12339–12351, 2022.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information
processing systems, 33:6840–6851, 2020.
Paul W Holland, Kathryn Blackmond Laskey, and Samuel Leinhardt. Stochastic blockmodels: First steps. Social
networks, 5(2):109–137, 1983.
Han Huang, Leilei Sun, Bowen Du, and Weifeng Lv. Conditional diffusion based on discrete graph structures for
molecular graph generation. In NeurIPS 2022 Workshop on Score-Based Methods, 2022.
John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. Zinc: a free tool to
discover chemistry for biology. Journal of chemical information and modeling, 52(7):1757–1768, 2012.
Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph
generation. In International conference on machine learning, pages 2323–2332. PMLR, 2018.
Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs via the system of stochastic
differential equations. In International Conference on Machine Learning, pages 10362–10383. PMLR, 2022.
Jaehyeong Jo, Dongki Kim, and Sung Ju Hwang. Graph generation with diffusion mixture. arXiv preprint
arXiv:2302.03596, 2023.
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative
models. Advances in Neural Information Processing Systems, 35:26565–26577, 2022.
Thomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016.
Lingkai Kong, Jiaming Cui, Haotian Sun, Yuchen Zhuang, B Aditya Prakash, and Chao Zhang. Autoregressive
diffusionmodelforgraphgeneration. InInternational Conference on Machine Learning,pages17391–17408.PMLR,
2023.
Greg Landrum et al. Rdkit: Open-source cheminformatics, 2006.
ClementLeeandDarrenJWilkinson. Areviewofstochasticblockmodelsandextensionsforgraphclustering. Applied
Network Science, 4(1):1–50, 2019.
Chen Li and Yoshihiro Yamanishi. Tengan: Pure transformer encoders make an efficient discrete gan for de novo
molecular generation. In International Conference on Artificial Intelligence and Statistics, pages 361–369. PMLR,
2024.
Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep generative models of graphs.
In International conference on machine learning. PMLR, 2018.
15Langzhang Liang, Zenglin Xu, Zixing Song, Irwin King, Yuan Qi, and Jieping Ye. Tackling long-tailed distribution
issue in graph neural networks via normalization. IEEE Transactions on Knowledge and Data Engineering, 2023.
Jenny Liu, Aviral Kumar, Jimmy Ba, Jamie Kiros, and Kevin Swersky. Graph normalizing flows. Advances in Neural
Information Processing Systems, 32, 2019.
Qi Liu, Miltiadis Allamanis, Marc Brockschmidt, and Alexander Gaunt. Constrained graph variational autoencoders
for molecule design. Advances in neural information processing systems, 31, 2018.
Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-Instruct: A universal
approach for transferring knowledge from pre-trained diffusion models. In Thirty-seventh Conference on Neural
Information Processing Systems, 2023. URL https://openreview.net/forum?id=MLIs5iRq4w.
Youzhi Luo, Keqiang Yan, and Shuiwang Ji. Graphdf: A discrete flow model for molecular graph generation. In
International conference on machine learning, pages 7192–7203. PMLR, 2021.
Karolis Martinkus, Andreas Loukas, Nathanaël Perraudin, and Roger Wattenhofer. Spectre: Spectral conditioning
helps to overcome the expressivity limits of one-shot graph generators. In International Conference on Machine
Learning, pages 15159–15179. PMLR, 2022.
Nikhil Mehta, Lawrence Carin, and Piyush Rai. Stochastic blockmodels meet graph neural networks. In International
Conference on Machine Learning (ICML), pages 4466–4474, 2019.
MarkEJNewman,DuncanJWatts,andStevenHStrogatz. Randomgraphmodelsofsocialnetworks. Proceedings of
the national academy of sciences, 99(suppl_1):2566–2572, 2002.
Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutation invariant
graphgenerationviascore-basedgenerativemodeling. InSilviaChiappaandRobertoCalandra,editors,Proceedings
of the Twenty Third International Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of
Machine Learning Research, pages 4474–4484. PMLR, 26–28 Aug 2020a.
Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permutation invariant
graph generation via score-based generative modeling. In International Conference on Artificial Intelligence and
Statistics, pages 4474–4484. PMLR, 2020b.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.
Advances in neural information processing systems, 32, 2019.
Raghunathan Ramakrishnan, Pavlo O Dral, Matthias Rupp, and O Anatole Von Lilienfeld. Quantum chemistry
structures and properties of 134 kilo molecules. Scientific data, 1(1):1–7, 2014.
Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective
classification in network data. AI magazine, 29(3):93–93, 2008.
Chence Shi, Minkai Xu, Zhaocheng Zhu, Weinan Zhang, Ming Zhang, and Jian Tang. Graphaf: a flow-based
autoregressive model for molecular graph generation. In International Conference on Learning Representations,
2020.
Pouyan Shirzadian, Blessy Antony, Akshaykumar G Gattani, Nure Tasnina, and Lenwood S Heath. A time evolving
online social network generation algorithm. Scientific Reports, 13(1):2395, 2023.
Martin Simonovsky and Nikos Komodakis. Graphvae: Towards generation of small graphs using variational autoen-
coders. In Artificial Neural Networks and Machine Learning–ICANN 2018: 27th International Conference on
Artificial Neural Networks, Rhodes, Greece, October 4-7, 2018, Proceedings, Part I 27, pages 412–422. Springer,
2018.
Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using
nonequilibrium thermodynamics. In International conference on machine learning, pages 2256–2265. PMLR, 2015.
16Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in
neural information processing systems, 32, 2019.
Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-
based generative modeling through stochastic differential equations. In International Conference on Learning
Representations, 2021.
ClementVignac,IgorKrawczuk,AntoineSiraudin,BohanWang,VolkanCevher,andPascalFrossard.Digress: Discrete
denoising diffusion for graph generation. In The Eleventh International Conference on Learning Representations,
2023.
Haohui Wang, Baoyu Jing, Kaize Ding, Yada Zhu, Liqing Zhang, and Dawei Zhou. Characterizing long-tail categories
on graphs. arXiv preprint arXiv:2305.09938, 2023.
Hongwei Wang, Jia Wang, Jialin Wang, Miao Zhao, Weinan Zhang, Fuzheng Zhang, Xing Xie, and Minyi Guo.
Graphgan: Graph representation learning with generative adversarial nets. In Proceedings of the AAAI conference
on artificial intelligence, volume 32, 2018.
Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. Graphrnn: Generating realistic graphs
with deep auto-regressive models. In International conference on machine learning, pages 5708–5717. PMLR, 2018.
Chengxi Zang and Fei Wang. Moflow: an invertible flow model for generating molecular graphs. In Proceedings of the
26th ACM SIGKDD international conference on knowledge discovery & data mining, pages 617–626, 2020.
Mingyuan Zhou. Infinite edge partition models for overlapping community detection and link prediction. In
International Conference on Artificial Intelligence and Statistics (AISTATS), volume 38 of Proceedings of Machine
Learning Research, pages 1135–1143, 2015.
Mingyuan Zhou, Tianqi Chen, Zhendong Wang, and Huangjie Zheng. Beta diffusion. In NeurIPS 2023: Neural
Information Processing Systems, 2023.
Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation:
Exponentially fast distillation of pretrained diffusion models for one-step generation. In International Conference
on Machine Learning, 2024.
17A Experimental Details
A.1 General graph generation
Datasets We evaluated our model using three synthetic and real datasets of varying size and connectivity,
previouslyusedasbenchmarksintheliterature[Choetal.,2023,Joetal.,2022]: Ego-small[Senetal.,2008]
consistsof200smallrealsub-graphsfromtheCiteseernetworkdatasetwith4≤N ≤18. Community-small
consists of 100 randomly generated synthetic graphs with 12≤N ≤20, where the graphs are constructed by
two equal-sized communities, each of which is generated by the Erdös–Rényi model [Erdős et al., 1960], with
p=0.7 and 0.05N inter-community edges are added with uniform probability as in previous works [Jo et al.,
2022,Niuetal.,2020b]. Gridconsistsofrandomlygenerated100standard2Dgridgraphswith100≤N ≤400
and the maximum number of edges per node is 4 since all nodes are arranged in a regular lattice.
Evaluation metrics For a fair comparison, we follow the experimental and evaluation settings of Jo et al.
[2022, 2023], using the same train/test split, where 80% of the data is used as the training set and the
remaining 20% as the test set. We adopt maximum mean discrepancy (MMD) as our evaluation metric to
compare three graph property distributions between test graphs and the same number of generated graphs:
degree (Deg.), clustering coefficient (Clus.), count of orbits with 4 nodes (Orbit) and their average score
(Avg.). NotethatweusetheGaussianEarthMover’sDistance(EMD)kerneltocomputetheMMDsfollowing
the method used in previous work [Jo et al., 2022, Cho et al., 2023].
Implementation details We follow the evaluation setting of Jo et al. [2022], Cho et al. [2023] to generate
graphs of the same size as the test data in each run and we report the mean and standard deviation obtained
from 3 independent runs for each dataset. We report the baseline results taken from Cho et al. [2023], except
for the results of ConGress in Tables 1 and 2, which we obtained by running its corresponding open-source
code. For a fair comparison, we adopt the Graph Transformer [Dwivedi and Bresson, 2020b, Vignac et al.,
2023] as the neural network used in GDSS+Transformer [Jo et al., 2022], DiGress [Vignac et al., 2023],
and DruM [Jo et al., 2023]. We set the diffusion steps to 1000 for all the diffusion models. For important
hyperparameters mentioned in Sec 2.4, we usually set S =0.9, S =0.09. and η =[10000,100,30,10]
cale hift
for the normalized degrees corresponding to the intervals falling in the interval split by [1.0,0.8,0.4,0.1],
respectively. In practice, we set threshold as 0.9 to quantize generated continue adjacency matrix.
A.2 2D molecule generation
Datasets We utilize two widely-used molecular datasets as benchmarks, as described in Jo et al. [2023]:
QM9 [Ramakrishnan et al., 2014], consisting of 133,885 molecules with N ≤9 nodes from 4 different node
types and ZINC250k [Irwin et al., 2012], consisting of 249,455 molecules with N ≤38 nodes from 9 node
types. Molecules in both datasets have 3 edge types, namely single bond, double bond, and triple bond.
Following the standard procedure in the literature [Shi et al., 2020, Luo et al., 2021, Jo et al., 2022, 2023], we
kekulize the molecules using the RDKit library [Landrum et al., 2006] and remove the hydrogen atoms from
the molecules in the QM9 and ZINC250k datasets.
Evaluation metrics Following the evaluation setting of Jo et al. [2022], we generate 10,000 molecules
for each dataset and evaluate them with four metrics: the ratio of valid molecules without correction
(Val.). Frechet ChemNet Distance (FCD) evaluates the chemical properties of the molecules by measuring
the distance between the feature vectors of generated molecules and those in the test set using ChemNet.
Neighborhood Subgraph Pairwise Distance Kernel (NSPDK) assesses the quality of the graph structure by
measuring the MMD between the generated molecular graphs and the molecular graphs from the test set.
Scaffold Similarity (Scaf.) evaluates the ability to generate similar substructures by measuring the cosine
similarity of the frequencies of Bemis-Murcko scaffolds [Bemis and Murcko, 1996].
18Implementation details We follow the evaluation setting of Jo et al. [2022, 2023] to generate 10,000
molecules and evaluate graphs with test data for each dataset. We quote the baselines results from Jo
et al. [2023]. For a fair comparison, we adopt the Graph Transformer [Dwivedi and Bresson, 2020b, Vignac
et al., 2023] as the neural network used in GDSS+Transformer [Jo et al., 2022], DiGress [Vignac et al.,
2023], and DruM [Jo et al., 2023]. We apply the exponential moving average (EMA) to the parameters while
sampling and set the diffusion steps to 1000 for all the diffusion models. For both QM9 and ZINC250k,
we encode nodes and edges to one-hot and set S = 0.9, S = 0.09. For η modulated in molecule
cale hift
generation, with the help of chemical knowledge, we apply η =[10000,100,100,100,30] on carbon-carbon
bond, carbon-nitrogen, carbon-oxygen, carbon-fluorine, and other possible bonds, respectively. For η of nodes,
we apply η =[10000,100,100,30] on carbon atom, nitrogen atom, oxygen atom and other possible atoms,
respectively. As described in Sec 2.4, applying appropriate η for different node types and edge types can
prolong the presence of related substructures during the diffusion process. In practice, we set threshold as
0.9 to quantize generated continue adjacency matrix and the value in discrete adjacency matrix is 0 after
quantizing if and only if all values in each dimension are all 0.
A.3 Computing resources
For all experiments, we utilized the PyTorch [Paszke et al., 2019] framework to implement GBD and trained
the model with NVIDIA GeForce RTX 4090 and RTX A5000 GPUs.
B Visualization
B.1 Generative process of GBD on general datasets
We visualize generative process of GBD on the Community-small and the Ego-small dataset in Figures 3
and 4, respectively.
B.2 Generated graphs of GBD on 2D molecule datasets
We provide the visualization of the 2D molecules generated by GBD on the QM9 and the ZINC250k
datasets in Figure 5 and in Figure 6, respectively.
19Figure 3: Visualization of the generative process of GBD on the Community-small dataset.
20Figure 4: Visualization of the generative process of GBD on the Ego-small dataset.
21Figure 5: Visualization of the generated graphs of GBD on the QM9 dataset.
22Figure 6: Visualization of the generative process of GBD on the ZINC250k dataset.
23