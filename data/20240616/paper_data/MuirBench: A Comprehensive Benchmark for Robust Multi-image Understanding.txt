MUIRBENCH: A Comprehensive Benchmark for
Robust Multi-image Understanding
FeiWang1∗ XingyuFu2∗ JamesY.Huang1† ZekunLi3† QinLiu4† XiaogengLiu5†
MingyuDerekMa6† NanXu1† WenxuanZhou1† KaiZhang7 TianyiLorenaYan1
WenjieJackyMo1 Hsiang-HuiLiu3 PanLu6 ChunyuanLi8 ChaoweiXiao5 Kai-WeiChang6
DanRoth2 ShengZhang9 HoifungPoon9 MuhaoChen4
1USC 2UPenn 3UMN 4UCDavis5UW–Madison 6UCLA 7OSU 8Bytedance 9MicrosoftResearch
Figure1:TheMUIRBENCHBenchmark. MUIRBENCHcontains11,264imagesand2,600multiple-
choicequestions,providingrobustevaluationon12multi-imageunderstandingtasks. Eachexample
comesfromonetaskinMUIRBENCH,presentingdiversemulti-imagerelations.
Abstract
WeintroduceMUIRBENCH,acomprehensivebenchmarkthatfocusesonrobust
multi-imageunderstandingcapabilitiesofmultimodalLLMs. MUIRBENCHcon-
sists of 12 diverse multi-image tasks (e.g., scene understanding, ordering) that
involve10categoriesofmulti-imagerelations(e.g.,multiview,temporalrelations).
Comprising 11,264 images and 2,600 multiple-choice questions, MUIRBENCH
iscreatedinapairwisemanner,whereeachstandardinstanceispairedwithan
unanswerablevariantthathasminimalsemanticdifferences,inorderforareliable
assessment. Evaluatedupon20recentmulti-modalLLMs,ourresultsrevealthat
eventhebest-performingmodelslikeGPT-4oandGeminiProfinditchallenging
to solve MUIRBENCH, achieving 68.0% and 49.3% in accuracy. Open-source
multimodalLLMstrainedonsingleimagescanhardlygeneralizetomulti-image
questions, hovering below 33.3% in accuracy. These results highlight the im-
portanceofMUIRBENCHinencouragingthecommunitytodevelopmultimodal
LLMs that can look beyond a single image, suggesting potential pathways for
futureimprovements.
∗Equalleadership.Correspondanceto<fwang598@usc.edu;xingyuf2@seas.upenn.edu>.
†Equalcontribution;alphabeticorder.
Projectpage:https://huggingface.co/datasets/MUIRBENCH/MUIRBENCH
Preprint.Underreview.
4202
nuJ
31
]VC.sc[
1v11490.6042:viXraFigure 2: Compared with previous benchmarks, MUIRBENCH has several novel features: (1) It
evaluates on a comprehensive range of 12 multi-image understanding abilities, e.g. geographic
understandinganddiagramunderstandingasintroducedin§3,whilepriorbenchmarksgenerally
containsingle-imagequestions. (2)Itcontains10diversemulti-imagerelations,e.g.narrativeand
complementaryasdiscussedin§3. (3)Itprovidesarobustevaluationonmodelsbyunanswerable
instancevariants. Thesamplesofpreviousbenchmarksarefrom [25,37,53].
1 Introduction
Theproverb“apictureisworthathousandwords”isoftencitedtoemphasizetherichnessofvisual
information hidden in one image [21, 24]. However, an image is only a single projection of the
realworldcapturedfromaspecificangleataspecificmomentintime[23]. Incontrast, humans
naturallyobservemultipleimages–multiplepiecesofsuchprojectionsfromdiscretemomentsunder
variousscenes–toperceiveandunderstandtheworldasaholisticpart. Humansexcelatsynthesizing
informationfrommultipleimagesources,whetheritinvolvestellingstoriesfromaseriesofcartoon
images [10, 30], drawing comparisons among multiple charts and diagrams to infer holistic new
insights [46], learning from diverse visual experiences such as online lesson slides to adopt new
skills[48],predictingfutureeventactionsfrompastscreenshots[16,49],orconductingtemporal
reasoningbasedonnuanceddifferencesbetweenphotographs[18]. Moreover,multi-imageinput
hastheadvantageofconveyingvisuospatialideasdirectly–combiningmultipleimagesofthesame
scenecanrevealspatialrelationsorothermoreabstractrelationsintheworld [15]. Multi-image
inputalsoovercomesthelimitationsofresolutionthatsingleimagesface,allowingforbettervisual
perceptionandunderstanding[27].
Asmultimodallargelanguagemodels(LLMs)[1,3,6,9,13,34,35,39,50,57,58,63,65]havebegun
toshowsuperiorperformanceacrossvarioussingle-imagetasks,wenowexpectthemtosolvehard
tasksthatrequireanholisticunderstandingofmultipleimages. Thisworkaimsathighlightingcrucial
aspectsofmulti-imageunderstandingthathavebeenoverlookedwhenevaluatingmultimodalLLMs,
andprovidingacomprehensivebenchmarkforrobustmulti-imagereasoning. AsshowninFigure2,
currentevaluations[19,30,33,37,38,41,64]generallyfocusonsingle-imageunderstanding,thereby
neglectingthericher,morecomplextasksofintegratingandreasoningacrossmultipleimages. While
manyofthesebenchmarkshavebeenpopularizedasthedefactoevaluationmeasuresforinfluential
modelslikeGPT-4-Turbo[50]andGemini-Pro[58],thisoversightlimitsthepotentialofthesemodels
toconductadvanced-levelmultimodalcomprehension. Thoughsomerecentbenchmarksstartto
includemulti-imagequestionsinevaluation(e.g.,Mantis-Eval[25]andBLINK[17]),theyarefar
frombeingcomprehensiveinmulti-imageevaluationthatinvolvemulti-persectives,multi-relations
androbustnessconcerns.
Inthispaper,weintroduceMUIRBENCH(MULTI-IMAGEUNDERSTANDINGBENCHMARK),a
comprehensivebenchmarkdesignedtorigorouslyassessandevaluatemulti-imageunderstanding
bymultimodalLLMs. MUIRBENCHencompasses11,264imagesand2,600multiple-choiceques-
tionsspanningacross12distinctivemulti-imageunderstandingtasks,e.g.visualretrieval,cartoon
understanding,andattributesimilarity,etc. AsillustratedinFigure1,therecanbemultipleimages
interleaved in the contexts or questions, or presented as choices in our benchmark. Instances in
MUIRBENCHalsocontaindiversekindsofmulti-imagerelations,e.g.temporal,ordered-pages,or
2narrativerelations, etc. asshown inFigure 4. Thequestions and choices areeither derivedfrom
thedatasets,ormanuallywrittenbyexperts. Additionally,MUIRBENCHadoptsapairwisedesign
approach,whereeachquestion-answeringinstanceispairedwithaexpert-annotatedunanswerable
counterpart[51]featuringminimaldifferencesfollowingFigure5. Thisdesignensuresareliable
assessmentofmultimodalLLMs,mitigatingtheriskofachievingcorrectanswersthroughvision
orlanguageshortcuts. Wealsoincludevariousfine-grainedexpertannotatedlabelssuchasimage
positionsandimagetypesinMUIRBENCH,tofacilitatedetailedmodelanalysis.
Weconductacomprehensiveevaluationon MUIRBENCH using20multimodalLLMsofvarious
sizes, including models that accept multi-image inputs and those originally designed for single-
imageinputs. Experimentalresultsunderscorethecurrentlimitationsofeventhemostinfluential
multimodalLLMs,e.g.GPT-4oandGeminiPro,inhandlingmulti-imagescenarios. Forinstance,
GPT-4oandGeminiProachievemere68.0%and49.3%ofaccuracyrespectively,whichare25.1
%and43.8%lowerthanhumanperformance. WealsoshowthatmultimodalLLMsperformmuch
worseonunanswerablequestionsthantheiranswerablecounterparts,withGPT-4oandGeminiPro
exhibitingaccuracygapsof26.8%and21.5%. Furthermore,multimodalLLMstrainedsolelyon
singleimagesdemonstrateimpairedgeneralizationtomulti-imagecontexts. Thesefindingshighlight
thesignificanceofMUIRBENCHindrivingthedevelopmentofmultimodalLLMsintranscending
single-imagelimitations. WebelieveMUIRBENCHcanserveasaneffectivetestbedforholisticmulti-
imageunderstanding,encouragingthecommunitytocultivatemodelswithamorecomprehensive
andintegratedunderstandingofthevisualworld.
2 RelatedWork
2.1 MultimodalUnderstandingBenchmarks
Anumberofrecentbenchmarkshavebeendevelopedtocomprehensivelyassessthemultimodal
understandingandreasoningcapabilitiesofmultimodallanguagemodels(LLMs)[30,37,41–43,64,
66]. However,mostofthesebenchmarksprimarilyfocusonsingle-imagescenarios. Whilesome
benchmarks,suchasMathVista[41],includemulti-imageexamples,theytypicallyrequirelimited
aspectsofcapacities(e.g.,imagecomparisonforMathVista)anddonotprovideacomprehensive
assessmentofmultimodalLLMsinmulti-imagescenarios. Whilesomebenchmarksfeaturevideo
understanding[20,45]orin-contextlearning[26,52],theassessedcapabilitiesarefundamentally
differentfrommulti-imageunderstanding. Videounderstandingfocusesoncontinuousstreamsof
framescapturingdynamicchangesovertime,whilein-contextlearningfocusesontaskadaptation
using few-shot examples. In contrast, multi-image understanding challenges models to integrate
andanalyzespatialandcontextualcuesfromvariedperspectives,settings,andmoments,thereby
simulatingthewayhumansprocessinformationfrommultiplevisualsources. Recently,therehave
beendedicatedeffortstoassessmultimodalLLMsinmulti-imagescenarios. Forexample,MANTIS-
Eval[25]isahuman-annotatedbenchmarkcomprising207examplesformulti-imagereasoning,
suchassizeperceptionsandweightcomparisons. DEMON[31]evaluateswhethermultimodalLLMs
canfollowzero-shotdemonstrativeinstructions. However,thesebenchmarksstillfocusonlimited
multi-imagerelationsorreasoningprocessesandlackofrobustevaluation. Incontrast,MUIRBENCH
providesacomprehensiveassessmentofmultimodalLLMs,coveringabroaderrangeofmulti-image
capacities.
2.2 MultimodalLargeLanguageModels
InspiredbytheremarkableachievementsinrecentLLMs[5,50,60,61,67],aseriesofstudieshave
begunexploringmultimodalLLMsthatcanconcurrentlyinterpretvisualandlinguisticinformation.
However, mostofearlymultimodalLLMsaretrainedonsingle-imagedatasetsandoverlookthe
complicatedtasksofmulti-imageunderstanding[7,13,34]. Recentworkstartstrainingmultimodal
LLMsoninterleavedimage-textcorpussuchasMMC4[70]andOBELICS[28]forpretrainingas
wellasMantis-Instruct[25]forinstructiontuning, whichenablesmodelstogeneratetextsgiven
multiple images. While some of these models, like Flamingo [1], Idefics [28], Emu [55], and
VILA [32], have demonstrated in-context learning capabilities, there is still a lack of evidence
regardingtheircapabilitiesinunderstandingmultipleimageswithinindependentinstances. Although
instructiontunedmodelssuchasMantis[25]andGPT-4-Turbo[50]haveshowntopossesscounting
andcomparisonskillsovermulti-imageinputs, theirabilityinunderstandingandreasoningover
multiple images with different relations across diverse tasks, though critical, remain unexplored.
3Figure 3: Data distribution by tasks in Figure4:Datadistributionbymulti-imagerelation
MUIRBENCH. Moredetailsarein§3. categories. Moredetailsarein§3.
Therefore,weproposeMUIRBENCHtoconductcomprehensiveevaluationandprovideinsightsto
furtherimprovetheircapabilitiesinhandlingrealisticmulti-imagetasks.
3 MUIRBENCH
Our benchmark is meticulously curated for comprehensively assessing multimodal LLMs’ capa-
bilitiesinholisticmulti-imageunderstanding. Weintroducetheoveralldesignandkeyfeaturesof
MUIRBENCHin§3.1,anddelvedeepintothedatacurationprocessin§3.2.
3.1 BenchmarkOverview
Focusingonmulti-imageunderstanding,MUIRBENCHconsistsof11,264imagesand2,600multiple-
choicequestions,withanaverageof4.3imagesperinstance. Ingeneral,MUIRBENCHadheresto
two key design principles. First, it seeks to provide a comprehensive and holistic evaluation on
multimodalLLMs’multi-imageunderstandingcapabilities,bycontaining12diversemulti-image
taskscovering10distinctivemulti-imagerelationcategories. Additionalfine-grainedlabelssuchas
inputimagepositionsandimagetypesarealsoincludedtosupportcomprehensiveanalysisofmodels.
Second,itseekstoprovidearobustevaluation,followingapairwisedesignwhereeachanswerable
instanceispairedwithanunanswerablecounterpartfeaturingminimaldifferences.
ComprehensiveMulti-ImageEvaluation. MUIRBENCHprovidesancomprehensiveassessment
through12distinctivemulti-imageunderstandingtasks,withselectedexamplesofeachtaskshown
inFigure6. AsillustratedinFigure3,eachtaskrepresents2.5%to17.8%ofthewholebenchmark.
[ACTIONUNDERSTANDING]aimstoevaluatetheabilityofmodelstounderstandcontinuousimages
inchronologicalorderandmatchitwithanaction. [ATTRIBUTE SIMILARITY] aimstoevaluate
the ability of models to identify a specific given attribute among multiple images. [CARTOON
UNDERSTANDING]aimstoevaluatetheabilityofmodelstounderstandstoriesconveyedincartoon
images. [COUNTING]aimstoevaluatetheabilityofmodelstocountthenumberofspecificobjects
acrossmultipleimages. [DIAGRAM UNDERSTANDING]aimstoevaluatetheabilityofmodelsto
understandinformationconveyedindiagramimages. [DIFFERENCESPOTTING]aimstoevaluatethe
abilityofmodelstoidentifydifferencesacrossmultipleimages. [GEOGRAPHICUNDERSTANDING]
aims to evaluate the ability of models to understand maps and reason upon geographic features.
[IMAGE-TEXTMATCHING]aimstoevaluatetheabilityofmodelstounderstandthemeaningofa
textsnippetandmatchitwiththecorrespondingvisualcontentorviceversa. [ORDERING]aimsto
evaluatetheabilityofmodelstoorderaseriesofimagesbasedonthetextualdescription. [SCENE
UNDERSTANDING]aimstoevaluatetheabilityofmodelstounderstandascenecomprisedofmultiple
viewsfrommultiplesurveillanceimages. [VISUAL GROUNDING] aimstoevaluatetheabilityof
modelstogroundaspecificobjectandseekinformationaboutitwithinmultipleimages. [VISUAL
RETRIEVAL]aimstoevaluatetheabilityofmodelstoretrievalimagesthatcontainthesamebuilding.
4Figure5: ThreemajorstrategiesareusedinMUIRBENCHtocreateunanswerableinstancesfrom
theiranswerablecounterpartswithminimalchanges(§3.2).Intheaboveexamples,bluemarksdenote
theoriginalinputintheanswerablecase,andredmarkshighlighttheinputintheunanswerablecase.
Additionally,MUIRBENCHincludesimagescovering10variouscategoriesofmulti-imagerelations,
suchasnarrativeimagesconveyingstoriesorideas,orderedpagesofdocumentsandslidesproviding
collectiveinsights,imagesformingatemporalsequencepresentingevents,andmultipleviewsof
objectsor3Dscenesofferingacompletevision,withthecompletedistributionshowninFigure4. In
termsofimagepresentation,thenumberofimagesineachinstancerangesfromtwotonine,while
theinputpositionsofimagescanbethebeginningofquestion,middleofquestion,endofquestion,
options,andamixofthesepositions. MUIRBENCHalsoexhibitsvariousimagetypes,includingbut
notlimitedtoslides,maps,medicalimages,drone/satelliteimages,animations,memes,graphics,and
3Dviews. Thedatadiversityfromtheaforementionedperspectivesenhancesthecomprehensiveness
ofourbenchmark. MoredetailscanbefoundinAppendixA.
RobustEvaluation. Existingdatasetsprimarilyassessmodels’capabilitiesinsolvinganswerable
questions but overlook their ability to recognize what they do not know [47, 51]. In real-world
scenarios,thereisnoguaranteethatuserqueriesareanswerable. AreliablemultimodalLLMshould
directlyindicatewhenaqueryisunanswerableratherthanprovidingananswerthatismostlikely
tobecorrect. Inlightofthis,wepaireachanswerableinstancewithanunanswerablecounterpart,
featuringminimaldifferences,toprovideamorerobustevaluation,simulatingreal-worldscenarios.
Weadoptmultiplestrategiestomanuallydesigntheunanswerableinstances,withmajorstrategiesof
imagereplacingorreordering,questionmodification,andoptionmodificationintroducedinFigure5.
MoredetailscanbefoundinAppendixA.
3.2 DataCollection
Answerable Data Collection. We invest our efforts in collecting multi-image multiple-choice
questionanswering(MCQA)datacoveringvarioustasksandmulti-imagerelations. Diversedata
attributesenablefine-grainedanddiagnosticevaluation,whilethemultiple-choiceformatensures
deterministic results. To achieve this goal, we consider three sources of data, including existing
datasets, dataset derivations, as well as newly collected data. Existing data (40.8%) come from
GeneCIS[62],SeedBench[30],andIconQA[44]. Deriveddata(21.7%)reformatdataintoMCQA
format,usingmultiplestrategiesincludingquestiongeneration,optionrewriting,andsingle-image
QA combination, etc. upon instances from NLVR2 [53], HallusionBench [22], ISVQA [4], and
MMBench[37]. Newdata(37.5%)addresscertaintasks(e.g.geographicunderstandingandvisual
retrieval)thatareunderrepresentedintheaforementionedcollectiontofulfillamorecomprehensive
evaluation. We manually create the question and choices for these data based on images from
the National Geologic Map Database2, University-1652 [68, 69], PubMed papers3, and SciDuet
slides[54]. DetailsaboutcurationprocessanddatasourcesforeachtaskcanbefoundinAppendixA.
UnanswerableDataCollection. AsshowninFigure5,weconsiderthreestrategiesformodifying
ananswerableinstancetoitsunanswerablecounterpartwithminimalchanges. Wefirstreplaceor
reordersomeimagestodisruptthequestion-imageandimage-imagerelations(24.2%). Wealso
modifythequestiontomakeitincompatiblewiththeimagesandoptions(35.3%). Inaddition,we
2https://ngmdb.usgs.gov/ngmdb/ngmdb_home.html
3https://pubmed.ncbi.nlm.nih.gov/
5replaceoptionstocreateascenariowithnocorrectanswer(40.5%). Foreachanswerableinstance,
weapplyoneofthesethreestrategies. MoredetailscanbefoundinAppendixA.
Quality Control. We employ two types of quality control throughout the annotation process:
automaticcheckwithpredefinedrules,andamanualexaminationofeachinstancetofilteroutany
low-qualitydata. Theautomaticcheckverifiesvalidinstanceformat,answers,metadatavalues,and
thecoreferencebetweenimageplaceholdersandimages(ensuringnoredundantimage),aswellas
theaccessibilityofimages. Themanualexaminationisconductedbyfourexpertsworkinginthis
field,andfiltersoutambiguousqueries,unclearimages,andconfusinginstances.
4 Experiments
Inthissection,wefirstdescribetheexperimentalsetupandthebaselines(§4.1). Thenwepresenta
comprehensiveevaluationof20recentmultimodalLLMs(§4.2). Wedemonstratethatwhilehumans
can answer the questions with high accuracy, MUIRBENCH is challenging for existing models.
Finally,weconductvariousanalysesonmultipleexperimentsettings,includingsensitivitytovarious
resolutionanderroranalysis(§4.3).
4.1 ExperimentalSetup
MultimodalLLMs: WeevaluateMUIRBENCHon20recentmultimodalLLMs,includingmodels
designedforconsideringmulti-imageinputsandthoseoriginallydesignedforsingle-imageinputs.For
multi-imageinputmultimodalLLMs,weevaluateonGPT-4o,GPT-4-Turbo[50],GeminiPro[58],
Mantis(Idefics2,clip-llama3,andsiglip-llama3versions;8B)[25],VILA(v1.5-13B)[32],Idefics
(9B-Instructandv2-8B)[28,29],Emu2(Chat)[55]andOpenFlamingo(v2-9B)[2].Forsingle-image
inputmultimodalLLMs,weevaluateonLLaVA(v1.5,NeXT,internLM,andxtunerversions,model
size7B,13B,and34B)[12,34–36,59],Yi-VL-6B4,MiniGPT-4-v2[7],andCogVLM[63]. Werefer
thereaderstoAppendixBformoredetails.
Evaluationsetup: WefollowthestandardsetupasitisinVLMEvalKit[11],wherethetemperature
issetto0andretryissetto10. Forthemodelsthatdonotsupportmultipleimagesasinput,we
concatenatetheimagestoconstituteoneinput. Weextractthechoicefromthemodels’outputwith
a set of pre-defined rules. We refer the readers to Appendix §C for more details on multi-image
concatenation,visualprompting,answerextraction,andthehumanevaluationprotocol.
4.2 MainResults
Overallperformance: AsshowninTable1,theaverageaccuraciesofthemostadvancedmultimodal
LLMsonMUIRBENCHarenobetterthan68%,whicharestillfarfromenablingsatisfactoryutility.
Themeanaccuraciesofopen-sourcemultimodalLLMsthathaveconsideredmulti-imageshover
between23.73%and44.50%,whichfallbehindfromadvancedproprietaryLLMs. Notably,there
is no obvious correlation between model sizes and performances, indicating the importance of
trainingdataandtrainingprocessesindevelopingmultimodalLLMswithmulti-imageunderstanding
capabilities. For certain models and tasks, some results are only on par or even below random
guessing. Weprovidemorein-depthmodelanalysesinthefollowingandinAppendixD.
In which multi-image tasks do multimodal LLMs show relative strengths and weaknesses?
Figure7visualizestheaccuraciesofthebest-performingmodelsonMUIRBENCH. Weobservethat
multimodalLLMsperformrelativelybetteronimage-textmatching,visualretrieval,anddiagram
understanding. Incontrast,multi-imageorderingandvisualgroundingappeartobemorechallenging
for these models, because these tasks require understanding the whole multi-image context and
conductingmorecomplicatedreasoningprocessesacrossimagesandmodalitiesafterwards.
Canmodelsdesignedforsingle-imageinputsperformmulti-imagetasks? Ingeneral,models
acceptingmulti-imageinputs(e.g.,Mantis-8B),evenwithfewerparameters,performbetterthansingle-
imageinputmultimodalLLMs(e.g.,LLaVA-NeXT-34B).Thisobservationshowsthatgeneralizing
4Moredetailsareattheofficialwebsiteathttps://www.01.ai/
6Overall Counting Action. Grounding. Matching. Ordering Scene.
(2,600) (234) (164) (84) (464) (64) (186)
RandomChoice 23.99 20.98 23.41 25.00 24.12 22.81 25.00
Human 93.15 94.87 97.56 85.71 94.83 87.50 94.62
Multi-ImageinputmultimodalLLMs
GPT-4o[50] 68.00 49.15 44.51 36.90 86.85 23.44 71.51
GPT-4-Turbo[50] 62.31 42.31 39.63 53.57 80.39 35.94 59.14
GeminiPro[58] 49.35 28.63 35.98 28.57 66.59 12.50 59.14
Mantis-8B-Idefics2[25] 44.50 38.46 33.54 26.19 53.88 18.75 56.99
Mantis-8B-clip-llama3[25] 37.38 29.06 36.59 21.43 43.32 18.75 56.99
Mantis-8B-siglip-llama3[25] 36.12 27.35 37.20 22.62 43.75 7.81 54.30
Idefics-9B-Instruct[28] 35.43 29.91 28.05 13.10 35.99 12.50 27.41
Emu2-Chat(37B)[55] 33.62 31.20 27.44 26.19 37.28 15.63 48.39
VILA1.5-13B[32] 33.12 19.66 28.66 25.00 40.95 10.94 56.45
Idefics2-8B[29] 26.08 21.79 26.22 26.19 24.78 15.62 56.45
OpenFlamingo-v2-9B[2] 23.73 21.79 26.83 30.95 24.14 21.88 22.58
Single-ImageinputmultimodalLLMs
LLaVA-NeXT-34B[35] 33.31 36.32 26.22 33.33 37.93 21.88 54.30
LLaVA-v1.5-7B-xtuner[12] 33.23 26.92 25.61 23.81 22.84 4.69 39.78
Yi-VL-6B7 28.69 28.21 27.44 28.57 25.00 7.81 38.71
LLaVA-internLM2-7B[59] 28.15 34.19 26.22 32.14 25.65 7.81 42.47
LLaVA-v1.5-13B[34] 24.38 25.21 29.27 14.29 20.26 20.31 36.56
LLaVA-v1.5-7B[34] 23.46 23.08 27.44 14.29 23.49 23.44 34.95
LLaVA-v1.5-13B-xtuner[12] 21.69 23.08 23.17 16.67 21.98 14.06 47.85
CogVLM[63] 20.85 14.10 26.22 16.67 21.34 12.50 41.40
MiniGPT-4-v2[7] 17.35 11.97 14.02 25.00 17.03 18.75 14.52
Difference. Cartoon. Diagram. Geographic. Attribute. Retrieval.
(340) (78) (398) (100) (196) (292)
RandomChoice 23.18 25.00 29.56 25.00 20.00 21.30
Human 92.94 82.05 98.99 98.00 87.76 86.30
Multi-ImageinputmultimodalLLMs
GPT-4o[50] 60.29 51.28 88.69 56.00 56.12 80.14
GPT-4-Turbo[50] 60.59 52.56 79.15 57.00 50.51 64.04
GeminiPro[58] 45.29 47.44 64.82 48.00 41.33 43.84
Mantis-8B-Idefics2[25] 28.82 38.46 67.59 26.00 48.47 35.62
Mantis-8B-clip-llama3[25] 24.12 43.59 54.27 16.00 33.67 31.85
Mantis-8B-siglip-llama3[25] 27.35 46.15 47.99 22.00 31.63 28.08
Idefics-9B-Instruct[28] 34.41 48.72 46.98 35.00 32.65 43.49
Emu2-Chat(37B)[55] 32.65 43.59 37.69 34.00 31.63 23.97
VILA1.5-13B[32] 24.71 30.77 42.71 31.00 24.49 30.14
Idefics2-8B[29] 27.65 39.74 25.38 21.00 17.86 17.12
OpenFlamingo-v2-9B[2] 21.76 25.64 31.91 25.00 18.88 15.41
Single-ImageinputmultimodalLLMs
LLaVA-NeXT-34B[35] 22.06 41.03 38.19 12.00 38.27 25.00
LLaVA-v1.5-7B-xtuner[12] 33.53 29.49 44.72 26.00 38.78 47.60
Yi-VL-6B7 25.59 50.00 35.68 17.00 34.18 22.60
LLaVA-internLM2-7B[59] 19.12 39.74 35.43 12.00 23.98 28.42
LLaVA-v1.5-13B[34] 20.00 25.64 31.66 20.00 22.96 20.89
LLaVA-v1.5-7B[34] 20.00 24.36 25.13 20.00 22.96 19.86
LLaVA-v1.5-13B-xtuner[12] 12.94 30.77 20.10 11.00 18.37 21.58
CogVLM[63] 19.71 41.03 19.60 13.00 16.33 15.75
MiniGPT-4-v2[7] 20.00 21.79 21.61 13.00 17.35 14.73
Table1: ExperimentresultsonMUIRBENCH. Thefirstrowshowstasknamesandnumberoftest
data. Weseethatmostmodelsperformsimilarlytorandomchoice,andarefarfromhumans(§4.3).
fromsingle-imagetrainingtomulti-imageinferenceisnon-trivial. Reasonably,modelsbenefitfrom
multi-imagetrainingdataandlearningprocessestodevelopmulti-imageunderstandingcapabilities.
4.3 Analysis
DomultimodalLLMsperformworseontheunanswerableset? Figure8comparesperformances
on answerable and unanswerable sets for some best-performing models. All the studied models
havesevereperformancedropwhenchanginganswerableinstancestounanswerablecounterparts. A
closerlookoftheerrorcasesrevealsthatmodelsoftenavoidabstentionwhenfacingunanswerable
7ActionUnderstanding 1 2 AttributeSimilarity
Q:What is the action displayedin the video? Q:Whichofthefollowingimagessharesthesamescenewith<img1>
butcontainstheobjectpottedplant?
(a)Placing something onto something 1 (a)None
(b)Flicking something onto something 3 4
(c)Spilling something onto something (b) (c) (d)
(d)Descendsomething onto something
(e)None of the choices provided
CartoonUnderstanding Counting
Q:What isthemaincontentofthiscomicstrip? Q:How many vases have a painted
design all over in the images?
(a)None of the choices provided
(b)Aduckisdrinking (a)One (b)Three (c)Four
(c)AducksitsonthesofawhilewatchingTV,
drinkingandeatingpopcorn (d)Two (e)Noneofthechoices
(d)Aduckisenjoyingitsnightlife
DiagramUnderstanding DifferenceSpotting Q:Can you determine which slide serves a
different function compared to the others?
Q:Whichobjectisbelowthebed?
(a) (b) (c)
(a)None (b)
(c) (d)
(d)None
GeographicUnderstanding Image-TextMatching
Q:Among these map images, which one depicts
1 overlapping geographic regions like<img1>? Q:Which imagehas1appleand5bananas?
(a)None
(b) (c) (d) (a)None (b) (c) (d)
Ordering SceneUnderstanding
Q:The baby attempts to take off the 1 2 3 Q:What's the color of the car parked behind the black van in the given images?
clothes. What is the correct order of
images according to the given context?
(a)3-1-2 (b)2-3-1
(c)2-1-3 (d)Noneofthechoices (a)Green (b)Noneofthechoicesprovided (c)Red (d)White
VisualGrounding VisualRetrieval
Q:This is the McDonald‘s my sister bought<img1>.This is theMcDonald's Q:Can you find the images containing the same building as in <img1>?
$ sp1 e $ n2 t o$ n3 tD ho isl l Mar c M De on nu a l< di 'm s?g2>.Could you please tell me how much my sister 1 (a) (c) (d)
1 2
(a)3dollars
(b)1dollar
(c)Noneoftheoptions
(d)9dollars
(b)Noneofthechoicesprovided
Ground Truth GPT-4o GeminiPro Mantis
Figure6: QualitativeresultsonMUIRBENCH. Foreachtask,weshowtheground-truthanswerin
blue, andchoiceofGPT-4o[50], GeminiPro[58], andMantis-8B-Idefics2[25]. Noticethatthe
examplecasesareslightlymodifiedwithchangeofwordandimagereductionforbetterillustration.
questions. Theseobservationsnotonlyhighlighttheimportanceofassessingmodelbehaviorundera
morerealisticsetting,butalsoshowthatthepairwisedesignimprovesthereliabilityofMUIRBENCH.
Error analysis of GPT-4o: We randomly sampled 100 error instances made by GPT-4o on
MUIRBENCH and meticulously examined them. The most common error category (26% of er-
rorcases)isthefailureofcapturingdetailsinimages. Therest20%oferrorsareduetoinaccurate
objectcountingorreasoning,followedbyerrorsinlogicalreasoning(18%),identificationofthe
sameobjectindifferentscenes(14%),andinferringtheintentsimpliedbyimagesequences(12%).
8Image-TextMatching
Ordering Visual Grounding Answerable Unanswerable
100
Scene Action
Understanding Understanding 75
50
Difference
Spotting 40 60 80 100 Counting
25
0
UndC ea rr st to ao nn d ing Visual Retrieval GPT-4o GPT-4-Turbo Gemini MPr ao ntis-8B-Idefic Ls L2 aVA-NeXT-34B
Diagram Attribute
Understanding Geographic Similarity Figure 8: Model performance on answerable
Understanding
and unanswerable instances. An obvious per-
LLaVA-NeXT-34B formancegapcanbeobservedbetweenthetwo
Figure7: Modelperformancebytasks. setsonallbest-performingmodels.
QualitativeResults: Figure6presentssomequalitativeresults,onepertask. Anotablephenomenon
isthatmultimodalLLMsmayhallucinatebyattemptingtofindanerroneousoptionthatappears
tobelikelycorrectforanunanswerablequestionratherthanabstaining(seeexamplesforcartoon
understanding,diagramunderstanding,visualgrounding,andvisualretrieval). Thisillustratesthe
obviousperformancegapbetweenanswerableandunanswerableinstancesinFigure8. Wereferthe
readerstoAppendix§Dformorein-depthdiscussions.
5 Conclusion
Inthiswork,weintroducedMUIRBENCH,acomprehensivebenchmarkdesignedtoprovidearobust
evaluationonthemulti-imageunderstandingcapabilitiesofmultimodalLLMs. Experimentalresults
of 20 multimodal LLMs, including the prominent models like GPT-4 and Gemini Pro, revealed
substantial limitations in their ability to handle multi-image scenarios. These models showed
significantperformancedeficitscomparedtohumanaccuracyandstruggledmorewithunanswerable
questionsin MUIRBENCH. OurfindingsunderscoretheneedformultimodalLLMstotranscend
single-imagelimitationsandachievemoreholisticvisualcomprehension. MUIRBENCHprovidesa
rigorousframeworkforsuchassessments,encouragingthecommunitytodevelopmodelsthatcan
effectivelysynthesizeandreasonacrossmultiplevisualsources.
References
[1] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, AntoineMiech, IainBarr, YanaHasson,
KarelLenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisual
languagemodelforfew-shotlearning. AdvancesinNeuralInformationProcessingSystems,
35:23716–23736,2022.
[2] AnasAwadalla,IrenaGao,JoshGardner,JackHessel,YusufHanafy,WanrongZhu,Kalyani
Marathe,YonatanBitton,SamirGadre,ShioriSagawa,JeniaJitsev,SimonKornblith,PangWei
Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig Schmidt. Openflamingo: An open-
source framework for training large autoregressive vision-language models. arXiv preprint
arXiv:2308.01390,2023.
[3] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,Chang
Zhou, and Jingren Zhou. Qwen-vl: A versatile vision-language model for understanding,
localization,textreading,andbeyond,2023.
[4] AnkanBansal,YutingZhang,andRamaChellappa. Visualquestionansweringonimagesets.
InEuropeanConferenceonComputerVision,pages51–67,2020.
[5] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
9[6] JuanManuelZambranoChaves,Shih-ChengHuang,YanboXu,HanwenXu,NaotoUsuyama,
Sheng Zhang, Fei Wang, Yujia Xie, Mahmoud Khademi, Ziyi Yang, et al. Training small
multimodalmodelstobridgebiomedicalcompetencygap: Acasestudyinradiologyimaging.
arXivpreprintarXiv:2403.08002,2024.
[7] JunChen,DeyaoZhu,XiaoqianShen,XiangLi,ZechunLiu,PengchuanZhang,Raghuraman
Krishnamoorthi,VikasChandra,YunyangXiong,andMohamedElhoseiny. Minigpt-v2: large
languagemodelasaunifiedinterfaceforvision-languagemulti-tasklearning,2023.
[8] JunChen,DeyaoZhu,XiaoqianShen,XiangLi,ZechunLiu,PengchuanZhang,Raghuraman
Krishnamoorthi,VikasChandra,YunyangXiong,andMohamedElhoseiny. Minigpt-v2: large
languagemodelasaunifiedinterfaceforvision-languagemulti-tasklearning. arXivpreprint
arXiv:2310.09478,2023.
[9] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,ZhongMuyan,Qinglong
Zhang,XizhouZhu,LeweiLu,etal. Internvl:Scalingupvisionfoundationmodelsandaligning
forgenericvisual-linguistictasks. arXivpreprintarXiv:2312.14238,2023.
[10] Neil Cohn, Ryan Taylor, and Kaitlin Pederson. A picture is worth more words over time:
Multimodality and narrative structure across eight decades of american superhero comics.
MultimodalCommunication,6(1):19–37,2017.
[11] OpenCompassContributors. Opencompass: Auniversalevaluationplatformforfoundation
models. https://github.com/open-compass/opencompass,2023.
[12] XTunerContributors. Xtuner:Atoolkitforefficientlyfine-tuningllm. https://github.com/
InternLM/xtuner,2023.
[13] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng
Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose
vision-languagemodelswithinstructiontuning,2023.
[14] YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu,XinggangWang,TiejunHuang,
Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation
learningatscale. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages19358–19369,2023.
[15] OlivierFaugeras,Quang-TuanLuong,andTheoPapadopoulo. Thegeometryofmultipleimages:
thelawsthatgoverntheformationofmultipleimagesofasceneandsomeoftheirapplications.
MITpress,2001.
[16] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical in-
teractionthroughvideoprediction. Advancesinneuralinformationprocessingsystems,29,
2016.
[17] XingyuFu,YushiHu,BangzhengLi,YuFeng,HaoyuWang,XudongLin,DanRoth,NoahA
Smith,Wei-ChiuMa,andRanjayKrishna. Blink: Multimodallargelanguagemodelscansee
butnotperceive. arXivpreprintarXiv:2404.12390,2024.
[18] Xingyu Fu, Ben Zhou, Ishaan Chandratreya, Carl Vondrick, and Dan Roth. There’s a time
andplaceforreasoningbeyondtheimage. InProceedingsofthe60thAnnualMeetingofthe
AssociationforComputationalLinguistics(Volume1: LongPapers),May2022.
[19] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh. MakingtheV
inVQAmatter: ElevatingtheroleofimageunderstandinginVisualQuestionAnswering. In
ConferenceonComputerVisionandPatternRecognition(CVPR),2017.
[20] KristenGrauman,AndrewWestbury,EugeneByrne,ZacharyChavis,AntoninoFurnari,Rohit
Girdhar,JacksonHamburger,HaoJiang,MiaoLiu,XingyuLiu,etal. Ego4d: Aroundtheworld
in3,000hoursofegocentricvideo. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages18995–19012,2022.
[21] GeorgeLGropper. Whyisapictureworthathousandwords? AudioVisualCommunication
Review,pages75–95,1963.
10[22] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang,
LichangChen,FurongHuang,YaserYacoob,etal. Hallusionbench: Anadvanceddiagnostic
suiteforentangledlanguagehallucination&visualillusioninlargevision-languagemodels.
arXivpreprintarXiv:2310.14566,2023.
[23] JamesHaysandAlexeiAEfros. Im2gps: estimatinggeographicinformationfromasingle
image. In2008ieeeconferenceoncomputervisionandpatternrecognition,pages1–8.IEEE,
2008.
[24] AnneNielsenHibbingandJoanLRankin-Erickson.Apictureisworthathousandwords:Using
visualimagestoimprovecomprehensionformiddleschoolstrugglingreaders. Thereading
teacher,56(8):758–770,2003.
[25] DongfuJiang,XuanHe,HuayeZeng,CongWei,MaxKu,QianLiu,andWenhuChen. Mantis:
Interleavedmulti-imageinstructiontuning. arXivpreprintarXiv:2405.01483,2024.
[26] YixingJiang,JeremyIrvin,JiHunWang,MuhammadAhmedChaudhry,JonathanHChen,and
AndrewYNg. Many-shotin-contextlearninginmultimodalfoundationmodels. arXivpreprint
arXiv:2405.09798,2024.
[27] Michal Kawulok, Pawel Benecki, Szymon Piechaczek, Krzysztof Hrynczenko, Daniel
Kostrzewa, and Jakub Nalepa. Deep learning for multiple-image super-resolution. IEEE
GeoscienceandRemoteSensingLetters,17(6):1062–1066,2019.
[28] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton
Lozhkov,ThomasWang,SiddharthKaramcheti,AlexanderRush,DouweKiela,etal. Obelics:
Anopenweb-scalefiltereddatasetofinterleavedimage-textdocuments. AdvancesinNeural
InformationProcessingSystems,36,2024.
[29] HugoLaurençon,LéoTronchon,MatthieuCord,andVictorSanh. Whatmatterswhenbuilding
vision-languagemodels?,2024.
[30] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying
Shan. Seed-bench-2: Benchmarking multimodal large language models. arXiv preprint
arXiv:2311.17092,2023.
[31] JunchengLi,KaihangPan,ZhiqiGe,MingheGao,WeiJi,WenqiaoZhang,Tat-SengChua,
SiliangTang,HanwangZhang,andYuetingZhuang. Fine-tuningmultimodalllmstofollow
zero-shotdemonstrativeinstructions. InTheTwelfthInternationalConferenceonLearning
Representations,2024.
[32] JiLin,HongxuYin,WeiPing,YaoLu,PavloMolchanov,AndrewTao,HuiziMao,JanKautz,
MohammadShoeybi,andSongHan. Vila: Onpre-trainingforvisuallanguagemodels,2023.
[33] FangyuLiu,GuyEmerson,andNigelCollier. Visualspatialreasoning. Transactionsofthe
AssociationforComputationalLinguistics,11:635–651,2023.
[34] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual
instructiontuning,2023.
[35] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee.
Llava-next: Improvedreasoning,ocr,andworldknowledge,January2024.
[36] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning.Advances
inneuralinformationprocessingsystems,36,2024.
[37] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,
JiaqiWang,ConghuiHe,ZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelanall-around
player? arXivpreprintarXiv:2307.06281,2023.
[38] Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu
Liu,MingruiChen,ChunyuanLi,LianwenJin,etal. Onthehiddenmysteryofocrinlarge
multimodalmodels. arXivpreprintarXiv:2305.07895,2023.
11[39] JiasenLu,ChristopherClark,SanghoLee,ZichenZhang,SavyaKhosla,RyanMarten,Derek
Hoiem,andAniruddhaKembhavi. Unified-io2: Scalingautoregressivemultimodalmodels
withvision,language,audio,andaction. arXivpreprintarXiv:2312.17172,2023.
[40] JiasenLu,ChristopherClark,SanghoLee,ZichenZhang,SavyaKhosla,RyanMarten,Derek
Hoiem,andAniruddhaKembhavi. Unified-io2: Scalingautoregressivemultimodalmodels
withvision,language,audio,andaction. arXivpreprintarXiv:2312.17172,2023.
[41] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao
Cheng,Kai-WeiChang,MichelGalley,andJianfengGao. Mathvista: Evaluatingmathematical
reasoningoffoundationmodelsinvisualcontexts. InTheTwelfthInternationalConferenceon
LearningRepresentations,2024.
[42] PanLu,RanGong,ShibiaoJiang,LiangQiu,SiyuanHuang,XiaodanLiang,andSong-Chun
Zhu. Inter-GPS:Interpretablegeometryproblemsolvingwithformallanguageandsymbolic
reasoning. InThe59thAnnualMeetingoftheAssociationforComputationalLinguistics(ACL),
2021.
[43] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind
Tafjord,PeterClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathought
chainsforsciencequestionanswering. InThe36thConferenceonNeuralInformationProcess-
ingSystems(NeurIPS),2022.
[44] PanLu,LiangQiu,JiaqiChen,TonyXia,YizhouZhao,WeiZhang,ZhouYu,XiaodanLiang,
andSong-ChunZhu. Iconqa: Anewbenchmarkforabstractdiagramunderstandingandvisual
language reasoning. In Thirty-fifth Conference on Neural Information Processing Systems
DatasetsandBenchmarksTrack(Round2),2021.
[45] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFahadShahbazKhan. Video-chatgpt:
Towardsdetailedvideounderstandingvialargevisionandlanguagemodels. arXivpreprint
arXiv:2306.05424,2023.
[46] AhmedMasry,DoLong,JiaQingTan,ShafiqJoty,andEnamulHoque. ChartQA:Abenchmark
for question answering about charts with visual and logical reasoning. In Findings of the
AssociationforComputationalLinguistics: ACL2022,pages2263–2279,Dublin,Ireland,May
2022.AssociationforComputationalLinguistics.
[47] AtsuyukiMiyai,JingkangYang,JingyangZhang,YifeiMing,QingYu,GoIrie,YixuanLi,Hai
Li,ZiweiLiu,andKiyoharuAizawa. Unsolvableproblemdetection:Evaluatingtrustworthiness
ofvisionlanguagemodels. arXivpreprintarXiv:2403.20331,2024.
[48] HosseinNouriandAbdusShahid. Theeffectofpowerpointpresentationsonstudentlearning
andattitudes. Globalperspectivesonaccountingeducation,2:53,2005.
[49] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-
conditionalvideopredictionusingdeepnetworksinatarigames.Advancesinneuralinformation
processingsystems,28,2015.
[50] OpenAI. Gpt-4technicalreport,2023.
[51] PranavRajpurkar,RobinJia,andPercyLiang. Knowwhatyoudon’tknow:Unanswerableques-
tionsforsquad.InProceedingsofthe56thAnnualMeetingoftheAssociationforComputational
Linguistics(Volume2: ShortPapers),pages784–789,2018.
[52] Mustafa Shukor, Alexandre Rame, Corentin Dancette, and Matthieu Cord. Beyond task
performance: evaluatingandreducingtheflawsoflargemultimodalmodelswithin-context-
learning. InTheTwelfthInternationalConferenceonLearningRepresentations,2023.
[53] AlaneSuhr,StephanieZhou,AllyZhang,IrisZhang,HuajunBai,andYoavArtzi. Acorpusfor
reasoningaboutnaturallanguagegroundedinphotographs. InProceedingsofthe57thAnnual
MeetingoftheAssociationforComputationalLinguistics,pages6418–6428,2019.
12[54] EdwardSun,YufangHou,DakuoWang,YunfengZhang,andNancyXRWang.D2s:Document-
to-slidegenerationviaquery-basedtextsummarization. InProceedingsofthe2021Conference
of the North American Chapter of the Association for Computational Linguistics: Human
LanguageTechnologies,pages1405–1418,2021.
[55] QuanSun,YufengCui,XiaosongZhang,FanZhang,QiyingYu,ZhengxiongLuo,YuezeWang,
YongmingRao,JingjingLiu,TiejunHuang,etal. Generativemultimodalmodelsarein-context
learners. arXivpreprintarXiv:2312.13286,2023.
[56] QuanSun,YuxinFang,LedellWu,XinlongWang,andYueCao. Eva-clip: Improvedtraining
techniquesforclipatscale. arXivpreprintarXiv:2303.15389,2023.
[57] ChameleonTeam. Chameleon: Mixed-modalearly-fusionfoundationmodels. 2024.
[58] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,
RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighly
capablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[59] InternLM Team. Internlm: A multilingual language model with progressively enhanced
capabilities. https://github.com/InternLM/InternLM,2023.
[60] MosaicMLNLPTeam. Introducingmpt-7b: Anewstandardforopen-source,commercially
usablellms,2023. Accessed: 2023-05-05.
[61] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[62] SagarVaze,NicolasCarion,andIshanMisra. Genecis: Abenchmarkforgeneralconditional
imagesimilarity. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages6862–6872,2023.
[63] WeihanWang,QingsongLv,WenmengYu,WenyiHong,JiQi,YanWang,JunhuiJi,Zhuoyi
Yang,LeiZhao,XixuanSong,JiazhengXu,BinXu,JuanziLi,YuxiaoDong,MingDing,and
JieTang. Cogvlm: Visualexpertforpretrainedlanguagemodels,2023.
[64] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,
DongfuJiang,WeimingRen,YuxuanSun,etal.Mmmu:Amassivemulti-disciplinemultimodal
understandingandreasoningbenchmarkforexpertagi. arXivpreprintarXiv:2311.16502,2023.
[65] RenruiZhang, JiamingHan, AojunZhou, XiangfeiHu, ShilinYan, PanLu, HongshengLi,
PengGao,andYuQiao. Llama-adapter: Efficientfine-tuningoflanguagemodelswithzero-init
attention. InInternationalConferenceonLearningRepresentations(ICLR),2024.
[66] RenruiZhang,DongzhiJiang,YichiZhang,HaokunLin,ZiyuGuo,PengshuoQiu,AojunZhou,
PanLu,Kai-WeiChang,PengGao,andHongshengLi. Mathverse: Doesyourmulti-modalllm
trulyseethediagramsinvisualmathproblems? arXivpreprintarXiv:2403.14624,2024.
[67] LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,
ZiLin,ZhuohanLi,DachengLi,EricXing,etal. Judgingllm-as-a-judgewithmt-benchand
chatbotarena. AdvancesinNeuralInformationProcessingSystems,36,2024.
[68] ZhedongZheng,YujiaoShi,TingyuWang,JunLiu,JianwuFang,YunchaoWei,andTat-seng
Chua. Uavm’23: 2023 workshop on uavs in multimedia: Capturing the world from a new
perspective. InProceedingsofthe31stACMInternationalConferenceonMultimedia,pages
9715–9717,2023.
[69] Zhedong Zheng, Yunchao Wei, and Yi Yang. University-1652: A multi-view multi-source
benchmarkfordrone-basedgeo-localization. ACMMultimedia,2020.
[70] WanrongZhu,JackHessel,AnasAwadalla,SamirYitzhakGadre,JesseDodge,AlexFang,
YoungjaeYu,LudwigSchmidt,WilliamYangWang,andYejinChoi. Multimodalc4: Anopen,
billion-scalecorpusofimagesinterleavedwithtext. AdvancesinNeuralInformationProcessing
Systems,36,2024.
13Appendices
Table of Contents
A MUIRBENCHDetails 14
A.1 DatasetStatistics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.2 DatasetCurationDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
A.3 Multi-imageRelations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.4 HumanEvaluationProtocol . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
B BaselineModels 17
C ExperimentSettingDetails 18
C.1 ModelPrompts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
C.2 EvaluationTool . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
D ErrorAnalysis 18
E Limitations 19
E.1 Limitationandfuturework . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
E.2 Societalimpacts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
F License 19
G AccessibilityofMUIRBENCH 20
G.1 DatasetDocumentationandFormat . . . . . . . . . . . . . . . . . . . . . . . . 20
G.2 LinksandMaintenancePlan. . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
G.3 AuthorStatement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
G.4 IntendedUses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A MUIRBENCH Details
A.1 DatasetStatistics
Figure9presentstheoverallstatisticsof MUIRBENCH. Figure10showsthedatadistributionby
thetypeofimages. MUIRBENCHcoversawiderangeofimagetypes,rangingfromcommontypes
likephotographytospecificareassuchasmedicalimages,slides,anddroneandsatelliteimagery.
Figure 11 demonstrates the data distribution by the number of images. MUIRBENCH contains
instancesrangingfromtwoimagestonineimages. Figure12presentsthedatadistributionbythe
positionofimages,includingthebeginning/middle/endofaquestion,options,andamixofthese
positions.
A.2 DatasetCurationDetails
Answerable Data Collection. We invest our efforts in collecting multi-image multiple-choice
questionanswering(MCQA)datacoveringvarioustasksandmulti-imagerelations. Diversedata
attributesenablefine-grainedanddiagnosticevaluation,whilethemultiple-choiceformatensures
deterministic results. To achieve this goal, we consider three sources of data, including existing
datasets,datasetderivations,aswellasnewlycollecteddata. Existingdatacomefromdatasetsthat
focus on a single aspect of multi-image reasoning, such as GeneCIS [62]; and from datasets not
specificallydesignedforthemulti-imagesettingbutcontainingaportionofmulti-imagedata,such
asSeedBench[30]andIconQA[44]. Forafairrepresentationofeachtask,wesampleupto200test
examplesfromeachdataset. Thispartcontributes40.8%ofthedatainthefinalbenchmark. Derived
14TotalInstances 2600
TotalImages 11264
TotalTasks 12
TotalImageRelations 10
AnswerableInstances 1300
-existingdata 531(40.8%)
-deriveddata 282(21.7%)
-newdata 487(37.5%)
UnanswerableInstances 1300
-changeimage 315(24.2%)
-changequestion 459(35.3%)
-changeoption 526(40.5%)
Averageimagenumber 4.3
Averagequestionlength 21.6
Averageoptionlength 3.7
Averageoptionnumber 4.4
Figure 9: Overall statistics of
MUIRBENCH. Figure10: Datadistributionbytypeofimages.
Figure11: Datadistributionbynumber Figure12: Datadistributionbyposition
ofimages. ofimages.
datareformatbinaryQA,suchasNLVR2[53]andHallusionBench[22],intoMCQAbymodifying
questionsandoptions;orrewritingopenQA,suchasISVQA[4],intoMCQAbyaddingoptions;and
reconstructingsingle-imageMCQA,suchasMMBench[37],intomulti-imageMCQAbyreplacing
textoptionswithcorrespondingimages. Similartothosefromtheexistingdatasets,wesampleupto
200testexamplesfromeachdataset. Thispartcontributes21.7%ofthedatainthefinalbenchmark.
Newdataaddresscertaintasks(e.g.geographicunderstanding),imagerelations(e.g.multiview),and
types(e.g.medicalimages)remainingabsentorunderrepresentedintheaforementionedcollectionto
fulfilamorecomprehensiveevaluation. Wepresentfournewdatasets: HistoricalMap,UnivBuilding,
PubMedMQA,andSciSlides. HistoricalMaprequiresidentifyingmappatchescoveringthesame
regionscollectedfromtheNationalGeologicMapDatabase.5 UnivBuildingrequiresidentifying
differentviewsofthesamebuilding,orbuildingsfromthesameuniversities. Theimagedataarefrom
University-1652[68,69]. PubMedMQAcontainsquestionsregardingthesubfiguresfrommedical
papers on PubMed.6 SciSlides consists of questions regarding the slides for paper presentation
collectedfromSciDuet[54]. Thispartcontributes37.5%ofthedatainthefinalbenchmark.
5https://ngmdb.usgs.gov/ngmdb/ngmdb_home.html
6https://pubmed.ncbi.nlm.nih.gov/
15UnanswerableDataCollection. AsshowninFigure5,weconsiderthreestrategiesformodifying
ananswerableinstancetoitsunanswerablecounterpartwithminimalchanges. Wefirstreplaceor
reordersomeimagestodisruptthequestion-imageandimage-imagerelations. Wealsomodifythe
questiontomakeitincompatiblewiththeimagesandoptions. Inaddition,wereplaceoptionsto
createascenariowithnocorrectanswer. Foreachanswerableinstance,weapplyoneofthesethree
strategies. Amongalltheinstances,24.2%oftheunanswerableinstancesarecreatedbyreplacing
orreorderingtheimagesintheiranswerablecounterparts,35.3%bymodifyingthequestions,and
40.5%bychangingtheoptions. Thisstepdoublesthesizeofdata,leadingtoabalanceddistribution
ofanswerableandunanswerableinstances.
MetadataAnnotation. Fine-grainedmetadataenableadiagnosticanalysisofmultimodalLLMs’
weaknesses across various aspects. We annotate image relations, tasks, image types, number of
images,andimagepositionsforallinstances. Amongalloftheseattributes,imagerelationsarea
crucialfactorthatinfluencesthemodel’scapabilityformulti-imagereasoning,yettheyarerarely
annotatedinexistingdata.Therefore,wemanuallyannotatethem.Tasksandimagetypesarepartially
annotatedinexistingdata. Wematchtheexistingcategorieswithourtaxonomyandmanuallyfill
inanymissingones. Numberofimagesandimagepositionsareautomaticallydetectable,sowe
conductautomaticannotation. TheannotationinterfaceisshowninFigure16.
Quality Control. We employ two types of quality control throughout the annotation process:
automaticcheckwithpredefinedrules,andamanualexaminationofeachinstancetofilteroutany
low-qualitydata. Theautomaticcheckverifiesvalidinstanceformat,answers,metadatavalues,and
thecoreferencebetweenimageplaceholdersandimages(ensuringnoredundantimage),aswellas
theaccessibilityofimages. Themanualexaminationatlastfiltersoutambiguousqueries,unclear
images,andinstanceswithothererrors,resultingintheretentionof86.3%ofinstances.
A.3 Multi-imageRelations
MUIRBENCHconsistsof10multi-imagerelations:
• TemporalRelation:Imagesarerelatedbytime,showingprogressionorchangeoveraperiod.
Examplesincludetime-lapsephotographyorsequentialframesfromavideo.
• Ordered Pages: Images are part of a sequence, such as pages in a book or slides in a
presentation,wheretheorderconveysmeaning.
• ComplementaryRelation:Imagesthat,whenviewedtogether,provideadditionalinformation
orcontextthatenhancestheunderstandingofthesubject. Theycomplementeachotherby
fillingingapsorprovidingdifferentperspectives.
• Cropped/ZoomedImages:Oneimageisazoomed-inorcroppedversionofanother,focusing
onaspecificpartoftheoriginalimagetohighlightdetails.
• Narrative: Aseriesofimagesthattogethertellastoryorconveyasequenceofevents,much
likeacomicstriporastoryboard.
• Scene-Multiview: Multipleimagesofthesamescenetakenfromdifferentanglesorperspec-
tives,providingamorecomprehensiveviewofthescene.
• Object-Multiview: Imagesofthesameobjectcapturedfromvariousanglesorperspectives,
usefulforunderstandingtheobject’sthree-dimensionalshape.
• OverallSimilarity: Imagesthataregenerallysimilarincontent,style,orsubjectmatter,but
notnecessarilyidentical. Theymightsharecommonthemesorvisualelements.
• PartialSimilarity: Imagesthatsharesome,butnotall,elements. Theymighthaveoverlap-
pingfeaturesorsubjectsbutalsocontaindistinctdifferences.
• IndependentImages: Imagesthatdonothaveaclearrelationtoeachother. Theyarenot
connectedbytime,sequence,context,orcontent.
A.4 HumanEvaluationProtocol
Twoexpertsindomainconductthehumanevaluation. Eachanswerableinstanceanditsunanswerable
counterpartsarerandomlyassignedtodifferentexpertsensuringafairevaluation. Theinterfacefor
humanevaluationisshowninFigure13.
16Figure13: Humanevaluationinterface.
B BaselineModels
WeevaluateMUIRBENCHon20recentmultimodalLLMs,includingmodelsdesignedforconsidering
multi-imageinputsandthoseoriginallydesignedforsingle-imageinputs. Formostmodelfamilies,
weusethelatestandbest-performingavailablecheckpointtodate. Thelistofbaselinemodelsareas
follows:
(i-ii)GPT-4[50]isknowntobeoneofthebestmultimodalmodelstodate. Wetestwithtwomost
up-to-datecheckpoints: gpt-4-turboandgpt-4o. NoticethattheGPT-4performancewouldchangeif
thisspecificcheckpointgetsupdated. (iii)GeminiPro[58]isoneofthemostpowerfulmultimodal
models,andweusetheGemini1.0ProVisionversionofit. (iv-vi)Mantis(Idefics2,clip-llama3,
andsiglip-llama3versions;8B)[25]isarecentstrongmodelspecificallyfinetunedformulti-image
relatedtasks. (vii)VILA(v1.5-13B)[32],(viii-ix)Idefics(9B-Instructandv2-8B)[28,29],(x)Emu2
(Chat) [55] and (xi) OpenFlamingo (v2-9B) [2] are four recent multimodal models that can take
17multipleimagesasinput. (xii-xvii)LLaVA(v1.5,NeXT,internLM,andxtunerversions,modelsize
7B,13B,and34B)[12,34–36,59]areincludedaswell. Whilethey’redesignedforsingle-image
input, we concatenate all the images in order. (xviii) Yi-VL-6B7 has shown great performance
recently. (xix)MiniGPT-4-v2[8]adaptsEVA[14]asvisualbackbone,LLaMA2-chat(7B)[61]as
languagemodelbackbone,anddesignsalinearprojectionlayerforvisualunderstandingabilities.
(xx)CogVLM[63]addsatrainablevisualexpertmoduleintheattentionandFFNlayerstobridge
differentmodalitiesbetter. ItusesEVA-CLIP[56]asvisionencoderandVicuna[67]aslanguage
backbone.
C ExperimentSettingDetails
C.1 ModelPrompts
Following [41],8 our prompt consists of four parts, the question, options, the hint indicating the
answerformat,andaprefixoftheanswer. Forimages,weinsertthemintothetexttoformacoherent
prompt. Thecompletepromptisasfollows:
ModelPrompts
Question:{QUESTION}
Choices:
(A){OPTION_A}
(B){OPTION_B}
(C){OPTION_C}
(D){OPTION_D}
Hint:Pleaseprovidethecorrectoptionletter,suchasA,B,C,D,directly.
Answer:
C.2 EvaluationTool
Following[64],Weusearule-basedautomatictool9toextracttheexactanswer. First,thetooldetects
ifavalidoptionindexappearsinthemodeloutput. Ifnodirectanswerisfound,thetoolmatchesthe
outputtothecontentofeachoption. Ifthereisstillnomatch,itwillrandomlyselectanoptionasthe
answer. Whenmorethanonevalidanswerisdetected,thetoolwillusethefirstonethatappearsas
thefinalanswer.
D ErrorAnalysis
Figure 14: Model performance by image posi- Figure15: Modelperformancebyunanswerable
tions. types.
7Moredetailsareattheofficialwebsiteathttps://www.01.ai/
8https://github.com/lupantech/MathVista/blob/9ed0e8b52c0911e31faa75308082af5dcf8e63b2/
evaluation/build_query.py#L152
9https://github.com/MMMU-Benchmark/MMMU/blob/f3e473e1e7af2c65a56ab66d7b3cf09c5dbaf0b9/
eval/utils/eval_utils.py#L10
18Doimagepositionscorrelatewitherrorrates?Weanalyzetheerrorratesofvaryinginputpositions
ofimagesandreporttheperformanceofGPT-4o,GeminiProVision,andMantis-8B-Idefics2. As
showninFigure14,thehighestaccuracyisachievedwhenimagesarepositionedinoptions,whilethe
highesterrorratecanbeobservedwhenimagesareinthemiddleofquestions. Thisconsistenttrend
acrossdifferentmodelssuggeststhatthepositionofimageswithinaquestioncorrelateswiththe
errorrate. Thecauseofhighererrorratesmightbethatimagesinthemiddleorendofaquestionmay
interrupttheflowofcontextprocessing,increasingcomplexityandthusreducingmodelperformance.
Itmayalsobeattributedtothetrainingprocess. Thesemodelsmayhaveseenlessdatawithimages
inthemiddleduringtraining.
Dounanswerabletypescorrelatewitherrorrates? Wefurtheranalyzetheerrorratesofvarying
unanswerabletypesandreporttheperformanceofthesamethreemodelsinFigure15. Resultsshow
that the error rate also correlates with the type of unanswerable instances. All the three models
performrelativelybetterwhenweonlychangethequestionstomakeitincompatiblewithoriginal
imagesandoptions. However,allmodelsareconfusedwhenthecorrectoptionisremovedandfail
tochoose“noneoftheotheroptions”inthisscenario. Theperformanceonunanswerableinstances
createdbyreorderingorreplacingimagesisdivergent. GPT-4operformsmuchbetterthantheother
modelsinthesecases.
E Limitations
E.1 Limitationandfuturework
Thereareseverallimitationstothiswork.First,wefocusourscopeon2Dimages,andfutureresearch
canfurtherextendtheideaofworkto3Dproblems,andincludemoremulti-imagetasksandrelation
categories. Wehopeourworkcanguidefutureeffortsinprovidingrobustandfaithfulevaluation
inmultimodalbenchmarks. Ourstrategiesofcreatingunanswerableinstances,asinFigure5,do
notcoverallstrategiesthatcanbeusedtocreatesuchinstances. Also,wefocusourevaluationson
multimodallargelanguagemodels. Futureworkcouldincludemorevision-languagefoundation
modelssuchasUnified-IO2[40]andChameleon[57].
E.2 Societalimpacts
Ourworkproposes MUIRBENCH,providingarobustevaluationonmulti-imagetasksusingmul-
timodalLLMs. Whileitincludesacomprehensivelistof12tasks,allofthemareinEnglishand
couldinducebiasonmultilingualresearchsettings. Also,ifmisused,themultimodalLLMsmay
be used to generate harmful vision and text artifacts. Nevertheless, this is not directly related to
ourresearch,andthedatawecuratedonotcontainpersonallyidentifiableinformationoroffensive
content. However,moreresearchersshouldbeencouragedtogetinvolvedinresearchonthesafety
issuesinamultimodalcontext.
F License
WereleaseourdataunderCC-BY4.0license. Forspecificinstanceswefollowtheiroriginallicenses.
Thedatasetsweusedandtheirlicensesareasfollows:
• GeneCISisreleasedundertheCC-BY-NC4.0license.10
• SEED-BenchisreleasedundertheCC-BY-NC4.0license.11
• IconQAisreleasedundertheCCBY-NC-SAlicense.12
• NLVR2isreleasedundertheCC-BY-4.0license.13
• HallusionBenchisreleasedundertheBSD3-Clauselicense.14
10https://github.com/facebookresearch/genecis/tree/main?tab=readme-ov-file#license
11https://huggingface.co/datasets/AILab-CVC/SEED-Bench
12https://iconqa.github.io/
13https://github.com/lil-lab/nlvr/tree/master?tab=readme-ov-file#licensing
14https://github.com/tianyi-lab/HallusionBench?tab=readme-ov-file#license
19• ISVQA annotation is released under the CC BY-NC-SA 2.0 license.15 We only use the
imagesfromnuScenes,whichisreleasedundertheCCBY-NC-SA4.0license.16
• MMBenchisreleasedundertheApache-2.0license.17
• NationalGeologicMapDatabaseisfreeinthepublicdomain.18
• University-1652isreleasedundertheMITlicense.19
• PubMedisafreeandpublicdatabase,withopenaccessarticlesunderaCreativeCommons
orsimilarlicense.20
• SciDuetisreleasedundertheApache2.0licensewithpaperslidesfromACL,ICML,and
NeurIPS.21
G Accessibilityof MUIRBENCH
G.1 DatasetDocumentationandFormat
The full documentation of MUIRBENCH is on the project page at https://huggingface.co/
datasets/MUIRBENCH/MUIRBENCH. ForeachdataentryinMUIRBENCH,itincludesmetadataof
index(idx), task, question, options, answer, imagerelation, imagetype, images, andcounterpart
instanceidx.
G.2 LinksandMaintenancePlan
MUIRBENCHishostedonHuggingface/Datasets,22wherelicenseandmetadata23arealsoavailable.
Wemaintainourbenchmarkonthispageandwillcontinuallyupdateit. Theevaluationcodeand
outputswillbeprovidedtofacilitateeasyreproductionandanalysesoftheresultsinthepaper.
G.3 AuthorStatement
Weconfirmthatwebearallresponsibilityincaseofviolationofrightsduringthecollectionofdata
onMUIRBENCH,ensuringaccountabilityandcommitmenttomaintainingethicalstandards. Wewill
takeappropriateactionwhenneeded.
G.4 IntendedUses
Thedatasetisforacademicpurposesonlyandnotforcommercialusage.
15https://github.com/ankanbansal/ISVQA-Dataset/tree/master?tab=License-1-ov-file
16https://www.nuscenes.org/terms-of-use
17https://github.com/open-compass/MMBench?tab=Apache-2.0-1-ov-file
18https://www.usgs.gov/faqs/what-are-terms-uselicensing-map-services-and-data-national-map
19https://github.com/layumi/University1652-Baseline?tab=MIT-1-ov-file#readme
20https://www.ncbi.nlm.nih.gov/pmc/about/copyright/
21https://github.com/IBM/document2slides?tab=Apache-2.0-1-ov-file
22https://huggingface.co/datasets/MUIRBENCH/MUIRBENCH
23https://huggingface.co/api/datasets/MUIRBENCH/MUIRBENCH/croissant
20Figure16: Annotationinterface.
21