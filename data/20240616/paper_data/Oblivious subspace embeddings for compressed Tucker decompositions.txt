Oblivious subspace embeddings for compressed Tucker
decompositions
MatthewPietrosanu BeiJiang LinglongKong
DepartmentofMathematical& DepartmentofMathematical& DepartmentofMathematical&
StatisticalSciences,Universityof StatisticalSciences,Universityof StatisticalSciences,Universityof
Alberta Alberta Alberta
Edmonton,Canada Edmonton,Canada Edmonton,Canada
pietrosa@ualberta.ca bei1@ualberta.ca lkong@ualberta.ca
ABSTRACT 1 BACKGROUNDANDMOTIVATION
Emphasisinthetensorliteratureonrandomembeddings(tools Low-dimensionaldecompositionslieattheheartofmanytensor-
forlow-distortiondimensionreduction)forthecanonicalpolyadic basedmethodsforstatisticalinference,representation,andfeature
(CP)tensordecompositionhasleftanalogousresultsforthemore extraction[13].Tensor-specializedmethodsarethemselvesoften
expressiveTuckerdecompositioncomparativelylacking.Thiswork motivatedbyspecificappliedresearchquestionsandadvancements
establishesgeneralJohnson–Lindenstrauss(JL)typeguarantees indata-collectiontechnologies(e.g.,inpharmacology[6]andneu-
fortheestimationofTuckerdecompositionswhenanoblivious roimaging[21]).Inthesesettings,naivelyworkingwithvectorized
randomembeddingisappliedalongeachmode.Whentheseem- dataistypicallyneitherconceptuallysoundnorcomputationally
beddingsaredrawnfromaJL-optimalfamily,thedecomposition feasible.Dimension-reducingmapsthatpreservedatageometry
canbeestimatedwithin𝜀relativeerrorunderrestrictionsonthe haveconsequentlyfoundsubstantialapplicationintensorresearch,
embeddingdimensionthatareinlinewithrecentCPresults.Weim- wherecomputationalefficiencyandrepresentationqualityarepri-
plementahigher-orderorthogonaliteration(HOOI)decomposition maryconcernsduetoprohibitivesizeoftensordata.
algorithmwithrandomembeddingstodemonstratethepractical Thewell-knownJohnson–Lindenstrauss(JL)lemma[3]provides
benefitsofthisapproachanditspotentialtoimprovetheacces- atheoreticalbasisforsuchmaps.Wesaythatalineartransforma-
sibilityofotherwiseprohibitivetensoranalyses.Onmoderately tion𝑨isan𝜀-JLembeddingofasetS ⊂R𝑛intoR𝑚 if,forevery
largefaceimageandfMRIneuroimagingdatasets,empiricalre- 𝑥 ∈S,thereexists𝜀 𝑥 ∈ (−𝜀,𝜀)suchthat
s mu il nts imsh ao lw incth rea at ss eub insta ren ct oia nl sd trim uce tn ios nion err re od ru rc et li ao tn ivi es tp oos ts ri ab dl ie tiw onit ah
l
∥𝑨𝑥∥2 2=(1+𝜀 𝑥)∥𝑥∥2 2.
HOOI(≤5%largererror,50%–60%lowercomputationtimeforlarge JL-embeddingsaretypicallygeneratedrandomlyfromaclassof
modelswith50%dimensionreductionalongeachmode).Especially maps,independentof(orobliviousto)thedatatowhichitwillbe
forlargetensors,ourmethodoutperformstraditionalhigher-order applied.
singularvaluedecomposition(HOSVD)andrecentlyproposedTen- Withmuchemphasisonscalable,computationallyefficientten-
sorSketchmethods. sormethods,itisnotsurprisingthatthemajorityofdevelopments
intheliteraturefavorthecanonicalpolyadic(CP)decomposition
CCSCONCEPTS foritssparsity[9,16,18].Thishaslefttheoreticalresultsforthe
Tuckerdecompositioncomparativelylimited.Foragiventensor
•Mathematicsofcomputing→Dimensionalityreduction;•
Theoryofcomputation→Sketchingandsampling. 𝒀
∈R𝑛1×···×𝑛𝑞,theTuckerdecompositiontakestheform
𝑅1 𝑅𝑞
KEYWORDS 𝒀 = ∑︁ ···∑︁ 𝜆 𝑟1,...,𝑟𝑞Γ 1,𝑟1◦···◦Γ 𝑞,𝑟𝑞
alternatingleastsquares,dimensionreduction,higher-orderorthog- 𝑟1=1 𝑟𝑞=1
onaliteration,Johnson–Lindenstrauss,low-rankapproximation, =: [𝚲| 𝚪 1,...,𝚪 𝑞] (1)
neuroimaging,randomembedding,tensordecomposition
foraprespecifiedrank(𝑅1,...,𝑅 𝑞)(𝑅 𝑗 ≤𝑛 𝑗),where𝚲=(𝜆 𝑟1,...,𝑟𝑞)
A MC atM theR wef Pe ier te rn oc sae nF uo ,r Bm eia Jit a:
ng,andLinglongKong.2024.Oblivioussubspace
∈ R𝑅1×···×𝑅𝑞 iscalledthecoretensor and𝚪 𝑗 = [Γ 𝑗,1,...,Γ 𝑗,𝑅𝑗] ∈
embeddingsforcompressedTuckerdecompositions.InProceedingsofConf
R𝑛𝑗×𝑅𝑗 iscalledthe𝑗thfactormatrix.Briefly,theCPdecomposition
title(Abbr).ACM,NewYork,NY,USA,11pages. requires𝜆 𝑟1,...,𝑟𝑞 =0unless𝑟1=···=𝑟 𝑞 [10].Thoughlesssparse
andcomputationallymorecomplex,Tuckerdecompositionsprovide
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor aricherclassofdecompositionsandarethesubjectofcontinued
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation researchinstatisticsandotherappliedfields[10,11,13].
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe Recent notable work on Tucker decompositions by
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
Malik&Becker[15],Ma&Solomonik[14],andMinsteretal.[17]
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. incorporatedimensionreductionviaCountSketchoperatorsand
Abbr,MMMDD–DD,YYYY,Location randomGaussianmatrices—bothspecificJLclasses.Thelatterfo-
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. cusesonrandomizedhigher-orderSVD(HOSVD)foritscomputa-
ACMISBN978-1-4503-XXXX-X/18/06
tionalbenefits,whereashigher-orderorthogonaliteration(HOOI)
4202
nuJ
31
]LM.tats[
1v78390.6042:viXraAbbr,MMMDD–DD,YYYY,Location MatthewPietrosanu,BeiJiang,andLinglongKong
isknowntoprovidebetterdecompositions[4].Thework’sran- 𝑿[𝑗] ∈R𝑛𝑗×(cid:206)𝑞 𝑘=1,𝑘≠𝑗𝑛𝑗 bethemode-𝑗matricizationof𝑿.Vectoriza-
domizedHOSVDandsequentiallytruncatedHOSVDalgorithms tionandmatricizationaretakentobecompatibleinthesensethat
furthermoredonotrespecttheKroneckerstructureoftheTucker vec𝑿 =vec𝑿[1].Let×𝑗 denotemode-𝑗 multiplicationofatensor
decompositionoronlysequentiallyapplyreductionsalongthedata byamatrix.Fornotationalconvenience,wewrite𝑿 ×𝑗∈[𝑞] 𝑨𝑗
modes.Neitheroftheseworksprovidegeneraltheoreticalresults todenotetherepeatedmode 𝑗 multiplication𝑿 ×1𝑨1···×𝑞𝑨𝑞
thatapplybeyondaspecificclassofembeddings. (where order of operations is irrelevant as multiplication along
Similarcommentsapplytoearlierliteraturethatproposeran- differentmodescommute).Forfurtherdetailontensoroperations,
domizedTuckerdecompositionalgorithms(almostexclusivelyvia seeKolda&Bader[10].Fortensors𝑿and𝒀 ofthesamesize,define
HOSVD)throughaspecificclassofJLembeddingortakeadifferent thetensorinnerproduct⟨𝑿,𝒀⟩=⟨vec𝑿,vec𝒀⟩andtheassociated
approachentirely(e.g.,[2,5,19]).Foranoverviewandusefulclas- norm∥𝑿∥=∥vec𝑿∥2.
sificationofrandomizedalgorithms,see[1].Theseworkshighlight Anotationforbasiscoherence,introducedpreviouslyinIwenet
thewideinterestinTuckerdecompositionsandthepotentialfor al.[9]fortheCPdecomposition(andarguablygeneralizedhere),
applicationtoabreathofproblems,butagainemphasizethelack willbeconvenientwhenstudyingtheTuckerdecomposition(but
oftheoreticalguaranteesforgeneralJLembeddingframeworks. lessusefulconceptuallyduetotheorthogonalityrestrictiononthe
ThisarticleconsiderstheproblemofestimatingaTuckerde- 𝚪 𝑗s).Definethemodewisecoherenceofanydecompositionofthe
compositionofagiventensor𝒀 ∈R𝑛1×···×𝑛𝑞 usinga“compressed” forminEquation1as𝜇
𝒀
=max 𝑗∈[𝑞]𝜇 𝒀,𝑗,where
version𝒀 ×1𝑨1···×𝑞 𝑨𝑞 ∈ R𝑚1×···×𝑚𝑞 ofthedata,wherethe
𝑨 Tu𝑗 cs ka erre dea cr ob mitr pa or sy itiJ oL nse ,m nb amed ed lyin ,g ws h. eW ree tf ho ecu fas cs too rle mly ao trn ico er st hh aog vo en oa rl
-
𝜇 𝒀,𝑗 = 𝑘,ℎm ∈a [𝑅x
𝑗]
∥| Γ⟨ 𝑗Γ ,𝑘𝑗,𝑘 ∥2,Γ ∥𝑗 Γ,ℎ 𝑗,ℎ⟩|
∥ 2
thonormalcolumns(i.e.,each𝚪
𝑗
liesonaStiefelmanifold).The 𝑘≠ℎ
twoprimarycontributionsofthisworkareasfollows. iscalledthemode-𝑗 coherence.Asinotherworks[9],owingtothe
nonuniquenessofCP/Tuckerdecompositions,wecalculatecoher-
• WeestablishJL-typetheoreticalresultsboundingtheerror
enceusingtherank-1termsofanexplicitlygivendecomposition.
introducedinexactandinexactTuckerdecompositionswhen
randomembeddingsfromaJL-optimalfamilyareapplied
2.2 ExactDecompositionsUnderJLEmbeddings
alongeachdatamode.Weemphasizethattheseresultsapply
generallytoJL-optimalfamiliesandnottoaspecifictypeof Webeginourtheoreticalanalysiswithanelementaryresulton
embedding,unlikeotherworks[15,17]. howTuckerdecompositionsareperturbedunderarbitrarymode-
• WeproposeanewHOOIalgorithmthatusesrandomem- 𝑗 multiplication. See the appendix for a proof of the following
beddingstoestimateTuckerdecompositions.Empirically, claim,whichreliesonroutinemanipulationandpropertiesoftensor
forlargemodels,ourapproachrequiressubstantiallyless matricization.
computationtimewithonlyasmallincreaseinreconstruc-
tionerrorrelativetotraditionalHOOIandcanmakelarge Lemma 2.1. Let 𝑗 ∈ [𝑞] and 𝑩 ∈ R𝑚𝑗×𝑛𝑗. Suppose that 𝒀 ∈
tensoranalysesfeasibleonmoderatecomputingresources.
R𝑛1×···×𝑛𝑞 hasarank-(𝑅1,...,𝑅 𝑞)Tuckerdecomposition𝒀 = [𝚲 |
Unlikeotherworks[17],ourapproachtakesadvantageofthe
𝚪 1,...,𝚪 𝑞]andthatmin 𝑟∈[𝑅𝑗]∥𝑩Γ 𝑗,𝑟∥2 >0.Then
KroneckerstructureoftheTuckerdecompositionanduses 𝒀′ =𝒀 ×𝑗 𝑩= [𝚲′ | 𝚪 1′,...,𝚪 𝑞′],
(nearly)fullycompresseddatainallupdatesoftheestimated
decomposition.OurapproachoutperformsHOSVD[10]and where𝜆 𝑟′ 1,...,𝑟𝑞 = 𝜆 𝑟1,...,𝑟𝑞∥𝑩Γ 𝑗,𝑟𝑗∥,𝚪 𝑘′ = 𝚪 𝑘 for𝑘 ≠ 𝑗,and𝚪′ 𝑗 has
recentlyproposedTensorSketchmethods[15]forTucker columnsΓ 𝑗,𝑟/∥𝑩Γ 𝑗,𝑟∥,𝑟 ∈ [𝑅 𝑗].Itfollowsthat𝜇 𝒀′,𝑘 = 𝜇 𝒀,𝑘 when
decomposition. 𝑘 ≠ 𝑗 and
meO ntu sr ba yp Ip wr eo nac eh tac ll .o [s 9e ]ly fof ro tl hlo ew Cs Pp da er ct oo mf pre oc se itn iot ns ,u wbs itt han at nia il md pe ov re tl ao np t- 𝜇 𝒀′,𝑗 = 𝑟,𝑠m ∈a [𝑅x
𝑗]
∥𝑩|⟨ Γ𝑩 𝑗Γ ,𝑟𝑗 ∥,𝑟 2, ∥𝑩 𝑩Γ Γ𝑗, 𝑗𝑠 ,𝑠⟩ ∥| 2.
𝑟≠𝑠
distinctionasidefromthedifferentdecomposition.Theauthors’
remarkinSection3.2thatCPresultscanbeapplieddirectlyto Furthermore,
Tuckerdecompositionsisonlytruewhenthecoretensorhasa 𝑅𝑗 𝑅𝑗
specificpatternofhighsparsity(astheauthorsnote,throughan ∥𝒀′∥2 =∑︁∑︁ (𝚿 𝑗𝚿⊤ 𝑗)𝑟,𝑠⟨𝑩Γ 𝑗,𝑟,𝑩Γ 𝑗,𝑠⟩,
appropriatechoiceoftensorbasis).Adirectapplicationofthesepre- 𝑟=1𝑠=1
v suio mu ss )r ve is ou ll at ts et so ag ne in me pra ol rT tau nc tk be ar sd isec ino cm op ho es ri et nio cn es re(d qu ue irt eo mt eh ne tn .Tes ht ued s, with𝚿 𝑗 =𝚲 [𝑗](cid:0)(cid:203) 𝑘1 =𝑞,𝑘≠𝑗𝚪 𝑘(cid:1)⊤ ∈R𝑅𝑗×𝑁𝑗 and𝑁 𝑗 =(cid:206)𝑞 𝑘=1,𝑘≠𝑗𝑛 𝑘.
themodifiedapproachtakeninthisworkisindeednecessary.
When𝑩inLemma2.1isan𝜀-JLembeddingthatproperlyembeds
thecolumnspaceofthefactormatricesofaTuckerdecomposition,
2 THEORETICALRESULTS changes to the core tensor, coherence, and tensor norm can be
controlled.ThisnotionisformalizedinProposition2.2.
2.1 Notation
Asstandardoperations,let◦denotethetensorouterproduct,⊗the Proposition2.2. Fix 𝑗 ∈ [𝑞]andsupposethat𝒀 ∈ R𝑛1×···×𝑛𝑞
Kroneckerproduct,and⊡theHadamardproduct.Letvecdenote permitstherank-(𝑅1,...,𝑅 𝑞)Tuckerdecomposition𝒀 = [𝚲| 𝚪 1,...,
theusualvectorizationoperator.Foratensor𝑿 ∈R𝑛1×···×𝑛𝑞,let 𝚪 𝑞],where𝚪
𝑗
hascolumnsofunitlength.Supposethat𝑨∈R𝑚×𝑛𝑗OblivioussubspaceembeddingsforcompressedTuckerdecompositions Abbr,MMMDD–DD,YYYY,Location
isan𝜀-JLembeddingoftheset where𝑫𝑘 =diag({∥𝑨𝑘,𝑟𝚪 𝑘,𝑟∥2}𝑟∈[𝑅𝑘])and𝒀(0) =𝒀.Since𝚪 𝑗(𝑗)
S𝑗 = (cid:216) {Γ 𝑗,𝑟 ±Γ 𝑗,𝑠}∪ (cid:216) {Γ 𝑗,𝑟}⊂R𝑛𝑗. h apa ps lc ieo dlu tm on ps aiw rsit oh fu thn eit fn oo rmrm (, 𝒀th (𝑗e −r 1e )s ,u 𝒀l (ts 𝑗)o )f .Proposition2.2canbe
𝑟,𝑠∈[𝑅𝑗] 𝑟∈[𝑅𝑗]
Towardthemainresult,
𝑟<𝑠
L 𝒀e bt u𝒀 t′ w= ith𝒀 a× c𝑗 o𝑨 re,w teh ni sc oh r( 𝚲b ′y wL ie tm hm elea m2 e.1 n) th sa 𝜆s
𝑟′
1t ,h ..e .,𝑟s 𝑞am =e 𝜆d 𝑟1e ,c ..o .,m 𝑟𝑞p ∥o 𝑨si Γti 𝑗o ,𝑟n 𝑗∥a 2s (cid:12) (cid:12) (cid:12)∥𝒀∥2 −(cid:13) (cid:13)𝒀 𝑗∈× [𝑞]𝑨𝑗(cid:13) (cid:13)2(cid:12) (cid:12) (cid:12)=(cid:12) (cid:12)
(cid:12)
𝑗∑︁ ∈[𝑞](cid:13) (cid:13)𝒀(𝑗−1)(cid:13) (cid:13)2 −(cid:13) (cid:13)𝒀(𝑗)(cid:13) (cid:13)2(cid:12) (cid:12)
(cid:12)
and (ia )𝑗 |t 𝜆h
𝑟′
1f ,a ..c .,𝑟to 𝑞r −m 𝜆a 𝑟t 1r ,.i .x .,𝑟w 𝑞|it ≤hc 𝜀o |𝜆lu 𝑟m 1,..n .,s 𝑟𝑞𝑨 |;Γ 𝑗,𝑟𝑗/∥𝑨Γ 𝑗,𝑟𝑗∥2.Then ≤ 𝑗∑︁ ∈[𝑞](cid:12) (cid:12)1 𝑅⊤ 𝑗[𝑬𝑗 ⊡(𝚿 𝑗𝚿⊤ 𝑗)]1 𝑅𝑗(cid:12) (cid:12)
(( iii ii )) (cid:12) (cid:12)𝜇 ∥𝒀 𝒀′, ′𝑗 ∥2≤ −𝜀/ ∥( 𝒀1 ∥− 2(cid:12) (cid:12)𝜀 ≤)a (cid:12) (cid:12)1n 𝑅⊤d 𝑗𝜇 [𝑬𝒀 𝑗′,𝑘 ⊡= (𝚿𝜇 𝒀 𝑗𝚿,𝑘 ⊤ 𝑗fo )]r 1a 𝑅l 𝑗l (cid:12) (cid:12)𝑘 , ≠ 𝑗;and b By efc ol ra eim pro(i ci ei) eo dif nP gr ,o wp eos mit uio stn e2 x. a2 m,h ine ere thw eit gh en𝑬 e𝑗 ra∈ lt( e− rm𝜀/𝑞 o, f𝜀 t/ h𝑞 e)𝑅 ab𝑗× o𝑅 v𝑗 e.
where𝑬𝑗 ∈ (−𝜀,𝜀)𝑅𝑗×𝑅𝑗 and𝚿 𝑗 isasdefinedinLemma2.1. sum:
Proof. We provide a sketch of the proof. To prove claim (i), (cid:12) (cid:12)1 𝑅⊤ 𝑗[𝑬𝑗 ⊡(𝚿 𝑗𝚿⊤ 𝑗)]1 𝑅𝑗(cid:12) (cid:12)
observetha |𝜆t 𝑟′
1,...,𝑟𝑞
−𝜆 𝑟1,...,𝑟𝑞|=|𝜆 𝑟1,...,𝑟𝑞|(cid:12) (cid:12)∥𝑨Γ 𝑗,𝑟𝑗∥2−1(cid:12)
(cid:12)
≤∑︁ 𝑟𝑅 =𝑗 1∑︁ 𝑠𝑅 =𝑗 1𝑞𝜀 ∥𝑒 𝑟⊤𝚲 [( 𝑗𝑗) ]∥2∥𝑒 𝑠⊤𝚲 [( 𝑗𝑗) ]∥2(cid:13) (cid:13)(cid:204) 𝑘=1
𝑞
𝚪 𝑘(𝑗)⊤𝚪 𝑘(𝑗)(cid:13) (cid:13)2.
≤𝜀|𝜆 𝑟1,...,𝑟𝑞|, 𝑘≠𝑗
where(cid:12) (cid:12)∥𝑨Γ 𝑗,𝑟∥2−1(cid:12) (cid:12)≤𝜀sinceΓ 𝑗,𝑟 ∈S𝑗 hasunitnorm. Wecalculatethefinaloperatornorm ∥(cid:203) 𝑘1 =𝑞,𝑘≠𝑗𝚪 𝑘(𝑗)⊤𝚪 𝑘(𝑗) ∥2 =
To prove claim (ii), note that, for any distinct 𝑟 and 𝑠, (cid:206)𝑞 ∥𝚪(𝑗) ∥2intwocases.When𝑘 > 𝑗,itisclearthat∥𝚪(𝑗) ∥2 =
|⟨𝑨Γ 𝑗,𝑟,𝑨Γ 𝑗,𝑠⟩−⟨Γ 𝑗,𝑟,Γ 𝑗,𝑠⟩| ≤𝜀byLemmaA.1since{Γ 𝑗,𝑟 ±Γ 𝑗,𝑠}⊂ 𝑘=𝑞,𝑘≠𝑗 𝑘 2 𝑘 2
S𝑗.Ontheotherhand, 1.Ontheotherhand,when𝑘 ≤ 𝑗,
∥𝑨Γ 𝑗,𝑟∥2∥𝑨Γ 𝑗,𝑠∥2 ≥ 𝑡∈m [i 𝑅n 𝑗]∥𝑨Γ 𝑗,𝑡∥2
2
≥1−𝜀 (𝚪 𝑘(𝑗)⊤𝚪 𝑘(𝑗) )𝑟,𝑠 = ∥𝑨⟨ 𝑘𝑨 Γ𝑘 𝑘Γ ,𝑟𝑘 ∥,𝑟 2, ∥𝑨 𝑨𝑘 𝑘Γ Γ𝑘 𝑘,𝑠 ,𝑠⟩
∥2
asΓ
𝑗,𝑡
∈S𝑗.Thus, for𝑟,𝑠 ∈ [𝑅 𝑘].Thisquantityisequalto1when𝑟 =𝑠and,byclaim
(ii)ofProposition2.2,isabsolutelyboundedby𝜀/(𝑞−𝜀)when𝑟 ≠𝑠.
𝜇 𝒀′,𝑗 = 𝑟,𝑠m ∈a [𝑅x 𝑗] ∥𝑨|⟨ Γ𝑨 𝑗Γ ,𝑟𝑗 ∥,𝑟 2, ∥𝑨 𝑨Γ Γ𝑗, 𝑗𝑠 ,𝑠⟩ ∥| 2 ≤ 1−𝜀 𝜀. T abh su os l, u𝚪 te𝑘( l𝑗 y)⊤ b𝚪 o𝑘 u(𝑗 n) d= ed𝑰𝑅 b𝑘 y+ 𝜀/( (1 𝑞𝑅𝑘 −1 𝑅 𝜀⊤ )𝑘 .− Fr𝑰 o𝑅 m𝑘) t⊡ hi𝑭 s𝑘 ,, itw ishe sr tre a𝑭 igh ha tfs oe rl wem are dn tt os
𝑟≠𝑠
Itisclearthat𝜇 𝒀′,𝑘 =𝜇 𝒀,𝑘 for𝑘 ≠ 𝑗 sincethe𝑘thfactormatrixis sho Rw ett uh ra nt in∥ g𝚪 𝑘( to𝑗) t∥ h2 2 e≤ m1 ai+ n𝜀 r( e𝑅 s𝑘 ul− t,w1) e/( c𝑞 an− c𝜀 o) n. cludethat
thesamebetween𝒀 and𝒀′.
witF hin 𝑩al =ly 𝑨,to anp dro 𝑩ve =c 𝑰l 𝑛a 𝑗im .C( oi mii) b, ic no in ngsi td he er sa ep rp epli rc ea st eio nn tas tio of nL se om f𝒀m ′a a2 n. d1 (cid:12) (cid:12) (cid:12)∥𝒀∥2 −(cid:13) (cid:13)𝒀 𝑗×𝑞 =1𝑨𝑗(cid:13) (cid:13)2(cid:12) (cid:12)
(cid:12)
𝒀 y Pi re old ps osc il ta ii om n2(i .i 2i) ca on nd troco lsm thp ele Tte us ckth ee rdp er co oo mfo pf osth ite iop nr ro ep so us li tt ii no gn f. rom□
≤ 𝑞𝜀
∑︁𝑞
𝑅 𝑗(cid:16) 1+
𝑞𝜀(cid:17)2𝑗(cid:214)𝑗−1
(cid:104) 1+(cid:16) 𝑞−𝜀 𝜀(cid:17) (𝑅 𝑘 −1)(cid:105) ∥𝚲∥2
𝑗=1 𝑘=1
theapplicationofasingleembedding𝑨alongmode𝑗.Proposition
𝑅˜𝜀𝑒𝜀(2+𝑅˜+2/𝑞)
2.3repeatedlyappliesthisresulttoobtainaJL-typeboundforthe
≤
∥𝚲∥2,
applicationofanembedding𝑨𝑗 alongeachmode. 1+𝜀/(2𝑞)
Proposition2.3. Let𝜀 ∈ (0,1)andsupposethat𝒀 ∈R𝑛1×···×𝑛𝑞 wherethefinalinequalityholdssince∥𝚲(𝑗)∥2 ≤ (1+𝜀/𝑞)2𝑗∥𝚲∥2
(byarecursiveapplicationofclaim(i)ofProposition2.2)andby
permitstherank-(𝑅1,...,𝑅 𝑞)orthogonalTuckerdecomposition𝒀 =
[𝚲 | 𝚪 1,...,𝚪 𝑞].Let𝑨𝑗 ∈ R𝑚𝑗×𝑛𝑗 bean𝜀/𝑞-JLembeddingofS𝑗 other standard bounds. The result follows directly since ∥𝒀∥ =
(fromProposition2.2)foreach𝑗 ∈ [𝑞].Then
∥𝚲×𝑘∈[𝑞] 𝚪 𝑘∥=∥𝚲∥. □
(cid:12) (cid:12) (cid:12)∥𝒀∥2 −(cid:13) (cid:13)𝒀 𝑗×𝑞 =1𝑨𝑗(cid:13) (cid:13)(cid:12) (cid:12) (cid:12)≤ 𝑅˜𝜀 1𝑒𝜀 +(2 𝜀+ /𝑅 (˜+ 2𝑞2/ )𝑞) ∥𝒀∥2, Wac eB o sne af vo yer ne thiep anr to t ac te o fe o ad l mi fn o ig lr yt to h ofeth pte h rm oeo ba r ai en bt iir lc ie a ts ylu al dt n io a sf tly rt ih s bii s us tos ie f oc nJt L sio e Pn m, 𝑚w b ,𝑛ee di odn nit nr Ro gd s 𝑚u [ ×9c 𝑛e ].
where𝑅˜ =max 𝑗∈[𝑞]𝑅 𝑗. over (𝑚,𝑛) ∈ N×Nisan𝜂-optimalfamilyofJLembeddings if
thereexistsanabsoluteconstant𝐶 > 0suchthat,forany𝑚 <𝑛
Proof. Weprovideasketchoftheproof.For 𝑗 ∈ [𝑞],define and any set S ⊂ R𝑛 with cardinality |S| ≤ 𝜂exp(𝜀2𝑚/𝐶), the
𝒀(𝑗) = 𝒀(𝑗−1) ×𝑗 𝑨𝑗 = [𝚲(𝑗) | 𝚪 1(𝑗),...,𝚪 𝑞(𝑗) ],where𝜆 𝑟( 1𝑗 ,) ...,𝑟𝑞 = randommatrix𝑨∼P𝑚,𝑛isan𝜀-JLembeddingofSintoR𝑚 with
𝜆 𝑟1,...,𝑟𝑞(cid:206) 𝑘𝑗 =1∥𝑨𝑗𝚪 𝑘,𝑟𝑘∥2, p ofro JLba eb mili bt ey da dt inle ga sst (a1 s− nu𝜂 m.T eh rois usco on pc te impt ap lfe ar mm ii lt is esge en xe isr ta )l .discussion
𝚪(𝑗) =(cid:40) 𝑨𝑘𝚪 𝑘𝑫 𝑘−1 if𝑘 ≤ 𝑗 , (2) Theorem 2.4. Fix 𝜂,𝜀 ∈ (0,1) such that 𝜀 ≤ [𝑅˜−1 + 2−1 +
𝑘 𝚪 𝑘 if𝑘 > 𝑗 (𝑞𝑅˜ )−1]−1ln2. Let L = span{⃝𝑞 𝑗=1Γ 𝑗,𝑟𝑗 : 𝑟 𝑗 ∈ [𝑅 𝑗],𝑗 ∈ [𝑞]},Abbr,MMMDD–DD,YYYY,Location MatthewPietrosanu,BeiJiang,andLinglongKong
where each 𝚪 𝑗 ∈ R𝑛𝑗×𝑅𝑗 has orthonormal columns. Draw 𝑨𝑗 ∈ L𝑗 = {[𝚲 | 𝚪 1,...,𝚪 𝑞] : 𝚪 𝑗 ∈ R𝑛𝑗×𝑅𝑗,𝚪⊤ 𝑗 𝚪 𝑗 = 𝑰𝑅𝑗}.Let𝑨𝑘 ∈
R𝑚𝑗×𝑛𝑗 froman𝜂/𝑞-optimalfamilyofJLdistributions,with R𝑚𝑘×𝑛𝑘,𝑘 ∈ [𝑞],with
𝑚 𝑗 ≥ 𝐶˜ 𝑗𝑅 𝜀˜ 22𝑞2 ln(cid:16)𝑅 𝜂2 𝑗𝑞 (cid:17) 𝑚 𝑘 ≥ 𝐶˜ 𝑗𝑞 𝜀22𝑝˜ 𝑗 ln(cid:16)2𝐷 𝑗( 𝜂𝑞−1)(cid:17) ,
andwhere𝐶˜
𝑗
>0issomeabsoluteconstant.Thenwithprobability
bedrawnfroman𝜂/(2𝑞)-optimalfamilyofJLdistributions,where
atleast1−𝜂, (cid:12) (cid:12)∥𝒀 𝑗×𝑞 =1𝑨𝑗∥2 −∥𝒀∥2(cid:12) (cid:12)≤𝜀∥𝒀∥2 𝐶 a 𝒁˜ n𝑗 ×di
𝑞
𝑘s 𝑝˜ =a 𝑗 1n = 𝑨a 𝑘db .is mo Wlu s itpt he anc po rLn obs 𝑗t a.a bDn it e li, fi t𝐷 yne𝑗 att≥ h lee( a2 r sa𝑝 t˜ n𝑗 1d+ −om1 𝜂) ,(cid:0) li(cid:206) ne𝑘 ℓ a=− r11 m𝑚 aℓ p(cid:1) 𝐿(cid:0)(cid:206) vi𝑞 ℓ a=𝑘 𝐿+ (1 𝒁𝑛 )ℓ =(cid:1) ,
forall𝒀 ∈L.
(cid:12) (cid:12)∥𝐿(𝑿−𝒀)∥2 −∥𝑿−𝒀∥2(cid:12) (cid:12)≤𝜀∥𝑿−𝒀∥2
Proof. Whendrawnfroman𝜂/𝑞-optimalfamilyofJLdistri-
butions, 𝑨𝑗 is a𝛿/𝑞-JL embedding of S𝑗 ⊂ R𝑛𝑗 into R𝑚𝑗 with forall𝒀 ∈ L𝑗.Inparticular,if2𝑅 𝑗 <𝑛 𝑗,asufficientconditionfor
probability1−𝜂/𝑑provided theembeddingdimensionis
|S𝑗|=𝑅2 𝑗 ≤ 𝜂 𝑞 exp(cid:16)(𝛿/𝑞 𝐶) 𝑗2𝑚 𝑗(cid:17) , 𝑚 𝑘 ≥ 𝐶˜ 𝑗(𝑞 𝜀+ 21)3𝑝˜ 𝑗 ln(cid:16) 𝑞4 +√𝑛 1˜ 𝜂(cid:17) ,
where𝐶 >0issomeabsoluteconstant.Thissufficientcondition
isequiva𝑗
lentto𝑚
𝑗
≥𝐶 𝑗𝑞2𝛿−2ln(𝑅2 𝑗𝑞𝜂−1).
where𝑛˜ =max 𝑘∈[𝑞]𝑛 𝑘.
ByProposition2.3(conditionalonthe𝑨𝑗sbeingappropriate Proof. Wepresentasketchoftheproof.Considerany𝒀 ∈L𝑗.
JL-embeddings), ByProposition2.3andanargumentsimilartothatinTheorem2.4,
(cid:12) (cid:12) (cid:12)∥𝒀∥2 −(cid:13) (cid:13)𝒀 𝑗×𝑞 =1𝑨𝑗(cid:13) (cid:13)(cid:12) (cid:12) (cid:12)≤ 𝑅˜𝛿 1𝑒 +𝛿( 𝛿2+ /𝑅 (˜ 2+ 𝑞2/ )𝑞) ∥𝒀∥2. drawingthe𝑨𝑗sfro
(cid:12)
(cid:12)∥m 𝒀a ∥n
2
−𝜀/ ∥( 𝐿4𝑅 (˜ 𝒀𝑞) )- ∥o 2p
(cid:12)
(cid:12)t ≤im 2𝜀al ∥𝒀JL ∥2familyyields
Taking𝛿 =𝜀/(2𝑅˜ )yieldsthedesiredsufficientconditionon𝑚
𝑗
and
allowstheRHSabovetobeboundedby𝜀∥𝒀∥2.Thefinalpartof withprobability1−𝜂/2.Thesufficientlowerboundon𝑚 𝑘isomitted
asatighterboundwillbeintroducedshortly.
theclaim,the1−𝜂probabilitybound,holdsbyaunionboundover
Observethat
𝑗 ∈ [𝑞]. □
1
2.3 InexactTuckerDecompositionsUnderJL vec(𝒀 [⊤ 𝑗])=(cid:8) 𝑰𝑛𝑗 ⊗ (cid:2)(cid:0)(cid:204) 𝚪 𝑘(cid:1)𝚲⊤ [𝑗](cid:3)(cid:9) 𝑪𝑗𝛾 𝑗 =:𝑩𝑗𝛾 𝑗
Embeddings 𝑘=𝑞
𝑘≠𝑗
WheretheprevioussectionconcernedtensorswithanexactTucker
decomposition,thefollowingresultsconsiderapproximatedecom- where𝐶 𝑗𝛾 𝑗 = vec(𝚪⊤ 𝑗 ) ∈ R𝑛𝑗𝑅𝑗 representsvec(𝚪⊤ 𝑗 ) ∈ R𝑛𝑗𝑅𝑗 in
positions.ThefollowinglemmsisadirectmodificationofTheorem termsof𝛾
𝑗
∈R𝑝𝑗 (with𝑪𝑗 ∈R(𝑛𝑗𝑅𝑗)×𝑝𝑗 and𝑝
𝑗
<𝑛 𝑗𝑅 𝑗),whichis
5of[9],whichwepresentherewithoutproof. possibledueto𝚪 𝑗’sorthogonalcolumns.Let𝑝˜
𝑗
≤𝑝
𝑗
denotethe
whL ere em tm hea 𝑻2 𝑟. s5 f. orL met a𝑿 no∈ rthR o𝑛 n1 o× r· m·· a× l𝑛 s𝑞 eta in nd R𝒀 𝑛1∈ ×··L ·×𝑛⊂ 𝑞.s Lp ea tn P{ L𝑻 ⊥𝑟} d𝑟 e∈ n[ o𝑅 t] e, r s tha un ib sk s spo paf ac ce𝑩 eo𝑗 .. fI Rt 𝑛is 1×cl ·e ··a ×r 𝑛t 𝑞h .a Lt eL t{𝑗 𝑻i 𝑟s }c 𝑟o ∈n [𝑝t ˜a 𝑗i ]n be ed aw nit oh ri tn hoa n𝑝˜ o𝑗 r- mdi am le bn ass ii son foa rl
theorthogonalprojectionoperatorontoL⊥.Fix𝜀 ∈ (0,1)andlet𝐿
Nowconsiderthe2𝑝˜
𝑗
+1elementsof
bealinearoperatorsuchthat
(( iii )) 𝐿 𝐿ani ids sa an n𝜀 𝜀/ /2 (- 2J √L 𝑅e )m -Jb Led ed min bg edo df iL ng∪ o{ fP L⊥(𝑿)}intoR𝑚1×···×𝑚𝑞, S 𝑗′ ,0={P L⊥(𝑿)}∪ 𝑟∈(cid:216)
[𝑝˜
𝑗](cid:110) ∥P PL L⊥ ⊥( (𝑿 𝑿)
)∥
±𝑻𝑟(cid:111) .
intoR𝑚1×··S ·×′ 𝑚= 𝑞.𝑟∈(cid:216) [𝑅](cid:110) ∥PP LL ⊥⊥ (( 𝑿𝑿 ))
∥
±𝑻𝑟(cid:111) I
𝑨
(cid:0)n (cid:206)𝑘du
𝑞
ℓisc =t 𝑘ai +v
n
1el 𝑛𝜀y
/
ℓd
(
(cid:1)2e m𝑒fi
𝑞
on
√︁
de e𝑝S
˜ -𝑗
𝑘𝑗′ ),𝑘
- fiJL
b=
ee
r{
m
s𝒁 1b× oe𝑘 fd𝑨
td
hi𝑘 eng: el𝒁
o
emf∈
t
ehS
ne
t𝑗′ s, (𝑘
2
o−
𝑝
f˜1
𝑗
S}.
+
𝑗′F ,𝑘1o −)r 1(cid:0)e (cid:206)a wc
i𝑘
ℓh
t=−
h𝑘
11
p∈
𝑚
roℓ[ b𝑞
(cid:1)×
a] -,
bilityatleast1−𝜂/(2𝑞)providedthat
Then(cid:12) (cid:12)∥𝐿(𝑿−𝒀)∥2−∥𝑿−𝒀∥2(cid:12) (cid:12)≤𝜀∥𝑿−𝒀∥2 forall𝒀 ∈L.
𝑘−1 𝑞
Loosely,Lemma2.5providesconditionsunderwhichalinear (2𝑝˜
𝑗
+1)(cid:0)(cid:214) 𝑚 ℓ(cid:1)(cid:0) (cid:214) 𝑛 ℓ(cid:1)
operatorcanuniformly(over𝒀 insomesubspaceofinterestL)
ℓ=1 ℓ=𝑘+1
e nm eeb de ed d𝑿 for− th𝒀 ef mor as joo rm re esa ur lb ti it nra Try het oe rn es mor 2𝑿 .6. bT eh lois wg 6e an ..eralresultis
≤
𝜂 exp(cid:16)[𝜀/(2𝑒𝑞√︁𝑝˜ 𝑗)]2𝑚 𝑗(cid:17)
.
2(𝑞−1) 𝐶
𝑗
Theorem2.6. For𝑞 ≥2,fix𝑿 ∈R𝑛1×···×𝑛𝑞;𝜂,𝜀 ∈ (0,1)with𝜀 ≤
[(2𝑅˜ )−1+4−1+(2𝑞𝑅˜ )−1]−1ln2;and𝑗 ∈ [𝑞].Alsofix𝚪 𝑘 ∈R𝑛𝑘×𝑅𝑘, 1Amode-𝑘 fiberofatensor𝑿 ∈ R𝑛1×···×𝑛𝑞 (𝑞 ≥ 𝑘)isavectoroftheform
𝑘 ∈ [𝑞]\{𝑗},withorthonormalcolumnsand𝚲∈R𝑅1×···×𝑅𝑞.Define (𝑿𝑛1,...,𝑛𝑘−1,𝑖,𝑛𝑘+1,...,𝑛𝑞)𝑖∈[𝑛𝑘],where𝑟𝑗 ∈[𝑛𝑗]for𝑗≠𝑘areOblivioussubspaceembeddingsforcompressedTuckerdecompositions Abbr,MMMDD–DD,YYYY,Location
Where𝐷
𝑗
isanupperboundfortheLHSabove,asufficientcondi- Algorithm1OrthogonalTuckerdecompositionviaHOOIwith
tionforthisresultis randomembeddings(HOOI-RE)
𝐶˜ 𝑗𝑞2𝑝˜ 𝑗 (cid:16)2𝐷 𝑗(𝑞−1)(cid:17) Require: data𝑞-tensor𝑿;initialestimates𝚪,...,𝚪 𝑞,𝚲;maximum
𝑚 𝑘 ≥ 𝜀2 ln 𝜂 . iterations𝑁 iter;relativetolerance𝜀 rel
Initialize𝑫𝑗,𝑗 ∈ [𝑞]
anB 𝜀/y (( 2a √︁s 𝑝l ˜i 𝑗g )h -Jt Lm eo md bifi edca dt ii no gn oo ff S)L 𝑗′e (m wim tha p9 roo bf a[ b9 i] l, iti ytf ao tll lo ew ass tt 1h −at 𝜂𝐿 /2is
).
𝑿 r˜ ep← ea𝑿
t
×𝑗∈[𝑞] (𝑭𝑗𝑫𝑗) ⊲Mix
Lemma2.5thusappliesandyieldsthedesiredresult.Thesecond
√ √ Generate𝑺𝑗,𝑗 ∈ [𝑞] ⊲Formembedding
boundfollowsunder2𝑅 𝑗 <𝑛 𝑗 bythebound 𝑞+1𝑞 ≤ 𝑒𝑒 ≤2. □ for𝑗 ∈ [𝑞]do ⊲Update𝚪 𝑗s
3 EMPIRICALEVALUATION
𝑿 𝚪˜ 𝑗𝑗 ←← a𝑿 r˜ g× m𝑘≠ in𝑗 𝚪𝑺 𝑗𝑘
∥𝑿˜
𝑗
−𝚲×𝑘≠𝑗 (𝑺𝑘𝚪
𝑘⊲ )A ×p 𝑗p 𝚪l 𝑗y ∥embedding
3.1 DataandSetup endfor
Wenowpresenttworeal-worldapplicationsofrandomembed- 𝑿˜ ←𝑿˜ ×𝑘∈[𝑞]𝑺𝑗 ⊲Applyembedding
dingstotheestimationofTuckerdecompositions.Ourmaingoal 𝚲←argmin 𝚲∥𝑿˜ 𝑗 −𝚲×𝑘∈[𝑞] (𝑺𝑘𝚪 𝑘)∥
istodemonstrateempiricallythatcompressedestimationcanoffer ⊲Update𝚲
significantreductionsincomputationtimeinpractice,evenfor untilrelativefitdoesnotimprovebyatleast𝜀 reloruntil𝑁
iter
moderatelysizedproblems,withminimalimpactonreconstruction iterationsreached
error.AllanalyseswereperformedinMATLAB2023aandusethe for𝑗 ∈ [𝑞]do ⊲Unmix
implementationofmodewisetensormultiplicationintheTensor 𝚪
𝑗
←𝑫𝑗𝑭𝑗⊤𝚪
𝑗
Toolboxpackage(v3.5)onanInteli7-8550UCPUwith16GBof
endfor
RAM. return𝚪 1,...,𝚪 𝑞,𝚲
ThefirstanalysisusestheORLdatabaseoffaces,2acollectionof
400grayscaleimagesofsize92×112,featuring40subjectsunder
10lightingconditionsandwithdifferentfacialexpressions.We
form𝑨𝑗 =𝑺𝑗𝑭𝑗𝑫𝑗 whenupdatingeachfactormatrix𝚪 𝑗 andthe
treatthedatasetasa92×112×400tensorandconsiderTucker coretensor𝚲.Specifically,𝑺𝑗 ∈R𝑚𝑗×𝑛𝑗 isarow-samplingmatrix,
decompositionsofrank(𝑅,𝑅,𝑅)for𝑅 ∈{5,15,30}.Theembedding 𝑭𝑗 ∈ R𝑛𝑗×𝑛𝑗 is someorthogonal matrix (herea discrete cosine
dimension(𝑚1,𝑚2,𝑚3)iscontrolledbyasingle(approximate)di- transformationmatrix),and𝑫𝑗 ∈R𝑛𝑗×𝑛𝑗 isadiagonalRademacher
mensionreductionfactorDR(i.e.,DR≈𝑚 𝑗/𝑛 𝑗 forallmodes𝑗 ∈ [𝑞] matrix.Thethirdalgorithm,HOOI-RE*,isthesameasHOOI-REbut
towhichcompresionisapplied).
usesthefulldatatoestimatethecoretensor:specifically,thecore
Thesecondanalysisusesresting-statefMRIdata,specifically
forthehippocampus,obtainedfromtheAlzheimer’sDiseaseNeu-
tensorupdateinHOOI-RE*seekstominimize∥𝑿−𝚲×𝑘∈[𝑞] 𝚪 𝑘∥.
roimagingInitiative(ADNI).3ThedataisasubsetofthatinWang
HOOI-REapplies𝑭𝑗𝑫𝑗 inaninitialpreprocessingstep:loosely,
thismixingstep“spreads”informationwithinthedata𝑿andmakes
etal.[20],wheredetailsregardingdataacquisition,extraction,and
subsequentupdateslesssensitiveto𝑺𝑗.Decomposingthemixed
processingcanbefound.Thehippocampalsurfaceofeachofthe
data𝑿˜ isequivalenttodecomposing𝑿,soweneedonly“unmix”the
824subjectsisparameterizedasa100×150×2tensor(withmodes
estimatesandreturntotheoriginal𝑿 spaceattheend.Algorithm
correspondingtorotationaroundthehippocampalsurface,length
alongthesurface,andtheleft/righthippocampus,respectively,as 1usesclosed-formupdatesfortensorcomponents,namely𝚪 𝑗 ←
showninFigure6intheappendix).Foursurface-basedfMRImea-
𝑼𝑗𝑽𝑗⊤,where𝑼𝑗 and𝑽𝑗 containtheleftandrightsingularvectors
sures,radialdistanceandthreemTBMfeatures,areavailablefor fromathinSVDof[𝑿˜ 𝑗 ×𝑘≠𝑗 (𝑺𝑘𝚪 𝑘)] [𝑗]𝚲⊤ [𝑗],and𝚲←𝑿˜ ×𝑗∈[𝑞]
eachsubject,fora100×150×2×4×824datatensor.Weconsider {[(𝑺𝑗𝚪 𝑗)⊤(𝑺𝑗𝚪 𝑗)]−1(𝑺𝑗𝚪 𝑗)⊤} (whereapseudoinverseisusedin
Tuckerdecompositionsofrank(𝑅,𝑅,2,4,𝑅)for𝑅 ∈{5,15,30}and theonecasewhere𝑚 𝑗 < 𝑅 𝑗).Furthermore,the𝑺𝑗 matricesare
reductionsalongthefirst,second,andlastmodes.Noembeddings formedandappliedimplicitlyviasubsettingratherthanexplicit
areappliedtothethirdorfourthmodes. matrixmultiplication.HOOI,HOOI-RE,andHOOI-RE*use𝜀 rel=
Ineachanalysis,weconsidersixestimationmethods.Thefirstis 1×10−5and𝑁 iter=100(wherethelatterisneverreached).
atraditionalalternatingleastsquares(ALS)algorithm,specifically, ThelastthreemethodsuseHOSVD[10]andtwoTensorSketch
higher-orderorthogonaliteration(HOOI)[10].Thesecond,presented algorithms(TUCKER-TSandTUCKER-TTMTSwithdefaultset-
inAlgorithm1asHOOI-RE,4appliesobliviousJLembeddingsofthe tings,asproposedandimplementedin[15]).Theyarepresentedfor
thesakeofcomparisontoother(traditionalandrecent)approaches
2AvailablefromAT&TLaboratoriesCambridgeathttp://cam-orl.co.uk/facedatabase. forestimatingTuckerdecompositions.
html
3DatausedinpreparationofthisarticlewereobtainedfromtheAlzheimer’sDis-
easeNeuroimagingInitiative(ADNI)database(adni.loni.usc.edu).Assuch,thein- 3.2 Results
vestigatorswithintheADNIcontributedtothedesignandimplementationofADNI Figure1visualizesthetotalcomputationtimeandfinalreconstruc-
and/orprovideddatabutdidnotparticipateinanalysisorwritingofthisreport.A
completelistingofADNIinvestigatorscanbefoundat:http://adni.loni.usc.edu/wp- tionerrorfortheHOOI,HOOI-RE,andHOOI-RE*algorithmsover
content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf 100replications.Table1providesanumericalsummaryofthere-
4Ourimplementationandcodeforthefollowingnumericalstudiesareavailable
sultsforallsixalgorithms.Forlargedecompositions,improvements
inananonymizedrepositoryathttps://anonymous.4open.science/r/tucker_hooi-re-
0CE3/README.md incomputationtimeareclearwhenDR < 0.8forHOOI-REandAbbr,MMMDD–DD,YYYY,Location MatthewPietrosanu,BeiJiang,andLinglongKong
Face
HOOI
HOOI−RE*
HOOI−RE
R = 5 R = 15 R = 30 R = 5 R = 15 R = 30
DR 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 DR 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Hippocampus
R = 5 R = 15 R = 30 R = 5 R = 15 R = 30
DR 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 DR 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2 1.0 0.8 0.6 0.4 0.2 0.8 0.6 0.4 0.2
Figure1:TotalruntimeandFrobeniusreconstructionerrorfortheHOOI,HOOI-RE*,andHOOI-REalgorithmsintheface(left)
andhippocampus(right)analyses,summarizedover100replications.HorizontaldottedlinesindicatemedianHOOIresults.
SeeTable1forHOOI-REerrorsfor𝑅=30andDR=0.2,whichareomittedheretoavoidskewingtheplot.
HOOI-RE*:56%–73%reductionsinthefaceanalysisand40%–75% HOOI (R = 5) HOOI-RE (R = 5)
reductionsinthehippocampusanalysiswhen𝑅=30.Atthesame
time,reconstructionerroronlysuffersslightly:a3%–11%increase
inFrobeniusreconstructionerrorinthesamesettingacrossboth
analyses.
NotableexceptionstothistrendoccurforHOOI-REwhenDR=
0.2orwhenDR=0.4and𝑅=30—thatis,where𝑚
𝑗
issmallrelative
to𝑅 .Inthesesettings,HOOI-REyieldsamuchhigherreconstruc-
𝑗
tionerrorthanHOOI.AcomparisonoftheresultsforHOOI-RE
andHOOI-RE*suggestthattheincreasederrorisattributableto
theinstabilityoftheleast-squaresupdateof𝚲when𝑚 iscloseto
𝑗 HOOI (R = 30) HOOI-RE (R = 30)
orlowerthan𝑅 .Insuchextremesettings,HOOI-RE*retainsgood
𝑗
performance.
AsshowninTable1,HOSVDandHOOI-REaregenerallycom-
parableinthefaceanalysis.However,withthemuchlargerhip-
pocampusdata,HOSVDtakessubstantiallylongerthanHOOI-RE
(evenunderminimalcompression)andyieldsestimateswithsub-
stantially larger reconstruction error. In both analyses, the two
TensorSketchmethodsyieldreconstructionerrorshigherthanthat
forHOOI-REwithDR=0.4.Inmostsettingsinthehippocampus
analysis,theTensorSketchmethodsrunoutofmemory(evenwhen
usingvariantsofthealgorithmsthatneverholdthefulldatatensor
inmemory). Figure2:HOOIandHOOI-RE(DR=0.5)reconstructionsfor
In the face analysis, for the same value of𝑅 (which controls 16randomlyselectedfacesinthefaceanalysis.Theoriginal
thecoarsenessofthedecomposition),HOOI-REappearstoencode dataisvisualizedinFigure4.
thesamelevelofdetailasHOOI,albeitwithextranoise(Figure2).
HOOIandHOOI-REreconstructionsinthehippocampusanalysis
arealsocomparable(Figure3).Foranillustrationoftheeffectof
Forsmallmodelsorwhendimensionreductionisnotsubstantial
increaseddimensionreductioninthefaceanalysis,seeFigure4.
(e.g., when𝑅 = 5 or DR = 0.8), HOOI-RE and HOOI-RE* tend
)s(
emiT
)s(
emiT
5.1
0.1
5.0
0.0
07
05
03
01
0
rorrE
rorrE
004
003
002
001
0
0006
0004
0002
0OblivioussubspaceembeddingsforcompressedTuckerdecompositions Abbr,MMMDD–DD,YYYY,Location
Original DR 0.8
HOOI (RD) HOOI-RE (RD)
DR 0.6 DR 0.4
HOOI (mTBM3) HOOI-RE (mTBM3)
Figure4:EffectofdimensionreductiononHOOI-RErecon-
Figure3:HOOIandHOOI-REreconstructions(𝑅=30,DR= structionsinthefaceanalysis(𝑅 = 30)for16randomlyse-
0.5)oftheradialdistance(RD)andmTBM3imagingmeasures lectedimages.
forasinglepatientinthehippocampusanalysis.
problemsizesubstantiallyandobtainhigh-qualityTuckerdecom-
positionsinafractionofthetime.RelativetotraditionalHOOI
to require more computation time and, as before, yield greater andHOSVD,ourmethodappearstoscalewellwithdatasize𝑛
reconstructionerrorthanHOOI.Thereasonforthisisclearfrom anddecompositionsize𝑅.Theproposedmethodalsosubstantially
Figure5,whichsummarizesaveragetimeperiterationspentoneach outperformedrecentlydevelopedTensorSketchmethodsforHOOI
partoftheHOOIalgorithms(withnumericalresultsinTable2ofthe intermsofreconstructionerrorand,particularlyforlargertensors,
appendix).Briefly,usingembeddingsincursanoverheadcostthat computationtimeandcomputationalfeasibility.
mayormaynotbeoutweighedbytheimprovementincomputation While “compressed” tensor decompositions are not the only
timeneededtoupdatethefactormatricesorcoretensor.There general tool needed, decompositions and low-rank approxima-
isanetimprovementwhenthesizeofthedecompositionislarge tionsarearguablyanimportantpartofmanytensormethods[13].
or when the amount of dimension reduction is substantial (i.e., Ourresultsencourage furtherapplications totensorregression
large𝑅orsmallDR).HOSVD,likeHOOI,spendsalargemajority andotherspecializedmethods,particularlythoseseekingarich
ofitsruntimeonupdatestothefactormatrices(Table2inthe model/decompositionspacethroughTuckerrepresentations.There
appendix).Ourresultshighlighthowtheproposedapproachwith aresettingswhereTuckerdecompositionsmaybepreferredoverCP
JLembeddingscanreduceproblemdimensionalityandmitigate decompositionsforreasonsbeyondflexibility.Thelatterrequires
thiscomputationalbottleneckwhilepreservingtheintegrityofthe the𝑅 stobeequal,butitmaybemoreparsimonioustouseaTucker
𝑗
estimateddecompositioncomponents. decompositionwithgreatlyvarying𝑅 s[11].Whenanalyzingour
𝑗
hippocampusdata,forexample,onemaydesiregreatervariability
4 SIGNIFICANCEANDIMPACT betweenpatient-levelreconstructions,soitmaybepreferableto
Theimportanceofefficientmethodsfortensoranalysisgrowsto-
have𝑅5(alongthe“patient”mode)largeandtheother𝑅 𝑗ssmall.
ExpandedresultsforTuckerdecompositions,suchasthoseinthis
getherwiththesizeandrichnessofdataavailabletoresearchers.
work,canthussupportdomain-specificdevelopmentsevenifcor-
Thisisespeciallytrueinappliedfieldswheredomain-specificin-
respondingresultsforCPdecompositionsexist.5
sightistetheredtodataacquisitionandrelatedtechnology.Medical
WeconsideredanHOOIalgorithminthisworkpartlybecause
imagingisaprimeexampleofthis.Datasizemayinitiallyencour-
ofitsrarityintheliteraturerelativetoHOSVD.Morespecifically,
ageresearcherstoreduceoraltogethereliminatetensordatafrom
while HOSVD is typically favored for computational speed [4],
ananalysisplan—e.g.,bysummarizingneuroimagingfeaturesover
HOOI(andthus,algorithmsthatwidentheapplicabilityofHOOIto
predefinedregionsofinterestorbydownsamplingtoamanageable
largetensordata)areofspecificinterestforthesakeforimproved
size,evenwhennotstatisticallyjustifiable.However,tensor-based
methodsfordimensionreductioncanmakelarge-scaleanalyses
5WhileTheorem2.4appearstoberestrictedto𝚲,Theorem2.6canbemodifiedto
feasibleonreadilyaccessiblecomputingresources.Theempirical
resultsinSection3showthatourapproachtoHOOIcanreduce a ac pc po lyun totf 𝚲or bt yhi as d( jP uL st⊥ in( g𝑿 L) 𝑗= .W0) ec ha ase v. eS oim mi il ta tr el dy, tT heh se eor de em tai2 ls.6 foca rn brb ee ve ita ys .ilymodifiedtoAbbr,MMMDD–DD,YYYY,Location MatthewPietrosanu,BeiJiang,andLinglongKong
Figure5:AveragetimespentperiterationoneachpartoftheHOOI,HOOI-RE*,andHOOI-REalgorithmsintheface(left)and
hippocampus(right)analyses,averagedover100replications.Barsrepresentstandarderror.Preprocessingtimeisnotincluded,
butseeTable2intheappendixforquantitativesummaries.
decompositionquality.Ourresultsinfactshowthattheproposed quality.Wedidnotinvestigatethistheoreticallyorpresentrelated
HOOImethodcanoutperformHOSVDintermsofbothcompu- empiricalresultsinthiswork.Ourchoicetovarythe𝑺𝑗swaspri-
tationtimeanddecompositionquality,withtheperformancegap marilymotivatedbyourinterestinthissourceofrandomness,from
growingwiththesizeandorderofthedata.Themajorityofthe theperspectiveofposteriorapproxmationinBayesiansettings[12].
improvementincomputationtimeperiterationappearstostem Specifically,workonBayesianhierarchicaltensormodelingiscur-
fromimprovementsinfactormatrixupdates,inturnduetofaster rently quite limited: while very few works consider dimension
SVDs.RandomizedSVDhasitselfreceivedmuchattentioninthe reductionforBayesianregressionwithnon-tensordata[7,8],none
broadertensorliterature[1]. considerthisinthecontextofjointtensormodels.Wearecurrently
Weacknowledgethatfurtherimprovementinruntimeispossible developingacompressedBayesianTuckerdecompositionthatwe
byfixingthesamplingmatrices(i.e.,the𝑺𝑗s)acrossiterations(sim- ultimatelyaimtoincorporateintojointhierarchicalmodels.
ilartothefixedsketchesin[15]),butthisinourexperiencetends Wedidnotconsiderasecondstageofdimensionreduction(e.g.,
toincreasereconstructionerrorandvariationinreconstruction byvectorizingandembeddingthecompressedtensorintoalow-
dimensionalvectorspacesuchasR𝑚)asinIwenetal.[9]fortheCP
decomposition.AsimilarresultfortheTuckerdecompositionmay
Table1:Mean(standarddeviation)runtime(s)andFrobenius followreadily,butwehavenotconsideredthathereandleavethe
reconstruction error for the HOOI, HOOI-RE, HOOI-RE*, developmentforfuturework.Inanothervein,theoreticalconver-
HOSVD,andTensorSketchalgorithms(with𝐾 =10)inboth genceguaranteesaregenerallydifficulttoobtainforALSalgorithms
numericalstudies,calculatedover100simulations. (includingHOOI,evenwithoutrandomembeddings)[10],sowe
havenotconsideredsuchresultshere,norhaveweconsidereda
Method DRTime Error formalruntimeanalysis.
𝑅=5 𝑅=15 𝑅=30 𝑅=5 𝑅=15 𝑅=30
Face
HOOI 0.47(0.02) 0.57(0.01) 1.13(0.04) 237.5(0.0) 186.4(0.0) 158.1(0.0)
HOOI-RE*0.8 0.58(0.14) 0.55(0.10) 0.60(0.12) 240.0(0.8) 189.6(0.5) 161.9(0.5) REFERENCES
0.6 0.44(0.08) 0.42(0.07) 0.47(0.07) 241.2(1.8) 190.6(0.6) 163.2(0.6) [1] SalmanAhmadi-Asl,StanislavAbukhovich,MaameG.Asante-Mensah,Andrzej
0.4 0.39(0.07) 0.34(0.06) 0.38(0.07) 242.9(1.7) 193.0(1.0) 166.1(1.1) Cichocki,AnhHuyPhan,TohishisaTanaka,andIvanOseledets.2021.Random-
0.2 0.32(0.06) 0.27(0.04) 0.31(0.06) 249.4(3.1) 201.5(2.3) 176.3(2.6)
izedalgorithmsforcomputationofTuckerdecompositionandhigherorderSVD
HOOI-RE0.8 0.68(0.16) 0.70(0.13) 0.78(0.15) 238.3(0.5) 188.0(0.2) 160.8(0.2)
0.6 0.49(0.12) 0.48(0.08) 0.50(0.09) 239.7(1.2) 190.4(0.4) 166.3(0.5)
(HOSVD).IEEEAccess9(2021),28684–28706. https://doi.org/10.1109/ACCESS.
2021.3058103
0.4 0.38(0.09) 0.34(0.06) 0.30(0.06) 242.4(1.7) 196.5(0.8) 197.3(5.1)
[2] MicheleN.daCosta,RenatoR.Lopes,andJoa¯oMarcosT.Romano.2016.Random-
0.2 0.28(0.05) 0.24(0.04) 0.19(0.04) 252.5(3.2) 284.7(32.9) 831.9(42.5)
HOSVD 0.45(0.04) 0.45(0.04) 0.45(0.05) 241.0(0.0) 186.8(0.0) 158.2(0.0)
izedmethodsforhigher-ordersubspaceseparation.In201624thEuropeanSignal
TS 0.25(0.01)106.24(9.42) † 262.9(4.0) 199.7(0.4) †
ProcessingConference(EUSIPCO).215–219. https://doi.org/10.1109/EUSIPCO.
2016.7760241
TTMTS 0.19(0.01) 2.97(0.06)144.22(3.24) 480.7(26.4) 394.1(6.9) 370.7(2.8)
[3] Sanjoy Dasgupta and Anupam Gupta. 2003. An elementary proof
Hippocampus
HOOI 14.60(0.28) 34.01(0.28) 44.74(0.41) 4480.1(0.0) 3481.5(0.0) 3099.2(0.0) of a theorem of Johnson and Lindenstrauss. Random Structures &
HOOI-RE*0.8 33.90(7.41) 42.72(8.05) 46.37(6.62) 4483.1(1.8) 3486.3(1.2) 3106.2(1.4) Algorithms 22, 1 (2003), 60–65. https://doi.org/10.1002/rsa.10073
0.6 20.88(3.75) 24.81(4.02) 26.69(3.57) 4487.7(2.4) 3492.7(2.7) 3116.2(2.5) arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/rsa.10073
0.4 13.07(2.17) 15.03(2.27) 16.43(2.36) 4499.5(9.8) 3504.8(4.6) 3135.4(4.7) [4] LievenDeLathauwer,BartDeMoor,andJoosVandewalle.2000. Onthe
0.2 9.23(1.20) 9.32(1.19) 10.85(1.48)4533.8(22.8) 3546.3(11.2)3200.5(12.9) bestrank-1andrank-(𝑅1,𝑅2,...,𝑅𝑁)approximationofhigher-ordertensors.
HOOI-RE0.8 29.98(7.11) 40.31(5.96) 44.47(7.56) 4490.6(2.9) 3494.3(2.1) 3124.7(2.6) SIAMJ.MatrixAnal.Appl.21,4(2000),1324–1342. https://doi.org/10.1137/
0.6 18.14(3.69) 21.30(3.18) 22.35(3.71)4508.4(10.5) 3516.1(3.5) 3179.9(9.5) S0895479898346995
0.4 11.68(2.02) 12.26(1.91) 11.16(1.68)4550.7(25.1) 3574.7(11.1)3466.8(58.8) [5] PetrosDrineasandMichaelW.Mahoney.2007.Arandomizedalgorithmfora
0.2 7.28(0.57) 7.28(0.89) 6.68(0.72)4712.4(69.1)4462.7(292.7)9900.4(11.2) tensor-basedgeneralizationofthesingularvaluedecomposition.LinearAlgebra
HOSVD 42.35(1.03) 42.35(0.87) 42.94(1.18) 5856.1(0.0) 4446.4(0.0) 3936.4(0.0) Appl.420,2(2007),553–571. https://doi.org/10.1016/j.laa.2006.08.023
TS 42.52(1.78)† † †4790.0(12.5) † † [6] MinaGachloo,YuxingWang,andJingboXia.2019. Areviewofdrugknowl-
TTMTS 6.63(0.11) † †6421.4(87.5) † † edgediscoveryusingBioNLPandtensorormatrixdecomposition.Genomics&
†indicatesthattheTSorTTMTSalgorithmranoutofmemory.Aversionofthe Informatics17,2(2019),e18.
[7] RajarshiGuhaniyogiandDavidB.Dunson.2015.Bayesiancompressedregres-
algorithmthatdoesnotholddatainmemory[15]wasusedinsteadwherepossible,
sion.J.Amer.Statist.Assoc.110,512(2015),1500–1514. https://doi.org/10.1080/
butinmostcasesalsoranoutofmemory. 01621459.2014.969425OblivioussubspaceembeddingsforcompressedTuckerdecompositions Abbr,MMMDD–DD,YYYY,Location
[8] RajarshiGuhaniyogiandAaronScheffler.2021. SketchinginBayesianhigh
dimensional regression with big data using Gaussian scale mixture priors.
arXiv:2105.04795[stat.ME]
[9] MarkA.Iwen,DeannaNeedell,ElizavetaRebrova,andAliZare.2021. Lower
memoryoblivious(tensor)subspaceembeddingswithfewerrandombits:Mode-
wisemethodsforleastsquares.SIAMJ.MatrixAnal.Appl.42,1(2021),376–416.
https://doi.org/10.1137/19M1308116
[10] TamaraG.KoldaandBrettW.Bader.2009.Tensordecompositionsandapplica-
tions.SIAMRev.51,3(2009),455–500. https://doi.org/10.1137/07070111X
[11] XiaoshanLi,DaXu,HuaZhou,andLexinLi.2018. Tuckertensorregression
andneuroimaginganalysis.StatisticsinBiosciences10,3(2018),520–545. https:
//doi.org/10.1007/s12561-018-9215-6
[12] JunS.LiuandYingNianWu.1999.Parameterexpansionfordataaugmentation.
J.Amer.Statist.Assoc.94,448(1999),1264–1274.
[13] YipengLiu.2022.TensorsforDataProcessing:Theory,MethodsandApplications.
Elsevier.
[14] LinjianMaandEdgarSolomonik.2021.Fastandaccuraterandomizedalgorithms
forlow-ranktensordecompositions.InAdvancesinNeuralInformationProcessing
Systems,A.Beygelzimer,Y.Dauphin,P.Liang,andJ.WortmanVaughan(Eds.).
https://openreview.net/forum?id=B4szfz7W7LU
[15] OsmanAsifMalikandStephenBecker.2018.Low-rankTuckerdecompositionof
largetensorsusingTensorSketch.InAdvancesinNeuralInformationProcessing
Systems,S.Bengio,H.Wallach,H.Larochelle,K.Grauman,N.Cesa-Bianchi,and
R.Garnett(Eds.),Vol.31.CurranAssociates,Inc. https://proceedings.neurips.cc/
paper_files/paper/2018/file/45a766fa266ea2ebeb6680fa139d2a3d-Paper.pdf
[16] OsmanAsifMalikandStephenBecker.2020.GuaranteesfortheKroneckerfast
Johnson–Lindenstrausstransformusingacoherenceandsamplingargument.
LinearAlgebraAppl.602(2020),120–137. https://doi.org/10.1016/j.laa.2020.05.004
[17] RachelMinster,ArvindK.Saibaba,andMishaE.Kilmer.2020. Randomized
algorithmsforlow-ranktensordecompositionsintheTuckerformat. SIAM
JournalonMathematicsofDataScience2,1(2020),189–215. https://doi.org/10.
1137/19M1261043
[18] BeheshtehRakhshanandGuillaumeRabusseau.2020.Tensorizedrandompro-
jections.InProceedingsoftheTwentyThirdInternationalConferenceonArtificial
IntelligenceandStatistics(ProceedingsofMachineLearningResearch,Vol.108),
SilviaChiappaandRobertoCalandra(Eds.).PMLR,3306–3316.
[19] Charalampos E. Tsourakakis. [n.d.]. MACH: Fast randomized tensor
decompositions. 689–700. https://doi.org/10.1137/1.9781611972801.60
arXiv:https://epubs.siam.org/doi/pdf/10.1137/1.9781611972801.60
[20] YalinWang,YangSong,PriyaRajagopalan,TuoAn,KrystalLiu,Yi-YuChou,
BorisGutman,ArthurW.Toga,andPaulM.Thompson.2011. Surface-based
TBMboostspowertodetectdiseaseeffectsonthebrain:AnN=804ADNIstudy.
NeuroImage56,4(2011),1993–2010. https://doi.org/10.1016/j.neuroimage.2011.
03.040
[21] Hua Zhou, Lexin Li, and Hongtu Zhu. 2013. Tensor regression with
applications in neuroimaging data analysis. J. Amer. Statist. Assoc.
108, 502 (2013), 540–552. https://doi.org/10.1080/01621459.2013.776499
arXiv:https://doi.org/10.1080/01621459.2013.776499Abbr,MMMDD–DD,YYYY,Location MatthewPietrosanu,BeiJiang,andLinglongKong
A ADDITIONALBACKGROUNDRESULTS Multiplyingthegeneraltermaboveby∥𝑩Γ 𝑗,𝑟𝑗∥/∥𝑩Γ 𝑗,𝑟𝑗∥(whichis
LemmaA.1. If𝑥,𝑦 ∈R𝑛 and𝑨∈R𝑚×𝑛 isan𝜀-JLembeddingof possibleunderthehypothesisthatmin 𝑟∈[𝑅𝑗]∥𝑩Γ 𝑗,𝑟∥2 >0)yields
{𝑥±𝑦}⊂R𝑛 intoR𝑚
,then
thefirstpartoftheclaim.
𝜀 Fromtheformof𝒀′above,itisclearthat𝜇 𝒀′,𝑘 =𝜇 𝒀,𝑘 for𝑘 ≠ 𝑗.
(cid:12) (cid:12)⟨𝑨𝑥,𝑨𝑦⟩−⟨𝑥,𝑦⟩(cid:12) (cid:12)≤ 2(∥𝑥∥2 2+∥𝑦∥2 2) Ontheotherhand,
≤𝜀max{∥𝑥∥2 2,∥𝑦∥2 2}. 𝜇 𝒀′,𝑗 = 𝑟,𝑠m ∈a [𝑅x
𝑗]
∥𝑩|⟨ Γ𝑩 𝑗Γ ,𝑟𝑗 ∥,𝑟 2, ∥𝑩 𝑩Γ Γ𝑗, 𝑗𝑠 ,𝑠⟩ ∥| 2.
Proof. Theclaimfollowsbyroutinemanipulation.Observethat
𝑟≠𝑠
Forthefinalpartoftheclaim,observethat
(cid:12) (cid:12)⟨𝑨𝑥,𝑨𝑦⟩−⟨𝑥,𝑦⟩(cid:12)
(cid:12) ∥𝒀′∥=∥𝒀 ×𝑘 𝑩∥2 =∥(𝒀 ×𝑗 𝑩) [𝑗]∥2
F
=(cid:12) (cid:12) (cid:12)1 4(cid:0)∥𝑨𝑥 −+
1
4𝑨 (cid:0)𝑦 ∥∥ 𝑥2 2 +− 𝑦∥ ∥𝑨
2
2𝑥 −− ∥𝑥𝑨 −𝑦∥ 𝑦2 2 ∥(cid:1)
2 2(cid:1)(cid:12) (cid:12)
(cid:12)
=(cid:13)
(cid:13) (cid:13)𝑩𝚪 𝑗𝚲
[𝑗](cid:16)(cid:204)
𝑘 𝑘=
≠1
𝑞
𝑗
𝚪
𝑘(cid:17)⊤(cid:13)
(cid:13)
(cid:13)2
F
= 1 4(cid:12) (cid:12) (cid:12)(cid:0)∥𝑨𝑥+𝑨𝑦∥2 2−∥𝑥+𝑦∥2 2(cid:1) =∥𝑩𝚪 𝑗𝚿 𝑗∥2 F.
≤
41(cid:104)(cid:12)
(cid:12)∥𝑨𝑥− +(cid:0) 𝑨∥𝑨 𝑦𝑥
∥2
2− −𝑨 ∥𝑦 𝑥∥ +2 2 𝑦− ∥∥
2
2𝑥
(cid:12)
(cid:12)
−𝑦∥2 2(cid:1)(cid:12) (cid:12) (cid:12) Thus,bydir ∥e 𝒀ct ′∥c 2om =p 𝑁∥u
𝑩
−t 𝑗a 𝚪t 𝑗io 𝚿n 𝑗, ∥2
F
+(cid:12) (cid:12)∥𝑨𝑥−𝑨𝑦∥2 2−∥𝑥−𝑦∥2 2(cid:12) (cid:12)(cid:105) =∑︁ 𝑖=1(cid:13) (cid:13)𝑩𝚪 𝑗Ψ 𝑖(cid:13) (cid:13)2 2
≤ 𝜀1 4(cid:0)𝜀∥𝑥+𝑦∥2 2+𝜀∥𝑥−𝑦∥2 2(cid:1) =𝑁 ∑︁−𝑗(cid:13)
(cid:13)
(cid:13)𝑩∑︁𝑅𝑗
Γ 𝑗,𝑟𝜓
𝑟,𝑖(cid:13)
(cid:13)
(cid:13)2
2
= 2(cid:0)∥𝑥∥2 2+∥𝑦∥2 2(cid:1) 𝑁𝑖= −1
𝑗
𝑟 𝑅= 𝑗1
𝑅𝑗
≤𝜀max{∥𝑥∥2 2,∥𝑦∥2 2}, =∑︁ ⟨𝑩∑︁ Γ 𝑗,𝑟𝜓 𝑟,𝑖,𝑩∑︁ Γ 𝑗,𝑠𝜓 𝑠,𝑖⟩
𝑖=1 𝑟=1 𝑠=1
whichprovestheclaim.Above,thefirstequalityholdsbythepolar-
izationidentityandtheinequalitiesbythetriangleinequality,the ∑︁𝑅𝑗 ∑︁𝑅𝑗 (cid:16)𝑁 ∑︁−𝑗 (cid:17)
hypothesisthat𝑨isan𝜀-JLembedding,andbytheparallelogram = 𝜓 𝑟,𝑖𝜓 𝑠,𝑖 ⟨𝑩Γ 𝑗,𝑟,𝑩Γ 𝑗,𝑠⟩
𝑟=1𝑠=1 𝑖=1
law(equivalently,bybasicpropertiesofinnerproducts). □
𝑅𝑗 𝑅𝑗
ThefollowingisaproofofLemma2.1ofthemaintext.
=∑︁∑︁ (𝚿𝚿⊤)𝑟,𝑠⟨𝑩Γ 𝑗,𝑟,𝑩Γ 𝑗,𝑠⟩.
𝑟=1𝑠=1
Proof. Theclaimfollowsbyroutinemanipulationandtheprop-
Above,thesecondequalityholdssince,foranyarbitrary𝑨∈R𝑛×𝑚,
ertiesoftensormatricization.Observethat ∥𝑨∥2
F
=(cid:205)𝑚 𝑖=1∥𝐴 𝑖∥2 2.Thethirdandfinalequalitiessimplyusethe
definitionofmatrixmultiplication.Thiscompletestheproof. □
𝒀 [′
𝑗]
=𝑩𝒀[𝑗]
𝑅1 𝑅𝑞 𝑞 ReceivedXXXXXXX20XX;revisedXXXXXXX20XX;acceptedXXXXXXX
=𝑩∑︁ ···∑︁ 𝜆 𝑟1,...,𝑟𝑞(cid:0) ⃝ Γ 𝑘,𝑟𝑘(cid:1)
[𝑗]
20XX
𝑟1=1 𝑟𝑞=1 𝑘=1
∑︁𝑅1 ∑︁𝑅𝑞 (cid:16)(cid:204)1
(cid:17)⊤
=𝑩 ··· 𝜆 𝑟1,...,𝑟𝑞Γ 𝑗,𝑟𝑗 Γ 𝑘,𝑟𝑘
𝑟1=1 𝑟𝑞=1 𝑘=𝑞
𝑘≠𝑗
∑︁𝑅1 ∑︁𝑅𝑞 (cid:16)(cid:204)1
(cid:17)⊤
= ··· 𝜆 𝑟1,...,𝑟𝑞(𝑩Γ 𝑗,𝑟𝑗) Γ 𝑘,𝑟𝑘 .
𝑟1=1 𝑟𝑞=1 𝑘=𝑞
𝑘≠𝑗
Thus,
𝒀′ =
∑︁𝑅1 ···∑︁𝑅𝑞
𝜆
𝑟1,...,𝑟𝑞(cid:16)𝑗 ⃝−1
Γ
𝑘,𝑟𝑘(cid:17)
◦(𝑩Γ 𝑗,𝑟𝑗)◦
𝑟1=1 𝑟𝑞=1 𝑘=1
(cid:16) 𝑞 (cid:17)
⃝ Γ 𝑘,𝑟𝑘 .
𝑘=𝑗+1OblivioussubspaceembeddingsforcompressedTuckerdecompositions Abbr,MMMDD–DD,YYYY,Location
Table2:Mean(standarddeviation)time(ms/iteration)spentoneachpartoftheHOOI,HOOI-RE*,HOOI-RE,andHOSVD
algorithmsinbothnumericalstudies,calculatedover100simulations.
Method DR𝑅=5 𝑅=15 𝑅=30
𝑨 𝚪 𝚲 𝑨 𝚪 𝚲 𝑨 𝚪 𝚲
Face
HOOI 0.0(0.0) 18.5(1.3) 3.8(0.6) 0.0(0.0) 23.4(2.2) 5.2(0.8) 0.0(0.0) 33.6(3.4) 8.8(0.9)
HOOI-RE*0.8 21.9(3.6) 14.8(0.8) 4.2(0.1) 22.2(2.9) 14.4(0.4) 4.7(0.1) 23.7(2.3) 19.2(2.2) 9.9(2.0)
0.6 14.0(0.1) 8.1(0.5) 3.5(0.5) 14.8(0.3) 8.7(0.2) 4.8(0.1) 15.4(0.3) 10.3(0.1) 8.7(0.1)
0.4 9.2(0.3) 4.9(0.2) 3.6(0.5) 9.2(0.1) 5.1(0.0) 4.6(0.0) 10.0(0.4) 6.6(0.8) 10.3(2.1)
0.2 5.2(0.2) 2.7(0.1) 3.0(0.0) 5.0(0.1) 2.6(0.2) 4.7(0.5) 6.0(0.1) 4.0(0.0) 11.4(0.3)
HOOI-RE 0.8 23.2(3.4) 14.4(0.6) 8.6(0.2) 23.1(2.8) 14.6(0.2) 8.8(0.1) 24.9(3.1) 17.5(0.3) 10.7(0.1)
0.6 14.9(0.3) 7.9(0.3) 4.8(0.1) 15.1(0.3) 8.2(0.1) 5.2(0.1) 16.2(0.3) 11.1(1.1) 6.7(0.3)
0.4 9.1(0.1) 4.6(0.1) 2.6(0.0) 9.7(0.3) 5.1(0.1) 3.0(0.0) 9.9(0.3) 5.9(0.1) 3.5(0.1)
0.2 5.0(0.1) 2.6(0.2) 1.7(0.2) 5.4(0.1) 2.9(0.1) 1.8(0.0) 6.6(0.6) 4.4(0.1) 2.8(0.2)
HOSVD 0.0(0.0) 445.7(43.4) 3.4(0.4) 0.0(0.0) 442.4(39.5) 5.1(0.7) 0.0(0.0) 440.6(45.2) 8.7(1.6)
Hippocampus
HOOI 1.0 0.0(0.0) 565.0(44.8) 64.6(10.1) 0.0(0.0) 920.2(6.9) 121.3(1.3) 0.0(0.0) 1692.5(17.8) 255.9(4.8)
HOOI-RE*0.8 1676.0(16.9) 764.6(16.1) 57.9(6.3)1724.9(20.5) 802.8(18.3) 125.2(6.7)1727.3(13.5) 823.6(5.6) 242.4(1.5)
0.6 934.9(12.2) 379.2(8.0) 64.0(6.8) 967.6(17.7) 408.3(12.8) 126.6(9.3) 999.5(11.9) 417.4(6.3) 245.7(2.4)
0.4 407.2(6.4) 148.8(4.4) 61.5(3.8) 398.8(5.6) 150.2(5.5) 121.5(5.9) 420.0(5.6) 168.4(11.1) 246.9(2.3)
0.2 74.8(2.1) 22.8(1.2) 70.0(9.4) 59.9(1.1) 20.4(1.1) 122.5(3.0) 77.5(2.5) 29.4(2.6)274.0(14.4)
HOOI-RE 0.8 1581.2(18.1) 739.0(4.3) 268.3(4.5)1665.9(19.0) 786.0(19.8) 306.1(7.7)1838.3(34.9) 865.0(16.9)410.0(11.6)
0.6 887.1(9.6) 358.9(3.4) 136.0(1.9) 918.8(9.5) 402.8(19.1) 153.5(2.2)1052.1(17.4) 433.1(14.1) 216.7(3.4)
0.4 428.0(4.2) 160.7(7.1) 52.8(1.7) 395.2(3.7) 148.0(1.8) 57.5(1.7) 441.5(7.0) 182.1(10.5) 89.1(4.1)
0.2 60.5(2.7) 20.6(1.5) 7.8(0.4) 63.3(1.3) 23.1(1.1) 9.5(0.3) 80.2(4.6) 33.2(2.1) 17.3(0.6)
HOSVD 1.0 0.0(0.0)42242.4(1032.7)108.7(14.2) 0.0(0.0)42131.2(861.3)219.4(21.0) 0.0(0.0)42484.0(1171.8)454.8(10.9)
Algorithmparts:𝑨,𝚪,and𝚲denotetheapplicationofrandomembeddings,updatesforthefactormatrices,andupdatesforthecoretensor,respectively.
HOSVD:HOSVDisnotaniterativealgorithmandterminatesafterasingleupdatetoeachofthefactormatricesandthecorematrix.
Omissions:Timespentformingtherandommatricesisnotprovidedforthesakeofspace;meantimefortheHOOI-RE*andHOOI-REalgorithmsacrossallsettingsrangedfrom0.2
to0.4(withstandarddeviationsfrom0.0to0.1)ms/iteration.Timespentpreprocessingisalsonotincluded,butisaccountedforbyothertablesandfiguresinthemaintext.
Specifically,timespent“mixing”thedatatookonaverage75.9(standarddeviation8.0)ms/replicationinthefaceanalysisand7868.3(standarddeviation746.7)ms/replicationinthe
hippocampusanalysis.
Tensor parameterization
3D template
Figure6:Visualizationofthe3Dtemplateand150×100×2parameterizationusedinthehippocampusanalysis.Thecolor
gradientsandsolidblacklinesindicatethecorrespondencebetweentherepresentations.