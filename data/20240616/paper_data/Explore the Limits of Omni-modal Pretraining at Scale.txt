Explore the Limits of Omni-modal Pretraining at Scale
YiyuanZhang1,4∗ HandongLi2,3∗ JingLiu2,3† XiangyuYue1
1MultimediaLab,TheChineseUniversityofHongKong
2SchoolofArtificialIntelligence,UniversityofChineseAcademyofSciences
3InstituteofAutomation,ChineseAcademyofScience4ShanghaiAILaboratory
Abstract
Weproposetobuildomni-modalintelligence,whichiscapableofunderstanding
anymodalityandlearninguniversalrepresentations. Inspecific,weproposeascal-
ablepretrainingparadigm,namedMultimodalContext(MiCo),whichcanscaleup
thenumbersofmodalitiesandamountofdata,togetherwiththemodelparameters,
in the pretraining process. With MiCo, the pretrained models show significant
emergentabilitiesinmultimodallearning,whichareevaluatedonthefollowing
tasks: i)single-modalityperceptionbenchmarksof10differentmodalities,ii)25
cross-modalityunderstandingtasksofretrieval,question-answering,captioning,
andiii)18multimodallargelanguagemodelbenchmarks. Ourmodelsestablish
37newrecordsforstate-of-the-artperformance. Wehopethatourresearchcould
contributetothedevelopmentofomni-modalintelligence. CodeandModels
1 Introduction
Working
Inthedevelopmentofartificialintelligence,scalablepre-traininghasemergedasapromisingpathway
towardsgeneralintelligence[1–5]. Additionally,pre-traininghasbeenestablishedasaneffective
Temporal Video Paired Depth Paired Normal Maps
Visual Content Caption
Training Omni-modal Intelligence
“Change a place, Will my life be better? when leaving here, I
know nothing When the hustle and bustle of the city covers
up the trembling of the heart, I always fantasize about a Learning Universal Representations
completely different life… Still far away In the scenery that I
could not reach when I was a child, I started to hear ”
Audio Paired Caption Video Paired Audio
“A man is speaking in a foreign language while
music plays in the background. ”, “The music is
playing” , “amale voice is being recorded.” .
Figure1:Omni-modalPretraining.Weproposecollectinglarge-scaleomni-modalpaireddata,includingtext,
image,video,depth,andnormalmaps,tolearnuniversalrepresentations.
approachforlearningmoregeneralandtransferablerepresentationsacrossvariousmodalities. For
∗EqualContribution
†CorrespondingAuthor
Preprint.Underreview.
4202
nuJ
31
]VC.sc[
1v21490.6042:viXraexample,CLIP[4]constructsmillion-scaletext-imagepairsforcross-modalcontrastivelearning,
makingitoneofthemostimpactfulfoundationmodelsinthecommunity[6,7]. Researchershave
furtherextendedthecapabilitiesofCLIP[4]tomoredatamodalities,e.g. audio[8],pointclouds[9],
andmorecomprehensivetasks, e.g. reasoningaboutimages/videoswithlargelanguagemodels
(LLMs)[10,11]. ThemaincontributionsofCLIP[4]aretwo-fold: collectingweb-scaletext-image
dataandproposingascalablevision-languagepretrainingparadigm. Asmoremodalitiese.g. audio,
video,and3Dcontent,aregettingwidelyusedinthismultimodalera[12,11,13,14,6,7],such
developmentspresentadditionalchallenges,includingmultimodalmisalignment,misinterpretation,
andbiasamplification,inachievingcoherentmultimodalunderstandingwithLLMs.
Working
Inthispaper,weaimtoenhancethecomprehensiveabilitiesofCLIPinvisualunderstandingand
furtherbolsteritsmultimodalcapacitiesacrossaudio,video,3Dcontent,andmore,asillustrated
inFigure1. Thisissignificantlychallenging. Therefore,weshiftourfocusfromtrainingageneral
multimodalmodeltounderstandinghowthehumanbrainperformscoherentmultimodalcognition.
AsoutlinedinRichardMayer’sCognitiveTheoryofMultimediaLearning[15],ourbrainprocesses
multimedia signals through two distinct channels—auditory and visual—in sensory memory, as
depictedinFigure2. ThesensorymemOomryni-iEnntecgodraetre (sVitTh)esesignalswithpriorknowledgethrough
words,transformingnewmultimediainformationintolong-termmemory. Notably,1)multimedia
signalsinthebrainsharechannels,and2)wordsfunctionasthereasoninginterfaceinourbrain.
Interface Sensory Memory Alignment Generative Reasoning
Words
LLM
Ears
“Interface” Modality
ViT
Prior Photos
Knowledge Eyes Learned
“Knowledge” Modality
Representations
(a) Dual-Channel Multimodal Cognition Theory (b) Brain-Inspired Omni-modal Learning Architecture
Figure2:MultimediaCognitionProcessinBrainInspiresourDesign.Wesplitdiversemodalitiesintotwo
typesandemployindividualneuralnetworkstolearnrepresentationsfromeachtyperespectively.
Inspiredbytheseinsights,wecategorizediversemodalitiesintotwotypes: “knowledgemodality”
and“interfacemodality”. Knowledgemodalities, primarilyderivedfromrawsensors, contribute
knowledgeindiverseformats. Forexample,imagesanddepthmapsoffervisualknowledge,while
audioandvideoprovideauditoryandspatiotemporalknowledge. Thelanguagemodality,developed
byhumans,isinherentlymoreabstractandnaturallyfunctionsastheinterfacemodality,facilitating
learning, reasoning, and the coordination of knowledge. To this end, we design an omni-modal
learning architecture, illustrated in Figure 2 (b), with two distinct branches: one for knowledge
modalitiesandonefortheinterfacemodality,i.e. naturallanguage. Theknowledgeandinterface
modalitiesarealignedthroughanovelgenerativereasoningmethod,asdetailedin§3.3.
Inadditiontothearchitecturedesign,thenextchallengeishowtofurtherenhancethebenefitsofinte-
gratingmultipledatamodalities. Thekeytolearningtokensequencesisthecontextrelationship[21],
whichassignsauniquevectortoeachinputpositioninasequence. Thisapproachimprovessequence
modelingbycapturingthesequentialrelationshipamongtokens. Moreover,sincedifferentmodalities
(e.g.,text,image,audio)offercomplementaryinformation,integratingthesesourcesfostersamore
comprehensiveunderstandingofthedata. Modelingtokensequencesfromdifferentmodalitiesunder
thesamecontextcanhelpthemodelunderstandmodalitycharacteristicsandjointsemantics.
Therefore,weproposeaMultimodalContext(MiCo)framework. Wefirstmapdifferentmodalities
intoajointembeddingspacebysharingbackbonenetworks. Thenwebuildcontextualrelationships
bysharingthesamepositionembeddingsandutilizingadditionalcontextembeddingstoenhance
coherentmultimodalunderstanding,asshowninFigure1. Subsequently,weemployomnimodal
contrastivelearning,omnimodalfeaturematching,andomnimodalcaptiongenerationprocessesfor
pretraining(detailedin§3.4). Moreover,MiCocanincorporateexistingtext-image,text-audio,and
text-videodatasetsforjointmultimodalcontextlearning(§3.3),whichleadstobetteromni-modal
learningcapacity,furthermodalityextensibility,andeasierscalabilityofmultimodaldata.Meanwhile,
wearethefirsttoexploremultimodalscalinglawsinpretrainingmodalities,modelparameters,and
datascales(detailedinFigure6).
2(a) Masked Modelling with Autoencoders (b) Contrastive Learning (c) Multimodal Context (Ours)
“When leaving here, I know nothing. When SimCLR, MoCo, etc. Visual Caption
the hustle and bustle of the city covers up the
trembling of the heart”
Audio Caption
CLIP
1. Feature Contrastive learning
“A trio of roses in soft
BERT, MAE, AudioMAE, etc. hues of pink and coral” 2. Token Masked Generation
Transferable, Omni-modality, Transferable,
General-purpose, Single-modality
Modality Tuples General-purpose
Figure3:EvolutionofPretrainingParadigms.Maskedmodeling [16–18]hasshowngreatsuccessinsingle-
modalitygeneral-purposeunderstanding. Contrastivelearning[19,4,20]distinguishestransferablefeatures
withmodalitytuples. Weaimtoachievegeneral-purposeomni-modalunderstandingandlearntransferable,
universalrepresentations.
AsshowninFigure3,wecompareMiCowithexistingpretrainingapproaches. Withomnimodal
contrastivelearning, omnimodalfeaturematching, andomnimodalcaptiongenerationprocesses,
MiCo successfully integrates the advantages of both masked modeling and contrastive learning.
Inotherwords, MiCorepresentsthenext-generationevolutionofmaskedmodeling[16–18]and
contrastivelearningmethods[19,4,20]forthemultimodalera,offeringsignificantbenefitsinomni-
modallearning,strongtransferability,andgeneral-purposerepresentations. Tothoroughlyevaluate
theeffectivenessofMiCo,weconductextensiveexperimentsonuniversalsingle-modalityperception
benchmarks,cross-modalretrieval,captioning,andquestion-answer(QA)benchmarks,aswellas
zero-shotQAbenchmarksformultimodallargelanguagemodels. MiCoachievesimpressiveresults
acrossthesebenchmarks,establishingmorethan37newstate-of-the-art(SOTA)performancesand
showingremarkableimprovementsofover20%onsomebenchmarks. Theseresultscompellingly
illustratethatMiCoisapromisingnext-generationpretrainingparadigmforthemultimodalera.
2 RelatedWork
Vision-LanguagePretraining. MCAN[22]firstalignsvisionandlanguagefeaturesbystacking
deepcross-attentionblocks. Thenmoreworks[23–26]scaletheirmodelsandimprovethevision-
languagefusionprocesstobuildbetteralignment. VL-BERT[27]introducedtheMaskedLanguage
Model (MLM) paradigm, focusing on generic tasks across both vision and language modalities.
Then Oscar [28] proposed to enrich the representation of object semantics by integrating visual
andtextualcontent. Subsequentframeworkshavefurtherrefinedandextendedthesecapabilities.
Notably,VinVL[29],SimVLM[24],VLMO[23],ALBEF[30],andFlorence[31]haveexplored
anddemonstratedtheadvantagesofjointrepresentationsthatensuresemanticconsistencyacross
the visual and natural language. Additionally, the versatility of multimodal models extends into
specializedapplicationssuchasfew-shotlearning[32],andsequence-to-sequence[25,33]. BEiT-
v3[26]treatsimagesasa“foreignlanguage”,employingacross-modalmask-and-reconstruction
processwithpartiallysharedparameters.
More-ModalityPretraining. MMV[34]pioneeredmultimodalpretrainingusingtext,video,and
audiopairs. Theyproposedmultimodalcontrastivelearningforalignment. ThenVATT[35]further
developedpretrainingmultiplemodalitieswithtransformers. AfterCLIP[4],moreworks[36,14,8,
9,37,38]proposetoadaptpretrainedCLIPmodelstomoremodalitiesincludingpointcloud,depth,
audio,video,etc. Anotherdirectionistoexploitmultimodalcomplementarybenefitsandconstruct
betterandmoremodalitypairsforpertainingfoundationmodelssuchasVAST[39]andVALOR[40],
whichimprovetheabilitiesformultimodalunderstanding.
Despite significant advancements in multimodal learning, several key challenges impede the de-
velopmentofcomprehensiveomni-modalintelligence: 1)FocusonVision-LanguageModalities:
Currentmethods[26,23,28,24]predominantlycatertovisionandlanguagetasks. Theinflexibility
oftheseworkslimitstheextensionwithmoremodalitiessuchasvideo,depth,normals,andaudio.
2) Architectural Constraints: The development of architectures capable of handling a broader
arrayofmodalitiesisstillinitsnascentstages. Craftingscalableandefficientmultimodallearning
architecturespresentsasignificantchallenge. 3)DataAvailability: Thereisanotablescarcityof
publiclyaccessiblemultimodaldatasetsthatincludepaireddata(suchasvideo,depth,audio,and
3captions).4)UtilizingMultimodalBenefits:Althoughleveragingthesynergisticbenefitsofmultiple
modalitiesiscrucialforachievingomni-modalintelligence[41],understandingandoptimizingthe
interactionbetweenhighlydisparatemodalitiesremainsacomplexandlargelyunexploredarea.
3 MultimodalContext
3.1 Large-ScaleDataCollection
WeusetheHD-VILA[42]dataset,whichcontains371.5Khoursof720p(1280×720)videos. We
removevideoclipsthatareshorterthan5sorlongerthan30s. Then,wecollectadatasetcontaining
1.7Mpairedvideoclips(∼510Mframes),audio,andsubtitles{(x ,xV,x )}. Thenweenrichthe
V T A
datasetbyaddingcaptionstovideoframes(images),andaudiowithpretrainedcaptioners[39],getting
(x ,xI)and(x ,xA). Finally,weusepretrainedmonoculardepthestimationmodels[43,44]3to
I T A T
generatedepthandnormalmaps,getting(x ,x ,x ). Thus,wecollectmillion-scalemultimodal
I D N
paired data {(x ,x ,x ,xI),(x ,xA),(x ,xV)}, where x ,x ,x ,x ,andx denote the
I D N T A T V T T I A V D
modality-specificsamplesoftextcaptions,image,audio,videoclips,depth,andnormalmaps. We
splitourdatasetintoseveralsubsetsincluding1M,10M,110M,and334Mmultimodaldatapairs,
andweprovidedetailedillustrationsinAppendixC.
3.2 ArchitectureDesignforOmni-modalLearning
Wefirstinvestigateseveralvariantsofencoderarchitectureswithfourdatamodalities. Withour
collected data, we pretrain architectures for 300K steps by the same contrastive [4] and masked-
generationlossfunctions[26](detailsinAppendixB).Wetakethecaptioningandretrievaltaskson
image,audio,andvideomodalitiesasthemainevaluationbenchmarkfordesigningarchitectures.
Architectural Designs. We construct the vanilla architecture from CLIP [4]. A text encoder of
Transformer[21]takestextinputsandoutputstextembeddingsz ,andanimageencoderofVision
T
Transformer[45]takesimageinputx ∈R3×H×W andoutputsimageembeddingsz ,respectively.
I I
ViT ViT ViT BERT Text Encoder (BERT) LLM (LLama2) ViT LLM
(i) Modality-Specific (ii) Text Encoder Only (iii) LLM Decoder Only (iv) ViT+ LLM
Figure4:OptionsofArchitectureDesignforOmni-ModalPretraining.
AsshowninFigure4, wepropose4architecturesforomni-modallearning: i)Modality-specific
encodersforeachmodality,employingindividualtransformerstoextractmultimodalembeddings,
thenfusethemasBEiT-3[26]. ii)BERT(textencoder)asaunifiedmultimodalencodertoextract
multimodalembeddingsandgeneratestexts. iii)LLM(textdecoder)asaunifiedmultimodalencoder
andtextgenerator. iv)AViTasaunifiedmultimodalencoderbesidestext,andanLLMdealswith
textembeddingsandgeneration.
EmpiricalDiscovery.ReferringtoTable1,weconcludeseveralguidelinesfordesigningarchitectures
inomni-modallearning: 1)Purelanguagemodelsaredifficulttoretrievaltasks. Both(ii)and(iii)
deliverasignificantperformancedropinretrievaltasks. 2)Nomorethan2Encoders. Comparing
(i)with(ii)&(iii),weobservethatadditionalencodersarebeneficialforretrievaltasks;however,
comparisonbetween(i)and(iv)suggeststhatdiscrepanciesamongmultipleencoderscanalsohinder
multimodalalignment. 3)Languageisanindividualbranchforalignment. Comparing(ii)&(iii),
with(iv),improvementsaresignificantinbothretrievalandcaptioning.
3Geowizard[43]deliverssignificantlybetterannotationsobviously,whileDPT[44]predictsmuchfaster
(about34˜.7×faster).WeusetheGeowizardtoannotatethehigh-qualitydataabout2M.
4Table1:ArchitectureDesignofOmni-ModalLearningParadigm.ThedefaultbackboneandLLMareViT-g
andLlama-2-7B[46].Wepretrainmodelsfor300ksteps,thenevaluateperformancesontheMSRVTT,VATEX,
AudioCaps,ClothoV2,COCO,andFlickerdatasetsforcaption(CIDEr)andretrievaltasks(R@1).
Video Audio Image
Architecture
MSRVTT VATEX AudioCaps ClothoV2 COCO Flickr
CIDEr(%) R@1(%) R@1(%) CIDEr(%) R@1(%) R@1(%)
(i)Modality-Specific 74.3 73.5 42.3 22.3 65.2 88.4
(ii)TextEncoder(BERT) 77.0 53.2 23.1 43.9 46.7 51.6
(iii)LLM(LLama-2-7B) 75.2 60.3 14.7 43.6 60.8 81.3
(iv)ViT+LLM 77.9 79.5 49.7 47.2 67.5 90.5
3.3 MultimodalContextConstruction
Preliminary. Thecontextisproposedtoassignauniquevectortoeachtokeninasequence[21],
whichreinforcespotentialrelevancebetweenpositions. Differentmodalities(e.g.,text,image,audio)
provide complementary information. Learning multimodal context leads to a more holistic and
nuancedunderstandingofdata. Itcanalsoleveragethestrengthsofeachmodalityandguidethe
modeltounderstandtheinteractionsbetweendifferenttypesofinformation. Therefore, weseek
toconstructthecontextrelationshipacrossdiversemodalitiesandextendthelearningcapacityto
omni-modalies. Weprovidetheoverviewof
micopretrainingparadigminFigure5.
Single Dataset with Multimodal Paired Data. As mentioned in § 3.1, we build a dataset with
multimodalpaireddata{(x ,x ,x ,xI),(x ,xA),(x ,xV)},thenweemploytheomni-modal
I D N T A T V T
encoder f(·;θ) to extract features z ,z ,z ,z , andz , then use text encoder to extract text
I A V D N
featuresz . Therefore,weconstructthecontextbyatop-downdesign: 1)Forthewholemultimodal
T
embeddings, they share the same position embeddings E to build a modality-fused context
Pos
relationshipacrossdiversemodalities. 2)Then,foreachspecificcontext,they’relabeledbymodality
embeddingsincludingEI,EA,EV,ED,EN,etctoindicatemodalitytypes. 3)Withinthesame
M M M M M
modalitycontext,weemploythecontextembeddingsEI toconstructuni-modalcontextrelationships.
C
Thus,theconstructionofthemultimodalcontextcanbeformulatedas:
z =[z1,z2,··· ,zLI]+EI, foreachmodality,
I I I I C (1)
z =[z +EI,z +EA,z +EV,z +ED,z +EN]+E ,
I M A M V M D M N M Pos
whereEI isuptothesamplelengthofaspecificmodality. Meanwhile,thetextfeaturesofspecific
C
captionscanbeeasilyconcatenated,wheretheirpositionembeddingsE′ arealsoshared:
Pos
z =[zI,zA,zV]+E′ . (2)
T T T T Pos
Pleasenotethat,inthecontextconstruction,weusevanillapositionembedding[21]tobuildthese
contextsinalloftheselearnableembeddingsE ,E′ ,E , andE insteadofRotaryPosition
Pos Pos M C
Embedding(RoPE)[47],whichissimilartoEVA-CLIP-8B[48]andEVA-CLIP-18B.
MultipleDatasetsCombinationofCross-ModalDatasets. Besidesmultimodalpaireddata,our
proposed paradigm can also leverage existing web-scale text-image, text-audio, and text-video
datasetstojointlypretrainingmodelstowardsomni-modaluniversalrepresentations. Givendatasets
D ={(xj,xj)}NI ,D ={(xj ,xj)}NA, andD ={(xj ,xj)}NV ,eachpairofdatapossess
I I T j=1 A A T j=1 V V T j=1
localandsimplecontext,forexample,apairoftext-imagedata(x ,x )correspondstoasimple
I T
context(z +E ,E′ ),whichmaylimitthelearnedrepresentationsofmodels. Weproposeto
I Pos Pos
buildthemultimodalcontextbycross-datasetjointsamplingwithsamplingcontextembeddingE :
Sam
(x ,xI)=Sample(D ), (x ,xA)=Sample(D ), (x ,xV)=Sample(D ),
I T I A T A V T V
z =f(x ;θ)+ET−I, zI =f′(x ;θ′)+ET−I, foreachmodality, (3)
I I Sam T T Sam
z =[z +EI,z +EA,z +EV]+E , z =[zI,zA,zV]+E′ .
I M A M V M Pos T T T T Pos
Inthisway,wesuccessfullycombineexistingmultiplecross-modaldatasetstowardslearningomni-
modaluniversalrepresentationsbybuildingmoreuniversalandcomplicatedmultimodalcontexts
(Equation3)forpretrainingmodels,therefore,
micocanoutperformexistingpretrainingmethodsbybettergeneralizationlearningability,modality
extensibility,andeasierforscalingdata.
5Working
Omni-Modal Contrastive Learning Omni-Modal Feature Matching Omni-Modal Caption Generation
V1 VN D1 DN A1 AN V1 VN Masked
T1 Concatenate D1 DN
T2
Logits “Will you choose to travel and
T3 update your mind?”
Causal Inference
T4 A1 AN
…
“A man is speaking with music
TN playing in the background”
… … … … … …
Omni-Encoder (ViT) Text Encoder / LLM
“Change a place, “A man is speaking in
Will my life be better? a foreign language
when leaving here I while music plays in
know nothing” the background. ”,
Temporal Video Paired Depth Video Paired Audio Paired Normal Visual Content Caption Audio Paired Caption
Figure5: OverviewofMultimodalContextPretrainingParadigm. WeuseasharedViTformultimodal
featureextraction,andanotherbranchistoemployatextencoder.Weconcatenatethesemultimodalsequences
asmultimodalcontextsandperformcontrastivelearningandmaskedmodeling.
3.4 PretrainingObjectives
Omni-modal Contrastive Learning. The omni-modality representations are denoted as z. Sub-
sequently,z andz areprojectedintothesamespaceusingMLPs. Theomni-modalcontrastive
T
learningisformulatedbythedotproductofzandz . Weusevz andvT todenoteprojectedvectors:
T
1(cid:88)NB exp(τ·<vz,vT >) 1(cid:88)NB exp(τ·<vz,vT >))
L =− log i i − log i i , (4)
Con 2 (cid:80)NB exp(τ·<vz,vT >)) 2 (cid:80)NB exp(τ·<vz,vT >))
i=1 j=1 i j i=1 j=1 j i
where<·,·>,N andτ denotethedotproduct,batchsize,andalearnableparameter.
B
Omni-modalFeatureMatchingProcessisdesignedtoimprovethesemanticalignmentbetween
multimodal(knowledgemodalities)andtextualfeatures. WeemployanMLPlayertoperformbinary
predictions p of (z,z ). Following a hard negative mining strategy [30], we assigns y = 1 if
v T
featuresarematched,andy =0otherwise.
L =E [ylogp +(1−y)log(1−p )] (5)
Match (vz,vT)∼(Z,T) v v
i i
Omni-modalCaptionGenerationProcess. Weemployconditionalcausalmasked(60%)language
modelingforgenerativeomni-modalreasoning. Inspecific,asingle-directionalcausalattentionmask
isusedtoavoidinformationleakage,andthemaskedtokensarereconstructedusingaprediction
layerofBERT[18]. Weusec andc todenotemaskedtokensandformertokens,respectively.
m <m
L =−E logP (c |c ,vz) (6)
Gen (vT,vT)∼(V,T) m <m
i i
4 Experiment
4.1 ExperimentalSetup
We evaluate our model on three different benchmarks: 1) Single-modality Understanding § 4.2
(following previous practices [4, 36, 14] in fine-tuning & zero-shot setting in classification and
forecastingtasks),2)Cross-modalityUnderstanding§4.3(followingBEiT-3[26],VAST[39]infine-
tuninganddatasetsplitsforCaption,QA,andretrievaltasks),and3)MultimodalUnderstandingwith
6
* * * * * *LargeLanguageModels§4.4(followingLLava[49],VideoChat[50],OneLLM[12]inmultimodal
zero-shotQA).Detailedexperimentalsettingsincludingdatasetsintroduction,splits,andevaluation
metricscanbefoundinourAppendixC.
ImplementationDetails. WeimplementourpretrainingparadigmwithViTbackbonescalingfrom
ViT-B(8NVIDIATeslaA100GPUs)toViT-g(64NVIDIATeslaA100GPUs). Ittakesabout2∼6
daysforpretraining. Wepretrainmodels200kstepsforViT-B,300kstepsforViT-LandViT-g. The
initiallearningrateissetto1e-4,andalineardecayscheduleisused. ThebatchsizeoneachGPUis
setto1,024. MoreimplementationdetailscanbefoundintheAppendixB.
Table2: State-of-the-artAbilitiesofMiCoforOmni-modalPerception. Weconductexperimentsonthe
single modality evaluation following the same practice of previous Sotas. We report the Accuracy (%) of
MMLU[51],IN-1K[52],K700[53],NYU-D[54],Ego4D[55],IndianPines,andFrauddatasets,R@1(%)
forMSR-VTT[56]andSYSU[57],mAPforAS-2M[58],F1-scoreforFraud,andMeanAbsulteError↓for
PCQM4MandGlobalWeatherForecasting[59]benchmarks.
Methods(Backbone) Text Image Video Depth Audio Thermal IMU Graph Time-Series Hyperspectral Tabular
MMLU IN-1K K700/MSR-VTT NYU-D AS-2M SYSU Ego4D PCQM4M GlobalWeather IP Fraud
ImageBind(ViT-H)[14] 43.6 80.2 42.9/36.8 54.0 43.4 72.6 25.0 0.815↓ 8.439↓ 83.6 0.847
Meta-Trans(ViT-L)[36] 37.3 88.1 33.2/31.5 41.5 38.9 71.3 73.9 0.886↓ 7.892↓ 78.1 0.809
AbsoluteSOTA 90.0[60] 91.0[33] 92.1/62.8[38] 76.7[61] 48.6[62] 77.9[63] 52.5[64] 0.123[65] 7.602↓[13] 98.0[66] 0.860[67]
MiCo(ViT-g)[Ours] 68.9 89.8 91.6/64.3 84.6 50.5 80.3 77.2 0.742↓ 7.834↓ 98.5 0.913
4.2 EvaluationonSingle-modalityUnderstanding
Exceptional Omni-modal Perception Abilities. As shown in Table 2, MiCo achieves state-
of-the-art performances on a range of benchmarks across 10 modalities. For text understanding
(MMLU),MiCoattainstheaccuracyof68.9%,outperformingbothImageBind[14](43.6%)and
Meta-Transformer [36] (37.3%). In image recognition (IN-1K), MiCo delivers Top-1 Acc. of
89.8%. OnK700andMSR-VTT,MiCoachieves91.6%forAcc. andR@1of64.3%,outperforming
existingretrievalmethods. Regrading3Dsinge-viewtasks(NYU-D),MiCooutperformstheabsolute
SOTA[61]by+7.9%. OnAS-2M,MiCoachievesthemAPof50.5%,whichisbetterthanBEATS-
3 [62] by +1.9%. MiCo also excels in thermal sensing (SYSU) and IMU tasks (Ego4D), MiCo
achievesanaccuracyof80.3%and77.2%,respectively.TheseresultshighlightMiCo’scomprehensive
andoutstandingperformances,establishingitasapowerfulmodelforomni-modalperception.
Table3:PowerfulCross-ModalAbilities.WeevaluateMiCoonthemainstreamcross-modaltasksincluding
11retrievaltasks(COCO[68],Flickr[69],ClothoV1[70],ClothoV2[70],AudioCaps[71],MSRVTT[72],
YouCook2[73],VALOR-32K[40],VATEX[74],DEDeMo[75],andANET[76]),7captiontasks(COCO,
ClothoV1,ClothoV2,AudioCaps,MSRVTT,YouCook2,VALOR-32K),and6question-answer(QA)tasks
(TGIF[77],MSVD[78],VQAv2[79],MSRVTT,MUSIC [80],andANET)withthemetricsofR@1,CIDEr,
andAcc.Impressively,MiCoarchives20newSoTAperformances.
Text-to-ImageRetrieval ImageCaption VisualQA
Image COCO Flickr Flickr(ZS) COCO TGIF MSVD VQAv2
SOTA 68.3[81] 90.3[26] 89.7[81] 154.9*[82] 78.7[40] 60.2[83] 84.3[84]
MiCo 68.1 91.1↑0.8 90.1↑0.4 152.4 78.9↑0.2 60.4↑0.2 80.5
Text-to-AudioRetrieval AudioCaption
Audio ClothoV1 ClothoV2 AudioCaps ClothoV1 ClothoV2 AudioCaps
SOTA 17.5[40] 21.5[85] 42.2[85] 42.3[40] 48.8[85] 78.7[85]
MiCo 21.2↑3.7 23.3↑1.8 41.0 49.6↑7.3 50.8↑2.0 66.2
Text-to-Video-AudioRetrieval
Video-Audio MSRVTT YouCook2 VALOR-32K VATEX DiDeMo ANET
SOTA 54.4[40] 31.3[86] 73.2[40] 76.9[40] 57.6[40] 63.4[40]
MiCo 64.3↑9.9 51.3↑20.0 78.7↑5.5 81.3↑4.4 63.6↑6.0 68.5↑5.1
Video-AudioCaption Video-AudioQA
Video-Audio MSRVTT YouCook2 VALOR-32K MSRVTT MUSIC ANET
SOTA 74.0[40] 190.0[87] 61.5[40] 49.2[40] 78.9[40] 48.6[40]
MiCo 79.3↑5.3 197.8↑7.8 62.8↑1.3 50.4↑1.2 79.7↑0.8 51.0↑2.4
7Table 4: Evaluation on LLM Benchmarks. The MLLM evaluation involves 6 VQA tasks (GQA [88],
VQAv2 [89], OKVQA [90], TextVQA (TVQA) [91], ScienceQA (SQA) [92] and Vizwiz [93]), 2 image
captioningtasks(Nocaps[94]andFlickr30K[69]),and4multimodalbenchmarks(MME[95],MMBench
(MMB) [96], MMVet [97] and SEED [98]). The LLMs are Chinchilla [99], Vicuna [100], Qwen [101],
LLaMA[102]andLLaMA2[46].TheevaluationmetricsforVQAandcaptioningtasksareaccuracyandCIDEr,
respectively.Theresultsinboldandunderlinearethebestandsecond-bestresults,respectively.
VisualQuestionAnswering ImageCaption MMBenchmark
Method LLM
GQA VQAv2 OKVQA TVQA SQA Vizwiz NoCaps Flickr MME MMB MMVet SEED
VisionSpecialistLLM
Flamingo-9B[32] Chinchilla-7B - 51.8 44.7 30.1 - 28.8 - 61.5 - - - -
Flamingo-80B[32] Chinchilla-70B - 56.3 50.6 31.8 - 31.6 - 67.2 - - - -
BLIP-2[81] Vicuna-7B - - - 40.1 53.8 - 107.5 74.9 - - - -
BLIP-2[81] Vicuna-13B 41.0 41.0 - 42.5 61 19.6 103.9 71.6 1293.8 - 22.4 -
InstructBLIP[103] Vicuna-7B 49.2 - - 50.1 60.5 34.5 123.1 82.4 - 36 26.2 -
InstructBLIP[103] Vicuna-13B 49.5 - - 50.7 63.1 34.3 121.9 82.8 1212.8 - 25.6 -
IDEFICS-9B[104] LLaMA-7B 38.4 50.9 38.4 25.9 - 35.5 - 27.3 - 48.2 - -
IDEFICS-80B[104] LLaMA-65B 45.2 60.0 45.2 30.9 - 36.0 - 53.7 - 54.5 - -
LLaMA-Ad.v2[105] LLaMA-7B 43.9 - 55.9 43.8 54.2 - 42.7 30.5 972.7 38.9 31.4 32.7
Qwen-VL[106] Qwen-7B 57.5 78.2 56.6 61.5 68.2 38.9 120.2 81.0 1487.5 60.6 - 58.2
LLaVA-v1.5[49] Vicuna-7B 62.0 78.5 - 58.2 66.8 50.0 - - 1510.7 64.3 30.5 58.6
MultimodalGeneralistLLM
ImageBind-LLM[107] LLaMA-7B 41.1 - - 24.0 51.4 - 29.6 23.5 775.7 - - -
ChatBridge-13B[108] Vicuna-13B 41.8 - 45.2 - - - 115.7 82.5 - - - -
AnyMAL-13B[109] LLaMA2-13B - 59.6 33.1 24.7 52.7 24.4 - - - - - -
AnyMAL-70B[109] LLaMA2-70B - 64.2 42.6 32.9 70.8 33.8 - - - - - -
OneLLM-7B [CVPR’24] LLaMA2-7B 59.5 71.6 58.9 34.0 63.4 45.9 115.9 78.6 1392.0 60.0 29.1 61.2
MiCo-Chat-7B Vicuna-7B 61.5 78.1 56.6 53.4 71.3 49.1 111.8 76.3 1485.7 65.2 31.4 67.7
Table 5: Zero-Shot Audio & Video generative benchmark with LLMs. We evaluate models by audio
captioningonClothoCaption[70],audioQAonClothoAQA[110]andVideoChatGPTscoringbenchmark[111]
usingthesameVicuna-7B.
ClothoCaption ClothoAQA Cor. Det. Con. Tem. Cons.
Method 0-shot CIDEr SPIDEr Acc. Method
WFe aa vtu cr ae pC su [8t 5[1 ]12] ✗ ✗ 4 43 8. .6 8 2 37 1. .9 0 - - VideoLLaMA[115] 1.96 2.18 2.16 1.82 1.79
MWAFM[113] ✗ - - 22.2 VideoChat[116] 2.23 2.50 2.53 1.94 2.24
Pengi[114] ✗ - 27.1 64.5 Video-ChatGPT[111] 2.40 2.52 2.62 1.98 2.37
ChatBridge-13B[108] ✓ 26.2 - - BT-Adapter[117] 2.68 2.69 3.27 2.34 2.46
OneLLM-7B ✓ 29.1 19.5 57.9 LLaMa-VID[118] 2.96 3.00 3.53 2.46 2.51
MiCo-Chat-7B[Ours] ✓ 33.3 21.9 63.9 MiCo-Chat-7B[Ours] 3.00 3.01 3.61 2.49 2.71
4.3 EvaluationonCross-ModalUnderstanding
Table3illustratesthepowerfulperformancesofMiCoon25cross-modalbenchmarks,achieving
morethan20newSOTAperformances. Fortext-to-imageretrieval,MiCoachievesoutstanding
resultswithR@1of68.1%onCOCO,and91.1%onFlickr,outperformingpreviousSOTAmethods.
ForVQA,MiCodemonstratesrobustperformancewithaccuracyscoresof78.9%onTGIF,60.4%on
MSVD,and80.5%onVQAv2,highlightingitsstrongvisualcomprehensionandreasoningabilities.
Intext-to-audioretrieval,MiCoachievesoutstandingperformancesof21.2%onClothoV1,23.3%
onClothoV2,and41.0%onAudioCaps,whileinaudiocaptioning,itachieves49.6%onClothoV1,
and50.8%onClothoV2,alloutperformingpreviousbestresults. Fortext-to-videoretrieval,MiCo
sets new SOTA performances with metrics of 64.3% R@1 on MSRVTT and 81.3% on VATEX,
andinvideo-audiocaption,itachievesimpressiveperformancesof79.3%onMSRVTT,197.8%on
YouCook2,and62.8%onVALOR-32K.Finally,invideo-audioQA,MiCoalsodeliverssuperior
performancesof50.4%onMSRVTT,79.9%onMUSIC,and51.0onANET.Theseresultscollectively
highlightMiCo’sexceptionalandversatilecapabilitiesincross-modalcomprehensionandreasoning
tasks,establishingitasapromisingdirectioninthisfield.
4.4 EvaluationonMultimodalUnderstandingwithLargeLanguageModels
MiCohighlightsitsOmni-modalZero-shotComprehensionandReasoningAbilities. Beyond
traditional caption, retrieval, and QA tasks, we also evaluate the abilities of MiCo aligned with
LLMsforzero-shotmultimodalQA.WeuseChatBridge[108]asourbaselineandVicuna-7Bas
thelargelanguagemodelforeachmodality. AsshowninTable4,5,and6,MiCo-Chat-7Bshows
outstanding performances across both Vision LLMs and Multimodal LLMs. It directly delivers
outstandingperformancesontheSQA(71.3%),MMB(65.2%),MMVet(31.4%),andSEED(67.7%)
benchmarks while another 4 competitive performances. Besides, MiCo-Chat-7B also delivers
significantlyimpressiveperformancesonbothzero-shotcaptionandQAtasksonaudioandvideo
modalities,whereMiCo-Chat-7Bachieves6newSOTAperformancesincludingClothoCaption,
8Table6:Zero-shotVideoQAwithLLMs.Incomparisonwithleadingmethods,wereportresultswith1token
foreachframe,whereRes.indicatesimageresolution.
MSVD-QA MSRVTT-QA ActivityNet-QA
Method LLM Res.
Acc Score Acc Score Acc Score
FrozenBiLM[119] DeBERTa-V2 224 32.2 – 16.8 – 24.7 –
VideoLLaMA[120] Vicuna-7B 224 51.6 2.5 29.6 1.8 12.4 1.1
LLaMA-Adapter[105] LLaMA-7B 224 54.9 3.1 43.8 2.7 34.2 2.7
VideoChat[116] Vicuna-7B 224 56.3 2.8 45.0 2.5 26.5 2.2
Video-ChatGPT[111] Vicuna-7B 224 64.9 3.3 49.3 2.8 35.2 2.7
LLaMA-VID[118] Vicuna-7B 224 69.7 3.7 57.7 3.2 47.4 3.3
VideoChat2[50][CVPR’24] Vicuna-7B 224 70.0 3.9 54.1 3.3 49.1 3.3
MiCo-Chat-7B Vicuna-7B 224 73.7 4.1 60.1 3.6 50.1 3.3
Table7:AblationStudyonpretrainingmodalities,datascale,pretrainingprocess,andparameters.Ourdefault
settingistopretrainabasemodelfor30kstepswith10Mdatausingallobjectivefunctionsandevaluateitonthe
MSRVTT,VATEX,DIDEMO,MSVD,AudioCaps,ClothoV2,COCO,andFlickerdatasetsforretrievaltasks.
Video Audio Image Average
Model Factors MSRVTT(VA) VATEX(VA) DIDEMO(VA) MSVD(V) AudioCaps(A) ClothoV2(A) COCO(I) Flickr(I)
PretrainingModalities
(a) I 39.7 57.3 38.4 39.7 10.2 4.4 50.2 75.7 39.4
(b) I+3D 42.0 58.5 38.1 40.1 10.8 4.2 51.2 76.9 40.2
(c) I+A 37.6 56.2 30.8 36.2 22.0 14.5 46.8 71.0 39.4
(d) I+V 41.7 60.9 39.2 42.6 12.2 5.1 51.3 77.0 41.3
(e) I+V+A 42.2 61.1 40.1 41.2 23.4 15.4 48.7 74.2 43.2
(f) I+V+A+3D 45.7↑6.0 64.0↑6.7 42.7↑4.3 42.8↑3.1 24.6↑14.4 15.9↑11.5 49.9 77.1↑1.4 45.3↑5.9
DataScale
(h) 1M 44.2 63.2 40.1 40.7 21.9 11.2 48.2 77.5 43.4
(i) 10M 45.7 64.0 42.7 42.8 24.6 15.9 49.9 77.1 45.3
(j) 110M 48.5 65.7 41.7 43.0 26.3 17.1 49.6 78.1 46.3
(k) 334M 49.1↑4.9 66.3↑3.1 43.2↑3.1 44.1↑3.4 27.0↑5.1 17.5↑6.3 51.5↑3.3 80.9↑3.4 47.5↑4.1
PretrainingProcess
(l) LCon 40.1 57.4 39.1 41.4 23.1 14.4 47.4 73.7 42.1
(m) LCon+LMatch 43.9 61.4 38.0 41.6 23.6 15.5 48.8 74.3 43.4
(n) LCon+LMatch+LGen 45.7↑5.6 64.0↑6.6 42.7↑3.6 42.8↑1.4 24.6↑1.5 15.9↑1.5 49.9↑2.5 77.1↑3.4 45.3↑3.2
ModelScale
(o) Base-86M 45.7 64.0 42.7 42.8 24.6 15.9 49.9 77.1 45.3
(p) Large-331M 58.2 72.0 57.2 52.8 31.6 18.7 60.8 87.5 54.9
(q) Giant-1.3B 62.5↑16.8 79.9↑15.9 61.1↑18.4 56.0↑13.2 37.4↑12.8 20.8↑4.9 67.1↑17.2 90.7↑13.6 59.4↑14.1
AQA,MSVD-QA,MSRVTT-QA,ActivityNet-QA.TheseresultsareimportantproofthattheMiCo
pretrainingparadigmshowsapromisingdirectionindevelopinglargeomni-modalmodels.
14 17.5
15.0 Image 1M ViT-L Contrastive
12.5 I Im ma ag ge e+ +V Vi id de eo o+Audio 12 1 10 0M 0M 15.0 ViT-g 3.0 C Co on n+ +M Ma at tc ch h+Gen
10.0 Image+Video+Audio+Depth 10 300M 12.5
2.5
7.5 8 10.0
5.0 7.5 2.0
6
2.5 5.0
4 1.5
0k 100k 200k 300k 0k 100k 200k 300k 0k 100k 200k 300k 0k 100k 200k 300k
Step Step Step Step
Figure6: MultimodalScalingLaw. TraininglosscurvesforMiCounderscalingfactors(modality,data,
parameters,process)settings.
4.5 AblationStudy: MultimodalScalingLaws
Scaling Modalities. From (a) to (f), we gradually scale up input modalities. In Figure 6, all
modalities(I+V+A+3D)achievesthehighestscores,highlightingtheimportanceandeffectivenessof
MiCofordiversemultimodalinputs.
ScalingMultimodalData. From(h)to(k)inTable7,weinvestigatetheimpactoftheomni-modal
datascalefrom1Mto334M.ItprovesthattheMiCohasgreatpotentialforfurtherscaling.
PretrainingObjectives. From(l)to(n),weanalyzetheimpactofeachpretrainingobjective. The
combinationofcontrastive,matching,andgenerativelosses(L +L +L )yieldsthebest
Con Match Gen
performance,demonstratingthevalueofmultiplecomplementaryobjectives.
ScalingParameters. From(o)to(q),weassesstheeffectofmodelsize. Largermodels,particularly
theGiant-1.3B,showsuperiorperformance,confirmingthatincreasingmodelparameterswithMiCo
enhanceslearningandgeneralizationabilitiesacrossdiversemodalities.
9
ssoL5 ConclusionandLimitation
Inthispaper,weproposeanovelframework,termedMiCo,totrainfoundationmodelswithenhanced
visualperceptionabilitiesandomni-modalcapacities.Withexperimentsonareasonablylargescaleof
bothmodelanddata,weconcludethatthekeytoomni-modallearningistosimulatethemultimedia
cognitionprocessofthehumanbrain. InMiCo,weuseimage,depth,andnormalmapstosimulate
thefundamentalvisualperceptionability,distancespatialawareness,andgeometryawarenessof
humanvisualcognition. Inaddition,captions,audio,andvideoprovidepriorknowledge,auditory
perception,andspatial-temporalawareness. Infuturework,weplantoenhanceourjointpretraining
byincorporatingadditionalmodalities,includingopticalflow,IMUdata,andeventfiles,etc. We
believeMiCoisanimportantattempttosimulatethemultimediacognitionofhumanbrains,andwe
expectitcouldinspirefutureworkstodevelopmorepowerfulomni-modalfoundationmodels.
References
[1] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal.
Languagemodelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019. 1
[2] OpenAI. Gpt-4technicalreport. ArXiv,abs/2303.08774,2023.
[3] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhari-
wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language
modelsarefew-shotlearners. Advancesinneuralinformationprocessingsystems,33:1877–
1901,2020.
[4] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InICML,pages8748–8763.PMLR,2021. 2,3,4,
6
[5] SébastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,Ece
Kamar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal. Sparksofartificialgeneral
intelligence: Earlyexperimentswithgpt-4. arXivpreprintarXiv:2303.12712,2023. 1
[6] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
High-resolutionimagesynthesiswithlatentdiffusionmodels.InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages10684–10695,2022. 2
[7] BenPoole,AjayJain,JonathanTBarron,andBenMildenhall. Dreamfusion: Text-to-3dusing
2ddiffusion. arXivpreprintarXiv:2209.14988,2022. 2
[8] AndreyGuzhov,FedericoRaue,JörnHees,andAndreasDengel. Audioclip: Extendingclipto
image,textandaudio. InICASSP2022-2022IEEEInternationalConferenceonAcoustics,
SpeechandSignalProcessing(ICASSP),pages976–980.IEEE,2022. 2,3
[9] LeXue,MingfeiGao,ChenXing,RobertoMartín-Martín,JiajunWu,CaimingXiong,Ran
Xu, Juan Carlos Niebles, and Silvio Savarese. Ulip: Learning a unified representation of
language,images,andpointcloudsfor3dunderstanding. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages1179–1189,2023. 2,3
[10] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning.
Advancesinneuralinformationprocessingsystems,36,2024. 2
[11] Kunchang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang,
Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint
arXiv:2305.06355,2023. 2
[12] JiamingHan,KaixiongGong,YiyuanZhang,JiaqiWang,KaipengZhang,DahuaLin,YuQiao,
PengGao,andXiangyuYue. Onellm: Oneframeworktoalignallmodalitieswithlanguage.
arXivpreprintarXiv:2312.03700,2023. 2,7
10[13] Xiaohan Ding, Yiyuan Zhang, Yixiao Ge, Sijie Zhao, Lin Song, Xiangyu Yue, and Ying
Shan. Unireplknet: Auniversalperceptionlarge-kernelconvnetforaudio,video,pointcloud,
time-seriesandimagerecognition. arXivpreprintarXiv:2311.15599,2023. 2,7,22
[14] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala,
Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
15180–15190,2023. 2,3,6,7
[15] RichardEMayer. Multimedialearning. InPsychologyoflearningandmotivation,volume41,
pages85–139.Elsevier,2002. 2
[16] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick. Masked
autoencodersarescalablevisionlearners. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages16000–16009,2022. 3
[17] Po-YaoHuang,HuXu,JunchengLi,AlexeiBaevski,MichaelAuli,WojciechGaluba,Florian
Metze,andChristophFeichtenhofer. Maskedautoencodersthatlisten. AdvancesinNeural
InformationProcessingSystems,35:28708–28720,2022.
[18] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingof
deepbidirectionaltransformersforlanguageunderstanding. InNAACL-HLT,2019. 3,6
[19] KaimingHe,HaoqiFan,YuxinWu,SainingXie,andRossGirshick. Momentumcontrastfor
unsupervisedvisualrepresentationlearning. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pages9729–9738,2020. 3
[20] TingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton. Asimpleframework
forcontrastivelearningofvisualrepresentations. arXivpreprintarXiv:2002.05709,2020. 3
[21] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. Advancesinneuralinformation
processingsystems,30,2017. 2,4,5
[22] ZhouYu,JunYu,YuhaoCui,DachengTao,andQiTian. Deepmodularco-attentionnetworks
forvisualquestionanswering. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages6281–6290,2019. 3
[23] WenhuiWang, HangboBao, LiDong, andFuruWei. Vlmo: Unifiedvision-languagepre-
trainingwithmixture-of-modality-experts. arXivpreprintarXiv:2111.02358,2021. 3
[24] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao.
Simvlm: Simplevisuallanguagemodelpretrainingwithweaksupervision. arXivpreprint
arXiv:2108.10904,2021. 3
[25] PengWang,AnYang,RuiMen,JunyangLin,ShuaiBai,ZhikangLi,JianxinMa,ChangZhou,
JingrenZhou, andHongxiaYang. Unifyingarchitectures, tasks, andmodalitiesthrougha
simplesequence-to-sequencelearningframework. arXivpreprintarXiv:2202.03052,2022. 3
[26] WenhuiWang,HangboBao,LiDong,JohanBjorck,ZhiliangPeng,QiangLiu,KritiAggarwal,
OwaisKhanMohammed,SakshamSinghal,SubhojitSom,etal. Imageasaforeignlanguage:
Beitpretrainingforallvisionandvision-languagetasks. arXivpreprintarXiv:2208.10442,
2022. 3,4,6,7
[27] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert:
Pre-trainingofgenericvisual-linguisticrepresentations. arXivpreprintarXiv:1908.08530,
2019. 3
[28] XiujunLi,XiYin,ChunyuanLi,PengchuanZhang,XiaoweiHu,LeiZhang,LijuanWang,
Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for
vision-languagetasks. InEuropeanConferenceonComputerVision,pages121–137.Springer,
2020. 3
11[29] PengchuanZhang, XiujunLi, XiaoweiHu, JianweiYang, LeiZhang, LijuanWang, Yejin
Choi,andJianfengGao. Vinvl: Revisitingvisualrepresentationsinvision-languagemodels.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages5579–5588,2021. 3
[30] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and
StevenChuHongHoi. Alignbeforefuse: Visionandlanguagerepresentationlearningwith
momentumdistillation. Advancesinneuralinformationprocessingsystems,34:9694–9705,
2021. 3,6
[31] LuYuan,DongdongChen,Yi-LingChen,NoelCodella,XiyangDai,JianfengGao,Houdong
Hu,XuedongHuang,BoxinLi,ChunyuanLi,etal. Florence: Anewfoundationmodelfor
computervision. arXivpreprintarXiv:2111.11432,2021. 3
[32] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,
KarelLenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisual
languagemodelforfew-shotlearning. NeurIPS,35:23716–23736,2022. 3,8
[33] JiahuiYu,ZiruiWang,VijayVasudevan,LeggYeung,MojtabaSeyedhosseini,andYonghui
Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint
arXiv:2205.01917,2022. 3,7
[34] Jean-BaptisteAlayrac,AdriaRecasens,RosaliaSchneider,ReljaArandjelovic´,JasonRama-
puram, Jeffrey De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. Self-
supervisedmultimodalversatilenetworks. AdvancesinNeuralInformationProcessingSys-
tems,33:25–37,2020. 3
[35] HassanAkbari,LiangzheYuan,RuiQian,Wei-HongChuang,Shih-FuChang,YinCui,and
BoqingGong. Vatt: Transformersformultimodalself-supervisedlearningfromrawvideo,
audioandtext. AdvancesinNeuralInformationProcessingSystems,34:24206–24221,2021.
3
[36] YiyuanZhang,KaixiongGong,KaipengZhang,HongshengLi,YuQiao,WanliOuyang,and
XiangyuYue. Meta-transformer: Aunifiedframeworkformultimodallearning. arXivpreprint
arXiv:2307.10802,2023. 3,6,7
[37] HuXu,GargiGhosh,Po-YaoHuang,DmytroOkhonko,ArmenAghajanyan,FlorianMetze,
Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for
zero-shotvideo-textunderstanding. arXivpreprintarXiv:2109.14084,2021. 3
[38] YiWang,KunchangLi,XinhaoLi,JiashuoYu,YinanHe,GuoChen,BaoqiPei,Rongkun
Zheng, Jilan Xu, Zun Wang, et al. Internvideo2: Scaling video foundation models for
multimodalvideounderstanding. arXivpreprintarXiv:2403.15377,2024. 3,7
[39] SihanChen,HandongLi,QunboWang,ZijiaZhao,MingzhenSun,XinxinZhu,andJingLiu.
Vast: Avision-audio-subtitle-textomni-modalityfoundationmodelanddataset. arXivpreprint
arXiv:2305.18500,2023. 3,4,6
[40] SihanChen,XingjianHe,LongtengGuo,XinxinZhu,WeiningWang,JinhuiTang,andJing
Liu. Valor: Vision-audio-language omni-perception pretraining model and dataset. arXiv
preprintarXiv:2304.08345,2023. 3,7,24
[41] NanyiFei,ZhiwuLu,YizhaoGao,GuoxingYang,YuqiHuo,JingyuanWen,HaoyuLu,Ruihua
Song, Xin Gao, Tao Xiang, et al. Towards artificial general intelligence via a multimodal
foundationmodel. NatureCommunications,13(1):3094,2022. 4
[42] HongweiXue,TiankaiHang,YanhongZeng,YuchongSun,BeiLiu,HuanYang,JianlongFu,
andBainingGuo. Advancinghigh-resolutionvideo-languagerepresentationwithlarge-scale
videotranscriptions. InInternationalConferenceonComputerVisionandPatternRecognition
(CVPR),2022. 4
[43] XiaoFu,WeiYin,MuHu,KaixuanWang,YuexinMa,PingTan,ShaojieShen,DahuaLin,
andXiaoxiaoLong. Geowizard: Unleashingthediffusionpriorsfor3dgeometryestimation
fromasingleimage. arXivpreprintarXiv:2403.12013,2024. 4
12[44] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: A scalable
pipeline for making multi-task mid-level vision datasets from 3d scans. In ICCV, pages
10786–10796,2021. 4
[45] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,
ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,
JakobUszkoreit,andNeilHoulsby. Animageisworth16x16words: Transformersforimage
recognitionatscale. ICLR,2021. 4
[46] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023. 5,8
[47] JianlinSu,MurtadhaAhmed,YuLu,ShengfengPan,WenBo,andYunfengLiu. Roformer:
Enhancedtransformerwithrotarypositionembedding. Neurocomputing,568:127063,2024. 5
[48] YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu,XinggangWang,TiejunHuang,
Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation
learningatscale.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages19358–19369,2023. 5
[49] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisual
instructiontuning,2023. 7,8
[50] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu,
GuoChen,PingLuo,etal. Mvbench: Acomprehensivemulti-modalvideounderstanding
benchmark. arXivpreprintarXiv:2311.17005,2023. 7,9
[51] DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,and
Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint
arXiv:2009.03300,2020. 7
[52] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scale
hierarchicalimagedatabase. InCVPR,pages248–255.Ieee,2009. 7,22
[53] WillKay,JoaoCarreira,KarenSimonyan,BrianZhang,ChloeHillier,SudheendraVijaya-
narasimhan,FabioViola,TimGreen,TrevorBack,PaulNatsev,etal. Thekineticshuman
actionvideodataset. arXivpreprintarXiv:1705.06950,2017. 7,24
[54] PushmeetKohliNathanSilberman,DerekHoiemandRobFergus. Indoorsegmentationand
supportinferencefromrgbdimages. InECCV,2012. 7
[55] KristenGrauman,AndrewWestbury,EugeneByrne,ZacharyChavis,AntoninoFurnari,Rohit
Girdhar,JacksonHamburger,HaoJiang,MiaoLiu,XingyuLiu,etal. Ego4d: Aroundthe
worldin3,000hoursofegocentricvideo. InCVPR,pages18995–19012,2022. 7
[56] JunXu, TaoMei, TingYao, andYongRui. Msr-vtt: Alargevideodescriptiondatasetfor
bridgingvideoandlanguage. InCVPR,pages5288–5296,2016. 7
[57] AncongWu,Wei-ShiZheng,Hong-XingYu,ShaogangGong,andJianhuangLai.Rgb-infrared
cross-modalitypersonre-identification. InICCV,pages5380–5389,2017. 7
[58] JortFGemmeke,DanielPWEllis,DylanFreedman,ArenJansen,WadeLawrence,RChanning
Moore,ManojPlakal,andMarvinRitter. Audioset: Anontologyandhuman-labeleddataset
for audio events. In 2017 IEEE international conference on acoustics, speech and signal
processing(ICASSP),pages776–780.IEEE,2017. 7,24
[59] HaixuWu,HangZhou,MingshengLong,andJianminWang.Interpretableweatherforecasting
forworldwidestationswithaunifieddeepmodel. NatureMachineIntelligence,5(6):602–611,
2023. 7,23
[60] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,Jiahui
Yu,RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyof
highlycapablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023. 7
13[61] RohitGirdhar,MannatSingh,NikhilaRavi,LaurensvanderMaaten,ArmandJoulin,andIshan
Misra. Omnivore: Asinglemodelformanyvisualmodalities. InCVPR,pages16102–16112,
2022. 7
[62] SanyuanChen,YuWu,ChengyiWang,ShujieLiu,DanielTompkins,ZhuoChen,andFuru
Wei. Beats: Audiopre-trainingwithacoustictokenizers. arXivpreprintarXiv:2212.09058,
2022. 7
[63] Yiyuan Zhang, Sanyuan Zhao, Yuhao Kang, and Jianbing Shen. Modality synergy com-
plementlearningwithcascadedaggregationforvisible-infraredpersonre-identification. In
ComputerVision–ECCV2022: 17thEuropeanConference,TelAviv,Israel,October23–27,
2022,Proceedings,PartXIV,pages462–479.Springer,2022. 7
[64] EvangelosKazakos,ArshaNagrani,AndrewZisserman,andDimaDamen. Slow-fastauditory
streams for audio recognition. In ICASSP 2021-2021 IEEE International Conference on
Acoustics,SpeechandSignalProcessing(ICASSP),pages855–859.IEEE,2021. 7
[65] ChengxuanYing,TianleCai,ShengjieLuo,ShuxinZheng,GuolinKe,DiHe,YanmingShen,
andTie-YanLiu. Dotransformersreallyperformbadlyforgraphrepresentation? Advancesin
neuralinformationprocessingsystems,34:28877–28888,2021. 7
[66] NeetuSigger,TuanThanhNguyen,GianlucaTozzi,Quoc-TuanVien,andSinhVanNguyen.
Diffspectralnet: Unveilingthepotentialofdiffusionmodelsforhyperspectralimageclassifica-
tion. arXivpreprintarXiv:2312.12441,2023. 7
[67] InkitPadhi,YairSchiff,IgorMelnyk,MattiaRigotti,YoussefMroueh,PierreDognin,Jerret
Ross, Ravi Nair, and Erik Altman. Tabular transformers for modeling multivariate time
series. InICASSP2021-2021IEEEInternationalConferenceonAcoustics,SpeechandSignal
Processing(ICASSP),pages3565–3569.IEEE,2021. 7
[68] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,
Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In
ComputerVision–ECCV2014: 13thEuropeanConference,Zurich,Switzerland,September
6-12,2014,Proceedings,PartV13,pages740–755.Springer,2014. 7,25
[69] BryanAPlummer,LiweiWang,ChrisMCervantes,JuanCCaicedo,JuliaHockenmaier,and
SvetlanaLazebnik. Flickr30kentities: Collectingregion-to-phrasecorrespondencesforricher
image-to-sentencemodels. InICCV,pages2641–2649,2015. 7,8,25
[70] KonstantinosDrossos,SamuelLipping,andTuomasVirtanen. Clotho: Anaudiocaptioning
dataset. InICASSP,pages736–740.IEEE,2020. 7,8,23,24
[71] ChrisDongjooKim,ByeongchangKim,HyunminLee,andGunheeKim. Audiocaps: Gen-
eratingcaptionsforaudiosinthewild. InProceedingsofthe2019ConferenceoftheNorth
AmericanChapteroftheAssociationforComputationalLinguistics: HumanLanguageTech-
nologies,Volume1(LongandShortPapers),pages119–132,2019. 7,23,24
[72] JunXu, TaoMei, TingYao, andYongRui. Msr-vtt: Alargevideodescriptiondatasetfor
bridgingvideoandlanguage. InCVPR,pages5288–5296,2016. 7,23,24
[73] LuoweiZhou,ChenliangXu,andJasonCorso.Towardsautomaticlearningofproceduresfrom
webinstructionalvideos. InProceedingsoftheAAAIConferenceonArtificialIntelligence,
volume32,2018. 7,24
[74] XinWang,JiaweiWu,JunkunChen,LeiLi,Yuan-FangWang,andWilliamYangWang.Vatex:
Alarge-scale,high-qualitymultilingualdatasetforvideo-and-languageresearch. InICCV,
pages4581–4591,2019. 7,24
[75] LisaAnneHendricks,OliverWang,EliShechtman,JosefSivic,TrevorDarrell,andBryan
Russell. Localizingmomentsinvideowithnaturallanguage. InProceedingsoftheIEEE
internationalconferenceoncomputervision,pages5803–5812,2017. 7,24
14[76] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao.
Activitynet-qa: Adatasetforunderstandingcomplexwebvideosviaquestionanswering. In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume33,pages9127–9134,
2019. 7,25
[77] YunchengLi,YaleSong,LiangliangCao,JoelTetreault,LarryGoldberg,AlejandroJaimes,
andJieboLuo.Tgif:Anewdatasetandbenchmarkonanimatedgifdescription.InProceedings
oftheIEEEConferenceonComputerVisionandPatternRecognition,pages4641–4650,2016.
7
[78] DejingXu,ZhouZhao,JunXiao,FeiWu,HanwangZhang,XiangnanHe,andYuetingZhuang.
Video question answering via gradually refined attention over appearance and motion. In
Proceedingsofthe25thACMinternationalconferenceonMultimedia,pages1645–1653,2017.
7,24
[79] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making
thevinvqamatter: Elevatingtheroleofimageunderstandinginvisualquestionanswering.
InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages
6904–6913,2017. 7,25
[80] GuangyaoLi,YakeWei,YapengTian,ChenliangXu,Ji-RongWen,andDiHu. Learningto
answerquestionsindynamicaudio-visualscenarios. InCVPR,pages19108–19118,2022. 7,
24
[81] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-
imagepre-trainingwithfrozenimageencodersandlargelanguagemodels. arXivpreprint
arXiv:2301.12597,2023. 7,8
[82] PengWang,AnYang,RuiMen,JunyangLin,ShuaiBai,ZhikangLi,JianxinMa,ChangZhou,
JingrenZhou,andHongxiaYang. Ofa: Unifyingarchitectures,tasks,andmodalitiesthrougha
simplesequence-to-sequencelearningframework. InInternationalConferenceonMachine
Learning,pages23318–23340.PMLR,2022. 7
[83] WeichengKuo,AJPiergiovanni,DahunKim,XiyangLuo,BenCaine,WeiLi,AbhijitOgale,
LuoweiZhou,AndrewDai,ZhifengChen,etal. Mammut: Asimplearchitectureforjoint
learningformultimodaltasks. arXivpreprintarXiv:2303.16839,2023. 7
[84] XiChen, XiaoWang, SoravitChangpinyo, AJPiergiovanni, PiotrPadlewski, DanielSalz,
SebastianGoodman,AdamGrycner,BasilMustafa,LucasBeyer,etal. Pali: Ajointly-scaled
multilinguallanguage-imagemodel. arXivpreprintarXiv:2209.06794,2022. 7
[85] XinhaoMei,ChutongMeng,HaoheLiu,QiuqiangKong,TomKo,ChengqiZhao,MarkD
Plumbley,YuexianZou,andWenwuWang.Wavcaps:Achatgpt-assistedweakly-labelledaudio
captioningdatasetforaudio-languagemultimodalresearch. arXivpreprintarXiv:2303.17395,
2023. 7,8
[86] LinjieLi,JieLei,ZheGan,LichengYu,Yen-ChunChen,RohitPillai,YuCheng,Luowei
Zhou,XinEricWang,WilliamYangWang,etal. Value: Amulti-taskbenchmarkforvideo-
and-languageunderstandingevaluation. arXivpreprintarXiv:2106.04632,2021. 7
[87] DohwanKo,JoonmyungChoi,HyeongKyuChoi,Kyoung-WoonOn,ByungseokRoh,and
Hyunwoo J Kim. Meltr: Meta loss transformer for learning to fine-tune video foundation
models. arXivpreprintarXiv:2303.13009,2023. 7
[88] Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual
reasoningandcompositionalquestionanswering. InCVPR,2019. 8
[89] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh. Makingthe
vinvqamatter: Elevatingtheroleofimageunderstandinginvisualquestionanswering. In
CVPR,pages6904–6913,2017. 8
[90] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A
visualquestionansweringbenchmarkrequiringexternalknowledge. InCVPR,2019. 8
15[91] AmanpreetSingh,VivekNatarajan,MeetShah,YuJiang,XinleiChen,DhruvBatra,Devi
Parikh,andMarcusRohrbach. Towardsvqamodelsthatcanread. InCVPR,pages8317–8326,
2019. 8
[92] PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,Oyvind
Tafjord,PeterClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathought
chainsforsciencequestionanswering. NeurIPS,2022. 8
[93] DannaGurari,QingLi,AbigaleJStangl,AnhongGuo,ChiLin,KristenGrauman,JieboLuo,
andJeffreyPBigham. Vizwizgrandchallenge: Answeringvisualquestionsfromblindpeople.
InCVPR,pages3608–3617,2018. 8
[94] HarshAgrawal,KaranDesai,YufeiWang,XinleiChen,RishabhJain,MarkJohnson,Dhruv
Batra,DeviParikh,StefanLee,andPeterAnderson. nocaps: novelobjectcaptioningatscale.
InICCV,2019. 8
[95] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,ZhenyuQiu,
WeiLin,JinruiYang,XiawuZheng,etal. Mme: Acomprehensiveevaluationbenchmarkfor
multimodallargelanguagemodels. arXivpreprintarXiv:2306.13394,2023. 8
[96] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike
Yuan,JiaqiWang,ConghuiHe,ZiweiLiu,etal. Mmbench: Isyourmulti-modalmodelan
all-aroundplayer? arXivpreprintarXiv:2307.06281,2023. 8
[97] WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,ZichengLiu,Xinchao
Wang,andLijuanWang. Mm-vet: Evaluatinglargemultimodalmodelsforintegratedcapabili-
ties. arXivpreprintarXiv:2308.02490,2023. 8
[98] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-
bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint
arXiv:2307.16125,2023. 8
[99] JordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,Eliza
Rutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.
Trainingcompute-optimallargelanguagemodels. arXivpreprintarXiv:2203.15556,2022. 8
[100] Vicuna. Vicuna: Anopen-sourcechatbotimpressinggpt-4with90%*chatgptquality. https:
//vicuna.lmsys.org/,2023. 8
[101] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,Wenbin
Ge,YuHan,FeiHuang,etal. Qwentechnicalreport. arXivpreprintarXiv:2309.16609,2023.
8
[102] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023. 8
[103] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng
Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose
vision-languagemodelswithinstructiontuning,2023. 8
[104] Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton
Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander M Rush, Douwe Kiela, et al.
Obelisc: An open web-scale filtered dataset of interleaved image-text documents. arXiv
preprintarXiv:2306.16527,2023. 8
[105] PengGao,JiamingHan,RenruiZhang,ZiyiLin,ShijieGeng,AojunZhou,WeiZhang,Pan
Lu,ConghuiHe,XiangyuYue,etal. Llama-adapterv2: Parameter-efficientvisualinstruction
model. arXivpreprintarXiv:2304.15010,2023. 8,9
[106] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,Chang
Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile
abilities. arXivpreprintarXiv:2308.12966,2023. 8
16[107] JiamingHan,RenruiZhang,WenqiShao,PengGao,PengXu,HanXiao,KaipengZhang,
Chris Liu, Song Wen, Ziyu Guo, et al. Imagebind-llm: Multi-modality instruction tuning.
arXivpreprintarXiv:2309.03905,2023. 8
[108] ZijiaZhao,LongtengGuo,TongtianYue,SihanChen,ShuaiShao,XinxinZhu,ZehuanYuan,
and Jing Liu. Chatbridge: Bridging modalities with large language model as a language
catalyst. arXivpreprintarXiv:2305.16103,2023. 8
[109] SeungwhanMoon,AndreaMadotto,ZhaojiangLin,TusharNagarajan,MattSmith,Shashank
Jain,Chun-FuYeh,PrakashMurugesan,PeymanHeidari,YueLiu,etal. Anymal: Anefficient
and scalable any-modality augmented language model. arXiv preprint arXiv:2309.16058,
2023. 8
[110] SamuelLipping,ParthasaarathySudarsanam,KonstantinosDrossos,andTuomasVirtanen.
Clotho-aqa: Acrowdsourceddatasetforaudioquestionanswering. In202230thEuropean
SignalProcessingConference(EUSIPCO),pages1140–1144.IEEE,2022. 8
[111] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFahadShahbazKhan. Video-chatgpt:
Towardsdetailedvideounderstandingvialargevisionandlanguagemodels. arXivpreprint
arXiv:2306.05424,2023. 8,9
[112] ZhongjieYe,YuqingWang,HelinWang,DongchaoYang,andYuexianZou. Featurecut: An
adaptivedataaugmentationforautomatedaudiocaptioning. In2022Asia-PacificSignaland
InformationProcessingAssociationAnnualSummitandConference(APSIPAASC),pages
313–318.IEEE,2022. 8
[113] GuangyaoLi,YixinXu,andDiHu. Multi-scaleattentionforaudioquestionanswering. arXiv
preprintarXiv:2305.17993,2023. 8
[114] SohamDeshmukh, BenjaminElizalde, RitaSingh, andHuamingWang. Pengi: Anaudio
languagemodelforaudiotasks. arXivpreprintarXiv:2305.11834,2023. 8
[115] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual
languagemodelforvideounderstanding. arXivpreprintarXiv:2306.02858,2023. 8
[116] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang,
Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding. arXiv preprint
arXiv:2305.06355,2023. 8,9
[117] RuyangLiu,ChenLi,YixiaoGe,YingShan,ThomasHLi,andGeLi. Oneforall: Video
conversationisfeasiblewithoutvideoinstructiontuning. arXivpreprintarXiv:2309.15785,
2023. 8
[118] YanweiLi,ChengyaoWang,andJiayaJia. Llama-vid: Animageisworth2tokensinlarge
languagemodels. arXivpreprintarXiv:2311.17043,2023. 8,9
[119] AntoineYang,AntoineMiech,JosefSivic,IvanLaptev,andCordeliaSchmid. Zero-shotvideo
questionansweringviafrozenbidirectionallanguagemodels. NeurIPS,35:124–141,2022. 9
[120] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned audio-visual
languagemodelforvideounderstanding. arXivpreprintarXiv:2306.02858,2023. 9
[121] WenhaiWang,EnzeXie,XiangLi,Deng-PingFan,KaitaoSong,DingLiang,TongLu,Ping
Luo,andLingShao. Pyramidvisiontransformer: Aversatilebackbonefordenseprediction
withoutconvolutions. InProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
vision,pages568–578,2021. 22
[122] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBaining
Guo.Swintransformer:Hierarchicalvisiontransformerusingshiftedwindows.InProceedings
oftheIEEE/CVFinternationalconferenceoncomputervision,pages10012–10022,2021.
[123] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSaining
Xie. Aconvnetforthe2020s. arXivpreprintarXiv:2201.03545,2022. 22
17[124] Tawsifur Rahman, Amith Khandakar, Muhammad Abdul Kadir, Khandaker Rejaul Islam,
KhandakarFIslam,RashidMazhar,TahirHamid,MohammadTariqulIslam,SaadKashem,
ZaidBinMahbub,etal. Reliabletuberculosisdetectionusingchestx-raywithdeeplearning,
segmentationandvisualization. IeeeAccess,8:191586–191601,2020. 23
[125] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-
captioningeventsinvideos. InProceedingsoftheIEEEinternationalconferenceoncomputer
vision,pages706–715,2017. 24
[126] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon, Christopher Pal, Hugo
Larochelle,AaronCourville,andBerntSchiele. Moviedescription. InternationalJournalof
ComputerVision,123:94–120,2017. 24
18Appendix
A Summary
Theappendixisorganizedasfollows:
• § B detailed training and evaluation settings of our models including hyper-parameters
regardingmodelsandoptimizers.
• §Cpresentsacomprehensiveintroductiononthedatasetsweuseforevaluationandtheir
correspondingmetrics.
B TrainingConfiguration
B.1 PretrainingSettings
WedetailthespecificpretrainingconfigurationsofMiCo,focusingonthemulti-datasetjointtraining
corpora, the dataset mix ratios for each corpus, and the learning objectives for each corpus. To
improvedataquality,weemployedatrainedvisioncaptionertogeneratenewcaptionsfortheCC4M
datasets,replacingtheoriginalcaptions. AlthoughMiCohasonlybeentrainedfor300,000steps,it
hasalreadydemonstratedoutstandingperformanceonvariousdownstreamtasks. Weanticipatethat
furtherincreasingthenumberoftrainingstepswillsignificantlyenhancethemodel’scapabilities.
ThepretrainingofMiCoinvolvesacombinationofdifferentdatasets,eachcontributinguniquelyto
themodel’slearningprocess. Themodel,withaparametersizeof1.0billionandasamplesizeof
334million,utilizesadiversetrainingcorpustoachieveitsresults.
1. VAST-27M:Thisdatasetcontributes324millionsamplestothetrainingprocess. Withabatchsize
of2048,themodelundergoes160,000steps,completingoneepoch.
2. VALOR-1M:Inthisdataset,1millionsamplesareusedwithabatchsizeof1024. Thetraining
spans70,000steps,whichequatestoapproximately71.7epochs.
3. WavCaps,CC4M,andWebVid-2.5M:Thesedatasetsarecombined,contributing9millionsamples
intotal. Thebatchsizeforthiscombineddatasetis1024,andthemodelistrainedover70,000steps,
resultingin8.0epochs.
Thecarefulselectionandcombinationofthesedatasets, alongwiththeapplicationofnew, high-
qualitycaptionsfortheCC4Mdatasets,enhancethetrainingefficiencyandthequalityofthelearned
representations.
B.2 Fine-tuningSettings
Wedetailthedownstreamtaskfinetuningsettings,specifyingthelearningrate,batchsize,epoch,
trainingobjectives,andresolution. Theconfigurationsalsoincludethenumberofsampledvideo
framesoraudioclipsusedintrainingandtestingphases. Herearethecomprehensivesettings:
RetrievalTasks(RET)
• Image-TextModality
– MSCOCO:Learningrateof1e-5,batchsizeof256,5epochs,withtheobjectivefor
retrieval,andaresolutionof384.
– Flickr: Learning rate of 1e-5, batch size of 256, 5 epochs, with the objective for
retrieval,andaresolutionof384.
• Audio-TextModality(A-T)
– ClothoV1/V2: Learningrateof2e-5,batchsizeof64,10epochs,withtheobjective
forretrieval,using3audioclipsduringbothtrainingandtesting.
– AudioCaps: Learningrateof2e-5,batchsizeof64,10epochs,withtheobjectivefor
retrieval,using1audioclipduringbothtrainingandtesting.
19• Multi-modal(MM)
– MSRVTT:Learningrateof2e-5,batchsizeof64,3.6epochs,withtheobjectivefor
retrieval,using8videoframesduringtrainingand16duringtesting,witharesolution
of224.
– YouCook2: Learningrateof3e-5,batchsizeof64,30epochs,withtheobjectivefor
retrieval,using8videoframesduringtrainingand16duringtesting,witharesolution
of224.
– VALOR-32K:Learningrateof2e-5,batchsizeof64,10epochs,withtheobjectivefor
retrieval,using8videoframesduringbothtrainingandtesting,witharesolutionof
224.
– VATEX:Learningrateof2e-5, batchsizeof64, 2.5epochs, withtheobjectivefor
retrieval,using8videoframesduringtrainingand16duringtesting,witharesolution
of224.
– DiDeMo: Learningrateof2e-5,batchsizeof64,40epochs,withtheobjectivefor
retrieval,using8videoframesduringtrainingand32duringtesting,and2audioclips
duringbothtrainingandtesting,witharesolutionof224.
– ANET: Learning rate of 2e-5, batch size of 64, 20 epochs, with the objective for
retrieval,using8videoframesduringtrainingand32duringtesting,and2audioclips
duringbothtrainingandtesting,witharesolutionof224.
CaptioningTasks(CAP)
• Image-TextModality
– MSCOCO:Learningrateof1e-5,batchsizeof64,5epochs,withtheobjectivefor
caption,andaresolutionof480.
– MSCOCO(SCST): Learning rate of 2.5e-6, batch size of 64, 2.5 epochs, with the
objectiveforcaption,andaresolutionof480.
• Audio-TextModality(A-T)
– ClothoV1/V2: Learningrateof2e-5,batchsizeof64,10epochs,withtheobjective
forcaption,using3audioclipsduringbothtrainingandtesting.
– AudioCaps: Learningrateof2e-5,batchsizeof64,10epochs,withtheobjectivefor
caption,using1audioclipduringbothtrainingandtesting.
• Multi-modal(MM)
– MSRVTT:Learningrateof2e-5,batchsizeof128,10epochs,withtheobjectivefor
caption,using8videoframesduringbothtrainingandtesting,witharesolutionof224.
– YouCook2: Learningrateof3e-5,batchsizeof64,30epochs,withtheobjectivefor
caption,using8videoframesduringtrainingand16duringtesting,witharesolution
of224.
– VALOR-32K:Learningrateof1e-5,batchsizeof64,10epochs,withtheobjectivefor
caption,using8videoframesduringtrainingand12duringtesting,witharesolution
of224.
QuestionAnsweringTasks(QA)
• Visual-TextModality(Vis)
– MSVD-QA:Learningrateof1e-5,batchsizeof64,10epochs,withtheobjectivefor
QA,using8videoframesduringtrainingand14duringtesting,witharesolutionof
224.
– TGIF-FrameQA:Learningrateof2e-5,batchsizeof64,10epochs,withtheobjective
forQA,using4videoframesduringbothtrainingandtesting,witharesolutionof224.
– VQAv2: Learningrateof2e-5,batchsizeof128,20epochs,withtheobjectiveforQA,
andaresolutionof384.
• Multi-modal(MM)
– MSRVTT-QA:Learningrateof2e-5,batchsizeof64,4.5epochs,withtheobjective
forQA,using8videoframesand1audioclipduringbothtrainingandtesting,witha
resolutionof224.
20Table8:DetailedtrainingconfigurationsofMiCoformultimodallearning.Apartfromtheconfigurations
showninthetable,forimagetasks,weuserandomleft-rightflipping,randomresizedcrop,colorjitterof0.4,
Auto-augment,andnorepeatedaugmentationforeverymodel.
Image Audio Video Depth&NormalMap
settings
ViT-L ViT-g ViT-L ViT-g ViT-L ViT-g ViT-L ViT-g
InputShape 224 224 224 224 224 224 224 224
batchsize 4096 512 4096 512 4096 512 4096 512
optimizer AdamW AdamW AdamW AdamW AdamW AdamW AdamW AdamW
LR 4×10−3 5×10−5 4×10−3 5×10−5 4×10−3 5×10−5 4×10−3 5×10−5
LRschedule cosine cosine cosine cosine cosine cosine cosine cosine
weightdecay 0.05 1×10−8 0.05 1×10−8 0.05 1×10−8 0.05 1×10−8
warmupepochs 5 0 5 0 5 0 5 0
epochs 90 30 90 30 90 20 90 20
mixupalpha 0.8 0.0 0.8 0.0 0.8 0.0 0.8 0.0
cutmixalpha 1.0 0.0 1.0 0.0 1.0 0.0 1.0 0.0
erasingprob. 0.25 0.25 0.25 0.25 0.25 0.25 0.25 0.25
dropoutrate 0.1 0.2 0.1 0.2 0.1 0.3 0.2 0.3
Algorithm1MultimodalContextPretrainingAlgorithm,PyTorch-like
def train(video_pixels=None, image_pixels=None, depth_pixels=None, audio_spectrograms=
None):
# Get Mixed Data
modal_inputs = [video_pixels, image_pixels, depth_pixels, audio_spectrograms]
modal_captions = [video_captions, image_captions, depth_captions, audio_captions]
# Extract Features
modal_feats = [self.encoder(modal) for modal in modal_inputs if modal is not None]
multimodal_feats = torch.cat(modal_feats)
concatenated_captions = ’’.join(modal_captions)
text_feats = self.text_encoder(concatenated_captions)
# Losses
contra_loss = Contrasive_Loss(multimodal_feats, text_feats)
matching_loss = Matching_Loss(modal_captions, multimodal_feats)
gen_loss = Generation_Loss(modal_captions.mask(0.6), multimodal_feats)
# Total Loss
loss = contra_loss + matching_loss + gen_loss
return loss
– MUSIC-AVQA:Learningrateof2e-5,batchsizeof64,20epochs,withtheobjective
forQA,using8videoframesand2audioclipsduringbothtrainingandtesting,witha
resolutionof224.
– ANET-QA:Learningrateof2e-5,batchsizeof64,10epochs,withtheobjectivefor
QA, using 8 video frames during training and 16 during testing, and 2 audio clips
duringbothtrainingandtesting,witharesolutionof224.
Thesesettingshavebeenoptimizedtobalanceefficiencyandperformance,eventhoughmosthyper-
parametersarenotpreciselytuned.
Forevaluationpurposes,weemploydifferentstrategiestailoredtospecifictasks:
1. RetrievalTasks: AllcandidatesareinitiallyrankedusingOmni-modalContrastiveLoss. Following
this,theTop-50candidatesundergoarerankingprocessthroughtheOmni-modalMatchingProcess.
2. CaptioningTasks: Beamsearchwithabeamsizeof3isutilizedtogeneratecaptions,ensuringa
comprehensiveexplorationofpossibleoutputs.
3. QuestionAnswering(QA)Tasks: Thesearetreatedasopen-endedgenerativeproblems. Questions
areusedasprefixes,andanswersaregeneratedwithoutanyconstraints,allowingforflexibleand
contextuallyappropriateresponses.
21Algorithm2DatasetSplitAlgorithm
import pandas as pd
from sklearn.model_selection import train_test_split
# Assume ‘data‘ is a DataFrame containing the full dataset with columns [’category’, ’
vision_caption’, ’audio_caption’, ’depth’, ’normal’, ’subtitle’]
# Adding an ’index’ column to keep track of the original indices
data[’index’] = data.index
# Define the sizes of each subset
subset_sizes = [1e6, 1e7, 1.1e7, 3.34e7]
# Function to create stratified samples
def create_subset(data, size):
subset, _ = train_test_split(data, train_size=size, stratify=data[’category’],
random_state=42)
return subset
# Creating subsets
subset_1M = create_subset(data, 1e6)
subset_10M = create_subset(data, 1e7)
subset_110M = create_subset(data, 1.1e7)
subset_334M = create_subset(data, 3.34e7)
# Reset index for each subset
subset_1M.reset_index(drop=True, inplace=True)
subset_10M.reset_index(drop=True, inplace=True)
subset_110M.reset_index(drop=True, inplace=True)
subset_334M.reset_index(drop=True, inplace=True)
Forcomparisonswithstate-of-the-art(SOTA)modelsandablationstudies, weusethefollowing
evaluation metrics: 1) Retrieval Tasks: Recall@1. 2) Captioning Tasks: CIDEr. 3) QA Tasks:
Accuracy (Acc) These metrics provide a comprehensive assessment of the model’s performance
acrossdifferenttypesoftasks.
C DatasetsandMetrics
DatasetSplitTosplitthemixofdatasetsintosubsetsof1M,10M,110M,and334Mvideoclipswhile
preservingitsdiversityandquality,weemployedaproportionalstratifiedsamplingmethod. Initially,
thedataset,whichspansover15categories(includingmusic,gaming,education,entertainment,and
animals)andincludesvision,audio,depth,normalmaps,andtextmodalities,wasorganizedand
labeled. Stratifiedrandomsamplingwasthenusedtoensureeachsubsetaccuratelyreflectedthe
distributionofcategoriesandmodalitiespresentinthefulldataset. Thismethodinvolvedselecting
samplesproportionallyfromeachcategorytomaintainrepresentativedistributions. Thevisionand
audiocaptionswerealsokeptproportionalinlengthandquantity,ensuringthateachsubsetretained
thecomprehensivecharacteristicsoftheoriginaldataset.
C.1 Single-modalityEvaluationDetails
Text. TheMMLU(MassiveMultitaskLanguageUnderstanding)benchmarkisdesignedtoevaluate
themultitaskaccuracyoflanguagemodelsacross57diversetasks,includingsubjectslikemathemat-
ics,history,andbiology. Itassessesmodels’abilitiestogeneralizeandapplyknowledgeinvarious
domains,providingacomprehensivemeasureoftextunderstandingandreasoningskills.
Image. We conduct experiments on ImageNet-1K [52], a dataset comprising approximately 1.3
millionimagesacross1,000categories. Inlinewithcommonpractices[121–123,13],base-scale
models are trained for 300 epochs. Large-scale models undergo pre-training on ImageNet-22K,
whichincludes14.2millionimages,for90epochs,followedbyfine-tuningonImageNet-1Kforan
additional20epochs.
22Thermal and Hyperspectral data understanding. We conduct experiments on infrared image
recognitionusingtheRegDBdataset,X-rayscananalysiswiththeChestX-Raydataset[124],and
hyperspectraldatarecognitionusingtheIndianPinedataset4.
Depth. TheNYUDepthDataset(NYU-D)comprisesRGBanddepthimagepairscapturedfrom
indoorscenes.Itincludes1,449denselylabeledpairsfortrainingandtesting,alongwithover400,000
unlabeledframes.
Audio. Foraudiorecognition, Audioset-2Mdatasetcomprisesover2millionhuman-labeled10-
secondaudioclipsdrawnfromYouTubevideos. Itcoversawiderangeof527soundeventclasses,
providingacomprehensiveresourcefortrainingandevaluatingaudioeventdetectionandclassifica-
tionmodels.
Video. TheKinetics-700datasetcontains700,000videoclipscovering700humanactionclasses,
usedforactionrecognitiontasks. TheMSR-VTTdatasetincludes10,000videoclipspairedwith
multiple textual descriptions, supporting video captioning, retrieval, and content understanding
research.
Time-series. GlobalWeatherForecasting[59]includesglobal,regional,andOlympicsdatafrom
NCEIandCMA,comprisinghourlyweathermeasurementsfromthousandsofstations. Evaluation
involvedsplittingdataintotraining,validation,andtestsets(7:1:2)usingMSEandMAEmetrics.
Graph. PCQM4M-LSCdatasetisalarge-scalecollectionof4.4millionorganicmolecules,eachwith
upto23heavyatomsandassociatedquantum-mechanicalproperties. Aimedatpredictingmolecular
propertiesthroughmachinelearning,thisdatasetishighlyrelevantforapplicationsindrugdiscovery
andmaterialscience.
Tabular. Thefrauddatasetcomprisestransactionrecords,includingfeaturesliketransactionamount,
location,time,anduserinformation. Itisdesignedformachinelearningmodelstodetectfraudulent
activities. Thisdatasetiscrucialfordevelopingandtestingalgorithmstoenhancesecurityinfinancial
systemsandreduceeconomiclossesduetofraud.
IMU.TheEgo4Ddatasetincludesinertialmeasurementunit(IMU)datacapturedfromwearable
devices,providingdetailedmotionandorientationinformation. Thisdatasetsupportsresearchin
humanactivityrecognition,augmentedreality,androbotics,offeringcomprehensiveinsightsinto
humanmovementsandinteractionswiththeenvironment.
C.2 Cross-modalityEvaluationDetails
WeevaluatedMiCoacrossseveralwell-knowndownstreamdatasets,includingMSRVTT,VATEX,
YouCook2,VALOR-32K,MSVD,DiDeMo,ActivityNetCaption,TGIF,MUSIC-AVQA,Clotho,
AudioCaps,MSCOCO,Flickr30K,andVQAv2. Thespecifictrain/validation/testsplitsforthese
benchmarksaredetailedbelow:
RetrievalTasks
Audio-TextModality(A-T)
• ClothoV1[70]: Thisdatasetincludes2,893audioclipsfortrainingand1,045forvalidation.
Thecorrespondingcaptionsnumber14,465fortrainingand5,225forvalidation.
• ClothoV2[70]:Contains3,839audioclipsfortrainingand1,045forvalidation,with19,195
captionsfortrainingand5,225forvalidation.
• AudioCaps[71]: Comprises49,291audioclipsfortraining,428forvalidation,and816for
testing,alongwith49,291captionsfortraining,2,140forvalidation,and4,080fortesting.
Video-TextModality(V-T)
• MSRVTT[72]: Comprises10Kvideoclipsand200Kcaptions,spanningdiversetopics
suchashumanactivities,sports,andnaturallandscapes. Weevaluatetext-to-videoretrieval,
4https://github.com/danfenghong/IEEE_TGRS_SpectralFormer/blob/main/data/IndianPine.
mat
23videocaptioning,andvideoQAusingthisdataset. Contains9,000videosfortrainingand
1,000fortesting,with180,000captionsfortrainingand1,000fortesting.
• YouCook2 [73]: Comprises 14K video clips extracted from 2K instructional cooking
videosonYouTube. Eachvideofeaturesmultipleactionsperformedbychefs,alongwith
correspondingtextualdescriptionsandtemporalannotations. Includes10,337videosfor
trainingand3,492forvalidation,withmatchingcaptions.
• VALOR-32K[40]: Anaudiovisualvideo-languagebenchmarkcontaining32K10-second
videoclipssourcedfromAudioSet[58]. Eachclipincludesannotationswithcaptionsthat
describeboththevisualandaudiocontent. Consistsof25,000videosfortraining,3,500for
validation,and3,500fortesting,eachwithcorrespondingcaptions.
• DiDeMo [75]: Comprises 10K long-form videos sourced from Flickr, with each video
annotatedwithfourshortsentencesintemporalorder. Forthisbenchmark,weconcatenate
theseshortsentences andevaluate’paragraph-to-video’retrieval, usingtheofficialsplit.
Features8,394videosfortraining,1,065forvalidation,and1,003fortesting,alongwith
theircaptions.
• ActivityNet(ANET)[125]:Includes20Klong-formvideos(averagelengthof180seconds)
fromYouTube, accompaniedby100Kcaptions. Weevaluatetext-to-videoretrievaland
videoQAonthisdataset. Comprises10,009videosfortrainingand4,917fortesting,with
correspondingcaptions.
• LSMDC[126]: Contains101,046videosfortraining,7,408forvalidation,and1,000for
testing,withcorrespondingcaptions.
CaptioningTasks
Audio-TextModality(A-T)
• ClothoV1[70]: Thisdatasetincludes2,893audioclipsfortrainingand1,045forvalidation.
Thecorrespondingcaptionsnumber14,465fortrainingand5,225forvalidation.
• ClothoV2[70]:Contains3,839audioclipsfortrainingand1,045forvalidation,with19,195
captionsfortrainingand5,225forvalidation.
• AudioCaps[71]: Comprises49,838audioclipsfortraining,495forvalidation,and975for
testing,alongwith49,438captionsfortraining,2,475forvalidation,and4,875fortesting.
Video-TextModality(V-T)
• MSRVTT[72]: Contains6,513videosfortraining,497forvalidation,and2,990fortesting,
with130,260captionsfortraining,9,940forvalidation,and59,800fortesting.
• YouCook2[73]: Includes10,337videosfortrainingand3,492forvalidation,withmatching
captions.
• VALOR-32K[40]: Consistsof25,000videosfortraining,3,500forvalidation,and3,500
fortesting,eachwithcorrespondingcaptions.
• VATEX[74]: Consistsof41,250videoclipssourcedfromtheKinetics-600dataset[53],
accompaniedby825,000sentence-leveldescriptions. Contains25,991videosfortraining,
3,000forvalidation,and6,000fortesting,with259,910captionsfortraining,30,000for
validation,and60,000fortesting.
QuestionAnswering(QA)Tasks
Video-TextModality(V-T)
• MSRVTT-QA[78]: Contains6,513videosfortraining,497forvalidation,and2,990for
testing,with158,581QApairsfortraining,12,278forvalidation,and72,821fortesting.
• MUSIC-AVQA [80]: An audiovisual video QA benchmark containing over 45K Q-A
pairs,covering33differentquestiontemplatesacrossvariousmodalitiesandquestiontypes.
Includes9,277videosfortraining,3,815forvalidation,and6,399fortesting,with32,087
QApairsfortraining,4,595forvalidation,and9,185fortesting.
24• ANET-QA[76]: Comprises3,200videosfortraining, 1,800forvalidation, and800for
testing,with32,000QApairsfortraining,18,000forvalidation,and8,000fortesting.
Image-BasedTasks
• MSCOCO [68]: Comprises 123K images, each paired with 5 annotated captions. We
evaluatetext-to-imageretrievalandimagecaptioningonthisdataset.
• Flickr30K[69]: Contains31Kimages, eachpairedwithfivedescriptivecaptions. This
datasetiswidelyusedforevaluatingimagecaptioningandtext-to-imageretrievaltasks.
VisualQuestionAnswering
• VQAv2 [79]: A large-scale Visual Question Answering dataset comprising over 265K
imagesand1.1Mquestions,designedtoimprovethebalanceofanswertypesperquestion.
This dataset is used to evaluate models’ abilities to understand and reason about visual
contentbyprovidingaccurateanswerstoquestionsbasedontheimages.
25