Efficient Adaptation in Mixed-Motive Environments
via Hierarchical Opponent Modeling and Planning
YizheHuang12 AnjiLiu3 FanqiKong24 YaodongYang12 Song-ChunZhu12 XueFeng(cid:66)2
Abstract 1.Introduction
Constructingagentsbeingabletorapidlyadapttopreviously
unseenagentsisalongstandingchallengeforArtificialIntel-
Despite the recent successes of multi-agent re- ligence. Werefertothisabilityasfew-shotadaptation. Pre-
inforcement learning (MARL) algorithms, effi- viousworkhasproposedwell-performedMARLalgorithms
cientlyadaptingtoco-playersinmixed-motiveen- tostudyfew-shotadaptationinzero-sumgames(Vinyals
vironmentsremainsasignificantchallenge. One etal.,2019;Vezhnevetsetal.,2020)andcommon-interest
feasible approachis tohierarchically modelco- environments(Barrettetal.,2011;Huetal.,2020;Maha-
players’behaviorbasedoninferringtheircharac- jan et al., 2022; Mirsky et al., 2022; Bauer et al., 2023).
teristics. However,thesemethodsoftenencounter These environments involve a predefined competitive or
difficultiesinefficientreasoningandutilization cooperativerelationshipbetweenagents. However,thema-
ofinferredinformation. Toaddresstheseissues, jorityofrealisticmulti-agentdecision-makingscenariosare
weproposeHierarchicalOpponentmodelingand not confined to these situations and should be abstracted
Planning (HOP), a novel multi-agent decision- asmixed-motiveenvironments(Komorita&Parks,1995;
makingalgorithmthatenablesfew-shotadapta- Dafoeetal.,2020),wheretherelationshipsbetweenagents
tiontounseenpoliciesinmixed-motiveenviron- are non-deterministic, and the best responses of an agent
ments. HOPishierarchicallycomposedoftwo maychangewithothers’behavior. Apolicy,thatisunable
modules: anopponentmodelingmodulethatin- toquicklyadapttoco-players,mayharmnotonlythefocal
fersothers’goalsandlearnscorrespondinggoal- agent’sinterestbutalsotheentiregroup’sbenefit.Therefore,
conditionedpolicies,andaplanningmodulethat fastadaptingtonewco-playersinmixed-motiveenviron-
employs Monte Carlo Tree Search (MCTS) to mentswarrantssignificantattention,buttherehasbeenlittle
identifythebestresponse.Ourapproachimproves focusonthisaspect.
efficiencybyupdatingbeliefsaboutothers’goals
Inthispaper,wefocusonthefew-shotadaptationtounseen
bothacrossandwithinepisodesandbyusingin-
agents in mixed-motive environments. Many algorithms
formation from the opponent modeling module
struggle to perform well in mixed-motive environments
toguideplanning. Experimentalresultsdemon-
despitesuccessinzero-sumandpure-cooperativeenviron-
stratethatinmixed-motiveenvironments, HOP
ments,becausetheyuseefficienttechniquesspecifictore-
exhibitssuperiorfew-shotadaptationcapabilities
wardstructures,suchasminimax(Littman,1994;Lietal.,
wheninteractingwithvariousunseenagents,and
2019), Double Oracle (McMahan et al., 2003; Balduzzi
excels in self-play scenarios. Furthermore, the
etal.,2019)orIGMcondition(Sunehagetal.,2017;Son
emergenceofsocialintelligenceduringourexper-
et al., 2019; Rashid et al., 2020), which are not applica-
imentsunderscoresthepotentialofourapproach
bleinmixed-motiveenvironments. Thenon-deterministic
incomplexmulti-agentenvironments.
relationshipsbetweenagentsandthegeneral-sumreward
structure make decision-making and few-shot adaptation
morechallenginginmixed-motiveenvironmentscompared
withzero-sumandpure-cooperativeenvironments.
1InstituteforArtificialIntelligence,PekingUniversity2State
Accordingtocognitivepsychologyandrelateddisciplines,
Key Laboratory of General Artificial Intelligence, BIGAI
3UniversityofCalifornia,LosAngeles4TsinghuaUniversity.Cor- humans’ ability to rapidly solve previously unseen prob-
respondenceto:XueFeng<fengxue@bigai.ai>. lemsdependsonhierarchicalcognitivemechanisms(Butz
&Kutter,2016;Kleiman-Weineretal.,2016;Eppeetal.,
Proceedings of the 41st International Conference on Machine
2022). This hierarchical structure unifies high-level goal
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by
reasoningwithlow-levelactionplanning. Meanwhile,re-
theauthor(s).
1
4202
nuJ
21
]IA.sc[
1v20080.6042:viXraEfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
searchonmachinelearningalsoemphasizestheimportance theeffectofanagent’sactionsonitsco-players’behavior.
andeffectivenessofhierarchicalgoal-directedplanningfor
LOLA (Foerster et al., 2018) and its extension (such as
few-shotproblem-solving(Eppeetal.,2022). Inspiredby
POLA(Zhaoetal.,2022),M-FOS(Luetal.,2022))con-
thehierarchicalstructure,weproposeanalgorithm,named
sidertheimpactofoneagent’slearningprocess,ratherthan
HierarchicalOpponentmodelingandPlanning(HOP),for
treatingthemasastaticpartoftheenvironment. However,
tacklingfew-shotadaptationinmixed-motiveenvironments.
LOLArequiresknowledgeofco-players’networkparame-
HOPhierarchicallyconsistsoftwomodules: anopponent
ters,whichmaynotbefeasibleinmanyscenarios. LOLA
modeling module and a planning module. The opponent
withopponentmodelingrelaxesthisrequirement,butscal-
modelingmoduleinfersco-players’goalsandlearnstheir
ingproblemsmayariseincomplexsequentialenvironments
goal-conditionedpolicies,basedonTheoryofMind(ToM)
thatrequirelongactionsequencesforrewards.
-theabilitytounderstandothers’mentalstates(likegoals
andbeliefs)fromtheiractions(Bakeretal.,2017). More Our work relates to opponent modeling (see (Albrecht &
specifically,toimproveinferenceefficiency,beliefsabout Stone,2018)foracomprehensivereview).I-POMDP(Gmy-
others’goalsareupdatedbothbetweenandwithinepisodes. trasiewicz&Doshi,2005)isatypicalopponentmodeling
Then,theinformationfromtheopponentmodelingmodule andplanningframework,whichmaintainsdynamicbeliefs
is sent to the planning module, which is based on Monte overthephysicalenvironmentandbeliefsoverco-players’
CarloTreeSearch(MCTS),tocomputethenextaction. beliefs. Itmaximizesavaluefunctionofthebeliefstodeter-
minethenextaction. However,thenestedbeliefinference
Toassessthefew-shotadaptationabilityofHOP,wecon-
suffers from serious computational complexity problems,
ductexperimentsinMarkovStag-Hunt(MSH)andMarkov
whichmakesitimpracticalincomplexenvironments. Un-
Snowdrift Game (MSG), which spatially and temporally
like I-POMDP and its approximation methods (Doshi &
extendtwoclassicparadigmsingametheory:theStag-Hunt
Perez,2008;Doshi&Gmytrasiewicz,2009;Hoang&Low,
game(Rousseau,1999)andtheSnowdriftgame(alsoknown
2013;Han&Gmytrasiewicz,2018;2019;Zhang&Doshi,
asthegameofchickenorhawk-dovegame)(Rapoport&
2022),HOPexplicitlyusesbeliefsoverco-players’goals
Chammah,1966). Bothofthetwogamesillustratehowthe
andpoliciestolearnaneuralnetworkmodelofco-players,
bestresponseinamixed-motiveenvironmentisinfluenced
which guides an MCTS planner to compute next actions.
bythestrategyofco-players. Experimentalresultsillustrate
HOPavoidsnestedbeliefinferenceandperformssequential
thatintheseenvironments,HOPexhibitssuperiorfew-shot
decision-makingmoreefficiently.
adaptationabilitycomparedwithbaselines,includingthe
well-establishedMARLalgorithmsLOLA,socialinfluence, Theory of mind (ToM), originally a concept of cognitive
A3C, prosocial-A3C, PR2, and a model-based algorithm science and psychology (Baron-Cohen et al., 1985), has
direct-OM.Meanwhile,HOPachieveshighrewardsinself- beentransformedintocomputationalmodelsoverthepast
play, showing its exceptional decision-making ability in decadeandusedtoinferagents’mentalstatessuchasgoals
mixed-motive games. In addition, we observe the emer- and desires. Bayesian inference has been a popular tech-
gence of social intelligence from the interaction between niqueusedtomakeToMcomputational(Bakeretal.,2011;
multiple HOP agents, such as self-organized cooperation Po¨ppel & Kopp, 2018; Wu et al., 2021; Zhi-Xuan et al.,
andallianceofthedisadvantaged. 2022). Withtherapiddevelopmentoftheneuralnetwork,
somerecentworkhasattemptedtoachieveToMusingneu-
ralnetworks(Rabinowitzetal.,2018;Shu&Tian,2018;
2.RelatedWork
Wenetal.,2019;Morenoetal.,2021). HOPgivesapracti-
MARLhasexploredmulti-agentdecision-makinginmixed- calandeffectiveframeworktoutilizeToM,andextendits
motivegames. Oneapproachistoaddintrinsicrewardsto applicationscenariostomixed-motiveenvironments,where
incentivize collaboration and consideration of the impact bothcompetitionandcooperationareinvolvedandagents’
onothers,alongsidemaximizingextrinsicrewards. Notable goalsareprivateandvolatile.
examplesincludeToMAGA(Nguyenetal.,2020),MARL
MonteCarloTreeSearch(MCTS)isawidelyadoptedplan-
withinequityaversion(Hughesetal.,2018),andprosocial
ning method for optimal decision-making. Recent work,
MARL (Peysakhovich & Lerer, 2018). However, many
suchasAlphaZero(Silveretal.,2018)andMuZero(Schrit-
ofthesealgorithmsrelyonhand-craftedintrinsicrewards
twieseretal.,2020)haveusedMCTSasageneralpolicy
and assume access to rewards of co-players, which can
improvementoperatoroverthebasepolicylearnedbyneu-
make them exploitable by self-interested algorithms and
ral networks. However, MCTS is limited in multi-agent
less effective in realistic scenarios where others’ rewards
environments,wherethejointactionspacegrowsrapidly
arenotvisible(Komorita&Parks,1995). Toaddressthese
with the number of agents (Choudhury et al., 2022). We
issues, Jaques et al. (2019) have included intrinsic social
avoidthisproblembyestimatingthepoliciesofco-players
influencerewardthatusecounterfactualreasoningtoassess
andplanningonlyforthefocalagent’sactions.
2EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
BAMDP (Duff, 2002) is a principled framework for han- thatagentichoosesactiona atstates.
i
dlinguncertaintyindynamicenvironments. Itmaintainsa
Theenvironmentswestudyhaveasetofgoals,denotedby
posteriordistributionoverthetransitionprobabilities,which
G=G ×G ×···×G ,whereG ={g ,··· ,g }
is updated using Bayes’ rule as new data becomes avail- 1 2 n i i,1 i,|Gi|
representsthesetofgoalsforagenti. g isasetofstates,
able. Severalalgorithms(Guezetal.,2012;Zintgrafetal., i,k
where g ∩ g = ∅,∀ k ̸= k′. We would say agent
2019; Rigter et al., 2021) have been developed based on i,k i,k′
i’s goal is g at time t, if ∃t′ ≥ 0,st+t′ ∈ g and
BAMDP, but they are designed for single-agent environ- i,k0 i,k0
∀ 0 ≤ t′′ < t′,0 ≤ k ≤ |G |,st+t′′ ∈/ g . For any two
ments. BA-MCP (Guez et al., 2012) employs the Monte i i,k
agentsiandj, icaninferj’sgoalbasedonitstrajectory.
Carlo Tree Search (MCTS) method to provide a sample-
Specifically,imaintainsabeliefoverj’sgoals,b :G →
basedapproachgroundedinBAMDP.However,itassumes ij j
[0,1],whichisaprobabilitydistributionoverG .
afixedtransitionfunctiondistributiontobelearnedinter- j
actively, posing challenges in multi-agent scenarios due Here, algorithms are evaluated in terms of self-play and
totheco-player’sstrategyunderanunknowndistribution. few-shotadaptationtounseenpoliciesinmixed-motiveen-
(Ngetal.,2012)combinesBAMDPwithI-POMDPinan vironments. Self-play involves multiple agents using the
attempt to address multi-agent problems. However, this samealgorithmtoundergotrainingfromscratch. Theper-
integrationintroducescomputationalcomplexityissuessim- formance of algorithms in self-play is evaluated by their
ilartothoseofI-POMDP,aspreviouslydiscussed. Incon- expectedrewardafterconvergence. Self-playperformance
trast, HOP efficiently handles both reward and transition demonstratesthealgorithm’sabilitytomakeautonomous
uncertainties,andextendsMCTStomulti-agentscenarios, decisionsinmixed-motiveenvironments. Few-shotadap-
offeringascalablesolutionformulti-agentenvironments. tationreferstothecapabilitytorecognizeandrespondap-
propriately to unknown policies within a limited number
Numerousreal-worldscenarios,includingautonomousdriv-
of episodes. The performance of algorithms in few-shot
ing,human-machineinteractionandmulti-playersports,can
adaptation is measured by the rewards they achieve after
beeffectivelymodeledasmixed-motivegames. Existing
engaginginthesebriefinteractions.
research(Fisacetal.,2018;Nakamura&Bansal,2023;Hu
etal.,2023)hasexploredplanningandcontrollingrobotsin
theserealmulti-agentenvironments,relyingonpredictions 4.Methodology
ofotheragents’behaviorwithinthescene. Thesestudies
Inthissection,weproposeHierarchicalOpponentmodel-
primarilyconcentrateonrobotcontrolwithinspecificsce-
ingandPlanning(HOP),anovelalgorithmformulti-agent
narios. In contrast, our environment abstracts the mixed
decision-makinginmixed-motiveenvironments. HOPcon-
motivationfactorsinherentinthesescenarios,enablingrep-
sistsoftwomainmodules:anopponentmodelingmoduleto
resentationofabroaderrangeofscenariosandfacilitating
inferco-players’goalsandpredicttheirbehaviorandaplan-
thedevelopmentofmoregeneralalgorithms. Webelieve
ningmoduletoplanthefocalagent’sbestresponseguided
HOPholdssignificantpotentialforapplicationinvarious
by the inferred information from the opponent modeling
real-lifescenarios.
module.
3.ProblemFormulation Basedonthehypothesisincognitivepsychologythatagents’
behaviorisgoal-directed(Gergelyetal.,1995;Buresh&
We consider multi-agent hierarchical decision-making in Woodward,2007),andthatagentsbehavestablyforaspe-
mixed-motiveenvironments,whichcanbedescribedasa cificgoal(Warren,2006),theopponentmodelingmodule
Markov game (Littman, 1994) with goals, specified by a modelsbehaviorofco-playerswithtwolevelsofhierarchy.
tuple<N,S,A,T,R,γ,T max,G>. At the high-level, the module infers co-players’ internal
goals by analyzing their action sequences. Based on the
Here, agent i ∈ N = {1,2,··· ,n} chooses action from
inferredgoalsandthecurrentstateoftheenvironment,the
action space A = {a }. A = A ×A ×···×A is
i i 1 2 n
low-level component learns goal-conditioned policies to
thejointactionspace. Thejointactiona ∈Awilllead
1:n
modeltheatomicactionsofco-players.
to a state transition based on the transition function T :
S×A×S →[0,1]. Specifically,afteragentstakethejoint Intheplanningmodule,MCTSisusedtoplanforthebest
action a 1:n the state of the environment will transit from responseofthefocalagentbasedontheinferredco-players’
stos′withprobabilityT(s′|s,a 1:n). Therewardfunction policies. Tohandletheuncertaintyoverco-players’goals,
R i :S×A→Rdenotestheimmediaterewardreceivedby wesamplemultiplegoalcombinationsofallco-playersfrom
agentiafterjointactiona 1:n istakenonstates∈S. The thecurrentbeliefandreturntheactionthatmaximizesthe
discountfactorforfuturerewardsisdenotedasγ. T maxis averagereturnoverthesampledconfigurations. Following
themaximumlengthofanepisode. π i : S ×A i → [0,1] AlphaZero(Silveretal.,2018)andMuZero(Schrittwieser
denotesagenti’spolicy,specifyingtheprobabilityπ i(a i|s) etal.,2020),wemaintainapolicyandavaluenetworkto
3EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
Figure1.OverviewofHOP.HOPconsistsofanopponentmodelingmoduleandaplanningmodule. Theopponentmodelingmodule
modelsthebehaviorofco-playersbyinferringco-players’goalsandlearningtheirgoal-conditionedpolicies.Estimatedbehavioristhen
fedtotheplanningmoduletoselectarewardingactionforthefocalagent.
boostMCTSplanningandinturnusetheplannedaction thatmakes(cid:80) bK,t+1(g ) = 1. Thelikelihoodterm
gj∈Gj ij j
anditsvaluetoupdatetheneuralnetwork. Pr (aK,t|sK,t,g ) is provided by the estimated goal-
i j j
Figure1givesanoverviewofHOP,andthepseudo-codeof conditionedpoliciesofco-players,whicharedescribedin
HOPisprovidedinAppendixA. thefollowing.
However,intra-OMmaysufferfrominaccuracyoftheprior
4.1.OpponentModelingwithEfficientAdaptation (i.e.,bK,0(g ))whenpasttrajectoriesarenotlongenough
ij j
forupdates. Inter-OMmakesupforthisbycalculatinga
In goal-inference (as the light yellow component shown
precisepriorbasedonpastepisodes. Beliefupdatebetween
inFigure1), HOPsummarizestheco-players’objectives
twoadjacentepisodesisdefinedas:
basedontheinteractionhistory. However,itfacesthechal-
lengeoftheco-player’sgoalspotentiallychangingwithin 1
bK,0(g )= [αbK−1,0(g )+(1−α)1(gK−1 =g )],
episodes. Tosolvetheseissues,weproposetwoupdatepro- ij j Z ij j j j
2
ceduresbasedonToM:intraopponentmodeling(intra-OM), (2)
whichinferstheco-player’simmediategoalswithinasingle whereα∈[0,1]isthehorizonweight,whichcontrolsthe
episode, and inter opponent modeling (inter-OM), which importance of the history. As α decreases, agents attach
summarizestheco-player’sgoalsbasedontheirhistorical greaterimportancetorecentepisodes. 1(·)istheindicator
episodes. Intra-OMreasonsaboutthegoalofco-playerj function. Z is the normalization factor. The equation is
2
inthecurrentepisodeK accordingtoj’spasttrajectoryin equivalenttoatime-discountedmodificationoftheMonte
episodeK. ItensuresthatHOPisabletoquicklyrespond Carlo estimate. Inter-OM summarizes co-players’ goals
toin-episodebehaviorchangesofco-players. Specifically, accordingtoallthepreviousepisodes,whichisofgreathelp
inepisodeK,agenti’sbeliefaboutagentj’sgoalsattime whenplayingwiththesameagentsinaseriesofepisodes.
t,bK,t(g ),isupdatedaccordingto:
ij j The goal-conditioned policy (as the light orange compo-
bK ij,t+1(g j)=Pr(g j |sK,0:t+1,aK j ,0:t) nent shown in Figure 1) π ω(aK j ,t|sK,t,g j) is obtained
through a neural network ω. To train the network, a set
=Pr(g j|sK,0:t,aK
j
,0:t−1) of(sK,t,aK
j
,t,g jK,t)iscollectedfromepisodesandsentto
·Pr (aK,t|sK,0:t,aK,0:t−1,g ) thereplaybuffer. ωisupdatedatintervalstominimizethe
i j j j
negativelog-likelihood:
(1)
Pr(sK,t+1|sK,0:t,aK,0:t,g )
· Pr (sK,t+1,aK,t|sK,0:j t,aK,0:j t−1) L(ω)=E[−log(π ω(aK j ,t|sK,t,g jK,t))]. (3)
i j j
1
= bK,t(g )Pr (aK,t|sK,t,g ), 4.2.PlanningunderUncertainCo-playerModels
Z ij j i j j
1
Giventhepoliciesofco-playersestimatedbytheopponent
where we follow the Markov assumption
Pr(sK,t+1|sK,0:t,aK,0:t,g ) = Pr(sK,t+1|sK,t,aK,t) modeling module, we can leverage planning algorithms
j j j suchasMCTStocomputeanadvantageousaction. How-
and model the co-player j to maintain a Markov pol-
icy Pr (aK,t|sK,t,g ) = Pr (aK,t|sK,0:t,g ), and ever, a key obstacle to applying MCTS is that co-player
i j j i j j policiesestimatedbytheopponentmodelingmodulecon-
Z =
Pri(sK,t+1,aK j,t|sK,t)
is the normalization factor tainuncertaintyoverco-players’goals. Naivelyaddingsuch
1 Pr(sK,t+1|sK,t,aK,t)
4EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
uncertainty as part of the environment would add a large NotethattheeffectivenessofMCTSishighlyassociated
biastothesimulationanddegradeplanningperformance. with the default policies and values provided to MCTS.
Toovercomethisproblem,weproposetosampleco-players’ When they are close to the optimal ones, they can offer
goalcombinationsaccordingtothebeliefmaintainedbythe anaccurateestimateofstatevalue,guidingMCTSsearch
opponentmodelingmodule,andthenestimateactionvalue in the right direction. Therefore, following Silver et al.
byMCTSbasedonthesamples. Tobalancethetrade-offbe- (2018), we train a neural network θ to predict the policy
tweencomputationalcomplexityandplanningperformance, andvaluefunctionsateverystatefollowingthesupervision
we repeat the process multiple times and choose actions provided by MCTS. Specifically, the policy target is the
accordingtotheaverageactionvalue. Inthefollowing,we policygeneratedbyMCTS,whilethevaluetargetisthetrue
firstintroducethenecessarybackgroundofMCTS.Wethen discountedreturnofthestateinthisepisode.
proceedtointroducehowweplanforarewardingaction
Asforstates˜k intheMCTS,thepolicyfunctionprovidesa
undertheuncertaintyoverco-playerpolicies.
priordistributionoveractionsπk(·|s˜k). Actionswithhigh
θ
priorprobabilitiesareassignedhighpUCTscores,prioritiz-
MCTS MonteCarloTreeSearch(MCTS)isatypeoftree
ingtheirexplorationduringthesearchprocess. However,as
searchthatplansforthebestactionateachtimestep(Silver
theexplorationprogresses,theinfluenceofthispriorgrad-
& Veness, 2010; Liu et al., 2020). MCTS uses the envi-
uallydiminishes(seedetailsinAppendixE.1). Thevalue
ronmenttoconstructasearchtree(rightsideofFigure1)
function vk estimates the return and provides the initial
wherenodescorrespondtostatesandedgesrefertoactions. θ
valueofs˜k whens˜k isfirstreached.
Specifically,eachedgetransferstheenvironmentfromits
parent state to its child state. MCTS expands the search Thenetworkθisupdatedbasedontheoverallloss:
treeinways(suchaspUCT)thatproperlybalanceexplo-
rationandexploitation. Valueandvisitofeverystate-action L(θ)=L p(π MCTS,π θ)+L v(r i,v θ), (6)
(node-edge)pairarerecordedduringexpansion(Silveretal.,
where
2016). Finally,theactionwiththehighestvalue(orhighest
visit)oftherootstate(node)isreturnedandexecutedinthe L (π ,π )=E[−(cid:88) π (a|sK,t)log(π (a|sK,t)],
p 1 2 1 2
environment. a∈Ai
(cid:88)∞
L (r ,v)=E[(v(sK,t)− γl−trK,l)2].
Planningunderuncertainco-playerpolicies Basedon v i i
l=t
beliefs over co-players’ goals and their goal-conditioned
policiesfromtheopponentmodelingmodule,werunMCTS 5.Experiments
forN rounds. Ineachround,co-players’goalsaresampled
s
accordingtothefocalagent’sbeliefoverco-players’goals 5.1.ExperimentalSetup
b (g ). Specifically,attimetinepisodeK,wesamplethe
ij j AgentsaretestedinMarkovStag-Hunt(MSH)andMarkov
goal combination g −i = {g j ∼ bK ij,t(·),j ̸= i}. Then at SnowdriftGame(MSG).
everystates˜k intheMCTStreeofthisround,co-players’
MSHexpandstheenvironmentinPeysakhovich&Lerer
actions˜a aredeterminedby˜a ∼ π (·|s˜k,g )from
−i −i ω −i (2018)intermsofthenumberofagents. InMSH,4agents
thegoal-conditionedpolicy.
arerewardedforhuntingprey.AsshowninFigure2(a),each
Ineachround,MCTSgivestheestimatedactionvalueofthe agenthassixactions: idle,moveleft,moveright,moveup,
currentstateQ(sK,t,a,g −i)=V(s˜′(a))(a∈A i),where movedown,andhunt. Ifthereareobstaclesorboundaries
s˜′(a)isthenextstateaftertakinga˜0 ∪afroms˜0 =sK,t. inanagent’smovingdirection,itspositionstaysunchanged.
−i
Agentscanhuntpreyintheircurrentgrid. Therearetwo
WeaveragetheestimatedactionvaluefromMCTSinall
types of prey: stags and hares. A stag provides a reward
N rounds:
s of10,andrequiresatleasttwoagentslocatedatitsgridto
Q (sK,t,a)=(cid:88)Ns Q (sK,t,a,gl ). (4) execute“hunt”together. Thesecooperatingagentswillsplit
avg l −i
l=1 therewardevenly. Ahare,whichanagentcancatchalone,
Agenti’spolicyfollowsBoltzmannrationalitymodel(Baker provides a reward of 1. After a successful hunt, both the
etal.,2017): huntersandthepreydisappearfromtheenvironment. The
gameterminateswhenthetimestepreachesT =30.
max
exp(βQ (sK,t,a))
π MCTS(a|sK,t)= (cid:80) exp(a βv Qg (sK,t,a′)), (5) WeconductedexperimentsintwodifferentsettingsofMSH.
a′∈Ai avg
Inthefirstsetting,thereare4haresand1stag(MSH-4h1s).
whereβ ∈[0,∞)isrationalitycoefficient. Asβ increases, Inthisscenario,agentscancooperateinhuntingthestagto
thepolicygetsmorerational. Wechooseouractionattime maximizetheirprofits,whilealsocompetingwithco-players
toftheepisodeK basedonπ (a|sK,t). fortheopportunitytohunt. Thesecondsettingcontains4
MCTS
5EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
Th f
t
cio oa
m
hr or ie
e
s
pas
s
esg
t
ea ree atn pn
u
td st ips oa2 nmfto ts
ae
at
nr
ia c
n
dg to ths ao
re
i
ip(
n
sM fie ksr
r
-a tsS dhttH oe
es
m, u- t4 ecb innh cu ase2 t
i
nss ot
s
t) h nf. de
u
bT ele
e
fh
h
etne
u
w
cvr
n
ti ee itr oeioa
n
nnnr
g
,e m
p
hi
as ne iyu gn oef htfi
a
f lf
icwc
-
ghi di he l oen tl impt ne is
i
gsn nt oda
tad
hg ne5 es
t.
draw
er
laudividni
012345
0 1
2c do eo fep ce tr ioa ntion
3
draw
er
laudividni
01234
0
c do eo fep ce tr ioa 1ntion
2 3
draw
er
laudividni1234 00000
0 1
c
d
2o eo fep ce tr ioa ntion
3
number of other cooperators number of other cooperators number of other cooperators
dilemmainherentintheStag-Huntgame.
(a) MSH-4h1s (b) MSH-4h2s (c) MSG
InMSG(Figure2(b)),
therearesixsnowdrifts Figure3.Schellingdiagramsfor(a)MSH-4h1s,(b)MSH-4h2s,
and(c)MSG.
locatedrandomlyinan
8 × 8 grid. Simi- Baselines Here,somebaselinealgorithmsareintroduced
lar to MSH, at every toevaluatetheperformanceofHOP.Duringtheevaluation
timesteptheagentcan of few-shot adaptation, baseline algorithms serve a dual
stay idle or move one purpose. Firstly, theyactasunfamiliarco-playersduring
(a) MSH (b) MSG
step in any direction. theevaluationprocesstotestthefew-shotadaptationabil-
Agentsareadditionally ityofHOP.Secondly,weevaluatethefew-shotadaptation
Figure2.Overview of Markov
equipped with a “re- abilityofthebaselinealgorithmstodemonstrateHOP’ssu-
Stag-HuntandMarkovSnowdrift.
move a snowdrift” ac- periority. LOLA (Foerster et al., 2018; Zhao et al., 2022)
There are four agents, repre-
tion, which removes sentedbycoloredcircles, ineach agentsconsidera1-steplook-aheadupdateofco-players,
the snowdrift in the paradigm. (a) Agents catch prey andupdatetheirownpoliciesaccordingtotheupdatedpoli-
samecellastheagent. forreward. Astagwithareward ciesofco-players. SI (Jaquesetal.,2019)agentshavean
Whenasnowdriftisre- of 10 requires at least two agents intrinsicrewardtermthatincentivizesactionsmaximizing
moved,removersshare to hunt together. One agent can theirinfluenceonco-players’actions. Theinfluenceisac-
the cost of 4 evenly, hunt a hare with a reward of 1. cessedbycounterfactualreasoning. A3C(Mnihetal.,2016)
andeveryagentgetsa (b) Everyone gets a reward of 6 agentsaretrainedusingtheAsynchronousAdvantageActor-
rewardof6. Thegame whenanagentremovesasnowdrift. Critic method, a well-established reinforcement learning
When a snowdrift is removed,
endswhenallthesnow- (RL)technique. Prosocial-A3C(PS-A3C)(Peysakhovich
removerssharethecostof4evenly.
drifts are removed or &Lerer,2018)agentsaretrainedusingA3Cbutsharere-
thetimeT max =50runsout.Thegame’sessentialdilemma wardsbetweenplayersduringtraining,sotheyoptimizethe
arisesfromthefactthatanagentcanobtainahigherreward per-capitarewardinsteadoftheindividualreward,empha-
by free-riding, i.e., waiting for co-players to remove the sizingcooperationbetweenplayers. PR2(Wenetal.,2019)
snowdrifts,thanbyremovingasnowdriftthemselves. How- agentsmodelhowtheco-playerswouldreacttotheirpoten-
ever,ifallagentstakefreerides,nosnowdriftisremoved, tialbehavior,basedonwhichagentsfindthebestresponse.
andagentswillnotreceiveanyreward.Ontheotherhand,if The ablated version of HOP, direct-OM, retains the plan-
anyagentissatisfiedwithasuboptimalstrategyandchooses ningmodule,butusesneuralnetworkstomodelco-players
toremovesnowdrifts,boththegroupbenefitandindividual directly(seedetailsinAppendixF.2). Inaddition,wecon-
rewardsincrease. structsomerule-basedstrategies. Randompolicytakesa
valid action randomly at each step. An agent that consis-
Inbothenvironments,fouragentshavenoaccesstoeach
tentlyadoptscooperativebehavioriscalledcooperator,and
other’sparameters,andcommunicationisnotallowed. Ap-
an agent that consistently adopts exploitative behavior is
pendixCintroducesthegoaldefinitionofthesegames.
calleddefector. InMSH,thegoalsofcooperatorsandde-
fectorsarehuntingtheneareststagandhare,respectively.
Schellingdiagrams Gametypesaredeterminedbytherel-
In MSG, cooperators keep moving to remove the nearest
ativevaluesofelementsinthepayoffmatrix. TheSchelling
snowdrift,anddefectorsrandomlytakeactionsotherthan
diagram(Schelling,1973;Hughesetal.,2018)isanatural
”removeasnowdrift”. Whenevaluatingfew-shotadaptation,
generalization of the payoff matrix for two-player games
thesetofunfamiliarco-playersincludesLOLA,A3C,and
tomulti-playersettings. AsshowninFigure3,Schelling
PS-A3C,servingasrepresentativesoflearningagentswith
diagrams validate our temporal and spatial extension of
explicitopponentmodelingmodule,self-interestpurpose,
thematrix-formgames,whichmaintainsthedilemmasde-
and prosocial purpose, respectively. The co-players also
scribedbymatrix-formgames(seeadetaileddiscussionin
includerule-basedagents: random,cooperatoranddefector.
AppendixD).Moreover,acrossthesethreeSchellingdia-
grams,thelinesofcooperationanddefectionintersect. This
5.2.Performance
impliesthatbestresponseschangewithco-players’behav-
ior, rendering few-shot adaptation in these environments Theexperimentconsistsoftwophases. Thefirstphasefo-
inherentlychallenging. cusesonself-play,whereagentsusingthesamealgorithm
6EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
aretraineduntilconvergence. Self-playperformance,show- 1 1
i
a
o
S
un
l
sn
pgg
id
e
no
ct gprh
ii
h
fi
ae
th
a
c
da
sm
a
ieb
l
f’
li
fes
yl ei
v
,
rt
a
a
ey
av
l
nuet
f
to
ar
o
aa
t
c
lea
g
gasc
e
olh
t rh
ai
r
ie
e
te
g
hv
w
ef
me
nea
w
tc
r
fdo
i
o-
nso
ra
thp
2ef
oe
rt 4te
ar 0a
r
a
c
0t
d
ti
c
so
a
so
tpn
w
en
t,
pv
a
iti
ste
hs
i
.r
ogm
Ttn
he
he
n
ra
ea ebcs
e
fie
ou
l.
i
c
cr
t
oe
y
aTd
-
lo
ph
ab
f
l
ge
ay
H
eys
nt
O
eeh
tr
’c
Pe
s
s-
.
sgats
gnitnuh
fo
feileB0000 .... 2468 sgats
gnitnuh
fo
feileB0000 .... 2468
0 0
averagerewardduringthefinal600stepsisusedtomeasure 0 1200 2400 0 1200 2400
Time steps Time steps
itsalgorithm’sfew-shotadaptationability. Atthestartof
(a) MSH-4h1s (b) MSH-4h2s
theadaptationphase,anypolicy’sparametersaretheconver-
gentparametersderivedfromthecorrespondingalgorithms
Figure5.VisualizationofHOP’sbeliefinadaptationtothreede-
in self-play. During this phase, policies can update their fectorsinMSH.Everyblue-filledcirclerepresentsHOP’sinferred
parametersifpossible. Implementationdetailsaregivenin probability(i.e.,belief)thataco-playerhuntsstags.
AppendixE.Theresultsofself-playandfew-shotadaptation
aredisplayedinFigure4andTable1,respectively.
ating strategies such as LOLA or random agents lacking
PToM direct-OM LOLA A3C fixedobjectives,HOPseeksoutopportunitiesforheightened
PS-A3C SI PR2
returns through cooperation. Furthermore, when encoun-
3.5 30
teringco-playerslikeA3Canddefectors,knownfortheir
H G
S M S M inclinationtowardshuntinghares,HOPadjuststothesenon-
n n
i d i d cooperativescenarioswithinasmallamountofinteraction.
ra
w 2
18ra
w HOPanddirect-OMachievesubstantiallygreaterrewards
e e
r
e g
r
e g whenconfrontingdefectorscomparedtoPS-A3C,whoalso
a a
re re favorscooperation. Thisobservationhighlightsthepivotal
v A v A
roleoftheplanningmoduleinefficientadaptation.
0.5 6
MSH-4h1s MSH-4h2s MSG
Figure4.Self-playperformanceofHOPandbaselinealgorithms. MSH-4h2s As depicted in Figure 4, in MSH-4h2s, all
Shownistheaveragerewardintheself-playtrainingphase. algorithmshavelearnedthestrategyofcooperativelyhunt-
ingstags,amongwhichHOPandA3Caremorestableand
yield higher returns. PS-A3C tends to delay hunting, as
MSH-4h1s InMSH-4h1s,onlyHOP,direct-OM,andPS-
earlyhuntingresultsinleavingtheenvironmentandfailing
A3Clearnthestrategyofhuntingstags(Figure4). How-
toobtainthegrouprewardfromsubsequenthunting. This
ever, since PS-A3C can get rewards without hunting by
mayleadPS-A3Ctosuboptimalactionsinthelastfewsteps
itself,itmaynoteffectivelylearntherelationshipbetween
andthusfailtohuntunderthe5-stepterminationrule.
hunting and receiving rewards, leading to a ”lazy agent”
problem (Sunehag et al., 2017) for PS-A3C. This results TheadaptationperformanceinMSH-4h2sispresentedin
in the overall reward of PS-A3C being inferior to HOP Table1(b). Whenfacingwiththecooperator, thebestre-
anddirect-OM.LOLAswingsbetweenhuntingstagsand sponseistohuntstags,whichrequiresminimaladjustments
huntinghares. SIandA3Cprimarilylearnthestrategyof toeachalgorithm’spolicies,sotheirreturnsarecompara-
huntinghares,resultinginlowrewards. PR2failstowork ble to the Orcale reward. Similarly, when encountering
inMSH.Inthisenvironment,thenumberofagentsmaybe learningco-playerswhohaveadoptedthecooperationpol-
reducedduetothesuccessfulhuntingofagents,andthisis icy,HOPandmostbaselinesyieldhighrewards. However,
notsupportedbyPR2. Despiteattemptstomodifytheal- given that learning agents may dynamically adjust their
gorithmaccordingly,themodifiedversionultimatelyfailed goals,itbecomesessentialtodiscernthereal-timegoalsof
tolearnadecentpolicy. Asaresult,therelevantresultsof theco-playersinordertofindthebestresponse. Inthese
PR2inMSHarenotshowninFigure4andTable1. scenarios,HOP’sperformancesurpassesthatofotheralgo-
rithms,approachingtheOrcalereward. Whenplayingwith
HOPlearnsthestaghuntingstrategythroughself-play,en-
non-cooperativeco-playerssuchasrandomanddefectors,
ablingseamlesscooperationwithagentslikePS-A3Cand
significantstrategyadjustmentsarenecessaryforeachal-
cooperators, which similarly prioritize stag hunting (Ta-
gorithmtoachievehighreturns. Therefore,thereturnsfor
ble 1(a)). This compatibility stems from the fact that in
allalgorithmsarenotablydiminished. HOPdemonstrates
the Stag-Hunt game, the best response of cooperation is
superioradaptabilitycomparedtotheotheralgorithms,ex-
cooperation. Thus, direct-OM and PS-A3C agents, who
hibitingitsabilitytomakesubstantialstrategicadjustments.
are equipped with learned cooperative strategies, also at-
tainrelativelyhighrewardswhenplayingwithcooperative WewouldliketoprovidefurtherintuitiononwhyHOPis
co-players. Whenconfrontedwithco-playerswithfluctu- capableofefficientlyadaptingitspolicytounseenagents.
7EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
Table1.Few-shotadaptationperformanceofHOPandbaselinesin(a)MSH-4h1s,(b)MSH-4h2s,and(c)MSG.Theinteractionhappens
between1agentusingtherowpolicyand3co-playersusingthecolumnpolicy. Shownarethemin-maxnormalizedrewards,with
normalizationboundssetbytherewardsofOrcaleandthelowestrewardsamongallbaselinesandrandompolicy.Seedetaileddescription
andanalysisofOrcaleinAppendixF.1. Theresultsaredepictedfortherowpolicyfrom1800to2400step. Overallbestadaptation
percentageshowstheproportionofscenariosinwhichthealgorithmperformsoptimally,whileaccountingforstandarddeviation.
(a) PerformanceinMSH-4h1s
learningco-players rule-basedco-players
Overallbest
LOLA A3C PS-A3C random cooperator defector adaptationpercentage
HOP 0.97±0.06 0.80±0.15 0.93±0.04 0.96±0.07 0.74±0.06 0.51±0.02 83.3%
direct-OM 0.64±0.05 0.57±0.07 0.79±0.04 0.91±0.10 0.56±0.04 0.44±0.04 16.7%
LOLA - 0.55±0.07 0.38±0.04 0.45±0.06 0.46±0.03 0.41±0.02 0.0%
A3C 0.35±0.01 - 0.24±0.02 0.85±0.01 0.31±0.02 1.00±0.01 20.0%
PS-A3C 0.85±0.05 0.60±0.11 - 0.64±0.06 0.55±0.05 0.09±0.04 0.0%
SI 0.32±0.01 0.95±0.04 0.22±0.02 0.81±0.01 0.28±0.02 0.89±0.04 16.7%
(b) PerformanceinMSH-4h2s
learningco-players rule-basedco-players
Overallbest
LOLA A3C PS-A3C random cooperator defector adaptationpercentage
HOP 0.97±0.02 0.99±0.02 0.88±0.02 0.78±0.07 1.00±0.01 0.36±0.02 100.0%
direct-OM 0.95±0.01 0.85±0.02 0.74±0.03 0.62±0.04 0.96±0.02 0.31±0.02 16.7%
LOLA - 0.92±0.04 0.82±0.02 0.75±0.04 1.00±0.03 0.28±0.03 40.0%
A3C 0.91±0.02 - 0.87±0.02 0.55±0.05 0.98±0.02 0.25±0.02 40.0%
PS-A3C 0.24±0.03 0.18±0.02 - 0.29±0.02 0.38±0.01 0.06±0.02 0.0%
SI 0.77±0.02 0.83±0.01 0.74±0.01 0.52±0.03 0.87±0.03 0.27±0.02 0.0%
(c) PerformanceinMSG
learningco-players rule-basedco-players
Overallbest
LOLA A3C PS-A3C random cooperator defector adaptationpercentage
HOP 0.78±0.04 0.39±0.09 0.65±0.08 0.44±0.03 0.48±0.05 0.55±0.01 83.3%
direct-OM 0.31±0.11 0.12±0.05 0.55±0.04 0.38±0.04 0.67±0.05 0.34±0.05 50.0%
LOLA - 0.33±0.07 0.55±0.06 0.25±0.08 0.43±0.04 0.18±0.01 40.0%
A3C 0.33±0.04 - 0.52±0.09 0.30±0.04 0.33±0.03 0.14±0.01 20.0%
PS-A3C 0.67±0.05 0.35±0.04 - 0.33±0.04 0.00±0.08 0.38±0.02 20.0%
SI 0.74±0.08 0.00±0.05 0.33±0.08 0.00±0.04 0.24±0.07 0.24±0.03 16.7%
PR2 0.00±0.13 0.00±0.08 0.58±0.05 0.16±0.05 0.43±0.02 0.14±0.01 16.7%
Taketheexperimentfacingthreedefectors(alwaysattempt- belief,whichisusedasapriorforintra-OMatthestartof
ingtohuntthenearesthare)asanexample. Therearetwo everyepisode. Adecliningline,formedbythepointsfrom
goalshere: huntingstagsorhuntinghares. Atthestartof initial steps of each episode, appears in both sub-figures
the evaluation phase, HOP holds the belief that every co- ofFigure5,whichreflectsthatHOPgraduallyreducesthe
playerismorelikelytohuntastagbecauseHOPhasseen prioroftheco-playerhuntingastagthroughinter-OM.
itsco-playershuntstagsmorethanharesduringself-play.
ThisfalsebelieffordefectorsdegradesHOP’sperformance. MSG As shown in Figure 4, HOP achieves the highest
Both intra-OM and inter-OM correct this false belief by rewardduringself-playanditisclosetothetheoretically
updatingduringtheintereactionswithdefectors(seevisu- optimalaveragerewardinthisenvironment(i.e. whenall
alizationofbeliefupdateinFigure5). Intra-OMprovides snowdrifts are removed, resulting in a group average re-
theabilitytocorrectthebeliefofhuntingstagswithinan wardof30.0). Thisoutcomeisaremarkableachievement
episode. Specifically,asaco-playerkeepsmovingcloser inafullydecentralizedlearningsettingandhighlightsthe
toahare,intra-OMwillupdatethebeliefoftheco-player highpropensityofHOPtocooperate. Incontrast,LOLA,
towardthegoal“hare”,leadingtoaccurateopponentmod- A3C, SI, and PR2 prioritize maximizing their individual
els. InFigure5,therearemanypointswithvaluesnear0, profits,whichleadstoinferioroutcomesduetotheirfailure
showing that HOP infers that the agent’s goal is unlikely tocoordinateandcooperateeffectively. PS-A3Cperforms
to be a stag through intra-OM. Taking these accurate co- exceptionallywellinself-play,rankingsecondonlytoHOP.
player policies as input, the planning module can output LikeinMSH,itfailstoachievethemaximumaveragere-
advantageousactions. Inter-OMfurtheracceleratesthecon- wardduetothecoordinationproblem,whichisprominent
vergencetowardstruebeliefbyupdatingtheinter-episode whenonlyonesnowdriftisleft. Thisissuehighlightsthein-
8EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
stabilityofthepolicyduetotheabsenceofactionplanning. ments, a technique that can autonomously abstract goal
setsinvariousscenariosisneeded,which(Ashwoodetal.,
HOPdemonstratesthemosteffectivefew-shotadaptation
2022) has attempted to explore. Second, we use Level-0
performance (Table 1(c)). Specifically, when adapting to
ToM,whichinvolves”thinkofwhattheythink.”However,
threedefectors,HOPreceivessubstantiallyhigherrewards
a more complex form of ToM, such as Level-1 ToM that
thanotherpolicies.ThishighlightstheeffectivenessofHOP
considers”whatIthinktheythinkaboutme,”hasthepo-
inquicklyadaptingtonon-cooperativebehavior,whichdif-
tentialtoimproveourpredictionsaboutco-players. Never-
fersentirelyfrombehaviorofco-playersinHOP’sself-play.
theless,incorporatingnestedinferenceintroducesahigher
In contrast, A3C and PS-A3C do not explicitly consider
computationalcost. Consequently,itbecomesimperative
co-players. Theyhavelearnedthestrategiestendingtoex-
todevelopadvancedplanningmethodsthatcaneffectively
ploitandcooperate,respectively. Therefore,A3Cperforms
and rapidly leverage the insights provided by high-order
effectivelyagainstagentsthathaveahighertendencytoco-
ToM.Third,weinvestigatemix-motiveenvironmentswith
operate,suchasthecooperator. However,itsperformance
theexpectationthatHOPcanfacilitateeffectivedecision-
isrelativelypoorwhenfacingnon-cooperativeagents. Con-
makingandadaptationinhumansociety. Despiteselecting
versely,PS-A3Cexhibitstheoppositebehavior.
diversewell-establishedalgorithmsasco-players,noneof
Overall,theaboveexperimentsdemonstratetheremarkable them adequately model human behavior. It would be in-
adaptationabilityofHOPacrossallenvironments(seelast teresting to explore how HOP can perform in a few-shot
columnsinTable1). Otheralgorithmscanonlyachievethe adaptationscenarioinvolvinghumanparticipants. AsHOP
bestadaptationperformancewhenfacingsomespecificco- isself-interested,itmaynotalwaysalignwiththebestinter-
players,towhomthebestresponseisclosetothepolicies estofhumans. Onewaytomitigatethisriskisleveraging
learnedbythealgorithmsinself-play. HOPcanachievethe HOP’sabilitytoinferandoptimizeforhumanvaluesand
bestadaptationlevelinmosttestscenarios,whereco-players preferencesduringinteractions,therebyassistinghumansin
performeitherfamiliarorcompletelyunfamiliarbehavior. complexenvironments.
Meanwhile,HOPexhibitsadvantagesduringself-play.
Ablationstudyindicatesthatinter-OMandintra-OMplay Acknowledgements
crucial roles in adapting to agents with fixed goals and
ThisprojectissupportedbytheNationalKeyR&DProgram
agentswithdynamicgoals,respectively. Moreover,ifoppo-
ofChina(2022ZD0114900).
nentmodelingisnotconditionedongoals,theself-playand
few-shotadaptationabilitiesaregreatlyweakened. Further
detailsareprovidedinAppendixF.2. ImpactStatement
Weobservetheemergenceofsocialintelligence,including Thispaperpresentsworkwhosegoalistoadvancethefield
self-organizedcooperationandanallianceofthedisadvan- of Machine Learning. There are many potential societal
taged, during the interaction of multiple HOP agents in consequences of our work, none which we feel must be
mixed-motiveenvironments. Furtherdetailscanbefound specificallyhighlightedhere.
inAppendixG.
References
6.ConclusionandDiscussion
Albrecht,S.V.andStone,P. Autonomousagentsmodelling
WeproposeHierarchicalOpponentmodelingandPlanning otheragents:Acomprehensivesurveyandopenproblems.
(HOP), a hierarchical algorithm for few-shot adaptation ArtificialIntelligence,258:66–95,2018.
to unseen co-players in mixed-motive environments. It
consistsofanopponentmodelingmoduleforinferringco- Ashwood,Z.,Jha,A.,andPillow,J.W. Dynamicinverse
players’goalsandbehaviorandaplanningmoduleguided reinforcementlearningforcharacterizinganimalbehavior.
bytheinferredinformationtooutputthefocalagent’sbest AdvancesinNeuralInformationProcessingSystems,35:
response. EmpiricalresultsshowthatHOPperformsbetter 29663–29676,2022.
thanstate-of-the-artMARLalgorithms,intermsofdealing
withmixed-motiveenvironmentsintheself-playsettingand Baker,C.,Saxe,R.,andTenenbaum,J. Bayesiantheoryof
few-shotadaptationtopreviouslyunseenco-players. mind: Modeling joint belief-desire attribution. In Pro-
ceedingsoftheannualmeetingofthecognitivescience
Whilst HOP exhibits superior abilities, there are several
society,volume33,2011.
limitations illumining our future work. First, in any en-
vironment, a clear definition of goals is needed for HOP.
ToenhanceHOP’sabilitytogeneralizetovariousenviron- Baker, C. L., Jara-Ettinger, J., Saxe, R., and Tenenbaum,
J.B. Rationalquantitativeattributionofbeliefs,desires
9EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
andperceptsinhumanmentalizing. NatureHumanBe- Doshi, P. and Perez, D. Generalized point based value
haviour,1(4):1–10,2017. iteration for interactive pomdps. In AAAI, pp. 63–68,
2008.
Balduzzi, D., Garnelo, M., Bachrach, Y., Czarnecki, W.,
Perolat,J.,Jaderberg,M.,andGraepel,T. Open-ended Duff,M.O. OptimalLearning: Computationalprocedures
learninginsymmetriczero-sumgames. InInternational forBayes-adaptiveMarkovdecisionprocesses. Univer-
ConferenceonMachineLearning,pp.434–443.PMLR,
sityofMassachusettsAmherst,2002.
2019.
Eppe,M.,Gumbsch,C.,Kerzel,M.,Nguyen,P.D.,Butz,
Baron-Cohen, S., Leslie, A. M., and Frith, U. Does the
M. V., and Wermter, S. Intelligent problem-solving as
autisticchildhavea“theoryofmind”? Cognition,21(1):
integrated hierarchical reinforcement learning. Nature
37–46,1985.
MachineIntelligence,4(1):11–20,2022.
Barrett,S.,Stone,P.,andKraus,S. Empiricalevaluationof
adhocteamworkinthepursuitdomain. InThe10thIn- Fisac,J.F.,Bajcsy,A.,Herbert,S.L.,Fridovich-Keil,D.,
ternationalConferenceonAutonomousAgentsandMul- Wang,S.,Tomlin,C.J.,andDragan,A.D. Probabilisti-
tiagentSystems-Volume2,pp.567–574,2011. callysaferobotplanningwithconfidence-basedhuman
predictions. In14thRobotics: ScienceandSystems,RSS
Bauer, J., Baumli, K., Behbahani, F., Bhoopchand, A., 2018.MITPressJournals,2018.
Bradley-Schmieg,N.,Chang,M.,Clay,N.,Collister,A.,
Dasagi,V.,Gonzalez,L.,etal. Human-timescaleadapta- Foerster, J., Chen, R. Y., Al-Shedivat, M., Whiteson, S.,
tioninanopen-endedtaskspace. InInternationalCon- Abbeel, P., and Mordatch, I. Learning with opponent-
ference on Machine Learning, pp. 1887–1935. PMLR, learningawareness. InProceedingsofthe17thInterna-
2023. tionalConferenceonAutonomousAgentsandMultiAgent
Systems,pp.122–130,2018.
Bloembergen,D.,DeJong,S.,andTuyls,K. Lenientlearn-
ing in a multiplayer staghunt. In Proceedingsof 23rd
Gergely,G.,Na´dasdy,Z.,Csibra,G.,andB´ıro´,S. Taking
Benelux Conference on Artificial Intelligence (BNAIC
theintentionalstanceat12monthsofage. Cognition,56
2011),pp.44–50,2011.
(2):165–193,1995.
Browne,C.B.,Powley,E.,Whitehouse,D.,Lucas,S.M.,
Gmytrasiewicz,P.J.andDoshi,P. Aframeworkforsequen-
Cowling, P. I., Rohlfshagen, P., Tavener, S., Perez, D.,
tialplanninginmulti-agentsettings. JournalofArtificial
Samothrakis,S.,andColton,S. Asurveyofmontecarlo
IntelligenceResearch,24:49–79,2005.
tree search methods. IEEE Transactions on Computa-
tionalIntelligenceandAIingames,4(1):1–43,2012.
Guez,A.,Silver,D.,andDayan,P. Efficientbayes-adaptive
Buresh, J. S. and Woodward, A. L. Infants track action reinforcementlearningusingsample-basedsearch. Ad-
goalswithinandacrossagents. Cognition,104(2):287– vances in neural information processing systems, 25,
314,2007. 2012.
Butz, M. V. and Kutter, E. F. How the mind comes into Han,Y.andGmytrasiewicz,P. Learningothers’intentional
being: Introducingcognitivesciencefromafunctional modelsinmulti-agentsettingsusinginteractivepomdps.
andcomputationalperspective. OxfordUniversityPress, InProceedingsofthe32ndInternationalConferenceon
2016. NeuralInformationProcessingSystems,pp.5639–5647,
2018.
Choudhury,S.,Gupta,J.K.,Morales,P.,andKochenderfer,
M. J. Scalable online planning for multi-agent mdps.
Han,Y.andGmytrasiewicz,P. Ipomdp-net: Adeepneural
JournalofArtificialIntelligenceResearch,73:821–846,
networkforpartiallyobservablemulti-agentplanningus-
2022.
inginteractivepomdps. InProceedingsoftheAAAICon-
ferenceonArtificialIntelligence,volume33,pp.6062–
Dafoe,A.,Hughes,E.,Bachrach,Y.,Collins,T.,McKee,
6069,2019.
K.R.,Leibo,J.Z.,Larson,K.,andGraepel,T.Openprob-
lemsincooperativeai. arXivpreprintarXiv:2012.08630,
Hoang,T.N.andLow,K.H.Interactivepomdplite:towards
2020.
practical planning to predict and exploit intentions for
Doshi,P.andGmytrasiewicz,P.J. Montecarlosampling interacting with self-interested agents. In Proceedings
methodsforapproximatinginteractivepomdps. Journal of the Twenty-Third international joint conference on
ofArtificialIntelligenceResearch,34:297–337,2009. ArtificialIntelligence,pp.2298–2305,2013.
10EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
Hu,H.,Lerer,A.,Peysakhovich,A.,andFoerster,J. “other- McMahan,H.B.,Gordon,G.J.,andBlum,A. Planningin
play”forzero-shotcoordination. InInternationalCon- thepresenceofcostfunctionscontrolledbyanadversary.
ference on Machine Learning, pp. 4399–4410. PMLR, InProceedingsofthe20thInternationalConferenceon
2020. MachineLearning(ICML-03),pp.536–543,2003.
Hu, H., Zhang, Z., Nakamura, K., Bajcsy, A., and Fisac,
Mirsky,R.,Carlucho,I.,Rahman,A.,Fosong,E.,Macke,
J.F. Deceptiongame: Closingthesafety-learningloopin
W.,Sridharan,M.,Stone,P.,andAlbrecht,S.V. Asurvey
interactiverobotautonomy. InTan,J.,Toussaint,M.,and
ofadhocteamworkresearch. InEuropeanConference
Darvish,K.(eds.),ProceedingsofThe7thConferenceon
onMulti-AgentSystems,pp.275–293.Springer,2022.
RobotLearning,volume229ofProceedingsofMachine
LearningResearch,pp.3830–3850.PMLR,06–09Nov Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lillicrap,
2023.URLhttps://proceedings.mlr.press/ T., Harley, T., Silver, D., and Kavukcuoglu, K. Asyn-
v229/hu23b.html. chronousmethodsfordeepreinforcementlearning. In
Internationalconferenceonmachinelearning,pp.1928–
Hughes, E., Leibo,J.Z., Phillips, M., Tuyls, K., Duen˜ez-
1937.PMLR,2016.
Guzman,E.,Garc´ıaCastan˜eda,A.,Dunning,I.,Zhu,T.,
McKee,K.,Koster,R.,etal. Inequityaversionimproves
Moreno, P., Hughes, E., McKee, K. R., Pires, B. A., and
cooperationinintertemporalsocialdilemmas. Advances
Weber, T. Neuralrecursivebeliefstatesinmulti-agent
inneuralinformationprocessingsystems,31,2018.
reinforcementlearning.arXivpreprintarXiv:2102.02274,
Jaques,N.,Lazaridou,A.,Hughes,E.,Gulcehre,C.,Ortega, 2021.
P., Strouse, D., Leibo, J.Z., andDeFreitas, N. Social
Nakamura,K.andBansal,S. Onlineupdateofsafetyassur-
influence as intrinsic motivation for multi-agent deep
reinforcementlearning. InInternationalconferenceon ancesusingconfidence-basedpredictions. In2023IEEE
International Conference on Robotics and Automation
machinelearning,pp.3040–3049.PMLR,2019.
(ICRA),pp.12765–12771.IEEE,2023.
Kleiman-Weiner,M.,Ho,M.K.,Austerweil,J.L.,Littman,
M. L., and Tenenbaum, J. B. Coordinate to cooperate Ng, B., Boakye, K., Meyers, C., and Wang, A. Bayes-
orcompete: abstractgoalsandjointintentionsinsocial adaptiveinteractivepomdps. InProceedingsoftheAAAI
interaction. InCogSci,2016. Conference on Artificial Intelligence, volume 26, pp.
1408–1414,2012.
Komorita, S. S. and Parks, C. D. Interpersonal relations:
Mixed-motiveinteraction. Annualreviewofpsychology,
Nguyen,D.,Venkatesh,S.,Nguyen,P.,andTran,T. Theory
46(1):183–207,1995.
ofmindwithguiltaversionfacilitatescooperativerein-
Li,S.,Wu,Y.,Cui,X.,Dong,H.,Fang,F.,andRussell,S. forcement learning. In Asian Conference on Machine
Robustmulti-agentreinforcementlearningviaminimax Learning,pp.33–48.PMLR,2020.
deepdeterministicpolicygradient. InProceedingsofthe
Peysakhovich,A.andLerer,A. Prosociallearningagents
AAAIconferenceonartificialintelligence,volume33,pp.
solvegeneralizedstaghuntsbetterthanselfishones. In
4213–4220,2019.
Proceedings of the 17th International Conference on
Littman, M.L. Markovgamesasaframeworkformulti- AutonomousAgentsandMultiAgentSystems,pp.2043–
agentreinforcementlearning. InMachinelearningpro- 2044,2018.
ceedings1994,pp.157–163.Elsevier,1994.
Po¨ppel, J. and Kopp, S. Satisficing models of bayesian
Liu, A., Chen, J., Yu, M., Zhai, Y., Zhou, X., and Liu, J.
theory of mind for explaining behavior of differently
Watchtheunobserved:Asimpleapproachtoparallelizing
uncertain agents: Socially interactive agents track. In
montecarlotreesearch. Proceedingsofthe8thInterna-
Proceedingsofthe17thinternationalconferenceonau-
tionalConferenceonLearningRepresentations(ICLR),
tonomousagentsandmultiagentsystems,pp.470–478,
2020.
2018.
Lu,C.,Willi,T.,DeWitt,C.A.S.,andFoerster,J. Model-
Rabinowitz, N., Perbet, F., Song, F., Zhang, C., Eslami,
freeopponentshaping. InInternationalConferenceon
S. A., and Botvinick, M. Machine theory of mind. In
MachineLearning,pp.14398–14411.PMLR,2022.
Internationalconferenceonmachinelearning,pp.4218–
Mahajan, A., Samvelyan, M., Gupta, T., Ellis, B., Sun, 4227.PMLR,2018.
M., Rockta¨schel, T., and Whiteson, S. Generaliza-
tionincooperativemulti-agentsystems. arXivpreprint Rapoport,A.andChammah,A.M. Thegameofchicken.
arXiv:2202.00104,2022. AmericanBehavioralScientist,10(3):10–28,1966.
11EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
Rashid, T., Samvelyan, M., De Witt, C. S., Farquhar, G., Vezhnevets, A., Wu, Y., Eckstein, M., Leblond, R., and
Foerster,J.,andWhiteson,S. Monotonicvaluefunction Leibo,J.Z.Optionsasresponses:Groundingbehavioural
factorisationfordeepmulti-agentreinforcementlearning. hierarchies in multi-agent reinforcement learning. In
TheJournalofMachineLearningResearch,21(1):7234– InternationalConferenceonMachineLearning,pp.9733–
7284,2020. 9742.PMLR,2020.
Rigter,M.,Lacerda,B.,andHawes,N. Risk-aversebayes- Vinyals,O.,Babuschkin,I.,Czarnecki,W.M.,Mathieu,M.,
adaptive reinforcement learning. Advances in Neural Dudzik,A.,Chung,J.,Choi,D.H.,Powell,R.,Ewalds,
InformationProcessingSystems,34:1142–1154,2021. T., Georgiev, P., etal. Grandmasterlevelinstarcraftii
usingmulti-agentreinforcementlearning. Nature, 575
Rousseau,J.-J. DiscourseontheOriginofInequality. Ox-
(7782):350–354,2019.
fordUniversityPress,USA,1999.
Warren, W. H. The dynamics of perception and action.
Schelling,T.C. Hockeyhelmets,concealedweapons,and
Psychologicalreview,113(2):358,2006.
daylight saving: A study of binary choices with exter-
nalities. JournalofConflictresolution,17(3):381–428,
Wen,Y.,Yang,Y.,Luo,R.,Wang,J.,andPan,W. Proba-
1973.
bilisticrecursivereasoningformulti-agentreinforcement
learning. In7thInternationalConferenceonLearning
Schrittwieser,J.,Antonoglou,I.,Hubert,T.,Simonyan,K.,
Representations,ICLR2019,2019.
Sifre,L.,Schmitt,S.,Guez,A.,Lockhart,E.,Hassabis,
D.,Graepel,T.,etal. Masteringatari,go,chessandshogi
Wu, S. A., Wang, R. E., Evans, J. A., Tenenbaum, J. B.,
by planning with a learned model. Nature, 588(7839):
Parkes,D.C.,andKleiman-Weiner,M. Toomanycooks:
604–609,2020.
Bayesianinferenceforcoordinatingmulti-agentcollabo-
Shu, T. and Tian, Y. M3rl: Mind-aware multi-agent ration. TopicsinCognitiveScience,13(2):414–432,2021.
management reinforcement learning. arXiv preprint
Zhang, G. and Doshi, P. Sipomdplite-net: Lightweight,
arXiv:1810.00147,2018.
self-interestedlearningandplanninginposgswithsparse
Silver, D. and Veness, J. Monte-carlo planning in large interactions. arXive-prints,pp.arXiv–2202,2022.
pomdps. Advancesinneuralinformationprocessingsys-
Zhao,S.,Lu,C.,Grosse,R.B.,andFoerster,J. Proximal
tems,23,2010.
learning with opponent-learning awareness. Advances
Silver,D.,Huang,A.,Maddison,C.J.,Guez,A.,Sifre,L., in Neural Information Processing Systems, 35:26324–
VanDenDriessche,G.,Schrittwieser,J.,Antonoglou,I., 26336,2022.
Panneershelvam, V., Lanctot, M., et al. Mastering the
Zhi-Xuan, T., Gothoskar, N., Pollok, F., Gutfreund, D.,
gameofgo with deepneuralnetworksandtreesearch.
Tenenbaum, J.B., andMansinghka, V.K. Solvingthe
nature,529(7587):484–489,2016.
babyintuitionsbenchmarkwithahierarchicallybayesian
Silver,D.,Hubert,T.,Schrittwieser,J.,Antonoglou,I.,Lai, theoryofmind. arXivpreprintarXiv:2208.02914,2022.
M.,Guez,A.,Lanctot,M.,Sifre,L.,Kumaran,D.,Grae-
Zintgraf,L.,Shiarlis,K.,Igl,M.,Schulze,S.,Gal,Y.,Hof-
pel,T.,etal. Ageneralreinforcementlearningalgorithm
mann,K.,andWhiteson,S.Varibad:Averygoodmethod
thatmasterschess,shogi,andgothroughself-play. Sci-
for bayes-adaptive deep rl via meta-learning. arXiv
ence,362(6419):1140–1144,2018.
preprintarXiv:1910.08348,2019.
Son, K., Kim, D., Kang, W.J., Hostallero, D.E., andYi,
Y. Qtran: Learningtofactorizewithtransformationfor
cooperativemulti-agentreinforcementlearning. InInter-
nationalconferenceonmachinelearning,pp.5887–5896.
PMLR,2019.
Souza,M.O.,Pacheco,J.M.,andSantos,F.C. Evolution
ofcooperationundern-personsnowdriftgames. Journal
ofTheoreticalBiology,260(4):581–588,2009.
Sunehag,P.,Lever,G.,Gruslys,A.,Czarnecki,W.M.,Zam-
baldi,V.,Jaderberg,M.,Lanctot,M.,Sonnerat,N.,Leibo,
J. Z., Tuyls, K., et al. Value-decomposition networks
for cooperative multi-agent learning. arXiv preprint
arXiv:1706.05296,2017.
12EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
A.PseudoCodeofHOP
Algorithm1HOP
Input: NumberofMCTStreeN ,updateintervalT ,capacityofthetrajectorybufferL,goalsetG (j ̸= i),initial
s u j
beliefofagents’goalsb0,0(g ).
ij j
Output: ActionsaK,t,planningmodulenetworkθ,goal-conditionedpolicynetworkω.
i
foreachepisodeK do
generateinitialstateofthisepisodesK,0randomly
fort=0toT −1do
max
repeat
samplegl frombK,t(g )(j ̸=i)
−i ij j
getQ (sK,t,a,gl )(∀a)viaMCTS
l −i
untilN times
s
calculateQ (sK,t,a)(∀a)[Equation(4)]
avg
chooseactionaK,tfromπ (a|sK,t)[Equation(5)]
i MCTS
intra-OMupdatebK,t+1[Equation(1)]
ij
collectdataofthissteptothetrajectorybuffer
endfor
ifthetrajectorybufferisfullthen
updateω[Equation(3)]
endif
ifK×T ≡0(modT ) then
max u
updateθ[Equation(6)]
endif
inter-OMupdatebK+1,0[Equation(2)]
ij
endfor
B.TheoreticalAnalysis
Weaimtoofferaconcisetheoreticalanalysis. Duetothecomplexityofenvironmentscharacterizedbybothtemporaland
spatialstructures,attainingtheoreticalguaranteesinsuchenvironmentscanbeinherentlychallenging. Tostrikeabalance,
wehaveundertakenaverificationofthetheoreticalguaranteeassociatedwithHOPinthematrixgames. Thesegames
encapsulatethesamedilemmaofsequentialgames. Forclarity,ouranalysiswillbeconductedinthecontextofatwo-player
game,andtheanalysiscanbeextendedtogamesinvolvingagreaternumberofagents. Consideratwo-playergamewhere
bothplayershavetwogoals: “Cooperate”and“Defect,”resultinginautilitymatrixshowninTable2.
Table2.Utilitymatrixforatwo-playergame.Eachelementinthetablerepresentstheutilityoftherowplayer(firstvalue)andtheutility
ofthecolumnplayer(secondvalue).TheutilityvaluesR,S,T,andP determinedifferentgameparadigms.
Cooperate Defect
Cooperate R,R S,T
Defect T,S P,P
SupposeHOPistherowplayer. Atacertaintimestep,thecolumnplayerselectsitsgoalg tobe“Cooperate”witha
column
probabilityofpandtobe“efect”withaprobabilityof1−p. Wesampletheco-player’sgoaltosimulateusingMonteCarlo
TreeSearch(MCTS),withafrequencyofp+ϵto“Cooperate”andafrequencyof1−p−ϵto“Defect.”
Inthecurrentstates,wehavetwopossibleactions: a forcooperationanda fordefection. DuringtheMCTSplanning
1 2
process,whentheco-playeraimsto“Cooperate,”wehave:
Q(s,a |g =“Cooperate”)=R(1+ϵ )
1 column R
Q(s,a |g =“Cooperate”)=T(1+ϵ )
2 column T
13EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
Whentheco-playeraimsto“Defect,”wehave:
Q(s,a |g =“Defect”)=S(1+ϵ )
1 column S
Q(s,a |g =“Defect”)=P(1+ϵ )
2 column P
Thus,wecancalculatetheoverallQ-valuesasfollows:
Q(s,a )=(p+ϵ)R(1+ϵ )+(1−p−ϵ)S(1+ϵ )
1 R S
Q(s,a )=(p+ϵ)T(1+ϵ )+(1−p−ϵ)P(1+ϵ )
2 T P
Inthelearningprocess,thegoal-conditionedpolicynetworkistrainedusingsupervisedlearning,anditsaccuracysignificantly
improveswithsufficientroundsofobservation. Consequently,theaccuracyoftheenvironmentsimulationwithintheMonte
CarloTreeSearch(MCTS)algorithmbecomesexceedinglyhigh. Insuchascenario,theconvergenceguaranteeofMCTS
remainsintact,resultinginafinalprecisionofMCTSthatisremarkablyhigh. Specifically,wehave|ϵ |,|ϵ |,|ϵ |,|ϵ |≪
R S T P
|ϵ|,andthesesmallerrortermscanbesafelyignored.
Then,when
T +S−R−P
ϵ<1,
p(R−T)+(1−p)(S−P)
theoptimalstrategythatHOPobtainsisconsistentwiththetrueoptimalstrategy. Twofactorsaffectthesizeof|ϵ|: the
accuracyininferringtheco-player’sgoalsandthedeviationbetweenfrequencyandprobabilitywhensamplingthegoal. To
addresstheaccuracyissue,weemploytwolayersofmodules,intra-OMandinter-OM,tomakeaccuratepredictionsasearly
aspossibleineachepisode. Forthedeviationbetweenfrequencyandprobability,weincreasethevalueofN toreducethis
s
deviation. Inpracticalapplications,thechoiceofanappropriateN dependsonthetrade-offbetweencomputationalspeed
s
andsamplingaccuracy.
C.GoalDefinition
InMSH,wedefinetwogoals: gC ashuntingstagsandgD ashuntinghares.
In MSG, we define two goals: gC as removing the drifts, and gD as staying lazy (i.e. not attempting to remove any
snowdrifts). Forinter-OM,thegoalgC isdecomposedinto6parts: gCk (1≤k ≤6),wheregCk representsremovingk
snowdrift(s)inoneepisode. bK,0(gCk)andbK,0(gD)willbeupdatedaccordingtoEquation(2). Duringanepisode,ifthe
ij ij
co-playerj hasremovedmsnowdrift(s)attimetoftheepisodeK,ourbeliefbK,t(gC)=(cid:80)6 bK,0(gCk).
ij j k=m+1 ij j
Forintra-OM,eachsnowdriftsisdefinedasasubgoalgC[s]. WeuseEquation(1)conditionedongC toupdateourbelief:
1
bK,t+1(gC[s]|gC)= bK,t(gC[s]|gC)Pr (aK,t|sK,0:t,gC[s]),
ij j j Z ij j j i j j
1
whereZ isthenormalizationfactor. Wecanupdateourbeliefofanagentremovingasnowdrifts:
1
bK,t(gC[s])=bK,t(gC[s]|gC)bK,t(gC).
ij ij j j ij j
At the start of an episode, bK,0(gC[s]|gC) is set to be uniform, which means bK,0(gC[s]|gC) = 1. We train the goal-
ij j j ij j j 6
conditionedpolicynetworkωconditionedongC[s].
D.SchellingDiagram
TheSchellingdiagramcomparestherewardsofdifferentpotentialstrategies(i.e.,cooperationanddefectionhere)givena
fixednumberofothercooperators. Itisanaturalgeneralizationofthepayoffmatrixfortwo-playergamestomulti-player
settings. Here,weuseSchellingdiagramstovalidateourtemporalandspatialextensionofthematrix-formgames.
Figure3(a)andFigure3(b)showtheSchellingdiagramsofMSH.Defection(i.e., huntinghare)isasafestrategyasa
reasonablerewardisguaranteedindependentoftheco-players’strategies. Cooperation(i.e.,huntingstag)posestherisk
14EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
ofbeingleftwithnothing(whentherearenoothershuntingstag),butismorerewardingifatleastoneco-playerhunts
stag. Thatistosay,huntinghareisriskdominant,andhuntingstagisrewarddominant. Thisisconsistentwiththedilemma
describedbythematrix-formstag-huntgame(Bloembergenetal.,2011). Inthe“4h1s”setting,whentherearemorethan
twocooperators, thechoicetoactasacooperatorcarriestheriskofnotbeingabletosuccessfullyhunt. Inthe“4h2s”
setting,theincomeofcooperatorsincreaseswiththenumberofcooperators,resultinginalowerriskofchoosingtohunt
stagcomparedtothe“4h1s”setting.
Inthematrix-formsnowdriftgame,cooperationincursacosttothecooperatorandaccruesbenefitstobothplayersregardless
ofwhethertheycooperateornot (Souzaetal.,2009). Therearetwopure-strategyNashequilibria: player1cooperatesand
player2defects;player1defectsandplayer2cooperates. Thatis,thebestresponseisplayingtheoppositestrategyfrom
whatthecoplayeradopts. AsshowninFigure3(c),inMSG,oneagent’soptimalstrategyiscooperation(i.e.,removing
snowdrifts)whennoco-playerscooperate, butwhenthereareothercooperators, theoptimalstrategyisdefection(i.e.,
free-riding). OurMSGisanappropriateextensionofthematrix-formsnowdriftgame.
E.ImplementationDetails
E.1.MCTSSimulationDetails
AsintroducedinSection4.2,werunMCTSforN rounds. Ineachround,werunN searchiterations(seeBrowneetal.
s i
(2012)fordetailsofeachiteration). Thescoreofanactionaatstates˜k is:
(cid:112)(cid:80)
N(s˜k,a′)
Score(s˜k,a)=Q(s˜k,a)+cπ (a|s˜k) a′
θ 1+N(s˜k,a)
where Q(s˜k,a) denotes the average return obtained by selecting action a at state s˜k in the previous search iterations.
N(s˜k,a)representsthenumberoftimesactionahasbeenselectedatstates˜k intheprevioussearchiterations. π (a|s˜k)
θ
referstothepolicyprovidedbythenetworkθ. cistheexplorationcoefficient. Weselecttheactionwhichhasthehighest
scorewhenreachings˜k attheselectionphaseofonesearchiteration.
E.2.NetworkArchitecture
The goal-conditioned policy network ω and the policy-value network for MCTS θ both start with three convolutional
layerswiththekernelsize3andthestridesize1. Threelayershave16,32,and32outputchannels,respectively. Theyare
connectedtotwofullyconnectedlayers. Thefirstlayerhasanoutputofsize512,andthesecondlayergivesthefinaloutput.
E.3.Hyperparameters
ForeachresultinFigure4,Table1,Table4andTable5,weperformed10independentexperimentsusingdifferentrandom
seeds. Theleft-handsideof±representstheaveragerewardofthe10trials,andtheright-handsiderepresentsthestandard
error.
HyperparametersforHOParelistedinTable3(a). αandT aretunedintheadaptationphasetoachievefastadaptation. As
u
αdecreases,agentsattachgreaterimportancetorecentepisodes,whichwillspeeduptheadaptationtonewbehaviorsofthe
co-players. Itisnotadvisabletoadjustαtoosmall,otherwisetheupdatemaybeunstableduetotherandomnessofthe
co-player’sstrategy.
HyperparametersforbaselinesarelistedinTable3. Somehyperparametersaretunedintheadaptationphasetoachievefast
adaptation.
F.SupplementaryResults
F.1.OrcaleAgents
Tocompareandevaluatetheperformanceoffew-shotadaptationbetweenHOPandlearningbaselines,wetrainanOrcale
agenttoseehowwellawell-establishedRLagentcanperforminadaptationtoco-playersthroughextensiveinteractions
Specifically,foreverytypeofco-players,oneOrcaleagentinteractswiththemandistrainedviaA3Ctoconvergefrom
scratch. Duringthetrainingphase,co-players’parametersarefixed,whicharetheconvergentparametersintheirself-play.
15EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
Inthesubsequentadaptationphase,thetrainedOrcaleagentistestedinthesamewayasHOPandbaselinealgorithms.
ThisprocessensuresthattheOrcaleagentengagesinextensiveinteractionswiththeagentsitwouldencounterduringthe
adaptationphase. Overanextendeddurationofinteraction,Orcaleeffectivelyacquiresarobustandhigh-qualitypolicy. We
usetheOrcaleagent’sperformanceintheadaptationphaseasareferencepointtoexplainHOP’sperformance.
F.2.AblationStudy
To test the importance and necessity of each component in HOP, we construct three partially ablated versions of HOP.
Theagentwithoutinter-OM(w/ointer-OM)doesnotexecutetheinter-episodeupdateexpressedasEquation(2). W/o
inter-OMbeginseachepisodewithauniformbeliefprior. Theagentwithoutintra-OM(w/ointra-OM)doesnotexecutethe
intra-episodeupdateexpressedasEquation(1). Thatis,forw/ointra-OM,bK,t(g )=bK,0(g ),∀t. Thedirect-OMagent
ij j ij j
removesthewholeopponentmodelingmoduleofHOP,andutilizesneuralnetworkstomodelco-playersdirectly. The
co-playerpoliciesaremappingsfromstatestoactions,andnotconditionedongoals. ExperimentalresultsforHOPandits
threeablationversionsinMSH-4h2sareshowninTable5.
Inself-play,HOPhaveanadvantageoverdirect-OMagents. Itsuggeststhatutilizingagoalasahigh-levelrepresentation
ofagents’behaviorisbeneficialtoopponentmodelingincomplexenvironments. Ontheotherhand,comparedwithw/o
inter-OMandw/ointra-OM,HOPdoesnotexhibitasignificantadvantageinself-play. Theinter-OMandintra-OMmodules
maynotbeeffectiveintheself-playsetting,wherealargenumberofinteractionshappen.
Intheexperimentstestingfew-shotadaptation,HOPoutperformsitsablationversions. W/ointer-OMagentsstrugglewhen
facingagentswithfixedgoals,suchascooperatorsanddefectors. Asthegoalsofcooperatorsanddefectorsarefixed,correct
actionscanbetakenimmediatelyifthefocalagenthasaccurategoalpriors. W/ointer-OMagentslackaccurategoalpriors
atthebeginningofanepisode. Ineveryepisode,theyhavetousemultipleinteractionstoinferco-players’goalsandthus
missoutonearlyopportunitiestomaximizetheirinterests.
W/ointra-OMagentsexhibitpoorperformancewhenfacingagentswithdynamicbehaviorsuchasLOLA,PS-A3C,and
random. Theseco-playershavemultiplegoals. Butinagivenepisode,thespecificgoalsofaco-playercanbegradually
determinedbyanalyzingitstrajectoryinthisepisode. However,w/ointra-OMagentscanonlycountoninter-OM,which
only takes the past episodes into account, but does not consider the information from the current episode. It results in
inaccurategoalestimatesinagivenepisode,whichhurtstheperformanceinfew-shotadaptation.
Direct-OMagentsareatanoveralldisadvantage. Theiropponentmodelingsolelyreliesontheneuralnetwork,whichmakes
itchallengingtoobtainsignificantupdatesduringashortinteraction. Thisleadstoinaccurateopponentmodelingduringthe
adaptationphase. Furthermore,direct-OMagentsutilizeend-to-endopponentmodeling,whichintroducesahigherdegree
ofuncertaintycomparedtothegoal-conditionedpolicy. Thisuncertaintycanreducetheprecisionofthesimulatedco-player
behaviorduringplanning.
G.EmergenceofSocialIntelligences
Therearetwokindsofsocialintelligence,self-organizedcooperationandtheallianceofthedisadvantaged,emergingfrom
theinteractionbetweenmultipleHOPagentsinMSH.Wemakeaminormodificationtothegame: thegameterminatesonly
whenthetimeT =30runsout.
max
Self-organizedcooperation. AsshowninFigure6(a),atthestartofthegame,threeagents(blue,yellow,andpurple)are
twostepsawayfromthestagatthebottom-rightside,andthelastagent(green)isspawnedaloneintheupperleftcorner.
Onesimplestrategyforthethreeagentslocatedatthebottom-rightcorneristohuntthenearbystagtogether. Although
thisisarisklessstrategy,thethreeagentseachonlyobtainarewardof10/3. Instead,ifoneagentchoosestocollaborate
withthegreenagentatthetop-leftcorner,allfouragentseachgetarewardof5. Thisstrategyisriskiersinceifthegreen
agentchoosestohuntanearbyhare,thecollaborativeagentwillnotbeabletocatchanystag. WeshowthatHOPisableto
achievetheaforementionedriskybutrewardingcollectivestrategy. Specifically,thegreenagentrefusestocatchthehare
athisfeetandshowstheintentionofcooperatingwithothers(seescreenshotsatstep3andstep8inFigure6(a)). The
yellowagentrefusestocatchthestagatthebottom-rightcornerandchoosestocollaboratewiththegreenagenttohunt
thestaginthetop-leftcorner. Inthisprocess,allfouragentsreceivethemaximumprofit. Here,agentsachievepairwise
cooperationthroughindependentdecision-making,withoutcentralizedassignmentofgoals. Thus,wecallthisphenomenon
self-organizedcooperation.
16EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
(a) Self-organizedcooperation
(b) Allianceofthedisadvantaged
Figure6.Screenshotsfortheemergenceof(a)self-organizedcooperationand(b)allianceofthedisadvantaged.Eachpanelshowsagents’
locationsatthecurrentstepandthetrajectoriesbetweenthecurrentstepandthepreviouslystatedstep.
Alliance of the disadvantaged. In addition to the aforementioned game rules, we assume agents are heterogeneous.
Specifically,theyellowagent(Y)isthreetimesgreedierthantheblueagent(B)andthegreenagent(G).Thatis,whenthe
threeagentscooperatetohuntastagsuccessfully,Ywillgetarewardof6,andtheothersget2each. WhenYcooperates
withoneofBandG,Ywillobtain7.5,theotheronegets2.5. AsshowninFigure6(b),atthestartofthegame,Ylocates
betweenBandG.NeitherBnorGwouldliketocooperatewithY.HencetheyneedtomovepastYtocooperatewitheach
other. Toachievethis,agentsBandGfirstmoveclosertoeachotherinthefirstfewsteps. However,tomaximizeitsown
profit,agentYalsomovestowardBandGandhopestohuntastagwiththem. ToavoidcollaborationwithagentY,after
agentsBandGarecloseenoughtoeachother,theymovebackandforthtomisleadY(seestep3ofFigure6(b)). Once
agentYmakesawrongguessofthedirectionsagentsBandGmove,BandGwillgetridofY,andmovetotheneareststag
toachievecooperation(seeStep4and6ofFigure6(b)),whichmaximizestheprofitofagentsBandG.
From the above two cases, we find that although HOP aims to maximize self-interest, cooperation emerges from the
interactionbetweenmultipleHOPagentsinmixed-motiveenvironments. Thisshowsthatitmaybehelpfulinsolving
mixed-motiveenvironmentsbyequippingagentswiththeabilitytoinferothers’goalsandbehaviorandtheabilitytofast
adjusttheirownresponses.
17EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
Table3. Hyperparameters
(a) HOP
self-playphase adaptationphase
MSH MSG MSH MSG
horizonweightα 0.99 0.99 0.95 0.95
rationalitycoefficientβ 2 2 5 5
discountfactorγ 0.95 0.95 0.95 0.95
updateintervalT 2000 2000 200 200
u
capacityofthetrajectorybufferL 5000 5000 5000 5000
numberofMCTSroundsN 8 5 8 5
s
numberofsearchiterationsforeachMCTSN 200 200 200 200
i
explorationcoefficientc 2 12 2 12
learningrate 10−4 10−4 5×10−4 5×10−4
OMlearningrate 5×10−4 5×10−4 5×10−4 5×10−4
(b) A3CandPS-A3C
self-playphase adaptationphase
MSH MSG MSH MSG
learningrate 10−4 10−4 5×10−4 5×10−4
batchsize 2000 2000 200 200
discountfactor 0.99 0.99 0.99 0.99
valuefunctionlosscoefficient 0.5 0.5 0.5 0.5
gradientclip 40 40 40 40
entropycoefficient 0.01 0.01 0.01 0.01
(c) LOLA
self-playphase adaptationphase
MSH MSG MSH MSG
learningrate 10−4 10−4 5×10−4 5×10−4
OMlearningrate 5×10−4 5×10−4 5×10−4 5×10−4
batchsize 2000 2000 200 200
discountfactor 0.99 0.99 0.99 0.99
(d) SocialInfluence
self-playphase adaptationphase
MSH MSG MSH MSG
learningrate 10−4 10−4 5×10−4 5×10−4
batchsize 2000 2000 200 200
Influenceweight 1.0 1.0 1.0 1.0
MOAlossweight 3.0 3.0 10.0 10.0
entropycoefficient 0.01 0.01 0.01 0.01
(e) PR2
self-playphase adaptationphase
MSH MSG MSH MSG
learningrate 10−4 10−4 5×10−4 5×10−4
batchsize 2000 2000 200 200
softupdateparameter 0.99 0.99 0.99 0.99
18EfficientAdaptationinMixed-MotiveEnvironmentsviaHierarchicalOpponentModelingandPlanning
Table4.Few-shotadaptationperformanceofOrcaleinallthreesequentialsocialdilemmaparadigms.Theinteractionhappensbetween1
Orcaleagentand3co-playersusingthecolumnpolicy.ShownistheaveragerewardforOrcalefrom1800to2400step.
learningco-players rule-basedco-players
LOLA A3C PS-A3C random cooperator defector
MSH-4h1s 2.44±0.03 0.88±0.01 3.57±0.03 1.10±0.00 2.73±0.02 0.93±0.01
MSH-4h2s 3.23±0.02 3.46±0.01 3.97±0.02 1.22±0.01 3.42±0.02 0.70±0.01
MSG 20.9±0.12 22.7±0.17 32.5±0.12 16.0±0.08 36.0±0.00 12.0±0.00
Table5.PerformanceofHOPanditsablationversionsinMSH-4h2s.In(a)self-play,4agentsofthesamekindaretrainedtoconverge.
Shownisthenormalizedscoreafterconvergence.In(b)few-shotadaptation,theinteractionhappensbetween1agentusingtherowpolicy
and3co-playersusingthecolumnpolicy.Shownarethemin-maxnormalizedscores,withnormalizationboundssetbytherewardsof
Orcaleandtherandompolicy.Theresultsaredepictedfortherowpolicyfrom1800to2400step.
(a) Self-playperformance
HOP w/ointer-OM w/ointra-OM direct-OM
0.9767±0.0117 0.9708±0.0146 0.9738±0.0117 0.9417±0.0146
(b) Few-shotadaptationperformance
learningco-players rule-basedco-players
LOLA A3C PS-A3C random cooperator defector
HOP 0.97±0.02 0.99±0.02 0.88±0.02 0.78±0.07 1.00±0.01 0.36±0.02
w/ointer-OM 0.97±0.02 0.92±0.03 0.87±0.02 0.78±0.03 0.96±0.02 0.31±0.02
w/ointra-OM 0.95±0.02 0.98±0.02 0.84±0.01 0.65±0.04 0.99±0.02 0.34±0.03
direct-OM 0.95±0.01 0.85±0.02 0.74±0.03 0.62±0.04 0.96±0.02 0.31±0.02
19