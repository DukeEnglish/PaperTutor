Improving Autoregressive Training with Dynamic Oracles
JianingYang♡♣∗ HarshineVisvanathan♣
YilinWang♢♣∗ XinyiHu♣ MatthewR.Gormley♣
♡UniversityofMichigan ♣CarnegieMellonUniversity ♢HarvardUniversity
jianingy@umich.edu
Abstract
Exposure Bias
Suboptimal Training h
M
q qu
ua
e
en
n
ny
t ci
eata
l
ts adk ges
gc
iw
i
nsii goth
n
ti on
p
trN
eo
xL
b
tlP
e gm
ec na sn
e,
rrb
aa
te
n
iogf nr ia
n
.m
g
He
f
ord
o
wa
m
es vs
s
eee r--
,
Metric Mismatch
(a)
osureBias
ricMismatc
formanytasks,thestandardtrainingmethods,
Method Prefix Supervision
Exp Met
including maximum likelihood (teacher forc-
TeacherForcing Gold Gold
ing) and scheduled sampling, suffer from ex-
ScheduledSampling Gold+Predicted Gold
posurebias(Ranzatoetal.,2016a)andamis-
DAgger Gold+Predicted DynamicOracle
matchbetweenmetricsemployedduringtrain-
(b)
ingandinference. DAgger(Rossetal.,2011)
providesasolutiontomitigatetheseproblems, Figure1: (a)Illustrationofissuesencounteredinstan-
yet it requires a metric-specific dynamic ora- dardsequencetrainingandtheconsequence. (b)Prob-
clealgorithm, whichdoesnotexistformany lems faced by Teacher Forcing, Scheduled Sampling
commonmetricslikespan-basedF1,ROUGE, andDAgger.
and BLEU. In this paper, we develop these
noveldynamicoraclesandshowtheymaintain forcontextandinsteadhastodependonitspreced-
DAgger’s no-regret guarantee for decompos- ingpredictions,whichmightbeflawed. Because
ablemetricslikespan-basedF1. Weevaluate
the decoder is not exposed to such flaws during
thealgorithm’sperformanceonnamedentity
training,itmaystruggletorectifysucherrorsdur-
recognition (NER), text summarization, and
ing inference. Second, as highlighted in prior re-
machinetranslation(MT).WhileDAggerwith
search(Ranzatoetal.,2016b;WisemanandRush,
dynamicoracleyieldslessfavorableresultsin
ourMTexperiments,itoutperformsthebase- 2016),there’sadiscrepancybetweenthetraining
linetechniquesinNERandtextsummarization. loss, encourages the model to align outputs with
thegroundtruth,andtheinferencemetric,which
1 Introduction couldbebasedonanon-differentiablemetric,like
span-basedF1,ROUGE,BLEU,amongothers.
NumerousNLPtasks,rangingfromsequencetag-
Scheduledsampling(Bengioetal.,2015)aims
gingtotextgeneration, canbeframedassequen-
to mitigate exposure bias by employing a prefix
tiallearningproblems. Frequently,algorithmstai-
thatissampledfromaninterleavingoftheground
loredforsuchsequencelearningemployanautore-
truthandmodel-generatedoutput. However,sched-
gressive decoder. where the decoder recurrently
uled sampling still relies on the ground truth as
uses its own output sequence as a prefix to de-
output supervision, even when the input prefix is
code the next token. In the conventional training
nolongerthegroundtruth. Thisapproachcanlead
paradigmknownasteacherforcing,thedecoderuti-
to suboptimal supervision during training, as de-
lizesthegroundtruthsequenceastheleftcontext
picted in Figure 2. For instance, when the word
for predicting the next token, as opposed to us-
“the”isomittedfromtheleftcontext,usingthegold
ingitsownpreviousoutputascontext—maximum
sequence as supervision may not be the most ap-
likelihood(MLE)trainingisteacherforcingwith
propriatechoice. Wearguethatincaseswherethe
cross-entropyloss. Teacherforcingfacestwokey
leftcontextisdynamicallygenerated,it’simpera-
challenges. The first is exposure bias: during in-
tivetoemployadynamicallygeneratedoraclefor
ference,themodelisdeprivedofthegroundtruth
supervisionratherthanrelyingonthestaticground
∗WorkdoneatCMU. truthsequence.
4202
nuJ
31
]LC.sc[
1v39390.6042:viXraGold so we should submit the conference paper by midnight
!
Supervision VD any in lla am Si cc
h
O edra uc ll ee
d Sampling
ss oo ww ee ss hh oo uu ll dd ss uu bb mm ii tt tt hh ee conp fa ep re er
nce
pab py
er
mid bn yight
midnight
B BL LE EU U== 33 67 .. 97
Which supervision
is better?
so we should submit conference paper by ???
Model
Prediction
Encoder Decoder
<bos> so we should submit the conference paper
Scheduled Sampling
Input Input Prefix <bos> so we should submit conference paper by
Head = Use Ground Truth
Head Tail Head Tail Head Tail Tail Tail
Tail = Use Model Prediction
Figure2: Dynamicoracleproducesbettersupervisionthanvanillascheduledsamplingfortrainingautoregressive
decoder. Intheaboveexample,whenusingscheduledsamplingtotrainadecoder,theword“the"wasnotpredicted
by the decoder in the prefix sequence. In this case, what supervision should one use for the red box? Vanilla
scheduledsamplingusesthesupervision“soweshouldsubmitconferencepaperbybymidnight"(notice“by"
appearstwice)whichleadstoaBLEUof36.9;whereasdynamicoracleusesthesupervision“soweshouldsubmit
conferencepaperbymidnight",whichleadstoaBLEUof37.7. Thedynamicoraclegivesbettersupervision.
Detailedexplanationonscheduledsampling: ateachautoregressivedecodingstep,oneflipsacointodecideifa
ground-truthtokenoramodel-predictedtokenshouldbeusedastheprefix. Asaresult,thetoken“the"happensto
benotintheinputprefix.
To tackle these challenges, we propose to use rizationwithROUGE(§3.3). OurMTexperi-
DAgger (Ross et al., 2011) in conjunction with mentswithBLEUindicatethatDAggermay
noveldynamicoracles(cf. §2.1and§2.2). These notconsistentlyoutperformrobustbaselines.
dynamic oracles are tailored to the specific test- However,ourresultsonpartialmatchF1for
time evaluation metric, and they serve as the op- NERandROUGEforsummarization,demon-
timal form of supervision, guiding the model in strate the superiority of DAgger with our
respondingtoerrorsmadeduringdecoding. How- dynamic oracles, which surpass commonly
ever,forseverallossfunctions,noknowndynamic usedtrainingmethodsliketeacherforcingand
oracle exists, including examples such as span- scheduledsampling.
based F1, ROUGE, and BLEU. In this study, we
2 Methods
aimtobridgethisgap. Ourmaincontributionsare:
We begin with an overview of the DAgger algo-
• We propose an algorithm to compute the ex-
rithmandtheroleofdynamicoraclesbeforepre-
actdynamicoraclesfordecomposablemetrics
sentingournovelexactdynamicoraclesforexact
suchasvariantsofspan-basedF1(§2.3). DAg-
matchF1/partialmatchF1andournewapproxi-
gerwiththisexactdynamicoracleprovidesa
matedynamicoraclesforROUGE/BLEU.
no-regretguarantee.
2.1 DAgger
• Fornon-decomposablemetricssuchasBLEU
(Papinenietal.,2002)orROUGE(Lin,2004), The DAgger algorithm (Ross et al., 2011), like
we propose an algorithm to compute the ap- otherimitationlearningalgorithms,guidesamodel
proximatedynamicoracle(§2.4). policy to be more like an expert policy. Applied
tosequencegeneration,themodelpolicyissimply
• We evaluate the proposed algorithms on thegreedydecoderforanautoregressivemodelthat
named entity recognition (NER) using the selectsthemostproprobablenexttokenyˆ ateach
t
partial-matchF1metric(§3.1),machinetrans- step to yield a sequence yˆ. At each time step t,
lation (MT) with BLEU (§3.2), and summa- theexpertpolicy(alsocalledadynamicoracle)ex-aminesboththegroundtruthsequencey∗ andthe Algorithm1:DynamicOracleforPartial
currentpartialoutputofthemodelyˆ 1:t−1 toreturn F1Score
thecompletionoftheoutputy˜ t:T thatoptimizesa Input :prev_gold_tag:previousgoldtag,
taskspecificmetric. Duringtraining,DAggerfeeds curr_gold_tag:currentgoldtag
prev_tag:previouspredictedsingletag
in the model’s own predictions yˆ to the decoder,
Output:nextbesttoken
whichmayhaveadifferentlengththantheground 1 ifstart(curr_gold_tag)=‘B’then
truth. TheDAggerlossfunction,ifbasedoncross- 2 ifstart(prev_tag)=‘B’andtype(curr_gold_tag)
=type(prev_tag)then
entropy, then at each time step t maximizes the
3 return‘I-’+type(curr_gold_tag)
likelihoodofthefirsttokeny˜ t fromtheexpertpol- 4 else
icy’s completion. When DAgger is implemented 5 returncurr_gold_tag
withanSGD-likeoptimizationalgorithm,wesim- 6 elseifstart(curr_gold_tag)=‘I’then
7 ifprev_tagis‘O’then
plysumtheselikelihoodsoveralltimestepstoget
8 return‘B-’+type(curr_gold_tag)
thelossforthatexample. Thisdiffersfromteacher 9 elseiftype(prev_tag)̸=type(prev_gold_tag)
forcing, which takes the ground truth y∗ as both then
10 return‘O’
input to the decoder and as supervision. DAgger
11 else
differsfromscheduledsampling,whichconstructs 12 returncurr_gold_tag
asequencewhoselengthisthesameastheground 13 else
14 return‘O’
truthbyrandomlyinterjectingsomemodelpredic-
15 Note:
tiontokensyˆ insteadofthegroundtruthtokeny∗
t t • start(tag)returnstheprefixofthetag,i.e,B,I,orO.
andthenusesthegroundtruthsequencey∗ assu-
• type(tag)returnstheentitytype,i.e,PER,LOC,etc.
pervision. DAggerisdesignedtoprovideno-regret
guarantees,ensuringitminimizestheknowledgeof
opportunitiesforsuperioractionsthatmighthave
beenmissed. However,itrequiresadynamicoracle
additively; e.g. BLEU or ROUGE both involve
customizedtospecificevaluationmetrics
n-gramcounts,butalsoglobalfeatures(precision,
brevitypenalty,etc.) thatpreventdecomposition.
2.2 DynamicOracles
Fordecomposablemetrics,wedefinetheexact
The term dynamic oracle, originally introduced
dynamic oracle to be the completion that gives
in the context of dependency parsing (Goldberg
thehighestpossiblescore(evaluatedbasedonthe
andNivre,2013),representstheexpertpolicyem-
metric) given a partial sequence. However, for
ployedbyDAgger. Thisdynamicoraclefunction
non-decomposable metrics, such computation is
acts as a guide to answer the question: Given a
usuallycomputationallyintractable;therefore,we
partial output sequence, which completion mini-
propose to use an approximate dynamic oracle,
mizesthelosswhencomparedtothegoldstandard
which is computed by approximation algorithm
output? It’s important to note that dynamic ora-
such as beam search. In the sequel, we give an
cles are inherently metric-dependent. In the con-
exactdynamicoraclealgorithmforspan-basedF1
text of sequence evaluation metrics, these can be
(forchunking/NER)andanapproximatedynamic
broadly categorized into decomposable and non-
oraclealgorithmforROUGE(forsummarization)
decomposable. Adecomposablemetriccanbeex-
andBLEU(formachinetranslation).
pressedinthefollowingform(Meisteretal.,2020):
2.3 ExactandPartialF1
(cid:88)Nx
score(x,y) = score(x ◦x ,y) (1) Earlyworkinchunking(CoNLL-2000sharedtask
1:t−1 t
t=1 (TjongKimSangandBuchholz,2000))andNER
(CoNLL-2003 shared task (Tjong Kim Sang and
Here y is the ground truth sequence, x is the se- DeMeulder,2003a))establishedexact-matchF1as
quence to be evaluated and N is its length. De- theprimarymetric,computingtheharmonicmean
x
composablemetrics,suchasworderrorrate(WER) of precision and recall based on exact spans in
andspan-basedF1,canbebrokendownintoaddi- referenceandsystemoutput. Forasinglesentence,
tivecomponentsbasedsolelyonindividualtokens this metric can be considered as a decomposable
(or n-grams) and the preceding partial sequence. metric, with an exact dynamic oracle algorithm.
Non-decomposablemetricscannotbebrokendown We introduce a dynamic oracle algorithm for theexactF1scoregiveninAppendixAlgorithm3. Algorithm2:ApproximateDynamicOra-
A limitation of the exact match F1 metric lies cleforROUGE/BLEUwithBeamSearch
in its failure to consider type mismatch and par- Input :x :inputprefixsequenceuptoi−1,
1:i−1
y:goldsequence,
tialmatches. Moreover,priorresearch(Manning,
score(seq1,seq2):scoringfunction
2006)suggeststhatoptimizingforexact-matchF1 Output:yD.O.:nextworddynamicoracle
i
scoresmaynotbesuitableforNERtasks,asitcan 1 Initializecandidatestobeasetofunigramsiny
leadtoasystem’sreluctancetorecognizeentities, 2 Initializequeuetobeapriorityqueue
3 Insertx 1:i−1withscore=−∞intoqueue
resultinginaprevalenceof’O’tags. Consequently, 4 forl=1...beam_lengthdo
wedevelopdynamicoraclestailoredforthepartial 5 forprefix∈queuedo
F1metric,asdefinedinMUC(ChinchorandSund- 6 forword∈candidatesdo
7 new_seq←prefixconcatenated
heim, 1993). Algorithm 1 presents our dynamic byword
oracleforpartial-matchF1. Liketheexact-match 8 s←score(new_seq,y)
9 Insertnew_seqwithscore=sinto
F1,partialF1isdecomposable,makingitsdynamic
queue
oraclealgorithmexact. Aformalproofofthealgo-
10 Truncatequeuebyonlykeepingthe
rithm’scorrectnessisavailableinAppendixA.1. top-beam_sizescoringsequences
This algorithm is designed to predict the next 11 y iD.O. ←selectthetop-1scoringsequencefrom
optimaltokeninNERtasks. Itssimplicityderives queueandgetthei-thword
from the nature of the count/divide approach uti- 12 returny iD.O.
lized by the metric. This algorithm incorporates
various conditions that account for each conceiv-
sequence is computationally prohibitive. Conse-
able output. We use an example to illustrate the
quently, we employ beam search as an approx-
rationalebehindthisalgorithm. Ifthetagginghas
imation method to search for the dynamic ora-
alreadybegun,withthesametype,i.e.,
cle in the context of ROUGE. In Algorithm 2,
we present an approximate dynamic oracle algo-
prev_gold_tag = ‘O’
rithmforROUGE,wheretheoptimalcompletion
curr_gold_tag = ‘B-LOC’
isdeterminedbycomputingtheROUGEscorefor
prev_tag = ‘B-LOC’ beam_size potential completions, each up to a
lengthofbeam_length,andselectingthecom-
Teacher forcing suggests the optimal subsequent pletionwiththehighestscore.
tag would be B-LOC, which would result in a The BLEU score (Papineni et al., 2002), is a
falsepositiveerror. TheDynamicOraclesuggests widely-used metric in machine translation tasks.
I-LOC,whichistheidealsucceedingcompletion Like ROUGE, BLEU relies on global n-gram
toattainafavorablepartialmatchF1score. counts, making it a non-decomposable metric.
The application of exact dynamic oracles isn’t Thus,weemploythesameapproximatedynamic
exclusive to the NER task or the partial F1 met- oraclealgorithmoutlinedinAlgorithm2thatuti-
ric. Anothermetricthatcanbenefitfromsuchan lizesBLEUasthescoringfunction. Inourstudy,
approach is the word error rate (WER), which is weconsiderBLEU-4,encompassingupto4-grams.
prevalentlyusedinautomaticspeechrecognition
3 Experiments
(ASR)tasks. Forcompleteness,wepresentthedy-
namicoraclealgorithmandproofofcorrectnessin
Togaugetheeffectivenessofourapproach,wecon-
AppendixB.
ductedexperimentsonthreesequence-to-sequence
(seq2seq)modelingproblems,includingnameden-
2.4 ROUGEandBLEU
tityrecognition(NER),machinetranslation(MT),
The ROUGE score (Lin, 2004) is a widely andsummarization.
adoptedmetricforevaluatingsummarizationtasks.
3.1 F1/NamedEntityRecognition
ROUGE measures both n-gram precision and re-
call, culminating in the harmonic mean of these Dataset We used three NER benchmarks. The
twoscores. However,itisnotdecomposabledue CoNLL-2003SharedTask(TjongKimSangand
totheglobalnatureofn-gramcountsusedinpre- De Meulder, 2003b) comprises formal writings
cisionandrecallcalculations. Identifyingtheexact sourcedfromtheReutersnewsarticlecorpus;we
completion that maximizes ROUGE for a partial evaluateonEnglishandGerman. TheWNUT2017Dataset+Model TrainingMethod F1 p(%) quent epoch. For the FLAIR experiments, we re-
CoNLL- TeacherForcing 86.55 trievetheembeddings,whichincludeGLoVeWord
03(De)+ ScheduledSampling 86.72 Embeddings(Penningtonetal.,2014), Character
BERT DAgger 87.02
Embeddings,ForwardPooledFLAIRembeddings,
TeacherForcing 89.89 andBackwardPooledFLAIRembeddings(Akbik
CoNLL-
03(En)+ ScheduledSampling 89.98 etal.,2018),fromtheFLAIRlibrary(Akbiketal.,
BERT DAgger 90.21 2019a). TheRNN-LMdecoderconsistsofasingle-
TeacherForcing 91.50 layerLSTMwith50hiddenunits.
CoNLL-
03(En)+ ScheduledSampling 92.72
FLAIR DAgger 93.08 Results TheresultsareinTable1. ForWNUT-
17 with the FLAIR mdoel, scheduled sampling
TeacherForcing 47.52
WNUT-17+ leadstoamarkedimprovementoverteacherforc-
ScheduledSampling 48.91
FLAIR ing(+1.39F1 ). However,theseq2seqmodelwith
DAgger 49.40 p
the DAgger (dynamic oracle) training achieves
Table1:PartialmatchF1(F1 )resultsonNERdatasets. a further improvement over scheduled sampling
p
DAggerwithexactdynamicoracleconsistentlyoutper- (+0.49F1 )andacorrespondinglygreatergapover
p
formsteacherforcingandscheduledsamplingacross
teacher forcing (+1.88 F1 ). Similar, albeit more
p
datasetsandbackbonemodels.
modest,trendsareobservedontheCoNLL-2003
SharedTask(Derczynskietal.,2017)focuseson benchmarkwithboththeFLAIRandBERTmodels
extractingemergingnamedentitiesfromnoisyuser- onbothEnglishandGerman. Forexample,DAgger
generatedtweets. (dynamicoracle)trainingwiththeFLAIRmodel
outperformsbothscheduledsampling(+0.36F1 )
ExperimentDetails Wefollowpriorwork(Shen p
andteacherforcing(+1.58F1 ).
et al., 2017) in formulating the NER task as a p
greedydecodingproblem.1 Weevaluatedourpro-
3.2 BLEU/MachineTranslation
posedlearningmethodontwomodels:
Dataset TheIWSLT’142 Sl-Endatasetcontains
BERT+RNN-LM Our first model uses BERT- 17,815parallelSlovenian-Englishsentences. For
base(Devlinetal.,2019)encoderfollowedby ourmachinetranslationexperiments,weoptedfor
anRNN-LMdecoder(Shenetal.,2017). thisrelativelysmalldatasettoenabletheexecution
ofagreaternumberofexperimentsandfacilitate
FLAIR+RNN-LM Our second model uses
morecomprehensiveanalysis.
pooled FLAIR embeddings (Akbik et al.,
2019b) fine-tuned in the encoder, followed
ExperimentDetails Inadditiontoteacherforc-
byanRNN-LMdecoder(Shenetal.,2017).
ing and scheduled sampling, we consider the fol-
lowingbaselinemethods:
We conducted a comparison of three training ap-
proachesfortheseseq2seqmodels,whichincluded
MinimumRisk Shen et al. (2016) trains the
TeacherForcing,ScheduledSampling,andDAgger
modeltominimizeempirical“risk”,defined
withanexactdynamicoracleforpartial-matchF1.
astheweightedsumoftheBLEUscorefora
Wechoseaninitiallearningrateof1.0andbatch
batchofpotentialtranslations. Eachscoreis
sizeof16fortheCoNLL-2003datasetandanini-
weightedbytheprobabilityofthecorrespond-
tiallearningrateof0.5andbatchsizeof32forthe
ingsampleaspredictedbythemodel.
WNUT-2017dataset. Thesechoicesarebasedon
hyper-parametersearch,withpotentialbatchsize
Word-LevelOracle Proposed by Zhang et al.
(8,16,32)andlearningrate(0.1,0.5,1.0).
(2019),thismethodinvolvessamplingcontext
ForscheduledsamplingandDAgger,wewarm-
words from both the ground truth sequence
start the model with several epochs of teacher
andthesequencepredictedbythemodeldur-
forcing, then switch to scheduled sampling on
ing training. It also added Gumbel noise
the third epoch, with 20% of the predicted out-
(Gumbel,1954)tothemodels’predictionto
put being fed back into the model every subse-
enhancerobustness,whichdiversifiesprefixes
1Thisisincontrasttomanypriorapproacheswhichusea feedintothemodel.
linear-chainCRFlayerandfindthemostprobablesequence
throughdynamicprogramming. 2https://workshop2014.iwslt.org/TrainingMethod BLEU
45%
Epoch167(baseline) 11.67
40%
TeacherForcing 11.28
35%
MinimumRisk 11.21
30%
Word-LevelOracle 12.08
25%
MIXER 12.04
20%
ScheduledSampling 11.25
15%
DAgger+GreedySearch 11.44 Epoch 2
DAgger+BeamSearch,size=3 11.16 10% Epoch 5
5% Epoch 10
DAgger+BeamSearch,size=5 11.24
Epoch 21
0%
beam=5 beam=20 beam=90 beam=1000
Table2: BLEUresultsonIWSLT’14Sl-En. Sizein-
dicates beam_size used in Algorithm 2. All methods (a)
excepttheword-leveloracleandMIXERfailtooutper- GroundTruth DynamicOracle(∆BLEU)
Epoch
formthebaseline. (BLEU) 5 20 90 1000
2 72.10 +2.98 +4.84 +4.97 +5.04
MIXER Ranzato et al. (2016a) employs any re- 5 74.66 +2.33 +4.19 +4.3 +4.37
wardfunctiontodirectlyoptimizetheevalua- 10 78.52 +0.24 +0.89 +1.23 +1.31
tionmetricsusedattesttime. 21 86.42 +0.11 +0.38 +0.52 +0.55
(b)
For each method, we used a standard encoder-
Figure3: LargerbeamsizesandinitiatingDAggertrain-
decoderTransformermodel(Vaswanietal.,2017)
ingearlierresultsinbetterdynamicoraclequality. (a)
andtraineditfromscratch. Theverticalaxisindicatesthepercentageofinstances
In this task, we aim to investigate the impact wherethedynamicoracle’scompletionyieldsahigher
of initiating sampling during the later stages of BLEU score than that of the ground truth. A higher
training. As proposed by Ranzato et al. (2016b), percentageimpliesgreaterbenefitsfromtransitioning
fromteacherforcingtothedynamicoracle. Importantly,
introducingdiversityintotheprefixduringthelater
bydesign,thedynamicoracle’sBLEUcannotbelower
trainingphasesmayexpeditethemodel’ssearchfor
thanthegroundtruth–italwaysselectsthebetterword
theoptimalpolicy. Inthisexperiment,wefirsttrain
between the ground truth and the beam search result.
the model using teacher forcing for 167 epochs. (b) Comparison of average BLEU scores for ground
Wethentrainthemodelusingeachaforementioned truthanddynamicoraclesupervisions,averagedover
technique for 5 epochs. We chose epoch 167 be- anentirebatch. Beamsizesof5,20,90,and1000are
causeitproducedthehighestBLEUscore(11.67) employedforthedynamicoracle.
This analysis offers two insights: (1) Starting
onthevalidationsetamongtheinitial300epochs
DAggerinearlyepochsgivesbetterdynamicora-
ofteacher-forcingtraining.
cles(intermsoftheabovetwometrics). (2)using
larger beam sizes gives better dynamic oracles.
Results As shown in table 2, DAgger with dy-
However,it’sworthnotingthatthisimprovement
namicoracleseemslessfavorablecomparedtothe
beginstoplateaubeyondabeamsizeof20.
baselineandothermethods. Itisnoteworthythat
other methods also have no or very marginal im- These insights partially explain the unsatisfac-
provementcomparedtothebaseline. Itispossible tory performance of DAgger when starting from
thatBLEUhassaturatedatthispoint. epoch168withabeamsizeof3or5. Insight(1)
isevidencedbycomparingthetwometricswhile
Analysis WepresentfurtheranalysisinFigure3 varying the starting epoch but keeping the beam
tostudyifthisphenomenonisrelatedtoourdeci- sizefixed—boththefrequencyofBLEUimprove-
siontoinitiateDAggerfromarelativelylateepoch. ment and the magnitude of BLEU improvement
Specifically,weseektoanswer: givenaprefix,how remain relatively low when starting from a later
doestheBLEUscoreofasentence,completedby epoch. Insight (2) is evidenced by keeping the
thedynamicoracle,comparewithonethat’scom- startingepochconstantandvaryingthebeamsize,
pleted by copying the ground truth starting from where the general trend reveals that larger beam
thesameprefixposition? Weconsidertwokeymet- sizesaremorebeneficial,thoughtheimprovements
rics: (1)Howmanytimesdoesdynamicoraclegive begin to diminish beyond a beam size of 20. It
higher BLEU than ground truth? (2) how much maybethatcarefullytuningtheuseoflargerbeam
better, in terms of average BLEU, does dynamic sizesandstartingDAggertraininginearlierepochs
oracleachieve,comparedtogroundtruth? couldyieldbetterresults.TrainingMethod ROUGE-1 ROUGE-2 ROUGE-L (+0.51 R-1, +0.77 R-2, +0.43 R-L) and teacher
TeacherForcing 41.30 19.59 37.86
forcing(+1.32R-1,+1.20R-2,+0.78R-L).
ScheduledSampling 42.11 20.02 38.21
DAgger+GreedySearch 42.35 20.49 38.3
DAgger+BeamSearch2 42.55 20.72 38.53 4 RelatedWork
DAgger+BeamSearch3 42.49 20.65 38.59
DAgger+BeamSearch5 42.62 20.79 38.64
4.1 DynamicOracleforOtherMetrics
Table3: DAggerwithdynamicoracletrainingimproves Dependency parsing has a rich history of dy-
ROUGEresultsonCNN/DailyMailsummarizationtask.
namic oracle training with algorithms that pre-
The dynamic oracles optimize the ROUGE-2 metric.
dateareanalogoustoDAgger(Rossetal.,2011).
Thenumberfollowing“beamsearch"isthebeamsize.
Transition-based,deterministic,tabularparserwith
DAgger with beam search size 5 consistently outper-
formsteacherforcingandscheduledsamplingacross non-deterministic dynamic oracle has been stud-
allROUGEmetrics. Weranapairedpermutationtest ied on the labeled attachment score (LAS) and
ontheresultsbetweenDAggerandscheduledsampling the unlabeled attachment score (UAS) (Gómez-
ontheROUGE-2metric. Thep-valueis0.055,which Rodrıguez et al., 2014; Gómez-Rodrıguez and
suggestsstatisticalsignificance.
Fernández-González). Dynamic oracles are also
3.3 ROUGE/Summarization used for projective and non-projective parsing
(Gómez-Rodrıguezetal.,2014;Gómez-Rodrıguez
Dataset For summarization, we use the
andFernández-González). Apartfromdependency
CNN/Daily-Mail (CNNDM) (Hermann et al.,
parsing, dynamic oracles are also designed for
2015; Nallapati et al., 2016) benchmark, which
Word Error Rate (WER) and constituency pars-
containsnewsarticlesandassociatedhighlightsas
ing (Sabour et al., 2019; Fried and Klein, 2018).
summaries.
Yet,thislineofworkdoesnotofferasolutionfor
non-decomposablemetrics.
ExperimentDetails Thebaselinesystemforour
These dynamic oracles can also be used in a
experiments is BART (Lewis et al., 2019), an
morerecentvariantofDAggerthattakesthebeam-
encoder-decodermodelconsistingofaBERT-style
searchapproximationintoaccount(Negrinhoetal.,
encoderfollowedbyapre-traineddecoder(GPT-2).
2018, 2020) and takes interactive no-regret into
Weusefairseq’sBART(Wangetal.,2019).
account(RossandBagnell,2014).
WecomparethreemethodsfortrainingBART:
Teacher Forcing, Scheduled Sampling, and DAg-
4.2 MethodsBasedonScheduledSampling
ger with our approximate dynamic oracle for
Scheduled sampling has been improved in vari-
ROUGE. We experimented using two configura-
ous ways. There exist differentiable versions of
tions of the approximate dynamic oracle algo-
scheduledsampling(Goyaletal.,2017;Xuetal.,
rithm. Thefirst,referredtoas"greedysearch,"sets
the beam_length parameter to 1. The second, 2019)andfasterversionsachievedviaparalleliza-
knownas“beamsearch,"setsthebeam_length tion(Duckworthetal.,2019;MihaylovaandMar-
tins,2019). Itcouldalsobeextendedtoallowmore
to2,3,and5.
diverseprefixesbyaddingnoisetomodels’predic-
For scheduled sampling and DAgger, we start
tions(Zhangetal.,2019)
thealgorithmatepoch2. Duringthisprocess,the
Morerecently,confidence-awarescheduledsam-
decoder’soutputisincorporatedbackintothede-
plingusesthemodel’spredictionsassupervision
coder as a prefix 25% of the time. We run all
where it’s confident and the ground truth as su-
experimentsfor15epochs.
pervision otherwise (Liu et al., 2021a). Studies
Results From the results in the table 3, it’s evi- findthatincreasingtheprobabilityofchoosingthe
dentthatthemodeltrainedwithDAgger(Dynamic model’s prediction as supervision in later decod-
Oracle)surpassesthosetrainedwithteacherforc- ingstepsoutperformsvanillascheduledsampling
ing and scheduled sampling for both the greedy (Liuetal.,2021b). Furthermore,wecanrandomly
searchandbeamsearchapproximatedoracles. The maskoutsometokensduringdecodingandusean
best performance is observed when employing a auxiliary task to recover the masked-out tokens,
beamsearchapproximationwithabeamsizeof5in whichyieldsperformancegains(Liuetal.,2021c).
conjunctionwiththeDAggeralgorithm,resulting Elasticweightconsolidationcanalsobeusedtoen-
improvementsacrossROUGE-1,ROUGE-2,and hancethemodel’srelianceonitspreviousoutputs
ROUGE-L scores over both scheduled sampling duringdecoding(KorakakisandVlachos,2022)4.3 ImitationLearning themethod.
SEARNcastslearningasreinforcementlearning, Runtime Thebeamsearchfordynamiccomple-
using the model’s own prediction to generate ex- tioncanbeoptimizedbasedonthespecificmetric.
amplesandthensearchingovertheactionspaceto Forexample,forROUGEandBLEU,wecancache
compute each action’s cost (Daumé et al., 2009). then-gramcountsasweexpandthebeamsearch.
TheLOLSandMIXERalgorithmsimproveupon We could also seek to improve searching for ap-
SEARN(Changetal.,2015;Ranzatoetal.,2016b). proximatedynamicoraclesbydrawingideasfrom
MIXERadditionallyallowedmixedgroundtruth existingworks(Dreyeretal.,2007;Sokolovetal.,
andmodelpredictiontobeusedastheprefix,which 2012)thatuseotherapproximationsinsearching
isparticularlyrelevanttothisstudy. foroptimalcompletionofBLEU.
4.4 Global-AwareTraining StochasticDynamicOracles Incertainscenar-
ios, it’s possible to encounter multiple dynamic
Anissuewithautoregressivegenerationinseq2seq
oracleswithequivalentscores. Insuchcases,one
modelsistheunderutilizationoftarget-sidefuture
approachcouldinvolverandomlyselectingacom-
informationduringtraining. Toaddressthis,Lyu
pletion from this set of equally-scored oracles to
etal.(2023)suggestlearningthedistributionoffu-
serveasthesupervisionfortraining. Thisdataaug-
turetranslationsateachdecodingstep. Theglobal-
mentationstrategyhasthepotentialtostimulatethe
awarebeamsearchalgorithmtakesglobalattention
modeltoproducemorediverseoutputs,fostering
distribution into account. The seer-forcing algo-
improvedgeneralization.
rithm employs an auxiliary Seer module, which
captures future target information and guides the Other Metrics and Models While we focused
decoderduringteacherforcing. (Fengetal.,2021) on conventional metrics, our proposed algorithm
could be extended to encompass more sophisti-
4.5 AlternativeLossObjectives
cated metrics, including model-based ones like
BeyondMLE,thereexistalternativelossfunctions BERTScore (Zhang et al., 2020). Furthermore,
to tackle exposure bias. For instance, minimum- althoughweprimarilyemployedencoder-decoder
risk training samples a batch of potential transla- modelsinthisstudy, ourproposedalgorithmcan
tions,andformulatesthelossasasthesumofeval- seamlesslybeappliedtodecoder-onlymodelswith-
uationmetrics(e.g.,BLEUscore)foreachsample outanymodifications.
translation,weightedbythemodel-assignedproba-
bilities(Shenetal.,2016). Anotherstudyemploys 6 Conclusion
beamsearchduringtraininganddefinesthelossas
In this work, we identified deficiencies of com-
thefrequencywithwhichthegoldsequencefalls
monlyusedtrainingtechniquesforsequencetrain-
offthebeam(WisemanandRush,2016). Further-
ing,includingteacherforcingandscheduledsam-
more,themixedcross-entropylossallowseachto-
pling, and proposed to use DAgger training with
kentohavemultiplesourcesofsupervision,which
loss-specificdynamicoracles. Wedesignednovel
canincludethegroundtruthorthemodel’sprevi-
dynamic oracle algorithms as a demonstration,
ousoutput(LiandLu,2021). Toenhancerobust-
exact dynamic oracles for decomposable metrics
ness,wecouldperturbtheinputandregularizethe
likeexact-matchF1andpartial-matchF1,andap-
modeltominimizetheoutputdifferencebetween
proximatedynamicoraclesbasedonbeamsearch
theoriginalandperturbedinputGuoetal.(2022)
for non-decomposable metrics like ROUGE and
BLEU. We empirically verified the effectiveness
5 Discussion&FutureWork
ofexactandapproximatedynamicoraclesonthree
Pre-trainedModels OurexperimentsforNER metricsandthreetasks: partial-matchF1fornamed
andsummarizationshowedDAggertobeveryef- entityrecognition,ROUGEforsummarization,and
fectiveatfine-tuningpre-trainedmodelslikeBERT, BLEUformachinetranslation. Ourresultsshowed
FLAIR, and BART. However, it failed when we that seq2seq models trained by DAgger with ex-
trained the Transformer from scratch for MT. As act and approximate dynamic oracles yield less
the original DAgger work suggests (Ross et al., favorableperformanceonmachinetranslation,but
2011),theroleofinitialtrainingofthemodelpol- outperformexistingtrainingtechniquesonnamed
icyseemstodeterminetheoveralleffectivenessof entityrecognitionandsummarization.7 Limitations HalDaumé,J.Langford,andD.Marcu.2009. Search-
based structured prediction. Machine Learning,
Inpractice,theruntimeofthedynamicoraclefor 75:297–325.
BLEUscoreisapproximately6timeslongerthan
that of teacher forcing, mainly due to the beam LeonDerczynski,EricNichols,MariekevanErp,and
NutLimsopatham.2017. Resultsofthewnut2017
search process. However, we can accelerate the
sharedtaskonnovelandemergingentityrecognition.
beam search procedure through multi-threading, InProceedingsofthe3rdWorkshoponNoisyUser-
albeit at the expense of increased CPU memory generatedText,pages140–147.
usage. ConsideringthatCPUmemorycostsarelow,
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
andtheadditionaloverheadisonlyincurredduring
Kristina Toutanova. 2019. BERT: Pre-training of
the training phase, our methods remain practical
DeepBidirectionalTransformersforLanguageUn-
fordeployment. derstanding. InProceedingsofthe2019Conference
Inthecaseofnon-decomposablemetrics,theno- oftheNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics: HumanLanguageTech-
regretguaranteeisforfeitedbecausethedynamic
nologies,Volume1(LongandShortPapers),pages
oraclebecomesapproximate. Thisleavesspacefor
4171–4186,Minneapolis,Minnesota.Associationfor
futureworktoexplorealgorithmsforapproximat- ComputationalLinguistics.
ing the dynamic oracle that approach optimality
moreclosely. Markus Dreyer, Keith Hall, and Sanjeev Khudanpur.
2007. ComparingreorderingconstraintsforSMTus-
ingefficientBLEUoraclecomputation. InProceed-
ingsofSSST,NAACL-HLT2007/AMTAWorkshop
References
on Syntax and Structure in Statistical Translation,
AlanAkbik,TanjaBergmann,DuncanBlythe,Kashif pages103–110,Rochester,NewYork.Association
Rasul,StefanSchweter,andRolandVollgraf.2019a. forComputationalLinguistics.
FLAIR:Aneasy-to-useframeworkforstate-of-the-
artNLP. InProceedingsofthe2019Conferenceof DanielDuckworth,ArvindNeelakantan,BenGoodrich,
theNorthAmericanChapteroftheAssociationfor Lukasz Kaiser, and Samy Bengio. 2019. Parallel
ComputationalLinguistics(Demonstrations),pages ScheduledSampling. arXiv:1906.04331[cs]. ArXiv:
54–59, Minneapolis, Minnesota. Association for 1906.04331.
ComputationalLinguistics.
YangFeng,ShuhaoGu,DengjiGuo,ZhengxinYang,
Alan Akbik, Tanja Bergmann, and Roland Vollgraf. and Chenze Shao. 2021. Guiding teacher forcing
2019b. Pooledcontextualizedembeddingsfornamed withseerforcingforneuralmachinetranslation. In
entityrecognition. InProceedingsofthe2019Con- Proceedingsofthe59thAnnualMeetingoftheAsso-
ferenceoftheNorthAmericanChapteroftheAsso- ciationforComputationalLinguisticsandthe11th
ciationforComputationalLinguistics: HumanLan- InternationalJointConferenceonNaturalLanguage
guageTechnologies,Volume1(LongandShortPa- Processing (Volume 1: Long Papers), pages 2862–
pers),pages724–728,Minneapolis,Minnesota.As- 2872, Online. Association for Computational Lin-
sociationforComputationalLinguistics. guistics.
AlanAkbik,DuncanBlythe,andRolandVollgraf.2018.
DanielFriedandD.Klein.2018. Policygradientasa
Contextual string embeddings for sequence label-
proxy for dynamic oracles in constituency parsing.
ing. InProceedingsofthe27thInternationalCon-
InACL.
ferenceonComputationalLinguistics,pages1638–
1649,SantaFe,NewMexico,USA.Associationfor
YoavGoldbergandJoakimNivre.2013. TrainingDe-
ComputationalLinguistics.
terministicParserswithNon-DeterministicOracles.
TransactionsoftheAssociationforComputational
SamyBengio,OriolVinyals,NavdeepJaitly,andNoam
Linguistics,1:403–414.
Shazeer. 2015. Scheduled sampling for sequence
prediction with recurrent neural networks. In Ad-
KartikGoyal,ChrisDyer,andTaylorBerg-Kirkpatrick.
vances in Neural Information Processing Systems,
2017. DifferentiableScheduledSamplingforCredit
pages1171–1179.
Assignment. InProceedingsofthe55thAnnualMeet-
Kai-WeiChang,A.Krishnamurthy,AlekhAgarwal,Hal ingoftheAssociationforComputationalLinguistics
Daumé,andJ.Langford.2015. Learningtosearch (Volume2:ShortPapers),pages366–371,Vancouver,
betterthanyourteacher. ArXiv,abs/1502.02206. Canada.AssociationforComputationalLinguistics.
Nancy Chinchor and Beth Sundheim. 1993. MUC-5 E.J.Gumbel.1954. StatisticalTheoryofExtremeValues
evaluationmetrics. InFifthMessageUnderstanding and Some Practical Applications: A Series of Lec-
Conference(MUC-5): ProceedingsofaConference tures. Appliedmathematicsseries.U.S.Government
HeldinBaltimore,Maryland,August25-27,1993. PrintingOffice.DengjiGuo,ZhengruiMa,MinZhang,andYangFeng. recovering. In Proceedings of the Thirtieth In-
2022. Prediction difference regularization against ternational Joint Conference on Artificial Intelli-
perturbationforneuralmachinetranslation. InPro- gence, IJCAI-21, pages 3878–3884. International
ceedingsofthe60thAnnualMeetingoftheAssocia- JointConferencesonArtificialIntelligenceOrgani-
tionforComputationalLinguistics(Volume1: Long zation. MainTrack.
Papers),pages7665–7675,Dublin,Ireland.Associa-
tionforComputationalLinguistics. Xinglin Lyu, Junhui Li, Min Zhang, Chenchen Ding,
HidekiTanaka,andMasaoUtiyama.2023. Refining
Carlos Gómez-Rodrıguez and Daniel Fernández- historyforfuture-awareneuralmachinetranslation.
González. An efficient dynamic oracle for unre- IEEE/ACMTransactionsonAudio,Speech,andLan-
stricted non-projective parsing. Volume 2: Short guageProcessing,31:500–512.
Papers,page256.
ChrisManning.2006. Doingnamedentityrecognition?
CarlosGómez-Rodrıguez,FrancescoSartorio,andGior- don’toptimizeforf1.
gioSatta.2014. Apolynomial-timedynamicoracle
fornon-projectivedependencyparsing. InProceed- Clara Meister, Tim Vieira, and Ryan Cotterell. 2020.
ingsofthe2014ConferenceonEmpiricalMethods Best-firstbeamsearch.
in Natural Language Processing (EMNLP), pages
917–927. TsvetomilaMihaylovaandAndréF.T.Martins.2019.
ScheduledSamplingforTransformers. InProceed-
KarlMoritzHermann,TomasKocisky,EdwardGrefen- ingsofthe57thAnnualMeetingoftheAssociationfor
stette,LasseEspeholt,WillKay,MustafaSuleyman, ComputationalLinguistics: StudentResearchWork-
andPhilBlunsom.2015. Teachingmachinestoread shop,pages351–356,Florence,Italy.Associationfor
andcomprehend. InAdvancesinneuralinformation ComputationalLinguistics.
processingsystems,pages1693–1701.
RameshNallapati,BowenZhou,CaglarGulcehre,Bing
Michalis Korakakis and Andreas Vlachos. 2022. Im- Xiang,etal.2016. Abstractivetextsummarization
provingscheduledsamplingwithelasticweightcon- usingsequence-to-sequencernnsandbeyond. arXiv
solidation for neural machine translation. In Find- preprintarXiv:1602.06023.
ingsoftheAssociationforComputationalLinguistics:
EMNLP2022,pages7247–7258,AbuDhabi,United RenatoNegrinho,MatthewR.Gormley,andGeoffGor-
ArabEmirates.AssociationforComputationalLin- don. 2020. An Empirical Investigation of Beam-
guistics. AwareTraininginSupertagging. InFindingsofthe
AssociationforComputationalLinguistics: EMNLP
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan 2020.
Ghazvininejad,AbdelrahmanMohamed,OmerLevy,
VesStoyanov,andLukeZettlemoyer.2019. Bart:De- RenatoNegrinho,MatthewR.Gormley,andGeoffreyJ.
noisingsequence-to-sequencepre-trainingfornatural Gordon. 2018. Learning beam search policies via
languagegeneration,translation,andcomprehension. imitationlearning. InProceedingsofNeurIPS.
arXivpreprintarXiv:1910.13461.
KishorePapineni,SalimRoukos,ToddWard,andWei-
HaoranLiandWeiLu.2021. Mixedcrossentropyloss JingZhu.2002. Bleu: amethodforautomaticevalu-
forneuralmachinetranslation. ationofmachinetranslation. InProceedingsofthe
40thAnnualMeetingoftheAssociationforCompu-
Chin-YewLin.2004. Rouge: Apackageforautomatic tational Linguistics, pages 311–318, Philadelphia,
evaluation of summaries. In Text summarization Pennsylvania,USA.AssociationforComputational
branchesout,pages74–81. Linguistics.
YijinLiu,FandongMeng,YufengChen,JinanXu,and Jeffrey Pennington, Richard Socher, and Christopher
JieZhou.2021a. Confidence-awarescheduledsam- Manning. 2014. GloVe: Global vectors for word
plingforneuralmachinetranslation. InFindingsof representation. InProceedingsofthe2014Confer-
theAssociationforComputationalLinguistics: ACL- enceonEmpiricalMethodsinNaturalLanguagePro-
IJCNLP2021,pages2327–2337,Online.Association cessing (EMNLP), pages 1532–1543, Doha, Qatar.
forComputationalLinguistics. AssociationforComputationalLinguistics.
YijinLiu,FandongMeng,YufengChen,JinanXu,and Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
JieZhou.2021b. Scheduledsamplingbasedonde- andWojciechZaremba.2016a. Sequenceleveltrain-
codingstepsforneuralmachinetranslation. InPro- ingwithrecurrentneuralnetworks.
ceedingsofthe2021ConferenceonEmpiricalMeth-
ods in Natural Language Processing, pages 3285– Marc’Aurelio Ranzato, Sumit Chopra, Michael Auli,
3296,OnlineandPuntaCana,DominicanRepublic. andWojciechZaremba.2016b. Sequenceleveltrain-
AssociationforComputationalLinguistics. ing with recurrent neural networks. In 4th Inter-
national Conference on Learning Representations,
ZhidongLiu, JunhuiLi, andMuhuaZhu.2021c. Im- ICLR2016,SanJuan,PuertoRico,May2-4,2016,
provingtextgenerationwithdynamicmaskingand ConferenceTrackProceedings.StephaneRossandJ.AndrewBagnell.2014. Reinforce- Yiming Wang, Tongfei Chen, Hainan Xu, Shuoyang
mentandimitationlearningviainteractiveno-regret Ding, Hang Lv, Yiwen Shao, Nanyun Peng, Lei
learning. Xie,ShinjiWatanabe,andSanjeevKhudanpur.2019.
Espresso: Afastend-to-endneuralspeechrecogni-
Stéphane Ross, Geoffrey Gordon, and Drew Bagnell. tiontoolkit. In2019IEEEAutomaticSpeechRecog-
2011. A reduction of imitation learning and struc- nitionandUnderstandingWorkshop(ASRU).
turedpredictiontono-regretonlinelearning. InPro-
ceedingsofthefourteenthinternationalconference SamWisemanandAlexanderM.Rush.2016. Sequence-
on artificial intelligence and statistics, pages 627– to-sequence learning as beam-search optimization.
635. In Proceedings of the 2016 Conference on Empiri-
calMethodsinNaturalLanguageProcessing,pages
SaraSabour,WilliamChan,andMohammadNorouzi. 1296–1306,Austin,Texas.AssociationforComputa-
2019. Optimalcompletiondistillationforsequence tionalLinguistics.
learning. InInternationalConferenceonLearning
Representations. WeijiaXu,XingNiu,andMarineCarpuat.2019. Dif-
ferentiableSamplingwithFlexibleReferenceWord
ShiqiShen,YongCheng,ZhongjunHe,WeiHe,Hua OrderforNeuralMachineTranslation. InProceed-
Wu,MaosongSun,andYangLiu.2016. Minimum ingsofthe2019ConferenceoftheNorthAmerican
risktrainingforneuralmachinetranslation. InPro- Chapter of the Association for Computational Lin-
ceedingsofthe54thAnnualMeetingoftheAssocia- guistics: HumanLanguageTechnologies,Volume1
tionforComputationalLinguistics(Volume1: Long (Long and Short Papers), pages 2047–2053, Min-
Papers),pages1683–1692,Berlin,Germany.Associ- neapolis,Minnesota.AssociationforComputational
ationforComputationalLinguistics. Linguistics.
YanyaoShen,HyokunYun,ZacharyCLipton,Yakov Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Kronrod,andAnimashreeAnandkumar.2017. Deep Weinberger,andYoavArtzi.2020. Bertscore: Evalu-
activelearningfornamedentityrecognition. arXiv atingtextgenerationwithbert.
preprintarXiv:1707.05928.
WenZhang,YangFeng,FandongMeng,DiYou,and
ArtemSokolov,GuillaumeWisniewski,andFrançois QunLiu.2019. Bridgingthegapbetweentraining
Yvon.2012. Computinglatticebleuoraclescoresfor andinferenceforneuralmachinetranslation. InPro-
machinetranslation. In13thConferenceoftheEuro- ceedings of the 57th Annual Meeting of the Asso-
peanChapteroftheAssociationforcomputational ciationforComputationalLinguistics,pages4334–
Linguistics,pages120–129. 4343,Florence,Italy.AssociationforComputational
Linguistics.
ErikF.TjongKimSangandSabineBuchholz.2000. In-
troductiontotheCoNLL-2000SharedTask: Chunk-
ing. In Proceedings of the Fourth Conference
onComputationalNaturalLanguageLearningand
theSecondLearningLanguageinLogicWorkshop,
ConLL’00,pages127–132,Stroudsburg,PA,USA.
AssociationforComputationalLinguistics.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003a. IntroductiontotheCoNLL-2003SharedTask:
Language-IndependentNamed EntityRecognition.
InProceedingsoftheSeventhConferenceonNatu-
ralLanguageLearningatHLT-NAACL2003,pages
142–147.
Erik F. Tjong Kim Sang and Fien De Meulder.
2003b. IntroductiontotheCoNLL-2003sharedtask:
Language-independentnamedentityrecognition. In
ProceedingsoftheSeventhConferenceonNatural
LanguageLearningatHLT-NAACL2003,pages142–
147.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser,andIlliaPolosukhin.2017. AttentionisAll
you Need. In I. Guyon, U. V. Luxburg, S. Ben-
gio, H. Wallach, R. Fergus, S. Vishwanathan, and
R.Garnett,editors,AdvancesinNeuralInformation
Processing Systems 30, pages 5998–6008. Curran
Associates,Inc.A Appendix
Algorithm3:DynamicOracleforexactF1
Score
A.1 PartialF1ScoreAlgorithmProofof
Input :prev_gold_tag: previousgold
Correctness
tag,
Algorithm1representsourdynamicoracleforpar-
curr_gold_tag: currentgoldtag,
tialF1Score. Herewegiveaproofofcorrectness
prev_tag: previouspredicted
tothisalgorithm.
singletag
Weenumerateallpossiblecases,andproveeach
Output:nextbest
caseforitscorrectness.
ifstart(curr_gold_tag)=‘B’then
1
Case1: Ifthetagginghasalreadybegun,with 2
ifprev_tag̸=‘O’then
thesametype,i.e, 3 return‘O’
prev_gold_tag = ’O’ 4 else
curr_gold_tag = ’B-LOC’ 5 returncurr_gold_tag
prev_tag = ’B-LOC’ elseifstart(curr_gold_tag)=‘I’then
6
toscorepartialmatchscore,thebestnexttag ifprev_tagis‘O’then
7
shouldbeI-LOC.
return‘O’
8
elseiftype(prev_tag)̸=
Case 2: If the tagging has not started or if 9
type(prev_gold_tag)and
thereistypemismatchintheprevioustag,i.e,
start(prev_tag)̸=start(prev_gold_tag)
prev_gold_tag = ’O’
then
curr_gold_tag = ’B-LOC’
return‘O’
prev_tag = ’I-ORG’ 10
else
toscorepartialorexactmatchscore,thebest 11
returncurr_gold_tag
nexttagshouldbeB-ORG. 12
else
13
Case3: Ifthetagginghasnotalreadybegun,
return‘O’
14
but in the gold sequence, the tagging has al-
Note:
readystarted,i.e, 15
prev_gold_tag = ’B-LOC’ • start(tag)returnstheprefixofthetag,
curr_gold_tag = ’I-LOC’ i.e,B,I,orO.
prev_tag = ’O’
• type(tag)returnstheentitytype,i.e,PER,
toscorepartialmatchscore,thebestnexttag
LOC,etc.
shouldbeB-LOC.
Case 4: If the previous tag type does not
matchthecurrenttagtype,i.e,
match (both in the span of tokens and the label)
prev_gold_tag = ’B-LOC’
betweenthereferenceandsystemoutput.
curr_gold_tag = ’I-LOC’
Algorithm 3 presents our dynamic oracle for
prev_tag = ’B-PER’
exact-match F1. This is a very simple algorithm
tonotreducetheF1ScoreduetoFalseNega-
forpredictingthenextbesttokenfortheNERtask.
tivesorFalsePositives,thebestnextpredic-
tioncaneitherbeI-PERorO,andthesafest
A.3 ExactF1Scorealgorithm
casewouldbetopredictO.
The dynamicoracle algorithm forexactF1 score
A.2 ExactMatchF1Score algorithmisgiveninAlgorithm3.
Earlyworkinchunking(CoNLL-2000sharedtask
B Worderrorrate(WER)
(TjongKimSangandBuchholz,2000))andNER
(CoNLL-2003 shared task (Tjong Kim Sang and Duringourstudy,wehavealsoexploreddynamic
DeMeulder,2003a))establishedF1astheprimary oracleforWordErrorRate(WER),whichiscom-
metricforevaluatingspan-basedsequencetagging monlyusedmetricinAutomaticSpeechRecogni-
tasks. Herewerefertothismetricasexact-match tion (ASR) tasks. We decided not to put this sec-
F1becauseitcomputestheharmonicmeanofpre- tioninthemaintextbecausewelaterfoundthere
cisionandrecallbasedononlyspanswhichexactly hasbeenaconcurrentandsimilarresearch(Sabouretal.,2019). Nonetheless,inhopeofhelpingfuture
researchers, here we present our dynamic oracle If the jth word in the gold sequence
forWER(SectionB.1)andaproofofcorrectness matches with the ith word in the predicted
(SectionB.2). sequence, the substitution WER does not
increase.
B.1 Dynamicoracle
NoteonTie-breaking Notethatonline15,there If the jth word in the gold sequence
existmultiplewaystobreakties. Currentlyweuse does not match with the ith word in the
thefollowingtiebreakingpreference: Iftheground predicted sequence, the substitution WER
truthindexisamongtheminima,selecttheground willincreasebyavalueof1whencompared
truthindex;ifnot,thenselectthelargestindex(this totheWERofthesequenceuptothatpoint,
effectively tells the dynamic oracle sequence to (i.e),theWERuntilthe(j −1)th wordinthe
prefershortersequence). goldsequencecorrespondingtothe(i−1)th
wordinthepredictedsequence.
NoteonS Itisimportanttounderstandthemean-
A penalty of 1 is added to the WER if the
ing of S here. S is the tokens fed to the model.
wordsdonotmatch. So,heretogetridofthis
If we are using Teacher Forcing (or equivalently,
penaltytermwhencalculatingthesubstitution
ScheduledSamplingwithsamplingprobabilityof
error, we always select the same word as
zero),thenS isjustG;ifweareusingScheduled
in the gold sequence when we are trying to
Samplingwithnon-zerosamplingprobability,then
extendthesequence.
S containsbothgroundtruthtokensandmodelpre-
dictedtokens;ifweareusingScheduledSampling
For example, if we consider the gold
with sampling probability of 1, then S is just the
sequenceasA B Candthecurrentpredicted
model’spredictedsequence.
sequenceisA B.ThechartisshowninTable
Note on modularization and optimization 4.
Notethattheabovealgorithmcanbeexecutedina By choosing A, B and C for each cell
post-hocfashion,meaningthatwecancalculatethe respectively when growing the chart, the
DynamicOracleaftertheentireGPUforwarding substitutionerrorswillbe,
is finished. This modularization allows us to
port the entire Dynamic Oracle implementation
Substitution Error for A = 2
to C as opposed to native Python, which makes
Substitution Error for B = 1
things faster; also, this formulation modularizes
Substitution Error for C = 0
our method as a plug-and-play component for
sequencegenerationtasksandallowsittoserveas
By choosing ∼A, ∼B and ∼C for each cell
aneasyextensiontoexistingsystems.
respectively when growing the chart, the
substitutionerrorswillbe,
B.2 ProofofCorrectness
Line14: Substitution Error for ∼A = 3
Substitution Error for ∼B = 2
Substitution Error for ∼C = 1
1. Choosing A is always superior to choosing
∼A,inthatchoosingAwillleadtonopenalty
Therefore,thereasonforwepreferchoosing
(i.e)substitutionerror.
A over ∼A here is because A (choosing the
same word as the gold sequence) does not
If the jth word in the gold sequence
incur any substitution error, whereas ∼A
matches with the ith word in the predicted
(choosing a different word) always incurs
sequence,thereisnosubstitutionthathasto
a substitution error. So, choosing the same
be done. But if the words do not match, the
wordisalwayssuperior.
jth wordinthegoldsequencewillhavetobe
replaced with the ith word in the predicted
sequence. 2. The terms DP + 1 and
W.E.R[i−1][j]Algorithm4:DynamicOracleforWordErrorRate
Input :G: tokenarray[L]-groundtruthsequence,
S: tokenarray[L]-previoustokensfedtomodel
Output:O:tokenarray[L]-dynamicoraclesequence
FunctionDynamicOracleWER(G,S):
1
DP ←emptyintarray[L+1][L+1]
2 WER
DP ←intarray[L+1][L+1]with+∞’s
3 D.O.
4 setDP WER[0][:],DP D.O.[0][:] tobe0...L // for the start token
5 setDP WER[:][0],DP D.O.[:][0] tobe0...L // for the start token
6
7 fori ← 1toL+1do // loop over rows
8 forj ← 1toL+1do // loop over columns
/* calculating WER for each candidate token, using dynamic
programming */
ifS =G then
9 [i] [j]
penalty ← 0
10
else
11
penalty ← 1
12
(cid:40) (cid:41)
DPWER[i−1][j−1]+penalty,
13 DP WER[i][j] ← min DPWER[i−1][j]+1,
DPWER[i][j−1]+1
(cid:40) (cid:41)
DPWER[i−1][j−1],
14 DP D.O.[i][j] ← min DPWER[i−1][j]+1,
DPD.O.[i][j−1]+1
15 idx← argmin{DP D.O.[i][:]} // select token with the lowest WER as label
ifidx=L+1then
16
17 O [i] ←⟨End⟩ // further growing the chart will not reduce WER score,
so output ⟨End⟩
else
18
O ← G
19 [i] idx
returnO
20 [1:L+1]ϵ A B C best of the 3 of them (i.e) the minimum of these
ϵ 0 1 2 3 values.
A 1 0 1 2
B 2 1 0 1
Line15:
Table4:ChartforGoldSequenceA B CandPredicted
ChoosingG ,whereind = argmin(DP ),
SequenceA B ind D.O[i]
asthefirsttokeninthegoldsequenceissuperior
tochoosingG ,whereind ̸= ind′,asthestart
ind′
DP + 1 correspond to inser- tokenforthegoldsequenceinthatG leadstoa
D.O[i][j−1] ind
tionanddeletionerrorsrespectively. sequenceofleastpossibleWER.
1. Case 1: There are no ties (i.e) there is only
If the ith word has to be inserted to the
oneoptimalsolutioninthatparticularstep.
sequence, then the WER will increase by 1
when compared to the WER corresponding
Letusassumeinsteadofchoosingtheargmin
to the sequence until that point (i.e) the
WER, we choose some other word (i.e) we
WERuntilthejth wordinthegoldsequence
choose some other word at index ind′ and
corresponding to the (i − 1)th word in the
ind′ ̸= indwhereind = argmin(DP ).
predictedsequence. O[i]
The DP is adjusted in a way that
If the ith word has to be deleted from W.E.R
the word that has been chosen is the word
thesequence,thentheWERwillincreaseby
correspondingtothewordatindexind′.
1whencomparedtotheWERcorresponding
tothesequenceuntilthatpoint(i.e)theWER
We know that at every step, there is a
untilthe(j −1)th wordinthegoldsequence
possibility of the best WER at the previous
correspondingtotheith wordinthepredicted
steptoincreaseby1ornotincreaseatall.
sequence.
The best WER will remain the same if the
next predicted word matches the word in
the gold sequence. In this case, there is no
For example, if the gold sequence is A
penaltyadded.
B Candthepredictedsequenceuntilnowis
Ifthewordsdonotmatch,apenaltywillhave
A B,
to be added (either as an insertion error or
as a substitution error), as explained in the
WER to insert C = WER
previousproof.
corresponding to AB + 1
(a) Choosingthenon-optimalword
Let us denote the WER at the ith
IfthegoldsequenceisA Bandthepredicted
predictedwordfortheindexind’asb.
sequenceuntilnowisA B C,
Uponfurthergrowingthechart,
WER to delete C = WER b ≤ minimum WER at the (i + 1)th
corresponding to ABC + 1 predictedword≤(b+1)
.
.
Therefore we can say that the second term
.
corresponds to the insertion error term and
b ≤ minimum WER at the (i + n)th
the third term corresponds to the deletion
predictedword≤(b+n)
errortermwhencalculatingtheWER.
wherendenotedthenumberofsteps.
Since these 3 operations are the only possible If we keep choosing the optimal
operationsthatwecoulddo,andforeachofthe3 wordatallthefollowingstepsandstop
wehavecalculatedtheoveralllowestcostofdoing thesequenceoncethestoppingcondition
thatthing,theoverallbestthingtodomustbethe isreached,theWERwillnotincrease.If we always choose something other case,havingchosentheoptimalworduptothe
thantheoptimalwordatallthefollowing theith stepandwecansaythatbychoosing
steps, the best score we can get will be any of the optimal words at the ith step and
b+n,assumingtherearensteps. bycontinuouslychoosingtheoptimalwordat
(b) Choosing the optimal word (i.e) the thefollowingsteps, wewillendupwiththe
wordcorrespondingtotheargminofthe sameWER.
WER Bychoosingawordatanyoftheseminimum
Let us denote the WER at the ith indices,wewillreachthesameoptimalWER
predicted word for the index ind as a, attheend.
wherea<b,sinceacorrespondstothe
argminword. In this implementation, we are choos-
ingthewordcorrespondingtotheminimum
Uponfurthergrowingthechart, index as it would help in matching the
a ≤ minimum WER at the (i + 1)th sequencelengthsthebest.
predictedword≤(a+1)
StoppingCondition:
.
IftheminimumWERcorrespondstothelastword
.
inthegoldsequence,theendofthesequencehas
.
beenreached.
a ≤ minimum WER at the (i + n)th
predictedword≤(a+n)
When the minimum WER corresponds to
wherendenotedthenumberofsteps.
the last word in the gold sequence (i.e)
min_WER = DP , extending the
Similar to the previous selection, D.O[i][L+1]
sequencefurtherwillonlyincreasetheWER.
if we keep choosing the optimal word
at all the following steps and stop the
Let us assume we are going to grow the
sequenceoncethestoppingconditionis
chart further after reaching the minimum WER
reached,theWERwillnotincrease.
at the last word, the argmin will once be the
If we always choose something other
last word. We know that, DP =
thantheoptimalwordatallthefollowing D.O[i+1][j]
min(DP ,DP +
steps, the best score we can get will be W.E.R[i][j−1] W.E.R[i][j]
1,DP +1)
a+n,assumingtherearensteps. D.O[i+1][j−1]
Assuming that we have only made optimal In the ith row, DP W.E.R[i][j] > DP W.E.R[i][L+1]
wordselectionuntiltheith word,andwewill forall0 ≤ j < L+1.
onlybemakingoptimalselectionaftertheith From the previous proofs, we can say that in the
word, the final WER will be a if we choose (i + 1)th row, DP D.O[i+1][j] > DP W.E.R[i][L+1]
thewordcorrespondingtotheindexind(the forall0 ≤ j ≤ L+1.
argminWERattheithstep)andtheWERwill
bebifwechoosethewordcorrespondingto This causes an increase in the optimal WER
theindexind’(awordwhichisdifferentfrom forthefinalsequence.
theargminWERattheith step),whichisnot Therefore the condition for stopping when the
the optimal path as another path with lower minimumWERcorrespondstothelastwordinthe
WERexists. goldsequenceiscorrect.
2. Case 2: There are ties (i.e) more than one
optimalsolutionexistsataparticularstep.
Let us assume we have argmin values
atindicesj,j +mandj +n,wherem,n∈
Z.
With a proof similar to the previous