Interpreting the Weight Space
of Customized Diffusion Models
AmilDravid∗1,2 YossiGandelsman∗1 Kuan-ChiehWang2
RameenAbdal3 GordonWetzstein3 AlexeiA.Efros1 KfirAberman2
1UCBerkeley 2SnapInc. 3StanfordUniversity
weights2weights (w2w)
Space
Diffusion Unet
weights beard
Single Image Inversion Identity Editing Sampling
Figure1: weights2weights(w2w)spaceenablesgenerativeapplicationsonmodelweights. We
modelamanifoldovercustomizeddiffusionmodelweightsasasubspaceencodingdifferenthuman
identities. Thisformsaspacethatsupportsinvertingthevisualidentityfromasingleimageinto
amodel,editingtheidentityencodedinthemodel,andsamplingnewmodelsthatencodediverse
instancesofpeople. Thisspaceeffectivelyservesasalatentspaceovercustomizedmodels.
Abstract
Weinvestigatethespaceofweightsspannedbyalargecollectionofcustomized
diffusion models. We populate this space by creating a dataset of over 60,000
models, each of which is a base model fine-tuned to insert a different person’s
visualidentity. Wemodeltheunderlyingmanifoldoftheseweightsasasubspace,
whichwetermweights2weights. Wedemonstratethreeimmediateapplications
ofthisspace–sampling,editing,andinversion. First,aseachpointinthespace
correspondstoanidentity, samplingasetofweightsfromitresultsinamodel
encodinganovelidentity. Next,wefindlineardirectionsinthisspacecorrespond-
ingtosemanticeditsoftheidentity(e.g.,addingabeard). Theseeditspersistin
appearance across generated samples. Finally, we show that inverting a single
imageintothisspacereconstructsarealisticidentity,eveniftheinputimageis
outofdistribution(e.g.,apainting). Ourresultsindicatethattheweightspaceof
fine-tuneddiffusionmodelsbehavesasaninterpretablelatentspaceofidentities.1
∗Equalcontribution;correspondencetoamil_dravid@berkeley.edu
1Projectpage:https://snap-research.github.io/weights2weights
Code:https://github.com/snap-research/weights2weights
Preprint.Underreview.
4202
nuJ
31
]VC.sc[
1v31490.6042:viXraSampling Editing Inversion
image reconstructed image
z+Δz
chubby latent z → image →image editing →z→
model θ∈w2w→ identity θ+Δθ chubby→ identity editing image reconstructed identity
A [v] person… A [v] person…
→θ→
Figure2: Theweights2weightsspaceenablesGAN-likeapplicationsonidentity-encodingmodel
weights. Novelidentitiescanbesampledfromthespaceandeditedbylinearlytraversingalong
semanticdirectionsinweightspace. Additionally,asingleimagecanbeinvertedintothespaceto
produceamodelthatconsistentlygeneratesthatidentity.
1 Introduction
Generativemodelshaveemergedasapowerfultooltomodelourrichvisualworld. Inparticular,the
latentspaceofsingle-stepgenerativemodels,suchasGenerativeAdversarialNetworks(GANs)[12,
23],hasbeenshowntolinearlyencodemeaningfulconceptsintheoutputimages. Forinstance,linear
directionsinGANsencodedifferentattributes(e.g.,genderorageoffaces)andcanbecomposed
for multi-attribute image edits [14, 19, 53]. Alas, in multi-step generative models, like diffusion
models[16,54],suchalinearlatentspaceisyettobefound.
Recentlyintroducedpersonalizationapproaches,suchasDreambooth[47]orCustomDiffusion[28],
mayhintatwheresuchaninterpretablelatentspacecanexistindiffusionmodels. Thesemethodsaim
tolearnaninstanceofasubject,suchasaperson’svisualidentity. Ratherthansearchingforalatent
codethatrepresentsanidentityintheinputnoisespace,theseapproachescustomizediffusionmodels
byfine-tuningonsubjectimages, whichresultsinidentity-specificmodelweights. Wetherefore
hypothesizethatalatentspacecanexistintheweightsthemselves.
Totestourhypothesis,wefine-tuneover60,000personalizedmodelsonindividualidentitiestoobtain
pointsthatlieonamanifoldofcustomizeddiffusionmodelweights. Toreducethedimensionality
of each data point, we use low-rank approximation (LoRA) [17] during fine-tuning and further
applyPrincipalComponentsAnalysis(PCA)tothesetofdatapoints. Thisformsourfinalspace:
weights2weights(w2w). UnlikeGANs,whichmodelthepixelspaceofimages,wemodeltheweight
space of these personalized models. Thus, each sample in our space corresponds to an identity-
specificmodelwhichcanconsistentlygeneratethatsubject. WeprovideaschematicinFig.2that
contraststheGANlatentspacewithourproposedw2wspace,demonstratingthedifferencesand
analogiesbetweenthesetworepresentations.
Creatingthisspaceunlocksavarietyofapplicationsthatinvolvetraversalinw2w(Fig.1). First,we
demonstratethatsamplingmodelweightsfromw2wspacecorrespondstoanewidentity. Second,
wefindlineardirectionsinthisspacecorrespondingtosemanticeditsintheidentity. Finally,we
showthatenforcingweightstoliveinthisspaceenablesadiffusionmodeltolearnanidentitygivena
singleimage,evenifitisoutofdistribution.
Wefindthatw2wspaceishighlyexpressivethroughquantitativeevaluationoneditingcustomized
models and encoding new identities given a single image. Qualitatively, we observe this space
supportssamplingmodelsthatencodediverseandrealisticidentities,whilealsocapturingthekey
characteristicsofout-of-distributionidentities.
2 RelatedWork
Image-basedgenerativemodels. Variousmodelshavebeenproposedforimagegeneration,includ-
ingVariationalAutoencoders(VAEs)[26],Flow-basedmodels[7,27,42],GenerativeAdversarial
Networks(GANs)[12],andDiffusionmodels[16,36,54]. Withintherealmofhigh-qualityphoto-
realisticimagegeneration,GANs[20,23,24]andDiffusionmodels[16,36,45,55]havegarnered
2
NAG
ecaps
w2wFigure3: Buildingweights2weights(w2w)space. Wecreateadatasetofmodelweightswhereeach
modelispersonalizedtoaspecificidentityusinglow-rankupdates(LoRA).Thesemodelweights
lieonaweightsmanifoldthatwefurtherprojectintoalower-dimensionalsubspacespannedbyits
principalcomponents. Wetrainlinearclassifierstofinddisentanglededitdirectionsinthisspace.
significantattentionduetotheircontrollabilityandabilitytoproducehigh-qualityimages. Leverag-
ingthecompositionalityofthesemodels,methodsforpersonalizationandcustomizationhavebeen
developedwhichaimtoinsertuser-definedconceptsviafine-tuning[10,28,33,47]. Variousworks
trytoreducethedimensionalityoftheoptimizedparametersforpersonalizationeitherbyoperating
inspecificmodellayers[28]orintext-embeddingspace[10],bytraininghypernetworks[48],andby
constructingalinearbasisintextembeddingspace[62].
Latentspaceofgenerativemodels. Linearlatentspacemodelsoffacialshapeandappearancewere
studiedextensivelyinthe1990s,usingbothPCA-basedrepresentations(e.g. ActiveAppearance
Models[5],3DMorphableModels[3])aswellasmodelsoperatingdirectlyinpixelandkeypoint
space[46]. However,thesetechniqueswererestrictedtoalignedandcroppedfrontalfaces. More
recently,GenerativeAdversarialNetworks(GANs),particularlytheStyleGANseries[21,22,23,24],
haveshowcasededitingcapabilitiesfacilitatedbytheirinterpretablelatentspace. Furthermore,linear
directionscanbefoundintheirlatentspacetoconductsemanticeditsbytraininglinearclassifiers
orapplyingPCA[14,53]. SeveralmethodsaimtoprojectrealimagesintotheGANlatentspacein
ordertoconductthisediting[1,44,56,65].
Althoughdiffusionmodelsarchitecturallylacksuchalatentspace,someworksaimtodiscovera
GAN-likelatentspaceinthem. ThishasbeenexploredintheUNetbottlenecklayer[29,34],noise
space[6],andtext-embeddingspace[2]. ConceptSliders[11]explorestheweightspaceforsemantic
imageeditingbyconductinglow-ranktrainingwithcontrastingimageortextpairs.
Weightsasdata. Pastworkshaveexploitedthestructurewithinweightspaceofdeepnetworksfor
variousapplications. Inparticular,somehavefoundlinearpropertiesofweights,enablingsimple
modelensemblingandeditingviaarithmeticoperations[18,49,52,59]. Otherworkscreatedatasets
ofneuralnetworkparametersfortraininghypernetworks[8,13,35,48,58],predictingpropertiesof
networks[51],andcreatingdesignspacesformodels[39,40].
3 Method
WestartbydemonstratinghowwecreateamanifoldofmodelweightsasillustratedinFig.3. We
explain how we obtain low-dimensional data points for this space, each of which represents an
individual identity. We then use these points to model a weights manifold. Next, we find linear
directionsinthismanifoldthatcorrespondtosemanticattributesandusethemforeditingtheidentities.
Finally,wedemonstratehowthismanifoldcanbeutilizedforconstraininganill-posedinversiontask
withasingleimagetoreconstructitsidentity.
3.1 Preliminaries
Inthissection,wefirstintroducelatentdiffusionmodels(LDM)[45],whichwewillusetocreatea
datasetofweights.Then,weexplaintheapproachforderivingidentity-specificmodelsfromLDMvia
Dreambooth[47]fine-tuning. Wefinallypresentaversionoffine-tuningthatuseslow-dimensional
weightupdates(LoRA[17]). Wewillusethefine-tunedlow-dimensionalper-identityweightsasdata
pointstoconstructtheweightsmanifoldinSec.3.2.
3Latentdiffusionmodels[45]. Wewillextractweightsfromlatentdiffusionmodelstocreatew2w
space. Thesemodelsfollowthestandarddiffusionobjective[16]whileoperatingonlatentsextracted
fromapre-trainedVariationalAutoencoder[9,26,43]. Withtext,theconditioningsignalisencoded
byatextencoder(suchasCLIP[37]),andtheresultingembeddingsareprovidedtothedenoising
UNetmodel. Thelossoflatentdiffusionmodelsis:
E [w ||ϵ−ϵ (x ,c,t)||2], (1)
x,c,ϵ,t t θ t 2
whereϵ isthedenoisingUNet,x isthenoisedversionofthelatentforanimage,cistheconditioning
θ t
signal,tisthediffusiontimestep,andw isatime-dependentweightontheloss.
t
Tosamplefromthemodel,arandomGaussianlatentx isdeterministicallydenoisedconditioned
T
onapromptforafixedsetoftimestepswithaDDIMsampler[55]. Thedenoisedlatentisthenfed
throughtheVAEdecodertogeneratethefinalimage.
Dreambooth[47]. Toobtainanidentity-specificmodel,weusetheDreamboothpersonalization
method. Dreamboothfine-tuningintroducesanovelsubjectintoapre-traineddiffusionmodelgiven
onlyafewimagesofit. Duringtraining,Dreamboothfollowsatwo-partobjective:
E [w ||ϵ−ϵ (x ,c,t)||2+λw ||ϵ′−ϵ (x′,c′,t′)||2], (2)
x,c,ϵ,t t θ t 2 t′ θ t 2
where the first term corresponds to the standard diffusion denoising objective using the subject-
specificdataxconditionedonthetextprompt“[identifier][classnoun]”(e.g.,“V*person”),denoted
c. Thesecondterm, weightedbyλ, correspondstoapriorpreservationloss, whichinvolvesthe
standarddenoisingobjectiveusingthemodel’sowngeneratedsamplesx′ forthebroaderclassc′
(e.g.,“person”). Thispreventsthemodelfromassociatingtheclassnamewiththespecificinstance,
whilealsoleveragingthesemanticpriorontheclass. Weutilizethisapproachtoobtainaper-subject
modelanduseitsweightstocreatetheinterpretableweightsmanifold.
LowRankAdaptation(LoRA)[17]. Dreamboothrequiresfine-tuningalltheweightsofamodel,
which is a high–dimensional space. We turn to a more efficient fine-tuning scheme, LoRA, that
modifiesonlyalow-rankversionoftheweights. LoRAusesweightupdates∆W withalowintrinsic
rank. ForabasemodellayerW ∈Rm×n,theLoRAupdateforthatlayer∆W canbedecomposed
into∆W = BA, whereB ∈ Rm×r andA ∈ Rr×n arelow-rankmatriceswithr ≪ min(m,n).
Duringtraining,foreachmodellayer,onlytheAandBareupdated. Thissignificantlyreducesthe
numberoftrainableparameters. Duringinference,thelow-rankweightsareaddedresiduallytothe
weightsofeachlayerinthebasemodelandscaledbyacoefficientα∈R: W +α∆W.
3.2 Constructingtheweightsmanifold
Creatingadatasetofmodelweights. Toconstructtheweights2weights(w2w)space,webeginby
creatingadatasetofmodelweightsθ . WeconductDreamboothfine-tuningonLatentDiffusion
i
modelsinordertoinsertnewsubjectswiththeabilitytocontrolimageinstancesusingtextprompts.
This training is done with LoRA in order to reduce the space of model parameters. Each model
is fine-tuned on a set of images corresponding to one human subject. After training, we flatten
andconcatenatealloftheLoRAmatrices,resultinginadatapointθ ∈ Rd whichrepresentsone
i
identity. After training over N different instances, we have our final dataset of model weights
D ={θ ,θ ,...,θ },representingadiversearrayofsubjects.
1 2 N
Modeling the weights manifold. We posit that our data D ⊆ Rd lies on a lower-dimensional
manifoldofweightsthatencodeidentities. ArandomlysampledsetofweightsinRd,wouldnotbe
guaranteedtoproduceavalidmodelencodingidentityastheddegreesoffreedomcanbefine-tuned
foranypurpose.Therefore,wehypothesizethatthismanifoldisasubsetoftheweightspace.Inspired
byfindingsthathigh-levelconceptscanbeencodedaslinearsubspacesofrepresentations[31,38,41],
wemodelthissubsetasalinearsubspaceRmwherem<d,andcallitweights2weights(w2w)space.
We represent points in this subspace as a linear combination of basis vectors w = {w ,...,w },
1 m
w ∈Rd. Inpractice,weapplyPrincipalComponentAnalysis(PCA)ontheN modelsandkeepthe
i
firstmprincipalcomponentsfordimensionalreductionandformingourbasisofmvectors.
Samplingfromtheweightsmanifold. Aftermodelingthisweightsmanifold,wecansampleanew
modelthatliesonit,resultinginanewmodelthatgeneratesanovelidentity. Wesampleamodel
representedwithbasiscoefficients{β ,...,β },whereeachcoefficientβ issampledfromanormal
1 m k
distributionwithmeanµ andstandarddeviationσ . Themeanandstandarddeviationarecalculated
k k
foreachprincipalcomponentkfromthecoefficientsamongallthetrainingmodels.
43.3 FindingInterpretableWeightSpaceDirections
Weseekadirectionn∈Rddefiningahyperplanethatseparatesbetweenbinaryidentityproperties
embeddedinthemodelweights(e.g.,male/female),similarlytohyperplanesobservedinthelatent
spaceofGANs[53].Weassumebinarylabelsaregivenforattributespresentintheidentitiesencoded
bythemodels. Wethentrainlinearclassifiersusingweightsofthemodelsasdatabasedonthese
labels,imposingseparatinghyperplanesinweightspace. Givenanidentityparameterizedbyweights
θ,wecanmanipulateasingleattributebytraversinginadirectionn,orthogonaltotheseparating
hyperplane: θ =θ+αn.
edit
3.4 Inversionintow2wSpace
Traditionally,inversionofagenerativemodelinvolvesfindinganinputsuchasalatentcodethat
bestreconstructsagivenimage[32,60]. Thiscorrespondstofindingaprojectionoftheinputonto
thelearneddatamanifold[65]. Withw2wspace,wemodelamanifoldofdatawhichhappenstobe
modelweightsratherthanimages. Inspiredbylatentoptimizationmethods[1,65],weproposea
gradient-basedmethodofinvertingasingleidentityfromanimageintoourdiscoveredspace.
Givenasingleimagex,wefollowaconstraineddenoisingobjective:
max E [w ||ϵ−ϵ (x ,c,t)||2] s.t. θ ∈w2w (3)
x,c,ϵ,t t θ t 2
θ
Specifically, we constrain the model weights to lie in w2w space by optimizing a set of basis
coefficients{β ,...,β }ratherthantheoriginalparameters. UnlikeDreambooth,wedonotemploy
1 m
apriorpreservationloss,sincetheoptimizedmodelliesinthesubspacedefinedbyourdatasetof
weights,andinheritstheirpriors.
4 Experiments
Wedemonstratew2wspaceonhumanidentitiesforavarietyofapplications. Webeginbydetailing
implementation details. Next, we use w2w space for 1) sampling new models encoding novel
identities,2)editingidentityattributesinaconsistentmannervialineartraversalinw2wspace,3)
embeddinganewidentitygivenasingleimage,and4)projectingout-of-distributionidentitiesinto
w2wspace. Finally,weanalyzehowscalingthenumberofmodelsinourdatasetofmodelweights
affectsthedisentanglementofattributedirectionsandpreservationofidentity.
4.1 ImplementationDetails
Creating an identity dataset. We generate a synthetic dataset of ∼65,000 identities using [57],
where each identity is associated with multiple images of that person. Each identity is based on
animagewithlabeledbinaryattributes(e.g.,male/female)fromCelebA[30]. Eachsetofimages
correspondingtoanidentityisthenusedasdatatofine-tunealatentdiffusionmodelwithDreambooth.
NotethatthesameidentitycanoccurmultipletimesindifferentimagesinCelebA.Assuch,some
ofourgeneratedidentitiesmayencodedifferentinstancesofthesameperson. Thisresultsinsome
ofthefine-tunedmodelsinourdatasetofweightsencodingdifferentinstancesofthesameperson.
FurtherdetailsareprovidedinAppendixE.
Encodingidentitiesintomodelweights. WeconductDreamboothfine-tuningusingLoRAwith
rank1ontheidentities. Following[28,49],weonlyfine-tunethekeyandvalueprojectionmatrices
inthecross-attentionlayers. WeutilizetheRealisticVision-v512checkpointbasedonStableDiffusion
1.5. ConductingDreamboothfine-tuningoneachidentitytrainingsetresultsinadatasetof∼65,000
weights θ where θ ∈ R100,000. We hold out 100 identities for evaluating edits, which results in
leavingout∼1000modelsbasedonhowweconstructedouridentitydatasets.
Finding semantic attribute directions. We utilize binary attribute labels from CelebA to train
linearclassifiersonthedatasetofmodelweightswecurated. WerunPrincipalComponentAnalysis
(PCA)onthe∼65,000trainingmodelsandprojecttothefirst1000principalcomponentsinorderto
reducethedimensionality. Theorthogonaleditdirectionsarecalculatedviatheanalyticleastsquares
2https://huggingface.co/stablediffusionapi/realistic-vision-v51
5solutiononthematrixofprojectedtrainingmodelsD ∈R65,000×1000,andthenunprojectedtothe
originaldimensionalityofthemodelweights: θ ∈R100,000.
4.2 Samplingfromw2wSpace
Sampled Identity Nearest Neighbor
Wepresentimagesgeneratedfrommodelsthat
weresampledfromtheweightsmanifold(i.e.,
w2wSpace)inFig.4. Wefollowthesampling
procedurefromSec.3.2,andgenerateimages
fromthesampledmodelwithvariousprompts
andseeds. Asshown,eachnewmodelencodes
anovel,realistic,andconsistentidentity. Addi-
tionally,wepresentthenearestneighbormodel
among the training dataset of model weights.
Weusecosinesimilarityonthemodels’princi-
Figure4: Identitysamplesfromw2wspace. We
palcomponentrepresentations. Comparingwith
showthesamplesfromw2wspacedonotoverfit
thenearestneighborsshowsthatthesesamples
tonearest-neighboridentities,althoughtheyincor-
arenotjustcopiesfromthedataset,butratheren-
poratefacialattributesfromthem. Theidentities
codediverseidentitieswithdifferentattributes.
arediverseandconsistentacrossgenerations.
Yet,thesamplesstilldemonstratesomesimilar
featurestothenearestneighbors. Theseincludejawlineandeyeshape(toprow),facialhair(middle
row),andnoseandeyeshape(bottomrow). AppendixAincludesmoresuchexamples.
4.3 EditingSubjects
Wedemonstratehowdirectionsfoundbythelinearclassifierscanbeusedtoeditsubjectsencodedin
themodels. Itisdesiredthattheseeditsare1)disentangled(i.e.,donotinterferewithotherattributes
oftheembeddedsubjectandpreserveallotherconceptssuchascontext)2)identitypreserving(i.e.,
thepersonisstillrecognizable)3)andsemanticallyalignedwiththeintendededit.
Baselines. Wecompareagainstanaïvebaselineofpromptingwiththedesiredattribute(e.g.,“[v]
personwithsmBaallldeyes”),andthenConceptSlideArsge[11],aninstance-specificedCithinugbbmyethodwhich
weadapttosubjectediting. Inparticular,wetraintheirmostaccessiblemethod,thetext-basedslider,
whichtrainsLoRAstomodulateattributesinapretraineddiffusionmodelbasedoncontrastingtext
prompts. Wethenapplythesesliderstothepersonalizedidentitymodels.
Evaluationprotocol. Weevaluatethesethreemethodsforidentitypreservation,disentanglement,
andeditcoherence. Tomeasureidentitypreservation,wefirstdetectfacesintheoriginalgenerated
images and the result of the edits using MTCNN [63]. We then calculate the similarity of the
FaceNet[50]embeddings. WealsouseLPIPS[64]computedbetweentheimagesbeforeandafter
theedittomeasurethedegreeofdisentanglementwithothervisualelements,andCLIPscore[15],to
measureifthedesirededitmatchesthetextcaptionfortheedit.
Woman Chubby Narrow Eyes
Figure 5: Qualitative comparison. w2w edits preserve identity while being disentangled and
semanticallyaligned. ConceptSliders[11]tendstoexaggerateeffectswhichinducesartifactsand
degradesidentity,whilepromptingthesubjectwiththedesirededithasunexpectedeffects.
6
draeB
gnitpmorP
sredilS
w2w
elaP
riaH
kcalBTable1: Editsinw2wspacepreserveidentity,aredisentangled,andsemanticallyaligned.
IDScore↑ LPIPS↓ CLIPScore↑
Prompting Sliders w2w Prompting Sliders w2w Prompting Sliders w2w
Gender 0.39±0.08 0.33±0.09 0.45±0.09 0.30±0.05 0.39±0.09 0.31±0.03 1.98±0.78 3.50±0.68 4.13±0.59
Chubby 0.29±0.14 0.33±0.09 0.45±0.09 0.41±0.05 0.38±0.04 0.36±0.04 1.12±0.61 2.21±0.61 2.16±0.51
Eyes 0.52±0.06 0.53±0.04 0.72±0.05 0.32±0.03 0.30±0.02 0.19±0.02 0.17±0.17 0.01±0.22 0.59±0.19
Original + Flat Brows + Bangs + Straight Hair Original + Jawline + Eye Bags + Narrow Eyes
Figure6:Composingmultipleattributespersistinappearance. Wedemonstratehowwecanapply
multipleeditsinw2wspacewithoutsignificantdegradationoftheoriginalidentityorinterference
withotherconcepts. Theseeditspersistandmaintaintheirappearanceacrossdifferentgeneration
seedsandprompts.
Togeneratesamples,wefixasetofpromptsandrandomseedswhichareusedasinputtotheheld-out
identitymodels. Then,wechooseasetofidentity-specificmanipulations. Forprompt-basedediting,
weaugmenttheattributedescriptiontothesetoffixedprompts(e.g., “chubby[v]person"). For
ConceptSlidersandw2w,weapplytheweightspaceeditdirectionstothepersonalizedmodelwitha
fixednormwhichdeterminestheeditstrength. Thenormiscalculatedusingthemaximumprojection
componentontotheeditdirectionamongthetrainingsetofmodelweights.
w2weditsareidentitypreservinganddisentangled. Weevaluateoverarangeofidentity-specific
attributesandpresentthree(gender,chubby,narroweyes)inTab.1.Editsinw2wpreservetheidentity
oftheoriginalsubjectasmeasuredbytheIDscore. Theseeditsaresemanticallyalignedwiththe
desiredeffectasindicatedbytheCLIPscorewhileminimallyinterferingwithothervisualconcepts,
asmeasuredbyLPIPS.WenotethattheCLIPscorecanbenoisyinthissettingastextcaptionscan
betoocoarsetodescribeattributesasnuancedasthoserelatedtothehumanface.
Qualitatively, w2w edits make the minimal amount of changes to achieve semantic and identity-
preservingedits(Fig.5). Forinstance,changingthegenderofthemandoesnotsignificantlychange
thefacialstructureorhair,unlikeConceptSlidersorpromptingwithtextdescriptions. Promptinghas
inconsistentresults,eithercreatingnoeffectormakingdrasticchanges. ConceptSliderstendsto
makecaricaturizedeffects,suchasmakingthemancartoonishlychubbyandbaby-like.
Composing edits. Edit directions in w2w space can be composed linearly as shown in Fig. 6.
The composed edits persist in appearance across different generations, binding to the identity.
Furthermore,theeditedweightsresultinanewmodel,wherethesubjecthasdifferentattributeswhile
stillmaintainingasmuchoftheprioridentity. Asweoperateonaweightmanifold,minimalchanges
aremadetootherconcepts,suchasscenelayoutorotherpeople. Forinstance,inFig.6,addingedits
tothewomandoesnotinterferewithObamastandingbyher.
4.4 InvertingSubjects
Evaluationprotocol. Wemeasurew2wspace’sabilitytorepresentnovelidentitiesbyinvertinga
setof100randomFFHQ[23]faceimages. Wefollowourinversionobjectivefromeq.3. Wethen
provideasetofdiversepromptstogeneratemultipleimagesandfollowtheidentitypreservation
metricfromSec.4.3tomeasuresubjectfidelity. ImplementationdetailsareprovidedinAppendixC.
WecompareourresultstotwoapproachesthatuseDreamboothwithrank-1LoRA.Thefirstistrained
on a single image. The second is trained on multiple images of each identity. We generate such
7Input Inversion +Chubby Input Inversion +Pointy Nose Input Inversion +Hair
Figure7: Singleimageinversionreconstructsidentityandenableseditinginw2wspace. Inverted
identities can be composed in novel contexts. Additionally, applying our discovered semantic
directionseditsattributesthatpersistinappearanceacrossgenerationseedsandprompts.
Input Projection Input Projection Input Projection
Figure8: Projectingout-of-distributionidentities. Weshowthatourinversionmethodcanconvert
unrealisticidentitiesintorealisticrenderingswithin-domainfacialfeatures. Theresultingidentity
canbecomposedinnovelscenes,suchasplayingtennisorrenderedintootherartisticdomains.
imagesbyfollowingouridentitydatasetconstructionfromSec.4.1). Thisapproachcanbeviewedas
apseudo-upperboundonmodelingidentityasitusesmultipleimages.
w2wspaceprovidesastrongidentityprior. Invertingasingleimageintow2wspaceimproveson
thesingleimageDreamboothbaselineandclosesthegapwiththeDreamboothbaselinethatuses
multipleidentityimages(Tab.2). ConductingDreamboothfine-tuningwithasingleimageinthe
originalweightspaceleadstoimageoverfittingandpoorsubjectreconstructionasindicatedbya
lowerIDscore. Incontrast,byconstrainingtheoptimizedweightstolieonamanifoldofidentity
weights,w2winversioninheritstherichpriorsofthemodelsusedtodiscoverthespace. Assuch,
itcanextractahigh-fidelityidentitythatisconsistentandcompositionalacrossgenerations. We
presentqualitativecomparisonsagainstDreamboothandsingle-imageDreamboothinAppendixC.
Table2: w2wInversionclosesthegap
Invertedmodelsareeditable. Fig.7demonstratesthat withDreambooth.
a diverse set of identities can be faithfully represented
inw2wspace. Afterinversion,theencodedidentitycan Method Single-Image IDScore↑
becomposedinnovelcontextsandposes. Forinstance,
DB-LoRA × 0.69±0.01
theinvertedman(rightmostexample)canbeseenposing DB-LoRA ✓ 0.43±0.03
withTaylorSwiftorrenderedasastatue. Moreover,se- w2w ✓ 0.64±0.01
manticeditscanbeappliedtotheinvertedmodelswhile
maintainingappearanceacrossgenerations.
4.5 Out-of-DistributionProjection
w2w space captures out-of-distribution identities. We follow the w2w inversion method from
Sec.4.4toprojectimagesofunrealisticidentities(e.g.,paintings,cartoons,etc.) ontotheweights
manifold,andpresentthesequalitativeresultsinFig.8. Byconstrainingtheoptimizedmodeltolive
inw2wspace,theinvertedidentitiesareconvertedintorealisticrenditionsofthestylizedidentities,
capturingprominentfacialfeatures. InFig.8,noticehowtheinvertedidentitiesgenerateasimilar
blondehairstyleandnosestructureinthefirstexample,definedjawlineandlipshapeinthesecond
example,andheadshapeandbignoseinthelastexample. Asalsoshowninthefigure,theinverted
identitiescanalsobetranslatedtootherartisticdomainsusingtextprompts. Wepresentavarietyof
domainsprojectedintow2wspaceinAppendixD.
4.6 EffectofNumberofModelsSpanningw2wSpace
Weablatethenumberofmodelsusedtocreatew2wspaceandinvestigatetheexpressivenessofthe
resultingspace. Inparticular,wemeasurethedegreeofentanglementamongtheeditdirectionand
howwellthisspacecancaptureidentity.
8
gnilims
sserd
eulb
a ni
puesolc
ecaf
sserd
a ni
gnisop
eutats
a sa
tfiwS
rolyaT
htiw1.0 0.7
0.6 0.8
0.5
0.6
0.4
0.4 0.3
Mean 0.2
0.2 Black Hair - Pale Skin
Young - Bald 0.1 w2w inversion
0.0 Male - Beard 0.0 DB-LoRA
101 102 103 104 105 102 103 104 105
# of Models # of Models
Figure9:Scalingourmodelweightsfurtherdis- Figure 10: Scaling the number of models im-
entanglesclassifierdirections. Wehighlightthe proves identity preservation. As the span
trendindisentanglementofthreeexampleswhere of w2w space increases, inversion can recon-
attributesmaybestronglycorrelatedamongiden- structsingle-imageidentitiesmorefaithfully,ap-
tities. As the number of models (Identities) is proachingthepseudo-upperboundofmulti-image
increased,thefeaturesarelessentangled. Dreambooth(DB-LoRA).
Disentanglementvs. thenumberofmodels. Wefindthatscalingthenumberofmodelsinour
datasetofweightsleadstolessentanglededitdirectionsinw2wspace(Fig.9). Wevarythenumber
of models in our dataset of weights and reapply PCA to establish a basis. We then measure the
absolutevalueofcosinesimilarity(lowerisbetter)betweenallpairsoflinearclassifierdirections
foundforCelebAlabels. Werepeatthisaswescalethenumberofmodelweightsusedtotrainthe
classifiers. Wereportthemeanandstandarddeviationforthesescores, alongwiththreenotable
semanticdirectionpairs. Weobserveatrendindecreasingcosinesimilarity. Notably,pairssuchas
“BlackHair-PaleSkin,”“Young-Bald,”and“Male-Beard”whichmaycorrelateinthedistribution
ofidentities,becomelesscorrelatedaswescaleourdatasetofmodelweights.
Identitypreservationvs. thenumberofmodels. Weobservethataswescalethenumberofmodels
inourdatasetofweights,identitiesaremorefaithfullyrepresentedinw2wspace(Fig.10). Wefollow
thesameprocedureasthedisentanglementablation,reapplyingPCAtoestablishabasisbasedon
thedatasetofmodelweights. Next,followingSec.4.4,weoptimizecoefficientsforthisbasisand
measure the average ID score over the 100 inverted FFHQ evaluation identities. As each model
in our dataset encodes a different instance of an identity, growing thisdataset increases the span
ofw2wspaceanditsabilitytocapturemorediverseidentities. Weplottheaveragemulti-image
DreamboothLoRA(DB-LoRA)IDscorefromSec.4.4,whichisagnostictoourdatasetofmodels.
Thisestablishesapseudo-upperboundonidentitypreservation. Scalingenablesw2wtorepresent
identitiesgivenasingleimagewithperformanceapproachingthatoftraditionalDreamboothwith
LoRA,whichusesmultipleimagesandtrainsinahigherdimensionalspace.
5 Limitations
Input Inversion
Aswithanydata-drivenmethod,w2wspacein-
herits the biases of the data used to discover
it. For instance, co-occurring attributes in the
identity-encoding models would cause linear
classifierdirectionstoentanglethem(e.g. gen-
derandfacialhair). However,aswescalethe
number of models, spurious correlations will
dropasevidencedbyFig.9. Thesedirections
arealsolimitedbythelabelspresentinCelebA. Figure11: weights2weightsfailstocaptureiden-
Additionally,thespanofthew2wspaceisdic-
titieswithundersampledattributes.
tatedbythemodelsusedtocreateit. Thus,w2w
spacecanstruggletorepresentmorecomplexidentitiesasseeninFig.11. Inversioninthesecases
amountstoprojectingontotheclosestidentityontheweightsmanifold. Despitetheselimitations,
ouranalysisonthesizeofthemodeldatasetrevealsthatformingaspaceusingalargerandmore
diversesetofidentity-encodingmodelscanmitigatethislimitation.
9
tnemelgnatnE
erocS
DI6 DiscussionandBroaderImpact
Wepresentedaparadigmforrepresentingdiffusionmodelweightsasapointinaspacedefinedby
othercustomizedmodels–weights2weights(w2w)space. Thisenabledapplicationsanalogousto
thoseofagenerativelatentspace–inversion,editing,andsampling–butproducingmodelweights
ratherthanimages. Wedemonstratedtheseapplicationsonmodelweightsencodinghumanidentities.
Althoughtheseapplicationscouldenablemaliciousmanipulationofrealhumanidentities,wehope
thecommunityusestheframeworktoexplorevisualcreativityaswellasutilizethisinterpretable
spaceforcontrollingmodelsforsafety. Wehypothesizethatsuchaframeworkcangeneralizetoother
concepts,beyondfacesandidentities,andplantoinvestigateitinfuturework.
Acknowledgements
The authors would like to thank Grace Luo, Lisa Dunlap, Konpat Preechakul, Sheng-Yu Wang,
StephanieFu,OrPatashnik,DanielCohen-Or,andSergeyTulyakovforhelpfuldiscussions. ADis
supportedbytheUSDepartmentofEnergyComputationalScienceGraduateFellowship. Partof
theworkwascompletedbyADasaninternwithSnapInc. YGisfundedbytheGoogleFellowship.
AdditionalfundingcamefromONRMURI.
References
[1] RameenAbdal,YipengQin,andPeterWonka. Image2stylegan: Howtoembedimagesintothe
styleganlatentspace? InProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
vision,pages4432–4441,2019.
[2] Stefan Andreas Baumann, Felix Krause, Michael Neumayr, Nick Stracke, Vincent Tao Hu,
andBjörnOmmer. Continuous,subject-specificattributecontrolint2imodelsbyidentifying
semanticdirections. arXivpreprintarXiv:2403.17064,2024.
[3] Volker Blanz and Thomas Vetter. A morphable model for the synthesis of 3d faces. In
Proceedingsofthe26thAnnualConferenceonComputerGraphicsandInteractiveTechniques,
SIGGRAPH’99,page187–194,USA,1999.ACMPress/Addison-WesleyPublishingCo.
[4] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng.
Masactrl: Tuning-freemutualself-attentioncontrolforconsistentimagesynthesisandediting.
InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages22560–
22570,2023.
[5] TimothyFCootes,GarethJEdwards,andChristopherJTaylor. Activeappearancemodels. In
ComputerVision—ECCV’98:5thEuropeanConferenceonComputerVisionFreiburg,Germany,
June2–6,1998Proceedings,VolumeII5,pages484–498.Springer,1998.
[6] YusufDalvaandPinarYanardag. Noiseclr: Acontrastivelearningapproachforunsupervised
discoveryofinterpretabledirectionsindiffusionmodels. arXivpreprintarXiv:2312.05390,
2023.
[7] LaurentDinh,JaschaSohl-Dickstein,andSamyBengio. Densityestimationusingrealnvp. In
InternationalConferenceonLearningRepresentations,2016.
[8] Ziya Erkoç, Fangchang Ma, Qi Shan, Matthias Nießner, and Angela Dai. Hyperdiffusion:
Generatingimplicitneuralfieldswithweight-spacediffusion. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision,pages14300–14310,2023.
[9] PatrickEsser,RobinRombach,andBjornOmmer. Tamingtransformersforhigh-resolution
imagesynthesis. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages12873–12883,2021.
[10] RinonGal,YuvalAlaluf,YuvalAtzmon,OrPatashnik,AmitHaimBermano,GalChechik,and
DanielCohen-or. Animageisworthoneword: Personalizingtext-to-imagegenerationusing
textualinversion. InTheEleventhInternationalConferenceonLearningRepresentations,2022.
10[11] RohitGandikota,JoannaMaterzynska,TingruiZhou,AntonioTorralba,andDavidBau.Concept
sliders: Loraadaptorsforprecisecontrolindiffusionmodels. arXivpreprintarXiv:2311.12092,
2023.
[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair,AaronCourville,andYoshuaBengio. Generativeadversarialnets. Advancesinneural
informationprocessingsystems,27,2014.
[13] DavidHa,AndrewMDai,andQuocVLe. Hypernetworks. InInternationalConferenceon
LearningRepresentations,2016.
[14] ErikHärkönen,AaronHertzmann,JaakkoLehtinen,andSylvainParis. Ganspace: Discovering
interpretablegancontrols. arXivpreprintarXiv:2004.02546,2020.
[15] JackHessel,AriHoltzman,MaxwellForbes,RonanLeBras,andYejinChoi. Clipscore: A
reference-freeevaluationmetricforimagecaptioning. arXivpreprintarXiv:2104.08718,2021.
[16] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advances
inneuralinformationprocessingsystems,33:6840–6851,2020.
[17] EdwardJHu,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,Weizhu
Chen,etal. Lora: Low-rankadaptationoflargelanguagemodels. InInternationalConference
onLearningRepresentations,2021.
[18] GabrielIlharco, MarcoTulioRibeiro, MitchellWortsman, LudwigSchmidt, HannanehHa-
jishirzi,andAliFarhadi. Editingmodelswithtaskarithmetic. InTheEleventhInternational
ConferenceonLearningRepresentations,2022.
[19] Ali Jahanian, Lucy Chai, and Phillip Isola. On the" steerability" of generative adversarial
networks. InInternationalConferenceonLearningRepresentations,2019.
[20] MingukKang,Jun-YanZhu,RichardZhang,JaesikPark,EliShechtman,SylvainParis,and
TaesungPark. Scalingupgansfortext-to-imagesynthesis. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages10124–10134,2023.
[21] TeroKarras, MiikaAittala, JanneHellsten, SamuliLaine, JaakkoLehtinen, andTimoAila.
Traininggenerativeadversarialnetworkswithlimiteddata. Advancesinneuralinformation
processingsystems,33:12104–12114,2020.
[22] TeroKarras,MiikaAittala,SamuliLaine,ErikHärkönen,JanneHellsten,JaakkoLehtinen,
andTimoAila. Alias-freegenerativeadversarialnetworks. Advancesinneuralinformation
processingsystems,34:852–863,2021.
[23] TeroKarras,SamuliLaine,andTimoAila. Astyle-basedgeneratorarchitectureforgenerative
adversarialnetworks. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pages4401–4410,2019.
[24] TeroKarras, SamuliLaine, MiikaAittala, JanneHellsten, JaakkoLehtinen, andTimoAila.
Analyzing and improving the image quality of stylegan. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages8110–8119,2020.
[25] DiederikPKingmaandJimmyBa.Adam:amethodforstochasticoptimization.InInternational
ConferenceonLearningRepresentations,2014.
[26] DiederikPKingmaandMaxWelling. Auto-encodingvariationalbayes. InternationalConfer-
enceonLearningRepresentations,2014.
[27] DurkPKingmaandPrafullaDhariwal. Glow: Generativeflowwithinvertible1x1convolutions.
Advancesinneuralinformationprocessingsystems,31,2018.
[28] NupurKumari, BingliangZhang, RichardZhang, EliShechtman, andJun-YanZhu. Multi-
conceptcustomizationoftext-to-imagediffusion. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages1931–1941,2023.
11[29] MingiKwon,JaeseokJeong,andYoungjungUh. Diffusionmodelsalreadyhaveasemantic
latentspace. InTheEleventhInternationalConferenceonLearningRepresentations,2022.
[30] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in
the wild. In Proceedings of the IEEE international conference on computer vision, pages
3730–3738,2015.
[31] TomasMikolov,IlyaSutskever,KaiChen,GregSCorrado,andJeffDean. Distributedrepre-
sentationsofwordsandphrasesandtheircompositionality. Advancesinneuralinformation
processingsystems,26,2013.
[32] RonMokady,AmirHertz,KfirAberman,YaelPritch,andDanielCohen-Or. Null-textinversion
for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages6038–6047,2023.
[33] YotamNitzan,KfirAberman,QiuruiHe,OrlyLiba,MichalYarom,YossiGandelsman,Inbar
Mosseri,YaelPritch,andDanielCohen-Or. Mystyle: Apersonalizedgenerativeprior. ACM
TransactionsonGraphics(TOG),41(6):1–10,2022.
[34] Yong-HyunPark,MingiKwon,JaewoongChoi,JunghyoJo,andYoungjungUh. Understanding
thelatentspaceofdiffusionmodelsthroughthelensofriemanniangeometry. Advancesin
NeuralInformationProcessingSystems,36:24129–24142,2023.
[35] WilliamPeebles,IlijaRadosavovic,TimBrooks,AlexeiAEfros,andJitendraMalik. Learning
tolearnwithgenerativemodelsofneuralnetworkcheckpoints.arXivpreprintarXiv:2209.12892,
2022.
[36] RyanPo,WangYifan,VladislavGolyanik,KfirAberman,JonathanTBarron,AmitHBermano,
EricRyanChan,TaliDekel,AleksanderHolynski,AngjooKanazawa,etal. Stateofthearton
diffusionmodelsforvisualcomputing. arXivpreprintarXiv:2310.07204,2023.
[37] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,
pages8748–8763.PMLR,2021.
[38] AlecRadford,LukeMetz,andSoumithChintala. Unsupervisedrepresentationlearningwith
deepconvolutionalgenerativeadversarialnetworks. arXivpreprintarXiv:1511.06434,2015.
[39] IlijaRadosavovic,JustinJohnson,SainingXie,Wan-YenLo,andPiotrDollár. Onnetwork
designspacesforvisualrecognition. InProceedingsoftheIEEE/CVFinternationalconference
oncomputervision,pages1882–1890,2019.
[40] IlijaRadosavovic,RajPrateekKosaraju,RossGirshick,KaimingHe,andPiotrDollár. Design-
ingnetworkdesignspaces. InProceedingsoftheIEEE/CVFconferenceoncomputervision
andpatternrecognition,pages10428–10436,2020.
[41] ShauliRavfogel,YanaiElazar,HilaGonen,MichaelTwiton,andYoavGoldberg. Nullitout:
Guarding protected attributes by iterative nullspace projection. In Proceedings of the 58th
AnnualMeetingoftheAssociationforComputationalLinguistics,pages7237–7256,2020.
[42] Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In
Internationalconferenceonmachinelearning,pages1530–1538.PMLR,2015.
[43] DaniloJimenezRezende,ShakirMohamed,andDaanWierstra. Stochasticbackpropagation
andapproximateinferenceindeepgenerativemodels. InInternationalconferenceonmachine
learning,pages1278–1286.PMLR,2014.
[44] Daniel Roich, Ron Mokady, Amit H Bermano, and Daniel Cohen-Or. Pivotal tuning for
latent-basededitingofrealimages. ACMTransactionsongraphics(TOG),42(1):1–13,2022.
[45] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages10684–10695,2022.
12[46] DuncanARowlandandDavidIPerrett. Manipulatingfacialappearancethroughshapeand
color. IEEEcomputergraphicsandapplications,15(5):70–76,1995.
[47] NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfirAberman.
Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
22500–22510,2023.
[48] NatanielRuiz,YuanzhenLi,VarunJampani,WeiWei,TingboHou,YaelPritch,NealWad-
hwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast
personalizationoftext-to-imagemodels. arXivpreprintarXiv:2307.06949,2023.
[49] SimoRyu. Low-rankadaptationforfasttext-to-imagediffusionfine-tuning,2023.
[50] FlorianSchroff,DmitryKalenichenko,andJamesPhilbin. Facenet: Aunifiedembeddingfor
facerecognitionandclustering. InProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pages815–823,2015.
[51] KonstantinSchürholt,DimcheKostadinov,andDamianBorth. Self-supervisedrepresentation
learningonneuralnetworkweightsformodelcharacteristicprediction. AdvancesinNeural
InformationProcessingSystems,34:16481–16493,2021.
[52] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and
VarunJampani. Ziplora: Anysubjectinanystylebyeffectivelymergingloras. arXivpreprint
arXiv:2311.13600,2023.
[53] YujunShen,JinjinGu,XiaoouTang,andBoleiZhou. Interpretingthelatentspaceofgansfor
semanticfaceediting. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pages9243–9252,2020.
[54] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli. Deepunsuper-
visedlearningusingnonequilibriumthermodynamics. InInternationalconferenceonmachine
learning,pages2256–2265.PMLR,2015.
[55] JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels. In
InternationalConferenceonLearningRepresentations,2020.
[56] OmerTov, YuvalAlaluf, YotamNitzan, OrPatashnik, andDanielCohen-Or. Designingan
encoderforstyleganimagemanipulation. ACMTransactionsonGraphics(TOG),40(4):1–14,
2021.
[57] Kuan-ChiehWang,DaniilOstashev,YuweiFang,SergeyTulyakov,andKfirAberman. Moa:
Mixture-of-attention for subject-context disentanglement in personalized image generation.
arXive-prints,pagesarXiv–2404,2024.
[58] Kuan-ChiehWang,PaulVicol,JamesLucas,LiGu,RogerGrosse,andRichardZemel. Adver-
sarialdistillationofbayesianneuralnetworkposteriors. InInternationalconferenceonmachine
learning,pages5190–5199.PMLR,2018.
[59] MitchellWortsman,GabrielIlharco,SamirYaGadre,RebeccaRoelofs,RaphaelGontijo-Lopes,
AriSMorcos,HongseokNamkoong,AliFarhadi,YairCarmon,SimonKornblith,etal. Model
soups: averagingweightsofmultiplefine-tunedmodelsimprovesaccuracywithoutincreasing
inferencetime. InInternationalconferenceonmachinelearning,pages23965–23998.PMLR,
2022.
[60] Weihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang.
Gan inversion: A survey. IEEE transactions on pattern analysis and machine intelligence,
45(3):3121–3138,2022.
[61] GuangxuanXiao,TianweiYin,WilliamTFreeman,FrédoDurand,andSongHan. Fastcom-
poser: Tuning-free multi-subject image generation with localized attention. arXiv preprint
arXiv:2305.10431,2023.
13[62] GeYuan,XiaodongCun,YongZhang,MaomaoLi,ChenyangQi,XintaoWang,YingShan,
and Huicheng Zheng. Inserting anybody in diffusion models via celeb basis. Advances in
NeuralInformationProcessingSystems,36,2024.
[63] KaipengZhang,ZhanpengZhang,ZhifengLi,andYuQiao. Jointfacedetectionandalignment
usingmultitaskcascadedconvolutionalnetworks. IEEEsignalprocessingletters,23(10):1499–
1503,2016.
[64] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunrea-
sonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE
conferenceoncomputervisionandpatternrecognition,pages586–595,2018.
[65] Jun-Yan Zhu, Philipp Krähenbühl, Eli Shechtman, and Alexei A Efros. Generative visual
manipulationonthenaturalimagemanifold. InComputerVision–ECCV2016: 14thEuropean
Conference,Amsterdam,TheNetherlands,October11-14,2016,Proceedings,PartV14,pages
597–613.Springer,2016.
14A Sampling
Wepresentadditionalexamplesofmodelssampledfromw2wspaceinFig.12. Thesampledmodels
encodeadiversearrayofidentitieswhicharenotcopiedfromthedatasetofmodelweights,asseen
bycomparingthemtothenearestneighbormodels. However,thereareattributesborrowedfromthe
nearestneighborswhichvisuallyappearinthesampledidentity. Forinstance,thesampledmanin
thefirstrowsharesasimilarjawlinetothenearestneighboridentity. Thesampledidentitiesalso
demonstratethesameabilityastheoriginaltrainingidentitiestobecomposedintonovelcontexts. A
varietyofpromptsareusedinFig.12,yettheidentitiesareconsistent.
Sampled Identity Nearest Neighbor
Figure12:Sampledidentity-encodingmodelsfromw2wspaceandtheirnearestneighbormodels.
Thesampledidentitiessharesomecharacteristicswiththenearestneighbors,butarestilldistinct.
Theseidentitiescanalsobecomposedintonovelcontextslikeastandardcustomizeddiffusionmodel.
15B ComposingEdits
Wedisplayadditionalexamplesofapplyingeditsinw2wspacebasedonthedirectionsdiscovered
usinglinearclassifiersandCelebAlabels. InFig.13,wedemonstratehowthestrengthoftheseedits
canbemodulatedandcombinedwithminimalinterference. Theseeditsareapparenteveninmore
complexscenesbeyondfaceimages. Also,theeditsdonotdegradeotherpresentconcepts,suchas
thedogneartheman(topleftexample).
InFigs.14and15,wedemonstratehowmultipleeditscanbeprogressivelyaddedinadisentangled
fashion with minimal degradation to the identity. Additionally, since we operate in a subspace
ofweightspace,theseeditspersistwithaconsistentappearanceacrossdifferentgenerations. For
instance,eventhemanexhibitstheeditsasapaintinginFig.14.
Bald Chubby
Pointy Nose Age
Figure13: Multipleeditscanbecontrolledinacontinuousmanner.
16
draeB
slruC
riaH
kcalB
elaPOriginal +Chubby +Mustache +Big Eyes +Balding Original +Flat Brows +Bangs +Straight Hair +Old
Original +Chubby +Mustache +Big Eyes +Balding Original +Flat Brows +Bangs +Straight Hair +Old
Figure14: Composingfourdifferenteditswithminimalidentitydegradation. Theseeditsbindto
theidentityandpersistinappearanceacrossmultiplegenerationseedsandprompts.
17Original +Pale +Small Lips +Male
Original +Pointy Nose +Eye Bags +Big Lips
Original +Cheekbones +Curls +Thick Brows
Figure 15: Additional examples of composing multiple edits. We provide more examples of
semanticeditsbasedonlabelsavailablefromCelebA.
18C Inversion
Wepresentadditionaldetailsonw2winversionandcomparisonsagainsttrainingDreamboothLoRA
onasingleimagevs. multipleimages.
Implementation Details: To conduct w2w inversion, we train on a single image following the
objectivefromeq.3. Wequalitativelyfindthatoptimizing10,000principalcomponentcoefficients
balancesidentitypreservationwitheditability. ThisisdiscussedinAppendixF.Weoptimizefor
400epochs,usingAdam[25]withlearningrate0.1,β =0.9,β =0.999andwithweightdecay
1 2
factor1e-10. ForconductingDreamboothfine-tuning,wefollowtheimplementationfromHugging
Face3usingLoRAwithrank1. Tocreateadatasetofmultipleimagesforanidentity,wefollowthe
procedurefromSec.4.4.
w2w inversion is more efficient than previous methods. Inversion into w2w space results in a
significant speedup in optimization as seen in Tab. 3, where we measure the training time on a
single NVIDIA A100 GPU. Standard Dreambooth fine-tuning operates on the full weight space
and incorporates an additional prior preservation loss which typically requires hundreds of prior
images. Incontrast, weonlyoptimizeastandarddenoisingobjectiveonasingleimagewithina
low-dimensional weight subspace. Despite operating with lower dimensionality, w2w inversion
performscloselytostandardDreamboothfine-tuningonmultipleimageswithLoRA.
Single Image w2w Table3: Inversionintow2wspacebalancesidentityprSeisnegrlvea Itmioangeandefwfi2ciwency.
Input DB-LoRA DB-LoRA Inversion Method InpSuintgle-Image #ParamDB-OLpotR.TAimeD(sB) -LIodRenAtityFiIdnevleitrysi↑on
DB-LoRA × 99,648 220 0.69±0.01
DB-LoRA ✓ 99,648 200 0.43±0.03
w2wInversion ✓ 10,000 55 0.64±0.01
QualitativeInversionComparison. InFigs.16and17,wepresentqualitativecomparisonsofw2w
inversionagainstDreamboothtrainedwithmultipleimagesandasingleimage. Althoughmult-image
Dreamboothslightlyoutperformsw2winversioninidentitypreservations,itssamplestendtolack
realismcomparedtow2winversion. Wehypothesizethatthismaybeduetousinggeneratedimages
for prior preservation and training on synthetic identity images. Dreambooth trained on a single
imageeithergeneratesanartifactedversionoftheoriginalimageorrandomidentities. Noticehow
inversionintow2wspaceisevenabletocapturekeycharacteristicsofthechildalthoughbabiesare
nearlytocompletelyabsentintheidentitesbasedonCelebAusedtofine-tuneourdatasetofmodels.
Single Image w2w Single Image w2w
Input DB-LoRA DB-LoRA Inversion Input DB-LoRA DB-LoRA Inversion
Figure16: Inversionintow2wspacepreservesidentityandrealism. WecompareagainstDream-
boothfine-tuningwithLoRAonmultipleimagesandasingleimage.
Single Image w2w Single Image w2w
3https://github.com/Ihnupgugtingface/peft
DB-LoRA DB-LoRA Inversion Input DB-LoRA DB-LoRA Inversion
19
Input DB-LoRA S Din Bgl -e L I om Ra Age Invw e2 rsw ion III nnn ppp uuu ttt DDD BBB --- LLL ooo RRR AAA SSS Di DDii nnn BBBggg lll ---eee LLL III ooommm RRRaaa AAAggg eee III nnn vvvwww eee222 rrr ssw sww iii ooo nnn I In np pu ut t D DB B- -L Lo oR RA A S S D Di in n B Bg gl l - -e e L L I I o om m R Ra a A Ag ge e I In nv vw w e e2 2 r rs sw w i io on n
Single Image w2w S Si in ng gl le e I Im ma ag ge e w w2 2w w Single Image w2w
Input DB-LoRA DB-LoRA Inversion I In np pu ut t D DB B- -L Lo oR RA A D DB B- -L Lo oR RA A I In nv ve er rs si io on n Input DB-LoRA DB-LoRA InversionSingle Image w2w Single Image w2w
Input DB-LoRA DB-LoRA Inversion Input DB-LoRA DB-LoRA Inversion
Single Image w2w Single Image w2w
Input DB-LoRA DB-LoRA Inversion Input DB-LoRA DB-LoRA Inversion
Single Image w2w Single Image w2w
Input DB-LoRA DB-LoRA Inversion Input DB-LoRA DB-LoRA Inversion
Input DB-LoRA S Din Bgl -e L I om Ra Age Invw e2 rsw ion III nnn ppp uuu ttt DDD BBB --- LLL ooo RRR AAA SSS Di DDii nnn BBBggg lll ---eee LLL III ooommm RRRaaa AAAggg eee III nnn vvvwww eee222 rrr ssw sww iii ooo nnn I In np pu ut t D DB B- -L Lo oR RA A S S D Di in n B Bg gl l - -e e L L I I o om m R Ra a A Ag ge e I In nv vw w e e2 2 r rs sw w i io on n
Single Image w2w S Si in ng gl le e I Im ma ag ge e w w2 2w w Single Image w2w
Input DB-LoRA DB-LoRA Inversion I In np pu ut t D DB B- -L Lo oR RA A D DB B- -L Lo oR RA A I In nv ve er rs si io on n Input DB-LoRA DB-LoRA Inversion
Figure17: Inversionintow2wspacepreservesidentityandrealism(cont.).
20D OutofDistributionProjection
Additionalexamplesofout-of-distributionprojectionsaredisplayedinFig.18. Adiversearrayof
stylesandsubjects(e.g. paintings,sketches,non-humans)canbedistilledintoamodelinw2wspace.
Afterembeddingan identityintothisspace, themodel still retainsthecompositionalityand rich
priorsofastandardpersonalizedmodel. Forinstance,wecangenerateimagesusingpromptssuch
as“[v]personwritingatadesk”(topexample),“[v]personwithadog”(middleexample),or“a
paintingof[v]personpaintingonacanvas”(bottomexample).
Input Projection
Input Projection
Input Projection
Figure18: Projectionintow2wspacegeneralizestoavarietyofinputs. Arangeofstylesand
entitiescanbeinvertedintoarealisticidentityinthisspace. Onceamodelisobtained, itcanbe
promptedtogeneratetheidentityinavarietyofcontexts.
21E IdentityDatasets
In Fig. 19, we present examples of synthetic identity datasets used to conduct our Dreambooth
fine-tuningasdiscussedinSec4. Eachdatasetisasetoftenimagesgeneratedwith[57]conditioned
onasingleCelebA[30]imagesassociatedwithbinaryattributelabels. Notethatweonlydisplaya
subsetofimagesperidentityinthefigure. Creatingthesesyntheticdatasetsreducesintra-dataset
diversityandcreatesamoreconsistentappearanceforeachsubject. Forinstance,thefirsttworows
inthefigurearethesameidentity,butlookdrasticallydifferent. Soweinsteadtreatthemasdifferent
identitiesassociatedwithadifferentsetofimages.
CelebA Prior Generated Dataset Examples
Figure19: Fine-tuningonsyntheticexamplesallowsDreamboothfine-tuningtodistillacon-
sistentidentity. TheleftcolumnshowsaCelebAimageusedtoconditiongenerationofasetof
identity-consistentimagesintherightcolumnassociatedwiththatidentity.Theconsistentappearance
oftheidentityenablesamoreconsistentidentityencoding.
22F PrincipalComponentBasis
Inthissection,weanalyzevariouspropertiesofthePrincipalComponent(PC)basisusedtodefine
w2wSpace. WeinvestigatethedistributionofPCcoefficientsandtheeffectofthenumberofPCson
identityeditingandinversion.
DistributionofPCCoefficients. Weplotthehistogramofthecoefficientvaluesforthefirstthree
PrincipalComponentsinFig.20. TheyappearroughlyGaussian. Next,werescalethecoefficients
forthesethreecomponentstounitvarianceforvisualizationpurposes. Wethenplotthepairwise
jointdistributionsfortheminFig.21. Thecircularshapesindicatesroughlydiagonalcovariances.
AlthoughthejointoverothercombinationsofPrincipalComponentsmayexhibitdifferentproperties,
theseresultsmotivateustomodelthePCsasindependentGaussians,leadingtothew2wsampling
strategyfromSec.3.2.
Number of Principal Components for Identity Editing We empirically observe that training
classifiersbasedonthe1000dimensionalPCrepresentations(first1000PCs)ofthemodelweights
resultsinthemostsemanticallyalignedanddisentanglededitsdirections. Wevisualizeacomparison
forthe“goatee"directioninFig.22. Afterfindingthedirection,wecalculatethemaximumprojection
componentontotheeditdirectionamongthetrainingsetofmodelweights. Thisdeterminestheedit
strength. Asseeninthefigure,restrictingtothefirst100PrincipalComponentsmaybetoocoarse
toachievethefine-grainededit,insteadrelyingonspuriouscuessuchasskincolor. Trainingwith
thefirst10,000PrincipalComponentssuffersfromthecurseofdimensionalityandthediscovered
directionmayeditotherconceptssuchaseyecolororclothes. Findingthedirectionusingthefirst
1000PrincipalComponentsachievesthedesirededitwithminimalentanglementwithotherconcepts.
NumberofPrincipalComponentsforIdentityInversionWequalitativelyobservethatinverting
intow2wSpaceusingthefirst10,000PrincipalComponentsbalancesidentitypreservationwhilenot
overfittingtothesourceimage. WevisualizeacomparisoninFig.23,whereeachcolumnhasafixed
seedandprompt. Optimizingwiththefirst1000PCsunderfitstheidentityanddoesnotgeneratea
consistentidentity. Inversionwiththefirst20,000PrincipalComponentsoverfitstothesourceimage
ofafaceshot,whichresultsinartifactedfaceimagesdespitedifferentgenerationseedsandprompts.
Optimizingwiththefirst10,000PrincipalComponentsenjoysthebenefitsofalowerdimensional
representationthantheoriginalLoRAparameterspace(∼100,000trainableparameters),whilestill
preservingidentityandcompositionality.
Figure20: HistogramofPrincipalComponentCoefficients. ThefirstthreePrincipalComponent
coefficientsappearapproximatelyGaussian.
Figure21: PairwiseJointHistogramofPrincipalComponentCoefficients. Werescalethefirst
three principal component coefficients and plot the pairwise joint distributions for visualization
purposes. GiventhatthemarginalsareroughlyGaussian,thecircularappearanceofthejointsuggests
pairwiseindependenceforthefirstthreecomponents.
23++CGhouatbebey
OOrriiggiinnaall 1T0=01 P00C0s 10T0=08 0P0Cs 10T0 =006 0P0Cs
Figure22: EditresultswithvaryingnumberofPrincipalComponents. Trainingclassifiersto
findsemanticweightspacedirectionswiththefirst1000PrincipalComponentsachievesthemost
semanticallyalignedanddisentangledresults.
Input Inversion with 1000 PCs
Inversion with 10000 PCs
Inversion with 20000 PCs
Figure23:IdentityinversionresultswithvaryingnumberofPrincipalComponents.Weoptimize
thecoefficientsforthefirst1000,10000,and20000PrincipalComponentcoefficients. Eachcolumn
indicatesafixedgenerationseedandprompt. Inversionwiththefirst10,000componentsbalances
parameterefficiency,realism,andidentitypreservation,andwithoutoverfittingtothesingleimage.
G TimestepAnalysis
Editsinw2wSpacecorrespondtoidentityeditswithminimalinterferencewithothervisualconcepts.
Althoughnotafocus,imageeditingisachievedasabyproduct. Forfurthercontextpreservation,
editsinw2wSpacecanbeintegratedwithdelayedinjection[4,11,61],whereafterT timesteps,the
editedweightsareusedinsteadoftheoriginalones. WevisualizethisinFig.24. LargerT inthe
range[700,1000]arehelpfulformoreglobalattributechanges,whilesmaller[400,700]canbeused
formorefine-grainededits. However,bydecreasingthetimestepT,thestrengthoftheeditislostin
favorofbettercontextpreservation. Forinstance,thedog’sfaceisbetterpreservedinthesecondrow
atT =600,althoughthemanisnotaschubbycomparedtootherT.
+Chubby
Original T=1000 T=800 T =600
Figure24: InjectingtheeditedweightsatalowertimestepT betterpreservescontextatthe
expenseofeditstrengthandfidelity.
24