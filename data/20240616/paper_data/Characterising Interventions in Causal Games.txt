Characterising Interventions in Causal Games
ManujMishra1 JamesFox1,2 MichaelWooldridge1
1DepartmentofComputerScience,UniversityofOxford,UK
2LondonInitiativeforSafeAI(LISA),UK
Abstract
selects a policy – independentconditionalprobability dis-
tributions (CPDs) over actions for each of their decision
variables–tomaximisetheirexpectedutility.
Causal games are probabilistic graphicalmodels
CausalBayesiannetworkshandleinterventionsinsettings
thatenablecausalqueriestobeansweredinmulti-
without agents by cutting any edges incident to the inter-
agent settings. They extend causal Bayesian net-
vened node in the DAG to represent that the effect of an
worksbyspecifyingdecisionandutilityvariables
interventioncan only propagatedownstream.However,to
to represent the agents’ degrees of freedom and
handlehowanagentmightormightnotadapttheirpolicy
objectives. In multi-agent settings, whether each
in response to an intervention, mechanised graphs extend
agentdecidesontheirpolicybeforeorafterknow-
theregularDAGbyexplicitlyrepresentingeachvariable’s
ingthecausalinterventionisimportantasthisaf-
distribution and showing which other variables’ distribu-
fectswhethertheycanrespondtotheintervention
tions matter to an agent optimising a particular decision
by adapting their policy. Consequently, previous
rule[Hammondetal.,2023,Dawid,2002].
workincausalgamesimposedchronologicalcon-
straintsonpermissibleinterventions.Werelaxthis
byoutliningasoundandcompletesetofprimitive Related Work: The effectof causal interventionsis im-
causal interventions so the effect of any arbitrar- portant in many fields such as economics [Heckman and
ilycomplexinterventionalquerycanbestudiedin Pinto,2022,LeRoy,2004],computerscience[Brandetal.,
multi-agentsettings.Wealsodemonstrateapplica- 2023] and public health [Ahern et al., 2009, Glass et al.,
tionstothedesignofsafeAIsystemsbyconsider- 2013].However,thesefieldsusemodelsthatdonotaccount
ingcausalmechanismdesignandcommitment. for the strategic nature of multi-agent systems. Recently,
causal games [Hammondet al., 2023] were introducedto
unify the power of causal and strategic reasoning in one
1 INTRODUCTION model.Causalgamesandtheirsingle-agentvariant,causal
influence diagrams[Everittet al., 2021a],have been used
to design safe and fair AI systems [Ashurst et al., 2022,
When designing a system for rational, self-interested
Everitt et al., 2021b, Farquhar et al., 2022, Carroll et al.,
agents, it is importantto incentivise behaviourthat aligns
2023], explore reasoning patterns and deception [Pfeffer
withhigh-levelgoals,suchasmaximisingsocialwelfareor
andGal,2007,Wardetal.,2022],andidentifyagentsfrom
minimisingtheharmtootheragents.Toaddressthis,game
data [Kenton et al., 2023]. The key limitation is that ex-
theory providesseveralrepresentationsthat have different
isting workon multi-agentcausalmodelsassumesthatan
strengths and weaknesses depending on the setting. Ham-
interventioniseitherfullypost-policy(entirelyinvisible)to
mondetal.[2023]recentlyintroducedcausalgamestoex-
allagentsorfullypre-policy(entirelyvisible)toallagents
tend Pearl [2009]’s ‘causal hierarchy’ to the multi-agent
before they decide on their decision rule at each decision
setting. Causal games are graphicalrepresentationsof dy-
point.
namic non-cooperative games, which can be more com-
pactandexpressivethanextensive-formgames[Kollerand
Milch,2003,Hammondetal.,2021].LikecausalBayesian Contributions: Ourmostimportantnovelcontributionis
networks, theyuse a directed acyclic graph(DAG)to rep- toextendthetheoryofinterventionsincausalgamestobe
resent causal relationships between random variables, but abletoaccommodatearbitraryquerieswhereagentschoose
theyalsospecifydecisionandutilityvariables.Eachagent theirdecisionrulesbasedonanysubsetoftheinterventions
4202
nuJ
31
]TG.sc[
1v81390.6042:viXraΘT ΘU1 ΘU2 variable V in a graphical representation and pa V the in-
stantiationofPa .WealsodefineCh ,Anc ,Desc ,and
V V V V
Fa :=Pa ∪{V}asthechildren,ancestors,descendants,
V V
T U1 U2 andfamilyofV,respectively.Aswithpa ,theirinstantia-
V
tionsarewritteninlowercase.Weusesuperscriptstoindi-
T U1 U2 D1 D2 cateanagenti ∈ N = {1,...,n}andsubscriptstoindex
the elements of a set; for example, the decision variables
belongingtoagentiaredenotedDi ={Di,...,Di }.
1 m
D1 D2 ΠD1 ΠD2
(a) (b) 2.1 CAUSALGAMES
Figure 1: A causal game’s (a) graph and (b) mechanised
Causal games (CGs) are causal multi-agent influence di-
graphforExample1.
agrams [Koller and Milch, 2003, Hammond et al., 2023].
Influencediagramswere initially devisedto modelsingle-
agent decision problems graphically [Howard and Mathe-
(thosevisibleto them).Thisisnecessaryto discussricher
son, 2005, Miller III et al., 1976]. They are defined simi-
propertiesof causal gamesand calculate certain specifica-
larlytoaBayesiannetwork(BN)butwithadditionalutility
tions.First,inSection3.1,wepresentasoundandcomplete
variables and parameter-less decision variables. A causal
setofprimitivecausalinterventionsthatenableanycausal
Bayesiannetwork(CBN)isaBNwithedgesthatfaithfully
intervention(agamemodification)tobedecomposedinto
represent causal relationships [Pearl, 2009]. So, a CG is
one of four operations acting on CPDs or functions act-
a game-theoretic CBN, where agents select a conditional
ingonsuchdistributions.Second,inSection3.2,weprove
distributionoveractionsattheirdecisionvariablestomax-
thatthisgeneralisesHammondetal.[2023]’snotionofpre-
imise the expected cumulative value of their utility vari-
policyandpost-policyinterventions,whichassumethatin-
ables. The simplest causal intervention do(Y = y) in a
terventions are either visible to all agents (pre-policy) or
CBNorCGfixesthevaluesofvariablesY tosomey;we
no agents (post-policy), to arbitrarily complex compound
denotetheresultingjointdistributionbyPr (V).
interventions.InSection4,weexplorehowourtheoretical y
contributions are useful for both qualitative and quantita- Definition 1. A causal game (CG) is a structure M =
tivespecificationsincausalmechanismdesign.Theformer (G,θ) where G = (N,V,E) specifies a set of agents
N = {1,...,n} and a directed acyclic graph (DAG)
exploits graphical properties of the causal game’s mecha-
(V,E) where V is partitioned into chance variables X,
nised graph, and the latter formalises the effect of taxa-
decision variables D = Di, and utility variables
tion and reward schemes. Finally, in Section 5, we show i∈N
U = Ui.Theparametersθ = {θ } define
howcausalgamescanbehelpfulforrepresenting‘commit- i∈N S V V∈V\D
theCPDsPr(V |Pa ;θ )foreachnon-decisionvariable
ment’,whereoneagentcangainastrategicadvantageover suchthSatforanyparV ameV
terisationofthedecisionvariable
othersbycommittingtoapolicybeforethegamebegins. CPDs,theinducedmodelwithjointdistributionPrπ(v)is
a causal Bayesian network, i.e., G is Markov compatible
withPrπ foreveryY ⊆V andy ∈dom(Y),andthat:
2 BACKGROUND y
1 V ∈Y,visconsistentwithy,
Prπ(v|pa )=
This section reviews Hammond et al. [2023]’s Causal y V (Prπ(v|pa V) V ∈/ Y,pa
V
isconsistentwithy.
Games.Webeginwithanexample.
Figure1adepictsacausalgameforExample1.Whitecir-
Example1(Spence[1973]’sJobMarketSignallingGame).
clesrepresentchancevariables,e.g.,theworker’stempera-
Aworkerwhoiseitherhard-workingorlazyishopingtobe
ment(T)with probabilitiesp forhard-workingand1−p
hiredbyafirm.Theycanchoosetopursueuniversity edu-
forlazy.Decisionandutilityvariablesaresquaresanddia-
cationbutknowthattheywillthensufferfromthreeyears monds,respectively.Theworker’sdecision(D1:attenduni-
of studying, especially if they are lazy. The firm prefers versityornot)andutility(U1)areshowninred,whilethe
hardworkersbutisusinganautomatedhiringsystemthat firm’sdecision(D2:offerajobornot)andutility(U2)are
canonlyobservetheworker’seducation,nottheirtemper- in blue.Missing edges,like T → D2, indicate an agent’s
ament.
lackofinformation.Theworkerreceivesutility5forajob
offer but incurs a 1 or 2 cost for attending university (de-
We use capital letters V for random variables, lowercase pendingontheirtemperament).Thefirmgains3forhiring
letters v for their instantiations, and bold letters V and v a hard worker but suffers a cost of 2 if they hire a lazy
respectively for sets of variables and their instantiations. worker or an opportunity cost of 1 if they reject a hard
We let dom(V) denote the finite domain of V and let worker.ParametersθdefineconditionaldistributionsforT,
dom(V) :=× dom(V). Pa denotes the parents of U1,andU2.
V∈V VGivenacausalgameM = (G,θ),adecisionruleπ for forkthroughT andthenaforwardchainthroughD1,ora
D
D ∈ D isaCPDπ (D | Pa )andapartialpolicypro- backwardchainthroughD2andthenaforkthroughD1.If
D D
fileπ D′isasetofdecisionrulesπ DforeachD ∈D′ ⊆D, Y = ∅, thenU2 is d-connectedto U1 (U2 6⊥
G
U1 | ∅),
wherewewriteπ −D′ forthesetofdecisionrulesforeach butifY ={T,D2}thenallofthepathshavebeenblocked
D ∈ D\D′.Apolicyπi referstoπ Di,anda(full)pol- byconditioningonY andsoU2 ⊥
G
U1 |Y.
icyprofileπ = (π1,...,πn)isatupleofpolicies,where
A causal game’s regular graph G captures the dependen-
π−i := (π1,...,πi−1,πi+1,...,πn). A decision rule is
ciesbetweenobject-levelvariablesintheenvironment,but
pure if π (d | pa ) ∈ {0,1} and fully stochastic if
D D its mechanised graph mG is an enhanced representation
π (d | pa ) > 0 forall d ∈ dom(D) and eachdecision
D D revealing the strategically relevant dependencies between
contextpa ∈dom(Pa );thisholdsforapolicy(profile)
D D agents’decisionrulesandtheparameterisationofthegame
ifitholdsforalldecisionrulesinthepolicy(profile).
[Hammond et al., 2023]. Collectively, decision rules and
By combining π with the partial distribution Pr over the CPDs are known as the mechanisms M of the decision,
chance and utility variables, we obtain a jointdistribution andchance/utilityvariables,respectively.Eachobject-level
Prπ(x,d,u) := Pr(v | pa )· π (d | variableV ∈ V hasamechanismparentM representing
V∈V\D V D∈D D V
pa )overallthevariablesinM;inducinga BN.Theex- thedistributiongoverningV.Morespecifically,eachdeci-
pecD tedutilityforAQ gentigivenapolicyprofiQ leπisdefined sionDhasanewdecisionruleparentΠ D =M D andeach
astheexpectedsumoftheirutilityvariablesinthisBN,that non-decision V has a new parameter parent Θ V = M V,
isE [Ui] = Prπ(U = u)·u.Apol- whose values parameterise the CPDs. The independent
π U∈Ui u∈dom(U)
i Ec (y π˜π i,πi −is i)a [Ube is ]Pt fr oe rs ap lo ln π˜Ps ie ∈to Πpr io .fi Ale Nπ a− si hif eqE u(π ili i, bπ r− iui) m[U (i N] E≥
)
m ede gc eh sa )n .isedgraphistheresult(ithasnointer-mechanism
is a policyprofile whereeach agentplaysa best response. However,agentsselectadecisionruleπ (i.e.,thevalueof
D
Acausalgameissolvedbyfindingapolicyprofilethatsat- a decisionrule variableΠ )based onboththeparameter-
D
isfiesasolutionconcept,usuallyanNE. isation of the game (i.e., the values of the parameter vari-
ables) and the selection of the otherdecisionrules π –
Causal games offer several explainability and complexity −D
sothesedependenciesarecapturedbytheedgesfromother
advantages over extensive form games Koller and Milch
mechanisms into decision rule nodes. These reflect some
[2003]. One key advantage is that probabilistic dependen-
rationalityassumptions,capturedbyasetofrationalityre-
cies between chance and strategic variables can be ex-
ploited using the d-separation graphical criterion [Pearl, lationsR={r D} D∈Dthatrepresenthowtheagentschoose
1988].
theirdecisionrules.EachdecisionruleΠ
D
isgovernedby
aserialrelationr ⊆dom(Pa )×dom(Π ),whichac-
D ΠD D
Definition 2. A path, p, in a DAG1 G = (V,E) is a se- countsforthefactthatanagentmaynotdeterministically
quence of adjacentvariablesin V. A path p is said to be chooseasingledecisionruleπ inresponsetosomepa .
D ΠD
d-separatedbyasetofvariablesY ifandonlyif: IfalloftherationalityrelationsR aresatisfied byπ, then
π isanR-rationaloutcomeofthegame.Weoftenassume
• pcontainsachainX → W → Z orX ← W ← Z, thattheagentsareplayingbestresponsesR=RBR,sothe
oraforkX ←W →Z,andW ∈Y. RBR-rationaloutcomesaresimplytheNEofthegame.
• p contains a collider X → W ← Z and ({W} ∪
Finally, a graphical criterion R-reachability (based on d-
Desc )∩Y =∅.
W separation)determineswhichoftheseedgesarenecessary
inthemechanisedgraph,e.g.,M →Π existsifandonly
A set Y d-separates X from Z (X ⊥ Z | Y), if Y V D
G ifthechoiceofbestresponsedecisionruleΠ dependson
d-separateseverypathinGfromavariableinX toavari- D
theCPD atM (M isRBR-relevanttoΠ ).Themecha-
able in Z. Sets of variables that are not d-separated are V V D
nised graph for Example 1 (in Figure 1b) shows that Θ ,
saidtobed-connected,denotedX 6⊥ Z |Y. T
G Θ U1, and Π D2 are all RBR-relevant to Π D1 whereas Θ T,
Θ U2, and Π D1 are RBR-relevant to Π D2. In contrast to a
If X ⊥ Z | Y in G, then X and Z are probabilis-
G causalgame’sregularDAG,theremayexistcyclesbetween
tically independent conditional on Y in the sense that
mechanisms(see[Hammondetal.,2023]formoredetails).
Pr(x | y,z) = Pr(x | y), in every distribution Pr that
isMarkovcompatiblewithG andforwhichPr(y,z) > 0. So, the mechanised graph mG takes the original graph G
Conversely,ifX 6⊥ Z |Y,thenX andZ aredependent and,foreachvariableV ∈V,addsmechanismparentnode
G
conditionalonY inatleastonedistributionMarkovcom- M V and edge M V → V as well as edgesM V → Π D for
patible with G. For example, there are several paths from eachdecisionruleΠ D whereM V isRBR-relevanttoΠ D.
U2 to U1 in Figure 1a: direct forks through T or D2, a
1Weusethatd-separationremainsavalidtestforconditional
independenceincyclicgraphsPearlandDechter[1996].3 CHARACTERISINGINTERVENTIONS pa )tothejointdistributionfactorisation.Graphically,this
Y
adds a new node Y to G and adds edges X → Y for all
CausalgamesadmitqueriesonleveltwoofPearl[2009]’s X ∈ Pa and Y → Z for all Z ∈ Ch . The induced
Y Y
CausalHierarchy.Importantly,in game-theoreticsettings, distributionis
we only assume that an R-rational outcome of the game
Prπ((v∪Y) )=PrI(y |pa )· Prπ(v |pa∗)
(e.g.,anNE)ischosenratherthansomeuniquepolicypro- I Y V
file π. We therefore evaluate queries with respect to a set V Y∈V
of policy profiles, e.g., ‘if D1 = g, is it the case that for
Pa ∪Y ifV ∈Ch
all NE...’. Whenan interventiontakesplace isimportant. where,forV ∈V,Pa∗ = V Y
Hammondetal.[2023]previouslyintroducedadistinction
V (Pa
V
otherwise
between pre-policyqueries,where the interventionoccurs (4) Removing an existing object-levelvariable: Remov-
beforethepolicyprofileisselected,andpost-policyqueries, ing an existing object-level variable Y removes the CPD
where the intervention occurs after. We extend this to ac- Prπ(y | pa ) from the joint distribution factorisation.
Y
commodatearbitraryquerieswhereeachagentmakesdeci- Graphically,thisremovesthenodeY fromG andremoves
sionsbasedonthesubsetofinterventionsvisibletothem. edges X → Y for all X ∈ Pa Y and Y → Z for all
Z ∈Ch .Theinduceddistributionis
Y
3.1 PRIMITIVEINTERVENTIONS Prπ((v\{Y}) )= Prπ(v |pa∗)
I V
Given a causal game M with mechanised graph mG and
V∈V Y\{Y}
rationalityrelationsR,aninterventionI isafunctionthat Pa \{Y} ifV ∈Ch
mapsasetofjointprobabilitydistributions{Prπ(v)} π∈R where,forV ∈V,Pa∗ V = (Pa VV otherwiseY
t io tya ren le aw tios ne st o{ fP tr hπ e( iv nI te) r} vπ e∈ nR ed∗ gw ah me ere wR it∗
h
a gr re apt hhe mr Gatio an na dl- Remark 1. After any intervention of type 1, 3, or 4, mG
I
Prπ(v )isthejointprobabilitydistributionrepresentedby must be updated to reflect any changes in R-reachability
I
theCBNinducedbymG whenparameterisedoverpolicy between mechanisms. Note that a type 1 intervention can
I
beconsideredatype4interventionfollowedbyatype3in-
profileπ.Wedefinefourprimitivetypesofintervention.
tervention,butweincludeitasaprimitiveforconvenience.
(1) Fixing an object-level variable: Intervening on vari-
able X replaces Prπ(x | pa ) with a new CPD PrI(x | Theorem1. Primitiveinterventionsareasoundandcom-
X
pa∗ ).Graphically,whenpa∗ 6=pa ,theincomingedges pleteformulationofcausalinterventions.
X X X
to X are changed such that V → X exists if and only if
V ∈pa∗ .Theinduceddistributionis: Proofsketch. Soundnesscomesbecauseeachprimitivein-
X
tervention corresponds with a function between a set of
Prπ(v I)=PrI(x|pa∗ X)· Prπ(v |pa V) probability distributions induced by R-rational outcomes
V∈V Y\{X} toanewsetofprobabilitydistributionsinducedby(apos-
siblydifferent)setofR-rationaloutcomes.Thismakesita
Ahardobject-levelinterventionassignsPrI =δ(X,g).In validcausalintervention.Completenessisshownby prov-
Pearl[2009]’sdo-calculus,thisiswrittendo(X =g).Any inganyvalidinterventioncanbedecomposedintoanequiv-
otherformofobject-levelinterventionisqualifiedassoft. alent set of primitive interventions. We relegate the full
prooftoAppendixA.
(2) Fixing a mechanism variable: A hard mechanism-
level intervention do(M = m ) sets the distribution
V V Therearea numberofotherinterestinginterventiontypes
overeachmechanismM toδ(M ,m ).Anyotherform
V V V thatcanbeconstructedbycomposingtheseprimitives.
of mechanism-level intervention is qualified as soft. A
mechanism-levelinterventionondecisionruleΠ replaces Unfixinganobject-levelvariable:Foreverytype1inter-
D
r : dom(Pa ) → dom(Π ) with a new rationality re- ventionI whichfixesvariableX,thereisatype1inverse
D ΠD D
lation rI : dom(Pa∗ ) → dom(Π ). Graphically,when interventionI′ which unfixesit. It restoresthe intervened
D ΠD D
Pa∗ 6= Pa , the incoming edges to variable Π are CPD to be based on the originalpolicy profile π and par-
ΠD ΠD D
changedsuchthatV →Π existsifandonlyifV ∈Pa∗ . entsPa ,ratherthanI andPa∗ .
D ΠD X X
ForaparametervariableΘ ofV ∈V\D,anintervention
V Unfixingamechanismvariable:Similarly,foreverytype
assignsanewdistributionfromthesetofallCPDsoverset
2 intervention I which fixes a variable Π , there exists
V given the values of its parents, set Pa . Note that pa- D
V a type 2 inverse intervention I′ which unfixes it. This re-
rameter variables don’t have parent mechanism variables
storestherationalityrelationassociatedwithΠ toitsde-
asinputstothechoiceofdistribution. D
faultr ,ratherthanrI.Italsomakesthemechanismcon-
D D
(3) Adding a new object-level variable: Adding a new ditionally dependent on the original parents Pa rather
ΠD
object-level variable Y introduces a new CPD PrI(y | thanPa∗ .
ΠDAdding an object-level dependency: Adding a depen- setsP ,...,P ,suchthat
0 m
dency,e.g., add(X → Y), is equivalentto a type 1 inter-
∀i∃j ∈{0,...,m}:Ii(M)=(P ◦P ...◦P )(M)
ventionwherePrI(y |pa )=Prπ(y |pa ∪{X}). j j−1 0
Y Y (1)
Removing an object-level dependency: Removing a de-
pendency,e.g.,del(X → Y), is equivalentto a type 1 in- That is to say, for any set of interventions I, where the
terventionwherePrI(y |pa )=Prπ(y |pa \{X}). visible set of each agent is an arbitrary subset, Ii ⊆ I,
Y Y
we canconstructanordered listofprimitiveinterventions
suchthat,afterthefirstjsetsofprimitiveinterventions,the
3.2 INTERVENTIONALQUERIES stateofthegameistheexactstatevisibletoAgentiwhen
choosingherpolicy.WeproveTheorem2inAppendixA.
An interventional query concerns the outcome of a game
Takingthisdecomposition,weuniquelypartitiontheagents
after a set of causal interventions I, where each agent is
into sets A ,...,A according to the state of the game
privyto the state of the gameafter a subsetof these inter- 0 m
visibletothem.ThedecomposefunctionmapsI tosets
ventionshasbeenperformed.Wesaythataninterventionis
P ,...,P satisfying Theorem2 and the corresponding
visibletoanagentiftheagenthasanopportunitytoadapt 0 m
partition of the agents A ,...,A . Then, Algorithm 1
theirpolicytothatintervention.ConsiderExample1.Unbe- 0 m
solvestheinterventionalquerybyiterativelycalculatingthe
knownsttothefirm,theworkermayhaveanalternativejob
R-rational outcomes (e.g., NEs if R = RBR), fixing the
offer which changes her best-response policy. Simultane-
policiesofagentswhocannotobservefutureinterventions,
ously,thefirmmayhavenewhiringquotas,whichchange
andapplyinginterventions.ThissubsumesHammondetal.
theirpayoffsand,therefore,theirbestresponse,butwhich
[2023]’spre-policyand post-policyinterventionalqueries.
are not disclosed to the worker. These two external inter-
The computational complexity of Algorithm 1 is (in gen-
ventions can be expressed in a unified analysis using our
eral)intractable,butasisalmostanyinferenceproblemin
framework.
BayesiannetworksKwisthout[2009].Foxetal.[2023]dis-
First, we introduce some new notation. P denotes a set cuss how algorithms such as this one will only be practi-
of primitive interventions. Ii ⊆ I denotes the set of in- calinsettingswithboundedtree-widthgraphs,numberof
terventions visible to agent i. I(M) denotes the state of agents,andactionsets. We leave improvingtheefficiency
the causal game after applying interventions I in any or- ofthisalgorithmtofuturework.
der. The ◦ operator denotes ordered composition where
Whenevertherationalityassumptionshaveasolutionexis-
(I ◦I )(M) is the state of the game after applying I
1 0 0 tenceguarantee(e.g.,ifR = RBR,thereisalwaysatleast
thenI .Asshorthand,I ◦I means{I }◦{I }.
1 0 1 0 1 oneNEofthegame),thenAlgorithm1successfullytermi-
nates.Therearetwospecialcases:
Remark 2. The order in which interventions are applied
is important because interventions are not commutative.
1. If P ∪...∪P = ∅, the agentis not privy to any
0 j
Consider, forexample,two hardobject-levelinterventions
interventionsandtheinterventionalqueryisfullypost-
on the same variable but to different CPDs, δ(X,a) and
policywithrespecttoAgenti.
δ(X,b). Then clearly (do(X = a)◦do(X = b))(M) 6≡
2. IfP ∪...∪P =I,theagentisprivytoallinterven-
(do(X =b)◦do(X =a))(M). 0 j
tions and the interventionalquery is fully pre-policy
withrespecttoAgenti.
TheR-rationaloutcomesofthegameaftereachagentihas
an opportunity to adapt to her visible interventions Ii, is Mechanism-level side effects: Object-level interventions
denotedR(M ). Θ denotestheparameterisationofnon- can have unintuitive mechanism-levelside effects. A side
I I
decisionmechanismsafterinterventionsI. Usingthis, we
effectisamodificationtotheinter-mechanismedgesinmG
defineaninterventionalquerywhichTheorem2provescan and6→denotesan edgeremoval.Proposition1formalises
alwaysbedecomposedintoprimitiveinterventionsets. thesideeffectsofanobject-levelintervention.
Definition 4 (Reachability Path). Let D ∈ Di. We write
Definition 3 (InterventionalQuery). Given CG M, ratio-
R(M → Π ) to denote the set of paths that make
nalityrelationsR,andsetofvisibleinterventionsforeach V D
M R-relevant to Π . A reachability path is any path
agentI1,...,IN,aninterventionalqueryφ(π)isafirst- V D
p ∈ R(M → Π ). That is, a non-repeating sequence
orderlogicalformulathatactsonthejointprobabilitydis- V D
ofnodesV ,...,V ∈m V oftheindependentmechanised
tributionPrπinducedbyR-rationaloutcomeπ ∈R(MI)
graphm
G0 s.tVj =M⊥
,andV isR-relevanttoΠ .
andparameterisationΘI whereI =I1∪...∪IN. ⊥ 0 V 0 D
Proposition1 (Object-levelinterventionside effects). An
Theorem2(DecompositionofInterventionSets). Forany object-level intervention PrI(x | pa∗ ) has side effect
X
setofinterventionsI,whereIi ⊆ I isthesubsetofinter- M 6→ Π if, ∀ reachability paths p ∈ R(M → Π )
V D V D
ventionsvisibletoagenti,thereareprimitiveintervention wehave∃W ∈Vs.t.(W 6∈Pa∗ )and((W →X)∈p)
XAlgorithm1Calculatetheresultofaninterventionalquery
ΘT ΘU1 ΘU2 ΘT ΘU1 ΘU2
1: Input: A causal game M with rationality relation R,
interventionsI =I1∪...∪IN,andqueryφ(π).
T U1 U2 T U1 U2
2: (P 0,...,P m),(A 0,...,A m)←DECOMPOSE(I)
3: A′ ←∅
4: forj =0,...,mdo D1 D2 g D2
5: A←(A j ∪...∪A N)\A′
6: πˆ ←uniformlysampleanR-rationaloutcome
ΠD1 ΠD2 ΠD1 ΠD2
7: fori∈A j do
8: do(Π Di =πˆ Di) (a) (b)
9: forP ∈P j do Figure 2: The Job Market game with an intervention
10: P(M)
11: ifP actsonV ∈Di∪Πithen do(D1 = g) satisfying qualitative specification Π D1 9
12: A′ ←A′∪{i}
Π D2.Mechanism-leveldependenciesarecolouredgrey.In
13: Prπ(v)← Prπ(v |pa )
(a), the blue edge indicates the RBR-relevance of Π D1 to
V∈V V Π D2 andrededgesindicateanactivereachabilitypath.In
14: returnφ(π) (b),theinterventiondo(D1 =g)breaksallthereachability
Q
pathswhichmadeΠ D1 relevanttoΠ D2.
That is to say, an interventionon X which severs at least
oneedgecriticaltoeachreachabilitypathbetweenM and 2015]. Current approaches establish error bounds on ex-
V
Π throughX,willdeletethecorrespondingedgebetween pected outcomes for particular families of games when
D
thosemechanismsinmG.Similarly,ifaninterventioncre- an intervention is conducted [Peysakhovich et al., 2019,
atesatleastonenewreachabilitypath,itwillresultinthe Paccagnan et al., 2022]. This section explores how our
additionofanewinter-mechanismedge. frameworkenablesasystematicapproachtocausalmecha-
nismdesign.
Minimuminterventionsets:Usingtheseobservations,we
formalise the minimum set of interventions required to
breaka causalmechanismdependencyM → Π .Since 4.1 QUALITATIVESPECIFICATIONS
V D
we areonlyinterestedininterventionsthatdonotdirectly
modifythetargetpolicyΠ ,andwerecallthatreachability Qualitativespecificationsareconcernedwithpropertiesof
D
pathsarecalculatedontheindependentmechanisedgraph theDAGG.ConsiderthemechanisedgraphofExample1
m G whichcontainsnoedgesoftheformM →Π ,we shown in Figure 2a. The cyclic structure between nodes
⊥ V D
canrestrictourattentiontoobject-levelinterventions.Then, Π D1 andΠ D2 meanstheoptimalpolicyforeachagentde-
the minimum intervention set is the minimum hitting set pendsontheotheragent’spolicy.
across all reachability paths, of the variables with incom-
A specification may requirea decision rule to be indepen-
ingedgesthatwouldbreakthedependencyifremoved.
dentofaparticularmechanism.Forexample,wemaywant
Definition5(Minimuminterventionset). Theminimumset the firm’s hiring policy to be independentof the worker’s
ofobjectsto interveneonin orderto breakcausalmecha- policywhendecidingtogotouniversity.Thatis,wewish
nismdependencyM
V
→Π
D
isX s.t.: to break the causal dependencyΠ D1 → Π D2. When this
edgedoesnotexist,itmeansthatthefirm’soptimalpolicy
X ∩S 6=∅forallS where does not depend on the worker’s policy for any parame-
i i
S ={V ∈V |∃W ∈V.(W →V)∈R(M →Π )} terisation of the game. There are two ways to satisfy this
i V D
specification.
This metric measures how robust a causal mechanism de- 1. Intervene on the target policy Π D2 with r DI 2 :
p the en mde in nc imy uis mto ne ux mte br en ra ol fin ot be jr ev ce tn -lt eio vn els. inT th ere vs ei nz te ioo nf sth reis quse irt ei ds dom(Pa∗ ΠD2) → dom(Π D2)suchthatΠ D1 6∈ Pa∗ ΠD2,
e.g., the hard intervention do(Π D2 = δ(D2,¬j))
to ensure that, under every parameterization of the game,
whichforcesthefirmtorejecteverycandidate.
there is no incentive for a target policy Π to depend on
D
themechanismvariableM . 2. Perform an object-level intervention to appropriately
V
change the reachability structure of the graph. There
are two paths that make Π D1 RBR-relevant to Π D2.
4 CAUSALMECHANISMDESIGN The first is Π D1 → D1 ← T → U2 when condi-
tionedon{D2,D1}sinceΠ D16⊥m ⊥G U2∩Desc D2 |
Mechanismdesign aims to modifya game to satisfy a de- D2,Pa D2.ThesecondisΠ D1 → D1 conditionedon
sired social outcome or agent behaviour [Börgers et al., ∅sinceΠ D16⊥m
⊥G
Pa D2.Option 1 is somewhat against the “spirit” of mechanism ofthegame,canaffectanagent’sR-rationalchoiceofpol-
design,whichseekstoinducecertainbehavioursorsocial icy.Forexample,considerthePrisoner’sDilemma,where
outcomeswithoutunderminingan agent’s ability to make the prisoners are restricted to pure policies. The mecha-
their own rational choices. However, the intervention on nised graph for this is the same as in Figure 3a. Suppose
Π D2 changespropertiesofthetargetagent’sbehaviourby weareasadisticgamedesignerwhowantstomaximisethe
directlyinterveningontheirpolicy. jailtimeofbothprisonersbyanymeans.Wecandothisin
several ways by modifyingthe usual payoffs of the game
Option 2 requires both active paths to be blocked. This
(Table1).
canbeachievedthroughaninterventiononD1oftheform
PrI(d1 | pa∗ D1) wherePa D1 = ∅. An examplewould be Table1:ThepayoffsinthePrisoner’sDilemma.
do(D1 = g),showninFigure2b.Thecyclicstructurebe-
tweenΠ D1 andΠ D2 isbroken,andthefirmhasnoincen- Agent2(Bob)
tivetoconsidertheworker’spolicy. Cooperate Defect
Cooperate (-1,-1) (-5,0)
Agent1(Alice)
HidingandRevealingInformation Anotherqualitative Defect (0,-5) (-2,-2)
specification is to hide or reveal certain information to
agents.Thiscanbedonebymodifyingtheincomingedges OnewayistodecreasethepayoffsoftheNE(D,D)(the
into a decision variable. Suppose we wish to hide the RBR-rational outcome). Typically, mutual defection leads
agent’sdecisionofgoingtouniversityfromthefirminEx- toatotaljailtimeof4years.Changing(D,D)to(−3,−3)
ample1.Interventiondel(D1 → D2)satisfiesthisbuthas yields6yearstotal.Infact,wecouldchangethepayoffof
mechanismlevelside-effectΠ D1 6→Π D2. (D,D) to (−5+ε,−5+ε) for arbitrary ε > 0 yielding
Amoregeneralquestionis:underwhatcircumstancesisit
−10+2εyearstotalwhile retaining(D,D)as thesingle
purepolicyNE.Sincethisinterventiondoesnotaffectthe
possibletohideorrevealinformationwithoutchangingthe
best-responseof either prisoner,it doesn’tmatter whether
mechanismdependencystructure?The mechanismdepen-
thisinterventionisimplementedasafullypre-policy,fully
dency structure is retained if, for any pair of mechanisms
post-policy,orinterleavedintervention.Theprisonerswill
with active reachabilitypaths, at least one path is notbro-
playthesamepoliciesandthesameNEwillbereached.
ken,andif,foranypairofmechanismswithnoactivereach-
abilitypaths,nonewpathsareintroduced.Wecallaninter- Anotherwayisbytaxingtheexistingrationaloutcome.In
ventionthatpreservesthisstructureincentiveinvariant. fact,byintroducingapartiallyvisibleintervention,wecan
also reward certain behavioursto satisfy the specification.
Definition 6 (Incentive Invariance). An intervention I is
incentive invariant if ∀ M ∈ M,∀ Π ∈ Π, we have IfAlicebelievesthatmutualcooperationwillleadtoboth
V D
pre-interventionreachabilitypathsRBR(M → Π ) and agentsgoingfree,whileBobbelievestheywillsuffer1year
V D
post-interventionreachabilitypathsRBR∗(M →Π )s.t. each, then (C,D) becomesa new NE. Thiscan be imple-
V D
mentedinoneoftwoways.
|RBR∗(M →Π )| =0,if|RBR(M V →Π D)|=0 Example 2 (Partially Visible Rewards). We want to influ-
V D (>0,if|RBR(M
V
→Π D)|>0 enceoneprisonerinthePrisoner’sDilemmatocooperate.
C and D indicate the pure policies “cooperate” and “de-
4.2 QUANTITATIVESPECIFICATIONS
fect”respectively.LetP = {do(Θ U1 = θ U∗ 1),do(Θ U2 =
θ∗ )}bethesetofprimitiveinterventionswith
U2
Quantitative specifications describe bounds on game out-
δ(u1,0) ifd1 =C andd2 =C
c oo fm anes a. gF eo nr te isxa gm rep al te e, rt th he ay ns sp oe mci efy vath lua et ,t th he atex thp eec pt re od bp aa by ilo itf yf θ U∗ 1(u1 |d1,d2)=
(θ U1(u1 |d1,d2) otherwise
of a certain event occurring is within some range, or that
and similar for θ∗ . Let P′ be the inverse. We make P
some social welfaremetric is maximised.Thereare many U2
visibletoonlyAliceinoneoftwoways:
waysofsatisfyingthesespecifications,includingthemod-
ifications to the object-level and mechanism-level depen- 1. P = ∅, P = P, A = {Bob}, A = {Alice}.
0 1 0 1
dencies discussed previously.Here, we focus on interven- This changesthe payoffsof (C,C) so bothprisoners
tionsthatdirectlymodifythechanceorutilityvariablesof gofree, butonlyAlice isinformedofthe change(the
thegameorthecorrespondingmechanism-levelparameter interventionishiddenfromBob).
variables.
2. P = P, P = P′, A = {Alice}, A = {Bob}.
0 1 0 1
Taxes and Rewards: One way of inducing certain be- This informs Alice that (C,C) will lead to both pris-
haviour is to modify the payoffs for certain outcomes onersgoingfreebutreversesthisinterventionbetween
throughtaxesandrewards.Theinter-mechanismedgesre- Alice’sandBob’spolicychoices,so it deceivesAlice
veal which utility variables, under some parameterisation intobelievinganinterventionhastakenplace.In either case, Alice believes there are two possible NE: 1. The worker always chooses g, The hiring system
(C,C)and(D,D),whereasBobbelievesthereisonlyone choosesj iftheworkerchoseg,otherwiseitchooses
(D,D).So,ifAliceplaysuniformdistributionoverherbest j withanyprobabilityq ∈[0,1].
1
responsesC andD,andBobplaysδ(D2,D),theexpected
2. The worker always chooses ¬g, The hiring system
totaljailtimeisE πˆ[U1+U2]= 1 2(0−5)+ 1 2(−2−2)=
choosesjiftheworkerchose¬g,otherwiseitchooses
−4.5Therefore,addingtotalrewardof2togameoutcome
j withanyprobabilityq ∈[0,4].
reducestheexpectedtotalpayoffby0.5. 2 5
So, the worker gets a job with probability 1 either way.
EnvironmentModifications:Anotherwayofsatisfyinga Therefore,theinterventionofEffortVillesatisfiesthespec-
quantitativespecificationistomodifythechancevariables. ification in the stochastic policy case, whereas the iden-
In Example 1, the worker’s temperament can affect both tity intervention does not. In our intervention framework,
agents’policies.Supposewe wantto maximisetheproba- we can model EffortVille with primitive intervention set
bility of the worker getting a job and R = RBR, i.e., we P = {do(T = h)} and A = {1,2}since we want the
0 0
want the probability of the worker getting the job under interventiontobefullypre-policy,allowingbothagentsto
anyNEoftheintervenedgametobeatleastashighasthe adapt their policies accordingly. Note, however, that it is
probabilityof the workergettingthe job underanyNE of typically not possible for game designers to intervene on
theoriginalgame.Formally,aninterventionI satisfiesthis thechancevariablesofthegameastheseareusuallyused
specificationif torepresent‘movesbynature’.Forexample,agovernment
maybeabletointerveneonutilityvariablesbytaxingorre-
min Prπˆ (j)≥ max Prπ(j)
πˆ∈RBR(MI) π∈RBR(M) wardingworkersandfirms,butitisunlikelythattheycan
affecttheunderlyingtemperamentoftheworkers.
One way to do this is to change the location of the game
to EffortVille where everyone is hard-working. This cor-
5 COMMITMENT
responds with a mechanism-level do(Θ = δ(T,h)) or
T
object-leveldo(T = h) intervention.In this case, the CG
Interventionsondecisionanddecisionrulevariablesenable
hasthreepurepolicyRBR-rationaloutcomes(NE).2
ustoreasonaboutcommitment.Insomegames,itispossi-
bleforthefirstmovingagent,theleader,togainastrategic
1. The worker always chooses g. The hiring system al-
wayschoosesj.SoE [U1]=5andE [U2]=3 advantageoverothers,calledfollowers,bycommittingtoa
π π
policybeforethegamebegins;theleadercansometimesin-
2. The worker always chooses g. The hiring system
fluencethefollower’sincentivesbyrevealingprivateinfor-
chooses j if the worker chooses g. Otherwise, it
mationabouttheirpolicy.ThesimplestexampleisaStack-
chooses¬j.SoE [U1]=5andE [U2]=3
π π elberggameconsistingofoneleaderandfollower.
3. The worker always chooses ¬g. The hiring system
We use an example from [Letchford and Conitzer, 2010],
chooses ¬j if the worker chooses g. Otherwise, it
choosesj.SoE [U1]=4andE [U2]=3 which shares the same game graph as in Figure 3a with
π π Agent 1 (2) the leader (follower) and with dom(D1) =
InalltheseNEs,theprobabilityoftheworkergettingajob {T,B} and dom(D2) = {L,R}. The utility parameter-
ization is shown in Table 3c. Pre-commitment, Action
is1,soitsatisfiesthespecification.Also,thefirsttwoNEs
oftheintervenedgamemaximiseutilitarianandegalitarian T strictly dominates B so (T,L) is a unique NE and
social welfare. The identity intervention, which does not E π[U1] = 2. However, by committingto the pure policy
change the game, would also have satisfied this specifica- B, the leader incentivises the follower to play R and so
tion because all three pure NEs of the original game also
E π[U1]=3.Note,inthiscase,theresultofcommitmentis
alsoaParetoimprovementovertheoriginalNE(i.e.,Stack-
resultintheworkergettinga jobwithprobability1.How-
elbergcommitmentcanalsoimprovesocialwelfare).
ever,thisisnotthecaseunderNEswithstochasticpolicies.
The original game has the following NE: If the worker is
Causalgamesnaturallyrepresentcommitmentwith asim-
hardworking, she chooses g with probability 1 2. If she is plecausalinterventiononnodeΠ D1 tobefixedtothecom-
lazy,she alwayschooses¬g. Ifshe choosesg,thefirmal-
mittedpolicyπ (showninFigure3b).Thepayoffreceived
1
wayschoosesj.Ifshechooses¬g,thefirmchoosesj with
by theleader aftercommitmentcanbe calculatedthrough
probability 4 . This yields a 9 probabilityof the worker
5 10 backwardinductiononthegraph[Hammondetal.,2021].
gettingajob.Ontheotherhand,theNEsoftheintervened
gameare By representingcommitment as a causal intervention, we
canprovewhetheraparticularcommitmentcanbebenefi-
cial for the leader. In the stochastic policysetting, the fol-
2NEsincausalgamescanbefoundusingPyCIDJamesFox lowerwillstillplayapurepolicysinceshehasnoincentive
etal.[2021]. to randomise after the leader’s commitment; she is effec-ΠD1 ΠD2 ΠD1 ΠD2 SRC Centre for Doctoral Training in Autonomous Intel-
ligent Machines and Systems (Reference: EP/S024050/1)
D1 D2 D1 D2 and Wooldridge was supported by a UKRI Turing
AI World Leading Researcher Fellowship (Reference:
U1 U2 U1 U2 L R EP/W002949/1).
T (2,1) (4,0)
ΘU1 ΘU2 ΘU1 ΘU2 B (1,0) (3,2)
References
(a) (b) (c)
Figure 3: The Stackleberg game represented as mecha- JenniferAhern,AlanHubbard,andSandroGalea. Estimat-
nised causal graphs (a) pre-commitment, and (b) post- ingtheeffectsofpotentialpublichealthinterventionson
commitment,withpayoffmatrix(c). populationdisease burden:a step-by-stepillustrationof
causalinferencemethods. Americanjournalofepidemi-
ology,169(9):1140–1147,2009.
tively playing a single-agent decision game. The leader’s
expectedutilityaftercommittingtopolicyπ = 1T + 1B Carolyn Ashurst, Ryan Carey, Silvia Chiappa, and Tom
is3.5(inAppendixB.1).Thisisgreaterthan1 the2 expec2 ted Everitt. Why fair labels can yield unfair predictions:
Graphicalconditionsforintroducedunfairness. In Pro-
utilityof2intheoriginalgame’suniqueNE,whichbenefits
ceedings of the AAAI Conference on Artificial Intelli-
theleader.
gence,volume36,pages9494–9503,2022.doi:10.1609/
A partiallyvisiblecommitmentisrepresentednaturallyin aaai.v36i9.21182.
ournewframeworkofcausalinterventions.Specifically,a
commitment that occurs in the primitive intervention set JennieEBrand,XiangZhou,andYuXie. Recentdevelop-
P can be revealed to all agents in A ,A ,...,A . mentsincausalinferenceandmachinelearning. Annual
j j j+1 m
For example, if we have P = ∅, P = {do(Π = ReviewofSociology,49:81–110,2023.
0 1 1
δ(D1,T))},A = {1},andA = {2}thenAgent1com-
0 1
mitstoplaying‘B”andrevealsittoAgent2.Algorithm1 TilmanBörgers,DanielKrähmer,andRolandStrausz. An
revealsthatAgent2willplayδ(D2,R)(i.e.alwaysplaying IntroductiontotheTheoryofMechanismDesign.Oxford
“R”) and Agent 1 will receive a payoff of 3. However, if UniversityPress, 07 2015. ISBN 9780199734023. doi:
Agent1’scommitmenttoplayingδ(D1,B)iskeptprivate 10.1093/acprof:oso/9780199734023.001.0001. URL
from Agent 2, then we have P = ∅, P = {do(Π = https://doi.org/10.1093/acprof:oso/9780199734023
0 1 1
δ(D1,B))},A ={1,2},andA =∅.Then,Agent2will
0 1
Micah Carroll, Alan Chan, Henry Ashton, and David
alwaysplay‘L’,inaccordancewiththeNEoftheoriginal
Krueger. CharacterizingmanipulationfromAIsystems.
system, calculatedafter P , givingAgent1 a payoffof 2.
0
In Proceedings of the 3rd ACM Conference on Equity
In AppendixB.2, we show that we can also use the inter-
and Access in Algorithms, Mechanisms, and Optimiza-
venedgraphtocalculatetheoptimalpolicytocommitto.
tion,pages1–13,2023.
A.P.Dawid. Influencediagramsforcausalmodellingand
6 CONCLUSION
inference. International Statistical Review, 70(2):161–
189,2002.
This work presents a sound and complete characterisa-
tion of arbitrary causal interventions in causal games. It TomEveritt,RyanCarey,EricDLanglois,PedroAOrtega,
usesthisframeworktoevaluateandsystematicallymodify andShaneLegg. Agentincentives:Acausalperspective.
incentive structures to satisfy qualitative and quantitative InProceedingsofthe AAAIConferenceonArtificialIn-
specifications,whichhasimportantapplicationsforcausal telligence,volume35,pages11487–11495,2021a.
mechanism design. Solving interventionalqueries is com-
putationallyexpensive,butweproveresultsandgivealgo- TomEveritt,MarcusHutter,RamanaKumar,andVictoria
rithms,showinghowtheycanbemademoretractable.Fi- Krakovna. Reward tampering problems and solutions
nally, we focus on pedagogicalexamples, but demonstrat- in reinforcement learning: A causal influence diagram
ingthemethodempiricallyonlargerexamplesisanimpor- perspective.Synthese,198(Suppl27):6435–6467,2021b.
tantdirectionforfuturework. doi:10.1007/s11229-021-03141-4.
Sebastian Farquhar, Ryan Carey, and Tom Everitt. Path-
Acknowledgements specificobjectivesforsaferagentincentives.InProceed-
ings of the AAAI Conference on Artificial Intelligence,
The authors wish to thank five anonymous reviewers for volume 36, pages 9529–9538,2022. doi: 10.1609/aaai.
their helpful comments. Fox was supported by the EP- v36i9.21186.JamesFox,MattMacDermott,LewisHammond,PaulHar- AllenCMillerIII,MileyWMerkhofer,RonaldAHoward,
renstein, Alessandro Abate, and Michael Wooldridge. James E Matheson, and ThomasR Rice. Development
On imperfect recall in multi-agent influence diagrams. ofautomatedaidsfordecisionanalysis.Technicalreport,
In Proceedings Nineteenth conference on Theoretical 1976.
Aspects of Rationality and Knowledge,, volume 379 of
Dario Paccagnan, Rahul Chandan, and Jason R Marden.
Electronic Proceedings in Theoretical Computer Sci-
Utility and mechanism design in multi-agent systems:
ence, pages 201–220, 2023. doi: 10.4204/EPTCS.379.
An overview. Annual Reviews in Control, 53:315–328,
17.
2022.
Thomas A Glass, Steven N Goodman, Miguel A Hernán,
Judea Pearl. Probabilistic Reasoning in Intelligent Sys-
andJonathanMSamet.Causalinferenceinpublichealth.
tems: Networks of Plausible Inference. Morgan Kauf-
Annualreviewofpublichealth,34:61–75,2013.
mannPublishersInc.,SanFrancisco,CA,USA,1988.
Lewis Hammond, James Fox, Tom Everitt, Alessandro
JudeaPearl. Causality. CambridgeUniversityPress,Cam-
Abate, and Michael Wooldridge. Equilibrium refine-
bridge,UK,2009.
ments for multi-agent influence diagrams: Theory and
practice. In Proceedingsofthe 20thInternationalCon- JudeaPearlandRinaDechter. Identifyingindependencies
ferenceonAutonomousAgentsandMultiAgentSystems, in causal graphs with feedback. In Proceedings of the
pages574–582,2021. TwelfthInternationalConferenceonUncertaintyinArti-
ficialIntelligence,pages420–426,1996.
Lewis Hammond, James Fox, Tom Everitt, Ryan Carey,
Alessandro Abate, and Michael Wooldridge. Reason- AlexanderPeysakhovich,ChristianKroer,andAdamLerer.
ingaboutcausalityingames. ArtificialIntelligence,320: Robustmulti-agentcounterfactualprediction. Advances
103919,2023. inNeuralInformationProcessingSystems,32,2019.
JamesJHeckmanandRodrigoPinto. Causalityandecono- AviPfefferandYa’akovGal. Onthereasoningpatternsof
metrics. Technicalreport,NationalBureauofEconomic agentsingames. InAAAI,pages102–109,2007.
Research,2022.
MichaelSpence.Jobmarketsignaling.TheQuarterlyJour-
nalofEconomics,87(3):355,1973.
Ronald A Howard and James E Matheson. Influence dia-
grams. DecisionAnalysis,2(3):127–143,2005.
Francis Rhys Ward, Francesca Toni, and Francesco Belar-
dinelli. On agentincentivesto manipulatehumanfeed-
James Fox, Tom Everitt, Ryan Carey, Eric Langlois,
back in multi-agent reward learning scenarios. In AA-
Alessandro Abate, and Michael Wooldridge. PyCID:
MAS,pages1759–1761,2022.
A Python Library for Causal Influence Diagrams. In
Proceedings of the 20th Python in Science Conference,
pages43–51,2021.
Zachary Kenton, Ramana Kumar, Sebastian Farquhar,
Jonathan Richens, Matt MacDermott, and Tom Everitt.
Discoveringagents. ArtificialIntelligence,322:103963,
2023.
DaphneKollerandBrianMilch. Multi-agentinfluencedi-
agramsforrepresentingandsolvinggames. Gamesand
economicbehavior,45(1):181–221,2003.
JohanKwisthout. Thecomputationalcomplexityofproba-
bilisticnetworks. UtrechtUniversity,2009.
StephenLeRoy.Causalityineconomics.LondonSchoolof
Economics,CentreforPhilosophyofNaturalandSocial
Sciences,2004.
Joshua Letchford and Vincent Conitzer. Computing opti-
malstrategiestocommittoinextensive-formgames. In
Proceedingsof the 11th ACM conference on Electronic
commerce,pages83–92,2010.Characterising Interventions in Causal Games
(Supplementary Material)
ManujMishra1 JamesFox1,2 MichaelWooldridge1
1DepartmentofComputerScience,UniversityofOxford,UK
2LondonInitiativeforSafeAI(LISA),UK
A PROOFS
Theorem1. Primitiveinterventionsareasoundandcompleteformulationofcausalinterventions.
Proof. Wefirstprovesoundness.Thisistrueusingthedefinitionsoftheprimitiveinterventions.LetPr(v )betheinduced
I
joint distribution by type 1, 3, and 4 interventions,as per the definitions, and Pr(v ) = Pr(v) for type 2 interventions.
I
Also, let R∗ be the inducedrationality relationsby type 2 interventionsand R∗ = R for type 1, 3, and 4 interventions.
Then, the effectof a primitiveinterventionP of anytype is the function{Prπ(v)} π∈R 7→ {Prπ(v I)} π∈R∗ which is a
validcausalintervention.
We now turn to completeness by showing that any interventionI can be decomposedinto a set of equivalentprimitive
interventionsP. Thatis to say, the state ofthe gameafter applyinginterventionI is equivalentto the state of the game
afterapplyinginterventionsP.Suppose,
I({Prπ(v)} π∈R,R)=({Prπ(v I)} π∈R∗,R∗)
s.t.Prπ(v)= Prπ(v |pa )
V
V∈V
Y
andPrπ(v )= Prπ(v |pa )
I I V
VIY∈VI
ThenthetrivialdecompositionofI is|V |type3interventionswhichmultiplythejointdistributionbyeachofPrπ(v |
I I
pa ), followedby|V| type4 interventionsthatdividethe jointdistributionby eachof Prπ(v | pa ), then|R∗| type 2
V V
interventionsthatattachtheappropriaterationalityrelationtoeachmechanismvariable.
Of course, more concise decompositionsare possible if there is overlap between R and R∗ as well as overlap between
{Pa } and{Pa } .
V V∈V VI VI∈VI
Proposition1(Object-levelinterventionside effects). Anobject-levelinterventionPrI(x | pa∗ )hassideeffectM 6→
X V
Π if,∀reachabilitypathsp∈R(M →Π )wehave∃W ∈Vs.t.(W 6∈Pa∗ )and((W →X)∈p)
D V D X
Proof. AninterventionPrI(x| pa∗ )hassideeffectM 6→Π ifintheintervenedgraphtherearenoreachabilitypaths
X V D
fromM toΠ .Thismeansatleastonecausalarrowisbrokenineachsuchreachabilitypathintheoriginalgraph.An
V D
object-levelinterventiononX breaksonlythecausalarrowsW →X whereW ∈Pa butW ∈/ Pa∗ .
X X
Theorem2(DecompositionofInterventionSets). ForanysetofinterventionsI,whereIi ⊆I isthesubsetofinterven-
tionsvisibletoagenti,thereareprimitiveinterventionsetsP ,...,P ,suchthat
0 m
∀i∃j ∈{0,...,m}:Ii(M)=(P ◦P ...◦P )(M) (2)
j j−1 0Proof. We show there is a set of primitive intervention sets that satisfy Theorem 2 for any set of interventions Ii by
constructing an example. We use the notation P(I) to denote the primitive decomposition of I as shown in the proof
of Theorem 1. Then, Pi = ∪ I∈IiP(I) are the primitive interventionsequivalentto each agent’s visible interventions.
ConsideranarbitraryorderingofPi =Pi,...,Pi.LetQi denotetheinverseofPi.Theconstructionisasfollows.
0 k k k
P =P0
0
P =Pj ◦...◦Pj ◦Qj−1◦...◦Qj−1forj ∈1,...,N
j k 0 0 k
A =∅
0
A ={j}forj ∈1,...,N
j
whereP0arethefullypre-policyinterventionsvisibletoallagents.Inthisconstruction,A aresingletonsets(exceptA )
j 0
sowehavem=N.Foralli,letj =i.Then,wemakeaninductiveargument.ThebasecaseI0(M)=P0(M)=P (M)
0
holdsbydefinitionofP0.AssumingIi−1(M)=(P ◦P ...◦P )(M),wehave
j−1 j−2 0
Ii(M)=Pi(M)
=(Pi◦Qi−1◦Pi−1)(M)
=(P ◦Pi−1)(M)
j
=(P ◦Ii−1)(M)
j
=(P ◦P ...◦P )(M)
j j−1 0
So,thisassignmentofprimitiveinterventionsetssatisfiesTheorem2.Algorithm1explicitlyshowshowthisconstruction
isusedtocalculateinterventionalqueries.
B COMMITMENT
B.1 EXPECTEDUTILITYAFTERCOMMITMENT
Wefirstshowthattheexpectedutilitytoagent1is3.5aftershecommitstopolicyπ ,pickingbetweenT andBwithequal
1
probability.
LetI = {do(Π = π )} andfix π ∈ Π. We use PrI(X)as shorthandforPrπ(X ), andE asshorthandforE [U ].
1 1 I I π I
Then
E [U1]:= u1PrI(u1) (E1)
I
u1∈d Xom(U1)
=2·PrI(D1 =T |π 1)PrI(D2 =L|Π D2)
+4·PrI(D1 =T |π 1)PrI(D2 =R|Π D2)
+1·PrI(D1 =B |π 1)PrI(D2 =L|Π D2)
+3·PrI(D1 =B |π 1)PrI(D2 =R|Π D2)
1 if0.5·U2(2,1)+0.5·U2(1,0)
PrI(D2 =L|Π D2)=

>0.5·U2(4,0)+0.5·U2(3,2)
0 otherwise
1 if0.5>1
=
(0 otherwise
=0
Similarly,Pr(D2 =R|Π D2)=1
=⇒ E [U1]=4·0.5+3·0.5=3.5
IB.2 OPTIMALSTOCHASTICBEHAVIOURALPOLICY
Let agent 1 have policy π where she plays T with probability p and agent 2 have policy π where she plays L with
1 2
probabilityq.Thentheoptimalpolicyπˆ =pˆT +(1−pˆ)B isgivenby:
1
1 ifp>2(1−p)
q =
(0 otherwise
1 ifp> 2
= 3
(0 otherwise
=⇒ pˆ=argmaxE [U1] (E2)
I
p
=argmax(2pq+4p(1−q)+(1−p)q+3(1−p)(1−q))
p
=argmax(p−2q+3)
p
2
=argmax p−2I p> +3
3
p
(cid:18) (cid:18) (cid:19) (cid:19)
2
=
3
whereI ={do(Π =πˆ )}.
1 1
Sotheoptimalpolicyforagent1tocommittoisπˆ = 2T + 1B withpayoffE [U1]= 2 +3=3.6˙.Thisisgreaterthan
1 3 3 I 3
thepayoffof2intheNEoftheoriginalgameandthepayoffof3.5intheNEinducedafteracommitmentto 1T + 1B as
2 2
shownintheprevioussection.This figure "barcelona.jpg" is available in "jpg"(cid:10) format from:
http://arxiv.org/ps/2406.09318v1