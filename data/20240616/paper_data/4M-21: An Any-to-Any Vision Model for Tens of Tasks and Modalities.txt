4M-21: An Any-to-Any Vision Model
for Tens of Tasks and Modalities
RomanBachmann1†∗ Og˘uzhanFatihKar1∗ DavidMizrahi2†∗
AliGarjani1 MingfeiGao2 DavidGriffiths2
JiamingHu2 AfshinDehghan2 AmirZamir1
1SwissFederalInstituteofTechnologyLausanne(EPFL) 2Apple
https://4m.epfl.ch
RGBmodalities Edgemodalities
RGB Colorpalette SAMedges Cannyedges
Geometricmodalities Textmodalities
Depth nS ou rr mfa ac le s 3D ph ou sem san Caption emT b5 e- dX dX inL gs Webtext
Any-to-anymodel Albany
International Gett min yg flr ie ga hd t!yfor Ai cr tp h eo ner tmt ers ae fj oorv rre ta hs i eras
CapitalRegion,
Northeastern...
Transformer Transformer
encoder decoder
Semanticmodalities Metadatamodalities
Bounding Semantic SAM Semantic Geometric Image
boxes segmentation instances metadata metadata metadata
F CLe
I
(a
P
dt efu
e
nr
a
se
t eu
)m rea spm fo
D (e
dId
a eN
ta nuOl srvi et e2i )ses
Im f (e da a eg t ne uB sr eein )sd
Glo
fD (e
gb
I a lN
ota
u
bOl
r
avf
e
l2e )sature Im
m f (e
gao
a lg
otd
e u
bBa
r aei
ll
n
)i st dies CO W l# ub# a tI j tn le k eH s c a ru t t ba snm …cin e li oa c s tyn re s e::s s :4: : 4 707 1 0 5%2 % % c Oo cm cG lp ue l s 2o e i 5 …m x o %i nte yt s:r ci 5c o5 r% e: O C B Sr o Ci r ag l io o g t. ur n hr f re u t t ar ns l a tn …e. is: oe st5 ns s:1 s ::4:2 4653x 00%55 %%%12
Figure1: Wedemonstratethepossibilityoftrainingasinglemodelontensofhighlydiversemodalitiesusinga
multimodalmaskingobjective[62],withoutalossinperformancecomparedtoexistingspecializedsingle/few
taskmodelstrainedtosolvesignificantlyfewermodalities.Themodalitiesaremappedtodiscretetokensusing
modality-specifictokenizers.Theresultingmodelcangenerateanyofthemodalitiesfromanysubsetofthem.
Abstract
Currentmultimodalandmultitaskfoundationmodelslike4M[62]orUnifiedIO[59,
58]showpromisingresults,butinpracticetheirout-of-the-boxabilitiestoaccept
diverseinputsandperformdiversetasksarelimitedbythe(usuallyrathersmall)
number of modalities and tasks they are trained on. In this paper, we expand
upon the capabilities of them by training a single any-to-any model on tens of
highlydiversemodalitiesandbyperformingco-trainingonlarge-scalemultimodal
datasetsandtextcorpora. Thisincludestrainingonseveralsemanticandgeometric
modalities, feature maps from recent state of the art models like DINOv2 and
ImageBind,pseudolabelsofspecialistmodelslikeSAMand4DHumans,anda
rangeofnewmodalitiesthatallowfornovelwaystointeractwiththemodeland
steerthegeneration,forexampleimagemetadataorcolorpalettes. Acrucialstep
inthisprocessisperformingdiscretetokenizationonvariousmodalities,whether
they are image-like, neural network feature maps, vectors, structured data like
instancesegmentationorhumanposes,ordatathatcanberepresentedastext.
Throughthis,weexpandontheout-of-the-boxcapabilitiesofmultimodalmodels
andspecificallyshowthepossibilityoftrainingonemodeltosolveatleast3xmore
tasks/modalitiesthanexistingonesanddoingsowithoutalossinperformance.This
*Equalcontribution&correspondingauthors.Randomizedorder.
†WorkpartiallydonewhileatAppleorEPFL,respectively.
Preprint.Underreview.
4202
nuJ
31
]VC.sc[
1v60490.6042:viXraenablesmorefine-grainedandcontrollablemultimodalgenerationcapabilitiesand
allowsustostudythedistillationofmodelstrainedondiversedataandobjectives
into one unified model. We successfully scale the training to a three billion
parameter model using tens of modalities and different datasets. The resulting
multimodalmodelsandtrainingcodeareopensourcedathttps://4m.epfl.ch.
1 Introduction
Havingasingleneuralnetworktohandleawideandvariedrangeoftasksandmodalitieshasbeen
alongstandinggoal. Suchamodel,especiallywhencapableofany-to-anypredictions,bringsnotable
advantages,suchasmodelsizeandtest-timecomputationalefficiencyandenablingmodalityfusion.
However,multitasklearninghascommonlyfacedsignificantchallenges. Forexample,thetraining
oftensuffersfromnegativetransferandtypicallyrequirescarefulstrategiesforbalancinglossesor
gradients[46,93,82,94,31]. Moreover,trainingasinglenetworkontasksandmodalitiesthatvary
greatlyintermsofdimensionality, datatype, andvaluerangespresentsadditionalcomplexities†.
Recentnotableeffortsinthespaceofmultimodalandmultitasktraining,suchasPix2Seq[16,17],
OFA[88],Unified-IO[59,58],or4M[62],havemadesignificantstridesinunifyingtherepresentation
spaceforconceptuallydifferentinputsandtargets. Alargepartoftheirsuccesscanbeattributed
totransformingdifferentmodalitiesintoacommonrepresentation,namelysequencesofdiscrete
tokens,andtrainingrelativelystandardTransformerarchitecturesonthem. Whiletheseworksshow
promisingresults,theyaretypicallytrainedonasmallsetofmodalities. Thisraisesthequestionif
increasingthesetoftasks/modalitiestheycansolvewillleadtoadegradationofperformance.
We build upon the multimodal masking pre-training scheme [62] and increase its capabilities by
trainingontensofhighlydiversemodalities. Concretely,weaddSAMsegments[47],3Dhuman
posesandshapesfrom4DHumans[35],cannyedgesextractedfromRGBandSAMinstances,color
palettes,multipletypesofimage,semanticandgeometricmetadata,aswellasT5-XXL[68]text
embeddings,inadditionto7morecommonmodalities. Ontopofthat,weincludedensefeature
mapsoftherecentstateoftheartmodelsDINOv2[65]andImageBind[33],aswellastheirglobal
embeddingvectorstoenablemultimodalretrievalabilities. Pleaseseefig.1foranoverview.
Weareabletotrainasingleunifiedmodelondiversemodalitiesbyencodingthemwithmodality-
specific discrete tokenizers (see fig. 3). For image-like modalities, e.g. RGB or edges, we train
ViT-based [24] VQ-VAE [63] tokenizers to map the inputs into a small grid of discrete tokens.
Formodalitieslike3Dhumanposesorimageembeddings,wetrainMLP-baseddiscreteVAEsto
compressthemintoasmallsetofdiscretetokens. Allothermodalitiesthatcanbemappedtoatext
representation,suchascaptionsormetadata,areencodedusingaWordPiecetokenizer[23].
Theresultingmodeldemonstratesthepossibilityoftrainingasinglemodelonalargenumberof
diversemodalities/taskswithoutanydegradationinperformanceandsignificantlyexpandsthe
out-of-the-boxcapabilitiescomparedtoexistingmodels. Addingallthesemodalitiesenablesnew
potentialformultimodalinteraction,suchasretrievalfromandacrossmultiplemodalities,orhighly
steerablegenerationofanyofthetrainingmodalities,allbyasinglemodel.
Inshort,weexpandthecapabilitiesofexistingmodelsacrossseveralkeyaxes:
• Modalities: Increasefrom7intheexistingbestany-to-anymodelsto21diversemodalities,
enablingnewcapabilitieslikecross-modalretrieval,controllablegeneration,andstrong
out-of-the-boxperformance. Thisisoneofthefirsttimesinthevisioncommunitythata
singlemodelcansolvetensofdiversetasksinanany-to-anymanner(seefig.2),without
sacrificing performance and especially do so without any of the conventional multitask
learningdifficulties[73,46,93,82,94,31].
• Diversity: Addsupportformorestructureddata, suchashumanposes, SAMinstances,
metadata,andcolorpalettesforcontrollablegeneration.
• Tokenization: Investigatediscretetokenizationofdiversemodalitiessuchasglobalimage
embeddings,humanposes,andsemanticinstancesusingmodality-specificapproaches.
• Scale: Scalethemodelsizeto3Bparametersanddatasetto0.5Bsamplesusing[11].
• Co-Training: Demonstrateco-trainingonvisionandlanguagemodelingsimultaneously.
†Modalityvstask: “Modalities"usuallydenotetheinputstoamodel(e.g. sensorysignals),and“tasks"
usuallydenotetheoutputs(e.g.semantics).Theadoptedarchitectureinmultimodalmaskedmodelingenablesa
symmetricinput-outputstructure,thusmodalitiesandtasksareusedinterchangeablyinthispaper.
2Input Predictions
RGB Caption Bobuonxdeisng seSgemmeanntatitcion Depth CLIP nSourrmfaacles Hpuomseasn DINOv2 ImageBind Metadata Cedagnnesy eSdAgMes insStaAnMces pCaolelottre
tthoep voi le of w o<kp f oer uorstmo nth>e gw.c lca uol tk tma eb rp ils.lei .ct .xy oi:tr ye0:: . 0068.904
<mpoeur asnsot aani >nfa ss m haiosly wwienlgl gw.c lca uol tk tma eb rp ils.le .i ct .xy oi:tr ye0:: . 1046.515
et nh te ra pv niiecerew a otf r ft ohtm hee t sh ee a gw.c lca uol tk tma eb rp ilsle .i c.tx .y oi:tr ye0:: . 1078.417
theto vpi eowf tfhroem h itllhe g.w c cla uol tkmta epb r li se .li .cxt .y oit: ry e0: :.0 0 3.0127
the tv moiepow uo nff trtaohim ne the g.w c cla uol tkmta epb r li se .li .cxt .y oit: ry e0: :.0 1 5.4054
th wtoe a pnv i aoe kfw at hf loreo o lm kaok teuh te gw.c laculok ttma eb rp ill s.ei .t c.xy oi:t ry0 e:. : 2 076.411
them roouandt ation sthe g.w c cla uol tkmta epb r li se .li .cxt .y oit: ry e0: :.0 1 6.8054
listhbeo ns,t rpeoerttsu ogfal gw.c laculok ttma eb rp ill s.ei .t c.xy oi:t ry0 e:. : 1 072.143
the tv moiepow uo nff trtaohim ne the g.w c cla uol tkmta epb r li se .li .cxt .y oit: ry e0: :.0 3 7.2051
latshte o af rut so fp athrte 2 gw.c laculok ttma eb rp ill s.ei .t c.xy oi:t ry0 e:. : 4 058.601
g.w c cla uol tkmta epb r li se .li .cxt .y oit: ry e0: :.0 0 4.6043 the blue lagoon
cat nhlaaekd et iu asr nqo fu r ootchis kee ies gw.c laculok ttma eb rp ill s.ei .t c.xy oi:t ry0 e:. : 1 070.506
the m sv oui uemnw tm f rriuto nom df lethe g.w c cla uol tkmta epb r li se .li .cxt .y oit: ry e0: :.0 0 6.6191
lothceh hloomteol nadt gw.c laculok ttma eb rp ill s.ei .t c.xy oi:t ry0 e:. : 1 066.918
tthh neeoe fta w h tner e zc eaeiena nh laati ul nlc,s di toyf gw.c laculok ttma eb rp ill s.ei .t c.xy oi:t ry0 e:. : 0 034.611
Figure 2: One-to-allgeneration. 4M-21cangenerateallmodalitiesfromanygiveninputmodalityusing
chainedgeneration[62].Noticethehighconsistencyamongthepredictionsofallmodalities.Eachrowstarts
fromadifferentmodalitycomingfromthesamedatasample.Highlightedingreenarenewinput/outputpairs
that4M[62]cannotpredictnoracceptasinput. Notethat,whilethisfigureshowspredictionsfromasingle
input,4M-21cangenerateanymodalityfromanysubsetofallmodalities.
2 Method
Weadoptthe4Mpre-trainingscheme[62]asithasbeenshowntobeaversatileapproachthatcanbe
efficientlyscaledtoadiversesetofmodalities. Wekeepthearchitectureandthemultimodalmasked
trainingobjectivethesame,butexpanduponthemodelanddatasetsize,thetypesandnumberof
modalitieswithwhichwetrainthemodel,andtrainjointlyonmultipledatasets. Allmodalitiesare
firsttransformedintosequencesofdiscretetokensusingmodality-specifictokenizers(Seefig.3).
Duringtraining,randomsubsetsofthesetokensareselectedfromallmodalitiesasinputsandtargets,
andtheobjectiveistopredictonesubsetfromtheother. Werelyonpseudolabelingtocreatealarge
pre-trainingdatasetwithmultiplealignedmodalities.
2.1 Modalities
Wetrainonalargeanddiversesetofmodalitiesthatwegroupintothefollowingcategories: RGB,
geometric, semantic, edges, feature maps, metadata, and text modalities. Below we provide a
summaryofthem(Seefig.1andappendicesDandEfordetails,andfig.2forgenerationexamples).
RGB:WeincludebothtokenizedandpixelversionsofRGBimagestofacilitatetransferlearning.
WealsoextractedtheircolorpalettesusingPyPalette[2],atvaryingnumberofcolors. Thisenables
ustoperformconditionalgenerationusingdesiredcolorsforbetterartisticcontrol.
3
BGR
noitpaC
gnsiedxnouboB
nocitiatntnaemmegSes
htpeD
PILC
selcaamfrruoSn
nsaemsoupH
2vONID
dniBegamI
atadateM
syengndaCe
sMegAdSe
secMnAatSsni
ertotelolaCpGeometricmodalities: Thesecontainsurfacenormals,depth,and3Dhumanposes&shapewhich
provideimportantinformationaboutthescenegeometry. Forthefirsttwo,weusedOmnidatamodels
from[26,44]forpseudolabelingduetotheirstronggeneralizationperformance. For3Dhuman
posesandshape,weleveragearecentstate-of-the-artmodel,4D-Humans[35].
Semanticmodalities: Weincludesemanticsegmentationandboundingboxestocapturethescene
semanticsandleverageMask2Former[19]andViTDet[54]modelsforpseudolabeling. Nextto
these,wealsoincorporatedpseudolabelsextractedfromSegmentAnythingModel[47](SAM)as
SAMinstancesforitsstrongobjectrepresentation.
Edges: As recent generative methods such as ControlNet [98] showed, edges carry important
informationaboutthescenelayoutandsemanticsthatarealsousefulforconditioning,abstraction,
and sketching. We consider two types of edges, specifically Canny edges and SAM edges. The
formerisextractedfromtheRGBimageswithOpenCV[1]. AsCannyedgesmaycontainlow-level
information,e.g. shadingedges,wealsoincludeedgesextractedfromSAMinstancestogetamore
semanticboundarymap. WetokenizeCannyandSAMedgeswithasharedtokenizer.
Featuremaps: WeextractembeddingsfromCLIP[67],DINOv2[65]andImageBind[33]asthey
demonstratedstrongtransferlearningandretrievalcapabilities. Previously,tokenizedCLIPfeatures
wereshowntobeaneffectivetargetformaskedimagemodelling[89,62]thatenablesdistillinga
usefulsemanticrepresentationofthescene. Wefollowasimilarapproachandtokenizethefeature
mapsfrompre-trainedCLIP-B16,DINOv2-B14andImageBind-H14models. Wealsoincludedthe
globalembeddingsofDINOv2andImageBindmodelsandtokenizedthemseparately.
Metadata:WeextractseveralusefulpiecesofinformationfromtheRGBimagesandothermodalities,
thatcanbecategorizedintosemanticmetadata,geometricmetadata,andimageprocessingmetadata.
Forthis,weusefunctionalitiesfromPillow[3]OpenCV[1],andOmnidata[26].
Thefollowingsemanticmetadataareextractedfromboundingboxes,poses,andsegmentationmaps:
• Crowdednessscore: numberofhumans(extractedfrom4DHumansinstances)
• SAMclutterscore: numberofSAMinstances
• COCOclutterscore: numberofCOCO[55]instances
• COCOinstancediversity: numberofuniqueCOCOinstanceclasses
• Objectnessscore: %ofpixelsthatbelongtocountableCOCOsemanticclasses
• Walkabilityscore: %ofpixelsbelongingtowalkableCOCOsemanticclassessuchas‘road’
• Semanticdiversity: numberofuniqueCOCOsemanticclasses
• Captionlength: lengthofthecaptionincharacters,words,andsentences
Theseareaimedtocapturethesemanticregularitiesofthesceneatamoreholisticlevelasopposed
topixel-basedrepresentations.
Similarly,geometricmetadatacapturesthescenegeometrymoreglobally. Theyareextractedfrom
surfacenormalsanddepthmaps:
• Geometriccomplexity: angularvarianceofsurfacenormals
• Occlusionscore: %ofocclusionedgesoverafixedthreshold
Finally,imageprocessingmetadatacontainsseveralaspectsofimagessuchasoriginalimageheight
andwidthbeforecropping,whichcanbeusedasconditioningtogeneratehigherqualityimages[66],
brightness,contrast,saturation,entropy,andcolorfulness[37]. Similartocolorpalette,thesehelp
withencodinglow-levelimagerepresentationsintothemodelandenablemoresteerablegeneration.
Text: Largelanguagemodels(LLMs)trainedonlargetextcorporalearnstrongrepresentationsas
shownbyseveralworks[23,68,85,64].WeincludecaptionsfromCC12M[15]andCOYO700M[11]
datasets,aswellaswebtextfromC4[68]forlanguagemodeling. Next,weemploybothastandard
WordPiece [23] tokenizer for captions as [62] as well as caption embeddings obtained from a
T5-XXL [68] encoder to capture better text representations, which have been shown to improve
text-to-imagegenerationfidelity[76,13](Seefig.4).
4SpatialdiscreteVAEwithdiffusiondecoder:RGB,normal,depth,edges SpatialdiscreteVAE:Segmentation,CLIP,DINOv2,ImageBind,SAMinst.
Noisedimage
1 2 3 1 2 3
encV oiT der VQ 4 5 6 D deiff cu os dio en r encV oiT der VQ 4 5 6 ViTdecoder
7 8 9 7 8 9
VQ-VAEquantizationloss VQ-VAEquantizationloss
Diffusionloss Reconstructionloss
MLPdiscreteVAE:Humanposes,DINOv2&ImageBindglobaltokens Sequencetokenizer:Text,boundingboxes,metadata,colorpalette
A B C
Inbamboothickets, Inbamboothickets,
enM coL dP er cM oe dm es- 1 2 3 MLPdecoder Are Ad utp ua mgn a nd z 'a e s' ,s cag le mntle WordPiece D E ...F WordPiece Are Ad utp ua mgn a nd z 'a e s' ,s cag le mntle
embrace. G embrace.
Reconstructionloss
Figure3: Tokenizationoverview.Weemploysuitabletokenizationschemesfordifferentmodalitiesbasedon
theirformatandperformance.Forimage-likemodalitiesandfeaturemaps,weusespatialVQ-VAEs[63]with
optionaldiffusiondecodersfordetailrichmodalitieslikeRGB.Fornon-spatialmodalitieslikeglobaltokensor
parameterizedposes,wecompressthemtoafixednumberofdiscretetokensusingMemcodes[60]withMLP
encodersanddecoders.AllsequencemodalitiesareencodedastextusingWordPiece[23].Theshownexamples
arerealtokenizerreconstructions.Noticethelowreconstructionerror.SeeappendixDformoredetails.
2.2 Tokenization
Tokenizationconsistsofconvertingmodalitiesandtasksintosequencesorsetsofdiscretetokens,
therebyunifyingtheirrepresentationspace. Thisiscriticalfortraininglargemultimodalmodelsas
itconfersthefollowingkeybenefits: 1)Itenablestrainingmultimodalandmultitaskmodelswith
asinglepre-trainingobjective. Aftertokenization,alltasksareformulatedasaper-tokenclassifi-
cationproblemusingthecross-entropyloss. Thisimprovestrainingstability,enablesfullparameter
sharing,andremovestheneedfortask-specificheads,lossfunctions,andlossbalancing. 2)Itmakes
generativetasksmoretractablebyallowingthemodeltoiterativelypredicttokens,eitherautoregres-
sively[69,91]orthroughprogressiveunmasking[14,13]. 3)Itreducescomputationalcomplexityby
compressingdensemodalitieslikeimagesintoasparsesequenceoftokens. Thisdecreasesmemory
andcomputerequirements,whichiscrucialwhenscalinguptolargerdatasetandmodelsizes.
Weusedifferenttokenizationapproachestodiscretizemodalitieswithdifferentcharacteristics. See
fig.3foranoverview. Tosummarize,wemainlyusethreedifferenttypesoftokenizers,asexplained
below. PleaseseeappendicesDandHformoredetailsandinsightsontokenizerdesignchoices.
ViTtokenizer(withoptionaldiffusiondecoder): Wetrainedmodality-specificViT[24]based
VQ-VAE[63]tokenizersforimage-likemodalitiessuchasedgesandfeaturemaps. Theresulting
tokensformasmallgridofsize14×14or16×16,accordingtothepseudo-labelerpatchsize. The
edgetokenizersuseadiffusiondecoder[78,62]togetvisuallymoreplausiblereconstructions.
MLPtokenizer: ForhumanposesandglobalembeddingsfromDINOv2andImageBind,weuse
BottleneckMLP[6]baseddiscreteVAEswithMemcodesquantization[60]totokenizethemintoa
smallnumberoftokens,e.g. 16.
Text tokenizer: We leverage a WordPiece [23] tokenizer which is used to encode not only text,
butalsoothermodalitiessuchasboundingboxes,colorpalettesandmetadatausingasharedsetof
specialtokenstoencodetheirtypeandvalues(SeeappendixD.6fordetails).
2.3 Trainingdetails
Datasets: Weperformthetrainingintwostages,namelya4Mpre-trainingstageonasignificantly
largerimagedataset,followedbyafine-tuningphaseonasmallerdatasetcontainingalargernumber
of modalities. Since the 4M-XL model showed signs of overfitting on sequence modalities when
trainedonCC12M[15],were-trainedthemodelsonCOYO700M[11],containing50timesmore
samples. COYO700M was pseudo labeled with the same modalities used for 4M. To cut down
on pseudo labeling cost when expanding the number of modalities, we decided to pseudo label
CC12MinsteadofCOYO700M,andfine-tunethemodelswithbothnewandoldmodalities. Toavoid
overfittingthelargermodels,weco-trainthemwithsamplesfromCOYO700M.Inadditiontothe
previouslymentionedmultimodaldatasets,wealsoincludedtheC4[68]textcorpusintraining. We
performthetrainingbyrandomlysamplingelementsofeachbatchfromanyofthesedatasets,given
5Fine-grainedmultimodalconditioningcontrol Improvedtextunderstandingcapabilities
Ctpuw pla aop yoetf noi ro so tn htwb ei ana prlplmitu cit nh: g Caalusa spthp ricot jtni uuo anrn ue gtliosenfp intuwato: Cagwstra arpe elpaekeit kin tniotpginnhogii lnn oop safou ntptw o: ho ledrs Cactshkla op eyp wa ssi ct nn ti rro st aeinn pewg tei an rwol sp kf iitu nhttwg:oon Captioninput:ametallicbluespheretotheleftofayellowboxmadeoffelt Captioninput:abluesemi-truckanditstrailerjumpingoverarowofmotorcycles
Humanpose
input:
4M-7(fromcaption) 4M-21(fromcaption) 4M-21(fromT5-XXLemb.) 4M-7(fromcaption) 4M-21(fromcaption) 4M-21(fromT5-XXLemb.)
Casfikgameup ti rcnt ei hi somn oafilin tswptout s: tick Capcooffieas rokp crpe iet ditleo oc bnh ruwoioi a ln dfflkp iab niu n gmut gs: oiin dne ets rhs ne Catamwna meopo aodt sil ui h oop ne wna tpaini hin nteipnru dgt s:oofn Caogdfroac ob wpo orlt nmioi dor tef hn wu eal ai nnp lakdp ia isnui ln egtt:ing Captioninput:ablackbackgroundwithalargeyellowcircleandasmallredsquare Captioninput:agreenpeppertotheleftofaredpepper
4M-7(fromcaption) 4M-21(fromcaption) 4M-21(fromT5-XXLemb.) 4M-7(fromcaption) 4M-21(fromcaption) 4M-21(fromT5-XXLemb.)
Probingwithgroundedgeneration Steerablemultimodaldatagenerationfrommetadata
Polygoninput RGBgeneration C ia np pt uio tn RGBgeneration M inpe uta td :ata Walkability #humans in# stS aA ncM es Oc sc cl ou rs eion reO sr oig luin tia ol n Contrast
10% 1 3 10% 64x64 15%
awinter
ridewith
family
SAMedges
input
C i a
s w
tn aoa ob bp up oo lu
p
edt wti
o
eo l nnn of
a
r b si ud lu nin e sg ec taa rat C i
a a
mna
p
Sp op
i
uwu ct nt
t
ii : suo tasrn
e inof
40% 4 30 25% 256x256 60%
scene 80% 20 200 85% 2048x2048 90%
ahistorical
photoofa
classiccar
Figure 4: Fine-grained & steerable multimodal generation. Top left: 4M-21 can generate variants of
imagesthataregroundedinanyinputmodality,herehumanposes. Bottomleft:Thisenablesustoperform
multimodaledits(e.g.editingtheshapeofapolygonorgroundinggenerationwithedges)andprobethelearned
representation.Forexample,byonlychangingtheshapeoftheellipse,4M-21rendersthebowlfromdifferent
angles.Topright:Bypre-trainingon21typesofmodalities,includingT5-XXLembeddings,andco-trainingon
alargetextcorpus,weshowimprovedtextunderstandingcapabilities(evenwhentheinputiscaptionsinsteadof
languagemodelembeddings). Bottomright: Comparedtogeneratingimagesfromcaptionsonly,metadata
providesamoredirectandsteerablewayofcontrollingthemultimodaldatagenerationprocess,enablingexciting
furtherresearchintogenerativedatasetdesign.
apre-determinedsetofsamplingweights,andperformlanguagemodelingonthem. Exactdetailson
thetrainingmixturearegiveninappendixE.2.
Architecture: We adopt 4M’s encoder-decoder based transformer architecture with additional
modality embeddings to accommodate new modalities. Similar to 4M, besides RGB tokens, the
encoderdirectlyacceptsRGBpixelswithalearnablepatch-wiseprojectiontoenableuseasaViT[24]
backbonefortransferlearning.
Maskingstrategy: Weusedbothmultimodalrandom[7,62]andspanmasking[68]strategiesthat
maskinputandtargettokens. WeinvokedatasetmixingratiosandDirichletsamplingparameters,α,
toensurestabletrainingonmultiplemodalitiesanddatasets,asdetailedinappendixE.2.
3 Multimodalcapabilities
Wedemonstrateabroadrangeofcapabilitiesunlockedby4M-21,includingsteerablemultimodal
generation(Sec. 3.1),multimodalretrieval(Sec.3.2)andstrongout-of-the-boxcapabilities(Sec.3.3).
Pleaseseetheprojectwebsiteformorevisualizationsdemonstratingthesecapabilities.
3.1 Steerablemultimodalgeneration
4M-21canpredictanytrainingmodalitybyiterativelydecodingtokens[62,14,13]. Thisisshownin
fig.2wherewecangenerateallmodalitiesfromagiveninputmodalityinaconsistentmanner.Further-
more,aswecangenerateanyofthetrainingmodalitiesfromanysubsetofothermodalities,bothcon-
ditionallyandunconditionally,itenablesseveralwaystoperformfine-grainedandmultimodalgener-
ation,asshowninfig.4. Thisincludesdiversecapabilitiessuchasperformingmultimodaledits,prob-
ingthelearnedrepresentations,andsteeringmultimodaldatageneration. Moreover,4M-21exhibits
improvedtextunderstandingcapabilitiesleadingtogeometricallyandsemanticallyplausiblegen-
erations,bothwhenconditioningonT5-XXLembeddingsandonregularcaptions(fig.4,topright).
6Any-to-RGBretrieval Any-to-anyretrieval Multimodalretrieval
Query Top-3Retrievals Query Top-3Retrievals Query Top-3Retrievals
skiseasoniscoming
afancymansion
brightness:30/255
afancymansion
brightness:200/255
abird’seyeview detailsofthe afancymansion
ofsunsetshores neuschwanstein
beachhotel castle brightness:200/255
walkability:75%
Figure5: Differentmodesofmultimodalretrieval.Weperformmultimodalretrievalsbypredictingglobal
embeddings(hereshownforDINOv2)fromagiveninput(ofanymodality)using4M-21andcomparingthe
cosinedistancesbetweenthequeryandretrievalsetembeddings.Left:RetrievingRGBimagesfromdistinctly
differentquerymodalities(hereRGB,segmentationmap,edges,depthmap,colorpalette,andcaption).Middle:
Retrieving any modality using any other modality as the query input. Each query modality constrains the
retrievalsdifferently,e.g.heretheRGBimageandcaptionqueriesalwaysyieldNeuschwansteincastleretrievals.
Incontrast,fordepthandsemanticqueries,thesceneismoreambiguous,thustheyretrieveotherbuildings
withsimilarcharacteristics.Right:Wecanalsocombineanysubsetofmodalitiestodefinethequeryinput,e.g.
surfacenormalsandacolorpalette,tobettercontroltheretrieval.SeeappendixB.2formoreresults.
3.2 Multimodalretrieval
OurmodelcanalsoperformmultimodalretrievalsbypredictingglobalembeddingsofDINOv2and
ImageBind from any (subset) of the input modalities. Once the global embeddings are obtained,
the retrieval is done by finding the retrieval set samples with the smallest cosine distance to the
query[65,33]. Asshowninfig.5,thisunlocksretrievalcapabilitiesthatwerenotpossiblewiththe
originalDINOv2andImageBindmodelssuchasretrievingRGBimagesoranyothermodalityvia
usinganyothermodalityasthequery. Furthermore,onecancombinemultiplemodalitiestopredict
theglobalembedding,resultinginbettercontroloverretrievals,asshownontheright.
RGB Input Predictions
Caption Bo bu on xd ei sng seS ge mm ea nn tati tc ion Depth CLIP nS ou rr mfa ac le s H pu om sea sn DINOv2 ImageBind Metadata C eda gn en sy eS dA gM es insS taA nM ces pC ao lelo ttr e
Figure6: Out-of-the-boxvisiontasks.GivenanRGBimage,4M-21canpredictalltaskssuccessfully,ascan
beseenfromtheirhighconsistencywiththepseudolabels.Seefig.7formoreresults.
7
lebaL
oduesP
snoitciderP
lebaL
oduesP
snoitciderP
lebaL
oduesP
snoitciderPTable1: Out-of-the-box(zero-shot)performance.Weshowtheperformanceforacommonsubsetoftasks:
surfacenormalsanddepthestimationonDIODE[86],semanticandinstancesegmentationonCOCO[55],
k-NNretrievalonImageNet-1K[75],and3Dhumankeypointestimationon3DPW[87].Wecomparetoasetof
strongbaselinesandspecialistmodels,includingourpseudolabelers.Ourmodellearnedtosolveallthesetasks
withoutlossofperformanceandissignificantlybetterthanthebaselinesandcompetitivewithpseudolabelers,
whilebeingasinglemodelforalltasks.✗denotesthatagivenmodelcannotsolvethetaskout-of-the-box.*
showsthetokenizerreconstructionqualityandprovidesanestimateontheperformanceupperbounddueto
tokenization.Seefig.13forqualitativecomparisons.Bestresultsarebolded,secondbestunderlined.
Method Normals↓ Depth↓ Sem.seg.↑ Inst.seg.↑ IN1KkNN↑ 3DhumanKP↓
Omnidata[44] 22.5 0.68 ✗ ✗ ✗ ✗
M2F-B[19] ✗ ✗ 45.7 ✗ ✗ ✗
SAM[47] ✗ ✗ ✗ 32.9 ✗ ✗
DINOv2-B14[65] ✗ ✗ ✗ ✗ 82.1/93.9 ✗
ImageBind-H14[33] ✗ ✗ ✗ ✗ 81.1/94.4 ✗
4D-Humans[35] ✗ ✗ ✗ ✗ ✗ 81.3
OASIS[18] 34.3 ✗ ✗ ✗ ✗ ✗
MiDaSDPT[70] ✗ 0.73 ✗ ✗ ✗ ✗
M2F-S[19] ✗ ✗ 44.6 ✗ ✗ ✗
M2F-L[19] ✗ ✗ 48.0 ✗ ✗ ✗
HMR[43] ✗ ✗ ✗ ✗ ✗ 130.0
UnifiedIO-B[59] 35.7 1.00 32.9 ✗ ✗ ✗
UnifiedIO-L[59] 33.9 0.87 41.6 ✗ ✗ ✗
UnifiedIO-XL[59] 31.0 0.82 44.3 ✗ ✗ ✗
UnifiedIO2-L[58] 37.1 0.96 38.9 ✗ ✗ ✗
UnifiedIO2-XL[58] 34.8 0.86 39.7 ✗ ✗ ✗
UnifiedIO2-XXL[58] 37.4 0.84 41.7 ✗ ✗ ✗
4M-7 B[62] 21.9 0.71 43.3 ✗ ✗ ✗
4M-21 B 21.7 0.71 42.5 15.9 73.1/89.7 108.3
4M-7 L[62] 21.5 0.69 47.2 ✗ ✗ ✗
4M-21 L 21.1 0.69 46.4 31.2 77.0/91.9 97.4
4M-7 XL[62] 20.6 0.69 48.1 ✗ ✗ ✗
4M-21 XL 20.8 0.68 48.1 32.0 78.3/92.4 92.0
Tokenizerbound* 4.0 0.06 90.5 91.2 80.2/93.0 17.5
3.3 Evaluatingout-of-the-boxcapabilities
4M-21iscapableofperformingarangeofcommonvisiontasksout-of-the-box,asdemonstrated
visuallyinfig.6. Intable1,weevaluatetheperformanceonDIODE[86]surfacenormalanddepth
estimation,COCO[55]semanticandinstancesegmentation,3DPW[87]3Dhumanposeestimation,
anddoImageNet-1K[75]kNNretrievalusingpredictedDINOv2globaltokens. Wecompareagainst
thepseudolabelingnetworks,strongbaselines,andthe4Mmodelfrom[62]trainedon7modalities.
Forsurfacenormalestimationandsemanticsegmentation,weobservedthatensemblingmultiple
predictionssignificantlyimprovesperformance,seeappendixFformoredetailsandresults.
Our model consistently achieves strong out-of-the-box performance, and often matches or even
outperformsthepseudolabelersandotherspecialistbaselines,whilebeingasinglemodelforall
tasks. Notice the large performance gap with other multitask models like Unified-IO [59] and
Unified-IO-2[58]. ForkNNretrieval,4M-21 XLperformanceapproachesthetokenizerbound,i.e.
theretrievalperformanceusingtheDINOv2tokenizerreconstructions. Whileoursmallermodelslag
behind4Mmodels,weobservethat4M-21 XLisabletomatchtheperformanceof4M-7 XL,butis
capableofinterfacingwiththreetimesthenumberofmodalities. Itisexpectedthatadditionalmodel
capacityisneeded,asitistrainedtosolvemanymoretasks.
4 Transferexperiments
Tostudythescalingcharacteristicsofpre-trainingany-to-anymodelsonamuchlargersetofmodal-
ities,wetrainmodelsacrossthreedifferentsizes: B,L,andXL.Wethentransfertheirencodersto
downstreamtasksandevaluateonbothunimodal(RGB)andmultimodal(RGB+Depth)settings.The
decodersarediscardedforalltransferexperiments,andweinsteadtraintask-specificheads. Weper-
formself-comparisonsinasimilarmannerto[62,7],aswellascomparingtoasetofstrongbaselines.
Unimodaltransfers. ForunimodaltransfersweleveragetheRGBpatchembeddingslearnedduring
thepre-training,asRGBpixelinputsareusedalongsidethetokenizedmodalities. FortheXLmodels
and DINOv2 g, we perform parameter-efficient fine-tuning using LoRA [40] instead of full fine-
8
srelebaloduesPTable 2: Unimodaltransferstudy. Wetransfer4M-21andbaselinestoImageNet-1K[75]classification,
ADE20K[100]semanticsegmentation,NYUv2[80]depthestimation,andARKitScenes[9](ARKS)3Dobject
detection. Weobservethat4M-211)doesnotloseperformanceforthetransfertasksthataresimilartothe
sevenmodalitiesof4M,i.e. firstthreecolumnsoftheresults,whilebeingabletosolvemanymore,and2)
leadstoimprovedperformancefornoveltasksthataremoredifferentfrom4Mmodalities,e.g. 3Dobject
detection(lastcolumn). Theimprovementsarefurtherverifiedinthemultimodaltransferresults(Table3)
showingtheusefulnessofnewmodalities.Bestresultspertaskarebolded,secondbestunderlined.
Pre-training Enc. IN1K ADE20K NYUv2-D ARKS
Method
data param. Acc.↑ mIoU↑ δ acc.↑ AP3D↑
1
MAEB[38] IN1K 84.2 46.1 89.1 30.9
DeiTIIIB[84] IN21K 85.4 49.0 87.4 36.1
MultiMAEB[7] IN1K 84.0 46.2 89.0 34.2
DINOv2B[65] LVD142M 86M 85.3 51.6 92.2 38.1
4M-7 B[62] CC12M 84.5 50.1 92.0 40.3
4M-7 B(Ours) COYO 84.4 49.4 91.4 38.6
4M-21 B CC12M+COYO+C4 84.5 50.1 90.8 42.4
MAEL[38] IN1K 86.8 51.8 93.6 36.2
DeiTIIIL[84] IN21K 87.0 52.0 89.6 40.3
DINOv2L[65] LVD142M 86.7 53.4 94.1 42.8
303M
4M-7 L[62] CC12M 86.6 53.4 94.4 46.8
4M-7 L(Ours) COYO 86.7 53.5 94.3 45.2
4M-21 L CC12M+COYO+C4 86.5 53.4 93.7 47.0
DINOv2g[65] LVD142M 1.1B 88.0 58.7 92.5 45.3
4M-7 XL[62] CC12M 87.0 55.0 96.1 48.1
4M-7 XL(Ours) COYO 1.2B 87.1 56.1 96.5 47.3
4M-21 XL CC12M+COYO+C4 87.1 56.0 96.5 48.4
tuning,whichsignificantlyimprovesresultsforXLmodels. Wedidnotobservesimilarperformance
gainsforthesmallermodels. FurthertrainingdetailsaredescribedinappendixG.
WeevaluateonImageNet-1Kclassification[22,75],ADE20Ksemanticsegmentation[100],NYUv2
depth estimation [80], and ARKitScenes [9] 3D object detection tasks. Some transfer tasks are
completelyunseenduringpre-training,e.g. objectclassificationor3Dobjectdetection,whileothers
areincludedasdifferentinstantiations,e.g.absolutedepthinsteadofrelativedepth,orusingADE20K
instead of COCO classes. We follow the best practices and commonly used settings from other
papers[62].
Theresultsareshownintable2. Wemakethefollowingobservations: 1)forthetransfertasksthat
aresimilartothesevenmodalitiesof4M,e.g. semanticsegmentationordepth,4M-21doesnotlose
performanceduetobeingtrainedonmanymoremodalities,2)fornoveltransfertaskslike3Dobject
detectionthataresufficientlydifferentfrom4Mmodalities,weobserveanimprovedperformance.
Moreover, the performance improves with larger model sizes, showing promising scaling trends.
Thesetrendscanbefurtherseeninthemultimodaltransferresults,whichwillbeexplainednext.
Multimodal transfers. We perform multimodal Table3: Multimodaltransferstudy. Wetrans-
transfers on NYUv2, Hypersim [72] semantic seg- ferboth4M-21and4M(pre-trainedonCC12M)
mentation,and3DobjectdetectiononARKitScenes. toNYUv2andHypersimsegmentation, and3D
WecomparetransfersusingRGBimagesonly,and objectdetectiononARKitScenes.Allmodelsare
RGB pixels + tokenized sensory depth as inputs. abletouseoptionallyavailabledepthwhenitis
ofhighquality(Hypersim&ARKitScenes),while
As table 3 shows, 4M-21 makes strong use of
ourmodelachievesthebestresults. Bestresults
optionally available depth inputs and significantly
arebolded,secondbestunderlined.
improvesuponthebaselines.
NYUv2-S Hypersim ARKitScenes
mIoU↑ mIoU↑ AP3D↑
5 RelatedWork
Method RGB RGB-D RGB RGB-D RGB RGB-D
4M-7 B 56.6 57.5 40.2 43.9 40.3 46.5
Multitask learning in vision involves training a 4M-21 B 58.7 59.7 38.6 46.4 42.4 48.1
single model to perform multiple visual tasks 4M-7 L 61.2 61.4 48.7 50.5 46.8 49.5
efficiently [12, 74]. Earlier methods [27, 61, 48] 4M-21 L 61.8 61.8 47.3 50.7 47.0 50.1
combinedmultipledensevisiontasksintoasingle 4M-7 XL 62.1 61.2 48.6 51.0 48.1 50.1
4M-21 XL 63.9 63.9 48.6 52.5 48.4 51.3
modelbutfacedchallengesscalingtoalargervariety
9oftasksandmodalities,limitedbytraininginstabilitiesandtheneedforcarefultaskselectionand
lossbalancingtoreducenegativetransfer[46,96,82,94].
Recently, discrete tokenization has enabled a shift towards integrating numerous vision tasks
into unified multimodal and multitask models such as Gato [71], OFA [88], Pix2Seq [16, 17],
UnifiedIO [59, 58], 4M [62], and more [49, 79, 5, 39, 25, 42, 4, 92, 83, 102, 52, 99]. These
methods first transform various modalities and tasks into sequences or sets of discrete to-
kens[23,50,63,29,90],andthentrainasingleTransformeronthesetokensusingeitherasequence
modeling[71,88,59,16,17,49,79]ormaskedmodelingobjective[62,89]. Somemethods(e.g.
Gato[71],UnifiedIO[59,58])performco-trainingonmultipledisjointdatasetsandarecapableof
performingawiderangeoftasks,butnotjointly. Incontrast,methodslike4M[62]trainonasingle
aligneddatasetthroughtheuseofpseudolabeling,enablingany-to-anymodalitypredictionbutona
typicallymorelimitedsetofmodalities. Wesignificantlyexpanduponthembyaddingtheabilityto
usethisframeworkforanevengreateramountofmodalitiesandcapabilities.
Furthermore, masked modeling has proven effective for learning useful representations in both
NLP[23,68]andvision[38,8,101,28]. Extendingittomultimodaldomains[7,34,89,62]enables
strongcross-modalrepresentationswhichiscriticalformultimodallearning. Whencombinedwith
tokenization,maskedmodelingalsoenablesgenerativeapplications[14,53,13,81,62]. Ourwork
highlightstheabilityofmaskedmodelingtoexpandtoamuchgreatersetofmodalitiesthanpreviously
shown,improvingupontheout-of-the-boxandmultimodalgenerationcapabilitiesofpreviousworks.
6 LimitationsandDiscussion
We demonstrate training an any-to-any model on tens of diverse modalities and tasks. This is
achieved by mapping all modalities to discrete sets of tokens via modality-specific tokenizers
and using a multimodal masked training objective [62]. We successfully scaled the training to
three billion parameters and to 21 modalities and different datasets, without a degradation in
performancecomparedtotheexistingmorespecializedsingle/fewtaskmodels. Thisresultsinstrong
out-of-the-boxcapabilitiesaswellnewpotentialformultimodalinteraction,generation,andretrieval,
allbyasingleunifiedmodel. Below,wediscusslimitationsofourmethodandfuturework.
Transfer/emergentcapabilities: Onehopefrommultitasktrainingisleadingtoamodelthatcansolve
noveltasks,oftenreferredtoas“transfer”or“emergent”capabilities. Whileamultitaskmodelbrings
severalkeyadvantagesevenwithouttransfer/emergenceadvantages(usingasinglemodelforbroad
out-of-the-boxcapabilitieswithoutsacrificingperformance,modalityfusion,etc.),andweshowed
success at them, we observe that the potential in transfer/emergence improvement remains
largely untapped. In general, compared to LLMs, vision/multimodal models in the community
havenotshownexcitingresultsintermsoftransfer/emergenceyet. Wefindthistobeanimportant
pointforusandthecommunitytoaddressinthefuture,e.g.,viadesigningmultitaskarchitectures
thathaveemergence,incontrasttoout-of-the-boxcapabilities,astheirmainobjective.
Better tokenization: Like any token-based model, 4M-21 can directly benefit from progress on
tokenizers,e.g. higherreconstructionfidelity.
Co-trainingonpartiallyaligneddatasets: Weshowedthepossibilityoftrainingonpartiallyaligned
datasets,e.g. textdatafromC4andothermodalitiesfromCC12M,yetfurtherinvestigationsand
alargermixtureofdatasetsareexpectedtobringstrongercapabilities,whichweaimasfuturework.
References
[1] OpenCV.https://opencv.org/ 4
[2] PyPalette.https://github.com/adamgrieger/pypalette 3,24
[3] PythonPillow.https://python-pillow.org/ 4
[4] Aghajanyan,A.,Huang,B.,Ross,C.,Karpukhin,V.,Xu,H.,Goyal,N.,Okhonko,D.,Joshi,M.,Ghosh,
G., Lewis, M., Zettlemoyer, L.: CM3: A causal masked multimodal model of the internet. ArXiv
abs/2201.07520(2022) 10
[5] Aghajanyan,A.,Yu,L.,Conneau,A.,Hsu,W.N.,Hambardzumyan,K.,Zhang,S.,Roller,S.,Goyal,
N., Levy, O., Zettlemoyer, L.: Scaling laws for generative mixed-modal language models. ArXiv
abs/2301.03728(2023) 10
10[6] Bachmann,G.,Anagnostidis,S.,Hofmann,T.: Scalingmlps: Ataleofinductivebias.arXivpreprint
arXiv:2306.13575(2023) 5,24
[7] Bachmann,R.,Mizrahi,D.,Atanov,A.,Zamir,A.:MultiMAE:Multi-modalmulti-taskmaskedautoen-
coders.EuropeanConferenceonComputerVision(2022) 6,8,9,10,28,29
[8] Bao,H.,Dong,L.,Piao,S.,Wei,F.:BEiT:BERTpre-trainingofimagetransformers.In:International
ConferenceonLearningRepresentations(2022) 10
[9] Baruch,G.,Chen,Z.,Dehghan,A.,Dimry,T.,Feigin,Y.,Fu,P.,Gebauer,T.,Joffe,B.,Kurz,D.,Schwartz,
A.,Shulman,E.: Arkitscenes-adiversereal-worlddatasetfor3dindoorsceneunderstandingusing
mobilergb-ddata.In:NeurIPS(2021),https://arxiv.org/pdf/2111.08897.pdf 9
[10] Burgess, N., Milanovic, J., Stephens, N., Monachopoulos, K., Mansell, D.: Bfloat16processingfor
neuralnetworks.In:2019IEEE26thSymposiumonComputerArithmetic(ARITH).pp.88–91(2019).
https://doi.org/10.1109/ARITH.2019.00022 26
[11] Byeon, M., Park, B., Kim, H., Lee, S., Baek, W., Kim, S.: COYO-700M: Image-text pair dataset.
https://github.com/kakaobrain/coyo-dataset(2022) 2,4,5,22,25
[12] Caruana, R.: Multitask learning. Machine Learning 28, 41–75 (1997).
https://doi.org/10.1023/A:1007379606734 9
[13] Chang,H.,Zhang,H.,Barber,J.,Maschinot,A.,Lezama,J.,Jiang,L.,Yang,M.,Murphy,K.P.,Freeman,
W.T., Rubinstein, M., Li, Y., Krishnan, D.: Muse: Text-to-image generation via masked generative
transformers.In:InternationalConferenceonMachineLearning(2023) 4,5,6,10,23
[14] Chang,H.,Zhang,H.,Jiang,L.,Liu,C.,Freeman,W.T.:MaskGIT:Maskedgenerativeimagetransformer.
2022IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR)pp.11305–11315
(2022) 5,6,10
[15] Changpinyo,S.,Sharma,P.K.,Ding,N.,Soricut,R.: Conceptual12m: Pushingweb-scaleimage-text
pre-trainingtorecognizelong-tailvisualconcepts.2021IEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPR)pp.3557–3567(2021) 4,5,25,31
[16] Chen,T.,Saxena,S.,Li,L.,Fleet,D.J.,Hinton,G.:Pix2seq:Alanguagemodelingframeworkforobject
detection.In:InternationalConferenceonLearningRepresentations(2022) 2,10,24,25
[17] Chen,T.,Saxena,S.,Li,L.,Lin,T.Y.,Fleet,D.J.,Hinton,G.E.:Aunifiedsequenceinterfaceforvision
tasks.AdvancesinNeuralInformationProcessingSystems35,31333–31346(2022) 2,10
[18] Chen,W.,Qian,S.,Fan,D.,Kojima,N.,Hamilton,M.,Deng,J.:Oasis:Alarge-scaledatasetforsingle
image3dinthewild.In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition.pp.679–688(2020) 8
[19] Cheng,B.,Misra,I.,Schwing,A.G.,Kirillov,A.,Girdhar,R.:Masked-attentionmasktransformerfor
universalimagesegmentation.CVPR(2022) 4,8,23
[20] Clark,K.,Luong,M.T.,Le,Q.V.,Manning,C.D.:Electra:Pre-trainingtextencodersasdiscriminators
ratherthangenerators.In:InternationalConferenceonLearningRepresentations(2020) 28,29
[21] Cubuk,E.D.,Zoph,B.,Shlens,J.,Le,Q.V.:Randaugment:Practicalautomateddataaugmentationwith
areducedsearchspace.In: ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognitionworkshops.pp.702–703(2020) 28
[22] Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,Fei-Fei,L.:ImageNet:Alarge-scalehierarchicalimage
database.2009IEEEConferenceonComputerVisionandPatternRecognitionpp.248–255(2009) 9
[23] Devlin,J.,Chang,M.W.,Lee,K.,Toutanova,K.:BERT:Pre-trainingofdeepbidirectionaltransformers
forlanguageunderstanding.In:NorthAmericanChapteroftheAssociationforComputationalLinguistics
(2019) 2,4,5,10
[24] Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,T.,Dehghani,M.,
Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.: An image is worth 16x16 words:
Transformersforimagerecognitionatscale.In:InternationalConferenceonLearningRepresentations
(2021) 2,5,6
[25] Driess,D.,Xia,F.,Sajjadi,M.S.,Lynch,C.,Chowdhery,A.,Ichter,B.,Wahid,A.,Tompson,J.,Vuong,
Q.,Yu,T.,etal.:Palm-e:Anembodiedmultimodallanguagemodel.arXivpreprintarXiv:2303.03378
(2023) 10
11[26] Eftekhar,A.,Sax,A.,Bachmann,R.,Malik,J.,Zamir,A.R.:Omnidata:Ascalablepipelineformaking
multi-taskmid-levelvisiondatasetsfrom3dscans.2021IEEE/CVFInternationalConferenceonComputer
Vision(ICCV)pp.10766–10776(2021) 4
[27] Eigen,D.,Fergus,R.:Predictingdepth,surfacenormalsandsemanticlabelswithacommonmulti-scale
convolutionalarchitecture.In:ProceedingsoftheIEEEinternationalconferenceoncomputervision.pp.
2650–2658(2015) 9
[28] El-Nouby,A.,Izacard,G.,Touvron,H.,Laptev,I.,Jegou,H.,Grave,E.:Arelarge-scaledatasetsnecessary
forself-supervisedpre-training?ArXivabs/2112.10740(2021) 10
[29] Esser,P.,Rombach,R.,Ommer,B.: Tamingtransformersforhigh-resolutionimagesynthesis.2021
IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR)pp.12868–12878(2020)
10
[30] Feichtenhofer,C.,Li,Y.,He,K.,etal.:Maskedautoencodersasspatiotemporallearners.Advancesin
neuralinformationprocessingsystems35,35946–35958(2022) 26
[31] Fifty, C., Amid, E., Zhao, Z., Yu, T., Anil, R., Finn, C.: Efficiently identifying task groupings for
multi-tasklearning.AdvancesinNeuralInformationProcessingSystems34,27503–27516(2021) 2
[32] Ghiasi,G.,Cui,Y.,Srinivas,A.,Qian,R.,Lin,T.Y.,Cubuk,E.D.,Le,Q.V.,Zoph,B.:Simplecopy-paste
is a strong data augmentation method for instance segmentation. In: Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition.pp.2918–2928(2021) 29
[33] Girdhar,R.,El-Nouby,A.,Liu,Z.,Singh,M.,Alwala,K.V.,Joulin,A.,Misra,I.: ImageBind: One
embeddingspacetobindthemall.In:ProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition.pp.15180–15190(2023) 2,4,7,8,23
[34] Girdhar,R.,El-Nouby,A.,Singh,M.,Alwala,K.V.,Joulin,A.,Misra,I.: OmniMAE:Singlemodel
maskedpretrainingonimagesandvideos.In:ProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition.pp.10406–10417(2023) 10
[35] Goel,S.,Pavlakos,G.,Rajasegaran,J.,Kanazawa,A.,Malik,J.: Humansin4D:Reconstructingand
trackinghumanswithtransformers.In:ICCV(2023) 2,4,8,23,28
[36] Goyal,P.,Dollár,P.,Girshick,R.B.,Noordhuis,P.,Wesolowski,L.,Kyrola,A.,Tulloch,A.,Jia,Y.,He,
K.:Accurate,largeminibatchsgd:Trainingimagenetin1hour.ArXivabs/1706.02677(2017) 26,28
[37] Hasler,D.,Suesstrunk,S.E.:Measuringcolorfulnessinnaturalimages.In:Humanvisionandelectronic
imagingVIII.vol.5007,pp.87–95.SPIE(2003) 4,23
[38] He,K.,Chen,X.,Xie,S.,Li,Y.,Doll’ar,P.,Girshick,R.B.: Maskedautoencodersarescalablevision
learners.2022IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR)pp.15979–
15988(2021) 9,10,23
[39] Hu,A.,Russell,L.,Yeo,H.,Murez,Z.,Fedoseev,G.,Kendall,A.,Shotton,J.,Corrado,G.:Gaia-1:A
generativeworldmodelforautonomousdriving.arXivpreprintarXiv:2309.17080(2023) 10
[40] Hu,J.E.,Shen,Y.,Wallis,P.,Allen-Zhu,Z.,Li,Y.,Wang,S.,Chen,W.: LoRA:Low-rankadaptation
of large language models. ArXiv abs/2106.09685 (2021), https://api.semanticscholar.org/
CorpusID:235458009 8,29
[41] Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deepnetworkswithstochasticdepth.In:
Europeanconferenceoncomputervision.pp.646–661.Springer(2016) 28,29
[42] Huang,S.,Dong,L.,Wang,W.,Hao,Y.,Singhal,S.,Ma,S.,Lv,T.,Cui,L.,Mohammed,O.K.,Liu,Q.,
Aggarwal,K.,Chi,Z.,Bjorck,J.,Chaudhary,V.,Som,S.,Song,X.,Wei,F.:Languageisnotallyouneed:
Aligningperceptionwithlanguagemodels.ArXivabs/2302.14045(2023) 10
[43] Kanazawa,A.,Black,M.J.,Jacobs,D.W.,Malik,J.:End-to-endrecoveryofhumanshapeandpose.In:
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.pp.7122–7131(2018) 8
[44] Kar,O.F.,Yeo,T.,Atanov,A.,Zamir,A.:3dcommoncorruptionsanddataaugmentation.In:Proceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.18963–18974(2022) 4,8
[45] Ke,L.,Ye,M.,Danelljan,M.,Liu,Y.,Tai,Y.W.,Tang,C.K.,Yu,F.:Segmentanythinginhighquality
(2023) 23
12[46] Kendall,A.,Gal,Y.,Cipolla,R.:Multi-tasklearningusinguncertaintytoweighlossesforscenegeometry
andsemantics.2018IEEE/CVFConferenceonComputerVisionandPatternRecognitionpp.7482–7491
(2017),https://api.semanticscholar.org/CorpusID:4800342 2,10
[47] Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S.,
Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.B.: Segmentanything.ArXivabs/2304.02643(2023),
https://api.semanticscholar.org/CorpusID:257952310 2,4,8,23,26
[48] Kokkinos,I.:Ubernet:Trainingauniversalconvolutionalneuralnetworkforlow-,mid-,andhigh-level
visionusingdiversedatasetsandlimitedmemory.In:ProceedingsoftheIEEEconferenceoncomputer
visionandpatternrecognition.pp.6129–6138(2017) 9
[49] Kolesnikov, A., Susano Pinto, A., Beyer, L., Zhai, X., Harmsen, J., Houlsby, N.: UViM:Aunified
modelingapproachforvisionwithlearnedguidingcodes.AdvancesinNeuralInformationProcessing
Systems35,26295–26308(2022) 10
[50] Kudo,T.,Richardson,J.: Sentencepiece: Asimpleandlanguageindependentsubwordtokenizerand
detokenizerforneuraltextprocessing.arXivpreprintarXiv:1808.06226(2018) 10
[51] Levine,Y.,Lenz,B.,Lieber,O.,Abend,O.,Leyton-Brown,K.,Tennenholtz,M.,Shoham,Y.:{PMI}-
masking:Principledmaskingofcorrelatedspans.In:InternationalConferenceonLearningRepresenta-
tions(2021),https://openreview.net/forum?id=3Aoft6NWFej 25
[52] Li,H.,Zhu,J.,Jiang,X.,Zhu,X.,Li,H.,Yuan,C.,Wang,X.,Qiao,Y.,Wang,X.,Wang,W.,Dai,J.:
Uni-perceiverv2:Ageneralistmodelforlarge-scalevisionandvision-languagetasks(2022) 10
[53] Li,T.,Chang,H.,Mishra,S.,Zhang,H.,Katabi,D.,Krishnan,D.:[mage]:MAskedgenerativeencoder
tounifyrepresentationlearningandimagesynthesis.In:ProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition.pp.2142–2152(2023) 10
[54] Li,Y.,Mao,H.,Girshick,R.,He,K.:Exploringplainvisiontransformerbackbonesforobjectdetection.
In:ComputerVision–ECCV2022:17thEuropeanConference,TelAviv,Israel,October23–27,2022,
Proceedings,PartIX.pp.280–296.Springer(2022) 4,23
[55] Lin,T.Y.,Maire,M.,Belongie,S.J.,Hays,J.,Perona,P.,Ramanan,D.,Dollár,P.,Zitnick,C.L.:Microsoft
COCO:Commonobjectsincontext.In:EuropeanConferenceonComputerVision(2014) 4,8
[56] Liu,Z.,Lin,Y.,Cao,Y.,Hu,H.,Wei,Y.,Zhang,Z.,Lin,S.,Guo,B.: Swintransformer: Hierarchical
visiontransformerusingshiftedwindows.In:ProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision(ICCV)(2021) 23
[57] Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International Conference on
LearningRepresentations(2019) 26,28,29,30
[58] Lu,J.,Clark,C.,Lee,S.,Zhang,Z.,Khosla,S.,Marten,R.,Hoiem,D.,Kembhavi,A.:Unified-IO2:Scal-
ingautoregressivemultimodalmodelswithvision,language,audio,andaction.ArXivabs/2312.17172
(2023),https://api.semanticscholar.org/CorpusID:266573555 1,2,8,10,26,27,28
[59] Lu, J., Clark, C., Zellers, R., Mottaghi, R., Kembhavi, A.: Unified-IO: A unified model for vision,
language,andmulti-modaltasks.In:TheEleventhInternationalConferenceonLearningRepresentations
(2023) 1,2,8,10,26,27,28
[60] Mama, R., Tyndel, M.S., Kadhim, H., Clifford, C., Thurairatnam, R.: Nwt: Towards natural
audio-to-videogenerationwithrepresentationlearning.ArXivabs/2106.04283(2021),https://api.
semanticscholar.org/CorpusID:235367982 5
[61] Misra, I., Shrivastava, A., Gupta, A., Hebert, M.: Cross-stitch networks for multi-task learning. In:
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.pp.3994–4003(2016) 9
[62] Mizrahi, D., Bachmann, R., Kar, O.F., Yeo, T., Gao, M., Dehghan, A., Zamir, A.: 4M: Massively
multimodalmaskedmodeling.In:AdvancesinNeuralInformationProcessingSystems(2023) 1,2,3,4,
5,6,8,9,10,22,24,25,26,28,29
[63] vandenOord,A.,Vinyals,O.,Kavukcuoglu,K.:Neuraldiscreterepresentationlearning.Advancesin
neuralinformationprocessingsystems30(2017) 2,5,10
[64] OpenAI:GPT-4technicalreport(2023) 4
13[65] Oquab,M.,Darcet,T.,Moutakanni,T.,Vo,H.Q.,Szafraniec,M.,Khalidov,V.,Fernandez,P.,Haziza,
D.,Massa,F.,El-Nouby,A.,Assran,M.,Ballas,N.,Galuba,W.,Howes,R.,Huang,P.Y.B.,Li,S.W.,
Misra,I.,Rabbat,M.G.,Sharma,V.,Synnaeve,G.,Xu,H.,Jégou,H.,Mairal,J.,Labatut,P.,Joulin,A.,
Bojanowski,P.:DINOv2:Learningrobustvisualfeatureswithoutsupervision.ArXivabs/2304.07193
(2023) 2,4,7,8,9,23,28
[66] Podell,D.,English,Z.,Lacey,K.,Blattmann,A.,Dockhorn,T.,Muller,J.,Penna,J.,Rombach,R.:
SDXL:Improvinglatentdiffusionmodelsforhigh-resolutionimagesynthesis.ArXivabs/2307.01952
(2023),https://api.semanticscholar.org/CorpusID:259341735 4,23
[67] Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,Askell,A.,Mishkin,P.,
Clark,J.,etal.:Learningtransferablevisualmodelsfromnaturallanguagesupervision.In:International
ConferenceonMachineLearning(2021) 4,23
[68] Raffel,C.,Shazeer,N.M.,Roberts,A.,Lee,K.,Narang,S.,Matena,M.,Zhou,Y.,Li,W.,Liu,P.J.:
Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.TheJournalofMachine
LearningResearch21(1),5485–5551(2020) 2,4,5,6,10,22,23,25
[69] Ramesh,A.,Pavlov,M.,Goh,G.,Gray,S.,Voss,C.,Radford,A.,Chen,M.,Sutskever,I.: Zero-shot
text-to-imagegeneration.In:InternationalConferenceonMachineLearning(2021) 5
[70] Ranftl, R., Bochkovskiy, A., Koltun, V.: Visiontransformersfordenseprediction.2021IEEE/CVF
InternationalConferenceonComputerVision(ICCV)pp.12159–12168(2021) 8,23
[71] Reed,S.,Zolna,K.,Parisotto,E.,Colmenarejo,S.G.,Novikov,A.,Barth-Maron,G.,Gimenez,M.,Sulsky,
Y.,Kay,J.,Springenberg,J.T.,etal.:Ageneralistagent.arXivpreprintarXiv:2205.06175(2022) 10
[72] Roberts,M.,Paczan,N.:Hypersim:Aphotorealisticsyntheticdatasetforholisticindoorsceneunder-
standing.2021IEEE/CVFInternationalConferenceonComputerVision(ICCV)pp.10892–10902(2020)
9
[73] Ruder,S.:Anoverviewofmulti-tasklearningindeepneuralnetworks.ArXivabs/1706.05098(2017),
https://api.semanticscholar.org/CorpusID:10175374 2
[74] Ruder,S.:Anoverviewofmulti-tasklearningindeepneuralnetworks.arXivpreprintarXiv:1706.05098
(2017) 9
[75] Russakovsky,O.,Deng,J.,Su,H.,Krause,J.,Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,A.,Khosla,A.,
Bernstein,M.S.,Berg,A.C.,Fei-Fei,L.:ImageNetlargescalevisualrecognitionchallenge.International
JournalofComputerVision115,211–252(2014) 8,9
[76] Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Denton,E.L.,Ghasemipour,K.,GontijoLopes,R.,
KaragolAyan,B.,Salimans,T.,etal.:Photorealistictext-to-imagediffusionmodelswithdeeplanguage
understanding.AdvancesinNeuralInformationProcessingSystems35,36479–36494(2022) 4,23
[77] Shazeer,N.:GLUvariantsimprovetransformer.ArXivabs/2002.05202(2020) 26
[78] Shi,J.,Wu,C.,Liang,J.,Liu,X.,Duan,N.: DiVAE:Photorealisticimagessynthesiswithdenoising
diffusiondecoder.ArXivabs/2206.00386(2022) 5
[79] Shukor,M.,Dancette,C.,Rame,A.,Cord,M.:Unifiedmodelforimage,video,audioandlanguagetasks.
arXivpreprintarXiv:2307.16184(2023) 10
[80] Silberman,N.,Hoiem,D.,Kohli,P.,Fergus,R.:IndoorsegmentationandsupportinferencefromRGBD
images.In:EuropeanConferenceonComputerVision(2012) 9
[81] Sohn,K.,Ruiz,N.,Lee,K.,Chin,D.C.,Blok,I.,Chang,H.,Barber,J.,Jiang,L.,Entis,G.,Li,Y.,etal.:
Styledrop:Text-to-imagegenerationinanystyle.arXivpreprintarXiv:2306.00983(2023) 10
[82] Standley,T.,Zamir,A.,Chen,D.,Guibas,L.,Malik,J.,Savarese,S.: Whichtasksshouldbelearned
togetherinmulti-tasklearning? In: InternationalConferenceonMachineLearning.pp.9120–9132.
PMLR(2020) 2,10
[83] Team,C.:Chameleon:Mixed-modalearly-fusionfoundationmodels(2024) 10
[84] Touvron,H.,Cord,M.,Jégou,H.:DeiTIII:Revengeofthevit.In:EuropeanConferenceonComputer
Vision.pp.516–533(2022) 9
14[85] Touvron,H.,Lavril,T.,Izacard,G.,Martinet,X.,Lachaux,M.A.,Lacroix,T.,Rozière,B.,Goyal,N.,
Hambro,E.,Azhar,F.,etal.: Llama: Openandefficientfoundationlanguagemodels.arXivpreprint
arXiv:2302.13971(2023) 4
[86] Vasiljevic,I.,Kolkin,N.,Zhang,S.,Luo,R.,Wang,H.,Dai,F.Z.,Daniele,A.F.,Mostajabi,M.,Basart,S.,
Walter,M.R.,etal.:Diode:Adenseindoorandoutdoordepthdataset.arXivpreprintarXiv:1908.00463
(2019) 8
[87] VonMarcard,T.,Henschel,R.,Black,M.J.,Rosenhahn,B.,Pons-Moll,G.: Recoveringaccurate3d
humanposeinthewildusingimusandamovingcamera.In:ProceedingsoftheEuropeanconferenceon
computervision(ECCV).pp.601–617(2018) 8
[88] Wang,P.,Yang,A.,Men,R.,Lin,J.,Bai,S.,Li,Z.,Ma,J.,Zhou,C.,Zhou,J.,Yang,H.:Ofa:Unifying
architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In:
InternationalConferenceonMachineLearning(2022) 2,10
[89] Wang,W.,Bao,H.,Dong,L.,Bjorck,J.,Peng,Z.,Liu,Q.,Aggarwal,K.,Mohammed,O.K.,Singhal,S.,
Som,S.,Wei,F.:Imageasaforeignlanguage:BEiTpretrainingforallvisionandvision-languagetasks.
ArXivabs/2208.10442(2022) 4,10
[90] Yu,J.,Li,X.,Koh,J.Y.,Zhang,H.,Pang,R.,Qin,J.,Ku,A.,Xu,Y.,Baldridge,J.,Wu,Y.: Vector-
quantizedimagemodelingwithimprovedVQGAN.In:InternationalConferenceonLearningRepresen-
tations(2022) 10
[91] Yu,J.,Xu,Y.,Koh,J.Y.,Luong,T.,Baid,G.,Wang,Z.,Vasudevan,V.,Ku,A.,Yang,Y.,Ayan,B.K.,
Hutchinson,B.,Han,W.,Parekh,Z.,Li,X.,Zhang,H.,Baldridge,J.,Wu,Y.: Scalingautoregressive
modelsforcontent-richtext-to-imagegeneration.TransactionsonMachineLearningResearch(2022) 5
[92] Yu,L.,Shi,B.,Pasunuru,R.,Muller,B.,Golovneva,O.Y.,Wang,T.,Babu,A.,Tang,B.,Karrer,B.,
Sheynin,S.,Ross,C.,Polyak,A.,Howes,R.,Sharma,V.,Xu,P.,Tamoyan,H.,Ashual,O.,Singer,U.,Li,
S.W.,Zhang,S.,James,R.,Ghosh,G.,Taigman,Y.,Fazel-Zarandi,M.,Celikyilmaz,A.,Zettlemoyer,L.,
Aghajanyan,A.:Scalingautoregressivemulti-modalmodels:Pretrainingandinstructiontuning.ArXiv
abs/2309.02591(2023) 10
[93] Yu,T.,Kumar,S.,Gupta,A.,Levine,S.,Hausman,K.,Finn,C.:Gradientsurgeryformulti-tasklearning.
ArXivabs/2001.06782(2020),https://api.semanticscholar.org/CorpusID:210839011 2
[94] Yu,T.,Kumar,S.,Gupta,A.,Levine,S.,Hausman,K.,Finn,C.:Gradientsurgeryformulti-tasklearning.
AdvancesinNeuralInformationProcessingSystems33,5824–5836(2020) 2,10
[95] Yun,S.,Han,D.,Oh,S.J.,Chun,S.,Choe,J.,Yoo,Y.:Cutmix:Regularizationstrategytotrainstrong
classifiers with localizable features. In: Proceedings of the IEEE/CVF international conference on
computervision.pp.6023–6032(2019) 28
[96] Zamir, A.R., Sax, A., Shen, B.W., Guibas, L.J., Malik, J., Savarese, S.: Taskonomy: Disentangling
tasktransferlearning.2018IEEE/CVFConferenceonComputerVisionandPatternRecognitionpp.
3712–3722(2018) 10
[97] Zhang,H.,Cisse,M.,Dauphin,Y.N.,Lopez-Paz,D.:mixup: Beyondempiricalriskminimization.In:
InternationalConferenceonLearningRepresentations(2018) 28
[98] Zhang,L.,Rao,A.,Agrawala,M.: Addingconditionalcontroltotext-to-imagediffusionmodels.In:
ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.pp.3836–3847(2023)4,24
[99] Zhang,Y.,Gong,K.,Zhang,K.,Li,H.,Qiao,Y.,Ouyang,W.,Yue,X.: Meta-transformer: Aunified
frameworkformultimodallearning(2023) 10
[100] Zhou,B.,Zhao,H.,Puig,X.,Fidler,S.,Barriuso,A.,Torralba,A.: SceneparsingthroughADE20K
dataset.2017IEEEConferenceonComputerVisionandPatternRecognition(CVPR)pp.5122–5130
(2017) 9
[101] Zhou,J.,Wei,C.,Wang,H.,Shen,W.,Xie,C.,Yuille,A.,Kong,T.:iBoT:ImageBERTpre-trainingwith
onlinetokenizer.In:InternationalConferenceonLearningRepresentations(2022) 10
[102] Zhu,X.,Zhu,J.,Li,H.,Wu,X.,Wang,X.,Li,H.,Wang,X.,Dai,J.:Uni-Perceiver:Pre-trainingunified
architectureforgenericperceptionforzero-shotandfew-shottasks.2022IEEE/CVFConferenceon
ComputerVisionandPatternRecognition(CVPR)pp.16783–16794(2021) 10
15Appendix
A Code,Pre-trainedModels&InteractiveVisualizations 17
B MultimodalCapabilities 17
B.1 Additionalmultimodalgeneration&probingvisualizations . . . . . . . . . . . . . 17
B.2 Additionalretrievalvisualizations . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C AdditionalAblations 22
C.1 Ablationofpre-trainingdataandmodalities . . . . . . . . . . . . . . . . . . . . . 22
C.2 Ablationofensemblingthepredictions . . . . . . . . . . . . . . . . . . . . . . . . 22
D MultimodalDataset&TokenizationDetails 23
D.1 Pseudolabeledmultimodaltrainingdataset . . . . . . . . . . . . . . . . . . . . . 23
D.2 Tokenizationofhumanposes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.3 TokenizationofSAMinstances . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.4 Tokenizationofglobalfeaturemaps . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.5 Tokenizationofdensefeaturemaps. . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.6 Tokenizationofsequencemodalities . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.7 TokenizationofCannyandSAMedges . . . . . . . . . . . . . . . . . . . . . . . 25
E TrainingDetails 25
E.1 Modality-specificaccommodations . . . . . . . . . . . . . . . . . . . . . . . . . . 25
E.2 Multidatasetco-traininganddiversifiedmultimodalmaskingstrategy. . . . . . . . 25
F Out-of-the-boxEvaluationDetails 26
F.1 SurfacenormalanddepthestimationonDIODE . . . . . . . . . . . . . . . . . . . 26
F.2 SemanticandinstancesegmentationonCOCO . . . . . . . . . . . . . . . . . . . 26
F.3 kNNretrievalonImageNet-1K . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
F.4 3Dhumanposepredictionon3DPW . . . . . . . . . . . . . . . . . . . . . . . . . 28
G TransferEvaluationDetails 28
H InvestigatingDifferentTokenizationSchemes 29
I BroaderImpact 31
I.1 Computationalcosts. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
I.2 Socialimpact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
16A Code,Pre-trainedModels&InteractiveVisualizations
Pleaseseeourwebsitefordocumentedopen-sourcecode,pre-trainedmodelandtokenizerweights,
aswellasanoverviewvideoandadditionalinteractivevisualizations.
B MultimodalCapabilities
B.1 Additionalmultimodalgeneration&probingvisualizations
PleaseseeFigures7,8,9,10foradditionalqualitativeresultsonany-to-anygeneration,controlled
generation,andtextunderstandingcapabilitiesofourmodel.
RGB Input Predictions
Caption Bo bu on xd ei sng seS ge mm ea nn tati tc io n Depth CLIP nS ou rr mfa ac le s H pu om sea sn DINOv2 ImageBind Metadata C eda gn en sy eS dA gM es insS taA nM c es pC ao lelo ttr e
Figure7: RGB-to-anygeneration. Thisisanextensionoffig.6andvisualizesthemodel’sout-of-
the-boxcapabilitiesonvariousvisiontasks,comparedtothepseudolabeleroutputs.
17
st)rselpexbEa
Lk soadTu
eelsgPn(iS
snoitciderP
st)rselpexbEa
Lk soadTu
eelsgPn(iS
snoitciderP
st)rselpexbEa
Lk soadTu
eelsgPn(iS
snoitciderP
st)rselpexbEa
Lk soadTu
eelsgPn(iS
snoitciderP
st)rselpexbEa
Lk soadTu
eelsgPn(iS
snoitciderP
st)rselpexbEa
Lk soadTu
eelsgPn(iS
snoitciderP
st)rselpexbEa
Lk soadTu
eelsgPn(iS
snoitciderP
st)rselpexbEa
Lk soadTu
eelsgPn(iS
snoitciderPImagemetadata Geometricmetadata
OriginalRes.Brightness SaturationColorfulness Contrast Entropy Geometriccomplexity Occlusionscore
128x128 10% 0% 15% 15% 3 40% 30%
768x768 40% 15% 40% 60% 5 50% 40%
2048x2048 85% 95% 65% 95% 9.5 80% 80%
Semanticmetadata(semanticclasses)
Walkabilityscore Semanticdiversity Objectnessscore
10% 3 20%
50% 10 40%
90% 25 100%
Semanticmetadata(instances)
Crowdednessscore SAMclutterscore COCOclutterscore COCOinstancediversity
2 10 5 40%
3 100 10 50%
10 300 20 80%
Figure8: Steerablemultimodalgenerationusingmetadata. Thisisanextensionoffig.4and
showsourmodel’scapabilityofgeneratingmultimodaldatabyconditioningonawidesetofcontrols.
Thecommoncaptionforallexamplesis"apaintingofabridgeinalushforest".
18Varying SAM polygon instances
SAMpolygoninstanceinputs&RGBgenerations
Captioninput
Aframed
paintingof
mountainsinside
abedroom
Colorpalette
input
Colorpalette
input
Varying color palette
Colorpaletteinputs&RGBgenerations
Captioninput
Outsideviewof
anapartment
Normalsinput
Figure9: Probingwithgroundedgeneration. Thisisanextensionoffig.4andfurthershowsour
model’s capability on performing generation by conditioning on multimodal input. The top row
variesSAMinstancesandcombinesthemwithafixedcaptionandcolorpaletteinput. Thebottom
rowfixesthenormalsandcaptioninputsandvariesthecolorpalette.
19Captioninput:aphotoofateddybearmadeofwater Captioninput:vibrantportraitpaintingofSalvadorDalíwitharobotichalfface
4M-7(fromcaption) 4M-21(fromcaption) 4M-21(fromT5-XXLemb.) 4M-7(fromcaption) 4M-21(fromcaption) 4M-21(fromT5-XXLemb.)
Captioninput:adrawingofahouseonamountain Captioninput:thesilhouetteoftheMillleniumWheelatdusk
4M-7(fromcaption) 4M-21(fromcaption) 4M-21(fromT5-XXLemb.) 4M-7(fromcaption) 4M-21(fromcaption) 4M-21(fromT5-XXLemb.)
Captioninput:astopsignwithabluebackground Captioninput:acloudintheshapeofateacup
4M-7(fromcaption) 4M-21(fromcaption) 4M-21(fromT5-XXLemb.) 4M-7(fromcaption) 4M-21(fromcaption) 4M-21(fromT5-XXLemb.)
Captioninput:apaintingofblackandwhitewitharedborder Captioninput:agiantgorillaatthetopoftheEmpireStateBuilding
4M-7(fromcaption) 4M-21(fromcaption) 4M-21(fromT5-XXLemb.) 4M-7(fromcaption) 4M-21(fromcaption) 4M-21(fromT5-XXLemb.)
Figure10: Textunderstanding. Thisisanextensionoffig.4andfurtherdemonstratesimproved
textunderstandingcapabilitiesofourmethodcomparedto4Mforseveralcaptioninputs.
20B.2 Additionalretrievalvisualizations
Please see Figures 11 and 12 for additional qualitative results on RGB-to-Any and Any-to-RGB
retrievals.
Query Top-3Retrievals Top-3Retrievals Top-3Retrievals Top-3Retrievals
Figure11:RGB-to-Anyretrieval.Thisisanextensionoffig.5andfurtherdemonstratescross-modal
retrievalcapabilitiesofourmodel. Hereourmodelsuccessfullyretrievesseveralmodalities(RGB,
depth,normals,segmentation)usingtheRGBimageasthequeryinput.
21Depth-to-RGBretrieval Normals-to-RGBretrieval Segmentation-to-RGBretrieval Caption-to-RGBretrieval
Query Top-3Retrievals QQuueerryy TToopp--33RReettrriieevvaallss Query Top-3Retrievals Query Top-3Retrievals
<mpoeur asnsotaani>nfass mhaioslywwienlgl
p cilfpe moeh <la oac o pt ra rut ebfonr u rliv g esn la or sg gas nlbpat >ph yshre sint
bec hao oi tin nb rm f sed to hs eapu ona rdat awo -c s c5 sw int tnni ic oo ogolem wn nwftsi tn ire,
loodfao ugr tw eb saii idd nrned <ientphng eef rsr skiug ouint nl: a>
top admnia oir ma gynay ag lsnat drotd op be pian eiar ayw rryith
fw om re da te ehsik sn oel sydeeymi urnst mthe irrea e melsc yp wiap !l ena eedn k- .
pw taaana ll ldar esc ssa pe tcw obio le, aufnp nilcco dduel ia nl,tn guthdr iee:
n
afigreawinosarttkane ibgxlp hatlcoksisokny
aaroboemrd ooao mtrtbh1ee2drsoionst
becvawi fin om voc perei eldaa ro a’ii rrns tpp oetoo roo srr pttfsthe
Figure12:Any-to-RGBretrieval.Thisisanextensionoffig.5andfurtherdemonstratescross-modal
retrievalcapabilitiesofourmodel. HereourmodelsuccessfullyretrievesRGBimageswhenthe
queryinputsarefromdepth,normals,segmentation,andcaptionmodalities.
C AdditionalAblations
C.1 Ablationofpre-trainingdataandmodalities
Fortraining4M-21,weinitializethetrainingusing4Mmodelsthatwepre-trainedonCOYO700M[11].
WeablateinTable4differentchoicesoftrainingdataandmodalities. Wecanseethatperformingco-
trainingonC4[68]andCOYO700M[11]hasthepotentialtoslightlyimprovetransferperformance
onaverage.
Table 4: Pre-training data and modality mixture ablation: We ablate different pre-training
modalityanddatasetchoicesonBmodels. *representsthemodelsinitializedfromthecorresponding
4MmodelstrainedonCOYO700M.
Pre-training ImageNet-1K ADE20K NYUv2depth ARKitScenes
Method
data Top-1acc.↑ mIoU↑ δ acc.↑ AP3D↑
1
4M-7 B[62] CC12M 84.5 50.1 92.0 40.3
4M-7 B COYO700M 84.4 49.4 91.4 38.6
4M-7 B* CC12M 84.5 49.2 91.0 39.5
4M-21 B* CC12M 84.4 49.2 90.9 40.0
4M-21 B* CC12M+C4 84.6 49.5 90.4 41.2
4M-21 B* CC12M+COYO700M+C4 84.5 50.1 90.8 42.4
C.2 Ablationofensemblingthepredictions
Unlike the deterministic pseudo labeler and other state of the art networks we compared against
intable1,ourmodelcanproducemultiplepredictiongiventhesameRGBinputthroughrepeated
samplingwithadifferentseed. AsshowninTable5,ensemblingtensamplesofpredictedsurface
normals and semantic segmentation maps can significantly improve the reported metrics. While
ensemblingimprovesuponthesemetrics,wenotethattheensembledpredictionscanbecomparatively
blurrieraroundobjectedgesthananyindividualsample.
22Table5: Ensemblingablation: WeablateensemblingmultiplepredictionsonDIODEnormalsand
COCOsemanticsegmentationcomparedtonoensembling. Astheresultssuggest,ensemblinginall
casesimprovesthequantitativeresults.
DIODENormals COCOSemseg
meanangleerror↓ meanIoU↑
Method NoEnsemble Ensemble NoEnsemble Ensemble
4M-21 B 22.3 21.7 39.0 42.5
4M-21 L 21.7 21.1 43.8 46.4
4M-21 XL 21.3 20.8 46.5 48.1
D MultimodalDataset&TokenizationDetails
D.1 Pseudolabeledmultimodaltrainingdataset
Similar to 4M, to have an aligned multimodal dataset, we pseudo label the CC12M dataset using
strongspecializedmodelsforeachtask. Thepseudolabelingofexistingmodalitiesisdoneinthe
same fashion as 4M, using Omnidata DPT-Hybrid [70] for surface normals and depth estimation,
COCOMask2Former [19]withaSwinB [56]backboneforsemanticsegmentation,COCOViTDet
ViT-Hmodel [54]initializedfromMAEweights [38]forboundingboxes,andCLIP-B16 [67]with
ViT-B/16visualbackbonebackboneforCLIPfeaturemaps.
3Dhumanposes. Weuse4D-Humans[35]toextract3DposeandshapeparameterizedbyanSMPL
model. FortheimagesinCC12Mwithouthumans,wesettheposelabeltoa“none"token. Forthe
imageswithhumans,weformasequencebyconcatenatingtheboundingbox,bodypose,camera,
andshapevaluesinasequenceforeachhumaninstance. Asdataaugmentation,werandomlyshuffle
theorderofeachcomponentinthesequence.
SAM instances. Besides semantic segmentation and bounding boxes, SAM [47] instance seg-
mentationalsoprovidessomelevelofsemanticinformationfromanimagebyclusteringtogether
semanticallysimilarpixelsinit. Unlikesemanticsegmentation,SAMinstancesarenotrestricted
toaspecificsetofclassesandcansegmentinmoredetail. WeusetheSAMHmodelandqueryit
withpointsinagridformattoobtaintheinstances. WealsoconsideredtheSAM-HQ[45]Hmodel,
howeverinthegrid-pointqueryingformat,ityieldsverysimilarresultstoSAM.Wefound32×32
querypointstobetheoptimalchoicebothforpseudolabelingspeedandquality.
DINOv2andImageBindglobalfeatures&featuremaps. Weextractbothdensefeaturemapsand
globalembeddings,i.e. clstokenembeddings,fromDINOv2-B14[65]andImageBind-H14[33]
pre-trained models. For the latter, we only extracted the image embeddings, incorporating other
modalityembeddingssuchasthermaloraudiocouldbeinterestingfuturework.
T5-XXLembeddings. Languagemodelembeddings,suchasfromT5-XXL[68],havebeenshown
toimprovethegenerationfidelityandtextunderstandingcapabilitiesoftext-to-imagegenerative
models[76,13]. Consequently,weusetheT5-XXLencodertoextracttextembeddingsfromall
CC12Mcaptions,withoutanypreprocessingofthetext. Unlikeothermodalities,wedonotconvert
thesetextembeddingstoasequenceofdiscretetokensortreatthemastargets(similartotheRGB
pixelmodalityvariant). Instead,weonlyprovidethemasinputsusingalinearprojectionfromthe
T5-XXLembeddingdimension(d =4096)toourmodel’sembeddingdimension.
T5-XXL
Imagemetadata. FromRGBimages,wedirectlyextractdifferenttypesofmetadataliketheoriginal
heightandwidthbeforecropping[66],brightness,contrast,saturationandentropy. Weadditionally
extractanotionofcolorfulness,following[37].
Semanticmetadata. Wecomputethecrowdednessscoreasthenumberofhumansinthepseudola-
beledhumanposes,theSAMclutterscoreasthenumberofSAMinstances,theCOCOclutterscoreas
thenumberofCOCOinstances,theCOCOinstancediversityasthenumberofuniqueCOCOinstance
classes,andthesemanticdiversityasthenumberofuniqueCOCOsemanticclassesinanimage. For
captionlength,wecountthenumberofcharacters,words,andsentences. Asobjectnessscore,we
countthepercentageofpixelsintheCOCOsemanticsegmentationmapthatbelongtocountable
classes (indices 87, 90, 96, 97, 98, 99, 100, 101, 102, 103, 105, 106, 109, 110,
23111, 112, 113, 117, 118, 119, 122, 123, 124, 125, 126, 129, 131, 132), and for
the walkability score we count classes such as ‘road’ (indices 87, 90, 97, 100, 102, 105,
106, 122, 123, 125, 126, 132).
Geometricmetadata. Tocomputetheocclusionscore,wefirstgenerateocclusionedgesfromdepth
imagesbyapplyingaSobelfilter,followedbycountingthepercentageofocclusionedgepixelsthat
surpassathresholdof0.3. Asanotionofgeometriccomplexity,weprojectsurfacenormalpixels
ontotheunitsphere,andcomputetheirangularvariance. Notethatimagesofindoorscenesorcaves
featuringlargesurfacespointinginalldifferentdirectionsreceiveahighscoreinthismetric,while
oneswithamorelocalizedgeometricvariancegetasomewhatlowerscore. Exploringotherpotential
notionsofgeometriccomplexitycanbeaninterestingfutureaddition.
Color palette. For every RGB image, we extract between one and seven color palettes using
PyPalette[2]. Duringtraining,werandomlysampleoneofthecolorpalettestoenableuserstoinput
paletteswithdifferentlevelsofgranularity.
SAMedgesandcannyedges. Edgesareaconvenientwayofgroundingimagegenerationonshapes
containedinimages[98]. Topseudolabeledges,weapplytheOpenCVcannyedgedetectoronSAM
instancemapsandRGB,toobtainSAMedgesandcannyedgesrespectively.
D.2 Tokenizationofhumanposes
WeuseaBottleneckMLP[6]with6blocksand1024widthtocompressposeinto8tokens. Weuse
1024vocabularysize,andtrainedusingsmoothL1lossfor15epochsonCC12Mtrainingdata. We
alsobinnedtheglobalorientation,bodyshape,andboundingboxesinto1000discretebinssimilar
to[16]. Thefinalsequenceisobtainedbyalsoaddingidentifiers,i.e. “bbox”,“pose”,“shape”,before
thecorrespondingsub-sequence.
D.3 TokenizationofSAMinstances
TheSAMinstancetokenizerisaViT-basedVQ-VAEthattokenizes64×64binarymasksinto16
tokensusingavocabularysizeof1024. Thetokenizeristrainedusingthecross-entropylossfor24
epochsonCC12Mtrainingdata,byresizingindividualmasksintoasquareaspectratioimageof
64×64pixels. TopreservetheSAMinstances’originallocation,width,andheightintheimage,
theirboundingboxesareextracted. Thefinalsequenceforeachinstanceisformedbyappendingthe
identifier“polygon”to4numbersthatspecifytheboundingboxoftheinstance,alongwiththe16
tokenIDs.
D.4 Tokenizationofglobalfeaturemaps
Similartohumanposes,weuseBottleneckMLPwith6blocksand1024widthtocompressDINOv2-
B14 and ImageBind-H14 global embeddings into 16 tokens. We use 8192 vocabulary size, and
trainedusingcosinesimilaritylossfor15epochs.
D.5 Tokenizationofdensefeaturemaps
Wefollow[62]andtokenizeCLIP-B16,DINOv2-B14,andImageBind-H14densefeaturemapsinto
196,256,and256tokens,respectively,usingaViT-basedVQVAEwith8192vocabularysizeand
smoothL1loss.
D.6 Tokenizationofsequencemodalities
We tokenize text, color palette, metadata, and bounding boxes using a WordPiece tok-
enizer by fitting it on all captions and 4000 “special value” tokens, with a joint vocab-
ulary size of 30k. These special tokens are divided into four groups, each with 1000
values, i.e. v0=0, v0=1, ..., v0=999, v1=0, v1=1, ... v1=999, v2=0, v2=1, ...,
v2=999, v3=0, v3=1, ..., v3=999. For bounding boxes, we follow 4M [62] and represent
xmin, ymin, xmax, ymaxcoordinatesusingv0, v1, v2, v3tokensrespectively. Othermodal-
itiesaretokenizedbybinningtheirvaluesintocorrespondingbins,e.g. colorpalettesequenceis
formedascolor = cR = r G = g B = bR = r,...wherectakesavaluebetween1and7and
24specifiesthenumberofcolorsinthepaletteandr,g,btakesvaluesbetween0-255. Wechoseto
modelmetadatausinginterleavedpairsofspecialtokens,wherethefirstonespecifiesthetypeof
metadatamodality,andthesecondspecifiesitsvalue. Forexample,acrowdednessscoreof3anda
brightnessof120wouldbespecifiedasthesequencev1=5 v0=3 v1=10 v0=120. Duringtraining
the number of metadata entries and their order is randomized. All of this results in a sequence
predictionformulation,following[62,16].
D.7 TokenizationofCannyandSAMedges
WeuseaVQ-VAEwithadiffusiondecoder,similarto[62]totokenizetheedgemodalities. Weuse
thesametokenizerasitreconstructsbothedgessimilarlywell.
E TrainingDetails
PleaseseeTab.6foranoverviewofpre-trainingsettings. Formoreaccuratemodelcomparisons,the
architectureandoveralltrainingobjectiveofourB,L,andXLmodelsarethesameasthoseof4M
models. However,wedomodifyandimprovevariousaspectsofthetrainingprocessthatallowusto
significantlyincreasethenumberoftrainingmodalities. Thesechangesconcernmodality-specific
accommodationstothemaskingstrategy,theabilitytoco-trainonseveraldatasets,andtheuseofa
morediversifiedmultimodalmaskingstrategy. Wedescribethesemodificationsbelow:
E.1 Modality-specificaccommodations
Positionalandmodalityembeddings. Aswith4M,4M-21incorporatesbothlearnablemodalityem-
beddingsandfixedsine-cosinepositionalembeddingsforeachmodality. Thepositionalembeddings
areeither1Dor2Ddependingonthemodalitytype.
Metadatagroupingandchunk-basedmasking. Toaddressthesparsityandnumberofdifferent
types of metadata, the metadata modalities are all grouped together as a single modality during
training. Thispreventstheover-allocationoftokenstosparsemetadata,enablingamorebalanced
distributionofthetokenbudgetacrossmodalities. However,thestandardspanmaskingfromT5[68]
and4M[62]performsrandomuniformmaskingatthetokenlevel,whichcanleadtopre-training
inefficiencies[51]andmakeconditioningonspecificmetadatadifficult,asconditioningonjustone
ofthemwouldrarelyoccurduringpre-trainingwiththismaskingstrategy. Instead,weproposeto
maskchunksofsequence(similartoPMI-Masking[51]),wherethespanmaskingisperformedper
chunkofmetadatainsteadofatthetokenlevel.
E.2 Multidatasetco-traininganddiversifiedmultimodalmaskingstrategy
Multi-datasetsupport. Unlike4Mwhichwasonlytrainedonasinglealigneddataset, wetrain
4M-21onmultipledatasetssimultaneously. Thisflexibilityallowsfortheinclusionofdatasetswith
varyingnumbersofmodalities,whichenablestrainingonbothlarge-scaledatasetswithasmaller
numberofmodalitiesandsmallerdatasetswithalargerdiversityofmodalities.
Samplingandmaskingstrategies. Ourdatasamplingprocessinvolvesselectingatrainingdataset
basedonitssamplingweight,followedbychoosingamaskingstrategyfromthedataset-specific
mixtureofmaskingstrategies. Inputandtargettokensarethensampledusingtheselectedstrategy.
Co-trainingdatasets. Weco-trainonseveraldatasetstoimprovethemodel’sperformanceandthe
datadiversity.TheseincludeCC12M[15],whichcomprisesabout10milliontext-imagesamplesfully
pseudolabeledwithall21modalities,andaccountsfor60%ofourtrainingsamples. Additionally,
weincludeCOYO700M[11],withapproximately500milliontext-imagesamplespseudolabeled
withthe7modalitiesof4M,andaccountsfor20%ofourtrainingsamples. Lastly, theColossal
CleanCrawledCorpus(C4)[68],alargetext-onlydataset,isusedforlanguagemodelco-training,
alsomakingup20%ofourtrainingsamples.
Diverse mixture of masking strategies. As with 4M [62], the masking strategy is governed by
Dirichletdistributionwithparameterα. Thisdistributioninfluencesthesamplingoftokensfrom
modalities: alowerαresultsinsamplesdominatedbyonemodality,whileahigherαleadsamore
balancedrepresentationacrossallmodalities. ForbothCC12MandCOYOdatasets,weimplement
25multiplemaskingstrategiestocatertospecifictrainingneeds,andrandomlysamplefromthemfor
everysampleinthebatch:
• All-to-allmasking: Involvesfourmaskingstrategieswithsymmetricinputandtargetαset
to0.01,0.1,1.0,and10.0respectively.
• RGB-to-allmasking: ConsistsofonlyRGBtokensasinput,withtargetαallsetto0.5.
• Caption-biasedmasking:Includestwostrategies,heavilyskewedtowardseitherunmasked
captionsorT5-XXLembeddingsasinput. Thesemaskingstrategiesareparticularlybenefi-
cialfortasksinvolvingtext-to-imagegeneration
Table6: Pre-trainingsettings. Trainingconfigurationfor4M-21usedinthetransferexperiments
andgenerationresults.
Configuration 4M-21 B 4M-21 L 4M-21 XL
Weightinitialization 4M(COYO)
Traininglength(ntokens) 500B
Warmuplength(ntokens) 10B
Optimizer AdamW[57]
Opt. momentum β ,β =0.9,0.95
1 2
Baselearningrate[36] 1e-4 1e-4 2e-5
Batchsize 8192
Weightdecay 0.05
Gradientclipping ✗ ✗ 3.0
Learningrateschedule Cosinedecay
Feedforwardactivation SwiGLU[77]
Inputtokenbudget 256
Targettokenbudget 256
Inputandtargetα Mixture(seeSec.E.2)
Maskingstrategy Mixture(seeSec.E.2)
Dataset Mixture(seeSec.E.2)
Imageresolution 2242
Augmentation None(CenterCrop)
Repeatedsampling[30] 4
Datatype bfloat16[10]
F Out-of-the-boxEvaluationDetails
Below,weprovidefurtherdetailsonout-of-the-boxevaluationsweperformed. Pleasealsoseefig.13
foraqualitativecomparisonbetweenourXLmodelandUnified-IOXL[59],aswellasUnified-IO
2XXL[58]. Furthermore,table7comparesUnified-IO,Unified-IO2,andourmodel’sout-of-the-
box capabilities on surface normal estimation, depth estimation, and semantic segmentation. As
demonstrated,ourmodeloutperformsUnified-IOandUnified-IO2inallthementionedtasks.
F.1 SurfacenormalanddepthestimationonDIODE
Wefollowtheevaluationsetupin[62]andevaluateonDIODEvalidationsetat224×224input
resolution.
F.2 SemanticandinstancesegmentationonCOCO
WeemployasimilarapproachasSAM[47]byqueryingourmodelontheboundingboxestoobtain
theinstances. Topredicttheinstances,onlythetargetboundingboxisprovidedintheinputfinal
sequence,andthetokensaremaskedforourmodeltopredictthem.
26DIODEdepth
RGBInput
GroundTruth
4M-21XL
Unified-IOXL
Unified-IO2XXL
DIODEsurfacenormals
RGBInput
GroundTruth
4M-21XL
Unified-IOXL
Unified-IO2XXL
COCOsemanticsegmentationmaps
RGBInput
GroundTruth
4M-21XL
Unified-IOXL
Unified-IO2XXL
Figure13:Comparing4M-21 XL,Unified-IOXL[59],andUnified-IO2XXL[58]out-of-the-box.
4M-21 XLdemonstratesstronggeneralizationtoinputsfromdifferentdatasetsandtasksout-of-the-
box(zeroshot),significantlyimprovingoverUnified-IO1and2.
27Table7: Out-of-the-boxcapabilties. ComparisonbetweenUnified-IO2andourmodelout-of-the-
taskcapabilitiesacrosssurfacenormalestimation,depthestimation,andsemanticsegmentation. We
usetheL1scoreasthemetricforsurfacenormalanddepthestimation,andmeanIoUforsemantic
segmentation.
Method Normals↓ Depth↓ Sem.seg.↑
Unified-IOB[59] 35.7 1.00 32.9
Unified-IOL 33.9 0.87 41.6
Unified-IOXL 31.0 0.82 44.3
Unified-IO2L[58] 37.1 0.96 38.9
Unified-IO2XL 34.8 0.86 39.7
Unified-IO2XXL 37.4 0.84 41.7
4M-21 B 21.7 0.71 42.5
4M-21 L 21.1 0.69 46.4
4M-21 XL 20.8 0.68 48.1
F.3 kNNretrievalonImageNet-1K
WefollowtheevaluationsetupfromDINOv2[65]andsetk =20andtemperatureto0.07.
F.4 3Dhumanposepredictionon3DPW
Wefollowtheevaluationimplementedinthe4D-Humans[35]codebase,withthedifferencethatwe
use224×224asinputimageresolutionasopposedto256×256.
G TransferEvaluationDetails
WeprovidethetransfersettingsinTables8,9,10.Wealsonotethatafteranextensivehyperparameter
searchfortheDINOv2-gbaselineonNYUv2,usingaConvNeXthead,itachievedonly92.5δ acc.,
1
whichislowerthanthereported95.0withfrozenencoderandDPThead.
Table8: Imageclassificationsettings. Configurationforintermediatefine-tuningonImageNet-21K
andfine-tuningonImageNet-1K,thesettingsfollowMultiMAE[7]and4M[62].
Configuration ImageNet-21K ImageNet-1K
Base Large XL Base Large XL
Fine-tuningepochs 20 50 20 20
Warmupepochs 2 2
Optimizer AdamW[57] AdamW[57]
Opt.momentum β ,β =0.9,0.95 β ,β =0.9,0.999
1 2 1 2
Baselearningrate[36] 1e-4 1e-4 5e-5 1e-4
Batchsize 4096 4096 4096 1024
Weightdecay 0.05 0.05
Learningrateschedule Cosinedecay Cosinedecay
Layer-wiselrdecay[20] 0.75 0.85 0.85 0.75 0.85 0.85
Droppath[41] 0.1 0.2 0.4 0.1 0.2 0.4
Inputresolution 2242 2242
Augmentation RandAug(9,0.5)[21] RandAug(9,0.5)[21]
Randomresizedcrop (0.5,1) (0.08,1)
Labelsmoothingε 0.1 0.1
Mixup[97] 0.1 0.1
Cutmix[95] 1.0 1.0
28Table9: Semanticsegmentationsettings. Configurationforsemanticsegmentationfine-tuningon
ADE20K,thesettingsfollowMultiMAE[7]and4M[62].
Configuration ADE20K
Base Large XL
Fine-tuningepochs 64 64 128
Warmupepochs 1
Optimizer AdamW[57]
Opt. momentum β ,β =0.9,0.999
1 2
Learningrate 2e-4 2e-4 3e-4
Batchsize 64
Weightdecay 0.05
Learningrateschedule Cosinedecay
Layer-wiselrdecay[20] 0.75 0.85 0.95
Droppath[41] 0.1 0.2 0.3
LoRA[40]rank/scale ✗ ✗ 64/1.0
Inputresolution 5122
Augmentation Large-scalejitter(LSJ)[32]
Colorjitter ✓
Table10: Depthestimationsettings. Configurationfordepthestimationfine-tuningonNYUv2,the
settingsfollowMultiMAE[7]and4M[62].
NYUv2
Configuration Base Large XL
Fine-tuningepochs 1000
Warmupepochs 100
Optimizer AdamW[57]
Opt. momentum β ,β =0.9,0.999
1 2
Learningrate 1e-4 1e-4 5e-5
Batchsize 128 128 16
Weightdecay 1e-4
Learningrateschedule Cosinedecay
Layer-wiselrdecay[20] 0.75 0.85 0.9
Droppath[41] 0.1 0.2 0.0
LoRA[40]rank/scale ✗ ✗ 8/1.0
Inputresolution 2562
Randomcrop ✓
Colorjitter ✓
H InvestigatingDifferentTokenizationSchemes
Aswedevelopseveraltokenizationstrategiesforeachmodality,ablatingtheirperformanceagainstall
possibledesignchoiceswouldbeprohibitivelyexpensive. Thus,wefocusononemodality,namely
SAMinstances,andprovideamoredetailedlookintotheimpactofdifferenttokenizationstrategies.
WestudytwoapproachesforSAMinstances: pathtokenizationandmasktokenization.
Pathtokenization:Werepresenteachinstanceintheimageasalistofpolygoncoordinates.Thenwe
tokenizethesecoordinatesusingaBottleneckMLP-basedVQ-VAEtokenizer. Toachieveafixed-size
input,thepolygonsareeithersimplifiedorextendedtohavethesamenumberofcornerpoints. We
foundthatfixingthemaximumnumberofcornersto128resultsinaminimalchangeintheoverall
polygonshape,thusweusethisvalueforallthepathtokenizationablations.
Masktokenization: Inthisscheme,wefirstconverteachinstancetoabinarymasksandresizethem
toafixedmasksize. Then,wetokenizethemusingaViT-basedVQ-VAEtokenizer,similartothe
waywetokenizeimage-likeandfeaturemapmodalities.
29Path Tokenization Mask Tokenization
Number of Tokens Vocabulary Size Loss Mask Size
100.0 100.0 100.0 100.0
97.5 97.5 97.5 97.5
95.0 95.0 95.0 95.0
92.5 92.5 92.5 92.5
90.0 90.0 90.0 90.0
87.5 87.5 87.5 87.5
85.0 85.0 85.0 85.0
9 16* 25 512 1024* 2048 L1 MSE CE* Dice 32 × 32 64 × 64* 128 × 128
Figure14:Ablatingtokenizationchoices:Weablatetheimpactofdifferenttokenizationchoices.Performance
isreportedasreconstructionIoUonCC12Mvalidationset. *showsthemasktokenizationconfigurationwe
usedinthefinaltokenizer.SeeappendixHfordetails.
Ablations: We investigated L1 and MSE losses for both tokenization schemes, and additionally
cross-entropyandDicelossforthemasktokenization. Wealsoinvestigatedtheeffectsofthetotal
numberoftokens,tokenvocabularysize,andmasksize. Tocomparetheperformanceoftheresulting
tokenizers,weusetheIoUbetweenthepseudo-labeledandreconstructedinstancesasourmetric.
fig.14illustratestheresultsofdifferentablatedconfigurations. Foreachconfiguration,theremaining
unspecifiedparametersarebydefaultsetto16forthenumberoftokens,1024forthevocabulary
size,L1fortheloss,and64×64forthemasksize. Theablationsshowthatusingmasktokenization
with16tokens,1024vocabularysize,and64×64masksizeperformswellandsetsagoodbalance
betweenreconstructionqualityandtotalsequencelength.
Inallablations,thetokenizersaretrainedfor24epochsstartingwith5warmupepochsusingthe
AdamW[57]optimizerwithβ ,β = 0.9,0.999andabatchsizeof128. Foralltheexperiments
1 2
excepttheDiceloss,alearningrateof1e-5isused. SinceusingthislearningratefortheDiceloss
experimentresultedininstabilities,wereduceditslearningrateto1e-6. Asdemonstratedinfig.15,
increasingthenumberoftokensresultsinbetterreconstructionqualitybothforthemasktokenizer
andthepathtokenizer. ComparedtoL1loss,thecross-entropylosstrainingobtainsreconstructions
withsmootheredgesandbettercoverage.
Masktokenizer Pathtokenizer
L1loss Crossentropy L1loss
Instance
groundtruth 3x3tokens 4x4tokens 5x5tokens 4x4tokens 9tokens 16tokens 25tokens
Figure15: DifferenttokenizationschemesforSAMinstances. Wecomparedifferenttokenization
schemestotokenizeSAMinstancesforpre-training. PleaseseeSec.Hfordetails.
30
UoII BroaderImpact
I.1 Computationalcosts
AllmodelsweretrainedonNvidiaA100GPUs. The4M-21 Bmodelwastrainedfor2daysusing
64 A100s. The 4M-21 L model was trained for 4 days using 128 A100s. The largest 4M-21 XL
modelrequired11daysusing128A100s. Fine-tuningandtransferlearningexperimentsforeach
modelusedapproximately20%additionalcomputecomparedtoitspre-training. Trainingthevarious
tokenizers(RGB,depth,normals,CLIP,DINOv2,ImageBind,semanticsegmentation,SAMedges,
andCannyedgedetection,SAMinstances,and3Dhumanposes)requiredroughly5daysusing8
A100seach,totalingapproximately60A100-days. Intotal,theprimaryexperimentsreportedinthe
paperusedapproximately120’000A100-hours,notincludingadditionalpreliminaryexperiments
andablations. Weestimatethetotalcomputeforthefullresearchproject,includingpreliminaryand
unreportedexperiments,tobe150’000A100-hours.
I.2 Socialimpact
Weareopensourcingourcodeandmodelstosupportresearcherswiththedemocratizationofthe
toolsandtoenabletransparentinspectionandsafeguarding. 4M-21modelsaretrainedonpublicly
availabledatasetswithsomecuration,e.g. people’snamesareredactedinCC12M[15]. However,
thisprocessisstillnoisy,henceweadvisecautionwhenusingthemodelsforgeneration.
31