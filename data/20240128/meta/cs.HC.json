[
    {
        "title": "The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support",
        "authors": "Inhwa SongSachin R. PendseNeha KumarMunmun De Choudhury",
        "links": "http://arxiv.org/abs/2401.14362v1",
        "entry_id": "http://arxiv.org/abs/2401.14362v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14362v1",
        "summary": "People experiencing severe distress increasingly use Large Language Model\n(LLM) chatbots as mental health support tools. Discussions on social media have\ndescribed how engagements were lifesaving for some, but evidence suggests that\ngeneral-purpose LLM chatbots also have notable risks that could endanger the\nwelfare of users if not designed responsibly. In this study, we investigate the\nlived experiences of people who have used LLM chatbots for mental health\nsupport. We build on interviews with 21 individuals from globally diverse\nbackgrounds to analyze how users create unique support roles for their\nchatbots, fill in gaps in everyday care, and navigate associated cultural\nlimitations when seeking support from chatbots. We ground our analysis in\npsychotherapy literature around effective support, and introduce the concept of\ntherapeutic alignment, or aligning AI with therapeutic values for mental health\ncontexts. Our study offers recommendations for how designers can approach the\nethical and effective use of LLM chatbots and other AI mental health support\ntools in mental health care.",
        "updated": "2024-01-25 18:08:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14362v1"
    },
    {
        "title": "GPTVoiceTasker: LLM-Powered Virtual Assistant for Smartphone",
        "authors": "Minh Duc VuHan WangZhuang LiJieshan ChenShengdong ZhaoZhenchang XingChunyang Chen",
        "links": "http://arxiv.org/abs/2401.14268v1",
        "entry_id": "http://arxiv.org/abs/2401.14268v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14268v1",
        "summary": "Virtual assistants have the potential to play an important role in helping\nusers achieves different tasks. However, these systems face challenges in their\nreal-world usability, characterized by inefficiency and struggles in grasping\nuser intentions. Leveraging recent advances in Large Language Models (LLMs), we\nintroduce GptVoiceTasker, a virtual assistant poised to enhance user\nexperiences and task efficiency on mobile devices. GptVoiceTasker excels at\nintelligently deciphering user commands and executing relevant device\ninteractions to streamline task completion. The system continually learns from\nhistorical user commands to automate subsequent usages, further enhancing\nexecution efficiency. Our experiments affirm GptVoiceTasker's exceptional\ncommand interpretation abilities and the precision of its task automation\nmodule. In our user study, GptVoiceTasker boosted task efficiency in real-world\nscenarios by 34.85%, accompanied by positive participant feedback. We made\nGptVoiceTasker open-source, inviting further research into LLMs utilization for\ndiverse tasks through prompt engineering and leveraging user usage data to\nimprove efficiency.",
        "updated": "2024-01-25 16:02:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14268v1"
    },
    {
        "title": "Evaluating User Experience and Data Quality in a Gamified Data Collection for Appearance-Based Gaze Estimation",
        "authors": "Mingtao YueTomomi SayudaMiles PenningtonYusuke Sugano",
        "links": "http://arxiv.org/abs/2401.14095v1",
        "entry_id": "http://arxiv.org/abs/2401.14095v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14095v1",
        "summary": "Appearance-based gaze estimation, which uses only a regular camera to\nestimate human gaze, is important in various application fields. While the\ntechnique faces data bias issues, data collection protocol is often demanding,\nand collecting data from a wide range of participants is difficult. It is an\nimportant challenge to design opportunities that allow a diverse range of\npeople to participate while ensuring the quality of the training data. To\ntackle this challenge, we introduce a novel gamified approach for collecting\ntraining data. In this game, two players communicate words via eye gaze through\na transparent letter board. Images captured during gameplay serve as valuable\ntraining data for gaze estimation models. The game is designed as a physical\ninstallation that involves communication between players, and it is expected to\nattract the interest of diverse participants. We assess the game's significance\non data quality and user experience through a comparative user study.",
        "updated": "2024-01-25 11:16:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14095v1"
    },
    {
        "title": "The Adaptive Architectural Layout: How the Control of a Semi-Autonomous Mobile Robotic Partition was Shared to Mediate the Environmental Demands and Resources of an Open-Plan Office",
        "authors": "Binh Vinh Duc NguyenAndrew Vande Moere",
        "links": "http://dx.doi.org/10.1145/3613904.3642465",
        "entry_id": "http://arxiv.org/abs/2401.14078v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14078v1",
        "summary": "A typical open-plan office layout is unable to optimally host multiple\ncollocated work activities, personal needs, and situational events, as its\nspace exerts a range of environmental demands on workers in terms of\nmaintaining their acoustic, visual or privacy comfort. As we hypothesise that\nthese demands could be coped by optimising the environmental resources of the\narchitectural layout, we deployed a mobile robotic partition that autonomously\nmanoeuvres between predetermined locations. During a five-weeks in-the-wild\nstudy within a real-world open-plan office, we studied how 13 workers adopted\nfour distinct adaptation strategies when sharing the spatiotemporal control of\nthe robotic partition. Based on their logged and self-reported reasoning, we\npresent six initiation regulating factors that determine the appropriateness of\neach adaptation strategy. This study thus contributes to how future\nhuman-building interaction could autonomously improve the experience, comfort,\nperformance, and even the health and wellbeing of multiple workers that share\nthe same workplace.",
        "updated": "2024-01-25 10:55:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14078v1"
    },
    {
        "title": "Leveraging Large Models for Crafting Narrative Visualization: A Survey",
        "authors": "Yi HeShixiong CaoYang ShiQing ChenKe XuNan Cao",
        "links": "http://arxiv.org/abs/2401.14010v1",
        "entry_id": "http://arxiv.org/abs/2401.14010v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14010v1",
        "summary": "Narrative visualization effectively transforms data into engaging stories,\nmaking complex information accessible to a broad audience. Large models,\nessential for narrative visualization, inherently facilitate this process\nthrough their superior ability to handle natural language queries and answers,\ngenerate cohesive narratives, and enhance visual communication. Inspired by\nprevious work in narrative visualization and recent advances in large models,\nwe synthesized potential tasks and opportunities for large models at various\nstages of narrative visualization. In our study, we surveyed 79 papers to\nexplore the role of large models in automating narrative visualization\ncreation. We propose a comprehensive pipeline that leverages large models for\ncrafting narrative visualization, categorizing the reviewed literature into\nfour essential phases: Data, Narration, Visualization, and Presentation.\nAdditionally, we identify ten specific tasks where large models are applied\nacross these stages. This study maps out the landscape of challenges and\nopportunities in the LM4NV process, providing insightful directions for future\nresearch and valuable guidance for scholars in the field.",
        "updated": "2024-01-25 08:19:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14010v1"
    }
]