[
    {
        "title": "Class-attribute Priors: Adapting Optimization to Heterogeneity and Fairness Objective",
        "authors": "Xuechen ZhangMingchen LiJiasi ChenChristos ThrampoulidisSamet Oymak",
        "links": "http://arxiv.org/abs/2401.14343v1",
        "entry_id": "http://arxiv.org/abs/2401.14343v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14343v1",
        "summary": "Modern classification problems exhibit heterogeneities across individual\nclasses: Each class may have unique attributes, such as sample size, label\nquality, or predictability (easy vs difficult), and variable importance at\ntest-time. Without care, these heterogeneities impede the learning process,\nmost notably, when optimizing fairness objectives. Confirming this, under a\ngaussian mixture setting, we show that the optimal SVM classifier for balanced\naccuracy needs to be adaptive to the class attributes. This motivates us to\npropose CAP: An effective and general method that generates a class-specific\nlearning strategy (e.g. hyperparameter) based on the attributes of that class.\nThis way, optimization process better adapts to heterogeneities. CAP leads to\nsubstantial improvements over the naive approach of assigning separate\nhyperparameters to each class. We instantiate CAP for loss function design and\npost-hoc logit adjustment, with emphasis on label-imbalanced problems. We show\nthat CAP is competitive with prior art and its flexibility unlocks clear\nbenefits for fairness objectives beyond balanced accuracy. Finally, we evaluate\nCAP on problems with label noise as well as weighted test objectives to\nshowcase how CAP can jointly adapt to different heterogeneities.",
        "updated": "2024-01-25 17:43:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14343v1"
    },
    {
        "title": "Estimation of partially known Gaussian graphical models with score-based structural priors",
        "authors": "Martín SevillaAntonio García MarquesSantiago Segarra",
        "links": "http://arxiv.org/abs/2401.14340v1",
        "entry_id": "http://arxiv.org/abs/2401.14340v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14340v1",
        "summary": "We propose a novel algorithm for the support estimation of partially known\nGaussian graphical models that incorporates prior information about the\nunderlying graph. In contrast to classical approaches that provide a point\nestimate based on a maximum likelihood or a maximum a posteriori criterion\nusing (simple) priors on the precision matrix, we consider a prior on the graph\nand rely on annealed Langevin diffusion to generate samples from the posterior\ndistribution. Since the Langevin sampler requires access to the score function\nof the underlying graph prior, we use graph neural networks to effectively\nestimate the score from a graph dataset (either available beforehand or\ngenerated from a known distribution). Numerical experiments demonstrate the\nbenefits of our approach.",
        "updated": "2024-01-25 17:39:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14340v1"
    },
    {
        "title": "Information Leakage Detection through Approximate Bayes-optimal Prediction",
        "authors": "Pritha GuptaMarcel WeverEyke Hüllermeier",
        "links": "http://arxiv.org/abs/2401.14283v1",
        "entry_id": "http://arxiv.org/abs/2401.14283v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14283v1",
        "summary": "In today's data-driven world, the proliferation of publicly available\ninformation intensifies the challenge of information leakage (IL), raising\nsecurity concerns. IL involves unintentionally exposing secret (sensitive)\ninformation to unauthorized parties via systems' observable information.\nConventional statistical approaches, which estimate mutual information (MI)\nbetween observable and secret information for detecting IL, face challenges\nsuch as the curse of dimensionality, convergence, computational complexity, and\nMI misestimation. Furthermore, emerging supervised machine learning (ML)\nmethods, though effective, are limited to binary system-sensitive information\nand lack a comprehensive theoretical framework. To address these limitations,\nwe establish a theoretical framework using statistical learning theory and\ninformation theory to accurately quantify and detect IL. We demonstrate that MI\ncan be accurately estimated by approximating the log-loss and accuracy of the\nBayes predictor. As the Bayes predictor is typically unknown in practice, we\npropose to approximate it with the help of automated machine learning (AutoML).\nFirst, we compare our MI estimation approaches against current baselines, using\nsynthetic data sets generated using the multivariate normal (MVN) distribution\nwith known MI. Second, we introduce a cut-off technique using one-sided\nstatistical tests to detect IL, employing the Holm-Bonferroni correction to\nincrease confidence in detection decisions. Our study evaluates IL detection\nperformance on real-world data sets, highlighting the effectiveness of the\nBayes predictor's log-loss estimation, and finds our proposed method to\neffectively estimate MI on synthetic data sets and thus detect ILs accurately.",
        "updated": "2024-01-25 16:15:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14283v1"
    },
    {
        "title": "At the junction between deep learning and statistics of extremes: formalizing the landslide hazard definition",
        "authors": "Ashok DahalRaphaël HuserLuigi Lombardo",
        "links": "http://arxiv.org/abs/2401.14210v1",
        "entry_id": "http://arxiv.org/abs/2401.14210v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14210v1",
        "summary": "The most adopted definition of landslide hazard combines spatial information\nabout landslide location (susceptibility), threat (intensity), and frequency\n(return period). Only the first two elements are usually considered and\nestimated when working over vast areas. Even then, separate models constitute\nthe standard, with frequency being rarely investigated. Frequency and intensity\nare intertwined and depend on each other because larger events occur less\nfrequently and vice versa. However, due to the lack of multi-temporal\ninventories and joint statistical models, modelling such properties via a\nunified hazard model has always been challenging and has yet to be attempted.\nHere, we develop a unified model to estimate landslide hazard at the slope unit\nlevel to address such gaps. We employed deep learning, combined with a model\nmotivated by extreme-value theory to analyse an inventory of 30 years of\nobserved rainfall-triggered landslides in Nepal and assess landslide hazard for\nmultiple return periods. We also use our model to further explore landslide\nhazard for the same return periods under different climate change scenarios up\nto the end of the century. Our results show that the proposed model performs\nexcellently and can be used to model landslide hazard in a unified manner.\nGeomorphologically, we find that under both climate change scenarios (SSP245\nand SSP885), landslide hazard is likely to increase up to two times on average\nin the lower Himalayan regions while remaining the same in the middle Himalayan\nregion whilst decreasing slightly in the upper Himalayan region areas.",
        "updated": "2024-01-25 14:48:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14210v1"
    },
    {
        "title": "Adapting tree-based multiple imputation methods for multi-level data? A simulation study",
        "authors": "Ketevan GurtskaiaJakob SchwerterPhilipp Doebler",
        "links": "http://arxiv.org/abs/2401.14161v1",
        "entry_id": "http://arxiv.org/abs/2401.14161v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14161v1",
        "summary": "This simulation study evaluates the effectiveness of multiple imputation (MI)\ntechniques for multilevel data. It compares the performance of traditional\nMultiple Imputation by Chained Equations (MICE) with tree-based methods such as\nChained Random Forests with Predictive Mean Matching and Extreme Gradient\nBoosting. Adapted versions that include dummy variables for cluster membership\nare also included for the tree-based methods. Methods are evaluated for\ncoefficient estimation bias, statistical power, and type I error rates on\nsimulated hierarchical data with different cluster sizes (25 and 50) and levels\nof missingness (10\\% and 50\\%). Coefficients are estimated using random\nintercept and random slope models. The results show that while MICE is\npreferred for accurate rejection rates, Extreme Gradient Boosting is\nadvantageous for reducing bias. Furthermore, the study finds that bias levels\nare similar across different cluster sizes, but rejection rates tend to be less\nfavorable with fewer clusters (lower power, higher type I error). In addition,\nthe inclusion of cluster dummies in tree-based methods improves estimation for\nLevel 1 variables, but is less effective for Level 2 variables. When data\nbecome too complex and MICE is too slow, extreme gradient boosting is a good\nalternative for hierarchical data.\n  Keywords: Multiple imputation; multi-level data; MICE; missRanger; mixgb",
        "updated": "2024-01-25 13:12:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14161v1"
    }
]