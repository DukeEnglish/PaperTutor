[
    {
        "title": "Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities",
        "authors": "Yiyuan ZhangXiaohan DingKaixiong GongYixiao GeYing ShanXiangyu Yue",
        "links": "http://arxiv.org/abs/2401.14405v1",
        "entry_id": "http://arxiv.org/abs/2401.14405v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14405v1",
        "summary": "We propose to improve transformers of a specific modality with irrelevant\ndata from other modalities, e.g., improve an ImageNet model with audio or point\ncloud datasets. We would like to highlight that the data samples of the target\nmodality are irrelevant to the other modalities, which distinguishes our method\nfrom other works utilizing paired (e.g., CLIP) or interleaved data of different\nmodalities. We propose a methodology named Multimodal Pathway - given a target\nmodality and a transformer designed for it, we use an auxiliary transformer\ntrained with data of another modality and construct pathways to connect\ncomponents of the two models so that data of the target modality can be\nprocessed by both models. In this way, we utilize the universal\nsequence-to-sequence modeling abilities of transformers obtained from two\nmodalities. As a concrete implementation, we use a modality-specific tokenizer\nand task-specific head as usual but utilize the transformer blocks of the\nauxiliary model via a proposed method named Cross-Modal Re-parameterization,\nwhich exploits the auxiliary weights without any inference costs. On the image,\npoint cloud, video, and audio recognition tasks, we observe significant and\nconsistent performance improvements with irrelevant data from other modalities.\nThe code and models are available at https://github.com/AILab-CVC/M2PT.",
        "updated": "2024-01-25 18:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14405v1"
    },
    {
        "title": "Deconstructing Denoising Diffusion Models for Self-Supervised Learning",
        "authors": "Xinlei ChenZhuang LiuSaining XieKaiming He",
        "links": "http://arxiv.org/abs/2401.14404v1",
        "entry_id": "http://arxiv.org/abs/2401.14404v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14404v1",
        "summary": "In this study, we examine the representation learning abilities of Denoising\nDiffusion Models (DDM) that were originally purposed for image generation. Our\nphilosophy is to deconstruct a DDM, gradually transforming it into a classical\nDenoising Autoencoder (DAE). This deconstructive procedure allows us to explore\nhow various components of modern DDMs influence self-supervised representation\nlearning. We observe that only a very few modern components are critical for\nlearning good representations, while many others are nonessential. Our study\nultimately arrives at an approach that is highly simplified and to a large\nextent resembles a classical DAE. We hope our study will rekindle interest in a\nfamily of classical methods within the realm of modern self-supervised\nlearning.",
        "updated": "2024-01-25 18:59:57 UTC",
        "interpretation": {
            "论文还有什么可以进一步探索的点？": "论文《Deconstructing Denoising Diffusion Models for Self-Supervised Learning》已经对Denoising Diffusion Models（DDMs）进行了深入的分析和简化，最终达到了一个高度简化的模型，在很大程度上类似于经典的Denoising Autoencoder（DAE）。作者们通过逐步去除DDM中的非必要组件，发现只有少数现代组件对于学习良好的表示至关重要，而许多其他组件并非必需。\n\n尽管论文已经取得了一定的成果，但仍然有一些方向可以进一步探索：\n\n1. **理论分析的深入**：尽管论文已经对DDM进行了深入的分析，但仍然可以进一步探索其内在的数学原理和优化方法。例如，可以研究扩散过程的更精细的数学特性，或者探索如何更好地将DDM与其他的优化技术相结合。\n\n2. **模型的泛化能力**：虽然论文中的简化模型在特定任务上表现良好，但可以进一步研究如何提高模型的泛化能力，以便在更多样化的数据集和任务上表现出色。\n\n3. **与其他方法的比较**：论文中提到的方法在性能上已经接近经典的DAE，但可以进一步与其他自监督学习方法进行比较，以确定其优势和局限性。\n\n4. **应用领域的拓展**：虽然论文主要关注的是图像处理领域，但可以探索将这些方法应用于其他领域，如自然语言处理或音频处理，以验证其通用性和跨领域能力。\n\n5. **超参数优化**：尽管论文中提到的方法已经取得了不错的效果，但超参数的选择可能还有进一步优化的空间。通过自动化的超参数搜索或更深入的分析，可以找到更优的超参数设置。\n\n6. **长期稳定性**：可以研究简化后的模型在长期使用中的稳定性，以及如何通过模型的更新或维护来保持其性能。\n\n7. **与其他技术的融合**：可以将DDMs与其他的机器学习技术相结合，例如强化学习或元学习，以探索是否能够进一步提高模型的性能和适应性。\n\n8. **可解释性和透明度**：可以探索如何提高模型的可解释性和透明度，以便更好地理解模型的工作机制，并减少潜在的偏差和错误。\n\n9. **隐私保护**：在处理敏感数据时，可以研究如何通过DDMs的设计来保护数据隐私，例如通过设计隐私保护的扩散过程。\n\n10. **对抗样本鲁棒性**：可以研究如何提高模型的对抗样本鲁棒性，即模型在面对故意设计的扰动时保持稳定的性能。\n\n这些方向可以为未来的研究提供新的思路和挑战，有助于推动自监督学习领域的发展。"
        },
        "id": "2401.14404v1"
    },
    {
        "title": "Adaptive Mobile Manipulation for Articulated Objects In the Open World",
        "authors": "Haoyu XiongRussell MendoncaKenneth ShawDeepak Pathak",
        "links": "http://arxiv.org/abs/2401.14403v1",
        "entry_id": "http://arxiv.org/abs/2401.14403v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14403v1",
        "summary": "Deploying robots in open-ended unstructured environments such as homes has\nbeen a long-standing research problem. However, robots are often studied only\nin closed-off lab settings, and prior mobile manipulation work is restricted to\npick-move-place, which is arguably just the tip of the iceberg in this area. In\nthis paper, we introduce Open-World Mobile Manipulation System, a full-stack\napproach to tackle realistic articulated object operation, e.g. real-world\ndoors, cabinets, drawers, and refrigerators in open-ended unstructured\nenvironments. The robot utilizes an adaptive learning framework to initially\nlearns from a small set of data through behavior cloning, followed by learning\nfrom online practice on novel objects that fall outside the training\ndistribution. We also develop a low-cost mobile manipulation hardware platform\ncapable of safe and autonomous online adaptation in unstructured environments\nwith a cost of around 20,000 USD. In our experiments we utilize 20 articulate\nobjects across 4 buildings in the CMU campus. With less than an hour of online\nlearning for each object, the system is able to increase success rate from 50%\nof BC pre-training to 95% using online adaptation. Video results at\nhttps://open-world-mobilemanip.github.io/",
        "updated": "2024-01-25 18:59:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14403v1"
    },
    {
        "title": "Range-Agnostic Multi-View Depth Estimation With Keyframe Selection",
        "authors": "Andrea ContiMatteo PoggiValerio CambareriStefano Mattoccia",
        "links": "http://arxiv.org/abs/2401.14401v1",
        "entry_id": "http://arxiv.org/abs/2401.14401v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14401v1",
        "summary": "Methods for 3D reconstruction from posed frames require prior knowledge about\nthe scene metric range, usually to recover matching cues along the epipolar\nlines and narrow the search range. However, such prior might not be directly\navailable or estimated inaccurately in real scenarios -- e.g., outdoor 3D\nreconstruction from video sequences -- therefore heavily hampering performance.\nIn this paper, we focus on multi-view depth estimation without requiring prior\nknowledge about the metric range of the scene by proposing RAMDepth, an\nefficient and purely 2D framework that reverses the depth estimation and\nmatching steps order. Moreover, we demonstrate the capability of our framework\nto provide rich insights about the quality of the views used for prediction.\nAdditional material can be found on our project page\nhttps://andreaconti.github.io/projects/range_agnostic_multi_view_depth.",
        "updated": "2024-01-25 18:59:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14401v1"
    },
    {
        "title": "pix2gestalt: Amodal Segmentation by Synthesizing Wholes",
        "authors": "Ege OzgurogluRuoshi LiuDídac SurísDian ChenAchal DavePavel TokmakovCarl Vondrick",
        "links": "http://arxiv.org/abs/2401.14398v1",
        "entry_id": "http://arxiv.org/abs/2401.14398v1",
        "pdf_url": "http://arxiv.org/pdf/2401.14398v1",
        "summary": "We introduce pix2gestalt, a framework for zero-shot amodal segmentation,\nwhich learns to estimate the shape and appearance of whole objects that are\nonly partially visible behind occlusions. By capitalizing on large-scale\ndiffusion models and transferring their representations to this task, we learn\na conditional diffusion model for reconstructing whole objects in challenging\nzero-shot cases, including examples that break natural and physical priors,\nsuch as art. As training data, we use a synthetically curated dataset\ncontaining occluded objects paired with their whole counterparts. Experiments\nshow that our approach outperforms supervised baselines on established\nbenchmarks. Our model can furthermore be used to significantly improve the\nperformance of existing object recognition and 3D reconstruction methods in the\npresence of occlusions.",
        "updated": "2024-01-25 18:57:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2401.14398v1"
    }
]