A Comparative Analysis of Noise Reduction Methods in Sentiment Analysis
on Noisy Bengali Texts
KaziToufiqueElahi,TasnuvaBinteRahman,ShakilShahriar,SamirSarker,
Md. TanvirRoufShawon,G.M.Shahariar
DepartmentofComputerScienceandEngineering
AhsanullahUniversityofScienceandTechnology,Dhaka,Bangladesh
{ktoufiquee,tasnuvabinterahmansrishti,shakilshahriararnob,rohitsarker5,
shawontanvir95,sshibli745}@gmail.com
Abstract Sentiment Data Noise
Neutral [B] Aenk idn OeyT Mixed
WhileBengaliisconsideredalanguagewith
Language
kraIeqn saeym vaI
limitedresources,sentimentanalysishasbeen
[E]Youkeptmewaiting LocalWord
asubjectofextensiveresearchintheliterature.
forseveraldaysbrother
Nevertheless,thereisascarcityofexploration
Sayem.
intosentimentanalysisspecificallyintherealm
ofnoisyBengalitexts. Inthispaper,weintro- Positive [B] Aaim maeC maeC JaI- Punctuation
duce a dataset (NC-SentNoB) that we anno- xub mangt xabar Error
tated manually to identify ten different types [E] I occasionally visit,
ofnoisefoundinapre-existingsentimentanal- and the food is of high
ysis dataset comprising of around 15K noisy quality.
Bengali texts. At first, given an input noisy Negative [B]vaI dyaker xabar nº Spacing
text,weidentifythenoisetype,addressingthis kreb(cid:223)a Error
asamulti-labelclassificationtask. Then, we [E] Please don’t waste Spelling
introducebaselinenoisereductionmethodsto foodbrother. Error
alleviate noise prior to conducting sentiment
analysis. Finally, we assess the performance Table1: FewexamplesfromourNC-SentNoBdataset
offine-tunedsentimentanalysismodelswith withsentimentontheleftmostcolumnandnoisetypes
both noisy and noise-reduced texts to make ontherightmostcolumn. Brepresentstheoriginaltext
comparisons. Theexperimentalfindingsindi- inBengaliandErepresentsthecorrespondingEnglish
catethatthenoisereductionmethodsutilized translation.
arenotsatisfactory,highlightingtheneedfor
moresuitablenoisereductionmethodsinfuture
researchendeavors. Wehavemadetheimple-
etal.,2019;Heetal.,2020;Raffeletal.,2020;Xue
mentationanddatasetpresentedinthispaper
publiclyavailable1. etal.,2020),therehasbeenanotableenhancement
in the sentiment analysis task. However, when
1 Introduction
confrontedwithincreasedtextualnoise,theperfor-
Sentimentanalysisistheprocessofanalyzingand mance of PLMs drops drastically (around 50%),
categorizingtheemotionsoropinionsexpressedin primarily due to the inability of the tokenizer to
textual content. This process holds considerable handlemisspelledwords(Srivastavaetal.,2020).
importance in evaluating public sentiments, ana- This issue is less pronounced in English, where
lyzingsocialmediaposts,andassessingcustomer most typing tools and applications offer robust
feedback. Itcontributessignificantlytogainingin- auto-correction systems. However, Bengali, de-
sightsintoongoingsocialmediadynamics. There spitebeingtheseventhmostspokenlanguagewith
have been nearly 7000 papers published on this aminimumof272.7millionspeakers(Wikipedia,
topic and 99% of the papers have appeared after 2023), faces significant challenges due to the ab-
2004,makingsentimentanalysisoneofthefastest- senceofaneffectiveauto-correctionsystemindigi-
growingresearchareas(Mäntyläetal.,2018). taldevicesandsoftware. Asaresult,aconsiderable
With the recent emergence of pre-trained lan- amount of text shared on social media platforms
guage models (PLMs) (Devlin et al., 2018a; Liu oftenexhibitsdiverseformsofnoise,includingin-
formallanguage,regionalwords,spellingerrors,ty-
1https://github.com/ktoufiquee/A-Comparative
pographicerrors,punctuationerrors,coinedwords,
-Analysis-of-Noise-Reduction-Methods-in-Sentime
nt-Analysis-on-Noisy-Bengali-Texts embeddedmetadata,amixtureoftwoormorelan-
AcceptedinThe9thWorkshoponNoisyandUser-generatedText(W-NUT),18thConferenceoftheEuropean
ChapteroftheAssociationforComputationalLinguistics(EACL2024)
4202
naJ
52
]LC.sc[
1v06341.1042:viXraguages (code-mixed text), grammatical mistakes 2 RelatedWorks
andsoforth(Srivastavaetal.,2020). Forexample,
Haqueetal.(2023)integrated42,036samplesfrom
the sentence "na , muI Oega ikqu kI naI , duhxu paIeb"
twopubliclyavailableBengalidatasets,achieving
(English: No, I did not tell them anything, they
thehighestaccuracy(85.8%)inmulti-classsenti-
willgetsad)incorporatesregionalwordslike"muI"
mentanalysiswiththeirproposedC-LSTM.Islam
("Aaim",I),"Oega"("Oedr",them),"kI"("bil",tell),
etal.(2020)introducedtwomanuallytaggedBen-
"paIeb"("paeb",get),alongsideaspellingerror"duhx"u
gali datasets, achieving 71% accuracy for binary
("duhx",sad).
classification and 60% for multi-class classifica-
Recent investigations into Bengali sentiment
tionusingBERTwithGRU.BhowmickandJana
analysishaveprimarilyfocusedonBengalitexts,
(2021)outperformedthebaselinemodelproposed
RomanizedBengalitexts(Hassanetal.,2016),and
by Islam et al. (2020), attaining a 95% accuracy
socialmediacomments(Chakrabortyetal.,2022).
onbinaryclassificationbyfine-tuningm-BERTand
However, there is a notable scarcity of research
XLM-RoBERTa. Samia et al. (2022) utilized BERT,
specifically addressing noisy Bengali texts, and
BiLSTM,andLSTMforaspect-basedsentimentanal-
theavailabledatasetsforsuchstudiesarelimited.
ysis,whereBERTperformedbestbyachieving95%
To address this gap, the SentNoB dataset (Islam
in aspect detection and 77% sentiment classifica-
etal.,2021)hasbeenrecentlyintroduced,aiming
tion. Hasan et al. (2023) fine-tuned transformer
totacklechallengesassociatedwithsentimentanal-
modelswhereBanglaBERTsurpassedothermodels
ysisinnoisyBengalitexts. Nevertheless,itisworth
with86%accuracyandamacroF1-scoreof0.82
notingthatthisdatasetlacksannotationsfornoise
inmulti-classsetting.
typespresentinthenoisytextsanddoesnotincor-
porateanynoisereductionmethods. Thepresence Bengali sentiment analysis has also been ex-
ofnoisesignificantlyimpactstheperformanceof tended to address the challenges of noisy social
modelscomparedtotheirperformanceonnoiseless media texts. One of the notable contributions is
text,whichindicatesapotentialareaforfurtherre- SentNoB, a dataset of over 15,000 social media
search. Toaddresstheseissues,wehavemadethe comments developed by Islam et al. (2021). It
followingcontributions: was benchmarked by SVM with lexical features,
neural networks, and pre-trained language mod-
• We present a dataset named NC-SentNoB
els. The best micro-averaged F1-Score (0.646)
(Noise Classification on SentNoB dataset),
was achieved by SVM with word and character n-
designedfortheidentificationoftendistinct
grams. Hoq et al. (2021) added Twitter data to
types of noise found in approximately 15K
SentNoB and got 87.31% accuracy with multi-
noisyBengalitexts. Fewsampleinstancesare
layer perceptrons. Islam et al. (2023) developed
providedinTable1.
SentiGOLD, which is a balanced Bengali senti-
• Weemploymachinelearning,deeplearning mentdatasetconsistingof70,000entrieswithfive
andfine-tunepre-trainedtransformermodels classes which utilized SentNoB for cross-dataset
toidentifynoisetypesinnoisyBengalitexts evaluation. It was benchmarked by BiLSTM, HAN,
(a multi-label classification task) and to per- BiLSTM, CNN with attention and BanglaBERT.
form sentiment analysis on both noisy and ThebestmacroF1-Score(0.62)wasachievedby
noise-reduced texts (a multi-class classifica- fine-tuning BanglaBERT, which also got an F1-
tiontask). Score(0.61)onSentNoBduringcross-datasettest-
ing.
• We conduct experiments with various tech-
As for the correction of noisy texts, Koyama
niques to reduce noise from Bengali texts
etal.(2021)performedacomparativeanalysisof
including spell correction, back translation,
grammaticalerrorcorrectionusingback-translation
paraphrasingandmasking. Toassesstheiref-
models. Itwasobservedthatthetransformer-based
fectiveness,wecomparetheperformanceof
modelachievedthehighestscoreontheCoNLL-
thesemethodsagainstasetof1000random,
2014dataset(Ngetal.,2014). SunandJiang(2019)
noisytextsthathavebeenmanuallycorrected
employedaBERT-basedmaskedlanguagemodel-
byannotators.
ing for contextual noise reduction. This method
• Wehavemadeourdatasetandcodesopenly involvessequentiallymaskingandcorrectingeach
accessibleforfurtherresearchinthisfield. word in a sentence, starting from the left. Theyfoundthatthisnoisereductionmethodsignificantly able in the dataset. To do this, the authors thor-
enhancesperformanceinapplicationssuchasneu- oughly investigated the SentNoB dataset, deter-
ralmachinetranslation,naturallanguageinterfaces, mined ten categories, and defined rules for each
andparaphrasedetectioninnoisytexts. noisetypeastheannotationguidelines. Thedetails
ofeachnoisecategoryarepresentedinAppendix
3 NoiseIdentification
C. We first invited seven native Bengali speakers
toassistuswiththeannotatingprocess. Next,we
In this section, we first manually annotate all the
asked each participant to label 50 samples, from
instancesfromSentNoBdataset,categorizingthem
which we determined their trustworthiness score
intotenseparatenoisecategories. Asingleinstance
(Price et al., 2020). We used 10 samples out of
mayfallintomultiplenoisecategories. Then,we
the50ascontrolsamplesanddiscoveredthatonly
outline the process of noise identification, where
fourparticipantsachievedthe90%trustworthiness
the objective is to determine the type of noise
score threshold. The degree of agreement across
present in a given noisy Bengali text. This task
annotatorsiscalculatedusingFleiss’kappascore
isframedasamulti-labelclassificationtask.
(Fleiss,1971)tomaintainthequalityoftheannota-
3.1 ExistingDataset tion. Aftercomputingthescoresforfourindepen-
dentannotators,wefoundareliablescoreof0.69,
TheSentNoBdataset(Islametal.,2021)hasatotal
indicatingasubstantialdegreeofagreement.
of 15,728 noisy Bengali texts. While the dataset
offersacollectionofnoisyBengalitexts,itlacks
information regarding the specific types of noise
present in these texts. The dataset is partitioned
intothreesubsets: train(80%),test(10%),andval-
idation(10%). Eachtextiscategorizedintooneof
threelabels: positive,neutral,andnegative. These
labelsrepresentthesentimentortoneexpressedin
eachtext.
3.2 DatasetDevelopment
To the best of our knowledge, there is currently
nodatasetspecificallydesignedforthepurposeof
identifyingnoiseinBengalitexts. Toaddressthis
gap, we expanded the SentNoB dataset to create
Figure1: Length-FrequencydistributionofTexts.
anoiseidentificationdatasetnamedNC-SentNoB
(NoiseClassificationonSentNoBdataset),encom-
3.4 DatasetStatistics
passingatotalof15,176noisytexts. Intheprocess,
weeliminated552duplicatevaluespresentinthe It is evident from Table 2 that the dataset is im-
originaldatasettoenhancedataintegrity. Wemain- balanced, with the number of texts in the neutral
tainedthetrain-validation-testsplittingratioofthe categorysignificantlylowerthanthoseinboththe
originaldatasetandthedistributionofdataineach positiveandnegativecategories. Inadditiontothe
partitionisdetailedinTable2.
Class Instances #Word/Instance
LocalWord 2,084(0.136%) 16.05
Neutral Positive Negative
WordMisuse 661(0.043%) 18.55
Train 2,767 4,948 4,318
Context/WordMissing 550(0.036%) 13.19
Test 361 650 570 WrongSerial 69(0.005%) 15.30
Validation 354 621 587 MixedLanguage 6,267(0.410%) 17.91
Total 3,482 6,219 5,475 PunctuationError 5,988(0.391%) 17.25
SpacingError 2,456(0.161%) 18.78
Table2: Datadistributionineachpartition. SpellingError 5,817(0.380%) 17.30
CoinedWord 549(0.036%) 15.45
Others 1,263(0.083%) 16.52
3.3 Annotation
Table3: StatisticsofNC-SentNoBpernoiseclass.
The primary idea behind developing the NC-
SentNoBdatasetwastocategorizethenoisesavail- class imbalance, the dataset also exhibits a widevariation in the length of the texts. On an aver- believethatthispre-processingstepwillplayavi-
age,thetextshavealengthof66characters. The tal role in addressing challenges associated with
longest text is 314 characters, while the shortest noisyBengalitextsbyaidinginthedevelopment
text is only 11 characters long. Figure 1 shows ofnoisespecificreductionmethods.
thelengthfrequencydistributionofthetextsover
3.6 ExperimentalSetup
thewholedataset. Table3showsthestatisticsof
different types of noise we found. This provides SVMwasimplementedwitharegularizationparam-
aninsightintothemostcommonnoiseofBengali eterof1. AsforBiLSTMandBangla-BERT-Base,
texts found on the dataset. The table shows that BinaryCross-EntropyLosswasused. Bothmodels
MixedLanguageisthemostcommonnoisetype, weretrainedusingtheAdamWoptimizer, witha
Spelling Error is the second most common, and learningrateof1e−6forBiLSTMand1e−5for
Wrong Serial is the least common. Figure 2 indi- Bangla-BERT-Base. The batch sizes were set at
cateslowcorrelationcoefficients,suggestingamin- 256forBiLSTMand128forBangla-BERT-Base.
imal linear association between noise categories.
3.7 Results&Analysis
Notably,MixedLanguageandSpellingErrorhave
theleastcorrelationat-0.12,implyingaslightin- Table 4 presents the performance comparison of
verse relationship between these two types. This the implemented models on noise identification.
indicates if a sentence in the dataset contains an Bangla-BERT-BaseachievesthehighestmicroF1-
errorofMixedLanguage,ithasahigherpossibility score at 0.62, while SVM with character-level fea-
ofnothavinganySpellingErrorandviceversa. turessecuresthesecond-bestscoreof0.57. How-
ever,BiLSTMhasthelowestmicroF1-scoreof0.24.
ThecomparisonbetweenSVMwithcharacter-level
Model Precision Recall F1-Score
SVM(C) 0.76 0.45 0.57
SVM(W) 0.64 0.38 0.48
SVM(C+W) 0.75 0.45 0.56
Bi-LSTM 0.36 0.18 0.24
Bangla-BERT-Base 0.73 0.54 0.62
Table4: Performancecomparisonofdifferentmodels
onnoiseidentification. Crepresentscharacterleveln-
gramandWrepresentswordleveln-gram.
features and SVM with word-level features shows
that the former attains a higher score. This sug-
geststhatcharacter-levelinformationismorecru-
cialfornoiseidentification. Implementingasimilar
Figure 2: Heatmap of correlation coefficients among character-levelapproachinneuralnetworkmodels
differentnoisetypesinNC-SentNoB. andfine-tuningotherpre-trainedlanguagemodels
mayimprovethenoiseidentificationperformance
whichweleaveopenforfuturework. Table5illus-
3.5 Baselines trates the performance of Bangla-BERT-Base on
each type of noise. It can be seen that the model
Fornoiseidentification,weimplementedSupport
failstoclassifyinstancesoftheWrongSerialtype.
Vector Machine (SVM) (Cortes and Vapnik, 1995)
This is primarily due to the low amount of data
(utilizingbothcharacterandwordn-gramfeatures),
availableforthisspecificclassinthedataset.
BidirectionalLongShortTermMemory(BiLSTM)
(Hochreiter and Schmidhuber, 1997) network,
4 SentimentAnalysis
andfine-tunedthepre-trainedBangla-BERT-Base
(Sarker,2020)model. Thedescriptionsofthemod- In this section, we outline the methodology em-
els can be found in Appendix A. The rationale ployed for conducting sentiment analysis on the
behindtheclassificationistodevelopanautomatic NC-SentNoBdataset. Weemployacost-sensitive
text pre-processing step that identifies different learning objective to fine-tune seven pre-trained
typesofnoisepresentinBengalitexts. Wefirmly transformermodelsforthesentimentanalysistask.Class Precision Recall F1-Score optimizerwasusedwithalearningrateof1e−5,
LocalWord 0.46 0.49 0.47
betassetat(0.9,0.9999),anepsilonvalueof1e−9,
WordMisuse 0.65 0.16 0.25
andaweightdecayof0.08. Duetoresourcecon-
Context/WordMissing 0.33 0.06 0.10
WrongSerial 0.00 0.00 0.00 straints,batchsizewassetto48forsahajBERT,32
MixedLanguage 0.75 0.85 0.80 forBanglaBERT Large,and128fortherestofthe
PunctuationError 0.83 0.54 0.65
models.
SpacingError 0.86 0.21 0.33
SpellingError 0.64 0.55 0.59
Model Precision Recall F1-Score
CoinedWord 0.82 0.89 0.86
Bangla-BERT-Base 0.72 0.72 0.72
Others 0.76 0.76 0.76
BanglaBERT 0.75 0.75 0.75
BanglaBERTLarge 0.74 0.74 0.74
Table 5: Class-wise performance of Bangla-BERT-
BanglaBERTGenerator 0.72 0.72 0.72
Baseonnoiseidentificationtask. sahajBERT 0.72 0.72 0.72
Bangla-Electra 0.68 0.68 0.68
MuRIL 0.73 0.73 0.73
Weconducttwodistinctexperiments: thefirstin-
Table6: Performanceofsentimentanalysismodelsfine-
volvesfine-tuningtransformersonthenoisytexts,
tunedonnoisytexts.
while the second entails fine-tuning transformers
afterreducingnoisefromtheoriginalnoisytexts.
4.4 Experimentwithnoise
4.1 Baselines
Table6illustratestheperformancecomparisonof
We utilized seven publicly available pre-trained
the seven fine-tuned models. BanglaBERT yields
transformermodels: Bangla-Bert-Base(Sarker,
the highest scores across all evaluation metrics
2020), BanglaBERT (Bhattacharjee et al., 2022a),
with a micro F1-score of 0.75. This result out-
BanglaBERT Large(Bhattacharjeeetal.,2022a),
SahajBERT2, Bangla-Electra3, MuRIL (Khanuja performs the highest micro F1-Score of 0.6461
withSVMpreviouslyreportedbyIslametal.(2021).
et al., 2021). The descriptions of the models can
It is also noteworthy that all other models except
befoundinAppendixA.
Bangla-Electra have demonstrated results that
4.2 Cost-sensitiveLearning are somewhat comparable with ranges between
0.72and0.75intermsofmicroF1-score.
Cost-sensitivelearning(Elkan,2001)isaprocess
of training where we can make the model priori-
4.5 Experimentbyreducingnoise
tize samples from the minority class above those
In this experiment, we first outline the noise re-
fromthemajorityclassbysuggestingamanually
ductionstrategiesutilizedpriortosentimentanaly-
establishedweightforeveryclasslabelinthecost
sis. Wethenrandomlyselect1000noisytextsand
functionthatisbeingminimized. Weadoptedthis
manuallycorrectthem. Weusethese1000manu-
method in the sentiment analysis task. In order
allycorrectedtextsasgroundtruthformeasuring
to provide a more equitable and balanced model
the performance of the noise reduction methods
performance,wetriedimposinglargercoststothe
in terms of semantic similarity. To assess perfor-
classesthatareintheminorityinnumbersdueto
mance,weemployvariousestablishedevaluation
theimbalancescenariointheNC-SentNoBdataset,
metrics.
asseeninTable2. Thiswasaccomplishedbypro-
vidingclassweightstotheCross-Entropylossfunc-
Class Instances
tionusedtotrainthemodels. LocalWord 132(13.2%)
WordMisuse 32(03.2%)
4.3 ExperimentalSetup Context/WordMissing 39(03.9%)
WrongSerial 4(00.4%)
Cost-sensitive learning was incorporated by us-
MixedLanguage 416(41.6%)
ing class weights as a cost matrix into the Cross- PunctuationError 323(32.3%)
Entropylossfunction. Theclassweightsweresetat SpacingError 133(13.3%)
SpellingError 376(37.6%)
1.4496forneutral,0.8106forpositive,and0.9289
CoinedWord 33(03.3%)
fornegativeclasses. Forfine-tuning,theAdamW
Others 92(09.2%)
2https://huggingface.co/neuropark/sahajBERT
Table7: Statisticsofnoisetypesonmanuallycorrected
3https://huggingface.co/monsoon-nlp/bangla-e
1000data.
lectra4.5.1 ProcessofNoiseReduction spelledwords. Toaddressthisissue,weusedthe
spellingcorrectoralgorithmpriorprovidinginput
Completeeliminationofnoisefromthenoisytexts
tothemodel.
is impossible. However, our aim is to minimize
(d) Mask Prediction. To improve the qual-
noisetothegreatestextentpossible. Thissection
ity of noisy texts and address out-of-vocabulary
details four distinct methods for reducing noise
words, we replaced OOV words with <MASK>
in noisy texts: back-translation, spelling correc-
and used the predictions generated by a Masked
tion, paraphrasing and replacing out of vocabu-
Language Model (MLM). We also implemented
lary (OOV) words with predictions generated by
randommaskingforreplacementwitheachword
amaskedlanguagemodel(MLM).Additionalde-
having a 20% possibility of getting replaced by
tailsabouttheemployedmethodscanbefoundin
the MLM model. For both cases, we used
AppendixA.
BanglaBERT Generator (Kowsher et al., 2022)
(a)Back-translation. Back-translationservesasa
model.
methodtocorrectvariouserrorswithinasentence.
Aspre-trainedmodelshavebeentrainedonexten-
4.5.2 EvaluationofNoiseReduction
sivecorporaofnoiselesssentences,theycangener-
Wefirstuseseveralwell-knownmetricstoquantify
ateanoiselesstranslatedsentencewhenpresented
theperformanceofthenoisereductiontechniques.
with a noisy sentence as input. Also, translating
Theevaluationisperformedbasedon1000manu-
thatsentencebackintotheoriginallanguagemay
allycorrectedtexts. Thefirstfourauthorsindivid-
resultinacorrectedversion. Forthisstudy,allin-
uallycorrected250textseach,whilethelasttwo
puttextswereinitiallytranslatedintoEnglishand
authorsverifiedcorrectionsfor500textseach. We
thenintoBengaliusingback-translation. Twomod-
thencompareandanalyzetheperformanceofthe
elswerechosenforthispurpose: GoogleTranslate,
noisereductionmethods.
awebserviceemployinganRNN-basedmodeland
Evaluation Metrics. To evaluate the noise re-
BanglaT5modelspre-trainedontheBanglaNMT
duction methods, we employed a range of met-
English-BanglaandBanglaNMTBangla-English
rics including BLEU, ROUGE-L, BERTScore,
dataset(Bhattacharjeeetal.,2022b).
SBERT Score, BSTS, BERT-iBLEU, and Word
(b) Spelling Correction. For the noisy texts we
Coverage (utilizing Word2Vec, FastText, and
are working with, correcting spelling errors can
Bangla-BERT-Base). Additionally,weconducted
be a beneficial process as spelling errors can af-
humanevaluationsofthenoisereducedsentences
fectthetokenizationprocess. Toaddressthisissue,
bynativeBengalispeakers. Thedetaileddescrip-
weimplementedaspellcorrectionalgorithmbased
tions of the evaluation metrics along with the hu-
onSoundexandLevenshteindistance. Thisalgo-
man evaluation procedure are presented in Ap-
rithm replaces misspelled words with the closest
pendixB.
matchingwordsfoundintheBengalidictionary4.
NoiseReductionPerformance. Fromthedatapre-
However, as it is not context-based, there are in-
sented in Table 8, it can be seen that the original
stances where it fails to correct all spellings and
noisytextsscoredhighestonBLEUandROUGE-
may even introduce out-of-context words in the
L, which is unsurprising since the ground truth
sentence.
sentencescontainnearlyidenticalwords. Thisob-
(c)Paraphrasing. Paraphrasinginvolveschanging
servationisfurthersupportedbythespell-corrected
thewordsofasentencewithoutalteringitsmean-
sentences,whichalsoachieveasimilarscoredue
ing. Similar to translation models, paraphrasing
to having nearly identical words. Similarly, for
models have the potential to provide a noiseless
BERTScore,SBERTScore,andBSTS,thescores
paraphrasedoutputwhengivenanoisyinput. For
arehigherfornoisytexts. Thisisprimarilybecause
thisstudy,weusedtheBanglaT5modelpre-trained
of the nature of textual embeddings and the tok-
ontheBanglaParaphrasedataset(Akiletal.,2022).
enizationmethodused. Asmentionedearlier,BERT
We observed the performance of the BanglaT5
usesWordPiecetokenization,whichcanresultin
paraphrasemodelonsomerandomlyselectednoisy
identicalwordshavingthesametoken. Therefore,
texts from our dataset and found that the model
whencomparingnoisytextswiththeircorrespond-
performspoorlywhentheinputdatacontainsmis-
inggroundtruthsentences,manytokensarelikely
to match perfectly, leading to higher cosine sim-
4https://github.com/MinhasKamal/BengaliDictio
nary ilarity scores. However, although not having theBERT SBERT BERT WordCoverage Human
BLEU ROUGE-L BSTS
Score Score -iBLEU Bangla Evaluation
Word2Vec FastText
BERT (%)
NoisyText 65.77 79.71 93.21 88.32 93.67 51.65 75.54 82.92 71.26 X
Google
21.55 39.46 84.72 81.04 84.28 80.93 87.52 89.01 84.86 37.90
Translate
BanglaT5
16.57 32.09 81.30 75.27 82.15 80.12 89.01 87.52 85.66 21.10
Translate
Spell
61.17 77.35 92.29 87.86 92.94 56.50 82.72 88.51 80.76 35.80
Correction(SC)
SC+
20.35 36.44 83.32 74.15 85.60 80.63 86.79 86.79 83.89 20.80
Paraphrase
MLM
60.99 76.44 90.72 86.90 91.82 56.60 88.51 82.27 87.18 26.80
(OOV)
MLM
44.17 70.00 90.76 85.26 93.45 68.93 86.41 88.35 93.20 10.40
(Random)
Table8: Performancecomparisonofdifferentnoisereductionmethods
havingahigherpossibilityofbeingnoiselesswords
fromtheirrespectivevocabularies. Allofthescores
[N]Aapin eta Hat zya vuel egeln vaI
are based on the textual similarity of the ground
[C]Aapin eta Hat ezaya vuel egeln vaI
[E]Brotheryouforgottowashyourhands. truths and noise reduced sentences. Thus, we re-
lied on human evaluation to select the best noise
reduction method where 4 native Bengali speak-
[S]Aapin eta Hat dya vuel egeln vaI ersevaluatedthesentencesanddiscoveredthatthe
[SP]tuim etamar Hat-payer dya vuel egq, vaI. back-translationmethodutilizingGoogleTranslate
[TG]vaI Aapin Hat zuet vuel egeqn
APIwasthemostreliableintermsofmaintaining
[TM]tuim Hat zret vuel egeqa
contextualmeaning. Theinputandoutputofeach
[MO]Aapin eta Hat Haraet vuel egeln vaI
noisereductionmethodforasinglenoisytextare
[MR]Aapin eta Hat zret vuel egeln vaI
shownintable9. Exceptforback-translationusing
Table9: Inputandoutputofasinglenoisytextbythe Google Translate, all methods fail to rectify the
noisereductionmethods. Ndenotestheoriginalnoisy spelling problem in the input. Most approaches
text,Cindicatesthecorrectedtext,andErepresentsEn- change the meaning of the sentence by changing
glishtranslationofthecorrectedtext. S,SP,TG,TM,
thenoisyword.
MO,andMRrepresentoutputsofspellingcorrection,
paraphrasingwithspellingcorrection,back-translation
4.5.3 Results&Analysis
usingGoogleTranslate,back-translationwithT5mod-
els,maskedlanguagemodelingforout-of-vocabulary We prioritized the human evaluation score based
words,andrandommaskedlanguagemodelingrespec- ontheresultsofTable8andusedback-translated
tively. Foreachsentence,noisywordsaremarkedwith
data obtained from Google Translate to execute
Red color, and noise reduced words are marked with
the sentiment analysis task by fine-tuning seven
Greencolor.
pre-trained transformer models. We applied the
samenoisereductionmethodonboththetestand
highestscore,back-translation,paraphrasing,and validationsets. Wecomparedthesentimentanaly-
maskpredictionmethodsscoreabove80%inboth sisperformanceofthemodelsfine-tunedonnoisy
BERTScoreandBSTS,implyingthattheyarese- and noiseless data presented in Tables 6 and 10.
manticallysimilarandthemeaningofthesentences From Table 10, it can be seen that models fine-
havenotchangeddrastically. BERT-iBLEUscore tunedonback-translateddataonlyattainthehigh-
accountsforthepresenceoftextuallysimilarwords est F1-Score of 0.73. This outcome remains con-
byapplyingpenalizationwhileemphasizingseman- sistentacrossallmodelsevaluatedduringourex-
ticmeaning,leadingtoGoogleTranslateachieving perimentation. Themodelfine-tunedonnoisydata
thehighestscoreinthismetric. Moreover,theword outperformedthesamemodelfine-tunedonback-
coverage results show different methods scoring translated data. The reason for this disparity of
thehighestinsteadofnoisytexts. Thisisduetothe performanceisthat,whileback-translationcanmit-
generated words or sentences from these models igatesomesourcesofnoise,itcanalsointroduce
noitcudererofeB
noitcuderretfAModel Precision Recall F1-Score back-translation by Google Translate. Although
Bangla-BERT-Base 0.69 0.69 0.69
other noise reduction methods performed poorly
BanglaBERT 0.72 0.72 0.72
in human evaluation, it would be interesting to
BanglaBERTLarge 0.73 0.73 0.73
BanglaBERTGenerator 0.70 0.70 0.70 study whether their performance in noise reduc-
sahajBERT 0.70 0.70 0.70 tioncorrelateswiththeperformanceinsentiment
Bangla-Electra 0.66 0.66 0.66
analysis. Furthermore, the NC-SentNoB dataset
MuRIL 0.71 0.71 0.71
containsonlyaverysmallnumberofWrongSerial
Table 10: Performance of sentiment analysis models datainstances. OthercategoriessuchasContext/-
fine-tunedonnoisereducedtexts(back-translationwith WordMissing,WordMisuse,andCoinedWord are
googletranslate).
alsounderrepresented. Infuture,wewouldliketo
increasethedatainthesecategoriestotackledata
changesinthecontextualmeaningofthesentences imbalance,whichmaypotentiallyenhancetheper-
(seeAppendixD).Becauseofthis,ithadascoreof formanceofthetransformermodels. Inaddition,to
37.90%onhumanevaluationwhereourmainpri- combatnoisecomingfromspellingvariationand
orityofscoringwasthecontextualmeaningofthe dialectal differences, we plan to incorporate text
sentence. Weusedthehumanevaluationscoreto normalization methods i.e. character-level spell
achievethebestnoisereductionstrategy,although correction models (Farra et al., 2014; Zaky and
asshowninTable8,othertechniquesscoredwell Romadhony,2019)andcharacter-levelNeuralMa-
onseveralmetricsaswell. Nevertheless,itisworth- chineTranslation(NMT)models(Leeetal.,2017;
while to explore alternative approaches beyond Edman et al., 2023) for back-translation. We hy-
back-translationtodeterminewhetheraparticular pothesizethattextnormalizationmethodsmightbe
noisereductionmethodyieldssuperiorresultsinad- aviablesolutionduetotheirabilitytocomprehend
dressingspecifictypesofnoisytexts. Table11illus- contextatcharacterlevel. Finally,wewillinvesti-
gatenoise-specificreductiontechniquesandreport
Class Precision Recall F1-Score onthenoisereductionapproachesthatdemonstrate
Neutral 0.53 0.51 0.52 superior results in addressing particular types of
Positive 0.77 0.77 0.77
noisytexts.
Negative 0.78 0.80 0.79
Micro 0.73 0.73 0.73 6 Conclusion
Macro 0.69 0.69 0.69
Weighted 0.72 0.73 0.72 Thisstudyinvolvesacomparisonofvariousnoise
reduction techniques to assess their effectiveness
Table 11: Class-wise performance of BanglaBERT
inreducingnoisewithintheNC-SentNoBdataset,
Large on noise reduced texts (back-translation with
googletranslate). which includes ten distinct types of noises. The
results indicate that none of the noise reduction
tratestheclass-wiseresultsofourbest-performing methodseffectivelyreducenoiseinthetexts,lead-
model-BanglaBERT Largeonnoisereduceddata. ingtoalowerF1-scorecomparedtothesentiment
It is clear from the table that the results are quite analysisofnoisytexts. Thisunderscorestheneces-
high for the positive and negative classes but the sityforthedevelopmentofnoise-specificreduction
opposite for the neutral class. Few training data techniques. Weconductedastatisticalanalysisof
pointsmightbethereasonforthislowperformance our NC-SentNoB dataset and employed baseline
inthatparticularclass. modelstoidentifythenoises. However,thedataim-
balanceadverselyimpactsthemodelperformance
5 LimitationsandFutureWorks suggestingpotentialenhancementuponaddressing
thisimbalance.
One obvious limitation is that none of the noise
reductionmethodsweemployedwereabletocor-
rectly reduce noise from the noisy texts. As a References
result, fine-tuned models achieved a lower score
AjwadAkil,NajrinSultana,AbhikBhattacharjee,and
in sentiment analysis than models fine-tuned on Rifat Shahriyar. 2022. Banglaparaphrase: A high-
noisy texts. Another limitation is that we have quality bangla paraphrase dataset. arXiv preprint
arXiv:2210.05109.
not evaluated sentiment analysis by considering
alternative noise reduction techniques other than Abhik Bhattacharjee, Tahmid Hasan, Wasi Ahmad,Kazi Samin Mubasshir, Md Saiful Islam, Anindya JosephLFleiss.1971. Measuringnominalscaleagree-
Iqbal, M. Sohel Rahman, and Rifat Shahriyar. ment among many raters. Psychological bulletin,
2022a. BanglaBERT:Languagemodelpretraining 76(5):378.
and benchmarks for low-resource language under-
standing evaluation in Bangla. In Findings of the RezaulHaque,NaimulIslam,MayishaTasneem,and
AssociationforComputationalLinguistics: NAACL AmitKumarDas.2023. Multi-classsentimentclas-
2022,pages1318–1327,Seattle,UnitedStates.Asso- sificationonbengalisocialmediacommentsusing
ciationforComputationalLinguistics. machinelearning. InternationalJournalofCognitive
ComputinginEngineering,4:21–35.
AbhikBhattacharjee,TahmidHasan,WasiUddinAh-
Mahmud Hasan, Labiba Islam, Ismat Jahan, Sab-
mad,andRifatShahriyar.2022b. Banglanlg: Bench-
rinaMannanMeem,andRashedurMRahman.2023.
marks and resources for evaluating low-resource
Naturallanguageprocessingandsentimentanalysis
natural language generation in bangla. CoRR,
onbanglasocialmediacommentsonrussia–ukraine
abs/2205.11081.
war using transformers. Vietnam Journal of Com-
AnirbanBhowmickandAbhikJana.2021. Sentiment puterScience,pages1–28.
analysisforbengaliusingtransformerbasedmodels.
AsifHassan,MohammadRashedulAmin,AbulKalam
InProceedingsofthe18thInternationalConference
AlAzad,andNabeelMohammed.2016. Sentiment
onNaturalLanguageProcessing(ICON),pages481–
analysisonbanglaandromanizedbanglatextusing
486.
deeprecurrentmodels. In2016InternationalWork-
shoponComputationalIntelligence(IWCI),pages
ParthaChakraborty,FarahNawar,andHumayraAfrin
51–56.IEEE.
Chowdhury. 2022. Sentiment analysis of bengali
facebookdatausingclassicalanddeeplearningap-
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and
proaches. In Innovation in Electrical Power Engi-
WeizhuChen.2020. Deberta: Decoding-enhanced
neering, Communication, and Computing Technol-
bert with disentangled attention. arXiv preprint
ogy: Proceedings of Second IEPCCT 2021, pages
arXiv:2006.03654.
209–218.Springer.
SeppHochreiterandJürgenSchmidhuber.1997. Long
Kevin Clark, Minh-Thang Luong, Quoc V Le, and
short-termmemory. Neuralcomputation,9(8):1735–
ChristopherDManning.2020. Electra: Pre-training
1780.
textencodersasdiscriminatorsratherthangenerators.
arXivpreprintarXiv:2003.10555. MuntasirHoq,PromilaHaque,andMohammedNazim
Uddin.2021. Sentimentanalysisofbanglalanguage
CorinnaCortesandVladimirVapnik.1995. Support- using deep learning approaches. In International
vectornetworks. Machinelearning,20:273–297. ConferenceonComputingScience,Communication
andSecurity,pages140–151.Springer.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018a. Bert: Pre-training of Khondoker Ittehadul Islam, Md Saiful Islam, and
deepbidirectionaltransformersforlanguageunder- MdRuhulAmin.2020. Sentimentanalysisinbengali
standing. arXivpreprintarXiv:1810.04805. viatransferlearningusingmulti-lingualbert. In2020
23rdInternationalConferenceonComputerandIn-
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
formationTechnology(ICCIT),pages1–5.IEEE.
Kristina Toutanova. 2018b. BERT: pre-training of
deepbidirectionaltransformersforlanguageunder- KhondokerIttehadulIslam,SudiptaKar,MdSaifulIs-
standing. CoRR,abs/1810.04805. lam,andMohammadRuhulAmin.2021. Sentnob:A
datasetforanalysingsentimentonnoisybanglatexts.
Lukas Edman, Antonio Toral, and Gertjan van No- In Findings of the Association for Computational
ord. 2023. Are character-level translations worth Linguistics: EMNLP2021,pages3265–3271.
thewait? anextensivecomparisonofcharacter-and
subword-levelmodelsformachinetranslation. arXiv MdEkramulIslam,LabibChowdhury,FaisalAhamed
preprintarXiv:2302.14220. Khan,ShazzadHossain,SouraveHossain,Moham-
mad Mamun Or Rashid, Nabeel Mohammed, and
CharlesElkan.2001. Thefoundationsofcost-sensitive Mohammad Ruhul Amin. 2023. Sentigold: A
learning. In International joint conference on ar- largebanglagoldstandardmulti-domainsentiment
tificial intelligence, volume 17, pages 973–978. analysis dataset and its evaluation. arXiv preprint
LawrenceErlbaumAssociatesLtd. arXiv:2306.06147.
NouraFarra,NadiTomeh,AllaRozovskaya,andNizar Simran Khanuja, Diksha Bansal, Sarvesh Mehtani,
Habash.2014. Generalizedcharacter-levelspelling Savya Khosla, Atreyee Dey, Balaji Gopalan,
errorcorrection. InProceedingsofthe52ndAnnual DilipKumarMargam,PoojaAggarwal,RajivTeja
Meeting of the Association for Computational Lin- Nagipogu, Shachi Dave, Shruti Gupta, Subhash
guistics(Volume2: ShortPapers), pages161–167, ChandraBoseGali,VishSubramanian,andPartha
Baltimore,Maryland.AssociationforComputational Talukdar.2021. Muril: Multilingualrepresentations
Linguistics. forindianlanguages.Md Kowsher, Abdullah As Sami, Nusrat Jahan Prot- IlanPrice,JordanGifford-Moore,JoryFleming,Saul
tasha, MohammadShamsulArefin, PranabKumar Musker, Maayan Roichman, Guillaume Sylvain,
Dhar, and Takeshi Koshiba. 2022. Bangla-bert: NithumThain,LucasDixon,andJeffreySorensen.
transformer-basedefficientmodelfortransferlearn- 2020. Sixattributesofunhealthyconversation. arXiv
ing and language understanding. IEEE Access, preprintarXiv:2010.07410.
10:91855–91870.
ColinRaffel,NoamShazeer,AdamRoberts,Katherine
AomiKoyama,KengoHotate,MasahiroKaneko,and Lee,SharanNarang,MichaelMatena,YanqiZhou,
MamoruKomachi.2021. Comparisonofgrammat- WeiLi,andPeterJLiu.2020. Exploringthelimits
icalerrorcorrectionusingback-translationmodels. oftransferlearningwithaunifiedtext-to-texttrans-
arXivpreprintarXiv:2104.07848. former. TheJournalofMachineLearningResearch,
21(1):5485–5551.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Soricut. NilsReimersandIrynaGurevych.2019. Sentence-bert:
2019. Albert: Alitebertforself-supervisedlearn- Sentenceembeddingsusingsiamesebert-networks.
ing of language representations. arXiv preprint InProceedingsofthe2019ConferenceonEmpirical
arXiv:1909.11942. MethodsinNaturalLanguageProcessing.Associa-
tionforComputationalLinguistics.
Jason Lee, Kyunghyun Cho, and Thomas Hofmann.
MoythryManirSamia,AlimulRajee,MdRakibHasan,
2017. Fullycharacter-levelneuralmachinetransla-
Mohammad Omar Faruq, and Pintu Chandra Paul.
tionwithoutexplicitsegmentation. Transactionsof
2022. Aspect-basedsentimentanalysisforbengali
theAssociationforComputationalLinguistics,5:365–
textusingbidirectionalencoderrepresentationsfrom
378.
transformers (bert). International Journal of Ad-
Chin-Yew Lin. 2004. ROUGE: A package for auto- vancedComputerScienceandApplications,13(12).
maticevaluationofsummaries. InTextSummariza-
SagorSarker.2020. Banglabert:Bengalimasklanguage
tionBranchesOut,pages74–81,Barcelona,Spain.
modelforbengalilanguageunderstanding.
AssociationforComputationalLinguistics.
SagorSarker. 2021. Bnlp: Natural languageprocess-
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
ing toolkit for bengali language. arXiv preprint
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
arXiv:2102.00405.
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap- MdShajalalandMasakiAono.2018. Semantictextual
proach. arXivpreprintarXiv:1907.11692. similarityinbengalitext. In2018InternationalCon-
ferenceonBanglaSpeechandLanguageProcessing
MikaVMäntylä,DanielGraziotin,andMiikkaKuutila. (ICBSLP),pages1–5.IEEE.
2018. The evolution of sentiment analysis—a re-
viewofresearchtopics,venues,andtopcitedpapers. Ankit Srivastava, Piyush Makhija, and Anuj Gupta.
ComputerScienceReview,27:16–32. 2020. Noisy text data: Achilles’ heel of bert. In
Proceedings of the Sixth Workshop on Noisy User-
Hwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian generatedText(W-NUT2020),pages16–21.
Hadiwinoto,RaymondHendySusanto,andChristo-
pher Bryant. 2014. The CoNLL-2014 shared task Yifu Sun and Haoming Jiang. 2019. Contextual text
ongrammaticalerrorcorrection. InProceedingsof denoising with masked language models. arXiv
theEighteenthConferenceonComputationalNatu- preprintarXiv:1910.14080.
ral Language Learning: Shared Task, pages 1–14,
Wikipedia.2023. Listoflanguagesbytotalnumberof
Baltimore,Maryland.AssociationforComputational
speakers — Wikipedia, the free encyclopedia. ht
Linguistics.
tps://en.wikipedia.org/wiki/List_of_lang
uages_by_total_number_of_speakers. [Online;
TongNiu,SemihYavuz,YingboZhou,NitishShirish
accessed13-June-2023].
Keskar,HuanWang,andCaimingXiong.2020. Un-
supervised paraphrasing with pretrained language LintingXue,NoahConstant,AdamRoberts,MihirKale,
models. arXivpreprintarXiv:2010.12885. RamiAl-Rfou,AdityaSiddhant,AdityaBarua,and
ColinRaffel.2020. mt5: Amassivelymultilingual
KishorePapineni,SalimRoukos,ToddWard,andWei-
pre-trainedtext-to-texttransformer. arXivpreprint
JingZhu.2002. Bleu: amethodforautomaticevalu-
arXiv:2010.11934.
ationofmachinetranslation. InProceedingsofthe
40thAnnualMeetingoftheAssociationforCompu- DamarZakyandAdeRomadhony.2019. Anlstm-based
tational Linguistics, pages 311–318, Philadelphia, spellcheckerforindonesiantext. 2019International
Pennsylvania,USA.AssociationforComputational ConferenceofAdvancedInformatics: Concepts,The-
Linguistics. oryandApplications(ICAICTA),pages1–6.
David MW Powers. 2020. Evaluation: from pre- Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q
cision, recall and f-measure to roc, informed- Weinberger,andYoavArtzi.2019. Bertscore: Eval-
ness, markedness and correlation. arXiv preprint uating text generation with bert. arXiv preprint
arXiv:2010.16061. arXiv:1904.09675.Appendix pre-trainedusingtheMaskedLanguageModeling
(MLM) objective, specifically on extensive Ben-
A ModelDescriptions
gali corpora (Bhattacharjee et al., 2022a). It has
an embedding size of 768 and consists of 110M
A.1 NoiseIdentification
parameters. Thismodelhasbeenemployedtoper-
(a) SVM. Support Vector Machine (SVM) is de-
form the MLM task on out-of-vocabulary words
signedtofindahyperplaneinahigh-dimensional
andtoexecuterandomMLMwitheachwordhav-
space. This hyperplane separates data points of
inga20%possibilityofbeingmasked.
differentclasseswhilemaximizingthemarginbe-
tween these classes. For feature extraction, the A.3 SentimentAnalysis
TF-IDFVectorizerwasemployed,utilizingbotha
(a) BanglaBERT. An ELECTRA (Clark et al.,
characteranalyzerandawordanalyzer. Theseare
2020)discriminatormodelpre-trainedwiththeRe-
representedasSVM(C)forthecharacteranalyzer
placedTokenDetection(RTD)objective. Ithasan
andSVM(W)forthewordanalyzer,respectively,
embeddingsizeof768andatotalof110Mparam-
usingn-gramsintherangeof1to4. Additionally,
eters(Bhattacharjeeetal.,2022a).
acombinationofbothcharacterandwordn-gram
(b) BanglaBERT Large. A larger variant of
featureswastested,denotedasSVM(C+W).
BanglaBERT, with 335M parameters and an em-
(b)BiLSTM.BiLSTMcaptureslong-rangedepen-
beddingsizeof1024(Bhattacharjeeetal.,2022a).
denciesandcontextualinformationamongitemsin
(c)sahajBERT5.Pre-trainedinBengalilanguage
asequence. IthastwoLSTMlayers,onethatreads
using Masked Language Modeling (MLM) and
theinputsequenceinaforwarddirectionandthe
Sentence Order Prediction (SOP) objectives. It
other in a reverse direction. The outputs of these
followsALiteBERT(ALBERT)(Lanetal.,2019)
twolayersarethenconcatenatedtoproduceafinal
architectureandhasatotalof18Mparametersand
outputforeachiteminthesequence. OurBiLSTM
anembeddingsizeof128.
implementationfeaturesanembeddingsizeof512,
(d) Bangla-Electra6. Trained with ELECTRA-
ahiddensizeof110,andconsistsof2layers.
small(Clarketal.,2020)withanembeddingsize
(c)Bangla-BERT-Base. ApretrainedBengalilan-
of128andatotalof14Mparameters.
guage model using mask language modeling ob-
(e)MuRIL.ABERTmodelpre-trainedon17In-
jective(Sarker,2020). Ithasthesamearchitecture
dianlanguagesandtheirtransliteratedcounterparts
asthebert-base-uncased(Devlinetal.,2018b)
(Khanuja et al., 2021). It has 110M parameters
modelwithanembeddingsizeof768andatotal
andanembeddingsizeof768foreachtoken. The
parameterof110M.
modelispre-trainedonbothmonolingualandpar-
allelsegments.
A.2 NoiseReduction
(a) BanglaT5. A sequence-to-sequence trans- B PerformanceEvaluationMetrics
former model that has been pre-trained using the
B.1 NoiseReduction
span corruption objective (Bhattacharjee et al.,
2022b). Itconsistsof247millionparametersand (a)BLEU.BiLingualEvaluationUnderstudy(Pa-
hasanembeddingsizeof768. Fortheimplementa- pineni et al., 2002) is a commonly used scoring
tionoftheback-translationmethod,theBanglaT5 method that measures the overlap between refer-
model, pre-trained on the BanglaNMT Bangla- enceandcandidatesentences,providingasimilar-
English dataset (Bhattacharjee et al., 2022b), is itymeasurement.
usedforBengalitoEnglishtranslation. Conversely, (b) ROUGE-L. Recall-Oriented Understudy for
for English to Bengali translation, the BanglaT5 Gisting Evaluation - Longest Common Subse-
model pre-trained on the BanglaNMT English- quence (Lin, 2004) computes a similarity score
Bangladataset(Bhattacharjeeetal.,2022b)isuti- by taking into account of longest common sub-
lized. Additionally, the paraphrasing model em- sequencesappearinginbothreferenceandcandi-
ployed by us is also BanglaT5 model, which has date sentences. Similar to the BLEU score, this
beenpre-trainedontheBanglaParaphrasedataset scoringmethoddoesnotprovidemuchinsightinto
(Akiletal.,2022).
5https://huggingface.co/neuropark/sahajBERT
(b) BanglaBERT Generator. This is an ELEC-
6https://huggingface.co/monsoon-nlp/bangla-e
TRA (Clark et al., 2020) generator that has been lectrasemanticmeasurements,onlythesimilarityofover- definedas:
lappingwords/sub-sequences.
x
(c)BERTScore. BERTScore(Zhangetal.,2019) Score(HumanEvaluation) = ∗100
T
usesthecosinesimilarityofcontextualembedding
ofthetokenprovidedfromaBERT-basedmodel. Here,x=Accuratelynoisereduceddata
Forthis,weusedthebert-score7 library,which T=Totalnumberofdata
usesamultilingualBERTforBengalisentences.
B.2 Classification
(d) SBERT Score. For this method, we em-
Forbothclassificationtasks(noiseandsentiment),
ployed paraphrase-multilingual-MiniLM-L12-v2
weusedmicroprecision,recall,andF1-score.
(ReimersandGurevych,2019),amodelthatmaps
(a)Precision. Precisionmeasurestheaccuracyof
sentences and paragraphs to a 384 dimensional
positivepredictions,specificallyhowmanyofthem
densevectorspace. Itsupportsmorethan50lan-
arecorrect(truepositives)(Powers,2020). Alter-
guagesandemployscosinesimilaritytoassessthe
nativelyknownasTruePositiveAccuracy(TPA),
similarity between the input text and the ground
itiscalculatedas:
truth.
(e)BSTS.BanglaSemanticTextualSimilaritywas TP
Precision =
first introduced by (Shajalal and Aono, 2018). It
TP +FP
usesembeddingsofWord2Vectocalculatethesim-
whereTPindicatestruepositiveandFPindicates
ilaritybetweentwosentences.
falsepositive.
(f)BERT-iBLEU.Thescoringmethodwasorigi-
(b) Recall. Recall, or True Positive Rate (TPR),
nally proposed by (Niu et al., 2020), which com-
gaugestheclassifier’sabilitytoaccuratelypredict
binesBERT-ScoreandBLEUScoretomeasurethe
positivecasesbydetermininghowmanyofthemit
semanticsimilarityofsentenceswhilepenalizing
correctlyidentifiedoutofallthepositivecasesin
for the presence of similar words. This scoring
thedataset(Powers,2020). Itisdefinedas:
systemisparticularlysuitableforourneeds,aswe
intendtoevaluatethemethodbasedonitsability
TP
tokeepthesemanticmeaningintactwhilemaking Recall =
TP +FN
necessarychangestoreducenoises.
(g)WordCoverage. Pre-trainedwordembedding whereTPindicatestruepositiveandFNindicates
modelslikeFastText(Sarker,2021),andWord2Vec falsenegative.
(Sarker, 2021) create a vocabulary on the corpus (c)F1-Score. TheF1-scoreistheharmonicmean
they are trained on. As they are trained on noise- of precision and recall, providing a balance be-
less sources like Wikipedia articles, their vocab- tween the two in cases where one may be more
ularycontainsaccuratewords. Bymeasuringthe significantthantheother. F1-scoreisdefinedas:
percentageoftokensofourdatacoveredintheirvo-
Precision×Recall
cabulary,wecangaininsightintowhatpercentage F1-Score = 2×
Precision+Recall
oftokenswerenoisereducedproperly. However,
this method may not address all types of noises.
Additionally,wealsocalculatedwordcoverageus-
ingthevocabularyofBangla-BERT-Base(Sarker,
2020).
(h) Human Evaluation. The output texts were
evaluatedbyannotatorsbycomparingthemtothe
1000 established ground truths. A noise reduced
output was considered correct if it retained the
samemeaningasthegroundtruthandreducedat
leastsomeofthenoiseorcompletenoisefromthe
originalsentence. Inessence,thescorerepresents
theproportionofaccuratenoisereduceddatarel-
ative to the 1000 ground truth. The score can be
7https://pypi.org/project/bert-score/C TypesofNoiseinNC-SentNoB
NC-SentNoBdatasetcontainslabeleddatafor10typesofnoise. Table12illustratesthedefinitionofeach
noisetypeannotatorsusedfortheannotationprocess. IncaseofPunctuationError,anexceptionwas
made for sentences that end without a period "." due to the nature of the data. If such instances were
considerederrors,themajorityofthedatawouldbelabeledashavingpunctuationerrors. Thiscouldlead
totrainedmodelspredominantlyfocusingonthissingletypeoferror,ratherthanrecognizingandlearning
fromabroaderrangeofpunctuationerrors.
Type Definition ExamplewithCorrection
LocalWord Any regional words even if there is a [N](cid:23)eS(cid:26)r saeQ U(cid:209)err ekan iml paIlam na
spellingerror [C](cid:23)eS(cid:26)r saeQ U(cid:209)err ekan iml eplam na
[E]Ididnotfindanysimilaritybetween
thequestionandtheanswer.
WordMisuse Wronguseofwordsorunnecessaryrepe- [N]taek AaIenr AaOtay Sai(cid:218) edOya eHak
titionsofwords [C]taek AaIenr AaOtay Sai(cid:239) edOya eHak
[E]Heshouldbepunishedunderthelaw.
Context/Wordmiss- Notenoughinformationormissingwords [N]itin EkmaE paern EI mHaibpd - p(cid:144)iQb(cid:140)ek
ing rXa kret
[C] itin Ekma(cid:213) paern EI mHaibpd eQek p(cid:144)iQ-
b(cid:140)ek rXa kret
[E]Heistheonlyonewhocansavethe
worldfromthiscatastrophe.
WrongSerial Wrongorderofthewords [N]saraedeS Apraz(cid:140) x(cid:132)ujun , Aaera Hey HenY
[C]Aaera HenY Hey saraedeS Apraz(cid:140) x(cid:132)ujun
[E]Searchforthecriminaldesperately.
MixedLanguage Wordsinanotherlanguage. Foreignwords [N]vaIer EI inUjTa esra inIj
that were adopted into the Bengali lan- [C]vaIer, EI xbrTa esra xbr
guage over time are excluded from this [E]Brother,thisnewsisthebestnews.
type.
PunctuationError Improper placement or missing punctua- [N]perr paflTguela keb Aaseb vaI 1
tion. Sentences ending without "." (d(cid:132)ai(cid:136)) [C]perr pflbguela keb Aaseb vaI?
wereexcludedfromthistype. [E] When will the next episodes air
brother?
SpacingError Improperuseofwhitespace [N]p(cid:136)aeSana Ta cailey egel vaela Heta
[C]p(cid:136)aeSanaTa cailey egel vaela Heta
[E]Itwouldbebettertocontinuestudying
SpellingError Words not following spelling of Bangla [N]baibek Et jal xaOyaena iFk na
AcademyDictionary [C]vab(cid:140)ek Et Cal xaOyaena iFk na
[E]Itisnotrighttofeedthesister-in-law
somuchspice.
CoinedWord Emoji,symbolicemoji,link [N]Aaeg janel Aapnar saeQ edxa krtam
[C]✗
[E]IfIknewIwould’vemetyouearlier
Others Noisesthatdonotfallintocategoriesmen- [N]red ku(cid:209)ar ba£caedr f(cid:132)ais caI
tionedabove. [C]✗
[E]Iwantthosesonsofbitcheshanged.
Table12: Typesofnoisewiththedefinitionthatwasusedtoannotatethedataset. Nrepresentstheoriginalnoisy
sentence,Crepresentsthecorrectedsentence,andErepresentsthecorrespondingEnglishtranslation. Thetypes
CoinedWord,andOthersdonothaveanycorrectionasthesetypesofnoiseareessentialtothemeaningofthe
sentence. Foreachexample,noisywordsofthatparticulartypearemarkedwithRedcolor,andtheircorrectionis
markedwithGreencolor.D FailureCasesofBack-translation
Toprovideinsightintotheperformancedrop,wehaveillustratedexampleswheretheback-translation
methodusingGoogleTranslatefailstoadequatelyreducenoiseintheinputtextintable13. Moreover,it
oftenaltersorcompletelyremovesimportantcontextualwords,whichpossiblyimpactstheperformance
ofsentimentanalysis. Givenahumanevaluationscoreof37.90%,itcanbesaidthatback-translationvia
GoogleTranslatefailstoeffectivelycorrectmorethan50%ofthe1000manuallycorrecteddata.
NoisydataandcorrespondingBack-Translation Observation
[N]EI juyar Taka papn ept Aamar men Hy Theinputtextcontainedonlyaspellingmistake,but
[C]EI juyar Taka papn epeta Aamar men Hy theback-translationintroducednewwords,removed
[E]IthinkthegamblingmoneywenttoPapan. anamedentity,andalteredthesentence’smeaning.
[B]Aaim men kir EI juyar Taka pireSaz kra Heb
[BE]Ithinkthisgamblingmoneywillberepaid.
[N] vaI xabaerr saz ejmin eHak na ekn Aapnar muex egel The input text had multiple spelling mistakes and
esTa AimRt Hey Jay z(cid:223)bad punctuation errors. The back-translation corrected
[C] vaI xabaerr (cid:156)Wad eJmin eHak na ekn Aapnar muex egel oneoftheseerrorsbutchangedthemeaningofpart
esTa Am(cid:144)t Hey Jay, znYbad ofthesentence.
[E]Brother,whateverthetasteofthefoodis,itbe-
comesnecterinyourmouth,thanks.
[B] vaI, xabaerr (cid:156)Wad JaI eHak na ekn, ETa Aapnar muex
Aaeq.
[BE]Brother,whateverthetasteofthefoodis,it’sin
yourmouth.
[N]Eedr ipeTr cam(cid:136)a etala Heb The input text contained only a spelling mistake.
[C]Eedr ipeFr cam(cid:136)a etala Heb However,theback-translationremovedcontextually
[E]Theirbackswillbeskinned. important words, rendering the sentence meaning-
[B]tara cam(cid:136)a Heb less.
[BE]Theywillbecomeskin.
[N]A amar emeqr saeQ EI eHaeTl The back-translation altered a keyword in the sen-
[C]Aamar emesr saeQ EI eHaeTl tence,whichresultedinalossofmeaning.
[E]Thishoteliswithmyhostel.
[B]Aamar jal idey EI eHaeTl
[BE]Thishotelwithmynet.
[N]ilbur saet ik Aada exet Heb 1 Theback-translationfailedtocorrectaspellingmis-
[C]elbur saeQ ik Aada exet Heb? takeandconvertedthewordintoEnglish,butitsuc-
[E]DoIneedtoeatgingerwithlemon? cessfullyaddedthemissingpunctuation.
[B]Aaim ikLiburSateE Aada xaOya Uict?
[BE]ShouldIeatgingerwithLiburSate?
[N]rana edr mt eqelra jaet Hairey na jay Theinputsentencehadspacingandspellingerrors.
[C]ranaedr mt eqelra Jaet Hairey na Jay Theback-translationfixedthespacingissuebutin-
[E]SothatboyslikeRanadon’tgetlost. troduced mixed language, changing the sentence’s
[B]ranar meta eqelra eres eHer Jay na meaning.
[BE]BoyslikeRanadonotloseinrace.
Table 13: Example scenarios where back-translation with google translate fails to reduce noise in the text. N
representstheoriginalnoisysentence,Crepresentsthecorrectedsentence,ErepresentsitsEnglishtranslation,B
representstheresultofback-translation,andBErepresentsthedirectEnglishtranslationofback-translatedoutput.
Foreachexample,noisywordsaremarkedwithRedcolorandnoisereducedwordsaremarkedwithGreencolor.