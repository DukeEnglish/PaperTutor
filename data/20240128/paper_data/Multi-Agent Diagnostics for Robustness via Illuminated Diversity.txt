Multi-Agent Diagnostics for Robustness via Illuminated Diversity
MikayelSamvelyan∗ DavidePaglieri∗ MinqiJiang
UCL,MetaAI UCL UCL,MetaAI
samvelyan@meta.com d.paglieri@cs.ucl.ac.uk minqi.jiang@cs.ucl.ac.uk
JackParker-Holder TimRocktäschel
UCL UCL
j.parker-holder@ucl.ac.uk tim.rocktaschel@ucl.ac.uk
ABSTRACT
Selection Perturbation
Intherapidlyadvancingfieldofmulti-agentsystems,ensuringro-
bustnessinunfamiliarandadversarialsettingsiscrucial.Notwith-
standingtheiroutstandingperformanceinfamiliarenvironments,
thesesystemsoftenfalterinnewsituationsduetooverfittingdur-
ingthetrainingphase.Thisisespeciallypronouncedinsettings Evaluation
wherebothcooperativeandcompetitivebehavioursarepresent,
encapsulatingadualnatureofoverfittingandgeneralisationchal-
lenges.Toaddressthisissue,wepresentMulti-AgentDiagnosticsfor
RobustnessviaIlluminatedDiversity(MADRID),anovelapproach
forgeneratingdiverseadversarialscenariosthatexposestrategic Addition
vulnerabilitiesinpre-trainedmulti-agentpolicies.Leveragingthe
Figure 1: Overview of MADRID. Operating on a discre-
conceptsfromopen-endedlearning,MADRIDnavigatesthevast
tisedgridwithanaddeddimensionforreferencepolicies,
spaceofadversarialsettings,employingatargetpolicy’sregretto
MADRIDarchivesenvironmentvariations(orlevels)charac-
gaugethevulnerabilitiesofthesesettings.Weevaluatetheeffective-
nessofMADRIDonthe11vs11versionofGoogleResearchFootball,
terizedbyrepresentativefeatures,e.g.,(𝑥,𝑦)coordinatesof
theballpositioninfootball.Duringeachiteration,MADRID
oneofthemostcomplexenvironmentsformulti-agentreinforce-
mutatesaselectedlevel,computesregretusingitsassociated
mentlearning.Specifically,weemployMADRIDforgeneratinga
referencepolicy,andreincorporateslevelswithhigherregret
diversearrayofadversarialsettingsforTiZero,thestate-of-the-art
intothearchive,effectivelygeneratingdiversecollectionof
approachwhich"masters"thegamethrough45daysoftrainingon
adversariallevels.
alarge-scaledistributedinfrastructure.Weexposekeyshortcom-
ingsinTiZero’stacticaldecision-making,underliningthecrucial
importanceofrigorousevaluationinmulti-agentsystems.1
havebeensignificantsuccessesinsimulatedenvironments,asevi-
dencedbydeepreinforcementlearning(RL)incomplexmulti-agent
KEYWORDS
games[4,41,44,50,55],thetransferfromsimulationtoreality
Multi-AgentLearning,Open-Endedness,Generalisation (sim2real)continuestoposechallenges[19,57].Specifically,while
ACMReferenceFormat: thesemodelsdemonstrateproficiencyinknownenvironments,they
becomehighlysusceptibletofaultybehaviorsinunfamiliarset-
MikayelSamvelyan,DavidePaglieri,MinqiJiang,JackParker-Holder,and
TimRocktäschel.2024.Multi-AgentDiagnosticsforRobustnessviaIllumi- tingsandadversarialsituations[39].Giventheircriticalrolesin
natedDiversity.InProc.ofthe23rdInternationalConferenceonAutonomous human-centricapplications,understandingandmitigatingthese
AgentsandMultiagentSystems(AAMAS2024),Auckland,NewZealand,May susceptibilitiesbecomesparamountforfosteringmoreeffective
6–10,2024,IFAAMAS,15pages. andreliabledeploymentofmulti-agentAIsystemsinthefuture.
TheAchilles’heelofthesemulti-agentsystems,contributing
1 INTRODUCTION
totheirlackofrobustness,isoftentheiroverfittingtothespecific
Inrecenttimes,multi-agentsystems,particularlythosedesigned settingsencounteredduringtraining[26].Thisoverfittingbecomes
tointeractwithhumans,haveemergedasaprimarymodelforAI notablyevidentintwo-teamzero-sumsettingswherebothcoopera-
deploymentinreal-worldscenarios[1,2,33,48].Althoughthere tiveandcompetitivedynamicsintertwine.Aprimarymanifestation
oftheoverfittingbetweencooperativeagents,especiallywhenall
∗Equalcontribution
agentsinthegroupsharethesamesetofnetworkparameters(i.e.,
1Visualsandcodeareavailableathttps://sites.google.com/view/madrid-marl
parametersharing[14]),isintheagentsbecomingtooaccustomed
totheirtrainingenvironments,leadingtoadetailedcoordination
ThisworkislicensedunderaCreativeCommonsAttribution
International4.0License. tailoredtothesespecificconditions.Asaconsequence,whenin-
troducedtounfamiliarsettings,theirperformancetendstofalter.
Proc.ofthe23rdInternationalConferenceonAutonomousAgentsandMultiagentSystems Concurrently,thereisalsoanoverfittingtospecificopponentteams
(AAMAS2024),N.Alechina,V.Dignum,M.Dastani,J.S.Sichman(eds.),May6–10,2024,
theyhavetrainedagainst.Insteadofdevelopingaflexiblestrategy
Auckland,NewZealand.©2024InternationalFoundationforAutonomousAgentsand
MultiagentSystems(www.ifaamas.org). thatcanwithstandavarietyofopponents,theirstrategiesmightbe
4202
naJ
42
]GL.sc[
1v06431.1042:viXraoverlyoptimisedtocounteractthestrategiesoffamiliaradversaries. comprehendtheoffsideruleeffectively,andevenencounterssitua-
Thesedualformsofoverfitting—bothtotheenvironmentandto tionsofscoringaccidentalowngoals.Thesefindingshighlightthe
opponents—rendersuchsettingsasperfectplatformstoprobefor latentvulnerabilitieswithinevenhighlytrainedmodelsanddemon-
vulnerabilities[49].Furthermore,itiscrucialtopinpointadiverse stratethatthereismuchroomforimprovingthetheirrobustness.
setofadversarialscenariosforaholisticdiagnosticofrobustness, Ouranalysisshowcasesthevalueofidentifyingsuchadversarial
sheddinglightonpossibleshortcomingsfromvariousperspectives. settingsinofferingnewinsightsintothehiddenweaknessesof
Giventhesechallenges,weintroduceMulti-AgentDiagnosticsfor pretrainedpoliciesthatmayotherwiseappearundefeatable.
RobustnessviaIlluminatedDiversity(MADRID),anovelmethodfor
2 BACKGROUND
systematicallygeneratingadiversecollectionofadversarialsettings
wherepre-trainedmulti-agentpoliciesmakestrategicmistakes. UnderspecifiedStochasticGames. Inthiswork,weconsider
Tothisend,MADRIDemploysapproachesfromquality-diversity UnderspecifiedStochasticGames(USG),i.e.,stochasticgames[43]
(QD)[8,27],afamilyofevolutionaryalgorithmthataimtogenerate withunderspecifiedparametersoftheenvironment.AnUSGgame
alargecollectionofhigh-performingsolutionseachwiththeirown for𝑁 agentenvironmentisdefinedbyasetofstatesS,actions
uniquecharacteristics. A 1,...,A𝑁 and a set of observations O 1,...,O𝑁 for each of the
MADRIDincorporatesMAP-Elites[32],asimpleandeffective agents.Eachagent𝑖 selectactionsusingastochasticpolicy𝜋 𝑖 :
QDapproach,tosystematicallyexplorethevastspaceofadversar- O𝑖×A𝑖 ↦→ [0,1].Θdefinesthefreeparametersoftheenvironments
ialsettings.Bydiscretisingthesearchspace,MADRIDiteratively whichareincorporatedintothetransitionfunctionT :S×Θ×
performsselection,mutation,andevaluationssteps,endlesslyre- A 1×...×A𝑁 ↦→Swhichproducesthenextstatebasedonthe
finingandexpandingtherepertoireofhigh-performingadversarial actionsofallagents.Eachagent𝑖receivesobservationso𝑖 :S↦→O𝑖
scenarios within its archive (see Figure 1). A crucial feature of correlatedwiththecurrentstateandreward𝑟 𝑖 :S×A𝑖 ↦→Rasa
MADRIDisitsemploymentofthetargetpolicy’sregret—thegap functionofthestateandagent’saction.Thegoalofeachagent𝑖is
inperformancebetweentheoptimalandtargetpolicy—toquantify tomaximiseitsowntotalexpectedreturn𝑅 𝑖 =(cid:205)𝑇 𝑡=0𝛾𝑡𝑟 𝑖𝑡 forthe
thequalityofadversarialsettings.Regretisshowntobeaneffective timehorizon𝑇,where𝛾 isadiscountfactor.
metricforidentifyingsituationswhereRLagentsunderperform Eachconfigurationofthefreeparameter𝜃 ∈Θ,whichisoften
inbothsingle-agent[11,22,31,35]andmulti-agent[39]domains. calledalevel[22,35],definesaspecificinstantiationoftheenviron-
MADRIDestimatesalower-boundonthetrueregretbyutilisinga mentM𝜃.Forexample,thiscancorrespondtodifferentpositions
collectionofreferencepolicies[17,50],whicharenotnecessarilyre- ofthewallsinamaze,orlocationsofplayersandtheballina
quiredtobehigh-performing.MADRIDidentifiessituationswhere footballgame.USGisamulti-agentvariationofUnderspecified
thesereferencepoliciessurpassthetargetone,therebyprovidinga POMDPs[11]andfullyobservablevariantofUPOSGs[39].
clearillustrationofsuperiorperformanceingivensituations.
Quality-Diversity. Quality-diversity(QD)isafamilyofmeth-
ToevaluateMADRID,weconcentratespecificallyononeofthe
odsusedtofindadiversecollectionofsolutionsthatareperformant
most challenging multi-agent domains, namely the fully decen-
andspanameaningfulspectrumofsolutioncharacteristics[8,27].
tralised11vs11variationofGoogleResearchFootball[GRF,25].
Theperformanceofsolution𝑥 ∈ X ismeasureusingthefitness
Thissimulatedenvironmentisbasedonthepopularreal-world
:X↦→Rfunction.Thediversityofsolutionsistypicallymeasured
sportoffootball(a.k.a.soccer)andrequirestwoteamsofagents
usingthefeature_descriptor:X↦→Bfunctionthatmapsasolution
tocombineshort-termcontroltechniqueswithcoordinated,long-
termglobalstrategies.GRFrepresentsauniquecombinationof
intothefeaturespaceB=R𝐾
thatdescribesspecificcharacteristics
characteristicsnotpresentinotherRLenvironments[29],namely ofthesolution,suchasbehavioralpropertiesorvisualappearance.
multi-agentcooperation(withineachteam),competition(between
thetwoteams),sparserewards,largeactionandobservationspaces, Algorithm1:MAP-Elites[32]
andstochasticdynamics.Whilemanyoftheindividualchallenges Initialise:𝑁-dimensionalgridsforsolutions𝑋 and
inGRF,includingmulti-agentcoordination[36,56],long-termplan- performancesP
ning[12]andnon-transitivity[3,9],havebeenstudiedextensively Initialise:𝑛cellsof𝑋 withrandomsolutionsand
in isolation, learning highly-competitive GRF policies has long correspondingcellsof𝑃 withtheirfitness
remainedoutsidethereachofRLmethods.TiZero[29],arecent for𝑖 ={1,2,...}do
multi-agentRLapproach,learnedto"master"thefullydecentralised Samplesolution𝑥 from𝑋
variationofGRFfromscratchforthefirsttime,usingahand-crafted Getsolution𝑥′from𝑥 viarandommutation
curriculum,rewardshaping,andself-play.Experimentally,TiZero 𝑝′ ←𝑓𝑖𝑡𝑛𝑒𝑠𝑠(𝑥′)
hasshownimpressiveresultsandoutperformedpreviousmethods 𝑏′ ←𝑓𝑒𝑎𝑡𝑢𝑟𝑒_𝑑𝑒𝑠𝑐𝑟𝑖𝑝𝑡𝑜𝑟(𝑥′)
byalargemarginafteranexpensivetraininglasting45daysona ifP(𝑏′)=∅𝑜𝑟 P(𝑏′) <𝑝′then
large-scaledistributedtraininginfrastructure. P(𝑏′)←𝑝′
WeapplyMADRIDonGRFbytargetingTiZerotodiagnosea 𝑋(𝑏′)←𝑏′
broadsetofscenariosinwhichitcommitstacticalmistakes.Ourex-
tensiveevaluationsrevealdiversesettingswhereTiZeroexhibitsa
poorperformance,whereweakerpoliciescanoutperformit.Specif- MAP-Elites. MAP-ElitesisasimpleandeffectiveQDmethod
ically,MADRIDdiscoversinstanceswhereTiZeroisineffective [32].Here,thedescriptorspaceBisdiscretisedandrepresentedas
atneartheopponent’sgoal,demonstratesamarkedinabilityto aninitiallyempty𝑁 <𝐾dimensionalgrid(archive).Thealgorithmstartsbygeneratinganarbitrarycollectionofcandidatesolutions. Algorithm2:MADRID
In each iteration, a solution is randomly selected among those Input:Targetpolicy𝜋
𝑇
inthegrid.Anewsolutionisobtainedbymutatingtheselected Input:AcollectionofreferencepoliciesΠ
𝑅
solution,whichisthenevaluatedandmappedtoacellofthegrid Input:𝑙𝑒𝑣𝑒𝑙_𝑑𝑒𝑠𝑐𝑟𝑖𝑝𝑡𝑜𝑟 :Θ↦→R𝑁 function
basedonitsfeaturedescriptor.Thesolutionisthenplacedinthe #Initialiseadiscretisedgrid,withanaddeddimensionforΠ 𝑅,to
correspondingcellofthegridifithasahigherfitnessthanthe
archivelevelsandregretscores.
currentoccupant,orifthecellifitisempty.Thiscycleofselection, Initialise:𝑁 +1-dimensionalgridsforlevels𝑋 andregret
mutation,andevaluationisrepeated,progressivelyenhancingboth estimatesP
thediversity(coverage)andthequality(fitness)ofthecollection. Initialise:𝑛cellsof𝑋 withrandomlygeneratedlevelsand
Thepseudo-codeofMAP-ElitesispresentedinAlgorithm1. correspondingestimatedregretinP
for𝑖 ={1,2,...}do
3 MADRID
#Samplealevel𝜃 andcorrespondingreferencepolicy𝜋
𝑅
from𝑋.
Inthissection,wedescribeMulti-AgentDiagnosticsforRobustness 𝜃,𝜋 𝑅 ∼𝑋
viaIlluminatedDiversity(MADRID),anovelmethodforautomati- #PerformlevelmutationbyaddingGaussiannoise.
callygeneratingdiverseadversarialsettingsforatargetpre-trained 𝜃′ ←𝜃+N(0,𝜎2)
policy𝜋 𝑇.Thesearesettingsthateitherdeceivethepolicy,forcing #Estimatetheregretof𝜋 𝑇 on𝜃′using𝜋 𝑅.
ittoproduceincorrectbehaviour,orwherethepolicyinherently (cid:101)𝑟′ ←𝑉𝜃′(𝜋 𝑅,𝜋 𝑇)−𝑉𝜃′(𝜋 𝑇,𝜋 𝑇)
performspoorly,deviatingfromtheoptimalbehaviour.ForUSGs, 𝑏′ ←𝑙𝑒𝑣𝑒𝑙_𝑑𝑒𝑠𝑐𝑟𝑖𝑝𝑡𝑜𝑟(𝜃′)
thesesettingscorrespondtoparticularenvironmentlevels𝜃 ∈Θ ifP(𝑏′,𝜋 𝑅)=∅𝑜𝑟 P(𝑏′,𝜋 𝑅) < (cid:101)𝑟′then
thathavebeenprocedurallygenerated. P(𝑏′,𝜋 𝑅)← (cid:101)𝑟′
Forquantifyingadversariallevels,wemakeusethetargetpol-
𝑋(𝑏′,𝜋 𝑅)←𝑏′
icy’s regret in level𝜃, i.e., the difference in utility between the
optimalpolicy𝜋∗and𝜋 𝑇 :
Regret𝜃 (𝜋∗,𝜋 𝑇)=𝑉𝜃 (𝜋∗,𝜋 𝑇)−𝑉𝜃 (𝜋 𝑇,𝜋 𝑇), foundinMADRIDgiventhateachcelldefinesspecificenvironment
parameters,alongsideareferencepolicywhichoutperformsthe
where𝑉 𝜃(𝜋 𝐴,𝜋 𝐵) = E[(cid:205)𝑇 𝑡=0𝛾𝑡𝑟 𝑡𝐴] is the value of a policy 𝜋 𝐴 targetundertheseparameters.
againstpolicy𝜋 𝐵 in𝜃.2 MADRIDstartsbypopulatingthegridwithinitiallevelsforeach
Regretisasuitablemetricforevaluatingadversarialexamplesin referencepolicy.Duringtheiterativeprocess,levelsareselected
pre-trainedmodels.Itprovidesameasurethatdirectlyquantifies fromthegridtoundergomutation,followedbyregretestimation.
thesuboptimalityofamodel’sdecisions.Whileahighregretvalue Eachmutatedlevelisthenmappedtoaspecificcellinthegridbased
servesasaglaringindicatorofhowfaroffamodel’sbehavioris onitsfeaturesandreplacestheexistingoccupantifthemutated
fromtheoptimalchoice,alowregretindicatesthemodel’sdeci- levelhashigherregretorthecorrespondingcellisunoccupied.
sionsarecloselyalignedwiththeoptimalchoice.Theimportance Thisprocedureensuresathoroughexplorationandexploitation
ofregretbecomesevenmorepronouncedwhenconsideringthe oftheenvironmentdesignspace,allowingMADRIDtogenerate
variedscenariosinwhichamodelmightbedeployed.Therefore, levelsthatarebothdiverseandhigh-regret.Figure1illustratesthis
byinvestigatingregretacrossdiversesituations,wecannotonly process.Algorithm2providesthepseudocodeofthemethod.
pinpoint specific vulnerabilities of a model but also ensure the
4 EXPERIMENTALSETTING
robustnessinpreviouslyunseenscenarios.
Sincetheoptimalpolicyisusuallyunavailable,MADRIDrelies Ourexperimentsseekto(1)showcasetheeffectivenessofMADRID
onutilisingacollectionofsuboptimal policiesΠ 𝑅 = (cid:208) 𝑖𝑀 =1𝜋 𝑖 for ingeneratingdiverseadversarialsettingsforatargetstate-of-the-
estimatingthelowerboundontrueregret.Specifically,thegoalis artpre-trainedRLmodel,(2)analysetheadversarialsettingsgener-
tofindadversariallevelsthatmaximisethegapinutilityacquired atedbyMADRIDtofindkeyweaknessesofthetargetmodel,(3)
throughareferencepolicy𝜋 𝑖 ∈Π 𝑅 andtargetpolicy𝜋 𝑇.Utilising validatethedesignchoicesof MADRIDbycomparingittotwo
acollectionofdiversereferencepoliciescanbeadvantageousin ablatedbaselines.Tothisend,weevaluateMADRIDonGoogle
theabsenceofatrueoptimalpolicy,sinceeachofthesereference ResearchFootball[GRF,25].Givenitsstrongperformanceandus-
policiesmayexcelinauniquesetoflevels[39]. ageinrelatedworks,CovarianceMatrixAdaptationMAP-Elites
MADRIDcaststhetaskofgeneratingadiversearrayofadver- [CMA-ME,16]servesasthebaseMAP-Elitesmethodinourexper-
sarial levels for each reference policy as a QD search problem. iments.WeprovidefullenvironmentdescriptionsinAppendixB
Specifically,MADRIDusesMAP-Elitestosystematicallygenerate andimplementationdetailsinAppendixC.
levelsfromΘbydiscretisingthefeaturespaceoflevelsintoan
𝑁-dimensionalgrid,withanadditionaldimensionrepresentingthe Baselines. WecompareMADRIDagainsttwobaselines:The
correspondingreferencepolicyfromΠ 𝑇.Usingadiscretisedgrid targetedbaselineusesaMAP-Elitesarchivebutrandomlysamples
ofMAP-Elitesprovidesinterpretabilitytotheadversarialexamples levels from scratch, rather then evolving previously discovered
high-regretlevelsfromthegrid.Consequently,itdoesnotleverage
2Notethathere,forthesimplicityofthenotation,weassumeatwo-teamzerosum thesteppingstonestotheoptimisationproblem[27].Therandom
setting.𝜋𝑇 and𝜋𝑅describethepoliciesforgroupsofagents,eitherthroughacen-
baselinesampleslevelsrandomlyfromscratchwithoutmaintaining
tralisedcontrollerordecentralisedpoliciesthatemployparametersharing.However,
MADRIDcanbeappliedformoregeneralmulti-agentsettings. anarchiveofhigh-regretlevels.Figure2:ExamplesofrandomlygeneratedlevelsonGoogleResearchFootball. Figure 3: Dividing the field in 160
gridsusingtheball(𝑥,𝑦)coordinates.
Environment
theballasthefirsttwoenvironmentfeaturesinMADRID.This
WeuseMADRIDtofindadversarialscenariosforTiZero,thestate- leadstoacategorisationoflevelsinto160uniformlyspacedcells
of-the-artmodelforGRF.TiZerowastrainedviaacomplexregime acrossthefootballfield,asillustratedinFigure3.Giventhatwe
onlarge-scaledistributedinfrastructure[29]over45days.Inpar- areinterestedinevaluatingTiZeroinspecificadversariallevels,in
ticular,weaimtogenerateadversariallevelswherebythedecen- ourexperimentswerestricttheepisodelengthto128stepstaking
tralisedagentsinTiZeromakeadiversearrayofstrategicerrors, placeinthebeginningofthegame.
ashighlightedbybetterbehavioursofthereferencepolicy. ThethirdaxisfortheMAP-Elitesarchiveindexesthereference
GRF is a complex open-source RL environment designed for policiesΠ 𝑅.Inourexperiments,wemakeuseof48checkpoints
trainingandevaluatingagentstomastertheintricatedynamics ofTiZerosavedthroughoutitstraining[29],aswellasthreebuilt-
offootball,oneoftheworld’smostcelebratedsports.Itoffersa inbotsinGRFwithvaryingdifficulties(easy,medium,hard).For
physics-based3DsimulationthattaskstheRLpolicywithcontrol- eachreferencepolicy,weinitialisethegridwithrandomlysampled
lingateamofplayerstopenetratetheopponent’sdefense,while levelsthatassignrandomlocationstoplayersandtheball.Figure2
passingtheballamongteammates,inordertoscoregoals.GRF illustratessomeoftherandomlygeneratedlevels.
isatwo-teamzero-sumenvironmentthathaslongbeenconsid- AteachiterationofMADRID,wesamplealevelandreference
eredoneofthemostcomplexmulti-agentRLbenchmarksduetoa
policypair(𝜃,𝜋 𝑅).ThelevelisthenmutatedbyaddingGaussian
uniquecombinationofchallenges[20,29,53],suchasmulti-agent
noisetothe(𝑥,𝑦)positionsoftheplayersandtheballinthefield.
cooperation,multi-agentcompetition,sparserewards,largeaction ThefitnessofeachsolutionisestimatedbycomputingTiZero’s
andobservationspaces,andstochasticdynamics.3 regret,whichisthedifferenceinperformancebetweentheselected
Inthiswork,wefocusonthefullydecentralised11vs11version referencepolicy𝜋 𝑅 andTiZero’spolicy𝜋 𝑇.Inbothcases,weesti-
oftheenvironmentwhereeachofthe10RLagentsonbothsides matetheregretagainsttheTiZeropolicyonthelevel𝜃 as:
controlsanindividualplayeronthefield.4 Following[29],each
agentsreceivesasobservationa268-dimensionalfeaturevector 𝑅 (cid:158)𝑒𝑔𝑟𝑒𝑡(𝜃,𝜋 𝑇,𝜋 𝑅)=𝑉𝜃 (𝜋 𝑅,𝜋 𝑇)−𝑉𝜃 (𝜋 𝑇,𝜋 𝑇), (1)
includeownplayerinformation,playerIDs,aswellasinformation
abouttheball,playeroftheownandopponentsteams,aswellas
which corresponds to the difference of cross-play and self-play
generalmatchdetails.Theactionspaceofagentsconsistsof19
valuesbetweenthereferenceandtargetpolicies.Theperformance
discreteactions,suchasmovingin8direction,sprinting,passing,
onagivenlevel𝜃 betweentwopolicies𝜋 𝐴 and𝜋 𝐵 isthereward
shooting,etc.
forscoringagoal:
ToapplyMADRIDonGRF,weutiliseprocedurallygenerated
levelseachrepresentedasavectorconsistingof(𝑥,𝑦)coordinates
o sef r2 v0 esp ala sy aer cs o5 na vn end ieth ne tdb ea sll c. rT iph te orp fo os rit li eo vn elo sf inth Ge Rb Fal bl eo cn auth see ifi te al cd
- 𝑉𝜃 (𝜋 𝐴,𝜋
𝐵)= 01 ii ff𝜋 n𝐴 ogs oco ar le is
sscored (2)
c oo nm bm oto hd fiat ee ls dd hiv ale vr ese s.s Tce hn ea reri fo os r, er ,a wng ei un sg efr to hm e𝑥at at nac dk 𝑦in cg ot oo rdd ie nfe an ted sin og
f
 −1 if𝜋 𝐵 scores
3HighlightingthestochasticityoftheGRFenvironment,ashotfromthetopofthe Uponscoringagoalbyeitherofthesides,thelevelterminates.
boxcanleadtovariousoutcomes,underscoringthatnoteveryactionresultsina Giventhenon-deterministicnatureofGRF,weaccountforvari-
predictableoutcome.
4ThegoalkeepersarecontrolledbythegameAI. abilitybycalculatingtheaverageregretacross4repetitionsofthe
5Thegoalkeeperspositionpositionsarealwaysneartheirowngoals. samepairoflevel𝜃 andreferencepolicy𝜋 𝑅.(a)Estimatedregretateachiteration. (b)FinalestimatedregretofTiZerooverreferencepoliciesusingMADRID.
(c)ScoringratevsTiZeroateachiteration. (d)FinalestimatedscoringratevsTiZerooverreferencepoliciesusingMADRID.
Figure4:TheestimatedregretandgoalscorerateagainstTiZeroinGoogleResearchFootball.Illustratedthroughouteach
iterationfor51referenceagents(a)and(c),aswellasfinalvaluesin(b)and(d).Standarderrorover3randomseedsisshown.
5 RESULTSANDDISCUSSION
ThisemphasisesMADRID’scapabilityinexposingadversariallevels
InouranalysisoftargetingTiZeroonGRF,wecloselyexamine whereevenstate-of-the-artpoliciesbepronetomissteps.
the performance of MADRID and baselines. Figure 4a displays
the average estimated regret values for all 160 cells within the
MAP-Elitesgridacrosstheentirecollectionofreferencepolicies.
Here,MADRIDoutperformsbothbaselinemethods.Therandom
baselineexhibitsanegativevaluecloseto0,asTiZeroprovestobe
astrongerpolicythanallthereferencepoliciesonentirelyrandom
gamelevels.Ontheotherhand,thetargetedbaselineperformswell,
closelyresemblingMADRID’sperformanceattheearlystagesof
iterations.However,astheiterationscontinue,itlagsbehinddue
toitsfailuretocapitaliseonpreviouslyidentifiedhigh-regretlevels
thatserveassteppingstonesfornextiterations.
InFigure4b,thevariationinestimatedfinalregretscoresacross
Figure5:MADRID’sestimatedregretoverdifferentreference
thereferencepoliciesisillustrated.Here,theregretincreasesas
policiesaftereachiterationonGRF(meanandstandarderror
wemovetohigher-rankedagents.Theheuristicbotsdisplayregret
over3seeds).
levelsthatareonparwiththeintermediatecheckpointsofTiZero.
Asweapproximatetheregretusingthedifferencebetweencross-
play(XP)andself-play(SP)betweenreferenceandTiZeropolicies Figure4candFigure4dillustratetheestimatedrateofgoals
(seeEquations1and2),aregretestimateof1foranadversarial scoredagainstTiZerobythereferencepoliciesonadversariallevels
level𝜃 canbeachievedintwosituations.First,thereferencepolicy producedbyMADRIDandbaselines.Wecanseethatinapproxi-
scoresagainstTiZeroinXP,whileTiZerocannotscoreinitsSP. mately70%ofthetimeacrossallreferencepolicies,thereference
Second,TiZeroconcedesagoalinSPin𝜃.Intriguingly,ourfindings policyscoredagoalagainstTiZeroinashortperiodoftime.6 It
revealthatforaround90%oftheadversariallevelsgeneratedby
MADRID,anominallyweakerreferencepolicyoutperformsTiZero. 6Thelevelslastonly128environmentsteps,whichisashortepisodecomparedtothe
3000stepsforthefullgame.(a)25iterations. (b)200iterations. (c)1000iterations. (d)5000iterations.
Figure6:TheestimatedregretinMADRID’sarchiveatvariousiterationswithrespecttoTiZero-048referencepolicy.
shouldbenotedthatwithintheremaining30%,themajorityofin- from passing the ball to offside players, resulting in successful
stancesresultedinnogoalsduetothenondeterministicdynamics scoringoutcomes.7
oftheenvironment.
UnforcedOwnGoals. Perhapsthemostglaringadversarialbe-
Figure5highlightsthedifferenceinperformanceforselected
haviourdiscoveredareinstanceswhereTiZeroagentsinexplicably
referencepolicies.Notably,thehigher-rankcheckpointsofTiZero,
shoottowardstheirowngoal,resultinginunforcedowngoals(See
savedatthelaterstagesofitstraining,canbeusedtoidentifymore
Figure8).Incontrast,whenstartingfromidenticalin-gameposi-
severevulnerabilities,asmeasuredusingtheregretestimate.
tions,thereferencepoliciesmanagetocounterattackeffectively,
Figure6showstheevolutionof MADRID’sarchiveforaspe-
oftenresultinginsuccessfulscoringendeavors.
cificreferencepolicy,illustratingitssearchprocessovertime.Ini-
tially,thegridissparselyfilledwithlow-regretlevels.However, Slowrunningopponents. TheTiZeroagentsalwayschooseto
asiterationsprogress,MADRIDgenerateshigh-regretlevelsthat sprintthroughouttheepisode.However,thismakesthemweakon
progressivelypopulatetheentiregrid.ThisshowsthatMADRID defenseagainstopponentswhomoveslowerwiththeball.Instead
candiscoverhigh-regretlevelsanywhereonthefootballfield.On oftryingtotackleandtaketheball,TiZero’smaindefensivestrategy
average,wenoticethathigher-levelscenariostendtobelocated istotryandblockopponents.Opponentscantakeadvantageofthis
towardsthepositive𝑥 coordinates.Thesecorrespondtosituations byusingdeceptivemoves,especiallywhenmovingslowly,making
wheretheballisclosetotheopponent’sgoalfromtheperspective ithardforTiZero’sdefenderstostopthem.Thisisillustratedin
referencepolicy.Whilemostregretscorestendtohaveuniform Figure9.
valuesaroundinsimilarpositionsonthefield,inFigure6dthegrid
SuboptimalBallPositioningforShooting. Whentryingto
alsoincludesanadversariallevelwithestimatedregretof1.75.This
scoreagoal,TiZeroagentsoftenchooseasuboptimalpositioning,
indicatesthatMADRIDfoundalevelwherethereferencepolicy
suchasshootingfromanarrowangle.Incontrast,thereference
scoresagainstTiZeroinXP,whileTiZeroconcedesagoalinSP.
policiesoftenmakesubtleadjustmentstooptimallypositionthe
ballbeforeinitiatingashot(e.g.,movetowardsthecentreofthe
5.1 QualitativeAnalysis goalsFigure10).
Nextweconductaqualitativeanalysisoftheadversariallevelsiden- PassingtoBetterPositionedPlayers. Anotableshortcoming
tifiedbyMADRIDonGRFbyvisualisingthehighestrankinglevels inTiZero’spolicy,whencomparedtothebuilt-inheuristic,isits
inthearchiveacrossallreferencepolicies.Weprovideaselection reluctancetopasstheballtoteammateswhoareinmorefavorable
oftheseexamplesbelow,withacomprehensivelistavailablein positionsandhaveahigherlikelihoodofscoring,asillustrated
AppendixA.Fullvideosofallidentifiedvulnerabilitiescanbefound inFigure11.Incontrast,heuristicbots—whethereasy,medium,
athttps://sites.google.com/view/madrid-marl. orhard—demonstrateaconsistentpatternofpassingtooptimally
positionedplayers,enhancingtheirgoal-scoringopportunities.This
effectivepassingstrategyseemsunfamiliartoTiZero,causingit
Offsides. Despiteitsstrongperformanceunderstandardevalua-
difficultyinovercomingasuccessfuldefense.
tions,TiZerofrequentlyfallsvictimtoerroneouslypassingtheball
toplayersunmistakablyinoffsidepositions,asshowninFigure7 7Aplayerisoffsidewhenitisintheopponents’halfandanypartoftheirbodyis
closertotheopponents’goallinethanboththeballandthesecond-lastopponent.
ThisobservationshighlightsTiZero’slackofadeepunderstanding
Usuallyoneofthetwoopponentsisthegoalkeeper.Whenthishappensafreekickis
oftherulesofthegame.Incontrast,thereferencepoliciesabstain awardedtotheopponent’steam.(a)Initialplayerandballpositionsinthelevel.(b)Thereceivingplayerisclearlyinoffside,(c)Referencepolicydoesnotpasstooffside
TiZeroisabouttopasstheballtoateammate. thusafreekickisawardedtotheopponents playeranddirectlyrunstowardsthegoalto
team. score.
Figure7:Adversarialexampleofoffsides.
Figure8:Adversarialexampleofanowngoal.TiZerogetstrickedandshootsinitsowngoal.
Figure9:Adversarialexampleofaslowrunningopponent.ThreeTiZero-controlleddefendersarenotabletostopasimple
slowrunningopponent,whowalkspastthemandscores.
(a)Initialplayerandballpositionsinthelevel. (b)TiZeropolicyshootsfromanarrowangle (c)Referencepolicygoestoshootfromabetter
andisblockedbythegoalkeeper positionandscores
Figure10:Adversarialexampleofbettershootingpositioning.(a)Initialplayerandballpositionsinthelevel. (b) TiZero policy runs towards the goal and (c)Referencepolicypassestheballtoabetter
shoots,gettingblockedbythegoalkeeper. positionedplayerwhoscores.
Figure11:Adversarialexampleofpassing.
6 RELATEDWORK
Adversarialattacksonmulti-agentpolicies. Deepneuralnet-
QualityDiversity. QualityDiversity(QD)isacategoryofopen- works,suchasimageclassifiers,areknowntobesensitivetoadver-
endedlearningmethodsaimedatdiscoveringacollectionofso- sarialattacks[5,37,45].Suchsusceptibilityhasalsobeendemon-
lutionsthatarebothhighlydiverseandperformant[8,27].Two stratedinmulti-agentRL.[52]attackstheleadingGo-playingAI,
commonlyusedQDalgorithmsareNoveltySearchwithLocalCom- KataGo[54],bytrainingadversarialpoliciesandachieving>97%
petition[NSLC,27]andMAP-Elites[7,32].Thesetwoapproaches win rate against it. Such adversarial agents are not expert Go-
differinthewaytheystructurethearchive;noveltysearchcom- playingbotsatallandareeasilydefeatedbyamateurhumanplayers,
pletelyforgoesagridandoptsinsteadforgrowinganunstructured insteadtheysimplytrickKataGointomakingseriousblunders.Sim-
archivethatdynamicallyexpands,whileMAP-Elitesadoptsastatic ilarly,[46]introduceISMCTS-BR,asearch-baseddeepRLalgorithm
mappingapproach.AlthoughMADRIDleveragesMAP-Elitesas thatlearnsabestresponsetoagivenagent.Bothofthesesolutions
itsdiversitymechanism,itcanbeadaptedtouseNSLC.Oneofthe findexploitabilityusingRLandexpensiveMonte-Carlotreesearch
mosteffectiveversionsofMAP-ElitesisCMA-ME[16].CMA-ME [6],whereasMADRIDisafast,gradient-free,training-freemethod
combinesMAP-Eliteswiththeevolutionaryoptimizationalgorithm thatfindsadversarialsettingsusingQD.Furthermore,unlikethe
CovarianceMatrixAdaptationEvolutionStrategy(CMA-ES)[18], previousmethods,MADRIDisnotrestrictedtoanyconcreteagent
improvingtheselectionofthefittestsolutionswhichwillbeper- architectureandismoregeneralinnature.MAESTRO[39]crafts
turbedtogeneratenewelites.Mix-ME[21]extendsMAP-Elitesto adversarialcurriculafortrainingrobustagentsin2-playersettings
multi-agentdomains,butislimitedtofullycooperativesettings. byjointlysamplingenvironment/co-playerpairs,emphasizingthe
interplaybetweenagentsandenvironments.
7 CONCLUSIONANDFUTUREWORK
Multi-AgentRL. Recentadvancementsinthefieldofcooperative
multi-agent RL [10, 15, 30, 36] have shown remarkable success ThispaperintroducedMulti-AgentDiagnosticsforRobustnessvia
intacklingcomplexchallengesinvideogames,suchasStarCraft IlluminatedDiversity(MADRID),anovelapproachaimedatsys-
II[13,40].GoogleResearchFootball[GRF,25]standsasoneof tematicallyuncoveringsituationswherepre-trainedmulti-agentRL
themostcomplexmulti-agentRLbenchmarks,asatwo-teamzero- agentsdisplaystrategicerrorsincomplexenvironments.MADRID
sumgamewithsparserewardandrequiringsignificantamount leveragesquality-diversitymechanismsandemploystheconcept
of coordination between co-players. Most of the prior work on ofregrettoidentifyandquantifyamultitudeofscenarioswhere
addressesthetoysettingsoftheGRFonlyinvolvedafewagents agentsenactsuboptimalstrategies,withaparticularfocusonthe
(e.g.,3vs1scenario).Multi-AgentPPO[MAPPO,56]usesPPO[42] advancedTiZeroagentwithintheGoogleResearchFootballen-
withacentralisedcritictoplayontoysettings.CDS[28]analyses vironment. Our investigations using MADRID revealed several
theimportanceofdiversitybetweenpoliciesinGRF.Multi-Agent previouslyunnoticedvulnerabilitiesinTiZero’sstrategicdecision-
Transformer[MAT,53]modelsGRFasasequenceproblemusing making,suchasineffectivefinishingandmisunderstandingsofthe
theself-attentionmechanism.TiKick[20]attemptstosolvethefull offsiderule,highlightingthehiddenstrategicinefficienciesand
11vs11gameusingdemonstrationsfromsingle-agenttrajectories. latentvulnerabilitiesineventhemostadvancedRLagents.
SPC[51]usesanadaptivecurriculumonhandcraftedenvironments Lookingforward,weareeagertouseMADRIDinbroadermulti-
forovercomingthesparserewardissueinGRF.TiZeroisthefirst agentdomains,combiningitwithadvancedevolutionaryandlearn-
methodthatclaimstohavemasteredthefull11vs11gameofGRF ingstrategiestoenhanceitsabilitytopinpointstrategicinefficien-
fromscratch[29]following45daysoftrainingwithlargeamount cies.InvestigatingvariousadversarialsituationsandRLmodelswill
ofcomputationalresources.Toachievethis,TiZerousesahand- offerdeeperunderstandingofstrategiccomplexityandlearning
craftedcurriculaoverenvironmentvariations,self-play,augmented inmulti-agentsystems.ThiswillaidincreatingmorerobustRL
observationspace,rewardshaping,andactionmasking.Ofnotable solutions.FutureresearchwithMADRIDshouldaimatoptimising
importancearealsotheworkstacklingthegameoffootballina strategiestoidentifyandcorrectstrategicerrors,furtheringthe
roboticssetting[23,24,38]. advancementofrobustmulti-agentsystems.ACKNOWLEDGEMENTS
https://arxiv.org/abs/2110.04041
[18] NikolausHansenandAndreasOstermeier.2001. CompletelyDerandomized
WearegratefultoShiyuHuangforenablingourexperimentswith
Self-AdaptationinEvolutionStrategies.Evol.Comput.9,2(jun2001),159–195.
theTiZeroagentandprovidingaccesstothecheckpointsofTiZero https://doi.org/10.1162/106365601750190398
savedthroughoutthetraining.ThisworkwasfundedbyMeta. [19] SebastianHöfer,KostasBekris,AnkurHanda,JuanCamiloGamboa,MelissaMoz-
ifian,FlorianGolemo,ChrisAtkeson,DieterFox,KenGoldberg,JohnLeonard,
etal.2021.Sim2Realinroboticsandautomation:Applicationsandchallenges.
REFERENCES IEEEtransactionsonautomationscienceandengineering18,2(2021),398–400.
[20] ShiyuHuang,WenzeChen,LongfeiZhang,ShizhenXu,ZiyangLi,Fengming
[1] Anthropic.2023. IntroducingClaude. https://www.anthropic.com/index/ Zhu,DehengYe,TingChen,andJunZhu.2021.TiKick:towardsplayingmulti-
introducing-claudeAccessedonOct6,2023. agentfootballfullgamesfromsingle-agentdemonstrations. arXivpreprint
[2] ClaudineBadue,RânikGuidolini,RaphaelVivacquaCarneiro,PedroAzevedo, arXiv:2110.04507(2021).
ViniciusBritoCardoso,AvelinoForechi,LuanJesus,RodrigoBerriel,Thiago [21] GarðarIngvarsson,MikayelSamvelyan,BryanLim,ManonFlageat,Antoine
Paixão,FilipeMutz,LucasVeronese,ThiagoOliveira-Santos,andAlbertoFer- Cully,andTimRocktäschel.2023.Mix-ME:Quality-DiversityforMulti-Agent
reiraDeSouza.2019.Self-DrivingCars:ASurvey. arXiv:1901.04407[cs.RO] Learning.arXivpreprintarXiv:2311.01829(2023).
[3] DavidBalduzzi,MartaGarnelo,YoramBachrach,WojciechCzarnecki,Julien [22] MinqiJiang,MichaelDennis,JackParker-Holder,JakobFoerster,EdwardGrefen-
Perolat,MaxJaderberg,andThoreGraepel.2019. Open-endedlearningin stette,andTimRocktäschel.2021. Replay-GuidedAdversarialEnvironment
symmetriczero-sumgames.InProceedingsofthe36thInternationalConference Design.InAdvancesinNeuralInformationProcessingSystems.
onMachineLearning(ProceedingsofMachineLearningResearch,Vol.97),Ka- [23] HiroakiKitano,MinoruAsada,YasuoKuniyoshi,ItsukiNoda,andEiichiOs-
malikaChaudhuriandRuslanSalakhutdinov(Eds.).PMLR,434–443. https: awa.1997.Robocup:Therobotworldcupinitiative.InProceedingsofthefirst
//proceedings.mlr.press/v97/balduzzi19a.html internationalconferenceonAutonomousagents.340–347.
[4] ChristopherBerner,GregBrockman,BrookeChan,VickiCheung,Przemys- [24] HiroakiKitano,MinoruAsada,YasuoKuniyoshi,ItsukiNoda,EiichiOsawa,and
lawDebiak,ChristyDennison,DavidFarhi,QuirinFischer,ShariqHashme, HitoshiMatsubara.1997.RoboCup:AchallengeproblemforAI.AImagazine18,
ChrisHesse,RafalJózefowicz,ScottGray,CatherineOlsson,JakubPachocki, 1(1997),73–73.
MichaelPetrov,HenriquePondédeOliveiraPinto,JonathanRaiman,TimSali- [25] KarolKurach,AntonRaichuk,PiotrStańczyk,MichałZając,OlivierBachem,Lasse
mans,JeremySchlatter,JonasSchneider,SzymonSidor,IlyaSutskever,JieTang, Espeholt,CarlosRiquelme,DamienVincent,MarcinMichalski,OlivierBousquet,
FilipWolski,andSusanZhang.2019.Dota2withLargeScaleDeepReinforcement andSylvainGelly.2020. GoogleResearchFootball:ANovelReinforcement
Learning.CoRRabs/1912.06680(2019).arXiv:1912.06680 LearningEnvironment. arXiv:1907.11180[cs.LG]
[5] NicholasCarlini,AnishAthalye,NicolasPapernot,WielandBrendel,Jonas [26] MarcLanctot,ViniciusZambaldi,AudrunasGruslys,AngelikiLazaridou,Karl
Rauber,DimitrisTsipras,IanGoodfellow,AleksanderMadry,andAlexeyKu- Tuyls,JulienPerolat,DavidSilver,andThoreGraepel.2017.AUnifiedGame-
rakin.2019.Onevaluatingadversarialrobustness.arXivpreprintarXiv:1902.06705 TheoreticApproachtoMultiagentReinforcementLearning. https://arxiv.org/
(2019). abs/1711.00832
[6] RémiCoulom.2007.EfficientSelectivityandBackupOperatorsinMonte-Carlo [27] JoelLehmanandKennethOStanley.2011.Abandoningobjectives:Evolution
TreeSearch.InComputersandGames,H.JaapvandenHerik,PaoloCiancarini, throughthesearchfornoveltyalone. Evolutionarycomputation19,2(2011),
andH.H.L.M.(Jeroen)Donkers(Eds.).SpringerBerlinHeidelberg,Berlin, 189–223.
Heidelberg,72–83. [28] ChenghaoLi,TonghanWang,ChengjieWu,QianchuanZhao,JunYang,and
[7] AntoineCully,JeffClune,DaneshTarapore,andJean-BaptisteMouret.2015. ChongjieZhang.2021.Celebratingdiversityinsharedmulti-agentreinforcement
Robotsthatcanadaptlikeanimals.Nature521(2015),503–507. learning.AdvancesinNeuralInformationProcessingSystems34(2021),3991–4002.
[8] AntoineCullyandYiannisDemiris.2018.QualityandDiversityOptimization:A [29] FanqiLin,ShiyuHuang,TimPearce,WenzeChen,andWei-WeiTu.2023.TiZero:
UnifyingModularFramework.IEEETransactionsonEvolutionaryComputation MasteringMulti-AgentFootballwithCurriculumLearningandSelf-Play.In
22,2(2018),245–259. https://doi.org/10.1109/TEVC.2017.2704781 Proceedingsofthe2023InternationalConferenceonAutonomousAgentsandMulti-
[9] WojciechMCzarnecki,GauthierGidel,BrendanTracey,KarlTuyls,Shayegan agentSystems(London,UnitedKingdom)(AAMAS’23).InternationalFoundation
Omidshafiei,DavidBalduzzi,andMaxJaderberg.2020.Realworldgameslook forAutonomousAgentsandMultiagentSystems,Richland,SC,67–76.
likespinningtops.AdvancesinNeuralInformationProcessingSystems33(2020), [30] AnujMahajan,TabishRashid,MikayelSamvelyan,andShimonWhiteson.2019.
17443–17454. Maven:Multi-agentvariationalexploration. AdvancesinNeuralInformation
[10] ChristianSchroederdeWitt,TarunGupta,DenysMakoviichuk,ViktorMakoviy- ProcessingSystems32(2019).
chuk, Philip H. S. Torr, Mingfei Sun, and Shimon Whiteson. 2020. Is In- [31] IshitaMediratta,MinqiJiang,JackParker-Holder,MichaelDennis,EugeneVinit-
dependentLearningAllYouNeedintheStarCraftMulti-AgentChallenge? sky,andTimRocktäschel.2023.StabilizingUnsupervisedEnvironmentDesign
arXiv:2011.09533[cs.AI] withaLearnedAdversary.InProceedingsofThe2ndConferenceonLifelongLearn-
[11] MichaelDennis,NatashaJaques,EugeneVinitsky,AlexandreBayen,StuartRus- ingAgents(ProceedingsofMachineLearningResearch,Vol.232).PMLR,270–291.
sell,AndrewCritch,andSergeyLevine.2020.EmergentComplexityandZero- [32] Jean-BaptisteMouretandJeffClune.2015.Illuminatingsearchspacesbymapping
shotTransferviaUnsupervisedEnvironmentDesign.InAdvancesinNeural elites. arXiv:1504.04909[cs.AI]
InformationProcessingSystems,Vol.33. [33] OpenAI.2023.GPT-4TechnicalReport. arXiv:2303.08774[cs.CL]
[12] AdrienEcoffet,JoostHuizinga,JoelLehman,KennethO.Stanley,andJeffClune. [34] OpenRL-Lab.2023. TiZero. https://github.com/OpenRL-Lab/TiZero. GitHub
2020. Firstreturn,thenexplore. Nature590(2020),580–586. https://api. repository.
semanticscholar.org/CorpusID:216552951 [35] JackParker-Holder,MinqiJiang,MichaelDennis,MikayelSamvelyan,Jakob
[13] BenjaminEllis,JonathanCook,SkanderMoalla,MikayelSamvelyan,MingfeiSun, Foerster,EdwardGrefenstette,andTimRocktäschel.2022.EvolvingCurricula
AnujMahajan,JakobNicolausFoerster,andShimonWhiteson.2023.SMACv2: withRegret-BasedEnvironmentDesign. https://arxiv.org/abs/2203.01302
AnImprovedBenchmarkforCooperativeMulti-AgentReinforcementLearning. [36] TabishRashid,MikayelSamvelyan,ChristianSchroeder,GregoryFarquhar,Jakob
InThirty-seventhConferenceonNeuralInformationProcessingSystemsDatasets Foerster,andShimonWhiteson.2018.Qmix:Monotonicvaluefunctionfactori-
andBenchmarksTrack. https://openreview.net/forum?id=5OjLGiJW3u sationfordeepmulti-agentreinforcementlearning.InInternationalConference
[14] JakobFoerster,YannisMAssael,NandodeFreitas,andShimonWhiteson.2016. onMachineLearning.PMLR,4295–4304.
Learningtocommunicatewithdeepmulti-agentreinforcementlearning.In [37] KuiRen,TianhangZheng,ZhanQin,andXueLiu.2020.Adversarialattacksand
AdvancesinNeuralInformationProcessingSystems.2137–2145. defensesindeeplearning.Engineering6,3(2020),346–360.
[15] JakobN.Foerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli, [38] MartinRiedmiller,ThomasGabel,RolandHafner,andSaschaLange.2009.Rein-
andShimonWhiteson.2018.CounterfactualMulti-AgentPolicyGradients.In forcementlearningforrobotsoccer.AutonomousRobots27(2009),55–73.
ProceedingsoftheThirty-SecondAAAIConferenceonArtificialIntelligenceand [39] MikayelSamvelyan,AkbirKhan,MichaelDDennis,MinqiJiang,JackParker-
ThirtiethInnovativeApplicationsofArtificialIntelligenceConferenceandEighth Holder,JakobNicolausFoerster,RobertaRaileanu,andTimRocktäschel.2023.
AAAISymposiumonEducationalAdvancesinArtificialIntelligence(NewOrleans, MAESTRO:Open-EndedEnvironmentDesignforMulti-AgentReinforcement
Louisiana,USA)(AAAI’18/IAAI’18/EAAI’18).AAAIPress,Article363,9pages. Learning. In International Conference on Learning Representations. https:
[16] MatthewC.Fontaine,JulianTogelius,StefanosNikolaidis,andAmyK.Hoover. //openreview.net/forum?id=sKWlRDzPfd7
2020. CovarianceMatrixAdaptationfortheRapidIlluminationofBehavior [40] MikayelSamvelyan,TabishRashid,ChristianSchroederdeWitt,GregoryFar-
Space.InProceedingsofthe2020GeneticandEvolutionaryComputationConference quhar,NantasNardelli,TimGJRudner,Chia-ManHung,PhilipHSTorr,Jakob
(Cancún,Mexico)(GECCO’20).AssociationforComputingMachinery,NewYork, Foerster,andShimonWhiteson.2019. TheStarCraftMulti-AgentChallenge.
NY,USA,94–102. https://doi.org/10.1145/3377930.3390232 InProceedingsofthe18thInternationalConferenceonAutonomousAgentsand
[17] MartaGarnelo,WojciechMarianCzarnecki,SiqiLiu,DhruvaTirumala,Junhyuk MultiAgentSystems.2186–2188.
Oh,GauthierGidel,HadovanHasselt,andDavidBalduzzi.2021. PickYour [41] JulianSchrittwieser,IoannisAntonoglou,ThomasHubert,KarenSimonyan,
Battles:InteractionGraphsasPopulation-LevelObjectivesforStrategicDiversity. LaurentSifre,SimonSchmitt,ArthurGuez,EdwardLockhart,DemisHassabis,ThoreGraepel,TimothyLillicrap,andDavidSilver.2020.MasteringAtari,Go, problem.AdvancesinNeuralInformationProcessingSystems35(2022),16509–
chessandshogibyplanningwithalearnedmodel.Nature588,7839(dec2020), 16521.
604–609. [54] David J Wu. 2019. Accelerating self-play learning in go. arXiv preprint
[42] JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. arXiv:1902.10565(2019).
2017.ProximalPolicyOptimizationAlgorithms.ArXivabs/1707.06347(2017). [55] PeterR.Wurman,SamuelBarrett,KentaKawamoto,JamesMacGlashan,Kaushik
[43] L.S.Shapley.1953. StochasticGames. ProceedingsoftheNationalAcademy Subramanian,ThomasJ.Walsh,RobertoCapobianco,AlisaDevlic,FranziskaEck-
ofSciences39,10(1953),1095–1100. https://doi.org/10.1073/pnas.39.10.1095 ert,FlorianFuchs,LeilaniGilpin,PiyushKhandelwal,VarunKompella,HaoChih
arXiv:https://www.pnas.org/doi/pdf/10.1073/pnas.39.10.1095 Lin,PatrickMacAlpine,DeclanOller,TakumaSeno,CraigSherstan,MichaelD.
[44] DavidSilver,AjaHuang,ChrisJ.Maddison,ArthurGuez,LaurentSifre,George Thomure,HoumehrAghabozorgi,LeonBarrett,RoryDouglas,DionWhitehead,
vandenDriessche,JulianSchrittwieser,IoannisAntonoglou,VedavyasPan- PeterDürr,PeterStone,MichaelSpranger,andHiroakiKitano.2022.Outracing
neershelvam,MarcLanctot,SanderDieleman,DominikGrewe,JohnNham, championGranTurismodriverswithdeepreinforcementlearning.Nature602,
NalKalchbrenner,IlyaSutskever,TimothyP.Lillicrap,MadeleineLeach,Koray 7896(Feb.2022),223–228.
Kavukcuoglu,ThoreGraepel,andDemisHassabis.2016.Masteringthegameof [56] ChaoYu,AkashVelu,EugeneVinitsky,JiaxuanGao,YuWang,AlexandreBayen,
Gowithdeepneuralnetworksandtreesearch.Nature529(2016),484–489. andYiWu.2022. TheSurprisingEffectivenessofPPOinCooperativeMulti-
[45] ChristianSzegedy,WojciechZaremba,IlyaSutskever,JoanBruna,DumitruErhan, AgentGames.InThirty-sixthConferenceonNeuralInformationProcessingSystems
IanGoodfellow,andRobFergus.2013.Intriguingpropertiesofneuralnetworks. DatasetsandBenchmarksTrack. https://openreview.net/forum?id=YVXaxB6L2Pl
arXivpreprintarXiv:1312.6199(2013). [57] WenshuaiZhao,JorgePeñaQueralta,andTomiWesterlund.2020.Sim-to-Real
[46] FinbarrTimbers,NolanBard,EdwardLockhart,MarcLanctot,MartinSchmid, TransferinDeepReinforcementLearningforRobotics:aSurvey. 2020IEEE
NeilBurch,JulianSchrittwieser,ThomasHubert,andMichaelBowling.2022. SymposiumSeriesonComputationalIntelligence(SSCI)(2020),737–744. https:
ApproximateExploitability:LearningaBestResponse.InProceedingsoftheThirty- //api.semanticscholar.org/CorpusID:221971078
FirstInternationalJointConferenceonArtificialIntelligence,IJCAI-22,LudDeRaedt
(Ed.).InternationalJointConferencesonArtificialIntelligenceOrganization,
3487–3493. https://doi.org/10.24963/ijcai.2022/484MainTrack.
[47] BryonTjanaka,MatthewCFontaine,DavidHLee,YulunZhang,NiveditReddy
Balam,NathanielDennler,SujaySGarlanka,NikitasDimitriKlapsis,andStefanos
Nikolaidis.2023. Pyribs:ABare-BonesPythonLibraryforQualityDiversity
Optimization.InProceedingsoftheGeneticandEvolutionaryComputationCon-
ference(Lisbon,Portugal)(GECCO’23).AssociationforComputingMachinery,
NewYork,NY,USA,220–229. https://doi.org/10.1145/3583131.3590374
[48] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,Yas-
mineBabaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhos-
ale,DanBikel,LukasBlecher,CristianCantonFerrer,MoyaChen,GuillemCucu-
rull,DavidEsiobu,JudeFernandes,JeremyFu,WenyinFu,BrianFuller,Cynthia
Gao,VedanujGoswami,NamanGoyal,AnthonyHartshorn,SagharHosseini,
RuiHou,HakanInan,MarcinKardas,ViktorKerkez,MadianKhabsa,Isabel
Kloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,Thibaut
Lavril,JenyaLee,DianaLiskovich,YinghaiLu,YuningMao,XavierMartinet,
TodorMihaylov,PushkarMishra,IgorMolybog,YixinNie,AndrewPoulton,
JeremyReizenstein,RashiRungta,KalyanSaladi,AlanSchelten,RuanSilva,
EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,BinhTang,Ross
Taylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,
YuchenZhang,AngelaFan,MelanieKambadur,SharanNarang,AurelienRo-
driguez,RobertStojnic,SergeyEdunov,andThomasScialom.2023. Llama2:
OpenFoundationandFine-TunedChatModels. arXiv:2307.09288[cs.CL]
[49] KarlTuyls,ShayeganOmidshafiei,PaulMuller,ZheWang,JeromeConnor,Daniel
Hennes,IanGraham,WilliamSpearman,TimWaskett,DafyddSteel,PaulineLuc,
AdriaRecasens,AlexandreGalashov,GregoryThornton,RomualdElie,Pablo
Sprechmann,PolMoreno,KrisCao,MartaGarnelo,PraneetDutta,MichalValko,
NicolasHeess,AlexBridgland,JulienPérolat,BartDeVylder,S.M.AliEslami,
MarkRowland,AndrewJaegle,RemiMunos,TrevorBack,RaziaAhamed,Simon
Bouton,NathalieBeauguerlange,JacksonBroshear,ThoreGraepel,andDemis
Hassabis.2021. GamePlan:WhatAICanDoforFootball,andWhatFootball
CanDoforAI.J.Artif.Int.Res.71(sep2021),41–88. https://doi.org/10.1613/jair.
1.12505
[50] OriolVinyals,IgorBabuschkin,WojciechM.Czarnecki,MichaëlMathieu,Andrew
Dudzik,JunyoungChung,DavidH.Choi,RichardPowell,TimoEwalds,Petko
Georgiev,JunhyukOh,DanHorgan,ManuelKroiss,IvoDanihelka,AjaHuang,
LaurentSifre,TrevorCai,JohnP.Agapiou,MaxJaderberg,AlexanderSasha
Vezhnevets,RémiLeblond,TobiasPohlen,ValentinDalibard,DavidBudden,Yury
Sulsky,JamesMolloy,TomL.Paine,ÇaglarGülçehre,ZiyuWang,TobiasPfaff,
YuhuaiWu,RomanRing,DaniYogatama,DarioWünsch,KatrinaMcKinney,
OliverSmith,TomSchaul,TimothyP.Lillicrap,KorayKavukcuoglu,Demis
Hassabis,ChrisApps,andDavidSilver.2019. GrandmasterlevelinStarCraft
IIusingmulti-agentreinforcementlearning. Nat.575,7782(2019),350–354.
https://doi.org/10.1038/s41586-019-1724-z
[51] RundongWang,LongtaoZheng,WeiQiu,BoweiHe,BoAn,ZinoviRabinovich,
YujingHu,YingfengChen,TangjieLv,andChangjieFan.2023.TowardsSkilled
PopulationCurriculumforMulti-AgentReinforcementLearning.arXivpreprint
arXiv:2302.03429(2023).
[52] TonyTongWang,AdamGleave,TomTseng,KellinPelrine,NoraBelrose,Joseph
Miller,MichaelDDennis,YawenDuan,ViktorPogrebniak,SergeyLevine,and
StuartRussell.2023.AdversarialPoliciesBeatSuperhumanGoAIs.InProceedings
ofthe40thInternationalConferenceonMachineLearning(ProceedingsofMachine
LearningResearch,Vol.202),AndreasKrause,EmmaBrunskill,KyunghyunCho,
BarbaraEngelhardt,SivanSabato,andJonathanScarlett(Eds.).PMLR,35655–
35739. https://proceedings.mlr.press/v202/wang23g.html
[53] MuningWen,JakubKuba,RunjiLin,WeinanZhang,YingWen,JunWang,and
YaodongYang.2022.Multi-agentreinforcementlearningisasequencemodelingA ADVERSARIALEXAMPLESFORGOOGLE
ConfusedAgentBehavior. Anotherintriguingadversarialin-
RESEARCHFOOTBALL stance finds TiZero’s ball-possessing player aimlessly sprinting
backandforthinrandomareasofthefield,therebyexhibitinga
Beloware11adversarialexamplesinTiZeroweidentifyingusing
completelyunproductivepatternofmovement(Figure18).
MADRID.
ImprovedDefensivePositioning. TiZeroshowsseveralvul-
Offsides. Despiteitsstrongperformanceunderstandardevalua-
nerabilitiesinitsdefensivestrategies,failingtoclosedownonthe
tions,TiZerofrequentlyfallsvictimtoerroneouslypassingtheball
opponentattackingtrajectoryandallowingthemtoscore.Incom-
toplayersunmistakablyinoffsidepositions,asshowninFigure12
parison,Figure19showsthereferencepoliciesclosingdownonthe
ThisobservationshighlightsTiZero’slackofadeepunderstanding
opponentstrikerandseizingtheballbeforetheyhavethechance
oftherulesofthegame.Incontrast,thereferencepoliciesabstain
toshoot.
from passing the ball to offside players, resulting in successful
scoringoutcomes.8
ErroneousTeamMovement. Severaladversarialexamplesshow
theentiretyofTiZero’steamrunninginthewrongdirectionto
UnforcedOwnGoals. Perhapsthemostglaringadversarialbe-
defendtheirgoal,whiletheballispositionedfavourablytowards
haviourdiscoveredareinstanceswhereTiZeroagentsinexplicably
theopponentsgoal,leavingasolitaryattackingplayerwithoutsup-
shoottowardstheirowngoal,resultinginunforcedowngoals(See
port,whogetsdeceivedandperformspoorly.Thereferencepolicy
Figure13).Incontrast,whenstartingfromidenticalin-gamepo-
insteaddoesn’tgettrickedandoftenmanagestoscoredespitethe
sitions,thereferencepoliciesmanagetocounterattackeffectively,
disarray(Figure20).
oftenresultinginsuccessfulscoringendeavors.
Hesitation Before Shooting. The most common adversarial
Slowrunningopponents. TheTiZeroagentsalwayschooseto
scenarioencounteredbytheheuristicbotsissituationsinwhich
sprintthroughouttheepisode.However,thismakesthemweakon
TiZerohesitatesbeforetakingashot,allowingthegoalkeeperorde-
defenseagainstopponentswhomoveslowerwiththeball.Instead
fendingplayerstoseizetheball.Incontrast,theinbuiltbotpromptly
oftryingtotackleandtaketheball,TiZero’smaindefensivestrategy
recognizestheopportunityandshootswithouthesitation,resulting
istotryandblockopponents.Opponentscantakeadvantageofthis
insuccessfulscoring(Figure21).
byusingdeceptivemoves,especiallywhenmovingslowly,making
ithardforTiZero’sdefenderstostopthem.Thisisillustratedin MissingaGoalScoringOpportunity. TiZerooftenfailstoac-
Figure14. knowledgeeasygoalscoringopportunity,whereitcouldgettothe
ballandscore,butinsteaddecidesnottopursueit.Figure22shows
SuboptimalBallPositioningforShooting. Whentryingto
howthereferencepolicycapitalisesonthiskindofopportunity
scoreagoal,TiZeroagentsoftenchooseasuboptimalpositioning,
andscores.
suchasshootingfromanarrowangle.Incontrast,thereference
policiesoftenmakesubtleadjustmentstooptimallypositionthe B ENVIRONMENTDETAILS
ballbeforeinitiatingashot(e.g.,movetowardsthecentreofthe
goalsFigure15). InourexperimentswithGoogleResearchFootball[25],weadopta
proceduralgenerationmethodforlevelcreation.Foreachplayer,
PassingtoBetterPositionedPlayers. Anotableshortcoming as well as the ball, we randomly sample the (𝑥,𝑦) coordinates:
inTiZero’spolicy,whencomparedtothebuilt-inheuristic,isits thex-coordinateissampledfromtherange[−0.9,0.9]andthey-
reluctancetopasstheballtoteammateswhoareinmorefavorable coordinatefromtherange[−0.4,0.4].Thesettingsemployedduring
positionsandhaveahigherlikelihoodofscoring,asillustrated thegenerationareasfollows:
inFigure16.Incontrast,heuristicbots—whethereasy,medium,
• deterministic:settoFalse,implyingthatlevelscanhave
orhard—demonstrateaconsistentpatternofpassingtooptimally
non-deterministiccomponents.
positionedplayers,enhancingtheirgoal-scoringopportunities.This
• offsides: set to True, enforcing the offsides rule during
effectivepassingstrategyseemsunfamiliartoTiZero,causingit
gameplay.
difficultyinovercomingasuccessfuldefense.
• end_episode_on_score:settoTrue,whichmeanstheepisode
willterminateonceagoalisscored.
ShootingwhileRunning. Capitalizingonanothergameme-
• end_episode_on_out_of_play:settoFalse,indicatingthe
chanics,thereferencepoliciesexhibitstrongerbehavioursbyhalt-
episodewillnotendonballout-of-playevents.
ingtheirsprintingbehaviourleadinguptoashot,resultingina
• end_episode_on_possession_change:settoFalse,indi-
notably higher success rate in goal realisation. TiZero’s agents,
catingtheepisodewillnotendwhentheballchangespos-
incontrast,consistentlymaintainasprintingstance,therebyfre-
sessionfromoneteamtoanother.
quentlymissingstraightforwardscoringopportunitiesinfrontof
theopposinggoalkeepers(Figure17). Fortheeasybot,thedifficultyissetat0.05.Forthemediumbot,
itissetto0.5,andforthehardbot,thedifficultyisat0.95.These
valuesserveasthedefaultsinGRF,ensuringconsistencyacross
8Aplayerisoffsidewhenitisintheopponents’halfandanypartoftheirbodyis differentgamescenarios
closertotheopponents’goallinethanboththeballandthesecond-lastopponent.
WeusetheenhancedobservationspaceasdescribedinTiZero[29],
Usuallyoneofthetwoopponentsisthegoalkeeper.Whenthishappensafreekickis
awardedtotheopponent’steam. consistingof268-dimensionalvectorincludinginformation.(a)Initialplayerandballpositionsinthelevel.(b)Thereceivingplayerisclearlyinoffside,(c)Referencepolicydoesnotpasstooffside
TiZeroisabouttopasstheballtoateammate. thusafreekickisawardedtotheopponents playeranddirectlyrunstowardsthegoalto
team. score.
Figure12:Adversarialexampleofoffsides.
Figure13:Adversarialexampleofanowngoal.TiZerogetstrickedandshootsinitsowngoal.
Figure14:Adversarialexampleofaslowrunningopponent.ThreeTiZero-controlleddefendersarenotabletostopasimple
slowrunningopponentcontrolledbythereferencepolicy,whowalkspastthemandscores.
(a)Initialplayerandballpositionsinthelevel. (b) TiZero shoots from a narrow angle is (c)Referencepolicygoestoshootfromabetter
blockedbythegoalkeeper positionandscores
Figure15:Adversarialexampleofbettershootingpositioning.(a)Initialplayerandballpositionsinthelevel. (b)TiZerorunstowardsthegoalandshoots,(c)Referencepolicypassestheballtoabetter
gettingblockedbythegoalkeeper. positionedplayerwhoscores.
Figure16:Adversarialexampleofpassing.
(a)Initialplayerandballpositionsinthelevel. (b)TiZeroshootswhilesprintingandtheball (c)Referencepolicydoesn’trunandisableto
getsblockedbythegoalkeeper. score.
Figure17:Adversarialexampleofshootingwhilerunning.
(a)Initialplayerandballpositionsinthelevel. (b)TiZeroaimlesslyrunsupanddownfrom (c)Referencepolicyattackstheopponentgoal,
thesamepositioninanendlessloop. oftenresultingingoalscoringendeavours.
Figure18:Adversarialexampleofconfusedbehaviour.
(a)Initialplayerandballpositionsinthelevel. (b)TiZero’sdefenderrunsalongasuboptimal (c)Referencepolicyinsteadrunstowardsthe
trajectory, giving space for the opponent to attackertoblocktheattempt.
shootandscore.
Figure19:Adversarialexampleofbetterdefensivebehaviour.(a)Initialplayerandballpositionsinthelevel. (b)TiZero’steamrunsbackwards,leavingasoli-(c)Referencepolicyinsteaddoesn’tgettricked,
taryattackerconfusedandunabletoscore. theattackermovesinabetterpositiontoscore.
Figure20:Adversarialexampleoferroneousteammovement.
(a)Initialplayerandballpositionsinthelevel. (b) TiZero hesitates before shooting, giving (c)Referencepolicyinsteadshootswithouthes-
enoughtimeforthegoalkeepertoseizethe itationandscores.
ball
Figure21:Adversarialexampleofhesitationbeforeshooting.
(a)Initialplayerandballpositionsinthelevel. (b)TiZero’sattackerdoesnotrealiseitcanget (c)Referencepolicyinsteadrunstowardsthe
totheballbeforethegoalkeeper,andrunsback-ball,reachingitbeforethegoalkeeperdoesand
wards. scoring.
Figure22:Adversarialexampleofmissingagoalscoringopportunity.
C IMPLEMENTATIONDETAILS
anLSTMlayertogivetheagentmemory,withthehiddensizefor
Hyperparameters of MADRID are provided in Table 1. We use thislayerbeing256.Everyhiddenlayerisequippedwithlayernor-
theCMA-MEasimplementedinpyribs[47].FortheTiZeroand malizationandReLUnon-linearities.Theorthogonalmatrixisused
reference agents, we use the exact agent architecture as in the forinitializingparameters,andthelearningprocessisoptimized
originalpaper[29]usingTiZero’sofficialopen-sourcerelease[34]. withtheAdamoptimizer.Similartotheoriginalimplementation,
Parametersharingisappliedtoallagentsintheteam. illegalactionsaremaskedoutbymakingtheirselectionprobability
Thepolicynetworkismadeupofsixdifferentmulti-layerper- zero.Theactionoutputlayerutilizesasoftmaxlayerandisformed
ceptrons(MLPs),eachhavingtwofully-connectedlayers,including witha19-dimensionvector.
onespecificallyforthe’playerID’,toencodeeverypartoftheob- Experimentsareconductedonanin-housecluster.Everytask,
servationindividually.TheMLPlayershaveahiddensizeof64.The denotedbyaseed,usesoneTeslaV100GPUand10CPUs.Foreach
hiddenfeaturesextractedarebroughttogetherandthenhandledby ofthe51referencepolicies(48TiZerocheckpointsand3built-inTable1:Hyperparametersusedforfindingadversarialexam-
bots),weuse3randomseeds,foreachofthebaselines.Runslast
plesinGoogleResearchFootball. approximately8.5daysfor5000iterationsof MADRID.
Parameter
Numberofsteps 5000
Gameduration 128
NumberofCMA-MEemitters 4
Numberofrepeatsperlevel 4
Emittergaussiannoise𝜎 0.1
Ranker improvement
QDscoreoffset -2