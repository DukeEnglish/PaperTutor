Modular Adaptation of Multilingual Encoders to
Written Swiss German Dialect
JannisVamvas NoëmiAepli RicoSennrich
DepartmentofComputationalLinguistics,UniversityofZurich
{vamvas,naepli,sennrich}@cl.uzh.ch
Abstract
Monolithic Modular
CreatingneuraltextencodersforwrittenSwiss
Germanischallengingduetoadearthoftrain- XLM-R→ X-MOD/SwissBERT→
ingdatacombinedwithdialectalvariation. In SwissGermanXLM-R SwissGermanadapter
thispaper,webuildonseveralexistingmultilin-
gualencodersandadaptthemtoSwissGerman
X-MOD/SwissBERT→
using continued pre-training. Evaluation on CANINE→
SwissGerman
threediversedownstreamtasksshowsthatsim- SwissGermanCANINE
character-leveladapter
plyaddingaSwissGermanadaptertoamodu-
larencoderachieves97.5%offullymonolithic
adaptationperformance. Wefurtherfindthat Table1: Overviewoftheencodermodelswerelease.
for the task of retrieving Swiss German sen-
tencesgivenStandardGermanqueries,adapt-
ing a character-level model is more effective improvement of 10%–45% in average accuracy
thantheotheradaptationstrategies. Werelease across the three downstream tasks. We then fo-
ourcodeandthemodelstrainedforourexperi- cus on comparing monolithic adaptation, where
ments.1
alltheparametersoftheencoderareupdateddur-
ingcontinuedpre-training,tomodularadaptation
1 Introduction
withlanguage-specificmodularcomponents(lan-
Whenapplyingnaturallanguageprocessing(NLP) guageadapters;Pfeifferetal.,2022). Eventhough
techniques to languages with dialectal variation, modularadaptationonlyupdatesafractionofthe
two typical challenges are a lack of public train- parameters, it is competitive to monolithic adap-
ing data as well as varying spelling conventions. tation. Given these findings, we propose to ex-
InthecaseofSwissGerman,whichisspokenby tendtheSwissBERTmodel(Vamvasetal.,2023),
around5millionpeopleandisoftenusedforinfor- whichwastrainedonStandardGermanandother
malwrittencommunicationinSwitzerland,these languages,withaSwissGermanadapter(Table1).
factorsmakeitmorechallengingtotrainaBERT- Wefurtherhypothesizethatthearchitectureof
liketextencoderforwrittentext. CANINE (Clark et al., 2022), a tokenization-free
Inthispaper,weadaptpre-trainedmultilingual modelthatoperatesoncharacters,mightbebetter
encoders to Swiss German using continued pre- suitedtothehighlyvariablespellingofSwissGer-
training on a modest amount of Swiss German man. Indeed, a CANINE model adapted to Swiss
training data. We evaluate the approaches on German excels on the retrieval tasks, while POS
part-of-speech(POS)taggingwithzero-shotcross- taggingworksbetterwithsubwords.
lingual transfer from Standard German (Aepli Finally, we aim to combine the best of both
and Sennrich, 2022), as well as dialect identifi- worlds by integrating character-level down- and
cation (Zampieri et al., 2019) and cross-lingual upsamplingmodulesintoasubword-basedmodel
sentence retrieval based on a parallel Standard and training a character-level adapter for Swiss
German–SwissGermantestset(Aeplietal.,2023). German. However, this jointly modular and
Wefindthatdependingonthemultilingualen- tokenization-freestrategyunderperformstheindi-
coder, continued pre-training leads to an average vidualapproaches. Wehopethatourfindingscan
informthedevelopmentofmodularapproachesfor
1https://github.com/ZurichNLP/
swiss-german-text-encoders otherlanguageswithdialectalvariation.
4202
naJ
52
]LC.sc[
1v00441.1042:viXra
sdrowbuS
sretcarahC2 AdaptationScenario sequences and does not require a tokenizer at in-
ference time. This is achieved by extending the
Our goal is to train an encoder model for Swiss
standard transformer architecture with character
German(languagecodegsw)withlimitedtraining
down-andupsamplingmodules.
data. SinceStandardGerman(languagecodede)
The downsampling module combines a single-
isacloselyrelatedlanguage,wefocusontransfer
layerblockwisetransformerwithstridedconvolu-
learningfromStandardGermantoSwissGerman.
tion,whichreducesthesequencelengthbyafactor
We rely on pre-trained multilingual models that
ofr = 4, wherer isahyperparameter. Asacon-
have already been trained on Standard German,
sequence, the standard transformer does not see
andadaptthemtoSwissGermanusingcontinued
every character individually, but only sees down-
pre-training.
sampledpositions. The upsamplingmodule,which
isneededfortoken-leveltasks,mirrorsthedown-
SwissGermanadaptationdata Fortrainingon
sampling procedure and restores the original se-
SwissGerman,weusetheSwissCrawlcorpus(Lin-
quencelength. WerefertoClarketal.(2022)fora
der et al., 2020), which contains 11M tokens of
detaileddescriptionofthearchitecture.
Swiss German text extracted from the web. The
Clark et al. (2022) describe two alternative ap-
text in SwissCrawl exhibits some normalizations
thateventualinputtextwillnothave,e.g.,isolation
proachesforpre-training: CANINE-S,whichuses
atokenizertodeterminemaskedtokensandissim-
ofindividualsentences,normalizationofpunctua-
tionandemojiremoval. Todiversifythetraining
ilartostandardMLM,andCANINE-C,whichisan
autoregressivecharacterloss. Inourexperiments,
data,weextendthepre-trainingdatasetwithacus-
tom collection of 382k Swiss German tweets. In
we use CANINE-S with the SwissBERT subword
tokenizertoperformcontinuedpre-training.
total,weuse18Mtokensforpre-trainingonSwiss
German. Bothdatasetswereautomaticallymined
4 ModularApproaches
andmaycontainsometextinotherlanguages.
4.1 SwissBERT
Standard German data To promote transfer
fromStandardGermantoSwissGermanlateron, We base our adapter experiments on
weincludeanequalpartofStandardGermandata SwissBERT (Vamvas et al., 2023), a variant
inthecontinuedpre-trainingdata. Weuseasample of X-MOD (Pfeiffer et al., 2022) that includes
ofnewsarticlesretrievedfromtheSwissdox@LiRI language adapters for Standard German, French,
database, comparable to the data the SwissBERT Italian and Romansh. Compared to the original
modelhasbeentrainedon(Vamvasetal.,2023). X-MODmodel,whichwastrainedwithlanguage
adapters for 81 languages, SwissBERT has a
3 MonolithicApproaches custom SentencePiece vocabulary and word
embeddingsoptimizedforSwitzerland-relatedtext,
We evaluate a subword-based model and a
andweassumethatthisisbeneficialforcontinued
character-basedmodel,withandwithoutcontinued
pre-trainingonSwissGerman.
pre-trainingonSwissGerman. Wecallthesemod-
els monolithic (non-modular), because the entire 4.2 Subword-levelAdapterforSwissBERT
modelisupdatedduringcontinuedpre-training.
We add a Swiss German adapter to SwissBERT
andfreezetheparametersofthemodelexceptfor
3.1 XLM-R
theadaptermodulesduringcontinuedpre-training.
We train XLM-R (Conneau et al., 2020) with We initialize the Swiss German adapter with the
maskedlanguagemodeling(MLM).XLM-Rwas weightsoftheStandardGermanadapterandpre-
pre-trainedon100languages,whichincludeStan- train it on the Swiss German part of our dataset.
dardGermanbutnotSwissGerman. Duringfine-tuningondownstreamtasks,wefreeze
theadaptersandupdatetheremainderofthemodel.
3.2 CANINE
For this approach, we only use the Swiss Ger-
The CANINE model (Clark et al., 2022) was pre- manpartofourpre-trainingcorpusforcontinued
trained on 104 languages, again including Stan- pre-training, andnotStandardGerman, sincethe
dard German but excluding Swiss German. Un- modulararchitectureisexpectedtoallowforcross-
like XLM-R, CANINE directly encodes character lingual transfer without continued pre-trainingPOS GDI Retrieval Macro-Avg.
GSW-BE GSW-ZH
XLM-R:
–withoutcontinuedpre-training 52.6±1.8 47.2±15.1 60.6 75.7 56.0
–withcontinuedpre-training 86.9±0.3 62.1±0.8 91.1 96.0 80.9
CANINE:
–withoutcontinuedpre-training 46.7±1.3 59.0±0.6 92.8 94.8 66.5
–withcontinuedpre-training 60.9±1.4 60.8±0.4 96.4 96.9 72.8
SwissBERT:
–DEadapterwithoutcontinuedpre-training 64.8±2.0 61.3±0.5 66.1 82.2 66.7
–subword-levelGSWadapter 83.2±0.3 62.0±0.4 82.9 92.4 77.6
–character-levelGSW adapter 41.5±0.9 51.9±1.3 35.6 42.6 44.2
Table 2: Comparison of different models on three downstream tasks: part-of-speech (POS) tagging accuracy,
Germandialectidentification(GDI)F1-score,andcross-lingualsentenceretrievalaccuracy. Forthesupervised
tasks,wereporttheaverageandstandarddeviationacross5fine-tuningruns. Underlinedresultsindicatethebest
performanceforatask.
on the source language. Table A4 provides an theremainder,analogoustothesubword-levelex-
overviewofthelanguagesusedforeachapproach. periment.
4.3 Character-levelAdapterforSwissBERT 5 Evaluation
Previous work has found that learning a custom
5.1 Part-of-SpeechTagging(POS)
subword segmentation and embeddings that are
FollowingAepliandSennrich(2022),weevaluate
adapted to the vocabulary of the target language
ourmodelsonPOStaggingwithzero-shotcross-
canimproveperformance(Wangetal.,2019;Pfeif-
lingual transfer from Standard German. To train
feretal.,2021;Vamvasetal.,2023). However,this
themodels,weusetheGermanHDTUniversalDe-
limitsthedegreeofmodularity,andwethusinvesti-
pendenciesTreebank(BorgesVölkeretal.,2019)
gateatokenization-freeapproachasanalternative.
and test on a dataset introduced by Hollenstein
Inthisexperiment,wediscardSwissBERT’ssub-
and Aepli (2014). We report accuracy across the
wordembeddingswhentrainingtheSwissGerman
54STTStags(Schilleretal.,1999).3 Werelyon
adapter, and instead add the downsampling and
upsamplingmodulesoftheCANINEarchitecture.2 theprovidedwordsegmentationandlabelthefirst
token(subword/character/byte)ofeachword.
Addingthesemodulesresultsinexactlythesame
architectureasCANINE,exceptthatweoptforbyte
5.2 GermanDialectIdentification(GDI)
embeddingsinsteadofcharacterhashembeddings.
CANINE uses a hash embedding method that can The GDI task (Zampieri et al., 2019) is based
mapanyUnicodecodepointtoafixed-sizeembed- on transcripts of the ArchiMob corpus of spo-
ding. SinceStandardGermanandSwissGerman kenSwissGerman(Samardžic´ etal.,2016). This
aremainlywritteninLatinscriptandtherearelim- datasetcontainsfourdialects,namely,Bern,Basel,
ited training data, we forgo the hash embedding Lucerne,andZurichregions,constitutingfourdis-
andlearnUTF-8byteembeddingsinstead. tinctclasses. WereporttheweightedF1-score.
UsingtheCANINE-Sobjective,wefirstpre-train
5.3 SentenceRetrieval
the character modules on Standard German pre-
training data. We then continue pre-training the For evaluating cross-lingual sentence retrieval,
adapters and the joint character modules on both we use human translations of the English
languages, while freezing the rest of the model. newstest2019 source dataset (Barrault et al.,
Duringfine-tuning,wefreezetheadaptersandtrain 2019) into different languages. Translations into
2WetermthisapproachGLOBI(GranularLocalizationof 3WemasktheAPPRARTgoldtag,whichisnotincludedin
BidirectionalEncoders). thetrainingtagset,whencalculatingaccuracy.POS GDI Retrieval Macro-Avg.
GSW-BE GSW-ZH
SwissBERTsubword-levelGSW adapter:
–onlyupdatingtheadapterweights 83.2±0.3 62.0±0.4 82.9 92.4 77.6(97.5%)
–alsoupdatingthewordembeddings 83.9±0.1 62.1±0.3 86.0 93.7 78.6(98.7%)
–updatingalltheweights 85.7±0.3 63.1±0.3 86.6 93.4 79.6 (100%)
Table3: Effectofmodularityoncontinuedpre-training: Onlyupdatingtheadapterweightsduringcontinuedpre-
trainingachieves97.5%oftheaccuracyofamonolithicbaselinewhereweupdatealltheparametersofSwissBERT.
StandardGermanareprovidedbyNTREX-128(Fe- reportaverageandstandarddeviationacross5fine-
dermannetal.,2022);translationsintoSwissGer- tuningrunswithdifferentrandomseeds.
man are provided by Aepli et al. (2023) for two
regions,Bern(gsw-be)andZurich(gsw-zh). 7 Results
ForbothSwissGermantestsets,wereportthe
top-1 accuracy of retrieving the correct transla- Table2presentsacomparisonofthedifferentmod-
tionamongall1,997translations, giventheStan- elsonthethreedownstreamtasks. Continuedpre-
dard German equivalent. Note that 100% accu- trainingishighlybeneficialforwrittenSwissGer-
racy is not attainable, since newstest2019 has a man,confirmingpreviouswork(Mulleretal.,2021;
small number of duplicate or near-duplicate sen- AepliandSennrich,2022;Aeplietal.,2023). This
tences. Following an evaluation approach used finding extends to the CANINE model, for which
forSwissBERT(Vamvasetal.,2023),weperform language-adaptivepre-traininghasnotbeentested
unsupervised retrieval with the BERTScore met- before,toourknowledge.
ric (Zhang et al., 2020). We average the hidden Theadapted CANINE showsstate-of-the-artper-
statesacrossallencoderlayers. Inthecaseofthe formance on the retrieval tasks. A simple ChrF
CANINE-stylemodels,weuseonlythetransformer baseline(Popovic´,2015)achievesonly90.9%and
layersthatrepresentthedownsampledpositions. 93.0%accuracyonthetworetrievaltasks,andboth
the original and the adapted CANINE clearly sur-
6 ExperimentalSetup pass this baseline. However, the CANINE model
has low accuracy on POS tagging, reflecting pre-
Continuedpre-training WecombineSwissGer-
viousfindingsfornamedentityrecognition(Clark
manandStandardGermantrainingdatawitha1:1
etal.,2022). Futureworkcouldexplorealternative
ratio. Theresultingbilingualdatasetcontains37M
strategiesfortoken-levelclassificationtasks.
tokens in total, and we set aside 5% for valida-
While the monolithic XLM-R model performs
tion (Table A6). We set the learning rate to 1e-4
bestoverall,weconsideraddingasubword-based
and select the best checkpoint based on the val-
Swiss German adapter to SwissBERT a competi-
idation loss out of 10 epochs; otherwise we use
tive alternative, with the number of trainable pa-
thedefaultsettingsofHuggingFacetransformer’s
rametersreducedby95%(seeTableA1foracom-
MLM example script. We train the models on a
parisonofthemodelsizes). Table3confirmsthat
NvidiaV100GPUwith32GBofmemoryandad-
restrictingthecontinuedpre-trainingtotheadapter
justthebatchsizedynamicallytofittheavailable
weightsconservesmostoftheaccuracy,compared
memory. Withthesubword-basedmodels,weset
toupdatingalltheparametersofSwissBERT.
thesequencelengthto512. WiththeCANINE-style
Finally, a character-level adapter, where char-
models,weusethedefaultdownsamplingrateof
acter up- and downsampling modules are added
r = 4 and a sequence length of r ×512 = 2048
to the model specifically for Swiss German, per-
tokens(charactersorbytes).
forms better than random but clearly worse than
Fine-tuning For the downstream tasks that in- thestandardapproaches. Thisindicatesthatwhile
volvefine-tuning(POSandGDI),wefine-tunethe thetransformerlayersofasubword-basedmodel
modelwithalearningrateof2e-5andabatchsize bearsomesimilaritytothedownsampledpositions
of 16. We train for 10 epochs and select the best intheCANINEarchitecture,continuedpre-training
checkpointbasedonthevalidationaccuracy. We cannotcompletelybridgethegapbetweenthetwoarchitectures. Futureworkcouldpre-trainamod- References
ularcharacter-levelmodelfromscratchtofurther
Noëmi Aepli, Chantal Amrhein, Florian Schottmann,
improveadaptabilitytonewlanguagesanddialects, andRicoSennrich.2023. Abenchmarkforevaluat-
whiletakingintoaccountmorerecentfindingsre- ingmachinetranslationmetricsondialectswithout
gardingtheoptimaldesignofcharacter-levelmod- standardorthography. InProceedingsoftheEighth
Conference on Machine Translation, pages 1045–
ulesfortextencoding(Tayetal.,2022;Cao,2023).
1065,Singapore.AssociationforComputationalLin-
guistics.
8 Conclusion
NoëmiAepliandRicoSennrich.2022. Improvingzero-
Wecomparedstrategiesforadaptingmultilingual
shot cross-lingual transfer between closely related
encoders to Swiss German. We found that the languagesbyinjectingcharacter-levelnoise. InFind-
monolithic approach of continued pre-training ingsoftheAssociationforComputationalLinguis-
tics: ACL2022,pages4074–4083,Dublin,Ireland.
XLM-Risastrongbaseline. AddingaSwissGer-
AssociationforComputationalLinguistics.
manadaptertoSwissBERT,amodelwithamod-
ular architecture, is a viable alternative. Finally, Loïc Barrault, Ondˇrej Bojar, Marta R. Costa-jussà,
adaptingCANINEonSwissGermanworkswellfor Christian Federmann, Mark Fishel, Yvette Gra-
ham,BarryHaddow,MatthiasHuck,PhilippKoehn,
cross-lingualretrieval. ThefourSwissGermanen-
Shervin Malmasi, Christof Monz, Mathias Müller,
codermodelswetrainedforourexperimentswill
SantanuPal,MattPost,andMarcosZampieri.2019.
bemadeavailabletotheresearchcommunity. Findingsofthe2019conferenceonmachinetrans-
lation(WMT19). InProceedingsoftheFourthCon-
Limitations ferenceonMachineTranslation(Volume2: Shared
TaskPapers,Day1),pages1–61,Florence,Italy.As-
Differencesbetweenthepre-trainedmodelsmake sociationforComputationalLinguistics.
afaircomparisonmoredifficult. Theencodermod-
EmanuelBorgesVölker,MaximilianWendt,FelixHen-
els we compare have originally been pre-trained
nig,andArneKöhn.2019. HDT-UD:Averylarge
withdifferentdataandhyperparameters(butnever Universal Dependencies treebank for German. In
onSwissGerman). Theyalsodifferintheirnum- ProceedingsoftheThirdWorkshoponUniversalDe-
berofparametersandvocabularysizes,asdetailed
pendencies(UDW,SyntaxFest2019),pages46–57,
Paris, France. Association for Computational Lin-
in Table A1. Furthermore, we use a single, stan-
guistics.
dard set of hyperparameters for pre-training and
forevaluation,respectively. Optimizingthesehy- KrisCao.2023. Whatisthebestrecipeforcharacter-
levelencoder-onlymodelling? InProceedingsofthe
perparameters for each model individually could
61stAnnualMeetingoftheAssociationforCompu-
leadtofurtherimprovements.
tationalLinguistics(Volume1: LongPapers),pages
Finally, the evaluation results show that it is 5924–5938,Toronto,Canada.AssociationforCom-
challenging to perform GDI classification purely putationalLinguistics.
based on written text, as previously discussed
JonathanH.Clark,DanGarrette,IuliaTurc,andJohn
by Zampieri et al. (2017). In interpreting the re-
Wieting. 2022. Canine: Pre-training an efficient
sults,wefocusmainlyontheothertwotasks,but tokenization-free encoder for language representa-
still report results for GDI to provide a complete tion. TransactionsoftheAssociationforComputa-
tionalLinguistics,10:73–91.
picture.
AlexisConneau,KartikayKhandelwal,NamanGoyal,
Acknowledgements
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
ThisworkwasfundedbytheSwissNationalSci-
moyer,andVeselinStoyanov.2020. Unsupervised
enceFoundation(projectnos.213976and191934). cross-lingualrepresentationlearningatscale. InPro-
WethankStefanLangerforhelpfuladviceoncol- ceedings of the 58th Annual Meeting of the Asso-
lectingtheSwissGermantweetdataset,andChan- ciationforComputationalLinguistics,pages8440–
8451, Online. Association for Computational Lin-
talAmrheinfortheprovisionoftestdata. Forthis
guistics.
publication, use was made of media data made
availableviaSwissdox@LiRIbytheLinguisticRe- ChristianFedermann,TomKocmi,andYingXin.2022.
search Infrastructure of the University of Zurich NTREX-128–newstestreferencesforMTevalua-
tion of 128 languages. In Proceedings of the First
(see https://t.uzh.ch/1hI for more informa-
Workshop on Scaling Up Multilingual Evaluation,
tion).
pages21–24,Online.AssociationforComputational
Linguistics.NoraHollensteinandNoëmiAepli.2014. Compilation AnneSchiller,SimoneTeufel,ChristineStöckert,and
ofaSwissGermandialectcorpusanditsapplication ChristineThielen.1999. GuidelinesfürdasTagging
toPoStagging. InProceedingsoftheFirstWorkshop deutscherTextkorporamitSTTS.
onApplyingNLPToolstoSimilarLanguages,Vari-
etiesandDialects,pages85–94,Dublin,Ireland.As- Yi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Gupta,
sociationforComputationalLinguisticsandDublin HyungWonChung, DaraBahri, ZhenQin, Simon
CityUniversity. Baumgartner,CongYu,andDonaldMetzler.2022.
Charformer: Fastcharactertransformersviagradient-
TakuKudoandJohnRichardson.2018. SentencePiece: basedsubwordtokenization. InInternationalCon-
A simple and language independent subword tok- ferenceonLearningRepresentations.
enizeranddetokenizerforneuraltextprocessing. In
Proceedings of the 2018 Conference on Empirical Jannis Vamvas, Johannes Graën, and Rico Sennrich.
Methods in Natural Language Processing: System 2023. SwissBERT:Themultilinguallanguagemodel
Demonstrations, pages 66–71, Brussels, Belgium. for Switzerland. In Proceedings of the 8th edition
AssociationforComputationalLinguistics. oftheSwissTextAnalyticsConference,pages54–69,
Neuchatel, Switzerland. Association for Computa-
Lucy Linder, Michael Jungo, Jean Hennebert, tionalLinguistics.
ClaudiuCristianMusat,andAndreasFischer.2020.
HaiWang,DianYu,KaiSun,JianshuChen,andDong
Automaticcreationoftextcorporaforlow-resource
Yu.2019. Improvingpre-trainedmultilingualmodel
languagesfromtheInternet: ThecaseofSwissGer-
man. In Proceedings of the Twelfth Language Re- with vocabulary expansion. In Proceedings of the
23rd Conference on Computational Natural Lan-
sources and Evaluation Conference, pages 2706–
guage Learning (CoNLL), pages 316–327, Hong
2711, Marseille, France. European Language Re-
Kong, China. Association for Computational Lin-
sourcesAssociation.
guistics.
Benjamin Muller, Antonios Anastasopoulos, Benoît
MarcosZampieri,ShervinMalmasi,NikolaLjubešic´,
Sagot, and Djamé Seddah. 2021. When being un-
PreslavNakov,AhmedAli,JörgTiedemann,Yves
seenfrommBERTisjustthebeginning: Handling
Scherrer, and Noëmi Aepli. 2017. Findings of the
new languages with multilingual language models.
VarDialevaluationcampaign2017. InProceedings
InProceedingsofthe2021ConferenceoftheNorth
of the Fourth Workshop on NLP for Similar Lan-
AmericanChapteroftheAssociationforComputa-
guages,VarietiesandDialects(VarDial),pages1–15,
tionalLinguistics: HumanLanguageTechnologies,
Valencia,Spain.AssociationforComputationalLin-
pages 448–462, Online. Association for Computa-
guistics.
tionalLinguistics.
Marcos Zampieri, Shervin Malmasi, Yves Scherrer,
JonasPfeiffer,NamanGoyal,XiLin,XianLi,James
TanjaSamardžic´,FrancisTyers,MiikkaSilfverberg,
Cross, Sebastian Riedel, and Mikel Artetxe. 2022.
Natalia Klyueva, Tung-Le Pan, Chu-Ren Huang,
Lifting the curse of multilinguality by pre-training
RaduTudorIonescu,AndreiM.Butnaru,andTommi
modulartransformers. InProceedingsofthe2022
Jauhiainen.2019. AreportonthethirdVarDialeval-
Conference of the North American Chapter of the
uationcampaign. InProceedingsoftheSixthWork-
AssociationforComputationalLinguistics: Human
shoponNLPforSimilarLanguages,Varietiesand
LanguageTechnologies,pages3479–3495,Seattle,
Dialects,pages1–16,AnnArbor,Michigan.Associ-
United States. Association for Computational Lin-
ationforComputationalLinguistics.
guistics.
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
JonasPfeiffer,IvanVulic´,IrynaGurevych,andSebas-
Weinberger, and Yoav Artzi. 2020. BERTScore:
tianRuder.2021. UNKseverywhere: Adaptingmul-
EvaluatingtextgenerationwithBERT. InInterna-
tilinguallanguagemodelstonewscripts. InProceed-
tionalConferenceonLearningRepresentations.
ingsofthe2021ConferenceonEmpiricalMethodsin
NaturalLanguageProcessing,pages10186–10203,
OnlineandPuntaCana,DominicanRepublic.Asso-
ciationforComputationalLinguistics.
Maja Popovic´. 2015. chrF: character n-gram F-score
forautomaticMTevaluation. InProceedingsofthe
TenthWorkshoponStatisticalMachineTranslation,
pages 392–395, Lisbon, Portugal. Association for
ComputationalLinguistics.
Tanja Samardžic´, Yves Scherrer, and Elvira Glaser.
2016. ArchiMob-acorpusofspokenSwissGerman.
InProceedingsoftheTenthInternationalConference
onLanguageResourcesandEvaluation(LREC’16),
pages4061–4066,Portorož,Slovenia.EuropeanLan-
guageResourcesAssociation(ELRA).A ListofEncoderModels
Model Totalparameters Trained Vocabularysize URLs(original→adapted)
XLM-R 278M 278M 250,002 →
CANINE 132M† 132M - →
SwissBERT
–subword-leveladaptation 139M‡ 8M 50,262 →
–character-leveladaptation 123M‡ 38M‡ 261 →
TableA1: Themainencoderstrainedinthiswork. † FiguredoesnotincludetheCANINE-Soutputembeddings,
whichcanbediscardedafterpre-training. ‡Figureincludestwoadapters(SwissGermanandStandardGerman).
B AblationStudy: CustomSubwordVocabulary
POS GDI Retrieval Macro-Avg.
GSW-BE GSW-ZH
XLM-R:
–XLM-Rvocabulary 86.9±0.3 62.1±0.8 91.1 96.0 80.9
–customGSWvocabulary 60.3±0.4 60.0±0.6 64.2 79.9 64.1
SwissBERTsubword-levelGSW adapter†:
–SwissBERTvocabulary 83.9±0.1 62.1±0.3 86.0 93.7 78.6
–customGSWvocabulary 23.7±2.3 56.9±0.6 65.6 77.3 50.7
CANINE:
– CANINE-S withSwissBERTvocabulary 60.9±1.4 60.8±0.4 96.4 96.9 72.8
– CANINE-S withcustomGSWvocabulary 57.8±1.2 62.1±0.6 95.6 96.3 71.9
SwissBERTcharacter-levelGSWadapter:
– CANINE-S withSwissBERTvocabulary 41.5±0.9 51.9±1.3 35.6 42.6 44.2
– CANINE-S withcustomGSWvocabulary 40.6±1.2 11.0±1.9 28.7 38.4 28.4
TableA2: Inanablationexperiment,wecreateacustomsubwordvocabularyforourcontinuedpre-trainingdataset
usingSentencePiece(KudoandRichardson,2018). Forthesubword-basedmodels,wetrainanewembedding
matrix while initializing it with lexically overlapping embeddings from the original model. Using the custom
vocabularyforSwissGermandecreasesperformanceonalldownstreamtasks,probablyduetothelimitedamount
of training data. For the character-based models, we use the CANINE-S objective with the custom vocabulary.
Surprisingly, the custom vocabulary decreases performance, possibly because it is less similar to the subword
vocabularyoriginallyusedbyClarketal.(2022)totrainCANINE-S. †Inthisexperiment,weupdatetheembedding
weightsofSwissBERTtoenableafaircomparison.
Vocabulary VocabularySize CompressionRatio
XLM-Rvocabulary 250,002 3.36
SwissBERTvocabulary 50,262 3.37
CustomGSWvocabulary 50,262 4.17
Table A3: Comparison of the SentencePiece vocabularies involved in the above ablation study. We report the
compressionratioasthenumberofcharacterspersubwordtokeninatokenizedsampleofourcontinuedpre-training
dataset.C ModelTrainingDetails
Approach Languagestrained Trainingsamplespersecond
XLM-Rcontinuedpre-training GSW+ DE 88.9
CANINEcontinuedpre-training GSW+ DE 149.6
SwissBERTcharacter-leveladapter GSW+ DE 127.1
SwissBERTsubword-leveladapter:
–onlyupdatingtheadapterweights GSW 215.3
–alsoupdatingthewordembeddings GSW 202.4
–updatingalltheweights GSW 225.9
Table A4: Empirical training speed in terms of training samples per second. Note that training speed is only
comparableformodelstrainedonthesamelanguages,sincetheDEsamplesarelongerthantheGSWsamples.
D Pre-trainingDatasets
Dataset Language TimeRange Examples Tokens URL
SwissCrawl(Linderetal.,2020) GSW until2019 563,037 10,961,075
SwissGermanTweets GSW 2007–2018 381,654 7,259,477 -
SwissdoxSample DE 2021 409,572 351,643,710
TableA5: Detailsofthedatasetsfromwhichwesourcedataforcontinuedpre-training.
Split Examples(newsarticles/tweets/sentences) Tokens
TrainingGSW 897,477 17,308,288
TrainingDE 20,140 17,459,689
ValidationGSW 47,214 912,264
ValidationDE 1,082 905,476
TableA6: Trainingandvalidationsplitsusedforcontinuedpre-training.
E EvaluationDatasets
Dataset Examples Tokens Citation URL
POSDE(train) 75,617 13,655,973 BorgesVölkeretal.(2019)
POSDE(validation) 18,434 324,848 BorgesVölkeretal.(2019)
POSGSW(test) 7,320 113,565 HollensteinandAepli(2014)
GDI(train) 14,279 112,707 Zampierietal.(2019) -
GDI(validation) 4,530 33,579 Zampierietal.(2019) -
GDI(test) 4,743 42,699 Zampierietal.(2019) -
RetrievalDE 1,997 50,833 Federmannetal.(2022)
RetrievalGSW-BE 1,997 53,119 Aeplietal.(2023)
RetrievalGSW-ZH 1,997 54,501 Aeplietal.(2023)
TableA7: Datasetstatisticsforthedownstreamtasks.