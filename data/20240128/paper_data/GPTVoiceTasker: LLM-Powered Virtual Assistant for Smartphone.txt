GPTVoiceTasker:LLM-PoweredVirtualAssistantforSmartphone
MINHDUCVU,HANWANG,andZHUANGLI,MonashUniversity,Australia
JIESHANCHEN,CSIRO’sData61,Australia
SHENGDONGZHAO,CityUniversityofHongKong,China
ZHENCHANGXING,CSIRO’sData61&AustralianNationalUniversity,Australia
CHUNYANGCHEN,MonashUniversity,Australia
Fig.1. AnexampleusecaseinHomeWorkoutapplicationwhentheuserneedstointeractwiththesmartphonehands-freedue
tophysicalbusyness.Whenperforminganon-screeninteraction,GptVoiceTaskerrepeatedlypromptsLLMtopredicton-screen
actionswithcurrentUIinformationandexecutetheresponsetoachieveusertasks.Theexecutioninformationisthensavedto
streamlinetheexecutionofsubsequentsimilartasks.
Authors’addresses:MinhDucVu,dustin.vu@monash.edu;HanWang,han.wang@monash.edu;ZhuangLi,zhuang.li@monash.edu,MonashUniversity,
Melbourne,Australia;JieshanChen,jieshan.chen@data61.csiro.au,CSIRO’sData61,Sydney,Australia;ShengdongZhao,CityUniversityofHongKong,
HongKong,China,shezhao@cityu.edu.hk;ZhenchangXing,CSIRO’sData61&AustralianNationalUniversity,Canberra,Australia,zhenchang.xing@
data61.csiro.au;ChunyangChen,MonashUniversity,Melbourne,Australia,chunyang.chen@monash.edu.
2024.ManuscriptsubmittedtoACM
ManuscriptsubmittedtoACM 1
4202
naJ
52
]CH.sc[
1v86241.1042:viXra2 Vuetal.
Virtualassistantshavethepotentialtoplayanimportantroleinhelpingusersachievesdifferenttasks.However,thesesystemsface
challengesintheirreal-worldusability,characterizedbyinefficiencyandstrugglesingraspinguserintentions.Leveragingrecent
advancesinLargeLanguageModels(LLMs),weintroduceGptVoiceTasker,avirtualassistantpoisedtoenhanceuserexperiencesand
taskefficiencyonmobiledevices.GptVoiceTaskerexcelsatintelligentlydecipheringusercommandsandexecutingrelevantdevice
interactionstostreamlinetaskcompletion.Thesystemcontinuallylearnsfromhistoricalusercommandstoautomatesubsequent
usages,furtherenhancingexecutionefficiency.OurexperimentsaffirmGptVoiceTasker’sexceptionalcommandinterpretation
abilitiesandtheprecisionofitstaskautomationmodule.Inouruserstudy,GptVoiceTaskerboostedtaskefficiencyinreal-world
scenariosby34.85%,accompaniedbypositiveparticipantfeedback.WemadeGptVoiceTaskeropen-source,invitingfurtherresearch
intoLLMsutilizationfordiversetasksthroughpromptengineeringandleveraginguserusagedatatoimproveefficiency.
CCSConcepts:•Human-centeredcomputing→Interactiontechniques;Smartphones;Naturallanguageinterfaces;Sound-
basedinput/output.
ACMReferenceFormat:
MinhDucVu,HanWang,ZhuangLi,JieshanChen,ShengdongZhao,ZhenchangXing,andChunyangChen.2024.GPTVoiceTasker:
LLM-PoweredVirtualAssistantforSmartphone. 1,1(January2024),22pages.https://doi.org/10.1145/nnnnnnn.nnnnnnn
1 INTRODUCTION
Theadvancementsinvoicecontroltechnologyhavesparkedanewwaveofinnovation,drivingtheexplorationofits
potentialintransformingsmartphoneinteractions[28,29].Withtheintegrationofvoicecontrol,userscaneffortlessly
navigatethroughvariousapplications,composemessages,andeveninitiatetaskslikecheckingtheweatherorplayinga
videoonYouTube[60].Thisseamlessandnaturalmodeofinteractionnotonlysavestimebutalsopromotesahands-free
experience,allowingindividualstoengagewiththeirsmartphonesinsituationswheremanualinputoperationis
impracticalorinconvenient[39,71](seeFig.1).Moreover,thesuccessstoriesofwidelyrecognizedvoiceassistantslike
GoogleVoiceAssistant[5]andSiri[3]havefurtherpropelledtheadoptionofthistechnology,inspiringresearchers
anddeveloperstodelvedeeperintoitscapabilitiesandrefineitsusabilityforanevenbroaderrangeofusers.As
thisfieldcontinuestoevolve,theintegrationofvoicecontroltechnologywithsmartphonesholdsimmensepotential
torevolutionizethewayweinteractwithourdevices,empoweringuserswithenhancedaccessibilityandamore
convenientuserexperience.
Developingefficientandreliablevoice-controlledsystemsinvolvesaddressingvariouschallengesthatsignificantly
impacttheaccuracyandusabilityofthesesystems[47].Onemajorhurdleisaccuratelycomprehendingusercommands
andseamlesslymappingthemtospecificsmartphoneactions[51].Achievingsatisfactoryaccuracylevelsoftenrelies
on substantial training data, comprised of user voice commands, and complex deep learning models [36]. These
modelsareemployedtodecipherthesemanticnuancesofthecommands,comprehendtheirintendedmeaning,and
subsequentlyselecttheappropriateuserinterface(UI)elementonthescreenforinteraction.However,acquiringand
curatingsubstantialdatasetsfortraininglanguagemodelspresentsalaboriousandtime-consumingchallenge,impeding
progressandscalabilityindevelopingvirtualassistants.Moreover,thecollecteddatasetmaynotcomprehensivelycover
thediversecommandsrequiredformodernapplicationswithrichfunctionalities,potentiallylimitingthelanguage
model’scapabilities.Additionally,therapidadvancementoftechnologyconstantlyintroducesnewfunctionalities,
quicklyrenderingtheexistingdatasetoutdated,andthesemodelsoftenstrugglewithaccommodatingtheinterpreting
speechrecognizererrorsduetoaccents[59]andhomophones[48].Theinherentcomplexityofthesemodelshampers
their adaptability to the dynamic landscape of human natural language and pronunciation variations, making it
challengingtoaccountforreal-worldusagescenarios’intricaciesandvariations.
ManuscriptsubmittedtoACMGPTVoiceTasker:LLM-PoweredVirtualAssistantforSmartphone 3
Furthermore,voiceassistantsoftenfaceperformanceissueswhencomparedtophysicaltouchesonmobiledevices[2].
Extensiveresearchanddevelopmenthaveexploredvariousmethodstoenhancevoiceassistants’capabilities.For
instance,Voicify[60]employsappanalysisanddeeplinkingtechniquestospeedupvoicecommandprocessing.Modern
voiceassistantsalsoenhanceuserexperiencebypersonalizinginteractionsbasedonindividualusagepatternsand
preferences[12,70].Thispersonalizedlearningapproachholdstremendouspotentialinmitigatingtheperformance
limitationsofvoice-controlledsystems,enhancingefficiency,andacceleratinguserinteractions[35,51].Bycontinuously
learningfromuserinteractions,voiceassistantscanimprovetheiraccuracyandreliability,ultimatelyensuringefficiency
andoveralluserexperienceinreal-worldusagescenarios.
LargeLanguageModels(LLMs)haverevolutionizednaturallanguageprocessing(NLP),showcasingtheirgeneral
intelligenceandexcellenceinreadingcomprehension,triviaquizzes,translation,andtextcompletion[9].Theconcept
ofFew-ShotLearningfurtherenhancesLLMs’powerbyenablingthemtoadapttonewtaskswithminimalexamplesor
prompts,eliminatingtheneedfortask-specificmodelsanddatasets[53].Thisflexibilityandefficiencyinhandlingdiverse
conversationalinteractionswithoutextensiveretrainingpresentaninnovativeandpromisingapproach.Leveraging
few-shotlearning,developerscantapintothevastknowledgeandlanguageunderstandingcapabilitiesofLLMs,
facilitatingrapiddevelopmentanddeploymentoflanguage-basedautomationsolutionsinvariousdomains[23,44].
Therefore,weaimtoinvestigatethepotentialofLLMsinenhancingtheintuitivenessofvoicecontrolformobile
applications.
ThispaperintroducesGptVoiceTasker,anovelsystemthatleveragesLLMstoenhancevoicecontrolforAndroid
devices.Oursystememploysadvancedpromptengineeringtechniquestoensureapreciseunderstandingofuser
commandswithoutextensivemodeltraining.Bybridgingthegapbetweennaturallanguagecommandsandinteractive
tasksonmobiledevices,GptVoiceTaskerfacilitatesseamlessautomationofdailyphysicalinteractions,including
scrolling,tapping,andinputtingtext,solelythroughvoicecommands.Inaddition,thesystemautomaticallyrecords
andlearnsfromusercommandsandin-appusages,enablingthereproductionoftasksforsimilarrequestsinthefuture
toimprovetaskefficiency.GptVoiceTaskersupportsthesequentialexecutionofmultiplecommandswithinasingle
utterance,furtherenhancingtheefficiencyofvoiceinteractiononmobiledevices.
WevalidatedthetechnicalcontributionsofGptVoiceTaskerbyevaluatingi)theabilitytoparseusercommandsinto
executableactionsandii)theabilitytostreamlinesavedtasks.Thecommandparserachievedover90%accuracyonthe
humancommanddatasetcollectedfromauserstudy.Ourautomatedexecutionachieved82.7%successratefordirect
matchtasksand72.0%successratefortaskswithdifferentparameters.Tovalidatetheusabilityof GptVoiceTasker,
weconductedauserevaluationwith18participants,eachcompletingasetoftasksusingGptVoiceTaskerandtwo
state-of-the-artbaselines.Wecollectedthetimetakentocompleteeachtask,aswellasquantitativeandqualityfeedback
fromusers.TheresultsshowthatGptVoiceTaskeracceleratesthetasksby34.85%andreceivedpositivefeedback
regardingusability.
Tosummarize,thecontributionsofthispaperinclude:
• Developmentof GptVoiceTasker,avoiceassistantthatharnessesthecapabilitiesofLLMstostreamlinethe
automationofmulti-steptasksbypredictingthemostoptimalsteponeachindividualscreen.
• Agraph-basedlocaldatabasedesignthatautomatestherecordingandretrievalofpersonalappusages,enhancing
taskexecutionefficiencyforvirtualassistantinteractions.
• Conductingalarge-scaleuserevaluationtovalidatetheeffectivenessofourapproach,alongwithempirical
findingsonsystemlimitationsandconsiderationsforvoiceassistantdesign.
ManuscriptsubmittedtoACM4 Vuetal.
• GptVoiceTasker1isopen-sourcedsothatanyonecanuseandcontinuetoimprovethesystem.
2 BACKGROUND&RELATEDWORKS
2.1 VoiceControl&AssistantsonMobileDevices
Inrecentyears,theprogressinNaturalLanguageUnderstanding(NLU)hasfueledtheproliferationofvoiceassistants
acrossdiverseplatforms,includingubiquitoussystems[7,32]andhomeappliances[52].Onpersonalsmartphones,
numerousvoiceassistantshavebeenproposed,leveragingintelligentNLUmodelstocomprehenduserspeechinput
andmapittouserinterfaceactions.AnearlymilestoneinsmartphonevoicecontrolinterfaceswasJustSpeak,which
harnessedGoogle’sAutomaticSpeechRecognition(ASR)torecordusercommandsandintroducedinnovativeutterance
parsingtechniques[71].Subsequently,theSmartVoiceAssistantexpandedonJustSpeak’scapabilitiesbyenablingusers
tomanagecallsandSMSthroughvoicecommands[11].Asimilarendeavor,the"VoiceNavigator"applicationbyWeberet
al.in2016,focusedonenhancingthevisibilityandlearnabilityofmobilevoiceuserinterfaceapplications[18].However,
theseinitialapproaches,foundationalastheywere,encounteredusabilityissuesstemmingfromrigidlanguageparsing
heuristicsandlimitedusecases,whichspurredtheneedforfurtherdevelopmentofsmartphonevirtualassistants.
Inrecentyears,significantadvancementsinlanguageparsingcapabilitieshavebeenachievedthroughdeeplearning
models.SAVANTleveragedDialogflowasaconversationalagenttoextractuserintentfromutterances[4],while
DoThisHereemployedthepre-builtAlmondlanguagemodeltoenablevoicecontrolforretrievingandsettingUI
contentsinAndroid[69].GooglereleasedVoiceAccess[1],aimedtoreplacemanualinteractionswithvoicecommand,
whichhasover100millionsdownloadsonGooglePlayStore.Morerecently,Voicify[60]introducedVoicifyParser,
anadvanceddeeplearningapproachforparsingusercommandsintoon-screeninteractions,whileAutoVCI[51]
concentratedondevelopingvoiceinterfacesforautomatingmobileUItasks.However,theinteractionparadigmwith
theseexistingapproachesremainssomewhatunnatural,requiringuserstoissueprecisemachine-likeinstructions,such
as“Presssavebutton”.Thislimitationmeansthattheymaystruggletofullycomprehendhigh-leveluserintentions,such
as“Iwanttosavethisnote”.WeproposeGptVoiceTaskertoaddressthesechallengesandrevolutionisethevoice-based
interactionsbetweenhumanandsoftwaresystems.OursolutionleveragesthecapabilitiesofLLMstomaphigh-level
userintentionstoexecutableactions,enablingon-screeninteractionsthroughintention-basedvoicecommands.This
approachseekstoaccommodatetheflexibilityandnaturallanguageofhumancommands,usheringinaneweraof
user-friendlyassistivetools.
2.2 LargeLanguageModelsforEnhancedHuman-AICollaboration
TheadventofgenerativeAIhasgivenrisetoinnovativeLLMs,suchasGPT-4[50]andDALL-E[55].TheseLLMshave
revolutionizedthelandscapeofAIdevelopmentbyenablingdeveloperstoachievecomplextasksthroughfew-shot
prompting,eliminatingtheneedforextensivecustommodeltraining.Theirremarkableversatilityhasspurredactive
researchinbothITandnon-ITdomains,spanningareaslikesoftwaretesting[24,43],high-performancecomputing[15],
finance[65],andhealthscience[25].LLMshaveparticularlyexcelledinenhancingtheintuitivenessofexistingmethods,
asseeninsoftwaretesting,wheretheygenerateauthentictextinputsbasedonthecurrentUIpageinformation,
replacingtheconventionalrandomtextinputapproach[43].ThisdemonstratesthetransformativepotentialofLLMsin
advancingresearchandinnovationacrossamultitudeofdomains.
1https://github.com/vuminhduc796/GPTVoiceTasker
ManuscriptsubmittedtoACMGPTVoiceTasker:LLM-PoweredVirtualAssistantforSmartphone 5
ThecapabilitiesofLLMshavesparkedasurgeintheirapplicationwithinassistivetechnology,revolutionizingthe
translationofusercommandsintoexecutabletasksacrossdiversesystems.Recentresearchinthisdomainhaswitnessed
thetransformationofhumannaturallanguagecommandsintovarioustypesoftasks,includingvisualizationtasks[62],
operatingsystemtasks[42],androbotictasks[38,57].LLMshaveenabledthesesystemstotacklemoreintricate
commandsbeyondthescopeofexistingheuristicapproaches.Theyalsoexhibitaremarkableabilitytocomprehend
variationsofcommandsthatsharesimilarintentionsbutareexpresseddifferently.Thispioneeringframework,withthe
supportofLLMs,pavesthewayforanovel(semi)automatedtaskexecutionparadigm,erasingtheboundariesbetween
traditionalcommandpatternsandintuitivecommandmodalities.
Withinthedomainofmobileassistants,LLMshaveemergedasatransformativeforce,supplantingconventional
machinelearningmodelsasseeninpriorrelatedworks[51,60].Thisparadigmshiftsimplifiestheprocessoftranslating
user’snaturalvoicecommandsintoactionableoperationsonmobileUserInterfaces(UIs).Wangetal.[61]utilisedLLMs
toallowconversation-alikeinteractionwithmobileUIs,whichdemonstratesLLM’sbetterabilityinunderstanding
on-screenelementscomparedwithtraditionalmachinelearningapproaches[37].WhileWangetal.’sapproachin
smartphonevirtualassistantdevelopmentisnoteworthy,itsemployedpromptingstrategyremainsbasic,impacting
itsabilitytoparsenaturalusercommands.Ourworkaimstoelevatethisapproachbyincorporatingstate-of-the-art
promptingstrategiestoenhanceaccuracyandcreateamorecomprehensivesmartphonevirtualassistant.UnlikeWang
etal.’sfocusonsingleon-screenactions,ourresearchtakesastepfurther.WestrivetounlockthefullpotentialofLLMs
inautomatingcomplexuserrequestsinvolvingmultiplestepstoachievespecificobjectives,asexemplifiedinFig.1
withthequery"Showmehowtodothechestflyexercise".OurgoalistobroadenthescopeofLLMs,assistingusersnot
onlyinknowntasksbutalsoinaccomplishingunfamiliarones,allwhilesignificantlyreducingthetimerequiredto
completethesetasksonasmartphone.
3 THEGPTVOICETASKERSYSTEM
WeintroduceGptVoiceTasker,avirtualassistantthatempowersuserstoefficientlyperformvarioustasksontheirsmart-
phonesusingvoicecommands.Byapplyingdifferentpromptengineeringtechniques(Section3.1),GptVoiceTasker
harnessthepowerofLLMstoperformdifferentlogicaltasks.Uponreceivingausercommand,GptVoiceTaskerfirst
attemptstoautomaticallyexecutethetaskusingthesaveddatabase(Section3.3).Shouldataskproveunfeasibleusing
thepre-existingdatabase,GptVoiceTaskerwillperformaseriesofstep-by-steppredictionsofon-screennavigationto
completeatask(Section3.2).Simultaneously,thesystemrecordstheseinteractionsforsubsequentautomatedexecution.
3.1 PromptEngineering
While a naive approach to prompting LLMs for various tasks may yield suboptimal results due to low accuracy
and randomness in responses [17], we propose the adoption of different prompt engineering techniques. These
techniquesinvolvecraftingpromptsaccordingtospecificrulesandcomponentstoelicitoptimalresponsesfrom
LLMs [41]. We created multiple prompt templates2, which is applied in both On-Screen Interaction (Section 3.2)
andUsage-basedExecution(Section3.3).InFig.2,weillustrateanexampleofourpromptdesignedtodetermine
themostappropriatetargetfortapping.Thissectionexplainsdifferentpromptengineeringtechniquesappliedin
GptVoiceTasker, showcasing their significance in harnessing LLMs for complex reasoning tasks across multiple
componentsinoursystem.
2https://github.com/vuminhduc796/GPTVoiceTasker/blob/main/prompts.txt
ManuscriptsubmittedtoACM6 Vuetal.
Fig.2. Anexampleofourpromptandresponseformattodeterminethemostrelevanttargettopress.
3.1.1 Least-to-mostPrompting. Inaddressingcomplextasks,suchaspredictingtheactiontoperformonthescreen
fromtheusercommand,GptVoiceTaskeremploystheLeast-to-mostPromptingstrategy[72].Thistechniqueinvolves
breakingdownacomplextaskintosmaller,manageableprompts.Thegoalistoenhancethemodel’sunderstanding
ofthelogicalflow,therebyimprovingaccuracy.Forexample,GptVoiceTaskerimplementsatwo-stepprompting
approachforOn-ScreenInteractions(Section3.2).First,themodelispromptedtomapuserintenttoaspecificaction
(e.g.,tappinganelement,enteringtext,scrolling).Subsequently,basedonthedeterminedaction,subsequentprompts
aresenttoidentifythetargetUIelementforexecutingthataction(Fig.2).ThisapproachallowstheLLMstoconcentrate
onatomictasks,contributingtoimprovedaccuracycomparedtoanaiveapproachofidentifyingbothactionandtarget
inthesameprompt[61].
3.1.2 Few-shotPrompting. Few-shotprompts[13],incorporatingcompletedtaskexamplesintotheprompt(asin
Examplar1inFig.2)itself,empowerLLMslikeGPTtofacilitaterapidcomprehensionofspecificrulesandguidelines,
therebymarkedlyimprovingoutputaccuracyandreliability.Forinstance,whendiscerningtheactioninausercommand,
we include exemplars with synonyms and homophones to enhance adaptability and address speech recognition
variations.Thisintegrationoffew-shotlearning,coupledwiththeLLM’sadeptvocabularyandgrammarcomprehension,
boostsversatilityacrossdiversescenarios.Importantly,iteffectivelyrectifiesterminologydiscrepancies,acommon
issueinvoiceassistantsusingautomaticspeechrecognition[10].Thisadvancementrepresentsanotableimprovement
overheuristic-basedmethods,especiallyinnavigatinglinguisticintricacies.
3.1.3 ChainofThought. Whendealingwithcomplexpromptsthatdemandlogicalreasoning,LLMsoftenstruggleto
fullyinterpretanddeliveraccurateoutput.Toaddressthis,weintegrate“ChainofThought”[64]intoourfew-shot
exemplarstohelpLLMssimulatehuman-likereasoningandprovidelogicaloutput.Byemployingthismechanism,we
ensureconsistentandcoherentreasoning,enablingaseamlessprogressionofideasandactionsingeneratedresponses.
InGptVoiceTasker,thechainofthoughtisaddedtoeachresponseinexampleswherefew-shotpromptingisapplied.
Infew-shotexemplar1,asdepictedinFig.2,weprovidedachainofthoughttopredictthemostrelevantUIelement
ManuscriptsubmittedtoACMGPTVoiceTasker:LLM-PoweredVirtualAssistantforSmartphone 7
tofindthe“videohistoryinYouTube”.Despitethetaskrequiringmultiplestepstonavigatetothehistorypage,we
askedtheLLMtoidentifykeyindicatorsandexecutelogicalreasoningspecifictothetask.Thisenabledittoaccurately
predictthemostlikelybuttonforinteraction.Asaresult,thesystemcanrepeatthesamecommand,incorporateitinto
theprompts,proceedwiththepredictedactions,andcontinueuntilitreachestheintendeddestination.
3.2 On-screenInteraction
Uponreceivingataskfromusers,GptVoiceTaskerwillautomaticallyperformmultipleon-screenactionexecutions
untilthetaskisaccomplished.Wefirstcollectrun-timeUIelements,executereasoningtasksfromusercommands,and
performactionsontheuser’sdevice.GptVoiceTaskercontinuestorepeatthisiterativeprocesstoperformeachaction
toaccomplishtheusertasks.
3.2.1 UISemanticExtraction. ToensureacomprehensiveunderstandingofthecurrentUIelementsandtheirsemantics,
GptVoiceTaskerutilisesAndroidAccessibilityServicetocontinuouslycaptureUIandanalyzesnewchangesinthe
mobilescreen,followingpreviousresearch[60].NotethattherawUIelementsweextractedmaybeafflictedbyUI
noise,whichisaprevalentissuelinkedtothereal-timegatheringofUIelements[34].Thisproblemariseswhenthe
collectedUIinformationdoesnotalignwithitsvisualrepresentation,whichaffectsthesemanticunderstandingof
LLMsoncurrentUIelements,resultinginincorrectinteractions.Toaddressthis,weimplementheuristicstomitigate
potentialinaccuraciesandensurethereliabilityofcollectedUIinformation.First,weutilisethecollectedcoordination
ofeachUIelementstoeliminateout-of-boundoremptyelements.Wealsoeliminateviewsarefullyoverlappedby
otherviews,whichdoesareinvisibleandnotinteractible.Inaddition,weremovethoseviewsthatdonotcontainany
interprettableinformation,suchasemptyviewcontainers.
OurprimaryemphasisisonrepresentingmobileUIscreensinatextualformatthatcanbeinterpretedbyLargeLan-
guageModels(LLMs)throughtext-basedinput.WhilerecentresearchproposedthetranslationofXMLrepresentation
ofUIscreenstoHTMLformat[24,61]toovercomepromptlengthlimitations,thisapproachbecomeslessrelevantas
modernmodelsrelaxrestrictionsonpromptlength,allowingdeveloperstopresentmoreinformationinaprompt.For
textualcomponentsliketextfieldsandbuttons,wegatheressentialsemanticinformationfromtheirlabelsandcontents.
Wecollectalternativetextandresourcenamesforiconsandimages,whichoffersmeaningfulinformationasdefinedby
developers.Additionally,weaugmentthecollectedinformationwithattributessuchasthepreciseelementlocation
onthescreenandamorecomprehensiveviewhierarchicalstructure,previouslyabsentinHTMLrepresentations.
ThesedetailsplayacrucialroleinillustratingrelationshipsbetweenUIelements,asdemonstratedbytheexample
ofdeliverytimesforeachrestaurantintheUberEatsapp(Fig.3).Thiscapabilityprovesvaluableinaccommodating
usercommands,suchas“Selecttherestaurantwiththefastestdeliverytime”,ataskunattainablewithaflattenedUI
representationapproach.Furthermore,wecancaterforcommandsthatuserreferencestoUIelementsbytheirlocations,
suchas“Presstheiconatthetop-rightcorner”.Lastly,eachUIelementisenrichedwithsupportedinteractiontypes,
indicatingpotentialactionslikePRESSorENTER_TEXTtosupportLLMproposetheappropriateaction.
ToenhancethecontextualunderstandingandreasoningcapabilitiesoftheLLM,wenotonlyfetchon-screendata
butalsoretrieverelevantsystem-relatedinformation,includingtheappnameandactivitynameofthecurrentscreen.
Byincorporatingthesesystem-relateddetails,GptVoiceTaskergainsacomprehensiveunderstandingoftheuser’s
currentcontext,whichfurtheraidsinvalidatingeachstepinmulti-stepexecutions.
3.2.2 ActionExecutor. WecombinetheusercommandwithUIandcontextualinformationcollectedinpreviousstep
andappliesdifferentpromptengineeringtechniquesasdescribedinSection3.1tocreateprompts.Thesepromptsare
ManuscriptsubmittedtoACM8 Vuetal.
thensenttoLLMs,whichwillfirstdetectmostappropriateactionsandthendetectstheassociatedtargetUIelement
forthisaction.GptVoiceTaskerusesthisinformationtoperformtheinteractionsonuser’sdevice,suchastapping,
scrolling,orenteringtextoncertainUIelement.Priormobileautomationapproaches[51]encounterchallengesin
handlingruntimeUIchangesandappupdatesthatmightalterUIrepresentationsandoperationsequences.Incontrast,
ourapproachexhibitsrobustnessinthefaceofsuchissues,asthesequenceofstepsisdynamicallydependenton
theUIonthescreen.ThisadaptabilityallowsGptVoiceTaskertonavigatethroughdynamicUIchangesandupdates,
ensuringreliabilityintaskautomationonsmartphones.Additionally,GptVoiceTaskerprovidesaudiofeedbackto
users,confirmingthatthesystemisautomaticallyproceedingtothenextcommand.Thisreal-timefeedbackensuresa
smoothandintuitiveuserexperiencewithGptVoiceTasker’sinteractioncapabilities.
Toaddresschallengesinherentinautomationwithinreal-timesystems,includingthedetectionoffailuresineach
automatedstep[66],ourimplementationintegratesascreentransitiondetector.ThisdetectorutilizestheHamming
distance[31]togaugethedifferencebetweentwoscreens,ensuringthattheUIaccuratelyreflectschangesresulting
fromtheautomatedaction.Moreover,forvalidatingthesuccessofactions,weimplementadditionalheuristics.For
example,inscenariosinvolvingENTER_TEXT,weverifythepresenceoftheenteredtextinthetargettextbox.If
anactionprovesinexecutable(i.e.,notinducingappropriatechangestotheUI),weiterativelyrepeatthestepwith
supplementaryinformationaboutthefailedaction,therebyexcludingtheseactionsfromtheselection.
AnotherchallengeinmobileappautomationarisesfromtheprevalentdependencyofmobileUIscreensonlive
internetcontent,oftenloadedasynchronouslyafterthescreenappearsonsmartphones.Thecollectionandutilization
ofincompleteorloadingUIscancompromisetheaccuracyofdetectingsuitableactions.Inresponse,GptVoiceTasker
introducesaninnovativeapproachbydelayingsubsequentactionsuntilthescreenisfullyloaded.Thisisachieved
bydetectingscreen-loadingwidgetsinAndroid,leveraginginformationsuchaswidgetnames,types,andshapesto
detectprogressbarsandloadingindicators.Furthermore,weenhanceourapproachbycollectingdataonnetwork
transmissionstoidentifyongoingcontentdownloadingtasks,inspiredbythemethodologypresentedin[40],which
leveragesnetworkanalysisfordetectingads.ThesemethodssignificantlyenhancethereliabilityofGptVoiceTasker
ineffectivelynavigatingthroughtheapp’sinterface.
3.3 Usage-basedExecution
Inmobileapps,theUIelementsonaspecificapppagearepredefinedbydeveloperswhendevelopinganapppage.
Therefore,theseriesofuserinteractionsonthescreentoachieveataskwillbeconsistentacrossdifferenttimes.Based
onthismatter,GptVoiceTaskerautomaticallycreateasavedpathforeachusercommand,allowingGptVoiceTasker
toreplicateinteractionswhenreceivingasimilarcommandfromusers.Unlikepreviousapproaches[35,51]thatdepend
onamanualtaskcreationprocessforautomationsupport,GptVoiceTaskerautomaticallyrecordsapptransitions
throughon-screennavigationinSection3.2.Thisautomatedprocessnotonlyallowwidercoverageofautomatedtasks
butalsoeliminatestheneedformanualeffortsinpredefiningshortcuttasks.Inthissection,weoutlineourmethod
(asinFig.3)tostreamlinesubsequentsimilartasksfromusers,whichcombinesbothLLMs-basedandheuristic-based
modules.WeintroduceourdatabasedesigninSection3.3.1.First,weidentifythecurrentUIscreendisplaysonuser
deviceandthedestinationscreen(Section3.3.2).Afterthat,wefindthemostviablepathfromcurrentscreentothe
destinationscreen(Section3.3.3).Finally,weuseincorporatehumaninteractivefeedbacktovalidateandfinetunethe
executionforfurtherusages(Section3.3.4).
ManuscriptsubmittedtoACMGPTVoiceTasker:LLM-PoweredVirtualAssistantforSmartphone 9
Fig.3. AnexampleusecaseinUberEatstohowGptVoiceTaskerusethehistoricaltaskstoexecuteusernewcommand.
3.3.1 TransitionGraph. Inlinewithpreviousapproachestoautomatingtasksinmobileapps[16],GptVoiceTasker
incorporatesadirectedgraphforeachapptofacilitateseamlesstransitionsbetweendifferentapppages.Eachnodein
thedatabasecorrespondstoaUIpageintheapp,containingauniqueIDandascreendescriptiongeneratedasoutlined
inSection3.3.2.Moreover,foreachnode,wemaintainalistofpreviouslyusedcommandsbyusersthatconcluded
onthispage.Thiscompilationsignifiesthattherequestedfunctionalityfromtheseusercommandsisassociatedwith
thisspecificapppage.ThedirectededgesbetweennodesrepresentpotentialtransitionsfromoneUIpagetoanother,
capturinginformationabouttheassociatedactiontypeandthetargetUIelementforthetransition.Theseedgedetails
empowerGptVoiceTaskertoreplicateuseractionsandscreentraversals.Asusersengagewiththeapp,thisgraph
dynamicallyexpands,automaticallyincorporatingnewUIpagesandassociatedtransitions.
3.3.2 ScreenDescription&CommandPatternMatching. Whilepreviousautomationapproachesfocusonexecuting
tasksstartingfromthelauncherpageoftheapp[4,51],theymaynotbesuitableforcaseswhereusersarecurrentlyon
otherpages,necessitatingadditionalnavigationstepstoreturntothelauncherpage.Toaddressthischallenge,wefirst
identifythecurrentscreenoftheappthattheuserisviewinginourgraphdatabase.However,heuristicallycomparing
andidentifyingaUIpageusingtheon-screentextualinformationcollectedfromtheXMLrepresentationproves
impracticalduetothecontextualdynamicityofinformationwithinUIelementsonaspecificapppage.Forexample,
considerthesearchresultpageintheUberEatsapp,wheredistinctrestaurantsaredisplayedfor“spaghetti” and“sushi”
searchresults.Despitefeaturingdifferentcontent,thesepagessharethesamelayoutand,therefore,shouldbeidentified
asthesamenodeinourgraphdatabase.Toachievethis,weleveragethecapabilitiesofLLMstosemanticallysummarize
theUIcontent[61]intoasemi-structureddescription.WecollectcontentfromtheXMLhierarchicalrepresentationof
thescreen,alongwithadditionalcontextaboutthecurrentactivityandappname,togenerateahigh-leveldescription
oftheoverallfunctionalitythatthescreenserves.Subsequently,GptVoiceTaskersummarizesthelistofinteractive
elements,includingclickable,scrollable,andtext-editableelements,andappendsthemtothedescription.Tolocatethe
ManuscriptsubmittedtoACM10 Vuetal.
currentUIpagewithinthelistofvisitedscreensinanapp,wepromptLLMstoperformsemanticmatchingbetween
thecurrentscreendescriptionandthesavedlistofscreendescriptionsfromtheapp.Thisprocessallowsustotrace
backtheIDofthescreeninthegraphdatabase,effectivelyaddressingthenavigationchallengesfromanypointwithin
theapp.
Afteridentifyingthecurrentscreen,weperformsemanticmatchingtofinddestinationscreen.Wedefinedestination
screenasthemostrelevantscreenthatcanserveuserrequest,forexample,thedestinationscreenforusercommand“I
wanttoorderspaghetti” isthesearchresultpageforthekeyword“spaghetti”.Weutilisetheusersavedcommandsand
screendescriptionforeachapppagetopromptLLMstodeterminethemostrelevantapppagefromtheapp.Webuilda
promptthatincludesuserrequestandthelistofsavedscreenintheapptoranktherelevancyofeachappscreenwith
therequestthatusermade.
3.3.3 PathFinding&Execution. Afteridentifyingthecurrentanddestinationpageswithinthesavedgraphdatabase,
GptVoiceTaskerutilizestheShortestPathalgorithm[26]todeterminethesequenceofactionsrequiredtonavigate
fromthestarttothedestinationnode.GptVoiceTaskerextractsthissequencefromtheedgesconnectingthestart
nodetothedestinationnodeandexecuteseachactioninsequenceusingtheActionExecutordescribedinSection3.2.2.
GiventhedynamicnatureofsmartphoneGUIs,wheretherelativecoordinatesofbuttonsmayvary,especiallyin
scrollablescreensorduetounexpectedpop-upsandads,executionvalidationiscrucial.Toaddressthis,GptVoiceTasker
performsvalidationoneachUIpageinthepath,ensuringtheautomationfollowstheexpectedsequence.Aftereach
actioninthesequence,thescreendescriptionisgeneratedbasedoncurrentdisplayedUI(asexplainedinSection3.3.2)
andcomparedtotheexpecteddescriptionstoredinthegraphnoderepresentingtheanticipatedUI.Ifthedescriptions
donotmatch,thetoolnavigatesbacktothepreviousapppageandpromptsLLMstoproposeanaction.Theprompt
createdforon-screenactionpredictionsisthenusedtofeedtheactiontotheLLM,regeneratingtheappropriateaction.
ThisiterativeprocessensuresthattheautomationadaptstochangesinthesmartphoneGUI,allowingGptVoiceTasker
todynamicallyexecuteactionsbasedonreal-timescreeninformation.
Additionally,GptVoiceTaskerleveragescommandparameterizationtechniques[33,51],allowingsavedpathsto
bereusedforsimilartaskswithdifferentparameters.Utilisingtheenrichedvocabularyandrobustnaturallanguage
understandingcapabilitiesoftheLLM,GptVoiceTaskerpromptstheLLMtofunctionasanadvancedNamedEntity
Recognition(NER)system.Thissystemidentifiesandreplacesalloccurrencesofsubstitutablewordswithnewkeywords
intheactionsequence.Forexample,intheUberEatsappscenarioshowninFig.3,whereauserhasasavedcommand
for“Iwanttoordersomesushi”,GptVoiceTaskerrecognizes“sushi” asaparametervaluerepresentingthetypeoffood,
whichisreplacedby“spaghetti”,transformingthecommandfromENTER“sushi” toENTER“spaghetti”.Subsequently,
GptVoiceTaskerexecutesthisadaptedsequenceofactionstoaccomplishthetask.
3.3.4 HumanFeedbackLoop. Thesavedcommandshaveproventheirapplicabilityinsubsequentuserinteractions
withthedevice,yetcertaininconsistenciesmayariseduetoappversionupdatesanddynamicchangesinUIelement
availability.Toaddresserrorhandlingandcorrections,GptVoiceTaskerincorporateshumanfeedbacktoenhancethe
saveddataforeachtask.Theautomatedtasksperformedbythetoolallowuserstoefficientlyreverseactionsandprompt
thesystemtoactivelyadjustthenewexecutionpathtoaccomplishtaskseffectively.Thisvaluableinformationisstored
andfedbacktotheLLM,resultinginimprovedoutputforfutureexecutions.Whenusersexpresssatisfactionwiththe
resultandrequestthenexttask,wealsoincreasetheconfidencescoreforthissavedpath,helpingGptVoiceTaskerto
selectthemostreliablepathtoexecuteifmultipleexecutionpathsarefound.Inaddition,wealsoprovideanadditional
ManuscriptsubmittedtoACMGPTVoiceTasker:LLM-PoweredVirtualAssistantforSmartphone 11
interfaceforuserstoamendsavedcommandsmanually.Throughuserfeedbackanditerativelearning,GptVoiceTasker
continuouslyrefinesitsperformance,ensuringincreasinglyaccurateandeffectiveresponsesovertime.
3.4 Implementation
WeimplementedGptVoiceTaskerasanAndroidapplicationwiththeAccessibilityServiceinAndroidOSusingJava[21].
WithintheJavacode,GptVoiceTaskersubscribestothetypeWindowContentChangedaccessibilityevents[21]toreceive
notificationswheneverUIchangesoccuronthescreen.WecreatedadynamicpipelinetoextractUIelementsina
hierarchicalstructurefromAccessibilityNodeInfoobjects[20],whichserveasdatarepresentationsofon-screenUI
elementsprovidedbyAndroidAccessibilityService.Inaddition,Weobtaintheappnameandtheactivitynameusing
theAndroidPackageManagerclass.
TocommunicatewiththeLLM,weutilisedtheAPIserviceprovidedbyOpenAI.WechosetheGPT-4model,asitisthe
latestmodeltrainedbyOpenAI[49].OncewereceivearesponsefromtheLLM,thetoolleveragestheperformAction()
method[20]toexecuteactionsonthecorrespondingelements.Alldatarelatedtopersonalizedservices,suchasscreen
descriptionandthetransitiongraph,arestoredinthephonememoryforfutureusage.
4 TECHNICALEVALUATION
Toevaluatetheeffectivenessandreliabilityoftheproposedsystem,weconductedtwoexperimentsonourcommand
interpretingmoduleandusage-basedexecution.Specifically,wefirstassessthesystem’sabilitytocomprehenduser
commandsandperformon-screeninteractions,comparingitsperformancetootherstate-of-the-artapproaches.We
theninvestigatethesystem’scapabilitytoexecutemulti-steptasksbasedonthesaveduserusages..
4.1 On-screenInteractionEvaluation
4.1.1 ExperimentSetup&Metric. Datasets:Wecollectaspecialisedtestsettoevaluateoursystem’scapabilitiesin
understandingnaturallanguagecommandsandmappingthemtoappropriateactionsandtargetUIelements.This
datasetcomprises278naturallanguageusercommandstointeractwithAndroidUIs.
Althoughpriorresearch[14,60]hasproducedasimilartestset,itisnotdirectlyadaptabletoourcontextfortwo
criticalreasons.First,someinstancesinthetestsetareartificiallysynthesizedbasedonpredeterminedheuristicrules.
Theresultingnaturallanguagecommandsarelinguisticallybiasedtowardsimplerlinguisticpatternsanddonotalign
withthecomplexlinguisticvariantsinherentinreal-worldhumanspokenutterances.Second,sometestexamplesin
theexistingdatasethavebecomeobsoleteorarenolongerreplicableduetoupdatesinthecorrespondingapplications.
Toconstructatestsetthatmorecloselyalignswithreal-worlduserinteractions,weadoptedadata-drivenapproach.
Weengaged31participants(17females,14males),with4individualshavingneverutilizedvoiceassistantsbefore,4
usingthem3-4timesaweek,6usingthemdaily,and17usingthemlessthan3-4timesaweek.Allparticipantsare
workprofessionalsandtherestfromuniversitycommunitywhousesmartphonesdaily.Weprovidedtheseparticipants
withscreenshotsalongsideaspecifictasktoaccomplish.Wethenrecordedtheverbalcommandstheyissuedtotheir
mobiledevicetocompletethegiventask.Afterthecollection,weannotatedthecommandstospecifytheintended
actionandtargetUIelementswithintheAndroidsystem;here,thetermactionreferstoexecutablefunctions,while
targetdenotesspecificUIcomponentsorelementsonthecurrentscreen.Asaresult,wecollected278naturaluser
commandsforthedataset.
Metrics:SimilartoVuetal.[60],weadoptthreeevaluationmetrics,namelyExactMatchAccuracy(EM),TargetF1
andActionF1.EMcalculatesthepercentageofinstancesinthetestsetwherethepredictedsequenceexactlymatches
ManuscriptsubmittedtoACM12 Vuetal.
itscorrespondingground-truthsequence.ThemeasuresTargetF1andActionF1quantifytheaveragemicroF1score
forthetarget(i.e.,theUIcomponentstobeinteractedwith)andtheaction(i.e.,theactionstobeperformedontheUI
components),respectively.TheF1scoreforeachinstanceiscomputedusingtheformula:
2×|pred∩gold|
F1=
|pred|+|gold|
where |pred| representsthesizeofthesetofpredictedtargetsoractions,and |gold| denotesthesizeofthesetof
ground-truthtargetsoractions.TheaveragemicroF1scoreiscalculatedacrossallinstancesforeithertargetsoractions.
Baselines:Weconsiderfourbaselinesforconvertingnaturallanguageintosemanticmeaningrepresentations,
whichcompriseactionsandtargets.ThesebaselinesarevanillaSeq2Seq[6],BERT-LSTM[67],VoicifyParser[60],
andWangetal.’swork[61].IntheoriginalworkbyVoicify,allthreebaselinesemploydeeplearningmodelstrained
ondatasetssynthesizedusingtheOvernightmethod[63].Thismethodgeneratestrainingsetsbasedonpredefinedlists
ofactionsandtargetsthataredesignedforevaluationscenariosinVoicify.Toensureafaircomparison,wemodified
theseliststoincludetheactionsandtargetspresentinourtestdataset.Wethenre-synthesizethetrainingset,which
includes1,384instances,usingtheOvernightmethod,adheringtotheimplementationoutlinedinVoicify’swork.Wang
etal.’swork[61]wasthefirsttoincorporateLLMsforinteractingwithmobileinterfaces.Weincorporatedthe2-shots
LLMpromptstheydemonstratedinthepaperformappinginstructiontoUIactions.Additionally,wehaveenhanced
theirmodelbyintegratingthemoreadvancedGPT-4,whichalsoinlinewiththeoneweusedinGptVoiceTasker.
4.1.2 EvaluationResult. Table1presentstheresultsofourtechnicalexperiments.Overall,GptVoiceTaskersignifi-
cantlyoutperformsallbaselinemodelsacrossallmetrics,achievingan84.7%EMaccuracy,a91.7%ActionF1score,and
a84.7%TargetF1score.AmongthebaselineswithouttheLLMs,theVoicifyParserperformsthebest,aligningwiththe
resultsreportedinitsoriginalpaper[60].However,itsperformancesufferswhenfacedwithlinguisticvariationsinour
newtestset.Forinstance,whilethecommand“back”iscorrectlyinterpretedas“(PRESS,back)”,thephrase“return
tolastpage”,whichrepresentsthesamecommand,isincorrectlyparsedas“(SWIPE,DOWN)”.BothBERT-LSTM
andSeq2Seqmodelsencountersimilarissues,largelybecausetheysharearchitecturalandtrainingsimilaritieswith
theVoicifyParser,yetperformevenworseduetoVoicifyParserbeingspecificallyoptimizedfortaskcompletionon
Androidsystems.ThemethodbyWangetal.[61]demonstratesthehighestcapabilityamongthebaselines,courtesyof
theLLM’sintervention,effectivelyrectifyingtheerrorspreviouslynoted.However,thelackofthechain-of-thought
andleast-to-mostprompttechniquesoccasionallyleadstoinaccuracies.Thisisevidentininstanceswherethesystem
misinterpretstheintendeddirectionincommands,suchasconfusing"DOWN"with"UP,"orwhenitcannotadequately
differentiatebetweenactionslike"PRESS,""ENTER,"or"OPEN"whenvariousverbsareemployedinthecommands.
BenefitingfromtheintegrationofLLMandthepromptingtechniques,ourGptVoiceTaskerexcelsathandling
linguisticvariants,consistentlyderivingtheintendedactionandtargetregardlessofvariationsintheinput.TheAction
F1scoreforGptVoiceTaskerreaches91.7,indicatingitsenhancedabilitytopredictactionsacrossvariouslinguistic
patterns.Moreover,weobservedthatLLMeffectivelylearnsthetrueassociationsbetweenactionsandtargets,thereby
excellingattargetpredictionaswell.Forinstance,PRESSisexclusivelypredictedwithUIbuttons,ENTER_TEXT
islinkedsolelywithtextinputfields,andOPENcorrespondstoappnames.Incontrast,thebaselinesoftenlearns
incorrectassociationsandoutputswrongtargetpredictions.OurexperimentalresultsdemonstratethatGptVoiceTasker
possessessuperiorcapabilitiesforunderstandingandaccuratelyprocessingvariouslinguisticvariations,highlighting
itsadaptabilityinreal-worldscenarios.
ManuscriptsubmittedtoACMGPTVoiceTasker:LLM-PoweredVirtualAssistantforSmartphone 13
Table1. TheexperimentresultsofdifferentbaselinescomparedwithGptVoiceTaskerinthreemetrics.
Models EMAccuracy(%) ActionF1(%) TargetF1(%)
Seq2Seq 25.2 47.6 35.6
BERT-LSTM 41.4 59.7 57.3
VoicifyParser 47.5 64.0 58.8
Wangetal.[61] 79.9 85.4 83.4
GptVoiceTasker 84.7 91.7 84.7
4.2 DatabaseExecutionEvaluation
Table2. Savedtaskexecutionevaluationresultfordirectmatchtasksandparameterisedtasksacross5categories.
Category AverageNumberofAutomatedSteps SuccessRate(%)
DirectMatch Parameterised
MessageFriends 4.33 93.33 86.67
ListentoMusic 5.27 80.00 73.33
SetanAlarm 5.73 73.33 53.33
CheckWeather 5.07 80.00 73.33
GetDirections&Map 5.53 86.67 73.33
Average 5.19 82.67 72.00
4.2.1 ExperimentalSetup&Metric. Inthisexperiment,weassessedGptVoiceTasker’sabilitytoautomatetasksusing
theusage-basedexecutionmodule.Weinitiallyidentifiedthefivecommonsmartphoneapplicationcategories,asshown
inpreviousstudy[4].Withineachapplicationcategory,werandomlyselectedfivepopularapplicationsfromtheGoogle
PlayStore,withdownloadsrangingfrom1milliontoover1billion.Foreachselectedapp,weidentifiedthreefeatures
introducedbythedevelopersintheirPlayStoredescriptions.Eachfeaturewasthenusedtocreatebothadirectmatch
task,involvingastraightforwardmatchbetweenusercommandsandcorrespondingappactions,andaparameterized
task,requiringGptVoiceTaskertoperformkeywordsubstitutionstocompletethetasksuccessfully,asshownin
Section3.3.3.Forcreatingthedirectmatchtestcases,weparaphrasedeachsavedcommandusingstate-of-the-art
paraphrasingtoolQuillbot3,asin[56].Inthecaseofparameterizedtasks,wesubstitutedoneentityintheparaphrased
commandwithanotherentitythathassimilarsemantic.Forexample,considerthesavedtask“Getdirectionstothe
nearestsupermarket”.Inthiscase,thedirectmatchingtaskwouldbe“Findthenearestsupermarket’slocation”,whilethe
parameterizedtaskwouldinvolvesubstituting“restaurant”for“supermarket”,resultingin“Findthenearestrestaurant’s
location”.Thisprocessresultedinatotaloffiveappcategories,eachcategorycontains15directmatchtasksand15
parameterisedtasks.Thesetasksinvolve4to7steps,withanaverageof5.19stepspertaskasillustratedinTable2.All
taskscanbeautomatedwithonevoicecommandwiththesaveduserappusagepatterns.Foradetailedlistoftheapps
andfeaturesusedintheexperiment,pleaserefertoourGitHubrepository4.
Topopulatethetransitiongraphandstorescreendescriptions,wemanuallynavigatedthrougheachscreenin
everyapplicationusingGptVoiceTasker.Subsequently,weconfiguredthesavedcommandstoreachtherespective
screensasthegroundtruth.Weusedthesuccessrateastheprimarymetric,eachtestcaseismarkedassuccessif
GptVoiceTaskercansuccessfullyopenedthedesiredfeatureusingasinglecommand.
3https://quillbot.com/
4https://github.com/vuminhduc796/GPTVoiceTasker
ManuscriptsubmittedtoACM14 Vuetal.
4.2.2 Results. Table 2 illustrates the accuracy of our saved task execution modules. Our findings indicate that
GptVoiceTaskerachievedanimpressivelevelofautomation,successfullyhandling82.7%ofexactmatchtasksand
72.0%ofparameterizedtasks.Notably,GptVoiceTaskerexhibitedexceptionalperformanceintasksrelatedtomessag-
inganddirections&mapsapplications.Thissuccesscanbeattributedtotherelativelystaticnatureoftheseapps,where
userinterfacesmaintainaconsistentstructure.OurresultsunderscoreGptVoiceTasker’sproficiencyincommand
analysis,semanticmatchingtosavedtasks,andparameterizedphrasesubstitutionwithinthesecontexts.However,the
accuracyofGptVoiceTaskerdiminishedwhenconfrontedwithtasksrelatedtosettingalarms.Tobetterunderstand
therootcausesofthisdeclineinperformance,weconductedanerroranalysisonthefailedtestcases.Severalkey
issuesemerged:
ComplexParameterizedTasks:Forparameterizedtaskswithadditionalsteps,suchassettinganalarmfor7:30instead
of7:00,GptVoiceTaskerstruggledduetotheextrastepinvolvedinselectingtheminutes,whichwasonaseparateUI
element.FurtherworksincludemakingGptVoiceTaskeradaptabletotheseadditionalstepsintheautomationprocess.
Pop-upsAdsandUnusualUIElements:Certainapplicationspresentedpop-upsadsandunusualUIelementsinrun
timethatwerenotencounteredduringtheinitialtask-savingprocess.Consequently,GptVoiceTaskerfaceddifficulties
incompletingthesetasks.Toimprovetherobustnessofourapproach,werecommendexploringtheintegrationofa
deeplearningmodeltodetectandhandlesuchadwidgetsandunusualUIelements,asin[22,40].
5 USERSTUDY
Todemonstratethepracticalutilityofourtool,weconductedauserstudytoevaluatetheholisticperformanceof
theGptVoiceTaskersystemwithinreal-worldscenarios.Ourevaluationinvolvedacomparativeanalysisagainsttwo
baselinesystems:1)VoiceAccess[68],theofficialvoiceassistantproductdevelopedbyGoogle,withover100million
downloads,and2)Voicify[60],thestate-of-the-artresearchproductendeavorincorporatingdeeplearningmodelsto
enhancecommandcomprehension.Thisstudypursuedathreefoldobjective:i)establishaperformancebenchmark
foruserinteractionsutilizingtheGptVoiceTaskersystemasopposedtotheaforementionedbaselinesystems,ii)
juxtaposeuserfeedbackconcerningthecognitiveloadandoverallusabilityoftheGptVoiceTaskersystemagainstthe
baselinesandiii)capturequalitativeinsightsfromparticipants,thusenablingtheidentificationofpotentialavenuesfor
enhancingtheGptVoiceTaskersystem.Inordertoachievetheseobjectives,werecordedthetaskcompletiontimes
fortasksundertakenusingboththeGptVoiceTaskersystemandthebaselines.Furthermore,acomprehensivepost-
experimentinterviewwasconductedwitheachparticipant,facilitatingthecollectionandanalysisofbothquantitative
andqualitativefeedback.
Table3. Thelistoftasksforuserevaluation.
No. Task #Steps AppName #Downloads
1 Checktheweatherwithinaparticularcity. 6 BOMWeather 1M+
2 Searchforaspecificsongandplayit. 6 AppleMusic 100M+
3 Createanoteandwrite"Helloworld"anddeleteit. 8 Notes 1M+
4 Checkforanunreadmessage,replywithames- 8 Messages 1B+
sageanddeletetheconversation.
5 Searchforapizzastore,andcompletetheorder. 10 UberEats 100M+
6 Createanewalarmandsaveit. 10 ChallengesAlarmClock 1M+
ManuscriptsubmittedtoACMGPTVoiceTasker:LLM-PoweredVirtualAssistantforSmartphone 15
Table4. Averagenumberofautomatedstepsbyallparticipantsineachtask.
Average Task1 Task2 Task3 Task4 Task5 Task6
#StepsAutomated 2.22 2.67 2.00 1.67 2.17 2.50 2.33
5.1 Tasks
Wedesigned6experimentaltasks,encompassingabroadspectrumofthemostcommoninteractionsperformedon
thescreen,rangingfromtappingandswipingtoenteringtext.Eachtaskwasstructuredtocomprisebetween6to10
sequentialsteps.ThedetailedlistofthesetasksisoutlinedinTable3.
5.2 Participants
Werecruited18participants,consistingof10malesand8females,agedbetween18and31yearsoldforourstudy.8
participantsarenativeEnglishspeakerwhileallotherparticipantsareproficientinEnglish.Allparticipantspossess
acommendableleveloffamiliaritywithtechnologicaldevicesandactivelyusesmartphonesintheirdailyroutines.
WhileparticipantsexhibitedexposuretovirtualassistantslikeSiriorGoogleAssistant,nonewereacquaintedwith
utilizingassistivetoolsforsmartphonecontrolviavoicecommands.Specifically,noneoftheparticipantshadprior
experiencewithanyoftheexperimentaltoolsemployedinourstudy.Thisparticipantselectionwasdeliberate,asour
studysoughttogaugethelearnabilityaspectoftheexperimentaltools.EachparticipantreceivedaUSD$30giftcard
fortheparticipation.
5.3 Procedure
Weconductedface-to-faceuserevaluationsusinganAndroiddeviceastheexperimentaltool.Onthisdevice,wehad
thegraphofeachexperimentalapppopulated,whichincludethemajorityofapppagesandnavigationwithinthe
app.Atthestartofthesessions,participantswereintroducedtoallexperimentaltoolsviademonstrativevideos.The
preliminaryphaseinvolvedpracticingbasictasksacrossalltools,enhancingparticipants’familiaritywithstep-by-step
instructionsandinformativewalk-throughvideos.WealsousethesearchingforexercisetasksinFig.1asthepractice
tasks,allowinguserstoachievethistaskusingeachofthetool.
Afterthat,participantsindependentlyexecutedsixdistincttaskswithnoexperimenterintervention.Eachtoolwas
employedforthecompletionoftwotasks,andparticipantsremainedunawareofwhichtoolwasdevelopedbyus.
Tomitigateanypotentialbiases,theorderoftasksandthetoolsusedweresystematicallycounterbalancedforeach
participant[19].
Weappliedatimecapof60secondsperstep.Werecordedthetimetakentofulfileachtask,includingthecut-off
timetoperformquantitativeanalysis.Wecollected108dataentriessinceeachofthe18participantshasfinished6
tasks.Intheend,usingtheSystemUsabilityScale(SUS)[8]formwitha5-pointLikertscale,weevaluatetheusability
of GptVoiceTasker,comparedtoVoiceAccessandVoicify.Inaddition,weinvestigatedthecognitiveloadwhen
experimentingwitheachtoolusingtheNASA-TLX[27]formwitha7-pointLikertscale.Lastly,wecollectedqualitative
feedbackonwhichparttheylikedthemostaboutGptVoiceTaskerandwhatmightimprovethesystem.
5.4 Result
5.4.1 OverallUserPerformance. InFig.4,wepresenttheaveragetaskcompletiontimesforeachexperimentaltool.Our
GptVoiceTaskerstandsoutwithanaveragecompletiontimeof92.5seconds,significantlysurpassingVoiceAccess
ManuscriptsubmittedtoACM16 Vuetal.
Fig.4. TheaveragetimetakentocompleteeachtaskusingGptVoiceTaskerandthebaselinesinseconds.
(162.2seconds)andVoicify(141.9seconds).ThissubstantialimprovementinGptVoiceTasker’sperformancecanbe
attributedtotwoprimaryfactors.Firstly,GptVoiceTaskerisbetteratcomprehendinguserintentionsandmapping
usercommandstothecorrectactionsonspecificUIelements,regardlessofthecommandformat.Incontrast,baseline
toolsoftendemandspecificcommandformats,introducingerrorsinvarioususages.Thisissuecausedextratimecostsas
participantsneededtoseekdifferentwaystoexpresstheirintentionswiththebaselinetools.Forexample,participants
triedtotaptheoptionbutton,intheNotesappwithVoiceAccessbymultipleattemptssuchas“pressontheoption
button”,“pressthethree-doticons”,“tapiconforoptions”beforesuccessfullygivetherightcommand“tapoption”.Secondly,
GptVoiceTaskeroptimizestheperformancebyautomatingseveralstepsinoneusercommand,asshowninTable4.
Onaverage,theparticipantssaved2.2stepsacrossallsixtasks.Forinstance,inTask2,GptVoiceTaskerefficiently
automatedtheprocessofsearchingforLoveYourselfsong(asinFig.5(B)),drawingfromapreviouslystoredaction
designedforsearchingothersongs.Thiseliminatedtheneedforthreestepsrequiredforin-appnavigation.However,
someparticipantsdidnotrealizethattheycouldeasilytriggerthesavedtasks,leadingtoamissedopportunityfora
significantperformanceboost.Inaddition,GptVoiceTaskerrelatestonetworklatencywhensendingandreceiving
datafromtheLLMsAPIendpoint.Thisissuecouldbemitigatedwithabetternetworkconnection.
5.4.2 Cognitive Load & Usability Ratings. Fig. 5(A) presents an overview of participant feedback regarding their
cognitiveloadlevelsforeachsystem,assessedusingtheNASA-TLXform.Weconductedaone-wayANOVAstatistical
analysis,confirmingthatGptVoiceTaskersignificantlyenhancesuserperformancewhilereducingfrustrationlevels
(p<0.001).Participantsreporteddecreasedmentaldemand,temporaldemand,andeffortwhenusingGptVoiceTasker
incomparisontothebaselinesystems.ThisresultsignifiesasubstantialimprovementinGptVoiceTasker’sabilityto
reducethecognitiveloadrequiredforoperation,aligningwithourdesigngoal.
ToassessGptVoiceTasker’susabilityincomparisontothebaselinesystems,weemployedaone-wayANOVA
statisticalanalysisoncollectedSystemUsabilityScale(SUS)scores,asdepictedinFig.6.Theanalysisverifiedthe
enhanced usability of the voice control system, with GptVoiceTasker achieving an average SUS score of 79.861,
surpassingVoicify(47.917)andVoiceAccess(36.528).ParticipantsfoundGptVoiceTaskereasytouse(p<0.001)and
well-integrated(p<0.001),leadingtoincreasedconfidencelevels(p<0.001).Thisremarkableoutcomecanbeattributed
toGptVoiceTasker’sabilitytoeffortlesslycomprehendnaturalhumancommands,reducingtheneedforextensive
ManuscriptsubmittedtoACMGPTVoiceTasker:LLM-PoweredVirtualAssistantforSmartphone 17
Fig.5. ThecomparisonbetweenGptVoiceTasker,Voicify,andVoiceAccessforA)theaveragecognitiveloadwhenusingNASA-TLX
form(lowerisbetter)*:p<0.01,**:p<0.001andB)Task2fromtheuserevaluationwithGptVoiceTaskerandotherbaselines.
Fig.6. ThecomparisonbetweenGptVoiceTasker,Voicify,andVoiceAccessfortheSystemUsabilityScale(SUS).*:p<0.01,**:p<
0.001.
trainingandpractice.Thelowerlikelihoodofmisinterpretingusercommandsalsocontributedtothepositiveresults.
comparedtothebaselines,indicatingitsquicklearnabilityandusersatisfaction,thuspromotingfrequentusage.
5.4.3 QualitativeFeedback. Inthissection,wecollatequalitativefeedbackfromparticipantsaftertheexperiment.
Overall,theparticipantsaresatisfiedwiththetool,aswellasprovidingsuggestionsforfurtherimprovements.
Abilitytopreciselyinterpretandexecutehumancommand.Participantsexpressedenthusiasmabouttheremarkable
abilityof GptVoiceTaskertointerprethumancommandsnaturally,enhancingtheoverallsystem’sintuitiveness.
P1andP12highlightedthattheycouldissuecommands“intheirpreferredmanner” and“conversenaturally” with
GptVoiceTasker.Thisaddressescognitiveoverloadconcerns,asP4appreciatedthe“stress-freeexperience”,andP6and
P7foundGptVoiceTaskermore“comfortabletouse”.Forinstance,whenaddinganewnote,userscouldsimplysay“add
anewnote” topromptGptVoiceTaskertopresstheaddbuttononthescreen.Moreover,participantswereimpressed
byourtool’saccuracyinhandlinguserinputerrors.P3notedtheirsatisfactionwithhowGptVoiceTasker“canstill
executethecorrectactionevenwhenImakemistakesinmycommands”.BothP4andP17highlightedthetool’susefulness
ManuscriptsubmittedtoACM18 Vuetal.
indailytasks,asiteliminatestheneedto“exercisecautionandstayalert”wheninteractingwithGptVoiceTasker.These
feedbackremarksstronglyaffirmthepracticalityofourapproachinreal-worldtaskscenarios.Incontrast,traditional
approachestypicallydemandfixedinputformats,makingthemill-suitedforreal-worldscenarioswhereuserinputcan
varysignificantly.
Automated execution helps accelerate tasks and improve user experiences. Participants offered positive feedback
regardingtheuseofsavedtaskautomation,highlightingitssignificantimpactonefficiencyanduserexperiences.P11
mentionedthatthisfeatureis“acceleratingthetasks” whileP13emphasizedthepotentialutilityof GptVoiceTasker
duringphysicalactivities,statingitwouldbe“reallyusefulwhenIworkout”.P5appreciatedthisfeature,describingitas
“perfectforvoice-interactingtools”,asitmitigatestheinherentchallengesofvoicecommandinteractions.Additionally,P18
praisedthefeature,notingthattasksbecame“fairlyeasy” withitsimplementation,indicatingsignificantperformance
improvements.This,combinedwiththeadvancedcapabilitytounderstanduserintentions,enhancestheintuitiveness
ofvoice-basedinterfaces.Whenusingasmartphone,usersoftenhaveaspecifictaskinmind,suchassettingan
alarmorcheckingthenews.Unlikeotherapproachesthatrequireuserstoperformadditionalstepstotranslatetheir
intentionintoexecutablecommandsthatavoiceinterfacecanunderstandandexecute,GptVoiceTaskercandirectly
executethesetaskswithoutcausingadditionalmentalstress.However,usersalsoprovidedvaluablesuggestionsfor
enhancement.TheyexpressedthedesireforGptVoiceTaskertosuggestexecutablesavedtasksanddisplayalistof
savedtasks.Furthermore,participantssuggestedimprovingtheintroductionofthisfeature,asP4noteditwas“not
familiaratfirst”,andP6emphasizedtheneedfor“betterintroduction.” Theseinsightsunderscoreopportunitiestorefine
thefeature’susabilityanduseronboarding,ultimatelyenhancingoverallusersatisfaction.
Suggestionsforenhancinguserexperience.Participantsprovidedvaluablesuggestionsforimprovingtheintuitiveness
of GptVoiceTasker.RegardingUIdesign,P14recommendedtheinclusionofa“livetranscription” featuretodisplay
recognizedvoicecommands.Thiswouldhelpusersconfirmthattheircommandswerecorrectlyreceivedandmake
necessaryadjustmentsifneeded.Furthermore,P1andP15suggestedincorporatinga“loadingindicator” tosignify
ongoingexecutions,addressinglatencyissuescausedbyexecutiondelays.Intermsoffunctionality,P7proposed
displayingalistofavailabletasksassuggestions,enhancinguserinteraction.Additionally,P15discussedthepotential
foraninterfacethatallowsuserstomodifysavedtasks,providinggreatercustomization.Lastly,participantsP7and
P12suggestedmakingtheaudiofeedbackfromGptVoiceTaskerclearer.Thesesuggestionsholdsignificantvaluefor
GptVoiceTasker’scontinuousimprovement,aimingtodeliveramoreseamlessuserexperience.
6 DISCUSSION
WeintroducedGptVoiceTaskerasapioneeringexampleofleveragingLLMsinthedevelopmentofspeech-based
virtualassistants.Inthissection,wedelveintotheimplicationsandlimitationsof GptVoiceTasker.
Towardswidespreadadoptionofthevoice-centricinterface.Advancementsinnaturallanguageunderstanding,LLMslike
GPTandBard[54],havecatalyzedtheshifttowardsvoice-centricinterfaces,extendingtheirusebeyondsmartphonesto
wearabledeviceslikesmartwatchesandAR-VRhead-mounteddisplays.Theseinterfacesnotonlyseamlesslyintegrate
softwareintodailylifebutalsosignificantlyenhanceaccessibilityforuserswithvisualimpairments[71].However,
visual-manualmethodssuchastappingonsmartphonesormouse-clickingindesktopshavebeenpreferredfortheir
speedandaccuracy.Therefore,thetransitionfromthedominantvisual-manualinteractiontoavoice-drivenapproach
presentschallenges,stemmingnotonlyfromthefundamentaldifferencesbetweentheseinteractionmodesbutalsofrom
userunfamiliaritywithvoice-basedinteractions.OurtoolovercomesthisbyusingLLMstoenhancetheintuitiveness
ofvoiceinteractions.Thisallowsformoreintelligentmappingofuserintentionstovisualelements,easingtheshiftto
ManuscriptsubmittedtoACMGPTVoiceTasker:LLM-PoweredVirtualAssistantforSmartphone 19
voice-assistedinteractionsandsuggestingwideradoptionofvoice-centricinterfaces.Despitethepromise,challenges
suchastheeffectivenessofvoicerecognitionindiverseenvironmentsstillpersist.Addressingthesewillbecrucialfor
thebroaderadoptionofvoice-centricinterfaces,likesmarthomesandhealthcare.Thistransition,whilechallenging,
opensnewavenuesforuserinteractionandemphasizestheneedforcontinuedresearchintheHCIdomain.
LLMsfortaskautomationonuservisualinterfaces.ResearchhashighlightedthecapabilityofLLMstoprovide
reasoningbasedontheUIlayout,applyingtotaskautomationandtestingtools[24].Thesemodelsshowremarkable
capabilitiesinincorporatingextensiveknowledgeconcerningprevalentappdesignprinciplesandrecognizingstandard
mobileinterfaceelements,includingthetoolbar,navigationdrawer,andbottomnavigationbar[44]tosignificantly
enhanceproficiencyinfacilitatingprecisein-appnavigation.Ourstudyhighlightedthevitalroleofspatialinformation
andhierarchicalUIrepresentationsforLLMsincomprehendingsemanticconnectionsbetweendiverseUIelements,
particularlyusefulforelementslackingtextualinformationlikeunlabelediconsorimages.Inouruserstudy,when
taskedwithdeletingamessagelackingavisibledeletebutton,LLMintelligentlysuggestedinitiatingtheprocessby
pressingtheunlabellediconbuttonatthetopright,typicallythelocationoftheoptionbutton,andthenselecting
“delete” fromtheensuingoptionslist.Thecoreofthisresearchliesinthetransformationofvisualinterfacesinto
textualdescriptionsthatLLMscanprocess,acriticalstepforenablingeffectivetaskexecutionbasedonuserinputs.
Futureresearchshouldaddressthemodels’limitationsinunconventionalUIscenariosandfocusonexpandingtheir
adaptabilityacrossvariedinterfacedesignsandcomplexusertasks.SuchprogressinLLMcapabilitiesispivotalfor
advancinguserinterfaceautomation,leadingtomoreuser-friendlyandefficientdigitalexperiences.
TowardsresponsibleAIinsoftwaresystems.Inrecentyears,theremarkableadvancementsinLLMshaveenabled
theseamlessintegrationofAIintovarioussoftwareandsystems,withtheflexibilityforfine-tuningandfew-shot
learningtotacklediverseandchallengingdownstreamtasks.However,thisintegrationraisessignificantconcerns,
particularlyregardingdataprivacyandsecurity[58].TheverynatureofAI-integratedsystemsrequiresaccesstodata,
potentiallyputtingsensitiveorconfidentialinformationatrisk.Putinthecontextofvoiceassistantsonsmartphones,
usersarescepticalassmartphonescontainmanypersonalandsensitivedata[30].ToolslikeGptVoiceTaskercanread
suchon-screendataandfurtherprocessthemtoLLMs.Tomitigatetheserisks,severalessentialmeasuresmustbe
implementedtonotonlyprotectusersbutalsobuildtrust,fosteringgreateradoptionofAI-baseduserinteractive
systems.First,datade-identificationtechniques[45]shouldbeappliedforon-screensensitivedata,especiallyforuser
informationsuchaspasswordsandpersonaldetails.Thiscanbesetuppriortousagebytheusertohidespecific
keywordsortermsorbedetectedinreal-timeusingheuristicsanddeeplearningmodels[46].Furthermore,transparent
userconsentmechanismsarecrucialtoinformingindividualsaboutdataaccess,usage,andpurposes,ensuringthey
maintaincontrolovertheirinformation.Usersshouldhavetheabilitytogrant,deny,oradjustpermissionsasneeded
toprotecttheirprivacywithinAI-integratedsystems.
Limitations.Thecurrentapproachposesseverallimitations.Firstly,theusage-basedexecutionreliespriorusagein
theparticularapplication,thereforeitisinapplicabletounusedapps.Toaddressthischallenge,ourfutureworkaimsto
developamoregeneralizedapproachtoapplicationusage,categorizingappsbytheirprimaryfunctions.Forinstance,
wecoulddeviseastandardizedsetofstepsforsearchingandplayingasongthatcouldbeapplicableacrossvarious
musicapplications,therebysimplifyingtheprocessfornewandunfamiliarapps.Secondly,whileoursystemshows
proficiencyonAndroidsmartphones,itseffectivenessonotherAndroid-baseddevicesremainsuntested.Aspreviously
indicated,there’spotentialtoextendthisvoice-centricinterfacetoabroaderrangeofgadgets,includingsmartwatches
andAR-VRhead-mounteddisplays.AlthoughthevocalcommandsmightbeprocessedbyLLMsacrossdevices,theuser
interfaces(UIs)ofthesedevicescanvarysignificantlyintheirlogicandlayout.Forinstance,thestreamlinedinterface
ManuscriptsubmittedtoACM20 Vuetal.
ofasmartwatchmightnecessitatemoreconciseoutputduetoitssmallerscreen,whiletheimmersiveenvironment
ofanAR-VRdevicecouldintroducenewinteractionparadigms.ThisdiversityinUIdesignandinteractionmethods
acrossdifferentdevicesrequiresmoreinvestigationsinfutureworks.
7 CONCLUSION
Inthispaper,weintroduceGptVoiceTasker,aninnovativevirtualassistantdesignedtoenhanceuserinteractionsand
performanceonsmartphones.GptVoiceTaskerleveragedadvancedpromptengineeringtechniquestoharnessthe
capabilitiesofLargeLanguageModelsforinterpretingusercommandsandconstructinglogicalreasoningcomponents.
GptVoiceTaskerfurtherstreamlineduserinteractionsbyautomaticallystoringprevioususagestoautomatesubsequent
repetitivetasks.Ourexperimentsdemonstratedoutstandingcommandinterpretationaccuracyandtheeffectiveness
ofautomatedexecutionbasedonhistoricalusage.Inaddition,theuserevaluationvalidatedGptVoiceTasker’shigh
usabilityinreal-worldtasksbyimprovinguserperformanceandreducingmentalstressload,aligningwithourdesign
objectives.Asanopen-sourceproject,GptVoiceTaskerpavesthewayforfutureenhancementsinvirtualassistant
intuitiveness,contributingtotheevolutionofhuman-computerinteractions.Furtherresearchincludesapplyingour
versatiledatabaseexecutionapproachacrossdiverseplatformsandoperatingsystems,aswellasexploringinnovative
promptengineeringtechniquestofine-tuneLLMsforvariousreasoningtasks.
REFERENCES
[1] VoiceAccess.2022.TroubleshootVoiceAccess. https://support.google.com/accessibility/android/answer/6377053?hl=en#:~:text=If%20you%20have%
20trouble%20starting,Access%20from%20the%20lock%20screen.
[2] LeonardoAngelini,JürgenBaumgartner,FrancescoCarrino,StefanoCarrino,MaurizioCaon,OmarAbouKhaled,JürgenSauer,DenisLalanne,
ElenaMugellini,andAndreasSonderegger.2016.AComparisonofThreeInteractionModalitiesintheCar:Gestures,VoiceandTouch(IHM’16).
AssociationforComputingMachinery,NewYork,NY,USA,188–196. https://doi.org/10.1145/3004107.3004118
[3] Apple.[n.d.].Siri. https://www.apple.com/au/siri/
[4] DenizArsan,AliZaidi,AravindSagar,andRanjithaKumar.2021. App-BasedTaskShortcutsforVirtualAssistants.InThe34thAnnualACM
SymposiumonUserInterfaceSoftwareandTechnology.1089–1099.
[5] GoogleAssistant.2022.Assistant. https://assistant.google.com/intl/en_au/platforms/phones/
[6] DzmitryBahdanau,KyungHyunCho,andYoshuaBengio.2015. Neuralmachinetranslationbyjointlylearningtoalignandtranslate.In3rd
InternationalConferenceonLearningRepresentations,ICLR2015.
[7] LijunBai.2022.Researchonvoicecontroltechnologyforsmarthomesystem.InProceedingsoftheAsiaConferenceonElectrical,PowerandComputer
Engineering.1–7.
[8] AaronBangor,PhilipTKortum,andJamesTMiller.2008.Anempiricalevaluationofthesystemusabilityscale.Intl.JournalofHuman–Computer
Interaction24,6(2008),574–594.
[9] PatrickBareiß,BeatrizSouza,Marcelod’Amorim,andMichaelPradel.2022.Codegenerationtools(almost)forfree?astudyoffew-shot,pre-trained
languagemodelsoncode.arXivpreprintarXiv:2206.01335(2022).
[10] BrettBarrosandTheoGoguely.2021.Auto-suggestQueryRefinementUsingN-bestAlternativeHomophones.(2021).
[11] AditiBhalerao,SamiraBhilare,AnaghaBondade,andMonalShingade.2017.SmartVoiceAssistant:auniversalvoicecontrolsolutionfornon-visual
accesstotheAndroidoperatingsystem.Int.Res.J.Eng.Technol4,2(2017).
[12] MichaelBraunandFlorianAlt.2019. AffectiveAssistants:AMatterofStatesandTraits.InExtendedAbstractsofthe2019CHIConferenceon
HumanFactorsinComputingSystems(Glasgow,ScotlandUk)(CHIEA’19).AssociationforComputingMachinery,NewYork,NY,USA,1–6.
https://doi.org/10.1145/3290607.3313051
[13] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,
AmandaAskell,etal.2020.Languagemodelsarefew-shotlearners.Advancesinneuralinformationprocessingsystems33(2020),1877–1901.
[14] AndreaBurns,DenizArsan,SanjnaAgrawal,RanjithaKumar,KateSaenko,andBryanA.Plummer.2022.ADatasetforInteractiveVisionLanguage
NavigationwithUnknownCommandFeasibility.InEuropeanConferenceonComputerVision(ECCV).
[15] LeChen,Pei-HungLin,TristanVanderbruggen,ChunhuaLiao,MuraliEmani,andBronisdeSupinski.2023.LM4HPC:TowardsEffectiveLanguage
ModelApplicationinHigh-PerformanceComputing.InInternationalWorkshoponOpenMP.Springer,18–33.
[16] SenChen,LinglingFan,ChunyangChen,TingSu,WenheLi,YangLiu,andLihuaXu.2019.StoryDroid:AutomatedGenerationofStoryboardfor
AndroidApps.In2019IEEE/ACM41stInternationalConferenceonSoftwareEngineering(ICSE).596–607. https://doi.org/10.1109/ICSE.2019.00070
ManuscriptsubmittedtoACMGPTVoiceTasker:LLM-PoweredVirtualAssistantforSmartphone 21
[17] WenhuChen,XueguangMa,XinyiWang,andWilliamWCohen.2022.Programofthoughtsprompting:Disentanglingcomputationfromreasoning
fornumericalreasoningtasks.arXivpreprintarXiv:2211.12588(2022).
[18] EricCorbettandAstridWeber.2016.WhatcanIsay?Proceedingsofthe18thInternationalConferenceonHuman-ComputerInteractionwithMobile
DevicesandServices(2016).
[19] VenitaDePuyandVanceWBerger.2014.Counterbalancing.WileyStatsRef:StatisticsReferenceOnline(2014).
[20] AndroidDevelopers.2022. AccessibilityNodeInfo. https://developer.android.com/reference/android/view/accessibility/AccessibilityNodeInfo.
AccessibilityAction
[21] AndroidDevelopers.2022.AccessibilityService. https://developer.android.com/guide/topics/ui/accessibility/service
[22] FengDong,HaoyuWang,LiLi,YaoGuo,TegawendéFBissyandé,TianmingLiu,GuoaiXu,andJacquesKlein.2018.Frauddroid:Automatedad
frauddetectionforandroidapps.InProceedingsofthe201826thACMJointMeetingonEuropeanSoftwareEngineeringConferenceandSymposiumon
theFoundationsofSoftwareEngineering.257–268.
[23] WenqiFan,ZihuaiZhao,JiatongLi,YunqingLiu,XiaoweiMei,YiqiWang,JiliangTang,andQingLi.2023.Recommendersystemsintheeraoflarge
languagemodels(llms).arXivpreprintarXiv:2307.02046(2023).
[24] SidongFengandChunyangChen.2023.PromptingIsAllYourNeed:AutomatedAndroidBugReplaywithLargeLanguageModels.arXivpreprint
arXiv:2306.01987(2023).
[25] StephenGilbert,HughHarvey,TomMelvin,ErikVollebregt,andPaulWicks.2023.LargelanguagemodelAIchatbotsrequireapprovalasmedical
devices.NatureMedicine(2023),1–3.
[26] BruceGolden.1976.Shortest-pathalgorithms:Acomparison.OperationsResearch24,6(1976),1164–1168.
[27] SandraGHart.2006.NASA-taskloadindex(NASA-TLX);20yearslater.InProceedingsofthehumanfactorsandergonomicssocietyannualmeeting,
Vol.50.SagepublicationsSageCA:LosAngeles,CA,904–908.
[28] JuliaHirschbergandChristopherD.Manning.2015.Advancesinnaturallanguageprocessing.Science349,6245(2015),261–266.
[29] MatthewBHoy.2018.Alexa,Siri,Cortana,andmore:anintroductiontovoiceassistants.Medicalreferenceservicesquarterly37,1(2018),81–88.
[30] SpyrosKokolakis.2017.Privacyattitudesandprivacybehaviour:Areviewofcurrentresearchontheprivacyparadoxphenomenon.Computers&
security64(2017),122–134.
[31] TsviKopelowitzandElyPorat.2018.Asimplealgorithmforapproximatingthetext-to-patternhammingdistance.In1stSymposiumonSimplicityin
Algorithms(SOSA2018).SchlossDagstuhl-Leibniz-ZentrumfuerInformatik.
[32] YBalaKrishnaandSNagendram.2012.Zigbeebasedvoicecontrolsystemforsmarthome.InternationalJournalonComputerTechnologyand
Applications3,1(2012),163–168.
[33] RebeccaKrosnickandSteveOney.2022.ParamMacros:CreatingUIAutomationLeveragingEnd-UserNaturalLanguageParameterization.In2022
IEEESymposiumonVisualLanguagesandHuman-CentricComputing(VL/HCC).1–10. https://doi.org/10.1109/VL/HCC53370.2022.9833005
[34] GangLi,GillesBaechler,ManuelTragut,andYangLi.2022. LearningtoDenoiseRawMobileUILayoutsforImprovingDatasetsatScale.In
Proceedingsofthe2022CHIConferenceonHumanFactorsinComputingSystems(NewOrleans,LA,USA)(CHI’22).AssociationforComputing
Machinery,NewYork,NY,USA,Article67,13pages. https://doi.org/10.1145/3491102.3502042
[35] TobyJia-JunLi,AmosAzaria,andBradAMyers.2017.SUGILITE:creatingmultimodalsmartphoneautomationbydemonstration.InProceedingsof
the2017CHIconferenceonhumanfactorsincomputingsystems.6038–6049.
[36] YangLi,JiacongHe,XinZhou,YuanZhang,andJasonBaldridge.2020.MappingNaturalLanguageInstructionstoMobileUIActionSequences.In
AnnualConferenceoftheAssociationforComputationalLinguistics(ACL2020). https://www.aclweb.org/anthology/2020.acl-main.729.pdf
[37] YangLi,JiacongHe,XinZhou,YuanZhang,andJasonBaldridge.2020.MappingnaturallanguageinstructionstomobileUIactionsequences.arXiv
preprintarXiv:2005.03776(2020).
[38] JasonXinyuLiu,ZiyiYang,IfrahIdrees,SamLiang,BenjaminSchornstein,StefanieTellex,andAnkitShah.2023.Lang2LTL:TranslatingNatural
LanguageCommandstoTemporalRobotTaskSpecification.arXivpreprintarXiv:2302.11649(2023).
[39] Kuei-ChunLiu,Ching-HungWu,Shau-YinTseng,andYin-TeTsai.2015.Voicehelper:Amobileassistivesystemforvisuallyimpairedpersons.In
2015IEEEInternationalConferenceonComputerandInformationTechnology;UbiquitousComputingandCommunications;Dependable,Autonomicand
SecureComputing;PervasiveIntelligenceandComputing.IEEE,1400–1405.
[40] TianmingLiu,HaoyuWang,LiLi,XiapuLuo,FengDong,YaoGuo,LiuWang,TegawendéBissyandé,andJacquesKlein.2020. Maddroid:
Characterizinganddetectingdeviousadcontentsforandroidapps.InProceedingsofTheWebConference2020.1715–1726.
[41] VivianLiuandLydiaBChilton.2022.Designguidelinesforpromptengineeringtext-to-imagegenerativemodels.InProceedingsofthe2022CHI
ConferenceonHumanFactorsinComputingSystems.1–23.
[42] XiaoLiu,HaoYu,HanchenZhang,YifanXu,XuanyuLei,HanyuLai,YuGu,HangliangDing,KaiwenMen,KejuanYang,etal.2023.Agentbench:
Evaluatingllmsasagents.arXivpreprintarXiv:2308.03688(2023).
[43] ZheLiu,ChunyangChen,JunjieWang,XingChe,YuekaiHuang,JunHu,andQingWang.2023.Fillintheblank:Context-awareautomatedtext
inputgenerationformobileguitesting.In2023IEEE/ACM45thInternationalConferenceonSoftwareEngineering(ICSE).IEEE,1355–1367.
[44] ZheLiu,ChunyangChen,JunjieWang,MengzhuoChen,BoyuWu,XingChe,DandanWang,andQingWang.2023.ChattingwithGPT-3for
Zero-ShotHuman-LikeMobileAutomatedGUITesting.arXivpreprintarXiv:2305.09434(2023).
[45] ZhengliangLiu,XiaoweiYu,LuZhang,ZihaoWu,ChaoCao,HaixingDai,LinZhao,WeiLiu,DinggangShen,QuanzhengLi,etal.2023.Deid-gpt:
Zero-shotmedicaltextde-identificationbygpt-4.arXivpreprintarXiv:2303.11032(2023).
ManuscriptsubmittedtoACM22 Vuetal.
[46] EvgenyMyasnikovandAndreySavchenko.2019. Detectionofsensitivetextualinformationinuserphotoalbumsonmobiledevices.In2019
InternationalMulti-ConferenceonEngineering,ComputerandInformationSciences(SIBIRCON).IEEE,0384–0390.
[47] ChelseaMyers,AnushayFurqan,JessicaNebolsky,KarinaCaro,andJichenZhu.2018.PatternsforHowUsersOvercomeObstaclesinVoiceUser
Interfaces(CHI’18).AssociationforComputingMachinery,NewYork,NY,USA,1–7. https://doi.org/10.1145/3173574.3173580
[48] PrakashMNadkarni,LucilaOhno-Machado,andWendyWChapman.2011.Naturallanguageprocessing:anintroduction.JournaloftheAmerican
MedicalInformaticsAssociation18,5(2011),544–551.
[49] OpenAI.2023.APIReference-OpenAIAPI. https://platform.openai.com/docs/api-reference/introduction
[50] OpenAI.2023.GPT-4TechnicalReport.arXivpreprintarXiv:2303.08774(2023).
[51] LihangPan,ChunYu,JiaHuiLi,TianHuang,XiaojunBi,andYuanchunShi.2022. AutomaticallyGeneratingandImprovingVoiceCommand
InterfacefromOperationSequencesonSmartphones.InProceedingsofthe2022CHIConferenceonHumanFactorsinComputingSystems(NewOrleans,
LA,USA)(CHI’22).AssociationforComputingMachinery,NewYork,NY,USA,Article208,21pages. https://doi.org/10.1145/3491102.3517459
[52] GeonwooParkandHarksooKim.2018. Low-costimplementationofanamedentityrecognitionsystemforvoice-activatedhuman-appliance
interfacesinasmarthome.Sustainability10,2(2018),488.
[53] EthanPerez,DouweKiela,andKyunghyunCho.2021.Truefew-shotlearningwithlanguagemodels.Advancesinneuralinformationprocessing
systems34(2021),11054–11070.
[54] MdSaidurRahaman,MMAhsan,NishathAnjum,MdMizanurRahman,andMdNafizurRahman.2023. TheAIraceison!Google’sBardand
OpenAI’sChatGPTheadtohead:anopinionarticle.MizanurandRahman,MdNafizur,TheAIRaceison(2023).
[55] AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,MarkChen,andIlyaSutskever.2021.Zero-shottext-to-image
generation.InInternationalConferenceonMachineLearning.PMLR,8821–8831.
[56] FatemehShiri,TerryYueZhuo,ZhuangLi,ShiruiPan,WeiqingWang,RezaHaffari,Yuan-FangLi,andVanNguyen.2022.ParaphrasingTechniques
forMaritimeQAsystem.In202225thInternationalConferenceonInformationFusion(FUSION).IEEE,1–8.
[57] IshikaSingh,ValtsBlukis,ArsalanMousavian,AnkitGoyal,DanfeiXu,JonathanTremblay,DieterFox,JesseThomason,andAnimeshGarg.2023.
Progprompt:Generatingsituatedrobottaskplansusinglargelanguagemodels.In2023IEEEInternationalConferenceonRoboticsandAutomation
(ICRA).IEEE,11523–11530.
[58] AlbertYuSun,EliottZemour,ArushiSaxena,UdithVaidyanathan,EricLin,ChristianLau,andVaikkunthMugunthan.2023.Doesfine-tuning
GPT-3withtheOpenAIAPIleakpersonally-identifiableinformation?arXivpreprintarXiv:2307.16382(2023).
[59] DivyaTadimeti,KallirroiGeorgila,andDavidTraum.2021.Howwellcananagentunderstanddifferentaccents.In5thWideningNLP(WiNLP)
Workshop-Co-locatedwithEMNLP,PuntaCana,DominicanRepublic.
[60] MinhDucVu,HanWang,ZhuangLi,GholamrezaHaffari,ZhenchangXing,andChunyangChen.2023.VoicifyYourUI:TowardsAndroidAppControl
withVoiceCommands.Proc.ACMInteract.Mob.WearableUbiquitousTechnol.7,1,Article44(mar2023),22pages. https://doi.org/10.1145/3581998
[61] BryanWang,GangLi,andYangLi.2023.Enablingconversationalinteractionwithmobileuiusinglargelanguagemodels.InProceedingsofthe2023
CHIConferenceonHumanFactorsinComputingSystems.1–17.
[62] LeiWang,SonghengZhang,YunWang,Ee-PengLim,andYongWang.2023.LLM4Vis:ExplainableVisualizationRecommendationusingChatGPT.
arXivpreprintarXiv:2310.07652(2023).
[63] YushiWang,JonathanBerant,andPercyLiang.2015.Buildingasemanticparserovernight.InProceedingsofthe53rdAnnualMeetingoftheAssociation
forComputationalLinguisticsandthe7thInternationalJointConferenceonNaturalLanguageProcessing(Volume1:LongPapers).1332–1342.
[64] JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,EdChi,QuocLe,andDennyZhou.2022.Chainofthoughtpromptingelicitsreasoning
inlargelanguagemodels.arXivpreprintarXiv:2201.11903(2022).
[65] ShijieWu,OzanIrsoy,StevenLu,VadimDabravolski,MarkDredze,SebastianGehrmann,PrabhanjanKambadur,DavidRosenberg,andGideon
Mann.2023.Bloomberggpt:Alargelanguagemodelforfinance.arXivpreprintarXiv:2303.17564(2023).
[66] FengXiaoandLongWang.2008.AsynchronousConsensusinContinuous-TimeMulti-AgentSystemsWithSwitchingTopologyandTime-Varying
Delays.IEEETrans.Automat.Control53,8(2008),1804–1816. https://doi.org/10.1109/TAC.2008.929381
[67] SileiXu,SinaSemnani,GiovanniCampagna,andMonicaLam.2020. AutoQA:FromDatabasesToQASemanticParsersWithOnlySynthetic
TrainingData.InProceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).422–434.
[68] KannonYamada.2020.HowtocontrolyourAndroiddeviceentirelywithyourvoice. https://www.makeuseof.com/tag/control-android-device-
entirely-voice/
[69] JackieYang,MonicaSLam,andJamesALanday.2020.Dothishere:multimodalinteractiontoimprovecross-applicationtasksonmobiledevices.In
Proceedingsofthe33rdAnnualACMSymposiumonUserInterfaceSoftwareandTechnology.35–44.
[70] YeYuan,StrykerThompson,KathleenWatson,AliceChase,AshwinSenthilkumar,AJBernheimBrush,andSvetlanaYarosh.2019.Speechinterface
reformulationsandvoiceassistantpersonificationpreferencesofchildrenandparents.InternationalJournalofChild-ComputerInteraction21(2019),
77–88.
[71] YuZhong,T.V.Raman,CaseyBurkhardt,FadiBiadsy,andJeffreyP.Bigham.2014.JustSpeak.Proceedingsofthe11thWebforAllConferenceon-
W4A14(2014).
[72] DennyZhou,NathanaelSchärli,LeHou,JasonWei,NathanScales,XuezhiWang,DaleSchuurmans,ClaireCui,OlivierBousquet,QuocLe,etal.
2022.Least-to-mostpromptingenablescomplexreasoninginlargelanguagemodels.arXivpreprintarXiv:2205.10625(2022).
ManuscriptsubmittedtoACM