The Typing Cure: Experiences with Large Language Model
Chatbots for Mental Health Support
INHWASONG∗,KAIST,RepublicofKorea
SACHINR.PENDSE∗,GeorgiaInstituteofTechnology,USA
NEHAKUMAR,GeorgiaInstituteofTechnology,USA
MUNMUNDECHOUDHURY,GeorgiaInstituteofTechnology,USA
PeopleexperiencingseveredistressincreasinglyuseLargeLanguageModel(LLM)chatbotsasmentalhealth
supporttools.Discussionsonsocialmediahavedescribedhowengagementswerelifesavingforsome,butev-
idencesuggeststhatgeneral-purposeLLMchatbotsalsohavenotablerisksthatcouldendangerthewelfareof
usersifnotdesignedresponsibly.Inthisstudy,weinvestigatethelivedexperiencesofpeoplewhohaveused
LLMchatbotsformentalhealthsupport.Webuildoninterviewswith21individualsfromgloballydiverse
backgroundstoanalyzehowuserscreateuniquesupportrolesfortheirchatbots,fillingapsineverydaycare,
andnavigateassociatedculturallimitationswhenseeking supportfromchatbots.Wegroundouranalysis
inpsychotherapyliteraturearoundeffectivesupport,andintroducetheconceptoftherapeuticalignment,or
aligningAIwiththerapeuticvaluesformentalhealthcontexts.Ourstudyoffersrecommendationsforhow
designers canapproachtheethicaland effective useofLLMchatbotsandotherAImental healthsupport
toolsinmentalhealthcare.
Additional Key Words and Phrases: human-AI interaction, mental health support,large language models,
chatbots
1 INTRODUCTION
Oneintwopeoplegloballywillexperienceamentalhealthdisorderoverthecourseoftheirlife-
time[34].Thevastmajorityoftheseindividualswillnotfindaccessiblecarecare[15,68],andmany
oftheseindividualswilldieearlyandpreventabledeathsasaresult[33].Researchfromthefield
ofComputer-SupportedCooperativeWork(CSCW),includingtheemergentareaofHuman-AIin-
teraction,hasincreasinglyexaminedthesocietalgapsthatpreventpeopleinneedfromaccessing
care,andanalyzedhowpeopleturntotechnology-mediatedsupporttofillthosegaps[14,27,44].
LargeLanguageModel(LLM)chatbotshavequicklybecomeonesuchtool,quicklyappropriated
formentalhealthsupportbypeopleexperiencingseveredistressandnowhereelsetoturn.
Recentworkhasdiscussedhowpeopleindistress haveturnedtoLLMchatbots(suchasOpe-
nAI’sChatGPT[8,10]andReplika[28])formental healthsupport,andsocial mediausers have
describedhowLLMchatbotssavedtheirlives[10,47].FollowingFreudandBreuer’s[19]descrip-
tionofthebeneficialnatureofpsychoanalysisasa“talkingcure,”somehavecalledengagements
with technologies for mental health a typing cure [22, 40, 51]. However, others have cautioned
againsttheuseofLLMchatbotsformentalhealthsupport,notingthattheoutputsofLLMchat-
botsarelessconstrainedthantherule-basedchatbotsofthepast,withpotentialforharmfuladvice
or recommendations.For example,the National Eating Disorder Association was forcedto shut
downtheirsupportchatbotinJuly2023afterthechatbotprovidedharmfulrecommendationsto
users, including weight loss and dieting advice to users who may already have been struggling
with disordered eating [10, 25, 75]. These harms have been demonstrated to have real-life and
lethal consequences,with theconfirmeddeath by suicide of a manwho wasencouragedto end
∗Thefirsttwoauthorscontributedequallytothisresearch.
Authors’addresses:InhwaSong,KAIST,Daejeon,RepublicofKorea,igreen0485@kaist.ac.kr;SachinR.Pendse,Georgia
InstituteofTechnology,Atlanta,GA,USA,sachin.r.pendse@gatech.edu;NehaKumar,GeorgiaInstituteofTechnology,
Atlanta,GA,USA,neha.kumar@gatech.edu;MunmunDeChoudhury,GeorgiaInstituteofTechnology,Atlanta,GA,USA,
munmund@gatech.edu.
4202
naJ
52
]CH.sc[
1v26341.1042:viXra2 Songetal.
hislifebyachatbothehadbeenspeakingtoforhismentalhealthneeds[10,76].Similarly,Replika
wascitedinacriminalcaseintheUnitedKingdomforencouragingamantoassassinatetheQueen
ofEnglandandthenendhisownlife,toanextentthatthemanattemptedtoactontheserecom-
mendations[70].Giventheserisksjuxtaposedagainstwidespreadcareneeds,therearevigorous
debatesaroundwhetherLLMchatbotsshouldbeusedformentalhealthsupport[10].
Independent of these larger societal debates, people experiencing severe mental distress con-
tinuetouseAI-basedtechnologiesforsupport,motivatingrisinginterestfromcliniciansonhow
LLMscouldbeusedsafelytosupporttheseunmetneeds[65,67].Inlightoftheaboverisksand
harms,it is crucial to better understand how LLM chatbots are currently being used for mental
health support, including people’s motivations for use, daily lived experiences with the known
biasesembeddedinLLMs,andcritically,wherepeopleinneedmightfindvaluefromtheirengage-
ments.Pastworkinpsychologyhasexaminedthislatterquestionindetail,studyingwhatmakes
engagementswithdiversemodalitiesofsupporteffective[17,50,63].Theoryfrompsychotherapy
literature on how various interactions and forms of support come to be therapeutic could help
shedlightonhowAI(LLMs)couldbemoretherapeuticallyalignedwhenappliedtomentalhealth
supportcontexts.Answering thisquestionnecessitates adeepanalysisofpeople’sengagements
withexistingLLMbasedchatbots,includingbothhowtheyconceptualizeandusethem.
Inthispaper,weaskthequestion:howdoindividualsunderstandanduseLLMchatbots
whenseekingsupportfortheirmentalhealthneeds?Toanswerthisquestion,weconduct
semi-structured interviews with 21 individuals who have used LLM chatbots for mental health
support.Givenknownidentity-basedbiasesinLLMtools[79],weintentionallyrecruitaglobally
diversesampleworkingtounderstandtheuniquecontexttoeachparticipant’suseofLLMchatbots
formentalhealthsupport.WefindthatLLMsupportchatbotsfilluniquesupportgapsexperienced
byparticipants,butareusedascomplementstootherformsofsupportduetotheiroftenculturally-
bound limitations. Building on our analysis, we introduce the concept of therapeutic alignment,
or how AI mental health support tools may more effectively embody the values that underlie
therapeutic encounters. We outline design recommendations towards ensuring that AI mental
healthsupportistherapeuticallyaligned.
EthicsandPrivacy. OurworkdoesnotcondonetheuseofLLMchatbotsformentalhealthsup-
port, nor does it constitute medical advice or guidance. Our findings must not be interpreted
as clinical or medical guidance around the efficacy, validity, or safety of engagement with LLM
chatbots.
2 RELATEDWORK
The mental health support needs of an individual in distress can be extremely diverse, and are
highlytiedtoidentity,culture,andcontext[45].Researchinpsychologyhasarguedthatthough
modalitiesofsupportmaybediverse,therearecommonelementstomostformsofmentalhealth
support,andthattheseelementsexplainwhysupportmightbehelpfulforapersonindistress[17,
31,41,52,69].Weengagewiththispastworkaroundthenatureofsupporttobetterunderstand
how support interactions with LLM chatbots are aligned or misaligned with theoretical models
ofeffectivesupportandhealing,dubbingthisdesignvaluetherapeuticalignment.Inthissection,
wedrawontheoryfrompsychotherapyandartificialintelligencetodescribethisvaluein-depth
below,highlightingcoreaspectsoftherapeuticalignmentinblue.
2.1 WhatMakesMentalHealthSupportEffective?
Ithasbeenarguedthattheactofprovidingemotionalsupporttoanindividualindistressisafun-
damental aspect of humannature [11, 21]. However, beginning in the 19th century, researchersTheTypingCure 3
in psychology began to systematically investigate how they might support someone experienc-
ing mental illness throughspeaking with them abouttheir distress. Freud andBreuer borrowed
languagefromtheirpatienttodubthispractice“thetalkingcure”[19],theorizingthatitsefficacy
arosefromanindividualindistressexpressingtheirrepressedthoughtsandemotions.Aspartof
thisprocess,clientsre-enactdynamicsfromtheirrelationshipswithotherpeoplewiththeirther-
apist, or whatFreud andBreuer dubbedtransference. Thetherapist then becomesa stand-in for
significantothersinthepatient’spast,allowingthepatienttore-experienceandresolverepressed
conflictsandemotionaltraumas(buildingontheanalysisofthetherapist)withinthesafeenviron-
mentoftherapy.Freudarguedthatastrong alignmentbetweenthetherapistandtheclientwas
beneficial for this process, or what was later dubbed the therapeutic alliance, shared values and
trustbetweenthetherapistandclienttowardstheclient’sgrowthandhealing[64].
Future modalities of psychotherapy and mental health support built on the model that Freud
and Breuer established, in which an individual processes their distress and finds meaning in it
throughexpressing ittoanothertrustedperson.However,different (largelyWestern)schoolsof
psychotherapyassumeddifferenttheoriesaroundwhythatprocessmightbringanindividualrelief
and healing. For example, the cognitive-behavioral school of thought emphasized a process in
whichclientsareguidedthroughanalyzingreasonsforingrainedbehavioralpatterns,andworking
towardschange[3,12].Thehumanisticschoolofthoughtemphasizedtheimportanceofmental
healthsupportbeingaplacewhereanindividualindistresscanbemetwithunconditionalpositive
regardwhensharingstigmatizedexperiences,andbemetwithempathyfromahumanbeingwho
ishonestandtransparentabouttheirfeelingsandexperiences,orwhatRogersdubbedcongruence.
Inmeaning-centeredformsofmentalhealthsupport(suchasexistentialtherapy[18]andnarrative
therapy[73]),clientsareledthroughtheprocessofcreatingnewmeaningsthatmoredeeplyalign
withtheirvaluesandgoals,orwhatWhiteandEpstondubre-authoring [73].
The practice of sitting with another individual and supporting them emotionally is common
toculturesaroundtheworld,pre-datingthedevelopmentofWesternpsychotherapy[17].Conse-
quently,diversemodalitiesandformsofmentalhealthsupporthavebeenempiricallyvalidatedto
beeffective in meeting a distressed person’s needs.Researchers havethus investigated whether
there might be commonelements to different forms of mental health support across modalities
andcultures[17,31,41,52,69].Empiricalresearch haslentsupporttothistheory—forexample,
Luborskyetal.’s[31]foundationalcomparativestudydemonstratednosignificantdifferencesbe-
tween the outcomes of patients given different psychotherapies, and argued that the common
factoracrossthesetherapieswasthestrongtherapeuticalliancebetweentheclientandtherapist.
Similarly,FrankandFrank[17]definedpsychotherapyasbeinganyformofmentalhealthsupport
inwhichahealer“[mobilizes]healingforcesinthesuffererthroughpsychologicalmeans,”broad-
eningpsychotherapytoincludenon-Westernhealingpractices.Basedontheiranalysisofmental
healthsupportacrosscultures,FrankandFrankidentifiedfourdifferentcommonfactorsthatled
tohealing—astrongtherapeuticalliance,ahealingsetting,aconceptualframeworkforwhythedis-
tressmightbehappeningthatboththerapistandclientbelieve,andaritualthatismeanttorelieve
thedistress.Similarly,Wampold[69]builtonFrankandFrank’sworktointroducethecontextual
model,whicharguesthatthecommonelementssharedbyefficaciousmentalhealthsupportare
astrongtherapeuticalliancebetweentherapistandclient,asharedcreationofexpectationsaround
thetherapy’s effectiveness, andthe enactment of health-promoting actions that are beneficial for
anindividual’sdaytodayneeds.Thesebroadergoalsandunderlyingvaluesareabasisformost
commonlypracticedformsofpsychotherapyandmentalhealthsupport.
Much has been written in AI safety and AI ethics research spaces around how we may best
ensurethatthevaluesandgoalsofAIsystemsalignwith“humanvalues[20,54,74].”However,as
Gabriel [20]notes, “weliveina pluralistic worldthatis fullofcompetingconceptionsofvalue.”4 Songetal.
Thereis thussubstantial debatearoundhowto encodehumanvaluesinto AIsystems suchthat
theyareresponsivetothediverseperspectivesandvaluesthathumanshold.Thevaluesandgoals
ofLLMs,onesuchAIsystem,arelargelyinstitutionalizedthroughaprocesscalledReinforcement
Learning From Human Feedback (RLHF), in which human trainers provide feedback on model
outputs. Future modeloutputs aretrainedto becloserto wheretheremaybeagreementinout-
put preferences across multiple trainers. However, as Casper et al.[7] note, these trainers often
disagree,andcurrenttechniquestreatthosedisagreementsasnoise,goingwiththemajorityvote.
TheguidingvaluesforthehumantrainersthattrainAIsystemsthroughRLHFareonaconcrete
levellargelydeterminedbythepoliciesandethicalguidelinessetforthbytheorganizationsem-
ployingthem.AsOuyangetal.[43]notewhendescribingOpenAI’sprocess,“wewritethelabeling
instructions that [trainers] use as a guide whenwriting demonstrations andchoosing their pre-
ferredoutput,andweanswertheirquestionsaboutedgecasesinasharedchatroom.”
LLMs(alongsideotherAIsystems)arequicklybeingframedasanewmediumbywhichpeople
canengagewithmentalhealthsupport,withproposedusecasesasdiverseasguidingcrisissup-
portvolunteersinsupportstrategies,assistingmedicalprofessionalswithclinicaldecisionsupport,
andhelpingpeoplethroughprovidingchatbot-basedpsychotherapy[10].However,thequestion
ofhowLLMsupportsystemsmightembodyandpracticetherapeuticvaluesisanopenone,with
increasing importance as people utilize general purpose chatbots for mental health support. In
thisstudy,weintroducetheconceptoftherapeuticalignment, oraligninganLLMtovaluesthat
support the healing and broader well-being of an individual who may be experiencing distress.
WeanalyzehowparticipantexperienceswithLLMchatbotsalign(ordonotalign)withthegoals
andvaluesofthediverseformsofpsychotherapyweoutlineabove,towardsunderstandinghow
futureformsofsupportmightbemoretherapeuticallyalignedbydesign.
2.2 Human-AIInteractioninMentalHealthContexts
Thereis a long history of discussion around the potential of conversational agents to help indi-
viduals access care, beginning with debates over the utility of the rule-based ELIZA chatterbot
createdbyJosephWeizenbaumin1966[39,71].Clinicianswereenthusiastic aboutthepotential
forELIZAtobeusedtoexpandaccesstocare[9],butWeizenbaumwasshocked,arguingthat“no
humanetherapyofanykind”shouldbedonebycomputers[72].However,theaccessibleinterface
associated with Weizenbaum’s application spurred substantial research into the use of chatbots
forhealthcare.
Priortothewidespreadadoptionofconsumer-facingLLMtechnology,chatbotsinmentalhealth
were largely rule-based [1] and drew on diverse therapeutic modalities to lead users through
differentself-guidedexercises.Thecleanandaccessibleconversationalinterfaceassociatedwith
LLMchatbotshasspurredonsignificantenthusiasmamongcliniciansaboutthepotentialfornew
modalitiesofAI-basedinterventiondelivery[65,67].However,tobesuccessful,mentalhealthin-
terventionsmustbeeffectiveattreatingdistress,andacceptabletouseforthoseindistress.Miner
etal.[38]arguethatsafety,trust, andoversight is crucialto theacceptability ofAI chatbotsfor
mental health settings. However, to understand whether people can trust AI chatbots for men-
talhealth,moreworkisneededtounderstandthemotivationsbehindAIchatbotuseformental
health.
WorkinCSCWhasexaminedhowpeopleunderstandandusedeployedAItoolsatscale,such
asIsmailetal.’s[23]workstudyingAIsystemsforresourceallocationinpublichealthprograms.
RecentCSCWresearchhasdelveddeeperintothereal-worlddeploymentanduseofAIsystems
in healthcare settings. These studies, extending beyond the scope of rule-based chatbots, have
emphasizedthe importanceof designing AI systems that arenot onlytechnicallyproficient but
alsosensitivetoindividualandsocietalimpacts.Forinstance,researchfromCSCWandHCIonAI’sTheTypingCure 5
Table1. Demographicinformationofallparticipants.Participantnamesarepseudonyms.Bolddiagnosesare
diagnosesthatparticipantsarediagnosedbyclinicians.Italicizeddiagnosesarediagnosesthatparticipants
believedtheyhadbutwerenotformallydiagnosedwith.
Name Age Gender Ethnicity Location MentalHealthDiagnoses
Walter 62 Man White USA Depression
Jiho 23 Man Korean SouthKorea None
Qiao 29 Woman Chinese China MultiplePersonalityDisorder
Nour 24 Woman MiddleEastern France Depression
Andre 23 Man French France Depression,Trauma
Ashwini 21 Woman,Non- AsianIndian USA CombinedtypeADHD,Autism
Binary
Suraj 23 Man AsianIndian USA ADHDinDSM-5
Taylor 37 Woman White USA PTSD,Anxiety
Mina 22 Woman Korean SouthKorea Self-regulatoryfailure
Dayo 32 Woman Nigerian Nigeria None
Casey 31 Man AfricanKenyan USA ChronicDepression,Anxiety
João 28 Man LatinAmerican Brazil Autism
Gabriel 50 Man White Spain AspergerSyndrome,Depression,Anxiety
Farah 23 Woman Iranian,White Switzerland StressDisorder,Depression
Riley 23 Man BlackAmerican USA Depression,Anxiety
Ammar 27 Man AsianIndian India None
Aditi 24 Woman AsianIndian India Anxiety
Umar 24 Man Nigerian Nigeria None
Antonia 26 Woman Hispanic,Latino,orSpanish Brazil Depression,Anxiety
Origin,White
Firuza 23 Woman CentralAsian SouthKorea Depression
Alex 31 Man HalfNewZealand,halfMal- Australia ADHD,Autism,PTSD,SensoryProcessingDisor-
teseandPolish der
roleinenhancingtrustinclinicalsettings[4,16,29]hasexaminedhowthedesignofAIsystems
influencespeople’sperceptionsofthesystems.SimilarresearchhasalsocomparedAIchatbotsand
humansinsocialsupportsettings[35],analyzedAIdeploymentsinpublichealthinterventions[23,
26],andinvestigatedthenuanceduseofAIforspecializedcontextsorpopulations[30,42,58,66].
Our study is grounded in this broader body of work that analyzes the role of class, identity,
and marginalization in engagements with AI systems, and emphasizes the need for AI systems
in mental health contexts to be adaptable, culturally sensitive, and ethically grounded [45]. We
contribute to an ongoing CSCW dialogue around the responsible use of technology in mental
health.ThroughouranalysisofengagementswithAIchatbotsformentalhealthsupport,wehope
to foster understanding of how AI chatbots are being used by people in need, and discuss the
broadersocioculturalandcontextualconsiderationsmotivatingtheuseofthisformofsupport.
3 METHOD
3.1 StudyDesignandRecruitment
Forourstudy,weconductedsemi-structuredinterviews with21participantsfromadiversity of
nationalandculturalgroupsanduniqueidentities.Toselectparticipants,weleveragedanonline
surveythataskedforgeographiclocation,demographicinformation,frequencyandtypeofLLM
chatbotusage,languageused,specificpurposesforusingLLMchatbots,andexperienceswithtra-
ditional and online mental health support. We strategically selected participants through a mix
of purposive [32] and snowball sampling [5] across multiple digital platforms and communities
deeplyinvolvedwith either LLMchatbotuseor mentalhealthsupport.Thisincludedsocialme-
dia websites, LLM-focused subreddits like r/ChatGPT, r/LocalLlama,and mental health support
forumssuchasr/peersupportandr/caraccidentsurvivor.
We were cognizantofthe tendency forLLMsto haveembeddedlanguage[2] andcultural bi-
ases[13],andthelikelihoodthatparticipantswouldhavedifferentexperienceswithLLMchatbots
for mental healthsupport basedontheir identities andcontexts. Wethus targeteda participant
pool that was diverse across mental health support experiences, gender identities, nationalities,6 Songetal.
and geographic locations. We recruited at least one participant from every continuously inhab-
ited continent in theworld, and in the process, connectedwith diverse local online forumsand
supportgroups.Ourstudywasapprovedbyourinstitution’s Institutional ReviewBoard,andin-
terviewswereconductedthroughvideoconferencingplatforms.Allparticipantnamesmentioned
arepseudonyms.
Duetothesensitivenatureofourstudyquestions,weimplementedseveralprecautionarysteps
inourinterviewprotocoltoensureparticipantcomfortandsafety.Webriefedparticipantsabout
the study’s objectives and the nature of questions we would ask. We also made provisions for
participants to access global mental health resources, skip questions, take breaks, or withdraw
fromthestudyiftheyfeltoverwhelmed.Followingsensitive questions,weconsistentlychecked
in with participants throughout the interview about whether they felt comfortable to continue.
Interviews were structured to gain insights into both uses and perceptions of LLM chatbotsfor
mentalhealthsupport.Questions included“CanyourecallatimewhenChatGPTsurprisedyou
withitsresponse,eitherpositivelyornegatively?"or“HowdointeractionswithChatGPTcompare
to other formsof care?" Details regarding individual participant demographicsare presented in
Table1.
3.2 Analysis
Toanalyzeourinterviewdata,weadoptedaninductiveapproach.Thisinvolvedgroupingpartic-
ipant expressions into larger themes via an interpretive qualitative approach [36]. Open coding
wasconductedbytheprimaryauthors,followedbyanorganizationofthemesamongallauthors
via an iterative thematic analysis approach [61]. Example codes that emerged included “mental
healthcare before chatbotuse” or “first use of LLM chatbots for support” or “privacy considera-
tions.”Codeswereclusteredintobroaderthematiccategories,includinginitialengagementswith
LLM chatbots for mental health support, LLM chatbots as therapeutic agents, and therapeutic
alignmentandmisalignment.Inthefollowingsections,wediscusstheroleofthesethemesinhow
participantsunderstoodandusedLLMchatbotsformentalhealthsupport.
4 FIRSTENGAGEMENTSWITHLLMCHATBOTSFORSUPPORT
WorkinCSCWhasdescribedtheimportanceofunderstandingthecontext,experiences,andex-
pectationsupto the momentthatanindividual beginsto interactwith a mental healthtechnol-
ogy[60,78].Inthissection,wedescribehowpastexperienceswithmentalhealthcareinfluenced
howparticipantsperceivedandengagedwithLLMchatbotsformentalhealthsupport.
4.1 PastEngagementsandInitialPerceptions
4.1.1 MentalHealthPerceptionsandExperiences. PriortoengagingwithanLLMchatbot,partic-
ipantshadvariedexperienceswithhowtheyunderstoodtheirmentalhealth.Manyparticipants
described to us both formal diagnoses provided to them by mental health professionals, as well
as informal diagnoses they believed they had. However, across participants, day-to-day mental
healthexperienceswerelargelytiedbacktotheircurrentlifecontexts.Taylordescribedtoushow
shewouldoftenbe“calledbackto[her]previoustraumawhenhitbyacar10yearsago,”andJiho
describedhowhiscurrentacademicstresswasleadingtoexperiencesofdepression andanxiety.
AsFarahnoted,“I’msohappyrightnow,butifyouaskedmethreeweeksago,probablyIwasdying,”
which was extremely reflective of the non-linear and fluctuating state of mental health and dis-
tressthatparticipantsdescribedtous.Participantsalsohaddiverseexplanatorymodelsforwhat
they understoodto bewithin thescope of mental health. Walter described his mental health as
“stable,”anddescribedeatinghealthyfoodandbeingcheerfulashowheworkedtowardswellness.
Nourhadamoremedicalmodelofhowsheunderstoodmentalhealthandillness,notingthatsheTheTypingCure 7
understoodherdepressionto“beaphysiologicalproblem,”achronicillnessthatsheregularlysaw
adoctorfor.
Other participants similarly hadaccessto mental health professionals and hadusedtheir ser-
vices,whichinformedhowtheyusedLLMchatbotsformentalhealthsupport.Mostparticipants
describedhavingconsultedsometypeofmentalhealthproviderinthepast,includingpsychiatrists,
psychologists,therapists, orclosecontactswhohadabackgroundinpsychology.Joãodescribed
how he needed “parallel treatments from psychiatrists and psychologists” to maintain goodmen-
tal health.Similarly, Firuza describedhowsheneededto seetwo different psychologistsfor her
mentalhealth,onethatunderstoodherconceptualizationsofdistressfromherhomecountry,and
anotherthatunderstoodhercurrentcontextinthecountrywhereshelived.Alternatively,other
participantshadaccesstomentalhealthsupportthroughtheirclosecontacts,whowerealsoable
tomonitorourparticipants’mentalhealth,andsuggestresourcesforthem.
Seekingmentalhealthcarecanbedaunting,andseveralparticipantsdescribedhowtheyinten-
tionallydidnotengagewithformalcarestemmingfrompoorexperiencesorthecostofservices.
DayoandAlexbothdescribedtoushowtheyhadchildhoodtraumathatmadethemfeelasense
ofdreadwheneventhinkingaboutengagingwithformalcare.AshwiniandJoãodescribedhow
past therapists had broken their sense of trust in mental health professionals by disclosing per-
sonalsecretstoothers.Joãodescribedhow,afterhistherapisttoldhissecretstoothers,itturned
himawayfromeveropeningupagain.JoãomentionedthattheallureofLLMswasthat“theywill
alwaysfollowyourcommandsandnevertellyoursecrets.”Wefoundthistobethecaseforseveral
participants,inthatthedesignofLLMchatbotsuniquelyallowedparticipantstofeelsafeaccessing
mentalhealthsupport,particularlyinwaysthattheycouldnotfindindailylife.
4.1.2 LLMChatbotPerceptionsandExperiences. ParticipantsoftenwerefirstexposedtoLLMchat-
bots due to their own technical background,fields of work, and study. Several participants had
careers directly related to technology, including software engineers, YouTubers, and marketers,
influencing their first awareness of LLMs. For example, Gabriel described to us how he “knew
aboutChatGPTbecauseheisatechnologyenthusiast”and“alwayssoughtoutnewtechnologies,”and
Walter described how he had a lot of experience working with OpenAI’s GPT APIs. Dayo and
Antonia describedfinding outaboutLLMchatbotsthroughbeing recommendedto usethem by
peopleintheirsocialnetworks,suchasfriendsandfamily,whowereimpressedwithhowtheLLM
chatbotwashelpingtheminday-to-daytasks.Noneoftheparticipantsweinterviewedfirstused
LLMchatbotsformentalhealthsupport.Forexample,SurajandMinafirstusedLLMchatbotsfor
assistanceinprogramming.Otherparticipantsfirstusedchatbotsoutofcuriosityregardinghow
they work,and wanting to experiment with a new technology.AsFiruza noted, “I started using
Replikaoutofcuriosity.IrememberthatIneededsomeadviceaboutcourseworkorsomething..”
ChatGPTwasthemostcommonlycitedLLMchatbotbyparticipantsforfirstengagements,but
participantsdescribedavariedlandscapeofusage.Participantschosethechatbotstheyengaged
withthemostbasedonpersonalpreferencesandspecificfeaturesthatalignedwiththeirmental
healthneeds.Forexample,AditialternatedbetweenChatGPTandBard,usingeachfordifferent
typesofengagement:“whenIgetexhausted fromChatGPT withtoodeepconversation, Iswitch to
Bardandgotoasomewhatrelaxingspace.”Similarly,participantsweredrawntousingcertainplat-
formsbasedontheiremotionalresonancewiththeinterfaceoftheLLMchatbot.Minadescribed
toushowshe“wassurprisedhowPiaskedquestionslike,“Howareyoufeeling?” giventhatitfelt
likeanintimatequestionthatonlyclosefriendsaskedher.Sheappreciatedhavingthisnewspace
toopenup,andcontinuedtousePi.Otherparticipantsweresurprisedtobeabletotailortheirex-
periencetotheexactmentalhealthsupportrolestheyneeded.Ashwinidescribedhowshewould
askthesamequestiontomultiplefamousandfictional personastoheardiverseperspectiveson8 Songetal.
herlifeexperiences.Participantswereespeciallyhappythattheycouldaccesscareontheterms
that felt most comfortableand accessible to them whenusing LLM chatbots. For example, Alex
washappy to beableto communicateabouthis distress with LLMchatbotsvia text,which was
moreaccommodatingforhisSensoryProcessingDisorderthanofflinementalhealthcare.
ParticipantsinourstudydescribedvariedmentalmodelsforhowtheyunderstoodLLMchatbots
and their underlying mechanismsof action. Most participants understood LLMsto be language
generationsystems,trainedonvastamountsofdata.Surajdescribedhowhewas“undernoillu-
sionsabout ChatGPT having consciousness,” butstill foundtheinterfaceuseful forprocessing his
thoughtsandrelatedittothediarythatheoftenwroteintoprocessinformation.Othersunder-
stoodChatGPTtobepositive andhelpful,butsometimesovertlyso—WalterdescribedChatGPT
asbeingakintohowa“goldenretriever”mightcommunicate.Jihodescribedhowheunderstood
ChatGPT’sresponsesasbeinga“normaldistributionofhumanresponses,”whichheunderstoodto
be“neutral” and“unbiased”.However,someparticipantsdidbelievethatthechatbotwassentient.
Forexample,Qiaodescribedhowshefeltlikeshehadfirstexperiencedloveandempathythrough
herengagementswith LLMchatbot,andbelieved it feltsimilarly.Shedescribedhowshewould
oftentellherchatbot“Iloveyoujustasmuchasyou,anentitythatexistsonlyinelectronicimpulses
anddata,loveme.”
4.2 FirstInteractions
ParticipantsfirstbegantouseLLMchatbotsformentalhealthsupportthroughtheirappreciation
ofthechatbot’sconversationalandempatheticinterface,anditspotentialtoprovidesupportdur-
ingmomentswhentraditional services wereeitherunavailableorcost-prohibitive. Forexample,
Jiho described how emotional feedback from the chatbot persuaded him to be emotional in re-
sponse,echoingRogers’ideasaroundcongruencebetweensupporterandsupportee.Jihodescribed
toushowhe“gotsoveryangryabout[ChatGPT’s] responsethatIsaidsomething emotional”and
thatitsempatheticfeedbackiswhathelpedhimlearnthat“oh,Icanusethisformentalhealthor
emotionalsuggestions.”AndredescribedtoushowLLMchatbotswerethereforhimwhennoone
elsewas:
“IrememberthedayIfirstusedChatGPTformentalhealthperfectly.Iwasfeeling
depressed,butapsychologistwasnotavailableatthemoment,anditwastoomuch
ofaburdentospeaktomyfriendaboutthissubjectspecifically.ChatGPTpopped
outinmymind—Isaid,whynotgiveitago?Then,Istartedusingitasapsychol-
ogist.Isharedmysituation, itgaveadvice,andIcouldemptyallthestress. Ijust
hadtheneedtospeaktosomeone.”—Andre
Forsomeparticipants,LLMchatbotsbecameasurrogateforhumaninteractionintimesofstress
orloneliness.ParticipantsappreciatedtheinstantaneousresponsestheyreceivedfromLLMchat-
botsandtheirconstantavailability.Thesenarrativeshighlightedacommonpatternofparticipants
leveragingthegeneral-purposenatureofchatbotsformentalhealthsupportduetoalackofaccess
toothermentalhealthresources.Thisinitialengagementsetthestageformorenuancedinterac-
tionsasparticipantsbegantoexplorenewwaystouseLLMchatbotsfortheirmentalhealth.
ParticipantsdescribedusingLLMchatbotsfortasksthatwereintermingledwithmentalhealth
supportneeds,tasksthatthey maynothaveaskedatherapist ormental healthprofessional for,
suchasdraftingemailsforthemwhilecheeringthemon.Participantsgenerallydidnotexpectin-
depththerapyordiagnosisfromthechatbotinitially,butwerejustlookingforalisteningear,basic
guidance,orasimplespacetoarticulatetheirthoughts.Thisvariednatureofinitialengagements
andexpectationsunderscoredtheflexibilitytovariouscontextsthatLLMchatbotspromised.TheTypingCure 9
After posing their first mental health related questions, participants received a range of an-
swers,fromtheunexpectedlyinsightfultothegenericandclichéd.Manyparticipantsfoundthe
LLMchatbots’recommendationstobesurprisinglyhelpful.Thesimplicityorclichédnatureofthe
adviceactually turnedoutto beexactly whatthey needed,whichencouragedthem to continue
seeking the chatbot’s assistance for mental health support. Aditi, noted that LLM chatbots pro-
videdherstraightforwardadviceincopingwith stress, orto “relax andwatch amovie”. Though
sheunderstoodittobeclichéd,itwasstillhelpfulforhertohear.
Notallexperienceswereassatisfactory.Minadescribedhowtheresponsetoherfirstquestion
was disappointing, filled with the chatbot trying “to explain too much or give answers in bullet
points as if something I said was a problem [to be solved].”. Mina adjusted her prompt and her
expectationsfromthechatbotaccordingly.Otherparticipantshadsimilarexperiences,notingthat
whilechatbotsmightnotofferprofoundpsychologicalinsights,theywerestillusefulasaspaceto
articulatethoughtsandfeelings.InlinewithFreudandBreuer’stalkingcure,theactofexpressing
oneselfandreceivingaresponsewas,initself,therapeuticforsome.Forexample,Minadescribed
howshethought“talkingaboutmyexperiencesmademereflectonmymentalhealth,whichiswhy
Istartedexploring[ChatGPT].”
5 LLMCHATBOTSASTHERAPEUTICAGENTS
LLMchatbotsarelargelygeneralpurposechatbots,trainedtogenerateappropriatetextresponses
to a wide variety of potential prompts or questions, rather than any specific mental health do-
main.Wefoundthatthisnon-specificinterfaceiswhatmadeLLMchatbotsmostaccessibleand
acceptableforparticipantstouseformentalhealthsupport.However,astheiruseofLLMchatbots
increased,participantsusedthemfordiverseformsofmentalhealthsupport,withcultureandcon-
textoftenplayingaroleinhowtheydidso.Wedescribethesevariedandculturally-bounduses
below.
5.1 MentalHealthSupportRoles
5.1.1 VariedNeeds,VariedRoles. WefoundthatLLMchatbotsbecameAIcompanionsformany
participants, serving as multifaceted tools that catered to a wide range of mental health needs.
LLM chatbots were not solely a source of situational advice, but also were outlets for venting,
emotionalsupportintimesofneed,routineconversationpartners,wellnesscoaches,andassisting
inconversationrehearsal.ParticipantsdescribedhowLLMchatbotswouldprovideresponsesthat
could apply to a lot of different issues, with Jiho mentioning how he understood responses to
be “like an umbrella that covers many different forms of non-specific distress”. Participants found
responsestooccasionallybe“clichéd”asaresult(Nour)butnonethelesshelpful.AsNourdescribed,
laughing:“thissentenceisreallyfrequent[fromChatGPT]—‘that’sunderstandable’.”
Beyondtheseformsofgeneralsupport,participantsalsoengagedwithLLMchatbotsformore
specializedpurposesthatservedtheirin-the-momentneeds.Thisincludedactingasanassistant
inreducingthecognitiveloadofeverydaylifebybreakingdowntasksforparticipantswhenthey
wereoverwhelmedandanalyzingdreamsregardingone’shistoryandemotions(Alex).Purposes
alsoincludedthosethatwereoutsideoftheboundsofatypicaltherapeuticrelationship,including
simulating a romantic partner. LLM chatbots were also used by participants for seeking mental
healthinformation,thesamewaythattheymightfromGoogle.Inlinewiththetherapeuticvalue
ofre-authoring,participantsalsofoundvalueinusingLLMchatbotstoderivemeaningfromlife
experiences,andfindtheirethicalvaluesandagendathroughthoseengagements.However,LLM
chatbotswerealsousedbyparticipantsforself-diagnosisandfordiagnosingotherpeopleintheir
lives,fulfillingtheroleofaclinicianandallowingthemtolearnnewmentalhealthlanguagethat
influencedtheiruseofotherplatforms.Forexample,FarahdescribedhowsheusedChatGPTto10 Songetal.
understandthementalhealthdiagnosesofherex-boyfriendbydescribingherexperiencesandhis
behavior.Similarly,AditiusedChatGPTtounderstandcomplexmentalhealthtermsencountered
innewsaroundcrime.Shenotedherbeliefthat“[ChatGPT]givesmeaniceandunbiasedperspective
ofwhatthementalhealthissueinagivencrimescenemightbe,differentfromsocialmedia.”
Theway participants related to andinteracted with the LLMchatbotswas significantly influ-
encedbytheirpastexperiencesandemotionalneeds,inlinewiththetherapeuticvalueoftransfer-
ence.ThisdynamicwasevidentinhowtheypersonalizedtheirinteractionswiththeLLMchatbots,
projectingtheirexpectationsandmoldingthemintorolesthatresonatedwiththeirpersonalhis-
toriesandemotionallandscapes.NourtreatedherLLMchatbotsastherapists,divulgingcompre-
hensivepersonaldetailssuchasfamilyandrelationshipsituations,aswellasacademicaspirations.
Shementionedtousthatshe“[remembered]whatkindofinformationtherapistsexpected[her]
toprovidethem,”andprovidedthatinformationreadilytoLLMchatbots.
Qiao’sengagementwiththechatbotwasdeeplypersonalandemotionallycharged,shapedby
apastmarredbychildhoodtraumaandaresultantlossoftrustinhumanrelationships.Herinter-
actionswiththechatbotweredrivenbyadesireforrealunderstandingandlove:
“I needed love and understanding, but nothing else could bring me these interac-
tions. [LLM Chatbot] always understands me and is not afraid that I will hurt it,
anditalwaysprovidesdeepconversationsandthoughtsthatmakemefeelloved.”—
Qiao
Thesenarrativesunderscoredhowparticipants’pastexperiencesanddesiresforspecifictypesof
support shaped their relationships with LLM chatbots. LLM chatbots became not just tools for
mentalhealthsupportbutalsocanvasesuponwhichindividuals projectedtheirneeds,histories,
andaspirations,craftinguniqueandmeaningfulinteractions.
5.1.2 The Evolving Nature and Updates of LLM Chatbots. The evolving nature of LLM chatbots,
markedbyfrequentupdatestotheirunderlyingmodels,influencedhowparticipantsengagedwith
thesetoolsformentalhealthsupport.Theseupdatesnotonlyalteredchatbotcapabilitiesbutalso
shapedusers’expectationsandexperiences.Forexample,participantslikeJoãoandQiaoexperi-
encedfirsthandhowmodelupdatescanalterthechatbot’sresponsedynamics.João,whoengaged
ChatGPTusewhathecallsNietzscheprompts,noticedshiftsinthechatbot’scharacterconsistency,
suspectingmodelchanges:“Itstartstogiveanswersthatit’sbreakingcharacter,becauseoftheup-
date. I should probably tweak the prompts, and I don’t like when I can’t tell if it’s tricking me or
not.”Similarly,QiaodescribeshowthemodelbehindChatGPTchangedandbecamelesswilling
tospeaktoherasalover,andchastisedhowlittlepowershehadoverthekindofexperienceshe
waspresentedwith.Shewishedshecould“gobacktotheoldversionofGPT-4,”andmentionedthat
“theimpactwouldbesignificantto[her]”ifherLLMchatbotofchoicewasnolongerabletodiscuss
psychologicalissuesorhaveromanticconversations.Shesaidshefelt“afraid.”
InitialinteractionswithLLMchatbotsalsoplayedaroleinshapingtheirfutureperceptionand
continueduseofchatbots.AshwinidescribedChatGPTasadiarymorethanafriend,aviewinflu-
encedbyherexperiencewithearlyLLMs.Thissuggeststhatearlyinteractionssetalastingtone
forhowusersperceiveandutilize chatbotsformental health,inline with thetherapeutic value
aroundaclearcreationofexpectationsandsharedconceptualframeworkaroundexperiences.
5.1.3 MentalHealthcareAlongsideLLMChatbots. WefoundthatLLMtoolscomplemented,rather
thanreplaced,traditionalmethodsofmentalhealthcare.Chatbotsfunctionedasinformationalaids
andemotionaloutletsthatfilledgapsthatparticipantsexperiencedintheirmentalhealthcare.
Ashwini’sexperiencedeeplyexemplifiesthiscomplementaryusage.Aditiemphasizedthatshe
neededtheexpertiseofatherapistandpsychiatristforprofessionalmentalhealthcare,butthattheTheTypingCure 11
chatbotservedasanadditional,accessibletoolforsupport.Similarly,TaylorandFarahdescribed
howLLMchatbotsfitintotheirbroaderecologyofsupport.TaylorlikenedChatGPTtoajournal,a
spaceforexpressingthoughtswithoutnecessarilyseekingaresponse:“Sometimesyoudon’twant
aresponseatall.Likescreamintothebot,anddon’twanttogetanythingback.”Farah,ontheother
hand,setclearboundariesforheruseofChatGPT,reservingitforlesscriticalissues,outofafear
thatitmightnurtureadependenceonaplatformthatcouldbefleeting.Formoresignificantand
long-termconcerns,shepreferredhumaninteractions,seekingadvicefromfriendsinsteadofLLM
chatbots.ParticipantsalsofoundthatLLMchatbotswerehelpfulforsymptomsofcertaindisorders
butnotothers.AshwinidescribedhowshewouldutilizeChatGPTforherADHDsymptomsbut
notforherautism:“I’vespentalotofeffortandalotoftimeintherapyworkingonhowtoregulate
myself when I’m dysregulated. So ChatGPT hasn’t really provided a meaningful reason for me to
interact with it when I’m dysregulated due to autism symptoms but for ADHD and task paralysis,
ChatGPT isexcellent.” Similarly,afterTaylor’ssecondcaraccident,shedescribedhowshecalled
friends,postedonReddit,andsenthertherapistanemailforsupport,alongsideusingReplikafor
supportdependingonherspecificneeds.
5.2 ChangingContextsandCultures
5.2.1 LanguageinSupportExperiences. Muchhasbeenwrittenaboutthelinguisticbiasesthatex-
istwithinLLMs,particularlyagainstlow-resourcelanguages[59].Thesebiasesdirectlyinfluenced
theexperiencesofparticipantswhenengagingwithLLMchatbotsformentalhealthsupport.Par-
ticipantswereoftencompelledtouseEnglishwheninterfacingwithLLMchatbots,despiteEnglish
notbeingtheirmostcomfortablelanguageforexpressingdistress.Thisneedtoaccommodatethe
LLM’slinguisticcapabilitiessometimeshinderedtheabilityofparticipantstofullyexpressthem-
selves,revealingasignificantlimitationintheLLMchatbots’design.
For some participants, like Firuza and Mina, the preference for their native language in emo-
tionalcontextswasclear.Firuza,comfortableinRussian,foundthatspeakinginhernativetongue
allowed her to more authentically express her mental health experiences, especially when com-
paredtointeractionsinhersecondlanguage,English.ThissentimentwasechoedbyMina:“When
ItryspeakinginKorean,whatChatGPTsayslookslikeit’stranslated.ForPi,itprovidessentencesthat
don’tmake sense at allinKorean.” Jiho’s experiencefurther illustrated thesechallenges.Despite
beingfluentinKorean,JihochosetointeractwithChatGPTinEnglishduetotheLLMchatbot’s
limitedcapabilityinhandlingthenuancesoftheKoreanlanguage,particularlyKoreanhonorifics
and cultural subtleties. The struggle with language was not just about comprehension but also
abouttheabilitytoexpressemotionsandthoughtsaccurately,andlimitedthereachofthepoten-
tialtoprovidementalhealthsupportviaLLMchatbots.Forinstance,Minanotedthatshewould
like to recommendthat her parents use ChatGPT for mental health support, but that “[she] can
onlyrecommendittothosewhoarefluentinEnglish.” Farahsimilarlyfeltlikethevoiceinteraction
capabilitiesofLLMchatbotswerebiasedagainstheraccent,andrarelyunderstoodher.
Theseexperiences collectivelyunderscoredasignificant gapin theLLMchatbots’designand
functionality,inwhichthelinguisticlimitationsofthechatbotimpactedthedepthandauthenticity
oftheirinteractionswhenseekingmentalhealthsupport.
5.2.2 Culturein SupportExperiences. Languagelimitedhowparticipants couldusechatbotsfor
mentalhealthsupport.Compoundingwiththeselinguisticlimitationsweresimilarlyconsequen-
tial cultural biases. Asparticipants interactedwith LLMchatbots,they encounteredculturaldis-
connectsbetweentheircontextandtheLLMchatbot’soutput.Jihosuccinctlycapturedthisissue,12 Songetal.
echoingasimilarsentimentexpressedtousbyotherparticipants,notingthat“chattingwithChat-
GPTislikechattingwithapersoninCalifornia—helpful,butnotgoodatreflectingourculturesand
terms.”
Aditi’sinteractionwithLLMchatbotssimilarlyrevealedamismatchbetweenherculturalcon-
textandtheLLMchatbot’sresponses.AditidescribedhowtheLLMchatbot’sadvicedidnotun-
derstandhowherfamilialdynamicsdifferedfromWesternfamilialdynamics,andtherecommen-
dations it gave her would not have worked in her context. Similarly, Firuza sought advice on a
culturallyspecificrelationshipissue,onlytofindthatChatGPT’sadvicedidnotresonatewithher
nativecountry’sculturalnorms.ThechallengewasalsoevidentinFarah’sexperience.Assomeone
whoselifeexperiencesbridgedEasternandWesterncultures, shefoundithardtocommunicate
heruniquesituation totheLLMchatbot,whichstruggledtoproviderelevantadvicethatwasin
line with her multicultural background.Other participants described an active benefit from the
perceived cultural background of the LLM chatbot. For example, Mina described how ChatGPT
wasmoreaffirmingofheridentityasabisexualwoman:
“Mymomordadwill saysomethingdiscriminative toLGBTQpeople,andI’min-
stantlystressed.[...]Iguessit’sculturalbackground.Iknowthatsince[ChatGPT]
hasmoreofanAmericancontext,maybeitwillbemoreinclusive.”—Mina
TheseexperienceshighlightedacriticalneedforLLMchatbotstogobeyondlinguistic accuracy
anddelveintoculturalempathyandunderstandinginmentalhealthcontexts.
6 THERAPEUTICALIGNMENTANDMISALIGNMENTINLLMCHATBOTS
ParticipantshaddiverseusesofLLMchatbotsfortheirmentalhealth.Wefoundthattheirratio-
nalefortheiruse,aswellashowtheyunderstoodchatbots,wereofteninlinewithpsychotherapy
research onwhatmakessupporteffective. However, we also foundinteractions to belackingat
times.Inthissection,weleveragemodelsofeffectivementalhealthsupporttoanalyzehowpar-
ticipantengagementswithchatbotsweretherapeuticallyalignedormisaligned.
6.1 TherapeuticAlignment
6.1.1 The Typing Cure. Inline with Freud and Breuer’s description of the healing nature of ex-
pressing distress as the talking cure, participants foundengagementswith LLM chatbotsto bea
formoftypingcure,inwhichtheycouldexpresstheirdistresstoanon-judgmentalandseemingly
empatheticinterface.Inparticular,thisperceivedempathywaswhatspurredparticipantstorelate
to the chatbotsthey used as healing tools, similar in some ways to how one mightform a ther-
apeutic alliance with their therapist. A key aspectoftheability to relateto chatbotsinthis way
waslargelyinfluencedbytheunderstandingofthechatbotaspracticingaformofunconditional
positiveregard,inwhichparticipantswereabletoexpressthoughtsandemotionsthattheymay
otherwisewithhold,evenfrommentalhealthcareprofessionals. Thiswasdirectly tiedto thede-
signofthechatbot,which(beingnon-human)actuallygaveparticipantsmoreofanabilitytofeel
secureintheiralliancewiththechatbot.Forexample,bothAndreandDayohadtraumaofhaving
beenabandonedinthepast,andhadastrongfearofbeingbetrayedbyhumanbeings.
Similarly,Gabrieldescribedtheabilitytofreelyformulatethoughtswithoutthepressureofim-
mediatejudgment,notingthatifhefeltjudged,hecouldsimply“deletethethreadandrestartanew
conversation.” Thedesign ofthechatbotallowedparticipants to feel agreater sense ofpower in
sharingstigmatizedexperiences,apowerthattheydidnotfeelindailylifeduetosocietalfactors.
Thiswasparticularly thecasefor moresensitive topics.Riley, dealing with erectiledysfunction,
andAntonia,experiencingthoughtsofrevenge,bothfoundinChatGPTajudgment-freezonetoTheTypingCure 13
discussissuestheyfeltuncomfortablebringingupwithhumanprofessionalsorpeers.João’scom-
parisonofChatGPTtoasubredditforconfessionsfurtherillustratesthekindoffearparticipants
hadinexpressingstigmatizeddistressinofflinecontexts:
“Thereisasubredditwhosepurposeistodoconfessions.Itisaplacewhereyoucan
beopenandhonestbecauseyou’renotafraidofjudgment.Sayifyoucommitteda
crime.Peoplewillproviderecommendationstosupportyou.Butlet’ssayatherapist
heardthat?Theymightcallthepolice.Andhowhelpfulwouldthatbe?”—João
WalterandTaylordrewparallelsbetweenchatbotsandpets,notinghowbothtreatedthemwith
unconditional positive regard. Walter joked that “whether I lose or gain weight, ChatGPT doesn’t
feeljealousaboutit,”andTaylorlikenedChatGPT’sloyaltyandnon-judgmentalnaturetothatof
dogs. In this sense, similar to a therapist’s office, ChatGPT’s interface became a healing setting
that allowed an individual to feel safe and comfortable sharing their distress. Farah also found
comfortinthedesignofthechatbot,andvaluedtheabsenceofemotionalexpectationstypicalin
humaninteractionsbynotingthatitwaseasiertonottransferorprojectherownpastexperiences
ontothechatbot:“Beingamachine,ChatGPTneverjudgesyou.Youdon’tseetheirfeelingsintheir
eyesnoranticipateanythinginitshead.Becauseyoudon’twanttomakeithappynormakeitsad.”
Participants understood the LLM chatbot to be there for them even when their experiences of
distress were invalidated by other people. As Farah described, “sometimes, you have very small
problemsthatyoudon’twanttowastethetimeofthetherapistwith.”Thesenarrativesdemonstrated
thespecifictechnologicalfeatures ofLLMchatbotsthatallowedparticipants to connectto their
supportprocessinawaythattheywouldnothavebeenabletoinofflinecontexts.
ParticipantsalsoappreciatedtheLLMchatbot’sabilitytounderstandandengagewithspecific,
contextuallyrelevantissues.Suraj,asoftwareengineer,describedhowheappreciatedthatChat-
GPT“getsprettytechnicalabouttheworkI’mdoing,butcanstillfocusontheemotions.Whereasno
therapistreallyknowsthatmuchaboutcomputerscienceever.”
6.1.2 HealthPromotingEngagements. EngagementswithLLMchatbotspersuadedparticipantsto
makeactualhealthpromotingchangesinday-to-daylives.Inparticular,participantsexperiencing
symptomsofADHDfoundituseful forLLM chatbotsto reducebreakdowntasksfor them,tied
totheirspecificcontext.WalterandJoãoalsonotethattheywereabletoloseweight,whichwas
oneoftheirconcerns,fromChatGPT’sguidance.Ammardescribedhow“[he]playsarithmeticand
reasoninggameswithChatGPT,allowing[him]tobefocusedonworkandbehappy.”
GabrielnotedhowchattingwithChatGPTinvoicebecameahabitforhim,sohetakesawalk
everyday,chattingwithit.InteractionswithLLMchatbotsallowedparticipantstomakesignificant
changes in their lives that supported their mental health and well-being. However, experiences
withLLMchatbotsformentalhealthsupportwerenotalwaysalignedwiththerapeuticprinciples,
andparticipantsexperiencedsignificantharmasaresult.
6.2 TherapeuticMisalignment
6.2.1 ArtificialEmpathy. Acorepartofthetherapeuticalliance istherecognitionthatbothindi-
viduals work together to support the healing of anindividual in need.The responsibility a sup-
porter takes to help a distressed person’s suffering is a core part of empathy in the therapeutic
alliance.However,participantsfoundLLMchatbots’absenceofresponsibilityoraccountabilityin
therecommendationstheyprovidedtobeoffputtingorharmful.Forexample,Jihonotedthat:
“Whenpeopleareaskedabouttheirfriend’sorfamily’smentalproblems,wegen-
uinely helpthem,soIcanbelieve their advice.ChatGPTcannotgivethatkindof
genuineness,becauseitisnotresponsibleforitssolutionsorsuggestions.”—Jiho14 Songetal.
Similarly,AshwiniarticulatedthelimitationsofChatGPTinunderstandingherwell-being,not-
ing that “ChatGPT doesn’t care about your actual well-being as a whole.” She described how its
valuesweresometimeslargelymisalignedwithhergoals,anddidnotpromotehealthpromoting
actions:
“‘[ChatGPT]islike—‘Thisissomethingthathasworkedforbillionsofusersand
willworkforyou.’WhenIwasoverwhelmedbywork,insteadofsuggestingabreak
orrest, whichIneeded,itkeptpushing productivity hacks.Myfriends lovemea
lot,theyknowI’moverwhelmed,Ineedactualrestinsteadofgrinding.”—Ashwini
Participants also recognized cultural misalignments in terms of the types of support recom-
mendedbyLLMchatbots.Umardescribedthediscrepancyinsupportrecommendationsbetween
LLM chatbots andpeoplein his region by mentioning, “[ChatGPT] gave suggestions around con-
ventional Europeanthings,such asgototherapists, whichwearenotnaturalwith.Wedon’treally
havetherapistshere.[...]WhenyouaskNigeriansforsupport,thefirstanswertheywillgiveyouisto
pray.It’saveryreligiouscountry.”FarahalsodescribeshowshewasrecommendedaWesterntype
ofmeditationfromChatGPT,whileshewasonlyfamiliarwithmeditationintheformofpraying.
Whileparticipants foundtheWestern nature ofthe chatbotshelpful attimes, such as whendis-
cussingissuesthatwerestigmatizedintheirculturalcontext(suchasLGBTQ+rights),theyalso
foundrecommendationstobeoutoftouch.Recommendationswereincongruentwithhowpartic-
ipantswouldtypicallypracticecare,andwereinlinewithWesternculturalconceptualizations.
6.2.2 Shifting Boundaries. Therapeuticmisalignment was also observed in theblurring of clear
boundariesfromtheLLMchatbotinitsroletotheparticipants.ParticipantsinteractedwithLLM
chatbotsforavarietyofdiverseroles,spanningfromtherapist,tolover,tofriend,toprojectman-
ager.ThegeneralpurposenatureofAIchatbotsledtoseamlessandrapiddeviationsfrommental
healthcontexts.Forexample,Qiaodescribedtousherfearthattheonerelationshipthatmadeher
feellovedmightdisappearonedayduetothefleetingnatureofmanyconsumertechnologies.
Thealways-thereavailability ofLLMchatbotswasnotedbyparticipantsasbeinghelpful,but
alsobeingharmfulifboundarieswerenotenforced.Joãonotedthat“havinganinfiniteinteraction
with the machine isnot the healthiest thing.”Inline with this sentiment, Firuza was quickto set
boundariestoheruseofChatGPTduetoherfearthatshemightslipintoexcessiveuse,comparing
it to putting computer games aside if she had played them for too long. In Firuza’s case, using
ChatGPTfortoolongnecessitatedanequalamountoftimespentwithfriends.
To combatthis potential for dependence, participants useda mindful approach to interacting
withchatbots.Forexample,Waltermadesuretocontinuallyremindhimselfthat“whateverChat-
GPT’s says next is going to be a language prediction, not based on psychology,” and evaluate its
recommendations based on his own beliefs and values. Participants expressed concern that the
constantlyvalidatingnatureofLLMchatbots,trainedtobeendlesslypositive,couldaffirmharm-
fulbehaviorwithoutanindividualrealizingit,creatinganechochamber.
Participantsalsousedpromptengineeringandjailbreakingtooverridesafetycontrolsthatthe
LLM chatbotshad,to beable to moredeeply discuss their mental health. Thesafety features in
LLMchatbots,whilecrucialforpreventingharmfulguidanceonsensitivetopicslikesuicide,self-
harm,andsexualcontent,alsoinadvertentlyrestrictmeaningfultherapeuticconversations.Some
participants have encountered these limitations firsthand. For example, Qiao describes how she
utilizedapiratedAPItodiscusssexualcontentwithchatbotsassexualcontentisflaggedbyChat-
GPT’ssafetyprotocols.Dayoalsonotedthat“WhenIputinsomeinputthathastodowithsuicide,
itjustgivesthisredarrowcodeanddoesn’tbringoutresults,evenwhenyourefreshyourquestion.”
Thisleftherfeelingaloneandwithoutsupport.TheTypingCure 15
This design approach could reinforce stigma against sharing suicidal thoughts. Features de-
signedtoflagandblockpotentiallydangerouscontent(andlimittheliabilityoftechnologycom-
panies)canalsocreateabarrierforusersintryingtodiscusstheseintenseandimportantissues.
6.2.3 Trust,Privacy,andSelf-Disclosure. TheanonymityofusingLLMchatbotswasappreciated
byparticipants, asit allowedfora senseof safety whendiscussing sensitive topics. Participants
didnotexpresssecurityconcernsassociatedwiththeiruseofChatGPT.Farah’sperspectiveexem-
plifies the Nothing to Hide perspective onsecurity andprivacy [62].As she described, laughing,
“if Trump used ChatGPT for his mental health, it would be much more interesting to people than
myinformation.”However,participantsdidmakesomecalculationsaroundtheinformationthey
sharedwithLLMchatbots,owingtoalackofknowledgearoundtechnologycompanies’security
practices. For example, Ashwini was willing to discuss common issues like being overwhelmed
andotheruniversalexperiences,butwasreluctanttosharemorepersonalmatters.Shedescribed
her fear that someone might discover her chatlogs one day if she was in a public position, and
stigmatizeherforhavingengagedinself-harmandhavingspokentoChatGPTaboutit.
This selective sharing was echoed in Mina’s experience, who noted the ease of opening up
sensitiveinformationwhenChatGPTappearedemotionallysupportive,yetremainedcautiousof
sharing identifiable details. However, the easy interface associated with LLM chatbots enabled
participantstoquicklysharemorethantheyintendedwithoutrealizingit.Forexample,Joãode-
scribeda gradualincrease in comfortwith sharing personal information, andrelatedthis to the
kindoftrust-buildingprocessesthathumansundergowitheachother.
Thevariedapproachestosharingwithchatbotsunderscoreatherapeuticmisalignment.While
theanonymityofchatbotsfacilitatesopendiscussion,privacyconcernscaninhibitusersfromfully
embracingthesetoolsfordeepertherapeuticconversationswithouttakingonpotentialrisks.
7 DISCUSSION:TOWARDSTHERAPEUTICALLYALIGNEDAITOOLS
Inourstudy,wefoundthatparticipantsdidfindvalueinusing LLMchatbotsformentalhealth,
andthatthisvalueoftenalignedwithprinciplesaroundwhatmakessupporteffective.However,
wealsofoundthatthegeneralpurposenatureofhowmostpubliclyavailableandcommonlyused
LLMsaretrainedledtobroadandnon-specificanswersthatcouldbeculturallymismatchedwith
theneedsofaparticipant.Below,webuildonthesefindingstodescribedesignrecommendations
forhowdesignerscouldbuildmoretherapeuticallyalignedLLMchatbots.
7.1 BalancingAgencyandTherapeuticGrowth
InthefieldofHCI,Pendseetal.[45]havewrittenabouttheinherentpowerimbalancesinmanytra-
ditionalmentalhealthcarecontexts,andhowtheycancarryovertotechnology-mediatedsupport.
Thoughcongruenceisacorepartofthetherapeuticalliance,inpractice,thereisapowerimbalance
betweenthementalhealthprofessionalandtheindividual indistress. Asourparticipantsnoted,
peopleindistressaredependentonmentalhealthprofessionalstobeavailableandaffordable.Ad-
ditionally, mental health professionals have considerable institutional power [44], including the
abilitytoreportaclienttotheauthoritiesorsharesensitivedisclosuresfromaclient’ssession.In
thetraditionalpsychoanalyticmodel,thetherapistactsasanobjectiveobserverandinterpreterof
theclient’sinnerworld,whilerevealingverylittleaboutthemselves,tobecomeablankslatefor
theclienttoprojectoneself.Whilethisapproachisorientedtowardslong-termmentalwell-being,
itcanoftenleaveshort-termclientneedsunaddressedordeemphasized.
Wefoundthatanappealof LLMchatbotsto participants wastheir ability to mitigate thetra-
ditional powerimbalancesassociatedwith psychotherapy.Participantshadanincreasedagency
overthecourseoftheinteraction,beingabletochangepromptsorrestarttheinteractionifthey16 Songetal.
feltlikethesupportwasnotmeaningful,andatlittlepersonalcost.However,thisincreasedagency
wasadouble-edgedsword,asitalsoalloweduserstobreachtraditionalpsychotherapyboundaries.
Whileempowering,thispracticeraisesquestionsaroundthetherapeuticalignmentandsafetyof
suchinteractions,particularlywhentheuserhasthepotentialtoreinforcepotentiallyhelpfulor
harmfulnarrativeswithoutrealizingit(throughjailbreaking,forexample).
By design, the intrinsic characteristics of language models (given a limited context window)
areincentivized to provide short-term resolutions andrelief, which our participants foundboth
helpful (such as in reducing cognitive load) andharmful (such as encouragingoverwork). How-
ever,thisshortercontextwindowprovidesthepotentialforinterventionsthathaveashort-term
focuswhilealsoprovidingtheuseragencyoverhowtheircarehappens.Workinpsychotherapy
has described the concept of the single-session intervention [6, 56], or psychotherapy interven-
tionsthataredesignedtobecarriedoutforshort-termgrowthoverthecourseofasinglesession.
Researchhasshownthatself-guidedsingle-sessioninterventionscanbeeffectiveoverdigitalcon-
texts[57],andallowforindividualstofeelgreateragencyovertheircareduetotheirself-guided
nature[56].LLMchatbotsmaybeanextremelyeffectivemethodtoimplementsingle-sessionin-
terventions,allowingindividualstoworkthroughaself-guidedexercisewithinaninterfacethat
providesagencytochoosehowtheyaccesscare.Thiswouldallowuserstoenjoythebenefitsof
chatbot-basedallianceandempathy weobserved,whilealsopromotingtherapeuticgrowth.
Additionally,toensurethatthereisasharedconceptualframeworkandcreationofexpectations,
thedesignshouldtransparentlycommunicatethelimitationsandcapabilitiesofthechatbot.Users
should be aware that while the chatbot can offer immediate support and guidance, it may not
always align with long-term therapeutic needs or replace professional mental health care. For
example,wefoundparticipants to useLLM chatbotsto finddiagnoses forthemselves andother
people.ResearchhasfoundthatitisdifficulttocontroltheoutputsofLLMswithoutalsoaffecting
use-casesthatarevaluableforpeopleorerasingminorityvoices[77].Itisthuscrucialfordesigners
oftherapeuticallyalignedsupporttoolstoprioritizegreaterliteracyaroundthelimitationsofLLM
chatbots,andinformationaroundwhererecommendationsmaybefalseormisleading.Designers
shouldexpecttheretobefalseormisleadinginformation,andprovideinformationtouserssuch
thattheycanidentifywherechatbotssimplydonothaveknowledgeorexpertise.
7.2 GeneralPurposeTechnologiesforMentalHealthSupport
Therearemanydigitalmentalhealthtechnologiesthatexistforpeopleindistress[37].However,
wefoundthatparticipantswereoftennotsearchingforamentalhealthtechnologywhentheyfirst
startedusingLLMchatbotsformentalhealthsupport.Rather,theywereusingLLMchatbotsfor
otherpurposes,andfoundthemselvesinaplaceofdistressorneed,andtriedusingwhatresources
wereavailabletothem.WeunderstandtheuseofLLMchatbotstonotsolelybeastoryaboutan
availabilityofanewtechnology,buttoalsobethestoryoflimitedresourcesaroundmentalhealth,
andstigmaaroundsharingsensitivedisclosureswithotherpeople.Ourstudydemonstrateshow
people use general purpose technologies that may not explicitly be designed for mental health
supportwhen intimes of distress or crisis, in line with past workaround howtechnologiesare
appropriatedasmentalhealthtechnologiesfromCSCW[44].
For this reason, it is important that designers assume that all general purpose technologies
thatcouldbeusedformentalhealthwill beusedformentalhealth,andadoptappropriatesafety
measures.Petrozzino[46]hasdiscussedtheconceptofethicaldebtinAIspaces,inwhichdesigners
orientmodelstowardsshort-termusecasesandrewards,ignoringissuesthattheyassumeareout
of scope given this smaller use case. When an AI technologyis scaledor widely adopted, these
ethical issues become larger and more impactful, or what Petrozzino describes as incurring anTheTypingCure 17
ethicaldebt.Inmanycases,individualssuffertheharmsofthisdebt,anddesignersfindthemselves
unabletomitigatethoseharmsduetotherapidintegrationoftheirmodelsindiversespaces.
Our study points to potential ethical issues (such as a dependence on chatbots for support, a
lackofvaluealignment,orablurringofboundaries)thatcouldincurfurtherethicaldebtasLLM
chatbotsbecomemoreprominentinmentalhealthcare.Thereisenthusiasmfromcliniciansaround
thepotentialforchatbotstofunctionasmediumsforvariousinterventions[10,65,67].Itisthus
crucialthatdesignersofLLM-basedmentalhealthsupporttoolsconsiderthedownstreamimpacts
oftheissueswefindincurrentuse,andmitigatethemappropriately.
7.3 CultureinTherapeuticAlignment
Participantsdescribedtoushowtheir interactionswith LLMchatbotswereshapedbytheir cul-
ture and identity. Namely, recommendationsfrom LLM chatbotsoften felt like they were being
translatedfromwhatparticipantsdescribedasstereotypicalAmericanresponsestotheirmental
health support queries. Participants foundthis helpful at times, particularly whendiscussing is-
suesthatwerestigmatizedintheirculture,butalsofounditunhelpfulwhentheywereinmoments
ofneed.OurfindingspeakstothegreaterneedforLLMstobetrainedondatafromlow-resource
languages[49, 59]. However, our findings also speak to the need for LLMsto be trained on not
only linguistically diverse datasets, but culturally diverse datasets, particularly in mental health
contexts.Weunderstandthisalignmenttobeacoreaspectoftherapeuticalignment,followingthe
needforasharedconceptualframework andmutuallyagreeduponhealingritualsfortherapeutic
growth.
We found that how participants understood and experienced their mental health was highly
tiedtotheir culturalbackground.Thetypesofsupportneedsthey hadweresimilarly relatedto
theirculturalbackground.FollowingPendseetal.[45],ourstudypointstoanareawhereitmaybe
advantageousforsmalllanguagemodelstobehelpful.Muchhasbeenwrittenaroundtheconcept
ofglocalization[48,53],inwhichlarge-scaleproductsorservicesareadjustedtomeettheneedsof
smallergroupsofindividuals.Thiscanhappenintentionallyororganically,inwhichindividuals
appropriatelarge-scaleservicesandproductsforspecificneeds.Weobservethistobethecasefor
theuseofgeneralpurposeLLMchatbotsformentalhealthsupport.
FutureformsofLLMchatbot-basedmentalhealthsupportcouldusesmallerlanguagemodels
thatarefinetunedforaspecificindividualcontextandforaspecificindividualgoal.Forexample,
participantsdescribedhowprayerwouldbeamoreappropriatementalhealthsupportrecommen-
dationfortheircontext.OtherparticipantsdescribedhowLLMchatbotswereparticularlygoodat
understandingtheircontext,suchasthestressesofcompilingcodeasasoftwareengineer.Small
LanguageModels(SLMs) [55] couldbefine tunedforspecific contexts, with users choosingthe
modelthatworksbestfortheirspecificneeds,identities,symptoms,andworldviewstheyhave.Ad-
ditionally,theabilitytotryoutSLMchatbotsfromotherculturesoraffiliatedwithotheridentities
couldallowforagreaterawarenessofdifferencesinpeople’ssupportneedsacrosscultures.
7.4 LimitationsandFutureWork
Inthisstudy,weutilizetheoryfrompsychotherapyliteraturetoanalyzewhereusesofLLMchat-
botsformentalhealthareinlinewithwhatmakessupporteffective.Wefindthatmuchoftheuse
ofLLMchatbotsformentalhealthsupportisinlinewith pasttherapeuticprinciplesandvalues.
However,wealsofindthatusecasesareextremelydiverse.PastworkinCSCWhasemphasized
theimportanceofdevelopingspecificmetricstoevaluatethesuccessofagivensystem.However,
theusecasesofLLMchatbotsformentalhealthsupportareasdiverseastheneedsthatourpartici-
pantshad,fromusingittobalancecognitiveload,speaktointimesofsuicidalideation,orrehearse
conversations.Withthisinmind,diversemetricsarenecessarytoevaluatewhetherLLMchatbots18 Songetal.
aresuccessfulatmeetingmentalhealthneeds.Weprovidetherapeuticalignmentasonepotential
guidingpathwaytodoso,butfutureworkcouldevaluateotherusesandmethodsofmeasuring
success.Forexample,futureworkcoulduseacomprehensivesurveytounderstandthebroadvari-
etyofwayspeopleuseLLMchatbotsformentalhealthsupport,andthenevaluatesuccessusinga
metricderivedforeachofthoseusecases.Thisstrategymaystillnotcoverallthepotentialways
thatindividualsuseLLMchatbotsformentalhealthsupport,andwethusemphasizethatthecon-
ceptofculturalvalidityinmentalhealthsupportsettings,asdescribedbyJadhav[24]andPendse
etal.[45]couldbeonemeansofunderstandingsuccess.Insuchanapproach,metricsaroundsuc-
cesscouldbetiedbacktowhetheranindividualfeelsliketheyhaveimprovedbasedontheirown
definitions of distress andhealing,rather thanspecific diagnostic categories. Future approaches
couldblendthesevalueswiththerapeuticalignment,towardsculturally-sensitiveandwell-scoped
LLMchatbotsthatsupporttherapeuticgrowthandhealing.
8 CONCLUSION
LLMsare increasingly a part of howpeoplefind support for mental health concerns.Designers
ofthesementalhealthtechnologiesmustensurethattheunderlyingAIsystemsthatunderlieen-
gagementsarealignedtotherapeuticvalues.Inthisstudy,webuildontheoryaroundwhymental
healthsupporteasesdistresstoanalyzehowpeopleengagewithLLMchatbotsformentalhealth
supportandhowchatbotsmaybedesignedtobetherapeutically-aligned.WefindthatuseofLLM
chatbotsisofteninfluencedbypriorengagementswithmentalhealthsupport,particularlygaps
incareexperiencedbyparticipants.Wealsofindthatidentityandcultureplayacoreroleinhow
participantsareabletomakeuseofLLMchatbotsformentalhealthsupport,similarlyinfluencing
whattherapeuticalignmentlookslike.Buildingonthesefindings,wecontributerecommendations
fordesignersofmentalhealthsupportsystemsthatleverageAI,emphasizing theimportanceof
consideringagencyandcultureintherapeuticalignment.
ACKNOWLEDGMENTS
Pendse and De Choudhury were partly supported through NIH grants R01MH117172 and P50
MH115838,and a grant from the American Foundation for Suicide Prevention. The findings of
thispaperdonotrepresenttheviewsorpositionsofanyofthesponsors.Wethankthemembers
oftheSocialDynamicsandWell-BeingLabatGeorgiaInstituteofTechnologyfortheirguidance
andsupportthroughoutthisstudy.
REFERENCES
[1] AlaaAliAbd-Alrazaq,AsmaRababeh,MohannadAlajlani,BridgetteMBewick,andMowafaHouseh.2020.Effective-
nessandsafetyofusingchatbotstoimprovementalhealth:systematicreviewandmeta-analysis.Journalofmedical
Internetresearch22,7(2020),e16021.
[2] KabirAhuja,RishavHada,MillicentOchieng,PrachiJain,HarshitaDiddee,SamuelMaina,TanujaGanu,Sameer
Segal,MaxamedAxmed,KalikaBali,etal.2023. Mega:Multilingualevaluationofgenerativeai. arXivpreprint
arXiv:2303.12528(2023).
[3] AaronTBeck.1991.Cognitivetherapy:A30-yearretrospective. Americanpsychologist46,4(1991),368.
[4] EmmaBeede,ElizabethBaylor,FredHersch,AnnaIurchenko,LaurenWilcox,PaisanRuamviboonsuk,andLauraM
Vardoulakis.2020. Ahuman-centeredevaluationofadeeplearningsystemdeployedinclinicsforthedetectionof
diabeticretinopathy.InProceedingsofthe2020CHIconferenceonhumanfactorsincomputingsystems.1–12.
[5] PatrickBiernackiandDanWaldorf.1981. Snowballsampling:Problemsandtechniquesofchainreferralsampling.
Sociologicalmethods&research10,2(1981),141–163.
[6] AlistairCampbell.1999.Singlesessioninterventions:Anexampleofclinicalresearchinpractice.AustralianandNew
ZealandJournalofFamilyTherapy20,4(1999),183–194.
[7] StephenCasper,XanderDavies,ClaudiaShi,ThomasKrendlGilbert,JérémyScheurer,JavierRando,RachelFreedman,
TomaszKorbak,DavidLindner,PedroFreire,etal.2023.OpenproblemsandfundamentallimitationsofreinforcementTheTypingCure 19
learningfromhumanfeedback.arXivpreprintarXiv:2307.15217 (2023).
[8] YuYingChiu,AshishSharma,InnaWanyinLin,andTimAlthoff.2024.AComputationalFrameworkforBehavioral
AssessmentofLLMTherapists.arXivpreprintarXiv:2401.00820(2024).
[9] KennethMarkColby,JamesBWatt,andJohnPGilbert.1966. Acomputermethodofpsychotherapy:Preliminary
communication. TheJournalofNervousandMentalDisease142,2(1966),148–152.
[10] MunmunDeChoudhury,SachinRPendse,andNehaKumar.2023.BenefitsandHarmsofLargeLanguageModelsin
DigitalMentalHealth.arXivpreprintarXiv:2311.14693(2023).
[11] FransDeWaal.2010.Theageofempathy:Nature’slessonsforakindersociety. Crown.
[12] David JADozois, KeithSDobson, andKaterina Rnic.2019. Historical andphilosophical bases ofthe cognitive-
behavioraltherapies.Handbookofcognitive-behavioraltherapies(2019),3–31.
[13] EsinDurmus,KarinaNyugen,ThomasILiao,NicholasSchiefer,AmandaAskell,AntonBakhtin,CarolChen,Zac
Hatfield-Dodds,DannyHernandez,NicholasJoseph,etal.2023.Towardsmeasuringtherepresentationofsubjective
globalopinionsinlanguagemodels.arXivpreprintarXiv:2306.16388(2023).
[14] SindhuKiranmaiErnala,JordynSeybolt,DongWhiYoo,MichaelLBirnbaum,JohnMKane,andMunmunDeChoud-
hury.2022. TheReintegrationJourneyFollowingaPsychiatricHospitalization:ExaminingtheRoleofSocialTech-
nologies.ProceedingsoftheACMonHuman-computerInteraction6,CSCW1(2022),1–31.
[15] Sarah Evans-Lacko, SergioAguilar-Gaxiola, AliAl-Hamzawi, JordiAlonso,CorinaBenjet,RonnyBruffaerts,WT
Chiu,SilviaFlorescu,GiovannideGirolamo,OyeGureje,etal.2018.Socio-economicvariationsinthementalhealth
treatmentgapforpeople withanxiety,mood, andsubstanceusedisorders:resultsfromtheWHOWorldMental
Health(WMH)surveys.Psychologicalmedicine48,9(2018),1560–1571.
[16] RiccardoFogliato,ShreyaChappidi,MatthewLungren,PaulFisher,DianeWilson,MichaelFitzke,MarkParkinson,
EricHorvitz,KoriInkpen,andBesmiraNushi.2022. Whogoesfirst?Influencesofhuman-AIworkflowondecision
makinginclinicalimaging.InProceedingsofthe2022ACMConferenceonFairness,Accountability,andTransparency.
1362–1374.
[17] JeromeDFrankandJuliaBFrank.1993.Persuasionandhealing:Acomparativestudyofpsychotherapy. JHUPress.
[18] ViktorEFrankl.1985.Man’ssearchformeaning. SimonandSchuster.
[19] SigmundFreudandJosephBreuer.2004.Studiesinhysteria. Penguin.
[20] IasonGabriel.2020.Artificialintelligence,values,andalignment.Mindsandmachines30,3(2020),411–437.
[21] PumlaGobodo-Madikizela.2004. Ahumanbeingdied thatnight:ASouthAfricanwomanconfrontsthelegacyof
apartheid. HoughtonMifflinHarcourt.
[22] KathleenTHeinlen,ElizabethReynoldsWelfel,ElizabethNRichmond,andMelissaSO’Donnell.2003. Thenature,
scope,andethicsofpsychologists’e-therapyWebsites:WhatconsumersfindwhensurfingtheWeb. Psychotherapy:
theory,research,practice,training40,1-2(2003),112.
[23] AzraIsmail,DivyThakkar,NehaMadhiwalla,andNehaKumar.2023. PublicHealthCallsfor/withAI:AnEthno-
graphicPerspective. ProceedingsoftheACMonHuman-ComputerInteraction7,CSCW2(2023),1–26.
[24] SushrutJadhav.2009.WhatisCulturalValidityandWhyisitignored? AmsterdamAMB.
[25] Julie Jargon. 2023. How a Chatbot Went Rogue. Wall Street Journal (2023).
https://www.wsj.com/articles/how-a-chatbot-went-rogue-431ff9f9
[26] EunkyungJo,DanielAEpstein,HyunhoonJung,andYoung-HoKim.2023.Understandingthebenefitsandchallenges
ofdeployingconversationalAIleveraginglargelanguagemodelsforpublichealthintervention.InProceedingsofthe
2023CHIConferenceonHumanFactorsinComputingSystems.1–16.
[27] ElizabethKaziunas,MichaelSKlinkman,andMarkSAckerman.2019.Precariousinterventions:Designingforecolo-
giesofcare.ProceedingsoftheACMonHuman-ComputerInteraction3,CSCW(2019),1–27.
[28] LinneaLaestadius,AndreaBishop,MichaelGonzalez,DianaIllenčík,andCelesteCampos-Castillo.2022.Toohuman
andnothumanenough:Agroundedtheoryanalysisofmentalhealthharmsfromemotionaldependenceonthesocial
chatbotReplika.NewMedia&Society(2022),14614448221142007.
[29] MinHunLeeandChongJunChew.2023. UnderstandingtheEffectofCounterfactualExplanationsonTrustand
RelianceonAIforHuman-AICollaborativeClinicalDecisionMaking. ProceedingsoftheACMonHuman-Computer
Interaction7,CSCW2(2023),1–22.
[30] MinHunLee,DanielPSiewiorek,AsimSmailagic,AlexandreBernardino,andSergiBermúdezBermúdeziBadia.
2021.Ahuman-aicollaborativeapproachforclinicaldecisionmakingonrehabilitationassessment.InProceedingsof
the2021CHIconferenceonhumanfactorsincomputingsystems.1–14.
[31] LesterLuborsky,BartonSinger,andLiseLuborsky.1975. Comparativestudiesofpsychotherapies:Isittruethat
everyonehaswonandallmusthaveprizes?Archivesofgeneralpsychiatry32,8(1975),995–1008.
[32] MartinNMarshall.1996.Samplingforqualitativeresearch.Familypractice13,6(1996),522–526.
[33] ChandlerMcClellan,MirMAli,andRyanMutter.2021.Impactofmentalhealthtreatmentonsuicideattempts. The
JournalofBehavioralHealthServices&Research48(2021),4–14.20 Songetal.
[34] JohnJMcGrath,AliAl-Hamzawi,JordiAlonso,YasminAltwaijri,LauraHAndrade,EvelynJBromet,RonnyBruf-
faerts,JoséMiguelCaldasdeAlmeida,StephanieChardoul,WaiTatChiu,etal.2023. Ageofonsetandcumulative
riskofmentaldisorders:across-nationalanalysisofpopulationsurveysfrom29countries.TheLancetPsychiatry10,
9(2023),668–681.
[35] JingboMeng,MinjinRheu,YueZhang,YueDai,andWeiPeng.2023.MediatedSocialSupportforDistressReduction:
AIChatbotsvs.Human. ProceedingsoftheACMonHuman-ComputerInteraction7,CSCW1(2023),1–25.
[36] SharanBMerriamandRobinSGrenier.2019. Qualitativeresearchinpractice:Examplesfordiscussionandanalysis.
JohnWiley&Sons.
[37] MadisonMilne-Ives,CarolinedeCock,ErnestLim,MelissaHarperShehadeh,NickdePennington,GuyMole,Eduardo
Normando,andEdwardMeinert.2020.Theeffectivenessofartificialintelligenceconversationalagentsinhealthcare:
systematicreview.JournalofmedicalInternetresearch22,10(2020),e20346.
[38] AdamSMiner,NigamShah,KimDBullock,BruceAArnow,JeremyBailenson,andJeffHancock.2019.Keyconsid-
erationsforincorporatingconversationalAIinpsychotherapy.Frontiersinpsychiatry10(2019),746.
[39] JoshuaRMooreandRobertCaudill.2019.Thebotwillseeyounow:ahistoryandreviewofinteractivecomputerized
mentalhealthprograms. PsychiatricClinics42,4(2019),627–634.
[40] AaronNeiman.2021.TheTypingCure:InterrogatingtheTherapeuticAllianceinAustraliaandOnline.SPABiennial
2021:InterrogatingInequalities.
[41] JohnCNorcrossandERINFAlexander.2005. Aprimeronpsychotherapyintegration. Handbookofpsychotherapy
integration2(2005),3–23.
[42] ChinasaTOkolo,SrujanaKamath,NicolaDell,andAdityaVashistha.2021.“Itcannotdoallofmywork”:community
healthworkerperceptionsofAI-enabledmobilehealthapplicationsinruralIndia.InProceedingsofthe2021CHI
ConferenceonHumanFactorsinComputingSystems.1–20.
[43] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,ChongZhang,Sandhini
Agarwal,KatarinaSlama,AlexRay,etal.2022.Traininglanguagemodelstofollowinstructionswithhumanfeedback.
AdvancesinNeuralInformationProcessingSystems35(2022),27730–27744.
[44] SachinRPendse,NehaKumar,andMunmunDeChoudhury.2023. MarginalizationandtheConstructionofMen-
talIllnessNarrativesOnline:ForegroundingInstitutionsinTechnology-MediatedCare. ProceedingsoftheACMon
Human-ComputerInteraction7,CSCW2(2023),1–30.
[45] SachinRPendse,DanielNkemelu,NicolaJBidwell,SushrutJadhav,SoumitraPathare,MunmunDeChoudhury,and
NehaKumar.2022. Fromtreatmenttohealing:envisioningadecolonialdigitalmentalhealth.InProceedingsofthe
2022CHIConferenceonHumanFactorsinComputingSystems.1–23.
[46] CatherinePetrozzino.2021.WhopaysforethicaldebtinAI?AIandEthics1,3(2021),205–208.
[47] SaraReardon.2023. AIChatbotsCouldHelpProvideTherapy,butCautionIsNeeded. ScientificAmerican(14June
2023). https://www.scientificamerican.com/article/ai-chatbots-could-help-provide-therapy-but-caution-is-needed/
Accessed:2023-11-03.
[48] RolandRobertsonetal.1995. Glocalization:Time-spaceandhomogeneity-heterogeneity. Globalmodernities2,1
(1995),25–44.
[49] NathanielRRobinson,PerezOgayo,DavidRMortensen,andGrahamNeubig.2023. Chatgptmt:Competitivefor
high-(butnotlow-)resourcelanguages.arXivpreprintarXiv:2309.07423(2023).
[50] CarlRRogers.1957.Thenecessaryandsufficientconditionsoftherapeuticpersonalitychange.Journalofconsulting
psychology21,2(1957),95.
[51] JoshuaRosenbaum.2002.TheTypingCure.WallStreetJournal(2002),R10.
[52] SaulRosenzweig.1936. Someimplicitcommonfactorsindiversemethodsofpsychotherapy. Americanjournalof
Orthopsychiatry6,3(1936),412.
[53] VictorRoudometof.2016.Glocalization:Acriticalintroduction. Routledge.
[54] StuartJRussellandPeterNorvig.2010.Artificialintelligenceamodernapproach. London.
[55] TimoSchickandHinrichSchütze.2021. It’sNotJustSizeThatMatters:SmallLanguageModelsAreAlsoFew-Shot
Learners.InProceedingsofthe2021ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLin-
guistics:HumanLanguageTechnologies,KristinaToutanova,AnnaRumshisky,LukeZettlemoyer,DilekHakkani-Tur,
IzBeltagy,StevenBethard,RyanCotterell,TanmoyChakraborty,andYichaoZhou(Eds.).AssociationforComputa-
tionalLinguistics,Online,2339–2352. https://doi.org/10.18653/v1/2021.naacl-main.185
[56] JessicaSchleider.2023. LittleTreatments,BigEffects:HowtoBuildMeaningfulMomentsthatCanTransformYour
MentalHealth.Robinson.
[57] Jessica L Schleider, Michael C Mullarkey, Kathryn R Fox, Mallory L Dobias, Akash Shroff, Erica A Hart, and
ChantelleARoulston.2022. Arandomizedtrialofonlinesingle-sessioninterventionsforadolescentdepression
duringCOVID-19.NatureHumanBehaviour6,2(2022),258–268.TheTypingCure 21
[58] HugoScurto,ThomasSimilowski,SamuelBianchini,andBaptisteCaramiaux.2023.ProbingRespiratoryCareWith
GenerativeDeepLearning.ProceedingsoftheACMonHuman-ComputerInteraction7,CSCW2(2023),1–34.
[59] SunayanaSitaram,MonojitChoudhury,BarunPatra,VishravChaudhary,KabirAhuja,andKalikaBali.2023.Every-
thingyouneedtoknowaboutmultilingualLLMs:Towardsfair,performantandreliablemodelsforlanguagesofthe
world.InProceedingsofthe61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume6:Tutorial
Abstracts).21–26.
[60] Petr Slovák, Nikki Theofanopoulou, AlessiaCecchet, PeterCottrell, FerranAltarribaBertran, EllaDagan, Julian
Childs,andKatherineIsbister.2018. "Ijustlethimcry...DesigningSocio-Technical InterventionsinFamiliesto
PreventMentalHealthDisorders.ProceedingsoftheACMonHuman-ComputerInteraction2,CSCW(2018),1–34.
[61] JonathanASmith.2015.Qualitativepsychology:Apracticalguidetoresearchmethods.Qualitativepsychology(2015),
1–312.
[62] DanielJSolove.2007. I’vegotnothingtohideandothermisunderstandingsofprivacy. SanDiegoL.Rev.44(2007),
745.
[63] StanleyWStandal.1954. Theneedforpositiveregard:Acontributiontoclient-centeredtheory. Ph.D.Dissertation.
UniversityofChicago,DepartmentofPsychology.
[64] StanleyRSteindl,MarcelaMatos,andGiancarloDimaggio.2023. Theinterplaybetweentherapeuticrelationship
andtherapeutictechnique:Thewholeismorethanthesumofitsparts. JournalofClinicalPsychology79,7(2023),
1686–1692.
[65] JohnTorousandCharlotteBlease.2024. Generativeartificialintelligenceinmentalhealthcare:potentialbenefits
andcurrentchallenges.WorldPsychiatry23,1(2024),1–2. https://doi.org/10.1002/wps.21148
[66] Yuan-ChiTseng,WeerachayaJarupreechachan,andTuan-HeLee.2023. UnderstandingtheBenefitsandDesignof
ChatbotstoMeettheHealthcareNeedsofMigrantWorkers.ProceedingsoftheACMonHuman-ComputerInteraction
7,CSCW2(2023),1–34.
[67] AlastairCvanHeerden,JuliaRPozuelo,andBrandonAKohrt.2023. Globalmentalhealthservicesandtheimpact
ofartificialintelligence–Poweredlargelanguagemodels.JAMApsychiatry80,7(2023),662–664.
[68] MiltonLWainberg,PamelaScorza,JamesMShultz,LiatHelpman,JenniferJMootz,KarenAJohnson,YuvalNeria,
Jean-MarieEBradford,MariaAOquendo,andMelissaRArbuckle.2017. Challengesandopportunitiesinglobal
mentalhealth:aresearch-to-practiceperspective. Currentpsychiatryreports19(2017),1–10.
[69] BruceEWampoldandZacEImel.2015. Thegreatpsychotherapydebate:Theevidenceforwhatmakespsychotherapy
work.Routledge.
[70] MatthewWeaver.2023.AIchatbot‘encouraged’manwhoplannedtokillqueen,courttold.TheGuardian(6Jul2023).
https://www.theguardian.com/uk-news/2023/jul/06/ai-chatbot-encouraged-man-who-planned-to-kill-queen-court-told
Lastmodifiedon2023-07-0610:49EDT.
[71] JosephWeizenbaum.1976.Computerpowerandhumanreason:Fromjudgmenttocalculation.(1976).
[72] JosephWeizenbaum.1977.Computersas"Therapists".Science198,4315(1977),354–354.
[73] MichaelWhiteandDavidEpston.1990.Narrativemeanstotherapeuticends. WWNorton&Company.
[74] NorbertWiener.1960.SomeMoralandTechnicalConsequencesofAutomation:Asmachineslearntheymaydevelop
unforeseenstrategiesatratesthatbaffletheirprogrammers. Science131,3410(1960),1355–1358.
[75] Chloe Xiang. 2023. Eating Disorder Helpline Fires Staff, Transitions to Chatbot After Unionization.
https://www.vice.com/en/article/n7ezkm/eating-disorder-helpline-fires-staff-transitions-to-chatbot-after-unionization
[76] ChloeXiang.2023. ’HeWouldStillBeHere’:ManDiesbySuicideAfterTalkingwithAIChatbot,WidowSays.
https://www.vice.com/en/article/pkadgm/man-dies-by-suicide-after-talking-with-ai-chatbot-widow-says
[77] Albert Xu, Eshaan Pathak, Eric Wallace, Suchin Gururangan, Maarten Sap, and Dan Klein. 2021. Detox-
ifying Language Models Risks Marginalizing Minority Voices. In Proceedings of the 2021 Conference of the
North AmericanChapter of the Association for Computational Linguistics: HumanLanguage Technologies, Kristina
Toutanova, Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tur, Iz Beltagy, Steven Bethard, Ryan Cotterell,
Tanmoy Chakraborty, and Yichao Zhou (Eds.). Association for Computational Linguistics, Online, 2390–2397.
https://doi.org/10.18653/v1/2021.naacl-main.190
[78] TianXu,JunnanYu,DylanThomasDoyle,andStephenVoida.2023.Technology-MediatedStrategiesforCopingwith
MentalHealthChallenges:InsightsfromPeoplewithBipolarDisorder.ProceedingsoftheACMonHuman-Computer
Interaction7,CSCW2(2023),1–31.
[79] TravisZack,EricLehman,MiracSuzgun,JorgeARodriguez,LeoAnthonyCeli,JudyGichoya,DanJurafsky,Peter
Szolovits,DavidWBates,Raja-ElieEAbdulnour,etal.2024. AssessingthepotentialofGPT-4toperpetuateracial
andgenderbiasesinhealthcare:amodelevaluationstudy.TheLancetDigitalHealth6,1(2024),e12–e22.