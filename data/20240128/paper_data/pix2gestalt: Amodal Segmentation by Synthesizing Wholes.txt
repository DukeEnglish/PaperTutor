pix2gestalt: Amodal Segmentation by Synthesizing Wholes
EgeOzguroglu1 RuoshiLiu1 D´ıdacSur´ıs1 DianChen2 AchalDave2 PavelTokmakov2 CarlVondrick1
1ColumbiaUniversity 2ToyotaResearchInstitute
gestalt.cs.columbia.edu
Abstract closed-world settings, restricted to only operating on the
We introduce pix2gestalt, a framework for zero-shot datasetsonwhichtheytrained.
amodal segmentation, which learns to estimate the shape In this paper, we propose an approach for zero-shot
and appearance of whole objects that are only partially amodalsegmentationandreconstructionbylearningtosyn-
visible behind occlusions. By capitalizing on large-scale thesize whole objects first. Our approach capitalizes on
diffusion models and transferring their representations to denoising diffusion models [14], which are excellent rep-
thistask,welearnaconditionaldiffusionmodelforrecon- resentations of the natural image manifold and capture all
structing whole objects in challenging zero-shot cases, in- different types of whole objects and their occlusions. Due
cluding examples that break natural and physical priors, totheirlarge-scaletrainingdata, wehypothesizesuchpre-
suchasart.Astrainingdata,weuseasyntheticallycurated trained models have implicitly learned amodal representa-
datasetcontainingoccludedobjectspairedwiththeirwhole tions (Figure 2), which we can reconfigure to encode ob-
counterparts. Experimentsshowthatourapproachoutper- jectgroupingandperformamodalcompletion. Bylearning
formssupervisedbaselinesonestablishedbenchmarks.Our fromasyntheticdatasetofocclusionsandtheirwholecoun-
modelcanfurthermorebeusedtosignificantlyimprovethe terparts,wecreateaconditionaldiffusionmodelthat,given
performance of existing object recognition and 3D recon- anRGBimageandapointprompt,generateswholeobjects
structionmethodsinthepresenceofocclusions. behindocclusionsandotherobstructions.
Our main result is showing that we are able to achieve
1.Introduction state-of-the-art amodal segmentation results in a zero-shot
setting, outperforming the methods that were specifically
Although only parts of the objects in Figure 1 are visible,
supervisedonthosebenchmarks.Wefurthermoreshowthat
you are able to visualize the whole object, recognize the
ourmethodcanbeusedasadrop-inmoduletosignificantly
category,andimagineits3Dgeometry.Amodalcompletion
improvetheperformanceofexistingobjectrecognitionand
isthetaskofpredictingthewholeshapeandappearanceof
3D reconstruction methods in the presence of occlusions.
objects that are not fully visible, and this ability is crucial
Anadditionalbenefitofthediffusionframeworkisthatital-
formanydownstreamapplicationsinvision, graphics, and
lowssamplingseveralvariationsofthereconstruction,nat-
robotics. Learned by children from an early age [30], the
urallyhandlingtheinherentambiguityoftheocclusions.
abilitycanbepartlyexplainedbyexperience,butweseem
tobeabletogeneralizetochallengingsituationsthatbreak 2.RelatedWork
natural priors and physical constraints with ease. In fact,
wecanimaginetheappearanceofobjectsduringocclusions Webrieflyreviewrelatedworkinamodalcompletion,anal-
thatcannotexistinthephysicalworld,suchasthehorsein ysisbysynthesis,anddenoisingdiffusionmodelsforvision.
Magritte’sTheBlankSignature.
2.1.AmodalCompletionandSegmentation
What makes amodal completion challenging compared
toothersynthesistasksisthatitrequiresgroupingforboth In this work, we define amodal completion as the task of
thevisibleandhiddenpartsofanobject.Tocompleteanob- generating the image of the whole object [10, 49], amodal
ject,wemustbeabletofirstrecognizetheobjectfrompar- segmentation as generating the segmentation mask of the
tial observations, then synthesize only the missing regions wholeobject[18,21,33,35,53], andamodaldetectionas
fortheobject. Computervisionresearchersandgestaltpsy- predicting the bounding box of the whole object [15, 17].
chologists have extensively studied amodal completion in Most prior work focuses on the latter two tasks, due to
the past [10, 17, 18, 21, 33, 35, 49, 53], creating mod- thechallengesingeneratingthe(possiblyambiguous)pix-
elsthatexplicitlylearnfigure-groundseparation. However, elsbehindanocclusion. Inaddition,toourknowledge,all
the prior work has been limited to representing objects in priorworkonthesetasksislimitedtoasmallclosed-world
1
4202
naJ
52
]VC.sc[
1v89341.1042:viXraAmodal Amodal Amodal Novel Amodal 3D
Input Completion Segmentation View Synthesis Reconstruction
“cow”
“horse”
“person”
“couch”
“seagull”
“chair”
Figure1.AmodalSegmentationandReconstructionviaSynthesis.Wepresentpix2gestalt,amethodtosynthesizewholeobjectsfrom
onlypartiallyvisibleones,enablingamodalsegmentation,recognition,novel-viewsynthesis,and3Dreconstructionofoccludedobjects.
2tasksindifferentdomainsuchasimageediting[6,11,37],
3D [7, 25, 45], and modal segmentation [2, 3, 46]. In this
work, we leverage the strong occlusion and complete ob-
ject priors provided by internet-pretrained diffusion model
tosolvethezero-shotamodalcompletiontask.
3.AmodalCompletionviaGeneration
Figure2.WholeObjects.Pre-traineddiffusionmodelsareableto
GivenanRGBimagexwithanoccludedobjectthatispar-
generateallkindsofwholeobjects.Weshowsamplesconditioned
tially visible, our goal is to predict a new image with the
onacategoryfromStableDiffusion. Weleveragethissynthesis
shape and appearance of the whole object, and only the
abilityforzero-shotamodalreconstructionandsegmentation.
wholeobject. Ourapproachwillacceptanypointormask
asapromptpindicatingthemodalobject:
ofobjects[17,18,21,33,49]ortosyntheticdata[10]. For
example, PCNet [49], the previous state-of-the-art method
xˆ =f (x,p)
for amodal segmentation, operates only on a closed-world p θ
setofclassesinAmodalCOCO[53].
wherexˆ isourestimateofthewholeobjectindicatedbyp.
p
Ourapproach,bycontrast,providesrichimagecomple-
Mapping from x to this unified whole form, i.e. gestalt of
tionswithaccuratemasks,generalizingtodiversezero-shot
theoccludedobject,wenameourmethodpix2gestalt. We
settings, while still outperforming state-of-the-art methods
wantxˆtobeperceptuallysimilartothetruebutunobserved
in a closed-world. To achieve this degree of generaliza-
whole of the object as if there was no occlusion. We will
tion, we capitalize on large-scale diffusion models, which
useaconditionaldiffusionmodel(seeFigure3)forf .
θ
implicitly learn internal representations of whole objects.
Anadvantageofthisapproachisthat,onceweestimate
Weproposetounlockthiscapabilitybyfine-tuningadiffu-
animageofthewholeobjectxˆ,weareabletoperformany
sionmodelonasyntheticallygenerated,realisticdatasetof
othercomputervisiontaskonit,providingaunifiedmethod
variedocclusions.
tohandleocclusionsacrossdifferenttasks.Sincewewilldi-
rectlysynthesizethepixelsofthewholeobject,wecanaid
2.2.AnalysisbySynthesis
off-the-shelfapproachestoperformsegmentation,recogni-
Our approach is heavily inspired by analysis by synthe- tion,and3Dreconstructionofoccludedobjects.
sis [47] – a generative approach for visual reasoning. Im- Toperformamodalcompletion,f needstolearnarepre-
ageparsing[42]wasarepresentativeworkthatunifiesseg- sentationofwholeobjectsinthevisualworld. Duetotheir
mentation, recognition, and detection by generation. Prior scaleoftrainingdata,wewillcapitalizeonlargepretrained
workshaveappliedtheanalysisbysynthesisapproacheson diffusion models, such as Stable Diffusion, which are ex-
various problems including face recognition [5, 42], pose cellent representations of the natural image manifold and
estimation [27, 51], 3D reconstruction [22, 23], semantic havethesupporttogenerateunoccludedobjects. However,
imageediting[1,24,52]. Inthispaper, weaimtoharness althoughtheygeneratehigh-qualityimages,theirrepresen-
the power of generative models trained with internet-scale tationsdonotexplicitlyencodethegroupingofobjectsand
dataforthetaskofamodalcompletion,therebyaidingvari- theirboundariestothebackground.
oustaskssuchasrecognition,segmentation,and3Drecon-
3.1.Whole-PartPairs
structioninthepresenceofocclusions.
To learn the conditional diffusion model f with the ability
2.3.DiffusionModels
for grouping, we build a large-scale paired dataset of oc-
Recently,DenoisingDiffusionProbabilisticModel[14],or cludedobjectsandtheirwholecounterparts. Unfortunately,
DDPM, has emerged as one of the most widely used gen- collectinganaturalimagedatasetofthesepairsischalleng-
erative architectures in computer vision due to its ability ingatscale.Priordatasetsprovideamodalsegmentationan-
to model multi-modal distributions, training stability, and notations[33,53],buttheydonotrevealthepixelsbehind
scalability. [8] first showed that diffusion models outper- anocclusion. Otherdatasetshavereliedongraphicalsimu-
formGANs[12]inimagesynthesis. StableDiffusion[36], lation[16],whichlacktherealisticcomplexityandscaleof
trained on LAION-5B [39], applied diffusion model in everydayobjectcategories.
the latent space of a variational autoencoder [19] to im- Webuildpaireddatabyautomaticallyoverlayingobjects
prove computational efficiency. Later, a series of major over natural images. The original images provide ground-
improvements were made to improve diffusion model per- truthforthecontentbehindocclusions. However,weneed
formance[13,41]. WiththereleaseofStableDiffusionasa to ensure that we only occlude whole objects in this con-
stronggenerativeprior,manyworkshaveadaptedittosolve struction, as otherwise our model could learn to generate
3Input Image + Prompt Synthesized Whole
Gaussian
Noise
pix2gestalt
VAE conditioning:
concat( , , )
“bench”
CLIP conditioning:
Amodal Novel View 3D
Visible (Modal) Mask Latent Diffusion
Segmentation Synthesis Reconstruction
Figure3.pix2gestaltisanamodalcompletionmodelusingalatentdiffusionarchitecture.Conditionedonaninputocclusionimageanda
regionofinterest,thewhole(amodal)formissynthesized,therebyallowingothervisualtaskstobeperformedonittoo. Forconditioning
details,seesection3.2.
incomplete objects. To this end, we use a heuristic that, channelconcatenateE(x)andz ,providinglow-levelvisual
t
if the object is closer to the camera than its neighboring details(shade,color,texture),aswellasE(p)toindicatethe
objects, then it is likely a whole object. We use Segment visibleregionoftheobject.
Anything[20]toautomaticallyfindobjectcandidatesinthe After ϵ is trained, f can generate xˆ by performing it-
θ p
SA-1B dataset, and use the off-the-shelf monocular depth erative denoising [36]. The CFG can be scaled to control
estimatorMiDaS[4]toselectwhichobjectsarewhole. For impactoftheconditioningonthecompletion.
each image with at least one whole object, we sample an
occluderandsuperimposeit,resultinginapaireddatasetof
3.3.AmodalBaseRepresentations
837K imagesandtheirwholecounterparts. Figure4illus-
tratesthisconstructionandshowsexamplesoftheheuristic. Since we synthesize RGB images of the whole object, our
approach makes it straightforward to equip various com-
3.2.ConditionalDiffusion
putervisionmethodswiththeabilitytohandleocclusions.
Givenpairsofanimagexanditswholecounterpartxˆ p,we Wediscussafewcommoncases.
fine-tune a conditional diffusion model to perform amodal
Image Segmentation aims to find the spatial bound-
completion while maintaining the zero-shot capabilities of aries of an object given an image x and an initial prompt
the pre-trained model. We solve for the following latent p. We can perform amodal segmentation by completing
diffusionobjective: an occluded object with f, then thresholding the result to
min E (cid:2) ||ϵ−ϵ (z ,E(x),t,E(p),C(x))||2(cid:3) obtain an amodal segmentation map. Note that this prob-
θ z∼E(x),t,ϵ∼N(0,1) θ t 2 lemisunder-constrainedastherearemultiplepossiblesolu-
tions. Giventheuncertainty,wefoundthatsamplingmulti-
where0≤t<1000isthediffusiontimestep,z istheem-
t
plecompletionsandperformingamajorityvoteontheseg-
beddingofthenoisedamodaltargetimagexˆ . C(x)isthe
p
mentationmasksworksbestinpractice.
CLIPembeddingoftheinputimage,andE(·)isaVAEem-
bedding. Following [6, 25], we apply classifier-free guid- Object Recognition is the task of classifying an object
ance (CFG) [13] by settingthe conditional information to located in an bounding box or mask p. We can zero-shot
anullvectorrandomly. recognizesignificantlyoccludedobjectsbyfirstcompleting
Amodalcompletionrequiresreasoningaboutthewhole thewholeobjectwithf, thenclassifyingtheamodalcom-
shape, its appearance, and contextual visual cues of the pletionwithCLIP.
scene. Weadaptthedesignin [6,25]toconditionthedif- 3DReconstructionestimatestheappearanceandgeom-
fusion model ϵ in two separate streams. C(x) conditions etryofanobject. Wecanzero-shotreconstructobjectswith
θ
the diffusion model ϵ via cross-attention on the semantic partialocclusionsbyfirstcompletingthewholeobjectwith
θ
features of the partially visible object in x as specified by f,thenapplyingSyncDreamerandScoreDistillationSam-
p,providinghigh-levelperception. OntheVAEstream,we pling[32]toestimateatexturedmesh.
4Figure4.ConstructingTrainingData.Toensureweonlyoccludewholeobjects,weuseaheuristicthatobjectsclosertothecamerathan
itsneighborsarelikelywholeobjects. Thegreenoutlinearoundtheobjectshowswheretheestimateddepthisclosertothecamerathan
thebackground(theredshowswhenitisnot).
4.Experiments forourmethod.
Results. Table1comparespix2gestaltwithpriorwork.
We evaluate pix2gestalt’s ability to perform zero-shot
DespitenevertrainingontheCOCO-Adataset,ourmethod
amodal completion for three tasks: amodal segmentation,
outperforms all baselines, including PCNet, which uses
occludedobjectrecognition,andamodal3Dreconstruction.
COCO-Aimagesfortraining,andevenPCNet-Sup,which
Weshowthatourmethodprovidesamodalcompletionsthat
issupervisedusinghuman-annotatedamodalsegmentations
directlyleadtostrongresultsinalltasks.
fromCOCO-A’strainingset. Comparedtootherzero-shot
4.1.AmodalSegmentation methods, our improvements are dramatic, validating the
generalizationabilitiesofourmethod.Notably,wealsoout-
Setup. Amodal segmentation requires segmenting the full
performtheinpaintingbaselinewhichisbasedoffalarger,
extentofa(possiblyoccluded)object.Weevaluatethistask
morerecent variantofStable Diffusion[31]. This demon-
ontheAmodalCOCO(COCO-A)[53]andAmodalBerke-
stratesthatinternet-scaletrainingaloneisnotsufficientand
ley Segmentation (BSDS-A) datasets [28]. For evaluation,
our fine-tuning approach is key to reconfigure priors from
COCO-Aprovides13,000amodalannotationsofobjectsin
pre-trainingforamodalcompletion.
2,500 images, while BSDS-A provides 650 objects from
We further analyze amodal completions qualitatively in
200 images. For both datasets, we evaluate methods that
Figure 6. WhileSD-XLoftenhallucinatesextraneous,un-
take as input an image and a (modal) mask of the visible
realistic details (e.g. person in front of the bus in the sec-
extentofanobject,andoutputanamodalmaskofthefull-
ond row), PCNet tends to fail to recover the full extent of
extent of the object. Following [49], we evaluate segmen-
objects—oftenonlygeneratingthevisibleregion,asinthe
tationsusingmeanintersection-over-union(mIoU).Wefol-
Marioexampleinthethirdrow.Incontrast,pix2gestaltpro-
lowthestrategyinSection3.3toconvertouramodalcom-
vides accurate, complete reconstructions of occluded ob-
pletionsintosegmentationmasks.
jectsonbothCOCO-A(Figure 6)andBSDS-A(Figure 7).
We evaluate three baselines for amodal segmentation.
Our method generalizes well beyond the typical occlusion
ThefirstmethodisPCNet[49],whichistrainedforamodal
scenariosfoundinthosebenchmarks. Figure5showssev-
segmentationspecificallyforCOCO-A.Next, wecompare
eral examples of out-of-distribution images, including art
totwozero-shotmethods,whichdonottrainonCOCO-A:
pieces,illusions,andimagestakenbyourselvesthataresuc-
Segment Anything (SAM) [20], a strong modal segmen-
cessfully handled by our method. Note that no prior work
tation method, and Inpainting using Stable Diffusion-XL
hasshownopen-worldgeneralization(see2.1).
[31]. To evaluate inpainting methods, we provide as input
an image with all but the visible object region erased, and Figure 8 illustrates the ability of the approach to gen-
convertthecompletedimageoutputbythemethodintoan erate diverse samples in shape and appearance when there
amodal segmentation mask following the same strategy as is uncertainty in the final completion. For example, it is
5Figure 5. In-the-wild Amodal Completion and Segmentation. We find that pix2gestalt is able to synthesize whole objects in novel
situations,includingartisticpieces,imagestakenbyaniPhone,andillusions.
Table1. AmodalSegmentationResults. WereportmIoU(%) der occlusions. The former consists of partially occluded
↑onAmodalCOCO[53]andonAmodalBerkeleySegmentation objects, whereas Separated COCO contains objects whose
Dataset [28, 53]. ∗PCNet-Sup trains using ground truth amodal
modalregionisseparatedintodisjointsegmentsbytheoc-
masksfromCOCO-Amodal.SeeSection4.1foranalysis.
cluder(s), resulting in a more challenging problem setting.
We evaluate on all 80 COCO semantic categories in the
Zero-shot Method COCO-A BSDS-A
datasetsusingTop1andTop3accuracy.
✗ PCNet [49] 81.35 - WeuseCLIP[34]asthebaseopen-vocabularyclassifier.
✗ PCNet-Sup∗[49] 82.53∗ - As baselines, we evaluate CLIP without any completion,
reportingthreevariants: providingtheentireimage(CLIP),
(cid:68) SAM[20] 67.21 65.25
providingtheentireimagewithavisualprompt(aredcir-
(cid:68) SD-XLInpainting[31] 76.52 74.19 cle,asinShtedritskietal.[40])aroundtheoccludedobject,
(cid:68) Ours 82.87 80.76 orprovidinganimagewithallbutthevisibleportionofthe
occludedobjectmaskedout. Toevaluateourapproach,we
(cid:68) Ours: Bestof3 87.10 85.68
firstutilizeittocompletetheoccludedobject,andthenclas-
sifytheoutputimageusingCLIP.
Results. Table 2 compares our method with the base-
able to synthesize several plausible completions of the oc-
lines. Visualpromptingwitharedcircle(RC)andmasking
cludedhouseinthepainting. Wequantitativelyevaluatethe
allbutthevisibleobject(Vis.Obj.) provideimprovements
diversityofoursamplesinthelastrowofTable1bysam-
overdirectlypassingtheimagetoCLIPonthesimplerOc-
plingfromourmodelthreetimesandreportingtheperfor-
cluded COCO benchmark, but fail to improve, and some
manceforthebestsample(“Bestof3”). Finally,wefound
timesevendecreasestheperformanceofthebaselineCLIP
limitations of our approach in situations that require com-
on the more challenging Separated COCO variant. Our
monsenseorphysicalreasoning. Weshowtwoexamplesin
method (Ours + CLIP), however, strongly outperforms all
Figure9.
baselinesforboththeoccludedandseparateddatasets,ver-
ifyingthequalityofourcompletions.
4.2.OccludedObjectRecognition
4.3.Amodal3DReconstruction
Next,weevaluatetheutilityofourmethodforrecognizing
occludedobjects. Finally, we evaluate our method for improving 3D recon-
Setup. We use the Occluded and Separated COCO structionofoccludedobjects.
benchmarks [48] for evaluating classification accuracy un- Setup.Wefocusontwotasks:Novel-viewsynthesisand
6Input Occlusion Input Modal Mask PCNet SD-XL Inpainting Ours GT Amodal Mask
Figure 6. Amodal Completion and Segmentation Qualitative Results on Amodal COCO. In blue circles, we highlight completion
regionsthat,uponacloserlook,haveadistortedtextureinthePCNetbaseline,andacorrectoneinourresults.
single-view3Dreconstruction. holdobjects3Dscannedforuseinembodied,synthetic,and
3Dperceptiontasks. Weuse30randomlysampledobjects
To demonstrate pix2gestalt’s performance as a drop-in
fromGSOrangingfromdailyobjectstoanimals. Foreach
moduleto3Dfoundationmodels[25,26,38],wereplicate
object, we render a 256x256 image with synthetic occlu-
theevaluationprocedureofZero-1-to-3[25,26]onGoogle
sionssampledfromthefulldatasetof1,030objectsinGSO.
Scanned Objects (GSO) [9], a dataset of common house-
7Figure 7. Amodal Berkeley Segmentation Dataset Qualitative Results. Our method provides accurate, complete reconstructions of
occludedobjects.
Figure8.DiversityinSamples.Amodalcompletionhasinherent Figure 9. Common-sense and Physics Failures. Left: recon-
uncertainties. By sampling from the diffusion process multiple structionhasthecargoinginthewrongdirection. Right: recon-
times, themethodsynthesizesmultipleplausiblewholesthatare struction contradicts physics, failing to capture that a hand must
consistentwiththeinputobservations. beholdingthedonutbox.
Table2.OccludedObjectRecognition.Wereportzero-shotclas-
sificationaccuracyonOccludedandSeparatedCOCO[48].While
metricIoUandChamferDistancemetrics. Wecompareour
simple baselines fail to improve CLIP performance in the more
approach with SyncDreamer [26], a 3D generative model
challengingSeparatedCOCOsetting,ourmethodconsistentlyim-
that fine-tunes Zero123-XL [7, 25] for multi-view consis-
provesrecognitionaccuracybylargemargins.SeeSection4.2for
tentnovelviewsynthesisandconsequent3Dreconstruction
analysis.
withNeuS[43]andNeRF[29]. Ourfirstbaselineprovides
asinputtoSyncDreamerthesegmentationmaskofallfore-
Method Top1Acc. (%)↑ Top3Acc. (%)↑
ground objects, following the standard protocol. To avoid
Occluded Sep. Occluded Sep.
reconstructing occluded objects, we additionally evaluate
CLIP[34] 23.33 26.04 43.84 43.19 twovariantsthatuseSAM[20]toestimatethemaskofonly
CLIP+RC[40] 23.46 25.64 43.86 43.24 theobjectofinterest,orthegroundtruthmaskfortheobject
Vis. Obj. +CLIP 34.00 21.10 49.26 34.70 ofinterest(GTMask). Finally,toevaluateourmethod,we
Ours+CLIP 43.39 31.15 58.97 45.77 provide as input the full object completed by our method,
along with the corresponding amodal mask. We evaluate
twovariantsofourmethod:Onewhereweprovideamodal
maskfortheobjectofinterestedasestimatedbySAM(Ours
Werenderfromarandomlysampledviewtoavoidcanoni-
(SAMMask))andonewhereweusethegroundtruthmodal
calposes,andgeneratetwooccludedimagesforeachofthe
mask(Ours(GTMask)).
30objects,resultingin60samples.
Foramodalnovel-viewsynthesis,wequantitativelyeval- Results. We compare our approach with the two base-
uate our method using 3 metrics: PSNR, SSIM [44], and lines in Table 4 for novel view synthesis and Table 3 for
LPIPS[50],measuringtheimage-similarityoftheinputand 3Dreconstruction. Quantitativeresultsdemonstratethatwe
groundtruthviews.For3Dreconstruction,weusetheVolu- stronglyoutperformthebaselinesforbothtasks. Innovel-
8Input View Novel View 3D Geometry Input View Novel View 3D Geometry
Figure 10. Amodal 3D Reconstruction qualitative results. The object of interest is specified by a point prompt, shown in yellow.
Incorporatingpix2gestaltasadrop-inmoduletostate-of-the-art3Dreconstructionmodelsallowsustoaddresschallenginganddiverse
occlusionscenarioswithease.
Table 3. Single-view 3D Reconstruction. We report Chamfer Table 4. Novel-view synthesis from one image. We report re-
Distance and Volumetric IoU for Google Scanned Objects. See sultsonGoogleScannedObjects[9].NoteSSIMmeasuresimage
Section4.3foranalysis. quality,notnovel-viewaccuracy.SeeSection4.3foranalysis.
CD↓ IoU↑ LPIPS↓ PSNR↑ SSIM↑
SyncDreamer[26] 0.0884 0.2741 SyncDreamer[26] 0.3221 11.914 0.6808
SAMMask+SyncDr. 0.1182 0.0926 SAM+SyncDr. 0.3060 12.432 0.7248
Ours(SAMMask)+SyncDr. 0.0784 0.3312 Ours(SAMMask)+SyncDr. 0.2848 13.868 0.7211
GTMask+SyncDr. 0.1084 0.1027 GTMask+SyncDr. 0.2905 12.561 0.7322
Ours(GTMask)+SyncDr. 0.0681 0.3639 Ours(GTMask)+SyncDr. 0.2631 14.657 0.7328
view synthesis, we outperform SAM + SyncDreamer on qualitativeevaluationfor3Dreconstructionofoccludedob-
the image reconstruction metrics, LPIPS [50] and PSNR jects,rangingfromanEscherlithographtoin-the-wildim-
[44]. Compared to SAM as a modal pre-processor, we ages.
obtain these improvements as a drop-in module to Sync-
Dreamerwhilestillretainingequivalentimagequality(Ta- 5.Conclusion
ble4, SSIM[44]). Withgroundtruthmaskinputs, weob-
tain further image reconstruction gains. Moreover, even In this work, we proposed a novel approach for zero-shot
though our approach utilizes an additional diffusion step amodal segmentation via synthesis. Our model capitalizes
comparedtoSyncDreameronly,wedemonstratelessimage on whole object priors learned by internet-scale diffusion
qualitydegradation. modelsandunlocksthemviafine-tuningonasynthetically
For reconstruction of the 3D geometry, our fully auto- generated dataset of realistic occlusions. We then demon-
maticmethodoutperformsallofthebaselinesforbothvol- stratedthatsynthesizingthewholeobjectmakesitstraight-
umetric IoU and Chamfer distance metrics, even the base- forwardtoequipvariouscomputervisionmethodswiththe
linesthatusegroundmasks. Providingthegroundtruthto abilitytohandleocclusions.Inparticular,wereportedstate-
ourapproachfurtherimprovestheresults. Figure10shows of-theartresultsonseveralbenchmarksforamodalsegmen-
9tation,occludedobjectrecognitionand3Dreconstruction. [14] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu-
Acknowledgements: This research is based on work sionprobabilisticmodels. NeurIPS,33,2020. 1,3
partially supported by the Toyota Research Institute, the [15] Cheng-YenHsieh,TarashaKhurana,AchalDave,andDeva
DARPA MCS program under Federal Agreement No. Ramanan. Trackinganyobjectamodally,2023. 1
N660011924032, the NSF NRI Award #1925157, and the [16] Y.-T. Hu, H.-S. Chen, K. Hui, J.-B. Huang, and A. G.
NSF AI Institute for Artificial and Natural Intelligence Schwing. SAIL-VOS: Semantic Amodal Instance Level
VideoObjectSegmentation–ASyntheticDatasetandBase-
Award #2229929. DS is supported by the Microsoft PhD
lines. InProc.CVPR,2019. 3
Fellowship.
[17] AbhishekKar, ShubhamTulsiani, JoaoCarreira, andJiten-
draMalik.Amodalcompletionandsizeconstancyinnatural
References
scenes. InICCV,2015. 1,3
[18] LeiKe,Yu-WingTai,andChi-KeungTang.Deepocclusion-
[1] Rameen Abdal, Yipeng Qin, and Peter Wonka. Im-
aware instance segmentation with overlapping bilayers. In
age2StyleGAN:Howtoembedimagesintothestyleganla-
CVPR,2021. 1,3
tentspace? InICCV,2019. 3
[19] DiederikPKingmaandMaxWelling. Auto-encodingvaria-
[2] Tomer Amit, Tal Shaharbany, Eliya Nachmani, and Lior
tionalbayes. arXivpreprintarXiv:1312.6114,2013. 3
Wolf. Segdiff: Image segmentation with diffusion proba-
[20] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,
bilisticmodels. arXivpreprintarXiv:2112.00390,2021. 3
ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite-
[3] Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov,
head, Alexander C. Berg, Wan-Yen Lo, Piotr Dolla´r, and
ValentinKhrulkov,andArtemBabenko. Label-efficientse-
Ross Girshick. Segment anything. In ICCV, 2023. 4, 5,
manticsegmentationwithdiffusionmodels. arXivpreprint
6,8
arXiv:2112.03126,2021. 3
[21] HuanLing,DavidAcuna,KarstenKreis,SeungWookKim,
[4] ReinerBirkl,DianaWofk,andMatthiasMu¨ller. Midasv3.1
and Sanja Fidler. Variational amodal object completion.
–amodelzooforrobustmonocularrelativedepthestimation.
NeurIPS,2020. 1,3
arXivpreprintarXiv:2307.14460,2023. 4
[22] RuoshiLiuandCarlVondrick.Humansaslightbulbs:3dhu-
[5] VolkerBlanzandThomasVetter.Amorphablemodelforthe
manreconstructionfromthermalreflection. InCVPR,2023.
synthesisof3dfaces. InSeminalGraphicsPapers:Pushing
3
theBoundaries,Volume2,pages157–164.2023. 3
[23] RuoshiLiu,SachitMenon,ChengzhiMao,DennisPark,Si-
[6] TimBrooks,AleksanderHolynski,andAlexeiA.Efros. In-
mon Stent, and Carl Vondrick. Shadows shed light on 3d
structpix2pix:Learningtofollowimageeditinginstructions.
objects. arXivpreprintarXiv:2206.08990,2022. 3
InCVPR,2023. 3,4
[24] Ruoshi Liu, Chengzhi Mao, Purva Tendulkar, Hao Wang,
[7] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong
andCarlVondrick. Landscapelearningforneuralnetwork
Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Chris- inversion. InICCV,2023. 3
tian Laforte, Vikram Voleti, Samir Yitzhak Gadre, et al.
[25] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
Objaverse-xl:Auniverseof10m+3dobjects.arXivpreprint
makov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3:
arXiv:2307.05663,2023. 3,8
Zero-shotoneimageto3dobject. InICCV,2023. 3,4,7,8
[8] PrafullaDhariwalandAlexanderNichol. Diffusionmodels
[26] YuanLiu,ChengLin,ZijiaoZeng,XiaoxiaoLong,Lingjie
beatgansonimagesynthesis. NeurIPS,2021. 3
Liu, Taku Komura, and Wenping Wang. Syncdreamer:
[9] LauraDowns,AnthonyFrancis,NateKoenig,BrandonKin- Learning to generate multiview-consistent images from a
man,RyanHickman,KristaReymann,ThomasB.McHugh, single-viewimage. arXivpreprintarXiv:2309.03453,2023.
and Vincent Vanhoucke. Google scanned objects: A high- 7,8,9
quality dataset of 3D scanned household items. In ICRA, [27] Wufei Ma, Angtian Wang, Alan Yuille, and Adam Ko-
2022. 7,9 rtylewski. Robust category-level 6D pose estimation with
[10] KianaEhsani, RoozbehMottaghi, andAliFarhadi. Segan: coarse-to-finerenderingofneuralfeatures. InECCV,2022.
Segmentingandgeneratingtheinvisible. InCVPR,2018. 1, 3
3 [28] D. Martin, C. Fowlkes, D. Tal, and J. Malik. A database
[11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash- of human segmented natural images and its application to
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen- evaluatingsegmentationalgorithmsandmeasuringecologi-
Or. An image is worth one word: Personalizing text-to- calstatistics. InICCV,2001. 5,6
image generation using textual inversion. arXiv preprint [29] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
arXiv:2208.01618,2022. 3 JonathanT.Barron,RaviRamamoorthi,andRenNg. Nerf:
[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Representingscenesasneuralradiancefieldsforviewsyn-
Xu,DavidWarde-Farley,SherjilOzair,AaronCourville,and thesis. InECCV,2020. 8
YoshuaBengio.Generativeadversarialnets.NeurIPS,2014. [30] JeanPiaget. Theconstructionofrealityinthechild. Rout-
3 ledge,2013. 1
[13] Jonathan Ho and Tim Salimans. Classifier-free diffusion [31] Dustin Podell, Zion English, Kyle Lacey, Andreas
guidance. arXivpreprintarXiv:2207.12598,2022. 3,4 Blattmann, Tim Dockhorn, Jonas Mu¨ller, Joe Penna, and
10Robin Rombach. Sdxl: Improving latent diffusion models [47] Alan Yuille and Daniel Kersten. Vision as bayesian infer-
forhigh-resolutionimagesynthesis,2023. 5,6 ence: analysisbysynthesis? Trendsincognitivesciences,
[32] BenPoole,AjayJain,JonathanTBarron,andBenMilden- 10(7):301–308,2006. 3
hall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv [48] GuanqiZhan,WeidiXie,andAndrewZisserman.Atri-layer
preprintarXiv:2209.14988,2022. 4 plugintoimproveoccludeddetection. BMVC,2022. 6,8
[33] Lu Qi, Li Jiang, Shu Liu, Xiaoyong Shen, and Jiaya Jia. [49] Xiaohang Zhan, Xingang Pan, Bo Dai, Ziwei Liu, Dahua
AmodalinstancesegmentationwithKINSdataset.InCVPR, Lin, and Chen Change Loy. Self-supervised scene de-
2019. 1,3 occlusion. InCVPR,2020. 1,3,5,6
[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya [50] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, and Oliver Wang. The unreasonable effectiveness of deep
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen featuresasaperceptualmetric. InCVPR,2018. 8,9
Krueger, and Ilya Sutskever. Learning transferable visual
[51] YiZhang,PengliangJi,AngtianWang,JieruMei,AdamKo-
modelsfromnaturallanguagesupervision,2021. 6,8
rtylewski,andAlanYuille.3D-Awareneuralbodyfittingfor
[35] N Dinesh Reddy, Robert Tamburo, and Srinivasa G occlusionrobust3dhumanposeestimation. InICCV,2023.
Narasimhan. Walt: Watch and learn 2d amodal represen- 3
tationfromtime-lapseimagery. InCVPR,2022. 1
[52] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-
[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
domain GAN inversion for real image editing. In ECCV,
PatrickEsser,andBjo¨rnOmmer.High-resolutionimagesyn-
2020. 3
thesiswithlatentdiffusionmodels. InCVPR,2022. 3,4
[53] Yan Zhu, Yuandong Tian, Dimitris Metaxas, and Piotr
[37] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Dolla´r. Semanticamodalsegmentation. InCVPR,2017. 1,
MichaelRubinstein,andKfirAberman. Dreambooth: Fine
3,5,6
tuning text-to-image diffusion models for subject-driven
generation. InCVPR,2023. 3
[38] KyleSargent,ZizhangLi,TanmayShah,CharlesHerrmann,
Hong-XingYu,YunzhiZhang,EricRyanChan,DmitryLa-
gun, Li Fei-Fei, Deqing Sun, and Jiajun Wu. ZeroNVS:
Zero-shot360-degreeviewsynthesisfromasinglerealim-
age. arXivpreprintarXiv:2310.17994,2023. 7
[39] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man,etal.Laion-5B:Anopenlarge-scaledatasetfortraining
nextgenerationimage-textmodels. NeurIPS,2022. 3
[40] Aleksandar Shtedritski, Christian Rupprecht, and Andrea
Vedaldi. What does clip know about a red circle? visual
promptengineeringforvlms. InICCV,2023. 6,8
[41] Jiaming Song, Chenlin Meng, and Stefano Ermon.
Denoising diffusion implicit models. arXiv preprint
arXiv:2010.02502,2020. 3
[42] Zhuowen Tu, Xiangrong Chen, Alan L Yuille, and Song-
ChunZhu.Imageparsing:Unifyingsegmentation,detection,
and recognition. International Journal of computer vision,
63:113–140,2005. 3
[43] PengWang,LingjieLiu,YuanLiu,ChristianTheobalt,Taku
Komura,andWenpingWang.Neus:Learningneuralimplicit
surfacesbyvolumerenderingformulti-viewreconstruction.
arXivpreprintarXiv:2106.10689,2021. 8
[44] ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSi-
moncelli. Imagequalityassessment: fromerrorvisibilityto
structuralsimilarity. IEEETransactionsonImageProcess-
ing,13(4):600–612,2004. 8,9
[45] RundiWu,RuoshiLiu,CarlVondrick,andChangxiZheng.
Sin3dm: Learning a diffusion model from a single 3d tex-
turedshape. arXivpreprintarXiv:2305.15399,2023. 3
[46] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xi-
aolong Wang, and Shalini De Mello. Open-Vocabulary
PanopticSegmentationwithText-to-ImageDiffusionMod-
els. arXivpreprintarXiv:2303.04803,2023. 3
11