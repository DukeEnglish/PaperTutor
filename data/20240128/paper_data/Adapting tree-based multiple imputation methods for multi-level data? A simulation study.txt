Adapting tree-based multiple imputation methods for multi-level
data? A simulation study
Ketevan Gurtskaiaa*, Jakob Schwerter a* and Philipp Doeblera
aDepartment of Statistics, TU Dortmund University, Dortmund, Germany.
*Shared first authorship
ARTICLE HISTORY
Compiled January 26, 2024
Abstract
This simulation study evaluates the effectiveness of multiple imputation (MI) tech-
niques for multilevel data. It compares the performance of traditional Multiple Im-
putation by Chained Equations (MICE) with tree-based methods such as Chained
Random Forests with Predictive Mean Matching and Extreme Gradient Boosting.
Adaptedversionsthatincludedummyvariablesforclustermembershiparealsoin-
cludedforthetree-basedmethods.Methodsareevaluatedforcoefficientestimation
bias,statisticalpower,andtypeIerrorratesonsimulatedhierarchicaldatawithdif-
ferentclustersizes(25and50)andlevelsofmissingness(10%and50%).Coefficients
are estimated using random intercept and random slope models. The results show
thatwhileMICEispreferredforaccuraterejectionrates,ExtremeGradientBoost-
ing is advantageous for reducing bias. Furthermore, the study finds that bias levels
aresimilaracrossdifferentclustersizes,butrejectionratestendtobelessfavorable
with fewer clusters (lower power, higher type I error). In addition, the inclusion of
cluster dummies in tree-based methods improves estimation for Level 1 variables,
butislesseffectiveforLevel2variables.WhendatabecometoocomplexandMICE
is too slow, extreme gradient boosting is a good alternative for hierarchical data.
KEYWORDS
multiple imputation; multi-level data; MICE; missRanger; mixgb
Communicating author contact details: Jakob Schwerter ( 0000-0001-5818-2431), TU Dortmund Uni-
versity, Department of Statistics, Martin-Schmeisser-Weg 13, D-44227 Dortmund, Germany. Email:
jakob.schwerter@tu-dortmund.de,
4202
naJ
52
]PA.tats[
1v16141.1042:viXra1. Introduction
The issue of missing data, historically overlooked due to limited understanding and
computational capabilities, gained prominence in the early 1970s, a pivotal period
marked by technological advances (Heymann, 2017; Schafer & Olsen, 1998). Rubin
(1976) recognized the shortcomings of conventional methods and challenged the idea
that the causes of missing data were unimportant. Building on this, Little and Rubin
(2002) elaborated on the treatment of missingness indicators as random variables and
discussed contemporary methods for dealing with the problem. This paper examines
new technological advances in the area of tree-based imputation methods to test how
well they perform in terms of bias and inference in hierarchical data.
Missing data can occur for a variety of reasons, ranging from respondents’ refusal
to answer certain questions in a survey – known as “item nonresponse” – to data
loss during collection or storage processes, or intentional omission (Graham, 2012).
Complete case analysis, i.e., deleting incomplete data prior to analysis, can have a
significant impact on the validity and reliability of the analysis, as ignoring missing
data often leads to biased results and incorrect conclusions and should therefore be
avoided (van der Heijden, Donders, Stijnen, & Moons, 2006).
A prominent and widely used technique for dealing with missing data is multiple
imputation (MI), which involves creating multiple reasonable values for each missing
value,sothatmultiplecompletedatasetsaregenerated(Little&Rubin,2002).Eachof
the complete data sets is used separately to perform statistical analysis using standard
statisticaltechniques.Theresultsoftheanalysesarethencombinedusingmethodsthat
account for the variability of the imputed values, resulting in more reliable estimates
(Rubin, 1987). MI is particularly useful when a large amount of data is missing and
the missingness mechanism is different from missing completely at random (Little &
Rubin, 2002; Schafer, 2000).
Imputation models should preserve the relationships that exist in the data and
account for the process that created the missing data (van Buuren & Groothuis-
Oudshoorn, 2011). For hierarchical data with multiple levels, such as clustering at
individual and higher-level units like classes or schools, maintaining the hierarchical
structure is challenging and often overlooked (van Buuren, 2011). Although fully con-
ditional specification (FCS) or multiple imputation by chained equations (MICE) is
a prevalent approach in the social sciences, it presents several limitations, including
its considerable complexity due to the challenges of the specification of imputation
model and computational intensity (van Buuren & Groothuis-Oudshoorn, 2011). Since
MICE heavily relies on model specifications, it can lead to issues like overfitting and
convergence errors, especially when dealing with multicollinearity and other instability
problems (van Buuren & Oudshoorn, 1999). In addition, MICE is time-consuming and
itsuserestrictedincaseswhere,forexample,dataneedstoberesampledorthenumber
ofvariablesis(too)high.ExploringalternativestoMICE,suchasnon-parametrictree-
based methods, which are less assumption-dependent, can enhance the robustness and
reliabilityofstatisticalanalysisinempiricalstudies.Therefore,inthissimulationstudy,
weaddressthefollowingresearchquestions:Dotree-basedimputationmethodsexhibit
similar performance in terms of bias, type I error, and power compared to the stan-
dard level-2 imputation method? The specific tree-based methods are chained random
forests (Stekhoven & Bühlmann, 2012; Tang & Ishwaran, 2017) and extreme gradi-
ent boosting (Deng, 2023). To accommodate the data structure, we also use dummy
variables for each cluster to adapt the tree-based imputation methods by incorpo-
rating the multilevel structure of the data (Lüdtke, Robitzsch, & Grund, 2017). We
2examinetheperformancewith25and50clusterstoevaluatehowtheresultsdependon
whetherthedatacontainafewclusters(25)oralargenumberofclusters(50).Though
many different methods are suggested to be used in MI, like multilevel MI with joint
modeling, multilevel MI with fully conditional specifications, multilevel substantive-
model-compatible MI with sequential modeling, model-based treatment with Bayesian
estimation, as reviewed recently by (Grund, Lüdtke, & Robitzsch, 2024), tree-based
methods have not been evaluated for multilevel data.
The remainder of the paper is organized as follows: In the next section, we briefly
review multilevel data and existing (parametric) MI approaches for multilevel data
structures. Next, the simulation study is detailed in Section 3, starting with a review
oftheimputationapproaches,anexplanationofthesimulatedmissingnessmechanisms
andthefittedlinearmixedmodels.WepresentsimulationresultsinSection5alsowith
thehelpofdotplotsfortherelativeperformance,andwecloseinSection5withremarks
on the current study and potential further work.
2. Multilevel data
Multilevel data structures are common in social and behavioral sciences research (Hox
&Roberts,2010).Thisisoftenseenin,e.g.,educationalresearchwherestudents(Level
1) are nested within classes, schools, or regions (Level 2) (Hox & Roberts, 2010).
The presence of higher-level variables can significantly influence the outcome variable,
callingforrobustanalysismethodsthataccountforthecomplexityintroducedbythese
hierarchical structures (Grund et al., 2024; Steenbergen & Jones, 2002), since simply
ignoring the dependencies or aggregating everything to a single level can be deceptive
(Aitkin & Longford, 1986; Grund et al., 2024).
Tomaintainstatisticalintegrity,itiscrucialtoemployappropriateimputationmeth-
odstailoredformultileveldata.Thesemethodsshouldconsiderbothwithin-clusterand
between-cluster variability for a more accurate representation of the underlying data
(Audigier et al., 2018; Grund et al., 2024; van Buuren, 2011). Ignoring the nested
structure of data through aggregation or disaggregation to a single level is suboptimal
and can result in misleading conclusions (Aitkin & Longford, 1986). To address this,
multilevel modeling techniques, such as hierarchical linear models including the ran-
dom intercept model as well as the random intercept and random slope model, have
been developed to address the variance components properly at each level, providing
a sophisticated statistical framework for analyzing hierarchical data structures (Hox,
1998; Hox & Roberts, 2010).
2.1. Existing imputation approaches for multilevel data
For the multilevel analysis to be valid, the dependency between observations (multi-
level structure) should be taken into account in the imputation model. Otherwise, the
estimations might be biased even when the statistical methods are appropriate (Au-
digier et al., 2018; Hox & Roberts, 2010). A study from Enders, Mistler, and Keller
(2016) compared two imputation frameworks for multilevel data: joint modeling (JM)
and chained equation imputation (MICE with a two-level normal model 2l.norm).
The joint model turned out to be better for analysis postulating distinct within- and
between-cluster relations and chained equations imputation turned out to be supe-
rior in random slope analysis (Enders et al., 2016). With JM, imputations are created
according to a joint model for all variables simultaneously as draws from the fitted
3distribution, whereas the observations are grouped according to missing data patterns
(van Buuren in Hox & Roberts, 2010, pp. 173-196).
Next to JM, another standard MI procedures for multivariate multilevel data is a
fully conditional specification ofMI(FCS).ImputationswithFCSareconstructedona
variable-by-variable basis. For each variable with missing parts, an imputation model
is specified and imputations are generated iteratively (van Buuren, 2012).
Forclustereddatabothofthesemethodsareeffectiveinthebroadcontextofrandom
intercept models, even with variables at higher levels, as simulation results by Grund,
Lüdtke, and Robitzsch (2018b) indicate. However, for random slopes models, FCS
appears to be more flexible than JM, but still has some limitations and is not all
that reliable when data in an explanatory variable is missing (Grund et al., 2018b).
SimilarresultswereshownbyEndersetal.(2016)forrandomintercepts.Althoughthey
also show that for random slope analysis, chained equations imputation, which is an
imputation implementation under conditionally specified models, creates a substantial
upgrade over JM. Additionally, Grund, Lüdtke, and Robitzsch (2018a) show that both
approaches(chainedequationsandjointmodeling)provideusefultoolsfordealingwith
missing data at Level 2 in most applications in practice (especially for balanced data).
One advantage of choosing FCS over JM is that FCS grants more flexibility in
creatingmultilevelmodelsbysplittingak-dimensionalproblemintok one-dimensional
problems. I.e., for each of the variables with missing data, a regression model with a
univariateoutcomeconditionalontheotherk−1variablesisconstructed.Furthermore,
itiseasiertoavoidlogicalinconsistenciesintheimputeddataandincorporatemethods
to preserve unique features in the data (van Buuren, Brand, Groothuis-Oudshoorn, &
Rubin,2006),e.g.,temporaldependencycanbetakenintoaccountinlongitudinaldata.
Multiple imputation by chained equations (MICE), often used synonymously to FCS,
is a specific implementation of the broader FCS framework. MICE uses the conditional
imputation approach from FCS and chains the variables by imputing iteratively one
variable at a time based on the others (van Buuren & Groothuis-Oudshoorn, 2011).
3. Simulation study design
Theuseoftree-basedmethodsgrowsinempiricalresearch,especiallywithmissRanger
(Levi, Wolf, Sommer, & Howe, 2023; Sajeev, Champion, Maeder, & Gordon, 2022;
Schwerter,Bleher,Doebler,&McElvany,2023).However,itisstillunclearhowreliable
the statistical inference is for data that has been imputed with tree-based methods.
While a recent simulation study by Schwerter, Gurtskaia, Romero, Zeyer-Gliozzo, and
Pauly (2023) investigated the performance of tree-based methods in longitudinal data,
wefocusontheirperformanceonhierarchicaldata,especiallywhenthereismissingness
at a higher level. How trustworthy are tree-based imputation methods for multilevel
data in terms of the reliability of the statistical inference? In particular, we aim to
find the strengths and limitations of tree-based imputation methods concerning type I
error rates, statistical power, and coefficient bias, compared to the widely-used MICE
approach, for simulated multilevel data with data missing at random, completely at
random and not at random.
3.1. Multiple imputation methods
Overview of imputation methods. In this simulation study, we implemented three main
imputationmethods:Multipleimputationbychainedequations(MICE)asourbaseline
4since it has been shown in the literature to be superior to other imputation methods
(see Enders et al., 2016; Grund et al., 2018a, 2018b). Additionally, we include more re-
cent tree-based imputation methods: a fast implementation of random forests from the
missRangerpackage(Mayer,2021),andmultipleimputationbyXGBoostimplemented
in the mixgb package (Deng, 2023). For MICE the imputation methods and the predic-
tion matrix for each variable are carefully adjusted to take into account the multilevel
data structure. For Level 1 variables the Level 1 normal model (2l.norm) is used and
for Level 2 variables the Level 2 class predictive mean matching (2lonly.pmm). Two
factors were varied for missRanger: the number of predictive mean matching donors
(3 or 5)1, and the variant (standard or an adapted implementation of missRanger).
The adapted variant respects the multilevel structure of the data by including dummy
variables for the respective cluster, while the standard implementation does not. Sim-
ilarly, mixgb was adapted with additional dummy variables. We used 5 imputations2
Details of each method follow.
3.1.1. Multiple imputation by chained equations
Multiple Imputation by Chained Equations (MICE) is a flexible and efficient impu-
tation method that can treat missingness in a wide range of data types and analysis
models.Itsapplicationscanbefoundindiverseresearchfields,includingmedicine,epi-
demiology, psychology, management, politics, and sociology (van Buuren & Groothuis-
Oudshoorn, 2011). MICE derives imputations for each missing value iteratively from
the other observed variables in the dataset.
TheproceduredescribedbyvanBuurenetal.(2006)andvanBuurenandGroothuis-
Oudshoorn (2011) is the following: Suppose Y = (Y ,...,Y ) is a random sample
1 k
from the k-variate multivariate distribution P(Y|θ), which is assumed to be com-
pletely specified by a vector of unknown parameters θ (van Buuren & Groothuis-
Oudshoorn, 2011, p. 6). The algorithm obtains the posterior distribution of θ by
sampling iteratively from conditional distributions of the form: P(Y |Y ,θ ), where
ℓ −ℓ ℓ
Y = (Y ,...,Y ,Y ,...,Y ). The parameters are specific to the respective condi-
−ℓ 1 ℓ−1 ℓ+1 k
tional densities. The t-th iteration of the algorithm contains the following successive
1ThedefaultvalueformissRangeris3,andthedefaultvalueinMICEis5.NotusingPMMformissRanger
showedinflatedtypeIerrorsinanothercontext(Ramosaj,Amro,&Pauly,2020).
2To choose a number of imputations a mini-simulation on randomly selected design was run with 5 and 20
imputations with MICE and missRanger (standard and adjusted with three donors). The results suggested
verylittletonodifferencesbetweenthenumberofimputationsintermsofCoefficientestimationbiasandH0
rejection rates, but imputing 20 times takes approximately four times longer than performing 5 imputations
(18.2145,1.9713,1.9248secondswith5imputationsand72.7969,8.1181,7.8682secondswith20imputations
forMICE,missRangerandadjustedmissRangerrespectively).Consequently,thewholesimulationisrunwith
5imputationsforeachmethod,makingthesimulationconsiderablyfaster.
5draws of the Gibbs sampler3:
θ∗(t)
∼ P(θ
|Yobs,Y(t−1) ,...,Y(t−1)
)
1 1 1 2 k
Y∗(t)
∼ P(Y
|Yobs,Y(t−1) ,...,Y(t−1) ,θ∗(t)
)
1 1 1 2 k 1
.
.
.
θ∗(t)
∼ P(θ
|Yobs,Y(t) ,...,Y(t)
)
k k k 1 k−1
Y∗(t)
∼ P(Y
|Yobs,Y(t) ,...,Y(t) ,θ∗(t)
)
k k k 1 k k
The MICE procedure for multilevel structured data iterates between the available
variables as output variables. For a random intercept model (M1) with k variables with
missing data in all variables, a proper MI by chained equations imputes on the basis
of the following univariate models:
Y1 = β +β Y2 +β Y3 +...+β Yk +δ +ε
ij 0(Y1) 1(Y1) ij 2(Y1) ij k−1(Y1) ij 0j(Y1) ij(Y1)
.
.
.
Yk = β +β Y1 +β Y2 +...+β Yk−1+δ +ε
ij 0(Yk) 1(Yk) ij 2(Yk) ij k−1(Yk) ij 0j(Yk) ij(Yk)
Where Yℓ indicates the ℓ-th variable, for i = 1,....,n observations in j = 1,...,J clus-
ij ℓ
ters. Imputing at Level 2 requires additional considerations. One possibility is to have
a separate model for Level 2 variables, for example, a regression model that includes
other Level 2 variables as well as cluster-level components (e.g, mean, median) of the
Level 1 variables. As described by Grund et al. (2018a), imputations at Level 2 may
be generated from L ∼ P(L |L˜ ,L ,θ ). Where L are cluster j values for
2jk 2jk 1j 2j(−k) k 2jk
variable k at Level 2, L˜ can be means of variables within each group at Level 1 and
1j
θ is the parameter of the model.
k
Asmentioned,miceisanRpackageforchainedequationsimputationsusingafunction
of the same name. To impute Level 1 missing values 2l.norm4 is used, which uses uni-
variate missing data imputation with a two-level normal model. For Level 2 variables,
a level-2 class predictive mean matching (2lonly.pmm) is applied. Although Enders et
al. (2016) used 2lonely.norm, we use 2lonly.pmm because 2lonely.norm follows the
normality assumption, which does not hold in our case because we transformed some
variablestoaggregatethematLevel2.2lonly.pmmisasemi-parametricmethod,which
is why it works in a wider range of cases.
3.1.2. Chained random forest imputation
Random forests (RF) build and combine multiple decision trees to improve upon a
single decision tree’s stability and precision. Each tree uses a random subset sampled
from the data independently and with the same distribution for all trees in the forest,
increasinggeneralizabilityandreducingoverfitting(Breiman,2001).Afteralargenum-
ber of trees are generated, the final prediction is chosen, either with a majority vote
3GibbssamplerisaMarkovChainMonteCarlo(MCMC)algorithmusedfordrawingsamplesfromamulti-
variatedistribution.Thealgorithmiterativelydrawsavalueforonevariablefromitsconditionaldistribution,
giventhevaluesofallothervariablesinthemodel(Schafer,2000,pp.69-70)
42l indicates the data structure which is multi-level, it does not mean the calculation happens at a higher
level,butitconsidersthattherearevariablesatanotherlevel
6(forcategoricalvariables)orastheaverage(forcontinuousvariables)ofthepredictions
of the individual trees. Since RF can handle mixed types of data, is capable to address
interactions and nonlinearity, and does not overfit (because of the Law of Large Num-
bers, Breiman, 2001), RF is an attractive tool for imputing missingness (Stekhoven &
Bühlmann,2012;Tang&Ishwaran,2017).missForestisanon-parametricMImethod
introduced by Stekhoven and Bühlmann (2012), which converts the problem of miss-
ingness into a prediction problem. In a first step, starting values for all missing values
are generated. Iteratively, a random forest for each variable to be imputed is fitted,
based on current predictions of missing values using currently fitted random forest for
the other variables (Tang & Ishwaran, 2017). This means that for k variables, k forests
are to be fitted in each iteration, which could be challenging and is slow for certain
multivariate problems.
The package ranger, a memory-efficient, parallelized and hence fast implementation
of random forests for high dimensional data (Wright & Ziegler, 2017) can process large
data sets. For splitting, the feature values are either sorted beforehand and accessed
by their index, or the raw values are retrieved and sorted while splitting (Wright &
Ziegler, 2017). In accordance with Breiman (2003)’s design of how RF works, the
algorithm randomly selects a subset of data (of the same size) with replacement, i.e., a
bootstrap sample. K, the number of variables to be selected is specified, and for each
bootstrappedsampleK variablesarerandomlychosen(therefore,thetreesarerandom
and diverse).
Mayer (2021) used the ranger implementation of RF to develop a fast missing value
imputationmethodbychainedrandomforestsbasedonmissForest.missRangerallows
for fast multivariate imputation. Additionally and contrary to missForest, it also
offers the option of predictive mean matching (Mayer, 2021). The basic steps for MI
with missRanger are (i) splitting the data into complete and missing parts. (ii) For
each variable with missing values, RF is fitted based on other variables, and (iii) the
fit is used to predict missing values. Then, (iv) a set of reasonable imputed values is
created based on the trained RF models (Breiman, 2003; Mayer, 2021; Stekhoven &
Bühlmann, 2012). Optionally for PMM, (v) for each predicted value in the test set,
one of the closest k values in the trained set is randomly chosen and its observed value
is returned. The number of donors k is an adjustable tuning parameter. For multiple
imputations, this process is repeated multiple times (5 times in our simulation). In the
decision tree construction, the Extremely randomized trees (ExtraTrees) algorithm is
used for splitting, creating diverse set of trees (Geurts, Ernst, & Wehenkel, 2006). The
procedure takes a random subset of the variables that will be considered at each split
(different for each tree). For each variable, several possible thresholds are randomly
generated. The candidates are then evaluated and the threshold that ensues the best
split is chosen. The node is split on the threshold and the procedure is continued
recursively until the criteria for stopping the algorithm are met.
3.1.3. MI through extreme gradient boosting (XGBoost)
Extreme Gradient Boosting (XGBoost) is a machine learning algorithm that belongs
to the family of gradient boosting methods (Chen & Guestrin, 2016). Instead of simply
taking the mean of multiple decision trees, XGBoost uses gradient boosting to com-
bine multiple regression trees. It also employs regularization and shrinkage techniques.
Typically,thedepthoftheregressiontreesusedinsequentialboostingisnotverydeep.
One advantage of XGBoost over Random Forest is that XGBoost selects only one of a
set of highly correlated features, because in sequential boosting, features are added to
7the model incrementally. If a feature is selected early, other highly correlated features
can only improve the prediction if they provide new information.
XGBoost uses gradient descent optimization techniques to iteratively minimize the
loss function, making it highly efficient and effective in finding the optimal solution.
L1 and L2 regularization prevent overfitting and improve generalization. Tree pruning
removes unnecessary branches and reduces model complexity, further improving XG-
Boost’s predictive performance. Lastly, XGBoost can take advantage of parallel pro-
cessing capabilities, making it suitable for large datasets and reducing training time.
Overall, XGBoost is known for its ability to handle complex datasets, handle missing
values, and provide accurate predictions. Its popularity is evident in various domains,
including Kaggle competitions and real-world applications (Chen & Guestrin, 2016).
The mixgb R package uses XGBoost to implement missing value imputation in a
scalable and efficient manner (Deng & Lumley, online first). It addresses the challenge
ofmissingdatainlargedatasetswithcomplexstructures.mixgbimputesmissingvalues
in the order of variables with fewer missing values. This strategic approach aims to
prioritize variables with more available information during the imputation process.
Initial values for imputation are filled with random values drawn from the observed
data. This helps kickstart the imputation process (Suh & Song, 2023). mixgb provides
a versatile approach to missing data imputation, leveraging XGBoost, bootstrapping,
andPMMtoenhanceimputationaccuracy,especiallyforcontinuousdata.Unlikesome
otherimputationframeworks,thedefaultimputationinmixgbisnon-iterative,butthe
package allows users to set the number of iterations for imputation (Deng & Lumley,
online first), for this simulation we used 5 iterations.
3.1.4. Adapting tree-based imputation methods with cluster membership dummy
variable
By default, both, missRanger and mixgb, are unaware of the multilevel data structure.
We study variants of the two procedures that add dummy variables for cluster mem-
bership (Lüdtke et al., 2017). If there are J clusters, J dummy variables are added,
and the jth variable is equal to 1 if the data in row i is from cluster j and 0 otherwise.
For the tree methods, in contrast to the regression methods as in Lüdtke et al. (2017),
no reference group is necessary. Adding the dummy variables is possible in all standard
software packages. Potentially, cluster-level effects can now influence the imputations,
since the models can generate cluster-specific predictions.
If regression trees have high enough tree depth, the flexibility gained by the dummy
variables is potentially higher compared to a procedure that would include some kind
of random intercepts, since some interactions with the dummy variables are possible,
though not all kinds of interactions (Wright, Ziegler, & König, 2016). Potentially,
more dummy variables are added than there are variables originally, so the dummy
variableprocedurereliesonimputationmethodsthathavesomeformofregularization.
Tree-based ensemble methods like RF and XGBoost do have this property by design
(Breiman, 2001; Chen & Guestrin, 2016).
3.2. Simulated data
For the simulation setup, four general factors are varied, resulting in a total of 16
simulation designs. The varying factors are: number of clusters, data generation
model, missing rate, and missing mechanism.Therearetwonumbersofclusters:
25 and 50. This results in two cluster sizes of 40 and 20 for a balanced design with a
8sample size of N = 1000. This variation in the number of clusters allows us to examine
the effect of cluster size on the performance of the imputation methods. As for the
data generation process, we first simulate six covariates using the fungible package in
R (Waller, 2022). The monte function generates clustered data with predefined char-
acteristics. For this simulation study, intra-cluster correlations and indicator validities
(cluster separations for each variable) are randomly constructed. Four variables are
Level 1 (individual level) variables and two variables are aggregated to Level 2. All
covariates are continuous numerical variables. Finally, two data generation models are
considered for the outcome variable: random intercept and random intercept and ran-
dom slope (hereafter denoted as random slope model). Based on the model, the output
variable is entered with
Yo = 0.3
ij
+δ +0.5×Y +1×Y +0×Y +1.3×Y
0j ij1 ij2 ij3 ij4
+1.4×L +0.7×L +ϵ
ij1 ij2 ij
for random intercept model and
Yo = 0.3
ij
+δ +(0.5+δ )×Y +(1+δ )×Y +0×Y +1.3×Y
0j 1j ij1 2j ij2 ij3 ij4
+1.4×L +0.7×L +ϵ
ij1 ij2 ij
for random slope model, where δ ,δ ,δ ,ϵ ∼ N(0,1) are randomly generated.
0j 1j 2j ij
3.2.1. Missingness mechanism
Missingness in the generated data is induced at two levels: moderately low 10% and
relatively high 50%. This range of missingness levels reflects real-world scenarios. Two
missingness mechanisms (MAR, MCAR) are considered. The introduction of missing
data is based on the selected missingness mechanism and the specified missing rate.
For MCAR, a simple function is implemented in R that randomly sets each data point
to NA with probability equal to the missingness level for each variable. To introduce
missingness according to the MAR mechanism, the algorithm described by Thurow,
Dumpert, Ramosaj, and Pauly (2021) is used, with slight adjustments for purely nu-
merical data. The algorithm operates as follows: starting with one variable it generates
missing values under MCAR with the overall missing rate. The missingness in all other
variables depends only on the remaining (observed) data in the first variable to exhibit
MAR missingness. After introducing missingness in the first variable, it is converted to
a categorical variable by grouping values into intervals. For each subsequent variable
i.i.d. uniformly (0,1) distributed random numbers are generated, one for each unique
value of the first variable, and treated as probabilities. The probabilities are assigned
to the values of the selected variable. Then the probability of obtaining a missing value
in that variable is calculated according to the assigned probability and the absolute
frequenciesofthedistinctvaluesintheconvertedvariable.Lastly,indicesarerandomly
selected based on the computed probabilities and the corresponding values are set to
missing for each variable. Thurow et al. (2021) argue that this leads to the desired
overall missingness rate.
9The resulting simulation has 2×2×2×2 = 16 conditions. Each combination of the
above factors is replicated 1,000 times.
3.3. Evaluation using linear mixed models
Onewaytoanalyzemultilevel(clusteredobservations,repeatedmeasures,longitudinal,
multivariate) data without ignoring existing correlations is to use linear mixed effects
models (LMM, Grund et al., 2024). A major advantage of LMM is that it allows
the estimation of individual or group specific effects while taking into account the
nested structure or the correlation caused by the multilevel structure. Ignoring the
multileveldatastructurecanleadtopotentiallyspuriousstandarderrorsandexcessive
type I error rates (Grund et al., 2024; Steenbergen & Jones, 2002). Because a critical
assumptionforclassicallinearmodelsistheindependenceoftheobservationscollected.
This is not the case for multilevel hierarchical structures, where Level 1 observations
arenotindependent;forexample,studentsfromthesameschoolmayhavesimilarities,
even more so within a given class, so classical linear models may not be appropriate
for analyzing such data. However, they are a good starting point and can be extended
to more general models for multilevel data that allow relaxing the assumptions of
independence and variance homogeneity. In particular, LMM allows the inclusion of
the correlation of observations contained in a data set (Gałecki & Burzykowski, 2013).
Mixedeffectsmodels,asthenameimplies,includebothfixed andrandom effects.Fixed
effects are constant parameters that do not vary between clusters. Random effects, on
theotherhand,canbeconsideredasrandomvariablesandarenotobservable(Pinheiro
& Bates, 2004).
For a single level of grouping, the classical LMM, at a given level i of the grouping
factor is defined by (Fahrmeir, Kneib, Lang, & Marx, 2013; Gałecki & Burzykowski,
2013; Pinheiro & Bates, 2004) as follows:
y = X β+Z b +ε (1)
i i i i i
where n is the ith group size, y is the n -dimensional response vector, X is the n ×p
i i i i i
fixed effects design matrix, β is the p-dimensional unknown parameter of the fixed
effects, ε is a vector of residual errors for group i (within-group error), and Z and b
i i i
are the random effects regressor matrix and the corresponding random effects vector.
The q covariates of the matrix Z are known, and their corresponding effects b are
i i
unobservable. Both b and ε follow a multivariate normal distribution b ∼ N (0,D),
i i i q
ε ∼ N (0,R ). Furthermore, the residual errors are independent of the random
i n i
i
effects. It is also assumed that for different groups (i ̸= i′) b is independent of ε .
i i′
For an unknown scale parameter σ2, the positive definite matrices D and R can be
i
defined as follows D = D(θ ) for the variance-covariance matrix of the random effects
D
b , with a vector of parameters θ . There are no other restrictions on D (Gałecki &
i D
Burzykowski, 2013). Finally, R = σ2R , which is not identifiable in its general form.
i i
For i.i.d. errors, R simplifies to I (Fahrmeir et al., 2013).
i n
i
LMM, as described above, implies that marginally
y ∼ N (X β,Z D(θ )Z′+σ2I ) (2)
i n i i i D i n i
Equation 1 can be specified for all data in a compact by defining vectors (Fahrmeir
et al., 2013; Gałecki & Burzykowski, 2013): Let Y = (cid:0) y′ ··· y′ (cid:1) , b = (cid:0) b′ ··· b′ (cid:1)
1 m 1 m
10and ε = (cid:0) ε′ ··· ε′ (cid:1) for all groups i = 1,...,m, and set the design matrices to
1 m
 
 X  Z 1 0 ··· 0
.1  0 Z 2 ··· 0 
X =   . .   and Z =  

. .
.
. .
.
... . .
.
 

X
m 0 0 ··· Z
m
Then
(cid:18) (cid:19) (cid:18)(cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
b 0 D(θ ) 0
Y = Xβ+Zb+ε with ∼ N , D (3)
ε 0 0 σ2I
Dependingontheresearchquestion,Equation3canalsobedefinedmorespecifically.
If the slope is the same for all Level 1 individuals (e.g., students, patients) in all groups
(constant slope β ), but the intercept (β ) is group specific and contains random
1 0j
effects, then the resulting model will be a random intercept model. On the other
hand,iftheinterceptisconstantforallgroups(β ),buttherearegroup-specificslopes
0
(β ), then it is a random slope model. If both the intercept and slope are random
1j
and vary between groups (β and β for group j), then it is a random intercept
0j 1j
and random slope model.
4. Simulation Study Results
In this simulation study, various imputation methods were evaluated and compared
to address missing data in multilevel designs. The primary objective was to assess
the performance of these methods in terms of decision making, accuracy and overall
robustness across different settings. The simulated data was designed to mimic real-
worldscenarioswheremultilevelstructuresarequitecommonwithmissingnesspatterns
that are commonly encountered in diverse research settings.
4.1. Random intercept results
4.1.1. Rejection rates
Figure 1 displays the average H (β = 0) rejection rates over replications for ran-
0
dom intercept models by missingness mechanism. Notably, as Figure 1a shows, among
the different imputation methods evaluated, only MICE consistently achieved a type
I error rate below 5% in the presence of MCAR missingness when the true under-
lying coefficient was indeed zero, missRanger only for 10% missingness. mixgb had
the highest type I errors for 10% missingness but was consistently closest to the com-
plete data results. For 50% missingness, dummy-adjusted mixgb and even standard
mixgb performed at least as well (for 50 clusters) or better (for 25 clusters) than MICE
for non-zero Level 1 coefficients. For Level 2 coefficients, standard mixgb outperforms
dummy-adjusted mixgb and is only beat by MICE. All methods, except the standard
implementations of missRanger, perform better for 50 clusters than for 25.
The results were similar for MAR missingness with 10% missing data (Figure 1b).
For the zero coefficients again only MICE was consistently below 5% rejection rate,
missRanger only for 10% missingness and mixgb was again close to the generated data
11results. For 10% missingness, missRanger had the desired 5% rejection rate, but its
performance deteriorated at 50%. For all non-zero (β ̸= 0) Level 1 coefficients except
the intercept and 10% missingness, all methods had high rejection rates, but power
decreased with increasing missingness. The ranger-based methods had no power for
testing the non-zero intercept, but MICE and mixgb had some power, albeit non at
50%missingnessrateand25clusters.Dummy-adjustedvariantsofimputationmethods
alwaysimprovedthestatisticalpoweroverstandardLevel1implementations,especially
for mixgb, which performed almost as well and sometimes even better than MICE.
4.1.2. Coefficient bias
Figure2presentsthecoefficientbiasofeachmethodforrandominterceptmodelsunder
MCAR and MAR. Under MCAR missingness (Figure 2a), both mixgb methods have
thelowestestimationbiasforLevel1variableswith10%missingness.Forthetruezero
coefficient (X3) all the methods have low bias. For Level 2 variables, all missRanger
implementations have high coefficient estimation bias which aligns with the worst test
decisions (least power). As the missingness rate increases, so does the bias, especially
for MICE. At 50% missingness, MICE has the highest estimation errors for one of the
Level 1 variables, except for the true zero coefficient, where MICE still has the lowest
bias.
The situation is a bit different with MAR mechanism (Figure 2b). For 10% miss-
ingness, MICE has the highest bias for Level 1 and one of the Level 2 variables and
mixgb has the lowest. With increasing missingness, standard missRanger sometimes
evenreducesbias(variablesX1,X4)toalmostzeroorincreasesthebiasthemost(X2).
Among the imputation methods (for MAR and MCAR), the two standardmissRanger
variants are the least affected by the increasing missingness rate, however, especially
the bias for the Level 2 variables and some Level 1 variables is substantial.
4.2. Random slope results
4.2.1. Rejections rates
Forrandominterceptandrandomslopemodels,Figure3displaystheaveragerejection
rates over replications grouped by the missingness mechanism. Under MCAR (Figure
3a), both missRanger variants and MICE have the desired type I error (5%) for the
true zero variable with 10% missingness. Both mixgb implementations are closer to the
rejection rate of the simulated data, which is around 10%. For X1, one the coefficients
with random slopes, none of the methods reach the rejection rate of the complete
data, although all but the unadjusted missRanger-methods are close. All methods
have higher rejection rates with 50 clusters than with 25 clusters. Dummy-adjusted
missRanger is better than standard missRanger, especially for 50 clusters. For X2,
also a variable with a random slope, all methods have a rejection rate of 100%. For
Level 2 variables, the missRanger variants have the lowest power and MICE has the
highest. mixgb has less power than MICE but much more power than missRanger.
As the missingness rate increases, all methods perform considerably worse. For the
true zero coefficient, only MICE retains a type I error below 5%. Standard missRanger
and dummy-adjusted mixgb lose more power for 25 clusters than for 50. For X1 all
the rejection rates drop below 50%, especially MICE loses the most power. Dummy-
adjusted mixgb has the highest rejection rate. Standard missRanger loses a lot of
powerwith increasingmissingnessforX2 (rejectionratebelow50%). Dummy-adjusted
12Figure 1.: Rejection rates for random intercept model
(a) MCAR
(b) MAR
Note:VariableX3hasatrue-zerocoefficient.
Rejectionratesforrandominterceptdesignswith10%(firstrows)and50%(secondrows)missingnessand25
(firstcolumns)and50(secondcolumns)clusters
13Figure 2.: Coefficient Bias for random intercept model
(a) MCAR
(b) MAR
Note:VariableX3hasatrue-zerocoefficient.
Coefficientestimationbiasforrandominterceptdesignswith10%(firstrows)and50%(secondrows)missing-
nessand25(firstcolumns)and50(secondcolumns)clusters
14missRangershowsbetterperformancethanMICEwith25clusterandMICEhashigher
rejection rate with 50 clusters. Dummy-adjusted mixgb has the highest power in both
cases. For Level 2, the performance of missRanger improves, especially the standard
implementations. The standard mixgb performs better than the adjusted one and even
increases the power with 50 clusters. MICE still has the highest rejection rates.
The methods perform similarly for 10% under MAR (Figure 3b) as well. With in-
creasing missingness rate, all methods lose power. MICE remains as the only method
with a type I error under 5% for the true zero coefficient, whereas the standard im-
plementations of missRanger has the highest rejection rate especially with 50 clusters.
For X2 MICE loses the most power and is too conservative. Dummy-adjusted mixgb
performs best among the imputation methods for coefficients with random slopes, es-
pecially with 50 clusters. For X4 mixgb and MICE remain over 95% rejection rate. For
Level 2 variables, MICE once again has the highest rejection rate, followed by mixgb
and dummy-adjusted mixgb. While the standard implementations of missRanger still
have low power, the rejection rates for dummy-adjusted missRanger even increases for
a higher rate of missingness, especially for 50 clusters, where the power almost doubles
with 50% missing rate.
4.2.2. Coefficient bias
UnderMCAR(Figure4a)and10%missingness,bothmixgbapproacheshavethelowest
bias for Level 1 variables (β ̸= 0). MICE has the highest bias with 25 clusters. For
Level 2 variables missRanger has considerably higher bias than other methods while
MICEorstandardmixgbhavethelowest.Forthetruezerocoefficientbothmixgbhave
slightly higher bias than the others.
The bias increases with increasing missingness. At Level 1, the bias for MICE
increases the most and becomes the highest for both 25 and 50 clusters. Stan-
dard missRanger remains more stable and has lower bias rate than dummy-adjusted
missRanger and standard mixgb has the lowest bias for 50 clusters. At Level 2,
missRanger increases its bias the least. MICE remains stable for one coefficient but
the error rate more than triples for the other. For these coefficients, dummy-adjusted
mixgb has the lowest bias. For the true zero coefficient, both mixgb once again have
the highest bias.
Figure 4b for MAR mechanism looks somewhat similar. With 10% missingness,
mixgb has the lowest bias at Level 1 (β ̸= 0) and MICE has the highest. At Level 2,
missRanger has the highest bias and either MICE or standard mixgb has the lowest.
The positions change slightly at 50% missingness, missRanger has either the highest
(X2) or lowest (X1 and X4) bias, while MICE has the highest for X1 and X4. At Level
2 the situation is similar to MCAR, except that missRanger actually reduces the bias
with increasing missingness, especially dummy-adjusted variations.
5. Discussion
This paper critically evaluates the performance of novel tree-based imputation meth-
ods for handling missing data in hierarchical data structures for Level 1 and Level 2
variables with two different data generation processes (random intercept and random
slope),twomissingnessrates(10%and50%),andtwomissingnessmechanisms(MCAR
and MAR). Through a comprehensive simulation study, we contrast these novel tech-
niques with the more conventional multiple imputation MICE, focusing in particular
15Figure 3.: Rejection rates for random slope models
(a) MCAR
(b) MAR
Note:RejectionRatesforrandomslopedesignswith10%(firstrows)and50%(secondrows)missingnessand
25(firstcolumns)and50(secondcolumns)clusters
16Figure 4.: Coefficient Bias for random slope models
(a) MCAR
(b) MAR
Note: Coefficient estimation bias for random slope designs with 10% (first rows) and 50% (second rows)
missingnessand25(firstcolumns)and50(secondcolumns)clusters
17on bias and inference. Our results indicate that MICE is characterized by consistent
accuracy in rejection rates when using the 2l.norm method for Level 1 variables and
2lonly.pmm for Level 2 variables. This consistency underscores MICE’s robustness to
hierarchical data and closely approximates true rejection rates (i.e., type I error and
power). Our results are consistent with the findings in Grund et al. (2018a), demon-
strating the practical usefulness of MICE while dealing with missingness in multilevel
structures, especially in the case of missingness at Level 2. Similar to Enders et al.
(2016), we also found that MICE was superior, not just for random slope but for ran-
dom intercept models as well, when the imputation model was specified according to
data generation process.
Inaddition,MICEconsistentlyoutperformsothermethodsacrossmissingnessmech-
anisms and rates in cases involving true zero Level 1 coefficients. This aspect under-
scores MICE’s reliability in accurately identifying non-significant variables and rein-
forces its position as a versatile tool in statistical analysis. As in multi-level multiple
imputation dependencies have to be taken into account (Audigier et al., 2018), tree-
based method might miss this compared to our MICE framework. For example, for
the random intercept model, in scenarios with 10% missingness on Level 2 variables,
both mixgb variants and MICE have both good type I error and power over differ-
ent settings (25 vs. 50 clusters and random intercept vs. random slope models), but
at 50% missingness, the rejection rates worsen for mixgb, while they remain mostly
robust for MICE. Such variability in performance based on missingness rate and impu-
tation method indicates the context-dependent effectiveness of each method. However,
tree-based methods, especially Mixed Gradient Boosting (mixgb), exhibit lower biases,
suggesting their potential in scenarios where bias reduction is a priority. This finding
is particularly noteworthy as it points to the evolving capabilities of non-traditional
methods in dealing with missing data.
We included the 25 and 50 cluster scenarios because when data are clustered, for
asymptotically valid standard errors, not only the number of observations but also the
numberofclustersmustgotoinfinity(i.e.,begreaterthan30,Cameron&Miller,2015).
Our results confirm that with 50 clusters, the differences between 10% missingness and
50% missingness are smaller than with 25 clusters, where the differences between 10%
missingness and 50% missingness get worse. Thus, having more clusters helps to get
more reliable rejection rates. More specifically, for the random intercept model, fewer
clusters generally showed lower performance, especially notable for mixgb and MICE,
while missRanger showed minimal difference. The trend in type I error rates under the
random intercept model did not show a clear pattern. However, in the random slope
model, power tended to be lower with fewer clusters, and there were slightly higher
type I errors in scenarios with fewer clusters. In terms of bias, the random intercept
model with 10% missing data and 25 clusters in the MCAR condition showed the least
bias. Beyond this specific case, the difference in bias between the two cluster sizes
remained marginal. A similar trend was observed for the random slope model, where
the variation in bias between different cluster sizes was negligible, indicating a relative
consistency in bias across cluster sizes.
The results do not necessarily show that adjusted tree-based methods (which also
include dummies for the Level 2 cluster) outperform standard tree-based methods. For
example, for MCAR and MAR, standard mixgb had higher power than adjusted mixgb
for Level 2 coefficients with 50% missingness for both random intercept and random
slope, while adjusted mixgb sometimes had higher power for Level 1 coefficients when
not similar. The differences for missRanger were generally smaller than for mixgb.
Thus, whether to use standard or adjusted tree-based imputation methods depends on
18whether the main variable of interest is at Level 1 or Level 2. For example, adjusted
mixgb performed very similarly for Level 1 coefficients with 10% and 50% missingness.
In our simulation example, none of the missRanger variants outperformed MICE or
mixgb, although it was reliable for Level 1 coefficients. This is in contrast to (Schw-
erter, Gurtskaia, et al., 2023), a simulation with longitudinal data without hierarchical
structure, where MICE missRanger (and Random Forest, not tested here) performed
better than mixgb (and MICE PMM, not tested here).
The general picture is very similar for the random slope data generation process.
E.g., at Level 2, both, standard missRanger PMM = 3 and PMM = 5, have higher
rejectionratesforMCARwithhighmissingness.ForrandomslopesmodelunderMAR
adjusted missRanger, the power for Level 2 variables with high missingness improves,
especially with 50 clusters the power almost doubles. Although MICE generally has
the best rejection rates, it shifts for the random slope model when looking at Level
1 coefficients with 50% missingness. In this particular case, mixgb outperforms MICE
in most cases, as mixgb is less affected by the increase in missingness. In addition,
MICE suffers more from a few clusters moving from low to high missingness than for
50 clusters. Finally, missRanger performs better under MAR than MCAR for Level 2
coefficients.
It is unclear how the differences in rejection rates relate to the differences in bias.
In general, MICE has better rejection rates and mixgb has lower bias, which would
disqualifythehypothesisthatthehigherthebias,theworsetherejectionrate.However,
within models, moving from low to high missingness, such a pattern can be seen: The
higher bias under MCAR for high missingness is associated with a worse rejection
rate Level 1 coefficients for MICE. Similarly, under MAR, all missRanger variants,
especially standard missRanger become more biased for high missingness, and power
decreases and type I error increases.
Based on these mixed results, recommendations for which method to use are very
case-specific. If researchers can be confident that the true data generation process fol-
lowsarandominterceptorrandomslopemodel,MICEislikelytoprovidemorereliable
rejection rates than tree-based methods, especially with a high number of clusters. In
cases where rejection rates are not important, but bias is, mixgb should be used to im-
pute missing data. If MICE cannot be used (due to too many variables, uncertain data
generation model, high collinearity between variables, too time-consuming calculations
or similar), adjusted tree-based methods that include dummies for the clusters should
be used if Level I coefficients are of interest, while standard tree-based methods should
be used for Level 2 coefficients. In most cases, mixgb seems to outperform missRanger
and should therefore be the first imputation alternative for multilevel data. While the
differences between (standard and adjusted) missRanger PMM = 3 and PMM = 5 are
small, in cases where there are differences, PMM = 5 outperforms PMM = 3. Thus,
when using missRanger, it is advisable to increase the number of donors to 5.
There is no “one size fits all” solution: For the data generation process following
both random intercept and random slope models, there was one variable (X2) where
allmethodsbecomesignificantlyworsewithhighermissingnessunderMAR.Especially
MICE and standard missRanger with PMM = 3.
Ofalltheimputationmethodsevaluated,MICEprovedtobetheslowest(seeFigures
A1 and A2 in the appendix). In general, MICE needs 8-10 times longer for imputation
thanmissRanger.Therefore,theefficiencyofMICEshouldbeconsideredinlightofits
computational requirements. Furthermore, while including the cluster dummies only
increased the time for missRanger with PMM = 5 (up to the same level as mixgb),
adjusting mixgb with dummy variables made it significantly slower, almost always
19doubling the total imputation time.
5.1. Limitations, strengths and outlook
Although the study is comprehensive, it is not without limitations. The simplicity of
the data structure used in our simulations, characterized by a low number of variables,
contrasts with the often complex and variable-rich data sets encountered in real-world
scenarios.Asaresult,thegeneralizabilityofourresultstomorecomplicateddatasetsis
uncertain.TherathersimpledatasetcouldalsobeareasonwhyMICEhasmorereliable
rejection rates than the tree-based methods. One advantage of tree-based methods
is handling large numbers of variables, that cannot shine here in our relatively low-
dimensional data typical for applications of LMMs. In addition, among all imputation
methods, MICE is the one that best fits the true underlying data generation process,
but this process may not be known in empirical data. Future research investigating the
effectofincreasingthenumberofvariablesonbothrejectionrateandbiascouldprovide
a more complete understanding of the performance of the methods in more complex
data settings. Especially, simulating data beyond a linear (mixed) model could prove
interesting, e.g., a semi- or non-parametric data generating model that is nevertheless
plausible enough for an LMM-analysis.
Thestrengthofthecurrentstudy,however,liesinitsnovelty.Weintroduceandeval-
uate the combination of the latest tree-based imputation methods and a simple mul-
tilevel adjustment via dummy variables against established techniques such as MICE.
This approach provides new insights into the evolving landscape of data imputation
methods, especially in the context of hierarchical data structures.
Another interesting area to explore would be to improve the performance of tree-
based methods. This could involve experimenting with multivariate tree-based meth-
ods, like the multivariate random forest (Sega & Xiao, 2011). Tree-based methods
with random effects have been developed (Fokkema, Edbrooke-Childs, & Wolpert,
2020; Fokkema, Smits, Zeileis, Hothorn, & Kelderman, 2018; Hajjem, Bellavance, &
Larocque, 2014; Hajjem, Larocque, & Bellavance, 2017; Sela & Simonoff, 2012), but
still have to be adapted to missing data. Another, simpler approach could be to av-
erage all data at Level 2, followed by imputation and then integration back into the
full dataset. Such an approach could improve the effectiveness of tree-based methods
in dealing with hierarchical data.
In conclusion, our study underscores the continued effectiveness of MICE in dealing
with hierarchical data, particularly in terms of rejection rates. However, the emerging
tree-based methods, especially mixgb, show potential for bias reduction, suggesting
their usefulness as alternatives in certain contexts. This dual finding opens new av-
enues for future research and practical applications in data imputation, highlighting
the dynamic nature of the field.
Acknowledgement
The authors gratefully acknowledge the computing time provided on the Linux HPC
cluster at Technical University Dortmund (LiDO3), partially funded in the course of
the Large-Scale Equipment Initiative by the Deutsche Forschungsgemeinschaft (DFG,
German Research Foundation) as project 271512359.
20Funding
The project “From Prediction to Agile Interventions in the Social Sciences (FAIR)” is
receivingfundingfromtheprogramme“Profilbildung2020”,aninitiativeoftheMinistry
of Culture and Science of the State of Northrhine Westphalia. The sole responsibility
for the content of this publication lies with the authors.
Contributions
Authorcontributions:Conceptualization: JS,PD;Data curation: JS,KG;Formal
analysis: JS, KG; Funding acquisition: PD; Investigation: JS, KG; Methodol-
ogy: JS, KG, PD; Project administration: JS; Resources: ; Software: KG; Su-
pervision: PD; Validation: JS, KG; Visualization: JS, KG; Writing – original
draft: JS, KG; Writing – review & editing: JS, KG, PD
References
Aitkin, M., & Longford, N. (1986). Statistical modelling issues in school effectiveness
studies. Journal of the Royal Statistical Society: Series A (General), 149(1),
1-26. doi:https://doi.org/10.2307/2981882
Audigier, V., White, I. R., Jolani, S., Debray, T. P. A., Quartagno, M., Carpenter,
J., ... Resche-Rigon, M. (2018). Multiple Imputation for Multilevel Data
with Continuous and Binary Variables. Statistical Science, 33(2), 160-183.
doi:10.1214/18-STS646
Breiman, L. (2001). Random forest missing data algorithms. Machine Learning, 45,
5-32. doi:10.1023/A:1010933404324
Breiman,L. (2003). Manual: Setting up, using, and understanding random forests V4.0
(Tech. Rep.). Retrieved from https://www.stat.berkeley.edu/~breiman/
Using_random_forests_v4.0.pdf
Cameron, R. C., & Miller, D. L. (2015). A Practitioner’s Guide to Cluster-Robust
Inference. Journal of Human Resources,50,317–372. doi:10.3368/jhr.50.2.317
Chen, T., & Guestrin, C. (2016). XGBoost: A scalable tree boosting system. Pro-
ceedings of the ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining, 13-17-August, 785–794. doi:10.1145/2939672.2939785
Deng, Y. (2023). mixgb: Multiple imputation through ’xgboost’ [Computer software
manual]. (R package version 1.0.2) Retrieved from https://CRAN.R-project
.org/package=mixgb
Deng, Y., & Lumley, T. (online first). Multiple imputation through
XGBoost. Journal of Computational and Graphical Statistics, 19 p.
doi:10.1080/10618600.2023.2252501
Enders, C. K., Mistler, S. A., & Keller, B. T. (2016). Multilevel multiple imputation:
A review and evaluation of joint modeling and chained equations imputation.
Psychological Methods, 21(2), 222-240. doi:10.1037/met0000063
Fahrmeir, L., Kneib, T., Lang, S., & Marx, B. (2013). Regression: Models, methods
and applications. Berlin: Springer.
Fokkema, M., Edbrooke-Childs, J., & Wolpert, M. (2020). Generalized linear mixed-
model (GLMM) trees: A flexible decision-tree method for multilevel and longitu-
dinaldata. Psychotherapy Research,1–13. doi:10.1080/10503307.2020.1785037
21Fokkema, M., Smits, N., Zeileis, A., Hothorn, T., & Kelderman, H. (2018). De-
tecting treatment-subgroup interactions in clustered data with generalized lin-
ear mixed-effects model trees. Behavior Research Methods, 50(5), 2016–2034.
doi:10.3758/s13428-017-0971-x
Gałecki, A., & Burzykowski, T. (2013). Linear mixed-effects models using R: A step-
by-step approach. New York: Springer.
Geurts, P., Ernst, D., & Wehenkel, L. (2006). Extremely randomized trees. Machine
Learning, 63, 3-42. doi:10.1007/s10994-006-6226-1
Graham, J. W. (2012). Missing data. analysis and design. New York: Springer.
Grund, S., Lüdtke, O., & Robitzsch, A. (2024). Missing data in the analysis of mul-
tilevel and dependent data. In M. Stemmler, W. Wiedermann, & F. Huang
(Eds.), Dependent data in social sciences research: Forms, issues, and methods
of analysis (2nd ed.). Springer.
Grund, S., Lüdtke, O., & Robitzsch, A. (2018a). Multiple imputation of missing
data at level 2: A comparison of fully conditional and joint modeling in multi-
level designs. Journal of Educational and Behavioral Statistics, 43(3), 316-353.
doi:10.3102/1076998617738087
Grund, S., Lüdtke, O., & Robitzsch, A. (2018b). Multiple imputation of missing
data for multilevel models: Simulations and recommendations. Organizational
Research Methods, 21(1), 111-149. doi:10.1177/1094428117703686
Hajjem, A., Bellavance, F., & Larocque, D. (2014). Mixed-effects random forest for
clustered data. Journal of Statistical Computation and Simulation, 84(6), 1313–
1328. doi:10.1080/00949655.2012.741599
Hajjem, A., Larocque, D., & Bellavance, F. (2017). Generalized mixed ef-
fects regression trees. Statistics and Probability Letters, 126, 114–118.
doi:10.1016/j.spl.2017.02.033
Heymann, M. (2017). 1970s: Turn of an era in the history of science? Centaurus,
59(1-2), 1-9. doi:https://doi.org/10.1111/1600-0498.12146
Hox, J. (1998). Multilevel modeling: When and why. In I. Balderjahn, R. Mathar,
& M. Schader (Eds.), Classification, data analysis, and data highways (pp. 147–
154). Berlin: Springer.
Hox, J., & Roberts, J. K. (2010). Handbook of advanced multilevel analysis (First ed.).
New York: Routledge. doi:10.4324/9780203848852
Levi, S., Wolf, I., Sommer, S., & Howe, P. D. (2023). Local support of climate change
policies in Germany over time. Environmental Research Letters, 18(6), 1–19.
doi:10.1088/1748-9326/acd406
Little, R. J., & Rubin, D. B. (2002). Statistical analysis with missing data (Second
ed.). Hoboken, NJ: Wiley & Sons.
Lüdtke, O., Robitzsch, A., & Grund, S. (2017). Multiple imputation of missing data in
multilevel designs: A comparison of different strategies. Psychological Methods,
22(1), 141–165. doi:10.1037/met0000096
Mayer, M. (2021). missranger: Fast imputation of missing values [Computer software
manual]. (R package version 2.1.3) Retrieved from https://CRAN.R-project
.org/package=missRanger
Pinheiro, J. C., & Bates, D. M. (2004). Mixed-effects models in S and S-Plus. New
York: Springer.
Ramosaj, B., Amro, L., & Pauly, M. (2020). A cautionary tale on using imputation
methods for inference in matched-pairs design. Bioinformatics, 36(10), 3099-
3106. doi:10.1093/bioinformatics/btaa082
Rubin, D. B. (1976). Inference and missing data. Biometrika, 63(3), 581-592.
22doi:10.1093/biomet/63.3.581
Rubin, D. B. (1987). Multiple imputation for nonresponse in surveys (Second ed.).
USA: Wiley & Sons.
Sajeev, S., Champion, S., Maeder, A., & Gordon, S. (2022). Machine learning models
for identifying pre-frailty in community dwelling older adults. BMC Geriatrics,
22(1), 1–12. doi:10.1186/s12877-022-03475-9
Schafer, J. L. (2000). Analysis of incomplete multivariate data. Boca Raton: Chapman
& Hall/CRC.
Schafer, J. L., & Olsen, M. K. (1998). Multiple imputation for multivariate missing-
data problems: A data analyst’s perspective. Multivariate Behavioral Research,
33(4), 545-571. (PMID: 26753828) doi:10.1207/s15327906mbr3304\_5
Schwerter, J., Bleher, J., Doebler, P., & McElvany, N. (2023). Metropolitan, ur-
ban, and rural regions–how regional differences affect elementary school stu-
dents’ academic achievement, well-being, and motivation. Working paper.
doi:https://dx.doi.org/10.2139/ssrn.4368170
Schwerter, J., Gurtskaia, K., Romero, A., Zeyer-Gliozzo, B., & Pauly, M.
(2023). Evaluating tree-based imputation methods as an alternative to
MICE PMM for drawing inference in empirical studies. Working paper.
doi:https://doi.org/10.48550/arXiv.2401.09602
Sega, M., & Xiao, Y. (2011). Multivariate random forests. Wiley Inter-
disciplinary Reviews: Data Mining and Knowledge Discovery, 1(1), 80–87.
doi:10.1002/widm.12
Sela, R. J., & Simonoff, J. S. (2012). RE-EM trees: A data mining approach
for longitudinal and clustered data. Machine Learning, 86(2), 169–207.
doi:10.1007/s10994-011-5258-3
Steenbergen, M. R., & Jones, B. S. (2002). Modeling multilevel data
structures. American Journal of Political Science, 46(1), 218-237.
doi:https://doi.org/10.2307/3088424
Stekhoven, D. J., & Bühlmann, P. (2012). Missforest—non-parametric miss-
ing value imputation for mixed-type data. Bioinformatics, 28(1), 112-118.
doi:10.1093/bioinformatics/btr597
Suh, H., & Song, J. (2023). A comparison of imputation methods using machine learn-
ing models. Communications for Statistical Applications and Methods, 30(3),
331–341. doi:10.29220/CSAM.2023.30.3.331
Tang, F., & Ishwaran, H. (2017). Random forest missing data algorithms. Statisti-
cal Analysis and Data Mining: The ASA Data Science Journal, 10(6), 363-377.
doi:10.1002/sam.11348
Thurow, M., Dumpert, F., Ramosaj, B., & Pauly, M. (2021). Goodness (of
fit) of Imputation Accuracy: The GoodImpact Analysis. Working paper.
doi:https://doi.org/10.48550/arXiv.2101.07532
van Buuren, S., & Groothuis-Oudshoorn, K. (2011). mice: Multivariate imputa-
tion by chained equations in R. Journal of Statistical Software, 45(3), 1-67.
doi:10.18637/jss.v045.i03
van der Heijden, G. J., Donders, A. R. T., Stijnen, T., & Moons, K. G.
(2006). Imputation of missing values is superior to complete case anal-
ysis and the missing-indicator method in multivariable diagnostic research:
A clinical example. Journal of Clinical Epidemiology, 59(10), 1102-1109.
doi:https://doi.org/10.1016/j.jclinepi.2006.01.015
van Buuren, S. (2011). Multiple imputation of multilevel data. In J. Hox & J. Roberts
(Eds.),Handbook of advanced multilevel analysis (pp.173–196). NewYork:Rout-
23ledge.
van Buuren, S. (2012). Flexible imputation of missing data. Boca Raton: CRC Press.
van Buuren, S., Brand, J., Groothuis-Oudshoorn, C., & Rubin, D. (2006). Fully
conditional specification in multivariate imputation. Journal of Statistical Com-
putation and Simulation, 76(12), 1049-1064. doi:10.1080/10629360600810434
van Buuren, S., & Groothuis-Oudshoorn, K. (2011). mice: Multivariate imputa-
tion by chained equations in R. Journal of Statistical Software, 45(3), 1–67.
doi:10.18637/jss.v045.i03
van Buuren, S., & Oudshoorn, K. (1999). Flexible multivariate imputation by mice.
Leiden: TNO.
Waller, N. G. (2022). fungible: Psychometric functions from the waller lab. [Computer
software manual]. (version 2.2.1)
Wright,M.N.,&Ziegler,A. (2017). ranger:AFastImplementationofRandomForests
forHighDimensionalDatainC++andR. Journal of Statistical Software,77(1).
doi:10.18637/jss.v077.i01
Wright, M. N., Ziegler, A., & König, I. R. (2016). Do little interactions get lost in
dark random forests? BMC Bioinformatics, 17, 1–10.
24Appendix A. Running times
To also compare the computational time costs for the methods used, Figure A1 il-
lustrates the box plots of the imputation running times for each method. Among all
methods MICE is the slowest under all designs. Dummy-adjusted XGBoost is only
slightly faster than MICE for data with 25 clusters and almost twice as fast as MICE
with 50 clusters. The standard implementations of missRanger are always the fastest
(almost always ten times faster than MICE). All methods take more time imputing
data with 50 clusters than 25 clusters.
As for random slopes designs, MICE once again has the longest running times as
shown in Figure A2. Dummy-adjusted XGBoost takes almost as much time as MICE
with 10% missingness and 25 clusters. All the other methods are at least twice as fast.
Especially standard missRanger, which is around eight times faster than MICE. The
standard XGBoost takes more than twice as much time as standard missRanger.
25Figure A1.: Running Times for random intercept model
(a) MCAR
(b) MAR
Running Times for random intercept designs with 10% (first rows) and 50% (second rows) missingness and
25 (first columns) and 50 (second columns) clusters
26Figure A2.: Running Times for random slope models
(a) MCAR
(b) MAR
Imputation Running Times for random intercept designs with 10% (first rows) and 50% (second rows)
missingness and 25 (first columns) and 50 (second columns) clusters
27