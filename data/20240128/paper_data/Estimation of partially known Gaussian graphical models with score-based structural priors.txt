Estimation of partially known Gaussian graphical models with
score-based structural priors
Mart´ın Sevilla Antonio G. Marques Santiago Segarra
Rice University, USA King Juan Carlos University, Spain Rice University, USA
Abstract examples including gene interactions (Dobra et al.,
2004; Wang et al., 2020), spectrometric data (Codazzi
et al., 2022), metabolic association networks (Tan
We propose a novel algorithm for the sup-
et al., 2017), macroeconomic growth (Dobra et al.,
port estimation of partially known Gaussian
2010), and social networks (Li et al., 2020).
graphical models that incorporates prior in-
formation about the underlying graph. In ThesuccessofGGMsinsuchabroadrangeofdatasets
contrast to classical approaches that provide stems partly from their intuitive interpretability. To
a point estimate based on a maximum like- be specific, let A 0,1 n n be the adjacency matrix
× lihood or a maximum a posteriori criterion of , with n= ∈ , a{ nd} let x Rn be a random vector
G |V| ∈
using(simple)priorson the precision matrix, such that x (000,Σ). We then say that x is a GGM
∼N
we consider a prior on the graph and rely on with respect to if and only if the precision matrix
annealedLangevindiffusiontogeneratesam- Θ=Σ −1 andAG havethesamesupport(i.e.,thesame
ples from the posterior distribution. Since zero pattern) (Rue and Held, 2005). As a result, the
the Langevin sampler requires access to the graph associated with a GGM can be estimated from
score function of the underlying graph prior, the non-zero values of Θ.
we use graph neural networks to effectively
estimate the score from a graph dataset (ei- Related work and limitations. The problem ofes-
theravailablebeforehandorgeneratedfroma timating the precision matrix Θ (and, consequently,
known distribution). Numerical experiments its support) is classically known as covariance selec-
demonstrate the benefits of our approach. tion (Dempster, 1972). Specifically, given a set of k
observations x ,...,x , each of dimension n, the goal
1 k
is to recover the n n matrix Θ and, in particular,
×
supp(Θ), which reveals the conditional independence
1 Introduction
relationships in the GGM. Given that p(x Θ) has a
|
closed-form expression for a GGM, a straightforward
Graphical models are useful probabilistic tools repre-
approach is to find the maximum likelihood (ML) es-
sented by graphs = ( , ), where nodes encode
G V E V timator for Θ, which is given by the inverse of the
random variables, and edges encode information re-
E sample covarianceS(CasellaandBerger,2021). How-
garding the variables’ joint distribution. Markov ran-
ever, S 1 typically does not contain exact zero en-
dom fields (MRFs) – an important class of graphical −
tries. Hence, one common technique is thresholding
models–offertheattractivepropertythattheabsence
S 1 (Qiu and Liyanage, 2019), but this may yield un-
ofanedgebetweentwonodesmeansthatthetwoasso- −
satisfactory results if the threshold is not adequately
ciatedrandomvariablesareconditionallyindependent
chosen or if the number of observations k is limited.
given the rest (Bishop and Nasrabadi, 2006). When
the distribution is a multivariate Gaussian, the MRF More sophisticated techniques generally propose
issaidtobeaGaussianMarkovrandomfield(GMRF) penalties in optimization problems that use the log-
or Gaussian graphical model (GGM) (Rue and Held, likelihoodascostfunction(Tsaietal.,2022;Williams,
2005). GGMs have been used to model complex rela- 2020). From a Bayesian standpoint, these penalties
tionshipsinawidevarietyofdisciplines,withrelevant can be considered prior distributions p(Θ). For any
penalty function P(Θ), there is an associated prior
distributionp(Θ) exp( P(Θ))suchthatthepenal-
∝ −
Copyright 2024 by the author(s).
4202
naJ
52
]LM.tats[
1v04341.1042:viXraEstimation of partially known Gaussian graphical models with score-based structural priors
ized ML estimation boils down to Additionally, generating samples to create a synthetic
is easy for many random graphs, but the associated
Θ est =argmaxp(x 1,...,x k Θ)p(Θ) A graphdistributioncannotbeexpressedinclosedform.
|
Θ 0
⪰ Relevant examples include Barab´asi–Albert and small
=argmaxlogdetΘ tr(SΘ) P(Θ), (1)
world graphs. In the particular case of the actual dis-
− −
Θ 0
⪰ tribution p(A) being known, one could sample from it
with the most popular penalty being P(Θ) = and generate the dataset of graphs .
(cid:80) A
λ Θ , which translates to a Laplace prior from
i=j| ij | Additionally, we allow edge-to-edge constraints (in a
aBay̸ esianperspective. Thispenaltyencouragesspar-
WGL fashion) so that edges that are known to either
sityinΘandisconvex,rendering(1)easytooptimize
existorbeabsentleadtovaluesof1or0inA,respec-
using the graphical lasso (GL) algorithm (Banerjee
tively. Thisisusefulincaseswheresomepairsofvari-
etal.,2008;Friedmanetal.,2008). Asimilarapproach
ables are known to be conditionally independent, but
that allows to penalize each element in Θ by a differ-
we want to estimate the rest of the graph. Examples
entvalueistheweighted graphicallasso(WGL),which
include gene expression data (Li and Jackson, 2015),
imposes a penalty of the form P(Θ) = Λ Θ =
(cid:80) ∥ ◦ ∥1 neurotoxicologytests(Grzebyketal.,2004),socialnet-
Λ Θ (Li and Jackson, 2015; Zhuang et al.,
ij ij | ij | works (Wu et al., 2019), or brain networks (Simpson
2022; Zuo et al., 2017). Non-convex regularizers were
and Laurienti, 2015).
alsoproposedintheliterature(Williams,2020). While
more involved, they all boil down to encouraging dif- Our algorithm is based on Langevin dynamics, an
ferent forms of sparsity. MCMC sampler (Robert and Casella, 1999; Roberts
andTweedie,1996). Wedirectlysamplefromthepos-
Albeitlessnumerous,worksincorporatingpriordistri-
terior by defining a stochastic dynamic process whose
butionsthatdonotinvolvesparsityalsoexist. InZhou
stationary distribution matches the desired posterior
etal.(2021), abasegraphstructureisusedasaprior,
distribution. If the interest is in a point estimate, we
which could become too restrictive as it requires in-
can readily use the samples to estimate, e.g., the pos-
formation from the specific graph whose support we
terior mean of the missing values in A.
wanttoestimate. InWangandLi(2012),aG-Wishart
priordistributionisusedtogetherwithaMarkovchain Contributions. Our three main contributions are:
Monte Carlo (MCMC) sampler to determine the un-
1) We propose a novel GGM estimator based on sam-
derlying GGM. A similar approach is taken in Fried-
pling froma posterior distributionratherthanfinding
man and Koller (2003), but using a standard Wishart
theMLorMAPestimatorsandshowthatourestima-
instead. Another framework is given in Hosseini and
tor is consistent.
Lee(2016),focusingonmodularity. Eventhoughthese
approachesdonotassumemeresparsity,theypropose 2) We leverage annealed Langevin dynamics to imple-
pre-specified prior structures that may not apply to mentsuchanestimator,whichallowsustoincorporate
the graph under study. an arbitrary prior distribution learned from data. We
allow the known graphs to be of different sizes, as is
All techniques proposed so far for GGM estimation
the case in many practical applications.
impose limitations in the prior knowledge that can be
incorporated. Importantly, the imposed priors are too 3) Through numerical experiments, we show that in-
simple (e.g.,sparsity)or restrictive (e.g.,G-Wishart), corporating arbitrary prior distributions outperforms
and these are imposed on Θ while structural priors estimators that only consider sparsity as prior infor-
wouldbemorenaturallydefinedontheunderlyingad- mation or are just based on the likelihood of the ob-
jacency matrix A. servations.
Addressingtheselimitations. Thispaperproposes Notation. Scalars, vectors, and matrices are denoted
a new approach that allows the introduction of ar- by lowercase (y), lowercase bold (y), and uppercase
bitrary prior information directly on A, based on a bold (Y) letters, respectively. For a matrix Y, Y
ij
dataset of adjacency matrices (of potentially vary- denotesits(i,j)-thentry. Foravectory,itsi-thcom-
A
ing sizes) whose distribution we use as a prior p(A). ponentisrepresentedbyy . Iistheidentitymatrixof
i
This is particularly useful for real-world applications appropriate dimensions. The operation denotes the
◦
where we often have datasets of graphs instead of Hadamard product. We define the element-wise indi-
a closed-form prior distribution p(A). For instance, cator function as I , and the support of a matrix
when learning brain networks, leveraging graphs from as supp(Y)=I Y{ =·} 0 (i.e., a binary-valued matrix
{ ̸ }
other patients is feasible (and valuable) because they that is 0 in the (i,j) entries such that Y =0 and is 1
ij
oftensharesimilarstructures. Thisideaalsoappliesto otherwise).
other areas like molecular datasets or social networks.Mart´ın Sevilla, Antonio G. Marques, Santiago Segarra
2 Problem formulation which is infeasible to do. Second, even if p(X A)
|
were available, carrying out the maximization in (4)
We consider an unweighted and undirected graph wouldbeintractablesincethefeasiblesetcontains2
|U|
G
with no self-loops that consists of n nodes and a par- possible matrices.
tially known set of edges. The edge information is
Within the realm of point estimators, this work pro-
encoded in the adjacency matrix A 0,1 n n. To
∈ { } × poses an alternative approach to Problem 1, under
distinguish between the entries of A that are known
which we estimate A as the posterior mean instead
and the ones we aim toestimate, we define two sets of 0
of the posterior mode. That is, we aim to compute
indices and such that
O U (cid:88)
E[A X]= A p(A X). (6)
= (i,j):A is observed i<j , and (2) | · |
ij
O { ∧ } As.t.Aij=AO
ij
= (i,j):A is unknown i<j . (3)
ij
U { ∧ } Note that the estimation of A can be considered a
0
Throughout this work, we refer to the known and un- classification problem, where each edge is classified as
known fractions of the adjacency matrix as A and 0or1. Hence,choosingathresholdedversionof (6)as
O
A , respectively. The condition that i < j implies an estimator offers the desirable property of minimiz-
U
that we do not take into account the diagonal (since ing the edge classification error rate. However, even
the graph has no self-loops), and we just consider the if we knew p(A X), the summation in (6) requires
|
upper-triangular part of A (since it is symmetric). computing 2 terms. Our approach to bypass this is
|U|
to approximate (6) by taking the sample mean across
Apart from the known fraction of the graph, we also
M samples:
assume that k independent observations are avail-
able. We arrange them as columns in the matrix M
1 (cid:88)
X = (cid:2) x ... x (cid:3) Rn k. Each observation x fol- E[A X] A(m). (7)
lows a n1 ormal dk ist∈ ributi× on
N
(cid:0) 000,Θ −01(cid:1) , where Θ
0
is | ≃ M m=1
thetrueprecisionmatrix. SinceA 0 =supp(Θ 0),then ThesamplesA(m) shouldbedrawnfromtheposterior
Θ isknowntobe0whereA is0andisknowntobe
0 O0
p(A X) p(X A)p(A), (8)
differentfrom0whereA O0 is1. Wearealsogivenaset | ∝ |
of adjacency matrices drawn from the distribution where we omitted conditioning on A to avoid cum-
A O
p(A),thesamedistributionfromwhichA 0wasdrawn. bersome notation. As already explained, computing
In this setting, our problem is defined as follows: p(X A) in (8) is, in general, infeasible. As a result,
|
ratherthantryingtoobtainp(A X),ourapproachis
Problem 1. Given the k observations X, a partially |
todesignanalgorithmcapableofsamplingfrom(8)di-
known adjacency matrix A , and structural prior in-
O0 rectlywithoutexplicitlycomputingtheposterior. The
formation given by a set of matrices , find an esti-
A designofsuchanalgorithm,whichhasvalueperseand
mate of A .
U0 canbeusedtodesignotherpointestimators,istackled
in Section 3.
A natural way to solve Problem 1 would be to com-
pute the MAP, forcing the entries of the estimate to
3 Langevin for support estimation
be equal to those of A for all positions (i,j) .
O0
∈ O
Mathematically, this is given by
This section explains how to use annealed Langevin
Aˆ =argmax p(X A)p(A) (4) dynamics to solve Problem 1. In Section 3.1, we pro-
MAP
A | pose a distribution: i) that approximates the actual
subject to A =A (i,j) . posterior(8)andii)fromwhichsamplescanbedrawn
ij O0ij
∀ ∈O
by leveraging Langevin dynamics. Based on this ap-
There are two main issues when solving (4), which proximateposterior,wedefineanestimatorofA and
0
we will describe in detail next. First, the likelihood showthatitisconsistent. Sections3.2and3.3explain
p(X A) is not easy to calculate, since only the ex- how Langevin dynamics and its annealed counterpart
|
pression for p(X Θ) is available, which is workandwhytheyprovideawaytoincorporateprior
|
informationintheestimationofthebinarymatrixA
(cid:115) 0
detΘk (cid:18) k (cid:19) via the so-called score function. Then, in Section 3.4,
p(X Θ)= exp tr(SΘ) , (5)
| (2π)nk −2 we study how to use the dataset as prior knowledge
A
bytrainingagraphneuralnetwork(GNN)whoseout-
where S = 1XX is the sample covariance. Hence, put is directly plugged into the Langevin dynamics.
k ⊤
computing p(X A) requires integrating (5) over all Section 3.5 describes our final algorithm, which com-
|
possible precision matrices such that supp(Θ) = A, bines and summarizes the results of this section.Estimation of partially known Gaussian graphical models with score-based structural priors
3.1 Proposed estimator Proof. See Section F.2 in the SM.
The first step of our algorithm consists of computing
WenowleverageLemmas1and2toshowconsistency
the following estimator for Θ 0, of Aˆ.
Θˆ =argmax logdetΘ tr(SΘ) Theorem 1. Aˆ as defined in (11) is a consistent esti-
−
Θ 0 mator of the true adjacency matrix A when M
⪰ 0
→∞
s. to Θ ij =0 ∀(i,j):A O0ij =0, (9) and τ k −k −→ −−∞ →1.
which corresponds to the positive definite matrix that
Proof. According to Lemma 2, the only matrices A
maximizesthelikelihoodwhilerespectingthezeropat-
withapositiveprobabilityofbeingsampledask
tern known to exist. The optimization problem in (9)
→∞
are those that satisfy
can be efficiently solved by using the WGL algorithm
described in Section 1. The constraint is equivalent to
Θ =Θ (A+I). (13)
setting a penalty Λ ij to an arbitrarily large constant 0 0 ◦
forthoseentrieswhereAisknown tobe0,andsetting
Let A(m) be the m-th sample drawn from (12). The
Λ =0 otherwise.
ij
condition in (13) leads to
Let (Θ) = p(X Θ) denote the likelihood of the
preciL siX onmatrixgive| ntheobserveddata. Then,based P(cid:104) A(m) =0 Θ =0(cid:105) =0 m=1,...,M. (14)
ij | 0ij ̸ ∀
on the estimator in (9), we approximate the posterior
p(A X) in (8) as SincetheestimatorAˆ from(11)isthemeanofsamples
|
(cid:16) (cid:17) that follow (14), for τ >0 we have that
pˆ(A X) Θˆ (A+I) p(A), (10) k
X
| ∝L ◦ (cid:104) (cid:105)
P Aˆ =0 Θ =0 =0. (15)
wherewerecallthat istheentry-wiseproduct. Since ij | 0ij ̸
◦
the entries of (A+I) are binary, the entry-wise mul-
On the other hand, false positives have a non-zero
tiplication can be understood as a mask that sets to
probability of being sampled:
zero the entries of the precision that are not associ-
a sate md plw eit fh roman (e 1d 0g )e. anL det letus Asu (p mp )os Me now denth oa tet w the ec sa en
t
P(cid:104)
Aˆ =1 Θ
=0(cid:105) =P(cid:34) (cid:88)M A( ijm)
τ
(cid:12) (cid:12)
(cid:12)Θ
=0(cid:35)
.
of M generated independe{ nt sam} pm l= es1 . Then, the set ij | 0ij m=1 M ≥ k(cid:12) (cid:12) 0ij
A(m) M can be used to characterize the posterior. (16)
{ Wefoc} um s= o1 ntheposteriorsamplemeanestimatorpre- In the context of this proof, τ k 1. Additionally,
sented in (6)–(7). Then, the estimator for A that we the summation in (16) can be at
m→
ost 1, since
A(m)
0 ij ∈
propose boils down to 0,1 . Thus,
{ }
Aˆ
=I(cid:40)(cid:32) M1 m(cid:88)M =1A(m)(cid:33)
≥τ
k(cid:41)
, (11)
P(cid:104)
Aˆ ij=1 |Θ
0ij=0(cid:105)
−k −→ −−∞
→P(cid:34) m(cid:88)M =1A M( ijm) =1(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)Θ
0ij=0(cid:35)
.
(17)
where τ is a tunable threshold that should increase
k
Another way of writing (17) is
with the sample size k.
We aim to prove that (11) is a consistent estimator of P(cid:34) (cid:88)M A( ijm) =1(cid:12) (cid:12)
(cid:12)Θ
=0(cid:35) =(cid:16) P(cid:104)
A(1)=1 Θ
=0(cid:105)(cid:17)M
,
A 0, a fundamental result in our study. Before delving M (cid:12) (cid:12) 0ij ij | 0ij
intosuchaproof,weestablishtwoimportantinterme-
m=1
(18)
diate results.
as each sample is drawn independently from the rest.
Lemma 1. Θˆ as defined in (9) is a consistent esti- Since A p(A), then from (12) it follows that
0
mator (as k ) of the true precision matrix Θ . ∼
→∞ 0 (cid:104) (cid:105)
P Aˆ(1) =0 Θ =0 >0. (19)
ij | 0ij
Proof. See Section F.1 in the Supplementary Material
(SM). Namely, given that the true adjacency matrix A has
0
a prior distribution p(A), it would not be possible for
Lemma 2. The approximate posterior pˆ(A X)
| this matrix to have zero probability of being sampled
in (10) converges in distribution to
from (12). Combining (19) with (18), and then taking
k
p(A) the limit of (17) when M we get
pˆ(A X) →∞ δ(Θ 0 (A+I) Θ 0), (12) →∞
| −−−−→ C ◦ −
(cid:104) (cid:105) k
where C is a constant and δ( ·) is the Dirac delta. P Aˆ ij =1 |Θ 0ij =0 −M −→ −→ −∞ ∞ →0. (20)Mart´ın Sevilla, Antonio G. Marques, Santiago Segarra
From (20) and (14) it follows that, if both M continuous, and the iterative procedure involving an-
→ ∞
and τ 1, then (11) converges in probability to the nealed Langevin dynamics for our problem is given by
k
→
true adjacency matrix when k .
→∞ a˜ =a˜ +α logp(a˜ X)+√2α z , (23)
t+1 t t a˜ t t t
∇ |
TocomputeAˆ asin(11)weneedtobeabletosample whereα =ϵ σ2 /σ2 andl(t)isanincreasingfunction
t · l(t) L
from the posterior distribution in (10). To this end, mapping time steps t to the annealing noise levels l.
we utilize the stochastic diffusion process of Langevin Note that the noise present in a˜ (i.e., the variance of
t
dynamics. v ) decreases with t, as given by the varying step
l(t)
size α .
t
3.2 Langevin dynamics The annealed version of the dynamics was initially in-
troducedtoallowthealgorithmtoconvergefasterand
The Langevin dynamics algorithm is an MCMC
perform better (Song and Ermon, 2019). However, in
method that allows us to draw samples from a dis-
our case, it also offers the advantage of rendering the
tributiondifficulttosamplefromdirectly(Robertand
problem differentiable. Consequently, the annealing
Casella,1999;RobertsandTweedie,1996). Thissam-
enables the computation of the score functions and
pler’s great advantage is that it does not require an
the use of Langevin dynamics to sample from an orig-
expression for the target distribution but rather for inally discrete distribution. If the noise levels σ L
the gradient of its logarithm. Generically, to sample { l }l=1
and the step size ϵ are chosen adequately (Song and
from p(w) via Langevin, only logp(w) is needed.
∇w Ermon, 2019), after a sufficiently large number of it-
This gradient receives the name of score function and
erations,thesamplea˜ isarbitrarilyclosetoanactual
t
is of paramount relevance in the ensuing sections.
sample from the discrete distribution p(a X). The
|
For a generic target distribution p(w), the Langevin noisy sample a˜ t must be projected onto the set 0,1
{ }
dynamics are given by if an actual sample is needed.
Now we need to compute the annealed score
w =w +ϵ logp(w )+√2ϵz , (21)
t+1 t ∇w t t a˜logp(a˜ X) to sample graphs using (23). To avoid
∇ |
the use of cumbersome notation in what follows, from
where t is an iteration index, ϵ is the step size and
now on, we drop the reference to a˜ in the gradients,
z (0,I). In each iteration, w tends to move in
t ∼ N t as we always take the derivatives with respect to that
the direction of the score function but is also affected
vector. Using (10), we express the (approximate) an-
bywhitenoisethatpreventsitfromcollapsinginlocal
nealed posterior score as
maxima. Under some regularity conditions, w con-
t
verges to be a sample from p(w) when ϵ 0 and logpˆ(A˜ X)= log (Θ˜)+ logp(A˜), (24)
t (Welling and Teh, 2011). → ∇ | ∇ LX ∇
→∞
where we have defined
It should be noted that, in our case, we are trying
to sample a discrete random vector (i.e., a vectorized Θ˜ =Θˆ (A˜ +I). (25)
unweighted adjacency matrix). Hence, the gradient ◦
of the target log-density is not defined in our setting. We next discuss each of the two terms in (24). Start-
A noisy (continuous) version of the random vector is ingwith log (Θ˜),referredtoastheannealed like-
X
∇ L
used to circumvent this obstacle. This idea leads to lihood score, we compute it as [cf. (5)]
the annealed Langevin dynamics (Kawar et al., 2021;
k k
Song and Ermon, 2019). log (Θ˜)= logdet(Θ˜) tr(SΘ˜), (26)
X
∇ L 2∇ − 2∇
3.3 Annealed Langevin dynamics withthetwogradientsin(26)beingstraightforwardto
compute (Petersen and Pedersen, 2012). Specifically,
To simplify the notation of what follows, we use A let us define ∆Σ˜ = Θ˜−1 S and use Tij to denote a
and its half-vectorization a = vech(A) interchange- −
matrix whose entries are all equal to zero except the
ably. Consider a noisy version of a,
(i,j)-th and the (j,i)-th ones, which are one. Then,
a˜ =a+v, (22) ∂log (Θ˜) k (cid:104)(cid:16) (cid:17)(cid:16) (cid:17)(cid:105)
LX = tr 2∆Σ˜ +∆Σ˜ I Θˆ Tij .
∂A˜ 2 ◦ ◦
wherevrepresentsadditiveGaussiannoise. Morepre- ij
(27)
cisely, let σ L be a sequence of noise levels such
that σ >{ σl } >l=1 > σ > 0. Then, for each noise We shift now to logp(A˜), the second term in (24),
1 2 L
··· ∇
level we define v (000,σ2I). In this setting, a˜ is which is referred to as the annealed prior score and is
l ∼ N lEstimation of partially known Gaussian graphical models with score-based structural priors
more difficult to obtain. Note that computing p(A˜) Algorithm1AnnealedLangevinforGGMestimation
requires convolving p(A) with the distribution of the Require: X,A ,g (), σ L ,M,T,ϵ,τ
noise [cf. (22)], which is infeasible not only because of 1: S 1XX O0 ξ · { l }l=1 k
the computational burden of that task but also be- ← k ⊤
2: Compute Θˆ as in (9)
cause we do not know p(A). The alternative that
3: ▷ Set of generated samples
we propose is to estimate the annealed prior score S ←{}
logp(A˜)justusingsamplesfromthepriorp(A)(i.e., 4: repeat
∇ t (h 20e 23av ).aila Wbl ee mda ot da es let
thA
is), esa ts imi an teSe av sill aa Gan Nd N,Se wg har er ra
e
5 6:
:
I A˜n O0iti ←aliz Ae O0A˜ U0 ∼N(0.5,0 ▷.5 FI i)
x the known values
7: for l 1toL do
weights are trained on the dataset , as we explain ←
in Section 3.4. A 8: α l ←ϵ ·σ l2/σ L2 ▷ Change the noise level
9: for t 1toT do
←
10: Draw Z (000,I)
t
3.4 Learned annealed scores 11: Compute∼N log (Θ˜ ) as in (27)
X t
12: Compute ∇ g (A˜ L ,σ )
Let g (a˜,σ) be the output of the GNN we wish to ξ t 1 l
ξ 13: ∆ log (− Θ˜ )+g (A˜ ,σ )
lt
at
er
h
sva
e
ei pn
lo
o,
u σ
sw
st
l
ip
bi ot
u
lh
f et
tξ
tf ho
oeb
r
te
a c
hin
u
egg
rri av
ei ct
ne
ts
n t
ut air
a˜ t
la ei sr(n cawa otib
rit o
el he n)tp ha
s le
ohra goam
u ps l
(se
d
a˜ot )e
c b
.r ias e.
t Te
adI hsd ee
n
ca
o l lo
ol il
s s
sy
ee
s,
11 1 64 5 :: : endAA˜ ˜
ft
OtUt
o←
r←
←∇
A A˜ ˜Ut Ot− −1
1L +X
α
l∆t
Ut
+√ξ
2α
lt Z−1
t
l
function to learn ξ should be d∇ esigned to minimize 17: A˜ 0 A˜ T
←
the mean squared error across all noise levels jointly. 18: end for
To achieve this, we define the distance 19: A˜ A˜ T ▷ A sample from p(A˜ X)
← (cid:110) (cid:111) |
20: A I A˜ 0.5 ▷ Project onto 0,1
D(a˜ |ξ,σ l)= ∥g ξ(a˜,σ l) −∇logp(a˜ |a) ∥2
2
(28)
21:
← (cid:83) A≥ { }
=(cid:13)
(cid:13)g ξ(a˜,σ l) −(a −a˜)/σ
l2(cid:13) (cid:13)2
2
22:
untS il←
S
cS ont{ ain}
s M samples
23: Store the sample mean of in A
mean
and the associated loss function 24: Aˆ I A τ S
mean k
← { ≥ }
25: return Aˆ
L
(cid:0) ξ σ L (cid:1) = 1 (cid:88) σ2E[ (a˜ ξ,σ )]. (29)
J |{ l }l=1 2L l D | l
l=1
Following the proof in Vincent (2011), it follows that
the output of a GNN trained with (29) correctly es-
timates logp(a˜). It is worth pointing out that the dataset in order to be able to compute g ξ(a˜,σ)
∇ A ≃
term (a −a˜)/σ l2 is known during training: a is one ∇logp(a˜) for the different noise levels σ l.
element of and both a˜ and σ are the GNN inputs.
A l ThefirststepinAlgorithm1istocomputeΘˆ asin(9).
ThearchitectureoftheGNNmustaccountforthefact Wethendrawsamplesfromtheapproximateposterior
that the same graph can be represented by different distributionpˆ(A X)byrunningthedynamicsin(23).
|
adjacency matrices, depending on the node labeling. Recall that this is possible because a) we count on a
In this work, we leverage the EDP-GNN (Niu et al., closed-form(approximate)expressionfortheannealed
2020), designed to perform score-matching on graphs likelihood(27),andb)wehavefoundawaytoestimate
by proposing a permutation equivariant method to the annealed prior score by training a GNN with .
A
model the score function of interest. Notice that, in each step, we just update the values of
A˜ , leaving the known values in A˜ fixed.
U O
3.5 Final algorithm
After LT steps for each sample, the algorithm gen-
erates a continuous matrix A˜. As we work with un-
Now we need to put all the pieces together: the
proposed (consistent) estimator Aˆ (Section 3.1), the weighted graphs, it is necessary to make the predic-
tion binary-valued. Therefore, the algorithm draws
Langevindynamicstogetthesamplestocomputethat (cid:110) (cid:111)
estimator (Sections 3.2 and 3.3), and the GNN train- I A˜ 0.5 as a sample instead, representing an
≥
ing to estimate the score needed to run the Langevin element-wise projection onto the set 0,1 . Following
{ }
dynamics (Section 3.4). The final scheme is described thisprocedure,wedrawM samplesandthencompute
in Algorithm 1. Notice that the score estimator g () their average. Lastly, we apply a threshold τ to the
ξ k
·
is an input. Namely, before performing any GGM es- approximateposteriormeantocomputetheconsistent
timation, a GNN has to be trained with the desired estimator Aˆ.Mart´ın Sevilla, Antonio G. Marques, Santiago Segarra
4 Numerical results Partially unknown grids. We consider grids of dif-
ferent heights and widths with few additional random
We carry out simulations in different setups 1 to edges. Results are shown in Figure 1.
demonstrateourscheme’spracticalrelevanceandgain
As k increases, the performance of the predictors that
insightregardinghowinformativethepriorknowledge
use X increases, except for GraphSAGE. Recall that
is when estimating A .
0 the presence of an edge between two nodes does not
In all the simulations, we first generate a fully-known imply a direct correlation between the variables, but
graph and then drop random entries of a , which rather conditional dependence given the rest of the
0
we then try to estima| tU e| . We generate M = 10 sam- graph. Considering that this relationship is not cap-
ples for each graph to compute (11). We compare our tured by local neighborhoods, which is how Graph-
method with: 2 SAGE aggregates node data, this method is expected
to not benefit from including more observations.
• WGL (Li and Jackson, 2015). We penalize the in- For both sizes of and for the four different ratios
|U|
dicesin withaparameterλ, useanarbitrarilylarge k/ , LPost outperforms all the other approaches.
U |U|
penaltywhereA is0,anddonotpenalizetheentries However,itisworthpointingoutthatthegapismuch
O0
where A is 1. moreprominentinFigure1awhen issmaller. LPr’s
O0 |U|
• Thresholding. We compute Θˆ and threshold it. predictionsalsopresentahigherF1scoreinthatcase.
• TIGER. The GGM estimation method in Liu and This behavior leads to thinking that the information
Wang (2017) which does not require tuning. provided by decreases as (and, thus, the dimen-
A |U|
• GraphSAGE. A link prediction method (not a sions of the space from which the Langevin process is
GGM estimation method like the others) based on sampling) increases. A complementary experiment on
GNNs (Hamilton et al., 2017). We use the measure- theperformancedependenceon foradifferenttype
|A|
ments X as node features and A as the training set of graph is presented in Section C.1 of the SM.
O0
while testing A .
U0 When is small, the prior probability mass is con-
|U|
centrated among fewer possible graphs. Intuitively, in
Allthethresholds(τ k forouralgorithmandthoseused this case, Langevin generally samples either the same
forthethresholdingandGraphSAGEmethods)andλ graphorsimilaronesthroughoutthedifferentM sam-
for WGL are tuned using a training set. It is worth ples. Thus, the sample mean yields a satisfactory es-
pointingoutthattheinformationgivenbyA O0 cannot timate. When
|U|
is large, the probability mass is
be used within the TIGER algorithm, as it requires spread across many adjacency matrices in the high-
fixing some entries of Θ. dimensional space of p(A˜). This leads to Langevin
converging to diverse graphs each time we sample, re-
Additionally, we use as a benchmark a variant of Al-
ducingtheusefulnessofthesamplemeanasanestima-
gorithm 1. We label it as “Langevin prior” (LPr),
tor. An additional experiment illustrating the perfor-
since it consists of just using prior information (i.e.,
∆ t =g ξ(A˜ t −1,σ l) in line 13). In other words, we test m ofa tn hc ee Sd Mep .endence on |U| is presented in Section H.2
the algorithm when no observations are available, but
only is. Our method is labeled as “Langevin poste- Partially unknown Barab´asi–Albert graphs.
A
rior”(LPost),usingboththepriorandlikelihoodscore
Now we consider the dual Barab´asi–Albert preferen-
functions.
tial attachment model (Moshiri, 2018). All graphs in
Werunsimulationsforthreedifferentkindsofgraphs. are such that n 47,49,51,53 , while we used
A ∈ { }
Two of them, grid graphs and Barab´asi–Albert graphswithn 46,48,50,52 nodestotestthealgo-
∈{ }
graphs (Barab´asi and Albert, 1999), are synthetic, rithm. ThisallowsustoverifywhethertheEDP-GNN
whilethethirdoneconsistsofego-netsofEasternEu- correctly generalizes the score estimation. The results
ropean users collected from the music streaming ser- are shown in Figure 2a.
vice Deezer (Rozemberczki et al., 2020). Next, we re-
Once again, LPost yields better results than the other
portanddiscussthenumericalresultsforallsimulated
algorithms, mainly when k is small. As more obser-
scenarios. We report the average F1 score over 10 dif-
vations are available, all of the methods (except for
ferent train/test splits over 100 graphs in each case.
LPr and GraphSAGE) have approximately the same
performance–theinformationprovidedby becomes
1Source code is available at https://github.com/ negligible compared to that offered by X. A
Tenceto/langevin_ggm.
2Additionaldetailsonhyperparameterchoices, proper- TheF1scoreachievedbyLPrisrelativelypoordueto
ties of datasets, and computation of the reported metrics thelargerandomnessinthegenerationoftheunderly-
can be found in Section B of the SM.Estimation of partially known Gaussian graphical models with score-based structural priors
0.9
LPost Threshold LPost Threshold
0.8 LPr TIGER 0.6 LPr TIGER
0.7 WGL GraphSAGE WGL GraphSAGE
0.5
0.6
0.5 0.4
0.4
0.3
0.3
0.2 0.2
0.1
0.1
0.15 0.35 0.85 2.00 0.15 0.35 0.85 2.00
k/ |U| k/ |U|
(a) (a)
0.6 0.8
LPost
LPr 0.7
0.5 WGL
Threshold 0.6
TIGER
0.4 GraphSAGE 0.5
0.4
0.3
0.3 LPost Threshold
LPr TIGER
0.2
0.2 WGL GraphSAGE
0.15 0.35 0.85 2.00 0.15 0.35 0.85 2.00
k/ k/
|U| |U|
(b) (b)
Figure1: F1scoreofseveralmethodsusinggridgraphs Figure 2: F1 score of several methods using
with 40 n 50 where (a) 10% and (b) 20% of the (a)Barab´asi-Albertgraphswith =0.1dim(a),and
≤ ≤ |U|
values in a are unknown. (b) ego-nets with =0.5dim(a).
|U|
Ego-nets,likegrids,presentastrongstructure,render-
inggraphs. Namely,itisalwaysworsethantheoneob- ingthepriorhighlypredictive. Eventhoughhalfofthe
tained using WGL. On the contrary, when the under- graph is unknown, the F1 score of LPr is the highest
lying graph presents more structure (for instance, the among all the experiments (cf. Figures 1 and 2a).
grid graphs in Figure 1a), LPr was shown to outper-
Overall, our numerical experiments show that i) our
form WGL for some values of k/ . We can conclude
|U| approachleadstobettergraphestimationresultsthan
thatsomepriorsoffermorepredictivepowerthanoth-
theclassicalalternativesconsideredandii)thebenefits
ers: the more substantial the structure of the graphs,
ofourapproacharemoresignificantwhenthenumber
the more useful p(A) becomes.
ofobservationsissmall,andthegraphpresentsmarked
structural features.
Partially unknown ego-nets. Now,weconsiderthe
graphsintheDeezerdatasetwithn 25. Theresults
≤
are shown in Figure 2b. 5 Conclusions
Once again, our method exhibits a higher edge pre-
diction performance than the rest. The behavior is We proposed a GGM estimation algorithm based on
similartotheoneobservedintheprevioussetups: the annealedLangevindynamicsthatallowsustoleverage
accuracy of all GGM-based methods increases with k. graphstructuralpriorsbeyondsparsity. Ourapproach
GraphSAGE slightly outperforms LPost in this sce- exploitsasetofknowngraphstoextractthepriordis-
narioforthesmallestvaluesofk. Ego-netsarestrongly tribution. We designed an algorithm that, by combin-
local-based, and GraphSAGE is expected to outper- ing annealed Langevin dynamics with a GNN-based
form the rest of the approaches when the information annealed prior score estimator, was able to draw sam-
provided by the observations is negligible. plesfromtheposteriordistributionofinterest,namely
erocS1F
erocS1F
erocS1F
erocS1FMart´ın Sevilla, Antonio G. Marques, Santiago Segarra
thedistributionoftheunknownedgesgiventheknown Dobra, A., Hans, C., Jones, B., Nevins, J. R., Yao,
ones, the structural prior, and the GMRF observa- G., and West, M. (2004). Sparse graphical mod-
tions. Finally, we proposed a consistent point esti- els for exploring gene expression data. Journal of
mate for the graph that underlies the GGM based on Multivariate Analysis, 90(1):196–212.
the sample posterior mean. Through numerical ex-
Friedman, J., Hastie, T., and Tibshirani, R. (2008).
periments, we showed our method outperforms classi-
Sparseinversecovarianceestimationwiththegraph-
cal ones, especially in cases with few observations and ical lasso. Biostatistics, 9(3):432–441.
highly structured graphs.
Friedman, N. and Koller, D. (2003). Being Bayesian
about network structure. A Bayesian approach to
Acknowledgments
structure discovery in Bayesian networks. Machine
learning, 50:95–125.
This research was sponsored by the Army Research
Office under Grant Number W911NF-17-S-0002; the Grzebyk,M.,Wild,P.,andChouani`ere,D.(2004). On
Spanish(MCIN/AEI/10.13039/501100011033)Grants identificationofmulti-factormodelswithcorrelated
PID2019-105032GB-I00 and PID2022-136887NB-I00; residuals. Biometrika, 91(1):141–151.
theAutonomousCommunityofMadridwithintheEL- Hamilton, W., Ying, Z., and Leskovec, J. (2017). In-
LIS Unit Madrid framework; and the Fulbright U.S. ductiverepresentationlearningonlargegraphs. Ad-
Student Program, in turn sponsored by the U.S. De- vances in Neural Information Processing systems,
partment of State and the U.S.–Argentina Fulbright 30.
Commission. The views and conclusions contained in
Hosseini, M. J. and Lee, S.-I. (2016). Learning sparse
thisdocumentarethoseoftheauthorsandshouldnot
Gaussian graphical models with overlapping blocks.
be interpreted as representing the official policies, ei-
Advances in neural information processing systems,
ther expressed or implied, of the Army Research Of-
29.
fice, the U.S. Army, the Fulbright Program, the U.S.–
Hunter, D. R. and Handcock, M. S. (2006). Inference
Argentina Fulbright Commission, or the U.S. Govern-
in curved exponential family models for networks.
ment. The U.S. Government is authorized to repro-
Journal of Computational and Graphical Statistics,
duceanddistributereprintsforGovernmentpurposes,
15(3):565–583.
notwithstanding any copyright notation herein.
Kawar,B.,Vaksman,G.,andElad,M.(2021). SNIPS:
Solving noisy inverse problems stochastically. Ad-
References
vances in Neural Information Processing Systems,
Banerjee, O., El Ghaoui, L., and d’Aspremont, A. 34:21757–21769.
(2008). Model selection through sparse maximum
Li,T.,Qian,C.,Levina,E.,andZhu,J.(2020). High-
likelihood estimation for multivariate Gaussian or
dimensionalGaussiangraphicalmodelsonnetwork-
binary data. The Journal of Machine Learning Re-
linked data. The Journal of Machine Learning Re-
search, 9:485–516.
search, 21(1):2851–2895.
Barab´asi, A.-L. and Albert, R. (1999). Emer- Li, Y. and Jackson, S. (2015). Gene network recon-
gence of scaling in random networks. Science, struction by integration of biological prior knowl-
286(5439):509–512. edge. G3-Genes Genomes Genetics, 5:1075–1079.
Bishop, C. M. and Nasrabadi, N. M. (2006). Pattern Liu, H. and Wang, L. (2017). TIGER: A tuning-
Recognition and Machine Learning. Springer. insensitive approach for optimally estimating Gaus-
sian graphical models. Electronic Journal of Statis-
Casella, G.andBerger, R.L.(2021). Statistical Infer-
tics, 11(1):241 – 294.
ence. Cengage Learning.
Moshiri, N. (2018). The dual-Barab´asi-Albert model.
Codazzi, L., Colombi, A., Gianella, M., Argiento, R.,
Niu, C., Song, Y., Song, J., Zhao, S., Grover, A., and
Paci, L., and Pini, A. (2022). Gaussian graphical
Ermon,S.(2020). Permutationinvariantgraphgen-
modeling for spectrometric data analysis. Compu-
eration via score-based generative modeling. In In-
tational Statistics & Data Analysis, 174:107416.
ternationalConferenceonArtificialIntelligenceand
Dempster,A.P.(1972). Covarianceselection. Biomet- Statistics, pages 4474–4484. PMLR.
rics, pages 157–175. Petersen, K. B. and Pedersen, M. S. (2012). The ma-
trix cookbook. Version 20121115.
Dobra, A., Eicher, T. S., and Lenkoski, A. (2010).
Modeling uncertainty in macroeconomic growth de- Qiu,Y.andLiyanage,J.S.(2019).Thresholdselection
terminantsusingGaussiangraphicalmodels. Statis- for covariance estimation. Biometrics, 75(3):895–
tical Methodology, 7(3):292–306. 905.Estimation of partially known Gaussian graphical models with score-based structural priors
Ravikumar, P., Wainwright, M. J., Raskutti, G., Wang, Y., Segarra, S., and Uhler, C. (2020). High-
and Yu, B. (2011). High-dimensional covari- dimensional joint estimation of multiple directed
ance estimation by minimizing l1-penalized log- Gaussian graphical models. Electronic Journal of
determinant divergence. Electronic Journal of Statistics, 14(1):2439 – 2483.
Statistics, 5(none):935 – 980.
Welling, M.andTeh, Y.W.(2011). Bayesianlearning
Robert, C. and Casella, G. (1999). Monte Carlo Sta- via stochastic gradient Langevin dynamics. In Intl.
tistical Method. Springer. Conf. on Machine Learning, page 681–688.
Roberts, G. O. and Tweedie, R. L. (1996). Exponen- Williams,D.R.(2020).Beyondlasso: Asurveyofnon-
tial convergence of Langevin distributions and their convexregularizationinGaussiangraphicalmodels.
discrete approximations. Bernoulli, 2:341–363. Wu, Q., Zhang, Z., Waltz, J., Ma, T., Milton, D., and
Rozemberczki, B., Kiss, O., and Sarkar, R. (2020). Chen,S.(2019). Predictinglatentlinksfromincom-
KarateClub: AnAPIOrientedOpen-sourcePython pletenetworkdatausingexponentialrandomgraph
Framework for Unsupervised Learning on Graphs. model with outcome misclassification. bioRxiv.
In ACM International Conference on Informa- Zhou, J., Hoen, A., Mcritchie, S., Pathmasiri, W.,
tion and Knowledge Management, page 3125–3132. Viles, W., Nguyen, Q., Madan, J., Dade, E., Kara-
ACM. gas, M., and Gui, J. (2021). Information enhanced
Rue, H. and Held, L. (2005). Gaussian Markov Ran- model selection for Gaussian graphical model with
dom Fields: Theory and Applications. CRC press. application to metabolomic data. Biostatistics, 23.
Sevilla, M. and Segarra, S. (2023). Bayesian topology Zhuang, Y., Xing, F., Ghosh, D., Banaei-Kashani,
inference on partially known networks from input- F., Bowler, R. P., and Kechris, K. (2022). An
output pairs. augmentedhigh-dimensionalgraphicallassomethod
to incorporate prior biological knowledge for global
Simpson, S. L. and Laurienti, P. J. (2015). A
network learning. Frontiers in Genetics, page 2405.
two-part mixed-effects modeling framework for an-
alyzing whole-brain network data. NeuroImage, Zuo, Y., Cui, Y., Yu, G., Li, R., and Ressom, H.
113:310–319. (2017). Incorporatingpriorbiologicalknowledgefor
network-based differential gene expression analysis
Snijders, T. A. B., Pattison, P. E., Robins, G. L.,
using differentially weighted graphical lasso. BMC
and Handcock, M. S. (2006). New specifications
Bioinformatics, 18.
for exponential random graph models. Sociological
Methodology, 36(1):99–153.
Song, Y. and Ermon, S. (2019). Generative modeling
byestimatinggradientsofthedatadistribution. Ad-
vances in Neural Information Processing Systems,
32.
Sundaram,R.K.(1996).Afirstcourseinoptimization
theory. Cambridge University Press.
Tan, L. S. L., Jasra, A., Iorio, M. D., and Ebbels, T.
M.D.(2017). BayesianinferenceformultipleGaus-
siangraphicalmodelswithapplicationto metabolic
associationnetworks. The Annals of Applied Statis-
tics, 11(4):2222–2251.
Tsai, K., Koyejo, O., and Kolar, M. (2022). Joint
Gaussian graphical model estimation: A survey.
Wiley Interdisciplinary Reviews: Computational
Statistics, 14(6):e1582.
Vincent,P.(2011).Aconnectionbetweenscorematch-
ing and denoising autoencoders. Neural Computa-
tion, 23(7):1661–1674.
Wang, H. and Li, S. Z. (2012). Efficient Gaussian
graphical model determination under G-Wishart
distributions. Electronic Journal of Statistics,
6:168–198.Mart´ın Sevilla, Antonio G. Marques, Santiago Segarra
Estimation of partially known Gaussian graphical models with
score-based structural priors
Supplementary Materials
F PROOFS OF LEMMAS
F.1 Proof of Lemma 1
Let n = V Rn n V 0 be the set of all positive semidefinite matrices. Then, we define a function
S+ { ∈ × | ⪰ }
h: n n such that
S+ →S+
h(V)=argmaxf(Θ;V)
Θ 0
⪰
s. to Θ =0 (i,j):A =0, (SM30)
ij
∀
Oij
where f : n R is f(Θ;V) = logdetΘ tr(VΘ). It immediately follows that the estimator in (9) of the
S+ → −
main paper satisfies
Θˆ =h(S), (SM31)
with S = 1XX being the sample covariance matrix. The estimator Θˆ is consistent if h(S) approaches Θ as
k ⊤ 0
k increases. Hence, this is what we want to prove next.
First, we compute
h(cid:0)
Θ
−01(cid:1)
. To this end, we first check what matrix maximizes f without considering the
constraint in (SM30). Since f is continuous and strictly concave in n (Ravikumar et al., 2011), if follows that
S+
the maximizer is unique. Taking the gradient of f with respect to Θ yields
∂f(Θ;V)
=2Θ −1 Θ −1 I 2V V I=000 Θ=V −1. (SM32)
∂Θ − ◦ − − ◦ ⇐⇒
Hence, f(Θ;Θ 0−1)ismaximizedwhenΘ=Θ 0. Furthermore, Θ
0
satisfiestheconstraintin(SM30). Asaresult,
it holds that
h(cid:0)
Θ
−01(cid:1)
=Θ 0. (SM33)
Notice that the mapping h is continuous. This can be proven by means of the maximum theorem (Sundaram,
1996). Let = V Rn n V = 0 (i,j) : A = 0 be the set of constrained matrices we are interested in.
C { ∈
×
|
ij
∀
Oij
}
Any linear combination of matrices in is still in , and thus is a convex set. Therefore, since n is convex as
C C C S+
well, the intersection = n (which is the set over which f is maximized in (SM30) to compute h) is convex
I C∩S+
too.
The function f is continuous and strictly concave in , since n. Hence, by the maximum theorem
I I ⊆ S+
under convexity (Sundaram, 1996), the argmax mapping of f(Θ;V) within is a continuous function of V.
I
Consequently, h is a continuous mapping.
Consider that by the law of large numbers,
k
S= k1 (cid:88) x ix ⊤i −k −→ −−∞ →E(cid:2) xx ⊤(cid:3) =Θ −01. (SM34)
i=1
Consequently, by the continuous mapping theorem (which can be applied because h is continuous), consider-
ing (SM34), (SM31) and (SM33), we conclude that
Θˆ =h(S) −k −→ −−∞ →h(cid:0) Θ −01(cid:1) =Θ 0. (SM35)Estimation of partially known Gaussian graphical models with score-based structural priors
F.2 Proof of Lemma 2
Inordertostudyhowpˆ(A X)behavesask ,wefirstanalyzethelikelihoodfunction (Θ). UsingBayes’
X
| →∞ L
theorem, we get
p(Θ X) (Θ)p(Θ). (SM36)
X
| ∝L
k
By the Bernstein–von Mises theorem, we know that p(Θ X) →∞ δ(Θ Θ 0), where δ() is the Dirac delta.
| −−−−→ − ·
Combining this with (SM36) it follows that
k
X(Θ) →∞ 0 Θ=Θ 0. (SM37)
L −−−−→ ∀ ̸
The result in (SM37) holds for any prior p(Θ), since the prior can be considered a constant with respect to k.
Given (SM37) and the consistency of Θˆ in Lemma 1, we conclude that [cf. (10) in the main paper]
k
p(A)
pˆ(A X) →∞ δ(Θ 0 (A+I) Θ 0), (SM38)
| −−−−→ C ◦ −
where C is a normalization constant.
G EXPERIMENTAL DETAILS
G.1 Hyperparameters
In terms of the Langevin sampler, in all the experiments we use L = 10 noise levels, evenly spaced between
σ =0.5 and σ =0.03, and T =300 steps per level. We set the step size at ϵ=10 6.
1 L −
Inordertotunetheregularizationparameterλforgraphicallasso,wefitafunctionoftheformλ(k)=alog(k)2+
blog(k)+c using a training set, and then evaluate that function at inference time for the given value of k. We
selectedthisspecificfunctionalformasitshowedaverysatisfactoryfitforallofourcases. Anexampleisshown
in Figure SM3.
k=25 0.045 Data
0.8 k=56 Fittedcurve
k=128 0.040
k=291
0.7 0.035
k=661
k=1500
0.030
0.6
0.025
0.5
0.020
0.4 0.015
0.010
10−3 10−2 10−1 102 103
λ k
FigureSM3: Fittingofλ(k)=alog(k)2+blog(k)+cfortheego-netsdatasetwith =0.5dim(a). Theorange
|U|
curve on the right is the one used at inference time.
The thresholds used to obtain a binary-valued estimate of A when using GraphSAGE, the thresholding of Θˆ
0
andourownmethod(inthislastcase,thethresholdisτ )aresetusingthesameprocedure. GivenRsimulations
k
(in all of our experiments, R=100), we take R/2 of those continuous estimated matrices and find the threshold
within some grid that works the best for each method. Then, with those tuned values, we threshold the other
T
R/2 matrices left and only evaluate performance over them. All the plots shown in Section 4 from the original
erocS1F λlamitpOMart´ın Sevilla, Antonio G. Marques, Santiago Segarra
Algorithm SM2 Metric computation
Require: a ,a˜ R , , S
{
Ur Ur}r=1
T
1: J 0
←
2: for s 1toS do ▷ Repeat the process with S different splits
←
3: Store R/2 randomly chosen tuples (a ,a˜ ) into
Ur Ur Atrain
4: Store the other R/2 tuples in
test
A
5: for τ do ▷ Get the best threshold in the grid
6:
Co∈ mT
pute metric(a
,I(cid:8)
a˜
τ(cid:9)
) for each tuple in
Ur Ur
≥
Atrain
7: If the average train metric is the best so far, store the current threshold in τ⋆
8: end for
9: Compute metric(a
,I(cid:8)
a˜
τ⋆(cid:9)
) for each tuple in
Ur Ur
≥
Atest
10: Store the average of the test metrics in J
s
11: J J +J
s
←
12: end for
13: return J ▷ Return the mean across splits
S
paper as well as in this document correspond to averages following this procedure S times (i.e., across S = 10
different train/test splits in our case), each with its own threshold. A more clear description of our method is
provided in Algorithm SM2.
G.2 Datasets’ details
For the grids, we generated graphs with 40 n 50. Additionally, to introduce randomness into the graphs, we
≤ ≤
addedbetween2and5edgesuniformlyatrandomtotheedgeset . WetrainedaGNNwith =5000graphs
E |A|
generated with the described procedure.
InthecaseofthedualBarab´asi–Albertmodel,thegenerationisinitializedwithanemptygraphwithmax(n ,n )
1 2
nodes. Then, the remaining n max(n ,n ) vertices are added iteratively. For each new node, n edges are
1 2 1
−
addedwithprobabilityπ,andn edgesareaddedwithprobability1 π. Edgesareaddedfollowingapreferential
2
−
attachment criterion. In our case, we set n = 2, n = 4 and π = 0.5. We simulated = 1000 graphs in this
1 2
|A|
setting.
Regarding the ego-nets, we used = 2926 of the graphs available in the whole dataset (i.e., only those such
|A|
that n 25) to train the EDP-GNN, leaving 100 graphs for testing.
≤
G.3 Implementation details
All experiments shown both in the main paper as well as in this document, together with the training of the
EDP-GNNs, were run on an NVIDIA DGX A100 system. Our code is provided as part of the supplementary
material and not in a GitHub repository for the sake of anonymity. The code corresponding to the EDP-GNN
implementation was taken from the original repository of Niu et al. (2020) under a GPL-3.0 license.
Regarding the computational complexity, the key steps to consider from Algorithm 1 are those in lines 2, 11,
and 12. The computation time of Θˆ is (n3) for dense problems, but much less in sparse ones (Friedman et al.,
O
2008),andevenlessifsomeoftheentriesarefixedasisthecaseof (9). However,hereweconsidertheworst-case
scenario. After computing Θˆ, we evaluate (27), which requires to compute the inverse of Θ˜, leading to a time
complexity of (n3) as well. Feed-forwarding the GNN consists of just matrix and vector multiplications that
O
scale as (n2). Therefore, the dominating steps are the first two. The step in 11 is repeated L T times per
O ·
sample, and since the sampling of each A is completely parallelizable, the total complexity of Algorithm 1 is
i
(L T n3), considering that typically LT 1.
O · · ≫
H ADDITIONAL EXPERIMENTS
In this section, we provide more experiments to gain additional insights on our method.Estimation of partially known Gaussian graphical models with score-based structural priors
H.1 Performance dependence on
|A|
We want to analyze how the predictive power of the prior learned by the EDP-GNN changes when different
dataset sizes are used for training. To this end, we use a different family of graphs, known as exponential
|A|
randomgraphmodels (ERGMs),whichhasaclosed-formdistribution(uptoanormalizationconstant)thatrelies
on a set of network statistics and parameters (Hunter and Handcock, 2006; Snijders et al., 2006). Namely, for a
vector of r statistics ψ(A)= (cid:2) ψ 1(A) ψ 2(A) ψ r(A)(cid:3) ⊤ and a set of parameters β Rr, the distribution
··· ∈
of A is given by
1 (cid:16) (cid:17)
p(A)= exp β ⊤ψ(A) . (SM39)
C (β)
ψ
The statistics can be, e.g., the number of triangles in the graph, the number of d-stars, or the number of edges,
among many others. In the simulations, we consider an ERGM distribution with statistics
ψ(A)=(cid:2)
AKS γ(A)
21(cid:80)
ijA
ij(cid:3)
⊤. (SM40)
The first one corresponds to the alternated d-stars statistic (Hunter and Handcock, 2006), defined as
p 1
AKS
(A)=(cid:88)−
(
1)dS d(A)
, (SM41)
γ − γd 2
d=2 −
with S () being the number of d-stars in the given graph and γ a constant. In our simulations we set γ =0.3.
d
· (cid:2) (cid:3)
Thesecondstatisticcorrespondstothenumberofedges . Weuseβ = 0.7 2 ⊤ ascoefficients. Thegraphs
|E| −
we generate have n = 50 nodes, and we remove = 30 values from A . We drop zeros and ones with equal
0
|U|
probability, so that we can use accuracy instead of F1 score for this experiment. We generated = 1000
|A|
graphs from this distribution to train the EDP-GNN and, to evaluate the impact the dataset size has on
|A|
the estimation performance, we compare the edge prediction accuracy when fewer training samples are used.
The reported accuracies correspond to the average over 25 runs, following the same procedure described in
Section G.1.
0.9
LPost(1000)
0.8 LPost(500)
LPost(300)
LPost(250)
0.7 LPost(150)
GLasso
Threshold
0.6
TIGER
GraphSAGE
0.5
25 100 350 1300
k
Figure SM4: Prediction accuracy of several methods using ERGM graphs with = 30. The comparison
|U|
includes five versions of Algorithm 1, each considering a prior that was learned with datasets of different sizes
A
(the value of is indicated in the legend), as well as three benchmarks available in the literature.
|A|
Figure SM4 shows that the accuracy improves drastically as increases, as expected. However, this enhance-
|A|
ment seems to saturate as the dataset size approaches = 1000. All the other GGM estimation methods
|A|
perform similarly, logically presenting a higher prediction accuracy as k increases. Notably, whenever 300,
|A|≥
all other methods underperform our algorithm .
GraphSAGE performs rather poorly – it predicts 0 or 1 uniformly at random. Additionally, the incorporation of
additional observations does not increase its accuracy. As discussed in detail in the main paper, this behavior is
ycaruccanoitciderpegdEMart´ın Sevilla, Antonio G. Marques, Santiago Segarra
expected and associated with the fact of GraphSAGE aggregating data using local neighborhoods, which is not
a good fit for the setup at hand.
H.2 Performance dependence on
|U|
In order to understand how impacts the estimation performance, we run an additional experiment that
|U|
complements the results presented in Section 4 when using grid graphs with different . We use the test set
|U|
of the ego-nets and fix k = 100, varying the percentage of unknown values in a. We run the simulations using
three versions of Algorithm 1:
• Posterior (LPost). Our original approach, implementing all the steps in Algorithm 1.
• Prior (LPr). This simplified version of Algorithm 1 only exploits prior information. This corresponds to
running Algorithm 1 where in line 11 we set log (Θ˜ )=0.
X t
∇ L
• Likelihood (LL).ThissimplifiedversionofAlgorithm1omitsthelearnedpriorandusesonlytheobserva-
tionsX. Morespecifically,thisentailstorunaversionofAlgorithm1whereinline13wesetg(A˜ ,σ )=0.
t 1 l
−
In this experiment, we use the AUC score in lieu of F1 to assess the prediction performance. This allows us to
avoidthetuningofτ ,whichisnotneededforthisanalysissincewearecomparingthreeversionsofAlgorithm1
k
that return continuous predictions if no thresholding is applied. Results are shown in Figure SM5.
The main observations are: i) the information provided by decreases as increases; and ii) LPost always
A |U|
outperforms LPr and LL. Both are expected results that were observed in previous experiments. Analyzing
more specific details, we note that the performance of LPr drops rapidly as increases, while that of LPost
|U|
decaysmoreslowly. ThesameistrueforLL,whoseAUClevelsareaquitestableanddecayataveryslowpace.
Finally, it is worth noticing that when a large portion of the graph is unknown, LPost approaches LL, since the
information provided by becomes less useful.
|A|
0.9
0.8
0.7
0.6
LPost
LPr
LL
0.5
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
/dim(a)
|U|
Figure SM5: AUC score achieved by different versions Algorithm 1 when using an ego-net prior and k = 100
observations. Thehorizontalaxisrepresentsdifferentvaluesof . Forthisexperimentweskipthethresholding
|U|
implemented in line 24 of Algorithm 1.
erocS
CUA