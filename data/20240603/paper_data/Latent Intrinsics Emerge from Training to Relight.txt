Latent Intrinsics Emerge from Training to Relight
XiaoZhang1 WilliamGao1 SeemandharJain2 MichaelMaire1
D.A.Forsyth2 AnandBhattad3
1UniversityofChicago 2UniversityofIllinoisUrbanaChampaign
3ToyotaTechnologicalInstituteatChicago
zhang7@uchicago.edu bhattad@ttic.edu
(Zoomintochromeball)
Input Reference Relight Input Reference Relight Input Reference Relight
Input Albedo Input Albedo Input Albedo
Figure 1: We describe a purely data-driven image relighting model. Our model recovers latent
variablesrepresentingsceneintrinsicpropertiesfromoneimage,latentvariablesrepresentinglighting
fromanother,thenappliesthelightingtotheintrinsicstoproducearelightedscene(toprow). There
isnophysicalmodelofintrinsics,extrinsicsortheirinteraction. Ourmodelrelightsimagesofreal
sceneswithSOTAaccuracyandismoreaccuratethancurrentsupervisedmethods. Notehow,for
the chrome ball detail in top center, the specular reflections on the chrome ball (which give an
approximateenvironmentmap)changewhentheextrinsicsarechanged.Notehowourmodelascribes
lightingtovisibleluminaireswhenitcan(topright),despitetheabsenceofanyphysicalmodel. A
physicalmodelaccountsonlyforeffectsinthatmodel,andmostphysicalmodelsofsurfacesare
approximate;incontrast,alatentintrinsicmodelaccountsforwhateverproducessubstantialeffects
intrainingdata. Latentintrinsicsyieldalbedoinanaturalfashion(lightthescenewithanappropriate
illuminant). BottomrowshowsSOTAalbedoestimatesrecoveredfromourlatentintrinsics.
Abstract
Imagerelightingisthetaskofshowingwhatascenefromasourceimagewould
looklikeifilluminateddifferently. Inversegraphicsschemesrecoveranexplicit
representationofgeometryandasetofchosenintrinsics,thenrelightwithsome
formofrenderer.Howevererrorcontrolforinversegraphicsisdifficult,andinverse
graphicsmethodscanrepresentonlytheeffectsofthechosenintrinsics. Thispaper
describes a relighting method that is entirely data-driven, where intrinsics and
lightingareeachrepresentedaslatentvariables. OurapproachproducesSOTA
relightingsofrealscenes,asmeasuredbystandardmetrics. Weshowthatalbedo
canberecoveredfromourlatentintrinsicswithoutusinganyexamplealbedos,and
thatthealbedosrecoveredarecompetitivewithSOTAmethods.
Preprint.Underreview.
4202
yaM
13
]VC.sc[
1v47012.5042:viXra1 Introduction
Relighting–takinganimageofascene,thenadjustingitsoitlooksasthoughithadbeentaken
underanotherlight–hasarangeofapplications,includingcommercialart(e.g.,photoenhancement)
anddataaugmentation(e.g.,makingvisionmodelsrobusttovaryingillumination). Asatechnical
problem,relightingisveryhardindeed,likelybecausehowascenechangesinappearancewhen
thelightischangedcandependoncomplexsurfacedetails(groovesinscrews;barkontrees;wood
grain)thatarehardtocaptureeitheringeometricorsurfacemodels.
Onecommonstrategytorelightasceneistoinferscenecharacteristics(geometry,surfaceproperties)
usinginversegraphicsmethods, thenrenderthescenewithanewlightsource. Thisapproachis
fraught with difficulties, including the challenge of selecting which material properties to infer
andmanagingerrorpropagation. Thesemethodsperformbestinoutdoorsceneswithsignificant
shadowmovementsbutstrugglewithindoorsceneswhereinterreflectionscreatecomplexeffects
(Section4.2).
Asthispaperdemonstrates,apurelydata-drivenmethodoffersanattractivealternative. Asource
scene (represented by an image) is encoded to produce a latent representation of intrinsic scene
properties. A source illumination (represented by another image) is encoded to produce a latent
representationofilluminationproperties. Theseintrinsicandextrinsicpropertiesarecombinedand
thendecodedtoproducetherelightedimage. Asabyproductofthistraining,wefindthatthelatent
representationofintrinsicscenepropertiesbehaveslikeanalbedo,whileanotherlatentrepresentation
actsasalightingcontroller.
Our model can capture complex scene characteristics without explicit supervision by capturing
intrinsicpropertiesaslatentphenomena,makingitparticularlyappealing. Incontrasttoaphysical
model,wearenotrequiredtochoosewhicheffectstocapture. Thislatentapproachreducestheneed
fordetailedgeometricandsurfacemodels,simplifiesthelearningprocess,andenhancesthemodel’s
abilitytogeneralizetodiverseandunseenscenes. Thismakesithighlyapplicabletoawiderangeof
real-worldscenarios.
Contributions: Wepresentthefirstfullydata-drivenrelightingmethodapplicabletoimagesofreal
complexscenes. Ourapproachrequiresnoexplicitlightingsupervision,learningtorelightusing
pairedimagesalone. Wedemonstratethatthismethodeffectivelytrainsandgeneralizes,producing
highlyaccuraterelightings. Furthermore,wedemonstratethatalbedo-likemapscanbegenerated
from the model without supervision or prior knowledge of albedo-like images. These intrinsic
propertiesemergenaturallywithinthemodel. Wevalidateourmodelonaheld-outdataset,applying
targetlightingconditionsfromvariousscenestoassessitsgeneralizationcapabilityandprecisionin
real-worldscenarios(Section4.2).
2 RelatedWork
IntrinsicImages. Humanshavebeenknowntoperceivescenepropertiesindependentoflighting
sinceatleast1867[45,20,4,19].Incomputervision,theideadatestoBarrowandTenenbaum[3]and
comprisesatleastdepth,normal,albedo,andsurfacematerialmaps. Depthandnormalestimationare
nowwellestablished(eg[23]).Thereisarichliteratureonalbedoestimation(datingto1959[29,30]!).
Adetailedreviewappearsin[15],whichbreaksoutmethodsastowhatkindsoftrainingdatathey
see. Early methods do not see any form of training data, but more recently both CGI data and
manualannotationsofrelativelightness(labels)havebecomeavailable. Earlyefforts,suchSIRFS[1],
focusedonusingshadinginformationtorecovershape,illumination,andreflectance,highlighting
theimportanceofmodelingthesefactorsforintrinsicimageanalysis. Recentstrategiesinclude: deep
networkstrainedonsyntheticdata[32,22,14];andconditionalgenerativemodels[27].
Theweightedhumandisagreementratio(WHDR)evaluationframeworkwasintroducedby[5]using
theIIWdataset. Thisisadatasetofhumanjudgmentsthatcomparetheabsolutelightnessatpairs
ofpointsinrealimages. Eachpairislabeledwithoneofthreecases(firstlighter;secondlighter;
indistinguishable)andaweight,whichcapturesthecertaintyoflabelers. Oneevaluatesbycomputing
a weighted comparison of algorithm predictions with human predictions; WHDR scores can be
improvedbypostprocessingbecausemostmethodsproducealbedofieldswithveryslowgradients,
ratherthanpiecewiseconstantalbedos. [9]demonstratethevalueof“flattening”albedo(seealso
[38]);[10]employafastbilateralfilter[2]toobtainsignificantimprovementsinWHDR.
2UsingIntrinsicImagesforRelighting. BhattadandForsyth[6]demonstratedthatintrinsicimages
could be used for reshading inserted objects. This approach can be extended by adjusting the
shadinginboththeforegroundandbackgroundtoeliminatediscrepancies[11]. Intrinsicimagesand
geometry-awarenetworkshavebeenusedformulti-viewrelighting[40]. StyLitGAN[8]introduced
amethodtorelightimagesbyidentifyingdirectionalvectorsinthelatentspaceofStyleGAN,butcan
onlyrelightStyleGANgeneratedimagesandrequiresexplicitalbedoandshadingtoguiderelighting.
LightIt[28]controlslightingchangesinimagegenerationusingdiffusionmodels,byconditioning
onshadingandnormalmapstoachieveconsistentandcontrollablelighting. Likethesemethods,we
useintrinsicsandextrinsicstorelight,butoursarelatent,withnoexplicitphysicalmeaning.
ColorConstancy. Imagecolorisambiguous: agreenpixelcouldbetheresultofawhitelighton
agreensurface,oragreenlightonawhitesurface. Humansareunaffectedbythisambiguity(eg
[20,4];recentreviewin[49]). Thereisextensivecomputervisionliterature;arecentreviewappears
in[31]. Wedonotestimateilluminationcolorbutestimateasinglecolorcorrection(Section4.2).
LightingEstimationandRepresentation. Accuratelightingrepresentationiscrucialfortaskslike
objectinsertionandrelighting. Traditionalmethodsusedparametricmodelssuchasenvironment
maps and spherical harmonics to represent illumination [12, 41]. Debevec’s seminal work [12]
on recovering environment maps from images of mirrored spheres set the foundation for many
subsequentworks. MethodsbyKarschetal.[25,26],Gardneretal.[16,17],Garonetal.[18]and
Weberatal.[48]advancedthefieldbyusinglearnedmodelstorecoverparametric,semi-parametric
orpanoramicrepresentationsofillumination. Recentapproachesincluderepresentingillumination
fieldsasdense2Dgridsofsphericalharmonicsources[33,35]orlearning3Dvolumesofspherical
Gaussians[47]. Thesemethodscanmodelcomplexlight-dependenteffectsbutrequireextensive
CGIdatasetsfortraining[42,34]. Ourapproachdivergesbynotrelyingonlabeledillumination
representationsorCGIdata,insteadproducingabstractrepresentationsofilluminationthroughdeep
featureswithoutspecificphysicalinterpretations.
Image-basedRelighting. Otherworksfocusonportraitrelightingusingdeeplearning[44,54,39,
43],whicharetypicallyspecializedtofacesandtrainedonpairedorlight-stagedata. Self-supervised
methodsforoutdoorimagerelightingleveragesingle-imagedecompositionwithparametricoutdoor
illumination,benefitingfromsimplerlightingconditionsdominatedbyskyandsunlight[53,36]. [21]
introducedaself-attentionautoencodermodeltore-renderasourceimagetomatchtheilluminationof
aguideimage,focusingonseparatingscenerepresentationandlightingestimationwithaself-attention
mechanismfortargetedrelighting. Similarly, [50]proposedadepth-guidedimagerelighting,which
combinessourceandguideimagesalongwiththeirdepthmapstogeneraterelitimages. Incontrast,
ourworkshowsthatintrinsicpropertiesrelevanttorelightingcanemergenaturallyfromtrainingto
relight,facilitatingcomplexscenerelightingwithouttheneedforexplicitlightingestimation. We
comparewithboth [21]and [50]forrelightingcapabilitiesonrealscenes.
Emergent Intrinsic Properties. Bhattad et al. [7] and Du et al. [13] demonstrate that intrinsic
imagescanbeextractedfromgenerativemodelsusingasmallintrinsicimagedatasetobtainedfrom
pretrainedoff-the-shelfintrinsicimagemodels. Ourworkexploreshowintrinsicimageproperties
emergeasaresultoftrainingamodelforrelighting,withouttheneedforanintrinsicimagedataset.
3 LearningLatentIntrinsicfromRelighting.
Ourrelightingmodelcanbeseenasaformofautoencoder. Oneencodercomputesalatentrepresen-
tationofsceneintrinsicsfromanimageofatargetscene;anothercomputesalatentrepresentationof
sceneextrinsicsfromanimageofaplaceholdersceneinthereferencelighting. Thesearecombined,
then decoded into a final image of the target scene in the reference lighting. Losses impose the
requirementsthat(a)thefinalimageisrightand(b)thelatentintrinsicscomputedforascenearenot
affectedbyillumination. Theprocedureforcombiningintrinsicsandextrinsicsiscarefullydesigned
tomakeitverydifficultforintrinsicfeaturesoftheplaceholdersceneto“leak”intothefinalimage.
3.1 Modelstructure
Decoder setup: Write Il P RHˆWˆ3 for the input image, captured from scene s with lighting
s
configurationl. TrainingusespairsIl1 andIl2,representingthesamescenesunderdifferentlighting
s s
conditionsl andl . Themodeldoesnotseedetailedlightinginformation(forexample,theindexof
1 2
3Constrained Scaling
Inference time:
E Ll s2 0<α≪1 relight with arbitrary I sl 2
Zoom In 1+α⋅tanh(MLP(Lsl2))
I sl 2
Share {Sl s2 ,i}i I sl 1 I sl 22 ˜Il s1 1→l 2
Parameters D
Inference time:
˜Il s1→l 2 1+0⋅tanh(MLP(Lsl1)) estimate albedo for free
Ll s1
E
I sl 1
{Sl s1 ,i}i
I sl 1 ˜Il s1→l1(α=0)
Figure2: Thenetworkdiagramofourrelightingmodel. Themodelfunctionsasanautoencoder,
comprisinganencoderEandadecoderD.LeftHalf:TheencoderEmapsinputimageIl,captured
s
underscenesandlightingl,tolow-dimensionalextrinsicfeaturesLl andsetofintrinsicfeatures
s
map tSl u . The decoder D then generates new images based on these intrinsic and extrinsic
s,i i
representations. Right Half: We employ constrained scaling for the injection of Ll, utilizing
s
0 ă α ! 1 to regularize the information passed from Ll, thereby enforcing a low-dimensional
s
parameterizationoftheextrinsicfeatures. Wetrainoursystemtorelighttargetimagesgiveninput
pairedofimagecapturedunderthesamescenes. Duringinference,oursystemdemonstratesthe
capabilitytogeneralizetoarbitraryreferenceimagesforrelightingandcanestimatealbedoforfree.
thelighting)duringtraining,becausestandardizinglightingsettingsacrossvariousscenesisoften
impractical.
WriteE fortheencoder,Dforthedecoder. Theencodermustproducetheintrinsicandextrinsic
representationsfromtheinputimage. WriteSl PRpHiˆWiqˆCi forspatialfeaturemapsyielding
s,i
theintrinsicrepresentation,withiforthelayerindex,andLl PRC forextrinsicfeatures;wehave:
s
EpIlq:“tSl u ,Ll (1)
s s,i i s
We apply L2 normalization along the feature channel to both sets of features. During training,
weintroducerandomGaussiannoisetotheinputimagetoenhancesemanticsceneunderstanding
capabilities:
EpIl `σϵq:“tSl u ,Ll (2)
s s,i i s
Decodersetup: ThedecoderDrelightsIl1 usingextrinsicfeaturesextractedfromIl2:
s s
DptSl1u,Ll2q:“Irl1Ñl2
(3)
s s s
Weoptimizetheautoencoderusingapixel-wiselossonbothrelightedandreconstructedimages:
L :“L
pIrl1Ñl2,Il2q`L pIrl2Ñl2,Il2q
(4)
relight pixel s s pixel s s
whereL representsthepixel-wiselosses:L2distanceonpixels;structuralsimilarityindex(SSIM)
pixel
[46];andl2distanceonimagespatialgradient(weights10,0.1and1respectively).
3.2 Intrinsicness
Intrinsicness: Our model should report the same latent intrinsic for the same scene in different
lightings,soweapplythefollowinglosstotheencoder:
ÿ
L :“ }Sl1 ´Sl2} `1e-3¨L pSl1q (5)
intrinsic s,i s,i 2 reg s,i
i
whereL isaregularizationtermonintrinsicfeatures,definedasfollows:
reg
p
L pSq :“ }RpSq´RpSq} (6)
reg ˆ 2 ˙
d
RpSq :“ logdet I` SJS (7)
nλ2
4whereRpSqisthecodingrate[52]foramatrixS P Rnˆd witheachrowl2normalized,undera
p p
distortionconstantλ. S isarandommatrixwiththesameshapeofS andeachrowofS issampled
p
from uniform hyperspherical distribution at the start of learning. In Eqn.5, RpSq serves as the
optimizationtargetofRpSqtoencouragetheS touniformlyspreadoutinthehypersphericalspace.
Thisstrategyisnowwidelyusedinself-supervisedlearning;withouttheregularizationterm,the
modelcanminimizethefeaturedistancebysimplycollapsingthedistributionofSl withsmall
s,i
variance,whichwillnotyieldeffectivelightinginvariance.
3.3 Combiningintrinsicsandextrinsics
Theplaceholdersceneisnecessarytocommunicateilluminationtothemodel, buthasimportant
nuisancefeatures. Intrinsicinformationfromthisscenecould“leak”intothefinalimage,spoiling
results. We introduce constrained scaling, a structural bottleneck that restricts the amount of
informationtransmittedfromthelearnedextrinsicfeatures.
WriteF PRhˆwˆcforthefeaturemapfedtothedecoder. Constrainedscalingcombinesintrinsic
andextrinsicfeaturesby
` ` ` ˘˘˘
Fr :“F d 1`α¨tanh MLP Ll (8)
s
where MLP, a series of fully connected layers with non-linear activation, aligns Ll to the latent
s
channeldimensionofF andα!1isasmallnon-negativescalar(weuse5e-3).Thisapproachmeans
thatanysingleextrinsicfeaturevectorhaslittleeffectonthefeature–foraneffect,theextrinsics
mustbepooledovermultiplelocations. Illuminationfieldstendtobespatiallysmooth,supporting
theinsightthatenforcedpoolingisagoodidea.
Constrainedscalingcompresseslatentvectorsintoaverysmallnumericalrange,makinglearning
difficult. WeusearegularizertopromoteauniformdistributionofLl,whichimprovesoptimization.
s
Inparticular,wehave
L :“L pLlq (9)
extrinsic reg s
Bychoosingα!1andtrainingmodelwithuniformregularizationtermEqn.9,weeffectivelypush
thelightingcodetouniformlyspreadover[´α,α]wheretheabsolutevalueofeachchannelindicates
thestrengthofthelight. Asasideeffect,bysettingα“0todisablethecontributionofthelighting
code,wegetimagealbedoestimationfromourmodelforfree.
Ourfinaltrainingobjectiveisweightedcombinationofallindividuallossterms:
L:“L `1e-1¨L `1e-4¨L (10)
relight intrinsic extrinsic
4 Experiments
4.1 ExperimentDetails
TrainingDetailsWetrainourmodelusingtheMITmulti-illuminationdataset[37],whichincludes
images of 1,015 indoor scenes captured under 25 fixed lighting patterns, totaling 25,375 images.
Wefollowtheofficialdatasplitandtrainourmodelonthe985trainingscenes. Duringtraining,
werandomlysamplepairsofimagesfromthesamesceneunderdifferentlightingconditionsand
performrandomspatialcropping,withthecropratiorandomlyselectedbetween0.2and1.0,followed
byresizingthecroppedimagetoaresolutionof256x256. Forfurtherdetails,pleaserefertoour
appendix.
4.2 Evaluatingimagerelighting
RelightingontheMulti-illuminationdataset: Werelightimagesofscenesinthetestsetusing
referenceimagesfromthetestset,thencomparetothecorrectknownrelightingfromthetestset
usingvariousmetrics. Foreachinputimage,werandomlysamplereferenceimagesfromdifferent
scenesandlightingconditions. Toreducetheeffectofrandomnessincomparingdifferentrelighting
strategies, we select 12 random reference images for each input image, and maintain the same
image-referencepairswhenevaluatingdifferentmodels. Wereporttheresults,measuredinRMSE
5SAAE SAAE S3Net Ours SAAE SAAE S3Net Ours
Input Ref sup unsup Depth unsup Target Input sup unsup Depth unsup Target
Figure3: Ourmethodsurpassesallotherapproachesinestimatinglightandrenderingthescene. The
UnsupervisedSA-AE[21]methodfailsbyincorporatingintrinsicelementsfromreferenceimages.
TheS3Net[50]approachstruggleswithrenderingwhenusingunpairedreferenceimages. Right: A
zoomed-inviewofthechromeballusedasaprobetoevaluatedetailpreservationintheenvironment
map. Ourmethodeffectivelyretainstheintricateroomlayoutandaccuratelyrenderstheappropriate
lightingpatterns.
Methods Labels RawOutput ColorCorrection α RawOutput ColorCorrection
RMSEÓ SSIMÒ RMSEÓ SSIMÒ RMSEÓ SSIMÒ RMSEÓ SSIMÒ
InputImg - 0.384 0.438 0.312 0.492 8 0.471 0.287 0.352 0.407
SA-AE[21] Light 0.288 0.484 0.232 0.559 1e-2 0.314 0.444 0.238 0.546
SA-AE[21] - 0.443 0.300 0.317 0.431 5e-3 0.297 0.473 0.222 0.571
S3Net[50] Depth 0.512 0.331 0.418 0.374 1e-3 0.312 0.453 0.256 0.524
S3Net[50] - 0.499 0.336 0.414 0.377 5e-4 0.309 0.460 0.253 0.533
Ours(σ“0) - 0.326 0.232 0.242 0.541
Ours - 0.297 0.473 0.222 0.571
Table1: Weassessthequalityofimagerelightingusing Table 2: We analyze the impact of α
the multi-illumination dataset [37]. Our method, when on relighting quality using the multi-
evaluated on raw output, significantly outperforms all illuminationdataset[37].Settingαto8,
otherunsupervisedapproachesandachievescompetitive which removes the scaling constraints,
results compared to the supervised SA-SA [21], which results in poor relighting quality, indi-
requiresgroundtruthlightsupervision. Whenwecorrect catingthatrestrictinginformationfrom
thecolorsbyeliminatingglobalcolordriftcausedbylight extrinsicsourcessignificantlyimproves
ambiguity, our method surpasses all other approaches. generationquality. Withinalimitedpa-
Additionally, warming up the model as a denoising au- rametersearch, 5e-3yieldsthebestre-
toencoderprovesbeneficialcomparedtowhenitisnot sults.
warmedup(σ “0).
andSSIM,inTable1. Wereportthesemetricsbothforabsolutepredictionsandforpredictions
whereanyglobalcolorshiftiscorrectedbyasingle,least-squaresscaleofeachpredictedcolorlayer
(i.e. onescaleforR;oneforG;oneforB).Thiscolorcorrectionallowsustodistinguishbetween
spatialerrorsandglobalcolorshifts;theseappeartohaveasignificanteffect,possiblybecausethere
arevisiblecolorshiftspresentinsomeofthedatasetimages.
InTable1,wecomparetoSA-AE[21],amodelthatrequiresagroundtruthlightindexforsupervision,
andS3-Net, whichneedsagroundtruthdepthmapasaconditionalinput. ForS3-Net, weusea
state-of-the-art depth estimator to provide pseudo-GT on the relighting dataset as input. For a
faircomparisontoourmodel,whichdoesnotrequireanysupervisionoutsideofthegroundtruth
relighting,wealsoreportresultsformodifiedversionsofthebaselinestrainedwithoutadditional
supervision. ForSA-AE,wetraintheirlightestimationmodelandrelightingmodelend-to-endby
removingthelossfromlightsupervision. ForS3-Net[50],wesimplyremovethedepthfromthe
model’sinput.
6Figure 4: Latent extrinsics can be interpolated successfully; leftmost and rightmost columns
are images from the multi-illumination dataset, and intermediate images are obtained by linear
interpolationonthelatentextrinsics(light-dependentrepresentations),thendecoding. Notehowthe
lightseemsto"move"acrossspace.
Withoutcolorcorrection,onlylight-supervisedSA-AEslightlyoutperformsourmodel,whileall
otherbaselinesaresignificantlyworse. TheunsupervisedversionofSA-AEperformsmuchworse
because their light estimator struggles to distinguish the extrinsic from the intrinsic components.
Specifically, SA-AE also parameterizes the extrinsic as a lower-dimensional representation but
withouttheconstrainedscalingthatourmodeluses. Asaresult,theestimatedextrinsicfromtheir
unsupervisedmodelalsocarriesintrinsicinformation,andonecansee“leaks”. S3-Netperforms
worseinbothversionssincetheyconcatenateinputandreferenceimagesbeforefeedingtheminto
themodels,whichsignificantlyaffectsthemodel’sgeneralizationability,especiallyduringtesttime
whenweuseimagesfromdifferentscenesasreferences.
Oncolor-correctedimages,ourapproachoutperformsallmethods,includingthelight-supervised
versionofSA-AE,indicatingthat,uptotheconstantcolordrift,ourextrinsicestimationnetworkisat
leastasgoodas,orevenbetterthan,alightestimationnetworktrainedwithsupervision. Removing
thedenoisingsetupfromourmodel(σ “0)resultsinworseperformanceinbothcasesduetoinferior
semanticsceneunderstanding. WeadditionallyprovideablationstudiesonthechoicesofαinTable2
andfindα“5e´3producesthebestresults.
Eachimageinthemulti-illuminationdatasetshowsachromeball,whichgivesagoodestimateofan
environmentmapforthatimage. Correctlyrenderingtheeffectsoflightingchangesonthesechrome
ballsappearstobeextremelydifficult;thechangesaresubstantial,andconcentratedinasmallregion
oftheimage(socorrectrepresentationofthesechangeshaslittleeffectontypicalimagelosses).
Figure3showsacropofourresultsaroundthischromeball. Ourmethodrepresentsthesechanges
well;weareawareofnootherresultsreportedforthiseffect. Comparedtootherapproaches,our
modelaccuratelypreservestheroomlayout,evenincasesofextremelightchanges.
Unlikeclassicalrenderingmodelsthatuseaspecificparameterizedformtorepresentextrinsics,our
frameworklearnsanimplicitextrinsicrepresentation. However,wecanstillparameterizethelearned
extrinsicrepresentationtocreatenewlightsources. InFigure4,wedemonstratethiscapabilityby
renderingimagesusinginterpolatedextrinsicrepresentations.
RelightingsyntheticallyrelightedimagesfromStyLitGAN:StyLitGAN[8]isarecentmethod
thatcanproducemultipleilluminationsofasinglegeneratedroomscenebymanipulatingStyleGAN
latentsappropriately. Inthemulti-illuminationdataset,referencelightandtargetimagestendtoshare
astrongspatialcorrelationinlightpatterns. Incontrast,StyLitGANgeneratesextremelychallenging
images where very significant changes in lighting occur. Furthermore, StyLitGAN images have
visibleluminaires. Torelighttheinput,themodelmustinferhigh-levelconceptsratherthansimply
copying the spatially corresponding light patterns from the reference. We train our model using
StyLitGAN images to evaluate generalization qualitatively (quantitative evaluation would be of
dubiousvalue,becauseStyLitGANimagesaregeneratedratherthanreal). Figure5showsresults.
Notice how our method successfully relights from references, achieves brighter illuminationsby
turningonluminaires(herebedsidelights),achievesdarkerscenesbyturningoffluminaires,andis
somewhatlessinclinedtoinventluminairesthanStyLitGANis. Themodelknowsthatlightmust
comefromsomewhere,andhowtheeffectsoflightaredistributed.
4.3 Zero-shotalbedoevaluation
Constrainedscalingallowsustoinferalbedowithoutanydecoding(andwithoutanyalbedodata!)
bysettingα“0duringinference. WebenchmarkthesealbedoestimatesusingtheWHDRmetricon
7StyleGAN StyLitGAN StyleGAN StyLitGAN
Generation Ref Ours Relight Generation Ref Ours Relight
Figure 5: Qualitative results for relighting interior scenes using our relighter trained on images
obtainedfromStyLitGAN(whichproducesmultipleilluminationsofageneratedscene). StyLitGAN
hasastrongtendencytoincreaseordecreaseilluminationbyadjustingluminaires,typicallybedside
lightsbutalsolightcomingthroughfrenchwindows,etc. Ontheleft,wherethereferencelighting
tendstobebrighterandmoreconcentrated, noticehowforthetwotopimages, ourrelighterhas
identified and "turned up" the bedside lights; for the third, it has resisted StyLitGAN’s tendency
to invent helpful luminaires (there isn’t a bedside light where StyLitGAN imputed one, as close
inspectionshows). Ontheright,wherethereferencelightingismuchmoreuniform,ourrelighter
hasachievedthisby"turningdown"bedsidelights. Thisisanemergentphenomenon;themethodis
notsuppliedwithanyexplicitluminairemodel,etc.
Methods labels Flat Tuneδ WHDR α WHDR
IntrinsicDiffusion[27] CG No No 22.61 δ=0.1 optimalδ
IntrinsicDiffusion[27] CG Yes Yes 17.10 w/F w/oF w/F w/oF
InverserRender[51] No No No 21.40 1e-2 17.64 28.97 15.81 19.09
BBA[15] No No Yes 17.04 5e-3 18.93 31.81 16.02 19.53
Ours No No No 28.97 1e-3 18.00 29.77 15.84 19.13
Ours No No Yes 19.09 5e-4 18.04 29.62 15.85 19.12
Ours No Yes Yes 15.81
Table 3: We benchmark our albedo esimation on test Table 4: We conduct ablation experi-
setofIIWdataset[5]andcomparewithothers,though mentstoassesstheimpactofα onthe
thereliabilityhasbeenquestionedbyrecentpapers[15]. quality of albedo. "w/F" and "w/o F"
Flatdenotespostprocessingimageswithflattening[9]. denotepost-processingimageswithand
Despiteourmodelneverbeingtrainedonalbedomaps withoutflattening[9],respectively. The
orCGdata,ourbestconfigurationsignificantlyoutper- settingofδ “0.1andw/oFisthemost
formsallothermethodssuggestingourmodellearnshigh affected by α. Despite this, all values
qualityintrinsicrepresentations of α achieve high performance in our
optimalconfigurations.
theIIW[5]dataset(Section2). WeuseWHDRbecauseitiswidelyusedandallowscomparisons,but
existingliteraturerecordssignificantproblemsininterpretingthemeasure[15,6,27]. Amongother
irritatingfeatures,themetricseemstopreferoddcolors,andcanbehackedbyheavilyquantized
albedomaps. Asisstandard, weobtainlightnessbyaveragingR,G,andBalbedoandcompute
relativelightnessoftwopixellocationsi ,i bycomparingtoaconfidencethresholdδ:
$ 1 2 ,
& 1 ifRs {Rs ą1`δ.
r s si1 si2
J i,δpRq“
%
2 ifR i2{R
i1
ą1`δ
-
(11)
E otherwise
Theresultingclassification(onelighterthantwo;twolighterthanone;equivalent)isthencompared
tohumanannotationsJ usingtheconfidencescorew foreachannotationpair. WereportWHDRon
i
theIIWtestsplitinTable3tofacilitatecomparisonwithotherapproaches. Sinceourmodelisnot
trainedwithanyalbedomapsorcomputer-generatedimages,weneedtoadjustthethresholdforthe
optimalperformance. Followingpriorwork,weoptimizeδonthetrainingsplit,whichsignificantly
improves our performance from 28.97 to 19.09. Additionally, we enhance our performance by
8Input+ Ours+ Intrinsic Input+ Ours+ Intrinsic
Input Flatten Ours Flatten Diffusion Input Flatten Ours Flatten Diffusion
Figure6: QualitativeComparisonofAlbedoEstimationontheIIWDataset. Althoughourmodel
hasneverbeentrainedonanyalbedo-likemaps,iteffectivelyremovestheeffectsofexternallight
anddarkshadowsfromtheinput. Incontrast,IntrinsicDiffusion[27],asupervisedmethodtrained
oncomputergraphicsdata,oftenproducescolor-driftedestimations,likelyduetothedomainshift
betweenCGdataandrealimages.
post-processingouralbedomapusingflattening[9],anoptimizationtechniquetofurtherreducecolor
variations. Withthisimprovement,ourresultsreach15.81,substantiallyoutperformingtheintrinsic
diffusionmodel [27],adiffusion-basedalbedoregressionmodeltrainedoncomputergraphicsdata.
InFigure6,weshowsomequalitativecomparisonstointrinsicdiffusion. Weobservethatourmethod
effectivelyremovesexternallightingeffectsanddoesnotsufferfromcolor-driftduetodomaingap
unlikeintrinsicdiffusion,whichistrainedonCGdata.
5 Discussion,LimitationsandFutureWork
Ourmethodpresentsasignificantadvancementinimagerelightingbydemonstratingthatintrinsic
propertiessuchasalbedocanemergenaturallyfromtrainingonrelightingtaskswithoutexplicit
supervision. Thisfindingsimplifiestherelightingprocess,eliminatingtheneedfordetailedgeometric
andsurfacemodelsandenhancingthemodel’sabilitytogeneralizeacrossdiverseandunseenscenes.
Byencodingsceneandilluminationpropertiesaslatentrepresentations,weachieveaccurateand
flexiblerelighting,applicabletovariousfieldssuchasvirtualrealityandcinematicpost-production.
Thisapproachreducesthelearningprocess’scomplexityandoffersanewperspectiveondesigning
deeplearningmodelstocaptureandutilizeintrinsicsceneproperties. Thesefindingscanguidefuture
researchtowardmoreefficientandscalablerelightingtechniques,encouragingthedevelopmentof
modelsthatcanhandlevariouslightingconditionsandscenecomplexities.
Thecurrenttaxonomyofsurfaceintrinsics—typically,depth,normal,albedo,andperhapsspecular
albedoandroughness—isquitelimiting(comparehumanlanguageforsurfaceproperties[4]). Our
method,whichcomputeslatentintrinsicandextrinsicrepresentationsfromimagesandcombines
thesetotransferlightingconditionsacrossscenes, capturesphysicalconceptslikeluminaireand
albedowithoutexplicitphysicalparametrization. Thisabilitytorepresentsignificantimageeffects
withoutchoosingasurfacemodelofferssubstantialflexibility.
However,ourmethodhasseverallimitations. Itreliesonpairsofrelighteddatacapturedinthesame
scene,whichcanberesource-intensivetoobtain. Additionally,itdoesnotcopewellwithsaturated
pixelvaluescommoninLDRimages. Theintrinsicinformationbeinglatentisanotherlimitation
sincemanyapplicationsrequireexplicitintrinsicinformationlikedepthandnormals.
Nonetheless, there is good evidence that explicit intrinsic information can be extracted from our
latentintrinsics. Ourmethodclearly“knows”albedo,andthisinformationcanbeelicitedwithout
examples. Similarly, it “knows” something about luminaires, such as their locations and effects.
Itisintriguingtospeculatethatit“knows”otherinformationrelevanttorelighting,suchasdepth
or surface microstructure. Future work will pursue this line of inquiry further and also focus on
developingapurelyunsupervisedframeworktoinferintrinsicandextrinsicpropertiesfromcollections
ofin-the-wildimages. Thiswillincluderefiningprobingtechniquesforbetterextractionofexplicit
intrinsicsandidentifyingadditionalintrinsicpropertiescrucialforrelightingthatdonotalignwith
thecurrenttaxonomy. Webelievethiswillimprovetheapplicabilityandrobustnessofourapproach,
makingitsuitableforawiderrangeofreal-worldscenarios.
9Acknowledgment
ABthanksStephanR.Richterforthediscussionsthatledtotheconsiderationofintrinsicimagesas
latentvariables.
References
[1] J.T.BarronandJ.Malik. Shape,illumination,andreflectancefromshading. IEEEtransactions
onpatternanalysisandmachineintelligence,2014.
[2] J.T.BarronandB.Poole. Thefastbilateralsolver. InECCV,2016.
[3] H. Barrow and J. Tenenbaum. Recovering intrinsic scene characteristics from images. In
ComputerVisionSystems,1978.
[4] J.Beck. Surfacecolorperception. CornellUniversityPress,1972.
[5] S. Bell, K. Bala, and N. Snavely. Intrinsic images in the wild. ACM Trans. on Graphics
(SIGGRAPH),2014.
[6] A.BhattadandD.A.Forsyth. Cut-and-pasteobjectinsertionbyenablingdeepimagepriorfor
reshading. In2022InternationalConferenceon3DVision(3DV),pages332–341.IEEE,2022.
[7] A.Bhattad,D.McKee,D.Hoiem,andD.Forsyth. Styleganknowsnormal,depth,albedo,and
more. AdvancesinNeuralInformationProcessingSystems,36,2024.
[8] A.Bhattad,J.Soole,andD.Forsyth. Stylitgan: Image-basedrelightingvialatentcontrol. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),
2024.
[9] S.Bi,X.Han,andY.Yu. Anl1imagetransformforedge-preservingsmoothingandscene-level
intrinsicdecomposition. ACMTransactionsonGraphics(TOG),2015.
[10] S.Bi,N.K.Kalantari,andR.Ramamoorthi.Deephybridrealandsynthetictrainingforintrinsic
decomposition. InEurographicsSymposiumonRendering,2018.
[11] C.Careaga,S.M.H.Miangoleh,andY.Aksoy. Intrinsicharmonizationforillumination-aware
imagecompositing. InSIGGRAPHAsia2023ConferencePapers,pages1–10,2023.
[12] P.Debevec. Renderingsyntheticobjectsintorealscenes: bridgingtraditionalandimage-based
graphicswithglobalilluminationandhighdynamicrangephotography. InProceedingsofthe
25thannualconferenceonComputergraphicsandinteractivetechniques,1998.
[13] X.Du,N.Kolkin,G.Shakhnarovich,andA.Bhattad. Generativemodels: Whatdotheyknow?
dotheyknowthings? let’sfindout! arXivpreprintarXiv:2311.17137,2023.
[14] Q.Fan,J.Yang,G.Hua,B.Chen,andD.Wipf. Revisitingdeepintrinsicimagedecompositions.
2018.
[15] D.ForsythandJ.J.Rock. Intrinsicimagedecompositionusingparadigms. IEEEtransactions
onpatternanalysisandmachineintelligence,44(11):7624–7637,2021.
[16] M.-A.Gardner,K.Sunkavalli,E.Yumer,X.Shen,E.Gambaretto,C.Gagné,andJ.-F.Lalonde.
Learningtopredictindoorilluminationfromasingleimage. ACMTransactionsonGraphics,
2017.
[17] M.-A.Gardner,Y.Hold-Geoffroy,K.Sunkavalli,C.Gagné,andJ.-F.Lalonde. Deepparametric
indoorlightingestimation. InProceedingsoftheIEEEInternationalConferenceonComputer
Vision,pages7175–7183,2019.
[18] M.Garon,K.Sunkavalli,S.Hadap,N.Carr,andJ.-F.Lalonde. Fastspatially-varyingindoor
lightingestimation. InProceedingsoftheIEEEConferenceonComputerVisionandPattern
Recognition,2019.
10[19] A.Gilchrist. SeeingBlackandWhite. OxfordUniversityPress,2006.
[20] E.Hering. Outlinesofatheoryofthelightsense. 1964. TranslatedfromtheGermanof1874
byL.MHurvichandD.Jameson.
[21] Z.Hu,X.Huang,Y.Li,andQ.Wang. Sa-aeforany-to-anyrelighting. InEuropeanConference
onComputerVision,pages535–549.Springer,2020.
[22] M.Janner,J.Wu,T.D.Kulkarni,I.Yildirim,andJ.Tenenbaum. Self-supervisedintrinsicimage
decomposition. InAdvancesinNeuralInformationProcessingSystems,2017.
[23] O.F.Kar,T.Yeo,A.Atanov,andA.Zamir. 3dcommoncorruptionsanddataaugmentation. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
18963–18974,2022.
[24] T.Karras,M.Aittala,T.Aila,andS.Laine. Elucidatingthedesignspaceofdiffusion-based
generative models. Advances in Neural Information Processing Systems, 35:26565–26577,
2022.
[25] K. Karsch, V. Hedau, D. Forsyth, and D. Hoiem. Rendering synthetic objects into legacy
photographs. ACMTransactionsonGraphics(TOG),2011.
[26] K. Karsch, K. Sunkavalli, S. Hadap, N. Carr, H. Jin, R. Fonte, M. Sittig, and D. Forsyth.
Automaticsceneinferencefor3dobjectcompositing. ACMTransactionsonGraphics(TOG),
2014.
[27] P. Kocsis, V. Sitzmann, and M. Nießner. Intrinsic image diffusion for single-view material
estimation. arXivpreprintarXiv:2312.12274,2023.
[28] P.Kocsis,J.Philip,K.Sunkavalli,M.Nießner,andY.Hold-Geoffroy. Lightit: Illumination
modelingandcontrolfordiffusionmodels. arXivpreprintarXiv:2403.10615,2024.
[29] E.Land. Colorvisionandthenaturalimage: Parti. PNAS,45(1):115–129,January1959.
[30] E.Land. Colorvisionandthenaturalimage: Partii. PNAS,45(4):636–644,April1959.
[31] B.Li,H.Qin,W.Xiong,Y.Li,S.Feng,W.Hu,andS.Maybank.Ranking-basedcolorconstancy
withlimitedtrainingsamples. IEEETransactionsonPatternAnalysisandMachineIntelligence,
45(10):12304–12320,2023. doi: 10.1109/TPAMI.2023.3278832.
[32] Z.LiandN.Snavely. Cgintrinsics: Betterintrinsicimagedecompositionthroughphysically-
basedrendering. InProceedingsoftheEuropeanConferenceonComputerVision(ECCV),
pages371–387,2018.
[33] Z.Li,M.Shafiei,R.Ramamoorthi,K.Sunkavalli,andM.Chandraker. Inverserenderingfor
complexindoorscenes: Shape,spatially-varyinglightingandsvbrdffromasingleimage. 2020.
[34] Z.Li,T.-W.Yu,S.Sang,S.Wang,M.Song,Y.Liu,Y.-Y.Yeh,R.Zhu,N.Gundavarapu,J.Shi,
etal. Openrooms: Anopenframeworkforphotorealisticindoorscenedatasets. InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages7190–7199,
2021.
[35] Z.Li,J.Shi,S.Bi,R.Zhu,K.Sunkavalli,M.Hašan,Z.Xu,R.Ramamoorthi,andM.Chandraker.
Physically-basededitingofindoorscenelightingfromasingleimage. InComputerVision–
ECCV2022: 17thEuropeanConference,TelAviv,Israel,October23–27,2022,Proceedings,
PartVI,pages555–572.Springer,2022.
[36] A.Liu,S.Ginosar,T.Zhou,A.A.Efros,andN.Snavely. Learningtofactorizeandrelighta
city. InEuropeanConferenceonComputerVision,pages544–561.Springer,2020.
[37] L.Murmann,M.Gharbi,M.Aittala,andF.Durand. Adatasetofmulti-illuminationimagesin
thewild. InProceedingsoftheIEEEInternationalConferenceonComputerVision,2019.
11[38] T.NestmeyerandP.V.Gehler.Reflectanceadaptivefilteringimprovesintrinsicimageestimation.
InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pages
6789–6798,2017.
[39] T.Nestmeyer,J.-F.Lalonde,I.Matthews,E.Games,A.Lehrmann,andA.Borealis. Learning
physics-guidedfacerelightingunderdirectionallight. 2020.
[40] J.Philip, M.Gharbi, T.Zhou, A.A.Efros, andG.Drettakis. Multi-viewrelightingusinga
geometry-awarenetwork. ACMTrans.Graph.,38(4):78–1,2019.
[41] R. Ramamoorthi and P. Hanrahan. On the relationship between radiance and irradiance:
determiningtheilluminationfromimagesofaconvexlambertianobject. JOSAA,2001.
[42] M.Roberts,J.Ramapuram,A.Ranjan,A.Kumar,M.A.Bautista,N.Paczan,R.Webb,andJ.M.
Susskind. Hypersim: Aphotorealisticsyntheticdatasetforholisticindoorsceneunderstanding.
InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,2021.
[43] S.Sengupta,B.Curless,I.Kemelmacher-Shlizerman,andS.M.Seitz. Alightstageonevery
desk. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,2021.
[44] T.Sun,J.T.Barron,Y.-T.Tsai,Z.Xu,X.Yu,G.Fyffe,C.Rhemann,J.Busch,P.Debevec,and
R.Ramamoorthi.Singleimageportraitrelighting.ACMTransactionsonGraphics(Proceedings
SIGGRAPH),2019.
[45] H.vonHelmholtz. Helmholtz’treatiseonphysiologicaloptics. 1924-1925. Translatedfromthe
3rdGermanEditionof1867,editedbyJ.PSouthall.
[46] Z.Wang,A.C.Bovik,H.R.Sheikh,andE.P.Simoncelli. Imagequalityassessment: fromerror
visibilitytostructuralsimilarity. IEEEtransactionsonimageprocessing,13(4):600–612,2004.
[47] Z.Wang,J.Philion,S.Fidler,andJ.Kautz. Learningindoorinverserenderingwith3dspatially-
varying lighting. In Proceedings of the IEEE/CVF International Conference on Computer
Vision,2021.
[48] H.Weber,M.Garon,andJ.-F.Lalonde.Editableindoorlightingestimation.InComputerVision–
ECCV2022: 17thEuropeanConference,TelAviv,Israel,October23–27,2022,Proceedings,
PartVI,pages677–692.Springer,2022.
[49] C.WitzelandK.R.Gegenfurtner. Colorperception:Objects,constancy,andcategories. Annual
ReviewofVisionScience,4(Volume4,2018):475–499,2018.
[50] H.-H.Yang, W.-T.Chen, andS.-Y.Kuo. S3net: Asinglestreamstructurefordepthguided
imagerelighting. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages276–283,2021.
[51] Y. Yu and W. A. Smith. Inverserendernet: Learning single image inverse rendering. In
ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,2019.
[52] Y.Yu,K.H.R.Chan,C.You,C.Song,andY.Ma. Learningdiverseanddiscriminativerepre-
sentationsviatheprincipleofmaximalcodingratereduction. AdvancesinNeuralInformation
ProcessingSystems,33:9422–9434,2020.
[53] Y. Yu, A. Meka, M. Elgharib, H.-P. Seidel, C. Theobalt, and W. A. Smith. Self-supervised
outdoorscenerelighting. InEuropeanConferenceonComputerVision,2020.
[54] H.Zhou,S.Hadap,K.Sunkavalli,andD.W.Jacobs. Deepsingle-imageportraitrelighting.
InProceedingsoftheIEEEInternationalConferenceonComputerVision,pages7194–7202,
2019.
12A ExperimentDetails
TrainingDetailsWetrainourmodelwithabatchsizeof256for1,000epochsusingtheAdamW
optimizer,withaconstantlearningrateof2e-4andaweightdecayratioof1e-2. Toimprovethe
semantic representation, we corrupt images with Gaussian noise during the first 400 epochs and
followKarrasetal.[24]tosamplethestandarddeviationσwithlnpσq„Np´1.2,1.22q. Inthelater
600epochs,weturnofftheGaussiannoisetofocusonenhancingtheimagequality. Wetrainour
modelwith4A40andacompletetrainingrequires40hours.
ModelDetailsOurautoencoderemploysaU-Netarchitecture,incorporatingresidualconvolutional
blocksasthefundamentalcomponents. Eachblockiscomposedoftwoconvolutionallayers,group
normalization,andanonlinearactivationfunction. Thestructurespecifies[1,2,2,4,4,4]blocksat
eachresolutionlevel,startingfromaresolutionof256,withtheresolutionhalvingaftereachlevel.
Thecorrespondingconfigurationsforlatentchannelsattheselevelsare[32,64,128,128,256,512].
Theintrinsicfeatures,denotedasSl ,aregatheredfromtheoutputofthefinalblockateachresolution
s,i
level,startingfromaresolutionof128x128downtothebottleneck. Forgeneratingextrinsicfeatures
Ll,multipleMLPlayersareappliedtothebottleneckfeaturesoftheencoder,followedbyaveraging
s
acrossallspatialfeatures. Welimitthechannelnumberoftheextrinsicfeaturesto16topreventthem
fromconveyinghigh-frequencycomponents.
13Input Ref Ours Target Input Ref Ours Target
Figure7: Wevisualizemoreexamplesfortheimagerelightingtaskinmulti-illuminationdataset[37].
Right: Zoomed-in view of the chrome ball used as a probe to evaluate detail preservation in the
environmentmap.
14