Recurrent neural networks: vanishing and exploding
gradients are not the end of the story
NicolasZucchet AntonioOrvieto
DepartmentofComputerScience ELLISInstituteTübingen
ETHZürich MPIforIntelligentSystems
nzucchet@ethz.ch TübingenAICenter
antonio@tue.ellis.eu
Abstract
Recurrentneuralnetworks(RNNs)notoriouslystruggletolearnlong-termmem-
ories,primarilyduetovanishingandexplodinggradients. Therecentsuccessof
state-space models (SSMs), a subclass of RNNs, to overcome such difficulties
challengesourtheoreticalunderstanding. Inthispaper,wedelveintotheoptimiza-
tionchallengesofRNNsanddiscoverthat,asthememoryofanetworkincreases,
changes in its parameters result in increasingly large output variations, making
gradient-basedlearninghighlysensitive,evenwithoutexplodinggradients. Our
analysisfurtherrevealstheimportanceoftheelement-wiserecurrencedesignpat-
terncombinedwithcarefulparametrizationsinmitigatingthiseffect. Thisfeature
ispresentinSSMs,aswellasinotherarchitectures,suchasLSTMs. Overall,our
insightsprovideanewexplanationforsomeofthedifficultiesingradient-based
learningofRNNsandwhysomearchitecturesperformbetterthanothers.
Recurrent neural networks [RNNs; 1, 2] have long been the canonical architecture for modeling
temporal data [3, 4]. However, they are notoriously difficult to train on long sequences, as error
signalsflowingbackwardintimetendtoeithervanishorexplode[5–8]. Attentionmechanisms[9],
asfeaturedintransformers[10],addresstheseissuesbyenablingdirecttoken-to-tokencommuni-
cation,considerablysimplifyingsignalpropagationacrosslongtimeintervals. Yet,theirsuperior
performancecomeswithincreasedcomputationalandmemorycosts,duetotheirquadraticscalingin
thesequencelength. Thislimitationhasmotivatedsignificantresearchaimedatmakingtransformers
moreefficient[11–15].
Apromisinglineofresearchinthisdirectioninvolvesanewtypeoflinearrecurrentnetworksknown
asstate-spacemodels[SSMs;16–22]. Thesemodelstradeexpressivityforfastertrainingspeed,and
theyhavebeenshowntobeparticulareffectiveatcapturinglong-rangedependencies. Inthispaper,
wewonderwhetherthiseffectivenesscanbesolelyattributedtotheirabilitytoavoidvanishingand
explodinggradients. Thesimplicityofsuchmodelspresentsanopportunityforin-depththeoretical
analysis. Wefocusonsignalpropagationwithinthesemodels.
AfterreviewingclassicalresultsonrecurrentneuralnetworksinSection1,wedemonstratethatthey
can suffer an understudied problem: the curse of memory (Section 2). As the recurrent network
encodeslongermemories,thenetwork’sactivitybecomesincreasinglysensitivetochangesinits
parameters,evenwhenitsdynamicsremainsstable. InSection3,wethenshowthatSSMs,aswell
asotherarchitecturessuchasLSTMs,arewellequippedtomitigatethisissue. Weconcludeour
studybyanalyzingasimpleteacher-studenttask,whichalreadyrevealstheremarkablecomplexity
underlyingthelearningoflinearrecurrentnetworks(Section4). Finally,wediscusshowourfindings
extendtomorerealisticscenarios(Section5). Overall,ourresultsrevealthatvanishingandexploding
gradientsarenottheendofthestoryandthatthereexistsanadditionallayerofcomplexitybeyond
them.
4202
yaM
13
]GL.sc[
1v46012.5042:viXra1 Vanishingandexplodinggradients
Letusfirstintroducethenotationswewillbeusingthroughouttherestofthepaper. Weconsider
arecurrentneuralnetworkwithhiddenstateh ,updatefunctionf parametrizedbyθ,andinput
t θ
sequence(x ) . TheaverageperformanceofthenetworkismeasuredbyalossL. Wehave
t t
(cid:34) T (cid:35)
(cid:88)
h =f (h ,x ) and L=E L (h ) . (1)
t+1 θ t t+1 t t
t=1
ThegradientoftheinstantaneouslossL withrespecttotheparametersθisthenequalto
t
dL t = ∂L tdh t = ∂L t (cid:88) dh t ∂f θ(h ,x ) (2)
dθ ∂h dθ ∂h dh ∂θ
t′−1 t′
t t t′
t′≤t
Intheequationabove,weused∂ todenotepartialderivativesanddfortotalderivatives. Usingthis
notationenablesustodistinguishbetween∂ L ,whichcorrespondstotheerrorbackpropagatedfrom
ht t
thecurrentlosstermtothehiddenstatethroughthereadoutfunction,andd L,whichaccumulates
ht
theerrorsthatarebackpropagatedthroughthefuturehiddenstatevalues. Inparticular,∂ L=∂ L
(cid:80)
ht ht t
andd L=∂ L (h )+ d L (h ). Whenstackingseveralrecurrentlayersontopofeach
ht ht t t t′>t ht t′ t′
other,∂ Lcorrespondstothecurrenterrorbeingbackpropagatedpropagatedtothehiddenstateh
ht t
throughthehierarchyofthenetworkandd Ltofutureerrorsignalsbackpropagatedthroughthe
ht
recurrence.
Early work [5] highlighted the difficulty for gradient descent to make recurrent neural networks
rememberpastinputsthatwilllaterusefultoproduceadesiredbehavior. Thisisduetothefactthat
errorsignalsflowingbackwardintimetendtoeitherexplodeorvanish. Thekeyquantityis
t−1 t−1
dh t = (cid:89) ∂h i+1 = (cid:89) ∂f θ(h ,x ). (3)
dh ∂h ∂h i i+1
t′ i
i=t′ i=t′
One can remark that this quantity exponentially converges to 0 when the spectral radius of the
Jacobian∂ f isupperboundedbyaconstantstrictlysmallerthan1,andcanexponentiallyexplode
h θ
if thereexistssome component biggerthan 1. Theerror signal attime t backpropagated totime
t′ behavessimilarly,asd L = ∂ L d h . Gradient-basedlearningoflong-termmemoriesis
h t′ t ht t h′ t t
thusdifficult: thecontributionofpasthiddenstatestothecurrentlossbecomeseithernegligibleor
predominantasthetimespanconsideredincreases.
Sincethen,theanalysishasbeenrefined[6–8]andthedevelopmentofrecurrentarchitectureshas
mostly been driven by the desire to solve this pathological issue. Most famously, the LSTM [3]
unit,andlaterontheGRU[23],solvethisproblembyusingmemoryneuronsthatfacilitatedirect
informationstorageandretrieval,andbythesamewayerrorbackpropagation. Otherapproachesto
solvingthisproblem,tonameafew,involvegradientclipping[24,8],activitynormalization[25–
27],carefulweightinitialization[28,29]orenforcingarchitecturalconstraintssuchashierachical
processing[30,31],orthogonalweightmatrices[32–34]andoscillations[35–37].
2 Thecurseofmemory
According to common deep learning wisdom, it is often believed that solving the vanishing and
explodinggradientsproblemenablesrecurrentneuralnetworkstolearnlong-termdependencies. We
challengethisviewandquestion: issolvingthoseissuesreallyenoughtoensurewell-behavedloss
landscapes? We answer negatively by showing that gradients can explode as the memory of the
networkincreases,evenwhenthedynamicsofthenetworkremainsstable.
2.1 Intuition
Recurrentneuralnetworkshavesomethingspecial: theverysameupdatefunctionf isappliedover
θ
andover. Therefore,modifyingtheparametersθ willnotonlyinfluenceoneupdate,aschanging
theweightsofagivenlayerinafeedforwardneuralnetworkwould,butall. Asthememoryofthe
networkincreases,thehiddenstateskeepatraceoftheeffectofmoreupdates. Hiddenstatesthus
becomeincreasinglysensitivetoparameterchanges. Thisisthecurseofmemory. Weborrowthe
2A B C
1015 102 101
1011 0 0. .0 9 100 10 1 *0 0. .9 99 8 107 0 0. .99 99
9
11 00 42
10 3
0 0. .9 96
2
103 variance 1.0
10 6 10 5
0 0. .8 64
8
0.36
0 0.99 0.9999 0.0 0.5 1.0 0
*
Figure1: Optimizationofrecurrentneuralnetworksgetsharderastheirmemoryincreases.
A.Evolutionofthevarianceofd h asafunctionoftherecurrentparameterλandoftheinputx
λ t
auto-correlation decay rate ρ, when h = λh +x . As the memory of the network increases
t+1 t t
(λ→1),h becomesmoresensitivetochangesinλ,particularlyastheelementsintheinputsequence
t
becomemorecorrelated(ρ→1). Theexplosionofd h isfasterthantheoneofh ,ashighlighted
λ t t
with the grey line obtained for ρ = 1. See Section 2.2 for more detail. B, C. Illustration of the
phenomenononthetoyone-dimensionalteacher-studenttaskofSection4.1,inwhichtheteacher
isparametrizedbyarealnumberλ∗andthestudentbyacomplexnumberλ. InB.,λvariesonthe
positiverealaxisanditvariesonthecircleofradiusλ∗parametrizedbyθinC. Thelossbecomes
sharperinformationiskeptlongerinmemory,makinggradient-basedoptimizationnearlyimpossible.
termfrom[38,39],andnotethatMartensandSutskever[40]hypothesizedthatsuchaphenomenon
couldariseinRNNsandhindertheiroptimization.
Letusformalizeourintuitionandconsiderthesensitivityofthehiddenstateh ontheparametersθ:
t
dh t = (cid:88) dh t ∂f θ(h ,x ). (4)
dθ dh ∂θ
t′−1 t′
t′
t′≤t
Wheninformationstaysinthenetwork’smemoryforlonger,thenumberofnon-negligibleJacobian
d h termsincreases. Asaresult,themagnitudeofthissensitivityincreaseswhenthenetwork
h t′ t
encodeslonger-termdependencies,andlearningθ becomestrickier. Itiscriticaltonotethatthis
phenomenonarisesevenwhenexplodinggradientsareremovedfromthepicturebyconstrainingthe
eigenvaluesoftherecurrentJacobiantobesmallerthanoneandensuringthatthenetworkdynamics
remainsstable. Therestofthissectionwillbededicatedtostudyingquantitativelythisbehavior.
2.2 Signalpropagationinlineardiagonalrecurrentneuralnetworks
We study how hidden state and gradient magnitudes evolve as the network encodes longer-term
dependencies.Ideally,wewouldlikethesequantitiesnottovanishorexplode.Thispropertyimproves
the conditioning of the loss landscape [41] and hence eases optimization [42, 43]. We make the
followingassumptions:
a) Lineardiagonalrecurrentneuralnetworks. Werestrictourselvestoupdatefunctionsofthe
formf (h ,x ) = λ⊙h +x withλavectorofthesizeofh and⊙theelement-wise
θ t t+1 t t+1 t
product. Foreaseofexposition,wepresentresultsforreal-valuedλhere;seeAppendixA.2for
thecomplex-valuedsetting. Whilethisassumptionisstrong,itallowsustoidentifysomecrucial
mechanismsanditissatisfiedforsomemodelslikeS4[17]andLRUs[20]. Welatershowour
analysiscanmodelsomefeaturesofmoresophisticatednetworks.
b) Infinitetimehorizon. Weconsiderinfinitesequencesandinitializethenetworkdynamicsat
t =−∞. Itsimplifiesourcalculationswhilebeingareasonableassumptionwhenthesequences
0
consideredarelongerthanthecharacteristictimescalesofthedependencieswewanttolearn.
c) Wide-sensestationarity. Weassumethedifferentquantitiesthatthenetworkreceives,which
includetheinputsx , tobewide-sensestationary(WSS).ArandomprocessX issaidtobe
t t
WSSifitsauto-correlationfunctionisindependentoftime, thatis, forallt ∈ Zand∆ ∈ Z,
E [X X ] =: R (∆),whereE denotestheexpectationoverthedata. Itcorrespondsto
X t+∆ t X X
assumingthatthestatisticsofthedifferentsequenceswithinthedataareinvarianttotimeshifts.
Wearenowequippedtoanalyzesignalpropagationinonerecurrentlayer,bothintheforwardand
backwardpasses. Weshowthatbothhiddenstatesandbackpropagatederrorsexplodeas|λ|→1.
3
ecnairaV ssoLForward pass. Here, we are interested in understanding how the hidden state variance E[h2]
t
evolvesasafunctionofthecharacteristictimescaleofthenetworkencodedinλaswellastheinput
auto-correlationfunctionR . AfteracalculationthatwedefertoAppendixA.2,weobtain
x
 
E(cid:2) h2 t(cid:3) = 1−1
λ2
R x(0)+2(cid:88) λ∆R x(∆). (5)
∆≥1
Importantly,thevarianceofthehiddenstategoestoinfinityaslonger-termdependenciesareencoded
withinthenetwork,thatis|λ| → 1. Additionally,thedivergencespeeddependsontheinputdata
distribution: itincreasesasconsecutivetimestepsintheinputdistributionbecomemorecorrelated
(i.e.,lessoftheR (∆)termsarenegligible). Thisbehavioralreadyhighlightspotentialdifficultiesof
x
gradient-basedlearningofdeepneuralnetworkscontaininglinearrecurrentlayersasthevarianceof
neuralactivitycanbecomearbitrarilylarge,hinderinglearningabilitiesofdeeperlayers.
Backwardpass. Letusfirstderivethegradientofthelosswithrespecttoλ. Usingthechainrule
(cid:80)
wehaved L = ∂ Ld h . Wethusseekto understandhow d h behaves. We remarkthat
λ t ht λ t λ t
d h =λd h +h sothatd h isalowapassfilteredversionofthehiddenstate,whichisitself
λ t+1 λ t t λ t
alowpassfilterversionoftheinputs. Itthereforecomesasnosurprisethatthevarianceofd h
λ t
divergesfasterthantheoneofh when|λ|→1. Moreprecisely,weget
t
E(cid:34)(cid:18)
d dh
λt(cid:19)2(cid:35)
=
(11 −+ λλ 22
)3

R
x(0)+2(cid:88)
λ∆R
x(∆)
+
(1−2
λ2)2

(cid:88)
∆λ∆R
x(∆)
. (6)
∆≥1 ∆≥1
Weplottheexactbehaviorofthisquantitywhentheauto-correlationofxsatisfiesR (∆)=ρ|∆|on
x
Figure1andrefertheinterestedreadertotheappendixforaderivationofEquation6. Framedmore
generally,thehiddenstateofthenetwork,andthusitsfinaloutput,becomesincreasinglysensitiveto
changesinrecurrentparametersasthenetworkreachestheedgeofdynamicalstability(|λ|→1).
Thelastquantitythatweneedtoconsideristheerrorthatisbackpropagatedtotheinputsxofthe
recurrentlayer. Itcanbeobservedthatthebackwardpassisdualtotheforwardpassinthesensethat
itisarecurrentprocessthatreceivesbackpropagatederrors∂ Landitrunsinreversetime:
ht
dL dL ∂h dL ∂h ∂L dL ∂L
= t = t+1 + =λ + , (7)
dx dh ∂x dh ∂h ∂h dh ∂h
t t t t+1 t t t+1 t
inwhichwemadeuseof∂ h =1.Itfollowsthattheanalysiswedidfortheforwardpassalsoholds
xt t
here. Crucially,thisimpliesthattheexplosionbehaviorwillbemostsignificantfortherecurrent
parametersratherthanforpotentialinputorreadoutweights.
2.3 Extendingtheanalysistothenondiagonalcase
We now generalize our results to fully connected linear recurrent neural networks of the form
h =Ah +x . Forthesakeoftheanalysis,weassumethatAiscomplexdiagonalizable,thatis
t+1 t t
thereexistsacomplex-valuedmatrixP andacomplex-valuedvectorλsuchthatA=Pdiag(λ)P−1.
NotethatthisoccurswithprobabilityoneunderrandominitializationofA[20]. Inthiscase,
h =Phdiag withhdiag =diag(λ)hdiag+P−1x (8)
t t t+1 t t+1
and
dh ∂h ∂P ∂h dhdiag ∂λ ∂h dhdiag∂P−1
t = t + t t + t t . (9)
dA ∂P ∂A ∂hdiag dλ ∂A ∂hdiag dP−1 ∂A
t t
Fromthepreviousanalysis,weknowthatthedominatingterminthelimit|λ| → 1among∂ h ,
P t
d h and d−1h is d h , as P and P−1 act as readout and input weights. Given that all other
λ t P t λ t
termsdonotdirectlydependonthemagnitudeofλ,wehavethatd h ≃∂ h d hdiag∂ λ;c.f.
A t hdiag t λ t A
t
AppendixA.2.3forformalstatements. Thishastwoconsequences: First,thesensitivityofh on
t
AwillexplodeaslongermemoriesareencodedandthisdirectlycomesfromtheeigenvaluesofA.
Second,aseachentryofAtypicallyimpactsalleigenvaluesofthematrix,theexplosionbehavior
willbedistributedacrossallentries,whereasitwasconcentratedontheeigenvaluesforthediagonal
case. Wewilllaterobservethatthishassignificantpracticalconsequencesandpartlyexplainswhy
fullyconnectedlinearRNNsaredifficulttotrain. Asasidenote,weremarkthatenforcingthematrix
Atobeorthogonalsolvesvanishingandexplodinggradientissuesbuttheseweightsmayremain
sensitivetolearnbecauseofthecurseofmemory.
43 Mitigatingthecurseofmemory
Wehavediscussedthesensitivityofrecurrentnetworkstoparameterupdates. Giventhisproblem,
howcanitbemitigated? Recurrentnetworkswithdiagonalconnectivityareparticularlywellsuited
forthispurpose. BesidesenablingcontrolovertheJacobianandavoidingexplodinggradients,they
facilitatethemitigationofthecurseofmemory. Inthiscontext, wedemonstratethatstate-space
modelsandgatedRNNsinherentlyincorporatesuchmechanisms.
3.1 Asolution: normalizationandreparametrization
Bothforwardandbackwardpassesexplodeasthenetworkencodeslongermemories. Whenh =
t+1
λh +x ,wearguethatitisdecentlyeasytomitigatethiseffect.WeaimtokeepE[h2],E[(d h )2]
t t+1 t λ t
andE[(d h )2]independentofλ,similarlytoinitializationschemesensuringthemagnitudeofneural
xt t
activityremainsconstantindeepnetworks[44,45]andindependentofthelayerwidth[42,46,43].
Input normalization. A simple way to enforce E[h2 t] A 108
to stay constant is to introduce a scaling factor γ(λ), without
applied to the inputs a neuron receives, that satisfies 105
γ(λ)2E[h2]=Θ(1).Giventhatthebackwardpropagation
t 102
ofoutputerrorstoinputsisdualtotheforwardpass,the
role of γ has in the backward pass will be similar. The 0.0
valueγ needstotakethereforebothdependsontheinput 0 0.99 0.9999 0.9
B 0.99
d this etr oi ubu tpti uo tn et ro ron ro dr im sta rl ii bz ue tit oh ne tf oor nw oa rr md ap la izs es, ta hs ew bae cll ka ws ao rn
d
105 w ai nth do eu xt p 0 1. .9 099
pass. Perfectnormalizationislikelyunrealistic,butsome 102
normalizationcanhelp,asshowninFigure2.A. 10 1
Eigenvaluereparametrization. Wearenowleftwith 0 0.99 0.9999
keeping the gradient of the loss with respect to λ under
Figure2: Illustrationoftheeffectsof
control. Inputnormalizationpartlyreducesthememory-
normalizationandreparametrization.
inducedexplodingeffect,butnotentirelyasthevariance
Itcaneffectivelycontrolthemagnitude
ofd h ismuchlargerthantheoneofh (c.f. Fig.1.A).
λ t t ofA.E[h2]andB.E[(d h )2]overall
Reparametrization can close that gap. Indeed, if λ is t λ t
λvalueswhentheinputauto-correlation
parametrizedbyω,wehavethatd h =d h d λ.Choos-
ω t λ t ω
ingaparameterizationthatismoreandmoregranularas satisfies R x(∆) = ρ|∆| with ρ = 0,
λ goes to 1 thus helps in keeping d h constant. As- butdoesnotmanagedotosoforother
ω t
suming γ is independent of λ for simplicity, achieving type of distributi √ons (ρ ̸= 0). Here,
E[(d h )2]=Θ(1)requiressolvingthedifferentialequa- we use γ(λ) = 1−λ2, decouple it
ω t
tion γ(λ)2λ′(ω)2E[(d h )2] = 1. While deriving a uni- from λ when differentiating, and take
λ t
versaloptimalparametrizationisagainunrealisticdueto λ=exp(−exp(ν))asin[20].Thegrey
dependency on the input distribution, reparametrization lineindicatesthevaluethetwoquanti-
definitelyhelps,asshowninFigure2.B.Figure6illustrates tiestakewithoutanynormalizationand
howitcanaffectthelosslandscape. reparametrization,whenρ=1.
Whataboutcomplexnumbers? Wehavenotyetdiscussedthecaseλ ∈ C,relevantforSSMs
suchasS4[17]. WeextendouranalysistocomplexλsinAppendixA.3.2andhighlightthattheyare
difficulttoparametrizecorrectly. Briefly,ouranalysisrevealsthatifλisparametrizedasνexp(iθ),
parametrizationofθmustdependontheoneofν,butthereverseisnotnecessary. However,doing
sodoesnothurtslearning,asweexemplifyinAppendixA.3.2.
3.2 SeveralRNNarchitecturesimplicitlyalleviatesthecurseofmemory
State-spacemodels,aswellasgatedRNNs,featuresomeformofnormalizationandreparametrization
whichfacilitatessignalpropagation. Wediscusshowbelow.
State-space models. SSMs are originally motivated as discretizations of the continuous-time
differential equation h˙ = Ah+Bx [16]. Naïve discretization of the differential equation yields
h =(Id+dtA)h +dtBx whichalreadyactsassomeinputnormalizationwhend issmall.
t+1 t t+1 t
5
ecnairaV
ecnairaVMoreelaboratediscretizationschemes,suchasthezero-orderhold,effectivelyreparametrizethe
A matrix, e.g. with exp(dtA). Here, diagonalization arises from computational efficiency and
simplicityreasons[18]. Whilesuchmodelscancanapproximateanysmoothmappings[47,48],
theirexpressivityremainslimited[49]. Thenextgenerationofthesemodels,includingMamba[21],
incorporatesinput-dependentgateswhichmodulatedtdependingontheinputx . Thetheorywe
t
developedabovedoesnotstrictlyapplytothissettingasdtisnotconstant. However,sincetherest
ofthemodel’sstructureremainsunchangedweexpectthisbehavior,andthusitsremedies,toremain.
Gated RNNs. While the original motivation behind gated RNNs such as LSTMs [3] or GRUs
[23]largelydiffersfromtheoneofSSMs,theysharesimilarmechanisms. Inthesenetworks,the
memorycontentstoredinhiddenneuronscanbeerasedthroughaforgetgate,andincominginputs
canselectivelybewritteninmemorythroughaninputgate. Mathematically, thiscorrespondsto
hiddenstateupdatesoftheformh =f ⊙h +i ⊙x ,withtheforgetf andinputi
t+1 t+1 t t+1 t+1 t+1 t+1
gatesbeingindependentnon-linearfunctionsofx andh . Theforgetgateisakintoλandusually
t+1 t
involvesasigmoidnon-linearity,whichhasasimilareffectinthebackwardpassasreparametrizing
λ. Theinputgatecanactasaninputnormalizationdependingontheinitializationofthenetwork
or if is coupled to the forget gate as in the GRU (f = 1−i ) [29]. Importantly, the gates here
t t
dependonthehiddenstatesandthusmaketheJacobian∂ h nondiagonal. Yet,wearguethat
ht t+1
thesearchitecturesstillhaveabiastowardsdiagonality. Indeed,thecontributionsofthehiddenstate
throughtheforgetandinputgatesareindirect,andtheycanbeignoredwhentheweightsconnecting
thehiddenstatestothegatesaresmall. Wethereforegetbacktothesettingwediscussedinthe
previousparagraph;weconfirmthisintuitioninSection5. Inregimesinwhichthisapproximation
doesnothold,studyingsignalpropagationrequiresamuchmoresophisticatedanaylsisthantheone
wehavedonehere[50].
4 Alinearteacher-studentanalysis
Weconsiderateacher-studenttaskwithlinearrecurrentnetworks[51]. Thisisarguablythesimplest
settinginwhichonecantrainrecurrentnetworks,andyet,asweshallsee,itisremarkablycomplex.
Wefirstturntotheone-dimensionalsettingtoprovideanintuitiveillustrationofhowthecurseof
memoryandvanishinggradientsinterplay. Wethenaddressthegeneralsettingandobservethatlinear
networksindeedsufferfromthecurseofmemory,andthattheremedieswestudiedinthelastsection
areeffective. Weadditionallyfindthatdiagonalitygreatlymodifiesthestructureofthelosslandscape
andhelpsoptimizerswithadaptivelearningratestocompensateforaneventualincreasedsensitivity.
4.1 Theone-dimensionalcase
Wefirstconsiderastudentandteacherfollowingtheone-dimensionaldynamicsh =λh +x ,
t+1 t t+1
withcomplex-valuedparameterλforthestudentandλ∗ fortheteacher. Forsimplicity,wedraw
x from a normal distribution with mean 0 and standard deviation 1 and note that other input
t+1
distributionsdonotqualitativelychangetheresults. Theperformanceofthestudentismeasuredbya
lossLthataveragesthepertime-steplossesL := 1|h −h∗|2overtheentiresequence.
t 2 t t
Thissimplemodelalreadycapturestwokeydifficultiesofgradient-basedlearningofrecurrentneural
networks. InFigure1,weplottheresultinglosslandscapefordifferentλ∗values,whenλevolves
on the positive part of the real axis (Fig. 1.B) and when it evolves on the circle of radius |λ∗| in
thecomplexplane(Fig.1.C).Werestrictλstohaveabsolutevaluessmallerthanone: exploding
gradientsareoutofthepicture. Still,twodifficultiesforgradient-basedlearningappearhere. Onone
side,vanishinggradientsleadtoflatlossregionsthatarehardtoescape. Ontheotherside,theloss
sharpensasthestudentencodeslongermemoriesbecauseofthecurseofmemory. Asaconsequence,
gradient-basedoptimizationisextremelytedious,alreadyinthissimpleexample.
4.2 Diagonalconnectivitysimplifiesoptimization
Wenowmovetothegeneralcaseinwhichtheteacherevolvesaccordingto
h =Ah +Bx and y =Ch +Dx . (10)
t+1 t t+1 t t t
withh ∈Rn,x ∈Rdrawni.i.d. fromN(0,1),A∈Rn×n,B ∈Rn×1,C ∈R1×nandD ∈R1×1.
t t
Herebothinputsandoutputsarescalars.
6A B Figure3: LRUsarebetteratreplicating
10 1 RNN 10 1 a teacher’s behavior than linear RNNs.
LRU A.Astheteacherencodeslongerdependen-
10 3 10 2 cies(ν → 1),thelinearRNNstrugglesto
10 5 10 3 reproduceit,butnottheLRU.B.Anabla-
10 7 10 4 tionstudy(ν =0.99)revealsthatthisgap
mainly comes from having a 2×2 block
0.32 0.84 0.96 0.99 diagonalweightmatrix,andthenreplacing
thoseblockswithcomplexnumbers.
Given the intuition we have developed so far, we expect fully connected linear recurrent neural
networkstostruggleinsolvingthetaskwhentheteacherencodeslongermemories,notonlybecause
of exploding gradients but also due to the curse of memory. Conversely, diagonality facilitates
eigenvaluereparametrizationtoavoidexplodinggradientsandmakethembetterbehaved. Werun
thefollowingexperimenttoverifythisintuition. Wedrawrandomteacherswithhiddendimension
n=10andtransformthecomplexeigenvaluesoftherecurrentmatrixAtohavemagnitudesclose
toavalueν thatwecontrol1. Thelargerν is,thelongerthememoriesencodedbytheteacherare.
WetrainalinearRNN,aswellasanLRU[20],withhiddendimension64onthistask. Thestudents
arethereforelargelyoverparametrized. WechosetheLRUarchitecturetorepresentSSMsdueto
itssimplicity. Thisarchitectureusesinputnormalizationandanexponentialreparametrizationof
theeigenvalues,similartowhatweanalyzeinSection3. BothnetworkaretrainedusingtheAdam
optimizer[52]andcosineannealingschedulefor10ksteps,onbatchesofsize128. Thesequences
contain300timesteps. Learningratesaretunedseparatelyforeachmethodandtrainingdistribution.
Theresults,whichweplotinFigure3.A,confirmourintuition: LRUssignificantlyoutperformlinear
RNNswhenlongmemorieshavetobelearned,despitehaving10timesfewerparameters.
Next,wewonderwhichdesignchoicesbehindtheLRUarchitecturearecrucialtothisperformance
improvement. Tothisend,weinterpolatebetweenalinearRNNandanLRUinthefollowingway:
First,werestricttheweightmatrixofthelinearRNNtoablockdiagonalwithblocksofsize2. Each
ofsuchblockscanrepresentacomplexnumber,so32complexnumbersintotal. Weadditionally
doublethenumberofhiddenneurons. Second,wechangethose2×2blocks(andtheirinputand
outputweights)tobecomplexnumbers.Finally,weaddtheγinputnormalizationandtheexponential
parametrization to obtain the final LRU architecture. We report the results of this experiment in
Figure3.B.Wefindthatmostofthegapcomesfromtheintroductionofcomplexnumbersandcan
bepartiallyreducedbymakingtheweightmatrixblockdiagonal. Interestingly,thosetwochanges
reducethenumberofparametersthemodelhasandslightlyreducethemodelexpressivitysoan
explanationofthisbehaviorislikelytoberelatedtotheoptimizationpropertiesofthosemodels. We
confirmthishypothesisinthenextsection.
4.3 Ontheimportanceofadaptivelearningrates
Sofar, ourresultshighlighttheimportanceofdirectlyparametrizingthecomplexeigenvaluesof
therecurrentconnectivitymatrix. Thisparametrizationdoesnotmitigateanyexplodingbehavior
butmodifiesthelosslandscape, makingitpossibleforoptimizerswithadaptivelearningratesto
compensateforthesebehaviors. Todemonstratethis,westudytheHessianoftheloss:
(cid:34) (cid:35)
d2L =(cid:88) E dh t∂2L tdh t⊤ + ∂L td2h t . (11)
dθ2 x dθ ∂h2 dθ ∂h dθ2
t t t
Ifthenetworkcanperfectlyfitthetargetdata,whichisthecasehere,thesecondtermvanishesat
optimality. WeplottheHessianatoptimalityinFigure4.AandBforastandardlinearrecurrent
networkandonewithcomplexdiagonalparametrization,bothwith4hiddenneurons(ν = 0.99).
Weobservethattheeigenvaluespectraaresimilarforthetwoarchitectures,bothexhibitinglarge
terms that characteristic of the curse of memory, which makes learning with stochastic gradient
√
1WedraweachentryofAfromN(0,1/ n),complexdiagonalizeit,andapplythetransformationx(cid:55)→
ν+(1−ν)tanh(x)totheabsolutevaluesoftheeigenvalues.
7
ssoL
NNR kcolb lanogaid erom snoruen xelpmoc srebmun .mron URLA A B C D B re im Bre Bⁱm Cre Cⁱm D
106
re
104 C
A 103 im 102 10 4
100 Bre 100 1100 66
0 Bⁱm 0 10 8
B 100 100
C
103 C Cr ⁱme 11 00 42 10 10
D 106 D A B C D
D 1 D 1 D
0 0 10 3
1 1
10 4 107 107
100 100 10 5
10 7 10 7 re im Bre Bⁱm Cre Cⁱm D
0 10 20 0 10 20
Index eigenvalue Index eigenvalue
Figure4: Differencesinlearningabilitiesbetweenfullyconnectedandcomplexdiagonallinear
RNNsareduetoabetterstructureofthelosslandscape. A,B.Hessianofthelossatoptimality,
its10eigenvectorswithgreatesteigenvaluesanditseigenspectraforafullyconnectedRNN(A)and
acomplexdiagonalone(B).Thespectraarealmostthesamebuttopeigenvectorsareconcentrated
onfewcoordinatesforthecomplexdiagonalonebutnotforthefullyconnectedone. C,D.This
structuremakesitpossibleforAdamtoefficientlydealwiththeextrasensitivity,asshownwiththe
effectivelearningratesthatitusesattheendoflearning. Forthefullyconnectedone(C),Adam
uses very low learning rates to compensate for the sensitivity, whereas it can use larger ones for
thecomplexdiagonalonewithouthinderingtrainingstability. Thehorizontalgreylineshowsthe
learningrateused,whichishere10−3.
descentalmostimpossible2. However,theirstructurediffers. ForthefullyconnectedlinearRNN,
thetopeigenvectorsaredistributedovermanycoordinates,whereastheyareconcentratedonafew
coordinatesforthecomplexdiagonalone. Thisfeatureaidsadaptiveoptimization[e.g.,56]: adapting
to large curvature is much easier for Adam when the pathological directions are aligned to the
canonicalbasis. Thisiswhatweobserveinpractice. InFigure4.CandD,wecomparetheeffective
learningrateusedbyAdam,whichwecomputebyprovidingavectorofonestotheoptimizer.Forthe
denselinearRNN,theadaptivelearningratescannotcompensatefortheintricatecouplingbetween
components,resultinginverysmalllearningrates. Conversely,thesensitivityofcomplexdiagonal
RNNsisconcentratedonfewparameters,whichadaptivelearningratescancompensatefor,leading
totargetedandoveralllargerlearningrates,significantlyspeedinguplearning. Asasidenote,the
complexeigenvaluesoftheteachercomeinconjugatepairs. However,duringtraining,thecomplex
valuesofthecomplexRNNarenotconjugatesofeachother,therebyincreasingHessiandiagonality.
Finally,performingthisanalysisfortheLRU,wefindthattheHessianspectrumissimilartothe
diagonalsettingandthattheexplodingdimensionsoftheHessianarealmostexclusivelyduetothe
angleparameter,consistentlywithourtheoreticalanalysis;seeFigure9.
Beforeconcludingthissection,weinvestigatewhetherthereexisteigenvaluedistributionsthatbreak
thediagonalstructureoftheHessian,makingoptimizationharderandincreasingthepressureon
eigenvaluereparametrization. WetheoreticallyproveinAppendixB.2theintuitiveresultthatthe
moreconcentratedtheeigenvaluesare,thelessdiagonaltheHessianis. Asaconsequence,thegap
betweencomplex-valueddiagonalnetworksandLRUswidens,buttheformerstillgreatlyoutperform
theirfully-connectedcounterpart;seeFigure10.
2ThegradientLipschitzconstantLofthelossequalsthemaximumHessianeigenvalue[53].Thisquantity
setsabound2/Lforthemaximumgloballystablelearningrate.Whileconvergencemighthappeninasubspace,
itisgenerallyalignedwiththetopHessianeigenspacenearthesolution[54,55].
8
naisseH
01
poT
eulavnegiE
.tcev
.gie
RL
evitceffE
RL
evitceffEA 105 cRNN B
108
cRNN C
no norm
lay. 1 107
rest
103 lay. 4 103 LRU 103 rest
LRU lay. 1 10 2 10 1 l na oy re mr
101
lay. 4 10 7 rest 10 5
LSTM LSTM rest
0 0.9 0.99 lay. 1 0 0.9 0.99 LSTM 0 0.9 0.99
lay. 4 ff
Figure5: Signalpropagationindeeprecurrentnetworksatinitializationisconsistentwithour
theory. A.E[h2]afterthefirstandthefourthlayer,asafunctionofthememoryparameterν,for
t
complex-valueddiagonalRNN(cRNN),LRU,andLSTMrecurrentlayers. Theinputnormalization
presentintheLRUeffectivelykeepsneuralactivitybounded. B.Comparisonoftheevolutionof
E[d h2]forthedifferentrecurrentlayersandspecificgroupsofparameters. Forthecomplexdiagonal
θ t
RNN, the gradients of all parameters explode, whereas only the ones of θ explode for the LRU.
RegardingtheLSTM,themangnitudeofthegradientsdoesnotdependonν,withtheLSTM-specific
parametersexhibitingsmallergradientsthanthefeedforward(ff)ones. C.Layernormalizationkeeps
theoverallgradientmagnitudeundercontrol. Batchnormalizationyieldssimilarresults.
5 Signalpropagationindeeprecurrentnetworksatinitialization
Theultimategoalofourtheoreticalquestistogainpracticalinsightsintothetrainingofrecurrent
networks. Specifically,weaimtoverifywhetherthetrendsestablishedtheoreticallyandincontrolled
experimentsholdinpractice,bystudyingsignalpropagationatinitialization.
Weprovidesequencesof512texttokensasinputtodeeprecurrentnetworksthatcontainfourblocks
of256hiddenneuronseachanduseanext-tokenpredictionlosstomeasuretheirperformance. Each
blockconsistsofarecurrentlayerfollowedbyafeedforwardgatedlinearunit[57]. Bydefault,there
arenonormalizationlayersinthisarchitecture. MoredetailscanbefoundinAppendixC.1. We
empiricallystudyhowE[h2]andE[d h2]evolvewhenthememoryoftherecurrentlayer,controlled
t θ t
by ν, increases. We compare three different recurrent layers: a complex-valued diagonal RNN
(cRNN),aLRUandaLSTMinitializedwiththechronoinitialization[29].
Theresultsareconsistentwithourtheory. Complex-valuedRNNssufferfromthecurseofmemory.
LRUsalmostperfectlymitigatethiseffectintheforwardpass(Fig.5.A)aswellasinthebackward
pass (Fig. 5.B), except for the angle parameter θ, as expected. We also wonder whether layer
normalizationcanreplacetheinputnormalizationandreparametrizationoftheLRU.Wefindthatit
mitigatesthememory-inducedgradientexplosionatthemacroscopiclevel(Fig.5.C),butitlikely
kills any learning signal for the smallest eigenvalues. Finally, the LSTM manages to keep the
gradientnormconstantoverdifferentlevelofmemory,consistentlywiththeintuitionwedeveloped
inSection3.2,althoughtheLSTM-specificparametersexhibitsmallergradientsthanthefeedforward
parameters.
6 Conclusion
Vanishingandexplodinggradientscomplicatelearningrecurrentnetworks,butsolvingtheseproblems
isnotenough. Weuncoveredyetanotherdifficultyoftrainingsuchnetworks,whichisrootedin
theiriterativenatureandarisesattheedgeofdynamicalstability. Reparametrizationsandadaptive
learningratescaneffectivelymitigatethisbehaviorinpractice, anddiagonalizingtherecurrence
simplifiesboth. Ouranalysisadditionallyrevealsthecomplexityoflearningtheangleofcomplex
eigenvalues,whichmayexplainwhycomplexnumberswerenotfoundtobeusefulinmostrecent
state-spacemodelarchitectures[21,22].
Asidefindingofourstudyisthesymbiosisbetweenindependentmodules,whicharehereneurons
andcanmorebemoregenerallysmallheads,withadaptivelearningrateoptimizersinlinearrecurrent
networks. Such a design pattern has promising properties: it facilitates online learning [58] and
compositionalgeneralization[59],allowsforhighlevelofparallelization[22],andmatches,atahigh
level,themodularorganizationofthecortexincorticalcolumns[60]. Understandinghowtoincrease
theexpressivityofsmalllinearmoduleswhilekeepingtheirgreatoptimizationpropertiesconstitutes
apromisingavenueforfutureresearch.
9
ecnairaV ecnairaV ecnairaVAcknowledgments
TheauthorsthankRobertMeier, JoãoSacramento, GuillaumeLajoie, EzekielWilliams, Razvan
Pascanu,ImanolSchlagandBobbyHeforinsightfuldiscussions. NicolasZucchetwassupportedby
anETHResearchGrant(ETH-2321-1)andAntonioOrvietoacknowledgesthefinancialsupportof
theHectorFoundation.
10References
[1] DavidERumelhart,PaulSmolensky,JamesLMcClelland,andGHinton. Sequentialthoughtprocessesin
PDPmodels. Paralleldistributedprocessing:explorationsinthemicrostructuresofcognition,2,1986.
[2] JeffreyLElman. Findingstructureintime. Cognitivescience,14(2),1990.
[3] SeppHochreiterandJürgenSchmidhuber. LongShort-TermMemory. NeuralComputation,9(8),1997.
[4] IlyaSutskever,OriolVinyals,andQuocV.Le. Sequencetosequencelearningwithneuralnetworks. In
AdvancesinNeuralInformationProcessingSystems,2014.
[5] Sepp Hochreiter. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Technische
UniversitätMünchen,1991.
[6] YoshuaBengio, PatriceSimard, andPaoloFrasconi. Learninglong-termdependencieswithgradient
descentisdifficult. IEEEtransactionsonneuralnetworks,5(2),1994.
[7] SeppHochreiter,YoshuaBengio,PaoloFrasconi,andJurgenSchmidhuber. Gradientflowinrecurrent
nets:thedifficultyoflearninglong-termdependencies. InAfieldguidetodynamicalrecurrentnetworks.
IEEE,2001.
[8] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural
networks. InInternationalConferenceonMachineLearning,2013.
[9] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointlylearning
toalignandtranslate. InInternationalConferenceonLearningRepresentations,2015.
[10] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,Łukasz
Kaiser,andIlliaPolosukhin. Attentionisallyouneed. InAdvancesinNeuralInformationProcessing
Systems,2017.
[11] QuentinFournier,GaétanMarceauCaron,andDanielAloise. Apracticalsurveyonfasterandlighter
transformers. ACMComputingSurveys,55(14s),2023.
[12] AngelosKatharopoulos,ApoorvVyas,NikolaosPappas,andFrançoisFleuret. TransformersareRNNs:
fastautoregressiveTransformerswithlinearattention. InInternationalConferenceonMachineLearning,
2020.
[13] TriDao,DanielY.Fu,StefanoErmon,AtriRudra,andChristopherRé. FlashAttention:fastandmemory-
efficientexactattentionwithIO-awareness. arXivpreprintarXiv:2205.14135,2022.
[14] WilliamFedus,BarretZoph,andNoamShazeer.SwitchTransformers:scalingtotrillionparametermodels
withsimpleandefficientsparsity. JournalofMachineLearningResearch,2022.
[15] ShumingMa,HongyuWang,LingxiaoMa,LeiWang,WenhuiWang,ShaohanHuang,LiDong,Ruiping
Wang,JilongXue,andFuruWei. Theeraof1-bitLLMs: allLargeLanguageModelsarein1.58bits.
arXivpreprintarXiv:2402.17764,2024.
[16] AlbertGu,IsysJohnson,KaranGoel,KhaledSaab,TriDao,AtriRudra,andChristopherRé. Combining
recurrent,convolutional,andcontinuous-timemodelswithlinearstate-spacelayers. InAdvancesinNeural
InformationProcessingSystems,2021.
[17] AlbertGu,KaranGoel,andChristopherRé. Efficientlymodelinglongsequenceswithstructuredstate
spaces. InInternationalConferenceonLearningRepresentations,2022.
[18] AnkitGupta,AlbertGu,andJonathanBerant. Diagonalstatespacesareaseffectiveasstructuredstates
spaces. InAdvancesinNeuralInformationProcessingSystems,volume35,2022.
[19] JimmyT.H.Smith,AndrewWarrington,andScottW.Linderman.Simplifiedstatespacelayersforsequence
modeling. InInternationalConferenceonLearningRepresentations,2023.
[20] AntonioOrvieto,SamuelLSmith,AlbertGu,AnushanFernando,CaglarGulcehre,RazvanPascanu,and
SohamDe. Resurrectingrecurrentneuralnetworksforlongsequences. InInternationalConferenceon
MachineLearning,2023.
[21] AlbertGuandTriDao. Mamba:linear-timesequencemodelingwithselectivestatespaces. arXivpreprint
arXiv:2312.00752,2023.
11[22] SohamDe,SamuelL.Smith,AnushanFernando,AleksandarBotev,GeorgeCristian-Muraru,AlbertGu,
RubaHaroun,LeonardBerrada,YutianChen,SrivatsanSrinivasan,GuillaumeDesjardins,ArnaudDoucet,
DavidBudden,YeeWhyeTeh,RazvanPascanu,NandoDeFreitas,andCaglarGulcehre. Griffin:mixing
gatedlinearrecurrenceswithlocalattentionforefficientlanguagemodels.arXivpreprintarXiv:2402.19427,
2024.
[23] KyunghyunCho,BartvanMerrienboer,DzmitryBahdanau,andYoshuaBengio. Onthepropertiesof
neuralmachinetranslation:encoder-decoderapproaches. InProceedingsofSSST-8,EighthWorkshopon
Syntax,SemanticsandStructureinStatisticalTranslation,2014.
[24] TomasMikolov. Statisticallanguagemodelsbasedonneuralnetworks. PhDthesis,BrnoUniversityof
Technology,2012.
[25] SergeyIoffeandChristianSzegedy. Batchnormalization:acceleratingdeepnetworktrainingbyreducing
internalcovariateshift. InInternationalConferenceonMachineLearning,2015.
[26] JimmyLeiBa,JamieRyanKiros,andGeoffreyE.Hinton. Layernormalization. InNeuralInformation
ProcessingSystems-DeepLearningSymposium,2016.
[27] TimCooijmans,NicolasBallas,CésarLaurent,Çag˘larGülçehre,andAaronCourville. Recurrentbatch
normalization. InInternationalConferenceonLearningRepresentations,2017.
[28] QuocV.Le,NavdeepJaitly,andGeoffreyE.Hinton. Asimplewaytoinitializerecurrentnetworksof
rectifiedlinearunits. arXivpreprintarXiv:1504.00941,2015.
[29] CorentinTallecandYannOllivier. Canrecurrentneuralnetworkswarptime? InInternationalConference
onLearningRepresentations,2018.
[30] SalahElHihiandYoshuaBengio. Hierarchicalrecurrentneuralnetworksforlong-termdependencies. In
NeuralInformationProcessingSystems,1995.
[31] Asier Mujika, Florian Meier, and Angelika Steger. Fast-slow recurrent neural networks. In Neural
InformationProcessingSystems,2017.
[32] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In
InternationalConferenceonMachineLearning,2016.
[33] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality and learning
recurrentnetworkswithlongtermdependencies. InInternationalConferenceonMachineLearning,2017.
[34] KyleHelfrich,DevinWillmott,andQiangYe. OrthogonalrecurrentneuralnetworkswithscaledCayley
transform. InInternationalConferenceonMachineLearning,2018.
[35] T.KonstantinRuschandSiddharthaMishra. Coupledoscillatoryrecurrentneuralnetwork(coRNN):an
accurateand(gradient)stablearchitectureforlearninglongtimedependencies.InInternationalConference
onLearningRepresentations,2021.
[36] T Anderson Keller and Max Welling. Neural wave machines: learning spatiotemporally structured
representationswithlocallycoupledoscillatoryrecurrentneuralnetworks. InInternationalConferenceon
MachineLearning,2023.
[37] IlMemmingPark,ÁbelSágodi,andPiotrAleksanderSokół. Persistentlearningsignalsandworking
memorywithoutcontinuousattractors. arXivpreprintarXiv:2308.12585,2023.
[38] ZhongLi,JiequnHan,WeinanE,andQianxiaoLi. Approximationandoptimizationtheoryforlinear
continuous-timerecurrentneuralnetworks. JournalofMachineLearningResearch,23(42),2022.
[39] ShidaWang,ZhongLi,andQianxiaoLi. Inverseapproximationtheoryfornonlinearrecurrentneural
networks. InInternationalConferenceonLearningRepresentations,2024.
[40] JamesMartensandIlyaSutskever. Learningrecurrentneuralnetworkswithhessian-freeoptimization. In
InternationalConferenceonMachineLearning,2011.
[41] Lorenzo Noci, Alexandru Meterez, Thomas Hofmann, and Antonio Orvieto. Why do learning rates
transfer?Reconcilingoptimizationandscalinglimitsfordeeplearning. arXivpreprintarXiv:2402.17457,
2024.
[42] Greg Yang and Edward J Hu. Feature learning in infinite-width neural networks. arXiv preprint
arXiv:2011.14522,2020.
12[43] GregYang,JamesBSimon,andJeremyBernstein.Aspectralconditionforfeaturelearning.arXivpreprint
arXiv:2310.17813,2023.
[44] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural
networks. InArtificialIntelligenceandStatistics,2010.
[45] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. DelvingDeepintoRectifiers: Surpassing
Human-LevelPerformanceonImageNetClassification. InInternationalConferenceonComputerVision,
2015.
[46] GregYang, EdwardJ.Hu, IgorBabuschkin, SzymonSidor, XiaodongLiu, DavidFarhi, NickRyder,
JakubPachocki,WeizhuChen,andJianfengGao. TensorProgramsV:TuningLargeNeuralNetworksvia
Zero-ShotHyperparameterTransfer. arXivpreprintarXiv:2203.03466,2022.
[47] S.BoydandL.Chua. FadingmemoryandtheproblemofapproximatingnonlinearoperatorswithVolterra
series. IEEETransactionsonCircuitsandSystems,32(11),1985.
[48] AntonioOrvieto,SohamDe,CaglarGulcehre,RazvanPascanu,andSamuelL.Smith. Universalityof
linearrecurrencesfollowedbynon-linearprojections: finite-widthguaranteesandbenefitsofcomplex
eigenvalues. InInternationalConferenceonMachineLearning,2024.
[49] WilliamMerrill,JacksonPetty,andAshishSabharwal. Theillusionofstateinstate-spacemodels. In
InternationalConferenceonMachineLearning,2024.
[50] MinminChen,JeffreyPennington,andSamuelS.Schoenholz. Dynamicalisometryandameanfield
theoryofRNNs:gatingenablessignalpropagationinrecurrentneuralnetworks. 2018.
[51] MoritzHardt,TengyuMa,andBenjaminRecht.Gradientdescentlearnslineardynamicalsystems.Journal
ofMachineLearningResearch,19(29),2018.
[52] Diederik P. Kingma and Jimmy Ba. Adam: a method for stochastic optimization. In International
ConferenceonLearningRepresentations,2015.
[53] YuriiNesterovandothers. Lecturesonconvexoptimization,volume137. Springer,2018.
[54] JeremyCohen,SimranKaur,YuanzhiLi,JZicoKolter,andAmeetTalwalkar. Gradientdescentonneural
networkstypicallyoccursattheedgeofstability.InInternationalConferenceonLearningRepresentations,
2020.
[55] GuyGur-Ari,DanielARoberts,andEthanDyer. Gradientdescenthappensinatinysubspace. arXiv
preprintarXiv:1812.04754,2018.
[56] YanPanandYuanzhiLi. TowardunderstandingwhyAdamconvergesfasterthanSGDfortransformers.
arXivpreprintarXiv:2306.00204,2023.
[57] Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated
convolutionalnetworks. InInternationalConferenceonMachineLearning,2017.
[58] NicolasZucchet,RobertMeier,SimonSchug,AsierMujika,andJoãoSacramento. Onlinelearningof
long-rangedependencies. InAdvancesinNeuralInformationProcessingSystems,2023.
[59] AnirudhGoyal,AlexLamb,JordanHoffmann,ShagunSodhani,SergeyLevine,YoshuaBengio,and
Bernhard Schölkopf. Recurrent independent mechanisms. In International Conference on Learning
Representations,2021.
[60] VBMountcastle. Thecolumnarorganizationoftheneocortex. Brain,120(4),1997.
[61] ChristophBoeddeker,PatrickHanebrink,LukasDrude,JahnHeymann,andReinholdHaeb-Umbach. On
thecomputationofcomplex-valuedgradientswithapplicationtostatisticallyoptimumbeamforming.arXiv
preprintarXiv:1701.00392,2017.
[62] JamesBradbury,RoyFrostig,PeterHawkins,MatthewJamesJohnson,ChrisLeary,DougalMaclaurin,
GeorgeNecula,AdamPaszke,JakeVanderPlas,SkyeWanderman-Milne,andQiaoZhang. JAX:compos-
abletransformationsofPython+NumPyprograms,2018. URLhttp://github.com/google/jax.
[63] JonathanHeek,AnselmLevskaya,AvitalOliver,MarvinRitter,BertrandRondepierre,AndreasSteiner,
andMarcvanZee. Flax:AneuralnetworklibraryandecosystemforJAX,2023. URLhttp://github.
com/google/flax.
13[64] NicolasZucchet,RobertMeier,andSimonSchug. MinimalLRU,2023. URLhttps://github.com/
NicolasZucchet/minimal-LRU.
[65] WikimediaFoundation. WikimediaDownloads. URLhttps://dumps.wikimedia.org.
[66] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of Deep
BidirectionalTransformersforLanguageUnderstanding. InJillBurstein,ChristyDoran,andThamar
Solorio,editors,Proceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics:HumanLanguageTechnologies,Volume1(LongandShortPapers),2019.
14Appendix
Table of contents
A Theory 16
A.1 Usefullemmas . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.2 Thecurseofmemory:signalpropagationanalysis . . . . . . . . . . . . . . . . 16
A.2.1 Forwardpass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.2.2 Backwardpass . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.2.3 Extensiontofully-connectednetworks . . . . . . . . . . . . . . . . . . 19
A.3 Impactofinputnormalizationandparametrization . . . . . . . . . . . . . . . . 20
A.3.1 Realcase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
A.3.2 Onthedifficultyofparametrizingcomplexnumbers . . . . . . . . . . . 20
B Linearteacher-studenttask 21
B.1 1Dsetting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.1.1 Calculationoftheloss. . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.1.2 Optimalnormalizationandreparametrizationwithuncorrelatedinputs . . 22
B.1.3 Visualizationoftheeffectofinputnormalizationandreparametrization . 23
B.1.4 Learningtheangleisdifficultinpractice:anexample . . . . . . . . . . 24
B.2 StructureoftheHessianatoptimality . . . . . . . . . . . . . . . . . . . . . . . 25
B.2.1 Hessianforcomplex-valuedvariables . . . . . . . . . . . . . . . . . . . 25
B.2.2 Hessianwithrespecttotherecurrenteigenvalues . . . . . . . . . . . . . 26
B.2.3 Hessianfordifferentparametrizations . . . . . . . . . . . . . . . . . . 28
B.3 Experimentaldetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
B.4 Additionalanalyses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
B.4.1 StructureofthelosslandscapeforLRUs . . . . . . . . . . . . . . . . . 31
B.4.2 Concentratingeigenvaluedistributions . . . . . . . . . . . . . . . . . . 31
C Signalpropagationindeeprecurrentneuralnetworksatinitialization 32
C.1 Experimentalsetup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
15A Theory
Thissectionintroducesallthetheoreticalresultswedirectlyorindirectlymentioninthemaintext,as
wellasprovidesaproofforthem.
A.1 Usefullemmas
Most,ifnotallthecalculations,thatwewillbedoinginthissectioninvolvesinfinitesums. Westate
andprovetwousefullemmastosimplifylatercalculations.
Lemma1. Forα,β ∈Csatisfying|α|<1and|β|<1,and(u n) n∈Zaboundedsequencesatisfying
u =u ,wehave
−n n
 
(cid:88) 1 (cid:88)
αnβnu n−m = 1−αβ u 0+ (α∆+β∆)u ∆ (12)
n,m≥0 ∆≥1
Proof. Theproofnaturallycomesfromseparatingtheindicesnandminthreesets: oneinwhich
thetwoareequals,oneinwhichnislargerandoneinwhichmislarger. Thisgives
(cid:88) (cid:88) (cid:88) (cid:88)
αnβmu = αnβmu + αnβmu + αnβmu (13)
n−m n−m n−m n−m
n,m≥0 n=m n>m n<m
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
= αnβnu + αmβm α∆u + αnβn β∆u (14)
0 ∆ −∆
n m ∆≥1 n ∆≥1
 
(cid:88) (cid:88)
= αnβn 1+ (α∆+β∆)u ∆ (15)
n ∆≥1
 
1 (cid:88)
=
1−αβ
1+ (α∆+β∆)u ∆ (16)
∆≥1
Lemma2. InthesameconditionsasLemma1,wehave
  
(cid:88) d d 1 (cid:88)
nmαn−1βm−1u n−m = dαdβ  1−αβ u 0+ (α∆+β∆)u ∆ (17)
n,m≥0 ∆≥1
Proof. Thisfollowsfromremarkingthat
   
d d (cid:88) d (cid:88)
dαdβ
 αnβmu n−m= dα mαnβm−1u n−m (18)
n,m≥0 n,m≥0
(cid:88)
= nmαn−1βm−1u (19)
n−m
n,m≥0
andusingLemma1togetthefinalresult.
A.2 Thecurseofmemory: signalpropagationanalysis
WerecalltheassumptionsthatwestatedinSection2.2:
a) Linear diagonal recurrent neural networks. We restrict ourselves to networks satisfying
h =λ⊙h +x withλ,h andx complexnumbers. Withoutlossofgenerality,wefocus
t+1 t t+1 t t
ontheonedimensionalsetting. Weadditionallyconsiderλswithabsolutevaluessmallerthan1.
b) Infinitetimehorizon. Weconsiderinfinitesequencesandinitializethenetworkdynamicsat
t =−∞.
0
16c) Wide-sensestationarity. Weassumethedifferentquantitiesthatthenetworkreceives,which
includestheinputsx ,tobewise-sensestationary(WSS).ArandomprocessX issaidtobe
t t
WSSifitsauto-correlationfunctionisindependentoftime, thatis,forallt ∈ Rand∆ ∈ R,
E(cid:2) X X¯ (cid:3) :=R (∆).
t+∆ t X
A.2.1 Forwardpass
Without loss of generality, we can take t = 0 given the wide-sense stationarity and infinite time
horizonassumptions. Letusfirstremarkthatwehave
(cid:88)
h = λnx (20)
0 −n
n≥0
sothat
E[|h |2]= (cid:88) λnλ¯mE[x x¯ ] (21)
0 −n −m
n,m≥0
= (cid:88) λnλ¯mR (n−m) (22)
x
n,m≥0
 
= 1−1
|λ|2
R x(0)+ (cid:88) (λ¯∆+λ∆)R x(∆). (23)
∆≥1
WeusedLemma1toobtainthelastequality. InSection2.2,wefocusedontherealcaseλ¯ = λ,
sothisformulabecomesEquation5. Ifwefurtherassumethattheauto-correlationofxdecreases
exponentiallywithdecayrateρ,thatisR (∆)=ρ|∆|,wecanfurthersimplifythelastexpression:
x
 
E[|h 0|2]= 1−1
|λ|2
1+ (cid:88) (λ¯∆+λ∆)ρ∆  (24)
∆≥1
1 (cid:18) λ¯ρ λρ (cid:19)
= 1+ + (25)
1−|λ|2 1−λ¯ρ 1−λρ
1−ρ2|λ|2
= (26)
|1−ρλ|2(1−|λ|2)
Itfollowsthatiftheinputsarei.i.d. (ρ=0),wehaveE[|h |2]=(1−|λ|2)−1,andiftheinputsare
0
constantequalto1(ρ=1),wehaveE[|h |2]=|1−λ|−2.
0
A.2.2 Backwardpass
Differentiatingtheupdateh =λh +x withrespecttoλgives
t+1 t t+1
dh dh
t =λ t−1 +h (27)
dλ dλ t−1
sothat
dh 0 = (cid:88) λnh (28)
dλ −n
n≥0
(cid:88) (cid:88)
= λn λmx (29)
−n−m
n≥0 m≥0
(cid:88)
= λn+mx (30)
−(n+m)
n,m≥0
(cid:88)
= nλn−1x . (31)
−n
n≥0
Notethatsomeextratechnicalitiesareneededtojustifytheseequationsasλandh arecomplex
t
valued: theseformulasholdastheywouldinthereal-valuedcaseash isanholomorphicfunctionof
t
λ.
17Wecannowcomputethevarianceofthesensitivityofthehiddenstatewithrespecttotheparameters.
E(cid:34)(cid:12)
(cid:12) (cid:12) (cid:12)d dh
λt(cid:12)
(cid:12) (cid:12)
(cid:12)2(cid:35)
= (cid:88) (cid:88) nmλn−1λ¯m−1R x(n−m). (32)
n≥0m≥0
UsingLemma2gives
E(cid:34)(cid:12)
(cid:12) (cid:12) (cid:12)d dh
λt(cid:12)
(cid:12) (cid:12)
(cid:12)2(cid:35)
= dd αdd
β

 1−1
αβ

R x(0)+ (cid:88) (α∆+β∆)R
x(∆) 
 . (33)
∆≥1 α=λ,β=λ¯
Differentiatingthisquantityasaproductgives
E(cid:34)(cid:12)
(cid:12) (cid:12) (cid:12)d dh
λt(cid:12)
(cid:12) (cid:12)
(cid:12)2(cid:35) =
 (11 −+ αα ββ
)3

R x(0)+ (cid:88) (α∆+β∆)R
x(∆)
+0
∆≥1
   
α (cid:88) β (cid:88)
+
(1−αβ)2
 ∆α∆−1R x(∆)+
(1−αβ)2
 ∆β∆−1R x(∆) , (34)
∆≥1 ∆≥1 α=λ,β=λ¯
whichthensimplifiesas
E(cid:34)(cid:12) (cid:12) (cid:12) (cid:12)d dh λt(cid:12) (cid:12) (cid:12) (cid:12)2(cid:35) = (1 1+ −|| λλ || )2
3
 R x(0)+ (cid:88) (λ∆+λ¯∆)R x(∆) 
∆≥1
 
+ (1−1
|λ|2)2
(cid:88) ∆(λ∆+λ¯∆)R x(∆). (35)
∆≥1
NotethatEquation6inthemaintextisthereal-valuedversionofthatformula.
LetusnowfurthersimplifythisequationwhenR (∆)=ρ|∆|. Ifweusethisinthedifferentiated
x
quantitybeforedifferentiatingit,weget
E(cid:34)(cid:12) (cid:12) (cid:12)dh t(cid:12) (cid:12) (cid:12)2(cid:35)
=
d d (cid:20) 1 (cid:18) 1−ρ2αβ (cid:19)(cid:21)
. (36)
(cid:12)dλ(cid:12) dαdβ 1−αβ (1−ρα)(1−ρβ)
α=λ,β=λ¯
Calculatingthisquantitymanuallyispainful. Instead,weusethefollowingtrick. Itsdenominator
israthereasytocompute,itisequalto(1−αβ)3(1−ρα)2(1−ρβ)2. Wethusmultiplyittothe
derivativeofthefunctionwewanttocomputeinordertoobtainapolynomialwithunknownfactors,
and use polynomial regression tools to derive the resulting coefficients. Massaging the obtained
expressiontomakeiteasiertocomputetheclosed-formvalueofthisquantitywhenρ=0andρ=1,
weget
E(cid:34)(cid:12) (cid:12) (cid:12)dh t(cid:12) (cid:12) (cid:12)2(cid:35)
=
(1−ρ)(1+|λ|2)+ρ2(1−|λ|2)3+ρ(1−ρ)|λ|2(ρ|λ|2(1+|λ|2)−2λ−2λ¯)
.
(cid:12)dλ(cid:12) (1−|λ|2)3|1−ρλ|4
(37)
ThisisthequantityweplotonFigure1.A,whenλisreal-valued. Whenρ=0,thisquantitybecomes
E(cid:34)(cid:12) (cid:12) (cid:12)dh t(cid:12) (cid:12) (cid:12)2(cid:35)
=
1+|λ|2
, (38)
(cid:12)dλ(cid:12) (1−|λ|2)3
anditisequalto
E(cid:34)(cid:12)
(cid:12) (cid:12)dh
t(cid:12)
(cid:12)
(cid:12)2(cid:35)
=
1
, (39)
(cid:12)dλ(cid:12) |1−λ|4
whenρ = 1. Additionally,itwilldivergewhenever|λ| → 1whenρ < 1,andwhenλ → 1when
ρ=1.
Regardingthebackpropagationoferrorstotheinputs,theanalysiswedidinthemaintextalsoholds
forcomplexnumbergiventhath isanholomorphicfunctionofx anditthusbehavesastheforward
t t
passoncereplacingtheinputdistributionwiththeoneofoutputerrors∂ L .
ht t
18A.2.3 Extensiontofully-connectednetworks
Wenowturntothenon-diagonalcase. Forthesakeofsimplicity,weassumethatrecurrentmatrixis
complexdiagonalizableandthatitseigenvaluesarealldifferent. Thiswillenableustodifferentiate
theeigenvaluesandtheeigenvectors. Weconsiderdynamicsoftheform
h =Ah +x (40)
t+1 t t+1
AsAiscomplexdiagonalizable,thereexistsacomplex-valuedmatrixP andacomplex-valuedvector
λsuchthat
A=Pdiag(λ)P−1 (41)
P†P =1 ∀i. (42)
:i :i
Thelinearrecurrentneuralnetworkconsideredaboveisequivalenttoitsdiagonalversion
hdiag =λhdiag+P−1x (43)
t+1 t t+1
h =Phdiag. (44)
t t
Wenowdifferentiateh w.r.t. toAusingthediagonalparametrizationandobtain
t
dh ∂h ∂P ∂h dhdiag ∂λ ∂h dhdiag∂P−1
t = t + t t + t t . (45)
dA ∂P ∂A ∂hdiag dλ ∂A ∂hdiag dP−1 ∂A
t t
d P,d P−1andd λcanbeconsideredconstant. Intuitively,theeigenvaluesandeigenvectors
A A A
movesmoothlyaswerestrictedourselvestothecaseinwhicheigenvaluesaresingular. Ifthisis
notthecase,mathbecomestrickierastheeigenvectorsarenotuniquelydefined. Wecanstudythe
behaviorofthosequantitiesinmoredetail,followingBoeddekeretal.[61]:
(cid:18) (cid:19)
∂λ ∂A
=diag P−1 P (46)
∂A ∂A
ij ij
(cid:18) (cid:18) (cid:19)(cid:19)
dP ∂A
=P F ⊙ P−1 P (47)
dA ∂A
ij ij
TheF introducedinthelastequationisequalto
(cid:26) 1 if i̸=j
F
ij
:= 0λj−λi
otherwise.
(48)
Importantly,thosetwoquantitiesdonotgrowtoinfinityastheabsolutevalueoftheeigenvaluesgoes
to1,whichmeansthatwecanconsiderthosederivativestobeindependentof|λ|forthesakeofour
analysis. Notethatthepreviousargumentassumesthateigenvaluesdonotcollapse.
d hdiagisthedominatingtermind h . Wewonderwhichofthethreedifferenttermsthatappear
λ t A t
ind h (Equation45)willbethedominatingoneas|λ|(orλ)goesto1. Inthepreviousparagraph,
A t
wehaveshownthatthederivativeofP−1,P andλcanbeconsideredconstantforthesakeofour
analysis. Wethusfocusontheotherterms.
First,wehave
∂h
t,l =hdiag1 (49)
∂P t,i j=l
ij
sothemagnitudeofthisquantityisroughlytheoneofhdiag, whichcorrespondstothelowpass
t
filteringoftheinputswithdifferentλvalues.
Second,weknowthat∂ h doesnotchangeinmagnitudeasλchanges,asP remainsbounded.
hdiag t
t
So,forthethirdtermofthesum,wearelefttostudythebehaviorofd hdiag. Wecanshowthatit
P−1 t
evolvesaccordingto
dhdiag dhdiag
t+1,k =λ t+1,k +x if k =i (50)
dP−1 i dP−1 t+1,j
ij ij
dhdiag
t+1,k =0 otherwise. (51)
dP−1
ij
19Itfollowsthatthethirdterminthesumalsocorrespondstoalowpassfilteringoftheinputs.
Finally,weknowthatthesecondterm,theoneind hdiagwillgrowfastertoinfinityasitcorresponds
λ t
totwoconsecutivelowpassfilterswiththesameλvalues(c.f. calculationabove). Itwillthusbethe
dominatingtermintheinfinitememorylimit.
A.3 Impactofinputnormalizationandparametrization
Inthissection,weconsideradiagonallinearrecurrentneuralnetworkoftheform
h =λ(ω)h +γ(λ)x (52)
t+1 t t+1
with γ(λ) the input normalization factor and λ parametrized by a vector ω. Next, we study the
effectofinputnormalizationandreparametrization,firstinthereal-valuedsettingandtheninthe
complex-valuedone.
A.3.1 Realcase
Letusstartwiththeforwardpass: as
(cid:88)
h = λnγ(λ)x , (53)
t t−n
n≥0
γ rescalesthevaluethehiddenstatetakes. Toavoidanyexplosionbehavior,wethusideallywantγ
tobetheinverseofvalueofE[(h )2]withoutnormalization,whichwehavecomputedinEquation23.
t
Thesamebehaviorholdsforthebackpropagationoferrorstotheinputsas
(cid:18) (cid:19)
dL dL ∂L
=γ(λ) λ + . (54)
dx dx ∂h
t t+1 t
Wenowmovetotheimpactoftheparametrization. Tosimplifythecalculation,wewillignorethe
dependencyofγ onλwhendifferentiatingit. Thiscaneasilybedoneinautomaticdifferentiation
softwarebyremovingthisdependencyfromthecomputationalgraphwithγ(stop_gradient(λ)). We
thenhave
dh dh dλ
t = t (55)
dω dλ dω
andd h whichisrescaledbyγ comparedtothecalculationwedidabove. Asaconsequence,both
λ t
theinputnormalizationandtheparametrizationcanhelptomitigatethecurseofmemory.
A.3.2 Onthedifficultyofparametrizingcomplexnumbers
Wenowextendthepreviousanalysistothecomplexcase, andtakeapolarparametrizationofλ:
λ(ω) = ν(ω)exp(iθ(ω)). Theeffectoftheinputnormalizationdoesnotchangewhenmovingto
complex numbers. The role of the reparametrization is however a bit more subtle. As h is an
t
holomorphicfunctionofλ,wehaved λ¯h
t
=0and
(cid:18) (cid:19)
dh dh dλ dh 1dν i dθ
t = t = t exp(iθ)+ ν exp(iθ) . (56)
dω dλ dω dλ 2dω 2 dω
Itfollowsthat
E(cid:34)(cid:12) (cid:12) (cid:12)dh t(cid:12) (cid:12) (cid:12)2(cid:35)
=
1 E(cid:34)(cid:12) (cid:12) (cid:12)dh t(cid:12) (cid:12) (cid:12)2(cid:35)(cid:12) (cid:12) (cid:12)dν exp(iθ)+iνdθ exp(iθ)(cid:12) (cid:12) (cid:12)2
(57)
(cid:12)dω(cid:12) 4 (cid:12)dλ(cid:12) (cid:12)dω dω (cid:12)
=
1 E(cid:34)(cid:12) (cid:12) (cid:12)dh t(cid:12) (cid:12) (cid:12)2(cid:35)(cid:12) (cid:12) (cid:12)dν +iνdθ(cid:12) (cid:12) (cid:12)2
(58)
4 (cid:12)dλ(cid:12) (cid:12)dω dω(cid:12)
=
1 E(cid:34)(cid:12) (cid:12) (cid:12)dh t(cid:12) (cid:12) (cid:12)2(cid:35)(cid:18) dν2 +ν2dθ2(cid:19)
. (59)
4 (cid:12)dλ(cid:12) dω dω
Tosimplifytheanalysis,wewillfurtherassumethatE[|d h |2]isonlyafunctionofν.Thisasumption
λ t
holdsinthecaseofρ=0andρ=1,c.f. SectionA.2.2,butnotnecessarilyotherwise. Toensurethat
20thatthisquantitydoesnotdependonλ,wethuswantd ν2E(ν)=Θ(1)andν2d θ2E(ν)=Θ(1).
ω ω
The second means that the angle parametrization must depend on the value ν takes. Let us take
the ρ = 0 example to get an idea of what the ideal parametrization should be. First, we have
√
γ(λ)= 1−ν2sothat
1+ν2 1+ν2
E(ν)=γ(λ)2 = . (60)
(1−ν2)3 (1−ν2)2
We are left with the differential equation ν′ = Θ(1 − ν2), which is for example solved with
ν =tanh(ω ). Nowletuslookattheparametrizationofθ. Ifweignoretheν2termforsimplicity,
ν
theapproximatedifferentialequationitneedstosolveisd θ = Θ(1−ν2),whichcanbesolved
ω
byθ =stop_gradient(1−ν2)ω . Theexactdetailofthiscalculationdonotreallymatterasthisis
θ
heavilyinputdistributiondependent. However,theinterestingparthereisthattheangleparameter
mustberescaledbyafunctionofν. Thismakesintuitivesensewhenlookinglookingatthesharpness
ofthelossaroundoptimalityinFigure1.C,butthisalsomakesthelossevenflatterfurtheraway
fromoptimality. WewillcomebacktothispointinSectionB.1.4,showingthatinpractice,such
aparametrizationcomplicatesthelearningoftheθ. Learningcomplexnumbersisthusdifficulty,
becauseoftheangle.
B Linearteacher-studenttask
Thissectionisdedicatedtodetailthetheoreticalresultsbehindouranalysisoftheteacher-student
task,presentallthedetailsnecessarytoreproduceourempiricalexperiments,andprovideadditional
analysis.
B.1 1Dsetting
B.1.1 Calculationoftheloss
In this toy example, we are interested in learning a simple 1-dimensional linear recurrent neural
networkwhichfollowsthedynamics
h =λh +x (61)
t+1 t t+1
toreproducethehiddenstateh∗ofateacherwithrecurrentparameterλ∗. Notethatwehereallowall
t
variablestobecomplex-valued. Wetakethelosstobe
T
L(λ,λ∗):=
1 (cid:88)
E
(cid:104)
|h
−h∗|2(cid:105)
(62)
2T x t t
t=1
Weassumextobedrawnfromawide-sensestationarydistributionsothatwecanfocusonstudying
(cid:104) (cid:105)
thebehaviorofoneL (λ,λ∗):= 1E |h −h∗|2 tounderstandthebehaviorofthefulllossL,in
t 2 x t t
thelimitofinfinitelylongsequences(T →∞). Moreover,tofurthersimplifythecalculations,we
assumethatxisreal-valuedandthatR (∆)=ρ|∆|.
x
Letusnowproceedwiththecalculation:
L (λ,λ∗):= 1 E (cid:2) h h¯ +h∗h¯∗−h h¯∗−h¯h∗(cid:3) . (63)
t 2 x t t t t t t t t
WehaveshowninSectionA.2thatinthelimitoft→∞,
E (cid:2) h h¯(cid:3) = 1 (cid:18) 1+ ρλ + ρλ¯ (cid:19) (64)
x t t 1−λλ¯ 1−ρλ 1−ρλ¯
(65)
Similarderivationsholdfortheotherthreetermsintheloss. Groupingthemgivestheexactvalue
oftheloss. Weomittheformulaasitisnotparticularlyinsightful. Inthecaseofconstantinputs
(ρ=1),wehave
L t(λ,λ∗)= 1 2(cid:12) (cid:12) (cid:12) (cid:12)1−1
λ
− 1−1 λ∗(cid:12) (cid:12) (cid:12) (cid:12)2 . (66)
21Inthecaseofi.i.d. inputs(ρ=0),wehave
(cid:18) (cid:20) (cid:21)(cid:19)
1 1 1 2
L (λ,λ∗)= + −Re . (67)
t 2 1−|λ|2 1−|λ∗|2 1−λ¯λ∗
ThisisthelossweplotonFigure1.BandC.
B.1.2 Optimalnormalizationandreparametrizationwithuncorrelatedinputs
Havingasimpleclosed-formsolutionforthevaluethelosstakesgivesusthepossibilitytoinvestigate
inmoredetailwhatanoptimalnormalizationandparametrizationshouldbe. Wefocusonthecase
ρ=0.
(cid:112)
For ρ = 0, the optimal normalization is γ(λ) = 1−|λ|2. Given that we now add an input
normalizationtothestudent,wemustalsoaddittotheteacherforthestudenttobeabletofitit. The
lossbecomes
1(cid:18) γ(λ) γ(λ∗) (cid:20) 2γ(λ)γ(λ∗)(cid:21)(cid:19)
L = + −Re (68)
t 2 1−|λ|2 1−|λ∗|2 1−λ¯λ∗
(cid:20) γ(λ)γ(λ∗)(cid:21)
=1−Re . (69)
1−λ¯λ∗
Next,weparametrizeλasλ=ν(ω )exp(iθ(ω ))andseektofindaparametrizationsuchthat,at
ν θ
optimality,E[(d h )2] = 1andE[(d h )2] = 1. Giventhatthestudentperfectlyfittheteacher-
ων t ωθ t
generateddataatoptimalityandthatthelossweuseisthemean-squarederror,thiscorrespondsto
havingd2 L =1andd2 L =1.
ων t ωθ t
Derivingtheoptimalν parametrization. WenowcomputetheHessianofthelossw.r.t. ω . First,
ν
wecansimplifyourcalculationsbyrestrictingourselvestothecaseθ =θ∗. Thelossbecomes
γ(ν)γ(ν∗)
L =1− . (70)
t 1−νν∗
Differentiatingthisfunctionafirsttime,weobtain
dL γ(ν∗)γ′(ν) γ(ν∗)ν∗γ(ν)
t =− − . (71)
dν 1−νν∗ (1−νν∗)2
Differentiatingitasecondtimegives
d2L γ(ν∗)γ′′(ν) 2γ(ν∗)ν∗γ′(ν) 2γ(ν∗)ν∗2γ(ν)
t =− − − . (72)
dν2 1−νν∗ (1−νν∗)2 (1−νν∗)3
Leveragingthefactthat
−ν −γ(ν)2−ν2
γ′(ν)= and γ′′(ν)= , (73)
γ(ν) γ(ν)3
wefinallyget,whenν =ν∗,
d2L 1
t = . (74)
dν2 (1−ν2)2
Giventhatweareatoptimality,wehave
d2L (cid:20) dh d2L dh (cid:21) (cid:20) dh d2L dh (cid:21) d2L
t =E t t t =ν′(ω )2E t t t =ν′(ω )2 t. (75)
dω2 dω dh2 dω ν dν dh2 dν ν dν2
ν ν t ν t
Tokeepthatquantityconstant,wethushavetosolvethedifferentialequation
ν′(ω )=(1−ν2), (76)
ν
whichgivesν(ω )=tanh(ω ).
ν ν
22Derivingtheoptimalθparametrization. Wenowmovetotheparametrizationofθ. Wehave
(cid:20) γ(ν)γ(ν∗)(cid:21) γ(ν)γ(ν∗)(1−νν∗cos(θ−θ∗))
L =1−Re =1− . (77)
t 1−λ¯λ∗ |1−λ¯λ∗|2
Fornotationalconvenience,wedenote
α(θ−θ∗):=|1−λ¯λ∗|2 =(1−νν∗cos(θ−θ∗))2+ν2ν∗2sin(θ−θ∗)2. (78)
Wehave
dL (cid:18) νν∗sin(θ−θ∗) (1−νν∗cos(θ−θ∗))α′(θ−θ∗)(cid:19)
t =γ(ν)γ(ν∗) − + (79)
dθ α(θ−θ∗) α(θ−θ∗)2
and
d2L (cid:18) νν∗cos(θ−θ∗) νν∗sin(θ−θ∗)α′(θ−θ∗)
t =γ(ν)γ(ν∗) − +2
dθ2 α(θ−θ∗) α(θ−θ∗)2
(1−νν∗cos(θ−θ∗))α′′(θ−θ∗) (1−νν∗cos(θ−θ∗))α′(θ−θ∗)2(cid:19)
+ −2 (80)
α(θ−θ∗)2 α(θ−θ∗)3
Atoptimality(θ =θ∗andν =ν∗),wehaveα(0)=(1−ν2)2,α′(0)=0andα′′(0)=2ν2,sothat
d2L ν2(1+ν2)
t = . (81)
dθ2 (1−ν2)2
Theoptimalparametrizationthushastosatisfy
1−ν2
θ′(ω )= √ , (82)
θ
ν 1+ν2
thatis
1−ν2
θ(ω )=ω √ (83)
θ θ
ν 1+ν2
Therearetwothingswecanremark:
– First,theparametrizationthatwederivedforthegeneralcaseinSectionA.3.2,whichadditionally
ignoredthedependenceofγ onλ,isrelativelyaccurate. Theonlydifferenceistheapparitionof
theextraν term,whichbecomesinsignificantinthelongmemorylimitν →1.
– Second,theoptimalθparametrizationhastobeafunctionofν,andthusω ,sothedifferential
ν
equationν needstosatisfychanges. Yet,thisconsiderablysimplifiesthecalculationandthereis
nosimplesolutiontothatproblem. Onecouldstillarguethattheinitialchoicewemade,thatis
touseapolarparametrization,istheissue. Itcouldbe,butmostpracticalmodelsendupusing
thatchoicesohighlightingthelimitationsofthischoicehasimportantpracticalconsequences.
Intherestofthissection,weignorethedependencyofθonν,andconsidertheoptimalparametriza-
tioninthissettingtobe
ν(ωopt)=tanh(ωopt) (84)
ν ν
1−ν2
θ(ωopt)=ωopt √ . (85)
θ θ ν 1+ν2
B.1.3 Visualizationoftheeffectofinputnormalizationandreparametrization
Wenowvisualizetheeffectofinputnormalizationandreparametrizationonthelosslandscape. We
focusontwosuchreparametrizations:
√
– the one used in the LRU [20, 22] with γ(λ) = 1−λ2, ν = exp(−exp(ωexp)) and θ =
ν
exp(ωexp).
θ
– theoptimalonewederivedinthepreviousSection(c.f. Equations84and85),whichistaylored
tothisspecificsetting.
23107
10 1 104 10 1 103
10 4 101 10 3 101
10 7 10 2 10 5 10 1
0.0 0.5 1.0 0.0 0.5 1.0 1 0 1 1 0 1
10 1 100 100 * 102 *
10 4 10 1 10 3 10 1
10 6
10 7 10 2 10 9 10 4
10 3
5 0 5 0 2 0 2 0
100 100 10 1
100
10 3 10 1 10 4
10 6 10 2 10 7 10 2
10 3
0 5 0 5 5 0 5 5 0 5
0.99 0.98 0.96 0.92 0.84 0.68 0.36
*
Figure6: Visualiz| a|tionofthelosslandscapewithinputnormalization,intheteacherandthe
student,fordifferentparametrizations. Theteachersatisfiesλ∗ =|λ∗|exp(iπ/100),fordifferent
|λ∗| values. The first two columns correspond to students with correct angle θ = θ∗ but wrong
absolute value ν and the last two columns to students with correct absolute value ν = |λ∗| but
wrongangle. Whenwefixonevariable,weignorehowitaffectsthelossfortheHessiancaclulation.
Each line corresponds to a different parametrization: the first line uses a polar parametrization
(λ=νexp(iθ)),thesecondlineusesthedoubleexponentialparametrizationusedintheLRU(exp)
andthethirdoneistheoptimalparametrizationforthattask(tanh). Overall,bothreparametrizations
enable to control the explosion of the Hessian. However, the size of basins of attraction around
optimality,ortheirnumber,shrinksas|λ∗|goesto1fortheangle,butnotfortheabsolutevalue,
highlightinghowdifficultlearningtheanglecanbe.
B.1.4 Learningtheangleisdifficultinpractice: anexample
Weusethisone-dimensionalteacher-studentsettingtotestwhetherhavingaparametrizationthat
avoidsexplodingbehaviorsatoptimality,suchastheonewederivedinSectionB.1.2,facilitates
learning. Figure6alreadyhintstowardsthefactthebasinofattractionoftheglobalminimaiseither
extremelynarroworthattheirnumberdecreasesaslongermemoriesareconsidered,makinglearning
moretedious. Figure7confirmsit. Inthisfigure,weplotthelearningdynamicsobtainedusingthe
Adamoptimizerwithalearningrateof10−3for50ksteps,startingfromλ =0.99exp(iπ/4). We
0
considerthreedifferentparametrizationsoftheangle:
θ(ωpolar)=ωpolar (86)
θ θ
θ(ωexp)=log(ωexp) (87)
θ θ
(1−ν2)
θ(ωopt)= √ ω . (88)
θ ν 1+ν2 θ
Thefirstonedoesnotreparametrizetheangle,thesecondoneistheoneusedintheLRUandthe
thirdoneistheoptimalonewederivedabove. Weuseν =tanh(ω )toparametrizethemagnitude
ν
inthethreecases. Wesetλ∗toλ∗ =0.99exp(iπ/100). Theθlandscapewhenν iscorrecttherefore
correspondstotheonesplottedinthelasttwocolumnsofFigure6. Thisexampleshowsthatefforts
toreducethesharpnessofthelossatoptimality,asdoneinthelastparametrization,inevitablymake
thelossflatterelsewhereandoptimizationimpossible.
24
ssoL
ssoL
ssoL
naisseH
naisseH
naisseH
ssoL
ssoL
ssoL
naisseH
naisseH
naisseHA B 1 C 80 102
1.00 0 60
101
0.75 1
40
0.50 2 100
20
0.25 3
10 1
0.00 4 0
5 20 10 2
1.5 2.0 2.5 3.0 1 2 3 1 2 3
101 101 101
10 1 10 1 10 1
10 3 10 3 10 3
0 20k 40k 0 20k 40k 0 20k 40k
Step Step Step
Figure7: Learningtheangleisdifficult,eveninasimpleone-dimensionaltask. Thetargetλ
valueisequaltoλ∗ =0.99exp(iπ/100)andisplottedinyellow. Theblacklinescorrespondtothe
Adamlearningdynamics. A.Whentheangleisnotreparametrized(θ =ω ),thelosslandscapeis
θ
extremelysharpintheω direction,butAdamcompensatesforit. B.Whentheangleisparametrized
θ
exponentially(θ =exp(ω )),thelosslandscapebecomessmoother. However,thisonlyholdwhen
θ
the considered angles are small enough, as the exponential parametrization does not bring extra
granularityelsewhere.C.Whenreparametrizingtheangletoreducethegradientexplosionas|λ|→1,
thelossbecomesextremelytrickytonavigate. Theparametersarefirstattractedtoanearbyvalley,
which is flat on the ω direction and only indirectly connected to the global minimum. Such a
θ
reparametrizationthushindersoptimizationfarawayfromoptimality. SeeSectionB.1.4formore
detail.
B.2 StructureoftheHessianatoptimality
InSection4,wearguethattheHessianatoptimalityisanimportantobjecttounderstandthelearning
dynamicsinthelinearteacher-studenttaskweconsider. Wehereprovidesometheoreticalanalysisof
itsstructureinthecomplexdiagonalsetting,thatisweconsiderarecurrentnetworkoftheform
h =λ⊙h +bx (89)
t+1 t t
y =Re[c⊤h ]+dx . (90)
t t t
withλ,bandccomplexvectorsofsizen,withnthenumberofhiddenneurons,anddascalar. We
additionallytakethelosstobethemean-squareerror,whichisalsotheoneweuseinournumerical
experiments. Note that, as in our theoretical analysis of Section 2, we consider infinitely long
sequencesandwide-sensestationaryinputs.
RecallthattheHessianofthelossisequalto
(cid:34) (cid:35)
d2L =(cid:88) E dh t∂2L tdh t⊤ + ∂L td2h t . (91)
dθ2 x dθ ∂h2 dθ ∂h dθ2
t t t
Atoptimality,onlythefirsttermremains,as∂ L is0foralldatapoints. Giventhatwehaveshown
ht t
earlier,e.g. inSectionA.2,thatthemostsensitiveparameterstolearnaretherecurrentonesλ,we
focusontheHessianwithrespecttotheseparametersinthefollowing.
B.2.1 Hessianforcomplex-valuedvariables
Beforedelvingintomorespecificcalculations,wemakeafewremarksonhowtodealtheHessian
whenhavingcomplex-valuedparameters. WewillmostlyleveragethefactthatthelossLisreal-
valued.
Beforethat,werecallafewfactsaboutWirtingerderivatives:
25
ssoL
ssoL– Forf(z)acomplex-valuedfunctionofz,theWirtingerderivativesaredefinedas:
(cid:18) (cid:19)
df 1 dRe[f] dIm[f]
= −i (92)
dz 2 dRe[z] dIm[z]
(cid:18) (cid:19)
df 1 dRe[f] dIm[f]
= +i . (93)
dz¯ 2 dRe[z] dIm[z]
– Wehave
df df
= . (94)
dz dz¯
– LeveragingthefactthatLisreal-valuedsothatL¯ =L,wehave
(cid:34) (cid:35)
d2L d dL⊤
= (95)
dλ2 dλ dλ
(cid:34) (cid:35)
d dL⊤
= (96)
dλ dλ¯
d2L
= (97)
dλ¯2
and, similarly, d λd λ¯L = d λ¯d λL. Secondderivativesaresymmetric, soweadditionallyhave
d λd λ¯L=d λ¯d λL⊤,whichmeansthatthecomplexHessianisaHermitianmatrix.
Takenalltogether,thisshowsthatthefullcomplexHessian,whichcontainsallcrossderivatives,has
asimilarstructuretotherealcase.
B.2.2 Hessianwithrespecttotherecurrenteigenvalues
Inthissection,wecomputethefullcomplexHessianwithrespecttotherecurrenteigenvalueλand
defertheanalysisofreparametrizationtothenextsection.
First,letusremarkthat
dL ∂L dh
t = t c⊤ t (98)
dλ ∂y dλ
t
(99)
sothat
(cid:34) (cid:35)
d2L d dh ⊤ ∂L ⊤
t = t c t (100)
dλ2 dλ dλ ∂y
t
d2h ∂L ⊤ dh ⊤ ∂2L dh
= tc t + t c tc⊤ t (101)
dλ2 ∂y dλ ∂y2 dλ
t t
Weassumedthatweareatoptimalitysothatthenetworkperfectlyfitsthetargettrajectoriesand
∂ L =0. Additionally,L isthemean-squarederrorlosssothat∂2 L =Id. Itfollowsthat
yt t t yt t
(cid:18) d2L (cid:19) (cid:32) dh ⊤ dh (cid:33)
t = t cc⊤ t (102)
dλ2 dλ dλ
ij ij
(cid:18) (cid:19) (cid:18) (cid:19)
dh dh
= c⊤ t c⊤ t (103)
dλ dλ
i j
dh dh
=c t,ic t,j. (104)
i dλ j dλ
i j
Inthelastequation,wemadeuseofthefactthattheparameterλ onlyaffectsthehiddenstateh
i t,i
andnottheothers,sod h =0ifi̸=j.
λj t,i
26Thepreviouscalculationappliedtoonesequence,wenowtaketheexpectationoverthedata:
(cid:34) (cid:35)
d2L dh dh ⊤
=(cc⊤)⊙E lim t t (105)
dλ2 x,y t→∞ dλ dλ
Notethatweintroducedaslightabuseofnotationinthepreviousequationasd h isingenerala
λ t
matrix. However,giventhatthehiddenneuronsareindependenthereduetothediagonalconnectivity,
itiseffectivelyavector,andwetreatitthatway. Letusnowcomputetheexpectation,usingsimilar
calculationtechniquestotheoneweusedinSectionA.2:
 
(cid:20) (cid:21)
E x,y tl →im ∞d dh λt i,id dh λt j,j =E  (cid:88) nλn i−1b ix −nmλm j −1b jx −m (106)
n,m≥0
 
(cid:88)
=b ib jE  nλn i−1x −nmλm
j
−1x −m (107)
n,m≥0
(cid:88)
=b b nmλn−1λm−1R (n−m) (108)
i j i j x
n,m≥0
WecannowremarkthatthisquantityisverysimilartotheonewehaveencounteredinSectionA.2,
uptothepresenceofb b ,andcanbesimplifiedusingLemma2. Forconciseness,wenoteS(λ ,λ )
i j i j
theright-handsideofthelastequationwithouttheb b factor. PuttingthisresultbackintheHessian,
i j
weget
d2L
=b b c c S(λ ,λ ) (109)
dλ dλ i j i j i j
i j
To gain further intuition of the behavior of this quantity, we take R (∆) = ρ|∆|, ρ being a real
x
number. AsimilarcalculationtotheonewedidinSectionA.2gives
(1−ρ)(1+λ λ )+ρ2(1−λ λ )3+ρ(1−ρ)λ λ (ρλ λ (1+λ λ )−2λ −2λ )
S(λ ,λ )= i j i j i j i j i j i j .
i j (1−λ λ )3(1−ρλ )2(1−ρλ )2
i j i j
(110)
Thisformulabeingstillhardtograsp,wevisualizethemagnitudeofS(λ ,λ )onFigure8. Interest-
i j
ingly,weobservethisquantityislargewhenλ andλ areconjugatetoeachotherandinputsare
i j
uncorrelated. However,aselementsintheinputsequencegetmorecorrelated(ρ→1),thiseffect
disappearsand|S|increasesasoneofthetwoeigenvaluegetscloserto1inthecomplexplane. In
bothcases,theeffectgetsamplifiedasthemagnitudeoftheeigenvalueincreases.
105
1 1 1 1 1
= . = . = . = . = .
103
0 0 0 0 0 101
1 1 1 1 1 10 1
1 0 1 1 0 1 1 0 1 1 0 1 1 0 1
Re Re Re Re Re
Figure 8: V[ i]sualization of λ[ ](cid:55)→ |S(λ,λ )| f[o ]r λ = 0.99exp[ ](iπ/4). This te[r m] measures how
0 0
"similar"eigenvaluesareintheHessian. Whenρ=0,eigenvaluesaremostly"similar"whentheyare
conjugatetoeachother. Asρincreases,thiseffectdecreasesandeigenvaluesbecomemore"similar"
ifoneofthemgetscloseto1.
Wealsoneedtocomputed λ¯d λLtogetthefullcomplexHessian.Similarlytothepreviouscalculation,
wefirsthave
dL dL¯ dL ∂L dh
t = t = t = t c¯⊤ t. (111)
dλ¯ dλ dλ ∂y dλ
t
27
mI
]
[
|)
,
,
(
|Itfollowsthat
d2L (cid:20) dh dh (cid:21)
=E t,jc¯ c t,i (112)
dλ dλ¯ dλ j i dλ
i j j i
=c c¯ b¯b S(λ ,λ¯ ). (113)
i j i j i j
UsingthesymmetrywiththecomplexHessianmatrix,wenowhaveallitscomponents.
B.2.3 Hessianfordifferentparametrizations
Sofar,wehavecomputedthecomplexHessian,whichisnotofdirectuseasweendupoptimizing
realnumbersinpractice. Here,westudytheimpactofdifferentparametrizationsofλontheHessian.
Giventhatthisparametrizationonlyaffectsλandnottheotherparametersinthenetworkandthatwe
onlyconsidertheHessianatoptimalityhere,computingtheHessianofthoseparametersreducesto
leftandrightmultiplyingtheHessianbyderivativesofλandλ¯withrespecttotheseparameters. For
futurereference,weintroduce
(cid:32) d2L d2L (cid:33) (cid:18) (cid:19)
H iλ
j
:= dλ di 2d Lλj dλ di 2d Lλ¯ j = BA ¯ij AB ¯ij
.
(114)
dλ¯ idλj dλ¯ idλ¯ j ij ij
withA :=b b c c S(λ ,λ )andB =b¯b c c¯ S(λ ,λ¯ ).
ij i j i j i j ij i j i j i j
Real-imaginaryparametrization: λ=ω +ω . Weaimatcomputingthematrix
re im
(cid:32) d2L d2L (cid:33)
HRI := dωre,idωre,j dωre,idωim,j , (115)
ij d2L d2L
dωim,idωre,j dωim,idωim,j
whichisthebuildingblocktocomputethefullHessian. First, letusremarkthatd λ = 1/2,
ωre,i i
d λ¯ =1/2,d λ =i/2andd λ¯ =−i/2. Itfollowsthat
ωre,i i ωim,i i ωim,i i
d2L
=(d λ d λ¯ )Hλ(d λ d λ¯ )⊤ (116)
dω dω ωre,j j ωre,j j ij ωre,i i ωre,i i
re,i re,j
1
= (1 1)Hλ(1 1)⊤ (117)
4 ij
1
= (Re[A ]+Re[B ]). (118)
2 ij ij
Onceagainweemphasizethatthefirstlineonlyholdsasweareatoptimality. Similarcalculations
givetherestoftheelementsofHRI:
ij
(cid:18) (cid:19)
1 Re[A +B ] Im[−A +B ]
HRI := ij ij ij ij . (119)
ij 2 Im[−A ij −B ij] Re[−A ij +B ij].
GiventheintuitionwegainedonthestructureofS previously, andthefactthatA ∝ S(λ ,λ )
ij i j
andB ∝S(λ ,λ¯ ),weknowthatthisblockwillhavelargecomponentsifthetwocorresponding
ij i j
eigenvaluesareconjugateofeachotheroralignedtoeachother,orifoneofthemiscloseto1.
OneotherquantitythatwecancalculateisthetraceoftheHessianHRI,whichisequaltothesumof
itseigenvalues. NotethatthisdoesnotcorrespondtotheeigenvaluesofthefullHessianmatrix,asit
additionallycontainsentriesforotherparameters. Yetitalreadyprovidessomeideaofhowlargethe
Hessianwillbe,asthevalueofthistraceappearsinthevalueofthefulltrace. Wehave
(cid:88)1
Tr[HRI]= (Re[A +B ]+Re[−A +B ]) (120)
2 ii ii ii ii
i
(cid:88)
= Re[B ] (121)
ii
i
=(cid:88) |b |2|c |2S(λ ,λ¯ ) (122)
i i i i
i
whereweusedthatS(λ ,λ¯ )isreal-valuedinthelastline. Asasidenote,thisformulapartlyjustifies
i i
whystudyingtheexpectedsquaredmagnitudeofd h inSection2makesgeneralsense,as
λ t
E(cid:34)(cid:12)
(cid:12) (cid:12) (cid:12)d dh
θt,i(cid:12)
(cid:12) (cid:12)
(cid:12)2(cid:35)
=|b i|2S(λ i,λ¯ i). (123)
28Magnitude-angle parametrization: λ = ν(ω )exp(iθ(ω )). The calculations for this
ν θ
parametrizationaresimilartothepreviousone,withthefollowingdifferences:
dλ ν′(ω )exp(iθ(ω ))
= ν θ (124)
dω 2
ν
dλ¯ ν′(ω )exp(−iθ(ω ))
= ν θ (125)
dω 2
ν
dλ iν(ω )θ′(ω )exp(iθ(ω ))
= ν θ θ (126)
dω 2
θ
dλ¯ iν(ω )θ′(ω )exp(−iθ(ω ))
=− ν θ θ . (127)
dω 2
θ
Aftersomecalculationsweobtain
d2L ν′(ω )ν′(ω )
dω dω
= ν,i
2
ν,j Re[ei(θ(ωθ,i)+θ(ωθ,j)A
ij
+ei(θ(ωθ,i)−θ(ωθ,j))B ij] (128)
ν,i ν,j
d2L ν′(ω )ν(ω )θ′(ω )
dω dω
= ν,i 2ν,j θ,j Im[−ei(θ(ωθ,i)+θ(ωθ,j)A
ij
+ei(θ(ωθ,i)−θ(ωθ,j))B ij] (129)
ν,i θ,j
d2L ν(ω )θ′(ω )ν′(ω )
dω dω
= ν,i 2θ,i ν,j Im[−ei(θ(ωθ,i)−θ(ωθ,j)A
ij
+ei(θ(ωθ,i)−θ(ωθ,j))B ij] (130)
θ,i ν,j
d2L ν(ω )θ′(ω )ν(ω )θ′(ω )
dω dω
= θ,i θ,i
2
θ,j θ,j Re[−ei(θ(ωθ,i)+θ(ωθ,j)A
ij
+ei(θ(ωθ,i)−θ(ωθ,j))B ij]
θ,i θ,j
(131)
B.3 Experimentaldetails
WerecallthesetupweconsiderinSection4:
h =Ah +Bx and y =Ch +Dx . (132)
t+1 t t+1 t t t
withh ∈Rn,x ∈Rdrawni.i.d. fromN(0,1),A∈Rn×n,B ∈Rn×1,C ∈R1×nandD ∈R1×1.
t t
WedrawB,C andDfromtruncatednormaldistributionswithfan_inscaling. Wedraweachentryof
√
AfromN(0,1/ n)andthenapplythefollowingpostprocessingtoit: Firstwecomplexdiagonalize
A,whichwecandoalmostsurely. Noteλitseigenvalues. Wethentransformthemaccordingto
(cid:18) (cid:19)
angle(λ)
λ←(ν+(1−ν)tanh(|λ|))exp i θ (133)
π 0
withνandθ twoscalarsthatwecontrol. Thistransformationhasseveralbenefits: weareguaranteed
0
thatthemagnitudeofλiswithin[ν,1](andin[ν,ν+(1−ν)tanh(1)]inthelimitn → ∞asthe
eigenvaluesofAstaywithintheunitcircleinthatlimit),andconjugatepairsofeigenvaluesremain
conjugate. Thislastpointensuresthattheresultingmatrixremainsrealwithouthavingtochangethe
eigenvectors.
WeimplementourexperimentsinJAX[62],usingthedefaultFlax[63]implementationofRNNsand
theLRUimplementationofZucchetetal.[64]. WeinitializeRNNsinthesamewayweinitializedthe
teacher,andinitializetheeigenvaluesoftheLRUandothercomplex-valuednetworkswithmagnitude
in[ν,1]andanglewithin[−θ ,θ ].
0 0
Giventhatweareinterestedintheoptimizationpropertiesofthedifferentarchitectures,weonly
reporttraininglossesanddonotperformanycrossvalidation.
Hereareadditionaldetailsrelatedtothedifferentfigures:
– Figure3: seeTables1and2.
– Figure4: forpanelsAandB,weuseν =0.99anddrawAinaslightlydifferentmannertothe
onedescribedabove(wedirectlydrawtheeigenvaluesandeigenvectorssothatwehavetwopairs
ofcomplexeigenvalues). WeuseautomaticdifferentiationtocomputetheHessian. ForpanelsC
andD,weusethesamesetupasdescribedinTable2,butkeepthelearningrateconstantoverthe
courseoflearning. Wereporttheeffectivelearningrateattheendoflearning.
29RNN LRU
Batchsize 128
Sequencelength 300
Hiddenneurons(teacher) 10
Input/outputdimension 1
ν {0.32,0.68,0.84,0.92,0.96,0.98,0.99}
θ π
0
Hiddenneurons(student) 64
loglearningrate [−5,−4.5,−4,−3.5,−3,−2.5] [−2.5,−2,−1.5,−1,−0.5]
Optimizer(schedule) Adam(cosine)
Initialization [ν teacher,ν =0] ν teacher
Numberiterations 10k
Seeds 10
Table1: ExperimentaldetailsforFigure3.A.Weuse[···]todenotehyperparametersthatwere
scannedoverwithgridsearchand{···}todenotethevariablesofinterestforthefigure. Wechose
thelearningratesforthetwoarchitecturesonpreliminaryscansandverifiedthatnonoftheextreme
learningrateswereoptimalinthefinalscan. FortheRNN,wefoundthatinitializingwithν =0gave
betterresultsthaninitializingwiththesamedistributiontheteacherhas,soweincludedthischoicein
thescan.
RNN/BLOCKDIAG. RNN COMPLEXDIAG. RNN/LRU
Batchsize 128
Sequencelength 300
Hiddenneurons(teacher) 10
Input/outputdimension 1
ν 0.99
θ π
0
Hiddenneurons(student) 64/64and128 64
loglearningrate [−5,−4.5,−4,−3.5,−3,−2.5] [−2.5,−2,−1.5,−1,−0.5]
Optimizer(schedule) Adam(cosine)
Initialization [ν teacher,ν =0] ν teacher
Numberiterations 10k
Seeds 10
Table2: ExperimentaldetailsforFigure3.B.Weuse[···]todenotehyperparametersthatwere
scannedoverwithgridsearchand{···}todenotethevariablesofinterestforthefigure. Wechose
thelearningratesforthetwoarchitecturetypesonpreliminaryscansandverifiedthatnonofthe
extremelearningrateswereoptimalinthefinalscan. FortheRNN,wefoundthatinitializingwith
ν =0gavebetterresultsthaninitializingwiththesamedistributiontheteacherhas,soweincluded
thischoiceinthescan. FortheRNNs,weused64neuronsforthe"RNN"entry,64forthe"block
diagonal"one,and128forthe"moreneurons"one.
– Figure 10: for panels A, B and C, we draw the magnitude and angle of 10 λ independently,
uniformlyin[ν,1+ν]and[−θ ,θ ]. Importantly,thismeansthattherearenoconjugatepairs,
2 0 0
whichleadstomorediagonalHessianmatricesatoptimalitythaninFigure4. ForpanelD,see
Table3.
Asaruleofthumb,eachLRU(orcomplex-valueddiagonalnetwork)experimenttakes3minuteson
aconsumer-scaleGPU(NVIDIAGeForceRTX3070)andeachRNNexperimenttakes10minutes
on a CPU. The scans behind the results reported in the different figures require on the order of
fewhundredsruneach. Includingourpreliminaryexploration,theresultswereportinthissection
required30daysofcompute,onethirdofitonGPUsandtwothirdsonCPUs.
30RNN COMPLEXDIAG. RNN/LRU
Batchsize 128
Sequencelength 300
Hiddenneurons(teacher) 10
Input/outputdimension 1
ν 0.99
log(θ /π) {−2,−1.5,−1,−0.5,0}
0
Hiddenneurons(student) 64
loglearningrate [−4.5,−4,−3.5,−3] [−3.5,−3,··· ,−0.5,0]
Optimizer(schedule) Adam(cosine)
Initialization [ν teacher,ν =0]+θteacher ν teacher+θteacher
Numberiterations 10k
Seeds 10
Table3: ExperimentaldetailsforFigure10. Weuse[···]todenotehyperparametersthatwere
scannedoverwithgridsearchand{···}todenotethevariablesofinterestforthefigure. Wechose
thelearningratesforthetwoarchitecturesonpreliminaryscansandverifiedthatnonoftheextreme
learningrateswereoptimalinthefinalscan. FortheRNN,wefoundthatinitializingwithν =0gave
betterresultsthaninitializingwiththesamedistributiontheteacherhas,soweincludedthischoicein
thescan.
B.4 Additionalanalyses
B.4.1 StructureofthelosslandscapeforLRUs
Inthemaintext, weonlyprovideananalysisofthelosslandscapeforthefullyconnectedlinear
recurrentneuralnetworkanditscomplex-valueddiagonalcounterpart. Weherecompletethisresult
byperformingthesameanalysisfortheLRU.
A BreBⁱmCreCⁱmD
106
103
100
Bre 0
Bⁱm 100
Cre 103 B
Cⁱm 10 3
D 106
10 4
D 1 10 5
0
1
BreBⁱmCreCⁱm D
107
10 4
0 10 20 30
Index eigenvalue
Figure9: EquivalentofFigure4fortheLRU.Theexponentialparametrizationofthemagnitude
ν = exp(−exp(ω )) efficiently mitigates the Hessian explosion but not the one of the angle
ν
θ =exp(ω ),consistentlywiththetheoreticalandempiricalevidencewehaveaccumulatedsofar.
θ
B.4.2 Concentratingeigenvaluedistributions
Thegoalofthisexperimentistobetterunderstandhowtheconcentrationofeigenvaluesλaffect
the learning dynamics. For fully connected RNNs, there is no reason to expect a major change
in behavior. However, it is different for diagonal RNNs. The theoretical analysis we have done
inSectionB.2providesuswiththefollowinginsights. Whentheelementsintheinputsequence
31
naisseH
01
poT
eulavnegiE
.tcev
.gie
RL
evitceffEare uncorralated, as it is the case here, the entries in the Hessian corresponding to two different
eigenvaluesincreaseiftheyarealignedorconjugatetoeachother,andiftheirmagnitudeislarge. We
thereforeexpectthat,astheintervalonwhichtheangleoftheteacher’seigenvaluesshrinks(θ →0),
0
thoseeigenvalueswillbemorelikelytobe"similar"toeachother. Thisresultsinlargenon-diagonal
terms,asweconfirminFigure10.A,BandC.TheLRUsufferslessfromthisproblemthankstoits
reparametrization,whichreducestheoverallmagnitudeofHessianentriesrelatedtothemagnitude,
andpartlytheoneofangleparameters(whenitisasmallpositivenumber). Asaconsequence,the
performancebetweenthesetwoarchitecturesincreasesasθ →0,asseenonFigure10.D.
0
A B C
re im re im re im
= / 108 = / 106 = 104
re 104 re 103 re 102
100 100 100
0 0 0
100 100 100
im 104 im 103 im 102
108 106 104
D 1 D 1 D 1 D
0 0 0
1 1 1
109
106 104
103 105 103
104
0 10 0 10 0 10
Index eigenvalue Index eigenvalue Index eigenvalue
D
10 1
RNN 10 3
Complex RNN
LRU
10 5
10 2 10 1 100
Figure10:ConcentratingeigenvaluesmaketheHessianlessdiagonal(θ →0)andconsequently
0
increasesthegapbetweentheLRUandthecomplex-valueddiagonalRNN.A,B,C.Hessian
ofthelosswithrespecttotheλparametersinthecomplex-valueddiagonalRNN.TheHessianis
computedthroughthetheoreticalformulaofEquation119; computingitnumericallymarginally
affectstheresults. ConsistentlywiththeintuitionwedevelopedinSectionB.2,concentratingthe
eigenvaluesaffectthestructureofthelosslandscape. ItmakestheHessianatoptimalitylessdiagonal
andAdamcannotefficientlycompensateit. TheLRUdoesnotsufferasmuchfromthisproblem,and
thegapbetweenthetwoarchitecturewidensasθ →0.
0
C Signalpropagationindeeprecurrentneuralnetworksatinitialization
C.1 Experimentalsetup
WeheredetailtheexperimentalsetupweusedinSection5. Wetake1024randomsequencesfrom
theWikipediadataset[65]andpassitthroughtheBERT[66]tokenizerandembeddinglayer. This
providesuswithadatasetof1024examples,wecuttheirlengthat512. Eachembeddinghas724
features.
Weconsidernetworkswith4blocksofthefollowingstructure: arecurrentlayer,anon-linearity,a
gatedlinearunit[57,GLU]andaskipconnection. Bydefault,wedonotuseanynormalizationlayer,
32
naisseH
01
poT
eulavnegiE
.tcev
.gie
ssoLbutwhenwedo,asinFigure5.C,weincludeonenormalizationlayerbeforetherecurrentlayerand
anotheronebeforetheGLU.Allthelayersinvolveduse256neurons. Weadditionallyaddalinear
encoderatthebeginningofthenetwork,andalineardecoderattheend.
InFigure5wevaryν,whichcontrolsthemagntitudeoftheeigenvaluesoftherecurrentJacobian.
Moreprecisely,wesamplethosemagnitudesintheinterval[ν,(1+ν)/2]. Forthecomplex-valued
diagonal RNN and the LRU, we use the LRU initialization. For the LSTM, we use the chrono
initializationofTallecandOllivier[29]: itinitializesthebiasoftheforgetandinputgatessuchthat,
whentheinputxandthehiddenstatehareequalto0,thetimeconstantassociatedtof isuniformly
sampledfrom[ 1 , 2 ]andtheinputgateiisequalto1−f.
1−ν 1−ν
Thelossthatweuseisanext-tokenmean-squarederror,thatis
1
L = ∥xˆ (x )−x ∥2 (134)
t 2 t 1:t−1 t
withxˆ (x )thepredictionofthenetwork. ThequantitiesreportedinFigure5aretheaverage
t 1:t−1
squaredvaluethehiddenstateorthegradienttakes. Theaverageistakenoverallthesequences,but
alsooverallneurons/parametersandoveralltimesteps. Gradientsarecomputedonbatchesofsize
8.
33