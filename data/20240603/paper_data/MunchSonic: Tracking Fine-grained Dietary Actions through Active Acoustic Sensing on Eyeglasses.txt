MunchSonic: Tracking Fine-grained Dietary Actions through
Active Acoustic Sensing on Eyeglasses
SaifMahmud DevanshAgarwal AshwinAjit
CornellUniversity CornellUniversity CornellUniversity
Ithaca,NY,USA Ithaca,NY,USA Ithaca,NY,USA
sm2446@cornell.edu da398@cornell.edu aa794@cornell.edu
QikangLiang ThaliaViranda Fran√ßoisGuimbreti√®re
CornellUniversity CornellUniversity CornellUniversity
Ithaca,NY,USA Ithaca,NY,USA Ithaca,NY,USA
ql75@cornell.edu tv74@cornell.edu francois@cs.cornell.edu
ChengZhang
CornellUniversity
Ithaca,NY,USA
chengzhang@cornell.edu
ABSTRACT smartwatches[63],requiresensingmodalitiesatmultiplebodylo-
WeintroduceMunchSonic,anAI-poweredactiveacousticsensing cationstoachievethis,whichcanbecostly,power-inefficient,and
systemintegratedintoeyeglasses,designedtotrackfine-grained cumbersomeformanyusers.
dietaryactionssuchashand-to-mouthmovementsforfoodintake, Recently,theactiveacousticsensingsystemActSonic[35]showed
chewing, and drinking. MunchSonic emits inaudible ultrasonic promisingefficacyintrackingeverydayactivities,includingidenti-
wavesfromacommodityeyeglassframe.Thereflectedsignalscon- fyingeatinganddrinkingepisodes,usingasinglepointofinstru-
taindetailedinformationaboutthepositionsandmovementsof mentationoneyeglasses.However,thissystemhasnotyetbeen
various body parts, including the mouth, jaw, arms, and hands, abletopredictfine-graineddietaryactions,suchaschewingor
allinvolvedineatingactivities.Thesesignalsareprocessedbya hand-to-mouth intake gestures, which are related to important
customdeeplearningpipelinetoclassifysixactions:foodintake, biomarkersforlongitudinalhealthmonitoring[47],suchaschew-
chewing,drinking,talking,face-handtouching,andotheractivities ingpatterns,drinkinghabits,orsnackingbehaviors.Inthispaper,
notbelongingtothetrackingset(null).Inanunconstraineduser weaimtoaddressthefollowingresearchquestion:
studywith12participants,MunchSonicachieveda93.5%macro
F1-scoreinauser-independentevaluationwitha2-secondtimeres- ‚Ä¢ Cananactiveacousticsensingsysteminstrumentedonsmart
olution,demonstratingitseffectiveness.Additionally,MunchSonic glassesbeusedtofurtherdistinguishfine-graineddietaryac-
accuratelytrackseatingepisodesandthefrequencyoffoodintake tionsinunconstrainedenvironmentsbeyondonlyidentifying
withinthoseepisodes. eatingmoments?
To address this challenge, we developed MunchSonic, a fine-
1 INTRODUCTION graineddietaryactionrecognitionsystemintheformfactorof
Trackingdietaryactivityiscrucialformeasuringnutritionlevels commodity eyeglasses using active acoustic sensing. Similar to
andmanagingchronicdiseases.However,thelackofreliableauto- ActSonic[35],oursystememploystwopairsofmicrophonesand
mateddietarymonitoringsystemsforcesmostpeopletorelyon speakers attached to the hinges of the eyeglasses. MunchSonic
manualinput-basedapplications,whicharepronetoself-reporting transmitsinaudibleultrasonicwavesthatreflectoffvariousbody
errors.Toaddressthis,researchershavedevelopedwearabledi- parts, generating a two-dimensional echo profile, which serves
etaryactivityrecognitionsystems.Despiteadvancementsinsens- asarangespectrogramofthescannedbodyparts.Thisprofileis
ingmodalitiesandmachinelearningalgorithmstoprocessthedata, thenanalyzedbyalightweightdeeplearningframeworktoinfer
trackingfine-graineddietaryactionssuchasfoodintake,chewing, fine-graineddietaryactions.Oursystemisbasedontheobserva-
anddrinking,whiledistinguishingthemfromsimilarbodymove- tionthatfine-graineddietaryactionsinvolvemovementsofthe
mentsliketalking,orfacetouching,aswellasfromnon-eating mouth,jaw,andarms,whichcanbeinferredfromreflectedultra-
activities,remainsasignificantchallengeforthewearablecomput- sound,asshowninpriorwork[31,32,34].Therefore,webelieveit
ingresearchcommunity. ispossibletodistinguishtheseactionsfromthereflectedsignals
Dietaryactivitiessuchaseatinganddrinkingrequirecoordi- beyondmerelyidentifyingeatingordrinkingmoments[35].We
natedhand,jaw,andmouthmovements.Detectingfine-grained evaluatedoursysteminauserstudywith12participantsinun-
dietaryactionsnecessitatesthesimultaneoustrackingofbothhand constrainedenvironmentsoftheirchoice.Theresultsshowedthat
andmouthmovements.Currentstate-of-the-artdietarymonitor- MunchSoniccanaccuratelyrecognizefine-graineddietaryactions
ingsystems,whetheroneyeglasses[4,36,45],earables[10],or such as hand movements for intake, chewing, and drinking, as
4202
yaM
13
]CH.sc[
1v40012.5042:viXraSaifMahmudetal.
wellasnon-dietaryactionsliketalking,facetouching,andother Insummary,comparedtothesepriorwork,MunchSonicisthe
activities(null),achievinganF1-scoreof93.5%. firsttoinferfine-graineddietaryactionsbyemployingactiveacous-
ThecontributionsoftheMunchSonicsystemareasfollows: ticsensingoneyeglassestocaptureinformationrelatedtofine-
grainedjawandhandmovements.Thisapproachallowsforde-
taileddietaryactionextraction,suchasintakeandchewing,unlike
othersystemssummarizedinTable1,whichprimarilytrackthe
‚Ä¢ Advancedtheknowledgeofwearable-basedactiveacoustic
momentsofeatingactivities.
sensing by demonstrating the feasibility of using active
acousticsensingonglassestorecognizefine-graineddi-
etaryactionsbeyondonlyidentifyingeatingmoments. 3 SYSTEMIMPLEMENTATION
‚Ä¢ Evaluationoftheproposedsystemthrougha12-participant ThegoalofMunchSonicistocapturetheinformationrelatedto
userstudyinunconstrainedenvironments,withground movementsonmultiplebodypartsinvolvedindietaryactivities
truthdataannotatedforeachsecond. usinganeyeglassesformfactor.Previousresearchhasshownac-
tive acoustic sensing‚Äôs efficacy in tracking facial muscle move-
ments[31,32,62],upperbodylimbs[34],respiration[55],sign
languagegestures[25,26],andeverydayactivities[35].Inspired
2 RELATEDWORK by these prior work, especially [35], MunchSonic integrates ac-
tiveacousticsensingintoeyeglassestotrackthesefine-grained
Variouswearabledeviceswithdifferentsensingmodalitieshave
dietaryactions.Thissectiondiscussestheactiveacousticsensing
beenproposedtotrackeatingevents.Thissectionfirstdiscusses
mechanism,thehardwareimplementation,andthedeeplearning
non-eyeglasswearables,followedbyeyeglasses.Table1summarizes
frameworkforprocessingthecaptureddata.
priorstudiesontrackingeatingactivities.
FormFactorsOtherthanEyeglasses.Sensingsystemsforde-
tectingeatingbehaviorshavebeenintegratedintovariouswearable 3.1 SensingMechanism
devices.Smartwatch-basedsystemsforeating[14,30,43,51,58,63]
The design of MunchSonic‚Äôs sensing system is inspired by the
anddrinking[23],wornontheuser‚Äôsdominanthand,mainlyuse
activeacousticsensingapproachfrom[35].Inessence,MunchSonic
InertialMeasurementUnits(IMUs)totrackhand-to-mouthges-
usesasimilaractiveacousticsensingprocessingmethod,basedon
tures.Thesesystemsfacechallengesindistinguishingsimilarhand
Cross-correlationbasedFrequencyModulatedContinuousWave
motions,leadingtoahighincidenceoffalsepositivesinreal-world
(C-FMCW)[55].Thesechirpstransmittedfromeyeglasses‚Äôhinges,
applications[11,53].Ear-worndevices,orearables,arealsousedto
withfrequencyrangesof18-21.5KHzand21.5-24.5KHzforthe
trackeatingeventsduetotheirproximitytothejaw,whichmoves
leftandrighttransmitters,respectively.Thereceiversamplesat50
duringchewing.Techniquessuchasin-earproximitysensing[7,8],
KHz,witheachchirpcontaining600samplesandasweepperiod
passiveacousticsensing[10,21],oracombinationofboth[5],have
of0.012seconds.Thesystemcandetectchangesassmallas3.43
beenemployedtomonitorchewingactivity.Neckbands[16,38,57]
mm,giventheslowspeedofmouth,jaw,andhandmovements
andnecklaces[1,12,27,46,64]usepassiveacousticandproximity
comparedtothespeedofsound(343m/s).Thesensingrangeis
sensingtotracksoundsandmotionsassociatedwithchewingand
upto2.058meters,thoughashorterrangeisusedforfine-grained
swallowing.Despitetheseadvancements,necklaces,andneckbands
dietaryactions.
havenotgainedthesamepopularityasotherwearables[3].
MunchSonic‚Äôssignalprocessingpipelineinvolvescomputing
EyeglassesFormFactor.Dietarymonitoringusingeyeglasses
thecross-correlationoftransmittedandreceivedultrasonicwaves.
isofparticularinterestbecausesensorslocatednearthemouth
Thereceivedsignalisfirstfilteredthroughabandpassfilter(18-21
andjawcancapturesignificantmovementduringeatinganddrink-
KHzand21.5-24.5KHz)toeliminateaudiblefrequencies.Thecross-
ing.Additionally,eyeglasseshavehighersocialacceptance[29]
correlationoutput,calledtheEchoProfile,functionsasaRange-FFT
(64%adoptionamongtheUSpopulation[19])comparedtoother
usingacousticC-FMCWwaves.TheEchoProfileisprocessedasa
wearables.ResearchershaveattachedElectromyography(EMG)
two-dimensionaltensor(ùë•-axis:time,ùë¶-axis:distancefromMunch-
electrodestoeyeglassestomonitoreatingbehaviors[61].Piezo-
Soniceyeglassesformfactor).Tocapturebodypartmovementsand
electricsensors[17,18],whichmakeskincontact,havebeenused
eliminatestaticobjectreflections,thefirstderivativeoftheEcho
totrackjawmovementsbuttheperformancetendstobeimpacted
Profile,termedtheDifferentialEchoProfile,isusedasinputforthe
underconditionsofsweatingorrapidusermovement.Inertialsen-
deeplearningarchitecturetotrackfine-graineddietaryactions.
sors[19]andloadcells[13]havebeenproposedfordietaryevent
trackingbuthavenotshownpromisingresults.Sensorfusionap-
proaches[4,6,36,39,45],integratinggyroscopes,accelerometers, 3.2 HardwareandFormFactor
proximitysensors,andmicrophonesoneyeglasses,haveshownper- WereplicatedthesensingsystemfromActSonic[35]withminor
formanceimprovementsintrackingeatingevents.Passiveacoustic changestothesensorsandcontrollerunit,usingtheOWR-05049T-
sensingusingcontactmicrophones[10]captureschewingsounds 38DspeakerandICS-43434microphone[50].Ourcustomizedcon-
butrequiresclosecontactbetweenthesensorandtheskin.Acoustic trollerunit,showninFigure1(b),featuresthenRF52840microcon-
sensing-basedactivityrecognitionsystemshaveincludedlabelsfor troller[42],twoMAX98357Aaudioamplifiers,aBLESGW1110[44]
eating[35]ordrinking[35,37],buttheyfocusoneatingepisodes module,powermanagementmodules,andanSDcardslotwith
ratherthanfine-grainedactions. a SanDisk Extreme Pro microSD card [2], optimized for powerMunchSonic:TrackingFine-grainedDietaryActionsthroughActiveAcousticSensingonEyeglasses
Sensing Detected Evaluation Power
Year Study Wearable
Modalitie(s) Event(s) Metric Consumption
Wrist-mounted Microphone
2015 Thomazetal.[52] Meals F1-score=79.8% -
Device (PassiveAudio)
Food
2016 Farooqetal.[17] Eyeglasses Piezoelectric
Intake
F1-score=100.0%‚àó -
Bluetooth Microphone Eating
2016 iHearFood[21] Accuracy=76.82% -
Headset (PassiveAudio) Episodes
OuterEar IMU,IRProxomity
2017 EarBit[5] Chewing F1-score=80.1% -
Interface Sensor,Microphone
HeadMovement,
2017 GlasSense[13] Eyeglasses LoadCell
Talking,Chewing
F1-score=94.0%‚àó -
IMU,Microphone, Precision=31%
2017 Mirtchouketal.[36] GoogleGlass Meals -
MotionSensor Recall=87%
Contact Eating
2018 Auracle[10] Earpiece F1-score=77.5% 14.47mW
Microphone Episodes
Eating
2018 Zhangetal.[61] Eyeglasses EMGElectrode F1-score=77% 81.96mW
Event
Precision=82.8%(Eat-
1Camera, Eatingand
-ing),56.7%(Drinking)
2020 FitByte[4] Eyeglasses 1Proximity Drinking 105.08mW
Recall=93.8%(Eating),
Sensor,6IMUs Episodes
65.5%(Drinking)
Accelerometer, Eating
2022 MyDJ[45] Eyeglasses F1-score=92% 26.06mW
Piezoelectric Episodes
ActiveAcoustic Intake,Chewing,Drink-
2024 MunchSonic Eyeglasses F1-score=93.50% 96.5mW
Sensing -ing,Talking,FaceTouch
Table1:Comparisonofeatingdetectionstudiesbasedonwearableformfactors,sensingmodalities,granularityofdetected
events,evaluationmetrics(F1-scoreorprecision/recall),andpowerdraw(inmW).Asterisks(‚àó)intheEvaluationMetriccolumn
indicatethatthesystemwasevaluatedinlabstudies;otherwise,theevaluationwasconductedinfree-livingconditions.
capturedbytheMunchSonicsystem.Thedifferentialechoprofile
describedin3.1servesastheinputtothedeeplearningmodel.We
createoverlappingslidingwindowsfromthestreamofdifferential
echoprofiledata.Toaugmentthedata,weapplyGaussiannoise
to 5% of the sliding windows and mask 5% of the distance axis
(ùë¶-axis)oftheslidingwindowtosimulateenvironmentalnoiseand
unrelatedmovements.
ThepreprocessedslidingwindowsarefedintoaMobileNetV2[40]
convolutionalneuralnetworkencoder.Thislightweightencoder,
Figure1:MunchSonicHardwareandFormFactor:(a)User
effectiveforedgedevices,generatesa256-dimensionalembedding
wearingeyeglassesformfactor,(b)Customizedcontroller
vectorforeachslidingwindow.Thisvectoristhenfedintoaclassi-
unitwithnRF52840microcontroller,(c)Topviewoftheeye-
fiernetworkwiththreefeedforwardlayers(128,64,and6neurons
glassesformfactor,(d)MunchSonictransceiverforactive
respectively).Eachlayer,exceptthelastone,includesbatchnor-
acousticsensinghousingonespeaker(top)andonemicro-
malization[24],dropout[49]withaprobabilityof0.25,andLeaky
phone(bottom).
ReLU[56]activation.Finally,asoftmaxoperationprovidestheclass
probabilitydistribution.
efficiency.Thetransceiverboards,eachwithaspeakerandmicro-
3.3.2 ModelTraining. TheinputtoMunchSonic‚Äôsdeeplearning
phone,areattachedtotheeyeglasshinges,withplacementrefined
frameworkconsistsofslidingwindowsofdifferentialechopro-
foroptimaltrackingofmouth,jaw,andhandmovements.Thecon-
files.TheoptimalslidingwindowfortheMunchSonicdatasetis2
trollerunitandLiPobatteryareattachedtoonelegoftheglasses
secondslongwith50%overlap.Thiswindowlengthwaschosen
frame,connectedviaFlexiblePrintedCircuit(FPC)cablesandaJST
becauseitappropriatelyfitsaneatingintakegesture.Theoptimal
connector.Thesystemdraws96.5mWofpower,withavoltageof
sensingrangeis150pixelsontheùë¶-axisofthedifferentialechopro-
4.02Vandcurrentof24.0mA,andtheprototypeusedintheMunch-
file,correspondingto51.45cmfromtheMunchSonicdevice.With
Sonicstudy,featuringa290mAhLiPobattery,lastsapproximately
speakersandmicrophonesco-locatedoneachhingeoftheeye-
11.25hoursinSDcardstoragemode.
glassesandtransmittingacousticwavesintwofrequencyranges,
therearefourchannelsinthedifferentialechoprofile(twodirect
3.3 DeepLearningFramework andtwoindirectpaths).Thus,theshapeoftheinputslidingwindow
3.3.1 DataProcessingandModelArchitecture. Wedesignedalight- is(4√ó150√ó166),aseachsecondoftheslidingwindowcontains
weightdeeplearningframeworktoprocesstheactiveacousticdata ‚åä50000 ‚åã =83samples.
600SaifMahmudetal.
Weusefocalloss[33]fortrainingtheMunchSonicmodeldue accidentallyturnedoffthechest-mountedcamera,resultinginno
toitseffectivenessinhandlingclassimbalanceintheMunchSonic ground truth data for that participant. Therefore, we discarded
dataset.TheAdam[28]optimizerwithacosineannealinglearning thatparticipantfromthestudyandevaluatedthesystemonthe
rateschedulerisemployed,startingwithaninitiallearningrateof remaining12participants.
10‚àí2.Themodel,implementedusingPyTorchandPyTorchLight- Participantswereaskedtobringameal(breakfast,lunch,or
ningframeworks,istrainedfor30epochswithabatchsizeof128 dinner)fromanysource(home-cookedorrestaurant)andcometo
onGeForceRTX2080TiGPUs. thelab.Afterbriefingthemaboutthestudyprocedure,weequipped
themwiththeMunchSoniceyeglassesandthechest-mountedcam-
4 USERSTUDY era.Weverifiedthedatacollectionsystemwithashort5-minute
Weconducteda45-minuteuserstudytoevaluateMunchSonicin session.Onceverified,thedatacollectionsystemwasstarted,and
anunconstrainedsetting.Thissectiondetailsthestudydesignand participantswerefreetogoanywherewhilewearingtheeyeglasses
theresultingdataset. and camera. The only requirement was to finish the meal they
broughttoensuresufficientsamplesofdietarymovements.Apart
4.1 DesignofUserStudy fromthat,theycouldcontinuewiththeirdailyroutine.Thetotal
durationofthisunconstrainedstudywas45minutes.Uponcom-
Theactiveacousticsensing-basedactivityrecognitionsystemAct-
pletion,participantsreturnedtothelabtoreturnthedevicesand
Sonic [35] has proven effective in tracking eating and drinking
werereimbursedwitha$20USDgiftcard.
episodesinreal-worldsettingsoverlong-durationstudies.Forexam-
ple,itcanidentifyeatinganddrinkingmomentswithaone-second
resolution,achievingF1scoresof90%and88%respectively,without 4.3 DatasetStatistics
needingtrainingdatafromnewusersinnewenvironments.Build-
We collected 540 minutes of data from 12 participants. Two re-
ingonthiscapability,theMunchSonicuserstudywasdesigned
searcherslabeledthereferencevideofromthechest-mountedcam-
withtheassumptionthatactiveacousticsensingonglassescan
eraateachsecondusingtheANU-CVMLVideoAnnotationTool
alreadyaccuratelydistinguisheatingmomentsfromotheractivi-
(Vidat)[60].Thedatawascategorizedintosixclasses:hand-to-
ties.Thispaperfocusesonfurtherextractingfine-graineddietary
mouthmovementforfoodintake,chewing,drinking,andtalking,
actions,suchashand-to-mouthmovementsforfoodintake,chew-
facetouch,andanullclassforotheractivities.Asynchronization
ing,anddrinking,aswellasdistinguishingeatingfromnon-eating
scriptwasusedtoextrapolategroundtruthlabelsforthedifferential
actionsfromactiveacousticsignalsonceeatingmomentsarerec-
echoprofileslidingwindows.Theclassdistributionwas53.5%null,
ognized.Specifically,theMunchSonicuserstudyaimedtoevaluate
5.9%hand-to-mouthforfoodintake,22.0%chewing,2.2%drinking,
thesystem‚Äôsperformanceinunconstrainedconditionsintracking
15.1%talking,and1.3%touchingfacewithhandfornon-eating
fine-graineddietaryactionsanddifferentiatingthesefromnon-
activities.
eatingactions.Weadoptedarelativelyshorter-durationfree-living
study,leveragingActSonic‚Äôsprovenefficacyinlonger-durationin-
the-wildstudies.Additionally,groundtruthannotationinourstudy 5 PERFORMANCEEVALUATION
wasdonepersecond,unlikeothereatingdetectionsystems[45] Wecomputedprecision,recall,andmacroF1-scoretoassessMunch-
thatrelyonself-reporting,enablingustorecognizedietaryactions Sonic‚Äôsabilitytotrackfine-graineddietaryactions.Evaluations
withahightimeresolutionoftwoseconds. wereconductedattwolevels:frame-levelinferenceforeachtwo-
secondslidingwindowandepisode-levelinferenceforintakecount
andchewingtimeestimation.Employingaleave-one-participant-
outevaluationstrategyenableduser-independentassessment,using
datafromoneparticipantasthetestsetandtheremaining11for
trainingandvalidation.Thisevaluationdemonstratesthesystem‚Äôs
performancewithoutrequiringtrainingdatafromnewusers,which
makesthesystemeasiertodeployatscale.
Figure2:Foodsconsumedbyuserstudyparticipants. 5.1 EvaluationofFrame-LevelInference
Wesummarizetheresultsoftheleave-one-participant-outevalu-
ationofslidingwindowpredictioninFigure3.Accordingtothe
4.2 StudyProtocol evaluation,theaveragemacroF1-scoreacrossall12participants
Weconductedauserstudyapprovedbyourorganization‚ÄôsInsti- is0.935withastandarddeviationof0.031.Itisevidentthatthe
tutionalReviewBoard(IRB)with13participants,6identifyingas MunchSonic system can track fine-grained dietary events with
maleand7asfemale,withanaverageageof26.5years,ranging precisionandrecallofmorethan90%formostparticipants.P08
from21to36years.TheparticipantsworetheMunchSoniceye- demonstratestheworstperformanceintermsofbothprecision
glassesformfactorandachest-mountedGoProHERO9camera[22] andrecall.Additionally,P04andP13yieldedslightlylowerrecall
facingupwardtocapturetheireatingactivitiesaroundthefaceas comparedtotheaverage.Ouranalysisofthegroundtruthvideos
thegroundtruth.Thecamerarecorded720pvideoat30fpswith forthoseparticipantssuggeststhattheslightlyworseperformance
adiagonalfieldofviewof148‚ó¶.Notethatoneparticipant(P09) canbeattributedtotheparticipantstouchingtheirfacesseveralMunchSonic:TrackingFine-grainedDietaryActionsthroughActiveAcousticSensingonEyeglasses
Figure5:Episode-levelevaluationofMunchSonic:(a)Seg-
mentationofuserstudydatainto4.5-minute-longepisodes,
Figure3:Precisionandrecallofleave-one-participant-out
(b)Detectionoffoodintakewithineachepisode.
evaluationofMunchSonic,wheredatafromeachparticipant
ontheùë•-axisservesasthetestset.
food_intakeorchewing.Thenumberofundetectedeatingepisodes
fortheMunchSonicdatasetis1outof43groundtruthepisodes,
timesduringthestudyforactivitiesnotrelatedtoeating.Thisphe-
resultinginaFalseNegativeRateof0.023.Additionally,therewere
nomenonisalsoevidentintheconfusionmatrixinFigure4.The
77non-eatingepisodesinthedataset,ofwhich2wereincorrectly
performancedegradationofMunchSonicforface-touchactionscan
detectedaseatingbyMunchSonic,leadingtoaFalsePositiveRate
berelatedtotheimbalancednatureofthedatasetcollectedina
of0.026.
completelyuncontrainedmanner,whichcontainsveryfewsam-
plesofthisparticularactivity.Nonetheless,MunchSonicexhibits 5.2.2 FoodIntakeCounting. Furthermore,tocountthenumberof
robustperformanceacrossallactivities,withameanprecisionand intakesineachsegment,weincrementtheintakecountforthat
recallof0.941¬±0.027and0.930¬±0.041respectively,acrossall12 segmentby1ifwefindonefood_intakeframefollowedbyatleast
participants. twochewingframesorwindowswithinthenext3.0seconds.We
computetheMeanAbsoluteError(MAE)betweenthegroundtruth
numberofintakesandthepredictedcountasametrictoevaluate
this. The average ground truth intake count in one 4.5-minute
eatingsegment(containingmorethan5groundtruthintakeswithin
thetimeframe)andnon-eatingsegment(containingfewerthan5
ground truth intakes) is 17.65 and 1.19, respectively. The MAE
forcountingintakesintheeatingsegmentsis2.01,leadingtoan
averageerrorof11.39%inpredictingthenumberofeatingintakes.
Forthenon-eatingsegments,theMAEforcountingintakesis0.047,
leadingtoameanerrorof3.95%inpredictingtheintakecounts
duringnon-eatingepisodes.
5.2.3 ChewingTimeEstimation. Toevaluatethecoverageofpre-
dictedchewingtime,wecalculatetheMeanAbsoluteError(MAE)
betweenthetotalgroundtruthchewingtimeina4.5-minuteepisode
andthepredictedchewingtime,measuredinseconds,acrossall12
userstudyparticipants.Theaveragechewingtimeacrosseating
Figure 4: Normalized confusion matrix of the leave-one-
episodesis163.6seconds(2.73minutes)and9.17secondsfornon-
participant-outevaluationacross12participantsoftheuser
eatingepisodesintheMunchSonicdataset.Interpolatingfromthe
studyofMunchSonic.
frame-levelpredictionsofMunchSonic,theMAEoftheestimated
chewing time for eating episodes is 12.87 seconds, leading to a
coverageof92.13%ofchewingtime.Forthenon-eatingepisodes,
5.2 EvaluationofEpisode-LevelInference
theMAEoftheestimatedchewingtimeis1.61seconds,leading
5.2.1 Eating Episode Detection. To evaluate MunchSonic in de- to a coverage of 82.5%. This higher coverage across non-eating
tectingeatingepisodesandintakecountswithinepisodes,weseg-
episodesindicatesMunchSonic‚Äôspotentialindetectingsnacking
mentedthe45-minutedatacollectedfromeachparticipantinthe
events,whichwasoutsidethescopeoftheuserstudy.
userstudyinto10segmentsofequallength(4.5minuteseach).Ac-
cordingtothedefinitionprovidedin[51,63],ifasegmentoflength 6 DISCUSSION
ùë° ùë§containsùë°ùë§ 5+20intakes,whichis5intakesfortheaforementioned
6.1 AnalyzingthePassiveSensedUltrasonic
segments,thenthatsegmentisdefinedasaneatingepisode.To
evaluateMunchSonic‚Äôsefficacyindetectingtheseeatingepisodes, RangeforTrackingDietaryActions
we deployed a majority voting strategy where a segment is la- Theoretically,oursystemfocusesonnearlyinaudiblesoundfre-
beledasaneatingepisodeif50%oftheframesarelabeledaseither quenciesabove18KHz,whichshouldnotinterferewithmostdailySaifMahmudetal.
Figure6:Differentialechoprofilesofactionswithactiveandpassiveacousticsensing.Theùë•-axisrepresentstime,andthe
ùë¶-axisrepresentstheechodistancefromtheMunchSonicformfactor,rangingfrom0cmto50cm.
activities,includingeating.However,somesoundsgeneratedby
eatingcanpotentiallyhavefrequencycomponentsabove18KHz.
Toevaluatetheimpactofthesepassivelysensedultrasonicsig-
nalsondistinguishingdietaryactions,weconductedapreliminary
studywithoneresearcherasaparticipant.Inthisstudy,weturned
offbothspeakersofMunchSonictopreventthemfromtransmit-
tinganyC-FMCWchirpsandperformedthesamesetofactivities
asintheMunchSonicuserstudy.Themicrophonesonbothsides
continued to record the surrounding acoustic signals while the
researcherconductedsimilardietaryactions.Wethencreateddif-
ferentialechoprofilewindows(illustratedinthebottomrowof
Figure6)inthesamewayasbeforeandtrainedtheMunchSonic
deeplearningmodel.Themeancross-sessionmacroF1-scorefor Figure7:Ablationstudytomeasuretheimpactofsensing
thisuser-dependentmodelwas0.322.Incontrast,whenwecol- rangeandslidingwindowsizeontheperformanceofMunch-
lectedsimilardatafromthesameparticipantwiththespeakers Sonic.Here,alltheslidingwindowsizesmentionedhavea
transmittingultrasonicchirps,themeancross-sessionmacroF1- 50%overlap.
scorewas0.976.AsillustratedinFigure6,andevidentfromthe
performancedifferencementioned,activeacousticsensingplaysa
crucialroleindistinguishingdietaryactivitiesbytrackingmove- mostmovementsrelatedtothedietaryactionsMunchSonictracks
mentsofbodyparts(e.g.,face,arms)involvedindietaryactions. happeninthejawandmouthregion,whichiswithin30cmof
Whilepassiveultrasonicsignalsmighthavesomeminorimpacton theeyeglassesformfactor.Additionally,MunchSonictrackshand
detectingfine-grainedevents,quantifyingthisimpactnumerically movementsforfoodintake,soextendingthesensingrangeto50
is challenging and beyond the scope of this paper. Our experi- cmyieldsthebestperformance.Thepoorerperformanceatlonger
mentsindicatethatchewingcrunchyfoods(e.g.,chips)generated rangescanberationalizedbythefactthatvariousactivitiescan
moreultrasoniccomponentsinthespectrogram.However,thesig- occurinthatregion,whichmaynotberelatedtoeating.
nalstrengthofreflectionsfromtheultrasonicchirpstransmitted
by MunchSonic is much higher than the passively sensed com- 6.3 PotentialApplicationScenarios
ponents,potentiallyovershadowingtheirfeatureimportancein
WeenvisionMunchSonicbeingpotentiallyusefulinvariouscon-
MunchSonic‚Äôsdeeplearningmodeltraining.Furtherinvestigation
texts,includingeatingbehaviorassessment,chronicdiseaseman-
intothiswillbeleftforfuturework.
agement,andoverallwell-being.Oursystemcanenhanceeating
behaviorassessmentbyprovidingapassiveandobjectivemeasure-
mentofeatingepisodesandmicro-levelactions(e.g.,eatingpauses,
6.2 ImpactofSensingRangeandTemporal
drinking,chewing)inreal-worldenvironments.Byaccuratelyclassi-
Context fyingtheseactions,wecanderivethe"microstructureofmeals"[9],
Weevaluatetheimpactofthesensingrangeandtemporalcontext includingeatingduration,eatingspeed,chewingrate,andchewing
fortheMunchSonicsysteminthisexperiment.Figure7demon- efficacy,andtheirvariationsacrossepisodes.Traditionally,these
stratesthataslidingwindowsizeof2.00secondswitha50%overlap metricsareassessedthroughsemi-structuredinterviews(e.g.,Eat-
yieldsthebesttrackingperformance.Wealsoobservethatasensing ingDisorderExamination[15,20])orself-reportquestionnaires
rangeof50to80cmdemonstratesoptimalperformanceintracking (e.g.,[41]),whichrelyonrecallandapproximationandareprone
fine-graineddietaryactions.Thiscanbeattributedtothefactthat tobiases.Oursystemoffersanautomated,precise,andnaturalisticMunchSonic:TrackingFine-grainedDietaryActionsthroughActiveAcousticSensingonEyeglasses
approachtocapturingthesebehaviors.Clinicalstudiesindicatethat Sensor.Proc.ACMInteract.Mob.WearableUbiquitousTechnol.2,1,Article4(mar
thesemetricsarelinkedtovarioushealthoutcomes.Forinstance, 2018),21pages. https://doi.org/10.1145/3191736
[13] JungmanChung,JungminChung,WonjunOh,YongkyuYoo,WonGuLee,and
highereatingandchewingratesareassociatedwithhigherbody
HyunwooBang.2017.Aglasses-typewearabledeviceformonitoringthepatterns
massindex(e.g.,obesity[48]),eatingdisorders[54],andmetabolic offoodintakeandfacialactivity.Scientificreports7,1(2017),41690.
diseases[59].Therefore,preciseassessmentofthesemetricscould [14] YujieDong,JennaScisco,MikeWilson,EricMuth,andAdamHoover.2013.
Detectingperiodsofeatingduringfree-livingbytrackingwristmotion.IEEE
enhancethemonitoringandmanagementofthesehealthoutcomes journalofbiomedicalandhealthinformatics18,4(2013),1253‚Äì1260.
overtime,facilitatingreal-timemonitoringandinterventions. [15] ChristopherFairburnandG.Wilson.1993.BingeEating:Nature,Assessment,
andTreatment.JournalofNervousandMentalDisease-JNERVMENTDIS183
(Jan.1993).
7 CONCLUSION [16] MuhammadFarooq,JuanMFontana,andEdwardSazonov.2014. Anovel
approachforfoodintakedetectionusingelectroglottography. Physiological
MunchSonicdemonstratesaccuratetrackingoffine-graineddietary measurement35,5(2014),739.
actionsinreal-worldsettings,surpassingothersystemswithits [17] MuhammadFarooqandEdwardSazonov.2016. Anovelwearabledevicefor
singleinstrumentationpoint,highprecision,privacypreservation, foodintakeandphysicalactivityrecognition.Sensors16,7(2016),1067.
[18] MuhammadFarooqandEdwardSazonov.2016.Segmentationandcharacteri-
andscalability.Ourworkholdssignificantpotentialforadvancing zationofchewingboutsbymonitoringtemporalismuscleusingsmartglasses
dietaryhealthassessmentandmonitoringinbothtechnicaland withpiezoelectricsensor.IEEEjournalofbiomedicalandhealthinformatics21,6
(2016),1495‚Äì1503.
healthcaredomains.
[19] MuhammadFarooqandEdwardSazonov.2018.Accelerometer-baseddetection
offoodintakeinfree-livingindividuals.IEEEsensorsjournal18,9(2018),3752‚Äì
3758.
REFERENCES
[20] CentreforResearchonEatingDisordersatOxford(CREDO).[n.d.]. Eating
[1] NabilAlshurafa,HaikKalantarian,MohammadPourhomayoun,JasonJLiu, DisorderExamination(Edition17.0D).https://www.cbte.co/site/download/ede-
ShrutiSarin,BehnamShahbazi,andMajidSarrafzadeh.2015. Recognitionof 17-0d/?wpdmdl=615&masterkey=5c644ef9b6149. Accessed:2024-05-26.
nutritionintakeusingtime-frequencydecompositioninawearablenecklace [21] YangGao,NingZhang,HonghaoWang,XiangDing,XuYe,GuanlingChen,and
usingapiezoelectricsensor.IEEEsensorsjournal15,7(2015),3909‚Äì3916. YuCao.2016.iHearfood:eatingdetectionusingcommoditybluetoothheadsets.
[2] Amazon.com. [n.d.]. SanDisk Extreme Pro 32GB SDHC UHS-I Card In2016IEEEFirstInternationalConferenceonConnectedHealth:Applications,
(SDSDXXG-032G-GN4IN).https://www.amazon.com/SanDisk-Extreme-32GB- SystemsandEngineeringTechnologies(CHASE).IEEE,163‚Äì172.
UHS-I-SDSDXXG-032G-GN4IN/dp/B01J5RHBQ4. [Online;accessed21-May- [22] GoPro.2020.HERO9Black.https://gopro.com/en/us/shop/cameras/hero9-black/
2024]. CHDHX-901-master.html. [Online;accessed22-May-2024].
[3] JudithAmores,MaeDotan,andPattieMaes.2019.Anexplorationofformfactors [23] TakashiHamatani,MoustafaElhamshary,AkiraUchiyama,andTeruoHigashino.
forsleep-olfactoryinterfaces.In201941stannualinternationalconferenceofthe 2018.FluidMeter:Gaugingthehumandailyfluidintakeusingsmartwatches.Pro-
IEEEengineeringinmedicineandbiologysociety(EMBC).IEEE,1456‚Äì1460. ceedingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies
[4] AbdelkareemBedri,DianaLi,RushilKhurana,KunalBhuwalka,andMayank 2,3(2018),1‚Äì25.
Goel.2020.FitByte:AutomaticDietMonitoringinUnconstrainedSituationsUs- [24] SergeyIoffeandChristianSzegedy.2015. BatchNormalization:Accelerating
ingMultimodalSensingonEyeglasses.InProceedingsofthe2020CHIConference DeepNetworkTrainingbyReducingInternalCovariateShift.448‚Äì456. http:
onHumanFactorsinComputingSystems(<conf-loc>,<city>Honolulu</city>, //jmlr.org/proceedings/papers/v37/ioffe15.pdf
<state>HI</state>,<country>USA</country>,</conf-loc>)(CHI‚Äô20).Associa- [25] YinchengJin,SeokminChoi,YangGao,JiyangLi,ZhengxiongLi,andZhan-
tionforComputingMachinery,NewYork,NY,USA,1‚Äì12. https://doi.org/10. pengJin.2023.TransASL:ASmartGlassbasedComprehensiveASLRecognizer
1145/3313831.3376869 inDailyLife.InProceedingsofthe28thInternationalConferenceonIntelligent
[5] AbdelkareemBedri,RichardLi,MalcolmHaynes,RajPrateekKosaraju,Ishaan UserInterfaces(<conf-loc>,<city>Sydney</city>,<state>NSW</state>,<coun-
Grover,TemiloluwaPrioleau,MinYanBeh,MayankGoel,ThadStarner,and try>Australia</country>,</conf-loc>)(IUI‚Äô23).AssociationforComputingMa-
GregoryAbowd.2017.EarBit:usingwearablesensorstodetecteatingepisodes chinery,NewYork,NY,USA,802‚Äì818. https://doi.org/10.1145/3581641.3584071
inunconstrainedenvironments.ProceedingsoftheACMoninteractive,mobile, [26] YinchengJin,YangGao,YanjunZhu,WeiWang,JiyangLi,SeokminChoi,
wearableandubiquitoustechnologies1,3(2017),1‚Äì20. ZhangyuLi,JagmohanChauhan,AnindK.Dey,andZhanpengJin.2021.Soni-
[6] AbdelkareemBedri,YuchenLiang,SudershanBoovaraghavan,GeoffKaufman, cASL:AnAcoustic-basedSignLanguageGestureRecognizerUsingEarphones.
andMayankGoel.2022. FitNibble:AFieldStudytoEvaluatetheUtilityand Proc.ACMInteract.Mob.WearableUbiquitousTechnol.5,2,Article67(jun2021),
UsabilityofAutomaticDietMonitoringinFoodJournalingUsinganEyeglasses- 30pages. https://doi.org/10.1145/3463519
basedWearable.InProceedingsofthe27thInternationalConferenceonIntelligent [27] HaikKalantarian,NabilAlshurafa,TuanLe,andMajidSarrafzadeh.2015.Moni-
UserInterfaces(<conf-loc>,<city>Helsinki</city>,<country>Finland</country>, toringeatinghabitsusingapiezoelectricsensor-basednecklace.Computersin
</conf-loc>)(IUI‚Äô22).AssociationforComputingMachinery,NewYork,NY, biologyandmedicine58(2015),46‚Äì55.
USA,79‚Äì92. https://doi.org/10.1145/3490099.3511154 [28] DiederikPKingmaandJimmyBa.2014.Adam:Amethodforstochasticopti-
[7] AbdelkareemBedri,ApoorvaVerlekar,EdisonThomaz,ValerieAvva,andThad mization.arXivpreprintarXiv:1412.6980(2014).
Starner.2015.DetectingMastication:AWearableApproach.InProceedingsof [29] NataliyaKosmyna,CaitlinMorris,UtkarshSarawgi,andPattieMaes.2019.At-
the2015ACMonInternationalConferenceonMultimodalInteraction(Seattle, tentivU:ABiofeedbackSystemforReal-timeMonitoringandImprovementof
Washington,USA)(ICMI‚Äô15).AssociationforComputingMachinery,NewYork, Engagement.InExtendedAbstractsofthe2019CHIConferenceonHumanFactors
NY,USA,247‚Äì250. https://doi.org/10.1145/2818346.2820767 inComputingSystems(Glasgow,ScotlandUk)(CHIEA‚Äô19).AssociationforCom-
[8] AbdelkareemBedri,ApoorvaVerlekar,EdisonThomaz,ValerieAvva,andThad putingMachinery,NewYork,NY,USA,1‚Äì2. https://doi.org/10.1145/3290607.
Starner.2015.Awearablesystemfordetectingeatingactivitieswithproximity 3311768
sensorsintheouterear.InProceedingsofthe2015ACMInternationalSymposium [30] KonstantinosKyritsis,ChristosDiou,andAnastasiosDelopoulos.2019.Detecting
onWearableComputers.91‚Äì92. mealsinthewildusingtheinertialdataofatypicalsmartwatch.In201941st
[9] FranceBellisle.2020.Edograms:recordingthemicrostructureofmealintakein AnnualInternationalConferenceoftheIEEEEngineeringinMedicineandBiology
humans‚Äîawindowonappetitemechanisms.InternationalJournalofObesity Society(EMBC).IEEE,4229‚Äì4232.
44,12(Dec.2020),2347‚Äì2357. https://doi.org/10.1038/s41366-020-00653-w [31] KeLi,RuidongZhang,SiyuanChen,BoaoChen,MoseSakashita,Fran√ßoisGuim-
Publisher:NaturePublishingGroup. breti√®re,andChengZhang.2024.EyeEcho:ContinuousandLow-powerFacial
[10] ShengjieBi,TaoWang,NicoleTobias,JosephineNordrum,ShangWang,George ExpressionTrackingonGlasses.arXivpreprintarXiv:2402.12388(2024).
Halvorsen,SougataSen,RonaldPeterson,KofiOdame,KellyCaine,RyanHalter, [32] KeLi,RuidongZhang,BoLiang,Fran√ßoisGuimbreti√®re,andChengZhang.
JacobSorber,andDavidKotz.2018.Auracle:DetectingEatingEpisodeswithan 2022.EarIO:ALow-PowerAcousticSensingEarableforContinuouslyTracking
Ear-mountedSensor.Proc.ACMInteract.Mob.WearableUbiquitousTechnol.2,3, DetailedFacialMovements.6,2,Article62(jul2022),24pages. https://doi.org/
Article92(sep2018),27pages. https://doi.org/10.1145/3264902 10.1145/3534621
[11] KeumSanChun,SarnabBhattacharya,andEdisonThomaz.2018. Detecting [33] Tsung-YiLin,PriyaGoyal,RossGirshick,KaimingHe,andPiotrDoll√°r.2017.
eatingepisodesbytrackingjawbonemovementswithanon-contactwearable Focallossfordenseobjectdetection.InProceedingsoftheIEEEinternational
sensor.ProceedingsoftheACMoninteractive,mobile,wearableandubiquitous conferenceoncomputervision.2980‚Äì2988.
technologies2,1(2018),1‚Äì21. [34] SaifMahmud,KeLi,GuilinHu,HaoChen,RichardJin,RuidongZhang,Fran√ßois
[12] KeumSanChun,SarnabBhattacharya,andEdisonThomaz.2018. Detecting Guimbreti√®re,andChengZhang.2023. PoseSonic:3DUpperBodyPoseEs-
EatingEpisodesbyTrackingJawboneMovementswithaNon-ContactWearable timationThroughEgocentricAcousticSensingonSmartglasses. Proc.ACMSaifMahmudetal.
Interact.Mob.WearableUbiquitousTechnol.7,3,Article111(sep2023),28pages. 39‚Äì44. https://doi.org/10.1145/3376897.3377867
https://doi.org/10.1145/3610895 [54] B.TimothyWalsh,HarryR.Kissileff,SusanM.Cassidy,andSondraDantzic.
[35] SaifMahmud,VineetParikh,QikangLiang,KeLi,RuidongZhang,Ashwin 1989.EatingBehaviorofWomenWithBulimia.ArchivesofGeneralPsychiatry
Ajit,VipinGunda,DevanshAgarwal,Fran√ßoisGuimbreti√®re,andChengZhang. 46,1(Jan.1989),54‚Äì58. https://doi.org/10.1001/archpsyc.1989.01810010056008
2024.ActSonic:RecognizingEverydayActivitiesfromInaudibleAcousticWaves [55] TianbenWang,DaqingZhang,YuanqingZheng,TaoGu,XingsheZhou,and
AroundtheBody. arXiv:2404.13924[cs.HC] BernadetteDorizzi.2018. C-FMCWBasedContactlessRespirationDetection
[36] MarkMirtchouk,DrewLustig,AlexandraSmith,IvanChing,MinZheng,and UsingAcousticSignal.1,4,Article170(jan2018),20pages. https://doi.org/10.
SamanthaKleinberg.2017.RecognizingEatingfromBody-WornSensors:Com- 1145/3161188
biningFree-livingandLaboratoryData. Proc.ACMInteract.Mob.Wearable [56] BingXu,NaiyanWang,TianqiChen,andMuLi.2015.Empiricalevaluationof
UbiquitousTechnol.1,3,Article85(sep2017),20pages. https://doi.org/10.1145/ rectifiedactivationsinconvolutionalnetwork.arXivpreprintarXiv:1505.00853
3131894 (2015).
[37] VimalMollyn,KaranAhuja,DhruvVerma,ChrisHarrison,andMayankGoel. [57] KojiYataniandKhaiNTruong.2012.Bodyscope:awearableacousticsensorfor
2022.SAMoSA:SensingActivitieswithMotionandSubsampledAudio.Proceed- activityrecognition.InProceedingsofthe2012ACMConferenceonUbiquitous
ingsoftheACMonInteractive,Mobile,WearableandUbiquitousTechnologies6,3 Computing.341‚Äì350.
(2022),1‚Äì19. [58] XuYe,GuanlingChen,YangGao,HonghaoWang,andYuCao.2016.Assisting
[38] TauhidurRahman,AlexanderT.Adams,MiZhang,ErinCherry,BobbyZhou, foodjournalingwithautomaticeatingdetection.InProceedingsofthe2016CHI
HuaishuPeng,andTanzeemChoudhury.2014.BodyBeat:amobilesystemfor conferenceextendedabstractsonhumanfactorsincomputingsystems.3255‚Äì3262.
sensingnon-speechbodysounds.InProceedingsofthe12thAnnualInternational [59] Shu-qianYuan,Ying-mingLiu,WeiLiang,Fei-feiLi,YuanZeng,Yin-yueLiu,
ConferenceonMobileSystems,Applications,andServices(BrettonWoods,New Shu-zhenHuang,Quan-yuanHe,BinhQuach,JiaoJiao,JulienS.Baker,and
Hampshire,USA)(MobiSys‚Äô14).AssociationforComputingMachinery,New Yi-deYang.2021.AssociationBetweenEatingSpeedandMetabolicSyndrome:
York,NY,USA,2‚Äì13. https://doi.org/10.1145/2594368.2594386 ASystematicReviewandMeta-Analysis. FrontiersinNutrition8(Oct.2021).
[39] TauhidurRahman,MaryCzerwinski,RanGilad-Bachrach,andPaulJohns.2016. https://doi.org/10.3389/fnut.2021.700936Publisher:Frontiers.
Predicting"About-to-Eat"MomentsforJust-in-TimeEatingIntervention.In [60] JiahaoZhang,StephenGould,andItzikBen-Shabat.2020.Vidat‚ÄîANUCVML
Proceedingsofthe6thInternationalConferenceonDigitalHealthConference(Mon- VideoAnnotationTool.https://github.com/anucvml/vidat.
tr√©al,Qu√©bec,Canada)(DH‚Äô16).AssociationforComputingMachinery,New [61] RuiZhangandOliverAmft.2017.Monitoringchewingandeatinginfree-living
York,NY,USA,141‚Äì150. https://doi.org/10.1145/2896338.2896359 usingsmarteyeglasses.IEEEjournalofbiomedicalandhealthinformatics22,1
[40] MarkSandler,AndrewHoward,MenglongZhu,AndreyZhmoginov,andLiang- (2017),23‚Äì32.
ChiehChen.2018. Mobilenetv2:Invertedresidualsandlinearbottlenecks.In [62] RuidongZhang,KeLi,YihongHao,YufanWang,ZhengnanLai,Fran√ßoisGuim-
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition. breti√®re,andChengZhang.2023.EchoSpeech:ContinuousSilentSpeechRecog-
4510‚Äì4520. nitiononMinimally-ObtrusiveEyewearPoweredbyAcousticSensing.InPro-
[41] S.Sasaki,A.Katagiri,T.Tsuji,T.Shimoda,andK.Amano.2003.Self-reported ceedingsofthe2023CHIConferenceonHumanFactorsinComputingSystems
rateofeatingcorrelateswithbodymassindexin18-y-oldJapanesewomen. (Hamburg,Germany)(CHI‚Äô23).AssociationforComputingMachinery,New
InternationalJournalofObesity27,11(Nov.2003),1405‚Äì1410. https://doi.org/ York,NY,USA,Article852,18pages. https://doi.org/10.1145/3544548.3580801
10.1038/sj.ijo.0802425Publisher:NaturePublishingGroup. [63] RuidongZhang,JihaiZhang,NitishGade,PengCao,SeyunKim,JunchiYan,and
[42] NordicSemiconductors.[n.d.]. nRF52840MultiprotocolBluetooth5.4SoC ChengZhang.2022.EatingTrak:DetectingFine-GrainedEatingMomentsinthe
supportingBluetoothLowEnergy,Bluetoothmesh,NFC,ThreadandZigbee. WildUsingaWrist-MountedIMU.Proc.ACMHum.-Comput.Interact.6,MHCI,
https://www.nordicsemi.com/products/nrf52840. [Online;accessed29-Nov- Article214(sep2022),22pages. https://doi.org/10.1145/3546749
2023]. [64] ShiboZhang,YuqiZhao,DzungTriNguyen,RunshengXu,SougataSen,Josiah
[43] SougataSen,VigneshwaranSubbaraju,ArchanMisra,RajeshBalan,andYoungki Hester,andNabilAlshurafa.2020. NeckSense:AMulti-SensorNecklacefor
Lee.2018.Annapurna:buildingareal-worldsmartwatch-basedautomatedfood DetectingEatingActivitiesinFree-LivingConditions.Proc.ACMInteract.Mob.
journal.In2018IEEE19thInternationalSymposiumon"AWorldofWireless,Mobile WearableUbiquitousTechnol.4,2,Article72(jun2020),26pages. https://doi.
andMultimediaNetworks"(WoWMoM).IEEE,1‚Äì6. org/10.1145/3397313
[44] SGWireless.[n.d.]. SGW111XBLEModules. https://www.sgwireless.com/
product/SGW111X. [Online;accessed29-Nov-2023].
[45] JaeminShin,SeungjooLee,TaesikGong,HyungjunYoon,HyunchulRoh,Andrea
Bianchi,andSung-JuLee.2022.MyDJ:SensingFoodIntakeswithanAttachable
onYourEyeglassFrame.InProceedingsofthe2022CHIConferenceonHuman
FactorsinComputingSystems(NewOrleans,LA,USA)(CHI‚Äô22).Association
forComputingMachinery,NewYork,NY,USA,Article341,17pages. https:
//doi.org/10.1145/3491102.3502041
[46] JaeminShin,SeungjooLee,andSung-JuLee.2019.Accurateeatingdetection
onadailywearablenecklace.InProceedingsofthe17thAnnualInternational
ConferenceonMobileSystems,Applications,andServices.649‚Äì650.
[47] ArnoldSlyper.2021. OralProcessing,SatiationandObesity:Overviewand
Hypotheses.Diabetes,MetabolicSyndromeandObesity:TargetsandTherapy14
(July2021),3399‚Äì3415. https://doi.org/10.2147/DMSO.S314379
[48] ChikanobuSonoda,HidekiFukuda,MasayasuKitamura,HideakiHayashida,
YumikoKawashita,ReikoFurugen,ZenyaKoyama,andToshiyukiSaito.2018.
AssociationsamongObesity,EatingSpeed,andOralHealth.ObesityFacts11,2
(April2018),165‚Äì175. https://doi.org/10.1159/000488533
[49] NitishSrivastava,GeoffreyHinton,AlexKrizhevsky,IlyaSutskever,andRuslan
Salakhutdinov.2014.Dropout:asimplewaytopreventneuralnetworksfrom
overfitting.Thejournalofmachinelearningresearch15,1(2014),1929‚Äì1958.
[50] TDK.[n.d.].ICS-43434Multi-ModeMicrophonewithI2SDigitalOutput.https:
//invensense.tdk.com/products/ics-43434/. [Online;accessed29-Nov-2023].
[51] EdisonThomaz,IrfanEssa,andGregoryDAbowd.2015.Apracticalapproach
forrecognizingeatingmomentswithwrist-mountedinertialsensing.InProceed-
ingsofthe2015ACMinternationaljointconferenceonpervasiveandubiquitous
computing.1029‚Äì1040.
[52] EdisonThomaz,ChengZhang,IrfanEssa,andGregoryD.Abowd.2015.Inferring
MealEatingActivitiesinRealWorldSettingsfromAmbientSounds:AFeasibility
Study.InProceedingsofthe20thInternationalConferenceonIntelligentUser
Interfaces(Atlanta,Georgia,USA)(IUI‚Äô15).AssociationforComputingMachinery,
NewYork,NY,USA,427‚Äì431. https://doi.org/10.1145/2678025.2701405
[53] CatherineTong,ShyamA.Tailor,andNicholasD.Lane.2020.AreAccelerometers
forActivityRecognitionaDead-End?.InProceedingsofthe21stInternational
WorkshoponMobileComputingSystemsandApplications(Austin,TX,USA)
(HotMobile‚Äô20).AssociationforComputingMachinery,NewYork,NY,USA,