Code Pretraining Improves Entity Tracking Abilities of
Language Models
NajoungKim* SebastianSchuster* ShubhamToshniwal*
DepartmentofLinguistics DepartmentofLinguistics NVIDIA
BostonUniversity UniversityCollegeLondon stoshniwal@nvidia.com
najoung@bu.edu s.schuster@ucl.ac.uk
Abstract
Recent work has provided indirect evidence that pretraining language
modelsoncodeimprovestheabilityofmodelstotrackstatechangesof
discourseentitiesexpressedinnaturallanguage. Inthiswork,wesystem-
atically test this claim by comparing pairs of language models on their
entity tracking performance. Critically, the pairs consist of base models
andmodelstrainedontopofthesebasemodelswithadditionalcodedata.
Weextendthisanalysistoadditionallyexaminetheeffectofmathtraining,
anotherhighlystructureddatatype,andalignmenttuning,animportant
step for enhancing the usability of models. We find clear evidence that
modelsadditionallytrainedonlargeamountsofcodeoutperformthebase
models. On the other hand, we find no consistent benefit of additional
mathtrainingoralignmenttuningacrossvariousmodelfamilies.
1 Introduction
Entity tracking, the capacity to track how properties of discourse entities and their rela-
tionshipschangeasadiscourseunfolds,isanimportantabilityforunderstandinglonger
contextsaswellasothercriticalcapabilitiessuchasplanning. Forexample,tosuccessfully
parsethefollowingrecipe,anagentneedstotrackwhathappenstothedifferententities,
suchasingredients.
(1) Puttheeggs,sugar,flour,andbakingpowderinabowlandmixtoformalightbatter.
Makesurethatthefinalbatterdoesnotcontainanylumpsofflourorsugar.
Kim & Schuster (2023) showed that several Transformer-based large language models
(LLMs),suchasGPT-3.5,exhibitanon-trivialentitytrackingcapacity. Atthesametime,
theyfoundthatsimilarmodels,suchasGPT-3,seemtolackthisability. Basedonthelimited
informationavailableaboutthedifferencesbetweentheGPT-3andGPT-3.5models,Kim
&Schuster(2023)hypothesizedthatpretrainingonlargeamountsofcodeimbuesLLMs
withentitytrackingabilities. However,duetotheopacityoftrainingdataspecificationsof
thesemodels,itremainsunclearwhethercodepretrainingindeedisthecriticaldifference.
Inthiswork,were-evaluatethisclaimwithopen-sourceLLMsforwhichmoreinformation
about the pretraining process is available. We extend our analysis to the effect of math
andinstructiontuningaswellascode. Uponcomparingpairsofbasemodelsandmodels
additionallytrainedoncode,math,oralignmenttuned,1 wefindaclearbenefitofcode
trainingbutnoconsistentbenefitofmathtrainingoralignmenttuning.
∗Equalcontribution.
1Weusethetermalignmenttuningtorefertovariousmethodsofmakinglanguagemodelsmore
usefulforinteractivesettings,includingsupervisedinstructionfinetuning(SFT)(Weietal.,2022a),
reinforcementlearningfromhumanfeedback(RLHF)(Ouyangetal.,2022),anddirectpreference
optimization(DPO)(Rafailovetal.,2023).
1
4202
yaM
13
]LC.sc[
1v86012.5042:viXraAdditionalTraining
Exp Model Size BaseModel
+Code +Math +Instruct/Chat
Exp1 Llama2 7B–70B -
(code) CodeLlama 7B–70B ✓ Llama2
DeepSeek 7B -
DeepSeek-Coder 7B ✓ DeepSeek
Gemma 8B -
CodeGemma 8B ✓ Gemma
Exp2 Llama 7B -
(math) FLoat 7B ✓(instruct) Llama
Mistral 7B -
OpenMathMistral 7B ✓(instruct) Mistral
DeepSeek-Coder 7B ✓ DeepSeek
DeepSeek-Math 7B ✓ ✓ DeepSeek-Coder
CodeLlama 7B,34B ✓ Llama2
Llemma 7B,34B ✓ ✓ CodeLlama
Exp3 Llama2 7B–70B -
(alignment) Llama2-Chat 7B–70B ✓ Llama2
CodeLlama 7B–70B ✓ Llama2
CodeLlama-Instruct 7B–70B ✓ ✓ CodeLlama2
Gemma 8B -
Gemma-Instruct 8B ✓ Gemma
CodeGemma 8B ✓ Gemma
CodeGemma-Instruct 8B ✓ ✓ CodeGemma
DeepSeek 7B -
DeepSeek-Chat 7B ✓ DeepSeek
DeepSeek-Coder 7B ✓ DeepSeek
DeepSeek-Coder-Instruct 7B ✓ ✓ DeepSeek-Coder
Table1: Summaryofthemodelscomparedandtheirpretrainingdatacomposition.
2 Relatedwork
Includingcodeinthepretrainingdatamixture,evenformodelsnotexplicitlyspecialized
for code, has become increasingly customary in LLM training (Chowdhery et al., 2023;
Touvronetal.,2023b;GeminiTeametal.,2024;Groeneveldetal.,2024,i.a.). Inadditionto
servingthepopularusecaseofLLMsincodecompletionandgeneration(Chenetal.,2021),
addingcodetothepretrainingdatamixturehasbeenclaimedtoimprovegeneralreasoning
capacities of LLMs (Fu et al., 2022; Ma et al., 2024; Yang et al., 2024). Kim & Schuster
(2023)hypothesizedthataconcretecapacitythatcanbenefitfromcodeisentitytracking:
convergingevidencetowardsthisclaimiscontributedbyobservationsfromMadaanetal.
(2022)(codepretrainedmodelslikeCodexperformbetterthanmodelsprimarilytrainedon
languagedataonProPara(Dalvietal.,2019)),Sapetal.(2022)(GPT-3.5performsbetteron
objecttrackingthanGPT-3),andMuennighoffetal.(2023)(addingcodetothepretraining
dataimprovesperformancetheonbAbItasks(Westonetal.,2016)). Furthermore,Prakash
etal.(2024)observedthatabasemodelfinetunedonarithmetictasksimprovedperformance
onasimplifiedversionoftheentitytrackingtaskbyKim&Schuster(2023), suggesting
thatstructureddataingeneralbeyondcodemaycontributetothedevelopmentofanentity
trackingcapacityinlanguagemodels.
3 Experiments
We aim to systematically test the hypothesis that code pretraining leads to better entity
trackingputforwardbyKim&Schuster(2023),throughaseriesofexperimentscomparing
basemodelsandmodelscontinuedtobetrainedoncodeontopofthebasemodels. We
additionallytestthehypothesisthatpretrainingonmath,anothertypeofstructureddata,
leadstobetterentitytrackingperformancethroughsimilarcomparisons.
22-shotprompt
Giventhedescriptionafter"Description:",writeatruestatementaboutallboxes
andtheircontentstothedescriptionafter"Statement:".
Description: Box0containsthecar,Box1containsthecross,Box2containsthebag
andthemachine,Box3containsthepaperandthestring,Box4containsthebill,
Box5containstheappleandthecashandtheglass,Box6containsthebottleand
themap.
Statement: Box0containsthecar,Box1containsthecross,Box2containsthebag
andthemachine,Box3containsthepaperandthestring,Box4containsthebill,
Box5containstheappleandthecashandtheglass,Box6containsthebottleand
themap.
Description: Box0containsthecar,Box1containsthecross,Box2containsthebag
andthemachine,Box3containsthepaperandthestring,Box4containsthebill,
Box5containstheappleandthecashandtheglass,Box6containsthebottleand
themap. RemovethecarfromBox0. RemovethepaperandthestringfromBox3.
PuttheplaneintoBox0. MovethemapfromBox6toBox2. Removethebillfrom
Box4. PutthecoatintoBox3.
Statement: Box0containstheplane,Box1containsthecross,Box2containsthe
bagandthemachineandthemap,Box3containsthecoat,Box4containsnothing,
Box5containstheappleandthecashandtheglass,Box6containsthebottle.
Description: {description}
Statement: Box0contains
Table2: Promptswith2-shotin-contextdemonstrations.
3.1 Models
Weselectedmodelpairsthathavebeenreportedtovaryonlyintermsoftheirpretraining
data. Fortestingthecodehypothesis,wecomparedthefollowingpairsofmodels: (Llama2,
CodeLlama),(DeepSeek,DeepSeek-Coder),and(Gemma,CodeGemma),wherethesecond
modelineachpairisobtainedbycontinuingtotrainthefirstmodelonadditionalcodedata.
Wetested7B,13B,and70BmodelsintheLlama2series. Fortestingthemathhypothesis,
wecomparedthefollowingfourpairsofmodels: (CodeLlama,Llemma),(DeepSeek-Coder,
DeepSeek-Math),(Llama,FLoat),and(Mistral,OpenMathMistral). Again,thesecondmodel
ineachpairisobtainedbytrainingthefirstmodelonadditionalmathdata. Foralignment
tuning,wecompared(Llama2,Llama2-chat),(CodeLlama,CodeLlama-Instruct),(Gemma,
Gemma-Instruct),(CodeGemma,CodeGemma-Instruct),(DeepSeek,DeepSeek-Chat),and
(DeepSeek-Coder,DeepSeek-Coder-Instruct). ThesecomparisonsaresummarizedinTable1.
SeeAppendixA.1,Table4formoredetailsaboutthemodels.
3.2 Evaluationsetup
Weadoptedtheboxestask(the“base”version)fromKim&Schuster(2023)fortestingthe
models’entitytrackingcapacity. Inthistask,theinputtotheLLMisatextualdescription
ofthecontentsofsevenboxesfollowedby1–12descriptionsofoperationsthatchangethe
contentsoftheindividualboxes. Inresponsetothisinput,theLLMispromptedtostatethe
contentsofeachboxaccordingtotheinitialdescriptionandthestate-changingoperations.
Weusedthesamepromptand2-shotin-contextlearningexamplesasKim&Schuster(2023)
(seeTable2foranexample). Weusedaslightlydifferentpromptformatforchat-optimized
modelstoalignthetaskbettertotheinputformatthemodelsweretrainedon. Theinputsto
themodelareprovidedas“user”prompts,andtheexpectedmodeloutputsareformatted
as“assistant”(seeTable5inAppendixforanexampleprompt).
3DeepSeek (7B) Gemma (8B) Llama 2 (7B) Llama 2 (13B) Llama 2 (70B)
100
75
50
25
0
0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7
Number of operations affecting box state
Additional code training no yes
Figure1: EntitytrackingresultsforDeepSeek,Gemma,andLlama2models. Errorbars
indicate95%confidenceintervals,andtheblackdashedlinesshowtheperformanceofthe
randombaseline.
Wenoticedthatsmallermodelssufferedfromformattingissues,oftendeviatingfromthe
formatspecifiedbythepromptoromittingthecontentsofsomeboxes. Forthisreason,we
usedregularexpression-basedconstraineddecodingusingtheoutlineslibrary(Willard&
Louf,2023).2
Wereportallresultsdividedintothenumberofoperationsaffectingthetargetboxrather
than reporting one aggregate accuracy metric. This is to distinguish trivial cases from
casesthatactuallyrequiretrackingstatechanges—whenthenumberofoperationsaffecting
the target box is 0, simply copying from the initial state description yields the correct
answer. Furthermore,wecomparethemodelresultstothestrongrandombaselinebyKim
&Schuster(2023). Forthisbaseline,werandomlysample0to3objectsforeachboxfrom
thesetofobjectsthathavebeenpreviouslymentionedinaclausewiththeboxinquestion.
3.3 Experiment1: EffectofCode
Figure 1 compares the entity tracking performance of base models (red lines) and code
models (blue lines) for models from the DeepSeek (DeepSeek-AI et al., 2024; Guo et al.,
2024),Gemma(GemmaTeametal.,2024),andLlama2familiesofvarioussizes(Touvron
etal.,2023b;Rozièreetal.,2024). Ingeneral,wefindclearevidencethatcontinuedtraining
onlargeamountsofcodeimprovesentitytrackingabilities,ascanbeseenfortheLlama
213Band70BmodelsaswellasfortheDeepSeekmodels. Inthesemodelcomparisons,
themodelstrainedoncodeconsistentlyoutperformedthebasemodelsonthenontrivial
casesofentitytracking(numberofoperationsaffectingboxstate≥1). Inthecaseof13B
models,aboostintrivialcasesisalsoobserved(numberofoperations=0);in70Bmodels,
performanceonthetrivialcasesisalreadysaturatedinthebasemodel.
InLlama27Bmodels,thegainsthroughadditionalcodetrainingarerelativelyminor,with
mostofthegainsderivingfromboostsinexampleswherethenumberofoperationsiseither
0or1. SimilarlyminorgainswereobservedinCodeGemma8B,exceptthatthegainswere
observedinexampleswith1and2operations. Forbothofthesemodels,wealsoobserved
thatfornumberofoperationsgreaterthan0(Llama27B)and2(Gemma8B),neitherthe
basenorthecodevariantsperformbetterthanourrandombaseline. Theseresultssuggest
thatthereisbothapossibleeffectofscaleintheeffectivenessofcodetrainingasobservedin
theLlama2series,andaneffectoftheamountofadditionalcodetraining(DeepSeek-Coder:
2Ttokens,CodeLlama: 500Btokens).
3.4 Experiment2: EffectofMath
In evaluating the effect of additional math training, we start by revisiting the claim of
Prakash et al. (2024) that the FLoat model obtained by finetuning Llama 7B (Touvron
et al., 2023a) on arithmetic tasks from Liu & Low (2023) yields superior entity tracking
2https://github.com/outlines-dev/outlines
4
ycaruccAFLoat (7B) OpenMath−Mistral (7B) DeepSeek−Math (7B) Llemma (7B) Llemma (34B)
100
75
50
25
0
0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7
Number of operations affecting box state
Additional math training no yes
Figure2: Entitytrackingresultsformodelstrainedwithadditionalmathdata. SeeTable1
for the model names of the base and math models. Error bars indicate 95% confidence
intervals,andtheblackdashedlinesshowtheperformanceoftherandombaseline.
performance. AscanbeseeninTable3,FLoatdidshowslightlyhigheraccuracyonthe
non-trivialtrackingcases(numberofoperations ≥ 1)thanthebasemodel, butthegain
wasmarginal.3 Furthermore,neitherthebaseLlamamodelnortheFLoatmodelperformed
betterthanourrandombaselineonnon-trivialentitytrackingexamples(Figure2,farleft).
Model Aggregate NumOps=0 NumOps≥1
Llama7B 28.67 97.93 4.34
FLoat7B 27.55 89.33 5.85
Table3: Llama7Bvs. FLoat7Bresults.
Following this observation, we compared Mistral and OpenMathMistral models where
the latter is a model further trained on OpenMathInstruct-1, a synthetically generated
instruction-tuningdatasetcontaining1.8Muniquesolutionstomathproblemssourcedfrom
MATHandGSM8Kdatasets(Toshniwaletal.,2024). AsshowninFigure2,OpenMathMis-
tralonlyachievedmarginalgainsoverthebaseMistralmodelwhenthereare7operations
affectingthetargetbox,andinmostothercases,thebasemodelconsistentlyoutperforms
themath-finetunedmodel. Furthermore,neithermodeloutperformedtherandombaseline
forexampleswithmorethan2operationsaffectingtheboxofinterest.
Theunclearbenefitofadditionalmathtrainingisfurthercorroboratedbymarginalgainsin
modelstrainedonmathdatathatarenotin“instruct”formatlikeFLoatandOpenMath-
Mistral. Figure 2 shows that DeepSeek-Math (Shao et al., 2024) performed close to the
DeepSeek-Codermodelformostcases. Thegainsareevenmorelimitedinthecomparison
betweenCodeLlamavsLlemma(Azerbayevetal.,2024). Llemma34BoutperformedCode
Llama 34B by a narrow margin for the non-trivial tracking cases (Llemma: 47.86, Code
Llama: 45.46forexampleswherethenumberofoperations≥ 1). Theseresultssuggesta
limitedbenefitofadditionalmathpretrainingonentitytracking.
3.5 Experiment3: EffectofAlignmentTuning
Finally,weexploretheeffectofalignmenttuningonentitytracking.FormodelsintheLlama
2family,alignmenttuningthebasemodelsledtominorgains(Figure3,orangevs. green
linesinthetoprowpanels),whereasalignment-tunedcodemodelsdidnotconsistently
leadtogainsandsometimesperformedworsethanthenon-alignment-tunedcounterparts
(orange vs. green lines in the bottom row panels). Nevertheless, the best-performing
modelwasCodeLlama70B-instruct(64.9accuracyon1+operations),combiningcodeand
alignmenttuning.
3ThesenumbersarenotexpectedtoalignwithnumbersreportedinPrakashetal.(2024)because
theyusedamodifiedversionoftheoriginaltask.
5
ycaruccADeepSeek (7B) Gemma (8B) Llama 2 (7B) Llama 2 (13B) Llama 2 (70B)
100
75
50
25
0
100
75
50
25
0
0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7
Number of operations affecting box state
base instruct/chat
Figure 3: Entity tracking results for alignment-tuned DeepSeek, Gemma and Llama 2
models. Thetoppanelsshowmodelswithoutadditionalcodetraining,whereasthebottom
panelsshowmodelsthathavebeentrainedonadditionalamountsofcodebeforealignment
tuning. Errorbarsindicate95%confidenceintervals,andtheblackdashedlinesshowthe
performanceoftherandombaseline.
TheDeepSeekmodelsshowedsimilartrendstothegeneralobservationmadeabove: align-
menttuningofthebasemodelledtogains,whereasalignmenttuningofthecodemodeldid
not. Gemma8BandCodeGemma8Bmodelsdidbenefitfromalignmenttuning,similarly
toLlama27BandCodeLlama7Bmodels,althoughthegainsweresmaller.
Overall,alignmenttuningaffectsbaseandcodemodelsdifferently,wherethegainsforbase
modelstendtobegreater. Thebenefitofalignmenttuningforbasemodelsseemstobe
inverselycorrelatedwithscale: smallerbasemodelsbenefitmorefromalignmenttuning.
4 ConclusionandFutureWork
We explored the effect of code, math, and alignment tuning on LLMs’ capacity to track
entitiesinnaturallanguagetext. Ourmainfindingsarethreefold:
1. Additionalcodetrainingleadstoconsistentimprovementsacrossmodelfamilies
andsizes.
2. Additionalmathtrainingdoesnotyieldconsistentimprovements,andtheperfor-
mancegainsareatbestmarginal.
3. Alignmenttuningleadstodifferentpatternsofimprovementdependingonwhether
itwasappliedtobasemodelsorcodemodels.Basemodelsconsistentlybenefitfrom
alignmenttuning,andsmallermodelsseemoreimprovement. Thebenefitforcode
modelsismoremixed,butthebestperformanceisachievedthroughcombining
codeandinstructiontuning.
Ourworkthusaddstoagrowingbodyofliteraturethatsuggeststhatpretrainingoncode
improvesLLMperformanceonreasoningtasks,includingcommonsensereasoning(Madaan
etal.,2022),chain-of-thoughtreasoning(Weietal.,2022b),mathematicalproblems(Razeghi
etal.,2024),andentitytrackingtasks(Muennighoffetal.,2023). Whymightthisbethecase?
Kim&Schuster(2023)arguedthatkeepingtrackofthestatesofvariablesisimportantfor
producingcorrectcode,andhypothesizedthatthiskindofproceduralinputmayprovidea
6
ycaruccA
no
add.
code
training
add.
code
trainingstrongertrainingsignalthanpurenaturallanguagetext. Weconsiderinvestigatinghow
codetrainingimbuesmodelswithentitytrackingandotherreasoningabilitiesanimportant
directionforfutureresearch.
Limitations Whilethepairsofmodelswecomparedare“minimalpairs”,severalpossible
confoundsremaininourinterpretation. Forexample,weinterpretedpartsoftheresults
inExperiment2asmarginalbenefitofmathcomparedtocode,buttheOpenMathInstruct
dataset (1.5 GB) is two orders of magnitude smaller in terms of the number of tokens
comparedtoCodeLlama’scodedata(500Btokens),sothesizeoftheadditionaltraining
datacouldbeaconfound. TheadditionalmathtrainingdataofDeepSeek-Mathismore
comparable(120Btokens),butwedonothaveamodelthatisonlycontinuallytrainedon
mathdata;DeepSeek-Mathistrainedonbothcodeandmath. Furthermore,themathdata
varyalongseveralotherimportantdimensions: OpenMathInstruct,andFLoatmodelsuse
syntheticdata,whereasothersusenaturallyoccurringdata. FLoatandOpenMathMistral
aretunedonmathdataininstructionformat,whereasthetrainingdataofDeepSeek-Math
andLlemmaarenot. Unfortunately,wecannotfullyteaseaparttheeffectoftheformatof
themathdata(instructvs. non-instruct)herebecausetheformatco-varieswithwhether
codewasadditionallyinthetrainingmixture: allmodelsthatwereadditionallytrained
withnon-instruction-formattedmathdatawerecontinuouslytrainedfrommodelsthatwere
trainedoncodealready. Lastly,fortheexperimentsinvestigatingtheeffectofalignment
tuning, we considered models that were alignment tuned through a range of different
methods and types of data, and some of the diverging findings in this experiment may
beattributedtothesedifferences. Weplantoaddresstheseexistinglimitationsthrough
controlledtrainingexperimentsinfutureinvestigations.
Acknowledgments
WethankAbdulRafayforrunningpreliminaryexperimentsaspartofhismaster’sthesis.
WealsothankCookie.
References
ZhangirAzerbayev,HaileySchoelkopf,KeiranPaster,MarcoDosSantos,StephenMarcus
McAleer,AlbertQ.Jiang,JiaDeng,StellaBiderman,andSeanWelleck. Llemma:AnOpen
LanguageModelforMathematics. InICLR,2024.
MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveiraPinto,
JaredKaplan,HarriEdwards,YuriBurda,NicholasJoseph,GregBrockman,AlexRay,
RaulPuri,GretchenKrueger,MichaelPetrov,HeidyKhlaaf,GirishSastry,PamelaMishkin,
BrookeChan,ScottGray,NickRyder,MikhailPavlov,AletheaPower,LukaszKaiser,Mo-
hammadBavarian,ClemensWinter,PhilippeTillet,FelipePetroskiSuch,DaveCummings,
MatthiasPlappert,FotiosChantzis,ElizabethBarnes,ArielHerbert-Voss,WilliamHebgen
Guss,AlexNichol,AlexPaino,NikolasTezak,JieTang,IgorBabuschkin,SuchirBalaji,
ShantanuJain,WilliamSaunders,ChristopherHesse,AndrewN.Carr,JanLeike,Josh
Achiam,VedantMisra,EvanMorikawa,AlecRadford,MatthewKnight,MilesBrundage,
MiraMurati,KatieMayer,PeterWelinder,BobMcGrew,DarioAmodei,SamMcCandlish,
IlyaSutskever,andWojciechZaremba. EvaluatingLargeLanguageModelsTrainedon
Code. arXiv:2107.03374,2021.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
AdamRoberts,PaulBarham,HyungWonChung,CharlesSutton,SebastianGehrmann,
ParkerSchuh,KensenShi,SashaTsvyashchenko,JoshuaMaynez,AbhishekRao,Parker
Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben
Hutchinson, ReinerPope, JamesBradbury, JacobAustin, MichaelIsard, GuyGur-Ari,
Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Hen-
ryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny
Zhou,DaphneIppolito,DavidLuan,HyeontaekLim,BarretZoph,AlexanderSpiridonov,
RyanSepassi,DavidDohan,ShivaniAgrawal,MarkOmernick,AndrewM.Dai,Thanu-
malayanSankaranarayanaPillai,MariePellat,AitorLewkowycz,EricaMoreira,Rewon
7Child,OleksandrPolozov,KatherineLee,ZongweiZhou,XuezhiWang,BrennanSaeta,
Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas
Eck,JeffDean,SlavPetrov,andNoahFiedel. PaLM:ScalingLanguageModelingwith
Pathways. JMLR,2023.
BhavanaDalvi,NiketTandon,AntoineBosselut,Wen-tauYih,andPeterClark. Everything
HappensforaReason:DiscoveringthePurposeofActionsinProceduralText. InEMNLP-
IJCNLP,2019.
DeepSeek-AI,:,XiaoBi,DeliChen,GuantingChen,ShanhuangChen,DamaiDai,Chengqi
Deng,HonghuiDing,KaiDong,QiushiDu,ZheFu,HuazuoGao,KaigeGao,Wenjun
Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao,
Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li,
WenfengLiang,FangyunLin,A.X.Liu,BoLiu,WenLiu,XiaodongLiu,XinLiu,Yiyuan
Liu,HaoyuLu,ShanghaoLu,FuliLuo,ShirongMa,XiaotaoNie,TianPei,YishiPiao,
JunjieQiu,HuiQu,TongzhengRen,ZehuiRen,ChongRuan,ZhangliSha,ZhihongShao,
JunxiaoSong,XuechengSu,JingxiangSun,YaofengSun,MinghuiTang,BingxuanWang,
PeiyiWang,ShiyuWang,YaohuiWang,YongjiWang,TongWu,Y.Wu,XinXie,Zhenda
Xie,ZiweiXie,YiliangXiong,HanweiXu,R.X.Xu,YanhongXu,DejianYang,Yuxiang
You,ShuipingYu,XingkaiYu,B.Zhang,HaoweiZhang,LecongZhang,LiyueZhang,
MingchuanZhang,MinghuaZhang,WentaoZhang,YichaoZhang,ChenggangZhao,
YaoZhao,ShangyanZhou,ShunfengZhou,QihaoZhu,andYuhengZou. DeepSeekLLM:
ScalingOpen-SourceLanguageModelswithLongtermism,2024.
Yao Fu, Hao Peng, and Tushar Khot. How does GPT Obtain its
Ability? Tracing Emergent Abilities of Language Models to their
Sources. Yao Fu’s Notion, Dec 2022. URL https://yaofu.notion.
site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-\
Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1.
GeminiTeam,RohanAnil,SebastianBorgeaud,Jean-BaptisteAlayrac,JiahuiYu,RaduSori-
cut,JohanSchalkwyk,AndrewM.Dai,AnjaHauth,KatieMillican,DavidSilver,Melvin
Johnson,IoannisAntonoglou,JulianSchrittwieser,AmeliaGlaese,JilinChen,EmilyPitler,
TimothyLillicrap,AngelikiLazaridou,OrhanFirat,JamesMolloy,MichaelIsard,PaulR.
Barham,TomHennigan,BenjaminLee,FabioViola,MalcolmReynolds,YuanzhongXu,
Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem
Ayoub, Megha Goel, Jack Krawczyk, Cosmo Du, Ed Chi, Heng-Tze Cheng, Eric Ni,
PurviShah,PatrickKane,BettyChan,ManaalFaruqui,AliakseiSeveryn,HanzhaoLin,
YaGuangLi,YongCheng,AbeIttycheriah,MahdisMahdieh,MiaChen,PeiSun,Dustin
Tran,SumitBagri,BalajiLakshminarayanan,JeremiahLiu,AndrasOrban,FabianGüra,
HaoZhou,XinyingSong,AurelienBoffy,HarishGanapathy,StevenZheng,HyunJeong
Choe,ÁgostonWeisz,TaoZhu,YifengLu,SiddharthGopal,JarrodKahn,MaciejKula,
JeffPitman,RushinShah,EmanuelTaropa,MajdAlMerey,MartinBaeuml,ZhifengChen,
Laurent El Shafey, Yujing Zhang, Olcan Sercinoglu, George Tucker, Enrique Piqueras,
MaximKrikun,IainBarr,NikolaySavinov,IvoDanihelka,BeccaRoelofs,AnaïsWhite,
AndersAndreassen,TamaravonGlehn,LakshmanYagati,MehranKazemi,LucasGonza-
lez,MishaKhalman,JakubSygnowski,AlexandreFrechette,CharlotteSmith,LauraCulp,
LevProleev,YiLuan,XiChen,JamesLottes,NathanSchucher,FedericoLebron,Alban
Rrustemi,NatalieClay,PhilCrone,TomasKocisky,JeffreyZhao,BartekPerz,DianYu,
HeidiHoward,AdamBloniarz,JackW.Rae,HanLu,LaurentSifre,MarcelloMaggioni,
Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel
Barth-Maron,WilliamWong,RishabhJoshi,RahmaChaabouni,DeeniFatiha,ArunAhuja,
Gaurav Singh Tomar, Evan Senter, Martin Chadwick, Ilya Kornakov, Nithya Attaluri,
IñakiIturrate,RuiboLiu,YunxuanLi,SarahCogan,JeremyChen,ChaoJia,ChenjieGu,
QiaoZhang,JordanGrimstad,AleJakseHartman,XavierGarcia,ThanumalayanSankara-
narayanaPillai,JacobDevlin,MichaelLaskin,DiegodeLasCasas,DashaValter,Connie
Tao, LorenzoBlanco, AdriàPuigdomènechBadia, DavidReitter, MiannaChen, Jenny
Brennan,ClaraRivera,SergeyBrin,ShariqIqbal,GabrielaSurita,JaneLabanowski,Abhi
Rao,StephanieWinkler,EmilioParisotto,YimingGu,KateOlszewska,RaviAddanki,
AntoineMiech,AnnieLouis,DenisTeplyashin,GeoffBrown,ElliotCatt,JanBalaguer,
8JackieXiang,PidongWang,ZoeAshwood,AntonBriukhov,AlbertWebson,SanjayGana-
pathy,SmitSanghavi,AjayKannan,Ming-WeiChang,AxelStjerngren,JosipDjolonga,
YutingSun,AnkurBapna,MatthewAitchison,PedramPejman,HenrykMichalewski,
TianheYu,CindyWang,JulietteLove,JunwhanAhn,DawnBloxwich,KehangHan,Peter
Humphreys,ThibaultSellam,JamesBradbury,VarunGodbole,SinaSamangooei,Bogdan
Damoc,AlexKaskasoli,SébastienM.R.Arnold,VijayVasudevan,ShubhamAgrawal,
JasonRiesa,DmitryLepikhin,RichardTanburn,SrivatsanSrinivasan,HyeontaekLim,
Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le
Paine,JianLi,YujiaLi,MinhGiang,AlexanderNeitz,ZaheerAbbas,SarahYork,Machel
Reid,ElizabethCole,AakankshaChowdhery,DipanjanDas,DominikaRogozin´ska,Vi-
taliyNikolaev,PabloSprechmann,ZacharyNado,LukasZilka,FlavienProst,Luheng
He,MarianneMonteiro,GauravMishra,ChrisWelty,JoshNewlan,DaweiJia,Miltiadis
Allamanis,ClaraHuiyiHu,RaouldeLiedekerke,JustinGilmer,CarlSaroufim,Shruti
Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan
Ozturel,AlbinCassirer,YunhanXu,DanielSohn,DevendraSachan,ReinaldKimAm-
playo,CraigSwanson,DessiePetrova,ShashiNarayan,ArthurGuez,SiddharthaBrahma,
Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia,
MatthewRahtz,MaiGiménez,LeggYeung,JamesKeeling,PetkoGeorgiev,DianaMincu,
BoxiWu,SalemHaykal,RachelSaputro,KiranVodrahalli,JamesQin,ZeynepCankara,
AbhanshuSharma,NickFernando,WillHawkins,BehnamNeyshabur,SolomonKim,
AdrianHutter,PriyankaAgrawal,AlexCastro-Ros,GeorgevandenDriessche,TaoWang,
FanYang,ShuoyiinChang,PaulKomarek,RossMcIlroy,MarioLucˇic´,GuodongZhang,
WaelFarhan,MichaelSharman,PaulNatsev,PaulMichel,YaminiBansal,SiyuanQiao,
KrisCao,SiamakShakeri,ChristinaButterfield,JustinChung,PaulKishanRubenstein,
ShivaniAgrawal,ArthurMensch,KedarSoparkar,KarelLenc,TimothyChung,Aedan
Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary
Phuong,TaylorTobin,AndreaTacchetti,MajaTrebacz,KevinRobinson,YashKatariya,
SebastianRiedel,PaigeBailey,KefanXiao,NimeshGhelani,LoraAroyo,AmbroseSlone,
NeilHoulsby,XuehanXiong,ZhenYang,ElenaGribovskaya,JonasAdler,MateoWirth,
Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova,
Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko
Iinuma,PolinaZablotskaia,JamesBesley,Da-WoonChung,TimothyDozat,RamonaCo-
manescu,XianceSi,JeremyGreer,GuolongSu,MartinPolacek,RaphaëlLopezKaufman,
SimonTokumine,HexiangHu,ElenaBuchatskaya,YingjieMiao,MohamedElhawaty,
AdityaSiddhant,NenadTomasev,JinweiXing,ChristinaGreer,HelenMiller,Shereen
Ashraf, AurkoRoy, ZizhaoZhang, AdaMa, AngelosFilos, MilosBesta, RoryBlevins,
Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas
Pajarskas,CarrieMuir,VeredCohen,CharlineLeLan,KrishnaHaridasan,AmitMarathe,
StevenHansen,SholtoDouglas,RajkumarSamuel,MingqiuWang,SophiaAustin,Chang
Lan,JiepuJiang,JustinChiu,JaimeAlonsoLorenzo,LarsLoweSjösund,SébastienCevey,
ZachGleicher,ThiAvrahami,AnudhyanBoral,HansaSrinivasan,VittorioSelo,RhysMay,
KonstantinosAisopos,LéonardHussenot,LivioBaldiniSoares,KateBaumli,MichaelB.
Chang,AdriàRecasens,BenCaine,AlexanderPritzel,FilipPavetic,FabioPardo,Anita
Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner,
SubhrajitRoy,EthanDyer,VíctorCamposCampos,AlexTomala,YunhaoTang,DaliaEl
Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram,
ZhitaoGong, SergiCaelles, RossHemsley, GregoryThornton, FangxiaoyuFeng, Woj-
ciechStokowiec,CeZheng,PhoebeThacker,Çag˘larÜnlü,ZhishuaiZhang,Mohammad
Saleh,JamesSvensson,MaxBileschi,PiyushPatil,AnkeshAnand,RomanRing,Katerina
Tsihlas, ArpiVezer, MarcoSelvi, TobyShevlane, MikelRodriguez, TomKwiatkowski,
SamiraDaruki,KeranRong,AllanDafoe,NicholasFitzGerald,KerenGu-Lemberg,Mina
Khan,LisaAnneHendricks,MariePellat,VladimirFeinberg,JamesCobon-Kerr,Tara
Sainath,MaribethRauh,SayedHadiHashemi,RichardIves,YanaHasson,EricNoland,
Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini,
Jean-BaptisteLespiau,AlexandreMoufarek,SamerHassan,KaushikShivakumar,Joost
van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, An-
drew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja Rakic´evic´, Mostafa
Dehghani,FangyuLiu,SidMittal,JunhyukOh,SebNoury,ErenSezener,FantineHuot,
MatthewLamm,NicolaDeCao,CharlieChen,SidharthMudgal,RominaStella,Kevin
9Brooks, Gautam Vasudevan, Chenxi Liu, Mainak Chain, Nivedita Melinkeri, Aaron
Cohen, Venus Wang, Kristie Seymore, Sergey Zubkov, Rahul Goel, Summer Yue, Sai
Krishnakumaran, BrianAlbert, NateHurley, MotokiSano, AnhadMohananey, Jonah
Joughin,EgorFilonov,TomaszKepa,YomnaEldawy,JiawernLim,RahulRishi,Shirin
Badiezadegan,TaylorBos,JerryChang,SanilJain,SriGayatriSundaraPadmanabhan,
SubhaPuttagunta,KalpeshKrishna,LeslieBaker,NorbertKalb,VamsiBedapudi,Adam
Kurzrok, Shuntong Lei, Anthony Yu, Oren Litvin, Xiang Zhou, Zhichun Wu, Sam So-
bell,AndreaSiciliano,AlanPapir,RobbyNeale,JonasBragagnolo,TejToor,TinaChen,
ValentinAnklin,FeiranWang,RichieFeng,MiladGholami,KevinLing,LijuanLiu,Jules
Walter, Hamid Moghaddam, Arun Kishore, Jakub Adamek, Tyler Mercado, Jonathan
Mallinson,SiddhinitaWandekar,StephenCagle,EranOfek,GuillermoGarrido,Clemens
Lombriser,MaksimMukha,BotuSun,HafeezulRahmanMohammad,JosipMatak,Yadi
Qian,VikasPeswani,PawelJanus,QuanYuan,LeifSchelin,OanaDavid,AnkurGarg,
YifanHe,OleksiiDuzhyi,AntonÄlgmyr,TimothéeLottaz,QiLi,VikasYadav,LuyaoXu,
AlexChinien,RakeshShivanna,AleksandrChuklin,JosieLi,CarrieSpadine,TravisWolfe,
KareemMohamed,SubhabrataDas,ZihangDai,KyleHe,DanielvonDincklage,Shyam
Upadhyay, Akanksha Maurya, Luyan Chi, Sebastian Krause, Khalid Salama, Pam G
Rabinovitch,PavanKumarReddyM,AarushSelvan,MikhailDektiarev,GolnazGhiasi,
Erdem Guven, Himanshu Gupta, Boyi Liu, Deepak Sharma, Idan Heimlich Shtacher,
ShachiPaul,OscarAkerlund,François-XavierAubet,TerryHuang,ChenZhu,EricZhu,
ElicoTeixeira,MatthewFritze,FrancescoBertolini,Liana-EleonoraMarinescu,Martin
Bölle,DominikPaulus,KhyattiGupta,TejasiLatkar,MaxChang,JasonSanders,Roopa
Wilson,XueweiWu,Yi-XuanTan,LamNguyenThiet,TulseeDoshi,SidLall,Swaroop
Mishra,WanmingChen,ThangLuong,SethBenjamin,JasmineLee,EwaAndrejczuk,
DominikRabiej,VipulRanjan,KrzysztofStyrc,PengchengYin,JonSimon,MalcolmRose
Harriott, MuditBansal, AlexeiRobsky, GeoffBacon, DavidGreene, DaniilMirylenka,
ChenZhou,ObaidSarvana,AbhimanyuGoyal,SamuelAndermatt,PatrickSiegler,Ben
Horn,AssafIsrael,FrancescoPongetti,Chih-Wei"Louis"Chen,MarcoSelvatici,Pedro
Silva,KathieWang,JacksonTolins,KelvinGuu,RoeyYogev,XiaochenCai,Alessandro
Agostini,MaulikShah,HungNguyen,NoahÓDonnaile,SébastienPereira,LindaFriso,
AdamStambler,AdamKurzrok,ChenkaiKuang,YanRomanikhin,MarkGeller,ZJYan,
KaneJang,Cheng-ChunLee,WojciechFica,EricMalmi,QijunTan,DanBanica,Daniel
Balle,RyanPham,YanpingHuang,DianaAvram,HongzhiShi,JasjotSingh,ChrisHidey,
NiharikaAhuja,PranabSaxena,DanDooley,SrividyaPranaviPotharaju,EileenO’Neill,
AnandGokulchandran,RyanFoley,KaiZhao,MikeDusenberry,YuanLiu,PulkitMehta,
RaghaKotikalapudi,ChalenceSafranek-Shrader,AndrewGoodman,JoshuaKessinger,
EranGloben, PrateekKolhar, ChrisGorgolewski, AliIbrahim, YangSong, AliEichen-
baum,ThomasBrovelli,SahityaPotluri,PreethiLahoti,CipBaetu,AliGhorbani,Charles
Chen,AndyCrawford,ShaliniPal,MukundSridhar,PetruGurita,AsierMujika,Igor
Petrovski,Pierre-LouisCedoz,ChenmeiLi,ShiyuanChen,NiccolòDalSanto,Siddharth
Goyal,JiteshPunjabi,KarthikKappaganthu,ChesterKwak,PallaviLV,SarmishtaVelury,
HimadriChoudhury,JamieHall,PremalShah,RicardoFigueira,MattThomas,MinjieLu,
TingZhou,ChintuKumar,ThomasJurdi,SharatChikkerur,YenaiMa,AdamsYu,Soo
Kwak,VictorÄhdel,SujeevanRajayogam,TravisChoma,FeiLiu,AdityaBarua,Colin
Ji,JiHoPark,VincentHellendoorn,AlexBailey,TaylanBilal,HuanjieZhou,Mehrdad
Khatir,CharlesSutton,WojciechRzadkowski,FionaMacintosh,KonstantinShagin,Paul
Medina,ChenLiang,JinjingZhou,PararthShah,YingyingBi,AttilaDankovics,Shipra
Banga,SabineLehmann,MarissaBredesen,ZifanLin,JohnEricHoffmann,JonathanLai,
RaynaldChung,KaiYang,NihalBalani,ArthurBražinskas,AndreiSozanschi,Matthew
Hayes,HéctorFernándezAlcalde,PeterMakarov,WillChen,AntonioStella,Liselotte
Snijders,MichaelMandl,AnteKärrman,PawełNowak,XinyiWu,AlexDyck,Krishnan
Vaidyanathan,RaghavenderR,JessicaMallet,MitchRudominer,EricJohnston,Sushil
Mittal,AkhilUdathu,JanaraChristensen,VishalVerma,ZachIrving,AndreasSantucci,
GamaleldinElsayed, ElnazDavoodi, MarinGeorgiev, IanTenney, NanHua, Geoffrey
Cideron,EdouardLeurent,MahmoudAlnahlawi,IonutGeorgescu,NanWei,IvyZheng,
DylanScandinaro,HeinrichJiang,JasperSnoek,MukundSundararajan,XuezhiWang,
ZackOntiveros,ItayKaro,JeremyCole,VinuRajashekhar,LaraTumeh,EyalBen-David,
Rishub Jain, Jonathan Uesato, Romina Datta, Oskar Bunyan, Shimu Wu, John Zhang,
10Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew
Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin,
FaizanMuhammad, JinMiao, AndrewLee, NinoVieillard, JanePark, JiagengZhang,
Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar,
LuoweiZhou,JonathanEvens,WilliamIsaac,GeoffreyIrving,EdwardLoper,Michael
Fink,IshaArkatkar,NanxinChen,IzhakShafran,IvanPetrychenko,ZheChen,Johnson
Jia,AnselmLevskaya,ZhenkaiZhu,PeterGrabowski,YuMao,AlbertoMagni,Kaisheng
Yao,JavierSnaider,NormanCasagrande,EvanPalmer,PaulSuganthan,AlfonsoCastaño,
IreneGiannoumis,WooyeolKim,MikołajRybin´ski,AshwinSreevatsa,JenniferPrendki,
DavidSoergel,AdrianGoedeckemeyer,WilliGierke,MohsenJafari,MeenuGaba,Jeremy
Wiesner,DianaGageWright,YawenWei,HarshaVashisht,YanaKulizhskaya,JayHoover,
MaigoLe,LuLi,ChimezieIwuanyanwu,LuLiu,KevinRamirez,AndreyKhorlin,Albert
Cui,TianLIN,MarcusWu,RicardoAguilar,KeithPallo,AbhishekChakladar,Ginger
Perng, Elena Allica Abellan, Mingyang Zhang, Ishita Dasgupta, Nate Kushman, Ivo
Penchev,AlenaRepina,XihuiWu,TomvanderWeide,PriyaPonnapalli,CarolineKa-
plan,JiriSimsa,ShuangfengLi,OlivierDousse,FanYang,JeffPiper,NathanIe,Rama
Pasumarthi,NathanLintz,AnithaVijayakumar,DanielAndor,PedroValenzuela,Min-
nieLui,CosminPaduraru,DaiyiPeng,KatherineLee,ShuyuanZhang,SomerGreene,
DucDungNguyen,PaulaKurylowicz,CassidyHardin,LucasDixon,LiliJanzer,Kiam
Choo,ZiqiangFeng,BiaoZhang,AchintyaSinghal,DayouDu,DanMcKinnon,Natasha
Antropova,TolgaBolukbasi,OrgadKeller,DavidReid,DanielFinchelstein,MariaAbi
Raad,RemiCrocker,PeterHawkins,RobertDadashi,ColinGaffney,KenFranko,Anna
Bulanova,RémiLeblond,ShirleyChung,HarryAskham,LuisC.Cobo,KelvinXu,Felix
Fischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Alek
Dimitriev,HannahForbes,DylanBanarse,ZoraTung,MarkOmernick,ColtonBishop,
RachelSterneck,RohanJain,JiaweiXia,EhsanAmid,FrancescoPiccinno,XingyuWang,
PraseemBanzal,DanielJ.Mankowitz,AlexPolozov,VictoriaKrakovna,SashaBrown,
MohammadHosseinBateni,DennisDuan,VladFiroiu,MeghanaThotakuri,TomNatan,
MatthieuGeist,SertanGirgin,HuiLi,JiayuYe,OfirRoval,ReikoTojo,MichaelKwong,
James Lee-Thorp, Christopher Yew, Danila Sinopalnikov, Sabela Ramos, John Mellor,
AbhishekSharma,KathyWu,DavidMiller,NicolasSonnerat,DenisVnukov,RoryGreig,
JenniferBeattie,EmilyCaveness,LibinBai,JulianEisenschlos,AlexKorchemniy,Tomy
Tsai,MimiJasarevic,WeizeKong,PhuongDao,ZeyuZheng,FrederickLiu,FanYang,
RuiZhu,TianHueyTeh,JasonSanmiya,EvgenyGladchenko,NejcTrdin,DanielToyama,
Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Car-
penter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, Rishika
Sinha,AliceTalbert,DianeWu,DeneseOwusu-Afriyie,CosmoDu,ChloeThornton,Jordi
Pont-Tuset,PradyumnaNarayana,JingLi,SaaberFatehi,JohnWieting,OmarAjmeri,Be-
nignoUria,YeongilKo,LauraKnight,AmélieHéliou,NingNiu,ShaneGu,ChenxiPang,
YeqingLi,NirLevine,ArielStolovich,RebecaSantamaria-Fernandez,SonamGoenka,
WennyYustalim,RobinStrudel,AliElqursh,CharlieDeck,HyoLee,ZonglinLi,Kyle
Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Sho Arora, Christy
Koh,SoheilHassasYeganeh,SiimPõder,MukarramTariq,YanhuaSun,LucianIonita,
MojtabaSeyedhosseini,PouyaTafti,ZhiyuLiu,AnmolGulati,JasmineLiu,XinyuYe,
BartChrzaszcz,LilyWang,NikhilSethi,TianrunLi,BenBrown,ShreyaSingh,WeiFan,
AaronParisi,JoeStanton,VinodKoverkathu,ChristopherA.Choquette-Choo,Yunjie
Li, TJ Lu, Abe Ittycheriah, Prakash Shroff, Mani Varadarajan, Sanaz Bahargam, Rob
Willoughby,DavidGaddy,GuillaumeDesjardins,MarcoCornero,BronaRobenek,Bhav-
ishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza
Ghaffarkhah,MorganeRivière,AlannaWalton,ClémentCrepy,AliciaParrish,Zongwei
Zhou,ClementFarabet,CareyRadebaugh,PraveenSrinivasan,ClaudiavanderSalm,
AndreasFidjeland,SalvatoreScellato,EriLatorre-Chimoto,HannaKlimczak-Plucin´ska,
David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker,
AlexMorris,MatthewMauger,AlexeyGuseynov,AlisonReid,SethOdoom,LuciaLoher,
VictorCotruta,MadhaviYenugula,DominikGrewe,AnastasiaPetrushkina,TomDuerig,
AntonioSanchez,SteveYadlowsky,AmyShen,AmirGloberson,LynetteWebb,SahilDua,
DongLi,SuryaBhupatiraju,DanHurt,HaroonQureshi,AnanthAgarwal,TomerShani,
MatanEyal,AnujKhare,ShreyasRammohanBelle,LeiWang,ChetanTekur,MihirSanjay
Kale,JinliangWei,RuoxinSang,BrennanSaeta,TylerLiechty,YiSun,YaoZhao,Stephan
11Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas,
Martin Wicke, Xiao Ma, Evgenii Eltyshev, Nina Martin, Hardie Cate, James Manyika,
KeyvanAmiri,YelinKim,XiXiong,KaiKang,FlorianLuisier,NileshTripuraneni,David
Madras,MandyGuo,AustinWaters,OliverWang,JoshuaAinslie,JasonBaldridge,Han
Zhang,GarimaPruthi,JakobBauer,FengYang,RihamMansour,JasonGelman,YangXu,
GeorgePolovets,JiLiu,HonglongCai,WarrenChen,XiangHaiSheng,EmilyXue,Sherjil
Ozair,ChristofAngermueller,XiaoweiLi,AnoopSinha,WeirenWang,JuliaWiesinger,
EmmanouilKoukoumidis,YuanTian,AnandIyer,MadhuGurumurthy,MarkGoldenson,
Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki,
ChrisanthaFernando,KenDurden,HarshMehta,NikolaMomchev,ElaheRahimtoroghi,
Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Denny
Zhou, Komal Jalan, Dinghua Li, Blake Hechtman, Parker Schuh, Milad Nasr, Kieran
Milan,VladimirMikulik,JulianaFranco,TimGreen,NamNguyen,JoeKelley,Aroma
Mahendru,AndreaHu,JoshuaHowland,BenVargas,JeffreyHui,KshitijBansal,Vikram
Rao, RakeshGhiya, EmmaWang, KeYe, JeanMichelSarr, MelanieMoranskiPreston,
MadeleineElish,SteveLi,AakashKaku,JigarGupta,IcePasupat,Da-ChengJuan,Milan
Someswar,TejviM.,XinyunChen,AidaAmini,AlexFabrikant,EricChu,XuanyiDong,
Amruta Muthal, Senaka Buthpitiya, Sarthak Jauhari, Nan Hua, Urvashi Khandelwal,
Ayal Hitron, Jie Ren, Larissa Rinaldi, Shahar Drath, Avigail Dabush, Nan-Jiang Jiang,
HarshalGodhia,UliSachs,AnthonyChen,YichengFan,HagaiTaitelbaum,HilaNoga,
ZhuyunDai,JamesWang,ChenLiang,JennyHamer,Chun-SungFerng,ChenelElkind,
AvielAtias,PaulinaLee,VítListík,MathiasCarlen,JanvandeKerkhof,MarcinPikus,
KrunoslavZaher,PaulMüller,SashaZykova,RichardStefanec,VitalyGatsko,Christoph
Hirnschall,AshwinSethi,XingyuFedericoXu,ChetanAhuja,BethTsai,AncaStefanoiu,
BoFeng,KeshavDhandhania,ManishKatyal,AkshayGupta,AtharvaParulekar,Divya
Pitta,JingZhao,VivaanBhatia,YashodhaBhavnani,OmarAlhadlaq,XiaolinLi,Peter
Danenberg,DennisTu,AlexPine,VeraFilippova,AbhipsoGhosh,BenLimonchik,Bhar-
gavaUrala,ChaitanyaKrishnaLanka,DerikClive,YiSun,EdwardLi,HaoWu,Kevin
Hongtongsak,IannaLi,KalindThakkar,KuanyshOmarov,KushalMajmundar,Michael
Alverson,MichaelKucharski,MohakPatel,MuditJain,MaksimZabelin,PaoloPelagatti,
Rohan Kohli, Saurabh Kumar, Joseph Kim, Swetha Sankar, Vineet Shah, Lakshmi Ra-
machandruni,XiangkaiZeng,BenBariach,LauraWeidinger,AmarSubramanya,Sissie
Hsiao,DemisHassabis,KorayKavukcuoglu,AdamSadovsky,QuocLe,TrevorStrohman,
YonghuiWu,SlavPetrov,JeffreyDean,andOriolVinyals. Gemini: AFamilyofHighly
CapableMultimodalModels. arXiv:2312.11805,2024.
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,
ShreyaPathak,LaurentSifre,MorganeRivière,MihirSanjayKale,JulietteLove,Pouya
Tafti,LéonardHussenot,PierGiuseppeSessa,AakankshaChowdhery,AdamRoberts,
Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea
Tacchetti,AnnaBulanova,AntoniaPaterson,BethTsai,BobakShahriari,CharlineLeLan,
ChristopherA.Choquette-Choo,ClémentCrepy,DanielCer,DaphneIppolito,DavidReid,
ElenaBuchatskaya,EricNi, EricNoland, GengYan, GeorgeTucker, George-Christian
Muraru,GrigoryRozhdestvenskiy,HenrykMichalewski,IanTenney,IvanGrishchenko,
JacobAustin,JamesKeeling,JaneLabanowski,Jean-BaptisteLespiau,JeffStanway,Jenny
Brennan,JeremyChen,JohanFerret,JustinChiu,JustinMao-Jones,KatherineLee,Kathy
Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej
Mikuła,MateoWirth,MichaelSharman,NikolaiChinaev,NithumThain,OlivierBachem,
OscarChang,OscarWahltinez,PaigeBailey,PaulMichel,PetkoYotov,RahmaChaabouni,
RamonaComanescu,ReenaJana,RohanAnil,RossMcIlroy,RuiboLiu,RyanMullins,
Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya,
Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech
Stokowiec,YuhuiChen,ZafaraliAhmed,ZhitaoGong,TrisWarkentin,LudovicPeran,
Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis
Hassabis,ZoubinGhahramani,DouglasEck,JoelleBarral,FernandoPereira,EliCollins,
ArmandJoulin,NoahFiedel,EvanSenter,AlekAndreev,andKathleenKenealy. Gemma:
OpenModelsBasedonGeminiResearchandTechnology. arXiv:2403.08295,2024.
DirkGroeneveld,IzBeltagy,PeteWalsh,AkshitaBhagia,RodneyKinney,OyvindTafjord,
AnanyaHarshJha,HamishIvison,IanMagnusson,YizhongWang,ShaneArora,David
12Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas,
YanaiElazar,YulingGu,JackHessel,TusharKhot,WilliamMerrill,JacobMorrison,Niklas
Muennighoff,AakankshaNaik,CrystalNam,MatthewE.Peters,ValentinaPyatkin,Ab-
hilashaRavichander,DustinSchwenk,SaurabhShah,WillSmith,EmmaStrubell,Nishant
Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson,
LukeZettlemoyer,JesseDodge,KyleLo,LucaSoldaini,NoahA.Smith,andHannaneh
Hajishirzi. OLMo: AcceleratingtheScienceofLanguageModels. arXiv:2402.00838,2024.
DayaGuo,QihaoZhu,DejianYang,ZhendaXie,KaiDong,WentaoZhang,GuantingChen,
XiaoBi,Y.Wu,Y.K.Li,FuliLuo,YingfeiXiong,andWenfengLiang. DeepSeek-Coder:
WhentheLargeLanguageModelMeetsProgramming–TheRiseofCodeIntelligence.
arXiv:2401.14196,2024.
NajoungKimandSebastianSchuster. EntityTrackinginLanguageModels. InACL,2023.
TiedongLiuandBryanKianHsiangLow. Goat: Fine-tunedLLaMAOutperformsGPT-4on
ArithmeticTasks. arXiv:2305.14201,2023.
YingweiMa,YueLiu,YueYu,YuanliangZhang,YuJiang,ChangjianWang,andShanshan
Li. AtWhichTrainingStageDoesCodeDataHelpLLMsReasoning? InICLR,2024.
AmanMadaan, ShuyanZhou, UriAlon, YimingYang, andGrahamNeubig. Language
ModelsofCodeareFew-ShotCommonsenseLearners. InEMNLP,2022.
NiklasMuennighoff,AlexanderRush,BoazBarak,TevenLeScao,NouamaneTazi,Aleksan-
draPiktus,SampoPyysalo,ThomasWolf,andColinARaffel. ScalingData-Constrained
LanguageModels. InNeurIPS,2023.
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,
ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,JohnSchulman,JacobHilton,
Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F
Christiano,JanLeike,andRyanLowe. Traininglanguagemodelstofollowinstructions
withhumanfeedback. InNeurIPS,2022.
NikhilPrakash,TamarRottShaham,TalHaklay,YonatanBelinkov,andDavidBau. Fine-
Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking. In ICLR,
2024.
RafaelRafailov,ArchitSharma,EricMitchell,ChristopherDManning,StefanoErmon,and
ChelseaFinn. DirectPreferenceOptimization:YourLanguageModelisSecretlyaReward
Model. InNeurIPS,2023.
YasamanRazeghi,HamishIvison,SameerSingh,andYanaiElazar. BacktrackingMathe-
maticalReasoningofLanguageModelstothePretrainingData. InTheSecondTinyPapers
TrackatICLR2024,2024.
BaptisteRozière,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,
YossiAdi,JingyuLiu,RomainSauvestre,TalRemez,JérémyRapin,ArtyomKozhevnikov,
IvanEvtimov, JoannaBitton, ManishBhatt, CristianCantonFerrer, AaronGrattafiori,
Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis
Martin,NicolasUsunier,ThomasScialom,andGabrielSynnaeve. CodeLlama: Open
FoundationModelsforCode,2024.
MaartenSap,RonanLeBras,DanielFried,andYejinChoi. NeuralTheory-of-Mind? Onthe
LimitsofSocialIntelligenceinLargeLMs. InEMNLP,2022.
ZhihongShao,PeiyiWang,QihaoZhu,RunxinXu,JunxiaoSong,MingchuanZhang,Y.K.
Li,Y.Wu,andDayaGuo. DeepSeekMath: PushingtheLimitsofMathematicalReasoning
inOpenLanguageModels. arXiv:2402.03300,2024.
Shubham Toshniwal, Ivan Moshkov, Sean Narenthiran, Daria Gitman, Fei Jia, and
Igor Gitman. OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset.
arXiv:2402.10176,2024.
13Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
TimothéeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,Aurelien
Rodriguez,ArmandJoulin,EdouardGrave,andGuillaumeLample. LLaMA:Openand
efficientfoundationlanguagemodels. arXiv:2302.13971,2023a.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,Lukas
Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude
Fernandes,JeremyFu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,Naman
Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,
ViktorKerkez,MadianKhabsa,IsabelKloumann,ArtemKorenev,PunitSinghKoura,
Marie-AnneLachaux,ThibautLavril,JenyaLee,DianaLiskovich,YinghaiLu,Yuning
Mao,XavierMartinet,TodorMihaylov,PushkarMishra,IgorMolybog,YixinNie,Andrew
Poulton, JeremyReizenstein, RashiRungta, KalyanSaladi, AlanSchelten, RuanSilva,
EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,BinhTang,RossTaylor,
AdinaWilliams, Jian XiangKuan, Puxin Xu, Zheng Yan, IliyanZarov, Yuchen Zhang,
Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
SergeyEdunov,andThomasScialom. Llama2: OpenFoundationandFine-TunedChat
Models. arXiv:2307.09288,2023b.
JasonWei,MaartenBosma,VincentZhao,KelvinGuu,AdamsWeiYu,BrianLester,Nan
Du,AndrewM.Dai,andQuocVLe. FinetunedLanguageModelsareZero-ShotLearners.
InICLR,2022a.
JasonWei,XuezhiWang, DaleSchuurmans, MaartenBosma, brianichter, FeiXia, EdH.
Chi,QuocVLe,andDennyZhou. Chainofthoughtpromptingelicitsreasoninginlarge
languagemodels.InAliceH.Oh,AlekhAgarwal,DanielleBelgrave,andKyunghyunCho
(eds.),AdvancesinNeuralInformationProcessingSystems,2022b. URLhttps://openreview.
net/forum?id=_VjQlMeSB_J.
JasonWeston,AntoineBordes,SumitChopra,andTomásMikolov. TowardsAI-Complete
QuestionAnswering: ASetofPrerequisiteToyTasks. InICLR,2016.
BrandonT.WillardandRémiLouf.EfficientGuidedGenerationforLargeLanguageModels.
ArXiv,abs/2307.09702,2023.
ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,Anthony
Moi,PierricCistac,TimRault,RemiLouf,MorganFuntowicz,JoeDavison,SamShleifer,
Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao,
SylvainGugger,MariamaDrame,QuentinLhoest,andAlexanderRush. Transformers:
State-of-the-ArtNaturalLanguageProcessing. InQunLiuandDavidSchlangen(eds.),
EMNLP:SystemDemonstrations,2020.
KeYang,JiatengLiu,JohnWu,ChaoqiYang,YiR.Fung,ShaLi,ZixuanHuang,XuCao,
XingyaoWang,YiquanWang,HengJi,andChengxiangZhai. IfLLMIstheWizard,Then
CodeIstheWand: ASurveyonHowCodeEmpowersLargeLanguageModelstoServe
asIntelligentAgents. arXiv:2401.00812,2024.
14Model Size HuggingFaceIdentifier Chatformat
7B meta-llama/Llama-2-7b-hf –
Llama2 13B meta-llama/Llama-2-13b-hf –
70B meta-llama/Llama-2-70b-hf –
7B meta-llama/Llama-2-7b-chat-hf ✓
Llama2-Chat 13B meta-llama/Llama-2-13b-chat-hf ✓
70B meta-llama/Llama-2-70b-chat-hf ✓
7B codellama/CodeLlama-7b-hf –
13B codellama/CodeLlama-13b-hf –
CodeLlama
34B codellama/CodeLlama-34b-hf –
70B codellama/CodeLlama-70b-hf –
7B codellama/CodeLlama-7b-Instruct-hf ✗
13B codellama/CodeLlama-13b-Instruct-hf ✗
CodeLlama-Instruct 34B codellama/CodeLlama-34b-Instruct-hf ✗
70B codellama/CodeLlama-70b-Instruct-hf ✗
DeepSeek 7B deepseek-ai/deepseek-llm-7b-base –
DeepSeek-Chat 7B deepseek-ai/deepseek-llm-7b-chat ✓
DeepSeek-Coder 7B deepseek-ai/deepseek-coder-7b-base-v1.5 –
DeepSeek-Coder-Instruct 7B deepseek-ai/deepseek-coder-7b-instruct-v1.5 ✗
DeepSeek-Math 7B deepseek-ai/deepseek-math-7b-base –
Gemma 8B google/gemma-7b –
Gemma-Instruct 8B google/gemma-1.1-7b-it ✗
CodeGemma 8B google/codegemma-7b –
CodeGemma-Instruct 8B google/codegemma-1.1-7b-it ✗
7B EleutherAI/llemma_7b –
Llemma
34B EleutherAI/llemma_34b –
Llama 7B huggyllama/llama-7b –
FLoat 7B nikhil07prakash/float-7b –
Mistral 7B mistralai/Mistral-7B-v0.1 –
OpenMathMistral 7B nvidia/OpenMath-Mistral-7B-v0.1 –
Table4: Detailsofallthemodelsevaluatedinthepaper. Therightmostcolumnindicates
whetherthechatformatwasusedforpromptingthemodel.
A Appendix
A.1 ModelDetails
Weusedthetransformerslibrary(Wolfetal.,2020)byHuggingFaceforallourexperiments.
Table4presentsallthemodelsalongwiththeirHuggingFaceidentifiers.
Foralignment-tunedmodels,weexperimentedwithpromptingthemodelwithandwithout
chatformatting(seeTable5andTable2forthedifferentformats). Basedonresultsovera
held-outdevelopmentset,weselectedthebest-performingpromptformat. Wefindthat
except for Llama 2-Chat models and DeepSeek-Chat, all other alignment-tuned models
performedbetterwiththenon-chatformat.
A.2 ConstrainedDecoding
Statement: Box 0 contains( [a-zA-Z]+)*, Box 1 contains( [a-zA-Z]+)*, Box
2 contains( [a-zA-Z]+)*, Box 3 contains( [a-zA-Z]+)*, Box 4 contains(
[a-zA-Z]+)*, Box 5 contains( [a-zA-Z]+)*, Box 6 contains( [a-zA-Z]+)*.
Figure4: Regularexpressionusedforconstraineddecodingofentitystates.
152-shotprompt
<s>[INST]«SYS»Giventhedescriptionafter"Description:",writeatruestatement
aboutallboxesandtheircontentstothedescriptionafter"Statement:". «/SYS»
Description: Box0containsthecar,Box1containsthecross,Box2containsthe
bagandthemachine,Box3containsthepaperandthestring,Box4containsthe
bill,Box5containstheappleandthecashandtheglass,Box6containsthebottle
andthemap. [/INST]Statement: Box0containsthecar,Box1containsthecross,
Box2containsthebagandthemachine,Box3containsthepaperandthestring,
Box4containsthebill,Box5containstheappleandthecashandtheglass,Box6
containsthebottleandthemap. </s>
<s>[INST] Description: Box 0 contains the car, Box 1 contains the cross, Box 2
containsthebagandthemachine,Box3containsthepaperandthestring,Box
4 contains the bill, Box 5 contains the apple and the cash and the glass, Box 6
containsthebottleandthemap. RemovethecarfromBox0. Removethepaper
andthestringfromBox3. PuttheplaneintoBox0. MovethemapfromBox6to
Box2. RemovethebillfromBox4. PutthecoatintoBox3. [/INST]Statement:
Box 0 contains the plane, Box 1 contains the cross, Box 2 contains the bag and
themachineandthemap,Box3containsthecoat,Box4containsnothing,Box5
containstheappleandthecashandtheglass,Box6containsthebottle. </s>
<s>[INST]Description: description
Table5: Chat-formattedpromptwith2-shotin-contextdemonstrations.
Inourexperiments, wefoundthatthemodelsstruggledtoadheretotheoutputformat
specifiedviathefew-shotpromptexamples. Luckily,theexpectedoutputcanbedescribed
preciselybytheregularexpressionshowninFigure4.Weusedtheoutlineslibrary(Willard
&Louf,2023)whichsupportsregex-basedconstraineddecoding. Wefoundasignificant
improvement with constrained decoding. For e.g., the performance of the Llama 2 70B
modelwentupfrom54.95to62.13withconstraineddecoding.
A.3 DetailedResults
Table6presentsthedetailedresultsofallthemodelsevaluatedinthiswork. Theresultsare
categorizedbythenumberofoperationsaffectingtheentityofinterest.
16Performancesplitbynumberofoperations
Model
Overall 0 1 2 3 4 5 6 7
(5012) (1303) (1410) (1083) (651) (288) (142) (106) (29)
Random 21.08 41.06 17.85 12.70 11.87 10.58 9.16 7.51 12.59
Llama2-7B 21.31 65.54 4.96 7.94 4.30 5.56 4.93 5.66 3.45
Llama2-7BChat 41.28 87.95 35.53 24.38 14.29 12.15 14.79 7.55 3.45
Llama2-13B 33.28 77.05 25.18 15.97 12.75 8.68 8.45 10.38 17.24
Llama2-13BChat 38.05 77.51 36.67 19.94 16.28 9.03 11.27 11.32 13.79
Llama-270B 62.13 99.00 71.91 43.67 31.18 21.88 25.35 25.47 27.59
Llama-270BChat 63.43 93.25 75.67 47.00 37.33 26.39 22.54 24.53 37.93
CodeLlama7B 31.94 90.87 13.69 12.28 7.99 6.60 4.93 8.49 13.79
CodeLlama7BInstruct 41.34 96.16 36.03 18.19 12.29 6.60 5.63 3.77 10.34
CodeLlama13B 54.79 95.70 60.78 32.87 25.35 20.14 21.83 26.42 13.79
CodeLlama13BInstruct 58.14 97.77 71.13 35.83 23.35 17.01 14.79 19.81 20.69
CodeLlama34B 58.26 94.70 71.28 33.89 31.18 21.53 14.79 19.81 24.14
CodeLlama34BInstruct 61.47 95.09 77.09 39.98 31.03 22.57 19.01 20.75 20.69
CodeLlama70B 69.77 99.39 76.60 54.20 47.31 40.97 36.62 43.40 37.93
CodeLlama70BInstruct 73.66 98.70 83.40 59.65 51.92 46.18 40.85 39.62 44.83
Llemma7B 34.12 84.65 26.81 13.20 8.29 4.51 6.34 9.43 0.00
Llemma34B 60.18 95.24 75.60 36.10 31.64 21.53 14.79 21.70 20.69
DeepSeek7B 23.46 71.83 8.44 7.29 4.61 1.04 2.11 3.77 6.90
DeepSeek7BChat 32.24 70.30 27.30 17.27 11.21 11.81 9.15 6.60 3.45
DeepSeek7BCoderBase 53.67 97.16 57.52 33.61 23.96 19.10 17.61 8.49 13.79
DeepSeek7BCoderInstruct 48.76 91.71 54.54 26.41 18.13 13.89 11.27 17.92 3.45
DeepSeek7BMathBase 56.40 98.93 60.28 35.73 26.73 24.65 16.20 23.58 27.59
Gemma-7B 42.66 95.78 45.82 14.04 8.60 5.56 7.75 7.55 3.45
Gemma-7BInstruct 47.79 93.25 55.96 21.98 15.51 7.99 8.45 11.32 17.24
CodeGemma-7B 46.09 91.33 53.76 21.70 13.67 6.94 5.63 8.49 3.45
CodeGemma-7BInstruct 53.49 93.48 69.57 26.22 19.82 12.50 12.68 11.32 10.34
Mistral7B 45.67 96.01 50.28 18.56 11.37 8.68 12.68 7.55 10.34
OpenMathMistral 38.25 90.94 34.40 14.22 8.76 5.90 4.93 5.66 20.69
LLama7B 28.67 97.93 0.71 10.53 2.92 3.82 3.52 0.94 3.45
Float7BInstruct 27.55 89.33 1.56 12.37 5.07 5.21 2.82 4.72 13.79
Table6: EntityTrackingperformanceofmodelscategorizedbythenumberofoperations
affectingtheentityofinterest. Thecountoftestsetinstanceswiththenumberofoperations
affectinganentityisindicatedinparenthesesbelowthecorrespondingcolumntitle.
17