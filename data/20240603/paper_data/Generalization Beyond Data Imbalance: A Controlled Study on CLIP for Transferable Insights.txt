Generalization Beyond Data Imbalance:
A Controlled Study on CLIP for Transferable Insights
XinWen1 BingchenZhao2 YilunChen3 JiangmiaoPang3 XiaojuanQi1
1TheUniversityofHongKong 2UniversityofEdinburgh 3ShanghaiAILaboratory
{wenxin, xjqi}@eee.hku.hk pangjiangmiao@pjlab.org.cn
Abstract
Severedataimbalancenaturallyexistsamongweb-scalevision-languagedatasets.
Despitethis,wefindCLIPpre-trainedthereuponexhibitsnotablerobustnessto
thedataimbalancecomparedtosupervisedlearning,anddemonstratessignificant
effectivenessinlearninggeneralizablerepresentations. Withanaimtoinvestigate
thereasonsbehindthisfinding,weconductcontrolledexperimentstostudyvarious
underlyingfactors,andrevealthatCLIP’spretexttaskformsadynamicclassifica-
tionproblemwhereinonlyasubsetofclassesispresentintraining. Thisisolates
thebiasfromdominantclassesandimplicitlybalancesthelearningsignal. Further-
more,therobustnessanddiscriminabilityofCLIPimprovewithmoredescriptive
languagesupervision,largerdatascale,andbroaderopen-worldconcepts,which
areinaccessibletosupervisedlearning. Ourstudynotonlyuncoversthemech-
anismsbehindCLIP’sgeneralizabilitybeyonddataimbalancebutalsoprovides
transferableinsightsfortheresearchcommunity. Thefindingsarevalidatedinboth
supervisedandself-supervisedlearning,enablingmodelstrainedonimbalanced
datatoachieveCLIP-levelperformanceondiverserecognitiontasks. Codewillbe
availableat: https://github.com/CVMI-Lab/clip-beyond-tail.
1 Introduction
Thedevelopmentofcontrastivelanguage-imagepre-training(CLIP)[35,42,54,65,90]hasdemon-
stratedunprecedentedsuccessinlearninggeneralizablerepresentations,empoweringzero-shotvision
tasksandrobustnesstonaturaldistributionalshifts. Thissuccesscanbeprimarilyattributedtothe
effectiveuseoflarge-scaleuncuratedimagecaptioningdatasetscollectedfromtheweb. Arecent
trendinvolvesdelvingintothedistributionofthesedatasetsandexplicitlyintroducinginterventions
tothecurationprocesstocreatebetterdatafortraining[28,88]. However,limitedresearchhasbeen
conductedonanalyzingthedistributionofconcepts/classesinthesedatasetsandthebehaviorof
CLIPundervaryingdistributions. Thisworkthusstartsbypresentingaconcept-centricanalysisof
existingweb-scaleimage-textdatasetsandmodelspre-trainedaccordingly(Fig.1).
Motivation. OurmotivationforthisstudyarisesfromanintriguingobservationofCLIP’szero-shot
performanceonImageNet:CLIPisnotablymorerobusttopre-traineddataimbalancethansupervised
learning. Weexaminevariousvision-languagedatasetsatdifferentscales,andanalyzetheirdistribu-
tionwithrespecttoImageNetclasses.Wefindthatimage-textdatasetsshareanextremelyimbalanced
classdistribution(Fig.1a). Interestingly,wefindthatthezero-shotclassificationperformanceof
trainedCLIPmodelsismorerobusttothisimbalance,especiallycomparedtomodelsobtainedby
supervisedlearning. Thisisevidencedbyaweakercorrelationbetweenaclass’sperformanceandits
frequency(Fig.1b). ThistrendisconsistentacrossCLIPmodelsandpre-trainingdatasetsandeven
holdstrueforsmaller-scaledatasetslikeCC-12M[11]. Thisphenomenoninspiresustostudythe
underlyingcausesforCLIP’srelativerobustnesstowarddataimbalanceandwhatwecanlearnfrom.
Preprint.Underreview.
4202
yaM
13
]VC.sc[
1v07012.5042:viXra107 MetaCLIP-2.5B MetaCLIP-2.5B Model
105 LAION-2B CLIP SL
LAION-2B
103 Correlation to ...
Per-class accuracy
106 MetaCLIP-400M MetaCLIP-400M Per-class #prediction
LAION-400M
104 LAION-400M
102
YFCC-15M
YFCC-15M
104 CC-12M
CC-12M
102
100 LAIONet-3M
0 200 400 600 800 1000 0.0 0.2 0.4 0.6 0.8
ImageNet classes ranked by the class frequency of LAION-400M Spearman corr. to per-class frequency
(a)Classfrequencies(logscale)rankedbyLAION-400M. (b)Correlationbetweenclass-wisestatistics.
Figure1:Per-classstatisticsofimage-textdatasetsandmodelstrainedontop.(a)Ahighlyimbalanced
class distribution is shared across datasets.1(b) Compared to supervised learning (✖ SL), CLIP’s
performance(measuredby•accuracy)islessbiasedbydatafrequency,andtheclassifierisnotably
uncorrelated (measured by model’s number of • prediction per class). Besides, the correlation
narrowsasdatascalesup. Bothaspectsindicateimplicitre-balancingmechanismsexistinCLIP.
Our study and findings. To answer the question above, we conduct controlled experiments to
analyzefactorsincludingsupervisionsignalandpretexttask(Fig.3),datadistribution(Fig.4),scale
(Fig.5),andopen-worldconcepts(Fig.6). Ourextensivestudieshaveledustothefollowingfindings:
• Language supervision, particularly the texts with increased descriptiveness (informativeness),
enhancesboththerobustnessanddiscriminabilityofCLIP,andpreservesmorefeaturevariation.
• CLIP’spretexttaskformsdynamicclassificationproblems, whereinonlyasubsetofclassesis
presentduringtraining,effectivelyisolatesbiasestodominantclasses,andbalanceslearningsignal.
• Severedataimbalanceinwebdatasetsincreasestheriskofbiasinmodels. However,distribution
shiftandhigherdatadiversityinthemcanenhancerobustness,albeitatrade-offindataefficiency.
• CLIP’srobustnessanddiscriminabilityimprovetogetherwithdatascaling,benefittingfromits
abilitytoutilizeopen-worlddata,aprivilegenotaccessibletosupervisedlearning.
Applications. Inspiredbythefindingsofourstudy,wefoundthatthisrobustnesstodataimbalance
can be transferred to supervised and self-supervised learning models with simple techniques by
makingtheclassificationtaskdynamicduringtraining. Underextremelyimbalanceddatascenarios,
we show that a vanilla classification model can also generalize well to tail (or even open-world)
classesaswellasCLIPvia1)fixingtheclassifierwithclassprototypesfrompre-trainedCLIPtext
encoder,and2)trainingwithrandomlysubsampledvocabulary(resultsinFig.8,andanalysisin
Fig.9). Beyondclassification,wealsoshowimprovedtransferabilityonDINO[10]pre-trainedon
uncuratedwebdatabysimplyrandomlysubsamplingtheprototypesintraining(Fig.10).
Summary. OurstudyisoneofthepioneeringeffortstoexploreCLIP’srobustnessinthecontextof
imbalanceddatadistributions. Ourexplorationprovidesacomprehensiveanalysisthatuncoversthe
mechanismscontributingtoCLIP’srobustnessagainstdataimbalance. Aswewilldemonstrateinthis
paper,theinsightsgainedfromourresearcharetransferabletootherdomains,includingsupervised
andself-supervisedlearningframeworks.
2 Relatedwork
CLIP’sdistributionalrobustness. ThedebutofCLIPnotonlysetthestate-of-the-artperformance
onconventionalimageclassificationbenchmarksbutalsodemonstratedunprecedentedrobustness
tochallengingdistributionshifts. Studieshaveshownthatthisrobustnessstemsfromthediverse
1MetaCLIP[88]isrelativelymorebalancedthanotherdatasetsduetoconceptre-balancingincuration.
2Image from ImageNet ImageNet-Captions Class frequency
(linear scale)
Class:payphone URL:flickr.com/…
Title: A Phone Call at Night
Retrieve metadata
Desc.: I might have a thing with telephones…
with Flickr API 449K images
Tags: phone, telephone, black and white, … 999 classes
Image from LAION-400M LAIONet
Class frequency
Caption: Craneloading container shipby harbor… (linear scale)
∈ImageNet ∈ImageNet ∉ImageNet
Def.:“crane, which is…” “crane (bird) is…” “container shipis.…”
3.26M images
Filter by caption-to-definition
Class:crane,container ship 943 classes
similarity of CLIP text encoder
Figure2:Curationprocessanddistributionofdatasetsusedinourcontrolledstudy.Top:IN-Caps[26]
augmentstrainimagesofImageNetwithtextsbyqueryingFlickrwithimageURLs.Thetextsinclude
title,description,andtags. Bottom:LAIONet[74]isafilteredsubsetofLAION-400M[70],obtained
bymatchingImageNetclasseswithcaptionsandfilteringbyCLIPtextencoderfordisambiguation.
trainingdistributionsCLIPhasseenduringtrainingtime[26,66]. Also,itisshownthatthedata
qualityplaysanimportantroleinenhancingthedistributionalrobustnessofCLIP[55]. Itmayseem
thatCLIPobtainstheimprovementdistributionalrobustnessduetothesimilarityofpretrainingdata
tothedistributionshifteddata,but[52]showsthatitisnotthecasewhereevenafterpruningsimilar
data,CLIPstillobtainsstrongrobustness,indicatinggeneralizablerepresentationsarelearned.
Learningfromuncurateddata. Apartfromrobustnesstodistributionshifts,previousworkshave
alsodelvedintothenatureofuncuratedlarge-scaledatasets[34,46,74,88]. Studieshaveshown
thatself-supervisedlearningcanproducemorerobustmodelsthansupervisedlearningonuncurated
data[34,46]. Moreover,focusingonlearningofsubsetsoftheentiredataset[8,79]hasshownto
furtherenhanceself-supervisedlearningfromuncurateddata. Onthelearningonuncurateddata,
thelanguageinformationhasshowntohelplearngoodrepresentations[68]. Balancingtheconcept
distributionofuncurateddatahasshowntobeascalablewayoflearninggoodmodels[88]. However,
theuncurateddataisnotallharmfulforperformance,thelowerintra-classsimilarityofthedatais
showntohelppreserveinformation/variationinrepresentations[74],butatlowdataefficiency[82].
Generalizationofvisionmodels. Oneofthemainthemesofcomputervisionresearchintheeraof
deeplearningisthesearchformoregeneralizablemodels. Workshavefocusedonself-supervised
pretrainingwithonlyimages,amongwhichcontrastivelearning[12]andself-distillation[10,58]are
showntobeeffective.Withtheintroductionoflarge-scaleimage-textdatasets[70,71],thereisahuge
interestinlearningmoregeneralizablevisionrepresentationsfromadditionallanguagesupervision.
Whiletechniquesforincorporatinglanguagesupervisionhavebeenproposed[18,35,65,69,91],
further exploration of how semantic grounding help improves the generalization is needed [20].
Tofullyutilizelanguagesupervision,usingsyntheticdatafromlargelanguagemodelstoimprove
languagesupervisionisanewlyemergedresearcharea[24,25].
3 WhatmakesCLIPmorerobusttopre-trainingdataimbalance?
Inthefollowing,weconductaseriesofcontrolledexperimentstosystematicallyevaluatetheroleof
variousfactorsontherobustnessofCLIPtodataimbalance. Thesefactorsincludesupervisionsignal
(Sec.3.2),pretexttask(Sec.3.3),datadistribution(Sec.3.4),datascale(Sec.3.5),andopen-world
concepts(Sec.3.6). Moreover,wealsoprovidesomeinsightsonCLIP’sfeaturespaceinSec.3.7.
3.1 Setting
Datasets. Experimentsinthisstudyareconductedonvariantsoftwoimage-textdatasets: ImageNet-
Captions[26]andLAIONet[74]toallowbetterdata-centriccontrol. AnoverviewisshowninFig.2.
Bothdatasetsprovideimageswiththeirpairedcaptions,andclasslabelsonImageNet. Thecaptions
3ofImageNet-Captionsareintheformatoftitle,description,andtags(somecanbemissingfora
specificimage),whichallowscontrolofcaptions’descriptiveness. ImagesofLAIONetaredrawn
fromLAION,whichhasahigherintra-classvariabilityandisextremelyimbalancedacrossclasses.
Thismakesitmorechallengingtotrainonandallowsisolatingtheeffectofdatadistribution.
Models. We consider both CLIP and supervised learning (SL) with ResNet-50 as the backbone.
Given that CNNs are generally considered less robust than ViTs [3], this choice also enables us
toinfertherobustnessofothermodels. ForSL,wealignmostdetailswithCLIP[65]toruleout
the effect of irrelevant factors. E.g., we use the same weak data augmentation as CLIP, adopt a
prototypicalclassificationhead(i.e.,ℓ -normalizingbothfeaturesandclassifierweights),andapply
2
a learnable temperature to logits. The training schedules of CLIP and SL follow [14] and [26],
respectively. Modelsarefullytrainedfromscratchbydefault. MoredetailsareprovidedinAppx.C.
Metrics. WecomputeSpearmancorrelationcoefficients[75]betweenclassfrequencyandmodels’
statistics(class-wisetop-1accuracyandnumberofsamplespredictedaseachclass). Besides,wealso
considermetricsfromneuralcollapseliterature[31,60]foranalyzingfeaturedistribution. Formally,
defining the global feature mean µ = Avg h , class-level means µ = Avg h , within-
G i,c i,c c i i,c
class covariance Σ = Avg (h −µ )(h −µ )⊤, and between-class covariance Σ =
W i,c i,c c i,c c B
Avg (µ −µ )(µ −µ )⊤,themetricsaredefinedas:
c c G c G
NC1=Tr(cid:16) Σ WΣ† B/C(cid:17) , NC2=Avg c,c′(cid:12) (cid:12) (cid:12) (cid:12)∥µµ⊤ c ∥∥µ µc′
∥
+ C1 −1(cid:12) (cid:12) (cid:12) (cid:12), (1)
c c′
where†denotestheMoore-Penrosepseudoinverse,h isthefeatureofthei-thexampleinclass
i,c
c, andC isthetotalnumberofclasses. Intuitively, NC1andNC2measurethecompactnessand
separationofclusters,respectively. NC1approacheszerowhenthewithin-classvariationoffeatures
becomesnegligible,andNC2convergestozerowhenclassifiersreachmaximalandequalmargins
(i.e.,ETFstructure)[60]. Notethatthesetwometricsareoriginallydefinedasanaverageacross
classes,anditissimpletoobtainper-classNC1andNC2metrics,measuringthevariabilityofa
specificclassoritsaveragemargintoallotherclasses.
3.2 (Descriptive)languageassupervisionsignal
Setting. Westartbyexaminingtheimpactoflanguagesupervision,theprimarydistinctionbetween
CLIPandothercontrastivelearningapproaches.Thisisdonebycreatingtextswithroughlymonotonic
increasingdescriptivenessgivenmetadataofImageNet-Captions. Forthelow-diversitytexts,we
create•syntheticclass-centrictextsusingclassificationtemplatesfromCLIP[65]givenclassnames
orsynset[53]. The•naturallanguage-basedtextsarecreatedbyconcatenatingdifferenttypesof
a b
Prediction Bias Perspective Overall Accuracy Perspective Model
0.6
0.5
better
small
oe cr
abulary
56 50
better
C
C
SC
L
ylL
aI
nPI sP
s
sT
e
e
T
tx
e
Tt
m
e
T mpyp
l
pae
lt
aeS
ts
eL
s
v 50 Description Only
0.4
Title Only
45
Tags Only
0.3 40 Title + Description
Tags + Description
0.2 h dig eh sce rr i pte tix vt eness 33 05
SL
T F Vui ot ll cle aT b+ ex
u
T t laa rg ys
Size
0.1
25 50 500
100 1000
0.0 20 200
0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.25 0.30 0.35 0.40 0.45 0.50 0.55
Correlation (freq. vs. acc.) Correlation (freq. vs. acc.)
Figure3: ResultsonIN-Capsabout•textdescriptivenessand✖vocabularysize. 1)Increasing•text
descriptivenessimprovesboth(a)robustnessand(b)discriminabilityofCLIP,butthetendencyvaries
ifusing•lessdescriptive(template-based)supervision. 2)ThegapbetweenSLandCLIP(a)implies
CLIPre-balancespredictions,whichisreplicableby✖subsamplingthevocabularySLtrainswith.
4
).derp#
.sv
.qerf(
noitalerroC
)%
,1-pot(
.cca
llarevOcaptions(seeFig.2),andthedescriptivenessoflanguagesupervisioniscontrolledbythenumberof
texttypesused. MoredetailsareavailableinAppx.C.2.
Results. Fig.3provideacomprehensivecomparisonbetweenmodelvariantsfromdifferentperspec-
tives. RestrictingourviewtoCLIPmodelsinthefirsttwosubfigures,•highertextdescriptiveness
resultsinimprovementsinbothrobustnessanddiscriminabilityofCLIP,asshownbylowercorre-
lation(Fig.3a)andhigheroverallaccuracy(Fig.3b,y-axis). Ontheotherhand,•relativelyless
descriptivetextsshowweakerresultsthatareclosetoresultsof•templated-basedCLIP(Fig.3a,
x-axis). Weseethisaslessdescriptivetextscouldcollapsetoclass-centricsupervisionwithoutmuch
additionalvariance. Despitethis,predictionsof•template-basedCLIParestillnotablylessbiased
bypre-trainingdatathan✖SL(Fig.3b),indicatingotherfactorsmayre-balanceCLIP’spredictions.
3.3 Dynamicclassification(usingsubsampledvocabulary)aspretexttask
Setting. Wenotethatthepretextof•template-basedCLIPstilldiffersfrom✖SL.Althoughboth
formedasdiscriminationtasks,thevocabulary(classesinamini-batch)ofCLIPismuchsmaller
thanSL(allclasses). Takeusingabatchsizeof1024forinstance,afterdeduplication,thevocabulary
onlycontainsaround600classes(forImageNet-Captions). Ifnegativesamplesarenotsharedacross
devices,thevocabularyreceivedbyeachGPUcanbeevensmaller. Incontrast,thevocabularyofSL
isconsistent: 1000classesforImageNet. ConsideringCLIPseesfarmorethan1000classesfroma
web-crawleddataset,theportionthatCLIP’strainingvocabularytakesisevensmaller. Toisolatethe
influenceoftrainingvocabulary,weexperimentbyformingdynamicclassifiersduringSLtraining.
Thisisdonebyrandomlysubsamplingthevocabulary(candidateclasses)toasmallersizeduring
training,thusformingdynamicclassificationtaskssimilartoCLIP(seedetailsinAppx.C.3).
Results. AsshowninFig.3a,samplinga✖smallervocabularynotablyreducesSL’spredictionbias,
andobtainsrobustnesssimilarto•template-basedCLIP.Regardingthefavorablevocabularysize,
smaller ones are more effective in reducing prediction bias (Fig. 3a), and intermediate ones also
improveaccuracy(Fig.3b). ThepreferredvocabularysizeforImageNet-Captionsisaround100.
Discussion.Ourintuitionofthephenomenaaboveisthatdynamicclassificationinsomewayachieves
class-levelre-balancing. Whenthegroundtruth(GT)correspondstoatailclass,asmallvocabulary
isolatesthenegativeimpactofmostheadclasses,avoidingbiastowardsthemandenablingthemodel
tofocusonclassifyingthetailclassitself. Besides,itisworthnotingthatasdemonstratedin[31,47],
optimizationcontinuesafterthemodel’spredictionsreachzeroerror,andseeksminimumintra-class
variabilityandmaximuminter-classmargin(especiallylargermarginaroundheadclasses). Thus
whentheGTisaheadclass,thisapproachlimitsthenumberofnegativeclassesandcouldprevent
themodelfromexcessivelydistortingtherepresentationsofthemthroughover-optimization.
3.4 Datadistribution(levelofimbalance,webdistributionshift,andintra-classdiversity)
ImageNet-Captions LAIONet (thr=0.7) LAIONet (thr=0.8) Rand init head Frozen CLIP head IN-Caps results
LAIONet (match freq) LAIONet (thr=0.75) LAIONet (thr=0.82)
3.0 40
104 0.6
2.5 35
103 2.0 0.5 intra-class diversity 30
102 1.5
25 0.4
1.0
101
20
0.5
100
0.3 data imbalance
0.0 15
0 250 500 750 1000 0.2 0.4 0.6 0.8 1.0
Ranked classes Intra-class image-image CLIP sim. 0.7 0.75 0.8 0.82 =freq 0.7 0.75 0.8 0.82 =freq
LAIONet text-def. sim. thresh. LAIONet text-def. sim. thresh.
(a)Distrib.ofLAIONetvariants(samescaleasIN-Caps). (b)ResultsofCLIPtrainedonLAIONetvariants.
Figure4: ResultsonLAIONetaboutdatadistribution(levelofdataimbalance,distributionshift,and
datadiversity). 1)Extremedataimbalancemakesmodelsmorepronetobias(lastcolumnvs.others).
2)Distributionshift(••vs.■■,lastcolumn)harmsdiscriminabilitybutcouldimproverobustnessif
pre-trainedtextheadisused. 3)Higherdatadiversity(smallerthreshold)alsoimprovesrobustness.
5
)elacs
gol( ycneuqerf
ssalC
noitcnuf
ytisned
ytilibaborP
)%
,1-pot(
.cca
teNegamI
tfihs
.irtsid
evomer
).cca
.sv .qerf(
noitalerroCMotivation. Motivatedbythefindingsof[26]regardingtheimpactofimagedistributiononCLIP’s
robustnesstonaturaldistributionshifts,ourstudyalsoexaminesitsinfluenceonrobustnesstodata
imbalance. Asshownin[74],ahigherfilterthresholdleadstoamorecondensedimagedistribution,
a result that is confirmed in Fig. 4a. We thus create LAIONet variants of different intra-class
variationsbyadjustingthisthreshold. Allvariantsinthissectionkeepthedatascalethesameas
ImageNet-Captions(0.45M).Inaddition,duetothedisparityinclassdistributionbetweenLAIONet
andImageNet-Captions,wealsocreateavariantthatalignswiththeclassfrequenciesofImageNet-
Captions(‘=freq’)whilepreservingwebimagedistribution. Thisvariantissampledfromthefull
version(3.26M)thatusesathresholdof0.7. MoredetailsaboutdatasetsareprovidedinAppx.C.5.
Results. AcomparisonbetweenmodelstrainedontheaforementioneddatasetsispresentinFig.4b.
Wefindthatwebdataisnotnaturallyfriendlyforde-biasing, butcouldhavemademodelsmore
biasedduetoextremedataimbalance(comparing‘=freq’withothercolumns). Thedistributionshift
ofwebdatacouldimproverobustnessifa•pre-trainedtextheadisavailable(circlesvs.squares,last
column). Ifnot,scalingmayhelp. Moreover,resultswithsmallerthresholdsalsoturnouttobemore
robust,indicatingthathigherintra-classdatadiversity(smallerthreshold)improvesrobustness.
3.5 Datascaling(alsoachievablevialanguagepre-training)
Motivation. WenotethatthecorrelationsofCLIPinFig.3a(x-axis)arestillhigherthanthatof
open-sourcemodelsinFig.1b. Onekeyremainingfactoristhescaleofpre-trainingdata(seeFig.1b
for large-scale results). Given that ImageNet-Captions is small-scaled (see Fig. 2), experiments
followingareconductedonLAIONet. SeeAppxs.C.4andC.5formoredetailsaboutthesetting.
Rand init head CLIP init head Frozen CLIP head Frozen RoBERTa head SL w/ frozen CLIP head
a b c d
25 0.26
40 0.6 20 com clp ua sc tete rsr 0.24 mala rgrg ine sr
30 0.22
0.5 15
0.20
20 10
0.4
0.18
10 5 0.3 0.16
0 0.14
0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3
Train data scale (million) Train data scale (million) Train data scale (million) Train data scale (million)
Figure5: ResultsonLAIONetsubsetsaboutdatascaleandtextencoder. 1)CLIP(a)discriminability
and(b)robustnessimproveasdatascalesup,andcanbeboostedbypre-trainedheads. 2)Afrozen
headhelpsCLIP(c)preserveintra-classvariationwhile(d)notharmingmargins,whichcanbelostif
fine-tuned. ItisalsounattainablebySLevenusingthesamehead. 3)Languagepre-trainingusing
CLIPismorefavorableforimage-texttasksthanpurelanguagemodeling(e.g.,RoBERTa[48]).
Results. Fig.5presentstheresultsobtainedfromuniformlysubsampledsubsetsofLAIONet. These
findingsextendthescalinglaw: asdatascales,ImageNetzero-shotaccuracy(Fig.5a)andmodels’
robustness to data imbalance (Fig. 5b) improve simultaneously. We also provide a comparison
between text encoders: • training from scratch, initializing with • pre-trained CLIP (frozen) or
•frozenRoBERTa[48],or•fine-tuningthetextencodertogether. •FrozenCLIPlanguagehead
enablesthevisionmodeltoleverageawell-establishedfeaturespaceassupervision,achievingbetter
dataefficiency(Fig.5a)androbustnesstodataimbalance(Fig.5b). •Fine-tuningCLIPtexthead
resultsinover-fitting(similarresultswith•trainingfromscratch),and•RoBERTadoesnotsuit
thecontrastivetaskandadversariallyaffectsperformance. FurtherinvestigationthroughNC-based
metricsshows••frozenheadseffectivelypreservesintra-classvariation(Fig.5c),whichisatriskof
beinglostwhen•fine-tuned. Both•frozenand•fine-tunedheadscontributetointer-classmargins
(Fig. 5d), and if • randomly initialized, scaling training data still can achieve improved margins.
Comparedto✖SL,CLIPcanbetterutilizeweb-crawleddataandpre-trainedtextencoder(Fig.5a).
Butnotethatwhenevaluatingclose-setaccuracy,thedataefficiencyofCLIPisstillmuchlowerthan
SLtrainedonclassificationdatasets(e.g.,ImageNet).
6
)%
,1-pot(
ycarucca
teNegamI
).cca
.sv
.qerf(
noitalerroC
)
noitairav
ssalc-artni(
1CN
)
nigram
ssalc-retni(
2CN3.6 Utilizationofopen-worldconcepts
Motivation. OneoverlookedfactorinSec.3.5(on CLIP Corr. (freq. vs acc.)
1K ImageNet classes) is the existence of massive a SL Corr. (freq. vs #pred.)
open-worldconceptsinweb-crawleddatasets. CLIP
onlyrequiresweakimage-textsupervisionandisthus
0.4
not bound by a pre-defined vocabulary. The open-
world concepts may share useful information with
0.2
close-setonesandgeneralizationcouldhappenwhen
datascalesup. Thissectionpresentsexperimentson
0.0
ImageNet-CaptionsandYFCC-15Msubsetsthatre-
vealscalingeffectsofthenumberofconcepts/classes. IN-Caps-100 IN-Caps (10%) IN-Caps
#concept #data
Results are shown in Fig. 6 and details of datasets
b
canbefoundinAppx.C.5. 0.8
SL can not utilize open-world data
Results. WepresentresultsonImageNet-Captions 0.6
subsets (evaluate on 100 classes) and YFCC-15M
subsets(evaluateon1Kclasses)inFig.6tovalidate 0.4
this. IN-Caps-100 stands for a 100-class subset of
0.2
ImageNet-Captions,andIN-Caps(10%)denotea1K-
class subset at the same scale as IN-Caps-100. In
YFCC15M-Cls YFCC-15M
Fig.6a,bothSLandCLIPattainadditionalrobust- #concept #data
nessfromthescalingofconceptanddata. However, Figure6: CLIPcanbenefitfromopen-world
expandingthevocabularyforSLislabel-expensivein concepts. (a)TrainonIN-Capsvariants,and
practice. ThusconceptsotherthanImageNetclasses evaluateon100classes. (b)TrainonYFCC-
inYFCC-15MdonotbenefitSLinFig.6b. 15Mvariants,andevaluateon1Kclasses.
3.7 UnderstandingthefeaturedistributionofCLIPpre-trainedatscale
Setting. Theresultsabovehaveshownthatthediscriminabilityandrobustnesstodataimbalance
improvesimultaneouslyaspre-trainingdatascalesup(Sec.3.5). Thenifpre-trainedonsufficient
data,whendoesCLIPfail(Fig.7a.1),whatdoesdataimbalanceaffect(Fig.7a.2),andhowarethey
reflectedinthefeaturespace(Fig.7b)? Toanswerthesequestions, weconsider3visionfeature-
relatedmetrics(•NC1,•NC2 M,•NC2n Mn)and2textfeature-relatedmetrics(•NC2 W,•NC2n Wn).
NC2 usesvisionfeaturecenters,andNC2 takesCLIP’stextclassifierasfeaturecenters. Margins
M W
arecomputedasaverageoverallotherclassesforNC2,andthattothenearestneighborforNC2nn.
NC1 NC2M NC2W NC2n Mn NC2n Wn
1 2
MetaCLIP-2.5B MetaCLIP-2.5B
LAION-2B LAION-2B
MetaCLIP-400M MetaCLIP-400M
LAION-400M LAION-400M
YFCC-15M YFCC-15M
CC-12M CC-12M
LAIONet-3M LAIONet-3M
0.4 0.2 0.0 0.2 0.4 0.2 0.0 0.2 0.4 0.6
Corr. to per-class accuracy Corr. to per-class data frequency
(b)t-SNEofCLIPtextcenters
(a)Correlationbetweenmodel(left),data(right),andfeaturestatistics. (pre-trainonLAION-400M).
Figure7: InspectingCLIP’sfailuresandeffectsofdataimbalancefromNC-basedmetrics. 1)Fail
classesofsmaller-scalemodels(12/15M)arehardlydiscriminativetomostclasses,whilelarger-scale
models(≥400M)onlyfailonsomenearest-neighborclasses. 2)Dataimbalanceisweaklycorrelated
withmostfeaturestatisticsexceptNC2 ,denotingdenserheadandcoarsertailclassesintextspace.
W
Results. Cluster compactness (• NC1) does not show a strong correlation with CLIP’s failures
(Fig.7a.1),andthefrequentclassesofLAIONmodelstendtopreservemoreintra-classvariation
(Fig.7a.2). Besides,therearesomeimplicationsfromthemarginbetweenclasscenters(••NC2).
7
001-NI
no
noitalerroC
K1-NI
no
noitalerroC
daeH
liaTFor example, Fig. 7a.1 shows that the fail classes of smaller-scale models (12/15M) are hardly
discriminativetomostclasses(•NC2 ),whilelarger-scalemodels(≥ 400M)onlyfailonsome
M
nearest-neighbor classes (• NC2nn). This indicates that the failing classes already have good
M
separationfrommostotherclasses,andtheconfusionprimarilycomesfromveryfewhardclasses.
RegardingtheeffectsofdataimbalanceonCLIP(Fig.7a.2),wefindastrongconnectionto•NC2 ,
W
denotingdenserheadandcoarsertailclassesintextspace. t-SNE[83]oftheclasscentersisprovided
inFig.7bforreference,andmorevisualizationsofvisionfeaturescanbefoundinFig.20.
Discussions. Thoughweaklycorrelatedtotheclassfrequency,CLIP’sperformanceisstillhighly
biased[84,95].Ifdataimbalanceisnotthemaincause,thenwhatareothersuspectofCLIP’sfailures?
WehypothesizethatImageNetisintrinsicallybiased. Theclassesarenotofequaldifficulty[16]and
someareevenambiguous[5,37,72],e.g.,“sunglass”vs.“sunglasses”. Inthiscase,itispossible
foramodeltrainedonthebalancedImageNettobebiased[16],andsomeerrorsareunsolvableno
matterhowmuchtrainingdataisadded. Besides,CLIPleveragesopen-worldconceptsintraining,
whicharenotcountedforfrequencybutstillcouldaffectclose-setperformance. Moreover,such
biasesmightbeconnectedwithCLIP’shallucination[30,50,89]. Webelievethesearevaluable
questionstobeexplored. Insupplementtothisdiscussion,wealsodiscussCLIP’sbiasmeasuredon
broadersetsofconceptsinAppx.A.2andtheeffectsofdataimbalanceonCLIPinAppx.A.5.
4 AcquiringCLIP-levelgeneralization
ThissectionshowsfindingsfromCLIP’sunderlyingmechanismscanbeappliedtobothsupervised
learning(Sec.4.1)andself-supervisedlearning(Sec.4.2)underseveredataimbalance.
4.1 Data-imbalancedlearning: anextremecase
InquestofthelimitofCLIP’srobustnesstopre-trainingdataimbalance,wecreateanextremecase
basedonImageNet-Captions: trimmingthetailclassestoonlyoneshot,orevencompletelyzeroshot
(i.e.,anopen-worldsetting). Wethentrainmodelsonthistrimmeddataset,andevaluateperformance
onImageNetregardingtail/otherclasses. AsshowninFig.8, atthescaleofImageNet-Captions
(∼0.45M), • CLIP trained from scratch also fails on tail classes when trained under severe data
imbalance. Despitethis,byadoptinga•pre-trainedtextencoderfollowingSec.3.5,CLIPacquires
open-worldknowledgeanddemonstratessuperiorgeneralizationontail(andopen-world)classes.
ThenhowmuchcananSLmodelacquiresuchgeneralization? Surprisingly,wefindtrainingitwith
✖frozenclassprototypesproducedbyCLIPtextheadisnoteffective. Instead,also✖subsampling
thevocabularyduringtrainingisnecessarytoachieveasimilarlevelofgeneralizationasCLIP.
CLIP SL Random init head Frozen CLIP head Frozen CLIP head + sub voc. (SL)
a Train w/ 1-shot tail b Train w/ 1-shot tail c Train w/ 0-shot tail d Train w/ 0-shot tail
500 500 500 500
200 200 200 200
100 100 100 100
50 50 50 50
20 20 20 20
5 5 5 5
40 50 60 0 5 10 15 20 40 50 60 0 5 10 15 20
Avg head & mid acc (top-1, %) Avg tail acc (top-1, %) Avg head & mid acc (top-1, %) Avg tail acc (top-1, %)
Figure8: Anextremecase: wetrainSLmodelsonIN-Capsvariantsthathavetailclassestrimmed
toonlyoneshot(a&b)orevenzeroshot(c&d),andevaluatetheaccuracyonthetailandother
classes. •CLIPwithafrozenpre-trainedtextencodershowssuperiorgeneralization,whichcanbe
acquiredbya✖SLmodelwith✖fixedclassprototypesfromCLIPand✖vocabularysubsampling.
Tounderstandtheunderlyingmechanisms,wepresentacasestudyontheaffinitymatrixbetween
classifiers, and tail class accuracies under the zero-shot tail (50 classes) setting in Fig. 9. The
affinitymatricesoftheclassificationhead(seeFig.9a,wesubsample100classesforvisualization)
demonstratethatthelearnedtailprototypescollapsetosingularity,whiletheclassprototypesfrom
8
sessalc
liat
fo
rebmuNClassifier sim. (rand init) Classifier sim. (frozen CLIP) Tail per-class acc. (top-1, %) Tail per-class #predictions
1.0 1.0
60 Rand init (y=0)
0.8 0.8 80 Frozen CLIP
Frozen CLIP + sub voc.
0.6 0.6 40 60
0.4 0.4
40
0.2 0.2 20
20
0.0 0.0
0 0
(a)Affinitymatricesoftheclassificationhead. (b)Distributionsofmodels’predictionsperclass.
Figure9: AcasestudyofSLunderthezero-shottailsetting. (a)SLmodelsseekmaximalmargins
betweenclassifiers,andtailprototypescollapsetogether. Instead,CLIPhasahealthierstructure. (b)
UsingCLIPheadsolelyislesseffective,andvoc.subsamplingisneededforCLIP-likegeneralization.
CLIP maintain a healthier structure. Replacing the learned head with frozen CLIP prototypes
alleviatesclassifierbias. However,per-classaccuracies(seeFig.9b)showthatusingthisheadalone
ismerelyeffective,onlysmallimprovementsareobservedinveryfewclasses,indicatingthatthe
representationsarestillbiased. Additionally,applyingvocabularysubsamplingovercomesthehidden
biasinsupervision,allowstherepresentationstofitthemanifoldencodedbyCLIPtextembeddings,
andgeneralizestoopenclassesthatCLIPhasseeninpre-training. Wenotethatthissettingshares
similarities with open-vocabulary recognition. Surprisingly, we indeed find a similar technique
(termedfederatedloss)usedinopen-vocabularyobjectdetection(OVOD)[94],butfewexplorations
existintherelevantliterature. Ourstudyprovidesathoroughanalysisofthistechniquefromanother
perspective,andwehopeitcanmotivatefutureapplicationsinthisfield.
4.2 Empoweringself-supervisedlearningin-the-wildatscale
To show the universality of the aforementioned techniques, we also explore the application in
improvingself-supervisedlearningwhenpre-trainedonimbalanceddata. Asdiscussedin[2,58],
DINO’sperformanceissensitivetotheimbalanceinweb-crawledpre-trainingdata,andthusdata
deduplicationisacrucialprocessinDINOv2[58]. Asdiscussedbyarecentstudy[29],thelearnable
prototypes of DINO (akin to the classifier of SL) may be biased to imbalanced data and many
collapses(likeFig.9a). Wehypothesizethatapplyingsubsamplingtotheprototypesmayalleviate
thisphenomenon. Ourintuitionisthattheoperationresemblesdropoutandcouldencouragebetter
utilizationoftheonline-learnedprototypesofDINO,thusimprovingrepresentationslearnedfrom
uncuratedwebdata. BasedonvanillaDINO[10],werandomlysubsampleprototypes(insteadof
usingthemall)duringthecalculationoftheself-distillationloss(seedetailsinAppx.D).Allmodels
arepre-trainedfor100epochsonLAIONet,andevaluatedonthetransferlearningbenchmarkof[38].
DINO variants on LAIONet vs. vanilla DINO on ImageNet
25 .. 50
Birdsnap
OxfordPets
Food101
Caltech101
PascalVOC20 C0 I7
FAR100
CIFAR10 SUN397
Flowers102
0.0
52 .. 05
DTD
FGVCAircraft
StanfordCars
7.5
Method Vocabulary size
10.0 Vanilla DINO 1024 16384
+ Sub voc. 4096 65536
12.5
Figure10: TransferlearningresultsofDINOvariantspre-trainedonLAIONetvs.vanillaDINO
trained on ImageNet. Extreme data imbalance makes LAIONet much harder for DINO to learn
transferrable representations. The ■ vocabulary subsampling strategy effectively helps ■ DINO
alleviatesuchdefectsandgenerallymatchImageNet-pretrainedperformance.
9
)%(
erocS
eborP
raeniLResultsinFig.10andTab.2showthatcomparedtopre-trainingonImageNet,■vanillaDINO’s
performancedropsnotablyamong11datasetsoutof12. Instead,■vocabulary-subsamplingnarrows
thegapbyalargemargin,highlightingthistechnique’seffectivenessonlarge-scaledatainthewild.
Toruleouttheinfluenceoftotalvocabularysize(numberofprototypes),wealsotrain■vanillaDINO
withreducedvocabulary(16384). Thismodelisnotablyweakerthanthattrainedwith■subsampling
(16384foreachtrainingiter,65536intotal),andsupportstheimprovement’seffectiveness.
5 Limitations,futurework,andbroaderimpacts
Limitations. OurstudyhasfocusedontherobustnessofCLIP-typemodelsinrelationtothedata
imbalance naturally raised from web data sources. We have demonstrated that our findings are
transferrabletothesupervisedandself-supervisedlearningsettingforclassificationtasks. However,
weacknowledgethatourestimationofimage-textdatasets’conceptfrequencyisbasedonasimple
rule-basedpipeline,whichcouldbepronetocaptionnoise,multi-label,andambiguity. Besides,CLIP
modelsarenotonlyemployedforclassificationtasks,thestudyofleveragingCLIPforopen-world
detectionorsegmentationistheareaourstudydoesnotcover. Additionally,giventhenatureofthe
web-baseddatasourcesusedinourstudy,weacknowledgethatthedatamaycontainimplicitbiasor
harmfulinformation. WeprovidemorediscussionsinAppx.A.
Futurework. Ourfindingscoverinsightsinlanguagesupervision,pretexttask,datascaling,and
conceptscaling,butonlyasmallportionarevalidatedinapplication. Onedirectionforfutureworkis
toexploretheuseoflanguagesupervisionandopen-worlddatainrecognitionmodels. Besides,a
recentwork[41]findsAdamoptimizertooutperform(stochastic)gradientdescentonheavy-tailed
data,whichcanbeanotherfactorinCLIP’srobustnessandisworthfurtherexploration. Ontheother
hand,weareinterestedinextendingourdiscoverytotheopen-worlddetectionandsegmentation
taskstoseeifourfindingsstillholdunderthesemorechallengingscenarios. Furthermore,aswehave
analyzedinourstudy,languagesupervisionplaysanimportantroleinachievingsuchrobustnesstothe
dataimbalance,thuswearealsointerestedinstudyingwhetherornotsimilartracesofgeneralization
existin(multi-modal)largelanguagemodels(e.g.,Llama[80],BLIP-2[43],LLaVA[45],etc.)and
potentiallyrevealthereasonfortheireffectivenessinfuturestudies.
Broaderimpacts. Weprovideanin-depthanalysisofCLIP’srobustnesstodataimbalance,which
helpsunderstandtheeffectivenessofCLIP.Thetechniquesherearealsoshowntobeeffectivefor
otherdomains(supervisedlearningandself-supervisedlearning)toovercomebiasesintailunder-
representedclasses. Thus,weexpectourworknottoposepotentialnegativesocietalconsequences
butrathertoimprovesociety’soverallequalityandinclusiveness.
6 Concludingremarks
Our work starts with the observation that although web-crawled datasets share an extremely im-
balanceddatadistribution,CLIPisrelativelymorerobusttoit. Extensivestudieson1)language
supervision,2)pretexttask,3)webdatadistribution,4)datascaling,and5)open-worldconcepts
revealsignificantfindingsabouttheunderlyingmechanismsofthisrobustness. Wehavealsodemon-
stratedthatthesefindingscanbetransferredtoclassificationandself-supervisedlearningmethods,
yieldingimprovedgeneralizationunderpre-trainingdataimbalance. Ourstudyuncoverskeyfactors
ofCLIP’srobustnesstopre-trainingdataimbalance,andprovidesnewperspectivestounderstandits
generalizability. Theinsightslearnedarevalidatedontasksfromextremelylong-tailedsupervised
learningtoself-supervisedlearningonweb-crawleddata. WhileCLIPhasbeenagamechangerin
theseresearchfields,ithaslongbeenutilizedasis. Ourstudy,instead,delvedintothemechanisms
behindCLIP,providinganopportunitytoimprovedownstreamtasksbyleveragingtheunderlying
mechanismsratherthanrelyingsolelyonthemodelitself,withgreaterflexibilityandadaptability.
Acknowledgments
This work has been supported by Hong Kong Research Grant Council — Early Career Scheme
(GrantNo. 27209621),GeneralResearchFundScheme(GrantNo. 17202422),andRGCResearch
MatchingGrantScheme(RMGS).PartofthedescribedresearchworkisconductedintheJCSTEM
LabofRoboticsforSoftMaterialsfundedbyTheHongKongJockeyClubCharitiesTrust.
10References
[1] YukiM.Asano,ChristianRupprecht,andAndreaVedaldi. Self-labellingviasimultaneousclusteringand
representationlearning. InTheEighthInternationalConferenceonLearningRepresentations,Virtual,26
Apr–1May2020.
[2] MidoAssran,RandallBalestriero,QuentinDuval,FlorianBordes,IshanMisra,PiotrBojanowski,Pascal
Vincent,MichaelRabbat,andNicolasBallas. Thehiddenuniformclusterpriorinself-supervisedlearning.
InTheEleventhInternationalConferenceonLearningRepresentations,Kigali,Rwanda,1–5May2023.
[3] YutongBai,JieruMei,AlanL.Yuille,andCihangXie. AreTransformersmorerobustthanCNNs? In
M.Ranzato,A.Beygelzimer,Y.Dauphin,P.Liang,andJ.W.Vaughan,editors,AdvancesinNeuralInfor-
mationProcessingSystems,volume34,pages26831–26843,Virtual,6–14Dec2021.CurranAssociates,
Inc.
[4] ThomasBerg, JiongxinLiu, SeungWooLee, MichelleL.Alexander, DavidW.Jacobs, andPeterN.
Belhumeur. Birdsnap:Large-scalefine-grainedvisualcategorizationofbirds. InProceedingsoftheIEEE
ConferenceonComputerVisionandPatternRecognition(CVPR),pages2011–2018,Columbus,OH,USA,
23–28Jun2014.IEEE.
[5] LucasBeyer,OlivierJ.Hénaff,AlexanderKolesnikov,XiaohuaZhai,andAäronvandenOord. Arewe
donewithImageNet? arXiv:2006.07159,Jun2020.
[6] LukasBossard,MatthieuGuillaumin,andLucVanGool. Food-101–miningdiscriminativecomponents
withrandomforests. InD.Fleet,T.Pajdla,B.Schiele,andT.Tuytelaars,editors,ComputerVision–ECCV
2014,volume8694ofLNCS,pages446–461,Zurich,Switzerland,6–12Sep2014.Springer.
[7] MathildeCaron,PiotrBojanowski,ArmandJoulin,andMatthijsDouze. Deepclusteringforunsupervised
learningofvisualfeatures. InV.Ferrari,M.Hebert,C.Sminchisescu,andY.Weiss,editors,Computer
Vision–ECCV2018,volume11218ofLNCS,pages139–156,Munich,Germany,8–14Sep2018.Springer.
[8] MathildeCaron,PiotrBojanowski,JulienMairal,andArmandJoulin. Unsupervisedpre-trainingofimage
featuresonnon-curateddata. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision(ICCV),pages2959–2968,Seoul,Korea,27Oct–2Nov2019.IEEE/CVF.
[9] MathildeCaron,IshanMisra,JulienMairal,PriyaGoyal,PiotrBojanowski,andArmandJoulin. Unsuper-
visedlearningofvisualfeaturesbycontrastingclusterassignments. InAdvancesinNeuralInformation
ProcessingSystems,volume33,pages9912–9924,Virtual,6–12Dec2020.CurranAssociates,Inc.
[10] MathildeCaron,HugoTouvron,IshanMisra,HervéJégou,JulienMairal,PiotrBojanowski,andArmand
Joulin. Emergingpropertiesinself-supervisedvisiontransformers. InProceedingsoftheIEEE/CVFInter-
nationalConferenceonComputerVision(ICCV),pages9650–9660,Virtual,11–17Oct2021.IEEE/CVF.
[11] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-
scaleimage-textpre-trainingtorecognizelong-tailvisualconcepts. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),pages3558–3568,Virtual,19–25Jun
2021.IEEE/CVF.
[12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastivelearningofvisualrepresentations. InH.D.IIIandA.Singh,editors,Proceedingsofthe37th
InternationalConferenceonMachineLearning,volume119ofPMLR,pages1597–1607,Virtual,13–18
Jul2020.PMLR.
[13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and
CLawrenceZitnick. MicrosoftCOCOcaptions:Datacollectionandevaluationserver. arXiv:1504.00325,
Apr2015.
[14] MehdiCherti,RomainBeaumont,RossWightman,MitchellWortsman,GabrielIlharco,CadeGordon,
Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive
language-imagelearning. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),pages2818–2829,Vancouver,Canada,18–22Jun2023.IEEE/CVF.
[15] MirceaCimpoi,SubhransuMaji,IasonasKokkinos,SammyMohamed,andAndreaVedaldi. Describing
texturesinthewild. InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition
(CVPR),pages3606–3613,Columbus,OH,USA,23–28Jun2014.IEEE.
[16] JiequanCui,BeierZhu,XinWen,XiaojuanQi,BeiYu,andHanwangZhang. Classesarenotequal:An
empiricalstudyonimagerecognitionfairness. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR),Seattle,WA,USA,17–21Jun2024.IEEE/CVF.
11[17] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.ImageNet:Alarge-scalehierarchical
imagedatabase. InIEEEConferenceonComputerVisionandPatternRecognition,pages248–255,Miami,
FL,USA,20–25Jun2009.IEEE.
[18] KaranDesaiandJustinJohnson. VirTex: Learningvisualrepresentationsfromtextualannotations. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),pages
11162–11173,Virtual,19–25Jun2021.IEEE/CVF.
[19] KaranDesai,GauravKaul,ZubinAysola,andJustinJohnson. RedCaps: Web-curatedimage-textdata
createdbythepeople,forthepeople. InJ.VanschorenandS.Yeung,editors,ProceedingsoftheNeural
InformationProcessingSystemsTrackonDatasetsandBenchmarks,volume1,Virtual,6–14Dec2021.
[20] Benjamin Devillers, Bhavin Choksi, Romain Bielawski, and Rufin VanRullen. Does language help
generalizationinvisionmodels? InA.BisazzaandO.Abend,editors,Proceedingsofthe25thConference
onComputationalNaturalLanguageLearning,pages171–182,Online,10–11Nov2021.ACL.
[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,and
NeilHoulsby. Animageisworth16x16words:Transformersforimagerecognitionatscale. InTheNinth
InternationalConferenceonLearningRepresentations,Virtual,3–7May2021.
[22] LinusEricsson,HenryGouk,andTimothyM.Hospedales. Howwelldoself-supervisedmodelstransfer?
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),pages
5414–5423,Virtual,19–25Jun2021.IEEE/CVF.
[23] MarkEveringham,LucVanGool,ChristopherK.I.Williams,JohnWinn,andAndrewZisserman. The
Pascalvisualobjectclasses(VOC)challenge. InternationalJournalofComputerVision,88:303–338,
2010.
[24] LijieFan,DilipKrishnan,PhillipIsola,DinaKatabi,andYonglongTian. ImprovingCLIPtrainingwith
languagerewrites. InA.Oh,T.Neumann,A.Globerson,K.Saenko,M.Hardt,andS.Levine,editors,
AdvancesinNeuralInformationProcessingSystems,volume36,pages35544–35575,NewOrleans,LA,
USA,10–16Dec2023.CurranAssociates,Inc.
[25] LijieFan,KaifengChen,DilipKrishnan,DinaKatabi,PhillipIsola,andYonglongTian. Scalinglawsof
syntheticimagesformodeltraining...fornow. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR),Seattle,WA,USA,17–21Jun2024.IEEE/CVF.
[26] AlexFang,GabrielIlharco,MitchellWortsman,YuhaoWan,VaishaalShankar,AchalDave,andLudwig
Schmidt. Datadeterminesdistributionalrobustnessincontrastivelanguageimagepre-training(CLIP). In
K.Chaudhuri,S.Jegelka,L.Song,C.Szepesvari,G.Niu,andS.Sabato,editors,Proceedingsofthe39th
InternationalConferenceonMachineLearning,volume162ofPMLR,pages6216–6234,Baltimore,MD,
USA,17–23Jul2022.PMLR.
[27] LiFei-Fei,RobFergus,andPietroPerona. One-shotlearningofobjectcategories. IEEETransactionson
PatternAnalysisandMachineIntelligence,28(4):594–611,2006.
[28] SamirY.Gadre,GabrielIlharco,AlexFang,JonathanHayase,GeorgiosSmyrnis,ThaoNguyen,Ryan
Marten,MitchellWortsman,DhrubaGhosh,JieyuZhang,EyalOrgad,RahimEntezari,GiannisDaras,
SarahPratt,VivekRamanujan,YonatanBitton,KalyaniMarathe,StephenMussmann,RichardVencu,
MehdiCherti,RanjayKrishna,PangWeiKoh,OlgaSaukh,AlexanderJ.Ratner,ShuranSong,Hannaneh
Hajishirzi,AliFarhadi,RomainBeaumont,SewoongOh,AlexDimakis,JeniaJitsev,YairCarmon,Vaishaal
Shankar,andLudwigSchmidt. DataComp:Insearchofthenextgenerationofmultimodaldatasets. In
A.Oh,T.Neumann,A.Globerson,K.Saenko,M.Hardt,andS.Levine,editors,AdvancesinNeural
InformationProcessingSystems,volume36,pages27092–27112,NewOrleans,LA,USA,10–16Dec
2023.CurranAssociates,Inc.
[29] HariprasathGovindarajan,PerSidén,JacobRoll,andFredrikLindsten. Onpartialprototypecollapsein
clustering-basedself-supervisedlearning.SubmissiontoTheTwelfthInternationalConferenceonLearning
Representations,2024.
[30] MelissaHall,LaurensvanderMaaten,LauraGustafson,MaxwellJones,andAaronAdcock. Asystematic
studyofbiasamplification. arXiv:2201.11706,Jan2022.
[31] X.Y.Han,VardanPapyan,andDavidL.Donoho. NeuralcollapseunderMSEloss: Proximitytoand
dynamicsonthecentralpath. InTheTenthInternationalConferenceonLearningRepresentations,Virtual,
25–29Apr2022.
12[32] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecognition.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),pages
770–778,LasVegas,NV,USA,26Jun–1Jul2016.IEEE/CVF.
[33] PatrickHelber,BenjaminBischke,AndreasDengel,andDamianBorth. IntroducingEuroSAT:Anovel
datasetanddeeplearningbenchmarkforlanduseandlandcoverclassification. InIEEEInternational
GeoscienceandRemoteSensingSymposium,pages204–207,Valencia,Spain,22–27Jul2018.IEEE.
[34] DanHendrycks,MantasMazeika,SauravKadavath,andDawnSong. Usingself-supervisedlearningcan
improvemodelrobustnessanduncertainty. InH.Wallach,H.Larochelle,A.Beygelzimer,F.d'Alché-Buc,
E.Fox,andR.Garnett,editors,AdvancesinNeuralInformationProcessingSystems,volume32,pages
15584–15595,Vancouver,BC,Canada,8–14Dec2019.CurranAssociates,Inc.
[35] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,HieuPham,QuocV.Le,Yun-HsuanSung,
ZhenLi,andTomDuerig. Scalingupvisualandvision-languagerepresentationlearningwithnoisytext
supervision. InM.MeilaandT.Zhang,editors,Proceedingsofthe38thInternationalConferenceon
MachineLearning,volume139ofPMLR,pages4904–4916,Virtual,18–24Jul2021.PMLR.
[36] Bingyi Kang, Saining Xie, Marcus Rohrbach, Zhicheng Yan, Albert Gordo, Jiashi Feng, and Yannis
Kalantidis.Decouplingrepresentationandclassifierforlong-tailedrecognition.InTheEighthInternational
ConferenceonLearningRepresentations,Virtual,26Apr–1May2020.
[37] Polina Kirichenko, Mark Ibrahim, Randall Balestriero, Diane Bouchacourt, Ramakrishna Vedantam,
HamedFirooz,andAndrewGordonWilson. Understandingthedetrimentalclass-leveleffectsofdata
augmentation.InA.Oh,T.Neumann,A.Globerson,K.Saenko,M.Hardt,andS.Levine,editors,Advances
inNeuralInformationProcessingSystems,volume36,pages17498–17526,NewOrleans,LA,USA,
10–16Dec2023.CurranAssociates,Inc.
[38] SimonKornblith, JonathonShlens, andQuocV.Le. DobetterImageNetmodelstransferbetter? In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR),pages
2661–2671,LongBeach,CA,USA,16–20Jun2019.IEEE/CVF.
[39] JonathanKrause,MichaelStark,JiaDeng,andLiFei-Fei. 3Dobjectrepresentationsforfine-grained
categorization. InIEEEInternationalConferenceonComputerVisionWorkshops,pages554–561,Sydney,
NSW,Australia,2–8Dec2013.IEEE.
[40] AlexKrizhevsky. Learningmultiplelayersoffeaturesfromtinyimages. Technicalreport,Universityof
Toronto,2009.
[41] FrederikKunstner,RobinYadav,AlanMilligan,MarkSchmidt,andAlbertoBietti. Heavy-tailedclass
imbalanceandwhyAdamoutperformsgradientdescentonlanguagemodels. arXiv:2402.19449,Feb2024.
[42] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping language-image pre-
trainingforunifiedvision-languageunderstandingandgeneration. InK.Chaudhuri,S.Jegelka,L.Song,
C.Szepesvari,G.Niu,andS.Sabato,editors,Proceedingsofthe39thInternationalConferenceonMachine
Learning,volume162ofPMLR,pages12888–12900,Baltimore,MD,USA,17–23Jul2022.PMLR.
[43] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. BLIP-2: Bootstrappinglanguage-imagepre-
trainingwithfrozenimageencodersandlargelanguagemodels. InA.Krause, E.Brunskill, K.Cho,
B.Engelhardt,S.Sabato,andJ.Scarlett,editors,Proceedingsofthe40thInternationalConferenceon
MachineLearning, volume202ofPMLR,pages19730–19742, Honolulu, HI,USA,23–29Jul2023.
PMLR.
[44] WeixinLiang,YuhuiZhang,YongchanKwon,SerenaYeung,andJamesY.Zou. Mindthegap:Under-
standingthemodalitygapinmulti-modalcontrastiverepresentationlearning. InS.Koyejo,S.Mohamed,
A.Agarwal,D.Belgrave,K.Cho,andA.Oh,editors,AdvancesinNeuralInformationProcessingSystems,
volume35,pages17612–17625,NewOrleans,LA,USA,28Nov–9Dec2022.CurranAssociates,Inc.
[45] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. In A. Oh,
T.Naumann,A.Globerson,K.Saenko,M.Hardt,andS.Levine,editors,AdvancesinNeuralInformation
ProcessingSystems,volume36,pages34892–34916,NewOrleans,LA,USA,10–16Dec2023.Curran
Associates,Inc.
[46] HongLiu,JeffZ.HaoChen,AdrienGaidon,andTengyuMa. Self-supervisedlearningismorerobustto
datasetimbalance. InTheTenthInternationalConferenceonLearningRepresentations,Virtual,25–29
Apr2022.
13[47] XuantongLiu,JianfengZhang,TianyangHu,HeCao,YuanYao,andLujiaPan. Inducingneuralcollapse
indeep long-tailedlearning. InF.Ruiz, J. Dy, andJ.-W.vandeMeent, editors, ProceedingsofThe
26th International Conference on Artificial Intelligence and Statistics, volume 206 of PMLR, pages
11534–11544,Valencia,Spain,25–27Apr2023.PMLR.
[48] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,MikeLewis,
LukeZettlemoyer,andVeselinStoyanov. RoBERTa:ArobustlyoptimizedBERTpretrainingapproach.
arXiv:1907.11692,Jul2019.
[49] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeichtenhofer,TrevorDarrell,andSainingXie. A
convnetforthe2020s. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),pages11976–11986,NewOrleans,LA,USA,19–24Jun2022.IEEE/CVF.
[50] ZixianMa,JerryHong,MustafaOmerGul,MonaGandhi,IrenaGao,andRanjayKrishna. CREPE:Can
vision-languagefoundationmodelsreasoncompositionally? InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition(CVPR),pages2818–2829,Vancouver,Canada,18–22Jun
2023.IEEE/CVF.
[51] SubhransuMaji,EsaRahtu,JuhoKannala,MatthewBlaschko,andAndreaVedaldi. Fine-grainedvisual
classificationofaircraft. arXiv:1306.5151,Jun2013.
[52] PrasannaMayilvahanan,ThaddäusWiedemer,EvgeniaRusak,MatthiasBethge,andWielandBrendel.
DoesCLIP’sgeneralizationperformancemainlystemfromhightrain-testsimilarity? InTheTwelfth
InternationalConferenceonLearningRepresentations,Vienna,Austria,7–11May2024.
[53] GeorgeA.Miller. WordNet:alexicaldatabaseforEnglish. CommunicationsoftheACM,38(11):39–41,
Nov1995.
[54] NormanMu,AlexanderKirillov,DavidWagner,andSainingXie. SLIP:Self-supervisionmeetslanguage-
imagepre-training. InS.Avidan,G.Brostow,M.Cissé,G.M.Farinella,andT.Hassner,editors,Computer
Vision–ECCV2022,volume13686ofLNCS,pages529–544,TelAviv,Israel,23–27Oct2022.Springer.
[55] ThaoNguyen, GabrielIlharco, MitchellWortsman, SewoongOh, andLudwigSchmidt. Qualitynot
quantity:OntheinteractionbetweendatasetdesignandrobustnessofCLIP. InS.Koyejo,S.Mohamed,
A.Agarwal,D.Belgrave,K.Cho,andA.Oh,editors,AdvancesinNeuralInformationProcessingSystems,
volume35,pages21455–21469,NewOrleans,LA,USA,28Nov–9Dec2022.CurranAssociates,Inc.
[56] Maria-ElenaNilsbackandAndrewZisserman. Automatedflowerclassificationoveralargenumberof
classes. InSixthIndianConferenceonComputerVision,GraphicsandImageProcessing,pages722–729,
Bhubaneswar,India,16–19Dec2008.IEEE.
[57] OpenAI. GPT-4technicalreport. arXiv:2303.08774,Mar2023.
[58] MaximeOquab,TimothéeDarcet,ThéoMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,Pierre
Fernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,MidoAssran,NicolasBallas,Wojciech
Galuba,RussellHowes,Po-YaoHuang,Shang-WenLi,IshanMisra,MichaelRabbat,VasuSharma,Gabriel
Synnaeve,HuXu,HerveJegou,JulienMairal,PatrickLabatut,ArmandJoulin,andPiotrBojanowski.
DINOv2:Learningrobustvisualfeatureswithoutsupervision.TransactionsonMachineLearningResearch,
2024.
[59] Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2Text: Describing images using 1 million
captionedphotographs. InJ.Shawe-Taylor,R.Zemel,P.Bartlett,F.Pereira,andK.Weinberger,editors,
AdvancesinNeuralInformationProcessingSystems,volume24,pages1143–1151,Granada,Spain,12–25
Dec2011.CurranAssociates,Inc.
[60] VardanPapyan,X.Y.Han,andDavidL.Donoho. Prevalenceofneuralcollapseduringtheterminalphase
ofdeeplearningtraining. ProceedingsoftheNationalAcademyofSciences,117(40):24652–24663,2020.
[61] ShubhamParashar,ZhiqiuLin,TianLiu,XiangjueDong,YananLi,DevaRamanan,JamesCaverlee,and
ShuKong. Theneglectedtailsofvision-languagemodels. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition(CVPR),Seattle,WA,USA,17–21Jun2024.IEEE/CVF.
[62] OmkarM.Parkhi, AndreaVedaldi, AndrewZisserman, andC.V.Jawahar. Catsanddogs. InIEEE
ConferenceonComputerVisionandPatternRecognition,pages3498–3505,Providence,RI,USA,2012.
IEEE.
[63] KarlPearsonandFrancisGalton.Noteonregressionandinheritanceinthecaseoftwoparents.Proceedings
oftheRoyalSocietyofLondon,58:240–242,1895.
14[64] HieuPham,ZihangDai,GolnazGhiasi,KenjiKawaguchi,HanxiaoLiu,AdamsWeiYu,JiahuiYu,Yi-Ting
Chen,Minh-ThangLuong,YonghuiWu,MingxingTan,andQuocV.Le. Combinedscalingforzero-shot
transferlearning. Neurocomputing,555:126658,2023.
[65] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever. Learning
transferablevisualmodelsfromnaturallanguagesupervision. InM.MeilaandT.Zhang,editors,Proceed-
ingsofthe38thInternationalConferenceonMachineLearning,volume139ofPMLR,pages8748–8763,
Virtual,18–24Jul2021.PMLR.
[66] VivekRamanujan,ThaoNguyen,SewoongOh,AliFarhadi,andLudwigSchmidt. Ontheconnection
betweenpre-trainingdatadiversityandfine-tuningrobustness. InA.Oh,T.Neumann,A.Globerson,
K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems,
volume36,pages66426–66437,NewOrleans,LA,USA,10–16Dec2023.CurranAssociates,Inc.
[67] BenjaminRecht, RebeccaRoelofs, LudwigSchmidt, andVaishaalShankar. DoImageNetclassifiers
generalize to ImageNet? In K. Chaudhuri and R. Salakhutdinov, editors, Proceedings of the 36th
InternationalConferenceonMachineLearning,volume97ofPMLR,pages5389–5400,LongBeach,CA,
USA,9–15Jun2019.PMLR.
[68] ShibaniSanturkar,YannDubois,RohanTaori,PercyLiang,andTatsunoriHashimoto. Isacaptionworth
athousandimages? Astudyonrepresentationlearning. InTheEleventhInternationalConferenceon
LearningRepresentations,Kigali,Rwanda,1–5May2023.
[69] MertBulentSariyildiz, JulienPerez, andDianeLarlus. Learningvisualrepresentationswithcaption
annotations. InA.Vedaldi,H.Bischof,T.Brox,andJ.-M.Frahm,editors,ComputerVision–ECCV2020,
volume12353ofLNCS,pages153–170,Online,23–28Aug2020.Springer.
[70] ChristophSchuhmann,RichardVencu,RomainBeaumont,RobertKaczmarczyk,ClaytonMullis,Aarush
Katta,TheoCoombes,JeniaJitsev,andAranKomatsuzaki. LAION-400M:OpendatasetofCLIP-filtered
400millionimage-textpairs. arXiv:2111.02114,Nov2021.
[71] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,MehdiCherti,
TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,PatrickSchramowski,SrivatsaKun-
durthy,KatherineCrowson,LudwigSchmidt,RobertKaczmarczyk,andJeniaJitsev. LAION-5B:Anopen
large-scaledatasetfortrainingnextgenerationimage-textmodels. InS.Koyejo,S.Mohamed,A.Agarwal,
D.Belgrave,K.Cho,andA.Oh,editors,AdvancesinNeuralInformationProcessingSystems,volume35,
pages25278–25294,NewOrleans,LA,USA,28Nov–9Dec2022.CurranAssociates,Inc.
[72] Jie-JingShao,Jiang-XinShi,Xiao-WenYang,Lan-ZheGuo,andYu-FengLi. Investigatingthelimitation
ofCLIPmodels:Theworst-performingcategories. arXiv:2310.03324,Oct2023.
[73] PiyushSharma,NanDing,SebastianGoodman,andRaduSoricut. ConceptualCaptions: Acleaned,
hypernymed,imagealt-textdatasetforautomaticimagecaptioning. InI.GurevychandY.Miyao,editors,
Proceedingsofthe56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:Long
Papers),pages2556–2565,Melbourne,Australia,15–20Jul2018.ACL.
[74] AliShiraliandMoritzHardt. WhatmakesImageNetlookunlikeLAION. arXiv:2306.15769,Jun2023.
[75] CharlesE.Spearman. Theproofandmeasurementofassociationbetweentwothings. TheAmerican
JournalofPsychology,15(1):72–101,1904.
[76] KrishnaSrinivasan,KarthikRaman,JiecaoChen,MichaelBendersky,andMarcNajork. WIT:Wikipedia-
based image text dataset for multimodal multilingual machine learning. In Proceedings of the 44th
InternationalACMSIGIRConferenceonResearchandDevelopmentinInformationRetrieval,SIGIR’21,
pages2443–2449,Virtual,2021.ACM.
[77] BartThomee,DavidA.Shamma,GeraldFriedland,BenjaminElizalde,KarlNi,DouglasPoland,Damian
Borth,andLi-JiaLi. YFCC100M:thenewdatainmultimediaresearch. CommunicationsoftheACM,59
(2):64–73,Jan2016.
[78] YonglongTian,DilipKrishnan,andPhillipIsola. Contrastivemultiviewcoding. InA.Vedaldi,H.Bischof,
T.Brox,andJ.-M.Frahm,editors,ComputerVision–ECCV2020,volume12353ofLNCS,pages776–794,
Online,23–28Aug2020.Springer.
[79] YonglongTian,OlivierJ.Hénaff,andAäronvandenOord. Divideandcontrast:Self-supervisedlearning
fromuncurateddata. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision
(ICCV),pages10063–10074,Virtual,11–17Oct2021.IEEE/CVF.
15[80] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,ArmandJoulin,Edouard
Grave,andGuillaumeLample. Llama:Openandefficientfoundationlanguagemodels. arXiv:2302.13971,
Feb2023.
[81] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov, SoumyaBatra, PrajjwalBhargava, ShrutiBhosale, etal. Llama2: Openfoundationand
fine-tunedchatmodels. arXiv:2307.09288,Jul2023.
[82] VishaalUdandarao,AmeyaPrabhu,AdhirajGhosh,YashSharma,PhilipH.S.Torr,AdelBibi,Samuel
Albanie,andMatthiasBethge. No"zero-shot"withoutexponentialdata:Pretrainingconceptfrequency
determinesmultimodalmodelperformance. arXiv:2404.04125,Apr2024.
[83] LaurensvanderMaatenandGeoffreyHinton. Visualizingdatausingt-SNE. JournalofMachineLearning
Research,9(86):2579–2605,2008.
[84] XudongWang,ZhirongWu,LongLian,andStellaX.Yu. Debiasedlearningfromnaturallyimbalanced
pseudo-labels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR),pages14647–14657,NewOrleans,LA,USA,19–24Jun2022.IEEE/CVF.
[85] PeterWelinder,SteveBranson,TakeshiMita,CatherineWah,FlorianSchroff,SergeBelongie,andPietro
Perona. Caltech-UCSDbirds200. TechnicalReportCNS-TR-201,Caltech,2010.
[86] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,AnthonyMoi,Pierric
Cistac,TimRault,RemiLouf,MorganFuntowicz,JoeDavison,SamShleifer,PatrickvonPlaten,Clara
Ma,YacineJernite,JulienPlu,CanwenXu,TevenLeScao,SylvainGugger,MariamaDrame,Quentin
Lhoest,andAlexanderRush. Transformers:State-of-the-artnaturallanguageprocessing. InQ.Liuand
D.Schlangen,editors,Proceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguage
Processing:SystemDemonstrations,pages38–45,Online,16–20Nov2020.ACL.
[87] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN database:
Large-scalescenerecognitionfromabbeytozoo. InIEEEConferenceonComputerVisionandPattern
Recognition,pages3485–3492,SanFrancisco,CA,USA,2010.IEEE.
[88] HuXu,SainingXie,XiaoqingEllenTan,Po-YaoHuang,RussellHowes,VasuSharma,Shang-WenLi,
GargiGhosh,LukeZettlemoyer,andChristophFeichtenhofer. DemystifyingCLIPdata. InTheTwelfth
InternationalConferenceonLearningRepresentations,Vienna,Austria,7–11May2024.
[89] MertYuksekgonul,FedericoBianchi,PratyushaKalluri,DanJurafsky,andJamesY.Zou. Whenandwhy
vision-languagemodelsbehavelikebags-of-words,andwhattodoaboutit? InTheEleventhInternational
ConferenceonLearningRepresentations,Kigali,Rwanda,1–5May2023.
[90] XiaohuaZhai,XiaoWang,BasilMustafa,AndreasSteiner,DanielKeysers,AlexanderKolesnikov,and
LucasBeyer. LiT:Zero-shottransferwithlocked-imagetexttuning. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition(CVPR),pages18123–18133,NewOrleans,LA,
USA,19–24Jun2022.IEEE/CVF.
[91] YuhaoZhang,HangJiang,YasuhideMiura,ChristopherD.Manning,andCurtisP.Langlotz. Contrastive
learning of medical visual representations from paired images and text. In Z. Lipton, R. Ranganath,
M.Sendak,M.Sjoding,andS.Yeung,editors,Proceedingsofthe7thMachineLearningforHealthcare
Conference,volume182ofPMLR,pages2–25,Durham,NC,USA,5–6Aug2022.PMLR.
[92] BoleiZhou,AgataLapedriza,AdityaKhosla,AudeOliva,andAntonioTorralba. Places:A10million
imagedatabaseforscenerecognition. IEEETransactionsonPatternAnalysisandMachineIntelligence,
40(6):1452–1464,2018.
[93] Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. BBN: Bilateral-branch network with
cumulativelearningforlong-tailedvisualrecognition. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition(CVPR),pages9719–9728,Virtual,14–19Jun2020.IEEE/CVF.
[94] XingyiZhou,RohitGirdhar,ArmandJoulin,PhilippKrähenbühl,andIshanMisra. Detectingtwenty-
thousandclassesusingimage-levelsupervision. InS.Avidan,G.Brostow,M.Cissé,G.M.Farinella,and
T.Hassner,editors,ComputerVision–ECCV2022,volume13669ofLNCS,pages350–368,TelAviv,
Israel,23–27Oct2022.Springer.
[95] BeierZhu,KaihuaTang,QianruSun,andHanwangZhang. Generalizedlogitadjustment:Calibratingfine-
tunedmodelsbyremovinglabelbiasinfoundationmodels.InA.Oh,T.Neumann,A.Globerson,K.Saenko,
M.Hardt,andS.Levine,editors,AdvancesinNeuralInformationProcessingSystems,volume36,pages
64663–64680,NewOrleans,LA,USA,10–16Dec2023.CurranAssociates,Inc.
16Generalization Beyond Data Imbalance:
A Controlled Study on CLIP for Transferable Insights
Supplementary Material
XinWen1 BingchenZhao2 YilunChen3 JiangmiaoPang3 XiaojuanQi1
1TheUniversityofHongKong 2UniversityofEdinburgh 3ShanghaiAILaboratory
{wenxin, xjqi}@eee.hku.hk pangjiangmiao@pjlab.org.cn
Contents
A Extendeddiscussions 18
A.1 Whatmakesagoodcorrelationindicatorforper-classstatistics? . . . . . . . . . . 18
A.2 Correlationstatisticsonbroadersetsofconcepts . . . . . . . . . . . . . . . . . . . 18
A.3 Distributionalconvergenceoflarge-scaleimage-textdatasets . . . . . . . . . . . . 19
A.4 Conceptfrequencyestimationcomparedtoconcurrentwork . . . . . . . . . . . . 19
A.5 IsdataimbalancenotaconcernforCLIP? . . . . . . . . . . . . . . . . . . . . . . 20
A.6 Motivationbehindthechoiceoffactorstostudy . . . . . . . . . . . . . . . . . . . 20
A.7 CanCLIPachieverobustgeneralizationtoextremelyrareconcepts? . . . . . . . . . 21
B Detailsaboutclassfrequencyestimation 21
B.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.2 Obtainingclassfrequencystatistics . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.3 Open-sourceCLIPmodels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C Detailsaboutthecontrolledstudy 22
C.1 Trainingdetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.2 DetailsabouttextformationinImageNet-Captions . . . . . . . . . . . . . . . . . 22
C.3 DetailsaboutvocabularysubsamplinginSL . . . . . . . . . . . . . . . . . . . . . 23
C.4 Detailsaboutmodels’heads . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.5 Detailsaboutimage-textdatasetvariants . . . . . . . . . . . . . . . . . . . . . . . 23
C.6 Evaluationsetting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
C.7 Computingresources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D DetailsaboutDINOexperiments 24
D.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.2 Trainingdetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.3 Transferlearningdetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
E Extendedresults 25
E.1 ExamplesofclassdistributionandCLIPperformance . . . . . . . . . . . . . . . . 25
E.2 ExtensionofFig.1bwithper-modelresults . . . . . . . . . . . . . . . . . . . . . 25
E.3 ExtensionofFig.3withlanguagepre-training . . . . . . . . . . . . . . . . . . . . 26
E.4 ExtendedvisualizationsofCLIP’smulti-modalfeaturespace . . . . . . . . . . . . 26
E.5 OriginalnumericdataofDINOtransferlearningresults . . . . . . . . . . . . . . . 27
E.6 Zoominginattheclassdistributions(linearscale) . . . . . . . . . . . . . . . . . . 27
171 SL Acc (IN-Caps) 2 CLIP Acc (IN-Caps) 3 SL #Pred (IN-Caps) 4 CLIP #Pred (IN-Caps)
100 Pearson's r=0.55 100 Pearson's r=0.27 140 Pearson's r=0.59 160 Pearson's r=0.05
Spearman's =0.52 Spearman's =0.28 Spearman's =0.57 Spearman's =0.08
120 140 80 80
100 120
60 60 80 100
80
40 40 60 60
40 40
20 20
20 20
0 0 0 0
0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000
Class frequency (linear scale) Class frequency (linear scale) Class frequency (linear scale) Class frequency (linear scale)
(a)Correlationstatisticsofmodelspre-trainedonImageNet-Captions.
1 SL Acc (LAIONet) 2 CLIP Acc (LAIONet) 3 SL #Pred (LAIONet) 4 CLIP #Pred (LAIONet)
100 Pearson's r (linear)=0.32 Pearson's r (linear)=0.27 500 Pearson's r (linear)=0.60 200 Pearson's r (linear)=0.26
Pearson's r (log)=0.58 Pearson's r (log)=0.50 Pearson's r (log)=0.62 Pearson's r (log)=0.34
80 S Sp pe ea ar rm ma an n' 's s ( (l li on ge )a =r 0)= .60 3.63 80 S Sp pe ea ar rm ma an n' 's s ( (l li on ge )a =r 0)= .50 2.52 400 S Sp pe ea ar rm ma an n' 's s ( (l li on ge )a =r 0)= .70 6.76 175 S Sp pe ea ar rm ma an n' 's s ( (l li on ge )a =r 0)= .30 2.32
150
60 60 300 125
100
40 40 200 75
20 20 100 50
25
0 0 0 0
100 101 102 103 104 105 100 101 102 103 104 105 100 101 102 103 104 105 100 101 102 103 104 105
Class frequency (log scale) Class frequency (log scale) Class frequency (log scale) Class frequency (log scale)
(b)Correlationstatisticsofmodelspre-trainedonLAIONet.
1 SL Acc (YFCC-15M) 2 CLIP Acc (YFCC-15M) 3 SL #Pred (YFCC-15M) 4 CLIP #Pred (YFCC-15M)
100 Pearson's r (linear)=0.20 100 Pearson's r (linear)=0.11 Pearson's r (linear)=0.91 Pearson's r (linear)=0.03
P Se pa er as ro mn a's n r 's ( lo g (l) in= e0 a.5 r)6 =0.64 P Se pa er as ro mn a's n r 's ( lo g (l) in= e0 a.3 r)8 =0.41 800 P Se pa er as ro mn a's n r 's ( lo g (l) in= e0 a.4 r)1 =0.82 250 P Se pa er as ro mn a's n r 's ( lo g (l) in= e0 a.0 r)8 =0.14
80 Spearman's (log)=0.64 80 Spearman's (log)=0.41 Spearman's (log)=0.82 Spearman's (log)=0.14
200
600
60 60 150
400
40 40 100
20 20 200 50
0 0 0 0
100 101 102 103 104 105 100 101 102 103 104 105 100 101 102 103 104 105 100 101 102 103 104 105
Class frequency (log scale) Class frequency (log scale) Class frequency (log scale) Class frequency (log scale)
(c)Correlationstatisticsofmodelspre-trainedonYFCC-15M.
Figure11: Whichisabetterindicatorforper-classstatistics? (a)ForlessimbalancedIN-Caps,both
Pearson’sr[63]andSpearman’sρ[75]canmodelthecorrelationbetweenstatisticswell. (b&c)For
extremelyimbalanceddatasets(e.g.,LAIONet,YFCC-15M,andotherwebdatasets),Peason’srmay
failevenifclassfrequenciesareprocessedtologscale. Incontrast,Spearman’sρremainsrobust.
A Extendeddiscussions
A.1 Whatmakesagoodcorrelationindicatorforper-classstatistics?
Per-classstatistics,especiallyclassfrequencydata,canbeofdifferentlevelsofimbalance. Agood
correlationindicatorshouldremainrobusttothechangesinimbalancelevelsandfaithfullyreflectthe
correlationbetweenstatistics. ThecommonlyusedPearsoncorrelationcoefficient[63](r)doesnot
fitthiscriterion. Weconsiderthreedatasetsinthisdiscussion: ImageNet-Captions,LAIONet,and
YFCC-15M,whichhaveincreasinglevelsofdataimbalance. AsshowninFig.11,Pearson’srcan
modelmoderateimbalancelikeImageNet-Captions,highimbalancelikeLAIONetifprocessingthe
frequenciestologscale,butcanfailifanextremeimbalanceismet(e.g.,Fig.11c.2). Incontrast,the
Spearmancorrelationcoefficient[75](ρ,definedasPearson’srappliedtodataranks)remainsrobust
acrossscenarios. WethustakeSpearman’sρasthedefaultcorrelationindicatorusedinthispaper.
A.2 Correlationstatisticsonbroadersetsofconcepts
Resultsinthemainpaperonlyconsiderthedistributionofconcepts/classesinImageNet-1K.Inthis
discussion,wealsoconsidertheconceptsetsofbroaderdatasets,includingCUB[85],Food-101[6],
18
)%
,1-pot(
ycarucca
teNegamI
)%
,1-pot(
ycarucca
teNegamI
)%
,1-pot(
ycarucca
teNegamI
)%
,1-pot(
ycarucca
teNegamI
)%
,1-pot(
ycarucca
teNegamI
)%
,1-pot(
ycarucca
teNegamI
ssalc
rep
noitciderP#
ssalc
rep
noitciderP#
ssalc
rep
noitciderP#
ssalc
rep
noitciderP#
ssalc
rep
noitciderP#
ssalc
rep
noitciderP#CC-12M YFCC-15M LAION-400M MetaCLIP-400M LAION-2B MetaCLIP-2.5B
a b
CUB CUB
Food101 Food101
OxfordPets OxfordPets
Flowers102 Flowers102
Places365 Places365
EuroSAT EuroSAT
DTD DTD
0.4 0.2 0.0 0.2 0.4 0.6 0.4 0.2 0.0 0.2 0.4
Correlation to per-class accuracy Correlation to per-class #prediction
Figure12: CorrelationstatisticsofCLIPevaluatedonbroadersetsofconcepts. Modelspre-trained
at scale (≥ 400M) remain robust on most datasets except fine-trained (e.g., CUB and Flowers)
anddomain-specificones(e.g.,EuroSAT).Thesedatamightberelativelyrareontheweborhave
significantgapswithotherdata,thushardtobenefitfromscalingorgeneralizationfromexistingdata.
Oxford-IIITPets[62],Flowers-102[56],Places365[92],EuroSAT[33],andDescribableTextures
(DTD)[15]. Pre-trainedCLIPmodels’correlationstatisticsontheseconceptsetsareasshownin
Fig. 12. Models pre-trained at scale (≥ 400M) remain robust on most datasets. However, some
fine-trained(e.g.,CUBandFlowers-102)anddomain-specific(e.g.,EuroSAT)datasetstendtobe
harder to learn and easier to bias. These data might be relatively rare on the web and can have
significant gaps with other data formats (satellite images are relatively uncommon), thus hard to
benefitfromscalingorgeneralizationfromexistingdata.
A.3 Distributionalconvergenceoflarge-scaleimage-textdatasets
Fig. 1a in the main paper has illustrated qualitatively that the class distributions of large-scale
image-textdatasetsareroughlyshared(correlated). Here,wealsoprovidequantitativeresultsabout
thecorrelationcoefficientsbetweentheclassdistributionofdifferentimage-textdatasetsFig.13.
Undermostconceptsets,thecorrelationishighandsupportsourclaim: thereexistsadistributional
convergenceacrosslarge-scaleimage-textdatasets. ResultsofMetaCLIP[88]variantsarerelatively
lesscorrelated,whichmightbeduetothere-balancingoperationinthecurationprocess.
A.4 Conceptfrequencyestimationcomparedtoconcurrentwork
Ourestimationofconceptfrequencyisbasedonasimplerule-basedpipeline(seedetailsinAppx.B.2),
whichcouldbepronetocaptionnoise,multi-label,andambiguity. AconcurrentworkbyParashar
etal.[61]findsconceptsynonymsusingChatGPT[57],andestimatestheclassfrequenciesofeach
captionusingLlama2[81]. Theseadvancedtechniquesmayproducemoreaccurateclassfrequencies.
InFig.14,weprovidethecorrelationcoefficientsbetweenourestimationsandtheresultsof[61].The
highcorrelationacrossmostdatasetsimpliesanagreementandverifiesthevalidityofourestimations.
ThereisanexceptionforDTD[15],inwhichclassnamesareaboutdescriptivetextures. Thisis
moreabstractthannaturalconceptsandcanbemoresemanticallyambiguous[61],andrequiremore
sophisticateddesignsinfrequencyestimation.
19a ImageNet b CUB c Food101 d OxfordPets
CC-12M 1 1 1 1
YFCC-15M 0.82 1 0.7 1 0.87 1 0.64 1
LAION-400M 0.930.78 1 0.740.85 1 0.880.85 1 0.790.81 1
MetaCLIP-400M 0.760.690.76 1 0.5 0.490.52 1 0.680.630.67 1 0.630.680.77 1
LAION-2B 0.930.770.990.76 1 0.740.850.970.52 1 0.890.860.990.66 1 0.820.850.94 0.8 1
MetaCLIP-2.5B 0.770.680.77 1 0.78 1 0.5 0.490.52 1 0.52 1 0.680.630.67 1 0.66 1 0.660.660.760.980.81 1
e Flowers102 f Places365 g EuroSAT h DTD
CC-12M 1 1 1 1
YFCC-15M 0.85 1 0.91 1 0.96 1 0.92 1
LAION-400M 0.890.95 1 0.970.91 1 1 0.96 1 0.910.84 1
MetaCLIP-400M 0.730.820.83 1 0.77 0.8 0.82 1 0.890.890.89 1 0.930.84 0.9 1
LAION-2B 0.9 0.950.990.82 1 0.970.91 1 0.81 1 1 0.96 1 0.89 1 0.920.85 1 0.91 1
MetaCLIP-2.5B 0.730.820.83 1 0.82 1 0.77 0.8 0.83 1 0.81 1 0.890.890.89 1 0.89 1 0.930.840.89 1 0.9 1
CC-1 Y2
FM
CC L- A1 I5 O
MM
N e- t4 a0 C0
LIM
P-400
LAM
I MO etN a-2
CLB IP-2.5B
CC-1 Y2
FM
CC L- A1 I5 O
MM
N e- t4 a0 C0
LIM
P-400
LAM
I MO etN a-2
CLB IP-2.5B
CC-1 Y2
FM
CC L- A1 I5 O
MM
N e- t4 a0 C0
LIM
P-400
LAM
I MO etN a-2
CLB IP-2.5B
CC-1 Y2
FM
CC L- A1 I5 O
MM
N e- t4 a0 C0
LIM
P-400
LAM
I MO etN a-2
CLB IP-2.5B
Figure 13: Correlation between class frequency statistics of different pre-training datasets under
differentconceptsets.Thereisaconvergenceofdatadistributionoverlarge-scaleimage-textdatasets.
a ImageNet b CUB c Food101 dFlowers102 e EuroSAT f DTD
LAION-400M 0.83 0.83 0.93 0.9 0.9 0.9 0.8 0.79 0.86 0.86 0.48 0.48
LAION-2B 0.83 0.84 0.9 0.91 0.91 0.92 0.8 0.79 0.88 0.88 0.5 0.5
LAION-400M LAION-2B LAION-400M LAION-2B LAION-400M LAION-2B LAION-400M LAION-2B LAION-400M LAION-2B LAION-400M LAION-2B
Ours Ours Ours Ours Ours Ours
Figure14: Correlationbetweenclassfrequencystatisticsofourestimationsandconcurrentresultsof
Parasharetal.[61]. ThereisanagreementonmostconceptsetsexceptDTD[15],whichisabout
descriptivetexturesandcanbemoresemanticallyambiguous[61].
A.5 IsdataimbalancenotaconcernforCLIP?
AsillustratedinFigs.1band5,thediscriminabilityandrobustnesstopre-trainingdataimbalance
improvesimultaneouslyasdatascalesup. ButneitherdoesitmeanCLIPisunbiased(seediscussions
inSec.3.7),nordoesitindicateCLIPisabsolutelyrobusttodataimbalance. InFig.15,weplot
binnedresultsofCLIPfollowingParasharetal.[61]. Lookingattheaveragetrend,thetailclasses
arestillofinferiorperformance. However,notethatthestandarddeviationishigh,indicatingthere
arestillmanygood-performingtailclasses. Moreover,thefigurealsoverifiesCLIPismorerobust
thanSL(Fig.15a),andtheharmofdataimbalancealleviatesasdatascalesup(Fig.15b).
A.6 Motivationbehindthechoiceoffactorstostudy
Thedesignofourstudyislargelyinfluencedby[26],whichisamongthefirsttostudydata’seffect
onCLIP’srobustness. Afterrulingouttheeffectsoflanguagesupervisionanddatadistribution,we
foundthereisstillanotablegapbetweenCLIPandSLinFig.3. Wethenexhaustedeveryfactorwe
couldtoaligndetailsbetweenmodels,andfinallyfoundthepretexttaskofdynamicclassificationto
beakeyfactor,whichcouldimplicitlyde-biasclassifiers,andreproduciblebyapplyingvocabulary
subsampling. Besides that, we also considered factors like vision backbone, vision pre-training,
andtest-timeprompts,whichdonothavenoticeableeffects. Additionally,wealsolookedintothe
propertiesofthedatasetinsteadofmodelsandfoundwebdatahavemixedeffects. Further,weextend
thescalinglawofCLIPandfindopen-worlddatatobeaneffectivefactor.
20
.la
te
rahsaraP
.la
te
rahsaraP
.la
te
rahsaraP
.la
te
rahsaraP
.la
te
rahsaraP
.la
te
rahsaraP1 SL Acc (YFCC-15M) 2 CLIP Acc (YFCC-15M) 3 SL #Pred (YFCC-15M) 4 CLIP #Pred (YFCC-15M)
100 100 600 100
80 80 500 80
400
60 60 60
300
40 40 40
200
20 20 20
100
0 0 0 0
Class frequency (binned, log scale) Class frequency (binned, log scale) Class frequency (binned, log scale) Class frequency (binned, log scale)
(a)Binnedstatisticsofmodelspre-trainedonYFCC-15M(avg±std).
1 CLIP Acc (LAION-400M) 2 CLIP Acc (LAION-2B) 3 CLIP #Pred (LAION-400M) 4 CLIP #Pred (LAION-2B)
100 100
100
80
80 80 70 80
60
60 60 60
50
40 40 40 40
30
20 20 20
20
0 0 10 0
0
Class frequency (binned, log scale) Class frequency (binned, log scale) Class frequency (binned, log scale) Class frequency (binned, log scale)
(b)Binnedstatisticsofmodelspre-trainedonLAION-400MandLAION-2B(avg±std).
Figure15: CLIPstillcanbebiasedbypre-trainingdata. ItisrelativelymorerobustthanSL(a),and
thebiasreducestosomeextentasdatascales(bvs.a),butthetailclassesstillunderperform.
A.7 CanCLIPachieverobustgeneralizationtoextremelyrareconcepts?
Weindeedobservemany.Forexample,among1KImageNetclasses,29classesappearinYFCC-15M
lessthan10times, 20classesappearlessthan5times, 6classesappear1time, and2classesdo
notappear. Withintheseclasses,CLIPtrainedaccordinglyfromscratchhas≥50%accuracyon12
classes. WeprovidedetailedstatisticsinTab.1.
Table1: ResultsofthetailclassesonYFCC-15M.
Freq. 9 9 8 6 5 5 5 5 5 4 4 4 4 3 3 3 2 2 2 2 2 1 1 1 1 1 1 0 0
Acc. 46 34 44 98 14 10 44 48 42 62 52 50 90 88 94 100 16 12 50 22 74 82 0 44 72 22 6 48 20
The names of last-5 classes are: “potter’s wheel”, “Sussex Spaniel”, “Curly-coated Retriever”,
“Kuvasz”,and“DandieDinmontTerrier”. Wenotethatalthoughourfrequencycalculationtriesto
maximizerecall(e.g.,matchesclassnamestocaptionsasbag-of-words,andusessynsets),theremay
stillbecasesmissedbyus. Nevertheless,theresultsverifyCLIPasagoodfew-shotlearner.
BesidesYFCC-15M,wealsoprovideexamplesofLAION-400MandMetaCLIP-400MinFig.17.
B Detailsaboutclassfrequencyestimation
B.1 Preliminaries
ContrastiveLanguage-ImagePre-training(CLIP).Takingpairedimage-captiondataasinput,the
pretexttaskisformulatedasacross-modalcontrastivelearningtaskthatdiscriminatesthepaired
textfromalargebatchofnegativesamples,andviceversa. Afterearlyexplorations[18,69,91],
emergentperformanceinrepresentationlearning,zero-shotevaluation,anddistributionalrobustness
wasachievedbyCLIP[65]andALIGN[35]throughtrainingondatasetsatunprecedentedscale.
Follow-upworksincludeBASIC[64],LiT[90],BLIP[42],SLIP[54],etc. Withoutlossofgenerality,
thisstudytakesaspecialinterestinCLIP.
Image-text datasets. Web-crawled image captioning data are typically formatted as image-text
pairs,whichcanbecrawledfromtheweb. Existingworksprovideawiderangeofoptionsacross
21
)%
,1-pot(
ycarucca
teNegamI
)%
,1-pot(
ycarucca
teNegamI
)%
,1-pot(
ycarucca
teNegamI
)%
,1-pot(
ycarucca
teNegamI
ssalc
rep
noitciderP#
ssalc
rep
noitciderP#
ssalc
rep
noitciderP#
ssalc
rep
noitciderP#scales, including MS-COCO [13], CC-3M [73] and 12M [11], YFCC-100M [77] and 15M [65],
WIT[76],SBU[59],RedCaps[19],LAION-400M[70]and2B/5B[71],andMetaCLIP[88],etc.
Thisworkconsidersthosewithbothmetadataandpre-trainedCLIPpubliclyavailable,i.e.,CC-12M,
YFCC-15M,LAION-400M/2B,andMetaCLIP-400M/2.5B.
B.2 Obtainingclassfrequencystatistics
ThisstudyspecificallyexaminestheclassesofImageNet[17],whichencompasses1Kcommonobject
categories.Toobtaintheclassdistributiononimage-textdatasets,wefollowthecommonpractice[26,
74,88]toquerycaptionswithclassnamesandtheirWordNet[53]synset. Inimplementation,wealso
loosenthesub-stringmatchingconditiontoset-levelmatching(overlookingtheorderofwords)fora
higherrecall,andmanuallyintroducednegativewords(e.g.,‘vehicle’,‘truck’forclass‘ram’,‘bird’,
and‘wing’forclass‘crane’)toreducefalsepositives. Besides,wenormalizeletterstolowercase,
removenon-letterandnon-numbersymbols,andlemmatizewordstonouns. ForMetaCLIP,which
providesareadilyavailabledistributionof500Kconcepts,wesimplysummedupthestatisticsof
targetconcepts(classes). Andforotherdatasets,weranthesearchoverallcaptioningdata.
B.3 Open-sourceCLIPmodels
ThemodelsarecollectedfromthemodelsofOpenCLIP[14]. Weselectmodelsthathavecaptionsor
metadataofthepre-trainingdatasetpublicallyavailable,andrestrictthebackbonestoResNet[32],
ConvNeXt[49],andViT[21]. Theremainingsetcomprises41modelscoveringdifferentmodel
architectures(6ResNets,8ConvNeXts,and27ViTs),modelscales(ResNet-50/101,ConvNeXt-
B/L/XL,andViT-B/L/H/G),datascales(from12Mto2.5B),trainingschedules,andoptimization
techniques. AnoverviewoftheresultsofthesemodelsisprovidedinFig.18.
C Detailsaboutthecontrolledstudy
C.1 Trainingdetails
Ourtrainingsettingsfollowthecommonpracticein[26],CLIPexperimentsutilizecross-entropy
lossesandtheAdamWoptimizer. Theinitiallearningrateissetto0.001,andacosine-annealing
learningrateschedulewith500warmupstepsisemployed. Thehyper-parametersforAdamWare
β = 0.9, β = 0.999, andϵ = 10−8. Thebatchsizeissetto1024. Modeltraininglastsfor32
1 2
epochs. Wealsotriedtraining90epochstomatchthatofSLbutfound32epochsisenoughfor
convergenceandlongertraininghasnonotablebenefit.
SLmodelsaretrainedusingSGDwithNesterovmomentumfor90epochs. Theweightdecayisset
to0.0001,momentumto0.9,andbatchsizeto256. Theinitiallearningrateis0.1andisdecayedby
0.1atepochs30,50,and70.
TomaximallyaligndetailswithCLIP,bothmethodsadopttheslightlymodifiedResNetstructureas
in[65].Theaugmentationpipelineisalsokeptconsistent:randomresizedcroptosize224withascale
rangeof(0.9,1.0),followedbynormalizationwithameanof(0.48145466,0.4578275,0.40821073),
andastandarddeviationof(0.26862954,0.26130258,0.27577711).Notethatthisdataaugmentation
pipelineisnotablyweakerthanthosecommonlyusedbySL.
C.2 DetailsabouttextformationinImageNet-Captions
For • template-based captions, the caption of an image is generated using a randomly sampled
templatefrom80classtemplatesprovidedin[65],e.g.,“aphotoofa[class]”. Ifsynsetsareused,
theclassname[class]isalsorandomlysampledfromitssynsets. For•natural-languagecaptions,
werefertoFig.2(upper)foranexampleimageandcorrespondingtextmetadata(includingtitle,
description,andtags). MoreexamplescanbefoundinFig. 3of[26]. Thewaycaptionsarecreated
issimplybyconcatenatingmetadatatogetherwithspacing. E.g.,ifthe[title]is“Aphonecall
andnight”,andthe[description]is“Imighthaveathingwithtelephones...”,thentheresulting
captionis[title description]:“AphonecallandnightImighthaveathingwithtelephones...”.
Thisfollowsthepracticeof[26],andisalsothewayCLIP[65]curatescaptiondatafromYFCC-15M.
22C.3 DetailsaboutvocabularysubsamplinginSL
Thetrainingvocabularyreferstothelabelsetthatamodelclassifiesataspecifictrainingiteration.
Givenamini-batchofsamples,aminimallabelsetisformedastheunionofallGTsinthismini-batch.
Iftheexpectedvocabularysizeisnotmet,weadditionallysampleclassesfromtheremaining,and
theprobabilityaclassisselectedisdeterminedbythefrequencyofthecorrespondingclassinthe
pre-training data. Note that the sampling is performed at the class level, which differs from the
samplingstrategiesinlong-taillearningthataredoneatthesamplelevel. Wealsotried uniform
sampling,i.e.,treatingeachclasswithequalprobability,whichyieldedslightlyweakerresults.
Discussions. For SL, vocabulary subsampling refers to randomly reducing the size of candidate
classes (akin to dropout on the classification head) when classifying an image during training.
1) Regarding how it works, Fig. 3a (y-axis) shows it effectively reduces the model’s predictions’
correlationtoclassfrequencies,akeyindicatorofclassifier’sbias. 2)Regardingwhythistechnique
can de-bias classifiers, our intuition is that this plays a similar role to dropout: the classifier is
regularizedtoputequalimportanceonallclasses. Biasesstillexistinthesubsampledclasses,butthe
gradientscancelouteachotherduringtraining. 3)Regardingwhyfrequency-basedsamplingworks
betterthandroppingallclasseswithequalprobability,wehypothesizethatthedroppingoperationcan
de-biastheclassifierregardlessofhowclassesareselected,andsamplingbyfrequencyismorehelpful
forrepresentationlearning. Theintuitioncomesfromthefindinginlong-taillearningthatresampling
databyinversefrequencyhelpsde-biasclassifier,butharmsrepresentationlearning[36,93].
C.4 Detailsaboutmodels’heads
ForCLIPexperiments,thetextencoderistrainedfromscratchbydefault. Ifthetextencoderuses
frozenCLIP,thismeansthetextencoderisinitializedbythepre-trainedCLIPweightsfrom[65].
During training, the parameters of the text encoder remain unchanged. In the CLIP init setting,
afterinitialization,thetextencoderisalsofine-tunedinthetrainingprocess. Further,forRoBERTa
experiments, wefollowtheimplementationof[14]andreplacethetextencoderwithpre-trained
RoBERTa [48] available on HuggingFace [86]. This is kept frozen during training, as we found
fine-tuningitresultsinNaNloss.
ForSLexperiments,wereplacethecommonlyusedlinearclassifierwithaprototypicalclassifierto
betterfollowCLIP’sstructure. Thismeansthebiasterminthislinearlayerisomitted,andboththe
featurefromthebackboneandtheclassifier’sweightareℓ -normalized,thusweightsinthelinear
2
layercanbeviewedasasetofprototypes(featurevectors). Tofacilitateoptimization,alearnable
scalerwithamaximumscaleof100isaddedasCLIP[65]toupscalelogits.Forthesettingusingfixed
prototypesobtainedfromCLIP,weformateachclasstoasentenceusingthetemplate“a[class]”,
feedthemtothetextencoderofapre-trainedCLIP,andkeeptheoutputclass-wisetextfeaturesasSL
model’sclassificationhead/prototypes.
C.5 Detailsaboutimage-textdatasetvariants
ImageNet-Captions subsets. Starting from the original ImageNet-Captions [26], we take only
image-textpairsthatcorrespondtothe100classesofTianetal.[78], thusobtaininga100-class
subsetcalledImageNet-Captions-100. Besides,werandomlysamplefromImageNet-Captionsand
constructasubsetthatisofthesamescaleasImageNet-Captions-100butwiththesamenumberof
classesasImageNet-Captions. ThissubsetiscalledImageNet-Captions(10%). Notethatitisofthe
samescaleofImageNet-Captions-100,andnotnecessarily10%ofImageNet-Captions.
LAIONet variants. LAIONet [74] is a subset of
LAION-400M [70] created by matching between 105 Subsample Ratio
100% 12.5%
ImageNet class synsets and captions. Items with
104 50% 6.25%
low CLIP text similarity between the caption and 25% 3.125%
103
classdefinitionarefilteredouttoreducelabelnoise.
Our reproduction sets 0.7 as the default threshold, 102
and3.26Mimagesaresuccessfullycrawled. Experi- 101
mentsinSec.3.4considerLAIONetvariantsfiltered
100
with different text-definition similarity thresholds:
0 200 400 600 800 1000
0.7,0.75,0.8,0.82,andthesizesofcorresponding Classes ranked by frequency of LAIONet (threshold=0.7)
LAION-400Msubsetsareoriginally3.26M,1.93M, Figure16: DistributionofLAIONetsubsets.
230.88M,and0.58M.WethenrandomlysubsamplethemtobethesamescaleasImageNet-Captions
(0.45M).Besides,thevariantthatmatchestheclassdistributionofImageNet-Captionsissampled
from the 3.26M version, and the scale is also kept the same as ImageNet-Captions. In addition,
experimentsinSec.3.5useLAIONetsubsetsrandomlysampledfromthe3.26Mversion(threshold
setto0.7),attheportionof1/1,1/2,1/4,1/8,1/16,and1/32,respectively. Thedistributionsofthese
randomlysampledsubsetsareshowninFig.16.
CC-12M-ClsandYFCC-15M-Cls. TheseareclassificationsubsetsofCC-12MandYFCC-15Mthat
havecorrespondingclasslabelsof1KImageNetclassesforeachimage. Thecurationprocessfollows
Fangetal.[26],exceptthatweallowclassnamematchestobenotcase-sensitive. Incomparisonto
LAIONet,itissimplysubstringmatchingwithoutfiltering. Theresultingdatasetsareatascaleof
3.48M(CC-12M-Cls)and2.90M(YFCC-15M-Cls),respectively.
C.6 Evaluationsetting
Unlessotherwisespecified,theevaluationofmodelsisallperformedonImageNetvalidationsplit.
ForCLIP,thedefaultzero-shotclassificationsettingisapplied. Thatis,eachclassisembeddedasan
averagevectoroftextfeaturesproducedusing80classtemplatesprovidedin[65]. Thenforboth
CLIPandSL,thepredictedclassisthatofthenearestneighborclassprototype.
C.7 Computingresources
ExperimentsareconductedonNVIDIAA100GPUs. EachCLIPandSLtrainingexperimentrunson
4GPUsinparallel,andthereareroughly400experiments(datapoints)forthecontrolledstudy.
D DetailsaboutDINOexperiments
D.1 Preliminaries
Self-supervisedlearningfrompseudo-labels. ItisnaturaltoextendSLtoself-supervisedsettings
forrepresentationlearning,aslongaspseudo-labelsareavailable. Earlierwork[7]appliesk-means
clusteringtodeepfeaturesandtakesclusterassignmentsaspseudo-labels. Followingworks[1,9]
reformpseudo-labelingasoptimaltransportandsolveitwiththeSinkhornKnoppalgorithm. Thisis
thensimplifiedbyDINO[10]withcenteringandsharpeningoperationsonthemodel’spredictions,
andextendedtosoftlabels(thuscalledself-distillationinsteadofself-labeling).
KnowledgeDIstillationwithNOlabels(DINO).DINO[10]isadiscriminativeself-supervised
visualpre-trainingmethod. Thepretexttaskisformulatedasself-distillation: enforcingthestudent
model’spredictionstobeclosetoteachermodels’softpseudolabels. Theinputtotwomodelsare
randomaugmentedviewsofthesameimage,andtheteachermodelisupdatedastheexponential
movingaverageofthestudentmodel(alsocalled“meanteacher”). DINOlearnsasetofprototypes
(featurevectors)astheclassificationhead,andisusedbystudentandteachermodelstoproduce
logitsandpseudolabels. Sincetheprototypesresembleaclassificationhead,theaforementioned
vocabularysubsamplingstrategycanalsobesimilarlyappliedtoDINO.
D.2 Trainingdetails
ThetrainingdetailsfollowthesuggestedpracticesofDINO[10]fortrainingResNets. Thatis,train
usingSGDoptimizerwithabaselearningrateof0.03,andfixedweightdecayof0.0001. Thescale
ofglobalcropsis(0.14,1),andthescaleoflocalcropsis(0.05,0.14). Otherhyper-parametersare
keptasdefault. WeusetheResNet-50backbonewiththesamestructureasRadfordetal.[65],and
trainfor100epochswithabatchsizeof1024.
ThelastlayerofDINO’sprojectionheadisequivalenttoasetofprototypes, thusitisnaturalto
integratethetechniquesexperimentedtobevalidonclassificationmodels. Wekeepthetotalnumber
ofprototypesto65536asdefault.
Forvocabularysampling-basedDINO,wesubsamplethesamesetofprototypesfortheteacherand
studentmodelsandcomputetheself-distillationlossonthisrestrictedprototypeset. Thevocabulary
(prototypeset)issharedinamini-batch,anddifferentacrosstrainingiterations.
24D.3 Transferlearningdetails
Datasetsandmetrics. Wetestmodels’transferlearningperformanceonthebenchmarkinitially
proposed in [38], and adopt the implementation from [22]. The datasets in this benchmark in-
clude: Food-101[6],CIFAR10/100[40],Birdsnap[4],SUN397[87],StanfordCars[39],FGVC
Aircraft[51],PASCALVOC2007[23],DescribableTextures(DTD)[15],Oxford-IIITPets[62],
Caltech-101[27],andFlowers-102[56]. Theevaluationmetricismostlytop-1accuracy,withexcep-
tionsofmeanper-classaccuracyonFGVCAircraft,Oxford-IIITPets,Caltech-101,andFlowers-102,
and11-pointmAPonPASCALVOC2007.
Linearprobing. Imagefeaturesareextractedfromthebackboneoftheteachermodelfollowing[10].
Thenfollowing[22],wetrainanℓ -regularizedmultinomiallogisticregressionclassifieronfrozen
2
featuresextractedfromthebackbone. ThemodelisoptimizedusingL-BFGSonthesoftmaxcross-
entropyobjective. Nodataaugmentationisapplied,andtheimagesareresizedto224pixelsalong
theshortsizeusingbicubicresamplingandcenter-croppedto224×224. Thehyper-parametersfor
ℓ -regularizationaresearchedfrom45logarithmicallyspacedvaluesbetween10−6and105.
2
E Extendedresults
E.1 ExamplesofclassdistributionandCLIPperformance
InFig.17,weprovideanexampleofthedistributionofsubsampledclassesandper-classzero-shot
accuracyofCLIP(ViT-B/32)pre-trainedon•LAION-400Mand■MetaCLIP-400Maccordingly.
Theheadclassesareeasytobefoundtheweb,e.g.,“T-shirt”,“mobilephone”,“throne”,and“goose”,
etc. In contrast, the tail classes are dominated by fine-trained biological concepts, ranging from
“barnspider”,“earthstarfungus”,to“gyromitra”. Collectingsuchdataishardandrequiresexpert
knowledge. Despitethis,wefindbothmodelscanachievegoodperformanceonsometailclasses.
107
LAION-400M MetaCLIP-400M 100
106
80
105
60
104
103 40
102
20
101
0
T-shir mt c oo br iln ee t phon te hrone goos te ool kit go sri hl il pa wreck whistl re e pd i srf oao t ax e p s dih wi s ip p ne dn os we r sha bd ae ssi sn te ut r Gg ee a rlo bn ma atr
n
o Ss es hg og rtn Gho li eag io or Gfn re ff
r
ei d os atP yh
'
eo si rt n sot Sy pe i wr t id se er sr r i EMe m nr o to lu Gn eink t bae a uniy t
c
n hS yeD c r
e
o h llSg n
o
hea ewnu nz n ge e oar fn r th d hu e en n d s wp oi m od u de d sr tu mr ut pl s ae thr ao s o mm wo on rke m s y s mn o ga o otk se h
s
eb an aa rre mtn ew h
r
t -s sp t wi iad r n e f gr eu dn g b bu u as t nt der efl d y ge gc yk ro omitra
Figure17: Examplesofthedistributionofsubsampledclasses(barplot),andper-classzero-shot
accuracy(lineplot)ofCLIP(ViT-B/32)pre-trainedaccordingly(•LAION-400Mand■MetaCLIP-
400M).Bothmodelsshowaweakcorrelationbetweenclassfrequencyandaccuracy.
E.2 ExtensionofFig.1bwithper-modelresults
InsupplementtotheanalysisinFig.1bwhereresultsofCLIPareaveragedbythedatasetittrains
on,weprovidedmoredetailedresultsofCLIPinFig.18. Besideszero-shotclassificationresultson
ImageNet[17],Fig.18alsoprovidesresultsevaluatedonImageNetV2[67]. Resultsareconsistent.
25
)tolprab
,M004-NOIAL
yb
deknar(
ycneuqerf
ssalc
niarterP
)tolpenil
,%
,1-pot(
ycarucca
tohs
orez
teNegamIImageNet ImageNetV2 Dataset
0.2 0.2 CC-12M
YFCC-15M
LAION-400M
MetaCLIP-400M
0.1 0.1 LAION-2B
MetaCLIP-2.5B
Backbone
ResNet
0.0 0.0
ConvNeXt
ViT
Acc (top-1, %)
32
48
0.0 0.2 0.4 0.0 0.2 0.4 72
Corr. (freq. vs. acc.) Corr. (freq. vs. acc.)
Figure18: Anoverviewofthecorrelationbetweenopen-sourceCLIPmodels’per-classaccuracy,
andpredictiondistributionwithpre-trainingdata’sclassfrequency. Theweakcorrelationtosample
frequencyisconsistentwhetherevaluatedonImageNet[17]orImageNetV2[67].
E.3 ExtensionofFig.3withlanguagepre-training
In supplementary to the analysis in Fig. 3, which is conducted under the setting that models are
trainedfromscratch. HerewealsoprovidetheresultsthatallmodelsaretrainedusingfrozenCLIP
textencoders/headsinFig.19. Wefindthattheresultsaregenerallyconsistentwiththoseinthemain
paper. Inaddition,wefindlanguagepre-trainingprovidesashortcuttomodelsandallowsthemto
leveragelanguagesupervision(CLIP)anddebiasedpretexttasks(SL)withhighereffectiveness. This
issupportedbythesharperslopesin(a,blueline)and(b,greenline)incomparisontoFig.3.
a b
Prediction Bias Perspective Overall Accuracy Perspective Model
0.6
0.5
better
small
oer
cabulary 56 50
better
C
C
SC
L
ylL
aI
nPI sP
s
sT
e
e
T
tx
e
Tt
m
e
T mpyp
l
pae
lt
aeS
ts
eL
s
v 50 Description Only
0.4
Title Only
45
Tags Only
0.3 40 Title + Description
Tags + Description
0.2 35 Title + Tags
Full Text
0.1 h dig eh sce rr i pte tix vt eness 23 50 SL 5Vo 0cabulary Si 5ze
00
100 1000
0.0 20 200
0.25 0.30 0.35 0.40 0.45 0.50 0.55 0.25 0.30 0.35 0.40 0.45 0.50 0.55
Correlation (freq. vs. acc.) Correlation (freq. vs. acc.)
Figure19: ResultsonIN-Capsaboutcaptiondiversityandvocabularysize. BothCLIPandSLuse
frozentextencoders/prototypesfrompre-trainedCLIP.ThetrendsaremostlyconsistentwithFig.3.
Inaddition,themodelsusing•template-basedsupervisionare(a)lessbiasedand(b)showbetter
accuracythanthetraining-from-scratchcounterpartsinFig.3,indicatingtheknowledgeinlanguage
pre-trainingtobeobtainedbyCLIP.ThisalsoholdstrueforSLand•naturallanguage-supervised
CLIP,assupportedbyshaperslopesin(a,blueline)and(b,greenline).
E.4 ExtendedvisualizationsofCLIP’smulti-modalfeaturespace
InsupplementofFig.7b,wealsoplotthevisionfeaturecentersandcorrespondingsamplefeatures
ofsomeclassesinFig.20. ResultsareproducedbyaCLIPViT-B/32modelpre-trainedonLAION-
400M,andobtainedbyinferencingontheImageNetvalidationsplit.Notethatvisionandtextfeatures
areplottedseparatelyduetothemodalitygap(despitebeinginthesamefeaturespace)[44]. Fig.20a
showsthefeaturesofimagesfromsomesubsampledclasses,andcorrespondingvisionfeaturecenters.
26
).derp#
.sv
.qerf(
.rroC
).derp#
.sv
.qerf(
noitalerroC
).derp#
.sv
.qerf(
.rroC
)%
,1-pot(
.cca
llarevOsamples centers
(a)CLIPvisionsamplesandcenters. (b)CLIPvisioncenters. (c)CLIPtextcenters.
Figure20: t-SNEvisualizationofsamplesencodedbyCLIPvision/textencodersinthemulti-modal
featurespace(onImageNetvalidationset). (a)ImagesencodedbyCLIPvisionencoder,andtheir
class-wisemeanfeatures. Classesaresubsampled. (b)VisionfeaturecentersofallImageNetclasses.
(c)ClasstemplatesencodedbyCLIPtextencoder,thesameasFig.7b. Visionandtextfeaturesare
plottedseparatelyduetothemodalitygap(despitebeinginthesamefeaturespace)[44].
IncoherencetoresultsinFig.7a.2,thereisnotacleartendencyonwhetherheadortailclassesform
compactorclusters. Inaddition,Fig.20bandFig.20cshowthevisionandtextfeaturecentersof
allImageNet-1Kclasses,whereheadandtailclassesarehighlighted. Thevisionfeaturecentersare
producedbyaveragingsamplefeaturesbyclasses,andthetextfeaturecentersareasoftheclassifier
usedbyCLIP,asdescribedinAppx.C.6. Themarginsbetweentailclassesencodedbythevision
encoderarenotablysmaller. Incontrast,tailclasscentersproducedbythetextencoderarebetter
separated. Thisphenomenonmightbeconnectedwiththemodalitygap[44],andisofresearchvalue
forfutureexplorations.
E.5 OriginalnumericdataofDINOtransferlearningresults
InTab.2,weprovidetheoriginalnumericdatausedtoobtainFig.10forreference.
Table2: LinearprobingevaluationresultsofDINOvariantspre-trainedonLAIONetfor100epochs.
ExtremedataimbalancemakesLAIONetmuchharderforDINOtolearntransferrablerepresentations,
andvocabularysubsamplingstrategyeffectivelyhelpsDINOovercomesuchdefects.
Dataset |Voc| Aircr Birds C101 Cars CF10 CF100 DTD Flower Food Pets SUN VOC Avg
ResultsofvanillaDINO
ImageNet 65536 27.0 37.1 82.3 23.6 86.4 62.9 68.7 80.8 55.8 66.4 57.0 81.6 60.8
LAIONet 16384 29.7 28.2 78.2 22.5 83.6 60.7 67.9 78.3 49.5 55.0 55.9 76.1 57.1
LAIONet 65536 26.7 24.6 77.8 24.6 83.5 60.0 67.1 79.3 48.6 55.5 55.4 77.3 56.7
ResultsofDINO+vocabularysampling(65536prototypesintotal)
LAIONet 1024 30.8 27.2 78.6 23.8 83.9 61.4 68.1 80.5 50.7 57.7 56.0 77.2 58.0
LAIONet 4096 30.3 30.1 78.9 24.8 84.6 63.4 69.5 77.7 53.3 61.0 56.9 78.6 59.1
LAIONet 16384 32.2 31.2 79.4 25.2 85.4 63.9 70.2 79.1 54.3 62.2 57.7 79.0 60.0
E.6 Zoominginattheclassdistributions(linearscale)
Toprovideaclearerimageoftheimbalancedclassdistributionofpre-trainingdatasets,weshow
a zoomed-in version of Fig. 1a with linear scale in Fig. 21. Also, we see that MetaCLIP does
successfullyalleviatethedominanceofheadclasses. Butnotethatunfortunately,alldatasetsarestill
extremelyimbalanced,andhowtoimprovemodels’robustnesstoitisstilltobeexplored.
27
daeH
liaT
daeH
liaT
daeH
liaT0.2 0.4 0.6 0.8 1.0 1.2 1.4 1 2 3 4 5 6 0.25 0.50 0.75 1.00 1.25 1.50 1.75
0
×105 ×106 ×107
25
50
75
100
125
150 CC-12M MetaCLIP-400M MetaCLIP-2.5B
175 YFCC-15M LAION-400M LAION-2B
200
1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 1 2 3 4 5 6 7 8 1 2 3 4 5 6
200
×104 ×105 ×106
225
250
275
300
325
350
375
400
0.6 0.8 1.0 1.2 1.4 1.6 1 2 3 4 5 6 1 2 3 4 5
400
×103 ×105 ×106
425
450
475
500
525
550
575
600
2 3 4 5 6 7 8 9 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.5 1.0 1.5 2.0 2.5 3.0
600
×103 ×105 ×106
625
650
675
700
725
750
775
800
0.5 1.0 1.5 2.0 2.5 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75
800
×103 ×105 ×106
825
850
875
900
925
950
975
1000
Figure21: Azoom-inversionofFig.1ashowingclassfrequencies(linearscale)rankedbyLAION-
400M.Animbalancedclassdistributionissharedacrossdatasets.
28