Aligning Multiclass Neural Network Classifier
Criterion with Task Performance via F -Score
β
NathanTsoi DeyuanLi TaesooDanielLee
YaleUniversity YaleUniversity YaleUniversity
nathan.tsoi@yale.edu deyuan.li@yale.edu taesoo.d.lee@yale.edu
MarynelVázquez
YaleUniversity
marynel.vazquez@yale.edu
Abstract
Multiclassneuralnetworkclassifiersaretypicallytrainedusingcross-entropyloss.
Followingtraining,theperformanceofthissameneuralnetworkisevaluatedusing
anapplication-specificmetricbasedonthemulticlassconfusionmatrix,suchas
the Macro F -Score. It is questionable whether the use of cross-entropy will
β
yield a classifier that aligns with the intended application-specific performance
criteria,particularlyinscenarioswherethereisaneedtoemphasizeoneaspectof
classifierperformance. Forexample,ifgreaterprecisionispreferredoverrecall,
theβ valueintheF evaluationmetriccanbeadjustedaccordingly,butthecross-
β
entropyobjectiveremainsunawareofthispreferenceduringtraining. Wepropose
amethodthataddressesthistraining-evaluationgapformulticlassneuralnetwork
classifiers such that users can train these models informed by the desired final
F -Score. Followingpriorworkinbinaryclassification,weutilizetheconcepts
β
of the soft-set confusion matrices and a piecewise-linear approximation of the
Heavisidestepfunction. Ourmethodextendsthe2×2binarysoft-setconfusion
matrixtoamulticlassd×dconfusionmatrixandproposesdynamicadaptation
of the threshold value τ, which parameterizes the piecewise-linear Heaviside
approximationduringrun-time. Wepresentatheoreticalanalysisthatshowsthat
ourmethodcanbeusedtooptimizeforasoft-setbasedapproximationofMacro-F
β
thatisaconsistentestimatorofMacro-F ,andourextensiveexperimentsshowthe
β
practicaleffectivenessofourapproach.
1 Introduction
Whentrainingmulticlassneuralnetworkclassifiers,thereisoftenmisalignmentbetweenthecriterion
used to train the network and the performance metric on which it is evaluated. In particular, the
performanceofaneuralnetworkclassifieristypicallyevaluatedusingametricsuchasF -Score,
1
whichbalancesbetweenprecisionandrecall,butthesamenetworkisoptimizedusingadifferent
criterion, such as cross-entropy. An ideal solution to bridge the gap between a neural network’s
trainingcriterionanditsevaluationmetricwouldinvolvedirectlyusingtheevaluationmetricasthe
trainingcriterion[3,5,8,17]. However,thisapproachisgenerallyimpracticalwhenoptimizinga
neuralnetworkviabackpropagation[17]. Thereasonisthatcommonevaluationmetricscomputed
from confusion-matrix values, like the F -Score, rely on the Heaviside step function. This step
1
functionhasgradientofzeroatallpointsexceptatthethresholdτ,wherethegradientisundefined.
Tobridgethedividebetweenthetrainingandevaluationofmulticlassneuralnetworkclassifiers,this
paperproposesanovelapproachtotrainingneuralnetworksformulticlassclassificationusingaclose
Preprint.Underreview.
4202
yaM
13
]GL.sc[
1v45902.5042:viXraapproximationoftheF -Score,whichgeneralizestheF -Scoreandothersuchscoresthatbalance
β 1
betweenprecisionandrecall.
Followingpriorworkinbinaryclassification[19],weutilizetheconceptsofthesoft-setconfusion
matricesandapiecewise-linearapproximationoftheHeavisidestepfunctioninourapproach. In
particular,ourmethodextendsthe2×2binarysoft-setconfusionmatrixfrom[19]toamultidimen-
sionald×dsoft-setconfusionmatrixandproposesdynamicadaptationofthethresholdvalueτ,
whichparameterizesthepiecewise-linearHeavisideapproximationduringrun-time. Wepresenta
theoreticalanalysisofourapproachshowingthat,inthelimit,ourmethodconvergestothetrueMacro
F -Score. Also,wepresentextensiveexperimentsonthepracticaleffectivenessofourapproach.
β
Insummary,ourmaincontributionsarethreefold: 1)anovelmethodfortrainingmulticlassclassifi-
cationneuralnetworksusinganapproximationoftheF -Score(Section4)asasurrogateloss;2)a
β
theoreticalanalysisofourapproach(Section5);and3)experimentalresultsondatasetswithvarying
levelsofclassimbalancethatshowcompetitiveperformancewithrespecttocross-entropylossas
wellastheabilitytooptimizeforaspecificclassificationmetricpreference,suchasforincreased
recall(Section6). Weprovideanopen-sourceimplementationofourmethodforreproducibilityand
tofacilitatefutureresearch.
2 Relatedwork
OurresearchisinspiredbyTsoietal.’swork[19]thatexploredtheoptimizationofconfusion-matrix-
basedmetricsforbinaryneuralnetworkclassifiers. Theirworkpresentstwoconceptsnecessaryto
optimizebinaryclassificationneuralnetworksusingtypicalmeasuressuchasF -ScoreandAccuracy.
1
First,theauthorsviewthevaluesofthebinaryconfusionmatrixprobabilisticallyandusesoftsets[12]
torepresenttheprobabilitythatasamplebelongstoagivenset. Second,theyproposeapiecewise-
linearapproximationoftheHeavisidestepfunction,similarto[16],withpropertiesthatmakethe
piecewise-linear approximation preferable to alternative approaches [10, 18]. In particular, for a
giventhresholdvalueτ andinputvalue0≤p≤1,theHeavisidefunctionis:
(cid:26)
1 p≥τ
H(p,τ)= (1)
0 p<τ
andthepiecewise-linearapproximationtotheHeavisidefunctionproposedin[19]is:

p·m
1
ifp<τ − τ 2m
Hl(p,τ)= p·m +(1−δ−m (τ + τm)) ifp>τ + τm (2)
3 3 2 2
p·m +(0.5−m τ) otherwise
2 2
whereτ
m
=min{τ,1−τ}andm
1
=δ/(τ−τ 2m),m
2
=(1−2δ)/τ m,m
3
=δ/(1−τ−τ 2m). The
piecewise-linearHeavisideapproximationdependsonaparameterδwhichparameterizestheslope
ofthelinearmid-sectionofthepiecewisefunction. Weuseδ =0.2assuggestedbyTsoietal.[19].
TheapproachproposedbyTsoietal.[19]isnotdirectlyapplicabletothemulticlassclassification
settingforseveralreasons. Forexample,ina2-classsettingwheretheprobabilityofmembershipin
oneclassisp,theprobabilityofmembershipintheotherclassmustbe1−p. Thisprobabilitypcan
informthechoiceofafixedthresholdτ forclassification[19].Inthecaseofd-dimensionalmulticlass
classification with d > 2, there is no natural threshold τ′ for which indices i with probabilities
p ≥τ′canalwaysbeconsideredasthetruepredictedclasswhiletheotherindicesareconsidered
i
otherwise. Indeed,ifτ′ > 1 andtheoutputprobabilitiespareuniformoveralldclasses,thenthe
d
inputexamplewouldnotbeassignedtooneoftheclasses. Ontheotherhand,ifτ′ ≤ 1 andthe
d
outputprobabilitiespareuniformoveralldclasses,thentheinputexamplewouldbeconsidereda
memberofalloftheclasses. Thisissuewillalwaysoccurifwetrytoenforcesomefixedthresholdτ′
forassigningmembershiptoaclass.
Beyondneuralnetworkclassifiers,otherclassificationtechniqueshavebeenextensivelyresearched.
For example, Support Vector Machines (SVMs) [1], clustering techniques such as k-means [7],
and NaiveBayes are commonapproaches. Once such a classifierhas been trained, the classifier
canbemadetobetteralignwithauser’sreal-worldobjectivebyadjustingthethresholdatwhich
outputsbelongtoagivenclassandthencomputingarelevantmetric. Adownsideofthisapproach
isthat, dependingontheapproach, manythresholdsmustbeevaluated. Forexample, SVMsare
2designedonlyforbinaryclassificationand,thus,mustbeadaptedtothemulticlasssettingusinga
one-versus-restorone-versus-oneapproach. Intheone-versus-oneapproach,givendclasses,upto
d(d−1)/2classifiersandanequalnumberofthresholdsarerequired.
Ourapproachusesadynamicthresholdattrainingtime,whichaddressesthischallengeassociated
with Tsoi et al.’s approach [19]. Moreover, dynamic thresholding avoids the need to perform a
two-stepapproach,whichiswhereanempiricallydeterminedthresholdisappliedasasecondstep,
afterobtainingaclassifier(e.g. aplug-inclassifier),suchasthoseanalyzedin[13].
3 PreliminariesAboutMulticlassClassification
Multiclassclassificationinvolvesdeterminingtheclassmembershipofagivendatasampleamong
twoormorepossibleclasses,whereeachclassismutuallyexclusive.Neuralnetwork-basedclassifiers
aretypicallyusedtopredictaprobabilitydistributionoverthepotentialclasses,whereeachdata
sampleisultimatelyassignedtooneandonlyoneclass. Formally,letsuchaclassifierhavedoutput
nodesz = [n ,...,n ]⊤ whereeachi-thvalueinz, for1 ≤ i ≤ d, correspondstothelikelihood
1 d
that the input example belongs to the i-th class. Typically, a softmax function is applied to z to
obtainaprobabilityvector,p. Thisvectorrepresentstheneuralnetwork’sbeliefthatagivenoutput
correspondstothetruelabel,wherethei-thcoordinateisp
i
=ezi/(cid:80)d j=1ezj. Theprobabilitiesp
i
arethenusedtotrainthemodelwithacross-entropylossandbackpropagation. Duringevaluation,
anexampleisassignedtothepredictedclassyˆH basedontheprobabilityoutputsmentionedabove,
andyˆH =argmax p . Thepredictedclassisfinallycomparedtotheground-truthlabelforthe
1≤i≤d i
exampletodeterminewhichpossibleconfusion-matrixentrythepredictionfallsinto,fromwhich
commonmetricscanbecomputedlikeF -Score.
β
Inamulticlasssettingwithdclasses,ad×dmulticlassconfusionmatrixcanbeconstructed,where
rowscorrespondtoeachofthetrueclassesandcolumnsarethepredictedclasses. Theentriesofthe
multiclassconfusionmatrixconsistof{c } ,wherethec entryequalsthenumberoftotal
ij 1≤i,j≤d ij
inputscorrespondingtotrueclassithatwereassignedapredictedlabelj bytheclassifier.
Themulticlassconfusionmatrixcanalsoberepresentedasacollectionof2×2binaryconfusion
matrices,withonematrixperclass. Forinstance,theentryinthebinaryconfusionmatrixforagiven
classk,wherekisthetrueclasslabel,is:
(cid:88) (cid:88) (cid:88)(cid:88)
|TP |=c |FN |= c |FP |= c |TN |= c . (3)
k kk k ki k ik k ij
i̸=k i̸=k i̸=kj̸=k
The entries of the class-specific confusion matrices are used to compute common classification
metricsperclass,fromwhichasummaryperformancestatisticcanbederived. FortheF -Score
β
specifically,itiscommontocombineresultswithmacro-averaging:firstcomputeindividualscoresper
class,andthenaveragetheresults. Forexample,forthek-thclass,letPrecision =|TP |/(|TP |+
k k k
|FP |)andRecall = |TP |/(|TP |+|FN |). Then,F -Scoreistheweightedharmonicmean
k k k k k β
ofprecisionandrecallforthatclass,withF β-Score
k
=(1+β2) β2P ·Pre rc ei cs ii so in ok nk·R +e Rca el clk allk,andthemacro-
averagedscoreforallclassesbecomes:
d
1(cid:88)
MacroF -Score= F -Score . (4)
β d β k
k=1
MacroF -Scorecannotbeuseddirectlyasalosstotrainneuralnetworksclassifiersviagradient
β
descentbecause,foranyneuralnetworkinput,thepredictedclassiscomputedviaargmax p .
1≤i≤d i
Thisoperationisnotusefulforbackpropagationinpbecauseitsgradientis0everywhereitisdefined.
Itisalsopossiblethatmultiplecoordinatesofpequalthemaximumprobability,inwhichcasethe
gradientisnotdefined. Ourproposedmethod,discussedinSection4,addressestheseissues.
4 Method
Inthissection,wepresentourapproachfortrainingmulticlassneuralnetworkclassifiersusinga
surrogatelossthatapproximatestheMacroF -Score,asinEquation(4). Ourapproachbuildson
β
Tsoietal.’swork[19],whointroducedamethodtoaddressthetraining-testinggapforbinaryneural
networkclassifiers,asdescribedinSection2.
3Ourapproachovercomesthetwokeylimitationsoftheirapproach:thefixedthresholdτ forcomputing
theentriesoftheconfusionmatrix,andthedependencyonasingle,binarysoft-setconfusionmatrix.
Thechoiceofathresholdvalueisnotimmediatelyobviousinamultidimensionalsetting,andwe
thereforeproposeanovelmethodofdynamicthresholding. Wealsogeneralizetheuseofsoftsetsto
computeamulticlassconfusionmatrixfromwhichmulticlassevaluationmetricscanbecomputed.
4.1 MulticlassApplicationofthePiecewise-linearHeavisideApproximation
AsinSection3,lettheprobabilityvectorpcorrespondtotheoutputofaneuralnetworkclassifier
afterthesoftmaxisapplied,withphavingdimensiond,thenumberofclassesintheclassification
problem of interest. Also, let yˆH = argmax p be the index of the class with the highest
1≤i≤d i
probability in p, and let yˆH be a one-hot-encoded vector representation of yˆH. We observe that
yˆH canbecomputedviatheHeavisidefunctionH atathresholdτ perEquation(1);ifweassign
τ to be between the two largest values of p, then yˆH = H(p,τ) = [H(p ,τ),...,H(p ,τ)]⊤.
1 d
Unfortunately,theHeavisidefunctionH isnotcontinuousatτ andhasagradientof0elsewhere,
makingitunsuitablefortrainingneuralnetworkclassifiersviabackpropagation.
Toaddresstheabovechallenge,weproposetoapproximationtoyˆH viathedifferentiablepiecewise-
linearHeavisideapproximation,Hl,proposedin[19]andusingathresholdτ valuewhichequalsthe
averageofthetwolargestvaluesofp. Notethatthisthresholdisdynamic,incontrastto[19],because
itdependsontheoutputofthenetwork. Specifically,lettheapproximationofyˆH bedenotedyˆH:
Hl(p,τ) (cid:20) Hl(p ,τ) Hl(p ,τ) (cid:21)⊤
yˆH = = 1 ,..., d . (5)
∥Hl(p,τ)∥ 1 (cid:80)d i=1Hl(p i,τ) (cid:80)d i=1Hl(p i,τ)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(yˆH)1 (yˆH)d
ThefunctionHl(p,τ)aboveresultsinacontinuousoutput,butnotaprobabilitydistributionbecause
thereisnoguaranteethat(cid:80)d Hl(p ,τ) = 1. Therefore,weproposeusingL normalizationin
i=1 i 1
thedenominatorofEquation(5)tonormalizeeachelementofHl(p,τ),effectivelyconvertingthe
entriesofourapproximationyˆHintoprobabilityvalues.
4.2 ComputingConfusionMatrixCardinalities
For any class 1 ≤ i ≤ d, the i-th entry (yˆH) of yˆH in Equation (5) can then be summed into
i
thesoft-setmulticlassconfusionmatrixforthepredictedlabeli. Class-wiseprobabilitiesarethen
summedintotheappropriateentriesofthed×dsoft-setconfusionmatrix.
Thecardinalityofeachentryofthe2×2confusionmatrixcorrespondingtoaclassk isdefined
in Equation (3). From these equations, we derive soft-set versions of evaluation metrics. To do
this,weapplythesameformulasfortheconfusionmatrixentriesbutreplacetheHeavisidefunction
with Hl, as shown in Equation 3. This makes the soft-set confusion matrix and derived soft-set
MacroF -Scorecontinuousanddifferentiableintheinput. Ourapproachisthensuitablefortraining
β
multiclassneuralnetworkclassifiersviabackpropagation.
5 TheoreticalGrounding
Priorworkhasshownhowasoft-setconfusionmatrixcanbeusedtotrainbinaryclassifiersthatbridge
the gap between binary classifier training losses and evaluation metrics [19]. Building upon this
priorwork,weprovideatheoreticalanalysisofourapproachtotrainneuralnetworksformulticlass
classification with an objective that approximates the Macro F -Score. Our analysis shows two
β
importantpropertiesforourapproach. First,theMacroF -Scorecomputedwiththesoft-setversion
β
oftheconfusionmatrixisLipschitzcontinuousintheoutputoftheneuralnetwork. Thisfacilitates
trainingwithstochasticgradientdescent. Second, undercertainassumptionsdetailedbelow, our
approximationoftheMacroF -Scorewithasoft-setconfusionmatrixhasasymptoticconvergence
β
tothetrueF -Scoreasthesizeofourdatasetapproachesinfinity.
β
45.1 MulticlassLipschitzContinuityofMetricsviaSoft-SetConfusionMatrix
Tsoi et al. [19] proved that each entry of the soft-set confusion matrix based on the Heaviside
approximationisLipschitzcontinuous. Wenowgeneralizethispropertytod-dimensionalmulticlass
classification,whered≥2isnotnecessarilyrestrictedtothebinarycase.
Theorem5.1. Ind-dimensionalmulticlassclassification,whered ≥ 2,everyentryofthesoft-set
confusionmatrixbasedonthepiecewise-linearHeavisideapproximationdescribedinSection4is
Lipschitzcontinuousintheoutputsoftheneuralnetwork.
Proof. ThisisaconsequenceofthefactthatHl(p,τ)isLipschitzcontinuousinpwithLipschitz
constantmax{m ,m ,m }[19]. Thenbyboundingourproposedmethod’sdynamicthresholdτ,
1 2 3
we achieve a Lipschitz constant that does not depend on the exact threshold τ used. Finally, by
expressingeachentryofthed×dsoft-setconfusionmatrixasacompositionofLipschitzcontinuous
functions,includingHl,weprovethattheelementsofthesoft-setconfusionmatrixareLipschitz
continuousintheoutputsp.
ThefullproofofTheorem5.1canbefoundintheSupplementaryMaterial.
WhiletheentriesoftheconfusionmatrixareLipschitzcontinuousintheoutputsoftheneuralnetwork,
theproposedsoft-setMacroF -ScoreisLipschitzcontinuousintheconfusionmatrixentries. Thus,
β
theproposedsoft-setMacroF -ScoreisanevaluationmetricthatisalsoLipschitzcontinuousin
β
theoutputsoftheneuralnetwork. Furthermore,manymulticlassclassificationmetricsderivedfrom
thed×dconfusionmatrix,includingF -Score,areLipschitzcontinuousintheconfusionmatrix
β
entries[2]andarethereforealsoLipschitzcontinuousintheoutputsoftheneuralnetwork.
Inourcaseofneuralnetworkclassification,whenoptimizedviastochasticgradientdescent,Lipschitz
continuityofalossfunctionintheoutputsoftheneuralnetworkensuresthattraininglossesdonot
varywildlyduringconvergencewhenoptimizingusingstochasticgradientdescent[19].
5.2 ApproximationofConfusion-MatrixBasedMetricswithSoftSets
Wenowprovideastatisticalandtheoreticalgroundingfortheclaimthatasthesizeofourdataset
approachesinfinity,MacroF -ScorecalculatedwithsoftsetsconvergestothetrueMacroF -Score
β β
under a set of assumptions. This theoretical analysis extends prior work [19] on the asymptotic
convergenceofmetricsbasedonconfusionmatrixvalueswithsoft-setsfrombinaryclassificationto
multiclassclassification. Unlikeinbinaryclassification,multiclassclassificationinddimensions
involvesdoutputnodes(asopposedtojustone). Theincreaseinoutputdimensionalityintroduces
complexitiesinourproofgivenmultipledegreesoffreedomandmorepossibleoutcomes.
Consider a training dataset of size n for a d-dimensional multiclass classifier, with examples
(x ,...,x ) and corresponding classes (y ,...,y ), respectively, so that each y ∈ {1,...,d}.
1 n 1 n i
Supposeforagiveninputexamplex thenetworkoutputsaprobabilitydistributionpforitsclass.
i
Typically,thepredictedlabelsarecalculatedbyapplyingtheHeavisidefunctiontopwhichresults
in a single predicted class yˆH for input example x , as described in Section 4. However, when
i i
using soft sets under our method, we instead replace the Heaviside function with the Heaviside
approximationHl andapplyL normalizationtogeneratesoftsetvaluesforourpredictionclass
1
yˆH =(yˆH,...,yˆH),whereyˆHdenotesthesoftsetmembershipofinputx assignedbytheneural
i i1 id ij i
networktoclassj. Hence,(cid:80)d yˆH =1.
j=1 ij
Foranyconstantβ >0,letF denotetheF -Scoreforclasskforbrevity,where1≤k ≤d. Asin
β,k β
Section3,
(1+β2)|TP |
F = k . (6)
β,k (1+β2)|TP |+|FP |+β2|FN |
k k k
Then,perEquation(4),MacroF -Scoreisdefinedas:
β
d
1 (cid:88)
Macro-F = F .
β d β,k
k=1
Foreachclassk, letq = 1 (cid:80)n 1{y = k}betheproportionofexampleswithtruelabelk in
k n i=1 i
ourdataset,sowemusthave(cid:80)d
q =1. Considerq tobefixed,sinceweassumeourdatasetis
k=1 k k
5sampledindependentlyatrandomfromapopulationwithsomefixedproportionforeachlabel. For
1≤i,j ≤d,supposethatthemulticlassclassifierclassifiesanyexamplewithtruelabeliintoclassj
withprobabilityp . Thenp correspondstotheprobabilitythatanexamplewithtruelabeliwill
ij ij
endupcontributingtothe(i,j)-thentryoftheconfusionmatrix.
Wemustalsohave(cid:80)d
p =1
j=1 ij
foreveryi,sinceeachexamplewillbeassignedtoexactlyoneclassbytheclassifier.
BecauseMacroF -ScoreiscalculatedwithdiscreteyˆH,weassumethattheclassifierwillclassifyx
β i i
asaCategoricalrandomvariableyˆH ∼Categorical(p ,...,p ). Inparticular,ifexamplex has
i yi1 yid i
truelabely =k,thenwehaveyˆH ∼Categorical(p ,...,p ).
i i k1 kd
However, in the soft set Macro F case, denoted as Macro Fs, the classifications can take on
β β
continuousvaluesinS ={v ∈[0,1]d :∥v∥ =1},whichcorrespondstoastandard(d−1)-simplex.
1
Thus,weconsiderthatyˆHisarandomvariabledrawnfromaDirichletdistribution,whichhassupport
i
S. WenotethatsinceallthemarginaldistributionsoftheDirichletdistributionareBetadistributions,
thisservesasageneralizationtotheBetadistributionusedbyTsoietal. [19]. Inparticular,assume
thatyˆH = (yˆH,...,yˆH) ∼ Dir(α ,...,α ). Thus, ifthetruelabelofexamplex isy = k,
i i1 id yi1 yid i i
thenwehavesoftsetclassificationsof(yˆH,...,yˆH)∼Dir(α ,...,α ). Forevery1≤i,j ≤d,
i1 id k1 kd
let
(cid:80)d
kα =i 1j
αik
=p ij,soforanyi,j,wehaveE[yˆ iH j]=
(cid:80)d
kα =y 1i αj
yik
=p yij.
Thenundertheaboveassumptions,bothMacroF andMacroFshavethesameaverageclassification
β β
correctness. In particular, for any given 1 ≤ i,j ≤ d, if y = k, then we have E[yˆH] = p =
i ij kj
P(yˆH =j).
i
Now,considerany1 ≤ k ≤ d. LetY = {i : y = k}bethesetofallexampleswhosetrueclass
k i
labelisk,so|Y |=nq .
k k
Thenweseethat
(1+β2)|TP |
F = k
β,k (1+β2)|TP |+|FP |+β2|FN |
k k k
(1+β2)(cid:80) 1{yˆH =k} (7)
= i∈Yk i .
(1+β2)(cid:80) 1{yˆH =k}+(cid:80) 1{yˆH =k}+β2(cid:80) 1{yˆH ̸=k}
i∈Yk i i∈/Yk i i∈Yk i
Ifforeachj weletU =(cid:80) 1{yˆH =k}∼Binomial(nq ,p )denotethenumberofexamples
j i∈Yj i j jk
withtruelabelj butwithpredictionlabelk,thenpluggingthisingivesus
(1+β2)U
F β,k = (1+β2)U +(cid:80) U +k β2(nq −U ). (8)
k j̸=k j k k
BytheStrongLawofLargeNumbers,weknowthat 1 U →a.s. p foreachj,meaning Uj →a.s. p q
nqj j jk n jk j
convergeswithprobability1asn→∞. ThenbytheContinuousMappingTheorem,asn→∞,
(1+β2)U (1+β2)U /n
F β,k = (1+β2)U +(cid:80) U +k β2(nq −U ) = (1+β2)U /n+(cid:80) U /k n+β2(q −U /n)
k j̸=k j k k k j̸=k j k k
(1+β2)p q (1+β2)p q
→a.s. (1+β2)p kkq k+(cid:80) j̸=kp jkk qk j+k β2(q k−p kkq k) = β2q k+(cid:80)d j=k 1k p jk kq j.
(9)
ApplyingtheContinuousMappingTheoremonemoretimeyields
Macro-F =
1(cid:88)d
F →a.s.
1(cid:88)d (1+β2)p kkq
k . (10)
β d β,k d β2q +(cid:80)d p q
k=1 k=1 k j=1 jk j
Ontheotherhand,forFs ,notethat
β,k
Fs = (1+β2)|TP k| = (1+β2)(cid:80) i∈Ykyˆ iH k .
β,k (1+β2)|TP |+|FP |+β2|FN | (1+β2)(cid:80) yˆH+(cid:80) yˆH+β2(cid:80) (1−yˆH)
k k k i∈Yk ik i∈/Yk ik i∈Yk ik
(11)
Foreachj,letUs =(cid:80) yH denotethetotalmembershipofexampleswithtrueclassj assigned
j i∈Yj ik
bytheclassifiertoclassk. Thenforanyi ∈ Y , weseethat(yˆH,...,yˆH) ∼ Dir(α ,...,α ),
j i1 id j1 jd
6meaningE[yˆH]= αjk =p . ThenbytheStrongLawofLargeNumbers,weknowthat
ik (cid:80)d t=1αjt jk
1 Us = 1 (cid:88) yˆH→a.s.p , (12)
nq j nq ik jk
j j
i∈Yj
soUs/n→a.s.
p q . ApplyingtheContinuousMappingTheorem,wethereforeseethat
j jk j
(1+β2)(cid:80) yˆH
Fs = i∈Yk ik
β,k (1+β2)(cid:80) yˆH+(cid:80) yˆH+β2(cid:80) (1−yˆH)
i∈Yk ik i∈/Yk ik i∈Yk ik
(1+β2)Us/n
= (1+β2)Us/n+(cid:80) Us/k n+β2(q −Us/n)
k j̸=k j k k (13)
(1+β2)p q
→a.s.
(1+β2)p q +(cid:80) p
k qk +k
β2(q −p q )
kk k j̸=k jk j k kk k
(1+β2)p q
= kk k .
β2q +(cid:80)d p q
k j=1 jk j
Similartothediscretecase,applyingtheContinuousMappingTheoremonceagainyields
Macro-Fs =
1(cid:88)d
Fs →a.s.
1(cid:88)d (1+β2)p kkq
k . (14)
β d β,k d β2q +(cid:80)d p q
k=1 k=1 k j=1 jk j
Thus,Macro-F andMacro-Fsbothconvergea.s.tothesamevalueasn→∞.BytheBoundedCon-
β β
vergenceTheorem,itfollowsthatE[Macro-F ],E[Macro-Fs]→a.s. 1(cid:80)d (1+β2)pkkqk . Similar
β β d k=1 β2qk+(cid:80)d j=1pjkqj
to the theoretical results from Tsoi et al. [19], it follows that, though not unbiased for finite n,
Macro-FsisaconsistentandasymptoticallyunbiasedestimatorofMacro-F asn→∞.
β β
6 Experiments
Our method allows for training-time optimization of multiclass neural network classifiers on an
approximationoftheF -Score. Ourexperimentsshowhowourmethodcanoptimizeforapreference
β
towardsprecisionorrecallbyvaryingtheper-classβvalueintheF -Score. Thisapproachtotraining
β
isparticularlyusefulinreal-worldscenarios. Forexample,wherethereisahighcostassociatedwith
misseddetections,onemaywanttoprioritizerecalloverprecision. Wealsopresentresultsshowing
thatourmulticlassclassificationmethodcanbedirectlyappliedto2-classproblems(equivalentto
binaryclassification)andoutperformsthepriormethodproposedbyTsoietal.[19].
6.1 Protocol
Wetrainneuralnetworksforeachdatasetusingourproposedmethodtooptimizeforanapproximation
ofF -Scorewithdifferentweightingtowardsprecisionforoneparticularclassorrecallforthesame
β
class. Wecomparetobaselinetrainingusingthesamenetworkarchitectureandtrainingregimebut
withF -ScoreusingourmethodandthetypicalCross-Entropyloss. Eachdatasetwassegmented
β
intotraining,validation,andtestingsplits. Uniformnetworkarchitecturesandtrainingprotocols
wereappliedwhereverpossible. TheAdamWoptimizer[11]wasusedfortrainingalongwithfast
early stopping [15] when 100 epochs elapsed without decreasing validation set loss. Given the
potential impact of hyperparameters on classifier performance, we performed a hyperparameter
grid search for each dataset and loss function combination. We chose the hyperparameters that
minimizedvalidation-setlossandthenperformed10trialsthatvariedtheneuralnetworkrandom
weightinitialization. Wethencalculateandreportthemeanandstandarddeviationoftheresults
acrossalltrials. SeetheSupplementaryMaterialSection??fordetails.
6.2 TrainingHardwareandSoftwareVersions
TrainingsystemswereequippedwithavarietyofNVIDIAgeneral-purposecomputingongraphics
processingunits(GPGPUs)includingTitanX,TitanV,RTXA4000,RTX6000,RTX2080tiand
7Table1: Modelstrainedtotrade-offbetweenprecisionandrecallfortheDogclass. TheFP training
β
criterionprefersprecisionfortheDogclassandtheFR trainingcriterionprefersrecallfortheDog
β
class. Results are reported for Precision and Recall on the Dog class. We also report the Macro
F -Score,whichisF -Scoreaveragedoverallclasses. Boldindicatesbetterperformancethanthe
1 1
CEbaseline.
CIFAR-10(µ±σ) Caltech256(µ±σ)
Loss Precision(Dog) Recall(Dog) MacroF -Score Precision(Dog) Recall(Dog) MacroF -Score
1 1
FP∗ 0.831±0.03 0.503±0.03 0.763±0.01 0.078±0.10 0.046±0.04 0.364±0.01
β
FR∗ 0.469±0.03 0.810±0.03 0.755±0.01 0.049±0.03 0.185±0.13 0.370±0.02
β
CE 0.655±0.07 0.684±0.06 0.746±0.01 0.059±0.10 0.054±0.08 0.326±0.01
RTX 3090ti. Systems hosting these GPGPUs had between 32 and 256GB of system RAM and
between12and38CPUcores. WeusedPytorch2.2.1withCUDA12.1runinsideaDockercontainer
forconsistencyacrosstrainingmachines.
6.3 Datasets
WeconductedexperimentsusingthemutliclassCIFAR-10[9]datasetandCaltech256[6]datasets.
TheCIFAR-10datasetconsistsof10classesandisevenlybalanced. TheCaltech256dataset,with
256objectclasses,hasarelativelybalancedsetofclasseswithaShannon’sEquitabilityIndex[14]
of0.87. Wealsoconductedexperimentsonthefourbinarydatasetsproposedin[19]whichshow
theperformanceofourmethodondatasetsofdifferentlevelsofclassimbalanceforbinarydatasets.
The CocktailParty dataset considers social group membership and has a 30.29% positive class
balance [22]. The Adult dataset is composed of salary data which has a 23.93% positive class
balance [4]. The Mammography dataset consists of data on microcalcifications and has a 2.32%
positiveclassbalance[21]. TheKaggleCreditCardFraudDetectiondatasethasa0.17%positive
classbalance[20].
6.4 Results
Considerauserthatisconcernedwithclassifierperformanceforaparticularclass. Forexample,in
ourexperiments,wechosetheDogclass,whichwaspresentinallmulticlassdatasets. Then,using
ourproposedmethod,itispossibletotraintheclassifiertopreferprecisionfortheparticularDog
classortopreferrecall.Usingourmethod,theneuralnetworklearnstooutputlabelscorrespondingto
increasedprecisionorrecallasdirectedduringtrainingwhilemaintaininganoverallMacroF -Score
1
whichstilloutperformsthebaseline,cross-entropyloss,asshowninTable1. Networkstrainedto
prefer precision for the Dog class are shown on the FP∗ line, where we used a value of β = 5.
β
Alternatively,networkstrainedtopreferrecallfortheDogclassareshownontheFR∗line,where
β
β =0.25,
We tested the proposed multiclass classification method on 4 of the binary datasets proposed by
[19]andreportresultsinTable2. TheauthorsoftheBridgingtheGap(BtG)approach[19]used
averysmallfeedforwardneuralnetworkconsistingofthreefullyconnectedlayersof32units,16
units,and1unit. Weusedaslightlylargernetworkarchitecturewhichhadfourlayersofunitsizes
{512,256,128,d}. Wealsoperformedamoreextensivehyperparametersearch, describedinthe
SupplementaryMaterialSection??. Anincreaseinthenumberofparametersinourneuralnetwork
combinedwiththemoreextensivehyperparametersearchresultedinanincreaseinthebaseline(CE)
performance. However,thetrendisthesame,whichisthatourproposedmethodperformssimilarly
orbetterthantheCEbaselinewereportonline(3)ofTable2.
7 Broaderimpactandethics
Ourworkhasthepotentialtoallowthetrainingofmulticlassclassificationneuralnetworkstobetter
alignwithadesiredF -Score. Multiclassclassificationisacommonprobleminmachinelearning
β
andbetteralignmentwithametricofinterestcouldhaveanimpactonclassificationproblemsina
wide-rangeofdomains. Forexample,ourresultsincludedatasetsusedforsocialsignalprocessing
8Table 2: Models trained on 4 binary classification datasets and evaluated on F -Score. Losses
1
(rows)areF *,whichisourproposedmethodusingthepiecewise-linearHeavisideapproximation
1
andabaselinetrainedusingcross-entropy(CE)loss. Hyperparametergridsearchforeachmodel
wasperformedandthen10xmodelsweretrainedusingthebesthyperparameterschosenviafast
early-stoppingonthebestvalidation-splitloss. Boldindicatesbestperformingmodelperdataset.
Loss CocktailParty(µ±σ) Adult(µ±σ) Mammography(µ±σ) Kaggle(µ±σ)
F1* 0.727±0.01 0.251±0.09 0.731±0.03 0.779±0.03
CE 0.730±0.01 0.146±0.01 0.642±0.05 0.294±0.01
andmedicalresearch. Importantly,whilebetteralignmentofaclassifierwithadesiredmetriccan
have a positive impact on these application domains, machine learning methods should be built
carefullyandusedinathoughtfulmanner. Inparticular,weurgepractitionerstoconsiderthecases
withintheirapplicationdomainwhereimprovingclassifieralignmenttoreal-worldobjectivesmay
haveunintendedsideeffects.
8 Limitations
Ourworkonaligningmulticlassneuralnetworkclassifierswithapplication-specificF -Scoresdoes
β
havesomelimitations. Ourexperimentalresultsshowedthatinmanycasesourmethodoutperforms
the baseline; however, training neural networks is a complex and nuanced process. We tested
optimizingforalimitedsetofperformancemetricsusingourmethod,onalimitednumberofdatasets.
Futureworkshouldexploremoremetricsanddatasets.Ourtheoreticalanalysisshowsthatourmethod
allows neural networks to be trained to better align with a user’s objective by optimizing for an
approximationofaparticularF -Scoremetricusingaconfusionmatrixbasedonsoft-sets. However,
β
thetheoreticalresultsrelyoncertainassumptions,suchasMacroF andMacroFsachievingthe
β β
same average classification correctness. In the future, we consider exploring how our proposed
methodcouldbegeneralizedtoothermulticlassmetricsthatarebasedonthed×dsoft-setconfusion
matrixvalues. Thiswouldallowuserstobetteraligntheirobjectivewithneuralnetworktraining
whentheirobjectivemaynotbeimmediatelyexpressedusingMacroF -Score.
β
9 Conclusion
Ourresearchaddressesacommonandoftenoverlookedgapbetweenmulticlassclassificationneural
network optimization criteron and the F -Score, a common metric on which these networks are
β
evaluated. Keycomponentsofourmethodarethechoiceofadynamicthreshold(τ)basedonthe
dataattrainingtimeandtheuseofamulticlasssoft-setconfusionmatrix. Thetheoreticalanalysis
showsthatourmethodallowsthetrainingofneuralnetworksthataremorecloselyalignedwiththe
evaluationmetricthanwhenothercriteria, suchascross-entropy, areused. Experimentalresults
showtheimprovedperformanceofourmethodonseveralmulticlassclassificationdatasetsusingour
methodversusthetypicalcross-entropylossasabaseline. Overall,ourworkoffersacontribution
tothefieldofmachinelearningthatcouldenablepractitionerstodevelopclassifiersthatarehigher
performingthanusingthestandardcross-entropyloss. Wearealsoexcitedtoseethatourmethod
applieswelltobinaryclassification,makingitvaluableacrossavarietyofclassificationproblems.
10 Acknowledgements
ThisworkwassupportedbytheNationalScienceFoundation(NSF),GrantNo. (IIS-1924802). The
findingsandconclusionsinthisarticlearethoseoftheauthorsanddonotnecessarilyreflectthe
viewsoftheNSF.
References
[1] CorinnaCortesandVladimirVapnik. Support-vectornetworks. Machinelearning,20:273–297,
1995.
9[2] KrzysztofDembczyn´ski,WojciechKotłowski,OluwasanmiKoyejo,andNagarajanNatarajan.
Consistencyanalysisforbinaryclassificationrevisited. InInternationalConferenceonMachine
Learning,pages961–969.PMLR,2017.
[3] Priya Donti, Brandon Amos, and J Zico Kolter. Task-based end-to-end model learning in
stochasticoptimization. Advancesinneuralinformationprocessingsystems,30,2017.
[4] DheeruDuaandCaseyGraff. UCImachinelearningrepository,2017.
[5] EladEban,MarianoSchain,AlanMackey,ArielGordon,RyanRifkin,andGalElidan. Scalable
learningofnon-decomposableobjectives.InArtificialintelligenceandstatistics,pages832–840.
PMLR,2017.
[6] GregoryGriffin,AlexHolub,andPietroPerona. Caltech256,Apr2022.
[7] JohnAHartiganandManchekAWong. Algorithmas136: Ak-meansclusteringalgorithm.
Journaloftheroyalstatisticalsociety.seriesc(appliedstatistics),28(1):100–108,1979.
[8] Alan Herschtal and Bhavani Raskutti. Optimising area under the roc curve using gradient
descent. InICML,page49,2004.
[9] AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiplelayersoffeaturesfromtinyimages.
Master’sthesis,DepartmentofComputerScience,UniversityofToronto,2009.
[10] Nikolay Kyurkchiev and Svetoslav Markov. Sigmoid functions: some approximation and
modellingaspects. LAPLAMBERTAcademicPublishing,Saarbrucken,2015.
[11] IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. arXiv:1711.05101,
2017.
[12] DmitriyMolodtsov. Softsettheory—firstresults. CAMWA,1999.
[13] HarikrishnaNarasimhan,RohitVaish,andShivaniAgarwal. Onthestatisticalconsistencyof
plug-inclassifiersfornon-decomposableperformancemeasures. InNeurIPS,pages1493–1501,
2014.
[14] Evelyn C Pielou. The measurement of diversity in different types of biological collections.
Journaloftheoreticalbiology,13:131–144,1966.
[15] LutzPrechelt. Earlystopping-butwhen? InNeuralNetworks: Tricksofthetrade,pages55–69.
Springer,2002.
[16] RyanRiegel,AlexanderGray,FrancoisLuus,NaweedKhan,NdivhuwoMakondo,IsmailYunus
Akhalwaya, HaifengQian, RonaldFagin, FranciscoBarahona, UditSharma, etal. Logical
neuralnetworks. arXivpreprintarXiv:2006.13155,2020.
[17] YangSong,AlexanderSchwing,RaquelUrtasun,etal. Trainingdeepneuralnetworksviadirect
lossminimization. InICML,pages2169–2177,2016.
[18] HyungJuSuh,MaxSimchowitz,KaiqingZhang,andRussTedrake.Dodifferentiablesimulators
givebetterpolicygradients? InInternationalConferenceonMachineLearning,pages20668–
20696.PMLR,2022.
[19] NathanTsoi,KateCandon,DeyuanLi,YoftiMilkessa,andMarynelVázquez. Bridgingthe
gap: Unifyingthetrainingandevaluationofneuralnetworkbinaryclassifiers. Advancesin
NeuralInformationProcessingSystems,35:23121–23134,2022.
[20] Machine Learning Group UBL. Credit card fraud detection. https://www.kaggle.com/
mlg-ulb/creditcardfraud. Accessed: 2022.
[21] Kevin S Woods, Jeffrey L Solka, Carey E Priebe, Chris C Doss, Kevin W Bowyer, and
LaurencePClarke. Comparativeevaluationofpatternrecognitiontechniquesfordetectionof
microcalcifications. InBiomedicalImageProcessingandBiomedicalVisualization,1993.
[22] GloriaZen,BrunoLepri,ElisaRicci,andOswaldLanz. Spacespeaks: towardssociallyand
personalityawarevisualsurveillance. InProceedingsofthe1stACMinternationalworkshop
onMultimodalpervasivevideoanalysis,pages37–42,2010.
10