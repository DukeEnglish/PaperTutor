[
    {
        "title": "Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis",
        "authors": "Chaoyou FuYuhan DaiYondong LuoLei LiShuhuai RenRenrui ZhangZihan WangChenyu ZhouYunhang ShenMengdan ZhangPeixian ChenYanwei LiShaohui LinSirui ZhaoKe LiTong XuXiawu ZhengEnhong ChenRongrong JiXing Sun",
        "links": "http://arxiv.org/abs/2405.21075v1",
        "entry_id": "http://arxiv.org/abs/2405.21075v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21075v1",
        "summary": "In the quest for artificial general intelligence, Multi-modal Large Language\nModels (MLLMs) have emerged as a focal point in recent advancements. However,\nthe predominant focus remains on developing their capabilities in static image\nunderstanding. The potential of MLLMs in processing sequential visual data is\nstill insufficiently explored, highlighting the absence of a comprehensive,\nhigh-quality assessment of their performance. In this paper, we introduce\nVideo-MME, the first-ever full-spectrum, Multi-Modal Evaluation benchmark of\nMLLMs in Video analysis. Our work distinguishes from existing benchmarks\nthrough four key features: 1) Diversity in video types, spanning 6 primary\nvisual domains with 30 subfields to ensure broad scenario generalizability; 2)\nDuration in temporal dimension, encompassing both short-, medium-, and\nlong-term videos, ranging from 11 seconds to 1 hour, for robust contextual\ndynamics; 3) Breadth in data modalities, integrating multi-modal inputs besides\nvideo frames, including subtitles and audios, to unveil the all-round\ncapabilities of MLLMs; 4) Quality in annotations, utilizing rigorous manual\nlabeling by expert annotators to facilitate precise and reliable model\nassessment. 900 videos with a total of 256 hours are manually selected and\nannotated by repeatedly viewing all the video content, resulting in 2,700\nquestion-answer pairs. With Video-MME, we extensively evaluate various\nstate-of-the-art MLLMs, including GPT-4 series and Gemini 1.5 Pro, as well as\nopen-source image models like InternVL-Chat-V1.5 and video models like\nLLaVA-NeXT-Video. Our experiments reveal that Gemini 1.5 Pro is the\nbest-performing commercial model, significantly outperforming the open-source\nmodels. Our dataset along with these findings underscores the need for further\nimprovements in handling longer sequences and multi-modal data. Project Page:\nhttps://video-mme.github.io",
        "updated": "2024-05-31 17:59:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21075v1"
    },
    {
        "title": "Generalization Beyond Data Imbalance: A Controlled Study on CLIP for Transferable Insights",
        "authors": "Xin WenBingchen ZhaoYilun ChenJiangmiao PangXiaojuan Qi",
        "links": "http://arxiv.org/abs/2405.21070v1",
        "entry_id": "http://arxiv.org/abs/2405.21070v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21070v1",
        "summary": "Severe data imbalance naturally exists among web-scale vision-language\ndatasets. Despite this, we find CLIP pre-trained thereupon exhibits notable\nrobustness to the data imbalance compared to supervised learning, and\ndemonstrates significant effectiveness in learning generalizable\nrepresentations. With an aim to investigate the reasons behind this finding, we\nconduct controlled experiments to study various underlying factors, and reveal\nthat CLIP's pretext task forms a dynamic classification problem wherein only a\nsubset of classes is present in training. This isolates the bias from dominant\nclasses and implicitly balances the learning signal. Furthermore, the\nrobustness and discriminability of CLIP improve with more descriptive language\nsupervision, larger data scale, and broader open-world concepts, which are\ninaccessible to supervised learning. Our study not only uncovers the mechanisms\nbehind CLIP's generalizability beyond data imbalance but also provides\ntransferable insights for the research community. The findings are validated in\nboth supervised and self-supervised learning, enabling models trained on\nimbalanced data to achieve CLIP-level performance on diverse recognition tasks.\nCode will be available at: https://github.com/CVMI-Lab/clip-beyond-tail.",
        "updated": "2024-05-31 17:57:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21070v1"
    },
    {
        "title": "Code Pretraining Improves Entity Tracking Abilities of Language Models",
        "authors": "Najoung KimSebastian SchusterShubham Toshniwal",
        "links": "http://arxiv.org/abs/2405.21068v1",
        "entry_id": "http://arxiv.org/abs/2405.21068v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21068v1",
        "summary": "Recent work has provided indirect evidence that pretraining language models\non code improves the ability of models to track state changes of discourse\nentities expressed in natural language. In this work, we systematically test\nthis claim by comparing pairs of language models on their entity tracking\nperformance. Critically, the pairs consist of base models and models trained on\ntop of these base models with additional code data. We extend this analysis to\nadditionally examine the effect of math training, another highly structured\ndata type, and alignment tuning, an important step for enhancing the usability\nof models. We find clear evidence that models additionally trained on large\namounts of code outperform the base models. On the other hand, we find no\nconsistent benefit of additional math training or alignment tuning across\nvarious model families.",
        "updated": "2024-05-31 17:56:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21068v1"
    },
    {
        "title": "Grammar-Aligned Decoding",
        "authors": "Kanghee ParkJiayu WangTaylor Berg-KirkpatrickNadia PolikarpovaLoris D'Antoni",
        "links": "http://arxiv.org/abs/2405.21047v1",
        "entry_id": "http://arxiv.org/abs/2405.21047v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21047v1",
        "summary": "Large Language Models (LLMs) struggle with reliably generating highly\nstructured outputs, such as program code, mathematical formulas, or well-formed\nmarkup. Constrained decoding approaches mitigate this problem by greedily\nrestricting what tokens an LLM can output at each step to guarantee that the\noutput matches a given constraint. Specifically, in grammar-constrained\ndecoding (GCD), the LLM's output must follow a given grammar. In this paper we\ndemonstrate that GCD techniques (and in general constrained decoding\ntechniques) can distort the LLM's distribution, leading to outputs that are\ngrammatical but appear with likelihoods that are not proportional to the ones\ngiven by the LLM, and so ultimately are low-quality. We call the problem of\naligning sampling with a grammar constraint, grammar-aligned decoding (GAD),\nand propose adaptive sampling with approximate expected futures (ASAp), a\ndecoding algorithm that guarantees the output to be grammatical while provably\nproducing outputs that match the conditional probability of the LLM's\ndistribution conditioned on the given grammar constraint. Our algorithm uses\nprior sample outputs to soundly overapproximate the future grammaticality of\ndifferent output prefixes. Our evaluation on code generation and structured NLP\ntasks shows how ASAp often produces outputs with higher likelihood (according\nto the LLM's distribution) than existing GCD techniques, while still enforcing\nthe desired grammatical constraints.",
        "updated": "2024-05-31 17:39:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21047v1"
    },
    {
        "title": "Exploratory Preference Optimization: Harnessing Implicit Q*-Approximation for Sample-Efficient RLHF",
        "authors": "Tengyang XieDylan J. FosterAkshay KrishnamurthyCorby RossetAhmed AwadallahAlexander Rakhlin",
        "links": "http://arxiv.org/abs/2405.21046v1",
        "entry_id": "http://arxiv.org/abs/2405.21046v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21046v1",
        "summary": "Reinforcement learning from human feedback (RLHF) has emerged as a central\ntool for language model alignment. We consider online exploration in RLHF,\nwhich exploits interactive access to human or AI feedback by deliberately\nencouraging the model to produce diverse, maximally informative responses. By\nallowing RLHF to confidently stray from the pre-trained model, online\nexploration offers the possibility of novel, potentially super-human\ncapabilities, but its full potential as a paradigm for language model training\nhas yet to be realized, owing to computational and statistical bottlenecks in\ndirectly adapting existing reinforcement learning techniques. We propose a new\nalgorithm for online exploration in RLHF, Exploratory Preference Optimization\n(XPO), which is simple and practical -- a one-line change to (online) Direct\nPreference Optimization (DPO; Rafailov et al., 2023) -- yet enjoys the\nstrongest known provable guarantees and promising empirical performance. XPO\naugments the DPO objective with a novel and principled exploration bonus,\nempowering the algorithm to explore outside the support of the initial model\nand human feedback data. In theory, we show that XPO is provably\nsample-efficient and converges to a near-optimal language model policy under\nnatural exploration conditions, irrespective of whether the initial model has\ngood coverage. Our analysis, which builds on the observation that DPO\nimplicitly performs a form of $Q^{\\star}$-approximation (or, Bellman error\nminimization), combines previously disparate techniques from language modeling\nand theoretical reinforcement learning in a serendipitous fashion through the\nperspective of KL-regularized Markov decision processes. Empirically, we find\nthat XPO is more sample-efficient than non-exploratory DPO variants in a\npreliminary evaluation.",
        "updated": "2024-05-31 17:39:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21046v1"
    }
]