[
    {
        "title": "Generalization Beyond Data Imbalance: A Controlled Study on CLIP for Transferable Insights",
        "authors": "Xin WenBingchen ZhaoYilun ChenJiangmiao PangXiaojuan Qi",
        "links": "http://arxiv.org/abs/2405.21070v1",
        "entry_id": "http://arxiv.org/abs/2405.21070v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21070v1",
        "summary": "Severe data imbalance naturally exists among web-scale vision-language\ndatasets. Despite this, we find CLIP pre-trained thereupon exhibits notable\nrobustness to the data imbalance compared to supervised learning, and\ndemonstrates significant effectiveness in learning generalizable\nrepresentations. With an aim to investigate the reasons behind this finding, we\nconduct controlled experiments to study various underlying factors, and reveal\nthat CLIP's pretext task forms a dynamic classification problem wherein only a\nsubset of classes is present in training. This isolates the bias from dominant\nclasses and implicitly balances the learning signal. Furthermore, the\nrobustness and discriminability of CLIP improve with more descriptive language\nsupervision, larger data scale, and broader open-world concepts, which are\ninaccessible to supervised learning. Our study not only uncovers the mechanisms\nbehind CLIP's generalizability beyond data imbalance but also provides\ntransferable insights for the research community. The findings are validated in\nboth supervised and self-supervised learning, enabling models trained on\nimbalanced data to achieve CLIP-level performance on diverse recognition tasks.\nCode will be available at: https://github.com/CVMI-Lab/clip-beyond-tail.",
        "updated": "2024-05-31 17:57:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21070v1"
    },
    {
        "title": "Recurrent neural networks: vanishing and exploding gradients are not the end of the story",
        "authors": "Nicolas ZucchetAntonio Orvieto",
        "links": "http://arxiv.org/abs/2405.21064v1",
        "entry_id": "http://arxiv.org/abs/2405.21064v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21064v1",
        "summary": "Recurrent neural networks (RNNs) notoriously struggle to learn long-term\nmemories, primarily due to vanishing and exploding gradients. The recent\nsuccess of state-space models (SSMs), a subclass of RNNs, to overcome such\ndifficulties challenges our theoretical understanding. In this paper, we delve\ninto the optimization challenges of RNNs and discover that, as the memory of a\nnetwork increases, changes in its parameters result in increasingly large\noutput variations, making gradient-based learning highly sensitive, even\nwithout exploding gradients. Our analysis further reveals the importance of the\nelement-wise recurrence design pattern combined with careful parametrizations\nin mitigating this effect. This feature is present in SSMs, as well as in other\narchitectures, such as LSTMs. Overall, our insights provide a new explanation\nfor some of the difficulties in gradient-based learning of RNNs and why some\narchitectures perform better than others.",
        "updated": "2024-05-31 17:53:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21064v1"
    },
    {
        "title": "Neural Network Verification with Branch-and-Bound for General Nonlinearities",
        "authors": "Zhouxing ShiQirui JinZico KolterSuman JanaCho-Jui HsiehHuan Zhang",
        "links": "http://arxiv.org/abs/2405.21063v1",
        "entry_id": "http://arxiv.org/abs/2405.21063v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21063v1",
        "summary": "Branch-and-bound (BaB) is among the most effective methods for neural network\n(NN) verification. However, existing works on BaB have mostly focused on NNs\nwith piecewise linear activations, especially ReLU networks. In this paper, we\ndevelop a general framework, named GenBaB, to conduct BaB for general\nnonlinearities in general computational graphs based on linear bound\npropagation. To decide which neuron to branch, we design a new branching\nheuristic which leverages linear bounds as shortcuts to efficiently estimate\nthe potential improvement after branching. To decide nontrivial branching\npoints for general nonlinear functions, we propose to optimize branching points\noffline, which can be efficiently leveraged during verification with a lookup\ntable. We demonstrate the effectiveness of our GenBaB on verifying a wide range\nof NNs, including networks with activation functions such as Sigmoid, Tanh,\nSine and GeLU, as well as networks involving multi-dimensional nonlinear\noperations such as multiplications in LSTMs and Vision Transformers. Our\nframework also allows the verification of general nonlinear computation graphs\nand enables verification applications beyond simple neural networks,\nparticularly for AC Optimal Power Flow (ACOPF). GenBaB is part of the latest\n$\\alpha,\\!\\beta$-CROWN, the winner of the 4th International Verification of\nNeural Networks Competition (VNN-COMP 2023).",
        "updated": "2024-05-31 17:51:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21063v1"
    },
    {
        "title": "Graph External Attention Enhanced Transformer",
        "authors": "Jianqing LiangMin ChenJiye Liang",
        "links": "http://arxiv.org/abs/2405.21061v1",
        "entry_id": "http://arxiv.org/abs/2405.21061v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21061v1",
        "summary": "The Transformer architecture has recently gained considerable attention in\nthe field of graph representation learning, as it naturally overcomes several\nlimitations of Graph Neural Networks (GNNs) with customized attention\nmechanisms or positional and structural encodings. Despite making some\nprogress, existing works tend to overlook external information of graphs,\nspecifically the correlation between graphs. Intuitively, graphs with similar\nstructures should have similar representations. Therefore, we propose Graph\nExternal Attention (GEA) -- a novel attention mechanism that leverages multiple\nexternal node/edge key-value units to capture inter-graph correlations\nimplicitly. On this basis, we design an effective architecture called Graph\nExternal Attention Enhanced Transformer (GEAET), which integrates local\nstructure and global interaction information for more comprehensive graph\nrepresentations. Extensive experiments on benchmark datasets demonstrate that\nGEAET achieves state-of-the-art empirical performance. The source code is\navailable for reproducibility at: https://github.com/icm1018/GEAET.",
        "updated": "2024-05-31 17:50:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21061v1"
    },
    {
        "title": "Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality",
        "authors": "Tri DaoAlbert Gu",
        "links": "http://arxiv.org/abs/2405.21060v1",
        "entry_id": "http://arxiv.org/abs/2405.21060v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21060v1",
        "summary": "While Transformers have been the main architecture behind deep learning's\nsuccess in language modeling, state-space models (SSMs) such as Mamba have\nrecently been shown to match or outperform Transformers at small to medium\nscale. We show that these families of models are actually quite closely\nrelated, and develop a rich framework of theoretical connections between SSMs\nand variants of attention, connected through various decompositions of a\nwell-studied class of structured semiseparable matrices. Our state space\nduality (SSD) framework allows us to design a new architecture (Mamba-2) whose\ncore layer is an a refinement of Mamba's selective SSM that is 2-8X faster,\nwhile continuing to be competitive with Transformers on language modeling.",
        "updated": "2024-05-31 17:50:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21060v1"
    }
]