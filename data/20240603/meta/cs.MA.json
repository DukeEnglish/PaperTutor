[
    {
        "title": "Fusion-PSRO: Nash Policy Fusion for Policy Space Response Oracles",
        "authors": "Jiesong LianYucong HuangMingzhi WangChengdong MaYixue HaoYing WenYaodong Yang",
        "links": "http://arxiv.org/abs/2405.21027v1",
        "entry_id": "http://arxiv.org/abs/2405.21027v1",
        "pdf_url": "http://arxiv.org/pdf/2405.21027v1",
        "summary": "For solving zero-sum games involving non-transitivity, a common approach is\nto maintain population policies to approximate the Nash Equilibrium (NE).\nPrevious research has shown that the Policy Space Response Oracle (PSRO) is an\neffective multi-agent reinforcement learning framework for these games.\nHowever, repeatedly training new policies from scratch to approximate the Best\nResponse (BR) to opponents' mixed policies at each iteration is inefficient and\ncostly. While some PSRO methods initialize a new BR policy by inheriting from\npast BR policies, this approach limits the exploration of new policies,\nespecially against challenging opponents.To address this issue, we propose\nFusion-PSRO, which uses model fusion to initialize the policy for better\napproximation to BR. With Top-k probabilities from NE, we select high-quality\nbase policies and fuse them into a new BR policy through model averaging. This\napproach allows the initialized policy to incorporate multiple expert policies,\nmaking it easier to handle difficult opponents compared to inheriting or\ninitializing from scratch. Additionally, our method only modifies the policy\ninitialization, enabling its application to nearly all PSRO variants without\nadditional training overhead.Our experiments with non-transitive matrix games,\nLeduc poker, and the more complex Liars Dice demonstrate that Fusion-PSRO\nenhances the performance of nearly all PSRO variants, achieving lower\nexploitability.",
        "updated": "2024-05-31 17:16:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.21027v1"
    },
    {
        "title": "Congestion-Aware Path Re-routing Strategy for Dense Urban Airspace",
        "authors": "Sajid Ahamed M APrathyush P MenonDebasish Ghose",
        "links": "http://arxiv.org/abs/2405.20972v1",
        "entry_id": "http://arxiv.org/abs/2405.20972v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20972v1",
        "summary": "Existing UAS Traffic Management (UTM) frameworks designate preplanned flight\npaths to uncrewed aircraft systems (UAS), enabling the UAS to deliver payloads.\nHowever, with increasing delivery demand between the source-destination pairs\nin the urban airspace, UAS will likely experience considerable congestion on\nthe nominal paths. We propose a rule-based congestion mitigation strategy that\nimproves UAS safety and airspace utilization in congested traffic streams. The\nstrategy relies on nominal path information from the UTM and positional\ninformation of other UAS in the vicinity. Following the strategy, UAS opts for\nalternative local paths in the unoccupied airspace surrounding the nominal path\nand avoids congested regions. The strategy results in UAS traffic exploring and\nspreading to alternative adjacent routes on encountering congestion. The paper\npresents queuing models to estimate the expected traffic spread for varying\nstochastic delivery demand at the source, thus helping to reserve the airspace\naround the nominal path beforehand to accommodate any foreseen congestion.\nSimulations are presented to validate the queuing results in the presence of\nstatic obstacles and intersecting UAS streams.",
        "updated": "2024-05-31 16:20:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20972v1"
    },
    {
        "title": "Paying to Do Better: Games with Payments between Learning Agents",
        "authors": "Yoav KolumbusJoe HalpernÉva Tardos",
        "links": "http://arxiv.org/abs/2405.20880v1",
        "entry_id": "http://arxiv.org/abs/2405.20880v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20880v1",
        "summary": "In repeated games, such as auctions, players typically use learning\nalgorithms to choose their actions. The use of such autonomous learning agents\nhas become widespread on online platforms. In this paper, we explore the impact\nof players incorporating monetary transfers into their agents' algorithms,\naiming to incentivize behavior in their favor. Our focus is on understanding\nwhen players have incentives to make use of monetary transfers, how these\npayments affect learning dynamics, and what the implications are for welfare\nand its distribution among the players. We propose a simple game-theoretic\nmodel to capture such scenarios. Our results on general games show that in a\nbroad class of games, players benefit from letting their learning agents make\npayments to other learners during the game dynamics, and that in many cases,\nthis kind of behavior improves welfare for all players. Our results on first-\nand second-price auctions show that in equilibria of the ``payment policy\ngame,'' the agents' dynamics can reach strong collusive outcomes with low\nrevenue for the auctioneer. These results highlight a challenge for mechanism\ndesign in systems where automated learning agents can benefit from interacting\nwith their peers outside the boundaries of the mechanism.",
        "updated": "2024-05-31 14:55:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20880v1"
    },
    {
        "title": "Optimally Improving Cooperative Learning in a Social Setting",
        "authors": "Shahrzad HaddadanCheng XinJie Gao",
        "links": "http://arxiv.org/abs/2405.20808v1",
        "entry_id": "http://arxiv.org/abs/2405.20808v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20808v1",
        "summary": "We consider a cooperative learning scenario where a collection of networked\nagents with individually owned classifiers dynamically update their\npredictions, for the same classification task, through communication or\nobservations of each other's predictions. Clearly if highly influential\nvertices use erroneous classifiers, there will be a negative effect on the\naccuracy of all the agents in the network. We ask the following question: how\ncan we optimally fix the prediction of a few classifiers so as maximize the\noverall accuracy in the entire network. To this end we consider an aggregate\nand an egalitarian objective function. We show a polynomial time algorithm for\noptimizing the aggregate objective function, and show that optimizing the\negalitarian objective function is NP-hard. Furthermore, we develop\napproximation algorithms for the egalitarian improvement. The performance of\nall of our algorithms are guaranteed by mathematical analysis and backed by\nexperiments on synthetic and real data.",
        "updated": "2024-05-31 14:07:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20808v1"
    },
    {
        "title": "No-Regret Learning for Fair Multi-Agent Social Welfare Optimization",
        "authors": "Mengxiao ZhangRamiro Deo-Campo VuongHaipeng Luo",
        "links": "http://arxiv.org/abs/2405.20678v1",
        "entry_id": "http://arxiv.org/abs/2405.20678v1",
        "pdf_url": "http://arxiv.org/pdf/2405.20678v1",
        "summary": "We consider the problem of online multi-agent Nash social welfare (NSW)\nmaximization. While previous works of Hossain et al. [2021], Jones et al.\n[2023] study similar problems in stochastic multi-agent multi-armed bandits and\nshow that $\\sqrt{T}$-regret is possible after $T$ rounds, their fairness\nmeasure is the product of all agents' rewards, instead of their NSW (that is,\ntheir geometric mean). Given the fundamental role of NSW in the fairness\nliterature, it is more than natural to ask whether no-regret fair learning with\nNSW as the objective is possible. In this work, we provide a complete answer to\nthis question in various settings. Specifically, in stochastic $N$-agent\n$K$-armed bandits, we develop an algorithm with\n$\\widetilde{\\mathcal{O}}\\left(K^{\\frac{2}{N}}T^{\\frac{N-1}{N}}\\right)$ regret\nand prove that the dependence on $T$ is tight, making it a sharp contrast to\nthe $\\sqrt{T}$-regret bounds of Hossain et al. [2021], Jones et al. [2023]. We\nthen consider a more challenging version of the problem with adversarial\nrewards. Somewhat surprisingly, despite NSW being a concave function, we prove\nthat no algorithm can achieve sublinear regret. To circumvent such negative\nresults, we further consider a setting with full-information feedback and\ndesign two algorithms with $\\sqrt{T}$-regret: the first one has no dependence\non $N$ at all and is applicable to not just NSW but a broad class of welfare\nfunctions, while the second one has better dependence on $K$ and is preferable\nwhen $N$ is small. Finally, we also show that logarithmic regret is possible\nwhenever there exists one agent who is indifferent about different arms.",
        "updated": "2024-05-31 08:21:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.20678v1"
    }
]