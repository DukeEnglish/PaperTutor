[
    {
        "title": "LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts",
        "authors": "Anh-Quan CaoMaximilian JaritzMatthieu GuillauminRaoul de CharetteLoris Bazzani",
        "links": "http://arxiv.org/abs/2410.08211v1",
        "entry_id": "http://arxiv.org/abs/2410.08211v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08211v1",
        "summary": "Large-scale vision-language pre-trained (VLP) models (e.g., CLIP) are\nrenowned for their versatility, as they can be applied to diverse applications\nin a zero-shot setup. However, when these models are used in specific domains,\ntheir performance often falls short due to domain gaps or the\nunder-representation of these domains in the training data. While fine-tuning\nVLP models on custom datasets with human-annotated labels can address this\nissue, annotating even a small-scale dataset (e.g., 100k samples) can be an\nexpensive endeavor, often requiring expert annotators if the task is complex.\nTo address these challenges, we propose LatteCLIP, an unsupervised method for\nfine-tuning CLIP models on classification with known class names in custom\ndomains, without relying on human annotations. Our method leverages Large\nMultimodal Models (LMMs) to generate expressive textual descriptions for both\nindividual images and groups of images. These provide additional contextual\ninformation to guide the fine-tuning process in the custom domains. Since\nLMM-generated descriptions are prone to hallucination or missing details, we\nintroduce a novel strategy to distill only the useful information and stabilize\nthe training. Specifically, we learn rich per-class prototype representations\nfrom noisy generated texts and dual pseudo-labels. Our experiments on 10\ndomain-specific datasets show that LatteCLIP outperforms pre-trained zero-shot\nmethods by an average improvement of +4.74 points in top-1 accuracy and other\nstate-of-the-art unsupervised methods by +3.45 points.",
        "updated": "2024-10-10 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08211v1"
    },
    {
        "title": "PointOBB-v2: Towards Simpler, Faster, and Stronger Single Point Supervised Oriented Object Detection",
        "authors": "Botao RenXue YangYi YuJunwei LuoZhidong Deng",
        "links": "http://arxiv.org/abs/2410.08210v1",
        "entry_id": "http://arxiv.org/abs/2410.08210v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08210v1",
        "summary": "Single point supervised oriented object detection has gained attention and\nmade initial progress within the community. Diverse from those approaches\nrelying on one-shot samples or powerful pretrained models (e.g. SAM), PointOBB\nhas shown promise due to its prior-free feature. In this paper, we propose\nPointOBB-v2, a simpler, faster, and stronger method to generate pseudo rotated\nboxes from points without relying on any other prior. Specifically, we first\ngenerate a Class Probability Map (CPM) by training the network with non-uniform\npositive and negative sampling. We show that the CPM is able to learn the\napproximate object regions and their contours. Then, Principal Component\nAnalysis (PCA) is applied to accurately estimate the orientation and the\nboundary of objects. By further incorporating a separation mechanism, we\nresolve the confusion caused by the overlapping on the CPM, enabling its\noperation in high-density scenarios. Extensive comparisons demonstrate that our\nmethod achieves a training speed 15.58x faster and an accuracy improvement of\n11.60%/25.15%/21.19% on the DOTA-v1.0/v1.5/v2.0 datasets compared to the\nprevious state-of-the-art, PointOBB. This significantly advances the cutting\nedge of single point supervised oriented detection in the modular track.",
        "updated": "2024-10-10 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08210v1"
    },
    {
        "title": "Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision",
        "authors": "Shengcao CaoLiang-Yan GuiYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2410.08209v1",
        "entry_id": "http://arxiv.org/abs/2410.08209v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08209v1",
        "summary": "Current large multimodal models (LMMs) face challenges in grounding, which\nrequires the model to relate language components to visual entities. Contrary\nto the common practice that fine-tunes LMMs with additional grounding\nsupervision, we find that the grounding ability can in fact emerge in LMMs\ntrained without explicit grounding supervision. To reveal this emerging\ngrounding, we introduce an \"attend-and-segment\" method which leverages\nattention maps from standard LMMs to perform pixel-level segmentation.\nFurthermore, to enhance the grounding ability, we propose DIFFLMM, an LMM\nutilizing a diffusion-based visual encoder, as opposed to the standard CLIP\nvisual encoder, and trained with the same weak supervision. Without being\nconstrained by the biases and limited scale of grounding-specific supervision\ndata, our approach is more generalizable and scalable. We achieve competitive\nperformance on both grounding-specific and general visual question answering\nbenchmarks, compared with grounding LMMs and generalist LMMs, respectively.\nNotably, we achieve a 44.2 grounding mask recall on grounded conversation\ngeneration without any grounding supervision, outperforming the extensively\nsupervised model GLaMM. Project page: https://groundLMM.github.io.",
        "updated": "2024-10-10 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08209v1"
    },
    {
        "title": "SPA: 3D Spatial-Awareness Enables Effective Embodied Representation",
        "authors": "Haoyi ZhuHonghui YangYating WangJiange YangLimin WangTong He",
        "links": "http://arxiv.org/abs/2410.08208v1",
        "entry_id": "http://arxiv.org/abs/2410.08208v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08208v1",
        "summary": "In this paper, we introduce SPA, a novel representation learning framework\nthat emphasizes the importance of 3D spatial awareness in embodied AI. Our\napproach leverages differentiable neural rendering on multi-view images to\nendow a vanilla Vision Transformer (ViT) with intrinsic spatial understanding.\nWe present the most comprehensive evaluation of embodied representation\nlearning to date, covering 268 tasks across 8 simulators with diverse policies\nin both single-task and language-conditioned multi-task scenarios. The results\nare compelling: SPA consistently outperforms more than 10 state-of-the-art\nrepresentation methods, including those specifically designed for embodied AI,\nvision-centric tasks, and multi-modal applications, while using less training\ndata. Furthermore, we conduct a series of real-world experiments to confirm its\neffectiveness in practical scenarios. These results highlight the critical role\nof 3D spatial awareness for embodied representation learning. Our strongest\nmodel takes more than 6000 GPU hours to train and we are committed to\nopen-sourcing all code and model weights to foster future research in embodied\nrepresentation learning. Project Page: https://haoyizhu.github.io/spa/.",
        "updated": "2024-10-10 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08208v1"
    },
    {
        "title": "From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions",
        "authors": "Changle QuSunhao DaiXiaochi WeiHengyi CaiShuaiqiang WangDawei YinJun XuJi-Rong Wen",
        "links": "http://arxiv.org/abs/2410.08197v1",
        "entry_id": "http://arxiv.org/abs/2410.08197v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08197v1",
        "summary": "Tool learning enables Large Language Models (LLMs) to interact with external\nenvironments by invoking tools, serving as an effective strategy to mitigate\nthe limitations inherent in their pre-training data. In this process, tool\ndocumentation plays a crucial role by providing usage instructions for LLMs,\nthereby facilitating effective tool utilization. This paper concentrates on the\ncritical challenge of bridging the comprehension gap between LLMs and external\ntools due to the inadequacies and inaccuracies inherent in existing\nhuman-centric tool documentation. We propose a novel framework, DRAFT, aimed at\nDynamically Refining tool documentation through the Analysis of Feedback and\nTrails emanating from LLMs' interactions with external tools. This methodology\npivots on an innovative trial-and-error approach, consisting of three distinct\nlearning phases: experience gathering, learning from experience, and\ndocumentation rewriting, to iteratively enhance the tool documentation. This\nprocess is further optimized by implementing a diversity-promoting exploration\nstrategy to ensure explorative diversity and a tool-adaptive termination\nmechanism to prevent overfitting while enhancing efficiency. Extensive\nexperiments on multiple datasets demonstrate that DRAFT's iterative,\nfeedback-based refinement significantly ameliorates documentation quality,\nfostering a deeper comprehension and more effective utilization of tools by\nLLMs. Notably, our analysis reveals that the tool documentation refined via our\napproach demonstrates robust cross-model generalization capabilities.",
        "updated": "2024-10-10 17:58:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08197v1"
    }
]