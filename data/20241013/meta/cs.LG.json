[
    {
        "title": "Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision",
        "authors": "Shengcao CaoLiang-Yan GuiYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2410.08209v1",
        "entry_id": "http://arxiv.org/abs/2410.08209v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08209v1",
        "summary": "Current large multimodal models (LMMs) face challenges in grounding, which\nrequires the model to relate language components to visual entities. Contrary\nto the common practice that fine-tunes LMMs with additional grounding\nsupervision, we find that the grounding ability can in fact emerge in LMMs\ntrained without explicit grounding supervision. To reveal this emerging\ngrounding, we introduce an \"attend-and-segment\" method which leverages\nattention maps from standard LMMs to perform pixel-level segmentation.\nFurthermore, to enhance the grounding ability, we propose DIFFLMM, an LMM\nutilizing a diffusion-based visual encoder, as opposed to the standard CLIP\nvisual encoder, and trained with the same weak supervision. Without being\nconstrained by the biases and limited scale of grounding-specific supervision\ndata, our approach is more generalizable and scalable. We achieve competitive\nperformance on both grounding-specific and general visual question answering\nbenchmarks, compared with grounding LMMs and generalist LMMs, respectively.\nNotably, we achieve a 44.2 grounding mask recall on grounded conversation\ngeneration without any grounding supervision, outperforming the extensively\nsupervised model GLaMM. Project page: https://groundLMM.github.io.",
        "updated": "2024-10-10 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08209v1"
    },
    {
        "title": "SPA: 3D Spatial-Awareness Enables Effective Embodied Representation",
        "authors": "Haoyi ZhuHonghui YangYating WangJiange YangLimin WangTong He",
        "links": "http://arxiv.org/abs/2410.08208v1",
        "entry_id": "http://arxiv.org/abs/2410.08208v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08208v1",
        "summary": "In this paper, we introduce SPA, a novel representation learning framework\nthat emphasizes the importance of 3D spatial awareness in embodied AI. Our\napproach leverages differentiable neural rendering on multi-view images to\nendow a vanilla Vision Transformer (ViT) with intrinsic spatial understanding.\nWe present the most comprehensive evaluation of embodied representation\nlearning to date, covering 268 tasks across 8 simulators with diverse policies\nin both single-task and language-conditioned multi-task scenarios. The results\nare compelling: SPA consistently outperforms more than 10 state-of-the-art\nrepresentation methods, including those specifically designed for embodied AI,\nvision-centric tasks, and multi-modal applications, while using less training\ndata. Furthermore, we conduct a series of real-world experiments to confirm its\neffectiveness in practical scenarios. These results highlight the critical role\nof 3D spatial awareness for embodied representation learning. Our strongest\nmodel takes more than 6000 GPU hours to train and we are committed to\nopen-sourcing all code and model weights to foster future research in embodied\nrepresentation learning. Project Page: https://haoyizhu.github.io/spa/.",
        "updated": "2024-10-10 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08208v1"
    },
    {
        "title": "DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models",
        "authors": "Xiaoxiao HeLigong HanQuan DaoSong WenMinhao BaiDi LiuHan ZhangMartin Renqiang MinFelix Juefei-XuChaowei TanBo LiuKang LiHongdong LiJunzhou HuangFaez AhmedAkash SrivastavaDimitris Metaxas",
        "links": "http://arxiv.org/abs/2410.08207v1",
        "entry_id": "http://arxiv.org/abs/2410.08207v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08207v1",
        "summary": "Discrete diffusion models have achieved success in tasks like image\ngeneration and masked language modeling but face limitations in controlled\ncontent editing. We introduce DICE (Discrete Inversion for Controllable\nEditing), the first approach to enable precise inversion for discrete diffusion\nmodels, including multinomial diffusion and masked generative models. By\nrecording noise sequences and masking patterns during the reverse diffusion\nprocess, DICE enables accurate reconstruction and flexible editing of discrete\ndata without the need for predefined masks or attention manipulation. We\ndemonstrate the effectiveness of DICE across both image and text domains,\nevaluating it on models such as VQ-Diffusion, Paella, and RoBERTa. Our results\nshow that DICE preserves high data fidelity while enhancing editing\ncapabilities, offering new opportunities for fine-grained content manipulation\nin discrete spaces. For project webpage, see\nhttps://hexiaoxiao-cs.github.io/DICE/.",
        "updated": "2024-10-10 17:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08207v1"
    },
    {
        "title": "Efficient Dictionary Learning with Switch Sparse Autoencoders",
        "authors": "Anish MudideJoshua EngelsEric J. MichaudMax TegmarkChristian Schroeder de Witt",
        "links": "http://arxiv.org/abs/2410.08201v1",
        "entry_id": "http://arxiv.org/abs/2410.08201v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08201v1",
        "summary": "Sparse autoencoders (SAEs) are a recent technique for decomposing neural\nnetwork activations into human-interpretable features. However, in order for\nSAEs to identify all features represented in frontier models, it will be\nnecessary to scale them up to very high width, posing a computational\nchallenge. In this work, we introduce Switch Sparse Autoencoders, a novel SAE\narchitecture aimed at reducing the compute cost of training SAEs. Inspired by\nsparse mixture of experts models, Switch SAEs route activation vectors between\nsmaller \"expert\" SAEs, enabling SAEs to efficiently scale to many more\nfeatures. We present experiments comparing Switch SAEs with other SAE\narchitectures, and find that Switch SAEs deliver a substantial Pareto\nimprovement in the reconstruction vs. sparsity frontier for a given fixed\ntraining compute budget. We also study the geometry of features across experts,\nanalyze features duplicated across experts, and verify that Switch SAE features\nare as interpretable as features found by other SAE architectures.",
        "updated": "2024-10-10 17:59:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08201v1"
    },
    {
        "title": "Adam Exploits $\\ell_\\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity",
        "authors": "Shuo XieMohamad Amin MohamadiZhiyuan Li",
        "links": "http://arxiv.org/abs/2410.08198v1",
        "entry_id": "http://arxiv.org/abs/2410.08198v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08198v1",
        "summary": "Adam outperforms SGD when training language models. Yet this advantage is not\nwell-understood theoretically -- previous convergence analysis for Adam and SGD\nmainly focuses on the number of steps $T$ and is already minimax-optimal in\nnon-convex cases, which are both $\\widetilde{O}(T^{-1/4})$. In this work, we\nargue that the exploitation of nice $\\ell_\\infty$-geometry is the key advantage\nof Adam over SGD. More specifically, we give a new convergence analysis for\nAdam under novel assumptions that loss is smooth under $\\ell_\\infty$-geometry\nrather than the more common $\\ell_2$-geometry, which yields a much better\nempirical smoothness constant for GPT-2 and ResNet models. Our experiments\nconfirm that Adam performs much worse when the favorable $\\ell_\\infty$-geometry\nis changed while SGD provably remains unaffected. We also extend the\nconvergence analysis to blockwise Adam under novel blockwise smoothness\nassumptions.",
        "updated": "2024-10-10 17:58:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08198v1"
    }
]