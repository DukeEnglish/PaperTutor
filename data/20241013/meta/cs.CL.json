[
    {
        "title": "LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts",
        "authors": "Anh-Quan CaoMaximilian JaritzMatthieu GuillauminRaoul de CharetteLoris Bazzani",
        "links": "http://arxiv.org/abs/2410.08211v1",
        "entry_id": "http://arxiv.org/abs/2410.08211v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08211v1",
        "summary": "Large-scale vision-language pre-trained (VLP) models (e.g., CLIP) are\nrenowned for their versatility, as they can be applied to diverse applications\nin a zero-shot setup. However, when these models are used in specific domains,\ntheir performance often falls short due to domain gaps or the\nunder-representation of these domains in the training data. While fine-tuning\nVLP models on custom datasets with human-annotated labels can address this\nissue, annotating even a small-scale dataset (e.g., 100k samples) can be an\nexpensive endeavor, often requiring expert annotators if the task is complex.\nTo address these challenges, we propose LatteCLIP, an unsupervised method for\nfine-tuning CLIP models on classification with known class names in custom\ndomains, without relying on human annotations. Our method leverages Large\nMultimodal Models (LMMs) to generate expressive textual descriptions for both\nindividual images and groups of images. These provide additional contextual\ninformation to guide the fine-tuning process in the custom domains. Since\nLMM-generated descriptions are prone to hallucination or missing details, we\nintroduce a novel strategy to distill only the useful information and stabilize\nthe training. Specifically, we learn rich per-class prototype representations\nfrom noisy generated texts and dual pseudo-labels. Our experiments on 10\ndomain-specific datasets show that LatteCLIP outperforms pre-trained zero-shot\nmethods by an average improvement of +4.74 points in top-1 accuracy and other\nstate-of-the-art unsupervised methods by +3.45 points.",
        "updated": "2024-10-10 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08211v1"
    },
    {
        "title": "Mono-InternVL: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training",
        "authors": "Gen LuoXue YangWenhan DouZhaokai WangJifeng DaiYu QiaoXizhou Zhu",
        "links": "http://arxiv.org/abs/2410.08202v1",
        "entry_id": "http://arxiv.org/abs/2410.08202v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08202v1",
        "summary": "The rapid advancement of Large Language Models (LLMs) has led to an influx of\nefforts to extend their capabilities to multimodal tasks. Among them, growing\nattention has been focused on monolithic Multimodal Large Language Models\n(MLLMs) that integrate visual encoding and language decoding into a single LLM.\nDespite the structural simplicity and deployment-friendliness, training a\nmonolithic MLLM with promising performance still remains challenging. In\nparticular, the popular approaches adopt continuous pre-training to extend a\npre-trained LLM to a monolithic MLLM, which suffers from catastrophic\nforgetting and leads to performance degeneration. In this paper, we aim to\novercome this limitation from the perspective of delta tuning. Specifically,\nour core idea is to embed visual parameters into a pre-trained LLM, thereby\nincrementally learning visual knowledge from massive data via delta tuning,\ni.e., freezing the LLM when optimizing the visual parameters. Based on this\nprinciple, we present Mono-InternVL, a novel monolithic MLLM that seamlessly\nintegrates a set of visual experts via a multimodal mixture-of-experts\nstructure. Moreover, we propose an innovative pre-training strategy to maximize\nthe visual capability of Mono-InternVL, namely Endogenous Visual Pre-training\n(EViP). In particular, EViP is designed as a progressive learning process for\nvisual experts, which aims to fully exploit the visual knowledge from noisy\ndata to high-quality data. To validate our approach, we conduct extensive\nexperiments on 16 benchmarks. Experimental results not only validate the\nsuperior performance of Mono-InternVL compared to the state-of-the-art MLLM on\n6 multimodal benchmarks, e.g., +113 points over InternVL-1.5 on OCRBench, but\nalso confirm its better deployment efficiency, with first token latency reduced\nby up to 67%.",
        "updated": "2024-10-10 17:59:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08202v1"
    },
    {
        "title": "From Exploration to Mastery: Enabling LLMs to Master Tools via Self-Driven Interactions",
        "authors": "Changle QuSunhao DaiXiaochi WeiHengyi CaiShuaiqiang WangDawei YinJun XuJi-Rong Wen",
        "links": "http://arxiv.org/abs/2410.08197v1",
        "entry_id": "http://arxiv.org/abs/2410.08197v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08197v1",
        "summary": "Tool learning enables Large Language Models (LLMs) to interact with external\nenvironments by invoking tools, serving as an effective strategy to mitigate\nthe limitations inherent in their pre-training data. In this process, tool\ndocumentation plays a crucial role by providing usage instructions for LLMs,\nthereby facilitating effective tool utilization. This paper concentrates on the\ncritical challenge of bridging the comprehension gap between LLMs and external\ntools due to the inadequacies and inaccuracies inherent in existing\nhuman-centric tool documentation. We propose a novel framework, DRAFT, aimed at\nDynamically Refining tool documentation through the Analysis of Feedback and\nTrails emanating from LLMs' interactions with external tools. This methodology\npivots on an innovative trial-and-error approach, consisting of three distinct\nlearning phases: experience gathering, learning from experience, and\ndocumentation rewriting, to iteratively enhance the tool documentation. This\nprocess is further optimized by implementing a diversity-promoting exploration\nstrategy to ensure explorative diversity and a tool-adaptive termination\nmechanism to prevent overfitting while enhancing efficiency. Extensive\nexperiments on multiple datasets demonstrate that DRAFT's iterative,\nfeedback-based refinement significantly ameliorates documentation quality,\nfostering a deeper comprehension and more effective utilization of tools by\nLLMs. Notably, our analysis reveals that the tool documentation refined via our\napproach demonstrates robust cross-model generalization capabilities.",
        "updated": "2024-10-10 17:58:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08197v1"
    },
    {
        "title": "MathCoder2: Better Math Reasoning from Continued Pretraining on Model-translated Mathematical Code",
        "authors": "Zimu LuAojun ZhouKe WangHouxing RenWeikang ShiJunting PanMingjie ZhanHongsheng Li",
        "links": "http://arxiv.org/abs/2410.08196v1",
        "entry_id": "http://arxiv.org/abs/2410.08196v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08196v1",
        "summary": "Code has been shown to be effective in enhancing the mathematical reasoning\nabilities of large language models due to its precision and accuracy. Previous\nworks involving continued mathematical pretraining often include code that\nutilizes math-related packages, which are primarily designed for fields such as\nengineering, machine learning, signal processing, or module testing, rather\nthan being directly focused on mathematical reasoning. In this paper, we\nintroduce a novel method for generating mathematical code accompanied with\ncorresponding reasoning steps for continued pretraining. Our approach begins\nwith the construction of a high-quality mathematical continued pretraining\ndataset by incorporating math-related web data, code using mathematical\npackages, math textbooks, and synthetic data. Next, we construct reasoning\nsteps by extracting LaTeX expressions, the conditions needed for the\nexpressions, and the results of the expressions from the previously collected\ndataset. Based on this extracted information, we generate corresponding code to\naccurately capture the mathematical reasoning process. Appending the generated\ncode to each reasoning step results in data consisting of paired natural\nlanguage reasoning steps and their corresponding code. Combining this data with\nthe original dataset results in a 19.2B-token high-performing mathematical\npretraining corpus, which we name MathCode-Pile. Training several popular base\nmodels with this corpus significantly improves their mathematical abilities,\nleading to the creation of the MathCoder2 family of models. All of our data\nprocessing and training code is open-sourced, ensuring full transparency and\neasy reproducibility of the entire data collection and training pipeline. The\ncode is released at https://github.com/mathllm/MathCoder2 .",
        "updated": "2024-10-10 17:58:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08196v1"
    },
    {
        "title": "GenARM: Reward Guided Generation with Autoregressive Reward Model for Test-time Alignment",
        "authors": "Yuancheng XuUdari Madhushani SehwagAlec KoppelSicheng ZhuBang AnFurong HuangSumitra Ganesh",
        "links": "http://arxiv.org/abs/2410.08193v1",
        "entry_id": "http://arxiv.org/abs/2410.08193v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08193v1",
        "summary": "Large Language Models (LLMs) exhibit impressive capabilities but require\ncareful alignment with human preferences. Traditional training-time methods\nfinetune LLMs using human preference datasets but incur significant training\ncosts and require repeated training to handle diverse user preferences.\nTest-time alignment methods address this by using reward models (RMs) to guide\nfrozen LLMs without retraining. However, existing test-time approaches rely on\ntrajectory-level RMs which are designed to evaluate complete responses, making\nthem unsuitable for autoregressive text generation that requires computing\nnext-token rewards from partial responses. To address this, we introduce\nGenARM, a test-time alignment approach that leverages the Autoregressive Reward\nModel--a novel reward parametrization designed to predict next-token rewards\nfor efficient and effective autoregressive generation. Theoretically, we\ndemonstrate that this parametrization can provably guide frozen LLMs toward any\ndistribution achievable by traditional RMs within the KL-regularized\nreinforcement learning framework. Experimental results show that GenARM\nsignificantly outperforms prior test-time alignment baselines and matches the\nperformance of training-time methods. Additionally, GenARM enables efficient\nweak-to-strong guidance, aligning larger LLMs with smaller RMs without the high\ncosts of training larger models. Furthermore, GenARM supports multi-objective\nalignment, allowing real-time trade-offs between preference dimensions and\ncatering to diverse user preferences without retraining.",
        "updated": "2024-10-10 17:58:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08193v1"
    }
]