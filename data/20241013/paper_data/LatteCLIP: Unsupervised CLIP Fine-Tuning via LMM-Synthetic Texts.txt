LATTECLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts
Anh-QuanCao2* MaximilianJaritz1 MatthieuGuillaumin1 RaouldeCharette2 LorisBazzani1
1Amazon 2Inria
Abstract
LMM-Synthetic Texts
Large-scalevision-languagepre-trained(VLP)models
Dual Pseudo-labels
(e.g.,CLIP[46])arerenownedfortheirversatility,asthey
canbeappliedtodiverseapplicationsinazero-shotsetup.
However,whenthesemodelsareusedinspecificdomains,
Prototype-based
theirperformanceoftenfallsshortduetodomaingapsor CLIP fine-tuning
Prototypes
the under-representation of these domains in the training
Domain-specific
data. Whilefine-tuningVLPmodelsoncustomdatasetswith Unlabeled images
(e.g., texture)
human-annotatedlabelscanaddressthisissue,annotating
evenasmall-scaledataset(e.g.,100ksamples)canbean Cosine Similarity
expensiveendeavor,oftenrequiringexpertannotatorsifthe Predict
taskiscomplex. Toaddressthesechallenges, wepropose "Cracked" texture
LATTECLIP,anunsupervisedmethodforfine-tuningCLIP
modelsonclassificationwithknownclassnamesincustom Figure1.OverviewofLATTECLIP.Ourprototype-basedmethod
domains,withoutrelyingonhumanannotations.Ourmethod leveragesdifferenttypesofpseudo-labelsandLMM-synthetictexts
leveragesLargeMultimodalModels(LMMs)togenerateex- forimprovedunsupervisedCLIPfine-tuningondomain-specific
pressivetextualdescriptionsforbothindividualimagesand datasets(e.g.,texture).Duringinference,imagefeaturesarecom-
paredwithprototypestogeneratepredictions.Here,f(·)andg(·)
groupsofimages. Theseprovideadditionalcontextualin-
aretheCLIPimageandtextencoders,respectively.
formation to guide the fine-tuning process in the custom
domains. SinceLMM-generateddescriptionsareproneto
hallucinationormissingdetails,weintroduceanovelstrat-
in specialized domains due to domain discrepancies and
egy to distill only the useful information and stabilise the
insufficient representation in the training data. Prior stud-
training. Specifically,welearnrichper-classprototyperep-
ies have demonstrated improvements on custom datasets
resentations from noisy generated texts and dual pseudo-
through supervised fine-tuning [15,60] or few-shot learn-
labels. Our experiments on 10 domain-specific datasets
ing[50,68]. Nevertheless,acquiringhuman-annotatedlabels
showthat LATTECLIP outperformspre-trainedzero-shot
iscostly,evenforrelativelysmalldatasets(e.g., 100ksam-
methodsbyanaverageimprovementof+4.74pointsintop-1
ples),andoftenrequiresexpertannotatorsforcomplextasks.
accuracyandotherstate-of-the-artunsupervisedmethods
Toaddressthis,weproposeLATTECLIP,whichfine-tunes
by+3.45points.
CLIPforclassificationonunlabeledtrainingdatatomaxi-
mizeperformanceonatestsetfromthesamedomain. Here,
a domain refers to a set of shared characteristics within a
1.Introduction
dataset(e.g.,cars,flowers,textures). LikeinUnsupervised
Domain Adaptation (UDA) [12,19,58], we consider the
Large-scale vision-language pre-training [46] has
list of class names to be known a priori. An overview of
emergedrecentlyanddemonstratedimpressivegeneraliza-
tionperformanceonvariousdownstreamtasks[7,8,23,30,
LATTECLIPisshowninFig.1.
RecentprogressofLargeLanguageModels(LLMs)[3,
44],especiallyinzero-shotclassification[46,47]. Thissuc-
24,41,55,56]andLargeMultimodalModels(LMMs)[4,34]
cess is attributed to its robust visio-linguistic representa-
haveledtoafundamentalshiftintrainingandfine-tuning
tion,learnedfromavastamountoflarge-scaleweb-scraped
methodologies. The research community is transitioning
datasets[48]. However,thesemodelsoftenfacechallenges
fromaclass-focusedparadigmtowardsamoredescriptive
*ThemainworkwasdonewhileinterningatAmazon. approach,wheredataisannotatedwithdetailedtextualde-
1
4202
tcO
01
]VC.sc[
1v11280.0142:viXrascriptionsfortraining,andrichtextualanswersareprovided modelandfine-tuningmodelsignificantlyimprovesper-
atinferencetime. Consequently,anincreasingnumberof formance;theformerpreservespre-trainedknowledge,
methods[10,16,28]nowleveragesynthetically-generated while the latter improves the accuracy on the target
textfromLMMsasanadditionalsourceofsupervisionor distribution. ExperimentsshowthatLATTECLIPsig-
contextualinformationtoimproveperformance. Similarto nificantlyoutperformsallbaselinesonaverageacross
theseapproaches,weharnessthepowerofLMMstogener- 10domain-specificdatasets.
atedescriptionsfortraining,butwithastrongemphasison
producingmoreexpressivedescriptions. Insteadofonlygen- 2.Relatedworks
eratingper-imagedescriptions,wealsogeneratedescriptions
forgroupsofimages,capturingtheircommoncharacteris- Adapting CLIP for Classification. CLIP-based meth-
tics,aswellasclass-leveldescriptionsforallimageswithin ods[46,62,63,65]exhibitcompetitivezero-shotclassifica-
a category. These descriptions provide better contextual tionperformance. Forfurtherimprovementondownstream
information, offeringrichersupervisionfortraining, lead- classificationdatasets,CLIPcanbeadaptedtoclosethegap
ingtoimprovedclassificationaccuracyinspecificdomains betweenpre-trainedrepresentationsandspecificdomains.
comparedtothelimitedinformationfrompseudo-labelsand Infew-shotlearning,onehasaccesstoasmallnumber
labelpropagation[20,29]. of labels, typically between 1-16 samples per class, and
However,directlyfine-tuningCLIPwithLMM-generated manyworkshaveadaptedittoCLIP[9,13,14,38,45,50,64,
textsleadstopoorperformanceduetoCLIPoverfittingto 66,68,69]. Prototypicallearning[51]isaseminalworkin
hallucinationsandnoisepresentinthesetexts. Toaddress few-shotlearningandbuildsanaverageembedding(proto-
this, we propose a fine-tuning framework based on proto- type)foreachclass. Duringinference,onethenmatchesthe
type learning [1,39], where classes are represented as a testsampletothenearestprototype. Thisconceptrecently
set of prototypes, typically as feature vectors. Prototypes re-emergedforadaptingCLIP,bybuildingacachemodel
providebettercontrolandinterpretabilityofclassrepresen- holdingtheknowledgefromthefew-shottrainingset[66,69].
tationsthroughdirectmanipulationintheembeddingspace, Differentfromthat,wecontinuouslyupdatetheprototypes
helpingregulatetheinfluenceofeachsyntheticdescription withmomentumduringthetrainingprocesswithmultimodal
duringtraining. Tofurtherimprovetheper-classprototype featuresfromunsupervisedtextsandimages. Otherworks
representations, we combine the synthetic texts with two leveragepromptlearning[68]orefficientfine-tuning[9,64].
typesofpseudo-labelsderivedfrombothzero-shotandfine- Supervised fine-tuning methods require a significant
tuningmodels. Thezero-shotmodeloffersbettergeneraliza- amountoflabeledexamplesfortraining[15,26,46,59,60].
tionthankstopre-trainingknowledge,whilethefine-tuning Linearprobing[46,60]isasimpletechniquethattrainsa
modelprovidesstrongerin-domainperformance. Duringin- classifier on top of frozen image features, but can lead to
ference,theseprototypesarecomparedwithimagefeatures worseresultsduetooverfitting. Thisproblemhasbeentack-
forclassification. AsLMMsareonlyemployedduringdur- ledbyusingtwo-steptrainingschedulesoflinearprobing
ingfine-tuning,theinferencetimeremainsconsistentwith andfullfine-tuning[26],maskedimagemodeling[59]and
standardCLIPmethods. Wevalidatetheeffectivenessofour byfine-tuningwithcontrastivelossbyaligningtheimage
method across 10 domain-specific datasets. Compared to withatemplatetextincludingtheclasslabel(FLYP[15]).
pre-trainedCLIPmodels,LATTECLIPachievesanaverage IncontrasttoFLYP,weaddLMM-generateddescriptionsto
improvementof+4.74pointsintop-1accuracy,surpassing thecontrastiveloss,andstabilizeunsupervisedtrainingby
otherunsupervisedfine-tuningbaselinesby+3.45points. learningprototypeswithmomentum.
Differentfromfew-shotandfine-tuning,wefocusonthe
Ourcontributionscanbesummarizedasfollows:
challengingscenarioofunsupervisedfine-tuning,whereno
• We propose LATTECLIP, a novel method that syn-
labelsareavailable,becausetheyaretoocostlytoannotate.
thesizes multiple types of image descriptions to en-
hance the unsupervised fine-tuning of CLIP models UnsupervisedModelAdaptation OurUnsupervisedfine-
ondomain-specificdatasets, leveragingthelanguage tuningtaskisrelatedtoUnsupervisedDomainAdaptation,
expressivenessofLMMs. where one typically reduces the discrepancy between the
• To make training robust to noisy texts and pseudo- sourceandtargetdata. However,lately,thetaskofsource-
labels,weemployaprototypeframeworkwithamo- freedomainadaptation(SFDA)hasemerged,wheretarget
mentum update, enabling us to control the influence adaptationisperformedwithoutaccesstothesourcedata,
ofsynthetictextfeatures. Tofurtherrefinetheuseful seesurveypaper[31]. Manymethodsexploitthatthesource
imagedescriptions,weintroduceaDynamicFeature model can partially generalize to the target domain, and
Mixermodulethatassignshigherweighttoimportant fine-tunewithpseudo-labels[33],adversariallearning[32],
text,resultinginbetter-combinedtextembeddings. historicalcontrastivelearning[21]ormixup[27].While[21]
• We show that mixing pseudo-labels from zero-shot performmomentumcontrastivelearning[17]ondifferent
2image augmentations, we contrast image-text pairs. The Prompt: Image-description
Describe the flower in the photo
above SFDA works train on a narrow source distribution. concisely, less than 20 words.
LLAVA " The flower is pink and yellow."
Instead,ReCLIP[20]leveragesCLIP,whichispre-trained
on wide-distribution large-scale data. ReCLIP leverages
pseudo labels, cross entropy between separate modalities Predict Class-description
andfocusontransductivesetup(train/testontestset). Test- DahPs le iu ad o f-l la obe wl er "a photo of a dahilia flower."
timeadaptationmethods[35,49,53,57,67]updatethemodel Images with
toalignwiththetargetdistributionattesttimeusingasingle same pseudo-label
Group-description
imageinself-supervisedmanner,requiringoptimizationat
inference. Unlike these approaches, we leverage LMM- LLAVA " Bright, large, orange and
yellow flowers with many petals."
generatedtextstomaximizetestperformancebyfine-tuning
Prompt:
modelparametersonunlabeledtrainingdata,thuskeeping Describe the common visual attributes
of the flowers in all the photos
concisely, less than 20 words.
themodelparametersfixedduringtesting.
LMMs for Synthetic Labels. Using synthetically- Figure2. TextGenerationwithLMM.Inadditiontotheusual
generated labels and textual descriptions is becoming a class-description(middle),combiningtemplatetextandpseudo-
standardinthefield,becauseofthegeneralavailabilityof label,weleverageLMM[34]togenerateimage-description(top)
LLMs and LMMs that can be prompted and guided with which provide more expressive visual description of the image.
task-specificexamples[2,10,16,28,34,42,54]. Thispro- Further, byconsideringrandomgroupofimageswiththesame
videsanopportunityforVLP,thattypicallyuseslarge-scale pseudo-labels,weprompt[34]tocapturesharedcharacteristicsas
group-description(bottom).
image-textpairdatascrapedfromtheinternet,e.g.,LAION-
5B [48]. Instead of using noisy and inconsistent captions
orannotatingalargeset,wesynthetically-generatedescrip-
by proposing an expressive unsupervised text genera-
tions. While LaCLIP [10] rewrites existing captions with
tion (Sec. 3.1) and a prototype-based learning mecha-
LLMs(text-onlyinput),VeCLIP[28]promptsanLMMto
nism(Sec.3.2)tomitigatenoisypseudolabels. Toimprove
captiontheimage,followedbyLLMtextprocessing. Synth-
expressivity beyond pseudo-labeling, we build upon a re-
CLIP[16]synthesizesfirstthetext,andthentheimageswith
centLMM[34],generatingdescriptionsatmultiplelevels
text-to-imagegenerativemodels. Inourwork,weleverage
ofcontextualgranularity,describingtheindividualimage,
LMMstocaptionimages,focusingonfine-tuningratherthan
group of similar images, and entire class. Individual im-
pre-training,combiningthepre-trainedmodel’sknowledge
agedescriptionsofferdetailedthoughpossiblyextraneous
withnewsyntheticcaptionsinabalancedapproach.
information,whichisaddressedbygroupdescriptionsthat
capturesharedcharacteristicsofsimilarimages,albeitwith
3.Method
some noise. This noise is mitigated by class descriptions,
whichprovidestablerepresentationstoaddressinconsisten-
Fine-tuningCLIPwithcombinationofpredefinedtem-
cies. Equippedwithsuchtextualdescription,weaddition-
plates,suchas“a photo of a [class].”,wasshown
ally introduce a prototype-based learning framework that
to yield effective results when using ground-truth class
learnsasetofclassprototypesfromthegeneratedtextfea-
labels [15,26,60]. However, in the absence of ground-
tures. Theseprototypesareupdatedinamomentumsetting
truth class labels, fine-tuning CLIP models with pseudo-
labels[29],usingFLYP[15],leadstolimitedimprovements1 toproduceasmoothoptimizationoverthewholetraining
set, reducing the effect of noise from outlier samples and
Thiscanbecausedbytwofactors.First,thetextemployedas
incorrectsynthesizedtexts.
supervision,resultingfromthecombinationofthetemplate
and pseudo-label, lacks expressivity and discriminativity.
3.1.ExpressiveTextGenerationwithLMMs
This is typically the case for classes that are not visually
descriptive,suchastypesoflanduse(e.g.,annualcrop,in- Withoutaccesstoground-truthlabelsfortrainingCLIP
dustrial,etc.)ornamesoftextures(e.g.,paisley,sprinkled, models,wemustrelyonnoisypseudo-labels. Furthermore,
etc.). Second,pseudo-labelsareinherentlynoisy,whichneg- classnamesaloneoftenlackvisualdescriptiveness. Conse-
atively affects the downstream classification performance quently,usingonlycross-entropylossorsolelyrelyingon
duetodomainshiftsrelativetotheoriginaltrainingdata. classnamesleadstosuboptimalperformanceinoursetting.
Our method, LATTECLIP, addresses these limitations Toaddressthischallenge,weintroduceanovelapproachthat
leveragesgeneratedtexttoprovideadditionalcontextualin-
1Tab. 1 reports the performance of “FLYP + pseudo labels” where formation.Inadditiontothemorestandardclass-description
weshowlimitedimprovementswithrespecttoCLIPacross10datasets
mentionedabove,weproposetwoadditionalwaysusinga
onaverage. Wealsoobserveperformancedropsonsomedatasets(e.g.,
Food101,Flower102). recent LMM [34] to generate textual descriptions of im-
3ages: image-descriptionandgroup-description,asdepicted
"The flower is
inFig.2. Thesegeneratedtextsholdcomplementaryinfor- pink and yellow."
mation with increasing semantic abstraction, from class2,
Shared
tosingleimage,togroupofimages,allofwhichhelpthe
"Bright, large, orange
modeltolearnmorepreciseclassificationboundaries. The and yellow flowers
with many petals."
image-descriptiontextsprovidedetaileddescriptionsofin-
Shared
dividualimages,capturingtheiruniquecharacteristicsand
"a photo of a
subtlefeatures. Thegroup-descriptiontextsofferacompre- dahilia flower." FeD aty un ra em Mic ix er
hensivedescriptionrepresentingtheentireclass,covering
sharedfeaturesandcommonattributes. Dual Pseudo-labels
zero-shot
Importantly,wefoundthattheabovementioneddescrip- or Momentum
fine-tuning update
tions are complementary to the use of template text with
pseudo-labelclass,whichwereferasclass-description. In
Prototypes
fact,welatershowthatpreservingthisclass-descriptionin
Contrastive loss
thetrainingprocessiscrucialasourgeneratedtextscanbe
noisyduetomissingdetailsorhallucination. Thecombina-
tionofclass-/image-/group-descriptionprovidesastableand
Figure3. Training. Forimagex, wepredictpseudo-labelc ∈
reliablerepresentationcorrespondingtotheclasses. {c ,c }andcreatethreetypeofdescriptionsperpseudo-label
zs ft
Moreformally,foreachimagexwegeneratethreetexts, asdescribedinSec.3.1. OurDynamicFeatureMixercombines
illustratedinFig.2,anddefinedasfollows: thesedescriptionswiththecorrespondingprototypep ctoproduce
Class-description(Tclass)providesaconsistentclassrep- a prototype-text embedding t¯, which updates the prototype p c.
Lastly,thecontrastivelossEq.(3)iscomputedbetweent¯andand
resentation using template “a photo of a [class].”
theimageembeddingf(x).
where[class]issubstitutedwiththeimagepseudo-label
cobtainedfromaCLIPzero-shot.
Image-description (Timage) captures unique features of
3.2.Prototype-basedCLIPfine-tuning
image x. We generate Timage by prompting LLAVA [34]
with: “Describe the [domain] in the photo AdoptingdirectlythegeneratedtextsfromSec.3.1isin-
concisely, using less than 20 words." where effective,becausethetextencoderoverfitstothedistribution
[domain] is replaced with the dataset domain (e.g., ofgeneratedtexts,whicharenoisybyconstructiondueto
flower,product,pet,car,etc.). Weshowimage-description hallucinationsoftheLMMandmissingdetails. Weconfirm
examplesin Fig.5. thisexperimentallyinTab.2, rows4, 5, 6. Therefore, we
Group-description (Tgroup) captures shared visual propose a prototype learning approach that is capable of
characteristics between similar images, to combat known determiningtheimportantsynthetictextsandlearningbetter
limitation of LMM which may miss or hallucinate visual classrepresentationsfromthem. Ourapproachmixesthree
characteristics[36]. TogenerateTgroup fromimagex,we keyingredientsasshowninFig.3: (1)asimplestrategyto
randomly sample multiple images with the same pseudo- preserverobustnessbyleveragingpseudo-labelsfromboth
label as x. These are collaged into a single image xgroup frozen and fine-tuning CLIP models; (2) a feature mixer
fed to LLAVA which is prompted with: “Describe the thatdynamicallybalancestheimportanceofeachtextTclass,
common visual attributes of the [domain] in TimageandTgroup;(3)amodulethatupdatestheprototypes
all the photos concisely, in fewer than 20 duringtraining,stabilizingthelearningprocess.
words.". Examples of such group-descriptions are
DualPseudo-labels. AsinWISE-FT[60],weobserve
illustratedinFig.5.
thattrainingonlywithpseudo-labelsfromthefine-tuning
Togeneratethesedescriptions,weuseLLAVA1.6[34] model improves accuracy but at the cost of overfitting to
witha4-bitquantizedMistral7Bmodel.Thismodelrequires thetrainingdistribution. Hence,topreserverobustness,for
approximately5GBofGPUmemoryandtakesaround1.2 eachimageweemploytwopseudo-labels{c ,c }originat-
zs ft
secondstogenerateasingledescriptionperimageonaTesla ingfromboththezero-shotmodel(c )andthefine-tuning
zs
V100 GPU. This makes description generation relatively model(c ). Welatershowthatthissimplestrategyoffers
ft
cost-effective,aswecanrunfiveinstancesofthismodelin greatergeneralizationandaccuracy.
parallelonaTeslaV10032GBGPU,takingapproximately
Prototype Learning. From the generated texts and
3.4hourstogeneratedescriptionsfor50kimages.
pseudo-labels, we aim to learn a set of prototypes corre-
sponding to all classes, denoted as {p }C . These proto-
c c=1
2Here,"class"referstotheclass-description. typesaredesignedtocaptureclass-specificdetailsofthesyn-
4thesizedtextsandpseudo-labelswithintheCLIPembedding Prototypes
space. First,theprototypesareinitializedwithfeaturesde-
rivedfromtheTclass,generatedbasedonitsassociatedclass
Cosine Similarity
name. Then,foranimagex,weuseourdualpseudo-labels Text Feature
from zero-shot and fine-tuning {c ,c } to generate two
zs ft
class-descriptiontexts{Tclass,Tclass}andselectthecorre- | - |=
zs ft Weight
spondingprototypesp andp . Ourfeaturemixerstrategy,
zs ft
detailed below, then combines the two class-descriptions Assign
Weights
withtheimage-descriptionandthegroup-description,there- Text Features Text Weights
fore obtaining two prototype-text embeddings t¯ and t¯ ,
zs ft
seeFig.3. Wethenapplyamomentumupdatetothecor-
Prototype
responding prototypes. Finally, we apply two contrastive Weighted Avg.
losses[15]betweentheimageembeddingf(x)andeachof Prototype-Text
theprototype-textembeddingst¯ zsandt¯ ft. Prototype Embedding
Weight
DynamicFeatureMixer. Tocompensatefornoisytext Figure4.DynamicFeatureMixer.Wecomputecosinesimilarities
descriptions,weproposeamechanismthatdynamicallyre- betweeneachtextfeatureandallprototypes. Weightsaredeter-
weights the three descriptions as a function of the cosine minedbythedifferencebetweenthetoptwosimilarityscores.We
similaritybetweeneachdescriptionembeddingandcorre- calculateaweightedaverageofthefeaturesandcombineitwith
theprototype(Sec.3.2),creatingarepresentationrelevanttothe
sponding prototype, see Fig. 4. Intuitively, our goal is to
inputprototypeyetdistinctfromothers.
assignhigherweightstodescriptionsuniquelydescribinga
classandlowerweightstogenericdescriptions. Inthegen-
eralcase,foratextT wefirstcomputethecosinesimilarities
between its CLIP embedding g(T) and each of the proto-
Training. Given an image x, we train both the image
types,andobtainitsweightwfromthedifferencebetween
encoderf(·)andtextencoderg(·)usingcontrastivelossto
thetwoclosestsimilarities. Thiswrites:
align the image embedding f(x) with both the prototype-
(cid:18)
g(T)·p
(cid:19)C (cid:18)
g(T)·p
(cid:19)C text embeddings t¯
zs
and t¯ ft, resulting in two losses L
zs
=
w=top 1 ∥g(T)∥∥pc ∥ −top 2 ∥g(T)∥∥pc ∥ L con(x,t¯ zs)andL ft =L con(x,t¯ ft)respectivelywithL con(·,·)
c c=1 c c=1 definedas:
(1)
w lah rge ere stt vo ap lu1 e(· s) oa fn td het io np p2 u( t·) ser te ,t ru er sn peth cte ivl ea lr yg .e Ast ha in gd hs we ec io gn hd
t L (x,t¯)=−
1 (cid:88)N
log
exp(f(x)·t¯ i/τ)
indicatesalargegapbetweentop 1(·)andtop 2(·)similari- con N
i=1
(cid:80)N j=1exp(f(x)·t¯ j/τ)
(3)
t pi re os t, oe tn ys pu eri wng hit lh ee dt ie sx st imfe ia latu rr fe rois mun thiq eu re ely st,si am si tla or pto (·a )s vin alg ul ee
−
1 (cid:88)N
log
exp(t¯ i·f(x)/τ)
,
servingasanupperboundforthesimilarityofthe2 remaining N i=1 (cid:80)N j=1exp(t¯ j ·f(x j)/τ)
prototypes. Alternatively,wecouldusethemeanormedian,
whereN isthebatchsizeandτ isthetemperatureparameter
butthismightresultinatextbeingverysimilartoafewpro-
as in [46]. The first term of Eq. (3) normalizes over text
totypeswhileremainingdissimilartoothers. Subsequently,
given the set of texts {Timage,Tgroup,Tclass} and the embeddings to match the correct text to an image, while
weights {wimage,wgroup,wclass} computed using Eq. (1). thesecondnormalizesoverimageembeddingstomatchthe
Theresultingprototypeembeddingt¯isdefinedas correctimagetoatext. ThefinallossisL zs+L ft.
Momentum update prototypes. For a pseudo-label c,
(cid:80) wi·g(Ti) wederivethecorrespondingprototype-textembeddingt¯for
t¯=(1−α) i∈I +αp (2)
(cid:80) wi c eachimage. Duringtraining,theaverageprototype-textem-
i∈I
bedding t¯ is computed over the images in the batch.
batch
where I={image,group,class} and α is the prototype Usingthepseudo-labelc,weupdatetherespectiveprototype
weight. We empirically set α to 0.99 in all experiments p withamomentumµ,obtainingtheupdatedembedding
c
tostabilizetraining,astheprototypesaremorereliablethan p¯ =(1−µ)t¯ +µp ,whichisthenstoredbackinthe
c batch c
thesynthetictextembeddings.Thus,thisactastrongregular- prototypebankastheprototypeforclassc. Momentumup-
izationmechanismagainstthenoiseinducedbythesynthetic dateworkseffectivelywhenµ∈{0.99,0.999,0.9999}[17].
texts. Yet,t¯remainstailoredforeachimageasTimageand As we fine-tune on smaller dataset with fewer iterations,
Tgroupdiffer. Withtwopseudo-labelsperimage,thisresults wesetµto0.99forfasterupdatesoftheprototypes. Intu-
intwoprototype-textembeddings{t¯ ,t¯ }. itively,theprototypecanbeviewedastherunningaverage
zs ft
5Method AverageEuroSATSun397Food101Flower102DTDFGVCOxfordPets Cars UCF101Caltech101
Oracle 81.76 94.46 77.45 85.01 87.90 76.65 37.95 92.42 90.21 79.49 96.02
LLAVAzero-shot 27.23 44.78 15.74 29.81 6.58 20.27 3.18 28.92 3.38 44.25 75.38
Pre-trainedCLIP 67.49 42.95 68.20 78.65 71.30 55.32 23.79 87.30 88.25 64.37 94.73
ReCLIP[20] 68.78 49.25 69.07 77.91 71.13 56.91 25.92 88.50 87.84 68.86 92.37
FLYP[15]+pseudo-label[29] 70.01 67.12 70.19 76.83 68.78 61.82 17.40 88.96 84.19 69.44 94.69
LATTECLIP(ours) 72.23 80.27 70.68 79.63 71.94 56.26 22.02 89.21 87.40 70.08 94.77
Table1.Top-1accuracyon10classificationdatasets.Wereporttheresultsforfivebaselinesandourmethod.The‘Average’columnshows
theaverageresultsacrossalldatasets.Best/2ndbest.
ofthetext-prototypeembeddingsassignedtotheclass. This maintain a fair comparison with ReCLIP, which also
processisrepeatforeachpseudo-labelin{c ,c }. does not employ weight ensembling. Finally, we add
zs ft
"LLAVA zero-shot" baseline which prompts LLAVA to
Inference. Thepredictionsforanimagexaremadeby
classify the image from a given list of classes, using the
comparing the image embedding f(x), where f(·) is the
following prompt “Select the most appropriate
fine-tunedCLIPimageencoder,with{p }C andtakingthe
c c=1 category for the image from the following
prototypewiththehighestcosinesimilarityasoutput.
options:[options]. Write only the category
name." , where options is replaced with the list of class
4.Experiments
names. Forthesupervisedbaseline, wetrainFLYPusing
ground-truthlabels,servingasanoracle. Theevaluationis
WeevaluateLATTECLIPonthetaskoffine-tuningon10
performedinazero-shotfashion,likezero-shotCLIP.Since
specializedclassificationdatasets,withoutusinganyground
ourmethodusesprototypes,noclasstemplateembeddings
truthlabels.Weusethetrainingsetforunsupervisedtraining
have to be computed, and we directly use the prototype
andusethetestsettocomputethetop-1accuracy.
vectors. Forallbaselinesandours,weuseOpenCLIP[22],
Datasets. We employ a mixture of datasets covering the open-source implementation of CLIP [46], with a
various specialized domains, including satellite imagery, ViT/B-32architecture,pre-trainedontheLAION-2Bdataset.
food dishes, airplane models, and others: EuroSAT [18], Performanceisreportedbasedonthelastepochsincewe
SUN397 [61], Food101 [5], DTD [6], FGVC [37], Ox- have no supervision signal. Additional implementation
ford Pets [43], Cars [25], UCF101 [52], Caltech101 [11], detailsareinAppendixB.
Flower102[40]. Thesedatasetsfeaturespecificclasses,such
asthecarmodel,makingtheunsupervisedfine-tuningsetup 4.1.Results
challenging. We follow the standard train/val/test splits
Themainresultswithtop-1accuracyonthe10datasets
in [68]. We train LATTECLIP using the combined train
are shown in Tab. 1. Across all datasets, LATTECLIP
andvalsetsandreportitsperformanceonthetestset.
improves the average top-1 accuracy of CLIP by 4.74
Baselines. Wecompareourmethodtofourunsupervised points. Furthermore,itoutperformsallunsupervisedbase-
baselinesandonefullysupervisedbaseline,whichservesas lines,includingtherecentlypublishedReCLIP[20]andour
an oracle. First, we perform zero-shot classification with proposed baseline that integrates FLYP [15] with pseudo-
a pre-trained CLIP model. As in CLIP [46], we compute labeling[29], by3.45and2.22points, respectively. Inter-
text embeddings for all classes with template “a photo estingly,FLYP+pseudo-labeloutperformsReCLIP,likely
of a [class].”. For classification, we compute the duetotherobustnessandeffectivenessoffine-tuningboth
cosine similarity between each image and all class text imageandtextencoderswithcontrastiveloss,insteadofjust
embeddings. Our second baseline, ReCLIP [20], also theimageencoderwithcross-entropyloss,asdemonstrated
performs fine-tuning without labels but utilizes improved inFLYP[15]. Notably,LLAVAzero-shothaslowoverall
pseudolabelsandself-training. However,ReCLIPprimarily performance, which could be attributed to LLAVA being
focusesonexperimentsconductedinatransductivemanner, trainedingenerativeautogressivemanner,thusnotoptimal
which involves training and evaluating on the test split of fordiscriminativetasks. Lastly,theoracleisshownonthe
each dataset. To ensure a fair comparison, we retrained first line by training FLYP with ground-truth labels. The
ReCLIP using the same CLIP-based model and identical 9.53-pointaverageperformancegapbetweenthefullysuper-
datasetsplitsasourmethod. Third,wecombinedFLYP[15] visedoracleandunsupervisedLATTECLIPhighlightsroom
with pseudo-labeling [29] for unsupervised fine-tuning, forimprovement. Still,LATTECLIPperformscompetitively,
as the original method relies on supervised fine-tuning. narrowingthegapacrossmultipledatasets,particularlyon
Note that we use FLYP without weight ensembling to OxfordPets,Cars,andCaltech101,tolessthan3%.
6Tclass Timage Tgroup Average EuroSAT Sun397 Food101 Flower102 DTD FGVC OxfordPets Cars UCF101 Caltech101
1 ✓ ✓ ✓ 72.23 80.27 70.68 79.63 71.94 56.26 22.02 89.21 87.40 70.08 94.77
2 ✓ ✓ 70.74 79.98 64.85 75.52 72.31 57.03 16.44 89.45 87.09 69.50 95.21
3 ✓ 70.67 78.22 59.79 81.52 71.21 56.74 16.50 90.00 87.89 69.84 94.97
4 ✓ ✓ 55.97 64.75 63.28 76.73 50.95 48.76 9.00 64.51 32.98 60.45 88.24
5 ✓ 52.37 44.31 62.54 77.00 48.44 43.09 7.89 56.75 33.86 58.58 91.24
6 ✓ 53.52 59.35 65.00 77.06 31.67 49.05 9.57 64.49 25.99 66.90 86.09
7 ✓ ✓ 71.63 79.68 70.07 80.16 71.99 57.47 18.00 89.81 86.15 68.84 94.12
Table2.Impactofgeneratedtexts.Bestperformanceisachievedwhenusingalltypesofdescriptions.Best/2ndbest.
Class: Forest(Eurosat) Class: Banded(DTD) In Fig. 5, we show examples of generated image-
descriptionTimage andgroup-descriptionTgroup. Overall,
Tgroupoffersmorecomprehensiveandcontextualinforma-
tion.Forinstance,inthetop-rightexample,Timageissimply
"stripedpattern,"whereasTgroupprovidesricherdetails,in-
cluding"zebra,animalprint,geometricshape,lines". This
trendisevidentinotherexamplesaswell. Forexample,in
thebottom-rightexample,Timageis"Girlapplyingmakeup,"
whileTgroupelaborateswith"Makeupapplication,close-up,
handsholdingtools."Additionally,image-descriptionfailsto
capture"forest"inthetop-leftexample,describingitmerely
Tgroup: Dark blue, green, Tgroup: Stripes, zebra, ani-
as"Theimageshowsalargebodyofwaterwithnovisible
andbrowncolors,indicating malprint,geometricshapes,
landuse."Incontrast,Tgroupincludesrelevantdetailssuch
water,vegetation,andland. lines,andboldcolors.
as"vegetation,land,greenandbrowncolors."
Timage: Theimageshowsa Timage: Stripedpattern.
largebodyofwaterwithno 4.2.Ablations
visiblelanduse.
Class: IndoorFactory Class: ApplyEyeMakeup Different types of synthetic descriptions. Tab. 2 il-
(SUN397) (UCF101) lustratestheimpactofdifferentgeneratedtextsonoverall
performance. We observe that all texts are essential for
achievingthebestperformance. Specifically,excludingthe
image-descriptionreducestheaverageperformanceacross
all datasets by 0.6 (row 1 vs. row 7). The impact of re-
movingthegroup-descriptionisevenmoresignificantwith
a 1.49 points reduction (row 1 vs. row 2). Additionally,
omittingboththeimage-descriptionandgroup-description
resultsinanevenlargerlossof1.56points(comparingrow
1torow3). Rows4, 5, and6showthatrelyingsolelyon
synthetictextscausesadropinperformanceduetothenoise
Tgroup: Industrial factory Tgroup: Makeupapplication, andinaccuraciesintroducedbythegenerateddescriptions.
with white walls, industrial close-up, hands holding
DynamicFeatureMixer. WeablateourDynamicFea-
equipment, machinery, tools.
clocks,anddoors. Timage: Girl applying tureMixerinTab.3(row"w/oDynamicFeatureMixer")by
Timage: Large industrial makeup. settingallthetextweightsto1.0,sothatalltextscontribute
equally. Theaverageperformancedropsby2points,with
buildingwithdoors.
significantdecreasesonmultipledatasets,suchas-14.23on
Figure5. Examplesofgeneratedcaptions. Weeithergenerate EuroSAT,-1.41onDTD,and-2.03onCars. Thisdemon-
acaptionfromthegroupof4images,byinputtingthemastiled stratesthatourDynamicFeatureMixermoduleeffectively
singleimageintoLLaVA(Tgroup),orweinputasingleimageto
assignsrelevantweightstothemeaningfuldescriptions.
LLaVA(Timage). Forsimplicity,inthisfigure,weonlyshowa
singleimagecaption(highlightedbyredboundingbox). DualPseudo-Labels. Bestperformanceisachievedusing
bothzero-shotandfine-tuningpseudo-labels{c ,c }. This
zs ft
isassessedinTab.3byremovingthecorrespondinglosses.
Removingthezero-shotpseudo-label(row"w/oL ")leads
zs
7Method AverageEuroSATSun397Food101Flower102 DTD FGVCOxfordPets Cars UCF101Caltech101
LATTECLIP(ours) 72.23 80.27 70.68 79.63 71.94 56.26 22.02 89.21 87.40 70.08 94.77
w/oDynamicFeatureMixer 70.23 66.04 69.41 80.18 72.51 54.85 23.43 87.14 85.37 69.02 94.32
w/oL 68.26 47.06 69.80 79.23 70.48 56.97 23.82 87.65 87.19 65.77 94.60
ft
w/oL 70.58 76.96 68.18 70.29 71.01 61.05 19.89 87.49 86.13 70.39 94.36
zs
w/oMomentumUpdate 45.72 31.19 56.17 68.08 57.41 31.32 13.41 13.03 43.31 54.69 88.56
Table3.Methodablation.Allcomponentscontributetothebestperformance.Best/2ndbest.
toasignificantdropacrossmultipledatasets: -3.31onEu-
roSAT,-2.5onSUN397,-9.34onFood101,andanaverage
declineof-1.65acrossalldatasets. Furthermore,removing
thefine-tunedpseudo-labels(row"w/oL ")resultsinan
ft
evenmoresubstantialaverageperformancedropof-3.97,
withparticularlynotabledecreasesof-33.21onEuroSAT
and-4.31onUCF101. Weconjecturethatthisisbecausethe
zero-shotpseudo-labelismorerobust,whilethefine-tuned
pseudo-labelhashigheraccuracyonthetrainingdataset.
Figure 6. Impact of amount of training data. Average top-1
Momentum Update. We ablate the impact of the mo-
accuracyon10datasetswhilevaryingtheamountoftrainingdata.
mentumupdatebysettingµ=0,asshowninrow"w/oMo-
mentumUpdate"inTab.3,thereforedirectlyreplacingthe
prototypebythenewweightedtextfeatures. Withoutmo-
formanceplateausafter4images,possiblyduetothefixed
mentumupdate,performancedeclinesdramatically,withan
inputresolutionofLLAVA[34],leadingtolowerper-image
averagedecreaseof−26.51acrossalldatasets. Significant
resolutionasthenumberofimagesincreases. Weprovide
declinesareobservedinmanydatasets,suchas-44.09on
theperformanceforalldatasetsinTab.5.
Cars, -14.51onSUN397, and-14.53onFlower102. This
substantialdropisattributedtothehighvarianceinthepro- Impactofamountoftrainingdata. Sec.4.2illustrates
totypesduetothenoisygeneratedtexts. the effect of training data size on the average top-1 accu-
racyofLATTECLIPandoracleacross10datasets. Despite
IncorrectimagesingeneratingTgroup. Weanalyzehow
being unsupervised, LATTECLIP exhibits strong robust-
incorrectimageswithinagroupaffectthegeneratedgroup-
ness to varying amounts of training data, comparable to
descriptionTgroup.Wetestgroupsof4imageswithdifferent
anoracle. Specifically, LATTECLIP’sperformancedrops
numberofcorrectimages,selectedusingground-truthlabels.
only0.77/6.36on20%/1%data,respectively,comparedto
For1,2,3and4correctimages,thisresultsintop-1accuracy
7.28/1.30oftheoracle. Overall,moredataimprovesperfor-
of 72.48, 72.61, 72.72, and 72.64, respectively, averaged
mancebutdiminishesnotablyforbothafter20%.
acrossalldatasets.Performanceimprovesslightlywithmore
correct images. Notably, our method using pseudo-labels 5.Conclusion
achievesaperformanceof72.23,whichiscompetitivewith
theground-truthlabelselection. Thisdemonstratestolerance LATTECLIP is a novel method for unsupervised CLIP
to noise and pseudo-label inaccuracies within the image fine-tuningonspecializeddatasetswherehumanannotations
group. DetailedperformanceisprovidedinTab.4. arecostlyorrequireexpertknowledge. LeveragingLMMs,
LATTECLIP generates rich and expressive synthetic tex-
Numberofimagespergroup. Weanalyzetheimpactof
tualdescriptionsatvariouslevelsofcontextualgranularity,
increasingthenumberofimagespergrouponperformance
includingimage-description,group-description,andclass-
bytestinggroupswith2,4,8,and16images,resultinginav-
description. Toeffectivelylearnfromthesepotentiallynoisy
erageperformancescoresof71.55,72.23,72.31,and72.49,
descriptions, we propose a prototype learning framework
respectively,acrossalldatasets. Theperformanceimproves
withthreekeyelements: (1)dualpseudo-labelsfromfrozen
withmoreimagespergroup,likelybecausetheprobability
andfine-tuningCLIPmodels;(2)aDynamicFeatureMixer
ofincludingthecorrectimagesincreases. Themostnotable
foroptimaltextfeatureweighting;and(3)momentumup-
improvementoccurswhenincreasingthenumberofimages
date to enhance training stability. LATTECLIP surpasses
pergroupfrom2to4,withscorerisingfrom71.55to72.23.
comparablebaselinesonaverageacrossalldatasets.
This is because achieving a majority with only 2 images
requires100%accuracy,whereaslargergroupscantolerate
someerrorswhilestillmaintainingacorrectmajority. Per-
8labeltype #correct Avg. EuroSATSun397Food101Flower102 DTD FGVCOxfordPets Cars UCF101Caltech101
Pseudo(ours)N/A 72.23 80.27 70.68 79.63 71.94 56.26 22.02 89.21 87.40 70.08 94.77
1 72.48 80.02 69.19 79.04 72.88 61.11 20.55 89.62 87.35 70.16 94.89
2 72.61 81.28 69.73 79.13 72.55 60.82 20.76 89.51 87.53 69.65 95.13
Ground-truth
3 72.72 80.81 70.28 79.80 72.72 60.28 21.87 89.48 87.29 69.52 95.17
4 72.64 80.40 70.54 78.79 72.96 60.17 21.42 89.62 87.96 70.00 94.56
Table4.Impactofvaryingthenumberofcorrectlychosenimagesbasedonground-truthlabelswhenusing4imagesforgroup-description
generation.Ourapproachyieldscomparableperformancedespiterelyingsolelyonpseudo-labelsforimageselection.
#Images Average EuroSAT Sun397 Food101 Flower102 DTD FGVC OxfordPets Cars UCF101 Caltech101
2 71.55 80.74 69.36 76.03 71.24 56.03 21.12 89.29 87.32 69.88 94.52
4 72.23 80.27 70.68 79.63 71.94 56.26 22.02 89.21 87.40 70.08 94.77
8 72.31 79.90 69.90 79.55 73.04 57.69 22.00 89.18 87.65 69.71 94.44
16 72.49 80.67 70.18 78.24 73.20 58.64 22.28 89.53 87.85 70.05 94.28
Table5.Numberofimagespergroupimpactongeneratinggroup-descriptions.Overall,moreimagesimproveperformanceduetoricher
informationandincreasedrobustnessagainsttheinclusionofincorrectimages.However,theperformanceplateausonsomedatasets,e.g.,
UCF101orSUN397,couldbeattributedtoLLAVA’sfixedresolution,resultinginlowerresolutionperimagewhenusingmoreimages.
We use a batch size of 512 and a learning rate of 1e-7
for the datasets Caltech101 [11], DTD [6], Eurosat [18],
Acknowledgment. Theresearchwasconductedduring
FGVC[37], OxfordPets[43], Cars[25], Flower102[40],
Quan’sinternshipatAmazon. Theresearchwasalsosup-
and UCF101 [52]. For the datasets Food101 [5] and
portedbytheANRprojectSIGHT(ANR-20-CE23-0016)
andSAMBAcollaborativeprojectco-fundedbyBpiFrance
SUN397[61],weusealearningrateof1e-6. LATTECLIP
istrainedformin{2000iterations,50epochs}.
in the Investissement d’Avenir Program. Computation
ForFLYP[15], wereimplementitbasedonitsofficial
wasperformedusingHPCresourcesfromGENCI–IDRIS
implementation3andOpenCLIP[22],asitsideaisintuitive
(AD011012808R2,AD011014102R1). WethankAjanthan
and simple: fine-tuning using contrastive loss with class
ThalaiyasingamandMohammadFahesfortheirinsightful
templates instead of cross-entropy loss. We use the same
suggestions. We also extend our gratitude to Mohammad
OpenCLIP-based model and training hyperparameters as
FahesandIvanLopesfortheirthoroughproofreading.
LATTECLIP.Thepseudo-labelsarerecalculatedafterevery
Appendices weightupdate,following[29].
ForReCLIP[20],weusetheofficialimplementation4,but
A.Limitations substituteOpenCLIPasthebaseCLIPmodeltoensureafair
comparisonacrossallmethods. WhileReCLIPisdesigned
Despitepromisingresults,LATTECLIPconsidersalim-
fortransductivelearning(train/testontestset),asshownin
ited number of description types. Expanding description
thepaperandbyitsofficialimplementation,weadaptitto
generationtoincludemorecontextuallevels,suchasscenes,
ourexperimentalsetup. Specifically,weretrainandevaluate
objects,andattributes,wouldproviderichercontextualin-
ReCLIPusingidenticaldatasetsplitsasLATTECLIP.
formation. Additionally, our performance is constrained
by the underlying LMM model, and improvements could C.Additionalablations
be made with better models in the future. Lastly, it is un-
clear why the method improves on some datasets but not IncorrectimagesingeneratingTgroup. Tab.4presents
others. Understandingthisdiscrepancycouldleadtobetter
theresultsacrossalldatasetswhenvaryingthenumberof
methods.
correctimages,whichareselectedusingground-truthlabels,
ingroupsof4imagesusedforgeneratinggroup-descriptions.
B.Implementationdetails
Usingmorecorrectimagesgenerallyleadstoimprovements
in most datasets. However, the average performance gap
WeimplementLATTECLIPbasedonthestandardfine-
remainssmall,demonstratingtherobustnessofourmethod
tuningpipelineofOpenCLIP[22]usingtheVIT-B/32model.
Thehyperparametersusedarethedefaultonesprovidedin 3https://github.com/locuslab/FLYP
OpenCLIP [22], except for batch size and learning rate. 4https://github.com/michiganleon/ReCLIP_WACV
9x Timage xgroup Tgroup pseudo-labels GT
Green,brown,andbluecolors, c : permanent
Buildingsandgreen zs river
indicatingvegetation,soil,and cropland,c :
spaces. ft (Eurosat)
water. river
Thetextureinthe
photoisawooden Zigzagpatterns,geometric c : zigzagged,c : zigzagged
zs ft
floorwitha shapes,andvibrantcolors. grooved (DTD)
herringbonepattern.
Thepinkprimrose
flowerinthephotois
Purpleandyellowpetals,green c : pinkprimrose, gardenphlox
abeautifulandvibrant zs
stems,multiplelayersofpetals. c : silverbush (Flower102)
displayofnature’s ft
beauty.
Shoes,women,shopping,retail,
store,display,merchandise,
Womaninwhiteshirt c : shoeshop,c : shoeshop
fashion,sales,shoppingcenter, zs ft
holdingblueshoe. shoeshop (SUN397)
mall,departmentstore,
commercial,consumer.
Gymnastics,acrobatics,high c : unevenbars, parallelbars
Personontrampoline. zs
jumps,flips,andaerialstunts. c : parallelbars (UCF101)
ft
Figure7. Examplesofimage-descriptionTimage generatedfromimagexandgroup-descriptionTgroup generatedfromimagegroup
xgroup,andtwotypesofpseudo-labels:zero-shotc andfine-tuningc .Theclass-descriptionisgeneratedbysubstitutingthepseudo-label
zs ft
c∈{c ,c }intoapredefinedtemplate:“a photo of a [c].”.
zs ft
tothepresenceofincorrectimagesinthegroup. Thisrobust- addingmoreimagesresultsinlowerresolutionperimage.
nessisfurtherevidencedbytheperformanceofLATTECLIP, Thiscouldexplaintheperformanceplateauondatasetswith
which remains competitive even when relying on pseudo- moreimagedetails,suchasUCF101orSUN397.
labelsforimageselectioninsteadofground-truthlabels.
D.Additionalresults
Numberofimagespergroup. Tab.5analyzestheper-
formanceasthenumberofimagespergroupusedforgener- Examples of LMM-synthetic texts and pseudo-labels.
atinggroup-descriptionincreases. Generally,moreimages Fig.7illustratesexamplesofimage-descriptionTimageand
pergroupleadtohigherperformanceonmostdatasets. This group-descriptionTgroupgeneratedfromindividualimages
isintuitive,asmoreimagesprovidericherinformationand x and image groups xgroup, respectively. The figure also
a higher chance of including correct images. Using only presentsground-truthlabels(GT)alongwithpseudo-labels
twoimagesresultsintheworstperformancebecauseselect- derivedfromthefrozenCLIPmodel(c )andthefine-tuning
zs
ingawrongimagewouldsignificantlyimpacttheoutcome, model (c ). Note that the class-description is generated
ft
making50%or100%oftheselectedimagesincorrect. Con- by substituting the pseudo-label c ∈ {c ,c } into a pre-
zs ft
sequently, larger groups are more robust to the inclusion definedtemplate: “a photo of a [c].”. Combining
of wrong images. As LLAVA [34] has a fixed resolution, bothtypesofpseudo-labelsincreasesthechanceofcapturing
10theground-truthlabel,aseachtypeofpseudo-labeliscorrect [13] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao
fordifferentexamples. Forinstance,c iscorrectforrows Fang,YongfengZhang,HongshengLi,andYuQiao. Clip-
zs
2,3,and4,whilec iscorrectforrows1and4. Regarding adapter:Bettervision-languagemodelswithfeatureadapters.
ft
thesyntheticdescription,Tgroup providesrichercontextual IJCV,2024. 2
information,particularlyinrows1,2,4,and5,andcontains [14] MuhammadWaleedGondal,JochenGast,InigoAlonsoRuiz,
lesshallucinatedinformationcomparedtoTimage,asseen RichardDroste,TommasoMacri,SurenKumar,andLuitpold
Staudigl. Domainalignedclipforfew-shotclassification. In
inrows2and3,withgreateraccuracyinrows1,4,and5.
WACV,2024. 2
[15] SachinGoyal, AnanyaKumar, SankalpGarg, ZicoKolter,
References
andAditiRaghunathan.Finetunelikeyoupretrain:Improved
finetuningofzero-shotvisionmodels. InCVPR,2023. 1,2,
[1] AgnarAamodtandEnricPlaza. Case-basedreasoning:Foun-
3,5,6,9
dational issues, methodological variations, and system ap-
proaches. AIcommunications,1994. 2 [16] HasanAbedAlKaderHammoud,HaniItani,FabioPizzati,
PhilipTorr,AdelBibi,andBernardGhanem. Synthclip:Are
[2] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,
wereadyforafullysyntheticcliptraining? arXiv,2024. 2,3
IlgeAkkaya,FlorenciaLeoniAleman,DiogoAlmeida,Janko
[17] KaimingHe,HaoqiFan,YuxinWu,SainingXie,andRoss
Altenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4
Girshick. Momentumcontrastforunsupervisedvisualrepre-
technicalreport. arXiv,2023. 3
sentationlearning. InCVPR,2020. 2,5
[3] AI Anthropic. The claude 3 model family: Opus, sonnet,
[18] Patrick Helber, Benjamin Bischke, Andreas Dengel, and
haiku. Claude-3ModelCard,2024. 1
DamianBorth. Eurosat: Anoveldatasetanddeeplearning
[4] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,Xi-
benchmarkforlanduseandlandcoverclassification. JSTAR,
aodongDeng, YangFan, WenbinGe, YuHan, FeiHuang,
2019. 6,9
BinyuanHui,LuoJi,MeiLi,JunyangLin,RunjiLin,Dayi-
[19] LukasHoyer,DengxinDai,HaoranWang,andLucVanGool.
hengLiu,GaoLiu,ChengqiangLu,KemingLu,JianxinMa,
Mic:Maskedimageconsistencyforcontext-enhanceddomain
Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan,
adaptation. InCVPR,2023. 1
SinanTan,JianhongTu,PengWang,ShijieWang,WeiWang,
[20] Xuefeng Hu, Ke Zhang, Lu Xia, Albert Chen, Jiajia Luo,
ShengguangWu,BenfengXu,JinXu,AnYang,HaoYang,
YuyinSun,KenWang,NanQiao,XiaoZeng,MinSun,etal.
JianYang,ShushengYang,YangYao,BowenYu,Hongyi
Reclip:Refinecontrastivelanguageimagepre-trainingwith
Yuan,ZhengYuan,JianweiZhang,XingxuanZhang,Yichang
sourcefreedomainadaptation. InWACV,2024. 2,3,6,9
Zhang,ZhenruZhang,ChangZhou,JingrenZhou,Xiaohuan
[21] JiaxingHuang, DayanGuan, AoranXiao, andShijianLu.
Zhou,andTianhangZhu. Qwentechnicalreport,2023. 1
Modeladaptation: Historicalcontrastivelearningforunsu-
[5] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.
perviseddomainadaptationwithoutsourcedata. InNeurIPS,
Food-101–miningdiscriminativecomponentswithrandom
2021. 2
forests. InECCV,2014. 6,9
[22] GabrielIlharco,MitchellWortsman,RossWightman,Cade
[6] MirceaCimpoi,SubhransuMaji,IasonasKokkinos,Sammy
Gordon,NicholasCarlini,RohanTaori,AchalDave,Vaishaal
Mohamed,andAndreaVedaldi. Describingtexturesinthe
Shankar,HongseokNamkoong,JohnMiller,HannanehHa-
wild. InCVPR,2014. 6,9
jishirzi,AliFarhadi,andLudwigSchmidt.Openclip,jul2021.
[7] MohammadFahes,Tuan-HungVu,AndreiBursuc,Patrick
6,9
Pérez,andRaouldeCharette. Poda:Prompt-drivenzero-shot
[23] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,
domainadaptation. InICCV,2023. 1
HieuPham,QuocLe,Yun-HsuanSung,ZhenLi,andTom
[8] MohammadFahes,Tuan-HungVu,AndreiBursuc,Patrick
Duerig. Scalingupvisualandvision-languagerepresentation
Pérez,andRaouldeCharette. Asimplerecipeforlanguage- learningwithnoisytextsupervision. InICML,2021. 1
guideddomaingeneralizedsegmentation. InCVPR,2024.
[24] Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux,
1
Arthur Mensch, Blanche Savary, Chris Bamford, Deven-
[9] MohammadFahes,Tuan-HungVu,AndreiBursuc,Patrick draSinghChaplot,DiegodelasCasas,EmmaBouHanna,
Pérez,andRaouldeCharette. Fine-tuningclip’slastvisual Florian Bressand, Gianna Lengyel, Guillaume Bour, Guil-
projector:Afew-shotcornucopia. arXiv,2024. 2 laumeLample,LélioRenardLavaud,LucileSaulnier,Marie-
[10] LijieFan,DilipKrishnan,PhillipIsola,DinaKatabi,andYon- AnneLachaux,PierreStock,SandeepSubramanian,Sophia
glongTian. Improvingcliptrainingwithlanguagerewrites. Yang,SzymonAntoniak,TevenLeScao,ThéophileGervet,
InNeurIPS,2023. 2,3 Thibaut Lavril, Thomas Wang, Timothée Lacroix, and
[11] LiFei-Fei,RobFergus,andPietroPerona. Learninggener- WilliamElSayed. Mixtralofexperts,2024. 1
ativevisualmodelsfromfewtrainingexamples: Anincre- [25] JonathanKrause, MichaelStark, JiaDeng, andLiFei-Fei.
mental bayesian approach tested on 101 object categories. 3dobjectrepresentationsforfine-grainedcategorization. In
CVPRW,2004. 6,9 ICCVW,2013. 6,9
[12] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal [26] AnanyaKumar,AditiRaghunathan,RobbieMatthewJones,
Germain,HugoLarochelle,FrançoisLaviolette,MarioMarch, TengyuMa, andPercyLiang. Fine-tuningcandistortpre-
andVictorLempitsky. Domain-adversarialtrainingofneural trained features and underperform out-of-distribution. In
networks. JMLR,2016. 1 ICML,2022. 2,3
11[27] JogendraNathKundu,AkshayRKulkarni,SuvaanshBham- [44] OrPatashnik,ZongzeWu,EliShechtman,DanielCohen-Or,
bri,DeepeshMehta,ShreyasAnandKulkarni,VarunJampani, andDaniLischinski. Styleclip:Text-drivenmanipulationof
andVenkateshBabuRadhakrishnan.Balancingdiscriminabil- styleganimagery. InICCV,2021. 1
ityandtransferabilityforsource-freedomainadaptation. In [45] QiQian,YuanhongXu,andJuhuaHu. Intra-modalproxy
ICML,2022. 2 learning for zero-shot visual categorization with clip. In
[28] ZhengfengLai,HaotianZhang,BowenZhang,WentaoWu, NeurIPS,2023. 2
HaopingBai,AlekseiTimofeev,XianzhiDu,ZheGan,Jiu- [46] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
longShan,Chen-NeeChuah,YinfeiYang,andMengCao. Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Veclip:Improvingcliptrainingviavisual-enrichedcaptions. AmandaAskell,PamelaMishkin,JackClark,etal. Learning
InECCV,2024. 2,3 transferablevisualmodelsfromnaturallanguagesupervision.
[29] Dong-HyunLeeetal. Pseudo-label:Thesimpleandefficient InICML,2021. 1,2,5,6
semi-supervisedlearningmethodfordeepneuralnetworks.
[47] Karsten Roth, Jae Myung Kim, A. Sophia Koepke, Oriol
InICMLW,2013. 2,3,6,9
Vinyals, Cordelia Schmid, and Zeynep Akata. Waffling
[30] JunnanLi,RamprasaathSelvaraju,AkhileshGotmare,Shafiq aroundforperformance: Visualclassificationwithrandom
Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align wordsandbroadconcepts. InICCV,2023. 1
beforefuse:Visionandlanguagerepresentationlearningwith
[48] ChristophSchuhmann,RomainBeaumont,RichardVencu,
momentumdistillation. InNeurIPS,2021. 1
CadeGordon,RossWightman,MehdiCherti,TheoCoombes,
[31] JingjingLi, ZhiqiYu, ZhekaiDu, LeiZhu, andHengTao
AarushKatta,ClaytonMullis,MitchellWortsman,Patrick
Shen. Acomprehensivesurveyonsource-freedomainadap-
Schramowski,SrivatsaKundurthy,KatherineCrowson,Lud-
tation. TPAMI,2024. 2
wigSchmidt,RobertKaczmarczyk,andJeniaJitsev. Laion-
[32] RuiLi, QianfenJiao, WenmingCao, Hau-SanWong, and 5b:Anopenlarge-scaledatasetfortrainingnextgeneration
SiWu. Modeladaptation:Unsuperviseddomainadaptation image-textmodels,2022. 1,3
withoutsourcedata. InCVPR,2020. 2
[49] ManliShu,WeiliNie,De-AnHuang,ZhidingYu,TomGold-
[33] JianLiang,DapengHu,andJiashiFeng. Dowereallyneed
stein, Anima Anandkumar, and Chaowei Xiao. Test-time
to access the source data? source hypothesis transfer for
prompttuningforzero-shotgeneralizationinvision-language
unsuperviseddomainadaptation. InICML,2020. 2
models. InNeurIPS,2022. 3
[34] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
[50] JulioSilva-Rodriguez,SinaHajimiri,IsmailBenAyed,and
Visualinstructiontuning. InNeurIPS,2023. 1,3,4,8,10
JoseDolz. Acloserlookatthefew-shotadaptationoflarge
[35] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste
vision-languagemodels. InCVPR,2024. 1,2
Bellot-Gurlet,TaylorMordan,andAlexandreAlahi. Ttt++:
[51] JakeSnell,KevinSwersky,andRichardZemel. Prototypical
Whendoesself-supervisedtest-timetrainingfailorthrive?
networksforfew-shotlearning. InNeurIPS,2017. 2
InNeurIPS,2021. 3
[52] KhurramSoomro,AmirRoshanZamir,andMubarakShah.
[36] YanqingLiu, KaiWang, WenqiShao, PingLuo, YuQiao,
Ucf101:Adatasetof101humanactionsclassesfromvideos
MikeZhengShou,KaipengZhang,andYangYou. Mllms-
inthewild. arXiv,2012. 6,9
augmentedvisual-languagerepresentationlearning. arXiv,
2023. 4 [53] YuSun,XiaolongWang,ZhuangLiu,JohnMiller,Alexei
Efros, and Moritz Hardt. Test-time training with self-
[37] Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew
supervisionforgeneralizationunderdistributionshifts. In
Blaschko, and Andrea Vedaldi. Fine-grained visual clas-
ICML,2020. 3
sificationofaircraft. arXiv,2013. 6,9
[38] Ségolène Martin, Yunshi Huang, Fereshteh Shakeri, Jean- [54] ZhenTan,AlimohammadBeigi,SongWang,RuochengGuo,
ChristophePesquet,andIsmailBenAyed. Transductivezero- AmritaBhattacharjee,BohanJiang,MansoorehKarami,Jun-
shotandfew-shotclip. InCVPR,2024. 2 dongLi,LuCheng,andHuanLiu. Largelanguagemodels
fordataannotation:Asurvey. arXiv,2024. 3
[39] AllenNewell,HerbertAlexanderSimon,etal. Humanprob-
lemsolving,volume104. Prentice-hallEnglewoodCliffs,NJ, [55] Gemini Team. Gemini 1.5: Unlocking multimodalunder-
1972. 2 standingacrossmillionsoftokensofcontext,2024. 1
[40] Maria-ElenaNilsbackandAndrewZisserman. Automated [56] HugoTouvron,ThibautLavril,GautierIzacard,XavierMar-
flowerclassificationoveralargenumberofclasses. InIndian tinet,Marie-AnneLachaux,TimothéeLacroix,BaptisteRoz-
conferenceoncomputervision,graphics&imageprocessing, ière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien
2008. 6,9 Rodriguez,ArmandJoulin,EdouardGrave,andGuillaume
[41] OpenAI. Gpt-4technicalreport,2024. 1 Lample. Llama: Open and efficient foundation language
[42] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,Car- models,2023. 1
rollWainwright,PamelaMishkin,ChongZhang,Sandhini [57] Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Ol-
Agarwal, Katarina Slama, Alex Ray, et al. Training lan- shausen,andTrevorDarrell. Tent:Fullytest-timeadaptation
guagemodelstofollowinstructionswithhumanfeedback. In byentropyminimization. InICLR,2021. 3
NeurIPS,2022. 3 [58] GuoqiangWei,CuilingLan,WenjunZeng,andZhiboChen.
[43] OmkarMParkhi,AndreaVedaldi,AndrewZisserman,and Metaalign:Coordinatingdomainalignmentandclassification
CVJawahar. Catsanddogs. InCVPR,2012. 6,9 forunsuperviseddomainadaptation. InCVPR,2021. 1
12[59] YixuanWei,HanHu,ZhendaXie,ZeLiu,ZhengZhang,Yue
Cao,JianminBao,DongChen,andBainingGuo. Improving
clipfine-tuningperformance. InICCV,2023. 2
[60] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim,
MikeLi,SimonKornblith,RebeccaRoelofs,RaphaelGon-
tijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok
Namkoong, etal. Robustfine-tuningofzero-shotmodels.
InCVPR,2022. 1,2,3,4
[61] JianxiongXiao,JamesHays,KristaAEhinger,AudeOliva,
andAntonioTorralba.Sundatabase:Large-scalescenerecog-
nitionfromabbeytozoo. InCVPR,2010. 6,9
[62] LeweiYao,RunhuiHuang,LuHou,GuansongLu,Minzhe
Niu,HangXu,XiaodanLiang,ZhenguoLi,XinJiang,and
ChunjingXu. Filip:Fine-grainedinteractivelanguage-image
pre-training. InICLR,2022. 2
[63] JiahuiYu,ZiruiWang,VijayVasudevan,LeggYeung,Mo-
jtabaSeyedhosseini, andYonghuiWu. Coca: Contrastive
captionersareimage-textfoundationmodels. TMLR,2022. 2
[64] MaximeZanellaandIsmailBenAyed. Low-rankfew-shot
adaptationofvision-languagemodels. InCVPR,2024. 2
[65] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and
LucasBeyer. Sigmoidlossforlanguageimagepre-training.
InICCV,2023. 2
[66] RenruiZhang,WeiZhang,RongyaoFang,PengGao,Kun-
chang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-
adapter:Training-freeadaptionofclipforfew-shotclassifica-
tion. InECCV,2022. 2
[67] ShuaiZhao,XiaohanWang,LinchaoZhu,andYiYang. Test-
timeadaptationwithcliprewardforzero-shotgeneralization
invision-languagemodels. InICLR,2024. 3
[68] KaiyangZhou,JingkangYang,ChenChangeLoy,andZiwei
Liu. Learningtopromptforvision-languagemodels. IJCV,
2022. 1,2,6
[69] XiangyangZhu,RenruiZhang,BoweiHe,AojunZhou,Dong
Wang, Bin Zhao, and Peng Gao. Not all features matter:
Enhancingfew-shotclipwithadaptivepriorrefinement. In
ICCV,2023. 2
13