Gaussian Process Thompson Sampling via Rootfinding
TaiwoA.Adebiyi BachDo RudaZhang
UniversityofHouston UniversityofHouston UniversityofHouston
taadebiyi2@uh.edu bdo3@uh.edu rudaz@uh.edu
Abstract
Thompson sampling (TS) is a simple, effective stochastic policy in Bayesian
decision making. It samples the posterior belief about the reward profile and
optimizesthesampletoobtainacandidatedecision.Incontinuousoptimization,the
posterioroftheobjectivefunctionisoftenaGaussianprocess(GP),whosesample
pathshavenumerouslocaloptima,makingtheirglobaloptimizationchallenging.
In this work, we introduce an efficient global optimization strategy for GP-TS
thatcarefullyselectsstartingpointsforgradient-basedmulti-startoptimizers. It
identifiesalllocaloptimaofthepriorsampleviaunivariateglobalrootfinding,and
optimizestheposteriorsampleusingadifferentiable,decoupledrepresentation. We
demonstrateremarkableimprovementintheglobaloptimizationofGPposterior
samples,especiallyinhighdimensions.Thisleadstodramaticimprovementsinthe
overallperformanceofBayesianoptimizationusingGP-TSacquisitionfunctions,
surprisinglyoutperformingalternativeslikeGP-UCBandEI.
1 Introduction
Bayesianoptimization(BO)isahighlysuccessfulapproachtotheglobaloptimizationofexpensive-
to-evaluateblack-boxfunctions[1]. Effectively,therearetwonestediterationsinBO:theouter-loop
seekstooptimizetheobjectivefunctionf(x);andtheinner-loopseekstooptimizetheacquisition
functionα(x)ateachstage. ThepremiseofBOisthattheinner-loopoptimizationcanbesolved
accuratelyandefficiently,sothattheouter-loopproceedsinformativelywithanegligibleaddedcost.
Infact,theconvergenceguaranteesofmanyBOstrategiesassumeexactglobaloptimizationofthe
acquisitionfunction[2–4]. However,thisismorechallengingthancommonlyassumed[5].
GaussianprocessThompsonsampling(GP-TS)[6]usesposteriorsamplesdirectlyasacquisition
functions. Ithasstrongtheoreticalguarantees[4],scalestohighdimensions[7],andcanbeeasily
parallelized[8,9,6]. Itisalsousedincomputinginformation-theoreticacquisitionfunctionssuchas
predictiveentropysearch[10]andmax-valueentropysearch[11]. Butposteriorsamplefunctionsare
notoriouslydifficulttooptimizeduetotheircomplexity.
WepresentanefficientstrategythatgloballyoptimizesGP-TSacquisitionfunctionsbyjudiciously
selecting starting points for gradient-based multi-start optimizers. It exploits the separability of
multivariateGPpriorsandthedecompositionofGPposteriorsamplesper“Matheron’srule”[12].
The former allows for the identification of all local minima of a GP prior sample using a robust
rootfindingalgorithm;thelatterlinksthepriorsampleandthedatatoaposteriorsample,relatestheir
criticalpoints,andfacilitatestheselectionofstartingpoints.
2 SpectralRepresentationofGaussianProcesses
GaussianProcesses.GivenatrainingdatasetD ={(X,y)}={(xi,yi)}N ,wherexiisaninput
i=1
locationandyi isthecorrespondingobservation. Defineanobservationmodely(x) = f(x)+ϵ,
WorkshoponBayesianDecision-makingandUncertainty,38thConferenceonNeuralInformationProcessing
Systems(NeurIPS2024).
4202
tcO
01
]GL.sc[
1v17080.0142:viXrawhere f(x) is an unknown objective function and ϵ ∼iid N(0,σ2) is independent and identically
n
distributed(iid)zero-meanGaussiannoise.
AGPassumesthatanyfinitesubsetoffunctionvalueshasajointGaussiandistribution[13]. This
assumption is encoded in the GP prior f(x) ∼ GP(m(x),κ(x,x′)), where m(x) is the mean
functionandκ(x,x′)apositivedefinitecovariancefunction. ConditioningtheGPprioronthedata
provides a posterior that is also a GP. Assuming m(x) = 0, the GP posterior can be written as
f(x )|D ∼ GP(m(x ),κ(x ,x′)). Heretheposteriormeanm(x ) = k⊺(x ,X)C−1y andthe
⋆ (cid:98) ⋆ (cid:98) ⋆ ⋆ (cid:98) ⋆ ⋆
posteriorcovarianceκ(x ,x′)=κ(x ,x′)−k⊺(x ,X)C−1k(x′,X),whereC=K+σ2Iwith
K
=κ(xi,xj)(i,j(cid:98) ∈{⋆ 1..⋆ .,N}),a⋆ ndk⋆
(x
,X)=⋆ (cid:2)
κ(x
,x1),.⋆
..,κ(x
,xN)(cid:3)⊺
.
n
ij ⋆ ⋆ ⋆
SpectralRepresentationofGaussianProcesses.PerMercer’stheoremonprobabilityspaces[13],
anypositivedefinitecovariancefunctionthatisessentiallyboundedwithrespecttosomeprobability
measureµonthedomainX hasaspectralrepresentationκ(x,x′)=(cid:80)∞ λ ϕ (x)ϕ (x′),where
k=0 k k k
(λ ,ϕ (x))isapairofeigenvalueandeigenfunctionofthekernelintegraloperator.Asamplefunction
k k
√
fromtheGPpriorcanthusbewrittenasf(x)=(cid:80)∞
w λ ϕ (x),wherew
∼iid
N(0,1).
i=0 k k k k
DecoupledRepresentationofGPPosteriors.GivenaGPpriorsamplef(x),wecandetermine
aposteriorsamplef(cid:101)(x)usingadecoupledrepresentationthatupdatesthepriorsampleaccording
toMatheron’srule[12]. ForobservationscontaminatedbyiidGaussiannoise,wehavef(cid:101)(x ⋆) =
f(x )+k⊺(x ,X)C−1(y−f −ϵ),wheref =(cid:2) f(x1),...,f(xN)(cid:3)⊺ andϵ∼iid N (0,σ2I ).
⋆ ⋆ N n N
3 GlobalOptimizationtoThompsonSamplingAcquisitionFunctions
Assumptions.WeuseaGPpriorwithaseparablecovariancefunction. Werequirethattheunivariate
components of this covariance function either have known spectral representations per Mercer’s
theorem,orhaveknownspectraldensitiesperBochner’stheoremwithaneffectivediscretization
(seee.g.,[14,7]),andthatthecorrespondingsamplesarecontinuouslydifferentiable. Thesquared
exponential(SE)covariancefunctionmeetstheserequirementsandisofourfocusinthefollowing.
WefurtherassumethattheobjectivefunctionisdefinedonahypercubeX
=(cid:81)d
[x ,x ].
i=1 i i
SpectrumofSECovarianceFunction.ConsidertheunivariateSEcovariancefunctionκ(x,x′;l)=
exp(cid:0) −1(x−x′)2/l2(cid:1)
, where l is the characteristic length scale. Per Mercer’s theorem, it has a
2
spectralrepresentationκ(x,x′) = (cid:80)∞ λ ϕ (x)ϕ (x′). ForaGaussianmeasureµ = N(0,σ2)
k=0 k k k √
over the real line, let a = (2σ2)−1, b = (2l)−1, c = a2+4ab, and A = 1a + b + 1c.
2 2
For k ∈ N, the kth eigenvalue is λ = (cid:112) a/A(b/A)k and the corresponding eigenfunction is
k
√
ϕ (x) = (πc/a)1/4ψ ( cx)exp(cid:0)1ax2(cid:1) , where ψ (x) = (cid:0) π1/22kk!(cid:1)−1/2 H (x)exp(cid:0) −1x2(cid:1)
k k 2 k k 2
andH (x) = (−1)kexp(x2) dk exp(−x2)thekth-orderHermitepolynomial(seee.g.,[15]Sec.
k dxk
4). Inageneralcasewithk = (k ,...,k ), λ =
(cid:81)d
λ andϕ (x) =
(cid:81)d
ϕ (x ), where
1 d k j=1 kj k j=1 kj j
λ andϕ (x)aretheeigenvaluesandeigenfunctionsofthed-variatecovariancefunctionκ(x,x′),
k k
respectively.
PriorSampleFunctions.Theseparabilityofthecovariancefunctionandtheknownspectralrep-
resentations of the component covariance functions allow us to accurately approximate the prior
sample as f(x) ≈
(cid:81)d (cid:80)Ni−1w (cid:112)
λ ϕ(x ). Here N is selected for each variate such that
i=0 k=1 i,k i,k i i
λ /λ ≤η ,whereη issufficientlysmall,e.g.,η =10−16.
i,Ni−1 i,1 i i i
PropertiesofPosteriorSampleFunctions.WiththedecoupledrepresentationofGPposteriors,each
posteriorsamplehastheformoff(cid:101)(x)=f(x)+b(x).
Here,thepriorsamplef(x)=(cid:81)d
i=1f i(x i)
isfast-varyingandseparable,enablingtheuseofefficientunivariateroot-findingalgorithms[16]
toidentifyallitscriticalpoints. Thedataadjustmentb(x) = (cid:80)N v κ(x,xj),wherev ∈ R,is
j=1 j j
aweightedsumofcanonicalbasisfunctions. Whilenotseparable,itissmoother,hasmuchfewer
criticalpointsthanthepriorsample,andhaslimitedeffectawayfromdatapoints.
CriticalPointsofMultivariateSeparableFunctions.Thecriticalpointsofthemultivariateseparable
priorsamplef(x)=(cid:81)d
f (x )areexactlythecriticalpointsofitsunivariatecomponentsf (x ),
i=1 i i i i
arbitrarilycombined,exceptforwhenf(x) = 0. Asaresult,wecanfindalltherelevantcritical
2(a) (b)
Figure1:Illustrationofexplorationandexploitation setsforglobaloptimizationofGP-TSacquisition
functions. (a)WhentheglobalminimumoftheGP-TSacquisitionfunctionliesoutsidetheinterpola-
tionregion,itistypicallyiden tifiedbystartingthegradient-basedoptimizeratalocalminimumof
thepriorsample. (b)Whentheglobalminimumiswithintheinterpolationregion,itcanbefoundby
startingthegradient-basedoptimizerateitheradatapointoralocalminimumofthepriorsample.
pointsofthepriorsamplefunctionf(x)bysolvingaglobalrootfindingproblemforthederivative
ofeachofitsunivariatecomponents: f′(x )=0,i∈{1,··· ,d}. Wealsoaddtheupperandlower
i i
boundsofeachvariablex tothesetofcriticalpointsoff ,astheycandefinetheextremaoff(x)on
i i
theboundeddomain. Let{ζ }ri representthesetofcriticalpointsoff (x ).
i,j j=1 i i
LocalMinimaofMultivariateSeparableFunctions.Giventheunivariatecriticalpoints,identifying
alocalminimumofamultivariateseparablepriorsamplef(x)isstraightforward.Letx=(ζ )d
i,s(i) i=1
beanarbitrarycombinationoftheunivariatecriticalpoints. Ifxisaninteriorpoint, itisalocal
minimumif∇2f(x) ≻ 0,whichhastheform∇2f(x) = diag{(cid:81) f (x )f′′(x )}d . Ifxisa
i̸=j j j i i i=1
boundarypoint,thecriterionisslightlymodified. Withoutenumeratingallcombinations,bestsubsets
ofthelocalminimaS canbeefficientlyidentifiedasfollows: (1)filterthecriticalpointsforlocal
min
extrema,exploitingthestructuresof∇2f(x)and∇f(x);(2)selectthelocalextremawiththelargest
|f|,usingamaxheapdatastructure;and(3)localminimahavenegativef values.
GlobalOptimizationtoThompsonSampling.TogloballyoptimizeaGP-TSacquisitionfunction
ineachBOiteration,weusetwosetsofstartingpointsforagradient-basedmulti-startoptimizer,
namelyexploitationS andexplorationS . HereS containsthedatapoints,whileS iseitherS ,
p e p e min
orasubsetofS whenthenumberofitsmembersistoolarge,e.g.,>1000. Figure1illustrates
min
theexplorationandexploitationsetsforglobaloptimizationoftwoGP-TSacquisitionfunctions.
Ouroptimizationstrategyismotivatedbyafewobservations. Whenthepriorsamplef isaddedto
thesmootherlandscapeofthedataadjustmentb,eachlocalminimumoff willbelocatednearbya
localminimumoftheposteriorsamplef(cid:101). Searchingfromdatapointscandiscovergoodlocalminima
off(cid:101)inthevicinityofthedataset,whichcanpickupsomelocalminimanotreadilydiscoveredbythe
localminimaoff. Thisisespeciallytrueiff isrelativelyflatnearadatapoint. Startingfromboth
S eandS pcanthusgivesufficientcoverageofthebestlocalminimaoff(cid:101),leadingtoefficientglobal
optimizationofGP-TSacquisitionfunctions.
4 Experiments
Inner-LoopOptimization.Weminimizethe2dSchwefeland10dLevyfunctions[17]toassess
thequalityoftheinner-loopsolutionsrecommendedbyourproposedmethod. Westartwith10d
datapoints,normalizetheinputdatato[−1,1]d,andstandardizetheoutputdata. Wesetσ =1and
σ = 10−6. Forcomparison,weoptimizethesameGP-TSacquisitionfunctionsusingagradient-
n
based multi-start optimizer with random starting points (i.e., random multi-start) and a genetic
algorithm. Thenumberofstartingpointsfortherandommulti-startandthepopulationsizeofthe
geneticalgorithmareequaltothenumberofstartingpointsofourmethod. Thesamestoppingcriteria
areusedforthethreealgorithms.
32d Schwefel (a) 10d Levy
2d Schwefel (b) 10d Levy
(a)
(b)
Figure2: Optimizationresultsfor(a)2dSchwefeland(b)10dLevyfunctions. Top-left: Cumulative
distancesbetweennewcandidatesolutionsx⋆ tothetrueglobalminimumsxt oftheGP-TSacquisi-
k k
tionfunctionsf(cid:101)(x)for2dSchwefelfunction. Bottom-left: Cumulativeoptimizedvaluesf(cid:101)⋆for10d
k
Levyfunction. Middle: Cumulativeruntimet
k
requiredforoptimizingf(cid:101)(x). Right: Historiesof
mediansandinterquartilerangesofsolutionsfrom20runsofourmethod,TS-RF,EI,andLCB.
Figure2comparestheinner-loopsolutionsandCPUtimesforinner-loopoptimizationbyourmethod,
randommulti-start,andgeneticalgorithm. Forbothproblems,ourmethodoutperformstherandom
multi-startandgeneticalgorithmintermsoftheinner-loopsolutionqualityandtheruntimerequired
foroptimizingGP-TSacquisitionfunctions. Thisverifiestheimportanceofjudiciousselectionof
startingpointsforoptimizingGP-TSacquisitionfunctionsinbothlowandhighdimensions.
Outer-LoopOptimization.Wecomparetheouter-loopsolutionsbyourmethodwiththosebyother
BOmethods,includingTSwithrandomFourierfeatures(TS-RF)[18,10],expectedimprovement(EI)
[19],andlowerconfidencebound(LCB)—theversionofGP-UCB[2]forminimization. Theinner-
loopoptimizationforTS-RF,EI,andLCBisperformedviaagradient-basedmulti-startoptimizer
with random starting points. The number of starting points and the termination criteria for this
optimizerarethesameasthoseforourmethod. IneachBOiteration,werecordthelogsimpleregret
log(y −f⋆),wherey isthebestobservationuptothatiterationandf⋆isthetrueminimumof
min min
theobjectivefunction.
Figure2showsthemediansandinterquartilerangesofsolutionsfrom20runsofeachBOmethodfor
thetestfunctions. Onthe2dSchwefelfunction,ourmethodcanachievebetterobjectivefunction
valuesthanallotherconsideredmethods. Italsoprovidesacompetitiveresultinoptimizingthe10d
Levyfunction. Acrossthetwoexamples,EIandLCBtendtoperformwellintheinitialiterations,
whileourmethodshowsfastimprovementinlateriterations, highlightingtheexploratorynature
anddelayedrewardoftheGP-TSpolicy. Consideringrobustnesstotheobjectivefunction,GP-TS
(perhapssurprisingly)outperformsEIandLCB,whenoptimizedusingourmethod.
5 Summary
We propose a method to optimize GP-TS acquisition functions globally. The method relies on
local minima of prior samples obtained from a univariate rootfinding algorithm, the data points,
andagradient-basedmulti-startoptimizerwithcarefullyselectedstartingpoints. Itseffectiveness
is supported by the prevalent use of separable covariance functions in BO, where the univariate
covariance components is expressed in terms of their spectral representations. The optimization
resultsshowthattheproposedmethodoffershigher-qualitysolutionstooptimizingGP-TSacquisition
functionsinbothlow-andhigh-dimensionalsettings,comparedtoarandommulti-startandagenetic
algorithm. Italsoshowsdramaticimprovementsinouter-loopoptimization.
4References
[1] RomanGarnett. BayesianOptimization. CambridgeUniversityPress,Cambridge,2023. doi:
10.1017/9781108348973.
[2] NiranjanSrinivas,AndreasKrause,ShamM.Kakade,andMatthiasSeeger. Gaussianprocess
optimizationinthebanditsetting: Noregretandexperimentaldesign. InProceedingsofthe
27thInternationalConferenceonMachineLearning,volume13,pages1015–1022,2010. URL
https://icml.cc/Conferences/2010/papers/422.pdf.
[3] Adam D. Bull. Convergence rates of efficient global optimization algorithms. Journal of
Machine Learning Research, 12(88):2879–2904, 2011. URL http://jmlr.org/papers/
v12/bull11a.html.
[4] DanielRussoandBenjaminVanRoy.Learningtooptimizeviaposteriorsampling.Mathematics
ofOperationsResearch,39(4):1221–1243,2014. doi: 10.1287/moor.2014.0650.
[5] James Wilson, Frank Hutter, and Marc Deisenroth. Maximizing acquisition functions for
Bayesianoptimization. InAdvancesinNeuralInformationProcessingSystems,volume31,
pages 9884–9895, 2018. URL https://proceedings.neurips.cc/paper/2018/hash/
498f2c21688f6451d9f5fd09d53edda7-Abstract.html.
[6] KirthevasanKandasamy,AkshayKrishnamurthy,JeffSchneider,andBarnabasPoczos. Par-
allelisedBayesianoptimisationviaThompsonsampling. InProceedingsoftheTwenty-First
InternationalConferenceonArtificialIntelligenceandStatistics,volume84,pages133–142,
2018. URLhttps://proceedings.mlr.press/v84/kandasamy18a.html.
[7] MojmirMutnyandAndreasKrause. EfficienthighdimensionalBayesianoptimizationwith
additivity and quadrature Fourier features. In Advances in Neural Information Processing
Systems,volume31,pages9005–9016,2018. URLhttps://proceedings.neurips.cc/
paper/2018/hash/4e5046fc8d6a97d18a5f54beaed54dea-Abstract.html.
[8] Amar Shah and Zoubin Ghahramani. Parallel predictive entropy search for batch global
optimizationofexpensiveobjectivefunctions. InAdvancesinNeuralInformationProcessing
Systems,volume28,pages3330–3338,2015. URLhttps://proceedings.neurips.cc/
paper_files/paper/2015/file/57c0531e13f40b91b3b0f1a30b529a1d-Paper.pdf.
[9] JoséMiguelHernández-Lobato,JamesRequeima,EdwardO.Pyzer-Knapp,andAlánAspuru-
Guzik. Parallel and distributed Thompson sampling for large-scale accelerated exploration
ofchemicalspace. InProceedingsofthe34thInternationalConferenceonMachineLearn-
ing, volume 70, pages 1470–1479, 2017. URL https://proceedings.mlr.press/v70/
hernandez-lobato17a.html.
[10] José Miguel Hernández-Lobato, Matthew W. Hoffman, and Zoubin Ghahramani. Pre-
dictive entropy search for efficient global optimization of black-box functions. In
Advances in Neural Information Processing Systems, volume 27, pages 918–926,
2014. URL https://proceedings.neurips.cc/paper_files/paper/2014/hash/
069d3bb002acd8d7dd095917f9efe4cb-Abstract.html.
[11] ZiWangandStefanieJegelka. Max-valueentropysearchforefficientBayesianoptimization.
InProceedingsofthe34thInternationalConferenceonMachineLearning,volume70,pages
3627–3635,2017. URLhttps://proceedings.mlr.press/v70/wang17e.html.
[12] JamesWilson,ViacheslavBorovitskiy,AlexanderTerenin,PeterMostowsky,andMarcDeisen-
roth. EfficientlysamplingfunctionsfromGaussianprocessposteriors. InProceedingsofthe
37thInternationalConferenceonMachineLearning,volume119,pages10292–10302,2020.
URLhttps://proceedings.mlr.press/v119/wilson20a.html.
[13] Carl Edward Rasmussen and Christopher K. I. Williams. Gaussian processes for machine
learning. TheMITPress,2006. ISBN9780521872508. doi: 10.7551/mitpress/3206.001.0001.
[14] Arno Solin and Simo Särkkä. Hilbert space methods for reduced-rank Gaussian process
regression.StatisticsandComputing,30(2):419–446,2020.doi:10.1007/s11222-019-09886-w.
5[15] HuaiyuZhu,ChristopherK.I.Williams,RichardRohwer,andMichalMorciniec. Gaussian
regression and optimal finite dimensional linear models. In Neural Networks and Machine
Learning,1998. URLhttps://publications.aston.ac.uk/id/eprint/38366/.
[16] LloydN.Trefethen. ApproximationTheoryandApproximationPractice,ExtendedEdition,
volume164. SocietyforIndustrialandAppliedMathematics,Philadelphia,PA,2019. ISBN
978-1-61197-593-2. doi: 10.1137/1.9781611975949.
[17] SonjaSurjanovicandDerekBingham. Virtuallibraryofsimulationexperiments: Testfunctions
anddatasets,2013. URLhttp://www.sfu.ca/~ssurjano.
[18] Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines.
In Advances in Neural Information Processing Systems, volume 20, pages 1177–1184,
2007. URL https://proceedings.neurips.cc/paper_files/paper/2007/file/
013a006f03dbc5392effeb8f18fda755-Paper.pdf.
[19] DonaldR.Jones,MatthiasSchonlau,andWilliamJ.Welch. Efficientglobaloptimizationof
expensiveblack-boxfunctions. JournalofGlobalOptimization,13(4):455–492,1998. doi:
10.1023/A:1008306431147.
6