Adam Exploits ℓ -geometry of Loss Landscape
∞
via Coordinate-wise Adaptivity
ShuoXie MohamadAminMohamadi ZhiyuanLi
ToyotaTechnologicalInstituteatChicago
{shuox,mohamadamin,zhiyuanli}@ttic.edu
Abstract
Adam outperforms SGD when training language models. Yet this advantage is not well-understood
theoretically–previousconvergenceanalysisforAdamandSGDmainlyfocusesonthenumberofstepsT
andisalreadyminimax-optimalinnon-convexcases,whicharebothO(cid:101)(T−1/4). Inthiswork,weargue
thattheexploitationofniceℓ -geometryisthekeyadvantageofAdamoverSGD.Morespecifically,we
∞
giveanewconvergenceanalysisforAdamundernovelassumptionsthatlossissmoothunderℓ -geometry ∞
ratherthanthemorecommonℓ -geometry,whichyieldsamuchbetterempiricalsmoothnessconstantfor
2
GPT-2andResNetmodels. OurexperimentsconfirmthatAdamperformsmuchworsewhenthefavorable
ℓ -geometryischangedwhileSGDprovablyremainsunaffected. Wealsoextendtheconvergenceanalysis
∞
toblockwiseAdamundernovelblockwisesmoothnessassumptions.
1 Introduction
Large language models (LLMs) have gained phenomenal capabilities as their scale grows (Radford et al.,
2019;Kaplanetal.,2020;Brownetal.,2020;Zhangetal.,2022a;Touvronetal.,2023;OpenAI,2023;Reid
etal.,2024). However,pre-trainingLLMsareincrediblytime-consuming. AdaptiveMomentumEstimation
(Adam)(Kingma&Ba,2014)isthecurrentto-gooptimizationalgorithmforLLMsduetoitsfastconvergence.
Incontrast,SGD,apopularandarguablythesimplestoptimizer,optimizeslanguagemodellossmuchslower
thanAdam.
However, the optimization benefit of Adam over SGD cannot be explained by existing theory. Existing
convergenceanalysesforAdamandSGDfocusonthedependenceonthenumberofstepsunderassumptionson
thesmoothnessandgradientboundsoftheloss(Défossezetal.,2022),andithasbeenshownthatbothAdam
andSGDachievetheminimaxconvergencerateO(cid:101)(T−1/4)inthenon-convexsettings(Arjevanietal.,2023).
Thusaccordingtothetheory,intheworstcase,SGDwouldbemoredesirablecomparedtoAdambecausethey
havethesameconvergencerate,andyetAdamislessmemory-efficientduetoitscoordinate-wiseadaptivity,
which needs to store the empirical moving average of second-order moments of past stochastic gradients.
Therefore,wehypothesizethatthecoordinate-wiseadaptivityinAdamisexploitingsomeunknownproperties
ofLLMswhichSGDcannotmakeuseof.
Towardsthisend,weidentifiedasignificantdifferencebetweenAdamandSGDinthispaper. Thisdifference,
often ignored in previous works, is that SGD is rotation-invariant, while Adam is only permutation-invariant
(seedefinitionsinSection2). Intuitively,thismeansifwerotatethelosslandscape,theoptimizationtrajectory
ofSGDwouldbethesame(uptosomerotation),whilethetrajectoryofAdamcouldbecompletelydifferent.
If Adam optimizes much slower more slowly after rotation, then it suggests Adam is exploiting some non-
rotational-invariantpropertiesofthelossfunction,whichisnotcapturedbystandardsmoothnessassumptions
intheconvergenceanalysis.
Figure1summarizesourfindingsbycomparingAdamontheoriginalandrotatedloss. Theperformance
of Adam on rotated loss does become much worse than Adam on the original loss. We also test a memory-
efficientandrotational-invariantvariantofSGD, AdaSGD(Wang&Wiens,2020)(definedinAlgorithm2)1.
1Thereisonesmalldifference. Weuseanexponentialaverageofthegradientformtinsteadofmomentum. Ourdefinitionmakes
AdaSGDthesameasAdaminaone-dimensionalproblem.
1
4202
tcO
01
]GL.sc[
1v89180.0142:viXraTraining Loss Evaluation Loss
4.50 4.50
AdaSGD
4.25 4.25 SGD
Adam
4.00 4.00
Rotated Adam
3.75 3.75
3.50 3.50
3.25 3.25
3.00 3.00
0 20000 40000 60000 80000 100000 0 20000 40000 60000 80000 100000
Iterations Iterations
Figure1: TrainingandevaluationlossesofAdam,AdaSGDandSGDonGPT-2. rotatedAdammeansrunning
Adamonarotatedloss. Adamontheoriginallossconvergesthefastestasexpected. ButconvergenceofAdam
onarotatedlossismuchslower,notablyevenworsethanAdaSGD.
Surprisingly, the rotated Adam performs even much worse than the SGD variant. The results suggest it
is impossible to explain the superior optimization performance of Adam over SGD just using rotationally
invariantassumptionsonthelossfunction,whichraisesthenaturalquestion,
Whatnon-rotation-invariantpropertiesoflossfunctionsenableAdamtoconvergefasterthanSGD?
Wehypothesizethattheℓ lipschitznessoflossgradientdoesnotprovideatight-enoughcharacterization
2
oflosslandscapeofdeeplearningmodelsinpractice, suchthatwecanseparateAdamandotherrotational-
invariant algorithms. Inspired by the similarity between Adam and SignGD and the fact that SignGD is the
normalized steepest descent with respect to ℓ norm, we propose to use ℓ -norm related smoothness as a
∞ ∞
bettertooltoanalyzeAdam. Inparticular,ourmainresultsusethe(1,1)-normofHessianoflossdividedby
variabledimensiondinreplacementofthespectralnormofHessianasthesmoothnessmeasure,andprovea
convergencerateofO(√1 )forAdamwithoutnoise,orO((logT)1/4)withnoise. Ourresultshavethesame
T T
dependenceonT aspreviousresults,butmuchsmallersmoothnessconstantwhenwemeasureitempirically.
We empirically verify that (1,1)-norm of Hessian positively correlates with final training loss of Adam on
both synthetic tasks like quadratic loss and real tasks like training GPT2 on OpenWebText and ResNet on
CIFAR10.
Wesummarizeourcontributionsbelow:
1. WeshowbyexperimentsthattheempiricaloptimizationadvantageofAdamoverSGDcannotexplained
solelyunderrotation-invariantassumptions. (Figure1)
2. We propose a new complexity metric for the optimization problem, which is the (1,1)-norm of the
(cid:13) (cid:13)
Hessianmatrixofloss, (cid:13)∇2L(x)(cid:13) . WepresentanovelconvergenceresultforAdamdependingon
1,1
thismetricinthecaseofβ =0. (Theorem3.5)
1
3. WefurthergeneralizethetheoreticalanalysisforAdamtoblockwiseAdam(Algorithm3)whoseconver-
genceratecanbecharacterizedbyanovelsmoothnessmeasure(Theorem3.11). AdamandAdaSGDare
twonotableexamplesofblockwiseAdam. InAdam, allblocksareofsize1. InAdaSGD, thereisonly
oneblock.
4. We empirically verify that when Adam converges more slowly on the rotated loss, the (1,1)-norm
of Hessian also increases, which suggests that our new complexity metric for Adam’s convergence is
practicallyrelevant.(Section4).2
2Thecodeisavailableathttps://github.com/mohamad-amin/adam-coordinate-adaptivity.
2
ssoL ssoLAlgorithm1Adam
Algorithm2AdaSGD
Hyperparam: β ,β ,ϵ≥0,totalstepsT,learning
1 2
Hyperparam: β ,β ,ϵ≥0,totalstepsT,learning
rate{η }T ,ϵ,initialm ,v 1 2
t t=1 0 0 rate{η }T ,initialm ,v
Input: initialization x , stochastic loss functions t t=1 0 0
0
Input: initialization x , stochastic loss functions
{L }T 0
t t=1 {L }T
v ←v t t=1
0,i 0
fort=1,2,··· ,T :
fort=1,2,··· ,T :
g ←∇ L (x )
g ←∇ L (x ) t,i i t t−1
t,i i t t−1
m ←β m +(1−β )g
m ←β m +(1−β )g t,i 1 t−1,i 1 t,i
xv
t t,
,t
i
i,i ← ←β
x2
t1 −v
t 1−
,it 1− −,i1, + ηi t( √1
m
v−
tt ,,
iiβ
+2
ϵ)1 g t2 ,it,i xv t t,i← ←β 2 xv t−t− 11 ,i+ −( η1 t−
√m
vβ
tt
+,2 i) ϵ(∥g t∥2 2/d)
returnx
T
returnx
T
2 Preliminaries
Notations. Forx ∈ Rd, wedefinethevectorp-norm∥x∥ as((cid:80) xp)1/p forp ∈ [1,∞]. Foramatrix
p i=1d i
A∈Rd1×d2,its(1,1)-norm∥A∥ 1,1isdefinedas(cid:80)d i=1 1(cid:80)d j=2 1|A i,j|anditsoperatornorminducedbyvector
p-norm∥·∥ assup ∥Ax∥q,denotedby∥A∥ ,where 1+1 =1and∥·∥ isthedualnormof∥·∥ . For
p x∈Rd ∥x∥p p q p q p
adeterministiclossfunctionL(x),weconsideroptimizationoverLwithonlyaccessindependentstochastic
functions{L (x)}T suchthatEL (x)=L(x)foranyinputx∈Rd.
t t=1 t
Rotation. For an invertible function T : Rd → Rd, T is a rotating transformation if there exists an
orthogonal matrix T ∈ Rd×d such that T(x) = Tx. T is a permutating transformation if there exists a
permutationπ : [d] → [d]suchthatT(x) = [x ,...,x ]⊤. Apermutatingtransformationisalwaysa
π(1) π(d)
rotatingtransformation. WewilluseRtodenotearotatingtransformation.
Definition2.1. Forinitializationx andstochasticlosses{L }T ,wecangetx whenrunningalgorithmA
0 t t=1 t
on(x ,{L }T ). ForatransformationT,wecanalsogetx˜ whenrunningAwiththesamehyperparameters
0 t t=1 t
on(x˜ ,{L˜ }T )withx˜ =T−1(x )andL˜ =L ◦T.
0 t t=1 0 0 t t
An algorithm A is invariantw.r.t. T if it always holds that x˜ = T−1(x ) for any hyperparameters,
t t
initializationandstochasticlosses. AnalgorithmAisrotationinvariantifitisinvariantw.r.t. anyrotating
transformationR. AndAispermutationinvariantifitisinvariantw.r.t. anypermutatingtransformation.
ThefollowingTheorem2.2showsthedifferencebetweenAdamandAdaSGD,whoseproofisinAppendixB.
Theorem2.2. SGDandAdaSGDarerotation-invariant. AdamandSignGDareonlypermutation-invariant.
3 Main Results: Convergence Rates of Adam
In this section, we present our main theoretical results, starting with a convergence analysis of Adam for
stochasticsmoothlosswithcoordinate-wisegradientnoise(Theorem3.5). Weallownon-convexlossesand
thustheconvergenceismeasuredbytheℓ normofthegradient. Fordeterministicloss,ourbestconvergence
1
rate(Theorem3.2)isachievedbySignGD(Adamwithβ =β =0). Forstochasticlosswithboundedgradient
1 2
noisevariance,ourbestrate(Corollary3.6)isachievedbyRMSProp(Adamwithβ =0andβ ∈[0,1]).
1 2
Then we extend our analysis of Adam to more general blockwise Adam (Theorem 3.11), which contains
bothAdamandAdaSGDasspecialcases. Wealsocomesupwithnovelsmoothnessmeasures(Definition3.9)
correspondingtothesetofblocksusedinblockwiseAdam.
Similartopreviousworks(Défossezetal.,2022),ouranalysiscouldbeextendedtothemostgeneralcase
of Adam, where both β , β are non-zero, but the rate becomes strictly worse than the RMSProp (the case
1 2
of β = 0), as there will be some extra polynomials of 1 . We decide not to include result for the most
1 1−β1
generalcase,ononehandforeaseofpresentation,andontheotherhand,becausesuchresultcouldexplain
3the optimization benefit of momentum (β > 0) in practice and does not add any insight on the benefit of
1
Adam. We hypothesis that we are missing some important features of loss landscape of transformers in the
theoreticalassumptionsandweleavethisforfuturework.
3.1 Warmup: SignGD(β = β = 0)
1 2
In this section, we use the convergence analysis for SignGD (Adam with β = β = 0) as a warm-up and
1 2
illustrate how Adam could benefit from a non-rotational invariant property of the loss landscape, which in
particularistheℓ smoothness. ThekeyobservationhereisthatSignGDisthenormalizedsteepestdescent
∞
withrespecttoℓ norm(see(Xie&Li,2024)),andthusitismorenaturaltoanalyzeitsconvergenceusing
∞
ℓ -norm-relatedgeometryoftheloss.
∞
Definition3.1. Givenanorm∥·∥onRd and∥·∥ asitsdualnorm,wesayafunctionLisH-smoothw.r.t.
∗
∥·∥ifforanyx,y ∈Rd,wehavethat∥∇L(x)−∇L(y)∥ ≤H∥x−y∥.
∗
Theorem 3.2. Let L be H-smooth with respect to ℓ norm and {x }T be the iterates of SignGD (Adam
∞ t t=1
withβ =β =0)onLwithinitializationx andlearningrateη,itholdsthat
1 2 0
L(x )−minL Hη
min ∥∇L(x )∥ ≤ 0 +
1≤t≤T t 1 Tη 2
(cid:113) (cid:113)
ifwechooseη = 2(L(x0)−minL),thenmin ∥∇L(x )∥ ≤ 2H(L(x0)−minL).
TH 1≤t≤T t 1 T
3.2 Mainresult: RMSProp(β = 0,β ∈ [0,1])
1 2
Itiswell-knownthatSignGDmightnotconvergeinthestochasticcaseastheexpectationofdescentdirection
formini-batchlossmaynotbeadescentdirection,andRMSPropisproposedtoaddressthisissuebyusinga
movingaverageofthesquaredgradientpercoordinatetoreducethecoorleationbetweenthedenominatorand
thenumerator,thusmakingtheexpectedupdatedirectionlessbiased(Hintonetal.,2012). Inthissubsection
weformalizetheaboveintuitionandshowindeedapositiveβ inAdamhelpsconvergenceinthestochastic
2
case. The main challenges here are from both lower bounding the first-order term and upper bounding the
second-orderterminthemodifieddescentlemma(thecounterpartofEquation8forRMSProp).
L(x t)−L(x t−1)≤−η t∇L(x t)⊤√ vg t
+ϵ
+ H
2
η t2(cid:13) (cid:13) (cid:13) (cid:13)√ vg t +ϵ(cid:13) (cid:13) (cid:13) (cid:13)2
t t ∞
(cid:13) (cid:13)2
Wecanonlyupperbound(cid:13)√gt (cid:13) by 1 withoutmorefine-grainedanalysisontherelationshipbetween
(cid:13) vt+ϵ(cid:13)
∞
1−β2
gradients in each step, which will greatly hurt the dependence of convergence rate on 1−β . However,
2
eventhoughtheupdateatsteptforonespecificcoordinateicanbeaslargeas √ 1 withsomeverylarge
1−β2
g , the average moving speed for each coordinate should be close to 1. Therefore, we introduce a slightly
t,i
strongerdefinitioninDefinition3.3,whichallowsustodecomposethesecondordertermintoeachcoordinate
accordingtoLemma3.12. Italsofacilitatestheanalysisforthecoordinate-wisefirstorderterm. Wenotethis
definitionalsoappearsinAssumption2.3oftheconcurrentworkMaladkaretal.(2024).
Definition3.3. ForanyH=(H ,...,H )∈Rd,wesayafunctionLisH-smoothcoordinate-wiselyw.r.t.
1 d
ℓ norm,iffforanyi∈[d],x,y ∈Rd,|∇ L(x)−∇ L(y)|≤H ∥x−y∥ .
∞ i i i ∞
Bydefinition, H-smoothnesscoordinate-wiselyw.r.t. ℓ
normimplies(cid:80)d
H smoothnessw.r.t. ℓ
∞ i=1 i ∞
norm. WealsoneedAssumption3.4tomeasuretheinfluenceofnoiseinstochasticsetting.
Assumption3.4(Coordinate-wisenoise). Thereexistconstantsσ suchthat
i
E(∇ L (x)−∇ L(x))2 ≤σ2
i t i i
foranyi∈[d],t∈Nandx∈Rd.
4Duetothelimitationofspace,weonlypresentthemainresulthere. Thesketchoftheproofispresented
inSection3.4. WepresentthecompleteproofforthegeneralizedblockwiseAdamalgorithminAppendixC.
TheproofincorporatessomekeystepsfromLi&Lin(2024),extendingthemtoaccommodatethegeneralized
algorithmanddifferentsmoothnessassumptions.
Theorem3.5(Main,Adam). Let{L }T beindependentstochasticlossessatisfyingAssumption3.4andthat
t t=1
theirexpectationLisH-coordinate-wiselysmoothw.r.t. ℓ norm. ForAdamwithβ =0,wehavethat
∞ 1
 (cid:118) 
√ (cid:117) (cid:117) βT 4 (cid:88)d √
Tm <ti ≤n TE∥∇L(x t)∥
1
≤OE+ E(cid:116) T(1−2
β
2)dv 0+ σ i+d ϵ
2 i=1
with
(cid:18) (cid:19)(cid:32) d d (cid:33)
E =
2
E[L(x )−L(x )]+ 1+
β 2F η(cid:88)
H
+(cid:112)
1−β
(cid:88)
σ
ηT 0 T T(1−β ) i 2 i
2
i=1 i=1
and
(cid:32) (cid:80)d σ2+∥∇L(x )∥2 +max H2η2T(T + 1 )(cid:33)
F =2ln 1+ i=1 i 0 ∞ i∈[d] i 1−β2 +ln32.
v +ϵ
0
WecandeterminetheconvergencerateofRMSpropbyapplyingappropriatehyperparametersonTheo-
rem 3.5. The optimal hyperparameters η and β can be selected by minimizing E. We would assume that
2
v +ϵ > ((cid:80)d σ2+∥∇L(x )∥2 +max H2η2)/poly(T)and 1 = poly(T). Thenwecansimplify
0 i=1 i 0 ∞ i i 1−β2
thetermbyconsideringF =O(logT).
(cid:18) (cid:16) (cid:17)1(cid:19)
The two terms involving (cid:80)d σ have a lower bound Θ (cid:80)d σ logT 2 , which can reached
i=1 i i=1 i T
(cid:16) (cid:17)
by 1 − β = Θ logT . With this choice of 1 − β , the three terms involving η has a lower bound
2 T 2
Θ(cid:18)(cid:113) (L(x0)−minxL(x))(cid:80)d i=1Hi(cid:19)
reachedbyη =
Θ(cid:16)(cid:113) L(x0)−minxL(x)(cid:17)
. Suchchoicesofhyperparame-
T T(cid:80)d i=1Hi
ters can give the optimal convergence rate for stochastic case in Corollary 3.6. For convenience, we define
R≜(L(x )−min L(x))(cid:80)d H ,whichwillbethecoreterminCorollaries3.6and3.7.
0 x i=1 i
Corollary3.6(StochasticCase,generalσ ). Let{L }T beindependentstochasticlossessatisfyingAssump-
i t t=1
tion3.4andthattheirexpectationLisH-coordinate-wiselysmoothw.r.t. ℓ norm. Forβ =0,1−β =
∞ 1 2
(cid:16)(cid:113) (cid:17)
Θ(logT),ϵ=0,η =Θ L(x0)−minxL(x) andv >((cid:80)d σ2+∥∇L(x )∥2 +max H2η2)/poly(T),
T T(cid:80)d i=1Hi 0 i=1 i 0 ∞ i i
wehavethat
 (cid:118) 
(cid:114) (cid:117) d (cid:18) (cid:19)1 d (cid:18) (cid:19)1
R (cid:117)(cid:88) R 4 (cid:88) logT 4
Tm <ti ≤n TE∥g t∥
1
=O
T
+(cid:116) σ
i T
+ σ
i T
+δ T.
2 i=1 i=1
withδ
T
=(cid:113) T(1d −v0 β2)exp(cid:16) −T(1− 8β2)(cid:17)(cid:20) (cid:0)R T(cid:1)1 4 +(cid:113) (cid:80)d i=1σ i(cid:16) lo TgT(cid:17)1 4(cid:21)
Hereδ canbesmallerthananypolynomialofT bymanipulatingthevalueof T(1−β2) = Θ(1). Then
T logT
(cid:16) (cid:17)1
(cid:80)d σ logT 4 istheleadingtermw.r.t. T intherate. Thecoefficientonlyinvolves(cid:80)d σ ,suggesting
i=1 i T i=1 i
thattheratecanbemuchimprovedwhennoiseissmall. Belowwegettheconvergenceratewiththesame
hyperparametersindeterministiccaseinCorollary3.7.
Corollary3.7(DeterministicCase,σ = 0). Let{L }T bedeterministiclossessatisfyingAssumption3.4
i t t=1
andthattheirexpectationLisH-coordinate-wiselysmoothw.r.t. ℓ norm. Forβ =0,1−β =Ω(logT),
∞ 1 2 T
5(cid:16)(cid:113) (cid:17)
ϵ = 0, η = Θ L(x0)−minxL(x) andv > ((cid:80)d σ2+∥∇L(x )∥2 +max H2η2)/poly(T)forany
T(cid:80)d i=1Hi 0 i=1 i 0 ∞ i i
polynomialpoly(T),wehavethat
(cid:32)(cid:114) (cid:33)
R
min ∥g ∥ =O +δ .
T<t≤T t 1 T T
2
withδ
T
=(cid:113) T(1d −v0 β2)exp(cid:16) −T(1− 8β2)(cid:17)(cid:0)R T(cid:1)1 4.
Corollary3.7almostrecoversTheorem3.2,exceptthesmoothnessconstantheresup ∥∇2L(x)∥ is
x (1,1)
worsethanthatinTheorem3.2,whichissup ∥∇2L(x)∥ ,becauseitalwaysholdsthat∥·∥ ≥ ∥·∥ .
x ∞ 1,1 ∞
This gap is due to a technical difficulty of analyzing Adam or RMSProp, as mentioned in the beginning of
Section3.2.
Dependence on ϵ, v and β . While many previous works rely on the relatively large magnitude of ϵ
0 2
comparedtov andgiveaboundintheregimeofSGDwhentheadaptiveeffectisdominatedbytheconstant
t
ϵ (Zaheer et al., 2018; De et al., 2018), our result actually prefers ϵ to be 0 while maintaining the value of
v +ϵ. We also note the dependence of our bound in Theorem 3.5 on v is very mild and logarithmic.
0 0
Theorem3.5hassimilarconvergencerateforallv ofmagnitudeatmostpoly(T),whilemostpreviousresult
0
only addresses the case where v is at the scale of noise (Li & Lin, 2024) or 0. The main reason for this
0,i
adaptivitytoawiderangeofv isourspecificchoiceofβ =1−Θ(logT),whichallowstheinitiallargev to
0 2 T 0
decayfastandresumenormaltraining. Otherexistingresultsusingβ =1−Θ(1/T)(Défossezetal.,2022;
2
Li&Lin,2024) cannotallowlargeinitialvaluev becausev onlydecaysaconstantfractionthroughoutthe
0 0
trainingandtheeffectivelearningratewillbetoosmall.
3.3 AunifiedanalysisforblockwiseAdam
In this subsection, we present convergence analysis for a broader class of adaptive algorithms defined in
Algorithm 3, which could be thought as a coarser version of Adam. It does pre-conditioning blockwisely
(specifiedbyapartitionfunctionΦ:[d]→[B]whereBisthenumberofblocks)insteadofcoordinate-wisely.
Since Adam and AdaSGD can be viewed as special cases blockwise Adam (Algorithm 3) with Φ : i (cid:55)→ i
Adam
andΦ :i(cid:55)→1respectively,anyconvergenceresultsforAlgorithm3wouldimplyconvergenceofAdam
AdaSGD
andAdaSGD. FinallywealsonotethatsuchblockwiseAdamhasbeenrecentlystudiedempiricallybysome
concurrentwork,wherethealgorithmisnamedbyAdamini(Zhangetal.,2024b)andAdalayer(Zhaoetal.,
2024).
Algorithm3BlockwiseAdam
Hyperparam: β ,β ,ϵ≥0,blockpartitionΦ:[d]→[B],totalstepsT,learningrateschedule{η }T ,ϵ,
1 2 t t=1
initialm ,v .
0 0
Input: initializationx ,stochasticlossfunctions{L }T
0 t t=1
v ←v
0,b 0
fort=1,2,··· ,T :
g ←∇ L (x )
t,i i t t−1
m ←β m +(1−β )g
t,i 1 t−1,i 1 t,i
(cid:16) (cid:17)
v ←β v +(1−β ) (cid:80) g2 /d
t,b 2 t−1,b 2 B(i)=b t,i b
x ←x −η √ mt,i
t,i t−1,i t
vt,B(i)+ϵ
returnx
T
Wefirstintroducesomemorenotations. Wedefinethevectorx as[x ] andthesubmatrixA
(b) i Φ(i)=b (b),(b′)
as[A ] .
i,j Φ(i)=b,Φ(j)=b′
6Definition 3.8 (Φ-norm). We define the (∞,2)-norm w.r.t. partition Φ of vector x as the ℓ norm of the
∞
(cid:18) (cid:19)B
vector
∥x √(b)∥
2 ,whichismax
∥x √(b)∥
2. Forconvenience,wewilldenoteitby∥x∥ orjustcallit
db b∈[B] db Φ
b=1 √
Φ-norm. Wedenoteitsdualnormby∥x∥
Φ,∗,whichisequalto(cid:80)B
b=1
d
b(cid:13)
(cid:13)x
(b)(cid:13)
(cid:13) 2.
Definition 3.9 (Generalized version of Definition 3.3). For any partition function Φ : [d] → [B] and
H = (H ,...,H ) ∈ RB, we say a function L is H-smooth blockwisely w.r.t. (∞,2) norm , iff for any
1 B
b∈[B],x,y ∈Rd,
(cid:112) (cid:13) (cid:13)
d b(cid:13)∇ (b)L(x)−∇ (b)L(y)(cid:13)
2
≤H b∥x−y∥
Φ
WefurtherdefinetheΦ-smoothnessoflossLbyH(L,Φ) = (cid:80)B H ,where{H }B arethesmallest
b=1 b b b=1
numbersmakingLisH-smoothblockwiselyintheabovesense.
Wenotethattheabovedefinedblockwise(∞,2)-smoothnessisbothageneralizationofthecoordinate-
wise smoothness defined in Definition 3.1 (corresponding to the case of each block only containing 1
coordinate) and the standard ℓ smoothness (corresponding to the case of only having one block). In the
2
former case, we have B = d, Φ is the identity mapping i (cid:55)→ i and it holds that H(L,i (cid:55)→ i) ≥
Adam
(cid:13) (cid:13) (cid:13) (cid:13)
sup x∈Rd(cid:13)∇2L(x)(cid:13)
1,1
≥sup x∈Rd(cid:13)∇2L(x)(cid:13) ∞. Inthelattercase,wehaveB =1,Φ
AdaSGD
isthemapping
(cid:13) (cid:13)
i(cid:55)→1andH(L,i(cid:55)→1)=sup x∈Rd(cid:13)∇2L(x)(cid:13) 2.
Similartothecoordinate-wisecase,onecanshowthat(cid:80)B
H w.r.t. partitionΦisanupperboundfor
b=1 b
thesmoothnessoflossLw.r.t. toΦ-norm.
Assumption3.10(GeneralizedversionofAssumption3.4). Thereexistsconstantσ suchthat
b
E(cid:13) (cid:13)∇ (b)L t(x)−∇ (b)L(x)(cid:13) (cid:13)2
2
≤d bσ b2
foranyblockb∈[B],t∈Nandx∈Rd.
Theorem3.11(Main,BlockwiseAdam). UnderAssumption3.10,supposeLisH-smoothblockwiselyw.r.t.
(∞,2)-norm,whereH=(H ,...,H )∈RB,forAlgorithm3,wehavethat
1 B
(cid:118)
Tm <ti ≤n
TE(cid:88)B (cid:112)
d
b(cid:13)
(cid:13)∇ (b)L(x
t)(cid:13)
(cid:13)
2
≤E+√ E(cid:117) (cid:117)
(cid:116)
T(1β −2T 4
β
2)d√
v
0+(cid:88)B
d bσ
b+d√
ϵ
2 b=1 b=1
with
(cid:18) (cid:19)(cid:32) B B (cid:33)
E =
2
E[L(x )−L(x )]+ 1+
β 2F η(cid:88)
H
+(cid:112)
1−β
(cid:88)
d σ ,
ηT 0 T T(1−β ) b 2 b b
2
b=1 b=1
and
(cid:32) (cid:80)B σ2+∥∇L(x )∥2 +max H2η2T(T + 1 )(cid:33)
F =2ln 1+ b=1 b 0 Φ b∈[B] b 1−β2 +ln32.
v +ϵ
0
(1,1)-norm as a surrogate complexity measure for H(L,Φ ). H in Definition 3.3 is determined by
Adam i
sup (cid:80)d (cid:12) (cid:12)∇2 L(x)(cid:12) (cid:12), which is difficult to compute because it requires taking supreme over the entire
x j=1 i,j
domain. An computationally-tractable alternative is to approximate
(cid:80)d
H locally by the (1,1)-norm of
i=1 i
Hessianoflossalongthetrainingtrajectory. Weprovideanefficientapproximationalgorithmwithguarantees
byusinghessian-vectorproductagainstrandomCauchyvectorsinAppendixD.2.
7Different norms for smoothness. As an implication of Theorem 3.11, we immediately get analogs
of Corollaries 3.6 to 3.7 for AdaSGD, with the corresponding new noise assumption and smoothness as-
sumption. When the optimization is not noise-dominated, i.e., the main term in the deterministic case
(cid:113) (cid:113)
R = (L(x0)−minxL(x))H(L,Φ) becomes the leading term, the choice of Φ now matters a lot. Here
T T
the biggest change from AdaSGD to Adam is the difference between H(L,Φ ) and H(L,Φ ), which
AdaSGD Adam
roughlycorrespondtosup ∥∇2L(x)∥ andsup ∥∇2L(x)∥ ,usingtheabovementionedapproximation.
x 2 x 1,1
PreviousanalysesofAdam’sconvergence(Shi&Li,2021;Défossezetal.,2022;Li&Lin,2024)usually
assumesmoothnessundertheℓ norm. Whenusingthisassumption,theresultingconvergencerateforAdam
2
ends up being identical to the rate for AdaSGD, which fails to capture why Adam often performs better than
AdaSGDinpractice. Byshiftingtoanℓ normsmoothnessassumption,wecanobservethekeydifference:
∞
thecoefficientforAdam’sconvergenceratechangesfromsup ∥∇2L(x)∥ tosup ∥∇2L(x)∥ ,wherethe
x 2 x 1,1
latteristypicallymuchsmallerwhenAdamoptimizesfaster. Thischangeleadstoadivergenceintheratesof
thetwoalgorithms, andcomparingthesenewcoefficientscanprovideinsightintowhichalgorithmmaybe
moreeffectiveunderdifferentconditions.
Finally, we note that the Φ -smoothness H(L,Φ ) is not rotation-invariant in the sense that
Adam Adam
H(L,Φ )̸=H(L◦R,Φ )foratypicalrotationR. Inpractice,the(1,1)-normofHessianmatrixcan
Adam Adam
varyalotwhenarotationisperformedonthelossasshowninSection4.1. Incontrast,Φ -smoothness
AdaSGD
H(L,Φ )isinvariantunderlossrotations.
AdaSGD
3.4 ProofsketchofTheorem3.11
Wewilluseg¯ =E[g |x ]=∇L(x )todenotethefullbatchgradient. WefirstintroduceLemma3.12,
t t <t t−1
whichisthemainandonlyusageofournewsmoothnessassumptionDefinition3.9.
Lemma3.12. ForanytwicedifferentiablelosswhichisH-smoothblock-wiselyw.r.t. (∞,2)-norm(Defini-
tion3.9),wehaveforanyxand∆∈Rd,
B
∆⊤∇2L(x)∆≤(cid:88)H db (cid:13) (cid:13)∆ (b)(cid:13) (cid:13)2 2. (1)
b
b=1
WestartbyconsideringthedecreaseofL(x )inasinglestept. Wecanupperboundthesecondorder
t
termintheTaylorexpansion(Equation2)withLemma3.12. Thenwecangetlossdecreaseinasinglestep
L(x )−L(x
)≤−η(cid:88)d g t,ig¯
t,i +
1 η2(cid:88)B H b(cid:13) (cid:13)g t,(b)(cid:13) (cid:13)2
2 (2)
t t−1 (cid:112) v +ϵ 2 d v +ϵ
t,Φ(i) b t,b
i=1 b=1
≤−η(cid:88)B g √t⊤ ,(b)g¯ t,(b)
+
1 η2(cid:88)B
H
(cid:13) (cid:13)g t,(b)(cid:13) (cid:13)2 2/d b
(3)
v +ϵ 2 b v +ϵ
t,b t,b
b=1 b=1
Theproofcontainstwomainparts: lowerboundingthefirstordertermandupperboundingthesecondorder
term. Below we address the second-order term first. At each step, the second order term
∥gt,(b)∥2 2/db
can
vt,b+ϵ
be as large as 1 . But if we sum over all the steps, we can employ Lemma 3.13 to bound the sum by
1−β2
T + β2 lnvT,b+ϵ ratherthan T .
1−β2 v0,b+ϵ 1−β2
Lemma3.13. Givenany0<β <1,foranyscalarsequences{v }T and{g }T satisfythatv ≥0,v >
2 t t=0 t t=1 0 1
0andv −β v ≥(1−β )g2fort≥1,itholdsthat
t 2 t−1 2 t
(cid:88)T g t2
≤T +
β
2
lnv
T. (4)
v 1−β v
t 2 0
t=1
Now we turn to the first term. Ideally, we would like to connect the first order term to ∥g¯ ∥2 by
t,(b) 2
taking expectation, e.g., E g√t⊤ ,(b)g¯ t,(b) ≈ E t√g t⊤ ,(b)g¯ t,(b) = ∥√g¯ t,(b)∥2 2, where we use E [·] as abbreviation for
t t
vt,b+ϵ vt,b+ϵ vt,b+ϵ
8E[·|x ]. However, this is not correct because both the numerator and denominator in (cid:80)B g√t⊤ ,(b)g¯ t,(b)
<t b=1 vt,b+ϵ
dependonthestochasticgradientnoiseatstept. Tocircumventthisdifficulty,weuseLemma3.14tolower
boundeachE tg√t⊤ ,( vb) t,g¯ bt +,( ϵb) by E √tg Et⊤ , t( vb) t,g¯ bt +,( ϵb) ≥ ∥ √g¯ vt ˜, t( ,b b) +∥ ϵ2 2 minuserrortermsrelatedtonoisemagnitudeσ b,where
v˜
t,b
=β 2v t−1,b+(1−β
2)(cid:16)(cid:13)
(cid:13)g¯
t,(b)(cid:13) (cid:13)2
2/d b+σ
b2(cid:17)
.
Lemma3.14(first-orderapproximation). Foranyblockb∈[B],itholdsthat
E(cid:88)T g √t⊤ ,(b)g¯ t,(b) ≥ 1 E(cid:88)T (cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2 −(cid:112) 1−β Td σ − √d bσ bβ 2 E(cid:20) lnv T,b+ϵ(cid:21) . (5)
v +ϵ 2 (cid:112) v˜ +ϵ 2 b b 1−β v +ϵ
t=1 t,b t=1 t,b 2 0,b
Finally,weemployCauchyinequalitytoupperbound∥g¯ ∥ :
t Φ,∗
(cid:118) (cid:118)
(cid:88)T ∥g¯ t∥
Φ,∗
= (cid:88)T (cid:88)B (cid:112) d b(cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)
2
≤(cid:117) (cid:117) (cid:117)
(cid:116)
(cid:88)T (cid:88)B (cid:112)(cid:13) (cid:13)g¯ v˜t,(b) +(cid:13) (cid:13)2 2 ϵ(cid:117) (cid:117) (cid:117)
(cid:116)
(cid:88)T (cid:88)B d b(cid:112) v˜ t,b+ϵ. (6)
t=T+1 t=T+1b=1 t=T+1b=1 t,b t=T+1b=1
2 2 2 2
WeuseLemma3.15toupperbound(cid:80)T
t=T
2+1d b(cid:112) v˜ t,b+ϵby(cid:80)T
t=1
∥ √g¯ vt ˜, t( ,b b) +∥ ϵ2 2. (cid:80)T t=1(cid:80)B
b=1
∥ √g¯ vt ˜, t( ,b b) +∥ ϵ2 2 canbe
boundedfromanalysisabove,whichfinishestheproof.
Lemma3.15. Foranyblockb∈[B],itholdsthat
(cid:88)T E(cid:104)(cid:112)
v˜
+ϵ(cid:105)
≤
2β 2T 4 √
v +
T
σ +
T√ ϵ+2(cid:88)T E(cid:34)(cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b(cid:35)
. (7)
t,b 1−β 0,b 2 b 2 (cid:112) v˜ +ϵ
t=T+1 2 t=1 t,b
2
4 Experiments
In order to empirically investigate and confirm the implications of our propsosed theory, we compare the
trainingperformanceofAdamwithAdaSGD,SGDandrotatedAdamonmultipledifferenttasks. InAdamand
itsvariants(includingAdaSGD)wealwayssetβ ,β = (0.9,0.99). MomentuminSGDisalsosetto0.9.
1 2
Weightdecayisalwaysdeactivated.
4.1 Quadraticloss
We perform controlled experiments on quadratic loss to study the relationship between optimization speed
of Adam and the shape of Hessian, in terms of Φ -smoothness. More specifically, we consider Σ =
Adam
diag(1,··· ,1,1, 1 , 1 ,··· , 1 ) ∈ R1000×1000 and optimize the corresponding quadratic loss of Σ by
22 32 9902
(cid:124) (cid:123)(cid:122) (cid:125)
10
Adam, with different levels of rotations. We manually generate orthogonal matrices R in the following
i
way. We first sample M ∈ Rd×d where M is i.i.d. sampled from N(0,1). Then A = M−M⊤ is a
i,j
skew-symmetricmatrixandexp(tA)representsacontinuousfamilyofmatrices. WedefineR =exp(t A)
i i
fordifferentt . Whent =0,weknowR =I. Whent →∞,R convergestoarandomorthogonalmatrix
i i i i i
indistribution.
Then we optimize L (x) = x⊤Σx with AdaSGD and Adam and optimize L (x) = x⊤R⊤ΣR x with
0 i i i
Adamfor100steps. BecauseAdaSGDisrotationalinvariant,theoptimizationperformanceofAdaSGDisthe
sameonallL . WetunelearningratesforeachsettingandpresenttheirbestresultsinTable1.
i
WefindaclearpatternthatAdamoptimizesworsewhenthe(1,1)normofHessianmatrixincreases,as
suggestedbyourCorollary3.7. Moreover, when(1,1)normdividedbydimensionissmallerthanspectral
norm,AdamtendstooptimizefasterthanAdaSGDandviceversa,assuggestedbyourTheorem3.11.
9Optimizer (1,1)-norm/d Loss(β =β =0) Loss(β =0.9,β =0.99)
1 2 1 2
AdaSGD 0.00582 0.00881 0.00172
Adam 0.00582 0.00030 0.00001
Adam(R ) 0.04162 0.00317 0.00062
1
Adam(R ) 0.25364 0.00588 0.00122
2
Adam(R ) 0.61866 0.00747 0.00179
3
Adam(R ) 1.29959 0.00920 0.00239
4
Table1: Thefinallossvaluesobtainedbydifferentoptimizersandthe(1,1)-normofHessianmatrixforthe
correspondingunrotatedobjectiveandrotatedobjectives. ThespectralnormoftheHessianmatrixisalways
1. Adamoptimizesworsewhenthe(1,1)-normofHessianmatrixincreases,assuggestedbyourCorollary3.7.
Moreover, when (1,1)-norm is smaller than spectral norm times space dimension, Adam tends to optimize
fasterthanAdaSGDandviceversa,whichjustifiestheeffectivenessofΦ-smoothnessasatooltopredictthe
optimizationspeedofblockwiseAdam.
4.2 GPT-2onlanguagemodelingtask
We train GPT-2 small (124M parameters)3 on the OpenWebText corpus containing more than 9B tokens
for 100k iterations with sequence length of 512 sequence length and 480 sentences per batch. We use
cosinelearningratescheduleofthesamepeaklearningrate6×10−4,whichisalsothedefaultof nanoGPT
codebase. We did a grid search to find the maximum possible peak learning rate for SGD4. The training
lossesandevaluationlossesofdifferentoptimizersareplottedinFigure1. AsmentionedinSection1,Adam
converges faster than AdaSGD while they both converge faster than rotatedAdam. Since we propose the
(1,1)-normofHessianasanonrotation-invariantmetricthatcanaffecttheconvergencerateofAdam,wealso
measureitfororiginallossfunctionLandrotatedlossfunctionL˜oncheckpointstrainedwithdifferentlosses.
TheresultsarepresentedinTable2. Thesamecorrelationbetweennormsandconvergenceratesholdshere.
Thesmallerthenormis,thefastertheoptimizerworks.
Optimizer SmoothnessMetric MatrixNorm EstimatedValue
(cid:13) (cid:13)
AdaSGD H(L,Φ AdaSGD) (cid:13)(cid:13)∇2L(x) (cid:13)(cid:13)
2
24.86
Adam H(L,Φ Adam)
(cid:13)
(cid:13)∇2L(x)(cid:13)
1,(cid:13)1
3.2
RotatedAdam H(L◦R,Φ Adam) (cid:13)R⊤∇2L(x)R(cid:13)
1,1
36.16
Table2: HessiannormsforthelastGPT-2checkpointstrainedwithdifferentoptimizers.
GPT-2 small models have more than 100 million parameters, and thus the size of its hessian of loss as
wellastherotationmatrixismorethan1016,whichiswaymorethanthestorageofthemoderncomputers.
Thusourexperimentsfacethefollowingtwoimplementationchallenges. Belowwebrieflydiscussthemand
leavethefulldetailsinAppendixD.
Efficientgenerationofrandomorthogonalmatrices. Duetothehugeparametercount,itiscomputation-
ally infeasible to sample a random orthogonal matrix from Haar measure over the orthogonal group. Thus
wedecidetosamplefromanalternativedistributionoverorthogonalmatrices,wherethefinalrotationmatrix
is the composition of a sequence of simple orthogonal matrices, including random permutation and apply
randomorthogonaltransformationonthebothsideofmatrix-valuedparameters. SeedetailsinAppendixD.1.
Efficientestimationof(1,1)-normofHessianmatrix. Wefirstsubsampleafixedbatchoftrainingdata
forestimatingthe(1,1)-normofHessianmatrix. Thehigh-levelideahereistocomputetheHessianvector
productbetweenHessianoftraininglossonthisbatchandasequenceofrandomCauchyvectorsandtakethe
3WeusenanoGPTcodebase,https://github.com/karpathy/nanoGPT
4Wetried0.00001,0.00003,0.0001,0.0003,0.001,0.003,0.01,0.03,0.1,0.3,1.
10ℓ normofthecoordinate-wisemedianoftheresultingsequenceofHessianvectorproducts. BecauseCauchy
1
distributionis1-stable,theresultingproductisalsoavectorofCauchyrandomvariables,andeachofthemhas
themagnitudeequaltoℓ normofthecorrespondingrowoftheHessian. Thuswhenwehaveinfinitelymany
1
samples,theℓ normofthecoordinate-wisemedianconvergesalmostsurelytothe(1,1)-normofHessian.
1
SeedetailsinAppendixD.2,wherewealsoprovideanon-asymptotichigh-probabilitymultiplicativebound
fortheestimationerror.
4.3 ResNet18onCIFAR-10
TofurthertestthecorrelationbetweenΦ-smoothnessandtheoptimizationperformanceholdsforarchitectures
otherthantransformers,weconductasimilarexperimentonResNet18trainedonCIFAR-10(Krizhevsky&
Hinton,2009). Weappliedrandomcropandrandomhorizontalflipaugmentationsoverthetrainingdatato
promote better generalization. We tuned each optimizer through searching over the same grid of learning
rates5 The number of iterations is adjusted per batch size to result in 20 epochs for each training run (for
instance,4000iterationswereusedforabatchsizeof256,and1000iterationswereusedforabatchsizeof
1024). ForthetraininglossandtrainingaccuracyplottedinFigure2,itismeasuredonasubsetofaugmented
training data that is the same size of evaluation set. The evaluation loss and accuracy are measured on the
entireevaluationsetwithouttheaugmentation. Trackrunningstatsissettofalseatinitialization.
Train and Evaluation Loss Train and Evaluation Accuracy
2.5 100
95
1
90
5e-1
85
2e-1
SGD 80 SGD
1e-1 AdaSGD AdaSGD
Adam 75 Adam
Rotated AdamW Rotated AdamW
3e-2 70
0 500 1000 1500 2000 2500 3000 3500 4000 0 500 1000 1500 2000 2500 3000 3500 4000
Iterations Iterations
Figure2: Trainingandtestlosses(left)andaccuracy(right)ofResNet18onCIFAR-10withAdam,AdaSGD,
rotatedAdam,andSGD. Weusebatchsize256andtheoptimallearningrateintermsoftraininglossfrom
gridsearch. Solidanddashedlinescorrespondtothetrainingandevaluationsetmetricsrespectively. Adam
convergesfasterthanotheralgorithms.
Figure 2 depicts the loss and accuracy curves for the best performing hyperparameters chosen over the
trainingset’sfinallossforbatchsize256.6 WealsoprovidetheresultsforotherchoicesofbatchsizeinTable3.
When it comes to optimization speed, even for ResNet18, Adam is always better than and they are always
betterthanAdaSGD,SGD,androtatedAdamacrossdifferentbatchsizes. Notethatthisdoesnotcontradict
withcommonpracticeoftrainingResNetwithSGD, wherethemaingoalistogetbettergeneralizationand
thetrainingbudgetislargesoalloptimizerscaneasilyachievefulltrainingaccuracy. Inourexperiment,we
studyoptimizationspeedandintentionallylimitthenumberofsteps.
We also measure the Hessian for checkpoints obtained at batch size 256 and the results are in Table 4.
Thecorrelationbetweennormsandconvergenceratesstillholdshere. Whenthe(1,1)-normdividedbydis
smallerthanspectralnorm,AdamoptimizesfasterthanAdaSGD.
5Weusedthefollowingvalues: 6.25×10−4,1.25×10−3,2.5×10−3,5.0×10−3,1.0×10−2,2.0×10−2,4.0×10−2,
8.0×10−2,1.6×10−1,3.2×10−1,6.4×10−1,1.28×100.
6Wehaveintentionallylimitedthenumberoftrainingiterationstoemphasizethedifferenceofoptimizersintermsoftrainingspeed
overgeneralization.
11
ssoL
ycaruccABatchSize SGD AdaSGD Adam RotatedAdam
16 0.0777 0.114 0.064 0.0905
64 0.0698 0.0854 0.0472 0.0574
256 0.0723 0.0787 0.0359 0.0485
1024 0.1115 0.0915 0.0735 0.0817
Table 3: Training losses of ResNet for different optimizers and different batch sizes within 20 epochs. For
each setting, we choose the optimal performance over all the learning rates. The performance of Adam is
consistentlythebestamongallfouroptimizers.
Optimizer SmoothnessMetric MatrixNorm EstimatedValue
(cid:13) (cid:13)
AdaSGD H(L,Φ AdaSGD) (cid:13)(cid:13)∇2L(x) (cid:13)(cid:13)
2
1.5355
Adam H(L,Φ Adam)
(cid:13)
(cid:13)∇2L(x)(cid:13)
1,(cid:13)1
0.0036
RotatedAdam H(L◦R,Φ Adam) (cid:13)R⊤∇2L(x)R(cid:13)
1,1
0.9868
Table4: HessiannormsforoptimalResNetcheckpointstrainedwithdifferentoptimizersandbatchsize256.
5 Related Works
Comparison between Adam and SGD Previous work tries to analyze the difference between Adam and
SGDfromdifferentperspectives. Zhouetal.(2018)provesafasterconvergencerateofAdamthanSGDwhen
the stochastic gradients are sparse. Zhang et al. (2020) suggests that SGD suffers more from heavy-tailed
noisethanAdam. Pan&Li(2023)claimsthatAdamhaslowerdirectionalsharpnessbecauseoftheeffectof
coordinate-wise clipping. Other works also consider the coordinate-wise normalization of Adam (Balles &
Hennig,2018;Kunstneretal.,2022). Kunstneretal.(2024)showsthattheheavy-tailedclassimbalancein
language modeling tasks will cause SGD to converge slower when it can only optimize majority class well.
Zhang etal. (2024a) findsthat Adam is betterat handling theblock heterogeneity ofHessian matrix, which
is a specific phenomenon in transformers. When viewing Adam as an adaptive method, there are works
showingthatadaptivemethodshaveanadvantageofachievingoptimalconvergenceratewithoutrelyingon
problem-dependentconstant(Wardetal.,2020;Levyetal.,2021).
ConvergencerateofAdam TherearemanyworksshowingconvergencerateforAdam(Zhouetal.,2018;
Chen et al., 2018; Zou et al., 2019; Shi & Li, 2021; Guo et al., 2021; Défossez et al., 2022; Zhang et al.,
2022b). Mostofthemrelyonthesmoothnessofthelossfunction,whichismeasuredw.r.t. ℓ norm. Zhang
2
etal.(2019)proposesthe(L ,L )smoothnessconditionshouldbemorereasonablethangloballybounded
0 1
smoothness. Lietal.(2024)furthergeneralizesthe(L ,L )smoothnesscondition. However,theystillfocus
0 1
onthedefaultℓ normwhichisrotation-invariant. Tothebestofourknowledge,wearethefirsttoassume
2
gradientLipschitznessunderℓ normfortheanalysisonAdam.
∞
ComparisonwithLi&Lin(2024) Li&Lin(2024)employsthesameℓ normforgradientandimproves
1
thedependenceondimensiondcomparedtopreviousresultsforℓ norm. Buttheystillassumethecommon
2
ℓ normsmoothnesswhileweadapttheirresultsunderℓ normsmoothnesstopotentiallyfurtherimprove
2 ∞
dependenceond. AnotherdrawbackofLi&Lin(2024)issettingv basedonnoisemagnitudeσ. whichis
0
impracticalinrealexperimentsbecauseσ isunknown. Overestimationforσ willresultinslowconvergence
because large v causes Adam to behave similarly with SGD without adjusting the coordinate-wise learning
0
rateadaptively. Incontrast,weallowforgeneralinitializationforv andourconvergenceratecanworkwell
0
(cid:16) (cid:17)
inbothnoisysettinganddeterministicsetting. Wealsouse1−β = Θ logT toobtainourconvergence
2 T
ratewhileLi&Lin(2024)requires1−β
=Θ(cid:0)1(cid:1)
.
2 T
126 Conclusion
We give a new convergence analysis (Theorem 3.5) for Adam in the stochastic non-convex setting using a
(cid:16) (cid:17)
novel smoothness assumption. We show the convergence rate for the 1-norm of the gradient is O √1
T
(cid:18)(cid:16) (cid:17)1/4(cid:19)
in the deterministic case (Corollary 3.7) and O logT in the stochastic case (Corollary 3.6). We
T
alsoextendouranalysistoblockwiseAdamonlossLwithrespecttoanarbitrarypartitionoftheparameters
Φ(Theorem3.11)usingthecorrespondingsmoothnessH(L,Φ)(Definition3.9). OurboundforAdaminvolves
(1,1)-normofHessian,ratherthanthespectralnormofHessian,whichisrelevanttotheconvergencespeedof
AdaSGD. ThisleadstosignificantlybettersmoothnessconditionsfordeeplearningmodelsincludingResNet-
18 and GPT2 empirically. Our experiments also verify that the smoothness measure H(L,Φ) positively
correlateswiththeoptimizationspeedofblockwiseAdamwithrespecttothepartitionΦ.
Acknowledgement
TheauthorswouldliketothankKhashayarGatmiryandSharutGuptaforhelpfuldiscussionandpreliminary
experimentsinexploringtheideaofrotatedAdam. ZLissupportedbyOpenAI.
References
YossiArjevani,YairCarmon,JohnCDuchi,DylanJFoster,NathanSrebro,andBlakeWoodworth. Lower
boundsfornon-convexstochasticoptimization. MathematicalProgramming,2023. 1
LukasBallesandPhilippHennig.Dissectingadam: Thesign,magnitudeandvarianceofstochasticgradients.
InInternationalConferenceonMachineLearning,2018. 12
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners.
Advancesinneuralinformationprocessingsystems,2020. 1
Xiangyi Chen, Sĳia Liu, Ruoyu Sun, and Mingyi Hong. On the Convergence of A Class of Adam-Type
Algorithms for Non-Convex Optimization. In International Conference on Learning Representations,
2018. 12
Soham De, Anirbit Mukherjee, and Enayat Ullah. Convergence guarantees for RMSProp and ADAM
in non-convex optimization and an empirical comparison to Nesterov acceleration. arXiv preprint
arXiv:1807.06766,2018. 6
AlexandreDéfossez,LeonBottou,FrancisBach,andNicolasUsunier.ASimpleConvergenceProofofAdam
andAdagrad. TransactionsonMachineLearningResearch,2022. 1,3,6,8,12
ZhishuaiGuo,YiXu,WotaoYin,RongJin,andTianbaoYang. Anovelconvergenceanalysisforalgorithms
oftheadamfamily. arXivpreprintarXiv:2112.03459,2021. 12
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky. Neural networks for machine learning lecture 6a
overviewofmini-batchgradientdescent. Citedon,2012. 4
JaredKaplan,SamMcCandlish,TomHenighan,TomBBrown,BenjaminChess,RewonChild,ScottGray,
Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint
arXiv:2001.08361,2020. 1
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980,2014. 1
AlexKrizhevskyandGeoffreyHinton. Learningmultiplelayersoffeaturesfromtinyimages. 2009. 11
13Frederik Kunstner, Jacques Chen, Jonathan Wilder Lavington, and Mark Schmidt. Noise Is Not the Main
Factor Behind the Gap Between Sgd and Adam on Transformers, But Sign Descent Might Be. In The
EleventhInternationalConferenceonLearningRepresentations,2022. 12
Frederik Kunstner, Robin Yadav, Alan Milligan, Mark Schmidt, and Alberto Bietti. Heavy-Tailed Class
ImbalanceandWhyAdamOutperformsGradientDescentonLanguageModels. CoRR,2024. 12
KfirLevy,AliKavis,andVolkanCevher.Storm+: Fullyadaptivesgdwithrecursivemomentumfornonconvex
optimization. AdvancesinNeuralInformationProcessingSystems,2021. 12
Haochuan Li, Alexander Rakhlin, and Ali Jadbabaie. Convergence of Adam under relaxed assumptions.
AdvancesinNeuralInformationProcessingSystems,2024. 12
√
HuanLiandZhouchenLin. OntheO( d )ConvergenceRateofRMSPropandItsMomentumExtension
T1/4
Measuredbyell_1Norm: BetterDependenceontheDimension. arXivpreprintarXiv:2402.00389,2024.
5,6,8,12,21
DevyaniMaladkar,RuichenJiang,andAryanMokhtari.ConvergenceAnalysisofAdaptiveGradientMethods
underRefinedSmoothnessandNoiseAssumptions. arXivpreprintarXiv:2406.04592,2024. 4
OpenAI. GPT-4technicalreport. arXiv,2023. 1
YanPanandYuanzhiLi. Towardunderstandingwhyadamconvergesfasterthansgdfortransformers. arXiv
preprintarXiv:2306.00204,2023. 12
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Languagemodels
areunsupervisedmultitasklearners. OpenAIblog,2019. 1
MachelReid,NikolaySavinov,DenisTeplyashin,DmitryLepikhin,TimothyLillicrap,Jean-baptisteAlayrac,
RaduSoricut,AngelikiLazaridou,OrhanFirat,JulianSchrittwieser,etal. Gemini1.5: Unlockingmulti-
modalunderstandingacrossmillionsoftokensofcontext. arXivpreprintarXiv:2403.05530,2024. 1
NaichenShiandDaweiLi. Rmspropconvergeswithproperhyperparameter. InInternationalconferenceon
learningrepresentation,2021. 8,12
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Openandefficientfoundation
languagemodels. arXivpreprintarXiv:2302.13971,2023. 1
Jiaxuan Wang and Jenna Wiens. AdaSGD: Bridging the gap between SGD and Adam. arXiv preprint
arXiv:2006.16541,2020. 1
Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex land-
scapes. JournalofMachineLearningResearch,2020. 12
Shuo Xie and Zhiyuan Li. Implicit Bias of AdamW: ℓ Norm Constrained Optimization. arXiv preprint
∞
arXiv:2404.04454,2024. 4
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods for
nonconvexoptimization. Advancesinneuralinformationprocessingsystems,2018. 6
JingzhaoZhang,TianxingHe,SuvritSra,andAliJadbabaie. WhyGradientClippingAcceleratesTraining:
ATheoreticalJustificationforAdaptivity.InInternationalConferenceonLearningRepresentations,2019.
12
Jingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank Reddi, Sanjiv Kumar,
and Suvrit Sra. Why are adaptive methods good for attention models? Advances in Neural Information
ProcessingSystems,2020. 12
14SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen,ShuohuiChen,ChristopherDewan,
Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv
preprintarXiv:2205.01068,2022a. 1
YushunZhang,CongliangChen,NaichenShi,RuoyuSun,andZhi-QuanLuo. AdamCanConvergeWithout
AnyModificationOnUpdateRules. InAdvancesinNeuralInformationProcessingSystems,2022b. 12
Yushun Zhang, Congliang Chen, Tian Ding, Ziniu Li, Ruoyu Sun, and Zhi-Quan Luo. Why Transformers
NeedAdam: AHessianPerspective. arXivpreprintarXiv:2402.16788,2024a. 12
Yushun Zhang, Congliang Chen, Ziniu Li, Tian Ding, Chenwei Wu, Yinyu Ye, Zhi-Quan Luo, and Ruoyu
Sun. Adam-mini: UseFewerLearningRatesToGainMore. arXivpreprintarXiv:2406.16793,2024b. 6
RosieZhao,DepenMorwani,DavidBrandfonbrener,NikhilVyas,andShamKakade. Deconstructingwhat
makesagoodoptimizerforlanguagemodels. arXivpreprintarXiv:2407.07972,2024. 6
DongruoZhou,JinghuiChen,YuanCao,YiqiTang,ZiyanYang,andQuanquanGu. Ontheconvergenceof
adaptivegradientmethodsfornonconvexoptimization. arXivpreprintarXiv:1808.05671,2018. 12
Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. A sufficient condition for convergences
of adam and rmsprop. In Proceedings of the IEEE/CVF Conference on computer vision and pattern
recognition,2019. 12
15Contents
1 Introduction 1
2 Preliminaries 3
3 MainResults: ConvergenceRatesofAdam 3
3.1 Warmup: SignGD(β =β =0) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1 2
3.2 Mainresult: RMSProp(β =0,β ∈[0,1]) . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1 2
3.3 AunifiedanalysisforblockwiseAdam . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
3.4 ProofsketchofTheorem3.11. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
4 Experiments 9
4.1 Quadraticloss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.2 GPT-2onlanguagemodelingtask . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
4.3 ResNet18onCIFAR-10 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
5 RelatedWorks 12
6 Conclusion 13
A ConvergencerateofSignGDfordeterministicloss 16
B InvariancepropertyofAdamandSGD 16
C ProofDetails 17
C.1 ProofforconvergencerateofblockwiseAdam . . . . . . . . . . . . . . . . . . . . . . . . . 17
D ExperimentDetails 27
D.1 Adamonarotatedloss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
D.2 Computationofmatrixnorms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
A Convergence rate of SignGD for deterministic loss
ProofofTheorem3.2. WewilldirectlyproveamoregeneralverionofTheorem3.2. BecauseLisH-smooth
withrespectto∥·∥ ,wehavethat
∞
H
L(x )−L(x )≤−∇L(x )⊤(x −x )+ ∥x −x ∥2
t+1 t t t t+1 2 t t+1
η2H
≤−η∥∇L(x )∥ + η2 (8)
t ∗ 2
Thisimpliesthat
T
min ∥∇L(x )∥ ≤
1 (cid:88)
∥∇L(x )∥ ≤
L(x 0)−L(x T)
+
Hη
,
1≤t≤T t ∗ T t ∗ Tη 2
t=1
whichcompletestheproof.
B Invariance property of Adam and SGD
Theorem2.2. SGDandAdaSGDarerotation-invariant. AdamandSignGDareonlypermutation-invariant.
16ProofofTheorem2.2. ForSGDandAdaSGD,wewillshowtheyarerotation-invariantbyinduction. Forany
rotatingtransformationR(x) = Rx,supposex˜ = R−1(x ) = R⊤x holdsfors ≤ t−1. Thenwehave
s s s
that g˜ = ∇ L˜ (x˜ ) = R⊤∇ L(R−1x˜ ) = R⊤∇ L(x ) = R⊤g and m˜ = R⊤m . From the
t x˜ t t x t−1 x t−1 t t t
updateruleofSGD,wehavethatx˜ =x˜ −η m˜ =R⊤x −η R⊤m =R⊤(x −η m )=R⊤x .
t t−1 t t t−1 t t t−1 t t t
FortheupdateruleofAdaSGD,wefurtherhavethat∥g˜ ∥2 =∥g ∥2becauseRisanorthogonalmatrix. Then
t 2 t 2
v˜ =v andthederivationissimilar.
t t
For Adam and SignGD, it is easy to show by induction they are invariant w.r.t. any permutating trans-
formation because the operation on gradient is performed on each coordinate separately. We only need
to show they are not invariant w.r.t. a rotating transformation. We choose R = [√1 ,√1 ;√1 ,−√1 ],
2 2 2 2
L (x)=L(x)=2x2+x2. DuetotheupdateruleofSignGD,itcanonlyupdatexandx˜inthedirectionof
t 1 2
[1,1]and[1,−1]. Butwhenrotatingtheupdatedirectiononx˜ backtothespaceofx. Theupdatedirection
canonlybe[1,0]or[0,1]thataredifferentfromtheupdatedirectionintheoriginalspace. Becausethefirst
stepinAdamtakesthesamedirectioninSignGD,wesimultaneouslyshowthatbothSignGDandAdamarenot
rotation-invariant.
C Proof Details
C.1 ProofforconvergencerateofblockwiseAdam
WealsoneednewnotationsanddifferentassumptionsforspecificpartitionΦ(·). Forspecificblockb,(b)is
definedasΦ−1(b)={i|Φ(i)=b}andd =#(b),thenumberofparametersinblockb.
b
AsmentionedinSection3.4,wewilluseLemma3.12toboundthesecondordertermundersmoothness
Definition3.9anduseLemma3.13tobettercontrolthegrowthofthesumofsecondorderterm.
Lemma3.12. ForanytwicedifferentiablelosswhichisH-smoothblock-wiselyw.r.t. (∞,2)-norm(Defini-
tion3.9),wehaveforanyxand∆∈Rd,
B
∆⊤∇2L(x)∆≤(cid:88)H db (cid:13) (cid:13)∆ (b)(cid:13) (cid:13)2 2. (1)
b
b=1
ProofofLemma3.12. FromDefinition3.9,weknowthat
√
(cid:13) (cid:13)
H ≥sup
d b(cid:13)∇ (b)L(x+∆)−∇ (b)L(x)(cid:13)
2
b
x,∆ max b′∈[B]
∥∆ √(b′)∥
2
d b′
√ (cid:13) (cid:13)
d (cid:13)∇2 L(x)∆(cid:13)
b(cid:13) (b),: (cid:13)
=sup 2
x,∆ max b′∈[B]
∥∆ √(b′)∥
2
d b′
√ (cid:13) (cid:13)
d (cid:13)(cid:80)B ∇2 L(x)∆ (cid:13)
b(cid:13) b′=1 (b),(b′) (b′)(cid:13)
=sup 2
x,∆ max b′∈[B]
∥∆ √(b′)∥
2
d b′
√ (cid:68) (cid:69)
d ∆′ ,(cid:80)B ∇2 L(x)∆
b (b) b′=1 (b),(b′) (b′)
= sup
x,∆,(cid:13) (cid:13) (cid:13)∆′ (b)(cid:13) (cid:13) (cid:13) 2≤1 max b′∈[B] ∥∆ √(b d′ b) ′∥ 2
(cid:42) B (cid:43)
(cid:112) (cid:88)
= sup d ∆′ , ∇2 L(x)∆
√ b (b) (b),(b′) (b′)
(cid:13) (cid:13)
x,∥∆ (b′)∥ 2≤ d b′,(cid:13) (cid:13)∆′ (b)(cid:13)
(cid:13)
2≤1 b′=1
√ √
B
= x,s ∆u ,p ∆′ b(cid:88) ′=1(cid:13) (cid:13) (cid:13)∆′ (b)(cid:13) (cid:13) (cid:13)d 2b (cid:13) (cid:13)∆d b (′ b′)(cid:13) (cid:13)
2
(cid:68) ∆′ (b),∇2 (b),(b′)L(x)∆ (b′)(cid:69)
17Thenforanyxand∆,weknowthat
H
d
bb (cid:13) (cid:13)∆ (b)(cid:13) (cid:13)2
2
≥ (cid:13) (cid:13)∆ d(b b)(cid:13) (cid:13)2 2 b(cid:88) ′B =1(cid:13)
(cid:13)∆
(b√ )(cid:13) (cid:13)d 2b√ (cid:13) (cid:13)∆d b (′ b′)(cid:13)
(cid:13)
2
(cid:68) ∆ (b),∇2 (b),(b′)L(x)∆ (b′)(cid:69)
√
=
(cid:88)B √d b′(cid:13) (cid:13)∆ (b)(cid:13) (cid:13)
2
(cid:68)
∆ ,∇2 L(x)∆
(cid:69)
b′=1
d b(cid:13) (cid:13)∆ (b′)(cid:13) (cid:13)
2
(b) (b),(b′) (b′)
and
B
2(cid:88)H db (cid:13) (cid:13)∆ (b)(cid:13) (cid:13)2
2
b
b=1
B B
=(cid:88)H db (cid:13) (cid:13)∆ (b)(cid:13) (cid:13)2 2+ (cid:88) H db′ (cid:13) (cid:13)∆ (b′)(cid:13) (cid:13)2
2
b b′
b=1 b′=1
√ √
≥(cid:88)B (cid:88)B √d b′(cid:13) (cid:13)∆ (b)(cid:13) (cid:13)
2
(cid:68)
∆ ,∇2 L(x)∆
(cid:69)
+
(cid:88)B (cid:88)B √d b(cid:13) (cid:13)∆ (b′)(cid:13) (cid:13)
2
(cid:68)
∆ ,∇2 L(x)∆
(cid:69)
b=1b′=1
d b(cid:13) (cid:13)∆ (b′)(cid:13) (cid:13)
2
(b) (b),(b′) (b′)
b′=1b=1
d b′(cid:13) (cid:13)∆ (b)(cid:13) (cid:13)
2
(b′) (b′),(b) (b)
B B
(cid:88)(cid:88)
≥2 ∆⊤ ∇2 L(x)∆ =2∆⊤∇2L(x)∆.
(b) (b),(b′) (b′)
b=1b′=1
Thelastinequalitycomesfrommeaninequality.
Lemma3.13. Givenany0<β <1,foranyscalarsequences{v }T and{g }T satisfythatv ≥0,v >
2 t t=0 t t=1 0 1
0andv −β v ≥(1−β )g2fort≥1,itholdsthat
t 2 t−1 2 t
(cid:88)T g t2
≤T +
β
2
lnv
T. (4)
v 1−β v
t 2 0
t=1
ProofofLemma3.13. Noticethat1−x≤ln 1 foranypositivex. Wecanhavethat
x
(cid:88)T g t2 ≤(cid:88)T v t−β 2v
t−1
v (1−β )v
t 2 t
t=1 t=1
T (cid:20) (cid:18) (cid:19)(cid:21)
=(cid:88) 1+ β 2 1− v t−1
1−β v
2 t
t=1
T
≤T + β 2 (cid:88) ln v t
1−β v
2 t−1
t=1
β v
=T + 2 ln T. (9)
1−β v
2 0
whenv ̸=0. Whenv =0,wecanstillhavethat
0 0
(cid:88)T g2 1 (cid:88)T g2
t ≤ + t
v 1−β v
t 2 t
t=1 t=2
1 β v
≤ +(T −1)+ 2 ln T
1−β 1−β v
2 2 1
β v
=T + 2 ln T .
1−β v /e
2 1
18Nextwedealwiththefirstordertermbyapproximatingitwithadeterministicterm. Hereweneedsomenew
notations. Recallthatg denotesthegradientofmini-batchL (x )atstept. AndE[g |x ]=∇L(x )
t t t−1 t t−1 t−1
becauseEL = L. Thefull-batchgradientisg¯ = ∇L(x ). Differentkindsofsecond-ordermomentum
t t t−1
aredefinedinthefollowingway.
t−1
v
t,b
=β
2t(cid:13)
(cid:13)g
1,(b)(cid:13) (cid:13)2
2/d b+(1−β
2)(cid:88)
β
2j(cid:16)(cid:13)
(cid:13)g
t−j,(b)(cid:13) (cid:13)2 2(cid:17)
/d
b
j=0
v˜
t,b
=(1−β
2)(cid:16)(cid:13)
(cid:13)g¯
t,(b)(cid:13) (cid:13)2
2/d b+σ
b2(cid:17)
+β 2v
t−1,b
Lemma3.14(first-orderapproximation). Foranyblockb∈[B],itholdsthat
E(cid:88)T g √t⊤ ,(b)g¯ t,(b) ≥ 1 E(cid:88)T (cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2 −(cid:112) 1−β Td σ − √d bσ bβ 2 E(cid:20) lnv T,b+ϵ(cid:21) . (5)
v +ϵ 2 (cid:112) v˜ +ϵ 2 b b 1−β v +ϵ
t=1 t,b t=1 t,b 2 0,b
ProofofLemma3.14. Thefirstorderchangeinblockbcandecomposedintotwoterms.
 
T T T
E(cid:88) (cid:88) √g t,ig¯ t,i =E(cid:88) (cid:88) (cid:112)g t,ig¯ t,i +E (cid:88) (cid:88) √g t,ig¯ t,i − (cid:112)g t,ig¯ t,i 
v +ϵ v˜ +ϵ v +ϵ v˜ +ϵ
t=1Φ(i)=b t,b t=1Φ(i)=b t,b t=1Φ(i)=b t,b t,b
=E(cid:88) t=T 1Φ(cid:88) (i)=bE(cid:34) (cid:112)g v˜t, ti ,bg¯ t +,i ϵ(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)x t−1(cid:35) +E (cid:88) t=T 1Φ(cid:88) (i)=b√g vt t,i ,bg¯ t +,i ϵ − (cid:112)g v˜t, ti ,bg¯ t +,i ϵ 
 
=E(cid:88)T (cid:88) (cid:112) g¯ t2 ,i +E (cid:88)T (cid:88) √g t,ig¯ t,i − (cid:112)g t,ig¯ t,i 
v˜ +ϵ v +ϵ v˜ +ϵ
t=1Φ(i)=b t,b t=1Φ(i)=b t,b t,b
(10)
Forthesecondterm,wehavethat
(cid:12) (cid:32) (cid:33)(cid:12)
(cid:88) (cid:12) 1 1 (cid:12)
(cid:12)g g¯ √ − (cid:12)
(cid:12) (cid:12) t,i t,i v t,b+ϵ (cid:112) v˜ t,b+ϵ (cid:12) (cid:12)
Φ(i)=b
=
(cid:88)
√
(cid:112)|g t,ig¯ t,i( (cid:0)v˜ √t,b−v t,b)|
(cid:112) (cid:1)
v +ϵ v˜ +ϵ v +ϵ+ v˜ +ϵ
t,b t,b t,b t,b
Φ(i)=b
(cid:88)
(cid:12) (cid:12) (cid:12)g t,ig¯ t,i(1−β 2)(cid:16)(cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b+σ b2−(cid:13) (cid:13)g t,(b)(cid:13) (cid:13)2 2/d b(cid:17)(cid:12) (cid:12)
(cid:12)
= √ (cid:112) (cid:0)√ (cid:112) (cid:1)
v +ϵ v˜ +ϵ v +ϵ+ v˜ +ϵ
t,b t,b t,b t,b
Φ(i)=b
(cid:12) (cid:18)(cid:113) (cid:113) (cid:19)(cid:18)(cid:113) (cid:113) (cid:19)(cid:12)
(cid:88)
(cid:12) (cid:12) (cid:12)g t,ig¯ t,i(1−β 2) (cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b+σ b2+ (cid:13) (cid:13)g t,(b)(cid:13) (cid:13)2 2/d b (cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b+σ b2− (cid:13) (cid:13)g t,(b)(cid:13) (cid:13)2 2/d b (cid:12) (cid:12)
(cid:12)
= √ (cid:112) (cid:0)√ (cid:112) (cid:1)
v +ϵ v˜ +ϵ v +ϵ+ v˜ +ϵ
t,b t,b t,b t,b
Φ(i)=b
(cid:12) √ (cid:18)(cid:113) (cid:113) (cid:19)(cid:12)
(cid:88)
(cid:12) (cid:12) (cid:12)g t,ig¯ t,i 1−β 2 (cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b+σ b2− (cid:13) (cid:13)g t,(b)(cid:13) (cid:13)2 2/d b (cid:12) (cid:12)
(cid:12)
≤ √ (cid:112)
v +ϵ v˜ +ϵ
t,b t,b
Φ(i)=b
(cid:18)(cid:113) (cid:113) (cid:19)2
≤1 (cid:88) g¯ t2 ,i
(cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b+σ b2− (cid:13) (cid:13)g t,(b)(cid:13) (cid:13)2 2/d
b
2 (cid:112) v˜ +ϵ (cid:18)(cid:113) (cid:113) (cid:19)2
Φ(i)=b t,b E[ (cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b+σ b2− (cid:13) (cid:13)g t,(b)(cid:13) (cid:13)2 2/d
b
|x t−1]
(cid:18)(cid:113) (cid:113) (cid:19)2
1 (cid:88)
(1−β 2)g t2 ,iE[ (cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b+σ b2− (cid:13) (cid:13)g t,(b)(cid:13) (cid:13)2 2/d
b
|x t−1]
+
(cid:112)
2 (v +ϵ) v˜ +ϵ
t,b t,b
Φ(i)=b
19Thefirstinequalityisbecausev t,b+ϵ≥(1−β
2)(cid:13)
(cid:13)g
t,(b)(cid:13) (cid:13)2
2/d bandv˜ t,b+ϵ≥(1−β
2)(cid:16)(cid:13)
(cid:13)g¯
t,(b)(cid:13) (cid:13)2
2/d b+σ
b2(cid:17)
.
Forthefirstterm,itwillbeexactly 1 2∥ √g¯ vt ˜, t( ,b b) +∥ ϵ2 2 aftertakingexpectationconditionalonx t−1. Forthesecond
term,wehavethefollowinginequality
E(cid:34)(cid:18)(cid:113)
(cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b+σ b2−(cid:113) (cid:13) (cid:13)g t,(b)(cid:13) (cid:13)2 2/d
b(cid:19)2(cid:12)
(cid:12) (cid:12) (cid:12)x
t−1(cid:35)
(cid:12)
 (cid:12) 
(cid:113) (cid:113) (cid:12)
=E (cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b+σ b2+ (cid:88) g t2 ,j/d b−2 (cid:13) (cid:13)g t,(b)(cid:13) (cid:13)2 2/d b (cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b+σ i2(cid:12) (cid:12) (cid:12)x t−1
Φ(j)=b (cid:12)
≤2(cid:16)(cid:13)
(cid:13)g¯
t,(b)(cid:13) (cid:13)2
2/d b+σ
b2(cid:17) −2(cid:113) (cid:13)
(cid:13)g¯
t,(b)(cid:13) (cid:13)2
2/d b+σ
b2E(cid:20)(cid:113) (cid:13)
(cid:13)g
t,(b)(cid:13) (cid:13)2
2/d
b(cid:12) (cid:12)
(cid:12) (cid:12)x
t−1(cid:21)
≤2(cid:16)(cid:13)
(cid:13)g¯
t,(b)(cid:13) (cid:13)2
2/d b+σ
b2(cid:17) −2(cid:113) (cid:13)
(cid:13)g¯
t,(b)(cid:13) (cid:13)2
2/d b+σ
b2(cid:113) (cid:13)
(cid:13)g¯
t,(b)(cid:13) (cid:13)2
2/d
b
(cid:113) (cid:18)(cid:113) (cid:113) (cid:19)
=2 (cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b+σ b2 (cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b+σ b2− (cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d
b
(cid:113)
≤2 (cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b+σ b2σ b.
The first inequality comes from Assumption 3.4. The second inequality is because ℓ norm is a convex
2
function. Thenweknowthat
(cid:34)(cid:18)(cid:113) (cid:113) (cid:19)2 (cid:35)
(1−β 2)g t2 ,iE (cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b+σ b2− (cid:13) (cid:13)g t,(b)(cid:13) (cid:13)2 2/d
b
|x
t−1
(cid:88)
(cid:112)
(v +ϵ) v˜ +ϵ
t,b t,b
Φ(i)=b
(cid:113)
≤
(cid:88)
(1−β 2)g t2 ,i2 (cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b+σ b2σ
b
(cid:112)
(v +ϵ) v˜ +ϵ
t,b t,b
Φ(i)=b
≤2(cid:112) 1−β σ (cid:88) g t2 ,i .
2 b v +ϵ
t,b
Φ(i)=b
ThenbacktoEquation10,wehavethat
 
E(cid:88)T (cid:88) √g t,ig¯ t,i =E(cid:88)T (cid:88) (cid:112) g¯ t2 ,i +E (cid:88)T (cid:88) √g t,ig¯ t,i − (cid:112)g t,ig¯ t,i 
v +ϵ v˜ +ϵ v +ϵ v˜ +ϵ
t=1Φ(i)=b t,b t=1Φ(i)=b t,b t=1Φ(i)=b t,b t,b
≥E(cid:88)T (cid:88) g¯ t2 ,i − 1 E(cid:88)T (cid:88) g¯ t2 ,i − 1 2(cid:112) 1−β σ E(cid:88)T (cid:13) (cid:13)g t,(b)(cid:13) (cid:13)2 2
(cid:112) v˜ +ϵ 2 (cid:112) v˜ +ϵ 2 2 b v +ϵ
t=1Φ(i)=b t,b t=1Φ(i)=b t,b t=1 t,b
= 1 E(cid:88)T (cid:88) g¯ t2 ,i −(cid:112) 1−β σ E(cid:88)T (cid:13) (cid:13)g t,(b)(cid:13) (cid:13)2 2
2 (cid:112) v˜ +ϵ 2 b v +ϵ
t=1Φ(i)=b t,b t=1 t,b
Forthesecondterm,wecanapplyLemma3.13andgetthat
(cid:88)T (cid:80) Φ(i)=bg t2 ,i/d b ≤T + β 2 lnv T,b+ϵ .
v +ϵ 1−β v +ϵ
t,b 2 0,b
t=1
Combiningthesetwoterms,wecangetthat
E(cid:88)T (cid:88) √g t,ig¯ t,i ≥ 1 E(cid:88)T (cid:88) g¯ t2 ,i −(cid:112) 1−β Td σ − √d bσ bβ 2 E(cid:20) lnv T,b+ϵ(cid:21) .
v +ϵ 2 (cid:112) v˜ +ϵ 2 b b 1−β v +ϵ
t=1Φ(i)=b t,b t=1Φ(i)=b t,b 2 0,b
20NextweneedLemma3.15todealwiththedenominatorintheapproximatedfirstorderterm. Thelemma
islargelyinspiredbyLemma6inLi&Lin(2024),wherewefurthergeneralizeittothecaseofblock-wise
Adam.
Lemma3.15. Foranyblockb∈[B],itholdsthat
(cid:88)T E(cid:104)(cid:112)
v˜
+ϵ(cid:105)
≤
2β 2T 4 √
v +
T
σ +
T√ ϵ+2(cid:88)T E(cid:34)(cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b(cid:35)
. (7)
t,b 1−β 0,b 2 b 2 (cid:112) v˜ +ϵ
t=T+1 2 t=1 t,b
2
ProofofLemma3.15. Foreacht≤T,wehavethat
E(cid:104)(cid:112)
v˜
+ϵ(cid:105)
t,b
(cid:20)(cid:113) (cid:21)
=E β 2v t−1,b+(1−β 2)((cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b+σ b2)+ϵ
  (cid:34)(cid:13) (cid:13)2 (cid:35)
=E (cid:113)
β 2v
t−1,b+β 2 (v 1t− −1, βb 2+ )(( (cid:80)1− Φ(iβ )2 =) bσ g¯b2
t2
,+ i/dϵ
b+σ
b2)+ϵ+(1−β 2)E (cid:13)g (cid:112)¯ t, v( ˜b t) ,b(cid:13) 2 +/ ϵd b
≤E(cid:20)(cid:113)
β v +(1−β
)σ2+ϵ(cid:21)
+(1−β
)E(cid:34)(cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b(cid:35)
.
2 t−1,b 2 b 2 (cid:112) v˜ +ϵ
t,b
Andforeachs≤t−1,wehavethat
(cid:20)(cid:113) (cid:21)
E βsv +(1−βs)σ2+ϵ
2 t−s,b 2 b
 
(cid:115)
(cid:88)
=E  β 2s+1v t−s−1,b+β 2s(1−β 2) g t2 −s,i/d b+(1−β 2s)σ b2+ϵ
Φ(i)=b
  (cid:12) 
(cid:115) (cid:12)
=E E  β 2s+1v t−s−1,b+β 2s(1−β 2) (cid:88) g t2 −s,i/d b+(1−β 2s)σ b2+ϵ(cid:12) (cid:12) (cid:12)x t−s−1
Φ(i)=b (cid:12)
(cid:118) (cid:117)  (cid:12)  
≤E (cid:117) (cid:117) (cid:116)β 2s+1v t−s−1,b+β 2s(1−β 2)E  (cid:88) g t2 −s,i/d b(cid:12) (cid:12) (cid:12) (cid:12)x t−s−1+(1−β 2s)σ b2+ϵ 
Φ(i)=b (cid:12)
 
(cid:115)
(cid:88)
≤E  β 2s+1v t−s−1,b+β 2s(1−β 2) g¯ t2 −s,i/d b+(1−β 2s+1)σ b2+ϵ
Φ(i)=b
 
βs+1v +(1−βs+1)σ2+ϵ
=E
(cid:113)
2 t−s−1,b 2 b

βs+1v +βs(1−β )(cid:80) g¯2 /d +(1−βs+1)σ2+ϵ
2 t−s−1,b 2 2 Φ(i)=b t−s,i b 2 b
 
βs(1−β )(cid:80) g¯2 /d
+E
(cid:113)
2 2 Φ(i)=b t−s,i b

βs+1v +βs(1−β )(cid:80) g¯2 /d +(1−βs+1)σ2+ϵ
2 t−s−1,b 2 2 Φ(i)=b t−s,i b 2 b
(cid:20)(cid:113) (cid:21) (cid:34)(cid:80) g¯2 /d (cid:35)
≤E βs+1v +(1−βs+1)σ2+ϵ +(cid:112) βs(1−β )E Φ(i)=b t−s,i b .
2 t−s−1,b 2 b 2 2 (cid:112) v˜ +ϵ
t−s,b
21Bysummingtheaboveinequalityovers=1,··· ,t−1,wehavethat
(cid:20)(cid:113) (cid:21)
E β v +(1−β )σ2+ϵ
2 t−1,b 2 b
≤E(cid:20)(cid:113)
βtv
+(1−βt)σ2+ϵ(cid:21) +(cid:88)t−1
(cid:112)
βs(1−β
)E(cid:34)(cid:80) Φ(i)=bg¯ t2 −s,i/d b(cid:35)
2 0,b 2 b 2 2 (cid:112) v˜ +ϵ
s=1 t−s,b
≤(cid:113)
βtv
+(cid:113) σ2+ϵ+(cid:88)t−1
(cid:112)
βs(1−β
)E(cid:34)(cid:80) Φ(i)=bg¯ t2 −s,i/d b(cid:35)
.
2 0,b b 2 2 (cid:112) v˜ +ϵ
s=1 t−s,b
and
E(cid:104)(cid:112)
v˜
+ϵ(cid:105) ≤(cid:113)
βtv
+(cid:113) σ2+ϵ+(cid:88)t−1 (cid:112)
βs(1−β
)E(cid:34)(cid:80) Φ(i)=bg¯ t2 −s,i/d b(cid:35)
.
t,b 2 0,b b 2 2 (cid:112) v˜ +ϵ
s=0 t−s,b
Bysummingtheaboveinequalityovert= T +1,··· ,T,wehavethat
2
(cid:88)T (cid:104)(cid:112)
v˜
+ϵ(cid:105)
≤
(cid:88)T (cid:113)
βtv +
T(cid:113)
σ2+ϵ+
(cid:88)T (cid:88)t−1 (cid:112)
βs(1−β
)E(cid:34)(cid:80) Φ(i)=bg¯ t2 −s,i/d b(cid:35)
t,b 2 0,b 2 b 2 2 (cid:112) v˜ +ϵ
t=T+1 t=T+1 t=T+1s=0 t−s,b
2 2 2
≤
β 2√T 4 √
v +
T(cid:113)
σ2+ϵ+
1− √β
2
(cid:88)T E(cid:34)(cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b(cid:35)
1− β 0,b 2 b 1− β (cid:112) v˜ +ϵ
2 2 t=1 t,b
=
β 2√T 4 √
v +
T(cid:113) σ2+ϵ+(1+(cid:112)
β
)(cid:88)T E(cid:34)(cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b(cid:35)
1− β 0,b 2 b 2 (cid:112) v˜ +ϵ
2 t=1 t,b
≤
2β 2T 4 √
v +
T
σ +
T√ ϵ+2(cid:88)T E(cid:34)(cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)2 2/d b(cid:35)
.
1−β 0,b 2 b 2 (cid:112) v˜ +ϵ
2 t=1 t,b
ThisfollowingLemmaC.1istocontrolthegrowthofv sothattherighthandsideinLemma3.13is
T,b
(cid:16) (cid:17)
indeedΘ T + logT insteadofΘ(T)whenalltheconstantsarepoly(T).
1−β2
LemmaC.1. ForanyT,itholdsthat
lnEmax b∈[B]v T,b+ϵ ≤2ln(cid:32)
1+
(cid:80)B b=1σ b2+∥∇L(x 0)∥2 Φ+max b∈[B]H b2η2T(T + 1−1 β2)(cid:33)
+ln32
v +ϵ v +ϵ
0 0
22ProofofLemmaC.1. Fromthedefinitionofv andAssumption3.4,wehavethat
t,i
Emaxv
t,b
b∈[B]
(cid:34) t (cid:35)
=E bm ∈[a Bx
]
β 2tv 0,b+(1−β 2)(cid:88) β 2t−s(cid:13) (cid:13)g s,(b)(cid:13) (cid:13)2 2/d
b
s=1
t
≤β 2t∥v 0∥ ∞+(1−β 2)E bm ∈[a Bx ](cid:88) β 2t−s(cid:13) (cid:13)g s,(b)(cid:13) (cid:13)2 2/d
b
s=1
t
=β 2t∥v 0∥ ∞+(1−β 2)E bm ∈[a Bx ](cid:88) β 2t−s(cid:13) (cid:13)E[g s,(b)|x s−1]+g s,(b)−E[g s,(b)|x s−1](cid:13) (cid:13)2 2/d
b
s=1
t
≤β 2t∥v 0∥ ∞+(1−β 2)E bm ∈[a Bx ](cid:88) β 2t−s(cid:104) 2(cid:13) (cid:13)E[g s,(b)|x s−1](cid:13) (cid:13)2 2+2(cid:13) (cid:13)g s,(b)−E[g s,(b)|x s−1](cid:13) (cid:13)2 2(cid:105) /d
b
s=1
B t t
≤β 2t∥v 0∥ ∞+2(1−β 2)E(cid:88)(cid:88) β 2t−s(cid:13) (cid:13)g s,(b)−E[g s,(b)|x s−1](cid:13) (cid:13)2 2/d b+2(1−β 2)E bm ∈[a Bx ](cid:88) β 2t−s(cid:13) (cid:13)∇ (b)L(x s−1)(cid:13) (cid:13)2 2/d
b
b=1s=1 s=1
B t
≤β 2t∥v 0∥ ∞+2(1−β 2t)(cid:88) σ b2+2(1−β 2)E bm ∈[a Bx ](cid:88) β 2t−s(cid:104) 2(cid:13) (cid:13)∇ (b)L(x 0)(cid:13) (cid:13)2 2+2(cid:13) (cid:13)∇ (b)L(x s−1)−∇ (b)L(x 0)(cid:13) (cid:13)2 2(cid:105) /d
b
b=1 s=1
B
≤β 2t∥v 0∥ ∞+2(1−β 2t)(cid:88) σ b2+4(1−β 2t) bm ∈[a Bx ](cid:13) (cid:13)∇ (b)L(x 0)(cid:13) (cid:13)2 2/d
b
b=1
t
+4(1−β 2)E bm ∈[a Bx ](cid:88) β 2t−s(cid:13) (cid:13)∇ (b)L(x s−1)−∇ (b)L(x 0)(cid:13) (cid:13)2 2/d
b
s=1
B
≤β 2t∥v 0∥ ∞+2(1−β 2t)(cid:88) σ b2+4(1−β 2t) bm ∈[a Bx ](cid:13) (cid:13)∇ (b)L(x 0)(cid:13) (cid:13)2 2/d
b
b=1
+4(1−β
)Emax(cid:88)t βt−sH b2
max
(cid:13) (cid:13)x s−1,(b′)−x 0,(b′)(cid:13) (cid:13)2
2
2 b∈[B] s=1 2 d2 b b′∈[B] d b′
≤β 2t∥v 0∥ ∞+2(cid:88) b=B 1σ b2+4 bm ∈[a Bx ](cid:13) (cid:13)∇ (b)L(x 0)(cid:13) (cid:13)2 2/d b+4(1−β 2)( bm ∈[a Bx ]H
d2
bb2 )E(cid:88) s=t 1β 2t−s bm ′∈a [Bx ](cid:13) (cid:13)x s−1,(b′) d−
b′
x 0,(b′)(cid:13) (cid:13)2 2.
We define C = v
0
+2(cid:80)B b=1σ b2 +4max b∈[B](cid:13) (cid:13)∇ (b)L(x 0)(cid:13) (cid:13)2 2/d
b
for simplicity. From Lemma 3.13 and
Cauchyinequality,weknowthat
d1 (cid:13) (cid:13)x t,(b′)−x 0,(b′)(cid:13) (cid:13)2
2
= dη2 (cid:88) (cid:12) (cid:12) (cid:12) (cid:12)(cid:88)t √ vg s,j +ϵ(cid:12) (cid:12) (cid:12) (cid:12)2
b′ b′ (cid:12) s,b′ (cid:12)
Φ(j)=b′ s=1
≤ η2 (cid:88) t(cid:88)t g s2 ,j
d v +ϵ
b′ s,b′
Φ(j)=b′ s=1
t (cid:80) g2 /d
=η2t(cid:88) Φ(j)=b′ s,j b′
v +ϵ
s,b′
s=1
(cid:18) (cid:19)
β v +ϵ
≤η2t t+ 2 ln t,b′
1−β v +ϵ
2 0,b′
β max v +ϵ
≤η2t2+η2t 2 ln b∈[B] t,b
1−β v +ϵ
2 0
23Therighthandsideisindependentofspecificblockb. Sowecangetthat
(cid:13) (cid:13)2
max
(cid:13)x t,(b′)−x 0,(b′)(cid:13)
2 ≤η2t2+η2t
β
2
lnmax b∈[B]v t,b+ϵ
b′∈[B] d b′ 1−β 2 v 0+ϵ
and
(cid:13) (cid:13)2
E max
(cid:13)x t,(b′)−x 0,(b′)(cid:13)
2
b′∈[B] d b′
β max v +ϵ
≤η2t2+η2t 2 Eln b∈[B] t,b
1−β v +ϵ
2 0
β Emax v +ϵ
≤η2t2+η2t 2 ln b∈[B] t,b
1−β v +ϵ
2 0
≤η2t2+η2t β 2 lnϵ+C+4(1−β 2)max b∈[B] H d2 bb2E(cid:80)t s=1β 2t−smax b′∈[B] ∥x s−1,(b′ d) b− ′x 0,(b′)∥2 2 .
1−β v +ϵ
2 0
DefineG=max
1≤t≤T
Emax
b′∈[B]
∥x t,(b′)−
d
bx ′0,(b′)∥2 2. Thereexistst≤T suchthat
(cid:13) (cid:13)2
G=E max
(cid:13)x t,(b′)−x 0,(b′)(cid:13)
2
b′∈[B] d b′
≤η2t2+η2t β 2 lnϵ+C+4(1−β 2)max b∈[B] H d2 bb2E(cid:80)t s=1β 2t−smax b′∈[B] ∥x s−1,(b′ d) b− ′x 0,(b′)∥2 2
1−β v +ϵ
2 0
≤η2t2+η2t β 2
lnϵ+C+4(1−β 2)max
b∈[B]
H
d2
bb2 (cid:80)t s=1β 2t−sG
1−β v +ϵ
2 0
≤η2T2+η2T β 2
lnϵ+C+4max
b∈[B]
H
d2
bb2 G
.
1−β v +ϵ
2 0
WefurtherdefineG′ =ϵ+C+4max H b2 Gandgetthat
b∈[B] d2
b
H2 H2 β G′
G′ ≤ϵ+C+4max bη2T2+4max bη2T 2 ln
b∈[B] d2 b b∈[B] d2 b 1−β 2 v 0+ϵ
H2
≤ϵ+C+4max bη2T2
b∈[B]
d2
b
 
H2 β G′(1−β ) H2 β
+4max bη2T 2 ln 2 +ln4max bη2T 2 
b∈[B] d2 b 1−β 2 4max b∈[B] H d2b2 η2Tβ 2 b∈[B] d2 b (1−β 2)(v 0+ϵ)
b
H2 G′ H2 β H2 β
≤ϵ+C+4max bη2T2+ +4max bη2T 2 ln4max bη2T 2
b∈[B] d2 b 2 b∈[B] d2 b 1−β 2 b∈[B] d2 b (1−β 2)(v 0+ϵ)
H2 G′ (cid:18) H2 β (cid:19)2
≤ϵ+C+4max bη2T2+ +(v +ϵ) 4max bη2T 2 .
b∈[B] d2 b 2 0 b∈[B] d2 b (1−β 2)(v 0+ϵ)
Thelasttwoinequalitescomefromlnx≤ x andxln(x)≤x2. Thenwecangetthat
2
H2 (cid:88)t
Emaxv +ϵ≤ϵ+C+4(1−β )max b βt−sG
b∈[B]
T,b 2
b∈[B]
d2
b s=1
2
H2
≤ϵ+C+4max bG=G′
b∈[B]
d2
b
H2 (cid:18) H2 β (cid:19)2
≤2ϵ+2C+8max bη2T2+32(v +ϵ) max bη2T 2
b∈[B] d2 b 0 b∈[B] d2 b (1−β 2)(v 0+ϵ)
24and
Emax v +ϵ
b∈[B] T,b
ln
v +ϵ
0
≤ln2ϵ+2C+8max
b∈[B]
H
d2
bb2 η2T2+32(v 0+ϵ)(cid:16) max
b∈[B]
H
d2
bb2 η2T (1−β2β )2 (v0+ϵ)(cid:17)2
v +ϵ
0
≤ln

2
1+
(cid:80)B b=1σ b2+
v
2 +∥∇
ϵ
L(x 0)∥2
Φ +
2max b v∈[B +] H ϵd2 bb2 η2T2
+
4m (1ax −b∈ β[B )] (H vd2 bb2 +η2 ϵT )β 2 2


0 0 2 0
(cid:32) (cid:80)B σ2+∥∇L(x )∥2 +max H2η2T(T + 1 )(cid:33)
≤2ln 1+ b=1 b 0 Φ b∈[B] b 1−β2 +ln32
v +ϵ
0
Finally,wegivetheproofforTheorem3.11. WhenΦ(i)=i,i.e.,eachparameterformsasingleblock,it
becomestheproofforTheorem3.5.
Theorem3.11(Main,BlockwiseAdam). UnderAssumption3.10,supposeLisH-smoothblockwiselyw.r.t.
(∞,2)-norm,whereH=(H ,...,H )∈RB,forAlgorithm3,wehavethat
1 B
(cid:118)
Tm <ti ≤n
TE(cid:88)B (cid:112)
d
b(cid:13)
(cid:13)∇ (b)L(x
t)(cid:13)
(cid:13)
2
≤E+√ E(cid:117) (cid:117)
(cid:116)
T(1β −2T 4
β
2)d√
v
0+(cid:88)B
d bσ
b+d√
ϵ
2 b=1 b=1
with
(cid:18) (cid:19)(cid:32) B B (cid:33)
E =
2
E[L(x )−L(x )]+ 1+
β 2F η(cid:88)
H
+(cid:112)
1−β
(cid:88)
d σ ,
ηT 0 T T(1−β ) b 2 b b
2
b=1 b=1
and
(cid:32) (cid:80)B σ2+∥∇L(x )∥2 +max H2η2T(T + 1 )(cid:33)
F =2ln 1+ b=1 b 0 Φ b∈[B] b 1−β2 +ln32.
v +ϵ
0
ProofofTheorem3.11. Inasinglestep,wecanapplyLemma3.12andhavethat
B
L(x )−L(x )≤∇L(x )⊤(x −x )+ 1(cid:88)H b (cid:88) (x −x )2
t t−1 t−1 t t−1 2 d t,i t−1,i
b
b=1 Φ(i)=b
=−η(cid:88)d g t,ig¯
t,i +
1 η2(cid:88)B H b(cid:13) (cid:13)g t,(b)(cid:13) (cid:13)2
2.
(cid:112)
v +ϵ 2 d v +ϵ
t,Φ(i) b t,b
i=1 b=1
Ifwesumovertfrom1toT andtakeexpectation,wecanget
E[L(x )−L(x
)]≤−E(cid:34) η(cid:88)d (cid:88)T g t,ig¯
t,i
(cid:35)
+
1 η2E(cid:34) (cid:88)B H
b
(cid:88)T (cid:13) (cid:13)g t,(b)(cid:13) (cid:13)2 2(cid:35)
T 0 (cid:112) v +ϵ 2 d v +ϵ
i=1 t=1 t,Φ(i) b=1 b t=1 t,b
(cid:34) d T (cid:35) (cid:34) B (cid:18) (cid:19)(cid:35)
≤−E η(cid:88)(cid:88) g t,ig¯ t,i + 1 η2E (cid:88) H T + β 2 lnv T,b+ϵ .
(cid:112) v +ϵ 2 b 1−β v +ϵ
i=1 t=1 t,Φ(i) b=1 2 0,b
25ThesecondinequalitycomesfromapplyingLemma3.13. ByLemma3.14,wehavethat
1 E(cid:34) (cid:88)d (cid:88)T g¯ t2
,i
(cid:35)
≤
2
E[L(x )−L(x )]+
η E(cid:34) (cid:88)B
H
(cid:18)
T +
β
2
lnv T,b+ϵ(cid:19)(cid:35)
T (cid:112) v˜ +ϵ ηT 0 T T b 1−β v +ϵ
i=1 t=1 t,Φ(i) b=1 2 0,b
B (cid:18) (cid:19)
+ 1 (cid:88) d σ (cid:112) 1−β T + β 2 Elnv T,b+ϵ
T b b 2 1−β v +ϵ
2 0,b
b=1
B B
2 (cid:88) (cid:112) (cid:88)
≤ E[L(x )−L(x )]+η H + 1−β d σ
ηT 0 T b 2 b b
b=1 b=1
(cid:32) B B (cid:33)
+ β 2 η(cid:88) H +(cid:112) 1−β (cid:88) σ maxElnv T,b+ϵ
T(1−β 2) b 2 b b∈[B] v 0,b+ϵ
b=1 b=1
B B
2 (cid:88) (cid:112) (cid:88)
≤ E[L(x )−L(x )]+η H + 1−β d σ
ηT 0 T b 2 b b
b=1 b=1
+ β 2 (cid:32) η(cid:88)B H +(cid:112) 1−β (cid:88)B d σ (cid:33) lnEmax b∈[B]v T,b+ϵ
T(1−β ) b 2 b b v +ϵ
2 0
b=1 b=1
FromLemmaC.1,wecandefine
B B (cid:32) B B (cid:33)
E = 2 E[L(x )−L(x )]+η(cid:88) H +(cid:112) 1−β (cid:88) d σ + β 2 η(cid:88) H +(cid:112) 1−β (cid:88) d σ F,
ηT 0 T b 2 b b T(1−β ) b 2 b b
2
b=1 b=1 b=1 b=1
with
(cid:32) (cid:80)B σ2+∥∇L(x )∥2 +max H2η2T(T + 1 )(cid:33)
F =2ln 1+ b=1 b 0 Φ b∈[B] b 1−β2 +ln32.
v +ϵ
0
Thenitholdsthat
1 E(cid:34) (cid:88)d (cid:88)T g¯ t2
,i
(cid:35)
≤E.
(cid:112)
T v˜ +ϵ
i=1 t=1 t,Φ(i)
ByLemma3.15andCauchyinequality,wehavethat
 1  1
T2 E (cid:88)T (cid:88)B (cid:112) d b(cid:13) (cid:13)g¯ t,(b)(cid:13) (cid:13)
2
≤ T2 E (cid:88)T (cid:88)B (cid:112)(cid:13) (cid:13)g¯ v˜t,(b) +(cid:13) (cid:13)2 2 ϵ2  T2 E (cid:88)T (cid:88)B d b(cid:112) v˜ t,b+ϵ2
t=T+1b=1 t=T+1b=1 t,b t=T+1b=1
2 2 2
 1  1
≤ T2 E (cid:88)T (cid:88)B (cid:88) (cid:112) v˜g¯ t2 ,i +ϵ2  T2 E (cid:88)T (cid:88)B d b(cid:112) v˜ t,b+ϵ2
t=T+1b=1Φ(i)=b t,b t=T+1b=1
2 2
√ (cid:32) 4βT 4 √ (cid:88)B √ (cid:33) 21
≤ 2E 4E+ 2 d v + d σ +d ϵ
T(1−β ) 0 b b
2
b=1
(cid:118)
√ √ √ (cid:117) (cid:117) 4βT 4 √ (cid:88)B √
≤2 2E+ 2 E(cid:116) 2 d v + d σ +d ϵ.
T(1−β ) 0 b b
2
b=1
Thiscompletestheproof.
26D Experiment Details
D.1 Adamonarotatedloss
AkeydifficultyinimplementingrotatedAdamarisesfromapplyinganorthogonalrotationontheparameters
beforecalculatingtheloss. Itiscomputationallyinfeasibletoapplya125M×125Morthogonalmatrixon
the 125M-sized parameter vector. To avoid such computation, we design a new orthogonal transformer to
rotatetheparametersofthenetwork. Inwhatfollows,weelaborateonthisrotation.
RandPerm. Givenavectorvofsized,wecanorthogonallyrotateitbyrepeatedlyapplyingtheseconsecutive
operations: 1. Permute the entries of the vector according to a randomly chosen permutation π ∈ S . 2.
d
Reshape the permuted vector into a 3D tensor of size [s ,s ,s ], apply a fixed orthogonal rotation of size
1 2 3
s×soneachsideofthetensorandthenreshapeitbacktoavectorofsized.
This operation performs an orthogonal transformation R on the input vector v. We can chain multiple
operations of this kind and construct RandPermk, where k is a positive number indicating the number
of consecutive RandPerm s applied. Building upon this rotation, we train GPT-2 125M with Adam on
L◦RandPerm2toanalyzeourhypothesisregardingtheℓ geometryofthelosslandscapeandtoverifythat
∞
Adam will indeed suffer from the induced orthogonal equivariance. Figure 1 confirms our findings, as the
performanceofrotatedAdamwithRandPerm2issignificantlyworsethanAdam. ThissuggeststhatAdamis
highlysensitivetotherotationandadaptivityalonecan’texplainitsadvantage.
D.2 Computationofmatrixnorms
ItiscomputationallyinfeasibletogetthefullHessianmatrixanddirectlycomputenormsofit. Insteadwe
leverageHessianvectorproductfunctioninJaxtoprobetheHessianmatrix. WeuseLanczosalgorithmto
estimatespectralnorm. Theestimationof(1,1)-normreliesonthepropertiesofCauchydistribution. Given
a = [a ,··· ,a ]andi.i.d. standardCauchyvariablesX ,··· ,X ,
(cid:80)n
a X isCauchydistributedwith
1 n 1 n i=1 i i
mean0andscale(cid:80)n
|a |. ForasinglevalueCauchydistributionwithmean0andscalingγ,anestimator
i=1 i
forγ isthemedianoftheabsolutevaluesofallthesamples.
Therefore,weproposeAlgorithm4toestimatethesumofabsolutevaluesforeachrowandsumoverall
therowstoget(1,1)-normofHessianmatrix. Wechoosen=50forallthemeasurementexperiments. We
alsoproveaconcentrationinequalityinTheoremD.1.
Algorithm4Estimationof(1,1)-NormofHessian,∇2L(θ)
Input: NumberofCauchyvectorsn,parameterθ ∈Rd,lossL
1: fori=1→n:
2: SampleaindependentCauchyvectorv(i) ∈Rdwherev(i) i. ∼i.d. Cauchy(0,1)forj =1,...,d.
j
3: H :,i ←∇2L(θ)·v(i) (Usinghessian-vectorproduct)
4:
return(cid:80)d
j=1median(abs(H j,:))
TheoremD.1. FortheestimateinAlgorithm4,itholdsthat
(cid:12) (cid:12) 
(cid:12) d (cid:12) (cid:18) (cid:19)
P (cid:12) (cid:12) (cid:12)(cid:88) median(|H j,:|)−(cid:13) (cid:13)∇2L(θ)(cid:13) (cid:13) 1,1(cid:12) (cid:12) (cid:12)≥ϵ(cid:13) (cid:13)∇2L(θ)(cid:13) (cid:13) 1,1≤2d·exp − 28 5n π2ϵ2
(cid:12)j=1 (cid:12)
foreveryϵ∈(0,1).
ProofofTheoremD.1. WefirstprovethefollowingconcentrationinequalityforM =median(|X |,··· ,|X |)
n 1 n
i.i.d.
inwhichX ,··· ,X ∼ Cauchy(0,1).
1 n
(cid:18) (cid:19)
8n
P(|M −1|≥ϵ)≤2exp − ϵ2 . (11)
n 25π2
27GivenX ∼ Cauchy[0,1],thecumulativedistributionfunctionof|X|isF(x) = 2arctan(x)andthemedian
π
of|X|is1becauseF(1)=0.5. Forafixedϵ∈(0,1),definep = 2arctan(1−ϵ)tobetheprobabilitythat
|X|issmallerthan1−ϵandS
=(cid:80)n
1 . Since1
1 π
followsi.i.d. Bernoullidistributionwith
i=1 |Xi|≤1−ϵ |Xi|≤1−ϵ
p ,S ∼Bin(n,p ).
1 1
M ≤ 1−ϵ if and only if at least n+1 X ’s are smaller than 1−ϵ. And we can apply Hoeffding’s
n 2 i
inequalityonS andgetthat
n+1 n
P(M ≤1−ϵ)=P(S ≥ )≤P(S ≥ )
n 2 2
n
=P(S−np ≥ −np )
1 2 1
(cid:18) 2(n −np )2(cid:19)
≤exp − 2 1
n
(cid:32) (cid:18)
1 2
(cid:19)2(cid:33)
=exp −2n − arctan(1−ϵ) .
2 π
Withsimilarderivation,wecanalsogetthat
(cid:32) (cid:18)
1 2
(cid:19)2(cid:33)
P(M ≥1+ϵ)≤exp −2n − arctan(1+ϵ) .
n 2 π
Whenϵ∈[−1,1],wehavethat
(cid:18)
1 2
(cid:19)2
4 4
− arctan(1+ϵ) = (arctan(1)−arctan(1+ϵ))2 ≥ ϵ2.
2 π π2 25π2
becausearctan′(x)= 1 isalwaysgreaterthan 1 forx∈[0,2]. ThenwecanhaveEquation11.
1+x2 5
Whenv j(i) i. ∼i.d. Cauchy(0,1),itholdsthatH
j,i
=∇2L(θ) j,:·v(i) followsCauchy(0,(cid:80)d k=1(cid:12) (cid:12)∇2L(θ) j,k(cid:12) (cid:12))
becauseCauchydistributionis1-stable. ByEquation11,wecanhavethat
P (cid:32)(cid:12) (cid:12) (cid:12) (cid:12)median(|H j,1|,··· ,|H j,n|)−(cid:88)d (cid:12) (cid:12)∇2L(θ) j,k(cid:12) (cid:12)(cid:12) (cid:12) (cid:12) (cid:12)≥ϵ(cid:88)d (cid:12) (cid:12)∇2L(θ) j,k(cid:12) (cid:12)(cid:33) ≤2exp(cid:18) − 28 5n π2ϵ2(cid:19) .
(cid:12) (cid:12)
k=1 k=1
Andwecansumthetailprobabilityoveralltherowsj andgetthat
(cid:12) (cid:12) 
(cid:12) d (cid:12) (cid:18) (cid:19)
P (cid:12) (cid:12) (cid:12)(cid:88) median(|H j,:|)−(cid:13) (cid:13)∇2L(θ)(cid:13) (cid:13) 1,1(cid:12) (cid:12) (cid:12)≥ϵ(cid:13) (cid:13)∇2L(θ)(cid:13) (cid:13) 1,1≤2dexp − 28 5n π2ϵ2 .
(cid:12)j=1 (cid:12)
28