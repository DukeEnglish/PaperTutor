Generalizing Stochastic Smoothing for
Differentiation and Gradient Estimation
Felix Petersen1, Christian Borgelt2, Aashwin Mishra1, Stefano Ermon1
1Stanford University, 2University of Salzburg
Abstract
Wedealwiththeproblemofgradientestimationforstochasticdifferentiablerelaxationsof
algorithms,operators,simulators,andothernon-differentiablefunctions. Stochasticsmoothing
conventionally perturbs the input of a non-differentiable function with a differentiable density
distribution with full support, smoothing it and enabling gradient estimation. Our theory
starts at first principles to derive stochastic smoothing with reduced assumptions, without
requiring a differentiable density nor full support, and we present a general framework for
relaxationandgradientestimationofnon-differentiableblack-boxfunctionsf :Rn →Rm. We
developvariancereductionforgradientestimationfrom3orthogonalperspectives. Empirically,
we benchmark 6 distributions and up to 24 variance reduction strategies for differentiable
sorting and ranking, differentiable shortest-paths on graphs, differentiable rendering for pose
estimation, as well as differentiable cryo-ET simulations.
1 Introduction
Thedifferentiationofalgorithms,operators,andothernon-differentiablefunctionshasbeenatopic
of rapidly increasing interest in the machine learning community [1]–[7]. In particular, whenever
we want to integrate a non-differentiable operation (such as ranking) into a machine learning
pipeline, we need to relax it into a differentiable form in order to allow for backpropagation. To
give a concrete example, a body of recent work considered continuously relaxing the sorting and
ranking operators for tasks like learning-to-rank [5], [7]–[15]. These works can be categorized
into either casting sorting and ranking as a related problem (e.g., optimal transport [16]) and
differentiablyrelaxingit(e.g.,viaentropy-regularizedOT[7])orbyconsideringasortingalgorithm
and continuously relaxing it on the level of individual operations or program statements [5], [11],
[14], [15]. To give another example, in the space of differentiable graph algorithms and clustering,
popular directions either relax algorithms on a statement level [5] or cast the algorithm as a
convex optimization problem and differentiate the solution of the optimization problem under
perturbed parameterization [1], [3], [17].
Complementary to these directions of research, in this work, we consider algorithms, operators,
simulators, andothernon-differentiablefunctionsdirectlyasblack-boxfunctionsanddifferentiably
relax them via stochastic smoothing [18], i.e., via stochastic perturbations of the inputs and via
multiple function evaluation. This is challenging as, so far, gradient estimators have come with
large variance and supported only a restrictive set of smoothing distributions. More concretely,
for a black-box function f : Rn → Rm, we consider the problem of estimating the derivative (or
gradient) of the relaxation
(cid:90)
f (x) = E (cid:2) f(x+ϵ)(cid:3) = f(x+ϵ)µ(ϵ)dϵ (1)
ϵ ϵ∼µ
where ϵ is a sample from a probability distribution with an (absolutely) continuous density µ(ϵ).
f is a differentiable function (regardless of differentiability properties of f itself, see Section 2 for
ϵ
1
4202
tcO
01
]GL.sc[
1v52180.0142:viXradetails) and its gradient is well defined. Under limiting restrictions on the probability distribution
µ used for smoothing, gradient estimators exist in the literature [1], [18]–[20].
Thecontributionofthisworkliesinprovidingmoregeneralizedgradientestimators(reducing
assumptions on µ, Lemma 3) that exhibit reduced variances (Sec. 2.2) for the application
of differentiably relaxing conventionally non-differentiable algorithms. Moreover, we enable
smoothing with and differentiation wrt. non-diagonal covariances (Thm. 7), characterize formal
requirements for f (Lem. 9+10), discriminate smoothing of algorithms and losses (Sec. 2.3),
and provide a k-sample median extension (Apx. C). The proposed approach is applicable for
differentiation of (i) arbitrary1 functions, which are (ii) considered as a black-box, which (iii) can
be called many times (primarily) at low cost, and (iv) should be smoothed with any distribution
with absolutely continuous density on R. This contrasts prior work, which smoothed (i) convex
optimizers [1], [3], (ii) used first-order gradients [7], [11], [15], [21], (iii) allowed calling an
environment only once or few times in RL [20], and/or (iv) smoothed with fully supported
differentiable density distributions [1], [3], [18], [22]. In machine learning, many other subfields
also utilize the ideas underlying stochastic smoothing; stochastic smoothing and similar methods
can be found, e.g., in REINFORCE [20], the score function estimator [23], the CatLog-Derivative
trick [24], perturbed optimizers [1], [3], among others.
2 Differentiation via Stochastic Smoothing
We begin by recapitulating the core of the stochastic smoothing method. The idea behind
smoothing is that given a (potentially non-differentiable) function f : Rn → Rm 1, we can relax
the function to a differentiable function by perturbing its argument with a probability distribution:
if ϵ ∈ Rn follows a distribution with a differentiable density µ(ϵ), then f (x) = E [f(x+ϵ)] is
ϵ ϵ
differentiable.
For the case of m = 1, i.e., for a scalar function f, we can compute the gradient of f by
ϵ
following and extending part of Lemma 1.5 in Abernethy et al. [18] as follows:
Lemma 1 (DifferentiableDensitySmoothing). Given a function f : Rn → R 1 and a differentiable
probability density function µ(ϵ) with full support on Rn, then f is differentiable and
ϵ
∇ f (x) = ∇ E (cid:2) f(x+ϵ)(cid:3) = E (cid:2) f(x+ϵ)·∇−logµ(ϵ)(cid:3) . (2)
x ϵ x ϵ∼µ ϵ∼µ ϵ
Proof. For didactic reasons, we include a full proof in the paper to support the reader’s under-
standing of the core of the method. Via a change of variables, replacing x+ϵ by u, we obtain
(dϵ/du = 1)
(cid:90) (cid:90)
f (x) = f(x+ϵ)µ(ϵ)dϵ = f(u)µ(u−x)du. (3)
ϵ
Now,
(cid:90) (cid:90)
∇ f (x) = ∇ f(u)µ(u−x)du = f(u)∇ µ(u−x)du. (4)
x ϵ x x
Using ∇ µ(u−x) = −∇µ(ϵ), (cid:90)
x ϵ ∇ f (x) = − f(x+ϵ)∇µ(ϵ)dϵ. (5)
x ϵ ϵ
Because ∂µ(ϵ) = µ(ϵ)· ∂logµ(ϵ) , we can simplify the expression to
∂ϵ ∂ϵ
(cid:90) (cid:104) (cid:105)
∇ f (x) = − f(x+ϵ)µ(ϵ)∇ logµ(ϵ)dϵ = E f(x+ϵ)∇−logµ(ϵ) . (6)
x ϵ ϵ ϵ∼µ ϵ
1Traditionally and formally, only functions with compact range have been considered for f (i.e., f : Rn →
[a,b]) [19], [25]. More recently, i.a., Abernethy et al. [18] have considered the general case of function with the real
range. Whilethisisveryhelpful,e.g.,enablinglinearfunctionsforf,thisis(evenwithoutourgeneralizations)not
always finitely defined as we discuss with the help of degenerate examples in Appendix B. There, we characterize
the set of valid f leading to finitely defined f in Lemma 9 as well as ∇f in Lemma 10 in dependence on µ. We
ϵ ϵ
remark that, beyond discussions in Appendix B, we assume this to be satisfied by f.
2Empirically, for a number of samples s, this gradient estimator can be evaluated without bias via
s
1 (cid:88) (cid:104) (cid:105)
∇ f (x) ≜ f(x+ϵ )∇ −logµ(ϵ ) ϵ ,...,ϵ ∼ µ. (7)
x ϵ
s
i ϵi i 1 s
i=1
Corollary 2 (Differentiable Density Smoothing for Vector-valued Functions). We can extend
Lemma 1 to vector-valued functions f : Rn → Rm, allowing to compute the Jacobian matrix
J ∈ Rm×n as
fϵ
J (x) = E
(cid:104)
f(x+ϵ)·(cid:0)
∇−logµ(ϵ)(cid:1)⊤(cid:105)
. (8)
fϵ ϵ∼µ ϵ
We remark that prior work (e.g., [18]) limits µ to be a differentiable density with full support
onR, typicallyofexponentialfamily, whereaswegeneralizeittoanyabsolutelycontinuousdensity,
and include additional generalizations. This has important implications for distributions such as
Cauchy, Laplace, and Triangular, which we show to have considerable practical relevance.
Lemma 3 (Requirement of Continuity of µ). If µ(ϵ) is absolutely continuous (and not necessarily
differentiable), then f is continuous and differentiable everywhere.
ϵ
(cid:104) (cid:105)
∇ f (x) = E f(x+ϵ) ·1 · ∇−logµ(ϵ) . (9)
x ϵ ϵ∼µ ϵ∈/Ω ϵ
Ω is the zero-measure set of points with undefined gradient. We provide the proof in Appendix A.1.
Lemma3hasimportantimplications. Inparticular,itenablessmoothingwithnon-differentiable
density distributions such as the Laplace distribution, the triangular distribution, and the Wigner
Semicircle distribution [26], [27] while maintaining differentiability of f .
ϵ
Remark 4 (RequirementofContinuityofµ). However, itiscrucialtomentionthat, forstochastic
smoothing (Lemmas 1, 3, Corollary 2), µ has to be continuous. For example, the uniform
distribution is not a valid choice because it does not have a continuous density on R. (U(a,b) has
discontinuities at a,b where it jumps between 0 and 1/(b−a).) With other formulations, e.g.,
[28], [29], it is possible to perform smoothing with a uniform distribution over a ball; however, if
f is discontinuous, uniform smoothing may not lead to a differentiable function. Continuity is a
requirement but not a sufficient condition, and absolutely continuous is a sufficient condition;
however, the difference to continuity corresponds only to non-practical and adversarial examples,
e.g., the Cantor or Weierstrass functions.
Remark 5 (Gaussian Smoothing). A popular special case of differentiable stochastic smoothing
is smoothing with a Gaussian distribution µ = N(0 ,I ). Here, due to the nature of the
N n n
probability density function of a Gaussian, ∇−logµ (ϵ) = ϵ. Further, when µ = N(0 ,σ2I ),
ϵ N Nσ n n
then ∇−logµ (ϵ) = ϵ/σ. We emphasize that this equality only holds for the Gaussian
ϵ Nσ
distribution.
Equipped with the core idea behind stochastic smoothing, we can differentiate any function f
via perturbation with a probability distribution with (absolutely) continuous density.
Typically, probability distributions that we consider for smoothing are parameterized via a
scale parameter, vi7., the standard deviation σ in a Gaussian distribution or the scale γ in a
Cauchy distribution. Extending the formalism above, we may be interested in differentiating with
respect to the scale parameter γ of our distribution µ. This becomes especially attractive when
optimizing the scale and, thereby, degree of relaxation of our probability distribution. While our
formalism allows reparameterization to express γ within µ, we can also explicitly write it as
(cid:105)
∇ f (x) = ∇ E (cid:2) f(x+γ ·ϵ)(cid:3) = E (cid:2) f(x+γ ·ϵ)·(cid:0) ∇−logµ(ϵ)(cid:1) /γ . (10)
x γϵ x ϵ∼µ ϵ∼µ ϵ
Now, we can differentiate wrt. γ, i.e., we can compute ∇ f (x).
γ γϵ
3Lemma 6 (Differentiation wrt. γ). Extending Lemma 1, Corollary 2, and Lemma 3, we have
(cid:104) (cid:105)
∇ f (x) = ∇ E f(x+γ ·ϵ)(cid:3) = E (cid:2) f(x+γ ·ϵ)·(cid:0) −1+(∇−logµ(ϵ))⊤·ϵ(cid:1) /γ . (11)
γ γϵ γ ϵ∼µ ϵ∼µ ϵ
The proof is deferred to Appendix A.2.
We can extend γ for multivariate distributions to a scale matrix Σ/L (e.g., a covariance matrix).
Theorem 7 (Multivariate Smoothing with Covariance Matrix). We have a function f : Rn →
Rm. We assume ϵ is drawn from a multivariate distribution with absolutely continuous density in
Rn. We have an invertible scale matrix L ∈ Rn×n (e.g., for a covariance matrix Σ, L is based
on its Cholesky decomposition LL⊤ = Σ). We define f (x) = E (cid:2) f(x+L·ϵ)(cid:3) . Then, our
Lϵ ϵ∼µ
derivatives ∂f (x)/∂x (∈ Rm×n) and ∂f (x)/∂L (∈ Rm×n×n) can be computed as
Lϵ Lϵ
(cid:104) (cid:105)
∇ (cid:0) f (x)(cid:1) = E f(x+L·ϵ) ·L−1·(cid:0) ∇−logµ(ϵ)(cid:1) , (12)
x Lϵ i ϵ∼µ i ϵ
(cid:104) (cid:105)
∇ (cid:0) f (x)(cid:1) = E f(x+L·ϵ) ·L−⊤·(cid:0) −1+(∇−logµ(ϵ))·ϵ⊤(cid:1) . (13)
L Lϵ i ϵ∼µ i ϵ
Above, the indicator (from (9)) is omitted for a simplified exposition. Proofs are deferred to
Apx. A.3.
Before we continue with examples for distributions, we discuss two extensions, vi7. differentiating
output covariances of smoothing, and differentiating the expected k-sample median.
For uncertainty quantification (UQ) applications, e.g., in the context of propagating distri-
butions [30], we may further be interested in computing the derivative of the output covariance
matrix, as illustrated by the following theorem.
Theorem 8 (Output Covariance of Multivariate Smoothing for UQ). Given the assumptions
of Theorem 7, we may also be interested in computing the output covariance G :
Lϵ
G (x) = Cov (cid:2) f(x+L·ϵ)(cid:3) . (14)
Lϵ ϵ∼µ
We can compute the derivative of the output covariance wrt. the input ∂G (x)/∂x(∈ Rm×m×n) as
Lϵ
(cid:104) (cid:105)
∇ G (x) = E f(x+Lϵ) ·f(x+Lϵ) ·L−1·∇ −logµ(ϵ)
x Lϵ i,j ϵ∼µ i j ϵ (15)
−f (x) ·∇ f (x) −f (x) ·∇ f (x)
Lϵ i x Lϵ j Lϵ j x Lϵ i
Further, we can compute the derivative wrt. the input scale matrix ∂G (x)/∂L(∈ Rm×m×n×n) as
Lϵ
(cid:104) (cid:105)
∇ G (x) = −E f(x+Lϵ) ·f(x+Lϵ) ·(cid:0) L−⊤·∇ µ(ϵ)·ϵ⊤/µ(ϵ)+L−⊤(cid:1)
L Lϵ i,j ϵ∼µ i j ϵ (16)
−f (x) ·∇ f (x) −f (x) ·∇ f (x)
Lϵ i L Lϵ j Lϵ j L Lϵ i
The proofs are deferred to Appendix A.4.
In Appendix C, we additionally extend stochastic smoothing to differentiating the expected
k-sample median, show that it is differentiable, and provide an unbiased gradient estimator in
Lemma 13.
2.1 Distribution Examples
After covering the underlying theory of generalized stochastic smoothing, in this section, we
provideexamplesofspecificdistributionsthatourtheoryappliesto. Weillustratethedistributions
in Table 1.
4Table 1: Probability distributions considered for generalized stochastic smoothing. Displayed is (from left
to right) the density of the distribution µ(ϵ) (plot + equation), the derivative of the NLL (equation), and
the product between the density and the derivative of the NLL (plot). The latter plot corresponds to
the kernel that f is effectively convolved by to estimate the gradient. (∗): applies to ϵ∈(−1,1)\{0},
otherwise 0 or undefined.
Distribution Density / PDF µ(ϵ) ∇−logµ(ϵ) µ(ϵ)·∇−logµ(ϵ)
ϵ ϵ
Gaussian √1 exp(cid:0) −1/2·ϵ2(cid:1) ϵ
2π
exp(−ϵ)
Logistic tanh(ϵ/2)
(1+exp(−ϵ))2
Gumbel exp(−ϵ−exp(−ϵ)) 1−exp(−ϵ)
1 2·ϵ
Cauchy
π·(1+ϵ2) 1+ϵ2
Laplace 1/2·exp(−|ϵ|) sign(ϵ)
sign(ϵ)
Triangular max(0,1−|ϵ|) (∗)
1−|ϵ|
Before delving into individual choices for distributions, we provide a clarification for multivari-
ate densities µ : Rn → R : We consider the n-dimensional multivariate form of a distribution
≥0
as the concatenation of n independent univariate distributions. Thus, for µ as the univariate
1
formulation of the density, we have the proportionality µ(ϵ) ≃ (cid:81)n µ (ϵ ). We remark that the
i=1 1 i
distribution by which we smooth (Lϵ) is not an isotropic (per-dimension independent) distribu-
tion. Instead, through transformation by the scale matrix L, e.g., in the case of the Gaussian
distribution, Lϵ covers the entire space of multivariate Gaussian distributions with arbitrary
covariance matrices.
Beyond the Gaussian distribution, the logistic distribution offers heavier tails, and the
Gumbel distribution provides max-stability, which can be important for specific tasks. The
Cauchy distribution [31], with its undefined mean and infinite variance, also has important
implications in smoothing: e.g., the Cauchy distribution is shown to provide monotonicity
in differentiable sorting networks [12]. While prior art [22] heuristically utilized the Cauchy
distribution for stochastic smoothing of argmax, this had been, thus far, without a general formal
justification.
In this work, for the first time, we consider Laplace and triangular distributions. First, the
Laplace distribution, as the symmetric extension of the exponential distribution, does not lie in
the space of exponential family distributions, and is not differentiable at 0. Via Lemma 3, we show
that stochastic smoothing can still be applied and is exactly correct despite non-differentiablity
of the distribution. A benefit of the Laplace distribution is that all samples contribute equally to
the gradient computation (|∇ −logµ(ϵ)| = 1), reducing variance. Second, with the triangular
ϵ
distribution, we illustrate, for the first time, that stochastic smoothing can be performed even
with a non-differentiable distribution with compact support ([−1,1]). This is crucial if the domain
of f has to be limited to a compact set rather than the real domain, or in applications where
smoothing beyond a limited distance to the original point is not meaningful. A hypothetical
application for this could be differentiating a physical motor controlled robot in reinforcement
learning where we may not want to support an infinite range for safety considerations.
2.2 Variance Reduction
Given an unbiased estimator of the gradient, e.g., in its simplest form (2), we desire reducing
its variance, or, in other words, improve the quality of the gradient estimate for a given number
5of samples. For this, we consider 3 orthogonal perspectives of variance reduction: covariates,
antithetic samples, and (randomized) quasi-Monte Carlo.
To illustratively derive the first
5 5 5
two variance reductions, let us con- none f(x) LOO
4 4 4
siderthecaseofsmoothingaconstant
3 3 3
function f(x) = v for some large con- 2 2 2
stant v ≫ 0. Naturally, f (x) = v 1 1 1
ϵ
0 0 0
and ∇ f (x) = 0. However, for a
x ϵ −1 −1 −1
finite number of samples s, our em-
−2 −2 −2
pirical estimate (e.g., (7)) will differ −2−1 0 1 2 3 4 −2−1 0 1 2 3 4 −2−1 0 1 2 3 4
from0almostsurely. Asthegradient Figure1: Comparisonofcovariates: anon-differentiablefunction
off (x)−cwrt.xdoesnotdependon (dark blue) is smoothed with a logistic distribution (light blue).
ϵ
c, we have ∇ f (x) = ∇ (f (x)−c). The original gradient (dark red) is not everywhere defined, and
x ϵ x ϵ
If we choose c = v, the variance of does not meaningfully represent the gradient. The gradient
of the smoothed function is shown in pink. Grey illustrates
the gradient estimator is reduced to
the variance of a gradient estimate with 5 samples via the
0. For general and non-constant f,
[25%,75%] (dark grey) and [10%,90%] (light grey) percentiles.
we can estimate the optimal choice
Using f(x) as a covariate, instead of using none reduces the
of c via c = f(x) or via the leave-one- gradient variance, in particular whenever f(x) is large. Leave-
out estimator [32], [33] of f (x). In one-out (LOO) further improves over f(x) at discontinuities of
ϵ
the fields of stochastic smoothing of the original function f (i.e., at x=1), but has slightly higher
optimizers and reinforcement learn- variance than f(x) where f is continuous and has large values
(i.e., at x=−2.)
ing this is known as the method of
covariates. f(x) and LOO were previously considered for smoothing, e.g., in [22] and [34], respec-
tively. We illustrate the effects of both choices of covariates in Figure 1.
From an orthogonal perspective, we observe that E (cid:2) ∇−logµ(ϵ))(cid:3) =0, which follows, e.g.,
ϵ∼µ ϵ
from ∇ f (x) = 0=E (cid:2) v·∇−logµ(ϵ)(cid:3). For symmetric distributions, we can guarantee an
x ϵ ϵ∼µ ϵ
empirical estimate to be 0 by always using pairs of antithetic samples [35], i.e., complementary ϵs.
Using ϵ′ = −ϵ, we have ∇ logµ(ϵ)+∇ logµ(ϵ′) = 0. This is illustrated in Figure 2 (2). In our
ϵ ϵ′
experiments in the next section, we observe antithetic sampling to generally perform poorly in
comparison to other variance reduction techniques.
Figure 2: Sampling strategies. Left to right: Monte-Carlo (MC), Antithetic Monte-Carlo, Cartesian Quasi-
Monte-Carlo (QMC), Cartesian Randomized-Quasi-Monte-Carlo (RQMC), Latin-Hypercube Sampled
QMC and RQMC. Samples can be transformed via the inverse CDF of a respective distribution.
A third perspective considers that points sampled with standard Monte Carlo (MC) methods
(see Fig. 2 (1)), due to the random nature of the sampling, often form (accidental) clumps while
other areas are void of samples. To counteract this, quasi-Monte Carlo (QMC) methods [36]
spread out sampled points as evenly as possible by foregoing randomness and choosing points
from a regular grid, e.g., a simple Cartesian grid, taking the grid cell centers as samples (see
Fig. 2 (3)). Via the inverse CDF of the respective distribution, the points can be mapped from
the unit hypercube to samples from a respective distribution. However, discarding randomness
makes the sampling process deterministic, limits the dispersion introduced by the smoothing
distribution to concrete points, and hence makes the estimator biased. Randomized quasi-Monte
Carlo (RQMC) [37] methods overcome this difficulty by reintroducing some randomness. Like
6QMC, RQMC uses a grid to subdivide [0,1]n into cells, but then samples a point from each
cell (see Fig. 2 (4)) instead of taking the grid cell center. While regular MC sampling leads
√
to variances of O(1/ s), RQMC reduces them to O(1/s1+2/n) for a number s of samples and
an input dimension of n [38]. For large n, we still have a rate of at least O(1/s) ⊃ O(1/s1+2/n),
√
which constitutes a substantial improvement over the regular reduction in O(1/ s). However,
the default (i.e., Cartesian) QMC and RQMC methods require numbers of samples s = kn for
k ∈ N , which can become infeasible for large input dimensionalities. Therefore, we also consider
+
Latin-Hypercube Sampling (LHS) [39], which uses a subset of grid cells such that each interval
in every dimension is covered exactly once (see Fig. 2 (5+6)). Finally, we remark that, to our
knowledge, QMC and RQMC sampling strategies have not been considered in the field of gradient
estimation.
2.3 Smoothing of the Algorithm vs. the Objective
In many learning problems, we can write our training objective as ℓ(h(y)) where y is the output
of a neural network model, h is the algorithm, and the scalar function ℓ is the training objective
(loss function) applied to the output of the algorithms. In such cases, we can distinguish between
smoothing the algorithms (f = h) and smoothing the loss (f = ℓ◦h).
When smoothing the algorithm, we compute the value and derivative of ℓ(cid:0) E [h(y+ϵ)](cid:1). This
ϵ
requires our loss function ℓ to be differentiable and capable of receiving relaxed inputs. (For
example, if the output of h is binary, then ℓ has to be able to operate on real-valued inputs from
(0,1).) In this case, the derivative of E [h(y+ϵ)] is a Jacobian matrix (see Corollary 2).
ϵ
When smoothing the objective / loss function, we compute the value and derivative of
E [ℓ(h(y+ϵ))]. Here, the objective / loss ℓ does not need to be differentiable and can be limited
ϵ
to operate on discrete outputs of the algorithm h. Here, the derivative of E [ℓ(h(y +ϵ))] is a
ϵ
gradient.
The optimal choice between smoothing the algorithm and smoothing the objective depends
on different factors including the problem setting and algorithm, the availability of a real-variate
and real-valued ℓ, and the number of samples that can be afforded. In practice, we observe that,
whenever we can afford large numbers of samples, smoothing of the algorithm performs better.
3 Related Work
Inthetheoreticalliteratureofgradient-freeoptimization,stochasticsmoothinghasbeenextensively
studied [18], [19], [40], [41]. Our work extends existing results, generalizing the set of allowed
distributions, considering vector-valued functions, anisotropic scale matrices, enabling k-sample
median differentiation, and a characterization of finite definedness of expectations and their
gradients based on the relationship between characteristics of the density and smoothed functions.
From a more applied perspective, stochastic smoothing has been applied for relaxing convex
optimization problems [1], [3], [22]. In particular, convex optimization formulations of argmax [1],
[22], the shortest-path problem [1], and the clustering problem [3] have been considered. We
remark that the perspective of smoothing any function or algorithm f, as in this work, differs
from the perspective of perturbed optimizers. In particular, optimizers are a special case of the
functions we consider.
While we consider smoothing functions with real-valued inputs, there is also a rich literature
of differentiating stochastic discrete programs [42]–[44]. These works typically use the inherent
stochasticity from discrete random variables in programs and explicitly model the internals of the
programs. We consider real-variate black-box functions and smooth them with added input noise.
7In the literature of reinforcement learning, a special case or analogous idea to stochastic
smoothing can be found in the REINFORCE formulation where the (scalar) score function is
smoothed via a policy [18], [20], [45], [46]. Compared to the literature, we enable new distributions
and respective characterizations of requirements for the score functions. We hope our results will
pave their way into future RL research directions as they are also applicable to RL without major
modification.
4 Experiments
For the experiments, we consider 4 experimental domains: sorting & ranking, graph algorithms,
3D mesh rendering, and cryo-electron tomography (cryoET) simulations. The primary objective
of the empirical evaluations is to compare different distributions as well as different variance
reduction techniques. We begin our evaluations by measuring the variance of the gradient
estimators, and then continue with optimizations and using the differentiable relaxations in deep
learning tasks. We remark that, in each of the 4 experiments, f does not have any non-zero
gradients, and thus using first-order or path-wise gradients or gradient estimators is not possible.
4.1 Variance of Gradient Estimators
We evaluate the gradient variances for different variance reduction techniques in Figures 3 and 4.
For differentiable sorting and ranking, we smooth the (hard) permutation matrix that sorts
an input vector (f : Rn → {0,1}n×n). For diff. shortest-paths, we smooth the function that
Gaussian Logistic Gumbel Cauchy Laplace Triangular
Figure 3: Average L norms between ground truth (oracle) and estimated
2
gradientfordifferentnumbersofelementstosortandrankn, anddifferent
distributions. Each plot compares different variance reduction strategies
as indicated in the legend to the right of the caption. Darker is better MC
QMC(latin)
(smaller values). Colors are only comparable within each subplot. We use
RQMC(latin)
1024 samples, except for Cartesian and n=3 where we use 103 =1000
RQMC(cart.)
samples. An extension with n∈{7,10} can be found in Figure 11 in the
regular antithetic
appendix. Absolute values are reported in Table 4.
Gaussian Logistic Gumbel Cauchy Laplace Triangular
Figure 4: Average L norms between ground truth (oracle) and estimated
2
gradientforsmoothingshortest-pathalgorithms,anddifferentdistributions.
Each plot compares different variance reduction strategies as indicated in
MC
the legend to the right of the caption. Darker is better (smaller values).
QMC(latin)
Colors are only comparable within each subplot. We use 1024 samples. RQMC(latin)
Absolute values are reported in Table 5.
regular antithetic
8
3=n
5=n
8×8
21×21
enon
enon
)x(f
)x(f
OOL
OOL
enon
enon
)x(f
)x(f
OOL
OOLmaps from a 2D cost-map to a binary encoding of the shortest-path under 8-neighborhood
(f : Rn×n → {0,1}n×n). Both functions are not only non-differentiable, but also have no non-zero
gradients anywhere. For each distribution, we compare all combinations of the 3 complementary
variance reduction techniques.
On the axis of sampling strategy, we can observe that, whenever available, Cartesian RQMC
delivers the lowest variance. The only exception is the triangular distribution, where latin QMC
provides the lowest uncentered gradient variance (despite being a biased estimator) because of
large contributions to the gradient for samples close to −1 and 1. Between latin QMC and
RQMC, we can observe that their variance is equal except for the high-dimension cases of the
Cauchy distribution and a few cases of the Gumbel distribution, where QMC is of lower variance.
However, due to the bias in QMC, RQMC would typically still be preferable over QMC. We
do not consider Cartesian QMC due to its substantially greater bias. In heuristic conclusion,
RQMC(c.) ≻ RQMC(l.) ⪰ QMC(l.) ≻ MC.
On the axis of using antithetic sampling (left vs. right in each subplot), we observe that
it consistently performs worse than the regular counterpart, except for vanilla MC without a
covariate. Thereasonforthisisthatantitheticsamplingdoesnotleadtoagoodsample-utilization
trade-off once we consider quasi Monte-Carlo strategies. For vanilla Monte-Carlo, antithetic
sampling improves the results as long as we do not use the LOO covariate. Thus, in the following,
we consider antithetic only for MC.
On the axis of the covariate, we observe that LOO consistently provides the lowest gradient
variances. This aligns with intuition from Figure 1 where LOO provides the lowest variance at
discontinuities (in this subsection, f is discontinuous or constant everywhere). Comparing no
covariate and f(x), the better choice has a strong dependence on the individual setting, which
makes sense considering the binary outputs of the algorithms. f(x) would perform well for
functions that attain large values while having fewer discontinuities.
In conclusion, the best setting is Cartesian
RQMC with the LOO covariate and without
antithetic sampling whenever available (only for
s = kn samples for k ∈ N). The next best 0.84
choice is typically RQMC with Latin hypercube
sampling.
0.82
4.2 Differentiable Sorting & Ranking
After investigating the choices of variance re-
duction techniques wrt. the variance alone, in
this section, we explore the utility of stochastic 0.80
smoothing on the 4-digit MNIST sorting bench-
mark [8]. Here, at each step, a set of n=5 4-digit
MNIST images (such as ) is presented
to a CNN, which predicts the displayed scalar 0.78
value for each of the n images independently.
For training the model, no absolute information
about the displayed value is provided, and only
0.76
theorderingorrankingofthenimagesaccording
to their ground truth value is supervised. The
goal is to learn an order-preserving CNN, and Gaussian Logistic Gumbel Cauchy Laplace Triangular
Figure 5: Sortingbenchmark(n=5).
the evaluation metric is the fraction of correctly
Exact match (EM) accuracy. Brighter
inferred orders from the CNN (exact match accu-
is better (greater values). Values MC
racy). TrainingtheCNNrequiresadifferentiable between subplots are compara- MC(at.)
QMC(lat.)
ranking operator (that maps from a vector to a ble. IQM over 12 seeds and dis-
RQMC(lat.)
differentiable permutation matrix) for the rank- played range of [75%,85.5%]. RQMC(car.)
9
652=s#
4201=s#
8402=s#
2918=s#
86723=s#
enon )x(f OOLing loss. Previous work has considered NeuralSort [8], SoftSort [9], casting sorting as a regularized
OT problem [7], and differentiable sorting networks (DSNs) [11], [12]. The state-of-the-art
is monotonic DSN [12], which utilizes a relaxation based on Cauchy distributions to provide
monotonic differentiable sorting, which has strong theoretical and empirical advantages.
In Figure 5, we evaluate the perfor-
Table 2: Sorting benchmark results (n = 5), avg. over
mance of generalized stochastic smoothing
12 seeds. ‘best (cv)’ refers to the best sampling strategy,
with different distributions and different as determined via cross-validation (thus, there is no bias
numbers of samples for each variance re- from the selection of the strategy). Table 3 includes
duction technique. We observe that, while additional num. of samples and stds. Baselines are Neu-
theCauchydistributionperformspoorlyfor ralSort [8], SoftSort [9], Logistic DSNs [11], Cauchy and
Error-optimal DSNs [12], and OT Sort [7], avg. over at
smallnumbersofsamples,forlargenumbers
least 5 seeds each.
of samples, the Cauchy distribution per-
formsbest. ThismakessenseastheCauchy Baselines Neu.S. Soft.S. L.DSN C.DSN E.DSN OT.S.
distribution has infinite variance and, for — 71.3 70.7 77.2 84.9 85.0 81.1
DSNs, provides monotonicity. We remark
Sampling #s Gauss. Logis. Gumbel Cauchy Laplace Trian.
that large numbers of samples can easily be
vanilla 256 82.3 82.8 79.2 68.1 82.6 81.3
afforded in many applications (when com-
best(cv) 256 83.1 82.7 81.6 55.6 83.7 82.7
paring the high cost of neural networks to
vanilla 1k 81.3 83.7 82.0 68.5 80.6 82.8
thevanishingcostofsorting/rankingwithin
best(cv) 1k 83.9 84.0 84.2 73.0 84.3 82.4
a loss function). (Nevertheless, for 32768
vanilla 32k 84.2 84.1 84.5 84.9 84.4 83.4
samples, the sorting operation starts to be-
best(cv) 32k 84.4 84.4 84.8 85.1 84.4 84.0
come the bottleneck.) The Laplace distri-
bution is the best choice for smaller numbers of samples, which aligns with the characterization of
it having the lowest variance because all samples contribute equally to the gradient. Wrt. variance
reduction, we continue to observe that vanilla MC performs worst. RQMC performs best, except
for Triangular, where QMC is best. For the Gumbel distribution, we observe reduced performance
for latin sampling. Generally, we observe that f(x) is the worst choice of covariate, but the
effect lies within standard deviations. In Table 2, we provide a numerical comparison to other
differentiable sorting approaches. We can observe that all choices of distributions improve over
all baselines except for the monotonic DSNs, even at smaller numbers of samples (i.e., without
measurable impact on training speed). Finally, the Cauchy distribution leads to a minor improve-
ment over the SOTA, without requiring a manually designed differentiable sorting algorithm;
however, only at the computational cost of 32768 samples.
4.3 Differentiable Shortest-Paths
The Warcraft shortest-path bench-
mark [17] is the established bench- 0.95
mark for differentiable shortest-path
0.90
algorithms (e.g., [1], [5], [17]). Here,
a Warcraft pixel map is provided, a 0.85
CNN predicts a 12×12 cost matrix, 0.80
a differentiable algorithm computes
the shortest-path, and the supervi- 0.75
sionisonlythegroundtruthshortest-
0.70
Gaussian Logistic Gumbel Cauchy Laplace Triangular
path. Berthet et al. [1] considered
Figure 6: Warcraft shortest-path experiment with
stochastic smoothing with Fenchel-
1000 samples. Brighter is better (larger values).
Young (FY) losses, which improves Values between subplots are
sample efficiency for small numbers comparable. Exact match accuracy MC
of samples. However, the FY loss avg. over 5 seeds and displayed MC(at.)
QMC(lat.)
range [70%,96%]. Additional
does not improve for larger numbers RQMC(lat.)
settings in Figures 13 and 14.
of samples (e.g., Tab. 7.5 in [4]).
10
ogla
ssol
enon )x(f OOLAs computing the shortest-path is computation-
0.95
ally efficient and parallelizable (our implementation 1
≈ 5000× faster than the Dijkstra implementation
3 0.90
used in previous work [1], [17]), we can afford sub-
stantially larger numbers of samples, improving the 10
0.85
quality of gradient estimation. In Figure 6, we
30
compare the performance of different smoothing
0.80
100 strategies. The logistic distribution performs best,
and smoothing of the algorithm (top) performs bet- 300
0.75
ter than smoothing of the loss (bottom). Variance
1000
reduction via sampling strategies (antithetic, QMC, 0.70
3 10 30 100 300 1000 300010000
or RQMC) improves performance, and the best co- # samples
variate is LOO. For reference, the FY loss [1] leads Figure 7: Warcraft shortest-path experiment
to an accuracy of 80.6%, regardless of the number using Gaussian smoothing of the algorithm
(RQMC with latin hypercube-sampling and
of samples. GSS consistently achieves 90%+ using
LOO covariate). Comparing the effects between
100 samples (see Fig. 13 right). Using 10000 sam-
the inverse temperature β and the number of
ples, and variance reduction, we achieve 96.6% in samples. Weobservethatwithgrowingnumbers
the best setting (Fig. 14) compared to the SOTA
of samples, the optimal inverse temperature in-
of 95.8% [5]. In Fig. 7, we illustrate that smaller creases, i.e., the optimal standard deviation for
standard deviations (larger β) are better for more the Gaussian noise decreases. Averaged over 5
samples. seeds.
4.4 Differentiable Rendering
For differentiable rendering [22], [47]–[52], 0.9
we smooth a non-differentiable hard ren-
0.8
derer via sampling. This differs from
DRPO [22], which uses stochastic smooth- 0.7
ing to relax the Heaviside and Argmax
0.6
functions within an already differentiable
renderer. Instead, we consider the ren- 0.5
derer as a black-box function. This has
0.4
the advantage of noise parameterized in
the coordinate space rather than the im- 0.3
age space.
0.2
We benchmark stochastic smoothing
for rendering by optimizing the camera- 0.1
pose (4-DoF) for a Utah teapot, an ex-
0.0
periment inspired by [22], [52]. We illus- Gaussian Logistic Gumbel Cauchy Laplace Triangular
Figure 8: Utah teapot camera pose optimiza-
trate the results in Figure 8. Here, the tion. The metric is fraction of camera
logistic distribution performs best, and
poses recovered; the initialization an-
QMC/RQMC as well as LOO lead to gle errors are uniformly distributed MC
MC(at.)
the largest improvements. While Fig. 8 in [15◦,75◦]. Brighter is better.
QMC(lat.)
shows smoothing the rendering algorithm, Avg. over 768 seeds. The dis- RQMC(lat.)
Fig.12performssmoothingofthetraining played range is [0%,90%]. RQMC(car.)
objective / loss. Smoothing the algorithm is better because the loss (MSE), while well-defined on
discrete renderings, is less meaningful on discrete renderings.
4.5 Differentiable Cryo-Electron Tomography
Transmission Electron Microscopy (TEM) transmits electron beams through thin specimens to
form images [53]. Due to the small electron beam wavelength, TEM leads to higher resolutions of
11
61=s#
46=s#
652=s#
/1=
erutarepmet
esrevni
enon )x(f OOLup to single columns of atoms. Obtaining high
(a) (b)
resolution images from TEM involves adjust-
ments of various experimental parameters. We
apply smoothing to a realistic black-box TEM
simulator [54], optimizing sets of parameters
to approximate referenceTobaccoMosaicVirus
(TMV) [55] micrographs. In Figure 10, we per-
form two experiments: a 2-parameter study
optimizing the microscope acceleration volt- Figure 9: (a) Simulated Transmission Electron micro-
age and x-position of the specimen, and a 4- graph, (b) TMV structure with RNA (orange) and
parameter study with additional parameters protein stacks (blue).
of the particle’s y-position and the primary lens focal length. The micrograph image sizes are
400×400 pixels, and accordingly we use smoothing of the loss.
10
MC [none]
6
MC [LOO]
8
QMC (latin) [LOO]
RQMC (cart.) [LOO]
6 RQMC (latin) [none] 4
RQMC (latin) [LOO]
4 RQMC (latin) [f(x)]
MC Search 2
2
0 0
0 100 200 300 400 500 600 700 0 200 400 600 800 1000
num. samples num. samples
Figure 10: RMSE to Ground Truth parameters for the 2-parameter (left) and 4-parameter experiment
(right). We optimize the L loss between generated and GT images using loss smoothing. No marker lines
2
correspond to Gaussian, × to Laplace and △ to Triangular distributions. Laplace and Triangular perform
best; LOO leads to the largest improvements. Add. results are in Figure 15.
Summary of Experimental Results Generally, we observe that QMC and RQMC perform
best, whereas antithetic sampling performs rather poorly. In low-dimensional problems, it is
advisable to use RQMC (cartesian), and in higher dimensional problems (R)QMC (latin), still
works well. As for the covariate, LOO typically performs best; however, the choice of sampling
strategy (QMC/RQMC) is more important than choosing the covariate. In sorting and ranking,
the Cauchy distribution performs best for large numbers of samples and for smaller numbers
of samples, the Laplace distribution performs best. In the shortest-path case, the logistic
distribution performs best, and Gaussian closely follows. Here, we also observe that with larger
numbers of samples, the optimal standard deviation decreases. For differentiable rendering, the
logistic distribution performs best.
Limitations A limitation of our work is that zeroth-order gradient estimators are generally only
competitive if the first-order gradients do not exist (see [56] for discussions on exceptions). In this
vein, in order to be competitive with custom designed continuous relaxations like a differentiable
renderer, we may need a very large number of samples, which could become prohibitive for
expensive functions f. The optimal choice of distribution depends on the function to be smoothed,
which means there is no singular distribution that is optimal for all f; however, if one wants to
limit the distribution to a single choice, we recommend the logistic or Laplace distribution, as,
with their simple exponential convergence, they give a good middle ground between heavy-tailed
and light-tailed distributions. Finally, the variance reduction techniques like QMC/RQMC are
not immediately applicable in single sample settings, and the variance reduction techniques in
this paper build on evaluating f many times.
12
TG
ot
ESMR
TG
ot
ESMR5 Conclusion
In this work, we derived stochastic smoothing with reduced assumptions and outline a general
framework for relaxation and gradient estimation of non-differentiable black-box functions. This
enables an increased set of distributions for stochastic smoothing, e.g., enabling smoothing with
the triangular distribution while maintaining full differentiablility of f . We investigated variance
ϵ
reduction for stochastic smoothing–based gradient estimation from 3 orthogonal perspectives,
finding that RQMC and LOO are generally the best methods, whereas the popular antithetic sam-
pling method performs rather poorly. Moreover, enabled by supporting vector-valued functions,
we disentangled the algorithm and objective, thus smoothing f while analytically backpropagating
through the loss ℓ, improving gradient estimation. We applied stochastic smoothing to differen-
tiable sorting and ranking, diff. shortest-paths on graphs, diff. rendering for pose estimation and
diff. cryo-ET simulations. We hope that our work inspires the community to develop their own
stochastic relaxations for differentiating non-differentiable algorithms, operators, and simulators.
Acknowledgments
We would like to acknowledge helpful discussions with Michael Kagan, Daniel Ratner, and
Terry Suh. This work was in part supported by the Land Salzburg within the WISS 2025
project IDA-Lab (20102-F1901166-KZP and 20204-WISS/225/197-2019), the U.S. DOE Contract
No. DE-AC02-76SF00515, Zoox Inc, ARO (W911NF-21-1-0125), ONR (N00014-23-1-2159), the
CZ Biohub, and SPRIN-D.
References
[1] Q. Berthet, M. Blondel, O. Teboul, M. Cuturi, J.-P. Vert, and F. Bach, “Learning with
Differentiable Perturbed Optimizers,” in Proc. Neural Information Processing Systems
(NeurIPS), 2020.
[2] F. Petersen, M. Cuturi, M. Niepert, H. Kuehne, M. Kagan, W. Neiswanger, and S. Ermon,
“Differentiable Almost Everything: Differentiable Relaxations, Algorithms, Operators, and
Simulators Workshop at ICML 2023,” 2023.
[3] L.Stewart,F.S.Bach,F.L.López,andQ.Berthet,“Differentiableclusteringwithperturbed
spanning forests,” Proc. Neural Information Processing Systems (NeurIPS), 2023.
[4] F. Petersen, “Learning with differentiable algorithms,” Ph.D. dissertation, Universität
Konstanz, 2022.
[5] F. Petersen, C. Borgelt, H. Kuehne, and O. Deussen, “Learning with algorithmic supervision
via continuous relaxations,” in Proc. Neural Information Processing Systems (NeurIPS),
2021.
[6] M. Cuturi and M. Blondel, “Soft-DTW: A Differentiable Loss Function for Time-Series,” in
Proc. International Conference on Machine Learning (ICML), 2017.
[7] M. Cuturi, O. Teboul, and J.-P. Vert, “Differentiable ranking and sorting using optimal
transport,” in Proc. Neural Information Processing Systems (NeurIPS), 2019.
[8] A. Grover, E. Wang, A. Zweig, and S. Ermon, “Stochastic Optimization of Sorting Networks
via Continuous Relaxations,” in Proc. International Conference on Learning Representations
(ICLR), 2019.
[9] S. Prillo and J. Eisenschlos, “Softsort: A continuous relaxation for the argsort operator,” in
Proc. International Conference on Machine Learning (ICML), 2020.
[10] M. Blondel, O. Teboul, Q. Berthet, and J. Djolonga, “Fast Differentiable Sorting and
Ranking,” in Proc. International Conference on Machine Learning (ICML), 2020.
13[11] F. Petersen, C. Borgelt, H. Kuehne, and O. Deussen, “Differentiable sorting networks for
scalable sorting and ranking supervision,” in Proc. International Conference on Machine
Learning (ICML), 2021.
[12] F. Petersen, C. Borgelt, H. Kuehne, and O. Deussen, “Monotonic differentiable sorting
networks,” in Proc. International Conference on Learning Representations (ICLR), 2022.
[13] M. E. Sander, J. Puigcerver, J. Djolonga, G. Peyré, and M. Blondel, “Fast, differentiable
and sparse top-k: A convex analysis perspective,” in Proc. International Conference on
Machine Learning (ICML), 2023.
[14] A. Vauvelle, B. Wild, R. Eils, and S. Denaxas, “Differentiable sorting for censored time-to-
event data,” in ICML 2023 Workshop on Differentiable Almost Everything: Differentiable
Relaxations, Algorithms, Operators, and Simulators, 2023.
[15] N. Shvetsova, F. Petersen, A. Kukleva, B. Schiele, and H. Kuehne, “Learning by sorting:
Self-supervised learning with group ordering constraints,” in Proc. International Conference
on Computer Vision (ICCV), 2023.
[16] M.Cuturi,“Sinkhorndistances:Lightspeedcomputationofoptimaltransport,” inProc. Neu-
ral Information Processing Systems (NeurIPS), 2013.
[17] M. Vlastelica, A. Paulus, V. Musil, G. Martius, and M. Rolinek, “Differentiation of blackbox
combinatorial solvers,” in Proc. International Conference on Learning Representations
(ICLR), 2020.
[18] J. Abernethy, C. Lee, and A. Tewari, “Perturbation techniques in online learning and
optimization,” Perturbations, Optimization, and Statistics, 2016.
[19] P. Glasserman, Gradient estimation via perturbation analysis. Springer Science & Business
Media, 1990, vol. 116.
[20] R. J. Williams, “Simple statistical gradient-following algorithms for connectionist reinforce-
ment learning,” Machine learning, vol. 8, pp. 229–256, 1992.
[21] S. Liu, T. Li, W. Chen, and H. Li, “Soft Rasterizer: A Differentiable Renderer for Image-
based 3D Reasoning,” in Proc. International Conference on Computer Vision (ICCV),
2019.
[22] Q.L.Lidec,I.Laptev,C.Schmid,andJ.Carpentier,“Differentiablerenderingwithperturbed
optimizers,” in Proc. Neural Information Processing Systems (NeurIPS), 2021.
[23] M. C. Fu, “Gradient estimation,” Handbooks in operations research and management science,
vol. 13, pp. 575–616, 2006.
[24] L.DeSmet,E.Sansone,andP.ZuidbergDosMartires,“Differentiablesamplingofcategorical
distributions using the catlog-derivative trick,” Advances in Neural Information Processing
Systems, vol. 36, 2024.
[25] R. E. Showalter, Hilbert space methods in partial differential equations. Courier Corporation,
2010.
[26] E. P. Wigner, “Characteristic vectors of bordered matrices with infinite dimensions,” Annals
of Mathematics, vol. 62, pp. 548–564, 3 1955.
[27] E. P. Wigner, “On the distribution of the roots of certain symmetric matrices,” Annals of
Mathematics, vol. 67, pp. 325–328, 2 1958.
[28] A. S. Berahas, L. Cao, K. Choromanski, and K. Scheinberg, “A theoretical and empirical
comparison of gradient approximations in derivative-free optimization,” Foundations of
Computational Mathematics, vol. 22, no. 2, pp. 507–560, 2022.
[29] A. D. Flaxman, A. T. Kalai, and H. B. McMahan, “Online convex optimization in the
bandit setting: Gradient descent without a gradient,” arXiv preprint cs/0408007, 2004.
14[30] F. Petersen, A. Mishra, H. Kuehne, C. Borgelt, O. Deussen, and M. Yurochkin, “Uncertainty
quantification via stable distribution propagation,” in Proc. International Conference on
Learning Representations (ICLR), 2024.
[31] T. S. Ferguson, “A representation of the symmetric bivariate cauchy distribution,” The
Annals of Mathematical Statistics, vol. 33, pp. 1256–1266, 4 1962.
[32] M. H. Quenouille, “Notes on bias in estimation,” Biometrika, vol. 43, pp. 353–360, 3–4 1956.
[33] J. W. Tukey, “Bias and confidence in not quite large samples,” The Annals of Mathematical
Statistics, vol. 29, p. 614, 2 1958.
[34] T. Mimori and M. Hamada, “Geophy: Differentiable phylogenetic inference via geometric
gradients of tree topologies,” Advances in Neural Information Processing Systems, 2023.
[35] J. M. Hammersley and K. W. Morton, “A new Monte Carlo technique: Antithetic variates,”
Mathematical proceedings of the Cambridge philosophical society, vol. 52, pp. 449–475, 3
1956.
[36] N. Metropolis and S. M. Ulam, “The monte carlo method,” Journal of the American
Statistical Association, vol. 44, pp. 335–341, 1949.
[37] H. Niederreiter, “Quasi-Monte Carlo methods and pseudo-random numbers,” Bulletin of the
American Mathematical Society, vol. 84, pp. 957–1041, 6 1978.
[38] P. L’Ecuyer, Randomized quasi-Monte Carlo: An introduction for practitioners. Springer,
2018.
[39] M. D. McKay, R. J. Beckman, and W. J. Conover, “A comparison of three methods
for selecting values of input variables in the analysis of output from a computer code,”
Technometrics, vol. 21, pp. 239–245, 2 1979.
[40] F. Yousefian, A. Nedić, and U. V. Shanbhag, “Convex nondifferentiable stochastic opti-
mization: A local randomized smoothing technique,” in Proceedings of the 2010 American
Control Conference, IEEE, 2010, pp. 4875–4880.
[41] J. C. Duchi, P. L. Bartlett, and M. J. Wainwright, “Randomized smoothing for stochastic
optimization,” SIAM Journal on Optimization, vol. 22, no. 2, pp. 674–701, 2012.
[42] E. Krieken, J. Tomczak, and A. Ten Teije, “Storchastic: A framework for general stochastic
automatic differentiation,” Advances in Neural Information Processing Systems, vol. 34,
pp. 7574–7587, 2021.
[43] G. Arya, M. Schauer, F. Schäfer, and C. Rackauckas, “Automatic differentiation of programs
with discrete randomness,” Advances in Neural Information Processing Systems, vol. 35,
pp. 10435–10447, 2022.
[44] M. Kagan and L. Heinrich, “Branches of a tree: Taking derivatives of programs with discrete
and branching randomness in high energy physics,” arXiv preprint arXiv:2308.16680, 2023.
[45] J. Schmidhuber, Making the world differentiable: on using self supervised fully recurrent
neural networks for dynamic reinforcement learning and planning in non-stationary envi-
ronments. Inst. für Informatik, 1990, vol. 126.
[46] R. Sutton and A. Barto, Reinforcement Learning, second edition: An Introduction (Adaptive
Computation and Machine Learning series). MIT Press, 2018, isbn: 9780262039246.
[47] M. M. Loper and M. J. Black, “OpenDR: An approximate differentiable renderer,” in
Proc. European Conference on Computer Vision (ECCV), 2014.
[48] H. Kato, Y. Ushiku, and T. Harada, “Neural 3D mesh renderer,” in Proc. International
Conference on Computer Vision and Pattern Recognition (CVPR), 2018.
15[49] F. Petersen, A. H. Bermano, O. Deussen, and D. Cohen-Or, “Pix2Vex: Image-to-Geometry
Reconstruction using a Smooth Differentiable Renderer,” Computing Research Repository
(CoRR) in arXiv, 2019.
[50] H. Kato, D. Beker, M. Morariu, T. Ando, T. Matsuoka, W. Kehl, and A. Gaidon, “Differen-
tiable rendering: A survey,” Computing Research Repository (CoRR) in arXiv, 2020.
[51] F. Petersen, B. Goldluecke, O. Deussen, and H. Kuehne, “Style agnostic 3d reconstruction
via adversarial style transfer,” in IEEE Winter Conference on Applications of Computer
Vision (WACV), 2022.
[52] F. Petersen, B. Goldluecke, C. Borgelt, and O. Deussen, “GenDR: A Generalized Differ-
entiable Renderer,” in Proc. International Conference on Computer Vision and Pattern
Recognition (CVPR), 2022.
[53] D. B. Williams, C. B. Carter, D. B. Williams, and C. B. Carter, The transmission electron
microscope. Springer, 1996.
[54] H. Rullgård, L.-G. Öfverstedt, S. Masich, B. Daneholt, and O. Öktem, “Simulation of
transmission electron microscope images of biological specimens,” Journal of microscopy,
vol. 243, no. 3, pp. 234–256, 2011.
[55] C. Sachse, J. Z. Chen, P.-D. Coureux, M. E. Stroupe, M. Fändrich, and N. Grigorieff,
“High-resolution electron microscopy of helical specimens: A fresh look at tobacco mosaic
virus,” Journal of molecular biology, vol. 371, no. 3, pp. 812–835, 2007.
[56] H. J. Suh, M. Simchowitz, K. Zhang, and R. Tedrake, “Do differentiable simulators give
better policy gradients?” In Proc. International Conference on Machine Learning (ICML),
2022.
[57] J. T. Chu and H. Hotelling, “The moments of the sample median,” The Annals of Mathe-
matical Statistics, pp. 593–606, 1955.
[58] J. Arvo and D. Kirk, “Fast ray tracing by ray classification,” ACM Siggraph Computer
Graphics, vol. 21, no. 4, pp. 55–64, 1987.
[59] Y. LeCun, C. Cortes, and C. Burges, “Mnist handwritten digit database,” 2010. [Online].
Available: http://yann.lecun.com/exdb/mnist.
[60] A. Paszke, S. Gross, F. Massa, et al., “Pytorch: An imperative style, high-performance deep
learning library,” in Proc. Neural Information Processing Systems (NeurIPS), 2019.
16A Proofs
A.1 Proof of Lemma 3
Proof of Lemma 3. Let Ω ⊂ Rn be the set of values where ∇µ(ϵ) is undefined. µ is differentiable
ϵ
a.e. and Ω has Lebesgue measure 0.
Recapitulating (5) from the proof of Lemma 1, we have
(cid:90)
∇ f (x) = − f(x+ϵ)∇µ(ϵ)dϵ. (17)
x ϵ ϵ
Replacing ∇µ(ϵ) by any of the weak derivatives ν of µ, which exists and is integrable due to
ϵ
absolute continuity, we have
(cid:90)
∇ f (x) = − f(x+ϵ)ν(ϵ)dϵ (18)
x ϵ
(cid:90) (cid:90)
= − f(x+ϵ)ν(ϵ)dϵ − f(x+ϵ)ν(ϵ)dϵ. (19)
Rn\Ω Ω
Because µ is absolutely continuous and as the Lebesgue measure of Ω is 0, per Hölder’s inequality
(cid:90) (cid:90) (cid:90) (cid:90)
|f(x+ϵ)ν(ϵ)|dϵ ≤ |f(x+ϵ)|dϵ · |ν(ϵ)|dϵ = |f(x+ϵ)|dϵ ·0 = 0 (20)
Ω Ω Ω Ω
where (cid:82) |ν(ϵ)|dϵ = 0 follows from absolute continuity of µ. Thus,
Ω
(cid:90)
f(x+ϵ)ν(ϵ)dϵ = 0. (21)
Ω
As ν = ∇µ(ϵ) for all ϵ ∈ Rn\Ω
ϵ
(cid:90) (cid:90) (cid:90)
∇ f (x) = − f(x+ϵ)ν(ϵ)dϵ − f(x+ϵ)ν(ϵ)dϵ = − f(x+ϵ)∇µ(ϵ)dϵ, (22)
x ϵ ϵ
Rn\Ω Ω Rn\Ω
showing that for all possible choices of ν, the gradient estimator coincides. Thus, we complete
our proof via
(cid:90) (cid:104) (cid:105)
∇ f (x) = − f(x+ϵ)µ(ϵ)∇ logµ(ϵ)dϵ = E f(x+ϵ) ·1 · ∇−logµ(ϵ) . (23)
x ϵ ϵ ϵ∼µ ϵ∈/Ω ϵ
Rn\Ω
After completing the proof, we remark that, if the density was not continuous, e.g., uniform
U([0,1]), then (cid:82) ∇µ(ϵ)dϵ =
(cid:104) µ(ϵ)(cid:105)ϵ↘0
= 1. This means that the weak derivative is not defined
{0} ϵ
ϵ↗0
(or loosely speaking “the derivative is infinity”), thereby violating the assumptions of Hölder’s
inequality (Eq. 20). This concludes that continuity is required for the proof to hold.
17A.2 Proof of Lemma 6
Proof of Lemma 6.
∇ f (x) = ∇ E [f(x+γ ·ϵ)] (24)
γ γϵ γ ϵ∼µ
(cid:90)
= ∇ f(x+γ ·ϵ)µ(ϵ)dϵ (25)
γ
(cid:0) u = x+ϵ·γ ⇒ ϵ = u−x ; du = γ ⇒ dϵ = 1du(cid:1) (26)
γ dϵ γ
(cid:90)
= ∇ f(u)µ(ϵ)1du (27)
γ γ
(cid:90)
= f(u)∇ (µ(ϵ)1)du (28)
γ γ
(cid:90)
= f(u)(1∇ µ(ϵ)+µ(ϵ)∇ 1)du (29)
γ γ γγ
(cid:90)
= f(u)(1(∇µ(ϵ))⊤∂ϵ −µ(ϵ) 1 )du (30)
γ ϵ ∂γ γ2
(cid:90)
= f(u)(1(∇µ(ϵ))⊤ ∂ u−x −µ(ϵ) 1 )du (31)
γ ϵ ∂γ γ γ2
(cid:90)
= f(u)(1(∇µ(ϵ))⊤(−ϵ)− 1 µ(ϵ))du (32)
γ ϵ γ γ2
(cid:90)
= f(u)(−(∇µ(ϵ))⊤ϵ−µ(ϵ)) 1 du (33)
ϵ γ2
(cid:16) (cid:17)
∇ logµ(ϵ) = 1 ∇µ(ϵ) ⇒ ∇µ(ϵ) = µ(ϵ)∇ logµ(ϵ) (34)
ϵ µ(ϵ) ϵ ϵ ϵ
(cid:90)
= f(u)(−(µ(ϵ)∇ logµ(ϵ))⊤ϵ−µ(ϵ)) 1 · γdϵ (35)
ϵ γ2
(cid:124)(cid:123)(cid:122)(cid:125)
(cid:90) =du
= f(u)·(−(∇ logµ(ϵ))⊤ϵ−1)· 1 ·µ(ϵ)dϵ (36)
ϵ γ
(cid:90)
= f(u)·(−1+(∇−logµ(ϵ))⊤ϵ)· 1 ·µ(ϵ)dϵ (37)
ϵ γ
= E (cid:2) f(x+γ ·ϵ)·(cid:0) −1+(∇−logµ(ϵ))⊤·ϵ(cid:1) /γ(cid:3) . (38)
ϵ∼µ ϵ
A.3 Proof of Theorem 7
Proof of Theorem 7.
Part 1: ∂f (x)/∂x
Lϵ
We perform a change of variables, u = x+Lϵ =⇒ ϵ = L−1(u−x) and
du dϵ dL−1(u−x) dL−1u
dϵ = dϵ = du = du = du = det(L−1)du (39)
du du du du
Thus,
(cid:90) (cid:90)
f (x) = f(x+Lϵ)µ(ϵ)dϵ = f(u)·µ(L−1(u−x))·det(L−1)du. (40)
Lϵ
18Now,
(cid:90)
∇ f (x) = ∇ f(u) ·µ(L−1(u−x))·det(L−1)du (41)
x Lϵ i x i
(cid:90)
= f(u) ·∇ (cid:0) µ(L−1(u−x))(cid:1) ·det(L−1)du (42)
i x
(cid:90)
= f(u) ·L−1·(cid:0) ∇−µ(ϵ)(cid:1) ·det(L−1)du (43)
i ϵ
(cid:90)
= f(x+Lϵ) ·L−1·∇−µ(ϵ)dϵ (44)
i ϵ
(cid:90)
= f(x+Lϵ) ·L−1·µ(ϵ)·∇−logµ(ϵ)dϵ (45)
i ϵ
(cid:104) (cid:105)
= E f(x+Lϵ) ·L−1·∇−logµ(ϵ) (46)
ϵ∼µ i ϵ
Part 2: ∂f (x)/∂L
Lϵ
We use the same change of variables as above.
∇ E (cid:2) f(x+L·ϵ) (cid:3) (47)
L ϵ∼µ i
(cid:90)
= ∇ f(x+Lϵ) ·µ(ϵ)dϵ (48)
L i
(cid:90)
= ∇ f(u) ·µ(L−1(u−x))·det(L−1)du (49)
L i
(cid:90) (cid:16) (cid:17)
= f(u) ·∇ µ(L−1(u−x))·det(L−1) du (50)
i L
(cid:90) (cid:16) (cid:17)
= f(x+Lϵ) ·∇ µ(L−1(u−x))·det(L−1) /det(L−1)dϵ (51)
i L
(cid:104) (cid:16) (cid:17) (cid:105)
= E f(x+Lϵ) ·∇ µ(L−1(u−x))·det(L−1) ·det(L)/µ(ϵ) (52)
ϵ∼µ i L
(cid:16) (cid:17)
Now, while ∇ µ(L−1(u−x))·det(L−1) may be computed via automatic differentiation, we
L
can also solve it in closed-form. Firstly, we can observe that
∇ µ(L−1(u−x)) = ∇ µ(ϵ)·∇ (L−1·(u−x)) (53)
L ϵ⊤ L
= ∇ (∇ µ(ϵ)·L−1·(u−x)) (54)
L ϵ⊤
= −L−⊤·∇µ(ϵ)·(u−x)⊤·L−⊤ (55)
ϵ
and
∇ det(L−1) = −det(L)−1·L−⊤. (56)
L
We can combine this to resolve it in closed form to:
(cid:16) (cid:17)
∇ µ(L−1(u−x))·det(L−1) = −L−⊤·∇µ(ϵ)·(u−x)⊤·L−⊤·det(L−1)
L ϵ
−µ(L−1(u−x))·det(L)−1 · L−⊤ (57)
= −L−⊤·∇µ(ϵ)·(cid:0) L−1(u−x)(cid:1)⊤ ·det(L−1)
ϵ
−µ(ϵ)·det(L)−1 · L−⊤ (58)
= −L−⊤·∇µ(ϵ)·ϵ⊤·det(L−1)
ϵ
−µ(ϵ)·det(L)−1·L−⊤ (59)
= −det(L−1)·(cid:0) L−⊤·∇µ(ϵ)·ϵ⊤+µ(ϵ)·L−⊤(cid:1) . (60)
ϵ
19Combing this with equation (52), we have
(cid:2) (cid:3)
∇ E f(x+L·ϵ)
L ϵ∼µ i
(cid:34) (cid:35)
(cid:16) (cid:17)
= E f(x+Lϵ) ·∇ µ(L−1(u−x))·det(L−1) ·det(L)/µ(ϵ)
ϵ∼µ i L
(cid:34) (cid:35)
= E f(x+Lϵ) · −det(L−1)·(cid:0) L−⊤·∇µ(ϵ)·ϵ⊤+µ(ϵ)·L−⊤(cid:1) · det(L)/µ(ϵ) (61)
ϵ∼µ i ϵ
(cid:34) (cid:35)
= E f(x+Lϵ) · −(cid:0) L−⊤·∇µ(ϵ)·ϵ⊤+µ(ϵ)·L−⊤(cid:1) /µ(ϵ) (62)
ϵ∼µ i ϵ
(cid:34) (cid:35)
= E f(x+Lϵ) · −(cid:0) L−⊤·∇µ(ϵ)·ϵ⊤/µ(ϵ)+L−⊤(cid:1) (63)
ϵ∼µ i ϵ
(cid:104) (cid:105)
= E f(x+Lϵ) · L−⊤ · (cid:0) −1+∇−logµ(ϵ)·ϵ⊤(cid:1) . (64)
ϵ∼µ i ϵ
A.4 Proof of Theorem 8
Proof of Theorem 8.
Part 1: ∂G (x)/∂x We start by stating some helpful preliminaries:
Lϵ
(cid:104) (cid:105)
G (x) = Cov (f(x+Lϵ)) = E (cid:0) f(x+Lϵ)−f (x)(cid:1)(cid:0) f(x+Lϵ)−f (x)(cid:1)⊤ (65)
Lϵ ϵ∼µ ϵ∼µ Lϵ Lϵ
(cid:104) (cid:105)
= E f(x+Lϵ)f(x+Lϵ)⊤ −f (x)f (x)⊤, (66)
ϵ∼µ Lϵ Lϵ
G (x) = Cov (f(x+Lϵ) ,f(x+Lϵ) ) (67)
Lϵ i,j ϵ∼µ i j
(cid:104) (cid:105)
= E (cid:0) f(x+Lϵ)−f (x)(cid:1) · (cid:0) f(x+Lϵ)−f (x)(cid:1) (68)
ϵ∼µ Lϵ i Lϵ j
(cid:104) (cid:105)
= E f(x+Lϵ) ·f(x+Lϵ) −f (x) ·f (x) . (69)
ϵ∼µ i j Lϵ i Lϵ j
We proceed by computing the derivative of the left part of Equation 69.
(cid:104) (cid:105)
∇ E f(x+Lϵ) ·f(x+Lϵ) (70)
x ϵ∼µ i j
(cid:104) (cid:105)
= E f(x+Lϵ) ·f(x+Lϵ) ·L−1·∇ (cid:0) −logµ(ϵ)(cid:1) , (71)
ϵ∼µ i j ϵ
which is analogous to Equations 41 until 46. Now,
(cid:104) (cid:105)
∇ G (x) = ∇ E f(x+Lϵ) ·f(x+Lϵ) −∇ (cid:0) f (x) ·f (x) (cid:1) (72)
x Lϵ i,j x ϵ∼µ i j x Lϵ i Lϵ j
(cid:104) (cid:105)
= E f(x+Lϵ) ·f(x+Lϵ) ·L−1·∇ (cid:0) −logµ(ϵ)(cid:1) (73)
ϵ∼µ i j ϵ
−f (x) ·∇ f (x) −f (x) ·∇ f (x) (74)
Lϵ i x Lϵ j Lϵ j x Lϵ i
where ∇ f (x) is defined as in part 1 of the proof of Theorem 7.
x Lϵ
Part 2: ∂G (x)/∂L
Lϵ
∇ G (x) (75)
L Lϵ i,j
(cid:104) (cid:105)
= ∇ E f(x+Lϵ) ·f(x+Lϵ) −∇ (cid:0) f (x) ·f (x) (cid:1) (76)
L ϵ∼µ i j L Lϵ i Lϵ j
(cid:104) (cid:16) (cid:17) (cid:105)
= E f(x+Lϵ) ·f(x+Lϵ) ·∇ µ(L−1(u−x))·det(L−1) ·det(L)/µ(ϵ) (77)
ϵ∼µ i j L
−f (x) ·∇ f (x) −f (x) ·∇ f (x) (78)
Lϵ i L Lϵ j Lϵ j L Lϵ i
20where ∇ f (x) is defined as in Theorem 7.
L Lϵ
(cid:16) (cid:17)
Using the closed-form solution for ∇ µ(L−1(u−x))·det(L−1) from the proof of Theorem 7,
L
we can simplify it to
∇ G (x) (79)
L Lϵ i,j
(cid:104) (cid:16) (cid:17) (cid:105)
= E f(x+Lϵ) ·f(x+Lϵ) ·∇ µ(L−1(u−x))·det(L−1) ·det(L)/µ(ϵ)
ϵ∼µ i j L
−f (x) ·∇ f (x) −f (x) ·∇ f (x) (80)
Lϵ i L Lϵ j Lϵ j L Lϵ i
(cid:104) (cid:105)
= −E f(x+Lϵ) ·f(x+Lϵ) ·(cid:0) L−⊤·∇ µ(ϵ)·ϵ⊤+µ(ϵ)·L−⊤(cid:1) /µ(ϵ)
ϵ∼µ i j ϵ
−f (x) ·∇ f (x) −f (x) ·∇ f (x) (81)
Lϵ i L Lϵ j Lϵ j L Lϵ i
(cid:104) (cid:105)
= −E f(x+Lϵ) ·f(x+Lϵ) ·(cid:0) L−⊤·∇ µ(ϵ)·ϵ⊤/µ(ϵ)+L−⊤(cid:1)
ϵ∼µ i j ϵ
−f (x) ·∇ f (x) −f (x) ·∇ f (x) (82)
Lϵ i L Lϵ j Lϵ j L Lϵ i
B Discussion of Properties of f for Finitely Defined f and ∇f
ϵ ϵ
When we have a function f that is not defined with a compact range with f : Rn → R, and have
a density µ with unbounded support (e.g., Gaussian or Cauchy), we may experience f or even
ϵ
∇f to not be finitely defined. For example, virtually any distribution with full support on R
ϵ
leads to the smoothing f of the degenerate function f : x (cid:55)→ exp(exp(exp(exp(x2)))) to not be
ϵ
finitely defined.
We say a function, as described via an expectation, is finitely defined iff it is defined (i.e., the
expectation has a value) and its value is finite (i.e., not infinity). For example, the first moment
of the Cauchy distribution is undefined, and the second moment is infinite; thus, both moments
are not finitely defined.
We remark that the considerations in this appendix also apply to prior works that enable the
real plane as the output space of f. We further remark that writing an expression for smoothing
and the gradient of a arbitrary function with non-compact range is not necessarily false; however,
e.g., any claim that smoothness is guaranteed if the gradient jumps from −∞ to ∞ (e.g., the
power tower in the first paragraph) is not formally correct. We remark that characterizing valid
fs via a Lipschitz or other continuity requirement is not applicable because this would defeat the
goal of differentiating non-differentiable and discontinuous f.
In the following, we discuss when f or ∇f are finitely defined. For this, let us cover a few
ϵ ϵ
preliminaries:
Let a function f(x) be called O(b(x)) bounded if there exist c,v ∈ O(b(x)) and c¯,v¯∈ R such
that
c¯+c(x) ≤ f(x) ≤ v¯+v(x) ∀x. (83)
For example, a function may be called polynomially bounded (wrt. a polynomial b(x)) if (but not
only if) −b(x) ≤ f(x) ≤ b(x).
Moreover, let a density µ with support R be called decaying faster than b(x) if µ(x) ∈ o(b(x)).
Forexample,thestandardGaussiandensitydecaysfasterthanexp(−|x|),i.e.,µ(x) ∈ o(exp(−|x|)).
Additionally, we can say that Gaussian density decays at rate exp(−x2), i.e., µ(x) ∈ θ(exp(−x2)).
Now, we can formally characterize finite definedness of f and ∇f :
ϵ ϵ
Lemma 9 (Finite Definedness of f ). f is finitely defined if there exists an increasing function
ϵ ϵ
b(·) such that
f(x) is bounded by O(b(x)) and µ(ϵ) ∈ O(1/b(ϵ+αϵ)/ϵ(1+α)) (84)
for some α > 0.
21Proof. To show that f exists, we need to show that
ϵ
(cid:90)
(cid:12) (cid:12)f(x+ϵ)·µ(ϵ)(cid:12) (cid:12)dϵ (85)
R
isfiniteforallx. Letf˜beanabsolutelyupperboundoff,andw.l.o.g.letuschoosef˜(y) = b(y)+¯b
with b(y) > 1 for y ∈ R. Further, as per the assumptions µ(ϵ) < 1 ·w for all ϵ < ω as
ϵ(1+α)·b(ϵ+αϵ) 1
well as all ϵ > ω for some w,ω ,ω . Let us restrict ω ,ω to ω < −|x|/α and ω > |x|/α. It is
2 1 2 1 2 1 2
trivial to see that
(cid:90) ω2(cid:12)
(cid:12)f(x+ϵ)·µ(ϵ)(cid:12) (cid:12)dϵ < ∞. (86)
ω1
W.l.o.g., let us consider the upper remainder:
(cid:90) ∞ (cid:90) ∞
(cid:12) (cid:12)f(x+ϵ)·µ(ϵ)(cid:12) (cid:12)dϵ ≤ (cid:12) (cid:12)f˜(x+ϵ)·µ(ϵ)(cid:12) (cid:12)dϵ (87)
ω2 ω2
≤
(cid:90) ∞(cid:12)
(cid:12) (cid:12)(b(x+ϵ)+¯b)· 1
·w(cid:12)
(cid:12) (cid:12) dϵ (88)
(cid:12) ϵ(1+α)·b(ϵ+αϵ) (cid:12)
ω2
= (cid:90) ∞(cid:12) (cid:12) (cid:12)(cid:18) b(x+ϵ) + ¯b (cid:19) ·w(cid:12) (cid:12) (cid:12) dϵ (89)
(cid:12) ϵ(1+α)·b(ϵ+αϵ) ϵ(1+α)·b(ϵ+αϵ) (cid:12)
ω2
≤ (cid:90) ∞(cid:12) (cid:12) (cid:12)(cid:18) b(x+ϵ) + ¯b (cid:19) ·w(cid:12) (cid:12) (cid:12) dϵ (90)
(cid:12) ϵ(1+α)·b(ϵ+|x|) ϵ(1+α)·b(ϵ+αϵ) (cid:12)
ω2
≤ (cid:90) ∞(cid:12) (cid:12) (cid:12)(cid:18) 1 + ¯b (cid:19) ·w(cid:12) (cid:12) (cid:12) dϵ (91)
(cid:12) ϵ(1+α) ϵ(1+α)·b(ϵ+αϵ) (cid:12)
ω2
< (cid:90) ∞(cid:12) (cid:12) (cid:12) 1 + ¯b (cid:12) (cid:12) (cid:12) dϵ·w (92)
(cid:12)ϵ(1+α) ϵ(1+α)(cid:12)
ω2
=
(cid:90) ∞(cid:12)
(cid:12) (cid:12) 1
(cid:12)
(cid:12) (cid:12) dϵ·w·(1+¯b) < ∞. (93)
(cid:12)ϵ(1+α)(cid:12)
ω2
That (cid:82)∞ 1 dϵ is finite for the step in (93) can be shown via
ω2 ϵ(1+α)
(cid:90) ∞ 1 (cid:90) ∞ (cid:20) 1 (cid:21)∞ (cid:20) 1 1 (cid:21) 1
dϵ = ϵ−1−αdϵ = − ϵ−α = − lim ϵ−α+ ω−α = ω−α.
ϵ(1+α) α α ϵ→∞ α 2 α 2
ω2 ω2 ω2
The same can be shown analogously for the integral (cid:82)ω1 . This completes the proof.
−∞
Lemma 10 (Finite Definedness of ∇f ). ∇f is finitely defined if there exists an increasing
ϵ ϵ
function b(·) such that
f(x) is bounded by O(b(x)) and (cid:12) (cid:12)µ(ϵ)·∇ ϵ−logµ(ϵ)(cid:12) (cid:12) ∈ O(1/b(ϵ+αϵ)/ϵ(1+α)) (94)
for some α > 0.
Proof. The proof of Lemma 9 also applies here, but with (cid:12) (cid:12)µ(ϵ)·∇ ϵ−logµ(ϵ)(cid:12) (cid:12) < ϵ(1+α)·1
b(ϵ+αϵ)
·w
for all ϵ < ω as well as all ϵ > ω for some w,ω ,ω .
1 2 1 2
Example 11 (Cauchy and the Identity). Let µ be the density of a Cauchy distribution and let
f(x) = x. The tightest b for f(x) ∈ O(b(x)) is b(x) = x.
We have µ(ϵ) ∈ θ(1/ϵ2) and thus µ(ϵ) ∈/ o(1/ϵ2). f , i.e., the mean of the Cauchy distribution
ϵ
is not defined.
However, its gradient ∇f = 1 is indeed finitely defined. In particular, we can see that
ϵ
2ϵ
µ(ϵ)·∇−logµ(ϵ) = ∈ O(1/ϵ3). (95)
ϵ π·(1+ϵ2)·(1+ϵ2)
22This is an intriguing property of the Cauchy distribution (or other edge cases) where f is
ϵ
undefined whereas ∇f is finitely and well-defined. In practice, we often only require the gradient
ϵ
for stochastic gradient descent, which means that we often only require ∇f to be well defined
ϵ
and do not necessarily need to evaluate f depending on the application.
ϵ
Additional discussions for the Cauchy distribution and an extension of stochastic smoothing
to the k-sample median can be found in the next appendix.
C Stochastic Smoothing, Medians, and the Cauchy Distribution
In this section, we provide a discussion of a special case of stochastic smoothing with the Cauchy
distribution, and provide an extension of stochastic smoothing to the k-sample median. This
becomes important if the range of f is not subset of a compact set, and thus E (cid:2) f(x+ϵ)(cid:3)
ϵ∼µ
becomes undefined for some choice of distribution µ. For example, for f(x + ϵ) = ϵ and µ
being the density of a Cauchy distribution, E (cid:2) f(x+ϵ)(cid:3) = E (cid:2) ϵ(cid:3) is undefined. Nevertheless,
ϵ∼µ ϵ∼µ
even in this case, the gradient estimators discussed in this paper for ∇ E (cid:2) f(x+ϵ)(cid:3) remain
x ϵ∼µ
well defined. This is practically relevant because E (cid:2) f(x+ϵ)(cid:3) does not need to be finitely
ϵ∼µ
defined as long as ∇ E (cid:2) f(x+ϵ)(cid:3) is well defined. Further, we remark that the undefinedness
x ϵ∼µ
of E (cid:2) f(x+ϵ)(cid:3) requires the range of f to be unbounded, i.e., if there exists a maximum /
ϵ∼µ
minimum possible output, then it is well defined. Moreover, there exist f with unbounded range
for which E (cid:2) f(x+ϵ)(cid:3) also remains well defined.
ϵ∼µ
To account for cases where E (cid:2) f(x+ϵ)(cid:3) may not be well defined or not a robust statistic, we
ϵ∼µ
introduce an extension of smoothing to the median. We begin by defining the k-sample median.
Definition 12 (k-Sample Median). For a number of samples k > 1, and a distribution ζ, we say
that
(cid:104) (cid:105)
E median{z ,z ,...,z } (96)
z1,z2,...,z k∼ζ 1 2 k
is the k-sample median. For multivariate distributions, let median be the per-dimension median.
Indeed, for k ≥ 5, the k-sample median estimator is shown to have finite variance for the
Cauchy distribution (Theorem 3 and Example 2 in [57]), which implies a well defined k-sample
median. Moreover, for any distribution with a density of the median bounded away from 0, the
first and second moments are guaranteed to be finitely defined for sufficiently large k. This is
important for non-trivial f with f(ϵ) ̸= ϵ for at least one ϵ with ϵ ∼ µ, which implies ζ ̸= µ. Thus,
rather than computing and differentiating the expected value, we can differentiate the k-sample
median.
Lemma 13 (Differentiation of the k-Sample Median). With the k-sample median smoothing as
(cid:104) (cid:105)
f(k)(x) = E median{f(x+ϵ ),...,f(x+ϵ )} , (97)
ϵ ϵ1,...,ϵ k∼µ 1 k
(k)
we can differentiate f (x) as
ϵ
(cid:104) (cid:105)
∇ f(k)(x) = E f(x+ϵ )·∇ −logµ(ϵ ) (98)
x ϵ ϵ1,...,ϵ k∼µ r(ϵ) ϵ r(ϵ) r(ϵ)
where r(ϵ) is the arg-median of the set {f(x+ϵ ),...,f(x+ϵ )}, which is equivalent to the implicit
1 k
definition via f(x+ϵ ) = median{f(x+ϵ ),...,f(x+ϵ )}.
r(ϵ) 1 k
23Proof. We denote ϵ ∼ µ(1:k) such that ϵ = (cid:2) ϵ⊤,...,ϵ⊤(cid:3)⊤ and ϵ ∼ µ ∀i ∈ {1,...,k}.
1:k 1:k 1 k i
(cid:104) (cid:105)
∇ f(k)(x) = ∇ E median{f(x+ϵ ),...,f(x+ϵ )} (99)
x ϵ x ϵ1,...,ϵ k∼µ 1 k
(cid:104) (cid:105)
= ∇ E median{f(x+ϵ ),...,f(x+ϵ )} (100)
x ϵ ∼µ(1:k) 1 k
1:k
(cid:90)
= ∇ median{f(x+ϵ ),...,f(x+ϵ )}·µ(1:k)(ϵ )dϵ (101)
x 1 k 1:k 1:k
Rn·k
k (cid:90)
(cid:0) x ,...,x =x(cid:1) = (cid:88) ∇ median{f(x +ϵ ),...,f(x +ϵ )}·µ(1:k)(ϵ )dϵ (102)
1 k xj 1 1 k k 1:k 1:k
Rn·k
j=1
As a shorthand, we abbreviate the indicator 1 as 1 and
f(xj+ϵj)=median{f(x1+ϵ1),...,f(x k+ϵ k)} j,ϵ 1:k
abbreviate 1 as 1 :
f(uj)=median{f(u1),...,f(u k)} j,u 1:k
k (cid:90)
(cid:88)
∇ f(k)(x) = ∇ f(x +ϵ )·1 ·µ(1:k)(ϵ )dϵ (103)
x ϵ xj j j j,ϵ 1:k 1:k 1:k
Rn·k
j=1
k (cid:90)
(cid:88)
= ∇ f(u)·1 ·µ(1:k)(u −x)du (104)
xj j,u1:k 1:k 1:k
Rn·k
j=1
k (cid:90)
(cid:88)
= f(u)·1 ·∇ µ(1:k)(u −x)du (105)
j,u1:k xj 1:k 1:k
Rn·k
j=1
k (cid:90)
(cid:88)
= f(x+ϵ )·1 ·−∇ µ(1:k)(ϵ )dϵ (106)
j j,ϵ 1:k ϵj 1:k 1:k
Rn·k
j=1
We have
∇ µ(1:k)(ϵ ) = µ(1:k)(ϵ )·∇ logµ(1:k)(ϵ ) = µ(1:k)(ϵ )·∇ logµ(ϵ ). (107)
ϵj 1:k 1:k ϵj 1:k 1:k ϵj j
Thus,
k (cid:90)
(cid:88)
∇ f(k)(x) = f(x+ϵ )·1 ·−µ(1:k)(ϵ )·∇ logµ(ϵ )dϵ (108)
x ϵ j j,ϵ 1:k 1:k ϵj j 1:k
Rn·k
j=1
(cid:90) k
= (cid:88)(cid:2) 1 ·f(x+ϵ )·∇ −logµ(ϵ )(cid:3) ·µ(1:k)(ϵ )dϵ (109)
j,ϵ 1:k j ϵj j 1:k 1:k
Rn·k
j=1
Indicating the choice of median in dependence of ϵ , we define r(ϵ ) s.t. 1 = 1. Thus,
1:k 1:k r(ϵ ),ϵ
1:k 1:k
(cid:90)
∇ f(k)(x) = f(x+ϵ )·∇ −logµ(ϵ )·µ(1:k)(ϵ )dϵ (110)
x ϵ
Rn·k
r(ϵ 1:k) ϵ r(ϵ1:k) r(ϵ 1:k) 1:k 1:k
(cid:104) (cid:105)
= E f(x+ϵ )·∇ −logµ(ϵ ) (111)
ϵ 1:k∼µ(1:k) r(ϵ 1:k) ϵ r(ϵ1:k) r(ϵ 1:k)
This concludes the proof.
Empirically, we can estimate ∇ f(k) (x) for s propagated samples (s > k) without bias as
x ϵ
s
(cid:88) (cid:104) (cid:105)
∇ f(k)(x) ≜ q ·f(x+ϵ )·∇ −logµ(ϵ ) ϵ ,...,ϵ ∼ µ (112)
x ϵ i i ϵi i 1 s
i=1
24where q is the probability of f(x+ϵ ) being the median in a subset of k samples, i.e., under
i i
uniqueness of g s, we have
i
(cid:88) (cid:0) (cid:1)
1 g = median{h ,...,h }
i 1 k
q = {h1,...,h k}⊂{g1,...,gs} g := f(x+ϵ ). (113)
i (cid:18) (cid:19) i i
s
k
We remark that, in case of non-uniqueness, it is adequate to split the probability among the
candidates; however, under non-discreteness assumptions on f (density of ζ < ∞, the converse
typically implies the range of f being a subset of a compact set), this almost surely (with
probability 1) does not occur.
We have shown that the k-sample median f(k) (x) is differentiable and demonstrated an
ϵ
unbiasedgradientestimatorforit. Astraightforwardextensionforthecaseoff beingdifferentiable
is differentiating through the median via a k → ∞-sample median, e.g., via setting s = k2. The
k → ∞ extension for differentiating through the median itself requires f being differentiable
because, for discontinuous f, f(k) (x) is differentiable only for k < ∞. (As an illustration, the
ϵ
median of the Heaviside function under a symmetric perturbation µ with density at 0 bounded
away from 0 is the exactly the Heaviside function.)
D Experimental Details
MNIST Sorting Benchmark Experiments We train for 100000 steps at a learning rate
of 0.001 with the Adam optimizer using a batch size of 100. Following the requirements of the
benchmark, we use the same model as previous works [7], [8], [11]. That is, two convolutional
layers with a kernel size of 5×5, 32 and 64 channels respectively, each followed by a ReLU and
MaxPool layer; after flattening, this is followed by a fully connected layer with a size of 64, a
ReLU layer, and a fully connected output layer mapping to a scalar. For each distribution and
number of samples, we choose the optimal γ ∈ {1,1/3,0.1}.
Warcraft Shortest-Path Benchmark Experiments Following the established protocol [17],
we train for 50 epochs with the Adam optimizer at a batch size of 70 and an initial learning rate
of 0.001. The learning rate decays by a factor of 10 after 30 and 40 epochs each. The model is
the first block of ResNet18. The hyperparameter γ = 1/β as specified in Figures 13 and 14.
Utah Teapot Camera Pose Optimization Experiments We initialize the pose to be
perturbed by angles uniformly sampled from [15◦,75◦]. The ground truth orientation is randomly
sampled from the sphere of possible orientations. The ground truth camera angle is 20◦, and
the ground truth camera distance is uniformly sampled from [2.5,4]. The initial camera distance
is sampled as being uniformly offset by [−0.5,6], thus the feasible set of initial camera distance
guesses lies in [2,10]. The initial camera angle is uniformly sampled from [10◦,30◦]. We optimize
for 1000 steps with the Adam optimizer [(β ,β ) = (0.5,0.99)] and the CosineAnnealingLR
1 2
scheduler with an initial learning rate of 0.3. We schedule the diagonal of L to decay exponentially
from [0.1,5◦,5◦,0.25◦]·100.75 to [0.1,5◦,5◦,0.25◦]·10−1.75 (the dimensions are camera distance, 2
pose angles, and the camera angle). As discussed, the success criterion is finding the angle within
5◦ of the ground truth angle. There is typically no local minimum within 5◦ and it is a reliable
indicator for successful alignment.
Differentiable Cryo-Electron Tomography Experiments The ground truth values of the
parameters are set to 300 kV for acceleration voltage, 3 mm for the focal length, and the ground
truth sample specimen is centered as (x,y) = (0,0) nm units. For reporting the RMSE metric,
25the acceleration voltages are normalized by a factor of 100 to ensure that all parameters vary
over commensurate ranges. For the 2-parameter optimization, the feasible set of acceleration
voltage varied over a range of [0,1000] kV and the feasible set of the specimen’s x-position varied
over the range [−5,5]. For the 4-parameter optimization, the feasible set of acceleration voltage
varied over a range of [0,600] kV, the focal length ranges over [0,6] mm, the x- and y-positions
range over [−3,3]. We use the Adam optimizer for both experiments, with [(β ,β ) = (0.5,0.9)].
1 2
For the MC Search baseline, we generate sets of n uniform random points in the feasible region
of the parameters, generate micrographs for these random parameter tuples using the TEM
simulator [54], and identify the parameter tuple in the set having the lowest mean squared error
with respect to the ground truth image. The RMSE between this parameter tuple and the ground
truth parameters is the metric for the specific set of n randomly generated values. This is repeated
20 times to obtain the mean and standard deviation of the RMSE metric at that n.
D.1 Assets
List of assets:
• The sixth platonic solid (aka. Teapotahedron or Utah tea pot) [58] [License N/A]
• Multi-digit MNIST [8], which builds on MNIST [59] [MIT License / CC License]
• Warcraft shortest-path data set [17] [MIT License]
• PyTorch [60] [BSD 3-Clause License]
• TEM-simulator [54] [GNU General Public License]
D.2 Runtimes
The runtimes for sorting and shortest-path experiments are for one full training on 1 GPU. The
pose optimization experiment runtimes are the total time for all 768 seeds on 1 GPU. For the
TEM-simulator, we report the CPU time per simulation sample, which is the dominant and only
the measureable component of the total optimization routine time. The choice of distribution,
covariate, and choice of variance reduction does not have a measurable effect on training times.
• MNIST Sorting Benchmark Experiments [1 Nvidia V100 GPU]
– Training w/ 256 samples: 65 min
– Training w/ 1024 samples: 67 min
– Training w/ 2048 samples: 68 min
– Training w/ 8192 samples: 77 min
– Training w/ 32768 samples: 118 min
• Warcraft Shortest-Path Benchmark Experiments [1 Nvidia V100 GPU]
– Training w/ 10 samples: 9 min
– Training w/ 100 samples: 19 min
– Training w/ 1000 samples: 26 min
– Training w/ 10000 samples: 101 min
• Utah Teapot Camera Pose Optimization Experiments [1 Nvidia A6000 GPU]
– Optimization on 768 seeds w/ 16 samples: 25 min
– Optimization on 768 seeds w/ 64 samples: 81 min
26– Optimization on 768 seeds w/ 256 samples: 362 min
• Differentiable Cryo-Electron Tomography Experiments [CPU: 44 Intel Xeon Gold 5118]
– Simulator time per sample on 1 CPU core: 67 sec
E Additional Experimental Results
Table 3: Extension of Table 2 with additional numbers of samples and standard deviations.
Baselines Neu.S. Soft.S. L.DSN C.DSN E.DSN OT.S.
— 71.3 70.7 77.2 84.9 85.0 81.1
Sampling #s Gauss. Logis. Gumbel Cauchy Laplace Trian.
vanilla 256 82.3±2.0 82.8±0.9 79.2±9.7 68.1±19.3 82.6±0.8 81.3±1.2
best (cv) 256 83.1±1.6 82.7±1.8 81.6±3.6 55.6±13.3 83.7±0.8 82.7±1.1
vanilla 1024 81.3±9.1 83.7±0.7 82.0±1.6 68.5±24.8 80.6±9.0 82.8±1.0
best (cv) 1024 83.9±0.6 84.0±0.5 84.2±0.6 73.0±12.6 84.3±0.6 82.4±1.6
vanilla 2048 84.1±0.6 83.6±0.8 84.0±0.5 75.7±11.6 83.8±0.7 83.2±0.6
best (cv) 2048 84.2±0.5 84.2±0.6 84.6±0.4 82.0±2.2 84.8±0.5 83.4±0.5
vanilla 8192 84.0±0.6 84.2±0.8 84.0±0.6 83.6±1.0 83.9±1.0 83.6±0.7
best (cv) 8192 84.4±0.6 84.5±0.5 84.1±0.7 84.3±0.5 84.3±0.4 83.7±0.4
vanilla 32768 84.2±0.5 84.1±0.4 84.5±0.7 84.9±0.5 84.4±0.5 83.4±0.8
best (cv) 32768 84.4±0.4 84.4±0.4 84.8±0.5 85.1±0.4 84.4±0.4 84.0±0.3
Gaussian Logistic Gumbel Cauchy Laplace Triangular
Figure11: AverageL normsbetweengroundtruth(oracle)andestimated
2
gradientfordifferentnumbersofelementstosortandrankn, anddifferent
distributions. Each plot compares different variance reduction strategies
as indicated in the legend to the right of the caption. Darker is better MC
QMC(latin)
(smaller values). Colors are only comparable within each subplot. We use
RQMC(latin)
1024 samples, except for Cartesian and n=3 where we use 103 =1000 RQMC(cart.)
samples.
regular antithetic
27
3=n
5=n
7=n
01=n
enon )x(f OOL enon )x(f OOL0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
Gaussian Logistic Gumbel Cauchy Laplace Triangular
Figure 12: Utah teapot camera pose optimization with smoothing of the
loss, compared to Figure 8, which performs smoothing of the algorithm. MC
Smoothing the algorithm is consistently better, with the largest effect for MC(at.)
larger numbers of samples. Results averaged over 768 seeds. QMC(lat.)
RQMC(lat.)
RQMC(car.)
28
61=s#
46=s#
652=s#
enon )x(f OOLGaussian Logistic Gumbel Cauchy Laplace Triangular Gaussian Logistic Gumbel Cauchy Laplace Triangular
0.95
Figure 13: Warcraft shortest-path experiment. Left: 10 samples. Right: 0.90
100 samples. Averaged over 5 seeds. Brighter is better. Values between MC 0.85
subplots are comparable. The displayed range is [70%,96.5%]. MC(antith.)
0.80
QMC(latin)
RQMC(latin) 0.75
0.70
29
1=
|
ogla
3=
|
ogla
01=
|
ogla
03=
|
ogla
001=
|
ogla
1=
|
ssol
3=
|
ssol
01=
|
ssol
03=
|
ssol
001=
|
ssol
1=
|
ogla
3=
|
ogla
01=
|
ogla
03=
|
ogla
001=
|
ogla
1=
|
ssol
3=
|
ssol
01=
|
ssol
03=
|
ssol
001=
|
ssol
enon )x(f OOLGaussian Logistic Gumbel Cauchy Laplace Triangular Gaussian Logistic Gumbel Cauchy Laplace Triangular
0.95
Figure 14: Warcraft shortest-path experiment. Left: 1000 samples. Right: 0.90
10000 samples. Averaged over 5 seeds. Brighter is better. Values between MC 0.85
subplots are comparable. The displayed range is [70%,96.5%]. MC(antith.)
0.80
QMC(latin)
RQMC(latin) 0.75
0.70
30
3=
|
ogla
01=
|
ogla
03=
|
ogla
001=
|
ogla
003=
|
ogla
3=
|
ssol
01=
|
ssol
03=
|
ssol
001=
|
ssol
003=
|
ssol
3=
|
ogla
01=
|
ogla
03=
|
ogla
001=
|
ogla
003=
|
ogla
3=
|
ssol
01=
|
ssol
03=
|
ssol
001=
|
ssol
003=
|
ssol
enon )x(f OOL10 MC [none] 10 MC [none]
MC [LOO] MC [LOO]
8 QMC (latin) [LOO] 8 QMC (latin) [LOO]
RQMC (cart.) [LOO] RQMC (cart.) [LOO]
RQMC (latin) [none] RQMC (latin) [none]
6 RQMC (latin) [LOO] 6 RQMC (latin) [LOO]
RQMC (latin) [f(x)] RQMC (latin) [f(x)]
4 MC Search 4 MC Search
2 2
0 0
0 100 200 300 400 500 600 700 0 100 200 300 400 500 600 700
num. samples num. samples
10 MC [none] 8 MC [none]
MC [LOO] MC [LOO]
QMC (latin) [LOO] QMC (latin) [LOO]
8 RQMC (cart.) [LOO] 6 RQMC (cart.) [LOO]
RQMC (latin) [none] RQMC (latin) [none]
6 RQMC (latin) [LOO] RQMC (latin) [LOO]
RQMC (latin) [f(x)] 4 RQMC (latin) [f(x)]
4 MC Search MC Search
2
2
0 0
0 100 200 300 400 500 600 700 0 200 400 600 800 1000
num. samples num. samples
Figure 15: Cryo-Electron Tomography Experiments: RMSE with respect to Ground Truth parameters for
different number of parameters optimized and for different number of samples per optimization step: (Top
Left) 2-parameters & number of samples=9, (Top Right) 2-parameters & number of samples=25, (Bottom
Left) 2-parameters & number of samples=36, (Bottom Right) 4-parameters. No marker lines correspond
to Gaussian, × corresponds to Laplace, and △ corresponds to Triangular distributions. Ascertaining
optimal parameters with minimal evaluations is important not just for high resolution imaging, but also
to minimize radiation damage to the specimen. In this light, of the covariate choices, LOO generally
leads to best improvement and none consistently leads to deterioration in performance. The Laplace and
Triangular distributions lead to best performance. For the Gaussian distribution, Cartesian RQMC is
generally exhibiting best results.
31
TG
ot
ESMR
TG
ot
ESMR
TG
ot
ESMR
TG
ot
ESMRTable 4: Individual absolute values from the variance simulations for differentiable sorting in Figure 3.
The minimum and values within 1% of the minimum are indicated as bold.
(a) values for Gaussian (n=3) (b) values for Gaussian (n=5)
none f(x) LOO none f(x) LOO none f(x) LOO none f(x) LOO
regular antithetic regular antithetic
MC 0.0084 0.0079 0.0046 0.0055 0.0054 0.0053 MC 0.0241 0.0308 0.0171 0.0192 0.0192 0.0192
QMC(lat.) 0.0029 0.0030 0.0030 0.0036 0.0036 0.0036 QMC(lat.) 0.0143 0.0144 0.0144 0.0164 0.0164 0.0164
RQMC(l.) 0.0030 0.0030 0.0030 0.0036 0.0035 0.0036 RQMC(l.) 0.0145 0.0145 0.0144 0.0164 0.0164 0.0162
RQMC(c.) 0.0012 0.0013 000...000000111222 0.0014 0.0014 0.0014 RQMC(c.) 0.0103 0.0116 000...000000999777 — — —
(c) values for Logistic (n=3) (d) values for Logistic (n=5)
none f(x) LOO none f(x) LOO none f(x) LOO none f(x) LOO
regular antithetic regular antithetic
MC 0.0028 0.0030 0.0016 0.0019 0.0019 0.0019 MC 0.0081 0.0114 0.0061 0.0067 0.0067 0.0067
QMC(lat.) 0.0012 0.0012 0.0012 0.0014 0.0014 0.0014 QMC(lat.) 0.0053 0.0053 0.0054 0.0060 0.0060 0.0060
RQMC(l.) 0.0012 0.0012 0.0012 0.0014 0.0013 0.0014 RQMC(l.) 0.0053 0.0054 0.0053 0.0060 0.0060 0.0059
RQMC(c.) 000...000000000333 0.0003 000...000000000333 0.0004 0.0004 0.0004 RQMC(c.) 0.0033 0.0036 000...000000333333 — — —
(e) values for Gumbel (n=3) (f) values for Gumbel (n=5)
none f(x) LOO none f(x) LOO none f(x) LOO none f(x) LOO
regular antithetic regular antithetic
MC 0.0086 0.0082 0.0048 — — — MC 0.0243 0.0323 0.0177 — — —
QMC(lat.) 0.0033 0.0033 0.0032 — — — QMC(lat.) 0.0151 0.0149 0.0150 — — —
RQMC(l.) 0.0033 0.0033 0.0033 — — — RQMC(l.) 0.0150 0.0151 0.0150 — — —
RQMC(c.) 0.0017 0.0018 000...000000111444 — — — RQMC(c.) 0.0124 0.0148 000...000111000999 — — —
(g) values for Cauchy (n=3) (h) values for Cauchy (n=5)
none f(x) LOO none f(x) LOO none f(x) LOO none f(x) LOO
regular antithetic regular antithetic
MC 0.0043 0.0044 0.0026 0.0030 0.0030 0.0030 MC 0.0123 0.0169 0.0094 0.0102 0.0101 0.0102
QMC(lat.) 0.0022 0.0022 0.0022 0.0027 0.0027 0.0027 QMC(lat.) 0.0088 0.0087 0.0088 0.0098 0.0098 0.0098
RQMC(l.) 0.0022 0.0022 0.0022 0.0027 0.0026 0.0027 RQMC(l.) 0.0088 0.0088 0.0087 0.0098 0.0097 0.0097
RQMC(c.) 0.0006 0.0006 000...000000000555 0.0006 0.0006 0.0006 RQMC(c.) 0.0061 0.0070 000...000000555666 — — —
(i) values for Laplace (n=3) (j) values for Laplace (n=5)
none f(x) LOO none f(x) LOO none f(x) LOO none f(x) LOO
regular antithetic regular antithetic
MC 0.0086 0.0074 0.0044 0.0054 0.0054 0.0054 MC 0.0245 0.0305 0.0176 0.0191 0.0192 0.0192
QMC(lat.) 0.0037 0.0037 0.0038 0.0046 0.0046 0.0047 QMC(lat.) 0.0159 0.0160 0.0160 0.0182 0.0180 0.0182
RQMC(l.) 0.0037 0.0037 0.0037 0.0047 0.0046 0.0046 RQMC(l.) 0.0160 0.0159 0.0159 0.0182 0.0181 0.0181
RQMC(c.) 000...000000000999 000...000000000999 000...000000000999 0.0010 0.0011 0.0010 RQMC(c.) 000...000000999111 000...000000999111 000...000000999111 — — —
(k) values for Triangular (n=3) (l) values for Triangular (n=5)
none f(x) LOO none f(x) LOO none f(x) LOO none f(x) LOO
regular antithetic regular antithetic
MC 0.1191 0.0683 0.0490 0.0659 0.0624 0.0602 MC 0.3329 0.2779 0.1857 0.2255 0.2157 0.2149
QMC(lat.) 000...000111666666 0.0169 000...000111666666 0.0189 0.0188 0.0188 QMC(lat.) 000...000888444444 000...000888444555 000...000888555111 0.0932 0.0931 0.0928
RQMC(l.) 0.0498 0.0358 0.0352 0.0444 0.0417 0.0431 RQMC(l.) 0.1768 0.1872 0.1479 0.1827 0.1765 0.1737
RQMC(c.) 0.0682 0.0494 0.0361 0.0435 0.0461 0.0452 RQMC(c.) 0.2251 0.2325 0.1430 — — —
32Table 5: Individual absolute values from the variance simulations for differentiable shortest-paths in
Figure 4. The minimum and values within 1% of the minimum are indicated as bold.
(a) values for Gaussian (8×8) (b) values for Gaussian (12×12)
none f(x) LOO none f(x) LOO none f(x) LOO none f(x) LOO
regular antithetic regular antithetic
MC 1330.01 4.17 4.17 8.32 8.32 8.34 MC 6800.98 20.93 20.95 41.82 41.78 41.88
QMC (lat.) 444...000444 444...000444 444...000444 8.04 8.04 8.07 QMC (lat.) 222000...666000 222000...666000 222000...666555 41.12 41.11 41.18
RQMC (l.) 4.25 444...000555 444...000555 8.10 8.09 8.12 RQMC (l.) 21.69 222000...666666 222000...666888 41.31 41.33 41.42
(c) values for Logistic (8×8) (d) values for Logistic (12×12)
none f(x) LOO none f(x) LOO none f(x) LOO none f(x) LOO
regular antithetic regular antithetic
MC 1449.44 4.53 4.53 9.04 9.04 9.05 MC 7447.38 22.83 22.86 45.62 45.61 45.75
QMC (lat.) 444...444222 444...444222 444...444333 8.80 8.80 8.83 QMC (lat.) 222222...555666 222222...555666 222222...666111 45.01 44.99 45.07
RQMC (l.) 444...444444 444...444444 444...444444 8.88 8.87 8.90 RQMC (l.) 222222...666666 222222...666555 222222...666888 45.30 45.32 45.41
(e) values for Gumbel (8×8) (f) values for Gumbel (12×12)
none f(x) LOO none f(x) LOO none f(x) LOO none f(x) LOO
regular antithetic regular antithetic
MC 2275.31 10.35 9.08 — — — MC 11642.74 52.89 46.11 — — —
QMC (lat.) 9.11 888...888444 888...888555 — — — QMC (lat.) 46.88 444555...444111 444555...444888 — — —
RQMC (l.) 11.33 888...999111 888...999111 — — — RQMC (l.) 58.12 444555...777444 444555...888000 — — —
(g) values for Cauchy (8×8)
none f(x) LOO none f(x) LOO
regular antithetic
MC 249027.67 263426.66 255440.59 507004.19 525973.88 509764.25
QMC (lat.) 222555333333...222444 222555333222...999333 222555333777...333222 222555333111...222444 222555333222...999222 222555333777...333555
RQMC (l.) 251018.28 267124.91 264146.84 476293.00 507766.00 529030.06
(h) values for Cauchy (12×12)
none f(x) LOO none f(x) LOO
regular antithetic
MC 1316801.88 1284078.38 1297748.25 2657888.00 2631427.25 2633413.50
QMC (lat.) 111222999222222...777999 111222999222222...333111 111222999444888...777555 111222999333111...222888 111222999222888...222222 111222999444555...222777
RQMC (l.) 1318297.38 1299869.75 1365709.75 2606723.50 2615697.50 2529304.00
(i) values for Laplace (8×8) (j) values for Laplace (12×12)
none f(x) LOO none f(x) LOO none f(x) LOO none f(x) LOO
regular antithetic regular antithetic
MC 2641.38 8.15 8.15 16.28 16.27 16.29 MC 13593.82 444111...444000 444111...444555 82.73 82.71 82.92
QMC (lat.) 888...000444 888...000555 888...000666 16.01 16.00 16.04 QMC (lat.) 444111...000666 444111...000777 444111...111666 81.78 81.75 81.92
RQMC (l.) 888...000999 888...000999 888...111000 16.19 16.17 16.22 RQMC (l.) 444111...333222 444111...333111 444111...333666 82.62 82.64 82.80
(k) values for Triangular (8×8) (l) values for Triangular (12×12)
none f(x) LOO none f(x) LOO none f(x) LOO none f(x) LOO
regular antithetic regular antithetic
MC 3090.80 10.21 10.11 20.27 20.43 20.07 MC 15975.60 49.73 49.89 99.81 99.32 100.31
QMC (lat.) 555...555777 555...555777 555...555777 10.17 10.18 10.20 QMC (lat.) 222888...222888 222888...222888 222888...333444 51.79 51.79 51.86
RQMC (l.) 884.22 9.88 9.82 19.14 19.71 19.76 RQMC (l.) 4606.71 49.01 49.47 98.56 98.66 98.01
33